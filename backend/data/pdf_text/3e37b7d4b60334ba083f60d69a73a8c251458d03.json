{
  "paperId": "3e37b7d4b60334ba083f60d69a73a8c251458d03",
  "title": "Research Frontiers in Information Retrieval: Report from the Third Strategic Workshop on Information Retrieval in Lorne (SWIRL 2018)",
  "pdfPath": "3e37b7d4b60334ba083f60d69a73a8c251458d03.pdf",
  "text": " \n \n \nResearch Frontiers in Information Retrieval: Report from the Third Strategic Workshop on\nInformation Retrieval in Lorne (SWIRL 2018)\nCulpepper, J.; Diaz, F.; Smucker, M.D.; Vries, A.P. de\n2018, Article / Letter to editor (Sigir Forum, 52, 1, (2018), pp. 34-90)\nDoi link to publisher: https://doi.org/10.1145/3274784.3274788\n \n \n \n \n \n \n \nVersion of the following full text: Publisher’s version\nPublished under the terms of article 25fa of the Dutch copyright act. Please follow this link for the\nTerms of Use: https://repository.ubn.ru.nl/page/termsofuse\nDownloaded from: https://hdl.handle.net/2066/199898\nDownload date: 2025-11-11\n \n \n \n \n \n \n \n \n \n \nNote:\nTo cite this publication please use the final published version (if applicable).\n\nARTICLE\nResearch Frontiers in Information Retrieval\nReport from the Third Strategic Workshop\non Information Retrieval in Lorne\n(SWIRL 2018)\nEditors\nJ. Shane Culpepper, Fernando Diaz, and Mark D. Smucker\nAuthors and Participants (listed alphabetically)\nJames Allan, Jaime Arguello, Leif Azzopardi, Peter Bailey, Tim Baldwin, Kr isztian Balog,\nHannah Bast, Nick Belkin, Klaus Berberich, Bodo Billerbeck, Jamie C allan, Rob Capra,\nMark Carman, Ben Carterette, Charles L. A. Clarke, Kevyn Collins-Thomp son, Nick Craswell\nW. Bruce Croft, J. Shane Culpepper, Jeﬀ Dalton, Gianluca Demartini , Fernado Diaz\nLaura Dietz, Susan Dumais, Carsten Eickhoﬀ, Nicola Ferro, Norbert Fuhr, Shlomo Geva\nClaudia Hauﬀ, David Hawking, Hideo Joho, Gareth Jones, Jaap Kamps, Noriko Kand o, Diane\nKelly, Jaewon Kim, Julia Kiseleva, Yiqun Liu, Xiaolu Lu, Stefano Mizzaro, Alistair Moﬀat,\nJian-Yun Nie, Alexandra Olteanu, Iadh Ounis, Filip Radlinski, Maarte n de Rijke, Mark Sanderson,\nFalk Scholer, Laurianne Sitbon, Mark Smucker, Ian Soboroﬀ, Damiano Spina, T orsten Suel,\nJames Thom, Paul Thomas, Andrew Trotman, Ellen Voorhees, Arjen P. de Vri es, Emine Yilmaz,\nGuido Zuccon\nAbstract\nThe purpose of the Strategic Workshop in Information Retrieval in Lorne is to explore\nthe long-range issues of the Information Retrieval ﬁeld, to recognize ch allenges that are\non – or even over – the horizon, to build consensus on some of the key chall enges, and\nto disseminate the resulting information to the research communit y. The intent is that this\ndescription of open problems will help to inspire researchers and gr aduate students to address\nthe questions, and will provide funding agencies data to focus and c oordinate support for\ninformation retrieval research.\nACM SIGIR Forum 34 Vol. 52 No. 1 June 2018\n\n1 Introduction\nOver the past ﬁfteen years, three Strategic IR Workshops hav e been organized in Lorne, Aus-\ntralia, all of which have had a singular vision – to look back at how research has evolved in the\nInformation Retrieval community, and to look forward on wher e the research frontier is taking us.\nThe ﬁrst SWIRL workshop was organized by Alistair Moﬀat and Just in Zobel in 2004, and had\n35 participants – several of which were PhD students. The maj or output of the meeting was the\nSIGIR Forum article “Recommended Reading for IR Research St udents.”1\nIn 2012, the second SWIRL workshop was organized by James Allan, Bruce Croft, Alistair\nMoﬀat, MarkSanderson, andJustinZobel. Thethemeofthework shopshiftedawayfromprevious\nwork, and focused more on future directions for the IR resear ch community. Together, the 45\nattendees debated several possible research topics, and ev entually converged on 6 main themes\nand 21 minor themes. These themes were then summarized and pu blished in the SIGIR Forum\narticle “Frontiers, Challenges, and Opportunities for Inf ormation Retrieval.”2\nMany of the themes described in the 2012 SWIRL report have seen signiﬁcant progress in the\nensuing years, but not all of them. At the 25th Anniversary TRE C reception in 2016, several IR\nresearchers reminisced about the SWIRL outcomes, and agreed t hat the major research directions\nin IR had evolved enough to warrant a third SWIRL. From these di scussions, the main theme of\nthe Third SWIRL emerged – How has research in IR evolved in the la st ﬁve years, and where\ndo we expect to be ﬁve years from now? In order to achieve this g oal, a third SWIRL was\norganized by Shane Culpepper and Fernando Diaz. A total of 60 IR researchers, 20 from three\nregions (North/South America, Europe, Oceania) were invited to Lorne to discuss the future of\nIR research. This report captures the ensuing surveys and ho mework assignments in the lead up\nto SWIRL 2018, and summarizes the main outcomes of the meeting i n Lorne.\n1.1 Workshop Format\nThe workshop followed the format originally devised in the 2 012 SWIRL meeting. On the ﬁrst\nevening, a reception was held, and answers from the homework assignments were summarized and\ndiscussed. A bus then took all of the participants from Melbo urne to Lorne the next morning. Af-\nter lunch, six seed talks were given, and summarized below. O n the second day of the workshop,\nthe morning sessions were composed of six groups of ten parti cipants breaking out and brain-\nstorming about the future of IR based on the initial seed disc ussions. Each group then voted, and\npitched three ideas that they thought were the most important . These 18 ideas were then grouped\nby similarity, and participants voted on the topics they were most interested in exploring further.\nThe afternoon session then contained the breakout focus gro ups. A total of eight focus groups\nformed, and these make up the main sections of this report. Oth er topics that were proposed but\nthat did not progress to the focus group stage are included at t he end of the report as “Minor\nTopics”. The ﬁnal day of the workshop was a continuation of the focus groups. All participants\nﬁnished up discussions on the topics, and worked together to produce a summary report of these\ndiscussions.\n1https://doi.org/10.1145/1113343.1113344\n2https://doi.org/10.1145/2215676.2215678\nACM SIGIR Forum 35 Vol. 52 No. 1 June 2018\n\n1.2 Invitation Questionnaire\nAs part of the initial RSVP for SWIRL, participants were asked wh at topics they thought were\nimportant. Table 1 shows the most common responses. The numb er of respondents suggesting the\ntopic is shown in parenthesis. There was a strong consensus t hat Conversational Search, Machine\nLearning, and Fairness, Accountability, Conﬁdentiality, a nd Transparency (FACT*) / Responsible\nIR were three important topics for discussion at the worksho p.\n1.3 Pre-Meeting Homework\n1.3.1 Retrospective Questionnaire of Previous SWIRL Reports\nThe ﬁrst homework task assigned to participants was to go bac k and read the SWIRL reports\nfrom 2004 and 2012, and asked three questions:\n(1) What do you think previous SWIRL attendees accurately predi cted about the future of\nInformation Retrieval (i.e. true positives: what did we get right)?\n(2) What do you think previous SWIRL attendees did not accurately predict about the future\nof Information Retrieval (i.e. false positives: what did we get wrong)?\n(3) What do you think previous SWIRL attendees did not predict ab out the future of Informa-\ntion Retrieval (i.e. false negatives: what did we miss)?\nFrom these questions, common themes were aggregated. In Tab le 2, we observed several\ninteresting trends. While there was strong agreement about t he second SWIRL missing Neural IR,\nbut recognizing that Conversational IR would be important, perhaps the most interesting trend is\nthedisagreement. Forexample,10participantsthoughttha twewereontargetwiththepredictions\naboutMobile,while11othersbelievedwegotitwrong. Nevert heless,severalimportantnewtrends\nwere identiﬁed in this exercise, including the increasing i mportance of machine learning in IR,\nsearch bias & opinion engineering (broadly speaking – Fairn ess, Accountability, Conﬁdentiality,\nand Transparency) Other topics of interest mentioned as imp ortant future directions included\nmedical IR, monetization, video, green computing, eﬃciency (generally), session level search,\nArXiV, reproducibility, cross device search, explainability o f algorithms, and responsible IR.\nconversational search (15) responsible IR (5)\nevaluation (13) task-based IR (4)\nIR and AI (12) interpretability/decision support (4)\nreproducibility (7) virtual/augmented reality (3)\nSIGIR organization (7) user understanding (3)\nnew applications (6)\nTable 1: Important topics suggested during the initial RSVP p rocess.\nACM SIGIR Forum 36 Vol. 52 No. 1 June 2018\n\nQuestion 1 Question 2 Question 3\nConversational IR (13) Mobile (11) Neural IR (24)\nStructured (13) Search as Learning (7) ML Domination (11)\nMobile (10) Zero Query Search (6) Search bias (8)\nEmpowering Users (10) Evaluation (6) Online/User-Centered Evaluation ( 6)\nZero Query Search (7) Simulated Interaction (5) Opinion Engineering/F ake News (6)\nBeyond Ranked Retrieval (6) Axiometrics (5) Virtual Assistants/Devices (4)\nSearch as Learning (6) Personalization (5) Social Media Search (4)\nOpinion Engineering (6) Whole Page Optimization (4)\nTable2: ThemostcommonresponsesfromSWIRLparticipantson theretrospectivequestionnaire.\nValues shown in parenthesis are the total number of participan ts mentioning that item.\n1.3.2 Important Papers Since SWIRL 2012\nAs part of the homework assignment, we asked participants to se lect one paper from within\ntheir area of expertise and one paper from outside of their ar ea of expertise that they considered\nimportant for the information retrieval community. We manu ally classiﬁed all papers in order to\nunderstand the participants’ perspective on recent resear ch. The complete set of papers in these\ncategories can be found at the end of this manuscript.\nTable 3 shows the number of papers suggested by category in th e homework responses. As\nexpected based on the RSVP data, many participants selected p apers from the machine learning\ncommunity when asked about papers outside of the Core IR comm unity. Deep learning was\nrecognized as a fundamental tool that had powered signiﬁcan t advances in other ﬁelds. Sixteen\nparticipants cited Mikolov’s word2vec paper as an importan t recent contribution [MSC+13].\nAnother important theme was stateful search, deﬁned to inclu de conversational search and\nothermulti-turninformationaccess. Tenparticipantssele ctedRadlinskiandCraswell’stheoretical\nmodel for conversational search [RC17].\nThe social implications of information access systems are b eginning to get increased attention\nmorebroadlyintheacademiccommunity[BS16]. Commonthemes inthisareaincludealgorithmic\nbias, ethics, and transparency. While there were no papers re commended by multiple participants,\nthe subﬁeld of Responsible Information Retrieval is growing.\nThereweretwothemesrelatedtoevaluation: experimentati onandoﬀ-policyevaluation. While\nseveral of the participants selected Tetsuya Sakai’s meta- analysis of previously-published results\n[Sak16a], many participants recognized issues with replica ting and reproducing results, perhaps\ninspired by recent reproducibility concerns in psychology [Ope15]. The second theme concerned\nreuse of production log data for evaluating new treatments (e .g. algorithms, parameters). This\nproblem occurs often when evaluating and training retrieval models in industry and has been\nreceiving attention in the machine learning community. Sev eral participants cited the work of\nThorstenJoachimsandhisstudentsasrepresentativeofthisa rea[JSS17]. Theissueofquantifying\nthe eﬀect of unjudged documents was also a common theme, with Rank-Biased Precision [MZ08]\nand its successor INST [MBST17] being examples of weighted-p recision metrics in which this\nability was speciﬁcally explored.\nFinally, an eﬃciency theme also emerged. The most commonly ref erenced themes to watch in\nACM SIGIR Forum 37 Vol. 52 No. 1 June 2018\n\nDeep Learning (16) Stateful Search (10)\nResponsible IR (9) Experimentation (7)\nEﬃciency (6) Counterfactual/Oﬀ-policy Evaluation (5)\nCognitive Eﬀects (5) Temporal IR (4)\nUser/Topic Variability (3) Social Eﬀects (2)\nRecommendation (2) Rank-Biased Precision (2)\nMarketplaces (2) Brain (2)\nMultiturn IR (1)\nTable 3: Important paper topics since SWIRL 2012.\nthis area were related to improving the eﬃciency in learning s tages of multi-stage retrieval systems\n[LNO+15], explicitly learning trade-oﬀ costs [CGBC17], and the exc iting new area of combining\nlearning and indexing [BNMN16, KBC+17].\n1.4 Summary of Seed Talks\nBased on the RSVP questionnaires and the homework, six “ﬁre st arter” talks were proposed, with\nthe goal of capturing the interests of the participants, and to beprovocative . The seeded talk top-\nics were stateful search (conversation, exploratory searc h, task-based), reproducibility (collection\ndesign, experimentation), evaluation metrics (online and oﬄine measures), fairness/transparency\n(algorithmic bias), user issues (cognitive biases), and sy stem performance/indexing (scalability,\nmachine learning algorithms, ranking). A brief summary of ea ch talk is provided here for future\nreference.\nStateful search. This talk focused primarily on the challenges with task-bas ed search. The key\nproblems identiﬁed were task extraction / representation; task-based evaluation; design consider-\nations in task-based retrieval systems, and task-driven pe rsonalization. One of the key arguments\nwas that search will be a many-device problem, and so better t ask abstractions are needed. Other\nthoughts on the increasing importance of conversational IR were presented and discussed.\nReproducibility. This talk explored the dilemma in IR on the problem of reprodu cibility.\nEveryone believes it is important, but it is not newresearch. So, an argument was made that we\nneed to understand the link between reproducibility, valid ity, and performance prediction. More\nimportantly, we need a shift in culture, where reproducibil ity studies are part of the research\nprocess, and this work is somehow acknowledged as part of car eer progression. Other important\nquestions were deciding whatshould be reproduced, and having the proper assessment tool s in\nplace to help us know whenwe can consider something has been suﬃciently reproduced.\nEvaluation Metrics. This talk focused on the rift between resources and approach es between\nacademic and industry practitioners. More speciﬁcally, ho w do we bridge the gap between on-\nline and oﬀ-line measurement of search quality? Successin complex systems is an end-to-end\nprocess, but many of our tools look at individual components at small scale. Interactions between\ncomponents and people is often ignored. So the provocative q uestion proposed was: Can we get\nrid of oﬀ-line evaluation all together? Since the future inc ludes mobile, personal assistants, and\nACM SIGIR Forum 38 Vol. 52 No. 1 June 2018\n\n“intelligent” systems, the notions of relevance and meta-r elevance become even more muddled.\nFairness & Transparency. ThistalkfocusedonFACT*(Fairness, Accountability, Conﬁd ential-\nity, Transparency, Ethics, Bias, Explainability, Interpre tability, ...) in IR. This is an area receiving\na great deal of attention in the IR community at the moment. Se veral interesting problems were\nhighlighted, including:\n•IR without bias. How to avoid “unfair” conclusions even if they appear true?\n•IR that ensures conﬁdentiality. How to produce results without revealing secrets?\n•IR without guesswork. How to produce results with a guaranteed level of accuracy?\nWould that help or harm? When and why?\n•IR that provides transparency. How to clarify results such that they become trustwor-\nthy?\nLong term problems in machine-altered reality, when questi ons should / should not be an-\nswered, and autonomous algorithmic intervention were desc ribed, in addition to shorter term\nresearch problems of documenting biases / risks in current d atasets/tools, end-to-end analyses of\nbias, and explainable IR systems to help people make better d ecisions. This is clearly a multi-\ndisciplinary problem aﬀecting many research communities.\nUser Issues. This talk argued that we should be talking about people, and no t users, who are\nnot just actors who will stop doing what they are doing to enga ge in an IR system. The key point\nis that in the emerging technological and social-technical environment, people will be constantly\nand ubiquitously emerged in a sea of information. As such, seve ral diﬀerent future “users” were\ndescribed. They were:\n•Ubiquitous Users who are immersed in a sea of information from t he Internet of Things;\n•Thinking Users, where cognitive and neurophysiological con ditions aﬀect interactions with\ninformation;\n•Working Users, where search is a complex combination of a mult iplicity of tasks; and\n•Social Users, which encompass how systems can be designed to r espond to a persons social\nenvironment, in terms of supporting their interactions not only with information, but also\nwith others.\nThe key overall argument was that information interaction s hould be the focus, and not the\nsystems themselves.\nEﬃciency. This talk argued that the value of eﬃciency continues to be an i mportant research\narea in IR. A total of three challenges were presented. The ﬁr st challenge was at the systems\nlevel – How do we explore the trade-oﬀs between eﬃciency and eﬀe ctiveness as systems become\nincreasingly more complex? The second challenge focused on eﬃcient learning and NLP – How\ndo we scale complex neural networking models, and ﬁnd a balan ce between quality and cost in\nthe NLP models being used in IR? The third challenge was around m ultimodal indexing – As we\nmove beyond text, how do we eﬃciently combine, index, and searc h many diﬀerent data formats?\nACM SIGIR Forum 39 Vol. 52 No. 1 June 2018\n\n1.5 Summary of Brainstorming Breakout Sessions\nSix breakout groups discussed themes from the seed talks as w ell as any other topics that partic-\nipants felt was not covered in those talks. After aggregation , the following themes emerged,\n•Decision Support over Pathways : Understanding and designing systems to help people\nin making decisions.\n•Generating New Information Objects :Ad hocgeneration, composition, and summa-\nrization of new text, and layouts in response to an informati on need.\n•Transparent/Explainable Information Retrieval : Explaining ranking decisions. Pro-\nviding reliable and responsible information access.\n•Cognitive-aware IR : Tracking and modeling user behavior and perception. Model ing\npolitical-correctness of decisions. Identifying fake new s and provenance.\n•Societal impact of information retrieval : Understanding the long term impact of IR\non society and the economy.\n•Personal information access : Federated personal information search and management\n(e.g. knowledge graphs). Biometrics for aﬀective state.\n•Next Generation Eﬃciency-Eﬀectiveness Issues : Eﬃcient machine learning inference.\nResource-constrained search.\n•Machine Learning and Search : Developing eﬀective machine-learned retrieval models\n(e.g. neural networks, reinforcement learning, meta-opti mization).\n•Personalized interaction : Diversiﬁed and personalized interactions.\n•Conversational information access : Information-seeking conversations. Learning repre-\nsentations for conversations.\n•New approaches to evaluation : Moving beyond the Cranﬁeld paradigm, topical rele-\nvance, and queries. Controlling for variability. Counterf actual evaluation and oﬀ-policy\nevaluation.\n•New interaction modes with information, multi-device search : Multi-device search.\n•Blending online and physical : Search in the context of mobile, smart environments, and\naugmented/virtual reality.\n•Task-speciﬁc representation learning : Adapting machine learned models for new search\ndomains.\n•Pertinent Context : Surfacing and using the relevant contextual information fo r search.\n•Success prediction : Formal models and principles to inform retrieval system de sign (build\nthe right bridge instead of build six bridges and see which su rvives).\nACM SIGIR Forum 40 Vol. 52 No. 1 June 2018\n\n1.6 Summary of Focus Group Breakouts\nA straw poll was held for participants to identify the three t opics they found most interesting.\nThis resulted in eight topics which formed the ﬁnal breakout focus groups. The focus groups spent\nthe ﬁnal day of the workshop discussing their topic, and deve loping the summary reports found\nin the following sections. The eight themes that emerged wer e:\nSection 2: Conversational Information Seeking\nSection 3: Fairness, Accountability, Conﬁdentiality and Tr ansparency in Information Retrieval\nSection 4: IR for Supporting Knowledge Goals and Decision-M aking\nSection 5: Evaluation\nSection 6: Machine Learning in Information Retrieval (Lear nable IR)\nSection 7: Generated Information Objects\nSection 8: Eﬃciency Challenges\nSection 9: Personal Information Access\nIn addition two minor themes emerged from the workshop, and w ere included in the report\nin Section 10. These were “IR for an IoT World” and “Impact of IR Systems in Society”. The\nremaining sections summarize the ﬁndings for all ten of thes e themes.\nIn the remainder of this report, each of the above themes is det ailed in its own section. Each\nsection follows a standard format with subsections of: desc ription, motivation, proposed research,\nresearch challenges, broader impact, broadening SIGIR, and obstacles and risks.\n2 Conversational Information Seeking\n2.1 Description\nConversational information seeking (CIS) is concerned wit h a task-oriented sequence of exchanges\nbetween one or more users and an information system. This enco mpasses user goals that in-\nclude complex information seeking and exploratory informa tion gathering, including multi-step\ntask completion and recommendation. Moreover, CIS focuses o n dialog settings with variable\ncommunication channels, such as where a screen or keyboard ma y be inconvenient or unavailable.\nBuilding on extensive recent progress in dialog systems, we d istinguish CIS from traditional\nsearch systems as including capabilities such as long term us er state (including tasks that may be\ncontinued or repeated with or without variation), taking in to account user needs beyond topical\nrelevance (how things are presented in addition to what is pr esented), and permitting initiative\nto be taken by either the user or the system at diﬀerent points o f time. As information is pre-\nsented, requested or clariﬁed by either the user or the syste m, the narrow channel assumption\nalso means that CIS must address issues including presentin g information provenance, user trust,\nfederation between structured and unstructured data sources and summarization of potentially\nlong or complex answers in easily consumable units.\nACM SIGIR Forum 41 Vol. 52 No. 1 June 2018\n\n2.2 Motivation\nConversations are a natural form for humans to seek informat ion, and there are decades of study\non formal dialogues and interactions of users with reference l ibrarians. The natural next step is\nto design automated systems that are ‘virtual librarians’, e liciting information needs, correcting\nmisconceptions, and providing the right amount of informat ion at the right time across all possible\ndomains. Multi-turn conversations should also become more natural in the digital environment\ntoday due to the increasing variety of devices that are acces sible anytime/anywhere (perhaps\nwithout screen or keyboard), the maturity of speech interfa ces, and recent developments in general\nrepresentation learning. Today’s digital assistants are o nly capable of very basic “conversations”,\nwhich usually means a single user question (“What’s the weathe r like today?” or “When does my\nﬂight leave tomorrow?”), followed by a single system answer . In contrast, this research direction\nwillleadtomulti-turn,multi-user,multi-taskandmulti- domainconversationalinformationseeking\nsystems.\n2.3 Proposed Research\nDevelopmentofconversationalinformationseekingsystems requiresnewresearchonabroadrange\nof topics related to information elicitation, user modeling, precision-oriented search, exploratory\nsearch, generated information objects (Section 7), descrip tion of retrieval results, session-based\nsearch, dialog systems capable of sustained multi-turn con versations, and evaluation. The IR\ncommunity is well-positioned to work on these issues due to i ts deep roots in studying elicita-\ntion, information seeking, information organization, and what makes search diﬃcult. Meaningful\nprogress is likely to require partnering with colleagues in research areas such NLP, dialog, speech,\nHCI, and information science that have complementary skills , thus broadening and enriching the\nﬁeld. Several promising research directions are described brieﬂy below, to give a sense of what\nthis topic entails.\nUser Models. User modeling in conversational information seeking system s involves inferring,\nrepresenting, and updating information about a person (fro m general information about their\ntastes and conversational style to their current cognitive and emotional state), their state of\nknowledge surrounding the current topic, their current goa l(s), and their previous interactions\nwith the system. The user model informs predictive tasks. Fo r example, based on the user model,\nthe system can decide what information to elicit from the use r, how to elicit the information,\nand what information to provide. We note that elicitation is one key diﬀerence from traditional\nsearch engines, allowing the system to proactively focus on resolving uncertainties in a person’s\ninformation need, both for the system and for the user. It also allows a person to explicitly refer\nto previous conversations with the system as a form of ground ing or disambiguation.\nImportant research questions involve knowing when to take th e initiative; inferring satisfac-\ntion; understanding which attributes of conversational in teractions inﬂuence outcomes related\nto engagement and/or mental workload; and knowing when the i nformation seeking session has\nconcluded.\nFinding Information. Conversational information seeking systems will require di stinct search\nstrategies for diﬀerent conversational states, for example , precision-oriented search when the in-\nformation need is speciﬁc or focused, and diverse recall-ori ented search when the information need\nACM SIGIR Forum 42 Vol. 52 No. 1 June 2018\n\nis uncertain or exploratory. Natural conversational delays create opportunities for anticipatory\nsearch or deeper analysis of search results to prepare for li kely next states in the dialog. After\nthe system gathers information, it must organize, summariz e, and describe what it found. The\ntype of organization and summarization depends upon the use r’s state, the state of the dialog and\nthe mode of communication. For example, it may be organized t o provide a broad overview of\nthe key concepts and to elicit additional information from t he user by supporting drilling down\ninto speciﬁc topics or information sources; or when the focu s is narrow and speciﬁc, it may be an\nabstractive summarization that covers key information sup ported from multiple sources.\nEngagement. In order to make a conversational information seeking syste m engaging to a wide\nvariety of people over a prolonged period of time, the system sho uld exhibit aﬀective traits: it\nshould be able to convey humor, sympathy and other traits in i ts interactions with its users in a\npersonalizedmanner. Atthesametime, theinteractionsnee dtoenablepeopletobuildanaccurate\nmental model of the system’s abilities to avoid causing disa ppointment, for example when having\nrepeated conversational turns that lead to unsatisfactory outcomes. We also note that response\ntime is likely to be a critical component of engagement in CIS systems; they must respond in a\ntolerable time, otherwise people will discontinue use of th e system.\nDomain generality and speciﬁcity. Like traditional web search engines, some conversational\ninformation seeking systems will support conversations acr oss diverse domains – potentially all\ndomains of human knowledge. For example, conversational inf ormation seeking may begin with\nthe request “Tell me about dementia”. Such systems will requ ire development of general-purpose\nmethods of eliciting, describing, and engaging. This type o f generality is not yet possible with\ntask-oriented dialog systems.\nGeneral open-domain systems will lack the depth and domain e xpertise that is possible in\ndomain-speciﬁcconversationalsystems. Adomain-speciﬁc applicationmaydeﬁnefocuseddomain-\nspeciﬁc intents on classes of entities in a specialized knowl edge base curated by experts. This par-\nallels current work in dialog systems focused on domain-spe ciﬁc models of intents with predeﬁned\nschema and slots. We envision that specialized conversatio nal models are needed to perform deep\nconversational tasks, for example a doctor performing more detailed research (“Tell me about the\nrelationship between dementia and thyroid problems.”)\nFailure modes. Given the complexity of multi-turn conversations, failure s will occur that may\ncause the user to end the conversation, for example, inabilit y to refer to an earlier conversational\nturn, failure to ﬁnd information, or failure to understand r etrieved information. A failure may be\ncaused by an individual component or by interactions across components. When failure happens,\nthesystemshouldguideapersontoprovideinformationthat allowsrecovery. Forexample, instead\nof saying, “I can’t help you with that” or falling back to readi ng web results, the system should\nengage the user to recover or avoid the issue.\nA key challenge is that the system should “know what it doesn’ t know” to express gaps\nin understanding of the request or the underlying informatio n. This means that the system\nnecessarily needs to quantify its conﬁdence in the responses generated, in terms of i) whether the\nsystem properly understood the utterance/request, ii) whe ther it was able to retrieve appropriate\ninformation, iii) whether it was able to properly organize a nd aggregate retrieved items into\ngenerated information objects (Section 7), and iv) whether it was able to render results (through\nbest answer selection, summarization, etc.) clearly to the user.\nWhen failure occurs, the user may correct the system, for exam ple by issuing a comment of the\nACM SIGIR Forum 43 Vol. 52 No. 1 June 2018\n\nform “No, what I meant was ...”. Such corrective feedback oﬀer s an opportunity for the system\nto both reinterpret the current dialog state, (realigning t he current information seeking process\ntoward successful conclusion), as well as provide useful tr aining data for improving the system for\nsubsequent interactions.\nEvaluation. Developing successful systems requires further understand ing of what constitutes\na successful information seeking conversation. One starti ng point could be to create collections\nof human-human information seeking conversations in which o ne person plays the role of the\nsystem (with access to one or more information services and/ or domain expertise) and the other\nperson plays the role of the user. So-called Wizard of Oz studi es can be used to gather example\nconversations and use questionnaires to measure outcomes suc h as task completion, workload, and\nperceived usability. By doing so, we may be able to gain insig hts about the correlations between\nspeciﬁc conversation characteristics and diﬀerent outcomes .\nEvaluating a conversational information seeking system req uires component-wise and end-to-\nend evaluation methods. CIS systems involve several compon ents that can be evaluated individ-\nually using specialized evaluation methodologies. First, gi ven the input from the user, the system\nneeds to accurately detect the task the user is trying to perf orm and the state they are in with\nrespect to the task. The system should also be able to detect t he possible next state(s) of the\nuser, or their possible next task(s)/goal(s). Hence, evaluat ion methodologies need to be designed\nin order to evaluate how well the system understands the user’ s task and state, and predicts future\nuser needs. Since the information delivered will be persona lized, we also need methods to evaluate\nthe quality of personalization. Finally, some conversation al responses will involve summarization.\nPrior research developed metrics for evaluating summarizat ion quality; however, summarization\nin the context of CIS systems is likely to require diﬀerent me trics because the response the user\nexpects from such systems is quite diﬀerent than typical doc ument summarization.\nBesides component-wise evaluation methods, it is also crit ical to have an end-to-end evalua-\ntion approach. End-to-end evaluation is necessary to compa re between diﬀerent systems and to\ndetermine whether dialog is the appropriate mode of interact ion (e.g., compared to a more tradi-\ntional mode). In this respect, we need methods and metrics th at can be compared across diﬀerent\nmodes of interaction. These metrics may need to consider out comes such as user engagement,\nsatisfaction, and task completion time.\nMulti-modal conversations. Informed conversations between humans often involve suppl e-\nmentary evidence such as documents, images, or videos. The c urrent multiplicity of devices could\nsupport such multi-modality provided that we can gain a bett er understanding of what are the\nappropriate modes depending on the device and user context ( previous/current activity, location),\nand potential materials available to the user (for example, the query may include an image) and\nto the system (for example, the system response may include a udio: “Do you mean a sound like\nthis?”). Models of turn taking (feedback, granule of inform ation) to drive conversations will need\nto incorporate the possibilities for a variety of devices an d modalities.\nCross-device conversations. When a conversational information seeking system is designed to\nsupport complex exploratory tasks, one needs to take cross- device behavior into account, since the\ninformation seeking session might continue for a long time u nder diﬀerent circumstances. A CIS\nsystem should be able to optimize the query acquisition, cla riﬁcation, and presentation methods\nbased on a device at hand. Supporting users to eﬀectively res ume an ongoing task across diﬀerent\ndevices with multiple modality (e.g., audio, text, multime dia) can be challenging.\nACM SIGIR Forum 44 Vol. 52 No. 1 June 2018\n\nCollaborative information seeking. Conversation/dialog does not necessarily occur between a\nCIS system and a single user. The system should also be able to s upport multiple users or a group\nofuserswhoengageinacollaborativetask. Thisinvolvesus eridentiﬁcationandtrackingduringan\ninformation seeking session, mining information needs, re levance feedback, pertinent contextual\nfactors from collaborative conversations, and personaliz ation / diversiﬁcation of results for the\ngroup or for the individual members of the group. Sensing the s tate of the discussion in the group\ncan also be an important signal to optimize the seeking sessio n.\n2.4 Research Challenges\nConversational IR systems can be seen as a federation of agen ts or subsystems, but they will also\nbe inherently complex systems, with models that will reach b eyond the boundaries of individual\ncomponents. Withthatwillarisechallengessuchashowtoboo tstrapsuchsystemswithreasonable\neﬀort, how to ensure they are responsive as a whole, how to perfo rm component-wise diagnosis,\nand at what level to consider their robustness.\nEthical challenges arising across the ﬁeld of information re trieval, such as trust in information,\nbiases, and transparency, will likely be exacerbated by the inherent narrowing of the communica-\ntion channel between the systems with their users.\n2.5 Broader Impact\nCurrent search engines are widely used in many settings. Con versational IR systems could replace\nor augment these for many tasks, reducing the cognitive burd en on the user and potentially\nsupporting them to achieve success more often or to improve t he eﬃciency or ease of their search.\nEﬀective search agent design will enable a greater level of co ntrol and transparency of search\nprocess and outputs. Conversational IR can provide support to users who are initially unable\nto express their information need suﬃciently well to properl y begin a search task, for example\nby providing feedback after a vague initial search and elici ting more information to progressively\nbuild a meaningful expression of the information need. The a bility of a search system to remember\nall or part of previous search sessions may prevent the user fr om needing to repeat previous search\ntasks or to provide support by reminding about previous searc h activities (e.g., “When you looked\nfor this before, you were interested in these items”).\nProactivity by the search agent could provide people with de tails about their search topics,\nretrieveddocuments, oropinionsexpressedinthedocument s. Proactiveanalysisandreportingcan\nenable broader, less-biased perspectives of a given topic, l eading to improved information literacy\nof end-users. Search currently requires a person to break oﬀ from their current activity or task\nand to engage in a separate activity. Conversational search may be more fully integrated in their\nwork. Forexample, maintainingdetailsofprevioussearchi nputs, resultsobtained, andmonitoring\nof ongoing work to be able to provide context relative search. I n addition, rich modalities in\nconversational search interaction (e.g., speech, sound, t ext, multimedia) can achieve an inclusive\nsystem for a wide range of users and situations including low literacy, disability, in hands-busy\nenvironments.\nACM SIGIR Forum 45 Vol. 52 No. 1 June 2018\n\n2.6 Broadening SIGIR\nDevelopment of successful conversational IR systems will r equire signiﬁcant expertise in eliciting,\nﬁnding, and delivering information, which are core strengt hs of the information retrieval research\ncommunity. It will also require user modeling, dialog syste ms, speech interfaces, and HCI skills\nthat provide opportunities for collaboration with colleag ues in other areas of computer science.\nPeople tackling this research problem will need to work acro ss disciplines.\n2.7 Obstacles and Risks\nThereareseveralobstaclesandriskstoresearchonthistop ic. Reusabledatasetsmaybediﬃcultto\ndesign or acquire due to the personalized, interactive natu re of the task and the detailed temporal\nuser models it develops. Conversational information seeki ng systems may retain information\nabout a person over long periods of time, which raises privac y and legal issues. People could be\nuncomfortable with systems learning and retaining detaile d information about what they know\nand how they acquire information. There is a possibility tha t successful systems might expose\npeople to a broader range of information than they consider n ow; and a risk that more eﬀective\norganization and ﬁltering of information might discourage critical thinking. Finally, the level of\nlanguage understanding required to provide useful assista nce might be too diﬃcult to enable more\nthan shallow systems during the next 5-10 years.\n3 FACT IR: Fairness, Accountability, Conﬁdentiality and\nTransparency in Information Retrieval\n3.1 Description\nIR is about connecting people to information. However, as wit h all software-based systems, IR\nsystems are not free of human inﬂuence; they embed the biases of those that create, maintain\nand use them. Empirical evidence suggests that certain comm unities have diﬀerential access to\ninformation; inotherwords, theirneedsmightnotbeequallywel lsupportedorcertaininformation\ntypes or sources might be more or less retrievable or might no t be well represented. In addition, as\nwe increasingly rely on the outcome of IR systems such as sear ch engines, recommender systems,\nand conversational agents for our decision making, there is a growing demand for these systems to\nbe explainable. Such problems are related to many fundamenta l aspects of information retrieval,\nincluding information representation, information or ans wer reliability, information retrievability\nand access, evaluation, and others. While, traditionally, t he IR community has been focused on\nbuilding systems that support a variety of applications and needs; it is becoming imperative that\nwe focus as much on the human,social, andeconomic impact of these systems as we do on the\nunderlying algorithms and systems.\nWe argue that an IR system should be fair(e.g., a system should avoid discriminating across\npeople),accountable (e.g, a system should be reliable and be able to justify the ac tions it takes),\nconﬁdential (e.g., a system should not reveal secrets), and transparent (e.g., a system should\nbe able to explain why results are returned). Judgment is need ed sometimes to balance these\nfour considerations (e.g., it is responsible to bias agains t unreliable sources). Other communities,\nACM SIGIR Forum 46 Vol. 52 No. 1 June 2018\n\nsuch as the machine learning, artiﬁcial intelligence, and co mputational social science, are already\nfocusing on these and other related issues, including how hu man behavior online is confounded by\nalgorithmic systems, how we can audit black box models, and ho w can we maximize beneﬁt and\nreduce risks. The research directions we describe here aim t o increase these eﬀorts as they apply\nto IR systems.\n3.2 Motivation\nWhy does it matter for IR? IR systems often capture associations between entities and/ or\nproperties, and depending on the semantic connotations of su ch relationships they might lead to\nreinforcing current stereotypes about various groups of pe ople, propagating and amplifying harm.\nFor example, these associations may originate from the data used to train the ranking models,\nwhich may not provide enough coverage for all possible assoc iations such that they can all be\nlearned. Certain groups of individuals may be over- or under -represented in the data, which could\nbe a reﬂection of greater societal disparities (e.g., unequa l access to health care can result in\nunequal representation in health records) or of the types of people who are able to contribute\ncontent, including the rate at which these contributions are made (e.g., women tend to be over-\nrepresented in Instagram data, but under-represented in St ackOverﬂow data). Representation is\nalso aﬀected by the quality of the tools used to capture the da ta. For example, it is more diﬃcult\nto do facial recognition of dark-skinned people in video sur veillance footage because of limitations\nwith how cameras are calibrated. As a result, an image retriev al system might fail to properly\nidentify images related to darker-skinned people, while an image assessment system might ﬂag\nthem more often for security interviews, or to scrutinize th em in more detail.\nWhat makes this speciﬁc to IR? Given the ubiquitous usage of IR systems, often broadly\nconstrued(e.g., search, recommendation, conversational agents), theirimpact—negativeincluded\n— is potentially wide ranging. For instance, research has sh own that people trust more sources\nranked higher in the search results, but the ranking criteri a may rather rely on signals indicative\nof user satisfaction, than on those indicative of factual in formation. For consequential searching\ntasks, such as medical, educational, or ﬁnancial, this may rai se concerns about the trade-oﬀs\nbetween satisfying users and providing reliable informati on.\nThe SIGIR community has the responsibility to address fairne ss, accountability, conﬁdentiality\nand transparency in all aspects of research and in the system s built in industry. Similar respon-\nsibility issues are addressed in related ﬁelds, however, th ere are speciﬁc issues in IR stemming\nfrom the characteristics of, and reliance on document colle ctions and the often imprecise nature\nof search and recommendation tasks. IR has a strong history o f using test collections during eval-\nuation. As evaluation tools, test collections also have cert ain types of bias built-in. For example,\nthe people who construct topics and make relevance assessme nts arguably are not representative\nof the larger population. In some cases, they have not been re presentative of the type of users who\nare being modeled (e.g., having people who do not read blogs ev aluate blogs). Evaluation mea-\nsures are designed to optimize certain performance criteri a and not others, and either implicitly\nor explicitly have built-in user models. Systems are then te sted and tuned within this evaluation\nframework, further reinforcing and rectifying any existin g biases. For example, in building test\ncollections, bias should be avoided by ensuring diversity in the sources of documents included and\nusing people from diverse backgrounds to create topics.\nACM SIGIR Forum 47 Vol. 52 No. 1 June 2018\n\nWhat are examples of human, social, and economic impact? Infrastructure and accessi-\nbility variations may introduce diﬀerential representati on in training data. For example, research\nhas shown that social media accounts with non-Western names are more likely to be ﬂagged as\nfraudulent, and argue that this is because the classiﬁers ha ve been trained on Western names. Bias\ncan also be introduced by the interfaces and tools that are pr esented to users. For example, query\nautocompletion is a common feature of search systems that le arn suggestions from past behaviors\nof users; however, often the people who type queries about par ticular topics are from a speciﬁc\nsegment of the population and the intent behind their querie s is often unclear. For example, the\nquery preﬁx “transgenders are ...” results in oﬀensive autoc omplete suggestions of “transgenders\nare freaks” and “transgenders are sick”.\nAnother motivation for this work is the growing concern about the understandability, explain-\nability and reliability of deep learning methods including t he complexity of the parameter space.\nThese techniques are being used in a variety of domains to ass ist with a range of high-impact\ntasks such as in the medical domain for diagnosis and the inte lligence community for detecting\nthreats and combating terrorism. Many of the domain experts working in these ﬁelds are not\nsatisﬁed with a simple answer, but rather desire to know about the reasoning and evidence behind\nthe answer that the system produces because the decisions they are making can have signiﬁcant\nconsequences. Moreover, the engineers who create systems o ften do not understand which parts\nof the system are responsible for failures, and it can be diﬃc ult to trace the origins of errors in\nsuch complex parameter spaces. However, it is unclear how suc h explanations, evidence-trails and\nprovenance might be communicated to the various user groups and how such communications\nmight change behaviors, and the quality, quantity, and natu re of human-computer interaction.\nWe, the IR community, should take the initiative before othe rs do in the face of changing legal\nframeworks. For example, in Europe with the General Data Pro tection Regulation, individuals\nhave a right to erasure of personal information and a right of explanation. IR systems need to\nincorporate these rights. Thus, an indexing scheme needs to be able to delete information and\nsearch results may require explanation.\n3.3 Proposed Research\nWe propose an agenda driven by the ideal of incorporating soc ial and ethical values into core in-\nformation retrieval research and development of algorithm s, systems, and interfaces. This necessi-\ntates a community eﬀort and a multi-disciplinary approach. We focus on fairness, accountability,\nconﬁdentiality, and transparency in IR:\n•Fair IR\n–How to avoid “unfair” conclusions even if they appear true?\n–For instance, in the case of people search, how do we make sure that results do not\nsuﬀer from being underrepresented in the training data?\n–Avoid “discrimination” even when attributes such as gender, n ationality or age are\nremoved. And even when the vox populi dictates a certain ranki ng. Avoid selection\nbias and ensure diversity.\n–To what extent is the assortment of information objects prese nted to us representative\nof all such objects ‘out there’?\nACM SIGIR Forum 48 Vol. 52 No. 1 June 2018\n\n–How can we measure and quantify fairness of an IR system?\n–Evaluation of fairness vs. fair evaluation. How can we measure ‘harm’, and variations\nin ‘harm’ across verticals?\n•Accountable IR\n–How can we avoid guesswork and produce answers and search resu lts with a guaranteed\nlevel of accuracy?\n–Would providing such a guaranteed level of accuracy help or ha rm? When and why?\n–Attach meaningful conﬁdence levels to results. Handling ver acity issues in data. When\nto roll out hot-ﬁxes? Rankings with solid guarantees on the rel iability of the displayed\nanswers and results.\n–Howmighttheassortmentofinformationobjectspresentedto usimpactourperceptions\nof reality and of ourselves?\n•Conﬁdential IR\n–How to produce results without revealing secrets?\n–Personalization without unintended leakage of information ( e.g., ﬁlter bubbles) by ran-\ndomization, aggregation, avoiding overﬁtting, etc.\n•Transparent IR\n–How to clarify results such that they become trustworthy?\n–Automatically explaining decisions made by the system (e.g. retrieved search results,\nanswers, etc.) allowing users to understand “Why am I seeing t his?”\n–Traceability of results (e.g., link to raw data underlying e ntity panels).\n3.4 Research Challenges\nThesegeneralresearchquestionsmanifestthemselvesalon gtheentireinformationretrieval“stack”\nand motivate a broad range of concrete research directions t o be investigated:\nDoes the desire to present fair answers to users necessitate diﬀerent content acquisition meth-\nods? If traceability is essential, how can we make sure that ba sic normalization steps — such\nas content ﬁltering, named entity normalization, etc. — do n ot obfuscate this? How can we\ngive assurances in terms of fairness towards novel retrieva l paradigms (e.g., neural retrieval mod-\nels being trained and evaluated on historic relevance label s obtained from pooling mainly exact\nterm-matching systems)?\nHow should we design an information retrieval system’s loggi ng and experimental environment\nin a way that guarantees fair, conﬁdential, and accurate oﬄi ne and online evaluation and learning?\nCanexploration policiesbedesignedsuch thattheycomplyw ithguaranteesonperformance? How\nare system changes learned online made explainable?\nIndexing structures and practices need to be designed/revisited in t erms of their ability to ac-\ncommodate downstream fairness and transparency operations . This may pose novel requirements\nACM SIGIR Forum 49 Vol. 52 No. 1 June 2018\n\ntowards compression and sharding schemes as fair retrieval systems begin requesting diﬀerent\naggregate statistics that go beyond what is currently requi red for ranking purposes.\nInterface design is faced with the challenge of presenting the newly generated types of infor-\nmation (such as provenance, explanations or audit material ) in a useful manner while retaining\neﬀectiveness towards their original purpose.\nRetrieval models are becoming more complex (e.g., deep neural networks for IR) and will\nrequire more sophisticated mechanisms for explainability and traceability. Models, especially in\nconversational interaction contexts, will need to be “inte rrogable”, i.e., make eﬀective use of users’\nqueries about explainability (e.g., “why is this search resu lt returned?”).\nRecommender systems haveahistoricdemandforexplainabilitygearedtowardsbo ostingadop-\ntion and conversion rates of recommendations. In addition to these primarily economic considera-\ntions, transparent and accountable recommender systems ne ed to advance further and ensure fair\nand auditable recommendations that are robust to changes in product portfolio or user context.\nSuch interventions may take a considerably diﬀerent shape t han those designed for explaining the\nresults of ranked retrieval systems.\nUser models will facethenovel challengesofpersonalizingretrieval s ervicesinafair, explainable,\nand transparent manner. This is particularly relevant in the context of diversity and the way in\nwhich biased or heavily polarizing topics and information so urces are handled. Additionally,\ntransparent retrieval systems will require new personaliza tion techniques that determine the right\nlevel of explanation that ﬁts diﬀerent sets of requirements (e.g., explanations that are eﬀective for\nnovice searchers, professional journalists or policy make rs vs. explanations for highly technology-\naﬃne search engineers investigating system failures). Fina lly, such personalization should be\nreliable in terms of robustness to confounding external con text changes.\nEﬃciency will be a key challenge in serving explanations at real time. Structures and models will\nneed to accommodate for on-demand calculation as well as cac hing or approximate explanations\nin order to meet run time and latency goals. In addition, a key challenge will be the design of\nindexing structures and models that are fair without compro mising eﬃciency.\nThere may surface a need for new evaluation metrics that capture the quality of an expla-\nnation and that understand user satisfaction as a composite of immediate goal accomplishment as\nwell as fairness, trustworthiness and transparency consid erations. Depending on the concrete ap-\nplication, there are diﬀerent trade-oﬀs between the severi ty of diﬀerent error types (e.g., misses vs.\nfalse alarms). Such investigations of failure consequence s can be conducted at diﬀerent temporal\nresolutions, ranging from immediate short-term eﬀects on si ngle users to long-term consequences\nat a population-wide resolution. Finally, this line of evalu ation motivates a stronger separation\nbetween organic traces of user behavior and users’ reaction s to platform and algorithm properties.\nThere are numerous challenges that the desire to realize fai r, accountable, conﬁdential and\ntransparentIRsystemsposesthatcutacrosstheIRstack. Th isincludes, forinstance, maintaining\nconﬁdentiality of information when providing explanation s involves indexing structures, models,\nand evaluation.\nACM SIGIR Forum 50 Vol. 52 No. 1 June 2018\n\n3.5 Broader Impact\nAs information retrieval systems (search engines, recommend er systems, conversational agents)\ntouch on every aspect of our life, the technology that we help develop should be informed by,\nand inform, the world around us. This starts with understand ing and replying to immediate\nstakeholders — users, decision makers, and engineers — acro ss applications such as web search,\ninformation discovery in social networks, HR analytics, med ical search, and e-commerce.\nBut there are broader concerns. Studies suggest that online platforms have impacted soci-\nety by leading to increasing polarization; changing the met rics can help to begin to assess and\nunderstand such issues. The IR ﬁeld needs to recruit more div erse people, and not just as collabo-\nrators, but also students. These changes might help IR resear chers better understand the methods\n(and communicate about them), which in turn could lead to add itional insights and theoretical\ndevelopments.\nFinally, a stronger emphasis on transparency will likely forc e us to document our methods and\nexperiments in a better and more systematic ways. This, in tu rn, will positively impact teaching\nand reproducibility in IR.\n3.6 Broadening SIGIR\nMany questions pertaining to responsible and accountable te chnology originate in other scientiﬁc\ncommunities. Often, they are social, ethical, or legal in na ture rather than purely technical. We\nneed technical skills to solve them but we should collaborat e with social scientists, psychologists,\neconomists and lawyers, e.g., to understand the impact of usi ng FACT IR systems in society, to\nbe exposed to suitable ethical frameworks, and to anchor the d eﬁnitions of the core concepts in\nFACT IR, such as what is an explanation in scientiﬁc discours es that have considered such notions\nfor decades.\n3.7 Obstacles and Risks\nTo enable this research we need broad collaborations betwee n IR researchers and communities\noutside IR. Finding eﬀective ways of collaborating and ﬁndin g a shared language requires con-\nsiderable eﬀort and investment that may not be properly “rew arded” by funding bodies and\nevaluation committees.\nAn important riskconcerns the diversity of perspectives on the deﬁnition of core concepts\nsuch as fairness, ethics, explanation or bias across scient iﬁc and engineering disciplines, gov-\nernments or regulating bodies. Having more transparent IR sy stems could make systems more\nvulnerable for adversaries as knowledge about the internal s of systems need to be shared through\nexplanations.\nA potential obstacle is initial resistance from system deve lopers and engineers, who might have\nto change their workﬂows in order for systems to be more transpa rent. Another possible obstacle\nis the tension between transparency and fairness, and an ent erprise’s commercial goals.\nAn inadvertent risk is introducing a new type of bias into our s ystems about which we are\nunaware.\nACM SIGIR Forum 51 Vol. 52 No. 1 June 2018\n\n4 IR for Supporting Knowledge Goals andDecision-Making\n4.1 Description\nIR systems should support complex, evolving, or long-term i nformation seeking goals such as\nacquiring broad knowledge either for its own sake or to make a n informed decision. Such support\nwill require understanding what information is needed to acc omplish the goal, scaﬀolding search\nsessions toward the goal, providing broader context as info rmation is gathered, identifying and\nﬂagging misleading or confusing information, and compensat ing for bias in both information and\nusers. It requires advances in algorithms, interfaces, and evaluation methods that support these\ngoals. It will be most successful if it incorporates growing understanding of cognitive processes:\nhow do people conceptualize their information need, how can contrasting information be most\neﬀectively portrayed, how do people react to information th at ﬂies in the face of their own biases,\nand so on.\n4.2 Motivation\nPeople seek to satisfy various information needs that invol ve acquiring knowledge and/or making\ndecisions, such as learning about world aﬀairs from reading news articles, understanding their\nmedical problems and possible treatments, or training for a job. Invariably, retrieval systems fall\nshort of the best possible outcome, or even user expectation s. The user may have had to expend\nmore eﬀort than ideally needed, or ended up with information that is inaccurate, biased, or lacking\nutility.\nIn order to successfully accomplish such knowledge-seekin g and decision-making tasks, users\noftenneedmoresupportthanthatcurrentlyoﬀeredbyinforma tionsystems. Thissupportneedsto\nbeoﬀeredatdiﬀerentstagesintheinformationseekingproc ess,startingevenbeforeaninformation\nneed is expressed: a search system should be aware of the cont ext of the user in which the\ninformation need is to be placed and of the user’s existing sk ills and knowledge. If a more\ncomplex task is to be accomplished (such as gathering diﬀere nt forms of evidence for a decision\ninvolving multiple constraints or aspects), the system may help by scaﬀolding the task at every\nstep, as needed by the user. The system needs to be aware of bia ses of the user and/or the search\nresults and take those into account when presenting these re sults to end up with the best possible\noutcome. Similarly, the user should be made more aware of the b roader context in which the\nreturned information exists. Ideally, a system should also be aware of and be able to competently\ndeal with distractions or lack of motivation of the user.\nWhile these demands on a retrieval system in a sense have alway s existed, it is more pertinent\nthan ever that these are incorporated in the information ret rieval process. Technology is much\nbetter suited now to help fulﬁll these requirements on the on e hand, and on the other, there\nis greater scope for the user to end up more misinformed after a search than before. To give\nan example, search systems (and related algorithms, such as r anking algorithms employed by\nsocial media systems) contributed to large amount of misinfo rmation during the 2016 presidential\nelection cycle in US politics.\nFinally, as learning is supplemented more and more with onlin e technology, improved methods\nfor getting students the right information for their learni ng goals could help increase student\nengagement, curiosity, and retention, as well as, in the long er-term, enable better knowledge\nACM SIGIR Forum 52 Vol. 52 No. 1 June 2018\n\ntransfer to other courses. As we describe in ‘Broadening SIGI R’ below, advances in this research\narea could also lead to advances in psychology and learning s ciences as systems are used to give\ninsights into, e.g., the relative ﬁdelity of cognitive mode ls in predicting outcomes or enhancing\nlearning.\nDeveloping solutions in these areas also includes signiﬁca nt challenges in evaluation. Current\nevaluation metrics and methods do not adequately capture not ions of users’ satisfaction, conﬁ-\ndence, and trust, or the quality of the outcomes or decisions made based on the search process.\nUsers may be trying to ﬁll diﬀerent types of knowledge gaps and t heir goals may evolve. Meth-\nods are needed to evaluate whether they ﬁll these gaps or not an d whether they ﬁll them with\ncorrect, adequate, and contextualized information. Evalu ation needs to consider these complex,\nlonger-term (and possibly on-going) aspects of the users’ i nformation goals.\n4.3 Proposed Research\nThe proposed research consists of several main threads: (1) u nderstanding cognitive aspects of\nusers that are relevant to their information seeking, (2) in vestigating ways that search systems\ncan provide information (beyond ranked lists and underlying documents) that will aid searchers in\nevaluatingandcontextualizingsearchresults,(3)explori ngwaysthatsearchsystemscanhelpusers\nmove through a learning or decision-making process, and (4) o vercoming challenges in evaluating\nhow well systems support users in learning and decision making .\nThe ﬁrst research area is concerned with understanding cogn itive aspects of users that may\ninﬂuencetheirinteractionswithinformationreturnedfro msearchsystems. Forexample,usersmay\nhave pre-existing knowledge and biases, diﬀering levels of curiosity and trust, or even diﬀerent\nlearning strengths. Research needs to explore how search sy stems can detect, represent, and\nproductively utilize cognitive aspects of users to help sup port learning and decision support during\nsearch processes. Speciﬁc research questions include: (a) How do cognitive models and processes\naﬀect searching and vice-versa? What cognitive biases make co ntent more diﬃcult to absorb?\n(b) How do people assess content (e.g., Is this information tr ue/factual versus opinion/biased?\nHow does this information relate to other content I’ve seen bef ore?), (c) How do we detect and\nrepresent users’ knowledge and knowledge states, cognitiv e processes, and the eﬀort and diﬃculty\nof processing information?, and (d) How do we represent diﬀere nt information facets for users to\nsupport meta-cognition?\nThe second area focuses on investigating ways that search sy stems can represent and provide\ninformation so as to aid searchers in evaluating and context ualizing search results. Research\nquestions in this area include: (a) what information or sour ces of information can be provided to\nhelp users overcome their cognitive biases (e.g. teenage mom s might trust other teenage moms);\n(b) what visualizations or presentations are useful to conv ey relationships between known and new\ninformation? (c) what interface choices leverage a user’s c ognitive biases in order to lead them\nto better learning or more informed decisions; and (d) what t ypes of metadata can be presented\n(and how can it be presented) to help a user understand the bia ses, trustworthiness, provenance,\nor utility of information?\nThe third area focuses on exploring ways that search systems can provide more explicit inter-\naction/interface support to users who are searching in orde r to help make a decision or to engage\nin ongoing learning about a topic. Research questions in thi s area include (a) How can we help\nACM SIGIR Forum 53 Vol. 52 No. 1 June 2018\n\nusers assess and contextualize information returned by sea rch systems (e.g., quality, trustworthi-\nness, opinion vs. fact, position of the information in the do main space), (b) How do we go beyond\ntopical clustering to uncover structure relevant to users’ knowledge goals (e.g., alternative views),\n(c) How can systems encourage meta-cognition and reﬂection, thus providing scaﬀolding and men-\ntoring toward their goals?, (d) how well does IR technology s upport decision making (comparing\nitems, understanding dimensions, testing hypotheses).\nThe fourth area focuses on developing new evaluation models s uited for evolving, complex,\nand longer term information seeking. Research questions in this area include (a) understanding\nhow well algorithms and systems support users in such tasks? (b) how do we measure the impact\nof diﬀering cognitive processes on information seeking? (c) how do we measure success for long-\nterm tasks where satisfaction may be ephemeral or may change i n light of information acquired\nlater? (d) how do we measure the quality of the ultimate decisi on, the user’s satisfaction with the\ndecision or the process, or the depth of the user’s learning d uring or after looking for information.\nPrioritization/Progression: Near-term work in this area co uld focus on understanding and sup-\nporting speciﬁc types of learning and decision-making task s. For example, work could investigate\n(a) how people assess whether information is true or not, (b) how interfaces can provide scaﬀold-\ning to guide a search, and (c) how to convey where information i s situated in a space or along a\nparticular set of dimensions. Longer-term work should cons ider (a) broader goals to understand\nhow to represent knowledge, biases, and cognitive processe s in users, (b) how documents, rankings\nand interactions operate as functions that change users’ kno wledge states and beliefs, and (c) how\nusers could use search systems to formulate hypotheses and u nderstand options.\nFinally, this proposed research connects with multiple SWIRL themes, including evaluation,\nfairness and accountability, and past themes like search as learning (2012). In particular, while\nthe latter focused on developing users’ search skills using a variety of tools and interfaces, we look\nat broader support of knowledge goals and incorporate cogni tive aspects, including bias, as part\nof automatically improving retrieval processes and outcom es.\n4.4 Research Challenges\nChallengesarefacedoneachoftheareasthattheproposedre searchcovers. Theproposedresearch\ntouches on the collection over which the search engine operat es, the user’s interaction with the\nsearch system, the user’s cognitive processes, and the eval uation of the changes to the user’s\nknowledge state or performance on tasks.\nAt the level of the collection, we are concerned with the mix of information that is available.\nForlarge scalecollections, such astheweb, itisverydiﬃcul ttounderstand theamountofmaterial\non a given topic, and thus is it hard to know what the existing bia ses are in the collection. For\nexample, we mightbe interested in measuring theaccuracy of decisionsthat usersmake after using\na search engine. Collections of interest will contain a mix o f correct and incorrect information, but\nthe scale of the collection will make it diﬃcult to understand t he amount of correct and incorrect\ninformation in the collection apriori to the user’s search s ession.\nThe ﬁeld of IR is still in its infancy with respect to understa nding user interaction and user\ncognitive processes. For us to be able to design systems that lead users to their desired knowledge\nstate or decision, we will need to better understand how thei r cognitive processes aﬀect their inter-\naction with the system and how the stream of information that they consume changes their mental\nACM SIGIR Forum 54 Vol. 52 No. 1 June 2018\n\nstate. A challenge here will be a lack of expertise in cogniti ve science and psychology (how people\nlearn, how people make decisions, biases). Progress in this area will likely require collaboration\noutside of IR and require input from and engagement of other c ommunities, including: cognitive\nscience, human-computerinteraction, psychology, behavio uraleconomics, andapplication/domain\nspeciﬁc communities (e.g., intelligence community, clini cal community). The envisioned systems\nmay require radical changes to aspects of user interfaces. Up take of new UI solutions, however, is\noften diﬃcult and poses extra onus on users, thus creating a h igh barrier to entry for the proposed\nnew systems.\nFinally, evaluation ranges from the simple to the complex. We are interested both in sim-\nple measures such as decision accuracy, and complex measures such as increases in curiosity.\nEvaluation is envisioned to embrace larger aspects of the user- system interaction than just the\ninformation seeking phase, e.g., evaluation of decisions u sers take given the information systems\nprovided. Given that almost all evaluation will be with resp ect to changes in the user, evaluation\nwill be as costly in time and eﬀort as all user studies and huma n research is. Evaluation may\nalso be hindered by diﬃculties in evaluating aspects such as learning, or the unavailability of a\nnormative reference to evaluate decisions. Indeed, there ar e many circumstances in which the\n“right decision” or the “right knowledge” depends on person al circumstances, or cultural/societal\nframeworks.\n4.5 Broader Impact\nThe proposed research will make users better informed and mo re aware of information quality\nand its broader context, by providing a broader, more balanced view of the information space and\nmeta-cognition for the further process of information seek ing or decision making. It also connects\na user’s information seeking behavior to a growing understa nding of the cognitive processes that\nunderlie a person’s searching, learning, and decision-maki ng. This should lead to users being more\nconﬁdent and eﬃcient in their learning and decision-making , to improvement in the overarching\ntask of connecting people with the right information, to sup port for complex matching tasks\nsuch as expert-ﬁnding or peer matching, and to enabling peop le to learn more and to learn more\neﬀectively.\n4.6 Broadening SIGIR\nThere is potential for cross-disciplinary collaboration a nd impact with a number of scientiﬁc\nﬁelds, including psychology, economics, learning sciences , and robotics. In fact, some IR advances\ndescribed in this report will require interdisciplinary sol utions that draw from paradigms and\nmethods in multiple areas.\n4.7 Obstacles and Risks\nWith existing search systems, we currently know little about the actual extent to which users are\nhelped or hurt in their ability to reach their desired knowledg e state or make decisions. As we\nattempt to measure and improve performance, we risk making s ystems worse. What if systems\nlead users to the wrong answers or to bad, possible harmful de cisions (e.g., bad health decisions)?\nACM SIGIR Forum 55 Vol. 52 No. 1 June 2018\n\nAnother risk involves systems that deliberately or accident ally over-represent or promote the\nvalues of certain cultural groups/majorities, and discard t he values, opinions and conventions of\nminorities.\nAdversarial aspects are a serious risk: in principle, system s using the proposed technology\ncould deliberately introduce biased, incomplete, or fraudu lent information. Moreover, people who\nknow the algorithms used in these systems could potentially design their material to work around\nthe safeguards and thus spam the users that the systems are de signed to support. To minimize\nthese risks, our evaluation methods will need to be designed to cover both oﬄine and online\nevaluation that includes adversarial scenarios.\n5 Evaluation\nWe describe three elements of evaluation research: online ev aluation, developing methods to pre-\ndict evaluation results, and the ever present challenge of i nteractive evaluation.\n5.1 Research opportunities arising from online evaluation\n5.1.1 Description\nFor more than a decade, online evaluation has proved itself t o be a challenging, but powerful,\nresearch methodology. Evaluating a fully functioning syst em based on implicit measurement of\nusers is a process that has transformed the way that companie s manage, trial, and test innovations\nfor their respective systems.\n5.1.2 Motivation\nWhile there has been much publication by both companies and ac ademic groups in this area,\ntrends in search interfaces as well as techniques that conne ct online with oﬄine evaluation mean\nthere are rich new opportunities for researchers to contrib ute to this critical area of evaluation.\n5.1.3 Proposed Research\nToillustratetherangeofpossibilitiesofthisbroadagend a, welistthefollowingsuggestedprojects:\n(1)Counterfactual analysis lies at the junction of online and oﬄine evaluation. It is a to ol\nfrom causal reasoning that allows the study of what users wou ld do if the retrieval system,\nthey interact with, was changed. Drawing on a system interact ion log, one can (oﬄine)\n“re-play” the log, re-weighting interactions according to their likelihood of being recorded\nunder the changed system. From the re-played interactions, an unbiased estimator of the\n“value” of the changed system can be calculated. Value metric s are typically based on user\ninteractions (e.g. clicks, dwell time, scrolling, etc) but can incorporate editorial judgments\nof relevance or other factors. Because the user/informatio n need sample is the same in\nevery experiment, variance due to those factors can be more c ontrolled than in open-ended\ninteraction studies.\nACM SIGIR Forum 56 Vol. 52 No. 1 June 2018\n\nCounterfactual analysis relies on a rich log that captures a w ide range of interactions. Typi-\ncally some fraction of users must be shown results that have b een perturbed in a systematic\nway, but may not be optimal for them. The main challenge is bal ancing the counterfactual\nneed for perturbed results against the need to show users opt imal results. There is extensive\nopportunity for research on means to minimize both the degree of perturbation of system\nresults and the amount of log data required to produce low-va riance, unbiased estimates.\n(2)Deﬁne the axiometrics of online evaluation metrics. In the 2012 SWIRL report,\ndetermining the axioms of oﬄine metrics was proposed and soon after the meeting two\nSWIRLcolleaguesweregrantedaGoogleFacultyAwardtoexplor ethisresearchideafurther.\nWe propose that axioms for online metrics be determined. Alrea dy some axioms of such\nmeasures have been deﬁned (e.g. directionality, sensitivi ty) but it is clear that such work is\nnot yet complete.\n(3)New online metrics from new online interactions. Current online metrics mainly\ndraw on na¨ ıve user interactions. There is a growing concern t hat determining value from\nsuch interactions misses important information from users , producing systems that optimize\nshort term beneﬁts rather than long term goals. Additionally , new modes of interaction,\nsuch as conversational systems as well as smaller interface forms such as smart watches\nwon’t capture clicks or scrolls.\nIt is necessary to move to more sophisticated interaction lo gging and understanding. Back-\nground ambient noise or richer understanding of context or u ser session (see 5.3), as well as\ntechnologies such as eye tracking, could be used to determin e how users are reacting and\nbeneﬁting from an online system.\n5.1.4 Research Challenges\nSome may think that online evaluation is oﬀ limits to academi a because of a need to ‘get’ live\nusers. However TREC, NTCIR, and CLEF have explored ways of maki ng such a provision. In\naddition, smaller-scale evaluation in laboratory or live- laboratory settings, or in situ, could lead\nto advances in evaluation taking account of rich contextual and individual data. We believe that\nit may also be possible to simulate user bases with recordings of user interaction in conjunction\nwith counterfactual logging. Such collections may include logs, crowd-sourced labels, and user\nengagement observations. Such data may be collected by mean s of user-as-a-service components\nthat can provide IR systems with on-demand users who can inte ract with the system (e.g., given\na persona description) to generate logs and the context wher e online evaluations can be carried\non.\n5.1.5 Broader Impact\nOnline evaluation is not just the domain of a few global searc h brands, it is an industry. For exam-\nple, the online evaluation/optimization company Optimizely has gone from 0 to 500+ employees\nin about six years. Such companies enable smaller companies to perform online evaluation and\ntest changes. Work in online evaluation of search will have a substantial impact on search as well\nas related topics, such as recommender systems.\nACM SIGIR Forum 57 Vol. 52 No. 1 June 2018\n\n5.1.6 Broadening SIGIR\nPapers on oﬄine evaluation through test collections domina te SIGIR evaluation papers. While\nsuch work is important, there are other research challenges t o address. Venues like KDD, NIPS,\nWSDM, and ICML are publishing much work in online evaluation, a nd SIGIR-focused researchers\nshould stake more of a claim. We have the expertise in large-sc ale reusable experimental design\nthat will be necessary to harness the full power of these metho ds for retrieval systems. If we can\nencourage more IR focused online evaluation research, we ho pe this will create a bridge between\nSIGIR and the other more ML focused conferences as well as att racting new SIGIR participants\nfrom those communities.\n5.1.7 Obstacles and Risks\nA common cry from academic evaluation researchers is a reques t for logs, but many years of\nasking have provided few widely available datasets of user i nteractions. We have to be more\ncreative than calling for others to help us. A key risk is that the smaller scale research and results\nthat we conduct will not translate to the large scale problem s of the search engines. However the\nonly way to understand such risks will be to try.\n5.2 Performance Explanation and Prediction\n5.2.1 Description\nDespite the wide success of IR systems, their design and deve lopment is a complex process, mostly\ndriven by an iterative trial-and-error approach. It is impo ssible to make strong predictions on\nhow a system will work until it is tested in an operational mod e. This is because IR lacks any\ncomprehensive model able to describe, explain, and predict t he performance of an IR system in a\ngiven operational context.\n5.2.2 Motivation\nThere are new IR applications launched every day (e.g. onlin e shops, enterprise search, domain-\nspeciﬁc information services), which often require substan tial investments. IR systems are com-\nplex: made up of pipelines of heterogeneous components. The y are used together with other\ntechnologies, for example, natural language processing, r ecommender systems, dialogue systems,\netc., and they serve complex user tasks in a wide range of conte xts. However, each new instanti-\nation of these applications can only be evaluated retrospec tively.\nThere is a growing need to predict the expected performance o f a method for an application\nbefore it is implemented and to have more sophisticated desi gn techniques that allow us to ensure\nthat IR systems meet expected performance in given operatio nal conditions. We cannot postpone\nany further the development of techniques for explaining an d predicting performance, if we wish\nto be able to improve and make more eﬀective the way in which we design IR systems in order to\nkeep pace with the challenges the systems have to address.\nACM SIGIR Forum 58 Vol. 52 No. 1 June 2018\n\n5.2.3 Proposed Research\nWe need a more insightful and richer explanation of IR system performance , which not\nonly allows us to account for why we observe given performanc e: e.g.failure analysis . We also\nneed to decompose a performance score into the diﬀerent comp onents of an IR system, how the\ncomponents interact, and how factors external to the system also impact overall performance.\nRicher explanations will provide the basis for strengtheni ng the investigation of the external\nvalidity of experimental ﬁndings, i.e. how much can ﬁndings be genera lized? This, in turn, this\nwill foster accurate performance prediction of IR systems.\nWith such research in place, stronger links with interactive IR will be possible: testing\ndiﬀerent types and degrees of comparability for their suitab ility for evaluation of interactive IR.\nThiswillalsoinvolveconstructingandtestingsimulation sofusermodels, toseeiftheycanbeused\nfor traditional comparative evaluation – calling for much m ore empirical work on characteristics\nof users, their tasks, their contexts and situations.\n5.2.4 Research Challenges\nThere have been past initial attempts to build explanatory models of performance based on\nlinear models validated through ANOVA but they are still far fr om satisfactory. Past approaches\ntypically relied on the generation of all the possible combi nations of components under examina-\ntion, leading to an explosion in the number of cases to consid er. Therefore, we need to develop\ngreedy approaches to avoid such a combinatorial explosion. Moreover, the assumptions under-\nlying IR models and methods, datasets, tasks, and metrics sh ould be identiﬁed and explicitly\nformulated, in order to determine how much we are departing fr om them in a speciﬁc application\nand leverage this knowledge to more precisely explain obser ved performance.\nWe need a better understanding of evaluation metrics Not all the metrics may be equally\ngood in detecting the eﬀect of diﬀerent components and we need to be able to predict which metric\nﬁts components and interaction better. Sets of more special ized metrics representing diﬀerent user\nstandpoints should be employed and the relationships betwe en system-oriented and user-/task-\noriented evaluation measures (e.g. satisfaction, usefulne ss) should be determined.\nA related research challenge is how to exploit richer explan ations of performance to design\nbetter and more re-usable experimental collections where the inﬂuence and bias of undesired\nand confounding factors is kept under control. Most importan tly, we need to determine the\nfeatures of datasets, systems, contexts, and tasks that aﬀect the per formance of a system. These\nfeatures together with the developed explanatory performa nce models can be eventually exploited\nto trainpredictive models able to anticipate the performance of IR systems in new and di ﬀerent\noperational conditions.\n5.2.5 Broader Impact\nA better understanding and a more insightful explanation of IR system performance opens up\nnew possibilities in terms of reproducibility, external va lidity, and generalizability of experimental\nresults since it provides the means to understand what succe eded or failed, especially if linked\nto failure analysis. Better analytic tools are also an indis pensable basis for moving IR toward\nbecoming a predictive science.\nACM SIGIR Forum 59 Vol. 52 No. 1 June 2018\n\n5.2.6 Broadening SIGIR\nThere are neighbourhood areas, such as Natural Language Proc essing and Recommender Systems,\nwhich suﬀer from similar issues in terms of explanation and p rediction of the performance of their\nsystems. These areas could beneﬁt from an advancement withi n SIGIR and, at the same time,\nSIGIR could beneﬁt from teaming up with these areas to jointly address these issues and come to\nmore general and robust solutions.\n5.2.7 Obstacles and Risks\nWhile some of the proposed research activities (metrics, perf ormance analysis, assumptions) can\nalready be carried out with existing resources, the identiﬁ cation of performance-critical applica-\ntion features and the development of performance models requ ire empirical data from a larger\nvariety of test collections. Thus, researchers should share their test collections both for support-\ning reproducibility and research on prediction. Indeed, wh ile individual contributions to such an\neﬀort might not seem worthwhile for researchers, collabora tive approaches in the form of eval-\nuation campaigns might be more promising. Another potential obstacle is the need for more\nsophisticated competencies in data modelling, statistics , and data analysis, and so on. Moreover,\nboth the explanatory and the predictive performance models m ay be quite demanding in terms of\ncomputational resource needed to train and compute them.\n5.3 Measures and Methods for Evaluating Interactive IR\n5.3.1 Description\nAll IR is, to some degree, inherently interactive with the int eraction taking place among a person\nseeking information for some goal / task / purpose, some corp us of information objects (including\nthe objects themselves), and some intermediary (e.g. an IR s ystem) acting to support the person’s\ninteraction with the information object(s). Methods for ev aluating system support for persons\nengaged in interaction must be developed in order for IR syst ems to continue to improve. Such\nmethods may be similar to those of the test collection model, but, given experience to date, it is\nclearly necessary to consider quite diﬀerent alternatives .\n5.3.2 Motivation\nThe classical IR evaluation model was designed to evaluate t he performance of the IR system\nwith respect to just one interaction instance: the response that the system provides to one query\nput to that system. The model has been extended in various way s, to diﬀerential eﬀect. Test\ncollections have used a surprisingly wide range of labeling criteria: topical relevance, home-page-\nfor, key page, spam, opinionated, a-venue-I-would-go-to, novelty, and others. Cranﬁeld assumes\nan atomic preference criterion: that is, an individual docum ent’s preference label is deﬁned with\nrespect to the document and topic only. Atomicity allows us t o build test collections scalably\nbecause documents can be labeled in a single pass.\nOther kinds of criteria for building test collections should be explored. For other atomic\nqualities we need to understand how to deﬁne them, how to deve lop labeling guidelines that\nare understandable enough for separate sites to label items comparably, how to measure the\nACM SIGIR Forum 60 Vol. 52 No. 1 June 2018\n\nconsistency and reliability of those labels, and how to meas ure the impact of label disagreements.\nAs research problems these questions deserve more attention .\nAlthough there have been serious attempts to design methods t o evaluate system support\nforinformation search sessions , these have uniformly failed. There are various reasons for\nthis failure. The atomic criterion of relevance, basic to th e model, does not easily apply to the\nevaluation of the success of a whole session, and the presence o f human beings, having varied\nintentions during the information search session, making i ndividual decisions during the search\nsession, and having varied individual characteristics, has m ade comparability of performance of\ndiﬀerent systems with diﬀerent persons, as required by the c lassic model, seemingly impossible.\nExtending the Cranﬁeld model into full interactions is hard b ecause it violates the atomicity\ncriterion. To consider an interaction where a user starts fr om diﬀerent queries, encounters docu-\nments diﬀerently, and moves towards completion of the task a long multiple paths, a test collection\nwould need, at a minimum, to deﬁne the relevance of each docume nt with respect to all docu-\nments already seen. Without constraining this within some sor t of structure, there would be an\nexponential number of relevance judgments needed. Taking a further step and allowing the user’s\nunderstanding of the task to evolve and criteria for success ful completion of that task to change\nduring the interaction adds another exponent.\n5.3.3 Proposed Research\n•Identifying criteria and metrics that can/should be used to e valuate:\n–Support by the system toward accomplishing that which led the person to engage in\ninformation seeking, i.e. evaluation of success of the sear ch session as a whole.\n–Support by the system with respect to what the person is tryin g to accomplish at each\nstage in the information searching process (search intenti ons).\n–Contribution of the activity of each stage of the information searching process to the\nultimate success of the search session as a whole.\n•Creating metrics that are sensitive to diﬀerent typesof motivating goals/tasks, and to\ndiﬀerent typesof search intentions – we need to learn about the types, and de sired outcomes\nfor the types.\n•Investigating how to apply those criteria and measures thro ugh user studies and test collec-\ntions that are aligned, so that researchers can beneﬁt from bo th.\nThere is also ample opportunity to incorporate these more de tailed investigations of users into\nonline evaluation.\n5.3.4 Broader Impact\nThis research presents an incredible opportunity to broade n the community, because it will open\na wide range of research questions, which have been largely i gnored, yet are of central concern\nto the evaluation of, for instance, support for complete sea rch sessions, or of personalization of\nsearch.\nACM SIGIR Forum 61 Vol. 52 No. 1 June 2018\n\n5.3.5 Broadening SIGIR\nAccomplishing the research program will require collaborat ion among researchers from diﬀerent\ndisciplinary, theoretical and methodological traditions , e.g. computer scientists, information sci-\nentists, human-computer interaction researchers, cognit ive and experimental psychologists. The\nSIGIR community needs to ensure that its core venues support the growth of research bridging\ninteractive IR and test collection-based experimentation. There is a great deal of foundational\nwork on methodologies, and that work is best conducted where research ideas are taken note of,\nin the conferences of record for the community.\n5.3.6 Obstacles and Risks\nTheproblemofevaluationofinteractivesearchsupportise xtremelydiﬃculttosolve,ifcomparison\nand generalization of results is to take place. There does no t currently exist a sound, generally\naccepted theoretical understanding or model of interactiv e IR, on the basis of which the evaluation\ncriteria, measures and methods can be derived.\n6 Learnable Information Retrieval\n6.1 Description\nThe availability of massive data and powerful computing has t he potential to signiﬁcantly advance\nalmost every aspect of information retrieval. While these met hods have been very successful in\nsome domains – such as vision, speech, audio, and NLP – these su ccesses have not been observed\nfor information retrieval tasks. This research area analyze s some of the reasons and proposes to\ninvestigate artiﬁcial intelligence approaches to represe ntation learning and reasoning for i) core\nretrievalproblems, ii)robust,cross-domainranking, andii i)novelorintractableretrievalscenarios;\nand to deal with limited training data by i) a community eﬀort to build labeled data sets that\nare an order or magnitude larger than existing ones, ii) impr oving low sample complexity models,\nand iii) automatically generating training data from scave nged public data. Work in this research\narea will not only lead to more eﬀective retrieval systems, b ut also provide new insights into the\nfundamental problems underlying search and relevance match ing. While deep learning has led\nto some level of concern and even suspicion in the academic com munity, who have seen previous\ninstances of “hype”, the impact of neural net approaches in m any ﬁelds such as vision and NLP\nis undeniable and is well-documented in many peer-reviewed articles. In summary, the neural\nrevolution in IR empowers the end-to-end learning of an enti re search engine from data.\n6.2 Motivation\nThe information retrieval community has a proud history of d eveloping algorithms for eﬃcient\nand eﬀective information access. However, these systems are shallow in their representation and\ncomprehension of text and other media, resulting in a discon nect between where search is now\nand where it could be. This shallow understanding limits our ability to perform more complex IR\ntasks such as conversational search, summarization, and mul timodal interaction, as well as search\ntasks that require deeper understanding of documents conte nts and the user information need.\nACM SIGIR Forum 62 Vol. 52 No. 1 June 2018\n\nWhile users have mostly been content with shallow search in th e past, they are expecting the next\ngeneration of IR systems to be more intelligent. This expect ation is ampliﬁed by recent develop-\nments in artiﬁcial intelligence. The traditional models wit h manually designed representations,\nfeatures and matching functions are likely unable to cope wit h this demand.\nAtthesametime, thetraditionalmodelsmayalsobelessable tobeadaptedtonewdomains, as\nourexistingapproacheswouldrequireanontrivialamounto ffeatureengineeringandretrainingfor\nthenewdomains. Similarly, modern-daydeeplearningmetho dsarehighlyversatileandadaptable,\nand can be used to combine multimodal data inputs and heterog eneous data views, and can be\ntrained jointly over multiple tasks simultaneously (possi bly with partial labelling).\nRecent advances in artiﬁcial intelligence have resulted in p erformance improvements in several\nareassuchascomputervision, speechrecognitionandNLP.Th enewapproachesbasedonmachine\nlearning, and more particularly, deep learning, oﬀer new op portunities to IR to design and learn\nnew models. However, IR tasks have their speciﬁcities. A naive u tilization of deep learning\napproaches developed for other areas may not be a good ﬁt for IR problems. In addition, existing\ndeep learning approaches often require a massive amount of t raining data to generalize suitably,\nwhich is hard to obtain in IR area, suggesting that we should i nvestigate methods for developing\nmodels with limited training data. Intensive investigatio ns in this area are thus required.\n6.3 Proposed Research\nThe proposed research can be divided into six areas: data eﬃci ency, core ranking, representation\nlearning, reinforcement learning, reasoning, and interpre tability. We anticipate these advances\ncomplementing, rather than replacing, current approaches t o information retrieval.\nData Eﬃciency. Limited data access has limited the ability for investigato rs to study deep\nlearning approaches to information retrieval. Unfortunatel y, although this data exists in industry,\ndistributing it to the academic community would incur subst antial risks to intellectual property\nand user privacy. As a result, the community needs to conduct r esearch into:\n•training robust, accurate models using small collections,\n•developing new techniques to expand current labeled datase ts (such attempts have been\nimplemented, e.g., with weak supervision),\n•dealing with incomplete and noisy data,\n•simulating user behavior (e.g., using RL),\n•developing robust global models eﬀective for data-poor doma ins, and\n•reusing trained models for new tasks (e.g., for domain adapta tion). Current approaches\nincludes progressive NN and transfer learning.\nAdvanced retrieval and ranking models. One of the core informa tion retrieval problems involves\nthe representation of documents and queries and comparing t hese representations to produce a\nranking based on estimated relevance. Neural information re trieval models have the potential\nof improving all aspects of this task by oﬀering new methods o f representing text at diﬀerent\nACM SIGIR Forum 63 Vol. 52 No. 1 June 2018\n\nlevels of granularity (sentences, passages, documents), n ew methods of representing information\nneeds and queries, and new architectures to support the infe rence process involved in comparing\nqueries and text to ﬁnd answers that depend on more than topica l relevance. For example, hybrid\nmodels combining diﬀerent structures such as CNNs and LSTMs ca n capture diﬀerent linguistic\nand topical structures, attention mechanisms can capture r elative term importance, and GANs\nmay be able to lead to ranking models that require less trainin g for a new corpus. It is not yet\nknown which architectures are the most eﬀective for a range o f information retrieval tasks, but\ntheir potential is driving an increasing amount of research . As new models are developed, it will be\ncritical that they are accompanied by in-depth analysis of h ow diﬀerent aspects of the models lead\nto success or failure. Models that work with existing feature -based approaches, such as learning\nto rank, will have a critical role in producing systems that i mpact current search applications.\nData Representation. Current deep learning techniques for data representation a re not directly\nsuitableforIRmodels,aswedealwithmultimodalinput,e.g .textdocuments,userfeatures,music,\nimages, videos. Therefore, we need to work on new ways of data representation speciﬁc to IR\nproblems. There may be other, external information of value available, that needs to be combined\nwith these dense representations in more eﬀective ways than a hard ﬁlter. Another aspect of this\nis semantic emergence: semantic properties emerging durin g training for a particular task that are\nnot directly related to the task and were not explicitly plan ned to emerge. An example for this is\nthe emergence of a “sentiment neuron” when learning a simple language model on a large set of\nreviews. In image classiﬁcation with deep neural networks, edge detection emerges on a certain\nlevel of the network. It will be interesting to ﬁnd out which ot her semantic concepts can emerge\nin this way when training for basic (or not so basic) informat ion retrieval tasks.\nReinforcement Learning. Because information access is often situated in an interact ive search\ntask, the ability to perform intelligent sequential decisi on-making is a fundamental — yet under-\nexplored — area of information retrieval. Recent advances in reinforcement learning suggest that\ntechniques are ready to be applied to complex domains like sea rch. That said, applying these\ntechniques to information retrieval requires substantial research into:\n•acting in extremely large, non-stationary state and action spaces, and\n•developing eﬀective unsupervised objective functions for multi-turn retrieval, and\n•modeling interactions through RL has a high potentials for u ser simulation task.\nEnd to end learning. Certain complex information retrieval problems might be le arnable in a\ncompletelyend-to-endfashion. Forexample,inputthequer y,outputthesetofrelevantdocuments.\nOr input a question in natural language, and output an answer . There is already a fair amount\nof work in that direction, for example: given a question in na tural language and a text, output\nthe passage or passages of the text that answer the question.\nMachine Reasoning. There has recently been signiﬁcant progress on machine reas oning in the\ncontext of tasks such as text understanding and reasoning, e. g. bAbi, and dialogue state tracking,\nfocusingon“memory”architecturesforselectivelycaptur ingdialogue/documentcontextasneeded\nfor long-distance inference. There are also many attempts to integrate domain knowledge or\nknowledge graphs in NLP (e.g. QA). There are direct applicatio ns for this style of model (and\nunique application areas) in information retrieval, inclu ding:\nACM SIGIR Forum 64 Vol. 52 No. 1 June 2018\n\n•tracking “state” in multi-turn information retrieval (e.g . conversational, session-based),\n•smoothing document- and term-level predictions across a ses sion/collection,\n•interpreting complex search requests,\n•supporting question answering, and\n•implementing domain-speciﬁc information retrieval.\nError Analysis/Explainability. Itisimportanttoadvancethecurrenterroranalysistechni ques\nand to make our model explainable for:\n•ﬁnding the errors in training sets that cause problems on out put level,\n•understanding for what cases old models work better and when NN models show better\nresults (e.g. NN work better for long queries), which may lead hybrid neural architecture,\nand\n•making results explainable for users/system designers.\n6.4 Research Challenges\nExisting high baselines : Over the long history of IR, we have developed models and appr oaches\nfor ad-hoc and other types of search. These models are based on human understanding of the\nsearch tasks, the languages and the ways that users formulat e queries. The models have been ﬁne-\ntunedusingtestcollections. Theareahasasetofmodelstha tworkfairlywellacrossdiﬀerenttypes\nof collections, search tasks and queries. Compared to other areas such as image understanding,\ninformation retrieval has very high baselines. A key challe nge in developing new models is to be\nable to produce competitive or superior performance with re spect to the baselines. In the learning\nsetting, a great challenge is to use machine learning method s to automatically capture important\nfeatures in representations, which have been manually engi neered in traditional models. While\ngreat potential has been demonstrated in other areas such as computer vision, the advantage of\nautomatically learned representations for information retr ieval has yet to be conﬁrmed in practice.\nThe current representation learning methods oﬀer a great op portunity for information retrieval\nsystems to create representations for documents, queries, users, etc. in an end-to-end manner.\nThe resulting representations are built to ﬁt a speciﬁc task. Potentially, they could be more\nadapted to the search task than a manually designed represen tation. However, the training of\nsuch representation will require a large amount of training data.\nLow data resources : representation learning, and supervised machine learnin g in general, is\nbased heavily on labeled training data. This poses an importa nt challenge for using this family\nof techniques for IR: How can we obtain a suﬃcient amount of tra ining data to train an infor-\nmation retrieval model? Large amounts of training data usua lly exist only in large search engine\ncompanies, and the obstacle to making the data available to t he whole research community seems\ndiﬃcult to overcome, at least in the short term. A grand chall enge for the community is to ﬁnd\nways to create proxy data that can be used for representation learning for IR. Examples include\nthe use of anchor texts, and weak supervision by a traditiona l model.\nACM SIGIR Forum 65 Vol. 52 No. 1 June 2018\n\nData-hungry learning methods have inherent limitations in many practical application areas\nsuch as IR. A related challenge is to design learning methods t hat require less training data.\nThis goal has much in common with that of the machine learning a rea. The information retrieval\ncommunity could target learning methods speciﬁcally desig ned for information retrieval tasks that\nrequire less labeled data.\n6.5 Broader Impact\nThe development of machine learning and AI “algorithms” for new products and services, and the\nsuccess of deep learning methods in high visibility competi tions, have received enormous attention\nin the media. New courses on machine learning and neural models are proliferating and many\nnew papers related to these topics are appearing every day on arXiv. Given that search is nearly\nubiquitous and that search engines are the best example of IR and NLP in action, it is not\nsurprising that there has been a signiﬁcant upsurge in inter est in applying neural models to many\naspects of IR and search, both by graduate students and by comp anies working on a broad range\nof applications. Although this research is still at an early s tage and much remains to be done to\ndemonstrate the levels of improvement in eﬀectiveness seen in other ﬁelds, the potential impact of\nnew retrieval approaches developed using learning techniq ues is enormous. If we are successful, for\nexample, we will make signiﬁcant progress towards achievin g one of the long-standing goals of IR -\nan “intelligent” conversational search assistant similar tothose seen in theearly Star Trek episodes\nthat inspired so many future computer scientists. On a more mu ndane level, we should also be\nable to develop search techniques that are substantially mo re portable and eﬀective across tasks,\ndomains, applications, and languages. Search will be subst antially more eﬀective and available to\nall segments of society.\n6.6 Broadening SIGIR\nThis area draws heavily in the ﬁrst instance on work done in ma chine/deep learning and statistical\nnatural language processing, and as such, any activity in th is space will naturally lead to stronger\nconnections with these ﬁelds through cross-fertilization o f ideas and greater visibility for SIGIR\nresearch. Beyond this, there are unique characteristics/c hallenges in IR that we can expect to\ngive rise to methodological breakthroughs with broader imp lications including:\n•IR has a very mature understanding of what types of document/ collection representation\nare needed for retrieval (e.g. inverted ﬁle indexing, posit ional indexing, document zoning,\ndocument graphs), more so than ﬁelds such as speech, NLP and com puter vision, and de-\nveloping representation learning methods that are able to c apture these rich data structures\nwill have implications well beyond IR.\n•IR has decades of experience in assigning, interpreting and l earning from document-level\nrelevance judgments; there isconsiderablescope totransf erthis expertisebeyond the bounds\nof IR.\n•IRhasarichhistoryofmultimodality(includingimages,sp eech,video,and(semi-)structured\ndata) with well-established datasets, and a relatively mat ure understanding of how to har-\nness that multimodality to draw inspiration from when devel oping new models.\nACM SIGIR Forum 66 Vol. 52 No. 1 June 2018\n\n•There is deep knowledge of methods for attaining run-time an d storage eﬃciency in IR, both\nof which are critical issues in machine/deep learning resea rch at present, and any advances\non the part of the IR community would have far-reaching implic ations beyond SIGIR.\n•IR expertise in evaluation, especially focusing on the user experience, has the potential to\nsigniﬁcantlyshaperesearchontaskssuchasquestion-answ ering,summarizationandmachine\nreading, where current evaluation practices are narrowly f ocused on string matching with a\ngold standard.\n6.7 Obstacles and Risks\nData requirements : the strong results that have been achieved by (deep) learni ng approaches in\nrecent years are usually predicated on large amounts of trai ning data. Training data is notoriously\nhardtogetbyinacademia,especiallyinlargequantitiesan dofaqualitythatreﬂectsrealusecases.\nIt is one of our challenges to ﬁgure out how to get by with less t raining data or to automatically\ngenerate training data in an unsupervised fashion. However, one outcome of these attempts may\nbe that large amounts of explicit training data are indispen sable.\nResource requirements : training eﬀective models usually requires large amounts o f computing\nresources (time and/or number of CPUs/GPUs/TPUs), even for onl y moderately large datasets.\nGetting access to these resources may be an obstacle for grou ps in academia, especially smaller\ngroups or groups endowed with less money. Training these mod els on very large datasets may still\nbe outside of our reach for a number of years.\nEﬃciency : the eﬀectiveness of learned models often comes at the price of an increased processing\ntime when using them for prediction or classiﬁcation or what ever it is that they were trained to\ndo. If these processing times are several orders of magnitud e slower than those of state-of-the-art\napproaches, these approaches may be of little use in product ion systems.\nExplainability : learned models may achieve an eﬀectiveness that is superio r to classical ap-\nproaches, but it may be hard or impossible to explain why a par ticular results was computed. It\nmay also be hard to provide guarantees or explain what went wr ong in case something went wrong.\nThese drawbacks may be major obstacles to adopting these tec hniques in production systems.\n7 Generated Information Objects\n7.1 Description\nIn modern devices, search results may be presented on a small screen or as a spoken response. This\nincreases the importance of generating a search result in a f orm that is most helpful to the user.\nIn such situations, and in general, it may be less than ideal f or the retrieval system to present raw\ninformation as-is. Better would be a summarization of exist ing information to support absorbing\ncomplex material eﬃciently. Current data processing pipel ines include an increasing amount of\nannotation, ﬁltering, and aggregation steps. These will le ad to new interaction paradigms beyond\nsearch engine result pages (SERPs). A conceptual framework to discuss such future research is\ncentered around Generated Information Objects (GIOs).\nACM SIGIR Forum 67 Vol. 52 No. 1 June 2018\n\nFigure 1: Example of a system using the GIO framework.\nGiven an information need the goal is to generate a single res ponse that consists of multi-\nple information units. These units can be derived from raw inf ormation but undergo a trans-\nformation/generation process. For instance, preprocessi ng, semantic annotation, segmentation,\nclean-up, and recombination are possible generation operat ions, but raw information can also be\nprovided as-is (as a trivial generation operation). It shal l also be possible to generate informa-\ntion objects through logical reasoning or natural language generation. Information objects can be\n“canned”, i.e., preprocessed and indexed in an appropriate representation, or generated “on-the-\nﬂy” at query time. The generated response can further be arch ived, shared, made available for\nbrowsing and be recycled for the next response.\nBoth information units and the response are considered gener ated information objects (GIOs).\nProvenance information about the original information sou rce and generation operations shall be\npreserved in the process. In the context of a response, compr ising information units are further\nassociated with an explanation of relevance (or usefulness) for a response. We expect users to\nquestion whyinformation is important and have a subsequent conversatio n about the provided\ninformation.\nThe central research question: What is the best way to store ge nerated information units, and\nhow to make best use of information units and previous respon ses when generating a response?\nThe question on data structure and representation of genera ted information objects is related\nto the question of appropriate forms of presentation of the i nformation. The same generated\nresponse object might be represented in diﬀerent modalitie s depending on the situation, user’s\npreferences, and situational context – as text, image, or vo ice.ACM SIGIR Forum 68 Vol. 52 No. 1 June 2018\n\n7.2 Motivation\nSo far retrieval means to preserve information as found, but not to modify or change it. Novel\napproaches towards semantic annotations, recombination o f information, and summaries of infor-\nmation are already breaking with this old paradigm, as the foll owing examples illustrate.\nGenerating abstractive summaries from text retrieval was st udied at the NTCIR One Click\ntrack. Combination of diﬀerent heterogeneous source archiv es is discussed in federated search.\nThe recombination of information into “information bundle s” or “entity cards” is another form\nof generation. Diversiﬁcation of search results is a form of generating rankings. When utilizing\nentity links and semantic annotations in retrieval, origin al information undergoes extraction and\nprocessing steps such as tokenization, entity linking, and e ntity type prediction. Such derived\ninformation needs to be stored in the right representation t o maximize usefulness for downstream\ntasks. Often information changes over time, demanding appro aches that ensure the consistency\nand freshness of information.\nConsidering how transformations of raw source information leads to generated information\nobjects (GIO), gives us the option to discuss best practices for representation, storage, and access.\nImportantly, this discussion can be independent of the chos en presentation form. We suggest a\nconceptual framework of generated information objects of w hich all aforementioned examples are\nspecial cases. We believe the GIO framework helps to identif y underlying patterns and share best\npractices regarding representation, storing of derived in formation and retrieval, recombining and\nrecycling partial answers.\n7.3 Proposed Research\nThe framework is intended to support a critical discussion a bout the state-of-the-art regarding:\n(1) Harvesting: How to segment input data into units to that the y are likely to be reused?\n(2) Representation: What is the most eﬀective representatio n of information units to be maxi-\nmally amenable and eﬀective for the downstream task?\n(3) Dynamic changes: How to represent information units whos e content changes over time?\nInformation may change and canned GIOs may become out-of-da te.\n(4) Connections: How to preserve connections between inform ation units that can be retrieved\n(not only “boring” connections)? How to make connections acces sible through retrieval\nmodels?\n(5) Storage: How to store the content while preserving provenan ce information across diﬀerent\ngeneration, recombination, and modiﬁcation operators?\n(6) Eﬃcient access: How to index information units so that cand idate sets for complex queries\n(i.e., many query terms) for eﬃcient retrieval of diverse ca nned GIO units?\n(7) Condensing: How to represent information units so that re dundant, entailing, similar, and\nrelated content can be eﬃciently identiﬁed? How to conﬂate di ﬀerent information units?\nACM SIGIR Forum 69 Vol. 52 No. 1 June 2018\n\n(8) Text generation: How to generate text for that is most appr opriate for diﬀerent devices and\nmodes of interaction?\n(9) Easeofuse: Howtooptimizeinformationstructureandﬂow ofGIOs? Forexamplestructure\nchronological or topical order of facts, salience, lack of r edundancy, and readability.\n(10) Recycling: How to recycle previous responses, especia lly in a turn-based dialog?\n(11) Privacy: When reusing and recycling GIOs that are create d in response to user interac-\ntions, this may leak private information into the system. How are user’s privacy concerns\nappropriately addressed?\n7.4 Research Challenges\nKnowledge Graph Representation in GIOs. The goal is to represent open domain infor-\nmation for any information need. Current knowledge graph sc hemas impose limitations on the\nkinds of information that can be preserved. Schuhmacher et a l. found that many KG schemas are\ninappropriate for open information needs. OpenIE does not l imit the schema, but only low-level\ninformation (sub-sentence) is extracted. In contrast, sem i-structured knowledge graphs such as\nDBpedia oﬀer a large amount of untyped relation information which is currently not utilizable.\nA challenging question is how to best construct and represent knowledge graphs so that they are\nmaximally useful for open domain information retrieval task s. This requires new approaches for\nrepresentation of knowledge graphs, acquisition of knowle dge graphs from raw sources, and align-\nment of knowledge graph elements and text. This new represent ation requires new approaches for\nindexing and retrieval of relevant knowledge graph element s.\nAdversarial GIOs. Not all GIOs are derived from trustworthy information. Some i nformation\necosystem actors are trying to manipulate the economics or a ttention within the ecosystem. It\nis impossible to identify “fake” information in objects wit hout good provenance. To gain the\nuser’s trust, it is important to avoid bias in the representat ion which can come from bias in the\nunderlying resources or in the generation algorithm itself. To accommodate the former, the GIO\nframework enables provenance tracing to raw sources. Additi onally, contradictions of information\nunits with respect to a larger knowledge base of accepted fac ts need to be identiﬁed. Such a\nknowledge base needs to be organized according to a range of po litical and religious beliefs, which\nmay otherwise lead to contradictions. The research question i s how to organize such a knowledge\nbase, and how to align it with harvested information units. Fin ally approaches for reasoning\nwithin a knowledge base of contradicting beliefs need to be d eveloped. Equally important is to\nquantify bias originating from machine learning algorithms which may amplify existing bias.\nMerging of Heterogeneous GIOs. To present pleasant responses, it is important to detect re-\ndundancy, merging units of information, such as sentences, i mages, paragraphs, knowledge graph\nitems. For example, this includes detecting when two senten ces are stating the same message (i.e.,\nentailment). For example “the prime minister visited Paris ” from a document about Margaret\nThatcher, and an identical sentence “the prime minister vis ited Paris” from a document about\nTony Blair, should not be conﬂated. Even more challenging is the detection of information units\nthat are vaguely related (according to a relation that is rele vant for the information need). The\navailability of such approaches would allow for structurin g and organizing content. Provenance\nACM SIGIR Forum 70 Vol. 52 No. 1 June 2018\n\nreferences of information units and textual context can pote ntially help the integration of informa-\ntion units. Record merging in data bases is achieved bycount ing agreements versus disagreements.\nThe research challenge is to perform such a merge in a multi-mo dal and open-domain setting.\nResource Location Across Turn-based Conversational Information Seeki ng.In multi-\nturn interactions, people engage with a GIO as a response. A u ser asks a question and is presented\nwith a response that is generated of multiple parts. It is like ly that this user would like to interact\nwith one part of the response. For example, the user may ask a f ollow-up question about one part\nor may reconsider a part of an earlier response at a later time. The research challenge is to provide\na representation of multi-part answers and an appropriate p resentation so that the user can refer\nto a part. This is even more challenging for voice interaction s for which new kinds of anaphora\nresolutionneedtobedeveloped. Resourcelocationisdiﬃcul teveninclick-and-pointpresentations,\nespecially when the answer arises from summarization of diﬀ erent information units. We suspect\nthat this requires aligning information units into groups th at the user intuitively interprets as one\nconcept.\nThisresearchisrelatedtopreviousapproachesofidentify inginformation“nuggets”asaninput\nto summarization. The challenge is to identify nuggets with out human involvement, by training\nalgorithms that identiﬁed re-usable informative componen ts that make for useful GIO information\nunits in diﬀerent context.\nDeriving Explanations from GIOs. The rationale of a generated response (with respect\nto the information need) needs to be explainable to the user. I n search snipped generation,\nsuch explanations are typically identiﬁed through high-de nsity regions of keyword matches. For\ncomplex generated information responses this will not be suﬃc ient. We envision that such an\nexplanation entails two parts: 1) Explain how the given info rmation need was interpreted, and 2)\nexplaining every part of the generated response.\nFor example, in a system that jointly reasons about relevant e ntities and relevant text, the\nsystem may be asked why a particular entity is relevant. While the relevant text related to\nthis entity may be one ﬁrst approach, one can imagine that a us er would rather hear a direct\nexplanation such as “This entity is relevant because ... ”. In order to give such explanations, a\nsystem must be able to understand the concepts contained in th e text and their relation to the\ninformation need. The research challenge is how to understa nd both the information need and\nresponse text on a conceptual level that would allow such exp lanations.\nContext and Personalization. Any information-seeking behavior is dependent on context:\nthe user’s prior knowledge, the task to be accomplished, and the previous interaction. The GIO\nframework allows for modeling, storing, and considering an y user context that is available. While\nstudied for many decades, it is still open which representati on of the user’s context is optimal for\nwhich task, and how to derive representations that are versa tile across diﬀerent retrieval tasks at\nonce. Even more important is to model shifts in user’s contex t and interest over time. This aﬀects\nboth the selection of information unit, the appropriate gen eration of the response object, and the\neﬀectiveness of the chosen presentation.\nEvaluation. It is diﬃcult to create reusable test collections when answe rs are summarizations of\ndiﬀerent parts. The entirety of a GIOs presentation may need to be evaluated in a holistic end-\nto-end information seeking context. However, early experim ents arising from the TREC Complex\nAnswer Retrieval evaluation demonstrate that it is practica l and feasible to evaluate individual\nACM SIGIR Forum 71 Vol. 52 No. 1 June 2018\n\nparts of the GIO response: Manual assessment of passages in t he Cranﬁeld paradigm correlated\nextremely well with alternative ground truths derived from gold articles.\nFurthermore, we hypothesize that GIO representations that a llow systems to generate expla-\nnations for the relevance/suitability of the GIO response or its parts, assist in creating assessor-\nindependent datasets. Asking assessors to only consider par ts as relevant if the submitted ex-\nplanation would support this claim, and not use external worl d knowledge, may lead to a high\ninter-annotator agreement.\nOne key research challenge in generated information object s is the transformation of raw\ndata through cleanup, combination, and modiﬁcation to the p oint where many challenges of\nsummarizationalsoapplytoGIOs. Evaluationofsummaries( andtherebyprocessesthatgenerated\nthem) are typically be based on the readability, cohesiveness , informativeness of content (intrinsic)\nor based on its helpfulness in the context of completing a task , such as reading comprehension,\nlearning (extrinsic). At the intrinsic level, automated ap proaches have included measures such as\nBLEU and ROUGE, where a new generated summary is compared agai nst a human-generated set\nof reference objects based on n-gram overlap. However, one ma y imagine an equally good or better\nsummary that uses diﬀerent words, which would lead to a bad sco re under the ROUGE measure,\nor equally a summary that is nonsensical from a human perspect ive but happens to score highly.\nWhile some studies have been shown that ROUGE can be correlated with human judgments of\nsummaryquality,othersusecasesidentiﬁedlimitationsint ermsofreﬂectingmeaningfuldiﬀerences\nin algorithms.\nThese downsides of summarization evaluation are a major obsta cle towards summarization-\nbased approaches in the IR community. The same issue holds fo r many other forms of generation,\nsuch as redundancy-removal, grouping of ranked results, or p resentation of heterogeneous informa-\ntion. Therefore, more research on alternatives for generat ed results are important. For example\nthe NTCIR One-Click track evaluated summaries by the number o f unique and positive facts\nincluded. The TREC Complex Answer Retrieval track, evaluated t he relevance of parts (i.e., enti-\nties) by the system provided explanation of their relevance . The approach taken by One-Click and\nCAR is to evaluate a part of the response, which does not ﬁt into th e intrinsic/extrinsic scheme\nused for summarization. More research in this direction of IR -based summarization will not just\nprovide an avenue for evaluating GIOs, but also have an impac t on the NLP/Summarization\ncommunity.\n7.5 Broader Impact\nSeveral sub-communities in information retrieval have cov ered various aspects of the GIO frame-\nwork. We think that this conceptual framework can facilitat e a spread of best practices across\nthe ﬁeld. New indexing approaches and search models will like ly arise from applications that use\ngenerated information objects to the fullest.\nAs the GIO framework requires us to rethink many moving parts o f typical IR systems, it bears\nthe potential for more collaboration across typical IR subﬁ elds. Additionally, by opening up the\ndeﬁnition of IR to explicitly include summarization, natur al language generation, and computer\nvision, itenhancesthetiesbetweentheIRcommunityand comm unitiesoninformationextraction,\nknowledge graph construction, text and vision, spoken dial ogs, and databases.\nACM SIGIR Forum 72 Vol. 52 No. 1 June 2018\n\n7.6 Broadening SIGIR\nAn explicit goal of research with generated information objec ts (GIOs) is to broaden the scope of\nthe SIGIR discipline. To address open domain information ne eds, GIOs will be combined from\nrich and varied generations of new information objects. In t he long term, GIOs are a stepping\nstone towards the synthesis of new information from existin g sources.\nThis research eﬀort will fuel a wide range of obvious cross-ﬁ eld collaborations: 1) NLP to\nunderstand linguistic representations, summarization, a nd discourse and dialog, 2) HCI to create\nuser-information interactions that are natural and help th e user accomplish her task eﬀectively,\nand 3) vision and language to enable eﬃcient presentation and interaction when multi-modal\ninformation is used as information units. We believe that cr eating challenge tasks with GIOs in\nmind will clear up some risks and concerns regarding feasibil ity and evaluation, and spur increased\ncollaboration within and across the community.\n7.7 Obstacles and Risks\nThe conceptual framework is quite general, and while many tas ks can be addressed with this\nframework, this generality might make it diﬃcult to pin down all possible challenges and see\nwhich practices can be shared.\nSome of the engineering structures required to support GIOs are much less developed and\ninvestigated than conventional IR system components. Parti cular challenges lie in an algorithmic\nunderstanding of information needs and response text. This r equires a representation and inter-\naction mechanism that allows referring to generated respon se parts, giving relevance explanations\nfor generated information units, and reasoning about conﬂic ts and trustworthiness of harvested\ninformation units.\nIndustrial applications of GIOs for task-speciﬁc purposes are likely to push the development of\nthis area quite quickly ahead of the research community. We r un the risk of falling behind rather\nthan leading this eﬀort.\n8 Next-Gen Eﬃciency Challenges: Smaller Faster Better\n8.1 Description\nEnsuring both eﬀectiveness and eﬃciency is paramount for th e practical deployment of web (and\nother) search engines. The last few years have seen the IR com munity tackle more complex\nsearch tasks and information needs coupled with a marked inc rease in the size of collections. New\nranking paradigms based on machine learning (for example, l earning to rank and neural networks)\nand new applications (for example, conversational informa tion seeking or searching on the cloud)\nare pushing the boundaries of existing IR architectures, rai sing new challenges, but at the same\ntime oﬀering opportunities for the IR community to investig ate new alternative architectures,\nrevisit existing index structure assumptions, optimize tra de-oﬀs between eﬀectiveness, eﬃciency\nand costs, and investigate new eﬃciency problems and paradi gms.\nACM SIGIR Forum 73 Vol. 52 No. 1 June 2018\n\n8.2 Motivation\nWhile there is a history of strong research on eﬃciency issues in the IR community, much attention\nhas focused on a few established and often narrow problems an d setups motivated by standard\nIR architectures and systems, leaving many other cases larg ely unexplored. There have also\nbeen a number of emerging changes in modern search systems th at call for new directions and\napproaches. In particular, eﬃciency researchers need to lo ok at issues related to Multi-Stage\nSearch Systems (MSSs), which are increasingly being deploy ed with machine-learned models, and\nexamine the end-to-end performance of methods under MSSs. T here are also opportunities for\nfurther eﬃciency improvements that require the applicatio n of machine learning and data mining\ntechniques, including methods that learn index structures, learn how to optimize queries, or that\nestimate query distributions and optimize for these. There might also be ways to completely\nbypass the currently used index structures via neural nets, s tructures from the Combinatorial\nPattern Matching community, or FPGA-based systems. Finally, n ew emerging IR applications\nalso require attention. In summary, it is time to reconsider some basic assumptions, and to step\naway (at least partially) from the comfortable world of inve rted lists and simple ranking functions\nthat so many (often strong) papers have addressed.\n8.3 Proposed Research\nWe organize the proposed work into four major streams of resea rch:\n(1)Eﬃciency and MSSs : Search engines have been using highly complex rankers for qui te\nsometime, buttheeﬃciencycommunityhasbeenslowtoadapt. Thelastfewyearshaveseen\nsome initial attempts to address this, but there are many rem aining opportunities. Future\nwork should evaluate new and existing end-to-end performan ce optimizations in the context\nof MSSs. We need automatic ways to create optimized index str uctures and to optimize\nquery execution for a given MSS. We need new measures and objec tives to optimize for in\nthe early stages of cascading systems, and eﬃcient ways to in dex and extract features for\nuse in the cascades. Finally, we need to look at the impact of se arch results diversiﬁcation\nand personalization in such systems.\n(2)ML for eﬃciency : Researchers are increasingly using machine learning and da ta min-\ning to improve algorithms and systems. Examples include lea rning of index structures for\nparticular datasets and ranking functions, modeling of que ry distributions to build small\nindex structures for likely queries, learning of query rout ing and scheduling policies to sat-\nisfy service level agreements (SLAs) on latency and quality w hile maximizing throughput,\nor prediction of query costs and the best execution strategy for particular queries. One\nmajor challenge is how to formalize and guarantee performan ce bounds on such machine-\nlearned components, which will enable reasoning about guara ntees for the overall system. In\nshort, ML and data mining techniques are popping up everywhe re in the search engine ar-\nchitecture, and will drive future performance improvement s, sometimes in unexpected ways.\nConversely, IR eﬃciency researchers should also use their s kills to make machine learning\ntools more eﬃcient, as training and evaluation currently req uire huge amounts of resources,\ne.g., deep neural nets.\nACM SIGIR Forum 74 Vol. 52 No. 1 June 2018\n\n(3)Challenging the current setup : The ready availability of alternative architectures such\nas vector processors (SSE, AVX instructions, and the like) an d FPGAs provide opportunities\nto examine IR eﬃciency from a new angle — research on these dev ices as well as GPUs is in\nits infancy. The introduction of General Purpose Graphics Pr ocessing Units (GPGPUs) and\nTensor Processing Units (TPUs) into general purpose CPUs will p rovide entirely unexplored\navenues for research. These hardware architectures will so on be available on all users’\nclients (from phone to tablet to laptop to desktop), providi ng opportunities to oﬀ-load\nwork from the data center and onto the user’s device. Beyond w eb search, there remain\nseveral unsolved fundamental problems. In an environment w ith high document turnover,\nnew index structures, beyond inverted ﬁles, may be called fo r. What is clear is that there is\ndemand for such structures, and updating an inverted ﬁle wit h additions and deletions is not\nnecessarily an eﬃcient process. In summary, the time has com e to reconsider the standard\nindex setup. Indeed, the index of the future might in extreme cases be just a set of weights\nin a neural network.\n(4)New applications : In a truly interactive IR system, such as a conversational i nformation\nseeking system, information needs are complex, typically r equiring iterative user dialogue\nwith the system, with each iteration encompassing query refor mulation and access to the\nindex. The costs of these iterations might be reduced in seve ral dimensions. For example,\nwe could leverage incremental computations across the sequ ence of iterations to enhance\nscalability and eﬃciency through suitable caching or predi ction of the remaining dialogue\nthe user will engage in. The development of real-time search applications on the Internet of\nThings (IoT) infrastructure also requires new indexing and search architectures, to allow the\nseamless ingestion, indexing and querying of data in real-ti me. Related to this, the emer-\ngence of new search services in the cloud, with service level g uarantees coupled with limited\nresources and various constraints, opens up a number of unexp lored directions in search\neﬃciency and eﬀectiveness trade-oﬀs. For example, the searc h engine has to continuously\nanalyze and decide on the best conﬁguration of its system giv en the available resources and\nthe guaranteed level of services. Moreover, the increasing importance of ensuring account-\nability, transparency, and explainability in machine learn ed MSSs entails additional costs\nfor the search engine, beyond the actual task of retrieving i nformation. Such costs include\nthe generation of the explanations for the retrieved results and their visualization, which\nrequire new eﬃciency research directions including the revi siting of data structures to cope\nwith such additional costs.\n8.4 Research Challenges\nSeveral interesting research challenges continue to exist when building traditional eﬃcient and\neﬀective IR systems (such as compression, ﬁrst stage query r esolution, and so on). In multi-\nstage retrieval systems the complexity is substantially hi gher and new areas need addressing. For\nexample, at present we do not even know where and why these syste ms are slow.\nAsmentionedabove, excitingnewchallengesexistinthearea sofconversationalIRandlearned\ndata structures. While the notion of combining learning with eﬃcient indexing is not an entirely\nnew idea, recent advances in neural IR models have shown that l earned data structures can in\nACM SIGIR Forum 75 Vol. 52 No. 1 June 2018\n\nfact be faster, smaller, and as eﬀective as their exact solut ion counterparts. However, enforc-\ning performance guarantees in learned data structures is st ill a research problem requiring work.\nLikewise, as search becomes even more interactive, new oppo rtunities for eﬃcient indexing and\nranking are emerging. For example, virtual assistants can l everage iterations on complex informa-\ntion in order to improve both eﬀectiveness and eﬃciency in the interaction. But how to evaluate\niterative changes for interactive search tasks is a signiﬁc ant challenge, and very few collections\ncurrently exist to test new approaches, let alone to test the end-to-end eﬃciency performance of\nsuch systems.\n8.5 Broader Impact\nThe goal of eﬃciency is to make it possible to process (in the g eneral sense) more data with\nfewer resources. For search this has the eﬀect of reducing th e overall quantity of hardware (and\nassociated infrastructure). In a data center with 100 ,000 machines, a reduction in execution time\nby 5% is the equivalent of 5 ,000 machines, together with the power, cooling, and mainten ance of\nthose machines. This is a green computing contribution (aka green IR). The released resources\ncouldbeusedtoabsorbmoreusergrowthwithoutrequiringad ditionalresources, couldbedirected\ntowards other workloads, or could be simply turned oﬀ until ne eded.\nA secondary eﬀect is an increase in the amount of data that can be processed on a single\nresource, lowering the barrier to entry for researchers in t he broader ﬁeld of IR, and for companies\nentering the marketplace. A cluster of machines would not be needed if we make the work eﬃcient\nenough such that it can be carried out on a laptop.\nIn the longer term, the quest for eﬃcient IR has the potential to impact the design of CPUs\nand other components in the computing system. CPUs with vecto r processors (AVX, etc) are\nalready in desktops – and the instructions are already being u sed in IR applications. FPGAs are\nnow being integrated into CPUs, and we know that Microsoft is a lready using FPGAs for search.\nFuture directions include GPGPU instruction integration i nto the main core.\nDecreased costs might be achieved by oﬄoading some of the wor k to the user’s device (edge\ncomputing). For example, the last phase of reranking using a n already learned (or adaptive)\nranking function might even be performed on the user’s mobil e phone. Such a change might be\ndisruptive to the standard model of internet or other search , and might also enable new privacy-\npreserving mechanisms.\n8.6 Broadening SIGIR\nSeveral opportunities exist to broaden the IR Community and interact with other communities on\nemerging eﬃciency challenges. Speciﬁc examples include th e embedded / distributed computing\ncommunity for cross-device search and machine-driven sear ch in IoT devices, the NLP community\nfor conversational IR, the ML community for complex ranking function optimization, the CPM\ncommunity for future index structures, and the database com munity for combining structured and\nunstructured resources and for query optimization ideas.\nACM SIGIR Forum 76 Vol. 52 No. 1 June 2018\n\n8.7 Obstacles and Risks\nFor academics seeking to undertake research in large-scale IR systems there are obvious risks,\nprimarily in regard to achieving genuine scale. Many of the r esearch questions that oﬀer the\ngreatest potential for improvement – and the greatest possi bilities for economic savings – involve\nworking with large volumes of data, and hence signiﬁcant comp utational investment. Finding\nways of collaborating across groups, for example, to share ha rdware and software resources, and\nto amortize development costs, is a clear area for improvement .\nCurrent practice in academic research in this area tends to r evolve around one-oﬀ software\ndevelopments,oftenbygraduatestudentswhoarenotnecess arilysoftwareengineers,asconvoluted\nextensions to previous code bases. At the end of each student ’s project, their software artifacts\nmay in turn be published to GitHub or the like, but be no less a com bination of string and glue\n(andawkandsedperhaps) than what they started with. Agreeing across resear ch groups on some\ncommon data formats, and some common starting implementati ons, would be an investment that\nshould pay oﬀ relatively quickly. If nothing else, it would a void the ever-increasing burden for\nevery starting graduate student to spend multiple months ac quiring, modifying, and extending a\ncode base that will provide baseline outcomes for their expe rimentation.\nHarder to address is the question of data scale and hardware sc ale. Large compute instal-\nlations are expensive, and while it remains possible, to at l east some extent, for a single server\nto be regarded as a micro-unit of a large server farm, there are also interactions that cannot be\nadequately handled in this way, including issues associate d with the interactions between diﬀerent\nparts of what is overall a very complex system. Acquiring a lar ge hardware resource that can be\nshared across groups might prove diﬃcult. Perhaps a combine d approach to, for example, Amazon\nWeb Services might be successful in being granted a large slab of storage and compute time to a\ngenuinely collaborative and international research group .\nHarder still is to arrange access to large-scale data. Public web crawls such as the Common\nCrawl can be used as a source of input data, but query logs are i nherently proprietary and diﬃcult\nto share. Whether public logs can be used in a sensible way is an ongoing question. Several prior\nattempts to build large logs have not been successful: the lo gs of CiteSeer and DBLP are heavily\nskewed towards titles and authors, and academic groups have been unable to mobilize suﬃciently\nlarge volumes of users to adopt instrumented toolbar and brow ser plugins. Attempts to use\ninstitutional proxy logs have shown that even with tens of tho usands of users, the log is relatively\nsparse.\nWhile eﬃciency does not automatically demand relevance judg ments or similar “quality of\nretrieval” resources, there is a need for at least some level of quality assurance to be provided\nas there is often an eﬃciency / eﬀectiveness trade-oﬀ to be qu antiﬁed. Obtaining access to\nassessments at the required scale may also become a problem. T o date, TREC resources have\ntypically been used, noting that it is acceptable practice t o measure eﬀectiveness using one set of\nqueries and documents, and then throughput using another set .\nACM SIGIR Forum 77 Vol. 52 No. 1 June 2018\n\n9 Personal Information Access\n9.1 Description\nInformation created by, connected to, or consumed by an indiv idual resides across a great number\nof separate information silos: personal devices (laptops, smart phones, watches, etc.); the web;\npersonal or enterprise ﬁle shares; messaging systems and so cial media; and systems from external\nparties including medical doctors, bank records, employer and government records, and many\nothers. Today, in order to search and make sense of this heter ogeneous and disconnected set of\ndata we depend on an equivalently large number of independen t sources and access mechanisms,\nand rely on knowing where something is stored and how to get to it.\nIn what follows we discuss two, mostly orthogonal, challeng es in accessing personal informa-\ntion: retrieval over personal information and personalized retrieval of information. These can be\naddressed separately, but each informs the other.\n9.2 Motivation\nPeople are producing and consuming more and more information . That information is stored in\na multitude of places (cloud, computer, phone) and in a multit ude of formats (e.g. mail, docs,\nslack, twitter, Facebook, apps, web searches, ﬁtbits, sens e cams, etc.). Without an integrated\npersonal IR service, users must resort to multiple interact ions over their own data. This is a\ntime consuming process and prone to error. The time spent wadi ng through and trying to ﬁnd\ninformation in personal repositories results in wasted tim e, and in frustration. In spite of these\nevident shortcomings of current information technologies , little support exists to help people ﬁnd,\nre-ﬁnd, manage, organize, and share their personal informa tion.\n9.3 Proposed Research\nThere are at least four broad research questions which we nee d to address. First, how can\nI ﬁnd stuﬀ that I’ve seen/interacted with before , or should see; eﬃciently, eﬀectively,\nand while preserving privacy? Second, are there abstract representations of content and\naccess patterns which we can share - without violating privacy - to help desig n systems, to train\nmachine learners, or to distribute computation? How can we sa fely generalize what we learn from\none person to another? Third, if we have a rich model of a person , based on personal data and\ninteractions, how can we use this to personalize content or presentation ? When should we?\nWhat should we consider? And ﬁnally, how can we search private information resources\nowned by others , as distinct from searching our own information in other col lections?\n9.4 Research Challenges\nUnderstanding (and Anticipating) Needs. A key challenge is understanding what informa-\ntion needs users have that the system should support. What the se needs are, and how they are\nexpressed, often depend on the device as well as the data. A pe rsonal digital assistant may also\naddress these information needs proactively, e.g., identi fy routing tasks and pulling up all the\nrelated material, like preparing a travel expense declarat ion.\nACM SIGIR Forum 78 Vol. 52 No. 1 June 2018\n\nTask Representation, Identiﬁcation & Abstraction. Once we understand something of\npersonal search tasks, we need to represent access patterns, information needs, and behaviours in\na way that existing systems can use to reason, and researcher s can use to investigate new systems.\nThis would involve extracting tasks from private data; abst racting them to allow insight; and\ndeveloping a common protocol for describing tasks, or classe s of tasks, without violating privacy\nor security.\nIndex and Schema Representation. A similar early research challenge is aggregation and\nrepresentation of heterogeneous data sources and formats. One has to come up with a represen-\ntation generic enough for data that ranges from unstructure d to fairly structured, and sometimes\nwith comprehensive metadata. Eﬀorts on data integration (e.g ., by the database community) and\ncommon vocabularies such as schema.org and Dublin Core (e.g ., by the semantic web community)\nmay be helpful to this end. As a community, we could distinguish better between the logical\nand physical representations of information, and express r etrieval models at the logical level while\ndelegating the actual relevance estimation over heterogen eous information in various data silos to\nthe underlying physical layer.\nOnce the data has been represented in a common format, one has to think about suitable ways\nof querying it and interfaces to expose to users. While a rich q uery language might be helpful as an\nintermediary, it is unlikely to be apt for common users who wo uld rather express their information\nneeds using natural language.\nLinking and Disambiguation across Silos. Once representational issues are addressed, a\nhigher order research challenge is extracting entities fro m the heterogeneous data repositories -\nperforming disambiguation, if required, and then linking e ntities within the collection. A more\ndiﬃcult challenge is linking of entities/objects to partic ular tasks, i.e. ﬁnding all the relevant\nartifacts associated with a given task, or set of tasks. This will enable a personal knowledge graph\nthat contextualizes the relationships in user data.\nOne challenge here is that entities in the personal knowledg e graph are unlikely to be generally\npopular, so that signals commonly used for named entity disam biguation (e.g., popularity and\ncoherence)mayjustnotwork. Alternativesignals(e.g., co- accesspatterns, similarityofusernames\nand email addresses) can potentially serve as replacements.\nRanking and Retrieval. Challenges related to search in personal data include the het erogeneity\nof information access tasks (ranking, summarization, etc. ), of data sources, and of type of inter-\nactions (depending on device and modality). Retrieval meth ods will have to take the diﬀerent\ncharacteristics of the data into account and also make use of metadata (e.g., creation times of\nﬁles). It is foreseeable that the type of query result will dep end on the information need at hand\nand may take the form of a list of ﬁles, bundles of interlinked ﬁles, or even a summary generated\nfrom the contents of relevant ﬁles.\nIn a new opportunity for IR, the searcher in a personal system may also be the author or\ncurator. Their work, e.g. in ﬁling into folders, can inform t he retrieval process.\nComputing over Aggregate Personal Data. Personaldataprovidesaveryrichrepresentation\nof an individual’s content and behavioral interaction patt erns. Aggregating across individuals\ncould augment this allowing generalization to new contexts.\nFor example, user interaction data is an important signal fo r learning-to-rank models in web\nsearch; thesemodelsrequireobservinginteractionsacros smanyusersforthe samequery-document\nACM SIGIR Forum 79 Vol. 52 No. 1 June 2018\n\npair. Thisischallengingwhenconsideringpersonal(andpri vate)data: thedocuments(e.g., emails\nor private ﬁles) are not shared across users, and queries are personal (e.g., “Sam’s email address”)\nand may not generalize well across users.\nPersonalization. As well as searching personal data, an agent tightly tied to a u ser (for example\none running on a phone) can be greatly personalized. It may be p ossible to build per-person\nmodels of reading, cognition, biometrics, eye tracking, fa ce recognition, or emotion. Challenges\nhere include understanding the costs and beneﬁts of such hyp er-personalization, suggesting search\ninterfaces or contents, and classifying users.\nPrivacy, Security, and Trust. A key challenge in PIA research is ensuring that the privacy\nand security of personal data is maintained and that the users trust that the system maintains\ntheir privacy. There is a wealth of work on privacy-preservi ng methods (for example in record\nlinkage, enterprise search, and in masking queries to IR syste ms), but we must also understand\nhow to distinguish private from shared from public data, wha t granularity to work at, which data\nto draw on in which circumstances, and how to explain these rul es to both the subjects and the\nowners of information. Knowing how much information to disc lose, to whom, and under what\ncircumstances, is a tremendous challenge when even the fact o f a query itself may cause harm if\nknown (for example, in medical or patent cases). IR systems w orking with private data may also\nneed to forget (or, forever hide) otherwise-accessible inf ormation under some circumstances, and\nboth policies and mechanisms need to be developed for this.\nArchitecture and Applications. Along the way, there are many problems to solve if we are to\nbuild working tools either for research or general use. The ba sic architectural choices are open:\nwhere should the index live, where should the search happen, how do we aggregate data across\nsilos? If we allow brokers to control access to silos, who get s to know what’s asked of a silo, and\nhow can we route requests without knowing what holdings each broker has? Interface choices\nare also wide open and range from conventional metasearch, th rough PIM tools, intelligent and\nproactive personal assistants, to visualization and explor ation tools to explore the data a person\nhas.\n9.5 Broader Impact\nThe most obvious, immediate, impact is the reduction in the co st of (re-)ﬁnding and (re-)using\none’s own data which exists in many information silos (e.g., is the SWIRL document on my laptop,\non a shared drive (which one?), or in my email (which account?) ). Improved access to personal\ninformation will also reduce the frustration in ﬁnding and t hus reduce the friction in working with\npersonal data. This impacts everyone with a connected devic e - virtually every human on the\nplanet. Improvementsinthisareawillalsotranslatetoﬁnd ingintheenterprise, whereinformation\nworkers spend large amounts of their time (re-)ﬁnding exist ing organizational knowledge within\ntheir own corporate repository and across all the repositor ies within the organization.\n9.6 Broadening SIGIR\nThere is a lot of related work in other research communities, a nd opportunities for collaboration\nas well as starting points for IR research. This includes wor k on dataspaces, the EU NEPOMUK\nACM SIGIR Forum 80 Vol. 52 No. 1 June 2018\n\nproject on semantic infrastructure for desktop retrieval,3and work on semantics and retrieval\nin the lifelogging community, for example in the Lifeloggin g Tools and Applications workshops.\nResearch in privacy-preserving data linkage and data minin g will also be relevant to problems in\nlinking and sharing data.\n9.7 Obstacles and Risks\nPersonal information access, under diﬀerent names, has been an outstanding problem for some\ntime. Why hasn’t the IR community made more progress? There ar e signiﬁcant obstacles to even\nstarting a research programme.\nFirst, we note that the cost of entry is high. There has to be a lo t of working parts to even be\nslightlyuseful, althoughitmaybepossibletostartsmallb yaggregatingafew, related, silos. There\nis substantial engineering required for a minimal working s ystem: to fetch data from diﬀerent silos,\nparse diﬀerent data formats, and monitor user activity. Fur ther, access to the personal data and\ninteraction data of a set of users is required in order to deve lop, test, and debug even a minimal\nworking system.\nSecond, experimental evaluation for approaches in this res earch direction is highly challenging\nand many of our common practices are hard to apply. Informati on needs, for instance, are spe-\nciﬁc to users, entailing that only the user itself can judge t he relevance of results. Likewise, the\nconﬁdentiality of the data impedes creating and sharing tes t collections, limiting the reproducibil-\nity of experimental results. At least initially, case studi es may be the only way to evaluate the\napproaches developed.\nThere are also obvious risks involved with searching among p rivate stores or personalizing\nsearch.\nMost obviously, when operating on personal data and consider ing sharing subsets of it among\nusers, there is an inherent risk of breaching private inform ation. Additionally, making use of\npersonal data for personalization of search results may lea d the user into a “ﬁlter bubble”, showing\nonly results reﬂecting its own opinions. When taking into acc ount only the personal data of a\nsingle user, this could be even more grave than in more tradit ional settings with many users.\nAﬁnalriskisthatthebusinesscase fordevelopinga“uniﬁed ”PIAsystem isunclear, especially\ngiven the diﬃculty of accessing data in the ﬁrst place. Unlike w eb search where the business\ncase is driven around advertising, a PIA system, despite its obvious beneﬁts, may not be worth\nimplementing (without the right business model in place). R esearch into this area is therefore not\nonly fraught with technical and evaluation diﬃculties, but also may not actually lead to a viable\nproduct in the short term. Within particular eco-systems (Go ogle, Microsoft, Apple, etc) there is\nsome integration between services - as each attempts to add v alue to their services, and each works\nwith data it stores or controls - but this will invariably mea n that such research will predominantly\nbe performed within such companies. It will be important to br eak down the problem into speciﬁc\ntasks which researchers can pursue, rather than trying to tac kle the whole system.\n3http://www.semanticdesktop.org\nACM SIGIR Forum 81 Vol. 52 No. 1 June 2018\n\n10 Minor Topics\n10.1 IR for an IoT World\nThis project aims to understand, design, implement, and eva luate an IR system for the Internet of\nThings (IoT) world, i.e., a world in which almost everything is connected and produces/contains\ndata and information.\nMotivation. IoT is a growing ﬁeld, also under the “Industry 4.0” label. New devices and\ntechnologies are going to hit the market soon. Many funding ag encies are devoting a large amount\ntheir funds to IoT-related projects. As a community, IR has th e potential to address issues that\nothercommunities(likemachinelearning, datamining)wil lmostlikelynotaddress: tostudyusers\nand their needs, to better model speciﬁc needs exploiting se nsor data, to devise novel and eﬀective\ninteraction modalities with information, and, overall, to apply results from speciﬁc IR subareas.\nThe IoT situation is not much diﬀerent from two previous wave s that hit the IR ﬁeld: the Web\nand mobile devices. Mobile IR was a successful story in SWIRL 2 012. Papers on Location- and\nContext-awareness for Mobile IR are now being published in IR venues. IoT might well be the\nnext wave after Web and Mobile.\nProposal. We foresee three main research directions:\n(1) Novel data and collections. Since IoT sensors and devices co ntain and share data, new no-\ntionsofcollections, documents, and GIOsarelikelytoaris e. Therewill bediﬀerentprotocols\nand formats, and a mixture of structured and unstructured da ta. Typical collections would\ncontain for example, Lifelog data, sport tracking data, per sonal car and driving habits data,\nbut more exotic scenarios might arise, like Thing retrieval (where it is not information but\nsomething in the real world that is retrieved). Eﬃciency wil l be very important (IoT devices\nneed to be low energy) and all the research done in distribute d/federated IR will need to be\nboth taken into account and extended.\n(2) New interaction modalities, for both information access a nd presentation. Because of the\nproliferation of sensors, context-aware IR systems (a.k.a . zero query) will have much more\ndata available and exploitable to better model user needs. C onversely, the new devices will\nallow to present information in novel ways, using not only va rious devices (e.g., glasses,\naugmented objects, information augmented reality) but also m odalities (e.g., Map- or Geo-\ncentric presentation modalities like www.thingful.net). This will open the possibility for\ninformation access and presentation to be combined by having users seamlessly browsing the\ninformation space by moving in the physical space, thus real izing the vision of a disappearing\n/ ubiquitous IR system allowing to situate information in im mersive spaces.\n(3) Understanding users and needs. It is unclear, and worth stu dying, if current user needs will\nsimply “scale-up” to the new collections and modalities, or if radically diﬀerent needs will\narise. Theopportunityforthisnewwaveofinformationacces stobeinclusiveandsupportive\nof neurodiversity will need to be supported by a wide range of user studies.\nChallenges. The main challenge will be to integrate the “syntactic” leve l of rough sensor data\nwithin a broader concept of information. This research will move away from search engines that\nACM SIGIR Forum 82 Vol. 52 No. 1 June 2018\n\nﬁnd IoT devices ( https://censys.io/ ,https://www.shodan.io/ ), towards search engines that\nallow users to satisfy their needs also using information co ntained/produced by IoT devices and\nsensors. We are not interested in retrieving a “thing” via it s name or other keywords speciﬁc\nto that thing, nor most sensor values of interest to data mine rs such as temperature, velocity,\netc., but likely on its relevance/usefulness to current nee ds of the user (e.g., GPS coordinates).\nOnly a deep understanding of the nature of “social sensors”, can provide richer ways of deriving\ncontextual information from the activities of the users on soc ial media (e.g., Foursquare Check-ins\nvs. Mobile phone position data).\nRelated eﬀorts. There are some recent research trends in the IR community, li ke Context-\nand Location-aware retrieval, Geographic IR, cross-device search, that will likely be exploited and\nadapted to a higher level. The research will mutually beneﬁt a ll other domains of IR: eﬃciency,\nconversational, interactive, GIOs, etc. Privacywillofco ursebeaprimaryconcern. Finally, related\nﬁelds like ML, data mining, Ubicomp, HCI, will likely address s imilar issues.\n10.2 Impact of IR Systems in Society\nIR systems and tools intermediate most of the information co nsumed today, be it top sources on a\ngiven topic or the top news stories everyday. This project is c oncerned with assessing both short\nand long term impacts of the IR artifacts that our IR communit y has developed and studied over\ntime.\nMotivation. For over two decades, IR systems have inﬂuenced the way peopl e around the world\nwork, communicate, learn, and even how they live. Search eng ines have eased the way we access\ninformation. Recommender systems have changed the way we se lect what products and services\nwe buy and consume. Social networks have changed how we keep i n touch with family, friends,\nand acquaintances. Personal and conversational assistant s are increasingly supporting us in our\nday-by-day tasks through reminders or contextual interven tions such as heads-ups about traﬃc\nor weather. In essence, IR systems aim to empower individual s through access to information.\nHowever, do these systems always deliver positive outcomes to individuals, society, politics, the\neconomy, and the environment? Information scientists with researchers from other disciplines\nshould study the long-term and large-scale impacts of IR syst ems and technologies. Previous\nresearch indicates some of the areas that IR technologies im pact, including:\n•Human cognitive processes. Psychologistshavebeenstudyingandraisedconcernsabout\nthe eﬀect that easy access to via search engines might be havi ng on, e.g., how people think\nand what people remember.\n•Individuals from minority communities. Latanya Sweeney showed that querying by\nnames predominantly used by black Americans is more likely to return results associated to\narrest records than when querying by names predominantly us ed by white Americans. How\nmany people might have been denied employment as a result? Simil arly, search results for\nimages of doctors or engineers are typically dominated by pi ctures of white men. How many\ngirls of color might have been discouraged to pursue interes ts in these ﬁelds?\n•Social Communities. Given the political climate around the world, many have rais ed\nconcerns about the potential of highly personalized consum ption of information and the\nACM SIGIR Forum 83 Vol. 52 No. 1 June 2018\n\nﬁlter bubbles that current IR systems can create, to drive in creased polarization along social\nand political lines within and across local and regional com munities.\n•Businesses. Search platforms have disrupted the news media ecosystem thr ough digital ads\nand targeting, being able to tailor content to each of their u ser interests, viewpoints, and\nbeliefs. But, what are the costs of, for instance, accessing newsworthy information without\nthe traditional journalistic curation?\n•Environment. Providing the energy required to run the data centres that su pport large-\nscale search engines can be at a signiﬁcant cost to the environ ment but IR technology can\nalso contribute to smarter more eﬃcient infrastructure for cities and transportation.\nProposal. While there is some consensus around the broad inﬂuence of IR, we often lack hard\nnumbers, and more eﬀorts should made towards identifying an d quantifying speciﬁc (both positive\nand negative) outcomes of IR systems on society, beyond the sh ort term goals of their users or\ntheir clients. Research directions include:\n•What is the long-term impact of the ubiquitous access to infor mation (anywhere, anytime)\nvia web search engines on how people think, how people learn, and what they remember?\n•How to educate developers of IR algorithms to avoid unfair bia s that may have negative\nconsequences for some individuals?\n•Does widespread access to information improve or damage soci al cohesion?\n•What is the economic impact of various developments in inform ation retrieval?\n•Can IR technologies have a net positive impact on the environm ent?\nChallenges. Multidisciplinary research that needs to involve other dis ciplines such as social\nsciences, media studies, environmental science, politica l science, psychology, socio-psychology,\npsycho-sociology, economics, and maybe even ethnography a nd anthropology. What is an appro-\npriate framework to evaluate the long term impact of develop ments in IR? How to isolate the\neﬀect of IR systems from other informational or societal-re lated factors?\nRelated eﬀorts. There is a growing number of eﬀorts in neighboring ﬁelds looki ng either broadly\nat the impact of computing systems, or at that of speciﬁc ML or AI tools, including prominent\ninitiatives such as dedicated research institutes, e.g., D ata & Society and AI Now; as well as\nworkshopsandconferences,e.g.,ConferenceonFairness,Ac countability,andTransparency(FAT*)\nor AAAI/ACM Conference on AI, Ethics, and Society (AIES). These ar e also related with existing\neﬀorts in the space of FACT IR, discussed earlier in this repo rt.\n11 Conclusion\nInformation Retrieval remains a vital and active area of rese arch in both academia and industry.\nSatisfying people’s information needs is a fundamental, mu lti-disciplinary problem, and this re-\nports captures the many important research themes in this imp ortant research area. The ﬁndings\nACM SIGIR Forum 84 Vol. 52 No. 1 June 2018\n\nare in no way prescriptive. That is, many more important rese arch themes were suggested than\ncould be explored in our brief time in Lorne. We hope that SWIRL can inspire future strategic\nworkshops which continue to shape and grow the Information Re trieval research community.\nDeep Learning\n[DZS+17] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kam ps, and W. Bruce\nCroft. Neural ranking models with weak supervision. In Proceedings of the 40th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval , SIGIR ’17, pages 65–74, New York, NY, USA, 2017. ACM.\n[HKG+15] Karl Moritz Hermann, Tom´ aˇ s Koˇ cisk´ y, Edward Grefenst ette, Lasse Espeholt, Will\nKay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and com-\nprehend. In Proceedings of the 28th International Conference on Neural Information\nProcessing Systems - Volume 1 , NIPS’15, pages 1693–1701, Cambridge, MA, USA,\n2015. MIT Press.\n[HXTS16] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, a nd Richard Socher. A\njoint many-task model: Growing a neural network for multiple NLP tasks. CoRR,\nabs/1611.01587, 2016.\n[MC18] Bhaskar Mitra and Nick Craswell. An introduction to neural information retrieval .\nFoundations and Trends in Information Retrieval. Now Publis hers Inc., 2018.\n[MCCD13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey De an. Eﬃcient estimation of\nword representations in vector space. CoRR, abs/1301.3781, 2013.\n[MDB17] G´ abor Melis, Chris Dyer, and Phil Blunsom. On the st ate of the art of evaluation in\nneural language models. CoRR, abs/1707.05589, 2017.\n[MDC17] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. Lea rning to match using local\nand distributed representations of text for web search. In Proceedings of the 26th\nInternational Conference on World Wide Web , WWW ’17, pages 1291–1299, Republic\nand Canton of Geneva, Switzerland, 2017. International Wor ld Wide Web Conferences\nSteering Committee.\n[MSC+13] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeﬀ Dean. Distributed\nrepresentations of words and phrases and their composition ality. In C.J.C. Burges,\nL. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, edi tors,Advances in\nNeural Information Processing Systems 26 , pages 3111–3119. Curran Associates, Inc.,\n2013.\n[SS17] Ryan Spring and Anshumali Shrivastava. Scalable and s ustainable deep learning via\nrandomized hashing. In Proceedings of the 23rd ACM SIGKDD International Confer-\nence on Knowledge Discovery and Data Mining , KDD ’17, pages 445–454, New York,\nNY, USA, 2017. ACM.\nACM SIGIR Forum 85 Vol. 52 No. 1 June 2018\n\n[SSWF15] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and R ob Fergus. End-to-end\nmemory networks. In Proceedings of the 28th International Conference on Neural\nInformation Processing Systems - Volume 2 , NIPS’15, pages 2440–2448, Cambridge,\nMA, USA, 2015. MIT Press.\n[SWM17] Wojciech Samek, Thomas Wiegand, and Klaus-Robert M¨ u ller. Explainable artiﬁcial\nintelligence: Understanding, visualizingandinterpretin gdeeplearningmodels. CoRR,\nabs/1708.08296, 2017.\n[WCB14] Jason Weston, Sumit Chopra, and Antoine Bordes. Memor y networks. CoRR,\nabs/1410.3916, 2014.\n[XCL17] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. Word-en tity duet representations for\ndocument ranking. In Proceedings of the 40th International ACM SIGIR Conference\non Research and Development in Information Retrieval , SIGIR ’17, pages 763–772,\nNew York, NY, USA, 2017. ACM.\n[XDC+17] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and R ussell Power. End-to-\nendneuralad-hocrankingwithkernelpooling. In Proceedings of the 40th International\nACM SIGIR Conference on Research and Development in Information Retrieval , SI-\nGIR ’17, pages 55–64, New York, NY, USA, 2017. ACM.\n[XMS16] Caiming Xiong, Stephen Merity, and Richard Socher. Dyn amic memory networks\nfor visual and textual question answering. In Proceedings of the 33rd International\nConference on International Conference on Machine Learning - Volume 48 , ICML’16,\npages 2397–2406. JMLR.org, 2016.\nStateful Search\n[CRH16] Konstantina Christakopoulou, Filip Radlinski, and Ka tja Hofmann. Towards con-\nversational recommender systems. In Proceedings of the 22Nd ACM SIGKDD In-\nternational Conference on Knowledge Discovery and Data Mining , KDD ’16, pages\n815–824, New York, NY, USA, 2016. ACM.\n[Guy16] Ido Guy. Searching by talking: Analysis of voice quer ies on mobile web search.\nInProceedings of the 39th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval , SIGIR ’16, pages 35–44, New York, NY, USA,\n2016. ACM.\n[HTML12] BrentHecht, JaimeTeevan, MeredithRingeMorris, and DanLiebling. Searchbuddies:\nBringing search engines into the conversation. In ICWSM 2012 - Proceedings of the\n6th International AAAI Conference on Weblogs and Social Media , pages 138–145, 12\n2012.\n[JHAJ+15] Jiepu Jiang, Ahmed Hassan Awadallah, Rosie Jones, Umut Ozertem, Imed Zitouni,\nRanjitha Gurunath Kulkarni, and Omar Zia Khan. Automatic onl ine evaluation of\nACM SIGIR Forum 86 Vol. 52 No. 1 June 2018\n\nintelligent assistants. In Proceedings of the 24th International Conference on World\nWide Web , WWW ’15, pages 506–516, Republic and Canton of Geneva, Switzer land,\n2015. International World Wide Web Conferences Steering Com mittee.\n[LMR+16] Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Gall ey, and Jianfeng Gao.\nDeep reinforcement learning for dialogue generation. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language Processing , pages 1192–1202. As-\nsociation for Computational Linguistics, 2016.\n[RC17] Filip Radlinski and Nick Craswell. A theoretical fram ework for conversational search.\nInProceedings of the 2017 Conference on Conference Human Information Interaction\nand Retrieval , CHIIR ’17, pages 117–126, New York, NY, USA, 2017. ACM.\n[SSB+16] Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Co urville, and Joelle\nPineau. Building end-to-end dialogue systems using genera tive hierarchical neural\nnetwork models. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intel-\nligence, AAAI’16, pages 3776–3783. AAAI Press, 2016.\n[VSAC17] Alexandra Vtyurina, Denis Savenkov, Eugene Agichtein , and Charles L. A. Clarke.\nExploring conversational search with humans, assistants, and wizards. In Proceedings\nof the 2017 CHI Conference Extended Abstracts on Human Factors in Computing\nSystems, CHI EA ’17, pages 2187–2193, New York, NY, USA, 2017. ACM.\n[WVM+17] Tsung-Hsien Wen, David Vandyke, Nikola Mrkˇ si´ c, Milica G asic, Lina M. Rojas Bara-\nhona, Pei-Hao Su, Stefan Ultes, and Steve Young. A network-bas ed end-to-end train-\nable task-oriented dialogue system. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computational Linguistics: Volume 1, Long\nPapers, pages 438–449. Association for Computational Linguistics , 2017.\n[YSW16] Rui Yan, Yiping Song, and Hua Wu. Learning to respond with deep neural networks\nfor retrieval-based human-computer conversation system. I nProceedings of the 39th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval , SIGIR ’16, pages 55–64, New York, NY, USA, 2016. ACM.\nResponsible Information Retrieval\n[BCZ+16] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Sa ligrama, and Adam T\nKalai. Man is to computer programmer as woman is to homemaker ? debiasing word\nembeddings. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, a nd R. Garnett,\neditors,Advances in Neural Information Processing Systems 29 , pages 4349–4357.\nCurran Associates, Inc., 2016.\n[BS16] Solon Barocas and Andrew D. Selbst. Big data’s dispara te impact. California Law\nReview, 104, 2016.\nACM SIGIR Forum 87 Vol. 52 No. 1 June 2018\n\n[CBN17] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Sem antics derived automat-\nically from language corpora contain human-like biases. Science, 356(6334):183–186,\nApril 2017.\n[DTD15] Amit Datta, Michael Carl Tschantz, and Anupam Datta. Au tomated experiments on\nad privacy settings. PoPETs, 2015(1):92–112, 2015.\n[Eic18] Carsten Eickhoﬀ. Cognitive biases in crowdsourcin g. InProceedings of the Eleventh\nACM International Conference on Web Search and Data Mining , WSDM ’18, pages\n162–170, New York, NY, USA, 2018. ACM.\n[JKB+15] Lilli Japec, Frauke Kreuter, Marcus Berg, Paul Biemer, Pa ul Decker, Cliﬀ Lampe,\nJulia Lane, Cathy O’Neil, and Abe Usher. Big data in survey resear chaapor task force\nreport.Public Opinion Quarterly , 79(4):839–880, 2015.\n[MSA+17] Rishabh Mehrotra, Amit Sharma, Ashton Anderson, Fernando D iaz, Hanna Wallach,\nand Emine Yilmaz. Auditing search engines for diﬀerential sat isfaction across demo-\ngraphics. In Proceedings of the 26th International Conference on World Wide Web ,\n2017.\n[OBC17] Jahna Otterbacher, Jo Bates, and Paul Clough. Competen t men and warm women:\nGender stereotypes and backlash in image search results. In Proceedings of the 2017\nCHI Conference on Human Factors in Computing Systems , CHI ’17, pages 6620–6631,\nNew York, NY, USA, 2017. ACM.\n[OCDK17] Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. Social data:\nBiases, methodological pitfalls, and ethical boundaries. Available at SSRN, 2017.\nEvaluation\n[ABSJ17] Aman Agarwal, Soumya Basu, Tobias Schnabel, and Thorste n Joachims. Eﬀective\nevaluation using logged bandit feedback from multiple logg ers. InProceedings of\nthe 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, KDD ’17, pages 687–696, New York, NY, USA, 2017. ACM.\n[BPnC+13] L´ eon Bottou, Jonas Peters, Joaquin Qui nonero Candela, De nis X. Charles, D. Max\nChickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counter-\nfactual reasoning and learning systems: The example of comp utational advertising.\nJournal of Machine Learning Research , 14:3207–3260, 2013.\n[Car12] Ben Carterette. Multiple testing in statistical ana lysis of systems-based information\nretrieval experiments. ACM Trans. Inf. Syst. , 30(1):4:1–4:34, March 2012.\n[Car15a] BenCarterette. Bayesianinferenceforinformati onretrievalevaluation. In Proceedings\nof the 2015 International Conference on The Theory of Information Retrieval , ICTIR\n’15, pages 31–40, New York, NY, USA, 2015. ACM.\nACM SIGIR Forum 88 Vol. 52 No. 1 June 2018\n\n[Car15b] Ben Carterette. The best published result is rando m: Sequential testing and its\neﬀect on reported eﬀectiveness. In Proceedings of the 38th International ACM SIGIR\nConference on Research and Development in Information Retrieval , SIGIR ’15, pages\n747–750, New York, NY, USA, 2015. ACM.\n[JSS17] Thorsten Joachims, Adith Swaminathan, and Tobias Schn abel. Unbiased learning-to-\nrankwithbiasedfeedback. In Proceedings of the Tenth ACM International Conference\non Web Search and Data Mining , WSDM ’17, pages 781–789, New York, NY, USA,\n2017. ACM.\n[MBST17] Alistair Moﬀat, Peter Bailey, Falk Scholer, and Pau l Thomas. Incorporating user\nexpectations and behavior into the measurement of search eﬀe ctiveness. ACM Trans-\nactions on Information Systems (TOIS) , 35(3):24, 2017.\n[MZ08] Alistair Moﬀat and Justin Zobel. Rank-biased precisio n for measurement of retrieval\neﬀectiveness. ACM Transactions on Information Systems (TOIS) , 27(1):2, 2008.\n[Ope15] Open Science Collaboration. Estimating the reprod ucibility of psychological science.\nScience, 349(6251), 2015.\n[Sak16a] Tetsuya Sakai. Statistical signiﬁcance, power, an d sample sizes: A systematic review\nof sigir and tois, 2006-2015. In Proceedings of the 39th International ACM SIGIR\nConference on Research and Development in Information Retrieval , SIGIR ’16, pages\n5–14, New York, NY, USA, 2016. ACM.\n[Sak16b] Tetsuya Sakai. Topic set size design. Inf. Retr. , 19(3):256–283, June 2016.\n[SKA+17] AdithSwaminathan,AkshayKrishnamurthy,AlekhAgarwal,Mi roDudik,JohnLang-\nford, Damien Jose, and Imed Zitouni. Oﬀ-policy evaluation fo r slate recommendation.\nIn I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vis hwanathan, and\nR. Garnett, editors, Advances in Neural Information Processing Systems 30 , pages\n3632–3642. Curran Associates, Inc., 2017.\n[VSS17] Ellen M. Voorhees, Daniel Samarov, and Ian Soboroﬀ. Usi ng replicates in information\nretrieval evaluation. ACM Trans. Inf. Syst. , 36(2):12:1–12:21, August 2017.\n[WBMN16] Xuanhui Wang, Michael Bendersky, Donald Metzler, and Marc Najork. Learning to\nrank with selection bias in personal search. In Proceedings of the 39th International\nACM SIGIR Conference on Research and Development in Information Retrieval , SI-\nGIR ’16, pages 115–124, New York, NY, USA, 2016. ACM.\nEﬃciency\n[BCR16] Xiao Bai, B. Barla Cambazoglu, and Archie Russell. Imp roved caching techniques\nfor large-scale image hosting services. In Proceedings of the 39th International ACM\nSIGIR Conference on Research and Development in Information Retrieval , SIGIR ’16,\npages 639–648, New York, NY, USA, 2016. ACM.\nACM SIGIR Forum 89 Vol. 52 No. 1 June 2018\n\n[BNMN16] Leonid Boytsov, David Novak, Yury Malkov, and Eric Nyberg . Oﬀ the beaten path:\nLet’s replace term-based retrieval with k-nn search. In Proceedings of the 25th ACM\nInternational on Conference on Information and Knowledge Management ,pages1099–\n1108. ACM, 2016.\n[CGBC17] Ruey-Cheng Chen, Luke Gallagher, Roi Blanco, and J Sh ane Culpepper. Eﬃcient\ncost-aware cascade ranking in multi-stage retrieval. In Proceedings of the 40th In-\nternational ACM SIGIR Conference on Research and Development in Information\nRetrieval , pages 445–454. ACM, 2017.\n[GHL+17] Bob Goodwin, Michael Hopcroft, Dan Luu, Alex Clemmer, Mihae la Curmei, Sameh\nElnikety, and Yuxiong He. Bitfunnel: Revisiting signatures for search. In Proceedings\nof the 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval , SIGIR ’17, pages 605–614, New York, NY, USA, 2017. ACM.\n[KBC+17] Tim Kraska, Alex Beutel, Ed H. Chi, Jeﬀ Dean, and Neoklis Polyz otis. The case for\nlearned index structures. 2017. ArXiV – https://arxiv.org/abs/1712.01208 .\n[LNO+15] Claudio Lucchese, Franco Maria Nardini, Salvatore Orland o, Raﬀaele Perego, Nicola\nTonellotto, and Rossano Venturini. Quickscorer: A fast algo rithm to rank documents\nwith additive ensembles of regression trees. In Proceedings of the 38th International\nACM SIGIR Conference on Research and Development in Information Retrieval , SI-\nGIR ’15, pages 73–82, New York, NY, USA, 2015. ACM.\n[OV14] Giuseppe Ottaviano and Rossano Venturini. Partitione d elias-fano indexes. In Pro-\nceedings of the 37th International ACM SIGIR Conference on Research &#38; De-\nvelopment in Information Retrieval , SIGIR ’14, pages 273–282, New York, NY, USA,\n2014. ACM.\nACM SIGIR Forum 90 Vol. 52 No. 1 June 2018",
  "textLength": 179051
}