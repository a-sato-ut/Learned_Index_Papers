{
  "paperId": "2555cfe0f77d55cc0d887da00fd58857d0c6edd5",
  "title": "Conformer-Kernel with Query Term Independence for Document Retrieval",
  "pdfPath": "2555cfe0f77d55cc0d887da00fd58857d0c6edd5.pdf",
  "text": "CONFORMER -KERNEL WITH QUERY TERM INDEPENDENCE FOR\nDOCUMENT RETRIEVAL\nBhaskar Mitra\nMicrosoft, UCL\nbmitra@microsoft.comSebastian Hofstätter\nTU Wien\ns.hofstatter@tuwien.ac.atHamed Zamani and Nick Craswell\nMicrosoft\n{hazamani, nickcr }@microsoft.com\nABSTRACT\nThe Transformer-Kernel (TK) model has demonstrated strong reranking performance on the TREC Deep Learning\nbenchmark—and can be considered to be an efﬁcient (but slightly less effective) alternative to BERT-based ranking\nmodels. In this work, we extend the TK architecture to the full retrieval setting by incorporating the query term\nindependence assumption. Furthermore, to reduce the memory complexity of the Transformer layers with respect to the\ninput sequence length, we propose a new Conformer layer. We show that the Conformer’s GPU memory requirement\nscales linearly with input sequence length, making it a more viable option when ranking long documents. Finally,\nwe demonstrate that incorporating explicit term matching signal into the model can be particularly useful in the full\nretrieval setting. We present preliminary results from our work in this paper.\nKeywords Deep learning\u0001Neural information retrieval \u0001Ad-hoc retrieval\n1 Introduction\nIn the inaugural year of the TREC Deep Learning track [Craswell et al., 2019], Transformer-based [Vaswani et al.,\n2017] ranking models demonstrated substantial improvements over traditional information retrieval (IR) methods.\nSeveral of these approaches— e.g., [Yilmaz et al., 2019, Yan et al., 2019]—employ BERT [Devlin et al., 2018], with\nlarge-scale pretraining, as their core architecture. Diverging from the trend of BERT-scale models, Hofstätter et al.\n[2020b] propose the Transformer-Kernel (TK) model with few key distinctions: (i) TK uses a shallower model with\nonly two Transformer layers, (ii) The parameters of the model are randomly initialized prior to training (skipping the\ncomputation-intensive pretraining step), and ﬁnally (iii) TK encodes the query and the document independently of\neach other allowing for ofﬂine precomputations for faster response times. Consequently, TK achieves competitive\nperformance at a fraction of the training and inference cost of its BERT-based peers.\nNotwithstanding these efﬁciency gains, the TK model shares two critical drawbacks with other Transformer-based\nmodels. Firstly, the memory complexity of the self-attention layers is quadratic O(n2)with respect to the length nof\nthe input sequence. This restricts the number of document terms that the model can inspect under ﬁxed GPU memory\nbudget. A trivial workaround involves inspecting only the ﬁrst kterms of the document. This approach can not only\nnegatively impact retrieval quality, but has been shown to speciﬁcally under-retrieve longer documents [Hofstätter\net al., 2020a]. Secondly, in any real IR system, it is impractical to evaluate every document in the collection for every\nquery—and therefore these systems typically either enforce some sparsity property to drastically narrow down the set\nof documents that should be evaluated or ﬁnd ways to prioritize the candidates for evaluation. TK employs a nonlinear\nmatching function over query-document pairs. Therefore, it is not obvious how the TK function can be directly used to\nretrieve from the full collection without exhaustively comparing every document to the query. This restricts TK’s scope\nof application to late stage reranking of smaller candidate sets that may have been identiﬁed by simpler retrieval models.\nIn this work, we extend the TK architecture to enable direct retrieval from the full collection of documents. Towards\nthat goal, we incorporate three speciﬁc changes:\n1.To scale to long document text, we replace each instance of the Transformer layer with a novel Conformer\nlayer whose memory complexity is O(n\u0002dkey), instead ofO(n2).arXiv:2007.10434v1  [cs.IR]  20 Jul 2020\n\n2.To enable fast retrieval with TK, we incorporate the query term independence assumption [Mitra et al., 2019]\ninto the architecture.\n3.And ﬁnally, as Mitra et al. [2016, 2017] point out, lexical term matching can complement latent matching\nmodels, and the combination can be particularly effective when retrieving from the full collection of candidates.\nSo, we extend TK with an explicit term matching submodel to minimize the impact of false positive matches\nin the latent space.\nWe describe the full model and present preliminary results from our work in this paper.\n2 Related work\n2.1 Scaling self-attention to long text\nThe self-attention layer, as proposed by Vaswani et al. [2017], can be described as follows:\nSelf-Attention (Q;K;V ) = \b(QK|\npdk)\u0001V (1)\nWhere,Q2Rn\u0002dkey,K2Rn\u0002dkey, andV2Rn\u0002dvalueare the query, key, and value matrices—and dkeyanddvalueare\nthe dimensions of the key and value embeddings, respectively, and nis the length of the input sequence. We use \bto\ndenote the softmax operation applied along the last dimension of the input matrix.\nThe quadraticO(n2)memory complexity of self-attention is a direct consequence of the component QK|that produces\na matrix of size n\u0002n. Recently, an increasing number of different approaches have been proposed in the literature to get\naround this quadratic complexity. Broadly speaking, most of these approaches can be classiﬁed as either: (i) Restricting\nself-attention to smaller windows over the input sequence which results in a memory complexity of O(n\u0002m), where\nmis the window size— e.g., [Parmar et al., 2018, Dai et al., 2019, Yang et al., 2019, Sukhbaatar et al., 2019], or\n(ii) Operating under the assumption that the attention matrix is low rank rand hence ﬁnding alternatives to explicitly\ncomputing the QK|matrix to achieve a complexity of O(n\u0002r)—e.g., [Kitaev et al., 2019, Roy et al., 2020, Tay\net al., 2020, Wang et al., 2020], or (iii) A hybrid of both approaches— e.g., [Child et al., 2019, Beltagy et al., 2020, Wu\net al., 2020]. In the IR literature, recently Hofstätter et al. [2020a] have extended the TK model to longer text using the\nlocal window-based attention approach. Other more general approaches to reducing the memory footprint of very deep\nmodels, such as model parallelization have also been extended to Transformer models [Shoeybi et al., 2019]. For more\ngeneral primer on self-attention and Transformer architectures, we point the reader to Weng [2018, 2020].\n2.2 Full retrieval with deep models\nEfﬁcient retrieval using complex machine learned relevance functions is an important challenge in neural IR [Mitra and\nCraswell, 2018, Guo et al., 2019]. One family of approaches involves the dual encoder architecture where the query\nand document are encoded independently of each other, and efﬁcient retrieval is achieved using approximate nearest-\nneighbour search [Lee et al., 2019, Chang et al., 2020, Karpukhin et al., 2020, Ahmad et al., 2019, Khattab and Zaharia,\n2020] or by employing other data structures, such as learning an inverted index based on latent representations [Zamani\net al., 2018]. Precise matching of terms or concepts may be difﬁcult using query-independent latent document\nrepresentations [Luan et al., 2020], and therefore these models are often combined with explicit term matching\nmethods [Nalisnick et al., 2016, Mitra et al., 2017]. Xiong et al. [2020] have recently demonstrated that the training\ndata distribution can also signiﬁcantly inﬂuence the performance of dual encoder models under the full retrieval setting.\nAuxilliary optimization objectives can also help guide the training of latent matching models to ﬁnd solutions that\nemphasize more precise matching of terms and concepts [Rosset et al., 2019].\nAn alternative approach assumes query term independence (QTI) in the design of the neural ranking model [Mitra et al.,\n2019]. For these family of models, the estimated relevance score Sq;dis factorized as a sum of the estimated relevance\nof the document to each individual query term.\nSq;d=X\nt2qst;d (2)\nReaders should note that the QTI assumption is already baked into several classical IR models, like BM25 [Robertson\net al., 2009]. Relevance models with QTI assumption can be used to precompute all term-document scores ofﬂine. The\nprecomputed scores can be subsequently leveraged for efﬁcient search using inverted-index data structures.\n2\n\nSeveral recent neural IR models [Mitra et al., 2019, Dai and Callan, 2019b,a, Mackenzie et al., 2020, Dai and Callan,\nMacAvaney et al., 2020] that incorporate the QTI assumption have obtained promising results under the full retrieval\nsetting. Document expansion based methods [Nogueira et al., 2019b,a], using large neural language models, can also be\nclassiﬁed as part of this family of approaches, assuming the subsequent retrieval step employs a traditional QTI model\nlike BM25. In all of these approaches, the focus of the machine learned function is to estimate the impact score of the\ndocument with respect to individual terms in the vocabulary, which can be precomputed ofﬂine during index creation.\nAn obvious alternative to document expansion based methods is to use the neural model to reformulate the\nquery [Nogueira and Cho, 2017, Van Gysel et al., 2017, Ma et al., 2020]—although these approaches have not\nyet demonstrated retrieval performance that can be considered competitive to other methods considered here.\nFinally, when the relevance of items are known, or a reliable proxy metric exists, machine learned policies [Kraska\net al., 2018, Oosterhuis et al., 2018, Rosset et al., 2018] can also be effective for efﬁcient search over indexes but these\nmethods are not directly relevant to our current discussion.\n3 Conformer-Kernel with QTI\nWe begin by brieﬂy describing the original TK model as outlined in Fig 1a. The initial word embedding layer in TK\nmaps both query and document to their respective sequences of term embeddings. These sequences are then passed\nthrough one or more stacked Transformer layers to derive contextualized vector representations for query and document\nterms. The learnable parameters of both query and document encoders are shared—which includes the initial term\nembeddings as well as the Transformer layers. Based on the contextualized term embeddings, TK creates an interaction\nmatrixX, such thatXijis the cosine similarity between the contextualized embeddings of the ithquery termqiand the\njthdocument term dj.\nXij=cos(~ vqi; ~ vdj) (3)\nThe Kernel-Pooling stage then creates kdistinct features per query term as follows:\nKik=logjdjX\njexp (\u0000(Xij\u0000\u0016k)2\n2\u001b2) (4)\nFinally, the query-document relevance is estimated by a nonlinear function—typically implemented as stacked feedfor-\nward layers—over these features. Next, we describe the proposed changes to this base architecture.\n3.1 Conformer\nIn Section 2.1, we note that the quadratic memory complexity of the self-attention layers w.r.t. the length of the input\nsequence is a direct result of explicitly computing the attention matrix QK|2Rn\u0002n. In this work, we propose a new\nseparable self-attention layer that allows us to avoid instantiating the full term-term attention matrix as follows:\nSeparable-Self-Attention (Q;K;V ) = \b(Q)\u0001A (5)\nwhere,A= \b(K|)\u0001V (6)\nAs previously, \bdenotes the softmax operation along the last dimension of the input matrix. Note that, however, in this\nseparable self-attention mechanism, the softmax operation is employed twice: (i) \b(Q)computes the softmax along the\ndkeydimension, and (ii) \b(K|)computes the softmax along the ndimension. By computing A2Rdkey\u0002dvalueﬁrst, we\navoid explicitly computing the full term-term attention matrix. The memory complexity of the separable self-attention\nlayer isO(n\u0002dkey), which is a signiﬁcant improvement when dkey\u001cn.\nWe modify the standard Transformer block as follows:\n1. We replace the standard self-attention layer with the more memory efﬁcient separable self-attention layer.\n2.Furthermore, we apply grouped convolution before the separable self-attention layers to better capture the\nlocal context based on the window of neighbouring terms.\n3\n\nd1 d2 d3 d4 d5EmbedStacked T ransformers\nq1 q2 q3Stacked T ransformers\nAggregator with Kernel -Pooling\nEmbed Embed Embed Embed\nEmbed Embed Embed(a) Transformer-Kernel (TK)\nd1 d2 d3 d4 d5Stacked Conformers\nq1 q2 q3Aggregator with Windowed Kernel -Pooling\nAggregator with Windowed Kernel -Pooling\nAggregator with Windowed Kernel -Pooling+\nEmbed Embed Embed Embed Embed\nEmbed Embed Embed\n(b) Conformer-Kernel (CK) with QTI\nFigure 1: A comparison of the TK and the proposed CK-with-QTI architectures. In addition to replacing the Transformer\nlayers with Conformers, the latter also simpliﬁes the query encoding to non-contextualized term embedding lookup and\nincorporates a windowed Kernel-Pooling based aggregation that is employed independently per query term.\n4\n\nWe refer to this combination of grouped convolution and Trans former with separable self-attention as a Conformer.\nWe incorporate the Conformer layer into TK as a direct replacement for the existing Transformer layers and name the\nnew architecture as a Conformer-Kernel (CK) model. In relation to handling long input sequences, we also replace the\nstandard Kernel-Pooling with windowed Kernel-Pooling [Hofstätter et al., 2020a] in our proposed architecture.\n3.2 Query term independence assumption\nTo incorporate the QTI assumption into TK, we make a couple of simple modiﬁcations to the original architecture.\nFirstly, we simplify the query encoder by getting rid of all the Transformer layers and only considering the non-\ncontextualized embeddings for the query terms. Secondly, instead of applying the aggregation function over the full\ninteraction matrix, we apply it to each row of the matrix individually, which corresponds to individual query terms. The\nscalar outputs from the aggregation function are linearly combined to produce the ﬁnal query-document score. These\nproposed changes are shown in Fig 1b.\n3.3 Explicit term matching\nWe adopt the Duet [Nanni et al., 2017, Mitra and Craswell, 2019b,a] framework wherein the term-document score is a\nlinear combination of outputs from a latent and and an explicit matching models.\nst;d=w1\u0001BN(s(latent)\nt;d) +w2\u0001BN(s(explicit)\nt;d) +b (7)\nWhere,fw1;w2;bgare learnable parameters and BN denotes the BatchNorm operation [Ioffe and Szegedy, 2015].\nBN(x) =x\u0000E[x]p\nVar[x](8)\nWe employ the CK model to compute s(latent)\nt;dand deﬁne a new lexical matching function modeled on BM25 for s(explicit)\nt;d.\ns(explicit)\nt;d=IDFt\u0001BS(TFt;d)\nBS(TFt;d) +ReLU (wdlen\u0001BS(jdj) +bdlen) +\u000f(9)\nWhere, IDFt,TFt;d, andjdjdenote the inverse-document frequency of the term t, the term-frequency of tin document d,\nand the length of the document, respectively. The wdlenandbdlenare the only two leanrable parameters of this submodel\nand\u000fis a small constant added to prevent a divide-by-zero error. The BatchScale (BS) operation is deﬁned as follows:\nBS(x) =x\nE[x] +\u000f(10)\n4 Experiments\n4.1 Task and data\nWe conduct preliminary experiments on the document retrieval benchmark provided as part of the TREC Deep Learning\ntrack [Craswell et al., 2019]. The benchmark is based on the MS MARCO dataset [Bajaj et al., 2016] and provides\na collection of 3;213;835documents and a training dataset with 384;597positively labeled query-document pairs.\nRecently, the benchmark also made available a click log dataset, called ORCAS [Craswell et al., 2020], that can be\nemployed as an additional document description ﬁeld. We refer the reader to the track website1for further details about\nthe benchmark.\nBecause we are interested in the full ranking setting, we do not make use of the provided document candidates and\ninstead use the proposed model to retrieve from the full collection. We compare different runs based on following three\nmetrics: mean reciprocal rank (MRR) [Craswell, 2009], normalized discounted cumulative gain (NDCG) [Järvelin and\nKekäläinen, 2002], and normalized cumulative gain (NCG) [Rosset et al., 2018].\n1https://microsoft.github.io/TREC-2020-Deep-Learning/\n5\n\nTable 1: Full retrieval results based on the TREC 2019 Deep Learning track test set.\nModel MRR NDCG@10 NCG@100\nNon-neural baselines\nBM25+RM3 run with best NDCG@10 0:807 0 :549 0 :559\nNon-neural run with best NDCG@10 0:872 0 :561 0 :560\nNeural baselines\nDeepCT run with best NDCG@10 0:872 0 :554 0 :498\nBERT-based document expansion + reranking run with best NCG@10 0:899 0 :646 0 :637\nBERT-based document expansion + reranking run with best NDCG@10 0:961 0 :726 0 :580\nOur models\nConformer-Kernel 0:845 0 :554 0 :464\nConformer-Kernel + learned BM25 0:906 0 :603 0 :533\nConformer-Kernel + learned BM25 + ORCAS ﬁeld 0:898 0 :620 0 :547\n4.2 Model training\nWe consider the ﬁrst 20terms for every query and the ﬁrst 4000 terms for every document. When incorporating the\nORCAS data as an additional document ﬁeld, we limit the maximum length of the ﬁeld to 2000 terms. We pretrain\nthe word embeddings using the word2vec [Mikolov et al., 2013a,b,c] implementation in FastText [Joulin et al., 2016].\nWe use a concatenation of the IN and OUT embeddings [Nalisnick et al., 2016, Mitra et al., 2016] from word2vec to\ninitialize the embedding layer parameters. The document encoder uses 2 Conformer layers and we set all the hidden\nlayer sizes to 256. We set the window size for the grouped convolution layers to 31and the number of groups to 32.\nCorrespondingly, we also set the number of attention heads to 32. We set the number of kernels kto10. For windowed\nKernel-Pooling, we set the window size to 300and the stride to 100. Finally, we set the dropout rate to 0:2. For further\ndetails, please refer to the publicly released model implementation in PyTorch.2All models are trained on four Tesla\nP100 GPUs, with 16 GB memory each, using data parallelism.\nWe train the model using the RankNet objective [Burges et al., 2005]. For every positively labeled query-document pair\nin the training data, we randomly sample one negative document from the provided top 100candidates corresponding\nto the query and two negative documents from the full collection. In addition to making pairs between the positively\nlabeled document and the three negative documents, we also create pairs between the negative document sampled from\nthe top 100candidates and those sampled from the full collection, treating the former as more relevant. This can be\ninterpreted as incorporating a form of weak supervision [Dehghani et al., 2017] as the top candidates were previously\ngenerated using a traditional IR function.\n5 Results\nTable 1 presents our main experiment results. As speciﬁed earlier, we evaluate our models on the full ranking setting\nwithout any explicit reranking step. The full model—with both Conformer-Kernel and explicit matching submodel—\nperforms signiﬁcantly better on NDCG@10 and MRR compared to the best traditional runs from the 2019 edition of\nthe track. The model also outperforms the DeepCT baseline which is a QTI-based baseline using BERT. The other\nBERT-based baselines outperform our model by signiﬁcant margins. We believe this observation should motivate future\nexploration on how to incorporate pretraining in the Conformer-Kernel model. Finally, we also notice improvements\nfrom incorporating the ORCAS data as an additional document descriptor ﬁeld.\nTo demonstrate how the GPU memory consumption scales with respect to input sequence length, we plot the peak\nmemory, across all four GPUs, for our proposed architecture using Transformer and Conformer layers, respectively,\nkeeping all other hyperparameters and architecture choices ﬁxed. Fig 2 shows the GPU memory requirement grows\nlinearly with increasing sequence length for the Conformer, while quadratically when Transformer layers are employed.\n6 Discussion and future work\nThe proposed CK-with-QTI architecture provides several advantages, with respect to inference cost, compared to\nits BERT-based peers. In addition to a shallower model and more memory-efﬁcient Conformer layers, the model\nallows for ofﬂine pre-encoding of documents during indexing. It is notable, that the document encoder, containing the\nstacked Conformer layers, is the computationally costliest part of the model. In the proposed architecture, the document\n2https://github.com/bmitra-msft/TREC-Deep-Learning-Quick-Start\n6\n\n0250050007500100001250015000\n0 500 1000 1500 2000 2500 3000 3500 4000 4500\nDocument length\nTransformer ConformerFigure 2: Comparison of peak GPU Memory Usage in MB, across all four GPUs, when employing Transformers vs.\nConformers in our proposed architecture. For the Transformer-based model, we only plot till sequence length of 512,\nbecause for longer sequences we run out of GPU memory when using Tesla P100s with 16 GB of memory.\nencoder needs to be evaluated only once per every document in the collection. This is in contrast to once per every\nquery-document pair in the case of BERT-based ranking models that accepts a concatenation of query and document as\ninput [Nogueira and Cho, 2019], and once per every term-document pair in the case of BERT-based ranking models\nwith QTI [Mitra et al., 2019].\nWhile the present study demonstrates promising progress towards using TK-style architectures for retrieval from the\nfull collection, it is worthwhile to highlight several challenges that needs further explorations. More in depth analysis of\nthe distribution of term-document scores is necessary which may divulge further insights about how sparsity properties\nand discretization can be enforced for practical operationlization of these models. Large scale pretraining in the the\ncontext of these models also presents itself as an important direction for future studies. Finally, for the full retrieval\nsetting, identifying appropriate negative document sampling strategies during training poses as an important challenge\nthat can strongly help or curtail the success these models achieve on these tasks.\nIn the ﬁrst year of the TREC Deep Learning track, there was a stronger focus on the reranking setting—although some\nsubmissions explored document expansion and other QTI-based strategies. We anticipate that in the 2020 edition of the\ntrack, we will observe more submissions using neural methods for the full retrieval setting, which may further improve\nthe reusability of the TREC benchmark [Yilmaz et al., 2020] for comparing these emerging family of approaches, and\nprovide additional insights for our line of exploration.\nReferences\nAmin Ahmad, Noah Constant, Yinfei Yang, and Daniel Cer. Reqa: An evaluation for end-to-end answer retrieval\nmodels. arXiv preprint arXiv:1907.04780 , 2019.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew\nMcNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: A human generated machine reading comprehension\ndataset. arXiv preprint arXiv:1611.09268 , 2016.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint\narXiv:2004.05150 , 2020.\nChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. Learning to\nrank using gradient descent. In Proc. ICML , pages 89–96. ACM, 2005.\nWei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. Pre-training tasks for embedding-\nbased large-scale retrieval. arXiv preprint arXiv:2002.03932 , 2020.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.\nCoRR , abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509 .\nNick Craswell. Mean reciprocal rank. In Encyclopedia of Database Systems , pages 1703–1703. Springer, 2009.\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. Overview of the trec 2019 deep learning track. In\nProc. TREC , 2019.\n7\n\nNick Craswell, Daniel Campos, Bhaskar Mitra, Emine Yilmaz, and Bodo Billerbeck. Orcas: 18 million clicked\nquery-document pairs for analyzing search. arXiv preprint arXiv:2006.05324 , 2020.\nZhuyun Dai and Jamie Callan. Context-aware passage term weighting for ﬁrst stage retrieval.\nZhuyun Dai and Jamie Callan. Deeper text understanding for ir with contextual neural language modeling. In\nProceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information\nRetrieval , pages 985–988, 2019a.\nZhuyun Dai and Jamie Callan. An evaluation of weakly-supervised deepct in the trec 2019 deep learning track. In\nTREC , 2019b.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl:\nAttentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860 , 2019.\nMostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce Croft. Neural ranking models with\nweak supervision. In Proc. SIGIR , pages 65–74. ACM, 2017.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\nJiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W. Bruce Croft, and Xueqi\nCheng. A deep look into neural ranking models for information retrieval. Information Processing & Management ,\n2019.\nSebastian Hofstätter, Hamed Zamani, Bhaskar Mitra, Nick Craswell, and Allan Hanbury. Local self-attention over long\ntext for efﬁcient document retrieval. In Proc. SIGIR . ACM, 2020a.\nSebastian Hofstätter, Markus Zlabinger, and Allan Hanbury. Interpretable & time-budget-constrained contextualization\nfor re-ranking. In Proc. of ECAI , 2020b.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. arXiv preprint arXiv:1502.03167 , 2015.\nKalervo Järvelin and Jaana Kekäläinen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on\nInformation Systems (TOIS) , 20(4):422–446, 2002.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efﬁcient text classiﬁcation.\narXiv preprint arXiv:1607.01759 , 2016.\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense\npassage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020.\nOmar Khattab and Matei Zaharia. Colbert: Efﬁcient and effective passage search via contextualized late interaction\nover bert. arXiv preprint arXiv:2004.12832 , 2020.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In International Conference\non Learning Representations , 2019.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index structures. In\nProceedings of the 2018 International Conference on Management of Data , pages 489–504, 2018.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question\nanswering. arXiv preprint arXiv:1906.00300 , 2019.\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, dense, and attentional representations for\ntext retrieval. arXiv preprint arXiv:2005.00181 , 2020.\nJi Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. Zero-shot neural retrieval via domain-targeted\nsynthetic query generation. arXiv preprint arXiv:2004.14503 , 2020.\nSean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, and Ophir Frieder.\nExpansion via prediction of importance with contextualization. arXiv preprint arXiv:2004.14245 , 2020.\nJoel Mackenzie, Zhuyun Dai, Luke Gallagher, and Jamie Callan. Efﬁciency implications of term weighting for\npassage retrieval. In Proceedings of the 43nd International ACM SIGIR Conference on Research & Development in\nInformation Retrieval , 2020.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations in vector\nspace. arXiv preprint arXiv:1301.3781 , 2013a.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and\nphrases and their compositionality. In Proc. NIPS , pages 3111–3119, 2013b.\n8\n\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In\nHLT-NAACL , pages 746–751. Citeseer, 2013c.\nBhaskar Mitra and Nick Craswell. An introduction to neural information retrieval. Foundations and Trends R\rin\nInformation Retrieval , 2018.\nBhaskar Mitra and Nick Craswell. Duet at trec 2019 deep learning track. In Proc. TREC , 2019a.\nBhaskar Mitra and Nick Craswell. An updated duet model for passage re-ranking. arXiv preprint arXiv:1903.07666 ,\n2019b.\nBhaskar Mitra, Eric Nalisnick, Nick Craswell, and Rich Caruana. A dual embedding space model for document ranking.\narXiv preprint arXiv:1602.01137 , 2016.\nBhaskar Mitra, Fernando Diaz, and Nick Craswell. Learning to match using local and distributed representations of text\nfor web search. In Proc. WWW , pages 1291–1299, 2017.\nBhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz, and Emine Yilmaz. Incorporating query\nterm independence assumption for efﬁcient retrieval and ranking using deep neural networks (under review). In Proc.\nACL, 2019.\nEric Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. Improving document ranking with dual word\nembeddings. In Proc. WWW , 2016.\nFederico Nanni, Bhaskar Mitra, Matt Magnusson, and Laura Dietz. Benchmark for complex answer retrieval. In Proc.\nICTIR , pages 293–296. ACM, 2017.\nRodrigo Nogueira and Kyunghyun Cho. Task-oriented query reformulation with reinforcement learning. In Proc.\nEMNLP , pages 574–583, 2017.\nRodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085 , 2019.\nRodrigo Nogueira, Jimmy Lin, and AI Epistemic. From doc2query to doctttttquery. Online preprint , 2019a.\nRodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction. arXiv\npreprint arXiv:1904.08375 , 2019b.\nHarrie Oosterhuis, J Shane Culpepper, and Maarten de Rijke. The potential of learned index structures for index\ncompression. In Proceedings of the 23rd Australasian Document Computing Symposium , pages 1–4, 2018.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image\ntransformer. arXiv preprint arXiv:1802.05751 , 2018.\nStephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and\nTrends R\rin Information Retrieval , 3(4):333–389, 2009.\nCorby Rosset, Damien Jose, Gargi Ghosh, Bhaskar Mitra, and Saurabh Tiwary. Optimizing query evaluations using\nreinforcement learning for web search. In Proc. SIGIR . ACM, 2018.\nCorby Rosset, Bhaskar Mitra, Chenyan Xiong, Nick Craswell, Xia Song, and Saurabh Tiwary. An axiomatic approach\nto regularizing neural ranking models. In Proc. SIGIR , pages 981–984, 2019.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse attention with\nrouting transformers. arXiv preprint arXiv:2003.05997 , 2020.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm:\nTraining multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv:1909.08053 ,\n2019.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers.\narXiv preprint arXiv:1905.07799 , 2019.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. arXiv preprint\narXiv:2002.11296 , 2020.\nChristophe Van Gysel, Bhaskar Mitra, Matteo Venanzi, Roy Rosemarin, Grzegorz Kukla, Piotr Grudzien, and Nicola\nCancedda. Reply with: Proactive recommendation of email attachments. In Proc. CIKM , 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural Information Processing Systems , pages 5998–6008,\n2017.\nSinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\narXiv preprint arXiv:2006.04768 , 2020.\n9\n\nLilian Weng. Attention? attention! lilianweng.github.io/lil-log , 2018. URL https://lilianweng.github.io/\nlil-log/2018/06/24/attention-attention.html .\nLilian Weng. The transformer family. lilianweng.github.io/lil-log , 2020. URL https://lilianweng.github.io/\nlil-log/2020/04/07/the-transformer-family.html .\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short range attention. In\nInternational Conference on Learning Representations , ICLR ’20, 2020.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk.\nApproximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808 ,\n2020.\nMing Yan, Chenliang Li, Chen Wu, Bin Bi, Wei Wang, Jiangnan Xia, and Luo Si. Idst at trec 2019 deep learning track:\nDeep cascade ranking with generation-based document expansion and pre-trained language modeling. In TREC ,\n2019.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized\nautoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237 , 2019.\nEmine Yilmaz, Nick Craswell, Bhaskar Mitra, and Daniel Campos. On the reliability of test collections to evaluating\nsystems of different types. In Proc. SIGIR . ACM, 2020.\nZeynep Akkalyoncu Yilmaz, Shengjin Wang, and Jimmy Lin. H2oloo at trec 2019: Combining sentence and document\nevidence in the deep learning track. In TREC , 2019.\nHamed Zamani, Mostafa Dehghani, W Bruce Croft, Erik Learned-Miller, and Jaap Kamps. From neural re-ranking to\nneural ranking: Learning a sparse representation for inverted indexing. In Proc. CIKM , pages 497–506. ACM, 2018.\n10",
  "textLength": 33749
}