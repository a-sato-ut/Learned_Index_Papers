{
  "paperId": "86f014f66e484f477fe729023ee5d30537a299d6",
  "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
  "pdfPath": "86f014f66e484f477fe729023ee5d30537a299d6.pdf",
  "text": "ProMoE : Fast MoE-based LLM Serving using Proactive Caching\nXiaoniu Song1Zihang Zhong3,‚àóRong Chen1,‚Ä°Haibo Chen1,2\n1Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University\n2Key Laboratory of System Software (Chinese Academy of Sciences)3Zhejiang University\nAbstract\nThe promising applications of large language models are often lim-\nited by the constrained GPU memory capacity available on edge\ndevices. Mixture-of-Experts (MoE) models help address this issue by\nactivating only a subset of the model‚Äôs parameters during computa-\ntion. This approach allows the unused parameters to be offloaded to\nhost memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache\nmisses reactively, which significantly impacts system performance.\nIn this paper, we introduce ProMoE , a novel proactive caching\nsystem that utilizes intermediate results to predict subsequent ex-\npert usage. By proactively fetching experts in advance, ProMoE\neliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with\noffloading. Our evaluations demonstrate that ProMoE achieves an\naverage speedup of 2.20 √ó(up to 3.21√ó) and 2.07√ó(up to 5.02√ó) in\nthe prefill and decode stages, respectively, compared to existing\noffloading solutions.\n1 Introduction\nLarge language models (LLMs) have revolutionized various fields,\nincluding natural language processing, content generation, and de-\ncision support [ 10,29,38,44,52]. Traditionally, these models have\nbeen deployed in data centers equipped with high-end GPUs. How-\never, there is growing interest in running LLMs on consumer-grade\nplatforms to enhance privacy and speed [ 12,41,50]. Despite this\ngrowing interest, significant challenges remain due to memory limi-\ntations. LLMs typically require substantial memory (often hundreds\nof gigabytes) [ 29,44,52], which exceeds the capacities of consumer-\ngrade GPUs, generally limited to around a dozen gigabytes. This\nlimitation leads to serious performance issues, ultimately hindering\nthe efficiency and adoption of LLMs on personal computers.\nMixture-of-Experts (MoE) [ 15,17,27,28,49] offers an oppor-\ntunity to address the GPU memory constraints faced by LLMs by\ndividing the model into multiple experts and activating only a few\nduring inference. This approach allows most experts to be offloaded\nto host memory, loading only the necessary ones into GPU memory.\nWhile this significantly reduces GPU memory requirements, expert\noffloading also introduces severe performance degradation up to\n8.9√ó[30] due to limited PCIe bandwidth between host and GPU\nmemory (32GB/s unidirectional on PCIe 4.0).\nRecently, researchers have proposed caching frequently accessed\nexperts in GPU memory to minimize offloading costs [ 18]. However,\nthis caching approach handles missing experts in a reactive manner.\nSpecifically, a cache miss is triggered passively when an expert is\n‚Ä°Rong Chen is the corresponding author (rongchen@sjtu.edu.cn).\n‚àóDuring internship at Shanghai Jiao Tong University.\nE1E2E4Load 2E5Load 5Prev LayerE1E4E2Fetch 2E5Fetch 5F3GateGatemissmissmissAttnPrev LayerAttnReactive CacheProactive CacheGPUCPUGPUCPUhigh pri. prefetchlow pri. prefetchFigure 1: A comparison of execution flow between reactive and proac-\ntive caching.\naccessed during inference, leaving the expensive loading on the\ncritical path (see Figure 1). For instance, when caching 50% of the\nexperts in the deepseek-moe [ 4] model, the time spent on loading\nmissing experts accounts for over 60% of the total inference time.\nAdditionally, the inherent low skewness and poor locality of expert\naccess patterns in MoE models, especially in modern decoder-only\narchitectures, significantly limit the potential improvements that\ncan be achieved through better caching policies.\nIn this paper, we propose ProMoE , a novel system to address\nthe performance challenges associated with offloading in MoE-\nbased LLMs through proactive caching, as shown in Figure 1. By\nactively predicting which experts will be needed and prefetching\ntheir parameters into a cache in GPU memory, ProMoE can take\nthe time required for fetching missing experts off the critical path.\nThis allows for better overlap with computation, enhancing overall\nperformance and GPU utilization.\nTo achieve effective proactive caching, ProMoE addresses two\nmain questions. First, given the dynamic nature of MoE models, Pro-\nMoE requires a predictive approach for prefetching. To evaluate the\nquality of a prediction method, ProMoE introduces a metric called\nGoodPred . This metric considers both the accuracy and efficiency\nof the predictions. To achieve a high GoodPred score, ProMoE pro-\nposes a learned predictor that prefetches experts in a stride manner.\nThis learned predictor identifies correlations between intermediate\nresults and expert selections , allowing for accurate predictions\nof experts while the stride prefetching technique perfectly hides\nprediction latency, ensuring high efficiency of prefetching.\nSecond, the processes of prefetching and inference can interfere\nwith each other, leading to low utilization of the GPU, cache, and\nbandwidth for prefetching. Therefore, ProMoE needs to carefully\ncoordinate these two processes to minimize interference. We ob-\nserved that the required experts for each layer can be identified\nall at once, which creates opportunities to optimize prefetching\nand inference for better overlap. Based on this insight, ProMoE\nproposes several techniques to coordinate the execution of prefetch-\ning and inference processes, including chunked prefetching, early\npreemption, and reordered inference. These techniques eliminate\npassive cache misses and maximize the overlap between prefetchingarXiv:2410.22134v3  [cs.DC]  1 Sep 2025\n\nlayer norm self-attn input vector# token# hidden dim+layer norm FFN+output vectorTransformer Layer\nComputeResidualMoE BlockgateE1E2E3En-1En¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑Transformer Transformer LM Head Embedding Once upon a\ntimethere\nis¬∑¬∑¬∑time\n,¬∑¬∑¬∑,\ntherePreÔ¨Åll StageDecode Stage¬∑¬∑¬∑Figure 2: (a) The execution flow of large language models (LLMs), (b) the architecture of a transformer layer in LLMs, and (c) the architecture of\na Mixture-of-Experts (MoE) block that replaces FFN in a transformer layer.\nTable 1: MoE-based LLMs description. P, L, and E denote parameters,\nlayers, and experts, respectively. Act.indicates the number of activated\nparameters or experts during the inference of a single token.\nMoE-based LLM#P#L#E per L\nTotal Act. Total Act.\nDeepseek-moe (DS-1) [4] 16.4B 2.8B 28 64 6\nDeepseek-v2-lite (DS-2) [5] 15.7B 2.7B 27 64 6\nQwen1.5-moe (QW-1) [7] 14.3B 2.7B 24 60 4\nQwen2-moe (QW-2) [8] 57.4B 14.2B 28 64 8\nMixtral-8x7B (Mixt) [1] 46.7B 12.9B 32 8 2\nand inference, thereby reducing inference latency and improving\nutilization.\nTo demonstrate the effectiveness of ProMoE in serving MoE-\nbased LLMs on consumer-grade hardware, we integrated ProMoE\ninto two widely used LLM frameworks: transformers and llama.cpp.\nCompared to hand-crafted caching baselines with state-of-the-art\nperformance, ProMoE achieves an average speedup of 1.78 √ó(up\nto 2.48√ó) in the prefill stage and 1.34 √ó(up to 1.79√ó) in the decode\nstage. When compared to existing offloading methods available\nin open-source LLM frameworks, ProMoE achieves an average\nspeedup of 2.20√ó(up to 3.21√ó) and 2.07√ó(up to 5.02√ó) for these\ntwo stages. The source code of ProMoE is publicly available at\nhttps://github.com/promoe-opensource/promoe.\nContributions . We make the following contributions.\n(1) A new metric called ‚ÄúGoodPred ‚Äùthat can holistically evaluate\nvarious predictors in expert prefetching ( ¬ß4).\n(2) A novel learned predictor, coupled with a stride mechanism,\nthat achieves high accuracy while hiding prediction latency ( ¬ß4).\n(3) A sophisticated proactive cache that eliminates passive cache\nmisses by coordinating prefetching and inference ( ¬ß5).\n(4) An implementation integrated into mainstream LLM frame-\nworks (¬ß6), along with an evaluation showing the efficacy and\nefficiency of ProMoE compared to state-of-the-art solutions ( ¬ß7).2 Background\n2.1 Mixture-of-Experts (MoE) based LLMs\nLarge Language Models (LLMs) perform inference in two stages:\nprefill and decode, as illustrated in Figure 2(a). During the prefill\nstage, the model processes the user‚Äôs input prompt in a single\niteration. The tokens within the prompt are processed in parallel\nby the model, and the first token of the response is generated at the\nend of this iteration. In the decode stage, each iteration processes\nonly one token generated from the previous iteration, producing\nthe next token. These tokens are fed into the model sequentially\nand ultimately concatenated to form the complete response. Due to\nthe differing computational scales between the two stages of LLM\ninference, their performance is typically measured separately. The\nperformance of the prefill stage is usually quantified by the Time\nto First Token (TTFT), which represents the duration users wait for\nthe LLM to process the prompt before it starts generating output.\nFor the decode stage, performance is commonly measured using\neither Tokens Per Second (TPS) or Time Per Output Token (TPOT).\nLarge Language Models (LLMs) consist of a series of transformer\nlayers. Each layer contains a self-attention block (self-attn) and a\nfeed-forward network (FFN), as shown in Figure 2(b). These com-\nponents process the input hidden states, add the results back to the\ninputs, and pass them to the next layer. Due to layer normalization,\nthe outputs are numerically smaller than their inputs, leading to a\nslow change in hidden states across layers [ 33,36]. Typically, the\ncosine similarity between hidden states of adjacent layers averages\naround 90%.\nThe Mixture-of-Experts (MoE) architecture enhances LLMs by\nexpanding the FFN into multiple experts, as depicted in Figure 2(c).\nThis approach increases the model‚Äôs parameters while reducing\noverall computation, since only a subset of experts is activated\nduring each forward pass. Specifically, each MoE block consists of\na gate function and multiple experts. The gate function prioritizes\nwhich experts should process the current token. Each expert is\nstructurally similar to the original FFN but contains fewer param-\neters. The output of the MoE block is a weighted average of the\noutputs from all activated experts.\nIn MoE-based LLMs, expert selection occurs independently for\neach token, as shown in Table 1. When processing multiple tokens\n2\n\n0 20 40 60\nExpert Cache Rate (%)02004006008001000 Time (ms)\nTTFT\nTTFT-PCIeTPOT\nTPOT-PCIe\n20 40 60\nExpert Cache Rate (%)020406080100 Time Breakdown (%)TTFT-GPU\nTTFT-PCIeTPOT-GPU\nTPOT-PCIeFigure 3: The (a) latency and (b) time breakdown of LRU caching\nunder different cache rates in transformers with DS-1 model.\n0 20 40 60\nExpert Cache Rate (%)02004006008001000 Time (ms)\nTTFT\nTTFT-PCIeTPOT\nTPOT-PCIe\n20 40 60\nExpert Cache Rate (%)020406080100 Time Breakdown (%)TTFT-GPU\nTTFT-PCIeTPOT-GPU\nTPOT-PCIe\nFigure 4: The (a) latency and (b) time breakdown of LRU caching\nunder different cache rates in llama.cpp with QW-2 model.\nsimultaneously (e.g., when processing prompts or batching multiple\nrequests), a larger portion of experts is activated, ranging from over\n50% to nearly 100%, depending on the number of tokens.\n2.2 Caching MoE-based LLMs\nIn MoE-based LLMs, each token utilizes only a subset of experts.\nMost experts can be offloaded to CPU memory, with only the nec-\nessary experts loaded into GPU memory. This allows MoE-based\nLLMs to run on consumer-grade hardware with limited GPU mem-\nory. However, due to the limited PCIe bandwidth, directly offloading\nparameters to CPU memory can lead to high latency and low GPU\nutilization. For instance, when running the DS-1 model with 50%\nof experts offloaded to CPU memory, the TPOT is 67.9 ms, while\nfetching experts from host memory takes 58.1 ms, accounting for\n85.6% of the total time. Each output token requires 2.67 GiB of\nexpert parameters in FP16 precision, with 1.33 GiB needing to be\ntransferred from CPU memory to GPU memory due to offloading.\nThe achieved bandwidth is 23 GB/s, which matches the achievable\nbandwidth (23.9 GB/s in our bandwidth test) from host to GPU\nusing PCIe 4.0x8.\nTo mitigate the performance issues caused by offloading, a tra-\nditional method is to cache frequently accessed experts in GPU\nmemory. A common approach is to use LRU (Least Recently Used)\nor static caching to store these frequently accessed experts. For\nexample, Mixtral-offloading [ 18] implements an LRU cache for the\nMixtral model. Another example is CUDA‚Äôs Unified Memory (UM),\nwhich leverages a paging mechanism to transfer data between the\nGPU and CPU on demand.\n0.0 0.2 0.4 0.6 0.8 1.0\n# Experts / Total Experts0.00.20.40.60.81.0Cumulative Access Rate\n0.0 0.2 0.4 0.6 0.8 1.0\nCache Rate0.00.20.40.60.81.0Hit RateSWI-128 SWI-256 NLLB-54B\n0.0 0.2 0.4 0.6 0.8 1.0\n# Experts / Total Experts0.00.20.40.60.81.0Cumulative Access Rate\n0.0 0.2 0.4 0.6 0.8 1.0\nCache Rate0.00.20.40.60.81.0Hit RateDS-1 DS-2 QW-1 QW-2 MixtFigure 5: The (a) CDF of expert access frequency and (b) hit rate of\nLRU caching. The upper figures show results of traditional encoder-\ndecoder models (switch-transformer [ 19], NLLB [ 13]), while the lower\nfigures show results of modern decoder-only models listed in Table 1.\nThe major challenge of caching in MoE is its reactive nature\nwhen handling cache misses. When the inference process encoun-\nters an expert that is missing from GPU memory, the computation\nis blocked until the expert is fetched from host memory, resulting\nin high latency overhead in the critical path of inference.\nWe evaluated the performance of LRU caching in transform-\ners [ 45] with the DS-1 (fp16) model, along with llama.cpp [ 22]\nusing the QW-2 (int4) model. Figure 3 and 4 illustrate the inference\nlatency and the time breakdown for both the prefill and decode\nstages. In the case of the DS-1 model, caching 50% of experts results\nin a blocking time of 60.4% on the critical path during the decode\nstage. The blocking time during the prefill stage is more severe, as\nmore experts are accessed, leading to an 82.7% blocking time on\nthe critical path. For llama.cpp, which achieves faster inference by\neliminating the overhead of the Python interpreter, the proportion\nof blocking time is even greater. Caching 50% of experts results in\n94.2% blocking time during the prefill stage and 79.0% during the\ndecode stage.\nAnother factor that exacerbates the impact of blocking time on\nthe critical path is the access frequencies of different experts in\nMoE-based LLMs, particularly modern decoder-only LLMs, which\ntend to be less skewed. Figure 5 shows the cumulative access fre-\nquency of experts and the hit rate of LRU caching. Traditional\nencoder-decoder MoE models like switch-transformer [ 19] (SWI)\nand NLLB [ 13] released in 2022 exhibit a power-law distribution\nin expert access frequencies, where a small number of experts are\naccessed more frequently than others. This high skewness leads\nto high hit rates and benefits both static and dynamic caching like\nLRU. In contrast, modern decoder-only MoE models exhibit a more\nuniform access pattern, as shown in the bottom of Figure 5. This\n3\n\nPrefetcherPredictorCPUGPU¬∑¬∑¬∑layer ilayer i+kHitMissCacheProMoE\nExperts\n¬∑¬∑¬∑¬∑¬∑¬∑Figure 6: The architecture of ProMoE .\nlow skewness creates unique challenges for offloading and caching\nin modern MoE models.\nThis uniform access pattern can be attributed to the deliberate\ndesign of modern MoE models, which utilize various techniques\nduring training to prevent any single expert from becoming a\nhotspot. This is crucial because uneven expert utilization can lead\nto inadequate training of certain experts, ultimately impacting the\nmodel‚Äôs performance. This phenomenon is referred to as ‚Äúrouting\ncollapse‚Äù [ 39]. To mitigate routing collapse, contemporary MoE\nmodels incorporate strategies such as Device-Limited Routing [ 17]\nand Expert-Level Balance Loss [ 15] during the training process.\nConsequently, the access frequencies of different experts tend to be\nmore uniform during inference. Combined with the reactive han-\ndling of cache misses, the caching solution significantly degrades\nthe critical path latency.\n3 Overview of ProMoE\nThis paper presents ProMoE , a system that achieves low-latency\ninference for MoE-based LLMs on consumer-grade platforms. Pro-\nMoE addresses the reactive nature of existing solutions, which\npassively trigger data transfers on the critical path of inference,\nleading to high latency. To tackle this issue, ProMoE adopts a proac-\ntive caching approach. Instead of directly reducing data transfers\nbetween the CPU and GPU, proactive caching moves data transfers\nout of the critical path, allowing them to overlap with inference.\nThe architecture of ProMoE is illustrated in Figure 6. It consists\nof two main components: the predictor and the prefetcher. The pre-\ndictor periodically predicts which experts will be selected. Based\non these predictions, the prefetcher preloads experts into the GPU\ncache. During inference, the LLM inference engine accesses experts\nstored in the cache and triggers misses for any experts that are ab-\nsent. Compared to existing solutions, most expert data transfers in\nProMoE occur outside the critical path of inference, thus reducing\nlatency and improving GPU utilization.\nTo achieve effective proactive caching, ProMoE must address the\nquestions of ‚Äú what to prefetch ‚Äù and ‚Äú how to prefetch ‚Äù as mentioned\nin¬ß1. The predictor in ProMoE tackles the first question by making\ngood predictions. To define what constitutes a good prediction, Pro-\nMoE proposes a GoodPred metric that considers both the accuracy\nand efficiency of the predictions. Based on this metric, ProMoE\nintroduces a learned predictor that prefetches experts in a stride\nmanner. This learned predictor memorizes the correlations betweenintermediate results and expert selections to make accurate predic-\ntions of expert selections. Additionally, through stride prefetching,\nProMoE overlaps the processes of prediction and prefetching to\nhide the latency of predictor.\nProMoE ‚Äôs prefetcher addresses the second question by care-\nfully coordinating the prefetching and inference processes. Naive\nprefetching can lead to interference between these processes, result-\ning in suboptimal performance. ProMoE leverages the observation\nthat the choice of experts for each layer becomes available all at\nonce after the gating function. Based on this insight, ProMoE pro-\nposes three key techniques to effectively coordinate prefetching and\ninference: chunked prefetching, early preemption, and reordered\ninference. With these techniques working in concert, ProMoE can\neliminate passive cache misses and maximize the overlap between\nprefetching and inference, thereby reducing inference latency.\n4 GoodPred, Prediction, and Prefetching\nThe dynamic nature of MoE models necessitates the deployment\nof a predictor in ProMoE to make approximate predictions of ex-\nperts for prefetching. To ensure effective prefetching, the predictor\nmust meet two primary requirements: accuracy and efficiency. In\nthis section, we first define a key metric called GoodPred , which\ncombines these two aspects to evaluate the performance of a pre-\ndictor. Subsequently, we introduce ProMoE ‚Äôs learned predictor and\nexplain how it improves both accuracy and efficiency.\n4.1 A New Prediction Metric: GoodPred\nA good predictor requires both high accuracy and efficiency. Higher\naccuracy increases the likelihood that predicted experts will be\nutilized, while higher efficiency allows more time to load these\npredicted experts. These two goals must be pursued simultane-\nously, though these two goals might initially seem contradictory‚Äî\nimproving accuracy often requires more prediction time, which can\nreduce the time available for prefetching.\nTo assess the performance of the predictor, we define the Good-\nPred metric as follows:\nGoodPred =Accuracy√óFetchRate\nGoodPred evaluates the effectiveness of the predictor in predicting\nexperts for prefetching by considering both Accuracy andFetchRate .\nTheAccuracy denotes the proportion of correctly predicted experts,\nwhile the FetchRate signifies the portion of predicted experts that\ncan be prefetched in time before they are accessed during LLM\ninference. Thus, GoodPred measures the volume of correct experts\nthat can be prefetched in a timely manner.\n4.2 Existing Approaches\nRecent research has proposed two main methods for predicting\nexpert usage. Previous studies [ 28,47] introduced a token-based\npredictor that predicts expert usage based on input tokens, allowing\nfor an iteration-wise prefetch pattern, as illustrated in Figure 7(a).\nThese studies suggest that the selection of experts in one iteration\nis closely related to the input token ID. This relationship can be\nintuitively explained: LLMs convert the input token ID into an\nembedding vector through a fixed mapping, and the computation\nin each iteration gradually adds contextual information to these\nembeddings. Consequently, the input token ID can be utilized to\n4\n\n¬∑¬∑¬∑i+1i+kinputtokenoutput tokeni\ni+1+k¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑i+1i+kinputtokenoutput tokeni\ni+1+k¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑i+1i+kinputtokenoutput tokeni\ni+1+k¬∑¬∑¬∑¬∑¬∑¬∑Iteration-wise PrefetchLayer-wise PrefetchStride PrefetchPredictTransformer LayerEmbedding/LM Head¬∑¬∑¬∑Prefetch\n¬∑¬∑¬∑¬∑¬∑¬∑Figure 7: Candidate prefetch manners.\npredict the selection of experts across all layers within that iteration.\nSpecifically, in the offline stage, a trace of input token IDs and their\nselected experts is collected. Then, during online inference, the\npredictor determines which experts to select for one iteration by\nidentifying the most frequently used experts from this trace based\non the input token ID.\nBy predicting experts for all layers before an iteration begins,\nthe token-based predictor achieves optimal FetchRate , maximiz-\ning available time for prefetching. However, this approach suffers\nfrom low Accuracy . The iteration-wise pattern conducts predic-\ntion over a long distance, leading to decreased accuracy. Moreover,\nthe input token ID lacks contextual information concerning the\nentire sequence. As shown in Figure 8, the average accuracy of\nthe token-based predictor is only 58.3%. Despite delivering a high\nFetchRate through iteration-wise prefetching, the low Accuracy\nrenders nearly half of this prefetching ineffective, resulting in a low\nGoodPred .\nAnother recent system [ 18] proposed a skip-based predictor\nthat facilitates a layer-wise prefetch manner, as illustrated in Fig-\nure 7(b). This approach leverages the high similarity between inputs\nacross layers in LLMs [ 33,36]. It establishes a skip connection that\ntransmits the input from ùëñ-th layer‚Äôs MoE gate directly to the MoE\ngate in ùëñ+1-th layer, thereby predicting the experts for ùëñ+1-th\nlayer at the point of ùëñ-th layer. For instance, in the DS-2 model, the\ncosine similarity between the consecutive layers‚Äô inputs is 91.7%.\nThus, passing the input of the ùëñ-th layer to the ùëñ+1-th layer‚Äôs gate\nis likely to yield accurate predictions.\nHowever, the skip-based predictor‚Äôs accuracy remains limited.It\ndepends on the similarity of inputs across different layers and\nthe numerical stability of the gate function, which does not uni-\nformly apply across all models. In Figure 8, the skip-based predictor\nachieves high accuracy with noticeable accuracy drop in the head\nand tail layers for the DS-1 model. However, the QW-2 model ex-\nperiences a significant accuracy decline with an average accuracy\nof only 66.9%. This discrepancy arises because the gate function in\nthe QW-2 model is sensitive to input variations, causing shifts in\npriority for expert selection even with slight input changes. Addi-\ntionally, the layer-wise prefetch pattern of the skip-based predictor\nincurs higher prediction overhead, thus limiting prefetch efficiency.\n0 4 8 12 16 20 24\nOutput Layer0.00.20.40.60.81.0Prediction AccuracyLearned\nSkip\nTokenLearned\nSkip\nToken\n0 4 8 12 16 20 24 28\nOutput Layer0.00.20.40.60.81.0Prediction AccuracyLearned\nSkip\nTokenLearned\nSkip\nTokenFigure 8: The prediction accuracy of each layer in (a) DS-1 model\nand (b) QW-2 model.\n4.3 Learning-based Predictor\nTo achieve high Accuracy ,ProMoE introduces a learned predictor\nto conduct layer-wise prefetch. The main idea is to collect the\ncorrelation between layer inputs and expert selections across layers\nand memorize this correlation in predictors. The predictor then uses\nthese correlations to make predictions. When paired with layer-\nwise prefetch, the learned predictor maintains high Accuracy .\nProMoE ‚Äôs learned predictor employs a small neural network\n(NN) to learn correlations between layer inputs and expert selec-\ntions. This approach, which utilizes a small NN like multi-layer\nperceptrons (MLPs) as the predictor, has been effectively applied\nand validated in various system research contexts [ 24,31,36,41].\nThese NNs are capable of learning complex correlations while pro-\nviding fast predictions, which can be more challenging for tradi-\ntional heuristic methods like nearest neighbor search. However, a\nsignificant drawback of NN-based methods is their lengthy train-\ning time. Fortunately, in the context of serving LLMs, the offline\ntraining is a one-time task for each LLM and is negligible compared\nto the extensive pre-training time required for LLMs [29].\nThe learned predictor in ProMoE operates in two phases: offline\ntraining and online prediction. In the offline phase, ProMoE trains a\nset of predictors by performing multiple iterations of LLM inference.\nThis process collects the input for each layer and the corresponding\noutput of the gate function. Based on these collected traces, ProMoE\ntrains a set of predictors for each layer to learn and memorize the\ncorrelations between inputs and outputs.\nTo ensure the predictor‚Äôs generalizability, ProMoE collects traces\nfrom the domain datasets used during either LLM training or in-\nference. This approach ensures that the predictor aligns with the\nmodel across various conditions. Following standard practices, the\ncollected traces are split into training and evaluation sets with a\n9:1 ratio. The predictor is trained solely on the training set and\nevaluated only on the unseen data from the evaluation set.\nIn the online inference, the input for each layer is collected and\nfed into the corresponding predictor(s) to make predictions. The\nprediction output, similar to a gate‚Äôs output, indicates the prefetch\npriority of experts in one layer. Based on this output, the predictor\nselects the same number of experts that the model would activate\nfor one token (e.g. 6 for the DS-1 model in Table 1) and hands over\nthese experts to the prefetcher for prefetching.\nTo assess the accuracy of different predictors, we evaluated them\nusing the evaluation set of collected traces. As shown in Figure 8,\nProMoE learned predictor maintains high Accuracy across both\nmodels, achieving an average accuracy of 84.7%. This improved\naccuracy enables ProMoE to accurately prefetch experts in a timely\n5\n\nmanner, optimizing the use of the limited bandwidth between the\nCPU and GPU.\n4.4 Stride Prefetching\nTo minimize the impact of the prediction on critical path latency,\nProMoE executes the predictor on the CPU, allowing it to run con-\ncurrently with LLM inference. The latency of a single predictor on\nthe CPU is about 200 microseconds. Compared to the millisecond-\nlevel computation time of a single layer in LLMs, this latency can\nbe hidden since the CPU-based prediction process operates in par-\nallel with the LLM inference on the GPU. However, in layer-wise\nprefetching, the predictor‚Äôs latency consumes available time for\nprefetching experts, resulting in a lower FetchRate .\nTo enhance the FetchRate ,ProMoE introduces stride prefetch-\ning as shown in Figure 7(c). Stride prefetching increases the pre-\ndiction distance by 1, allowing prefetching to begin earlier than in\nlayer-wise prefetching. Moreover, stride prefetching pipelines the\nprediction and prefetching processes, executing them simultane-\nously. In contrast to layer-wise prefetching, where prediction and\nprefetching are carried out sequentially, stride prefetching ensures\nthat all available bandwidth between the CPU and GPU is fully\nutilized for prefetching. Consequently, this approach maximizes\ntheFetchRate and provides a higher GoodPred .\nWhile increasing the prediction distance may lead to a decrease\nin the predictor‚Äôs Accuracy , practical observations reveal that the\naccuracy of ProMoE ‚Äôs learned predictor only declines by 5% during\nstride prefetching. Additionally, stride prefetching offers ample\ndesign space for more sophisticated predictors that may require\nadditional time to generate predictions.\n5 Coordination of Prefetching and Inference\nThe prefetcher in ProMoE is responsible for fetching experts into\nthe GPU cache based on prediction results. It consists of a worker\nthread and a task queue. The worker thread retrieves prefetch tasks\nfrom the queue and copies the corresponding experts into the GPU‚Äôs\nexpert cache. The task queue maintains two priority levels: low-\npriority speculative prefetch tasks provided by the predictor, and\nhigh-priority precise prefetch tasks triggered by cache misses dur-\ning LLM inference. The worker thread always prioritizes executing\nhigh-priority tasks over low-priority ones.\nTo further enhance the coordination between expert prefetch-\ning and LLM inference, ProMoE proposes several optimizations:\nchunked prefetching, early preemption, and reordered inference.\nThese optimizations aim to minimize interference and maximize\nthe overlap between prefetching and inference, as illustrated in\nFigure 9.\n5.1 Chunked Prefetching\nWhen high-priority prefetch tasks are added to the queue, there\nis typically ongoing fetching of expert parameters from CPU to\nGPU. This fetching may stem from an incomplete prefetch task\nof the current layer or from a prefetch task of subsequent layers\nthat has already begun. Due to the limitations of CUDA‚Äôs asyn-\nchronous copy mechanism, an ongoing copy operation cannot be\npreempted midway. As a result, high-priority prefetch tasks must\nwait for the current copy operation to complete before they can\nstart, introducing unnecessary latency into the critical path.\nFetch 2\nFetch 2Fetch 2\nFetch 2E5E5E5E5high pri. tasklow pri. task¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑ImprovementGateE1E4\nGateE4E2GateE2E1E4\nGateE2\nE2E1E4E1GPUCPUGPUCPUnaiveprefetch+earlyGPUCPU+chunkinferencewait for copy\nGPUCPU+reorderFigure 9: ProMoE coordinates prefetching with inference using\na series of optimizations. Assume experts 1,3,4,5are prefetched in\nadvance, and gate produces 1,2,4,5, where expert 2is not in cache.\nTo address this issue, ProMoE introduces chunk-based prefetch-\ning. The key idea is to split the parameters of each expert into mul-\ntiple chunks. When the prefetcher identifies the predicted experts\n(from predictor), it divides their parameters into several chunks\nand adds them to the prefetch queue as low-priority tasks. Each\ntask corresponds to one chunk of an expert‚Äôs parameters, rather\nthan the entire expert. This allows the worker thread to schedule\nlow-priority tasks at a smaller granularity. When a high-priority\nprefetch task arises, the worker thread can quickly switch to it,\nencountering a maximum delay of just one chunk.\nFigure 9 illustrates an example of chunked prefetching. The cache\nmiss for expert 2is triggered after the execution of expert 1. Since\nthe prefetcher is already working on a low-priority task, it must\nwait until this task completes before handling the high-priority\ntask of expert 2. With chunked prefetching, the low-priority task\nis divided into three chunks. The cache miss is triggered while\nthe prefetcher is working on the second chunk, allowing the high-\npriority task of expert 2to start immediately after the second chunk\nis completed. In practice, we found that experts in MoE models share\nthe same structure, consisting of three linear layers. Thus, ProMoE\nnaturally splits each expert into three chunks, corresponding to\nthese three linear layers. By implementing chunked prefetching,\nProMoE reduces the delay in starting high-priority prefetch tasks,\nthereby improving critical path latency.\n5.2 Early Preemption\nAlthough ProMoE ‚Äôs predictor aims to maximize prediction accu-\nracy, mispredictions are still unavoidable. This can result in nec-\nessary experts not being present in the GPU cache, triggering on-\ndemand copying of missing experts during the critical path. Tra-\nditionally, these misses are detected and addressed only when the\ncorresponding expert is accessed during inference, causing the in-\nference process to be blocked while waiting for the missing expert\nparameters to be copied from CPU memory to GPU. This leads to\nunder-utilization of the GPU and introduces high fetch latency in\nthe critical path of inference execution.\nTo tackle this issue, ProMoE proposes early preemption . We\nobserved that, in MoE models, the experts needed for the current\nlayer are determined all at once when the gate function completes.\n6\n\nAlgorithm 1 Prefetch Worker Thread\n1:while True do\n2: task‚Üêqueue.pop()\n3: iftask.chunk = 0 then\n4: evicted_expert‚Üêcache.replace_with(task.expert)\n5: evicted_expert.ready_chunk ‚Üê0\n6: end if\n7: cache_ptr‚Üêcache.get(task.expert)\n8: offset‚Üêtask.chunk√óchunk_size\n9: copy(cache_ptr + offset, task.host_ptr + offset, chunk_size)\n10: task.expert.ready_chunk ‚Üêtask.chunk + 1\n11:end while\nInstead of causing a cache miss each time an individual expert is ac-\ncessed, the system can preempt the prefetch queue in advance when\nit knows which experts will be activated after the gate function.\nThis allows the prefetching of any missing experts to begin much\nearlier, overlapping with the computation of the current layer. For\nexample, as shown in Figure 9, early preemption triggers the cache\nmiss for expert 2immediately after the gate function completes,\nrather than waiting until the completion of expert 1. As a result,\nthe high-priority task for expert 2is scheduled by the prefetcher\nbefore the second chunk of the low-priority task is processed.\nIn practice, ProMoE implements early preemption by inserting\na hook at the end of the gate function to obtain the list of required\nexperts in advance. These experts are then prioritized as high-\npriority tasks and added to the prefetch queue, ensuring that the\nprefetch thread prioritizes these tasks. During this process, there\nmay still be some low-priority speculative prefetch tasks for the\nsame layer that have not yet completed. However, since the system\nhas a precise list of the required experts, these low-priority tasks\ncan be discarded. The prefetch thread simply clears any remaining\nlow-priority speculative prefetch tasks for that layer, effectively\nachieving preemption.\nDuring inference, when encountering an expert that is not in the\ncache, ProMoE no longer triggers a cache miss. Instead, it waits for\nthe corresponding prefetch task to complete. As a result, all passive\ncache misses are transformed into proactive precise prefetching.\nThis approach allows for earlier initiation of accurate prefetching,\nwhich increases the overlap between prefetching and computation,\nultimately reducing latency on the critical path.\n5.3 Reordered Inference\nIn the inference process of LLMs, existing frameworks typically\nexecute computations for different experts in the order of their IDs.\nThis order often fails to fully utilize the cache status of experts,\nleading to unnecessary blocking and potential cache thrashing.\nConsider the example in Figure 9 where experts 1,4, and 5are\ncached, and expert 2is missing. Since the computations are executed\nbased on the order of expert ID, experts 4and 5must wait for the\nprefetch of expert 2to complete before they can start. Consequently,\nthe GPU remains underutilized while waiting for the prefetch of\nexpert 2, even though experts 4and5are already prefetched. More\ncritically, the prefetching of the missing expert might evict other\nsoon-to-be-accessed experts, causing cache thrashing. This issue isAlgorithm 2 Prefetcher Interface\n1:function PushPredictedExperts (layer, experts)\n2: foreinexperts do\n3: ife.ready_chunk > 0 then\n4: cache.hit(e)\n5: end if\n6: forchunk‚Üêe.ready_chunk tonum_chunks-1 do\n7: queue.push(Task(layer, e, chunk, LOW))\n8: end for\n9: end for\n10:end function\n11:function PushPreciseExperts (layer, experts)\n12: queue.remove_low_pri_task_with_layer(layer)\n13: experts‚Üêdesc_sort_by_ready_chunk(experts)\n14: foreinexperts do\n15: ife.ready_chunk > 0 then\n16: cache.hit(e)\n17: end if\n18: forchunk‚Üêe.ready_chunk tonum_chunks-1 do\n19: queue.push(Task(layer, e, chunk, HIGH))\n20: end for\n21: end for\n22: return experts\n23:end function\nparticularly severe when dealing with a large number of experts\nsequentially, such as during the prefill stage of inference.\nTo address this issue, ProMoE proposes reordered inference ,\nwhich alters the computation order of experts in a cache-aware\nmanner. We observe that in MoE models, the computation order\nof experts is interchangeable. There is no dependency between the\ncomputations of different experts because their outputs are simply\nsummed together. This property allows for adjusting the compu-\ntation order based on the cache and prefetch status, making the\ninference process more cache-friendly.\nSpecifically, once the gate function completes, ProMoE adjusts\nthe computation order accordingly. Experts already in the cache are\nprioritized first, followed by the experts currently being prefetched\n(if any), while experts whose prefetch has not yet begun are po-\nsitioned last. Consider the example in Figure 9. When the gate\nproduces experts 1,2,4, and 5, where expert 2is missing, ProMoE\nchanges the computation order to 1,4,5, and then 2. Therefore,\nthe prefetching of expert 2can be conducted in parallel with the\ncomputations of experts 4and 5, further reducing the impact of\nprefetching on the critical path.\nIn practice, the reordering process occurs simultaneously with\nearly preemption. After obtaining the list of experts to be accessed,\nProMoE first reorders them as described above. Experts whose\nprefetching is not yet complete are managed through early pre-\nemption and added to the prefetch queue as high-priority tasks.\nThe entire reordered sequence of experts is then returned to the\ninference framework for execution. This approach ensures that\nfor experts with incomplete prefetches, both the prefetch threads\nand inference threads process them in the same order, effectively\nestablishing a pipeline between computation and prefetching.\n7\n\n5.4 Prefetcher Workflow\nThe prefetcher‚Äôs workflow is summarized in Algorithms 1 and 2.\nAlgorithm 1 outlines the prefetcher‚Äôs worker thread, which contin-\nuously polls tasks from the queue and transfers expert parameters\nfrom host memory to GPU memory. Each task corresponds to a\nchunk of an expert‚Äôs parameters, thereby implementing chunked\nprefetching.\nThe Predictor and LLM framework interact with the prefetcher\nthrough the APIs outlined in Algorithm 2. The Predictor enqueues\npredicted experts as low-priority tasks using the PushPredictedExperts\nfunction, while the LLM framework enqueues the actually required\n(precise) experts as high-priority tasks with the PushPreciseExperts\nfunction after completing the gate function.\nWhen enqueuing high-priority tasks (precise experts), the system\nfirst clears any existing low-priority tasks from the queue (Line 12)\nto enable early preemption. The remaining precise experts are then\nreordered based on their current fetch status (Line 13). Subsequently,\nthe inference framework executes the experts according to this new\nordering (Line 22), thereby implementing reordered inference.\n6 Implementation\nProMoE is implemented as an extension to LLM frameworks, com-\nprising 6,600 lines of C++ code.\n6.1 Cache Implementation\nFor simplicity, the cache component of ProMoE is implemented\nas a standard per-layer LRU cache. Both prefetching and inference\ntrigger a cache access. When adding prefetch tasks, ProMoE lever-\nages LRU by accessing experts that are already cached, thereby\ndelaying their eviction. To reduce memory fragmentation, ProMoE\npre-allocates the expert cache as a contiguous memory region.\n6.2 System Integration\nWe have integrated ProMoE into two popular LLM frameworks:\ntransformers and llama.cpp. To achieve this integration, we added\nhooks to capture input logits from the MoE layers and to reorder\nexperts. We also implemented a dependency mechanism to ensure\nefficient prefetching and computation. Furthermore, ProMoE takes\nover the memory management for expert parameters. We did not\nintegrate ProMoE with frameworks like vLLM and TGI due to their\ninadequate support for quantized MoE at the time of submission.\nMoreover, these frameworks primarily focus on batched inference\nfor data centers and fuse the execution of multiple experts to en-\nhance GPU utilization. However, this optimization assumes that\nall experts are ready before computation can begin, which is atypi-\ncal on memory-constrained GPU platforms that ProMoE targets.\nIt necessitates additional GPU memory for activated experts and\nhampers the overlap between expert loading and execution.\n6.3 Training of Predictor\nEach layer‚Äôs predictor in ProMoE is implemented as a two-layer\nmulti-layer perceptron (MLP) with approximately 2 million param-\neters. The training of the learned predictor and the data collection\nfor training takes less than 1‚Äì2 hours on a single GPU. This is a\none-time offline task that can be parallelized across multiple GPUs.\nGiven the extensive pre-training times of large language models\n(LLMs), we consider this time commitment acceptable.7 Evaluation\n7.1 Experimental Setup\nHardware . The evaluation is conducted on a PC equipped with\nan NVIDIA RTX 4090 GPU (24 GB GDDR6X). The PC also fea-\ntures an Intel i9-14900K CPU and 128 GB of host DRAM. The GPU\nis connected to the CPU via PCIe 4.0, providing a unidirectional\nbandwidth of 32 GB/s. To simulate GPUs with varying memory\ncapacities, we include an evaluation in ¬ß7.4 that adjusts the cache\nratio to control memory occupancy.\nWorkload . We evaluated a broad range of MoE-based LLMs, as\nlisted in Table 1. By default, we evaluate DS-1, DS-2, and QW-1 using\nFP16 precision, while QW-2 and Mixt are evaluated using INT4\nprecision. To further study the impact of model size, we also include\nan evaluation in¬ß7.6 that varies the parameter size of the same\nmodel from FP16 to INT4. The evaluation utilizes the shareGPT\ndataset [ 3], which consists of user interactions with ChatGPT and\nserves as a representative example of real LLM services. We also\nconducted evaluations using the Alpaca dataset [ 43] and observed\nsimilar performance trends; results for this dataset are omitted due\nto space limitations. By default, we set the batch size to 1 to reflect\nedge deployment scenarios, and we include an evaluation in ¬ß7.5\nthat varies the batch size from 1 to 4.\nBaselines . Our evaluation relies on two well-known codebases:\nHugging Face transformers [ 23,45] and llama.cpp [ 22]. Transform-\ners supports a wide range of models and is easy to deploy, but it\nlacks optimal inference performance. We enhanced the efficiency of\nthe MoE block by reducing CPU-GPU synchronization. Llama.cpp,\nwhich is written in C++, delivers state-of-the-art inference perfor-\nmance by eliminating overhead from the Python interpreter.\nBoth systems offer offloading baselines: transformers offloads\nonly the parameters to the CPU (referred to as TO), while llama.cpp\noffloads both parameters and computations (referred to as LO). We\nimproved TO by incorporating pinned memory and asynchronous\ncopies. Additionally, we integrated ProMoE into both codebases\nand introduced three baselines: Unified Memory ( UM),static cache,\nandLRU cache. These baselines, along with ProMoE , manage only\nexpert parameters, while non-expert parameters consistently reside\non the GPU. The UM baseline is optimized using cudaMemAdvise to\nenable instantaneous page invalidation without incurring the cost\nof swapping pages back to CPU memory. The static baseline caches\na fixed set of experts and utilizes two additional expert buffers to\nload any missing experts.\nMetrics We evaluate the performance of ProMoE and its base-\nlines in the prefill and decode stages separately. The prefill stage\nperformance is assessed by TTFT (Time To First Token), which\nreflects the latency in processing the user‚Äôs prompt. The decode\nstage performance is measured using TPS (Tokens Per Second)\nand TPOT (Time Per Output Token), indicating the throughput\nand latency of the decoding process. We primarily report TPS as\nit is more intuitive and switch to TPOT for detailed breakdown\nanalyses. The total latency for a single request can be expressed\nasùêøùëéùë°ùëíùëõùëêùë¶ ùë°ùëúùë°ùëéùëô =TTFT+ùëÅ√óTPOT (where ùëÅis output length).\nWe measure the prefill and decode stages separately for two main\nreasons: (1) the significant variance in output lengths (ranging from\ntens to thousands of tokens) renders aggregated metrics unreliable\n8\n\nDS-1 DS-2 QW-1 QW-2 Mixt0.00.10.20.30.40.50.60.70.80.9TTFT (s)TO UM Static LRU ProMoE\nDS-1 DS-2 QW-1 QW-2 Mixt05101520253035404550TPSFigure 10: The overall performance of (a) prefill and (b) decode stage\nin transformers codebase.\nfor system comparisons, and (2) the prefill and decode stages exhibit\ndistinct computational patterns (e.g. more experts are activated in\nthe prefill stage).\n7.2 Overall Performance\nFigure 10 shows the overall performance of the prefill and decode\nstages within the transformers codebase. In the prefill stage, Pro-\nMoE outperforms static and LRU baselines by an average of 1.42 √ó\n(up to 1.61√ó) and 2.21√ó(up to 2.48√ó), respectively. The improvement\nofProMoE primarily stem from its prefetching technique, which\nmaximizes the overlap between loading parameters and computa-\ntion. When comparing ProMoE with LRU, the greater improvement\nis attributed to the cache thrashing caused by LRU (see ¬ß5.3). In\nthe prefill stage, nearly all experts are accessed since each token\nusually requires a different set of experts. As experts are accessed\naccording to their IDs, LRU evicts a cached expert with a higher\nID when it accesses a missing expert with a smaller ID first. The\nstatic cache avoids thrashing by fixing its cache set, while ProMoE\nintelligently reorders experts to minimize thrashing and reduce the\nblocking time caused by missing experts on the critical path.\nIn the decode stage, ProMoE outperforms the static and LRU\nbaselines by an average of 1.47 √ó(up to 1.77√ó) and 1.31√ó(up to\n1.46√ó), respectively. LRU outperforms the static cache during the\ndecode stage because cache thrashing occurs less frequently, and\nthere is some reuse of experts across iterations. ProMoE excels\nover these baselines by keeping most copies of missing experts off\nthe critical path through effective prefetching.\nThe TO (resp. UM) baseline consistently perform worse than the\nstatic (resp. LRU) baseline. Comparing to these baselines, ProMoE\nachieves a average speedup of 2.15 √ó(up to 2.78√ó) in the prefill stage\nand 2.47√ó(up to 5.02√ó) in the decode stage. This performance gap\narises because the static and LRU baselines can be seen as improved\nimplementations of static and dynamic cache, respectively. In static\ncache, the TO baseline offloads non-expert parameters to the CPU,\nwhile the static baseline only offloads parameters of the experts.\nIn dynamic cache, the UM baseline fetches parameters at the page\nlevel, which increases the volume of transferred data compared to\nDS-1 DS-2 QW-1 QW-2 Mixt0.00.10.20.30.40.50.60.70.80.91.0TTFT (s)LO UM Static LRU ProMoE\nDS-1 DS-2 QW-1 QW-2 Mixt01020304050607080TPSFigure 11: The overall performance of (a) prefill and (b) decode stage\nin llama.cpp codebase.\nthe LRU baseline. Therefore, in subsequent experiments, we mainly\nfocus on comparing the static, LRU, and ProMoE .\nFigure 11 shows the overall performance in the llama.cpp code-\nbase. ProMoE surpasses the static and LRU baselines by an average\nof 1.36√ó(up to 1.75√ó) and 2.12√ó(up to 2.22√ó) in the prefill stage,\nand by 1.49√ó(up to 1.79√ó) and 1.09√ó(up to 1.17√ó) in the decode\nstage, respectively. The improvement in the llama.cpp codebase\nfollows the same trend observed in transformers. However, it is less\npronounced due to the removal of the Python interpreter overhead\nduring inference, which provides fewer opportunities for ProMoE\nto prefetch experts.\nAs expected, the UM baseline consistently performs worse than\nthe LRU baseline. The LO baseline in llama.cpp offloads both param-\neters and computations to the CPU, resulting in slower performance\nthan that of the static baseline. Compared to these baselines, Pro-\nMoE achieves an average speedup of 2.25 √ó(up to 3.21√ó) in the\nprefill stage and 1.66 √ó(up to 2.07√ó) in the decode stage. However,\nwhen evaluating the Mixt model, the LO baseline is significantly\nfaster and even surpasses ProMoE in the decode stage. This is\nbecause the Mixt model activates a larger ratio of experts (25%) for\neach token, increasing the cost of fetching parameters to the GPU\ncompared with directly computing them on the CPU. We believe\nthis does not undermine the significance of our work, as most re-\ncently released MoE-based LLMs typically activate a smaller ratio\nof experts (averaging 10%), and the TO baseline continues to show\ninferior performance across most cases.\n7.3 Ablation Study\nFigure 12 shows the performance of transformers with different\noptimizations enabled in ProMoE , starting from the LRU baseline.\nDuring the prefill stage, enabling prefetching shows minimal im-\nprovement and may even degrade performance. This is because\nnearly all experts are accessed, and prefetching alone merely re-\nplaces the cache set. Additionally, naive prefetching delays the\nhandling of missing experts. The techniques of early preemption\nand reordered inference provide significant improvements, yielding\nspeedups of 1.27√óand 2.39√ócompared to the baseline, respectively.\n9\n\nDS-1 QW-20.00.10.20.30.40.50.60.70.8TTFT (s)\nDS-1 QW-20481216202428323640TPSbase +p +p+c +p+c+e +p+c+e+rFigure 12: The ablation study of (a) prefill and (b) decode stage\nin transformers codebase with different optimizations in ProMoE\nenabled. Base represents the LRU baseline, with prefetch, chunked-\nprefetch, e arly-preemption and r eordered-inference applied.\nDS-1 QW-20.00.10.20.30.40.50.60.7TTFT (s)\nDS-1 QW-2051015202530354045TPSbase +p +p+c +p+c+e +p+c+e+r\nFigure 13: The ablation study in llama.cpp codebase, following the\nsame setup and naming convention as Figure 12.\nIn the decode stage, these techniques gradually enhance perfor-\nmance, resulting in a 1.35 √óincrease over the baseline. Figure 13\npresents an ablation study for the llama.cpp. The trends is similar to\nthose observed in the transformers, except that in the prefill stage,\nmost of the improvement is attributed to the reordered inference.\n7.4 Impact of Cache Rate\nTo examine the impact of GPU memory capacity on ProMoE ‚Äôs per-\nformance, we varied the cache rate to control the memory occupied\nby the expert cache. Figure 14 and 15 show the performance of\nprefill and decode stages of systems in the transformers codebase,\nwith DS-1 and QW-2 models using different cache rates. During the\nprefill stage, LRU performs the worst due to cache thrashing, while\nProMoE outperforms LRU on the DS-1 and QW-2 models by 1.72 √ó\n(up to 2.36√ó) and 1.82√ó(up to 2.28√ó) on average, respectively. Com-\npared to static caching, ProMoE achieves speedups of 1.22 √óand\n1.39√óon average in the prefill stages of these two models, respec-\ntively. The enhancement in the QW-2 model is more pronounced\ndue to its increased computation during inference, allowing more\nopportunities for ProMoE to prefetch experts. Figure 15 also shows\nthe breakdown of time spent loading parameters on the critical\npath. ProMoE reduces the loading time on the critical path from\n69.68% to 30.96% in the QW-2 model as the cache rate increases,\nwhereas the static cache still suffers from a reduction of only 77.44%\nto 56.04%. In the decode stage, ProMoE outperforms both static\nand LRU baselines by 1.60 √óand 1.29√óon average, respectively.\nProMoE decreases the loading time on the critical path to 25.61%\nand 29.20% for the DS-1 and QW-2 models, while LRU (the fast\nbaseline) continues to endure loading times on the critical path of\n45.52% and 50.89%, respectively.\nWe conducted similar experiments on the llama.cpp codebase,\nusing layer-offloading (LO) included as a baseline. The results are\n10 20 30 40 50 60 70\nExpert Cache Rate (%)02004006008001000 TTFT (ms)\n10 20 30 40 50 60 70\nExpert Cache Rate (%)020406080100120140 TPOT (ms)Static-GPU\nStatic-PCIeLRU-GPU\nLRU-PCIeProMoE-GPU\nProMoE-PCIeFigure 14: The (a) TTFT and (b) TPOT of systems in transformers\ncodebase with DS-1 model using different cache rates.\n10 20 30 40 50 60\nExpert Cache Rate (%)02004006008001000 TTFT (ms)\n10 20 30 40 50 60\nExpert Cache Rate (%)020406080100120140160 TPOT (ms)Static-GPU\nStatic-PCIeLRU-GPU\nLRU-PCIeProMoE-GPU\nProMoE-PCIe\nFigure 15: The (a) TTFT and (b) TPOT of systems in transformers\ncodebase with QW-2 model using different cache rates.\n10 20 30 40 50 60 70\nRate of Params in GPU (%)040080012001600 TTFT (ms)\n10 20 30 40 50 60 70\nRate of Params in GPU (%)0306090120150 TPOT (ms)\nLO-GPU\nLO-PCIeStatic-GPU\nStatic-PCIeLRU-GPU\nLRU-PCIeProMoE-GPU\nProMoE-PCIe\nFigure 16: The (a) TTFT and (b) TPOT of systems in llama.cpp\ncodebase with QW-2 model using different cache rates.\nshown in Figure 16. In this case, the speedup of ProMoE over\nthe fast baseline is reduced due to the faster inference speed of\nthe llama.cpp codebase. ProMoE achieves performance improve-\nments of 1.53√ó(resp. 1.14√ó) and 1.10√ó(resp. 1.27√ó) over LRU and\nstatic baselines on average during the prefill (resp. decode) stage,\nrespectively. Notably, in the decode stage with a low cache rate,\nLO outperforms the other systems. Under low cache rates, the\ncache-based systems must fetch a significant number of experts\nthrough PCIe, while the limited computation makes offloading to\nthe CPU more advantageous. As the cache rate increases, however,\nthe cache-based systems quickly surpass LO.\n7.5 Impact of Batch Size\nWe also evaluated the impact of batch size on the performance\nofProMoE . Figure 17 and 18 show the throughput of systems in\n10\n\n1 2 3 4\nBatch Size024681012Prefill Throughput (prompts/s)\n1 2 3 4\nBatch Size01020304050Decode Throughput (tokens/s)\nLO UM Static LRU ProMoEFigure 17: The (a) prefill and (b) decode throughput of systems in\nllama.cpp codebase with DS-1 model as the batch size changes.\n1 2 3 4\nBatch Size0246810121416Prefill Throughput (prompts/s)\n1 2 3 4\nBatch Size0510152025303540Decode Throughput (tokens/s)\nLO UM Static LRU ProMoE\nFigure 18: The (a) prefill and (b) decode throughput of systems in\nllama.cpp codebase with QW-2 model as the batch size changes.\n1 2 3 4\nBatch Size0100200300400500600700800 TTFT (ms)\n1 2 3 4\nBatch Size020406080100120140160 TPOT (ms)LO-GPU\nLO-PCIeStatic-GPU\nStatic-PCIeLRU-GPU\nLRU-PCIeProMoE-GPU\nProMoE-PCIe\nFigure 19: The (a) TTFT and (b) TPOT of systems in llama.cpp\ncodebase with QW-2 model as the batch size changes.\nthe llama.cpp codebase with DS-1 and QW-2 models as the batch\nsize varies. During the prefill stage, throughput increases linearly\nwith the batch size. This linear growth occurs because the time\nis primarily dominated by loading all experts, and the increased\ncomputation associated with a larger batch size is almost ‚Äúfree‚Äù.\nThis is supported by Figure 19(a), which shows the time breakdown\nof the prefill stage for the QW-2 model. As the batch size increases,\nthe latency for one iteration in the prefill stage remains relatively\nstable. On average, ProMoE outperforms LRU and static baselines\nby 2.19√óand 1.19√ó, respectively, in the prefill stage.\nIn the decode stage, the number of experts activated grows al-\nmost linearly with the batch size. This rapid increase in latency\nper iteration during the decode stage limits the improvement of\nthroughput as the batch size increases. In this context, ProMoE\noutperforms both the LRU and static baselines by averages of 1.22 √ó\nand 1.59√ó, respectively. The improvement of ProMoE over LRU\ngrows progressively with increasing batch sizes. For instance, in the\n1 2 3 4\nBatch Size0246810Prefill Throughput (prompts/s)\n1 2 3 4\nBatch Size05101520253035Decode Throughput (tokens/s)\nTO UM Static LRU ProMoEFigure 20: The (a) prefill and (b) decode throughput of systems in\ntransformers codebase with QW-2 model as the batch size changes.\n1 2 3 4\nBatch Size020040060080010001200TTFT (ms)\n1 2 3 4\nBatch Size04080120160200240 TPOT (ms)Static-GPU\nStatic-PCIeLRU-GPU\nLRU-PCIeProMoE-GPU\nProMoE-PCIe\nFigure 21: The (a) TTFT and (b) TPOT of systems in transformers\ncodebase with QW-2 model as the batch size changes.\nf16 int8 int4\nPer Parameter Size (bit)0100200300400500600700TTFT (ms)\nf16 int8 int4\nPer Parameter Size (bit)051015202530354045TPOT (ms)LO-GPU\nLO-PCIeStatic-GPU\nStatic-PCIeLRU-GPU\nLRU-PCIeProMoE-GPU\nProMoE-PCIe\nFigure 22: The (a) TTFT and (b) TPOT of systems in llama.cpp\ncodebase with DS-1 model using different bits per weight.\nQW-2 model, the speedup of ProMoE over LRU is 1.16√ówhen the\nbatch size is 1 and increases to 1.34 √ówhen the batch size reaches 4.\nThis improvement is attributed to cache thrashing that occurs as\nthe batch size grows.\nWe further illustrate the impact of batch size in the transformers\ncodebase in Figure 20 and 21. Here, ProMoE outperforms LRU and\nstatic baselines by averages of 2.47 √ó(1.48√ó) and 1.54√ó(1.87√ó) in the\nprefill (decode) stage, respectively. The higher speedup is a result\nof longer computation times in the transformers codebase, which\nprovides ProMoE with more opportunities to perform additional\nprefetches.\n7.6 Impact of Model Size\nTo understand how model size affects ProMoE ‚Äôs performance, we\nvaried the number of bits per weight (BPW) from 16 to 4 for the same\nmodel. The variance in BPW impacts the model‚Äôs total memory\nfootprint while keeping the amount of computation, measured in\n11\n\nf16 int8 int4\nPer Parameter Size (bit)0100020003000400050006000 TTFT (ms)\nf16 int8 int4\nPer Parameter Size (bit)0100200300400500600 TPOT (ms)LO-GPU\nLO-PCIeStatic-GPU\nStatic-PCIeLRU-GPU\nLRU-PCIeProMoE-GPU\nProMoE-PCIeFigure 23: The (a) TTFT and (b) TPOT of systems in llama.cpp\ncodebase with QW-2 model using different bits per weight.\nf16 int8 int4\nPer Parameter Size (bit)0100200300400500600700800 TTFT (ms)\nf16 int8 int4\nPer Parameter Size (bit)0102030405060 TPOT (ms)Static-GPU\nStatic-PCIeLRU-GPU\nLRU-PCIeProMoE-GPU\nProMoE-PCIe\nFigure 24: The (a) TTFT and (b) TPOT of systems in transformers\ncodebase with DS-1 model using different bits per weight.\nFLOPs (floating-point operations), constant. This variability can\nalter the relative speed of parameter loading and computation.\nFigure 22 and 23 present the results for the DS-1 and QW-2\nmodels within the llama.cpp codebase. In the DS-1 model, we main-\ntained a fixed cache rate to keep a consistent ratio of parameters\nstored in the GPU. The GPU memory occupancy decreases with\nlower BPW values. As shown in Figure 22, the relative performance\nremains stable despite changes in BPW. ProMoE achieves an aver-\nage speedup of 2.05 √ó(1.21√ó) and 1.29√ó(1.74√ó) over LRU and static\nbaselines during the prefill (decode) stage, respectively.\nIn the QW-2 model, we reduce the cache rate as BPW increases\nfrom the default 4-bit to 16-bit to ensure the model fits within\nour 24 GB GPU memory. The decreased cache rate and increased\nmemory footprint per expert limit ProMoE ‚Äôs improvement. For\nexample, at INT4, ProMoE outperforms LRU by 2.06 √óin the prefill\nstage while the speedup drops to 1.105 √óat FP16.\nWe also conducted similar experiments in the transformers code-\nbase using the DS-1 model, as illustrated in Figure 24. The quan-\ntization was performed using the mainstream method GPTQ [ 21].\nIn this scenario, ProMoE effectively overlaps the prefetching of\nexperts with computations as BPW decreases.\n8 Related Work\nServing MoE-based LLMs with limited resources. Pre-gated\nMoE [ 26] modifies the computation flow of MoE models by provid-\ning gate results for the subsequent layer from the previous layer\ndirectly, allowing accurate layer-wise prefetching by determining\nthe required experts in advance. SwapMoE [30] maintains a set of\nimportant experts in the GPU memory, using only these duringinference to prevent offloading overhead. In the background, it\ndynamically adjusts this set based on workload changes. However,\nthese systems alter the original MoE model computation, inevitably\naffecting model accuracy. In contrast, ProMoE performs compu-\ntations that are equivalent to the original model, accelerating the\ninference of MoE-based LLMs on edge devices without compromis-\ning accuracy.\nMixtral-offloading [ 18] implements an LRU cache for the Mix-\ntral MoE model and introduces a skip-based prediction method\nto support expert prefetching. Brainstorm [ 14] designs a router\nabstraction to capture the dynamic aspects of models and proposes\nspeculative loading and execution based on static skewness sta-\ntistics. MoE-infinity [ 48] develops an Expert Activation Tracing\nmechanism for sequence-level prediction to facilitate prefetching,\nspecifically designed for MoE-based encoder-decoder LLMs and\naimed at throughput-oriented inference. In contrast, ProMoE uti-\nlizes a learned predictor with high GoodPred , focusing on latency-\noriented inference for edge devices.\nLLM serving on resource-constrained devices. Most modern\nframeworks [ 6,22,25,32,45] for serving LLMs provide basic of-\nfloading support that utilizes the CPU to handle parameters or\ncomputations, thereby reducing the GPU memory requirements.\nFlexGen [ 40] aggregates memory and computation resources from\nthe GPU, CPU, and disk. It optimizes tensor storage and access\npatterns while also compressing weights and the attention cache.\nThese frameworks mainly target general LLMs and emphasize\nthroughput-oriented inference with large batch sizes. Model quan-\ntization [ 9,21,35] and pruning [ 20,42] are techniques to reduce\nthe memory requirements of LLMs. DejaVu [ 36] takes advantage\nof contextual sparsity in LLMs to lower inference costs with mini-\nmal impact on model quality. It employs a low-cost algorithm to\npredict input-dependent sparse subsets of attention heads and MLP\nparameters on-the-fly, which reduces the number of parameters\nneeded during inference. Building on DejaVu, PowerInfer [ 41] uti-\nlizes the power-law distribution of neuron activations in LLMs,\npreloading frequently activated ‚Äúhot‚Äù parameters onto the GPU\nwhile processing less active ‚Äúcold‚Äù parameters on the CPU. Pro-\nMoE is orthogonal to these techniques and can be integrated with\nthem to further minimize memory usage and enhance inference\nspeed.\nGeneric LLM serving optimization. The rising demand for LLMs\nhas prompted various system optimizations [ 2,11,16,34,37,46] to\nimprove their performance and efficiency. vLLM [ 32] introduces\nPagedAttention, which manages the key-value cache for LLM serv-\ning and allows for sharing the cache across requests. This improves\nbatching efficiency and reduces memory fragmentation. Orca [ 51]\nproposes continuous and selective batching to optimize the per-\nformance of batched LLM serving. While these systems focus on\nenhancing batched LLM serving in the cloud environment, Pro-\nMoE is designed specifically for low-latency, single-request LLM\ninference on edge devices.\n9 Conclusion\nThis paper presents ProMoE , a proactive caching system that en-\nhances expert offloading for MoE-based LLMs. ProMoE leverages a\nlearned predictor and carefully coordinates prefetching with infer-\nence. Our evaluation shows the efficacy and efficiency of ProMoE .\n12\n\nReferences\n[1] 2023. mistralai/Mixtral-8x7B-Instruct-v0.1 ¬∑Hugging Face . https://huggingface.\nco/mistralai/Mixtral-8x7B-Instruct-v0.1\n[2]2023. NVIDIA/TensorRT-LLM . https://github.com/NVIDIA/TensorRT-LLM\noriginal-date: 2023-08-16T17:14:27Z.\n[3] 2024. anon8231489123/ShareGPT_Vicuna_unfiltered ¬∑Datasets at Hugging Face.\n[4] 2024. deepseek-ai/deepseek-moe-16b-chat ¬∑Hugging Face . https://huggingface.co/\ndeepseek-ai/deepseek-moe-16b-chat\n[5] 2024. deepseek-ai/DeepSeek-V2-Lite-Chat ¬∑Hugging Face . https://huggingface.co/\ndeepseek-ai/DeepSeek-V2-Lite-Chat\n[6] 2024. huggingface/text-generation-inference. https://github.com/huggingface/\ntext-generation-inference\n[7] 2024. Qwen/Qwen1.5-MoE-A2.7B-Chat ¬∑Hugging Face . https://huggingface.co/\nQwen/Qwen1.5-MoE-A2.7B-Chat\n[8] 2024. Qwen/Qwen2-57B-A14B-Instruct ¬∑Hugging Face . https://huggingface.co/\nQwen/Qwen2-57B-A14B-Instruct\n[9]Hicham Badri and Appu Shaji. 2023. Half-Quadratic Quantization of Large\nMachine Learning Models. https://mobiusml.github.io/hqq_blog/\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al .2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877‚Äì1901.\n[11] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, and Tri Dao. 2023.\nMedusa: Simple Framework for Accelerating LLM Generation with Multiple\nDecoding Heads. https://github.com/FasterDecoding/Medusa.\n[12] Byung-Gon Chun, Sunghwan Ihm, Petros Maniatis, Mayur Naik, and Ashwin\nPatti. 2011. CloneCloud: elastic execution between mobile device and cloud. In\nProceedings of the Sixth Conference on Computer Systems (Salzburg, Austria) (Eu-\nroSys ‚Äô11) . Association for Computing Machinery, New York, NY, USA, 301‚Äì314.\nhttps://doi.org/10.1145/1966445.1966473\n[13] Marta R Costa-juss√†, James Cross, Onur √áelebi, Maha Elbayad, Kenneth Heafield,\nKevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al .\n2022. No language left behind: Scaling human-centered machine translation.\narXiv preprint arXiv:2207.04672 (2022).\n[14] Weihao Cui, Zhenhua Han, Lingji Ouyang, Yichuan Wang, Ningxin Zheng, Lingx-\niao Ma, Yuqing Yang, Fan Yang, Jilong Xue, Lili Qiu, Lidong Zhou, Quan Chen,\nHaisheng Tan, and Minyi Guo. 2023. Optimizing Dynamic Neural Networks with\nBrainstorm. 797‚Äì815. https://www.usenix.org/conference/osdi23/presentation/\ncui\n[15] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen,\nJiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang,\nFuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. 2024. DeepSeekMoE:\nTowards Ultimate Expert Specialization in Mixture-of-Experts Language Models.\nhttps://doi.org/10.48550/arXiv.2401.06066 arXiv:2401.06066\n[16] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. 2022.\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.\nhttps://doi.org/10.48550/arXiv.2205.14135 arXiv:2205.14135\n[17] DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang\nZhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli\nChen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting\nChen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui\nDing, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong\nGuo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai\nDong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang\nZhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang,\nMinghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang,\nQihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan,\nRunxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen,\nShaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping\nYu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L.\nXiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao\nZhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan\nWang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun,\nXiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu\nYang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong\nXu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi\nZheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi\nPiao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen\nZhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang\nYou, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen\nZhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu,\nZhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie.\n2024. DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts\nLanguage Model. https://doi.org/10.48550/arXiv.2405.04434 arXiv:2405.04434\n[18] Artyom Eliseev and Denis Mazur. 2023. Fast Inference of Mixture-of-Experts\nLanguage Models with Offloading. arXiv:2312.17238 [cs.LG] https://arxiv.org/\nabs/2312.17238[19] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers:\nScaling to trillion parameter models with simple and efficient sparsity. Journal\nof Machine Learning Research 23, 120 (2022), 1‚Äì39.\n[20] Elias Frantar and Dan Alistarh. 2023. SparseGPT: Massive Language Models Can\nBe Accurately Pruned in One-Shot. arXiv:2301.00774 http://arxiv.org/abs/2301.\n00774\n[21] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ:\nAccurate Post-Training Quantization for Generative Pre-trained Transformers.\nhttps://doi.org/10.48550/arXiv.2210.17323 arXiv:2210.17323\n[22] Georgi Gerganov. 2024. ggerganov/llama.cpp. https://github.com/ggerganov/\nllama.cpp original-date: 2023-03-10T18:58:00Z.\n[23] Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller,\nSourab Mangrulkar, Marc Sun, and Benjamin Bossan. 2022. Accelerate: Training\nand inference at scale made simple, efficient and adaptable. https://github.com/\nhuggingface/accelerate.\n[24] Mingzhe Hao, Levent Toksoz, Nanqinqin Li, Edward Edberg Halim, Henry Hoff-\nmann, and Haryadi S. Gunawi. 2020. {LinnOS}: Predictability on Unpredictable\nFlash Storage with a Light Neural Network. 173‚Äì190. https://www.usenix.org/\nconference/osdi20/presentation/hao\n[25] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff\nRasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash\nBakhtiari, Lev Kurilenko, and Yuxiong He. 2024. DeepSpeed-FastGen: High-\nthroughput Text Generation for LLMs via MII and DeepSpeed-Inference. https:\n//doi.org/10.48550/arXiv.2401.08671 arXiv:2401.08671\n[26] Ranggi Hwang, Jianyu Wei, Shijie Cao, Changho Hwang, Xiaohu Tang, Ting\nCao, and Mao Yang. 2024. Pre-gated MoE: An Algorithm-System Co-Design for\nFast and Scalable Mixture-of-Expert Inference. In 2024 ACM/IEEE 51st Annual\nInternational Symposium on Computer Architecture (ISCA) . 1018‚Äì1031. https:\n//doi.org/10.1109/ISCA59077.2024.00078\n[27] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton.\n1991. Adaptive Mixtures of Local Experts. 3, 1 (1991), 79‚Äì87. https://doi.org/10.\n1162/neco.1991.3.1.79 Conference Name: Neural Computation.\n[28] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche\nSavary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou\nHanna, Florian Bressand, et al .2024. Mixtral of experts. arXiv preprint\narXiv:2401.04088 (2024).\n[29] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling Laws for Neural Language Models. https://doi.org/10.48550/arXiv.2001.\n08361 arXiv:2001.08361\n[30] Rui Kong, Yuanchun Li, Qingtian Feng, Weijun Wang, Xiaozhou Ye, Ye Ouyang,\nLinghe Kong, and Yunxin Liu. 2024. SwapMoE: Serving Off-the-shelf MoE-based\nLarge Language Models with Tunable Memory Budget. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers) . 6710‚Äì6720.\n[31] Tim Kraska, Alex Beutel, Ed H. Chi, Jeff Dean, and Neoklis Polyzotis. 2018. The\nCase for Learned Index Structures. https://arxiv.org/abs/1712.01208\n[32] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,\nCody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nMemory Management for Large Language Model Serving with PagedAttention.\nInProceedings of the 29th Symposium on Operating Systems Principles (New York,\nNY, USA, 2023-10-23) (SOSP ‚Äô23) . Association for Computing Machinery, 611‚Äì626.\nhttps://doi.org/10.1145/3600006.3613165\n[33] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. 2024. InfiniGen:\nEfficient Generative Inference of Large Language Models with Dynamic KV\nCache Management. In 18th USENIX Symposium on Operating Systems Design\nand Implementation (OSDI 24) . USENIX Association, Santa Clara, CA, 155‚Äì172.\nhttps://www.usenix.org/conference/osdi24/presentation/lee\n[34] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast Inference from\nTransformers via Speculative Decoding. https://doi.org/10.48550/arXiv.2211.\n17192 arXiv:2211.17192\n[35] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen\nWang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2023. AWQ:\nActivation-aware Weight Quantization for LLM Compression and Acceleration.\nhttps://doi.org/10.48550/arXiv.2306.00978 arXiv:2306.00978\n[36] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, An-\nshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, and Beidi Chen.\n2023. Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time. In Pro-\nceedings of the 40th International Conference on Machine Learning (2023-07-03).\nPMLR, 22137‚Äì22176. https://proceedings.mlr.press/v202/liu23am.html ISSN:\n2640-3498.\n[37] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang,\nZhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi,\nChunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhi-\nhao Jia. 2024. SpecInfer: Accelerating Large Language Model Serving with\nTree-based Speculative Inference and Verification. In Proceedings of the 29th\n13\n\nACM International Conference on Architectural Support for Programming Lan-\nguages and Operating Systems, Volume 3 (La Jolla, CA, USA) (ASPLOS ‚Äô24) . As-\nsociation for Computing Machinery, New York, NY, USA, 932‚Äì949. https:\n//doi.org/10.1145/3620666.3651335\n[38] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2024. Training\nlanguage models to follow instructions with human feedback. In Proceedings\nof the 36th International Conference on Neural Information Processing Systems\n(New Orleans, LA, USA) (NIPS ‚Äô22) . Curran Associates Inc., Red Hook, NY, USA,\nArticle 2011, 15 pages.\n[39] Noam M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le,\nGeoffrey E. Hinton, and Jeff Dean. 2017. Outrageously Large Neural Networks:\nThe Sparsely-Gated Mixture-of-Experts Layer. ArXiv abs/1701.06538 (2017).\nhttps://api.semanticscholar.org/CorpusID:12462234\n[40] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y.\nFu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang,\nChristopher R√©, Ion Stoica, and Ce Zhang. 2023. FlexGen: High-Throughput\nGenerative Inference of Large Language Models with a Single GPU. https:\n//doi.org/10.48550/arXiv.2303.06865 arXiv:2303.06865\n[41] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. 2023. PowerInfer: Fast Large\nLanguage Model Serving with a Consumer-grade GPU . https://arxiv.org/abs/2312.\n12456v1\n[42] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2023. A Simple and\nEffective Pruning Approach for Large Language Models. arXiv:2306.11695\nhttp://arxiv.org/abs/2306.11695\n[43] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An\nInstruction-following LLaMA model. https://github.com/tatsu-lab/stanford_\nalpaca.\n[44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al .2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[45] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement De-\nlangue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language\nProcessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations . Association for Computational Lin-\nguistics, Online, 38‚Äì45. https://www.aclweb.org/anthology/2020.emnlp-demos.6\n[46] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei\nQiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. 2023. Flash-LLM: Enabling\nCost-Effective and Highly-Efficient Large Generative Model Inference with Un-\nstructured Sparsity. arXiv:2309.10285 [cs.DC]\n[47] Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou,\nand Yang You. 2024. OpenMoE: An Early Effort on Open Mixture-of-Experts\nLanguage Models. In Forty-first International Conference on Machine Learning .\n[48] Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, and Mahesh Marina. 2024. MoE-Infinity:\nOffloading-Efficient MoE Model Serving. https://doi.org/10.48550/arXiv.2401.\n14361 arXiv:2401.14361\n[49] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Cheng-\npeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei,\nHuan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang\nLin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue,\nNa Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie\nWang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin\nGe, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei,\nXuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan,\nYunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao\nFan. 2024. Qwen2 Technical Report. https://doi.org/10.48550/arXiv.2407.10671\narXiv:2407.10671 version: 4.\n[50] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei\nXu. 2023. EdgeMoE: Fast On-Device Inference of MoE-based Large Language\nModels. https://doi.org/10.48550/arXiv.2308.14352 arXiv:2308.14352\n[51] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-\nGon Chun. 2022. Orca: A Distributed Serving System for Transformer-Based\nGenerative Models. In 16th USENIX Symposium on Operating Systems Design\nand Implementation (OSDI 22) . USENIX Association, Carlsbad, CA, 521‚Äì538.\nhttps://www.usenix.org/conference/osdi22/presentation/yu\n[52] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al .2022. Opt:\nOpen pre-trained transformer language models. arXiv preprint arXiv:2205.01068\n(2022).\n14",
  "textLength": 79566
}