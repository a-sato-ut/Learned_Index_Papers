{
  "paperId": "10a0ea3e1b3202d1c28903cc823d5ab8e9d5780d",
  "title": "FLAML: A Fast and Lightweight AutoML Library",
  "pdfPath": "10a0ea3e1b3202d1c28903cc823d5ab8e9d5780d.pdf",
  "text": "FLAML: A F AST AND LIGHTWEIGHT AUTOML L IBRARY\nChi Wang1Qingyun Wu1Markus Weimer1Erkang Zhu1\nABSTRACT\nWe study the problem of using low computational cost to automate the choices of learners and hyperparameters\nfor an ad-hoc training dataset and error metric, by conducting trials of different conﬁgurations on the given\ntraining data. We investigate the joint impact of multiple factors on both trial cost and model error, and propose\nseveral design guidelines. Following them, we build a fast and lightweight library FLAML which optimizes for\nlow computational resource in ﬁnding accurate models. FLAML integrates several simple but effective search\nstrategies into an adaptive system. It signiﬁcantly outperforms top-ranked AutoML libraries on a large open\nsource AutoML benchmark under equal, or sometimes orders of magnitude smaller budget constraints.\n1 I NTRODUCTION\nIt is predicted that in the next 10 years, hundreds of thou-\nsands of small teams will build millions of ML-infused\napplications – most just moderately remunerative, but with\nhuge collective value (Agrawal et al., 2020). Operating by\nlarge teams of ML experts and running on massive dedicated\ninfrastructures is not well justiﬁed for these new applica-\ntions. That motivates fast and economical software solutions\nto Automated Machine Learning (AutoML): Given a train-\ning dataset and an error metric, use low computational cost\nto search for learner and hyperparameter choices and pro-\nduce models optimizing the error metric in short time.\nTo provide a concrete context, let us consider the use of\nML in database systems. The database community has\ngrown an increasing interest of integrating data-driven de-\ncision making components fueled by machine learning\ntechniques. For example, classiﬁcation or regression mod-\nels have been explored for indexing (Kraska et al., 2018;\nGalakatos et al., 2019), cardinality and selectivity estima-\ntion (Kipf et al., 2019; Dutt et al., 2019), query performance\nprediction (Marcus & Papaemmanouil, 2019), and workload\nforecasting (Ma et al., 2018). These models make predic-\ntions by learning from a large amount of labeled data, which\nare generated automatically by the system for each dataset\nor workload instance. For example, a selectivity estima-\ntion model can be built for each table or join expression\nusing selectivity labels generated from synthetic queries or\na given workload (Dutt et al., 2019; 2020), and the best\nmodel conﬁgurations vary per instance of training dataset.\nAutoML solutions for these applications are required to be\n1Microsoft Corporation, Redmond, WA, USA. Correspondence\nto: Chi Wang <wang.chi@microsoft.com >.\nCopyright 2020 by the author(s).fast and economic, as the system needs to select hyperpa-\nrameters or learners frequently on different training data (for\nnumerous tables, join expressions, and frequent updates),\nand continuously and timely deploy them (Renggli et al.,\n2019). Computational resource of the system is precious\n(e.g., for answering database queries), and only a small frac-\ntion can be allocated to AutoML, e.g., a few CPU minutes\nper selectivity estimation model.\nA number of AutoML libraries have been developed, which\nusually involve multiple trials of different conﬁgurations.\nOne drawback in existing solutions is they require long time\nor large amounts of resources to produce accurate models\nfor large scale training datasets. For example, given one\nCPU hour, when tested on a recent large-scale AutoML\nbenchmark (Gijsbers et al., 2019), the state-of-the-art so-\nlutions underperform a tuned random forest baseline on\n36-51% of the tasks. And the ratio is even higher when the\nbudget is smaller.\nTo address the problem systematically, it is desirable to fac-\ntor the trial cost , i.e., the CPU cost of training and assessing\nthe model error, explicitly in the AutoML problem. We\nrecognize that the cost of one trial is jointly decided by\nthe following variables: the choice of learner, a subset of\nthe hyperparameters for the chosen learner, the size of the\ntraining data, and the resampling strategy. Those variables\nalso affect the trial error (i.e., the assessed model error)\njointly. Given an ad-hoc dataset, an AutoML solution that is\nonly optimized for low trial error may invoke unnecessarily\nexpensive trials, while a solution that is only optimized for\nlow trial cost may keep making cheap but erroneous trials.\nSome hyperparameter optimization methods made an effort\nto balance the two objectives, but the scope is limited and\nmost systems target resource-consuming clusters (Snoek\net al., 2012; Li et al., 2017; Falkner et al., 2018; Liaw et al.,arXiv:1911.04706v3  [cs.LG]  19 May 2021\n\nFLAML: A Fast and Lightweight AutoML Library\n101102103\ncost (s)103\n102\n101\nregret\nFLAML\nHpBandSter\n(a) Model auc regret vs. training cost for\nevery trial\n101102103\nautoml time (s)101102103cost (s)(b) Trial cost vs. the total time from start\nwhen each trial is ﬁnished\n101102103\nautoml time (s)103\n102\n101\nregret(c) Model auc regret vs. the total time from\nstart when each trial is ﬁnished\nFigure 1. Example of search performance for FLAML vs. a baseline in the same search space. Each marker corresponds to one trial\nof conﬁguration evaluation in a particular method. Model auc regret=best auc-model auc. Each marker corresponds to one trial of\nconﬁguration evaluation in a particular method. Subﬁgure (a) suggests that FLAML makes fewer expensive trials with high error (top right\ncorner) than HpBandSter. Subﬁgure (b) further displays that the expense of trials made by FLAML grows gradually with total time spent,\nwhile for HpBandSter there is no such trend. As a result, subﬁgure (c) shows that FLAML outperforms in both early and late stages.\n2019; Li et al., 2020). No previous AutoML system handles\nthe complex dependency among the multiple variables men-\ntioned above. Though challenging, it is desired to have an\neconomical system that holistically considers the multiple\nfactors in the cost-error tradeoff, and handles different tasks\nrobustly and efﬁciently.\nWe design and implement a lightweight Python library\nFLAML1. FLAML leverages the structure of the search\nspace to choose a search order optimized for both cost and\nerror. It iteratively decides the learner, hyperparameter, sam-\nple size and resampling strategy while leveraging their com-\npound impact on both cost and error as the search proceeds.\nFirst, we analyze the relation of these factors and deduce\ndesirable properties of an economical AutoML system. To\nsatisfy these properties, we integrate several non-traditional\nsearch strategies judiciously because commonly employed\nstrategies do not sufﬁciently exploit the analyzed relations\nof the multiple factors. Overall, the search tends to gradually\nmove from cheap trials and inaccurate models to expensive\ntrials and accurate models (a typical example is illustrated\nin Figure 1). FLAML is designed for robustly adapting to an\nad-hoc dataset out of the box, without relying on expensive\npreparation such as meta-learning. In fact, our system has\nalmost no computational overhead beyond the trial cost of\neach conﬁguration.\nWe perform extensive evaluation using a recent open source\nAutoML benchmark (Gijsbers et al., 2019) plus regression\ndatasets from a regression benchmark (Olson et al., 2017).\nWith varying time budget from one minute to one hour,\nFLAML outperforms top three open-source AutoML li-\nbraries as well as a commercial cloud-based AutoML ser-\nvice in a majority of the tasks given equal or smaller budget,\nwith signiﬁcant margins. We study an application to selec-\ntivity estimation in the end.\n1https://github.com/microsoft/FLAML2 R ELATED WORK\nFirst, we review the top-performing open-source AutoML li-\nbraries according to the AutoML Benchmark (Gijsbers et al.,\n2019). (1) Auto-sklearn (Feurer et al., 2015) is declared\nthe overall winner of the ChaLearn AutoML Challenge 1\nin 2015-2016 and 2 in 2017-2018. It employs Bayesian\noptimization (BO) (Hutter et al., 2011) for hyperparame-\nter tuning and learner selection, and uses meta-learning to\nwarm-start the search procedure with a few pipelines. (2)\nTPOT (Olson et al., 2016) (Tree-based Pipeline Optimiza-\ntion Tool) constructs machine learning pipelines of arbitrary\nlength using scikit-learn learners and XGBoost and uses\ngenetic programming for hyperparameter tuning. (3) H2O\nAutoML (H2O.ai) is a Java-based library. It performs ran-\ndomized grid search for each learner in the H2O machine\nlearning package, in addition to XGBoost. The learners are\nordered manually and each learner is allocated a predeﬁned\nportion of search iterations. They all use model ensembles\nto boost accuracy.\nA number of commercial platforms are available: Amazon\nAWS SageMaker (Liberty et al., 2020), DataRobot, Google\nCloud AutoML Tables, Microsoft AzureML AutoML, Sales-\nforce TransmogrifAI, H2O Driverless AI, Darwin AutoML\nand Oracle AutoML. They provide end-to-end AutoML ser-\nvice, i.e., directly consuming uncleaned raw data and then\nproducing trained models and predictions.\nTo summarize the learnings from existing AutoML systems,\nthe dominating approach is based on trials in a large search\nspace. The order of the trials thus has a large impact in the\nsearch efﬁciency. Meta-learning is one technique often pro-\nposed to improve the search order, with the assumption that\none can collect a large number of datasets and experiments\nfor meta-training, and the performance of learners and hy-\nperparameters from these experiments is indicative of their\nfuture performance in new datasets and tasks (Feurer et al.,\n2015; Fusi et al., 2018; Shang et al., 2019). In addition,\n\nFLAML: A Fast and Lightweight AutoML Library\nensemble of multiple learners is often considered useful\nfor boosting accuracy at the cost of increased inference\nlatency (Erickson et al., 2020).\nFLAML is designed to perform efﬁciently and robustly with-\nout relying on meta-learning or ensemble at ﬁrst order, for\nseveral usability reasons. First, this makes FLAML an easy\nplug-in in new application scenarios, without requiring a\ndeveloper to collect many diverse meta-training datasets be-\nfore being able to use it. Second, it allows the user to easily\ncustomize learners, search spaces and optimization metrics\nand use FLAML immediately after the customization, with-\nout waiting for another expensive round of meta-learning\nif any of these changes. Third, our customers prefer sin-\ngle learners over ensembles due to the advantage in model\ncomplexity, inference latency, ease of deployment, debugga-\nbility and explanability. How to leverage meta-learning and\nensemble with good usability is interesting future work.\nOne notable standalone subarea in AutoML is neural archi-\ntecture search (NAS) (Elsken et al., 2019) which speciﬁcally\ntargets neural networks. Most application scenarios of NAS\ninvolve unstructured data like images and text. While the\nsearch space and application scenario are different, our de-\nsign principles in cost minimization might be applicable.\n3 API, F ORMULATION AND ANALYSIS\nFLAML is implemented in Python because of its popularity\nin data science. It has a scikit-learn (Pedregosa et al., 2011)\nstyle API:\n1from flaml import AutoML\n2automl = AutoML()\n3automl.fit(X_train, y_train, task=’\nclassification’)\n4prediction = automl.predict(X_test)\nAdditional settings include time budget, optimization metric,\nestimator list etc. It is easy to add customized learners or\nmetrics in FLAML:\n1# MyLearner is a custom estimator class\n2automl.add_learner(learner_name=’mylearner’\n, learner_class=MyLearner)\n3# mymetric is a custom metric function\n4automl.fit(X_train, y_train, metric=\nmymetric, time_budget=60,\nestimator_list=[’mylearner’,’xgboost’])\nThe main innovation of FLAML is in its ﬁt() method: au-\ntomatically producing an accurate model (measured by a\ngiven error metric) for an ad-hoc featurized dataset2.\n2Given existing fast automatic featurization libraries such as\nautofeat (Horn et al., 2019) and azureml-sdk (Mukunthu et al.,\n2019), FLAML does not innovate on featurization techniques,\nthough the system can easily support feature preprocessors.Table 1. Notions and notations.\nL number of learners l learner\n~\u000f validation error \u000f test error\nh hyperparameter values \u001f conﬁguration\ns sample size r resampling strategy\nM trained model \u0014 trial cost\n3.1 Formulation\nWe consider Llearners, each with its own set of hyper-\nparameters. The learners can be customized by users, as\nlong as they have well-deﬁned train and prediction meth-\nods and search space of hyperparameters. We denote the\nsearch space of hyperparameters for learner lasHl. For\neach trial, we can choose a learner l, the hyperparameters\nh2Hl, together with two other variables: sample size s\nof training data, and resampling strategy r.sis an integer\nto denote the number of examples in a sample of training\ndata, andr2fcv;holdoutgis a binary choice between k-\nfold cross-validation and holdout with ratio \u001a.3Alearning\nconﬁguration is deﬁned as a tuple \u001f= (l;h;s;r). When\nwe make a trial with \u001f, we can obtain a validation error ~\u000f(\u001f)\nand a model M(\u001f). Depending on whether the resampling\nstrategy is cross validation or holdout, the model Mcorre-\nsponds to training data of size sors\u0001(1\u0000\u001a), where\u001ais the\nholdout ratio, and the error ~\u000fcorresponds to cross validation\nerror or error on the heldout validation data. ~\u000fis a proxy of\nthe actual error \u000f(M)on unseen test data. The cost of the\ntrial is mainly the CPU time of training and testing using\ncross-validation or holdout, denoted as \u0014(\u001f). The goal of\nfast and economical AutoML is to minimize the total cost\nbefore ﬁnding a model with the lowest test error. The total\ncost is expected to increase as the test error decreases, and\ndesired to be approximately optimal.\n3.2 Analysis\nWe ﬁrst analyze the factors considered in our search se-\nquence and several desirable properties of the search dynam-\nics about them. Figure 2 summarizes the relations among\nseveral variables, using notations summarized in Table 1.\nThe domain of the hyperparameters hdepends on the learner\nl. The test error \u000fis not observable during AutoML. It is a\nblackbox function of the learner l, the hyperparameters h,\nand the sample size s. It is approximated by the validation\nerror ~\u000f. We observe several non-blackbox relations among\nthe variables, which are not ﬁrst noticed by us but rarely\nleveraged by existing AutoML systems.\nObservation 1 (Sample size + resampling !error)\nFirst, it is reasonable to assume the test error \u000f, as well\n3In general, we can consider a large search space for the resam-\npling strategy by making kand\u001avariables as well. We make k\nand\u001aconstants in this work to simplify the problem.\n\nFLAML: A Fast and Lightweight AutoML Library\nFigure 2. Relations among the variables.\nas the gap between \u000fand~\u000f, decreases or stays with the\nincrease of the sample size swhen all the other factors\nare ﬁxed (Huang et al., 2019, pg. 2) ((Nakkiran et al.,\n2020) observes this after the sample size is above a\nthreshold). Second, the gap between \u000fand~\u000fis smaller for\ncross-validation than holdout, when all the other factors\nare ﬁxed (Kohavi, 1995; Feurer et al., 2015).\nObservation 2 (Hyperparameter + sample size !error)\nMany learners have a subset of hyperparameters related to\nmodel complexity or regularization, e.g., the number of trees\nand the depth of each tree in tree-based learners. For a\nﬁxed sample size, \u000fdoes not necessarily reach its minimum\nat maximal complexity. Generally speaking, smaller sample\nsize (in a local region) requires lower complexity and more\nregularization to avoid overﬁtting (Hastie et al., 2001;\nNakkiran et al., 2020).\nObservation 3 (Quantiﬁable impact on cost) For each\nﬁxed combination of learner land resampling strategy r,\nthe cost\u0014is approximately proportional to the sample size\nsand a subset of cost-related hyperparameters, such as the\nnumber of trees. When all the other factors are ﬁxed, k-fold\ncross-validation roughly takesk\u00001\n1\u0000\u001a\u0002cost as holdout using\nholdout ratio \u001a.\nBased on the joint effect of hyperparameter and sample\nsize on error and cost (Observation 2 and 3), we have the\nfollowing property.\nProperty 1 (SuitableSampleSize) Small sample size can\nbe used to train and compare low-complexity conﬁgura-\ntions, while large sample size is needed for comparing high-\ncomplexity conﬁgurations.\nFrom the compound impact of sample size and resampling\nstrategy on error and cost (Observation 1 and 3), when\nsample size is small, cross-validation reduces the variance of\nvalidation error while the cost is bounded. When sample size\nis large, validation error from holdout is close to test error,\nFigure 3. Major components in FLAML.\nand the cost is much lower than cross-validation. Since\nthe trial-based AutoML requires a fair selection mechanism\namong all the conﬁgurations, we have:\nProperty 2 (Resample) Cross-validation is preferred over\nholdout for small sample size or large time budget.\nFrom the target of error minimization and Observation 1, as\nwell as the fact that the optimal choice of l\u0003is unknown, we\nderive the following property.\nProperty 3 (FairChance) Given any search sequence pre-\nﬁx, every learner lshould have a chance to be searched\nagain, unless all the valid hyperparameter values of hhave\nbeen searched using the full training data size in the preﬁx.\nFrom the target of cost minimization and Observation 3,\nwe can derive the following property, which is in general\ndifﬁcult to achieve as the optimal conﬁguration is unknown.\nProperty 4 (OptimalTrial) The total cost of any search se-\nquence preﬁx is desired to be approximately optimal (i.e.,\nhave a bounded ratio over the optimal cost) for the lowest er-\nror it achieved. Similar for the subsequence corresponding\nto each learner l.\nAlthough these properties are idealistic properties and they\nare not necessarily complete, they provide meaningful guid-\nance in designing a low-cost AutoML system.\n4 FLAML\nWe present our design following the guidelines. Section 4.1\npresents an overview, and Section 4.2 details our search\nstrategy used in each component respectively.\n4.1 Design Overview\nOur design is presented in Figure 3, with the purpose of\neasy realization of the desired properties described in our\nanalysis. It consists of two layers, including a ML layer\nand an AutoML layer. The ML layer contains the candidate\nlearners. The AutoML layer includes a learner proposer,\n\nFLAML: A Fast and Lightweight AutoML Library\na hyperparameter and sample size proposer, a resampling\nstrategy proposer and a controller. The order of the control\nﬂow is indexed on the arrows in Figure 3 as four steps. Steps\n0-2 involve choosing the corresponding variables in each\ncomponent. In step 3, the controller will invoke the trial\nusing the selected learner in the ML layer, and observe the\ncorresponding validation error ~\u000fand cost\u0014. Steps 1-3 are\nrepeated by iterations until running out of budget. In a par-\nallel environment, the controller can execute a new iteration\nof steps 1-3 before an iteration ﬁnishes if there are available\nresources. Changing one strategy inside each component\ndoes not affect the strategy of others. This design allows\neasy upgrade by incorporation of novel search schemes to\nreplace each component.\nOur system differs from previous work in multiple perspec-\ntives: (1) It is different in how we decouple the searched\nvariables and search strategies (Table 2). For example, we\ncouple the decision of handsin our design to ensure sam-\nple size is decided together with the hyperparameters, which\nreﬂects Property 1. We decouple learner and hyperparam-\neters and use the order of flg!f h;sgto respect domain\ndependency. (2) As the ﬁrst trial-based library targeting\nad-hoc data (including large-scale datasets) using low-cost,\nFLAML focuses on the core search efﬁciency and does not\nuse meta-learning or ensemble. It is considered as future\nwork to develop lightweight meta-learning and ensemble\ntechniques for FLAML while keeping the system economic,\ngeneric and capable of handling ad-hoc datasets. (3) Since\nthe commonly used search strategies are not designed to\ndeeply exploit the compound relations of the multiple fac-\ntors as analyzed in Section 3.2, we employ new search\nstrategies as introduced in the next subsection.\n4.2 Search Strategy\nBefore introducing our search strategies, we ﬁrst introduce\nthe notion of estimated cost for improvement (ECI) which\nwill be used in the search strategies. For each learner l2[L],\nECI 1(l)(abrv.ECI 1) denotes our estimation of the cost\nforlto ﬁnd a conﬁguration with lower error than the current\nbest error (denoted as ~\u000fl) under the current sample size.\nECI 2(l)(abrv.ECI 2) denotes our estimation of the cost to\ntry the current conﬁguration for l(which took \u0014lcost) with\nincreased sample size (multiplied by a factor of c). Finally,\nECI (l)(abrv.ECI ) is our estimation of the cost it takes to\nﬁnd a conﬁguration with land lower error than the current\nbest error among all the learners (denoted as ~\u000f\u0003).\nLetK1> K 2be abbreviations of K1(l)> K 2(l), repre-\nsenting the total cost spent on lwhen the two most recent\nupdates of best conﬁgurations happened for lrespectively,\nK0(abbreviations of K0(l)) be the total cost spent on lso\nfar, and\u000e(abbreviations of \u000e(l)) be the error reduction be-\ntween the two corresponding best conﬁgurations. We set:\n101102103\nautoml time (s)103\n102\n101\nregretlgbm\nxgboost\nextra_tree\nrf\ncatboost\nlrl1Figure 4. Illustration of ECI-based prioritization.\nECI 1= max(K0\u0000K1;K1\u0000K2);ECI 2=c\u0001\u0014l\nECI = max\u0012(~\u000fl\u0000~\u000f\u0003)(K0\u0000K2)\n\u000e;min(ECI 1;ECI 2)\u0013\n(1)\nThe calculation of ECI 1is based on the assumption that it\ntakes higher cost to ﬁnd an improvement in the later stage\nof search.4ECI 2is set to bectimes as large as the trial\ncost of the current conﬁguration for l, because we expect the\nerror of the current conﬁguration to improve when given c\ntimes as large sample size. This simple cost estimation can\nbe reﬁned when the complexity of the training procedure\nis known with respect to sample size. It works well for the\nlearners in our experiments which have linear complexity.\nECI is calculated depending on whether lcurrently has the\nlowest error among all the learners:\n(a)lcurrently has the best error among all the learners. In\nthis case, by deﬁnition ECI = min(ECI 1;ECI 2).\n(b)ldoes not have the best error among all the learners\n4For learners which have not been tried in the search, the ECI 1\nis set to the smallest trial cost for those learners. Since the smallest\ntrial cost varies with input data, we ﬁrst run the fastest learner and\ngets its smallest cost on the input data, and then set the ECI 1for\nother learners as multiples of this cost using predeﬁned constants.\n\nFLAML: A Fast and Lightweight AutoML Library\nTable 2. Comparison of search strategy.\nTool Searched variable Search strategy\nAlpine Meadow (Shang et al., 2019) flg ! f hg ! fsgMeta learning !BO!Progressive sampling\nAuto-sklearn (Feurer et al., 2015) fl;hg Bayesian optimization, with meta-learning and ensemble\nH2O AutoML (H2O.ai) flg ! f hg Enumeration !Randomized grid search, with ensemble\nHpBandSter (Falkner et al., 2018; Li et al., 2017) fl;hg;fsg Bayesian optimization, Hyperband\nPMF-automl (Fusi et al., 2018) fl;hg Collaborative ﬁltering, with post-processing ensemble\nTPOT (Olson et al., 2016) fl;hg Genetic programming, with ensemble embedded\nFLAML flg ! ECI-based sampling of learner choices !\nfh;sg Randomized direct search, ECI-based choice of sample size\ncurrently. For lto match the current best error ~\u000f\u0003, it needs\nto improve its own error by at least ~\u000fl\u0000~\u000f\u0003. To estimate the\ncost to ﬁll that gap, we also need to estimate the efﬁciency\nof improvement vforl. That is, how fast lis expected to\nreduce the error in its own search sequence. We calculate v\nas:v=\u000e\n\u001c, where\u001c=K0\u0000K2is the estimated cost spent\nonlfor making the error reduction \u000e. In the special case\nwhere\u000e= 0, i.e., the ﬁrst conﬁguration searched for lis\nthe best conﬁguration for lso far, we set \u000e= ~\u000fl, and\u001cas\nthe total cost spent on l. In our implementation, we double\nthe cost to ﬁll the gap as the estimated cost for ﬁnding an\nimprovement because we assume the improvement with\nrespect to cost has a diminishing return.\nCombining the two cases we have Eq. (1). A visual demon-\nstration is provided in Figure 4 corresponding to the same\nexample in Figure 1. The ﬁgure on the top plots the best\nerror per learner vs. automl time, and visualizes the ECI\nof two learners LightGBM and XGBoost based on Eq. (1)\nwhen the current time point is 35s. To illustrate that ECI\nis self-adjustable, we add a hypothetical new trial of XG-\nBoost (the dashed triangle marker at 38s) which does not\nﬁnd a better model. In this case, ECI(xgb) will be increased\nas shown in the horizontal orange arrow, and the priority\nof XGBoost will be decreased. The ﬁgure on the bottom\nvisualizes the search trajectory of each learner.\nStep 0: The resampling strategy proposer chooses r.Re-\nsampling strategy is decided based on a simple thresholding\nrule. It is the simplest design which follows Property 2\n(Resample). If the training dataset has fewer than 100K\ninstances and # instances \u0002# features=budget is smaller\nthan 10M/hour, we use cross validation. Otherwise, we\nuse holdout. This simple thresholding rule can be easily\nreplaced by more complex rules, e.g., from meta learning.\nBy default, FLAML uses 5-fold cross-validation and 0.1 as\nthe holdout ratio.\nStep 1: The learner proposer chooses l.With the con-\ncept of ECI introduced, we design a search strategy where\neach learner lis chosen with probability proportional to\n1=ECI (l). There are several reasons why ECI is desir-\nable in our system: (1) This design follows Property 3 and 4.\nProperty 4 (OptimalTrial) suggests that we prioritize choiceswhich are expected to improve the error using small cost,\nhence we assign choices with lower ECI higher probability.\n(2) Instead of directly choosing the learner with lowest ECI,\nwe use randomization because Property 3 (FairChance) re-\nquires every learner to have chance to be searched again,\nand our estimation is not precise. Based on our sampling\nscheme, the expectation of ECI for the probabilistic choice\nisE[ECI ] =P\nlECI (l)\u0001ECI (l)\u00001\nP\nl0ECI (l0)\u00001=the harmonic mean\nof all the ECIs. That means, the expected cost for improve-\nment using our sampling scheme is still dominated by and\nclose to the lowest ECIs. (3) With more observations about\nlbeing collected, ECI will be updated dynamically. The\ndynamic update of ECI leads to a self-correcting behavior:\nIf our ECI is an overestimate, it will decrease; if it is an\nunderestimate, it will increase. This can be reﬂected from\nthe formula of ECI and Figure 4.\nAlthough a related concept EIperSec (Expected Improve-\nment per Second) was proposed in (Snoek et al., 2012), it\nis designed for a different context of Bayesian optimization\nand not applicable to our goal of learner selection.\nStep 2: The hyperparameter and sample size proposer\nchooses hands.\nFor hyperparameters, we adopt a recently proposed random-\nized direct search method (Wu et al., 2021), which can\nperform cost-effective optimization for cost-related hyper-\nparameters. The algorithm uses a low-cost conﬁguration\nas the start point. At each iteration, it samples a random\ndirection uin a(jhj\u00001)-dimensional unit sphere and then\ndecides whether to move to a new halong the randomly\nsampled direction (or the opposite direction) depending on\nthe observed sign of change of validation error. The cost\nof the next trial can be upper bounded with respect to the\ncost of the best conﬁg of the considered learner. This upper\nbound of trial cost is guaranteed by the search procedure\nused, and increases only progressively if the best error is\nreduced. Step-size of the move is adjusted adaptively (large\nin the beginning to fast approach the required complexity)\nand the search is restarted (from randomized initial points)\noccasionally to escape local optima.\nThough the randomized direct search method does not han-\ndle subsampling, it is a good option to use in our framework\n\nFLAML: A Fast and Lightweight AutoML Library\nTable 3. Details of the case study in Figure 1. It reveals that FLAML avoids trying unnecessarily expensive conﬁgs in the beginning more\nthan HpBandSter though they are given the same search space. Even though FLAML eventually tries expensive conﬁgs, it chooses the\nmore promising learner (in this example, XGBoost) based on the observed cost and error in early trials.\nIter Time (s) Learner Conﬁg tried by FLAML Error Cost (s)\n1 4LightGBM tree num: 4, leaf num: 4, min child weight: 20, learning rate: 0.1... 0.3272 3\n... ... ... ... ... ...\n9 40 XGBoost tree num: 13, leaf num: 9, min child weight: 18, learning rate: 0.4... 0.2242 5\n... ... ... ... ... ...\n20 402 XGBoost tree num: 76, leaf num: 116, min child weight: 3, learning rate: 0.2... 0.2003 26\n... ... ... ... ... ...\n26 1935 XGBoost tree num: 548, leaf num: 247, min child weight: 1.1, learning rate: 0.02... 0.1896 238\n27 3225 XGBoost tree num: 1312, leaf num: 739, min child weight: 1.1, learning rate: 0.01... 0.1882 1290\n... ... ... ... ... ...\nIter Time (s) Learner Conﬁg tried by HpBandSter Error Cost (s)\n1 16 XGBoost tree num: 47, leaf num: 50, min child weight: 0.004, learning rate: 0.8... 0.2497 15\n2 1193 XGBoost tree num: 17863, leaf num: 2735, min child weight: 3.7, learning rate: 0.1... 0.1979 1177\n3 1356 CatBoost early stop rounds: 15, learning rate: 0.03... 0.1978 163\n... ... ... ... ... ...\n7 2011 XGBoost tree num: 10369, leaf num: 369, min child weight: 0.1, learning rate: 0.4... 0.2036 583\n8 3325 RF tree num: 2155, max features: 0.36, criterion: entropy 0.2007 1313\n... ... ... ... ... ...\ndue to two important reasons: (1) The method proved its\neffectiveness in controlling trial cost both theoretically and\nempirically. The theoretical analysis of this method shows\nits alignment with Property 4 (OptimalTrial), and its em-\npirical study demonstrates superiority over Bayesian opti-\nmization methods including the one using EIPerSec when\ncost-related hyperparameters exist. (2) The method works\nwithout requiring the exact validation error of each trial\nas feedback, as long as the relative order of any two trials\ncan be determined, and we can leverage that to modify the\nmethod to incorporate data subsampling. We make several\nimportant adjustments to enable data subsampling. Specif-\nically, we begin with a small sample size (10K) for each\nl. For each requested l, we ﬁrst make a choice between\nincreasing the sample size and trying a new conﬁguration\nwith the current sample size, by comparing ECI 1(l)and\nECI 2(l). WhenECI 1(l)\u0015ECI 2(l), we keep the current\nhyperparameter values and increase the sample size. Oth-\nerwise, we stay with the current sample size, and generate\nhyperparameter values using the randomized direct search\nmethod described above. With this design, the system will\nadaptively change the sample size as needed. Once the sam-\nple size for a learner reaches the full data size, it keeps using\nthat size until convergence for that learner. That reduces the\nrisk of pruning good conﬁgurations by small sample size\ncompared to multi-ﬁdelity pruning. We reset the sample size\nto the initial value as the search for that learner is restarted.\nThe implementation of the randomized direct search method\nfollows (Wu et al., 2021). At each iteration, a random direc-\ntion is used ﬁrst to train a model. If the error does not reduce,\nwe train another model using the opposite direction. The\ninitial stepsize is set to bep\nd. It will be decreased when\nthe number of consecutively no improvement iterations islarger than 2d\u00001until it reaches a lower bound, i.e., con-\nverges. Speciﬁcally, stepsize is discounted by a reduction\nratio>1, which is intuitively the ratio between the total\nnumber of iterations taken in total since the last restart of\nthe search and the total number of iterations taken to ﬁnd the\nbest conﬁguration since the last restart. We perform adap-\ntive step-size adjustments and random restart only when the\nlargest sample size is reached. Our system shufﬂes the data\nrandomly in the beginning and to get a sample with size\ns, it takes the ﬁrst stuples of the shufﬂed data. Stratiﬁed\nshufﬂing is used for classiﬁcation tasks based on the labels.\nAdvantages of our design . First, our search strategies are\ndesigned toward strong ﬁnal performance (i.e., low ﬁnal\nerror) for ad-hoc datasets, which requires a large conﬁg-\nuration search space. The random sampling according to\nECI in Step 1 and the random restart in Step 2 help the\nmethod escape local optima. Second, our search strategies\nare designed toward strong anytime performance for ad-hoc\ndatasets. The ECI-based prioritization in Step 1 favors cheap\nlearners in the beginning but penalizes them later if the error\nimprovement is slow. The hyperparameter and sample size\nproposer in Step 2 tends to propose cheap conﬁgurations at\nthe beginning stage of the search, but quickly move to con-\nﬁgurations with high model complexity and large sample\nsize when needed in the later stage of the search. These de-\nsigns make FLAML navigate large search space efﬁciently\nfor both small and large datasets. Last, the computational\noverhead in the AutoML layer compared to the trial cost in\nthe ML layer is negligible in our solution: ECI-based sam-\npling, randomized direct search, and update of ECIs. The\ncomplexity of these operations for each iteration is linear\nwith the dimensionality of hyperparameters, and does not\ndepend on the number of trials.\n\nFLAML: A Fast and Lightweight AutoML Library\n5 E XPERIMENTS\nOur main empirical study is based on a combination of a\nrecent open source AutoML classiﬁcation benchmark (Gijs-\nbers et al., 2019) and a regression benchmark (Olson et al.,\n2017), for a total of 53 datasets (39 classiﬁcation + 14\nregression). The two benchmarks can be found at: Au-\ntoML Benchmark (classiﬁcation) - https://openml.\ngithub.io/automlbenchmark , and PMLB (re-\ngression) - https://github.com/EpistasisLab/\npenn-ml-benchmarks . The ﬁrst benchmark collects\n39 classiﬁcation tasks that represent real-world data science\nproblems of various sizes, domains and levels of difﬁculty\nfrom previous AutoML papers, competitions and bench-\nmarks. In the second benchmark PMLB, most datasets\nare of small scale, from which we selected the regression\ndatasets whose numbers of instances are larger than 10,000.\nThat results in 14 regression tasks. The statistics of all the\n53 datasets are listed in Table 6-8 in the appendix. The 53\ndatasets have #instance \u0002#feature ranging from 2,992 to\n85,920,000 and vary in the occurrence of numeric features,\ncategorical features and missing values. The referred Au-\ntoML classiﬁcation benchmark uses roc-auc and negative\nlog-loss as the scores to evaluate the performance on bi-\nnary classiﬁcation tasks and multi-class classiﬁcation tasks\nrespectively. It calibrates the original scores using a con-\nstant class prior predictor (=0) and a tuned random forest\n(=1), the higher the better. The tuned random forest is a\nstrong baseline taking a long time to ﬁnish, and achieving a\nscore above 1 is not easy according to (Gijsbers et al., 2019).\nFor regression tasks, we use the r2 score which is a metric\nbounded by 1 before calibration.\nWe compare FLAML (v0.1.3) to four trial-based AutoML li-\nbraries plus a hyperparameter optimization library designed\nfor budget constrained scenarios: auto-sklearn (v0.9.0)5,\nH2O AutoML (v3.30.0.3), TPOT (v0.10.1), cloud-automl\n(a commercial cloud-based AutoML service from one major\ncloud provider), and HpBandSter (v0.7.4, an implementa-\ntion of BOHB which integrates Bayesian optimization with\nHyperband). The ﬁrst three are the top three performers\nreported by the AutoML benchmark (Gijsbers et al., 2019)\nand their performance is close to each other. HpBandSter\nuses the same search space and resampling strategy as those\nof FLAML (Table 5 in the appendix). It is worth noting that\nall the libraries use a different search space from each other\nby design except for HpBandSter and FLAML. The search\nspace of FLAML (reported in the appendix) neither sub-\nsumes, nor is subsumed by the search space of auto-sklearn,\ncloud-automl, H2O AutoML or TPOT. It is very challeng-\ning, if not impossible, to equalize the search space due to the\nspeciﬁc designs of each library (e.g., meta-learning by auto-\nsklearn and cloud-automl, and special grid search by H2O\n5AutoSklearn2Classiﬁer (Feurer et al., 2020) for classiﬁcation.AutoML). We use their default setting and do not introduce\nour own bias.\nAll experiments are executed on an Ubuntu server with Intel\nXeon Gold 6140 2.3GHz, and 512GB RAM. We use 1 CPU\ncore for each compared solution and vary the time budget\nfrom one minute to one hour. We choose these settings\nbecause this work is mainly concerned about performance\nusing low resource, while the numbers in (Gijsbers et al.,\n2019) are obtained using a rather generous budget (8 to 32\nhours of total CPU time). Cloud-automl with 1m budget is\nnot reported since it does not return within 2 minutes. As\neach dataset has been split into 10 folds by OpenML (Van-\nschoren et al., 2014), all the reported results are averaged\nover the 10 folds.\n5.1 Comparative Study\nThe scaled scores of all the methods given different desired\ntime budgets (1m, 10m and 1h) on all the datasets are shown\nin Figure 5. Each of the radar charts shows the scaled scores\nof different methods on a group of datasets (spokes on the\nradar chart) given a desired time budget. Results of all 53\ndatasets are summarized into 3 sub-ﬁgures (rows) accord-\ning to their task types. Each row shows the performance\ncomparison on the same group of datasets given different\ndesired time budgets. Figure 6 presents the distribution of\nscore difference between FLAML and each individual base-\nline, under equal budget (the ﬁrst row) or smaller budget for\nFLAML (the second row).\nWhen using equal time budgets, FLAML clearly outper-\nforms every competitor with large margins in most cases.\nIn a small fraction of cases, FLAML underperforms by a\nsmall margin. Even with a smaller time budget, FLAML\ncan be better than or equal to the others in many cases.\nFor example, FLAML’s 1m result is no worse than others’\n10m result on 62%-83% datasets, and 72%-89% for 10m\nvs. 1h. In sum, FLAML demonstrates signiﬁcant margin\nover each competitor given equal budgets, and competitive\nperformance given smaller budgets than competitors . To\nbe fair, the prior libraries are not primarily designed for the\nsame low-resource setting as ours. We note that the search\nspace of FLAML contains both cheap and expensive conﬁg-\nurations, but our integrated search strategies make FLAML\nwisely prioritize them. As seen in Figure 1 and Table 3,\nFLAML’s adaptive behavior is the key to strong anytime\nperformance.\n5.2 Ablation Study\nWe study the effect of the three components in our system,\nby comparing FLAML with three alternatives: roundrobin ,\nwhich takes trials for each learner lin turn; fulldata , which\nuses the full data for the trials; and cv, which uses cross\nvalidation for all the trials. Figure 7 plots the validation\n\nFLAML: A Fast and Lightweight AutoML Library\nblood-taustral\ncredit-\nphoneme\nkc1\nsylvine\nkr-vs-k\namazon_\njasmine\nadult\nbank_manumeraihiggsairlinenomaominiboochristikddcup0apsfailalbertguillerriccard\n0160sAuto-sklearn Cloud-automl HpBandSter H2OAutoML TPOT FLAML\nblood-taustral\ncredit-\nphoneme\nkc1\nsylvine\nkr-vs-k\namazon_\njasmine\nadult\nbank_manumeraihiggsairlinenomaominiboochristikddcup0apsfailalbertguillerriccard\n01600s\nblood-taustral\ncredit-\nphoneme\nkc1\nsylvine\nkr-vs-k\namazon_\njasmine\nadult\nbank_manumeraihiggsairlinenomaominiboochristikddcup0apsfailalbertguillerriccard\n013600s\n(a) Binary classiﬁcation datasets ordered by size counter clockwise, from smallest blood-transfusion to largest riccardo\ncarvehicle\nsegment\njungle_\nmfeat-f\nshuttle\ncnae-9\nhelena\nconnect jannisfabertvolkertdilbertdioniscovertyfashionrobert\n01carvehicle\nsegment\njungle_\nmfeat-f\nshuttle\ncnae-9\nhelena\nconnect jannisfabertvolkertdilbertdioniscovertyfashionrobert\n01carvehicle\nsegment\njungle_\nmfeat-f\nshuttle\ncnae-9\nhelena\nconnect jannisfabertvolkertdilbertdioniscovertyfashionrobert\n01\n(b) Multi-class classiﬁcation datasets ordered by size counter clockwise, from smallest carto largest robert\nbng_ech\nhouses\nhouse_8\nbng_low\nhouse_1\n2dplane\nfried\nmvpolbng_brebng_pwlpokerbng_phabng_pbc\n01bng_ech\nhouses\nhouse_8\nbng_low\nhouse_1\n2dplane\nfried\nmvpolbng_brebng_pwlpokerbng_phabng_pbc\n01bng_ech\nhouses\nhouse_8\nbng_low\nhouse_1\n2dplane\nfried\nmvpolbng_brebng_pwlpokerbng_phabng_pbc\n01\n(c) Regression datasets ordered by size counter clockwise, from smallest bngechomonths to largest bngpbc\nFigure 5. Scaled scores of AutoML libraries on each dataset with each time budget. The longer is each spoke the better.\nerror of FLAML and its alternatives on a binary classiﬁca-\ntion dataset MiniBooNE , a multi-class classiﬁcation dataset\nDionis , and a regression dataset bngpbc. The ﬁgure shows\nhow the error improves with respect to search time. We\ncan see that when removing any of the three aspects of the\nFLAML search strategy, the search performance degrades.\nIn particular, the gap between FLAML and roundrobin in-\ncreases before converging due to FLAML’s prioritization\nto more promising learners. The gap between FLAML and\nfulldata is large initially because the initial trials of FLAML\nare very fast using small sample of training data. That gap\nreduces later as FLAML increases its sample size.\nThe ablation study veriﬁes the effectiveness of the search\nstrategies. Figure 8 in the appendix plots the score difference\nbetween FLAML and these alternatives over all the datasets.5.3 Application to Selectivity Estimation\nAs an example application to database systems, we evaluate\nthe performance of AutoML libraries to the selectivity esti-\nmation task. Selectivity estimates are necessary inputs for a\nquery optimizer to identify a good execution plan (Selinger\net al., 1979). A good selectivity estimator should provide\naccurate and fast estimates for a wide variety of intermediate\nquery expressions at reasonable construction overhead (Cor-\nmode et al., 2012). Estimators in most database systems\nmake use of limited statistics on the data, e.g., per-attribute\nhistograms or small data samples on the base tables (Or-\nacle docs; SQL Server docs; Neumann & Freitag, 2020).\n(Dutt et al., 2019; 2020) developed low-overhead regression\nmodels which achieve much lower q-error (a relative error\nmetric used by the selectivity estimation literature) than all\n\nFLAML: A Fast and Lightweight AutoML Library\n1\n 0 1 2TPOTH2OHpBand.Cloud.Auto-sk.\n60s vs. 60s\n1\n 0 1 2\n600s vs. 600s\n1\n 0 1 2\n3600s vs. 3600s\n1\n 0 1 2TPOTH2OHpBand.Cloud.Auto-sk.\n60s vs. 600s\n1\n 0 1 2\n600s vs. 3600s\n1\n 0 1 2\n60s vs. 3600s\nFigure 6. Box plot of scaled score difference between FLAML and other libraries when FLAML uses equal or smaller budget (positive\ndifference meaning FLAML is better).\n101102103\nWall clock time [s]2×102\n3×102\n4×102\n6×102\nerror\nroundrobin\nfulldata\ncv\nflaml\n(a) MiniBooNE: 1-auc\n101102103\nWall clock time [s]1002×1003×1004×100error\n (b) Dionis: logloss\n101102103\nWall clock time [s]6×101\n7×101\n8×101\nerror\n (c) bng pbc: 1-r2\nFigure 7. Variations of FLAML. Lines – average error over 10 folds; shades – max and min.\nTable 4. 95th-percentile q-error for selectivity estimation (search\ntime listed if at least one method exceeds time limit). H2O AutoML\ncannot return a model with the given budget.\nDataset FLAML Auto-sk. TPOT Manual\n2D-Forest 1.41 1.42 2.70 1.84\n2D-Power 2.03 3.28 4.70 4.09\n2D-TPCH 2.19(42s) 2.11(197s) N/A 3.04\n4D-Forest1 2.91 4.33 11.9 4.41\n4D-Forest2 4.40(45s) 5.93(55s) 17.4(146s) 6.26\n4D-Power 2.42(49s) 3.78(197s) 12.4(79s) 4.29\n7D-Higgs 3.16(60s) 5.83(55s) 9.65(91s) 6.54\n7D-Power 4.25(46s) 6.87(55s) 65.2(102s) 7.57\n7D-Weather 4.71(54s) 6.44(55s) 30.1(118s) 6.84\n10D-Forest 9.09(49s) 19.8(147s) 96.2(89s) 15.1\nthe other methods with similar inference time, including\nthe ones used in commercial database products. The recom-\nmended learner is XGBoost with 16 trees and 16 leaves per\ntree. We denote this conﬁguration as ‘Manual’.\nTable 4 compares the 95th-percentile q-error of the models\nfound by different AutoML libraries with one CPU minutebudget, using the same datasets from (Dutt et al., 2019).\nFLAML outperforms the other AutoML libraries as well as\nthe manual conﬁguration. On 10D-Forest, FLAML is the\nonly AutoML solution that outperforms Manual.\n6 F UTURE WORK\nWhile FLAML has superior performance on a variety of\nbenchmark tasks compared to the state of the art, it does\nnot use meta learning to optimize per task instance based\non previous experience. It is worthwhile to think how to\nleverage meta learning in the cost-optimizing framework\nwithout losing the robustness on ad-hoc datasets. Similarly,\nit is interesting to study the new tradeoff between cost and\nerror when model ensembles are introduced.\nACKNOWLEDGMENTS\nThe authors appreciate suggestions from Surajit Chaudhuri,\nNadiia Chepurko, Alex Deng, Anshuman Dutt, Johannes\nGehrke, Silu Huang, Christian Konig, and Haozhe Zhang.\n\nFLAML: A Fast and Lightweight AutoML Library\nREFERENCES\nAgrawal, A., Chatterjee, R., Curino, C., Floratou, A., Gow-\ndal, N., Interlandi, M., Jindal, A., Karanasos, K., Krish-\nnan, S., Kroth, B., et al. Cloudy with high chance of\ndbms: A 10-year prediction for enterprise-grade ml. In\nCIDR’20 , 2020.\nCormode, G., Garofalakis, M., Haas, P. J., and Jermaine,\nC. Synopses for massive data: Samples, histograms,\nwavelets, sketches. Found. Trends Databases , 4(1–3):\n1–294, January 2012.\nDutt, A., Wang, C., Nazi, A., Kandula, S., Narasayya, V . R.,\nand Chaudhuri, S. Selectivity estimation for range pred-\nicates using lightweight models. PVLDB , 12(9):1044–\n1057, 2019.\nDutt, A., Wang, C., Narasayya, V ., and Chaudhuri, S. Ef-\nﬁciently approximating selectivity functions using low\noverhead regression models. In 46th International Con-\nference on Very Large Data Bases , 2020.\nElsken, T., Metzen, J. H., and Hutter, F. Neural architecture\nsearch: A survey. Journal of Machine Learning Research ,\n20(55):1–21, 2019.\nErickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy,\nP., Li, M., and Smola, A. Autogluon-tabular: Robust and\naccurate automl for structured data. arXiv:2003.06505 ,\n2020.\nFalkner, S., Klein, A., and Hutter, F. BOHB: Robust and\nefﬁcient hyperparameter optimization at scale. In ICML ,\n2018.\nFeurer, M., Klein, A., Eggensperger, K., Springenberg, J.,\nBlum, M., and Hutter, F. Efﬁcient and robust automated\nmachine learning. In NIPS , 2015.\nFeurer, M., Eggensperger, K., Falkner, S., Lindauer, M., and\nHutter, F. Auto-sklearn 2.0. arXiv:2007.04074 , 2020.\nFusi, N., Sheth, R., and Elibol, M. Probabilistic matrix fac-\ntorization for automated machine learning. In Advances\nin Neural Information Processing Systems , 2018.\nGalakatos, A., Markovitch, M., Binnig, C., Fonseca, R., and\nKraska, T. Fiting-tree: A data-aware index structure. In\nSIGMOD , 2019.\nGijsbers, P., LeDell, E., Thomas, J., Poirier, S., Bischl, B.,\nand Vanschoren, J. An open source automl benchmark.\nInAutoML Workshop at ICML 2019 , 2019. URL http:\n//arxiv.org/abs/1907.00909 .\nH2O.ai. H2o automl. http://docs.h2o.ai/h2o/\nlatest-stable/h2o-docs/automl.html .\n2019-10-29.Hastie, T., Tibshirani, R., and Friedman, J. The Elements of\nStatistical Learning . Springer New York Inc., New York,\nNY , USA, 2001.\nHorn, F., Pack, R., and Rieger, M. The autofeat python\nlibrary for automatic feature engineering and selec-\ntion. In Machine Learning and Knowledge Discovery\nin Databases. ECML PKDD 2019 , 2019.\nHuang, S., Wang, C., Ding, B., and Chaudhuri, S. Efﬁcient\nidentiﬁcation of approximate best conﬁguration of train-\ning in large datasets. In Proceedings of the 33rd AAAI\nConference on Artiﬁcial Intelligence (AAAI) , 2019.\nHutter, F., Hoos, H. H., and Leyton-Brown, K. Sequential\nmodel-based optimization for general algorithm conﬁgu-\nration. In Coello, C. A. C. (ed.), Learning and Intelligent\nOptimization , 2011.\nKipf, A., Kipf, T., Radke, B., Leis, V ., Boncz, P., and Kem-\nper, A. Learned cardinalities: Estimating correlated joins\nwith deep learning. In CIDR , 2019.\nKohavi, R. A study of cross-validation and bootstrap for\naccuracy estimation and model selection. In IJCAI’95 ,\n1995.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis,\nN. The case for learned index structures. In SIGMOD ,\n2018.\nLi, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and\nTalwalkar, A. Hyperband: A novel bandit-based approach\nto hyperparameter optimization. In ICLR’17 , 2017.\nLi, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Ben-\ntzur, J., Hardt, M., Recht, B., and Talwalkar, A. A system\nfor massively parallel hyperparameter tuning. In Proceed-\nings of Machine Learning and Systems , 2020.\nLiaw, R., Bhardwaj, R., Dunlap, L., Zou, Y ., Gonzalez, J. E.,\nStoica, I., and Tumanov, A. Hypersched: Dynamic re-\nsource reallocation for model development on a deadline.\nSoCC’19, 2019.\nLiberty, E., Karnin, Z., Xiang, B., Rouesnel, L., Coskun,\nB., Nallapati, R., Delgado, J., Sadoughi, A., Astashonok,\nY ., Das, P., et al. Elastic machine learning algorithms in\namazon sagemaker. In SIGMOD’20 , 2020.\nMa, L., Aken, D. V ., Hefny, A., Mezerhane, G., Pavlo, A.,\nand Gordon, G. J. Query-based workload forecasting for\nself-driving database management systems. In SIGMOD ,\n2018.\nMarcus, R. C. and Papaemmanouil, O. Plan-structured deep\nneural network models for query performance prediction.\nPVLDB , 12(11):1733–1746, 2019.\n\nFLAML: A Fast and Lightweight AutoML Library\nMukunthu, D., Shah, P., and Tok, W. Practical Automated\nMachine Learning on Azure: Using Azure Machine Learn-\ning to Quickly Build AI Solutions . O’Reilly Media, Incor-\nporated, 2019.\nNakkiran, P., Kaplun, G., Bansal, Y ., Yang, T., Barak, B.,\nand Sutskever, I. Deep double descent: Where bigger\nmodels and more data hurt. In ICLR , 2020.\nNeumann, T. and Freitag, M. J. Umbra: A disk-based\nsystem with in-memory performance. In CIDR , 2020.\nOlson, R. S., Urbanowicz, R. J., Andrews, P. C., Lavender,\nN. A., Kidd, L. C., and Moore, J. H. Automating biomedi-\ncal data science through tree-based pipeline optimization.\nIn Squillero, G. and Burelli, P. (eds.), Applications of\nEvolutionary Computation , pp. 123–137. Springer Inter-\nnational Publishing, 2016.\nOlson, R. S., La Cava, W., Orzechowski, P., Urbanowicz,\nR. J., and Moore, J. H. Pmlb: a large benchmark suite for\nmachine learning evaluation and comparison. BioData\nMining , 10(1):36, Dec 2017. ISSN 1756-0381. doi: 10.\n1186/s13040-017-0154-4. URL https://doi.org/\n10.1186/s13040-017-0154-4 .\nOracle docs. Optimizer statistics (release\n18). https://docs.oracle.com/en/\ndatabase/oracle/oracle-database/18/\ntgsql/optimizer-statistics.html#\nGUID-0A2F3D52-A135-43E1-9CAB-55BFE068A297 .\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V .,\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\nWeiss, R., Dubourg, V ., Vanderplas, J., Passos, A., Cour-\nnapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.\nScikit-learn: Machine learning in python. JMLR , 12:\n2825–2830, November 2011.\nRenggli, C., Karla ˇs, B., Ding, B., Liu, F., Wu, W., and\nZhang, C. Continuous integration of machine learning\nmodels with ease.ml/ci: Towards a rigorous yet practical\ntreatment. In SysML Conference (SysML 2019) , 2019.\nSelinger, P. G., Astrahan, M. M., Chamberlin, D. D., Lo-\nrie, R. A., and Price, T. G. Access path selection in a\nrelational database management system. SIGMOD’79,\n1979.\nShang, Z., Zgraggen, E., Buratti, B., Kossmann, F., Eich-\nmann, P., Chung, Y ., Binnig, C., Upfal, E., and Kraska, T.\nDemocratizing data science through interactive curation\nof ml pipelines. In SIGMOD , 2019.\nSnoek, J., Larochelle, H., and Adams, R. P. Practical\nbayesian optimization of machine learning algorithms.\nInNIPS , 2012.SQL Server docs. Statistics in Microsoft SQL Server\n2017. https://docs.microsoft.com/en-us/\nsql/relational-databases/statistics/\nstatistics?view=sql-server-2017 .\nVanschoren, J., van Rijn, J. N., Bischl, B., and Torgo,\nL. Openml: Networked science in machine learning.\nSIGKDD Explor. Newsl. , 15(2):49–60, June 2014.\nWu, Q., Wang, C., and Huang, S. Frugal optimization for\ncost-related hyperparameters. In AAAI’21 , 2021.\n\nFLAML: A Fast and Lightweight AutoML Library\nTable 5. Default search space in FLAML. Sdenotes the number of\ntraining instances. Bold values indicate initialization correspond-\ning to lowest complexity and cost. lr - logistic regression.\nLearner Hyperparameter Type Range\ntree num int [4, min(32768,S)]\nleaf num int [4, min(32768,S)]\nmin child weight ﬂoat [0.01, 20]\nlearning rate ﬂoat [0.01, 1.0]\nXGBoost subsample ﬂoat [0.6, 1.0]\nreg alpha ﬂoat [1e-10, 1.0]\nreg lambda ﬂoat [1e-10, 1.0]\ncolsample by level ﬂoat [0.6, 1.0]\ncolsample by tree ﬂoat [0.7, 1.0]\ntree num int [4, min(32768,S)]\nleaf num int [4, min(32768,S)]\nmin child weight ﬂoat [0.01, 20]\nlearning rate ﬂoat [0.01, 1.0]\nLightGBM subsample ﬂoat [0.6, 1.0]\nreg alpha ﬂoat [1e-10, 1.0]\nreg lambda ﬂoat [1e-10, 1.0]\nmax bin ﬂoat [7, 1023]\ncolsample by tree ﬂoat [0.7, 1.0]\nCatBoost early stop rounds int [10, 150]\nlearning rate ﬂoat [0.005,0.2]\nsklearn tree num int [4, min(2048,S)]\nrandom max features ﬂoat [0.1, 1.0]\nforest split criterion cat fgini, entropy g\nsklearn tree num int [4, min(2048,S)]\nextra max features ﬂoat [0.1, 1.0]\ntrees split criterion cat fgini, entropy g\nsklearn lr C ﬂoat [0.03125, 32768]\nAPPENDIX\nThe default search space of FLAML is shown in Table 5.\nFor the learners which have not been tried in the search,\nECI 1(l)is set to the smallest trial cost for each learner l.\nSince the smallest trial cost varies with input data, we ﬁrst\nrun the fastest learner and get its smallest cost on the input\ndata, and then set the ECI 1for other learners as multiples\nof this cost using predeﬁned constants. These constants are\neasy to set as we only need to calibrate the running time of\nthe fastest conﬁguration of each learner ofﬂine. We use the\nfollowing constants: f‘lightgbm’:1, ‘xgboost’:1.6, ’extra\ntree’:1.9, ‘rf’:2, ’catboost’:15, ‘lr’:160 g. Meta-learning can\nbe potentially applied here to have a more instance-speciﬁc\nprediction of the running time of the initial conﬁguration.\nWe usec= 2as the multiplicative factor of sample size in\nthe experiments.\nFLAML is designed to work with low resource consumption,\nand the extra computation in FLAML beyond trial cost\nis negligible. So it tries one conﬁguration at a time and\nlets the learner consume all the given resources (cores and\nRAM). Since we start search from inexpensive models for\nevery learner, this design minimizes the latency between\ntwo iterations so that the proposers can get feedback as earlyTable 6. Binary classiﬁcation datasets.\nname task id # instance # feature\nadult 7592 48842 14\nAirlines 189354 539383 7\nAlbert 189356 425240 78\nAmazon employee access 34539 32769 9\nAPSFailure 168868 76000 170\nAustralian 146818 690 14\nbank marketing 14965 45211 16\nblood-transfusion 10101 748 4\nchristine 168908 5418 1636\ncredit-g 31 1000 20\nguillermo 168337 20000 4296\nhiggs 146606 98050 28\njasmine 168911 2984 144\nkc1 3917 2109 21\nKDDCup09 appetency 3945 50000 230\nkr-vs-kp 3 3196 36\nMiniBooNE 168335 130064 50\nnomao 9977 34465 118\nnumerai28.6 167120 96320 21\nphoneme 9952 5404 5\nriccardo 168333 20000 4296\nsylvine 168853 5124 20\nTable 7. Multi class classiﬁcation datasets.\nname task id # instance # feature\ncar 146821 1728 6\ncnae-9 9981 1080 856\nconnect-4 146195 67557 42\nCovertype 7593 581012 54\ndilbert 168909 10000 2000\nDionis 189355 416188 60\nfabert 168852 8237 800\nFashion-MNIST 146825 70000 784\nHelena 168329 65196 27\nJannis 168330 83733 54\njungle chess 2pcs... 167119 44819 6\nmfeat-factors 12 2000 216\nRobert 168332 10000 7200\nsegment 146822 2310 19\nshuttle 146212 58000 9\nvehicle 53 846 18\nvolkert 168810 58310 180\nTable 8. Regression Datasets.\nname task id # instance # feature\n2dplanes 2306 40768 10\nbngbreastTumor 7324 116640 9\nbngechomonths 7323 17496 9\nbnglowbwt 7320 31104 9\nbngpbc 7318 1000000 18\nbngpharynx 7322 1000000 11\nbngpwLinear 7325 177147 10\nfried 4885 40768 10\nhouse 16H 4893 22784 16\nhouse 8L 2309 22784 8\nhouses 5165 20640 8\nmv 4774 40768 10\npoker 10102 1025010 10\npol 2292 15000 48\n\nFLAML: A Fast and Lightweight AutoML Library\n0.0 0.5 1.0rrfulldatacv\n60s vs. 60s\n0.0 0.5 1.0rrfulldatacv\n600s vs. 600s\n0.0 0.5 1.0rrfulldatacv\n3600s vs. 3600s\nFigure 8. Score difference for FLAML vs. its own alternatives.\nTable 9. % of tasks where FLAML has better or matching error vs.\nthe baselines when using smaller time budget.\nFLAML vs. Baseline 1m vs 10m 10m vs 1h 1m vs 1h\nFLAML vs. Auto-sklearn 65% 79% 58%\nFLAML vs. Cloud-automl 62% 79% 48%\nFLAML vs. HpBandSter 63% 89% 65%\nFLAML vs. H2OAutoML 71% 72% 50%\nFLAML vs. TPOT 83% 85% 65%\nas possible. When abundant cores are available and a learner\ncannot consume all of them, we can extend FLAML to work\nwith multiple search threads in parallel. After choosing one\nlearner based on ECI to perform one search iteration, if\nthere are extra available resources, we can sample another\nlearner by ECI, and so on. When one search iteration for a\nlearner ﬁnishes, the resource is released and we can select a\nlearner again using updated ECIs. One learner can also have\nmultiple search threads by using different starting points of\nthe hyperparameters. Due to the search space decomposition\nand the randomized direct hyperparameter search strategy\nused, the multiple search threads are largely independent\nand do not interfere with each other.\nStacked ensemble can be added as a post-processing step\nlike existing libraries (H2O.ai). It requires remembering\nthe predictions on cross-validation folds of the models to\nensemble. And extra time needs to be spent on building the\nensemble and retraining each model. FLAML does not do it\nby default to keep the overhead low, but it offers the option\nto enable it when storage and extra computation time are\nnot concerns.\nRicher types of ML tasks or model assessment criteria can\nbe allowed via customized learners and metric functions.\nFor example, one may search for the cheapest model with\nerror below a threshold using our framework.\nIn Table 9, each row shows the percentage of datasets where\nFLAML is better than or equal to a particular baseline with\nsmaller time budget. For example, ‘1m vs 10m’ in the\nheader means FLAML’s time budget is one minute and the\nconcerned baseline’s time budget is ten minutes. We use a\ntolerance ratio of 0.1% to exclude the marginal differences\non the scaled scores, i.e., when the difference between two\nscores is within the tolerance ratio, they are consideredas close enough. FLAML’s performance in one minute is\nalready better than or equal to auto-sklearn, H2O AutoML\nand TPOT’s performance in one hour on more than half of\nthe tasks.",
  "textLength": 58778
}