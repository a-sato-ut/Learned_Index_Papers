{
  "paperId": "6b2cae9d709731c7ab41400c3286e37820252364",
  "title": "ALEX: An Updatable Adaptive Learned Index",
  "pdfPath": "6b2cae9d709731c7ab41400c3286e37820252364.pdf",
  "text": "ALEX: An Updatable Adaptive Learned Index\nJialin DingyUmar Farooq Minhas zJia Yux\nChi WangzJaeyoung DozYinan LizHantian Zhang\u0007Badrish Chandramouli z\nJohannes GehrkezDonald KossmannzDavid LometzTim Kraskay\nyMassachuse/t_ts Institute of TechnologyzMicroso/f_t ResearchxArizona State University\n\u0007Georgia Institute of Technology\nABSTRACT\nRecent work on “learned indexes” has changed the way we\nlook at the decades-old /f_ield of DBMS indexing. /T_he key idea\nis that indexes can be thought of as “models” that predict the\nposition of a key in a dataset. Indexes can, thus, be learned.\n/T_he original work by Kraska et al. shows that a learned index\nbeats a B+Tree by a factor of up to three in search time and\nby an order of magnitude in memory footprint. However, it\nis limited to static, read-only workloads.\nIn this paper, we present a new learned index called ALEX\nwhich addresses practical issues that arise when implement-\ning learned indexes for workloads that contain a mix of point\nlookups, short range queries, inserts, updates, and deletes.\nALEX eﬀectively combines the core insights from learned in-\ndexes with proven storage and indexing techniques to achieve\nhigh performance and low memory footprint. On read-only\nworkloads, ALEX beats the learned index from Kraska et al. by\nup to 2.2\u0002on performance with up to 15 \u0002smaller index size.\nAcross the spectrum of read-write workloads, ALEX beats\nB+Trees by up to 4.1 \u0002while never performing worse, with\nup to 2000\u0002smaller index size. We believe ALEX presents\na key step towards making learned indexes practical for a\nbroader class of database workloads with dynamic updates.\n1 INTRODUCTION\nRecent work by Kraska et al. [ 19], which we will refer to as the\nLearned Index, proposes to replace a standard database index\nwith a hierarchy of machine learning (ML) models. Given a\nkey, an intermediate node in the hierarchy is a model to pre-\ndict the child model to use, and a leaf node in this hierarchy is a\nmodel to predict the location of the key in a densely packed ar-\nray (Fig. 1). /T_he models for this Learned Index are trained from\nthe data. /T_heir key insight is that using (even simple) models\nthat adapt to the data distribution to make a “good enough”\nguess of a key’s actual location signi/f_icantly improves per-\nformance. However, their solution can only handle lookups\non read-only data, with no support for update operations.\n/T_his critical drawback makes the Learned Index unusable for\ndynamic, read-write workloads, common in practice.\nIn this work, we start by asking ourselves the following\nresearch question: Can we design a new high performanceindex for dynamic workloads that eﬀectively combines the core\ninsights from the Learned Index with proven storage & index-\ning techniques to deliver great performance in both time and\nspace? Our answer is a new in-memory index structure called\nALEX, a fully dynamic data structure that simultaneously pro-\nvides eﬃcient support for point lookups, short range queries,\ninserts, updates, deletes, and bulk loading. /T_his mix of opera-\ntions is commonplace in online transaction processing (OLTP)\nworkloads [6, 8, 32] and is also supported by B+Trees [29].\nImplementing writes with high performance requires a\ncareful design of the underlying data structure that stores\nrecords. [ 19] uses a sorted, densely packed array which works\nwell for static datasets but can result in high costs for shi/f_ting\nrecords if new records are inserted. Furthermore, the pre-\ndiction accuracy of the models can deteriorate as the data\ndistribution changes over time, requiring repeated retraining.\nTo address these challenges, we make the following technical\ncontributions in this paper:\n\u000fStorage layout optimized for models: Similar to\na B+Tree, ALEX builds a tree, but allows diﬀerent\nnodes to grow and shrink at diﬀerent rates. To store\nrecords in a data node, ALEX uses an array with\ngaps, a Gapped Array , which (1) amortizes the cost of\nshi/f_ting the keys for each insertion because gaps can\nabsorb inserts, and (2) allows more accurate place-\nment of data using model-based inserts to ensure that\nrecords are located closely to the predicted position\nwhen possible. For eﬃcient search, gaps are actually\n/f_illed with adjacent keys.\n\u000fSearch strategy optimized for models: ALEX ex-\nploits model-based inserts combined with exponential\nsearch starting from the predicted position. /T_his al-\nways beats binary search when models are accurate.\n\u000fKeepingmodelsaccuratewithdynamicdatadis-\ntributions and workloads: ALEX provides robust\nperformanceevenwhenthedatadistributionisskewed\nordynamicallychangesa/f_terindexinitialization. ALEX\nachieves this by exploiting adaptive expansion, and\nnodespli/t_tingmechanisms, pairedwithselectivemodel\nretraining, which is triggered by intelligent policies\nbased on simple cost models. Our cost models take thearXiv:1905.08898v2  [cs.DB]  21 May 2020\n\nactual workload into account and thus can eﬀectively\nrespond to dynamic changes in the workload. ALEX\nachieves all the above bene/f_its without needing to\nhand-tune parameters for each dataset or workload.\n\u000fDetailedevaluation: Wepresenttheresultsofanex-\ntensive experimental analysis with real-life datasets\nandvaryingread-writeworkloadsandcompareagainst\nstate of the art indexes that support range queries.\nOn read-only workloads, ALEX beats the Learned Index by\nup to 2.2\u0002on performance with up to 15 \u0002smaller index size.\nAcross the spectrum of read-write workloads, ALEX beats\nB+Tree by up to 4.1 \u0002while never performing worse, with up\nto 2000\u0002smaller index size. ALEX also beats an ML-enhanced\nB+Tree and the memory-optimized Adaptive Radix Tree,\nscales to large data sizes, and is robust to data distribution shi/f_t.\nIn the remainder of this paper, we give background (Sec-\ntion 2), present the architecture of ALEX (Section 3), describe\nthe operations on ALEX (Section 4), present an analysis of\nALEX performance (Section 5), present experimental results\n(Section 6), review related work (Section 7), and conclude\n(Section 8).\n2 BACKGROUND\n2.1 Traditional B+Tree Indexes\nB+Tree isaclassicrangeindexstructure. Itisaheight-balanced\ntree which stores either the data (primary index) or pointers\nto the data (secondary index) at the leaf level, in a sorted order\nto facilitate range queries.\nA B+Tree lookup operation can be broken down into two\nsteps: (1) traverse to leaf, and (2) search within the leaf. Start-\ning at the root, traverse to leaf performs comparisons with the\nkeys stored in each node, and branches via stored pointers to\nthe next level. When the tree is deep, the number of compar-\nisons and branches can be large, leading to many cache misses.\nOnce traverse to leaf identi/f_ies the correct leaf page, typically\na binary search is performed to /f_ind the position of the key\nwithin the node, which might incur additional cache misses.\n/T_he B+Tree is a dynamic data structure that supports in-\nserts, updates, and deletes; is robust to data sizes and distribu-\ntions; and is applicable in many diﬀerent scenarios, including\nin-memory and on-disk. However, the generality of B+Tree\ncomes at a cost. In some cases knowledge of the data helps\nimprove performance. As an extreme example, if the keys are\nconsecutive integers, we can store the data in an array and\nperform lookup in O ¹1ºtime. A B+Tree does not exploit such\nknowledge. Here, “learning” from the input data has an edge.\n2.2 /T_he Case for Learned Indexes\nKraska et al. [ 19] observed that B+Tree indexes can be thought\nof as models. Given a key, they predict the location of the key\nwithin a sorted array (logically) at the leaf level. If indexes are\nFigure 1: Learned Index by Kraska et al.\nmodels, they can be learned using traditional ML techniques\nby learning the cumulative distribution function (CDF) of the\ninput data. /T_he resulting Learned Index is optimized for the\nspeci/f_ic data distribution.\nAnother insight from Kraska et al. is that a single ML model\nlearned over the entire data is not accurate enough because of\nthe complexity of the CDF. To overcome this, they introduce\ntherecursive model index (RMI) [19]. RMI is a hierarchy of\nmodels, with a static depth of two or three, where a higher-\nlevel model picks the model at the next level, and so on, with\nthe leaf-level model making the /f_inal prediction for the posi-\ntion of the key in the data structure (Fig. 1). Logically, the RMI\nreplaces the internal B+Tree nodes with models. /T_he eﬀect\nis that comparisons and branches in internal B+Tree nodes\nduring traverse to leaf are replaced by model inferences in a\nLearned Index.\nIn [19], the keys are stored in an in-memory sorted array.\nGiven a key, the leaf-level model predicts the position (array\nindex) of the key. Since the model is not perfect, it could make\na wrong prediction. /T_he insight is that if the leaf model is\naccurate, a local search surrounding the predicted location is\nfaster than a binary search on the entire array. To support local\nsearch, [ 19] keeps minandmax error bounds for each model\nin RMI and performs binary search within these bounds.\nLast, each model in RMI can be a diﬀerent type of model.\nBoth linear regression and neural network based models are\nconsidered in [ 19]. /T_here is a trade-oﬀ between model accu-\nracy and model complexity. /T_he root of the RMI is tuned to be\neither a neural network or a linear regression, depending on\nwhich provides be/t_ter performance, while the simplicity and\nthe speed of computation for linear regression model is bene-\n/f_icial at the non-root levels. A linear regression model can be\nrepresented as /y.alt=ba\u0003x+bc, where xis the key and /y.altis the\npredicted position. A linear regression model needs to store\njust two parameters aandb, so storage overhead is low. /T_he\ninference with a single linear regression model requires only\none multiplication, one addition and one rounding, which are\nfast to execute on modern processors.\nUnlike B+Tree, which could have many internal levels, RMI\nuses two or three levels. Also, the storage space required for\nmodels (two or four 8-byte double values per model) is much\nsmaller than the storage space for internal nodes in B+Tree\n(which store keys and pointers). A Learned Index can be an\n2\n\norder of magnitude smaller in main memory storage (vs. inter-\nnal B+Tree nodes), while outperforming a B+Tree in lookup\nperformance by a factor of up to three [19].\n/T_he main drawback of the Learned Index is that it does\nnot support any modi/f_ications, including inserts, updates, or\ndeletes. Let us demonstrate a na ¨ıve insertion strategy for such\nan index. Given a key kto insert, we /f_irst use the model to /f_ind\nthe insertion position for k. /T_hen we create a new array whose\nlength is one plus the length of the old array. Next, we copy the\ndata from the old array to the new array, where the elements\non the right of the insertion position are shi/f_ted to the right\nby one position. We insert kat the insertion position of the\nnew array. Finally, we update the models to re/f_lect the change\nin the data distribution. Such a strategy has a linear time\ncomplexity with respect to the data size, which is unacceptable\nin practice. Kraska et al. suggest building delta-indexes to\nhandle inserts [ 19], which is complementary to our strategy.\nIn this paper, we describe an alternative data structure to\nmake modi/f_ications in a learned index more eﬃcient.\n3 ALEX OVERVIEW\n/T_he ALEX design (Fig. 2) takes advantage of two key insights.\nFirst, we propose a careful space-time trade-oﬀ that not only\nleads to an updatable data structure, but is also faster for\nlookups. To explore this trade-oﬀ, ALEX supports a Gapped\nArray (GA) layout for the leaf nodes, which we present in Sec-\ntion 3.2. Second, the Learned Index supports static RMI (SRMI)\nonly, where the number of levels and the number of models in\neach level is /f_ixed at initialization. SRMI performs poorly on in-\nserts if the data distribution is diﬃcult to model. ALEX can be\nupdated dynamically and eﬃciently at runtime and uses linear\ncost models that predict the latency of lookup and insert oper-\nations based on simple statistics measured from an RMI. ALEX\nuses these cost models to initialize the RMI structure and to\ndynamically adapt the RMI structure based on the workload.\nALEX aims to achieve the following goals w.r.t. the B+Tree\nand Learned Index. (1) Insert time should be competitive\nwith B+Tree, (2) lookup time should be faster than B+Tree\nand Learned Index, (3) index storage space should be smaller\nthan B+Tree and Learned Index (4) data storage space (leaf\nlevel) should be comparable to dynamic B+Tree. In general,\ndata storage space will overshadow index storage space, but\nthe space bene/f_it from smaller index storage space is still im-\nportant because it allows more indexes to /f_it into the same\nmemory budget. /T_he rest of this section describes how our\nALEX design achieves these goals.\n3.1 Design Overview\nALEX is an in-memory, updatable learned index. ALEX has\na number of diﬀerences from the Learned Index [19].\n/T_he /f_irst diﬀerence lies in the data structure used to store\nthe data at the leaf level. Like B+Tree, ALEX uses a node per\nMKey\nRoot\nNode\nM\nM MM\nM...\nM M\nM M M\nexponential\nsearchM\nposition M\nGapped \nArray...Internal\nNode\nData\nNodeLegend\nGapKeyAdaptive\nRMIFigure 2: ALEX Design\nleaf. /T_his allows the individual nodes to expand and split more\n/f_lexibly and also limits the number of shi/f_ts required during\nan insert. In a typical B+Tree, every leaf node stores an array\nof keys and payloads and has “free space” at the end of the\narray to absorb inserts. ALEX uses a similar design but more\ncarefully chooses how to use the free space. /T_he insight is\nthat by introducing gaps that are strategically placed between\nelements of the array, we can achieve faster insert and lookup\ntimes. As shown in Fig. 2, ALEX uses a Gapped Array (GA)\nlayout for each data node, which we describe in Section 3.2.\n/T_he second diﬀerence is that ALEX uses exponential search\nto /f_ind keys at the leaf level to correct mispredictions of the\nRMI, as shown in Fig. 2. In contrast, [ 19] uses binary search\nwithin the error bounds provided by the models. We exper-\nimentally veri/f_ied that exponential search without bounds is\nfaster than binary search with bounds (Section 6.3.1). /T_his\nis because if the models are good, their prediction is close\nenough to the correct position. Exponential search also re-\nmoves the need to store error bounds in the models of the RMI.\n/T_he third diﬀerence is that ALEX inserts keys into data\nnodes at the position where the models predict that the key\nshould be. We call this model-based insertion . In contrast, the\nLearned Index produces an RMI on an array of records without\nchanging the position of records in the array. Model-based\ninsertion has be/t_ter search performance because it reduces\nmodel misprediction errors.\n/T_he fourth diﬀerence is that ALEX dynamically adjusts the\nshape and height of the RMI depending on the workload. We\ndescribe the design of initializing and dynamically growing\nthe RMI structure in Section 4.\n/T_he /f_inal diﬀerence is that ALEX has no parameters that\nneed to be re-tuned for each dataset or workload, unlike the\nLearned Index, in which the number of models must be tuned.\nALEX automatically bulk loads and adjusts the structure of\nRMI to achieve high performance by using a cost model.\n3\n\nM\nM\nData NodeM\nData NodeMInternal\nNodeInternal Node Akeys   [0, 1) ∈\nFigure 3: Internal nodes allow diﬀerent resolutions in\ndiﬀerent parts of the key space »0;1º.\n3.2 Node Layout\n3.2.1 Data Nodes. Like a B+Tree, the leaf nodes of ALEX\nstore the data records and thus are referred to as data nodes ,\nshown as circles in Fig. 2. A data node stores a linear regres-\nsion model (two double values for slope and intercept), which\nmaps a key to a position, and two Gapped Arrays (described\nbelow), one for keys and one for payloads . We show only the\nkeys array in Fig. 2. By default, both keys and payloads are\n/f_ixed-size. (Note that payloads could be records or pointers to\nvariable-sized records, stored in separately allocated spaces\nin memory). We also impose a max node size for practical\nreasons (see details in Section 4).\nALEX uses a Gapped Array layout which uses model-based\ninserts to distribute extra space between the elements of the\narray, thereby achieving faster inserts and lookups. In con-\ntrast, B+Tree places all the gaps at the end of the array. Gapped\nArrays /f_ill the gaps with the closest key to the right of the gap,\nwhich helps maintain exponential search performance. In\norder to eﬃciently skip gaps when scanning, each data node\nmaintains a bitmap which tracks whether each location in\nthe node is occupied by a key or is a gap. /T_he bitmap is fast to\nquery and has low space overhead compared to the Gapped\nArray. We compare Gapped Array to an existing gapped data\nstructure called Packed Memory Array [4] in Appendix E.\n3.2.2 Internal Nodes. We refer to all the nodes which are\npart of the RMI structure as internal nodes , shown as rectan-\ngles in Fig. 2. Internal nodes store a linear regression model\nand an array containing pointers to children nodes. Like a\nB+Tree, internal nodes direct traversals down the tree, but un-\nlike B+Tree, internal nodes in ALEX use models to “compute”\nthe location, in the pointers array, of the next child pointer\nto follow. Similar to data nodes, we impose a max node size .\n/T_he internal nodes of ALEX serve a conceptually diﬀerent\npurpose than those of the Learned Index. Learned Index’s\ninternal nodes have models that are /f_it to the data; an inter-\nnal node with a perfect model partitions keys equally to its\nchildren, and an RMI with perfect internal nodes results in an\nequal number of keys in each data node. However, the goal of\nthe RMI structure is not to produce equally sized data nodes,but rather data nodes whose key distributions are roughly\nlinear, so that a linear model can be accurately /f_it to its keys.\n/T_herefore, the role of the internal nodes in ALEX is to\nprovide a /f_lexible way to partition the key space. Suppose\ninternal node A in Fig. 3 covers the key space »0;1ºand has\nfour child pointers. A Learned Index would assign a node\nto each of these pointers, either all internal nodes or all data\nnodes. However, ALEX more /f_lexibly partitions the space.\nInternal node A assigns the key spaces »0;14ºand»12;1º\nto data nodes (because the CDF in those spaces are linear),\nand assigns»14;12ºto another internal node (because the\nCDF is non-linear and the RMI requires more resolution into\nthis key space). As shown in the /f_igure, multiple pointers can\npoint to the same child node; this is useful for handling inserts\n(Section 4.3.3). We restrict the number of pointers in every\ninternal node to always be a power of 2. /T_his allows nodes to\nsplit without retraining its subtree (Section 4.3.3).\n4 ALEX ALGORITHMS\nIn this section, we describe the algorithms for lookups, inserts\n(including how to dynamically grow the RMI and the data\nnodes), deletes, out of bounds inserts, and bulk load.\n4.1 Lookups and Range /Q_ueries\nTo look up a key, starting at the root node of the RMI, we\niteratively use the model to “compute” a location in the point-\ners array, and we follow the pointer to a child node at the\nnext level, until we reach a data node. By construction, the\ninternal node models have perfect accuracy, so there is no\nsearch involved in the internal nodes. We use the model in the\ndata node to predict the position of the search key in the keys\narray, doing exponential search if needed to /f_ind the actual\nposition of the key. If a key is found, we read the correspond-\ning value at the same position from the payloads array and\nreturn the record. Else, we return a null record. We visually\nshow (using red arrows) a lookup in Fig. 2. A range query /f_irst\nperforms a lookup to /f_ind the position and data node of the\n/f_irst key whose value is not less than the range’s start value,\nthen scans forward until reaching the range’s end value, using\nthe node’s bitmap to skip over gaps and if necessary using\npointers stored in the node to jump to the next data node.\n4.2 Insert in non-full Data Node\nFor the insert algorithm, the logic to reach the correct data\nnode (i.e., TraverseToLeaf) is the same as in the lookup algo-\nrithm described above. In a non-full data node, to /f_ind the\ninsertion position for a new element, we use the model in the\ndata node to predict the insertion position. If the predicted\nposition is not correct (if inserting there would not maintain\nsorted order), we do exponential search to /f_ind the correct\ninsertion position. If the insertion position is a gap, then we\n4\n\nM M\nM M\nExpanded Data Node Data NodeFigure 4: Node Expansion\ninsert the element into the gap and are done. Else, we make a\ngap at the insertion position by shi/f_ting the elements by one\nposition in the direction of the closest gap. We then insert\nthe element into the newly created gap. /T_he Gapped Array\nachieves O¹lognºinsertion time with high probability [3].\n4.3 Insert in full Data Node\nWhen a data node becomes full, ALEX uses two mechanisms\nto create more space: expansions and splits. ALEX relies on\nsimple cost models to pick between diﬀerent mechanisms.\nBelow, we /f_irst de/f_ine the notion of “fullness,” then describe\nthe expansion and split mechanisms, and the cost models.\nWe then present the insertion algorithm that combines the\nmechanisms with the cost models. Algorithm 1 summarizes\nthe procedure for inserting into a data node.\n4.3.1 Criteria for Node Fullness. ALEX does not wait for\na data node to become 100% full, because insert performance\non a Gapped Array will deteriorate as the number of gaps\ndecreases. We introduce lower and upper density limits on the\nGapped Array: dl;du2¹0;1¼, with the constraint that dl<du.\nDensity is de/f_ined as the fraction of positions that are /f_illed by\nelements. A node is full if the next insert results in exceeding\ndu. By default we set dl=0:6 and du=0:8 to achieve average\ndata storage utilization of 0 :7, similar to B+Tree [ 13], which\nin our experience always produces good results and did not\nneed to be tuned. In contrast, B+Tree nodes typically have\ndl=0:5 and du=1. Section 5 presents a theoretical analysis of\nhow the density of the Gapped Array provides a way to trade\noﬀ between the space and the lookup performance for ALEX.\n4.3.2 Node Expansion Mechanism. To expand a data node\nthat contains nkeys, we allocate a new larger Gapped Array\nwith ndlslots. We then either scale or retrain the linear re-\ngression model, and then do model-based inserts of all the\nelements in this new larger node using the scaled or retrained\nmodel. A/f_ter creation, the new data node is at the lower den-\nsity limit dl. Fig. 4 shows an example data node expansion\nwhere the Gapped Array inside the data node is expanded\nfrom two slots on the le/f_t to four slots on the right.\n4.3.3 Node Split Mechanism. To split a data node in two,\nwe allocate the keys to two new data nodes, such that each\nnew node is responsible for half of the key space of the original\nnode. ALEX supports two ways to split a node:(1)Spli/t_tingsideways isconceptuallysimilartohowaB+Tree\nuses splits. /T_here are two cases: (a) If the parent internal node\nof the split data node is not yet at the max node size , we replace\nthe parent node’s pointers to the split data node with point-\ners to the two new data nodes. /T_he parent internal node’s\npointers array might have redundant pointers to the split data\nnode (Fig. 3). If so, we give half of the redundant pointers to\neach of the two new nodes. Else, we create a second pointer\nto the split data node by doubling the size of the parent node’s\npointers array and making a redundant copy for every pointer,\nand then give one of the redundant pointers to each of the two\nnew nodes. Fig. 5a shows an example of a sideways split that\ndoes not require an expansion of the parent internal node. (b)\nIf the parent internal node has reached max node size , then\nwe can choose to split the parent internal node, as we show\nin Fig. 5b. Note that by restricting all the internal node sizes\nto be powers of 2, we can always split a node in a “bound-\nary preserving” way, and thus require no retraining of any\nmodels below the split internal node. Note that the split can\npropagate all the way to the root node, just like in a B+Tree.\n(2)Spli/t_ting down converts a data node into an internal\nnode with two child data nodes, as we show in Fig. 5c. /T_he\nmodels in the two child data nodes are trained on their respec-\ntive keys. B+Tree does not have an analogous spli/t_ting down\nmechanism.\n4.3.4 Cost Models. To make decisions about which mech-\nanism to apply (expansion or various types of splits), ALEX\nrelies on simple linear cost models that predict average lookup\ntime and insert time based on two simple statistics tracked at\neach data node: (a) average number of exponential search it-\nerations, and (b) average number of shi/f_ts for inserts. Lookup\nperformance is directly correlated with (a) while insert per-\nformance is directly correlated with (a) and (b) (since an insert\n/f_irst needs to do a lookup to /f_ind the correct insertion posi-\ntion). /T_hese intra-node cost models predict the time to perform\noperations within a data node.\n/T_hese two statistics are not known when creating a data\nnode. To /f_ind the expected cost of a new data node, we compute\nthe expected value of these statistics under the assumption\nthat lookups are done uniformly on the existing keys, and\ninserts are done according to the existing key distribution.\nSpeci/f_ically, (a) is computed as the average base-2 logarithm\nof model prediction error for all keys; (b) is computed as the\naverage distance to the closest gap in the Gapped Array for all\nexisting keys. /T_hese expected values can be computed with-\nout creating the data node. If the data node is created using\na subset of keys from an existing data node, we can use the\nempirical ratio of lookups vs. inserts to weight the relative im-\nportance of the two statistics for computing the expected cost.\nIn addition to the intra-node cost model, ALEX uses a Tra-\nverseToLeaf cost model to predict the time for traversing from\n5\n\nM\nAM\nLeft Half Split Data NodeA A'\nRight Half\n(a)\nM\nAM\nSplit Data\nNodeCM\nB AM\nCM\nBM M\nA'\nLeft HalfLeft Half\nRight HalfRight Half\nFull Inner\nNode\n(b)\nM\nAM\nNew Data NodesM\nA A'New Inner\nNode\nSplit Data Node\n(c)Figure 5: Node Splits\nthe root node to a data node. /T_he TraverseToLeaf cost model\nuses two statistics: (1) the depth of the data node being tra-\nversed to, and (2) the total size (in bytes) of all inner nodes and\ndata node metadata (i.e., everything except for the keys and\npayloads). /T_hese statistics capture the cost of traversal: deeper\ndata nodes require more pointer chases to /f_ind, and larger size\nwill decrease CPU cache locality, which slows down the tra-\nversal to a data node. We provide more details about the cost\nmodels and show their low usage overhead in Appendix D.\n4.3.5 Insertion Algorithm. As lookups and inserts are done\non the data node, we count the number of exponential search\niterations and shi/f_ts per insert. From these statistics, we com-\npute the empirical cost of the data node using the intra-node\ncost model. Once the data node is full, we compare the ex-\npected cost (computed at node creation time) to the empirical\ncost. If they do not deviate signi/f_icantly, then we conclude\nthat the model is still accurate, and we perform node expan-\nsion (if the size a/f_ter expansion is less than the max node size ),\nscaling the model instead of retraining. /T_he models in the\ninternal nodes of the RMI are not retrained or rescaled. We\nde/f_ine signi/f_icant cost deviation as occurring when the empir-\nical cost is more than 50% higher than the expected cost. In\nour experience, this cost deviation threshold of 50% always\nproduces good results and did not need to be tuned.\nOtherwise, if the empirical cost has deviated from the ex-\npected cost, we must either (i) expand the data node and\nretrain the model, (ii) split the data node sideways, or (iii) split\nthe data node downwards. We select the action that results in\nlowest expected cost, according to our intra-node cost model.\nFor simplicity, ALEX always splits a data node in two. /T_he\ndata node could conceptually split into any power of 2, but\ndeciding the optimal fanout can be time-consuming, and we\nexperimentally veri/f_ied that a fanout of 2 is best according to\nthe cost model in most cases.\n4.3.6 Why would empirical cost deviate from expected cost?\n/T_his o/f_ten happens when the distribution of keys that are in-\nserted does not follow the distribution of existing keys, which\nresults in the model becoming inaccurate. An inaccurate\nmodel may lead to long contiguous regions without any gaps.Inserting into these fully-packed regions requires shi/f_ting up\nto half of the elements within it to create a gap, which in the\nworst case takes O¹nºtime. Performance may also degrade\nsimply due to random noise as the node grows larger or due\nto changing access pa/t_terns for lookups.\n4.4 Delete, update, and other operations\nTo delete a key, we do a lookup to /f_ind the location of the key,\nand then remove it and its payload. Deletes do not shi/f_t any\nexisting keys, so deletion is a strictly simpler operation than\ninserts and does not cause model accuracy to degrade. If a data\nnode hits the lower density limit dldue to deletions, then we\ncontract the data node (i.e., the opposite of expanding the data\nnode) in order to avoid low space utilization. Additionally,\nwe can use intra-node cost models to determine that two data\nnodes should merge together and potentially grow upwards,\nlocally decreasing the RMI depth by 1. However, for simplicity\nwe do not implement these merging operations.\nUpdates that modify the key are implemented by combining\nan insert and a delete. Updates that only modify the payload\nwill look up the key and write the new value into the payload.\nLike B+Trees, we can merge two ALEX indexes or /f_ind the\ndiﬀerence between two ALEX indexes by iterating over their\nsorted keys in tandem and bulk loading a new ALEX index.\n4.5 Handling out of bounds inserts\nA key that is lower or higher than the existing key space\nwould be inserted into the the le/f_t-most or right-most data\nnode, respectively. A series of out-of-bounds inserts, such\nas an append-only insert workload, would result in poor per-\nformance because that data node has no mechanism to split\nthe out-of-bounds key space. /T_herefore, ALEX has two ways\nto smoothly handle out-of-bounds inserts. Assume that the\nout-of-bounds inserts are to the right (e.g., inserted keys are\nincreasing); we apply analogous strategies when inserts are\nto the le/f_t.\nFirst, when an insert that is outside the existing key space is\ndetected, ALEX will expand the root node , thereby expanding\nthe key space, shown in Fig. 6. We expand the size of the\nchild pointers array to the right. Existing pointers to existing\n6\n\nAlgorithm 1 Gapped Array Insertion\n1:struct Nodefkeys[] (Gapped Array); num keys; du;dl;\nmodel: key!»0;keys.size);g\n2:procedure I/n.sc/s.sc/e.sc/r.sc/t.sc (ke/y.alt)\n3: ifnum keys / keys.size ¿= duthen\n4: ifexpected cost\u0019empirical cost then\n5: Expand(retrain=False)\n6: else\n7: Action with lowest cost /* described in Sec. 4.3.5 */\n8: end if\n9: end if\n10: predicted pos = model.predict(key)\n11: /* check for sorted order */\n12: insert pos = CorrectInsertPosition(predicted pos)\n13: ifkeys[insert pos] is occupied then\n14: MakeGap(insert pos) /* described in text */\n15: end if\n16: keys[insert pos] = key\n17: num keys++\n18:end procedure\n19:procedure E/x.sc/p.sc/a.sc/n.sc/d.sc( retrain )\n20: expanded size = num keys * 1/ dl\n21: /* allocate a new expanded array */\n22: expanded keys = array(size=expanded size)\n23: ifretrain == True then\n24: model = /* train linear model on keys */\n25: else\n26: /* scale existing model to expanded array */\n27: model *= expanded size / keys.size\n28: end if\n29: forkey : keys do\n30: ModelBasedInsert(key)\n31: end for\n32: keys = expanded keys\n33:end procedure\n34:procedure M/o.sc/d.sc/e.sc/l.scB/a.sc/s.sc/e.sc/d.scI/n.sc/s.sc/e.sc/r.sc/t.sc (ke/y.alt)\n35: insert pos = model.predict(key)\n36: ifkeys[insert pos] is occupied then\n37: insert pos = /f_irst gap to right of predicted pos\n38: end if\n39: keys[insert pos] = key\n40:end procedure\nchildren are not modi/f_ied. A new data node is created for every\nnew slot in the expanded pointers array. In case this expansion\nwould result in the root node exceeding the max node size,\nALEX will create a new root node. /T_he /f_irst child pointer of\nthe new root node will point to the old root node, and a new\ndata node is created for every other pointer slot of the new\nroot node. At the end of this process, the out-of-bounds key\nwill fall into one of the newly created data nodes.\nSecond, the right-most data node of ALEX detects append-\nonly insertion behavior by maintaining the value of the max-\nimum key in the node and keeping a counter for how many\ntimes an insert exceeds that maximum value. If most inserts\nM MRoot Node A\nB A A B C DExpanded Root Node A\nM\nB AORM\nCNew Root\nNode A'\nNode AFigure 6: Splitting the root\n40120\n70\n10 10 25 20\n9 9 1 1 10 15 10 20Level Cost\n0 120\n1 1 10\n2 65\n3 75\nFigure 7: Fanout Tree\nexceed the maximum value, that implies append-only be-\nhavior, so the data node expands to the right without doing\nmodel-based re-insertion; the expanded space is kept initially\nempty in anticipation of more append-like inserts.\n4.6 Bulk Load\nALEX supports a bulk load operation, which is used in prac-\ntice to index large amounts of data at initialization or for\nrebuilding an index. Our goal is to /f_ind an RMI structure with\nminimum cost, de/f_ined as the expected average time to do\nan operation (i.e., lookup or insert) on this RMI. Any ALEX\noperation is composed of TraverseToLeaf to the data node\nfollowed by an intra-node operation, so RMI cost is modeled\nby combining the TraverseToLeaf and intra-node cost models.\n4.6.1 Bulk Load Algorithm. Using the cost models, we\ngrow an RMI downwards greedily, starting from the root\nnode. At each node, we independently make a decision about\nwhether the node should be a data node or an internal node,\nand in the la/t_ter case, what the fanout should be. /T_he fanout\nmust be a power of 2, and child nodes will equally divide the\nkey space of the current node. Note that we can make this de-\ncision locally for each node because we use linear cost models,\nso decisions will have a purely additive eﬀect on the overall\ncost of the RMI. If we decide the node should be an internal\nnode, we recurse on each of its child nodes. /T_his continues\nuntil all the data is loaded in ALEX.\n4.6.2 The Fanout Tree. As we grow the RMI, the main\nchallenge is to determine the best fanout at each node. We\nintroduce the concept of a fanout tree (FT), which is a complete\nbinary tree. An FT will help decide the fanout for a single RMI\nnode; in our bulk loading algorithm, we construct an FT each\ntime we want to decide the best fanout for an RMI node. A\nfanout of 1 means that the RMI node should be a data node.\n7\n\nFig. 7 shows an example FT. Each FT node represents a pos-\nsible child of the RMI node. If the key space of the RMI node\nis»0;1º, then the i-th FT node on a level with nchildren rep-\nresents a child RMI node with key space »in;¹i+1ºnº. Each\nFT node is associated with the expected cost of constructing\na data node over its key space, as predicted by the intra-node\ncost models. Our goal is to /f_ind a set of FT nodes that cover the\nentire key space of the RMI node with minimum overall cost.\n/T_he overall cost of a covering set is the sum of the costs of its\nFT nodes, as well as the TraverseToLeaf cost due to model size\n(e.g., going a level deeper in the FT means the RMI node must\nhave twice as many pointers). /T_his covering set determines\nthe optimal fanout of the RMI node (i.e., the number of child\npointers) as well as the optimal way to allocate child pointers.\nWe use the following method to /f_ind a low-cost covering set:\n(1) Starting from the FT root, grow entire levels of the FT at a\ntime, and compute the cost of using each level as the covering\nset. Continue doing so until the costs of each successive level\nstart to increase. In Fig. 7, we /f_ind that level 2 has the lowest\ncombined cost, and we do not keep growing a/f_ter level 3. In\nconcept, a deeper level might have lower cost, but computing\nthe cost for each FT node is expensive. (2) Starting from the\nlevel of the FT with lowest combined cost, we start merging\nor spli/t_ting FT nodes locally. If the cost of two adjacent FT\nnodes is higher than the cost of its parent, then we merge\n(e.g., the nodes with cost 20 and 25 are merged to one with\ncost 40); this might happen when the two nodes have very\nfew keys, or when their distributions are similar. In the other\ndirection, if the cost of a FT node is higher than the cost of\nits two children, we split the FT node (e.g., the node with\ncost 10 is split into two nodes each with cost 1); this might\nhappen when the two halves of the key space have diﬀerent\ndistributions. We continue with this process of merging and\nspli/t_ting adjacent nodes locally until it is no longer possible.\nWe return the resulting covering set of FT nodes.\n5 ANALYSIS OF ALEX\nIn this section, we provide bounds on the RMI depth and com-\nplexity analysis. Bounds on the performance of model-based\nsearch are found in Appendix F.\n5.1 Bound on RMI depth\nIn this section we present a worst-case bound on maximum\nRMI depth and describe how to achieve it. Note that the goal of\nALEX is to maximize performance, not to minimize tree depth;\nthough the two are correlated, the la/t_ter is simply a proxy for\nthe former (e.g., depth is one input to our cost models). /T_here-\nfore, this analysis is useful for gaining intuition about RMI\ndepth, but does not re/f_lect worst-case guarantees in practice.\nLetmbe the maximum node size, de/f_ined in number of slots\n(in the pointers array for internal nodes, in the Gapped Array\nfor data nodes). We constrain node size to be a power of 2:m=2k. Internal nodes can have up to mchild pointers, and\ndata nodes must contain no more than mdukeys. Let all keys\nto be indexed fall within the key space s. Letpbe the minimum\nnumber of partitions such that when the key space sis divided\nintoppartitions of equal width, every partition contains no\nmore than mdukeys. De/f_ine the root node depth as 0.\nT/h.sc/e.sc/o.sc/r.sc/e.sc/m.sc 5.1. We can construct an RMI that satis/f_ies the\nmax node size and upper density limit constraints whose depth\nis no larger thandlogmpe—we call this the maximal depth. Fur-\nthermore, we can maintain maximal depth under inserts. (Note\nthatpmight change under inserts.)\nIn other words, the depth of the RMI is bounded by the\ndensity of the densest subregion of s. In contrast, B+Trees\nbound depth as a function of the number of keys. /T_heorem 5.1\ncan also be applied to a subspace within s, which would cor-\nrespond to some subtree within the RMI.\nP/r.sc/o.sc/o.sc/f.sc. ConstructinganRMIwithmaximaldepthisstraight-\nforward. /T_he densest subregion, which spans a key space of\nsizejsjp, is allocated to a data node. /T_he traversal path from\nthe root to this densest region is composed of internal nodes,\neach with mchild pointers. It takes dlogmpeinternal nodes\nto narrow the key space size from jsjtojsjp. To minimize\ndepth in other subtrees of the RMI, we apply this construction\nmechanism recursively to the remaining parts of the space s.\nStarting from an RMI that satis/f_ies maximal depth, we main-\ntain maximal depth using the mechanisms in Section 4.3 under\nthe following policy: (1) Data nodes expand until they reach\nmax node size. (2) When a data node must split due to max\nnode size, it splits sideways to maintain current depth (poten-\ntially propagating the split up to some ancestor internal node).\n(3) When spli/t_ting sideways is no longer possible (all ancestor\nnodes are at max node size), split downwards. By following\nthis policy, RMI only splits downward when pgrows by a\nfactor of m, thereby maintaining maximal depth. \u0003\n5.2 Complexity analysis\nHere we provide complexity of lookups and inserts, as well as\nthe mechanisms from Section 4.3. Both lookups and inserts\ndo TraverseToLeaf in dlogmpetime. Within the data node,\nexponential search for lookups is bounded in the worst case\nbyO¹logmº. In the best case, the data node model predicts\nthe key’s position perfectly, and lookup takes O¹1ºtime. We\nshow in the next sub-section that we can reduce exponential\nsearch time according to a space-time trade-oﬀ.\nInserts into a non-full node are composed of a lookup, po-\ntentially followed by shi/f_ts to introduce a gap for the new\nkey. /T_his is bounded in the worst case by O¹mº, but since\nGapped Array achieves O¹logmºshi/f_ts per insert with high\nprobability [ 3], we expect O¹logmºcomplexity in most cases.\nIn the best case, the predicted insertion position is correct\nand is a gap, and we place the key exactly where the model\n8\n\npredicts for insert complexity of O¹1º; furthermore, a later\nmodel-based lookup will result in a direct hit in O¹1º.\n/T_herearethreeimportantmechanismsinSection4.3, whose\ncosts are de/f_ined by how many elements must be copied: (1)\nExpansion of a data node, whose cost is bounded by O¹mº. (2)\nSpli/t_ting downwards into two nodes, whose cost is bounded\nbyO¹mº. (3) Spli/t_ting sideways into two nodes and propagat-\ning upwards in the path to some ancestor node, whose cost is\nbounded by O¹mdlogmpeºbecause every internal node on this\npath must also split. As a result, the worst-case performance\nfor insert into a full node is O¹mdlogmpeº.\n6 EVALUATION\nWe compare ALEX with the Learned Index, B+Tree, a model-\nenhanced B+Tree, and Adaptive Radix Tree (ART), using a\nvariety of datasets and workloads. /T_his evaluation demon-\nstrates that:\n\u000fOn read-only workloads, ALEX achieves up to 4.1 \u0002,\n2.2\u0002, 2.9\u0002, 3.0\u0002higher throughput and 800 \u0002, 15\u0002,\n160\u0002, 8000\u0002smallerindexsizethantheB+Tree, Learned\nIndex, Model B+Tree, and ART, respectively.\n\u000fOn read-write workloads, ALEX achieves up to 4.0 \u0002,\n2.7\u0002, 2.7\u0002higherthroughputand2000 \u0002, 475\u0002, 36000\u0002\nsmaller index size than the B+Tree, Model B+Tree,\nand ART, respectively.\n\u000fALEX has competitive bulk load times and maintains\nan advantage over other indexes when scaling to\nlarger datasets and under distribution shi/f_t due to\ndata skew.\n\u000fGapped Array and the adaptive RMI structure allow\nALEX to adapt to diﬀerent datasets and workloads.\n6.1 Experimental Setup\nWe implement ALEX in C++1. We perform our evaluation via\nsingle-threaded experiments on an Ubuntu Linux machine\nwith Intel Core i9-9900K 3.6GHz CPU and 64GB RAM. We\ncompare ALEX against four baselines. (1) A standard B+Tree,\nas implemented in the STX B+Tree [ 5]. (2) Our best-eﬀort\nreimplementation of the Learned Index [ 19], using a two-level\nRMI with linear models at each node and binary search for\nlookups.2(3) Model B+Tree, which maintains a linear model\nin every node of the B+Tree, stores each node as a Gapped\nArray, and uses model-based exponential search instead of bi-\nnary search, implemented on top of [ 5]; this shows the bene/f_it\nof using models while keeping the fundamental B+Tree struc-\nture. (4) Adaptive Radix Tree (ART) [ 21], a trie that adapts to\nthe data which is optimized for main memory indexing, imple-\nmented in C [ 9]. Since ALEX supports all operations common\n1h/t_tps://github.com/microso/f_t/ALEX\n2In private communication with the authors of [ 19], we learned that the added\ncomplexity of using a neural net for the root model usually is not justi/f_ied by\nthe resulting minor performance gains, which we also independently veri/f_ied.in OLTP workloads, we do not compare to hash tables and dy-\nnamic hashing techniques, which cannot eﬃciently support\nrange queries.\nFor each dataset and workload, we use grid search to tune\nthe page size for B+Tree and Model B+Tree and the number\nof models for Learned Index to achieve the best throughput.\nIn contrast, no tuning is necessary for ALEX, unless users\nplace additional constraints. For example, users might want\nto bound the latency of a single operation. We set a max\nnode size of 16MB to achieve tail latency (99.9th percentile) of\naround 2µs per operation, but max node size can be adjusted\naccording to user’s desired limits (Fig. 15).\nIndex size of ALEX and Learned Index is the sum of the sizes\nof all models used in the index and metadata; index size for\nALEX also includes internal node pointers. For ALEX, each\nlinear model consists of two 64-bit doubles which represent\nthe slope and intercept. Learned Index keeps two additional\nintegers per model that represent the error bounds. /T_he index\nsize of B+Tree and Model B+Tree is the sum of the sizes of all\ninner nodes, which for Model B+Tree includes the models in\neach node. /T_he index size of ART is the sum of inner node sizes\nminus the total size of keys, since keys are encoded into the in-\nner nodes. /T_he data size of ALEX is the sum of the sizes of the\narrays containing the keys and payloads, including gaps, as\nwell as the bitmap in each data node. /T_he data size of B+Tree\nis the sum of the sizes of all leaf nodes. At initialization, the\nGapped Arrays in data nodes are set to have 70% space utiliza-\ntion, comparable to B+Tree leaf node space utilization [13].\n6.1.1 Datasets. We run all experiments using 8-byte keys\nfrom some dataset and randomly generated /f_ixed-size pay-\nloads. We evaluate ALEX on 4 datasets, whose character-\nistics and CDFs are shown in Table 1 and Fig. 8. /T_he longi-\ntudes dataset consists of the longitudes of locations around\nthe world from Open Street Maps [ 2]. /T_he longlat dataset\nconsists of compound keys that combine longitudes and lati-\ntudes from Open Street Maps by applying the transformation\nk=180\u0001/f_loor¹longitudeº+latitude to every pair of longitude\nand latitude. /T_he resulting distribution of keys kis highly non-\nlinear. /T_he lognormal dataset has values generated according\nto a lognormal distribution with µ=0 andσ=2, multiplied\nby 109and rounded down to the nearest integer. /T_he YCSB\ndataset has values representing user IDs generated according\nto the YCSB Benchmark [ 8], which are uniformly distributed\nacross the full 64-bit domain, and uses an 80-byte payload.\n/T_hese datasets do not contain duplicate values. Unless other-\nwise stated, these datasets are randomly shuﬄed to simulate\na uniform dataset distribution over time.\n6.1.2 Workloads. OurprimarymetricforevaluatingALEX\nis average throughput. We evaluate throughput for /f_ive work-\nloads: (1) a read-only workload, (2) a read-heavy workload\nwith 95% reads and 5% inserts, (3) a write-heavy workload with\n9\n\n−100 0100\nKey0.000.250.500.751.00CDFlongitudes\n−25000 025000\nKeylonglat\n0 5\nKey 1e13lognormal\n0 1\nKey 1e19YCSB\n−50 0\nKey0.500.520.540.560.580.60CDFlongitudes\n−14000\nKeylonglat\n−40 −30 −20\nKey0.510500.510750.511000.511250.51150longitudes\n−14600\nKeylonglatFigure 8: Dataset CDFs, and zoomed-in CDFs.\nTable 1: Dataset Characteristics\nlongitudes longlat lognormal YCSB\nNum keys 1B 200M 190M 200M\nKey type double double 64-bit int 64-bit int\nPayload size 8B 8B 8B 80B\nTotal size 16GB 3.2GB 3.04GB 17.6GB\n50% reads and 50% inserts, (4) a short range query workload\nwith 95% reads and 5% inserts, and (5) a write-only workload,\nto complete the read-write spectrum. For the /f_irst three work-\nloads, reads consist of a lookup of a single key. For the short\nrange workload, a read consists of a key lookup followed by\na scan of the subsequent keys. /T_he number of keys to scan is\nselected randomly from a uniform distribution with a maxi-\nmum scan length of 100. For all workloads, keys to look up are\nselected randomly from the set of existing keys in the index\naccording to a Zip/f_ian distribution. /T_he /f_irst four workloads\nroughly correspond to Workloads C, B, A, and E from the YCSB\nbenchmark [ 8], respectively. For a given dataset, we initialize\nan index with 100 million keys. We then run the workload\nfor 60 seconds, inserting the remaining keys. We report the\nthroughput of operations completed in that time, where opera-\ntions are either inserts or reads. For the read-write workloads,\nwe interleave the operations: for the read-heavy workload\nand short range workload, we perform 19 reads/scans, then\n1 insert, then repeat the cycle; for the write-heavy workload,\nwe perform 1 read, then 1 insert, then repeat the cycle.\n6.2 Overall Results\n6.2.1 Read-only Workloads. For read-only workloads,\nFigs. 9a and 9f show that ALEX achieves up to 4.1 \u0002, 2.2\u0002,\n2.9\u0002, 3.0\u0002higher throughput and 800 \u0002, 15\u0002, 160\u0002, 8000\u0002\nsmaller index size than the B+Tree, Learned Index, Model\nB+Tree, and ART, respectively.Table 2: ALEX Statistics a/f_ter Bulk Load\nlongitudes longlat lognormal YCSB\nAvg depth 1.01 1.56 1.80 1\nMax depth 2 4 3 1\nNum inner nodes 55 1718 24 1\nNum data nodes 4450 23257 757 1024\nMin DN size 672B 16B 224B 12.3MB\nMedian DN size 161KB 39.6KB 2.99MB 12.3MB\nMax DN size 5.78MB 8.22MB 14.1MB 12.3MB\nOn the longlat and YCSB datasets, ALEX performance is\nsimilar to Learned Index. /T_he longlat dataset is highly non-\nuniform, so ALEX is unable to achieve high performance, even\nwith adaptive RMI. /T_he YCSB dataset is nearly uniform, so\nthe optimal allocation of models is uniform; ALEX adaptively\n/f_inds this optimal allocation, and Learned Index allocates this\nway by nature, so the resulting RMI structures are similar. On\nthe other two datasets, ALEX has more performance advan-\ntage over Learned Index, which we explain in Section 6.3.\nIn general, Model B+Tree outperforms B+Tree while also\nhaving smaller index size, because the tuned page size of\nModel B+Tree is always larger than those of B+Tree. /T_he ben-\ne/f_it of models in Model B+Tree is greatest when the key distri-\nbution within each node is more uniform, which is why Model\nB+Tree has least bene/f_it on non-uniform datasets like longlat.\n/T_he index size of ALEX is dependent on how well ALEX can\nmodel the data distribution. On the YCSB dataset, ALEX does\nnot require a large RMI to accurately model the distribution,\nso ALEX achieves small index size. However, on datasets that\nare more challenging to model such as longlat, ALEX has a\nlarger RMI with more nodes. ALEX has smaller index size\nthan the Learned Index, even when throughput is similar, for\ntwo reasons. First, ALEX uses model-based inserts to obtain\nbe/t_ter predictive accuracy for each model, which we show\nin Section 6.3, and therefore achieves high throughput while\nusing relatively fewer models. Second, ALEX adaptively al-\nlocates data nodes to diﬀerent parts of the key space and does\nnot use any more models than necessary (Fig. 3), whereas\nLearned Index /f_ixes the number of models and ends up with\nmany redundant models. /T_he index size of ART is higher than\nall other indexes. [ 21] claims that ART uses between 8 and\n52 bytes to store each key, which is in agreement with the\nobserved index sizes.\nTable 2 shows ALEX statistics a/f_ter bulk loading, including\ndata node (DN) sizes. /T_he root has depth 0. Average depth\nis averaged over keys. /T_he max depth of the tuned B+Tree is\n4 on the YCSB dataset and 5 on the other datasets. Datasets\nthat are easier to model result in fewer nodes. For uniform\ndatasets like YCSB, the data node sizes are also uniform.\n6.2.2 Read-Write Workloads. For read-write workloads,\nFigs. 9b to 9d and 9g to 9i show that ALEX achieves up to\n10\n\nFigure 9: ALEX vs. Baselines: /T_hroughput & Index Size. /T_hroughput includes model retraining time.\n4.0\u0002, 2.7\u0002, 2.7\u0002higher throughput and 2000 \u0002, 475\u0002, 36000\u0002\nsmaller index size than the B+Tree, Model B+Tree, and ART,\nrespectively. /T_he Learned Index has insert time orders of mag-\nnitude slower than ALEX and B+Tree, so we do not include\nit in these benchmarks.\n/T_he relative performance advantage of ALEX over base-\nlines decreases as the workload skews more towards writes,\nbecause all indexes must pay the cost of copying when split-\nting/expanding nodes. Copying has an especially big impact\nfor YCSB, for which payloads are 80 bytes. ART achieves\ncomparable throughput to ALEX on the write-only workload\nfor YCSB because ART does not keep payloads clustered, so\nit avoids the high cost of copying 80-byte payloads. Note that\nALEX could similarly avoid copying large payloads by storing\nunclustered payloads separately and keeping a pointer with\nevery key; however, this would impact scan performance. On\ndatasets that are challenging to model such as longlat, ALEX\nonly achieves comparable write-only throughput to Model\nB+Tree and ART, but is still faster than B+Tree.\n6.2.3 Range /Q_uery Workloads. Figs. 9e and 9j show that\nALEX maintains its advantage over B+Tree on the short range\nworkload, achieving up to 2.27 \u0002, 1.77\u0002higher throughput\nand 1000\u0002, 230\u0002smaller index size than B+Tree and Model\nB+Tree, respectively. However, the relative throughput ben-\ne/f_it decreases, compared to Fig. 9b. /T_his is because as scan\ntime begins to dominate overall query time, the speedups that\nALEX achieves on lookups become less apparent. /T_he ART\nimplementation from [ 9] does not support range queries; we\nsuspect range queries on ART would be slower than for the\nother indexes because ART does not cluster payloads, leading\nto poor scan locality. Appendix C.2 shows that ALEX contin-\nues to outperform other indexes on a workload that mixes\ninserts, point lookups, and short range queries.\nTo show how performance varies with range query selec-\ntivity, we compare ALEX against two B+Tree con/f_igurations\nwith increasingly larger range scan length over the longitudes\ndataset (Fig. 10a). In the /f_irst B+Tree con/f_iguration, we use\nFigure 10: (a) When scan length exceeds 1000 keys,\nALEX is slower on range queries than a B+Tree whose\npage size is re-tuned for diﬀerent scan lengths. (b)\nHowever, throughput of the re-tuned B+Tree suﬀers\nfor other operations, such as point lookups and inserts\nin the write-heavy workload.\nthe optimal B+Tree page size on the write-heavy workload\n(Fig. 9c), which is 1KB (solid green line). In the second B+Tree\ncon/f_iguration, we tune the B+Tree page size for each diﬀerent\nscan length (dashed green line).\nUnsurprisingly, Fig. 10a shows as scan length increases,\nthe throughput in terms of keys scanned per second increases\nfor all indexes due to be/t_ter locality and a smaller fraction of\ntime spent on the initial point lookup. Furthermore, ALEX\noutperforms the 1KB-page B+Tree for all scan lengths due to\nALEX’s larger nodes; median ALEX data node size is 161KB on\nthe longitudes dataset (Table 2), which bene/f_its scan locality—\nscanning larger contiguous chunks of memory leads to be/t_ter\nprefetching and fewer pointer chases. /T_his makes up for the\nGapped Array’s overhead.\nHowever, if we re-tune the B+Tree page size for each scan\nlength (dashed green line), the B+Tree outperforms ALEX\nwhen scan length exceeds 1000 keys because past this point,\nthe overhead of Gapped Array outpaces ALEX’s scan local-\nity advantage from having larger node sizes. However, this\ncomes at the cost of performance on other operations: Fig. 10b\nshows that if we run the re-tuned B+Tree on the write-heavy\nworkload, which includes both point lookups and inserts, its\n11\n\nFigure 11: ALEX takes 50% more than time than B+Tree\nto bulk load on average, but quickly makes up for this\nby having higher throughput.\nperformance would begin to decline when scan length ex-\nceeds 100 keys. In particular, larger B+Tree pages lead to a\nhigher number of search iterations for lookups and shi/f_ts for\ninserts; ALEX avoids both of these problems for large data\nnodes by using Gapped Arrays with model-based inserts. We\nshow in Appendix C.1 that this behavior also occurs on the\nother three datasets.\n6.2.4 Bulk Loading. We compare the time to initialize each\nindex with bulk loading, which includes the time to sort keys.\nFig. 11a shows that on average, ALEX only takes 50% more\ntime to bulk load than B+Tree, and in the worst case is only 2 \u0002\nslower than B+Tree. On the YCSB dataset, B+Tree and Model\nB+Tree take longer to bulk load due to the larger payload\nsize, but bulk loading ALEX remains eﬃcient due to its simple\nstructure (Table 2). Model B+Tree is slightly slower to bulk\nload than B+Tree due to the overhead of training models for\neach node. ART is slower to bulk load than B+Tree, Model\nB+Tree, and ALEX.\nALEX can quickly make up for its slower bulk loading\ntime than B+Tree by having higher throughput performance.\nFig. 11b shows that when running the read-heavy workload\non the longitudes dataset, ALEX’s total time usage (bulk load-\ning plus workload) drops below all other indexes a/f_ter only\n3 million inserts. We provide a more detailed bulk loading\nevaluation in Appendix A.\n6.2.5 Scalability. ALEX performance scales well to larger\ndatasets. We again run the read-heavy workload on the longi-\ntudes dataset, but instead of initializing the index with 100 mil-\nlion keys, we vary the number of initialization keys. Fig. 12a\nshows that as the number of indexed keys increases, ALEX\nmaintains higher throughput than B+Tree and Model B+Tree.\nIn fact, as dataset size increases, ALEX throughput decreases\nat a surprisingly slow rate. /T_his occurs because ALEX adapts\nits RMI structure in response to the incoming data.\n6.2.6 Dataset Distribution Shi/f_t. ALEX is robust to dataset\ndistribution shi/f_t. We initialize the index with the 50 million\nsmallest keys and run read-write workloads by inserting the\nremaining keys in random order. /T_his simulates distribution\nFigure 12: ALEX maintains high throughput when\nscaling to large datasets and under data distribution\nshi/f_ts. (RH = Read-Heavy, WH = Write-Heavy)\nshi/f_t because the keys we initialize with come from a com-\npletely disjoint domain than the keys we subsequently insert\nwith. Fig. 12b shows that ALEX maintains up to 3.2 \u0002higher\nthroughput than B+Tree in this scenario. ALEX is also robust\nto adversarial pa/t_terns such as sequential inserts in sorted\norder, in which new keys are always larger than the maximum\nkey currently indexed. Fig. 12c shows that when we initialize\nwith the 50 million smallest keys and insert the remaining\nkeys in ascending sorted order, ALEX has up to 3 :6\u0002higher\nthroughput than B+Tree. Appendix B further shows that\nALEX is robust to radically changing key distributions.\n6.3 Drilldown into ALEX Design Trade-oﬀs\nIn this section, we delve deeper into how node layout and\nadaptive RMI help ALEX achieve its design goals.\nPart of ALEX’s advantage over Learned Index comes from\nusing model-based insertion with Gapped Arrays in the data\nnodes, but most of ALEX’s advantage for dynamic workloads\ncomes from the adaptive RMI. To demonstrate the eﬀects of\neach contribution, Fig. 13 shows that taking a 2-layer Learned\nIndex and replacing the single dense array of values with a\nGapped Array per leaf (LI w/Gapped Array) already achieves\nsigni/f_icant speedup over Learned Index for the read-only\nworkload. However, a Learned Index with Gapped Arrays\nachieves poor performance on read-write workloads due to\nthe presence of fully-packed regions which require shi/f_ting\nmany keys for each insert. ALEX’s ability to adapt the RMI\nstructure to the data is necessary for good insert performance.\nDuring lookups, the majority of the time is spent doing\nlocal search around the predicted position. Smaller prediction\nerrors directly contribute to decreased lookup time. To ana-\nlyze the prediction errors of the Learned Index and ALEX, we\ninitialize an index with 100 million keys from the longitudes\ndataset, use the index to predict the position of each of the 100\nmillion keys, and track the distance between the predicted po-\nsition and the actual position. Fig. 14a shows that the Learned\nIndex has prediction error with mode around 8-32 positions,\nwith a long tail to the right. On the other hand, ALEX achieves\nmuch lower prediction error by using model-based inserts.\n12\n\nFigure 13: Impact of Gapped\nArray and adaptive RMI.\n Figure 14: ALEX achieves smaller prediction error than the Learned Index.\nTable 3: Data Node Actions When Full (Write-Heavy)\nlongitudes longlat lognormal YCSB\nExpand + scale 26157 113801 2383 1022\nExpand + retrain 219 2520 2 1026\nSplit sideways 79 2153 7 0\nSplit downwards 0 230 0 0\nTotal times full 26455 118704 2392 2048\nFigure 15: Latency of a\nsingle operation.\nFigure 16: Exponential vs.\nother search methods.\nFig. 14b shows that a/f_ter initializing, ALEX o/f_ten has no pre-\ndiction error, the errors that do occur are o/f_ten small, and the\nlong tail of errors has disappeared. Fig. 14c shows that even\na/f_ter 20 million inserts, ALEX maintains low prediction errors.\nOnce a data node becomes full, one of four actions happens:\nif there is no cost deviation, then (1) the node is expanded\nand the model is scaled. Otherwise, the node is either (2)\nexpanded and its model retrained, (3) split sideways, or (4)\nsplit downwards. Table 3 shows that in the vast majority of\ncases, the data node is simply expanded and the model scaled,\nwhich implies that models usually remain accurate even a/f_ter\ninserts, assuming no radical distribution shi/f_t. /T_he number of\noccurrences of a data node becoming full is correlated with\nthe number of data nodes (Table 2). On YCSB, expansion with\nmodel retraining is more common because the data nodes are\nlarge, so cost deviation o/f_ten results simply from randomness.\nUsers can adjust the max node size to achieve target tail la-\ntencies, if desired. In Fig. 15, we run the write-heavy workload\non the longitudes dataset, measuring the latency for every\noperation. As we increase the max node size, median and evenp99 latency of ALEX decreases, because ALEX has more /f_lex-\nibility to build a be/t_ter-performing RMI (e.g., ability to have\nhigher internal node fanout). However, maximum latency in-\ncreases, because an insert that triggers an expansion or split of\na large node is slow. If the user has strict latency requirements,\ntheycandecreasethemaxnodesizeaccordingly. A/f_terincreas-\ning the max node size beyond 64MB, latencies do not change\nbecause ALEX never decides to use a node larger than 64MB.\n6.3.1 Search Method Comparison. In order to show the\ntrade-oﬀ between exponential search and other search meth-\nods, we perform a microbenchmark on synthetic data. We cre-\nate a dataset with 100 million perfectly uniformly distributed\ndoubles. We then perform searches for 10 million randomly se-\nlected values from this dataset. We use three search methods:\nbinary search and biased quaternary search (proposed in [ 19]\nto take advantage of accurate predictions), each evaluated\nwith two diﬀerent error bound sizes, as well as exponential\nsearch. For each lookup, the search method is given a pre-\ndicted position that has some synthetic amount of error in the\ndistance to the actual position value. Fig. 16 shows that the\nsearch time of exponential search increases proportionally\nwith the logarithm of error size, whereas the binary search\nmethods take a constant amount of time, regardless of error\nsize. /T_his is because binary search must always begin search\nwithin its error bounds, and cannot take advantage of cases\nwhen the error is small. /T_herefore, exponential search should\noutperform binary search if the prediction error of the RMI\nmodels in ALEX is small. As we showed in Section 6.3, ALEX\nmaintains low prediction errors through model-based inserts.\n/T_herefore, ALEX is well suited to take advantage of expo-\nnential search. Biased quaternary search is competitive with\nexponential search when error is below σ(we setσ=8 for\nthis experiment; see [ 19] for details) because search can be\ncon/f_ined to a small range, but performs similarly to binary\nsearch when error exceeds σbecause the full error bound\nmust be searched. We prefer exponential search to biased qua-\nternary search due to its smoother performance degradation\nand simplicity of implementation (e.g., no need to tune σ).\n13\n\n7 RELATED WORK\nLearned Index Structures: /T_he most relevant work is the\nLearned Index [ 19], discussed in Section 2.2. Learned Index\nhas similarities to prior work that explored how to compute\ndown a tree index. Tries [ 17] use key pre/f_ixes instead of\nB+Tree spli/t_ters. Masstree [ 25] and Adaptive Radix Tree [ 21]\ncombine the ideas of B+Tree and trie to reduce cache misses.\nPlop-hashing [ 20] uses piecewise linear order-preserving\nhashing to distribute keys more evenly over pages. Digital\nB-tree [ 22] uses bits of a key to compute down the tree more\n/f_lexibly. [ 23] proposes to partially expand the space instead\nof always doubling when spli/t_ting in B+Tree. [ 12] proposes\nthe idea of interpolation search within B+Tree nodes; this\nidea was revisited in [ 28]. /T_he interpolation-based search\nalgorithms in [ 33] can complement ALEX’s search strategy.\nHermit [ 34] creates a succinct tree structure for secondary\nindexes.\nOther works propose replacing the leaf nodes of a B+Tree\nwith other data structures in order to compress the index,\nwhile maintaining search and update performance. FITing-\ntree [ 11] uses linear models in its leaf nodes, while BF-tree [ 1]\nuses bloom /f_ilters in its leaf nodes.\nAll these works share the idea that using extra computation\nor data structures can make search faster by reducing the num-\nber of binary search hops and corresponding cache misses,\nwhile allowing larger node sizes and hence a smaller index\nsize. However, ALEX is diﬀerent in several ways: (1) We use a\nmodel to split the key space, similar to a trie, but no search is re-\nquired until we reach the leaf level. (2) ALEX’s accurate linear\nmodels enable larger node sizes without sacri/f_icing search and\nupdate performance. (3) Model-based insertion reduces the\nimpact of model’s misprediction. (4) ALEX’s cost models au-\ntomatically adjust the index structure to dynamic workloads.\nMemory Optimized Indexes: /T_here is a large body of\nwork on optimizing tree index structures for main memory\nby exploiting hardware features such as CPU cache, multi-\ncore, SIMD, and prefetching. CSS-trees [ 30] improve B+Tree’s\ncache behavior by matching index node size to CPU cache-\nline size and eliminating pointers in index nodes by using\narithmetic operations to /f_ind child nodes. CSB+-tree [31] ex-\ntends the static CSS-trees by supporting incremental updates\nwithout sacri/f_icing CPU cache performance. [ 14] evaluates\nthe eﬀect of node size on the performance of CSB+-tree ana-\nlytically and empirically. pB+-tree [ 7] uses larger index nodes\nand relies on prefetching instructions to bring index nodes\ninto cache before nodes are accessed. In addition to optimizing\nfor cache performance, FAST [ 15] further optimizes searches\nwithin index nodes by exploiting SIMD parallelism.\nML in other DB components: Machine learning has\nbeen used to improve cardinality estimation [ 10,16], query op-\ntimization [ 26], workload forecasting [ 24], multi-dimensionalindexing [ 27], and data partitioning [ 35]. SageDB [ 18] envi-\nsions a database system in which every component is replaced\nby a learned component. /T_hese studies show that the use of\nmachine learning enables workload-speci/f_ic optimizations,\nwhich also inspired our work.\n8 CONCLUSION\nWe build on the excitement of learned indexes by proposing\nALEX, a new updatable learned index that eﬀectively com-\nbines the core insights from the Learned Index with proven\nstorage and indexing techniques. Speci/f_ically, we propose a\nGapped Array node layout that uses model-based inserts and\nexponential search, combined with an adaptive RMI structure\ndriven by simple cost models, to achieve high performance\nand low memory footprint on dynamic workloads. Our in-\ndepth experimental results show that ALEX not only consis-\ntently beats B+Tree across the read-write workload spectrum,\nit even beats the existing Learned Index, on all datasets, by\nup to 2.2\u0002with read-only workloads.\nWe believe this paper presents important learnings to our\ncommunity and opens avenues for future research in this area.\nWe intend to pursue open theoretical problems about ALEX\nperformance, supporting secondary storage for larger than\nmemory datasets, and new concurrency control techniques\ntailored to the ALEX design.\nAcknowledgements. /T_his research is supported by Google,\nIntel, and Microso/f_t as part of the MIT Data Systems and\nAI Lab (DSAIL) at MIT, NSF IIS 1900933, DARPA Award 16-\n43-D3M-FP040, and the MIT Air Force Arti/f_icial Intelligence\nInnovation Accelerator (AIIA).\nREFERENCES\n[1]Manos Athanassoulis and Anastasia Ailamaki. 2014. BF-Tree:\nApproximate Tree Indexing. Proc. VLDB Endow. 7, 14 (Oct. 2014),\n1881–1892. h/t_tps://doi.org/10.14778/2733085.2733094\n[2]Amazon AWS. [n.d.]. OpenStreetMap on AWS. h/t_tps:\n//registry.opendata.aws/osm/.\n[3]Michael Bender, Martin Farach-Colton, and Miguel Mosteiro. 2006.\nInsertion Sort is O(n log n). /T_heory of Computing Systems 39 (06 2006).\nh/t_tps://doi.org/10.1007/s00224-005-1237-z\n[4]Michael A Bender and Haodong Hu. 2007. An adaptive packed-memory\narray. ACM Transactions on Database Systems (TODS) 32, 4 (2007), 26.\n[5]Timo Bingmann. [n.d.]. STX B+ Tree. h/t_tps://panthema.net/2007/\nstx-btree/.\n[6]Zhichao Cao, Siying Dong, Sagar Vemuri, and David H.C. Du. 2020.\nCharacterizing, Modeling, and Benchmarking RocksDB Key-Value\nWorkloads at Facebook. In 18th USENIX Conference on File and Storage\nTechnologies (FAST 20) . USENIX Association, Santa Clara, CA, 209–223.\nh/t_tps://www.usenix.org/conference/fast20/presentation/cao-zhichao\n[7]Shimin Chen, Phillip B. Gibbons, and Todd C. Mowry. 2001. Improving\nIndex Performance through Prefetching. SIGMOD Rec. 30, 2 (May\n2001), 235–246. h/t_tps://doi.org/10.1145/376284.375688\n[8]Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan,\nand Russell Sears. 2010. Benchmarking Cloud Serving Systems with\nYCSB. In Proceedings of the 1st ACM Symposium on Cloud Computing\n14\n\n(Indianapolis, Indiana, USA) (SoCC ’10) . ACM, New York, NY, USA,\n143–154. h/t_tps://doi.org/10.1145/1807128.1807152\n[9] Armon Dadgar. [n.d.]. Adaptive Radix Trees implemented in C.\n[10] Anshuman Du/t_t, Chi Wang, Azade Nazi, Srikanth Kandula, Vivek\nNarasayya, and Surajit Chaudhuri. 2019. Selectivity Estimation for\nRange Predicates Using Lightweight Models. Proc. VLDB Endow. 12,\n9 (May 2019), 1044–1057. h/t_tps://doi.org/10.14778/3329772.3329780\n[11] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo\nFonseca, and Tim Kraska. 2019. FITing-Tree: A Data-Aware Index\nStructure. In Proceedings of the 2019 International Conference on\nManagement of Data (Amsterdam, Netherlands) (SIGMOD /f_i19) .\nAssociation for Computing Machinery, New York, NY, USA, 1189–1206.\nh/t_tps://doi.org/10.1145/3299869.3319860\n[12] Goetz Graefe. 2006. B-Tree Indexes, Interpolation Search, and\nSkew. In Proceedings of the 2nd International Workshop on Data\nManagement on New Hardware (Chicago, Illinois) (DaMoN /f_i06) .\nAssociation for Computing Machinery, New York, NY, USA, 5–es.\nh/t_tps://doi.org/10.1145/1140402.1140409\n[13] Goetz Graefe. 2011. Modern B-Tree Techniques. Found. Trends\nDatabases 3, 4 (April 2011), 203–402. h/t_tps://doi.org/10.1561/\n1900000028\n[14] Richard A. Hankins and Jignesh M. Patel. 2003. Eﬀect of Node Size\non the Performance of Cache-Conscious B+-Trees. In Proceedings of\nthe 2003 ACM SIGMETRICS International Conference on Measurement\nand Modeling of Computer Systems (San Diego, CA, USA) (SIGMETRICS\n/f_i03). Association for Computing Machinery, New York, NY, USA,\n283–294. h/t_tps://doi.org/10.1145/781027.781063\n[15] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar,\nAnthony D. Nguyen, Tim Kaldewey, Victor W. Lee, Sco/t_t A. Brandt, and\nPradeep Dubey. 2010. FAST: Fast Architecture Sensitive Tree Search\non Modern CPUs and GPUs. In Proceedings of the 2010 ACM SIGMOD\nInternational Conference on Management of Data (Indianapolis, Indiana,\nUSA) (SIGMOD /f_i10) . Association for Computing Machinery, New\nYork, NY, USA, 339–350. h/t_tps://doi.org/10.1145/1807167.1807206\n[16] Andreas Kipf, /T_homas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz,\nand Alfons Kemper. 2018. Learned Cardinalities: Estimating Correlated\nJoins with Deep Learning. arXiv preprint arXiv:1809.00677 (2018).\n[17] Donald E. Knuth. 1998. /T_he Art of Computer Programming, Volume 3:\n(2nd Ed.) Sorting and Searching . Addison Wesley Longman Publishing\nCo., Inc., USA.\n[18] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi, Jialin Ding,\nAni Kristo, Guillaume Leclerc, Samuel Madden, Hongzi Mao, and\nVikram Nathan. 2019. SageDB: A Learned Database System. In CIDR\n2019, 9th Biennial Conference on Innovative Data Systems Research, Asilo-\nmar, CA, USA, January 13-16, 2019, Online Proceedings . www.cidrdb.org.\nh/t_tp://cidrdb.org/cidr2019/papers/p117-kraska-cidr19.pdf\n[19] Tim Kraska, Alex Beutel, Ed H. Chi, Jeﬀrey Dean, and Neoklis Polyzotis.\n2018. /T_he Case for Learned Index Structures. In Proceedings of the 2018\nInternational Conference on Management of Data (Houston, TX, USA)\n(SIGMOD /f_i18) . Association for Computing Machinery, New York, NY,\nUSA, 489–504. h/t_tps://doi.org/10.1145/3183713.3196909\n[20] Hans-Peter Kriegel and Bernhard Seeger. 1988. PLOP-Hashing: A\nGrid File without Directory. In Proceedings of the Fourth International\nConference on Data Engineering . IEEE Computer Society, USA, 369–376.\n[21] Viktor Leis, Alfons Kemper, and /T_homas Neumann. 2013. /T_he\nAdaptive Radix Tree: ARTful Indexing for Main-Memory Databases.\nInProceedings of the 2013 IEEE International Conference on Data\nEngineering (ICDE 2013) (ICDE /f_i13) . IEEE Computer Society, USA,\n38–49. h/t_tps://doi.org/10.1109/ICDE.2013.6544812[22] David B Lomet. 1981. Digital B-trees. In Proceedings of the seventh\ninternational conference on Very Large Data Bases-Volume 7 . VLDB\nEndowment, 333–344.\n[23] David B. Lomet. 1987. Partial Expansions for File Organizations\nwith an Index. ACM Trans. Database Syst. 12, 1 (March 1987), 65–84.\nh/t_tps://doi.org/10.1145/12047.12049\n[24] Lin Ma, Dana Van Aken, Ahmed Hefny, Gustavo Mezerhane, Andrew\nPavlo, and Geoﬀrey J. Gordon. 2018. /Q_uery-Based Workload Forecast-\ning for Self-Driving Database Management Systems. In Proceedings\nof the 2018 International Conference on Management of Data (Houston,\nTX, USA) (SIGMOD /f_i18) . Association for Computing Machinery, New\nYork, NY, USA, 631–645. h/t_tps://doi.org/10.1145/3183713.3196908\n[25] Yandong Mao, Eddie Kohler, and Robert Tappan Morris. 2012. Cache\nCra/f_tiness for Fast Multicore Key-Value Storage. In Proceedings of the\n7th ACM European Conference on Computer Systems (Bern, Switzerland)\n(EuroSys /f_i12) . Association for Computing Machinery, New York, NY,\nUSA, 183–196. h/t_tps://doi.org/10.1145/2168836.2168855\n[26] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad\nAlizadeh, Tim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019.\nNeo: A Learned /Q_uery Optimizer. Proc. VLDB Endow. 12, 11 (July 2019),\n1705–1718. h/t_tps://doi.org/10.14778/3342263.3342644\n[27] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska.\n2020. Learning Multi-dimensional Indexes. In Proceedings of the 2020\nInternational Conference on Management of Data (Portland, OR, USA)\n(SIGMOD /f_i20) . Association for Computing Machinery, New York, NY,\nUSA. h/t_tps://doi.org/10.1145/3318464.3380579\n[28] /T_homas Neumann. 2017. /T_he Case for B-Tree Index Struc-\ntures. h/t_tp://databasearchitects.blogspot.com/2017/12/\nthe-case-for-b-tree-index-structures.html.\n[29] Raghu Ramakrishnan and Johannes Gehrke. 2002. Database\nManagement Systems (3 ed.). McGraw-Hill, Inc., USA.\n[30] Jun Rao and Kenneth A. Ross. 1999. Cache Conscious Indexing\nfor Decision-Support in Main Memory. In Proceedings of the 25th\nInternational Conference on Very Large Data Bases (VLDB /f_i99) . Morgan\nKaufmann Publishers Inc., San Francisco, CA, USA, 78–89.\n[31] Jun Rao and Kenneth A. Ross. 2000. Making B+- Trees Cache Conscious\nin Main Memory. In Proceedings of the 2000 ACM SIGMOD International\nConference on Management of Data (Dallas, Texas, USA) (SIGMOD /f_i00) .\nAssociation for Computing Machinery, New York, NY, USA, 475–486.\nh/t_tps://doi.org/10.1145/342009.335449\n[32] TPC. [n.d.]. TPC-C. h/t_tp://www.tpc.org/tpcc/.\n[33] Peter Van Sandt, Yannis Chronis, and Jignesh M. Patel. 2019. Eﬃciently\nSearching In-Memory Sorted Arrays: Revenge of the Interpolation\nSearch?. In Proceedings of the 2019 International Conference on\nManagement of Data (Amsterdam, Netherlands) (SIGMOD /f_i19) .\nAssociation for Computing Machinery, New York, NY, USA, 36–53.\nh/t_tps://doi.org/10.1145/3299869.3300075\n[34] Yingjun Wu, Jia Yu, Yuanyuan Tian, Richard Sidle, and Ronald\nBarber. 2019. Designing Succinct Secondary Indexing Mechanism by\nExploiting Column Correlations. In Proceedings of the 2019 Interna-\ntional Conference on Management of Data (Amsterdam, Netherlands)\n(SIGMOD /f_i19) . Association for Computing Machinery, New York, NY,\nUSA, 1223–1240. h/t_tps://doi.org/10.1145/3299869.3319861\n[35] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke,\nYinan Li, Umar F. Minhas, Per- ˚Ake Larson, Donald Kossmann, and\nRajeev Acharya. 2020. Qd-tree: Learning Data Layouts for Big Data\nAnalytics. In Proceedings of the 2020 International Conference on\nManagement of Data .\n15\n\nFigure 17: With both optimizations (AMC and ACC),\nALEX only takes 50% more than time than B+Tree to\nbulk load when averaged across four datasets.\nA Extended Bulk Loading Evaluation\nIn this section, we provide an extended version of Section 6.2.4,\nwhich evaluates the speed of ALEX’s bulk loading mechanism\nagainst other indexes. ALEX uses two optimizations for bulk\nloading—approximate model computation (AMC) and approx-\nimate cost computation (ACC)—which we explain in more\ndetail below.\nFor each index, we bulk load 100 million keys from each of\nthe four datasets from Section 6. /T_his includes the time used\nto sort the 100 million keys. Fig. 17, which is a more detailed\nversion of Fig. 11a, shows that on average, ALEX with no\noptimizations takes 3.6 \u0002more time to bulk load than B+Tree,\nwhich is the fastest index to bulk load. However, with the\nAMC optimization, ALEX takes 2.6 \u0002more time on average\nthan B+Tree. With both optimizations, ALEX only takes 50%\nmore than time on average than B+Tree, and in the worst\ncase is only 2\u0002slower than B+Tree. /T_he results for ALEX in\nSection 6.2.4 use both optimizations. On the YCSB dataset,\nALEX’s structure is very simple (Table 2), and therefore is\nvery eﬃcient to bulk load even when unoptimized; in fact,\nALEX’s large node sizes allow ALEX to bulk load faster than\nother indexes due to the bene/f_its of locality.\nFig. 18 shows the impact of the two optimizations on the\nthroughput of running a read-heavy or write-heavy work-\nload on ALEX a/f_ter bulk loading. /T_he AMC optimization\nhas negligible impact on throughput for all datasets and for\nboth workloads. Adding the ACC optimization has negligible\nimpact on the read-heavy workload, but decreases through-\nput by up to 9.6% on the write-heavy workload; we provide\nexplanation for this behavior below.\nBased on these results, we conclude that the AMC optimiza-\ntion should always be used to improve bulk loading perfor-\nmance, whereas the ACC optimization might cause a slight\nFigure 18: Using the AMC optimization when bulk\nloading does not cause any noticeable change in ALEX\nperformance, but ACC can cause a slight decrease in\nthroughput for write-heavy workloads.\ndecrease in throughput performance and therefore should be\nused only if faster bulk loading is required. We now explain\nthe two optimizations in more detail.\nA.1 Approximate Model Computation. We perform ap-\nproximate model computation (AMC) eﬃciently while achiev-\ning accuracy through progressive systematic sampling . Given\na data node of sorted keys, we perform systematic sampling\n(i.e., sampling every nth key) to obtain a small sample of keys,\nand compute a linear regression model using that sample.\nWe then repeatedly double the sample size and recompute\nthe linear model using the larger sample. When the relative\nchange in the model parameters (i.e., the slope and intercept)\nboth change by less than 1% from one sample to the next, we\nterminate the process. In our experience, this 1% threshold\nstrikes a balance between achieving accuracy and using small\nsample sizes, and did not need to be tuned.\nNote that by using systematic sampling, all keys in the\nsample used to compute the current model will also appear in\nall subsequent samples. /T_herefore, each linear model can be\ncomputed progressively starting from the existing model com-\nputed from the previous sample, instead of from scratch. No\nredundant work is done, and even in the worst case, AMC will\ntake no more time than computing one linear model from all\nkeys (if we ignore minor overheads and the eﬀects of locality).\nA.2 Approximate Cost Computation. We also perform ap-\nproximate intra-node cost computation (ACC) for a data node\nof sorted keys through progressive systematic sampling. How-\never, ACC diﬀers from AMC in two ways. First, the cost for\na data node must be computed from scratch for each sample;\nthe cost depends on where keys are placed within a Gapped\nArray, which itself depends on which keys are present in the\nsample. Second, and more importantly, the cost of a data node\nnaturally increases with the number of keys in the node. /T_his\nmakes ACC an extrapolation problem (i.e., use the cost of\na small sample to predict the cost of the entire data node),\nwhereas AMC is an estimation problem (i.e., use a small sam-\nple to directly estimate the model parameters).\n16\n\nACC repeatedly doubles its sample size. Let the latest three\nsamples be s1, which is half the size of s2, which is half the\nsize of s3. Let the costs computed from these samples be c1,c2,\nandc3, respectively. We use the c1andc2to perform a linear\nextrapolation to predict c3. If this prediction is accurate (i.e.,\nif relative error with the true c3is within 20%), then we use\nc2andc3to perform a linear extrapolation to predict the cost\nof the entire data node, and we terminate the process.3Oth-\nerwise, we continue doubling the sample size. /T_he intuition\nbehind this process is that we want to verify the accuracy of\nextrapolation using small samples before extrapolating to the\nentire data node. We allow a higher relative error than for\nAMC because the extrapolation process is inherently impre-\ncise, since it is impossible to accurately predict the cost using\na sample without a priori knowledge of the data distribution.\nWe can now explain why Fig. 18 shows that adding the\nACC optimization decreases throughput on the write-heavy\nworkload by up to 9.6%. It is because the average number of\nshi/f_ts per insert, which is one component of the intra-node\ncost, is diﬃcult to estimate accurately. /T_herefore, if ACC un-\nderestimates the component of cost related to shi/f_ts, the bulk\nloaded ALEX structure may be ineﬃcient for inserts (e.g., an\ninsert that requires more shi/f_ts than expected can be very\nslow). /T_he intra-node cost is more diﬃcult to approximate\naccurately for the longitudes and longlat datasets, which is\nwhy the decrease in throughput is most noticeable for those\ntwo datasets. However, note that over time, the dynamic\nnature of ALEX will eventually correct for incorrectly esti-\nmated costs, so throughput performance in the long run will\nbe independent of the bulk loading mechanism.\nB Extreme Distribution Shi/f_t Evaluation\nIn order to evaluate the performance of ALEX under a radi-\ncally changing key distribution, we combine the four datasets\nfrom Section 6 into one dataset by randomly selecting 50 mil-\nlion keys from each of the four datasets in order to create one\ncombined dataset with 200 million keys. We scaled keys from\neach dataset to /f_it in the same domain. Note that we would\nnot typically expect a single table to contain keys from four\nindependent distributions. /T_herefore, this complex combined\ndataset is an extreme stress test for the adaptibility of ALEX.\nWe run a write-heavy workload (50% point lookups and\n50% inserts) over the combined dataset, but we vary the order\nin which keys are bulk loaded and inserted. For all variants,\nwe bulk load using 50 million keys and run the write-heavy\nworkload until the remaining 150 million keys are all inserted.\nWe create four variants that represent distribution shi/f_t; each\n3In reality, we do not predict the cost directly, but rather each component\nof the cost (search iterations per lookup and shi/f_ts per insert) independently.\n/T_his is because the expected extrapolation behavior diﬀers: iterations per\nlookup grows logarithmically with sample size, whereas shi/f_ts per insert\ngrows linearly with sample size.\nFigure 19: ALEX maintains high performance un-\nder radically changing key distribution, although\nperformance does diﬀer slightly depending on the\ndistribution used for bulk loading.\nvariant bulk loads using the 50 million keys selected from\none of the four original datasets, then gradually inserts keys\nfrom the other three original datasets, in order. For exam-\nple, the “L-LN-LL-Y” variant bulk loads using the 50 million\nkeys selected from the longitudes (L) dataset, then runs the\nwrite-heavy workload by inserting the 50 million keys from\nthe lognormal (LN) dataset, then the longlat (LL) dataset, and\n/f_inally the YCSB (Y) dataset. For reference, we also include a\nvariant in which all 200 million keys are shuﬄed, so that no\nkey distribution shi/f_t is observed.\nFig. 19 provides three insights. First, on the workload that\nrepresents no distribution shi/f_t (“Shuﬄed”), ALEX continues\nto outperform other indexes. It is interesting to note that the\nthroughput of ALEX on the combined dataset is between the\nthroughputs achieved on each dataset individually (Fig. 9c):\nhigher than for longlat, and lower than for the other three\ndatasets. Second, ALEX achieves lower throughput in the\nfour variants that represent distribution shi/f_t than without\ndistribution shi/f_t, but still outperforms other indexes. /T_his\nresult aligns with the intuition that ALEX must spend extra\ntime restructuring itself to adapt to the changing key distribu-\ntion. /T_hird, the throughput diﬀers based on which dataset’s\nkeys are used to bulk load ALEX. When bulk loading using\nkeys from a complex key distribution, such as longlat, ALEX\nachieves throughput similar to the variant with no distribu-\ntion shi/f_t; on the other hand, when bulk loading using keys\nfrom a simple key distribution, such as YCSB, ALEX through-\nput suﬀers. /T_his is because when bulk loading with a simple\nkey distribution, the bulk loaded structure of ALEX will be\nshallow, with few nodes (Table 2). When the subsequently\ninserted keys come from a much more complex key distri-\nbution, ALEX must quickly adapt its structure to be deeper\n17\n\nand have more nodes, which can incur signi/f_icant overhead.\nOn the other hand, when bulk loading with a complex key\ndistribution, the bulk loaded structure is already deep, with\nmany nodes, and so can more readily adapt to changes in the\nkey distribution without too much overhead.\nTo allow ALEX to more quickly adapt the RMI structure to\nradically changing key distributions: (1) we check data nodes\nperiodically for cost deviation instead of only when the data\nnode is full, and (2) if the number of shi/f_ts per insert in a data\nnode is extremely high, we force the data node to split (as\nopposed to expanding and retraining the model). When no\ndistribution shi/f_t occurs, these two checks have negligible\nimpact on performance, because checking for cost deviation\nhas minimal overhead and cost deviation occurs infrequently\n(Table 3). By default, we check for cost deviation for every 64\ninserts into that data node, and over 100 shi/f_ts per insert is\nconsidered extremely high.\nC Extended Range /Q_uery Evaluation\nC.1 Varying Range /Q_uery Scan Length. We extend the ex-\nperiment from Fig. 10 to all four datasets. Fig. 20 shows that\nacross all datasets, ALEX maintains its advantage over /f_ixed-\npage-size B+Tree, and re-tuning the B+Tree page size can\nlead to be/t_ter range query performance but will decrease per-\nformance on point lookups and inserts. For both ALEX and\nB+Tree, performance on YCSB is slower than for the other\nthree datasets because YCSB has a larger payload size, which\nworsens scan locality.\nC.2 Mixed Workload Evaluation. We evaluate a mixed\nworkload with 5% inserts, 85% point lookups, and 10% range\nqueries with a maximum scan length of 100. /T_he remainder\nof the experimental setup is the same as in Section 6.1. Fig. 21\nshows that ALEX maintains its performance advantage over\nother indexes. /T_he ART implementation from [ 9] does not\nsupport range queries.\nD Drilldown into Cost Computation\nIn this section, we /f_irst provide more details about the cost\nmodel introduced in Section 4.3.4. We then evaluate the per-\nformance of computing costs using cost models.\nD.1 CostModelDetails. Weformallyde/f_inethecostmodel\nusing the terms in Table 4. At a high level, the intra-node cost\nof a data node represents the average time to perform an op-\neration (i.e., a point lookup or insert) on that data node, and\nthe TraverseToLeaf cost of a data node represents the time for\ntraversing from the root node to the data node.\nFor a given data node N2D, the intra-node cost CI¹Nºis\nde/f_ined as\nCI¹Nº=wsS¹Nº+wiI¹NºF¹Nº (1)Table 4: Terms used to describe the cost model\nTerm Description\nA An instantiation of ALEX\nN Set of all nodes inA\nD Set of data nodes in A. /T_his means thatD\u0012N\nS¹NºAverage number of exponential search iterations\nfor a lookup in N2D\nI¹Nº Average number of shi/f_ts for an insert into N2D\nK¹Nº Number of keys in N2D\nF¹NºFraction of operations that are inserts (as opposed\nto lookups) in N2D\nCI¹Nº Intra-node cost of N2D\nCT¹Nº TraverseToLeaf cost of N2D\nD¹Nº Depth of N2N(root node has depth 0)\nB¹Aº Total size in bytes of all nodes in A\nws;wi;wd;wbFixed pre-de/f_ined weight parameters\nBoth lookups and inserts must perform an exponential search,\nwhereas only inserts must perform shi/f_ts. /T_his is why I¹Nº\nis weighted by F¹Nº.\nFor a given data node N2D, the TraverseToLeaf cost\nCT¹Nºof traversing from the root node to Nis de/f_ined as\nCT¹Nº=wdD¹Nº+wbB¹Aº (2)\n/T_he depth of Nis the number of pointer chases needed to\nreach the data node. In our cost model, every traverse to leaf\nhas a /f_ixed cost that is caused by the total size of the ALEX\nRMI, because larger RMI causes worse cache locality.\nFor an instantiation of ALEX A, the cost ofArepresents\nthe average time to perform a query (i.e., a point lookup or\ninsert) starting from the root node, and is de/f_ined as\nC¹Aº=Í\nN2D¹CI¹Nº+CT¹NººK¹NºÍ\nN2DK¹Nº(3)\nIn other words, the cost of Ais the sum of the intra-node cost\nand TraverseToLeaf cost of each data node, normalized by\nhow many keys are contained in the data node. We normalize\nbecause each data node does not contribute equally to average\nquery time. For example, a data node that has high intra-node\ncost but is rarely queried might not have as much impact on\naverage query time as a data node with lower intra-node cost\nthat is frequently queried. We use the number of keys in each\ndata node as a proxy for its impact on the average query time.\nAn alternative is to normalize using the true query access\nfrequency of each data node.\n/T_he weight parameters ws;wi;wd;wbdo not need to be\ntuned for each dataset or workload, because they represent\n/f_ixed quantities. For our evaluation, we set ws=10,wi=\n1,wd=10, and wb=10\u00006. In terms of impact on throughput\nperformance, these weights intuitively mean that each expo-\nnential search iteration takes 10 ns, each shi/f_t takes 1 ns, each\npointer chase to traverse down one level of the RMI takes\n10 ns, and each MB of total size contributes a slowdown of 1\n18\n\nFigure 20: Across all datasets, ALEX maintains an advantage over /f_ixed-page-size B+Tree even for longer range\nscans.\nFigure 21: ALEX maintains high performance under\na mixed workload with 5% inserts, 85% point lookups,\nand 10% short range queries.\nns due to worse cache locality. As a side eﬀect, wbacts as a\nregularizer to prevent the RMI from growing unnecessarily\nlarge. We found that our simple cost model performed well\nthroughout our evaluation. However, it may still be bene/f_icial\nto formulate a more complex cost model that more accurately\nre/f_lects true runtime; this is le/f_t as future work.\nD.2 Cost Computation Performance. /T_he cost of the entire\nRMI, C¹Aº, is never explicitly computed. Instead, all deci-\nsions based on cost are made locally. /T_his is possible due to\nthe linearity of the cost model. For example, when deciding\nbetween expanding a data node and spli/t_ting the data node\nin two, we compare the incremental impact on C¹Aºbetween\nthe two options. /T_his only involves computing the intra-node\ncost of the expanded data node and each of the two split dataTable 5: Fraction of time spent on cost computation\nlongitudes longlat lognormal YCSB\nRead-Only 0 0 0 0\nRead-Heavy 0.000271 0.000214 0.000617 0\nWrite-Heavy 0.00142 0.00901 0.00452 0.116\nWrite-Only 0.0270 0.0732 0.0237 0.149\nnodes; the intra-node costs of all other data nodes in the RMI\nremain the same.\nCost computation occurs at two points during ALEX opera-\ntion (Section 4.3.5): (1) when a data node becomes full, the ex-\npected intra-node cost is compared to the empirical intra-node\ncost to check for cost deviation. /T_his comparison has very low\nperformance overhead because the empirical values of S¹Nº\nandI¹Nºare maintained by the data node, so computing the\nempirical intra-node cost merely involves three multiplica-\ntions and an addition. (2) If cost deviation is detected, ALEX\nmust make a cost-based decision about how to adjust the RMI\nstructure. /T_his involves computing the expected intra-node\ncost of candidate data nodes which may be created as a result\nof adjusting the RMI structure. Since the candidate data nodes\ndo not yet exist, we must compute the expected S¹NºandI¹Nº,\nwhich involves implicitly building the candidate data node.\n/T_he majority of time spent on cost-based decision making is\nspent on computing expected S¹NºandS¹Iº.\nTable 5 shows the fraction of overall workload time spent\non computing costs and making cost-based decisions. On\nthe read-only workload, no time is spent on cost-based de-\ncision because nodes never become full. As the fraction of\nwrites increases, an increasing fraction of time is spent on\ncost computation because nodes become full more frequently.\nHowever, even on the write-only workload, cost computation\ntakes up a small fraction of overall time spent on the workload.\n19\n\nYCSB sees the highest fraction of time spent on cost computa-\ntion, due to two factors: data nodes are larger, so computing\nS¹NºandI¹Nºfor larger candidate nodes takes more time,\nand lookups and inserts on YCSB are eﬃcient, so data nodes\nbecome full more quickly. Longlat sees the next highest frac-\ntion of time spent on cost computation, which is due to the\nhigh frequency with which data nodes become full (Table 3).\nE Comparison of Gapped Array and PMA\n/T_he Gapped Array structure introduced in Section 3.2 has\nsome similarities to an existing data structure known as the\nPacked Memory Array (PMA) [ 4]. In this section, we /f_irst\ndescribe the PMA, and then we describe why we choose to\nnot use the PMA within ALEX.\nLike the Gapped Array, PMA is an array with gaps. Unlike\nthe Gapped Array, PMA is designed to uniformly space its\ngaps between elements and to maintain this property as new\nelements are inserted. /T_he PMA achieves this goal by rebal-\nancing local portions of the array when the gaps are no longer\nuniformly spaced. Under random inserts from a static distri-\nbution, the PMA can insert elements in O¹lognºtime, which\nis the same as the Gapped Array. However, when inserts do\nnot come from a static distribution, the PMA can guarantee\nworst-case insertion in O¹log2nºtime, which is be/t_ter than\nthe worst case of the Gappd Array, which is O¹nºtime.\nWe now describe the PMA more concretely; more details\ncan be found in [ 4]. /T_he PMA is an array whose size is al-\nways a power of 2. /T_he PMA divides itself into equally spaced\nsegments, and the number of segments is also a power of 2.\n/T_he PMA builds an implicit binary tree on top of the array,\nwhere each segment is a leaf node, each inner node represents\nthe region of the array covered by its two children, and the\nroot node represents the entire array. /T_he PMA places density\nbounds on each node of this implicit binary tree, where the\ndensity bound determines the maximum ratio of elements to\npositions in the region of the array represented by the node.\n/T_he nodes nearer the leaves will have higher density bounds,\nand the nodes nearer the root will have lower density bounds.\n/T_he density bounds guarantee that no region of the array will\nbecome too packed. If an insertion into a segment will violate\nthe segment’s density bounds, then we can /f_ind some local\nregion of the array and uniformly redistribute all elements\nwithin this region, such that a/f_ter the redistribution, none of\nthe density bounds are violated. As the array becomes more\nfull, ultimately no local redistribution can avoid violating\ndensity bounds. At this point, the PMA expands by doubling\nin size and inserting all elements uniformly spaced in the\nexpanded array.\nWe do not use the PMA as the underlying storage structure\nfor ALEX data nodes because the PMA negates the bene/f_its\nof model-based inserts, which is critical for search perfor-\nmance. For example, when rebalancing a local portion ofthe array, the PMA spreads the keys in the local region over\nmore space, which worsens search performance because the\nkeys are moved further away from their predicted location.\nFurthermore, the main bene/f_it of PMA—eﬃcient inserts for\nnon-static or complex key distributions—is already achieved\nby ALEX through the adaptive RMI structure. In our evalu-\nation, we found that ALEX using data nodes built on Gapped\nArrays consistently outperformed data nodes built on PMA.\nF Analysis of Model-based Search\nModel-based inserts try to place keys in Gapped Array in their\npredicted positions. We analyze the trade-oﬀ between Gapped\nArray space usage and search performance in terms of c, the\nratio of Gapped Array slots to number of actual keys. Assume\nthe keys in the data node are x1<x2<\u0001\u0001\u0001<xn, and the linear\nmodel before rounding is /y.alt=ax+bwhen c=1, i.e., when no\nextra space is allocated. De/f_ine δi=xi+1\u0000xi;∆i=xi+2\u0000xi. We\n/f_irst present a condition under which all the keys in that data\nnode are placed in the predicted location, i.e., search for all\nkeys are direct hits.\nT/h.sc/e.sc/o.sc/r.sc/e.sc/m.sc .1. When c\u00151\naminn\u00001\ni=1δi, every key in the data node\nis placed in the predicted location exactly.\nP/r.sc/o.sc/o.sc/f.sc. Consider two keys in the leaf node xiandxj;i,j.\n/T_he predicted locations before rounding are /y.altiand/y.altj, re-\nspectively. Whenj/y.alti\u0000/y.altjj\u00151, we know that the rounded\nlocationsb/y.alticandb/y.altjccannot be equal. Under the linear\nmodel/y.alt=c¹ax+bº, we can write the condition as:\nj/y.alti\u0000/y.altjj=jca¹xi\u0000xjºj\u00151 (4)\nIf this condition is true for all the pairs ¹i;jº;i,j, then all the\nkeys will have a unique predicted location. For the condition\nEq. (4) to be true for all i,j, it suﬃces to have:\nn\u00001\nmin\ni=1ca¹xi+1\u0000xiº\u00151 (5)\nwhich is equivalent to c\u00151\naminn\u00001\ni=1δi. \u0003\nWe now understand that c=1 corresponds to the optimal\nspace, and c\u00151\naminn\u00001\ni=1δi=cmaxcorresponds to the optimal\nsearch time (ignoring the eﬀect of cache misses). We now\nbound the number of keys with direct hits when c<cmax.\nT/h.sc/e.sc/o.sc/r.sc/e.sc/m.sc .2. /T_he number of keys placed in the predicted\nlocation is no larger than 2+\f\ff1\u0014i\u0014n\u00002j∆i>1\ncag\f\f, where\f\ff1\u0014i\u0014n\u00002j∆i>1\ncag\f\fis the number of ∆i’s larger than1\nca.\nP/r.sc/o.sc/o.sc/f.sc. We de/f_ine a mapping f:»n\u00002¼!» n¼, where f¹iº\nis de/f_ined recursively according to the following cases:\nCase (1):/y.alti+2\u0000/y.alti>1. Let f¹iº=1. Case (2): /y.alti+2\u0000/y.alti\u0014\n1;b/y.alti+1c=b/y.altic;f¹i\u00001º\u0014iori=1. Let f¹iº=i+1. Case (3):\nNeither case (1) or (2) is true. Let f¹iº=i+2.\nWe prove that 81\u0014i<j\u0014n\u00002, iff¹iº>1;f¹jº>1, then\ni+1\u0014f¹iº\u0014i+2;j+1\u0014f¹jº\u0014j+2, and f¹iº<f¹jº.\n20\n\nFirst, when f¹iº>1;f¹jº>1, we know that case (1) is false\nfor both iandj. Sof¹iºis either i+1 ori+2, and f¹jºis either\nj+1 orj+2.\nSecond, if i+1<j, then f¹iº\u0014i+2<j+1\u0014f¹jº. So we only\nneed to prove f¹iº<f¹jºwhen i+1=j. Now consider the\nonly two possible values for f¹jº,j+1 and j+2, when i+1=j.\nIff¹jº=j+1=i+2, by de/f_inition we know that case (2) is true\nforf¹jº. /T_hat means f¹j\u00001º=jor 1. But we already know\nf¹j\u00001º=f¹iº>1. So f¹iº=f¹j\u00001º=j=i+1<i+2=f¹jº. If\nf¹jº=j+2, then f¹iº\u0014i+2<j+2=f¹jº.\nSo far, we have proved that f¹iºis unique when f¹iº>1.\nNow we prove that the key xf¹iºis not placed inb/y.altf¹iºcwhen\nf¹iº>1, i.e., either case (2) or case (3) is true for f¹iº. In both\ncases,/y.alti+2\u0000/y.alti\u00141, and the rounded integers b/y.alti+2candb/y.altic\nmust be either equal or adjacent: b/y.alti+2c\u0000b/y.altic\u00141. /T_hat means\nb/y.alti+1cmust be equal to either b/y.alti+2corb/y.altic.\nWe prove by mathematical induction. For the minimal i\ns.t.f¹iº>1, if case (2) is true, b/y.alti+1c=b/y.altic. /T_hat means xi+1\ncannot be placed at b/y.alti+1cbecause that location is already\noccupied before xi+1is inserted. And f¹iº=i+1 by de/f_ini-\ntion. If case (2) is false, since we already know /y.alti+2\u0000/y.alti\u00141,\nf¹i\u00001º=1 ori=1, it follows thatb/y.alti+1c>b/y.altic. /T_hat im-\npliesb/y.alti+1c=b/y.alti+2c. Soxi+2cannot be placed at b/y.alti+2c. And\nf¹iº=i+2 because case (3) happens.\nGiven that the key xf¹i\u00001ºis not placed atb/y.altf¹i\u00001ºcwhen\nf¹i\u00001º>1, we now prove it is also true for i. /T_he proof for case\n(2) is the same as above. If case (2) is false, and b/y.alti+1c>b/y.altic,\nthe proof is also the same as above. /T_he remaining possibil-\nity of case (3) is that b/y.alti+1c=b/y.altic, and f¹i\u00001º=i+1. /T_he\ninductive hypothesis states that xi+1is not placed atb/y.alti+1c.\n/T_hat means xi+1is placed at a location equal or larger than\nb/y.alti+1c+1=b/y.altic+1. But we also know that b/y.alti+2c\u0014b/y.altic+1.\nSoxi+2cannot be placed at b/y.alti+2cwhich is not on the right\nofxi+1’s location. Since case (3) is false, f¹iº=i+2.\nBy induction, we show that when f¹iº>1, the key xf¹iº\ncannot be placed at b/y.altf¹iºc. /T_hat means when we look up xf¹iº,\nwe cannot directly hit it from the model prediction. Since we\nalso proved that f¹iºhas a unique value when f¹iº>1, the\nnumber of misses from the model prediction is at least the\nsize of S=fi2»n\u00002¼jf¹iº>1g. By the de/f_inition of f¹iº,S=\nfi2»n\u00002¼j/y.alti+2\u0000/y.alti\u00141g. /T_herefore, the number of direct hits\nby the model is at most n\u0000jSj=2+jfi2»n\u00002¼j/y.alti+2\u0000/y.alti>1gj=\n2+\f\ff1\u0014i\u0014n\u00002j∆i\u00151\ncag\f\f.\n\u0003\n/T_his result presents an upper bound on the number of di-\nrect hits from the model, which is positively correlated with c.\n/T_his upper bound also applies to the Learned Index, which has\nc=1. /T_his explains why the Gapped Array has the potential\nto dramatically decrease the search time. Similarly, we can\nlower bound the number of direct hits.T/h.sc/e.sc/o.sc/r.sc/e.sc/m.sc .3. /T_he number of keys placed in the predicted\nlocation is no smaller than l+1, where lis the largest integer\nsuch that 81\u0014i\u0014l;δi\u00151\nca, ı.e., the number of consecutive δi’s\nfrom the beginning equal or larger than1\nca.\n/T_he proof is not hard based on the ideas from the previous\ntwo proofs.\n21",
  "textLength": 103628
}