{
  "paperId": "f401a919db41eca28aa1cff062d98cc03b7ab66b",
  "title": "Learning to Optimize: A Primer and A Benchmark",
  "pdfPath": "f401a919db41eca28aa1cff062d98cc03b7ab66b.pdf",
  "text": "Learning to Optimize: A Primer and A Benchmark\nTianlong Chen, Xiaohan Chen, Wuyang Chen, Zhangyang Wang\u0003\nftianlong.chen,xiaohan.chen,wuyang.chen,atlaswang g@utexas.edu\nDepartment of Electrical and Computer and Engineering\nThe University of Texas at Austin, Austin, TX 78712, USA\nHoward Heaton\nhheaton@ucla.edu\nDepartment of Mathematics, University of California Los Angeles, Los Angeles, CA 90095, USA\nJialin Liu, Wotao Yin\nfjialin.liu,wotao.yin g@alibaba-inc.com\nAlibaba US, Damo Academy, Decision Intelligence Lab, Bellevue, WA 98004, USA\nAbstract\nLearning to optimize ( L2O ) is an emerging approach that leverages machine learning\nto develop optimization methods, aiming at reducing the laborious iterations of hand en-\ngineering. It automates the design of an optimization method based on its performance\non a set of training problems. This data-driven procedure generates methods that can ef-\n\fciently solve problems similar to those in the training. In sharp contrast, the typical and\ntraditional designs of optimization methods are theory-driven, so they obtain performance\nguarantees over the classes of problems speci\fed by the theory. The di\u000berence makes L2O\nsuitable for repeatedly solving a certain type of optimization problems over a speci\fc dis-\ntribution of data, while it typically fails on out-of-distribution problems. The practicality\nof L2O depends on the type of target optimization, the chosen architecture of the method\nto learn, and the training procedure. This new paradigm has motivated a community of\nresearchers to explore L2O and report their \fndings.\nThis article is poised to be the \frst comprehensive survey and benchmark of L2O for\ncontinuous optimization. We set up taxonomies, categorize existing works and research\ndirections, present insights, and identify open challenges. We also benchmarked many\nexisting L2O approaches on a few but representative optimization problems. For repro-\nducible research and fair benchmarking purposes, we released our software implementation\nand data in the package Open-L2O athttps://github.com/VITA-Group/Open-L2O .\nKeywords: Learning to Optimize, Meta Learning, Optimization, Algorithm Unrolling\n1. Introduction\n1.1 Background and Motivation\nClassic optimization methods are typically hand-built by optimization experts based on\ntheories and their experience. As a paradigm shift from this conventional design, learning to\noptimize (L2O ) uses machine learning to improve an optimization method or even generate\na completely new method. This paper provides a timely and up-to-date review of the rapid\ngrowing body of L2O results, with a focus on continuous optimization.\n∗. All authors are listed in alphabetic order. Correspondence shall be addressed to Z. Wang.\n©2021 (\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ .arXiv:2103.12828v2  [math.OC]  2 Jul 2021\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\nSelected\nOptimizer\nOnlineClassical\nOptimizers\nNew Optimizees\n(a) Classic Optimizer\nLearned\nOptimizer\nNew Optmizees\nOnlineLearnable \nOptimizer Update\nTraining \nOptmizees\nTraining DynamicsUpdate\nOffline (b) Learned Optimizer by L2O\nFigure 1: (a) Classic optimizers are manually designed; they usually have few or no tuning parame-\nters; (b) Learned optimizers are trained in an L2O framework over a set of similar optimizees (called\na task distribution) and designed to solve unseen optimizees from the same distribution.\nClassic optimization methods are built upon components that are basic methods | such\nas gradient descent, conjugate gradient, Newton steps, Simplex basis update, and stochastic\nsampling | in a theoretically justi\fed manner. Most conventional optimization methods\ncan be written in a few lines, and their performance is guaranteed by their theories. To solve\nan optimization problem in practice, we can select a method that supports the problem type\nat hand and expect the method to return a solution no worse than its guarantee.\nL2O is an alternative paradigm that develops an optimization method by training, i.e.,\nlearning from its performance on sample problems. The method may lack a solid theoretical\nbasis but improves its performance during the training process. The training process often\noccurs o\u000fine and is time consuming. However, the online application of the method is\n(aimed to be) time saving. When it comes to problems where the target solutions are di\u000ecult\nto obtain, such as nonconvex optimization and inverse-problem applications, the solution\nof a well-trained L2O method can have better qualities than those of classic methods. Let\nus call the optimization method (either hand-engineered or trained by L2O) the optimizer\nand call the optimization problem solvable by the method the optimizee . Figure 1 compares\nclassic optimizers and L2O optimizers and illustrates how they are applied to optimizees\n(yellow boxes).\nIn many applications of optimization, the task is to perform a certain type of optimiza-\ntion over a speci\fc distribution of data repeatedly. Each time, the input data that de\fne\nthe optimization are new but similar to the past. We say such an application has a narrow\ntask distribution . Conventional optimizers may be tuned for a task distribution, but the\nunderlying methods are design for a theory-speci\fed class of optimization problems. We\noften describe a conventional optimizer by the formulation (and its math properties), not\nthe task distribution. For example, we say an optimizer can minimize a smooth-and-convex\nobjective function subject to linear constraints. In L2O, the training process shapes the\noptimizer according to both the formulation and the task distribution. When the distri-\nbution is concentrated, the learned optimizer can \\over\ft\" to the tasks and may discover\n\\short cuts\" that classic optimizers do not take.\nL2O aims to generate optimizers with the following strengths:\n1. An optimizer learned by L2O is expected to complete a set of optimizees from the\nsame task distribution at a much faster speed than classic methods. In particular,\nsuch an L2O method can run even faster than so-called optimal methods such as\nNesterov's faster gradient descent (FGD) [3] on a set of problems well suited for these\noptimal methods.\n2\n\nLearning to Optimize: A Primer and A Benchmark\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16-75-65-55-45-35-25-15-5NMSE (dB)\nISTA\nFISTA\nLISTALISTA-CPSS\nTiLISTA\nALISTA\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16-40-35-30-25-20-15-10-50NMSE (dB)\nISTA\nFISTA\nLISTALISTA-CPSS\nTiLISTA\nALISTA\nFigure 2: L2O for compressive sensing. In our prior work [1; 2], we successfully demonstrated that L2O\noptimizers (LISTA, LISTA-CPSS, TiLISTA and ALISTA) converge much faster than the two popular it-\nerative solvers (ISTA, FISTA). Left: noiseless case. Right: noisy case (SNR = 20). X-axis is the number\nof iterations; Y-axis is normalized mean squared error (lower is better). See more details from the image\nsource: Figure 1 of [2] .\n2. The learned optimizer may also return a higher-quality solution to a di\u000ecult task\nthan classic methods given a similar amount of computing budget. For instance, an\nL2O method can recover a sparse signal that is much more faithful (meanwhile much\nfaster) than LASSO1.\nTherefore, when it comes to a small set of optimization tasks, L2O presents a potential\napproach to break the limits of analytic methods.\nL2O has started to demonstrate great potential in some areas of optimization and appli-\ncations. Examples include convex `1-minimization [4], neural network training [5], black-box\noptimization [6] and combinatorial optimization [7]. For example, a class of L2O methods,\nin the form of algorithm unrolling (which we review below), has state-of-the-art results in\ncompressed sensing and inverse problems. The L2O methods in [1; 2] converge much faster\nthan the classic ISTA/FISTA optimizers (see Figure 2), using an order-of-magnitude fewer\niterations on unseen optimizees from the same distribution to reach the same accuracy.\nApplication areas bene\fting from L2O methods include computer vision [8; 9; 10], medical\nimaging [11; 12], signal processing and communication [13; 14], policy learning [15], game\ntheory [16; 17], computational biology [18; 19], and even software engineering [20]. For\nexample, the applications of algorithm unrolling, a popular form of model-based L2O, has\nbeen speci\fcally reviewed in Section 3.3. As another example, in the practical application\nof domain adaptation, model-free L2O has demonstrated the ability to adapt a pre-trained\nclassi\fer to a new domain at less computational resource costs, achieving the same accuracy\nor generalization performance [21; 22].\n1.2 Preliminaries\nL2O starts with an architecture of the optimizer with free parameters to learn, so one must\nprepare a set of training samples that represent the task distribution, as well as choosing\na training approach. The training process (Figure 1 (b) left) executes the approach, which\niteratively applies the current optimizer to the sample optimizees and uses the observed\nperformance to update the parameters. Over the time, the optimizer adapts to the training\nsamples. This process trains an optimizer to take a series of actions on the optimizees,\nanalogous of classic learning where we train ML models to make predictions or decisions.\n1. In this example, the task is to recover a sparse signal, not necessarily solving the LASSO minimization.\n3\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\nWe now formalize our notation. Consider an optimization problem min xf(x) where\nx2Rd. A classic optimizer often iteratively updates xbased on a handcrafted rule. For\nexample, the \frst-order gradient descent algorithm is to take an update at each iteration\ntbased on the local landscape information at the instantaneous point xt:xt+1=xt\u0000\n\u000brf(xt), where\u000bis the step size.\nL2O has lots of freedom to use the available information. The information ztavailable at\ntimetcan include the existing iterates x0;:::;xtas well as their gradients rf(x0);:::;rf(xt),\netc. Then the L2O approach models an update rule by a function gofzt:xt+1=xt\u0000g(zt;\u001e),\nwhere the mapping function gis parameterized by \u001e.\nFinding an optimal update rule can be formulated mathematically as searching for a\ngood\u001eover the parameter space of g. In order to \fnd a desired \u001eassociated with a fast\noptimizer, [5] proposed to minimize the weighted sum of the objective function f(xt) over\na time span (which is called the unrolling length) Tgiven by:\nmin\n\u001eEf2T\"TX\nt=1wtf(xt)#\n;with xt+1=xt\u0000g(zt;\u001e); t= 1;:::;T\u00001; (1)\nwherew1;:::;wTare the weights and frepresents an optimizee from an ensemble Tof\noptimizees that represent the target task distribution. Note that the parameter \u001edetermines\nthe objective value through determining the iterates xt. L2O solves the training problem\neq. (1) for a desirable \u001eand the update rule g(zt;\u001e).\nIn practice, the choices of wtvary by case and depend on empirical settings. For example,\nmany L2O models for sparse coding unroll to a \fxed length Tfor all optimizees, and then\nonly minimize the step- Tfunctional value [23; 24], i.e., wT= 1 andw1=\u0001\u0001\u0001=wT\u00001= 0.\nThe work in [5], on the other hand, assigns nonzero wtvalues and report more e\u000ecient\nBackpropagation Through Time (BPTT).\nIn L2O, one speci\fes the architecture of optimizer, which consists of both learnable\ncomponents \u001eand \fxed components. We address both of them in this paper. The update\nrulegis often parameterized by multi-layer neural networks or recurrent neural networks.\nIn theory, neural networks are universal approximators, so L2O may discover completely\nnew, optimal update rules without referring to any existing updates. Since this kind of L2O\narchitecture does not need a model, we refer to it as model-free L2O.\nThe shortcomings of model-free L2O include: lacking convergence guarantees and re-\nquiring a high number of training samples. On tasks where classic operations | such\nas projection, normalization, and decaying step sizes | are critical to good performance,\nmodel-free L2O either cannot achieve good performance or require largely many training\nproblems to discover classic operations from the scratch. To avoid these shortcomings, we\nconsider incorporating the existing methods as base or starting points for learning, which\nreduce the search to fewer parameters and a smaller space of algorithms. We call this\nalternative approach model-based L2O.\n1.3 Broader Contexts\nWhen the tasks are general machine learning tasks, e.g., determining the parameters of a\nprediction model by minimizing its training loss, L2O overlaps with meta-learning [25; 26;\n27]. It is worth noting that \\meta-learning\" in di\u000berent communities has di\u000berent meanings.\nHere, meta-learning refers to using a method to improve learning algorithm(s), also called\n\\learning to learn\" [5; 6]. Recent results in this line of research contributes a signi\fcant\n4\n\nLearning to Optimize: A Primer and A Benchmark\nportion of L2O development [28]. The goal of L2O captures two main aspects of meta\nlearning: rapid learning within tasks, and transferable learning across many tasks from\nsome same distribution. However, L2O is not entirely meta-learning, because it takes into\naccount domain knowledge of optimization and applies to many non-learning optimization\ntasks such as solving inverse problems.\nL2O shares many similarities with AutoML [29]. The term \\AutoML\" broadly refers to\nthe automation of any step(s) in the ML lifecycle. Traditionally, AutoML research focuses\non model selection, algorithm selection, and hyperparameter optimization. These methods\naccelerate the design iterations of many types of ML algorithms, such as random forests,\ngradient boosting, and neural networks. AutoML recently draws (back) the mainstream at-\ntention because of its signi\fcant success in enhancing the performance of deep learning [30].\nAmong the topics under AutoML, those most relevant to L2O are algorithm con\fguration\n[31] and hyperparameter optimization (HPO) [32]. Algorithm con\fguration determines a\nhigh-performing con\fguration of some algorithm across a given set of problem instances.\nHPO tunes a set of hyperparameters speci\fcally for an ML algorithm, via Bayesian opti-\nmization [33] or classi\fcation-based optimization [34]. [35] has combined algorithm selection\nand hyperparameter optimization, also known as CASH. The main di\u000berence between these\nworks and L2O lies in that L2O can discover new optimization methods from a parame-\nterized algorithmic space of optimizers, rather than only selecting from or tuning a few\nexisting optimizers. Yet, the boundary between HPO and L2O is often blurry since certain\nL2O methods [2; 36; 37] con\fgure or predict hyperparameters for existing optimizers only.\nL2O is closely related to the new frontier of learning-augmented algorithms [38]. Classic\nalgorithms are designed with worst-case performance guarantees in mind and do not adapt\nto actual inputs. On the other hand, ML algorithms often achieve competitive performance\nby adapting to inputs, but their worst-case performance on (unseen) inputs degrades sig-\nni\fcantly. Learning-augmented algorithms combine the best of both worlds, using ML to\nimprove the performance of classic algorithms, by adapting their behaviors to the input\ndistribution. Examples include learning-augmented data structures [39; 40], streaming and\nsketching algorithms [41; 42], online algorithms [43], error-correcting codes [44; 45], schedul-\ning algorithms [46], approximation algorithms [47], and safeguarded learned algorithms [24].\nL2O can be counted as a subset of those learning-augmented algorithms. A comprehensive\nlist of relevant materials can be found in [38].\nPrevious review articles and di\u000berences There have been a number of review articles\non meta learning [25; 26] and AutoML [29; 48; 49].\nThe work in [50] surveys algorithm unrolling. Work [51] reviews model-based deep\nlearning. These articles have overlapping scopes with ours as algorithm unrolling is a main\n(but not the only) technique of model-based L2O. This article features a more comprehensive\ncoverage including both model-based and model-free L2O approaches.\nLastly, let us mention recent works that leverage ML for combinatorial and discrete opti-\nmization [52; 53; 54; 55; 56]. [57] provides a comprehensive review on ML for combinatorial\noptimization.\n1.4 Paper Scope and Organization\nWe draw distinctions between model-free and model based L2O approaches and review many\nL2O techniques. Emphases are given to recurrent network-based L2O methods, algorithm\n5\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\nunrolling, and plug-and-play. We discuss how to train them e\u000bectively and how their designs\ncan bene\ft from the structures of the optimizees and classic optimization methods.\nWe benchmark existing model-based and model-based L2O approaches on a few repre-\nsentative optimization problems. We have released our software implementation and test\ncases as the Open-L2O package at ( https://github.com/VITA-Group/Open-L2O ), in the\nspirit of reproducible research and fair benchmarking.\nThe rest of the article is organized as follows. Section 2 and Section 3 review and\ncategorize the existing works in model-free and model-based L2O, respectively. Section 4\ndescribes the Open-L2O testbeds, our comparison experiments, and our result analyses.\nThe article is concluded by Section 5 with a discussion on research gaps and future work.\n2. Model-Free L2O Approaches\nA model-free L2O approach in general aims to learn a parameterized update rule of opti-\nmization without taking the form of any analytic update (except the updates being itera-\ntive). The recent mainstream works in this vein [5; 6; 58; 59; 18] leverage recurrent neural\nnetworks (RNNs), most of which use the long short-term memory (LSTM) architecture.\nAn LSTM is unrolled to perform iterative updates and trained to \fnd short optimization\ntrajectories. One set of parameters are shared across all the unrolled steps. At each step,\nthe LSTM takes as input the optimizee's local states (such as zero-order and \frst-order\ninformation) and returns the next iterate.\nModel-free L2O shares the same high-level work\row depicted in Figure 1 (b). It is\ndivided into two stages: an o\u000fine L2O training stage that learns the optimizer with a set\nof optimizees sampled from the task distribution T; then, an online L2O testing stage that\napplies the learned optimizer to new optimizees, assuming they are samples from the same\ntask distribution. Table 1 compares a few recent, representative works in this area, from\nmultiple lens including what optimizer architecture to select, what objectives and metrics\nthat meta training and meta testing each use, and what input feature and other techniques\nare adopted. There are a great variety.\nTable 1: Summary and comparison of representative model-free L2O methods.\nPaper Optimizer Architecture Input Feature Meta Training Objective Additional Technique Evaluation Metric\n[5] LSTM Gradient Meta LossTransform input gradient r\ninto log(r) and sign(r)Training Loss\n[6] LSTM Objective Value Objective Value N/A Objective Value\n[58] LSTM Gradient Meta LossRandom Scaling\nCombination with Convex FunctionsTraining Loss\n[59] Hierarchical RNNsScaled averaged gradients,\nrelative log gradient magnitudes,\nrelative log learning rateLog Meta LossGradient History Attention\nNesterov MomentumTraining Loss\n[60] MLP Gradient Meta Loss Unbiased Gradient EstimatorsTraining Loss\nTesting Loss\n[61] RNN Controller Loss, Gradient Meta Loss Coordinate Groups Training Loss\n[62]Searched Mathematical Rule\nby Primitive FunctionsScaled averaged gradients Meta Loss N/A Testing Accuracy\n[18] Multiple LSTMsGradient, momentum,\nparticle's velocity and attractionMeta Loss and\nEntropy RegularizerSample- and Feature- Attention Training Loss\n[63] RNN Input Images, Input Gradient Meta Loss N/AStandard and Robust\nTest Accuracies\n[64] LSTM Input Gradient Meta Loss N/ATraining Loss and\nRobust Test Accuracy\n6\n\nLearning to Optimize: A Primer and A Benchmark\n2.1 LSTM Optimizer for Continuous Minimization: Basic Idea and Variants\nAs the optimization process can be regarded as a trajectory of iterative updates, RNNs are\none natural option with a good inductive bias to learn the update rule. The \frst pioneering\nmodel-free L2O method [5] proposed to model the update rule implicitly by gradient descent.\nThis \frst LSTM method for L2O has often been referred to as L2O-DM in literature2.\nAt high-level, the learned optimizer (modeled by LSTM) is updated by the gradients\nfrom minimizing the loss induced by the optimizee, and optimizees are updated by the\nupdate rule predicted by the optimizer:\nL(\u001e) =E(\u00120;f)2T\"TX\nt=1wtf(\u0012t)#\nwhere\u0012t+1=\u0012t+gt \u0014\ngt\nht+1\u0015\n=m(rt;ht;\u001e): (2)\nIn Eq. 2,frepresents an optimizee (e.g. a neural network with its loss function), with \u00120\ndenoting the initialization of f. A set of optimizees and their initializations are sampled\nfrom the task distribution T.\u001eis the parameters of the L2O optimizer. rt=r\u0012f(\u0012t) is\nthe gradient of the objective function with respect to the optimizee's parameters. mis the\nLSTM optimizer, and gandhare update and hidden state produced by m, respectively. T\nis the maximal unrolling length for LTSM, often set due to the memory limitation; and wt\n= 1 was used here for every t.\nTwo ad-hoc training techniques were utilized here. Firstly, each optimization variable\n(or called coordinate) of the optimizee's parameter \u0012tshared the same LSTM weights, but\nwith di\u000berent hidden states, in order to reduce the memory overhead faced when scaling\nup the number of optimization coordinates; this trick has been inherited by many LSTM-\ntype L2O works. Secondly, to stabilize the optimizer's training, the authors proposed to\nreduce the dynamic range of the optimizee's gradient magnitudes, by preprocessing rtinto\n(log(rt);sgn(rt)) as the input into the optimizer.\nThe authors of [5] conducted a few proof-of-concept studies on small-scale tasks such\nas MNIST classi\fcation, where they showed the learned optimizer mcan converge faster\nthan some stochastic gradient descent based optimizers such as SGD, RMSprop, and Adam.\nDespite this initial success, the scalability and generalizability of [5] remain underwhelming.\nTwo speci\fc questionable shortcomings are:\n•Generalizability of learned optimizer to unseen and potentially more com-\nplicated optimizees. The L2O training set contains sampled optimizees from the\ntarget distribution. Just like typical machine learning models, we might expect a\nlearned optimizer to both interpolate and extrapolate from the seen (meta-)training\nset, the latter being more challenging. Taking training deep networks for example,\nduring meta-testing, we might expect the learned optimizer to generalize (extrapo-\nlate) to the training of deeper or wider networks beyond instances seen in the meta\ntraining set. This generalizability is demanded also due to the memory bottleneck\nduring meta training. To update the L2O optimizer m(\u001e) using back-propagation, we\nneed to keep the gradients and the computation graph of the optimizee in memory.\nTherefore, the memory bottleneck arises when the unroll length of the m(\u001e) becomes\nlarge, or the dimension of optimizee's parameter \u0012is high. Other types of general-\nization, such as training a network with unseen di\u000berent activation functions or loss\nfunctions, are also found to be desirable yet challenging [5].\n2. Methods that are underscored in this section are also later evaluated and benchmarked in Section. 4.\n7\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\nMoreover, applying the L2O optimizers to train more complex neural networks may\nmeet more sophisticated loss landscapes, which cannot be well learned from observing\nsimple training dynamics from small networks. For example, it is well known that\ntraining over-parameterized networks will exhibit sharply distinct behaviors at small\nand large learning rates, and the two regimes usually co-exist in practical training and\nare separated by a phase transition [65; 66; 67].\n•Generalizability of learned optimizer to longer training iterations. Train-\ning larger models naturally takes more iterations, which calls for longer-term L2O\nmodeling. An LSTM model can in principle characterize longer-term dependency\nby unfolding more time steps (i.e., longer unrolling length T). However, that often\nresults in L2O training instability due to the optimization artifacts of LSTM such\nas gradient explosion or vanishing, in addition to the practical memory bottleneck.\nTherefore, most LSTM-based L2O methods [5; 6; 58; 59; 60; 18; 19; 68] are forced to\nlimit their maximal unroll lengths and to truncate the unrolled optimization (e.g., up\nto 20 steps). As a result, the entire optimization trajectory is divided into consecutive\nshorter pieces, where each piece is optimized by applying a truncated LSTM.\nHowever, choosing the unrolling (or division) length faces a well-known dilemma [58]:\non one hand, a short-truncated LSTM can result in premature termination of the\niterative solution. Naively unfolding an LSTM-based L2O optimizer (trained with a\nsmall T) to more time stamps at meta-testing usually yields unsatisfactory results.\nThe resulting \\truncation bias\" causes learned optimizers to exhibit instability and\nyield poor-quality solutions when applied to training optimizees.\nThe above research gaps motivate several lines of follow-up works, as summarized below:\nDebiasing LSTM Truncation As explained above, selecting the unrolling length T\nre\rects a fundamental dilemma in LSTM-type L2O models: a longer Tcauses optimization\ndi\u000eculty at L2O training, while a smaller Tincurs larger \\truncation bias\" and hampers\nthe generalization at meta-testing.\nMany LSTM-type works focus on addressing this truncation bias. [59] drew the unroll\nlength of their truncated optimization from a heavy tailed distribution. [60] proposed to\nreplace LSTM with just MLP for the optimizer. The authors of [60] proposed to smooth\nthe loss landscape by using two unbiased gradient estimators with dynamic re-weighting:\ngrp=1\nSSX\ns=1r\u0012f(\u0012+ns); n 1;\u0001\u0001\u0001;nS\u0018N(0;\u001b2I) i:i:d: (3)\nges=1\nSSX\ns=1f(~\u0012s)r\u0012[log(P(~\u0012s))];~\u00121;\u0001\u0001\u0001;~\u0012S\u0018N(\u0012;\u001b2I) i:i:d: (4)\ngmerged =grp\u001b\u00002\nrp+ges\u001b\u00002\nes\n\u001b\u00002rp+\u001b\u00002es(5)\ngrpandgesare graident estimated by \\reparameterization trick\" [69] and \\evolutionary\nstrategies\" [70], and \u001brpand\u001besare empirical estimates of the variances of grpandges,\nrespectively. In this way, they demonstrated L2O can train convolutional network net-\nworks faster in wall-clock time compared to tuned \frst-order methods, with reduced test\nlosses. Their later work [71] also found that the meta-learned optimizer can train image\nclassi\fcation models such that they are robust to unseen image corruptions.\n8\n\nLearning to Optimize: A Primer and A Benchmark\nFigure 3: The dilemma of longer unrolling in both L2O testing and L2O training: (1) generalization\nfailure at L2O testing: the upper \fgure shows the meta-testing loss of the vanilla L2O method [5]\nto quickly diverge, as we increase the number of steps at the meta-testing. This failure case was\nalso observed in Figure 2 of [59] and in Figure 2 of [58]. (2) optimization failure at L2O training:\nthe bottom \fgure shows the L2O training loss to also not decrease when it adopts longer unrolling\nlengths, due to the optimization artifacts of LSTM such as gradient explosion or vanishing.\nStronger LSTM Architectures Another direction explored is to introduce stronger\nRNN/LSTM architecture to L2O. Instead of using a single RNN layer, Wichrowska et al.\n[59] leveraged three RNN layers to learn the optimization in di\u000berent scales. They organized\nthree RNN layers in a hierachical fashion, here we simply denote them as \\bottom RNN\",\n\\middle RNN\", \\upper RNN\". The \\bottom RNN\" directly takes the scaled gradients from\nthe optimizee as input. Given a speci\fc training step, the \\middle RNN\" receives the aver-\nage all hidden states from \\bottom RNN\" across the optimizee's parameter coordinates, and\noutputs a bias term to the \\bottom RNN\". Further, the \\upper RNN\" takes the averaged\nhidden states from \\middle RNN\" across a certain window of optimizee's training steps.\nBy using smaller hidden states in \\bottom RNN\", this hierarchical design of L2O optimizer\nachieved lower memory and compute overhead, while achieving better generalization. We\nrefer [59] as L2O-Scale in this article.\nImproved L2O Training Techniques In order to improve the generalization of the\nlearned optimizer to both longer unrolling (i.e. longer optimization iterations) and unseen\nfunctions, [58] proposed two training tricks. The \frst one, called random scaling , could\nbe viewed as a special \\data augmentation\" for L2O: a coordinate-wise scale factor cwas\nrandomly generated at each iteration to scale the parameters of the optimizee during L2O\ntraining:fc(\u0012) =f(c\u0012). It was motivated by the observation that in many optimization\nproblems such as the quadratic function f(\u0012) =\u0015k\u0012k2\n2, the ideal update rule should achieve\nthe same minima under varying \u0015. The second trick was to add a convex term during L2O\ntraining, as inspired by the proximal algorithms [72]; and avoided large random updates\nwhen the L2O optimizer is under-trained. We refer [58] as L2O-RNNprop in this article.\nLately, the authors of [73] took a deeper dive into improved training techniques for\nL2O models. The authors \frst presented a progressive training scheme, which gradually in-\ncreased the optimizer unroll length to mitigate the L2O dilemma of truncation bias (shorter\nunrolling) versus gradient explosion (longer unrolling). Furthermore, they presented an\no\u000b-policy imitation learning approach to guide the L2O training, by forcing the L2O op-\n9\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\ntimizer to mimic the update rules generated by analytic optimizers. The authors of [73]\nevaluated their improved training techniques with a variety of state-of-the-art L2O models\n[5; 58; 59], and achieved boosted performance (lower training losses of unseen optimizees)\nwithout changing the original L2O RNN architecture in each method. We refer [73] as\nL2O-enhanced in this article.\n2.2 Other Common Implementations for Mode-Free L2O\nWhile LSTM is so far the mainstream model, other optimizer models have also been ex-\nplored. We describe two alternatives: reinforcement learning (RL), and neural symbolics .\n[61] proposed to learn an RL policy \u0019to predict the update rule, as the learned opti-\nmizer. The policy \u0019samples the update steps from a Gaussian distribution. The mean and\nvariance of the Gaussian distribution are learnable parameters of the L2O policy, updated\nby reinforcement learning. The observation of the policy is composed of the current value\nof objective function (i.e. loss), the recent gradients, and changes of the objective values\nup toHsteps (H= 25 in their experiments). The policy receives the decrease of training\nloss as the reward. Logistic regression functions and two-layer neural nets were leveraged as\nthe testbeds. Further on, [28] proposed to group coordinates under permutation invariance\n(e.g., weight matrix or a bias vector) into a coordinate group. This formulation reduced\nthe computation cost of expensive optimization problems, making the proposed method\nextensible to wider neural networks. Generalizability of the learned policy was also stud-\nied by training the policy on shallow and narrow networks, and test on wider layers. [74]\nlearned to update optimizer hyperparameters instead of model parameters, directly using\nnovel features, actions, and a reward function to feed RL. They demonstrated promising\nscalability to large-scale real problems.\nIt is worthy to mention a special L2O work [62], that explored L2O from the neural\nsymbolic prospective. The authors also leveraged an RL controller, but avoided directly\nmodeling the update rules. Instead, they designed a search space of operands (gradient\ng, gradient's running exponential moving average ^ m, etc.), unary functions of input x(ex,\nlog(x),p\njxj, etc.), and binary functions (mapping ( x;y) tox+y,x\u0000y,x\u0003y, etc.). The\nRL controller was learned to select a sequence of elements from the search space, formulate\na function to process the input, and output the update rule. Their searched best L2O opti-\nmizer (the RL controller) shows strong transferability, and improved training performance\n(lower training losses) on di\u000berent tasks and architectures, including ImageNet classi\fcation\nand Google's neural machine translation system. Their idea was further developed in [75],\nto automatically discover complete machine learning algorithms from raw mathematical\noperations as building blocks, which concerns a more general problem than L2O.\n2.3 More Optimization Tasks for Model-Free L2O\nBlack-box Optimization [6] pioneered to extend the LSTM L2O framework [5] to\nderivative-free or black-box function optimization. Due to the absence of the optimizee's\ngradients as input, the authors of [6] instead treated the optimizee's input-output pair as the\nobservation of the optimizer, and formulated the optimization as an exploration-exploitation\ntrade-o\u000b problem. They updated the optimizer's hidden state htby the observation from\n10\n\nLearning to Optimize: A Primer and A Benchmark\nthe last step ( xt\u00001;yt\u00001) and then chose a new query point xtto explore\nht;xt= RNN\u0012(ht\u00001;xt\u00001;yt\u00001) (6)\nyt\u0018p(yjxt;\u0001\u0001\u0001;x1); (7)\nwhere the function value ytwas incrementally sampled from a Gaussian Process ( p) at\neach query point xtin the experiments. During L2O training, it was assumed that the\nderivatives of function value ytcan be computed with respect to the input xt, which means\nthe errors will be backpropagated for L2O training, but not needed at L2O testing time.\n[6] demonstrated that their learned RNN optimizers are competitive with state-of-the-art\nBayesian optimization packages (Spearmint [76], SMAC [31], and TPE [77]).\nParticle Swarm Optimization Current L2O methods mostly learn in the space of con-\ntinuous optimization algorithms that are point-based and uncertainty unaware. Inspired\nby population-based algorithms (e.g. swarm optimization), [18] estimated the posterior di-\nrectly over the global optimum and used an uncertainty measure to help guide the learning\nprocess. The authors designed a novel architecture where a population of LSTMs jointly\nlearned iterative update formula for a population of samples (or a swarm of particles). The\nmodel can take as input both point-based input features, such as gradient momentum; and\npopulation-based features, such as particle's velocity and attraction from swarm algorithms.\nTo balance exploration and exploitation in search, the authors directly estimated the pos-\nterior over the optimum and included in the meta-loss function the di\u000berential entropy\nof the posterior. Furthermore, they learn feature- and sample-level importance reweight-\ning (often called \\attention\" in deep learning) in the L2O model, for more interpretable\nlearned optimization rules. Their empirical results over non-convex test functions and the\nprotein-docking application demonstrated that this new L2O largely outperforms the o\u000b-\nthe-shelf Particle Swarm Optimization (PSO) algorithm [78] and the vanilla LSTM-based\nL2O methods that are not uncertainty-aware [5]. We refer [18] as L2O-Swarm in this article.\nMinimax Optimization One more challenging testbed for model-free L2O is to solve\ncontinuous minimax optimization, that is of extensive practical interest [79; 80; 81], yet\nnotoriously unstable and di\u000ecult. Three prior works [63; 82; 64] tried to plug in L2O into\na speci\fc application of minimax optimization called adversarial training [80]:\nmin\n\u0012E(x;y)\u0018D\u001a\nmax\nx02B(x;\u000f)L\u0000\nf\u0000\nx0\u0001\n;y\u0001\u001b\n; (8)\nwhereDis the empirical distribution of input data, and the inner maximization is de\fned\nas the worst-case loss within a small neighborhood B(x;\u000f). In both works, the L2O only\npredicted update directions for the inner maximization problem, while the outer minimiza-\ntion was still solved by classic optimizer. They empirically showed that L2O can improve\nthe solution quality to the inner maximization optimization, hence also leading to a better\nminimax solution and a more robustly trained model.\nA latest work [83] extended L2O to solve minimax optimization from end to end, for the\n\frst time. The authors proposed Twin L2O, consisting of two LSTMs for updating min and\nmax variables, respectively. This decoupled design was shown by ablation experiments to\nfacilitate learning, particularly when the min and max variables are highly non-symmetric.\nSeveral enhanced variants and training techniques were also discussed. The authored bench-\nmarked their L2O algorithm on several relatively basic and low-dimensional test problems,\n11\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\nand on which L2O compared favorably against state-of-the-art minimax solvers. How to\nscale up L2O for fully solving minimax problems of practical interest, such as adversarial\ntraining or GANs, remained to be an open challenge.\nGame Theory RL-based model-free L2O has very recently found interest from the game\ntheory \feld. [16] proposed to train multi-agent systems (MAS) to achieve symmetric pure\nNash equilibria. Such equilibria needs to satisfy certain constraints so that MAS are cali-\nbrated for practical use. The authors adopted a novel dual-RL-based algorithm to \ft emer-\ngent behaviors of agents in a shared equilibrium to external targets. They used parameter\nsharing with decentralized execution, to train multiple agents using a single policy network,\nwhile each agent can be conditioned on agent-speci\fc information. The methodology shared\nsimilarities to [18]. Another work [17] extended consensus optimization to the constrained\ncase. They introduced a new framework for online learning in non zero-sum games, where\nthe update rule's gradient and Hessian coe\u000ecients along a trajectory are learned by an RL\npolicy, conditioned on the game signature. The authors mentioned that the same problem\nmight potentially be solved by using the LSTM-based approach too [83].\nFew-shot Learning Another application of the LSTM L2O framework [5] explores the\napplication of model-free L2O in the small data regime. [84] \frst adopted a model-free L2O\napproach to learn a few-shot classi\fer, accessing only very few labeled examples per class\nat training. The authors of [84] proposed to learn an LSTM-based meta-learner to optimize\neach task-speci\fc classi\fer using the cell states in the LSTM, inspired by their observation\nthat the gradient descent update on the parameters in the classi\fer resembles the cell state\nupdate in an LSTM. [85] simpli\fed [84] by constraining the optimization on the learner\nclassi\fer to one step of gradient descent but with learnable initialization and step size.\n3. Model-Based L2O Approaches\nWe now overview model-based L2O approaches, which mark the recent movement to fuse\ntraditional model-based optimization algorithms with powerful deep learning architectures\n[50]. Rather than use general-purpose LSTMs, these methods model their iterative update\nrules through a learnable architecture that is inspired by analytic optimization algorithms.\nOften, these learned methods can approximate problem solutions with tens of iterations\nwhereas their classic counterparts make require hundreds or thousands of iterations [2].\nAt a high level, model-based L2O can be viewed as a \\semi-parameterized\" option that\ntakes advantage of both model-based structures/priors and data-driven learning capacity3.\nThe growing popularity of this approach lies in its demonstrated e\u000bectiveness in developing\ncompact, data-e\u000ecient, interpretable and high-performance architectures, when the under-\nlying optimization model is assumed available or can be partially inferred. There is a large\ndesign space to \rexibly balance between the two. Most model-based L2O methods take one\nof the two following mainstream approaches.\nThe \frst approach is known as plug and play (PnP) , and the key idea here is to\nplug a previously trained neural network (NN) into part of the update for an optimization\nalgorithm (i.e., in place of an analytic expression), and then play by immediately applying\nthe modi\fed algorithm to problems of interest (without additional training). We illustrate\nthis with the common alternating direction method of multipliers (ADMM) formulation of\n3. We slightly abused the term of semi-parametric model from statistical learning, to represent di\u000berent\nmeanings: here it refers to blending pre-de\fned structures/priors into black-box learning models.\n12\n\nLearning to Optimize: A Primer and A Benchmark\nPnP (e.g., see [86]). Here the underlying task of the original optimization algorithm is to\nminimize the sum of two functions fandg, using successive proximal operations. The PnP\nformulation replaces the proximal for gwith an operator H\u0012to obtain the iteration:\nxk+1=H\u0012(yk\u0000uk) (9a)\nyk+1= prox\u000bf(xk+1+uk) (9b)\nuk+1=uk+xk+1\u0000yk+1: (9c)\nBefore inserting H\u0012into the iteration above, the parameters \u0012are learned independently\nas the solution to a training problem, i.e.,\n\u00122arg min\n~\u0012L(~\u0012): (10)\nThe loss function Lis designed by an independent goal (e.g., to learn a natural image\ndenoising operator). For example, one might model an image recovery problem by using\ntotal variation (TV) as a regularizer [87]. In the optimization scheme chosen (e.g., ADMM)\nto recover the image, one step of the process could be to perform a proximal operation with\nTV, which e\u000bectively acts as a denoiser. The PnP framework proposes replacement of the\nTV proximal with an existing denoiser (e.g., a neural network [88] or BM3D [89]).\nThe second approach is known as algorithm unrolling , whichs unrolls a truncated\noptimization algorithm into the structure of a neural network [50]. Updates take the form\nxk+1=T(xk;\u0012k); k = 0;1;2;\u0001\u0001\u0001;K: (11)\nUpon establishing this form, the parameters are learned by an end-to-end approach:\nmin\nf\u0012kgK\u00001\nk=0L\u0000\nxK(f\u0012kgK\u00001\nk=0)\u0001\n; (12)\nwhereLis the loss function we use in training. We emphasize the distinction that the\nparameters in unrolled schemes are trained end-to-end using the iterate xKas a function\nof each\u0012kwhereas training occurs separately for PnP. We will introduce how to design L\nlater in this section. Below we discuss each of these approaches, their typical features, and\nwhich approach is suitable for various applications.\nOpt IP PnP\nTunable Optimization Model XX\nTunable Update Formulas XXX\nTraining Loss tied to Model X\nTraining Loss measure u?error X\nConvergence guarantee to ideal u?S S\nInterpretable Updates S SX\nTable 2: Comparison of properties for three types of model-based L2O methods: unrolled objective-\nbased (Opt), inverse problem (IP), and Plug and Play (PnP) methods. Here Xand S mean that\nthe corresponding property always and sometimes holds, respectively.\n13\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\nu1\n Tθ1(·;·) · · ·\nTθk(·;·)\n uK\nd\nu1u2uK−1uK\nFigure 4: A common approach in various L2O schemes is to form feed forward networks by unrolling\nan iterative algorithm, truncated to Kiterations, and tuning parameters at each layer/iteration k.\nThis generalizes the update formula uk+1=T(uk;d) to include a dependence on weights \u0012k, denoted\nby a subscript.\n3.1 Plug and Play\nPnP methods date back to 2013 when they were \frst proposed for ADMM [86]. The\nauthors replaced one of the proximal operators in ADMM with a denoiser (e.g., K-SVD\n[90], BM3D [89], and non-local means [91]) and showed empirical performance surpassing\nthe original ADMM algorithm. Several other PnP schemes were also introduced and their\nempirical success was shown in the literature [92; 93; 94; 95; 96; 97; 98; 99; 100; 101; 102;\n103; 104; 105; 106; 107; 108]. The theoretical guarantees of convergence of PnP-ADMM\nwas \frst provided in [109] under the assumption that the derivative of the denoiser was\ndoubly stochastic. Then [110] proved the convergence of PnP-ADMM with the use of a\nmore realistic \\bounded denoisers\" assumption. Some other PnP schemes were analyzed\nunder di\u000berent assumptions [111; 112; 113; 114; 115; 116; 117; 118; 119; 120; 121].\nPnP was initially not connected to L2O, until some concurrent works [122; 123; 124]\nintroduced the concept of \\learning operators\" into a PnP framework. Instead of using a\nmanual-designed denoiser, they modeled the proximal operator as a deep neural network\nand learned it from data. The empirical performance of such an approach largely exceeded\nthe prior PnP works. From an L2O perspective, the learned methods were able to improve\nperformance by either accelerating execution of a subroutine in the optimization algorithm\nor providing a \\better\" solution than previously obtained.\nBesides learning the operators in a PnP framework, one can also learn a functional\nas the regularizer. [125; 126] modeled the statistical distribution of nature images as a\nDenoising Autoencoder (DAE) and constructed the optimization objective by maximizing\nthe likelihood. The DAE is di\u000berentiable; thus, the inverse problem can be solved with\ngradient-based optimization algorithms. A parameterized discriminator function de\fned\nin the Wasserstein distance between two distributions was used by [127] as a learned a\nfunctional that discriminates between a ground-truth and fake images. The authors treated\nthis learned functional as a regularizer and used it in a gradient descent algorithm for image\ninverse problems.\nLearning is not only able to help \fnd the denoiser/regularizer in PnP, but is also able\nto be used in a more meta manner to help \fnd good parameters in PnP. [128] learned a\npolicy network to tune the parameters in Plug-and-Play with reinforcement learning. By\nusing the learned policy, the guided optimization can reach comparable results to the ones\nusing oracle parameters tuned via the inaccessible ground truth.\n14\n\nLearning to Optimize: A Primer and A Benchmark\nThe marriage of PnP with L2O also provides theoretical blessings. While a manual-\ndesigned regularizer may not guarantee the convergence of PnP to the desired solution, a\nlearning-based method provides the \rexibility to meet the condition of convergence. [88]\nstudied the convergence of some PnP frameworks using \fxed-point iterations, showing guar-\nanteed convergence under a certain Lipschitz condition on the denoisers. They then pro-\nposed a normalization method for training deep learning-based denoisers to satisfy the\nproposed Lipschitz condition. Similarly, [129] also provided an approach for building con-\nvergent PnP algorithms using monotone operator theory and constraining the Lipschitz\nconstant of the denoiser during training. This \fxed point framework was extended by\n[130] to the RED-PRO framework, which also showed convergence to global solutions. In-\ndependently of RED-PRO, [131] proposed a method within the RED-PRO framework for\nlearning projection operators onto compact manifolds of true data. Utilizing assumptions\nabout the manifold and su\u000ecient representation by the sampled data, the authors proved\ntheir approach to constructing a PnP operator can provably approximate the projection (in\nprobability) onto a low dimensional manifold of true data.\n3.2 Algorithm Unrolling\nHerein we overview L2O methods comprised of unrolling iterative optimization algorithms.\nWe start by emphasizing there are two distinct goals of unrolled L2O methods: either to\nminimize an objective, or to recover a signal. The distinction is that the aim for inverse\nproblems is to tune the parameters so that they minimize the reconstruction accuracy of\nsome \\true\" signal rather than \fnd a minimizer of the model's objective function. The prac-\nticality for objective minimization is to speed up convergence. Keeping these categorizations\nin mind will help provide the reader intuitive lenses for looking at di\u000berent approaches. See\nTable 2 for a comparison of common qualities in these methods and PnP.\nBelow we provide a comprehensive and organized review of existing works, along multiple\ndimensions: what problems they solve, what algorithms they unroll, what goals they pursue\n(objective minimization or signal recovery), and to what extent they freely parameterize\ntheir learnable update rule.\n3.2.1 Different target problems\nWe broadly categorize the main problems tackled by model-based L2O in four categories:\nprobabilistic graph models, sparse and low rank regression, di\u000berential equations, and\nquadratic optimization.\n•Sparse and low rank regression . The most investigated problems in the literature\nof unrolling is probably the sparse regression, inspired by the \frst seminal work [4],\nwhich unrolls the Iterative Shrinkage Thresholding Algorithm (ISTA) or its block\ncoordinate variant as a recurrent neural network to solve LASSO for fast sparse coding.\nThe unrolling philosophy is also used in low rank regression as it shares some common\nnature with sparse regression.\n{ LASSO: Most of unrolling works following [4] also solve LASSO-type optimiza-\ntion problems for sparse coding [132; 133; 134; 135; 23; 136; 137; 138; 139]. Be-\nyond naive LASSO, A natural extension is to apply the same unrolling and trun-\ncating methodology to solve group LASSO [140], i.e. \fnding solutions with struc-\n15\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\ntured sparsity constraints. [141; 142] extended solving (convolutional) LASSO\nin a multi-layer setting, and an adapted version of ISTA was unrolled in [143].\n{ Analysis model: Di\u000berent from the sparse prior in LASSO (also known as the\nSynthesis formulation), which assumes the signal is the linear combinations of\na small number of atoms in a dictionary, the sparse analysis model assumes the\nexistence of a forward transformation that sparsi\fes the signal. [144] applied un-\nrolling to regression problems with total variation regularizations, which follows\nthe analysis formulation. Problems with analysis sparse priors were also tackled\nusing unrolling in [145] in a bi-level optimization context.\n{ Other sparse/anti-sparse regression problems: [146] and [147] unroll the\niterative hard thresholding (IHT) algorithm that solves `0minimization problem\ninstead of LASSO. Unrolling for `1minimization was also considered by [148],\nleading to so-called anti-sparse representation learning [149].\n{ Low Rank Regression: [150; 151; 152; 153] all extend the unrolling idea to\nlow-rank matrix factorization. Speci\fcally, [152] proposes tailored pursuit archi-\ntectures for both robust PCA and non-negative matrix factorization with special-\nized algorithms inspired by the non-convex optimization techniques to alleviate\nthe expensive SVD involved in each iteration.\n•Probabilistic Graphical Model . The unrolling method can also be adopted to solve\nprobabilistic graphical models. For example, [150] interpret conventional networks\nas mean-\feld inference in Markov random \felds and obtains new architectures by\nmodeling the belief propagation algorithm to solve the Markov random \feld problems.\n[154] formulate Conditional Random Fields with Gaussian pairwise potentials and\nmean-\feld approximate inference as recurrent neural networks.\n•Di\u000berential equations . Another line of works [155; 156; 157; 158] unroll the evo-\nlution in Partial Di\u000berential Equation (PDE) systems. While PDEs are commonly\nderived based on empirical observations. those recent advances o\u000ber new opportuni-\nties for data-driven discovery of (time-dependent) PDEs from observed dynamic data.\nOne can train feed-forward or recurrent neural networks to approximate PDEs, with\napplications such as \ruid simulation [159]. Loosely related is also signi\fcant work on\nconnecting neural networks with Ordinary Di\u000berential Equation (ODE) systems.\n•Quadratic optimization . Some recent works investigate the unrolling method in\nquadratic optimization problems [37; 160], which are easier to solve compared to the\nproblems studied above. The focus here is more on the theoretical analysis of the\nconvergence, and/or the interplay between the unrolled algorithm and the resultant\ndeep model's property (e.g., generalization and stability).\nIn some scenarios, we are not directly interested in the output of the unrolled model but\nuse it for downstream tasks, e.g. clustering [161] and classi\fcation [146; 23; 138]. That will\noften lead to task-driven joint optimization and the end task output becomes the focus of\nevaluation, in place of the original unrolling algorithm's output \fdelity.\nInverse Problems In many cases, however, optimization problems with manually de-\nsigned objective functions only provide approximations to the original signals that we are\nreally interested. This is often due to inexact prior knowledge about the original signals. For\nexample, sparsity and total variation regularizations only partially re\rect the complexity of\n16\n\nLearning to Optimize: A Primer and A Benchmark\nnatural images (approximated sparsity and smoothness) which are hardly true in real-world\napplications and do not depict the exact characteristics of natural images. Therefore, many\nworks solve the inverse problem directly, striving to recover the original signals that we are\nreally interested in. We return to this task in a later subsection.\nConstrained Optimization Problems Most target problems mentioned above in this\nsubsection are unconstrained optimization problems. Some exceptions such as the sparsity-\nconstrained and low-rank regression problems, e.g., LASSO, have equivalent unconstrained\nformulation under proper regularizations. Some others have easy-to-implement forms of\nprojection onto the constraint sets, including the `0=1-constrained regression [146; 147; 148],\nand non-negative matrix factorization with non-negative constraints [152].\nThere have been a few e\u000borts directly tackling more general constrained optimization\nproblems using unrolling. [162] unrolled Frank-Wolfe algorithm to solve the structured re-\ngression with general `p-norm constraint ( p\u00151), and proposed a novel closed-form nonlinear\npooling unit parameterized by pfor the projection. [163] unrolled Frank-Wolfe algorithm\nfor least square problems with a\u000ene, non-negative and `p-norm constraints. It was also the\n\frst to apply the unrolled network to \fnancial data processing. [10; 164] investigated image\nrestoration with various hard constraints and unrolled proximal interior point algorithms\nwhile incorporating the constraints using a logarithmic barrier.\n3.2.2 Different Algorithms Unrolled\nFor the bulk of iterative optimization algorithms that have been unrolled, we classify them\ninto three categories: forward backward splitting, primal-dual methods, and others . The\n\frst two categories consist entirely of \frst-order algorithms, which is due to their low com-\nputational complexity and the fact their resultant L2O methods are often more reliable to\ntrain. We also emphasize that although various works discussed below may use the same\nunderlying algorithm as the base, they can vary greatly in their performance due to choices\nregarding what parameters are learned and what safeguard precautions are used to ensure\nconvergence (discussed further in subsequent sections). We also emphasize that the major-\nity of these algorithms revolve around obtaining some form of sparsity/low rank solution.\nWe begin with forward backward splitting (FBS). The simplest learned scheme is Gradi-\nent Descent , which is where the the gradient descent operation is applied (i.e., the forward\noperator) and the backward operator is simply the identity. This class of methods is studied\nin many works (e.g., see [135; 100; 165; 166; 160; 167; 168; 169; 170; 155; 127; 171; 139;\n172; 173]). However, the most popular focus in the literature is on problems with sparsity\nconstraints, which are usually modeled by `1/`0-minimization. The former, also known as\nLASSO, can be solved by the iterative shrinkage-thresholding algorithm (ISTA) [174] and\nits variants. This has yielded great interest in L2O schemes, as evidenced by the fact the\nworks [4; 161; 140; 8; 9; 1; 2; 175; 136; 176; 177; 137; 178; 179], among others, provide\nvarious ways to unroll the original ISTA algorithm. Additionally, [133; 134; 180; 178] un-\nroll a Nesterov accelerated ISTA known as FISTA (Fast ISTA) [181]. Continuing in the\nvein of sparsity, [182; 13; 183] unroll another algorithm, called approximate message passing\n(AMP), which introduces Onsager correction terms that whitens the noise in intermediate\nsignals while [184; 176; 175] unroll Orthogonal AMP [185], an extension to the original\nAMP. For the `0-minimization problems, the iterative hard-thresholding (IHT) algorithm,\nwhich replaces the soft-thresholding function in ISTA with hard-thresholding, is unrolled\n17\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\nin [146; 147]. Switching gears, for problems that involve minimize an objective that is the\nsum of several terms, the incremental proximal gradient method has also been unrolled\n[186]. Further generalization of FBS is given by an abstract collection of \rexible iterative\nmodularization algorithms (FIMAs) [187] that perform updates using the composition of\ntwo learned operators (provided the composition monotonically decreases an energy). A\ncaveat of this particular approach is that the learned operator is also composed with a\nclassic proximal-gradient operation.\nThe next category of unrolled L2O algorithms consists of primal-dual schemes. These\ntypically include variations of primal-dual hybrid gradient (PDHG) and the alternating\ndirection method of multipliers (ADMM) (e.g., see [132; 188; 123; 168; 189; 138; 190]). An-\nother variation used is the Condat-Vu primal-dual hybrid gradient [191]. As above, many\nof these methods also focus on leveraging the sparse structure of data.\nThe remaining group of miscellaneous methods take various forms. These include an-\nother \frst-order algorithm, Half Quadratic Splitting (HQS), for image restoration [124; 101].\nUsing a substitution and soft constraint, the HQS approach solves a problem using a model\nthat approximates variational problems (VPs). Beyond \frst-order algorithms, unrolling is\nalso applied to second-order (Newton or Quasi-Newton) methods [192; 193], Di\u000berential\nEquations [156; 194; 195] and Interior Point method [10; 164], and Frank-Wolfe algorithm\n[162; 163]. Conjugate Gradient was proposed [196] to help yield a more accurate enforce-\nment of the data-consistency constraint at each iteration than comparable proximal-gradient\nmethods. Lastly, [197] propose an unfolded version of a greedy pursuit algorithm, i.e., or-\nthogonal matching pursuit (OMP), that directly targets at the original combinatorial sparse\nselection problem. Their methods called Learned Greedy Method (LGM) can accommodate\na dynamic number of unfolded layers, and a stopping mechanism based on representation\nerror, both adapted to the input. Besides, there are also L2O schemes that appear to be\nbased entirely on heuristic combinations of classic optimization methods (e.g., see [198]).\n3.2.3 Objective-Based v.s. Inverse Problems\nObjective Based. The simplest L2O unrolling scheme is objective based. Training is\napplied here to yield rapid convergence for a particular distribution of data. The training\nloss can take various forms, including minimizing the expectation of an objective function\n[136], the objective's gradient norm, the distance to the optimal solution, or the \fxed point\nresidual of the algorithm [24] (e.g., if Tis the update operator, then kx\u0000T(x)kis the \fxed\npoint residual). Examples of objectives that have been extensively studied in the literature\nare presented in Section. 3.2.1.\nIn addition, safeguarding can be used in this situation, for guiding learned updates\nto ensure convergence [199; 24]. This can be accomplished in multiple ways. A typical\napproach is to generate a tentative update using an L2O scheme and then check whether\nthe tentative update satis\fes some form of descent inequality (e.g., yields a lesser energy\nvalue or \fxed point residual). If descent is obtained, then the tentative update is used;\notherwise, a classic optimization update is used. These safeguarded schemes provide the\nbene\ft of reducing computational costs via L2O machinery while maintaining theoretical\nconvergence guarantees.\n18\n\nLearning to Optimize: A Primer and A Benchmark\nInverse Problems Several L2O methods attempt to solve inverse problems (IPs) (e.g.,\nsee [200; 198; 191; 201; 173; 202; 187; 171]). Here the task is to reconstruct a signal x?from\nindirect noisy measurements. The measurements are typically expressed by d2Rmand are\nrelated to a forward operator A:Rn!Rmby\nd=A(x?) +\"; (13)\nwhere\"is noise. A few repeated themes arise the L2O IP literature. The typical process\nis to i) set up a variational problem (VP) as a surrogate model, ii) choose a parameterized\noptimization algorithm that can solve4(VP), and iii) perform supervised learning to identify\nthe optimal parameter settings. We expound upon these themes and their nuances below.\nFirst, by creating a variational problem, one assumes u?approximately solves\nmin\nx2Rn`(d;A(x)) +J(x); (VP)\nwhere`:Rm\u0002Rm!Ris a \fdelity term that encourages the estimate uto be consistent with\nthe measurements dandJ:Rn!Ris a regularizer. Learning comes into play since u?is\nusually notthe solution to (VP), but instead some \\close\" variant. Thus, upon choosing the\nform of`andJ, one includes tunable parameters in the model and unrolls a parameterized\noptimization scheme for (typically) a \fxed number Kof iterations. Algorithm updates\ncan be parameterized beyond their classic form (e.g., replace a \fxed matrix with a matrix\nthat has tunable entries as done in [4]). In most cases, the training loss takes the form of\nminimizing the expected value of the square of the Euclidean distance between the output\nestimateuKand the true signal u?, i.e., the parameters \u0002 are trained to solve\nmin\n\u0002Ed\u0018D\u0002\nkxK(\u0002;d)\u0000x?\ndk2\u0003\n: (14)\nThe primary alternative tranining loss for L2O is to use estimates of the Wasserstein-1\ndistance between the distribution of reconstructed signals and the distributio of true signals\n(e.g., see [127]), which yields unsupervised training.\n3.2.4 Learned Parameter Roles\nA key aspect of unrolled L2O methods is to determine how to parameterize each update.\nThis subsection discusses some of the common considerations and roles for these parameters.\nLearning parameters in the iterations First, the direct method of parameterization is\nto convert scalars/vectors/matrix/\flters used in iterative algorithms into learnable parame-\nters and learn them through data-driven training process. In this type of parameterization,\nlearning can overcome the need to hand choose hyperparameters. For example, LISTA [4]\nunrolls ISTA, which usually has update formulation\nxk+1=\u0011\u0015=L\u0012\nxk\u00001\nLAT(Axk\u0000d)\u0013\n; (15)\nwhere\u0015is the coe\u000ecient before the `1regularization in LASSO, Lis the largest eigenvalue\nofATA, and\u0011\u0012(\u0001) is the coordinate-wise soft-thresholding function parameterized with\nthreshold\u00125. Then LISTA parameterizes ISTA as a recurrent formulation\nxk+1=\u0011\u0012\u0010\nWed+Sxk\u0011\n; (16)\n4. We mean to say that, for some choice of parameters, the algorithms solves (VP).\n5. The soft-thresholding function takes the formula as \u0011\u0012(z) = sign(z)\u0001max(0;jzj\u0000\u0012)\n19\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\nwhere taking We\u0011AT=L,S\u0011I\u0000ATA=L and\u0012\u0011\u0015=L reduces (16) to ISTA. A large\namount of unrolling works follows the same methodology, but di\u000ber from each other in\nspeci\fc perspectives during the parameterization process, e.g. drop the recurrent structure\nin [4] and use feed-forward modeling by untying the learnable parameters in di\u000berent lay-\ners and update them independently [150; 182]. This untying formulation can enlarge the\ncapacity of the unrolled model [150] but can also cause the training instability due to the\noverwhelming parameter space and the di\u000eculty of theoretical analysis on the convergence\nand other properties of the unrolled model.\nTo this end, there has been a line of e\u000borts that strive to reduce the number of train-\nable parameters by relating and coupling di\u000berent parts of parameterization [147; 1] and\nanalytically generate parameters that satisfy certain constraints [2]. ALISTA [2] learns\nthresholding and step size parameters, reducing the number of parameters signi\fcantly {\nto two scalars per layer, which stabilizes the data-driven process while achieving state-of-\nthe-art recovery performance at the time with theoretical convergence guarantee.\nBesides, recent trends also started to apply model-free L2O methods, to predict hyper-\nparameters in classic iterative optimization algorithms. This is seen as the fusion among\nmodel-free L2O (since the algorithmic techniques are LSTM- or RL-based), model-speci\fc\nL2O (as the update rule is eventually based on the classic iterates), and more traditional\nhyperparameter optimization. Examples include learning an adaptive learning rate sched-\nule for training deep networks with stochastic graduate descent [36; 203; 204]; coordinating\nspeci\fcally for layer-wise training [205] or domain adaptation speeds [22]; predicting the\nupdate combination weights and decay rates in Adam [206]; or estimating training sample\nimportance [204; 207], among others.\nDeep priors learned from data Another popular parameterization in unrolling is to\nuse a deep model to learn a data-dependent prior on the interested signals to replace hand-\ndesigned priors such as sparsity and total variation, which are not accurate in real-world\napplications and thus introduce bias. However, previous works have various ways to use the\n\\learned prior\". For example in [123; 183; 100; 124], people used data-driven training to\nlearn a proximal or a projection operator that is iteratively used in the unrolled algorithm.\nThe learned operator takes recovered signals contaminated by noises and artifacts as inputs\nand outputs a re\fned estimation. Sometimes the learned operator is found to \ft a denoiser.\nThe main di\u000berence of these learned prior methods from Plug-and-Play methods is that\nthe operator is usually learned in a end-to-end way, making it over\ftting to the current\ntask or data distribution and not be able to be plugged into other algorithms. [208; 209]\nsupposed that the relevant vectors lie near the range of a generative model, and hence used\ngenerative adversarial networks (GANs) as a learning-based prior for image compressive\nsensing. [127; 171] perceive the prior as a loss function that can distinguish between coarsely\nrecovered signals without considering any priors, and real on-domain signals with good\nquality. The prior as a loss function is adversarially trained to output high losses for coarse\nrecoveries and low losses for real signals. Then we use the (sub-)gradient generative by the\nlearned network via back-propagation in the unrolled algorithms e.g. gradient descent.\nOthers Besides the above two major ways of parameterization, there are also works that\nlearn black-box agents that directly generate next-step estimations given the current iterate\nand historical information [167; 192; 170]. For instance, [179] learns an LSTM network that\ngenerates the step sizes and threshold parameters within each layer in ALISTA, instead of\ntraining them using back-propagation.\n20\n\nLearning to Optimize: A Primer and A Benchmark\n3.3 Applications\nUnrolled L2O schemes have found many applications. Below we identify several of these\nand note that there is some overlap among the three approaches discussed in this section.\n•Image Restoration and Reconstruction. The model-based L2O methods, includ-\ning both Plug-and-Play and unrolling methods, are widely used for various tasks in\nimage restoration, enhancement, and reconstruction. Popular application examples\ninclude denoising [165; 170; 124; 155; 210; 2; 189; 127; 211], deblurring [165; 124; 122;\n212; 10; 211; 171; 104; 213], super-resolution [123; 135; 170; 124; 155; 212; 104], inpaint-\ning [123; 170; 210; 178], and compressive sensing [123; 183; 1; 165; 134; 214; 9; 176; 211],\nJPEG artifacts reduction [8; 155; 215], demosaicking [122], dehazing [101] and derain-\ning [216]. Note that not all those works identically stick to the algorithm's original\narchitecture; instead many only follow loosely the idea, and replace various compo-\nnents with convolutions or other modern deep learning building blocks.\n•Medical and Biological Imaging. We speci\fcally separate Medical and Biology\nImaging applications from the previous Image Restoration part because the former\nhas its own special scenario and challenges. Imaging techniques in medical such as\nMRI and CT require accurate reconstructions of images with as few measurements\nas possible that result in minimal discomfort or side-e\u000bect. It is also challenging to\nextend the model-based methods in natural images to properly deal with the complex-\nvalued inputs [188]. Other work that applies model-based L2O to medical and biology\nimaging includes [168; 217; 167; 165; 127; 218; 171; 137; 219].\n•Wireless Communication. Tasks in wireless communication systems can also be\nsolved by unrolling methods, e.g. resource management [220; 175; 221], channel es-\ntimation [184], signal detection [184] and LDPC coding [172].For example, MIMO\ndetection, which can be formulated as a sparse recovery problem, was shown to ben-\ne\ft from L2O methods based on \\deep unfolding\" of an iterative algorithm added\nwith trainable parameters [14], such as LISTA. The model-based L2O approaches\nhave already exhibited superior robustness and stability to low signal-to-noise (SNR),\nchannel correlation, modulation symbol and MIMO con\fguration mismatches [184].\nWe refer the readers to a seminal survey about unfolding methods in communication\nsystems [14].\n•Seismic Imaging. Another important application of model-based L2O is seismic\nimaging [222; 223; 224; 225]. Most of them adopt a more plug-and-play manner to\nlearn CNN-based projectors that are \frst trained using data and then integrated into\nclassic iterative updates, due to the desirable emphasis on the physical modeling.\n•Miscellaneous Applications: such as clustering [161; 226; 227] and classi\fcation\n[146; 23; 138], phase retrieval [228], RNA second structure prediction [19], speech\nrecognition and source separation [150; 229], remote sensing [230], smart grid [231],\ngraph recovery [232], and photometric stereo estimation [147].\n3.4 Theoretical E\u000borts\nAlthough successful empirical results show signi\fcant potential of L2O, limited theory exists\ndue to black-box training pertaining to such learned optimizers. That important gap often\n21\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\nmakes the broader usability of L2O questionable. We note that, the speci\fc optimization\nproblem and algorithm structure in model-based L2O often o\u000ber more opportunities for their\ntheoretical analysis, by bridging us to the wealth of classic optimization tools, compared to\nmodel-free L2O. Therefore, model-based L2O in a few speci\fc problems has been the main\nfocus of existing theoretical e\u000borts so far.\nTo explain why and how model-based L2O methods outperform traditional optimization\nalgorithms, there are several essential questions at the heart of model-based L2O:\n•(Capacity). Given the model of L2O, do there exist parameters in the model that\nmakes L2O provably outperform traditional optimization algorithms, over the task\ndistribution (i.e., \\in distribution\")? Is there any \\safeguard\" mechanism available\non L2O to ensure them at least as good as traditional algorithms even on examples\nout of the task distribution (i.e., \\out of distribution\" or OoD)?\n•(Trainability). Given the existence of such parameters, what training method should\nwe use to obtain those parameters? Do guarantees exist that the training method\nconverges to the ideal parameters?\n•(Generalization). Does the trained model generalize, say, to testing instances from\nthe same source of training instances (i.e., \\interpolation\")? Can the trained models\n\\extrapolate\", e.g., on testing instances more complicated than any training one?\n•(Interpretability). How can we explain what the L2O models have learned?\nWe list brief answers to the four questions here. Capacity and interpretability have been\npartially solved on signal/image-processing related problems by some recent works, as to\nbe detailed in the subsections below. Generalization gains increasing attention recently\nand some works provide bounds of generalization gap of some speci\fc L2O models. Lastly,\nto the best of our knowledge, there has been very limited theoretical work on the train-\nability of L2O, due to the high nonconvexity of the training objectives (see Section 3.2.3).\nThe main exception here is the recent work [37] discussing the local minima and gradient\nexplosion/vanishing in L2O training for quadratic minimization. This was also partially\naddressed by [131] where the authors used sorting activation functions [233] and Lipschitz\nnetworks to encourage gradient norm preservation.\n3.4.1 Capacity\nTo our best knowledge, [220] is the \frst e\u000bort on theories of L2O. It approximates a tra-\nditional method WMMSE (weighted minimum mean square error) with a fully-connected\nneural network and proves that the output of the neural network can be arbitrarily close to\nthe result of WMMSE as long as its number of layers and units is large enough. In other\nwords, this work estimates the approximation capacity of the neural network.\nApproximation capacity is a generic notion in machine learning. Speci\fcally for L2O,\nconvergence can be used to describe the model capacity: Do there exist parameters f\u0012kgk\nin (11) that make fxkgconverge better than classic optimization algorithms as k!1 ?\nThe work [1] adopts this way to describe the convergence of L2O on the sparse recovery\nproblem and give a convergence rate of LISTA which is better than that of classic algo-\nrithms ISTA/FISTA. [2; 234; 178; 136; 235; 236] improve the theoretical result of LISTA\nby designing delicate models, in another word, designing operator Tin (11). [189] analyzes\nthe convergence rate of di\u000berentiable linearized ADMM.\n22\n\nLearning to Optimize: A Primer and A Benchmark\nInstead of studying parameters \u0012k, another approach to establish convergence is to\npropose mathematical conditions on the operators (for example, operator Tin (11) and\noperatorHin (9a)) in the models. These mathematical conditions should not only guarantee\nthe convergence but also can be satis\fed practically. [88] proposes a continuity condition\nthat can be satis\fed by speci\fcally designed training method. [237] assumes smoothness of\nthe operator that is satis\fed by choosing smooth activation functions in a neural network\n(e.g., the sigmoid function). [171] assumes convexity of the regularizer in their math proof\nand proposes to parameterize the regularizer by a convex neural network. [131] proves\nthe convergence under assumptions about the manifold and su\u000ecient representation by the\nsampled data, which are usually satis\fed in practice.\nWhile the above e\u000borts focus on studying the convergence and acceleration e\u000bect of\nL2O over the target task distribution, a parallel and same important topic is to bound or\ncharacterize the convergence of L2O under OoD inputs: how much can the L2O convergence\ndegrade when applied to optimizees deviating from the task distribution? Seemingly daunt-\ning at the \frst glance, that goal may be ful\flled by L2O with a safeguard mechanism, that\ncan provide a way to establish convergence independent of the parameters and data. In this\nsense, the capacity of the original L2O models can be considered as enlarged as the conver-\ngence is attained on more OoD optimizees. The most common approach is to i) compute\na tentative L2O update using any method under the sun, ii) check if the tentative update\nyields a reduction in the value of an energy, and iii) accept the L2O update if the energy is\nless than some relative bound (e.g., the energy at the current iterate). If the tentative L2O\nupdate is rejected, then a fallback scheme is to apply the update of a classic optimization\nalgorithm. Because the energy is monotonically decreasing, it can be shown that the overall\nalgorithm converges to a minimizer of the energy. The energy is typically de\fned to be the\nobjective function in the variational problem (VP) or its di\u000berential [212; 238; 187; 199].\nAn alternative approach is de\fne the energy to measure the residual between updates (e.g.,\n[24]). That is, if Tis the update operator for a classic algorithm, then at a tentative update\nukwe check if the residual kuk\u0000T(uk)kis less than some relevant bound.\n3.4.2 Interpretability\nInterpretability is signi\fcant to a learning model, now even more than ever. Some e\u000borts\nhave been made on the interpretability of L2O, mainly about linking or reducing their be-\nhaviors to those of some analytic, better understood optimization algorithms. [147] studies\nunrolled iterative hard-thresholding (IHT) and points out that unrolled IHT adopts bet-\nter dictionary and a wider range of RIP condition than IHT. [133] demonstrates that the\nmechanism of LISTA is related to a speci\fc matrix factorization of the Gram matrix of\nthe dictionary. [135] explains the success of LISTA with a tradeo\u000b between convergence\nspeed and reconstruction accuracy. [2] shows that the weights in LISTA has low coherence\nwith the dictionary and proposes an analytic approach to calculate the weights. [232] an-\nalyzes alternating minimization algorithm quantitatively on a graph recovery problem and\nreveals the analogy between the learned L2O model and solving a sequence of adaptive\nconvex programs iteratively. [166] points out the analogy between deep-unfolded gradient\ndescent and gradient descent with Chebyshev step-size and shows that the learned step size\nof deep-unfolded gradient descent can be qualitatively reproduced by Chebyshev step-size.\n23\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n3.4.3 Generalization\nGeneralization is an important topic for L2O, just like for any other ML domain. Recent\nwork [239; 74] described a \\generalization-\frst\" perspective for L2O. The relationship be-\ntween generalization gap and the number of training instances provides us an estimate on\nhow many samples we should use. In the recent literature, [160], [240] and [241] studied\nthe Rademacher complexity, an upper bound of generalization gap. [160] estimates the\nRademacher complexity of gradient descent and Nesterov's accelerated gradients on a pa-\nrameterized quadratic optimization problem; [240] estimates the Rademacher complexity\nof a deep thresholding network on sparse linear representation problem. [241] proposes a\nreweighted RNN for the signal reconstruction problem and provides its Rademacher com-\nplexity. As its de\fnition suggests, generalization measures how the trained L2O models\nperform on unseen samples from the same distribution seen in training, but not on samples\ndeviating from that distribution. Please refer to the previous discussion in Section. 3.4.1 on\nhow methods such as Safeguard can mitigate the challenges of OoD optimizees.\nStability is highly related with generalization [160], some L2O works analyze the stability\nof their methods. For example, [201] estimates Lipschitz constants of their method to\nprovide stability analysis results both with respect to input signals and network parameters.\nThis yields a growth condition inequality. Another measure of stability is to ensure stability\nof the model as the weighting of the regularizer term in (VP) tends to zero [173; 202]. A\nthird approach is that of \b-regularization used with null space networks [242], which learns\na regularizer based on the null space of A. Each of these methods yields di\u000berent insights\nabout the stability of L2O schemes for inverse problems.\nSimilar to the bias-variance trade-o\u000b in ML, L2O models are also subject to the trade-o\u000b\nbetween the capacity and the generalization. [160] provided insights on such trade-o\u000b in an\nunrolled model by proving a generalization bound as a function of model depth, and related\nto the properties of the algorithm unrolled.\n4. The Open-L2O Benchmark\nThe community has made diverse attempts to explore L2O and generated a rich literature\nfor solving di\u000berent optimization tasks on di\u000berent data or problems, as well as di\u000berent\nsoftware implementations on various platforms. Each L2O method has its own training\nrecipe and hyperparameters. Because a common benchmark for the \feld has not yet been\nestablished, comparisons among di\u000berent L2O methods are inconsistent and sometimes\nunfair. In this section, we present our e\u000borts toward creating a comprehensive benchmark\nthat enables fair comparisons. To the best of our knowledge, this is the \frst time of such\nan attempt.\nTestbed problems : We choose some popular and representative test problems that\nhave been used in the existing L2O literature: (i) convex sparse optimization, including both\nsparse inverse problems and LASSO minimization; (ii) minimizing the nonconvex Rastrigin\nfunction, and (iii) training neural networks (NNs), which is a more challenging nonconvex\nminimization problem.\nTask distributions : Inspired by both research practice and real-world demands, we\nde\fne task distributions in the following problem-speci\fc way: (i) for sparse optimization,\nthe optimizees during training and testing are optimization problems with the same objec-\ntive function and decision variables but di\u000berent data ; (ii) in the Rastrigin-function test,\nduring both training and testing, the optimizees have di\u000berent decision variables and use\n24\n\nLearning to Optimize: A Primer and A Benchmark\nrandom initializations; (iii) in the NN training test, the training optimizees use the same\nnetwork architecture and dataset but random initializations; however, testing samples opti-\nmizees from a di\u000berent distribution , that is, the L2O optimizer is applied to train a network\nof an unseen architecture and on a di\u000berent dataset.\nCompared Methods and Evaluation Settings : For each problem, we choose appli-\ncable approaches that include both model-free and/or model-based ones, implement them\nin the TensorFlow framework, ensure identical training/testing data, and evaluate them in\nthe same but problem-speci\fc metrics. After presenting the results, we draw observations\nfrom these benchmark experiments.\nOur datasets and software are available as the Open-L2O package at: https://\ngithub.com/VITA-Group/Open-L2O . We hope that Open-L2O will foster reproducible re-\nsearch, fair benchmarking, and coordinated development e\u000borts in the L2O \feld.\n4.1 Test 1: Convex Sparse Optimization\n4.1.1 Learning to perform sparse optimization and sparse-signal recovery\nProblem de\fnition The sparse recovery problem has been widely studied in the model-\nbased L2O literature [4; 1; 2]. The task is to recover a sparse vector from its noisy linear\nmeasurements:\nbq=Ax\u0003\nq+\"q; (17)\nwherex\u0003\nq2Rnis a sparse vector, \"q2Rmis an additive noise, and qindexes an optimizee\ninstance. While x\u0003\nq;bq;\"qchange for each q, the measurement matrix A2Rm\u0002nis \fxed\nacross all training and testing. In practice, there is a matrix Aassociated with each sensor\nor sensing procedure.\nData generation We generate 51 ;200 samples as the training set and 1 ;024 pairs as the\nvalidation and testing sets, following the i.i.d. sampling procedure in [1]. We sample sparse\nvectorsx\u0003\nqwith components drawn i.i.d. drawn from the distribution Ber(0 :1)\u0001N(0;1),\nyielding an average sparsity of \u001810%. We run numerical experiments in four settings:\n•Noiseless . We take ( m;n) = (256;512) and sample AwithAij\u0018N(0;1=m) and\nthen normalize its columns to have the unit `2norm. The noise \"is always zero.\n•Noisy . The same as above except for Gaussian measurement noises \u000fqwith a signal-\nto-noise ratio (SNR) of 20dB.\n•Coherent . A Gaussian random matrix is highly incoherent, making sparse recovery\nrelatively easy. To increase the challenge, we compute a dictionary D2R256\u0002512\nfrom 400 natural images in the BSD500 dataset [243] using the block proximal gra-\ndient method [244] and, then, use it as the measurement matrix A, which has a high\ncoherence. Other settings remain unchanged from the \frst setting above.\n•Larger scale . We scale up the noiseless case to ( m;n) = (512;1024); the other\nsettings stay the same.\nModel and training settings All our model-based L2O approaches take measurements\nbqas input and return estimates ^ xq\u0019x\u0003\nq. They are trained to minimize the mean squared\nerrorEq\u0018Qk^xq\u0000x\u0003\nqk2\n2. We adopt a progressive training scheme following [13; 1; 234]. We\n25\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\nuse a batch size of 128 for training and a learning rate of 5 \u000210\u00004. Other hyperparameters\nfollow the default suggestions in their original papers. After training, the learned models\nare evaluated on the test set in NMSE (Normalized Mean Squared Error) in the decibel\n(dB) unit:\nNMSE dB(^xq;x\u0003\nq) = 10 log10\u0000\nk^xq\u0000x\u0003\nqk2=kx\u0003\nqk2\u0001\n:\nWe compare the following model-based L2O approaches, all of which are unrolled to 16\niterations: (1) a feed-forward version LISTA as proposed in [4], with untied weights across\nlayers; (2) LISTA-CP , a variant of LISTA with weight coupling [147; 1]; (3) LISTA-\nCPSS , a variant of LISTA with weight coupling and support selection techniques [1; 2]; (4)\nALISTA [2], the variant of LISTA-CPSS with minimal learnable parameters; (5) LAMP\n[13], an L2O network unrolled from the AMP algorithm; (6) LFISTA [245], an L2O network\nunrolled from the FISTA algorithm; (7) GLISTA [234], an improved LISTA model by\nadding gating mechanisms.\nResults Figure 5 summarizes the comparisons; its four sub\fgures present the NMSEs of\nsparse recovery under noiseless, noisy, coherent dictionary, and large scale settings, respec-\ntively. We make the following observations :\n•In the noiseless setting, ALISTA yields the best \fnal NMSE, and the support selection\ntechnique in LISTA-CPSS contributes to good performance. LAMP and GLISTA\nachieve slightly worse NMSEs than LISTA-CPSS but are much better than LISTA.\nSurprisingly, LFISTA fails to outperform LISTA, which we attribute to its heavier\nparameterization in [245] than other models and consequently harder training.\n•In the noisy setting of 20dB SNR, the performance of LAMP degrades severely while\nLISTA-CPSS and ALISTA still perform robustly. GLISTA outperforms all the others\nin the setting.\n•In setting of a coherent dictionary, ALISTA su\u000bers signi\fcantly from the coherence\nwhile the other methods that learn the weight matrices from data cope this issue bet-\nter. In particular, GLISTA has the best performance thanks to the gate mechanisms.\n•When it comes to a larger scale setting ( m;n) = (512;1024), LISTA, LISTA-CP,\nLFISTA and GLISTA have performance degradation due to the higher parameter-\nization burden. In comparison, ALISTA becomes the best among all since it has\nthe fewest parameters to learn: only the layer-wise thresholds and step-sizes. There-\nfore, ALISTA is least impacted by the problem scale and produces nearly the same\nperformance as it does the smaller setting of ( m;n) = (256;512).\n4.1.2 Learning to minimize the Lasso model\nProblem de\fnition Instead of sparse recovery, which aims to recover the original sparse\nvectors, our goal here is to minimize the LASSO objective even though its solution is often\ndi\u000berent from the true sparse vector:\nxLasso\nq = arg min\nxfq(x);wherefq(x) =1\n2kAx\u0000bqk2\n2+\u0015kxk1; (18)\nwhereA2Rm\u0002nis a known, \fxed, and normalized dictionary matrix, whose elements\nare sampled i.i.d. from a Gaussian distribution. An optimizee instance with index qis\n26\n\nLearning to Optimize: A Primer and A Benchmark\n0 2 4 6 8 10 12 14 1680\n60\n40\n20\n0NMSE (dB)\n(a) Noiseless, (256, 512)\n0 2 4 6 8 10 12 14 1620\n15\n10\n5\n0\n(b) SNR = 20dB, (256, 512)\n0 2 4 6 8 10 12 14 16\nNumber of layer30\n20\n10\n0NMSE (dB)\n(c) Ill-conditioned, (256, 512)\n0 2 4 6 8 10 12 14 16\nNumber of layer80\n60\n40\n20\n0\n(d) Large scale, (512, 1024)\nLISTA\nLAMPLISTA-CP\nALISTALISTA-CPSS\nGLISTALFISTA\nFigure 5: Results of sparse recovery in four di\u000berent settings: (a) noiseless with ( m;n) = (256;512);\n(b) additive Gaussian measurement noises of SNR=20dB and ( m;n) = (256;512); (c) coherent\ndictionary with ( m;n) = (256;512); (d) larger scale with ( m;n) = (512;1024). The x-axis counts\nthe layers and the y-axis is the NMSE of the recovery.\ncharacterized by a sparse vector x\u0003\nqandbq2Rm\u00021is the observed measurement under\nAfromx\u0003\nqfollowing the same linear generation model in the previous subsection. \u0015is a\nhyperparameter usually selected manually or by cross-validation, and is chosen to be 0 :005\nby default in all our experiments.\nUniquely, we compare both model-based and model-free L2O methods in this sec-\ntion. For the former, we can adopt similar algorithm unrolling recipes as in the previous\nsubsection. For the latter, we treat the problem as generic minimization and apply LSTM-\nbased L2O methods. To our best knowledge, this is the \frst comparison between the two\ndistinct L2O mainstreams. We hope the results provide quantitative understanding how\nmuch we can gain from incorporating problem-speci\fc structures (when available) into L2O.\nData generation We run the experiments with ( m;n) = (5;10), as well as ( m;n) =\n(25;50). We did not go larger due to the high memory cost of LSTM-based model-free L2O\nmethods. We sample 12 ;800 pairs of x\u0003\nqandbqfor training and 1 ;280 pairs for validation\nand testing. The samples are noiseless. During the testing, we set 1 ;000 iterations for\nboth model-free L2O methods and classic optimizers. We run model-based L2O ones with\na smaller \fxed number of iterations (layers), which are standard for them.\nFor each sample bq, we letf\u0003\nqdenote the optimal Lasso objective value, fq(xLasso). The\noptimization performance is measured with a modi\fed relative loss:\nRf;Q(x) =Eq\u0018Q[fq(x)\u0000f\u0003\nq]\nEq\u0018Q[f\u0003q]; (19)\nwhere the optimal solution is generated by 2 ;000 iterations of FISTA and the expectations\nare taken over the entire testing set.\n27\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n100101102103\nIterations106\n105\n104\n103\n102\n101\n100101102Relative Loss\nAdam\nRMSProp\nGD\nL2O-DML2O-enhanced\nL2O-RNNprop\nISTAFISTA\nLISTA\nALISTA100101102103128130132\n(a) (m,n)=(5,10)\n100101102103\nIterations105\n104\n103\n102\n101\n100101102Relative Loss\nAdam\nRMSProp\nGD\nL2O-DML2O-enhanced\nL2O-RNNprop\nISTAFISTA\nLISTA\nALISTA100101102103100120140\n (b) (m,n)=(25,50)\nFigure 6: Evaluation comparisons among analytic, model-based L2O, and model-free L2O optimiz-\ners on Lasso. y-axis represents the modi\fed relative loss (19), and x-axis denotes the number of\niterations, both in the logarithmic scale.\nWe run four categories of methods to solve the LASSO optimization problem:\n•Three sub-gradient descent algorithms: GD, ADAM and RMSProp, which serve as\n\\natural baselines\" with neither learning nor problem-speci\fc structure. All these\nalgorithms use a step size of 10\u00003. We veri\fed that changing step sizes did not\nnotably alter their performance. (Even though GD and its generalizations do not\nhandle nonsmooth objectives without smoothing or using proximal operators, they\nare popular optimizers that people try on almost everything, so we choose to test\nthem anyway.)\n•Two problem-speci\fc analytic optimizers: ISTA [174], a forward backward splitting\nalgorithm using a soft-thresholding backward operator and its Nesterov-accelerated\nversion, FISTA [181]. For both methods, we use a step size of 1 =L, whereLis the\nlargest eigenvalue of ATA, and a threshold of \u0015=L for the soft-thresholding function.\n•Two model-based L2O models: vanilla LISTA [4] and ALISTA [2], both unrolled to\n16 layers. We follow the same training setting in the last subsection except that\nwe replace the training loss from the mean squared error (w.r.t. the true solution)\nwith the LASSO loss. Since other compared methods take far more iterations, we\nalso tried to expand LISTA/ALISTA to more iterations during testing by appending\nFISTA iterations.\n•Three model-free L2O methods: L2O-DM [5], L2O-RNNprop [58] and L2O-enhanced\noptimizers [73] (Section 2.1). All L2O optimizers are trained with the Lasso objective\nfunction as the reward for 100 epochs and 1 ;000 iterations per epoch. The reported\nevaluation is the average performance over 10 random starting points.\nResults From Figure 6, we make the following observations:\n•At the small problem size (5 ;10), both ISTA and FISTA converge fast in tens of\niterations with FISTA being slightly ahead. Both model-based L2O models, ISTA\nand ALISTA of 16 iterations (layers), converge to solutions of precision compared\nto what FISTA can achieve after hundreds of iterations and better than what ISTA\n28\n\nLearning to Optimize: A Primer and A Benchmark\ncan do at 1,000 iterations. The advantage of ISTA/ALISTA can sustain beyond 16\niterations using FISTA updates.\n•In comparison, at the small problem size (5 ;10), model-free L2O methods exhibit far\nworse performance. L2O-RNNprop has slower convergence than ISTA/FISTA and\nonly produces reasonably good solutions after 1,000 iterations { though still much\nbetter than analytic optimizers Adam and RMSProp. L2O-DM, L2O-enhanced, and\nvanilla GD completely fail to decrease the objective value.\n•At the larger problem size (25 ;50), LISTA and ALISTA still converge to high-precision\nsolutions with only 16 iterations with sustained advantages from the FISTA extension.\nInterestingly, ISTA now becomes much slower than FISTA. All the sub-gradient de-\nscent and model-free L2O methods remain to perform poorly; only L2O-RNNprop can\nconverge faster than ISTA and comparable to FISTA, though reaching lower precision.\n•Our experiments clearly demonstrate the dominant advantage of incorporating problem-\nspeci\fc structures to the optimizers when it comes to both analytic and learned op-\ntimizers.\n4.2 Test 2: Minimization of non-convex function Rastrigin\nWe now turn to non-convex minimization. One popular non-convex test function is called\nthe Rastrigin function:\nf(x) =1\n2nX\ni=1x2\ni\u0000nX\ni=1\u000bcos (2\u0019xi) +\u000bn; (20)\nwhere\u000b= 10. We consider a broad family of similar functions fq(x) that generalizes\nRastrigin function:\nfq(x) =1\n2kAqx\u0000bqk2\n2\u0000\u000bcqcos(2\u0019x) +\u000bn; (21)\nwhere Aq2Rn\u0002n,bq2Rn\u00021andcq2Rn\u00021are parameters whose elements are sampled\ni.i.d. fromN(0;1). Obviously, the function (20) is a special case in this family with\nA=I;b=f0;0;:::; 0gT;c=f1;1;:::; 1gT. For training, we sample 1 ;280 triplets offAq,\nbq,cqgom two problem scales: n= 2 andn= 10. For evaluation, we sample another 128\ncombinations of Aq,bqandcqand report the average performance over 10 random starting\npoints. The number of steps for evaluation is 1 ;000.\nTwo groups of methods are compared: (1) Four traditional gradient descent algorithms,\nincluding ADAM with a 10\u00001step size, RMSProp with a 3 \u000210\u00001step size, GD with the\nline-searched step size started from 10\u00001, and NAG (Nesterov Accelerated Gradient) with\nthe line-searched step size started from 10\u00001. All other hyperparamters are tuned by careful\ngrid search. (2) Five model-free L2O methods, including L2O-DM [5], L2O-enhanced [73],\nL2O-Scale [59], L2O-RNNprop [58] (Section 2.1), and L2O-Swarm [18] (Section 2.3). All\nL2O optimizers are trained for 100 epochs and 1000 iterations per epoch. At the testing\nstage, we evaluate and report the logarithmic loss of unseen functions from the same family,\nwhich are plotted in Figure 7.\n29\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n100101102103\nIterations10\n5\n05101520253035Loss\nOracle: 1.8232\nAdam\nRMSProp\nGDNAG\nL2O-DM\nL2O-enhancedL2O-Scale\nL2O-RNNprop\nL2O-Swarm1001011021035.05.56.06.5\n(a)n=2\n100101102103\nIterations25\n0255075100125150Loss\nOracle: 19.6117\nAdam\nRMSProp\nGDNAG\nL2O-DM\nL2O-enhancedL2O-Scale\nL2O-RNNprop\nL2O-Swarm100101102103405060\n (b)n=10\nFigure 7: Evaluation and comparison across model-free L2O and analytic optimizers on the gener-\nalized family of Rastrigin functions. y-axis represents the average loss of function values, and x-axis\ndenotes the number of iterations in the logarithm scale.\nResults From Figure 7 we draw the following observations:\n•ADAM and RMSProp converge slightly more quickly than GD and NAG with line\nsearch, especially in the early stage, for both n= 2 andn= 10. All analytic optimizers\nconverge to local minima of similar qualities for n= 2, and NAG \fnds a slightly better\nsolution for n= 10.\n•L2O-DM, L2O-enhanced, L2O-RNNProp, and L2O-Scale perform similarly to ana-\nlytic optimizers regarding both solution quality and convergence speed, showing no\nobvious advantage over analytic optimizers. For n= 10, L2O-enhanced \fnds a solu-\ntion of better quality than the other two. But the three model-free L2Os have larger\nerror bars at n= 10, which indicates model instability.\n•Although not converging faster than others, L2O-Swarm locates higher quality solu-\ntions (lower losses) for both nvalues, especially n= 10. Since L2O-Swarm is the only\nmethod that leverages a population of LSTM-based L2O \\particles\", it explores a\nlarger landscape than the other methods, so it is not surprising that its higher search\ncost leads to better solutions.\n•The oracle objectives (dashed black lines) in Figure 7 are generated by the Run-and-\nInspect method [246], which can provably \fnd a global minimum if the objective\nfunction can be decomposed into a smooth strongly convex function (e.g., a quadratic\nfunction) plus a restricted function (e.g., sinusoidal oscillations). For n= 2, almost\nall methods reach a similar loss lied in [5 :5;6:0] in the end. For n= 10, L2O-Swarm\nperforms signi\fcantly better than other methods in terms of the achieved \fnal loss,\nthough it converges more slowly than most of the other approaches.\n4.3 Test 3: Neural Network Training\nOur last test is training multi-layer neural networks (NNs), one of the most common tasks\nof L2O since its beginning. This has been the playground for model-free L2O methods.\nThere are few problem-speci\fc structures to explore. Common optimizers are stochastic\ngradient descent and their variants. We hope this test to address two questions:\n30\n\nLearning to Optimize: A Primer and A Benchmark\n•Can model-free L2O optimizers outperform analytic ones on neural network training?\nIf so, how much is the margin?\n•Can model-free L2O optimizers generalize to unseen network architectures and data?\nTo fairly compare di\u000berent methods and answer these questions, we train L2O optimizers\non the same neural network used in [5]: a simple Multi-layer Perceptron (MLP) with one 20-\ndimension hidden layer and the sigmoid activation function, trained on the MNIST dataset\nto minimize the cross-entropy loss. Therefore, the task distribution becomes optimizing the\nsame MLP model with di\u000berent random initializations.\nWe probe the (out of distribution) generalizability of the learned optimizers by testing\nthem on two unseen optimization tasks, following the practice in [5; 73]:\n1. Train another MLP with one 20-dimension hidden layer, but using the ReLU activation\nfunction, on the MNIST dataset.\n2. Train another ConvNet on the MNIST dataset, consisting two convolution layers, two\nmax pooling layers, and one last fully connected layer. The \frst convolution layer uses\n16 3\u00023 \flters with stride 2. The second convolution layer uses 32 5 \u00025 \flters with stride\n2. The max pooling layers have size 2 \u00022 with stride 2.\nWe compare four model-free L2O optimizers: L2O-DM [5], L2O-enhanced [73], L2O-Scale\n[59], and L2O-RNNprop [58] (Section 2.1), all following the hyperparameters suggested\nin their original papers. SGD, Adam, and RMSProp are analytical optimizers served as\nthe baselines. All L2O optimizers are trained with the single model from 10,000 di\u000berent\nrandom initializations drawn from N(0;01). On each optimizee, now corresponding to a\nrandom initialization, the optimizers run for 100 iterations. The training uses a batch size\nof 128. During each run of testing, we evaluate learned optimizers on an unseen testing\noptimizee for 10 ;000 steps, which is much more than the training iteration number. We\nthen report the training loss of the optimizee. We perform 10 independent runs and report\nthe error bars.\nIt is worth mentioning that current L2O-optimizers can hardly scale to training large\nneural networks with over 1 \u0002106parameters. This is because to update the L2O optimizer\nusing back-propagation, we need to keep the gradients during unrolling and the computation\ngraph of the optimizee in memory, therefore, consuming too much memory. This remains\nan open challenge.\nResults From Figure 8, we make the following observations:\n•The relative performance of analytic optimizers is consistent: both Adam and RM-\nSProp achieve similar training losses on both ReLU-MLP and ConvNets, and better\nthan SGD. However, the learned optimizers exhibit di\u000berent levels of transferabil-\nity. L2O-DM completely diverges on ConvNet, but works on ReLU-MLP when the\nnumber of iterations is small, showing poor generalization.\n•L2O-Scale and L2O-RNNprop can generalize to ReLU-MLP as they achieve similar\nor lower training losses than analytic optimizer), but they cannot e\u000bectively optimize\nConvNet. L2O-RNNprop only works for a small number of iterations less than 4,000.\nL2O-Scale generalizes better to larger iteration numbers, thanks to its random scaling\nduring training.\n31\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n0 2000 4000 6000 8000 10000\nTraining Iterations106\n105\n104\n103\n102\n101\n100101102Training Loss\nAdam\nRMSProp\nSGD\nL2O-DM\nL2O-RNNprop\nL2O-Scale\nL2O-enhanced\n(a) MLP\n0 2000 4000 6000 8000 10000\nTraining Iterations104\n103\n102\n101\n100101102Training Loss\nAdam\nRMSProp\nSGD\nL2O-DM\nL2O-RNNprop\nL2O-Scale\nL2O-enhanced (b) ConvNet\nFigure 8: Evaluation and comparison across model-free L2O and analytic optimizers on neural\nnetwork training. y-axis represents the average training loss values, and x-axis denotes the number\nof iterations. Each curve is the average of ten runs, and error bars are plotted.\n•L2O-enhanced, which adopts increasing unroll length and o\u000b-line imitation strategy,\nachieves the best performance on both unseen networks after extended (optimizee)\ntraining iterations.\n•The issue of lacking stability during testing exists for all L2O optimizers. On both\nReLU-MLP and ConvNet, all L2O optimizers su\u000ber from larger loss variances as the\ntraining iteration grows, causing the errors to \ructuate more severely that those of\nthe analytic optimizers.\n4.4 Take-home messages\n?When appropriate model or problem structures are available to be leveraged, model-\nbased L2O methods clearly outperform model-free ones and analytic optimizers. On\noptimizees sampled from the same task distribution, model-based L2O optimizers\nhave solid, consistent, and reliable performance.\n?Although in many cases we can also observe performance gains of model-free L2O\napproaches over analytic ones, the gains are not consistent. The bene\fts of current\nmodel-free L2O approaches come to questions when they are applied to larger opti-\nmizees or applied with more iterations during testing, let alone to \\out-of-distribution\"\noptimizees. We \fnd no clear winner in existing model-free L2O approaches that can\nconsistently outperforms others across most tests.\n5. Concluding Remarks\nThis article provided the \frst \\panoramic\" review of the state-of-the-arts in the emerging\nL2O \feld, accompanied with the \frst-of-its-kind benchmark. The article reveals that, de-\nspite its promise, the L2O research is still in its infancy, facing many open challenges as\nwell as research opportunities from practice to theory.\nOn the theory side, Section 3.4 has listed many open theoretical questions for us to\nunderstand why and how model-based L2O methods outperform traditional optimizers.\nBesides those, for model-free L2O methods, the theoretical foundation has been scarce if any\n32\n\nLearning to Optimize: A Primer and A Benchmark\nat all. For example, although the training of L2O optimizers is often successful empirically,\nthere has been almost no theoretical result established for the convergence performance of\nsuch a L2O training process, putting the general feasibility of obtaining L2O models in\nquestion. Also, for both model-based and mode-free L2O approaches, the generalization or\nadaption guarantees of trained L2O to optimizees out of the task distribution are under-\nstudied yet highly demanded.\nOn the practical side, the scalability of model-free L2O methods is perhaps the biggest\nhurdle for them to become more practical. That includes scaling up to both larger and more\ncomplicated models, and to more iterations during meta-testing. For model-speci\fc L2O,\nthe current empirical success is still limited to a few special instances in inverse problems\nand sparse optimization, and relying on case-by-case modeling. An exploration towards\nbroader applications, and perhaps building a more general framework, is demanded.\nFurthermore, there is no absolute border between mode-based and model-free L2O meth-\nods, and the spectrum between the two extremes can hint many new research opportunities.\n[50] suggested a good perspective that unrolled networks (as one example of model-based\nL2O) is an intermediate state between generic networks and analytic algorithms, and might\nbe more data-e\u000ecient to learn. The view was supported by [247], which further advocated\nthat the unrolled model might be considered as a robust starting point for subsequently\ndata-driven model search. We also believe that end-to-end learning approaches can be\nimproved with current continuous optimization algorithms to bene\ft from the theoretical\nguarantees and state-of-the-art algorithms already available.\nSo, to conclude this article, let us quote Sir Winston Churchill: \\Now this is not the\nend. It is not even the beginning of the end. But it is, perhaps, the end of the beginning.\"\nAlthough most approaches we discussed in this paper are still at an exploratory level of\ndeployment, and are apparently not yet ready as general-purpose or commercial solvers, we\nare strongly con\fdent that machine learning has just began to feed the classic optimization\n\feld, and the blowout of L2O research progress has yet to start.\n33\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\nAppendix A. List of Abbreviations\nAbbreviation Full Name Description\nAdam Adaptive Moment EstimationA \frst-order gradient-based optimization algorithm\nof stochastic objective functions, based on adaptive\nestimates of lower-order moments.\nADMMAlternating Direction\nMethod of MultipliersAn optimization algorithm that solves convex\nproblems by breaking them into smaller pieces, each\nof which are then easier to handle.\nAutoML Automated Machine LearningThe process of automating the process of applying\nmachine learning to real-world problems.\nCASHCombined Algorithm Selection\nand Hyperparameter OptimizationA model selection strategy that consider the\nchoosing of data preparation, learning algorithm,\nand algorithm hyperparameters as one large global\noptimization problem.\nGAN Generative Adversarial NetworkA class of machine learning frameworks [79],\nwhere two neural networks contest with each other\nin a zero-sum game, to learn to generate new data\nwith the same statistics as a given training set.\nHPO Hyperparameter OptimizationThe problem of choosing a set of optimal\nhyperparameters for a machine learning algorithm.\nL2O Learning to OptimizeLearnable optimizers to predict update rules (for\noptimizees) \ft from data.\nLASSOLeast Absolute Shrinkage\nand Selection OperatorA linear regression method that uses shrinkage to\nencourages simple, sparse models.\nLSTM Long-short Term MemoryA variant of arti\fcial recurrent neural network\n(RNN) architecture, typically used to process\nsequence data.\nMLP Multi-layer PerceptronA class of feedforward arti\fcial neural network\n(ANN) composed of multiple layers of perceptrons\n(with threshold activation).\nPGD Projected Gradient DescentProjected gradient descent minimizes a function\nsubject to a constraint. At each step we move in\nthe direction of the negative gradient, and then\n\\project\" onto the feasible set.\nReLU Recti\fed Linear UnitAn activation function used in neural networks,\nde\fned asf(x) = max(x;0).\nRL Reinforcement LearningRL is an area of machine learning regarding how to\nmake intelligent agents take actions in an environment\nto maximize the cumulative reward.\nRMSProp Root Mean Square PropagationRMSprop gradient descent maintains a moving\n(discounted) average of the square of gradients, and\nthen divide the gradient by the root of this average.\nReferences\n[1] Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear\nconvergence of unfolded ista and its practical weights and thresholds. In Advances in\nNeural Information Processing Systems , pages 9061{9071, 2018.\n[2] Jialin Liu, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. ALISTA: Analytic\nweights are as good as learned weights in LISTA. In International Conference on\nLearning Representations , 2019.\n[3] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical pro-\ngramming , 103(1):127{152, 2005.\n34\n\nLearning to Optimize: A Primer and A Benchmark\n[4] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In\nProceedings of the 27th international conference on international conference on ma-\nchine learning , pages 399{406, 2010.\n[5] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Ho\u000bman, David Pfau,\nTom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by\ngradient descent by gradient descent. In Advances in neural information processing\nsystems , pages 3981{3989, 2016.\n[6] Yutian Chen, Matthew W Ho\u000bman, Sergio G\u0013 omez Colmenarejo, Misha Denil, Timo-\nthy P Lillicrap, Matt Botvinick, and Nando Freitas. Learning to learn without gradient\ndescent by gradient descent. In International Conference on Machine Learning , pages\n748{756, 2017.\n[7] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning com-\nbinatorial optimization algorithms over graphs. Advances in neural information pro-\ncessing systems , 30:6348{6358, 2017.\n[8] Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, and Thomas S\nHuang. D3: Deep dual-domain based fast restoration of jpeg-compressed images. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,\npages 2764{2772, 2016.\n[9] Jian Zhang and Bernard Ghanem. Ista-net: Interpretable optimization-inspired deep\nnetwork for image compressive sensing. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 1828{1837, 2018.\n[10] M.-C. Corbineau, C. Bertocchi, E. Chouzenoux, M. Prato, and J.-C. Pesquet. Learned\nimage deblurring by unfolding a proximal interior point algorithm. In 2019 IEEE\nInternational Conference on Image Processing (ICIP) , page 4664{4668. IEEE, Sep\n2019. ISBN 978-1-5386-6249-6. doi: 10.1109/ICIP.2019.8803438.\n[11] Dong Liang, Jing Cheng, Ziwen Ke, and Leslie Ying. Deep magnetic resonance im-\nage reconstruction: Inverse problems meet neural networks. IEEE Signal Processing\nMagazine , 37(1):141{151, 2020.\n[12] Tianwei Yin, Zihui Wu, He Sun, Adrian V Dalca, Yisong Yue, and Katherine L\nBouman. End-to-end sequential sampling and reconstruction for mr imaging. arXiv\npreprint arXiv:2105.06460 , 2021.\n[13] Mark Borgerding, Philip Schniter, and Sundeep Rangan. Amp-inspired deep networks\nfor sparse linear inverse problems. IEEE Transactions on Signal Processing , 65(16):\n4293{4308, Aug 2017. ISSN 1941-0476. doi: 10.1109/TSP.2017.2708040.\n[14] Alexios Balatsoukas-Stimming and Christoph Studer. Deep unfolding for communi-\ncations systems: A survey and some new directions. In 2019 IEEE International\nWorkshop on Signal Processing Systems (SiPS) , pages 266{271. IEEE, 2019.\n[15] Joseph Marino, Alexandre Pich\u0013 e, Alessandro Davide Ialongo, and Yisong Yue. Itera-\ntive amortized policy optimization. arXiv preprint arXiv:2010.10670 , 2020.\n35\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n[16] Nelson Vadori, Sumitra Ganesh, Prashant Reddy, and Manuela Veloso. Calibration\nof shared equilibria in general sum partially observable markov games. Advances in\nNeural Information Processing Systems , 33, 2020.\n[17] Nelson Vadori, Rahul Savani, Thomas Spooner, and Sumitra Ganesh. Consensus mul-\ntiplicative weights update: Learning to learn using projector-based game signatures.\narXiv preprint arXiv:2106.02615 , 2021.\n[18] Yue Cao, Tianlong Chen, Zhangyang Wang, and Yang Shen. Learning to optimize in\nswarms. In Advances in Neural Information Processing Systems , pages 15018{15028,\n2019.\n[19] Xinshi Chen, Yu Li, Ramzan Umarov, Xin Gao, and Le Song. Rna secondary structure\nprediction by learning unrolled algorithms. In International Conference on Learning\nRepresentations , 2019.\n[20] Amritanshu Agrawal, Tim Menzies, Leandro L Minku, Markus Wagner, and Zhe Yu.\nBetter software analytics via \\duo\": Data mining algorithms using/used-by optimiz-\ners. Empirical Software Engineering , 25(3):2099{2136, 2020.\n[21] Chaojian Li, Tianlong Chen, Haoran You, Zhangyang Wang, and Yingyan Lin. Halo:\nHardware-aware learning to optimize. In Proceedings of the European Conference on\nComputer Vision (ECCV) , September 2020.\n[22] Wuyang Chen, Zhiding Yu, Zhangyang Wang, and Anima Anandkumar. Auto-\nmated synthetic-to-real generalization. International Conference on Machine Learning\n(ICML) , 2020.\n[23] Joey Tianyi Zhou, Kai Di, Jiawei Du, Xi Peng, Hao Yang, Sinno Jialin Pan, Ivor W\nTsang, Yong Liu, Zheng Qin, and Rick Siow Mong Goh. Sc2net: Sparse lstms for\nsparse coding. In AAAI , pages 4588{4595, 2018.\n[24] Howard Heaton, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. Safeguarded\nlearned convex optimization. arXiv preprint arXiv:2003.01880 , 2020.\n[25] Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning.\nArti\fcial intelligence review , 18(2):77{95, 2002.\n[26] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-\nlearning in neural networks: A survey. arXiv preprint arXiv:2004.05439 , 2020.\n[27] Xiaohan Chen, Siyu Tang, Krikamol Muandet, et al. Mate: Plugging in model aware-\nness to task embedding for meta learning. In Neural Information Processing Systems\n2020, 2020.\n[28] Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint\narXiv:1703.00441 , 2017.\n[29] Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, Hu Yi-Qi, Li Yu-\nFeng, Tu Wei-Wei, Yang Qiang, and Yu Yang. Taking human out of learning appli-\ncations: A survey on automated machine learning. arXiv preprint arXiv:1810.13306 ,\n2018.\n36\n\nLearning to Optimize: A Primer and A Benchmark\n[30] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search:\nA survey. arXiv preprint arXiv:1808.05377 , 2018.\n[31] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based opti-\nmization for general algorithm con\fguration. In International conference on learning\nand intelligent optimization , pages 507{523. Springer, 2011.\n[32] Matthias Feurer and Frank Hutter. Hyperparameter optimization. In Automated\nMachine Learning , pages 3{33. Springer, Cham, 2019.\n[33] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast\nbayesian optimization of machine learning hyperparameters on large datasets. In\nArti\fcial Intelligence and Statistics , pages 528{536. PMLR, 2017.\n[34] Yang Yu, Hong Qian, and Yi-Qi Hu. Derivative-free optimization via classi\fcation.\nInAAAI , volume 16, pages 2286{2292, 2016.\n[35] Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Auto-weka:\nCombined selection and hyperparameter optimization of classi\fcation algorithms. In\nProceedings of the 19th ACM SIGKDD international conference on Knowledge dis-\ncovery and data mining , pages 847{855, 2013.\n[36] Zhen Xu, Andrew M Dai, Jonas Kemp, and Luke Metz. Learning an adaptive learning\nrate schedule. arXiv preprint arXiv:1909.09712 , 2019.\n[37] Xiang Wang, Shuai Yuan, Chenwei Wu, and Rong Ge. Guarantees for tuning the step\nsize using a learning-to-learn approach. arXiv preprint arXiv:2006.16495 , 2020.\n[38] Mit eecs 6.890 learning-augmented algorithms. https://www.eecs.mit.edu/\nacademics-admissions/academic-information/subject-updates-spring-2019/\n6890 . Spring 2019.\n[39] Tim Kraska, Alex Beutel, Ed H Chi, Je\u000brey Dean, and Neoklis Polyzotis. The case\nfor learned index structures. In Proceedings of the 2018 International Conference on\nManagement of Data , pages 489{504, 2018.\n[40] Michael Mitzenmacher. A model for learned bloom \flters and optimizing by sand-\nwiching. In Advances in Neural Information Processing Systems , pages 464{473, 2018.\n[41] Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P Woodru\u000b. Learning-\naugmented data stream algorithms. In International Conference on Learning Repre-\nsentations , 2019.\n[42] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency\nestimation algorithms. In International Conference on Learning Representations ,\n2019.\n[43] Dylan J Foster, Alexander Rakhlin, and Karthik Sridharan. Online learning: Su\u000ecient\nstatistics and the burkholder method. In Conference On Learning Theory , pages 3028{\n3064, 2018.\n37\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n[44] Hyeji Kim, Yihan Jiang, Ranvir Rana, Sreeram Kannan, Sewoong Oh, and Pramod\nViswanath. Communication algorithms via deep learning. In 6th International Con-\nference on Learning Representations, ICLR 2018 , 2018.\n[45] Hyeji Kim, Yihan Jiang, Sreeram Kannan, Sewoong Oh, and Pramod Viswanath.\nDeepcode: Feedback codes via deep learning. IEEE Journal on Selected Areas in\nInformation Theory , 1(1):194{206, 2020.\n[46] Michael Mitzenmacher. Scheduling with predictions and the price of misprediction. In\n11th Innovations in Theoretical Computer Science Conference (ITCS 2020) . Schloss\nDagstuhl-Leibniz-Zentrum f ur Informatik, 2020.\n[47] Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations.\nInAdvances in Neural Information Processing Systems , pages 7402{7412, 2019.\n[48] Frank Hutter, Lars Kottho\u000b, and Joaquin Vanschoren. Automated machine learning:\nmethods, systems, challenges . Springer Nature, 2019.\n[49] Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art.\nKnowledge-Based Systems , 212:106622, 2021.\n[50] Vishal Monga, Yuelong Li, and Yonina C Eldar. Algorithm unrolling: Inter-\npretable, e\u000ecient deep learning for signal and image processing. arXiv preprint\narXiv:1912.10557 , 2019.\n[51] Nir Shlezinger, Jay Whang, Yonina C Eldar, and Alexandros G Dimakis. Model-based\ndeep learning. arXiv preprint arXiv:2012.08405 , 2020.\n[52] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph\nconvolutional networks and guided tree search. In Advances in Neural Information\nProcessing Systems , pages 539{548, 2018.\n[53] Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning\nCombinatorial Optimization Algorithms over Graphs. arXiv:1704.01665 , February\n2018.\n[54] Dimitris Bertsimas and Bartolomeo Stellato. Online mixed-integer optimization in\nmilliseconds. arXiv preprint arXiv:1907.02206 , 2019.\n[55] Dimitris Bertsimas and Bartolomeo Stellato. The voice of optimization. Machine\nLearning , 110(2):249{277, 2021.\n[56] Abhishek Cauligi, Preston Culbertson, Bartolomeo Stellato, Dimitris Bertsimas, Mac\nSchwager, and Marco Pavone. Learning mixed-integer convex optimization strategies\nfor robot planning and control. In 2020 59th IEEE Conference on Decision and\nControl (CDC) , pages 1698{1705. IEEE, 2020.\n[57] Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combina-\ntorial optimization: a methodological tour d'horizon. arXiv:1811.06128 , 2018.\n[58] Kaifeng Lv, Shunhua Jiang, and Jian Li. Learning gradient descent: Better general-\nization and longer horizons. arXiv preprint arXiv:1703.03633 , 2017.\n38\n\nLearning to Optimize: A Primer and A Benchmark\n[59] Olga Wichrowska, Niru Maheswaranathan, Matthew W. Ho\u000bman, Sergio Gomez Col-\nmenarejo, Misha Denil, Nando de Freitas, and Jascha Sohl-Dickstein. Learned opti-\nmizers that scale and generalize, 2017.\n[60] Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-\nDickstein. Understanding and correcting pathologies in the training of learned opti-\nmizers. In International Conference on Machine Learning , pages 4556{4565, 2019.\n[61] Ke Li and Jitendra Malik. Learning to optimize. In International Conference on\nLearning Representations (ICLR) , 2017.\n[62] Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V. Le. Neural optimizer search\nwith reinforcement learning. In Doina Precup and Yee Whye Teh, editors, Interna-\ntional Conference on Machine Learning , volume 70 of Proceedings of Machine Learn-\ning Research , pages 459{468, International Convention Centre, Sydney, Australia, 06{\n11 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/bello17a.html .\n[63] Haoming Jiang, Zhehui Chen, Yuyang Shi, Bo Dai, and Tuo Zhao. Learning to defense\nby learning to attack, 2018.\n[64] Yuanhao Xiong and Cho-Jui Hsieh. Improved adversarial training via learned opti-\nmizer, 2020.\n[65] Guillaume Leclerc and Aleksander Madry. The two regimes of deep network training.\narXiv preprint arXiv:2002.10376 , 2020.\n[66] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-\nAri. The large learning rate phase of deep learning: the catapult mechanism. arXiv\npreprint arXiv:2003.02218 , 2020.\n[67] Jingfeng Wu, Difan Zou, Vladimir Braverman, and Quanquan Gu. Direction matters:\nOn the implicit regularization e\u000bect of stochastic gradient descent with moderate\nlearning rate. arXiv preprint arXiv:2011.02538 , 2020.\n[68] Xinshi Chen, Hanjun Dai, Yu Li, Xin Gao, and Le Song. Learning to stop while\nlearning to predict. arXiv preprint arXiv:2006.05082 , 2020.\n[69] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114 , 2013.\n[70] Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural evolu-\ntion strategies. In 2008 IEEE Congress on Evolutionary Computation (IEEE World\nCongress on Computational Intelligence) , pages 3381{3387. IEEE, 2008.\n[71] Luke Metz, Niru Maheswaranathan, Jonathon Shlens, Jascha Sohl-Dickstein, and\nEkin D Cubuk. Using learned optimizers to make models robust to input noise. arXiv\npreprint arXiv:1906.03367 , 2019.\n[72] Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in\noptimization , 1(3):127{239, 2014.\n39\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n[73] Tianlong Chen, Weiyi Zhang, Jingyang Zhou, Shiyu Chang, Sijia Liu, Lisa Amini,\nand Zhangyang Wang. Training stronger baselines for learning to optimize. arXiv\npreprint arXiv:2010.09089 , 2020.\n[74] Diogo Almeida, Clemens Winter, Jie Tang, and Wojciech Zaremba. A generalizable\napproach to learning optimizers. arXiv preprint arXiv:2106.00958 , 2021.\n[75] Esteban Real, Chen Liang, David So, and Quoc Le. Automl-zero: evolving machine\nlearning algorithms from scratch. In International Conference on Machine Learning ,\npages 8007{8019. PMLR, 2020.\n[76] Jasper Snoek, Kevin Swersky, Rich Zemel, and Ryan Adams. Input warping for\nbayesian optimization of non-stationary functions. In International Conference on\nMachine Learning , pages 1674{1682, 2014.\n[77] James Bergstra, R\u0013 emi Bardenet, Yoshua Bengio, and Bal\u0013 azs K\u0013 egl. Algorithms for\nhyper-parameter optimization. Advances in neural information processing systems ,\n24:2546{2554, 2011.\n[78] Iain H Moal and Paul A Bates. Swarmdock and the use of normal modes in protein-\nprotein docking. International journal of molecular sciences , 11(10):3623{3648, 2010.\n[79] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In\nAdvances in neural information processing systems , pages 2672{2680, 2014.\n[80] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and\nAdrian Vladu. Towards deep learning models resistant to adversarial attacks. In\nInternational Conference on Learning Representations , 2018.\n[81] Zhenyu Wu, Zhangyang Wang, Zhaowen Wang, and Hailin Jin. Towards privacy-\npreserving visual recognition via adversarial training: A pilot study. In Proceedings\nof the European Conference on Computer Vision (ECCV) , pages 606{624, 2018.\n[82] Yangjun Ruan, Yuanhao Xiong, Sashank Reddi, Sanjiv Kumar, and Cho-Jui Hsieh.\nLearning to learn by zeroth-order oracle. arXiv preprint arXiv:1910.09464 , 2019.\n[83] Jiayi Shen, Xiaohan Chen, Howard Heaton, Tianlong Chen, Jialin Liu, Wotao Yin,\nand Zhangyang Wang. Learning a minimax optimizer: A pilot study. In International\nConference on Learning Representations (ICLR) , 2021.\n[84] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In\nInternational Conference on Learning Representations (ICLR) , 2016.\n[85] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn\nquickly for few-shot learning. arXiv preprint arXiv:1707.09835 , 2017.\n[86] Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. Plug-\nand-play priors for model based reconstruction. In 2013 IEEE Global Conference on\nSignal and Information Processing , pages 945{948. IEEE, 2013.\n[87] Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based\nnoise removal algorithms. Physica D: nonlinear phenomena , 60(1-4):259{268, 1992.\n40\n\nLearning to Optimize: A Primer and A Benchmark\n[88] Ernest Ryu, Jialin Liu, Sicheng Wang, Xiaohan Chen, Zhangyang Wang, and Wotao\nYin. Plug-and-play methods provably converge with properly trained denoisers. In\nInternational Conference on Machine Learning (ICML) , pages 5546{5557, 2019.\n[89] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image\ndenoising by sparse 3-d transform-domain collaborative \fltering. IEEE Transactions\non image processing , 16(8):2080{2095, 2007.\n[90] Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for de-\nsigning overcomplete dictionaries for sparse representation. IEEE Transactions on\nsignal processing , 54(11):4311{4322, 2006.\n[91] Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image\ndenoising. In 2005 IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR'05) , volume 2, pages 60{65. IEEE, 2005.\n[92] Felix Heide, Markus Steinberger, Yun-Ta Tsai, Mush\fqur Rouf, Dawid Pajak, Dikpal\nReddy, Orazio Gallo, Jing Liu, Wolfgang Heidrich, Karen Egiazarian, et al. Flexisp: A\n\rexible camera image processing framework. ACM Transactions on Graphics (TOG) ,\n33(6):1{13, 2014.\n[93] Christopher A Metzler, Arian Maleki, and Richard G Baraniuk. Bm3d-amp: A new\nimage recovery algorithm based on bm3d denoising. In 2015 IEEE International\nConference on Image Processing (ICIP) , pages 3116{3120. IEEE, 2015.\n[94] Arie Rond, Raja Giryes, and Michael Elad. Poisson inverse problems by the plug-\nand-play scheme. Journal of Visual Communication and Image Representation , 41:\n96{108, 2016.\n[95] Alon Brifman, Yaniv Romano, and Michael Elad. Turning a denoiser into a super-\nresolver using plug and play priors. In 2016 IEEE International Conference on Image\nProcessing (ICIP) , pages 1404{1408. IEEE, 2016.\n[96] Xiran Wang and Stanley H Chan. Parameter-free plug-and-play admm for image\nrestoration. In 2017 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 1323{1327. IEEE, 2017.\n[97] Shunsuke Ono. Primal-dual plug-and-play image restoration. IEEE Signal Processing\nLetters , 24(8):1108{1112, 2017.\n[98] Ulugbek S Kamilov, Hassan Mansour, and Brendt Wohlberg. A plug-and-play priors\napproach for solving nonlinear imaging inverse problems. IEEE Signal Processing\nLetters , 24(12):1872{1876, 2017.\n[99] Ji He, Yan Yang, Yongbo Wang, Dong Zeng, Zhaoying Bian, Hao Zhang, Jian Sun,\nZongben Xu, and Jianhua Ma. Optimizing a parameterized plug-and-play admm for\niterative low-dose ct reconstruction. IEEE transactions on medical imaging , 38(2):\n371{382, 2018.\n[100] Harshit Gupta, Kyong Hwan Jin, Ha Q Nguyen, Michael T McCann, and Michael\nUnser. Cnn-based projected gradient descent for consistent ct image reconstruction.\nIEEE transactions on medical imaging , 37(6):1440{1453, 2018.\n41\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n[101] Dong Yang and Jian Sun. Proximal dehaze-net: A prior learning-based deep network\nfor single image dehazing. In Proceedings of the European Conference on Computer\nVision (ECCV) , pages 702{717, 2018.\n[102] Dong Hye Ye, Somesh Srivastava, Jean-Baptiste Thibault, Ken Sauer, and Charles\nBouman. Deep residual learning for model-based iterative ct reconstruction using\nplug-and-play framework. In 2018 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 6668{6672. IEEE, 2018.\n[103] Qihui Lyu, Dan Ruan, John Ho\u000bman, Ryan Neph, Michael McNitt-Gray, and\nKe Sheng. Iterative reconstruction for low dose ct using plug-and-play alternating\ndirection method of multipliers (admm) framework. In Medical Imaging 2019: Im-\nage Processing , volume 10949, page 1094906. International Society for Optics and\nPhotonics, 2019.\n[104] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Deep plug-and-play super-resolution for\narbitrary blur kernels. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition , pages 1671{1681, 2019.\n[105] Xin Yuan, Yang Liu, Jinli Suo, and Qionghai Dai. Plug-and-play algorithms for large-\nscale snapshot compressive imaging. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 1447{1457, 2020.\n[106] Rizwan Ahmad, Charles A Bouman, Gregery T Buzzard, Stanley Chan, Sizhuo Liu,\nEdward T Reehorst, and Philip Schniter. Plug-and-play methods for magnetic reso-\nnance imaging: Using denoisers for image recovery. IEEE Signal Processing Magazine ,\n37(1):105{116, 2020.\n[107] Gary Mataev, Peyman Milanfar, and Michael Elad. Deepred: Deep image prior pow-\nered by red. In Proceedings of the IEEE/CVF International Conference on Computer\nVision Workshops , pages 0{0, 2019.\n[108] Guangxiao Song, Yu Sun, Jiaming Liu, Zhijie Wang, and Ulugbek S Kamilov. A new\nrecurrent plug-and-play prior based on the multiple self-similarity network. IEEE\nSignal Processing Letters , 27:451{455, 2020.\n[109] Suhas Sreehari, S Venkat Venkatakrishnan, Brendt Wohlberg, Gregery T Buzzard,\nLawrence F Drummy, Je\u000brey P Simmons, and Charles A Bouman. Plug-and-play\npriors for bright \feld electron tomography and sparse interpolation. IEEE Transac-\ntions on Computational Imaging , 2(4):408{423, 2016.\n[110] Stanley H Chan, Xiran Wang, and Omar A Elgendy. Plug-and-play admm for im-\nage restoration: Fixed-point convergence and applications. IEEE Transactions on\nComputational Imaging , 3(1):84{98, 2016.\n[111] Afonso M Teodoro, Jos\u0013 e M Bioucas-Dias, and M\u0013 ario AT Figueiredo. Scene-adapted\nplug-and-play algorithm with convergence guarantees. In 2017 IEEE 27th Inter-\nnational Workshop on Machine Learning for Signal Processing (MLSP) , pages 1{6.\nIEEE, 2017.\n42\n\nLearning to Optimize: A Primer and A Benchmark\n[112] Afonso M Teodoro, Jos\u0013 e M Bioucas-Dias, and M\u0013 ario AT Figueiredo. Image restora-\ntion and reconstruction using targeted plug-and-play priors. IEEE Transactions on\nComputational Imaging , 5(4):675{686, 2019.\n[113] Gregery T Buzzard, Stanley H Chan, Suhas Sreehari, and Charles A Bouman. Plug-\nand-play unplugged: Optimization-free reconstruction using consensus equilibrium.\nSIAM Journal on Imaging Sciences , 11(3):2001{2020, 2018.\n[114] Weisheng Dong, Peiyao Wang, Wotao Yin, Guangming Shi, Fangfang Wu, and Xi-\naotong Lu. Denoising prior driven deep neural network for image restoration. IEEE\ntransactions on pattern analysis and machine intelligence , 41(10):2305{2318, 2018.\n[115] Tom Tirer and Raja Giryes. Image restoration by iterative denoising and backward\nprojections. IEEE Transactions on Image Processing , 28(3):1220{1234, 2018.\n[116] Stanley H Chan. Performance analysis of plug-and-play admm: A graph signal pro-\ncessing perspective. IEEE Transactions on Computational Imaging , 5(2):274{286,\n2019.\n[117] Yu Sun, Brendt Wohlberg, and Ulugbek S Kamilov. An online plug-and-play algo-\nrithm for regularized image reconstruction. IEEE Transactions on Computational\nImaging , 5(3):395{408, 2019.\n[118] Ruturaj G Gavaskar and Kunal N Chaudhury. Plug-and-play ista converges with\nkernel denoisers. IEEE Signal Processing Letters , 27:610{614, 2020.\n[119] Xiaojian Xu, Yu Sun, Jiaming Liu, Brendt Wohlberg, and Ulugbek S Kamilov. Prov-\nable convergence of plug-and-play priors with mmse denoisers. IEEE Signal Processing\nLetters , 27:1280{1284, 2020.\n[120] Yu Sun, Zihui Wu, Brendt Wohlberg, and Ulugbek S Kamilov. Scalable plug-and-play\nadmm with convergence guarantees. arXiv preprint arXiv:2006.03224 , 2020.\n[121] Yu Sun, Jiaming Liu, Yiran Sun, Brendt Wohlberg, and Ulugbek S Kamilov. Async-\nred: A provably convergent asynchronous block parallel stochastic method using deep\ndenoising priors. arXiv preprint arXiv:2010.01446 , 2020.\n[122] Tim Meinhardt, Michael Moller, Caner Hazirbas, and Daniel Cremers. Learning\nproximal operators: Using denoising networks for regularizing inverse imaging prob-\nlems. In Proceedings of the IEEE International Conference on Computer Vision , pages\n1781{1790, 2017.\n[123] JH Rick Chang, Chun-Liang Li, Barnabas Poczos, BVK Vijaya Kumar, and Aswin C\nSankaranarayanan. One network to solve them all{solving linear inverse problems\nusing deep projection models. In Proceedings of the IEEE International Conference\non Computer Vision , pages 5888{5897, 2017.\n[124] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser\nprior for image restoration. In Proceedings of the IEEE conference on computer vision\nand pattern recognition , pages 3929{3938, 2017.\n43\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n[125] Siavash Arjomand Bigdeli, Matthias Zwicker, Paolo Favaro, and Meiguang Jin. Deep\nmean-shift priors for image restoration. In Advances in Neural Information Processing\nSystems , pages 763{772, 2017.\n[126] Siavash Arjomand Bigdeli. and Matthias Zwicker. Image restoration using autoen-\ncoding priors. In Proceedings of the 13th International Joint Conference on Com-\nputer Vision, Imaging and Computer Graphics Theory and Applications - Volume 5:\nVISAPP , pages 33{44. INSTICC, SciTePress, 2018. ISBN 978-989-758-290-5. doi:\n10.5220/0006532100330044.\n[127] Sebastian Lunz, Ozan Oktem, and Carola-Bibiane Sch onlieb. Adversarial regularizers\nin inverse problems. In Advances in Neural Information Processing Systems , pages\n8507{8516, 2018.\n[128] Kaixuan Wei, Angelica Aviles-Rivero, Jingwei Liang, Ying Fu, Carola-Bibiane Schn-\nlieb, and Hua Huang. Tuning-free plug-and-play proximal algorithm for inverse imag-\ning problems. arXiv preprint arXiv:2002.09611 , 2020.\n[129] Matthieu Terris, Audrey Repetti, Jean-Christophe Pesquet, and Yves Wiaux. En-\nhanced convergent pnp algorithms for image restoration. In IEEE ICIP 2021 Con-\nference Proceedings . IEEE, 2021.\n[130] Regev Cohen, Michael Elad, and Peyman Milanfar. Regularization by denoising via\n\fxed-point projection (red-pro). arXiv preprint arXiv:2008.00226 , 2020.\n[131] Howard Heaton, Samy Wu Fung, Alex Tong Lin, Stanley Osher, and Wotao Yin.\nProjecting to manifolds via unsupervised learning. arXiv preprint arXiv:2008.02200 ,\n2020.\n[132] Pablo Sprechmann, Roee Litman, Tal Ben Yakar, Alexander M Bronstein, and\nGuillermo Sapiro. Supervised sparse analysis and synthesis operators. Advances in\nNeural Information Processing Systems , 26:908{916, 2013.\n[133] Thomas Moreau and Joan Bruna. Understanding the learned iterative soft thresh-\nolding algorithm with matrix factorization. In International Conference on Learning\nRepresentations (ICLR) , 2017. URL http://arxiv.org/abs/1706.01338 .\n[134] Dimitris Perdios, Adrien Besson, Philippe Rossinelli, and Jean-Philippe Thiran.\nLearning the weight matrix for sparsity averaging in compressive imaging. In 2017\nIEEE International Conference on Image Processing (ICIP) , pages 3056{3060. IEEE,\n2017.\n[135] Raja Giryes, Yonina C. Eldar, Alex M. Bronstein, and Guillermo Sapiro. Tradeo\u000bs\nbetween convergence speed and reconstruction accuracy in inverse problems. IEEE\nTransactions on Signal Processing , 66(7):1676{1690, Apr 2018. ISSN 1941-0476. doi:\n10.1109/TSP.2018.2791945.\n[136] Pierre Ablin, Thomas Moreau, Mathurin Massias, and Alexandre Gramfort. Learning\nstep sizes for unfolded sparse coding. In Advances in Neural Information Processing\nSystems , pages 13100{13110, 2019.\n44\n\nLearning to Optimize: A Primer and A Benchmark\n[137] Satoshi Hara, Weichih Chen, Takashi Washio, Tetsuichi Wazawa, and Takeharu Nagai.\nSpod-net: Fast recovery of microscopic images using learned ista. In Asian Conference\non Machine Learning , page 694{709, Oct 2019.\n[138] Benjamin Cowen, Apoorva Nandini Saridena, and Anna Choromanska. Lsalsa: ac-\ncelerated source separation via learned sparse coding. Machine Learning , 108(8-9):\n1307{1327, 2019.\n[139] Shanshan Wu, Alexandros G. Dimakis, Sujay Sanghavi, Felix X. Yu, Daniel\nHoltmann-Rice, Dmitry Storcheus, Afshin Rostamizadeh, and Sanjiv Kumar.\nLearning a compressed sensing measurement matrix via gradient unrolling.\narXiv:1806.10175 [cs, math, stat] , Jul 2019. URL http://arxiv.org/abs/1806.\n10175 . arXiv: 1806.10175.\n[140] Pablo Sprechmann, Alex Bronstein, and Guillermo Sapiro. Learning e\u000ecient struc-\ntured sparse models. In International Conference on Machine Learning (ICML) , pages\n615{622, 2012.\n[141] Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks\nanalyzed via convolutional sparse coding. The Journal of Machine Learning Research ,\n18(1):2887{2938, 2017.\n[142] Aviad Aberdam, Jeremias Sulam, and Michael Elad. Multi-layer sparse coding: The\nholistic way. SIAM Journal on Mathematics of Data Science , 1(1):46{77, 2019.\n[143] Jeremias Sulam, Aviad Aberdam, Amir Beck, and Michael Elad. On multi-layer basis\npursuit, e\u000ecient algorithms and convolutional neural networks. IEEE transactions\non pattern analysis and machine intelligence , 42(8):1968{1980, 2019.\n[144] Hamza Cherkaoui, Jeremias Sulam, and Thomas Moreau. Learning to solve tv regu-\nlarised problems with unrolled algorithms. Advances in Neural Information Processing\nSystems , 33, 2020.\n[145] Beno^ \u0010t Mal\u0013 ezieux, Thomas Moreau, and Matthieu Kowalski. Dictionary and prior\nlearning with unrolled algorithms for unsupervised inverse problems. arXiv preprint\narXiv:2106.06338 , 2021.\n[146] Zhangyang Wang, Qing Ling, and Thomas S Huang. Learning deep `0 encoders. In\nThirtieth AAAI Conference on Arti\fcial Intelligence , 2016.\n[147] Bo Xin, Yizhou Wang, Wen Gao, David Wipf, and Baoyuan Wang. Maximal sparsity\nwith deep networks? In Advances in Neural Information Processing Systems , pages\n4340{4348, 2016.\n[148] Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, and Thomas S. Huang.\nLearning a Deep l1Encoder for Hashing. In Proceedings of the Twenty-Fifth Interna-\ntional Joint Conference on Arti\fcial Intelligence , IJCAI'16, pages 2174{2180. AAAI\nPress, 2016.\n[149] Christoph Studer, Tom Goldstein, Wotao Yin, and Richard G Baraniuk. Democratic\nrepresentations. arXiv preprint arXiv:1401.3420 , 2014.\n45\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n[150] John R. Hershey, Jonathan Le Roux, and Felix Weninger. Deep Unfolding: Model-\nBased Inspiration of Novel Deep Architectures. arXiv:1409.2574 , 2014.\n[151] P. Sprechmann, A. M. Bronstein, and G. Sapiro. Supervised non-euclidean sparse nmf\nvia bilevel optimization with applications to speech enhancement. In 2014 4th Joint\nWorkshop on Hands-free Speech Communication and Microphone Arrays (HSCMA) ,\npages 11{15, 2014.\n[152] P. Sprechmann, A. M. Bronstein, and G. Sapiro. Learning E\u000ecient Sparse and Low\nRank Models. IEEE Transactions on Pattern Analysis and Machine Intelligence , 37\n(9):1821{1833, September 2015.\n[153] Tal Ben Yakar, Roee Litman, Pablo Sprechmann, Alexander M Bronstein, and\nGuillermo Sapiro. Bilevel sparse models for polyphonic music transcription. In ISMIR ,\npages 65{70, 2013.\n[154] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet,\nZhizhong Su, Dalong Du, Chang Huang, and Philip HS Torr. Conditional random\n\felds as recurrent neural networks. In Proceedings of the IEEE international confer-\nence on computer vision , pages 1529{1537, 2015.\n[155] Yunjin Chen and Thomas Pock. Trainable nonlinear reaction di\u000busion: A \rexible\nframework for fast and e\u000bective image restoration. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 39(6):1256{1272, Jun 2017. ISSN 0162-8828, 2160-\n9292. doi: 10.1109/TPAMI.2016.2596743.\n[156] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from\ndata. In International Conference on Machine Learning (ICML) , pages 3208{3216,\n2018.\n[157] Zichao Long, Yiping Lu, and Bin Dong. PDE-Net 2.0: Learning PDEs from data\nwith a numeric-symbolic hybrid deep network. Journal of Computational Physics ,\n399:108925, December 2019. ISSN 0021-9991. doi: 10.1016/j.jcp.2019.108925.\n[158] Daniel Greenfeld, Meirav Galun, Ron Kimmel, Irad Yavneh, and Ronen Basri. Learn-\ning to Optimize Multigrid PDE Solvers. arXiv:1902.10248 [cs, math] , August 2019.\nURL http://arxiv.org/abs/1902.10248 . arXiv: 1902.10248.\n[159] Ste\u000ben Wiewel, Moritz Becher, and Nils Thuerey. Latent space physics: Towards\nlearning the temporal evolution of \ruid \row. In Computer Graphics Forum , volume 38,\npages 71{82. Wiley Online Library, 2019.\n[160] Xinshi Chen, Yufei Zhang, Christoph Reisinger, and Le Song. Understanding deep\narchitectures with reasoning layer. arXiv:2006.13401 [cs, stat] , Jun 2020. URL http:\n//arxiv.org/abs/2006.13401 . arXiv: 2006.13401.\n[161] Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, and Thomas S Huang.\nLearning a task-speci\fc deep architecture for clustering. In Proceedings of the 2016\nSIAM International Conference on Data Mining , pages 369{377. SIAM, 2016.\n[162] Dong Liu, Ke Sun, Zhangyang Wang, Runsheng Liu, and Zheng-Jun Zha. Frank-wolfe\nnetwork: An interpretable deep structure for non-sparse coding. IEEE Transactions\non Circuits and Systems for Video Technology , 2019.\n46\n\nLearning to Optimize: A Primer and A Benchmark\n[163] Ruben Pauwels, Evaggelia Tsiligianni, and Nikos Deligiannis. Hcgm-net: A deep un-\nfolding network for \fnancial index tracking. In ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) , pages 3910{3914.\nIEEE, 2021.\n[164] Carla Bertocchi, Emilie Chouzenoux, Marie-Caroline Corbineau, Jean-Christophe\nPesquet, and Marco Prato. Deep unfolding of a proximal interior point method for\nimage restoration. arXiv:1812.04276 [cs, math] , Jan 2020. URL http://arxiv.org/\nabs/1812.04276 . arXiv: 1812.04276.\n[165] Steven Diamond, Vincent Sitzmann, Felix Heide, and Gordon Wetzstein. Unrolled\noptimization with deep priors. arXiv:1705.08041 [cs] , Dec 2018. URL http://arxiv.\norg/abs/1705.08041 . arXiv: 1705.08041.\n[166] Satoshi Takabe and Tadashi Wadayama. Theoretical interpretation of learned step\nsize in deep-unfolded gradient descent. arXiv:2001.05142 [cs, math, stat] , Jan 2020.\nURL http://arxiv.org/abs/2001.05142 . arXiv: 2001.05142.\n[167] Jonas Adler and Ozan Oktem. Solving ill-posed inverse problems using iterative deep\nneural networks. Inverse Problems , 33(12):124007, 2017.\n[168] Jonas Adler and Ozan Oktem. Learned primal-dual reconstruction. IEEE transactions\non medical imaging , 37(6):1322{1332, 2018.\n[169] Justin Domke. Generic methods for optimization-based modeling. In Arti\fcial Intel-\nligence and Statistics , pages 318{326, 2012.\n[170] Patrick Putzky and Max Welling. Recurrent inference machines for solving inverse\nproblems. arXiv:1706.04008 [cs] , Jun 2017. URL http://arxiv.org/abs/1706.\n04008 . arXiv: 1706.04008.\n[171] Subhadip Mukherjee, S oren Dittmer, Zakhar Shumaylov, Sebastian Lunz, Ozan\nOktem, and Carola-Bibiane Sch onlieb. Learned convex regularizers for inverse prob-\nlems. arXiv preprint arXiv:2008.02839 , 2020.\n[172] Tadashi Wadayama and Satoshi Takabe. Deep learning-aided trainable projected\ngradient decoding for ldpc codes. arXiv:1901.04630 [cs, math] , Jan 2019. URL http:\n//arxiv.org/abs/1901.04630 . arXiv: 1901.04630.\n[173] Andreas Ko\rer, Markus Haltmeier, Tobias Schae\u000bter, Marc Kachelrie\u0019, Marc Dewey,\nChristian Wald, and Christoph Kolbitsch. Neural networks-based regularization for\nlarge-scale medical image reconstruction. Physics in Medicine & Biology , 65(13):\n135003, 2020.\n[174] Thomas Blumensath and Mike E Davies. Iterative thresholding for sparse approxi-\nmations. Journal of Fourier analysis and Applications , 14(5-6):629{654, 2008.\n[175] Satoshi Takabe, Tadashi Wadayama, and Yonina C Eldar. Complex trainable ista for\nlinear and nonlinear inverse problems. In IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 5020{5024, 2020.\n47\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n[176] Daisuke Ito, Satoshi Takabe, and Tadashi Wadayama. Trainable ista for sparse signal\nrecovery. IEEE Transactions on Signal Processing , 67(12):3113{3125, Jun 2019. ISSN\n1053-587X, 1941-0476. doi: 10.1109/TSP.2019.2912879.\n[177] Mengcheng Yao, Jian Dang, Zaichen Zhang, and Liang Wu. Sure-tista: A signal\nrecovery network for compressed sensing. In ICASSP 2019 - 2019 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) , page 3832{3836.\nIEEE, May 2019. ISBN 978-1-4799-8131-1. doi: 10.1109/ICASSP.2019.8683182. URL\nhttps://ieeexplore.ieee.org/document/8683182/ .\n[178] Aviad Aberdam, Alona Golts, and Michael Elad. Ada-lista: Learned solvers adaptive\nto varying models. arXiv:2001.08456 [cs, stat] , Feb 2020. URL http://arxiv.org/\nabs/2001.08456 . arXiv: 2001.08456.\n[179] Freya Behrens, Jonathan Sauder, and Peter Jung. Neurally augmented alista. In Inter-\nnational Conference on Learning Representations , 2021. URL https://openreview.\nnet/forum?id=q_S44KLQ_Aa .\n[180] Bahareh Tolooshams, Sourav Dey, and Demba Ba. Scalable convolutional dictionary\nlearning with constrained recurrent sparse auto-encoders. In 2018 IEEE 28th Inter-\nnational Workshop on Machine Learning for Signal Processing (MLSP) , pages 1{6.\nIEEE, 2018.\n[181] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for\nlinear inverse problems. SIAM journal on imaging sciences , 2(1):183{202, 2009.\n[182] Mark Borgerding and Philip Schniter. Onsager-corrected deep learning for sparse lin-\near inverse problems. In 2016 IEEE Global Conference on Signal and Information Pro-\ncessing (GlobalSIP) , page 227{231, Dec 2016. doi: 10.1109/GlobalSIP.2016.7905837.\n[183] Chris Metzler, Ali Mousavi, and Richard Baraniuk. Learned d-amp: Principled neu-\nral network based compressive image recovery. In Advances in Neural Information\nProcessing Systems , pages 1772{1783, 2017.\n[184] Hengtao He, Chao-Kai Wen, Shi Jin, and Geo\u000brey Ye Li. Model-driven deep learning\nfor mimo detection. IEEE Transactions on Signal Processing , 68:1702{1715, 2020.\n[185] Junjie Ma and Li Ping. Orthogonal amp. IEEE Access , 5:2020{2033, 2017.\n[186] Erich Kobler, Teresa Klatzer, Kerstin Hammernik, and Thomas Pock. Variational\nnetworks: connecting variational methods and deep learning. In German conference\non pattern recognition , pages 281{293. Springer, 2017.\n[187] Risheng Liu, Shichao Cheng, Yi He, Xin Fan, Zhouchen Lin, and Zhongxuan Luo. On\nthe convergence of learning-based iterative methods for nonconvex inverse problems.\nIEEE transactions on pattern analysis and machine intelligence , 2019.\n[188] Jian Sun, Huibin Li, Zongben Xu, et al. Deep admm-net for compressive sensing mri.\nInAdvances in neural information processing systems , pages 10{18, 2016.\n[189] Xingyu Xie, Jianlong Wu, Guangcan Liu, Zhisheng Zhong, and Zhouchen Lin. Di\u000ber-\nentiable linearized admm. In International Conference on Machine Learning , pages\n6902{6911, 2019.\n48\n\nLearning to Optimize: A Primer and A Benchmark\n[190] Jing Cheng, Haifeng Wang, Leslie Ying, and Dong Liang. Model learning: Primal\ndual networks for fast mr imaging. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention , pages 21{29. Springer, 2019.\n[191] Mingyuan Jiu and Nelly Pustelnik. A deep primal-dual proximal network for image\nrestoration. arXiv preprint arXiv:2007.00959 , 2020.\n[192] Xuehan Xiong and Fernando De la Torre. Supervised descent method and its ap-\nplications to face alignment. In 2013 IEEE Conference on Computer Vision and\nPattern Recognition , page 532{539. IEEE, Jun 2013. ISBN 978-0-7695-4989-7. doi:\n10.1109/CVPR.2013.75. URL http://ieeexplore.ieee.org/document/6618919/ .\n[193] Maojia Li, Jialin Liu, and Wotao Yin. Learning to combine quasi-newton methods.\n[194] Zichao Long, Yiping Lu, and Bin Dong. Pde-net 2.0: Learning pdes from data with\na numeric-symbolic hybrid deep network. Journal of Computational Physics , 399:\n108925, Dec 2019. ISSN 0021-9991. doi: 10.1016/j.jcp.2019.108925.\n[195] Xiaoshuai Zhang, Yiping Lu, Jiaying Liu, and Bin Dong. Dynamically unfold-\ning recurrent restorer: A moving endpoint control method for image restoration.\narXiv:1805.07709 [cs] , Oct 2018. URL http://arxiv.org/abs/1805.07709 . arXiv:\n1805.07709.\n[196] Hemant K Aggarwal, Merry P Mani, and Mathews Jacob. Modl: Model-based deep\nlearning architecture for inverse problems. IEEE transactions on medical imaging , 38\n(2):394{405, 2018.\n[197] Rajaei Khatib, Dror Simon, and Michael Elad. Learned greedy method (lgm): A novel\nneural architecture for sparse coding and beyond. arXiv preprint arXiv:2010.07069 ,\n2020.\n[198] Yixing Huang, Alexander Preuhs, Michael Manhart, Guenter Lauritsch, and Andreas\nMaier. Data consistent ct reconstruction from insu\u000ecient data with learned prior\nimages. arXiv preprint arXiv:2005.10034 , 2020.\n[199] Michael Moeller, Thomas Mollenho\u000b, and Daniel Cremers. Controlling neural net-\nworks via energy dissipation. In Proceedings of the IEEE International Conference on\nComputer Vision , pages 3256{3265, 2019.\n[200] Kerstin Hammernik, Tobias W ur\r, Thomas Pock, and Andreas Maier. A deep learn-\ning architecture for limited-angle computed tomography reconstruction. In Bildver-\narbeitung f ur die Medizin 2017 , pages 92{97. Springer, 2017.\n[201] Erich Kobler, Alexander E\u000fand, Karl Kunisch, and Thomas Pock. Total deep vari-\nation: A stable regularizer for inverse problems. arXiv preprint arXiv:2006.08789 ,\n2020.\n[202] Housen Li, Johannes Schwab, Stephan Antholzer, and Markus Haltmeier. Nett: Solv-\ning inverse problems with deep neural networks. Inverse Problems , 2020.\n[203] Lucas N Egidio, Anders Hansson, and Bo Wahlberg. Learning the step-size policy\nfor the limited-memory broyden-\retcher-goldfarb-shanno algorithm. arXiv preprint\narXiv:2010.01311 , 2020.\n49\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n[204] Jun Shu, Yanwen Zhu, Qian Zhao, Deyu Meng, and Zongben Xu. Meta-lr-schedule-\nnet: Learned lr schedules that scale and generalize. arXiv preprint arXiv:2007.14546 ,\n2020.\n[205] Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. L2-gcn: Layer-wise\nand learned e\u000ecient training of graph convolutional networks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2127{\n2135, 2020.\n[206] Shipeng Wang, Jian Sun, and Zongben Xu. Hyperadam: A learnable task-adaptive\nadam for network training. In Proceedings of the AAAI Conference on Arti\fcial\nIntelligence , volume 33, pages 5297{5304, 2019.\n[207] Xuxi Chen, Wuyang Chen, Tianlong Chen, Ye Yuan, Chen Gong, Kewei Chen, and\nZhangyang Wang. Self-pu: Self boosted and calibrated positive-unlabeled training.\nInternational Conference on Machine Learning (ICML) , 2020.\n[208] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing\nusing generative models. In International Conference on Machine Learning , pages\n537{546, 2017.\n[209] Dave Van Veen, Ajil Jalal, Mahdi Soltanolkotabi, Eric Price, Sriram Vishwanath,\nand Alexandros G Dimakis. Compressed sensing with deep image prior and learned\nregularization. arXiv preprint arXiv:1806.06438 , 2018.\n[210] Hillel Sreter and Raja Giryes. Learned convolutional sparse coding. In 2018 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages\n2191{2195. IEEE, 2018.\n[211] Morteza Mardani, Qingyun Sun, Vardan Papyan, Shreyas Vasanawala, John Pauly,\nand David Donoho. Degrees of freedom analysis of unrolled neural networks. arXiv\npreprint arXiv:1906.03742 , 2019.\n[212] Risheng Liu, Shichao Cheng, Long Ma, Xin Fan, Zhongxuan Luo, et al. A bridg-\ning framework for model optimization and deep propagation. Advances in Neural\nInformation Processing Systems , 31:4318{4327, 2018.\n[213] Yuelong Li, Mohammad To\fghi, Junyi Geng, Vishal Monga, and Yonina C Eldar.\nE\u000ecient and interpretable deep blind image deblurring via algorithm unrolling. IEEE\nTransactions on Computational Imaging , 6:666{681, 2020.\n[214] Morteza Mardani, Qingyun Sun, David Donoho, Vardan Papyan, Hatef Monajemi,\nShreyas Vasanawala, and John Pauly. Neural proximal gradient descent for com-\npressive imaging. Advances in Neural Information Processing Systems , 31:9573{9583,\n2018.\n[215] Xueyang Fu, Zheng-Jun Zha, Feng Wu, Xinghao Ding, and John Paisley. Jpeg ar-\ntifacts reduction via deep convolutional sparse coding. In Proceedings of the IEEE\nInternational Conference on Computer Vision , pages 2501{2510, 2019.\n[216] Hong Wang, Qi Xie, Qian Zhao, and Deyu Meng. A model-driven deep neural net-\nwork for single image rain removal. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 3103{3112, 2020.\n50\n\nLearning to Optimize: A Primer and A Benchmark\n[217] Jonathan I Tamir, X Yu Stella, and Michael Lustig. Unsupervised deep basis pursuit:\nLearning reconstruction without ground-truth data. In Proc. Intl. Soc. Mag. Reson.\nMed, volume 27, page 0660, 2019.\n[218] Gili Dardikman-Yo\u000be and Yonina C Eldar. Learned sparcom: Unfolded deep super-\nresolution microscopy. arXiv preprint arXiv:2004.09270 , 2020.\n[219] Oren Solomon, Regev Cohen, Yi Zhang, Yi Yang, Qiong He, Jianwen Luo, Ruud JG\nvan Sloun, and Yonina C Eldar. Deep unfolded robust pca with application to clutter\nsuppression in ultrasound. IEEE transactions on medical imaging , 39(4):1051{1063,\n2019.\n[220] Haoran Sun, Xiangyi Chen, Qingjiang Shi, Mingyi Hong, Xiao Fu, and Nikos D\nSidiropoulos. Learning to optimize: Training deep neural networks for wireless re-\nsource management. In 2017 IEEE 18th International Workshop on Signal Processing\nAdvances in Wireless Communications (SPAWC) , pages 1{6. IEEE, 2017.\n[221] Arindam Chowdhury, Gunjan Verma, Chirag Rao, Ananthram Swami, and Santiago\nSegarra. Unfolding wmmse using graph neural networks for e\u000ecient power allocation.\narXiv preprint arXiv:2009.10812 , 2020.\n[222] Wenlong Wang, Fangshu Yang, and Jianwei Ma. Velocity model building with a\nmodi\fed fully convolutional network. In SEG Technical Program Expanded Abstracts\n2018, pages 2086{2090. Society of Exploration Geophysicists, 2018.\n[223] Fangshu Yang and Jianwei Ma. Deep-learning inversion: A next-generation seismic\nvelocity model building method. Geophysics , 84(4):R583{R599, 2019.\n[224] Fangshu Yang, Thanh-an Pham, Harshit Gupta, Michael Unser, and Jianwei Ma.\nDeep-learning projector for optical di\u000braction tomography. Optics Express , 28(3):\n3905{3921, 2020.\n[225] Hao Zhang, Xiuyan Yang, and Jianwei Ma. Can learning from natural image denoising\nbe used for seismic data interpolation? Geophysics , 85(4):WA115{WA136, 2020.\n[226] Xi Peng, Shijie Xiao, Jiashi Feng, Wei-Yun Yau, and Zhang Yi. Deep subspace\nclustering with sparsity prior. In IJCAI , pages 1925{1931, 2016.\n[227] Xi Peng, Ivor W Tsang, Joey Tianyi Zhou, and Hongyuan Zhu. k-meansnet: When\nk-means meets di\u000berentiable programming. arXiv preprint arXiv:1808.07292 , 2018.\n[228] C \u0018 a\u0015 gatay I\u0018 s\u0010l, Figen S Oktem, and Aykut Ko\u0018 c. Deep iterative reconstruction for phase\nretrieval. Applied Optics , 58(20):5422{5431, 2019.\n[229] Zhong-Qiu Wang, Jonathan Le Roux, DeLiang Wang, and John R Hershey. End-to-\nend speech separation with unfolded iterative phase reconstruction. arXiv preprint\narXiv:1804.10204 , 2018.\n[230] Suhas Lohit, Dehong Liu, Hassan Mansour, and Petros T Boufounos. Unrolled pro-\njected gradient descent for multi-spectral image fusion. In ICASSP 2019-2019 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages\n7725{7729. IEEE, 2019.\n51\n\n(\u000b-\f) T. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang and W. Yin\n[231] Liang Zhang, Gang Wang, and Georgios B Giannakis. Real-time power system state\nestimation and forecasting via deep unrolled neural networks. IEEE Transactions on\nSignal Processing , 67(15):4069{4077, 2019.\n[232] Harsh Shrivastava, Xinshi Chen, Binghong Chen, Guanghui Lan, Srinivas Aluru, Han\nLiu, and Le Song. Glad: Learning sparse graph recovery. In International Confer-\nence on Learning Representations , 2020. URL https://openreview.net/forum?id=\nBkxpMTEtPB .\n[233] Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approxi-\nmation. In International Conference on Machine Learning , pages 291{301. PMLR,\n2019.\n[234] Kailun Wu, Yiwen Guo, Ziang Li, and Changshui Zhang. Sparse coding with gated\nlearned ista. In International Conference on Learning Representations , 2019.\n[235] Chengzhu Yang, Yuantao Gu, Badong Chen, Hongbing Ma, and Hing Cheung So.\nLearning proximal operator methods for nonconvex sparse recovery with theoretical\nguarantee. IEEE Transactions on Signal Processing , 2020.\n[236] John Zarka, Louis Thiry, Tom\u0013 as Angles, and St\u0013 ephane Mallat. Deep network\nclassi\fcation by scattering and homotopy dictionary learning. arXiv preprint\narXiv:1910.03561 , 2019.\n[237] Yunmei Chen, Hongcheng Liu, Xiaojing Ye, and Qingchao Zhang. Learnable de-\nscent algorithm for nonsmooth nonconvex image reconstruction. arXiv preprint\narXiv:2007.11245 , 2020.\n[238] Risheng Liu, Long Ma, Yiyang Wang, and Lei Zhang. Learning converged propaga-\ntions with deep prior ensemble for image enhancement. IEEE Transactions on Image\nProcessing , 28(3):1528{1543, 2018.\n[239] Luke Metz, Niru Maheswaranathan, C Daniel Freeman, Ben Poole, and Jascha Sohl-\nDickstein. Tasks, stability, architecture, and compute: Training more e\u000bective learned\noptimizers, and using them to train themselves. arXiv preprint arXiv:2009.11243 ,\n2020.\n[240] Arash Behboodi, Holger Rauhut, and Ekkehard Schnoor. Generalization bounds for\ndeep thresholding networks. arXiv preprint arXiv:2010.15658 , 2020.\n[241] Huynh Van Luong, Boris Joukovsky, and Nikos Deligiannis. Interpretable deep re-\ncurrent neural networks via unfolding reweighted `1-`1minimization: Architecture\ndesign and generalization analysis. arXiv preprint arXiv:2003.08334 , 2020.\n[242] Johannes Schwab, Stephan Antholzer, and Markus Haltmeier. Deep null space learn-\ning for inverse problems: convergence analysis and rates. Inverse Problems , 35(2):\n025008, 2019.\n[243] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human\nsegmented natural images and its application to evaluating segmentation algorithms\nand measuring ecological statistics. In Proceedings Eighth IEEE International Con-\nference on Computer Vision. ICCV 2001 , volume 2, pages 416{423. IEEE, 2001.\n52\n\nLearning to Optimize: A Primer and A Benchmark\n[244] Yangyang Xu and Wotao Yin. A block coordinate descent method for regularized\nmulticonvex optimization with applications to nonnegative tensor factorization and\ncompletion. SIAM Journal on imaging sciences , 6(3):1758{1789, 2013.\n[245] Thomas Moreau and Joan Bruna. Understanding Trainable Sparse Coding with Ma-\ntrix Factorization. 2017.\n[246] Yifan Chen, Yuejiao Sun, and Wotao Yin. Run-and-inspect method for nonconvex\noptimization and global optimality bounds for r-local minimizers. Mathematical Pro-\ngramming , 176(1):39{67, 2019.\n[247] Tianjian Meng, Xiaohan Chen, Yifan Jiang, and Zhangyang Wang. A design space\nstudy for lista and beyond. In International Conference on Learning Representations ,\n2021. URL https://openreview.net/forum?id=GMgHyUPrXa .\n53",
  "textLength": 160590
}