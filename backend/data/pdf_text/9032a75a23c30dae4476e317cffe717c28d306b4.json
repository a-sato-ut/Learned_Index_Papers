{
  "paperId": "9032a75a23c30dae4476e317cffe717c28d306b4",
  "title": "Robust Cardinality: a novel approach for cardinality prediction in SQL queries",
  "pdfPath": "9032a75a23c30dae4476e317cffe717c28d306b4.pdf",
  "text": "Journal of the\nBrazilian Computer SocietyPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 \nhttps://doi.org/10.1186/s13173-021-00115-9\nRESEARCH OpenAccess\nRobustCardinality:anovelapproachfor\ncardinalitypredictioninSQLqueries\nFranciscoD.B.S.Praciano*,PauloR.P.Amora,ItaloC.Abreu,FranciscoL.F.PereiraandJavamC.Machado\n*Correspondence:\ndaniel.praciano@lsbd.ufc.br\nLaboratoryofSystemsand\nDatabases,FederalUniversityof\nCeará,Ceará,Av.HumbertoMonte,\n60440-593Fortaleza,BrazilAbstract\nBackground: DatabaseManagementSystems(DBMSs)usedeclarativelanguageto\nexecutequeriestostoreddata.TheDBMSdefineshowdatawillbeprocessedand\nultimatelyretrieved.Therefore,itmustchoosethebestoptionfromthedifferent\npossibilitiesbasedonanestimationprocess.Theoptimizationprocessusesestimated\ncardinalitiestomakeoptimizationdecisions,suchaschoosingpredicateorder.\nMethods: Inthispaper,weproposeRobustCardinality,anapproachtocalculate\ncardinalityestimatesofqueryoperationstoguidetheexecutionengineoftheDBMSs\ntochoosethebestpossibleformoratleastavoidtheworstone.Byusingmachine\nlearning,insteadofthecurrenthistogramheuristics,itispossibletoimprovethese\nestimates;hence,leadingtomoreefficientqueryexecution.\nResults: WeperformexperimentaltestsusingPostgreSQL,comparingbothestimators\nandamoderntechniqueproposedintheliterature.WithRobustCardinality,alower\nestimationerrorofabatchofquerieswasobtainedandPostgreSQLexecutedthese\nqueriesmoreefficientlythanwhenusingthedefaultestimator.Weobserveda3%\nreductioninexecutiontimeafterreducing4timesthequeryestimationerror.\nConclusions: Fromtheresults,itispossibletoconcludethatthisnewapproach\nresultsinimprovementsinqueryprocessinginDBMSs,especiallyinthegenerationof\ncardinalityestimates.\nKeywords: Queryoptimization,Cardinalityestimation,Machinelearning\nIntroduction\nQueryoptimizationisawidelyexploredfieldofstudy.Oneofthemostimpactfultasksin\nthe process of query optimization is cardinality estimation. To select an optimal execu-\ntion plan among several plan alternatives, the query optimizer calculates how much the\nexecution plan will cost. As plans are not immediately executed, the system is unable to\ncalculate their costs in advance. Thus, it has to rely on estimates. If the estimates are off,\ntheconsequencesareslowqueriesandperformancevariation.\nToretrieveestimates,mostcommercialdatabasemanagementsystems(DBMS)relyon\nhistograms [ 1], since they allow for a trade-off in memory space to store the histogram\nand estimation speed, as it is very quick to get a sample from a histogram. However,\n©TheAuthor(s). 2021 OpenAccess ThisarticleislicensedunderaCreativeCommonsAttribution4.0InternationalLicense,\nwhichpermitsuse,sharing,adaptation,distributionandreproductioninanymediumorformat,aslongasyougiveappropriate\ncredittotheoriginalauthor(s)andthesource,providealinktotheCreativeCommonslicence,andindicateifchangeswere\nmade. Theimagesorotherthirdpartymaterialinthisarticleareincludedinthearticle’sCreativeCommonslicence,unless\nindicatedotherwiseinacreditlinetothematerial. Ifmaterialisnotincludedinthearticle’sCreativeCommonslicenceandyour\nintendeduseisnotpermittedbystatutoryregulationorexceedsthepermitteduse,youwillneedtoobtainpermissiondirectly\nfromthecopyrightholder. Toviewacopyofthislicence,visit http://creativecommons.org/licenses/by/4.0/ .\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page2of24\nhistograms are costly to build, they need to be rebuilt for refreshing and do not capture\nsomedatacharacteristics,suchasdatavaluescorrelation[ 2,3].\nAlthough many database systems can be configured to take these characteristics into\naccount, this configuration requires expert knowledge of both the database system and\ntheapplicationdatadomain.Withoutthisexpertknowledge,thisscenarioleadstoerrors\ninestimationandperformancehinderinginsystemswhentheworkloadinvolvescomplex\nqueriesandtablesdohavesomecorrelationbetweenattributes[ 2–5].\nAlso, according to Ioannidis et al. [ 4], cardinality estimation errors can be propagated\ninanexponentialfashion,highlyimpactingthequeryoptimizerdecision.\nPostgreSQL, for example, allows for that approach through the statistics object. It\nreceives a set of attributes for monitoring and, as queries are executed on the table, it\nattempts to discover a possible correlation between the existing values in each attribute,\ninsteadofconsideringthemseparately.Then,the analyzecommandcomputesthestatis-\ntics,throughasampleofthetable.Itshouldbenotedthatthisrequiresexpertknowledge\nof both the DBMS, on how to set up these statistics, and the dataset, to use the best\nattributes on the value set so that the estimations are improved. Without this execution\nofanalyzecommand, the PostgreSQL estimator is based on three principles: uniformity,\nindependence,andprincipleofinclusion[ 2].\nMachinelearningtechniquesarecurrentlybeingexploredinmanystagesoftheDBMS,\nfromparametertuning[ 6–8], to index creation and maintenance [ 9–11] and query opti-\nmization[ 12–14].Theevolvingcharacteristicofmanymachinelearningmodelsmakesa\nstrong case for using these techniques to model problems without a high cost to update\nandobtaingoodresultsasmoreinstancesareobserved.\nByusingmachinelearningmodels,itcanbepossibletoachievelowerestimationerrors\nand better execution times without having all the expert knowledge needed to do so,\ntherefore,improvingthegeneralperformanceoftheDBMS.\nThis work proposes Robust Cardinality, a machine learning backed approach to esti-\nmate query cardinalities with a given confidence, being able to judge whether its esti-\nmations are good enough. This technique is said to be robust since it is possible to\nmanagetheprediction’suncertainty.WeimplementedRobustCardinalityinPostgreSQL\nand obtained improvements in both error estimation and execution time. We also com-\npared estimates with a state-of-the-art machine learning estimator, obtaining a majority\nofbetterresults.Thiswork’scontributionsareasfollows:\nEfficientmachinelearningtechniquetopredictquerycardinality. Usingmachine\nlearningmodelsenablesustoachievebettercardinalityestimationsforSQLqueries,\nresultinginbettercostestimationandbetterqueryplanchoices.Ourapproach\nquicklybuildsmodelswithverylowoverheadonthequeryexecutiontimedifferently\nfromotherapproachesbasedonneuralnetworks.Simultaneously,considering\n90-percentileofestimates’accuracythroughtheQ-errormetric,RobustCardinality\nhas4xlowererrorthanthetraditionaltechniquebasedonhistogramand2xwhen\ncomparedwithamoderntechniquebasedonmachinelearningusingthesame\ndataset.Regardingthequeryexecutiontime,ourapproachcanreducethequery\nruntimebyaround3%incomparisontothedefaultPostgreSQLimplementation.\nEvolvingmodelconstruction. Theprocessofbuildingamodeliscontinuously\nevolvingasdataisupdatedandquerycardinalitychanges.RobustCardinality\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page3of24\naccumulatespreviousknowledgetobuildnewmodels;therefore,itconstantlyadapts\ntoquerydataevolution.Noneofourcompetitorspresentsanevolutionaryapproach\norkeepsthemetadataoutofdateasinmoretraditionalapproaches.\nRobustapproachtoinaccuratepredictions. RobustCardinalitysupportsaconfidence\nintervalonquerycardinalityprediction.Duringestimation,ifthepredictionseemsto\nbeinaccurate,anoptimizedexecuteplanfoundbythetraditionaltechniqueis\nscheduledforexecutiononthequeryengine.Thus,RobustCardinalityismore\nflexiblethanrecentrelatedworksthatarealsomachinelearning-basedtechniques.\nRealDBMSexperimentalresults. Wehavereportedresultsfromexperimentsusinga\nrecentimplementationofthePostgreSQLqueryengine.Althoughwecouldhave\nexperimentedindependentlyfromarelationalDBMSasmostoftheother\napproaches,wewantedtoevaluateRobustCardinalityinanestablishedandmore\nthanapprovedDBMSwitharealSQLqueryengine.Inaddition,wewereableto\nanalyzeourapproachagainsttraditionalhistogramtechniquesandmakeuseofthem\nwheneverourpredictionswereoutsideofaconfidenceinterval.\nThe paper is organized as follows: The “ Related work ” section discusses related works.\nThe “RobustCardinality ”sectionexplainshowRobustCardinalityisdesigned,highlight-\ning how the estimates are generated over time and how it integrates with a DBMS. The\n“Experimental evaluation ” section presents the experimental evaluation, and, finally, we\nconclude in the “ Conclusion and future work ” section presenting our final remarks and\npossiblefutureworks.\nRelatedwork\nCardinalityestimationisoneoftheproblemsofqueryprocessingthatmostgainedatten-\ntioninthedatabasescientificcommunitybecauseofitsimportanceinthewholedatabase\nsystemsstackandperformance,seerecentsurveys,suchas[ 15,16]foranoverview.His-\ntograms[ 1]andsamplingtechniques[ 17]werethefirsttwotechniquesproposedtosolve\nthisproblem.Thelatterwasnotadoptedbythemainsystemsbecauseofitsintrinsichigh\ncost,despiteitsgoodaccuracy.Incontrast,mostrelationaldatabasesystemsimplemented\nestimationtechniquesbasedonhistogramsusingsomepremisestodealwithallkindsof\nqueries. Although its accuracy is good for most cases, there are some cases where this\ntechnique does not fit at all. For example,the histogram is not suitable when there is too\nmuch correlated data, because usually it is uni-dimensional, therefore, does not capture\ncorrelationdatabetweenattributes.\nIn an attempt to overcome the cases where histogram-based techniques have a bad\nperformance,someworksintheliterature[ 18,19]proposetoincorporateafeedbackcar-\ndinality loop, a method that monitors the cardinalities present in the query plans at run\ntimewiththepurposeofobtainingthetruecardinalitiesand,consequently,injectingthem\nin the next queries that involve the cardinalities already acquired. Exploring this valu-\nableknowledgeobtainedthroughthequeriesalreadyexecutedbringstolightthepossible\nerrorsthatoccurredintheprocessofoptimizingthesequeries,enablingtheidentification\nofthepartwhereaninaccuracyofthehistograms’estimatesoccurs.\nLiuetal.[ 20]proposetheuseofmachinelearningtechniquestoaddressthecardinality\nestimation problem. The approach consists of using neural networks to learn selectivity\nfromSQLquerypredicates.Inthisway,theymodeltheproblemasasupervisedlearning\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page4of24\none which replaces the histogram with a neural network. This approach only allows for\nsingletablequeries.Thus,severalpracticalandrealisticqueriesarenotaddressedbyit.\nSince then, many works choose to dive into the subject [ 21]. Dutt et al. [ 22]p r e s e n ta\ncardinality estimator in a similar approach to Liu et al. Instead of only neural networks,\nthey use regression machine learning models, such as ensemble trees. This approach\nmakesallpredicateshaveaselectivityestimate.Todoso,theymodelqueriesinawaythat\nifanattributeisnotpresentinapredicateofthequery,apredicateboundedbyupperand\nlowervaluesiscreatedonlyasafiller,sincethemodelrequiresquerieswithallattributes\naspredicates.Asinthiswork,theexistingtechniqueappliedintheDBMSisnotexcluded,\norchestrating the prediction work with the DBMS. In contrast, our work considers the\njoiningclausesaswellthatarenotaddressedintheirwork.\nKipf et al. propose learned cardinalities [ 23], a technique that uses a deep learning\napproachformedbyaneuralnetworksetnamedMSCN[ 24].Queriesaresplitinto3sets\nthat are input in MSCN, a table set, a join set, and a predicate set. Each of these sets is\nencodedintoone-hotvectors.Afterthesesetsarebuilt,eachofthemisinputtoaneural\nnetwork, where the outputs are averaged through pooling, resulting in a compact repre-\nsentationofthequery.Thisisusedasinputtoafinalneuralnetworkthat,fromthisinput,\ngenerates the query cardinality estimate. An interesting contribution from that work is\na query generator, intended to create a labeled training set to the prediction model. In\nsummary,thealgorithmgeneratesuniformrandomnumberstoselectthetablesandpred-\nicatesinaquery,thenexecutesthequeryagainstthedatabasetoobtainitsrealcardinality,\nthen,bothqueryandcardinalityareaddedtothetrainingset.\nWoltmannetal.[ 25]changethepreviousworkontheneuralnetworks.Insteadofhav-\ning a model that oversees the entire database, it constructs a single neural network for\neach sub-part of the database, handling it as a local approach, not a global one. This\nchanges the query modeling and incurs in an extra decision for the optimizer, which\nmodels should be used to estimate cardinality. Then, several models can be used when\ngeneratingthequerycardinalityestimate.\nNegi et al. [ 26] tackle the cardinality estimation problem from a different perspective.\nInsteadoffocusingonminimizingthegeneralcardinalityestimationerror,ittakesamore\ndirectapproachincheckingwhetherthegeneratedplanhadabetterexecutiontimethan\nthe default through a metric called plan-error . As a consequence, the learned cardinal-\nity estimators are penalized according to execution time rather than estimation error. It\nshows that, although aggregates of estimate errors are higher, execution times are lower,\nbecause the estimator is more accurate in the nodes that impact most in the execution\ntimes.\nWe use learned cardinalities as a baseline in this work. Our query encoding technique\nis also inspired by this work, although with minor differences, for example, we chose to\nhave a single vector instead of three sets, and there is not a sampling related section in\nthevector,becausesamplingisnotused.Inourwork,weuseagradientboostingdecision\ntree (GBDT) model [ 27,28] instead of a neural network, as it has advantages in training\ntimeandawaytoincrementnewinformationtothemodel,providingtheabilitytoupdate\nit periodically, allowing for an evolutionary process. Besides that, we add a measure of\nuncertainty into the processes, so that the machine learning model can feedback how\ncloseitsestimationsaretotherealcardinality.Therefore,thedatabasesystemcanchoose\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page5of24\nbetween using that estimation or to make one using a standard method, histogram, for\nexample.\nThe related works highlight two open lines of research, which are addressed in this\nwork:uncertaintyhandlingintheestimatesandintegrationwitharealsystem.Whilethis\nis mentioned in Dutt et al., the experiments in these related works are focused on the\ncardinality estimates and do not perform end-to-end testing using a full-fledged system.\nAlso, they use predictions as they come, not allowing for expert knowledge to ponder\nwhetherthesepredictionscanbeimproved.\nRobustCardinality\nIn this paper, we propose Robust Cardinality, a cardinality estimation model using\nmachinelearningthatcanbeperiodicallyupdated.Itcanbeusedinanevolvingenviron-\nment,wheredataisstilladded,updated,andread.WeuseGBDTasthemachinelearning\nalgorithm, since it combines a number of weak learners to create a complete and strong\nmodel and allows for controlling the uncertainty of the answer. This machine learning\ntechniquehasbeenwidelyusedinmanytypesoflearningtasksduetoitsaccuracyaswell\nas efficiency. The “ GBDTin cardinality prediction ” section explains how this algorithm\nworksinmoredetailsandhowtoapplyitinourscenario.\nThis machine learning technique is specially adequate for our scenario because the\ncomputational cost and taken time for training and refreshing are low when compared\nwith other strategies, such as neural networks or deep learning models, and stays suffi-\nciently efficient. In this way, we can retrain the model periodically to capture database\nchangeswitheasiness.Thistechniquealsoallowscalculatingtheuncertaintyofthemodel\noutput, which means that we can know how confident is the model in the calculated\nresponse,enablingittooptwhethertouseitornot.\nOur model operates in the query processing and optimization phases. Its architecture\nis shown in Fig. 1. The model communicates with several components of the DBMS. At\nthe workload generator, a randomized workload can be generated for the initial training\ninawaysimilarto[ 23].Itisimportanttohighlightthatanymethodforgeneratingqueries\ncanbeused.\nThismodulecommunicateswiththecatalogtoobtainschemainformationtogenerate\nthe workload. The encoder module processes and transforms queries into input for the\nFig.1RobustCardinality.Proposedarchitecture\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page6of24\nmodel. Then, a decision is made whether the model must be refreshed or not. If so, the\ntraining thread retrieves encoded data and updates the model, else, the query is passed\nontothepredictormodule,whichusesthegeneratedmachinelearningmodeltopredict\nthequerycardinalityestimates.Afterthequeryisprocessedandtherearerealvaluesfor\nthe cardinality, these are stored in the accumulator and will be used once the decision to\nrefreshthemodelispositive.\nInFig.2,theDBMSintegrationisshown.Itshowsthecommunicationwiththecatalog\nand with other DBMS components. The query is preprocessed before it can be analyzed\nintheoptimizationprocess.Duringthequeryoptimization,theDBMSexploresmanyof\nthepossibleplantreesintheplangenerator.Toestimatethecostofeachoneofthem,the\ncardinality estimator will use the model to calculate each plan node’s cardinality, provid-\ning, as input to the model, a representation of these. The plan with the lowest estimated\ncost will be executed and, after execution ends, the model gets query results from the\nexecutorandstoresthemforitsupdatingprocessinthefuture.\nThe technique allows for a non-invasive model integration, where some components’\noutput is used as input for the model, such as retrieving table schemas from the cata-\nlog, reading current estimates from the estimator, and retrieving the actual cardinalities\nafterthequeryisexecuted.Meanwhile,themodelupdatestheestimatesinthecardinality\nestimator using its output, to allow the optimizer to choose the best plan based on more\naccurateestimates.Theremaybetimeswherethemodel’sestimatesarefaroff,forexam-\nple, when there is a sudden change in the workload. We discuss later on how to handle\ninadequateestimates.\nThe technique is designed as plug-and-play, requiring a minimal change in the DBMS\nor none, if there is an interface that allows the model to modify the values of the cardi-\nnalityestimator.ThisalsoallowsittofallbacktotheDBMSnativemethodofestimating\ncardinalities, in case the model is not confident enough in its estimates or more specific\nsituation,suchasselectionsfromabasetable,wherethehistogramcanperformwell.\nFig.2RobustCardinality.IntegrationwiththeDBMSarchitecture\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page7of24\nGBDTincardinalityprediction\nGBDT is an ensemble supervised learning algorithm. In this approach, a set of learners,\nnamelydecisiontreesaretrainedtolearnthequeries’cardinalitiesinorderthateachone\noftheselearnersgeneratesaprediction.Thesepredictionsarecombinedintoasinglefinal\nprediction to obtain the query cardinality’s estimates using some strategy, for instance,\nmeanvalue[ 29,30].Figure3showsthearchitectureofthisalgorithm\nInGBDT,alearnerissaidtobeweakwhenitspredictionsareslightlybetterthanthose\nof a random one. The motivation behind ensemble methods is that the combination of\nthepredictionsofseveralweaklearnersgeneratesarobust,powerful,morerepresentative\nmodel [32]. Lately, this technique has shown good predictive power in different types of\nlearning,suchasregression[ 33],whichisthetypeoftaskRobustCardinalityismodeled\nafter. Although there are many ensemble supervised learning algorithms, such as bag-\nging,boosting,stacking[ 34–37],wechoseGBDT,aboosting-basedalgorithm,duetothe\nfact that this technique allows to use the quantile regression technique [ 38]p e r m i t t i n g\nto manage the uncertainty of the model predictions. This is a strong point of our work\nsincetheDBAwillbeabletochooseaparameterthatwillbeusedtodecideifitusesthe\npredictionsofourmodelorfallsunderadefaultapproach.\nTrainingamodelusingGBDTinRobustCardinalityrequiresatrainingset Xcomposed\nof encoded queries and labels of their respective cardinalities y. Details about the query\nencoding process will be explained in the following subsection. This set will be used to\nspecializethestructureofthelearnersthroughtheboostingtechnique[ 39]inwhicheach\nofthelearnersisobtainedoneaftertheotherwiththemostcurrentlearnerbeingtrained\ntakingintoaccounttheerrorsthatoccurredinthelearningprocessofthepreviousones.\nFormally,thisprocesscanberesumedinthefollowingequation.\nMk(X)=Mk−1(X)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\ncurrentmodel+λγkEk(X)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright\nnewlearner\n=Mk−1(X)+argmin\nγk,Ekn/summationdisplay\ni=1L(yi,Mk−1(xi)+γkEk(xi))/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright\ncostfunction(1)\nTo obtain the model Mkcomposedbythe kalreadytrainedlearners,thecurrentmodel\nMk−1isusedalongsidethenewlearner Ek,parameterizedbytwofactors:learningrate λ\nand contribution factor γk. While λis provided by the user, γkis adjusted by the GBDT\nin order to calculate the influence that the new learner Ekwill have in the final model\nMk. Finally, a differentiable cost function Lis used to find the optimal splits of the new\nlearner’s nodes, i.e., that one that will minimize the overall cost. In this work, we chose\nFig.3AnexampleofGBDT’slearners[ 31].Eachofoneisadecisiontreethatwastrainedtogenerateagood\npredictionillustratedbythebluenodes.Allofthesepredictionsarethentransformedintoasinglefinal\npredictionthroughachosenfunction,commonlymeanorvoting.Hence,thisfinalpredictionwillbeusedas\nestimatesbytheoptimizer\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page8of24\nthe Q-error metric (defined in Eq. 2) as our cost function so that the query cardinality\nestimates’errorsaredecreased.\nIn this work, we used a new implementation of GBDT that incorporates two tech-\nniques, namely, the gradient-based one-side sampling (GOSS) and the exclusive feature\nbundling (EFB), in order to reduce the overhead of training a model when dealing with\nhugedatasets.\nGOSS is a technique that retains instances with large gradients while performing ran-\ndom sampling on instances with small gradients, on the assumption that instances with\nsmallgradientspresentsmalltrainingerror,andthosewithlargegradientspresentworst\nones.\nEFBmergesdatafeaturesthataremutuallyexclusivetoreducetrainingcomplexity.This\nreducesthenumberoffeaturesthatmustbeprocessedbythemodel,reducingthedimen-\nsionalityofdata.Itcandramaticallyreducethecomplexityofthealgorithmwhenworking\nwithhighdimensionaldata.\nEncoding\nTo use GBDT, the SQL query nor the structures such as abstract syntax trees and plan\ntreescanbeconsumedas-is.Therefore,thequeriesmustbeencoded,andthisprocedure\nisshowninFig. 4.Thequeryisencodedinavector,splitintosections,inawaythateach\ntableandpredicateisrepresented.Predicatevaluesmustbenumericandarenormalized\ninmin-maxfashion.Inthefigure,thisisrepresentedbythevector x.Inde x es x[0]tox[2]\nencodethetablesinthedatabasethatarebeingqueried,where1meansthistableisinthe\nquery,and0meansitisnot.Indexes x[3]tox[12]representthenon-joinpredicates,they\naresplitinaone-hotencodingvectorfortheattributes( x[3]tox[8]),aone-hotencoding\nvectorfortheoperator( x[9]tox[11])andanentryforthepredicatevalue( x[12]).Indexes\nx[13]tox[15]are added when there are joins, as a one-hot vector where each entry is a\npossibleattributejoin.Inthisexample,thejoinbetween D.id_dep andE.id_dep.\nTo be used as a label, cardinality goes through two transformations. The first is the\nlogarithmic transformation, which is the application of the logarithmic function in the\ncardinality and a min-max normalization similar to that used for numeric attributes. In\nother words, the model will receive as input the queries’ cardinality normalized by these\ntwo transformations. In this way, it will also produce predictions of normalized cardi-\nFig.4SQLqueryencoded\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page9of24\nnality. To obtain the cardinality estimate, two transformations that invert the operations\nperformed to normalize the cardinality are applied. First, a transformation is applied to\nundo the min-max normalization and, subsequently, the logarithmic transformation is\nundone by applying the exponential function to the value obtained immediately after\nundoingthemin-maxtransformation.Withthat,weobtainthecardinalityestimate.\nTrainingandevolution\nModeltrainingandevolutionareofflineoperationsfromtheDBMSperspectiveandhap-\npen asynchronously. The dataset is composed of the encoded queries and their label,\ni.e., the cardinality. As said above, the original cardinalities are also preprocessed, being\nsubjected to two transformations, in which a logarithmic function is applied and a nor-\nmalizationofthevaluesismade,usingmin-max.Whenapredictionismade,thereverse\noperationsareapplied,toretrievearealvaluefortheestimate.\nTo perform the initial training, a dataset composed of encoded queries and their car-\ndinalities is needed. Therefore, we can either have a set of previously executed queries,\nalongside their real cardinalities or generate a synthetic dataset based on the database\nschemas. The latter can be achieved by an algorithm similar to Kipf et al. [ 23]. However,\ninourwork,wevarythenumberofjoinsfrom0to4,insteadoftheoriginalthatconsiders\nonlyupto2joins.Itisimportanttonotethatthisnumberofjoinsisaparameterthatcan\nbe chosen according to the DBA. Furthermore, by increasing the number of joins in the\nqueries that can be generated, it is possible that a greater variety of queries is obtained,\nconsequently the dimensional space of the queries’ features is better covered. Thus, the\ntrained model is more likely to yield a better generalization for the cardinality estimates.\nThis is one of the possible ways to deal with the well-known problem of the curse of\ndimensionalitythatiscommoninmachinelearning.\nThetrainingalgorithmisshowninAlgorithm1.Insummary,itgeneratesuniformran-\ndomnumberstomakethechoiceofrelationsandtheselectionandjoinpredicatespresent\nin a query, executes this generated query in order to obtain its cardinality and then adds\nit to the training set that is being generated. Finally, it returns this training set Xlabeled\ntobeusedintrainingthemachinelearningmodel.\nThistypeoftrainingallowsforusingthetechniquefromstart,insteadofhavingtowait\nfor queries to execute without our input. This initial training set only has the query final\ncardinality, not the inner operator cardinalities. This observation is important because\nmore joins means more error propagation from one operator node to another, that is, a\ndifferencebetweentherealandtheestimatedcardinalityofanoperatornodeinthequery\ntreemaypropagateexponentiallyandcausemassivedifferencesinestimationsupthetree\n[4].Ourmodelpredictstheoutputofthequery,i.e.,therootoftheplantree.Toavoidonly\nhavingthefinalcardinality,theestimateissplitalongthetree,beingrecursivelycomputed\nfor each child node except the leaves. Each child node corresponds to an intermediate\noperationoftheplan.Toestimatetheleaves’cardinality,wemakeuseoftheDBMSstock\nmethodtoestimatecardinalities,usuallyahistogram.Thistechniqueisveryeffectivefor\nsingle node operators, which is the case of the leaves, and avoids more overhead. In this\nway, we can mitigate estimate errors while they do not outgrow the original values. It is\nalso important to notice that, due to the nature of the technique, neighbor nodes do not\ninfluenceoneanother,sincetheyarenotinputtedconcurrentlyintotheGBDT.\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page10of24\nAlgorithm1: TrainingSetGenerator.Basedon[ 23].\nInput:Relationset R,numberofqueries n\nOutput:LabeledtrainingsetX\n1begin\n2X←∅\n3fori=1→i=ndo\n4 R0←Chooserandomlyarelationthatisreferencedbyothersonset R\n5 Ri←R0\n6 NJ←Chooserandomlyanumberofjoinsinbetween0and4forthenew\nqueryQi\n7 Ji←∅\n8 forj=1→j≤NJdo\n9 Rj←Chooserandomlyarelationthatisreferencedbyatleastone\nrelationonset Ri\n10 Ri←Ri+Rj\n11 Ji←Ji+joinconditionbetweenR jandRi\n12 end\n13 Pi←∅\n14 forj=1→j≤|Ri|do\n15 NP←Chooserandomlyanumberofpredicatesbetween0andthe\nnumberofnon-keyattributesonrelation Ri[j]\n16 fork=1→k≤NPdo\n17 Pk←Chooserandomlyannon-keyattributeof Ri[j],an\nmathematicaloperatorandconstantvaluestomakeaselection\npredicate\n18 Pi←Pi+Pk\n19 end\n20 end\n21 Qi←Makeanewqueryusing Ri,JiandPi\n22 c←Runquery Qiandobtainitscardinality\n23 X←X+(Qi,c)\n24end\n25returnX\n26end\nModel evolution is achieved by using an observation window, in which Wqueries are\nstored,forminga QuerySet ,thenusedtoimprovetheexistingmodel.Figure 5illustrates\nthisprocess.Thefirst kqueriescorrespondtotheinitialtraining,then,whenthewindow\nreaches its limit, a decision to update the model is made. While the updated model is\nbeing generated, the current model stays active, since it is an offline process. When it is\nready,thecurrentmodel M1isreplacedbytheupdatedmodel M2.\nTo generate M2, the queries in the window are used to update M1, in this way, the\nprevious knowledge is not lost and the update process also does not take as long as the\ninitial training. The model also does not grow much in size because, when the update\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page11of24\nFig.5Windowingmodelevolution\nhappens, the previous model is summarized then updated. Therefore, the complete his-\ntory of the model is not stored, instead, a summary of previous evolutions alongside\nthe current update. This process is repeated until a Query Set n is reached. The num-\nber of times ncan be defined as much as necessary to obtain an accurate final model\ntrained.\nUncertainty\nAs shown by Leis et al. [ 2], errors in cardinality estimation impact greatly on the DBMS\nperformance. When using machine learning models to predict cardinality values, it is a\ngood practice to embed a form to evaluate how confident are these predictions, taking\nintoaccountthatitisnotalwayspossibletocoverthesearchspaceinthetrainingset.This\nis known as the curse of dimensionality and it is a well-known problem in the context of\nmachinelearning.Thepredictions’uncertaintymanagementmakesthemachinelearning\nmodels more reliable and allows for identifying the cases where their use can guarantee\nhigheraccuracyestimates.\nIn our approach, we manage uncertainty through quantile regression technique, used\non GBDT altering the loss function. This approach is not directly translatable for other\ntechniques,asstatedbyKipfetal.[ 23].\nUncertainty is a useful metric to avoid errors by the model, in this way, we calculate\nit from a prediction interval, which contains the real value in a known probability, i.e.,\nanerrormargin.Alongsidethecardinalityestimationmodel,weimplementtwoquantile\nprediction models that calculate upper and lower quantiles of this interval. The confi-\ndencelevelisthedifferencebetweenthequantiles,whichmeans,tohavea90%confidence\nlevel,boththe5%and95%quantilesmustbecalculated.Givenanewquery,thecardinal-\nityestimateswillbecalculatedusingthecurrentmodelandcheckiftheyaretrustworthy\nby calculating the quantiles using the others models. This uncertainty can be set by a\nDBA,whichallowsforhighercontrolandusageofexpertknowledge.\nBy this approach, if the prediction is off the interval or if the difference between the\nquantilesrealvalueisveryhigh,thereisaveryhighprobabilityofthepredictionbeingoff.\nWhenthishappens,thecardinalityestimatesgeneratedbythemodelarehighlypenalized,\nallowingfortheDBMStoignorethemandusethedefaultapproach.\nAfter the encoded query has its operator cardinalities estimated, Robust Cardinality\nupdates the values in the estimator, allowing for the optimizer to better select the exe-\ncution plan among the candidates because now the estimates are closer to the actual\ncost.\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page12of24\nExperimentalevaluation\nSetupandbenchmark\nThe experiments were executed with an Intel Xeon E5-2609 v3, with six physical cores,\n1.9 GHz per core, with an Ubuntu OS, with kernel version 4.15.0. Robust Cardinality is\nimplementedinPython3,usinglibraries Scipy[40],scikit-learn [41],andLightGBM [42]\ntoimplementtheGBDTalgorithm.FortheDBMS,weusedPostgreSQL12stable,asitis\nthemostrecentversion.\nWemodifiedPostgreSQLtobeabletoobtaintheexecutionplansandalsotoinjectthe\nnew cardinality estimates into it. To store the queries and executions,a hidden table was\ncreated within PostgreSQL, which stores the query and the actual cardinality. The new\nestimates injection is done by modifying PostgreSQL structures named paths, which are\nsimplerepresentationsofqueryplansthatarenotdirectlyexecutable.Afterevaluatingthe\npaths,theDBMSexpandsthebestofthemintoanactualexecutionplan.Noothermodi-\nficationswere made;so,anyotherPostgreSQLfeaturesthatallowforabettercardinality\nestimationbydefaultwereleftuntouched.\nTo evaluate Robust Cardinality, we used the Internet Movie Database (IMDb) dataset.\nIMDb stores data about movies, actors, directors, and their relationships and comprises\n21 tables. According to Leis et al. [ 43], given the characteristics of this database, such as\ncorrelated attributes, non-uniform distributions, among others, cardinality estimation is\na harder task for DBMSs. Concerning the workloads, we use three: synthetic, scale, and\nJOB-light. The synthetic one is composed by 100,000 synthetic queries that were gener-\nated using the process described in the “ Robust Cardinality ” section, 90% used to train\nthemodel,5%tovalidate,and5%totest.Thequeriesareselect-project-joinqueries.The\nother two workloads were proposed in [ 23]. While the workload scale is also synthetic,\nJOB-lightiscomposedof70queriesfromarealworkloadknownasjoinorderbenchmark\n(JOB)[2] based on the IMDb dataset. As proposed, the scale has 500 queries. However,\nwhen we re-execute these queries to obtain the true cardinalities in our recent dataset\nversion; one of these queries had five relations on joint operations. Moreover, as these\nfive relations have a large number of tuples, it was not possible to re-execute it, as a full\nmemory error has always occurred, even increasing the size of the memory destined for\nPostgreSQL. Thus, we leave this query out, making the scale composed of 499 queries.\nThe JOB-light, on the other hand, was successfully re-executed, allowing the 70 queries\ntobeusedforourevaluation.\nRegularizationandhyper-parametertuning\nWe used 90000 synthetic queries to conduct model training. A well-known problem in\nthe machine learning literature is the problem of over-training, which can lead to over-\nfitting. That is, as we are training our model with a large number of queries, there is a\nneed to verify that the model will not be very well trained to answer only those queries,\nwhileitspowerofgeneralizationwillbelowforqueriesnotseeninthetraining.Toavoid\nthis, we apply four approaches that are used to prevent the generated model from being\nbiased. The first is that we carry out an L2-regularization with a factor of 0.33 that adds\na term in the loss function to be optimized, preventing the generated model from being\nwell adjusted for training queries. Another approach is that we set up a minimum of five\ntraining queries in such a way that each leaf node of the generated decision trees must\nhave. Thus, the decision trees will not be as deep, therefore not adjusting too much to\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page13of24\nTable1Hyper-parametersandtheirvalues\nHyper-parameter Evaluatedvalues\nmax_depth 8;16\nnum_leaves 32;64;128\nn_estimators 1000;5000;10,000\nlearning_rate 0.01;0.1\nthe training data. Finally, the third and fourth approaches are interconnected. Basically,\nthey limit the number of data that will be used at each stage of the training. In the third,\nonly90%oftrainingqueriesarechosenatrandomtobeusedintheconstructionofeach\ndecision tree. Similarly, the fourth strategy limits the training process to use only 90% of\nthefeaturesofeachquerytobuildthemodel.\nFurthermore, we used 5000 synthetic queries to tune the configuration of hyper-\nparameters that LightGBM has for the model. To reduce our search space, four of them\nwere selected. Table 1shows the selected parameters and possible values. With the eval-\nuation of several combinations, the one that presented the lowest error overall was:\n{max_depth:16,num_leaves:64,n_estimators:10000,learning_rate:0.01}.\nErroriscalculatedthroughthe Q-errormetric,whichrepresentshowfarthepredicted\nvaluesarefromthetrueones.Thismetricissuitableforourscenariobecauseweareinter-\nestedintherelativeimpactfactorthatcardinalityerrorscanhaveonqueryprocessing,as\nstatedby[ 5].Formally, Q-errormetricispresentedinthefollowingequation,where cand\nˆcrepresentthetruecardinalityandthepredictedone,respectively:\nQ−error(c,ˆc)=max(c,ˆc)\nmin(c,ˆc)(2)\nFor a Q-error e,w eh a v et h a t e∈[1,+∞], since the values candˆcare always positive.\nNote that e=1o c c u r sw h e n ev e r max(c,ˆc)=min(c,ˆc), which implies c=ˆc,t h a ti s,t h e\nestimateisequaltotherealvalueofthecardinality.Therefore, e=1isoptimal,sincethe\npredictedandtruevaluesareequalsinthiscase,i.e.,thepredictioniscorrect.Insummary,\nthecloserthe eisto1,thecloser candˆcare.Consequently,alowerQ-errorisbetter.\nAccuracyanduncertaintymanagement\nTo verify our first contribution, we tested the Robust Cardinality model against the\nMSCN model proposed by Learned Cardinalities [ 23] and the model based on the his-\ntogramtechniquethatcomeswithPostgreSQL.Aftertrainingbothmodelsandpreparing\nall of PostgreSQL’s statistics, the same synthetic workload was tested against the three\ntechniques.Table 2presentstheaggregatedmetricsforeachone.\nMSCNhasthelowestmeanandthelowestmaximum;however,aswecanseefromthe\nmedianandpercentiles,RobustCardinalityachievesalowererrorinmostofthequeries,\nTable2Aggregated Q-errorofcardinalityestimatesofsyntheticworkloadbyeachmodel(loweris\nbetter)\nTechnique Mean MedianPercentileMaximum\n90 95 99\nPostgreSQL 171.120 1.775 20.352 80.000 964.000 314,187.000\nMSCN 7.659 2.000 9.829 20.618 77.337 6,586.000\nRobustCardinality 21.920 1.243 5 .000 12 .245 50 .690 32,803.500\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page14of24\nas shown by the median and percentiles. For comparison, Robust Cardinality’s 40th per-\ncentile(1.14)islowerthanMSCN’s10th-percentile(1.15),whichmeansthattheQ-error\nof 40% of the estimates of Robust Cardinality is lower than the error of 10% of the esti-\nmates of MSCN. Robust Cardinality’s 90% percentile also matches MSCN 80%, both at\nthe value of 5.00, meaning that we provide a higher number of estimates with errors\nbelow a given value. On the other hand, as can be seen in the table, Robust Cardinality\nis much more sensitive to some queries that can be considered as outliers, given that in\nmost of the workload the model produced good estimates of cardinality. This sensitivity\nis demonstrated in the maximum error values obtained by each technique. Our mean is\nhigher due to a few outliers. This few queries are characterized by selecting over thou-\nsandsoftuplesandreturningonly0or1tuple.Thissensitivityisduetothenormalization\nprocess appliedto queries’cardinality. As stated in the “ Encoding ” section,the exponen-\ntialfunctionisappliedtothemodel’soutputtoobtainthecardinalityestimate.Therefore,\nanysmallerrorthatthemodelmaypresentcanhaveanexponentialimpactonthecardi-\nnality estimate. As we will see below, this type of queriescan be controlled by our model\nthroughuncertaintymanagement.\nTo manage the uncertainty, we have defined a threshold that will quantify the level of\nuncertainty of the queries that will be accepted to use the cardinality estimates of our\nmodel.Thequeriesthatwillberejectedmeanthattheestimatesgivenbyourmodelhave\nahighlevelofuncertainty,andtherefore,itmaybeabetteroptiontoexecutethemusing\na standard technique such as histograms. As explained in the “ Uncertainty ”s e c t i o n ,w e\nhave,foreachquery,alowerandupperboundofthenormalizedcardinalityestimate.Fur-\nthermore,thegreatertheintervaldefinedbythesetwovalues,thegreatertheuncertainty\nlinkedtothecardinalityestimate.Inthisway,thethresholdisusedtolimitthisrangeand,\nconsequently,itlimitstheuncertaintyandalsothelikelyqueries’Q-errorthatwilluseour\ncardinalityestimates.Lastly,expertknowledgecanbeappliedbyregulatingthethreshold.\nFor the evaluation of how the uncertainty management impacts the estimates, we per-\nformed an experiment varying this acceptance threshold. As stated above, the threshold\nis related to the upper and lower bounds of the normalized cardinality estimate, and it is\ncompared to the value obtained by subtracting the lower bound from the upper. These\ntwo values are normalized; therefore, a threshold of 0.0 would accept only queries where\nthe lower bound is exactly equal to the upper one (i.e., there is a certainty that the pre-\ndictednormalizedcardinalityvalueisequaltothetrueone),andathresholdof1.0would\naccept all queries since it is the maximum possible difference. In summary, the lower\nthe threshold, the lower the confidence interval tolerance for cardinality estimates. The\nthreshold was varied from 0.1 to 0.5. On the one hand, a threshold of 0.1 signifies that\nonlyquerieswithhighconfidenceintheirestimatesareaccepted,i.e.,theupperandlower\nboundsdifferenceislessthanorequalto0.1.Itrestrictstheuncertaintyaboutthecardi-\nnalityestimatesfortheacceptedqueries.Ontheotherhand,athresholdof0.5represents\nan acceptable difference of 0.5 between the upper and lower bounds normalized values,\naveryhightoleranceintervaland,consequently,moreuncertaintyisallowed.Theresults\ncanbeseenatTable 3.\nEach column corresponds to a threshold value, i.e., the error tolerance for query esti-\nmates.Secondrowshowsthenumberofacceptedandrejectedqueriesforeachthreshold\nvalueandlastrow,the99thpercentileofQ-error.Increasingthethresholdmeanshaving\nthe model to accept estimations in which it has less confidence. That is why the bigger\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page15of24\nTable3SyntheticqueriesacceptedandrejectedandtheirQ-errorsforeachthresholdvalue\nThreshold 0.1 0.2 0.3 0.4 0.5\n#ofqueriesAcc Rej Acc Rej Acc Rej Acc Rej Acc Rej\n903 4097 2457 2543 3958 1042 4730 270 4956 44\n99%Q-error 3.3 59.0 9.4 90.8 16.8 170.1 43.4 292.5 54.9 157.2\nthe threshold, the more queries are accepted. It is also possible to verify that even with a\nhigh threshold of 0.5, the 99th percentile Q-error for the accepted queries is lower than\ntherejectedones.Itmeansthepredictionsforacceptedqueriesareveryaccurate.\nFigure6shows the amount of error for each query. As corroborated by Table 2,e x c e p t\nfor a few outliers, almost all queries have their errors close to 1. Two of the queries have\nanQ-errorover32000,beingthecauseoftheelevatedmean.Thesequerieswereremoved\nfrom the figure above because of the interval error would be very large; hence, it would\nnot be possible to get a close view of the errors. Furthermore, uncertainty management\nis also shown in this image through the colors of each of the points that represent the\nqueries.Notethatbluetonesrepresentquerieswithamorerestrictedthreshold,allowing\nlittleuncertaintyincardinalityestimates.Thepointswhosecolorislightbluearecloseto\n1,whichshowsthattheacceptedquerieswithathresholdequalto0.1obtainedcardinality\nestimatesquiteaccuratebyourmodel.\nUsing the same model trained by the synthetic workload, we investigate the results\nobtained by the other two workloads. Initially, we analyze the results obtained in the\nexecution of the scale workload. Looking at Table 4, it is possible to see that Robust\nFig.6Q-errorofsyntheticworkload\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page16of24\nTable4Aggregated Q-errorofcardinalityestimatesofscaleworkloadbyeachmodel(lowerisbetter)\nTechnique Mean MedianPercentileMaximum\n90 95 99\nPostgreSQL 530.244 2.682 132.972 393.228 1,641.207 223,040.000\nMSCN 31.005 3.211 41.068 96.326 601.186 2 ,221.609\nRobustCardinality 78.431 1.371 37 .638 91 .794 601.735 25,943.000\nCardinality’s estimates are still lower than that of MSCN in this workload as well, espe-\ncially the median, 90th and 95th percentile. It can be observed that our model showed a\nmedian more than twice as low as that of the MSCN, which implies that our model gen-\neratedmuchmorepreciselycardinalityestimates.Inaddition,the99thpercentileofboth\ntechniques are almost equal, around 601, differing in decimal places. Therefore, it can\nbe concluded that Robust Cardinality is generating more accurate cardinality estimates.\nHowever, it occurs again that the MSCN is less sensitive to those queries whose cardi-\nnality is small. This is seen in the analysis of the maximum Q-error values, which in the\nMSCNisapproximately10timeslessthanthatofRobustCardinality.\nWhenweanalyzeTable 5,weconcludeagainthatthistypeofqueriescanbeavoidedby\napplying the uncertainty management of our model. For example, in the most restrictive\nthreshold (0.1), we have that the number of accepted queries is 246 (almost half of the\nworkloadqueries)whose99thpercentileoftheQ-errorisatmost27.2,beingaverysmall\nestimationerror. In otherwords, outliersthat makeRobustCardinality’s meanandmax-\nimum sensitive are avoided in managing threshold, even when a threshold value of 0.5 is\nused.Notethatatthisthresholdvalue,sixquerieswerestillrejected.\nTo demonstrate the impact of uncertainty management on Robust Cardinality’s esti-\nmates,letuslookatthedistributionofacceptedqueriesforeachthresholdinFig. 7.Again,\nwe had to remove queries outliers that showed a very large Q-error so that the scale of\ntheplotcouldbesmaller.Thus,welimitthequeriesthatpresentedaQ-errorupto2000.\nSee in the figure that the light blue dots represent the queries that were accepted with a\nthreshold of 0.1, the most restrictive one. Therefore, it is clear that this level of threshold\nlimitstheQ-errortobearoundtheidealvalue1.Thatis,thecardinalityestimatesofthese\nqueries are quite accurate in our technique, which guarantees that the queries accepted\nat that threshold value will be estimated with good accuracy. Also note that a threshold\nof0.2stillhasalowlevelofQ-error.Finally,itisclearthatasthethresholdisrelaxed,the\nQ-errorincreaseinacceptedqueriesoccurs.\nLastly, we now analyze the results obtained in the execution of the JOB workload. In\nthe first part of the analysis, we observe the values presented in Table 6. While Post-\ngreSQLshowedabettermean,99thpercentileandmaximumvalue,intheothermetrics\ntheMSCNwonovertheothertwotechniques.Infact,theMSCN,especiallyinthiswork-\nload, presented the best result, since it had a higher mean than PostgreSQL due to its\nmaximum value being much higher than that, in addition to having a greater sensitivity\nTable5ScalequeriesacceptedandrejectedandtheirQ-errorsforeachthresholdvalue\nThreshold 0.1 0.2 0.3 0.4 0.5\n#ofqueriesAcc Rej Acc Rej Acc Rej Acc Rej Acc Rej\n246 253 371 128 437 62 475 24 493 6\n99%Q-error 27.2 976.4 119.9 1676.0 296.4 1278.6 452.7 1664.1 525.7 1832.3\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page17of24\nFig.7Q-errorofscaleworkload\nforqueriesthe99thpercentile.Butinallothermetrics,theMSCNgotbettervalues.Note\nthat the MSCN produced better estimates up to the 95th percentile, but from then on\ntheaccuracystartedtodeteriorate.Thatis,ingeneral,theMSCNpresentedthesmallest\ncardinalityestimationerrors.\nAlthough our technique showed Q-error values greater than the MSCN, the values up\nto the median were not so far apart as our median was approximately 12, less than twice\nas high as that of the MSCN. In the rest, our errors were much higher, even though our\nmaximum error was 6000, while that of the MSCN was approximately 4500. However,\nthese maximum errors are much higher than the error obtained by PostgreSQL, even\nthough this technique has worse results than the MSCN overall. So, at first glance, it\ncanbeconcludedthatourtechniquebehavedworsethantheothertwointhisworkload.\nNevertheless,wenowmoveontothesecondpartofouranalysis.\nIn the second part of the analysis, we include the management of uncertainty, this is\nwhereourtechniqueshowsitsstrengthinrelationtotheothers.InTable 7,wepresentthe\nresultsofthequeriesthatwereacceptedorrejectedaccordingtoanacceptancethreshold.\nIn particular, we note that 16 queries were accepted with a threshold equal to 0.1, with\nTable6Aggregated Q-errorofcardinalityestimatesofJOBworkloadbyeachmodel(lowerisbetter)\nTechnique Mean MedianPercentileMaximum\n90 95 99\nPostgreSQL 34.411 9.841 102.475 127.213 255.637 280 .122\nMSCN 86.983 6.622 85 .548 124 .868 1,572.327 4,506.700\nRobustCardinality 554.055 12.368 1,859.547 2,874.295 4,712.486 6,004.769\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page18of24\nTable7JOBqueriesacceptedandrejectedandtheirQ-errorsforeachthresholdvalue\nThreshold 0.1 0.2 0.3 0.4 0.5\n#ofqueriesAcc Rej Acc Rej Acc Rej Acc Rej Acc Rej\n16 54 24 46 39 31 65 5 68 2\n99%Q-error 12.7 5012.2 523.3 5162.0 475.6 5442.9 4806.1 2194.4 4749.9 2260.8\n99th percentile of the Q-error of these queries being 12.7, which is well below the values\npresented in the previous table of all three techniques, which shows that at this level of\nlow uncertainty it is possible for our technique to present very accurate estimates. Also,\nnote that the threshold with a value of 0.2 has a good value for the 99th percentile of\nthe Q-error, in addition to allowing a greater number of queries to be accepted, in this\ncase 24 queries. Therefore, even though our model presented less accurate estimates in\ngeneralforthisworkload,asseenearlier,whenconsideringuncertaintymanagement,itis\npossibletoobtaingoodcardinalityestimatesforthisworkloadaswell.\nTo finish our analysis, Fig. 8shows how the queries were distributed according to the\nlevelofaccepteduncertaintydefinedthroughthethreshold.Pleasenotethatthelightblue\ndots are very close to the value 1, which shows that these queries have good cardinality\nestimates.Similarly,pointswhosethresholdisequalto0.2alsohavegoodestimates.\nFinally,animportantpointtonoteisthefactthattheoutlierquerythatpresentedaQ-\nerror of approximately 6000 was accepted with a threshold of 0.5, but two other queries,\nrepresented by red dots, with much smaller Q-errors were not accepted with a thresh-\nold of at most 0.5. This situation is possible to occur considering that, as stated in the\n“Uncertainty ” section, the DBA must choose what level of confidence he wants for the\nFig.8Q-errorofJOBworkload\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page19of24\nuncertainty of the queries to be evaluated. In our case, the confidence level chosen was\n90%,whichmeansthatin90%ofthecasesouruncertaintyassessmentwillbecarriedout\ncorrectly.\nIn summary, while MSCN may constantly be off by an amount of error, Robust Car-\ndinality’s error is lower, but when an estimation error does happen, it may be very high.\nNevertheless, this is mitigated by the configurable confidence level. PostgreSQL’s his-\ntogram results were not commented with details because they happen to have the worst\noverallthanthetwomachinelearningapproaches.ItconfirmsthattheRobustCardinality\nmodelisefficientinitspredictions.\nQueryperformance\nAfter evaluating the quality of the cardinality estimates generated by our technique, we\nwillnowbeinterestedinmakingananalysisoftheimpactthatthegainbroughtbyRobust\nCardinality in the estimation process can have on the execution of the queries itself, that\nis, how the execution time of the queries will be impacted. For the overhead incurred by\nadding Robust Cardinality over PostgreSQL, 5000 synthetic queries were executed over\na clean install of PostgreSQL and one with Robust Cardinality. Model evolution was not\nconsidered in this experiment. First, let us look at the accumulated execution times, in\nminutes, that are shown in Table 8in order to assess the impact during the execution of\nthe queries. For that, we present the values for when 1250, 2500, 3750, and 5000 queries\nhavealreadybeenperformed.\nThistableshowsthat,evenwiththeoverheadimposedbyRobustCardinality,execution\ntimesarelowerthanwithstockPostgreSQL.Todetailthis,Fig. 9showsthedistributionof\nqueriesaccordingtotheirexecutiontime(ins)inboththecleanPostgreSQLandRobust\nCardinalityimplementedinit.\nFigure9compares distribution of query execution time using PostgreSQL histogram\ntechniqueandRobustCardinality.Itishardtoappointanyimpactoftheenhancementof\ncardinalityestimationusingRobustCardinalityintheexecutiontimeofqueriesthathave\na shorter execution time than 200 s. Nevertheless, it is possible to observe that queries\nthat have execution time greater than this value were performed considerably better on\nRobustCardinality.Inparticular,notethatthefrequencyofqueriesclosetothreehundred\nseconds has been reduced by approximately half when performed using our estimation\ntechnique. Hence, we conclude that our technique has a greater impact on queries that\nhavealongexecutiontime.\nAnother way to visualize that the improvement brought by Robust Cardinality is in\nthosequerieswithalongexecutiontime,observeFig. 10whichpresentsthefollowingval-\nues:themean,standarddeviationand90thpercentileoftheexecutiontimeofthequeries\nusing both solutions. PostgreSQL histogram and Robust Cardinality show similar mean\nand standard deviation metrics, but Robust Cardinality presents lower 90% percentile. It\nTable8Accumulatedexecutiontimeinminutesfor1250,2500,3750,and5000queriesover\nPostgreSQLandRobustCardinality\nTechnique ≤1250 ≤2500 ≤3750 ≤5000\nPostgreSQL 285.149 612.498 906.343 1,210.445\nRobustCardinality 277.094 598.768 886.042 1,182.964\nDifference 8.055 13.730 20.301 27.481\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page20of24\nFig.9Distributionofexecutiontime(insec)ofsyntheticworkload\nimplies that using Robust Cardinality in query processing helps to choose faster query\nplans for slower queries. To demonstrate this, the selected execution plans for a given\nquery are shown in Fig. 11, with left being the plan selected using histogram estimates\nandrighttheplanselectedusingRobustCardinalityestimates.\nTheintermediatetables’sizeismuchsmallerintheRobustCardinalityplan.Thehigh-\nestjoininthetreeisjoining42,965,075tupleswith104,281tuplesonintermediatetables\nin the plan generated by PostgreSQL, while the table sizes in Robust Cardinality are\n104281and17.Byusingcardinalitiesclosertotherealvalue,aplanusingindexscananda\nnestedloopjoinwasselectedoveraplanthatdidhashjoinsandasequentialscan,because\nthe original estimates misled the optimizer to believe that the hash join’s cost would be\nenoughtomitigatethesequentialscanhighcost.\nModelevolution\nTo observe how the model evolves, we performed an experiment in which the model\nis gradually trained and the Q-error metric is evaluated. This experiment shows how\nthe model accumulates experience over time, not wasting previous knowledge of the\nworkload.Figure 12showshowtheQ-errordecreasesgivenafewroundsoftraining.\nTheQ-errorhighlydecreasesfromthefirstiteration.Thefactthatnoincreaseshappen\nafterward, together with the rapid decrease in the early iterations shows that the model\nretains the previous knowledge and enhances it with newly observed samples, achieving\nabehaviorofevolution.Moreover,asitispossibletomanageuncertainty,thenthemodel\ncan now be used to generate cardinality estimates throughout the training process for\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page21of24\nFig.10Mean,standarddeviation,and90°percentileofexecutiontime(insec)ofsyntheticworkload\nthose queries that have low uncertainty at that time. Thus, the model is robust and can\nlearnandevolve,rejectingestimatesinwhichitisnotconfidentupon.\nGeneraldiscussion\nTakeaways\nOurresultssuggestthattheincorporationofmachinelearningmodelscanbringbenefits\nto the processing of queries in DBMSs. First, this type of models is able to generate good\nFig.11Executiontree:PostgreSQL(left) ×RobustCardinality(right)\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page22of24\nFig.12EvolutionofQ-error\ncardinalityestimates.Second,theuncertaintyabouttheestimatesisaveryusefulmetric,\nand its management may help the DBMS to avoid using poor cardinality estimates in\nqueryprocessing.Inourcase,itguaranteesthattheDBMSwillrarelyuseestimatesworse\nthan the standard method. Third, it is possible to successfully merge a standard strategy,\nproved by time and fruit of years of study and development, with a new solution with\ngreat potential that is based on machine learning. Finally, we demonstrated with Robust\nCardinality that it is possible to build a modular solution that requires little preparation\noftheDBMS.\nLimitations\nAs we see in the experimental evaluation presented above, although Robust Cardinality\nmay come with significant improvements, there is still much work to improve it. There\nare some limitations to this approach that must be highlighted. This approach does not\nhandlechangesinthetables’schema,anditiseasytounderstandwhy.Therepresentation\nofqueriesinvectors,whichisessentialtofeedtheGBDTmodel,isboundtotheschema.\nForexample,thefirstsectionofthevectorrepresentingthequery’stablesdependsonthe\nnumberoftables.Besidesthat,theencodingdoesnotconsidermanypossibleoperations\ninSQL,likewiseordering,alignedqueries,partialjoins,groupby,anddistinct.\nFurthermore,wecouldnottestthisapproachoveranextendedperiodoftime.Thesuc-\ncessive increment of new decision trees in the GBDT model may generate unpredictable\nresults.Inthescopeofthiswork,wedonotimplementaforgettingmechanism.\nConclusionandfuturework\nThis paper presented Robust Cardinality, a plug-and-play cardinality estimation tech-\nnique. We have shown that our approach is efficient to estimate query cardinality in the\noptimization phase query processing. Moreover, it builds machine learning models with\nvery low overhead on the query execution time. Robust Cardinality allows for model\nevolution accumulating previous knowledge to build new models. Our approach also\nmanages inaccurate predictions by supporting a confidence interval on query cardinal-\nity predictions. Whenever cardinality estimation is outside this interval, a conservative\ndecision is made for the DBMS to execute the best query plan found by its regular\noptimizer.\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page23of24\nWe have conducted experiments using PostgreSQL to support our contributions.\nExperimentalresultshaveshownthatourapproachcangenerate4xlowererrorinquery\ncardinality estimates and; therefore, it achieves better query runtime performance in\napproximately3%intheoverallexecutiontime.Hence,RobustCardinalitycanbeimple-\nmented in real-world systems with little overhead and can learn the data’s intrinsic\ncharacteristicsthroughthequeries,asourresultsshow.\nIn the future, other types of queries can be investigated, such as grouping, ordering,\nand nested queries. So far, no works have considered these types of operators. Also,\nthe technique only allows numeric predicates. Therefore, a new encoding technique can\nbe proposed to use more complex predicates, such as strings and operators. Finally, an\ninvestigation into the possibility of performing a deeper integration of machine learning\ntechniqueswiththeinternalDBMSqueryprocessingarchitectureisanexcitingresearch\nsubject.\nAbbreviations\nDBMS:Databasemanagementsystem.GBDT:Gradientboostingdecisiontree.IMDb:InternetMovieDatabase.MSCN:\nMulti-setconvolutionalnetwork.LightGBM:Lightgradientboostingmachine.LSBD:Laboratoryofsystemsand\ndatabases.UFC:FederalUniversityofCeará\nAcknowledgements\nTheauthorswouldliketothankLaboratoryofSystemsandDatabases(LSBD),FederalUniversityofCeará(UFC),andthe\nfundingagency.\nAuthors’contributions\nThisworkwasmainlydonebyFDBSPwhilehewasagraduatestudentsupervisedbyJCM.Botharethemainauthorsof\nthisresearch.ICAandFLFPhelpedtheminimplementationofRobustCardinality.PRPAcontributedonanalyzingthe\nexperimentalresultsandwritingthemanuscript.Allauthorsreadandapprovedthefinalmanuscript.\nFunding\nThisworkwaspartlyfundedbytheCoordenaçãodeAperfeiçoamentodePessoaldeNívelSuperior—Brasil(CAPES)—\ngrant1782887.\nAvailabilityofdataandmaterials\nIMDBusedduringthecurrentstudycanbeobtainedinthefollowinglink: https://imdbpy.readthedocs.io/en/latest/ .The\nqueriesusedand/oranalyzedduringthecurrentstudyareavailablefromthecorrespondingauthoronreasonable\nrequest.\nDeclarations\nCompetinginterests\nTheauthorsdeclarethattheyhavenocompetinginterests.\nReceived:26August2020 Accepted:30July2021\nReferences\n1. KooiRP(1980)Theoptimizationofqueriesinrelationaldatabases.PHDThesis.CaseWesternReserveUniversity\n2. LeisV,GubichevA,MirchevA,BonczPA,KemperA,NeumannT(2015)Howgoodarequeryoptimizers,really?Proc\nVLDBEndowment9(3):204–215\n3. LeisV,RadkeB,GubichevA,MirchevA,BonczPA,KemperA,NeumannT(2018)Queryoptimizationthroughthe\nlookingglass,andwhatwefoundrunningthejoinorderbenchmark.VLDBJ27(5):643–668\n4. IoannidisYE,ChristodoulakisS(1991)Onthepropagationoferrorsinthesizeofjoinresults.In:Proceedingsofthe\n1991ACMSIGMODInternationalConferenceonManagementofData.pp268–277. https://doi.org/10.1145/115790.\n115835\n5. MoerkotteG,NeumannT,SteidlG(2009)Preventingbadplansbyboundingtheimpactofcardinalityestimation\nerrors.ProcVLDBEndowment2(1):982–993\n6. AkenDV,PavloA,GordonGJ,ZhangB(2017)Automaticdatabasemanagementsystemtuningthroughlarge-scale\nmachinelearning.In:Proceedingsofthe2017ACMInternationalConferenceonManagementofData.pp1009–1024\n7. PavloA,AnguloG,ArulrajJ,LinH,LinJ,MaL,MenonP,MowryTC,PerronM,QuahI,SanturkarS,TomasicA,ToorS,\nAkenDV,WangZ,WuY,XianR,ZhangT(2017)Self-drivingdatabasemanagementsystems.In:8thBiennial\nConferenceonInnovativeDataSystemsResearch\n8. LimaMIV,deFariasVAE,PracianoFDBS,MachadoJC(2018)Workload-awareparameterselectionandperformance\npredictionforin-memorydatabases.In:SBBD.pp169–180\n\nPraciano etal. JournaloftheBrazilianComputerSociety           (2021) 27:11 Page24of24\n9. GraefeG,KunoHA(2010)Self-selecting,self-tuning,incrementallyoptimizedindexes.In:ManolescuI,Spaccapietra\nS,TeubnerJ,KitsuregawaM,LégerA,NaumannF,AilamakiA,ÖzcanF(eds).Proceedingsofthe13thInternational\nConferenceonExtendingDatabaseTechnologyVol.426.pp371–381\n10. TeixeiraEM,AmoraPRP,MachadoJC(2018)MetisIDX-fromadaptivetopredictivedataindexing.In:EDBT.\npp485–488\n11. KraskaT,BeutelA,ChiEH,DeanJ,PolyzotisN(2018)Thecaseforlearnedindexstructures.In:Proceedingsofthe\n2018InternationalConferenceonManagementofData.pp489–504\n12. KrishnanS,YangZ,GoldbergK,HellersteinJM,StoicaI(2018)Learningtooptimizejoinquerieswithdeep\nreinforcementlearning.CoRR. https://doi.org/abs/1808.03196\n13. MarcusR,PapaemmanouilO(2019)Towardsahands-freequeryoptimizerthroughdeeplearning.In:9thBiennial\nConferenceonInnovativeDataSystemsResearch\n14. OrtizJ,BalazinskaM,GehrkeJ,KeerthiSS(2018)Learningstaterepresentationsforqueryoptimizationwithdeep\nreinforcementlearning\n15. HarmouchH,NaumannF(2017)Cardinalityestimation:Anexperimentalsurvey.ProcVLDBEndowment\n11(4):499–512. https://doi.org/10.1145/3186728.3164145\n16. YinS,HameurlainA,MorvanF(2015)Robustqueryoptimizationmethodswithrespecttoestimationerrors:A\nsurvey.ACMSigmodRec44(3):25–36. https://doi.org/10.1145/2854006.2854012\n17. LiptonRJ,NaughtonJF,SchneiderDA(1990)Practicalselectivityestimationthroughadaptivesampling.In:\nProceedingsofthe1990ACMSIGMODInternationalConferenceonManagementofData.pp1–11\n18. StillgerM,LohmanGM,MarklV,KandilM(2001)LEO-db2’slearningoptimizer.In:Proceedingsof27thInternational\nConferenceonVeryLargeDataBases.pp19–28\n19. ChaudhuriS,NarasayyaVR,RamamurthyR(2008)Apay-as-you-goframeworkforqueryexecutionfeedback.Proc\nVLDBEndowment1(1):1141–1152\n20. LiuH,XuM,YuZ,CorvinelliV,ZuzarteC(2015)Cardinalityestimationusingneuralnetworks.In:Proceedingsof25th\nAnnualInternationalConferenceonComputerScienceandSoftwareEngineering.pp53–59\n21. ZhouX,ChaiC,LiG,SUNJ(2020)Databasemeetsartificialintelligence:asurvey.IEEETransactionsonKnowledge\nandDataEngineering:1–1. https://doi.org/10.1109/TKDE.2020.2994641\n22. DuttA,WangC,NaziA,KandulaS,NarasayyaVR,ChaudhuriS(2019)Selectivityestimationforrangepredicates\nusinglightweightmodels.ProcVLDBEndowment12(9):1044–1057\n23. KipfA,KipfT,RadkeB,LeisV,BonczPA,KemperA(2019)Learnedcardinalities:estimatingcorrelatedjoinswithdeep\nlearning.In:9thBiennialConferenceonInnovativeDataSystemsResearch\n24. ZaheerM,KotturS,RavanbakhshS,PoczosB,SalakhutdinovRR,SmolaAJ(2017)Deepsets.In:Advancesinneural\ninformationprocessingsystems.pp3391–3401\n25. WoltmannL,HartmannC,ThieleM,HabichD,LehnerW(2019)Cardinalityestimationwithlocaldeeplearning\nmodels.In:ProceedingsoftheSecondInternationalWorkshoponExploitingArtificialIntelligenceTechniquesfor\nDataManagement.pp1–8\n26. NegiP,MarcusR,MaoH,TatbulN,KraskaT,AlizadehM(2020)Cost-guidedcardinalityestimation:focuswhereit\nmatters.In:2020IEEE36thInternationalConferenceonDataEngineeringWorkshops(ICDEW).pp154–157\n27. BreimanL,FriedmanJ,OlshenR,StoneC(1984)Classificationandregressiontrees,wadsworthstatistics.Probability\nSeries,Belmont,California:Wadsworth\n28. FriedmanJH(2001)Greedyfunctionapproximation:agradientboostingmachine.AnnStat:1189–1232\n29. SollichP,KroghA(1996)Learningwithensembles:howoverfittingcanbeuseful.In:AdvancesinNeuralInformation\nProcessingSystems.pp190–196\n30. MarquésAI,GarcíaV,SánchezJS(2012)Exploringthebehaviourofbaseclassifiersincreditscoringensembles.\nExpertSystAppl39(11):10244–10250\n31. RogozhnikovA(2016)Gradientboostingexplained[demonstration]. http://arogozhnikov.github.io/2016/06/24/\ngradient_boosting_explained.html .Accessin01-15-2020\n32. JansenS(2018)Hands-OnMachineLearningforAlgorithmicTrading:Designandimplementinvestmentstrategies\nbasedonsmartalgorithmsthatlearnfromdatausingPython.PacktPublishingLtd\n33. HashemS,SchmeiserB(1995)Improvingmodelaccuracyusingoptimallinearcombinationsoftrainedneural\nnetworks.IEEETransNeuralNetw6(3):792–794\n34. BreimanL(1996)Baggingpredictors.MachLearn24(2):123–140\n35. SchapireRE,SingerY(1999)Improvedboostingalgorithmsusingconfidence-ratedpredictions.MachLearn\n37(3):297–336\n36. BauerE,KohaviR(1999)Anempiricalcomparisonofvotingclassificationalgorithms:bagging,boosting,and\nvariants.MachLearn36(1-2):105–139\n37. KazienkoP,LughoferE,TrawinskiB(2013)Hybridandensemblemethodsinmachinelearning.JUnivComputSci\n19(4):457–461\n38. MeinshausenN(2006)Quantileregressionforests.JMachLearnRes7:983–999\n39. FreundY,SchapireRE,etal(1996)Experimentswithanewboostingalgorithm.In:IcmlVol.96.pp148–156.Citeseer\n40. JonesE,OliphantT,PetersonP,etal(2001)SciPy:opensourcescientifictoolsforPython. http://www.scipy.org/\n41. PedregosaF,VaroquauxG,GramfortA,MichelV,ThirionB,GriselO,BlondelM,PrettenhoferP,WeissR,DubourgV,\nVanderplasJ,PassosA,CournapeauD,BrucherM,PerrotM,DuchesnayE(2011)Scikit-learn:machinelearningin\nPython.JMachLearnRes12:2825–2830\n42. KeG,MengQ,FinleyT,WangT,ChenW,MaW,YeQ,LiuT-Y(2017)Lightgbm:ahighlyefficientgradientboosting\ndecisiontree.AdvNeuralInformProcessSyst30:3146–3154\n43. LeisV,RadkeB,GubichevA,KemperA,NeumannT(2017)Cardinalityestimationdoneright:index-basedjoin\nsampling.In:8thBiennialConferenceonInnovativeDataSystemsResearch\nPublisher’sNote\nSpringerNatureremainsneutralwithregardtojurisdictionalclaimsinpublishedmapsandinstitutionalaffiliations.",
  "textLength": 66913
}