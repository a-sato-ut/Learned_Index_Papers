{
  "paperId": "8d246a3a0621844cf603e54627d5183df97efc18",
  "title": "Monotonic Cardinality Estimation of Similarity Selection: A Deep Learning Approach",
  "pdfPath": "8d246a3a0621844cf603e54627d5183df97efc18.pdf",
  "text": "Monotonic Cardinality Estimation of Similarity\nSelection: A Deep Learning Approach\nYaoshu Wang\nShenzhen Institute of\nComputing Sciences,\nShenzhen University\nyaoshuw@sics.ac.cnChuan Xiao\nOsaka University\n& Nagoya University\nchuanx@ist.osaka-\nu.ac.jpJianbin Qinâˆ—\nShenzhen Institute of\nComputing Sciences,\nShenzhen University\njqin@sics.ac.cnXin Cao\nThe University of New\nSouth Wales\nxin.cao@unsw.edu.au\nYifang Sun\nThe University of New\nSouth Wales\nyifangs@cse.unsw.edu.auWei Wang\nThe University of New\nSouth Wales\nweiw@cse.unsw.edu.auMakoto Onizuka\nOsaka University\nonizuka@ist.osaka-\nu.ac.jp\nABSTRACT\nDue to the outstanding capability of capturing underlying\ndata distributions, deep learning techniques have been re-\ncently utilized for a series of traditional database problems.\nIn this paper, we investigate the possibilities of utilizing deep\nlearning for cardinality estimation of similarity selection. An-\nswering this problem accurately and efficiently is essential\nto many data management applications, especially for query\noptimization. Moreover, in some applications the estimated\ncardinality is supposed to be consistent and interpretable.\nHence a monotonic estimation w.r.t. the query threshold is\npreferred. We propose a novel and generic method that can be\napplied to any data type and distance function. Our method\nconsists of a feature extraction model and a regression model.\nThe feature extraction model transforms original data and\nthreshold to a Hamming space, in which a deep learning-\nbased regression model is utilized to exploit the incremental\nproperty of cardinality w.r.t. the threshold for both accuracy\nand monotonicity. We develop a training strategy tailored to\nour model as well as techniques for fast estimation. We also\ndiscuss how to handle updates. We demonstrate the accuracy\nand the efficiency of our method through experiments, and\nshow how it improves the performance of a query optimizer.\nCCS CONCEPTS\nâ€¢Information systems â†’Query optimization ;Entity res-\nolution ;â€¢Computing methodologies â†’Neural networks .\nKEYWORDS\ncardinality estimation; similarity selection; machine learning\nfor data management\nâˆ—Corresponding author.1 INTRODUCTION\nDeep learning has been recently utilized to deal with tradi-\ntional database problems, such as indexing [ 43], query exe-\ncution [ 23,42,61,70], and database tuning [ 81]. Compared\nto traditional database methods and non-deep-learning mod-\nels (logistic regression, random forest, etc.), deep learning\nexhibits outstanding capability of reflecting the underlying\npatterns and correlations of data as well as exceptions and out-\nliers that capture the extreme anomalies of data instances [ 42].\nIn this paper, we explore in the direction of applying deep\nlearning techniques for a data management problem â€“ car-\ndinality estimation of similarity selection, i.e., given a set of\nrecordsD, a query record ğ‘¥, a distance function and a thresh-\noldğœƒ, to estimate the number of records in Dwhose distances\ntoğ‘¥are no greater than ğœƒ. It is an essential procedure in many\ndata management tasks, such as search and retrieval, data\nintegration, data exploration, and query optimization. For ex-\nample: (1) In image retrieval, images are converted to binary\nvectors (e.g., by a HashNet [ 15]), and then the vectors whose\nHamming distances to the query are within a threshold of\n16 are identified as candidates [ 82] for further image-level\nverification (e.g, by a CNN). Since the image-level verification\nis costly, estimating the cardinalities of similarity selection\nyields the number of candidates, and thus helps estimate the\noverall running time in an end-to-end system to create a ser-\nvice level agreement. (2) In query optimization, estimating\ncardinalities for similarity selection benefits the computa-\ntion of operation costs and the choice of execution orders of\nquery plans that involve multiple similarity predicates; e.g.,\nhands-off entity matching systems [ 20,28] extract paths from\nrandom forests and take each path (a conjunction of similarity\npredicates over multiple attributes) as a blocking rule. Such\nquery was also studied in [49] for sets and strings.arXiv:2002.06442v4  [cs.DB]  24 Sep 2021\n\n0 4 8 12 16\nDistance100101102103Cardinality\nQuery 1\nQuery 2Query 3\nQuery 4Query 5(a) Cardinality v.s. threshold.\n100101102103\nCardinality10âˆ’410âˆ’310âˆ’210âˆ’1100Percentage of Queries\nThreshold = 4\nThreshold = 8Threshold = 12\nThreshold = 16 (b) Percentage of queries v.s. cardinality.\nFigure 1: Cardinality distribution on ImageNet.\nThe reason why deep learning approaches may outperform\nother options for cardinality estimation of similarity selec-\ntion can be seen from the following example: (1) Figure 1(a)\nshows the cardinalities of five randomly chosen queries on the\nImageNet dataset [ 1] by varying Hamming distance thresh-\nold. The cardinalities keep unchanged at some thresholds\nbut surge at others. (2) Figure 1(b) shows the percentage of\nqueries (out of 30,000) for each cardinality value, under Ham-\nming distance thresholds 4, 8, 12, and 16. The cardinalities are\nsmall or moderate for most queries, yet exceptionally large\nfor long-tail queries (on the right side of the figure). Both\nfacts cause considerable difficulties for traditional database\nmethods which require large samples to achieve good accu-\nracy and traditional learning methods which are incapable to\nlearn such complex underlying distributions. In contrast, deep\nlearning is a good candidate to capture such data patterns and\ngeneralizes well on queries that are not covered by training\ndata, thereby delivering better accuracy. Another reason for\nchoosing deep learning is that the training data â€“ though large\ntraining sets are usually needed for deep learning â€“ are easily\nacquired by running similarity selection algorithms (without\nproducing label noise when exact algorithms are used).\nIn addition to accuracy , there are several other technical\nissues for cardinality estimation of similarity selection: (1) A\ngood estimation is supposed to be fast. (2) A generic method\nthat applies to a variety of data types and distance functions\nis preferred. (3) Users may want the estimated cardinality\nto be consistent and interpretable in applications like data\nexploration. Since the actual cardinality is monotonically\nincreasing with the threshold, when a greater threshold is\ngiven, a larger or equal number of results is preferable, so the\nuser is able to interpret the cardinality for better analysis.\nTo cope with these technical issues, we propose a novel\nmethod that separates data modelling and cardinality estima-\ntion into two components:\nâ€¢Afeature extraction component transforms original data\nand thresholds to a Hamming space such that the semantics\nof the input distance function is exactly or approximatelycaptured by Hamming distance. As such, our method be-\ncomes generic and applies to any data type and distance.\nâ€¢Aregression component models the estimation as a regres-\nsion problem and estimates the cardinality on the trans-\nformed vectors and threshold using deep learning.\nTo achieve good accuracy of regression, rather than feeding\na deep neural network with training data in a straightforward\nmanner, we devise a novel approach based on incremental\nprediction to exploit the incremental property of cardinality;\ni.e., when the threshold is increasing, the increment of cardi-\nnality is only caused by the records in the increased range of\ndistance. Since our feature extraction maps original distances\nto discrete distance values, we can use multiple regressors,\neach dealing with one distance value, and then sum up the\nindividual results to get the total cardinality. In doing so, we\nare able to learn the cardinality distribution for each distance\nvalue, so the overall estimation becomes more accurate . An-\nother benefit of incremental prediction is that it guarantees\nthemonotonicity w.r.t. the threshold, and thus yields more\ninterpretability of the estimated results. To estimate the car-\ndinality of each distance value, we utilize an encoder-decoder\nmodel through careful neural network design: (1) To cope\nwith the sparsity in Hamming space, as output by the feature\nextraction, we employ a variational auto-encoder to embed\nthe binary vector in Hamming space to a dense representation.\n(2) To generalize for queries and thresholds not covered by the\ntraining data, we also embed (Hamming) distance values. The\ndistance embeddings are concatenated to the binary vector\nand its dense representation, and then fed to a neural network\nto produce final embeddings. The decoders takes the final\nembeddings as input and outputs the estimated cardinality.\nWe design a loss function and a dynamic training strategy,\nboth tailored to our incremental prediction model. The loss\nfunction adds more loss to the distance values that tend to\ncause more estimation error. The impact of such loss is dynam-\nically adjusted through training to improve the accuracy and\nthe generalizability. For fast online estimation, optimizations\nare developed on top of our regression model by reducing mul-\ntiple encoders to one. As we are applying machine learning on\na traditional database problem, an important issue is whether\nthe solution works when update exists. For this reason, we\ndiscuss incremental learning to handle updates in the dataset.\nExtensive experiments were carried out on four common\ndistance functions using real datasets. We took a uniform\nsample of records from each dataset as a query workload for\ntraining, validation, and testing, and computed labels by run-\nning exact similarity selection algorithms. The takeaways are:\n(1) The proposed deep learning method is more accurate than\nexisting methods while also running faster with a moderate\nmodel size. (2) Incremental prediction guarantees monotonic-\nity and at the same time achieves high accuracy, substantially\noutperforming the method that simply feeding a deep neural\n2\n\nnetwork with training data. (3) The components in our model\nare all useful to improve accuracy and speed. (4) Incremental\nlearning is fast and effective against updates. (5) Our method\ndelivers excellent performance on long-tail queries having\nexceptionally large cardinalities and generalizes well on out-\nof-dataset queries that significantly differ from the dataset.\n(6) A case study shows that query processing performance is\nimproved by integrating our method into a query optimizer.\nOur contributions are summarized as follows.\nâ€¢We develop a deep learning method for cardinality estima-\ntion of similarity selection (Section 3). Our method guaran-\ntees the monotonicity of cardinality w.r.t. the threshold.\nâ€¢Through feature extraction (Section 4) and regression (Sec-\ntion 5), our method is generic to any data type and distance\nfunction, and exploits the incremental property of cardi-\nnality to achieve accuracy and monotonicity. The training\ntechniques that favor our method are developed (Section 6).\nâ€¢We accelerate our model for online estimation (Section 7)\nand propose incremental learning for updates (Section 8).\nâ€¢We conduct extensive experiments to demonstrate the su-\nperiority and the generalizability of our method, as well as\nhow it works in a query optimizer (Section 9).\n2 PRELIMINARIES\n2.1 Problem Definition and Notations\nLetObe a universe of records. ğ‘¥andğ‘¦are two records inO.\nğ‘“:OÃ—Oâ†’ Ris a function which evaluates the distance\n(similarity) of a pair of records. Common distance (similarity)\nfunctions include Hamming distance, Jaccard similarity, edit\ndistance, Euclidean distance, etc. Without loss of generality,\nwe assumeğ‘“is distance function. Given a collection of records\nDâŠ†O , a query record ğ‘¥âˆˆO, and a threshold ğœƒ, a similarity\nselection is to find all the records ğ‘¦âˆˆD such thatğ‘“(ğ‘¥,ğ‘¦)â‰¤ğœƒ.\nWe formally define our problem.\nProblem 1 (Cardinality Estimation of Similarity Se-\nlection). Given a collectionDof records, a query record ğ‘¥âˆˆO,\na distance function ğ‘“, and a threshold ğœƒâˆˆ[0,ğœƒmax], our task\nis to estimate the number of records that satisfy the similarity\nconstraint, i.e.,|{ğ‘¦|ğ‘“(ğ‘¥,ğ‘¦)â‰¤ğœƒ,ğ‘¦âˆˆD}| .\nğœƒmaxis the maximum threshold (reasonably large for sim-\nilarity selection to make sense) we are going to support. A\ngood estimation is supposed to be close to the actual cardinal-\nity. Mean squared error ( MSE ) and mean absolute percentage\nerror ( MAPE ) are two widely used evaluation metrics in the\ncardinality estimation problem [ 21,32,53,57,76]. Givenğ‘›\nsimilarity selection queries, let ğ‘ğ‘–denote the actual cardinality\nof theğ‘–-th selection and bğ‘ğ‘–denote the estimated cardinality.Table 1: Frequently used notations.\nSymbol Definition\nO,D a record universe, a dataset\nğ‘¥,ğ‘¦ records inO\nğ‘“ a distance function\nğœƒ,ğœƒmax a distance threshold and its maximum value\nğ‘,bğ‘ cardinality and the estimated value\nğ‘”,â„ regression function and feature extraction function\nx,ğ‘‘ the binary representation of ğ‘¥and its dimensionality\nğœ,ğœmax a threshold in Hamming space and its maximum value\neğ‘–the embedding of distance ğ‘–\nzğ‘–ğ‘¥ the embedding of xand distance ğ‘–\nMSE andMAPE are computed as\nMSE=1\nğ‘›ğ‘›âˆ‘ï¸\nğ‘–=1(ğ‘ğ‘–âˆ’bğ‘ğ‘–)2,MAPE =1\nğ‘›ğ‘›âˆ‘ï¸\nğ‘–=1\f\f\f\fğ‘ğ‘–âˆ’bğ‘ğ‘–\nğ‘ğ‘–\f\f\f\f.\nSmaller errors are preferred. We adopt these two metrics to\nevaluate the estimation accuracy. We focus on evaluating the\ncardinality estimation as stand-alone (in contrast to in an\nRDBMS) and only consider in-memory implementations.\nTable 1 lists the notations frequently used in this paper. We\nuse bold uppercase letters (e.g., A) for matrices; bold lowercase\nletters (e.g., a) for vectors; and non-bold lowercase letters (e.g.,\nğ‘) for scalars and other variables. Uppercase Greek symbols\n(e.g.,Î¦) are used to denote neural networks. A[ğ‘–,âˆ—]andA[âˆ—,ğ‘–]\ndenote theğ‘–-th row and the ğ‘–-th column of A, respectively.\na[ğ‘–]denotes the ğ‘–-th dimension of a. Semicolon represents\nthe concatenation of vectors; e.g., given an ğ‘-dimensional\nvector aand ağ‘-dimensional vector b,c=[a;b]means that\nc[1..ğ‘]=aandc[ğ‘+1..ğ‘+ğ‘]=b. Colon represents the\nconstruction of a matrix by column vectors or matrices; e.g.,\nC=[a:b]means that C[âˆ—,1]=aandC[âˆ—,2]=b.\n2.2 Related Work\n2.2.1 Database Methods. Auxiliary data structure is one of\nthe main types of database methods for the cardinality estima-\ntion of similarity selection. For binary vectors, histograms [ 63]\ncan be constructed to count the number of records by par-\ntitioning dimensions into buckets and enumerating binary\nvectors and thresholds. For strings and sets, semi-lattice struc-\ntures [ 45,46] and inverted indexes [ 36,58] are utilized for\nestimation. The major drawback of auxiliary structure meth-\nods is that they only perform well on low dimensionality and\nsmall thresholds. Another type of database methods is based\non sampling, e.g., uniform sampling, adaptive sampling [ 52],\nand sequential sampling [ 30]. State-of-the-art sampling strate-\ngies [ 35,48,75,83] focus on join size estimation in query opti-\nmization,andaredifficulttobeadoptedtoourproblemdefined\n3\n\non distance functions. Sampling methods are often combined\nwith sketches [ 27,47] to improve the performance. A state-\nof-the-art method was proposed in [ 76] for high-dimensional\ndata. In general, sampling methods need a large set of samples\nto achieve good accuracy, and thus become either slow or inac-\ncurate when applied on large datasets. As for the cardinalities\nof SQL queries, recent studies proposed a tighter bound for\nintermediate join cardinalities [ 14] and adopted inaccurate\ncardinalities to generate optimal query plans [71].\n2.2.2 Traditional Learning Models. A prevalent traditional\nlearning method for the cardinality estimation of similarity\nselection is to train a kernel-based estimator [ 32,57] using\na sample. Monotonicity is guaranteed when the sample is\ndeterministic w.r.t. the query record (i.e., the sample does\nnot change if only the threshold changes). Kernel methods\nrequire large number of instances for accurate estimation,\nhence resulting in low estimation speed. Moreover, they im-\npose strong assumptions on kernel functions, e.g., only diago-\nnal covariance matrix for Gaussian kernels. Other traditional\nlearning models [ 33], such as support vector regression, lo-\ngistic regression, and gradient boosting tree, are also adopted\nto solve the cardinality estimation problem. A query-driven\napproach [ 12] was proposed to learn several query prototypes\n(i.e., interpolation) that are differentiable. These approaches\ndeliver comparable performance to kernel methods. A recent\nstudy [ 24] explored the application of two-hidden-layer neu-\nral networks and tree-based ensembles (random forest and\ngradient boosted trees) on cardinality estimation of multi-\ndimensional range queries. It targets numerical attributes but\ndoes not apply to similarity selections.\n2.2.3 Deep Learning Models. Deep learning models are re-\ncently adopted to learn the best join order [ 44,54,55] or\nestimate the join size [ 41]. Deep reinforcement learning is\nalso explored to generate query plans [ 60]. Recent studies\nalso adopted local-oriented approach [ 74], tree or plan based\nlearning model [ 55,56,70], recurrent neural network [ 61], and\nautoregressive model [ 31,77]. The method that can be adapted\nto solve our problem is the mixture of expert model [ 67]. It\nutilizes a sparsely-gated mixture-of-experts layer and assigns\ngood experts (models) to these inputs. Based on this model,\nthe recursive-model index [ 43] was designed to replace the\ntraditional B-tree index for range queries. The two models\ndeliver good accuracy but do not guarantee monotonicity.\nFor monotonic methods, an early attempt used a min-max\n4-layer neural network for regression [ 19]. Lattice regres-\nsion [ 25,26,29,78] is recent monotonic method. It adopts a\nlattice structure to construct all combinations of monotonic\ninterpolation values. To handle high-dimensional monotonic\nfeatures, ensemble of lattices [ 25] splits lattices into severalsmall pieces using ensemble learning. To improve the per-\nformance of regression, deep lattice network (DLN) was pro-\nposed [ 78]. It consists of multiple calibration layers and an\nensemble of lattices layers. However, lattice regression does\nnot directly target our problem, and our experiments show\nthat DLN is rather inaccurate.\n3 CARDINALITY ESTIMATION\nFRAMEWORK\n3.1 Basic Idea\nLetğ‘(ğ‘¥,ğœƒ)denote the cardinality of a query ğ‘¥with threshold\nğœƒ. We model the estimation as a regression problem with\na unique framework designed to alleviate the challenges\nmentioned in Section 1. We would like to find a function\nbğ‘within a function family, such that bğ‘returns an approxi-\nmate value of ğ‘for any input ğ‘¥, i.e.,bğ‘(ğ‘¥,ğœƒ)â‰ˆğ‘(ğ‘¥,ğœƒ),âˆ€ğ‘¥âˆˆO\nandğœƒâˆˆ[0,ğœƒmax]. We consider bğ‘that belongs to the follow-\ning family: bğ‘Bğ‘”â—¦â„, whereâ„(ğ‘¥,ğœƒ)=(x,ğœ),ğ‘”(x,ğœ)âˆˆZâ‰¥0,\nxâˆˆ{0,1}ğ‘‘, andğœâˆˆZâ‰¥0. Intuitively, we can deem â„as a\nfeature extraction function, which maps an object ğ‘¥and a\nthresholdğœƒto a fixed-dimensional binary vector xand an\ninteger threshold ğœ. Then, the function ğ‘”essentially performs\ntheregression using the transformed input, i.e., the (x,ğœ)pair.\nThe rationales of such design are analyzed as follows:\nâ€¢This design separates data modelling and cardinality estima-\ntion into two functions, â„andğ‘”, respectively. On one hand,\nthis allows the system to cater for different data types, and\ndistance functions. On the other hand, it allows us to choose\nthe best models for the estimation problem. To decouple\nthe two components, some common interface needs to be\nestablished. We pose the constraints that (1) xbelonging to\na Hamming space, and (2) ğœis an non-negative integer. For\n(1), many DB applications deal with discrete objects (e.g.,\nsets and strings) or discrete object representations (e.g., bi-\nnary codes from learned hash functions). For (2), since there\nis a finite number of thresholds that make a difference in\ncardinality, in theory we can always map them to integers,\nalbeit a large number. Here, we take the approximation to\nlimit it to a fixed number. Other learning models also make\nsimilar modelling choice (e.g., most regularizations adopt\nthe smoothness bias in the function space). We will leave\nother interface design choices to future work due to their\nadded complexity. For instance, it is entirely possible to\nrestrict xtoRğ‘‘andğœâˆˆR. While the modelling power of\nthe framework gets increased, this will inevitably result in\nmore complex models that are potentially difficult to train.\nâ€¢The design can be deemed as an instance of the encoder-\ndecoder model, where two functions â„andğ‘”are used for\nsome prediction tasks. As a close analog, Google transla-\ntion [ 37] trains anâ„that maps inputs in the source language\nto a latent representation, and then train a ğ‘”that maps the\n4\n\nlatent representation to the destination language. As such,\nit can support translation between ğ‘›languages by training\nonly 2ğ‘›functions, instead of ğ‘›(ğ‘›âˆ’1)direct functions.\nBy this model design, the function bğ‘=ğ‘”â—¦â„(ğ‘¥,ğœƒ)is mono-\ntonic if it satisfies the condition in the following lemma.\nLemma 1. Consider a function â„(ğ‘¥,ğœƒ)monotonically increas-\ning withğœƒand a function ğ‘”(x,ğœ)monotonically increasing with\nğœ, our framework ğ‘”â—¦â„(ğ‘¥,ğœƒ)is monotonically increasing with\nğœƒ.\n3.2 Feature Extraction\nThe process of feature extraction is to transfer any data type\nand distance threshold into binary representation and integer\nthreshold. Formally, we have a function â„ğ‘Ÿğ‘’ğ‘:Oâ†’{ 0,1}ğ‘‘;\ni.e., given any record ğ‘¥âˆˆO,â„ğ‘Ÿğ‘’ğ‘mapsğ‘¥to ağ‘‘-dimensional\nbinary vector, denoted by x, calledğ‘¥â€™s binary representation.\nWe can plug in any user-defined functions or neural networks\nfor feature extraction. For the sake of estimation accuracy, the\ngeneral criteria is that the Hamming distance of the target ğ‘‘-\ndimensional binary vectors can equivalently or approximately\ncapture the semantics of the original distance function. We\nwill show some example feature extraction methods and a\nseries of case studies in Section 4.\nBesides the transformation to binary representations, we\nalsohaveamonotonicallyincreasing(asdemandedbyLemma1)\nfunctionâ„ğ‘¡â„ğ‘Ÿ:[0,ğœƒmax]â†’[ 0,ğœmax]to transform the thresh-\nold.ğœmaxis a tunable parameter to control the model size\n(as introduced later, there are (ğœ+1)decoders in our model,\nğœâ‰¤ğœmax). Given ağœƒâˆˆ[0,ğœƒmax],â„ğ‘¡â„ğ‘Ÿmapsğœƒto an integer\nbetween 0andğœmax, denoted by ğœ. The purpose of threshold\ntransformation is: for real-valued distance functions, it makes\nthe distances countable; for integer-valued distance functions,\nit can reduce the threshold to a small number, hence to pre-\nvent the model growing too big when the input threshold is\nlarge. As such, we are able to use finite number of estima-\ntors to predict the cardinality for each distance value. The\ndesign of threshold transformation depends on how original\ndata are transformed to binary representations. In general, a\ntransformation with less skew leads to better performance.\nUsing the threshold of the Hamming distance between binary\nrepresentations is not necessary, but would be a preferable\noption. A few case studies will be given in Section 4.\n3.3 Regression (in a Nutshell)\nOur method for the regression is based on the following obser-\nvation: given a binary representation xand a threshold ğœ1, the\ncardinality can be divided into (ğœ+1)parts, each representing\nthe cardinality of a Hamming distance ğ‘–,ğ‘–âˆˆ[0,ğœ]. This sug-\ngests that we can learn (ğœ+1)functionsğ‘”0(x),...,ğ‘”ğœ(x), each\n1Note that the threshold is ğœnotğœmaxhere because ğœƒis mapped to ğœ.ğ‘”ğ‘–(x)estimating the cardinality of the set of records whose\nHamming distances to xare exactlyğ‘–. So we have\nğ‘”(x,ğœ)=ğœâˆ‘ï¸\nğ‘–=0ğ‘”ğ‘–(x). (1)\nThis design has the following advantages:\nâ€¢As we have shown in Figure 1(a), the cardinalities for dif-\nferent distance values may differ significantly. By using in-\ndividual estimators, the distribution of each distance value\ncan be learned separately to achieve better overall accuracy.\nâ€¢Our method exploits the incremental property of cardinality:\nwhen the threshold increases from ğ‘–toğ‘–+1, the increased car-\ndinality is the cardinality for distance ğ‘–+1. This incremental\nprediction can guarantee the monotonicity of cardinality\nestimation:\nLemma 2.ğ‘”(x,ğœ)is monotonically increasing with ğœ, if\neachğ‘”ğ‘–(x),ğ‘–âˆˆ[0,ğœ]is deterministic and non-negative.\nThelemmasuggeststhatadeterministicandnon-negative\nmodel satisfies the requirement in Lemma 1, hence leading\nto the overall monotonicity.\nâ€¢We are able to control the size of the model by setting the\nmaximum number of estimators. Thus, working with the\nfeature extraction, the regression achieves fast speed even\nif the original threshold is large.\nWe employ a deep encoder-decoder model to process each\nregressionğ‘”ğ‘–. The reasons for choosing deep models are:\n(1) Cardinalities may significantly differ across queries, as\nshown in Figure 1(b). Deep models are able to learn a variety\nof underlying distributions and deliver salient performance\nfor general regression tasks [ 43,67,78]. (2) Deep models gen-\neralize well on queries that are not covered by the training\ndata. (3) Although deep models usually need large training\nsets for good accuracy, the training data here can be easily and\nefficiently acquired by running state-of-the-art similarity se-\nlection algorithms (and producing no label noise when exact\nalgorithms are used). (4) Deep models can be accelerated by\nmodern hardware (e.g., GPUs/TPUs) or software (e.g., Tensor-\nflow) that optimizes batch strategies or matrix manipulation2.\nWe employ a deep learning model to process each regres-\nsionğ‘”ğ‘–. By carefully choosing encoders and decoders, we can\nmeet the requirement in Lemma 2 to guarantee the mono-\ntonicity. The details will be given in Section 5. Before that, we\nshow some options and case studies of feature extraction.\n4 CASE STUDIES FOR FEATURE\nEXTRACTION\nAs stated in Section 3.2, for good accuracy, a desirable feature\nextraction is that the Hamming distance between the binary\n2Despite such possibilities for acceleration, we only use them for training but\nnot inference (estimation) in our experiments for the sake of fair comparison.\n5\n\nvectors can capture the semantics of the original distance\nfunction. We discuss a few example options.\nâ€¢Equivalency :Somedistancescanbeequivalentlyexpressed\nin a Hamming space, e.g., ğ¿1distance on integer values [ 27].\nâ€¢LSH : We useğ‘‘hash functions in the locality sensitive hash-\ning (LSH) family [ 27], each hashing a record to a bit. xand\nyagree on a bit with high probability if ğ‘“(ğ‘¥,ğ‘¦)â‰¤ğœƒ, thus\nyielding a small Hamming distance between xandy.\nâ€¢Bounding : We may derive a necessary condition of the\noriginaldistanceconstraint;e.g., ğ‘“(ğ‘¥,ğ‘¦)â‰¤ğœƒ=â‡’ğ»(x,y)â‰¤\nğœ, whereğ»(Â·,Â·)denotes the Hamming distance.\nFor the equivalency method, since the conversion to Ham-\nming distance is lossless, it can be used atop of the other two.\nThis is useful when the output of the hash function or the\nbound is not in a Hamming space. Note that our model is not\nlimited to these options. Other feature extraction methods,\nsuch as embedding [80], can be also used here.\nAs for threshold transformation, we have two parameters:\nğœƒmax, the maximum threshold we are going to support, and\nğœmax, a tunable parameter to control the size of our model. Any\nthresholdğœƒâˆˆ[0,ğœƒmax]is monotonically mapped to an integer\nğœâˆˆ[0,ğœmax]. In our case studies, we consider using a trans-\nformation proportional to the (expected/bounded) Hamming\ndistance between binary representations. Note that ğœƒmaxis not\nnecessarily mapped to ğœmax, because for integer-valued dis-\ntance functions, the number of available thresholds is smaller\nthanğœmax+1whenğœƒmax<ğœmax, meaning that only (ğœƒmax+1)\ndecoders are useful. In this case, ğœƒmaxis mapped to a value\nsmaller than ğœmax. Next we show four case studies of some\ncommon data types and distance functions.\n4.1 Hamming Distance\nWe consider binary vector data and Hamming distance as the\ninput distance function. The original data are directly fed to\nour regression model. Since the function is already Hamming\ndistance, we use the original threshold ğœƒasğœ, ifğœƒmaxâ‰¤ğœmax.\nOtherwise, we map ğœƒmaxtoğœmax, and other thresholds are\nmapped proportionally: ğœ=âŒŠğœmaxÂ·ğœƒ/ğœƒmaxâŒ‹. Although mul-\ntiple thresholds may map to the same ğœ, we can increase the\nnumber of decoders to mitigate the imprecision.\n4.2 Edit Distance\nThe (Levenshtein) edit distance measures the minimum num-\nber of operations, including insertion, deletion, and substitu-\ntion of a character, to transform one string to another.\nThe feature extraction is based on bounding. The basic idea\nis to map each character to (2ğœmax+1)bits, hence to cover\nthe effect of insertion and deletion. Let Î£denote the alphabet\nof strings, and ğ‘™maxdenote the maximum string length in D.\nEach binary vector has ğ‘‘=((ğ‘™max+2ğœmax)Â·|Î£|)bits. They are\ndivided into|Î£|groups, each group representing a characterinÎ£. For ease of exposition, we assume the subscript of a string\nstart from 0, and the subscript of each group of the binary\nvector start fromâˆ’ğœmax. All the bits are initialized as 0. Given\na stringğ‘¥, for each character ğœat positionğ‘–, we set the ğ‘—-th\nbit in theğœ-th group to 1, whereğ‘—iterates through ğ‘–âˆ’ğœmaxto\nğ‘–+ğœmax. For example, given a string ğ‘¥=abc,Î£={a,b,c,d},\nğ‘™max=4, andğœğ‘šğ‘ğ‘¥=1, the binary vector is 111000, 011100,\n001110, 000000 (groups separated by comma).\nIt can be proved that an edit operation causes at most\n(4ğœmax+2)different bits. Hence ğ‘“(ğ‘¥,ğ‘¦)edit operations yield a\nHamming distance no greater than ğ‘“(ğ‘¥,ğ‘¦)Â·(4ğœmax+2). Since\nit is proportional to ğ‘“(ğ‘¥,ğ‘¦)and thresholds are integers, we use\nthe same threshold transformation as for Hamming distance.\n4.3 Jaccard Distance\nGiven two sets ğ‘¥andğ‘¦, the Jaccard similarity is defined as\n|ğ‘¥âˆ©ğ‘¦|/|ğ‘¥âˆªğ‘¦|. For ease of exposition, we use its distance form:\nğ‘“(ğ‘¥,ğ‘¦)=1âˆ’|ğ‘¥âˆ©ğ‘¦|/|ğ‘¥âˆªğ‘¦|.\nWe useğ‘-bit minwise hashing [ 50] (LSH) for feature ex-\ntraction. Given a record ğ‘¥,ğœ‹(ğ‘¥)orders the elements of ğ‘¥\nby a permutation on the record universe O. We uniformly\nchoose a set of ğ‘˜permutations{ğœ‹1,...,ğœ‹ğ‘˜}. Letğ‘ğ‘šğ‘–ğ‘›(ğœ‹(ğ‘¥))\ndenote the last ğ‘bits of the smallest element of ğœ‹(ğ‘¥). We re-\ngardğ‘ğ‘šğ‘–ğ‘›(ğœ‹(ğ‘¥))as an integer in[0,2ğ‘âˆ’1]and transform\nit to a Hamming space. Let ğ‘ ğ‘’ğ‘¡_ğ‘ğ‘–ğ‘¡(ğ‘–,ğ‘—)produce a one-hot\nbinary vector such that only the ğ‘–-th bit is 1out ofğ‘—bits.ğ‘¥\nis transformed to a ğ‘‘-dimensional ( ğ‘‘=2ğ‘ğ‘˜) binary vector:\n[ğ‘ ğ‘’ğ‘¡_ğ‘ğ‘–ğ‘¡(ğ‘ğ‘šğ‘–ğ‘›(ğœ‹1(ğ‘¥)),2ğ‘);...;ğ‘ ğ‘’ğ‘¡_ğ‘ğ‘–ğ‘¡(ğ‘ğ‘šğ‘–ğ‘›(ğœ‹ğ‘˜(ğ‘¥)),2ğ‘)].For\nexample:ğ‘¥={1,2,4}.O={1,2,3,4,5}.ğœ‹1=12345 ,ğœ‹2=\n54321 , andğœ‹3=21453 .ğ‘=2. We haveğ‘ğ‘šğ‘–ğ‘›(ğœ‹1(ğ‘¥))=1,\nğ‘ğ‘šğ‘–ğ‘›(ğœ‹2(ğ‘¥))=0, andğ‘ğ‘šğ‘–ğ‘›(ğœ‹3(ğ‘¥))=2. Supposeğ‘ ğ‘’ğ‘¡_ğ‘ğ‘–ğ‘¡\ncounts from the lowest bit, starting from 0. The binary vector\nis0010, 0001, 0100 (permutations separated by comma).\nGiven two sets ğ‘¥andğ‘¦, the probability that ğ‘ğ‘šğ‘–ğ‘›(ğœ‹(ğ‘¥))=\nğ‘ğ‘šğ‘–ğ‘›(ğœ‹(ğ‘¦))equals to 1âˆ’ğ‘“(ğ‘¥,ğ‘¦)[50]. The expected Ham-\nming distance between xandyis thusğ‘“(ğ‘¥,ğ‘¦)Â·ğ‘‘. Since it is\nproportional to ğ‘“(ğ‘¥,ğ‘¦), we use the following threshold trans-\nformation:ğœ=âŒŠğœmaxÂ·ğœƒ/ğœƒmaxâŒ‹.\n4.4 Euclidean Distance\nWe use LSH based on ğ‘-stable distribution [ 22] to handle Eu-\nclidean distance on real-valued vectors. The hash function\nisâ„a,ğ‘(ğ‘¥)=âŒŠağ‘¥+ğ‘\nğ‘ŸâŒ‹, where ais a|ğ‘¥|-dimensional vector with\neach element independently drawn by a normal distribution\nN(0,1),ğ‘is a real number chosen uniformly from [0,ğ‘Ÿ], andğ‘Ÿ\nis a predefined constant value. Let ğ‘£denote the maximum hash\nvalue. We use the aforementioned ğ‘ ğ‘’ğ‘¡_ğ‘ğ‘–ğ‘¡function to trans-\nform hash values to a Hamming space. Given ğ‘˜hash functions,\nğ‘¥is transformed to a ğ‘‘-dimensional ( ğ‘‘=ğ‘˜(ğ‘£+1)) binary vec-\ntor:[ğ‘ ğ‘’ğ‘¡_ğ‘ğ‘–ğ‘¡(â„a1,ğ‘1(ğ‘¥),ğ‘£+1);...;ğ‘ ğ‘’ğ‘¡_ğ‘ğ‘–ğ‘¡(â„ağ‘˜,ğ‘ğ‘˜(ğ‘¥),ğ‘£+1)]. For\nexample:ğ‘¥=[0.1,0.2,0.4].ğ‘£=4.â„a1,ğ‘1(ğ‘¥)=1.â„a2,ğ‘2(ğ‘¥)=3.\n6\n\nâ„a3,ğ‘3(ğ‘¥)=4. Supposeğ‘ ğ‘’ğ‘¡_ğ‘ğ‘–ğ‘¡counts from the lowest bit,\nstarting from 0. The binary vector is 00010, 01000, 10000\n(hash functions separated by comma).\nGiven two records ğ‘¥andğ‘¦such thatğ‘“(ğ‘¥,ğ‘¦)=ğœƒ, the proba-\nbility that two hash values match is ğ‘ƒğ‘Ÿ{â„a,ğ‘(ğ‘¥)=â„a,ğ‘(ğ‘¦)}=\nğœ–(ğœƒ)=1âˆ’2Â·ğ‘›ğ‘œğ‘Ÿğ‘š(âˆ’ğ‘Ÿ/ğœƒ)âˆ’2âˆš\n2ğœ‹ğ‘Ÿ/ğœƒ(1âˆ’ğ‘’âˆ’(ğ‘Ÿ2/2ğœƒ2)), where\nğ‘›ğ‘œğ‘Ÿğ‘š(Â·)is the cumulative distribution function of a random\nvariable with normal distribution N(0,1)[22]. Hence the\nexpected Hamming distance between their binary represen-\ntations is(1âˆ’ğœ–(ğœƒ))Â·ğ‘‘. The threshold transformation is\nğœ=âŒŠğœmaxÂ·1âˆ’ğœ–(ğœƒ)\n1âˆ’ğœ–(ğœƒmax)âŒ‹.\n5 REGRESSION\nWe present the detailed regression model in this section. Fig-\nure 2 shows the framework of our model. (1) xandğœare input\nto the encoder Î¨, which returns ğœ+1embeddings zğ‘–\nğ‘¥. Specifi-\ncally, xis embedded to a dense vector space. Each distance ğ‘–\nis also embedded, concatenated to the embedding of x, and\nfed to a neural network Î¦to produce zğ‘–\nğ‘¥. (2) Each of the ğœ+1\ndecodersğ‘”ğ‘–takes an embedding zğ‘–\nğ‘¥as input and returns the\ncardinality of distance ğ‘–,ğ‘–âˆˆ[0,ğœ]. (3) Theğœ+1cardinalities\nare summed up to get the final cardinality.\n5.1 Encoder-Decoder Model\nOur solution to the regression is to embed the binary vector x\nand distance ğ‘–to a dense real-valued vector z1\nğ‘¥by an encoder\nÎ¨, and then model ğ‘”ğ‘–(x)as a decoder that performs an affine\ntransformation and applies an ReLU activation function:\nğ‘”ğ‘–(x)=ReLU(wT\nğ‘–Î¨(x,ğ‘–)+ğ‘ğ‘–)=ReLU(wT\nğ‘–zğ‘–\nğ‘¥+ğ‘ğ‘–).\nwğ‘–andğ‘ğ‘–are parameters of the mapping from the embed-\nding zğ‘–\nğ‘¥to the cardinality estimation of distance ğ‘–. From the\nmachine learning perspective, if a representation of input\nfeatures is well learned through an encoder, then a linear\nmodel (affine transformation) is capable of making final de-\ncisions [ 13].ReLU is chosen here because cardinality is non-\nnegative and matches the range of ReLU . The reason why\nwe also embed distance ğ‘–is as follows. Consider only xis em-\nbedded. If the cardinalities of two records ğ‘¥1andğ‘¥2are close\nfor distance values in a range [ğœ1,ğœ2]covered by the training\nexamples, their embeddings are likely to become similar after\ntraining, because the encoder may mistakenly regard ğ‘¥1and\nğ‘¥2as similar. This may cause ğ‘”ğ‘–(x1)â‰ˆğ‘”ğ‘–(x2)forğ‘–âˆ‰[ğœ1,ğœ2],\ni.e., the distance values not covered by the training examples,\neven if their actual cardinalities significantly differ.\nBy Equation 1, the output of the ğœdecoders are summed\nup to obtain the cardinality. ğ‘”ğ‘–(x)is deterministic if we use a\ndeterministic Î¨(Â·,Â·). Hence the model can satisfy the require-\nment in Lemma 2 to guarantee the monotonicity.5.2 Encoder in Detail\nTo encode both xand distance ğ‘–to embedding zğ‘–\nğ‘¥,Î¨includes a\nrepresentation network Î“that maps xto a dense vector space,\na distance embedding layer E, and a shared neural network Î¦\nthat outputs the embedding zğ‘–\nğ‘¥. Next we introduce the details.\n5.2.1 Representation Network. Given a binary representa-\ntion xgenerated by feature extraction function â„(Â·,Â·), we\ndesign a neural network Î“that maps xto another vector\nspace: xâ€²=Î“(x), because the correlations of sparse high-\ndimensional binary vectors are difficult to learn. Variational\nauto-encoder ( VAE) [40] is a generative model to estimate\ndata distribution by unsupervised learning. We can view auto-\nencoders (AEs) as non-linear PCAs to reduce dimensionality\nand extract meaningful and robust features, and VAEenforces\ncontinuity in the latent space. VAEimproves upon other types\nof AEs (such as denoising AEs and sparse AEs) by imposing\nsome regularization condition on the embeddings. This re-\nsults in embeddings that are robust and disentangled, and\nhence have been widely used in various models [ 72]. We use\nthe latent layer of VAEto produce a dense representation, de-\nnoted byğ‘‰ğ´ğ¸(x,ğœ–).ğœ–is a random noise generated by normal\ndistributionN(0,I).Î“concatenates xand the output of VAE;\ni.e.,xâ€²=[x;ğ‘‰ğ´ğ¸(x,ğœ–)]. The reason for such concatenation\n(i.e., not using only the output of VAE asxâ€²) is that the (co-\nsine) distance in the ğ‘‰ğ´ğ¸(x,ğœ–)space captures less semantics\nof the original distance than does the Hamming distance be-\ntween binary vectors. Due to the noise ğœ–, the output of VAE\nbecomes nondeterministic. Since we need a deterministic\noutput to guarantee the monotonicity, we choose the follow-\ning option: for training, we still use the nondeterministic\nxâ€²=[x;ğ‘‰ğ´ğ¸(x,ğœ–)], because this makes our model generalize\nto unseen records and thresholds; for inference (online es-\ntimation), we set xâ€²=[x; Eğœ–âˆ¼N( 0,I)[ğ‘‰ğ´ğ¸(x,ğœ–)]], where E[Â·]\ndenotes the expected value, so it becomes deterministic [ 68].\nExample 1. Figure 3 shows an example of xand its embed-\ndingxâ€². Suppose x=0010 . The VAE takes xas input and output\na dense vector, say [0.7,1.2]. Then they are concatenated to\nobtain xâ€²=[0010,0.7,1.2].\n5.2.2 Distance Embeddings. In order to embed xand distance\nğ‘–into the same vector, we design a distance embedding layer\n(a matrix) Eto embed each distance ğ‘–. Each column in Erepre-\nsents a distance embedding; i.e., eğ‘–=E[âˆ—,ğ‘–].Eis initialized\nrandomly, following standard normal distribution.\n5.2.3 Final Embeddings. The distance embedding eğ‘–is con-\ncatenated with xâ€²; i.e.,xğ‘–=[xâ€²;eğ‘–]. Then we use a feedforward\nneural network (FNN) Î¦to generate embeddings zğ‘–\nğ‘¥=Î¦(xğ‘–).\nExample 2. We follow Example 1. Suppose ğœ=2. Then we\nhave three distance embeddings e0=[1.1,0.7],e1=[1.5,0.3],\nande2=[1.8,0.9]. By concatenating xâ€²and each eğ‘–,x0=\n7\n\nx\nÎ“z0x DecoderÂ g0\nExâ€²\n0,1,â€¦,Ï„\n...e0\ne1\neÏ„Î¦\nÎ¦\nÎ¦...z1x\nzÏ„xDecoderÂ g1\nDecoderÂ gÏ„c0Ë†\nc1Ë†\ncÏ„Ë†Share parameters\n... ... + cË†\nEncoderÂ Î¨Figure 2: The regression model.\n0ï¿½\nÎ“\nï¿½ï¿½â€²\n0,1,2ï¿½0\nï¿½1\nï¿½20\n1\n00\n0\n1\n0\n0.7\n1.20\n0\n1\n0\n0.7\n1.2\n1.1\n0.70\n0\n1\n0\n0.7\n1.2\n1.5\n0.30\n0\n1\n0\n0.7\n1.2\n1.8\n0.9\nÎ¦ ï¿½0ï¿½ ï¿½1ï¿½ ï¿½2ï¿½1.1 0.7\n1.5 0.3\n1.8 0.90.2\n1.1\n0.80.9\n0.4\n1.10.8\n1.7\n1.4ï¿½0ï¿½1ï¿½2\nFigure 3: Example of encoder Î¨.\n[0010,0.7,1.2,1.1,0.7],x1=[0010,0.7,1.2,1.5,0.3], and x2=\n[0010,0.7,1.2,1.8,0.9]. Theyaresentto neuralnetwork Î¦,whose\noutput is z0\nğ‘¥=[0.2,1.1,0.8],z1\nğ‘¥=[0.9,0.4,1.1], and z2\nğ‘¥=\n[0.8,1.7,1.4].\n6 MODEL TRAINING\n6.1 Data Preparation\nConsider a query workload Qof records (see Section 9.1.1\nfor the choice ofQin our experiments). We split data in Q\nfor training, validation, and testing sets. Then we uniformly\ngenerate a set of thresholds in [0,ğœƒmax], denoted by ğ‘†. For\neach record ğ‘¥in the training set, we iterate through all the\nthresholdsğœƒinğ‘†and compute the cardinality ğ‘w.r.t.Dusing\nan exact similarity selection algorithm. Then âŸ¨ğ‘¥,ğœƒ,ğ‘âŸ©is used\nas a training example. We uniformly choose thresholds in ğ‘†\nfor validation and in [0,ğœƒmax]for testing.\n6.2 Loss Function & Dynamic Training\nThe loss function is defined as follows.\nL(bc,c)=Eğœâˆ¼ğ‘ƒ(Â·)[Lğ‘”(bc,c)]+ğœ†Lğ‘£ğ‘ğ‘’(x), (2)\nwhereLğ‘”(Â·,Â·)is the loss of regression model, and Lğ‘£ğ‘ğ‘’(Â·)\nis the loss of VAE.bcandcare two vectors, each dimensionrepresenting the estimated and the real cardinalities of a set\nof training examples, respectively. ğœ†is a positive hyperparam-\neter for the importance of VAE. A caveat is that although we\nuniformly sample thresholds in [0,ğœƒmax]for training data, it\ndoes not necessarily mean the threshold ğœafter feature extrac-\ntion is uniformly distributed in [0,ğœmax], e.g., for Euclidean\ndistance (Section 4.4). To take this factor into account, we\napproximate the probability of ğœusing the empirical proba-\nbility of thresholds after running feature extraction on the\nvalidation set; i.e., ğ‘ƒ(ğœ)â‰ˆÃ\nâŸ¨ğ‘¥,ğ‘–,ğ‘âŸ©âˆˆTğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ 1â„ğ‘¡â„ğ‘Ÿ(ğ‘–)=ğœ\n|Tğ‘£ğ‘ğ‘™ğ‘–ğ‘‘|, whereTğ‘£ğ‘ğ‘™ğ‘–ğ‘‘\nis the validation set, and 1is the indicator function.\nForLğ‘”, instead of using MSE orMAPE , we resort to the\nmean squared logarithmic error ( MSLE ) for the following\nreason: MSLE is an approximation of MAPE [62] and nar-\nrows down the large output space to a smaller one, thereby\ndecreasing the learning difficulty.\nThen we propose a training strategy for better accuracy.\nGiven a set of training examples, let c0,...,cğœandbc0,...,bcğœ\ndenote the cardinalities and the estimated values for distance\n0,...,ğœin these training examples, respectively. As we have\nshown in Figure 1(a), the cardinalities may vary significantly\nfor different distance values. Some of them may result in much\nworse estimations than others and compromise the overall\nperformance. The training procedure should gradually focus\non training these bad estimations. Thus, we consider the loss\ncaused by the estimation for distance ğ‘–, combined with MSLE :\nLğ‘”(bc,c)=MSLE(bc,c)+ğœ†Î”Â·\u0012ğœğ‘šğ‘ğ‘¥âˆ‘ï¸\nğ‘–=0ğœ”ğ‘–Â·MSLE(bcğ‘–.cğ‘–)\u0013\n.(3)\nEachğœ”ğ‘–is a hyperparameter automatically adjusted during\nthe training procedure. Hence we call it dynamic training. It\ncontrols the loss of each estimation for distance ğ‘–.Ãğœğ‘šğ‘ğ‘¥\nğ‘–=0ğœ”ğ‘–=\n1.ğœ†Î”is a hyperparameters to control the impact of the losses\nof all the estimations for ğ‘–âˆˆ[0,ğœmax].\nDue to the non-convexity of Lğ‘”, it is difficult to find the\ncorrect direction of gradient that reaches the global or a good\nlocal optimum. Nonetheless, we can adjust its direction by\n8\n\n...\u00001 \u00002 \u0000\u0000\n\u00001 \u00002 \u0000\u0000...\n\u0000\u0000â€²Figure 4: Î¦â€²in the accelerated regression model.\nconsidering the loss trend of the estimation for distance ğ‘–,\nhence to encourage the model to generalize rather than to\noverfit the training data. Let â„“ğ‘–(ğ‘¡)=MSLE(bcğ‘–,cğ‘–)denote the\nloss of the estimation for distance ğ‘–in theğ‘¡-th iteration of\nvalidation. The loss trend Î”â„“ğ‘–(ğ‘¡)=â„“ğ‘–(ğ‘¡)âˆ’â„“ğ‘–(ğ‘¡âˆ’1)is calculated,\nand then after each validation we adjust ğœ”ğ‘–by adding more\ngradients to where the loss occurs: (1) If Î”â„“ğ‘–(ğ‘¡)>0,ğœ”ğ‘–=\nÎ”â„“ğ‘–(ğ‘¡)Ã\nğ‘–âˆˆğ´Î”â„“ğ‘–(ğ‘¡),ğ´={ğ‘–|Î”â„“ğ‘–(ğ‘¡)>0,0â‰¤ğ‘–â‰¤ğœmax}; (2) otherwise, ğœ”ğ‘–=0.\n7 ACCELERATING ESTIMATION\nRecall in the regression model, we pair xâ€²and(ğœ+1)distance\nembeddings in encoder Î¨to produce embedings zğ‘–\nğ‘¥. This leads\nto high computation cost for online cardinality estimation\nwhenğœis large. To reduce the cost, we propose an accelerated\nmodel, using a neural network Î¦â€²to replace Î¦and the dis-\ntance embedding layer Eto output(ğœmax+1)zğ‘–\nğ‘¥embeddings\ntogether3.Î¦â€²only takes an input xâ€²and reduces the compu-\ntation cost from ğ‘‚((ğœ+1)|Î¦|)toğ‘‚(|Î¦â€²|), where|Î¦|and|Î¦â€²|\ndenote the number of parameters in the two neural networks.\nFigure 4 shows the framework of Î¦â€², an FNN comprised of ğ‘›\nhidden layers f1,f2,...,fğ‘›, each outputting some dimensions\nof the embeddings zğ‘–\nğ‘¥. Each zğ‘–\nğ‘¥is partitioned into ğ‘›regions,\ndenoted by zğ‘–\nğ‘¥[ğ‘Ÿ0,ğ‘Ÿ1],zğ‘–\nğ‘¥[ğ‘Ÿ1,ğ‘Ÿ2],...,zğ‘–\nğ‘¥[ğ‘Ÿğ‘›âˆ’1,ğ‘Ÿğ‘›], where 0=\nğ‘Ÿ0â‰¤ğ‘Ÿ1â‰¤...â‰¤ğ‘Ÿğ‘›andğ‘Ÿğ‘›equals to the dimensionality of zğ‘–\nğ‘¥. A\nhidden layer fğ‘—outputs the ğ‘—-th region of zğ‘–\nğ‘¥,ğ‘–âˆˆ[0,ğœmax]; i.e.,\nZğ‘—=[z0\nğ‘¥[ğ‘Ÿğ‘—âˆ’1,ğ‘Ÿğ‘—]:z1\nğ‘¥[ğ‘Ÿğ‘—âˆ’1,ğ‘Ÿğ‘—]:...:zğœğ‘šğ‘ğ‘¥ğ‘¥[ğ‘Ÿğ‘—âˆ’1,ğ‘Ÿğ‘—]]. Then\nwe concatenate all the regions: Z=[Z1:Z2:...:Zğ‘›]. Each\nrow of Zis an embedding: zğ‘–\nğ‘¥=Z[ğ‘–,âˆ—].\nIn addition to fast estimation, the model Î¦â€²has the follow-\ning advantages: (1) The parameters of each hidden layer fğ‘—\nare updated from the following layer fğ‘—+1and the final embed-\nding matrix Zthrough backpropagation, hence increasing the\nability to learn good embeddings. (2) Since each embedding\nis affected by all the hidden layers, the model is more likely to\nreach a better local optimum through training. (3) In contrast\nto one output layer, all the hidden layers output embeddings,\nso gradient vanishing and overfitting can be prevented.\n3We output(ğœmax+1)embeddings since it is constant for all queries, hence\nto favor implementation. Only the first (ğœ+1)embedding are used for ğœ.We analyze the complexities of our models. Assume an\nFNN has hidden layers a1,..., ağ‘›. Given an input xand an\noutput y, the complexity of an FNN is |FNN(x,y)|=|x|Â·\n|a1|+Ãğ‘›âˆ’1\nğ‘–=1|ağ‘–|Â·|ağ‘–+1|+|ağ‘›|Â·|y|. Our model has the follow-\ning components: Î¦,Î“,E, andğ‘”ğ‘–. The complexities of Î¦and\nÎ“are|FNN([xâ€²;eğ‘–],zğ‘–\nğ‘¥)|and|FNN(x,x)|, respectively.|E|=\n(ğœmax+1)|eğ‘–|.|ğ‘”ğ‘–|=(ğœmax+1)|zğ‘–\nğ‘¥|+ğœmax+1. Thus, the complex-\nity of our model without acceleration is |FNN([xâ€²;eğ‘–],zğ‘–\nğ‘¥)|+\n|FNN(x,x)|+(ğœmax+1)|eğ‘–|+(ğœmax+1)|zğ‘–\nğ‘¥|+ğœmax+1. With ac-\nceleration for FNN (AFNN), the complexity is |AFNN(xâ€²,Z)|+\n|FNN(x,x)|+(ğœmax+1)|zğ‘–\nğ‘¥|+ğœmax+1, where|AFNN(xâ€²,Z)|=\n|xâ€²|Â·|a1|+Ãğ‘›âˆ’1\nğ‘–=1|ağ‘–|Â·|ağ‘–+1|+(ğœmax+1)|ağ‘›|Â·|Z[ğ‘–,âˆ—]|.\n8 DEALING WITH UPDATES\nWhen the dataset is updated, the labels of the validation data\nare first updated by running a similarity selection algorithm\non the updated dataset. Then we monitor the error ( MSLE ) in\nthe validated data by running our model. If the error increases,\nwe train our model with incremental learning: First, the la-\nbels of the training data are updated by running a similarity\nselection algorithm on the updated dataset. Then the model\nis trained with the updated training data until the validation\nerror does not change for three consecutive epochs. Note that\n(1) the training does not start from scratch but from the cur-\nrent model, and it is processed on the entire training data to\nprevent catastrophic forgetting [ 39,59]; and (2) we always\nkeep the original queries and only update their labels.\n9 EXPERIMENTS\n9.1 Experiment Setup\n9.1.1 Datasets and Queries. We use eight datasets for four\ndistance functions: Hamming distance ( HM), edit distance\n(ED), Jaccard distance ( JC), and Euclidean distance ( EU). The\ndatasets and the statistics are shown in Table 2. Boldface\nindicates default datasets. Process indicates how we process\nthe dataset; e.g., HashNet [ 15] is adopted to convert images in\nthe ImageNet dataset [ 66] to hash codes. â„“ğ‘šğ‘ğ‘¥andâ„“ğ‘ğ‘£ğ‘”are the\nmaximum and average lengths or dimensionalities of records,\nrespectively. We uniformly sample 10% data from dataset\nDas the query workload Q. Then we follow the method\nin Section 6.1 to split Qin80 : 10 : 10 to create training,\nvalidation, and testing instances. Multiple uniform sampling\nis not considered here because even if our models are trained\non skewed data sampled with equal chance from each cluster\nofD, we observe only moderate change of accuracy of our\nmodels (up to 48% MSE ) when testing over multiple uniform\nsamples and they still perform significantly better than the\nother competitors. Thresholds and labels (cardinalities w.r.t.\nD) are generated using the method in Section 6.1.\n9\n\nTable 2: Statistics of datasets.\nDataset Source Process Data Type Domain # Records â„“ğ‘šğ‘ğ‘¥â„“ğ‘ğ‘£ğ‘” Distance ğœƒğ‘šğ‘ğ‘¥\nHM-ImageNet [1] HashNet [15] binary vector image 1,431,167 64 64 Hamming 20\nHM-PubChem [2] - binary vector biological sequence 1,000,000 881 881 Hamming 30\nED-AMiner [3] - string author name 1,712,433 109 13.02 edit 10\nED-DBLP [4] - string publication title 1,000,000 199 72.49 edit 20\nJC-BMS [5] - set product entry 515,597 164 6.54 Jaccard 0.4\nJC-DBLPğ‘3 [4] 3-gram set publication title 1,000,000 197 70.49 Jaccard 0.4\nEU-Glove 300 [6] normalize real-valued vector word embedding 1,917,494 300 300 Euclidean 0.8\nEU-Glove 50 [6] normalize real-valued vector word embedding 400,000 50 50 Euclidean 0.8\nTable 3: MSE , best values highlighted in boldface.\nModel HM-ImageNet HM-PubChem ED-AMiner ED-DBLP JC-BMS JC-DBLPğ‘3EU-Glove 300 EU-Glove 50\nDB-SE 41563 445182 8219583 1681 1722 177 116820 45631\nDB-US 27776 66255 159572 1095 3052 427 78552 16249\nTL-XGB 12082 882206 4147509 1657 2031 23 821937 557229\nTL-LGBM 14132 721609 4830965 2103 1394 49 844301 512984\nTL-KDE 279782 112952 3412627 2097 873 100 102200 169604\nDL-DLN 7307 189743 1285010 1664 1349 50 1063687 49389\nDL-MoE 7096 95447 265257 1235 425 23 988918 315437\nDL-RMI 6774 42186 93158 928 151 15 45165 6791\nDL-DNN 10075 231167 207286 1341 1103 138 1192426 27892\nDL-DNNsğœ 4236 51026 217193 984 1306 207 1178239 87991\nDL-BiLSTM - - 104152 1034 - - - -\nDL-BiLSTM-A - - 115111 1061 - - - -\nCardNet 2871 12809 52101 446 75 2 6822 3245\nCardNet-A 3044 11598 64831 427 64 3 16809 3269\nTable 4: MAPE (in percentage), best values highlighted in boldface.\nModel HM-ImageNet HM-PubChem ED-AMiner ED-DBLP JC-BMS JC-DBLPğ‘3EU-Glove 300 EU-Glove 50\nDB-SE 56.14 79.74 80.15 57.23 45.12 10.42 41.91 47.12\nDB-US 62.51 141.04 61.98 56.80 60.06 50.52 112.06 98.23\nTL-XGB 13.87 152.20 113.68 33.26 26.52 6.70 14.46 33.87\nTL-LGBM 14.12 110.22 115.88 30.29 21.39 10.71 17.49 37.54\nTL-KDE 85.57 179.39 105.17 60.23 27.59 37.79 52.38 59.84\nDL-DLN 20.72 174.69 73.48 39.10 42.50 6.54 21.67 33.95\nDL-MoE 11.93 49.47 57.79 31.81 16.37 4.10 11.94 26.82\nDL-RMI 12.36 50.57 52.81 32.24 15.02 4.78 5.48 15.03\nDL-DNN 14.24 198.36 51.36 34.12 28.41 5.11 7.24 17.57\nDL-DNNsğœ 13.00 46.43 53.82 30.91 19.67 5.81 9.19 21.95\nDL-BiLSTM - - 43.44 40.95 - - - -\nDL-BiLSTM-A - - 45.25 41.12 - - - -\nCardNet 8.41 35.66 42.26 22.53 11.25 3.18 4.04 11.85\nCardNet-A 9.63 36.57 44.78 23.07 13.94 3.05 4.58 12.71\n9.1.2 Models. Our models are referred to as CardNet and\nCardNet-A . The latter is equipped with the acceleration (Sec-\ntion 7). We compare with the following categories of methods:\n(1) Database methods: DB-SE , a specialized estimator for each\ndistance function (histogram [ 63] for HM, inverted index [ 36]\nforED, semi-lattice [ 46] for JC, and LSH-based sampling [ 76]\nforEU) and DB-US , which uniformly samples 1% records fromDand estimates cardinality using the sample. We do not con-\nsider higher sample ratios because 1% samples are already\nvery slow (Table 5). (2) Traditional learning methods: TL-XGB\n(XGBoost) [ 16],TL-LGBM (LightGBM) [ 38], and TL-KDE [57].\n(3) Deep learning methods: DL-DLN [78];DL-MoE [67];DL-\nRMI [43];DL-DNN , a vanilla FNN with four hidden layers;\nandDL-DNNs ğœ, a set of(ğœğ‘šğ‘ğ‘¥+1)independently learned\n10\n\ndeep neural networks, each against a threshold range (com-\nputed using the threshold transformation in Section 4). For\nED, we also have a method that replaces Î“inCardNet or\nCardNet-A with a character-level bidirectional LSTM [ 17],\nreferred to as DL-BiLSTM orDL-BiLSTM-A . Since the above\nlearning models need vectors (except for TL-KDE which is\nfed with original input) as input, we use the same feature\nextraction as our models on EDandJC. On HMandEU, they\nare fed with original vectors. As for other deep learning mod-\nels [31,41,55,56,60,61,70,74,77], when adapted for our\nproblem, [ 41] becomes a feature extraction by deep set [ 79]\nplus a regression by DL-DNN , while the others become ex-\nactly DL-DNN . We will show that deep set is outperformed by\nour feature extraction (Table 6, also observed when applying\nto other methods). Hence these models are not repeatedly\ncompared. Among the compared models, DB-SE ,TL-XGB ,\nTL-LGBM ,TL-KDE ,DL-DLN , and our models are monotonic.\nAmong the models involving FNNs, DL-MoE andDL-RMI\nare more complex than ours in most cases, depending on the\nnumber of FNNs and other hyperparameter tuning. DL-DNN\nis less complex than ours. DL-DNNs ğœis more complex.\n9.1.3 Hyperparameter Tuning. We use 256 hash functions for\nJaccarddistance,256(on EU-Glove 50)and512(on EU-Glove 300)\nhash functions for Euclidean distance. The VAE is a fully-\nconnected neural network, with three hidden layers of 256,\n128, and 128 nodes for both encoder and decoder. The activa-\ntion function is ELU, in line with [ 40]. The dimensionality of\ntheVAEâ€™s output is 40, 128, 128, 128, 64, 64, 64, 32 as per the\norder in Table 2. We use a fully-connected neural network\nwith four hidden layers of 512, 512, 256, and 256 nodes for both\nÎ¦andÎ¦â€². The activation function is ReLU . The dimensionality\nof distance embeddings is 5. The dimensionality of zğ‘–\nğ‘¥is 60.\nWe setğœ†in Equation 2 and ğœ†Î”in Equation 3 to both 0.1. The\nVAE is trained for 100 epochs. Our models are trained for 800\nepochs.\n9.1.4 Environments. The experiments were carried out on a\nserver with a Intel Xeon E5-2640 @2.40GHz CPU and 256GB\nRAM running Ubuntu 16.04.4 LTS. Non-deep models were\nimplemented in C++. Deep models were trained in Tensorflow,\nand then the parameters were copied to C++ implementations\nfor a fair comparison of estimation efficiency, in line with [ 43].\n9.2 Estimation Accuracy\nWe report the accuracies of various models in Tables 3 and 4,\nmeasured by MSE andMAPE .CardNet andCardNet-A report\nsimilar MSE andMAPE . They achieve the best performance\non almost all the datasets (except CardNet-A â€™sMAPE onED-\nAMiner ),showcasingthatthecomponentsinourmodeldesign\ncollectively lead to better accuracy. For the four distance func-\ntions, the MSE (of the better one of our two models) is at least\n02468101214161820\nThreshold0100101102103104105MSE\nCardNet\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN(a)MSE ,HM-ImageNet\n02468101214161820\nThreshold01020304050MAPE (%)\nCardNet\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN (b)MAPE ,HM-ImageNet\n012345678910\nThreshold102103104105106107MSE\nCardNet\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN\n(c)MSE ,ED-AMiner\n012345678910\nThreshold20406080100MAPE (%)\nCardNet\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN (d)MAPE ,ED-AMiner\n0.0 0.06 0.12 0.18 0.24 0.3 0.36\nThreshold0100101102103104MSE\nCardNet\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN\n(e)MSE ,JC-BMS\n0.0 0.06 0.12 0.18 0.24 0.3 0.36\nThreshold020406080100MAPE (%)\nCardNet\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN (f)MAPE ,JC-BMS\n0.0 0.12 0.24 0.36 0.48 0.6 0.72\nThreshold0100101102103104105106107108MSE\nCardNet\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN\n(g)MSE ,EU-Glove 300\n0.0 0.12 0.24 0.36 0.48 0.6 0.72\nThreshold020406080100MAPE (%)\nCardNet\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN (h)MAPE ,EU-Glove 300\nFigure 5: Accuracy v.s. threshold.\n1.5, 1.8, 4.1, and 2.1 times smaller than the best of the others,\nrespectively. The MAPE is reduced by at least 23.2%, 2.7%,\n25.6%, and 21.2% from the best of the others, respectively.\nIn general, deep learning methods are more accurate than\ndatabase and traditional learning methods. Among the deep\nlearning methods, DL-DLN â€™s performance is the worst, DL-\nMoE is in the middle, and DL-RMI is the runner-up to our\nmodels. The performance of DL-RMI relies on the models\n11\n\non upper levels. Although the neural networks on upper lev-\nels discretize output space into multiple regions, they tend\nto mispredict the cardinalities that are closest to the region\nboundaries. DL-DNN does not deliver good accuracy, and\nDL-DNNs ğœis even worse than in some cases due to overfitting.\nThis suggests that simply feeding deep neural networks with\ntraining data yields limited performance gain. DL-BiLSTM\nandDL-BiLSTM-A exhibit small MAPE onED-AMiner , but\nare outperformed by DL-RMI in the other cases, suggesting\nthey do not learn the semantics of edit distance very well.\nIn Figure 5, we evaluate the accuracy with varying thresh-\nolds on the four default datasets. We compare with the follow-\ning models: DB-US ,TL-XGB ,DL-DLN ,DL-MoE ,DL-RMI , the\nmore accurate or monotonic models out of each category. The\ngeneral trend is that the errors increase with the threshold,\nmeaning that larger thresholds are harder. The exceptions\nareMAPE on Hamming distance and MSE on edit distance.\nThe reason is that the cardinalities of some large thresholds\ntend to resemble for different queries, and regression models\nare more likely to predict well.\n9.3 Estimation Efficiency\nIn Table 5, we show the average estimation time. We also\nreport the time of running a state-of-the-art similarity se-\nlection algorithm [ 34,64] to process queries to obtain the\ncardinality (referred to as SimSelect ). The estimation time\nofCardNet is close to DL-RMI and faster than the database\nmethods and TL-KDE . Thanks to the acceleration technique,\nCardNet-A becomes the runner-up, and its speed is close to\nthe fastest model DL-DNN .CardNet-A is faster than running\nthe similarity selection algorithms by at least 24 times.\n9.4 Evaluation of Model Components\nWe evaluate the following components in our models: feature\nextraction, incremental prediction, variational auto-encoder\n(VAE), and dynamic training strategy. We use the following\nradio to measure the improvement by each component:\nğ›¾ğœ‰=ğœ‰(CardNet{-A}-C)âˆ’ğœ‰(CardNet{-A})\nğœ‰(CardNet{-A}-C),\nwhereğœ‰âˆˆ{MAPE,MSE mean q-error},CardNet{-A} is our\nmodel CardNet orCardNet-A , and CardNet{-A}-CisCardNet{-\nA}withcomponentCreplacedbyotheroptions;e.g., CardNet{-A}âˆ’VAE\nis our model with VAEreplaced. A positive ğ›¾ğœ‰means the com-\nponent has a positive effect in accuracy.\nWe consider the following replacement options: (1) For\nfeature extraction, we adopt a character-level bidirectional\nLSTM to transfer a string to a dense representation for edit\ndistance, a deep set model [ 79] to transfer a set to its represen-\ntation for Jaccard distance, and use the original record as input\nfor Euclidean distance. Hamming distance is not repeatedly\ntested as we use original vectors as input. (2) For incrementalprediction, we compare it with a deep neural network that\ntakes as input the concatenation of xâ€²and the embedding of ğœ\nand outputs the cardinality. (3) For VAE, we compare it with\nan option that directly concatenates the binary representa-\ntion and distance embeddings. (4) For dynamic training, we\ncompare it with using only MSLE as loss, i.e., removing the\nsecond term on the right side of Equation 3. We report the ğ›¾ğœ‰\nvalues on the four default datasets in Table 6. The effects of\nthe four components in our models are all positive, ranging\nfrom 5% to 93% improvement of MSE , 5% to 60% improve-\nment of MAPE , and 9% to 64% improvement of mean q-error.\nThe most useful component is incremental prediction, with\n38% to 93% performance improvement. This demonstrates\nthat using incremental prediction on deep neural networks\nis significantly better than directly feeding neural networks\nwith training data, in accord with what we have observed in\nTables 3 and 4.\n9.5 Number of Decoders\nIn Figure 6, we evaluate the accuracy by varying the number\nof decoders. In order to show the trend clearly, we use four\ndatasets with large lengths or dimensionality, whose statistics\nis given in Table 7. As seen from the experimental results, we\ndiscover that using the largest ğœmaxsetting does not always\nlead to the best performance. E.g., on HM-Youtube , the best\nperformance is achieved when ğœğ‘šğ‘ğ‘¥=326(327 decoders).\nWhen there are too few decoders, the feature extraction be-\ncomes lossy and cannot successfully capture the semantic\ninformation of the original distance functions. As the number\nof decoders increases, the feature extraction becomes more\neffective to capture the semantics. On the other hand, the\nperformance drops if we use an excessive number of decoders.\nThis is because given a query, the cardinality only increases at\na few thresholds (e.g., a threshold of 50 and 51 might produce\nthe same cardinality). Using too many decoders will involve\ntoo many non-increasing points, posing difficulty in learning\nthe regression model.\n9.6 Model Size\nTable 8 shows the storage sizes of the competitors on the four\ndefault datasets. DB-US does not need any storage and thus\nshows zero model size. TL-KDE has the smallest model size\namong the others, because it only stores the kernel instances\nfor estimation. For deep learning models, DL-DNN has the\nsmallest model size. Our model sizes range from 10 to 55 MB,\nsmaller than the other deep models except DL-DNN .\n9.7 Evaluation of Training\n9.7.1 Training Time. Table 9 shows the training times of var-\nious models on the four default datasets. Traditional learning\n12\n\nTable 5: Average estimation time (milliseconds).\nModel HM-ImageNet HM-PubChem ED-AMiner ED-DBLP JC-BMS JC-DBLPğ‘3EU-Glove 300 EU-Glove 50\nSimSelect 5.12 14.68 6.22 10.51 4.24 5.89 14.60 8.52\nDB-SE 6.20 8.50 7.64 10.01 4.67 5.78 8.45 7.34\nDB-US 1.17 3.60 1.26 6.08 1.75 1.44 6.23 1.05\nTL-XGB 0.41 0.41 0.36 0.41 0.71 0.65 0.69 0.60\nTL-LGBM 0.32 0.34 0.31 0.33 0.52 0.48 0.49 0.47\nTL-KDE 0.83 0.96 4.73 1.24 0.97 2.35 1.28 1.22\nDL-DLN 0.42 0.84 0.83 6.43 0.66 0.57 1.23 0.46\nDL-MoE 0.21 0.32 0.35 0.59 0.31 0.36 0.41 0.28\nDL-RMI 0.37 0.46 0.41 0.57 0.39 0.45 0.68 0.57\nDL-DNN 0.09 0.11 0.15 0.25 0.11 0.14 0.15 0.12\nDL-DNNsğœ 0.26 0.58 0.26 0.62 0.27 0.34 0.42 0.38\nDL-BiLSTM - - 3.11 5.22 - - - -\nDL-BiLSTM-A - - 3.46 5.80 - - - -\nCardNet 0.36 0.45 0.39 0.69 0.55 0.48 0.67 0.50\nCardNet-A 0.13 0.19 0.21 0.29 0.18 0.20 0.24 0.19\nTable 6: Performance of model components.\nMetric Dataset Feature Extraction Incremental Prediction Variational Auto-encoder Dynamic Training\nCardNet CardNet-A CardNet CardNet-A CardNet CardNet-A CardNet CardNet-A\nğ›¾MSEHM-ImageNet - - 84% 86% 13% 17% 20% 28%\nED-AMiner 49% 44% 57% 62% 11% 14% 14% 21%\nJC-BMS 31% 34% 83% 76% 34% 26% 21% 28%\nEU-Glove 300 5% 8% 93% 82% 18% 23% 26% 17%\nğ›¾MAPEHM-ImageNet - - 47% 46% 14% 16% 15% 16%\nED-AMiner 5% 6% 52% 51% 19% 18% 18% 22%\nJC-BMS 26% 16% 51% 60% 40% 29% 22% 34%\nEU-Glove 300 32% 21% 54% 48% 23% 14% 32% 27%\nTable 7: Statistics of datasets with high dimensionality.\nDataset Source Process Data Type Attribute # Record â„“ğ‘šğ‘ğ‘¥â„“ğ‘ğ‘£ğ‘” Distance ğœƒğ‘šğ‘ğ‘¥\nHM-Youtube Youtube_Faces [7] normalize real-valued vector video 346,194 1770 1770 Euclidean 0.8\nHM-GIST 2048 GIST [8] Spectral Hashing [73] binary vector image 982,677 2048 2048 Hamming 512\nED-DBLP [4] - string publication title 1,000,000 199 72.49 edit 20\nJC-Wikipedia Wikipedia [9] 3-gram string abstract 1,150,842 732 496.06 Jaccard 0.4\nmodels are faster to train. Our models spend 2 â€“ 4 hours, simi-\nlar to other deep models. DL-DNNs ğœis the slowest since its\nhas(ğœğ‘šğ‘ğ‘¥+1)independently learned deep neural networks.\n9.7.2 Varying the Size of Training Data. In Figure 7, we show\nthe performance of different models by varying the scale of\ntraining examples from 20% to 100% of the original training\ndata. We only plot MSE due to the page limitation. All the\nmodels have worse performance with fewer training data, but\nour models are more robust, showing moderate accuracy loss.\n9.8 Evaluation of Updates\nWe generate a stream of 200 update operations, each with an\ninsertion or deletion of 5 records. We compare three methods:\nIncLearn that utilizes incremental learning on CardNet-A ,Re-\ntrain that retrains CardNet-A for each operation, and +SampleTable 8: Model size (MB).\nModel HM-ImageNet ED-AMiner JC-BMS EU-Glove 300\nDB-SE 10.4 31.2 39.4 86.2\nDB-US 0.6 0.5 0.6 3.4\nTL-XGB 36.4 36.4 48.8 63.2\nTL-LGBM 32.6 32.8 45.4 60.4\nTL-KDE 4.5 1.5 3.6 18.1\nDL-DLN 28.4 75.4 28.6 64.4\nDL-MoE 16.8 52.5 35.4 52.5\nDL-RMI 57.7 84.8 54.6 66.1\nDL-DNN 5.0 14.5 8.7 9.8\nDL-DNNsğœ 105.4 154.2 183.2 158.4\nCardNet 9.6 40.2 16.4 23.8\nCardNet-A 16.2 54.5 22.8 35.3\nthat performs sampling ( DB-US ) on the updated data and add\n13\n\n83 164 245 327 408 489 571\nNumber of Decoders105106\n105MSE\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN(a)MSE ,HM-Youtube\n83 164 245 327 408 489 571\nNumber of Decoders1520253035MAPE (%)\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN (b)MAPE ,HM-Youtube\n35 47 57 74 103 171 257 513\nNumber of Decoders105106MSE\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN\n(c)MSE ,HM-GIST 2048\n35 47 57 74 103 171 257 513\nNumber of Decoders102030MAPE (%)\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN (d)MAPE ,HM-GIST 2048\n6 7 11 21\nNumber of Decoders102103MSE\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN\n(e)MSE ,ED-DBLP\n6 7 11 21\nNumber of Decoders304050MAPE (%)\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN (f)MAPE ,ED-DBLP\n104 206 309 411 513 616\nNumber of Decoders101102MSE\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN\n(g)MSE ,JC-Wikipedia\n104 206 309 411 513 616\nNumber of Decoders246810MAPE (%)\nCardNet-A\nTL-XGBDL-RMI\nDL-MoEDB-US\nDL-DLN (h)MAPE ,JC-Wikipedia\nFigure 6: Accuracy v.s. number of decoders.\nthe result to that of CardNet-A on the original data. Figure 8\nplots the MSE onHM-ImageNet andEU-Glove 300. We ob-\nserve that in most cases, IncLearn has similar performance to\nRetrain and performs better than +Sample , especially when\nthere are more updates. Compared to Retrain that spends\nseveral hours to retrain the model (Table 9), IncLearn only\nneeds 1.2 â€“ 1.5 minutes to perform incremental learning.Table 9: Training time (hours).\nModel HM-ImageNet ED-AMiner JC-BMS EU-Glove 300\nTL-XGB 0.8 0.8 1.0 1.2\nTL-LGBM 0.6 0.5 0.7 0.6\nTL-KDE 0.3 0.3 0.5 0.6\nDL-DLN 4.1 4.6 4.5 4.9\nDL-MoE 2.7 3.5 3.8 3.8\nDL-RMI 3.2 3.7 3.9 4.4\nDL-DNN 1.2 1.5 1.7 1.8\nDL-DNNsğœ 15 12 12 20\nCardNet 3.3 3.4 4.1 4.2\nCardNet-A 1.7 2.2 2.4 2.6\n20 40 60 80 100\nTraining Size0.51.01.52.02.5MSE (104)\nCardNet\nCardNet-ATL-XGB\nDL-RMIDL-MoE\nDL-DLN\n(a)MSE ,HM-ImageNet\n20 40 60 80 100\nTraining Size105106MSE\nCardNet\nCardNet-ATL-XGB\nDL-RMIDL-MoE\nDL-DLN (b)MSE ,ED-AMiner\n20 40 60 80 100\nTraining Size102103MSE\nCardNet\nCardNet-ATL-XGB\nDL-RMIDL-MoE\nDL-DLN\n(c)MSE ,JC-BMS\n20 40 60 80 100\nTraining Size104105106MSE\nCardNet\nCardNet-ATL-XGB\nDL-RMIDL-MoE\nDL-DLN (d)MSE ,EU-Glove 300\nFigure 7: Accuracy v.s. training data size.\n0 50 100 150 200\nSequence of Operations2345MSE (103)\nRetrain IncLearn +Sample\n(a)MSE ,HM-ImageNet\n0 50 100 150 200\nSequence of Operations0.81.01.21.41.61.82.0MSE (104)\nRetrain IncLearn +Sample (b)MSE ,EU-Glove 300\nFigure 8: Evaluation of updates.\n9.9 Evaluation of Long-tail Queries\nWe compare the performance on long-tail queries, i.e., those\nhaving exceptionally large cardinalities ( â‰¥1000). They are\noutliers and a hard case of estimation. We divide queries into\ndifferent cardinality groups by every thousand. Figure 9 shows\ntheMSE on the four default datasets by varying cardinality\n14\n\n[1, 2) [2, 3) [3, 4)â‰¥4\nCardinality Range (103)104105106MSECardNet\nCardNet-A\nDL-DLNTL-XGB\nDB-USDL-RMI\nDL-MoE(a)MSE ,HM-ImageNet\n[1, 2) [2, 4) [4, 6)â‰¥6\nCardinality Range (103)100101102MSE (105)CardNet\nCardNet-A\nDL-DLNTL-XGB\nDB-USDL-RMI\nDL-MoE (b)MSE ,ED-AMiner\n[0.5, 1) [1.0, 1.2) [1.2, 1.4)â‰¥1.4\nCardinality Range (103)105106MSECardNet\nCardNet-A\nDL-DLNTL-XGB\nDB-USDL-RMI\nDL-MoE\n(c)MSE ,JC-BMS\n[1, 2) [2, 4) [4, 6)â‰¥6\nCardinality Range (103)104105106107MSECardNet\nCardNet-A\nDL-DLNTL-XGB\nDB-USDL-RMI\nDL-MoE (d)MSE ,EU-Glove 300\nFigure 9: Evaluation of long-tail queries.\ngroups. The MSE increases with cardinality for all the meth-\nods. This is expected since the larger the cardinality is, the\nmore exceptional is the query (see Figure 1(b)). Our models\noutperform the others by 1 to 3 orders of magnitude. More-\nover, the MSE growth rates of our models w.r.t. cardinality\nare smaller than the others, suggesting that our models are\nmore robust against long-tail queries.\n9.10 Generalizability\nTo show the generalizability of our models, we evaluate the\nperformance on the queries that significantly differ from\nthe records in the dataset and the training data. To prepare\nsuch queries, we first perform a ğ‘˜-medoids clustering on the\ndataset, and then randomly generate 10,000 out-of-dataset\nqueries and pick the top-2,000 ones having the largest sum\nof squared distance to the ğ‘˜centroids. To prepare an out-of-\ndataset query, we generate a random query ğ‘and accept it\nonly if it is not in the dataset D. Specifically, (1) for binary\nvectors,ğ‘[ğ‘–]âˆ¼uniform{0,1}; (2) for strings, since AMiner\nand DBLP both contain author names, we take a random\nauthor name from the set (DBLP\\AMiner); (3) for sets, we\ngenerate a length ğ‘™âˆ¼uniform[ğ‘™min,ğ‘™max], whereğ‘™minandğ‘™max\nare the minimum and maximum set sizes in D, respectively,\nand then generate a random set of length ğ‘™sampled from\nthe universe of all the elements in D; (4) for real vectors,\nğ‘[ğ‘–]âˆ¼uniform[âˆ’1,1]. Figure 10 shows the MSE on the four\ndefault datasets by varying cardinality groups. The same trend\nis witnessed as we have seen for long-tail queries. Due to the\nuse of VAEand dynamic training, our models always perform\n[0, 4) [4, 8) [8, 12)â‰¥12\nCardinality Range (102)102103104105MSECardNet\nCardNet-A\nDL-DLNTL-XGB\nDB-USDL-RMI\nDL-MoE(a)MSE ,HM-ImageNet\n[0, 4) [4, 8) [8, 12)â‰¥12\nCardinality Range (102)103104105106MSECardNet\nCardNet-A\nDL-DLNTL-XGB\nDB-USDL-RMI\nDL-MoE (b)MSE ,ED-AMiner\n[0, 1) [1, 2) [2, 3)â‰¥3\nCardinality Range (102)103104105MSECardNet\nCardNet-A\nDL-DLNTL-XGB\nDB-USDL-RMI\nDL-MoE\n(c)MSE ,JC-BMS\n[0, 4) [4, 8) [8, 12)â‰¥12\nCardinality Range (102)101102103104105106MSECardNet\nCardNet-A\nDL-DLNTL-XGB\nDB-USDL-RMI\nDL-MoE (d)MSE ,EU-Glove 300\nFigure 10: Generalizability.\nExact\nCardNet-ADL-RMI TL-XGBDB-USMean0100101102103Total Processing Time (s)\n(a) Time, AMiner-Publication\nExact\nCardNet-ADL-RMI TL-XGBDB-USMean0100101102Total Processing Time (s) (b) Time, AMiner-Author\nExact\nCardNet-ADL-RMI TL-XGBDB-USMean0100101102103Total Processing Time (s)\n(c) Time, IMDB-Movie\nExact\nCardNet-ADL-RMI TL-XGBDB-USMean0100101102103Total Processing Time (s) (d) Time, IMDB-Actor\nFigure 11: Conjunctive euclidean distance query â€“\nquery processing time.\nbetter than the other methods, especially for Jaccard distance.\nThe results demonstrate that our models generalize well for\nout-of-dataset queries.\n9.11 Performance in a Query Optimizer\n9.11.1 Conjunctive Euclidean Distance Query. We consider a\ncase study of conjunctive queries. Four textual datasets with\n15\n\nTable 10: Statistics of datasets for conjunctive query optimizer.\nDataset Source Attributes # Records ğœƒminğœƒmax\nAMiner-Publication [3] title, authors, affiliations, venue, abstract 2,092,356 0.2 0.5\nAMiner-Author [3] name, affiliations, research interests 1,712,433 0.2 0.5\nIMDB-Movie [10] title type, primary title, original title, genres 6,250,486 0.2 0.5\nIMDB-Actor [10] primary name, primary profession 9,822,710 0.2 0.5\nExact\nCardNet-ADL-RMI TL-XGBDB-USMean0.00.20.40.60.81.0Precision\n(a) Time, AMiner-Publication\nExact\nCardNet-ADL-RMI TL-XGBDB-USMean0.00.20.40.60.81.0Precision (b) Time, AMiner-Author\nExact\nCardNet-ADL-RMI TL-XGBDB-USMean0.00.20.40.60.81.0Precision\n(c) Time, IMDB-Movie\nExact\nCardNet-ADL-RMI TL-XGBDB-USMean0.00.20.40.60.81.0Precision (d) Time, IMDB-Actor\nFigure 12: Conjunctive euclidean distance query â€“\nquery planning precision.\nmultiple attributes are used (statistics shown in Table 10).\nGiven a dataset, we convert each attribute to a word embed-\nding (768 dimensions) by Sentence-BERT [ 65]. A query is\na conjunction of Euclidean distance predicates (a.k.a. high-\ndimensional range predicates [ 51]) on normalized word em-\nbeddings, with thresholds uniformly sampled from [0.2,0.5];\ne.g., â€œ EU(name) â‰¤ 0.25AND EU(affiliations) â‰¤ 0.4AND\nEU(research interests) â‰¤ 0.45â€, where EU()measures the\nEuclidean distance between the embeddings of a query and a\ndatabase record. Such queries can be used for entity matching\nas blocking rules [20, 28].\nTo process a query, we first find the records that satisfy\none predicate by index lookup (by a cover tree [ 34]), and then\ncheck other predicates on the fly. We estimate for each predi-\ncate and pick the one with the smallest cardinality for index\nlookup. We compare CardNet-A with: (1) DB-US , sampling\nratio tuned for fastest query processing speed; (2) TL-XGB ;\n(3)DL-RMI ; (4)Mean , which returns the same cardinality for\na given threshold; each threshold is quantized to an integer\nin[0,255]using the threshold transformation in Section 4.4,\nand then we offline generate 10,000 random queries for eachinteger in[0,255]and take the mean; and (5) Exact , an oracle\nthat instantly returns the exact cardinality.\nFigure 11 reports the processing time of 1,000 queries. The\ntime is broken down to cardinality estimation (in blue) and\npostprocessing (in red, including index lookup and on-the-fly\ncheck). We observe: (1) more accurate cardinality estimation\n(as we have seen in Section 9.2) contributes to faster query\nprocessing speed; (2) cardinality estimation spends much\nless time than postprocessing; (3) uniform estimation ( Mean )\nhas the slowest overall speed; (4) deep learning performs\nbetter than database and traditional learning methods in both\nestimation and overall speeds; (5) except Exact ,CardNet-A\nis the fastest and most accurate in estimation, and its overall\nspeed is also the fastest (by 1.7 to 3.3 times faster than the\nrunner-up DL-RMI ) and even close to Exact .\nIn Figure 12, we show the precision of query planning,\ni.e., the percentage of queries on which a method picks the\nfastest (excluding estimation time) plan. The result is in accord\nwith what we have observed in Figure 11. The precision of\nCardNet-A ranges from 90% to 96%, second only to Exact . The\ngap between CardNet-A andDL-RMI is within 20%, but results\nin the speedup of 1.7 to 3.3 times, showcasing the effect of\ncorrect query planning. We also observe that Exact is not 100%\nprecise, though very close, indicating that smallest cardinality\ndoes not always yield the best query plan. Future work on\ncost estimation may further improve query processing.\n9.11.2 Hamming Distance Query. We also consider a case\nstudy of the GPH algorithm [ 63], which processes Hamming\ndistance queries over high dimensional vectors through a\nquery optimizer. To cope with the high dimensionality, the al-\ngorithmanswersaquery ğ‘bydividingitinto ğ‘šnon-overlapping\nparts and allocating a threshold (with dynamic programming)\nto each part using the pigeonhole principle. Each part itself is\na Hamming distance selection query that can be answered by\nbit enumeration and index lookup. The union of the answers\nof theğ‘šparts are the candidates of ğ‘. To allocate thresholds\nand hence to achieve small query processing cost, a query\noptimizer is used to minimize the sum of estimated cardi-\nnalities of the ğ‘šparts. We compare CardNet-A with the fol-\nlowing options: (1) Histogram , the histogram estimator in\n[63]; (2) DL-RMI ; (3)Mean , an naive estimator that returns\nthe same cardinality for a given threshold (for each thresh-\nold, we offline generate 10,000 random queries and take the\nmean); and (4) Exact , an oracle that instantly returns the exact\n16\n\nTable 11: Statistics of datasets for Hamming distance query optimizer.\nDataset Source Process Domain # Records â„“ğœƒğ‘šğ‘ğ‘¥\nHM-PubChem [2] - biological sequence 1,000,000 881 32\nHM-UQVideo [69] multiple feature hashing [69] video embedding 1,000,000 128 12\nHM-fastText [11] spectral hashing [73] word embedding 999,999 256 24\nHM-EMNIST [18] image binarization image pixel 814,255 784 32\n8 16 24 32\nThreshold100101102103Total Processing Time (s)Exact\nCardNet-AHistogram\nMeanDL-RMI\n(a) Time, HM-PubChem\n8 16 24 32\nThreshold10âˆ’210âˆ’1100101102103Total Processing Time (s)Exact\nCardNet-AHistogram\nMeanDL-RMI (b) Time, HM-UQVideo\n8 16 24 32\nThreshold10âˆ’1100101102103Total Processing Time (s)Exact\nCardNet-AHistogram\nMeanDL-RMI\n(c) Time, HM-fastText\n8 16 24 32\nThreshold100101102103Total Processing Time (s)Exact\nCardNet-AHistogram\nMeanDL-RMI (d) Time, HM-EMNIST\nFigure13:Hammingdistancequeryâ€“queryprocessing\ntime.\ncardinality. We use four datasets. The statistics is given in\nTable 11, where â„“denotes the dimensionality. The records are\nconverted to binary vectors as per the process in the table. We\nset each part to 32 bits (the last part is smaller if not divisible).\nFigure 13 reports the processing time of 1,000 queries by\nvarying thresholds from 8 to 32 on the four datasets. The time\nis broken down to threshold allocation time (in white, which\ncontains cardinality estimation) and postprocessing time (in\nred). The performance of CardNet-A is very close to Exact and\nfaster than Histogram by 1.6 to 4.9 times speedup. DL-RMI is\nslightlyfasterthan Histogram .Mean ismuchslower(typically\none order of magnitude) than other methods, suggesting that\ncardinality estimation is important for this application. The\nthreshold allocation (including cardinality estimation) spends\nless time than the subsequent query processing. CardNet-A\nalso performs better than Histogram in threshold allocation.\nThis is because (1) CardNet-A itself is faster in estimation, and\n(2) the more accuracy makes the dynamic programming-based\nallocation terminate earlier.\nNext we fix the threshold at 50% of the maximum threshold\nin Figure 13 and vary the size of the histogram. Figure 14\n10âˆ’210âˆ’1100101102\nHistogram Size (MB)102103Total Processing Time (s)\nHistogram\nCardNet-AMean DL-RMI(a) Time, HM-PubChem\n10âˆ’310âˆ’210âˆ’1100101\nHistogram Size (MB)101102Total Processing Time (s)\nHistogram\nCardNet-AMean DL-RMI (b) Time, HM-UQVideo\n10âˆ’210âˆ’1100101102\nHistogram Size (MB)102103Total Processing Time (s)\nHistogram\nCardNet-AMean DL-RMI\n(c) Time, HM-fastText\n10âˆ’210âˆ’1100101102\nHistogram Size (MB)102103Total Processing Time (s)\nHistogram\nCardNet-AMean DL-RMI (d) Time, HM-EMNIST\nFigure 14: Hamming distance query â€“ varying model\nsize.\nreports the average query processing time. The positions of\nother methods (except Exact ) are also marked in the figure.\nAs expected, the query processing time reduces when using\nlarger histograms. However, the speed is still 1.6 to 2.6 times\nslower than CardNet-A even if the size of the histogram ex-\nceeds the model size of CardNet-A (see the rightmost point\nofHistogram ). This result showcases the superiority of our\nmodel compared to the traditional database method in an\napplication of cardinality estimation of similarity selection.\n10 CONCLUSION\nWe investigated utilizing deep learning for cardinality esti-\nmation of similarity selection. Observing the challenges of\nthis problem and the advantages of using deep learning, we\ndesigned a method composed of two components. The feature\nextraction component transforms original data and thresh-\nold to Hamming space, hence to support any data types and\ndistance functions. The regression component estimates the\ncardinality in the Hamming space based on a deep learning\nmodel. We exploited the incremental property of cardinality\nto output monotonic results and devised a set of encoder and\n17\n\ndecoders that estimates the cardinality for each distance value.\nWe developed a training strategy tailored to our model and\nproposed optimization techniques to speed up estimation. We\ndiscussed incremental learning for updates. The experimental\nresults demonstrated the accuracy, efficiency, and generaliz-\nability of the proposed method as well as the effectiveness of\nintegrating our method to a query optimizer.\nACKNOWLEDGMENTS\nThisworkwassupportedbyJSPS16H01722,17H06099,18H04093,\nand 19K11979, NSFC 61702409, CCF DBIR2019001A, NKRDP\nof China 2018YFB1003201, ARC DE190100663, DP170103710,\nand DP180103411, and D2D CRC DC25002 and DC25003. The\nTitan V was donated by Nvidia. We thank Rui Zhang (the\nUniversity of Melbourne) for his precious comments.\nREFERENCES\n[1] http://www.image-net.org/.\n[2] https://pubchem.ncbi.nlm.nih.gov/.\n[3] https://aminer.org/.\n[4] https://dblp2.uni-trier.de/.\n[5] https://www.kdd.org/kdd-cup/view/kdd-cup-2000.\n[6] https://nlp.stanford.edu/projects/glove/.\n[7] http://www.cs.tau.ac.il/~wolf/ytfaces/index.html.\n[8] http://horatio.cs.nyu.edu/mit/tiny/data/index.html.\n[9] https://wiki.dbpedia.org/services-resources/documentation/datasets.\n[10] https://www.imdb.com/interfaces/.\n[11] https://fasttext.cc/docs/en/english-vectors.html.\n[12] C. Anagnostopoulos and P. Triantafillou. Query-driven learning for\npredictive analytics of data subspace cardinality. ACM Transactions on\nKnowledge Discovery from Data , 11(4):47, 2017.\n[13] Y. Bengio, A. C. Courville, and P. Vincent. Representation learning: A\nreview and new perspectives. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , 35(8):1798â€“1828, 2013.\n[14] W. Cai, M. Balazinska, and D. Suciu. Pessimistic cardinality estimation:\nTighter upper bounds for intermediate join cardinalities. In SIGMOD ,\npages 18â€“35, 2019.\n[15] Z. Cao, M. Long, J. Wang, and S. Y. Philip. Hashnet: Deep learning to\nhash by continuation. In ICCV , pages 5609â€“5618, 2017.\n[16] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In\nKDD , pages 785â€“794, 2016.\n[17] J. P. Chiu and E. Nichols. Named entity recognition with bidirectional\nlstm-cnns. Transactions of the Association for Computational Linguistics ,\n4:357â€“370, 2016.\n[18] G. Cohen, S. Afshar, J. Tapson, and A. van Schaik. EMNIST: extending\nMNIST to handwritten letters. In IJCNN , pages 2921â€“2926, 2017.\n[19] H. Daniels and M. Velikova. Monotone and partially monotone neural\nnetworks. IEEE Transactions on Neural Networks , 21(6):906â€“917, 2010.\n[20] S. Das, P. S. G. C., A. Doan, J. F. Naughton, G. Krishnan, R. Deep, E. Ar-\ncaute, V. Raghavendra, and Y. Park. Falcon: Scaling up hands-off crowd-\nsourced entity matching to build cloud services. In SIGMOD , pages\n1431â€“1446, 2017.\n[21] A. Dasgupta, X. Jin, B. Jewell, N. Zhang, and G. Das. Unbiased estimation\nof size and other aggregates over hidden web databases. In SIGMOD ,\npages 855â€“866, 2010.\n[22] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive\nhashing scheme based on p-stable distributions. In SOCG , pages 253â€“\n262, 2004.[23] B. Ding, S. Das, R. Marcus, W. Wu, S. Chaudhuri, and V. R. Narasayya.\nAI meets AI: leveraging query executions to improve index recommen-\ndations. In SIGMOD , pages 1241â€“1258, 2019.\n[24] A. Dutt, C. Wang, A. Nazi, S. Kandula, V. R. Narasayya, and S. Chaudhuri.\nSelectivity estimation for range predicates using lightweight models.\nPVLDB , 12(9):1044â€“1057, 2019.\n[25] M. M. Fard, K. Canini, A. Cotter, J. Pfeifer, and M. Gupta. Fast and\nflexible monotonic functions with ensembles of lattices. In NIPS , pages\n2919â€“2927, 2016.\n[26] E. Garcia and M. Gupta. Lattice regression. In NIPS , pages 594â€“602,\n2009.\n[27] A. Gionis, P. Indyk, and R. Motwani. Similarity search in high dimen-\nsions via hashing. In VLDB , pages 518â€“529, 1999.\n[28] C. Gokhale, S. Das, A. Doan, J. F. Naughton, N. Rampalli, J. W. Shavlik,\nand X. Zhu. Corleone: hands-off crowdsourcing for entity matching. In\nSIGMOD , pages 601â€“612, 2014.\n[29] M. Gupta, A. Cotter, J. Pfeifer, K. Voevodski, K. Canini, A. Mangylov,\nW. Moczydlowski, and A. Van Esbroeck. Monotonic calibrated in-\nterpolated look-up tables. The Journal of Machine Learning Research ,\n17(1):3790â€“3836, 2016.\n[30] P. J. Haas and A. N. Swami. Sequential Sampling Procedures for Query\nSize Estimation , volume 21. ACM, 1992.\n[31] S. Hasan, S. Thirumuruganathan, J. Augustine, N. Koudas, and G. Das.\nMulti-attribute selectivity estimation using deep learning. CoRR ,\nabs/1903.09999, 2019.\n[32] M. Heimel, M. Kiefer, and V. Markl. Self-tuning, GPU-accelerated kernel\ndensity models for multidimensional selectivity estimation. In SIGMOD ,\npages 1477â€“1492, 2015.\n[33] O. Ivanov and S. Bartunov. Adaptive cardinality estimation. arXiv\npreprint arXiv:1711.08330 , 2017.\n[34] M. Izbicki and C. R. Shelton. Faster cover trees. In ICML , pages 1162â€“\n1170, 2015.\n[35] H. Jiang. Uniform convergence rates for kernel density estimation. In\nICML , pages 1694â€“1703, 2017.\n[36] L. Jin, C. Li, and R. Vernica. SEPIA: estimating selectivities of approxi-\nmate string predicates in large databases. The VLDB Journal , 17(5):1213â€“\n1229, 2008.\n[37] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Tho-\nrat, F. B. ViÃ©gas, M. Wattenberg, G. Corrado, M. Hughes, and J. Dean.\nGoogleâ€™s multilingual neural machine translation system: Enabling\nzero-shot translation. Transactions of the Association for Computational\nLinguistics , 5:339â€“351, 2017.\n[38] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T. Liu.\nLightgbm: A highly efficient gradient boosting decision tree. In NIPS ,\npages 3149â€“3157, 2017.\n[39] R. Kemker, M. McClure, A. Abitino, T. L. Hayes, and C. Kanan. Measuring\ncatastrophic forgetting in neural networks. In AAAI , pages 3390â€“3398,\n2018.\n[40] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv\npreprint arXiv:1312.6114 , 2013.\n[41] A. Kipf, T. Kipf, B. Radke, V. Leis, P. A. Boncz, and A. Kemper. Learned\ncardinalities: Estimating correlated joins with deep learning. In CIDR ,\n2019.\n[42] T. Kraska, M. Alizadeh, A. Beutel, E. H. Chi, A. Kristo, G. Leclerc, S. Mad-\nden, H. Mao, and V. Nathan. Sagedb: A learned database system. In\nCIDR , 2019.\n[43] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for\nlearned index structures. In SIGMOD , pages 489â€“504, 2018.\n[44] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and I. Stoica. Learning to\noptimize join queries with deep reinforcement learning. arXiv preprint\narXiv:1808.03196 , 2018.\n18\n\n[45] H. Lee, R. T. Ng, and K. Shim. Extending q-grams to estimate selectivity\nof string matching with low edit distance. In VLDB , pages 195â€“206,\n2007.\n[46] H. Lee, R. T. Ng, and K. Shim. Power-law based estimation of set\nsimilarity join size. PVLDB , 2(1):658â€“669, 2009.\n[47] H. Lee, R. T. Ng, and K. Shim. Similarity join size estimation using\nlocality sensitive hashing. PVLDB , 4(6):338â€“349, 2011.\n[48] V. Leis, B. Radke, A. Gubichev, A. Kemper, and T. Neumann. Cardinality\nestimation done right: Index-based join sampling. In CIDR , 2017.\n[49] G. Li, J. He, D. Deng, and J. Li. Efficient similarity join and search on\nmulti-attribute data. In SIGMOD , pages 1137â€“1151, 2015.\n[50] P. Li and C. KÃ¶nig. b-bit minwise hashing. In WWW , pages 671â€“680,\n2010.\n[51] K. Lin, H. V. Jagadish, and C. Faloutsos. The tv-tree: An index structure\nfor high-dimensional data. The VLDB Journal , 3(4):517â€“542, 1994.\n[52] R. J. Lipton and J. F. Naughton. Query size estimation by adaptive\nsampling. In PODS , pages 40â€“46, 1990.\n[53] H. Liu, M. Xu, Z. Yu, V. Corvinelli, and C. Zuzarte. Cardinality estimation\nusing neural networks. In CSSE , pages 53â€“59, 2015.\n[54] R. Marcus and O. Papaemmanouil. Deep reinforcement learning for\njoin order enumeration. In aiDM@SIGMOD , pages 3:1â€“3:4, 2018.\n[55] R. C. Marcus, P. Negi, H. Mao, C. Zhang, M. Alizadeh, T. Kraska, O. Pa-\npaemmanouil, and N. Tatbul. Neo: A learned query optimizer. PVLDB ,\n12(11):1705â€“1718, 2019.\n[56] R. C. Marcus and O. Papaemmanouil. Plan-structured deep neural\nnetwork models for query performance prediction. PVLDB , 12(11):1733â€“\n1746, 2019.\n[57] M. Mattig, T. Fober, C. Beilschmidt, and B. Seeger. Kernel-based cardi-\nnality estimation on metric data. In EDBT , pages 349â€“360, 2018.\n[58] A. Mazeika, M. H. BÃ¶hlen, N. Koudas, and D. Srivastava. Estimating\nthe selectivity of approximate string queries. ACM Transactions on\nDatabase Systems , 32(2):12, 2007.\n[59] M. McCloskey and N. J. Cohen. Catastrophic interference in connec-\ntionist networks: The sequential learning problem. In Psychology of\nlearning and motivation , volume 24, pages 109â€“165. Elsevier, 1989.\n[60] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi. Learning state rep-\nresentations for query optimization with deep reinforcement learning.\nInDEEM@SIGMOD , pages 4:1â€“4:4, 2018.\n[61] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi. An empirical analysis\nof deep learning for cardinality estimation. CoRR , abs/1905.06425, 2019.\n[62] H. Park and L. Stefanski. Relative-error prediction. Statistics & Proba-\nbility Letters , 40(3):227â€“236, 1998.\n[63] J. Qin, Y. Wang, C. Xiao, W. Wang, X. Lin, and Y. Ishikawa. GPH:\nSimilarity search in hamming space. In ICDE , pages 29â€“40, 2018.\n[64] J. Qin and C. Xiao. Pigeonring: A principle for faster thresholded\nsimilarity search. PVLDB , 12(1):28â€“42, 2018.\n[65] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using\nsiamese bert-networks. In EMNLP-IJCNLP , pages 3980â€“3990, 2019.[66] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,\nA. Karpathy, A.Khosla, M. Bernstein,A. C. Berg,and L. Fei-Fei. ImageNet\nLarge Scale Visual Recognition Challenge. International Journal of\nComputer Vision , 115(3):211â€“252, 2015.\n[67] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and\nJ. Dean. Outrageously large neural networks: The sparsely-gated\nmixture-of-experts layer. arXiv preprint arXiv:1701.06538 , 2017.\n[68] K. Sohn, H. Lee, and X. Yan. Learning structured output representation\nusing deep conditional generative models. In NIPS , pages 3483â€“3491,\n2015.\n[69] J. Song, Y. Yang, Z. Huang, H. T. Shen, and J. Luo. Effective multiple\nfeature hashing for large-scale near-duplicate video retrieval. IEEE\nTransactions on Multimedia , 15(8):1997â€“2008, 2013.\n[70] J. Sun and G. Li. An end-to-end learning-based cost estimator. PVLDB ,\n13(3):307â€“319, 2019.\n[71] I. Trummer. Exact cardinality query optimization with bounded execu-\ntion cost. In SIGMOD , pages 2â€“17, 2019.\n[72] M. Tschannen, O. Bachem, and M. Lucic. Recent advances in\nautoencoder-based representation learning. CoRR , abs/1812.05069,\n2018.\n[73] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In NIPS , pages\n1753â€“1760, 2009.\n[74] L. Woltmann, C. Hartmann, M. Thiele, D. Habich, and W. Lehner. Cardi-\nnality estimation with local deep learning models. In aiDM@SIGMOD ,\npages 5:1â€“5:8, 2019.\n[75] W. Wu, J. F. Naughton, and H. Singh. Sampling-based query re-\noptimization. In SIGMOD , pages 1721â€“1736, 2016.\n[76] X. Wu, M. Charikar, and V. Natchu. Local density estimation in high\ndimensions. In ICML , pages 5293â€“5301, 2018.\n[77] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y. Duan, X. Chen, P. Abbeel, J. M.\nHellerstein, S. Krishnan, and I. Stoica. Selectivity estimation with deep\nlikelihood models. CoRR , abs/1905.04278, 2019.\n[78] S. You, D. Ding, K. Canini, J. Pfeifer, and M. Gupta. Deep lattice networks\nand partial monotonic functions. In NIPS , pages 2981â€“2989, 2017.\n[79] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov,\nand A. J. Smola. Deep sets. In NIPS , pages 3391â€“3401, 2017.\n[80] H. Zhang and Q. Zhang. Embedjoin: Efficient edit similarity joins via\nembeddings. In KDD , pages 585â€“594, 2017.\n[81] J. Zhang, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng, J. Xing, Y. Wang,\nT. Cheng, L. Liu, M. Ran, and Z. Li. An end-to-end automatic cloud\ndatabase tuning system using deep reinforcement learning. In SIGMOD ,\npages 415â€“432, 2019.\n[82] W. Zhang, K. Gao, Y. Zhang, and J. Li. Efficient approximate nearest\nneighbor search with integrated binary codes. In ACM Multimedia ,\npages 1189â€“1192, 2011.\n[83] Z. Zhao, R. Christensen, F. Li, X. Hu, and K. Yi. Random sampling over\njoins revisited. In SIGMOD , pages 1525â€“1539, 2018.\n19",
  "textLength": 88175
}