{
  "paperId": "052484fba7efd2c1bd6704728008d75965dce6e6",
  "title": "Local Differentially Private Frequency Estimation based on Learned Sketches",
  "pdfPath": "052484fba7efd2c1bd6704728008d75965dce6e6.pdf",
  "text": "Local Di\u000berentially Private Frequency Estimation\nbased on Learned Sketches\nMeifan Zhanga, Sixin Lina;b, Lihua Yina\u0003\naCyberspace Institute of Advanced Technology, Guangzhou\nUniversity, Guangzhou, 510006, China\nbPeng Cheng Laboratory, , Shenzhen, 518000, China\nAbstract\nSketches are widely used for frequency estimation of data with a large domain.\nHowever, sketches-based frequency estimation faces more challenges when con-\nsidering privacy. Local di\u000berential privacy (LDP) is a solution to frequency esti-\nmation on sensitive data while preserving the privacy. LDP enables each user to\nperturb its data on the client-side to protect the privacy, but it also introduces\nerrors to the frequency estimations. The hash collisions in the sketches make\nthe estimations for low-frequent items even worse. In this paper, we propose a\ntwo-phase frequency estimation framework for data with a large domain based\non an LDP learned sketch, which separates the high-frequent and low-frequent\nitems to avoid the errors caused by hash collisions. We theoretically proved\nthat the proposed method satis\fes LDP. Our method is more accurate than\nthe state-of-the-art frequency estimation methods under LDP including Apple-\nCMS, Apple-HCMS and FLH. The experimental results verify the performance\nof our method.\nKeywords: Frequency estimation, Local di\u000berential privacy, Sketches, Query\nprocessing\n\u0003Meifan Zhang and Sixin Lin should be considered joint \frst author.\nPreprint submitted to Information Sciences November 22, 2022arXiv:2211.01138v2  [cs.CR]  20 Nov 2022\n\n1. Introduction\nFrequency estimation is a traditional and important problem in data ana-\nlytics. At present, to solve the frequency estimation problem, it is necessary\nto consider not only the e\u000eciency and space cost but also the risk of privacy\nleakage. In the process of collecting data from users and aggregating the fre-\nquencies, some sensitive information may leak. For example, when the studies\ndo frequency statistics on some diseases or medicines, the data providers do not\nwant to reveal their true illnesses or medications. Thus, we need to protect\nusers' privacy while estimating the frequencies.\nHowever, privacy-preserving for big data frequency estimation is not an easy\ntask. Conventional encryption techniques are too costly for big data, thus, they\ncannot meet the requirement of fast response for big data analytics. In the\nmeantime, the data owners often use generalization and suppression techniques\nto achieve anonymization requirements and protect their data privacy [1]. But\nthe disadvantage of anonymization techniques is that they do not provide a\nmeasure of privacy loss in big data analysis. In recent years, di\u000berential privacy\n(DP) [2] is a popular privacy-preserving solution due to its strong mathematical\nboundary of the leaked privacy. It adds noises to the aggregations to avoid the\nleakage of individual privacy. But it is di\u000ecult to \fnd a trusted third party\nto aggregate the data from a large number of clients. Local di\u000berential privacy\n(LDP) [3] is a solution to this problem, which locally perturbs the raw data\nbefore sending it to the server. In this way, the server has no access to the raw\ndata, thus, the privacy of each client is protected. Many companies, such as the\nGoogle [4, 5], Apple [6], and Microsoft [7] adopt LDP to collect and aggregate\nsensitive data from users.\nHowever, frequency estimation for big data under LDP still faces some chal-\nlenges. On the one hand, it is di\u000ecult to get su\u000eciently accurate frequency\nestimation for big data with a large domain. Methods such as the Basic RAP-\nPOR [5], OUE [8], and OLH [8] can not handle the data with a large domain.\nSketches-based methods use hash functions to map the data with a large do-\n2\n\nmain to a sub-linear space. They are tailored for streaming data analysis on\narchitectures even with limited memory such as single-board computers that\nare widely exploited for IoT and edge computing [9]. The methods such as\nRAPPOR [4] and Apple-CMS [6] use the bloom-\flters or sketches to reduce\nthe domain size, but the accumulated errors due to hash collisions reduces the\nestimation accuracy as the data grows rapidly. Separating the storage of high-\nfrequent and low-frequent items is a way to avoid hash collisions. But it is\ndi\u000ecult for the server to distinguish them since the perturbed values of dif-\nferent items are su\u000eciently similar according to LDP. On the other hand, it\nis challenging to separate the storage of high-frequent items and low-frequent\nitems while preserving privacy. Both the server and the clients have the risk\nof privacy leakage when treating the high-frequent and low-frequent items in\ndi\u000berent ways. Thus, methods separating the items by their frequencies while\nremaining their privacy are required.\nTo tackle the \frst challenge, we try to improve the accuracy of frequency\nestimation under LDP by avoiding the collisions between the high-frequent items\nand low-frequent items. However, an individual user with no prior knowledge\ncannot identify whether an item is high-frequent or not. Instead of enabling the\nserver to distinguish the perturbed values, we try to enable each client to identify\nwhether its item is a high-frequent one or not. We train a frequency model\nbased on the aggregations of the perturbed values from some sample clients.\nThe clients other than those in the samples can use the model to distinguish\nwhether their items are high-frequent ones or not. Since the frequency of a\nhigh-frequent item can be accurately estimated based on the samples, we use\nthe model to replace the storage of high-frequent items and leave the sketch for\nthe low-frequent items. Thus, the hash collisions between high-frequent items\nand low-frequent items can be reduced.\nTo tackle the second challenge, we let the clients encode the high-frequent\nitems and low-frequent items in di\u000berent ways while satisfying the LDP. A naive\nidea to reduce the hash collisions between high-frequent items and low-frequent\nitems is to let the clients only send the low-frequent items to the server since\n3\n\nPhase 1 frequency -model training\nPerturb (Encode (Item) )\nPhase 2 low-frequent items  aggregation\nHigh -frequency: Perturb ({ -1}m)\nLow-frequency: Perturb (Encode (Item) )Frequency Model gItem x\nHigh -frequency: predict g(x) based on model\nLow-frequency: estimate(x)  based on sketchQueryFigure 1: The idea of LDPLCM framework.\nthe high-frequent items can be predicted according to the frequency model.\nHowever, if the client does not send information to the server, it reveals that its\nitem a high-frequent one. To avoid this privacy leakage, the clients with high-\nfrequent items should also send some values to the server, and these values must\nbe di\u000ecult to distinguish from the perturbed values of the low-frequent items.\nTo avoid the errors caused by involving the perturbed high-frequent items in\nthe sketch, we propose a method to make the perturbed values of high-frequent\nitems uniformly disperse in the sketch, so that we can accurately evaluate and\neliminate the impact of these items from the estimations. We prove that this\nmethod satis\fes LDP and reduces the variances of estimations for low-frequent\nitems.\nIn this paper, we propose a two-phase LDP frequency estimation algorithm\nas shown in Figure 1. The \frst phase trains a frequency model based on the\naggregations of the perturbed values from some sample clients. The model\nenables each client to distinguish high-frequent items and low-frequent items.\nIt also enables the server to estimate the frequencies of high-frequent items by\nthe predictions according to the model. In the second phase, each client uses\nthe frequency model to identify whether its item is high-frequent or not. The\nclients encode the high-frequent and low-frequent items in di\u000berent ways while\nsatisfying LDP. The server uses a sketch to aggregates the perturbed data from\nclients and estimate the frequencies of low-frequent items.\nThe main contributions of this work are summarized as follows:\n•We present a local di\u000berentially private frequency estimation framework\n4\n\nbased on learned sketches (LDPLCM). It can estimate the frequency of\nsensitive data with a large domain and guarantee the high utility of esti-\nmations.\n•We proved that the proposed method satis\fes LDP. It can treat the high-\nfrequent items and low-frequent items in di\u000berent ways to avoid the hash-\ncollisions while preserving the privacy.\n•We proved that the proposed method is more accurate for low-frequent\nitems than the LDP frequency estimation method Apple-CMS.\n•We conduct extensive experiments on both synthetic and real-world datasets.\nThe experimental results show that the proposed method outperforms\nthe state-of-the-art LDP frequency estimation algorithms including Apple-\nCMS, Apple-HCMS and FLH.\nThe remainder of this paper is organized as follows. In section 2, we survey the\nrelated work for this paper. In section 3, we introduce some preliminaries. Sec-\ntion 4 presents the LDPLCM algorithm. In section 5, the experimental results\nshow the performance of the proposed algorithm. In section 6, we conclude the\npaper with future directions.\n2. Related Works\nFrequency estimation can be applied in many \felds such as \fnding frequent\nitems [10, 11, 12], hierarchical heavy hitters [13, 14], network measurements [15,\n16]. In [17], the authors use frequency estimation and top-k items identi\fcation\nto perform network monitoring.\nDi\u000berential privacy is extensively studied for protecting users' privacy while\nenabling big data analysis. Frequency estimation of sensitive data poses a risk of\nprivacy leakage. For example, users report their symptoms to the disease control\ndepartment through their mobile phones in the medical Internet of Things.\nThere has a risk of privacy leakage in the report because the third-party data\nrecipients are not completely trusted. The central model of DP also faces the\n5\n\nsame problem that no trusted third-party server can be found. Thus, LDP [3]\nhas been implemented to protect clients' privacy in frequency estimation, such\nas RAPPOR [4], Apple-CMS [6], Apple-HCMS [6], pure LDP [8], k-RR [18].\nIn [4], Erlingsson et.al combine randomized response with Bloom \flters [19] to\nsatisfy\u000f-LDP [20]. They use the permanent random response instead of clients'\ninitial data and calculate the instantaneous randomized response to perturb the\npermanent random response. Their extending work from RAPPOR is to learn\nthe joint distributions and associations between unknown data dictionaries [5].\nThese ways enhance the di\u000eculty of tracking clients' activity for attackers, but\nthey complicate the decoding process on the server. To this end, Apple-CMS\nand Apple-HCMS estimate the frequencies based on the perturbed data items\nin sketches and directly calculate the average value from hash entries without\ndecoding. The advantage of Apple-HCMS over Apple-CMS is the reduction of\ncommunication costs. These two methods can be used for data in a large domain\nbut they fail to decrease the estimation errors caused by the hash collisions.\nFinding the trade-o\u000b between privacy budget and accuracy can improve\nthe data utility in frequency estimation [21, 22, 23, 24, 25]. Kairouz et.al [18]\npropose and prove that the hashed k-RR is optimal in the low privacy regime\ncompared with RAPPOR. Wang et.al [8] generalize the RAPPOR and design\nthree perturbation methods including Direct Encoding(DE), Optimized Unary\nEncoding(OUE), Optimized Local Hashing(OLH) in the pure LDP framework.\nThey prove an unbiased estimate and \fnd the best parameters to minimize the\nvariance of estimation. In [23], Murakami et.al consider the data sensitivity of\nclients and propose the utility-optimized LDP(ULDP) mechanisms to maximize\nthe utility. Jia et.al [26] associate the prior knowledge with LDP in Calibrate\nframework to count the true items, and model the distribution probability from\nprior knowledge. To improve the e\u000eciency, Flash Local Hashing(FLH) and\nHadamard Response(HR) [27] restrict the clients' choices of k0hash functions.\nThey are faster than OLH and Apple-HCMS by introducing a matrix k0\u0002\nm(k0\u001cn) to store the perturbed data items, which are suitable for small data\ndomains.\n6\n\nSketches have been re\fned over the past two decades as common tools for\nfrequency estimation on data with a large domain. Many typical sketches are\nproposed including Count-Sketches [28], Count-Min sketches [29], Augmented\nsketches [30], CU sketches [31]. These sketches have common structures, they\nuse hash functions to map a large amount of data to a two-dimensional array to\nreduce the space cost. But they also face the errors caused by hash collisions, the\nhash collisions between high-frequent items and low-frequent items introduce\ngreat errors for the estimation of low-frequent items. To reduce the errors,\nmany methods are proposed to distinguish the high-frequent items from the\nlow-frequent ones. Augmented sketches [30] add a \flter on the top of the Count-\nMin sketch to store the frequencies of top-k items. The cold \flter [32] and\nHeavyGuardian [33] separate the cold items and hot items into two stages.\nHowever, separating the high-frequent items from the low-frequent items is\nnon-trivial under LDP due to the privacy-preserving. The LDPMiner [34] is a\ntwo-phase heavy hitter mining algorithm. It gathers a candidate set of heavy\nhitters in the \frst phase and estimates the frequencies of these candidates in\nthe second phase. But it only focuses on the frequency estimation for the top-k\nfrequent items. Sending the frequency property of each item to the clients is\ncostly for data with a large domain. Inspired by the learned index [35], some\nlearning-based frequency estimation methods are proposed in recent years [36,\n37, 38]. We also attempt to use a lightweight model to learn the frequency\nproperties of the sensitive data from the clients. However, the learning-based\nsketch cannot be directly applied to the frequency estimation under LDP. We\nneed to separate the high-frequent items and low-frequent items while preserving\nprivacy. In this paper, we propose a two-phase frequency estimation algorithm\nbased on the learned sketch under LDP. In the \frst phase, we train a lightweight\nmodel to predict the frequency of each item based on the aggregations of the\nperturbed values from some sample clients. In the second phase, the rest of the\nclients identify and encode the high-frequent and low-frequent items in di\u000berent\nways according to the frequency model. The proposed method satis\fes LDP, and\nit is more accurate than the state-of-the-art sketch-based frequency estimation\n7\n\nmethod Apple-CMS.\n3. Preliminaries\nIn this section, we provide some preliminaries including LDP and sketches\nfor frequency estimation.\n3.1. Local Di\u000berential Privacy\nDP has been accepted as the de facto standard for data privacy. But DP is\nnot applicable when there is no trusted aggregator. LDP is proposed to handle\nthis obstacle. In the local setting for DP, there are a large number of users and\none aggregator. To protect the privacy of individual users, each user locally\nperturbs its private data and sends the perturbed value to the aggregator.\nDe\fnition 1. (\u000f-Local Di\u000berential Privacy). An algorithm A(\u0001)satis\fes\u000f-LDP\nif and only if for \u000f>0and any inputs v1;v22Dfrom the dataset D, we have\n8T2A(D) :Pr[A(v1)2T]\u0014e\u000fPr[A(v2)2T]; (1)\nwhereA(D)denotes the set of all possible outputs of the algorithm A.\nLDP ensures that the outputs of the random algorithm with di\u000berent inputs\nare similar enough, thus the perturbed values will not leak the privacy of the\ninputs.\n3.2. Sketches\nSketches support count queries over data with a large domain, and they\nsummarize a large amount of data into sub-linear space. In recent years, many\ntypical sketches such as the count sketches [28] and the count-min sketches [29]\nare proposed to provide more accurate estimations. The Apple-CMS and Apple-\nHCMS algorithms are also designed based on the count-min sketches to provide\nthe frequency estimation under LDP. Therefore, we review these two state-of-\nthe-art sketches including the count-min sketch and the count sketch.\n8\n\n3.2.1. Count-Min Sketch\nA Count-Min sketch [29] with parameters ( \u000fCM;\u000eCM) is represented by a\ntwo-dimensional array counts with width mand depthk:count [1;1],...,count [k;m].\nGiven parameters ( \u000fCM;\u000eCM), setm=de=\u000fCMeandk=dln(1=\u000eCM)e. Each\ncell of the array is initialised with a zero. The dhash functions h1;:::;hd:\nf1;:::;ng!f 1;:::;mgare used to update the count in each cell of the array.\nThe parameters ( \u000fCM;\u000eCM) have nothing to do with the parameter \u000fof DP. We\nadd subscripts \\CM\" to the parameters of the Count-Min sketch to distinguish\nthem. Count-Min sketches has two basic operations: update and estimate .\nUpdate(x):\ncount [i;hi(x)] count [i;hi(x)] + 1 (2)\nEstimation( x):\nmini2[1;k]fcount [i;hi(x)]g (3)\nThis is an over-estimation due to the hash collisions ^ ai\u0015ai, whereaiis\nthe count of the item xand ^aiis the estimation. The estimation has an upper\nbound, with the probability at least (1- \u000eCM), ^ai\u0014ai+\u000fCMkAk1, wherekAk1=\nPn\ni=1jaij[29].\nThe Apple-CMS [6] adopts a variance of Count-Min sketch to encode and\naggregate the sensitive data. It encodes the value into a one-hot vector with a\nhash function randomly chosen from the h1;:::;hk. It then perturbs the vector\nand sends the perturbed vector to the server. The server adds the vector to the\ncorresponding line of the sketch. It estimates the frequency of an item xwith\nthe appropriate correction of meani2[1;k]fcount [i;hi(x)]g.\n3.2.2. Count Sketch\nThe Count Sketch [28] has the same structure with the count-min sketch.\nThe only di\u000berence between them is that the count sketch adds another hash\nfunctionsimapping each item to f\u00001;1gfor each line of the array. That is,\nsi:x!f\u0000 1;1g. Its operations update and estimate are as follows.\nUpdate(x):\ncount [i;hi(x)] count [i;hi(x)] +si(x) (4)\n9\n\nEstimate(x):\nmeani2[1;d]fcount [i;hi(x)]\u0001si(x)g (5)\n4. LDP Learned Count-Mean Sketches (LDPLCM) for Frequency Es-\ntimation\n4.1. The Framework of LDPLCM\nIn this section, we will introduce the framework of the proposed method\nLDPLCM.\nThe hash collisions between high-frequent items and low-frequent items largely\nincrease the estimation errors, especially when the data domain is very large.\nTherefore, separating the high-frequent items and low-frequent items is a way to\nreduce the hash collisions, which is adopted in many sketches-based frequency\nestimation methods. However, separating the high-frequent and low-frequent\nitems is non-trivial under LDP. Each client under LDP does not know whether\nits value is a high-frequent one or not.\nOur main point is to reduce the errors caused by the hash collisions between\nthe high-frequent items and low-frequent items while satisfying LDP. We will\n\frst introduce the proposed framework and then detail how to achieve the goal\nof reducing errors by solving a series of tasks.\nThe framework of our LDPLCM algorithm is shown in Figure 2. In the \frst\nphase, we randomly choose some sample clients, and we encode and perturb\neach value of the sample clients in the same way as the Apple-CMS. The server\nconstructs a sketch with the perturbed values and trains a frequency model g\nmapping each data dito its frequency f(di) estimated by the sketch. It \fnds a\nboundaryPto separate the high-frequent and low-frequent items, which enables\nthe clients in the second phase to distinguish the high-frequent items and low-\nfrequent items. In the second phase, the remaining clients distinguish whether\ntheir items are high-frequent or not according to the frequency model gand\nthe boundary P. Ifg(di)< P, thendiis regarded as a low-frequent item and\ntreated in the same way as phase 1. If g(di)\u0015P, thendiis regarded as a\n10\n\nClients Server\nd1\nd2\nd|S|...-1-1+1-1-1-1\n-1-1-1-1-1+1\n+1-1-1-1-1-1Perturb\nPerturb\nPerturbConstruct Sketch...Construct \nFrequency Model\ng:dif(di)\ndi\ndn...-1-1+1-1-1-1\n-1-1-1-1-1-1low-frequent\nhigh-frequentPerturb\nPerturb\n...Construct Sketch\n-1-1-1-1+1-1\n-1-1-1-1-1-1low-frequent\nhigh-frequentPerturb\nPerturbPhase 1\nPhase 2Query(d)\nlow-frequenthigh-frequentFigure 2: The framework of LDPLCM.\nhigh-frequent item, and it is encoded with a vector of f\u00001gm. In this way, the\nhigh-frequent items and low-frequent items are di\u000berentially encoded, but they\nare perturbed in the same way. The server in phase 2 empties the sketch and\nreconstructs it with the perturbed values from the clients. When querying the\nfrequency of an item d, it still uses the frequency model to distinguish whether it\nis a high-frequent item or not. If it is a high-frequent item, the server predicts its\nfrequency according to the model, otherwise, the server estimates its frequency\nbased on the sketch.\nWe then detail how the framework reduces the estimation errors while pre-\nserving privacy by solving a series of tasks.\nThe \frst task is to enable each client to distinguish whether its item is a\nhigh-frequent one or not. Inspired by the LDPMiner, which \fnds the heavy-\nhitters in the \frst phase and estimates the frequencies of the heavy-hitters in\nthe second phase, we also propose a two-phase algorithm. In the \frst phase, we\nattempt to learn the frequency property of items according to the aggregations\n11\n\nof some sample clients. To avoid the signi\fcant cost of sending the frequency\nproperty of a large number of items, we train a lightweight model to predict the\nfrequency of each item. In the second phase, we enable the clients other than\nthe samples to distinguish the high-frequent items from the low-frequent ones\naccording to the frequency model learned in the \frst phase.\nThe second task is to avoid the errors caused by the hash collisions between\nthe high-frequent and low-frequent items. Even though the clients can distin-\nguish the high-frequent items and low-frequent items according to the model,\nit is still non-trivial for the server to separate the storage of high-frequent and\nlow-frequent ones. The reason is that each client perturbs its value before send-\ning it to the server. To separate the storage of items with di\u000berent frequency\nproperties, we reuse the frequency model to replace the storage of high-frequent\nitems and leave the sketch for the low-frequent ones. Since the model is trained\nbased on the aggregations of the sample clients, its predictions for high-frequent\nitems are more reliable than those for low-frequent ones. In this way, only the\nlow-frequent items are aggregated in the sketch in the second phase, thus the\nhash collisions between high-frequent ones and low-frequent ones are avoided.\nHowever, it causes another problem, the client leaks information to the server\nwhether its item is high-frequent or not if it only sends the perturbed low-\nfrequent items to the server. At such, the client also needs to send some infor-\nmation to the server to avoid leaking privacy.\nThe third task is to protect the privacy of each client while correcting the\nerrors caused by involving the perturbed values of high-frequent items into the\nsketch. This sounds contradictory, because DP works by making the probability\nof getting the same output from di\u000berent inputs similar. In order to achieve the\ngoal of correcting the errors, we encode all the high-frequent items to the same\nvectorf\u00001gm. Thus, even the high-frequent items are encoded, perturbed, and\nsent to the server, they cause no collisions with the low-frequent items. We\nprove that both the encoding and perturbing satisfy LDP. Since all the high-\nfrequent items are encoded into f\u00001gm, and each bit is perturbed with the same\nprobability, the errors caused by high-frequent items are uniformly dispersed in\n12\n\nthe sketch. Therefore, we can accurately evaluate and eliminate these errors\nfrom the estimations.\nIn this way, our method achieves both privacy-preserving and errors reduc-\ntion. On the one hand, it protects the privacy of each client according to LDP.\nOn the other hand, it avoids the errors caused by hash collisions between high-\nfrequent items and low-frequent items.\nAlgorithm 1 LDPLCM\n1:S sample clients with a sampling rate r .Phase 1: Train the frequency\nmodel\n2:foreach client Siwith datadido\n3:<~v(i);ji> LDPLCMclient( di,phase = 1)\n4: Send<~v(i);ji>to Server\n5:end for\n6:Server receives DataInput f<~v(1);j1>,<~v(2);j2>,...,<~v(jSj);jjSj>g\n7:Public frequency model g, and boundary P LDPLCMserver-\nconstruction( DataInput ,phase = 1)\n8:foreach of the clients other than the sample clients do .Phase 2:\nConstruct the sketch\n9:<~v(i);ji> LDPLCMclient( di,phase = 2)\n10: Send<~v(i);ji>to Server\n11:end for\n12:Server receives DataInput f<~v(1);j1>,<~v(2);j2>,...,<~v(n);jn>g\n13:Reinitialize M2f0gk\u0002m\n14:Get sketch M LDPLCMserver-construction( DataInput ,phase = 2)\n15:return:M,g,P\nThe pseudo-code of this framework is shown in Algorithm 1. The algorithm\nconstructs the frequency model in phase 1 (line 1-7). It \frst gets some sample\nclients (line 1). Each sample client chooses a hash function hjito encode its\nvalue and sends the perturbed value ~ v(i)along with the index of the chosen hash\nfunction to the server (line 2-5). The server trains the frequency model g, and\n13\n\ncomputes the frequency boundary Pbased on the aggregations of the perturbed\nvalues from the sample clients. The algorithm constructs the sketch in phase\n2 with the clients other than the samples (line 8-14). The client-side treats\nhigh-frequent items and low-frequent items in di\u000berent ways (line 9), which are\nintroduced later. Finally, the server constructs the sketch with the perturbed\nvalues from the clients in phase 2. We will introduce the client-side algorithm\nLDPLCMclient and the server-side algorithm LDPLCMserver in the remaining\npart of this section.\n4.2. Client-side algorithm of LDPLCM\nAs introduced in the framework, the client-side algorithm has two phases.\nIn phase 1, each sample client takes the same way as Aclient -CMS [6] to encode\nand perturb its value. In phase 2, each of the remaining clients identi\fes whether\nits value is high-frequent or not according to the frequency model. After that,\nthe clients encode the high-frequent items and low-frequent items in di\u000berent\nways to reduce the estimation errors caused by hash collisions.\nThe pseudo-code of the client-side algorithm is shown in Algorithm 2. In\nphase 1, the algorithm uses the client-side algorithm of Apple-CMS [6] to get\nthe perturbed values of each sample client (line 1-2). In phase 2, if the item d\nof the client is predicted as a low-frequent item, the client encodes and perturbs\nit in the same way as in phase 1 (line 4-5). If dis a high-frequent item, then\nit is encoded as a vector f\u00001gm(line 8). The vector vis then perturbed by\nmultiplying each of its bits with (-1) with a probability1\ne\u000f=2+1(line 10).\nWe give an example to show the di\u000berence between the encoded value of a\nhigh-frequent item and that of a low-frequent item in phase 2.\nExample 1. Suppose there is a high-frequent item dhigh and a low-frequent item\ndlow. The encoded value of dhigh is undoubtedly [-1,-1,-1,-1,-1,-1]. Suppose the\nlength of the sketch is m= 6 and the hash value of dlowishj(dlow) = 2 , thus\nthe encoded value of dlowis [-1,-1,+1,-1,-1,-1].\nWe can learn from this example that the high-frequent items have no hash\ncollisions with the low-frequent items in this way, since the high frequent items\n14\n\ndo not actually use the hash functions for encoding. The di\u000berence between the\nencoded value of a high-frequent item and a low-frequent item is at most 1 bit\nin our algorithm. Such a di\u000berence is smaller than that of two encoded values in\nthe Apple-CMS. Since the Apple-CMS satis\fes \u000f-LDP, intuitively, our algorithm\nalso satis\fes \u000f-LDP. We then formally prove that the client-side algorithm of\nLDPLCM satis\fes \u000f-LDP.\nAlgorithm 2 LDPLCMclient\nInput: data d2D,phase , privacy budget \u000f, sketch parameters ( m,\nk), frequency model g, threshold P\nOutput: perturbed value ~v, hash index j\n1:ifphase == 1 then .Phase 1\n2: ~v;j Aclient -CMS (d,\u000f,m,k) [6]\n3:else .Phase 2\n4: ifg(d)<Pthen . dis a low-frequent value\n5: ~v;j Aclient -CMS (d,\u000f,m,k) [6]\n6: else . dis a high-frequent value\n7: Samplejuniformly at random from [ k]\n8: Initialize a vector v f\u0000 1gm\n9: forifrom 1 tomdo .Perturbv\n10: ~v[i] v[i]\u0001(\u00001) with probability1\ne\u000f=2+1\n11: ~v[i] v[i]\u0001(+1) with probabilitye\u000f=2\ne\u000f=2+1\n12: end for\n13: end if\n14:end if\n15:return: ~v,j\nTheorem 1. The algorithm LDPLCMclient satis\fes \u000f-LDP.\nProof. Since the algorithm Aclient -CMS in reference [6] satis\fes \u000f-LDP, we just\nneed to prove that the phase 2 of Algorithm 2 satis\fes \u000f-LDP. As the input of all\nthe high-frequent items are the same, we only need to prove that the perturbed\n15\n\nresults of a high-frequent item dand a low-frequent item d0are su\u000eciently\nsimilar. We suppose the encode of dandd0arevandv0, respectively. According\nto the algorithm, each bit of vis -1, that is, the only di\u000berence between vand\nv0is on thehj(d0)-th bit, i.e., v[hj(d0)] =\u00001 andv0[hj(d0)] = 1. We denote\nAlgorithm 2 by Ain the following proof.\nPr[A(d) = (~v;j)]\nPr[A(d0) = (~v;j)]=Pr[perturb (v[hj(d0)]) = ~v[hj(d0)]]\nPr[perturb (v0[hj(d0)]) = ~v[hj(d0)]]=Pr[perturb (\u00001) = ~v[hj(d0)]]\nPr[perturb (1) = ~v[hj(d0)]]\n(6)\nSince the algorithm perturbs v[i] by multiplying (-1) and v[i] with a probability\n1\ne\u000f=2+1,\ne\u0000\u000f<e\u0000\u000f=2\u0014Pr[perturb (\u00001) = ~v[hj(d0)]]\nPr[perturb (1) = ~v[hj(d0)]]\u0014e\u000f=2<e\u000f(7)\nThus, the Algorithm 2 satis\fes \u000f-LDP.\n4.3. Server-side algorithm of LDPLCM\nThe server-side of LDPLCM also has two phases. The main point of phase 1\nis to build a frequency model based on the aggregations of the values from the\nsample clients. Since the frequency estimation based on the sampling are highly\naccurate for the high-frequent items, we use the frequency model to replace the\nstorage of these high-frequent items. We leave the task of approximately storing\nand estimating the frequencies of low-frequent items to the sketch in phase 2.\nThe pseudo-code of the server-side algorithm is shown in Algorithm 3. In\nphase 1, the server constructs the sketch with the perturbed inputs from the\nsample clients (line 2). Then, the algorithm randomly chooses tsample items\nfrom the data domain (line 3). We use the sketch Mto estimate the frequency\nof each item diaccording to the server-side algorithm of Apple-CMS (line 5).\nSince the server in phase 1 only gets information from the sample clients, the\nfrequency of each item in each client should be scaled with1\nr, whereris the\nsampling rate of phase 1. The algorithm then trains the frequency model gby\nmapping each dito its estimated frequency ^f(di) (line 7). We calculate the\npredictions of each item diand sort the predictions in descending order (line 8).\nWe compute the frequency boundary Pseparating the high-frequent items and\n16\n\nlow-frequent items by accumulating the frequencies of high-frequent items until\nit reaches\u0012times the sum of all the data (line 9-10). In phase 2, the algorithm\nconstructs the sketch Mwith the perturbed inputs of the clients other than the\nsamples (line 12).\nAlgorithm 3 LDPLCMserver\nInput: data d, privacy budget \u000f, sketch parameters ( m,k), ratio of\nhigh-frequent items \u0012, sampling rate r\nOutput: sketch M, frequency boundary P, frequency model g\n1:ifphase == 1 then .Phase 1\n2: Construct sketch Mwith inputs from clients.\n3:fd1;d2;:::;dtg samples randomly chosen from the data domain.\n4: forifrom 1 totdo\n5: ^f(di) Aserver -CMS (di)=r\n6: end for\n7: Train frequency model g:di!^f(di). .Frequency Model g\n8:SortedFre Sorti2[1;t]fg(di)g\n9:id maxfpjPp\ni=1SortedFre [i]\u0014\u0012\u0001Pt\ni=1SortedFre [i]g.Frequency\nBoundaryP\n10:P SortedFre [id]\n11:else ifphase == 2 then .Phase 2\n12: Construct sketch Mwith inputs from clients.\n13:end if\nAfter these two phases, we get a model predicting the frequencies for the\nhigh-frequent items and a sketch approximately estimating the frequencies of\nthe low-frequent items. The LDPLCM estimates the frequency of high-frequent\nitems and low-frequent items in di\u000berent ways. The high-frequent items are\npredicted according to the model, and the low-frequent items are estimated\nbased on the sketch. It is worth noting that we need to eliminate the error\ncaused by high-frequent items from the sketch-based estimation since the LD-\nPLCM involves some dummy values of high-frequent items into the sketch to\n17\n\navoid privacy leaks. As each bit of the dummy values is \ripped with the same\nprobability and the line to insert a perturbed dummy value is chosen randomly,\nthe perturbed dummy values almost identically in\ruent each cell of the sketch.\nIn addition, we can evaluate the impact of involving all these dummy values of\nthe high-frequent items on each cell of the sketch, since the total frequencies\nof high-frequent items can be computed by \u0012\u0001n, where\u0012is the ratio of the\ntotal frequencies of high-frequent items to the total frequencies of all the items.\nAt such, we can accurately evaluate these errors and eliminate them from the\nestimation.\nThe pseudo-code of our estimator is shown in Algorithm 4. If the model-\nbased prediction g(d) exceeds the boundary P, then the value dis judged to be\na high-frequent value. Thus, its estimated frequency is the prediction g(d) (line\n2). Otherwise, the algorithm estimates the frequency of a low-frequent value\naccording to the sketch M(line 4).\nAlgorithm 4 LDPLCM-estimator\nInput: data d, sketch parameters ( m,k), frequency model g, bound-\naryP, ratio of high-frequent items \u0012\nOutput: frequency estimation ^f(d)\n1:ifg(d)>Pthen\n2: ^f(d) =g(d)\n3:else\n4: ^f(d) =m\nm\u00001(1\nkPk\nl=1Ml;hl(d)\u0000(1\u0000\u0012)n\nm)\n5:end if\n6:return: ^f(d)\nThe following theorem proves that the output of our LDPLCM-estimator is\nan unbiased estimate of the frequency for a low-frequent item.\nTheorem 2. The estimated frequency of a low-frequent item provided by the\nLDPLCM is a unbiased estimate of f(d), i.e., E[m\nm\u00001(1\nkPk\nl=1Ml;hl(d)\u0000(1\u0000\n\u0012)n\nm)] =f(d).\n18\n\nProof. The LDPLCM estimates the frequency of a low-frequent item das^f(d).\nWe \frst analyze the contribution of each data entry d(i)to the estimation ^f(d).\nLetPerturb (v(i)) = (\u00001)\u0001v(i)with probabilitye\u000f=2\n1+e\u000f=2andJ\u0018Uniform [k].\nThe encoding vector of a high-frequent item is v(i)2f\u0000 1gm, and the encoding\nvector of a low-frequent item is \u00001 everywhere except at position hj(d(i)) for\nrecordi. We useM(j;hj(d))(i)to denote the contribution of the ith data to the\njline andhj(d) column of the sketch M.\nM(j;hj(d))(i)=k(c\u000fPerturb(v(i)[hj(d)]) + 1\n2) 1fJ=jg; (8)\nwherec\u000f=e\u000f=2+1\ne\u000f=2\u00001, andM(j;hj(d))(i)is nonzero only when J=j.\nThen, we analyze the expectation of E[M(j;hj(d))(i)] under di\u000berent condi-\ntions of the ith entryd(i).\n(1) Ifd(i)is a high-frequent item,\nE[M(j;hj(d))(i)] =k(c\u000fPerturb(v(i)[hj(d)]) + 1\n2)PrfJ=jg (9)\n=E[c\u000fPerturb(\u00001) + 1\n2] = 0 (10)\n(2)d(i)is a low-frequent item, and d(i)=d,\nE[M(j;hj(d))(i)] =k(c\u000fPerturb(v(i)[hj(d)]) + 1\n2)PrfJ=jg= 1 (11)\n(12)\n(3)d(i)is a low-frequent item, and d(i)6=d\nE[M(j;hj(d))(i)] =k(c\u000fPerturb(v(i)[hj(d)]) + 1\n2)PrfJ=jg (13)\n= (1\u00001\nm)E[c\u000fPerturb(\u00001) + 1\n2] +1\nmE[c\u000fPerturb(+1) + 1\n2]\n(14)\n= (1\u00001\nm)\u00010 +1\nm\u00011 =1\nm(15)\nThus, E[M(j;hj(d))(i)] = 0\u0001 1fd(i)\nhighg+ 1\u0001 1fd(i)\nlow=dg+1\nm\u0001 1fd(i)\nlow6=dg,\n19\n\nwhered(i)\nhighmeans a high-frequent item, and d(i)\nlowmeans a low-frequent item.\nE[1\nknX\ni=1kX\nj=1M(j;hj(d))(i)] = 0\u0001n\u0001\u0012+ 1\u0001f(d) +1\nm\u0001[n(1\u0000\u0012)\u0000f(d)] (16)\n= (1\u00001\nm)f(d) + (1\u0000\u0012)n\nm(17)\nThe expectation of the LDPLCM-based estimation:\nE[^f(d)] =E[m\nm\u00001(1\nknX\ni=1kX\nj=1M(j;hj(d))(i)\u0000(1\u0000\u0012)n\nm)] =f(d) (18)\nThus, the output of LDPLCM ^f(d) is a unbiased estimate of f(d).\nTo ensure the utility of LDPLCM-based frequency estimation, we prove that\nthe variance of the estimation is limited. Before computing the variance, we \frst\nprove some corresponding lemmas.\nLemma 1 computes the expectation of the squared entry of the ith data\nto thejline andhj(d) column of the sketch M. This lemma will be used to\ncompute the variance of LDPLCM-based estimation.\nLemma 1. E[(M(j;hj(d))(i))2] =k\n4(c2\n\u000f\u00001) +k\u0001 1fd(i)\nlow=dg+k\nm\u0001 1fd(i)\nlow6=dg\nProof. (1) Ifd(i)is a high-frequent item,\nE[(M(j;hj(d))(i))2] =k2E[(c\u000fPerturb(v(i)[hj(d)]) + 1\n2)21fJ=jg] (19)\n=kE[(c\u000fPerturb(\u00001) + 1\n2)2] (20)\n=k((\u0000c\u000f+ 1\n2)2(e\u000f=2\n1 +e\u000f=2) + (c\u000f+ 1\n2)(1\n1 +e\u000f=2)) (21)\n=k\n4(c\u000f2\u00001) (22)\n20\n\n(2)d(i)is a low-frequent item, and d(i)=d,\nE[(M(j;hj(d))(i))2] =k2E[(c\u000fPerturb(v(i)[hj(d)]) + 1\n2)21fJ=jg] (23)\n=kE[(c\u000fPerturb(+1) + 1\n2)2] (24)\n=k((c\u000f+ 1\n2)2(e\u000f=2\n1 +e\u000f=2) + (\u0000c\u000f+ 1\n2)(1\n1 +e\u000f=2)) (25)\n=k\n4(c\u000f2\u00001) +k (26)\n(3)d(i)is a low-frequent item, and d(i)6=d\nE[(M(j;hj(d))(i))2] =k2E[(c\u000fPerturb(v(i)[hj(d)]) + 1\n2)21fJ=jg] (27)\n=k\nmE[(c\u000fPerturb(+1) + 1\n2)2] +k(1\u00001\nm)E[(c\u000fPerturb(\u00001) + 1\n2)2]\n(28)\n=k\n4(c\u000f2\u00001) +k\nm(29)\nThus, E[(M(j;hj(d))(i))2] =k\n4(c2\n\u000f\u00001) + 0\u0001 1fd(i)\nhighg+k\u0001 1fd(i)\nlow=dg+\nk\nm\u0001 1fd(i)\nlow6=dg, whered(i)\nhighmeans a high-frequent item, and d(i)\nlowmeans a\nlow-frequent item.\nLemma 2 computes the covariance of the entries d(i1),d(i2)to the sketch M\nin di\u000berent cases.\nLemma 2. Leti16=i2be di\u000berent indices. We have:\n(1)Ifj16=j2, then\nCov(M(j1;hj1(d))(i1);M(j2;hj2(d))(i2)) = 0: (30)\n(2)Ifd(i1)andd(i2)are low-frequent values, d(i1)=dord(i2)=dord(i1)6=\nd(i2), then\nCov(M(j;hj(d))(i1);M(j;hj(d))(i2)) = 0: (31)\n(3)Ifd(i1)andd(i2)are low-frequent values, d(i1);d(i2)=d\u0003andd\u00036=d, then\nCov(M(j;hj(d))(i1);M(j;hj(d))(i2)) =1\nm\u00001\nm2: (32)\n21\n\n(4)Ifd(i1)ord(i2)is a high-frequent value, then\nCov(M(j;hj(d))(i1);M(j;hj(d))(i2)) = 0: (33)\nProof. The (1),(2), and (3) are proved in Reference [6]. We just need to prove\nthe (4).\nCov(M(j;hj(d))(i1);M(j;hj(d))(i2)) (34)\n=E[M(j;hj(d))(i1)\u0001M(j;hj(d))(i2)]\u0000E[M(j;hj(d))(i1)]E[M(j;hj(d))(i2)]\n(35)\n=1\n4fE[v(i1)[hj(d)]\u0001v(i2)[hj(d)]] +E[v(i1)[hj(d)]] +E[v(i2)[hj(d)]] + 1g\n\u0000E[M(j;hj(d))(i1)]\u0001E[M(j;hj(d))(i2)] (36)\nIfd(i1)is a high-frequent value, then v(i1)[hj(d)] =\u00001 and E[M(j;hj(d))(i1)] =\n0.\nThus, Cov( M(j;hj(d))(i1);M(j;hj(d))(i2)) = 0.\nTheorem 3 uses the above Lemma 1 and Lemma 2 to compute the variance\nof the LDPLCM-based frequency estimation for the low-frequent items.\nTheorem 3. The variance of the LDPLCM-based frequency estimation for a\nlow-frequent item is limited. That is Var[1\nkPn\ni=1Pk\nj=1M(j;hj(d))(i)]<n(c2\n\u000f\u00001)\n4+\nn(1\u0000\u0012)\u0000f(d)\nm+1\nkmP\nd\u00032Dlowf(d\u0003)2.\nProof. We compute the variance of the LDPLCM-based estimation for a low-\nfrequent item as follows.\nVar[1\nknX\ni=1kX\nj=1M(j;hj(d))(i)] (37)\n=nX\ni=1Var[1\nkkX\nj=1M(j;hj(d))(i)] +1\nk2X\ni16=i2Cov(kX\nj=1M(j;hj(d))(i1);kX\nj0=1M(j;hj(d))(i2))\n(38)\n=1\nk2nX\ni=1kX\nj=1Var[M(j;hj(d))(i)] +1\nk2X\ni16=i2kX\nj=1Cov(M(j;hj(d))(i1);M(j;hj(d))(i2))\n(39)\n22\n\nAccording to Lemma 2,\n1\nk2X\ni16=i2kX\nj=1Cov(M(j;hj(d))(i1);M(j;hj(d))(i2)) (40)\n= (1\nkm\u00001\nkm2)X\nd\u00036=dX\nd(i1)\nlow=d\u0003X\ni16=i21fd(i2)\nlow=d\u0003g (41)\n= (1\nkm\u00001\nkm2)X\nd\u00036=dX\nd(i)\nlow=d\u0003(f(d\u0003)\u00001) (42)\nwhered(i1)\nlowandd(i2)\nloware two low-frequent items, and d(i1)\nlow=d(i2)\nlow=d\u0003and\nd\u00036=d.\nThus,\nVar[1\nknX\ni=1kX\nj=1M(j;hj(d))(i)] (43)\n=1\nk2nX\ni=1kX\nj=1(E[(M(j;hj(d))(i))2]\u0000(E(M(j;hj(d))(i)])2) + (1\nkm\u00001\nkm2)X\nd\u00036=dX\nd(i)\nlow=d\u0003(f(d\u0003)\u00001)\n(44)\n=n(c2\n\u000f\u00001)\n4+n(1\u0000\u0012)\u0000f(d)\nm(1\u00001\nm) + (1\nkm\u00001\nkm2)X\nd\u00036=dX\nd(i)\nlow=d\u0003(f(d\u0003)\u00001)\n(45)\n=n(c2\n\u000f\u00001)\n4+n(1\u0000\u0012)\u0000f(d)\nm(1\u00001\nm\u00001\nk+1\nkm) + (1\nkm\u00001\nkm2)X\nd\u00036=df(d\u0003)2\n(46)\nSinced\u0003is the value of a low-frequent item,P\nd\u00036=df(d\u0003)2=P\nd\u00032Dlowf(d\u0003)2\u0000\nf(d)2. The variance of LDPLCM is limited by the following equation.\nVar[1\nknX\ni=1kX\nj=1M(j;hj(d))(i)]<n(c2\n\u000f\u00001)\n4+n(1\u0000\u0012)\u0000f(d)\nm+1\nkmX\nd\u00032Dlowf(d\u0003)2\n(47)\nThe variance of Apple-CMS isn(c2\n\u000f\u00001)\n4+n\u0000f(d)\nm(1\u00001\nm\u00001\nk+1\nkm) + (1\nkm\u0000\n1\nkm2)(P\nd\u00032Df(d\u0003)2\u0000f(d)2). According to the Theorem 3, our LDPLCM is\nmore accurate for the low-frequent items compared with the Apple-CMS.\n23\n\n5. Experimental Result\n5.1. Settings\nHardware and Library\nAll the experiments are implemented in Python 3.8 and run on Intel(R)\nXeon(R) Gold 5218 CPU, with 1TB of MEMORY and 256GB of RAM.\nDatasets\nWe test the performance of frequency estimation algorithms on three datasets,\nincluding two synthetic datasets and one real-world dataset.\n(1)The \frst dataset is a synthetic dataset including 10 million records gen-\nerated from a Zipf distribution with skewness s= 1:1. The domain size is\n2,817,991. We refer to this dataset as Zipf(10M) in the following experiments.\n(2)The second dataset is a synthetic dataset including 100 million records\ngenerated from a Zipf distribution with skewness s= 1:1. The domain size is\n22,774,443. We refer to this dataset as Zipf(100M) in the following experiments.\n(3)The third one is a real-world dataset named Wesad [39] for wearable stress\nand a\u000bect detection. This dataset is a 16GB dataset containing 63 million\nrecords. We use the attribute RESPIRATION for the frequency estimation\nexperiments. The domain size is 44,900. We refer to this dataset as WESAD in\nthe following experiments.\nError Metrics\nWe use metrics to measure the accuracy of di\u000berent algorithms. In the\nfollowing metrics, ddenotes the size of data domain, idenotes the i-th data item,\nf(xi) and ^f(xi) represent the true value and the predicted value, respectively.\n(1) Mean squared error (MSE):1\ndP\nxi2D(f(xi)\u0000^f(xi))2.\n(2) Sum squared error (SSE):Pi\n1(f(xi)\u0000^f(xi))2.\nParameters\n\u0012: the ratio of the total frequencies of high-frequent items to the frequencies\nof all items, \u0012=P\nxi2HighFref(xi)P\nx2Df(xi), whereHighFre means the set of all the\nhigh-frequent items.\nr: the sampling rate for the LDPLCM algorithm.\n24\n\nZIPF(10M) ZIPF(100M) Wesad\nDifferent datasets 105106107108109MSERFR\nGBRFigure 3: The performance on di\u000berent models for all datasets.\n\u000f: the privacy budget of clients in LDPLCM, Apple-CMS, and Apple-HCMS\nalgorithms.\n(m;k): the sketch matrix parameters with hash functions kand the domain\nsizemin LDPLCM, Apple-CMS, and Apple-HCMS algorithms.\nk0: the parameter is to restrict clients to uniformly choosing from k0hash\nfunctions in FLH.\nFrequency Model\nThe frequency model should be both lightweight and accurate. On the one\nhand, it must be small enough to be easily passed between the server and the\nclients. On the other hand, it should be su\u000eciently accurate to predict the\nfrequency for the high-frequent items. For the sake of fairness, we compared\nthe accuracy of the Random Forest Regressor model and the Gradient Boosting\nRegressor model with the same space cost on three datasets. Figure 3 shows\nthat the Gradient Boosting Regressor model is more accurate than the Ran-\ndom Forest Regressor model. Thus, we adopt the Gradient Boosting Regressor\nmodel to form the frequency model of LDPLCM. The advantage of the Gra-\ndient Boosting Regressor model is that it introduces a new weak classi\fer in\neach iteration to reduce the residuals of the previously existing classi\fer combi-\nnations, which results in improved \ftting and prediction data rather than just\nusing weak learning algorithms.\n25\n\n100101102103104105106107108109\nData Items1041051061071081091010101110121013101410151016101710181019SSELDPLCM\nApple-CMS\nApple-HCMS\nFLH(a) Zipf(10M).\n100101102103104105106107\nData Items10410510610710810910101011101210131014SSELDPLCM\nApple-CMS\nApple-HCMS\nFLH (b) Zipf(100M).\n0 10000 20000 30000 40000 50000 60000\nData Items10410510610710810910101011101210131014SSELDPLCM\nApple-CMS\nApple-HCMS\nFLH (c) Wesad.\nFigure 4: The Accuracy of Frequency Estimation\nWe set the parameters of the models as learningrate = 0:05; estimators =\n350;maxdepth = 5 for Zipf(10M), Zipf(100M) datasets, and learningrate = 0:1,\nestimators = 100,maxdepth = 3 for WESAD dataset. These parameters are\ntuned according to the GridSearchCV1.\nCompetitors\nIn the following experiments, we compare the performance of our LDPLCM\nwith state-of-the-art algorithms including Apple-CMS and Apple-HCMS.\n(1)Apple-CMS: The clients \rip each bit of their one-hot vectors with the\nprobability1\ne\u000f\n2+1and send the perturbed items to the server. Then, the server\nstores the perturbed items in a k\u0002msketch matrix and estimates by averaging\nthe hash entries.\n(2)Apple-HCMS: The clients compute the Hadamard transform of any one-\nhot vectors and \rip with the probability1\ne\u000f\n2+1. The algorithm in the server is\nas same as Apple-CMS.\n(3)FLH [27]: The clients are restricted by a parameter k0to choose hash\nfunctions. The hash functions map the data domain [ d] to [g], whereg=e\u000f. The\nservice calculates which domain elements a client's perturbed item contributes\nfrequency towards by pre-computing a k0\u0002dmatrix.\n26\n\n5.2. Accuracy\nIn this experiment, we compare the accuracy of our method with the Apple-\nCMS and Apple-HCMS on the three datasets. We set r= 0:1 and\u000f= 4. The\nexperimental results on Zipf(10M), Zipf(100M) and WESAD datasets are shown\nin Figure 4(a), Figure 4(b), and Figure 4(c), respectively. The error is measured\nby SSE. For the Zipf(10M) and Zipf(100M), we set the sketch parameters ( m,\nk) asm= 1024,k= 64 and\u0012= 0:5. For the WESAD dataset, we set the\nparameters as m= 512,k= 32 and\u0012= 0:4. We also set the parameter\nk0= 128 of FLH for all datasets. Since the domain of WESAD is smaller than\nthat of zipf datasets, we use a smaller sketch to test the impact of hash collisions.\nWe can learn from these \fgures that LDPLCM is more accurate than the\nApple-CMS, Apple-HCMS and FLH on di\u000berent datasets. The reason is that\nwe train a model on some samples to distinguish the high-frequent and low-\nfrequent items and only use the sketch to estimate the low-frequent items. The\nhigh-frequent items in phase 2 are encoded as dummy values causing no collisions\nwith the low-frequent items. The frequency estimation of FLH is worse than\nothers obviously because it maps the data to a small domain g=e\u000f+ 1. And\nin this experiment, g= 56, which is much smaller than the data domain.\nWe also can \fnd that the LDPLCM estimation error of high-frequent items\nis slightly higher than the Apple-CMS and Apple-HCMS. It is reasonable since\nthe high-frequent items are estimated based on the frequency model trained\naccording to the samples, the estimations are slightly higher than the other\nmethods that estimate frequency based on sketches built with the entire dataset.\nBut the total errors of the estimation for all the items are lower than the other\nmethods. It is because the estimations of our method for the low-frequent items\nare more accurate than the others. In addition, the sampling errors have little\ne\u000bect on the results for the larger datasets as shown in Figure 4(b), which\nillustrates the applicability of our method to a larger domain.\n1https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.\nGridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV\n27\n\n5.3. Space cost\nWe test the accuracy of di\u000berent methods with similar space costs on Zipf(100M).\nIn this experiment, we only compare LDPLCM with Apple-CMS and Apple-\nHCMS since they use the same structure(sketch) to estimate the frequency.\nThe space cost of LDPLCM includes the size of the frequency model and the\nsize of the sketch. The space costs of Apple-CMS and Apple-HCMS are only\nthe size of the sketches. We take a variety of settings to make di\u000berent methods\nhave similar space costs. The settings (the total space cost, the size of frequency\nmodel and the size of sketch) of di\u000berent methods are shown in Table 1.\nThe experimental result about space cost and accuracy is shown in Figure 5.\nWe can learn that the errors decrease with space cost. It is reasonable since a\nlarger sketch can reduce hash collisions. It is clear that our LDPLCM is more\naccurate than the Apple-CMS and Apple-HCMS while occupying the similar\nspace. The frequency model is more lightweight than sketch, so that it can be\ntransferred more quickly between the clients and the server.\nTable 1: Settings for the space cost experiment.\nLDPLCM Apple-CMS/Apple-HCMS\nTotal Space Cost(KB) Model Size(KB) Sketch Size(KB) Total Space Cost(KB)\n685 461 224 896\n2235 1339 896 1792\n3141 1349 1792 3584\n4921 1337 3584 7168\n8514 1346 7168 8848\n5.4. E\u000eciency\nWe test the query e\u000eciency of di\u000berent methods on the Zipf(100M) dataset.\nWe test 50,000 data records and average the query time as shown in Figure 6.\nOur algorithm is slightly less e\u000ecient than the Apple-CMS, Apple-HCMS. The\nreason is that the estimation time of our method includes not only the time to\n28\n\n0 2000 4000 6000 8000 10000\nSpace Cost (KB)1071081091010MSELDPLCM\nApple-CMS\nApple-HCMSFigure 5: The impact of space cost on the accuracy.\nLDPLCM Apple-CMS Apple-HCMS FLH0.000.010.020.030.040.050.060.070.08Query Processing Time (ms)\nFigure 6: E\u000eciency.\nestimate the frequency according to the sketch or matrix, but also the time to\npredict the frequency based on the model. FLH is the most e\u000ecient method,\nsince it maps the data to a small domain e\u000f+ 1. But its accuracy is worse than\nthe other methods as shown in Fig 4.\n5.5. The impact of parameters\nWe test the impact of di\u000berent parameters including sketch parameters\n(m;k), privacy budget \u000f, sampling rate r, and the ratio of high-frequent items\n\u0012on the accuracy.\n29\n\nk = 16 k = 32 k = 64 k = 128\nNumber of Hash Functions (k)103104105106107108109MSELDPLCM\nApple-CMS\nApple-HCMS(a) The impact of hash functions k.\nm = 256 m = 512 m = 1024 m = 2048\nSize of Sketch Vectors (m)103104105106107108109MSELDPLCM\nApple-CMS\nApple-HCMS(b) The impact of sketch vectors m.\nFigure 7: The impact of sketch parameters ( m;k) on accuracy (Zipf(10M) dataset).\n5.5.1. The impact of sketch parameters ( m,k) on the accuracy\nWe test the impact of sketch parameters ( m;k) on the accuracy with Zipf(10M)\nand Zipf(100M) datasets. We \fx m= 1024 in Figure 7(a) and Figure 8(a) and\nk= 64 in Figure 7(b) and Figure 8(b). The other parameters are \u0012= 0:5;r=\n0:1;\u000f= 4. Also, we \fx m= 512 in Figure 9(a) and k= 32 in Figure 9(b) while\nthe other parameters are \u0012= 0:4;r= 0:1;\u000f= 4. From the Figure 7, Figure 8\nand Figure 9, we can learn that LDPLCM outperforms Apple-CMS and Apple-\nHCMS as ( m;k) keeps increasing. The maximum gap is obtained when the\n(m;k) takes the minimum value. This phenomenon is reasonable because the\nsmaller sketches mean more hash collisions, and then the more signi\fcant the\naccuracy improvement of our method by avoiding the collisions. A widening gap\nbetween our method and Apple-CMS is more pronounced when the domain is\nlarger since a larger domain brings more hash collisions when the sketch is \fxed.\nLDPLCM uses the model to predict the estimation for high-frequent items and\nreduce the collisions so that it improves the accuracy for low-frequent items.\n5.5.2. The impact of privacy budget \u000fon the accuracy\nIn Figure 10, we vary the privacy budget \u000f2f1;2;:::;7gon the accuracy\nwith the Zipf(10M), Zipf(100M) and WESAD datasets. For Zipf(10M) and\nZipf(100M) datasets, we \fx k= 64;m= 1024;\u0012= 0:5;r= 0:1. For WESAD\ndataset, we \fx k= 32;m= 512;\u0012= 0:4;r= 0:1. We also \fx k0= 128 in FLH for\nall datasets. We can see that our algorithm is more accurate than Apple-CMS,\n30\n\nk = 32 k = 64 k = 128 k = 256\nNumber of Hash Functions (k)1051061071081091010MSELDPLCM\nApple-CMS\nApple-HCMS(a) The impact of hash functions k.\nm = 512 m = 1024 m = 2048 m = 4096\nSize of Sketch Vectors (m) 1051061071081091010MSELDPLCM\nApple-CMS\nApple-HCMS(b) The impact of sketch vectors m.\nFigure 8: The impact of sketch parameters ( m;k) on the accuracy (Zipf(100M) dataset).\nk = 8 k = 16 k = 32 k = 64\nNumber of Hash Functions (k)105106107108109MSELDPLCM\nApple-CMS\nApple-HCMS\n(a) The impact of hash functions k.\nm = 128 m = 256 m = 512 m = 1024\nNumber of Hash Functions (k)105106107108109MSELDPLCM\nApple-CMS\nApple-HCMS(b) The impact of sketch vectors m.\nFigure 9: The impact of sketch parameters ( m;k) on the accuracy (Wesad dataset).\n1 2 3 4 5 6 7\nPrivacy Budget ()\n1051061071081091010MSELDPLCM\nApple-CMS\nApple-HCMS\nFLH\n(a) Zipf(10M).\n1 2 3 4 5 6 7\nPrivacy Budget ()\n108109101010111012MSELDPLCM\nApple-CMS\nApple-HCMS\nFLH (b) Zipf(100M).\n1 2 3 4 5 6 7\nPrivacy Budget ()\n1071081091010MSELDPLCM\nApple-CMS\nApple-HCMS\nFLH (c) Wesad.\nFigure 10: The impact of privacy budget \u000fon the accuracy.\n31\n\nApple-HCMS and FLH with di\u000berent privacy budgets on di\u000berent datasets. The\nreason is that our method reduces the estimation errors of low-frequent items\nby avoiding hash collisions between high-frequent and low-frequent items, while\nthe other methods face the errors caused by a large number of hash collisions in\naddition to the noises introduced by LDP. By comparing the Fig 10(a), Fig 10(b)\nand Fig 10(c), we can learn that our method outperforms the other methods on\ndatasets with a large domain and small privacy budget.\n5.5.3. The impact of sampling rate ron the accuracy\nWe setk= 64,m= 1024,\u0012= 0:5 and\u000f= 4 in Figure 11. We vary the\nsampling rate r2f0:10;0:15;:::;0:30gon the accuracy with the Zipf(100M)\ndataset. To ensure fairness, we make the precision of each frequency model\nsimilar for di\u000berent sampling rates. We can \fnd that the error decreases as\nthe sampling rate increases. This phenomenon is consistent with the theoretical\nresult, because the higher sampling rate reduces the error of sampling-based\nestimation for high-frequent data items.\n0.10 0.15 0.20 0.25 0.30\nSampling Rate (r) 107108109MSELDPLCM\nFigure 11: The impact of sampling rate ron\nthe accuracy of LDPLCM.\n0.2 0.3 0.4 0.5 0.6 0.7\nThe Ratio of High-frequent Items  \n101110121013MSELDPLCMFigure 12: The impact of \u0012on the accuracy\nof LDPLCM.\n5.5.4. The impact of \u0012on the accuracy\nThe parameter \u0012means the ratio of the total frequencies of high-frequent\nitems to the frequencies of all the items. We vary \u00122f0:3;0:35;:::;0:6gand\n32\n\ns = 1.1 s = 1.2 s = 1.3 s = 1.4 s = 1.5\nskewness s10710810910101011MSELDPLCM\nApple-CMS\nApple-HCMSFigure 13: The impact of son the accuracy of LDPLCM.\ntest the impact of \u0012on the accuracy with the Zipf(100M) dataset. We set the\nparameters as k= 64,m= 1024,r= 0:1, and\u000f= 4. We can learn from\nFigure 12 that a higher \u0012makes LDPLCM more accurate. The reason is that\na higher\u0012means more high-frequent items are predicted by the model instead\nof the sketch. As a result, the hash collisions in the sketch are reduced, which\nleads to a more accurate estimation.\n5.5.5. The impact of son the accuracy\nThe parameter smeans the skewness of the Zipf distribution dataset. We\nvarys2f1:1;1:2;:::;1:5gand test the impact of son the accuracy with the\nZipf(100M) dataset. We set the parameters as k= 64,m= 1024,r= 0:1, and\n\u000f= 4. As shown in Figure 13, we can \fnd that a smaller smakes LDPLCM more\naccurate. As the sum of frequency of high-frequent items in the experiments\nwith di\u000berent skewness \\ s\" are the same, the lower skewness means more items\nwill be regarded as high-frequent items. Thus, more items are predicted by the\nfrequency model. As a result, it reduces more hash collisions and leads to a\nmore accurate estimation.\n5.6. Summary of experimental results\nThe experimental results are summarized as follows:\n33\n\n\u000fLDPLCM is more accurate than Apple-CMS, Apple-HCMS and FLH for\nfrequency estimation on datasets with large domains.\n\u000fThe higher the frequency of high-frequent items removed from the sketch,\nthe more accurate the low-frequent items estimations become.\n\u000fThe superiority of LDPLCM is more obvious when the sketch is limited\nto a small size.\n6. Conclusion\nIn this paper, we propose the LDPLCM algorithm to reduce hash collisions\nand provide more accurate frequency estimations under LDP, especially for data\nwith a larger domain. We train the frequency model to distinguish the high-\nfrequent items and the low-frequent items, and we separate the storage of the\nitems with di\u000berent frequency properties without leaking privacy. Our method\nfocuses on the frequency estimation for one-dimensional data under LDP. Ex-\ntending the method for multidimensional data is still challenging. Some fron-\ntier works adopt the multidimensional histograms or grids for multidimensional\nfrequency estimation under LDP, however, they still su\u000ber from the curse of\ndimension and the trade-o\u000b between the accuracy and the utility. We will try\nto extend our work for multidimensional data in the future.\nAcknowledgements\nThis work was supported by NSFC grant 62202113, the Major Key Project\nof PCL (Grant No.PCL2021A09, PCL2021A02, PCL2022A03).\nReferences\n[1] L. Sweeney, k-anonymity: A model for protecting privacy, International\nJournal of Uncertainty, Fuzziness and Knowledge-Based Systems 10 (05)\n(2002) 557{570.\n34\n\n[2] C. Dwork, Di\u000berential privacy: A survey of results, in: International con-\nference on theory and applications of models of computation, Springer,\n2008, pp. 1{19.\n[3] S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, A. Smith,\nWhat can we learn privately?, SIAM Journal on Computing 40 (3) (2011)\n793{826.\n[4]\u0013U. Erlingsson, V. Pihur, A. Korolova, Rappor: Randomized aggregatable\nprivacy-preserving ordinal response, in: Proceedings of the 2014 ACM\nSIGSAC conference on computer and communications security, 2014, pp.\n1054{1067.\n[5] G. Fanti, V. Pihur, \u0013U. Erlingsson, Building a rappor with the unknown:\nPrivacy-preserving learning of associations and data dictionaries, arXiv\npreprint arXiv:1503.01214.\n[6] A. Di\u000berential Privacy Team, Learning with privacy at scale.\n[7] B. Ding, J. Kulkarni, S. Yekhanin, Collecting telemetry data privately,\nAdvances in Neural Information Processing Systems 30.\n[8] T. Wang, J. Blocki, N. Li, S. Jha, Locally di\u000berentially private protocols\nfor frequency estimation, in: 26th USENIX Security Symposium (USENIX\nSecurity 17), 2017, pp. 729{745.\n[9] S. Y\u0010ld\u0010r\u0010m, K. Kaya, S. Ayd\u0010n, H. B. Erentu\u0015 g, Di\u000berentially private fre-\nquency sketches for intermittent queries on large data streams, in: 2020\nIEEE International Conference on Big Data (Big Data), IEEE, 2020, pp.\n4083{4092.\n[10] J. Li, Z. Li, Y. Xu, S. Jiang, T. Yang, B. Cui, Y. Dai, G. Zhang, Wav-\ningsketch: An unbiased and generic sketch for \fnding top-k items in data\nstreams, in: Proceedings of the 26th ACM SIGKDD International Confer-\nence on Knowledge Discovery & Data Mining, 2020, pp. 1574{1584.\n35\n\n[11] R. M. Karp, S. Shenker, C. H. Papadimitriou, A simple algorithm for \fnd-\ning frequent elements in streams and bags, ACM Transactions on Database\nSystems (TODS) 28 (1) (2003) 51{55.\n[12] K. S. Tai, V. Sharan, P. Bailis, G. Valiant, Sketching linear classi\fers over\ndata streams, in: Proceedings of the 2018 International Conference on\nManagement of Data, 2018, pp. 757{772.\n[13] G. Cormode, F. Korn, S. Muthukrishnan, D. Srivastava, Finding hierarchi-\ncal heavy hitters in data streams, in: Proceedings 2003 VLDB Conference,\nElsevier, 2003, pp. 464{475.\n[14] N. Tang, Q. Chen, P. Mitra, Graph stream summarization: From big bang\nto big crunch, in: Proceedings of the 2016 International Conference on\nManagement of Data, 2016, pp. 1481{1496.\n[15] T. Yang, J. Jiang, P. Liu, Q. Huang, J. Gong, Y. Zhou, R. Miao, X. Li,\nS. Uhlig, Adaptive measurements using one elastic sketch, IEEE/ACM\nTransactions on Networking 27 (6) (2019) 2236{2251.\n[16] Z. Liu, A. Manousis, G. Vorsanger, V. Sekar, V. Braverman, One sketch\nto rule them all: Rethinking network \row monitoring with univmon, in:\nProceedings of the 2016 ACM SIGCOMM Conference, 2016, pp. 101{114.\n[17] R. B. Basat, X. Chen, G. Einziger, R. Friedman, Y. Kassner, Random-\nized admission policy for e\u000ecient top-k, frequency, and volume estimation,\nIEEE/ACM Transactions on Networking 27 (4) (2019) 1432{1445.\n[18] P. Kairouz, K. Bonawitz, D. Ramage, Discrete distribution estimation\nunder local privacy, in: International Conference on Machine Learning,\nPMLR, 2016, pp. 2436{2444.\n[19] B. H. Bloom, Space/time trade-o\u000bs in hash coding with allowable errors,\nCommunications of the ACM 13 (7) (1970) 422{426.\n36\n\n[20] J. C. Duchi, M. I. Jordan, M. J. Wainwright, Local privacy and statistical\nminimax rates, in: 2013 IEEE 54th Annual Symposium on Foundations of\nComputer Science, IEEE, 2013, pp. 429{438.\n[21] T. Wang, M. Lopuha a-Zwakenberg, Z. Li, B. Skoric, N. Li, Locally dif-\nferentially private frequency estimation with consistency, arXiv preprint\narXiv:1905.08320.\n[22] T. Wang, B. Ding, J. Zhou, C. Hong, Z. Huang, N. Li, S. Jha, Answer-\ning multi-dimensional analytical queries under local di\u000berential privacy, in:\nProceedings of the 2019 International Conference on Management of Data,\n2019, pp. 159{176.\n[23] T. Murakami, Y. Kawamoto, fUtility-Optimized glocal di\u000berential privacy\nmechanisms for distribution estimation, in: 28th USENIX Security Sym-\nposium (USENIX Security 19), 2019, pp. 1877{1894.\n[24] C. Wei, S. Ji, C. Liu, W. Chen, T. Wang, Asgldp: collecting and gener-\nating decentralized attributed graphs with local di\u000berential privacy, IEEE\nTransactions on Information Forensics and Security 15 (2020) 3239{3254.\n[25] M. Xu, B. Ding, T. Wang, J. Zhou, Collecting and analyzing data jointly\nfrom multiple services under local di\u000berential privacy, Proceedings of the\nVLDB Endowment 13 (12) (2020) 2760{2772.\n[26] J. Jia, N. Z. Gong, Calibrate: Frequency estimation and heavy hitter iden-\nti\fcation with local di\u000berential privacy via incorporating prior knowledge,\nin: IEEE INFOCOM 2019-IEEE Conference on Computer Communica-\ntions, IEEE, 2019, pp. 2008{2016.\n[27] G. Cormode, S. Maddock, C. Maple, Frequency estimation under local\ndi\u000berential privacy, Proceedings of the VLDB Endowment 14 (11) (2021)\n2046{2058.\n37\n\n[28] M. Charikar, K. Chen, M. Farach-Colton, Finding frequent items in data\nstreams, in: International Colloquium on Automata, Languages, and Pro-\ngramming, Springer, 2002, pp. 693{703.\n[29] G. Cormode, S. Muthukrishnan, An improved data stream summary: the\ncount-min sketch and its applications, Journal of Algorithms 55 (1) (2005)\n58{75.\n[30] P. Roy, A. Khan, G. Alonso, Augmented sketch: Faster and more accurate\nstream processing, in: Proceedings of the 2016 International Conference on\nManagement of Data, 2016, pp. 1449{1463.\n[31] C. Estan, G. Varghese, New directions in tra\u000ec measurement and account-\ning, in: Proceedings of the 2002 conference on Applications, technologies,\narchitectures, and protocols for computer communications, 2002, pp. 323{\n336.\n[32] Y. Zhou, T. Yang, J. Jiang, B. Cui, M. Yu, X. Li, S. Uhlig, Cold \flter: A\nmeta-framework for faster and more accurate stream processing, in: Pro-\nceedings of the 2018 International Conference on Management of Data,\n2018, pp. 741{756.\n[33] T. Yang, J. Gong, H. Zhang, L. Zou, L. Shi, X. Li, Heavyguardian: Sep-\narate and guard hot items in data streams, in: Proceedings of the 24th\nACM SIGKDD International Conference on Knowledge Discovery & Data\nMining, 2018, pp. 2584{2593.\n[34] Z. Qin, Y. Yang, T. Yu, I. Khalil, X. Xiao, K. Ren, Heavy hitter estima-\ntion over set-valued data with local di\u000berential privacy, in: Proceedings\nof the 2016 ACM SIGSAC Conference on Computer and Communications\nSecurity, 2016, pp. 192{203.\n[35] T. Kraska, A. Beutel, E. H. Chi, J. Dean, N. Polyzotis, The case for learned\nindex structures, in: Proceedings of the 2018 international conference on\nmanagement of data, 2018, pp. 489{504.\n38\n\n[36] C.-Y. Hsu, P. Indyk, D. Katabi, A. Vakilian, Learning-based frequency\nestimation algorithms., in: International Conference on Learning Repre-\nsentations, 2019.\n[37] M. Zhang, H. Wang, J. Li, H. Gao, Learned sketches for frequency estima-\ntion, Information Sciences 507 (2020) 365{385.\n[38] Z. Zhou, D. Zhang, X. Hong, Rl-sketch: Scaling reinforcement learning\nfor adaptive and automate anomaly detection in network data streams, in:\n2019 IEEE 44th Conference on Local Computer Networks (LCN), IEEE,\n2019, pp. 340{347.\n[39] P. Schmidt, A. Reiss, R. Duerichen, C. Marberger, K. Van Laerhoven,\nIntroducing wesad, a multimodal dataset for wearable stress and a\u000bect\ndetection, in: Proceedings of the 20th ACM international conference on\nmultimodal interaction, 2018, pp. 400{408.\n39",
  "textLength": 63361
}