{
  "paperId": "50bad6619e38a6b7a33760e2c9dc834b4e970e4b",
  "title": "Incremental Approximate Single-Source Shortest Paths with Predictions",
  "pdfPath": "50bad6619e38a6b7a33760e2c9dc834b4e970e4b.pdf",
  "text": "arXiv:2502.08125v1  [cs.DS]  12 Feb 2025Incremental\nApproximate Single-Source Shortest Paths with Prediction s\nSamuel McCauley*\nWilliams College\nsam@cs.williams.eduBenjamin Moseley†\nCarnegie Mellon University\nmoseleyb@andrew.cmu.edu\nAidin Niaparast‡\nCarnegie Mellon University\naniapara@andrew.cmu.eduHelia Niaparast§\nCarnegie Mellon University\nhniapara@andrew.cmu.edu\nShikha Singh¶\nWilliams College\nshikha@cs.williams.edu\nAbstract\nThe algorithms-with-predictions framework has been used e xtensively to develop online algo-\nrithms with improved beyond-worst-case competitive ratio s. Recently, there is growing interest in\nleveraging predictions for designing data structures with improved beyond-worst-case running times.\nIn this paper, we study the fundamental data structure probl em of maintaining approximate shortest\npaths in incremental graphs in the algorithms-with-predic tions model. Given a sequence σof edges\nthat are inserted one at a time, the goal is to maintain approx imate shortest paths from the source\nto each vertex in the graph at each time step. Before any edges arrive, the data structure is given a\nprediction of the online edge sequence ˆσwhich is used to “warm start” its state.\nAs our main result, we design a learned algorithm that mainta ins(1 +ǫ)-approximate single-\nsource shortest paths, which runs in ˜O(mηlogW/ǫ)time, where Wis the weight of the heaviest\nedge and ηis the prediction error. We show these techniques immediate ly extend to the all-pairs\nshortest-path setting as well. Our algorithms are consiste nt (performing nearly as fast as the ofﬂine\nalgorithm) when predictions are nearly perfect, have a smoo th degradation in performance with re-\nspect to the prediction error and, in the worst case, match th e best ofﬂine algorithm up to logarithmic\nfactors. That is, the algorithms are “ideal” in the algorith ms-with-predictions model.\nAs a building block, we study the ofﬂine incremental approximate single-source shortest-path\n(SSSP) problem. In the ofﬂine incremental SSSP problem, the edge sequence σis known a priori\nand the goal is to construct a data structure that can efﬁcien tly return the length of the shortest paths\nin the intermediate graph Gtconsisting of the ﬁrst tedges, for all t. Note that the ofﬂine incremental\nproblem is deﬁned in the worst-case setting (without predic tions) and is of independent interest.\n1 Introduction\nThe efﬁciency of algorithms is typically measured in the wor st-case model. The worst-case model makes\na fundamental assumption that algorithmic problems are sol ved from scratch. In real applications, many\n*Supported in part by NSF CCF 2103813\n†Supported in part by Google Research Award, an Infor Researc h Award, a Carnegie Bosch Junior Faculty Chair, NSF\ngrants CCF-2121744 and CCF-1845146.\n‡Supported in part by the Air Force Ofﬁce of Scientiﬁc Researc h under award number FA9550-23-1-0031 and NSF award\nCCF-1845146.\n§Supported in part by NSF CCF-1845146.\n¶Supported in part by NSF CCF 1947789\n1\n\nproblems are solved repeatedly on similar input instances. There has been a growing interest in improv-\ning the running time of algorithms by leveraging similariti es across problem instances. The goal is to\nwarm start the algorithm; that is, initialize it with a machine-learne d state, speeding up its running time.\nThis initial state is learned from the past instances of the p roblem.\nThis recent line of work has given rise to a widely-applicabl e model for beyond-worst-case running\ntime analysis. The area has come to be known as algorithms with predictions . In the algorithms-with-\npredictions model, the goal is to establish strong worst-ca se-style guarantees for the algorithm. The\ncritical difference is that the running time of the algorith m is parameterized by the quality of the learned\nstarting state; that is, the prediction quality. The algori thms designed in this model are also frequently\nreferred to as learning-augmented or simply learned algorithms.\nIdeally, an algorithm outperforms the best worst-case algo rithm with high quality predictions (i.e.\nthe algorithm is consistent ). When the predictions are incorrect, the algorithm’s perf ormance should\ndegrade proportionally to the error (i.e. the algorithm sho uld be smooth ) and never be worse than the\nbest worst-case algorithm (i.e. the algorithm is robust ). If an algorithm is consistent, smooth, and robust,\nit is called an ideal learned algorithm .\nKraska et al. [26] empirically demonstrated how machine-learned predictions can speed up data\nstructures. This seminal paper inspired Dinitz et al. [15] t o propose a theoretical framework to leverage\npredictions to speed up ofﬂine algorithms. Since this work, several papers have used predi ctions to im-\nprove the running time of ofﬂine combinatorial optimizatio n problems such as maximum ﬂow [12, 38],\nshortest path [28], and convex optimization [41]. Recently , Srinivas and Blum [43] give a framework\nfor utilizing multiple ofﬂine predictions to improve the ru nning time of online optimization problems.\nThis growing body of theoretical and empirical work shows th e incredible potential of using machine-\nlearned predictions to improve the efﬁciency of algorithms . The prediction framework provides a rich\nand algorithmically interesting landscape that is yet to be understood.\nData structures are a critical building block for dynamic al gorithms for optimization problems.\nThere is a growing body of theoretical work on designing idea l algorithms for data structure prob-\nlems [1, 2, 14, 30, 31, 34, 35, 45, 46]; see Section 1.1 for deta ils. Predictions have been particularly\neffective at speeding up data structures for dynamic graph p roblems, which typically incur polynomial\nupdate times in the worst case. In particular, McCauley et al . [35] use predictions to design faster data\nstructures for incremental topological maintenance and cy cle detection. Henzinger et al. [21] and van\nden Brand et al. [45] initiate the use of predictions for desi gning faster dynamic graph algorithms. In\nparticular, van den Brand [45] solve the online matrix-vect or multiplication with predictions and use it\nto obtain faster algorithms for several dynamic graph probl ems such as incremental all-pairs shortest\npaths, reachability, and triangle detection. Liu and Srini vas [31] give efﬁcient black-box transformations\nfrom ofﬂine divide-and-conquer style algorithms to fully d ynamic learned algorithms that are given\npredictions about the update sequence. These initial resul ts demonstrate incredible potential and there\nis a need to develop algorithmic techniques for designing da ta structures that can leverage predictions.\nTowards this goal, in this paper, we study the fundamental da ta structure problem of maintaining approx-\nimate single-source shortest paths in dynamic graphs with e dge insertions. No learned data structure has\nbeen developed for this problem despite being a clear target in the area.\nIncremental Single-Source Shortest Paths. In this paper, we design a data structure to maintain short-\nest paths in a weighted directed graph when edges are inserte d over time. Initially, all nodes Vof the\ngraph are available and, in the single-source case, a source sis speciﬁed. There are medges that arrive\none by one: at each time step t, an edge (with a positive weight) is added to the graph. The go al is to de-\nsign a data structure that approximately stores the shortes t path distance from the source sto every other\nvertexvin the graph. Let dt(s,v)be the distance between sandvaftertedge insertions. Let ˆdt(s,v)\nbe an approximation of dt(s,v)computed by the algorithm after tedges arrive. The algorithm needs to\nefﬁciently compute ˆdt(s,v)such that dt(s,v)≤ˆdt(s,v)≤(1+ǫ)dt(s,v)for some constant ǫ >0.\nIncremental shortest paths is a fundamental algorithmic pr oblem used in applications as well as a\nbuilding block for other data structures [20, 40]. This prob lem has been extensively studied in the litera-\n2\n\nture [6,7,9,17–19,27]. The best-known worst-case update t imes for the problem are ˜O(n2logW/ǫ2.5+\nm)[39] for dense graphs and a ˜O(m4/3logW/ǫ2)algorithm [27] for sparse graphs.1\nRecently, Henzinger et al. [21] and van den Brand [45] applie d predictions to the problem of com-\nputing all-pairs shortest paths (APSP) in incremental grap hs. We follow their prediction model in which\nthe data structure is given a prediction of the online edge se quenceˆσbefore any edges arrive. The\nperformance of the algorithm is given in terms of the predict ion error. Deﬁne an edge e’s errorηeas\nthe difference between its arrival time in ˆσandσand the aggregate error ηas themaxeηe. They show\nthat for the incremental APSP problem, predictions can be us ed to support O(1)-lookup time and O(η2)\ntime per edge insert, which is optimal under the Online Matri x-Vector Multiplication Hypothesis [45].\nThe preprocessing time used in [21] is O(mn3), and it is O(n(3+ω)/2)in [45] where ωis the exponent\nfrom matrix multiplication.\nTheir work leaves open an important question—can predictio ns also help speed up the related and\nfundamental problem of maintaining approximate single-source shortest paths under edge inserts, which\nhas not yet been studied in this framework.\nOur Contributions. The main contribution of this paper is a new learned data stru cture for the incre-\nmental approximate SSSP problem and a demonstration that it is ideal (consistent, robust, and smooth)\nwith respect to the prediction error. As a building block, we study the ofﬂine version of the problem that\nhas not previously been considered, which is of independent interest.\nWe show that these techniques extend to the all-pairs shorte st-path problem as well.\nOfﬂine Incremental Single-Source Shortest Paths. We give a new algorithm for the ofﬂine version\nof the incremental shortest path problem. In the ofﬂine vers ion of the problem, the sequence of arriving\nedges is given in advance where each edge is assigned a unique time. The goal is to maintain approxi-\nmate distances ˆdt(s,v)for allvand all times tas efﬁciently as possible. That is, given a query (v,t), the\ndata structure outputs the approximate shortest path from s ourcesto vertex vin the graph with edges\ninserted up to time t. By reversing time, the incremental and decremental versio ns of this problem are\nimmediately equivalent.\nSurprisingly, to our knowledge, past work in the ofﬂine sett ing has focused solely on exact, rather\nthan approximate, incremental shortest path. These exact v ersions have strong lower bounds. Rod-\ndity and Zwick [40] show that for the incremental/decrement al single-source shortest paths problem\nin weighted directed (or undirected) graphs, the amortized query/update time must be n1−o(1), unless\nAPSP can be solved in truly subcubic time (i.e. n3−ǫfor constant ǫ >0).\nWe show that the ofﬂine approximate version of the problem can be solved signiﬁcantly faster tha n\nthe exact version in the worst-case setting. This natural pr oblem reveals key algorithmic ideas for de-\nsigning our learned online SSSP algorithm.\nTheorem 1. For the ofﬂine incremental SSSP problem there exists an algo rithm running in worst-case\ntotal time O(mlog(nW)(log3n)(loglogn)/ǫ)that returns (1+ǫ)approximate single-source shortest\npaths for each time t.\nPredictions Model and Learned Online Single Source Shortes t Paths. Letσ=e1,...,emdenote\nthe actual online sequence of edge inserts. Before any edges arrive, the algorithm receives a prediction\nof this sequence, ˆσ. This is the same prediction model considered by Henzinger e t al. [21] and van den\nBrand [45] for the all-pairs shortest-paths problem. Let in dex(e)be the index of einσand/hatwidestindex(e)be\nthe index of einˆσ. Deﬁne /hatwidestindex(e) :=m+1for edges ethat are not in ˆσ. Letηe=|index(e)−/hatwidestindex(e)|\nfor each edge einσ.\nWe ﬁrst describe the performance of our learned SSSP algorit hm in terms of parameters τand\nHIGH(τ). For any τ, deﬁne HIGH (τ)to be the set of edges einσwith error ηe> τ. Then, we show\nthat the bounds obtained are more robust than several natura l measures of prediction error.\n1The˜Onotation suppresses log factors.\n3\n\nTheorem 2. There is a learned online single-source shortest path algor ithm that given a prediction ˆσ\ngives the following guarantees:\n•The algorithm maintains a (1+ǫ)-approximate shortest path among edges that have arrived.\n•The total running time for all edge inserts is ˜O(m·minτ{τ+|HIGH(τ)|}logW/ǫ).\nOur algorithm uses the ofﬂine algorithm as a black box, and th erefore our results also apply to the\ndecremental problem in which edges are deleted one by one.\nThis theorem can be used to give results for two natural error measures. We call the ﬁrst error mea-\nsure, the edit distance Edit(σ,ˆσ), deﬁned as the minimum number of insertions and deletions ne eded to\ntransform σto the prediction ˆσ. To the best of our knowledge this is a new error measure.\nThe second error measure is η= max e∈σηethe maximum error of any edge; this measure was also\nused by past work (e.g. [34, 35, 45]). The theorem gives the fo llowing corollary.\nCorollary 1. There is a learned online algorithm that maintains (1+ǫ)-approximate shortest paths and\nhas running time at most the minimum of ˜O(m·Edit(σ,ˆσ)logW/ǫ)and˜O(mηlogW/ǫ).\nThe corollary can be seen as follows. By deﬁnition, there are no edges in HIGH (η). Thus, set-\ntingτ=ηin Theorem 2 gives an ˜O(mηlogW/ǫ)running time. Alternatively, setting τ=Edit(σ,ˆσ)\nensures that HIGH (τ)contains at most τedges. This is because any edge not inserted or deleted in\nthe process of transforming ˆσtoσcan move at most τpositions and so only inserted or deleted edges\ncontribute to HIGH (τ). This gives a running time of ˜O(mEdit(σ,ˆσ)logW/ǫ).\nDiscussion On Single-Source Shortest Paths. Notice that if a small number of edges have a large\nerrorηe, the Edit(σ,ˆσ)is small even though the maximum error is large, and thus the a lgorithm retains\nstrong running time guarantees on such inputs. Furthermore , Edit(σ,ˆσ)is small even if there is a small\nnumber of edges that are not predicted to arrive but do, or are predicted to arrive but never do (such\nedges are essentially insertions and deletions).\nOn the other hand, the edit distance bound is a loose upper bou nd in some cases: for example, even\nif a large number of edges are incorrect, but have small relat ive change in position between σandˆσ,\nthenηwill be small.\nThe algorithm is consistent as its running time is optimal (since Ω(m)is required to read the input)\nup to log factors when the predictions are perfect. It is smooth in that it has a slow linear degradation\nof running time in terms of the prediction error. We remark th atrobustness to arbitrarily erroneous pre-\ndictions can be achieved with respect to any worst-case algo rithm simply by switching to the worst-case\nalgorithm in the event that the learned algorithm’s run time grows larger than the worst-case guarantee.\nExtension to All-Pairs Shortest Paths. Next, we show that our techniques are generalizable by ap-\nplying them to the all-pairs shortest-path (APSP) problem. Similar to the SSSP case, we ﬁrst solve\nthe ofﬂine incremental version of the problem by running the SSSP algorithm multiple times. We then\nextend it to the online setting with predictions.\nFor the incremental APSP problem, it does not make sense to co nsider amortized cost—Bernstein [5]\ngives an algorithm with nearly-optimal total work for the on line problem even without predictions. As\na result, past work on approximate all-pairs shortest path w ith predictions has focused on improving the\nworst-case time for each update and query.\nFor the APSP problem, we follow [21,45] and assume that ˆσis a permutation of σ—the set of edges\npredicted to arrive are exactly the set that truly arrive.2\nTheorem 3. There is a learned online all-pairs shortest path algorithm with˜O(nmlogW/ǫ)prepro-\ncessing time, O(logn)worst-case update time, and O(η2loglog1+ǫ(nW))worst-case query time.\n2While this is in some cases a strong assumption, it seems unav oidable for worst-case update and query cost.\n4\n\nComparison to Prior Work. To the best of our knowledge, no prior work has considered the incremen-\ntal SSSP problem in the algorithms-with-predictions frame work. Our algorithm for the SSSP problem\nhas a recursive tree of subproblems, that we bring online wit h predictions on the input sequence. We\nremark that the work of Liu and Srinivas [31] gave a general fr amework for taking ofﬂine recursive tree\nalgorithms into the online setting with predictions. Their framework is general and applies to a large\nclass of problems that can be decomposed into recursive subp roblems with smaller cost. In contrast,\nour tree-based decomposition technique is tailored speciﬁ cally to shortest paths which enables our efﬁ-\ncient runtime. Moreover, our analysis differs signiﬁcantl y from [31] as well—we cannot spread the cost\nevenly over smaller-cost recursive subproblems. This is be cause a single edge insert can result in Ω(n)\nchanges in shortest paths. To avoid this, we allow subproble ms to grow and shrink as necessary and\ncharge the cost of a large subproblem to a large number of dist ance changes; see Section 3.\nThe incremental APSP problem with predictions was studied b y past work [21, 45]. Henzinger et\nal. [21] achieve ˜O(1)update time and ˜O(η2)query time, after O(mn3)preprocessing for any weighted\ngraph. van den Brand et al. [45] achieve similar update and qu ery times with ˜O(n(3+ω)/2)preprocess-\ning time; however, the result is limited to unweighted graph s. They further show that this query time\nis essentially optimal: under the Online Matrix-Vector Mul tiplication Hypothesis, it is not possible to\nobtainO(η2−δ)query time for any δ >0while maintaining polynomial preprocessing time and O(n)\nupdate time. Thus, Theorem 3 obtains faster preprocessing t ime for sparse graphs and supports weighted\ngraphs. Finally, we note that Liu and Srinivas [31] give boun ds for the fully dynamic weighted APSP\nproblem in the prediction-deletion model they propose and t heir bounds are incomparable to ours.\n1.1 Additional Related Work\nData Structures with Predictions. Data structures augmented with predictions have demonstra ted\nempirical success in key applications such as indexing [11, 13,26], caching [25,32], Bloom ﬁlters [36,44],\nfrequency estimation [23], page migration [24], routing [8 ].\nInitial theoretical work on data structures with predictio ns focused on improving their space com-\nplexity or competitive ratio, e.g. learned ﬁlters [3, 36, 44 ] and count-min sketch [16, 23] on stochastic\nlearned distributions. Several papers have since used pred ictions to provably improve the running time\nof dictionary data structures, e.g. learned binary-search trees [10, 14, 29, 47]. McCauley et al. [34]\npresented the ﬁrst “ideal” data structures with prediction for the list-labeling problem [34]. They use\na “prediction-decomposition” framework to obtain their bo unds and extend the technique to maintain\ntopological ordering and perform cycle detection in increm ental graphs with predictions [35].\nIncremental SSSP and APSP in the Worst-Case Setting. We review the state-of-the-art determinis-\ntic worst-case algorithms for the incremental (1+ǫ)SSSP problem.\nThe best-known deterministic algorithm for dense graphs is by Gutenberg et al. [39] with total update\ntime˜O(n2logW/ǫO(1)), which is nearly optimal for very dense graphs. For sparse gr aphs, Chechik and\nZhang [9] give an algorithm with total time ˜O(m5/3logW/ǫ), which is improved by Kyng et al. [27]\nto a total time ˜O(m3/2logW/ǫ); this is the best-known deterministic algorithm for sparse graphs. Note\nthat many of these solutions use the ES-tree data structure [ 42] as a key building block. The ES-tree can\nmaintain exact distances in an SSSP tree for weighted graphs with total running time O(mnW)[22],\nwhereWis the ratio of the largest to smallest edge weight. The ES-tr ee can be used to maintain (1+ǫ)-\napproximate distances in total update time ˜O(mnlogW/ǫ)for incremental/decremental directed SSSP;\nsee e.g. [4, 5, 33].\nThis paper shows that even modestly accurate predictions ca n be leveraged to circumvent these high\n(polynomial) worst-case update costs in incremental SSSP.\nFor the approximate incremental APSP problem without predi ctions, Bernstein [5] gave an algorithm\nthat has nearly-optimal total runtime O(nmlog4(n)log(nW)/ǫ)andO(1)look-up time.\nPeng and Rubinstein consider generally how incremental or d ecremental algorithms can be turned\ninto fully dynamic algorithms [37].\n5\n\n2 Preliminaries\nLetσ=e1,...,embe the sequence of all edge insertions, and let Vbe the set of vertices. We assume\nmis of the form 2kthroughout this paper, which can be assumed without loss of g enerality by allowing\ndummy edge inserts. Each edge eihas a weight w(ei)∈[1,W]. LetGtbe the graph consisting of\nverticesVand the ﬁrst tedgese1,...,et; we call this the graph at time t. We deﬁne G0to be the empty\ngraph on the vertex set V.\nThelength of a path is the sum of the weights of its edges. Let dt(v)be the length of the shortest\npath from stovinGt. Throughout this paper, all logarithms are base 2 unless oth erwise speciﬁed.\nProblem Deﬁnition. In the ofﬂine problem, the algorithm is given a sequence of ed ge inserts σ=\ne1,e2,...,em, the set of vertices V, and the source vertex s. The goal of the algorithm is to output a\ndata structure that answers queries (v,t)of the form: what is the length of the shortest path from sto\nvat timet? This answer should be a (1+ǫ)-approximation: speciﬁcally, for a query (v,t), if the data\nstructure returns d, thendt(v)≤d≤dt(v)(1+ǫ).\nIn the online problem, the edges in σ=e1,...,emarrive one at a time. Before any edge arrives, the\nalgorithm is given a prediction ˆσofσ, as well as the set of vertices V={v1,...,vn}and the source\nvertexs. We assume the length of ˆσis the same as σ. At all times, the algorithm must maintain an array\nDcontaining the length of the shortest path to each vertex. In particular, after edge tis inserted, Dmust\nsatisfydt(vi)≤D[i]≤dt(vi)(1+ǫ)for all vertices vi.\nAdjusting ǫ.Throughout this paper, we assume ǫ=O(1). In our algorithms, distance estimates\nˆdt(v)are used for each node vand time tthat satisfy the following invariant (see Lemmas 1 and 4):\ndt(v)≤ˆdt(v)≤dt(v)(1+ǫ/logm)logm.\nWe will need to slightly adjust ǫin our algorithms to account for lower-order terms. For ǫ <1.79, it\nis known that/parenleftbigg\n1+ǫ\nlogm/parenrightbigglogm\n≤eǫ≤1+ǫ+ǫ2,\nwhich ensures that dt(v)≤ˆdt(v)≤dt(v)(1+ǫ+ǫ2). To ensure our algorithms return (1+ǫ)approx-\nimations for dt(v), theǫparameters used are set to be (min{1.79,ǫ})/4.\nDeﬁning Recursive Subproblems. The ofﬂine algorithm is recursive. It divides the interval [0,m]in\nhalf and recurses on both sides. On each recursive call, the a lgorithm will process an interval [ℓ,r]and\nfurther subdivide the interval until it contains a single ed ge. We call each recursive call [ℓ,r]asubprob-\nlem. We refer to this subproblem both as subproblem [ℓ,r]andsubproblem x, wherex= (ℓ+r)/2is\nthe midpoint of [ℓ,r]. Each subproblem corresponds to a node in the recursion tree . We might refer to\nthe subproblem [ℓ,r]in the recursion tree as nodexin the recursion tree. Each time x∈ {1,...,m−1}\nis the midpoint of exactly one node in the tree. Let the level of a time xbe the depth of node xin the\nrecursion tree. So the level of m/2is1, the level of m/4and3m/4is2, and so on. In particular, the\nlevel ofxis one more than the maximum level of ℓandr. For ease of notation, we set the level of 0 and\nmto be 0. See Figure 1 for an illustration of the recursion tree . We refer to the ancestors and descendants\nof a node in the recursion tree as ancestors anddescendants of the corresponding subproblem. All these\ndeﬁnitions extend to the online algorithm as well, as it is bu ilt up on the ofﬂine algorithm.\n3 Technical Overview\n3.1 Ofﬂine Algorithm\nThe preliminary observation behind our algorithm is the fol lowing. Let’s say we round the distance to\neach vertex up to the nearest power of (1 +ǫ). Since the maximum distance to any vertex is nW, this\n6\n\nG0G1G2G3G4G5G6G7G8G9G10G11G12G13G14G15G16\nFigure 1: The recursion tree of the algorithm. Each node xin the tree is associated with an interval [ℓ,r]\nsuch that x= (ℓ+r)/2. The depth of a node is the number of nodes in the path from the r ootm/2to\nthat node. For example, the depth of node x= 6is 3, and its corresponding interval is [4,8].\nmeans that we group distances into O(log1+ǫ(nW)) =O(log(nW)/ǫ)“buckets.” Knowing that the\ndistance to any vertex only decreases over time, the buckets of all vertices only change O(nlog(nW)/ǫ)\ntimes in total. The goal of our algorithm is to list the O(log(nW)/ǫ)times when a vertex shifts from\none bucket to the next.\nThus, when an edge is inserted at time t, our goal is to run Dijkstra’s only on vertices whose distanc e\nis changing at time t, charging the cost to the changing distances.\nRemoving Vertices from Recursive Calls. Our algorithm recursively divides the sequence of edge\ninserts in half, starting with the entire sequence of edge in serts{e1,...,em}, and recursing until the\nsequence contains a single edge.\nConsider a sequence {eℓ,...,er}, which we denote by [ℓ,r]. We divide this interval into two equal\nhalves:[ℓ,x],[x,r].\nThe observation behind our algorithm is the following. Let’ s say we calculate the distance to all\nvertices at time x. Consider a vertex vthat is in the same bucket at ℓandx. Since the distance to v\nis non-increasing as new edges arrive, vmust be in this bucket throughout the interval, and we do not\nneed to continue calculating its distance. Therefore, we sh ould remove vfrom the recursive call for the\ninterval[ℓ,x]. Similarly, if vis in the same bucket at xandr, then we should remove vfrom[x,r].\nIf we are successful in removing vertices this way, we immedi ately improve the running time. Each\nvertexvis included in an interval [ℓ,r]if and only if it shifts from one bucket to another between ℓand\nr. Thus, each time vshifts buckets, it is included in at most logmintervals. This means that each vertex\nis included in O(logmlog(nW)/ǫ)intervals.\nLet’s assume that for an interval [ℓ,r], we can run Dijkstra’s algorithm for the midpoint xin time\n(ignoring log factors) proportional to the number of edges t hat have at least one endpoint included in\nthe interval [ℓ,r]. Since each vertex contributes to the running time of polylo g(nmW)/ǫdifferent sub-\nproblems, we can sum to obtain ˜O(mlogW/ǫ)total cost to calculate the shortest path to every vertex in\nevery subproblem.\nEnsuring Dijkstra’s Runs are Efﬁcient. Let us discuss in more detail how to “remove” a vertex from\na recursive call. We do not remove vertices from the graph; in stead, we say that a vertex is dead during\nan interval [ℓ,r]if its distance bucket does not change during the interval, a ndalive otherwise. Each\nedge(u,v)is alive if and only if vis alive.\nFor each time xwhich is the midpoint of an interval [ℓ,r], consider building a graph G′\nxconsisting\nof all alive vertices and edges in [ℓ,r]. If(u,v)is alive, but uis dead, we also add utoG′\nx. Now, the\ntime required to run Dijkstra’s algorithm on G′\nxis roughly proportional to the number of edges in G′\nxas\nwe desired, but the distances it gives are not meaningful—in fact,smay not even be in G′\nx.\nTo solve this problem, we observe that we already know (appro ximately) the distance to any dead\nvertexu: we marked uas dead because its distance bucket does not change from ℓtor. Thus, we add\nstoG′\nx; then, for each alive edge (u,v)whereuis dead, instead of adding utoG′\nx, we add an edge\n7\n\nfromstovwith weight equal to the rounded distance from stoufrom the previous recursive call plus\nw(u,v). Running Dijkstra’s algorithm on G′\nxnow gives (ostensibly) meaningful distances from s.\nWe calculate the distance to each vertex in G′\nx, round the distance up to obtain its bucket, and then\nrecurse (marking vertices dead as appropriate) on [ℓ,x]and[x,r].\nHandling Error Accumulations. Unfortunately, the above method does not quite work, as the e rror\naccumulates during each recursive call.\nFor an interval [ℓ,r]with midpoint x, we use the terms recursive call [ℓ,r]andrecursive call x\ninterchangeably. For a recursive call x, the shortest path to vmay begin with an edge (s,v′)(for some\ndead vertex v′) weighted according to the bucket of v′. The bucket of v′was calculated by rounding up\nthe length of the shortest path to v′in some graph G′\nyat timey; the length of this path again was also\nrounded up, and so on.\nIn other words, since the weight of each edge from sto a dead vertex v′is based on the bucket of v′,\nit has been rounded up to the nearest power of 1+ǫ. Thus, each recursive call loses a 1+ǫapproximation\nfactor in distance.\nTo solve this, we note that the recursion depth of our algorit hm islog2m. Thus, we split into ﬁner\nbuckets: rather than rounding up to the nearest power of 1 +ǫ, we round up to the nearest power of\n1 +ǫ/log2m. Thus, the total error accumulated over all recursive calls is(1 +ǫ/log2m)log2m≤\n1+ǫ+ǫ2(see Section 2); decreasing ǫslightly gives error 1+ǫ.\nRounding buckets up to the nearest power of 1+ǫ/logmchanges our running time analysis: most\ncritically, each vertex now changes buckets log1+ǫ/logmnW= Θ(log( nW)logm/ǫ)times.\n3.2 Ideal Algorithm with Predictions\nWith perfect predictions, the algorithm described above ru ns in˜O(mlogW/ǫ)time: we can just run\nthe algorithm on the predicted (in fact the actual) sequence ˆσ=σto obtain an approximate distance to\nevery vertex at each point in time. We show how to carefully re build portions of the ofﬂine approach to\nobtain an ideal algorithm.\nWarmup: Hamming Distance of Error. To start, let’s give a simple algorithm that uses prediction s\nthat contain error. Let Ham(σ,ˆσ)be the Hamming distance between σandˆσ—in other words, the\nnumber of edges whose predicted arrival time was not exactly correct in ˆσ. We give an algorithm that\nruns in˜O(m·Ham(σ,ˆσ)logW/ǫ)time.\nThe main idea behind this algorithm is to update the sequence of predictions ˆσas edges come in. At\ntimet, an edge earrives; if earrived at tinˆσ, the algorithm does nothing—the predictions created by\nrunning the ofﬂine algorithm on ˆσtookearriving at tinto account. If edid not arrive at tinˆσ, then the\nalgorithm creates a new ˆσ, replacing the edge arriving at twithe. The algorithm then reruns the ofﬂine\nalgorithm on the new ˆσin˜O(mlogW/ǫ)time.\nSince we run the ofﬂine algorithm (in ˜O(mlogW/ǫ)time) each time an edge was predicted incor-\nrectly, we immediately obtain ˜O(m·Ham(σ,ˆσ)logW/ǫ)time.\nHandling Nearby Edges More Effectively. Ideally, we would not rebuild from scratch each time an\nedge is predicted incorrectly—we would like the running tim e to be proportional to how far an edge’s\ntrue arrival time is from its predicted arrival time.\nOur ﬁnal algorithm improves over the warmup Hamming distanc e algorithm in two ways. First, it\nupdates the predicted sequence ˆσmore carefully. Second, it only rebuilds parts of the recurs ive calls of\nthe ofﬂine algorithm: speciﬁcally, only intervals that cha nged asˆσwas updated.\nFirst, let’s describe how to update ˆσ. As before, when an edge earrives at time t, ifewas predicted\nto arrive at tinˆσthe algorithm does nothing. If ewas predicted to arrive at time t′inˆσ, the algorithm\nmodiﬁes ˆσby inserting eat timet(shifting all edges after tdown one slot), and deleting eat timet′\n8\n\n(shifting all edges after t′up one slot). If eis not predicted in ˆσ, the algorithm only inserts eat timet,\nshifting subsequent edges down one slot.\nThe only entries in ˆσthat change are those between tandt′(inclusive). Therefore, we only need to\nrecalculate G′for times between tandt′.\nFinally, we update the distance array D. The algorithm greedily updates Dduring the above rebuilds\nto maintain the invariant that D[i]stores the estimated distance ˆdt(vi), which as discussed in Section 2,\nis a(1+ǫ)approximation for dt(vi).\nAnalysis. For any edge ethat appears at time index (e)inσand time /hatwidestindex(e)inˆσ, we deﬁne ηe=\n|index(e)−/hatwidestindex(e)|. Ifedoes not appear in ˆσthenηe=|m+1−index(e)|. Letηbe the maximum\nerror:η= max e∈σηe.\nWe show that an edge ecauses a rebuild of a graph G′\ntonly iftis between its predicted arrival\ntime/hatwidestindex(e)and its actual arrival time index (e). This means that each G′\ntcan only be rebuilt O(η)\ntimes. Since the total time to build all G′\ntis˜O(mlogW/ǫ), the total time to rebuild all G′\ntηtimes is\n˜O(mηlogW/ǫ).\nWith a more careful analysis, we can get the best of the Hammin g analysis and the max error analysis.\nFor anyτ, let HIGH (τ)be the set of edges with error more than τ. For all edges with error more than τ,\nwe may (in the worst case) rebuild the entire interval {1,...,m}, for total cost ˜O(m|HIGH(τ)|logW/ǫ).\nFor all edges with error at most τ, the total rebuild cost is, as above, ˜O(mτlogW/ǫ). Thus, we obtain\ntotal cost ˜O(m(logW/ǫ)·minτ{τ+|HIGH(τ)|}).\n4 Ofﬂine Incremental Weighted Directed Shortest Path\nThis section presents an algorithm for the ofﬂine problem wh ich takesO(mlog(nW)(log3n)loglogn/ǫ)\ntime to build, and O(loglog1+ǫ(nW))time to answer a query (v,t).\nThe algorithm maintains an estimate of the shortest path at a ll points in time. Let ˆdt(v)be the es-\ntimate of dt(v)obtained by the algorithm. The algorithm does not explicitl y maintain ˆdt(v)values for\neach vertex vand each time t, because of the following simple observation: if for times ℓandr,ℓ < r ,\nwe have ˆdℓ(v) =ˆdr(v), it means that from the algorithm’s perspective, node vhas the same distance\nfromsin graphs GℓandGr. Since the distances of the nodes from sare non-increasing over time, the\nalgorithm infers that ˆdt(v) =ˆdℓ(v)for eachℓ≤t≤r. Although in such case ˆdt(v)is not explicitly\nstored by the algorithm, we still use this notation to refer t oˆdℓ(v) =ˆdr(v).\nIn each subproblem xwith an interval [ℓ,r], wherex= (ℓ+r)/2, vertices and edges are marked\nby the algorithm as alive or dead. A vertex is alive in subproblem xif its estimated distances are not the\nsame in subproblems ℓandr, otherwise it is dead . In each subproblem x(i.e., node xin the recursion\ntree), the distance estimates ˆdx(v)for all alive nodes vare explicitly maintained in a balanced binary\nsearch tree. This allows us to access ˆdx(v)inO(logn)time for each alive node vin subproblem x.\nMoreover, for each node v, the algorithm maintains a list Lvof length log1+ǫ(nW), representing the\ntimes when v’s estimated distance from smoves from one integer power of (1+ǫ)to another. In partic-\nular, theith entry for a vertex v, denoted by Lv(i), represents the minimum tsuch that ˆdt(v)≤(1+ǫ)i.\nThis is the data structure that the algorithm outputs in orde r to answer queries (v,t). To answer a query\n(v,t), i.e., to obtain a (1+ǫ)-approximation of dt(v), the algorithm performs a binary search on L(v)to\nﬁnd anisuch that Lv(i)≤t < Lv(i−1), and then we return d= (1+ǫ)i. The time needed to obtain\ndis thenlog(log1+ǫ(nW)). Note that ˆdt(v)≤d≤(1 +ǫ)ˆdt(v), which as discussed in Section 2, it\nmeans that dis within a (1+ǫ)factor ofdt(v)for the original ǫ.\nNow we are ready to deﬁne the algorithm.\n4.1 The Ofﬂine Algorithm\nThe ofﬂine algorithm is recursive. It starts with the interv al[0,m], and it recursively divides the interval\nin half and continues on the two subintervals. Initially, th e algorithm marks all vertices and edges as\n9\n\nalive inGm, and all vertices as alive in G0. The algorithm runs Dijkstra’s on Gm, and for each vertex v,\nit setsˆdm(v) =dm(v). The algorithm sets ˆd0(s) = 0 , and for all v∈V\\{s}, it setsˆd0(v) =∞. The\nalgorithm then begins the recursive process on the interval [0,m]. The algorithm stops recursing when\nthe interval [ℓ,r]contains only one new edge; that is, when r−ℓ= 2.\nConsider when the algorithm is given an interval [ℓ,r]to process. On this recursive call (subprob-\nlem), the goal is to calculate the distance estimates ˆdx(v)for alive nodes v, wherex= (ℓ+r)/2is\nthe midpoint of [ℓ,r]. Since the algorithm processes the subproblems in the recur sion tree from top to\nbottom, it has already processed the subproblems ℓandr, i.e., the distance estimates are calculated for\nthe alive nodes in GℓandGr.\nJust to repeat, a vertex vis alive in Gxif it is alive in GℓandGr, and its estimated distance is not\nthe same in GℓandGr, i.e.,ˆdℓ(v)/\\e}atio\\slash=ˆdr(v). If a vertex is not alive, it is dead. A directed edge e= (u,v)\ninGxis said to be alive ifvis alive in Gx; otherwise, eisdead . After the description of the algorithm,\nwe discuss how to efﬁciently maintain alive and dead vertice s and edges. Although the algorithm does\nnot store ˆdx(v)for the dead vertices vin the subproblem x, we still use the notation ˆdx(v)to refer to\nˆdℓ(v) =ˆdr(v).\nNow, we can deﬁne how the algorithm calculates ˆdx(v)for all alive vertices v. The algorithm creates\nanew graph G′\nxwhose vertex set is only the alive vertices in Gxalong with s. Then, for each alive edge\ne∈Gxwithe= (u,v), there are two cases:\n• If both uandvare alive, add etoG′\nx.\n• If only vis alive, add an edge from stovwith weight ˆdx(u)+w(u,v)toG′\nx.\nTo compute ˆdx(u), whereuis a dead vertex in Gx, the algorithm needs to ﬁnd the ﬁrst ancestor of the\ncurrent subproblem in the recursion tree in which uis alive. To do this, the algorithm does a binary\nsearch on the ancestors of node xin the recursion tree, and for the ﬁrst ancestor x′whereuis alive in\nGx′, it recovers the value of ˆdx′(u) =ˆdx(u)using the binary search tree stored at node x′.\nThe algorithm then computes the distance from sto each vertex in G′\nxusing Dijkstra’s algorithm.\nFor any alive vertex v, the algorithm stores the length of the shortest path to vfound by Dijkstra’s\nalgorithm rounded up to the nearest integer power of (1+ǫ/logm)asˆdx(v).\nEfﬁciently Maintaining Alive Edges. For anyx, letmxbe the number of alive edges in Gx. The\nalgorithm maintains a list of all alive edges at each time x.\nWe now describe how to ﬁnd the alive vertices and edges in Gx, wherexis the midpoint of [ℓ,r].\nNote that if an edge (u,v)is alive in Gx, then its head vmust be alive in Gx, which in turn implies that\nvis alive in GℓandGr. Since(u,v)has arrived before time r, it is alive in Gr. Therefore the alive\nedges inGxare a subset of the alive edges in Gr. To obtain the list of alive edges in Gx, the algorithm\niterates over the alive edges in GrinO(mr)time, and removes the edges that either arrived after time x,\nor whose head node has the same estimated distance in both GℓandGr. While doing this, the algorithm\nadditionally updates if each vertex is alive or not in GxinO(mr)time.\nFrom this, the algorithm constructs G′\nxinO(mr)time, and runs Dijkstra’s algorithm in O(mrlogn)\ntime.\nEfﬁciently Maintaining LvLists of Time Indexed Distances. To obtain the Lv(i)values, initially,\nall the entries of Lvare empty. At each time x, when we are calculating the distance estimate ˆdx(v)for\nan alive node v, we update Lv: suppose (1+ǫ)i−1<ˆdx(v)≤(1+ǫ)i. IfLv(i)is empty or x < Lv(i),\nwe setLv(i)to bex. Otherwise, Lv(i)remains unchanged. In the end, the algorithm processes each\nof the lists, and for any empty Lv(i), the algorithm sets it to be equal to the last non-empty entry ofLv\nbeforeLv(i).\n4.2 Analysis of the Ofﬂine Algorithm\nThis section establishes the correctness and running time g uarantees of the algorithm.\n10\n\nLemma 1. Ifxis at level i, then for any vertex v,dx(v)≤ˆdx(v)≤dx(v)(1+ǫ/logm)i.\nProof. We show the lemma by induction on i. Fori= 0, i.e.,x= 0 orx=m, we obtain the exact\ndistance for every vertex.\nFirst, we show that dx(v)≤ˆdx(v). Ifvis dead in subproblem x, thenˆdx(v) =ˆdℓ(v), whereℓis\nthe start of the interval of node xin the recursion tree. So, the level of ℓis smaller than i. By induction,\ndℓ(v)≤ˆdℓ(v). Also, since ℓ≤x, we have dx(v)≤dℓ(v). Therefore, dx(v)≤dℓ(v)≤ˆdℓ(v) =ˆdx(v).\nSo, we can assume vis alive. Let p=s,v1,v2,···,vkbe the shortest path from stov=vkinG′\nx,\nsoˆdx(v)is the length of prounded up to the nearest integer power of 1+ǫ/logm. Note that the path\nv1,v2,···,vkexists inGxas well, since all the edges that only appear in G′\nxstart ats. If the edge (s,v1)\nis present in Gx, thendx(v)≤ˆdx(v), aspis a path from stovinGx, which means that dx(v), the\nlength of the shortest path in Gxfromstov, is at most the length of p. Otherwise, there exists an edge\n(u,v1)inGx, such that uis dead at time x, and the edge (s,v1)has weight ˆdx(u)+w(u,v1). As shown\nabove, since uis dead at time x, we have ˆdx(u)≥dx(u). Therefore,\nˆdx(v)≥ˆdx(u)+w(u,v1)+dG′x(v1,v)≥dx(u)+w(u,v1)+dGx(v1,v)≥dx(v).\nNext, we show that ˆdx(v)≤dx(v)(1+ǫ/logm)i. Letqbe the shortest path from stovinGx. Ifq\nis also present in G′\nx, it follows that ˆdx(v)≤dx(v)(1+ǫ/logm). Otherwise, let (a,b)be the last edge\ninqthat does not exist in G′\nx. Thus, either aorbis not alive in Gx.\nFirst, let’s assume bis not alive in Gx. Thusbmust be the last vertex in q—if there were an edge\n(b,c)inq, this edge would exist in G′\nx, sobwould be alive in Gx(note that b/\\e}atio\\slash=s). Sincebis the last\nvertex inq, andv=bis dead in Gx, it follows that\nˆdx(v) =ˆdr(v)≤dr(v)(1+ǫ/logm)i−1≤dx(v)(1+ǫ/logm)i−1,\nwhereris the end of the interval of node xin the recursion tree. The ﬁrst inequality follows from the\ninduction hypothesis and the fact that the level of ris strictly less than i, and the second inequality is\nbecausex≤rand adding more edges can only decrease the distance of each n ode from s.\nOtherwise, assume bis alive in Gx, which means that ais dead in Gx. Thus, there is an edge (s,b)\ninG′\nxwith weight ˆdx(a)+w(a,b) =ˆdr(a)+w(a,b). Prepending this edge to the sufﬁx of qbeginning\natbresults in a path in G′\nxfromstovof length ˆdr(a)+w(a,b)+dGx(b,v). Since the level of ris less\nthanx, by the induction hypothesis we have ˆdr(a)≤dr(a)(1+ǫ/logm)i−1. Hence,\nˆdx(v)≤/parenleftBig\nˆdr(a)+w(a,b)+dGx(b,v)/parenrightBig\n(1+ǫ/logm)\n≤/parenleftbig\ndr(a)(1+ǫ/logm)i−1+w(a,b)+dGx(b,v)/parenrightbig\n(1+ǫ/logm)\n≤(dx(a)+w(a,b)+dGx(b,v))(1+ǫ/logm)i\n=dx(v)(1+ǫ/logm)i.\nWith the correctness in place, the following lemma complete s the proof of Theorem 1 by bounding\nthe running time of the ofﬂine algorithm.\nLemma 2. The ofﬂine algorithm runs in time O(mlog(nW)(log3n)(loglogn)/ǫ).\nProof. Letmybe the number of alive edges in a subproblem y. For a subproblem [ℓ,r], the time to\nﬁnd the alive edges and nodes in GxisO(mr), wherexis the midpoint of [ℓ,r]. For an alive edge\ne= (u,v)inGx, ifuis alive in Gx, then inserting einG′\nxtakesO(1)time. Otherwise, the algorithm\nneeds to recover ˆdx(u)from the lowest ancestor of xin the recursion tree in which uis alive in order to\ncalculate the weight of the edge (s,v)inG′\nx. Each node yin the recursion tree maintains ˆdy(w)for all\nalive vertices winGyin a balanced binary search tree. Therefore, the algorithm c an check whether a\nvertex is alive in a subproblem in time O(logn). Also, if wis alive in Gy, it takesO(logn)time to ﬁnd\nˆdy(w). Since node xhas at most logmancestors, recovering ˆdx(u)can be done in O(lognloglogm)\n11\n\ntime by doing a binary search on the ancestors. Hence, buildi ngG′\nxtakesO(mxlognloglogm)time.\nThe time to run Dijkstra’s on G′\nxisO(mxlogn). Also, the time to build the balanced binary search tree\ncorresponding to subproblem xisO(mxlogn), as the number of alive nodes in Gxis bounded by the\nnumber of alive edges in Gx. Thus, the runtime of the algorithm is bounded by O(lognloglogm)times\nthe total number of alive edges in all the subproblems.\nFor a given iandv, letxbe the time when ˆd(v)decreases from (1+ǫ/logm)i+1to(1+ǫ/logm)i.\nThenvis alive in any subproblem that contains x; there is at most 1 such subproblem in each level of the\nrecursion tree. For each vertex v, summing over all log1+ǫ/logm(nW) =O(log(nW)logm/ǫ)values\nofiand thelogmlevels of the recursion tree, we have that there are in total O(log(nW)log2m/ǫ)\nsubproblems in which vis alive. Since an edge is only alive if its head is alive, ther e are only a total of\nmlog(nW)log2m/ǫ alive edges over all subproblems. Substituting logm=O(logn), we obtain an\naggregate running time O(mlog(nW)(log3n)(loglogn)/ǫ).\n5 Incremental Shortest Paths with Predictions\nThis section gives the algorithm and analysis for the online version of the problem with predictions.\nRecall that in this model the edges arrive online. The goal of the algorithm is to maintain an array Dof\nlengthn. At any time t,D[i]should contain a (1+ǫ)-approximation of dt(vi).\n5.1 The Algorithm\nThis section describes the online algorithm. Before any edg es arrive, the algorithm is given a prediction\nˆσof the edge arrival sequence.\nUpdated Predictions. Our algorithm dynamically maintains the predicted sequenc e of edge inserts by\nchanging ˆσonline. Speciﬁcally, after the tth edge arrival, the algorithm will construct a new predicti on\nwhich we refer to as ˆσt. Initially, ˆσ0= ˆσ. We call the sequence ˆσttheupdated prediction . Intuitively, the\nalgorithm modiﬁes the updated prediction after each edge ar rival based on which edge actually arrives.\nFor each edge e, let/hatwidestindext(e)be the position of einˆσt. Let index (e)denote the position of einσ\n(i.e. the true time when earrives). If e /∈ˆσt, then we deﬁne /hatwidestindext(e) :=m+1.\nOur algorithm updates the sequence of edges to agree with all edges seen so far. In other words, at\ntimet, the algorithm maintains that for all edges ethat arrive by tinσ,/hatwidestindext(e) =index(e).\nMaintaining Metadata from the Ofﬂine Algorithm. Since the online algorithm continuously up-\ndates the result of the ofﬂine approach, it stores informati on to help it navigate the result of the ofﬂine\nalgorithm.\nRecall that the ofﬂine algorithm stores, for each time t, the set of vertices and edges alive at time\nt, as well as a distance estimate ˆdt(v)for any vertex valive at time t. The online algorithm maintains\nexactly the same information. We will see that the algorithm updates these estimates continuously as the\nupdated prediction changes.\nAlgorithm Description. First, let us describe the preprocessing performed by the al gorithm before\nany edges arrive. The algorithm sets D[i] =∞for alli, except for D[1] = 0 , which represents the\nsource. Then, the algorithm runs the ofﬂine algorithm from S ection 4 on ˆσ. By running the ofﬂine\nalgorithm, the algorithm will store ˆdt(v)for all times tand all nodes valive at time t.\nNow, let us describe how the algorithm runs after the tth edge is inserted. At each time t, the al-\ngorithm rebuilds a subset of all the subproblems. These rebuilds update the pr ecomputed ˆdt(v). When\na subproblem is rebuilt, all of its descendants are rebuilt a s well. The rebuilding procedure is formally\ndescribed below.\n12\n\nG0G1G2G3G4G5G6G7G8G9G10G11G12G13G14G15G16\nFigure 2: An illustration of the subproblems that get rebuil t during one edge insertion. In this example,\nat timet= 4, the edge e4was predicted to arrive but edge e8has arrived. So the algorithm moves\nedgee8from position t′= 8 to position t= 4. The algorithm then rebuilds all the subproblems with\nt≤(ℓ+r)/2< t′(colored dark gray) and their descendants (colored light gr ay) from top to bottom.\nLetetbe the edge that arrives at time t, and lett′=/hatwidestindext−1(et), i.e.,t′is the predicted arrival time\nforetin the updated predictions immediately before it arrives. N ote that since the ﬁrst t−1edges ofσ\nandˆσt−1are the same, we always have t′≥t(assuming the edges in the input sequence are distinct).\nFirst, we describe how the algorithm updates ˆσt−1to getˆσt.\n• Ift=t′, i.e., the position of etis predicted correctly at the time it is seen, then set ˆσt:= ˆσt−1.\n• Ift/\\e}atio\\slash=t′andt′≤m; that is,etis predicted to arrive at a later time. In this case, the algor ithm\nmovesetfrom position t′to position tinˆσt−1, and shifts everything between tandt′one slot to\nthe right to obtain ˆσt.\n• Ift/\\e}atio\\slash=t′andt′=m+1; that is, at time t−1,etis not in the predicted sequence. In this case, the\nalgorithm inserts etin position t, shifts the rest of the sequence one slot to the right, and tru ncates\nthe predicted sequence to length mto obtain ˆσt.\nRebuilding Subproblems. Next, the algorithm rebuilds subproblems. Recall that the algorithm given\nin Section 4 is recursive; each of its recursive calls can be r epresented by a node in the recursion tree.\nTo rebuild a subproblem [ℓ,r], the ofﬂine algorithm is called on [ℓ,r]using the updated prediction\nˆσt. The rebuild makes recursive calls as normal. Any time a subp roblem with midpoint xis rebuilt, the\nvalue ofˆdx(v)is updated based on the rebuild for all alive vertices v.\nLet[ℓm,rm]be the largest subproblem with t≤(ℓm+rm)/2< t′; then the algorithm rebuilds\n[ℓm,rm]. As mentioned above, all descendants of [ℓm,rm]will be recursively called, and therefore re-\nbuilt as well. In other words, the algorithm rebuilds all the subproblems [ℓ,r], witht≤(ℓ+r)/2< t′,\nand all of their descendants from top to bottom (so in the ﬁrst case, no subproblem gets rebuilt). See\nFigure 2 for an illustration. Let rebuild (t)be the set of all times t′′such that t′′∈[ℓ,r]for some [ℓ,r]\nrebuilt at time t. If no subproblem is rebuilt at time t, we deﬁne rebuild (t) :={t}to insure that tis\nalways in rebuild (t).\nUpdating the Distance Array. Finally, the algorithm must update the array Dcontaining the esti-\nmated distance to each vertex. When a new edge is inserted, so me of the entries in Dneed to be over-\nwritten, as their estimated distance might have changed. At each time t, we want to have D[i] =ˆdt(vi)\nfor alli. To do so, the algorithm does the following for each time t′∈rebuild(t), wheret′≤t, in sorted\norder. The algorithm iterates through all alive vertices viinGt′, and sets D[i] =ˆdt′(vi).\n13\n\n5.2 Analysis\nThis section establishes the theoretical guarantees of the algorithm. That is, the correctness of the ap-\nproximation of the distances as well as the runtime bounds.\nWe begin with some structure that applies to any run of the ofﬂine algorithm. This will help us argue\ncorrectness of our algorithm.\nLemma 3. For any sequence of edge inserts σ′, letˆdt(v)be the distance estimates that result from\nrunning the ofﬂine algorithm on σ′. Then for any vertex vand time t≥1, ifvis dead at time t, then\nˆdt(v) =ˆdt−1(v).\nProof. Ift=m, thenvcannot be dead at time t. Otherwise, since 1≤t≤m−1,tis the midpoint of\nsome subproblem [ℓ,r]withr−ℓ≥2, which means ℓ < t . Ifvis dead at time t, thenˆdℓ(v) =ˆdt(v).\nIfℓ=t−1, the claim is proven. Otherwise, we must have t−1∈(ℓ,t). Therefore, t−1is the\nmidpoint of some descendant of [ℓ,t]in the recursion tree. Since vis dead during all such recursive\ncalls,ˆdt−1(v) =ˆdℓ(v) =ˆdt(v).\nIf the edge that arrives at time tis predicted to arrive at time t′according to ˆσt−1, we say the edge\njumps fromt′tot. In such case, we say it jumps over all the positions t≤i < t′.\nThe next two lemmas prove the correctness of the algorithm.\nLemma 4. For any time tand any vertex v, we have dt(v)≤ˆdt(v)≤dt(v)(1+ǫ).\nProof. For each time t= 0,...,m , letTtbe the recursion tree of all subproblems when the ofﬂine\nalgorithm is run on ˆσt, and letˆTtbe the recursion tree of the online algorithm after tedge insertions. In\neach of these trees, for each tree node x, the additional information of ˆdx(v)for all alive vertices vin\nGxis stored in a balanced binary search tree.\nWe show ˆTt=Ttfor allt= 0,...,m , which proves that the distance estimates for all the nodes\nare similar in ˆTtandTt. The result then follows from Lemma 1 and the fact that ˆσt(i) =σ(i)for\ni= 1,···,t.\nWe prove this by induction on t. Fort= 0, since the algorithm starts by running the ofﬂine algorithm\nonˆσ0, it follows that ˆT0=T0. Lett≥1and assume ˆTt−1=Tt−1. We want to show ˆTt=Tt.\nAt timet, if the newly arrived edge etwas correctly predicted, we have ˆσt= ˆσt−1and the on-\nline algorithm does not change the recursion tree, meaning t hatˆTt=ˆTt−1. Also, since ˆσt= ˆσt−1,\nwe have Tt=Tt−1, and by the induction hypothesis, we conclude that ˆTt=Tt. So, assume that\nt′=/hatwidestindext−1(et)> t. Soetjumps from t′tot, and the online algorithm rebuilds all the subproblems\n[ℓ,r]whereetjumps over their midpoint x= (ℓ+r)/2and all their descendants.\nWe show that the algorithm rebuilds all the necessary subpro blems. In other words, we show each\nsubproblem xthat is not rebuilt, i.e., is the same in ˆTtandˆTt−1, is also the same in TtandTt−1. Since,\nby the induction hypothesis, xis the same in trees ˆTt−1andTt−1, this proves that xis the same in ˆTtand\nTt, as desired.\nAssume for sake of contradiction that there is an interval [ℓ,r]with midpoint xsuch that etdoes not\njumps over the midpoint of [ℓ,r]or the midpoint of any of the ancestors of [ℓ,r], butxis different in\nTt−1andTt, meaning that the binary search trees that store ˆdx(v)for alive vertices vinGxare different\ninTt−1andTt. Without loss of generality, assume no ancestor of xhas these properties.\nSincexis not rebuilt, none of its ancestors is rebuilt either. By th e choice of x, this means that all the\nancestors of xare the same in Tt−1andTt. The binary search tree stored at xdepends on the auxiliary\ngraphG′\nx, which in turn only depends on the alive nodes and edges in Gx, and the distance estimates\nˆdx(u)for each dead vertex uthat is the tail of an alive edge. For any vertex uthat is dead in Gx,ˆdx(u)\nis stored in the binary search tree of some ancestor yofx, whereuis alive in Gy. Sinceyis the same in\nTt−1andTt, these distance estimates are the same in Tt−1andTt. A node is alive in Gxif and only if it is\nalive inGℓandGr, and its distance estimates differ at ℓandr. An edge is alive in GxinTt(respectively,\nTt−1) if it is among the ﬁrst xedges inˆσt(respectively, ˆσt−1) , and its head is alive. Since the edge ethas\nnot jumped over x, we conclude that the set of the ﬁrst xedges inˆσtis similar to that of ˆσt−1. The set of\n14\n\nalive vertices in Gxonly depends on the alive vertices in GℓandGr. Sinceℓandrare ancestors of x, the\nbinary search trees that store distance estimates to alive v ertices in GℓandGrare exactly the same in Tt−1\nandTt. Therefore, the set of alive vertices and edges in Gxare the same in Tt−1andTt. Putting every-\nthing together, we conclude that node xis similar in Tt−1andTt, which contradicts the choice of x.\nLemma 5. After inserting edge et, we have D[i] =ˆdt(vi)fori= 1,...,n .\nProof. We show the lemma by induction on t. The case t= 0is trivial.\nIfviis dead in Gt′for every t′∈rebuild(t), wheret′≤t, thenviis dead at time t(note that by deﬁ-\nnition we always have t∈rebuild(t)). Also, in this case, D[i]is not overwritten at time t. By Lemma 3\nand the induction hypothesis, D[i] =ˆdt−1(vi) =ˆdt(vi).\nOtherwise, let t′be the largest time less than or equal to tin rebuild (t)at whichviis alive. By def-\ninition of the algorithm, D[i] =ˆdt′(vi). Ift′=t, thenD[i] =ˆdt(vi), and ift′< t, then by repeatedly\napplying Lemma 3, ˆdt(vi) =ˆdt′(vi).\nNow we determine the aggregate runtime of the online algorit hm. Consider dividing σinto two\nsets of high- and low-error edges based on an integer paramet erτ≥0. Let LOW (τ) ={e∈σ:\n|index(e)−/hatwidestindex0(e)| ≤τ}and HIGH (τ) ={e1,...,em}\\LOW(τ). Therefore, HIGH (τ)is the set\nof edges whose initial predicted arrival time is more than τslots away from their actual arrival time.\nLemma 6. For any integer τ≥0and position i∈ {1,...,m}, there are at most τ+2|HIGH(τ)|jumps\noveri.\nProof. Fixτ. Lethtbe the number of edges in HIGH (τ)that have arrived by time t. To begin, we\nclaim that at any time t, each edge e∈LOW(τ)is at most τ+htslots ahead of its true position, i.e.,\n/hatwidestindext(e)−index(e)≤τ+ht. This is true at t= 0by deﬁnition. For sake of contradiction, consider the\nﬁrst time tat which this condition does not hold for some edge e, i.e.,/hatwidestindext−1(e)−index(e)≤τ+ht−1,\nbut/hatwidestindext(e)−index(e)> τ+ht. Notice that at each time the position of an edge can increase by\nat most 1, so the above can only happen if /hatwidestindext(e) =/hatwidestindext−1(e) + 1 andht=ht−1, meaning the\nedgee′inserted at time tis in LOW (τ). Since insertion of e′changes the position of e,e′must have\njumped from a position greater than /hatwidestindext−1(e)to position t. Thus/hatwidestindext−1(e′)>/hatwidestindext−1(e). Also\nnote that since e′has jumped over ein the updated predictions, it means that ehas not arrived yet, i.e.,\nindex(e)> t=index(e′). Therefore,\n/hatwidestindext−1(e′)−index(e′)≥/hatwidestindext−1(e)−index(e)+2 =/hatwidestindext(e)−index(e)+1> τ+ht=τ+ht−1,\nwhich contradicts the choice of t.\nNow, ﬁx a position i∈ {1,...,m}. At each time t, either no jump happens, or some edge jumps\nfrom position t′> tto position t. Consider the ﬁrst time t∗an edgee∈LOW(τ)jumps over i. We want\nto show that t∗cannot be “much smaller” than i. Assume this jump is from position t′> ito position\nt∗≤i. So, the position of eat timet∗−1in the updated prediction is t′, i.e.,/hatwidestindext∗−1(e) =t′. Also,\nthe actual position of eist∗, i.e., index (e) =t∗. We know that\nt′−t∗=/hatwidestindext∗−1(e)−index(e)≤τ+ht∗−1≤τ+|HIGH(τ)|,\nwhich means that i−t∗< t′−t∗≤τ+|HIGH(τ)|.\nBefore time t∗, all the edges that might have jumped over iare in HIGH (τ). So there are at most\n|HIGH(τ)|jumps over ibefore time t∗. Also, after time i, no edge can jump over i. Therefore, the total\nnumber of jumps over iis at most |HIGH(τ)|+(i−t∗+1)≤τ+2|HIGH(τ)|.\nWe now prove the running time guarantees of the online algori thm.\nLemma 7. The online algorithm runs in time ˜O/parenleftBig\nm·min\nτ{τ+|HIGH(τ)|}·logW/ǫ/parenrightBig\n.\n15\n\nProof. Each subproblem [ℓ,r]is rebuilt only if an edge jumps over its midpoint or the midpo int of one\nof its ancestors. Each subproblem has at most logmancestors. For each τ, it follows from Lemma 6\nthat the subproblem [ℓ,r]is rebuilt at most (logm)(τ+2|HIGH(τ)|)times. From Lemma 2, we know\nit takes˜O(mlogW/ǫ)time to rebuild all the subproblems once. Thus, the time it ta kes to do all the\nrebuilds in the online algorithm is ˜O/parenleftBig\nm·min\nτ{τ+|HIGH(τ)|}·logW/ǫ/parenrightBig\n.\nWe must also account for the time required to maintain the dis tance array D. Consider the updates\nonDat timet. If no subproblem gets rebuilt at time t, we only iterate through the alive vertices in Gt\nwhen updating D. We can charge this cost to the cost of the last rebuild of subp roblemt, as the last time\nsubproblem twas rebuilt, it had the same set of alive vertices as it has at t imet. Otherwise, in order to\nupdateD, we only iterate once through the alive vertices of a subset o f the subproblems that get rebuilt\nat timet, and we can charge this cost to the cost of rebuild of these sub problems at time t. Note that\nthis way, each subproblem that gets rebuilt throughout all t he edge insertions gets charged at most once,\nwhich means that maintaining the distance array Ddoes not have any asymptotic overhead.\nFinally, we need to add the time needed to update the predicte d sequence ˆσ. The total number\nof slots an edge e∈ {e1,...,em}is shifted by over all edges equals the total number of times a\npositioni∈ {1,...,m}is jumped over for all positions. By Lemma 6, each position ge ts jumped\nover at most τ+ 2|HIGH(τ)|times for any τ. Therefore, updating the predicted sequence takes\nO(m·(τ+|HIGH(τ)|))time for any τ.\nThus, the total runtime of the algorithm is ˜O/parenleftBig\nm·min\nτ{τ+|HIGH(τ)|}·logW/ǫ/parenrightBig\n.\nTheorem 2 follows from Lemma 5 and the discussion in Section 2 for the approximation guarantees\nand Lemma 7 for the runtime.\n6 All Pairs Shortest Paths\nOur single-source approach can be run repeatedly to approxi mate distances between all pairs of vertices.\nSpeciﬁcally, the goal is to preprocess G0,...,G msuch that given i,j, andt, we can quickly ﬁnd a\n(1+ǫ)-approximation of dt(i,j), the distance from itojinGt. We run the single source shortest path al-\ngorithm for each source s∈V, storing a separate data structure for each. This requires ˜O(nmlogW/ǫ)\ntime and ˜O(n2logW/ǫ)space, gives the following corollary.\nCorollary 2. For the ofﬂine incremental all-pairs shortest-paths probl em, there exists an algorithm run-\nning in total time O(nmlog(nW)log3nloglogn/ǫ)that returns (1+ǫ)approximate shortest paths for\neach pair of vertices for each time t.\nOnline Learned APSP Algorithm. For the online setting, we consider the worst-case update an d\nquery bounds. In particular, the algorithm ﬁrst preprocess esˆσ. Then, it obtains the edges from σone by\none; the time to process any such edge is the update time. At an y time during this sequence of inserts\nthe algorithm can be queried for the distance between any two vertices in the graph; the time required to\nanswer the query is the query time.\nWe can immediately combine this idea with the techniques of v an den Brand et al. [45, Theorem\n3.1] to obtain Theorem 3.\nProof of Theorem 3. We brieﬂy summarize the algorithm of van den Brand et al.; see the proof of\nTheorem 3.1 in [45] for the full details. To begin, we run the a lgorithm from Corollary 2 on ˆσin\n˜O(nmlogW/ǫ)time, so that for all t,i,j we can ﬁnd ˆdt(i,j)inO(loglog1+ǫ(nW))time.\nOn a query at time tfor vertices iandj, we do the following. Let t′be the latest time such that all\nedges that were predicted to appear by time t′inˆσhave actually appeared (in σ) by time t. Notice that\nifˆσis a permutation of σ, thent−t′≤η(see the discussion immediately after Theorem 3.1 in [45]\nfor a formal proof). Let E′be the set of all edges that have arrived by time tminus the edges that were\npredicted to arrive by time t′. Then,|E′|=t−t′≤η. We construct G′with a set of vertices equal to\n16\n\nall vertices in any edge in E′, plusiandj. The graph G′is complete. For each edge e= (u,v)inG′,\nthe weight of the edge is the minimum of: (1) the weight of any e dge from utovinE′, and (2)ˆdt′(u,v).\nWe run Dijkstra’s algorithm to obtain the distance from itojinG′; this answers the query. Constructing\nG′and running Dijkstra’s algorithm can be done in time O(η2loglog1+ǫ(nW)).\nLet us brieﬂy explain why this works. First, let’s show that t he answer is at least dt(i,j). Consider\nthe shortest path PfromitojinG′. Each edge e= (u,v)inPis one of two types: either an edge\ninE′, or an edge with weight ˆdt′(u,v). The edge with weight ˆdt′(u,v)upper bounds the length of the\nshortest path between uandvover edges that were predicted to arrive by t′. All such edges are in Gt\nby deﬁnition of t′. Thus, by concatenating the edges from PinE′and the subpaths corresponding to\nthe edges in Pnot inE′, we obtain a path P′inGtwith weight at most that of P. Now, we show the\nanswer is at most (1+ǫ)dt(i,j). Consider the shortest path QfromitojinGt. We can decompose Q\ninto a sequence of subpaths, each of which is either an edge in E′, or consists of a sequence of edges not\ninE′. For each edge and subpath, there is an edge in G′with weight at most (1+ǫ)larger by deﬁnition;\nconcatenating these edges we obtain a path Q′inG′of weight at most (1+ǫ)larger.\nWith queries in mind, there are two goals for updates: we must be able to construct E′efﬁciently,\nand we must maintain t′. We can store the inserted edges in a balanced binary-search tree, which takes\nO(logn)time per insert to maintain. At each time t, the BST contains /hatwidestindex(e)for all edges ethat have\narrived by time t. This allows us to ﬁnd t′and construct |E′|inO(logn)andO(|E′|logn) =O(ηlogn)\ntime, respectively (see the discussion in [45, Theorem 3.1] ).\n7 Conclusion\nLearned data structures have been shown to have strong empir ical performance and have the potential\nto be widely used in systems. There is a need to develop an algo rithmic foundation on how to leverage\npredictions to speed up worst-case data structures.\nIn this paper, we build on this recent line of work, and provid e new algorithmic techniques to solve\nthe fundamental problem of single-source shortest paths in incremental graphs. As our main result, we\ndesign an ideal (consistent, robust, and smooth) algorithm for this problem. Our algorithm is optimal\n(up to log factors) with perfect predictions and circumvent s the high worst-case update cost of state-of-\nthe-art solutions even under reasonably accurate predicti ons.\nReferences\n[1] Xingjian Bai and Christian Coester. Sorting with predic tions. In Thirty-seventh Conference on\nNeural Information Processing Systems , 2023.\n[2] Ziyad Benomar and Christian Coester. Learning-augment ed priority queues. In The Thirty-eighth\nAnnual Conference on Neural Information Processing System s, 2024.\n[3] Ioana O. Bercea, Jakob Bæk Tejs Houen, and Rasmus Pagh. Da isy Bloom Filters. In Hans L.\nBodlaender, editor, 19th Scandinavian Symposium and Workshops on Algorithm The ory (SWAT\n2024) , volume 294 of Leibniz International Proceedings in Informatics (LIPIcs ), pages 9:1–9:19,\nDagstuhl, Germany, 2024. Schloss Dagstuhl – Leibniz-Zentr um f¨ ur Informatik.\n[4] Aaron Bernstein. Fully dynamic (2+ ε) approximate all-pairs shortest paths with fast query and\nclose to linear update time. In 2009 50th Annual IEEE Symposium on Foundations of Computer\nScience , pages 693–702. IEEE, 2009.\n[5] Aaron Bernstein. Maintaining shortest paths under dele tions in weighted directed graphs. SIAM\nJournal on Computing , 45(2):548–574, 2016.\n17\n\n[6] Aaron Bernstein, Maximilian Probst Gutenberg, and That chaphol Saranurak. Deterministic\ndecremental SSSP and approximate min-cost ﬂow in almost-li near time. In 62nd IEEE Annual\nSymposium on Foundations of Computer Science, FOCS 2021, De nver, CO, USA, February 7-10,\n2022 , pages 1000–1008. IEEE, 2021.\n[7] Aaron Bernstein, Maximilian Probst Gutenberg, and Chri stian Wulff-Nilsen. Near-optimal decre-\nmental SSSP in dense weighted digraphs. In Sandy Irani, edit or,61st IEEE Annual Symposium on\nFoundations of Computer Science, FOCS 2020, Durham, NC, USA , November 16-19, 2020 , pages\n1112–1122. IEEE, 2020.\n[8] Aditya Bhaskara, Sreenivas Gollapudi, Kostas Kollias, and Kamesh Munagala. Adaptive probing\npolicies for shortest path routing. Advances in Neural Information Processing Systems (NeurIP S),\n33:8682–8692, 2020.\n[9] Shiri Chechik and Tianyi Zhang. Incremental single sour ce shortest paths in sparse digraphs.\nInProceedings of the 2021 ACM-SIAM Symposium on Discrete Algo rithms (SODA) , pages\n2463–2477. SIAM, 2021.\n[10] Jingbang Chen and Li Chen. On the power of learning-augm ented BSTs. arXiv preprint\narXiv:2211.09251 , 2022.\n[11] Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alag appan, Brian Kroth, Andrea\nArpaci-Dusseau, and Remzi Arpaci-Dusseau. From WiscKey to Bourbon: A learned index\nfor log-structured merge trees. In 14th USENIX Symposium on Operating Systems Design and\nImplementation (OSDI) , pages 155–171, 2020.\n[12] Sami Davies, Benjamin Moseley, Sergei Vassilvitskii, and Yuyan Wang. Predictive ﬂows for faster\nford-fulkerson. In Andreas Krause, Emma Brunskill, Kyungh yun Cho, Barbara Engelhardt, Sivan\nSabato, and Jonathan Scarlett, editors, Proc. of the 40th International Conference on Machine\nLearning (ICML) , volume 202 of Proceedings of Machine Learning Research , pages 7231–7248.\nPMLR, 23–29 Jul 2023.\n[13] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyo ung Do, Yinan Li, Hantian Zhang,\nBadrish Chandramouli, Johannes Gehrke, Donald Kossmann, e t al. ALEX: an updatable adaptive\nlearned index. In Proc. 46th Annual ACM International Conference on Manageme nt of Data\n(SIGMOD) , pages 969–984, 2020.\n[14] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjami n Moseley, Aidin Niaparast, and\nSergei Vassilvitskii. Binary search with distributional p redictions. In The Thirty-eighth Annual\nConference on Neural Information Processing Systems , 2024.\n[15] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjami n Moseley, and Sergei Vassilvitskii. Faster\nmatchings via learned duals. In Marc’Aurelio Ranzato, Alin a Beygelzimer, Yann N. Dauphin,\nPercy Liang, and Jennifer Wortman Vaughan, editors, Proc. 34th Conference on Advances in\nNeural Information Processing Systems (NeurIPS) , pages 10393–10406, 2021.\n[16] Elbert Du, Franklyn Wang, and Michael Mitzenmacher. Pu tting the “learning” into learning-\naugmented algorithms for frequency estimation. In Proc. 38th Annual International Conference\non Machine Learning (ICML) , pages 2860–2869. PMLR, 2021.\n[17] Maximilian Probst Gutenberg and Christian Wulff-Nils en. Deterministic algorithms for decre-\nmental approximate shortest paths: Faster and simpler. In S huchi Chawla, editor, Proceedings of\nthe 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA 20 20, Salt Lake City, UT, USA,\nJanuary 5-8, 2020 , pages 2522–2541. SIAM, 2020.\n18\n\n[18] Monika Henzinger, Sebastian Krinninger, and Danupon N anongkai. Sublinear-time decremental\nalgorithms for single-source reachability and shortest pa ths on directed graphs. In Proceedings of\nthe forty-sixth annual ACM symposium on Theory of computing , pages 674–683, 2014.\n[19] Monika Henzinger, Sebastian Krinninger, and Danupon N anongkai. Improved algorithms\nfor decremental single-source reachability on directed gr aphs. In Automata, Languages, and\nProgramming: 42nd International Colloquium, ICALP 2015, K yoto, Japan, July 6-10, 2015,\nProceedings, Part I 42 , pages 725–736. Springer, 2015.\n[20] Monika Henzinger, Sebastian Krinninger, and Danupon N anongkai. Dynamic approximate\nall-pairs shortest paths: Breaking the o(mn) barrier and de randomization. SIAM J. Comput. ,\n45(3):947–1006, 2016.\n[21] Monika Henzinger, Barna Saha, Martin P. Seybold, and Ch ristopher Ye. On the complexity of\nalgorithms with predictions for dynamic graph problems. In Venkatesan Guruswami, editor, 15th\nInnovations in Theoretical Computer Science Conference, I TCS 2024, January 30 to February\n2, 2024, Berkeley, CA, USA , volume 287 of LIPIcs , pages 62:1–62:25. Schloss Dagstuhl -\nLeibniz-Zentrum f¨ ur Informatik, 2024.\n[22] Monika Rauch Henzinger and Valerie King. Fully dynamic biconnectivity and transitive closure. In\nProceedings of IEEE 36th Annual Foundations of Computer Sci ence, pages 664–672. IEEE, 1995.\n[23] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian . Learning-based frequency estimation\nalgorithms. In Proc. 7th Annual International Conference on Learning Repr esentations (ICLR) ,\n2019.\n[24] Piotr Indyk, Frederik Mallmann-Trenn, Slobodan Mitro vic, and Ronitt Rubinfeld. Online page\nmigration with ML advice. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera,\neditors, International Conference on Artiﬁcial Intelligence and St atistics, (AISTATS) , volume 151\nofProceedings of Machine Learning Research , pages 1655–1670. PMLR, 2022.\n[25] Zhihao Jiang, Debmalya Panigrahi, and Kevin Sun. Onlin e algorithms for weighted paging with\npredictions. In Artur Czumaj, Anuj Dawar, and Emanuela Mere lli, editors, 47th International\nColloquium on Automata, Languages, and Programming, (ICAL P), volume 168 of LIPIcs , pages\n69:1–69:18. Schloss Dagstuhl - Leibniz-Zentrum f¨ ur Infor matik, 2020.\n[26] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neo klis Polyzotis. The case for learned\nindex structures. In Gautam Das, Christopher M. Jermaine, a nd Philip A. Bernstein, editors,\nProc. 44th Annual International Conference on Management o f Data, (SIGMOD) , pages 489–504.\nACM, 2018.\n[27] Rasmus Kyng, Simon Meierhans, and Maximilian Probst Gu tenberg. Incremental sssp for sparse\ndigraphs beyond the hopset barrier. In Proceedings of the 2022 Annual ACM-SIAM Symposium\non Discrete Algorithms (SODA) , pages 3452–3481. SIAM, 2022.\n[28] Silvio Lattanzi, Ola Svensson, and Sergei Vassilvitsk ii. Speeding up bellman ford via minimum vi-\nolation permutations. In Andreas Krause, Emma Brunskill, K yunghyun Cho, Barbara Engelhardt,\nSivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning,\nICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of Proceedings of Machine\nLearning Research , pages 18584–18598. PMLR, 2023.\n[29] Honghao Lin, Tian Luo, and David Woodruff. Learning aug mented binary search trees. In Proc.\n35th International Conference on Machine Learning (ICML) , pages 13431–13440. PMLR, 2022.\n19\n\n[30] Honghao Lin, Tian Luo, and David P. Woodruff. Learning a ugmented binary search trees. In\nKamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepe sv´ ari, Gang Niu, and Sivan Sabato,\neditors, International Conference on Machine Learning, ICML 2022, 1 7-23 July 2022, Baltimore,\nMaryland, USA , volume 162 of Proceedings of Machine Learning Research , pages 13431–13440.\nPMLR, 2022.\n[31] Quanquan C. Liu and Vaidehi Srinivas. The predicted-up dates dynamic model: Ofﬂine, incremen-\ntal, and decremental to fully dynamic transformations. In S hipra Agrawal and Aaron Roth, editors,\nProceedings of Thirty Seventh Conference on Learning Theor y, volume 247 of Proceedings of\nMachine Learning Research , pages 3582–3641. PMLR, 30 Jun–03 Jul 2024.\n[32] Thodoris Lykouris and Sergei Vassilvitskii. Competit ive caching with machine learned advice.\nJournal of the ACM (JACM) , 68(4):1–25, 2021.\n[33] Aleksander Madry. Faster approximation schemes for fr actional multicommodity ﬂow problems\nvia dynamic graph algorithms. In Proceedings of the forty-second ACM symposium on Theory of\ncomputing , pages 121–130, 2010.\n[34] Samuel McCauley, Benjamin Moseley, Aidin Niaparast, a nd Shikha Singh. Online list labeling\nwith predictions. In A. Oh, T. Naumann, A. Globerson, K. Saen ko, M. Hardt, and S. Levine,\neditors, Proc. 36th Conference on Neural Information Processing Sys tems (NeurIPS) , volume 36,\npages 60278–60290. Curran Associates, Inc., 2023.\n[35] Samuel Mccauley, Benjamin Moseley, Aidin Niaparast, a nd Shikha Singh. Incremental topological\nordering and cycle detection with predictions. In Ruslan Sa lakhutdinov, Zico Kolter, Katherine\nHeller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, an d Felix Berkenkamp, editors, Proceed-\nings of the 41st International Conference on Machine Learni ng, volume 235 of Proceedings of\nMachine Learning Research , pages 35240–35254. PMLR, 21–27 Jul 2024.\n[36] Michael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. Proc.\n31st Conference on Neural Information Processing Systems ( NeurIPS) , 31, 2018.\n[37] Binghui Peng and Aviad Rubinstein. Fully-dynamic-to- incremental reductions with known\ndeletion order (e.g. sliding window). In Telikepalli Kavit ha and Kurt Mehlhorn, editors, 2023\nSymposium on Simplicity in Algorithms, SOSA 2023, Florence , Italy, January 23-25, 2023 , pages\n261–271. SIAM, 2023.\n[38] Adam Polak and Maksym Zub. Learning-augmented maximum ﬂow. Information Processing\nLetters , 186:106487, 2024.\n[39] Maximilian Probst Gutenberg, Virginia Vassilevska Wi lliams, and Nicole Wein. New algorithms\nand hardness for incremental single-source shortest paths in directed graphs. In Proceedings of\nthe 52nd Annual ACM SIGACT Symposium on Theory of Computing , pages 153–166, 2020.\n[40] Liam Roditty and Uri Zwick. On dynamic shortest paths pr oblems. Algorithmica , 61(2):389–401,\n2011.\n[41] Shinsaku Sakaue and Taihei Oki. Discrete-convex-anal ysis-based framework for warm-starting\nalgorithms with predictions. In 35th Conference on Neural Information Processing Systems\n(NeurIPS) , 2022.\n[42] Yossi Shiloach and Shimon Even. An on-line edge-deleti on problem. Journal of the ACM (JACM) ,\n28(1):1–4, 1981.\n[43] Vaidehi Srinivas and Avrim Blum. Competitive strategi es to use “warm start” algorithms with\npredictions. In Proceedings of the 2025 ACM-SIAM Symposium on Discrete Algo rithms, SODA\n2025 . SIAM, 2025.\n20\n\n[44] Kapil Vaidya, Eric Knorr, Michael Mitzenmacher, and Ti m Kraska. Partitioned learned bloom\nﬁlters. In Proc. 9th Annual International Conference on Learning Repr esentations (ICLR) , 2021.\n[45] Jan van den Brand, Sebastian Forster, Yasamin Nazari, a nd Adam Polak. On dynamic graph\nalgorithms with predictions. In David P. Woodruff, editor, Proceedings of the 2024 ACM-SIAM\nSymposium on Discrete Algorithms, SODA 2024, Alexandria, V A, USA, January 7-10, 2024 , pages\n3534–3557. SIAM, 2024.\n[46] Ali Zeynali, Shahin Kamali, and Mohammad Hajiesmaili. Robust learning-augmented dictionaries.\nInForty-ﬁrst International Conference on Machine Learning , 2024.\n[47] Ali Zeynali, Shahin Kamali, and Mohammad Hajiesmaili. Robust learning-augmented dictionaries.\nInForty-ﬁrst International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\n21-27, 2024 . OpenReview.net, 2024.\n21",
  "textLength": 76202
}