{
  "paperId": "a0d8024548d522da1a9dc6fda741ebec14ba2815",
  "title": "The Third International Verification of Neural Networks Competition (VNN-COMP 2022): Summary and Results",
  "pdfPath": "a0d8024548d522da1a9dc6fda741ebec14ba2815.pdf",
  "text": "The Third International Veri\fcation of Neural Networks\nCompetition (VNN-COMP 2022): Summary and Results\nMark Niklas M uller\u00031, Christopher Brix\u00032, Stanley Bak3, Changliu Liu4, and\nTaylor T. Johnson5\n1ETH Zurich, Zurich, Switzerland\nmark.mueller@inf.ethz.ch\n2RWTH Aachen University, Aachen, Germany\nbrix@cs.rwth-aachen.de\n3Stony Brook University, Stony Brook, New York, USA\nstanley.bak@stonybrook.edu\n4Carnegie Mellon University, Pittsburgh, Pennsylvania, USA\ncliu6@andrew.cmu.edu\n5Vanderbilt University, Nashville, Tennessee, USA\ntaylor.johnson@vanderbilt.edu\nAbstract\nThis report summarizes the 3rd International Veri\fcation of Neural Networks Compe-\ntition (VNN-COMP 2022), held as a part of the 5th Workshop on Formal Methods for\nML-Enabled Autonomous Systems (FoMLAS), which was collocated with the 34th Inter-\nnational Conference on Computer-Aided Veri\fcation (CAV). VNN-COMP is held annually\nto facilitate the fair and objective comparison of state-of-the-art neural network veri\fca-\ntion tools, encourage the standardization of tool interfaces, and bring together the neural\nnetwork veri\fcation community. To this end, standardized formats for networks (ONNX)\nand speci\fcation (VNN-LIB) were de\fned, tools were evaluated on equal-cost hardware\n(using an automatic evaluation pipeline based on AWS instances), and tool parameters\nwere chosen by the participants before the \fnal test sets were made public. In the 2022\niteration, 11 teams participated on a diverse set of 12 scored benchmarks. This report\nsummarizes the rules, benchmarks, participating tools, results, and lessons learned from\nthis iteration of this competition.\n1 Introduction\nDeep Learning based systems are increasingly being deployed in a wide range of domains,\nincluding recommendation systems, computer vision, and autonomous driving. While the nom-\ninal performance of these methods has increased signi\fcantly over the last years, often even\nsurpassing human performance, they largely lack formal guarantees on their behavior. How-\never, in safety-critical applications, including autonomous systems, robotics, cybersecurity, and\ncyber-physical systems, such guarantees are essential for certi\fcation and con\fdent deployment.\nWhile the literature on the veri\fcation of traditionally designed systems is wide and success-\nful, neural network veri\fcation remains an open problem, despite signi\fcant e\u000borts over the last\nyears. In 2020, the International Veri\fcation of Neural Networks Competition (VNN-COMP)\nwas established to facilitate comparison between existing approaches, bring researchers working\non this problem together, and help shape future directions of the \feld. In 2022, the 3rd and\nmost recent iteration of the annual VNN-COMP1was held as a part of the 5th Workshop on\n\u0003Equal contribution\n1https://sites.google.com/view/vnn2022/homearXiv:2212.10376v2  [cs.LG]  16 Feb 2023\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nFormal Methods for ML-Enabled Autonomous Systems (FoMLAS) that was collocated with\nthe 34th International Conference on Computer-Aided Veri\fcation (CAV).\nThis third iteration of the VNN-COMP continues last year's trend of increasing standard-\nization and automatization, aiming to enable a fair comparison between the participating tool\nand simplify the evaluation of a large number of tools on a variety of (real-world) problems. The\nVNN-COMP 2022 standardizes 1) neural network and speci\fcation formats, ONNX for neural\nnetworks and VNN-LIB for speci\fcations, 2) evaluation hardware, providing participants the\nchoice of a range of cost-equivalent AWS instances with di\u000berent trade-o\u000bs between CPU and\nGPU performance, and 3) evaluation pipelines, enforcing a uniform interface for the installation\nand evaluation of tools.\nThe competition was kicked o\u000b with the solicitation for participation in February 2022.\nBy March, several teams had registered, allowing the rule discussion to be \fnalized in April\n2022 (see an overview in Section 2). From April to June 2022, benchmarks were proposed\nand discussed. Meanwhile, the organizing team decided to continue using AWS as the eval-\nuation platform and started to implement an automated submission and testing system for\nboth benchmarks and tools. By mid-July 2022, eleven teams submitted their tools and the\norganizers evaluated all entrants to obtain the \fnal results, discussed in Section 5 and pre-\nsented at FoMLAS on July 31st, 2022. Discussions were structured into three issues on the\no\u000ecial GitHub repository2: rules discussion, benchmarks discussion, and tool submission. All\nsubmitted benchmarks3and \fnal results4were aggregated in separate GitHub repositories.\nThe remainder of this report is organized as follows: Section 2 discusses the competition\nrules, Section 3 lists all participating tools, Section 4 lists all benchmarks, Section 5 summarizes\nthe results, and Section 6 concludes the report, discussing potential future improvements.\n2https://github.com/stanleybak/vnncomp2022/issues\n3https://github.com/ChristopherBrix/vnncomp2022_benchmarks\n4https://github.com/ChristopherBrix/vnncomp2022_results\n2\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n2 Rules\nTerminology Aninstance is de\fned by a property speci\fcation (pre- and post-condition), a\nnetwork, and a timeout). For example, one instance might consist of an MNIST classi\fer with\none input image, a given local robustness threshold \u000f, and a speci\fc timeout. A benchmark is\nde\fned as a set of related instances. For example, one benchmark might consist of a speci\fc\nMNIST classi\fer with 100 input images, potentially di\u000berent robustness thresholds \u000f, and one\ntimeout per input.\nRun-time caps Run-times are capped on a per-instance basis, i.e., any veri\fcation instance\nwill timeout (and be terminated) after at most X seconds, determined by the benchmark pro-\nposer. These can be di\u000berent for each instance. The total per-benchmark runtime (sum of\nall per-instance timeouts) may not exceed 6 hours per benchmark. For example, a benchmark\nproposal could have six instances with a one-hour timeout, or 100 instances with a 3.6-minute\ntimeout, each. To enable a fair comparison, we measure the startup overhead for each tool\nby running it on a range of tiny networks and subtract the minimal overhead from the total\nruntime.\nHardware To allow for comparability of results, all tools were evaluated on equal-cost hard-\nware using Amazon Web Services (AWS). Each team could decide between a range of AWS\ninstance types (see Table 1) providing a CPU, GPU, or mixed focus. Except for the much\nweaker t2.large instance, all instances were priced at around three dollars per hour.\nTable 1: Available AWS instances.\nvCPUs RAM [GB] GPU\nr5.12xlarge 48 384 7\np3.2xlarge 8 61 V100 GPU with 16 GB memory\nm5.16xlarge 64 256 7\ng5.8xlarge 32 128 A10G GPU with 24 GB memory\nt2.large 2 8 7\nScoring The \fnal score is aggregate as the sum of all benchmark scores. Each benchmark\nscore is the number of points (sum of instance scores discussed below) achieved by a given tool,\nnormalized by the maximum number of points achieved by any tool on that benchmark. Thus,\nthe tool with the highest sum of instance scores for a benchmark will get a benchmark score of\n100, ensuring that all benchmarks are weighted equally, regardless of the number of constituting\ninstances.\nInstance score Each instance is scored is as follows:\n•Correct hold (property proven): 10 points;\n•Correct violated (counterexample found): 10 points;\n•Incorrect result: -100 points.\nHowever, the ground truth for any given instance is generally not known a priori. In the case\nof disagreement between tools, we, therefore, place the burden of proof on the tool claiming\nthat a speci\fcation is violated, i.e. that a counter-example can be found, and deem it correct\nexactly if it produces a valid counter-example.\n3\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTime bonus Time bonuses are computed as follows.\n•The fastest tool for each solved instance will receive +2 points.\n•The second fastest tool will receive +1 point.\nAll runtimes below 1.0 seconds after overhead correction (explained below) are considered\nto be 1.0 seconds exactly for scoring purposes. If two tools have runtimes within 0.2 seconds\n(after all corrections), for scoring purposes we will consider them the same runtime. Thus,\nmultiple tools can receive the bonus for the fastest runtime.\nOverhead Correction According to the rules discussion, we decided to subtract tool overhead\ntime from the measured runtimes. For example, simply importing TensorFlow from Python\nand acquiring a GPU can sometimes take about 5 seconds. For particularly easy benchmarks\nwhere veri\fcation times can be signi\fcantly below one second, this would dominate the runtime.\nHowever, in a real-world application, typically, a large number of instances are analyzed without\nterminating the tool in between, leading to this overhead only being incurred once. Thus, we\nbelieve these corrected times to be more representative of real-world runtime behavior.\nTo measure the tool-speci\fc overhead, we created trivial network instances and included\nthose in the measurements. We then observed the minimum veri\fcation time over all instances\nand considered that to be the overhead time for the tool.\nFormat This year we standardized neural networks to be in onnx format, speci\fcations in\nvnnlib format, and counter-examples in a new format similar to the vnnlib format. Further,\ntool authors were required to provide scripts fully automating the installation process of their\ntool, including the acquisition of any licenses that might be needed. Similar to the previous year,\na preparation and execution script had to be provided for running their tool on a speci\fc instance\nconsisting of a network \fle, speci\fcation \fle, and timeout. The speci\fcations are interpreted as\nde\fnitions of counter-examples, meaning that a property is proven \\correct\" if the speci\fcation\nis shown to be unsatis\fable, conversely, the property is shown to be violated if a counterexample\nful\flling the speci\fcation is found. Speci\fcations consisted of disjunctions over conjunctions in\nboth pre- and post-conditions, allowing a wide range of properties from adversarial robustness\nover multiple hyper-boxes to safety constraints to be encoded. For example, robustness with\nrespect to inputs in a hyper-box had to be encoded as disjunctive property, where any of the\nother classes is predicted.\n4\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n3 Participants\nWe list the tools and teams that participated in the VNN-COMP 2022 in Table 2 and reproduce\ntheir own descriptions of their tools below.\nTable 2: Summary of the key features of participating tools. The hardware column describes\nthe used AWS instance with p3and g5making GPUs available, see Table 1 for more details.\nLicenses refer to the external licenses required to use the corresponding tool, not the licensing\nof the tool itself.\nTool References Organizations Place Hardware Licenses\n\u000b,\f-CROWN [56, 57, 50, 59]CMU, Drexel, UCLA, UIUC,\nColumbia1 g5 GUROBI, IBM CPLEX\nAveriNN [35] Kansas State University 11 t2 -\nCGDTest - University of Waterloo 5 m5 -\nDebona [4] RWTH Aachen University 8 m5 GUROBI\nFastBATLLNN [12] University of California 9 t2 -\nMarabou [23]Hebrew University of Jerusalem,\nStanford University, NRI Secure7 m5 GUROBI\nMN-BaB [13, 40, 33, 41] ETH Zurich 2 p3 GUROBI\nnnenum [2] Stony Brook University 4 m5 -\nPeregriNN [25] University of California 6 m5 GUROBI\nVeraPak [43] Utah State University 10 t2 -\nVeriNet [18, 19, 17] Imperial College London 3 g5 Xpress\n3.1\u000b;\f-CROWN\nTeam\u000b;\f-CROWN is developed by a multi-institutional team from CMU, UCLA, Drexel\nUniversity, UIUC, and Columbia University:\n•Developers: Huan Zhang (CMU), Kaidi Xu (Drexel), Zhouxing Shi (UCLA), Jinqi Chen\n(CMU), Linyi Li (UIUC), Shiqi Wang (Columbia), Zhuolin Yang (UIUC), Yihan Wang\n(UCLA)\n•Advisors: Zico Kolter (CMU), Cho-Jui Hsieh (UCLA), Bo Li (UIUC), Suman Jana\n(Columbia)\nDescription \u000b;\f-CROWN ( alpha-beta-CROWN ) is an e\u000ecient neural network veri\fer based\non the linear bound propagation framework, built on a series of works on bound-propagation-\nbased neural network veri\fers: CROWN [60, 56], \u000b-CROWN [57], \f-CROWN [50] and GCP-\nCROWN [59]. The original version of CROWN was also concurrently proposed as DeepPoly [41]\n(mathematically equivalent). Our latest work of linear bound propagation with general cut-\nting planes (GCP-CROWN [59]) is currently the most general formulation of the linear bound\npropagation framework for neural network veri\fcation.\nWe use the generalized version of bound propagation algorithms in the auto LiRPA li-\nbrary [56] which supports general neural network architectures (including convolutional layers,\npooling layers, residual connections, recurrent neural networks, and Transformers) and a wide\nrange of activation functions (e.g., ReLU, tanh, sigmoid, max pooling and average pooling),\n5\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nand is e\u000eciently implemented on GPUs with Pytorch and CUDA. We jointly optimize inter-\nmediate layer bounds and \fnal layer bounds using gradient ascent (referred to as \u000b-CROWN\nor optimized CROWN/LiRPA [57]). Most importantly, we use branch and bound [7] (BaB)\nand incorporate split constraints in BaB into the bound propagation procedure e\u000eciently via\nthe\f-CROWN algorithm [50], and use cutting-plane method in GCP-CROWN [59] to further\ntighten the bound. For smaller networks, we also use a mixed integer programming (MIP)\nformulation [44] combined with tight intermediate layer bounds from \u000b-CROWN (referred to\nas\u000b-CROWN + MIP [59]). The combination of e\u000ecient, optimizable and GPU-accelerated\nbound propagation with BaB produces a powerful and scalable neural network veri\fer.\nLink https://github.com/Verified-Intelligence/alpha-beta-CROWN (latest version)\nCompetition submission https://github.com/huanzhang12/alpha-beta-CROWN_vnncomp22\n(only for reproducing competition results; please use the latest version for other purposes)\nHardware and licenses CPU and GPU with 32-bit or 64-bit \roating point; Gurobi license\nrequired for the mnistfc benchmark; IBM CPLEX required for the oval21 benchmark.\nParticipated benchmarks All benchmarks.\n3.2 AVeriNN\nTeam Vishnu Bondalakunta (Kansas State University), Pavithra Prabhakar (Kansas State\nUniversity)\nDescription The Abstraction Veri\fcation of Neural Networks Tool (AVeriNN) is written in\nPython 3 and performs reachability analysis of abstract systems called Interval Neural Networks\n[35]. Potentially large-scale neural networks are converted to smaller Interval Neural Networks\nby clustering similar neurons together. The degree of reduction of the neural network is user-\ncontrolled via the 'delta' parameter. AVeriNN uses geometric representations that allow for a\nlayer-by-layer computation of the reachable set of Interval Neural Networks, similar to NNV\n[47]. However, the di\u000berence is that instead of using star sets, an extended version of star\nsets named Interval star sets are used. Furthermore, the obtained reach set is always an over-\napproximation of the exact reachable set of the original neural network.\nThe current implementation is a baseline proof-of-concept devoid of optimizations present\nin other star set based tools such as NNV and nnenum.\nLink https://github.com/vishnuteja97/AVeriNN.git\nCommit 38cb39df826b76bb0c396faf0e13a24c02e4afad\nHardware and licences CPU, No licenses required\nParticipated benchmarks rlbenchmarks\n3.3 CGDTest\nTeam Vineel Nagisetty (Borealis AI), Guanting Pan (University of Waterloo), Piyush Jha\n(University of Waterloo), Christopher Srinivasa (Borealis AI), Vijay Ganesh (University of\nWaterloo)\nDescription CGDTest is a DNN testing algorithm, which takes a DNN model, constraints,\nand label as inputs; it aims to \fnd an input such that the constraint needs to be satis\fed\nby the model. CGDTest is best viewed as a gradient-descent optimization method: Firstly,\nCGDTest can parse user-speci\fed constraints and convert them into a di\u000berentiable constraint\nloss function. When an input is constructed to satisfy all the constraints during iterations, the\nconstraint loss function is 0 for this input. Secondly, starting from a random input, CGDTest\n6\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nuses gradient descent to modify it to meet the stopping criteria. The stopping criteria are met\nwhen the constraint loss function results in 0.\nLink https://github.com/vin-nag/CGD.git\nCommit 57baf10076968f901fcab4751c530728c94e86a4\nHardware and licenses CPU and GPU; No licenses required\nParticipated benchmarks All benchmarks\n3.4 Debona\nTeam Christopher Brix (RWTH Aachen University), Thomas Noll (RWTH Aachen University)\nDescription Debona [4] is a veri\fcation toolkit built upon the 2020 version of VeriNet [18].\nIn addition to the so-called Error-based Symbolic Interval Propagation (ESIP), which relies on\nparallel upper and lower bounds for each neuron, it also supports Reversed Symbolic Interval\nPropagation (RSIP) with independent lower and upper bounds [41, 60]. As RSIP is computa-\ntionally more expensive, Debona \frst tries to verify the given property using ESIP, and falls\nback to RSIP only if this \frst attempt fails.\nLink https://github.com/ChristopherBrix/Debona\nCommit 675d5c450960a7c341ea1f42f9d11f918f9e1417\nHardware and licences CPU, Gurobi license\nParticipated benchmarks mnist fc,nn4sys ,reach prob density ,rlbenchmarks ,\ntllverifybench\n3.5 FastBATLLNN\nTeam James Ferlez (Developer), Haitham Khedr (Tester), and Yasser Shoukry (Supervisor)\n(University of California, Irvine)\nDescription FastBATLLNN [12] is a fast veri\fer of box-like (hyper-rectangle) output proper-\nties for Two-Level Lattice (TLL) Neural Networks (NN). FastBATLLNN uses both the unique\nsemantics of the TLL architecture and the decoupled nature of box-like output constraints to\nprovide a fast, polynomial-time veri\fcation algorithm: that is, polynomial-time in the num-\nber of neurons in the TLL NN to be veri\fed (for a \fxed input dimension). FastBATLLNN\nfundamentally works by converting the TLL veri\fcation problem into a region enumeration\nproblem for a hyperplane arrangement (the arrangement is jointly derived from the TLL NN\nand the veri\fcation property). However, as FastBATLLNN leverages the unique properties of\nTLL NNs and box-like output constraints, its use is necessarily limited to veri\fcation problems\nformulated in those terms. Hence, FastBATLLNN can only compete on the tllverifybench\nbenchmark.\nLink https://github.com/jferlez/FastBATLLNN-VNNCOMP\nCommit 6100258b50a3fadf9792aec4ccf39ed14778e338\nHardware and licenses CPU; no licenses required\nParticipated benchmarks tllverifybench\n3.6 Marabou\nTeam Haoze Wu (Stanford University), Aleksandar Zeljic (Stanford University), Teruhiro\nTagomori (Stanford University/NRI Secure), Clark Barrett (Stanford University), Guy Katz\n(Hebrew University of Jerusalem)\n7\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nDescription Marabou [24] is a user-friendly Neural Network Veri\fcation toolkit that can an-\nswer queries about a network's properties by encoding and solving these queries as constraint\nsatisfaction problems. It has both Python/C++ APIs through which users can load neural\nnetworks and de\fne arbitrary linear properties over the neural network. Marabou supports\nmany di\u000berent linear, piecewise-linear, and non-linear [54] operations and architectures (e.g.,\nFFNNs, CNNs, residual connections, Graph Neural Networks [52]).\nUnder the hood, Marabou employs a uniform solving strategy for a given veri\fcation query.\nIn particular, Marabou performs complete analysis that employs a specialized convex optimiza-\ntion procedure [55] and abstract interpretation [41, 52]. It also uses the Split-and-Conquer\nalgorithm [53] for parallelization.\nLink https://github.com/NeuralNetworkVerification/Marabou\nCommit 48a6f68267ee9e96dae7751f902139eac868\u000bf3\nHardware and Licenses CPU, no license required. Can also be accelerated with Gurobi\n(which requires a license)\nParticipated benchmarks cifar100 tinyimagenet resnet ,reach prob density ,mnist fc,\noval21 ,rlbenchmarks\n3.7 MN-BaB\nTeam Mark Niklas M uller (ETH Zurich), Robin Staab (ETH Zurich), Timon Gehr (ETH\nZurich), Claudio Ferrari (ETH Zurich), Martin Vechev (ETH Zurich)\nDescription MN-BaB [13] is an open-source neural network veri\fer leveraging precise multi-\nneuron constraints [40, 33] combined with e\u000ecient GPU-enabled linear bound-propagation [41]\nin a branch and bound framework. MN-BaB is complete for piece-wise linear activation func-\ntions and can handle fully-connected, convolutional, and residual network architectures contain-\ning ReLU, Sigmoid, Tanh, and Maxpool non-linearities. It uses either 64- or 32-bit precision\ndepending on the benchmark. Multi-neuron relaxations of non-linear activations [33] and po-\ntential MILP-based re\fnement of individual neuron-bounds[42] are computed on CPU, while\nthe core bound propagation is fully run on GPU. Depending on the benchmark, MN-BaB can\nuse di\u000berent veri\fcation modes besides the standard linear bound propagation based BaB in-\ncluding input-domain splitting with forward bound propagation and full MILP encodings [45].\nThese modes were selected manually per benchmark/network type. MN-BaB is implemented\nusing PyTorch [34], and uses Gurobi [14] for solving MILP instances.\nLink https://github.com/eth-sri/mn-bab\nCommit Please use the up-to-date main repository for anything but reproducing VNN-COMP\nresults. https://github.com/mnmueller/mn_bab_vnn_2022\n(c67bcd19e9e2c\u000b13f8c2785c94034b89b47ee05)\nHardware and licences CPU and GPU, GUROBI License\nParticipated benchmarks All benchmarks.\n3.8 nnenum\nTeam Stanley Bak (Stony Brook University)\nDescription The nnenum tool [2] uses multiple levels of abstraction to achieve high-\nperformance veri\fcation of ReLU networks without sacri\fcing completeness [1]. Analysis com-\nbines three types of zonotopes with star set (triangle) overapproximations [47], and uses e\u000ecient\n8\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nparallelized ReLU case splitting [3]. The ImageStar method [46] allows sets to be quickly prop-\nagated through all layers supported by the ONNX runtime, such as convolutional layers with\narbitrary parameters. The tool is written in Python 3 and uses GLPK for LP solving. New this\nyear we used code from DNNV [38] in order to convert maxpooling layers into multiple ReLU\nlayers, which permitted analysis of more networks including VGGNET.\nLink https://github.com/stanleybak/nnenum\nCommit c93a39cb568f58a26015bd151acafab34d2d4929\nHardware and licences CPU, No licenses required\nParticipated benchmarks AcasXu ,cifar2020 ,mnist fc,oval21 .\n3.9 PeregriNN\nTeam Haitham Khedr, James Ferlez, and Yasser Shoukry (University of California, Irvine)\nDescription PeregriNN[25] is a sound and complete Neural Network veri\fcation tool written\nin Python 3 focusing on ReLU activations. It uses search and optimization to verify the NN\nproperty. PeregriNN uses symbolic interval analysis for bound approximation and a linear\nprogram to check for violations of the speci\fcation. The solution of the linear program is used\nas a heuristic to guide the neuron conditioning (\fxing ReLUs) during abstraction re\fnement.\nGurobi is used for LP solving and the framework this year adds support for ONNX networks\nand vnnlib speci\fcations.\nLink https://github.com/haithamkhedr/PeregriNN/tree/vnn2022\nCommit 4d315135dd34da5fc25b258fc4d40e1cf5a14ed9\nHardware and licences CPU, Gurobi Academic license\nParticipated benchmarks collins rulcnn,mnist fc,nn4sys ,oval21 ,reach prob density ,\nrlbenchmarks ,tllverifybench .\n3.10 V ERAPA K\nTeam Bennett DenBleyker1, Joshua Smith2, Viswanathan Swaminathan3, Zhen Zhang1.\n1Utah State University,2Campbell Scienti\fc Inc.,3Adobe Systems Inc.\nDescription Veri\fcation for Robust neural networks through Abstraction, Partitioning and\nAttackmethods (VeRAPAk ) is a neural network veri\fcation framework that, by combining\nvarious input region abstraction, veri\fcation, and partitioning strategies, can potentially serve\nany desired use-case [43]. In abstraction, VeRAPAk \fnds representative points for a given\nregion and tests them to ensure safety. In veri\fcation, VeRAPAk checks the region as a whole\nfor safety. If veri\fcation fails or its result is unknown, the region is partitioned into multiple\nsubregions for further abstraction and veri\fcation. VeRAPAk is built to handle a myriad of\ndi\u000berent use-cases, where veri\fcation strategies may range from checking every discrete point\nfor safety or assuming safety after a certain threshold, to integrating another DNN veri\fcation\ntool entirely, or even focusing purely on generating counterexamples instead.\nThe version of VeRAPAk used in VNNCOMP was the same that is typically used for\nmodel training: built to rapidly produce as many counterexamples as possible in a uniform\ndistribution over the input region. These are then typically used for retraining to reinforce\nrobustness against counterexamples. The only changes for the competition were to use discrete\nsearch for veri\fcation and halt after the \frst iteration where it \fnds a counterexample. Even\nso, it tends to \fnd around 20 to 50 counterexamples before halting.\n9\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nFor the purposes of VNNCOMP, VeRAPAk used per-benchmark tuning to set granular-\nity. For the networks we deemed to be image-based classi\fcation networks, we set1\n256as our\ngranularity. For all others, we used1\n1000x, wherexis the range of each dimension as a vector.\nTheoretically, any property that can be set via con\fg \fle or command line \rags may also be\nset per-benchmark. VeRAPAk is also not \roating point sound.\nLink https://github.com/formal-verification-research/VERAPAK\nCommit 9181ef5a40672c0d82e5405037c9d8911587ab0d\nHardware and licenses CPU, No licenses required\nParticipated benchmarks cifar biasfield ,mnist fc,rlbenchmarks ,\n3.11 VeriNet\nTeam Patrick Henriksen, Alessio Lomuscio (Imperial College London).\nDescription VeriNet [18, 19, 17] is a complete Symbolic Interval Propagation (SIP) based\nveri\fcation toolkit for feed-forward neural networks. The underlying algorithm utilizes SIP\nto create a linear abstraction of the network, which, in turn, is used in an LP-encoding to\nanalyze the veri\fcation problem. A branch and bound-based re\fnement phase is used to achieve\ncompleteness.\nVeriNet implements various optimizations, including a gradient-based local search for coun-\nterexamples, optimal relaxations for Sigmoids, adaptive node splitting [18], succinct LP-\nencodings, and a novel splitting heuristic that takes into account indirect e\u000bects splits have\non succeeding relaxations [19].\nVeriNet supports a wide range of layers and activation functions, including Relu, Sigmoid,\nTanh, fully connected, convolutional, max and average pooling, batch normalization, reshape,\ncrop, and transpose operations, as well as additive residual connections.\nNote that VeriNet subsumes the Deepsplit method presented in [19].\nHardware and licences CPU and GPU, Xpress Solver license required for large networks.\nLink https://github.com/vas-group-imperial/VeriNet\n(66b70f6fef311c721a772c78573bbfae1644bc5e).\nNote: This is an old version of VeriNet; the version used for the competition is not public .\nParticipated benchmarks All benchmarks.\n10\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n4 Benchmarks\nIn this section we provide an overview of all benchmarks, reproducing the benchmark proposers'\ndescriptions.\nTable 3: Overview of all scored benchmarks.\nCategory Benchmark Application Network Types # Neurons E\u000bective Input Dim\nComplexCarvana UNet Image Segmentation Complex UNet 275k - 373k 4.3k\nNN4SysDataset Indexing\n& Cardinality PredictionComplex (ReLU + Sigmoid) 384 - 94k 1-4\nCNN\n& ResNetCifar Bias Field Image Classi\fcation Conv. + ReLU 45k 16\nCollins RUL CNN Condition Based Maintenance Conv. + ReLU 5.5k - 28k 2 - 200\nLarge ResNets Image Classi\fcation ResNet (Conv. + ReLU) 55k - 286k 3.1k - 12k\nOval21 Image Classi\fcation Conv. + ReLU 3.1k - 6.2k 3.1k\nSRI ResNet A/B Image Classi\fcation ResNet (Conv. + ReLU) 11k 3.1k\nVGGNet16 Image Classi\fcation Conv. + ReLU + MaxPool 13.6M 1 - 95k\nFCMNIST FC Image Classi\fcation FC. + ReLU 512 - 1.5k 784\nReach Prob Density Probability density estimation FC. + ReLU 64 - 192 3 - 14\nRL Benchmarks Reinforcement Learning FC. + ReLU 128 - 512 4 - 8\nTLL Verify Bench Two-Level Lattice NNTwo-Level Lattice NN\n(FC. + ReLU)252 - 16k 2\n4.1 Carvana UNet\nProposed by Yonggang Luo (Chongqing Changan Automobile Co. Ltd.), Jinyan Ma\n(Chongqing Changan Automobile Co. Ltd.).\nMotivations Currently, most Neural Network Veri\fcation benchmarks are focusing on image\nclassi\fcation tasks. However, in many practical scenarios, e.g., autonomous driving, people\nmay pay more attention to object detection or semantic segmentation [37] tasks. To bring some\nincentives for developing tools for more general purposes into the community, we proposed\nCarvana UNet for semantic segmentation tasks. We advocated that tools should handle more\npractical architectures and Carvana UNet is the \frst step towards this goal.\nNetworks The proposed Carvana UNet include two simpli\fed UNet models. Model one is\ncomposed of four Conv2d layers which are followed by a BN and ReLu layer; for model two,\nwe add one AveragePool layer and one TransposedConv Upsampling layer [48] to model one.\nThe input size is [1, 4, 31, 47], where 1 is the batchsize, 4 is the number of channels, and 31\nand 47 are the height and the width of samples respectively. The \frst three channels represent\nRGB values of images. The last channel represents the model-produced mask, which is used\nfor calculating the number of correctly predicted pixels by the model. The model has one\noutput, which is the number of correctly predicted pixels by the model, compared with the\nmodel-produced mask.\nSpeci\fcations The Carvana dataset consists of 5088 images covering 318 cars, which means\neach car has 16 images. We choose one image for each car, 318 images in total, as a test set.\nAnd the remaining 4700 images are used for training. There are 52 images whose 98.8 percent\npixels can be predicted correctly for model one, and 44 images whose 99 percent pixels can\nbe predicted correctly for model two. We randomly select 16 images from these images for\nveri\fcation. For each image, we specify the property that the number of correctly predicted\n11\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\npixels by the model is always greater than 1314 (90 percent of the total pixels) within `1norm\ninput perturbation of \u000f=1\n255or\u000f=3\n255. The per-example timeout is set to 300 seconds.\nLink https://github.com/pomodoromjy/vnn-comp-2022-Carvana-unet\n4.2 NN4Sys\nProposed by the\u000b;\f-CROWN team with collaborations with Cheng Tan and Haoyu He at\nNortheastern University.\nApplication The benchmark contains networks for database learned index and learned cardi-\nnality estimation, which maps input from various dimensions to a single scalar as output.\n•Background : learned index and learned cardinality are all instances in neural networks for\ncomputer systems (NN4Sys), which are neural network based methods performing system\noperations. These classes of methods show great potential but have one drawback|the\noutputs of an NN4Sys model (a neural network) can be arbitrary, which may lead to\nunexpected issues in systems.\n•What to verify : our benchmark provides multiple pairs of (1) trained NN4Sys model and\n(2) corresponding speci\fcations. We design these pairs with di\u000berent parameters such that\nthey cover a variety of user needs and have varied di\u000eculties for veri\fers. We describe\nbenchmark details in our NN4SysBench report [15]: http://naizhengtan.github.io/\ndoc/papers/characterizing22haoyu.pdf .\n•Translating NN4Sys applications to a VNN benchmark : the original NN4Sys applications\nhave sophisticated features that are hard to express. We tailored the neural networks\nand their speci\fcations to be suitable for VNN-COMP. For example, learned index [26]\ncontains multiple NNs in a tree structure that together serve one purpose. However,\nthis cascading structure is inconvenient/unsupported to verify because there is a \\switch\"\noperation|choosing one NN in the second stage based on the prediction of the \frst stage's\nNN. To convert learned indexes to a standard form, we merge the NNs into one larger\nNN.\n•A note on broader impact : using NNs for systems is a broad topic, but many existing\nworks lack strict safety guarantees. We believe that NN Veri\fcation can help system\ndevelopers gain con\fdence to apply NNs to critical systems. We hope our benchmark can\nbe an early step toward this vision.\nNetworks This benchmark has six networks with di\u000berent parameters: two for learned indexes\nand four for learned cardinality estimation. The learned index uses fully-connected feed-forward\nneural networks. The cardinality estimation has a relatively sophisticated internal structure;\nplease see our NN4SysBench report (URL listed above) for details.\nSpeci\fcations For learned indexes, the speci\fcation aims to check if the prediction error is\nbounded. The speci\fcation is a collection of pairs of input and output intervals such that\nany input in the input interval should be mapped to the corresponding output interval. For\nlearned cardinality estimation, the speci\fcations check the prediction error bounds (similar to\nthe learned indexes) and monotonicity of the networks. By monotonicity speci\fcations, we\nmean that for two inputs, the network should produce a larger output for the larger input,\nwhich is required by cardinality estimation.\nLink: https://github.com/Cli212/VNNComp22_NN4Sys\n12\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n4.3 Cifar Bias Field\nProposed by The VeriNet team.\nMotivation This benchmark considers veri\fcation of a Cifar 10 network against bias \feld\nperturbations. The bias \feld perturbations are encoded by creating augmented networks with\nonly 16 input parameters; thus, the problem has a signi\fcantly lower input dimensionality than\nmany other image-based benchmarks.\nNetworks For each image to be veri\fed, a separate bias \feld transform network is created [17]\nwhich consists of the FC transform layer followed by the Cifar CNN. The Cifar CNN consists of 8\nconvolutional layers followed by ReLUs. Each bias \feld transform network has 363k parameters\nand 45k nodes.\nSpeci\fcations The speci\fcation considers bias \feld perturbations of the input. The task is\nreduced to a standard `1speci\fcation by encoding bias \feld the bias \feld transformation into\nfully connected layers which are prepended to the network under consideration. The bias \feld\nperturbations used \u000f= 0:06 (as described in [17]) and a timeout of 5 minutes.\n4.4 Collins-RUL-CNN\nProposed by Collins Aerospace, Applied Research and Technology.\nMotivation Machine Learning (ML) is a disruptive technology for the aviation industry. This\nparticularly concerns safety-critical aircraft functions, where high-assurance design and veri\f-\ncation methods have to be used in order to obtain approval from certi\fcation authorities for\nthe new ML-based products. Assessment of correctness and robustness of trained models, such\nas neural networks, is a crucial step for demonstrating the absence of unintended functionali-\nties. The key motivation for providing this benchmark is to strengthen the interaction between\nthe VNN community and the aerospace industry by providing a realistic use case for neural\nnetworks in future avionics systems.\nApplication Remaining Useful Life (RUL) is a widely used metric in Prognostics and Health\nManagement (PHM) that manifests the remaining lifetime of a component (e.g., mechani-\ncal bearing, hydraulic pump, aircraft engine). RUL is used for Condition-Based Maintenance\n(CBM) to support aircraft maintenance and \right preparation. It contributes to such tasks as\naugmented manual inspection of components and scheduling of maintenance cycles for compo-\nnents, such as repair or replacement, thus moving from preventive maintenance to predictive\nmaintenance (do maintenance only when needed, based on component's current condition and\nestimated future condition). This could allow to eliminate or extend service operations and\ninspection periods, optimize component servicing (e.g., lubricant replacement), generate in-\nspection and maintenance schedules, and obtain signi\fcant cost savings. Finally, RUL function\ncan also be used in airborne (in-\right) applications to dynamically inform pilots on the health\nstate of aircraft components during \right. Multivariate time series data is often used as RUL\nfunction input, for example, measurements from a set of sensors monitoring the component\nstate, taken at several subsequent time steps (within a time window). Additional inputs may\ninclude information about the current \right phase, mission, and environment. Such highly\nmulti-dimensional input space motivates the use of Deep Learning (DL) solutions with their\ncapabilities of performing automatic feature extraction from raw data.\nNetworks The benchmark includes 3 convolutional neural networks (CNNs) of di\u000berent com-\nplexity: di\u000berent numbers of \flters and di\u000berent sizes of the input space. All networks contain\nonly convolutional and fully connected layers with ReLU activations. All CNNs perform the re-\n13\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\ngression function. They have been trained on the same dataset (time series data for mechanical\ncomponent degradation during \right).\nSpeci\fcations We propose 3 properties for the NN-based RUL estimation function. First,\ntwo properties (robustness and monotonicity) are local, i.e., de\fned around a given point. We\nprovide a script with an adjustable random seed that can generate these properties around input\npoints randomly picked from a test dataset. For robustness properties, the input perturbation\n(delta) is varied between 5% and 40%, while the number of perturbed inputs varies between\n2 and 16. For monotonicity properties, monotonic shifts between 5% and 20% from a given\npoint are considered. Properties of the last type (\"if-then\") require the output (RUL) to be\nin an expected value range given certain input ranges. Several if-then properties of di\u000berent\ncomplexity are provided (depending on range widths).\nLink https://github.com/loonwerks/vnncomp2022\n4.5 Large ResNets\nProposed by the\u000b;\f-CROWN team.\nMotivations With the rapid development of large ML models, it would be interesting to test\nthe scalability of current veri\fcation methods. We aim to keep scaling up our model size upon\nlast year's residual networks [16] (ResNet) structure and introducing more challenging datasets:\nTinyImageNet and CIFAR-100 with higher input dimension and more classi\fcation labels than\nmost existing benchmarks. The purpose of this benchmark is to evaluate veri\fcation tools'\nscalability to large models.\nNetworks We provide one large ResNet model for TinyImageNet, and four ResNet models\non CIFAR-100 with di\u000berent model widths and depths. These models are trained with a\ncombination of CROWN-IBP [58] and adversarial training [28]. The smallest model is 11-layer,\nand the largest model has 21 layers. The largest model has 286,820 neurons. The models\ninclude:\nTinyImageNet (input dimension 64 \u000264\u00023, 200 classes)\n•TinyImageNet-ResNet-medium : 8 residual blocks, 17 convolutional layers + 2 linear layers\nCIFAR-100 models (input dimension 32 \u000232\u00023, 100 classes)\n•CIFAR100-ResNet-small : 4 residual blocks, 9 convolutional layers + 2 linear layers\n•CIFAR100-ResNet-medium : 8 residual blocks, 17 convolutional layers + 2 linear layers\n•CIFAR100-ResNet-large : 8 residual blocks, 19 convolutional layers + 2 linear layers\n(almost identical to standard ResNet-18 architecture)\n•CIFAR100-ResNet-super : 9 residual blocks, 19 convolutional layers + 2 linear layers\nThe networks are trained via \\ mixed training \" by combining adversarial training loss and\ncerti\fed defense training (CROWN-IBP) loss with weights 0.05:0.95 forCIFAR100-ResNet-small\nandCIFAR100-ResNet-medium , and 0.01:0.99 for other models under `1perturbation norm of\n\u000f=1\n255. Empirically, we noticed that pure CROWN-IBP training results in low model accuracy\nand the models could be easily veri\fed using IBP (not ideal for competition purposes), while\npure adversarial training leads to unveri\fable models. We found that a \\ mixed training \"\nby combining adversarial training and certi\fed defense training loss could lead to an excellent\n14\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nbalance between model clean accuracy and robustness, and is bene\fcial for obtaining higher\nveri\fed accuracy. The model trained with mixed training cannot be directly veri\fed by IBP,\nbut their veri\fed accuracy under a strong veri\fer such as \u000b;\f-CROWN is higher than purely\nusing CROWN-IBP training.\nTo gauge the veri\fcation hardness of these models, we evaluated all our trained models using\na 100-step projected gradient descent (PGD) attack with 20 random restarts, and a simple\nbound propagation based veri\fcation algorithm CROWN [60] (mathematically equivalent to\nthe abstract interpretation used in DeepPoly [41]) under `1norm perturbations. To ensure\nthe appropriate level of di\u000eculty, we use \u000f=1\n255for all our models. The results are listed in\nTable 4.\nModel # Parameters # ReLUs Clean acc.\u000f= 1=255\nPGD\nAttack acc.CROWN\nVeri\fed acc.\nCIFAR100-ResNet-small 5.4M 55 460 51.61% 37.92% 20.14%\nCIFAR100-ResNet-medium 10.1M 55 460 54.57% 40.42% 29.08%\nCIFAR100-ResNet-large 15.2M 286 820 53.24% 39.31% 29.89%\nCIFAR100-ResNet-super 31.6M 68 836 53.95% 38.83% 27.53%\nTinyImageNet-ResNet-medium 14.4M 172 296 35.04% 21.85% 13.51%\nTable 4: Clean accuracy, PGD accuracy, and CROWN veri\fed accuracy for ResNet models.\nNote that the veri\fed accuracy is obtained via the vanilla version of CROWN/DeepPoly, which\nhas been widely used as a simple baseline, not the \u000b;\f-CROWN tool used in the competition.\nSpeci\fcations We randomly select 10 images from the CIFAR-100 test set with a veri\fcation\ntimeout of 240 seconds for the CIFAR100-ResNet-small model, 16 images with a timeout of 300\nseconds for the CIFAR100-ResNet-super model and 24 images with a timeout of 200 seconds\nfor other CIFAR-100 models. For TinyImageNet, we select 24 images from TinyImageNet test\nset with a timeout of 200 seconds for the TinyImageNet-ResNet-medium model. The overall\nruntime is guaranteed to be less than 6 hours. These images are classi\fed correctly and\ncannot be attacked by a 100-steps PGD attack with 20 random restarts. We also \fltered out\nthe samples which can be veri\fed by vanilla CROWN (which is used during training) to make\nthe benchmark more challenging. The \fltering process is done o\u000fine on a machine with a GPU\ndue to the large sizes of these models.\nLink https://github.com/Lucas110550/CIFAR100_TinyImageNet_ResNet\n4.6 Oval21\nProposed by The OVAL team.\nMotivations The majority of adversarial robustness benchmarks consider image-independent\nperturbation radii, possibly resulting in some properties that are either easily veri\fed by all\nveri\fcation methods, or too hard to be veri\fed (for commonly employed timeouts) by any of\nthem. In line with the OVAL veri\fcation dataset from VNN-COMP 2020 [49], whose versions\nhave already been used in various recent works [6, 27, 10, 8, 9, 50, 57, 20, 21], the OVAL21\nbenchmark associates to each image-network pair a perturbation radius found via binary search\nto ensure that all properties are challenging to solve.\n15\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nNetworks The benchmark includes 3 ReLU-based convolutional networks which were robustly\ntrained [51] against `1perturbations of radius \u000f= 2=255 on CIFAR10. Two of the networks,\nnamed base and wide , are composed of 2 convolutional layers followed by 2 fully connected\nlayers and have respectively 3172 and 6244 activations. The third model, named deep , has 2\nadditional convolutional layers and a total of 6756 activations.\nSpeci\fcations The veri\fcation properties represent untargeted adversarial robustness (with\nrespect to all possible misclassi\fcations) to `1perturbations of varying \u000f, with a per-instance\ntimeout of 720 seconds. The property generation procedure relies on commonly employed\nlower and upper bounds to the adversarial loss to exclude perturbation radii that yield trivial\nproperties. 10 correctly classi\fed images per network are randomly sampled from the entire\nCIFAR10 test set, and a distinct \u000f2[0;16=255] is associated to each. First, a binary search is\nrun to \fnd the largest \u000fvalue for which a popular iterative adversarial attack [11] fails to \fnd\nan adversarial example. Then, a second binary search is run to \fnd the smallest \u000fvalue for\nwhich bounds [57] from the element-wise convex hull of the activations (with \fxed intermediate\nbounds from [51, 60]) fail to prove robustness. Both binary search procedures are run with a\ntolerance of \u000ftol= 0:1. Denoting \u000flbas the smallest output from the two routines, and \u000fubas\nthe largest, the following perturbation radius is chosen: \u000f=1\n3\u000flb+2\n3\u000fub.\nLink https://github.com/stanleybak/vnncomp2021/tree/main/benchmarks/oval21\n4.7 SRI ResNet A/B\nProposed by: The MN-BaB team\nMotivation While in previous instantiations of the VNN-COMP many benchmarks considered\ndi\u000berent architectures, thus allowing to judge the e\u000bect of architecture changes on the (relative)\nperformance of di\u000berent veri\fers, none allowed for a direct comparison of the e\u000bect of di\u000berent\ntraining methods. To enable such a comparison, we propose two benchmarks considering the\nsame network architecture, trained using adversarial training of di\u000berent strengths.\nNetworks We consider two residual ReLU networks [16] with 3 ResBlocks, preceded by one\nconvolutional layer and followed by two fully-connected layers, yielding a total of eight ReLU\nlayers. Both networks were trained using adversarial training, with the \"A\" version of the\nbenchmark using a weaker attack compared to the \"B\" version.\nSpeci\fcations We repeat the following process until we have collected 72 instances. We\nsample a random image from the CIFAR-10 test set, rejecting it immediately if it gets mis-\nclassi\fed. If it gets classi\fed correctly, we conduct a bisection search to \fnd the smallest\nperturbation magnitude leading to misclassi\fcation under adversarial attacks (with a maxi-\nmum of epsilon=0.005). We then generate a speci\fcation describing correct classi\fcation under\nan`1-norm perturbation of at most 0 :7 times the previously found epsilon and allow a per\nsample timeout of 5 minutes.\n4.8 VGGNET16 2022\nProposed by Stanley Bak, Stony Brook University\nMotivation This benchmark tries to scale up the size of networks being analyzed by using the\nwell-studied VGGNET-16 architecture [39] that runs on ImageNet. Input-output properties\nare proposed on pixel-level perturbations that can lead to image misclassi\fcation.\nNetworks All properties are run on the same network, which includes 138 million parameters.\nThe network features convolution layers, ReLU activation functions, as well as max pooling\nlayers.\n16\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nSpeci\fcations Properties analyzed ranged from single-pixel perturbations to perturbations\non over 95k of the inputs. Full L-in\fnity perturbations were not used, as this caused an issue\nwith the public speci\fcation \fle parser, which is a change we can address next year. A subset of\nthe images was used to create the speci\fcations, one from each category, which was randomly\nchosen to attack. Pixels to perturb were also randomly selected according to a random seed.\nLink https://github.com/stanleybak/vggnet16_benchmark2022/\n4.9 MNIST FC\nProposed by The VeriNet team.\nMotivation This benchmark contains fully connected networks with ReLU activation func-\ntions and varying depths.\nNetworks The benchmark set consists of three fully-connected classi\fcation networks with 2,\n4 and 6 layers and 256 ReLU nodes in each layer trained on the MNIST dataset. The networks\nwere \frst presented in a benchmark in VNN-COMP 2020 [49].\nSpeci\fcations We randomly sampled 15 correctly classi\fed images from the MNIST test set.\nFor each network and image, the speci\fcation was a correct classi\fcation under l1perturbations\nof at most \u000f= 0:03 and\u000f= 0:05. The timeouts were 2 minutes per instance for the 2-layer\nnetwork and 5 minutes for the remaining two networks.\n4.10 Reach Prob Density\nProposed by Stanley Bak, Stony Brook University\nMotivation This benchmark analyzes neural networks used to learn how probability densities\nchange through di\u000berential equations [31]. Given a initial uniform distribution in a box, for\nexample, \rows will tend to accumulate at certain states and become more sparse in other parts\nof the sapce. The function computing these is learned using a neural network, which can then\nbe used to e\u000eciently query for probabilistic statements about the reachable states or to perform\ncontrol [30].\nNetworks All networks consist of fully connected layers and ReLU activation functions, with\n4.6k to 70k parameters and 128-512 neurons, with a low input dimension (4-8 inputs). The\nthree systems analyzed from the original work were the Vanderpol model, the Robot model,\nand the GCAS model.\nSpeci\fcations Properties were created based on random ranges over the input and proba-\nbilities in the speci\fcation. For example, for the robot system, a random radius was chosen\nuniformly between 0.0 and 0.3, along with a random log probability between 0.05 and 0.3,\nand then a state was searched for near the obstacle at the origin with that radius above the\nprobability threshold. More speci\fcs on the other systems are given in the link below.\nLink https://github.com/stanleybak/reach_probability_benchmark2022/\n4.11 RL Benchmarks\nProposed by Veena Krish, Stony Brook University\nMotivation This benchmark deals with open-loop sensor-noise robustness of reinforcement\nlearning systems. Discrete output systems are examined. Given a speci\fc state of the system,\ncan bounded sensor noise cause the output command the change? These networks were chosen\nfrom work looking at closed-loop robustness where the open-loop problem would need to be\nrepeatedly solved.\n17\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nNetworks All networks consist of fully connected layers and ReLU activation functions. All\nnetworks are small, with between 129 and 512 neurons. Networks were created from the Dubin's\nrejoin task from the SafeRL benchmark suite [36], as well as from the CartPole and LunarLander\nbenchmarks from OpenAI gym [5], trained using the StableBaselines3 library.\nSpeci\fcations Speci\fcations can be relatively complex disjunctions since the property is that\nany other command could have been chosen. The benchmarks check if the output command\nchanges within some bounded noise.\nLink https://github.com/Ethos-lab/min-err-trajs-vnncomp-benchmarks\n4.12 TLL Verify Bench\nProposed by James Ferlez (University of California, Irvine)\nMotivation This benchmark consists of Two-Level Lattice (TLL) NNs, which have been shown\nto be amenable to fast veri\fcation algorithms (e.g. [12]). Thus, this benchmark was proposed as\na means of comparing TLL-speci\fc veri\fcation algorithms with general-purpose NN veri\fcation\nalgorithms (i.e. algorithms that can verify arbitrary deep, fully-connected ReLU NNs).\nNetworks The networks in this benchmark are a subset of the ones used in [12, Experiment 3].\nEach of these TLL NNs has n= 2 inputs and m= 1 output. The architecture of a TLL NN is\nfurther speci\fed by two parameters: N, the number of local linear functions, and M, the number\nof selector sets. This benchmark contains TLLs of sizes N=M= 8;16;24;32;40;48;56;64,\nwith 30 randomly generated examples of each (the generation procedure is described in [12,\nSection 6.1.1]). At runtime, the speci\fed veri\fcation timeout determines how many of these\nnetworks are included in the benchmark so as to achieve an overall 6-hour run time; this\nselection process is deterministic. Finally, a TLL NN has a natural representation using multiple\ncomputation paths [12, Figure 1], but many tools are only compatible with fully-connected\nnetworks. Hence, the ONNX models in this benchmark implement TLL NNs by \\stacking\" these\ncomputation paths to make a fully connected NN (leading to sparse weight matrices: i.e. with\nmany zero weights and biases). The TLLnet class ( https://github.com/jferlez/TLLnet )\ncontains the code necessary to generate these implementations via the exportONNX method.\nSpeci\fcations All speci\fcations have as input constraints the hypercube [ \u00002;2]2. Since all\nnetworks have only a single output, the output properties consist of a randomly generated\nreal number and a randomly generated inequality direction. Random output samples from the\nnetwork are used to roughly ensure that the real number property has an equal likelihood of\nbeing within the output range of the NN and being outside of it (either above or below all NN\noutputs on the input constraint set). The inequality direction is generated independently and\nwith each direction having an equal probability. This scheme biases the benchmark towards\nveri\fcation problems for which counterexamples exist.\nLink https://github.com/jferlez/TLLVerifyBench\nCommit 199d2c26d0ec456e62906366b694a875a21\u000b7ef\n4.13 ACAS Xu (unscored)\nNetworks The ACASXu benchmark consists of ten properties de\fned over 45 neural networks\nused to issue turn advisories to aircraft to avoid collisions. The neural networks have 300 neurons\narranged in 6 layers, with ReLU activation functions. There are \fve inputs corresponding to\nthe aircraft states, and \fve network outputs, where the minimum output is used as the turn\nadvisory the system ultimately produces.\n18\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nSpeci\fcations We use the original 10 properties [22], where properties 1-4 are checked on all\n45 networks as was done in later work by the original authors [24]. Properties 5-10 are checked\non a single network. The total number of benchmarks is therefore 186. The original veri\fcation\ntimes ranged from seconds to days|including some benchmark instances that did not \fnish.\nThis year we used a timeout of around two minutes (116 seconds) for each property, in order\nto \ft within a total maximum runtime of six hours.\n4.14 Cifar2020 (unscored)\nMotivation This benchmark combines two convolutional CIFAR10 networks from last year's\nVNN-COMP 2020 with a new, larger network with the goal to evaluate the progress made by\nthe whole \feld of Neural Network veri\fcation.\nNetworks The two ReLU networks cifar 102255and cifar 108255with two convolu-\ntional and two fully-connected layers were trained for `1perturbations of \u000f=2\n255and8\n255,\nrespectively, using COLT [32] and the larger ConvBig with four convolutional and three fully-\nconnected networks, was trained using adversarial training [29] and \u000f=2\n255.\nSpeci\fcations We draw the \frst 100 images from the CIFAR10 test set and for every network\nreject incorrectly classi\fed ones. For the remaining images, the speci\fcations describe a correct\nclassi\fcation under an `1-norm perturbation of at most2\n255and8\n255forcifar 102255and\nConvBig andcifar 108255, respectively and allow a per sample timeout of 5 minutes.\n19\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n5 Results\nEach tool was run on each of the benchmarks and produced a csvresult \fle, that was provided\nas feedback to the tool authors using the online execution platform. The \fnal csv\fles for each\ntool as well as scoring scripts are available online: https://github.com/ChristopherBrix/\nvnncomp2022_results . The results were analyzed automatically to compute scores and create\nthe statistics presented in this section.\nFor purposes of scoring, recall that the minimum time after subtracting overhead was 1.0\nseconds, so all times less than 1.0 get set to 1.0. Then, the fastest tools are awarded bonus\npoints as described in Section 2. Penalties usually occur when a tool produces an incorrect\nresult, but in one case (with \u000b;\f-CROWN) a penalty was awarded because no counterexample\nwas provided by the competition version of the code (remedied in a later version).\n5.1 Overall Score\nTable 5: Overall Score\n# Tool Score\n1\u000b,\f-CROWN 1274.9\n2MN-BaB 1017.5\n3 Verinet 892.4\n4 Nnenum 534.0\n5 Cgdtest 408.4\n6 Peregrinn 399.0\n7 Marabou 372.2\n8 Debona 222.9\n9 Fastbatllnn 100.0\n10 Verapak 98.2\n11 Averinn 29.1The winner of the VNN-COMP 2022 is \u000b;\f-CROWN, followed\nbyMN-BaB and then Verinet. For a full ranking see Table 5.\nIn Figure 1 we show the number of instances that were solved\nby the di\u000berent veri\fers within a certain runtime. For a more\ndetailed summary of the results of the individual benchmarks see\nAppendix A.1 (scored) and Appendix A.2 (unscored). For even\nmore detailed per instance results see Appendix A.3.\nThe most successful (top 3) tools all used GPU-based veri\fers\ncombining linear bound propagation with the branch-and-bound\nparadigm and participated in all benchmarks. Similar to the pre-\nvious year, several tools produced mismatching results (see Ta-\nble 11). However, thanks to the mandatory counterexample \fles,\nthe ground truth could often be established allowing the identi\f-\ncation of the incorrect veri\fers. This highlighted in particular that\nCGDTest often incorrectly returned UNSAT , i.e., claimed a property\nholds when other tools were able to \fnd counterexamples.\n 0.01 0.1 1 10 100 1000\n 200  400  600  800  1000  1200Five MinutesTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nNnenum\nCgdtest\nPeregrinn\nMarabou\nDebona\nFastbatllnn\nVerapak\nAverinnAll Instances\nFigure 1: Cactus Plot for All Instances.\n20\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n5.2 Other Stats\nThis section presents other statistics related to the measurements that are interesting but did\nnot play a direct role in scoring this year.\nTable 6: Overhead\n# Tool Seconds\n1 Marabou 0.2\n2 Fastbatllnn 0.5\n3 Nnenum 0.9\n4 Cgdtest 1.3\n5 Peregrinn 1.3\n6 Debona 2.0\n7 Averinn 3.1\n8 Verinet 3.4\n9 Verapak 4.6\n10\u000b,\f-CROWN 6.7\n11MN-BaB 8.2Table 7: Num Benchmarks Participated\n# Tool Count\n1 Verinet 13\n2MN-BaB 13\n3\u000b,\f-CROWN 13\n4 Cgdtest 12\n5 Nnenum 9\n6 Peregrinn 7\n7 Marabou 6\n8 Debona 5\n9 Verapak 3\n10 Fastbatllnn 1\n11 Averinn 1\nTable 8: Num Instances Veri\fed\n# Tool Count\n1\u000b,\f-CROWN 950\n2MN-BaB 812\n3 Verinet 754\n4 Nnenum 515\n5 Peregrinn 478\n6 Marabou 450\n7 Cgdtest 405\n8 Debona 341\n9 Verapak 117\n10 Averinn 100\n11 Fastbatllnn 32Table 9: Num SAT\n# Tool Count\n1 Verinet 228\n2MN-BaB 227\n3\u000b,\f-CROWN 227\n4 Nnenum 196\n5 Peregrinn 191\n6 Marabou 148\n7 Debona 138\n8 Cgdtest 101\n9 Fastbatllnn 21\n10 Averinn 8\n11 Verapak 6Table 10: Num UNSAT\n# Tool Count\n1\u000b,\f-CROWN 723\n2MN-BaB 585\n3 Verinet 526\n4 Nnenum 319\n5 Cgdtest 304\n6 Marabou 302\n7 Peregrinn 287\n8 Debona 203\n9 Verapak 111\n10 Averinn 92\n11 Fastbatllnn 11\nTable 11: Incorrect Results (or Missing CE (*))\n# Tool Count\n1 Cgdtest 41\n2 Verapak 5\n3 Marabou 1\n4\u000b,\f-CROWN*1\n*Result was correct, but counterexample (CE) was not saved. See Table 15.\n21\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n6 Conclusion and Ideas for Future Competitions\nThis report summarizes the 3rdVeri\fcation of Neural Networks Competition (VNN-COMP),\nheld in 2022. While we observed a signi\fcant increase in the diversity, complexity, and scale\nof the proposed benchmarks, the best-performing tools seem to converge to GPU-enabled lin-\near bound propagation methods using a branch-and-bound framework. In addition to the\nstandardization of input formats ( onnx and vnnlib ) and evaluation hardware, introduced for\nVNN-COMP 2021, VNN-COMP 2022 also standardized a format for counter-examples and\nintroduced a fully automated evaluation pipeline, requiring authors to provide complete instal-\nlation scripts. We hope that this increased standardization and automatization does not only\nsimplify the evaluation during the competition but also enables practitioners and researchers to\nmore easily apply a range of state-of-the-art veri\fcation methods to their individual problems.\nVNN-COMP 2022, successfully implemented a range of improvement opportunities identi-\n\fed during the previous iteration. These included requiring witnesses of found counter-examples\nto disambiguate tool disagreement, increasing automatization to enable a smoother \fnal eval-\nuation, and making a broader range of AWS instances available to allow for a better \ft with\ntools' requirements. Further ideas for future competitions include the use of scored bench-\nmarks speci\fcally designed for year-on-year progress tracking, the reduction of tool tuning, a\nbatch-processing mode, and more rigorous soundness evaluation.\nAcknowledgements\nThis competition is supported by a gift from the Lu Jin Family Foundation.\nThis research was supported in part by the Air Force Research Laboratory Information\nDirectorate, through the Air Force O\u000ece of Scienti\fc Research Summer Faculty Fellowship\nProgram, Contract Numbers FA8750-15-3-6003, FA9550-15-0001 and FA9550-20-F-0005. This\nmaterial is based upon work supported by the Air Force O\u000ece of Scienti\fc Research under\naward numbers FA9550-19-1-0288, FA9550-21-1-0121, and FA9550-22-1-0019, the National Sci-\nence Foundation (NSF) under grant numbers 1918450, 1911017, 2028001, 2220401, and 2220426,\nand the Defense Advanced Research Projects Agency (DARPA) Assured Autonomy program\nthrough contract number FA8750-18-C-0089. Any opinions, \fndings, and conclusions or recom-\nmendations expressed in this material are those of the author(s) and do not necessarily re\rect\nthe views of the United States Air Force, DARPA, nor NSF.\nTool authors listed in Section 3 participated in the preparation and review of this report.\n22\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nReferences\n[1] Stanley Bak. Execution-guided overapproximation (ego) for improving scalability of neural network\nveri\fcation, 2020.\n[2] Stanley Bak. nnenum: Veri\fcation of relu neural networks with optimized abstraction re\fnement.\nInNASA Formal Methods Symposium , pages 19{36. Springer, 2021.\n[3] Stanley Bak, Hoang-Dung Tran, Kerianne Hobbs, and Taylor T. Johnson. Improved geometric path\nenumeration for verifying ReLU neural networks. In 32nd International Conference on Computer-\nAided Veri\fcation (CAV) , July 2020.\n[4] Christopher Brix and Thomas Noll. Debona: Decoupled boundary network analysis for tighter\nbounds and faster adversarial robustness proofs. CoRR , abs/2006.09040, 2020.\n[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540 , 2016.\n[6] Rudy Bunel, Alessandro De Palma, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet\nKohli, Philip HS Torr, and M Pawan Kumar. Lagrangian decomposition for neural network\nveri\fcation. Conference on Uncertainty in Arti\fcial Intelligence , 2020.\n[7] Rudy Bunel, Ilker Turkaslan, Philip HS Torr, Pushmeet Kohli, and M Pawan Kumar. A uni\fed\nview of piecewise linear neural network veri\fcation. Advances in Neural Information Processing\nSystems , 2018.\n[8] Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H. S. Torr, and M. Pawan Kumar.\nScaling the convex barrier with active sets. In International Conference on Learning Representa-\ntions , 2021.\n[9] Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H. S. Torr, and M. Pawan Kumar.\nScaling the convex barrier with sparse dual algorithms. arXiv preprint arXiv:2101.05844 , 2021.\n[10] Alessandro De Palma, Rudy Bunel, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet\nKohli, Philip HS Torr, and M Pawan Kumar. Improved branch and bound for neural network\nveri\fcation via lagrangian decomposition. arXiv preprint arXiv:2104.06718 , 2021.\n[11] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li.\nBoosting adversarial attacks with momentum. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 9185{9193, 2018.\n[12] James Ferlez, Haitham Khedr, and Yasser Shoukry. Fast BATLLNN: fast box analysis of two-\nlevel lattice neural networks. In Ezio Bartocci and Sylvie Putot, editors, HSCC '22: 25th ACM\nInternational Conference on Hybrid Systems: Computation and Control, Milan, Italy, May 4 - 6,\n2022, pages 23:1{23:11. ACM, 2022.\n[13] Claudio Ferrari, Mark Niklas M uller, Nikola Jovanovic, and Martin T. Vechev. Complete veri\f-\ncation via multi-neuron relaxation guided branch-and-bound. In The Tenth International Confer-\nence on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net,\n2022.\n[14] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2022.\n[15] Haoyu He, Tianhao Wei, Huan Zhang, Changliu Liu, and Cheng Tan. Characterizing neural\nnetwork veri\fcation for systems with NN4SYSBench. 1st Workshop on Formal Veri\fcation of\nMachine Learning , 2022.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 770{778, 2016.\n[17] P. Henriksen, K. Hammernik, D. Rueckert, and A. Lomuscio. Bias \feld robustness veri\fcation\nof large neural image classi\fers. In Proceedings of the 32nd British Machine Vision Conference\n(BMVC21) . BMVA Press, 2021.\n[18] P. Henriksen and A. Lomuscio. E\u000ecient neural network veri\fcation via adaptive re\fnement\n23\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nand adversarial search. In Proceedings of the 24th European Conference on Arti\fcial Intelligence\n(ECAI20) , 2020.\n[19] P. Henriksen and A. Lomuscio. Deepsplit: An e\u000ecient splitting method for neural network veri-\n\fcation via indirect e\u000bect analysis. In Proceedings of the 30th International Joint Conference on\nArti\fcial Intelligence (IJCAI21) , To appear, August 2021.\n[20] Florian Jaeckle and M Pawan Kumar. Generating adversarial examples with graph neural net-\nworks. Conference on Uncertainty in Arti\fcial Intelligence , 2021.\n[21] Florian Jaeckle, Jingyue Lu, and M Pawan Kumar. Neural network branch-and-bound for neural\nnetwork veri\fcation. arXiv preprint arXiv:2107.12855 , 2021.\n[22] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An\ne\u000ecient smt solver for verifying deep neural networks. In International Conference on Computer\nAided Veri\fcation , pages 97{117. Springer, 2017.\n[23] Guy Katz, Derek A. Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth\nShah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljic, David L. Dill, Mykel J. Kochenderfer,\nand Clark W. Barrett. The marabou framework for veri\fcation and analysis of deep neural net-\nworks. In Isil Dillig and Serdar Tasiran, editors, Computer Aided Veri\fcation - 31st International\nConference, CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings, Part I , volume\n11561 of Lecture Notes in Computer Science , pages 443{452. Springer, 2019.\n[24] Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth\nShah, Shantanu Thakoor, Haoze Wu, Aleksandar Zelji\u0013 c, et al. The marabou framework for ver-\ni\fcation and analysis of deep neural networks. In International Conference on Computer Aided\nVeri\fcation , pages 443{452. Springer, 2019.\n[25] Haitham Khedr, James Ferlez, and Yasser Shoukry. Peregrinn: Penalized-relaxation greedy neural\nnetwork veri\fer. In International Conference on Computer Aided Veri\fcation , pages 287{300.\nSpringer, 2021.\n[26] Tim Kraska, Alex Beutel, Ed H Chi, Je\u000brey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data ,\n2018.\n[27] Jingyue Lu and M Pawan Kumar. Neural network branching for neural network veri\fcation. In\nInternational Conference on Learning Representations , 2020.\n[28] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 ,\n2017.\n[29] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. In Proc. International Conference\non Learning Representations (ICLR) , 2018.\n[30] Yue Meng, Zeng Qiu, Md Tawhid Bin Waez, and Chuchu Fan. Case studies for computing density\nof reachable states for safe autonomous motion planning. In NASA Formal Methods Symposium ,\npages 251{271. Springer, 2022.\n[31] Yue Meng, Dawei Sun, Zeng Qiu, Md Tawhid Bin Waez, and Chuchu Fan. Learning density\ndistribution of reachable states for autonomous systems. In Conference on Robot Learning , pages\n124{136. PMLR, 2022.\n[32] Martin Vechev Mislav Balunovic. Adversarial training and provable defenses: Bridging the gap.\nInProc. International Conference on Learning Representations (ICLR) , 2020.\n[33] Mark Niklas M uller, Gleb Makarchuk, Gagandeep Singh, Markus P uschel, and Martin Vechev.\nPrima: Precise and general neural network certi\fcation via multi-neuron convex relaxations. arXiv\npreprint arXiv:2103.03638 , 2021.\n[34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K opf,\nEdward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit\n24\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nSteiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,\nFlorence d'Alch\u0013 e-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Informa-\ntion Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pages 8024{8035, 2019.\n[35] Pavithra Prabhakar and Zahra Rahimi Afzal. Abstraction based output range analysis for neural\nnetworks. Advances in Neural Information Processing Systems , 32, 2019.\n[36] Umberto J Ravaioli, James Cunningham, John McCarroll, Vardaan Gangal, Kyle Dunlap, and\nKerianne L Hobbs. Safe reinforcement learning benchmark environments for aerospace control\nsystems. In 2022 IEEE Aerospace Conference (AERO) , pages 1{20. IEEE, 2022.\n[37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-\nical image segmentation. In International Conference on Medical image computing and computer-\nassisted intervention , pages 234{241. Springer, 2015.\n[38] David Shriver, Sebastian G. Elbaum, and Matthew B. Dwyer. DNNV: A framework for deep\nneural network veri\fcation. In Alexandra Silva and K. Rustan M. Leino, editors, Computer\nAided Veri\fcation - 33rd International Conference, CAV 2021, Virtual Event, July 20-23, 2021,\nProceedings, Part I , volume 12759 of Lecture Notes in Computer Science , pages 137{150. Springer,\n2021.\n[39] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\n[40] Gagandeep Singh, Rupanshu Ganvir, Markus P uschel, and Martin Vechev. Beyond the single neu-\nron convex barrier for neural network certi\fcation. In Advances in Neural Information Processing\nSystems 32 , pages 15098{15109. Curran Associates, Inc., 2019.\n[41] Gagandeep Singh, Timon Gehr, Markus P uschel, and Martin T. Vechev. An abstract domain for\ncertifying neural networks. Proc. ACM Program. Lang. , 3(POPL):41:1{41:30, 2019.\n[42] Gagandeep Singh, Timon Gehr, Markus P uschel, and Martin T. Vechev. Boosting robustness\ncerti\fcation of neural networks. In 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019.\n[43] Joshua Smith, Jarom Allan, Viswanathan Swaminathan, and Zhen Zhang. Refutation-based ad-\nversarial robustness veri\fcation of deep neural networks. In Formal Methods for ML-Enabled\nAutonomous Systems , 7 2021.\n[44] Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with\nmixed integer programming. In ICLR , 2019.\n[45] Vincent Tjeng, Kai Yuanqing Xiao, and Russ Tedrake. Evaluating robustness of neural networks\nwith mixed integer programming. In 7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019.\n[46] Hoang-Dung Tran, Stanley Bak, Weiming Xiang, and Taylor T. Johnson. Veri\fcation of deep\nconvolutional neural networks using imagestars. In 32nd International Conference on Computer-\nAided Veri\fcation (CAV) . Springer, July 2020.\n[47] Hoang-Dung Tran, Patrick Musau, Diego Manzanas Lopez, Xiaodong Yang, Luan Viet Nguyen,\nWeiming Xiang, and Taylor T. Johnson. Star-based reachability analysis for deep neural networks.\nIn23rd International Symposium on Formal Methods (FM'19) . Springer International Publishing,\nOctober 2019.\n[48] Hoang-Dung Tran, Neelanjana Pal, Patrick Musau, Diego Manzanas Lopez, Nathaniel Hamilton,\nXiaodong Yang, Stanley Bak, and Taylor T Johnson. Robustness veri\fcation of semantic segmen-\ntation neural networks using relaxed reachability. In International Conference on Computer Aided\nVeri\fcation , pages 263{286. Springer, 2021.\n[49] VNN-COMP. International veri\fcation of neural networks competition (VNN-COMP). Veri\fca-\ntion of Neural Networks workshop at the International Conference on Computer-Aided Veri\fcation ,\n2020.\n25\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n[50] Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and Zico Kolter.\nBeta-CROWN: E\u000ecient bound propagation with per-neuron split constraints for complete and\nincomplete neural network veri\fcation. arXiv preprint arXiv:2103.06624 , 2021.\n[51] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer\nadversarial polytope. ICML , 2018.\n[52] Haoze Wu, Clark Barrett, Mahmood Sharif, Nina Narodytska, and Gagandeep Singh. Scalable\nveri\fcation of gnn-based job schedulers. 6(OOPSLA2), oct 2022.\n[53] Haoze Wu, Alex Ozdemir, Aleksandar Zeljic, Kyle Julian, Ahmed Irfan, Divya Gopinath, Sadjad\nFouladi, Guy Katz, Corina Pasareanu, and Clark Barrett. Parallelization techniques for verifying\nneural networks. In # PLACEHOLDER PARENT METADATA VALUE# , volume 1, pages 128{\n137. TU Wien Academic Press, 2020.\n[54] Haoze Wu, Teruhiro Tagomori, Alexander Robey, Fengjun Yang, Nikolai Matni, George Pappas,\nHamed Hassani, Corina Pasareanu, and Clark Barrett. Toward certi\fed robustness against real-\nworld distribution shifts. arXiv preprint arXiv:2206.03669 , 2022.\n[55] Haoze Wu, Aleksandar Zelji\u0013 c, Guy Katz, and Clark Barrett. E\u000ecient neural network analysis with\nsum-of-infeasibilities. In International Conference on Tools and Algorithms for the Construction\nand Analysis of Systems , pages 143{163. Springer, 2022.\n[56] Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya\nKailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certi\fed\nrobustness and beyond. Advances in Neural Information Processing Systems , 33, 2020.\n[57] Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast\nand Complete: Enabling complete neural network veri\fcation with rapid and massively parallel\nincomplete veri\fers. In International Conference on Learning Representations , 2021.\n[58] Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,\nand Cho-Jui Hsieh. Towards stable and e\u000ecient training of veri\fably robust neural networks.\narXiv preprint arXiv:1906.06316 , 2019.\n[59] Huan Zhang*, Shiqi Wang*, Kaidi Xu*, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, and J Zico\nKolter. General cutting planes for bound-propagation-based neural network veri\fcation. Advances\nin Neural Information Processing Systems (NeurIPS) , 2022.\n[60] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. E\u000ecient neural\nnetwork robustness certi\fcation with general activation functions. Advances in Neural Information\nProcessing Systems , 31:4939{4948, 2018.\n26\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nA Extended Results\nIn this section, we provide more \fne-grained results.\nA.1 Scored Benchmarks\nTable 12: Benchmark carvana-unet-2022\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1\u000b,\fCrown 39 0 39 0 468 100.0%\n2MN-BaB 19 0 0 0 209 44.7%\n3 Verinet 3 0 0 0 30 6.4%\n 1 10 100\n 5  10  15  20  25  30  35  40One MinuteTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinetCarvana Unet 2022\nFigure 2: Cactus Plot for Carvana Unet 2022.\nTable 13: Benchmark cifar100-tinyimagenet-resnet\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1\u000b,\fCrown 69 0 56 0 813 100.0%\n2 Cgdtest 95 0 28 3 725 89.2%\n3MN-BaB 60 3 10 0 674 82.9%\n4 Verinet 48 3 6 0 540 66.4%\n27\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n 1 10 100\n 10  20  30  40  50  60  70  80  90One MinuteTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nCgdtestCIFAR100 Tiny ImageNet ResNet\nFigure 3: Cactus Plot for CIFAR100 Tiny ImageNet ResNet.\nTable 14: Benchmark cifar-biasfield\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1\u000b,\fCrown 69 1 1 0 736 100.0%\n2 Cgdtest 71 0 55 1 731 99.3%\n3 Verinet 69 1 0 0 721 98.0%\n4 Verapak 71 0 0 1 635 86.3%\n5MN-BaB 36 1 17 0 404 54.9%\n6 Marabou 27 0 0 0 270 36.7%\n7 Nnenum 4 0 0 0 43 5.8%\n 1 10 100\n 10  20  30  40  50  60  70One MinuteTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nNnenum\nCgdtest\nMarabou\nVerapakCIFAR Biasﬁeld\nFigure 4: Cactus Plot for CIFAR Bias\feld.\n28\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 15: Benchmark collins-rul-cnn\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1 Nnenum 16 45 58 0 727 100.0%\n2MN-BaB 16 44 57 0 715 98.3%\n3\u000b,\fCrown 15 45 56 1*612 84.2%\n4 Verinet 16 43 0 0 590 81.2%\n5 Peregrinn 14 42 0 0 560 77.0%\n6 Cgdtest 1 42 43 15 -984 0%\n*During the competition, Cgdtest reported UNSAT for the 20th property, con\ricting with\nSAT reported by \u000b;\f-CROWN. \u000b;\f-CROWN team provided a counterexample (CE) after\nscoring, proving the ground truth to be SAT. However, \u000b;\f-CROWN was penalized because\nthe CE was not saved to disk during the competition.\n 0.01 0.1 1 10 100\n 10  20  30  40  50  60One MinuteTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nNnenum\nCgdtest\nPeregrinnCollins Rul CNN\nFigure 5: Cactus Plot for Collins Rul CNN.\nTable 16: Benchmark mnist-fc\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1\u000b,\fCrown 66 18 53 0 963 100.0%\n2 Verinet 53 18 50 0 817 84.8%\n3MN-BaB 53 18 47 0 804 83.5%\n4 Debona 48 18 38 0 737 76.5%\n5 Nnenum 48 11 29 0 649 67.4%\n6 Marabou 44 16 0 0 600 62.3%\n7 Peregrinn 27 11 7 0 394 40.9%\n8 Cgdtest 66 3 23 5 241 25.0%\n9 Verapak 40 2 42 4 104 10.8%\n29\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n 0.01 0.1 1 10 100\n 10  20  30  40  50  60  70  80One MinuteTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nNnenum\nCgdtest\nPeregrinn\nMarabou\nDebona\nVerapakMNIST FC\nFigure 6: Cactus Plot for MNIST FC.\nTable 17: Benchmark nn4sys\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1\u000b,\fCrown 152 0 132 0 1799 100.0%\n2MN-BaB 106 0 8 0 1140 63.4%\n3 Verinet 57 0 43 0 661 36.7%\n4 Peregrinn 24 0 22 0 284 15.8%\n5 Nnenum 23 0 8 0 246 13.7%\n6 Debona 2 0 2 0 24 1.3%\n7 Cgdtest 2 0 2 2 -176 0%\n 0.1 1 10 100\n 20  40  60  80  100  120  140One MinuteTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nNnenum\nCgdtest\nPeregrinn\nDebonaNN4SYS\nFigure 7: Cactus Plot for NN4SYS.\n30\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 18: Benchmark oval21\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1\u000b,\fCrown 25 1 10 0 291 100.0%\n2MN-BaB 19 1 2 0 205 70.4%\n3 Verinet 17 1 1 0 189 64.9%\n4 Marabou 19 0 17 1 125 43.0%\n5 Nnenum 3 1 0 0 40 13.7%\n6 Peregrinn 1 0 0 0 10 3.4%\n7 Cgdtest 11 0 1 7 -580 0%\n 1 10 100 1000\n 5  10  15  20  25Five MinutesTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nNnenum\nCgdtest\nPeregrinn\nMarabouOVAL 21\nFigure 8: Cactus Plot for OVAL 21.\nTable 19: Benchmark reach-prob-density\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1 Nnenum 22 14 22 0 411 100.0%\n2\u000b,\fCrown 22 14 23 0 406 98.8%\n3 Verinet 22 14 10 0 383 93.2%\n4MN-BaB 22 12 14 0 368 89.5%\n5 Marabou 17 14 12 0 334 81.3%\n6 Peregrinn 18 14 2 0 324 78.8%\n7 Cgdtest 0 5 5 0 60 14.6%\n8 Debona 0 2 2 0 24 5.8%\n31\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n 0.1 1 10 100\n 5  10  15  20  25  30  35Five MinutesTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nNnenum\nCgdtest\nPeregrinn\nMarabou\nDebonaReachability Probability Density\nFigure 9: Cactus Plot for Reachability Probability Density.\nTable 20: Benchmark rl-benchmarks\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1\u000b,\fCrown 193 103 296 0 3552 100.0%\n2 Verinet 193 103 292 0 3547 99.9%\n3MN-BaB 193 103 288 0 3536 99.5%\n4 Nnenum 191 103 283 0 3506 98.7%\n5 Peregrinn 193 103 271 0 3502 98.6%\n6 Marabou 191 103 278 0 3496 98.4%\n7 Debona 153 99 240 0 3000 84.5%\n8 Averinn 92 8 16 0 1032 29.1%\n9 Cgdtest 10 24 29 3 98 2.8%\n10 Verapak 0 4 0 0 40 1.1%\nTable 21: Benchmark sri-resnet-a\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1\u000b,\fCrown 20 12 7 0 356 100.0%\n2 Cgdtest 26 6 14 0 352 98.9%\n3MN-BaB 18 12 19 0 343 96.3%\n4 Verinet 12 12 4 0 248 69.7%\n32\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n 0.01 0.1 1 10 100\n 50  100  150  200  250  300One MinuteTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nNnenum\nCgdtest\nPeregrinn\nMarabou\nDebona\nVerapak\nAverinnReinforcement Learning Benchmarks\nFigure 10: Cactus Plot for Reinforcement Learning Benchmarks.\n 0.1 1 10 100\n 5  10  15  20  25  30One MinuteTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nCgdtestSRI Resnet A\nFigure 11: Cactus Plot for SRI Resnet A.\nTable 22: Benchmark sri-resnet-b\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1MN-BaB 27 11 24 0 435 100.0%\n2\u000b,\fCrown 28 11 9 0 435 100.0%\n3 Cgdtest 22 10 9 0 340 78.2%\n4 Verinet 20 11 4 0 321 73.8%\n33\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n 0.1 1 10 100\n 5  10  15  20  25  30  35  40One MinuteTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nCgdtestSRI Resnet B\nFigure 12: Cactus Plot for SRI Resnet B.\nTable 23: Benchmark tllverifybench\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1 Fastbatllnn 11 21 32 0 384 100.0%\n2MN-BaB 11 21 21 0 364 94.8%\n3\u000b,\fCrown 11 21 12 0 353 91.9%\n4 Peregrinn 10 21 7 0 324 84.4%\n5 Verinet 11 21 0 0 320 83.3%\n6 Nnenum 1 21 10 0 240 62.5%\n7 Debona 0 19 10 0 210 54.7%\n8 Marabou 4 15 2 0 194 50.5%\n9 Cgdtest 0 9 6 1 2 0.5%\n 0.01 0.1 1 10 100\n 5  10  15  20  25  30Five MinutesTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nNnenum\nCgdtest\nPeregrinn\nMarabou\nDebona\nFastbatllnnTwo-Level Lattice Verify Benchmark\nFigure 13: Cactus Plot for Two-Level Lattice Verify Benchmark.\n34\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 24: Benchmark vggnet16-2022\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1\u000b,\fCrown 14 1 11 0 176 100.0%\n2 Nnenum 11 1 0 0 127 72.2%\n3MN-BaB 5 1 4 0 69 39.2%\n4 Verinet 5 1 0 0 60 34.1%\n5 Cgdtest 0 2 1 4 -378 0%\n 1 10 100 1000\n 2  4  6  8  10  12  14Five MinutesTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nNnenum\nCgdtestVGGNet16 2022\nFigure 14: Cactus Plot for VGGNet16 2022.\n35\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nA.2 Unscored Benchmarks\nTable 25: Benchmark acasxu\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1 Nnenum 139 47 174 0 2218 100.0%\n2\u000b,\fCrown 139 46 59 0 2021 91.1%\n3MN-BaB 110 46 52 0 1664 75.0%\n4 Cgdtest 85 30 115 7 680 30.7%\n 0.01 0.1 1 10 100\n 20  40  60  80  100  120  140  160  180One MinuteTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nNnenum\nCgdtestACAS Xu (Unscored)\nFigure 15: Cactus Plot for ACAS Xu (Unscored).\nTable 26: Benchmark cifar2020\n# Tool Veri\fed Falsi\fed Fastest Penalty Score Percent\n1 Verinet 91 35 109 0 1486 100.0%\n2\u000b,\fCrown 95 34 78 0 1479 99.5%\n3MN-BaB 93 28 26 0 1275 85.8%\n4 Nnenum 66 19 0 0 850 57.2%\n5 Cgdtest 63 26 5 6 305 20.5%\n6 Verapak 0 15 1 0 152 10.2%\n7 Marabou 4 0 0 1 -60 0%\n36\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\n 0.01 0.1 1 10 100\n 20  40  60  80  100  120One MinuteTime (sec)\nNumber of Instances VeriﬁedAB-CROWN\nMN BaB\nVerinet\nNnenum\nCgdtest\nMarabou\nVerapakCIFAR 2020 (Unscored)\nFigure 16: Cactus Plot for CIFAR 2020 (Unscored).\n37\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nA.3 Detailed Results\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nCarvana 2022 0 unsat 7.53 - - - - - - - - - -\nCarvana 2022 1 unsat 26.1 76.4 - - - - - - - - -\nCarvana 2022 2 ? - - - - - - - - - - -\nCarvana 2022 3 ? - - - - - - - - - - -\nCarvana 2022 4 unsat 7.83 - - - - - - - - - -\nCarvana 2022 5 unsat 8.73 - - - - - - - - - -\nCarvana 2022 6 unsat 26.9 - - - - - - - - - -\nCarvana 2022 7 ? - - - - - - - - - - -\nCarvana 2022 8 ? - - - - - - - - - - -\nCarvana 2022 9 unsat 26.2 148 - - - - - - - - -\nCarvana 2022 10 ? - - - - - - - - - - -\nCarvana 2022 11 unsat 26.2 39.1 - - - - - - - - -\nCarvana 2022 12 unsat 26.9 - - - - - - - - - -\nCarvana 2022 13 unsat 31.2 44.7 - - - - - - - - -\nCarvana 2022 14 ? - - - - - - - - - - -\nCarvana 2022 15 ? - - - - - - - - - - -\nCarvana 2022 16 unsat 7.03 - - - - - - - - - -\nCarvana 2022 17 ? - - - - - - - - - - -\nCarvana 2022 18 unsat 7.59 - - - - - - - - - -\nCarvana 2022 19 unsat 7.01 - - - - - - - - - -\nCarvana 2022 20 unsat 28.4 67.1 - - - - - - - - -\nCarvana 2022 21 ? - - - - - - - - - - -\nCarvana 2022 22 unsat 6.08 - - - - - - - - - -\nCarvana 2022 23 ? - - - - - - - - - - -\nCarvana 2022 24 ? - - - - - - - - - - -\nCarvana 2022 25 unsat 26.2 39.2 - - - - - - - - -\nCarvana 2022 26 ? - - - - - - - - - - -\nCarvana 2022 27 ? - - - - - - - - - - -\nCarvana 2022 28 ? - - - - - - - - - - -\nCarvana 2022 29 ? - - - - - - - - - - -\nCarvana 2022 30 unsat 26.8 132 - - - - - - - - -\nCarvana 2022 31 ? - - - - - - - - - - -\nCarvana 2022 32 ? - - - - - - - - - - -\nCarvana 2022 33 ? - - - - - - - - - - -\nCarvana 2022 34 ? - - - - - - - - - - -\nCarvana 2022 35 ? - - - - - - - - - - -\nCarvana 2022 36 unsat 26.2 45.6 - - - - - - - - -\nCarvana 2022 37 unsat 6.99 - - - - - - - - - -\nCarvana 2022 38 unsat 2.29 28.0 264 - - - - - - - -\nCarvana 2022 39 unsat 26.4 84.6 - - - - - - - - -\nCarvana 2022 40 unsat 26.3 61.6 - - - - - - - - -\nCarvana 2022 41 ? - - - - - - - - - - -\nCarvana 2022 42 ? - - - - - - - - - - -\nCarvana 2022 43 unsat 7.60 - - - - - - - - - -\nCarvana 2022 44 unsat 7.53 - - - - - - - - - -\nCarvana 2022 45 ? - - - - - - - - - - -\nCarvana 2022 46 unsat 8.73 - - - - - - - - - -\nCarvana 2022 47 ? - - - - - - - - - - -\nCarvana 2022 48 ? - - - - - - - - - - -\nCarvana 2022 49 unsat 26.1 52.9 - - - - - - - - -\nCarvana 2022 50 ? - - - - - - - - - - -\nCarvana 2022 51 unsat 26.1 62.6 - - - - - - - - -\nCarvana 2022 52 unsat 8.64 - - - - - - - - - -\nCarvana 2022 53 unsat 26.8 - - - - - - - - - -\nCarvana 2022 54 ? - - - - - - - - - - -\nCarvana 2022 55 unsat 26.9 170 - - - - - - - - -\nCarvana 2022 56 ? - - - - - - - - - - -\nCarvana 2022 57 ? - - - - - - - - - - -\nCarvana 2022 58 ? - - - - - - - - - - -\nCarvana 2022 59 unsat 7.75 - - - - - - - - - -\nCarvana 2022 60 unsat 26.1 40.8 272 - - - - - - - -\nCarvana 2022 61 unsat 31.2 78.6 - - - - - - - - -\nCarvana 2022 62 unsat 26.2 52.9 - - - - - - - - -\nCarvana 2022 63 unsat 7.67 - - - - - - - - - -\nCarvana 2022 64 unsat 26.2 171 - - - - - - - - -\n38\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nCarvana 2022 65 ? - - - - - - - - - - -\nCarvana 2022 66 ? - - - - - - - - - - -\nCarvana 2022 67 unsat 25.9 31.8 257 - - - - - - - -\nCarvana 2022 68 unsat 6.19 - - - - - - - - - -\nCarvana 2022 69 unsat 8.68 - - - - - - - - - -\nCarvana 2022 70 ? - - - - - - - - - - -\nCarvana 2022 71 unsat 8.53 - - - - - - - - - -\nCifar100 Tiny 0 unsat 8.88 30.4 12.8 - 32.7 - - - - - -\nCifar100 Tiny 1 unsat 12.9 64.1 - - 32.3 - - - - - -\nCifar100 Tiny 2 unsat - - - - 32.7 - - - - - -\nCifar100 Tiny 3 unsat 13.2 52.1 - - 31.7 - - - - - -\nCifar100 Tiny 4 unsat - - - - 31.8 - - - - - -\nCifar100 Tiny 5 unsat - - - - 32.9 - - - - - -\nCifar100 Tiny 6 unsat - - - - 32.3 - - - - - -\nCifar100 Tiny 7 unsat 13.0 51.1 - - 34.5 - - - - - -\nCifar100 Tiny 8 unsat 26.1 - - - 34.3 - - - - - -\nCifar100 Tiny 9 unsat - - - - 32.9 - - - - - -\nCifar100 Tiny 10 unsat - - - - 38.4 - - - - - -\nCifar100 Tiny 11 unsat - - - - 34.2 - - - - - -\nCifar100 Tiny 12 unsat - - - - 35.4 - - - - - -\nCifar100 Tiny 13 unsat 9.82 31.4 65.4 - 35.9 - - - - - -\nCifar100 Tiny 14 unsat 9.02 29.1 12.7 - 37.1 - - - - - -\nCifar100 Tiny 15 unsat 9.57 31.0 22.0 - 34.5 - - - - - -\nCifar100 Tiny 16 unsat 9.17 23.0 14.6 - 35.0 - - - - - -\nCifar100 Tiny 17 unsat 14.7 138 - - 35.4 - - - - - -\nCifar100 Tiny 18 unsat 9.14 27.6 19.9 - 35.0 - - - - - -\nCifar100 Tiny 19 unsat 24.7 - - - 36.5 - - - - - -\nCifar100 Tiny 20 unsat 9.32 19.8 15.9 - 35.5 - - - - - -\nCifar100 Tiny 21 unsat 12.1 81.5 - - 35.0 - - - - - -\nCifar100 Tiny 22 unsat 9.96 52.2 73.1 - 35.3 - - - - - -\nCifar100 Tiny 23 unsat 10.4 45.5 100 - 35.4 - - - - - -\nCifar100 Tiny 24 unsat - - - - 35.5 - - - - - -\nCifar100 Tiny 25 unsat 9.08 23.3 8.58 - 36.6 - - - - - -\nCifar100 Tiny 26 unsat 11.7 69.1 - - 36.0 - - - - - -\nCifar100 Tiny 27 unsat - - - - 36.5 - - - - - -\nCifar100 Tiny 28 unsat 9.94 35.0 50.6 - 35.2 - - - - - -\nCifar100 Tiny 29 unsat 10.0 37.4 54.0 - 36.5 - - - - - -\nCifar100 Tiny 30 unsat 6.16 8.09 4.01 - 37.4 - - - - - -\nCifar100 Tiny 31 unsat 13.9 173 - - 35.1 - - - - - -\nCifar100 Tiny 32 unsat 9.02 25.7 20.2 - 38.7 - - - - - -\nCifar100 Tiny 33 unsat 10.1 35.5 71.1 - 35.2 - - - - - -\nCifar100 Tiny 34 unsat 11.8 14.6 15.7 - 47.7 - - - - - -\nCifar100 Tiny 35 unsat 53.4 - - - 48.8 - - - - - -\nCifar100 Tiny 36 unsat 12.3 12.4 13.2 - 48.5 - - - - - -\nCifar100 Tiny 37 unsat 13.1 11.7 12.8 - 47.8 - - - - - -\nCifar100 Tiny 38 unsat 27.5 - - - 49.3 - - - - - -\nCifar100 Tiny 39 unsat 4.86 6.56 4.63 - 48.2 - - - - - -\nCifar100 Tiny 40 unsat 12.7 24.8 128 - 48.2 - - - - - -\nCifar100 Tiny 41 unsat 13.1 43.7 - - 49.4 - - - - - -\nCifar100 Tiny 42 unsat 11.1 16.3 12.9 - 48.2 - - - - - -\nCifar100 Tiny 43 unsat 4.61 6.67 4.70 - 48.6 - - - - - -\nCifar100 Tiny 44 unsat - - - - 49.5 - - - - - -\nCifar100 Tiny 45 unsat 12.2 19.6 30.0 - 47.7 - - - - - -\nCifar100 Tiny 46 unsat 38.8 - - - 50.3 - - - - - -\nCifar100 Tiny 47 unsat 13.8 76.9 - - 48.1 - - - - - -\nCifar100 Tiny 48 unsat - - - - 48.4 - - - - - -\nCifar100 Tiny 49 unsat 11.7 22.1 14.2 - 49.2 - - - - - -\nCifar100 Tiny 50 unsat 6.57 6.70 4.79 - 48.2 - - - - - -\nCifar100 Tiny 51 unsat 12.0 14.1 18.7 - 48.1 - - - - - -\nCifar100 Tiny 52 unsat - - - - 48.7 - - - - - -\nCifar100 Tiny 53 unsat 10.6 20.1 17.7 - 48.1 - - - - - -\nCifar100 Tiny 54 unsat 11.6 26.5 192 - 49.2 - - - - - -\nCifar100 Tiny 55 unsat 10.7 8.68 10.9 - 50.9 - - - - - -\nCifar100 Tiny 56 unsat 11.3 11.0 12.2 - 47.9 - - - - - -\nCifar100 Tiny 57 unsat 11.6 22.3 86.6 - 48.4 - - - - - -\nCifar100 Tiny 58 unsat 9.55 24.1 25.5 - 38.1 - - - - - -\nCifar100 Tiny 59 unsat 9.72 14.0 14.5 - 38.4 - - - - - -\nCifar100 Tiny 60 unsat - - - - 37.9 - - - - - -\n39\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nCifar100 Tiny 61 unsat - - - - 38.3 - - - - - -\nCifar100 Tiny 62 unsat - - - - 38.3 - - - - - -\nCifar100 Tiny 63 unsat 10.9 36.3 115 - 38.3 - - - - - -\nCifar100 Tiny 64 unsat 10.0 14.3 13.1 - 40.6 - - - - - -\nCifar100 Tiny 65 unsat 10.2 28.6 59.7 - 38.5 - - - - - -\nCifar100 Tiny 66 unsat - - - - 38.1 - - - - - -\nCifar100 Tiny 67 unsat 9.72 22.0 16.6 - 38.6 - - - - - -\nCifar100 Tiny 68 unsat 5.95 6.52 4.27 - 38.2 - - - - - -\nCifar100 Tiny 69 unsat - - - - 41.0 - - - - - -\nCifar100 Tiny 70 unsat - - - - 38.6 - - - - - -\nCifar100 Tiny 71 unsat 46.9 - - - 38.7 - - - - - -\nCifar100 Tiny 72 unsat 11.1 32.4 101 - 37.8 - - - - - -\nCifar100 Tiny 73 unsat - - - - 40.0 - - - - - -\nCifar100 Tiny 74 unsat - - - - 109 - - - - - -\nCifar100 Tiny 75 unsat 11.7 7.26 11.5 - 106 - - - - - -\nCifar100 Tiny 76 unsat 16.2 81.8 - - 106 - - - - - -\nCifar100 Tiny 77 unsat 16.9 62.9 - - 106 - - - - - -\nCifar100 Tiny 78 unsat 4.82 7.91 12.3 - 107 - - - - - -\nCifar100 Tiny 79 unsat 13.5 16.1 53.1 - 105 - - - - - -\nCifar100 Tiny 80 unsat 13.7 22.4 76.8 - 105 - - - - - -\nCifar100 Tiny 81 unsat 12.9 9.24 13.5 - 106 - - - - - -\nCifar100 Tiny 82 sat - 1.46 5.44 - 7 - - - - - -\nCifar100 Tiny 83 unsat 14.4 29.3 - - 107 - - - - - -\nCifar100 Tiny 84 sat - 1.15 4.69 - 7 - - - - - -\nCifar100 Tiny 85 unsat - - - - 109 - - - - - -\nCifar100 Tiny 86 unsat 12.8 21.3 43.8 - 106 - - - - - -\nCifar100 Tiny 87 sat - 1.57 4.86 - 7 - - - - - -\nCifar100 Tiny 88 unsat - - - - 104 - - - - - -\nCifar100 Tiny 89 unsat - - - - 100 - - - - - -\nCifar100 Tiny 90 unsat 25.9 - - - 98.9 - - - - - -\nCifar100 Tiny 91 unsat - - - - 102 - - - - - -\nCifar100 Tiny 92 unsat 11.2 12.1 15.8 - 107 - - - - - -\nCifar100 Tiny 93 unsat 11.9 17.5 18.1 - 106 - - - - - -\nCifar100 Tiny 94 unsat 20.5 - - - 106 - - - - - -\nCifar100 Tiny 95 unsat 13.1 10.5 11.8 - 100 - - - - - -\nCifar100 Tiny 96 unsat - - - - 103 - - - - - -\nCifar100 Tiny 97 unsat 55.0 - - - 105 - - - - - -\nCifar Bias\feld 0 unsat 4.88 33.6 4.65 - 3.20 - 115 - - 5.23 -\nCifar Bias\feld 1 unsat 6.18 - 5.53 - 3.29 - 114 - - 5.04 -\nCifar Bias\feld 2 unsat 3.97 6.52 4.47 - 3.25 - 115 - - 5.21 -\nCifar Bias\feld 3 unsat 6.30 - 5.97 - 3.66 - - - - 5.12 -\nCifar Bias\feld 4 unsat 19.6 - 16.8 - 3.24 - - - - 5.08 -\nCifar Bias\feld 5 unsat 5.63 - 5.63 - 3.40 - - - - 5.10 -\nCifar Bias\feld 6 unsat 3.43 1.23 3.88 - 3.35 - - - - 5.07 -\nCifar Bias\feld 7 unsat 8.21 - 8.06 - 3.61 - 115 - - 5.47 -\nCifar Bias\feld 8 sat 0.52 1.19 4.30 - 7 - - - - 7 -\nCifar Bias\feld 9 unsat 6.06 151 5.10 - 3.08 - 115 - - 12.8 -\nCifar Bias\feld 10 unsat 8.96 - 8.81 - 3.50 - - - - 12.4 -\nCifar Bias\feld 11 unsat 5.33 - 4.67 - 3.28 - - - - 14.2 -\nCifar Bias\feld 12 unsat 150 - 72.8 - 3.38 - - - - 12.7 -\nCifar Bias\feld 13 unsat 3.42 1.41 3.83 - 3.54 - 117 - - 12.6 -\nCifar Bias\feld 14 unsat 9.42 - 11.7 - 3.62 - - - - 12.6 -\nCifar Bias\feld 15 unsat 3.41 1.18 3.84 2.76 3.48 - - - - 14.7 -\nCifar Bias\feld 16 unsat 7.66 - 7.19 - 3.59 - - - - 12.6 -\nCifar Bias\feld 17 unsat 3.40 1.16 3.80 - 3.58 - - - - 10.8 -\nCifar Bias\feld 18 unsat 3.96 6.32 4.38 - 3.49 - - - - 5.14 -\nCifar Bias\feld 19 unsat - - - - 3.25 - - - - 4.94 -\nCifar Bias\feld 20 unsat 39.3 - 26.5 - 3.29 - - - - 4.83 -\nCifar Bias\feld 21 unsat 61.1 - 60.5 - 3.52 - 117 - - 5.06 -\nCifar Bias\feld 22 unsat 3.62 25.0 4.35 - 3.31 - - - - 5.02 -\nCifar Bias\feld 23 unsat - - - - 3.33 - - - - 5.24 -\nCifar Bias\feld 24 unsat 3.37 1.15 3.86 - 3.46 - 118 - - 5.15 -\nCifar Bias\feld 25 unsat 4.66 31.2 4.55 - 3.32 - 116 - - 5.05 -\nCifar Bias\feld 26 unsat 3.40 1.19 3.78 - 3.13 - 117 - - 5.10 -\nCifar Bias\feld 27 unsat 4.33 153 4.37 - 3.58 - 116 - - 5.13 -\nCifar Bias\feld 28 unsat 5.55 80.3 4.85 - 3.62 - - - - 4.94 -\nCifar Bias\feld 29 unsat 104 - 53.1 - 3.39 - - - - 5.17 -\nCifar Bias\feld 30 unsat 4.18 44.1 4.52 - 3.48 - - - - 9.87 -\n40\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nCifar Bias\feld 31 unsat 6.18 - 6.61 - 3.44 - - - - 12.8 -\nCifar Bias\feld 32 unsat 3.40 1.21 3.86 - 3.49 - 116 - - 12.8 -\nCifar Bias\feld 33 unsat 3.79 41.9 4.81 - 3.42 - - - - 13.1 -\nCifar Bias\feld 34 unsat 3.40 1.20 3.89 - 3.46 - - - - 14.8 -\nCifar Bias\feld 35 unsat 4.88 42.2 4.64 - 3.38 - - - - 12.9 -\nCifar Bias\feld 36 unsat 6.22 - 5.42 - 3.53 - - - - 12.6 -\nCifar Bias\feld 37 unsat 5.73 62.7 4.84 - 3.34 - 116 - - 14.7 -\nCifar Bias\feld 38 unsat 4.35 7.38 4.40 - 3.41 - - - - 12.8 -\nCifar Bias\feld 39 unsat 24.9 - 19.9 - 3.08 - - - - 12.6 -\nCifar Bias\feld 40 unsat 3.43 0.93 3.90 - 3.30 - - - - 7.18 -\nCifar Bias\feld 41 unsat 3.42 1.15 3.85 - 3.35 - 117 - - 4.97 -\nCifar Bias\feld 42 unsat 5.51 - 5.45 - 3.59 - 116 - - 4.79 -\nCifar Bias\feld 43 unsat 3.63 - 4.28 - 3.37 - 116 - - 5.11 -\nCifar Bias\feld 44 unsat 4.89 - 6.16 - 3.32 - 117 - - 5.05 -\nCifar Bias\feld 45 unsat 3.69 45.8 4.44 - 3.35 - - - - 5.08 -\nCifar Bias\feld 46 unsat 3.42 1.20 3.83 - 3.65 - 116 - - 5.00 -\nCifar Bias\feld 47 unsat 3.40 1.31 3.85 - 3.69 - 116 - - 5.29 -\nCifar Bias\feld 48 unsat 5.23 - 5.00 - 3.52 - 116 - - 5.12 -\nCifar Bias\feld 49 unsat 4.57 7.05 4.42 - 3.60 - 117 - - 5.01 -\nCifar Bias\feld 50 unsat 59.7 - 62.4 - 3.11 - - - - 5.23 -\nCifar Bias\feld 51 unsat 72.1 - 88.6 - 3.12 - - - - 5.31 -\nCifar Bias\feld 52 unsat 4.58 9.97 4.38 - 3.48 - 117 - - 5.30 -\nCifar Bias\feld 53 unsat 3.96 42.7 4.43 - 3.29 - 115 - - 13.6 -\nCifar Bias\feld 54 unsat 6.32 - 5.72 - 3.36 - - - - 12.4 -\nCifar Bias\feld 55 unsat 6.85 - 7.10 - 3.26 - 117 - - 13.0 -\nCifar Bias\feld 56 unsat 3.41 1.39 3.86 3.74 3.43 - - - - 15.0 -\nCifar Bias\feld 57 unsat 4.52 25.7 4.44 - 3.47 - 108 - - 12.7 -\nCifar Bias\feld 58 unsat 19.7 - 30.9 - 3.28 - - - - 12.6 -\nCifar Bias\feld 59 unsat 5.22 - 5.21 - 3.19 - - - - 12.5 -\nCifar Bias\feld 60 unsat 8.25 - 7.31 - 3.17 - - - - 14.5 -\nCifar Bias\feld 61 unsat 3.41 1.06 3.81 2.47 3.26 - 117 - - 13.0 -\nCifar Bias\feld 62 unsat 257 - 109 - 3.43 - - - - 11.1 -\nCifar Bias\feld 63 unsat 6.12 - 4.87 - 3.26 - - - - 4.73 -\nCifar Bias\feld 64 unsat 6.49 - 5.50 - 3.53 - - - - 4.77 -\nCifar Bias\feld 65 unsat 3.43 1.18 3.80 - 3.49 - - - - 4.99 -\nCifar Bias\feld 66 unsat 3.66 32.9 4.30 - 3.42 - - - - 5.20 -\nCifar Bias\feld 67 unsat 3.42 1.14 3.83 3.12 3.33 - - - - 4.98 -\nCifar Bias\feld 68 unsat 14.4 - 15.2 - 3.27 - - - - 5.07 -\nCifar Bias\feld 69 unsat 4.59 7.88 4.45 - 3.56 - - - - 5.07 -\nCifar Bias\feld 70 unsat 8.59 - 10.8 - 3.32 - 115 - - 4.88 -\nCifar Bias\feld 71 unsat 10.4 - 10.0 - 3.38 - - - - 4.94 -\nCollins Rul Cnn 0 sat 0.05 0.26 3.56 0.54 0.14 2.79 - - - - -\nCollins Rul Cnn 1 sat 0.05 0.24 3.56 0.49 0.22 2.82 - - - - -\nCollins Rul Cnn 2 sat 0.05 0.28 3.65 0.61 0.21 2.85 - - - - -\nCollins Rul Cnn 3 sat 0.05 0.28 3.58 0.60 0.19 2.84 - - - - -\nCollins Rul Cnn 4 unsat 1.14 0.15 3.54 0.28 7 13.6 - - - - -\nCollins Rul Cnn 5 unsat 1.13 0.33 3.59 0.28 - 13.6 - - - - -\nCollins Rul Cnn 6 unsat 1.11 0.26 3.59 0.29 7 13.7 - - - - -\nCollins Rul Cnn 7 sat 0.05 0.26 3.62 0.63 0.19 2.83 - - - - -\nCollins Rul Cnn 8 sat 0.03 0.22 3.57 0.58 0.19 2.85 - - - - -\nCollins Rul Cnn 9 sat 0.04 0.26 3.60 0.53 0.18 2.85 - - - - -\nCollins Rul Cnn 10 unsat 1.12 0.26 3.63 0.33 7 13.6 - - - - -\nCollins Rul Cnn 11 sat 0.03 0.29 3.60 0.70 0.19 2.86 - - - - -\nCollins Rul Cnn 12 sat 0.02 0.32 3.58 0.67 0.18 2.82 - - - - -\nCollins Rul Cnn 13 sat 0.04 0.11 3.61 0.73 0.18 2.89 - - - - -\nCollins Rul Cnn 14 unsat 1.12 0.28 3.61 0.32 - 13.9 - - - - -\nCollins Rul Cnn 15 sat 0.04 0.36 3.64 0.55 0.18 2.84 - - - - -\nCollins Rul Cnn 16 sat 0.02 0.33 3.54 0.69 0.21 2.81 - - - - -\nCollins Rul Cnn 17 unsat 1.13 0.26 3.60 0.27 7 13.6 - - - - -\nCollins Rul Cnn 18 sat<0.01 0.28 3.60 0.53 0.23 2.86 - - - - -\nCollins Rul Cnn 19 sat 0.01 203 3.71 1.12 0.21 2.84 - - - - -\nCollins Rul Cnn 20 sat57 - - - 4.45 - - - - - -\nCollins Rul Cnn 21 sat 1.15 2.15 3.60 0.94 7 - - - - - -\n5Instance was shown to be SAT after the competition. \u000b,\f-C was awarded a penalty as they only provided\na counterexample supporting their claim of the instance being SAT after the competition. Thus, CGD was not\nawarded a penalty despite incorrectly claiming the instance to be UNSAT .\n41\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nCollins Rul Cnn 22 sat 0.05 0.20 3.62 0.59 0.20 5.18 - - - - -\nCollins Rul Cnn 23 sat 0.05 0.23 3.58 0.60 0.19 5.31 - - - - -\nCollins Rul Cnn 24 sat 0.04 0.23 3.62 0.62 0.19 5.32 - - - - -\nCollins Rul Cnn 25 sat 0.02 0.24 3.60 0.64 0.21 5.39 - - - - -\nCollins Rul Cnn 26 unsat 1.13 0.03 3.55 0.23 7 35.4 - - - - -\nCollins Rul Cnn 27 unsat 1.12 0.29 3.60 0.31 - 35.2 - - - - -\nCollins Rul Cnn 28 unsat 1.13 0.18 3.65 0.28 7 35.6 - - - - -\nCollins Rul Cnn 29 sat 0.03 0.34 3.55 0.64 0.19 5.18 - - - - -\nCollins Rul Cnn 30 sat 0.03 0.26 3.50 0.70 0.18 5.16 - - - - -\nCollins Rul Cnn 31 sat 0.02 0.34 3.59 0.70 0.17 5.18 - - - - -\nCollins Rul Cnn 32 unsat 1.12 0.26 3.58 0.29 7 35.6 - - - - -\nCollins Rul Cnn 33 sat<0.01 0.31 3.59 0.65 0.18 5.23 - - - - -\nCollins Rul Cnn 34 sat 0.02 0.25 3.57 0.61 0.22 5.16 - - - - -\nCollins Rul Cnn 35 sat 0.41 0.33 3.62 0.59 0.18 5.57 - - - - -\nCollins Rul Cnn 36 unsat 1.14 0.30 3.55 0.37 - 35.8 - - - - -\nCollins Rul Cnn 37 sat 0.02 0.28 3.59 0.76 0.20 5.15 - - - - -\nCollins Rul Cnn 38 sat 0.03 0.25 3.55 0.59 0.19 5.08 - - - - -\nCollins Rul Cnn 39 unsat 1.13 0.24 3.58 0.30 7 35.4 - - - - -\nCollins Rul Cnn 40 sat 0.05 0.29 3.60 0.67 0.19 5.16 - - - - -\nCollins Rul Cnn 41 sat 10.1 2.05 - 3.94 7 - - - - - -\nCollins Rul Cnn 42 sat 0.63 1.71 - 2.68 7 - - - - - -\nCollins Rul Cnn 43 sat 0.03 0.36 3.61 0.55 0.28 23.8 - - - - -\nCollins Rul Cnn 44 sat 0.02 0.30 3.60 0.57 0.28 23.6 - - - - -\nCollins Rul Cnn 45 sat 0.02 0.28 3.63 0.71 0.28 23.8 - - - - -\nCollins Rul Cnn 46 unsat 1.28 0.31 3.55 0.60 7 125 - - - - -\nCollins Rul Cnn 47 unsat 1.28 0.25 3.65 0.78 7 - - - - - -\nCollins Rul Cnn 48 sat 0.03 0.29 3.61 0.64 0.28 23.8 - - - - -\nCollins Rul Cnn 49 sat 0.04 0.18 3.62 0.69 0.32 23.6 - - - - -\nCollins Rul Cnn 50 sat 0.03 0.34 3.63 0.65 0.28 23.6 - - - - -\nCollins Rul Cnn 51 sat 0.04 0.21 3.60 0.85 0.28 23.8 - - - - -\nCollins Rul Cnn 52 sat 0.03 0.27 3.62 0.56 0.28 23.7 - - - - -\nCollins Rul Cnn 53 unsat 1.28 0.38 3.53 0.57 7 127 - - - - -\nCollins Rul Cnn 54 sat 0.04 0.27 3.57 0.59 0.28 23.8 - - - - -\nCollins Rul Cnn 55 sat 0.02 0.33 3.62 0.65 0.27 23.6 - - - - -\nCollins Rul Cnn 56 sat 0.06 0.33 3.58 0.65 0.28 23.6 - - - - -\nCollins Rul Cnn 57 sat 0.03 0.29 3.57 0.63 0.28 23.3 - - - - -\nCollins Rul Cnn 58 sat 0.10 - 3.72 3.99 0.26 23.7 - - - - -\nCollins Rul Cnn 59 sat 0.05 0.28 3.58 0.54 0.27 23.7 - - - - -\nCollins Rul Cnn 60 sat 0.05 0.10 3.62 0.76 0.28 23.8 - - - - -\nCollins Rul Cnn 61 unsat - 0.33 3.65 0.63 7 - - - - - -\nMnist Fc 0 unsat 0.54 0.22 0.83 0.39 7.84 9.64 4.51 12.1 - 0.17 -\nMnist Fc 1 sat 0.36 0.31 1.05 6.43 - 1.65 4.41 0.19 - 7 -\nMnist Fc 2 sat 2.59 0.25 0.99 2.13 7 9.00 4.56 0.19 - 7 -\nMnist Fc 3 unsat 0.55 0.17 0.84 0.38 8.86 0.63 4.51 0.18 - 0.03 -\nMnist Fc 4 unsat 0.56 3.03 1.04 0.37 8.31 9.73 7.87 0.67 - 0.03 -\nMnist Fc 5 unsat 0.56 0.05 0.88 0.38 7.95 4.59 3.96 12.2 - 0.03 -\nMnist Fc 6 unsat 2.97 15.7 2.99 12.1 - - 13.0 96.0 - <0.01 -\nMnist Fc 7 unsat 0.85 0.28 0.87 0.38 8.08 0.64 4.51 0.19 - 0.07 -\nMnist Fc 8 unsat 2.99 4.04 0.95 1.80 7.56 3.75 8.02 12.4 - 0.04 -\nMnist Fc 9 unsat 0.54 0.25 0.79 0.37 8.03 9.83 4.06 0.15 - 0.07 -\nMnist Fc 10 unsat 0.54 0.26 0.90 0.40 7.79 3.42 4.26 0.17 - 0.03 -\nMnist Fc 11 unsat 0.53 0.21 0.87 0.37 7.76 0.64 4.21 0.16 - 0.04 -\nMnist Fc 12 sat 0.04 0.14 0.95 0.37 - 0.66 3.96 0.19 - <0.01 -\nMnist Fc 13 unsat 0.53 0.30 0.83 0.38 8.63 0.66 4.20 0.19 - 0.02 -\nMnist Fc 14 unsat 0.53 0.32 0.86 0.40 7.99 0.64 4.46 0.18 - 0.04 -\nMnist Fc 15 sat 4.70 0.20 1.30 1.34 7 1.89 4.66 0.29 - - -\nMnist Fc 16 sat 0.37 0.27 0.95 1.54 0.22 2.00 4.81 0.17 - - -\nMnist Fc 17 sat 0.36 0.04 1.32 1.39 1.85 1.93 4.52 0.20 - - -\nMnist Fc 18 unsat 0.56 2.84 1.03 0.82 7.99 9.76 4.31 12.4 - - -\nMnist Fc 19 sat 0.38 0.25 1.48 40.7 7 1.78 6.07 0.28 - - -\nMnist Fc 20 sat 0.37 0.21 1.06 8.01 2.48 1.73 4.67 0.19 - - -\nMnist Fc 21 sat 0.38 0.26 1.01 31.4 - 2.01 4.11 0.30 - - -\nMnist Fc 22 unsat 0.53 0.27 0.88 0.79 8.24 8.69 4.06 12.2 - - -\nMnist Fc 23 sat 0.38 0.20 0.89 1.39 - 2.00 4.31 0.18 - - -\nMnist Fc 24 unsat 2.55 10.1 1.64 4.19 8.17 20.5 8.82 1.92 - - -\nMnist Fc 25 unsat 3.13 16.9 3.96 18.2 7.61 - 11.6 85.2 - - -\nMnist Fc 26 unsat 2.66 9.08 1.51 2.84 - - 8.58 22.0 - - -\nMnist Fc 27 sat 0.03 0.21 0.93 0.39 - 0.64 4.41 0.20 - 0.05 -\n42\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nMnist Fc 28 unsat 0.58 2.80 1.22 0.86 8.57 11.8 4.81 1.47 - - -\nMnist Fc 29 unsat 2.44 3.07 1.17 2.79 7.63 - 7.93 13.3 - - -\nMnist Fc 30 unsat 0.94 0.25 0.88 0.38 8.32 14.0 6.97 0.17 - 0.10 -\nMnist Fc 31 sat 0.09 0.29 1.15 - - - 18.6 0.28 - 7 -\nMnist Fc 32 unsat 0.71 108 1.49 4.07 8.24 169 16.3 13.2 - 0.09 -\nMnist Fc 33 unsat 0.93 0.28 0.95 0.39 7.74 14.5 6.11 0.20 - 0.11 -\nMnist Fc 34 unsat 0.63 112 0.96 3.77 8.35 79.2 6.46 12.7 - 0.14 -\nMnist Fc 35 unsat 0.62 0.26 0.87 0.39 8.15 14.3 6.36 0.21 - 0.13 -\nMnist Fc 36 unsat 112 - - - 8.23 - - - - 0.13 -\nMnist Fc 37 unsat 0.64 0.25 0.92 2.54 7.71 39.9 6.72 1.52 - 0.15 -\nMnist Fc 38 unsat 13.0 131 14.0 50.0 8.40 - - 263 - 0.19 -\nMnist Fc 39 unsat 0.63 0.28 0.98 3.19 8.49 28.9 6.67 12.4 - 0.12 -\nMnist Fc 40 unsat 0.64 0.16 0.89 0.39 7.97 13.9 6.37 0.21 - 0.12 -\nMnist Fc 41 unsat 0.63 0.24 0.91 0.38 7.61 10.8 6.32 0.20 - 0.14 -\nMnist Fc 42 unsat 0.63 0.22 0.89 2.54 - 64.1 6.72 12.4 - 0.16 -\nMnist Fc 43 unsat 0.61 0.26 0.86 0.43 7.65 14.2 6.42 12.5 - 0.09 -\nMnist Fc 44 unsat 0.92 0.25 0.89 0.38 7.68 13.7 6.07 0.19 - 0.12 -\nMnist Fc 45 unsat 38.9 164 62.2 - 7.76 - - - - - -\nMnist Fc 46 sat 0.11 0.28 1.03 - - - 17.2 0.17 - - -\nMnist Fc 47 unsat - - - - 7.98 - - - - - -\nMnist Fc 48 unsat 14.2 58.2 6.57 15.7 8.39 - 25.8 16.5 - - -\nMnist Fc 49 unsat 105 - - - 7.98 - - - - - -\nMnist Fc 50 unsat 38.0 124 39.8 270 7.84 - - - - - -\nMnist Fc 51 sat 0.10 0.26 1.84 - 7 - 52.9 0.32 - - -\nMnist Fc 52 unsat 57.8 - - - 7.83 - - - - - -\nMnist Fc 53 sat 0.13 0.27 1.01 - 7 - - 0.38 - - -\nMnist Fc 54 unsat 122 - - - 8.53 - - - - - -\nMnist Fc 55 unsat 47.3 202 - - 8.21 - - - - - -\nMnist Fc 56 unsat 19.6 122 10.7 134 7.99 - 285 215 - - -\nMnist Fc 57 unsat 68.1 - - - - - - - - - -\nMnist Fc 58 unsat 54.9 188 17.1 - 8.09 - - 116 - - -\nMnist Fc 59 unsat 20.0 123 10.8 55.9 7.64 - - 41.1 - - -\nMnist Fc 60 unsat 0.70 0.31 0.94 0.38 8.52 - 9.88 0.20 - 0.29 -\nMnist Fc 61 unsat 153 - - - 8.29 - - - - 0.22 -\nMnist Fc 62 unsat 0.78 95.4 1.80 8.21 8.64 - 23.9 13.1 - 0.27 -\nMnist Fc 63 unsat 0.71 0.24 0.96 0.38 8.89 - 9.77 0.20 - 0.22 -\nMnist Fc 64 unsat 0.69 0.29 0.88 0.38 8.25 - 9.17 0.20 - 0.20 -\nMnist Fc 65 unsat 0.68 0.27 0.95 9.33 8.31 - 9.02 12.4 - 0.23 -\nMnist Fc 66 sat 0.14 0.10 1.11 - - - 103 0.35 - 7 -\nMnist Fc 67 unsat 25.5 117 18.0 49.8 - - - 51.0 - 0.25 -\nMnist Fc 68 unsat 1.01 0.31 0.91 0.37 8.01 - 9.07 0.25 - 0.25 -\nMnist Fc 69 unsat 39.0 - 47.4 - 8.10 - - - - 0.18 -\nMnist Fc 70 unsat 0.67 0.22 0.87 0.41 8.53 - 8.42 0.23 - 0.21 -\nMnist Fc 71 unsat 0.68 0.31 0.93 0.37 8.25 - 9.17 0.19 - 0.16 -\nMnist Fc 72 unsat 1.83 95.9 2.79 14.9 8.61 - 39.1 15.7 - 0.27 -\nMnist Fc 73 unsat 0.67 0.20 0.92 6.74 8.85 - 8.92 12.5 - 0.22 -\nMnist Fc 74 unsat 0.97 0.26 1.00 0.38 8.38 - 9.67 0.20 - 0.28 -\nMnist Fc 75 unsat 63.8 136 - - 8.26 - - - - - -\nMnist Fc 76 sat 0.46 0.33 3.73 - - - - 90.6 - - -\nMnist Fc 77 unsat - - - - 8.15 - - - - - -\nMnist Fc 78 unsat 28.1 113 19.4 - 8.10 - - - - - -\nMnist Fc 79 unsat 61.3 - - - 8.03 - - - - - -\nMnist Fc 80 unsat - - - - 7.95 - - - - - -\nMnist Fc 81 sat 0.12 0.28 1.11 - - - 70.1 0.32 - - -\nMnist Fc 82 ? - - - - - - - - - - -\nMnist Fc 83 unsat 52.4 - 77.0 - 7.84 - - - - - -\nMnist Fc 84 unsat - - - - 7.76 - - - - - -\nMnist Fc 85 unsat 70.0 - - - 8.14 - - - - - -\nMnist Fc 86 unsat 117 - - - 8.18 - - - - - -\nMnist Fc 87 unsat - - - - 8.42 - - - - - -\nMnist Fc 88 unsat 159 - - - 7.64 - - - - - -\nMnist Fc 89 unsat 127 - - - 8.37 - - - - - -\nNn4sys 0 unsat 0.30 1.04 0.13 <0.01 0.46 0.06 - 0.09 - - -\nNn4sys 1 unsat 0.28 1.02 0.15 <0.01 0.39 0.09 - 0.06 - - -\nNn4sys 2 unsat 0.28 1.12 0.16 0.32 - 0.11 - - - - -\nNn4sys 3 unsat 0.27 1.28 0.19 0.36 - 0.10 - - - - -\nNn4sys 4 unsat 0.28 1.12 0.18 0.66 - 0.09 - - - - -\nNn4sys 5 unsat 0.30 1.12 0.21 0.74 - 0.16 - - - - -\n43\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nNn4sys 6 unsat 0.28 1.16 0.17 0.98 - 0.10 - - - - -\nNn4sys 7 unsat 0.31 1.47 0.21 1.11 - 0.15 - - - - -\nNn4sys 8 unsat 0.29 1.25 0.16 1.29 - 0.11 - - - - -\nNn4sys 9 unsat 0.31 1.39 0.20 1.45 - 0.15 - - - - -\nNn4sys 10 unsat 0.30 1.33 0.18 1.60 - 0.13 - - - - -\nNn4sys 11 unsat 0.31 1.18 0.20 1.82 - 0.16 - - - - -\nNn4sys 12 unsat 0.38 2.34 0.19 4.78 - 0.24 - - - - -\nNn4sys 13 unsat 0.38 2.61 0.23 5.45 - 0.27 - - - - -\nNn4sys 14 unsat 0.44 3.05 0.19 8.01 - 0.33 - - - - -\nNn4sys 15 unsat 0.49 3.47 0.24 8.99 - 0.37 - - - - -\nNn4sys 16 unsat 0.58 3.68 0.22 11.1 - 0.45 - - - - -\nNn4sys 17 unsat 0.60 4.27 0.25 12.5 - 0.67 - - - - -\nNn4sys 18 unsat 0.62 4.01 0.19 14.4 - 0.56 - - - - -\nNn4sys 19 unsat 0.69 5.11 0.28 16.0 - 0.60 - - - - -\nNn4sys 20 unsat 0.66 4.88 0.20 15.9 - 0.66 - - - - -\nNn4sys 21 unsat 0.77 5.57 0.28 17.9 - 0.71 - - - - -\nNn4sys 22 unsat 3.19 23.8 0.38 96.7 - 4.04 - - - - -\nNn4sys 23 unsat 3.36 26.8 0.62 - - 4.07 - - - - -\nNn4sys 24 unsat 1.78 2.08 0.21 - - - - - - - -\nNn4sys 25 unsat 2.36 1.24 0.60 - - - - - - - -\nNn4sys 26 unsat 2.42 1.88 0.72 - - - - - - - -\nNn4sys 27 unsat 2.09 2.34 1.08 - - - - - - - -\nNn4sys 28 unsat 2.54 2.60 1.26 - - - - - - - -\nNn4sys 29 unsat 2.20 2.85 1.49 - - - - - - - -\nNn4sys 30 unsat 3.07 3.19 1.85 - - - - - - - -\nNn4sys 31 unsat 3.18 3.49 2.09 - - - - - - - -\nNn4sys 32 unsat 3.18 4.06 2.27 - - - - - - - -\nNn4sys 33 unsat 3.28 3.92 2.51 - - - - - - - -\nNn4sys 34 unsat 3.31 4.54 2.64 - - - - - - - -\nNn4sys 35 unsat 4.01 5.85 3.47 - - - - - - - -\nNn4sys 36 unsat 4.73 6.85 4.06 - - - - - - - -\nNn4sys 37 unsat 4.84 7.88 4.92 - - - - - - - -\nNn4sys 38 unsat 5.60 7.86 5.69 - - - - - - - -\nNn4sys 39 unsat 5.70 9.58 6.34 - - - - - - - -\nNn4sys 40 unsat 6.43 10.5 7.23 - - - - - - - -\nNn4sys 41 unsat 7.32 12.1 8.11 - - - - - - - -\nNn4sys 42 unsat 7.42 13.2 8.93 - - - - - - - -\nNn4sys 43 unsat 8.24 13.2 8.98 - - - - - - - -\nNn4sys 44 unsat 2.43 1.38 0.36 - 7 - - - - - -\nNn4sys 45 unsat 2.95 3.33 0.62 - - - - - - - -\nNn4sys 46 unsat 3.23 5.52 8.94 - - - - - - - -\nNn4sys 47 unsat 3.33 8.29 13.5 - - - - - - - -\nNn4sys 48 unsat 3.41 14.5 16.5 - - - - - - - -\nNn4sys 49 unsat 3.46 13.7 - - - - - - - - -\nNn4sys 50 unsat 3.66 15.5 26.6 - - - - - - - -\nNn4sys 51 unsat 3.49 17.5 30.8 - - - - - - - -\nNn4sys 52 unsat 3.67 19.9 36.5 - - - - - - - -\nNn4sys 53 unsat 3.78 25.3 35.5 - - - - - - - -\nNn4sys 54 unsat 3.73 24.0 - - - - - - - - -\nNn4sys 55 unsat 4.04 28.6 - - - - - - - - -\nNn4sys 56 unsat 4.07 32.9 51.0 - - - - - - - -\nNn4sys 57 unsat 4.04 33.8 57.6 - - - - - - - -\nNn4sys 58 unsat 4.01 34.9 - - - - - - - - -\nNn4sys 59 unsat 4.28 39.6 - - - - - - - - -\nNn4sys 60 unsat 6.34 61.2 - - - - - - - - -\nNn4sys 61 unsat 6.94 73.2 - - - - - - - - -\nNn4sys 62 unsat 9.30 - - - - - - - - - -\nNn4sys 63 unsat 9.47 - - - - - - - - - -\nNn4sys 64 unsat 9.79 - - - - - - - - - -\nNn4sys 65 unsat 12.5 - - - - - - - - - -\nNn4sys 66 unsat 12.9 - - - - - - - - - -\nNn4sys 67 unsat 15.3 - - - - - - - - - -\nNn4sys 68 unsat 15.5 - - - - - - - - - -\nNn4sys 69 unsat 16.1 - - - - - - - - - -\nNn4sys 70 unsat 18.7 - - - - - - - - - -\nNn4sys 71 unsat 19.4 - - - - - - - - - -\nNn4sys 72 unsat 21.3 - - - - - - - - - -\nNn4sys 73 unsat 22.2 - - - - - - - - - -\n44\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nNn4sys 74 unsat 22.1 - - - - - - - - - -\nNn4sys 75 unsat 24.2 - - - - - - - - - -\nNn4sys 76 unsat 25.1 - - - - - - - - - -\nNn4sys 77 unsat 27.8 - - - - - - - - - -\nNn4sys 78 unsat 28.7 - - - - - - - - - -\nNn4sys 79 unsat 29.1 - - - - - - - - - -\nNn4sys 80 unsat 31.4 - - - - - - - - - -\nNn4sys 81 unsat 32.0 - - - - - - - - - -\nNn4sys 82 unsat 34.4 - - - - - - - - - -\nNn4sys 83 unsat 34.8 - - - - - - - - - -\nNn4sys 84 unsat 35.4 - - - - - - - - - -\nNn4sys 85 unsat 2.59 1.64 0.89 - - - - - - - -\nNn4sys 86 unsat 3.57 3.26 - - - - - - - - -\nNn4sys 87 unsat 4.33 5.33 - - - - - - - - -\nNn4sys 88 unsat 4.64 7.30 - - - - - - - - -\nNn4sys 89 unsat 5.55 9.82 - - - - - - - - -\nNn4sys 90 unsat 6.61 10.5 - - - - - - - - -\nNn4sys 91 unsat 7.81 12.0 - - - - - - - - -\nNn4sys 92 unsat 8.53 15.9 - - - - - - - - -\nNn4sys 93 unsat 9.24 17.4 - - - - - - - - -\nNn4sys 94 unsat 9.98 18.2 - - - - - - - - -\nNn4sys 95 unsat 10.7 21.8 - - - - - - - - -\nNn4sys 96 unsat 13.9 26.0 - - - - - - - - -\nNn4sys 97 unsat 16.6 31.5 - - - - - - - - -\nNn4sys 98 unsat 19.1 39.5 - - - - - - - - -\nNn4sys 99 unsat 22.2 44.9 - - - - - - - - -\nNn4sys 100 unsat 23.9 51.5 - - - - - - - - -\nNn4sys 101 unsat 27.2 56.1 - - - - - - - - -\nNn4sys 102 unsat 30.1 63.5 - - - - - - - - -\nNn4sys 103 unsat 31.8 67.8 - - - - - - - - -\nNn4sys 104 unsat 35.2 75.2 - - - - - - - - -\nNn4sys 105 unsat 37.4 79.5 - - - - - - - - -\nNn4sys 106 unsat 40.2 84.0 - - - - - - - - -\nNn4sys 107 unsat 43.4 92.2 - - - - - - - - -\nNn4sys 108 unsat 45.1 97.4 - - - - - - - - -\nNn4sys 109 unsat 48.5 106 - - - - - - - - -\nNn4sys 110 unsat 50.5 105 - - - - - - - - -\nNn4sys 111 unsat 3.88 2.29 1.14 - 7 - - - - - -\nNn4sys 112 unsat 5.01 8.80 - - - - - - - - -\nNn4sys 113 unsat 5.72 19.5 - - - - - - - - -\nNn4sys 114 unsat 6.63 23.6 - - - - - - - - -\nNn4sys 115 unsat 7.55 34.7 - - - - - - - - -\nNn4sys 116 unsat 8.78 49.2 - - - - - - - - -\nNn4sys 117 unsat 9.61 67.7 - - - - - - - - -\nNn4sys 118 unsat 10.6 87.5 - - - - - - - - -\nNn4sys 119 unsat 11.8 71.1 - - - - - - - - -\nNn4sys 120 unsat 12.9 77.1 - - - - - - - - -\nNn4sys 121 unsat 13.2 91.0 - - - - - - - - -\nNn4sys 122 unsat 15.2 108 - - - - - - - - -\nNn4sys 123 unsat 17.1 142 - - - - - - - - -\nNn4sys 124 unsat 16.2 126 - - - - - - - - -\nNn4sys 125 unsat 17.4 129 - - - - - - - - -\nNn4sys 126 unsat 19.3 146 - - - - - - - - -\nNn4sys 127 unsat 28.1 237 - - - - - - - - -\nNn4sys 128 unsat 34.0 256 - - - - - - - - -\nNn4sys 129 unsat 43.7 - - - - - - - - - -\nNn4sys 130 unsat 48.2 - - - - - - - - - -\nNn4sys 131 unsat 56.4 - - - - - - - - - -\nNn4sys 132 unsat 65.9 - - - - - - - - - -\nNn4sys 133 unsat 70.4 - - - - - - - - - -\nNn4sys 134 unsat 80.5 - - - - - - - - - -\nNn4sys 135 unsat 85.7 - - - - - - - - - -\nNn4sys 136 unsat 92.0 - - - - - - - - - -\nNn4sys 137 unsat 98.7 - - - - - - - - - -\nNn4sys 138 unsat 109 - - - - - - - - - -\nNn4sys 139 unsat 118 - - - - - - - - - -\nNn4sys 140 unsat 125 - - - - - - - - - -\nNn4sys 141 unsat 127 - - - - - - - - - -\n45\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nNn4sys 142 unsat 138 - - - - - - - - - -\nNn4sys 143 unsat 142 - - - - - - - - - -\nNn4sys 144 unsat 152 - - - - - - - - - -\nNn4sys 145 unsat 158 - - - - - - - - - -\nNn4sys 146 unsat 169 - - - - - - - - - -\nNn4sys 147 unsat 173 - - - - - - - - - -\nNn4sys 148 unsat 181 - - - - - - - - - -\nNn4sys 149 unsat 189 - - - - - - - - - -\nNn4sys 150 unsat 198 - - - - - - - - - -\nNn4sys 151 unsat 207 - - - - - - - - - -\nOval21 0 unsat 8.73 82.0 14.9 - 7 - - - - - -\nOval21 1 unsat 12.5 110 29.1 - 26.4 - - - - - -\nOval21 2 unsat 6.02 62.0 8.93 - 7 - - - - - -\nOval21 3 unsat - - - - 26.2 - - - - - -\nOval21 4 unsat 5.24 32.1 5.74 - - - - - - - -\nOval21 5 unsat 6.94 49.1 9.61 - - - - - - - -\nOval21 6 unsat 12.7 146 67.5 - 7 - - - - - -\nOval21 7 unsat 6.05 38.0 6.54 - 25.9 - - - - - -\nOval21 8 unsat 15.3 176 - - - - - - - - -\nOval21 9 unsat 25.6 307 - - 26.1 - - - - - -\nOval21 10 unsat - - - - 27.2 - 5.91 - - - -\nOval21 11 unsat - - - - 27.8 - 5.96 - - - -\nOval21 12 unsat 17.8 335 137 - - - 5.91 - - - -\nOval21 13 unsat 566 - - - 27.6 - 5.97 - - - -\nOval21 14 sat 1.22 0.36 3.68 14.3 - - 7 - - - -\nOval21 15 unsat - - - - 27.1 - 5.97 - - - -\nOval21 16 unsat 553 - - - 27.5 - 5.98 - - - -\nOval21 17 unsat 7.91 46.3 8.48 - - - 5.97 - - - -\nOval21 18 unsat 0.66 0.20 3.61 4.41 - 61.1 5.97 - - - -\nOval21 19 unsat 76.0 - - - - - 5.96 - - - -\nOval21 20 unsat 175 - - - - - 4.31 - - - -\nOval21 21 unsat 10.3 39.8 12.0 - - - 4.26 - - - -\nOval21 22 unsat 47.1 - - - - - 4.26 - - - -\nOval21 23 unsat 9.81 72.8 11.3 - 7 - 4.31 - - - -\nOval21 24 unsat 20.9 260 692 - 7 - 4.31 - - - -\nOval21 25 unsat 7.88 24.5 5.08 - - - 4.31 - - - -\nOval21 26 unsat 89.7 - - - 27.2 - 4.31 - - - -\nOval21 27 unsat 5.52 395 15.8 57.4 26.3 - 4.26 - - - -\nOval21 28 unsat 5.21 14.6 3.66 19.2 7 - 4.32 - - - -\nOval21 29 unsat 14.2 260 163 - 7 - 4.27 - - - -\nReach Prob Den 0 unsat 0.66 7.65 6.20 0.86 - 4.59 17.2 - - - -\nReach Prob Den 1 unsat 0.60 5.13 3.58 0.74 - 3.54 22.0 - - - -\nReach Prob Den 2 unsat 0.65 7.67 6.40 0.92 - 4.33 20.6 - - - -\nReach Prob Den 3 sat 0.64 5.65 2.74 0.53 - 2.88 7.72 - - - -\nReach Prob Den 4 unsat 0.64 6.36 5.15 0.74 - 4.37 20.3 - - - -\nReach Prob Den 5 unsat 0.66 7.29 5.50 0.76 - 4.34 24.0 - - - -\nReach Prob Den 6 sat 0.67 7.77 1.03 0.39 0.03 2.90 0.20 - - - -\nReach Prob Den 7 unsat 0.65 7.12 6.20 0.85 - 4.04 22.9 - - - -\nReach Prob Den 8 sat 0.63 6.15 2.75 0.54 - 4.34 9.58 - - - -\nReach Prob Den 9 unsat 0.63 6.01 4.56 1.00 - 3.67 23.0 - - - -\nReach Prob Den 10 unsat 0.65 7.07 5.76 0.84 - 4.40 22.4 - - - -\nReach Prob Den 11 unsat 0.59 4.28 3.14 1.02 - 3.56 18.0 - - - -\nReach Prob Den 12 sat 4.26 271 20.1 0.90 - 124 0.50 - - - -\nReach Prob Den 13 sat 0.62 3.46 4.15 0.58 0.07 17.3 0.70 - - - -\nReach Prob Den 14 sat 0.57 0.97 1.39 0.42 - 0.03 0.35 - - - -\nReach Prob Den 15 unsat 1.07 64.1 35.5 1.53 - 85.0 - - - - -\nReach Prob Den 16 sat 1.63 - 43.1 0.38 - 255 0.71 - - - -\nReach Prob Den 17 unsat 1.02 41.8 27.3 1.46 - 53.5 - - - - -\nReach Prob Den 18 unsat 0.98 38.7 26.1 1.36 - 51.1 - - - - -\nReach Prob Den 19 unsat 0.98 40.7 27.4 1.44 - 56.9 - - - - -\nReach Prob Den 20 unsat 0.98 39.5 27.3 1.41 - 56.2 - - - - -\nReach Prob Den 21 sat 1.48 1.00 4.91 0.80 - 23.4 0.75 - - - -\nReach Prob Den 22 sat 5.34 393 65.5 1.57 - 234 0.55 - - - -\nReach Prob Den 23 sat 3.11 - 45.0 1.29 - 187 0.65 - - - -\nReach Prob Den 24 unsat 7.04 1.98 2.40 11.2 - - 30.5 - - - -\nReach Prob Den 25 sat 0.45 0.45 0.80 0.34 0.07 29.5 0.20 - - - -\nReach Prob Den 26 sat 0.55 0.37 0.34 0.22 <0.01 0.02 0.20 0.07 - - -\n46\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nReach Prob Den 27 unsat 2.06 0.60 1.12 1.19 - 115 14.9 - - - -\nReach Prob Den 28 unsat 5.17 1.20 1.46 5.10 - - 22.9 - - - -\nReach Prob Den 29 unsat 4.14 1.42 1.37 3.54 - - 21.8 - - - -\nReach Prob Den 30 sat 0.58 0.43 0.83 0.23 - 231 0.21 - - - -\nReach Prob Den 31 unsat 2.10 0.60 1.03 0.73 - 12.9 6.02 - - - -\nReach Prob Den 32 unsat 2.06 0.87 1.07 1.38 - 121 11.3 - - - -\nReach Prob Den 33 sat 0.45 0.32 0.82 1.46 0.05 19.2 0.20 0.07 - - -\nReach Prob Den 34 unsat 4.74 0.72 1.44 7.26 - - 24.4 - - - -\nReach Prob Den 35 unsat 2.77 0.73 1.13 1.78 - 311 8.17 - - - -\nRl Benchmarks 0 unsat 0.29 0.25 0.07 0.04 0.28 0.23 0.10 0.06 - - 4.18\nRl Benchmarks 1 unsat 0.32 0.50 0.11 0.03 - 0.23 0.06 0.04 - - 3.83\nRl Benchmarks 2 unsat 0.32 0.61 0.07 0.01 - 0.22 0.06 0.04 - - -\nRl Benchmarks 3 unsat 0.32 0.42 0.08 <0.01 - 0.22 0.05 0.05 - - 10.0\nRl Benchmarks 4 unsat 0.31 0.43 0.05 0.01 - 0.23 0.06 <0.01 - - -\nRl Benchmarks 5 unsat 0.32 0.43 0.09 <0.01 - 0.22 0.06 0.02 - - 44.4\nRl Benchmarks 6 unsat 0.34 0.22 0.07 0.01 - 0.23 0.05 0.02 - - 16.7\nRl Benchmarks 7 unsat 0.32 0.20 0.05 0.02 - 0.23 0.06 0.03 - - 8.65\nRl Benchmarks 8 unsat 0.32 0.35 0.09 <0.01 - 0.24 0.06 0.04 - - -\nRl Benchmarks 9 unsat 0.32 0.18 0.11 0.04 - 0.09 0.06 0.05 - - 2.64\nRl Benchmarks 10 unsat 0.34 0.20 0.12 <0.01 - 0.22 0.05 0.08 - - -\nRl Benchmarks 11 unsat 0.40 0.24 0.05 0.04 - 0.21 0.05 0.08 - - 15.2\nRl Benchmarks 12 unsat 0.31 0.23 <0.01 0.01 - 0.22 0.06 0.06 - - 6.04\nRl Benchmarks 13 unsat 0.34 0.18 0.09 0.04 - 0.22 0.05 0.08 - - 76.0\nRl Benchmarks 14 unsat 0.32 0.25 0.09 0.02 - 0.08 0.05 0.07 - - 6.91\nRl Benchmarks 15 unsat 0.33 0.25 0.09 0.01 - 0.06 0.05 0.03 - - 2.19\nRl Benchmarks 16 unsat 0.33 0.17 0.09 0.03 - 0.23 0.05 0.04 - - 2.09\nRl Benchmarks 17 unsat 0.32 0.28 0.09 0.02 - 0.07 0.05 0.03 - - 2.12\nRl Benchmarks 18 unsat 0.34 0.16 0.06 0.02 - 0.22 0.06 0.03 - - 36.6\nRl Benchmarks 19 unsat 0.31 0.30 0.11 <0.01 - 0.23 0.05 0.03 - - 65.7\nRl Benchmarks 20 unsat 0.33 0.26 0.08 0.04 - 0.22 0.05 0.05 - - 6.32\nRl Benchmarks 21 unsat 0.31 0.24 0.11 0.01 - 0.23 0.05 0.03 - - 36.0\nRl Benchmarks 22 unsat 0.30 0.21 0.07 0.02 - 0.24 0.05 0.05 - - 62.7\nRl Benchmarks 23 unsat 0.31 0.22 0.06 0.02 - 0.24 0.06 0.11 - - 8.60\nRl Benchmarks 24 unsat 0.30 0.28 0.06 <0.01 - 0.24 0.05 0.06 - - 11.1\nRl Benchmarks 25 unsat 0.30 0.18 0.06 0.01 - 0.07 0.05 0.06 - - 2.13\nRl Benchmarks 26 unsat 0.30 0.29 0.08 <0.01 - 0.07 0.05 0.05 - - 1.29\nRl Benchmarks 27 unsat 0.31 0.17 0.10 0.01 - 0.23 0.05 0.07 - - 0.78\nRl Benchmarks 28 unsat 0.29 0.29 0.13 0.05 - 0.22 0.05 0.05 - - 7.38\nRl Benchmarks 29 sat 0.66 0.18 0.11 0.20 0.02 1.63 0.21 0.05 - - 3.66\nRl Benchmarks 30 unsat 0.29 0.19 0.05 <0.01 - 0.06 0.05 0.03 - - 8.16\nRl Benchmarks 31 unsat 0.31 0.25 0.08 0.01 - 0.06 0.05 0.02 - - 3.37\nRl Benchmarks 32 unsat 0.29 0.05 0.09 0.02 - 0.22 0.05 0.04 - - 4.15\nRl Benchmarks 33 unsat 0.32 0.21 0.08 <0.01 - 0.06 0.05 0.02 - - 3.82\nRl Benchmarks 34 unsat 0.33 0.17 0.08 <0.01 - 0.21 0.06 0.09 - - 2.65\nRl Benchmarks 35 unsat 0.33 0.25 0.08 0.02 - 0.07 0.05 0.08 - - 2.59\nRl Benchmarks 36 sat 0.34 0.19 0.09 0.19 0.04 0.07 0.20 0.06 - 2.85 5.35\nRl Benchmarks 37 unsat 0.33 0.36 0.07 <0.01 - 0.07 0.11 0.05 - - 9.75\nRl Benchmarks 38 unsat 0.33 0.22 0.12 <0.01 - 0.23 0.05 0.07 - - 2.53\nRl Benchmarks 39 unsat 0.33 0.18 0.09 0.05 - 0.07 0.06 0.06 - - 5.97\nRl Benchmarks 40 unsat 0.32 0.30 0.12 0.02 - 0.21 0.14 0.01 - - 11.0\nRl Benchmarks 41 unsat 0.35 0.07 0.08 <0.01 - 0.22 0.05 0.02 - - 3.44\nRl Benchmarks 42 sat 0.34 0.26 0.08 0.19 <0.01 0.06 0.20 0.03 - 8.58 6.20\nRl Benchmarks 43 unsat 0.34 0.20 0.05 <0.01 - 0.23 0.05 0.03 - - 7.34\nRl Benchmarks 44 sat 0.33 0.29 0.07 0.19 <0.01 0.06 0.20 0.04 - 8.64 8.13\nRl Benchmarks 45 unsat 0.35 0.22 0.10 0.04 - 0.08 0.05 0.05 - - 5.76\nRl Benchmarks 46 unsat 0.34 0.19 0.07 0.02 - 0.06 0.05 0.04 - - 6.57\nRl Benchmarks 47 unsat 0.35 0.26 0.05 0.01 - 0.07 0.05 0.04 - - 12.0\nRl Benchmarks 48 unsat 0.36 0.23 0.09 0.01 - 0.07 0.05 0.05 - - 4.39\nRl Benchmarks 49 unsat 0.34 0.24 0.12 <0.01 0.37 0.07 0.05 0.07 - - 4.43\nRl Benchmarks 50 unsat 0.33 0.59 0.07 0.03 - 0.07 0.06 0.05 - - 2.68\nRl Benchmarks 51 unsat 0.32 0.02 0.07 <0.01 - 0.06 0.06 0.03 - - 2.61\nRl Benchmarks 52 unsat 0.32<0.01 0.08 <0.01 - 0.07 0.05 0.07 - - 2.55\nRl Benchmarks 53 unsat 0.32 0.14 0.11 <0.01 - 0.22 0.05 0.02 - - 6.48\nRl Benchmarks 54 unsat 0.31 0.14 0.12 <0.01 - 0.09 0.05 0.03 - - 2.61\nRl Benchmarks 55 unsat 0.29 0.16 0.07 0.04 - 0.07 0.05 0.02 - - 17.4\nRl Benchmarks 56 unsat 0.29 0.23 0.10 0.02 - 0.08 0.05 0.03 - - 2.09\nRl Benchmarks 57 unsat 0.31 0.11 0.07 <0.01 - 0.06 0.05 0.06 - - 0.07\nRl Benchmarks 58 unsat 0.30 0.25 0.10 0.01 - 0.07 0.05 0.02 - - 0.22\n47\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nRl Benchmarks 59 unsat 0.28 0.20 0.09 <0.01 - 0.06 0.05 0.04 - - <0.01\nRl Benchmarks 60 unsat 0.32 0.23 0.12 0.05 - 0.06 0.05 0.08 - - 0.15\nRl Benchmarks 61 unsat 0.30 0.23 0.06 <0.01 - 0.08 0.06 0.05 - - 0.59\nRl Benchmarks 62 unsat 0.32 0.28 0.05 0.01 - 0.08 0.05 0.05 - - 0.05\nRl Benchmarks 63 unsat 0.29 0.14 0.08 <0.01 0.27 0.07 0.05 0.07 - - 0.95\nRl Benchmarks 64 unsat 0.31 0.93 0.09 <0.01 - 0.23 0.10 0.06 - - 0.74\nRl Benchmarks 65 unsat 0.30 0.13 0.05 <0.01 0.29 0.06 0.05 0.06 - - 0.07\nRl Benchmarks 66 unsat 0.31 0.09 0.06 <0.01 - 0.06 0.10 0.03 - - 0.13\nRl Benchmarks 67 unsat 0.31 0.02 0.08 0.02 - 0.07 0.06 0.03 - - 2.79\nRl Benchmarks 68 unsat 0.31 0.09 0.04 0.04 - 0.06 0.05 0.03 - - 0.04\nRl Benchmarks 69 unsat 0.32 0.06 0.06 0.01 - 0.07 0.05 0.03 - - 2.32\nRl Benchmarks 70 unsat 0.33 0.18 0.14 0.01 - 0.08 0.05 0.02 - - 6.18\nRl Benchmarks 71 unsat 0.34 1.34 0.06 <0.01 - 0.06 0.05 0.02 - - 1.73\nRl Benchmarks 72 unsat 0.31 0.23 0.08 <0.01 - 0.08 0.06 0.04 - - 6.44\nRl Benchmarks 73 unsat 0.31 0.30 0.08 0.01 - 0.07 0.05 <0.01 - - <0.01\nRl Benchmarks 74 unsat 0.30 0.20 0.10 <0.01 - 0.10 0.05 0.07 - - 0.04\nRl Benchmarks 75 unsat 0.31 0.18 0.06 0.01 - 0.07 0.05 0.03 - - 1.83\nRl Benchmarks 76 unsat 0.35 0.36 0.07 <0.01 - 0.22 0.05 0.06 - - 2.16\nRl Benchmarks 77 unsat 0.32 0.16 0.08 0.04 - 0.08 0.05 0.07 - - 1.34\nRl Benchmarks 78 unsat 0.35 0.24 0.08 0.01 - 0.08 0.06 0.05 - - 6.04\nRl Benchmarks 79 unsat 0.34 0.20 0.08 0.01 - 0.07 0.05 0.02 - - 0.17\nRl Benchmarks 80 unsat 0.31 0.32 0.08 <0.01 - 0.07 0.05 0.07 - - 0.71\nRl Benchmarks 81 unsat 0.34 0.11 0.09 <0.01 - 0.23 0.05 0.07 - - 33.9\nRl Benchmarks 82 unsat 0.31 0.20 0.07 <0.01 - 0.07 0.05 0.04 - - 3.09\nRl Benchmarks 83 unsat 0.32 0.26 0.09 <0.01 - 0.07 0.05 0.06 - - 2.56\nRl Benchmarks 84 unsat 0.31 0.23 0.10 <0.01 - 0.08 0.05 <0.01 - - 3.57\nRl Benchmarks 85 unsat 0.31 0.22 0.06 <0.01 - 0.07 0.05 0.02 - - 4.45\nRl Benchmarks 86 unsat 0.28 0.16 0.08 0.01 - 0.06 0.05 0.04 - - 3.66\nRl Benchmarks 87 unsat 0.33 0.14 0.13 0.01 - 0.11 0.05 0.03 - - 5.14\nRl Benchmarks 88 unsat 0.31 0.12 0.06 0.01 - 0.08 0.05 0.03 - - 2.69\nRl Benchmarks 89 unsat 0.29 0.15 0.07 <0.01 - 0.08 0.05 0.05 - - 2.64\nRl Benchmarks 90 unsat 0.28 0.15 0.06 <0.01 - 0.07 0.05 0.07 - - 3.56\nRl Benchmarks 91 unsat 0.30 0.28 0.09 <0.01 - 0.06 0.05 0.04 - - 2.60\nRl Benchmarks 92 unsat 0.31 0.14 0.08 0.01 - 0.07 0.05 0.05 - - 11.7\nRl Benchmarks 93 unsat 0.30 0.13 0.08 <0.01 0.29 0.08 0.05 0.06 - - 4.69\nRl Benchmarks 94 unsat 0.31 0.17 0.10 <0.01 - 0.07 0.05 0.03 - - 6.55\nRl Benchmarks 95 unsat 0.28 0.13 0.03 <0.01 - 0.07 0.05 0.02 - - 4.95\nRl Benchmarks 96 unsat 0.29 0.12 0.08 0.01 - 0.09 0.05 0.02 - - 7.76\nRl Benchmarks 97 unsat 0.31 0.12 0.11 <0.01 - 0.08 0.05 0.03 - - 2.58\nRl Benchmarks 98 unsat 0.31 0.11 0.08 <0.01 - 0.23 0.05 0.03 - - 17.9\nRl Benchmarks 99 sat 0.34 0.01 0.11 0.19 0.09 0.07 0.20 0.03 - 2.88 1.67\nRl Benchmarks 100 sat 0.32 0.15 0.10 0.19 - 0.06 0.21 0.02 - - -\nRl Benchmarks 101 sat 0.35 0.17 0.10 0.19 - 0.07 0.21 0.06 - - -\nRl Benchmarks 102 sat 0.36 0.25 0.04 0.18 - 0.08 0.21 0.03 - - -\nRl Benchmarks 103 sat 0.35 0.18 0.08 0.22 - 0.06 0.21 0.07 - - -\nRl Benchmarks 104 sat 0.32 0.13 0.13 0.20 - 0.07 0.26 0.06 - - -\nRl Benchmarks 105 sat 0.33 0.20 0.08 0.19 - 0.07 0.21 0.08 - - -\nRl Benchmarks 106 sat 0.32 0.14 0.08 0.23 - 0.07 0.21 0.05 - - -\nRl Benchmarks 107 sat 0.33 0.19 0.07 0.20 - 0.08 0.21 0.06 - - -\nRl Benchmarks 108 sat 0.35 0.13 0.09 0.19 - 0.07 0.21 0.05 - - -\nRl Benchmarks 109 sat 0.34 0.16 0.07 0.21 - 0.06 0.21 0.03 - - -\nRl Benchmarks 110 sat 0.33 0.22 0.06 0.19 - 0.08 0.21 0.05 - - -\nRl Benchmarks 111 sat 0.32 0.04 0.10 0.18 - 0.09 0.21 0.08 - - -\nRl Benchmarks 112 unsat 0.74 4.13 0.65 0.39 - 1.28 1.56 3.99 - - -\nRl Benchmarks 113 sat 0.35 0.11 0.09 0.19 - 0.06 0.21 0.05 - - -\nRl Benchmarks 114 sat 0.34 0.30 0.10 0.22 - 0.07 0.26 0.05 - - -\nRl Benchmarks 115 sat 0.33 0.15 0.07 0.19 - 0.07 0.21 0.04 - - -\nRl Benchmarks 116 sat 0.32 0.24 0.07 0.22 - 0.06 0.26 0.05 - - -\nRl Benchmarks 117 unsat 0.78 3.90 0.62 0.25 - 1.53 0.71 1.48 - - -\nRl Benchmarks 118 sat 0.32 0.24 0.08 0.24 - 0.08 0.21 0.06 - - -\nRl Benchmarks 119 unsat 0.31 0.22 0.04 <0.01 - 0.22 0.11 0.06 - - -\nRl Benchmarks 120 sat 0.33 0.16 0.13 0.19 - 0.07 0.21 0.04 - - -\nRl Benchmarks 121 sat 0.32 0.20 0.15 0.22 - 0.07 0.21 0.03 - - -\nRl Benchmarks 122 sat 0.29 0.98 0.11 0.18 - 0.07 0.21 0.05 - - -\nRl Benchmarks 123 sat 0.30 0.15 0.03 0.20 - 0.07 0.20 0.05 - - -\nRl Benchmarks 124 sat 0.30 0.27 0.04 0.19 - 0.06 0.21 0.03 - - -\nRl Benchmarks 125 sat 0.30 0.12 0.04 0.19 - 0.08 0.21 0.05 - - -\nRl Benchmarks 126 sat 0.33 0.33 0.07 0.21 - 0.07 0.21 0.03 - - -\n48\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nRl Benchmarks 127 sat 0.31 0.26 0.08 0.19 - 0.08 0.21 0.04 - - -\nRl Benchmarks 128 sat 0.31 0.14 0.05 0.18 - 0.07 0.21 0.04 - - -\nRl Benchmarks 129 sat 0.31 0.15 0.06 0.19 - 0.06 0.21 0.03 - - -\nRl Benchmarks 130 sat 0.32 0.16 0.09 0.20 - 0.07 0.21 0.04 - - -\nRl Benchmarks 131 sat 0.32 0.24 0.10 0.20 - 0.07 0.21 0.06 - - -\nRl Benchmarks 132 sat 0.33 0.26 0.08 0.20 - 0.07 0.21 0.06 - - -\nRl Benchmarks 133 sat 0.33 0.30 0.11 0.19 - 0.08 0.21 0.06 - - -\nRl Benchmarks 134 sat 0.32 0.13 0.05 0.21 - 0.08 0.20 0.05 - - -\nRl Benchmarks 135 sat 0.33 0.13 0.05 0.19 - 0.07 0.21 0.07 - - -\nRl Benchmarks 136 sat 0.35 0.27 0.07 0.21 - 0.07 0.21 0.06 - - -\nRl Benchmarks 137 sat 0.34 0.26 0.13 0.19 - 0.08 0.26 0.01 - - -\nRl Benchmarks 138 sat 0.31 0.20 0.05 0.23 - 0.06 0.21 0.03 - - -\nRl Benchmarks 139 sat 0.34 0.24 0.08 0.22 - 0.08 0.21 0.04 - - -\nRl Benchmarks 140 sat 0.34 0.26 0.06 0.18 - 0.07 0.21 0.02 - - -\nRl Benchmarks 141 sat 0.34 0.26 0.14 0.20 - 0.09 0.21 0.03 - - -\nRl Benchmarks 142 sat 0.34 0.14 0.10 0.22 - 0.08 0.21 0.08 - - -\nRl Benchmarks 143 sat 0.32 0.15 0.08 0.18 - 0.09 0.21 0.03 - - -\nRl Benchmarks 144 sat 0.35 0.49 0.12 0.19 - 0.07 0.21 0.08 - - -\nRl Benchmarks 145 sat 0.34 0.24 0.07 0.18 - 0.07 0.21 0.05 - - -\nRl Benchmarks 146 sat 0.36 0.10 0.05 0.18 - 0.08 0.21 0.07 - - -\nRl Benchmarks 147 sat 0.34 0.35 0.09 0.24 - 0.08 0.21 0.13 - - 84.6\nRl Benchmarks 148 sat 0.33 0.16 0.03 0.19 - 0.06 0.21 0.10 - - -\nRl Benchmarks 149 sat 0.34 0.13 0.14 0.19 - 0.08 0.21 0.08 - - -\nRl Benchmarks 150 sat 0.33 0.22 0.10 0.19 - 0.07 0.21 0.20 - - -\nRl Benchmarks 151 sat 0.33 0.20 0.07 0.25 - 0.08 0.21 0.05 - - -\nRl Benchmarks 152 unsat 0.31 0.15 0.09 0.01 - 0.08 0.06 0.05 - - -\nRl Benchmarks 153 sat 0.32 0.20 0.08 0.20 - 0.06 0.21 0.10 - - -\nRl Benchmarks 154 unsat 0.32 0.21 0.08 0.01 - 0.08 0.11 0.03 - - -\nRl Benchmarks 155 sat 0.32 0.23 0.10 0.22 - 0.07 0.22 0.07 - - -\nRl Benchmarks 156 sat 0.32 0.21 0.10 0.22 - 0.08 0.26 0.06 - - -\nRl Benchmarks 157 sat 0.31 1.64 0.08 0.22 - 0.09 0.21 0.04 - - -\nRl Benchmarks 158 sat 0.29 0.27 0.11 0.25 - 0.07 0.22 0.07 - - -\nRl Benchmarks 159 sat 0.32 0.17 0.06 0.22 - 0.07 0.25 0.12 - - -\nRl Benchmarks 160 sat 0.31 0.26 0.09 0.21 - 0.08 0.26 0.10 - - -\nRl Benchmarks 161 sat 0.30 0.26 0.10 0.21 - 0.06 0.21 0.09 - - -\nRl Benchmarks 162 sat 0.31 0.38 0.07 0.20 - 0.08 0.21 0.07 - - -\nRl Benchmarks 163 sat 0.29 0.38 0.08 0.20 - 0.07 0.21 0.07 - - -\nRl Benchmarks 164 unsat 0.29 0.43 0.11 0.01 - 0.07 0.11 0.07 - - -\nRl Benchmarks 165 sat 0.30 0.46 0.09 0.20 - 0.08 0.21 0.07 - - -\nRl Benchmarks 166 sat 0.31 0.38 0.14 0.22 - 0.06 0.21 0.07 - - -\nRl Benchmarks 167 unsat 0.31 0.40 0.05 0.02 - 0.11 0.11 0.05 - - -\nRl Benchmarks 168 sat 0.35 0.41 0.07 0.22 - 0.11 0.21 0.04 - - -\nRl Benchmarks 169 sat 0.34 0.39 0.08 0.22 - 0.08 0.21 0.03 - - -\nRl Benchmarks 170 sat 0.34 0.38 0.10 0.20 - 0.06 0.21 0.06 - - -\nRl Benchmarks 171 sat 0.31 0.28 0.06 0.19 - 0.06 0.21 0.07 - - -\nRl Benchmarks 172 unsat 0.34 0.31 0.14 0.02 - 0.06 0.05 0.09 - - -\nRl Benchmarks 173 sat 0.33 0.18 0.08 0.22 - 0.08 0.21 0.05 - - -\nRl Benchmarks 174 unsat 0.33 0.19 0.09 0.02 - 0.23 0.11 0.22 - - -\nRl Benchmarks 175 unsat 0.34 0.14 0.06 0.02 - 0.08 0.05 0.06 - - -\nRl Benchmarks 176 sat 0.33 0.24 0.05 0.23 - 0.08 0.21 0.06 - - -\nRl Benchmarks 177 sat 0.34 0.13 0.10 0.19 - 0.07 0.21 0.04 - - -\nRl Benchmarks 178 unsat 0.34 0.14 0.05 0.01 - 0.09 0.11 0.05 - - -\nRl Benchmarks 179 unsat 0.33 0.29 0.09 0.04 - 0.08 0.06 0.03 - - 86.1\nRl Benchmarks 180 unsat 0.33 0.14 0.08 <0.01 - 0.08 0.05 0.05 - - -\nRl Benchmarks 181 sat 0.33 0.18 0.09 0.20 - 0.06 0.21 0.03 - - -\nRl Benchmarks 182 unsat 0.30 0.15 0.10 0.02 - 0.09 0.11 0.06 - - -\nRl Benchmarks 183 unsat 0.31 0.26 0.08 0.01 - 0.09 0.06 0.07 - - -\nRl Benchmarks 184 sat 0.33 0.26 0.08 0.19 - 0.07 0.21 0.05 - - -\nRl Benchmarks 185 sat 0.35 0.32 0.08 0.19 - 0.07 0.20 0.06 - - -\nRl Benchmarks 186 sat 0.30 0.25 0.12 0.23 - 0.07 0.21 0.08 - - 57.4\nRl Benchmarks 187 sat 0.31 0.15 0.03 0.23 - 0.08 0.21 0.05 - - 70.5\nRl Benchmarks 188 sat 0.29 0.10 0.10 0.23 - 0.07 0.21 0.06 - - -\nRl Benchmarks 189 unsat 0.29 0.12 0.07 0.01 - 0.06 0.06 0.04 - - -\nRl Benchmarks 190 unsat 0.32 0.17 0.04 0.02 - 0.06 0.06 0.04 - - -\nRl Benchmarks 191 unsat 0.29 0.15 0.07 <0.01 - 0.09 0.06 0.02 - - -\nRl Benchmarks 192 unsat 0.29 0.11 0.07 0.01 - 0.08 0.05 0.02 - - -\nRl Benchmarks 193 sat 0.30 0.18 0.07 0.21 - 0.08 0.21 0.02 - - -\nRl Benchmarks 194 unsat 0.29 0.32 0.08 0.02 - 0.08 0.10 0.07 - - -\n49\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nRl Benchmarks 195 sat 0.32 0.21 0.07 0.20 - 0.12 0.21 0.06 - - -\nRl Benchmarks 196 sat 0.33 0.14 0.09 0.20 - 0.08 0.21 0.08 - - -\nRl Benchmarks 197 sat 0.32 0.25 0.10 0.21 - 0.06 0.21 0.06 - - -\nRl Benchmarks 198 sat 0.31 0.15 0.11 0.19 - 0.06 0.21 0.05 - - -\nRl Benchmarks 199 sat 0.66 0.25 0.08 0.21 0.04 0.07 0.21 0.06 - - -\nRl Benchmarks 200 unsat 0.15 0.25 0.09 0.76 - 0.52 0.31 12.0 - - -\nRl Benchmarks 201 unsat 0.16 0.32 0.08 1.06 - 0.52 0.31 95.5 - - -\nRl Benchmarks 202 unsat 0.17 0.42 0.08 0.99 - 0.54 0.31 - - - -\nRl Benchmarks 203 unsat 0.20 0.52 0.08 0.66 - 0.50 0.31 - - - -\nRl Benchmarks 204 unsat 0.17 0.41 0.10 0.70 - 0.50 0.31 - - - -\nRl Benchmarks 205 unsat 0.16 0.34 0.08 0.02 - 0.49 0.31 0.07 - - -\nRl Benchmarks 206 unsat 0.15 0.52 0.04 0.78 - 0.48 0.31 12.0 - - -\nRl Benchmarks 207 unsat 0.15 0.46 0.10 0.01 - 0.50 0.31 53.6 - - -\nRl Benchmarks 208 unsat 0.18 0.35 0.35 1.23 - 0.87 1.41 - - - -\nRl Benchmarks 209 unsat 0.17 0.47 0.10 0.71 - 0.53 1.06 - - - -\nRl Benchmarks 210 unsat 0.16 0.34 0.09 0.02 1.53 5.32 1.06 - - - -\nRl Benchmarks 211 unsat 0.15 0.35 0.10 <0.01 - 0.51 0.31 0.30 - - -\nRl Benchmarks 212 unsat 0.15 0.36 0.09 0.01 - 0.50 0.31 - - - -\nRl Benchmarks 213 unsat 0.16 0.35 0.11 0.01 1.41 5.22 0.91 0.25 - - -\nRl Benchmarks 214 unsat 0.17 0.41 0.08 0.02 - 0.48 0.31 - - - -\nRl Benchmarks 215 unsat 0.13 0.42 0.11 0.01 1.31 4.93 1.16 - - - -\nRl Benchmarks 216 unsat 0.15 0.44 0.07 <0.01 1.52 5.46 0.91 1.59 - - -\nRl Benchmarks 217 unsat 0.14 0.47 0.06 0.02 1.48 5.39 1.06 - - - -\nRl Benchmarks 218 unsat 0.50 0.39 0.50 2.55 - 1.29 5.67 - - - -\nRl Benchmarks 219 unsat 0.15 0.35 0.10 0.01 - 0.48 0.31 0.08 - - -\nRl Benchmarks 220 unsat 0.15 0.47 0.10 0.01 - 0.50 0.31 0.06 - - -\nRl Benchmarks 221 unsat 0.49 0.49 0.65 5.49 - 1.80 6.18 - - - -\nRl Benchmarks 222 unsat 0.15 0.47 0.08 0.02 - 0.46 0.31 0.05 - - -\nRl Benchmarks 223 unsat 0.14 0.26 0.05 <0.01 - 0.46 0.31 0.04 - - -\nRl Benchmarks 224 unsat 0.13 0.39 0.10 0.01 - 0.50 0.31 - - - -\nRl Benchmarks 225 unsat 0.65 2.11 1.43 - - 3.51 - - - - -\nRl Benchmarks 226 unsat 0.15 0.39 0.33 0.58 - 0.53 1.22 - - - -\nRl Benchmarks 227 unsat 0.19 0.50 0.05 0.01 - 0.47 0.31 0.06 - - -\nRl Benchmarks 228 unsat 0.18 0.49 0.33 0.94 - 1.16 2.16 - - - -\nRl Benchmarks 229 unsat 0.17 0.54 0.31 1.14 - 0.83 1.46 - - - -\nRl Benchmarks 230 unsat 0.15 0.33 0.10 0.02 - 0.45 0.31 0.07 - - -\nRl Benchmarks 231 unsat 0.17 0.35 0.07 0.01 - 0.49 0.31 - - - -\nRl Benchmarks 232 unsat 0.17 0.39 0.10 0.02 - 0.48 0.31 0.06 - - -\nRl Benchmarks 233 unsat 0.15 0.40 0.10 <0.01 - 0.50 0.31 2.32 - - -\nRl Benchmarks 234 unsat 0.17 0.36 0.07 0.02 - 0.46 0.31 0.11 - - -\nRl Benchmarks 235 unsat 0.16 0.38 0.06 0.01 - 0.51 0.31 0.49 - - -\nRl Benchmarks 236 unsat 0.17 0.42 0.10 <0.01 - 0.50 0.31 3.68 - - -\nRl Benchmarks 237 unsat 0.17 0.34 0.09 <0.01 - 0.51 0.31 0.04 - - -\nRl Benchmarks 238 unsat 0.15 0.35 0.09 <0.01 - 0.46 0.31 0.09 - - -\nRl Benchmarks 239 unsat 0.17 0.28 0.09 0.29 - 0.48 0.31 - - - -\nRl Benchmarks 240 unsat 0.15 0.22 0.04 0.01 - 0.46 0.31 0.04 - - -\nRl Benchmarks 241 unsat 0.15 0.13 0.08 0.01 - 0.49 0.31 0.34 - - -\nRl Benchmarks 242 unsat 0.14 0.21 0.07 0.59 - 0.48 0.91 13.4 - - -\nRl Benchmarks 243 unsat 0.51 0.28 0.33 1.85 - 0.51 0.96 - - - -\nRl Benchmarks 244 unsat 0.16 0.20 0.07 0.51 - 0.50 1.01 - - - -\nRl Benchmarks 245 unsat 0.13 0.22 0.10 0.01 - 0.47 0.31 1.39 - - -\nRl Benchmarks 246 unsat 0.15 0.20 0.09 0.03 - 0.46 0.31 0.09 - - -\nRl Benchmarks 247 unsat 0.13 0.28 0.10 0.20 - 0.48 0.31 0.30 - - -\nRl Benchmarks 248 unsat 0.13 0.27 0.07 0.40 - 0.47 0.96 - - - -\nRl Benchmarks 249 unsat 0.15 0.12 0.07 0.25 - 0.50 0.31 - - - -\nRl Benchmarks 250 unsat 0.15 0.34 0.09 0.53 - 0.49 0.91 - - - -\nRl Benchmarks 251 unsat 0.15 0.19 0.12 0.02 - 0.49 0.31 0.05 - - -\nRl Benchmarks 252 unsat 0.14 0.26 0.30 0.72 - 1.04 1.51 - - - -\nRl Benchmarks 253 unsat 0.68 2.50 1.57 - - 5.52 - - - - -\nRl Benchmarks 254 unsat 0.16 0.14 0.11 0.01 - 0.48 0.31 0.04 - - -\nRl Benchmarks 255 unsat 0.68 0.75 1.28 1.94 7 4.47 8.78 - - - -\nRl Benchmarks 256 unsat 0.17 0.30 0.10 0.02 - 0.47 0.31 0.06 - - -\nRl Benchmarks 257 unsat 0.52 0.45 0.42 2.96 - 1.72 3.27 - - - -\nRl Benchmarks 258 unsat 0.17 0.30 0.07 0.01 - 0.49 0.31 - - - -\nRl Benchmarks 259 sat 0.53 0.34 0.13 0.21 0.08 0.14 1.06 0.09 - - -\nRl Benchmarks 260 unsat 0.17 0.30 0.11 0.02 - 0.50 0.31 0.10 - - -\nRl Benchmarks 261 sat 0.19 0.30 0.12 0.25 0.01 0.12 0.96 0.18 - - -\nRl Benchmarks 262 unsat 0.51 0.38 0.28 0.73 - 1.01 2.62 - - - -\n50\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nRl Benchmarks 263 unsat 0.53 0.49 0.54 4.68 - 1.70 5.42 - - - -\nRl Benchmarks 264 unsat 0.16 0.73 0.32 0.81 - 0.78 1.46 - - - -\nRl Benchmarks 265 unsat 0.16 0.32 0.07 0.02 - 0.47 0.31 0.33 - - -\nRl Benchmarks 266 unsat 0.66 2.89 1.30 2.55 7 7.22 14.1 - - - -\nRl Benchmarks 267 sat 0.52 0.40 0.09 0.41 0.06 6.71 1.01 - - - -\nRl Benchmarks 268 sat 0.52 5.75 0.09 2.97 0.07 8.29 1.01 0.31 - - -\nRl Benchmarks 269 sat 0.18 0.41 0.10 0.21 0.06 0.15 0.96 0.52 - - -\nRl Benchmarks 270 unsat 0.16 0.38 0.09 0.01 - 0.47 0.31 - - - -\nRl Benchmarks 271 sat 0.52 0.28 0.09 0.25 0.07 5.54 0.96 0.63 - - -\nRl Benchmarks 272 unsat 0.58 1.11 0.86 2.23 - 2.32 12.0 - - - -\nRl Benchmarks 273 sat 0.48 0.34 0.12 3.41 0.10 6.06 1.06 - - - -\nRl Benchmarks 274 unsat 0.12 0.34 0.07 0.20 - 0.49 0.31 0.88 - - -\nRl Benchmarks 275 sat 0.48 0.39 0.15 0.23 0.06 0.14 0.96 0.68 - - -\nRl Benchmarks 276 unsat 0.50 0.32 0.10 0.20 - 0.48 0.81 - - - -\nRl Benchmarks 277 unsat 0.16 0.37 0.09 <0.01 - 0.45 0.31 0.08 - - -\nRl Benchmarks 278 sat 0.49 0.38 0.11 0.25 0.06 0.13 0.96 1.65 - - -\nRl Benchmarks 279 unsat 0.14 0.17 0.15 0.20 7 0.49 0.91 - - - -\nRl Benchmarks 280 sat 0.14 0.24 0.09 0.19 0.04 0.13 0.91 0.08 - - -\nRl Benchmarks 281 unsat 0.16 0.27 0.11 <0.01 - 0.47 0.31 0.06 - - -\nRl Benchmarks 282 sat 0.16 0.20 0.12 0.21 0.02 0.14 0.91 0.07 - - -\nRl Benchmarks 283 unsat 0.14 0.32 0.06 <0.01 - 0.45 0.31 0.06 - - -\nRl Benchmarks 284 sat 0.51 0.26 0.11 0.21 0.07 6.97 0.91 0.15 - - -\nRl Benchmarks 285 unsat 0.53 0.30 0.24 0.20 - 1.49 2.11 - - - -\nRl Benchmarks 286 unsat 0.17 0.34 0.07 0.21 - 0.47 0.91 - - - -\nRl Benchmarks 287 sat 0.52 0.14 0.08 0.26 0.07 2.46 0.91 0.14 - - -\nRl Benchmarks 288 sat 0.18 0.38 0.10 0.20 0.02 0.13 0.91 0.06 - - -\nRl Benchmarks 289 sat 0.50 0.12 0.11 0.20 0.11 5.96 0.86 - - - -\nRl Benchmarks 290 unsat 0.15 0.17 0.07 <0.01 - 0.48 0.31 - - - -\nRl Benchmarks 291 unsat 0.16 0.24 0.10 0.02 - 0.46 0.31 0.05 - - -\nRl Benchmarks 292 unsat 0.15 0.11 0.06 <0.01 - 0.46 0.31 0.05 - - -\nRl Benchmarks 293 sat 0.51 0.19 0.10 0.22 0.08 0.15 0.91 - - - -\nRl Benchmarks 294 sat 0.19 0.22 0.09 0.23 <0.01 0.17 0.91 0.15 - - -\nRl Benchmarks 295 sat 0.20 0.28 0.10 0.20 <0.01 0.17 0.86 0.15 - - -\nSri Resnet A 0 sat 2.98 0.47 4.33 - 6.96 - - - - - -\nSri Resnet A 1 ? - - - - - - - - - - -\nSri Resnet A 2 sat 3.40 0.64 4.51 - - - - - - - -\nSri Resnet A 3 ? - - - - - - - - - - -\nSri Resnet A 4 ? - - - - - - - - - - -\nSri Resnet A 5 ? - - - - - - - - - - -\nSri Resnet A 6 unsat - - - - 26.8 - - - - - -\nSri Resnet A 7 unsat - - - - 26.6 - - - - - -\nSri Resnet A 8 ? - - - - - - - - - - -\nSri Resnet A 9 unsat - - - - 26.9 - - - - - -\nSri Resnet A 10 unsat - - - - 27.3 - - - - - -\nSri Resnet A 11 sat 3.39 0.89 4.73 - - - - - - - -\nSri Resnet A 12 ? - - - - - - - - - - -\nSri Resnet A 13 sat 2.66 0.95 4.47 - - - - - - - -\nSri Resnet A 14 unsat - - - - 27.0 - - - - - -\nSri Resnet A 15 ? - - - - - - - - - - -\nSri Resnet A 16 unsat 2.62 0.67 4.29 - 27.5 - - - - - -\nSri Resnet A 17 unsat - - - - 26.9 - - - - - -\nSri Resnet A 18 sat 3.40 0.70 4.35 - 6.59 - - - - - -\nSri Resnet A 19 unsat 8.42 18.5 6.12 - - - - - - - -\nSri Resnet A 20 unsat - - - - 27.2 - - - - - -\nSri Resnet A 21 unsat 2.60 0.66 4.27 - 27.1 - - - - - -\nSri Resnet A 22 ? - - - - - - - - - - -\nSri Resnet A 23 sat 3.41 0.73 4.66 - - - - - - - -\nSri Resnet A 24 sat 3.41 0.72 4.72 - 5.99 - - - - - -\nSri Resnet A 25 unsat 19.5 126 - - 27.1 - - - - - -\nSri Resnet A 26 ? - - - - - - - - - - -\nSri Resnet A 27 ? - - - - - - - - - - -\nSri Resnet A 28 sat 3.42 0.71 4.75 - 5.86 - - - - - -\nSri Resnet A 29 ? - - - - - - - - - - -\nSri Resnet A 30 unsat 22.2 121 - - 26.2 - - - - - -\nSri Resnet A 31 sat 3.40 0.63 4.37 - 6.23 - - - - - -\nSri Resnet A 32 ? - - - - - - - - - - -\nSri Resnet A 33 ? - - - - - - - - - - -\nSri Resnet A 34 ? - - - - - - - - - - -\n51\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nSri Resnet A 35 ? - - - - - - - - - - -\nSri Resnet A 36 ? - - - - - - - - - - -\nSri Resnet A 37 ? - - - - - - - - - - -\nSri Resnet A 38 unsat - - - - 26.9 - - - - - -\nSri Resnet A 39 ? - - - - - - - - - - -\nSri Resnet A 40 ? - - - - - - - - - - -\nSri Resnet A 41 ? - - - - - - - - - - -\nSri Resnet A 42 unsat 2.64 0.45 4.29 - - - - - - - -\nSri Resnet A 43 unsat 2.64 0.37 4.25 - 27.3 - - - - - -\nSri Resnet A 44 unsat 7.84 5.15 4.32 - 27.2 - - - - - -\nSri Resnet A 45 unsat 2.30 0.37 4.23 - 27.3 - - - - - -\nSri Resnet A 46 unsat 54.5 238 - - - - - - - - -\nSri Resnet A 47 sat 3.44 0.53 4.89 - - - - - - - -\nSri Resnet A 48 unsat 19.6 58.9 35.8 - 26.7 - - - - - -\nSri Resnet A 49 unsat 2.62 0.40 4.34 - 27.9 - - - - - -\nSri Resnet A 50 ? - - - - - - - - - - -\nSri Resnet A 51 sat 3.46 0.64 4.85 - - - - - - - -\nSri Resnet A 52 unsat - - - - 27.3 - - - - - -\nSri Resnet A 53 unsat 20.0 32.0 - - 27.5 - - - - - -\nSri Resnet A 54 ? - - - - - - - - - - -\nSri Resnet A 55 unsat 27.2 130 - - 26.8 - - - - - -\nSri Resnet A 56 unsat 21.5 36.0 - - - - - - - - -\nSri Resnet A 57 ? - - - - - - - - - - -\nSri Resnet A 58 unsat - - - - 27.2 - - - - - -\nSri Resnet A 59 unsat 127 - - - 26.8 - - - - - -\nSri Resnet A 60 ? - - - - - - - - - - -\nSri Resnet A 61 ? - - - - - - - - - - -\nSri Resnet A 62 unsat 7.76 4.86 4.28 - 27.1 - - - - - -\nSri Resnet A 63 unsat 7.77 5.02 4.42 - 27.2 - - - - - -\nSri Resnet A 64 unsat - - - - 27.2 - - - - - -\nSri Resnet A 65 unsat 136 - - - - - - - - - -\nSri Resnet A 66 ? - - - - - - - - - - -\nSri Resnet A 67 ? - - - - - - - - - - -\nSri Resnet A 68 ? - - - - - - - - - - -\nSri Resnet A 69 unsat 2.63 0.42 4.31 - - - - - - - -\nSri Resnet A 70 unsat - - - - 27.1 - - - - - -\nSri Resnet A 71 sat 2.99 0.74 4.44 - 5.77 - - - - - -\nSri Resnet B 0 sat 2.68 0.45 4.42 - 6.14 - - - - - -\nSri Resnet B 1 unsat - - - - 27.9 - - - - - -\nSri Resnet B 2 sat 3.40 0.51 4.64 - 6.37 - - - - - -\nSri Resnet B 3 ? - - - - - - - - - - -\nSri Resnet B 4 unsat 20.2 107 - - - - - - - - -\nSri Resnet B 5 unsat 51.2 152 - - 26.7 - - - - - -\nSri Resnet B 6 unsat 46.3 - - - 27.2 - - - - - -\nSri Resnet B 7 unsat 20.7 144 - - 26.8 - - - - - -\nSri Resnet B 8 ? - - - - - - - - - - -\nSri Resnet B 9 sat 3.44 0.48 4.41 - 6.64 - - - - - -\nSri Resnet B 10 ? - - - - - - - - - - -\nSri Resnet B 11 ? - - - - - - - - - - -\nSri Resnet B 12 unsat 8.29 6.43 4.97 - 26.9 - - - - - -\nSri Resnet B 13 ? - - - - - - - - - - -\nSri Resnet B 14 sat 3.44 0.57 4.89 - - - - - - - -\nSri Resnet B 15 unsat 2.70 0.41 4.30 - - - - - - - -\nSri Resnet B 16 unsat - - - - 27.1 - - - - - -\nSri Resnet B 17 ? - - - - - - - - - - -\nSri Resnet B 18 unsat 19.4 23.8 20.5 - - - - - - - -\nSri Resnet B 19 unsat - - - - 27.0 - - - - - -\nSri Resnet B 20 unsat 2.66 0.36 4.28 - 27.2 - - - - - -\nSri Resnet B 21 sat 3.42 0.48 5.31 - 6.09 - - - - - -\nSri Resnet B 22 ? - - - - - - - - - - -\nSri Resnet B 23 sat 3.47 0.46 5.06 - 6.36 - - - - - -\nSri Resnet B 24 unsat 19.5 30.9 12.9 - 27.1 - - - - - -\nSri Resnet B 25 ? - - - - - - - - - - -\nSri Resnet B 26 sat 3.42 0.49 4.62 - 6.28 - - - - - -\nSri Resnet B 27 sat 3.06 0.45 4.41 - 5.06 - - - - - -\nSri Resnet B 28 unsat - - - - 27.8 - - - - - -\nSri Resnet B 29 unsat 2.64 0.41 4.26 - - - - - - - -\nSri Resnet B 30 unsat 19.7 53.7 25.8 - - - - - - - -\n52\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nSri Resnet B 31 ? - - - - - - - - - - -\nSri Resnet B 32 ? - - - - - - - - - - -\nSri Resnet B 33 ? - - - - - - - - - - -\nSri Resnet B 34 unsat 21.5 160 - - - - - - - - -\nSri Resnet B 35 ? - - - - - - - - - - -\nSri Resnet B 36 ? - - - - - - - - - - -\nSri Resnet B 37 ? - - - - - - - - - - -\nSri Resnet B 38 unsat - - - - 27.0 - - - - - -\nSri Resnet B 39 ? - - - - - - - - - - -\nSri Resnet B 40 unsat 2.69 0.49 4.31 - - - - - - - -\nSri Resnet B 41 unsat 2.65 0.24 4.30 - 27.0 - - - - - -\nSri Resnet B 42 unsat 19.9 32.0 25.5 - - - - - - - -\nSri Resnet B 43 unsat 2.31 0.33 4.34 - 26.9 - - - - - -\nSri Resnet B 44 unsat - - - - 27.2 - - - - - -\nSri Resnet B 45 unsat 8.15 5.98 4.50 - 26.9 - - - - - -\nSri Resnet B 46 unsat 2.64 0.44 4.36 - 27.3 - - - - - -\nSri Resnet B 47 ? - - - - - - - - - - -\nSri Resnet B 48 sat 3.01 0.51 4.50 - 6.10 - - - - - -\nSri Resnet B 49 ? - - - - - - - - - - -\nSri Resnet B 50 unsat 2.33 0.39 4.32 - - - - - - - -\nSri Resnet B 51 unsat 19.7 17.0 5.81 - 27.0 - - - - - -\nSri Resnet B 52 unsat 2.68 0.44 4.26 - 28.1 - - - - - -\nSri Resnet B 53 unsat 24.9 189 - - - - - - - - -\nSri Resnet B 54 unsat - - - - 27.2 - - - - - -\nSri Resnet B 55 ? - - - - - - - - - - -\nSri Resnet B 56 ? - - - - - - - - - - -\nSri Resnet B 57 sat 3.43 0.44 4.94 - 5.72 - - - - - -\nSri Resnet B 58 unsat 2.66 0.38 4.29 - - - - - - - -\nSri Resnet B 59 unsat 2.69 0.43 4.38 - 28.0 - - - - - -\nSri Resnet B 60 unsat 22.9 37.7 - - - - - - - - -\nSri Resnet B 61 unsat 22.7 132 - - 26.8 - - - - - -\nSri Resnet B 62 ? - - - - - - - - - - -\nSri Resnet B 63 ? - - - - - - - - - - -\nSri Resnet B 64 ? - - - - - - - - - - -\nSri Resnet B 65 unsat 2.68 0.45 4.33 - - - - - - - -\nSri Resnet B 66 ? - - - - - - - - - - -\nSri Resnet B 67 ? - - - - - - - - - - -\nSri Resnet B 68 ? - - - - - - - - - - -\nSri Resnet B 69 unsat 2.63 0.46 4.34 - 27.2 - - - - - -\nSri Resnet B 70 ? - - - - - - - - - - -\nSri Resnet B 71 sat 3.45 0.49 4.71 - 5.78 - - - - - -\nTllverifybench 0 sat 0.12 0.31 2.75 0.23 - 0.09 0.50 0.08 <0.01 - -\nTllverifybench 1 sat 0.14 0.32 4.31 0.24 0.07 0.11 3.11 1.75 <0.01 - -\nTllverifybench 2 sat 0.13 0.31 2.81 0.24 - 0.09 0.46 0.08 <0.01 - -\nTllverifybench 3 unsat 1.16 0.72 7.32 - - 2.90 41.1 - <0.01 - -\nTllverifybench 4 unsat 1.19 0.54 7.27 1.91 - 2.18 13.7 - 0.02 - -\nTllverifybench 5 sat 0.17 0.44 2.94 0.29 0.07 0.30 4.96 2.08 0.02 - -\nTllverifybench 6 unsat 1.35 1.86 9.81 - - 8.58 - - <0.01 - -\nTllverifybench 7 sat 0.46 0.34 8.64 0.29 0.43 0.33 10.1 0.08 0.06 - -\nTllverifybench 8 unsat 1.61 2.00 11.3 - - 8.81 55.6 - 0.02 - -\nTllverifybench 9 sat 0.27 0.39 2.94 0.45 - 0.97 29.3 0.19 <0.01 - -\nTllverifybench 10 unsat 1.63 2.61 18.6 - - 8.53 69.6 - <0.01 - -\nTllverifybench 11 sat 0.33 0.36 2.82 0.45 - 0.97 29.8 0.18 0.03 - -\nTllverifybench 12 unsat 3.56 13.4 22.1 - - 76.8 - - <0.01 - -\nTllverifybench 13 sat 0.65 0.47 2.89 0.93 0.19 2.59 182 0.35 0.03 - -\nTllverifybench 14 sat 0.69 0.44 2.95 0.93 - 2.72 94.8 0.38 0.02 - -\nTllverifybench 15 sat 0.68 0.40 2.89 0.91 - 2.70 96.7 0.40 0.01 - -\nTllverifybench 16 sat 1.59 0.56 3.15 1.90 - 5.96 253 0.87 0.03 - -\nTllverifybench 17 sat 1.47 0.49 3.23 2.27 0.41 6.01 243 0.97 <0.01 - -\nTllverifybench 18 unsat 5.86 28.0 45.5 - - 44.4 - - 0.03 - -\nTllverifybench 19 unsat 5.54 26.8 59.0 - - 52.1 - - 0.10 - -\nTllverifybench 20 unsat 6.92 22.9 84.1 - - 96.2 - - 0.08 - -\nTllverifybench 21 sat 3.03 0.70 30.2 5.07 0.71 11.9 522 - <0.01 - -\nTllverifybench 22 sat 2.93 0.73 3.70 4.67 - 11.8 525 1.92 0.03 - -\nTllverifybench 23 sat 2.91 0.71 3.63 5.02 - 11.8 506 2.02 <0.01 - -\nTllverifybench 24 sat 5.68 1.06 131 10.9 1.43 20.9 - - <0.01 - -\nTllverifybench 25 sat 5.64 0.96 4.33 10.9 - 20.9 - 4.11 0.04 - -\nTllverifybench 26 sat 5.35 0.82 4.41 11.0 - 21.0 - 4.19 0.02 - -\n53\n\nVNN-COMP 2022 Report M. M uller, C. Brix, S. Bak, C. Liu, T. Johnson\nTable 27: Instance Runtimes. Fastest times are blue. Second fastest are green. Penalties are red crosses ( 7).\nCategory Id Result \u000b,\f-C MnB Verin nnen CGD Pereg Marab Debon FastBaT Verap Averi\nTllverifybench 27 sat 5.68 0.80 156 10.7 1.45 21.0 - 4.06 0.15 - -\nTllverifybench 28 unsat 17.3 43.7 447 - 7 265 - - 0.10 - -\nTllverifybench 29 sat 8.99 1.39 6.61 21.0 - 36.3 - 8.24 0.02 - -\nTllverifybench 30 sat 9.88 1.31 6.64 21.1 2.31 35.5 - 7.72 0.22 - -\nTllverifybench 31 unsat 24.6 94.9 595 - - - - - 0.03 - -\nVggnet16 2022 0 unsat 7.79 1.70 11.0 31.0 - - - - - - -\nVggnet16 2022 1 unsat 7.78 1.77 62.1 27.6 - - - - - - -\nVggnet16 2022 2 unsat 7.83 353 742 45.9 7 - - - - - -\nVggnet16 2022 3 unsat 7.73 3.51 13.5 30.4 7 - - - - - -\nVggnet16 2022 4 unsat 72.1 - - 983 - - - - - - -\nVggnet16 2022 5 unsat 14.3 - - 197 - - - - - - -\nVggnet16 2022 6 unsat 7.76 5.45 16.3 32.6 7 - - - - - -\nVggnet16 2022 7 sat 7.20 8.65 17.3 29.4 644 - - - - - -\nVggnet16 2022 8 unsat 164 - - - 7 - - - - - -\nVggnet16 2022 9 unsat 18.9 - - 171 - - - - - - -\nVggnet16 2022 10 unsat 7.87 - - 61.8 - - - - - - -\nVggnet16 2022 11 unsat 54.2 - - 936 - - - - - - -\nVggnet16 2022 12 unsat 18.5 - - 145 - - - - - - -\nVggnet16 2022 13 unsat 1003 - - - - - - - - - -\nVggnet16 2022 14 unsat 857 - - - - - - - - - -\nVggnet16 2022 15 sat - - - - 642 - - - - - -\nVggnet16 2022 16 ? - - - - - - - - - - -\nVggnet16 2022 17 ? - - - - - - - - - - -\n54",
  "textLength": 151795
}