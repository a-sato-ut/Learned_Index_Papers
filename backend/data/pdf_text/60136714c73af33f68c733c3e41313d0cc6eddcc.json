{
  "paperId": "60136714c73af33f68c733c3e41313d0cc6eddcc",
  "title": "Learning-Augmented Frequency Estimation in Sliding Windows",
  "pdfPath": "60136714c73af33f68c733c3e41313d0cc6eddcc.pdf",
  "text": "Learning-Augmented Frequency Estimation in Sliding Windows\nRana Shahout1, Ibrahim Sabek2, and Michael Mitzenmacher1\n1Harvard University, USA\n2University of Southern California, USA\nAbstract\nWe show how to utilize machine learning approaches to improve sliding window algorithms for approximate\nfrequency estimation problems, under the ‚Äúalgorithms with predictions‚Äù framework. In this dynamic environment,\nprevious learning-augmented algorithms are less effective, since properties in sliding window resolution can differ\nsignificantly from the properties of the entire stream. Our focus is on the benefits of predicting and filtering out\nitems with large next arrival times ‚Äì that is, there is a large gap until their next appearance ‚Äì from the stream,\nwhich we show improves the memory-accuracy tradeoffs significantly. We provide theorems that provide insight\ninto how and by how much our technique can improve the sliding window algorithm, as well as experimental\nresults using real-world data sets. Our work demonstrates that predictors can be useful in the challenging sliding\nwindow setting.\n1 Introduction\nStream processing plays a crucial role in various applications, such as network monitoring, intrusion detection\nsystems, and sensor networks. Dealing with large and rapidly incoming data streams presents challenges in providing\naccurate responses to queries due to high computational and memory costs. Furthermore, these applications require\ntime-efficient algorithms to cope with high-speed data streams. To that end, stream processing algorithms often\nbuild compact approximate sketches (synopses) of the input streams.\nEstimating the frequency of certain items in a data stream is a fundamental step in data analysis. Several\nalgorithms, such as Count-Min Sketch [8], Count Sketch [7] have been proposed to estimate item frequencies in the\ndata streams.\nAs time passes, newer data often becomes more relevant than older data, necessitating an aging mechanism\nfor the sketches. In financial analysis, for instance, analysts prioritize current market trends, whereas in intrusion\ndetection systems, recent intrusion attempts are of primary concern. In both cases, outdated information loses\nsignificance over time. Retaining old data not only consumes valuable memory resources but also introduces noise,\ncomplicating the analysis of recent, relevant data. Many applications realize this by tracking the stream‚Äôs items over\na sliding window. The sliding window model [9] considers only a window of the most recent items in the stream,\nwhile older ones do not affect the quantity we wish to estimate. Indeed, standard approaches to the problem of\nmaintaining different types of sliding window statistics have been extensively studied [2,5,9,17,21].\nRecently, machine learning has been combined with traditional algorithms, yielding the paradigm of learning-\naugmented algorithms, also known as algorithms with predictions. This approach aims to improve algorithms by\nleveraging advice from machine learning models in the form of predictions. In the context of streaming algorithms,\nHsu et al [12] introduced learning-based frequency estimation, where machine learning is utilized to predict the\nmost frequent items, known as ‚Äôheavy hitters,‚Äô with the goal of reducing estimation errors. They utilized known\nhashing-based algorithms such as Count-Sketch and Count-Min Sketch, which approximately count item frequencies\nby hashing items to buckets. The learning process ensured that predicted heavy hitter items were not placed in\nthe sketch but assigned dedicated counters, facilitating accurate counting. In addition, [22] described a learned-\nbased algorithm for identifying heavy hitters, top k, and flow frequency estimation. They focus on the Space\nSaving algorithm and exclude predicted low-frequency items from updating the Space Saving, while ensuring that\npredicted heavy-hitter items remain in the structure. In this way, the Space Saving accuracy is improved.\nHowever, these approaches do not directly translate to the sliding window model, where different items can\nbecome heavy hitters or have low frequency locally within a window as time passes. In other words, an item that\nis a heavy hitter for the entire stream may not be a heavy hitter in any particular data window. Moreover, to thearXiv:2409.11516v1  [cs.DS]  17 Sep 2024\n\nbest of our knowledge, predictions have not been applied to the problem of approximate counting in the sliding\nwindow setting. In general, sliding window variations of approximate counting and other problems are generally\nmore challenging than variations without sliding windows, so perhaps this is unsurprising.\nIn this work, we aim to demonstrate the applicability of predictions to sliding window algorithms for frequency\nestimation problems, focusing on how natural predictions of the gap between item arrivals can lead to easy-to-\nimplement improvements in this setting.\nWe focus on Window Compact Space-Saving (WCSS) [5] and present the learned-based version, LWCSS. Our\nprimary idea is to exclude items that appear only once in the sliding window from being included in the data\nstructure used for tracking item frequencies, inspired by LSS [22].\nHowever, LSS employs predictors that relate to the entire stream, which cannot be directly applied to a sliding\nwindow. An item with low frequency in the entire stream might not be low-frequency within a particular window,\npotentially leading to unbounded estimation errors.\nOur key insight is the potential to exclude items that appear once by employing an effective predictor for each\nitem‚Äôs next arrival time in the stream. With a perfectly accurate predictor, we could identify single-occurrence\nitems within the sliding window by determining whether they have not appeared previously in the stream, and\nhave a predicted next appearance time that exceeds the current window size. Predictors of next arrivals have been\nemployed in for example caching with machine learning advice [18].\nWe treat the next arrival predictor as a black box and do not delve into its internal functioning; our approach can\ntherefore be utilized with any suitable learning scheme that produces a predictor. We analyze both theoretically and\nempirically the performance gains offered by predictors, as well as the robustness of our approach to predictor errors.\nWe also explore potential future directions, such as developing a predictor capable of learning item frequencies\nwithin a given time frame and periodically retraining itself using transfer learning techniques to adapt to shifts\nin the data distribution across frames. Furthermore, we discuss the prospect of extending our approach to handle\nother types of queries in the sliding window setting.\n2 Problem Definition\nGiven a universe U, astream S=u1, u2, . . .‚àà UNis a sequence of arrivals from the universe. (We assume the\nstream is finite here.) We denote by fW\nithe frequency of item iover the last Warrivals.\nWe seek algorithms that support the following operations:\n‚Ä¢ADD( i): given an element i‚àà U, append itoS.\n‚Ä¢Query( i): return an estimatedfW\nioffW\ni\nDefinition 1. An algorithm solves (W, Œµ)-WFrequency for if given any Query (i)it returnsdfW\nisatisfying\nfW\ni‚â§dfW\ni‚â§fW\ni+WŒµ.\n3 Sliding Window Algorithms\nSliding window algorithms have two conceptual steps: removing outdated data and recording incoming data. These\nalgorithms aim to avoid storing the entire window sequence, as the window size is typically large. Due to such\nmemory limitations, approximation algorithms are desirable and often employed. Such algorithms typically do\nnot maintain individual counters for each item, but instead employ an approximate counting algorithm to monitor\nitem frequencies.\nIn this work, we choose Window Compact Space Saving (WCSS) [5] as our underlying algorithm1. WCSS\nsolves ( W, Œµ)-WFrequency when the queries are frequency queries. It divides the stream into frames of size Wand\neach frame is divided into kequal-sized blocks as illustrated in Figure 1. The query window also has size Wand,\nimportantly, it overlaps with no more than 2 frames at any given time.\nWCSS counts how many times each item arrived during the last frame. When the counter exceeds a multiple of\nthe block size (W\nk,2W\nk, etc), WCSS identifies it as an overflow. The algorithm keeps track of the item ID associated\nwith each overflow, and selectively keeps only overflowed items for past blocks. To identify these overflowed items,\nWCSS uses an instance of the Space Saving algorithm (SS) [19] which is re-initialized as empty at the beginning of\n1Our approach could be applied to other sliding window streaming algorithms that follow the same setup of dividing the stream\ninto frames (e.g. [4,10])\n\nùüè2‚Ä¶ùíåEach block is of size ùëæ/ùíåThe current ùëæ-sized windowThe stream is divided into ùëæ-sized framesEach frame is partitioned into ùíåblocksFigure 1: WCSS algorithm overview (adapting a figure in [5])\neach frame. In addition, WCSS employs specific data structures to keep track of the overflowed item IDs, denoted\nbyoverflowsRecord .\nIn WCSS, overflowsRecord contains a collection of up to k+ 1 queues. Each queue corresponds to a block\nthat overlaps with the current window. Within each queue, WCSS stores the IDs of items that have overflowed\nin the corresponding block. Whenever an item overflows, WCSS appends its ID to the current block‚Äôs queue.\nWhen a block ends, WCSS removes the oldest queue from the collection. In order to estimate the frequency of an\nitem within a given window, WCSS counts the overflow occurrences of the item and multiplies the result byW\nk.\nSimilar setups are found in works that support other queries over sliding windows, such as in [4]. In this work,\noverflowsRecord contains a hierarchical tree structure consisting of frequency tables. These tables store overflowed\nitem IDs along with their respective frequencies within block intervals.\n4 Learned Sliding Window Algorithm\n4.1 Overview\nGiven the specific characteristics of the sliding window setup, we want a predictor that offers a time-sensitive\napproach, in contrast to a predictor for whether an item is a heavy hitter over the stream, which is time-insensitive.\nWe employ an oracle that, for a specific item iand timestamp t, predicts the next occurrence of i, or equivalently\nhow many arrivals will occur before item iappears again. A detailed description of this predictor, outlining its\nstructure and functionality, will be provided later in this section.\nBased on this prediction, LWCSS excludes items predicted to next appear again beyond the frame size from\nbeing inserted in the SS instance. These items are referred to as non-essential items . As the SS instance is reset at\nthe beginning of each frame, we obtain a ‚Äúcleaner‚Äù SS instance, as proposed in [22]. Note that the non-essential\nitems are not consistently ignored in every occurrence; rather we consider each arrival of the item and take an\nappropriate action based on the predictor. Note that if we had perfect predictions for non-essential items, then\nnot including them in the Space Saving instance allows us to minimize memory overhead with no cost in terms\nof accuracy.\nHowever, machine learning methodologies are inherently imperfect, and they may exhibit errors, including\nsubstantial and unexpected errors. Therefore, as suggested in the algorithms with predictions literature (see,\ne.g., [20]), algorithms based on machine learning predictions must demonstrate sufficient robustness to handle\nerrors that may occur. In particular, the notion of robustness that has become common is that the performance\nof algorithms using predictions should not be significantly inferior to that of conventional online algorithms that\ndo not rely on predictions, even if predictions are inaccurate. Unless some mitigating structure is added, ignoring\nitems predicted as non-essential can lead to unbounded errors. For example, if a heavy hitter of the current frame\nis predicted incorrectly as a non-essential item, this item is excluded from the sliding window algorithm, violating\nthe error guarantee (Definition 1).\nWe therefore apply the idea of keeping a Bloom filter of predicted non-essential items previously ignored during\nthe current frame to ensure robustness, suggested by [22]. The Bloom filter, like the Space Saving instance, is\nflushed at the beginning of each frame.\nThere are two sources of estimation error when some arrivals of item iare predicted to exceed the window size.\nThe first source of error arises when item arrivals are predicted incorrectly to exceed the window size. Ideally, these\narrivals should have been captured by the Bloom filter and inserted back into the SS. However, when we insert\nitem iinto the Bloom filter for the first arrival that exceeds the window size, we do not add it to the SS. Thus, if\nthe first arrival is mispredicted, we may miss one insertion (otherwise, it is correct not to insert this arrival into\nthe SS). Second, the Bloom filter flushing at the beginning of a new frame causes an underestimation. Since the\nqueried intervals can overlap with at most two frames, this imposes an underestimation error of no more than 1.\n\nùíäùíäùíèùíÜùíôùíïùë®ùíìùíìùíäùíóùíÇùíç(ùíä)ùíèùíÜùíôùíïùë®ùíìùíìùíäùíóùíÇùíç>ùëæ?Figure 2: Next arrival prediction in sliding window setting.\nFigure 3: Average single items ratio vs. frame size using real-world traces (described in Section 5).\nTo compensate for these two sources of error occurring together, we add 2 to the query.\nImportantly, because the predictor is concerned with the item‚Äôs next arrival, the number of single occurrence\nitems within the frame exceeds the number of single occurrence items across the entire stream and depends on the\nframe size as shown in Figure 3.\n4.2 Next occurrence oracle\nIn what follows, we use a slightly different approach that allows us to utilize a simpler predictor and still obtain\nstrong performance. Rather than try to predict the next arrival exactly, we predict whether the next arrival is\nlarger than Wor not. If the next arrival is larger than W, then it clearly lies outside the frame (Figure 2). This\nallows us to have our predictor perform a binary classification, rather than solve a regression problem.\nTo construct the next occurrence oracle, we trained a neural network to predict if an item will show up again\nwithin a specified period of time (the next Witems in the stream). By converting the problem into a two-category\nclassification problem, we simplify the prediction problem. The oracle is thus tasked to classify whether the next\nappearance of the item falls within our specified window size, tagged as 1, or exceeds this limit, indicated as 0.\nOur model is a Recurrent Neural Network (RNN) that utilizes Long Short-Term Memory (LSTM) cell. We chose\nthis framework for its effectiveness in processing sequential data as demonstrated in prior research [15]; however,\nwe emphasize that other predictors could be used.\nFor our test cases, we focus on networking problems, and items are source-destination IP address pairs. The\nnetwork begins by transforming the source and destination IP addresses into indices, which are then converted into\nembedding vectors. As a result of this process, IP addresses are represented in a dense and expressive manner. Our\nmodel uses these embeddings as input to the LSTM layer. In the LSTM layer, the sequence of IP address embeddings\nis processed, in order to capture temporal dependencies. After the LSTM produces the final hidden states, the\nresults are passed to a fully-connected (dense) layer that outputs one value. This output value, after being passed\nthrough a sigmoid activation function, represents the model‚Äôs prediction of whether the next appearance of an item\nfalls within the specified window size Wor not. This prediction is generated based on the sequence of IP addresses\nprovided as input. The model is trained using Binary Cross-Entropy with Logits Loss (BCEWithLogitsLoss)\nand the Adam optimizer. After training, the model‚Äôs performance is evaluated using the F1 score on a separate\ntest dataset. Again, the implementation of such a predictor is our tailored approach, and it is just one of many\npossible options. Depending on the requirements, our design may be replaced or enhanced with any other effective\nprediction technique.\n\nTable 1: Traces from [11] used in the evaluation\nTrace #Items #Uniques\nChicago 88,529,637 1,650,097\nNY 63,284,829 2,968,038\nSJ 188,511,031 2,922,904\n4.3 Robustness Result\nConsider a sliding window algorithm Athat follows the same setup of dividing the stream into frames. The WCSS\nalgorithm is an example of such an algorithm. In the following, we prove the robustness of our approach in the\ngeneral case using algorithm A. We show that our proposed algorithm is robust, in the sense that it cannot perform\nsignificantly worse than the corresponding algorithm that does not use predictions.\nTheorem 1. LetAbe an algorithm for (W, Œµ‚àí2\nW)-WFrequency. Then LWCSS solves (W, Œµ)-WFrequency.\nProof. For any item iand any window of size W, we may underestimate the count of item iby at most two. This\nis because we may undercount an item once when it is predicted not to occur within the next Wsteps and it is not\nalready a positive from the Bloom filter. As each window of length Wcan intersect two frames, and the Bloom\nfilter is reset to empty at frame boundaries, we can undercount an item at most twice over any Wconsecutive\nsteps.\nWe have\ndQW\ni‚âúA(W,Œµ‚àí2\nW).Query (i) + 2. (1)\nThat is, our estimate is the query result for ifromA, which is an algorithm for ( W, Œµ‚àí2\nW)-WFrequency, with at\nmost 2 instances of iremoved from the stream Aprocesses and an extra count of 2 added back in. It follows the\nsmallest possible return value is\n(QW\ni‚àí2) + 2 = QW\ni,\nand the largest possible return value is\n\u0012\nQW\ni+\u0012\nŒµ‚àí2\nW\u0013\nW\u0013\n+ 2 = QW\ni+ŒµW,\nproving the claim.\n5 Evaluation\nIn this section, we present an empirical study and compare WCSS and LWCSS.\nExperimental Setup. We implemented WCSS and LWCSS in Python 3.7.6. The evaluation was performed\non an AMD EPYC 7313 16-Core Processor with an NVIDIA A100 80GB PCIe GPU, running Ubuntu 20.04.6 LTS\nwith Linux kernel 5.4.0-172-generic, and TensorFlow 2.4.1. We extend WCSS by incorporating a predictor, which\nis built using an LSTM network, and compare it against traditional WCSS using real-world datasets.\nDatasets. Our real datasets comprise CAIDA Internet Traces [11] obtained from multiple sources: (1) equinix-\nchicago 2016 high-speed monitor is located at an Equinix datacenter in Chicago and is connected to a backbone\nlink of a Tier 1 ISP between Chicago, IL and Seattle, WA (2) equinix-sanjose 2014 monitor (SJ) is positioned at\nan Equinix datacenter in San Jose, CA, connected to a backbone link of a Tier 1 ISP between San Jose, CA and\nLos Angeles, CA (3) The equinix-nyc 2018 monitor is located at an Equinix datacenter in New York. These traces\nare summarized in Table 1.\nFor the synthetic data, we considered the Zipf distribution where the items are selected from a bounded universe,\nand the frequencies of an item with rank Ris given by f(R, Œ±) =constant\nRŒ±, where Œ±is a skewness parameter. For\npredictions, we employed a synthetic predictor that computes the true count for each item, and then adds an error\nwhich is drawn i.i.d. from a normal distribution with mean parameter 0 and standard deviation œÉ= 1.\nMetrics. In the approximation error evaluation, we use Root Mean Square Error (RMSE). When an item\nis encountered, we query its frequency estimation immediately and calculate the RMSE. For performance of\noperations, we use updates or queries per second.\n\nTable 2: Predictor Accuracy\nMetric Value\nF1 Score (Chicago) 81.3%\nF1 Score (NY) 87.3%\nF1 Score (SJ) 83.5%\n(a)Chicago\n (b)NY\n (c)SJ\n (d)Synthetic\nFigure 4: Accuracy comparison of WCSS and LWCSS vs. Memory (Megabytes) using real datasets (Chicago, NY\nand SJ)\n5.1 Accuracy Comparison\nFigure 4 shows the accuracy (RMSE) as a function of memory for WCSS and LWCSS setting W= 210. This\nevaluation includes three real datasets: Chicago, NY, and SJ, and utilizes a pre-trained LSTM predictor. The\npredictor‚Äôs performance on each dataset is summarized in Table 2. The results demonstrate that LWCSS outperforms\nWCSS in terms of accuracy across all three datasets since the filtering of single-occurrence items enhances the SS‚Äôs\naccuracy which is crucial for overall accuracy. Figure 4 demonstratea the feasibility of our approach using the next\narrival predictor.\nFigure 5a shows the RMSE as a function of window size using the Chicago dataset. As seen, as the window\nsize increases, the RMSE also increases. Additionally, the performance improves as the window is smaller because\nthe ratio of single items becomes larger, as shown in Figure 3.\n5.2 Performance Comparison\nFigures 5b and 5c show the performance of WCSS and LWCSS in terms of the update and query time respectively\nusing the Chicago dataset and setting W= 213. The query performance of the two algorithms appears closely\naligned due to their consistent operations. The update performance of LWCSS is slightly worse than WCSS due\nto the additional operation of inserting single occurence items into the Bloom filter.\n6 Future Work and Extensions\n6.1 An alternative approach: frequency prediction within time frames\nA potential direction for future work is to explore an alternative approach that learns item frequencies within each\ngiven time frame. By predicting the frequencies of each item in the frame, we can apply previous work introduced\nby [12] to the sketch that tracks item frequencies within each frame. However, a challenge with this approach is\nthat we need to retrain the predictor in each frame. One way to overcome this computational inefficiency in a\nstreaming setting is: Instead of training a new predictor for each frame, we could employ a single predictor and\nretrain it periodically to adapt to shifts in the data distribution across frames.\nSpecifically, we can perform the retraining process after observing items for Mframes. The value of Mwill\nbe determined based on the extent of accuracy drop in the predictor; smaller values of Mshould be set if the\npredictor incurs a higher accuracy drop. This periodic retraining aims to strike a balance between maintaining\n\n(b)Query Performance\n (c)Update Performance\nFigure 5: Query and update performance using Chicago dataset and setting W= 213.\nprediction accuracy and minimizing computational overhead. To enhance the efficiency of the retraining process,\nwe can utilize recent advances in transfer learning within deep learning [6]. In particular, we can avoid retraining\nthe model from scratch and shorten the training cycle by selectively retraining only specific layers in the model\nusing information from recent frames. The main intuition behind this approach is that in many neural networks,\nthe initial layers capture general features, while the later layers focus on specific features that are more dependent\non the problem at hand (in our case, these later layers are sensitive to distribution shifts). During the retraining\nprocess, we can freeze all hidden layers in the LSTM and fully-connected networks and then selectively retrain only\nthose layers connected to the input and output of each network. As long as the dimensions of the frozen layers\nremain constant at any point in time, it is acceptable to reuse them. By preserving the learned general features\nand selectively updating the task-specific layers, we can leverage the knowledge gained from previous frames while\nefficiently adapting to the current data distribution.\nAs a future direction, we will compare this frame-based frequency prediction approach with our existing method\nusing the next arrival predictor. This analysis could provide insights into the trade-offs between the two approaches\nand guide the selection of the appropriate technique based on the specific window size.\n6.2 General learned sliding window framework\nStream processing encompasses three fundamental tasks: membership queries, frequency queries, and heavy hitter\nqueries. Membership queries determine whether a given item is present within the sliding window. Frequency\nqueries report the occurrence count of a specific item. Heavy hitter queries identify all items whose frequencies\nexceed a predetermined threshold. Our research focuses on frequency queries over data streams within a sliding\nwindow setting. We recognized that the work proposed by [10] introduced a generic framework, termed Sliding\nSketches, which can be applied to existing solutions for the above three tasks, enabling them to support queries\nover sliding windows. This work employed a similar approach of dividing the stream into frames (referred to as\nsegments in their work). Consequently, our approach could intuitively be applied to this general framework.\n7 Related Work\nThe problem of estimating item frequencies over sliding windows was first studied in [3]. To estimate frequency\nwithin a WŒµadditive error over a window of size W, their algorithm requires O(Œµ‚àí1log2Œµ‚àí1logW) bits of memory.\nThis memory requirement was later optimized to O(Œµ‚àí1logW) bits as highlighted in [16]. Hung and Ting in [13]\nfurther refined this by improving the update time to a constant and locating all heavy hitters in the optimal O(Œµ‚àí1)\ntime. The WCSS algorithm, as introduced in [5], also provides frequency estimates in constant time.\nAlgorithms with predictions is, as we have stated, a rapidly growing area. The site [23] contains a collection\nof over a hundred papers on the topic. The idea of using predictions to specifically improve frequency estimation\nalgorithms appears to have originated with [12], where they augmented a learning oracle of the heavy hitters into\nfrequency estimation sketch-based algorithms. Later [14] and [22] explored the power of such an oracle, showing\nthat it can be applied to a wide array of problems in data streams. All these papers use neural networks to learn\ncertain properties of the input. We, however, differ from those papers because we consider the sliding window\nsetup, in which properties derived from window resolution can differ significantly from those derived from the\nentire stream, and therefore, other predictions are required.\n\n8 Conclusion\nWe have presented a novel method to improve sliding window algorithms for approximate frequency estimation by\nincorporating a learning model that filters out low-frequency ‚Äúnoisy‚Äù items. Past research on learning-augmented\nalgorithms does not perform well in the sliding window settings due to variations between the properties of the\nsliding window resolution and the entire data stream.\nWe have demonstrated the benefits of our design both analytically and empirically.\nCode Availability: All code is available online [1].\nAcknowledgments\nRana Shahout was supported in part by Schmidt Futures Initiative and Zuckerman Institute. Michael Mitzenmacher\nwas supported in part by NSF grants CCF-2101140, CNS-2107078, and DMS-2023528.\n\nReferences\n[1] Open source code. https://anonymous.4open.science/r/LWCSS-A283.\n[2] Arvind Arasu and Gurmeet Singh Manku. Approximate counts and quantiles over sliding windows. In\nProceedings of the twenty-third ACM SIGMOD-SIGACT-SIGART symposium on Principles of database\nsystems , pages 286‚Äì296, 2004.\n[3] Arvind Arasu and Gurmeet Singh Manku. Approximate counts and quantiles over sliding windows. In ACM\nPODS , 2004.\n[4] Ran Ben Basat, Roy Friedman, and Rana Shahout. Stream frequency over interval queries. Proceedings of the\nVLDB Endowment , 12(4):433‚Äì445, 2018.\n[5] Ran Ben-Basat, Gil Einziger, Roy Friedman, and Yaron Kassner. Heavy hitters in streams and sliding windows.\nInIEEE INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communications ,\npages 1‚Äì9. IEEE, 2016.\n[6] Yoshua Bengio. Deep Learning of Representations for Unsupervised and Transfer Learning. In Proceedings of\nICML Workshop on Unsupervised and Transfer Learning , 2012.\n[7] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In\nInternational Colloquium on Automata, Languages, and Programming , pages 693‚Äì703. Springer, 2002.\n[8] Graham Cormode and Shan Muthukrishnan. An improved data stream summary: the count-min sketch and\nits applications. Journal of Algorithms , 55(1):58‚Äì75, 2005.\n[9] Mayur Datar, Aristides Gionis, Piotr Indyk, and Rajeev Motwani. Maintaining stream statistics over sliding\nwindows. SIAM journal on computing , 31(6):1794‚Äì1813, 2002.\n[10] Xiangyang Gou, Long He, Yinda Zhang, Ke Wang, Xilai Liu, Tong Yang, Yi Wang, and Bin Cui. Sliding\nsketches: A framework using time zones for data stream processing in sliding windows. In Proceedings of\nthe 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 1015‚Äì1025,\n2020.\n[11] Paul Hick. CAIDA Anonymized Internet Trace, equinix-chicago, 2016.\n[12] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation algorithms.\nInInternational Conference on Learning Representations , 2019.\n[13] Regant Y. S. Hung, Lap-Kei Lee, and Hing-Fung Ting. Finding frequent items over sliding windows with\nconstant update time. Inf. Proc. Let‚Äô , 2010.\n[14] Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P Woodruff. Learning-augmented data stream\nalgorithms. In International Conference on Learning Representations , 2019.\n[15] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index structures.\nInProceedings of the 2018 international conference on management of data , pages 489‚Äì504, 2018.\n[16] Lap-Kei Lee and H. F. Ting. A simpler and more efficient deterministic scheme for finding frequent items over\nsliding windows. In ACM PODS , 2006.\n[17] Lap-Kei Lee and HF Ting. A simpler and more efficient deterministic scheme for finding frequent items over\nsliding windows. In Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles\nof database systems , pages 290‚Äì297, 2006.\n[18] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. Journal of the\nACM (JACM) , 68(4):1‚Äì25, 2021.\n[19] Ahmed Metwally, Divyakant Agrawal, and Amr El Abbadi. Efficient computation of frequent and top-k\nelements in data streams. In Database Theory-ICDT 2005: 10th International Conference, Edinburgh, UK,\nJanuary 5-7, 2005. Proceedings 10 , pages 398‚Äì412. Springer, 2005.\n\n[20] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. Communications of the ACM ,\n65(7):33‚Äì35, 2022.\n[21] Odysseas Papapetrou, Minos Garofalakis, and Antonios Deligiannakis. Sketching distributed sliding-window\ndata streams. The VLDB Journal , 24:345‚Äì368, 2015.\n[22] Rana Shahout and Michael Mitzenmacher. Learning-based heavy hitters and flow frequency estimation in\nstreams. IEEE ICNP , 2024.\n[23] https://algorithms-with-predictions.github.io/ . Algorithms with predictions github repository.",
  "textLength": 30903
}