{
  "paperId": "e80aba67e92aec3ef06ed31193832c6f81e9a2ed",
  "title": "INODE",
  "pdfPath": "e80aba67e92aec3ef06ed31193832c6f81e9a2ed.pdf",
  "text": "INODE: Building an End-to-End Data Exploration System\nin Practice [Extended Vision]\nSihem Amer-Yahia2, Georgia Koutrika1, Frederic Bastian7, Theofilos Belmpas1, Martin Braschler9,\nUrsin Brunner9, Diego Calvanese8, Maximilian Fabricius5, Orest Gkini1, Catherine Kosten9, Davide\nLanti8, Antonis Litke6, Hendrik Lücke-Tieke3, Francesco Alessandro Massucci6, Tarcisio Mendes\nde Farias7, Alessandro Mosca8, Francesco Multari6, Nikolaos Papadakis4, Dimitris Papadopoulos4,\nYogendra Patil2, Aurélien Personnaz2, Guillem Rull6, Ana Sima7, Ellery Smith9, Dimitrios Skoutas1,\nSrividya Subramanian5, Guohui Xiao8, Kurt Stockinger9\n(1) Athena Research Center, Greece (2) CNRS, University Grenoble Alpes, France (3) Fraunhofer IGD, Germany\n(4) Infili, Greece (5) Max Planck Institute, Germany (6) SIRIS Academic, Spain\n(7) SIB Swiss Institute of Bioinformatics, Switzerland (8) Free University of Bozen-Bolzano, Italy\n(9) ZHAW Zurich University of Applied Sciences, Switzerland\nABSTRACT\nA full-fledged data exploration system must combine different ac-\ncess modalities with a powerful concept of guiding the user in the\nexploration process, by being reactive andanticipative both for data\ndiscovery and for data linking. Such systems are a real opportunity\nfor our community to cater to users with different domain and data\nscience expertise.\nWe introduce INODE - an end-to-end data exploration system -\nthat leverages, on the one hand, Machine Learning and, on the other\nhand, semantics for the purpose of Data Management (DM). Our\nvision is to develop a classic unified, comprehensive platform that\nprovides extensive access to open datasets, and we demonstrate it\nin three significant use cases in the fields of Cancer Biomarker Re-\nsearch, Research and Innovation Policy Making, and Astrophysics.\nINODE offers sustainable services in (a) data modeling and linking,\n(b) integrated query processing using natural language, (c) guidance,\nand (d) data exploration through visualization, thus facilitating the\nuser in discovering new insights. We demonstrate that our system\nis uniquely accessible to a wide range of users from larger scientific\ncommunities to the public. Finally, we briefly illustrate how this\nwork paves the way for new research opportunities in DM.\n1 INTRODUCTION\nThe Data Management (DM) community has been actively catering\nto Machine Learning (ML) research by developing systems and\nalgorithms that enable data preparation and flexible model learning.\nThis has resulted in several major contributions in developing ML\npipelines, and formalizing algebras and languages to facilitate and\ndebug model learning, as well as designing and implementing algo-\nrithms and systems to speed up ML routines [ 3,42]. Existing work\nthat leverages ML for DM [ 45] is nascent and covers the use of\nML for query optimization [ 24] or for database indexing [ 23]. This\npaper makes the case for democratizing Intelligent Data Exploration\nby leveraging ML for DM.\nTraditionally, database systems assume the user has a specific\nquery in mind, and can express it in the language the system under-\nstands (e.g., SQL). However, today, users with different technical\nbackgrounds, roles, and tasks are accessing and leveraging volumi-\nnous and complex data sources. In many scenarios, they are onlypartially familiar with the data and its structure, and their needs\nare not well-formed. In such settings, expanding traditional query\nanswering to data exploration is a natural consequence and require-\nment and with it comes the need to redesign systems accordingly . This\nneed translates to several challenges at different levels.\n(Interaction ). Regarding interaction with the system, the biggest\nchallenge is to enable the user to express her needs through a\nvariety of access modalities , ranging from SQL and SPARQL to\nnatural language (NL) and visual query interfaces, that can be used\nand intermingled depending on the user needs and expertise as\nwell as the data exploration scenario. The second challenge is that\nofuser guidance , i.e., users should be allowed to provide feedback\nto the system, and the system should leverage that feedback to\nimprove subsequent exploration steps.\n(Linking ). Once a user need has been formulated and sent to the\nsystem, it is executed over a (fixed) data set. Users may be aware\nwhich additional data sets could be of interest. However, they do\nnot always know how to correctly link, integrate, and query more\nthan one data source to generate rich information. This introduces\nthe challenges of data linking , so that new data sources can be\nadded to the system, as well as knowledge generation , so that\nqueries over unstructured data can be supported. Both of these aim\nat enabling the continuous expansion of the “pool” of available data\nsources, thus making more data available to users.\n(Guidance) . Traditionally, the system will return to the user a\nset of tuples that concludes the search. There is a lot of work on\nhow to improve performance for query workloads (predict future\nqueries, build indices adaptively, etc.), but still the system has a\nrather passive role: anticipating or at best trying to predict the\nnext query and then optimize its performance accordingly. Hence,\nthe challenge of system proactiveness arises. The output is not\nonly the set of results but also recommendations for subsequent\nqueries or exploration choices. In our vision, the system guides the\nuser to find interesting, relevant or unexpected data and actively\nparticipates in shaping the query workload.\nIn a nutshell, a full-fledged data exploration system must com-\nbine different access modalities with a powerful concept of guiding\nthe user in the exploration process. It must be reactive and anticipa-\ntiveco-shaping with the user the data exploration process. Finally,\nwhile data integration has been around for a while, the ability toarXiv:2104.04194v1  [cs.LG]  9 Apr 2021\n\nINODE Project Partners\nFigure 1: Natural language query interface with user assistance. Step 1: User enters a natural language query. Step 2: System\nparses query and matches keywords against the available ontology to enable term disambiguation; the user iterates the process.\nStep 3: System visualizes various cancer types that are similar to lung cancer. The distance metric between the diseases can be\nchosen by the user, e.g. by semantic distance.\ntie together data discovery and linking is a central question in an\nintelligent data exploration system.\n(Evaluation) . An essential part of our proposal is the development\nof an evaluation framework to enable the end-to-end assessment of\nan intelligent data exploration system. This requires to formalize\nsystem and human metrics that are necessary for data linking and\nintegration, multi-modal data access, guidance, and visualization.\nRelated Work . Several systems address components of our vi-\nsion. A number of them enable NL-to-SQL [ 2], SQL-to-NL [ 22] or\nboth [ 20] (see a summary in [ 1]). Recommendation strategies can be\nleveraged to guide users [ 27]. Work on interactive data exploration\naims at helping the user discover interesting data patterns based on\nan integration of classification algorithms and data management\noptimization techniques [ 11]. Each of the above-mentioned systems\ntackles specific data management challenges as so-called insular so-\nlutions . However, these insular solutions have not been integrated to\ntackle the end-to-end aspect of intelligent data exploration targeted\nat a wide range of different end users.\nCombining all the challenges above requires an elaborated sys-\ntem whose multi-aspect behavior and functionality is the result of\na synergy between disjoint technologies, and integrates them into\na new ensemble. This gives rise to multiple approaches that vary\nin computational complexity, and raises new challenges that can\nbenefit from recent advances in ML.\nIn summary , this paper makes the following contributions. We\nadvocate for using ML to solve DM problems that arise when build-\ning intelligent data exploration systems. We illustrate the need for\nintelligent data exploration with relevant use cases (Section 2). We\ndescribe the solution we have today, INODE1, that we are currently\nbuilding as part of a European project (Section 3). To fully complete\nour vision, we provide open research challenges to be addressed at\nthe intersection of DM and ML (Section 4).\n2 USE CASES\nIn this section, we describe two use cases from cancer research and\nastrophysics and show how INODE can tackle them.\n1http://www.inode-project.eu/Use Case 1: Cancer Research (Natural Language and Visual Data\nExploration). Fred is a biologist who studies cancer. His goal is to\nfind which specific biomarkers are indicators for a certain type of\nlung cancer. He needs natural language exploration.\nINODE offers support for NL queries, query recommendations,\nand interactive visualizations triggered by NL queries (see Figure 1).\nFor instance, Fred starts with a request in NL for the topics related\nto lung cancer but is not sure how to continue after inspecting\nthe results. INODE steps up and recommends different options :\nto expand the search using experimental drugs for treating lung\ncancer, or to focus on a subset of lung cancer types associated with\na certain gene expression. Fred chooses to expand his search to\none of the recommended topics, and receives a new list of lung\ncancers, drugs and genes. Additionally, INODE explains in NL how\nresults are related. That helps him in selecting experimental drugs\nfor certain gene expressions. After a few such queries, the system\nvisually analyzes the results for Fred to study. Fred learns about\nthe similarity between different types of cancer based on distance\nmetrics that he can choose.\nUse Case 2: Astrophysics (Exploration with SQL-Pipelines). In\nthe era of big data, astronomers need to analyze dozens of databases\nat a time. With the ever increasing number of publically available\nastronomical databases from various astronomical surveys across\nthe globe, it is becoming increasingly challenging for scientists\nto penetrate deep into the data structure and their metadata in\norder to generate new scientific knowledge. Sri, an astrophysicist,\nexplores astronomical objects in SDSS, a large sky survey database2.\nSri would like to examine Green Pea galaxies, first discovered in\na citizen science project called ’Galaxy zoo’, that recently gained\nattention in astronomy as one of the potential sources that drove\ncosmic reionization.\nFigure 2 shows a sequence of three consecutive processes of ana-\nlyzing astrophysics data. Sri relies on selected examples at each step\nand requests to see comparable ones. In the first query, she asks\nto find galaxies with similar colors as Green Pea galaxies, she\nrequests objects with similar spectral properties, like emis-\nsion line measurements, star formation rates etc., as those\n2https://www.sdss.org/\n\nINODE: Building an End-to-End Data Exploration System\nin Practice [Extended Vision]\nFigure 2: Exploring astrophysics data.\nreturned in the first step. The last query finds similar galaxies in\nterms of their relative ratios and strength of emission lines .\nAs a result, Sri discovers that green pea emission line ratios are\nsimilar to high redshift galaxies.\nINODE guides any user in making such new discoveries in an\nintuitive simpler way, without having to write complicated SQL\nqueries or manual analysis of thousands of galaxies. For instance, IN-\nODE helps a user choose among similarity dimensions rather\nthan rely on her ability to provide them. Additionally, INODE shows\nto the user alternative queries to pay attention to, thus increasing\nthe chances of making new discoveries.\n3 CURRENT INODE ARCHITECTURE\nThe main novelty of INODE is bringing together different data\nmanagement solutions to enable intelligent data exploration (see\nFigure 3). Although some of these solutions and research challenges\nhave been tackled previously, they have not been combined into\nsuch an end-to-end intelligent data exploration system, which in\nturn opens up new research challenges.\nINODE’s major components are as follows: (1) Data Modeling\nand Linking enables integration of both structured and unstruc-\ntured data. (2) Integrated Query Processing enables efficient query\nprocessing across federated databases leveraging ontologies. (3)\nData Access and Exploration enables guided data exploration in var-\nious modalities such as by natural language, by certain operators\nor visually.\n3.1 Data Modeling and Linking\nThis component links loosely coupled collections of data sources\nsuch as relational databases, graph databases or text documents\nbased on the well-established ontology-based data access (OBDA)\nparadigm [ 47]. OBDA uses a global ontology (knowledge graph) to\nmodel the domain of interest and provides a conceptual representa-\ntion of the information in the data sources. The sources are linked\nto elements in the global ontology through declarative mappings . It\nis well-known that designing OBDA mappings manually is a time-\nconsuming and error-prone task. The Data Modeling and Linking\nFigure 3: Major components of the INODE architecture.\ncomponent of INODE aims at automatizing this task by providing\ntwo mechanisms: data-driven andtask-driven mapping generation.\nData-driven Mapping Generation . This mechanism deals with\nlinking novel data sources to the system. The idea is to rely on\nmapping patterns that describe well-assessed and sound schema-\ntransformation rules usually applied in the design process of re-\nlational databases. By analyzing (driven by the patterns) the data\nsources, it is possible to automatically derive a so-called putative\nontology [39] describing both the explicit entities and relationships\nconstituting the schema and the implicit ones inferrable from the\ndata. From the mapping patterns, one can also automatically derive\nmappings that link the data sources to the putative ontology.\nTask-driven Mapping Generation . This mechanism is applied\nwhenever a task or a query is formulated that uses specific tar-\ngetontology elements that are not yet aligned with the putative\nontology. In such scenario, the semantics of the query are used to\nautomatically generate mappings to align the target ontology with\nthe putative ontology.\nKnowledge Graph (KG) Generation . This service transforms un-\nstructured information hidden in large quantities of text (e.g. repos-\nitories of scientific papers) to an exploitable structured representa-\ntion through an NLP pipeline. INODE follows an Open Information\nExtraction (OIE) approach to convert each sentence of the corpus\ninto a set of relational triples, where each triple consists of a subject,\nan object, and a predicate (relationship) linking them. We lever-\nage a number of preprocessing techniques, including co-reference\nresolution and extractive summarization to improve the quality of\nthe extracted relational triples. We combine different OIE methods\n(rule-based, analytics-based and learning-based) to achieve both\nhigh precision and high recall [ 32]. The relational triples are fur-\nther linked with domain-specific ontology concepts before being\nintegrated into the knowledge graph.\n3.2 Integrated Query Processing\nThis component is responsible for the execution of queries using\nOntop [ 49], a the state-of-the-art OBDA system. Ontop allows the\nusers to formulate queries in terms of concepts andproperties of their\ndomain of expertise (represented in knowledge graphs ), rather than\nin terms of table and attribute names used in the actual data sources.\n\nINODE Project Partners\nHence, users do not have to be aware of the specific storage details\nof the underlying data sources in order to satisfy their information\nneeds.\nQuery Execution . This service provides on-the-fly reformulation\nof SPARQL queries over the domain ontology to SQL queries over\nthe data sources. An approach based on reformulation has the\nadvantage that the data available in the data sources does not need\nto be duplicated in the query processing system, but can be kept in\nthe data sources as-is. This means that the Query Execution service\nis guaranteed even in the common scenario where the user does\nnot own the data nor does have the right to copy them. To produce\nreformulations that can efficiently be executed over the data, in\nINODE we use optimization techniques such as self-join elimination\nfor denormalized data [49] and optimizations of left-joins arising\nfrom OPTIONAL andMINUS operators [48].\nSource Federation . The Source Federation service deals with dis-\ntributing the processing of queries over the available data sources.\nINODE provides different kinds of federation ranging from SQL fed-\neration to seamless SPARQL federation. With respect to SPARQL,\nwe can distinguish between two forms of federation: seamless fed-\neration and SPARQL 1.1 SERVICE federation.\nIn seamless federation, users send queries against a unified view\nof the remote endpoints without the need to be aware of the actual\nvocabularies used in the federated endpoints. The challenge is to\nautomatically detect to which sources which components of the\nquery need to be dispatched, to collect the retrieved results, and to\ncombine them into a coherent answer. We address this challenge\nby relying on the knowledge about the sources encoded in the\nOBDA mappings. Given that efficiency is a crucial requirement in\nthis setting, our approach requires a dedicated cost-model able to\nminimize the number of distributed joins over the federation layer,\nin order to favor more efficient joins at the level of the sources.\nInstead, SERVICE federation might be adopted in those cases\nwhere users want to directly refer to “external” SPARQL endpoints,\nnot yet integrated with the ontology. In this setting, the user directly\nreferences the desired endpoints at query time using the SPARQL 1.1\nSERVICE functionality. Observe that the user is required to be aware\nof the vocabulary used in the external endpoint. Hence, the Source\nFederation service can simply delegate the execution of the SPARQL\nquery component referenced in the SERVICE call.\nData Analytics . The data analytics service exploits novel and\nefficient query reformulation and optimization techniques [ 49] to\ncompute complex analytical functions. Such techniques are based\non algebraic transformations of the SPARQL algebra tree, rather\nthan on Datalog transformations as traditionally done in the OBDA\nliterature. This shift of paradigm allows for an efficient implementa-\ntion of analytical functions such as SPARQL aggregates. It is worth\nnoting that INODE, through Ontop, provides the first open-source\nreformulation-based system able to support SPARQL aggregates.\nThe Answer Justification service generates in an automatic way\ncompact and easy to understand explanations for query results.\nIn an OBDA setting, the explanations for a result must take into\naccount, in addition to the query, the three components of the input,\nnamely the ontology, the mappings, and the data sources [ 7]. The\nontology is taken care of by identifying the ontology axioms used\nfor the rewriting of the input query. As for the mappings, thoseconsidered in the justification are the ones that were used for the\nunfolding of the rewritten query to produce the SQL reformulation.\nFinally, the data is taken care of by identifying the actual tables\nand tuples that contributed to build the considered result, using an\napproach based on provenance semirings [38].\n3.3 Data Access and Exploration\nExploration by Natural Language . For translating a natural language\nquestion into SQL or SPARQL, INODE uses pattern-based ,graph-\nbased andneural network-based approaches . For translating from\nNL to SQL, INODE extends the pattern-based system SODA [ 2]\nwith NLP techniques such as lemmatization, stemming and POS\ntagging to allow both key word search queries as well as full natural\nlanguage questions. In addition, we use Bio-SODA [ 41], a graph-\nbased system to enable NL questions over RDF graph databases. In\norder to enable federated queries across both relational databases\nand RDF graph databases, INODE uses an ontology-based data\naccess technology leveraging Ontop [ 6]. The advantage of using\npattern-based or graph-based approaches over neural network-\nbased approaches is that they do not require training data, which\nis often very costly to gain.\nSince neural network-based approaches typically require large\namounts of training data, we also experimented with various train-\ning data generation approaches. INODE uses an inverse data annota-\ntion approach called OTTA [ 10]. Rather than writing NL questions\nand then the corresponding SQL or SPARQL statements, OTTA\nreverses the process. In particular, OTTA randomly generates so-\ncalled operator trees which are similar to logical query plans that\ncan easily be understood by non-tech savvy users. Afterwards,\ngiven these operator trees, crowd workers write the corresponding\nNL questions. In INODE, we use both crowd workers and domain\nexperts for generating training data.\nFinally, INODE integrates the neural network-based approach\nValueNet, which leverages transformer architectures to translate\nNL to SQL [ 5]. The ultimate goal of INODE is to combine all these\ntechniques into an intelligent hybrid-approach that improves on the\nerrors of each of the individual systems.\nExploration by Explanation . One of the biggest hurdles in today’s\nexploration systems is that the system provides no explanations\nof the results or system choices. Nor does the system trigger input\nfrom the user, for example, by asking the user to provide more\ninformation. In INODE, we enable a conversational setting, where\nthe system can (a) ask clarifications and (b) explain results in natu-\nral language. This interaction assumes that the system is capable\nof analyzing and understanding user requests and generating its\nanswers or questions in natural language.\nOne approach used in INODE builds on Template-based Synthesis\n[22]. This approach considers the database schema as a graph and a\nquery as a subgraph. We use templates that tell us how to compose\nsentences as we traverse the graph and we use different traversal\nstrategies that generate query descriptions as phrases in natural\nlanguage. Furthermore, to generate NL descriptions that use the\nvocabulary of a particular database, INODE enriches its vocabulary\nbyleveraging ontologies built by the Data Modeling and Linking\ncomponents. To further improve INODE’s explanation capabilities,\nwe are working on an approach to automatically learn templates,\n\nINODE: Building an End-to-End Data Exploration System\nin Practice [Extended Vision]\nFigure 4: Example data exploration pipeline for analyzing galaxies.\nwhich is especially critical for databases with no descriptive meta-\ndata, such as SDSS. Essentially, we are using neural-based methods\nto translate from SQL or SPARQL to natural language.\nFor example, user questions in natural language inevitably hide\nambiguity. Hence, the system can come up with several possible\nquery interpretations that may lead to unexpected results or need\nthe user’s help for query disambiguation.\nExploration by Example and by Analytics . By-example is a pow-\nerful operator that encapsulates multiple semantics. It takes a set\nof examples, such as galaxies or patients, and explores its different\nfacets, filters them, finds similar/dissimilar sets, finds overlapping\nsets, joins them with other sets, finds a superset, etc. Addition-\nally, by-example operators can be combined with by-analytics to\nfind sets that are similar/dissimilar wrt some value distributions.\nFigures 4 shows an example data exploration pipeline (DEP) for\nexploring galaxies with different instances of by-example.\nBy-example and by-analytics operators can be represented in the\nRegion Connection Calculus (RCC) [ 26] and are, in their general form,\ncomputationally challenging. For instance, by-subset is akin to\nsolving a set cover problem, which has been extensively studied [ 9].\nSimilarly, by-join requires to have appropriate indices. In INODE,\nwe adopt two approaches. One is based on a relational backend in\nwhich individual operators are translated into SQL. The other one is\nanin-memory Python implementation that relies on pre-computing\nand indexing sets. For some operators such as by-facet, the SQL\nversion is straightforward since it resolves into generating a GROUP\nBY-query. For others, such as by-overlap, the Python version is\nsimpler as it relies on using an index that records pairwise overlaps\nbetween pre-computed sets.\nExploration by Recommendation . In a mixed-initiative setting,\nthe system actively guides the user in what possible actions to\nperform or data to look at next. In INODE, we are interested in\nrecommendations in both cold-start (where the user has not given\nany input) and warm-start settings (where the user has asked one\nor more queries but may not know what to do next). In the former\ncase, the goal is to show a set of example or starter queries that\nthe users could use to get some initial answers from the dataset\n(e.g. [ 18]). In the latter case, the system can leverage the user’s\ninteractions (queries) to show possible next queries (e.g., [17]).\nA big differentiator is the availability of query logs. In case no\nquery logs are available, the system should still provide recommen-\ndations. In INODE we are addressing the recommendation problem\nfrom different angles, i.e., generating recommendations: (a) basedondata analysis [15] (b) by NL-based processing and query aug-\nmentation techniques leveraging knowledge bases (c) by user log\nanalysis .\nExploration by Visualization . In information retrieval, search\nqueries result in a list of candidates ranked by their matching score\n[28]. This also holds true for INODE, as most exploration operators\ngenerate multiple potential answers. However, results are not indi-\nvidual items such as documents, but data sets (i.e. sets of items) and\nhave to be communicated to the user differently to support their\ngoals. Not only do users have to decide, which data set contains the\nanswer they are looking for, but also to compare the results, to as-\nsess redundancies, discrepancies and other surprising or interesting\ndifferences to draw hints on how to continue the exploration.\nThe goals of the by-visualization data access and exploration\ninterface are two-fold: (1) Enable \"explorers\" to understand, com-\npare and decide based on the provided results and (2) enable them\nto interact with the results by enabling indirect query manipu-\nlation, identifying and highlighting parts that are of interest for\nfurther analysis and guiding them towards interesting regions\n[29,35,43,44]. Depending on the users information need, the opti-\nmal answer may take different forms. For example, some questions\ncan be satisfied by a single data cell, while others require aggre-\ngated values or even multiple tables. Hence, making the results\nexplorable with respect to the users’ needs is very challenging.\nOur processes for user requirements elicitation confirms our\ngoals stated above and is based on the User Centered Design stan-\ndard [ 19]. In addition to that, users emphasized the importance to\ncompare differences as well as similarities of queries and results.\nAs a base line, we enabled the visualization of multiple tables with\ndirect manipulation capabilities and currently work on an overview\nvisualization that spans the result data space.\nA simple data exploration process with INODE for exploring\nEU-projects stored in the CORDIS3database is shown in Figure 5.\n4 OPEN RESEARCH CHALLENGES BEYOND\nINODE\nA full-fledged data exploration system should learn about data\nsources ,learn about users and queries , and leverage this knowledge\nto facilitate and guide users . All these challenges constitute new\nopportunities for ML research to contribute to DM.\n3https://cordis.europa.eu/\n\nINODE Project Partners\nFigure 5: Data exploration process in INODE. (1) User starts\nwith an NL question such as Find all projects. (2) INODE\ntranslates NL question into SQL and returns two different\nresults (interpretations). (3) INODE explains the SQL results\nin NL. (4) The results can further be explored by pipeline op-\nerators such as by-filter to only show projects from France.\n4.1 Learning about Data Sources\nTraditionally SQL or SPARQL are used to query structured data\nstored in relational databases and graph databases, respectively.\nA manual time-consuming data integration process is usually re-\nquired to integrate new data sources. Additionally, data integration\ntechniques used for structured data are not directly applicable to\nunstructured data since they do not have a schema.\nThere are several data integration challenges that can be tackled\nwith ML. The first one is to automate data integration [16], in the\nspirit of existing work that improves entity matching using trans-\nformer architectures [4]. The second challenge is to automatically\ngenerate knowledge graphs , in the spirit of neural-network systems\nsuch as Snorkel [ 34]. Another powerful concept to automate knowl-\nedge graph construction is to combine user dialogs with graph\nconstruction [ 31]. The idea is to augment the knowledge graphs bylearning concepts that are commonly queried but do not exist in\nthe graph. In summary, there has been a large amount of research\non automatic knowledge base construction. However, the combina-\ntion of knowledge base construction with natural language query\nprocessing has been largely untapped.\n4.2 Understanding Users and Queries\nUnderstanding user interests and expertise is a vital component\nfor enabling intelligent data exploration. For instance, the general\npublic interested in black holes has different expectations from an\nexperienced astronomer with a vast knowledge of astrophysics. The\nchallenge is to avoid overwhelming a novice user while providing\ninteresting and relevant information to an expert user.\nThe system should constantly improve its behavior by learning\nand adapting to the user from task to task. Our operators are a great\nopportunity to learn and adapt to users, as they provide the ability\nto choose between utility and novelty, two dimensions that have\nnot been explored together in the past. Additionally, they enable\ncollecting user feedback at the level of individual operators and of\na DEP. While ML methods for learning user profiles exist in the\ncontext of individual systems for web search or recommendations,\nthey have not been studied before in the context of determining\nwhich operator caters for which user in the next step. A simple start\nis to use regression methods to determine the weight of utility and\nnovelty when exploring data. Additionally, probabilistic language\nmodels [ 8] or matrix factorization-based approaches [ 50] can be\nused to infer users’ topical interests, and latent factor models to\nmine user groups.\nTraditional systems such as SODA, ATHENA, NALIR, or Lo-\ngos [ 2,22,25,36] use pattern-based approach for NL to SQL or SQL\nto NL, or supervised ML [ 5,21]. A new opportunity here is to train\na neural network for sequence-to-sequence prediction [ 5,46] for\ntranslating from NL-to-SQL and vice versa. The key research chal-\nlenge is how to use the feedback provided by users to disambiguate\nqueries and feed the gained knowledge back into ML models to im-\nprove learning with semantic information for building ML models\nto tackle disambiguation and context modeling [40].\nThis should allow to model and solve the two symmetrical trans-\nlation problems at once. However, ML methods for query transla-\ntion typically require large amounts of training data. Our vision is\nthat a full-fledged exploration system should be able to leverage\nboth pattern-based and ML-based approaches to provide the most\nrelevant answers to the user.\n4.3 Generating Data Exploration Pipelines\nUnderstanding queries and users serves the ability to provide guid-\nance in generating DEPs. This challenge can be approached in\ndifferent ways depending on the user’s expertise and willingness\nto provide feedback. In a scenario where a DEP is given (see ex-\nample in Section 2), the problem could be cast as finding the right\nparameters for each query in the DEP. In a scenario where the user\nis providing the next query, it could be seen as a query completion\nproblem. In a scenario where the user does not write exploration\nqueries and only provides feedback on results, it could be seen as\nthe problem of learning the user’s DEP. All these cases result in\npartially-guided or fully-guided exploration.\n\nINODE: Building an End-to-End Data Exploration System\nin Practice [Extended Vision]\nFurthermore, since DEPs bring together several data access\nmodalities, which may be initiated by the user (e.g. a user query)\nor by the system (recommendations or explanations), the system\nneeds to learn how to use its options to help the user in meaningful\nand unobtrusive ways. While there has been work on each of these\ncapabilities individually (e.g., recommendations or query explana-\ntions), these efforts only focus on small parts of the problem lacking\na holistic understanding of the behavior and dynamics of a multi-\naspect system. ML approaches and in particular, Reinforcement\nLearning (RL) and Active Learning (AL) can be leveraged.\nPartial Guidance with AL . AL is claimed to be superior to faceted\nsearch when the goal is to help users formulate queries. Systems like\nAIDE [ 11] and REQUEST [ 14] assist users in constructing accurate\nexploratory queries, while at the same time minimizing the number\nof sample records presented to them for labeling. Both systems\nrely on training a decision tree classifier to build a model that\nclassifies unlabeled records. A bigger challenge is to leverage AL in\ngenerating and refining queries that go beyond SQL predicates.\nFull-Guidance with RL . RL and Deep RL are becoming [ 30] the\nmethods of choice for creating exploration pipelines and for gener-\nating DEPs based on a simulated agent experience [ 13,37]. In [ 13],\na Deep RL architecture is used for generating notebooks that show\ndiverse aspects of a data set in a coherent narrative. In [ 37], an\nend-to-end exploration policy is generated to find a set of users\nin a collection of groups. Both frameworks accept a wide class of\nexploration actions and do not need to gather exploration logs. An\nopen question is the applicability of this framework to specific data\nsets and the transferability of learned policies across data sets.\n4.4 Evaluating Data Exploration Pipelines\nEvaluating data exploration requires a holistic approach that ad-\ndresses performance ,quality anduser experience aspects [ 12]. The\niterative multi-step nature of DEPs makes evaluation particularly\nchallenging since it is not simply a matter of summing up multiple\nlocal evaluations at each step. Regarding performance, the challenge\nlies in the design of logging mechanisms to export the performance\nof different steps in DEPs. Such logging must capture in fine details\nthe time and memory usage each step takes so that they can be\naggregated to assess a full DEP. The aggregation must be carefully\ndesigned to include user actions such as exploring multiple paths\nand backtracking. While some users prefer the shortest path to a\ngoal, others may be more interested in exploring different paths.\nMultiple evaluation questions arise: “How good are DEPs? How\ngood are NL to SQL/SPARQL translations and vice versa? How\ngood are the next-operator or next-data recommendations and\nexplanations?” The novelty of our approach lies in the ability to\njointly assess performance, quality and user experience for each use\ncase.\nWe categorize INODE’s evaluation metrics into system metrics\nand human factors [33]. To answer above questions, certain key\nchallenges need to be answered. The first challenge lies in extracting\nmetrics from user interactions. We are addressing this by designing\nlogging mechanisms to export various parameters. We use latency\nand memory usage for evaluating the system performance of a\nDEP. We also record the number of clicks, interaction time anduser feedback. While some users prefer the shortest path to a goal,\nothers may be more interested in exploring different paths.\nAnother challenge lies in the iterative multi-step nature of DEPs\nsince it is not simply a matter of summing up multiple local evalua-\ntions at each step. An additional key challenge is human subject\nbias. To avoid such biases, our initial studies are designed in multi-\nple stages such as pre-qualification of users, randomly assigning\nusers to different treatment groups and finally feedback from users.\nUser acceptance of our system is strongly related to two key\nfactors: accuracy - which reflects the closeness of exploration results\nto desired results, and controllability - which reflects the ability of\nour system to guide the user in the exploration [ 12]. Accuracy is\ncomputed using standard methods such as precision and recall, and\ncontrollability is the inverse of the number of user-interactions. Our\nfirst endeavor is to deploy user studies that explore the relationship\nbetween accuracy and controllability. To do so, we will perform\nfactorial design analysis and deploy questionnaires for exploration\nscenarios we are defining with our use case providers.\nThe key idea of using ML techniques here is to capture the de-\npendence on users and data and learn different exploration profiles.\nA major challenge is to design pilot studies in a sound manner with\ndirect observations via questionnaires and indirect ones (such as\nmouse tracking) to generate labeled datasets for learning. Using\nML techniques, and in particular ensemble learning and multi-task\nlearning, constitutes an unprecedented opportunity to adapt the\nevaluation to users and data.\n5 ACKNOWLEDGEMENTS\nThis project has received funding from the European Union’s Hori-\nzon 2020 research and innovation program under grant agreement\nNo 863410.\nREFERENCES\n[1]Katrin Affolter, Kurt Stockinger, and Abraham Bernstein. 2019. A comparative\nsurvey of recent natural language interfaces for databases. The VLDB Journal 28,\n5 (2019), 793–819.\n[2]Lukas Blunschi, Claudio Jossen, Donald Kossmann, Magdalini Mori, and Kurt\nStockinger. 2012. Soda: Generating Sql for business users. Proceedings of the\nVLDB Endowment 5, 10 (2012), 932–943.\n[3]Matthias Boehm, Michael Dusenberry, Deron Eriksson, Alexandre V. Evfimievski,\nFaraz Makari Manshadi, Niketan Pansare, Berthold Reinwald, Frederick Reiss,\nPrithviraj Sen, Arvind Surve, and Shirish Tatikonda. 2016. SystemML: Declarative\nMachine Learning on Spark. Proc. VLDB Endow. 9, 13 (2016), 1425–1436.\n[4]Ursin Brunner and Kurt Stockinger. 2020. Entity matching with transformer\narchitectures-a step forward in data integration. In Int. Conference on Extending\nDatabase Technology, Copenhagen, 30 March-2 April 2020 .\n[5]Ursin Brunner and Kurt Stockinger. 2021. ValueNet: A Natural Language-to-SQL\nSystem that Learns from Database Information. International Conference on Data\nEngineering (ICDE) (2021).\n[6]Diego Calvanese, Benjamin Cogrel, Sarah Komla-Ebri, Roman Kontchakov, Da-\nvide Lanti, Martin Rezk, Mariano Rodriguez-Muro, and Guohui Xiao. 2017. Ontop:\nAnswering SPARQL Queries over Relational Databases. Semantic Web 8, 3 (2017),\n471–487.\n[7]Diego Calvanese, Davide Lanti, Ana Ozaki, Rafael Peñaloza, and Guohui Xiao.\n2019. Enriching Ontology-based Data Access with Provenance. In Proc. of the\n28th Int. Joint Conf. on Artificial Intelligence (IJCAI 2019) . IJCAI Org., 1616–1623.\nhttps://doi.org/10.24963/ijcai.2019/224\n[8] Cheng Cao, Hancheng Ge, Haokai Lu, Xia Hu, and James Caverlee. 2017. What\nAre You Known For?: Learning User Topical Profiles with Implicit and Explicit\nFootprints. In Proceedings of the 40th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August\n7-11, 2017 , Noriko Kando, Tetsuya Sakai, Hideo Joho, Hang Li, Arjen P. de Vries,\nand Ryen W. White (Eds.). ACM, 743–752.\n[9]Graham Cormode, Howard J. Karloff, and Anthony Wirth. 2010. Set cover\nalgorithms for very large datasets. In Proceedings of the 19th ACM Conference on\n\nINODE Project Partners\nInformation and Knowledge Management, CIKM 2010, Toronto, Ontario, Canada,\nOctober 26-30, 2010 . 479–488.\n[10] Jan Deriu, Katsiaryna Mlynchyk, Philippe Schläpfer, Alvaro Rodrigo, Dirk\nVon Grünigen, Nicolas Kaiser, Kurt Stockinger, Eneko Agirre, and Mark Cieliebak.\n2020. A Methodology for Creating Question Answering Corpora Using Inverse\nData Annotation. Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics (ACL) (2020).\n[11] Kyriaki Dimitriadou, Olga Papaemmanouil, and Yanlei Diao. 2016. AIDE: An\nActive Learning-Based Approach for Interactive Data Exploration. IEEE Trans.\nKnowl. Data Eng. 28, 11 (2016), 2842–2856.\n[12] Philipp Eichmann, Emanuel Zgraggen, Carsten Binnig, and Tim Kraska. 2020.\nIDEBench: A Benchmark for Interactive Data Exploration. In Proceedings of the\n2020 International Conference on Management of Data, SIGMOD Conference 2020,\nonline conference [Portland, OR, USA], June 14-19, 2020 . 1555–1569.\n[13] Ori Bar El, Tova Milo, and Amit Somech. 2020. Automatically Generating Data\nExploration Sessions Using Deep Reinforcement Learning. In Proceedings of the\n2020 International Conference on Management of Data, SIGMOD Conference 2020,\nonline conference [Portland, OR, USA], June 14-19, 2020 . 1527–1537.\n[14] Xiaoyu Ge, Yanbing Xue, Zhipeng Luo, Mohamed A. Sharaf, and Panos K.\nChrysanthis. 2016. REQUEST: A scalable framework for interactive construction\nof exploratory queries. In 2016 IEEE International Conference on Big Data, BigData\n2016, Washington DC, USA, December 5-8, 2016 . 646–655.\n[15] Apostolis Glenis, Yannis Stavrakas, and Georgia Koutrika. 2020. PyExplore:\nClustering-based SQL query recommendations. In under submission .\n[16] Behzad Golshan, Alon Halevy, George Mihaila, and Wang-Chiew Tan. 2017. Data\nintegration: After the teenage years. In Proceedings of the 36th ACM SIGMOD-\nSIGACT-SIGAI Symposium on Principles of Database Systems . 101–106.\n[17] Marie Le Guilly, Jean-Marc Petit, and Vasile-Marian Scuturici. 2018. SQL Query\nCompletion for Data Exploration. CoRR abs/1802.02872 (2018). arXiv:1802.02872\nhttp://arxiv.org/abs/1802.02872\n[18] Bill Howe, Garrett Cole, Nodira Khoussainova, and Leilani Battle. 2011. Auto-\nmatic example queries for ad hoc databases. In Proceedings of the ACM SIGMOD\nInternational Conference on Management of Data, SIGMOD 2011, Athens, Greece,\nJune 12-16, 2011 , Timos K. Sellis, Renée J. Miller, Anastasios Kementsietsidis, and\nYannis Velegrakis (Eds.). ACM, 1319–1322.\n[19] International Organization for Standardization. 2019. ISO 9241-210:2019 - Er-\ngonomics of Human-System Interaction — Part 210: Human-Centred Design for\nInteractive Systems. , 33 pages. https://www.iso.org/standard/77520.html\n[20] Rogers Jeffrey Leo John, Navneet Potti, and Jignesh M. Patel. 2017. Ava: From\nData to Insights Through Conversations. In CIDR 2017, 8th Biennial Conference\non Innovative Data Systems Research, Chaminade, CA, USA, January 8-11, 2017 .\n[21] Hyeonji Kim, Byeong-Hoon So, Wook-Shin Han, and Hongrae Lee. 2020. Natural\nlanguage to SQL: Where are we today? Proc. VLDB Endowment 13, 10 (2020),\n1737–1750. http://www.vldb.org/pvldb/vol13/p1737-kim.pdf\n[22] Andreas Kokkalis, Panagiotis Vagenas, Alexandros Zervakis, Alkis Simitsis, Geor-\ngia Koutrika, and Yannis E. Ioannidis. 2012. Logos: a system for translating\nqueries into narratives. In Proceedings of the ACM SIGMOD International Confer-\nence on Management of Data, SIGMOD 2012, Scottsdale, AZ, USA, May 20-24, 2012 .\nACM, 673–676.\n[23] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018 . 489–504.\n[24] Ani Kristo, Kapil Vaidya, Ugur Çetintemel, Sanchit Misra, and Tim Kraska. 2020.\nThe Case for a Learned Sorting Algorithm. In Proceedings of the 2020 International\nConference on Management of Data, SIGMOD Conference 2020, online conference,\nJune 14-19, 2020 .\n[25] Fei Li and H. V. Jagadish. 2014. Constructing an Interactive Natural Language\nInterface for Relational Databases. Proc. VLDB Endow. 8, 1 (Sept. 2014), 73–84.\n[26] Sanjiang Li and Mingsheng Ying. 2003. Region Connection Calculus: Its models\nand composition table. Artificial Intelligence 145, 1 (2003), 121 – 146.\n[27] Jianjun Liu, Zainab Zolaktaf, Rachel Pottinger, and Mostafa Milani. 2019. Im-\nprovement of SQL Recommendation on Scientific Database. In Proceedings of the\n31st International Conference on Scientific and Statistical Database Management,\nSSDBM 2019, Santa Cruz, CA, USA, July 23-25, 2019 , Carlos Maltzahn and Tanu\nMalik (Eds.). ACM, 206–209. https://doi.org/10.1145/3335783.3335800\n[28] Christopher D Manning, Prabhakar Raghavan, and Hinrich Schütze. [n.d.]. Intro-\nduction to Information Retrieval . Cambridge University Press.\n[29] Thorsten May, Martin Steiger, James Davey, and Jörn Kohlhammer. 2012. Using\nsignposts for navigation in large graphs. In Computer Graphics Forum , Vol. 31.\nWiley Online Library, 985–994.\n[30] Tova Milo and Amit Somech. 2020. Automating Exploratory Data Analysis\nvia Machine Learning: An Overview. In Proceedings of the 2020 International\nConference on Management of Data, SIGMOD Conference 2020, online conference\n[Portland, OR, USA], June 14-19, 2020 . 2617–2622.\n[31] Dat Ba Nguyen, Abdalghani Abujabal, Khanh Tran, Martin Theobald, and Gerhard\nWeikum. 2017. Query-driven on-the-fly knowledge base construction. Proceedings\nof the VLDB Endowment 11, 1 (2017), 66–79.[32] Dimitris Papadopoulos, Nikolaos Papadakis, and Antonis Litke. 2020. A Method-\nology for Open Information Extraction and Representation from Large Scientific\nCorpora: The CORD-19 Data Exploration Use Case. Applied Sciences 10, 16 (2020).\nhttps://www.mdpi.com/2076-3417/10/16/5630\n[33] Protiva Rahman, Lilong Jiang, and Arnab Nandi. 2020. Evaluating interactive\ndata systems. VLDB J. 29, 1 (2020), 119–146.\n[34] Alexander J Ratner, Stephen H Bach, Henry R Ehrenberg, and Chris Ré. 2017.\nSnorkel: Fast training set generation for information extraction. In Proceedings of\nthe 2017 ACM international conference on management of data . 1683–1686.\n[35] Tuukka Ruotsalo, Kumaripaba Athukorala, Dorota G\\ lowacka, Ksenia\nKonyushkova, Antti Oulasvirta, Samuli Kaipiainen, Samuel Kaski, and Giulio\nJacucci. [n.d.]. Supporting Exploratory Search Tasks with Interactive User Model-\ning. In Proceedings of the 76th ASIS&T Annual Meeting: Beyond the Cloud: Rethink-\ning Information Boundaries (Silver Springs, MD, USA, 2013) (ASIST ’13) . American\nSociety for Information Science, 39:1–39:10. http://dl.acm.org/citation.cfm?id=\n2655780.2655819\n[36] Diptikalyan Saha, Avrilia Floratou, Karthik Sankaranarayanan, Umar Farooq\nMinhas, Ashish R Mittal, and Fatma Özcan. 2016. Athena: An ontology-driven\nsystem for natural language querying over relational data stores. Proceedings of\nthe VLDB Endowment 9, 12 (2016), 1209–1220.\n[37] Mariia Seleznova, Behrooz Omidvar-Tehrani, Sihem Amer-Yahia, and Eric Simon.\n2020. Guided Exploration of User Groups. Proc. VLDB Endow. 13, 9 (2020),\n1469–1482.\n[38] Pierre Senellart, Louis Jachiet, Silviu Maniu, and Yann Ramusat. 2018. ProvSQL:\nProvenance and Probability Management in PostgreSQL. Proc. of the VLDB\nEndowment 11, 12 (2018), 2034–2037.\n[39] Juan F. Sequeda and Daniel P. Miranker. 2015. Ultrawrap Mapper: A Semi-\nAutomatic Relational Database to RDF (RDB2RDF) Mapping Tool.. In Proc. of\nthe 14th Int. Semantic Web Conf., Posters & Demonstrations Track (ISWC) (CEUR\nWorkshop Proceedings, http://ceur-ws.org/ ), Vol. 1486.\n[40] Saeedeh Shekarpour, Kemele M Endris, Ashwini Jaya Kumar, Denis Lukovnikov,\nKuldeep Singh, Harsh Thakkar, and Christoph Lange. 2016. Question answer-\ning on linked data: Challenges and future directions. In Proceedings of the 25th\nInternational Conference Companion on World Wide Web . 693–698.\n[41] Ana Claudia Sima, Tarcisio Mendes de Farias, Erich Zbinden, Maria Anisimova,\nManuel Gil, Heinz Stockinger, Kurt Stockinger, Marc Robinson-Rechavi, and\nChristophe Dessimoz. 2019. Enabling semantic queries across federated bioinfor-\nmatics databases. Database 2019 (2019).\n[42] Evan R. Sparks, Shivaram Venkataraman, Tomer Kaftan, Michael J. Franklin,\nand Benjamin Recht. 2017. KeystoneML: Optimizing Pipelines for Large-Scale\nAdvanced Analytics. In 33rd IEEE International Conference on Data Engineering,\nICDE 2017, San Diego, CA, USA, April 19-22, 2017 . 535–546.\n[43] J. Stahnke, M. Dörk, B. Müller, and A. Thom. [n.d.]. Probing Projections: Inter-\naction Techniques for Interpreting Arrangements and Errors of Dimensionality\nReductions. 22, 1 ([n. d.]), 629–638. https://doi.org/10.1109/TVCG.2015.2467717\n[44] Martin Steiger, Jürgen Bernard, Sebastian Mittelstädt, Hendrik Lücke-Tieke,\nDaniel Keim, Thorsten May, and Jörn Kohlhammer. [n.d.]. Visual Analysis\nof Time-Series Similarities for Anomaly Detection in Sensor Networks. 33, 3\n([n. d.]), 401–410. https://doi.org/10.1111/cgf.12396\n[45] Ion Stoica. 2020. Systems and ML: When the Sum is Greater than Its Parts. In\nProceedings of the 2020 International Conference on Management of Data, SIGMOD\nConference 2020, online conference, June 14-19, 2020 .\n[46] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning\nwith neural networks. In Advances in neural information processing systems . 3104–\n3112.\n[47] Guohui Xiao, Diego Calvanese, Roman Kontchakov, Domenico Lembo, Antonella\nPoggi, Riccardo Rosati, and Michael Zakharyaschev. 2018. Ontology-Based Data\nAccess: A Survey. In Proc. of the 27th Int. Joint Conf. on Artificial Intelligence\n(IJCAI) . IJCAI Org., 5511–5519. https://doi.org/10.24963/ijcai.2018/777\n[48] Guohui Xiao, Roman Kontchakov, Benjamin Cogrel, Diego Calvanese, and Elena\nBotoeva. 2018. Efficient Handling of SPARQL Optional for OBDA. In Proc. of the\n17th Int. Semantic Web Conf. (ISWC) (Lecture Notes in Computer Science) . Springer,\n354–373.\n[49] Guohui Xiao, Davide Lanti, Roman Kontchakov, Sarah Komla-Ebri, Elem Güzel-\nKalayci, Linfang Ding, Julien Corman, Benjamin Cogrel, Diego Calvanese, and\nElena Botoeva. 2020. The Virtual Knowledge Graph System Ontop. In Proc. of\nthe 19th Int. Semantic Web Conf. (ISWC 2020) , Vol. 12507. 259–277.\n[50] Zhe Zhao, Zhiyuan Cheng, Lichan Hong, and Ed Huai-hsin Chi. 2015. Improving\nUser Topic Interest Profiles by Behavior Factorization. In Proceedings of the 24th\nInternational Conference on World Wide Web, WWW 2015, Florence, Italy, May\n18-22, 2015 , Aldo Gangemi, Stefano Leonardi, and Alessandro Panconesi (Eds.).\nACM, 1406–1416.",
  "textLength": 50786
}