{
  "paperId": "7501ef97ee8d08eb12229f816453d9df6f62a29f",
  "title": "MQRLD: A Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake",
  "pdfPath": "7501ef97ee8d08eb12229f816453d9df6f62a29f.pdf",
  "text": "Highlights\nMQRLD:AMultimodalDataRetrievalPlatformwithQuery-awareFeatureRepresentation\nand Learned Index Based on Data Lake\nMing Sheng,Shuliang Wang,Yong Zhang,Kaige Wang,Jingyi Wang,Yi Luo,Rui Hao\n‚Ä¢Thestudyoffersamultimodaldataretrievalplatform,whichsupportstransparentdatastoragebyusingdatalake\ntechnology,enablesrichhybridqueriesthroughtheusageofamultimodalopenAPI,andprovidesaquery-aware\nmechanism to optimize retrieval.\n‚Ä¢The study proposes a multimodal feature representation technique that converts raw multimodal data into\nrepresentative features and optimal data layouts, improving the effective retrieval of multimodal data.\n‚Ä¢Thestudyintroducesahigh-dimensionallearnedindexthatcanadaptivelyoptimizeitsinnerstructure,enhancing\nefficient retrieval of multimodal data.arXiv:2408.16237v2  [cs.DB]  8 Feb 2025\n\nMQRLD: A Multimodal Data Retrieval Platform with Query-aware\nFeature Representation and Learned Index Based on Data Lake‚ãÜ\nMing Shenga, Shuliang Wanga,‚àó, Yong Zhangb,‚àó, Kaige Wangc, Jingyi Wanga, Yi Luoaand\nRui Haod\naSchool of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China\nbBNRist, DCST, RIIT, Tsinghua University, Beijing 100084, China\ncSchool of Artificial Intelligence, Henan University, Zhengzhou 450046, China\ndSchool of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 101408, China\nARTICLE INFO\nKeywords :\nMultimodal data retrieval\nFeature representation\nHigh-dimensional learned index\nQuery-aware mechanismABSTRACT\nMultimodal data has become a crucial element in the realm of big data analytics, driving\nadvancements in data exploration, data mining, and empowering artificial intelligence applica-\ntions. To support high-quality retrieval for these cutting-edge applications, a robust multimodal\ndata retrieval platform should meet the challenges of transparent data storage, rich hybrid\nqueries,effectivefeaturerepresentation,andhighqueryefficiency.However,amongtheexisting\nplatforms, traditional schema-on-write systems, multi-model databases, vector databases, and\ndatalakes,whicharetheprimaryoptionsformultimodaldataretrieval,makeitdifficulttofulfill\nthese challenges simultaneously. Therefore, there is an urgent need to develop a more versatile\nmultimodal data retrieval platform to address these issues.\nInthispaper,weintroducea MultimodalDataRetrievalPlatformwith Query-awareFeature\nRepresentation and Learned Index based on Data Lake ( MQRLD ). It leverages the transparent\nstorage capabilities of data lakes, integrates the multimodal open API to provide a unified\ninterface that supports rich hybrid queries, introduces a query-aware multimodal data feature\nrepresentationstrategytoobtaineffectivefeatures,andoffershigh-dimensionallearnedindexes\nto optimize data query. We conduct a comparative analysis of the query performance of\nMQRLD against other methods for rich hybrid queries. Our results underscore the superior\nefficiency of MQRLD in handling multimodal data retrieval tasks, demonstrating its potential\nto significantly improve retrieval performance in complex environments. We also clarify some\npotential concerns in the discussion.\n1. Introduction\nMultimodal data, including both structured and unstructured data, continuously influxes in vast volumes from\nvarioussources.Thesedataoftenneedtobeintegratedtoextractcomprehensiveandmeaningfulinsights(Lymperaiou\nandStamou,2024).Thismakes\"MultimodalDataRetrieval\"acoredisciplineofthemoregeneraldomainof\"Bigdata\"\nresearch. For instance, in power systems, various devices continuously collect multimodal data (supervisory control\nand data acquisition logs, meteorological data, satellite remote sensing images, surveillance videos, etc) from diverse\nsources,andthegrowinginteractivedemandresponserequirespowercompaniestoefficientlyextracttargetdatafrom\nmassive datasets (Zhao et al., 2022). Similarly, large e-commerce platforms like Taobao and Amazon can provide\nmillions of users with billions of items (the products‚Äô names, prices, images, promotional videos, etc), and the users\nusually search for their desired products on these platforms (Belem et al., 2020; Zheng et al., 2021; Rasappan et al.,\n2024). The efficient handling of large-scale multimodal data is crucial for digging deeper insights, such as big data\nanalysis,dataexplorationanddatamining,anddrivinginnovationacrossdiversedomains,includingrecommendation\n(Wuetal.,2024),diseasepredictioninhealthcare(Andorraetal.,2024;Thukraletal.,2023),fakenewsdetectioninlaw\nenforcement (Xue et al., 2021), etc. Moreover, with the evolution and advancement of large-scale models, especially\nmultimodal large-scale language models, AI is endowed with the ability to accept multi-sensory inputs like humans\nand provide more human-like interactions.\n‚àóCorresponding author\nshengming@bit.edu.cn (M. Sheng); slwang2011@bit.edu.cn (S. Wang); zhangyong05@tsinghua.edu.cn (Y. Zhang);\nwkg@henu.edu.cn (K. Wang); jwang2024@bit.edu.cn (J. Wang); luoyi@bit.edu.cn (Y. Luo); haorui24@mails.ucas.ac.cn (R. Hao)\nORCID(s):0009-0002-8813-4558 (M. Sheng); 0000-0001-5326-7209 (S. Wang); 0000-0001-8803-2055 (Y. Zhang);\n0009-0009-8101-2030 (K. Wang); 0000-0003-0518-1608 (Y. Luo)\nM. Sheng et al.: Preprint submitted to Elsevier Page 1 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nAll these applications heavily rely on multimodal data retrieval, as it can significantly impact their real-world\nperformance. In multimodal data retrieval, the data can be handled as multimodal objects (MMOs), which consist of\ncomplexinformationthatcombinesstructureddata(usuallyrepresentedasnumericalattributes)andunstructureddata\n(usuallyrepresentedasvectorfeatures).Asthereal-worldscenarioshowninFig1,multimodaldataretrievalplatforms\narerequiredtosupportcomplexjointqueries(wecallthemrichhybridqueries),withthequeryresultsreturnedinthe\nform of MMOs. However, existing multimodal data retrieval platforms, such as traditional schema-on-write systems,\nmulti-model databases, vector databases, and data lakes, exhibit limited capabilities in achieving transparent storage\nandsupportingrichhybridqueriessimultaneously.Moreover,theirqueryefficiencyandaccuracytendtodegradewhen\nhandling large-scale datasets. Therefore, the main objective of research on multimodal data retrieval platforms is to\ndevelopaplatformthatcanefficientlyandeffectivelyretrieveMMOswithinlarge-scalemultimodaldatathroughrich\nhybrid queries alongwith the concern oftransparent data storage (Mishra andMisra, 2017). To achieve this,retrieval\nplatforms must overcome the following challenges:\nFigure 1: Querying in multimodal data, including structured query attributes (\"10$-20$\" and \"0-24 hours\") and vector\nquery attributes (\"CupColor.jpg\" and \"CupDescription.mp3\").\n(1)Storage layer: Transparent data storage . Users usually aim to retrieve target MMOs through query statements.\nAs shown in Fig 1, the user is more concerned with obtaining all the information of his desired products (MMOs),\nwhichincludestheproducts‚Äônames,prices,images,promotionalvideos,etc.However,multimodaldataofteninvolves\nheterogeneous sources, varied modalities such as text, images, and videos, and complex interrelationships among\ndifferenttypesofinformation,lackingafixedorganizationorpredefinedformat.Thismakesitchallengingtostoreand\naccess these data in a seamless manner without requiring users to understand the underlying complexities (Gao et al.,\n2020). To address this limitation, a multimodal data retrieval platform should enable the conversion and tracking of\ninformation within MMOs, thereby supporting transparent data storage.\n(2)Querylayer:Richhybridqueriesandquery-awaremechanism .Therichnessofmultimodaldataexplorationand\nminingdependslargelyonthequeryfunctionality,whiletheretrievalperformancereliesontheabilityoftheplatform\nto perceive query behaviors. Compared to the hybrid queries proposed in existing research (Wei et al., 2020), which\nsupport queries containing a single vector and a set of structured attributes, many real-world scenarios require more\ncomplexjoint queries.Thesequeries arereferredto asrichhybrid queries,whichsupport moreflexiblecombinations\nofabroaderspectrumof\"attributes\",whethertheycanbestructuredorvectorattributes(asdetailedinSection4.2).For\ninstance,inFig1richhybridquerystatement,\"pricedbetween10and20\"and\"deliveredtomorrow\"serveasstructured\nquery attributes, while the \"CupColor.jpg\" and \"CupDescription.mp3\" as vector query attributes. Additionally, real-\nworld query behavior is often skewed rather than average. To enhance the performance of multimodal data retrieval,\nplatformsmustalsobeawareofthesequerybehaviors.Therefore,amultimodaldataretrievalplatformshouldsupport\nrich hybrid queries and be capable of perceiving query behaviors to provide effective and efficient responses.\n(3)Featurerepresentationlayer:Effectivefeaturerepresentation .Acommonapproachtomultimodaldataretrieval\nisto\"perceive\"and\"understand\"multimodaldatabyrepresentingitasfeaturesandobtainingdatawithcorresponding\nfeatures (Wang et al., 2016). Given the vast amount of multimodal data, where different types emphasize distinct\ninformation, flexible feature embedding methods and unified measurement techniques are essential to accommodate\nvarying datasets. Additionally, query behaviors often prioritize specific features, impacting retrieval precision and\nrecall. This requires a data-aware and query-aware feature enhancement approach to dynamically adjust data layouts.\nTherefore,amultimodaldataretrievalplatformshouldhaveaunifiedframeworkforfeatureembedding,measurement,\nand enhancement to achieve effective feature representation.\n(4)Indexlayer:Highqueryefficiency .Efficientmulti-dimensionalandhigh-dimensionalindexesareakeysolution\nfor supporting efficient rich hybrid queries on MMOs. Therefore, a multimodal data retrieval platform needs an\nindexing structure capable of handling rich hybrid queries that include both vector features and structured attributes.\nM. Sheng et al.: Preprint submitted to Elsevier Page 2 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nMoreover, different users usually have diverse query patterns. For instance, on an e-commerce platform, new parents\ntypically search for baby products, whereas for students, their queries are typically focused on study supplies.\nConsequently,designingahigh-dimensionalindexingstructuretailoredforrichhybridqueries,alongwithmechanisms\nto adaptively update the index structure based on query workloads to reduce computational costs, has become a\nsignificant challenge.\nFor fulfilling the storage and retrieval functions for multimodal data, data platforms have been evolving to meet\nthe growing need for inferring meaningful insights from these data. Existing retrieval platforms can be categorized\ninto traditional schema-on-write systems, multi-model databases, vector databases, and data lakes. (1) Traditional\nschema-on-write systems such as PostgreSQL (Obe and Hsu, 2017) and MySQL (Oracle Corporation, 2023), store\ndata in predefined schemas and are commonly used for structured data management and transactional operations.\nThese systems are often inflexible when handling multimodal data, as they are constrained by their fixed schema,\nmaking it difficult to realize transparent storage and support complex hybrid queries. (2) Multi-model databases have\nbeenproposedtointegrateseveralschemamodelswithinasingle,integratedbackend,allowingthemtohandleseveral\ndataformatswithinoneunifiedsystem(LuandHolubov√°,2019;Leietal.,2024).Forexample,MongoDB(MongoDB,\nInc.,2023)canoffertransparentstorageforcertaindatamodalities,butitsstoragecapabilitiesarelimitedwhendealing\nwith wilder range of multimodal data, such as video and audio. Additionally, their functionality and efficiency are\nconstrainedwhenhandlingrichhybridqueries,especiallyforhigh-dimensionaldata.(3) Vectordatabases aredesigned\nspecifically for efficient retrieval of high-dimensional vector data through vector similarity searches(Pan et al., 2024).\nThese platforms, such as Faiss (Johnson et al., 2019), are specialized in managing vector data, which is often used\nfor machine learning or AI-powered applications. However, they typically focus only on vector queries and neglect\nthe original multimodal data storage and feature representation process. While highly effective for tasks involving a\nsingle type of data (e.g., image retrieval or document search), vector databases may struggle with rich hybrid queries\nthat involve multiple data attributes, such as a combination of vectors and structured attributes (Taipalus, 2024). (4)\nData lakes , such as Apache Hudi (Hudi, 2021), are large repositories that transparently store data in its original, raw\nformat sourced from diverse origins(Khine and Wang, 2018; Hai et al., 2023). Data lakes are commonly seen as a\ncost-effective solution for storing vast amounts of raw data, and support adding additional computational layers, but\nmost of them lack support for rich hybrid queries(Ren et al., 2021b,a; Xiao et al., 2022). Furthermore, the efficiency\nofqueriesindatalakescanbecompromisedduetoinsufficientattentiontofeaturerepresentationandindexingofraw\ndata. Therefore, in this study, we explore how to leverage the potential of the data lake to propose a more full-fledged\nmultimodaldataretrievalplatform.Inadditiontoprovidinginfrastructuraltransparentstorageandrichhybridqueries,\nwe focus on improving the effectiveness and efficiency of the data retrieval platform.\nFor improving the effectiveness of multimodal data retrieval, feature representation is critical. Effective feature\nrepresentationinvolvesconvertingrawdataintorepresentativefeaturesandoptimaldatalayout.Therefore,aretrieval\nplatformmustconvertdataintooptimalrepresentativefeaturesthroughfeatureembeddingandmeasurement,followed\nbyfeatureenhancementtooptimizethedatalayout.However,mostexistingmultimodaldataretrievalplatformsrelyon\nfixedmethodsforfeatureembeddingandlayoutadjustments,failingtointegratefeatureembedding,measurement,and\nenhancementintoaunifiedframeworkforachievingeffectivefeaturerepresentation.Specifically,someprogresseshave\nbeen made in feature embedding, measurement, and enhancement separately. Current research has proposed various\nembedding methods, ranging from single-modality embedding methods like doc2vec (Le and Mikolov, 2014) and\ngraph2vec (Grohe, 2020) to advanced multi-modality embedding techniques such as CLIP (Radford et al., 2021).\nExistingfeaturemeasurementandenhancementmethodsprimarilyfocusonoptimizingfeaturesforbuildingmachine\nlearningmodels,whereasfeaturemeasurementtechniquesaimtoidentifyfeaturesbestsuitedformodelconstruction,\nandfeatureenhancementmethods,suchasimputation(Awawdehetal.,2022),discretization(Doughertyetal.,1995),\nand feature creation (Rohrer, 2011; Falcon et al., 2022), target at improving model training performance. Although\nthese methods perform well on specific scenarios, they are not well-suited for enabling more effective retrieval on\nmultimodal data retrieval platforms.\nTo improve the efficiency of multimodal data retrieval, the index must be query-aware and support rich hybrid\nqueries. Current indexing methods mainly include traditional multi-dimensional indexes, vector similarity indexes,\nand multi-dimensional learned indexes. (1) Traditional multi-dimensional indexes such as grid file (Nievergelt et al.,\n1984)andR-tree(Guttman,1984)aredesignedtoefficientlysupportnumericqueries,particularlyonlow-dimensional\ndata, while having difficulties when faced with rich hybrid queries and lacked query-aware mechanism. (2) Vector\nsimilarityindexes suchasANNOY(Bernhardsson,2015)andHNSW(MalkovandYashunin,2018),excelintherealm\nof high-dimensional vector data indexing through distance computation to find similar vectors. Nevertheless, they\nM. Sheng et al.: Preprint submitted to Elsevier Page 3 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\ncannot handle rich hybrid queries performed on MMOs, and increasing data records can significantly decrease their\nqueryefficiency.(3) Multi-dimensionallearnedindexes extendstraditionalmulti-dimensionalstructuresbyemploying\nmachine learning models to quickly find expected results (Al-Mamun et al., 2024). This method offers a query-aware\nmechanism for indexes to optimize their internal structures by learning data layout and query behaviors. But still,\ncurrentworksinlearnedindexesprimarilyfocusonefficientlyprovinglow-dimensionalqueries,failingtoachieverich\nhybrid queries. Current data retrieval platforms typically achieve rich hybrid queries by constructing two separate\nindexes: a high-dimensional vector index for unstructured data and a multi-dimensional index for structured data\n(Wei et al., 2020). So far, exploring an efficient query method that supports rich hybrid queries with a query-aware\nmechanism, remains a challenge yet to be fully resolved.\nTherefore, we summarize the key problems of multimodal data retrieval platforms to be solved in this paper as:\n1. Howtoachievetransparentdatastorageandrichhybridqueries,whereuserqueriesinvolveflexiblecombinations\nof structured and unstructured attributes, and the final results are returned as MMOs?\n2. Howtoimplementaunifiedfeaturerepresentationtechniqueforfeatureembedding,measurement,andenhance-\nment to convert multimodal data into optimal features and data layouts, thereby improving the effectiveness of\nquery results?\n3. How to implement a query-aware high-dimensional index that manages massive vectors and structured data,\nsupporting efficient multimodal data retrieval?\nToaddresstheseproblems,wepresentamultimodaldataretrievalplatformwithquery-awarefeaturerepresentation\nand learned index based on data lake (MQRLD), which supports transparent data storage and rich hybrid queries\nempoweredbyourhigh-dimensionallearnedindexthatbenefitsfromeffectivemultimodaldatafeaturerepresentation\nand a query-aware mechanism to deliver optimal performance. The main contributions are as follows:\n1. We provide a seamless and flexible access method for multimodal data, including the utilization of data lake\ntechnology to support transparent multimodal data storage, and the usage of multimodal open API to support\nrichhybridquerieswhichreturnqueryresultsintheformofMMOs,basedonwhichaquery-awaremechanism\nis built to enhance query performance. (Section 4)\n2. Weproposeaunifiedmultimodaldatafeaturerepresentationtechnique,whichisdesignedtoconvertmultimodal\nraw data into representative features and optimal data layout by perceiving and understanding both data char-\nacteristics and query behaviors, in order to facilitate index building, thereby enhancing the query effectiveness.\n(Section 5)\n3. We present an efficient high-dimensional learned index built on feature representation that can adaptively\noptimize its inner structure under different feature data layouts and various task scenarios to support flexible\nand efficient data retrieval of multimodal data. (Section 6)\n4. We conduct extensive performance evaluations using real and synthetic datasets. The results demonstrate that\nMQRLD outperforms other methods in multimodal data retrieval tasks, showing its ability to significantly\nenhance retrieval performance in complex environments. (Section 7)\n2. Related Work\nIn view of the challenges mentioned above, we summarize the current works on data retrieval platforms, feature\nrepresentationtechniques,andtypicalmulti-dimensionalandhigh-dimensionalindexesusedindataretrievalplatforms.\nWe first enumerate data retrieval platforms in Section 2.1, focusing on architecture and functionality which include\nstorage, query options, feature representation functionality, and index type. Then in Section 2.2, we introduce the\nexisting techniques in feature representation domain. Finally, we conduct a more detailed comparison of multi-\ndimensionalandhigh-dimensionalindexesinSection2.3,focusingonStructureandFunctionality,whichincludeindex\nstructure, query types, and whether data-aware or query-aware techniques are applied to improve index efficiency.\n2.1. Related Work on Data Retrieval Platform\nTable 1 lists existing data retrieval platforms that support multimodal data retrieval. We can classify them into\ntraditional schema-on-write systems, multi-model databases, vector databases, and data lakes.\nM. Sheng et al.: Preprint submitted to Elsevier Page 4 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nTable 1\nComparison of data retrieval platforms on Type, Storage Layer, Query Layer, Feature Representation Layer, and Index\nLayer.\nStorage\nLayerQuery Layer Feature Representation Layer Index Layer\nName TypeTransparent\nData\nStorageRich\nHybrid\nQueriesQuery-\naware\nMechanismEmbedding Measurement Enhancement Index\nOurs data lake ‚úì ‚úì ‚úì ‚úì ‚úì ‚úìhigh-dimensional\nlearned index\nPostgreSQL\n(Obe and\nHsu, 2017)schema-on-\nwrite system% ‚úì‚úó % % % %multi-dimensional\nindex/ vector\nsimilarity index\nArangoDB\n(Belgundi\net al., 2023)multi-model\ndatabase ‚úì‚úó ‚úì‚úó % % % %multi-dimensional\nindex\nAzure Cosmos\nDB\n(Guay Paz,\n2018)multi-model\ndatabase ‚úì‚úó ‚úì‚úó % % % %multi-dimensional\nindex\nOrientDB\n(Ritter et al.,\n2021)multi-model\ndatabase ‚úì‚úó ‚úì‚úó % % % %multi-dimensional\nindex\nMongoDB\n(MongoDB,\nInc., 2023)multi-model\ndatabase ‚úì‚úó ‚úì‚úó % % % %multi-dimensional\nindex\nFaiss\n(Johnson\net al., 2019)vector\ndatabase% % % % % %vector similarity\nindex\nPinecone\n(Pinecone,\n2024)vector\ndatabase% % % % % %vector similarity\nindex\nADAMpro\n(Giangreco\nand Schuldt,\n2016)vector\ndatabase% % % ‚úì % %vector similarity\nindex\nCottontail DB\n(Gasser et al.,\n2020)vector\ndatabase% ‚úì‚úó % ‚úì % %vector similarity\nindex\nMilvus (Wang\net al., 2021)vector\ndatabase% ‚úì‚úó % ‚úì % %vector similarity\nindex\nHMDFF (Ren\net al., 2021b)data lake ‚úì ‚úì ‚úó % ‚úì % %multi-dimensional\nindex\nMHDP (Ren\net al., 2021a)data lake ‚úì % % ‚úì % % N/A\nMHDML\n(Xiao et al.,\n2022)data lake ‚úì % % % % % N/A\nTraditionalschema-on-writesystemsrequireafixedschemafordatastorage,organizedataincolumns,andutilize\ntraditionalone-dimensionalormulti-dimensionalindexesfordataretrieval.Thisstructureiswell-suitedformanaging\nstructureddata,forexample,PostgreSQLperformseffectivelyinsupportingbanktransactions(Roskladkaetal.,2019).\nHowever, this fixed schema approach is less adaptable to high-dimensional data. Although plugins like pgvector\n(pgvector, 2024) have been introduced to handle high-dimensional vector data processing and similarity search, they\nare unable to deliver the high performance and scalability required for large-scale applications.\nMulti-modeldatabasesaimtoaccommodatedifferentdatatypesbydesigningseparateschemamodels(e.g.,graph\nmodel for network data, document model for JSON and XML data, key-value model for ID-identified data, etc.).\nWhile this targeted design provides centralized management for various data types, it lacks generality and struggles\nto accommodate the increasing variety of data modalities. Furthermore, although most multi-model databases can\nsupport simple hybrid queries such as text content search combined with numeric search, they are typically confined\nM. Sheng et al.: Preprint submitted to Elsevier Page 5 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nto specific data modalities. Examples include ArangoDB (Belgundi et al., 2023), OrientDB (Ritter et al., 2021), and\nCosmosDB(GuayPaz,2018).Duetotheabsenceoffeaturerepresentationfunctionality,thesedatabasesareincapable\nof supporting semantic searches across different data modalities, thus limiting their effectiveness in multimodal data\nretrieval.\nStandardvectordatabases,suchasFaiss(Johnsonetal.,2019)andPinecone(Pinecone,2024),focussolelyonthe\nstorage and retrieval of vector-format data and use only vector similarity indexes. This limitation prevents them from\nexecutingrichhybridqueries.ExtensionslikeADAMpro(GiangrecoandSchuldt,2016),CottontailDB(Gasseretal.,\n2020), and Milvus (Wang et al., 2021) offer additional functionalities, such as simple hybrid queries(e.g. a structured\nattributeandavectorfeature)andfeatureembedding.However,theyexhibitsignificantflaws:1.Firstly,theyneglectthe\nstorageofrawdataandneedtoartificiallydistinguishbetweentheprocessingofdifferentdatatypes.2.Secondly,they\nlack MMO management processing, making it explicit to track back original data. 3. Thirdly, despite providing basic\nfeature embedding functionality, they cannot guarantee the quality of features, thus impacting search effectiveness.\n4. Finally, although they can support simple hybrid queries, this comes at the expense of significantly reduced query\nperformance. This heavy query process leads to exponential degradation in performance as the data records increase,\nmaking it difficult to handle large-scale data.\nData lake (J., 2010), introduced around 2010, is more flexible for different types of analyses, as no decisions\nregarding the modeling and processing of data have to be made in advance. Due to their low operational costs, high\nscalability, and flexibility, distributed file systems or object storages, such as the Hadoop Distributed File System\n(HDFS) (Borthakur et al., 2008), are commonly employed within data lakes for storing raw data (Schneider et al.,\n2024). However, this flexibility comes at the cost of lower robustness, as the raw data can barely be validated on\ningestion. In addition, most data lakes lack structural management of raw data and effective index structure, limiting\nsearchperformanceforlargedatavolumes.Furthermore,manyworksbuiltondatalakeareoftentailoredfordomain-\nspecific data, such as HMDFF (Ren et al., 2021b), MHDP (Ren et al., 2021a), and MHDML (Xiao et al., 2022) focus\non medical or healthcare data, lacking generality.\nExisting data retrieval platforms perform well within their specific application areas; however, none of them are\nlikelytosolveallthechallengesrelatedtomultimodaldataretrieval.Moreover,noneoftheseplatformsimprovequery\nperformance using query-aware or learned index techniques, resulting in suboptimal query efficiency.\n2.2. Related Work on Feature Representation\nToachieveeffectivemultimodaldataretrieval,amultimodaldataretrievalplatformneedstoincorporateadvanced\nmultimodal feature representation techniques. Multimodal feature representation can be categorized into three\nstages: feature embedding, feature measurement, and feature enhancement. Each stage involves different techniques\nand methods, addressing distinct aspects of feature representation. Feature embedding and measurement convert\nmultimodal data into representative features, which facilitate subsequent tasks on the platform. Feature enhancement\noptimizesthelayoutoftheserepresentativefeatures,therebyimprovingqueryeffectivenessacrossdifferentworkloads.\nAlthough some platforms support a variety of embedding methods, these methods are typically chosen manually\nand may not be the most suitable for some datasets or query workloads. Furthermore, these platforms often fail to\napply feature measurement and enhancement techniques to improve feature fidelity and generalization, limiting the\nrobustnessandadaptabilityoftheirembeddingmethodsacrossdifferentretrievalscenarios.Whileexistingmultimodal\ndata retrieval platforms lack a unified technique for multimodal feature representation, researchers have made some\nprogress in feature embedding, measurement, and enhancement separately.\nThe existing feature embedding technique embeds the semantic information from raw data into a unified\nrepresentation space, allowing data from different modalities to be compared and analyzed within the same space.\nFeatureembeddinghasbeenwidelystudiedandhasevolvedfromsimplemethodslikeBoWandTF-IDFtoadvanced\ntechniquessuchasWord2Vec(Mikolovetal.,2013),GloVe(Penningtonetal.,2014),andCLIP(Radfordetal.,2021),\nprovidingdiverseoptionsacrossdifferentdatamodalities.Thesedevelopmentshaveenabledpracticalapplicationslike\nYouTube‚Äôs content recommendation (Covington et al., 2016).\nFeaturemeasurementinvolvestheassessmentandevaluationofmultimodalfeatureembedding.Currently,feature\nmeasurement is mainly used to assess the quality, importance, and relevance of embedding features in predictive\nmodels. Commonly used metrics like Matthews Correlation Coefficient (MCC), F-value, and variance threshold\nevaluateafeature‚Äôsabilitytodistinguishbetweenclasses,whilemetricssuchasaccuracyandrecallreflecttheoverall\nperformance of predictive models.\nM. Sheng et al.: Preprint submitted to Elsevier Page 6 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nFeatureenhancementreferstothefurtheroptimizationandenhancementofexistingembeddings,aimingtoimprove\ntheirexpressivepoweroradaptabilitytodownstreamtasks.Keytechniquesincludedimensionalityreductionmethods\n(e.g.,principalcomponentanalysis(PCA),t-SNE(Cieslaketal.,2020),LinearDiscriminantAnalysis(LDA)),scaling,\nimputation, discretization, and feature creation.\nHowever, existing feature embedding, measurement, and enhancement techniques still face challenges in meeting\nthedemandsforaccuracyandefficiencyinmultimodaldataretrievaltasks.Forembeddingmethods,somearegeneral-\npurpose but lack generalization, while others, although more accurate, are specific to certain data types, making it\nchallengingtoadaptthemtoavarietyofmultimodaldataretrievaltasks.Similarly,manyexistingmeasurementmetrics\noversimplify the evaluation of multimodal data embedding models and overlook the specific requirements of query\ntasks. Furthermore, current measurement methods often neglect the high fidelity needed in feature representations,\nwhich is crucial for capturing the intricate relationships between multimodal data. Regarding feature enhancement\ntechniques, many are not tailored specifically to the multimodal data retrieval scenario. Methods primarily focused\non improving clustering algorithms may enhance retrieval efficiency but can be limited by clustering performance\nbottlenecks.Theseapproachesfrequentlydisregardoptimizationatthedatalevelandfailtoaddresshowdatalayoutand\nquery-specificinformationnaturecanimproveclusteringperformance.Therefore,amultimodaldataretrievalplatform\nrequiresamultimodalfeaturerepresentationtechniquewhichcanconvertmultimodaldataintorepresentativefeatures\nand optimal data layouts.\n2.3. Related Work on Indexes for Multimodal Data Retrieval\nTable 2 lists typical indexes used in multimodal data retrieval. We can categorize them into vector similarity\nindexes, traditional multi-dimensional indexes, and multi-dimensional learned indexes.\nVector similarity indexes treat data as points in hyperspace and return the top-k nearest neighbors by comparing\n\"distance\" between points. They are primarily classified into four categories: table-based index (e.g., E2 LSH (Datar\netal.,2004),IVFADC(Jegouetal.,2010)),tree-basedindex(e.g.,FLANN(MujaandLowe,2009),RPTree(Dasgupta\nandSinha,2013)),graph-basedindex(e.g.,KNNGraph(Dongetal.,2011),HNSW)andhybridindex(e.g.,DB_LSH).\nAlthough these indexes are effective for vector queries, they struggle to search combinations of structured attributes\nand vector features and lack data-aware and query-aware mechanisms, thus limiting their effectiveness in querying\nMMOs and under different query scenarios. Moreover, their query performance declines sharply with an increase in\ndata records.\nTraditionalmulti-dimensionalindexes,suchastreestructures(R-tree),gridstructures(GridFile),andSFC(space-\nfillingcurve)-basedstructures(z-order(Morton,1966)),aredesignedforstructureddataqueriesonmulti-dimensions,\nfor example, looking up a location on a map by its X and Y coordinates. Their effectiveness is confined to queries of\nstructured attributes on small datasets, as they are functionally unable to support complex queries and cannot adjust\ntheir index structure according to query workload and data layout.\nMulti-dimensionallearnedindexeshaveemergedasaprominenttopicinrecentyears.Kraskaetal.(2018)proposed\nthat an index can be treated as a model that learns to locate the expected result, making it particularly suitable for\naccelerating retrieval in large datasets. Specifically, multi-dimensional learned indexes build upon traditional multi-\ndimensional index structures and construct a learning model to enhance or replace the inner structure of the index by\nlearning data layout (e.g., LISA (Li et al., 2020), RSMI (Qi et al., 2020)) and query workload patterns (e.g., Qd-tree\n(Yangetal.,2020),Flood(Nathanetal.,2020),Tsunami(Dingetal.,2020)).Althoughsomemulti-dimensionallearned\nindexes support numeric and vector queries separately (e.g., ML (Davitkova et al., 2020), LISA), they do not support\nrich hybrid queries that simultaneously search for several numeric and vector data. Moreover, while these indexes\ndemonstrate good performance on multi-dimensional data, they still suffer from the \"curse of dimensionality\" when\nappliedtohigher-dimensionaldatasets.Additionally,theseindexesstruggletobalancethetrade-offbetweenprediction\nmodelaccuracyandcomplexity.High-accuracymodelstendtobecomemorecomplex,resultinginincreasedtimeand\nspace costs for index construction.\nConsequently,thewidelyusedmulti-dimensionalandhigh-dimensionalindexesinexistingdataretrievalplatforms\nstillhavelimitationsinsupportingmultimodaldataretrieval,including:(1) Lackofversatilefunctionalityofrichhybrid\nquery.Existingindexesfailtosupportrichhybridqueryprocessing.Althoughsomemulti-dimensionallearnedindexes\naccommodatemultiplequerytypes,theyexecutethesequeriessequentially,ratherthansimultaneously.Theseindexes\ntypically create separate index structures for high-dimensional vector data and multi-dimensional structured data,\npreventing them from effectively handling the complex demands of multimodal queries. (2) Inefficient performance.\nAsshowninTable2,mostindexingmethodslackdata-awareandquery-awarecapabilities,whichpreventsthemfrom\nM. Sheng et al.: Preprint submitted to Elsevier Page 7 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nTable 2\nComparison of multi-dimensional and high-dimensional indexes on structure and functionality.\nStructure LearnedMulti-\ndimensional\nIndexHigh-\ndimensional\nIndexData-\nawareQuery-\nawareNumeric\nQueryVector\nQueryHybrid\nQueryRich\nHybrid\nQuery\nOurs Tree ‚úì % ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì\nE2 LSH (Datar\net al., 2004)Table % % ‚úì % % % ‚úì % %\nIVFADC (Jegou\net al., 2010)Table % % ‚úì % % % ‚úì % %\nFLANN (Muja and\nLowe, 2009)Tree % % ‚úì % % % ‚úì % %\nRPTree (Dasgupta\nand Sinha, 2013)Tree % % ‚úì % % % ‚úì % %\nKGraph (Dong\net al., 2011)Graph % % ‚úì % % % ‚úì % %\nHNSW (Malkov and\nYashunin, 2018)Graph % % ‚úì % % % ‚úì % %\nDB-LSH (Tian\net al., 2024)Table,\nTree ‚úì % ‚úì % ‚úì % ‚úì % %\nZM (Wang et al.,\n2019)SFC ‚úì ‚úì % % % ‚úì % % %\nML (Davitkova\net al., 2020)Grid ‚úì ‚úì % % % ‚úì ‚úì % %\nLISA (Li et al.,\n2020)Grid ‚úì ‚úì % ‚úì % ‚úì ‚úì % %\nRSMI (Qi et al.,\n2020)Grid,\nSFC ‚úì ‚úì % ‚úì % ‚úì % % %\nQd-tree (Yang\net al., 2020)Tree ‚úì ‚úì % ‚úì ‚úì ‚úì % % %\nFlood (Nathan\net al., 2020)Grid ‚úì ‚úì % ‚úì ‚úì ‚úì % % %\nTsunami (Ding\net al., 2020)Grid,\nTree ‚úì ‚úì % ‚úì ‚úì ‚úì % % %\nCOAX (Ghaffari\net al., 2020)Grid,\nTree ‚úì ‚úì % ‚úì ‚úì ‚úì % % %\nSPRIG (Zhang\net al., 2021)Grid ‚úì ‚úì % ‚úì ‚úì ‚úì % % %\nPAW (Li et al.,\n2022b)Grid ‚úì ‚úì % ‚úì ‚úì ‚úì % % %\nLIMS (Tian et al.,\n2022)Cluster ‚úì ‚úì % ‚úì % % ‚úì % %\nLMSFC (Gao et al.,\n2023)SFC ‚úì ‚úì % ‚úì ‚úì ‚úì % % %\nELSI (Liu et al.,\n2023a)Tree ‚úì ‚úì % ‚úì ‚úì ‚úì % % %\nmaintaining stable performance across different datasets and query workloads, leading to decreased query efficiency.\nAlthoughsomeindexstructurescanbemanuallymonitoredandadjustedtofitvaryingquerydemands,thisdependence\nonhumaninterventionisimpracticalanderror-prone,especiallyindynamicorlarge-scaleenvironmentswherequery\nworkloads frequently change. In contrast, learned indexes can adjust their structure based on data-aware and query-\naware mechanisms. However, they still suffer from the curse of dimensionality when dealing with high-dimensional\ndata,renderingthemunsuitableformultimodaldataretrievalscenarios.Therefore,amultimodaldataretrievalplatform\nrequires a unified query-aware high-dimensional index that supports rich hybrid queries.\nM. Sheng et al.: Preprint submitted to Elsevier Page 8 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\n3. Framework\nThe MQRLD platform fulfills efficiently and effectively multimodal data retrieval by supporting transparent data\nstorageandrichhybridqueries.Itleveragesourinnovativemultimodaldatafeaturerepresentationtechniquetoenhance\nthe learned index structure, thereby improving query performance. As illustrated in Fig 2, the overall framework\ncomprisesthreecoremodules:thebackbonearchitecture(Section4),aunifiedmultimodaldatafeaturerepresentation\ntechnique(Section5),andquery-awarehigh-dimensionallearnedindexsupportingrichhybridqueries(Section6).The\nbackbonearchitecturenotonlyprovidesastandardizedstorageformatandqueryinterfaceformultimodaldatabutalso\npreserves quantified records for multimodal query behavior. To further improve retrieval operations, we implement\ntwo key methods to flesh out the backbone architecture. First, we employ a multimodal data feature representation\ntechnique to convert multimodal raw data into an optimal format suitable for indexing. Second, we design a query-\naware high-dimensional learned index structure to efficiently retrieve MMOs.\nFigure 2: MQRLD framework.\nFig 3 shows the main highlight and overall workflow of MQRLD. We use a data lake to seamlessly store and\nmanage multimodal data, serving as the backend storage of our platform (Section 4.1). Building upon it, we provide\nmultimodalopenAPI(MOAPI)toofferaunifiedqueryinterfaceforrichhybridqueries(Section4.2).Whileexecuting\neach query, we record a series of meaningful statistic data from query behaviors to build the query-aware mechanism\n(Section4.3),whichisusedtorefinesubsequentfeaturerepresentation(Section5)andhigh-dimensionallearnedindex\nconstruction(Section6).Themultimodaldatafeaturerepresentation(Section5)actsasabridgebetweenmultimodal\nraw data and data retrieval, facilitating the conversion of multimodal raw data into representative features through\nsteps of feature embedding, measurement, and enhancement. The embedding and measurement (Section 5.1) aim at\ndiscerning representative features for multimodal data on different scenarios. The embedded features then undergo\nthe enhancement process (Section 5.2), which can be seen as an optimization process including transformation and\nmovement in hyperspace based on data patterns and query behaviors, resulting in optimal features and data layout.\nFinally, the high-dimensional learned index construction (Section 6) fully utilizes the feature representation results\nto build a cluster tree structure to support rich hybrid query (Section 6.1). To further enhance performance, we\noptimize index‚Äôs inner structure based on query behaviors (Section 6.2), making the index efficient across different\nquery workloads for rich hybrid queries.\n4. MQRLD Backbone Architecture\nThe MQRLD backbone architecture, based on a data lake, functionally supports multimodal data transparent\nstorage, rich hybrid queries, and query behavior recording. We introduce transparent storage for multimodal data in\nSection 4.1, MOAPI for rich hybrid queries in Section 4.2, and statistic table for query-aware mechanism in Section\n4.3.\nM. Sheng et al.: Preprint submitted to Elsevier Page 9 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nFigure 3: MQRLD workflow.\n4.1. Transparent Storage for Multimodal Data\nMultimodal data retrieval requires storing large amounts of data, each with different modalities that need to be\nidentifiedandretrieved.Therefore,theprimarychallengeofstorageliesinhowtomanagethislarge-scalemultimodal\ndata through a transparent process, ensuring that the final query results are returned in the form of MMOs.\nA data lake is a prominent and well-established kind of data platform that stores multimodal data in MMOs,\nthereby preserving all flexible options for future multimodal data retrieval. MQRLD integrates popular open-source\ndatalakeApacheHudi(Hudi,2021),whichcanbeintegratedwithApacheSpark(Shaikhetal.,2019),enablingparallel\nprocessingoflarge-scaledata.ItutilizesDataFrame,whichisconceptuallyequivalenttoatableinarelationaldatabase,\ncorrespondingtoaMMOs.AsshowninFig4,DataFramecolumnsrepresenttheattributes(suchasimagetexture,text\nsemantics...)oftheMMO,embeddedbydifferentfeaturevectors.EachMMOcanberepresentedasvariousvectorsand\nnumeric data according to different query tasks, allowing MQRLD to facilitate versatile query tasks across different\nmodalities.Foreachattribute,theDataFramealsorecordstheembeddingmodelsforunstructureddataandtheHDFS\npath of the original raw data, enabling quick tracing back to MMO.\n4.2. Multimodal Open API (MOAPI) for Rich Hybrid Queries\nTo achieve our goal of conducting rich hybrid queries on MQRLD, a versatile and easy-to-use query interface\nis essential. The Jina API (JINA, 2024) is a well-behaved open API that is specifically designed for interpreting\nand interacting with multimodal data, providing a uniform interface specification for querying multiple data types.\nTherefore, our MOAPI utilizes Jina API as the query interface and defines four basic query types, including:\n‚Ä¢Numeric Equal (N.E) Query: Returns results that are equal to the given value of an attribute.\n‚Ä¢Numeric Range (N.R) Query: Returns results where the value of an attribute falls within the given range.\n‚Ä¢VectorKNN(V.K)Query:Givenaset ùëÉ,aqueryobject ùëû‚ààùëÉ,andapositiveinteger ùëò,returnsùëòobjectsinùëÉ,\ndenoted as V.K( ùëû,ùëò), such that‚àÄùëù‚ààV.K(ùëû,ùëò),ùëù‚Ä≤‚ààùëÉ‚àñV.K(ùëû,ùëò),ùëëùëñùë†ùë°(ùëû,ùëù)‚â§ùëëùëñùë†ùë°(ùëû,ùëù‚Ä≤).\nM. Sheng et al.: Preprint submitted to Elsevier Page 10 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\n‚Ä¢VectorRange(V.R)Query:Givenaset ùëÉ,aqueryobject ùëû‚ààùëÉ,andaqueryradius ùëü‚â•0,returnsallobjectsin\nùëÉwithin the distance ùëüofùëû, i.e., V.R(ùëû,ùëü)={ùëù‚ààùëÉ|ùëëùëñùë†ùë°(ùëù,ùëû)‚â§ùëü}.\nFigure 4: Transparent data storage and rich hybrid queries in MQRLD.\nOur rich hybrid queries can be defined as combinations of these basic queries, i.e., ùëû1‚®Åùëû2‚®Å...‚®Åùëûùëõ, where\nùëûùëñ‚àà{N.E, N.R, V.R, V.K }, and‚®Åis the \"combine\" operation that can either be‚ãÇor‚ãÉ.‚ãÇdenotes an intersection\nof all the query requirements, e.g., finding images contains a player in red jersey and a player in white jersey, and‚ãÉdenotes a union of all the query requirements, e.g. finding images contains a player in red jersey or a player in\nwhite jersey. Fig 4 shows some of the typical rich hybrid queries, which can efficiently handle complex data retrieval\nrequirements.\n4.3. Statistic Table for Query-aware Mechanism\nWe establish the query-aware mechanism by building a Query Behavior Statistic (QBS) table and recording a\nseries of insightful variables during the query execution. This query-aware mechanism impacts the optimization of\nboth feature representation and index structure via the QBS table, which plays a vital role in constructing MQRLD\nthrough(1)reflectingtheperformanceofquerytaskstofacilitatefeaturemeasurement(Section5.1.2),(2)identifying\nthe feature of multimodal data representation in different query contexts to improve data layout (Section 5.2.2), and\n(3) dynamically adjusting and improving index structure to provide faster and more accurate retrieval (Section 6.2).\nTable 3\nQuery behavior statistic table.\nQuery\nStatementQuery\nMultimodal\nObject SetQuery\nAttributesQuery Type Recall@K CBR Query TimeQuery\nAccuracy\nTask1 DF1 Em1, Em2 NE,VK R1 X1 T1 A1\nTask2 DF2 Em3 VR R2 X2 T2 A2\n... ... ... ... ... ... ... ...\nAs shown in Table 3, the statistic variables recorded in the QBS table are as follows:\nM. Sheng et al.: Preprint submitted to Elsevier Page 11 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\n‚Ä¢Query Statement: The input query statement.\n‚Ä¢Query Multimodal Object Set: The DataFrame involved multimodal objects in the query statement.\n‚Ä¢Query Attributes: The columns involved in the query statement. In hybrid queries, achieving accurate results\noften requires utilizing two data attributes, while in rich hybrid queries, we may introduce more data attributes\ninto the query process.\n‚Ä¢Query Type: The four basic query types performed on each attribute.\n‚Ä¢Recall@K: The proportion of correct matches among the top K retrieved results.\n‚Ä¢CBR(CrossBucketRate):Theevaluationmetricfortheindexeffectiveness,akintothe\"Inter-bucketTraversal\nRate\" or \"Cross-partition Traversal Rate\". We utilize the DataFrames partitions to correspond to the \"buckets\"\nor \"partitions\".\n‚Ä¢Query Time: The average query execution time for queries of the specific task.\n‚Ä¢Query Accuracy: The ratio between the query results and the ground truth.\nThe QBS table is populated in two ways: logs from MOAPI, and statistical data from the Spark engine. MOAPI\nprimarily handles query interpretation, supporting a wide range of rich hybrid queries that can be covered by the four\nbasic query types provided. As a result, its logs are comprehensive enough to fully capture these rich hybrid queries‚Äô\ncharacteristics.Fromitslogs,wecanextractdetailedinformationabouttheQueryStatement,QueryMultimodalObject\nSet, Query Attributes, and Query Type. The Spark engine is responsible for executing queries, from which we can\nobtain statistical information such as Query Time, the number of buckets traversed by the query, query results, and\ncalculated metrics including Recall@K, CBR, and Query Accuracy. The data of new queries can be continuously\nappended to the QBS table, and different combinations of columns in the QBS table can be used as distinct training\ndatasets, supporting the query-aware feature measurement (Section 5.1.2), feature enhancement (Section 5.2.2), and\nindex optimization (Section 6.2).\n5. Multimodal Data Feature Representation\nCurrent multimodal data retrieval platforms often fail to fully exploit the synergistic capabilities of the three\nfeature representation stages‚Äîembedding, measurement, and enhancement‚Äîto optimize data itself for the purpose\nof enhancing query performance. To fill this gap, our objective in feature representation is to derive the most\nrepresentative features and optimized data layout for multimodal data, based on insights from both the data itself\nand the queries executed upon it, to refine the quality of the high-dimensional learned index and thereby ultimately\nenhancequeryperformance.AsshowninFig5,thisprocesscanbedividedintotwoparts:(1) FeatureEmbeddingand\nMeasurement : We obtain optimal features of multimodal data using our unique measurement process by concerning\nintrinsicandextrinsicmetricstoselecttheembeddingmodelwiththehighestscore.Thisservesasthefoundationfor\nsubsequentoptimizationofdatalayout.(2) FeatureEnhancement :Weoptimizeoveralldatalayoutthroughhyperspace\ntransformation and movement by analyzing data patterns and learning from query behaviors, making index and rich\nhybrid queries more effective and efficient.\n5.1. Feature Embedding and Measurement\nFeatureembeddingistheprocessofconvertingmultimodalrawdataintoasetoffeatures,whilethemeasurement\nprocedure serves as an assessment to determine the effectiveness of the embedding model, aiding in the selection\nof the more representative features for subsequent retrieval tasks. The overall workflow of feature embedding and\nmeasurement is shown in Fig 6. In the feature embedding process, MQRLD offers an embedding model pool for\nembedding the given multimodal raw data. The embedded features are then subjected to the measurement process,\nwhere we score each model using a data-aware intrinsic metric and a query-aware extrinsic metric to select the\nembedding model with the highest score.\nM. Sheng et al.: Preprint submitted to Elsevier Page 12 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nFigure 5: Overview of multimodal data representation, including feature embedding and measurement, feature enhance-\nment, and optimization based on the query-aware mechanism.\nFigure 6: Workflow of feature embedding and measurement. Embedding vector from each model is input into the\nmeasurement process consisting of extrinsic and intrinsic metrics to calculate a score, and select the model with the\nhighest score.\n5.1.1. Embedding\nInthefieldoffeatureembeddingdomain,numerousexistingworkshaveproposeddiversetechniquesforefficiently\nrepresenting raw data. Table 4 lists some of the notable works that have proven successful in multimodal data feature\nembedding, which we have collected and consolidated into the embedding model pool to support the embedding\nof wide-ranging attributes of multimodal data. Nevertheless, there is still a critical need to select a suitable model\nfor different scenarios, as the performance of these embedding models can vary significantly depending on factors\nsuch as data characteristics and query requirements. For instance, compared to CLIP4Clip, X-CLIP uses temporal\nembedding models to capture dynamic actions and interactions over time, making it more suitable for the task of\nembedding a soccer match video. Conversely, CLIP4Clip can effectively capture static, frame-level information,\nwhichismoreappropriateforliferecordingsthatconsistoffragmentedinformationwithweaktemporalcorrelations.\nConsequently, measurement metrics are needed to measure the embedding features, demonstrating the effectiveness\nof each embedding model.\nM. Sheng et al.: Preprint submitted to Elsevier Page 13 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nTable 4\nPre-trained cross-modal embedding models in embedding model pool.\nEmbedding ModelImage\nEmbeddingText\nEmbeddingVideo\nEmbeddingAudio\nEmbedding...\nCLIP (Radford et al., 2021) ‚úì ‚úì % % ...\nCLIP4Clip (Luo et al., 2021) ‚úì ‚úì ‚úì % ...\nX-CLIP (Ma et al., 2022) ‚úì ‚úì ‚úì % ...\nAudioCLIP (Guzhov et al., 2022) ‚úì ‚úì % ‚úì ...\nOPT (Liu et al., 2021) ‚úì ‚úì % ‚úì ...\n... ... ... ... ... ...\n5.1.2. Measurement\nEvaluating feature embedding models for multimodal data is a challenging task because it requires consideration\nofgeneralization,fidelity,andsupportforhigh-qualitydownstreamquerytasks.Inthispaper,weproposeacombined\nmeasurement metric: a widely-used extrinsic metric, complemented by our innovative intrinsic metric, to provide a\ncoarse-to-fine-grainedevaluationparadigm.AsshowninFig6,wedefineascoringsystemtoevaluatetheeffectiveness\nof each modal, which is formulated as:\nùëÜùëêùëúùëüùëí=ùë§1ùëÜ1+ùë§2ùëÜ2+ùë§3ùëÜ3 (1)\nwhere the weights ùë§1,ùë§2,ùë§3are used to control the influence of different metrics. ùëÜ1is an extrinsic measurement\nmetric, focusing on the performance in downstream tasks, while ùëÜ2andùëÜ3are intrinsic measurement metrics,\nemphasizing generalization and fidelity ability, respectively.\nTo be specific, extrinsic metric ùëÜ1is obtained after executing the specific downstream task by measuring three\nmetrics:Recall@K,QueryAccuracy,andQueryTime,whicharerecordedintheQBStableduringthequeryprocess.\nIncontrast,intrinsicmetrics ùëÜ2andùëÜ3areobtainedbyanalyzingthedatasetitself,bycalculatingSilhouetteCoefficient\n(SC) (Rousseeuw, 1987) and Fr√©chet distance (FID) (Heusel et al., 2017), respectively.\nTheremainderofthissectionwillintroducethesetwometricsindetail,andfinallypresentexperimentalvalidations\ndemonstrating that our measurement pipeline improves the selection of feature embedding models.\nSilhouette Coefficient (SC)\nSC is measured from the perspective of the whole dataset. Effective feature embeddings should exhibit well-\nclusteredcharacteristics,meaningtheintra-classdistanceshouldbesmallandtheinter-classdistanceshouldbelarge.\nTherefore,weintroducetheSCmetrictoassessthequalityofclustersformedbyembeddingmodels.Specifically,we\nembed the features using ùëõcandidate embedding models:\nùëãùëõ={Model1(ùêº),Model2(ùêº),.....Modelùëõ(ùêº)}, (2)\nwhereùêºis the input data, and ùëãùëõis theùëõsets of features embedded from ùëõcandidate models. Then we cluster each\nset of features into ùëòclasses:\nCluster(ùëãùëõ,ùëò)={{ùë•ùëñ\n1}ùëò\nùëñ=0,{ùë•ùëñ\n2}ùëò\nùëñ=0,...,{ùë•ùëñ\nùëõ}ùëò\nùëñ=0}, (3)\nwhere Cluster is a common clustering method, which can be K-means (Macqueen, 1967) or GMM (Reynolds et al.,\n2009), and{ùë•ùëñ\nùëó}ùëò\nùëñ=0is the clustered features encoded by ùëÄùëúùëëùëíùëôùëó. Next, we obtain the score of ùëÜ2for each model by\ncalculating the SC value of the clustered features:\nScore ofùëÜ2forùëÄùëúùëëùëíùëôùëó=ùëÜùê∂({ùë•ùëñ\nùëó}ùëò\nùëñ=0), (4)\nwhereùëÜùê∂()is the function to calculate the SC value of a given cluster results.\nFr√©chet distance (FID)\nUnlike the global consideration of SC, FID focuses on feature quality for each individual MMO. Recent\nadvancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse\nM. Sheng et al.: Preprint submitted to Elsevier Page 14 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nimages, building a bridge between text and image. These studies use FID to reflect the difference between the\noriginal image and the generated image. Therefore, We use FID to judge the fidelity of feature embedding for\neach MMO. Specifically, for embedding model ùëÄùëúùëëùëíùëôùëó, we input the embedded features into the same pre-trained\nhigh-performance generative model like Stable Diffusion (Rombach et al., 2022) to get the reconstructed images ÃÇùêº‚Ä≤\nùëó,\nÃÇùêº‚Ä≤\nùëó=Diffusion(ùëãùëõ).Thenweinputreconstructedimagesinturnwiththeoriginalimages ÃÇùêºùëóintotheinceptionnetwork\nto get FID value FIDùëó=Inception(ÃÇùêºùëó,ÃÇùêº‚Ä≤\nùëó), and the score of ùëÜ3can be represented as:\nScore ofùëÜ3forùëÄùëúùëëùëíùëôùëó=1‚àíNormalization (FIDùëó) (5)\nA small FID value between the generated image and the original image indicates that the embedding model can\neffectively learn the key information from the original data.\nIntegratingbothcoarse-grainedandfine-grainedmeasurements,wederivethefinalinnerscore(combinationof ùëÜ2\nandùëÜ3).Thisinnerscoreisthenfusedwiththeexternalscore,allowingforacomprehensiveselectionoftheembedding\nmodel for different scenarios.\nExperimental Validations\nTodemonstratetheeffectivenessofourmeasurementmethod,weselectasetofimagefeatureembeddingmodels\nfor evaluation. The experiments utilize a real dataset AI Challenger, with evaluation methods including the SC\n(Silhouette Coefficient), IN (Intrinsic Measurement), and IN + EX (a combination of Intrinsic Measurement and\nExtrinsic Measurement). Note that IN itself can function independently in feature measurement, making it applicable\nfor cold start. The scoring results are calculated based on different evaluation methods:\nùëÜùëêùëúùëüùëí=‚éß\n‚é™\n‚é®\n‚é™‚é©ùëÜ2 if method=SC\nùë§2ùëÜ2+ùë§3ùëÜ3 if method=IN\nùë§1ùëÜ1+ùë§2ùëÜ2+ùë§3ùëÜ3if method=IN+EX(6)\nwhere the weights for the IN are ùë§2=0.3andùë§3=0.7. For the IN + EX, the weights are ùë§1=0.2,ùë§2=0.3and\nùë§3=0.5.\nIntheexperiment,weselecttheimagefeatureembeddingmodelsRN50,ViT-B/16,andRN50x64asexperimental\nbenchmarks. Fig 7 illustrates the scoring result of different feature embedding models across various evaluation\nmethods compared with the result of downstream query tasks. The x-axis represents different sizes of image samples\nand downstream query tasks in the experiment, and the y-axis represents the scoring results after normalization. The\nscoring results for the downstream query task show that RN50x64 achieves the highest score, followed by ViT-B/16\nand RN50, reflecting the actual effectiveness of their respective features in the real query task after embedding. This\nscoring method effectively distinguishes the performance of different embedding models for downstream query tasks\nand aligns with the actual scoring results. Comparing the scoring results from different evaluation methods (SC, IN,\nIN+EX)withthoseofthedownstreamquerytasks,wefindthattheSCscoringresultsshowmoregeneralization,as\nthey always fluctuate in a small interval. On the other hand, scoring results IN and IN + EX show more variation and\naligncloselywiththescoringresultsofdownstreamquerytasks.ThisdifferentiationarisesbecauseSCemphasizesthe\ngeneralizationabilityoftheembeddingmodel,whileINintegratesFIDwhichmeasuresthefidelityofimagefeatures,\nimprovingevaluationaccuracycomparedtoSC.ComparedtotheSCandINevaluation,wefindthatIN+EXprovides\namoreaccurateassessmentofeachembeddingmodel,closelyreflectingtheiractualeffectiveness.Thisindicatesthat\ntheproposedquery-waremechanism,whichfacilitatesthecalculationofEX,significantlyimpactstheoptimizationof\nfeature representation. Furthermore, although the IN evaluation alone is less precise, it remains capable of selecting\nrelatively suitable embedding models. Therefore, in the absence of a query-aware mechanism (e.g., during platform\ncoldstarts),theplatformmayexperienceadeclineinitscapacitybutcanstillmaintainnormalfunctionalityanddeliver\nreliable performance.\n5.2. Feature Enhancement\nA well-designed data layout of multimodal data is crucial for reducing index scans and thus accelerating queries.\nThe primary objective of indexing is to enable effective and efficient data retrieval, and an optimized data layout is\nfundamentaltothisprocess.Byphysicallyarrangingsimilarorrelateddatapoints,suchlayoutsreducethesearchscope\nand significantly enhance indexing performance, particularly in high-dimensional or large-scale scenarios. Currently,\nM. Sheng et al.: Preprint submitted to Elsevier Page 15 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nFigure 7: Comparative evaluation results of different embedding models. The combination of IN and EX can best simulate\nthe score value of downstream tasks.\nmost existing approaches, focus on optimizing the data layout through clustering methods. However, these methods\ntypically treat original data as input, neglecting intrinsic data characteristics and their potential impact on layout\noptimization. In contrast, we propose a novel approach that transforms original data in hyperspace to intrinsically\nimprove the optimization process of obtaining the optimal data layout. To be specific, as shown in Fig 8, we treat a\nhigh-dimensionalvectorasadatapointinhyperspace,andthenrelocateittoanoptimalnewlocationthroughaunique\n\"projection\", considering both the dataset characteristics and query behaviors (Section 5.2.1). The specific method of\nthis projection includes Hyperspace Transformation (Section 5.2.2) and Hyperspace Movement (Section 5.2.3). This\nprojection allows the most discriminative dimensions of the data to be emphasized, and similar feature vectors are\nclustered closely together. Then, we provide evidence (Section 5.2.4) demonstrating that our feature representation\nprocess significantly improves clustering performance. This, in turn, validates the effectiveness of our enhancements\nin optimizing the data layout. Additionally, we show that both learned indexes and vector similarity indexes achieve\nimproved performance when operating on the optimized data layout.\nFigure 8: An overview of feature enhancement. Features from Dataframe are represented as a matrix and undergo\nhyperspace transformation and movement with query-awareness, ultimately being organized into an optimal data layout\nthat exhibits efficient clustering performance.\n5.2.1. Awareness of Projection\nTherearetwoaspectswetakeintoaccountintheprojectionprocess,whicharethedatasetcharacteristicsandquery\nbehaviors. From the perspective of dataset, we have the following considerations: (1) Identify and retain information-\nrich dimensions of certain attributes ; (2)Discern underlying distribution of entire dataset ; (3)Utilize relationships\nbetween MMOs .\nM. Sheng et al.: Preprint submitted to Elsevier Page 16 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nThe first two considerations are applied during the hyperspace transformation phase in Section 5.2.2, while the\nthird consideration is applied during the hyperspace movement phase in Section 5.2.3. From the perspective of query\nbehaviors,differentqueriesmayfocusondifferentfeatureswhensearchingformultimodaldata.Therefore,weaddan\noptimization step to the process of hyperspace transformation to facilitate locating relevant data.\n5.2.2. Hyperspace Transformation\nThe hyperspace transformation converts all data points (corresponding to vectors) into new, optimal positions by\nconsidering the data characteristics. This is achieved through a transformation matrix ùëá, which can be derived and\noptimized in four steps, as detailed below:\nStep1: Represent Data with Matrix ùê∑\nAlltheMMOsaremanagedinDataFrame,fromwhereweselectcolumnsofDataFrametobeindexedandrepresent\nthemwithamatrix ùê∑,asshowninFig9,where ùê∑‚àà‚Ñùùëö√óùëõ,witheachrowinmatrix ùê∑correspondingtoanMMO. ùëöis\nthetotalnumberofrecordsintheDataFrame,and ùëõisthedimensionnumberofallselectedcolumnsafterembedding.\nTo enhance the features of multimodal data without further compressing the information and maintain traceability\nto the original dataset, we define an ùëõ√óùëõtransformation matrix ùëáthat establishes a one-to-one mapping. Then, we\nmultiply the original matrix ùê∑by theùëáto get enhanced feature vectors denoted as ùê∑ùëá, whereùê∑ùëá‚àà‚Ñùùëö√óùëõ, and\nùê∑ùëá=ùê∑ùëá.\nFigure 9: Elements in matrix D correspond to features in DataFrame.\nStep2: Compute the Covariance Matrix ùê∂\nWe then compute the covariance matrix ùê∂(ùê∂‚àà‚Ñùùëõ√óùëõ) ofùê∑. The covariance matrix ùê∂not only reflects the joint\ndistributioncharacteristicsofthedatabutalsorevealstheimportanceofeachdimensionsinfeaturevectors.Therows\nof matrixùê∂capture the distribution pattern of data records. Minimal fluctuations in the rows lead to small deviations\nfrom the mean, resulting in low covariance. The columns of matrix ùê∂represent the importance of each dimension,\npreserving the complete dimensional information for accurate mapping of the selected DataFrame columns.\nStep3: Define the Transformation Matrix ùëá\nThrough the decomposition of ùê∂, we can construct the transformation matrix ùëáapplied to the feature vectors. ùëá\ncomprises a rotation matrix ùëÖand a scaling matrix ùëÜ, i.e.,ùëá=ùëÖùëÜ. The rotation matrix ùëÖindicates the direction of\ndata transformation in hyperspace, while ùëÜis a diagonal matrix reflecting the importance of each vector dimension\nthroughthescalingofitsdiagonalelements,thusstretchingeachdimensionoffeaturevectors. ùëÖandùëÜarecalculated\nthough eigen decomposition on the covariance matrix ùê∂,ùê∂=ùëâŒõùëâùëá.\nAccording to the linear transformation relationship of matrices, we know that the eigenvectors in ùëâserve as the\nbasis for the linear transformation of hyperspace coordinates, and the square root of each eigenvalue in Œõrepresents\nthe scaling factor for each dimension in hyperspace. Therefore, we can obtain the rotation matrix ùëÖ=ùëâand scaling\nmatrixùëÜ=‚àö\nŒõfrom the eigen decomposition results of ùê∂, and the transformation matrix ùëá=ùëÖùëÜ.\nThe invertibility of the hyperspace transformation is an innegligible issue, because after performing indexing, the\nindexed data needs to be re-transformed into the original vector to support query operations on MMOs. Therefore,\nto ensure the reversibility of matrix ùëáand maintain the linearity of data transformation properties, we impose the\nfollowing constraints on ùëÖandùëÜ:\n(1)ùëÖ,ùëÜ‚àà‚Ñùùëõ√óùëõ.\n(2)‚àÄùë£ùëñ,ùë£ùëó‚ààùëÖ,||ùë£ùëñ||2=1,ùë£ùëá\nùëñùë£ùëó=0,‚àÄùëñ,ùëó‚àà{1,2,‚Ä¶,ùëõ},ùëñ‚â†ùëó.\n(3)ùëÜ=ùëëùëñùëéùëî(ùëé11,ùëé22,‚ãØ,ùëéùëõùëõ),ùëéùëñùëñ>0,‚àÄùëñ‚àà{1,2,‚Ä¶,ùëõ}.(7)\nM. Sheng et al.: Preprint submitted to Elsevier Page 17 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nConstraint (1) indicates that after the transformation, the dataset retains ùëõdimensions in the hyperspace with no\nloss of dimensional information, ensuring that the original dataset is easily regained through one-to-one mapping.\nConstraint (2) guarantees that ùëÖis an orthonormal matrix, ensuring that the columns of the matrix transformed by ùëÖ\nare independent of other columns, allowing us to focus on specific dimensions corresponding to the feature vectors\nduring query-aware optimization without affecting other dimensions. Constraint (3) specifies that matrix ùëÜmust be a\npositivedefinitematrixbecauseweonlyconsiderscalingthefeaturevectorsinthepositivedirection.Therefore,after\ncalculatingùëá, we can get enhanced feature vectors ùê∑ùëá, whereùê∑ùëá=ùê∑ùëá. These three constraints ensure that matrix\nùëáis invertible through the inverse operation by multiplying ùê∑ùëábyùëá‚àí1.\nStep4: Optimize ùëáby Query Awareness\nToimprovequeryefficiencyandaccuracyunderdifferentqueryscenarios,weadjustthehyperspacetransformation\nprocessbyoptimizing ùëáthroughlearningfromqueryworkloads.Tosimplifythisoptimizationprocess,wedecompose\nthe objective of optimizing ùëáinto optimizing ùëÖandùëÜ.\nFor a multimodal dataset ùê∑and a given query workload ùëÑ, we aim to achieve high query accuracy, minimal\nquery time, and low CBR when executing queries. Specifically, we seek to identify the optimal ùëÖ‚àó,ùëÜ‚àóto optimize\ntheseobjectivesasmuchaspossible.Therefore,weformulatethisasamulti-objectiveoptimizationproblemwiththe\nobjective function:\nmin\nùëÖ,ùëÜ[ùëìtime(ùê∑,ùëû;ùëÖ,ùëÜ),ùëìùê∂ùêµùëÖ(ùê∑,ùëû;ùëÖ,ùëÜ),‚àíùëìacc(ùê∑,ùëû;ùëÖ,ùëÜ)]\ns.t. Formula (7)(8)\nThe constraints of this problem are identical to those in equation (7). In this multi-objective optimization problem,\nthere are three objective functions:\n(1)ùëìùë°ùëñùëöùëí(ùê∑,ùëû;ùëÖ,ùëÜ): Query time, which we aim to minimize.\n(2)ùëìùê∂ùêµùëÖ(ùê∑,ùëû;ùëÖ,ùëÜ): CBR, which we aim to minimize.\n(3)ùëìùëéùëêùëê(ùê∑,ùëû;ùëÖ,ùëÜ): Query accuracy, which we aim to maximize.\nSelectingamulti-objectiveoptimizationmethodinvolvesaddressingtwokeychallenges.First,theobjectivefunctions\nare multiple competing black-box objectives, each with high evaluation costs. Second, both ùëÖandùëÜare high-\ndimensionalhyperparameters,andtheirsearchspaceisalsohigh-dimensional.Toaddressthesechallenges,weleverage\nthe Bayesian multi-objective optimization algorithm, MORBO (Daulton et al., 2022). MORBO employs surrogate\nmodels to approximate the relationship between parameters and the actual values of the objective functions, thereby\nreducing evaluation costs. It also designs a coordinated strategy that enables parallel exploration of multiple local\nregionswithinthehigh-dimensionalhyperparametersearchspacetoidentifyglobaloptima.Buildingontheadvantages\noftheMORBOalgorithm,weproposeamatrixoptimizationalgorithm,asdescribedinAlgorithm1.First,weinitialize\nmultipletrustregionsbasedontherecordsofQueryAccuracy,QueryTime,andCBRintheQBStable.Then,within\neachtrustregion,wetrainalocalGaussianprocessmodel,selectnewcandidatepoints,andevaluatenewobservations.\nSubsequently,we dynamicallyupdateeach trustregionbased onthe latestcandidatepoints andobservations.Finally,\nweapproximatetheParetoFront(PF)usingthenewobservationsandextracttheapproximatePareto-optimalsolution\nset P. To obtain unique ùëÖ‚àóandùëÜ‚àófor our feature enhancement process, we assign appropriate weights to the various\nmulti-objective observations based on task-specific requirements. The optimal solution is determined by computing\nthe weighted cumulative sum.\nHyperspace Transformation Evaluation\nThe hyperspace transformation process in MQRLD is executed offline, utilizing large-scale of historical data. To\nprovideacomprehensiveanalysisofthisprocess,weevaluateboththecomplexityandtheeffectivenessofhyperspace\ntransformation. The complexity is assessed through theoretical and experimental analysis, and the effectiveness is\nassessed by comparing hyperspace transformation with other similar approaches.\nFirst, we evaluate the complexity of hyperspace transformation. In Step 1, the matrix ùê∑is obtained,ùê∑‚àà‚ÑùùëÅ√óùëë,\nwhereùëÅisthenumberofdatapoints,rangingfrom0.30Mto210Minourexperiments,and ùëëisthedimensionalityof\neachdatapoints,rangingfrom3to1026.Storingthematrix ùê∑requiresspacecomplexityof ùëÇ(ùëÅ√óùëë).Step2involves\nthe computation and storage of the covariance matrix ùê∂, which has a time complexity of ùëÇ(ùëÅ√óùëë2)and a space\ncomplexity of ùëÇ(ùëë2). In Step 3, the matrices ùëÖandùëÜare derived through eigen decomposition on the covariance\nmatrixùê∂, which requires ùëÇ(ùëë3)time andùëÇ(ùëë2)space using the Jacobi method. In Step 4, the time complexity of\nM. Sheng et al.: Preprint submitted to Elsevier Page 18 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nAlgorithm 1: Matrix Optimization Based On MORBO\nInput:dataset matrix ùê∑, workloadùëÑ, objective functions ùëì={ùëìùëéùëêùëê,ùëìùë°ùëñùëöùëí,ùëìùê∂ùêµùëÖ}, max iteration ùëò, trust region\nparameters:{ ùëõ,ùêøùëñùëõùëñùë°,ùêøùëöùëñùëõ}\nOutput: approximate Pareto optimal solution set ùëÉ\n1:Initialize the trust regions ùëá={ùëá1,...,ùëáùëõ},ùëã0=‚àÖ,ùëå0=‚àÖ.\n2:fori in [0,k]do\n3:Fit local gaussian process models ùêøùê∫ùëÉ={ùëôùëîùëù1,...ùëôùëîùëùùëõ}\n4:(ùëÖùë°\nùëõùëíùë•ùë°,ùëÜùë°\nùëõùëíùë•ùë°)ùë°‚àà[1,ùëõ]=ùëÜùëíùëôùëíùëêùë°ùëÅùëíùë•ùë°(ùêøùê∫ùëÉ,ùëá)/*Select new candidates in each trust region*/\n5:((ùëåùë°\nùë°ùëñùëöùëí,ùëåùë°\nùê∂ùêµùëÖ,ùëåùë°\nùëéùëêùëê)ùë°‚àà[1,ùëõ])=BatchEval(ùëì,ùê∑,ùëÑ;(ùëÖùëõùëíùë•ùë°\nùë°,ùëÜùëõùëíùë•ùë°\nùë°)ùë°‚àà[1,ùëõ]\n6:ùëã0.ùëéùëùùëùùëíùëõùëë((ùëÖùë°\nùëõùëíùë•ùë°,ùëÜùë°\nùëõùëíùë•ùë°)ùë°‚àà[1,ùëõ]),ùëå0.ùëéùëùùëùùëíùëõùëë((ùëåùë°\nùë°ùëñùëöùëí,ùëåùë°\nùê∂ùêµùëÖ,ùëåùë°\nùëéùëêùëê)ùë°‚àà[1,ùëõ])\n7:forj in [0, n] do\n8:ùëáùëó,ùêøùëó=ùëàùëùùëëùëéùë°ùëé(ùëáùëó,(ùëÖùëó\nùëõùëíùë•ùë°,ùëÜùëó\nùëõùëíùë•ùë°‚Ä≤ùëåùëó\nùë°ùëñùëöùëí‚Ä≤ùëåùëó\nùê∂ùêµùëÖ‚Ä≤ùëåùëó\nùëéùëêùëê))\n9:ifùêøùëó<ùêøminthen\n10: Terminateùëáùëó\n11:ùëáùëó,ùëã0,ùëå0=ùëÖùëíùëñùëõùëñùë°ùëñùëéùëôùëñùëßùëí(ùêøùëñùëõùëñùë°,ùëã0,ùëå0)/*Addnewregion centroidandobjectivefunctionvalues into ùëã0,\nùëå0*/\n12:else\n13: updateùëáùëó‚Äôs region center\n14:end if\n15:end for\n16:end for\n17:ùëÉ=ùëÜùëíùëôùëíùëêùë°ùëÉùêπ(ùëãùëò,ùëåùëò)\n18:returnùëÉ\n(a)\n (b)\nFigure 10: The cost time of T construction and data transformation with different number of tasks and dataset sizes.\n(a)Dataset size = 100M, (b) Task Number = 40.\nthe MORBO algorithm in each iteration is ùëÇ(ùëõ3\nùëÑùêµùëÜ), whereùëõùëÑùêµùëÜis the number of records in the QBS table. Since\noptimization of matrix ùëárequiresùëòiterations, the overall time complexity for Step 4 is bounded by ùëÇ(ùëò√óùëõ3\nùëÑùêµùëÜ).\nThe space complexity for Step 4 is bounded by ùëÇ(ùëë√óùëõùëÑùêµùëÜ). After obtaining the optimal matrix ùëá, the matrix\nmultiplication ùê∑√óùëáis performed, requires ùëÇ(ùëÅ√óùëë2)time for computation and ùëÇ(ùëÅ√óùëë)space for storing the\nresults.Consequently,theoveralltimecomplexityofhyperspacetransformationisboundedby ùëÇ(ùëÅ)+ùëÇ(ùëõ3\nùëÑùêµùëÜ),and\noverall space complexity is bounded by ùëÇ(ùëÅ)+ùëÇ(ùëõùëÑùêµùëÜ). As shown in Fig 10, we conduct experiments to evaluate\nthe computational overhead of hyperspace transformation. The ùëáconstruction process includes steps 1 to 4, while\nthe data transformation refers to the process of ùê∑√óùëá. The experimental results in Fig 10(a) demonstrate that we\ncanfurtheracceleratethehyperspacetransformationbyincreasingparallelism.Fig10(b)indicatesthat,asthedataset\nM. Sheng et al.: Preprint submitted to Elsevier Page 19 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nTable 5\nFeature Selection and Feature Scaling Methods\nMethods Type InvertibilityOptimization\nbased on\nQuery AwareClustering\nImprove-\nment\"Last-\nmile\"\nModel Im-\nprovementTime\nComplexitySpace\nComplexity\nXGBoost(Chen\nand Guestrin,\n2016)Feature\nSelection% % % % ùëÇ(ùêæ‚ÑéùëÅùëôùëúùëîùëÅ)ùëÇ(ùêæùëÅ)\nLDA(Xanthopoulos\net al., 2013)Feature\nSelection% % % % ùëÇ(ùëÅ) ùëÇ(ùëÅ)\nPCA(Abdi and\nWilliams, 2010)Feature\nSelection% % % % ùëÇ(ùëÅ) ùëÇ(ùëÅ)\nCCNF(Li et al.,\n2022a)Feature\nScaling‚úì % % % ùëÇ(ùêæ‚ÑéùëÅùëôùëúùëîùëÅ)ùëÇ(ùêæùëÅ)\nDTization(Islam,\n2024)Feature\nScaling‚úì % % % ùëÇ(ùëÅùëôùëúùëîùëÅ) ùëÇ(ùëÅ)\nOursHyperspace\nTransfor-\nmation‚úì ‚úì ‚úì ‚úìInit_T:ùëÇ(ùëÅ)\nOpt_T:ùëÇ(ùëÅ)+\nùëÇ(ùëõ3\nùëÑùêµùëÜ)Init_T:ùëÇ(ùëÅ)\nOpt_T:ùëÇ(ùëÅ)+\nùëÇ(ùëõùëÑùêµùëÜ)\naK is the number of tree.\nbh is the depth of tree.\ncInit_T(Initialized_T) represents the initialization process of matrix T in hyperspace transformation, which corresponds to Steps\n1 - 3.\ndOpt_T(Optimized_T) represents the initialization and optimization process of matrix T in hyperspace transformation which\ncorresponds to Steps 1 - 4.\nsize increases, the hyperspace transformation process does not incur high computational overhead in Hudi‚Äôs parallel\ncomputing environment.\nNext, we evaluate the effectiveness of hyperspace transformation. Our analytical framework includes two parts:\ntheoreticalanalysisandexperimentalanalysis.(1) Theoreticalanalysis :AsshowninTable5,wecomparehyperspace\ntransformation with several feature selection and feature scaling approaches, which share similarities with our\nhyperspace transformation process but also have notable differences. Feature selection is a dimensionality reduction\ntechnique aimed at selecting the most relevant features to enhance the performance and accuracy of machine\nlearning algorithms (Dhal and Azad, 2022). Typical feature selection methods, such as XGBoost, PCA, and LDA,\nreduce dimensions and may eliminate crucial numerical features necessary for responding to specific queries, thus\npreventing the retrieval of certain information. On the other hand, feature scaling involves rescaling all features\nto a new scale, improving outcomes and speeding up computations in data processing and machine learning tasks\n(Alshaher,2021).Typicalfeaturescalingmethods,suchasCCNFandDTizatoin,cansupportinvertibilitybyusingan\nùëÇ(ùëÅ)space to store feature transformation information and could potentially replace our hyperspace transformation\nprocess. However, these feature scaling methods are not well-integrated with query-aware processes, cannot support\nclustering improvement, or facilitate high-quality \"last-mile\" model training. Additionally, they still exhibit higher\ntime complexity compared to our approach. (2) Experimental analysis : We conduct experiments on these feature\nscaling methods compared with hyperspace transformation. As shown in Fig 11. The Optimized_T achieves the best\naverage query time and recall rate, followed by Initialized_T, with the feature scaling methods CCNF and DTization\nperformingworsethanthehyperspacetransformation,showingthatourhyperspacetransformationcanbetterimprove\nthe efficiency and accuracy of multimodal data retrieval than other methods.\n5.2.3. Hyperspace Movement\nHyperspace movement further optimizes the feature vectors on the result of previous hyperspace transformation\nby moving similar data points in hyperspace together to get ÃÇùê∑, where similar data can be placed in adjacent places or\nthe same cluster. Thus it could benefit index construction and multimodal data retrieval.\nTo achieve this objective, we propose a new data movement method called Local Parallelized Gravitational Field\n(LPGF) which improves on an existing data movement method HIBOG (Li et al., 2021). HIBOG posits that in\nhyperspace,adatapointisattractedbyitsnearest ùêæsimilardatapoints,causingittomovetowardstheresultantforce\ndirection of these attractions. Though HIBOG shows effectiveness in gathering similar data, it suffers the following\nM. Sheng et al.: Preprint submitted to Elsevier Page 20 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\n(a)\n (b)\nFigure 11: The average query times(a) and recall-time curves(b) of different methods in MQRLD.\ndrawbacks:(1) Computationallyexpensive :Findingthenearest ùêædatapointsforforcecalculationrequiressortingall\ndata points, which requires huge computation, especially for large datasets. Additionally, the parameter ùêæneeds to\nbe set and adjusted, further increasing the computation time. (2) Anomalies of movement : The calculation of forces\nbetweendatapointsmaynotbesuitableforallscenarios.Sometimes,thisforcecancauseanomaliesinthemovementof\ntightlyclustereddata.(3) Weakparallelcapability :Thespatialrangeofthenearest ùêæneighborsisnotpredetermined,\nleading to the inability to perform grid partitioning and parallel processing of data. To overcome these shortcomings,\nweintroduce thepipelineof LPGF,whichconsists ofthreesteps toimprovethe aboveshortcomings,as showninFig\n12.\nFigure 12: Overview of hyperspace movement. Force computation of each point is calculated within radius R. All data\npoints are diveded into n subspace, corresponding to n segments of Dataframe, and computed parallel in different nodes\nof cluster in Apach Hudi.\nStep1: Boundary of Force Area\nTolowercomputationallyexpensive,wedefinearadius ùëÖtocalculatetheresultantforceappliedtoasinglepoint\ninstead of using the nearest ùêæpoints. For each data point ‚Éñ‚Éñ‚ÉóùëÉùëñ, we only consider the forces exerted on it by other\npointswithin ùëÖ,eliminatingtheinfluenceofpointsoutside ùëÖ.Thesizeof ùëÖisrelatedtotheclustercomputingpower\nM. Sheng et al.: Preprint submitted to Elsevier Page 21 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nand typically set between 5 ùê∫and 10ùê∫, whereùê∫is the average distance from each point in the dataset to its nearest\nneighbor,and ùê∫=1\nùëö‚àëùëö\nùëñ=1‚à•‚Éñ‚Éñ‚Éñ‚Éñ‚ÉóùëÉùëñ1‚àí‚Éñ‚Éñ‚ÉóùëÉùëñ‚à•2(‚Éñ‚Éñ‚Éñ‚Éñ‚ÉóùëÉùëñ1isthenearestpointof ‚Éñ‚Éñ‚ÉóùëÉùëñ).Thisimprovementavoidstheneedforextensive\ncalculations and removes dependence on the value of ùêæ, thereby reducing calculation time.\nStep2: Distinct Regions of Force\nTo avoid anomalies of movement, we make the strength and direction of applied force on point ‚Éñ‚Éñ‚ÉóùëÉùëñdepend on\nthe distance between ‚Éñ‚Éñ‚ÉóùëÉùëñand all the other points within radius ùëÖ. To be specific, for each ‚Éñ‚Éñ‚ÉóùëÉùëñ, the resultant force ‚Éñ‚Éñ‚Éñ‚Éóùêπùëñis\ncalculated by summing the applied force from all relevant points ‚Éñ‚Éñ‚ÉóùëÉùëñinùëÖ.‚Éñ‚Éñ‚Éñ‚Éñ‚Éóùêπùëñùëóapplied by‚Éñ‚Éñ‚Éñ‚Éñ‚ÉóùëÉùëñùëódepends on the distance\nbetween‚Éñ‚Éñ‚ÉóùëÉùëñand‚Éñ‚Éñ‚Éñ‚Éñ‚ÉóùëÉùëñùëó, which is calculated as Fig 13, where ùê∂is a constant slightly larger than 1. For example, taking a\nvalueof1+10‚àí1.Duringthecalculationofgravitationalforces,datapointsarerepresentedinvectorform.Withinthe\nradiusùëÖof data point ‚Éñ‚Éñ‚ÉóùëÉùëñ, the resultant force ‚Éñ‚Éñ‚Éñ‚Éóùêπùëñexerted on‚Éñ‚Éñ‚ÉóùëÉùëñis determined by ‚Éñ‚Éñ‚Éñ‚Éóùêπùëñ=‚àëùëÅ\nùëó=1‚Éñ‚Éñ‚Éñ‚Éñ‚Éóùêπùëñùëó, where N is the number\nof data points contained within the radius ùëÖof‚Éñ‚Éñ‚ÉóùëÉùëñ.\nFigure 13: Force calculation for each point is dependent on all points within its radius R, and the applying force varies for\npoints at different distances.\nStep3: Parallelization of Force Computation\nTo improve parallel capability, we split the dataset into multiple regions and utilize the partitioning mechanism\nof Apache Hudi to perform the computation in parallel within each region. This is achievable because we define a\npaddingareawithradius ùëÖtolimittherangeforprocessingeachpoint,ensuringthatthecalculationareaforeachdata\npointisconfinedtoafixedregion.AfterapplyingLPGF,wecanobtainadisplacementmatrix ùëÄ,ùëÄ‚àà‚Ñùùëö√óùëõ,which\nrepresents the displacement of each data point.\n5.2.4. Feature Representation Evaluation\nThe effectiveness of the feature representation is demonstrated through clustering performance, and clustering\nimproves data layout‚Äôs separability and compactness. Meanwhile, superior data layout optimization can further\nenhance indexing performance, thereby benefiting efficient and effective queries. To validate this, we conduct three\nexperiments. The first experiment compares the clustering performance before and after our feature representation\nprocess(Evaluation1).Thesecondandthirdexperimentsassessthequeryperformanceoflearnedindexes(Evaluation\n2) and vector similarity indexes (Evaluation 3) on the optimized data layout, respectively.\nEvaluation 1: Clustering Performance Evaluation\nIn this evaluation, we show that the data layout after feature representation significantly enhances clustering\nperformance. We compare five feature representation methods for enhancing clustering performance, which are T,\nHIBOG, LPGF, T + HIBOG, and T + LPGF. T represents Hyperspace Transformation. HIBOG and LPGF denote\nHyperspaceMovement.T+HIBOGaswellasT+LPGFrepresentthecombinationofHyperspaceTransformationand\nHyperspaceMovement.TheexperimentsareconductedusingtherealdatasetFlame(FuandMedico,2007)andemploy\nthree well-known clustering methods, which are K-means, Agglomerative (Sokal and Michener, 1958), and Density\nPeaksClustering(DPC)(RodriguezandLaio,2014),torepresentdiverseclusteringparadigms.Theselectionofthese\nmethodsisnotmeanttoimplyexclusivitybutrathertodemonstratethegeneralizationabilityacrossvariousclustering\nalgorithms.Tocomprehensivelyevaluatetheimpactofdifferentrepresentationmethodsonclusteringperformance,we\nusetheSilhouetteCoefficient,Calinski-HarabaszIndex,andNormalizedMutualInformationasexperimentalmetrics.\nThese metrics assess clustering effectiveness from different perspectives.\nM. Sheng et al.: Preprint submitted to Elsevier Page 22 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nTable 6\nClustering enhancement results of feature representation\nClustering Method Optimization Method Silhouette CoefficientCalinski-Harabasz\nIndexNormalized Mutual\nInformation\nK-meansUnoptimized 0.412 202.695 0.476\nT 0.425 214.568 0.506\nHIBOG 0.448 222.804 0.523\nLPGF 0.455 228.109 0.541\nT + HIBOG 0.472 245.784 0.576\nT + LPGF 0.503 260.556 0.603\nAgglomerativeUnoptimized 0.329 122.782 0.330\nT 0.342 129.652 0.377\nHIBOG 0.363 144.331 0.425\nLPGF 0.364 144.868 0.425\nT + HIBOG 0.395 151.546 0.489\nT + LPGF 0.423 159.651 0.541\nDPCUnoptimized 0.338 133.615 0.413\nT 0.345 135.781 0.614\nHIBOG 0.351 138.855 0.890\nLPGF 0.410 169.101 0.936\nT + HIBOG 0.455 176.252 0.958\nT + LPGF 0.491 184.451 0.992\naBold font indicates the best result, and italic font indicates the second-best result.\nbT: Apply hyperspace Transformation for feature enhancement.\ncHIBOG: Apply HIBOG in hyperspace movement for feature enhancement.\ndLPGF: Apply LPGF in hyperspace movement for feature enhancement.\nTable 6 presents the experimental results of different feature representation methods. Comparing the results of\nLPGF and HIBOG, we observe that LPGF significantly enhances clustering performance across various algorithms,\nsuggesting that optimizations in gravitational field design in LPGF improve intra-class compactness and inter-class\nseparation more effectively than HIBOG. Additionally, T + HIBOG and T + LPGF demonstrate that the hyperspace\ntransformation matrix T not only enhances the discriminative features of multimodal data but also substantially\nimproves clustering effectiveness.\nEvaluation 2: Multi-dimensional Learned Index Query Performance Enhancement Evaluation\nThe core concept of the learned index is to use a learned model to narrow down queries to a \"leaf node\",\nfollowed by \"last-mile\" search within each leaf node for the final retrieval step. However, existing learned indexes\nfacelimitationsintermsofbothmodelaccuracyandcomplexity.Byoptimizingthedatalayout,weimproveclustering\nperformance,whichnotonlyacceleratesthe\"lastmile\"searchbutalsoimprovesbothprecisionandrecall.Additionally,\nthe optimized data layout results in a better distribution of the data points, reducing the cost of model training, and\nthereby accelerating index construction. We design two experiments to validate the effectiveness of our proposed\nfeaturerepresentationmethodinenhancingtheperformanceofhigh-dimensionallearnedindexes.Thefirstexperiment\nshowsthatourapproachenablesthetrainingofa\"lastmile\"modelwhichfacilitatesacceleratedqueryprocessingand\nindex construction, while the second evaluates its impact on shortening the query time and index construction time\nof current state-of-the-art multi-dimensional learned indexes. The experiments utilize a generated dataset comprising\n2M records, with data distributed according to a Gaussian mixture distribution.\nIn the first experiment, we predict the performance of the \"last mile\" model by computing the cumulative\ndistribution function (CDF) of training labels for the \"last mile\" model, which we denote as keys. As shown in Fig\n14, the key of a data point ùëÉ1ùëõis computed as the sum of two distances. The first distance is from ùëÉ1ùëõto its cluster‚Äôs\ncentroidùê∂1, and the second distance is from ùê∂1toùê∂0which is the barycenter of all clusters‚Äô centroids. A smooth\nCDF curve suggests that the \"last mile\" model can effectively learn with high accuracy without the need for complex\nstrategies, thereby reducing the query time as well as index construction time. We compare the CDF curves of four\ndatasets,whichareOriginalDataset,HIBOGOptimizedDataset,LPGFOptimizedDataset,andT+LPGFOptimized\nDataset.AsshowninFig14,bycomparingthesmoothnessoftheCDFcurvesforeachdataset,wefindthatT+LPGF\nachieves the highest improvement in the smoothness among all optimized methods. Compared to HIBOG, LPGF\nM. Sheng et al.: Preprint submitted to Elsevier Page 23 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nsignificantly smoothens the global CDF curve, because the force calculation method in LPGF reduces the probability\nofanomalousdatamovement,resultingintighterdataclustering.T+LPGFnotonlyretainstheadvantagesofLPGF\nbutalsohighlightscriticalclusteringfeaturesthroughmatrixT,furtherenhancingtheclusteringcapabilities.Overall,\ntheT+LPGFmethodresultsintightermultimodaldataclustering,improvingthesmoothnessoftheCDFcurve,and\neffectively reducing the complexity of the training process involved in index construction.\nFigure 14: The CDF curves of different feature representation methods. Keys denote training labels of the \"last-mile\"\nsearch model. The CDF curve after T+LPGF is the smoothest, indicating that the \"last-mile\" model can be trained in a\nsimpler, faster, and more accurate manner.\nInthesecondexperiment,wedemonstratetheeffectivenessofourmethodinimprovingtheperformanceoftypical\nmulti-dimensional learned indexes by comparing average query times and index construction times. We select five\ntypical multi-dimensional learned indexes, which are ML, ZM, LISA, Flood, and LIMS. These learned indexes all\nshare the characteristic that their query performance is significantly influenced by the smoothness of the CDF curve\nwhich directly impacts \"last-mile\" searching. As shown in Fig 15(a) and Fig 15(b), we compare the average query\ntimes and index construction times of different indexes on the original, the LPGF optimized, and the T + LPGF\noptimized datasets. The experimental results indicate that both LPGF and T + LPGF methods reduce query time and\nindexconstructiontimefortheseindexingmethods,withT+LPGFexhibitingthemostsubstantialimprovements.On\naverage, the query time of T + LPGF is 39.6% faster than that of the original data set, and the model training time is\nreduced by 56.7%. The average query times of ML, ZM, and LIMS have significantly improved because these index\n(a)\n (b)\nFigure 15: The average query times(a) and index construction times(b) of different multi-dimensional indexes on original\ndataset and optimized dataset.\nstructures heavily depend on data distribution and clustering results, which confirms the effectiveness of our method\nin enhancing dataclustering performance. LISA and Flood, beingvariants of grid indexes, benefit fromour method‚Äôs\nabilitytomakedatamoretightlyineachgrid,resultinginfasterindexing.Foralltheindexes,ourmethodimprovesthe\nM. Sheng et al.: Preprint submitted to Elsevier Page 24 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\ndatalayout,leadingtoasmootherCDFcurvethatacceleratestrainingtimeandultimatelyreducesindexconstruction\ntime.\n(a)\n(b)\nFigure 16: The average query times(a) and recall-time curves(b) of different vector similarity indexes on the original dataset\nand optimized dataset. Fig (a) shows that the query time of each index exhibits the most significant decrease after applying\nT+LGPF. Fig (b) shows that T+LGPF achieves the fastest attainment of 100% recall for each index.\nEvaluation 3: Vector Similarity Index Query Performance Enhancement Evaluation\nVector similarity index identifies similar data points by comparing their distances in space. Our feature represen-\ntationprocessbringssimilardatapointscloser togetherinthespace,leadingtobetterclusteringresultsandimproved\nperformance of vector similarity indexes. To demonstrate the effectiveness of our method in enhancing vector index\nperformance,wecomparetheaveragequerytimeandtheTime-Recallcurve.Weselectfourvectorindexes:DB-LSH,\nIVF, HNSW, and LSH. The performance of these indexes is significantly influenced by the data distribution, with\ndatasetsexhibitingclearclusteringfeaturesshowingsubstantialimprovementsinindexperformance.AsshowninFig\n16(a) and Fig 16(b), we compare the average query time and recall rate for queries on the original, LPGF-optimized,\nandT+LPGF-optimizeddatasets.Fromthecomparisonresults,weobservethatallvectorsimilarityindexesshowthe\ngreatestimprovementsontheT+LPGF-optimizeddataset,bothintermsofaveragequerytimeandthetimerequired\ntoachieve100%recall.T+LPGFreducesthequerytimebyanaverageof42.8%comparedtotheoriginaldataset,and\nthe time to reach 100% recall is shortened by 55.8%. IVF shows the most significant improvement, as the T + LPGF\nmethodgroupssimilardatapointswithinthesameclusteraswellasclarifiestheboundariesofdifferentclusters.This\nenablestherecallratetoreach100%inashortertime.DB-LSH,ahybridindex,reliesonthehashfunctiontopartition\ndatabasedonthehashvalues.TheT+LPGFmethodimprovesthisbytightlyclusteringsimilardatapoints,effectively\nreducingtheerrorbucketratecausedbyhashcollisions.Additionally,theadvantageofT+LPGFmakesthecentroids\nof different clusters more distinct. Using these centroids to build the HNSW graph structure significantly reduces the\ncomplexity of each layer, enhancing the data navigation efficiency.\n6. High-dimensional Learned Index\nTo fully leverage the advantages of the \"Multimodal Data Feature Representation\" introduced in Section 5 and\nfurther enhance multimodal data retrieval efficiency, we propose a high-dimensional learned index. Unlike multi-\ndimensionallearnedindexeslikeFloodandTsunami,etc.whicharerestrictedtomultiplenumericqueries,orMLand\nLIMS,etc.whichonlysupportsinglenumericqueriesandvectorqueries,ourindexisdesignedforrichhybridqueries\nandperformsbetteronbothnumericandvectorqueriesthantheseindexes.Comparedtovectorsimilarityindexessuch\nasFLANNandHNSW,etc.whichfocusonvectorqueriesonhigh-dimensionaldata,ourindexprovidesmoreversatile\nqueryoptionsandoffersbetterefficiencyandeffectivenessonlargedatasets.Ourindexhierarchicallyorganizesclusters\nand their sub-clusters to form a cluster tree, where the index of the leaf nodes is established through a \"last-mile\"\ntrainingapproach.Furthermore,theindex‚Äôsinnerstructurecanbeoptimizedthroughourquery-awaremechanism.An\noverview of our index is illustrated in Fig 17, detailing both the construction and optimization processes. During the\nconstruction phase, divisive hierarchical clustering is employed to form clusters at various levels, corresponding to\nM. Sheng et al.: Preprint submitted to Elsevier Page 25 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\ndifferent levels within the tree structure, with the leaf nodes representing the most granular, indivisible sub-clusters,\nthereby substantially enhancing the precision of the \"last-mile\" training for the index. Concurrently, the optimization\nphase employs a query-aware mechanism to refine the internal structure of the index, improving its adaptability to\ndiverse query demands. The detailed processes of index construction and optimization are elaborated in Section 6.1\nand Section 6.2, respectively.\nFigure 17: The index construction process consists of divisive hierarchical clustering, cluster tree building, and query-aware\noptimization.\n6.1. Index Construction\nThe process of constructing the index comprises two primary stages, which are divisive hierarchical clustering\nwith \"last-mile\" training and cluster tree construction.\n6.1.1. Divisive Hierarchical Clustering with \"Last-mile\" Training\nThis stage splits data points through recursive partitioning, creating nested clusters that progressively divide the\ndataset into smaller and more refined subsets, delineating the hierarchical level of each subset. This nested clustering\napproach enables a hierarchical narrowing of the search scope, followed by \"last-mile\" training in the most granular\nsubsets for faster lookup. The entire Divisive Hierarchical Clustering algorithm can be described as an iterative step\nof \"Division\" and \"Training based Evaluation\", as outlined in Algorithm 2.\nStep 1: Division\nIn the division step, the intermediate result set from the previous iteration is further divided into smaller subsets,\nwhilethetraining-basedevaluationstepassesseswhetherthesubsetsobtainedfromthepreviousiterationhavereached\nan optimal distribution. If optimal distribution is reached, the division stops, and the training results for each optimal\nsubsetaresavedforthe\"last-mile\"search.Otherwise,thedivisioncontinues.Theiterationstopswhenallsubsetsfrom\ntheprevious iterationhave achievedan optimaldistribution,resulting inthe finalclusteringhierarchy andcompleting\nthe \"last-mile\" training for all minimal subsets.\nWe begin by treating the entire dataset ÃÇùê∑, processed through the previous multimodal data feature representation\nprocess,asasinglecluster.UsingtheDPCalgorithm,wethenpartitionthisdatasetintoinitialsub-clusters {ÃÇùê∑ùëñ}.The\nselection of the DPC algorithm is based on evaluations of multiple metrics, including data division time, depth of\nM. Sheng et al.: Preprint submitted to Elsevier Page 26 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nTable 7\nData division results of different cluster methods\nCluster Method Number of Clusters Data Division Time(s) Depth of Index Tree Avg Query Time(ms)\nK-meansK = 2 1276.92 5 72.06\nK = 3 877.06 3 26.55\nK = 4 611.45 2 9.11\nAgglomerative - 1135.66 2 7.14\nDPC - 674.54 2 6.42\na\"-\" indicates that the number of clusters is determined by the algorithm itself.\nindex tree, and average query time. As presented in Table 7, different clustering methods are employed in Algorithm\n2.OurfindingsindicatethattheDPCmethodalignsmostsuitablywithourdatadivisionstrategy.WhiletheK-means\nalgorithmachievesalowerdatadivisiontimeatK=4comparedtotheDPCalgorithm,itreliesonamanuallydefined\nparameter K. Conversely, although the Agglomerative algorithm does not necessitate manual parameter setting, its\ncomputational complexity is higher as evidenced by significantly longer data partitioning time when compared to\nDPC.TheDPCalgorithmiscapableofautomaticallyascertainingtheoptimalnumberofsub-clusters,anddetermining\nclusters‚Äô centroids jointly by density and distance. Moreover, DPC utilizes the results of LPGF where data points are\nmoredenselygrouped,enhancingtheclusteringoutcomesandensuringmoreuniformdatadistributionwithinsubsets.\nStep 2: Training based Evaluation\nFollowingthestepofobtaininginitialsub-clusters {ÃÇùê∑ùëñ},weassesswhetherthedatapointsineachsub-clusterare\nwell-distributedtosupportaccurate\"last-mile\"trainingandthecorrespondingefficientsearching,therebydetermining\nwhether further subdivision of the cluster is necessary. We represent the centroid of ÃÇùê∑ùëñasùê∂ùëñusing the Average Mass\nmethod (Protter and Morrey, 1970), and calculate the distance from each point ùëùinÃÇùê∑ùëñtoùê∂ùëñ. If the points in ÃÇùê∑ùëñare\nsufficiently uniform, distances of all points in it should be evenly spread, so that a simple linear regression model is\nsufficient to fit their distribution. Therefore, we predict the search position ùë£of a given lookup point ùëùusing a model\nùë£=ùêπ(ùëù)‚àó|ÃÇùê∑ùëñ|,whereùêπ(ùëù)istheestimatedCDFobtainedthroughlinearregression,whichestimatesthelikelihood\nthat a point‚Äôs distance to ùê∂ùëñis less than or equal to the distance from the lookup point ùëù. The accuracy of this model\npredictionvalidatesthegoodnessthattheCDFfitsandfurtherreflectsthedistributionofthepointsin {ÃÇùê∑ùëñ}.Therefore,\nwe want the hit ratio of prediction for all points in the cluster to be greater than a given threshold ùõø. Letùë£(ùëù)be the\npredicted searching position of point ùëùwhose actual position is ùë£ùëé(ùëù), our requirement is formulated as:\n1\n|ÃÇùê∑ùëñ|‚àë\nùëù‚ààÃÇùê∑ùëñùêºùë†ùê∏ùëûùë¢ùëéùëô(ùë£ùëé(ùëù),ùë£(ùëù))‚â•ùõø (9)\nwhere the parameter ùõøis set to the optimal value 95.1% based on tuning with considerations of tree depth and query\ntime (see Fig 18(a)). If this requirement is fulfilled, the points in ÃÇùê∑ùëñare uniform enough and do not require further\npartitioning. In this scenario, the model ùêπ(ùëù)will be stored for predicting the search position within ÃÇùê∑ùëñ. Otherwise,\nÃÇùê∑ùëñmust be further subdivided to achieve a better data distribution within its sub-clusters.\nThroughexperimentalverification,weset ùõøto95.1%.Theexperimentsalsodemonstratethatthedepthofourtree\ndoes not significantly increase with the rapid growth of data volume. Detailed results are shown in Fig 18(a)-(c).\nFig 18(a) shows the cluster tree depth (we set the depth of the root node to 0) and average query time for various\nùõøvalues. It can be observed that the best average query performance is achieved when ùõø=95.1%. Although smaller ùõø\nvaluescanfurtherreducetheclustertreedepth,theyleadtolargerleafnodes,requiringthescanningofmoreirrelevant\ndatatoobtainqueryresults.Conversely,higher ùõøvaluesincreasetheclustertreedepth,resultinginmorenodetraversals\nduring the query process.\nFig18(b)presentsexperimentalresultsonthedepthoftheclustertreeforUniform,GuassMix,andSkeweddatasets\nwith different dataset sizes. The depth of the cluster tree increases gradually with the data volume, but the overall\ngrowth trend of the tree layers becomes more gradual. From the perspective of data distribution, the Skewed dataset\nresultsinthedeepestclustertree,reachingfivelayers.ThemaximumdepthsoftheclustertreesfortheGuassMixand\nUniformdatasetsarethesame,buttheaveragedepthoftheclustertreeforGuassMixissignificantlygreaterthanthat\nfor Uniform. Though data skewness causes some tree branches to be more deeply divided, the depth does not grow\nexcessively using our optimized feature representation strategies.\nM. Sheng et al.: Preprint submitted to Elsevier Page 27 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\n(a)\n (b)\n (c)\nFigure 18: Calculation of ùõø(a), depth of index tree(b) and leaf node characteristics for different dataset sizes(c). The results\nfrom (b) and (c) demonstrate that as the dataset size increases, there is a gradual convergence in both the depth of our\nindex tree and the number of leaf nodes, rather than a significant increase.\nFig 18(c) shows the number of leaf nodes, and the average data volume in the leaf nodes for Uniform, GuassMix,\nandSkeweddatasetswithdifferentdatasetsizes.Asthedatavolumeincreases,boththenumberofleafnodesandthe\naverage data volume within the leaf nodes increase. Observing the overall trend in the growth of leaf nodes, the data\nvolume within each leaf node increases significantly, while the number of leaf nodes does not increase significantly.\nThis is because the data-aware partitioning strategy controls node splitting based on the average error value, which is\nindependent of the data volume.\nAlgorithm 2: Divisive Hierarchical Clustering\nInput:datasetÃÇùê∑to be divided\nOutput: an arrayùê¥containing all subsets of ùëÜ\n1:Initialize a queue of datasets ùê∑ùëéùë°ùëéùë†ùëíùë°ùëÑùë¢ùëíùë¢ùëí ={}\n2:{ÃÇùê∑ùëñ}=ùê∑ùëÉùê∂(ÃÇùê∑),ùê∑ùëéùë°ùëéùë†ùëíùë°ùëÑùë¢ùëíùë¢ùëí.ùëùùë¢ùë†‚Ñé ({ÃÇùê∑ùëñ.ùëüùëíùë†ùëúùëüùë°()})\n3:while!ùê∑ùëéùë°ùëéùë†ùëíùë°ùëÑùë¢ùëíùë¢ùëí.ùëñùë†ùê∏ùëöùëùùë°ùë¶ ()do\n4:ùëÜ=ùê∑ùëéùë°ùëéùë†ùëíùë°ùëÑùë¢ùëíùë¢ùëí.ùëùùëúùëù ()\n5:{ÃÇùëÜ},{ÃÇùê∂}=ùê∑ùëÉùê∂(ùêøùëÉùê∫ùêπ(ùëÜ))/*{ÃÇùëÜ}is set of sub-clusters of S, and {ÃÇùê∂}is set of centroids of {ÃÇùëÜ}*/\n6:ùê∂ùëù=ùëîùëíùë°ùê∂ùëíùëõùë°ùëüùëúùëñùëë({ÃÇùê∂})/*computing parent cluster S‚Äôs centroid using sub-clusters‚Äô centroid set {ÃÇùê∂}*/\n7:{ÃÇùëÜ‚Ä≤},{ÃÇùê∂‚Ä≤}=ùëüùëíùë†ùëúùëüùë°({ÃÇùëÜ},{ÃÇùê∂},ùê∂ùëù)\n8:forÃÇùëÜùëñin{ÃÇùëÜ‚Ä≤}do\n9:{ùëò}=ùëëùëñùë†ùë°(ùê∂ùëñ,ùëù‚ààÃÇùëÜùëñ)\n10:construct position prediction model ùë£ùëñ=ùêπùëñ(ùëò)‚àó|ÃÇùëÜùëñ|, whereùêπùëñis the CDF model obtained from linear\nregression, formulated as ùêπùëñ(ùëò)=ùëé‚àóùëò+ùëè\n11:ùê¥.ùëéùëëùëë(ÃÇùëÜùëñ)\n12:if1\n|ÃÇùëÜùëñ|‚àë\nùëù‚ààÃÇùëÜùëñùêºùë†ùê∏ùëûùë¢ùëéùëô(ùë£ùëé(ùëù),ùë£(ùëù))‚â§ùõøthen\n13:ùê∑ùëéùë°ùëéùë†ùëíùë°ùëÑùë¢ùëíùë¢ùëí.ùëùùë¢ùë†‚Ñé (ÃÇùëÜùëñ)\n14:end if\n15:end for\n16:end while\n17:returnùê¥\n6.1.2. Cluster Tree Construction\nFollowingthedivisivehierarchicalclusteringstage,thedatasetispartitionedintonestedsub-clusters,establishing\na hierarchical structure of data points. We construct a cluster tree based on this partition result, encapsulating the\norder of subsets at each hierarchical level and the organization of each subset. At the top of this tree is the root node,\nrepresentingtheuniqueclusterthatcontainstheentiredataset.Aswetraversedownwardthroughthetree,eachinternal\nnoderepresentsaclusterthathasbeensubdividedintosmallersub-clusters.Aninternalnodewith ùëõchildrensignifies\nthat its corresponding cluster has been partitioned into ùëõdistinct sub-clusters. Furthermore, for each parent cluster\nM. Sheng et al.: Preprint submitted to Elsevier Page 28 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nùê∑ùëùwithùëõsub-clusters{ùê∑ùë†}, we calculate ùê∑ùëù‚Äôs centroidùê∂ùëùby Average Mass method using {ùê∂ùë†}, which is a set of\ncentroids for {ùê∑ùë†}. The sub-clusters {ùê∑ùë†}are then sorted based on the distances from their centroids {ùê∂ùë†}toùê∂ùëùin\nincreasedorder.Thissortingiscrucialfordeterminingthesearchorderamongthesesub-clusters,whichisanimportant\naspect when constructing the cluster tree. The leaf nodes represent the final, and undivided subsets. This hierarchical\norganization enables efficient data retrieval by progressively narrowing down the search space from the root to the\npertinent leaf nodes.\nTo efficiently traverse the cluster tree, each node needs to store specific information that facilitates data searches\nwithin the node or navigation to a child. Based on this requirement, we record a tuple ùë°for each node. For non-leaf\nnodes,ùë°={ùê∂,ùëÖ,ùêø}, whereùê∂is the centroid of the cluster corresponding to the node, and ùëÖrepresents the cluster‚Äôs\nradius,definedastheEuclideandistancefrom ùê∂tothefarthestpointinthecluster.Together, ùê∂andùëÖdefinethesearch\nrange for the data points within the node. ùêøis a pointer that directs to a list of its child nodes where these nodes are\nsortedandcanbescannedbasedonthespatialorderbasedonthesub-clusters‚Äôdistancestotheparentnode‚Äôscentroid.\nForleafnodes, ùë°={ùê∂,ùëÖ,ùëÄ}.Again,ùê∂andùëÖlimitthesearchrangeofeachleafnode. ùëÄpointstothesearchmodel\ntrained in the previous step, which is used to predict the search position of an input lookup point, thereby facilitating\nrapid query responses.\nAlgorithm 3: Tree Node Ordering Optimization\nInput:initial tree node list ùêøùëñ, query workload ùëÑ\nOutput: optimized bucket order ÃÇùêøùëñ\n1:/*ùêµis an array of scanned times of each node after executing query workload ùëÑ,ùë°is query time*/\n2:ùêµ,ùë°=ùê∏ùë•ùëíùëêùë¢ùë°ùëíùëÑùë¢ùëíùëüùë¶ (ùëÑ,ùêøùëñ)\n3:ÃÇùêøùëñ=sortInDescendingOrder( ùêøùëñ, B)\n4:startIter=0, endIter=0\n5:whileendIter<|ÃÇùêøùëñ|do\n6:size=0\n7:whileendIter<|ÃÇùêøùëñ| and B[startIter]==B[endIter] do\n8:size+=1;endIter+=1;\n9:end while\n10:ifsize>1then\n11:ùêøùë†ùë¢ùëè=ùëÉùëíùëüùëöùë¢ùë°ùëéùë°ùëñùëúùëõùë† (ÃÇùêøùëñ,startIter,ùë†ùëñùëßùëí‚àí1)\n12:forùëôinùêøùë†ùë¢ùëèdo\n13:ùêøùëô=ùëÖùëíùëêùëúùëõùë†ùë°ùëüùë¢ùëêùë°(ÃÇùêøùëñ,startIter,ùë†ùëñùëßùëí‚àí1)\n14:ùë°ùëô=ùê∏ùë•ùëíùëêùë¢ùë°ùëíùëÑùë¢ùëíùëüùë¶ (ùëÑ,ùêøùëô)\n15: ifùë°ùëô<ùë°then\n16: ùë°=ùë°ùëô,ÃÇùêøùëñ=ùêøùëô\n17: end if\n18:end for\n19:end if\n20:startIter=endIter\n21:end while\n22:returnÃÇùêøùëñ\n6.2. Index Optimization\nRegarding query efficiency, different index structures are suited to different query workloads. In this section,\nwe focus on optimizing and reordering sibling nodes (sub-clusters) that share the same parent node (parent cluster)\nbased on the query-aware mechanism. This optimization aims to refine the index structure, thereby improving query\nperformance and efficiency. Note that in our optimization strategy, we focus exclusively on the arrangement of all\nchildnodesundereachparentnodewhilekeepingtheinheritancerelationshipsunchanged.Forinstance,considerthe\nscenarioinFig17wheretherearethreenon-leafnodes( ùëÅ1,ùëÅ4,ùëÅ5)intheleftbranchoftheclustertree,eachpointing\nto a list of their respective child nodes: ùêø1={ùëÅ4,ùëÅ5},ùêø4={ùëÅ12,ùëÅ13,ùëÅ14},ùêø5={ùëÅ15,ùëÅ16,ùëÅ17}. Our method\nM. Sheng et al.: Preprint submitted to Elsevier Page 29 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nis to optimize the order of nodes within each list (e.g. ùêø1can be reordered as ùêø1= {ùëÅ5,ùëÅ4}), without altering the\nparent-child relationships between nodes (e.g. ùëÅ5cannot be moved to ùêø4).\nThisoptimizationisbasedonourobservationthatquerypatternscanvarywithdifferentqueryworkloads,leading\nto certain nodes being accessed more frequently than others. By reordering the sibling nodes based on their access\nfrequency, the index can be made more efficient. For example, as illustrated in Fig 17, if our lookup point ultimately\nresidesinùëÅ18,withtheinitialtreestructuresixnodesarerequiredforscanning,whileafteroptimization,thenumber\nof nodes scanned could be reduced to three, significantly saving query time.\nNowwedelveintothedetailsofouroptimizationalgorithm.Eachnon-leafnode ùëÅùëñstoresapointertoasortedlist\nwhich we denote as ùêøùëñ.ùêøùëñcomprises all child nodes of the parent node ùëÅùëñ, initially ordered by their distance to the\nparentnode‚Äôscentroid.Weiterativelytraverseallnon-leafnodesand,in ùëñùë°‚Ñéiteration,ouroptimizationgoalistoadjust\nthe ordering of ùëÅùëñ‚Äôs child node list ùêøùëñto obtain a new list ÃÇùêøùëñ, so that the query time of a given query workload ùëÑis\nminimized:\nÃÇùêøùëñ=ùëéùëüùëîùëöùëñùëõ‚àë\nùëû‚ààùëÑùëÑùë¢ùëíùëüùë¶ùëáùëñùëöùëí(ùëû,ùêøùëñ) (10)\nThe optimization algorithm is shown in Algorithm 3. Its key idea is to move frequently accessed nodes to the\nbeginning of the list to reduce the number of cross-leaf scans needed to find these \"hot\" nodes. We record the visit\nfrequencyofeachnodeandsortthemindescendingorder,placingthemostfrequentlyvisitedbucketattheheadofthe\nlist. For nodes with the same visit frequency, we employ a brute-force approach to test each possible order and select\nthe one that results in the minimum query time.\n7. Experiments\nWe implement our MQRLD index in Scala and apply it to the data lake and vector database respectively. The\ndata lake is deployed in a distributed cluster environment consisting of three nodes, using Apache Spark as the\ncomputing engine and Apache Hudi for data lake management. Each node is equipped with 64-bit Ubuntu 18.04,\nIntel(R)Core(TM)i7-11700FCPU@2.50GHz,and16GBRAM.Thevectordatabaseisdeployedonasingle-node\nserverusingMilvusasitsmanagementframework.Theserver‚Äôshardwareconfigurationincludes64-bitUbuntu20.04,\ntwo Intel(R) Xeon(R) Gold 6226 CPUs @ 2.90GHz, 256 GB RAM, and an NVIDIA RTX A5000 GPU.\n7.1. Experimental Setup\n7.1.1. DataSets\nTable 8 shows the seven real-world datasets with different characteristics and three synthetic datasets we used in\nour experiments. The real-world datasets are also used in the previous work.\nTable 8\nSummary of datasets.\nDatasets Type Cardinality Dim. Size(GB)\nOSM real 105M 6 5.04\nTaxi real 184M 5 11.8\nStocks real 210M 4 13.2\nYelp real 6.99M 64 13.2\nColor real 1.28M 32 4.2\nForest real 0.56M 12 1.5\nAI Challenger real 0.30M N.A. 38\nSIFT1B real 0.1-100M 128 16.2\nLAION400M real 1-100M 1026 83.9\nUniform synthetic 1-100M 3-16 N.A.\nGuassMix synthetic 1-100M 3-16 N.A.\nSkewed synthetic 1-100M 3-16 N.A.\nM. Sheng et al.: Preprint submitted to Elsevier Page 30 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nOSM1is a spatial dataset consisting of 105 million records randomly sampled from North America in the\nOpenStreetMap dataset. Each record includes six attributes, such as ID, timestamp, GPS coordinates, etc. Taxi2is\nrandomlysampledfromrecordsofyellowtaxitripsinNewYorkCityin2018and2019.Weselectfiveattributesfrom\nthedataset:thenumberofpassengers,tripdistance,pickup,drop-offlocations,andtotalfare,toformafive-dimensional\ndataset.Stock3contains210millionrecords,consistingofdailyhistoricalstockpricedataforover6,000stocksfrom\n1970to2018.Wefocusonfourfeatures:tradingvolume,openingprice,highprice,andadjustedclosingprice. Yelp4\nconsists of 6,990,280 user reviews of business places (e.g., cafes, restaurants, hotels, shops) from 11 metropolitan\nareas.Foreachreview,weextractasetof64-dimensionalsemanticvectorsastheexperimentaldataset. Color5isa32-\ndimensional image feature dataset extracted from the ImageNet dataset, which contains 1,281,167 records. Forest6is\ncollectedbytheUSGeologicalSurveyandtheUSForestService,whichincludes565,89212-dimensionalrecords. AI\nChallenger7isamultimodaldatasetthatcontainsmorethan300,000imageswithtextdescriptions.Inordertosimulate\ntherichhybridqueriesscenario,wedefinetwonumericattributesforeachimage:thelengthofthetextdescriptionand\nthesizeoftheimagefile. SIFT1B8isastandardbenchmarkdatasetthatcontains1billionof128-dimensionalfeature\nvectors. We select 0.1M-100M pieces of data from it. LAION400M9is a multimodal dataset containing 400 million\nimage-textpairs.Togeneraterichhybridqueries,weconcatenatetwo512-dimensionalfeaturevectorsrepresentingthe\nimage and text, along with a 2-dimensional scalar vector denoting the original dimensions of the image, resulting in\n1026-dimensional vectors. Synthetic Datasets include datasets with three different distributions, which are Uniform,\nGuassMix, and Skewed. The dimensions and data volumes of these datasets are adjustable. By default, we use a 3-\ndimensional dataset with a volume of 10 million records.\n7.1.2. Query Generation\nIntheexperiments,weassesstheperformanceofrangequeriesusingtheOSM,Taxi,andStockdatasets,andKNN\nqueries using the Yelp, Color, and Forest datasets. Additionally, we evaluate the performance of rich hybrid queries\nbased on the four basic query types proposed in Section 4.2, using the AI Challenger dataset.\nThe range queries and KNN queries, as common query operations, are each constituted by a record and its\ncorresponding parameter,with therecords being randomlyselected from thedataset intendedfor querying. Forrange\nqueries,theparameteristhequeryselectivity(i.e.,thepercentageofrecordsthatthequeryisexpectedtocover),which\nissettoadefaultof10%,witharangebetween0.1%and20%.ForKNNqueries,theparameteristhenumberofquery\nresultsùêæ, which defaults to 1,000, ranging from 1 to 10,000.\nForrichhybridqueries,weselectthreetypicaltypesofqueryamongtherichhybridqueriesshowninFig4,which\nare V.R‚®ÅV.K, V.R‚®ÅN.R and N.R‚®ÅV.K. To verify the ability of MQRLD to cope with more complex queries,\nwealsoinvolveaquerytypeofV.R √óN,whichisacombinationofNinstancesofV.R(N ‚àà[2,5]).Thesequeriesare\nallgeneratedfromtheAIChallengerdataset,whoseimageandtextdatacanbeembeddedinmultiplevectorattributes\nforperformingV.RorV.Kqueries.Theparametersinvolvedinrichhybridqueriesincludethequeryradius ùëÖforV.R\nquery,whichdefaultsto4%witharangefrom0.1%to10%;theparameter ùêæforV.Kquery,whichdefaultsto100with\na range from 1 to 1,000; and the selectivity for N.R query, which defaults to 10% with a range from 1% to 20%.\n7.1.3. Competitors\nWe compare our index with other multi-dimensional and high-dimensional indexes, as shown in Table 9. All\nmethods store data using DataFrame structures with data buckets as the smallest physical storage unit and apply the\nsame optimizations where applicable.\n7.1.4. Evaluation Metrics\nTo evaluate the performance of the retrieval, we adopt the following two key metrics.\n1https://download.geofabrik.de\n2https://www1.nyc.gov/site/tlc/about/tlc- trip- record- data.page\n3https://www.kaggle.com/datasets/ehallmar/daily-historical-stock-prices-1970-2018\n4https://www.yelp.com/dataset\n5https://image-net.org/download-images\n6https://www.kaggle.com/c/forest-cover-type-prediction/data\n7https://challenger.ai\n8http://corpus-texmex.irisa.fr/\n9https://laion.ai/\nM. Sheng et al.: Preprint submitted to Elsevier Page 31 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nTable 9\nCompetitors.\nIndex Type Range KNN\nZM (Wang et al., 2019) multi-dimensional ‚úì %\nML (Davitkova et al., 2020) multi-dimensional ‚úì ‚úì\nLISA (Tian et al., 2022) multi-dimensional ‚úì ‚úì\nQd-tree (Yang et al., 2020) multi-dimensional ‚úì %\nLIMS (Tian et al., 2022) multi-dimensional % ‚úì\nFlood (Nathan et al., 2020) multi-dimensional ‚úì %\nTsunami (Ding et al., 2020) multi-dimensional ‚úì %\nR*-tree (Beckmann et al., 1990) multi-dimensional ‚úì ‚úì\nSPB-tree (Chen et al., 2015) multi-dimensional % ‚úì\nM-tree (Ciaccia et al., 1997) multi-dimensional % ‚úì\nHNSW(Malkov and Yashunin, 2018) high-dimensional % ‚úì\nIVF(Moura and Cristo, 2009) high-dimensional % ‚úì\nLSH(Gionis et al., 1999) high-dimensional % ‚úì\nDB-LSH(Tian et al., 2024) high-dimensional % ‚úì\n(1)Query time. To more accurately assess the performance of each index, we perform five repeated experiments\nfor each group of generated queries and take the average query time as the experimental result.\n(2)Recall.RecallistheproportionoftruerelevantneighborsretrievedbyaKNNqueryoutofalltheactualrelevant\nneighbors. It measures the completeness of the search in finding the true nearest neighbors.\n(3)CBR.We use CBR to measure the dispersion of data access during the query process. A detailed definition\nrefers to Section 4.3. A lower CBR indicates that the query is more concentrated within fewer data buckets, implying\nhigher query efficiency and better indexing performance.\n7.2. Evaluation of Range Query for Multi-dimensional Indexes\nIn this section, we compare MQRLD with our competitors for multi-dimensional range queries. Fig 19 shows\nthe query time for each optimized index on the OSM, Taxi, and Stocks dataset. MQRLD uses the multimodal data\nrepresentationandhigh-dimensionallearnedindexconstructionprocessinSection5andSection6,whilewetunethe\nother methods as much as possible (e.g., ordered dimensions by selectivity and tuned the page sizes). This represents\nthe best-case scenario for the other indexes, that the database administrator had the time and ability to tune the index\nparameters. To ensure the generalizability of our experimental results, we test the average query time of various\nindex structures under different query selectivities. Our observations indicate that while the average query time for\nall indexes increases with higher query selectivity, MQRLD consistently outperforms other index structures across\ndifferent datasets:\n(1) Compared to tree-based indexes (R*-tree and Qd-tree), MQRLD demonstrates a significant improvement in\nqueryefficiency,withaverageperformancegainsof8.1 √óand3.9 √ó,respectively.ThisisattributedtoMQRLD‚Äôsability\nto better capture the data distribution in high-dimensional space during its construction, retaining the advantages of\ntree-based indexes while ensuring similar data points are stored in the same or neighboring nodes.\n(2) Against ZM, ML, and LISA, MQRLD shows average query efficiency improvements of 5.8 √ó, 5.1√óand 3.1 √ó,\nrespectively.AcommonfeatureofMQRLDandtheseindexesistheuseofone-dimensionalvaluemappingtoexpedite\ndata lookup. For instance, LISA uses a mapping function based on data distribution to map similar data into the\nsame segment. However, our data representation strategy goes a step further by clustering similar data based on their\ncharacteristics, resulting in a more efficient index structure.\n(3) MQRLD achieves 2.4 √óand 3.3 √óspeedup on query time compared to Flood and Tsunami, respectively.\nAlthough Flood‚Äôs cell division and Tsunami‚Äôs grid augmentation strategy can learn from query workloads to adjust\nits data layout, it is evident that the improvements from our multimodal data representation process are substantially\nmore effective.\nM. Sheng et al.: Preprint submitted to Elsevier Page 32 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nFigure 19: Range query evaluation results for multi-dimensional indexes on real datasets.\n7.3. Evaluation of KNN Query for Multi-dimensional Indexes\nNext, we evaluate MQRLD‚Äôs performance on KNN queries using the Yelp, Color, and Forest datasets. Similarly,\nunder the same query workload, we optimize other indexes as much as possible. It can be observed from Fig 20 that\nMQRLD maintains relatively low average query times across different ùêævalues:\n(1)Comparedwithnon-learnedmulti-dimensionalindexes,MQRLDshowsaveragequeryefficiencyimprovements\nof 4.2 √ó, 3.1√ó, and 3.3 √óover M-tree, R*-tree, and SPB-tree respectively. This is because the leaf nodes of MQRLD\ncontain highly similar data, and have a prediction model to improve the query speed.\n(2)Comparedwithlearnedmulti-dimensionalindexes,MQRLDimprovesqueryefficiencybyanaverageof2.2 √ó,\n1.9√ó, and 1.5 √óover ML, LISA, and LIMS datasets. ML uses clustering for data mapping but suffers from prediction\nerrors and longer search times due to its global predictive model. LISA and LIMS utilize local predictive models\nto improve the query efficiency for \"last-mile\" search, but their models are complex and the mapping process is time-\nconsuming.Incontrast,MQRLDtrainssimplelinearregressionmodelsateachleafnode,achievingmoreaccurateand\nfaster queries. This is enabled by MQRLD‚Äôs data-aware and query-aware data representation and high-dimensional\nlearned index construction strategy, which keep prediction model errors within a low threshold as well as improve\nMQRLD‚Äôs query efficiency.\nFigure 20: KNN query evaluation results for multi-dimensional indexes on real datasets. Due to incomparable results beyond\n12 dimensions for R*-tree and LISA, we disregard them on the Yelp and Color.\n7.4. Evaluation of CBR for Multi-dimensional Indexes\nIn this section, we investigate the CBR of several multi-dimensional indexes on both real and synthetic datasets.\nFig21reportstheCBRforrangequeriesacrossdifferentdatasets.WeobservethatMQRLDexhibitsthelowestCBR\nacrossvariousdatasets.Thetree-basedindexes,R*-treeandQd-tree,showrelativelylessCBR,demonstratingthattree\nstructurehastheadvantageofpruningasignificantamountofirrelevantdataregions.Floodusesgridindexesthatalso\neffectivelyfilteroutirrelevantdatathroughgridpartitioning,whileTsunami,whichcombinestreestructuresandgrid\nindexes,furtherleveragestheseadvantagestominimizeCBRduringqueries.ZMandML,however,showhigherCBR\nM. Sheng et al.: Preprint submitted to Elsevier Page 33 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nacross all datasets. This is due to the sequential scanning required on one-dimensional mapped values, compounded\nby the drawbacks of Z-order curves and prediction errors in learning models. Compared to these indexes, MQRLD\ncombinesthestructuraladvantagesoftreeindexeswithourindexoptimizationstrategythatensuresfrequentlyaccessed\nnodesarekeptinpositionsthataremorecache-friendly,allowingmostqueriestobecompletedbyscanningonlyafew\nleaf nodes.\nFigure 21: Cross-bucket rate evaluation results of range query and KNN query for multi-dimensional indexes on different\ndatasets. Due to incomparable results beyond 12 dimensions for R*-tree and LISA, we disregard them on Yelp and Color.\nFigure 22: Scalability evaluation results w.r.t. dataset size for multi-dimensional indexes.\n7.5. Evaluation of Scalability for Multi-dimensional Indexes\nDataset Size\nToshowhowMQRLDscaleswithdatasetsize,wecreatesyntheticdatasetsatsizesof{1M,10M,50M,100M}.We\ntrain and evaluate these datasets with the same train and test workloads as the full dataset. Fig 22 shows the results\nofeachindexondifferent-sizeddatasets,whereaveragequerytimeforallindexesexhibitsanearlylinearrelationship\nwith dataset size, with MQRLD demonstrating the best query performance overall.\nM. Sheng et al.: Preprint submitted to Elsevier Page 34 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nIntherangequeryexperiments,fortheUniformandSkeweddatasets,whichlackdistinctclusteringcharacteristics,\nthe query efficiency decreases. However, MQRLD‚Äôs data representation strategy, which involves repositioning data\npoints in hyperspace, enhances clustering characteristics and mitigates the impact of data distribution.\nIn the KNN query experiments, we can observe that the query efficiency of MQRLD outperforms other indexes\nby 1.6 √óto 5.4 √ó. Unlike other tree indexes, MQRLD‚Äôs tree depth and the number of leaf nodes do not significantly\nincreaseasthedatavolumeincreases.ThisallowsMQRLDtoeffectivelyprunealargeamountofirrelevantdataduring\nKNNqueries.Additionally,thepredictionmodelswithinitsleafnodesfurtherspeeduplocatingthelookupdatapoints.\nFigure 23: Scalability evaluation results w.r.t. dimension for multi-dimensional indexes. Due to incomparable results beyond\n12 dimensions for R*-tree and LISA, we disregard them on experiments of 16 dimensions.\nNumber of Dimensions\nToshowhowMQRLDscaleswithdimensions,wecreatesynthetic ùëë-dimensionaldatasets (ùëë‚â§16)with10million\nrecords whose values in each dimension are distributed uniformly at random. Fig 23 shows that MQRLD continues\nto outperform other indexes at each dimension. The growth rates of MQRLD, LIMS, and ML are lower compared\nto other indexes, indicating that clustering-based data techniques effectively mitigate the curse of dimensionality\nphenomenon. R*-tree and LISA maintain efficient query performance in low-dimensional queries. However, their\nquerytimeincreasesexponentiallywithdimensionality,andthus,theirresultsarenotrecordedbeyond12dimensions.\nBoth range and KNN queries on the GaussMix dataset show that the average query time for ML, LIMS, and\nMQRLD is lower than onother datasets, primarily due to the dataset‚Äôs inherent clustering-friendlynature. MQRLD‚Äôs\ndata representation strategy further enhances this clustering characteristic, thereby improving its query efficiency. In\nrangequeryexperiments,theincreaseindimensionalitysignificantlyraisesthenumberofgridsinFloodandTsunami,\nwhich in turn escalates grid filtering time during queries. In contrast, the number of leaf nodes in MQRLD remains\nunaffected by the dimensionality, ensuring consistent query performance. In KNN query experiments, LIMS shows\nhigheraveragequerytimesinlow-dimensionalscenarios.Thisisduetothelimitedpruninginformationavailableinlow\ndimensions, which increases query costs. Conversely, MQRLD‚Äôs pruning strategy, based on distance determination,\nremains stable across various dimensionality, demonstrating more robust performance.\n7.6. Evaluation of Rich Hybrid Queries for Multi-dimensional Indexes\nIn this section, we evaluate the performance of rich hybrid queries for multi-dimensional indexes. Except for\nMQRLD,otherindexesinourexperimentdonotsupportrichhybridqueriesandcanonlysearchasinglevector,sowe\nM. Sheng et al.: Preprint submitted to Elsevier Page 35 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\ncombinethemtosimulatetheexecutionofrichhybridqueries.Thesecombinationsincludesixdifferenthybridindexes:\nM-tree√óN,SPB-tree√óN,ML√óN,LIMS√óN,LIMS+Flood,andLIMS+Tsunami. √óNmeansacombinationofN\ninstances of the same index (N ‚àà[2,5]). Note that these combined indexes execute queries sequentially. For example,\nifML√ó2isusedtoperformaV.K‚®ÅV.Rquery,wewillfirstuseoneMLindextoexecutetheV.Kquery,thenbuild\nanother ML index to perform the V.R query.\nFig 24 presents the results of the average query times for these rich hybrid queries. We can observe that MQRLD\nconsistentlyachievesthelowestaveragequerytimesacrossallexperiments.Inexperimentsforthreetypicalrichhybrid\nqueries (V.R‚®ÅN.R, N.R‚®ÅV.K, and V.R‚®ÅV.K), MQRLD improves query performance by 2.8 √ó, 1.5√ó, and 3.2 √ó\nonaverage,respectively,comparedtothenextbest-performingindex,LIMS √ó2.Notably,intheV.R √óNexperiment,\nwhile the average query time for all indexes increases as the number of columns grows, MQRLD demonstrates the\nsmallestincrease.Itsqueryperformanceis,onaverage,4.7 √óbetterthanthatofthenextbest-performingindex,LIMS\n√óN. This superiority is due to MQRLD‚Äôs ability to maintain a single index for multiple feature vectors, whereas\nother indexes require combinations to complete the query tasks. Additionally, MQRLD‚Äôs data representation strategy\nand index optimization algorithm are effective in rich hybrid query scenarios, optimizing data distribution and index\nstructure based on query requirements, thus maintaining high query efficiency.\nFigure 24: Rich hybrid query evaluation results for multi-dimensional indexes.\n7.7. Evaluation of KNN Query for High-dimensional Indexes\nTo fully evaluate the KNN query performance of MQRLD against other high-dimensional indexes on datasets of\nvarying sizes, we select the SIFT1B dataset with sizes of 1M, 10M, 100M for comparison. We compare the average\nquery time and the recall-time curve of different high-dimensional indexes. As shown in the left figure of Fig 25,\nMQRLD‚Äôs advantage in both query time and recall efficiency becomes increasingly apparent as the dataset grows\nlarger. Onthe dataset of size100M, MQRLD‚Äôs average querytime is 1.3 √óto 2.8 √ófaster thanother high-dimensional\nindexes,andittakestheleasttimetoreachthesamerecall.Otherhigh-dimensionalindexes‚Äôdatastructuresaresensitive\nto increasing data volumes, such as the DB-LSH and LSH hash table structures, the clustering structures included in\nIVF, and the HNSW single-layer graph structures, resulting in their query times increasing rapidly as the data size\ngrows. In contrast, MQRLD‚Äôs structure is robust in the face of increasing data volume, due to the predictive models\nmaintained in the leaf nodes that allow for rapid filtering of large amounts of data during queries. At the same time,\nM. Sheng et al.: Preprint submitted to Elsevier Page 36 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nthehigh-precisionpredictivemodelscombinedwithefficientmultimodaldatarepresentationstrategiesgiveMQRLD\na significant advantage over other vector indexes in the time to reach the same recall on large-scale datasets.\nFigure 25: KNN query evaluation results for high-dimensional indexes.\n7.8. Evaluation of Rich Hybrid Queries for High-dimensional Indexes\nIn this section, we evaluate the performance of various high-dimensional indexes for rich hybrid queries. Except\nfor MQRLD, other high-dimensional indexes only support vector queries. Therefore, we combine these indexes with\nTsunamitoevaluaterichhybridqueries,similartoSection7.6.TheexperimentaldatasetisasubsetoftheLAION400M\ndataset with sizes of 1M, 10M, 100M. Fig 26 shows the average query time and recall-time curve for different\nindexes. It can be observed that MQRLD significantly outperforms other combined indexes on the 100M datasets.\nOn average, MQRLD‚Äôs query time is 1.2 √ófaster than the sub-optimal index DB-LSH+Tsunami, and its recall-time\ncurve demonstrates that it consistently achieves the same recall faster than other indexes. These experimental results\nhighlight that MQRLD not only supports rich hybrid queries but also substantially enhances query performance for\nhigh-dimensional data, especially on the largest dataset.\nFigure 26: Hybrid query evaluation results for high-dimensional indexes.\n7.9. Discussions\nInthissection,toclarifysomepotentialconcernsthathavenotbeenmentionedbefore,wewilldiscussthetrade-off\nbetween query workload and data appending, time and space complexity associated with MQRLD, and demonstrate\nthe effectiveness of our proposed optimization strategies through ablation experiments. Furthermore, all experiments\nconducted to verify these concerns utilize the GuassMix dataset as the experimental data.\nThe Trade-off Between Query Workload and Data Appending\nThe trade-off between multimodal data query workload (considered as read operations) and data appending\n(considered as write operations) is a hot topic, which is always analyzed in specific application scenarios. For\nmultimodal data retrieval platforms, the performance of data queries has a direct impact on downstream tasks. As a\nM. Sheng et al.: Preprint submitted to Elsevier Page 37 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nresult,ensuringtheperformanceofthequeryworkloadismorecriticalthansupportingdataappending.Theplatform‚Äôs\nability to handle query workload primarily depends on the multi-dimensional or high-dimensional indexing methods\nit employs, as discussed in Section 2. Most existing methods focus on achieving fast and accurate retrieval without\nanalyzing continuous data growing (e.g., Flood, Tsunami, LISA, ML, ZM, RSMI, LMSFC, etc). Only a few studies\ncan support the functionality of data appending (e.g., HNSW, IVFADC, E2LSH, DB-LSH, etc). Given that large-\nscale multimodal data is accumulated over long periods, with the data appending frequency being low and primarily\noccurring in batch forms, we offline established a high-dimensional learned index for MQRLD to support high-\nperformance retrieval and preserve its functionality of data appending. Since the learned index structure is based\nonaclustertree,dataappendingcanalsobeachievedthroughsearchoperations.Byqueryingnewdataandidentifying\nits nearest neighbor (i.e., equivalent to a KNN search, K=1), we can determine the most likely insert location for it.\nAs shown in Fig 18 (b) and (c), the index structure changes only when the data grows exponentially. Therefore, even\nwhendataisfrequentlyappending,theindexstructureremainsstable.However,itisundeniablethatifnewexcessive\ndata accumulates, the retrieval performance will deteriorate, requiring index reconstruction when the query accuracy\nfalls below a predefined threshold.\nThe Time Cost of QBS Table Construction\nInthispaper,theQBStablehasasignificantimpactonthequery-awaremechanism.However,sincethecalculation\nof statistics variables Recall@K and Query Accuracy in the QBS table is time-consuming, computing the statistics\ninformation corresponding to all query statements may result in significant time costs. Therefore, we sample query\nstatementsduringthequeryprocessandcalculatetheircorrespondingstatisticsinformationtoconstructtheQBStable.\nThe Time Cost of Feature Measurement\nIn feature measurement, we select the embedding model with the highest score by calculating the measurement\nmetricsacrossdifferentembeddingmodels.However,duetothecomputationsofSC,FID,andextrinsicmeasurement\nmetric(seedetailsinSection5.1.2)beingquiteslow,performingfeaturemeasurementontheentiredatasetwouldalso\nresult in a heavy workload. Therefore, similar to QBS table construction, we calculate the measurement metrics by\nsampling the data (for calculating SC and FID) and queries (for calculating extrinsic measurement metrics) to select\nthe embedding model with the highest score.\nThe Time Complexity of A Single Query\nAssuming a multimodal dataset contains ùëÅrecords, the complexity of the traverse time from the root node to the\nleaf node is bounded by ùëÇ(ùëôùëúùëîùëÅ), while the complexity of the \"last-mile\" search using the linear regression model\ninside the leaf node is ùëÇ(1). Therefore, the time complexity of a single query is bounded by ùëÇ(ùëôùëúùëîùëÅ).\nThe Time Complexity of Index Construction and Experiments\nIn each iteration of the Divisive Hierarchical Clustering process, DPC calculates the density and distance of each\ndata point to others in order to find cluster centers, resulting in a complexity of ùëÇ(ùëÅ2). The number of iterations is\nbounded by ùëÇ(ùëôùëúùëîùëÅ), so the overall time complexity for the Divisive Hierarchical Clustering phase is bounded by\nùëÇ(ùëÅ2ùëôùëúùëîùëÅ). Then, for each leaf node, a linear regression model is trained. The training step for all leaf nodes has\na time complexity of ùëÇ(ùëÅ). The overall time complexity of the indexing construction is dominated by the Divisive\nHierarchicalClusteringprocess,withacomplexityboundedby ùëÇ(ùëÅ2ùëôùëúùëîùëÅ).Fig27(a)illustratestheindexconstruction\ntimes for multi-dimensional and high-dimensional indexes. The multi-dimensional indexes are evaluated using the\nGaussMix dataset, where we observe that MQRLD‚Äôs index construction time is comparable to that of other multi-\ndimensionalindexes.Asforthehigh-dimensionalindexes,theSIFT10Mdatasetisemployed,andweobservethatthe\nindex construction times of high-dimensional indexes are lower than multi-dimensional indexes in general.\nThe Space Complexity of Index and Experiments\nSince our clustering algorithm ensures that intermediate nodes in the cluster tree have at least two child nodes,\ntheclustertreewill,in theworstcase,degradeintoafullbinarytreewith ùëÅleafnodes.Thetotalnumber ofnon-leaf\nnodes in such a tree is ùëÅ‚àí1. Leaf nodes store a tuple containing its centroid, radius, and a linear regression model,\nrequiringùëÇ(ùëÅ)space. Non-leaf nodes store a tuple consisting of its centroid, radius, and a pointer, also requiring\nùëÇ(ùëÅ)space. Therefore, the total space complexity of our index is bounded by ùëÇ(ùëÅ). However, due to the efficiency\nof our iterative and divisive approach, the tree depth is low, and the total number of nodes is significantly less than\nM. Sheng et al.: Preprint submitted to Elsevier Page 38 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\n2ùëÅ‚àí1(asshowninFig18(b)and(c)),makingourindexstructurepracticallyverysmall.Fig27(b)showstheindex\nsize of the index structure we constructed on datasets with varying data volumes. We can see that MQRLD saves an\naverage of 65.7%-90.1% space compared to multi-dimensional indexes and an average of 81.4%-99.7% compared to\nhigh-dimensional indexes.\n(a)\n (b)\n (c)\nFigure 27: The time cost of different index construction(a), The space cost of different index construction(b), and ablation\nstudy(c).\nExperiments of Deploying MQRLD on Vector Database\nIn this section, we apply MQRLD to the vector database Milvus and compare its KNN query performance with\nhigh-dimensional indexes HNSW, IVF, LSH, and DB-LSH. The experimental dataset uses the SIFT1B dataset with\nsizesof0.1M,1M,10M.DuetothelimitationsoftheMilvusdatabaseinourexperimentenvironment,wecannotuse\nlargerdatasetstoevaluatequeryperformance.TheleftfigureofFig28showstheaveragequerytimeforMQRLDand\ndifferent high-dimensional indexes at various dataset sizes. We can see that as the data scale increases, the advantage\nof MQRLD gradually becomes apparent. On the dataset of size 10M, MQRLD‚Äôs average query time is significantly\nlower than IVF and LSH. The increase in data scales of the same order of magnitude does not significantly impact\nthe tree depth of MQRLD‚Äôs cluster tree, thereby resulting in a negligible increase in MQRLD‚Äôs query time. The right\nfigure of Fig 28 shows the recall-time curves for MQRLD and different high-dimensional indexes at various dataset\nsizes. Compared to other indexes, MQRLD takes the least time to achieve a 100% recall.\nFigure 28: Evaluation results for high-dimensional indexes on vector database.\nAblation Experiments\nTo explore the impact of different components on MQRLD performance, we evaluate the performance of range\nqueries using MQRLD in various states. Fig 27(b) provides a detailed illustration of the average query times for\ndifferentqueryoptimizedmethods,inwhich\"FullScan\"referstoscanningtheentiredatasettoimplementrangequery,\n\"Initialized_MQRLD\" indicates querying with the initialized MQRLD without index optimization, \"Optimized_T\"\nindicates querying with MQRLD optimized after feature representation, and \"Optimized_Index\" indicates querying\nwith MQRLD optimized using index structures. It is noteworthy that the sampling frequency used at different\noptimizationstagesistheminimumeffectivevalue,whichis10%.Webelievethatasthesamplingfrequencyincreases,\nthe performance of MQRLD will continue to improve.\nM. Sheng et al.: Preprint submitted to Elsevier Page 39 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nFrom the experimental results, we can draw the following conclusions:\n(1) The results of \"Full Scan\" and \"Initialized_MQRLD\" shows that our data representation strategy and index\nstructure significantly reduce query times, even in the initial state.\n(2)Theresultsof\"Initialized_MQRLD\"and\"Optimized_T\"validatetheeffectivenessofourfeaturerepresentation\nmethod, with the optimized matrix T further enhancing query performance.\n(3)Theresultsof\"Optimized_T\"and\"Optimized_Index\"validatetheeffectivenessofourindexstructure,withthe\noptimized index structure effectively reducing the scanning of irrelevant data buckets and reducing query time.\n(4)Overallexperimentalresultsdemonstratetheeffectivenessofourquery-awaremechanism,withtheoptimized\nmatrix T and index structure further enhancing query performance.\n8. Conclusion\nIn this study, we introduce a multimodal data retrieval platform MQRLD. This platform builds on a data lake\nthat offers transparent data storage and integrates a multimodal open API for rich hybrid queries. In addition to these\ncapabilities, its multimodal data representation strategy transforms data into effective feature vectors through feature\nembedding,measurement,andenhancement,aidingaccuratedataretrieval.Tofurtherimprovequeryefficiency,high-\ndimensional learned indexes are built using divisive hierarchical clustering and cluster tree construction, with the\nquery-aware mechanism optimizing the index structure. Overall, MQRLD not only supports transparent data storage\nand rich hybrid queries but also ensures effective and efficient retrieval.\nAlthough our proposed MQRLD addresses the functionalities of transparent storage and rich hybrid queries, and\nenhancesqueryperformance,asmentionedinSection1,itstillfacesadditionalchallenges:(1)TheMQRLDplatform\nmainlyconsistsoftwomodules:featurerepresentationandhigh-dimensionallearnedindex.Experimentshaveshown\nthat these modules can effectively improve the retrieval performance of data lakes. However, the scalability of them\nonotherplatformsstillneedsfurtherdiscussion.(2)Despitethelargeaccumulationofmultimodaldataintheformof\nhistorical records, the development of Internet of Things (IoT) technologies supports multimodal data in a streaming\nformat. Furthermore, user query behaviors often exhibit varying preferences over time. Therefore, to maintain the\nplatform retrieval performance, whether it can support dynamic index reconstruction when facing frequent data\nappendinganddifferentqueryworkloadsremainsachallenge.(3)Largelanguagemodels(LLMs)havedemonstrated\ntheirhighscalabilityindifferentfields.ItremainstobeexploredwhetherMQRLDcanleverageLLMstoachievemore\nefficient and high-quality feature representation. (4) Although multimodal data are primarily accumulated in batch\nform, it is reasonable to assume that more recent data are likely to exhibit more similar characteristics. Additionally,\nqueryworkloadswithinaspecifictimeperiodarelikelytoexhibitsimilarpreferences.Therefore,itisworthexploring\nwhethertemporalinformationcanbeutilizedbytheplatformtoenhancefeaturerepresentationandindexconstruction.\nWe can further optimize our platform in the future, including: (1) We consider applying our multimodal data\nfeature representation strategy and high-dimensional learned index to other platforms to enhance their multimodal\ndata retrieval performance while preserving their inherent characteristics as discussed in Section 1. (2) Existing\nresearches typically maintain retrieval performance under frequent data appending and different query workloads by\nmonitoring query results to determine whether the index needs to be reconstructed (Ding et al., 2021; Xie et al.,\n2023). So how to determine the threshold for triggering index reconstruction is an opportunity worth exploring.\nMoreover,toavoidtheexpensivecostofreconstructingtheentireindex,itisworthconsideringwhetherpartialindex\noptimizationscanhelpbalancethetrade-offamongonlinedataappending,queryworkloads,andindexreconstruction.\n(3) LLMs demonstrate significant potential in representation learning. Currently, the feature embedding models pool\nin our feature representation includes the popular CLIP-based models (Radford et al., 2021; Luo et al., 2021; Ma\net al., 2022). However, these models do not support a unified training representation across various modalities.\nExploringthepossibilityofleveragingLLMstoofferaunifiedembeddingstrategyofdifferentmodalitiespresentsan\ninteresting opportunity for further investigation. (4) Liu et al. (2023b) shows that incorporating temporal information\ninto multimodal graph learning can effectively aid multimodal data classification tasks, which may help with feature\nrepresentation (aims to gather similar multimodal data). Additionally, Liu et al. (2023c) indicates that temporal\ninformationcanenhancethepredictionofinteractions,supportingtheimplementationofonlinequeryworkloadsonour\nplatform(allowingtheplatformtodynamicallyadjusttheindexinadvancebasedonpredictedqueryworkload(Pavlo\net al., 2021)). Therefore, leveraging temporal information to optimize our platform functionalities and performance\npresents a valuable opportunity.\nM. Sheng et al.: Preprint submitted to Elsevier Page 40 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nCRediT authorship contribution statement\nMing Sheng: Conceptualization, Methodology, Writing - Original Draft, Writing - Review & Editing, Formal\nAnalysis,Investigation,ProjectAdministration. ShuliangWang: Methodology,Writing-Review&Editing,Funding\nAcquisition. Yong Zhang: Conceptualization, Methodology, Writing - Original Draft, Writing - Review & Editing,\nProjectAdministration. KaigeWang: Software,Validation,Visualization,Writing-Review&Editing. JingyiWang:\nMethodology, Writing - Original Draft, Writing - Review & Editing, Investigation. Yi Luo:Writing - Original Draft,\nWriting - Review & Editing, Investigation. Rui Hao: Writing - Review & Editing.\nAcknowledgements\nThe work is funded by the National Natural Science Foundation of China under Grant 42371480.\nReferences\nAbdi, H., Williams, L.J., 2010. Principal component analysis. Wiley interdisciplinary reviews: computational statistics 2, 433‚Äì459.\nAl-Mamun, A., Wu, H., He, Q., Wang, J., Aref, W.G., 2024. A survey of learned indexes for the multi-dimensional space. arXiv preprint\narXiv:2403.06456 .\nAlshaher, H., 2021. Studying the effects of feature scaling in machine learning. Ph.D. thesis. North Carolina Agricultural and Technical State\nUniversity.\nAndorra,M.,Freire,A.,Zubizarreta,I.,deRosbo,N.K.,Bos,S.D.,Rinas,M.,H√∏gest√∏l,E.A.,deRodezBenavent,S.A.,Berge,T.,Brune-Ingebretse,\nS., et al., 2024. Predicting disease severity in multiple sclerosis using multimodal data and machine learning. Journal of Neurology 271, 1133‚Äì\n1149.\nAwawdeh,S.,Faris,H.,Hiary,H.,2022. Evoimputer:Anevolutionaryapproachformissingdataimputationandfeatureselectioninthecontextof\nsupervised learning. Knowledge-Based Systems 236, 107734.\nBeckmann, N., Kriegel, H.P., Schneider, R., Seeger, B., 1990. The r*-tree: An efficient and robust access method for points and rectangles, in:\nProceedings of the 1990 ACM SIGMOD international conference on Management of data, pp. 322‚Äì331.\nBelem, F.M., Silva, R.M., de Andrade, C.M., Person, G., Mingote, F., Ballet, R., Alponti, H., de Oliveira, H.P., Almeida, J.M., Goncalves, M.A.,\n2020. ‚Äúfixingthecurseofthebadproductdescriptions‚Äù‚Äìsearch-boostedtagrecommendationfore-commerceproducts. InformationProcessing\n& Management 57, 102289.\nBelgundi, R., Kulkarni, Y., Jagdale, B., 2023. Analysis of native multi-model database using arangodb, in: Proceedings of Third International\nConference on Sustainable Expert Systems: ICSES 2022, Springer. pp. 923‚Äì935.\nBernhardsson, E., 2015. Annoy at github. URL: https://github.com/spotify/annoy .\nBorthakur, D., et al., 2008. Hdfs architecture guide. Hadoop apache project 53, 2.\nChen,L.,Gao,Y.,Li,X.,Jensen,C.S.,Chen,G.,2015. Efficientmetricindexingforsimilaritysearch,in:2015IEEE31stInternationalConference\non Data Engineering, IEEE. pp. 591‚Äì602.\nChen, T., Guestrin, C., 2016. Xgboost: A scalable tree boosting system, in: Proceedings of the 22nd acm sigkdd international conference on\nknowledge discovery and data mining, pp. 785‚Äì794.\nCiaccia, P., Patella, M., Zezula, P., et al., 1997. M-tree: An efficient access method for similarity search in metric spaces, in: Vldb, Citeseer. pp.\n426‚Äì435.\nCieslak, M.C., Castelfranco, A.M., Roncalli, V., Lenz, P.H., Hartline, D.K., 2020. t-distributed stochastic neighbor embedding (t-sne): A tool for\neco-physiological transcriptomic analysis. Marine genomics 51, 100723.\nCovington, P., Adams, J., Sargin, E., 2016. Deep neural networks for youtube recommendations, in: Proceedings of the 10th ACM conference on\nrecommender systems, pp. 191‚Äì198.\nDasgupta,S.,Sinha,K.,2013. Randomizedpartitiontreesforexactnearestneighborsearch,in:Conferenceonlearningtheory,PMLR.pp.317‚Äì337.\nDatar,M.,Immorlica,N.,Indyk,P.,Mirrokni,V.S.,2004. Locality-sensitivehashingschemebasedonp-stabledistributions,in:Proceedingsofthe\ntwentieth annual symposium on Computational geometry, pp. 253‚Äì262.\nDaulton, S., Eriksson, D., Balandat, M., Bakshy, E., 2022. Multi-objective bayesian optimization over high-dimensional search spaces, in:\nUncertainty in Artificial Intelligence, PMLR. pp. 507‚Äì517.\nDavitkova, A., Milchevski, E., Michel, S., 2020. The ml-index: A multidimensional, learned index for point, range, and nearest-neighbor queries.,\nin: EDBT, pp. 407‚Äì410.\nDhal,P.,Azad,C.,2022. Acomprehensivesurveyonfeatureselectioninthevariousfieldsofmachinelearning. AppliedIntelligence52,4543‚Äì4581.\nDing, J., Minhas, U.F., Chandramouli, B., Wang, C., Li, Y., Li, Y., Kossmann, D., Gehrke, J., Kraska, T., 2021. Instance-optimized data layouts\nfor cloud analytics workloads, in: Li, G., Li, Z., Idreos, S., Srivastava, D. (Eds.), SIGMOD ‚Äô21: International Conference on Management of\nData, Virtual Event, China, June 20-25, 2021, ACM. pp. 418‚Äì431. URL: https://doi.org/10.1145/3448016.3457270 , doi: 10.1145/\n3448016.3457270 .\nDing,J.,Nathan,V.,Alizadeh,M.,Kraska,T.,2020. Tsunami:Alearnedmulti-dimensionalindexforcorrelateddataandskewedworkloads. Proc.\nVLDB Endow. 14, 74‚Äì86.\nDong, W., Moses, C., Li, K., 2011. Efficient k-nearest neighbor graph construction for generic similarity measures, in: Proceedings of the 20th\ninternational conference on World wide web, pp. 577‚Äì586.\nDougherty,J.,Kohavi,R.,Sahami,M.,1995. Supervisedandunsuperviseddiscretizationofcontinuousfeatures,in:Machinelearningproceedings\n1995. Elsevier, pp. 194‚Äì202.\nM. Sheng et al.: Preprint submitted to Elsevier Page 41 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nFalcon,A.,Serra,G.,Lanz,O.,2022. Afeature-spacemultimodaldataaugmentationtechniquefortext-videoretrieval,in:Proceedingsofthe30th\nACM International Conference on Multimedia, pp. 4385‚Äì4394.\nFu, L., Medico, E., 2007. Flame, a novel fuzzy clustering method for the analysis of dna microarray data. BMC bioinformatics 8, 1‚Äì15.\nGao, J., Cao, X., Yao, X., Zhang, G., Wang, W., 2023. Lmsfc: A novel multidimensional index based on learned monotonic space filling curves.\nProc. VLDB Endow. 16, 2605‚Äì2617.\nGao, J., Li, P., Chen, Z., Zhang, J., 2020. A survey on deep learning for multimodal data fusion. Neural Computation 32, 829‚Äì864.\nGasser, R., Rossetto, L., Heller, S., Schuldt, H., 2020. Cottontail db: an open source database system for multimedia retrieval and analysis, in:\nProceedings of the 28th ACM international conference on multimedia, pp. 4465‚Äì4468.\nGhaffari, B., Hadian, A., Heinis, T., 2020. Leveraging soft functional dependencies for indexing multi-dimensional data. arXiv preprint\narXiv:2006.16393 .\nGiangreco, I., Schuldt, H., 2016. Adampro: Database support for big multimedia retrieval. Datenbank-Spektrum 16, 17‚Äì26.\nGionis, A., Indyk, P., Motwani, R., et al., 1999. Similarity search in high dimensions via hashing, in: Vldb, pp. 518‚Äì529.\nGrohe, M., 2020. word2vec, node2vec, graph2vec, x2vec: Towards a theory of vector embeddings of structured data, in: Proceedings of the 39th\nACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, pp. 1‚Äì16.\nGuay Paz, J.R., 2018. Introduction to azure cosmos db. Springer. pp. 1‚Äì23.\nGuttman, A., 1984. R-trees: A dynamic index structure for spatial searching, in: Proceedings of the 1984 ACM SIGMOD international conference\non Management of data, pp. 47‚Äì57.\nGuzhov, A., Raue, F., Hees, J., Dengel, A., 2022. Audioclip: Extending clip to image, text and audio , 976‚Äì980.\nHai, R., Koutras, C., Quix, C., Jarke, M., 2023. Data lakes: A survey of functions and systems. IEEE Transactions on Knowledge and Data\nEngineering 35, 12571‚Äì12590.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S., 2017. Gans trained by a two time-scale update rule converge to a local nash\nequilibrium. Advances in neural information processing systems 30.\nHudi, 2021. Apache hudi. URL: https://hudi.apache.org/ .\nIslam, N., 2024. Dtization: A new method for supervised feature scaling. arXiv preprint arXiv:2404.17937 .\nJ., D., 2010. Pentaho, hadoop, and data lakes. URL: https://jamesdixon.wordpress.com/2010/10/14/\npentaho-hadoop-and-data-lakes/ . accessed on January 5, 2024.\nJegou, H., Douze, M., Schmid, C., 2010. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine\nintelligence 33, 117‚Äì128.\nJINA, 2024. Jina at github. URL: https://github.com/jina-ai/jina .\nJohnson, J., Douze, M., J√©gou, H., 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data 7, 535‚Äì547.\nKhine, P.P., Wang, Z.S., 2018. Data lake: a new ideology in big data era, in: ITM web of conferences, EDP Sciences. p. 03025.\nKraska, T., Beutel, A., Chi, E.H., Dean, J., Polyzotis, N., 2018. The case for learned index structures, in: Proceedings of the 2018 international\nconference on management of data, pp. 489‚Äì504.\nLe, Q., Mikolov, T., 2014. Distributed representations of sentences and documents, in: International conference on machine learning, PMLR. pp.\n1188‚Äì1196.\nLei,H.,Li,C.,Zhou,K.,Zhu,J.,Yan,K.,Xiao,F.,Xie,M.,Wang,J.,Di,S.,2024. X-stor:Acloud-nativenosqldatabaseservicewithmulti-model\nsupport [industry] In process.\nLi, H., Jia, R., Wan, X., 2022a. Time series classification based on complex network. Expert Systems with Applications 194, 116502.\nLi, P., Lu, H., Zheng, Q., Yang, L., Pan, G., 2020. Lisa: A learned index structure for spatial data, in: Proceedings of the 2020 ACM SIGMOD\ninternational conference on management of data, pp. 2119‚Äì2133.\nLi, Q., Wang, S., Zhao, C., Zhao, B., Yue, X., Geng, J., 2021. Hibog: improving the clustering accuracy by ameliorating dataset with gravitation.\nInformation Sciences 550, 41‚Äì56.\nLi, Z., Yiu, M.L., Chan, T.N., 2022b. Paw: Data partitioning meets workload variance, in: 2022 IEEE 38th International Conference on Data\nEngineering (ICDE), IEEE. pp. 123‚Äì135.\nLiu, G., Qi, J., Jensen, C.S., Bailey, J., Kulik, L., 2023a. Efficiently learning spatial indices, in: 2023 IEEE 39th International Conference on Data\nEngineering (ICDE), pp. 1572‚Äì1584.\nLiu, J., Zhu, X., Liu, F., Guo, L., Zhao, Z., Sun, M., Wang, W., Lu, H., Zhou, S., Zhang, J., et al., 2021. Opt: Omni-perception pre-trainer for\ncross-modal understanding and generation. arXiv preprint arXiv:2107.00249 .\nLiu, M., Liang, K., Hu, D., Yu, H., Liu, Y., Meng, L., Tu, W., Zhou, S., Liu, X., 2023b. Tmac: Temporal multi-modal graph learning for acoustic\nevent classification, in: El-Saddik, A., Mei, T., Cucchiara, R., Bertini, M., Vallejo, D.P.T., Atrey, P.K., Hossain, M.S. (Eds.), Proceedings of the\n31stACMInternationalConferenceonMultimedia,MM2023,Ottawa,ON,Canada,29October2023-3November2023,ACM.pp.3365‚Äì3374.\nURL: https://doi.org/10.1145/3581783.3611853 , doi: 10.1145/3581783.3611853 .\nLiu,M.,Liang,K.,Xiao,B.,Zhou,S.,Tu,W.,Liu,Y.,Yang,X.,Liu,X.,2023c.Self-supervisedtemporalgraphlearningwithtemporalandstructural\nintensityalignment.CoRRabs/2302.07491.URL: https://doi.org/10.48550/arXiv.2302.07491 ,doi:10.48550/ARXIV.2302.07491 ,\narXiv:2302.07491 .\nLu, J., Holubov√°, I., 2019. Multi-model databases: a new journey to handle the variety of data. ACM Computing Surveys (CSUR) 52, 1‚Äì38.\nLuo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T., 2021. Clip4clip: An empirical study of clip for end to end video clip retrieval.\nNeurocomputing 508, 293‚Äì304.\nLymperaiou, M., Stamou, G., 2024. A survey on knowledge-enhanced multimodal learning. Artificial Intelligence Review 57, 284.\nMa, Y., Xu, G., Sun, X., Yan, M., Zhang, J., Ji, R., 2022. X-clip: End-to-end multi-grained contrastive learning for video-text retrieval , 638‚Äì647.\nMacqueen, J., 1967. Some methods for classification and analysis of multivariate observations. University of California Press.\nMalkov, Y.A., Yashunin, D.A., 2018. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs,\nIEEE. pp. 824‚Äì836.\nM. Sheng et al.: Preprint submitted to Elsevier Page 42 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nMikolov, T., Chen, K., Corrado, G.S., Dean, J., 2013. Efficient estimation of word representations in vector space, in: International Conference on\nLearning Representations.\nMishra, S., Misra, A., 2017. Structured and unstructured big data analytics, in: 2017 International Conference on Current Trends in Computer,\nElectrical, Electronics and Communication (CTCEEC), IEEE. pp. 740‚Äì746.\nMongoDB, Inc., 2023. Mongodb. https://www.mongodb.com/ . Accessed: 2023-11-24.\nMorton, G.M., 1966. A computer oriented geodetic data base and a new technique in file sequencing .\nMoura, E.S.d., Cristo, M.A., 2009. Inverted Files. Springer US, Boston, MA. pp. 1571‚Äì1574. URL: https://doi.org/10.1007/\n978-0-387-39940-9_1136 , doi: 10.1007/978-0-387-39940-9_1136 .\nMuja, M., Lowe, D.G., 2009. Fast approximate nearest neighbors with automatic algorithm configuration. VISAPP (1) 2, 2.\nNathan,V.,Ding,J.,Alizadeh,M.,Kraska,T.,2020. Learningmulti-dimensionalindexes,in:Proceedingsofthe2020ACMSIGMODinternational\nconference on management of data, pp. 985‚Äì1000.\nNievergelt,J.,Hinterberger,H.,Sevcik,K.C.,1984. Thegridfile:Anadaptable,symmetricmultikeyfilestructure. ACMTransactionsonDatabase\nSystems (TODS) 9, 38‚Äì71.\nObe, R.O., Hsu, L.S., 2017. PostgreSQL: up and running: a practical guide to the advanced open source database. \" O‚ÄôReilly Media, Inc.\".\nOracle Corporation, 2023. Mysql. https://www.mysql.com/ . Accessed: 2023-11-24.\nPan, J.J., Wang, J., Li, G., 2024. Survey of vector database management systems. The VLDB Journal , 1‚Äì25.\nPavlo, A., Butrovich, M., Ma, L., Menon, P., Lim, W.S., Aken, D.V., Zhang, W., 2021. Make your database system dream of electric sheep:\nTowards self-driving operation. Proc. VLDB Endow. 14, 3211‚Äì3221. URL: http://www.vldb.org/pvldb/vol14/p3211-pavlo.pdf ,\ndoi:10.14778/3476311.3476411 .\nPennington,J.,Socher,R.,Manning,C.D.,2014. Glove:Globalvectorsforwordrepresentation,in:Proceedingsofthe2014conferenceonempirical\nmethods in natural language processing (EMNLP), pp. 1532‚Äì1543.\npgvector, 2024. pgvector at github. URL: https://github.com/pgvector/pgvector .\nPinecone, 2024. The vector database to build knowledgeable ai | pinecone. URL: https://www.pinecone.io/ .\nProtter, M., Morrey, C., 1970. College Calculus with Analytic Geometry. Addison-Wesley series in mathematics, Addison-Wesley.\nQi, J., Liu, G., Jensen, C.S., Kulik, L., 2020. Effectively learning spatial indices. Proceedings of the VLDB Endowment 13, 2341‚Äì2354.\nRadford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,Askell,A.,Mishkin,P.,Clark,J.,etal.,2021.Learningtransferable\nvisual models from natural language supervision , 8748‚Äì8763.\nRasappan, P., Premkumar, M., Sinha, G., Chandrasekaran, K., 2024. Transforming sentiment analysis for e-commerce product reviews: Hybrid\ndeep learning model with an innovative term weighting and feature selection. Information Processing & Management 61, 103654.\nRen,P., Li,S.,Hou, W.,Zheng, W.,Li,Z., Cui,Q., Chang,W.,Li, X.,Zeng,C., Sheng,M., etal.,2021a. Mhdp: anefficientdata lakeplatform for\nmedicalmulti-sourceheterogeneousdata,in:WebInformationSystemsandApplications:18thInternationalConference,WISA2021,Kaifeng,\nChina, September 24‚Äì26, 2021, Proceedings 18, Springer. pp. 727‚Äì738.\nRen,P.,Lin,W.,Liang,Y.,Wang,R.,Liu,X.,Zuo,B.,Chen,T.,Li,X.,Sheng,M.,Zhang,Y.,2021b. Hmdff:aheterogeneousmedicaldatafusion\nframeworksupportingmultimodalquery,in:HealthInformationScience:10thInternationalConference,HIS2021,Melbourne,VIC,Australia,\nOctober 25‚Äì28, 2021, Proceedings 10, Springer. pp. 254‚Äì266.\nReynolds, D.A., et al., 2009. Gaussian mixture models. Encyclopedia of biometrics 741.\nRitter, D., Dell‚ÄôAquila, L., Lomakin, A., Tagliaferri, E., 2021. Orientdb: A nosql, open source mmdms., in: BICOD, pp. 10‚Äì19.\nRodriguez, A., Laio, A., 2014. Clustering by fast search and find of density peaks. science 344, 1492‚Äì1496.\nRohrer, B.R., 2011. An implemented architecture for feature creation and general reinforcement learning. Technical Report. Sandia National\nLab.(SNL-NM), Albuquerque, NM (United States).\nRombach,R.,Blattmann,A.,Lorenz,D.,Esser,P.,Ommer,B.,2022. High-resolutionimagesynthesiswithlatentdiffusionmodels,in:Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nRoskladka,A.,Roskladka,N.,Kharlamova,G.,Baglai,R.,2019. Cloudbasedarchitectureofthecorebankingsystem.,in:ICTERIWorkshops,pp.\n1‚Äì16.\nRousseeuw, P.J., 1987. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied\nmathematics 20, 53‚Äì65.\nSchneider,J.,Gr√∂ger,C.,Lutsch,A.,Schwarz,H.,Mitschang,B.,2024. Thelakehouse:Stateoftheartonconceptsandtechnologies. SNComputer\nScience 5, 1‚Äì39.\nShaikh, E., Mohiuddin, I., Alufaisan, Y., Nahvi, I., 2019. Apache spark: A big data processing engine, in: 2019 2nd IEEE Middle East and North\nAfrica COMMunications Conference (MENACOMM), IEEE. pp. 1‚Äì6.\nSokal, R.R., Michener, C.D., 1958. A statistical method for evaluating systematic relationships .\nTaipalus, T., 2024. Vector database management systems: Fundamental concepts, use-cases, and current challenges. Cognitive Systems Research\n85, 101216.\nThukral,A.,Dhiman,S.,Meher,R.,Bedi,P.,2023. Knowledgegraphenrichmentfromclinicalnarrativesusingnlp,ner,andbiomedicalontologies\nfor healthcare applications. International Journal of Information Technology 15, 53‚Äì65.\nTian,Y.,Yan,T.,Zhao,X.,Huang,K.,Zhou,X.,2022.Alearnedindexforexactsimilaritysearchinmetricspaces.IEEETransactionsonKnowledge\nand Data Engineering 35, 7624‚Äì7638.\nTian,Y.,Zhao,X.,Zhou,X.,2024. Db-lsh2.0:Locality-sensitivehashingwithquery-baseddynamicbucketing. IEEETransactionsonKnowledge\nand Data Engineering 36, 1000‚Äì1015. doi: 10.1109/TKDE.2023.3295831 .\nWang,H.,Fu,X.,Xu,J.,Lu,H.,2019. Learnedindexforspatialqueries,in:201920thIEEEInternationalConferenceonMobileDataManagement\n(MDM), IEEE. pp. 569‚Äì574.\nWang, J., Yi, X., Guo, R., Jin, H., Xu, P., Li, S., Wang, X., Guo, X., Li, C., Xu, X., et al., 2021. Milvus: A purpose-built vector data management\nsystem, in: Proceedings of the 2021 International Conference on Management of Data, pp. 2614‚Äì2627.\nM. Sheng et al.: Preprint submitted to Elsevier Page 43 of 44\n\nA Multimodal Data Retrieval Platform with Query-aware Feature Representation and Learned Index Based on Data Lake\nWang,W.,Yang,X.,Ooi,B.C.,Zhang,D.,Zhuang,Y.,2016. Effectivedeeplearning-basedmulti-modalretrieval. TheVLDBJournal25,79‚Äì101.\nWei,C.,Wu,B.,Wang,S.,Lou,R.,Zhan,C.,Li,F.,Cai,Y.,2020. Analyticdb-v:ahybridanalyticalenginetowardsqueryfusionforstructuredand\nunstructured data. Proceedings of the VLDB Endowment 13, 3152‚Äì3165.\nWu, J., Yu, X., He, K., Gao, Z., Gong, T., 2024. Promise: A pre-trained knowledge-infused multimodal representation learning framework for\nmedication recommendation. Information Processing & Management 61, 103758.\nXanthopoulos, P., Pardalos, P.M., Trafalis, T.B., Xanthopoulos, P., Pardalos, P.M., Trafalis, T.B., 2013. Linear discriminant analysis. Robust data\nmining , 27‚Äì33.\nXiao, Q., Zheng, W., Mao, C., Hou, W., Lan, H., Han, D., Duan, Y., Ren, P., Sheng, M., 2022. Mhdml: construction of a medical lakehouse for\nmulti-source heterogeneous data, in: International Conference on Health Information Science, Springer. pp. 127‚Äì135.\nXie,X.,Shi,S.,Wang,H.,Li,M.,2023.SAT:samplingaccelerationtreeforadaptivedatabaserepartition.WorldWideWeb(WWW)26,3503‚Äì3533.\nURL: https://doi.org/10.1007/s11280-023-01199-3 , doi: 10.1007/S11280-023-01199-3 .\nXue, J., Wang, Y., Tian, Y., Li, Y., Shi, L., Wei, L., 2021. Detecting fake news by exploring the consistency of multimodal data. Information\nProcessing & Management 58, 102610.\nYang, Z., Chandramouli, B., Wang, C., Gehrke, J., Li, Y., Minhas, U.F., Larson, P.√Ö., Kossmann, D., Acharya, R., 2020. Qd-tree: Learning data\nlayouts for big data analytics, in: Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, pp. 193‚Äì208.\nZhang, S., Ray, S., Lu, R., Zheng, Y., 2021. Sprig: A learned spatial index for range and knn queries, in: Proceedings of the 17th International\nSymposium on Spatial and Temporal Databases, pp. 96‚Äì105.\nZhao, S., Guo, X., Qu, Z., Zhang, Z., Yu, T., 2022. Intelligent retrieval method for power grid operation data based on improved simhash and\nmulti-attribute decision making. Scientific Reports 12, 20994.\nZheng, J., Li, Q., Liao, J., 2021. Heterogeneous type-specific entity representation learning for recommendations in e-commerce network.\nInformation Processing & Management 58, 102629.\nM. Sheng et al.: Preprint submitted to Elsevier Page 44 of 44",
  "textLength": 160191
}