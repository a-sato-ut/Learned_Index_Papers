{
  "paperId": "7cda5ee0fbbfe14ad4339412d75c2aa720ecdd9e",
  "title": "Massivizing Computer Systems: A Vision to Understand, Design, and Engineer Computer Ecosystems Through and Beyond Modern Distributed Systems",
  "pdfPath": "7cda5ee0fbbfe14ad4339412d75c2aa720ecdd9e.pdf",
  "text": "Massivizing Computer Systems: a Vision to Understand, Design, and Engineer\nComputer Ecosystems through and beyond Modern Distributed Systems\nAlexandru Iosup\nDepartment of Computer Science,\nFaculty of Sciences, VU Amsterdam,\nThe Netherlands\nA.Iosup@vu.nlAlexandru Uta\nDepartment of Computer Science,\nFaculty of Sciences, VU Amsterdam,\nThe Netherlands\nA.Uta@vu.nlThe AtLarge Team\u0006\nVU Amsterdam and\nDelft University of Technology,\nThe Netherlands,\nhttps://atlarge-research.com\nAbstract —Our society is digital: industry, science, governance,\nand individuals depend, often transparently, on the inter-\noperation of large numbers of distributed computer systems.\nAlthough the society takes them almost for granted, these\ncomputer ecosystems are not available for all, may not be\naffordable for long, and raise numerous other research chal-\nlenges.\nInspired by these challenges and by our experience with\ndistributed computer systems, we envision Massivizing Com-\nputer Systems, a domain of computer science focusing on un-\nderstanding, controlling, and evolving successfully such ecosys-\ntems. Beyond establishing and growing a body of knowledge\nabout computer ecosystems and their constituent systems, the\ncommunity in this domain should also aim to educate many\nabout design and engineering for this domain, and all people\nabout its principles. This is a call to the entire community:\nthere is much to discover and achieve.\n1. Introduction\nThe modern lifestyle depends on computer1ecosystems .\nWe engage increasingly with each other, with governance,\nand with the Digital Economy [2] through diverse computer\necosystems comprised of globally distributed systems, de-\nveloped and operated by diverse organizations, interoperated\nacross diverse legal and administrative boundaries. These\ncomputer ecosystems create economies of scale, and un-\nderpin participation and innovation in the knowledge-based\nsociety: for example, in the European Union, information\n\rThe AtLarge team members co-authoring this article are: Georgios An-\ndreadis, Vincent van Beek, Erwin van Eyk, Tim Hegeman, Sacheendra\nTalluri, Lucian Toader, and Laurens Versluis.\n1. The analysis by E.W. Dijkstra [1] explains the main differences\nbetween “computer” and “computing” science: origins in the US vs. in the\n(current) EU, respectively, with American CS seen in the past as “more\nmachine-oriented, less mathematical, more closely linked to application\nareas, more quantitative, and more willing to absorb industrial products\nin its curriculum”. The differences have now softened, and participants\nbeyond US and EU have since joined our community.and communication technology (ICT)2, for which all ser-\nvices are migrating to computer ecosystems3, accounts for\nnearly 5% of the economy and accounts for nearly 50% of\nproductivity growth4. However positive, computer ecosys-\ntems are not merely larger, deeper structures (e.g., hierar-\nchies) of distributed computer systems. Although we have\nconquered many of the scientiﬁc and engineering challenges\nof distributed computer systems5, computer ecosystems add\nnumerous challenges stemming from the complexity of\nstructure, organization, and evolving and emerging use. We\nenvision in this work how computer systems can further\ndevelop as a positive technology for our society.\nVision : We envision a world where individuals and\nhuman-centered organizations are augmented by an\nautomated, sustainable layer of technology. At the\ncore of this technology is ICT, and at the core of\nICT are computer ecosystems, interoperating and\nperforming as utilities and services, under human\nguidance and control. In our vision, ICT is a fun-\ndamental human right, including the right to learn\nhow to use this technology.\nWe see a fundamental crisis, the ecosystems crisis , al-\nready at work and hampering our vision. The natural evo-\nlution from early Computer Systems to modern Distributed\nSystems has been until now halted by relatively few crises,\namong which standing out is the software crisis of the 1960s,\ndue to unbounded increase in complexity [9], [10]. We see\n2. ICT loosely encompasses all technology and processes used to process\ninformation (the “I”) and for communications (the “C”). Historically, the\ndistinction between the “I” and the “C” in ICT can be traced to the\nearly days of computing, where information was stored and processed\nasdigital data, and most communication was based on analog devices.\nThis distinction has lost importance starting with the advent of all-digital\nnetworks, completed in the 1990s Internet.\n3. Computer ecosystems build a world of cloud computing [3], artiﬁcial\nintelligence [4], and big data [5], underpinned by diverse software systems\nand networks interconnecting datacenters [6], and edge [7]/smart devices.\n4. Correspondingly, ICT receives about 25% of all business R&D fund-\ning and is at the core of EU’s H2020 programme, see https://ec.europa.eu/\nprogrammes/horizon2020/en/area/ict-research-innovation.\n5. M. van Steen and A. Tanenbaum provide an introduction [8].arXiv:1802.05465v2  [cs.DC]  22 Feb 2018\n\nthe ongoing ecosystems crisis as due to similar reasons,\nand leading the Distributed Systems ﬁeld to a fundamental\ndeﬁcit of knowledge and of technology6, with abundant fore-\nwarnings. In Section 2, we deﬁne and give practical exam-\nples of ﬁve fundamental problems of computer ecosystems\nthat we believe apply even to the most successful of the tech\ncompanies, such as Amazon, Alibaba, Google, Facebook,\netc. but even more so to the small and medium enterprises\nthat should develop the next generation of technology: (i)\nlacking the core laws and theories of computer ecosystems;\n(ii) lacking the technology to maintain today’s computer\necosystems; (iii) lacking the instruments to design, tune,\nand operate computer ecosystems against foreseeable needs;\n(iv) lacking peopleware7knowledge and processes; and (v)\ngoing beyond mere technology.\nOur vision, of Massivizing Computer Systems , focuses\non rethinking the body of knowledge, and the peopleware\nand methodological processes, associated with computer\necosystems. We aim to reuse what is valuable and available\nin Distributed Systems, and in the complementary ﬁelds of\nSoftware Engineering and Performance Engineering, and to\nfurther develop only what is needed. Grid computing and\ncloud computing, which both leverage the advent of the\nNetworked World8, of modern processes for the design and\ndevelopment of software systems, and of modern techniques\nfor performance engineering, are sources of technology for\nutility computing9. However, grid computing has succumbed\nto the enormous complexity of the ecosystems crisis, for ex-\nample, it did not reach needed automation for heterogeneous\nresources and non-functional requirements such as elasticity,\nand did not develop appropriate cost models.\nArmed with knowledge and practical tools similar to\ngrid computing, the pragmatic and economically viable Dis-\ntributed Systems domain of cloud computing started with the\nlimited goal of building a digital ecosystem where the core\nis largely homogeneous, and is still primarily operated from\nsingle-organization datacenters. Attempts to expand to more\ndiverse ecosystems have led to problems, some of which we\nhave already covered. Edge-centric computing [7] borrows\nfrom peer-to-peer computing and proposes to shift control\nto nodes at the edge, closer to the user and thus human-\ncentric in its security and trust models, but still relies on\ncurrent cloud technology instead of explicitly managing the\nfull-stack complexity of ecosystems.\n6. Like Arthur [11, Ch.2], we refute the dictionary deﬁnition of technol-\nogy, which superﬁcially places technology in a role secondary to (applied)\nscience. Instead, we use the ﬁrst-principle deﬁnition provided by Arthur:\ntechnology is (i) use-driven, (ii) a group of practices and components, typ-\nically becoming useful through the execution of a sequence of operations,\n(iii) the set of groups from (iv) across all engineering available to a human\nculture, forming thus Kevin Kelly’s “technium”.\n7. “If the organization is a development shop, it will optimize for the\nshort term, exploit people, cheat on the workplace, and do nothing to\nconserve its very lifeblood, the peopleware that is its only real asset. If\nwe ran our agricultural economy on the same basis, we’d eat our seed corn\nimmediately and starve next year.” [12, Kindle Loc. 1482-1484].\n8. Of which the Internet is a prominent example, but which further\nincludes networking in supercomputing, telco, and IoT-focused industries.\n9. We trace the use of “utility computing” in scientiﬁc publications to\nAndrzejak, Arlitt, and Rolia [13], and to Buyya [14].We propose to complement and extend the existing\nbody of knowledge with a focus on Massivizing Computer\nSystems, with the goal of deﬁning and supporting the core\nbody of knowledge and the skills relevant to this vision.\n(This path is successfully followed by other sciences with\nsigniﬁcant impact in the modern society, such as physics\nand its impact on high-precision industry, biology and its\nimpact on healthcare, ecology and its impact on wellbeing,\netc.) Toward this goal, we make a ﬁve-fold contribution:\n1) We propose the premises of a new ﬁeld10of sci-\nence, design, and engineering focusing on MCS (in\nSection 3). To mark this relationship with the vi-\nsion, we also call the ﬁeld MCS. We deﬁne MCS\nas a part of the Distributed Systems domain, but\nalso as synthesizing methods from Software Engi-\nneering and Performance Engineering.\n2) We propose ten core principles (Section 4). MCS\nhas not only a technology focus, but also considers\npeopleware and co-involvement of other sciences.\nOne of the principles has as corollary the periodic\nrevision of principles, and MCS will apply it—a\ncommunity challenge.\n3) We express the current systems, peopleware, and\nmethodological challenges raised by the ﬁeld of\nMCS (in Section 5). We cover diverse topics of\nresearch that evolve naturally from ongoing com-\nmunity research in Distributed Systems, Software\nEngineering, and Performance Engineering. We\nalso raise challenges in the process of designing\necosystems and their constituent systems.\n4) We predict the beneﬁts MCS can provide to a\nset of pragmatic yet high-reward application do-\nmains (in Section 6). Overall, we envision that\ncomputer ecosystems built on sound principles will\nlead to signiﬁcant beneﬁts, such as economies of\nscale, better non-functional properties of systems,\nlowering the barrier of expertise needed for use,\netc. We consider as immediate application areas big\nand democratized (e-)science, the future of online\ngaming and virtual reality, the future of banking,\ndatacenter-based operations including for hosting\nbusiness-critical workloads, and serverless app de-\nvelopment and operation.\n5) We compare MCS with other paradigms (Sec-\ntion 7). We explicitly compare MCS with the\nparadigms emerging from Distributed Systems, in-\ncluding grid, cloud, and edge-centric computing.\nWe further compare MCS with paradigms across\nother sciences and technical sciences.\n10. As conjectured by Denning [15], there is a high threshold for\nbecoming a ﬁeld of science, paraphrasing: focus on the natural and ar-\ntiﬁcial processes of a pervasive phenomenon, a body of knowledge and\nskills that can be codiﬁed and taught, experimental methods of discovery\nand validation, reproducibility of results and falsiﬁability of theoretical\nconstructs, the presence of meaningful discovery itself. Even if MCS does\nnot pass this threshold, the process of exploring it as a new ﬁeld can lead\nto surprising discoveries, as in other sciences (see Section 7).\n\n2. The Problem of Computer Ecosystems\nIn this section, we introduce systems, ecosystems, and\nﬁve fundamental problems of computer ecosystems.\n2.1. What Are Systems and Ecosystems?\nWe use Meadows’ deﬁnition of systems [16, p.188]:\nDeﬁnition : A system is “a set of elements or parts\ncoherently organized and interconnected in a pat-\ntern or structure that produces a characteristic set\nof behaviors, often classiﬁed as its “function” or\n“purpose.”\nThe system elements or parts can be systems themselves,\nproducing more ﬁne-grained functions. We see computer\necosystems as more than just complex computer systems, in\nthat they interact with people and have structure that is more\nadvanced, combinatorial and hierarchical as is the general\nnature of technology [11], etc.:\nDeﬁnition : A computer ecosystem is a heteroge-\nneous group of computer systems and, recursively,\nof computer ecosystems, collectively constituents .\nConstituents are autonomous, even in competition\nwith each other. The ecosystem structure and or-\nganization ensure its collective responsibility: com-\npleting functions with humans in the loop, providing\ndesirable non-functional properties that go beyond\ntraditional performance, subject to agreements with\nclients. Ecosystems experience short- and long-term\ndynamics : operating well despite challenging, pos-\nsibly changing conditions external to the control of\nthe ecosystem.\nCollective Responsibility: The ecosystem is designed to\nrespond to functional and non-functional requirements. The\necosystem constituents must be able to act independently\nof each other, but when they act collectively they can\nperform collective functions that are required and that are\nnot possible for any individual system, and/or they can add\nuseful non-functional characteristics to how they perform\nfunctions that could still be possible otherwise. At least\nsome of the collective functions involve the collaboration\nof a signiﬁcant fraction of the ecosystem constituents.\nBeyond Performance: When collaborating, the ecosys-\ntem constituents optimize or satisﬁce a decision problem\nfocusing on the trade-off between subsets of both the func-\ntional and the non-functional requirements, e.g., correct\nfunctional result and high performance vs. cost and avail-\nability. The non-functional requirements are diverse, beyond\ntraditional performance: e.g., high performance, high avail-\nability and/or reliability, high scalability and/or elasticity,\ntrustworthy and/or secure operation.\nAutonomy: ecosystem constituents can often operate\nautonomously if allowed, and may be self-aware as deﬁned\nDremel\nService \nTreeSQL HivePig JAQL\nMapReduce Model PACT\nMPI/\nErlang\nL\nF\nSNephele Dryad Hadoop/\nYARNHaloopDryadLINQScope\nPregel\nHDFS CosmosFSAzure\nEngineTera\nData\nEngine\nAzure\nData \nStoreTera\nData\nStoreStorage \nEngineExecution \nEngine\nVoldemortHigh-Level Language\nProgramming \nModel\nGFSBigQueryFlume\nFlume\nEngine\nS3Dataflow\nGiraphSawzall Meteor\n* Plus Zookeeper, CDN, etc.Figure 1. A view into the ecosystem of Big Data processing. (Reproduced\nand adapted from our previous work [22].) The four layers, High-Level\nLanguage ,Programming Model ,Execution Engine , and Storage Engine ,\nare conceptual, but applications that run in this ecosystem typically use\ncomponents across the full stack of layers (and more, as indicated by the\n\u000e). The highlighted components cover the minimum set of layers necessary\nfor execution for the MapReduce and Pregel sub-ecosystems.\nby Kounev et al. [17, Def.1.1]: they could continuously\n“learn models capturing knowledge about themselves and\nthe environment”, “reason using the models [...] enabling\nthem to act [...] in accordance with higher-level goals, which\nmay also be subject to change.”\nHow do ecosystems appear? Computer ecosystems\nappear naturally11, through a process of evolution that\ninvolves accumulation of technological artifacts in inter-\ncommunicating assemblies and hierarchies, and solving\nincreasingly more sophisticated problems12. Real-world\necosystems are distributed or include distributed systems\namong their constituents [8], and are operated by and for\nmultiple (competitive) stakeholders. Components often are\nheterogeneous, built by multiple developers, not using a\nveriﬁed reference architecture, and having to ﬁt with one\nanother despite not being designed end-to-end.\nA simpliﬁed example of ecosystems, sub-ecosystems,\nand their constituents: Developing applications, and tun-\nning, swapping, and adding or removing components re-\nquires a deep understanding of the ecosystem. Figure 1\ndepicts the four-layer reference architecture of the big\ndata ecosystem frequently used by the community. In this\necosystem, the programming model, e.g., MapReduce, or\nthe execution engine, e.g., Hadoop, typically give name to\nan entire family of applications of the ecosystem, i.e., “We\nrun Hadoop applications.” Such families of applications,\nand the components needed to support them, form complex\n(sub-)ecosystems themselves; this is a common feature in\ntechnology [11, Ch. “Structural Deepening”]. To exemplify\nthe big data ecosystem focusing on MapReduce, the ﬁgure\nemphasizes components in the bottom three layers, which\nare typically not under the control of the application devel-\noper but must nevertheless perform well to offer good non-\nfunctional properties, including performance, scalability, and\n11. Similar ecosystems appear in many areas of technology [11, Ch.2,7–\n9], and in many other kinds of systems [18, Ch.5–8].\n12. The co-evolution of problems and solutions appears in all areas of\ndesign [19], [20] [21, Ch.I, S2–5].\n\nreliability. This is due to vicissitude [22] in processing data\nin such ecosystem, that is, the presence of workﬂows of\ntasks that are arbitrarily compute- and data-intensive, and\nof unseen dependencies and (non-)functional issues.\nExamples of large-scale computer ecosystems: Un-\nlike their constituents, ecosystems are difﬁcult to iden-\ntify precisely, because their limits have not been deﬁned\nat design time, or shared in a single software repository\nor hardware blueprint. Large-scale examples of computer\necosystems include: (i) the over 1,000 Apache cloud and big\ndata components published as open-source Apache-licensed\nsoftware13, (ii) the Amazon AWS cloud ecosystem, which\nis further populated by companies running exclusively on\nthe AWS infrastructure, such as Netﬂix14, (iii) the emerging\necosystem built around the MapReduce and Spark15big-data\nprocessing systems.\nWhen is a system not an ecosystem? Under our deﬁni-\ntion, not every system can be an ecosystem, and even some\nadvanced systems do not qualify as ecosystems, including:\n(i) existing audited systems are rarely built as ecosystems,\nand especially avoid including multi-party software and\ntoo autonomous components, (ii) legacy monolithic systems\nwith tightly coupled components, (iii) legacy systems devel-\noped with relatively modern software engineering practices,\nbut which do not consider the sophisticated non-functional\nrequirements of modern stakeholders, (iv) systems devel-\noped for a speciﬁc customer or a speciﬁc business unit of\nan organization, which now need to offer open-access for\nmany and diverse clients.\nIt may not be possible to distinguish for all existing\nsystems whether they are within the scope of this work’s\ndeﬁnition of ecosystems. This type of ambiguity exists in\nthe deﬁnition of many new domains of computer science that\nare not tightly coupled to a speciﬁc technology, including\nembedded systems, meta-computing and grid computing\nsystems, cloud computing systems, and big data systems.\nThe ambiguity allows these ﬁelds to be diverse and useful,\nas rich ﬁeld of science and engineering.\n2.2. Fundamental Problems of Ecosystems\nThe ﬁrst fundamental problem is that we lack the system-\natic laws and theories to explain and predict the large-scale,\ncomplex operation and evolution of computer ecosystems.\nFor example, when an ecosystem under-performs or fails\nto meet increasingly more sophisticated non-functional re-\nquirements, customers stop using the service [23], [24], but\ncurrently we do not have the models to predict such under-\nperforming situations, or the instruments to infer what could\nhappen, even for simple ecosystems comprised of small\ncombinations of arbitrary distributed systems.\nThe second fundamental problem is that we lack the\ncomprehensive technology to maintain the current computer\necosystems. For example, we know from grid computing\n13. https://github.com/apache\n14. https://github.com/netﬂix\n15. https://github.com/databricksthe damage that a failure can trigger in the entire computer\necosystem [25], [26], [27], and far all the large cloud oper-\nators, including Amazon, Alibaba, Google, Microsoft, etc.,\nhave suffered signiﬁcant outages [28] and SLA issues [24]\ndespite extensive site reliability teams and considerable\nintellectual abilities. In turn, these outages have correlated\nfailures, as for example experienced when drafting this and\nother articles on the Amazon-based Overleaf. Moreover, we\nseem to have opened a Pandora’s Box of poorly designed\nsystems, which turned into targets and sources of cyber-\nattacks (e.g., hacking16, ransomware17, malware [29], and\nbotnets [30]).\nThe third fundamental problem is that we are not\nequipped to explore the future of computer ecosystems, and\nin particular we cannot now design, tune, and operate the\ncomputer ecosystems that can seamlessly support all the\nsocietally relevant application domains, to the point where\nmultiple, possibly competitive, organizations and individuals\ncan use computing as an utility (similarly to the electricity\ngrid, including its local shifts toward decentralized smart\ngrids) or as a service (as for the logistics and transporta-\ntion industry). For example, sophisticated users are already\ndemanding but not receiving detailed control over heteroge-\nneous resources and services, the right to co-design services\nwith functional requirements offered through everything as\na service [31], and the opportunity to control detailed facets\nof non-functional characteristics such as risk management,\nperformance isolation, and elasticity [32].\nThe fourth fundamental problem is that of peopleware,\nespecially because the personnel designing, developing, and\noperating these computer ecosystems already numbers mil-\nlions of people world-wide but is severely understaffed and\nwith insufﬁcient replacement available [33].\nThe ﬁfth fundamental problem is participating in the\nemerging practice beyond mere technology. Compounding\nthe other problems, the Distributed Systems community\nseems to focus excessively on technology, a separation of\nconcerns that was perhaps justiﬁable but is becoming self-\ndefeating. This focus has brought until now important ad-\nvantages in producing rapidly many successful ecosystems,\nbut is starting to have important drawbacks: (i) we have to\nanswer difﬁcult, interdisciplinary questions about how our\nsystems inﬂuence the modern society and its most vulner-\nable individuals [34], and in general about the emergence\nof human factors such as (anti-)social behavior [35], (ii) we\nhave to investigate general and speciﬁc questions about the\nevolution of systems, including how the knowledge and skill\nhave concentrated in relatively few large-scale ecosystems?,\nand what to do, and with which interdisciplinary toolkit,\n16. Examples: https://www.washingtonpost.com/world/national-\nsecurity/israel-hacked-kaspersky-then-tipped-the-nsa-that-its-tools-had-\nbeen-breached/2017/10/10/d48ce774-aa95-11e7-850e-2bdd1236be5d\nstory.html, http://www.bbc.com/news/technology-42056555,\nhttps://www.volkskrant.nl/tech/dutch-agencies-provide-crucial-intel-\nabout-russia-s-interference-in-us-elections \u0012a4561913/\n17. Examples: https://arstechnica.com/information-technology/2017/\n05/virulent-wcry-ransomware-worm-may-have-north-koreas-ﬁngerprints-\non-it/, https://www.theverge.com/2016/11/27/13758412/hackers-san-\nfrancisco-light-rail-system-ransomware-cybersecurity-muni\n\nMassivizing Computer Systems ( x3.1)\nWho? Stakeholders scientists, engineers, designers, others\nWhat? Central Paradigm properties derived from ecosystem\nFocus structure, organization, and dynamics\nConcerns functional and non-functional properties\nemergence, evolution\nHow? Design design methods and processes\nQuantitative measurement, observation\nExper. & Sim. methodology, TRL, benchmarking\nEmpirical correlation, causality iff. possible\nInstrumentation experiment infrastructure\nFormal models validated, calibrated, robust\nRelated Computer science Distrib.Sys., Sw.Eng., Perf.Eng.\n(x3.5) Systems/complexity General Systems Theory, etc.\nProblem solving computer-centric, human-centric\nTABLE 1. A N OVERVIEW OF MCS.\nto prevent this from hurting competition and future innova-\ntion [36]?\n3. Massivizing Computer Systems ( MCS )\nIn this section, we introduce the fundamental concepts\nand principles of MCS. We explain its background, give\na deﬁnition of MCS, explain its goal and central premise,\nand focus on key aspects of this domain. We explain how\nMCS extends the focus of traditional Distributed Systems,\nand how it synthesizes research methods from other related\ndomains.\n3.1. What Is MCS ?\nWe now deﬁne MCS as a use-inspired discipline [37]:\nDeﬁnition : MCS focuses on the science, design,\nand engineering of ecosystems. It aims to understand\necosystems and to make them useful to the society.\nTable 1 summarizes MCS: Who? What? How? Which\nother core issues? (all addressed in this section) and What\nare the related concepts MCS draws from? (addressed in\nSection 3.5). We now elaborate on each part, in turn.\nWho? Stakeholders: MCS involves a large number\nof stakeholders, characteristic and necessary for a domain\nthat applies to diverse problems with numerous users. We\nconsider explicitly the scientists, engineers, and designers of\nMCS systems involved in solving the numerous challenges\nof the ﬁeld (discussed in Section 5) and in using results in\npractice, the industry clients and their diverse applications\n(Section 6), the governance and legal stakeholders, etc. We\nalso consider as stakeholders the population: individuals at-\nlarge, as clients and as (life-long) students.Goal : The goal of MCS is to understand and even-\ntually control complex ecosystems and, recursively,\ntheir constituent parts, thus satisﬁcing possibly dy-\nnamic requirements and turning ecosystems into\nefﬁcient utilities. To this end, MCS must explain\nhow and why the ecosystem differs, functionally\nand non-functionally, from mere composition of its\nconstituents.\nWhat? The Central Premise: MCS starts from the\npremise that the interaction between systems in an ecosys-\ntem, and the way the ecosystems stakeholders interact with\nthe ecosystem (and among themselves), drives to a large\nextent the operation and characteristics of the ecosystem.\nThus, MCS focuses explicitly on the structure, organization,\nand dynamics of systems when operating in assemblies,\nhierarchies, and larger ecosystems, rather than understanding\nand building single systems working in isolation.\nBoth the functional and the non-functional properties of\nthese ecosystems, and recursively of their constituent sys-\ntems, are central to understanding and engineering ecosys-\ntems.\nOver periods of time both that are short (seconds to\ndays) or long (weeks to years), ecosystems may experience\nvarious forms of emergent and chaotic behavior , and of\nevolution (discussed in the following). Understanding emer-\ngent and evolutionary behavior, and controlling it subject to\nefﬁciency18considerations, is also central to MCS.\nHow? A general approach and methodology: To begin\nwork on MCS, we consider the following elements that\nwill need to be adapted, extended, and created for computer\necosystems, and ultimately will result in new approaches and\nmethodologies: (i) methods and processes characteristic to\ndesign [19], [20], and design science applied to information\nsystems [38] and to the design of (computer) systems; (ii)\nquantitative research, in particular collection of data through\nmeasurement and (longitudinal) observation, statistical mod-\neling of workloads [39], failures [26], [27], and reaching\nformal (analytical) models; (iii) experimental research, in-\ncluding real-world experimentation through prototypes, and\nsimulation, both under realistic workload conditions and\neven under community-wide benchmarking settings; (iv)\nempirical and phenomenological research, including qual-\nitative research resulting in comprehensive surveys [40] and\nﬁeld surveys; (v) modern system evaluation, using instru-\nmentation beyond what is needed to test typical Distributed\nSystems (e.g., large-scale infrastructure comparable with\nmedium-scale industry infrastructure [41]), focusing on an\nextended array of indicators and metrics (e.g., performance,\navailability, cost, risk, various forms of elasticity [32]), and\ndeveloping approaches for meaningful comparison across\nmany alternatives for the same component [42] or policy\npoint [43].\n18. Although process economics is better equipped than MCS to address\ncosting, pricing, and utility functions, in practice designers and engineers\nare expected to conduct or at least provide quantitative input for these tasks.\n\nHow? Other issues: We envision several other core\nissues important for MCS: (i) peopleware: processes for\ntraining, educating, engaging people, especially the next\ngeneration of scientists, designers, and engineers, (ii) mak-\ning available free and open-access artifacts, both open-\nsource software and common-format data, (iii) ensuring\na balance of recognition between scientiﬁc, design, and\nengineering outcomes, across the community, and (iv) ethics\nand other interdisciplinary issues.\n3.2. More on the Central Premise\nAmong the core aspects of the central premise, we see\nthe structure, organization, and dynamics of ecosystems, and\nthe functional and non-functional properties as being derived\nand expanded directly from Distributed Systems community,\nwith the main difference being that we focus here on the\nlarger, more complex ecosystems19. We now elaborate in\nturn on two distinguishing aspects of the central premise,\nemergence and chaotic behavior, and evolution.\nEmergence and chaotic behavior , both functional20\nand non-functional21, due to humans use or other non-\ndeterministic elements. Beyond classic emergence from\nComplex Adaptive Systems and the related domains of Gen-\neral Systems Theory (see Section 3.5), we consider within\nthe scope of MCS various biologically and socially inspired\nmechanisms of non-technical behavior that may change the\nneeds and thus use of the system, such as exaptation [47],\nsocial [48] and meta [49], [50] use of systems, toxicity [35]\nand other disruptive behavior, etc.\nEvolution: Over long periods time, MCS ecosystems\nevolve through internal (technology push) and external (so-\nciety pull) pressures. The mechanisms of evolution in-\nclude [11, Ch.9]: combining components into larger assem-\nblies, removing redundant or useless components, replac-\ning components with more advanced components, bridging\nbetween components and adapting the end-points of com-\nponents, adding new components to address new functions\nand new non-functional requirements, etc. Importantly, like\nArthur we envision that ecosystem evolution can be at\ntimes Darwinian , that is, incremental, selecting and vary-\ning closely related components of pre-existing technology,\nwith the better approaches propagating over technology\ngenerations; but also that ecosystem evolution can be non-\nDarwinian , that is, radically different and abrupt, combining\nseemingly unrelated technology and/or addressing novel\nneeds, with seemingly random events—which ecosystem\nadopted the technology ﬁrst, which individual co-sponsored\n19. Understanding assemblies where components are provided by differ-\nent developers, and used by multiple stakeholders, is challenging.\n20. DNS tunneling [44] is just one of the many examples of changing the\nfunction of a design: here, from facilitating access to the Web, to enabling\narbitrary Internet trafﬁc and thus signiﬁcant security breaches. Because the\necosystem is already too complex to supervise, it turns out DNS tunneling\nis also not a prime target of automated protection.\n21. For example, in the ﬁeld of big data, the community is starting to\nunderstand ecosystem performance as a complex function of Varbanescu’s\n“P-A-D Triangle” (i.e., platform, algorithm including data structures, and\ndataset). We have tested this empirically for graph processing [45], [46].the invention, how quickly it started to gain market share and\nother soft lock-in elements—contributing to the propagation\nof the technology. The mechanisms of ecosystem evolution\nare within the scope of MCS.\n3.3. More on the General Approach\nDesign: By deﬁnition, MCS employs a diverse body of\nknowledge and skill typical to modern science and engi-\nneering , from which we further distinguish design22. The\nwork we conduct in this ﬁeld aims to go beyond random\nwalks, and direct application or replication of prior work,\nWe aim to establish design methods and processes , based on\nprinciples and on instruments, that meet the goal of MCS.\nWe envision here, as a ﬁrst step, adapting and extending\ntechniques from the design of information systems [38] and\nof computer systems, and also from design not related to\ncomputers [20], [51] or even to technology [19].\nQuantitative results: Obtain quantitative, predictive,\nactionable understanding about the sophisticated functional\nand non-functional properties of ecosystems, and about\ntheir dynamics. It is here that advances in Performance\nEngineering, especially measurement and statistically sound\nobservation , can help the domain of MCS get started.\nSpeciﬁcally, collecting data from running ecosystems and\nfrom experimental settings, both real-world and simulated\n(see following heading), we can start accumulating knowl-\nedge . (The step to understanding cannot be fully automated,\nbecause it is dependent on the imagination of the people in\nthe loop.) These would lead to observational models, and,\nlater, possibly also to calibrated mechanistic models and\nfull-system (weakly emergent [18, p.171]) models.\nExperimentation and simulation: MCS depends on\nmethodologically sound real-world23and simulation-based24\nexperiments, which have complementary strengths and\nweaknesses but combined can provide essential feedback\nto scientists, engineers, and designers. Experimentation is\nvaluable in validating and demonstrating the technology-\nreadiness level (TRL)25of various concepts and theories, us-\ningprototypes or even higher-TRL artifacts running prefer-\nably in real-world environments26, in providing calibration\nand measurement data, in revealing aspects that we have\nnot considered before, etc. Benchmarking , a subﬁeld of\n22. We adopt here the argument made by Cross in the 1970s, and\nextended by Lawson [19, Ch.8, loc.2414, and Ch.16, loc.4988], that design\nis a distinct way of thinking about real-world problems with high degree\nof uncertainty, and of solving them: problems and solutions co-evolve .\n23. MCS follows the multi-decade tradition of experimental computer\nscience [52], [53], [54] and Distributed Systems [41], [55].\n24. Simon makes a compelling case that simulation can lead to new\nunderstanding, of both computer systems about which we know much and\nabout which we do not [18, Section “Understanding by Simulating”]. He\nrefutes that a simulator is “no better than the assumptions built into it”,\nthat they cannot reveal unexpected aspects, that they only apply for systems\nwhose laws of operation we already know.\n25. http://www.earto.eu/ﬁleadmin/content/03 Publications/The TRL\nScale asaRIPolicy Tool -EARTO Recommendations -Final.pdf\n26. Like Tichy [53], we disagree that mere demonstrations and proof-\nof-concepts can replace experimentation and simulation, even if they prove\nvaluable for engineering products and educating stakeholders.\n\nexperimentation, focuses the community on a set of common\nprocesses, knowledge, and instrumentation. Good bench-\nmarks often make experimentation also more affordable and\nfair, by establishing for the community a set of meaningful\nyet tractable experiments. Simulation is useful in investigat-\ning and comparing known and new designs, and dynamics\nincluding non-deterministic behavior, over long periods of\nsimulated-time. Simulation, and to some extent also real-\nworld experimentation, can also be used to replay interesting\nconditions from the past, giving the human in the loop more\ntime and more instruments to understand.\nEmpirical (correlation), and if possible also phe-\nnomenological (causal), research is necessary27, if we are\nto understand and control especially the emergent properties\nof ecosystems. Observation and measurement, and experi-\nmentation and simulation of ecosystems already are empiri-\ncal methods, with their beneﬁts and drawbacks, for studying\nand engineering the systems comprising the ecosystems.\nAdditionally, MCS must also study empirically the highly\nvariable, possibly non-deterministic processes that include\nhumans: their use of ecosystems and their new (practical)\nproblems with using ecosystems, and their study, design,\nand engineering of ecosystems. This latter part is much less\ndeveloped in Distributed Systems, but a rise in empirical\nmethods in Software Engineering [53], [59] and in design\nsciences [20, Ch.1] already employs: studying the artifacts\nthemselves (e.g., with static code analysis), interviews with\ndesigners, observations and case studies of one or several\ndesign projects, experimental studies typically of synthetic\nprojects, simulation by letting computers try to design and\nobserving the results, and reﬂecting and thinking about own\nexperience. The beneﬁts of using these methods include\ndeeper, including practical, understanding. The dangers in-\nclude relying on “soft methods” [53] and ignoring the\n“threats to validity” [59].\nInstrumentation: Similarly to other Big Science do-\nmains, such as astrophysics, high-energy physics, genomics\nand systems biology, and many other domains reliant today\non e-Science, MCS requires signiﬁcant instrumentation. It\nneeds adequate environments to experiment in, for example,\nthe DAS-5 in the Netherlands [41] and Grid’5000 in France.\nAs in the other natural sciences, creating these instruments\ncan lead to numerous advances in science and engineering;\nmoreover, these instruments are ecosystems themselves and\nthus an endogenous object of study for MCS. MCS also\nneeds the infrastructure needed to complement the human\nmind in the task of understanding the data collected about\necosystems, to generate hypotheses automatically, and to\npreserve this data for future generations of scientists, de-\nsigners, and engineers.\nFormal (analytical) models: We envision that a com-\nplex set of formal mathematical models, validated and cal-\n27. As a matter of pragmatism, our empirical research may need to\nbe data-driven (that is, discovery science [56]), instead of hypothesis-\ndriven, simply because the complexity of the problems seems to exceed\nthe capabilities of the unaided human mind. This is also the case made\nsince the mid-2000s by Systems Biology [56], [57] [58, Ch.1] and other\nsciences.ibrated with long-term data, robust and with explanatory\npower beyond past data, will emerge over time to support\nMCS. Such models will likely be hierarchical, compo-\nnentized. The key challenge to overcome for meaningful,\npredictive modeling is to support the dynamic, deeply hi-\nerarchical, emergent nature of modern ecosystems. There\nmay not be a steady-state, for example when users seem\nto behave chaotically, or high resource utilization triggers\nbursty resource (re-)leases in clouds.\nModels at different levels must support ordinary and\npartial differential equations (ODEs and PDEs have mul-\ntiple independent control-variables), time-dependent evolu-\ntion and events, discrete states and Boolean logic, stochastic\nproperties for each component and behavior, and capture\nemergent and feedback-based behavior (collectively, forms\nofecosystem-wide non-linearity ). Unlike other models used\nin traditional and computational sciences, models in MCS\nwill also need to capture the human-created design prin-\nciples and processes underlying the ecosystems, including\ntheir non-Darwinian evolution [11, Ch.6, loc.1875]. Thus,\nthe emerging models will likely be complex, unlike the ﬁrst-\norder approximations of classical physics, and may require\ncomputers to manipulate. Even then, the curse of dimen-\nsionality, i.e., too many states and parameters to explore,\nmay make these models intractable for online predictions.\n3.4. More on Other Issues\nThe Distributed Systems community seems to have al-\nready agreed on new education processes and is making\nprogress toward our notions of peopleware support. It has\nalso agreed that the release of software [60] and data ar-\ntifacts is beneﬁcial, although the funding and recognition\nare still lagging behind. MCS can build on this agreement\nand focus in this context on computer ecosystems. We now\nfocus on the third and fourth issues.\nThe balance of recognition: It is now common in the\ncomputing community, but hurtful to both the results and to\nthe community itself, to consider science above engineer-\ning28or vice-versa29, or to dismiss that design can be an\nindependent task30. In contrast, MCS explicitly postulates\nthat all jobs in this domain resulting in meaningful knowl-\nedge and technology are equally inspiring and useful, and\nthus should be equally prestigious. Science in this domain\ndiscovers artiﬁcial phenomena to be used in ecosystems, and\nthus operates in the continuum between curiosity-driven and\napplied, and is most commonly use-inspired in the sense\n28. It was and seems to remain common for science to dismiss engi-\nneering as merely an applied science, in general [61] [51, p.3-4].\n29. This appears to be a reverse process, in which engineers see scientiﬁc\ntheories as overly idealistic, abstract, and ignorant of actual conditions [61].\nAnecdotally, Andy Tanenbaum, then a student close to the early devel-\nopment of the time-sharing systems at MIT, recounts that the systems\ncommunity of the time had little to do with the contemporary theoretical\nadvances in queueing theory and modeling. Later, when starting Minix,\nhe was leading a team trying to make a running and useful distributed\ncomputer, rather than respond to needs arising from the scientiﬁc commu-\nnity [62]. (Also personal communication, March 2017.)\n30. Deﬁning design as an engineering task is countered by [20] and [63].\n\nof Pasteur Quadrant in the context of computer science, as\nanalyzed by Snir [37]. Engineering in this domain is not\nmere application of recipes; it requires considerable cre-\nativity, skill, and knowledge beyond traditionally scientiﬁc.\nThe third component at the core of MCS, (concept) design,\ndeserves the awe inspired in our society by the creative arts,\nand the respect deserved for solving complex problems.\nEthics and other interdisciplinary issues: Beyond the\nbalance of recognition, we see many issues where historical\naspects and ethics inﬂuence the evolution and development\nof computer ecosystems. We envision here an interdisci-\nplinary community that engages MCS practitioners, includ-\ning Distributed Systems experts, about the principles and the\ntechnology of computer systems; we see here as useful the\nmultidisciplinary invitation sent by the Dagstuhl Seminar on\nthe History of Software Engineering (1996) [p.1] [64].\n3.5. How Far Are We Already?\nTo understand the extent of progress we have made in\nMCS, we need to understand both what techniques and\nprocesses the ﬁeld is comprised of already (discussed in this\nsection), and what applications it can have (in Section 6).\nOverall, MCS has a large, valuable body of knowledge to\nbuild upon, which brings both the opportunity of having\na diverse, tested toolbox and the complex challenge of\nlearning and using it. Figure 2 depicts the evolution in this\nsense of technology, in Distributed Systems and in the com-\nplementary ﬁelds of Software Engineering and Performance\nEngineering.\nCommon ﬁelds of computer science: We see Massiviz-\ning Computer Systems as derived from Distributed Systems,\nwhich in turn are derived from core Computer Systems.\nAdditionally, Massivizing Computer Systems aims to syn-\nthesize interdisciplinary knowledge and skills primarily from\nSoftware Engineering and Performance Engineering. This\nis in agreement with Snir’s view that computer science is\n“one broad discipline, with strong interactions between its\nvarious components” [37], under which subdisciplines rein-\nforce each other, and multidisciplinary and interdisciplinary\nresearch and practice further enable the profession.\nWe have compiled a non-exhaustive list of principles\nand concepts MCS can import from established domains: (i)\nfrom Distributed Systems, scalability as a grand challenge\nextended to the concept of elasticity, communication as ﬁrst-\nclass concern, resource management including migration of\nworkload and sharing of resources, scheduling policies and\nrouting disciplines especially full automation, computational\nmodels including CSP and Valiant’s BSP, geo-distribution\nespecially through replication and sharding, the CAP the-\norem with related theoretical and practical work, concur-\nrency, etc.; (ii) from Computer Systems: hierarchy as basic\narchitecture, the modularity principle, the locality principle,\nthe principle of separation mechanism-policy, the separation\nof data and process, core workload models such as work-\nﬂows and dataﬂows, plus basic AI and machine-learning\ntechniques used for feedback and control loops (e.g., pat-\ntern recognition, signal classiﬁcation, deep learning andCNNs, Bayesian inference, expert systems), etc.; (iii) from\nSoftware Engineering: data structures, algorithms, code and\narchitectural patterns for software, processes for software\nengineering including testing, etc.; (iv) from Performance\nEngineering: many empirical processes, the concept of non-\nfunctional properties as ﬁrst-class concern, and instruments\nand tools to monitor, measure, analyze, model, and predict\nperformance, etc.\nGeneralized systems and complexity theory: We con-\nsider as important especially for the theoretical development\nof MCS the concepts and techniques from Complex Adap-\ntive Systems, and the related domains of General Systems\nTheory, Chaos Theory, Catastrophe Theory, Hierarchical\nTheory, etc.: networks, non-linear effects, non-stationary\nprocesses, control, etc. However, we are also aware that\nmuch distance must be covered between theory and practice,\nrelated to these ﬁelds.\nGeneralized problem-solving: For theories and tech-\nniques of problem solving and problem satisﬁcing31, we\nconsider two classes of techniques: computer-centric and\nhuman-centric .\nFor the former, we identify two wide-ranging and thor-\noughly investigated approaches: satisﬁcing using heuristics,\nand solving or optimizing for simpliﬁed models. Approxi-\nmate solutions generated via heuristics are generally pre-\nferred when ﬁnding optimal solutions is considered in-\ntractable32.\nPossibly the most widely used family of methods to\ninvestigate large solution spaces are the A*algorithm and\nits optimizations, such as the iterative deepening A* . Such\nmethods have been reﬁned by the artiﬁcial intelligence\ncommunity by using guided and procedural search, and\ndeveloped into new ﬁelds of study, such as evolutionary\ncomputing [65], which describes a wide variety of biology-\ninspired search algorithms: genetic algorithms, genetic pro-\ngramming, particle-swarm optimization, learning classiﬁer\nsystems, etc. In domains where data is abundant, data mining\nand machine learning techniques [66] leverage good results\nby extracting knowledge or building predictive models from\nthe available data. Simple heuristics addressing highly spe-\ncialized problems appear in control theory, with practical\napplications for relatively simple mechanical systems.\nIn domains where simpliﬁed (mathematical) models can\nbe drawn, ﬁnding (near-)optimal solutions becomes less\ndifﬁcult than ”blindly” exploring large search spaces. The\nsimpler and most widely used models is the basic linear (in-\nteger) programming method, or the dynamic programming\nparadigm used for ﬁnding (near-)optimal solutions when the\nsolution space can be bounded and well-deﬁned. This set\nof simpler models also includes rule-based expert systems,\nwhere a knowledge base is used as inferencing engine. More\ncomplex models, as the ones deﬁned by queuing theory\nled to seminal results such as Little’s Law , widely used\nin distributed systems, networking and scheduling. Models\n31. Satisﬁcing [18, p.28] is about ﬁnding a solution that meets a set of\nrequirements based on a threshold (“better than X”), instead of the goal of\noptimization to ﬁnd an optimum (“the absolute best”).\n32. For example, when the time to solution is superpolynomial.\n\nFigure 2. Main technologies leading to MCS. MCS is a response to the ecosystems crisis of late-2010s (see Section 1).\nhave also been used successfully for performance analysis\nand prediction. Frameworks such as the Rooﬂine model [67]\nare effective in predicting the performance achieved by\nmodern multicore architectures using only modest numbers\nof parameters (e.g., memory bandwidth, ﬂoating-poing per-\nformance, operational intensity).\nHuman-centric : Because many of the MCS still need de-\nsign and tuning, and because in deployed MCS systems it is\ncommon to have humans-in-the-loop, we also consider and\nplan on educating people about human-centric approaches\nfor problem solving as applied in MCS. Combining the\ntaxonomies proposed by Beitz et al. [68] and by Shah etal. [69], we consider intuitive anddiscursive (that is, recipe-\nbased) techniques. Among the intuitive techniques are: the\nbrainstorming of Osborn, the gallery method of Hellfritz,\nthe lateral-thinking method of DeBono, storyboarding, Fish-\nborne/Ishikawa cause-and-effect diagrams, the synectics of\nGordon, etc. Among the discursive techniques, we consider\nhistory-based techniques such as TRIZ by Altshuller, the\n(general) morphological analysis of Zwicky, design catalogs\nand comprehensive surveys, etc.; and analytical techniques\nsuch as selection and evaluation using systematic charts,\nsingle- and multi-values of merit for rating (and benchmark-\ning) systems, use-value analysis and utility functions, pair-\n\nPrinciple\nType Index Key aspects\nSystems ( x4.1) P1 The Age of Ecosystems\nP2 software-deﬁned everything\nP3 non-functional requirements\nP4 RM&S, Self-Awareness\nP5 super-distributed\nPeopleware ( x4.2) P6 fundamental rights\nP7 professional privilege\nMethodology ( x4.3) P8 science, practice, and culture of MCS\nP9 evolution and emergence\nP10 ethics and transparency\nTABLE 2. T HE10KEY PRINCIPLES OF MCS. (A CRONYMS : RM&S\nSTANDS FOR RESOURCE MANAGEMENT AND SCHEDULING .)\nwise tournaments and competitions, etc.\n4. Ten Core Principles of MCS\nWe introduce in this section ten core principles of MCS.\nOur principles are not focusing on the details of building\na particular system or ecosystem. Instead, they focus on\nunderstanding the higher principles that can shape how the\ncomputer ecosystems we envision are related to a science\nof systems, peopleware, and methodology (meta-science)\nenabling them. Any attempt to formulate a ﬁxed number of\nprinciples is artiﬁcial, but it can help guide the development\nof a scientiﬁc domain or ﬁeld of practice33.\nWe hold as our highest principle that:\nP1:This is the Age of Computer Ecosystems.\nAs indicated in Section 2.1 that large-scale ecosystems\nare now at the core of many if not most private and public\nutilities; this is the Age of Computer Ecosystems. Derived\nfrom its goal and as stated in its central premise (see\nSection 3.1), MCS aims to understand and design computer\necosystems, working efﬁciently at any scale, to beneﬁt the\nsociety. This requires a science of pragmatic, predictable,\naccountable computer systems that can be composed in\nnearly inﬁnite ways, be controlled and understood despite\nthe presence of complexity, emergence, and evolution, and\nwhose core operative skills can be taught to all people.\nOverall, this leads to the principles summarized by Table 2.\n4.1. Systems Principles\nMCS proposes a non-exhaustive set of principles guid-\ning work on computer systems and ecosystems.\nP2:Software-deﬁned everything, but humans can\nstill shape and control the loop.\n33. As did the Agile Manifesto’s 12 principles (agilemanifesto.org).The ecosystem is comprised of software and software-\ndeﬁned (virtual) hardware, which allow for advanced control\ncapabilities and for extreme ﬂexibility. “Software is eating\nthe world”34, but under control.\nHowever autonomous these ecosystems can become,\nhumans must still be able to control them35. Techniques\nfor ensuring human control work in parallel with increasing\nand even full automation, where humans delegate speciﬁc\ndecisions for a while. Because humans must still be in\ncontrol, MCS must go deeper than just building technology.\nP3:Non-functional properties are ﬁrst-class con-\ncerns, composable and portable, whose relative\nimportance and target values are dynamic.\nNon-functional requirements, including security, trust,\nprivacy, scalability, elasticity, availability, performance, are\nﬁrst-class concerns, but the importance and the characteris-\ntics of each requirement may be ﬂuid over time, and depends\non stakeholders, clients, and applications .\nWe envision guarantees of both functional and non-\nfunctional properties, however and whenever assemblies are\ncomposed, even when complexity, emergence, and evolution\nexist. Long-term, after the maturation of MCS, we envi-\nsion that even operational guarantees, including limits of\nemergence, can be ensured through the composability and\nportability of non-functional properties of ecosystem and\nsystem components.\nAmong the guarantees, we envision not only specialized\nservice objectives/targets (SLOs) and overall agreements\n(SLAs), but also general, ecosystem-wide guarantees such\nas performance isolation (vs. performance variability), toler-\nance to vicissitude (such as workload and requirement mixes\nand changes), tolerance to correlated failures, tolerance to\nintrusion and to other security attacks, etc.\nP4:Resource Management and Scheduling, and\ntheir combination with other capabilities to\nachieve local and global Self-Awareness, are key\nto ensure non-functional properties at runtime.\nResource Management and Scheduling is a key building\nblock without which MCS is not sustainable or often even\nachievable. Consequently also of the scale and complexity\nof modern ecosystems, disaggregation and re-aggregation\nof software and software-deﬁned hardware become key op-\nerations.\nSelf-awareness is a key building block, without which\nscalability and efﬁciency, and many other non-functional\nproperties, are not attainable and controllable in the long run.\nSelf-awareness includes monitoring and sensing, which give\ninput (feedback) to Resource Management and Scheduling\n34. https://tinyurl.com/Andreesen11\n35. Starting with the 1960s, (dystopian) sci-ﬁ has imagined many sce-\nnarios regarding loss of privacy, of sovereignty, and ultimately of free will.\nMany of these are only now emerging as real-world problems.\n\nand thus lead to better (albeit slower, and possibly uncon-\ntrolled) decisions.\nP5:Ecosystems are super-distributed.\nEverything in MCS is distributed36. Although some\necosystems operate primarily under one human-control unit,\ne.g., the management of Amazon controls the Amazon AWS\noperations and thus also the infrastructure, these ecosystems\nare still comprised of a set of systems that operate under\nthe central paradigm of Distributed Systems: “a collection\nof autonomous computing elements that appears to its users\nas a single coherent system” [8].\nMCS ecosysems are super-distributed : Following our\ndeﬁnition in Section 2.1 and as with any technology [11],\necosystems in MCS are recursively distributed. This is\na form of super-distribution : distributed ecosystems com-\nprised of distributed ecosystems, in turn comprised of dis-\ntributed ecosystems, etc.\nBeyond the traditional concerns of Distributed Systems,\nsuper-distribution is also concerned with many desirable\nsuper- properties: super-ﬂexibility and super-scalability (dis-\ncussed in the following), multiple ownership of compo-\nnents and federation, multi-tenancy, disaggregation and re-\naggregation of systems and workloads, interoperability in-\ncluding the grafting of third-party systems into the ecosys-\ntem, etc.\nExtending a term from management theory [71, Ch.2],\nwe deﬁne super-ﬂexibility as the ability of an ecosystem\nto ensure both the functional and non-functional properties\nassociated with stability and closed systems (e.g., correct-\nness, high performance, scalability, reliability, and security),\nandthose associated with dynamic and open systems (e.g.,\nelasticity, streaming and event-driven, composability and\nportability). Super-ﬂexibility also introduces a framework\nfor managing product mergers and break-ups (e.g., due to\ntechnical reasons, but also due to legal reasons such as anti-\nmonopoly/anti-trust law) on short-notice and quickly.\nSimilarly to super-ﬂexibility, super-scalability combines\nthe properties of closed systems (e.g., weak and strong\nscalability) and of open systems (e.g., the many faces of\nelasticity [32]). Inspired by Gray [72], we see this new form\nof scalability as a grand challenge in computer science.\n4.2. Peopleware Principles\nMCS provides services to hundreds of millions of peo-\nple, through ecosystems created by a large number of ama-\nteurs and professionals. Inspired by the software industry’s\nstruggle to manage and develop its human resources, we\nexplicitly set principles about peopleware.\nP6:People have a fundamental right to learn and\nto use ICT, and to understand their own use.\n36. The list of principles of computing proposed by Denning and\nMartell [70] curiously omits distribution, although it does include network-\ning and parallelism.MCS must lead to teachable technology: in our vision,\nall stakeholders of all public computer ecosystems can be\ntaught basic ecosystems-related skills. For example, individ-\nuals should be able to reading their own consumption meters\nand understand the reading, much as they do for their other\nutilities such as electricity and running water.\nAs a warning anecdote37, the Dutch Government has\ntried to introduce in the past decade various broad technolo-\ngies for governance, such as digital ids, digital documents,\nand digital voting. An important issue has proven so far the\ntechnical level required by the proposed solutions, which\ncurrently seems to exclude millions of people, especially\nold people and a part of the younger generation especially\nfrom poor and immigrant origins. It remains unacceptable\nto exclude large parts of a population from basic societal\nand governance services.\nP7:Experimenting, creating, and operating\necosystems are professional privileges, granted\nthrough provable professional competence and\nintegrity.\nTo limit damage to the society and to the profession\nitself, everyone who experiments with, creates, or operates\necosystems that others rely on must be subject to profes-\nsional checks and balances. As a community, we are no\nlonger in position to argue technology in general, and espe-\ncially ecosystems reaching many people, is only beneﬁcial\nand thus creating and operating such technology should be\ndone without restriction. Vardi observes “I realized recently\nthat computing is not a game–it is real–and it brings with\nit not only societal beneﬁts, but also signiﬁcant societal\ncosts” [73]. This puts our ﬁeld in line with medical and legal\nprofessions, but with the added pressure resulting from the\nincrease of contract work in our ﬁeld [74]38.\nAs has been argued about the profession of software\nengineering39, and later about the profession of computing\nin general [75] (whose terminology we follow), we need\nto establish a profession of Massivizing Computer Systems.\nThis requires establishing the core roles that stakeholders\ncan play, including the services professionals can provide\ntoclients . Clients have the right to be protected “from [...]\nown ignorance by such a professional” [19, loc.4338-4339].\nThe profession sanctions, through the guidelines of a profes-\nsional society, the body of knowledge and the skills used in\npractice, and the code of ethics of the profession. Bodies of\nknowledge expand through organized (scientiﬁc) disciplines,\nwhereas skills expand through the practice of organized\ntrades. Professional (accredited) education provides training\nfor both, and higher education also provides training into\nthe processes of expanding both. Trained professionals are\n37. Kindly proposed by Dick Epema.\n38. Besides the possible increase in quacks and shams among the prac-\ntitioners of our ﬁeld, due to lack of veriﬁable credentials, contract jobs\ncurrently have lower job beneﬁts and insurance [74], which can lead to\npressure to accept unprofessional and even unethical requirements.\n39. http://repository.cmu.edu/cgi/viewcontent.cgi?article=1192&\ncontext=sei\n\ncertiﬁed and accredited, and can lose their license or worse\non abuse.\nTo train MCS professionals, two elements need to be\nadded to the general computing-core disciplines proposed\nby Denning and Frailey [75]: systems thinking and design\nthinking.\nPeople with Systems Thinking skills can analyze com-\nputer ecosystems to ﬁnd their laws and to formulate theories\nof operation, and can synthesize and tune computer ecosys-\ntems.\nPeople with technology oriented Design Thinking skills\ncan design computer ecosystems and the interfaces that\nenable their interoperability, recursively across the super-\ndistributed, super-ﬂexible framework (see Principle 5). Sys-\ntems and design thinking will foster invention and creative\ndesigns, through the work of both many practitioners (e.g.,\nengineers), and (relatively few) scientists and designers.\n4.3. Methodological Principles\nAs a ﬁeld of computer systems, itself a ﬁeld of computer\nscience, MCS leverages their scientiﬁc principles, including\nthe list compiled by Denning [15, p.32]: (i) focusing on a\npervasive phenomenon, which it tries to understand, use,\nand control (MCS focuses on computer ecosystems); (ii)\nspans both artiﬁcial and natural processes regarding the\nphenomenon (MCS both designs and studies its artifacts\nat-large); (iii) aims to provide meaningful and non-trivial\nunderstanding of the phenomenon; (iv) aims to achieve\nreproducibility, and is concerned with the falsiﬁability of\nits proposed theories and models; etc. MCS also includes\nin its methodological principles a broader principle, related\nto the ethics of the profession (linked also with Principle 7).\nP8:We understand and create together a science,\npractice, and culture of computer ecosystems.\nWe envision fostering a domain of MCS where every-\nthing we develop is tested and benchmarked, reproducibly .\nAlthough providing a full set of principles leading to this\ngoal goes beyond the scope of this article40, we see a set\nof desirable steps toward this end: (i) Reproducibility as\nessential service to the community: we must mature as a\nscience and value reproducibility studies [76], including by\npublishing reproducibility studies as other domains do [77];\n(ii)Open-access, open-source: both software [60] and data\nartifacts are shared with all stakeholders, receiving for this\njust reward and recognition [78], including appropriate lev-\nels of funding; (iii) Negative results are useful: following\nan increasingly visible community in Software Engineer-\ning [79], we postulate that past failures, especially observed\nthrough experiments that falsify predicted results, must be\nrecorded and shared, leading to future success; (iv) Neutral\nresults are useful: in the current approach of the science of\ncomputer systems, it seems that results are rarely worthy\n40. This is part of our ongoing research as part of the international SPEC\nResearch Group, through its Cloud Group.of publication, unless the results are strongly positive (or,\nrarely, strongly negative). We envision that neutral, even if\npreviously unknown and expanding the body of knowledge\non meaningful problems41, will receive as much opportunity\nfor publication as the other kinds of results; (v) Laws and\ntheories of ecosystem operation are valuable: contrasting to\nwhat we perceive as a bias toward “working systems”, we\nsee an increasing need for conducting empirical and other\nforms of research leading to laws of operation and possibly\ntheories derived from it.\nP9:We are aware of the evolution and emergent\nbehavior of computer ecosystems, and control\nand nurture them. This also requires debate and\ninterdisciplinary expertise.\nShort- and long-term evolution, and short-term emergent\nbehavior, can shape the use of current and future ecosystems.\nPractitioners in MCS must be aware of the evolution of\nsystem properties, requirements, and stakeholders, and strive\nto be aware of emergent behavior.\nWe must study existing principles [70] and revisit peri-\nodically what is valuable in our and related ﬁelds. Corol-\nlary: this principle also requires to revisit periodically the\nprinciples of MCS discussed in this section.\nConstantly monitoring for evolutionary and emergent\nbehavior in ecosystems offers important opportunities and\nadvantages. With good hindsight, it is possible to steer and\nnurture the evolution of the ﬁeld efﬁciently, by ﬁrst re-using\nas much as possible what already exists, and only then, iff.\nneeded, developing new concepts, theories, and ultimately\nnew systems and ecosystems. With early identiﬁcation of\nemergent behavior, DevOps [81, p.3] can ﬁrst understand,\nthen tune or even change the system, e.g., by adding new\nincentives and mechanisms to steer (unwanted) human be-\nhavior [35], [82].\nAdhering to this principle is challenging, at least in the\ncomplexity of combining a diverse set of methodological\ntheories and techniques. For example, from methodology\nalready in use in Distributed Systems, Software Engineering,\nand Performance Engineering, key to MCS are the art\nand craft of the comprehensive survey, longitudinal studies\nrevealing long-term system operation, etc. From interdisci-\nplinary studies, key to MCS are ﬁeld surveys of common\npractice and its evolution, workshops that truly engage the\nexperts in debate [83], and involvement of society at-large\nin discussing the ethics and practice of the ﬁeld [84].\nP10:We consider and help develop the ethics of\ncomputer ecosystems, and inform and educate all\nstakeholders about them.\n41. For example, consider the notion of super-ﬂexibility. Making existing\necosystems multi-dimensionally elastic is desirable and can lead to signiﬁ-\ncant reduction in operational cost. However, this may lead currently to loss\nof performance, and many other trade-offs [80]. Exploring the trade-off may\nnot yet lead to new solutions, but it is valuable for the community at large.\nIn our experience, such studies are often rejected from top conferences.\n\nChallenge\nType Index Key aspects Princip.\nSystems C1 Ecosystems, overall P1\n(x5.1) C2 Software-deﬁned everything P2\nC3 Non-functional requirements P3, P5\nC4 Extreme heterogeneity P4\nC5 Socially aware P4\nC6 Adaptation, self-awareness P4\nC7 Scheduling, the dual problem P4, P5\nC8 Sophisticated services P4\nC9 The Ecosystem Navigation challenge P2–5\nC10 Interoperability, federation, delegation P4, P5\nPeopleware C11 Community engagement P6\n(x5.2) C12 Curriculum, BOKMCS P6\nC13 Explaining to all stakeholders P4, P6\nC14 The Design of Design challenge P6, P7\nMethodology C15 Simulation and P7, P8\n(x5.3) Real-world experimentation\nC16 Reproducibility and benchmarking P7, P8\nC17 Testing, validation, veriﬁcation P8\nC18 A Science of MCS P8, P9\nC19 The New World challenge P8, P9\nC20 The ethics of MCS P10\nTABLE 3. A SHORTLIST OF THE CHALLENGES RAISED BY MCS.\nWe have already indicated in Section 1 how our focus\nexclusively on technology exposes the community to various\nethical risks. Overall, we envision for MCS an ethical\nimperative to actually solve societal problems , which means\nour focus must broaden and become more interdisciplinary,\nand MCS must develop a body of ethics to complement\nthe body of knowledge . As a beneﬁt of considering ethical\nissues, we envision new functional and non-functional re-\nquirements to be addressed by design in a new generation\nof MCS ecosystems.\n5. Twenty Research Challenges for MCS\nAlthough we see well the challenges raised by the prolif-\neration of ecosystems and especially their constituents (see\nSections 1 and 3.1), we are just beginning to understand the\ndifﬁculties of working with ecosystems instead of merely\nsystems. Known difﬁculties include, but are not limited to:\nthe sheer volume, the group and hierarchical behavior under\nmultiple ownership and multi-tenancy, the interplay and\ncombined action of multiple adaptive technique, the super-\ndistributed properties, and the remaining issues captured by\nour principles (see Section 4).\nC1: Ecosystems instead of systems. (From P1)\nWe see as the grand challenge of MCS re-focusing on\nentire ecosystems:\nHow to take ecosystem-wide views? How to understand,\ndesign, implement, deploy, and operate ecosystems? How to\nbalance so many needs and capabilities? How to support so\nmany types of stakeholders? How do the challenges raisedby ecosystems co-evolve with their solutions? What new\nproperties will emerge in ecosystems at-large and how to\naddress them? These and similar questions raise numerous\nchallenges related to systems (see Section 5.1), people-\nware (see Section 5.2), and methodology (see Section 5.3).\nTable 3 summarizes this non-exhaustive list of challenges.\n5.1. Systems Challenges\nC2: Make ecosystems fully software-deﬁned, and\ncope with legacy and partially software-deﬁned sys-\ntems. (From P2)\nThe scale, diversity, and dynamicity of ecosystems ad-\nvocates for self-managed control42. The largest datacenters\nin the world span over millions of square feet43, contain\nup to hundreds of thousands of compute servers, and tens\nof thousands of switches and networking equipment. They\nservice up to millions of customers and their diverse work-\nloads. Manually conﬁguring and managing this volume of\ncomputing machinery and workloads is infeasible. Herein\nlies the need of fully software-deﬁned ecosystems .\nThe key principle behind software-deﬁned ecosystems\nis the dissociation (i.e., the separation of concerns) between\nthe physical resources and mechanisms, and the software-\nrelated interfaces and policies exposed to the users. Cloud\ncomputing has enabled software-deﬁned systems by ﬁrst\nvirtualizing compute hardware, via virtual machines. In\nthe early to mid 2010s, more resources and services have\nbeen virtualized: software-deﬁned networking [85], [86],\nsoftware-deﬁned storage [87], and even software-deﬁned\nsecurity [88].\nThe next step towards software-deﬁned ecosystems is\nthe design and implementation of software-deﬁned datacen-\nters [89] or clouds [90]. The aim is to enable seamless\nand efﬁcient, possibly federated, composition of software-\ndeﬁned ecosystems. In this paradigm, users and systems\ndevelopers need not be concerned with low-level hardware\nconﬁgurations and interactions, but rather declare and dy-\nnamically change their non-functional requirements: secu-\nrity and privacy policies (e.g., who can access what), level\nof fault-tolerance (e.g., on how many datacenters must data\nbe replicated), service-level agreements of network perfor-\nmance (e.g., guaranteed bandwidth or latency), scalability,\nand even trade-offs between availability and consistency.\nAn important challenge of fully software-deﬁned ecosys-\ntems is the integration with legacy systems, i.e., partially\nsoftware-deﬁned. This is an endemic problem in (dis-\ntributed) computer systems development, as re-designing\nand re-building successful legacy systems is an inefﬁcient\nand intricate endeavor. Such problems have been success-\nfully tackled in grid Computing by using an additional layer\n42. Although full self-management is not entirely possible, minimizing\nthe human administrator intervention is key for achieving performance.\n43. https://www.racksolutions.com/news/data-center-news/top-10-\nlargest-data-centers-world/\n\nof indirection, such as a meta-middleware [91], [92] that\nreconciles many different sub-components and brokers their\ninter-operation.\nC3: Make non-functional requirements ﬁrst-class\nconsiderations, understand key trade-offs between\nthem, and enable ways to specify targets (dynam-\nically) with minimal (specialist) input. (From P3,\nP5.)\nCustomer workloads are increasingly more diverse in\nterms of volume, variety, velocity, etc., and ultimately of vi-\ncissitude [22], that is, how each of these challenges becomes\nmore prominent at seemingly arbitrary moments of time.\nTo express this diversity, ecosystem customers and opera-\ntors must agree not only on functional requirements (what\nto run), but also on increasingly more sophisticated non-\nfunctional requirements (NFRs, see also P5) and Quality of\nService (QoS) guarantees expressed as Service-Level Agree-\nments (SLAs). For example, expressing elasticity could use\nany or a subset of the over ten available metrics [32]. When\nmore resources and services will be software-deﬁned (C2),\nNFRs could include additional SLA terms that relate to how\nresources and services are used.\nThis calls for non-functional requirements (NFRs) to\nbecome ﬁrst-class considerations in the design and operation\nof ecosystems.\nThe current practice is to deﬁne NFRs for entire appli-\ncations, including highly reconﬁgurable applications such as\nworkﬂows. This can lead to resource waste [93] and inability\nto express sophisticated needs. We envision that NFRs could\nbecome much more ﬁne-grained than currently in practice.\nSpeciﬁcally, we envision spatial ﬁne-grained NFRs , that is,\nexpressing detailed NFRs for each unit of work (e.g., task\nof a bag-of-task, function of a FaaS workﬂow, microservice\nof a service-based application), and temporal ﬁne-grained\nNFRs , that is, expressing NFRs that change over time pos-\nsibly dynamically (i.e., at runtime).\nAlthough ﬁner-grained NFRs can be beneﬁcial, they\nalso increase complexity for the user. Understanding and\nselecting complex NFRs and selecting the right SLOs to\nmeet them can become overwhelming [94].\nThis raises issues of balancing performance with the\nusability of cloud platforms. For example, to enable rela-\ntively unsophisticated clients to obtain good performance\nwith minimal (cost) overhead, we envision new methods to\nautomatically translate minimal specialist input into detailed\nrequirements and, consequently, actions taken by the system\nto adapt. Even for expert users, changing NFRs at runtime\ncan be cumbersome. We further envision that NFRs will\nchange dynamically, to respond to the monitored, predicted,\nor detected state of both ecosystem and application; for\nexample, changing SLOs upon detecting a resource overload\nor a straggling task.\nA variety of techniques for self-adaptation already exist\nin the space of cloud computing [95], including many auto-\nscaling approaches that work well in practice [43]. However,\nmuch more remains to be done, including: (i) investigatingthe ability of existing formalisms for workﬂows to express\nﬁne-grained NFRs, and designing (parts of) formalisms that\ncan address the missing elements, (ii) developing a re-\nsource management and scheduling architecture supporting\ndynamic NFRs as ﬁrst-class considerations, (iii) applying\nNFRs to new elements besides traditional performance, such\nas exploring the trade-off between power-consumption and\nother NFRs.\nC4: Manage extreme heterogeneity. (From P4)\nLarge-scale computer ecosystems exhibit unprecedented,\nextreme heterogeneity, which we characterize mainly as (i)\nworkload heterogenity, (ii) infrastructure, and (iii) people-\nware (addressed in C5). We discuss these in turn, then\nformulate the main challenge in this context.\nWe see workload heterogeneity as (i) functional, that\nis, applications require special hardware, such as GPUs,\nand special software, such as FaaS platforms, to function,\nand (ii) non-functional, that is, applications must satisfy\nSLAs, for example, web applications have low-latency re-\nquirements [96], whereas large data processing applications\nare primarily concerned with throughput [97]. Workloads\nachieve heterogeneity also through (iii) the interplay in\nthe same workload between mixtures of applications with\ndifferent functional and non-functional requirements.\nCorresponding to an increasing workload heterogene-\nity, we see an increase in infrastructure heterogeneity.\nGPUs [98] and TPUs [99] (ASICs) are increasingly used\nfor machine learning applications, and FPGAs are increas-\ningly used for internal datacenter and cloud operations44.\nNew kinds of memory, such as Intel Optane (3D XPoint),\nwith its unique latency and throughput characteristics, are\nnow becoming mainstream45. Even disregarding these new\ndevelopments, a plethora of compute and storage types are\nservicing a variety of needs in computer ecosystems; for\nexample, AWS alone has over 70 types of compute instances\n(excluding deprecations), and hundreds of cloud services\nsuch as Container Service and Lambda, each providing\nadditional options for running compute jobs. This is different\nfrom the past, when datacenters were ﬁlled with similar\nhardware, for ease of use and maintenance.\nThe interplay of heterogeneity in both applications and\ninfrastructure creates new research challenges. How to pro-\ngram applications easily? How to exploit the heterogene-\nity of infrastructure for optimal performance, cost savings,\nenergy efﬁciency, etc.? How to control the trade-off be-\ntween efﬁciency, and various non-functional and functional\nrequirements? How to create a uniform system out of several\nheterogeneous components that can be investigated also\ntheoretically?\nThere is already some work in this area, albeit pre-\nliminary. It includes languages and tools to program once\nand run on heterogeneous hardware [100], which extends\n44. https://aws.amazon.com/ec2/instance-types/f1/\n45. https://www.anandtech.com/show/12136/the-intel-optane-ssd-900p-\n480gb-review\n\nthe large body of previous work on middleware to abstract\nstorage devices and services.\nC5: Socially aware systems, with the human in the\ncontrol loop. (From P4)\nAt the scale of computer ecosystems, we observe social\nhereogeneity : it is likely that the presence of many distinct\nindividuals can be understood and managed through con-\ncepts of social networking, of users creating collective pat-\nterns of usage. Thus, the “convergence of technological and\nsocial networks” [101], if understood and managed, creates\nopportunities for better system design and also for better\nquality of experience for ecosystem-users. Preliminary work\nhints that understanding social-interaction patterns leads to\nbetter understanding of resource usage [6], to designing\nmore efﬁcient ecosystems [82], and to improved application\nexperience [48].\nWe see three main challenges in this context: (i) under-\nstanding, modeling, and predicting the key social interac-\ntions, (ii) leveraging the models and predictors to improve\nperformance and service-experience, and (iii) exploring the\ntrade-off between the degree of control afforded to users,\nincluding privacy of their usage data, and the performance\nthe system can achieve. In this work, we explore related\nwork for the ﬁrst two challenges; for the third, we point to\nthe extensive vision on trust proposed by Epema et al. [7].\nSeminal work has focused so far on understanding the\nsocial relationships between users where the relationships\nare implicit [82], [102], such as direct communication, direct\nexchange of data, and acting together. For example, these\nhave been shown to forming strong social relationships\n(ties) between users of online games [48], peer-to-peer ﬁle-\nsharing [103], high-performance workloads [104], [105],\netc. The key issue is to generalize these ﬁndings, across\ndifferent applications, types of ties and of graphs, and\nmethod of collecting relevant data.\nUsing implicit social relationships in computer ecosys-\ntems does not need to start from scratch. Our system for\ncollaborative downloads The 2fast [106] socially aware pro-\ntocol leads to optimal data sharing in peer-to-peer networks.\nAutomatic identiﬁcation of dominant users [107] and of\njob groupings [108] in scientiﬁc grid workloads led to\npioneering work by IBM [105]. These and more recent\nstudies [103], [109] indicate that new workload patterns do\nemerge from implicit social interaction and can be lever-\naged.\nC6: Make use of adaptation approaches, from simple\nfeedback loops to self-awareness, to respond auto-\nmatically to anomalies and to changes in require-\nments. (From P4)\nAdaptation approaches, up to and including self-\nawareness, can greatly help with a variety of MCS prob-\nlems. In our 2017 survey of the ﬁeld [95], we have identi-\nﬁed 10 classes of such problems with immediate practicaluse: (i) recovery planning, (ii) autoscaling of resources,\n(iii) runtime architectural reconﬁguration and load balanc-\ning, (iv) fault-tolerance in distributed systems, (v) energy-\nproportionality and energy-efﬁcient operation, (vi) workload\nprediction, (vii) performance isolation, (viii) diagnosis and\ntroubleshooting, (ix) discovery of application topology, and\n(x) intrusion detection and prevention.\nIn the same survey [95], we have also identiﬁed 7\nclasses of existing approaches: (i) feedback control-based\ntechniques, (ii) metric optimization with constraints, (iii)\nmachine learning-based techniques, (iv) portfolio schedul-\ning, (v) self-aware architecture reconﬁguration, (vi) stochas-\ntic performance models, (vii) other approaches. For each\nclass, we have provided a set of speciﬁc problems where\nthe approach has been applied in practice.\nThe key remaining challenges are to enable adaptation\nunder sophisticated non-functional requirements (challenge\nidentiﬁed as part of C3) and to (i) select from these ap-\nproaches those most promising to adapt easily to the ex-\npanded scope of entire computer ecosystems, the former as\nmuch as possible automatically and the latter with minimal\nportability effort, (ii) challenge the existing assumptions\nthat make adaptive methods tractable, e.g., can we com-\nplement traditional data structures and algorithms with ap-\nproaches that are adaptive, deterministic but complex, such\nas machine-learning-based indexes [110] and approximate\nindexes [111]? (iii) understand systematically the interplay\nbetween different adaptive approaches operating simultane-\nously or even in conjunction in the computer ecosystem,\n(iv) conduct relevant ﬁeld studies, of putting the adaptive\ntechniques in practice in (near-)production settings, (v) ex-\ntend traditional adaptive techniques to include with feedback\nfrom ecosystem engineers.\nWe have for years conducted inroads into these chal-\nlenges when applying adaptive approaches to resource man-\nagement and scheduling for datacenter ecosystems [22],\n[80], [112] (for many more remaining challenges, see Sec-\ntion 6.1).\nC7: Scheduling, consisting of both provisioning and\nallocation, on behalf of different, possibly delegating\nstakeholders. (From P4, P5 .)\nTwo phenomena concur to make scheduling in ecosys-\ntems more challenging than in typical systems, e.g., in op-\nerating systems. First, scheduling occurs on behalf of users\non resources typically offered by an operator; this means\nthat the scheduling process must both allocate resources to\nindividual jobs (as in traditional operating and distributed\nsystems), and also provision resources on behalf of the\nuser across super-distributed ecosystems (see P5)—this is\nthedual problem of scheduling in MCS. Second, for many\ncomputer ecosystems, new conditions and requirements have\nappeared. In particular, the diversity of users and the rapid\naddition of new technologies means workloads can change\ndrastically over both short and long periods of time. For\nexample, grid workloads exhibit short-term burstiness [113]\nand also increased fragmentation into smaller tasks over\n\nlong periods of time [39]. Similar phenomena appear in\ncloud environments [114], [115].\nWe envision a set of new scheduling challenges in this\ncontext. The dual scheduling problem requires either tight\ncollaboration between users and operators, or partial to full\nautomation of at least the work of one of the sides. Users and\noperators can agree on the use of auto-scalers [43], which\nprovision resources on behalf of users, dynamically, with\nminimal conﬁguration and expert-level support. Ofﬂoading,\nthat is, sending a part of the workload for execution to\nother resources (and possibly other operators), can also be\na useful technique for the user-operator collaboration [116].\nUsers can have their work automated by advanced, typically\njob-speciﬁc, execution engines (e.g., glide-in technology in\ngrids, and the Hadoop execution engine for MapReduce\nprocessing depicted in Figure 1). Operators can leverage\nauto-tiering and multi-level auto-scalers. Among the larger\nset of challenges here, the key remaining challenge in\nautoscaling consists of (i) selecting a good autoscaler that\nmatches the needs of the current workload46[43], possibly\ndynamically, (ii) inventing new autoscalers for emerging\nworkload-characteristics and -needs, and (iii) replacing as\nmuch as possible the workload-speciﬁc with workload-\nagnostic techniques [43].\nThe new conditions require adaptation of every tradi-\ntional approach, e.g., regular scheduling, scavenging for\nresources, migrating jobs. Allocating workloads to the pro-\nvisioned resources has been a topic of research in regular\nscheduling for decades, with hundreds of approaches and\npolicies [117], but selecting and adapting results to emerging\nworkloads remains non-trivial. Memory scavenging is a\nmethod applied to reduce compute resource consumption,\ne.g., for scientiﬁc workloads [118]. By using small portions\nof available memory from other tenants or nodes, a relative\nsmall performance overhead can be traded for signiﬁcant\ngains in resource consumption. Extending this technique,\ne.g., for use in a multi-tenant virtual machine environment,\ncan prove to be beneﬁcial for performance and resource\nconsumption, but could lead to conﬂicts in meeting other\nSLOs, such as performance isolation or operational risks\ndeﬁned by the SPEC RG group in [32].\nC8: Sophisticated components in the ecosystem of-\nfered as services. (From P4)\nThe emergence of cloud computing is transforming the\nglobal ICT industry, from which it employs a signiﬁcant\nfraction of skilled personnel and for which it delivers a\nsizable ﬁnancial revenue [119].\nWe are entering a period of XaaS (Everything-as-a-\nService), where any product or technology can be supplied\nas a service and delivered to the consumer through the\nInternet [31]. Enterprises such as Google, Microsoft, Ama-\nzon, Oracle, Adobe are moving away from the traditional\n46. https://atlarge-research.com/lfdversluis/2017-11-24 lfdversluis\nautoscaling-comparison.pdfperpetual license model and are choosing to offer their\nproducts on-demand; a consumption-based model.\nMany companies and organizations have transitioned\n(parts of) their operation to cloud-based services, shifting\ntheir operation model to take advantage of the elasticity,\navailability, security, and pay-as-you-go pricing model of the\ncloud [90]; for these companies and organizations, cloud-\nbased services are supplanting in-house infrastructure and\nlegacy software47.\nThe XaaS ecosystem is rapidly expanding, with emerg-\ning ’aaS models appearing next to the three main supporting\npillars [3]: Infrastructure-as-a-Service (IaaS), Platform-as-\na-Service (PaaS), and Software-as-a-Service (SaaS). These\nnew service models span one or more of the aforementioned\npillars, and include Data-as-a-Service, Benchmarking-as-a-\nService, Function-as-a-Service, Authentication-as-a-Service,\netc. This raises interesting new challenges, that relate tradi-\ntional non-functional requirements to cost, resource waste,\nand manageability; for example, we discuss some of the\nchallenges of serverless and FaaS operation in Section 6.5.\nOne of the main challenges of these new ’aaS models\nis the need for standardization [120], [121]. The cloud\narchitecture is still subject to rapid change, with hundreds of\nnew technologies being added in each of the past ﬁve years.\nProviders each have their own technology stack, raising\nconcerns of vendor lock-in and lack of interoperability. A\nnecessary key challenge therefore is to deﬁne appropriate\nreference architectures. Moreover, this challenge goes be-\nyond the merely technical: it requires communication and\ncollaboration between major cloud operators, and possibly\nalso with their main clients, to ensure adoption.\nCurrently, all the services in the XaaS ecosystems are\noffered through the Internet, making the Internet a bottle-\nneck, albeit widely distributed and fault-tolerant. Because\nnetwork performance is crucial, it would be advantageous to\nhave a uniﬁed platform that encapsulates both network and\ncloud services. Research is underway to address this issue,\nfor example, through EU-funded projects such as Scalable\nand Adaptive Internet Solution (SAIL48) and Unify49.\nC9: The Ecosystem Navigation challenge: solving\nproblems of comparison, selection, composition, re-\nplacement, and adaptation of components (and as-\nsemblies) on behalf of the user. (From P2–5 .)\nComputer ecosystems can seem daunting to starting\nusers. They pose numerous challenges related to the use\nof complex systems, complex code-bases, and seeming lack\nof control. For the user who wants to achieve some goal\nthrough the use of existing existing, the presence of many\nopen-source components for own deployment and API-\nbased hosted by cloud operators raises the problem of\nselection and conﬁguration (adaptation). For example, which\nof the tens of machine instances provided by Amazon EC2\n47. https://www.strategyand.pwc.com/reports/zero-infrastructure\n48. http://www.sail-project.eu/\n49. http://www.fp7-unify.eu/\n\nshould a researcher start to use? And which of the seemingly\nsimilar machine instances to use, among the many available\nclouds? For the starting developer, the problems extend\nto the full set of comparison, selection, composition, etc.\nThis raises the Ecosystem Navigation challenge , of solving\nthis set of problems on user’s behalf, subject to custom\nrequirements.\nWe envision two main challenges, derived from the\nSoftware Engineering and Distributed Systems communi-\nties, respectively: (i) satisﬁcing the Ecosystem Navigation\nchallenge for the restricted set of ecosystems whose com-\nponents are all based on explicit, narrow, well-deﬁned APIs,\nand (ii) satisﬁcing the Ecosystem Navigation challenge for\nthe general case where ecosystem components can use any\ninterface or API, and in particular the ecosystem can include\nlegacy systems and systems with poorly speciﬁed interfaces.\nThe challenge expressed at (i) has been a hot topic of\nresearch in Software Engineering for the past decade. The\nearly results focus on functional composition of compo-\nnents, with tens of methods already put in practice [122].\nMany challenges remain to be solved, either for the case\nwhere the components are hosted and their APIs only min-\nimally speciﬁed [123]. As an early example, IBM API Har-\nmony [124] focuses on automatically analyzing APIs pro-\nduced by different developers and producing recommen-\ndations of components that may be used together. An\nopen challenge still arises when the composition must also\nguarantee non-functional requirements are met (so, beyond\nfunctional requirements).\nThe latter remains largely an open challenge, with pre-\nliminary work focusing on empirical ﬁndings, e.g., perfor-\nmance studies across the entire community trying different\ncombinations of components in the ecosystem. However,\ncomparing performance studies and ﬁnding their shared\nﬁndings even in a narrow ﬁeld remains difﬁcult. We see as a\npromising avenue toward addressing this issue the creation\nof community-wide, general reference architectures, of the\nkind depicted by Figure 1; such a reference architecture can\nguide the exploration of alternatives of similar nature, but\nfor general systems instead of the components considered\nfor (i).\nC10: Interoperate assemblies, dynamically: geo-\ndistributed, federated, multi-DC operation, and ser-\nvice delegation. (From P4, P5 .)\nThe digital economy is ever expanding, leading to mas-\nsive businesses, such as computing services (e.g., cloud\nservices), telecom, media and entertainment providers. In-\nherently, these service-providers serve a geographically dis-\ntributed market, with clients spread across the globe. Conse-\nquently, datacenters are built close to customers and services\ndelegated (see also P5), to ensure good quality of service\n(e.g., low-latency) and to avoid moving large amounts of\ndata over vast distances. For example, some of the largest\ndatacenter operators manage tens to hundreds of datacenterseach50and some enterprises use more than a single cloud\nprovider51.\nWe envision the need for many and eventually all MCS\nto operate over multiple, federated, and geo-distributed\n(micro-)datacenters. We also advocate for effortless compo-\nsition of geo-distributed ecosystems or services. Such needs\nstem from several reasons. First, enabling efﬁcient wide-area\nanalytics [125] is key for business interoperability: multiple\nentities that operate in different regions may need to perform\nanalytics over vast collections of geo-distributed data, to de-\nrive new insights and to produce value from their joint data.\nSecond, avoiding vendor lock-in [120] is key for reducing\nand even optimizing operational costs, limiting the data loss\nin case of natural disasters, and avoiding the loss of privacy\nwhen one vendor is compromised. Third, consolidating the\nhardware resources of distributed datacenters in a cloud-\nof-clouds [126], [127] improves overall resource utilization,\naids in meeting user service-level agreements, and reduces\nmanagement costs.\nThe main challenge in achieving interoperability and\neffortless composition is derived from the core of Dis-\ntributed Systems: achieving efﬁcient, secure, and lightweight\ncommunication between possibly untrusted systems. We\nenvision a community-wide research effort that (i) assesses\nthe feasibility of existing systems in this area, such as grpc52,\nApache Thrift53, or the older Grid communication and in-\nteroperability layers, such as Ibis [92], or GridFTP [128],\n(ii) starting from the found limitations, provides the needed\nextension of existing and older research efforts, and possibly\ndesigns radically new communication libraries, and (iii)\nconsiders co-design of communication protocols/libraries\nand the data analytics engine, such that computation is\nperformed directly on encrypted data [129], without ana-\nlyzing in the clear and exposing data on compromised (or\nmalicious) sites.\n5.2. Peopleware Challenges\nC11: Create communities and environments for peo-\nple to engage with the design and operation of\necosystems. (From P6)\nMaking the core concepts of computer ecosystems ac-\ncessible to a wide audience is vital for both the society\nand the continued evolution of MCS. Currently, many of\nthe concepts involved in modern ecosystems, from het-\nerogeneous datacenters to abstract operational policies, are\ncomplex and hard to grasp. This poses barriers for people\nto engage in the study, design, and operation of computer\necosystems. To address this issue, we see as key issue the\n50. https://www.datamation.com/data-center/data-center-companies.\nhtml\n51. According to the 2014 EC-commissioned study [119] and to the 2018\nIHS Technology report discussed by SDXCentral https://www.sdxcentral.\ncom/articles/news/ovh-takes-aws-azure-google-us/2017/09/\n52. https://grpc.io/\n53. https://thrift.apache.org/\n\ncreation of appealing and understandable visual and textual\nabstractions that lower the barrier of entry and facilitate a\nvisual understanding of the dynamic processes typical to\ncomputer ecosystems. We also envision that global compe-\ntitions in key areas of computer ecosystems (e.g., resource\nmanagement and scheduling in datacenters, see P4) can\nencourage engagement with these tools.\nThe key challenge is to ﬁnd “the right model”, that\nis, to choose the layer of abstraction and the visual/textual\ndomain-speciﬁc language, while addressing multiple stake-\nholders with different levels of sophistication and different\nproblems to explore, and with the simulator still delivering\ngood performance.\nOne of our contributions to this effort is the OpenDC\nplatform for datacenter simulation [130]. Its visual interface\nallows users to build their own virtual datacenters and\nrun workloads on their simulated resources, seeing how\ndatacenters operate from an inside perspective. But this\nconcept of a visual builder does not need to be restricted\nto physical models: we also envision users will create their\nown scheduling policies in OpenDC (see Section 6.1).\nWe have already used OpenDC in our own research\nand in the classroom, including in a periodic workshop we\ngive at Restart.network, an education network for refugees\nin the Netherlands. Entirely through the visual interface of\nOpenDC, students use OpenDC to build their own data-\ncenters, and to simulate workloads running on them using\nvarious policies. Among the remaining challenges, we en-\nvision here the development of a library of components,\nassemblies, and workloads, to be shared across the MCS\ndomain.\nC12: Create a teachable common body of knowledge\nfor MCS (BOKMCS). (From P6)\nWe see as a long-term and perhaps unrealizable chal-\nlenge the design of an BOKMCS. However, we consider the\nprocess of reﬁning the new needs of a curriculum on MCS\nas valuable for both the community, and for future students\nand trainees who will want to join the community. Like\nSimon [18, p.113], we see the value of a general education\nin the fundamentals of natural sciences or of engineering,\nand preferably both at a good level.\nDerived from our practical experience with students\nfrom various universities and technical universities con-\nsistently ranked at the top-level of the global academic\nestablishment, we see a number of important additions\nto the common ACM/IEEE Curricula Recommendations\nfor Computer Science (2013) and Software Engineering\n(2014)54, and NSF/IEEE-TCPP Curriculum Initiative on\nParallel and Distributed Computing (2012)55: (i) General\nproblem-solving techniques, covering many and possibly all\nof the techniques we describe in Section 3.5, (ii) Systems\nThinking, including elements of Complex Adaptive Systems\nand Control Theory (see Section 3.5), (iii) Design Thinking,\n54. https://www.acm.org/education/curricula-recommendations\n55. https://grid.cs.gsu.edu/ \u0012tcpp/curriculum/?q=homeincluding the representation and evaluation of designs, and\ndesigns with quantitative, qualitative, and even no ﬁnal\ngoals [18, Ch.5-6], and possibly advanced cross-ﬁeld topics\nin design [19], [20].\nWe also see speciﬁc gaps: (iv) for students follow-\ning low-quality Software Engineering courses, we rec-\nommend taking more in-depth classes in the area of\n“SE/Requirements Engineering” (ACM/IEEE, p.178) and\npossibly also “HCI/User-Centered Design and Testing”\n(ACM/IEEE, p.92), especially focusing on the analysis\nofnon-functional requirements and design/modeling tools\nthat include realistic and quantitative aspects, and (v) for\nstudents following primarily a traditional curriculum, we\nfurther see the need for learning the basics of experiment\ndesign with software artifacts, of conducting systematic and\ncomprehensive literature surveys, and possibly of conduct-\ning user studies from “HCI/Statistical Methods for HCI”\n(ACM/IEEE, p.93).\nC13: Support for showing and explaining the oper-\nation of the ecosystem to all stakeholders, continu-\nously. (From P4, P6 .)\nCurrently, many institutions and individuals rely on the\navailability and correct operation of digital services. Typ-\nically, the operational details of these services are either\nfully hidden from or merely opaque to the user. As new\necosystems are developed, especially by combining services\nfrom multiple vendors, the operation of these ecosystems\nbecomes even more difﬁcult to oversee for most stake-\nholders. Key to regaining this oversight is adding support\nfor showing and explaining the operation of an ecosystem.\nWe envision that operators of ecosystems will have a duty,\npossibly legislated, to continuously and transparently inform\nstakeholders on a variety of operational properties, including\nrisk (e.g., frequency of outages, impact of security breaches,\npossibility of data loss), cost (e.g., ﬁnancial, energy), and\nlegal aspects (e.g., licensing, compliance with local laws).\nTo this end we identify two main challenges. First,\nthe MCS community must deﬁne metrics to quantify key\noperational properties and explain these metrics and their\nimplications for each stakeholder. Although such metrics\nexist for some established domains and applications (e.g.,\nelasticity and operational risk for cloud computing [32], cost\nmodels for cloud providers), it is unclear if and how these\nmetrics translate to ecosystems, and which metrics could be\nimportant or informative. Second, we must develop methods\nfor monitoring, inferring, and predicting an ecosystem’s\noperational metrics, from the dynamic metrics measured\npossibly only by the most transparent of the ecosystem\nconstituents.\nC14: The Design of Design. (From P6, P7 .)\nWe see design as a major challenge for the ﬁeld of MCS:\nnot only good designs are difﬁcult to achieve for the level of\ncomplexity posed by computer ecosystems, but also students\n\nand later practitioners in the ﬁeld do not have prior training\nin Design Thinking and sometimes even Systems Thinking\n(see also C12).\nWe envision creating design processes that trade-off the\nrigor and precision needed to make software ecosystems run,\nand the creativity and innovation needed to make software\necosystems perform new functions and better. We need to\nstart almost from scratch, with: (i) overall, understanding\nand creating good design processes, for individuals and for\nteams of designers, that increase the likelihood of obtaining\nuseful designs (a meta-design challenge ), (ii) understanding\nand creating ways to represent designs, (iii) understanding\nand creating ways to test and to compare designs.\nFurther steps, for example, the design of computer\necosystems with organizations in the loop [18, p.154-5] (in-\nstead of merely individuals), and advanced cross-ﬁeld topics\nin design [19], [20] remain long-term, open challenges.\nAlthough not a must for practical reasons, society also\nbeneﬁts when the general public understands the basic prin-\nciples of design and is able to enjoy them as art56(see also\nP6). This is an open challenge for MCS.\n5.3. Methodological Challenges\nC15: Simulation-based calibrated approaches and\nreal-world experimentation with methodology that\nensures reproducibility as key instruments for prob-\nlem exploration and solving, and for evaluating and\ncomparing ecosystems. (From P8)\nThe MCS must create its methodologies and instru-\nments, and show they can lead to successful problem explo-\nration and solving, and to adequate evaluation and compar-\nison of ecosystems. The consequences of failing in this task\nare dire: meaningful problems cannot be solved, and unre-\nproducible results and ecosystems that are not useful cannot\nbe distinguished from valuable contributions. We conjecture\nthat the MCS community can derive useful methodologies\nand instruments from two main approaches characteristic to\nempirical research, each with important beneﬁts and speciﬁc\nchallenges: (i) simulation of ecosystems, e.g., through a\ndiscrete-event model, and (ii) real-world experimentation\nbased on methodology that leads to reproducibility (see also\nC16). we discuss these approaches, in turn.\nSimulation-based experimentation can be fast and scal-\nable relatively to real-world experimentation. Simulation\nobviates the need for physical access to the resources being\nsimulated, and is thus democratizing research in the ﬁeld\nby allowing research groups without signiﬁcant resources\nto join the community. However, this approach challenges\nscientists to develop reasonably accurate models of both the\ntopology and the workload being simulated. Validating that\nthis is indeed the case, thus showing that the model is indeed\naccurate enough, is not only a key scientiﬁc challenge, but\nalso a challenge that requires reopening the discussion about\n56. Similar considerations apply to architecturevaluable contributions in the ﬁeld (see P8)—the community\nmust show it values results such as validation studies. Sim-\nilar challenges apply to real-world experimentation.\nReal-world experimentation has the advantage that the\nsystem under test is real, revealing possibly hidden issues\nthat have not been considered by models or whose mod-\nels are inaccurate. However, real-world experimentation for\ncomputer ecosystems has important, pragmatic limitations.\nThe scale of the system under test is typically reduced, and\nespecially large-scale systems on-par with real deployments\nare rarely available in practice; notable exceptions such as\nGrid’5000 and the DAS [41] are still only medium-scale in-\nfrastructure. Moreover, test suites and benchmarks typically\ncannot take control over the entire system under observation,\ndue to user-access limitations, and their execution time is\nconstrained because it matches that of the real-world system.\nThere are many technical and research challenges that need\nto be addressed in real-world experimentation, related to\nthe trade-off between (i) realism and statistical power of\nexperiments, and (ii) duration, cost, and access-rights.\nAs an exemplary challenge, we discuss event-based\nmodeling and simulation of datacenters. Although many\nsimulators exist [131], [132], [133], important features such\nperformance variability are only now being added to sim-\nulators [134] and being validated. Moreover, applying the\nexisting simulation tools to the diverse set of scenarios we\nobserve in real-world setups is not always a straightforward\nprocess. From a too-narrow set of modeled scenarios, to\nvery limited validation with real-world results, we still see\nimportant challenges standing in the way of their wider\napplication as system evaluators. Our own work, the open-\nsource simulation platform OpenDC [130], emphasizes the\nneed for a common, validated basis for datacenter simulation\n(in conjunction with other features, see C11).\nC16: Reproducibility of analysis results regarding\nfunctional and non-functional properties of systems,\nincluding through a new generation of evolving\nbenchmarks, and through processes and instruments\nfor preserving and sharing benchmarking results.\n(From P8)\nReproducibility is a key concern for MCS, which fol-\nlows the concerns raised by large-scale systems [135], [136],\n[137], [138]. Reproducing arbitrary experiments, to test\nclaims or to compare with previous approaches, is non-\ntrivial. Many factors inﬂuence experiments, besides the sys-\ntem under test and its possibly hidden parameters, including\nbut not limited to the workload, the environment, and met-\nrics.\nWe see two main directions to explore towards solutions\nfor reproducibility: (i) developing real-world benchmarks\nthat offer a good degree of control, and (ii) ensuring re-\nproducibility through methodological considerations.\nIt remains an open challenge to build high quality\nbenchmarks addressing the diverse problems the MCS ﬁeld\naddresses. For example, we have experienced ﬁrst-hand\nthe challenge of reproducibility in our work on evaluating\n\nthe performance, and later with benchmarking, distributed\ngraph-processing systems. Following years of experimen-\ntation, during which we have exposed various degrees of\nperformance sensitivity to various factors, we have pro-\nposed the LDBC Graphalytics [42] benchmark, which has\nbeen adopted by companies and researchers in the ﬁeld.\nCentral to Graphalytics is the idea of objective comparison\nbetween graph-processing platforms by controlling the key\nparameters, using (i) a comprehensive suite of real-world\nalgorithms, and synthetic and real-world datasets, (ii) an\nextensive set of metrics to quantify system performance,\nscalability (we quantify horizontal/vertical and weak/strong\nscalability), and robustness (we quantify failures and per-\nformance variability), and (iii) a renewal process to curate\nand possibly change the algorithms, datasets, and gath-\nered metrics. It is symptomatic that other de-facto standard\nbenchmarks in the ﬁeld do not have the properties (i)–(iii).\nTo improve reproducibility of experiments, the SPEC\nRG Cloud group57is developing new methodologies for\n(cloud) experimentation. These include guidelines on re-\nporting metrics and values, specifying the aspects of the\nenvironment that can lead to reproducibility, sharing the\nsoftware and data artifacts used during experimentation,\netc. Additionally, our group is focusing on tools and in-\nstruments to gather valuable (anonymized) real-world and\nsynthetic operational traces, and to provide them along-\nside software artifacts for benchmarking through artifact-\nrepositories available freely to individuals, industry, and\nacademia. A prime example of this is our current Grid\nWorkload Archive [139], in which we provide several real-\nworld traces and basic tools for analyzing them.\nC17: Testing, validation, veriﬁcation in this new\nworld. Manage the trade-offs between accuracy and\ntime to results. (From P8)\nTesting and validating for correct behavior in distributed\ncomputer systems is a strenuous activity from the perspec-\ntives of designers and engineers, and pragmatically due to\nthe large amount of (computing) resources needed for such\nendeavors [140]. This task has many intricate variables,\nmany interactions between the system components, and (too)\nmany possible states. For MCS, where many ecosystems\nintricately interact with each other, the problem of testing\nand validation is even more difﬁcult difﬁcult, in particular\ndue to the “curse of dimensionality” (that is, the search space\nincreases exponentially with the number of states).\nTo tackle this problem, we envision a dual approach\nfor testing and validation : (i) at a micro -level, and (ii) at\namacro -level. The former focuses on the small building\nblocks , and possibly their (limited) interactions. Conversely,\nthe latter focuses on interactions at the level of entire\necosystems . Similarly to our considerations at C15, testing\nand validation techniques in particular at a macro-level have\nto take into account trade-offs between time-to-solution,\n57. https://research.spec.org/working-groups/rg-cloud.htmlamount of resources used, and the quality and quantity of\nguarantees they provide.\nFor the micro -level testing, we see as the main chal-\nlenge selecting and adapting the techniques that are already\nsuccessfully used in practice. Designing benchmarks using\na choke-point analysis [141] could expose performance and\nfunctionality issues in key components of a system. Another\nuseful approach is model-checking, where specialized tools\nexist for checking both the design [142] and the implemen-\ntation [143] of (distributed) systems.\nWe believe that testing at a higher level in the hierarchy,\nor an entire ecosystem, is still an open research question.\nPreviously, we have identiﬁed several research directions for\nIaaS benchmarking [144]. We envision experiment compres-\nsion (i.e., combining real-world experiments with emulation\nand simulation) as key to achieving sustainable testing,\nvalidation, and benchmarking in MCS. We also propose\nevaluating the short-term dynamics andlong-term evolution\nthrough periodic testing using judiciously chosen frequen-\ncies of repetitions [145].\nC18: Build a science of Massivizing Computer\nSystems. Revisit in the process the principles of\nDistributed Systems, Software Engineering, Perfor-\nmance Engineering. (From P9)\nOverall, we see as more important the process of trying\nto deﬁne an independent ﬁeld of science, than actually see-\ning MCS recognized as an independent ﬁeld of science. We\npropose a pragmatic approach to meeting the high threshold\nconjectured by Denning [15] for becoming a ﬁeld of science.\nThe approach consists of: (i) studying the novel natural and\nartiﬁcial processes that appear in computer ecosystems (the\n“Central Premise” in Section 3.1), which we argue are perva-\nsive in the modern digital markets and critical to knowledge-\nbased societies; (ii) deﬁning a body of knowledge and\nskills that relate to computer ecosystems, as explained in\nSection 3.1, based on sound and far-reaching principles\n(Section 4), starting from the already large existing body\nof knowledge identiﬁed in Section 3.5; (iii) experimenting\nwith ecosystems and simulating them, to enable discovery\nand validation, and meet reproducibility and falsiﬁability\nprinciples (Section 4, especially P8); (iv) contributing to\ncodifying and teaching the body of knowledge and skills\ndeveloped at point (ii); (v) complementing (ii), and jointly\nwith all stakeholders, deﬁning, codifying, and teaching a\nbody of ethics relevant for work in computer ecosystems.\nAs we explain in Section 7.3, other sciences have taken this\npragmatic approach.\nWe ask several important questions that can be the next\nsteps in addressing this challenge. Considering the large\nbody of existing related knowledge, what existing laws,\ntheories, and concepts from (classic) Distributed Systems,\nSoftware Engineering, and Performance Engineering still\napply or do not apply anymore for MCS? What abstractions\ncan we reuse, e.g., can there be an operating system for\nmassivized computer systems? What new abstractions are\n\nneeded for achieving the MCS vision? Last, can we build\na science of MCS from ﬁrst-principles?\nC19: The New World challenge: understanding and\nexplaining new modes of use, including new, realis-\ntic, accurate, yet tractable models of workloads and\nenvironments. (From P9)\nThis is the challenge of understanding the fundamental\nproperties of the emerging ﬁeld of MCS, as described in\nSection 3.1 under empirical and phenomenological research,\nand under formal (analytical) modeling.\nOne speciﬁc difﬁculty is the inclusion, in such research\nand models, of versions andconﬁgurations of the software\nunder study (see C16). For this challenge, overall results\nmust be detailed for each set of versions and conﬁgurations\nthat diverge signiﬁcantly from the default software, where\nsigniﬁcant divergences are to be established both empirically\nand through validated models.\nC20: Understand challenges in the ethics of MCS,\nand evolve our instruments to support ethics in this\ncontext. (From P10)\nWe envision, non-exhaustively, some of the main issues\nMCS should address, and discuss them in turn. First, MCS\nshould involve all stakeholders and agree on a set of ethical\nchallenges that the community can pragmatically hope to\nsolve.\nMCS must consider the ethics of exclusion and of inclu-\nsion afforded by MCS technology. Complex technology can\nexclude from the use of critical ICT services the signiﬁcant\nfraction of the population who lacks expertise, and may even\ndiscriminate implicitly [146]. So far, the society has started\nto address algorithmic bias58, but should the ecosystems\nrunning the algorithms also be considered? At the other end\nof the expertise spectrum, the openness of the technology\nallows anyone with a grasp of Distributed Systems to de-\nvelop their own ecosystem-components, but also opens up\na Pandora’s Box of poorly designed, poorly implemented\nsystems, which raise issues including security and privacy.\nQuestions arise of transparency (when and how?), potential\nfor bias and abuse (how to assess? how to reduce?), etc.\nMCS must consider the ethics of technology-facilitated\nanti-social and destructive behavior. For example, due to\nits scale, reach, and degree of automation, the Facebook\nplatform is now having to answer hard questions [34] about\npolitical interference on behalf of various powers (including\nstate operators), false news, social separatism through echo\nchambers, harassment based on personal views including\npolitical. What are the limits of responsibility on the side\nof the ecosystem scientists, designers, engineers, and orga-\nnizations and societies where they conduct their work?\nThe ethics of ecosystem operation in various kinds of\nmarkets are also concerns for MCS. In free markets, if\n58. https://www.aclu.org/blog/privacy-technology/surveillance-\ntechnologies/new-york-city-takes-algorithmic-discriminationthe history of economics is used as predictor, the current\naccumulation of technology and skill in the hands of a\nfew large organizations is detrimental to the society [36].\nIn response, anti-trust laws affect already Amazon, Ap-\nple, Google, Facebook, Microsoft, and Qualcomm. Their\ndecade-long legal battles with the European Commission\nover anti-trust laws have intensiﬁed in 201859[36], [147],\nbut often the legal battles lack the technological knowledge\noffered by predictive tools. (Anti-trust decisions may push\necosystems to further open up and, in reverse, to break\nup main components [147], leading to new ecosystem con-\ncerns.) In closed markets, state monopolies, and broad legal\nand executive powers in the hands of state agencies, can\nlead to large computer ecosystems that reduce civil liberties\nand human rights in all countries60[148]. This raises a\nmeta-question of ethics, What are the limits of market and\npolitical aspects our community should consider?, whose\nanswer can deﬁne how MCS will address these issues.\nEthical issues arise also in the operation of our own\ncommunity, in particular about ethical peopleware. Similarly\nto other sciences, and especially their empirical domains61,\nwe envision that ecosystems science, design, and engineer-\ning will document more the key choices, data, and even\ndaily operations. We must address the ethics of a publish-\nor-perish publication in science, which incentivizes low-\nquality results, citation games, and even copying or ripping\noff the scientiﬁc results of others [150]. Rethinking the\npublication process [151], the meaning, role, and structure of\nconferences [152] and workshops [83] is within the scope\nof MCS. We must train peopleware, especially designers\nand engineers, to avoid developing the kind of technology\nthat led in the past couple of years to various class-action\nlawsuits against large technology companies, e.g., against\nUber62. Universities are taking note of this pressing concern\nand are starting to provide ethics courses for computer\nscience63, but there is still more to be done. What are\nthe emerging ethical issues inside our community? How to\nreduce their occurrence or even avoid them entirely?\n6. Massivizing Computer Systems: Use Cases\nIn this section, we discuss application domains and use-\ncases of MCS (Sections 6.1–6); we also identify classes\nof applications that will not beneﬁt immediately from ad-\nvances in MCS (Section 6.7). We envision that computer\necosystems built on the principles of MCS will lead to\nsigniﬁcant beneﬁts over the current approaches, and in some\ncases to technology disruption: achieving economies of scale\n59. https://www.nytimes.com/2018/01/24/business/eu-qualcomm-ﬁne-\nantitrust.html\n60. https://www.theatlantic.com/international/archive/2018/02/china-\nsurveillance/552203/\n61. “Meticulous record keeping is at the heart of good science, and this\nis especially true for ﬁeld scientists and naturalists.” [149, Kindle Loc.\n114].\n62. https://arstechnica.com/tech-policy/2017/04/uber-said-to-use-\nsophisticated-software-to-defraud-drivers-passengers/\n63. https://www.nytimes.com/2018/02/12/business/computer-science-\nethics-courses.html\n\nLoc. Description Key aspects\nEndogenous applications\nx6.1 Datacenter management RM&S, XaaS, ref.archi.\nx6.5 Emerging application structures serverless MCS\nx6.6 Generalized graph processing full MCS challenges\nExogenous applications\nx6.2 Future science e-, democratized science\nx6.3 Online gaming multi-functional MCS\nx6.4 Future banking regulated MCS\nTABLE 4. S ELECTED USE -CASES FOR MCS.\n(e.g., reducing resource waste and cost), ensuring better and\nmore diverse non-functional properties of systems, lowering\nthe barrier of expertise needed for use, removing the most\ntedious tasks from the daily tasks of engineers, etc. This can\nhave immediate impact in many application domains.\nTable 4 summarizes the non-exhaustive list of six ap-\nplication domains we discuss in this work. We distinguish\ntwo directions of application: (i) endogenous , that is, the\ncomputer science and in particular the computer systems\nareas using the concepts and technologies developed within\nthe science of MCS, and (ii) exogenous , that is, domains of\napplication that use ICT and in particular computer systems\ntechnology to augment or expand their capabilities. Among\nthe endogenous application domains, we count cloud com-\nputing and big data as directly beneﬁting from advances\nin MCS, and datacenter management (Section 6.1), future\napplication structures (Section 6.5), and generalized graph\nprocessing (Section 6.6) as application domains of MCS\ntechniques that enhance and extend existing capabilities.\nAmong the exogenous application domains, we foresee the\nmutual beneﬁts of MCS and e-Science and other forms of\ncomputational sciences that use ICT as a core part of their\ninstrumentation (Section 6.2), online gaming (Section 6.3),\nbanking (Section 6.4), etc.\n6.1. Datacenters: Managing the Digital Factories of\nthe Knowledge Economy\nIn the Digital Economy, datacenters serve the role of\nmodern factories, producing efﬁcient, dependable services.\nTheir clients range from scientists running complex simu-\nlations and data processing pipelines (further explored in\nSection 6.2), to consumers playing online games and meta-\ngaming (further explored in Section 6.3). To achieve their\npromise of efﬁciency and ﬂexibility, datacenters must both\nuse their resources near-optimally, and cover a broad range\nof scales and designs: from the large multi-cluster deploy-\nments typical to IaaS clouds such as Amazon EC2 and\nMicrosoft Azure, to the cloud-edge [153] micro-datacenters\nmore typical to video transcoding and streaming [154]. This\nraises numerous scientiﬁc, designerly [20], and engineering\nchallenges.\nIn our previous work [130], we have identiﬁed as a key\naim forming efﬁcient and controllable datacenter ecosystems\nFigure 3. Reference architecture for datacenters (2 levels of depth).\n(see C1) and, as a key challenge a fully automated resource\nmanagement and scheduling system for datacenters, able to\naddress: (i) the core principles of MCS, and (ii) in particular\nthe challenges introduced in Section 4.1. We address here\nseveral of these issues.\nWe envision datacenters are increasingly equipped with\na (fully) software-deﬁned stack (C2), managing nearly au-\ntomatically workload, infrastructure, and peopleware het-\nerogeneity (C4 and C5). This will alleviate the need for\nlive-teams of engineers spending time on relatively trivial\ndecisions, and instead allow them to focus on (i) monitoring,\ndiagnosing, and controlling new and particularly complex\nworkﬂows and dataﬂows on behalf of users, (ii) navigating\nthe ecosystem (C9), (iii) designing ecosystems, constructing\nand exploring what-if scenarios, etc.\nDatacenters will support increasingly more sophisticated\nnon-functional requirements (C3), emerging in MCS, e.g.,\nsuper-scalability and super-ﬂexibility (see P5), or in special-\nized classes of datacenters, e.g., trust and personalized con-\ntrol in edge-centric (micro-)datacenters [7]. We envision a\nguiding, non-mandatory reference architecture for datacenter\necosystems to capture and help manage the diversity of of-\nfered services and underlying software layers. For example,\nFigure 3 depicts our reference architecture for datacenters,\ncomprised of 5 core layers, Front-end for the application-\nlevel functionality, Back-end for task, resource, and service\nmanagement on behalf of the application, Resources for task,\nresource, and service management on behalf of the cloud\noperator, Operations Service for basic services that are typ-\nically associated with (distributed) operating systems, and\nInfrastructure for managing physical and virtual resources;\na 6th layer, DevOps covers functions essential to operating\nthe datacenter but orthogonal to the service provided to\ncustomers, such as monitoring, logging, and benchmarking.\nEmphasizing the intense focus of the community on sim-\nplifying the development of cloud-based applications, the\nlayers closest to the users are further reﬁned into 3 sub-\nlayers each; the sub-layers High Level Languages ,Program-\nming Models , and Execution andMemory & Storage engines\ncorrespond to the similarly named layers in Figure 1.\nOne of the key emerging capabilities of datacenters\n\nis to support complex services (C8) while making nearly\noptimal use of available resources (C7). The dual problem\nof provisioning and allocation is particularly challenging for\nthe diverse ecosystems active in datacenters. The complex\napproaches that are currently in use rely on complex op-\nerations, conﬁgured through relatively simple policies only\nin key points. Inspired by the work of Schopf [155], who\nproposed in 2004 a detailed 11-step abstraction for the\ngrid scheduling landscape, we envision the formulation of a\ndetailed reference architecture for scheduling in datacenters.\nIn this formulation, scheduling is a multi-stage workﬂow\nthat covers the set of most common actions in datacenter\nscheduling, with tasks ranging from ﬁltering resources avail-\nable to the user to task migration. We conjecture that this\nfocus on speciﬁc stages in the complex scheduling process\ncan facilitate new and competitive designs, and enables\nnewcomers to more easily understand the common structure\nof schedulers.\nThe reference architecture for scheduling in datacen-\nters further enables sharing of entire scheduling solutions\nor mere components (C11). Pursuing this goal, we en-\nvision [130] a global competition where participants can\ndesign and submit their own schedulers, or even parts of\nschedulers grafted into library-designs of complete sched-\nulers. After simulating their submissions with standard ex-\nperiments, we will publish the results and announce the\nwinning scheduler. A competition of this sort could foster\ninnovation in the domain, and inspire students to learn more\nabout the process.\n6.2. Science ÷MCS , Virtuous Cycle: The Future\nof Big Science, Democratic Science, and e-Science\nWe see the future of science as forming a virtuous cycle\nwith that of technology. Science is increasingly interwoven\nwith the technology that enables it [11, Ch.3, loc.903].\nModern science requires experimentation, observation, and\nreasoning that are possible only through modern technol-\nogy, and modern technology increasingly complements its\nown capabilities with the ﬁndings of modern science [11,\nCh.3]. Unsurprisingly, as the scientiﬁc experiments become\nincreasingly more ambitious and larger, the sophistication\nand scale of the computer systems supporting them also\nincrease, in a virtuous cycle. We discuss in this section three\nscientiﬁc drivers for MCS, in turn, Big Science, Democratic\nScience, and e-Science. For each, we see MCS as disruptive\ntechnology.\nBig Science64pushes the limits of current computer\necosystems and raises many of the challenges we raise\nin Section 5, e.g., massive projects requiring software-\ndeﬁned everything (C2), diverse non-functional properties\nincluding efﬁciency and trust (C3), various forms of sys-\ntem and peopleware heterogeneity (C4 and 5, respectively),\netc. For example, large scientiﬁc experiments rely on fed-\nerated infrastructure to perform data collection, ﬁltering,\n64. Massive scientiﬁc endeavors working as industrial-scale research\n[156, p.1]: large teams, large scientiﬁc apparatus, big budgets.and analytics, and especially their storage and processing\ninfrastructure spans federated, geo-distributed data centers.\nPossibly the best modern example of Big Science is the\nLarge Hadron Collider [157]65, which recently reached the\n200 petabyte milestone66. Analyzing such volume of data\nis already strenuous for modern computer systems. How-\never, upcoming Big Science projects are expected to deliver\neven larger volumes and vicissitudes. Such projects include\nthe Square Kilometer Array [158], the KM3NeT cubic-\nkilometer telescope searching for neutrinos, or the new\nLarge Hadron Collider67that is expected to be three times as\nlarge as its predecessor. We envision MCS to be the enabler\nof the computing technology and infrastructure behind such\nchallenging projects, which should provide a sustainable and\nefﬁcient data processing and storage layers.\nDemocratic science is also a scientiﬁc driver for MCS.\nRecent advances in hardware technology have made the\nmarket more accessible than ever: storing one gigabyte of\ndata costs below $0.0568, while performing 1 GFLOP costs\nbelow $0.169. Consequently, it is cheaper and more efﬁcient\ntoday to process large amounts of data and to simulate\ncomplex situations, than at any point in the history of the\nhuman kind. This, through XaaS (C8), give unprecedented\naccess to science-grade facilities to an increasing number\nof small laboratories and research groups around the world,\nenabling the acceleration of scientiﬁc discovery without\nthe large funding or teams speciﬁc to Big Science, thus\ndemocratizing and simplifying access to (virtual) comput-\ning infrastructure. Early proposals, such as OurGrid [159],\nenvisioned a single, global collaborative grid environment\nfor small teams. With the hardware resources of today,\nwe envision infrastructure for enabling large-scale scientiﬁc\nexperimentation and discovery, following MCS principles.\nThis could disrupt the elite echelons of science.\ne-Science and MCS can also form the virtuous cycle:\nﬁelds such as biology, astronomy, and physics are discov-\nering the beneﬁts, but also the challenges, of cloud com-\nputing and big data processing [160]. New and meaningful\nknowledge is found though the analysis of existing data, but\nwith increased heterogeneity of users, of workloads, and of\nresources and services, the computer ecosystems in this ﬁeld\nmust conquer the vicissitude of different “V”s posing chal-\nlenges at different moments in time [22]. Many of the appli-\ncations that run in clouds are structured as shareable work-\nﬂows, for example, BLAST [161] and Epigenomics [114]\nin bioinformatics, LIGO [114] and Montage [114] in com-\nputational astrophysics. However, workﬂow management\nacross heterogeneous resources and services, and scheduling\nmixtures of workﬂows on behalf of diverse users, remain\n65. The Large Hadron Collider is a successor of the heroic ﬁrst project\nin Big Science, initiated by Ernest O. Lawrence’s laboratory based on the\ncyclotron (a small, simple, Nobel-Prize-winning collider) and established\nthrough the extension of the laboratory to include a large number of diverse\nresearchers and especially external visitors, all starting in the early 1930s.\n66. https://tinyurl.com/CERN200PB\n67. https://tinyurl.com/LHC3x\n68. https://www.backblaze.com/blog/hard-drive-cost-per-gigabyte/\n69. AMD Radeon Vega 64 costs \u0012$2,000, for 13.7 TFLOPS (single).\n\nrelatively open challenges. The development of technol-\nogy regarding Internet-of-Things (IoT) is contributing to\nthe amount of data and sensor worldwide. Trusted data-\ncollection and -processing pipelines, which are crucial when\nthe number of laboratories involved in processing increases,\ncould leverage ecosystems that use novel trust-ensuring\ntechniques for provenance recording and checking (e.g., the\nemerging blockchain family of technologies). Smaller-scale\nthan in Big Science, but nevertheless signiﬁcant, data- and\ncompute-related appear also here, and must be solved for\nmore heterogeneous clients and with much lower budgets.\n6.3. Online Gaming: Can Small Studios Entertain\nOne Billion People with Near-Zero Up-Front Cost?\nOver one billion players70and over a third of a billion\nspectators71are valuable72clients of the gaming indus-\ntry. Gaming is not only the most valuable branch of the\nentertainment industry in most countries, it is also used\nin enterprise training and employee-interviews, in various\nforms of serious gaming including what-if and disaster-\nscenario analysis, and in (higher) education especially for\nsimulation of scenarios.\nOnline gaming is a complex, multi-functional applica-\ntion domain. Figure 4 summarizes the four key functions\npresent in online gaming through a house-like metaphor.\nBesides (i) the game itself, which provides the service of\nmaintaining a seamless Virtual World , online gaming must\nalso ensure: (ii) the analysis of game and especially player\ndata, through a Gaming Analytics platform that supports\npossibly complex business and operational decisions; (iii)\nthe generation, curation, and provision of content, from\nwhich the automated part is ensured by a platform for\nProcedural Content Generation [165]; (iv) through a Social\nMeta-Gaming platform, the management and fostering of a\ncommunity interested in using the game as a symbol relating\nto diverse, possibly non-game related, activities.\n70. The survey company Newzoo reports over 2.2 billion play-\ners in 2017 [https://newzoo.com/insights/articles/newzoo-2017-report-\ninsights-into-the-108-9-billion-global-games-market/], up from the 1.8 bil-\nlion players of which 711 millions are active as reported by Intel in\n2015 [https://blogs.intel.com/technology/2015/08/the-game-changer/]. The\nprogress reported here is consistent with the multi-decade-long ﬁgures\nprovided by the US Entertainment Software Association, for example in its\n2017 report [http://www.theesa.com/wp-content/uploads/2017/04/EF2017\nFinalDigital.pdf]. In the US, over two-thirds of the households have gamers\nwho spend signiﬁcant time playing games weekly; the average age of the\ngamer is mature, around 35 years.\n71. Similarly to competitive sports such as (European) football and\nthe Olympic Games, competitive electronic sports (eSports) are rapidly\ngrowing in audience. One of the ﬁrst eSport events dates back from\n1999 [162]. Social streaming platforms such as Justin.tv, own3d, and\nmore recently Twitch and Youtube Gaming, have greatly contributed to an\nincrease in popularity and interest. Live audiences have soared, with the\nmost popular events attractive 20 million unique viewers in 2014, and over\n45 million in 2017 [163]. Globally, the number of viewers has increased\nfrom 235 million in 2015, to 385 million in 2017, and it is expected that\nthis number reaches 598 million by 2020 [164].\n72. Newzoo estimates the global market value to have reached $109\nbillion in 2017, a continuous increase from the $70 billion reported in\n2012. The eSports spectator-related activities are expected to reach $1.5\nbillion by 2020, increasing from $696 million in 2017 [164].\n  Social Meta-Gaming Platform\nVirtual \nWorldGaming\nAnalyticsProcedural\nContent \nGenerationemergent behavior\n(Resource Management & Scheduling)\n(Distrib.Sys.: Naming, Consistency, etc.)Capacity planning / FLP\nCluster\nMulti-cluster, +/- Sharding\nCloud-based + Offloading\nSuperServer + Streaming\nNaming: central vs. P2P\nConsistency: dead reckoning vs.\n(continuous) lock-step vs.\n(eventual) AoI / Simulation vs.\n(conit-based) static vs. dynamic\n(Domain-specific)\nAvatar simulation\nNPC & world simulationCapacity planning / FLP\nCluster\nCloud-based\nHeterogeneity: +GPUs\nAccuracy vs. performance\nDistrib. graph-processing\nProcessing workflows\nData-intensive processing\nGaming analytics (privacy)\nAuto decision-making (toxicity)Capacity planning / FLP\nCluster\nContent complexity, freshness\nMatching players w/ content \nProcessing workflows\nCompute-intensive processing\nFigure 4. Functional reference architecture for online gaming, with main\ntopics (1 level of depth).\nAlthough, according to the ESA, over half of the fre-\nquent players are engaged in multiplayer games through\nonline gaming services, the services they receive remain\nsub-par: (i) the virtual worlds are not seamless, in that they\ncannot host more than a few thousands of players in the\nsame contiguous virtual-space, and in fast-paced games it\nis rarely possibly to engage more than a few tens of simul-\ntaneous players, (ii) the player activity is rarely analyzed\nin depth, correlating social-network and other data across\nlarge groups of players is not offered as a service to players,\nand large teams of community-managers still have to take\nmany decisions case-by-case, (iii) the game content is rarely\nupdated, rarely player-customized, and never fresh at the\nscale of the community, and (iv) the social platform enabling\nmeta-gaming [49], that is, spending time in activities related\nto the game itself, such as playing in a tournament or\nbeing spectators, offers only basic tools beyond viewing\nand sharing of basic content. These problems stem from\ndeﬁcient gaming ecosystems: the predominant industry ap-\nproach towards offering online gaming is self-hosting, that\nis, buying large-scale infrastructure and operating services\nin-house. This approach does not allow small studios to join\nthe market: the barrier of expertise and of start-up costs\nis too high. Figure 4 lists many challenges to overcome,\ngrouped by function. We discuss challenges (i) and (ii) in\nthe following; the computational challenge of (iii) is still\nlargely unsophisticated [166] and challenge (iv) still has to\novercome ﬁrst the issues described in Section 6.6. overall,\nwe see online gaming and MCS as mutually reinforcing,\nwith the solutions provided by MCS triggering continuously\nnew needs from online gaming.\nFor the Virtual World , MCS disrupts the current ap-\nproach by promising to eliminate the barriers of entry\nthrough the use of third-party services in diverse ecosystems.\nCan small studios entertain up to one billion people with\nnear-zero up-front costs? By leveraging cloud computing\n\ntechniques, online games can be massivized [167]: they\ncan be positioned close to each player [168] yet cost-\neffectively [169], can elastically scale with the ups and\ndowns of active players [170], and can be made highly\navailable for a fraction of the cost [171]. Coupled with\nmany other advances from Distributed Systems, Software\nEngineering, and Performance Engineering, and their inclu-\nsion in the larger ecosystem through MCS techniques, we\nenvision this could solve the key Virtual World challenges.\nForGaming Analytics , the challenge of processing con-\nnected data at scale remains largely open (see Section 6.6),\nbut the richest and most tech-savvy in the industry have\nstarted to leverage data-processing ecosystems. Since 2014\nand increasingly, the largest gaming companies have started\nto use third-party data-science services for gaming analytics,\ne.g., the engineering team at Twitch detailed the use of\nRedShift for more than 100k queries per second on terabytes\nof data without manual optimization73, Blizzard Entertain-\nment has hired Teradata to warehouse data for World of\nWarcraft, Overwatch, and other popular games74, and Riot\nGames has hired Databricks to process data for League of\nLegends using Spark75The challenge of enabling this scale\nand complexity, under a cost model affordable for small\nstudios remains open.\n6.4. The Future of Banking\nBanking is a vital component of modern industries,\nespecially for knowledge-based societies: banking facilitates\ntransferring, depositing, and lending capital. As a heavily\nregulated industry, banking has been traditionally slow to\nuptake novel technology and often operates with multi-\ndecade legacy ICT systems. However, since 2008 the indus-\ntry has seen a signiﬁcant change, combining two contrary\ndirections: (i) more regulation in terms of increased liability\nand lower tolerance for risk, with (ii) increased openness\nof the market aiming to provide better service for (retail-\n)consumers. For example, new regulation appeared in the\nbanking and ﬁnancial industry, in response to the 2007-2008\nﬁnancial crisis, including the Basel series of stress-tests. To\nopen the market, since 2008 a Single Euro Payments Area\n(SEPA) helps harmonizing bank transactions in Europe. To\nfurther open the market, in 2015 but with effective imple-\nmentation in 2018, the European Union has passed into law\nthe second Payment Services Directive (PSD2), which opens\nup the retail-banking market for more service providers\n(e.g., Mint for account information), ﬁntech companies (e.g.,\nAdyen and Klarna for payments, Tink for budget manage-\nment, OurCrowd for crowdfunding), and even the traditional\nconsumer-facing brands (e.g., Google, Apple, telcos, who\ncan combine online retail with banking functionality). We\nthus see the future of banking as becoming increasingly\ndependent on complex ecosystems, and thus requiring the\ncapabilities promised by MCS while offering back increas-\ningly more dynamic requirements.\n73. twitch.tv tech blog, https://tinyurl.com/TwitchDataArchi16\n74. Since around 2015, see https://tinyurl.com/BlizzardTeradata15\n75. Since 2016, see https://tinyurl.com/RiotDatabricks16.We will present our vision through a concrete example.\nThe PSD2 regulation is disruptive76, because banks have\nto open up payment functionality through APIs to other\nﬁnancial operators, and give access to personal data to\ncustomers; not only this can lead to banks losing access to\ntheir customers (to consumer-facing brands) and lucrative\nvalue-adding operations (to ﬁntech companies), but banks\nare now forced to integrate into a much more complex\nsoftware ecosystem. Moreover, PSD2 enforces strict per-\nformance targets, including deadlines in clearing ﬁnancial\ntransactions such as payments, contracts, and salaries; and\noffer more customer rights, including the right to refund.\nSecurity and privacy, which are guaranteed under PSD2,\nmust be reevaluated in the framework of new law, such\nas the new European General Data Protection Regulation\n(GDPR) [172]. The current banks must address these new\nchallenges, while managing thousands of their own ﬁnancial\napplications (over 1,400 at ING, the largest bank in the\nNetherlands [173]) across the diverse and changing legisla-\ntions of EU’s 28 member-states, while facing the signiﬁcant\ndeﬁcit of skilled personnel that affects the European ICT\nmarket77.\nWe envision that MCS can help with the necessary\ntransformations of the emerging banking ecosystems: (i)\nby offering banks its core principles (Section 4), effec-\ntively an ecosystem-oriented framework that allows them\nto design and compose their software with complex func-\ntional and non-functional requirements, and with complex\npeopleware and methodological issues; (ii) by building a\nbody of knowledge and skill to approach the daunting prob-\nlem of combining hundreds of components, some of them\nlegacy applications, others provided by third-parties, into an\necosystem offering meaningful guarantees of its properties;\n(iii) by making resource management and scheduling a\nkey building block, capable of ensuring the complex non-\nfunctional requirements appearing in this domain, including\ndeadlines, availability, security, and privacy; (iv) by making\nself-awareness, and in particular the ability to auto-scale\nand to tolerate failures (and malicious behavior), a key\nconcern; (v) by considering the inherent heterogeneity due\nto operation in private datacenters, private and public clouds,\nand edge [7] and fog [175] computing infrastructure.\nThe remaining challenges remain staggering, but we\nsee MCS as the only alternative to understand and create\nthese complex ecosystems. For example, much of banking\nsoftware must pass strict validation tests, which cannot be\neasily done; early attempts from Software Engineering, such\nas the use of formal speciﬁcations that represent banking\nknowledge and can be validated in Rascal [173], are already\nuseful but still have high Distributed Systems and Perfor-\nmance Engineering thresholds to pass: can they validate\nfor such diverse and large ecosystems?, can they validate\nfor non-functional requirements such as performance and\navailability? with Performance Engineering, Petri nets and\n76. The Economist, An earthquake in European banking, Mar 2017.\nhttps://tinyurl.com/TheEconomist17Banking\n77. Europe faces a deﬁcit of over 900,000 IT specialists by 2020 [174]\n\nFigure 5. FaaS Reference Architecture ordered from business logic (BL)\nto operational logic (OL). (Developed jointly with the SPEC RG Cloud\ngroup.)\nother workﬂow tools can help with understanding transition\nof data, but privacy legislation is still changing, so how can\nwe account for the different views on security and privacy\nthat diverge per country and/or culture [176]?\n6.5. The Future of Apps: Serverless, Service- and\nWorkﬂow-based Ecosystems\nSince 2011, starting with grid computing work-\nloads [39], we are observing a transition in software-\ndevelopment paradigms and best-practices, from the old,\ncoarse-grained, monolithic projects, to the more modern,\nﬁne-grained approach of splitting projects into ever-smaller,\nindependent, service-based components, often referred to as\nmicroservices . As a consequence of this transition, since\n2016 cloud vendors are beginning to offer serverless com-\nputing services; on-demand services billed at a very ﬁne\nresource-granularity. [177] Moreover, within this domain\nof serverless computing, the Function-as-a-Service (FaaS)\nparadigm is emerging (see C8), offering users the ability to\ndeploy and execute arbitrary functions on cloud infrastruc-\nture without the burden of resource management.\nUser-deﬁned functions are typically stateless and inter-\nact with each other through an event-driven paradigm, or\nthrough a separate storage layer. These FaaS workloads\ncan often be modeled as (complex) workﬂows. Therefore,\nresearch and engineering efforts will have to address the\nchallenges of workﬂow and resource management, and of\nscheduling subject to complex non-functional requirements,\nin this complex design space.\nBecause the ﬁeld is currently in its inception, we see\nestablishing a comprehensive reference architecture for FaaS\nplatforms as a key challenge to overcome. This will provide\nthe community with a valuable conceptual tool: consistent\nterminology, a set of typical components, and enough low-\nlevel details to focus on the key problems. The pragmatic\nchallenges we envision in the FaaS and serverless domains\nare related to achieving good performance while isolating\nthe operation of each function across multiple tenants.\nBased on the analysis of the major open-source and of\na handful of closed-source FaaS platforms so far, we haveled the creation, jointly with the SPEC RG Cloud group, of\nthe preliminary reference architecture depicted in Figure 5.\nDerived from the coarser reference architecture for datacen-\nters depicted in Figure 3, our FaaS reference architecture\nis based on the notions of business logic derived from a\ntypical serverless application of a FaaS user, i.e., image\ntranslation and processing. The Resource Layer represents\nthe available resources within a cloud. These resources are\nmanaged by the Resource Orchestration Layer , which is of-\nten implemented by modern IaaS orchestration services (i.e.,\nKubernetes) and corresponds to layer 3 in Figure 3. On top\nof these orchestrated resources, the Function Management\nLayer manages instances of the cloud-function abstraction,\nby scheduling and routing functions (the runtime engine\nin layer 4 in Figure 3); and the Function Composition\nLayer is responsible for the meta-scheduling, that is, creating\nworkﬂows of functions and submitting the individual tasks\nto the management layer (layer 5 in Figure 3). To validate\nthis reference architecture, we have already matched its\ncomponents with real-world FaaS platforms such as Open-\nWhisk78and Fission79.\n6.6. The Future of Data: Generalized Graph Pro-\ncessing for the Modern Society\nGraphs, and more generally, connected-data80are ele-\ngant and powerful abstractions that enable rich analysis.\nThe analysis ranges from traditional graph algorithms (e.g.,\ngraph search, shortest paths, transitive closures), to modern\nmachine learning [178] and to deep learning on graph\ndata [179]. There are many (open) data sources, such as\ntaxi trip data81, real-time trafﬁc speed82, historical event\nrepositories83, social networks84, IoT sensor data85(e.g.,\nmeasuring noise pollution or air quality) that could be used\nto enrich and improve the lives of the modern society.\nThe key challenge here is the lack of an adequate\ncomputer ecosystem (the all-encompassing C1): we are still\nlacking the technologies that can be integrated to connect,\nexplore, query, and analyze such data sources, and further\nderive knowledge with societal impact. We believe that\nthrough MCS generalized graph-processing would solve this\nproblem by: (i) offering a powerful abstraction for exploring\nsuch data repositories and creating links between them, (ii)\nleveraging all the previous knowledge on how to efﬁciently\nprocess graphs, (iii) taming the necessary computing infras-\ntructure and making it compatible with the demands of both\nthe challenging workloads, and the users.\n78. https://openwhisk.apache.org/\n79. https://platform9.com/ﬁssion/\n80. We deﬁne connected-data as unstructured and distributed repositories\nof data, which can be connected through logical and functional relationships\nto infer knowledge.\n81. https://data.cityofnewyork.us/Transportation/2016-Yellow-Taxi-\nTrip-Data/k67s-dv2t\n82. https://data.cityofnewyork.us/Transportation/Real-Time-Trafﬁc-\nSpeed-Data/xsat-x5sa\n83. http://diveplus.frontwise.com\n84. http://konect.uni-koblenz.de/\n85. http://dsa.labs.vu.nl:5001\n\nAs MCS is solving the basic issues of the generalized\ngraph-processing problem, we also envision that new re-\nquirements will emerge, inspired by practical use. Current\napproaches in mining connected-data are only beginning to\nscratch the surface of the importance of graph processing for\nthe modern society. Recent studies have already shown its\napplicability by building systems and abstractions to combat\nhuman trafﬁcking [180], monitor wildﬁres [181], study the\nhuman brain [182], and discover new drugs [183]. What can\nMCS further enable?\n6.7. Which Applications Will Not Beneﬁt?\nWe also identify application domains that will not beneﬁt\nimmediately from MCS, among them: (i) tiny applica-\ntions with few users or modest requirements for resources,\nand whose instances work in isolation; (ii) super-high-\nperformance applications, such as high-frequency trading;\n(iii) all the applications that seem contrary to operation in\nan ecosystem, as described in Section 2.1.\n7. Related Work\nIn this section, we compare MCS with other paradigms,\nﬁrst, of computer science and second, of sciences and tech-\nnical sciences.\n7.1. Is MCS New?\nWe argue that asking “Is MCS new?” is not a well-\nformulated question. As any paradigm derived from suc-\ncessful existing paradigms, in this case, the paradigms of\nDistributed Systems we discuss in Section 7.2, MCS faces\na question of novelty [58, Ch. “Did we know it all along?”].\nHowever, deciding the novelty of a ﬁeld using a vaguely\ndeﬁned notion of novelty can lead to absurd results, such\nas ﬁnding that physics and computer science are not novel\nﬁelds of science86. It is also an open challenge to deﬁne\na concept of novelty that is not vague, for a ﬁeld address-\ning heterogeneous users, workloads, resources and services,\nprocesses, etc., as MCS does.\nInstead, we focus on the real thresholds of establishing\na new ﬁeld, introduced by Denning [15, p.32]. We have\ndiscussed these thresholds in Section 4.3 and posed the\nchallenge of meeting them gradually in Section 5.3 (C18).\nWe address next, in Section 7.2, the key distinguishing\nelements that currently characterize MCS.\n86.Reductio ad absurdum : We could ask: Is the entire computer science\napplied mathematics and physics? Are mathematics and physics applied\nphilosophy? And answer: Computer science could be seen as applied\nmathematics and physics, in that computer science theories and artifacts\noperate as the artifacts of mathematics and the laws of physics predict.\nMathematics and physics could be seen as applied philosophy, in that they\noperate in the logical and knowledge framework provided by the latter. But\nthis is absurd, because we know today that computer science and physics\nare ﬁelds of science. QED . Arthur gives a more cogent, but book-length\nargument [11].7.2. Vs. Other Paradigms Emerging from Dis-\ntributed Systems\nIn the large ﬁeld of distributed systems, we identify\nthree major paradigms that MCS builds upon: cluster, grid,\nand more recently, cloud computing and edge comput-\ning [7]. Much like science is in a constant co-evolution\nwith technology, all these ﬁelds have historically co-evolved\ntogether with the application types demanded by their users.\nCluster computing served the needs of tightly coupled,\npossibly communication intensive, high-performance scien-\ntiﬁc computing applications. Grid workloads were mostly\nhigh-throughput computing applications, i.e., long-running,\nconveniently parallel applications [107], such as bags of\ntasks or scientiﬁc workﬂows. Together with the data del-\nuge, and the fourth paradigm of data-driven scientiﬁc dis-\ncovery [184], analytics workloads, available now in many\nshapes and formats (e.g., MapReduce, stream processing,\nmachine learning), have migrated to the cloud, which offers\na uniﬁed platform for cost-efﬁcient computation and storage.\nWe have explored in Section 1 some of the shortcomings\nof previous paradigms, such as grid, cloud, peer-to-peer, and\nedge computing, when addressing challenges of computer\necosystems. We have also summarized in Section 1 and\nexpanded in Section 2.2 ﬁve classes of problems that these\nparadigms have not solved, but are addressed by MCS.\nOverall, in contrast with these paradigms, MCS focuses\non new problems and challenges (i.e., related to ecosys-\ntems, considering peopleware, and the combined spectrum\nscience-engineering-design), for which it offers new views\n(e.g., ecosystems-ﬁrst), new and powerful (predictive) con-\ncepts and techniques including a synthesis of techniques\nacross Distributed Systems, Software Engineering, and Per-\nformance Engineering, and new and existing but improved\ntechnologies and instruments (e.g., ecosystems studied in\nsilico or through full-stack simulation).\nFurthermore, MCS brings computing closer to the users,\nempowering them to control how computing ecosystems\nbehave by means of expressive, modern non-functional re-\nquirements (such as elasticity and security) and by con-\nsidering universal access to services to also include less\nsophisticated users.\n7.3. Parallels with and Other Fields of Science\nWe see the emergence of MCS from Distributed Sys-\ntems as a process similar to the emergence of other science\ndomains, which we have witnessed in the past three decades.\nTable 5 summarizes how MCS matches emergent sub-\nﬁelds of other science domains, following the framework\nof Ropohl [61, p.4–7]. Overall, we ﬁnd that similar goals\nand approaches as taken by MCS have emerged from other\ndomains of science and practice.\nThese emerging ﬁelds have started humbly, part of a\nbroader paradigm. Through useful evolutions of the re-\nspective domains of science and practice, they have then\ndeveloped into domains themselves [37]. This is the model\nthat we envision for MCS.\n\nEmergence Epistemological Characteristics\u0006\nField (Decade Emerging) Crisis Continues Objectives Object Methodology Character\nModern Ecology (1990s) Biodiversity loss Ecology and Evolution DS Biosphere ADHS AC\nModern Chem. Process (1990s) Process complexity Chemical Engineering DE Chemical proc. ADHSP ACEM\nSystems Biology (2000s) Systems complexity Molecular biology S Biological sys. AHS ACEMTU\nModern Mech. Design (2000s) Process sustainability Technical Design DE Mechanical sys. DHSP ACEM\nModern Optoelectronics (2010s) Artiﬁcial media Microwave technology S Metamaterials DHSP ACEMTU\nMCS (this work) Systems complexity Distributed Systems DES Ecosystems ADHSP ACES\nAcronyms follow the framework of Ropohl [61, p.4–7]: Objectives: D = Design, E = Engineering, S = Scientiﬁc. Methodology: A = abstraction, D = design (abductive creation), H = hierarchy, I = idealization, S = simulation, P = prototyping.\nCharacter: A = applicability, C = approved by the scientiﬁc/design/engineering community, E = empirically accurate, H = harmony between results, M = mathematically detailed, S = simplicity, T = truth, U = universality.\nTABLE 5. C OMPARISON OF FIELDS . THE ROW FOR MCS IS ENVISIONED .\nAmong the ﬁelds we survey, closest to MCS is Sys-\ntems Biology. In contrast to Systems Biology, which has a\ndistinctly scientiﬁc orientation and thus a character focused\nespecially toward universality and mathematical formulation\nof results, MCS focuses explicitly on design and synthesis\n(engineering) in its objectives, and on the pragmatic, empir-\nical character of its results. Although we do not exclude a\nlater re-focus of MCS on universal, mathematically formu-\nlated models and theories, for the moment we see a large gap\nbetween theory and practice that prevents this development\nwithout groundbreaking progress.\n8. Conclusion\nResponding to the needs of an increasingly digital and\nknowledge-based society, we envision ever-larger roles for\nvast and complex combinations of distributed systems that\nserve individuals and human-centered organizations. How-\never, current technology seems ill-equipped to achieve this\nvision: an ongoing systems crisis hampers not only evolv-\ning toward the vision, but even the current operation of\nmodern distributed systems. In this work, we propose an\nalternative, Massivizing Computer Systems (MCS). MCS\nfocuses on systems combined into ecosystems, with sci-\nentiﬁc, design, and engineering techniques evolving from\nmodern Distributed Systems, Software Engineering, and\nPerformance Engineering, and with a focus on peopleware\nand methodological outlook beyond mere technology.\nOur contribution is ﬁve-fold: (1) we deﬁne MCS to\nfocus on a new central paradigm, computer ecosystems,\nthat distinguishes it among the sub-ﬁelds of Distributed\nSystems, with respect for the rights of expert and non-\nexpert individuals, and with various elements that we believe\ncan lead organically, in the long-term, to the formation\nof a new ﬁeld of science; (2) we propose a set of core\nprinciples for MCS, including principles that go beyond\nmere technological aspects, such as scientiﬁc, design, and\nethical concerns; (3) we propose a diverse set of challenges\nfocusing on systems, peopleware, and methodological as-\npects derived from the core principles; (4) we identify and\nexplore various beneﬁts we envision MCS can bring to thefuture of six application domains, both in the area of modern\ncomputer systems and to use-cases such as Big Science,\ndemocratized science, and e-Science; (5) we contrast MCS\nwith both paradigms of modern computer science ﬁelds such\nas Distributed Systems, and emerging ﬁelds of science and\ntechnical science.\nWe have started to address the research agenda formu-\nlated in this article, both as a single research group and,\nthrough the SPEC RG Cloud group, in collaboration with\nnumerous academic and industry partners. We hope this\nvision-article will stimulate a larger community to join us\nin addressing these complex yet rewarding challenges.\nAcknowledgments\nThis work is supported by the Dutch projects Vidi Magna-\nData and KIEM KIESA, by the Dutch Commit and the Commit\nproject Commissioner, and by generous donations from Oracle\nLabs, USA. We thank our ﬁrst (internal) reviewers at the Delft\nUniversity of Technology and Vrije Universiteit Amsterdam, the\nNetherlands, including Jan Rellermeyer and Cristiano Giuffrida,\nrespectively. We are indebted to Dick H. J. Epema, whose ideas\nhave helped shape the career of the ﬁrst author and thus this\nvision of the AtLarge research group. We thank for discussing\nideas related to this work to former and current members of the\nAtLarge team87, to our collaborators from the SPEC RG Cloud\ngroup, and to our collaborators from the LDBC consortium, in-\ncluding: Cristina L. Abad (Escuela Superior Polit ´ecnica del Litoral,\nEcuador); Tarek Abdelzaher (University of Illinois at Urbana\nChampaign, IL, USA); Andre Bauer, Simon Eismann, Johannes\nGrohmann, Nikolas Herbst, Samuel Kounev, and Simon Spinner\n(U. Wuertzburg, Germany); Sara Bouchenak (INSA Lyon, France);\nTim Brecht (University of Waterloo, Canada); Mihai Capot ˜a (Intel\nLabs, Portland, OR, USA); Hassan Chaﬁ, Sungpack Hong, Thomas\nManhardt, and Petr Koupy (Oracle Labs, SFBA, USA); Ahmed\nAli El-Din (UMASS, USA); Athanasia Evangelinou and George\nKousiouris (Nat’l. Tech. U. of Athens, Greece); Bogdan Ghit and\nWing Lung Ngai (Databricks Amsterdam, the Netherlands); Stijn\nHeldens (U. Twente, the Netherlands); Eva Kalyvianaki (Imperial\nCollege of London, UK); Rouven Krebs and Simon Seif (SAP SE,\n87. https://atlarge-research.com/people.html\n\nGermany); Martina Maggio (Lund University, Sweden); Ole Meng-\nshoel (CMU Silicon Valley at the NASA Ames Research Center,\nPA, USA); Arif Merchant (Google, Inc., CA, USA); Alessandro\nV . Papadopoulos (Maelardalen University, Sweden); Arnau Prat\nP´erez and Josep Larriba Pey (UPC Barcelona); Andy Tanenbaum\nand Peter Boncz (Vrije Universiteit Amsterdam, the Netherlands);\nIlie Gabriel T ˘anase (IBM TJ Watson Research Center, NY , USA);\nMarkus Th ¨ommes (IBM, Germany); Yinglong Xia (Huawei Mu-\nnich, Germany); and Xiaoyun Zhu (Futurewei Technologies, CA,\nUSA).\nReferences\n[1] E. W. Dijkstra, “Computing Science: Achievements and Challenges,”\ninSAC, 1999, p. 1.\n[2] D. Tapscott, The digital economy: Promise and peril in the age of\nnetworked intelligence . McGraw-Hill New York, 1996, vol. 1.\n[3] P. Mell et al. , “The NIST deﬁnition of cloud computing,” NIST\nreport, 2011.\n[4] D. Silver et al. , “Mastering the game of Go with deep neural\nnetworks and tree search,” Nature , vol. 529, pp. 484–489, 2016.\n[5] V . Mayer-Schnberger and K. Cukier, Big data: A revolution that will\ntransform how we live, work, and think . Taylor & Francis, 2014.\n[6] A. Roy et al. , “Inside the Social Network’s (Datacenter) Network,”\ninSIGCOMM , 2015, pp. 123–137.\n[7] P. G. L ´opez et al. , “Edge-centric Computing: Vision and Chal-\nlenges,” Computer Communication Review , vol. 45, pp. 37–42, 2015.\n[8] M. van Steen et al. , “A brief introduction to distributed systems,”\nComputing , vol. 98, pp. 967–1009, 2016.\n[9] E. W. Dijkstra, “The Humble Programmer,” Commun. ACM , vol. 15,\npp. 859–66, 1972.\n[10] N. Wirth, “A Brief History of Software Engineering,” IEEE Annals\nof the History of Computing , vol. 30, pp. 32–39, 2008.\n[11] W. B. Arthur, The Nature of Technology . Free Press, 2009.\n[12] T. DeMarco et al. ,Peopleware . Dorset House, 2012.\n[13] A. Andrzejak, M. Arlitt, and J. Rolia, “Bounding the resource\nsavings of utility computing models,” HP Laboratories Palo Alto\ntechnical report HPL-2002-339, Dec 2002.\n[14] R. Buyya, “The Gridbus Toolkit for Grid and Utility Computing,”\ninCLUSTER , 2003.\n[15] P. J. Denning, “The science in computer science,” Commun. ACM ,\nvol. 56, pp. 35–38, 2013.\n[16] D. Meadows, Thinking in Systems . Chelsea Green Publishing, 2008.\n[17] S. Kounev et al. , “The Notion of Self-aware Computing,” in Self-\nAware Computing Systems. , 2017, pp. 3–16.\n[18] H. A. Simon, The Sciences of the Artiﬁcial . MIT Pess, 1996.\n[19] B. Lawson, How Designers Think . Taylor and Francis, 2004.\n[20] N. Cross, Design Thinking . Berg, 2011.\n[21] F. P. Brooks, The Design of Design . Addison-Wesley / Pearson\nEducation, 2010.\n[22] B. Ghit et al. , “V for Vicissitude: The Challenge of Scaling Complex\nBig Data Workﬂows,” in CCGrid , 2014, pp. 927–932.\n[23] F. F. Nah, “A study on tolerable waiting time: how long are Web\nusers willing to wait?” Behaviour & IT , vol. 23, pp. 153–163, 2004.\n[24] R. Taylor et al. , “Death by a thousand SLAs: a short study of\ncommercial suicide pacts,” Forschungsbericht, HPLabs , 2005.\n[25] Iosup et al. , “On the dynamic resource availability in grids,” in\nGRID , 2007, pp. 26–33.[26] Gallet et al. , “A Model for Space-Correlated Failures in Large-Scale\nDistributed Systems,” in Euro-Par , 2010, pp. 88–100.\n[27] Yigitbasi et al. , “Analysis and modeling of time-correlated failures\nin large-scale distributed systems,” in GRID , 2010, pp. 65–72.\n[28] H. S. Gunawi et al. , “Why does the cloud stop computing? lessons\nfrom hundreds of service outages,” in SoCC , 2016, pp. 1–16.\n[29] C. Yang et al. , “Understanding the market-level and network-level\nbehaviors of the Android malware ecosystem,” in ICDCS , 2017.\n[30] G. Gu et al. , “BotMiner: Clustering analysis of network trafﬁc for\nbotnet detection,” in USENIX Sec. Symp. , 2008, pp. 139–154.\n[31] Y . Duan et al. , “Everything as a Service (XaaS) on the Cloud:\nOrigins, Current and Future Trends,” in CLOUD , 2015.\n[32] Herbst et al. , “Ready for Rain? A View from SPEC Research on\nthe Future of Cloud Metrics,” CoRR , vol. abs/1604.03470, 2016.\n[33] European Commission, “Uptake of cloud in Europe,” EU rep., 2014.\n[34] S. Chakrabarti, “Hard questions: What effect does social media have\non democracy?” Facebook Online Newsroom, https://newsroom.fb.\ncom/news/2018/01/effect-social-media-democracy/, Jan 2018.\n[35] M. M ¨artens et al. , “Toxicity detection in multiplayer online games,”\ninNETGAMES , 2015, pp. 1–6.\n[36] The Economist, “Taming the titans,” Jan 20–26, 2018, p.11–12,\nhttps://www.economist.com/printedition/2018-01-20, Jan 2018.\n[37] M. Snir, “Computer and information science and engineering: one\ndiscipline, many specialties,” Commun. ACM , vol. 54, no. 3, 2011.\n[38] A. R. Hevner et al. , “Design science in information systems re-\nsearch,” MIS Quarterly , vol. 28, no. 1, 2004.\n[39] Iosup et al. , “Grid Computing Workloads,” IEEE Internet Comput-\ning, vol. 15, pp. 19–26, 2011.\n[40] Brereton et al. , “Lessons from applying the systematic literature\nreview process within the software engineering domain,” J. of Sys.\nand Softw. , vol. 80, no. 4, 2007.\n[41] H. E. Bal et al. , “A Medium-Scale Distributed System for Com-\nputer Science Research: Infrastructure for the Long Term,” IEEE\nComputer , vol. 49, pp. 54–63, 2016.\n[42] A. Iosup et al. , “LDBC Graphalytics: A Benchmark for Large-Scale\nGraph Analysis on Parallel and Distributed Platforms,” PVLDB ,\nvol. 9, pp. 1317–1328, 2016.\n[43] A. Ilyushkin et al. , “An Experimental Performance Evaluation of\nAutoscalers for Complex Workﬂows,” in TOMPECS (Best Paper\nNominations from ICPE’17, revised and extended versions) , 2018.\n[44] M. Aiello et al. , “Performance assessment and analysis of DNS\ntunneling tools,” Logic Journal of the IGPL , vol. 21, no. 4, 2013.\n[45] Y . Guo et al. , “How well do graph-processing platforms perform?”\ninIPDPS , 2014, pp. 395–404.\n[46] ——, “An Empirical Performance Evaluation of GPU-Enabled\nGraph-Processing Systems,” in CCGrid , 2015, pp. 423–432.\n[47] S. J. Gould et al. , “Exaptation a missing term in the science of\nform,” Paleobiology , vol. 8, no. 1, 1982.\n[48] Lu Jia et al. , “Socializing by Gaming: Revealing Social Relation-\nships in Multiplayer Online Games,” TKDD , vol. 10, no. 11, 2015.\n[49] S. Shen et al. , “The XFire online meta-gaming network: observation\nand high-level analysis,” in MMVE , 2011.\n[50] A. L. Jia et al. , “When Game Becomes Life: The Creators and Spec-\ntators of Online Game Replays and Live Streaming,” TOMCCAP ,\nvol. 12, pp. 47:1–47:24, 2016.\n[51] W. G. Vincenti, What Engineers Know and How They Know It .\nJohnS Hopkins University Press, 1990.\n[52] J. Plaice, “Computer Science Is an Experimental Science,” ACM\nComput. Surv. , vol. 27, p. 33, 1995.\n\n[53] W. F. Tichy, “Should Computer Scientists Experiment More?” IEEE\nComputer , vol. 31, pp. 32–40, 1998.\n[54] D. Feitelson, “Experimental computer science: The need for a\ncultural change,” Tech.Rep., www.cs.huji.ac.il/ \u0012feit/exp/, Dec 2006.\n[55] D. Epema, “Computer science as an experimental science,”\nQuadraad, www.cs.vu.nl/das4/Quadraad 2011-2-DAS4.pdf, 2011.\n[56] T. Ideker et al. , “A new approach to decoding life: Systems Biology,”\nAnnu. Rev. Genomics Hum. Genet. , vol. 2, pp. 343–72, 2001.\n[57] H. Kitano, “Systems Biology: a brief overview,” Science , 2002.\n[58] L. Alberghina et al. ,Systems Biology . Springer, 2005.\n[59] B. Meyer, “Towards empirical answers to important software\nengineering questions,” bertrandmeyer.com/2018/01/26/towards-\nempirical-answers-important-software-engineering-questions/.\n[60] D. S. Katz et al. , “Engineering Academic Software (Dagstuhl Per-\nspectives Workshop 16252),” Dagstuhl Manifestos , vol. 6, 2017.\n[61] M. Boon, “Computer Science Is an Experimental Science,” Int’l.\nStudies in Phil. Sci. , vol. 30, pp. 27–48, 2006.\n[62] A. S. Tanenbaum, “Lessons learned from 30 years of MINIX,”\nCommun. ACM , vol. 59, no. 3, 2016.\n[63] M. Monteiro, Design Is a Job . A Book Apart, 2012.\n[64] W. Aspray et al. , “The History of Software Engineering (Dagstuhl\nPerspectives Workshop 9635),” Dagstuhl Reports , 1996.\n[65] A. E. Eiben et al. ,Introduction to evolutionary computing , 2nd ed.\nSpringer Verlag, 2015.\n[66] L. Valiant, Probably Approximately Correct: Nature’s Algorithms\nfor Learning and Prospering in a Complex World . Basic, 2013.\n[67] S. Williams et al. , “Rooﬂine: an insightful visual performance model\nfor multicore architectures,” Commun. ACM , vol. 52, no. 4, 2009.\n[68] G. Pahl et al. ,Engineering Design . Springer-Verlag, 2007.\n[69] J. J. Shah et al. , “Metrics for Measuring Ideation Effectiveness,”\nDesign Studies , vol. 24, no. 2, 2003.\n[70] P. J. Denning et al. ,Great Principles of Computing . MIT, 2015.\n[71] H. Bahrami et al. ,Super-Flexibility for Knowledge Enterprises .\nSpringer Verlag, 2005.\n[72] J. Gray, “What next?: A dozen information-technology research\ngoals,” J. ACM , vol. 50, pp. 41–57, 2003.\n[73] M. Y . Vardi, “Computer professionals for social responsibility,”\nCommun. ACM , vol. 61, no. 9, 2018.\n[74] Y . Noguchi, “Will work for no beneﬁts: The challenges of being\nin the new contract workforce,” NPR Series on the Rise of the\nContract Workers, https://www.npr.org/2018/01/23/579720874/will-\nwork-for-no-beneﬁts-the-challenges-of-being-in-the-new-contract-\nworkforce, Jan 2018.\n[75] P. J. Denning et al. , “The profession of IT - Who are we - now?”\nCommun. ACM , vol. 54, pp. 25–27, 2011.\n[76] D. G. Feitelson, “Experimental computer science: The need for a\ncultural change,” The Hebrew University of Jerusalem, Tech. Rep.,\n2005.\n[77] M. Baker, “Is there a reproducibility crisis?” Nature , vol. 533, no.\n7604, pp. 452–454, 2016.\n[78] A. M. Smith et al. , “Journal of Open Source Software (JOSS): design\nand ﬁrst-year review,” CoRR , vol. abs/1707.02264, 2017.\n[79] K. Maheshwari et al. , “Report on the ﬁrst workshop on negative and\nnull results in eScience,” Concurrency and Computation: Practice\nand Experience , vol. 29, 2017.\n[80] B. Ghit et al. , “Balanced resource allocations across multiple dy-\nnamic MapReduce clusters,” in SIGMETRICS , 2014, pp. 329–341.\n[81] L. Bass, Super-Flexibility for Knowledge Enterprises . Addison-\nWesley, 2015.[82] A. Iosup et al. , “Analyzing Implicit Social Networks in Multiplayer\nOnline Games,” IEEE Internet Computing , vol. 18, pp. 36–44, 2014.\n[83] M. Y . Vardi, “Where have all the workshops gone?” Commun. ACM ,\nvol. 54, p. 5, 2011.\n[84] ——, “More debate, please!” Commun. ACM , vol. 53, no. 5, 2010.\n[85] A. Ganguly et al. , “WOW: Self-Organizing Wide Area Overlay\nNetworks of Virtual Workstations,” in HPDC , 2006, pp. 30–42.\n[86] S. Jain et al. , “B4: experience with a globally-deployed software\ndeﬁned WAN,” in SIGCOMM , 2013, pp. 3–14.\n[87] E. Thereska et al. , “IOFlow: a software-deﬁned storage architecture,”\ninSOSP , 2013, pp. 182–196.\n[88] S. Shin et al. , “FRESCO: Modular Composable Security Services\nfor Software-Deﬁned Networks,” in NDSS , 2013.\n[89] A. Darabseh et al. , “SDDC: A Software Deﬁned Datacenter Exper-\nimental Framework,” in FiCloud , 2015, pp. 189–194.\n[90] Y . Jararweh et al. , “Software deﬁned cloud: Survey, system and\nevaluation,” FGCS , vol. 58, pp. 56–74, 2016.\n[91] A. Kert ´eszet al. , “Grid Interoperability Solutions in Grid Resource\nManagement,” IEEE Systems Journal , vol. 3, pp. 131–141, 2009.\n[92] H. E. Bal et al. , “Real-World Distributed Computer with Ibis,” IEEE\nComputer , vol. 43, pp. 54–62, 2010.\n[93] S. Shen et al. , “An availability-on-demand mechanism for datacen-\nters,” in CCGrid , 2015, pp. 495–504.\n[94] T. Dillon et al. , “Cloud computing: issues and challenges,” in AINA ,\n2010, pp. 27–33.\n[95] A. Iosup et al. , “Self-awareness of Cloud Applications,” in Self-\nAware Computing Systems. , 2017, pp. 575–610.\n[96] J. D. Brutlag et al. , “User preference and search engine latency,”\nJSM Proceedings, Qualtiy and Productivity Research Section , 2008.\n[97] X. Liu et al. , “Survey of real-time processing systems for big data,”\ninIDEAS , 2014, pp. 356–361.\n[98] T. Chen et al. , “MXNet: A Flexible and Efﬁcient Machine Learn-\ning Library for Heterogeneous Distributed Systems,” CoRR , vol.\nabs/1512.01274, 2015.\n[99] G. B. Team, “In-Datacenter Performance Analysis of a Tensor\nProcessing Unit,” in ISCA , 2017, pp. 1–12.\n[100] S. Palkar et al. , “Weld: Rethinking the Interface Between Data-\nIntensive Applications,” CoRR , vol. abs/1709.06416, 2017.\n[101] A. Datta et al. , “Infrastructures for Online Social Networking Ser-\nvices [Guest editorial],” IEEE Internet Computing , vol. 16, pp. 10–\n12, 2012.\n[102] C. Wilson et al. , “Beyond Social Graphs: User Interactions in Online\nSocial Networks and their Implications,” TWEB , vol. 6, pp. 17:1–\n17:31, 2012.\n[103] A. Iamnitchi et al. , “The Small World of File Sharing,” TPDS ,\nvol. 22, pp. 1120–34, 2011.\n[104] ——, “Filecules in High-Energy Physics: Characteristics and Impact\non Resource Management,” in HPDC , 2006, pp. 69–80.\n[105] S. Zheng et al. , “Analysis and Modeling of Social Inﬂuence in High\nPerformance Computing Workloads,” in Euro-Par , 2011, pp. 193–\n204.\n[106] P. Garbacki et al. , “2Fast : Collaborative Downloads in P2P Net-\nworks,” in P2P, 2006, pp. 23–30.\n[107] A. Iosup et al. , “How are Real Grids Used? The Analysis of Four\nGrid Traces and Its Implications,” in GRID , 2006, pp. 262–269.\n[108] ——, “The Characteristics and Performance of Groups of Jobs in\nGrids,” in Euro-Par , 2007, pp. 382–393.\n\n[109] A. Iamnitchi et al. , “The Social Hourglass: An Infrastructure for So-\ncially Aware Applications and Services,” IEEE Internet Computing ,\nvol. 16, pp. 13–23, 2012.\n[110] T. Kraska et al. , “The Case for Learned Index Structures,” CoRR ,\nvol. abs/1712.01208, 2017.\n[111] A. Galakatos et al. , “A-Tree: A Bounded Approximate Index Struc-\nture,” CoRR , vol. abs/1801.10207, 2018.\n[112] V . van Beek et al. , “Self-Expressive Management of Business-\nCritical Workloads in Virtualized Datacenters,” IEEE Computer ,\nvol. 48, pp. 46–54, 2015.\n[113] Li, “Realistic Workload Modeling and Its Performance Impacts in\nLarge-Scale eScience Grids,” TPDS , vol. 21, 2010.\n[114] Bharathi et al. , “Characterization of scientiﬁc workﬂows,” in\nWORKS , 2008.\n[115] C. Reiss et al. , “Heterogeneity and dynamicity of clouds at scale:\nGoogle trace analysis,” in SoCC , 2012, p. 7.\n[116] A. Olteanu et al. , “Extending the capabilities of mobile devices\nfor online social applications through cloud ofﬂoading,” in CCGrid ,\n2013, pp. 160–163.\n[117] B. Jennings et al. , “Resource management in clouds: Survey and\nresearch challenges,” J. Netw. and Sys. Mgmt. , vol. 23, no. 3, 2015.\n[118] A. Uta et al. , “Towards Resource Disaggregation–Memory Scaveng-\ning for Scientiﬁc Workloads,” in CLUSTER , 2016, pp. 100–109.\n[119] Gens et al. , “Worldwide and regional public it cloud services\n20132017 forecast,” Tech.Rep. by IDC, Doc #242464, Feb 2014.\n[120] H. Abu-Libdeh et al. , “RACS: a case for cloud storage diversity,”\ninSoCC , 2010, pp. 229–240.\n[121] S. Labes et al. , “Standardization Approaches within Cloud Com-\nputing: Evaluation of Infrastructure as a Service Architecture,” in\nFedCSIS , 2012, pp. 923–930.\n[122] M. P. Robillard et al. , “Automated API Property Inference Tech-\nniques,” IEEE Trans. Software Eng. , vol. 39, pp. 613–637, 2013.\n[123] E. Wittern et al. , “Opportunities in Software Engineering Research\nfor Web API Consumption,” in WAPI@ICSE 2017 , 2017, pp. 7–10.\n[124] ——, “API Harmony: Graph-based search and selection of APIs in\nthe cloud,” IBMRD , vol. 60, 2016.\n[125] A. Rabkin et al. , “Aggregation and degradation in JetStream:\nStreaming analytics in the wide area,” in USENIX NSDI , 2014.\n[126] B. Rochwerger et al. , “The Reservoir model and architecture for\nopen federated cloud computing,” IBMRD , vol. 53, no. 4, 2009.\n[127] R. H. Campbell et al. , “Open Cirrus Cloud Computing Testbed:\nFederated Data Centers for Open Source Systems and Services\nResearch.” HotCloud , vol. 9, no. 1–1, 2009.\n[128] W. E. Allcock et al. , “The Globus Striped GridFTP Framework and\nServer,” in SC2005 , 2005, p. 54.\n[129] M. X. Makkes et al. , “Pˆ2-SWAN: Real-Time Privacy Preserving\nComputation for IoT Ecosystems,” in ICFEC , 2017, pp. 1–10.\n[130] A. Iosup et al. , “The OpenDC Vision: Towards Collaborative Data-\ncenter Simulation and Exploration for Everybody,” in ISPDC , 2017.\n[131] Iosup et al. , “DGSim: Comparing Grid Resource Management Ar-\nchitectures through Trace-Based Simulation,” in Euro-Par , 2008.\n[132] Calheiros et al. , “CloudSim,” Softw., Pract. Exper. , vol. 41, 2011.\n[133] Kohne et al. , “FederatedCloudSim: A SLA-aware Federated Cloud\nSimulation Framework,” in CCB , 2014, pp. 3:1–3:5.\n[134] R. Math ´aet al. , “Simulation of a workﬂow execution as a real cloud\nby adding noise,” Sim. Model. Practice and Theory , vol. 79, 2017.\n[135] R. D. Peng, “Reproducible research in computational science,”\nScience , vol. 334, pp. 1226–1227, 2011.\n[136] D. C. Ince et al. , “The case for open computer programs,” Nature ,\nvol. 482, no. 7386, 2012.[137] B. Howe, “Virtual appliances, cloud computing, and reproducible\nresearch,” Comput. in Sci. & Eng. , vol. 14, pp. 36–41, 2012.\n[138] J. Vitek et al. , “R3: Repeatability, reproducibility and rigor,” ACM\nSIGPLAN Notices , vol. 47, no. 4a, 2012.\n[139] A. Iosup et al. , “The grid workloads archive,” FGCS , 2008.\n[140] D. Remenska et al. , “Using model checking to analyze the system\nbehavior of the LHC production grid,” FGCS , vol. 29, no. 8, 2013.\n[141] O. Erling et al. , “The LDBC Social Network Benchmark: Interactive\nWorkload,” in SIGMOD , 2015, pp. 619–630.\n[142] D. Remenska et al. , “Property speciﬁcation made easy: Harnessing\nthe power of model checking in UML designs,” in FORTE , 2014.\n[143] S. Vijzelaar et al. , “Bonsai: Cutting Models Down to Size,” in PSI,\n2014, pp. 361–375.\n[144] A. Iosup et al. , “IaaS cloud benchmarking: Approaches, challenges,\nand experience,” in Cloud Computing for Data-Intensive Applica-\ntions , 2014, pp. 83–104.\n[145] Iosup et al. , “On the Performance Variability of Production Cloud\nServices,” in CCGrid , 2011, pp. 104–113.\n[146] D. Boyd et al. , “The networked nature of algorithmic discrimina-\ntion,” Data and discrimination: Collected essays , 2014.\n[147] The Economist, “The techlash against Amazon, Facebook and\nGoogle,” Jan 20–26, 2018, p.21–26, https://www.economist.com/\nnews/brieﬁng/21735026-which-antitrust-remedies-welcome-which-\nﬁght-techlash-against-amazon-facebook-and, Jan 2018.\n[148] L. Introna et al. , “Picturing algorithmic surveillance: the politics of\nfacial recognition systems,” surveillance and Society , vol. 2, 2004.\n[149] M. R. Canﬁeld, Field Notes on Science and Nature . Harvard\nUniversity Press, 2011.\n[150] M. Y . Vardi, “Technology has social consequences,” Commun. ACM ,\nvol. 54, p. 5, 2011.\n[151] K. Mehlhorn et al. , “Publication Culture in Computing Research\n(Dagstuhl Perspectives Workshop 12452),” Dagstuhl Reports , 2012.\n[152] L. Fortnow, “Viewpoint - Time for computer science to grow up,”\nCommun. ACM , vol. 52, pp. 33–35, 2009.\n[153] M. Satyanarayanan, “The Emergence of Edge Computing,” IEEE\nComputer , vol. 50, pp. 30–39, 2017.\n[154] G. Ananthanarayanan et al. , “Real-time video analytics: The killer\napp for edge computing,” IEEE Computer , vol. 50, 2017.\n[155] Schopf, “Ten Actions When Grid Scheduling,” in Grid Resource\nManagement: State of the Art and Future Trends , 2004, pp. 15–23.\n[156] M. Hiltzik, Big Science: Ernest Lawrence and the Invention that\nLaunched the Military-industrial Complex . Simon&Schuster, 2016.\n[157] L. Evans, “The large hadron collider,” New Journal of Physics ,\nvol. 9, no. 9, 2007.\n[158] C. Carilli et al. , “Science with the Square Kilometer Array: mo-\ntivation, key science projects, standards and assumptions,” arXiv\npreprint astro-ph/0409274 , 2004.\n[159] W. Cirne et al. , “Labs of the World, Unite!!!” J. Grid Comput. ,\nvol. 4, pp. 225–246, 2006.\n[160] V . Marx, “Biology: The big challenges of big data,” 2013.\n[161] Altschul et al. , “Basic local alignment search tool,” Journal of\nmolecular biology , vol. 215, 1990.\n[162] M. G. Wagner, “On the Scientiﬁc Relevance of eSports.” in IC,\n2006, pp. 437–442.\n[163] R. Elder. (2017) Esports is far from fulﬁlling\nits revenue potential, but growing steadily. [Online].\nAvailable: http://www.businessinsider.com/esports-is-nascent-but-\ngrowing-steadily-2017-5?international=true\n\n[164] P. Warman. (2017) Esports revenues by 2020. [On-\nline]. Available: newzoo.com/insights/articles/esports-revenues-\nwill-reach-696-million-in-2017/\n[165] Hendrikx et al. , “Procedural content generation for games: A sur-\nvey,” TOMCCAP , vol. 9, pp. 1:1–1:22, 2013.\n[166] A. Iosup, “POGGI: generating puzzle instances for online games on\ngrid infrastructures,” CCPE , vol. 23, no. 2, 2011.\n[167] S. Shen et al. , “Massivizing Multi-player Online Games on Clouds,”\ninCCGrid , 2013, pp. 152–155.\n[168] Nae et al. , “Dynamic Resource Provisioning in Massively Multi-\nplayer Online Games,” TPDS , vol. 22, pp. 380–395, 2011.\n[169] ——, “A new business model for massively multiplayer online\ngames,” in ICPE , 2011, pp. 271–282.\n[170] Shen et al. , “Scheduling Jobs in the Cloud Using On-Demand and\nReserved Instances,” in Euro-Par , 2013, pp. 242–254.\n[171] ——, “An Availability-on-Demand Mechanism for Datacenters,” in\nCCGrid , 2015, pp. 495–504.\n[172] J. P. Albrecht, “How the GDPR Will Change the World,” Eur. Data\nProt. L. Rev. , vol. 2, no. 287, 2016.\n[173] S. et al., “Solving the bank with Rebel: on the design of the\nRebel speciﬁcation language and its application inside a bank,” in\nITSLE@SLASH 2016 , 2016, pp. 13–20.\n[174] Korte et al. , “e-skills for jobs in europe: Measuring progress and\nmoving ahead,” European Commission report., Sep 2014.[175] S. J. Stolfo et al. , “Fog computing: Mitigating insider data theft\nattacks in the cloud,” in SPW , 2012, pp. 125–128.\n[176] I. Ion et al. , “Home is safer than the cloud!: privacy concerns for\nconsumer cloud storage,” in Proceedings of the Seventh Symposium\non Usable Privacy and Security , 2011, p. 13.\n[177] E. van Eyk et al. , “The SPEC cloud group’s research vision on FaaS\nand serverless architectures,” in (WoSC , 2017, pp. 1–4.\n[178] Y . Low et al. , “Distributed GraphLab: A Framework for Machine\nLearning in the Cloud,” PVLDB , vol. 5, pp. 716–727, 2012.\n[179] A. Jain et al. , “Structural-RNN: Deep Learning on Spatio-Temporal\nGraphs,” in CVPR , 2016, pp. 5308–17.\n[180] P. A. Szekely et al. , “Building and Using a Knowledge Graph to\nCombat Human Trafﬁcking,” in ISWC , 2015, pp. 205–221.\n[181] M. Koubarakis et al. , “Real-time wildﬁre monitoring using scientiﬁc\ndatabase and linked data technologies,” in EDBT , 2013, pp. 649–660.\n[182] A. Fornito et al. , “Graph analysis of the human connectome:\nPromise, progress, and pitfalls,” NeuroImage , vol. 80, 2013.\n[183] A. L. Hopkins, “Network pharmacology: the next paradigm in drug\ndiscovery,” Nature chemical biology , vol. 4, no. 11, 2008.\n[184] T. Hey et al. ,The fourth paradigm: data-intensive scientiﬁc discov-\nery. Microsoft research Redmond, WA, 2009, vol. 1.",
  "textLength": 184191
}