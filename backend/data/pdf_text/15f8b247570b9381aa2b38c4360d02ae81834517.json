{
  "paperId": "15f8b247570b9381aa2b38c4360d02ae81834517",
  "title": "Sorting with Predictions",
  "pdfPath": "15f8b247570b9381aa2b38c4360d02ae81834517.pdf",
  "text": "Sorting with Predictions\nXingjian Bai\nDepartment of Computer Science\nUniversity of Oxford, UK\nxingjian.bai@sjc.ox.ac.ukChristian Coester\nDepartment of Computer Science\nUniversity of Oxford, UK\nchristian.coester@cs.ox.ac.uk\nNovember 3, 2023\nAbstract\nWe explore the fundamental problem of sorting through the lens of learning-augmented algorithms,\nwhere algorithms can leverage possibly erroneous predictions to improve their efficiency. We consider\ntwo different settings: In the first setting, each item is provided a prediction of its position in the\nsorted list. In the second setting, we assume there is a “quick-and-dirty” way of comparing items,\nin addition to slow-and-exact comparisons. For both settings, we design new and simple algorithms\nusing only O(P\nilogηi)exact comparisons, where ηiis a suitably defined prediction error for the ith\nelement. In particular, as the quality of predictions deteriorates, the number of comparisons degrades\nsmoothly from O(n)toO(nlogn). We prove that this comparison complexity is theoretically optimal\nwith respect to the examined error measures. An experimental evaluation against existing adaptive\nand non-adaptive sorting algorithms demonstrates the potential of applying learning-augmented\nalgorithms in sorting tasks.\n1 Introduction\nSorting is one of the most basic algorithmic problems, commonly featured as one of the initial topics in\ncomputer science education, and with a vast array of applications spanning various domains. In recent\nyears, the emerging field of algorithms with predictions (Lykouris and Vassilvitskii, 2021, Mitzenmacher\nand Vassilvitskii, 2022), also known as learning-augmented algorithms, has opened up new possibilities\nfor algorithmic improvement, where algorithms aim to leverage predictions (possibly generated through\nmachine learning, or otherwise) to improve their performance. However, the classical sorting problem\nwith predictions, along with the discussion of different types of predictors for sorting, appears to have\nbeen largely overlooked by this recent movement. This paper explores the problem of sorting through the\nlens of algorithms with predictions in two settings, aiming to overcome the classical Ω(nlogn)barrier\nwith the aid of various types of predictors.\nThe first setting involves each item having a prediction of its position in the sorted list. This type\nof predictor is commonly found in real-world scenarios. For instance, empirical estimations of element\ndistribution can generate positional predictions. Another example is that a fixed set of items has their\nranking evolve over time, with minor changes at each timestep. Here, an outdated ranking can serve as a\nnatural prediction for the current ranking. As re-evaluating the true relation between items can be costly,\nthe provided positional predictions offer useful information. The positional prediction setting is closely\nrelated to adaptive sorting of inputs with existing presortedness (Estivill-Castro and Wood, 1992), but\nwe consider different measures of error on the predictor, resulting in algorithms with a more fine-grained\ncomplexity.\nIn the second setting, a “dirty” comparison function is provided to assist sorting. In biological\nexperiments, for instance, some “indicating factors” might be used to approximately compare two\nmolecules or drugs. Despite potential errors due to the oversight of minor factors, these comparisons can\nstill offer preliminary insights into the properties of the subjects. More broadly, in experimental science,\nresearchers often carry out costly experiments to compare subject behaviours. By utilizing a proficient\nsorting algorithm that capitalizes on dirty comparisons, the need for costly experiments can be reduced\nand substituted by less expensive, albeit noisier, experiments.\nWe propose sorting algorithms to leverage either type of predictor. In the positional prediction\nsetting, we design two deterministic algorithms with different complexity bounds, while in the dirty\n1arXiv:2311.00749v1  [cs.DS]  1 Nov 2023\n\ncomparisons setting, we develop a randomized algorithm. In all settings, we provide bounds of the form\nO(Pn\ni=1log(ηi+ 2))on the number of exact comparisons, for different notions of element-wise prediction\nerrors ηi∈[0, n]. In particular, all three proposed algorithms only require O(n)exact comparisons if\npredictions are accurate ( consistency ), never use more than O(nlogn)comparison regardless of prediction\nquality ( robustness ), and their performance degrades slowly as a function of prediction error ( smoothness ).\nMoreover, we show that all algorithms have optimal comparison complexity with respect to the error\nmeasures examined.\nFinally, through experiments on both synthetic and real-world data, we evaluate the proposed\nalgorithms against existing (adaptive and non-adaptive) algorithms. Results demonstrate their superiority\nover the baselines in multiple scenarios.\n1.1 Preliminaries\nLetA=⟨a1, . . . , a n⟩be an array of nitems, equipped with a strict linear order <. Let p:[n]→[n]be\nthe permutation that maps each index ito the position of aiin the sorted list; that is, ap−1(1)< ap−1(2)<\n···< ap−1(n). We consider two settings of sorting with predictions.\nSorting with Positional Predictions. Insorting with positional predictions , the algorithm receives\nfor each item aia prediction ˆp(i)of its position p(i)in the sorted list. We allow ˆpto be any function\n[n]→[n], which need not be a permutation (i.e., it is possible that ˆp(i) = ˆp(j)for some i̸=j).\nPositional predictions can be generated by models that roughly sort the items, e.g. focusing on major\nfactors while neglecting minor ones. Or they can stem from past item rankings, while the properties of\nthe items evolve over time. In such cases, the objective is to obtain the latest ranking of items.\nThe error of a positional prediction can be naturally quantified by the displacement of each element’s\nprediction; that is, the absolute difference of the predicted ranking and the true ranking. We define the\ndisplacement error of item aias\nη∆\ni:=|ˆp(i)−p(i)|.\nThe following notion of one-sided error provides an alternative perspective to evaluate the complexity\nof algorithms with positional predictions. We denote the left-error and right-error of item aias\nηl\ni:=|{j∈[n]: ˆp(j)≤ˆp(i)∧p(j)> p(i)}|\nηr\ni:=|{j∈[n]: ˆp(j)≥ˆp(i)∧p(j)< p(i)}|.\nIn certain contexts, it may be impossible to obtain a predictor with small displacement error, but\npossible to obtain one with a small one-sided error. By developing algorithms for this setting, we expand\nthe space of problems where sorting algorithms with predictions can be applied.\nSorting with Dirty and Clean Comparisons. The other setting we consider involves a predictor\nthat estimates which of two elements is larger without conducting a proper comparison, providing a faster\nbut possibly inaccurate result. This type of predictor is applicable in scenarios where exact comparisons\nare costly but a rough estimate of the comparison outcome can be obtained more easily.\nFormally, in sorting with dirty comparisons , the algorithm has access to a complete, asymmetric\nrelation b<on the items, while still also having access to the exact comparisons <. That is, for any two\ndistinct items aiandaj, either aib< ajorajb< ai. We think of b<as an unreliable, but much faster to\nevaluate prediction of <. We also refer to the (fast) comparisons according to b<asdirtywhereas the\n(slow) comparisons according to <areclean. We emphasize that the relation b<need not be transitive , so\nit is not necessarily a linear order. Instead, b<induces a tournament graph on the items of A, containing\na directed edge (ai, aj)if and only if aib< aj, and in many applications, we expect this graph to have\ncycles.\nWe denote by ηithe number of incorrect dirty comparisons involving ai, that is,\nηi:=\f\f{j∈[n]: (ai< aj)̸= (aib< aj)}\f\f.\n1.2 Main Results\nOur main result for the dirty comparison setting is given by the following theorem:\n2\n\nTheorem 1.1. Augmented with dirty comparisons, there is a randomized algorithm that sorts an array\nwithin O(nlogn)running time, O(nlogn)queries to dirty comparisons, and O(Pn\ni=1log (ηi+ 2)) clean\ncomparisons in expectation.\nBy the classical lower bound, the total number of dirty+clean comparisons must be at least Ω(nlogn)\nregardless of prediction quality, but for sufficiently good predictions the theorem allows to replace most\nclean comparisons by dirty ones.\nThe following theorem generalizes the previous one to the case that there are kdifferent dirty\ncomparison operators:\nTheorem 1.2. Augmented with k≤2O(n/logn)dirty comparison predictors, where the error of the\npth predictor is denoted by ηp, there is a randomized algorithm that sorts an array with at most\nO(min pPn\ni=1log(ηp\ni+ 2)) clean comparisons.\nIn other words, the number of clean comparisons is as good as if we knew in advance which of the k\npredictors is best. The bound on kis almost tight, since already k≥2nlognwould mean there could be\none predictor for each of the n!possible sorting outcomes, which would render them useless.\nThe next two theorems capture our algorithms for the positional prediction setting:\nTheorem 1.3. Augmented with a positional predictor, there is a deterministic algorithm that sorts an\narray within O\u0000Pn\ni=1log\u0000\nη∆\ni+ 2\u0001\u0001\nrunning time and comparisons.\nTheorem 1.4. Augmented with a positional predictor, there is a deterministic algorithm that sorts an\narray within O\u0000Pn\ni=1log\u0000\nmin\b\nηl\ni, ηr\ni\t\n+ 2\u0001\u0001\ncomparisons.\nWe remark that there exist instances where the bound of Theorem 1.3 is stronger than that of\nTheorem 1.4 and vice versa.1\nThe following lower bounds show tightness of the aforementioned upper bounds.\nTheorem 1.5. Augmented with dirty comparisons, no sorting algorithm uses o(Pn\ni=1log (ηi+ 2)) clean\ncomparisons. Augmented with a positional predictor, no sorting algorithm uses o\u0000Pn\ni=1log\u0000\nη∆\ni+ 2\u0001\u0001\nor\no\u0000Pn\ni=1log(min\b\nηl\ni, ηr\ni\t\n+ 2)\u0001\ncomparisons.\nBounds in Terms of Global Error. One may wonder how the above bounds translate to a global\nerror measure such as the number of item pairs where the larger one is incorrectly predicted to be no larger\nthan the smaller one. Writing Dfor this error measure, in the dirty comparison setting we simply have\nD=1\n2P\niηi, and in the positional prediction setting we have D≥1\n2P\niη∆\niby (Rohatgi, 2020, Lemma 11).\nThus, concavity of logarithm and Jensen’s inequality yield an upper bound of O\u0000\nnlog\u0000D\nn+ 2\u0001\u0001\nfor both\nsettings. This bound is tight2as a function of Dand corresponds to the optimal complexity of adaptive\nsorting as a function of the number of inversions (Mannila, 1985).\nHowever, our guarantees in terms of element-wise error are strictly stronger whenever Jensen’s\ninequality is not tight, i.e., when the ηiare non-uniform. Furthermore, it is reasonable to expect\npredictors to exhibit varying levels of error for different items, especially when the error originates from\nelement-wise noise.\n1.3 Related Works\nAlgorithms with Predictions. Our study aligns with the broader field of learning-augmented\nalgorithms, also known as algorithms with predictions. The majority of research has focused on classical\nonline problems such caching (Lykouris and Vassilvitskii, 2021, Rohatgi, 2020, Wei, 2020, Bansal et al.,\n2022), rent-or-buy problems (Purohit et al., 2018, Gollapudi and Panigrahi, 2019, Angelopoulos et al.,\n2020, Wang et al., 2020, Antoniadis et al., 2021), scheduling (Purohit et al., 2018, Lattanzi et al., 2020,\nMitzenmacher, 2020, Azar et al., 2021, 2022, Lindermayr and Megow, 2022) and many others. In\ncomparison, research on learning-augmented algorithms to improve runnning time for offline problems is\nrelatively sparser, but examples include matching (Dinitz et al., 2021, Sakaue and Oki, 2022), clustering\n(Ergun et al., 2022), and graph algorithms (Chen et al., 2022, Davies et al., 2023). Motivated by the work\n1If predictions are correct except that the positions of the√nsmallest and√nlargest items are swapped, thenP\nilog(η∆\ni+ 2) = Θ( n), butP\nilog(min{ηl\ni, ηr\ni}+ 2)) = Θ( nlogn). Conversely, if ˆp(i) = p(i) +√nmod n, thenP\nilog(η∆\ni+ 2) = Θ( nlogn), butP\nilog(min {ηl\ni, ηr\ni}+ 2)) = Θ( n).\n2Indeed, our proof of Theorem 1.5 constructs a family of instances where each ηiis bounded by the same quantity, so\nJensen’s inequality is tight for these instances.\n3\n\nof (Kraska et al., 2018), (Lykouris and Vassilvitskii, 2021) describes a simple method to speed up binary\nsearch with predictions, which has been inspirational for our work. Learning-augmented algorithms have\nalso been applied to data structures such as binary search trees (Lin et al., 2022, Cao et al., 2023), and\nempirical works demonstrate the benefits of ML-augmentation for index structures (Kraska et al., 2018)\nand database systems (Kraska et al., 2019). There has also been increasing interest in settings where\nalgorithms have access to multiple predictors (Gollapudi and Panigrahi, 2019, Wang et al., 2020, Bhaskara\net al., 2020, Almanza et al., 2021, Emek et al., 2021, Dinitz et al., 2022, Anand et al., 2022, Antoniadis\net al., 2023).\nRelated to sorting, Lu et al. (2021) studied learning-augmented generalized sorting, a variant of sorting\nwhere some comparisons are allowed while others are forbidden. The predictions they consider are similar\nto our dirty comparisons. They proposed two algorithms with comparison complexities O(nlogn+w)\nandO(nw), where wis the total number of incorrect dirty comparisons. In the classical (non-generalized)\nsetting with all comparisons allowed, only the second bound theoretically improves upon O(nlogn), but\nthe dependence on the error is exponentially worse than for our algorithms; even with a single incorrect\ndirty comparison per item, the O(nw)bound becomes O(n2), whereas ours is O(n). A recent work of\nErlebach et al. (2023) studies sorting under explorable uncertainty with predictions, where initially only\nan interval around the value of each item is known, the exact values can be queried, a prediction of these\nvalues is given, and the goal is to minimize the number of queries needed to sort the list.\nDeep Learning-Based Sorting. The thriving development of deep learning has inspired research\ninto new sorting paradigms. A recent study by DeepMind (Mankowitz et al., 2023) recast sorting as a\nsingle-player game, where they trained agents to play effectively. This leads to the discovery of faster\nsorting routines for short sequences. Kristo et al. (2020) proposed a sorting algorithm that uses a learning\ncomponent to improve the empirical performance of sorting numerical values. Their algorithm tries to\napproximate the empirical CDF of the input by applying ML techniques to a small subset of the input.\nThe setting is very different from ours in several ways: If inputs are non-numeric (and no monotonous\nmapping to numbers is known), then one has to rely on a comparison function, and the approach of Kristo\net al. (2020) would not be well-defined, whereas our algorithms can sort arbitrary data types. On the\nother hand, the input in (Kristo et al., 2020) is only the list of items without any additional predictions.\nNote that predictions (or other assumptions) are necessary to beat the entropic Ω(nlogn)lower bound.3\nNoisy Sorting. Noisy sorting contemplates scenarios where comparison results may be incorrect. This\nmodel is useful to simulate potential faults in large systems. Two noisy sorting settings have primarily\nbeen considered: In independent noisy setting, each query’s result is independently flipped with probability\np∈(0,1\n2). Recently, Gu and Xu (2023) provided optimal bounds on the number of queries to sort n\nelements with high probability. Recurrent noisy setting (Braverman and Mossel, 2008) further assumes\nany repeated comparisons will yield consistent results. Geissmann et al. (2019) present an optimal\nalgorithm that guarantees O(nlogn)time, O(logn)maximum dislocation, and O(n)total dislocation\nwith high probability. While the recurrent noisy setting is closely related to dirty comparisons, studies\nin that field focus primarily on approximate sorting; to the best of our knowledge, no exactsorting\nalgorithms that use both dirty and clean comparisons have been studied.\nAdaptive Sorting. Adaptive sorting algorithms take advantage of various types of existing order\nwithin the input, thus reducing time complexity for partially sorted data. Notable examples of adaptive\nsorting algorithms include TimSort (Peters, 2002), which is the standard sorting algorithm in a variety\nof programming languages, Cook-Kim division (Cook and Kim, 1980), and Powersort (Munro and\nWild, 2018), which recently replaced TimSort in Python’s standard library. We refer to the survey of\nEstivill-Castro and Wood (1992) for a broader overview. The concept of pre-sortedness is closely related\nto positional predictions; however, without the motivation from predictors, the complexity bound on\nadaptive sorting algorithms were often considered under error measures on the entire array, instead of on\neach element. In contrast, our error measure is element-wise, allowing algorithms with stronger complexity\nbounds.\n3The theoretical guarantee of their algorithm is O(n2), although they observe much better empirical performance. Indeed,\nthis makes sense for numerical inputs drawn from a sufficiently nice distribution, since then one can extrapolate from a\nsmall part of the input to the rest.\n4\n\n2 Sorting with Dirty Comparisons\nGiven a dirty predictor b<, our goal is to sort Awith the least possible number of clean comparisons.\nNote that, if b<is accurate, Acan be sorted using only O(n)clean comparisons and O(nlogn)dirty\ncomparisons. This could be achieved, for example, by performing Merge Sort with dirty comparisons\nand then validating the result through clean comparisons between adjacent elements. This observation\nmotivates us to consider that not all O(n2)dirty comparisons are necessary, and we should devise an\nalgorithm minimizing the number of both clean and dirty comparisons.\nWe propose a randomized algorithm that sorts Awith expected O(Plog(2 +ηi))clean comparisons,\nexpected O(nlogn)dirty comparisons, and expected O(nlogn)running time. The key idea consists of\nthree parts: 1) Sequentially insert each element of Ainto a binary search tree, following random order;\n2) Guide each insertion primarily with dirty comparisons, while verifying the correctness of it using a\nminimal number of clean comparisons. 3) Correct the mistake induced by the dirty insertion, ensuring\nthat the clean comparisons needed for correction is O(logηi)in expectation.\nWe describe the algorithm in Section 2.1 and prove its performance guarantees in Section 2.2. In\nSections 2.3 and 2.4, we introduce two variants of the dirty comparisons setting. The first one assumes\nthat dirty comparisons are probabilistic; the second one discusses the setting where multiple predictors\nare available. We briefly discuss the extension of our algorithms and results in these new settings.\n2.1 Algorithm\nWe now describe the sorting algorithm with dirty comparisons in detail (Algorithm 1). We initialize Bas\nan empty binary search tree (BST). For any vertex vof the tree, we denote by left(v)andright (v)its left\nand right children, and by root(B)the root of B. Slightly abusing notation, we write valso for the item\nstored at vertex v. If any of these vertices is missing, the respective variable has value nil.\nAlgorithm 1: Sorting with dirty and clean comparisons\nInput: A=⟨a1, . . . , a n⟩, dirty comparator b<, clean comparator <\n1B←empty binary tree\n2fori∈[n]in uniformly random order do\n3 (L1, C1, R1)←(−∞,root( B),∞)\n4 t←1\n5while Ct̸=nildo ▷Dirty search\n6ifaib< Ctthen (Lt+1, Ct+1, Rt+1)←(Lt,left(Ct), Ct)\n7else (Lt+1, Ct+1, Rt+1)←(Ct,right( Ct), Rt)\n8 t←t+ 1\n9 C←Ct∗, where t∗≤tis maximal s.t. Lt∗< ai< R t∗ ▷Verification\n10while C̸=nildo ▷Clean search\n11 ifai< Cthen C←left(C)\n12 else C←right( C)\n13Insert aiatC\n14returninorder traversal of B\nWithin each iteration of the for-loop starting in line 2, we select one item aiofAuniformly randomly\nfrom the items that have not been processed yet. Then, we insert this item aiintoB, while maintaining\nthe invariant that Bremains a BST with respect to clean comparisons <.\nInserting item aiintoBrequires three phases, as illustrated in Figure 2.1. The first phase involves\nperforming a search for the insertion position using dirty comparisons b<, keeping track of the search path.\nHere, we denote by Ctthetth vertex on this path, and by LtandRtthe lower and upper bounds on\nitems that can be inserted in the subtree rooted at Ctwithout violating the BST property with respect\nto<. Correctness of the choice of LtandRtfollows from the fact that Bwas a BST with respect to <\nbefore the current insertion. This dirty procedure stops when the search path reaches a nil-leaf, regarded\nas the predicted position for ai’s insertion. However, since we used dirty comparisons to trace the path,\naimight violate one of the boundary conditions Lt< aiorai< R tat some recursion step t. We call a\nrecursion step tvalidforaiifLt< ai< R t. Then, we enter the verification phase in line 9. We traverse\nthe dirty search path in reverse order to locate the last valid step t∗. A naive method to do this (which\n5\n\nFigure 2.1: The insertion process in dirty comparison sorting.\nis sufficient for our asymptotic guarantees) is to repeatedly decrease tby1until tis valid; we discuss an\nalternate, more efficient method in Remark 2.6, which yields a better constant factor.\nThe final phase involves performing a clean search starting from Ct∗, to determine the correct insertion\nposition for ai. After inserting aiinto that position, Bremains a BST with respect to <. Once all items\nofAare inserted into B, we can obtain the sorted order through the inorder traversal of B.\n2.2 Complexity Analysis\nThe goal of this section is to prove Theorem 1.1. We start with several lemmas on the expected behavior\nof dirty search and clean search, focusing on a single iteration of the for-loop corresponding to item ai.\nRoughly, the idea is to show that the dirty search path has length O(logn), whereas the verification path\nand the clean search path only have depth O(logηi).\nFor an iteration of the dirty and clean search while-loops, respectively, we call the vertex stored as Ct\nresp. Cat the start of the iteration the pivot; the subtree rooted at the pivot is referred to as the active\nsubtree. The sizeof a subtree is the number of non- nilvertices it contains. For the dirty search, let st\ndenote the size of the active subtree at iteration t, and Tdenote the number of recursion steps needed.\nDenote by mdirty\nsandmclean\nsthe number of iterations of the dirty and clean search where the size of the\nactive subtree lies in (s\n2, s]. In particular, mdirty\ns=|{t:st∈(s\n2, s]}|.\nLemma 2.1. E[mdirty\ns] =O(1)andE[mclean\ns] =O(1)for all s. Moreover, E[mdirty\ns|st∗=s′] =O(1)and\nE[mclean\ns|st∗=s′] =O(1)for all s < s′withP(st∗=s′)>0.\nProof.We employ a percentile argument. Consider the first step of either while-loop where the active\nsubtree has at most svertices, and let Vbe the set of these vertices. Note that the pivot in this step is\nthe first element in Vinserted into the tree. Conditioned on the vertices of the active subtree being V,\nelements of Vare equally likely to be the pivot, since their insertion order is uniformly random. This is\ntrue even when conditioned on st∗=s′, since reordering the elements within the set Vdoes not change\nthe value of st∗, which is determined at higher vertices of the tree. Thus, we have at least a 50%chance\nthat the pivot lies between the 25th percentile and the 75th percentile, in which case both children\nsubtrees contain at most3\n4|V|vertices each. Hence, the size of active subtree shrinks by a factor of3\n4\nafter at most 2steps in expectation, and it shrinks to size smaller thans\n2after at most O(1)steps in\nexpectation.\nBased on this lemma, we are able to characterize the expected length of dirty search, clean search,\nand dirty search after time t∗.\nLemma 2.2. A dirty search takes O(logn)steps in expectation, i.e. E[T] =O(logn).\nProof.Ineachdirtysearch, theinitiallargestsubtreehassizeatmost n. WecanapplyLemma2.1repeated\nons=n,⌊n\n2⌋, . . . , 1. Bylinearityofexpectation, wederivethatadirtysearchtakes ⌈logn⌉·O(1) = O(logn)\nsteps in expectation.\nLemma 2.3. A clean search takes O(E[log(st∗+ 1)]) steps in expectation; in dirty search after reaching\nt∗, there are O(E[log(st∗+ 1)]) steps in expectation, i.e. E[T−t∗] =O(E[log(st∗+ 1)]) .\n6\n\nProof.Assume st∗=s′. Then, in dirty search after t∗, the initial largest subtree has size s′, and the\nrest of the active subtrees all have size at most s′−1. By applying the second part of Lemma 2.1 on\ns=s′−1,⌊s′−1\n2⌋, . . . , 1and summing, we obtain E[T−t∗|st∗=s′] =O(log(s′+ 1)). Thus,\nE[T−t∗] =X\ns′E[T−t∗|st∗=s′]·P[st∗=s′] =O(E[log (st∗+ 1)])\nThe bound on clean search follows in the same way.\nIn order to relate E[log (st∗+ 1)]to the prediction error ηi, the next lemma first characterizes the\nprobability of a given time step tbeing t∗as a function of the subtree size st.\nLemma 2.4. For any tandk,P\u0002\nt=t∗\f\fst∈\u0000\n2k−1,2k\u0003\u0003\n≤ηi/2k−1.\nProof.Recall that t∗is the last valid time step. A shift from a valid time step to an invalid time step\noccurs only if the dirty comparison between the pivot and aiis wrong. Among all stpotential pivots,\nat most ηican have mistaken dirty comparisons with ai, and they are equally likely to be the pivot\n(depending on which of them was inserted first). Hence, given st∈\u0000\n2k−1,2k\u0003\n, the probability of a pivot\nwith mistaken comparison is at most ηi/st≤ηi/2k−1.\nBasedonLemma2.4, wepresentthecentralclaimbridgingpredictionerrorwithcomparisoncomplexity.\nLemma 2.5. E[log (st∗)] =O(log ( ηi+ 1)) .\nProof.We have\nP\u0002\nst∗∈(2k−1,2k]\u0003\n=X\ntP\u0002\nt=t∗andst∈(2k−1,2k]\u0003\n=X\ntP\u0002\nst∈(2k−1,2k]\u0003\n·P\u0002\nt=t∗\f\fst∈(2k−1,2k]\u0003\n≤Eh\nmdirty\n2ki\n·ηi/2k−1\n≤ηi·O(2−k),\nwhere the first inequality uses Lemma 2.4 and the second is due to Lemma 2.1. Thus,\nE[log(st∗)]≤log(ηi+ 1) +∞X\nk=⌈log(ηi+1)⌉P\u0002\nst∗∈(2k−1,2k]\u0003\n·k\n≤log(ηi+ 1) + ηi∞X\nk=⌈log(ηi+1)⌉O\u0000\nk2−k\u0001\n=O(log ( ηi+ 1)) .\nTheorem 1.1 is subsequently deduced.\nProof of Theorem 1.1. Dirty comparisons are only conducted during dirty search, when the recursion\nstep is incremented by one. As per Lemma 2.2, the total number of dirty comparisons is bounded by the\nsum of steps across all insertions, which is n·O(logn).\nIn each verification phase, as we traverse the dirty search in reverse order to locate t∗, at most T−t∗+2\nclean comparisons suffice. In each clean search phase, the expected number of clean comparisons is the\nexpected number of steps in clean search. Therefore, based on Lemma 2.3 and Lemma 2.5, the total\nnumber of clean comparisons performed in both phases is O(Plog (ηi+ 2)).\nAdditionally, the running time is dominated by the number of dirty and clean comparisons.\nRemark 2.6. The algorithm can be implemented such that the number of clean comparisons is at most\nthat of quicksort plus O(nlog log n), regardless of prediction error. Thus, even with terrible predictions\nour algorithm matches the performance of quicksort up to a factor that tends to 1asn→ ∞.\nTo achieve this, we can implement the verification step by decreasing tin geometrically increasing\nstep sizes until a valid thas been found, and then perform a binary search for t∗between the last\ntwo attempted values of t. This reduces the number of clean comparisons in line 9 from O(T−t∗)to\n7\n\nO(log(T−t∗)), which is at most O(log log n)in expectation by Lemma 2.2, and at most O(nlog log n)for\nall verification steps together. The remaining clean comparisons are performed during the clean searches.\nIn the worst case (when all dirty comparisons are incorrect) all clean searches start from the root, and\ntogether they perform exactly the same set of comparisons as quicksort (by a coupling argument between\nthe random choices of the two algorithms: E.g., the root of the search tree corresponds to the initial\nuniformly random pivot of quicksort).\n2.3 Probabilistic Dirty Comparisons\nOur algorithm extends to the case where dirty comparisons are probabilistic. Assume that for each\npair of objects i, j, the dirty comparison between them yields an incorrect result with probability ηij.\nAlgorithm 1 can be directly applied in this setting, achieving the same guarantees by defining ηi=P\njηij.\nThe proof remains unchanged.\nIf repeatedly querying the same dirty comparison multiple times yields independent results, the\nnumber of clean comparisons can be further reduced: Let ϵij:=min{ηij,1/2)}. When querying a dirty\ncomparison 2ktimes, the probability that the correct answer fails to secure a majority vote is at most\n(4ϵij(1−ϵij))k: For ηij≥0.5, this bound is trivial. Otherwise, there are 22kstrings of length 2kover\nthe alphabet {correct ,incorrect }, and each string that is at least half incorrect has probability at most\n(ϵij(1−ϵij))k.\nSobyrepeatingeachdirtycomparisonquery 2ktimes, weobtainanalgorithmthatperforms O(knlogn)\ndirty comparisons and O\u0010P\nilog\u0010P\nj(4ϵij(1−ϵij))k\u0011\u0011\nclean comparisons.\n2.4 Multiple Predictors\nWe now discuss the setting where multiple predictors are available and prove Theorem 1.2. Suppose\nwe have kdifferent dirty comparison predictors. Let ηp\nidenote the number of incorrect comparisons by\npredictor pfor item i.\nWe prove Theorem 1.2 by reduction to the problem of “prediction with expert advice”: In this problem,\nthere are kexperts, and each incurs a loss in the range [0,1]per time step. An algorithm must select\nan expert in each round before the losses are revealed and then incurs the loss of the chosen expert.\nAccording to (Freund and Schapire, 1997, Equation(9)), their algorithm Hedgehas an expected loss of\nO(L+ log( k)), where Lis the total loss of the best expert in hindsight.\nIn our case, the experts correspond to the predictors, and time steps correspond to the niterations of\nthe for-loop of Algorithm 1 where an item is inserted into the BST. We define the loss of expert pin\nthe time step where aiought to be inserted by ℓp\ni=log(1 + ˜ηp\ni)/log(n), where ˜ηp\ni≤ηp\niis the number of\nincorrect comparisons of predictor pbetween aiand the items already in the BST at this time. Note that\nthe value of ˜ηp\nican be determined at the end of the time step (once aiis correctly placed in the BST)\nwithout any additional clean comparisons; the division by lognensures that ℓp\ni∈[0,1]as required.\nThe algorithm for multiple predictors proceeds as Algorithm 1, querying for dirty comparisons at a\ngiven time step the predictor pcorresponding to the expert chosen by Hedgeat that time step. Recall\nfrom the analysis in Section 2.2 that the expected number of clean comparisons in this time step is\nthen O(log(1 + ˜ηp\ni)), which is an O(logn)factor larger than the loss suffered by Hedge. Consequently,\nby the O(L+logk)bound on the cost of Hedge, the total expected number of clean comparisons is\nO(minpP\nilog(1 +ηp\ni) +log(k)log(n)). Since k≤2O(n/logn), the term log(k)log(n) =O(n)is negligible,\nand Theorem 1.2 follows.\n3 Sorting with Positional Predictions\nIn this section, we propose two algorithms that are capable of leveraging positional predictions. The\nfirst one has complexity bounds in terms of the displacement error measure, and the second one has\ncomplexity bounds in terms of ηlandηr, the one-sided error measures. Each algorithm is effective in\nsome tasks, where the given prediction is accurate with respect to its corresponding error measure.\n3.1 Displacement Sort\nWe present a sorting algorithm with positional prediction, whose comparison and time complexity rely\nsolely on the displacement error of the predictor. The algorithm is adapted from Local Insertion Sort\n(Mannila, 1985), an adaptive sorting algorithm proven to be optimal for various measures of presortedness.\n8\n\nWe use a classic data structure called finger tree (Guibas et al., 1977), which is a balanced binary\nsearch tree (BBST) equipped with a pointer (“finger”) pointing towards the last inserted vertex. When a\nnew value vis to be inserted, rather than searching for insertion position from the root, the insertion\nposition is found by moving the finger from the last inserted vertex uto the suitable new position. By\nthe balance property of BBST, the insertion can be performed in O(logd(u, v))amortized time, where\nd(u, v)is the number of vertices in the tree whose value lies in the closed interval from utov.\nAlgorithm 2 details the proposed method. We first bucket sort (in time O(n)) the items in Abased on\ntheir predicted positions, such that we may assume for all i < jthat ˆp(i)≤ˆp(j). Following the rearranged\norder, items in Aare sequentially inserted into an initially empty finger tree T. After all insertions, we\nobtain the exactly sorted array by an inorder traversal of T.\nAlgorithm 2: Sorting with complexity on η∆\ni\nInput: A=⟨a1, . . . , a n⟩, prediction ˆp\n1BucketSort (A,ˆp); ▷Bucket Sort Aaccording to ˆp, so that ˆp(1)≤ˆp(2)≤ ··· ≤ ˆp(n)\n2T←an empty one-finger tree;\n3fori= 1, . . . , ndo\n4Insert aiintoT;\n5returnnodes in Tin sorted order (via inorder traversal);\nProof of Theorem 1.3. We focus on the insertion process of each item aiin Algorithm 2. Let didenote\nthe number of nodes between aiandai−1in an inorder traversal of the tree after inserting ai, including\nthemselves. These nodes must have their correct ranking in the final sorted list between p(i−1)andp(i);\nhence\ndi≤ |p(i)−p(i−1)|+ 1,for all i= 2, . . . , n.\nTherefore, the running time and number of comparisons among all insertions are bounded by\nnX\ni=2O(log(di))≤nX\ni=2O(log(|p(i)−p(i−1)|+ 1))\n≤nX\ni=2O(log(|p(i)−ˆp(i)|+|p(i−1)−ˆp(i−1)|+ ˆp(i)−ˆp(i−1) + 1))\n≤nX\ni=2O(log(3 ·max{η∆\ni+ 1, η∆\ni−1+ 1,ˆp(i)−ˆp(i−1) + 1}))\n≤O(n) +nX\ni=1O(log(η∆\ni+ 1))\n≤O nX\ni=1log(η∆\ni+ 2)!\n.\nThe second inequality is by ˆp(i−1)≤ˆp(i)and the triangle inequality; the third inequality is by\nmonotonicity of logarithm. The penultimate inequality is justified by log(max{x, y, z})≤logx+logy+\nlogzfor all x, y, z ≥1and\nnX\ni=2log(ˆp(i)−ˆp(i−1) + 1) ≤nX\ni=2(ˆp(i)−ˆp(i−1))≤n.\nThis concludes the proof of Theorem 1.3.\n3.2 Double-Hoover Sort\nNow we turn our focus to settings where one-sided errors are small. We first describe a simple algorithm\nwith comparison complexity as a function of either ηlorηr. Following this, we introduce a two-sided\nalgorithm, the Double-Hoover Sort, that has comparison complexity as claimed in Theorem 1.4. Both\nalgorithms begin by bucket sorting Awith respect to the positional prediction in O(n)time, breaking\nties arbitrarily. Subsequently, it can be assumed that Ais rearranged such that ∀i < j, ˆp(i)≤ˆp(j).\n9\n\nA First Approach. A left-sided sorting complexity of O(Pn\ni=1log(2 +ηl\ni))can be easily achieved using\nthe standard technique of learning-augmented binary search, as described in (Lykouris and Vassilvitskii,\n2021, Mitzenmacher and Vassilvitskii, 2022). Specifically, a sorted array Lis maintained, and a1, . . . , a n\nare sequentially inserted into L. During each insertion, we perform a learning-augmented binary search\nstarting from the rightmost position of L, taking O(log(ηl\ni+ 2))comparisons to find the correct insertion\nposition. In total, O(P\nilog(ηl\ni+ 2))comparisons are taken among all insertions. By replacing the array\nLwith an appropriate data structure (e.g., a BBST with a finger that always returns to the rightmost\nelement), one can achieve the same bound also for time complexity. A reversed “right-sided” version of\nthis algorithm achieves complexity O(P\nilog(ηr\ni+ 2)). By simultaneously running the left-sided and right-\nsided algorithms, one can achieve the complexity bound of O\u0000\nmin\bPn\ni=1log\u0000\n2 +ηl\ni),Pn\ni=1log(2 + ηr\ni\u0001\t\u0001\n.\nHowever, moving the minoperator inside the summation requires more elaborate approach.\nDouble-Hoover Sort. The basic idea is that, to utilize a similar insertion scheme to that employed in\nthe one-sided algorithm, we maintain two sorted structures LandRat the same time, and insert each\nitem into one of them, depending on which operation is faster, hereby achieving a complexity bound of\nO(log(min(ηl\ni, ηr\ni) + 2)). Then, a final sorted list is attained by merging LandRin linear time. However,\na significant issue yet to be addressed is how to decide the insertion order of different items.\nConsider two items auandavwith u < v(soˆp(u)≤ˆp(v)by Bucket Sort). In our algorithm, if both\nare to be inserted into L, it is crucial that auis inserted prior to av. Otherwise, the insertion complexity\nofaucould exceed the bound of log(ηl\nu+ 2). Conversely, if both auandavare to be inserted into R, then\navshould be inserted prior to au. Since we cannot predict whether an item will be inserted into LorR,\nformulating an appropriate insertion order that respects the constraints on both sides is impossible.\nWe tackle this issue of insertion order with a strength-based, logn-rounds insertion scheme . Intuitively,\nwe think of LandRas two “hoovers”, with their “suction power” increasing simultaneously over time.\nEach item (as “dust”) is extracted from the array and inserted into one hoover once the suction power\nreaches the required strength: a hoover with suction power δis able to absorb items that can be inserted\ninto it with O(logδ)comparisons. Details are illustrated in Algorithm 3.\nAlgorithm 3: Double-Hoover Sort\nInput: A=⟨a1, . . . , a n⟩, prediction ˆp\n1BucketSort (A,ˆp); ▷Bucket Sort Aaccording to ˆp, so that ˆp(1)≤ˆp(2)≤ ··· ≤ ˆp(n)\n2L, R← ⟨⟩;\n3forδ= 20,21, . . . , 2⌈logn⌉do\n4fori= 1, . . . , n ifaihas not been inserted do\n5 L<i← {aj∈L:j < i};\n6 li\nδ←if|L<i|< δthen −∞elseδth largest item in L<i;\n7ifai> li\nδthen\n8 Insert aiintoLby binary search, starting on interval {x∈L:li\nδ≤x≤li},\n9 where li:= min δ′<δli\nδ′\n10fori=n, . . . , 1ifaihas not been inserted do\n11 R>i← {aj∈R:j > i};\n12 ri\nδ←if|R>i|< δthen ∞elseδth smallest item in R>i;\n13 ifai< ri\nδthen\n14 Insert aiintoRby binary search, starting on interval {x∈R:ri≤x≤ri\nδ},\n15 where ri:= max δ′<δri\nδ′\n16returnmerge(L, R);\nThe sorted structures LandRcan be implemented by arrays, though alternative data structures\ncould provide better time complexity.\nWe conduct insertions in ⌈logn⌉rounds, setting δto be 1,2,4, . . . , 2⌈logn⌉. In each round, we iterate\nover a1, . . . , a nto decide if they should be inserted to Lin the current round. Then, we iterate over\nan, . . . , a 1reversely, to decide if they should be inserted to Rin the current round. Note that if an item\nis inserted into either LorR, it is omitted in later rounds. The insertion process of an item in one round\nis depicted in Figure 3.1.\nTo decide whether aishould be inserted into Lwith strength δ, letli\nδdenote the δth largest item in\nLwith index smaller than i, representing the “boundary value” in this round. If there are less than δ\n10\n\nFigure 3.1: An example of the insertion process in the Double-Hoover sort.\neligible items in L, setli\nδto be−∞. Let lirepresent minδ′<δli\nδ′, the minimum boundary value in previous\nrounds. If li\nδis smaller than ai, we employ binary search to insert aiintoL, starting with the interval\n{x∈L:li\nδ≤x≤li}. Conversely, to decide whether aishould be inserted into R, we adopt a symmetrical\napproach as depicted in lines 11 to 15 of Algorithm 3.\nAfter all insertion rounds, we merge LandRin linear time to obtain the sorted result.\nCorrectness. The correctness of the Double-Hoover Sort arises from the invariance that both Land\nRremain sorted after all insertions. It is sufficient to show that the initial insertion intervals at line 8\nand line 14 always cover the value of ai. At line 8, li\nδ< aiholds trivially by conditioning. Since aiis not\ninserted into Lin any previous round, ai< li\nδ′for all δ′< δ. Since the minimum operation preserves\ninequality, ai< lialso holds. A similar argument can be made for the interval at line 14.\nTo discern the comparison complexity of the proposed algorithm, we prove the following lemma.\nLemma 3.1. Upon the insertion of an item aiintoL, all items in L\\L<iare larger than li. Similarly,\nwhen an item aiis inserted into R, all items in R\\R>iare smaller than ri. As a result, the initial\ninterval at line 8 and line 14 are subsets of L<iandR<i, and hence have sizes no larger than δ.\nProof.Assume aiis inserted into Lin round δ. Consider any aj∈L\\L<iat the time of aiinsertion.\nThen, ajwas inserted into Lpreviously with a smaller insertion strength δ′< δ. By the insert condition,\naj> lj\nδ′.\nSince L<iis a subset of L<j, the δ′th largest item of the latter set must exist and be no smaller than\ntheδ′th largest items of the former set. Then, we obtain\naj> lj\nδ′≥li\nδ′≥li.\nHence, the interval {x∈L|li\nδ≤x≤li}only contains items in L<i. Since there are δitems in L<ithat are\nno smaller than li\nδ, the interval has at most δitems. An analogous proof shows the symmetric property\ninR.\nThen, we can establish the comparison complexity bound of Algorithm 3.\nTheorem 3.2. For each item ai, its insertion process takes O(log(min\b\nηl\ni, ηr\ni\t\n+ 2)) comparisons.\nProof.Each item aigoes through some rejected insertions in earlier rounds, and then gets inserted into L\norRin a certain round. We refer to them as the exploration phase andinsertion phase , respectively, and\nprove that the number of comparisons needed in both phases is bounded by O(log(min\b\nηl\ni, ηr\ni\t\n+ 2)).\nFirst, we claim that each aiis inserted into either LorRprior to or during the round with insertion\nstrength δl\ni:= 2⌈log(ηl\ni+2)⌉. Ifaiis inserted before round δl\ni, the claim trivially holds. The claim also holds\nif at round δl\ni,L<icontains fewer than δl\niitems. In the absence of these conditions,\n\f\f\b\naj∈L<i:aj> ai\t\f\f=|{aj∈L:j < i∧aj> ai}|\n≤ |{j∈[n] : ˆp(j)≤ˆp(i)∧p(j)> p(i)}|\n=ηl\ni< δl\ni.\nHence, in round δl\ni,aimust be larger than the boundary value, which is the δl\nith largest item in L<i.\nConsequently, it will be inserted into Lin round δl\ni.\n11\n\nA similar argument shows that aiwill be inserted prior to or during round 2⌈log(ηr\ni+2)⌉. Combining\nthese two bounds, we find that aimust be inserted prior to or during round δi:= 2⌈log(min {ηl\ni,ηr\ni}+2)⌉.\nHence, the exploration phase only needs O(log(min\b\nηl\ni, ηr\ni\t\n+ 2))comparisons.\nNext, we continue to examine the number of comparisons needed in the insertion phase. Suppose aiis\ninserted into Lin some round δ≤δl\ni. Then, the binary search starts with an interval of size\n|{x∈L:li\nδ≤x≤li}| ≤δ≤δl\ni=O(min\b\nηl\ni, ηr\ni\t\n+ 2).\nThe first inequality is due to Lemma 3.1. Hence, the insertion phase of aiby binary search needs\nO(log(min\b\nηl\ni, ηr\ni\t\n+ 2))comparisons.\nProof of Theorem 1.4. Theorem 1.4 can be obtained from Theorem 3.2, by summing up the number of\ncomparisons in the insertion process of each ai.\n4 Lower Bounds on Comparison Complexity\nWe prove the lower bounds stated in Theorem 1.5.\n4.1 Optimality of Displacement Sort\nIn this section, we show that an exact sorting algorithm, augmented with a positional prediction, cannot\nsort the array with o(P\ni∈[n]log(η∆\ni))comparisons.\nDefinition 4.1. Given n, a positional prediction ˆp, and a real number U, define the size of the U-candidate\nset as\ncand(ˆp, U) :=\f\f\f\f\f{A∈Sn:nX\ni=1log(η∆\ni+ 2)≤U}\f\f\f\f\f,\nwhere Snis the set of permutations of [n](viewed as an array), η∆is calculated accordingly for each A\nagainst ˆp.\nTheorem4.2. Given any function f(U) =o(max ˆp∈Snlog(cand(ˆp, U))), there does not exist any positional\naugmented sorting algorithm with comparison complexity O(f(U))for instances withPn\ni=1log(η∆\ni+2)≤U.\nProof.Given the predictor ˆpand an upper bound Uon the error, a sorting algorithm needs to determine\nthecorrectpermutation Afromacandidatesetofsize cand(ˆp, U). Ifthealgorithmonlyuses xcomparisons,\nthen it can only distinguish 2xdifferent outcomes; if 2xis smaller than the number of candidates, it\ncannot determine the correct answer in every situation, since the information given by comparisons is not\nsufficient to distinguish all candidates. Hence, at least ⌈logcand(ˆp, U)⌉comparisons are needed.\nTheorem 4.3. For all n≤U≤O(nlogn), we have max ˆp∈Snlogcand(ˆp, U)) = Ω( U).\nProof.Proof by construction. Assume without loss of generality that Uis a multiple of nand let\nU′=U/n. Take ˆp=⟨1,2, . . . , n ⟩, the identity prediction. We aim to construct sufficient number of\ncandidate permutations A∈Sn, which all fall in the U-candidate set.\nConsider every permutation A∈Snconstructed in the follow way: Initially, set A=⟨1,2, . . . , n ⟩.\nThen, divide Ainton\n2U′adjacent subarrays, each containing 2U′items (the last subarray is potentially\nsmaller if there are not enough remaining elements). Finally, we permute the items in each subarray\narbitrarily, and retrieve Aafter the permutation.\nIn each possible outcome A,logη∆\ni≤U′for all isince each item is only permuted locally. Hence, the\nerror of ˆpw.r.t. Ais no larger than n·U′=U. Each Afalls inside the U-candidate set. Counting the\nnumber of possible different outcomes in our construction, we obtain\ncand(ˆp, U)≥h\u0010\n2U′\u0011\n!in\n2U′,\nwhere the right-hand-side represents the possible ways to permute 2U′items in each subarray.\nBy Stirling’s formula,\nlogcand(ˆp, U)≥Ω\u0010n\n2U′·(2U′·U′)\u0011\n= Ω(U).\n12\n\nDirectly combining Theorem 4.2 and Theorem 4.3, and noting that at least Ω(n)comparisons are\nalways needed to verify correctness of a sorted list, we obtain the following:\nCorollary 4.4. For sorting with positional predictions, there exists no algorithm with comparison\ncomplexity o(Pn\ni=1log(η∆\ni+ 2)) .\n4.2 Optimality of Dirty Comparisons Sort and Double-Hoover Sort\nWe can use the same definition of cand(ˆp, U)and construction as above. Every positional prediction\ncan be view as a total linear relation on array A, therefore induces a unique dirty-comparison predictor.\nSince each element is locally perturbed, we can prove that for each permutation Aobtained from the\nconstruction,Pn\ni=1log(ηi+ 2)andPn\ni=1log(min\b\nηl\ni, ηr\ni\t\n+ 2)are also upper bounded by U. Hence, the\noptimality of Algorithm 2 and 3 can be proven in the exact same way, and Theorem 1.5 follows.\n5 Experiments\nIn this section, we conduct experiments both on synthetic data, crafted to simulate predictions in\nreal-world settings, and also on real-world data of countries’ population ranking. The source code used\nfor experiments is available at https://github.com/xingjian-bai/learning-augmented-sorting .\nWe assess the performance of our proposed sorting algorithms against five well-established baselines.\nQuick Sort and Merge Sort are classic sorting algorithms with O(nlogn)complexity; Tim Sort (Peters,\n2002) is a popular hybrid sorting algorithm designed to perform efficiently on real-world datasets and\nwidely adopted in standard libraries. Further, we choose two adaptive sorting algorithms, Odd-Even\nStraight Merge Sort (Estivill-Castro and Wood, 1992) and Cook-Kim division (Cook and Kim, 1980),\nwhich are proven to be optimal with respect to several measures of disorderness. They serve as adaptive\nvariants of Merge Sort and Quick Sort. To apply adaptive sorting algorithms in positional prediction\nsettings, we first execute bucket sort on the items by their predicted ranking, breaking ties arbitrarily.\nThis “sorted-by-prediction” array is then inputted into the baselines.\nPositional Predictions. First, we elaborate our synthetic data generation process. In many sorting\ntasks, items belong to different “grades”, which represent a coarse version of the ranking. For example,\nstudents are classified into grade A, B, C, and D based on their exam scores; with their grades in hand,\nwe want to find out their accurate ranking. We denote this scenario as the class setting . Specifically, we\ndivide an array of nitems into cclasses, sampling the thresholds t0= 0≤t1< t2< . . . , t c=nuniformly\nat random. Then, for items aiwith tk−1< i≤tk, we say that they belong to the kth class, and their\npredicted position is uniformly generated from (tk−1, tk].\nTo model the tasks where we have an “outdated” ranking, we design the decay setting . The accurate\nranking is obtained as the prediction at time 0. Then, during each time step, one item is randomly\nselected to be perturbed: its predicted position is shifted by 1, towards either left or right, with uniform\nprobability. We then ask the sorting algorithms to retrieve the original ranking of items based on the\nprediction at each time step.\nWe also utilize data from a real-world setting. We draw the annual population ranking of countries\nand smaller regions from 1960 to 2010 from World Bank (2023). Then, we feed in the ranking in year\nx= 1960 , . . . , 2010respectively as the prediction, and ask the sorting algorithm to predict the ranking in\nyear 2010.\nIn all the plots, the X-axis indicates the quality of predictions, and the Y-axis indicates the number of\ncomparisons used. The red dotted line is nlog2n. The bold curves represent the proposed algorithms.\nAll experiments are repeated 30 times, with the standard deviation indicated by shade. A scapegoat tree\nimplementation of Double-Hoover Sort is used for synthetic settings, while an array implementation is\nused for population ranking given the small sample size.\nAs depicted in Figure 5.3, our algorithms consistently outperform the baselines in all settings with\nvarious task sizes. Specifically, in the class setting with n= 1,000,000, Displacement Sort and Double-\nHoover Sort outperform all baselines when the number of classes is larger than 0.05n. In the decay\nsetting, both our algorithms perform better than the others as time progresses. In the real-world dataset,\ncountry population ranking, Displacement Sort needs the fewest comparisons when the given prediction\nis within 5 years, and Double-Hoover Sort dominates the rest when the prediction is obtained 6 to 60\nyears ago. These experiments illustrate that the proposed algorithms can leverage positional predictions\nmore effectively than traditional adaptive and non-adaptive sorting algorithms in a variety of settings.\n13\n\n(a) class setting, n= 1,000\n (b) class setting, n= 10,000\n(c) class setting, n= 100 ,000\n (d) class setting, n= 1,000,000\nFigure 5.1: Class Settings.\nDirty Comparisons. In some sorting scenarios, some “indicating factors” can be used to cheaply\ncompare two items. For instance, in biology, we can compare the binding affinities of two molecules\nfor a specific target protein and provide information about their potential efficacy as drugs. However,\ncomparisons based on indicating factors may have error induced by element-wise noise. Hence, we consider\na two dirty-comparison settings in which a ratio rof items is damaged. We say a dirty comparison\nisperturbed if its outcome is uniformly random. In the Good-Dominating setting , a dirty comparison\nbetween two items is perturbed if both are damaged; in the Bad-Dominating setting , a dirty comparison\nbetween two items is perturbed if either item is damaged.\nIn dirty comparisons settings, we use the 3-approximation feedback arc set algorithm proposed by\nAilon et al. (2008) to preprocess the dirty comparisons. This algorithm uses O(nlogn)dirty comparisons,\nthe same order of magnitude as our Dirty-Clean Sort, to construct a positional prediction that roughly\naligns with the given dirty comparisons. Then, we feed in the induced positional prediction to the\nbaselines.\nAs showcased in Figure 5.4, when n= 100 ,000, in the Good-Dominanting setting, our proposed\nDirty-Clean Sort outperforms baselines when r≤0.7; in the Bad-Dominating setting, it outperforms\nother algorithms when r≤0.25. If the damage ratio is large, the prediction becomes chaotic, but it still\nperforms essentially no worse than Quick Sort as discussed in Remark 2.6.\n6 Limitations and Future Work\nIn the dirty-clean setting, our algorithm still requires a time complexity of O(nlogn)due to the processing\nofO(nlogn)dirty comparisons. Consequently, the algorithm is more appropriate for situations where\nexact comparisons are expensive than for those where comparisons are fast. In the positional prediction\nsetting, DisplacementSortachievesaboundof O(P\nilog(η∆\ni+2))forbothcomparison andtimecomplexity,\nwhereas the Double-Hoover sort achieves its guarantee O(P\nilog(min{ηl\ni, ηr\ni}+ 2))only for comparison\ncomplexity. An intriguing question is whether the latter bound can be achieved for time complexity as\n14\n\n(a) decay setting, n= 1,000\n (b) decay setting, n= 10,000\n(c) decay setting, n= 100 ,000\n (d) decay setting, n= 1,000,000\nFigure 5.2: Decay Settings.\nwell. Another potential limitation is that predictions might not be learnable in some sorting settings;\nfuture work could focus on exploring the conditions under which predictions are learnable.\nAcknowledgments. We thank the anonymous reviewers at NeurIPS and Luke Melas-Kyriazi for their\nvaluable comments.\nReferences\nN. Ailon, M. Charikar, and A. Newman. Aggregating inconsistent information: Ranking and clustering.\nJ. ACM, 55(5):23:1–23:27, 2008. URL https://doi.org/10.1145/1411509.1411513 .\nM. Almanza, F. Chierichetti, S. Lattanzi, A. Panconesi, and G. Re. Online facility location with\nmultiple advice. In NeurIPS , 2021. URL https://proceedings.neurips.cc/paper/2021/hash/\n250473494b245120a7eaf8b2e6b1f17c-Abstract.html .\nK. Anand, R. Ge, A. Kumar, and D. Panigrahi. Online algorithms with multiple predictions. In ICML,\n2022. URL https://proceedings.mlr.press/v162/anand22a.html .\nS. Angelopoulos, C. Dürr, S. Jin, S. Kamali, and M. P. Renault. Online computation with untrusted\nadvice. In ITCS, 2020. URL https://doi.org/10.4230/LIPIcs.ITCS.2020.52 .\nA. Antoniadis, C. Coester, M. Eliás, A. Polak, and B. Simon. Learning-augmented dynamic power manage-\nment with multiple states via new ski rental bounds. In NeurIPS , 2021. URL https://proceedings.\nneurips.cc/paper/2021/hash/8b8388180314a337c9aa3c5aa8e2f37a-Abstract.html .\nA. Antoniadis, C. Coester, M. Eliás, A. Polak, and B. Simon. Mixing predictions for online metric\nalgorithms. In ICML, 2023. URL https://proceedings.mlr.press/v202/antoniadis23b.html .\n15\n\nFigure 5.3: Country population ranking, n= 261.\nY. Azar, S. Leonardi, and N. Touitou. Flow time scheduling with uncertain processing time. In STOC,\n2021. URL https://doi.org/10.1145/3406325.3451023 .\nY. Azar, S. Leonardi, and N. Touitou. Distortion-oblivious algorithms for minimizing flow time. In SODA,\n2022. URL https://doi.org/10.1137/1.9781611977073.13 .\nN. Bansal, C. Coester, R. Kumar, M. Purohit, and E. Vee. Learning-augmented weighted paging. In\nSODA, 2022. URL https://doi.org/10.1137/1.9781611977073.4 .\nA. Bhaskara, A. Cutkosky, R. Kumar, and M. Purohit. Online linear optimization with\nmany hints. In NeurIPS , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n6c250b592dc94d4de38a79db4d2b18f2-Abstract.html .\nM. Braverman and E. Mossel. Noisy sorting without resampling. In SODA, 2008. URL http://dl.acm.\norg/citation.cfm?id=1347082.1347112 .\nX. Cao, J. Chen, L. Chen, C. Lambert, R. Peng, and D. Sleator. Learning-augmented b-trees, 2023. URL\nhttps://doi.org/10.48550/arXiv.2303.15379 .\nJ. Y. Chen, S. Silwal, A. Vakilian, and F. Zhang. Faster fundamental graph algorithms via learned\npredictions. In ICML, 2022. URL https://proceedings.mlr.press/v162/chen22v.html .\nC. R. Cook and D. J. Kim. Best sorting algorithm for nearly sorted lists. Commun. ACM , 23(11):620–624,\n1980. URL https://doi.org/10.1145/359024.359026 .\nS. Davies, B. Moseley, S. Vassilvitskii, and Y. Wang. Predictive flows for faster ford-fulkerson. In ICML,\n2023. URL https://proceedings.mlr.press/v202/davies23b.html .\nM. Dinitz, S. Im, T. Lavastida, B. Moseley, and S. Vassilvitskii. Faster matchings via\nlearned duals. In NeurIPS , 2021. URL https://proceedings.neurips.cc/paper/2021/hash/\n5616060fb8ae85d93f334e7267307664-Abstract.html .\nM. Dinitz, S. Im, T. Lavastida, B. Moseley, and S. Vassilvitskii. Algorithms with prediction\nportfolios. In NeurIPS , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\n7f9220f90cc85b0da693643add6618e6-Abstract-Conference.html .\nY. Emek, S. Kutten, and Y. Shi. Online paging with a vanishing regret. In ITCS, 2021. URL\nhttps://doi.org/10.4230/LIPIcs.ITCS.2021.67 .\nJ. Ergun, Z. Feng, S. Silwal, D. P. Woodruff, and S. Zhou. Learning-augmented k-means clustering. In\nICLR, 2022. URL https://openreview.net/forum?id=X8cLTHexYyY .\n16\n\n(a) good-dominating, n= 1,000\n (b) good-dominating, n= 10,000\n(c) good-dominating, n= 100 ,000\n (d) bad-dominating, n= 1,000\n(e) bad-dominating, n= 10,000\n (f) bad-dominating, n= 100 ,000\nFigure 5.4: Sorting with dirty comparisons, good- and bad-dominating settings\n17\n\nT. Erlebach, M. S. de Lima, N. Megow, and J. Schlöter. Sorting and hypergraph orientation under\nuncertainty with predictions. In IJCAI, 2023. URL https://doi.org/10.24963/ijcai.2023/619 .\nV. Estivill-Castro and D. Wood. A survey of adaptive sorting algorithms. ACM Comput. Surv. , 24(4):\n441–476, 1992. URL https://doi.org/10.1145/146370.146381 .\nY. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application\nto boosting. Journal of Computer and System Sciences , 55(1):119–139, 1997. URL https://www.\nsciencedirect.com/science/article/pii/S002200009791504X .\nB. Geissmann, S. Leucci, C. Liu, and P. Penna. Optimal sorting with persistent comparison errors. In\nESA, 2019. URL https://doi.org/10.4230/LIPIcs.ESA.2019.49 .\nS. Gollapudi and D. Panigrahi. Online algorithms for rent-or-buy with expert advice. In ICML, 2019.\nURL http://proceedings.mlr.press/v97/gollapudi19a.html .\nY. Gu and Y. Xu. Optimal bounds for noisy sorting. In STOC, 2023. URL https://doi.org/10.1145/\n3564246.3585131 .\nL. J. Guibas, E. M. McCreight, M. F. Plass, and J. R. Roberts. A new representation for linear lists. In\nSTOC, 1977. URL https://doi.org/10.1145/800105.803395 .\nT. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In\nSIGMOD , 2018. URL https://doi.org/10.1145/3183713.3196909 .\nT. Kraska, M. Alizadeh, A. Beutel, E. H. Chi, A. Kristo, G. Leclerc, S. Madden, H. Mao, and V. Nathan.\nSageDB: A learned database system. In CIDR, 2019. URL http://cidrdb.org/cidr2019/papers/\np117-kraska-cidr19.pdf .\nA. Kristo, K. Vaidya, U. Çetintemel, S. Misra, and T. Kraska. The case for a learned sorting algorithm.\nInSIGMOD , 2020. URL https://doi.org/10.1145/3318464.3389752 .\nS. Lattanzi, T. Lavastida, B. Moseley, and S. Vassilvitskii. Online scheduling via learned weights. In\nSODA, 2020. URL https://doi.org/10.1137/1.9781611975994.114 .\nH. Lin, T. Luo, and D. P. Woodruff. Learning augmented binary search trees. In ICML, 2022. URL\nhttps://proceedings.mlr.press/v162/lin22f.html .\nA. Lindermayr and N. Megow. Permutation predictions for non-clairvoyant scheduling. In SPAA, 2022.\nURL https://doi.org/10.1145/3490148.3538579 .\nP. Lu, X. Ren, E. Sun, and Y. Zhang. Generalized sorting with predictions. In SOSA, 2021. URL\nhttps://doi.org/10.1137/1.9781611976496.13 .\nT. Lykouris and S. Vassilvitskii. Competitive caching with machine learned advice. J. ACM, 68(4):\n24:1–24:25, 2021. URL https://doi.org/10.1145/3447579 .\nD. J. Mankowitz, A. Michi, A. Zhernov, M. Gelmi, M. Selvi, C. Paduraru, E. Leurent, S. Iqbal, J.-B.\nLespiau, A. Ahern, T. Köppe, K. Millikin, S. Gaffney, S. Elster, J. Broshear, C. Gamble, K. Milan,\nR. Tung, M. Hwang, T. Cemgil, M. Barekatain, Y. Li, A. Mandhane, T. Hubert, J. Schrittwieser,\nD. Hassabis, P. Kohli, M. Riedmiller, O. Vinyals, and D. Silver. Faster sorting algorithms discovered\nusing deep reinforcement learning. Nature, 618(7964):257–263, 2023. URL https://doi.org/10.1038/\ns41586-023-06004-9 .\nH. Mannila. Measures of presortedness and optimal sorting algorithms. IEEE Trans. Computers , 34(4):\n318–325, 1985. URL https://doi.org/10.1109/TC.1985.5009382 .\nM. Mitzenmacher. Scheduling with predictions and the price of misprediction. In ITCS, 2020. URL\nhttps://doi.org/10.4230/LIPIcs.ITCS.2020.14 .\nM. Mitzenmacher and S. Vassilvitskii. Algorithms with predictions. Commun. ACM , 65(7):33–35, 2022.\nURL https://doi.org/10.1145/3528087 .\nJ. I. Munro and S. Wild. Nearly-optimal mergesorts: Fast, practical sorting methods that optimally\nadapt to existing runs. In ESA, 2018. URL https://doi.org/10.4230/LIPIcs.ESA.2018.63 .\n18\n\nT. Peters. [Python-Dev] Sorting. Python Developers Mailinglist, 2002. URL https://mail.python.\norg/pipermail/python-dev/2002-July/026837.html .\nM. Purohit, Z. Svitkina, and R. Kumar. Improving online algorithms via ML pre-\ndictions. In NeurIPS , 2018. URL https://proceedings.neurips.cc/paper/2018/hash/\n73a427badebe0e32caa2e1fc7530b7f3-Abstract.html .\nD. Rohatgi. Near-optimal bounds for online caching with machine learned advice. In SODA, 2020. URL\nhttps://doi.org/10.1137/1.9781611975994.112 .\nS. Sakaue and T. Oki. Discrete-convex-analysis-based framework for warm-starting algorithms with\npredictions. In NeurIPS , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\n844e61124d9e1f58632bf0c8968ad728-Abstract-Conference.html .\nS. Wang, J. Li, and S. Wang. Online algorithms for multi-shop ski rental with machine\nlearned advice. In NeurIPS , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n5cc4bb753030a3d804351b2dfec0d8b5-Abstract.html .\nA. Wei. Better and simpler learning-augmented online caching. In APPROX/RANDOM , 2020. URL\nhttps://doi.org/10.4230/LIPIcs.APPROX/RANDOM.2020.60 .\nWorld Bank. Population, total, 2023. URL https://data.worldbank.org/indicator/SP.POP.TOTL .\nUnited Nations Population Division. World Population Prospects: 2022 Revision.\n19",
  "textLength": 61328
}