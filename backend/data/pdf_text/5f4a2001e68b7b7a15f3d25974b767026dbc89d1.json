{
  "paperId": "5f4a2001e68b7b7a15f3d25974b767026dbc89d1",
  "title": "NFL: Robust Learned Index via Distribution Transformation",
  "pdfPath": "5f4a2001e68b7b7a15f3d25974b767026dbc89d1.pdf",
  "text": "NFL: Robust Learned Index via Distribution Transformation\nShangyu Wu\nCity University of Hong Kong\nHong Kong\nshangyuwu2-c@my.cityu.edu.hkYufei Cui*\nCity University of Hong Kong\nHong Kong\nyufeicui3-c@my.cityu.edu.hkJinghuan Yu\nCity University of Hong Kong\nHong Kong\njinghuayu2-c@my.cityu.edu.hk\nXuan Sun\nCity University of Hong Kong\nHong Kong\nxuansun-c@my.cityu.edu.hkTei-Wei Kuo\nCity University of Hong Kong\nHong Kong\nteiwei.kuo@cityu.edu.hkChun Jason Xue\nCity University of Hong Kong\nHong Kong\njasonxue@cityu.edu.hk\nABSTRACT\nRecent works on learned index open a new direction for the index-\ning field. The key insight of the learned index is to approximate\nthe mapping between keys and positions with piece-wise linear\nfunctions. Such methods require partitioning key space for a better\napproximation. Although lots of heuristics are proposed to improve\nthe approximation quality, the bottleneck is that the segmentation\noverheads could hinder the overall performance.\nThis paper tackles the approximation problem by applying a dis-\ntribution transformation to the keys before constructing the learned\nindex. A two-stage Normalizing-Flow-based Learned index frame-\nwork (NFL) is proposed, which first transforms the original com-\nplex key distribution into a near-uniform distribution, then builds a\nlearned index leveraging the transformed keys. For effective distri-\nbution transformation, we propose a Numerical Normalizing Flow\n(Numerical NF). Based on the characteristics of the transformed\nkeys, we propose a robust After-Flow Learned Index (AFLI). To val-\nidate the performance, comprehensive evaluations are conducted\non both synthetic and real-world workloads, which shows that the\nproposed NFL produces the highest throughput and the lowest tail\nlatency compared to the state-of-the-art learned indexes.\nPVLDB Reference Format:\nShangyu Wu, Yufei Cui*, Jinghuan Yu, Xuan Sun, Tei-Wei Kuo, and Chun\nJason Xue. NFL: Robust Learned Index via Distribution Transformation.\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/luffy06/NFL.\n1 INTRODUCTION\nLearned Index [ 22], as an advance replacement of conventional\nindexes, has attracted a lot of attention in recent years. It lever-\nages machine learning models to learn the mapping between keys\nand corresponding positions, greatly accelerating the data retrieval\nprocess. Existing learned indexes [ 6,12,13,42] further address\nthe updatable issue in the original learned index, and achieve sig-\nnificant improvements on various workloads. The key insight of\nthese learned indexes is to regard linear models as a piece-wise\nlinear function to approximate the cumulative distribution function\n(CDF). To obtain a good approximation, existing learned indexes\npropose various segmentation methods, e.g., the convex-hull-based\n*Corresponding author.\nAccepted as a conference paper in VLDB 2022.segmentation [ 12]. However, such heuristic segmentation not only\nintroduces extra time and space overhead, but also requires lots of\nefforts to design for both the algorithm and supportive operations.\nThis paper solve the approximation problem from a different angle.\nIn this work, we propose a new methodology which transforms\nthe original keys to a near-uniform key space before constructing\nthe learned index, so that the learned index can make a much better\napproximation on the CDF. This distribution transformation tackles\nthe approximation problem from the root, and significantly improve\nthe performance for all aspects of learned indexes. Normalizing\nflows (NFs) [ 8,34], as a family of generative models, could be applied\nto perform distribution transformation .\nTo reach the goal of distribution transformation, we propose\na two-stage Normalizing-Flow-Learned index framework (NFL),\nconsisting of a Numerical Normalizing Flow (Numerical NF) for\ndistribution transformation and a robust After-Flow Learned Index\n(AFLI) for CDF approximation. Instead of directly segmenting the\nCDF curve, the proposed NFL first leverages the Numerical NF to\ntransform the original keys into the near-uniform distributed keys,\nso that the CDF curve becomes roughly linear. Then, based on the\ntransformed keys, the proposed AFLI could make a decent approx-\nimation on the transformed CDF. In addition, we also propose a\nnew type of conflict degree metric to evaluate the transformation\nquality of Numerical NF.\nExisting normalizing flows are not applicable for the case of\nlearned indexes, as they are designed for high dimensional data\nwith rich semantic or spatial features, e.g., computer vision [ 1,\n41]. However, the learned indexes deal with numerical data with\nless complexity for learning. Furthermore, existing normalizing\nflows are of too high complexity (e.g., 4 layers, 16 parameters)\nfor processing the queries. In the proposed Numerical NF, for the\nefficacy of NFs, we propose a feature space expansion scheme for\nenriching the learnable features. Moreover, a set of optimizations\nfor efficiency is proposed, including a switching mechanism based\nonconflict degree .\nAfter the transformation of Numerical NF, considering the char-\nacteristics of the transformed keys, the proposed AFLI only needs\na simple and efficient structure to handle local conflicts, providing\na robust performance in both throughput and tail latency. In AFLI,\nwe replaces the complex and expensive adjustments in existing\nlearned indexes with simple Modelling operations, which turns a\ndense array into a model-based node.\nFinally, we conduct a series of experiments on both synthetic\nand real-world workloads. The proposed NFL and learned index\n\nğ“œğ“œ\nğ“œğ“œ ğ“œğ“œ\nğ“œğ“œ ğ“œğ“œ ğ“œğ“œStage 1 Stage 2 Stage 3Key\nPositionPos.\nKeyCDF\nEstimated CDFData Points\nSegmentationFigure 1: The structure and the insight of the recursive model\nindex.\nare compared with the state-of-the-art learned indexes in terms\nof throughput, tail latency, index size, and bulk loading time. Ex-\nperimental results show that the proposed NFL beats all existing\nlearned index in almost all kinds of workloads with the highest\nthroughput and the lowest tail latency. Specifically, the proposed\nNFL can achieve 2.77x improvements on average in throughput and\n43% reductions on average in tail latency.\nThe main contributions of this paper are:\nâ€¢To the best of our knowledge, this is the first work that\nexplores the distribution transformation to address the ap-\nproximation challenge of the learned index.\nâ€¢We propose a two-stage normalizing-flow-based learned\nindex (NFL) framework, which transforms keys into another\nkey space, where the learned index could be easier to fit a\nmore linear CDF curve.\nâ€¢To improve the transformation quality of NF in the NFL, we\npropose a numerical normalizing flow, which could enrich\nthe poor feature in the indexing problem. We also make an\nefficient implementation of NF.\nâ€¢After the distribution transformation of NF, based on the\ncharacteristics of the transformed keys, we propose an\nAfter-Flow Learned Index (AFLI), which can efficiently sup-\nport almost all indexing operations, providing a robust per-\nformance in both throughput and tail latency.\nâ€¢We conduct a set of experiments with both synthetic and\nreal-world workloads to show the effectiveness and the\nefficiency of the proposed NFL framework.\nThe rest of this paper is organized as follows. Section 2 introduces\nthe background and motivations. Section 3 presents the proposed\nnormalizing-flow-learned index framework. Specifically, Section\n3.2 introduces the numerical normalizing flow in the NFL for distri-\nbution transformation. Section 3.3 presents the after-flow learned\nindex in detail that makes a better approximation on the trans-\nformed distribution. Section 4 presents the experimental results\nwith discussion. Section 5 presents the literature review. Finally, in\nSection 6, we conclude this paper and discuss the future work.\n2 BACKGROUND AND MOTIVATIONS\n2.1 Preliminary\nRecent works on Learned Index (LI) [ 22] observed that the mapping\nbetween keys and the corresponding positions can be formulated\nPos.\nKeyConvex \nHull\n(a) A convex-hull -based \nsegmentation in PGM -Index.Pos.\nKey\n(b) A prediction -based \nsegmentation in LIPP.\nPos.\nKey(c) An enumeration -based segmentation in Alex.Fitted Linear Model\nFanout -1 Fanout -2\nFanout -4 Fanout -8Minimal CostFigure 2: Different segmentation on the CDF.\nas a Cumulative Distribution Function (CDF) model,\npos=ğ¹(key)âˆ—ğ‘ (1)\nwhere posis the predicted position, ğ¹(Â·)is the approximated CDF\nthat estimates the likelihood ğ‘(ğ‘¥â‰¤key)of a keyğ‘¥smaller than or\nequal to the given key, and ğ‘is the total number of keys. Based on\nthe formulation, learned index can leverage ML models to approxi-\nmate the CDF to predict the position, replacing multiple comparison\noperations in BTree with computations.\nSince using a single model (e.g., a fully connected neural network)\nto approximate the CDF suffers the \"last mile\" issue, i.e., much\nmore space and time are required to reduce the error further from\nthousands to hundreds, therefore, Kraska et al. [ 22] propose the\nRecursive Model Index (RMI) , which is a hierarchy of linear models.\nAs shown in the left of Figure 1, the higher-level model takes the\nkey as input and picks the model at the next level, and so on. Later,\nthe leaf model predicts the estimated position of the input key.\nDue to prediction errors, the RMI will finally use local search (e.g.,\nbinary search) to correct the estimated position.\nFrom the description of the search process, we can find that the\napproximation is actually constructing a better piece-wise linear\nfunction, reducing amount of local search. As shown in the right\nof Figure 1, the root model in stage 1 first tries to approximate the\ncurve of the CDF. But, since using two linear models, jointly acting\nas a piece-wise linear function, can make a better approximation\nof the curve, the root model segments the curve of the CDF into\ntwo sub-curves. The first sub-curve is then approximated by the\nfirst child model in the stage 2. Similarly, the first sub-curve is\nalso segmented into two sub-curves, and so do other sub-curves.\nCompared to higher-level models, next-level models make a better\napproximation with less prediction errors as more segments are\nused to approximate the curve, so that the time cost of local search\ncan be reduced. However, a deeper hierarchy/tree prolongs the\ntraversal time, as it increases the number of stages thus the number\nof predictions of the ML model.\n2\n\nPos.\nKey\nProb.\nKeyPos.\nKey\nProb.\nKey\nPos.\nKey\nProb.\nKey\nğ“œğ“œ\nCDFEstimated CDF\nData PointsSegmentationThe Learned Index\nğ“œğ“œ\nğ“œğ“œğ“œğ“œ\nğ“œğ“œ Current Node\nPDF\nDistribution \nTransformationDistribution \nTransformation\nğ“œğ“œ\nğ“œğ“œğ“œğ“œ\nğ“œğ“œ\nğ“œğ“œ\nğ“œğ“œğ“œğ“œğ“œğ“œğ“œğ“œ\nPos.\nKey\nProb.\nKeyFigure 3: Applying a transformation on the CDF.\nInspired by the RMI, the follow-up learned indexes [ 6,12,42] not\nonly address the updatable issues in the RMI, but also improve the\napproximation quality of linear models, making the index deliver a\ncomparable performance on various indexing operations. As shown\nin the Figure 2(a), PGM-Index [ 12] propose a convex-hull-based\nsegmentation to partition the key space. Specifically, the key points\non the curve can be aggregated by a convex hull. If the convex hull\ncannot be enclosed in a rectangle of height no more than 2ğœ–, the set\nof keys in the convex hull are allocated to a segment. Then a new\nconvex hull is created for the subsequent segmentation. Although\nthe segmentation in PGM-Index can ensure that the prediction\nerror is less than ğœ–, the approximation quality is limited as the\nsegmentation doesnâ€™t refer to the global information of the CDF.\nMoreover, how to choose the hyper-parameter ğœ–is also a key point.\nDifferent from PGM-Index, ALEX [ 6] and LIPP [ 42] first fit a\nlinear model to learn the outline of the CDF, then segment the curve\nof the CDF based on predictions. Compared to prediction errors,\nLIPP is more concerned with conflict degrees , i.e., the number of\nkeys predicted to the same position. As shown in the Figure 2(b),\nLIPP allocates all conflicted keys to the same segment. Figure 2(c)\nshows the main idea of segmentation in ALEX. ALEX exhaustively\nsearches for the best number of segments. Specifically, before build-\ning the index, it tries different number of segments, namely fanout,\ncalculates the cost of each kind of fanout, and chooses the best\nfanout with the minimal cost. Both segmentation in ALEX and LIPP\nsignificantly improve the approximation quality so that deliver a\nhigh performance.\nAll these variants of learned index pay their attention on how\nto partition the key space by designing better heuristics, to obtain\nbetter piece-wise linear approximations for the CDF. However, such\nsegmentation requires a long period of time to process and large\npre-allocated space to maintain. Furthermore, each learned index\nrequires designing a set of supportive operations and optimizing\ntheir hyper-parameters by tuning.\n2.2 Motivation\nIn this work, we explore in direction which is different from previ-\nous works. Instead of directly approximating the original CDF withTable 1: Statistics of ALEX [ 6] on longlat (LLT) and facebook\n(FB) workloads. Throughput are presented in million ops/sec.\nWithout NF With NF\nLLTMax Tree Height 4 3\nAverage Tree Height 2.30 2.01\n# Prediction Errors 925,487,063 118,833,075\n# Predictions 124,831,692 101,108,381\nThroughput 8.21 11.57\nFBMax Tree Height 11 3\nAverage Tree Height 5.91 2.02\n# Prediction Errors 928,113,206 453,003,864\n# Predictions 492,112,591 100,462,387\nThroughput 4.13 9.16\na piece-wise linear function, the proposed method aims at trans-\nforming the original keys to another key space, where the learned\nindex could work on a less complex key distribution. The proposed\nmethod is adaptive to different workloads, thus it alleviates the\nheavy dependence of expertise for designing heuristics and tuning\nhyper-parameters.\nConceptually, previous works segment the curve of the CDF\ninto several sub-curves, in order to make every linear model fit the\nsub-curve better, so that the learned index can make a good approxi-\nmation with a piece-wise linear function on the whole curve (Figure\n3). The insight of such methods is to find a roughly linear sub-curve\nby repeatedly segmenting the curve, whose corresponding proba-\nbility density function (PDF) is a local near-uniform distribution.\nTo reach this goal, the learned index has to construct a deeper hier-\narchy, resulting in more traversal time and number of predictions.\nHowever, if there is an ideal key space where keys already satisfy a\nglobal near-uniform distribution, whose corresponding CDF curve\nis roughly linear, a single linear model is sufficient to fit such curve.\nTherefore, this motivates us to transform the original key space\nto a near-uniform key space by distribution transformation , before\nconstructing the learned index.\n3\n\nIn practice, there is no unified heuristic that works for trans-\nforming heterogeneous key space in real-world workloads. Based\non the Change of Variables Theorem , normalizing flow [ 34] can\nlearn the transformation between a base distribution and any given\ndata distribution. Previous NFs are only designed for high dimen-\nsional data with rich semantic or spatial features, e.g., computer\nvision [ 1,41]. For learned indexes, the keys are numerical data and\nwith less useful features. We propose the first numerical NF that\ntransforms the keys into a global near-uniform distribution. We\nillustrate the efficacy of the proposed numerical NF on real-world\nworkloads and a representative learned index. Table 1 shows the\nstatistics of ALEX on two workloads with a highly non-linear CDF\ncurve. After applying the proposed NF to transform the keys, the\ntree height of ALEX can be reduced from 11 to 3 levels on the face-\nbook workload. On longlat workload, although the reduction of tree\nlevels is 4âˆ’3=1, the prediction errors are greatly reduced as NF\ngenerates more uniform distributed keys which reduces the number\nof out-of-boundary keys within each node. Besides, the numbers of\npredictions are also reduced. Therefore, ALEX can achieves great\nimprovements on the throughput. The above results and analysis\njointly demonstrate the importance of distribution transformation.\n2.3 Normalizing Flows\nNormalizing Flows [ 34] are a family of generative models, which\ntransform a latent distribution ğ‘(ğ‘§)to a new distribution ğ‘ğº(ğ‘¥)by\na series of parameterized generators, where ğ‘§andğ‘¥are correspond-\ning random variables (r.v.). In our case, ğ‘§andğ‘¥corresponds to the\nideal (transformed) and original keys. We define {ğ‘§ğ‘–}ğ‘€\nğ‘–and{ğ‘¥ğ‘–}ğ‘€\nğ‘–\nas the samples of the two r.v., where ğ‘€is the size of the sample sets.\nWe want the generated distribution ğ‘ğº(ğ‘¥)to approximate the given\ndata distribution ğ‘ğ‘‘ğ‘ğ‘¡ğ‘(ğ‘¥). For training the NF, the objective is to\nmaximize the likelihood of the generated distribution ğ‘ğº(ğ‘¥)or op-\ntimize the Kullback-Leibler (KL) divergence between the generated\ndistribution ğ‘ğº(ğ‘¥)and the data distribution ğ‘ğ‘‘ğ‘ğ‘¡ğ‘(ğ‘¥),\nğºâˆ—=argmaxğºEğ‘¥âˆ¼ğ‘ğ‘‘ğ‘ğ‘¡ğ‘logğ‘ğº(ğ‘¥) (2)\nğºâˆ—=argminğºKL(ğ‘ğºâˆ¥ğ‘ğ‘‘ğ‘ğ‘¡ğ‘) (3)\nwhereğº=ğºğœƒ=ğºğœƒ(Â·)is the parameterized generator, which takes\na sample of ğ‘§and push-forward it to a sample of ğ‘¥,ğ‘¥ğ‘–=ğºğœƒ(ğ‘§ğ‘–).ğœƒ\nrepresents the parameters. ğºâˆ—is the optimal generator.\nThe key advantage of NFs over other generative models (e.g.,\nvariational autoencoder [ 19]) is that, the generator ğºis an invertible\nfunction. Once we have a well-trained ğº,ğºâˆ’1could be obtained\neasily by taking the inverse. Then, ğºâˆ’1could be used to encode the\noriginal key ğ‘¥ğ‘–to the ideal key ğ‘§ğ‘–, which conforms with our idea of\ndistribution transformation .\nSpecifically, since ğºğœƒis invertible, using the Change of Variables\nTheorem , we can derive,\nğ‘ğº(ğ‘¥)=\f\f\f\fğœ•ğºğœƒ(ğ‘§)\nğœ•ğ‘§\f\f\f\fâˆ’1\nğ‘(ğ‘§) (4)\nwhere\f\f\fğœ•ğºğœƒ(ğ‘§)\nğœ•ğ‘§\f\f\fis the determinant of ğºğœƒâ€™s Jacobian matrix. Another\nimportant point is that the determinant of the Jacobian matrix must\nbe enough cheap to compute, otherwise the NF might introduce\nnon-negligible overhead.Previous NFs [ 9,17] present various inveritble generators with\nefficient computations of the determinants. Considering a high-\ndimensional random variable, Kingma et al . [18] , Papamakarios\net al. [32] combine autoregressive models and NFs to learn the\nhidden states from previous dimensions and improve the transfor-\nmation. To make invertible functions more expressive, Cao et al .\n[2], Huang et al . [16] propose to learn a more complex bijection\nusing a monotonic neural network and achieve the state-of-the-art\nperformance on various datasets.\n2.4 Challenges\nMotivated by distribution transformation, this work targets at\nproviding a novel two-stage framework, called Normalizing-Flow-\nLearned Index (NFL), that first transforms the key distribution by\nnormalizing flow, then build an after-flow learned index that effec-\ntively leverages the transformed key distribution. However, this\nprocess is not straightforward and we list the following challenges:\nEfficacy of normalizing flow: Naively using NF is limited in a\nfew ways: 1) the NF perform poorly due to limited features from\nthe numerical data of keys; 2) the uniform distribution is hard to\nfunction directly as an training objective. We design a Numerical\nNormalizing Flow (Numerical NF) with a enriched feature space\n(see Section 3.2.1) and an easy-to-operate training objective.\nEfficiency of normalizing flow: The transformation must be\nan efficient online step. Such requirement also limits the complexity\nof normalizing flows. Directly reducing the number of parameters\nin normalizing flows might degrade the transformation quality so\nthat learned indexes require deeper hierarchy and more models to\napproximate the CDF. We design a set of optimizations for efficiency,\nwithout losing the efficacy of the NF. 3.2.2).\nLack of proper indexes for transformed keys: With the\ntransformation of Numerical NF that fundamentally makes linear\nmodels approximate better, the design of learned indexes should\nbe reconsidered in a new perspective. Therefore, based on the char-\nacteristics of the transformed data distribution, the locality of the\ntransformed data distribution should be considered in the design of\nthe learned index. We propose a After-Flow Learned Index (AFLI)\nfor fully leveraging the transformed keys. (see Section pay3.3).\n3 NFL: THE NORMALIZING-FLOW-LEARNED\nINDEX FRAMEWORK\n3.1 Framework\nFigure 4 shows the structure and the workflow of the Normalizing-\nFlow-Learned Index framework (NFL). The framework consists of\ntwo parts, a normalizing flow for distribution transformation and a\nlearned index for CDF approximation. The input keys are first fed\ninto the normalizing flow which transforms them to a near-uniform\ndistribution. Then all transformed keys are used for building linear\nmodels in the learned index. Since batching requests (e.g, batching\nqueries, batching insertions) is a common case in modern database\n[14, 15, 28, 29, 33], our NFL also processes requests in batches.\nThe theoretical derivation is as follows. Given an input key ğ‘¥\nsatisfying a certain distribution ğ‘(ğ‘¥), the normalizing flow first\ntransform it to a target distribution ğ‘(ğ‘§), i.e.,ğ‘§=ğºğœƒ(ğ‘¥). Note that\nwe donâ€™t need to know the actual form of the distribution ğ‘(ğ‘¥)as\nthe normalizing flow only needs to maximize the likelihood of the\n4\n\nOriginal PayloadsOriginal Keys\nğ“œğ“œ\n ğ“œğ“œNormalizing \nFlows\nLearned \nIndexes\nğ“œğ“œ\nğ“œğ“œ\nğ“œğ“œ\nğ“œğ“œKeys Payloads\nTransformed Keys\nSatisfied DistributionFigure 4: The structure and the workflow of the NFL.\ntransformed data on the target distribution. Then, the learned index\ntakes the transformed key ğ‘§as the input, and predicts its position,\ni.e.,ğ‘ğ‘œğ‘ =ğ¹(ğ‘§)âˆ—ğ‘.\nThe NFL offers a new perspective to fundamentally improve the\nlearned indexes.\nâ€¢The NFL is a unified framework universally applicable to\nany workload, due to the transformation capability and\nadaptability of flow-based generative models. It amortizes\nthe requirement of domain expertise of designing piece-\nwise approximation heuristics by splitting it into two sim-\npler stages.\nâ€¢The NFL is measurable and flexible. One implementation\nof NFL can be measured by the transformation quality of\nthe normalizing flow. A consistently high-quality transfor-\nmation makes an easier implementation of learned index.\nUnder this framework, we propose the generic prototypes of nor-\nmalizing flow (Numerical NF) and learned indexes (AFLI). Before\ngoing to the details of specific designs, we present a quantitative\nevaluation metric for the transformation quality.\n3.1.1 Conflict Degree. Since the log probability of the transformed\ndistribution in the NF cannot accurately evaluate how nearly uni-\nform the distribution should be for the learned index to deliver a\nhigh performance, we need a new metric that connects the trans-\nformation quality with the performance of the learned index. Con-\nsidering that placing data in the predicted positions can eliminate\nprediction errors, we are motivated to introduce the conflict degree\nand a new metric called tail conflict degree to jointly quantify how\nnearly uniform the distribution is and how well the learned index\nwould perform.\nFirst, we introduce the conflict degree on each position.\nDefinition 3.1. For a set of dataX={ğ‘¥1,...,ğ‘¥ğ‘›}and a fitted\nlinear modelM, the conflict degree of position ğ‘—is:\nğ·ğ‘—\nM=|{ğ‘¥ğ‘–âˆˆX|M(ğ‘¥ğ‘–)==ğ‘—}| (5)\nwhereğ‘—ranges from MIN({M(ğ‘¥ğ‘–)})to MAX({M(ğ‘¥ğ‘–)}).With the definition of the conflict degree, we further define the\ntail conflict degree to evaluate the transformation quality. The tail\nconflict degree indicates the upper bound of the conflicts for most\npositions.\nDefinition 3.2. Forğ‘špositions whose conflict degree is greater\nthan 0, we represent their conflict degree with {ğ·ğ‘—\nM}. Then, for a\ngiven tail percent ğ›¾, we letğ‘¡=INT(ğ‘šÃ—ğ›¾), where INTrepresents the\nflooring operation. The Tail Conflict Degree based on a tail percent\nğ›¾isğ‘¡-th larger conflict degrees among {ğ·ğ‘—\nM}, represented by ğ·ğ›¾\nM.\nIn this paper, we set the tail percent to 0.99. For example, given\n1000 positions{ğ‘—1,...,ğ‘— 1000}with conflict degrees {ğ·ğ‘—1\nM,...,ğ·ğ‘—1000\nM},\nbased on the tail percent ğ›¾=0.99,ğ‘¡=INT(1000Ã—0.99)=990. The\ntail conflict degree is 990-th larger conflict degree. The tail conflict\ndegree is a soft measurement. It could be useful in: 1) determine\nthe execution of flow (see Section 3.2.2); 2) determine the capacity\nthreshold of a node (see Section 3.3.1).\n3.2 Numerical Normalizing Flow\nThe proposed Numerical NF aims to transform the key distribu-\ntion into a near-uniform distribution. Since training normalizing\nflow with a uniform distribution as the objective might encounter\nthe \"Nan-loss\" issue or the \"INF-loss\" issue, we replace it with a\nnear-uniform distribution, i.e., a normal distribution with a large\nvariance. After that, we then focus on feature engineering for effi-\ncacy (See Section 3.2.1), model slimming and process optimization\nfor efficiency (See Section 3.2.2).\n3.2.1 Feature Space Expansion. Existing NFs are used in computer\nvision or natural language process to process high-dimensional\nimages or texts. Those kinds of data can provide rich features, e.g.,\nsemantic features or spatial features in images and texts. For ex-\nample, in image generation tasks like hand-written digits, an NF\ngenerating number â€œ9â€ could capture the spatial correlation among\ndifferent pixels, and the semantic knowledge. The performance\ncould be improved by feeding more images of number â€œ9â€ for train-\ning. However, the data of keys are numerical data with hardly any\nuseful knowledge. As the keys are required to be unique, feeding\nmore data would increase the burden of an NF rather than help\ntraining. Therefore, one major target of our design is to enrich the\nfeatures that the an NF could learn from, while maintaining high\nefficiency.\nAlgorithm 3.1 shows the whole process of key transformation .\nFirst, all input keys are normalized with a scaled min-max normal-\nization to avoid keys without integral or floating part (Line 2). After\nthe normalization, the algorithm start to expand feature space by\nrepeatedly obtaining the integral part and the final floating part of\nthe normalized keys (Line 3-17). After the feature expansion, the\nhigh-dimensional keys are fed into the NF (Line 18). Finally, the\ndecoding layer merges the high-dimensional features into 1D keys\n(Line 19-22) and passes them to the learned index (Line 23). The\ntime complexity of the feature expansion is ğ‘‚(ğ‘›Ã—ğ‘‘). Such brute-\nforce key space expansion provides a 1-to-1 mapping from 1-D keys\nto d-dimensional keys, while maintaining a linear complexity.\n3.2.2 Efficient Processing of Normalizing Flows. Different from lin-\near models, normalizing flows do not have an analytical solution\n5\n\nAlgorithm 3.1 Key Distribution Transformation.\nInput: A set of sorted keys X={ğ‘¥1,...,ğ‘¥ğ‘›}for bulk loading, the target\ndimensionğ‘‘, the scale factor ğœƒ, the normalizing flow ğ¹.\nOutput: A set of transformed keys Z={ğ‘§1,...,ğ‘§ğ‘›}.\n1:Xğ‘‘=âˆ…;\n2:ğœ‡=min(X),ğœ=max(X)âˆ’ min(X)\nğœƒ;\n3:forğ‘¥ğ‘–âˆˆXdo\n4:ğ‘¥norm=ğ‘¥ğ‘–âˆ’ğœ‡\nğœ; /* Encoder, expand features for NF */\n5:ğ‘¥vec\nğ‘–=[];\n6:ğ‘¥int=INT(ğ‘¥ğ‘–);\n7:ğ‘¥float=ğ‘¥ğ‘–âˆ’INT(ğ‘¥ğ‘–);\n8: Addğ‘¥intintoğ‘¥vec\nğ‘–;\n9: forğ‘˜from 1toğ‘‘âˆ’2do\n10: Addğ‘¥intintoğ‘¥vec\nğ‘–;\n11:ğ‘¥float=ğ‘¥floatâˆ—ğœƒ\n12:ğ‘¥int=INT(ğ‘¥float);\n13:ğ‘¥float=ğ‘¥floatâˆ’ğ‘¥int;\n14: end for\n15: Addğ‘¥floatintoğ‘¥vec\nğ‘–;\n16: Appendğ‘¥vec\nğ‘–intoXğ‘‘;\n17:end for\n18:ğ‘ğ‘‘=ğ¹(ğ‘‹ğ‘‘); /* Transformation by the NF */\n19:forğ‘§vec\nğ‘–âˆˆZğ‘‘do\n20: Letğ‘§ğ‘–be the sum of ğ‘§vec\nğ‘–; /* Decoder, merge features for index */\n21: Addğ‘§ğ‘–intoZ;\n22:end for\n23:returnZfor the index;\nand it takes time to train (about 38 seconds). However, since nor-\nmalizing flows have much better generalization, they donâ€™t need\nto re-train during every bulk loading phase or adjustments in the\nlearned indexes. Therefore, the training stage can be processed\noffline with neural network accelerators (e.g., GPU or FPGA) when\nthe index bulk loads for the first time or when the distribution sig-\nnificantly shifts. Meanwhile, the training of normalizing flows can\nbe easily finished in the background without affecting the learned\nindexes.\nHowever, the inference of normalizing flows must be finished\nonline. The transformed keys cannot be stored, as it would cause an-\nother indexing problem. By comprehensive tests, there are a few key\nfactors related to the inference efficiency: 1) the input dimensions\n2) the number of layers 3) the hidden dimensions 4) the implemen-\ntation platform. For the first three factors, we slightly search for the\nparameters without increasing the conflict degrees and with a low\nsearch cost. As Table 2 shows, the search space is small limited by\nthe unacceptable transformation overheads of larger NFs. For the\nacceleration platform, we implement the key computations in C++\nwith Intel Math Kernel Library [ 25]. The computations in inference\ncan be simplified as several matrix computations and nonlinear\nfunction computations.\nMoreover, since keys in some datasets are already near-uniform\ndistributed, it is unnecessary to spend extra time and memory to\ntransform them. Therefore, the NFL uses a switching mechanism,\nreferring to the tail conflict degree, to determine whether to use\nthe Numerical NF for distribution transformation. Specifically, the\nNFL first tries to transform the input keys, and computes the tail\nconflict degree based on the input keys and the transformed keys,Table 2: Average transformation latency of each key with\ndifferent NFs. \"H\" and \"L\" correspond to the hidden dimen-\nsion and the number of layers, respectively. The figure in\nbrackets (e.g., \"(12)\") represents the amount of parameters of\nNF. All latency is measured in nanosecond (ns).\nBatch Size 2H2L (8) 2H4L (16) 4H3L (32) 4H4L (48)\n1 169.53 384.84 320.15 463.81\n8 40.60 83.05 77.52 113.31\n32 15.28 34.75 33.80 49.40\n128 9.52 24.00 24.91 36.93\n256 8.38 21.66 23.52 35.07\n1024 7.40 19.81 22.21 33.13\n2048 7.29 19.63 22.00 32.73\nrespectively. If the latter tail conflict is larger, the NFL determines\nnot to use the Numerical NF.\n3.3 AFLI: After-Flow Learned Index\nAlthough different datasets have different original conflict degrees,\nafter the transformation of normalizing flows, the conflict degrees\ncan be kept around a low value (e.g., around 4 for the tail conflict\ndegree). Further reduction on conflict degrees requires more efforts\non the specific design of normalizing flows, which does not con-\nform with our original goal. Meanwhile, existing learning indexes\nset some empirical hyper-parameters (e.g., the maximum amount\nof keys assigned to a node) in order to guarantee an acceptable\nperformance on data sets with a high conflict degree at the cost\nof the performance degradation on data sets with a low conflict\ndegree. Therefore, those points motivate us to propose a robust\nafter-flow learned index, called AFLI , that takes the characteristics\nof transformed data sets into considerations.\nThe main idea of AFLI is to buffer local conflicts. For small\nconflicts at some positions, the conflicted keys are stored in a bucket.\nThe linear model directly built to handle the conflicted keys lacks\ngeneralization capability [ 42], as there are not enough keys to\nprovide information. However, too many keys stored in the buckets\nwould degrade the query performance of AFLI. Therefore, AFLI\nneeds to make a trade-off between the maximum number of keys\nstored in the bucket and the generalization capability of the linear\nmodel.\n3.3.1 Structure of AFLI. The overall structure of AFLI is shown\nin Figure 5. There are three types of nodes, the model node, the\nbucket, and the dense node.\nModel Node: The model node consists of an array of entries\nand a linear model. There are four types of entries,\nâ€¢Empty Slot. The entry is an unused slot, waiting to be filled\nby a pair of key-payload or a pointer.\nâ€¢Data Slot. The entry contains a pair of key-payload.\nâ€¢Bucket Pointer. The entry is a bucket pointer that stores the\naddress of a bucket.\nâ€¢Node Pointer. The entry is a node pointer that stores the\naddress of a model node or a dense node. Especially, there\nmight be some node pointers that contain the same address,\n6\n\nğ“œğ“œ\nğ“œğ“œ\nEmpty Slot\nBucket Pointer Node PointerDuplicated \nNode PointerKey\n......... Data Slot\nDense NodeModel Node\nBucketNode Type\nEntry TypeFigure 5: Structure of AFLI.\nwhich is called the duplicated node pointer. Since some keys\nare mapped to different but adjacent positions, for better\ngeneralization capability of the linear model, we assign\nthem to the same model child node, so that corresponding\npointers all point to the same address.\nTo make linear models produce fewer conflicts, we also scale the\npositions used to train linear models according to the scaling rela-\ntionship between keys.\nBucket: The bucket only consists of a short data array. The\nmaximum size of the data array is determined by the tail conflict\ndegreeğ·ğ›¾\nM, but will be kept within a preset threshold range. We\nprovide two kinds of buckets, the linear bucket (default) and the\nordered bucket.\nDense Node: The dense node also only consists of a data array.\nDifferent from the bucket, the dense node is generally a bit larger\nthan the bucket, but much smaller than the model node. The data\narray is an ordered and gapped array, the maximum number of\ngaps is set to ğ·ğ›¾\nM. Although in the Figure 5, the gap is represented\nby the empty space, we actually fill the slot with the data that is\nthe closest to the front of the gap. With such fillings, we do not\nneed an additional bitmap to indicate whether a position is empty\nor not, but directly compare adjacent positions to check whether\nstore the same data.\nAnalysis: Most of nodes in the index will be model nodes. Data\nstored in the model nodes are directly placed in the predicted posi-\ntions, which guarantees that the predictions are all precise in the\nmodel nodes. If the key space as a whole satisfies the near-uniform\ndistribution, but with many localities which means that keys in\nsmall sub key space are too close to be distinguished by a linear\nmodel, the index will create many buckets to handle local conflicts.\nWhen the index fails to build a model node as all keys in the node\nare too close (i.e., the slope of the fitted linear model is 0), the index\ndoes not further partition the key space but allocates a dense node\nto store them. The insight of designing buckets or dense nodes is\nto buffer keys without increasing the height of the index, until they\nhave enough difference to distinguish (dense nodes) or they can\nprovide enough information to build a linear model with better\ngeneralization capability (buckets).3.3.2 Operations of AFLI. In this section, we describe how our\nindex performs lookups, insertions, and the algorithm to rebuild a\nbucket or a dense node into a model node.\nQueries: To look up a key, the index recursively traverses the\ntree until the result is found. Firstly, the index feeds the queried\nkey into the root node, which can only be a model node or a dense\nnode, as the bucket is always extended from the model node. (1)\nIf the node is a model node, the queried key will be first fed into\nthe linear model. Based on the prediction of the linear model, we\ncheck the type of entry in the position based on two bitmaps. If it is\nan empty slot, the index doesnâ€™t contain the key. If it is a data slot,\nwe compare the stored key with the queried key. If it is a bucket\npointer, we search for the key further in the bucket. If it is a node\npointer, we do the same procedure in the child node. (2) If the node\nis a dense node, we use the binary search to find the result.\nIt is important to note that the predicted positions in model\nnodes are all precise positions, which means that there is no extra\nlocal search in model nodes, so that the query performance can be\nsignificantly improved. For queries in buckets, the linear search\nis enough to provide comparable efficiency. When looking up in\ndense nodes, we directly use the binary search for efficiency.\nInsertions: For inserting a pair of key and payload, starting\nfrom the root node, the index performs a recursive procedure based\non the node type,\nâ€¢If the key-payload pair is inputted into a model node, the\nlinear model first predicts a position based on the input key.\nConsidering the type of the entry in the predicted position,\n(1) If the entry is an empty slot, we directly store the key-\npayload pair in the position and modify corresponding\nbitmaps. (2) If the entry is a data slot, which means there is\nalready a key-payload pair, so we create a bucket to store\nthese two conflicted key-payload pairs, set the entry to the\nbucket pointer, and modify corresponding bitmaps. (3) If\nthe entry is a bucket pointer or a node pointer, we insert\nthe key-payload pair into the bucket or the child node.\nâ€¢For inserting the key-payload pair into a bucket, the key-\npayload pair will be directly appended to the tail of the\nstored data. If the bucket is in the ordered mode, the inser-\ntion will be performed as the insertion sorts.\nâ€¢To insert the key-payload pair into a dense node, the index\nfirst performs a binary search on the array. If the entry in\nthe position is an empty slot, we directly insert the key-\npayload pair. If the entry is a data slot, we shift the data to\nthe closest empty slot, then insert the key-payload pair.\nWhen the bucket or the dense node has no empty slots, we try to\nconvert it to a model node by a Modeling operation (more details\nin the next section).\nModelling the Bucket or the Dense Node: As shown in the\nFigure 6, although there are two types of nodes that need to be\nmodelled as a model node, the modelling process are the same\nexcept for sorting the bucket first, as the inputs can be formulated\nas an array of key-payload pairs. Therefore, given an array of key-\npayload pairs, we use Algorithm 3.2 to implement the modelling\noperation.\nIn Algorithm 3.2, we first try to build a linear model using the\nLinear Regression (Line 1). Then, we determine the node type based\n7\n\nğ“œğ“œ\nKey\n.........\nğ“œğ“œ.........\nğ“œğ“œ\n......... .........\n(a) Model  a bucket.\nğ“œğ“œ\nKey\n.........\nğ“œğ“œ\nğ“œğ“œ\n......... .........(b) Model a dense node.\n.........\nFigure 6: Modelling a bucket or a dense node.\non the prediction results. If the slope of the linear model is 0 or\nall keys are mapped to the same position, we build a dense node\nfor them without further partition on the key space (Line 2-4).\nThe reason is that those keys are too close so that the rounding\noperation rounds them to the same integer. If we successfully build\na linear model, we build a model node for all keys. We compute the\nconflict degrees ğ”‡of each predicted position (Line 6), and use it to\ndetermine whether store the key in the data slot, or build a bucket,\nor build a batch of keys in child nodes. For more precise placement,\nthe size of the entry array is the minimum value between the ğ›¼\ntimes the number of keys and the predicted size (Line 7). Then, we\niterate each predicted position, determining the entry type of each\nposition. If the conflict degree of a position is 1, we directly store the\nkey in the data slot (Line 10-13). If the conflict degree is larger than\n1 but smaller than the maximum size of the bucket ğ·ğ›¾\nMğ¿, we store\nthe conflicted keys within the position into a bucket (Line 14-17).\nOtherwise, for the case where the number of conflicted keys is larger\nthanğ·ğ›¾\nMğ¿, we collect the subsequent consecutive positions whose\nconflict degree is also larger than ğ·ğ›¾\nMğ¿, and allocate a new node\nto handle them (Line 18-21). After building the node, we duplicate\nthe node pointer and assign the address to other collected positions\n(Line 22). The modelling operation will be recursively performed.\nMore Operations :\nâ€¢BulkLoad. The bulkload operation first computes the tail\nconflict degree ğ·ğ›¾\nMğ¿, then follows the same procedure as\nthe modelling operation in Algorithm 3.2. The returned\nresult is the root node.\nâ€¢Update. An update for the payload can be finished by a\nlookup and an in-place update.\nâ€¢Delete. The deletion can be implemented by a lookup on\nthe deleted key and a modification on the corresponding\nnode. The modification on the model node is to unset the\ncorresponding bit in the bitmap. The modification on the\nbucket or the dense node is to overwrite the deleted key\nwith the following keys.Algorithm 3.2 Modelling ({âŸ¨ğ‘¥1,ğ‘£1âŸ©,...,âŸ¨ğ‘¥ğ‘›,ğ‘£ğ‘›âŸ©},ğ”«)\nInput: An array of sorted key-payload pairs {âŸ¨ğ‘¥1,ğ‘£1âŸ©,...,âŸ¨ğ‘¥ğ‘›,ğ‘£ğ‘›âŸ©}, the\nnode pointer ğ”«that points to the node storing all key-payload pairs,\nthe space amplification factor ğ›¼.\nOutput: The node pointer ğ”«of the model node.\n1:Build a linear model Mğ”«using the input keys and scaled positions.\n2:ifMğ”«.ğ‘==0or all keys are mapped to the same position then\n3:ğ”«.ğ‘ ğ‘–ğ‘§ğ‘’=ğ‘›+ğ·ğ›¾\nMğ¿;\n4: Allocate an array of size ğ”«.ğ‘ ğ‘–ğ‘§ğ‘’ toğ”«.E, and insert all key-payload\npairs, evenly gapped by a total of ğ·ğ›¾\nMğ¿gaps;\n5:else\n6: Compute the conflict degrees ğ·of each predicted position;\n7:ğ”«.ğ‘ ğ‘–ğ‘§ğ‘’=MIN(âŒŠğ‘›Â·ğ›¼âŒ‹,posğ‘™ğ‘ğ‘ ğ‘¡âˆ’posğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡+1);\n8: Allocate an array of size ğ”«.ğ‘ ğ‘–ğ‘§ğ‘’ toğ”«.E;\n9:ğ‘–=0;\n10: forposâˆˆall predicted positions do\n11: ifğ·[pos]==1then\n12: ğ”«.E[pos]=âŸ¨ğ‘¥ğ‘–,ğ‘£ğ‘–âŸ©;\n13:ğ‘–=ğ‘–+1;\n14: else ifğ·[pos]<ğ·ğ›¾\nMğ¿then\n15: Build a bucket ğ”Ÿof maximum size ğ·ğ›¾\nMğ¿, storing key-payload\npairs{âŸ¨ğ‘¥ğ‘–,ğ‘£ğ‘–âŸ©,...,âŸ¨ğ‘¥ğ‘–+ğ·[pos],ğ‘£ğ‘–+ğ·[pos]âŸ©};\n16: ğ”«.E[pos]= ğ”Ÿ;\n17:ğ‘–=ğ‘–+ğ·[pos];\n18: else ifğ·[pos]>=ğ·ğ›¾\nMğ¿then\n19: Iterate the subsequent positions posğ‘ ğ‘’ğ‘, whereğ·[posğ‘ ğ‘’ğ‘]>\nğ·ğ›¾\nMğ¿, and sum all conflict degrees ğ”‡ğ‘ ğ‘’ğ‘;\n20: Allocate a new node to ğ”«.E[pos];\n21: Modelling({âŸ¨ğ‘¥ğ‘–,ğ‘£ğ‘–âŸ©,...,âŸ¨ğ‘¥ğ‘–+ğ”‡ğ‘ ğ‘’ğ‘,ğ‘£ğ‘–+ğ”‡ğ‘ ğ‘’ğ‘âŸ©},ğ”«.E[pos]);\n22: For all positions from pos+1toposğ‘ ğ‘’ğ‘, set the pointer value\nof each position to ğ”«.E[pos];\n23:ğ‘–=ğ‘–+ğ”‡ğ‘ ğ‘’ğ‘;\n24: end if\n25: end for\n26:end if\n27:return ğ”«\n4 EVALUATION\n4.1 Experimental Setup\n4.1.1 Datasets and Workloads. We choose seven representative\ndatasets used in [ 6,20,42] to evaluate the effectiveness of the\nproposed NFL and index. For simplicity, all data sets consist of\nabout 200 million unique keys. The key type is â€™doubleâ€™, and the\npayload type is â€™int64â€™. The detailed information is as follows, (1)\nThe longitudes (LTD) dataset consists of the longitudes of locations\naround the world from Open Street Maps [ 31]. (2) The longlat (LLT)\ndataset consists of compound keys that combine longitudes and\nlatitudes from Open Street Maps by applying the transformation\nğ‘˜=180Â·FLOOR(ğ‘™ğ‘œğ‘›ğ‘”ğ‘–ğ‘¡ğ‘¢ğ‘‘ğ‘’)+ğ‘™ğ‘ğ‘¡ğ‘–ğ‘¡ğ‘¢ğ‘‘ğ‘’ to every pair of longitude\nand latitude. (3) The lognormal (LGN) dataset is a synthetic dataset\nwhose keys are sampled from a lognormal distribution with ğœ‡=0\nandğœ=2, multiplied by 109and rounded down to the nearest\ninteger. (4) The YCSB dataset is generated from the YCSB benchmark\n[4], whose keys representing user IDs. (5) The amazon (AMZN)\ndataset consists of book sale popularity data on the Amazon [ 35].\n(6) The facebook (FB) dataset is an upsampled version of a Facebook\n8\n\nLTD LLT LGN YCSB036 NFL  AFLI \n  LIPP  ALEX  PGM-Index  B-TreeThroughput (million ops/sec)\nLTD LLT LGN YCSB051015202530354045Throughput (million ops/sec)(a) Read-Only\nLTD LLT LGN YCSB05101520Throughput (million ops/sec) (b) Read-Heavy\nLTD LLT LGN YCSB0369Throughput (million ops/sec) (c) Write-Heavy\nLTD LLT LGN YCSB036Throughput (million ops/sec) (d) Write-Only\nAMZN FACE WIKI0510152025303540Throughput (million ops/sec)\n(e) Read-Only\nAMZN FACE WIKI05101520Throughput (million ops/sec) (f) Read-Heavy\nAMZN FACE WIKI0369Throughput (million ops/sec) (g) Write-Heavy\nAMZN FACE WIKI036Throughput (million ops/sec) (h) Write-Only\nFigure 7: Throughput of NFL and Baselines.\nuser ID dataset [ 36]. (7) The wikipedia (WIKI) dataset is Wikipedia\narticle edit timestamps [10].\nWe construct four types of workloads based on each kind of\ndataset. All workloads include two phases, the loading phase and\nthe running phase. For the loading phase, we use the bulk loading\noperation to load 50% key-payload pairs of a dataset. For the running\nphase, we generate requests based on different operation ratios. (1)\nThe read-only workloads only consist of query operations. (2) The\nread-heavy workloads consist of 80% query operations and 20%\ninsertion operations. (3) The write-heavy workloads consist of 20%\nquery operations and 80% insertion operations. (4) The write-only\nworkloads only consist of insertion operations. For all workloads,\nthe requested key is sampled from the given dataset based on a\nZipfian distribution, and all inserted keys are in the key space\nconsisting of all bulk-loaded keys, which means all insertions are\nknown-key-space insertions. We run each workload for 5 times,\nand collect the averaged Throughput, Tail Latency, Index Size .\n4.1.2 Baselines. We compare our NFL with existing state-of-the-\nart indexes, (1) LIPP [ 42], an updatable learned index with precise\npositions, which creates new nodes to handle conflicted keys. (2)\nALEX [ 6], an in-memory, updatable learned index, which uses gaps\nto handle new insertions. (3) PGM-Index [ 12], a fully-dynamic\ncompressed learnded index with provable worst-case bounds. (4)\nB-Tree [ 26], an efficient B-Tree implemented by Google. These\nsource codes are publicly available, and we download their codes\nand evaluate them with their default hyper-parameters.\n4.1.3 Environment and Parameters. We implement our proposed\nNFL and AFLI in C++, and perform the inference of NF by Intel Math\nKernel Library [ 25]. We perform our evaluation via single-thread,\nand compile all source codes with GCC 9.3.0 in O3 optimization\nmode. All experiments are conducted on an Ubuntu 20.04 Linuxmachine with a 2.9 GHz Intel Core i7-10700 (8 cores) CPU and 64GB\nmemory.\nFor NF, we train a modified B-NAF in PyTorch. The B-NAF is\nset to two layers, two input dimensions, two hidden dimensions,\nand a normal distribution with the variance of 1016as the latent\ndistribution. The training step is performed on the NVIDIA GeForce\nRTX 3080 with 10 GB GPU memory and 64 GB main memory. We\nonly sample 10% bulk-loaded keys for three times to train the NF.\nWe set the batch size to 256. For the proposed learned index, we\nlimit the maximum size of buckets to no more than 6.\n4.2 Throughput\nFigure 7 shows the average throughput of NFL and other baselines\non different workloads. To show the effectiveness of NF, we also\nevaluate our proposed AFLI. Since transforming the workloads with\nsmall conflict degrees (i.e., YCSB, AMZN, WIKI) increases the tail\nconflict degree, our NFL determines not to use NF.\nRead-Only Workloads. The performance on read-only workloads\nreflects the approximation quality during the loading phase. Fig-\nure 7 (a) and (e) shows that our proposed NFL can achieve 2.34x,\n2.46x, 3.82x, and 7.45x improvements on the throughput on average\ncompared to LIPP, ALEX, PGM-Index, and B-Tree, respectively. Es-\npecially, for workloads with large conflict degrees (i.e., LLT and FB),\nour proposed NFL can achieve up to 2.41x, 3.70x higher throughput\nthan LIPP, and ALEX, respectively. Such improvements benefit from\nboth the Numerical NF and the proposed AFLI. The NF transforms\nthe keys into another key space, where keys are more near-uniform\ndistributed, forming a more linear CDF curve. Therefore, based\non the transformed key space, the learned index can use less lin-\near models to make a better approximation, and the hierarchical\nstructure is much lower, leading to a significant improvement on\nthroughput.\n9\n\nLTD LLT LGN YCSB02040 NFL-Index  NFL-Trans  AFLI \n  LIPP   ALEX  PGM-Index  B-TreeThroughput (million ops/sec)\nLTD LLT LGN YCSB050100150200250300350P99 Latency (ns)(a) Read-Only\nLTD LLT LGN YCSB0100200300400500600700800P99 Latency (ns) (b) Read-Heavy\nLTD LLT LGN YCSB020040060080010001200P99 Latency (ns) (c) Write-Heavy\nLTD LLT LGN YCSB020040060080010001200P99 Latency (ns) (d) Write-Only\nAMZN FACE WIKI050100150200250300350P99 Latency (ns)\n(e) Read-Only\nAMZN FACE WIKI0100200300400500600700800P99 Latency (ns) (f) Read-Heavy\nAMZN FACE WIKI020040060080010001200P99 Latency (ns) (g) Write-Heavy\nAMZN FACE WIKI020040060080010001200P99 Latency (ns) (h) Write-Only\nFigure 8: Tail Latency (P99) of NFL and Baselines.\nCompared to the experimental results of the proposed AFLI on\nworkloads with large conflict degrees, the NF can revisionachieve\n2.25x improvement on the throughput. However, on workloads\nwith a little reduction on the tail conflict degree (e.g., LTD, LGN),\nNF degrades the throughput. This is due to that compared to the\nimprovements of the performance on transformed keys, the online\ninference of NF introduces a non-negligible overhead. On YCSB,\nAMZN, and WIKI, the NFL disables the NF due to that the distribu-\ntion transformation does not reduce the tail conflict degree, so the\nNFL achieves almost the same performance as the proposed AFLI.\nRead-Write Workloads. The performance on the read-write work-\nloads (i.e., read-heavy and write-heavy workloads) shows the ad-\njustment efficiency of the learned index during the loading phase.\nFigure 7 (b) and (f) shows that NFL can still achieve the same im-\nprovements on the throughput by 72.22%, 101.05%, 611.48%, and\n389.45% on average compared to LIPP, ALEX, PGM-Index, and\nB-Tree, respectively. Although the improvements on write-heavy\nworkloads degrade, NFL can still improve the throughput by 29.10%,\n39.28%, 50.88% and 162.92% on average compared to LIPP, ALEX,\nPGM-Index, and B-Tree. The reason for the drop on the improve-\nments is that more keys are buffered on the bucket, and most of\nthe buckets are almost full, resulting in a decrease in linear search\nperformance on the bucket. However, compared to other learned\nindexes which perform lots of complex and expensive adjustments\n(reflected in Figure 8), NFL uses tiny buckets to buffer keys and\nlater builds linear models for the buckets which is a quite cheap\nadjustments.\nDifferent with the high throughput on the read-only workloads\nof LTD and LGN, proposed AFLIâ€™s performance on the read-write\nworkloads of LTD and LGN is comparable to NFL. The reason lies\nin that the former performance of proposed AFLI is like the case of\noverfitting in Machine Learning, which means that almost all linear\nmodels are well fitted and there are fewer empty slots prepared for\nnew insertions, thus new insertions are more likely inserted intothe buckets. However, since the NF have much better generalization\ncapability than linear models, the data array of each node will be\nlarger than before, NFL can produce less conflict degrees and con-\nstruct a lower hierarchical index, which maintains the comparable\nperformance.\nWrite-Only Workloads. Figure 7 (d) and (h) show that except for\n21.43% degradation compared to PGM-Index, our proposed NFL\nimproves the throughput by 22.65%, 28.30%, and 131.58% compared\nto LIPP, ALEX, and B-Tree, respectively. The high insertion perfor-\nmance of PGM-Index benefits from the LSM-Tree structure, where\na small buffer of size 128 is used to receive new insertions. However,\nPGM-Index needs to perform the compaction operations period-\nically which is a quite expensive operation. The long tail latency\nin Figure 8 (d) and (h) also proves this point. Since the tail conflict\ndegree barely changes after the running phase (see Table 3 below),\nthe tiny buffer in NFL is enough to absorb locally conflicted keys\nwithout introducing more internal adjustments, thus NFL can still\nmaintain high throughput after insertions. Moreover, with the trans-\nformation of NF, all keys in the key space become more uniform,\nleading to the reduction on the tail conflict degree (e.g., LTD, LGN),\nthus more keys can are buffered in the buckets and the performance\nof NFL can be further improved.\n4.3 Tail Latency\nTo further display the impact of internal adjustments, we also eval-\nuate our proposed NFL and other baselines on the 99% (P99) latency.\nIn each run, we collect the latency of each batch of operations, sort\nthe latencies in the ascending order, and report the 99-th percentile\nbatch latency divided by the batch size in Figure 8. We also collect\nthe 99.99% (P99.99) latency and the maximum (max) latency on\nsome representative workloads.\nRead-Only Workloads. The tail latency on the read-only work-\nloads indicates the worst case of querying the learned index, corre-\nsponding to the approximation quality of the most conflicted sub\n10\n\nLTD LLT LGN YCSB02040 NFL-Index  NFL-Trans  AFLI \n  LIPP   ALEX  PGM-Index  B-TreeThroughput (million ops/sec)\nLTD LLT FACE WIKI04008002400P99.99 Latency (ns)5 us 19 us 3 us 19 us18 us\n2us19 us19 us(a) Read-Heavy\nLTD LLT FACE WIKI040080032003600Max Latency (ns)15 us 5 ms 51 us 6 ms282us\n3us437us6 ms (b) Read-Heavy\nFigure 9: P99.99 latency and max latency of NFL and Base-\nlines.\nkey space. Figure 8 (a) and (e) shows that NFL can reduce the P99\nlatency by 58.68%, 32.89%, 62.73%, and 80.77% compared to LIPP,\nALEX, PGM-Index, and B-Tree, respectively. Meanwhile, on the\nworkloads with large conflict degrees (e.g., LLT, FB), NFL can still\nkeep the P99 latency under 80 ns, while others produce over 100ns\nP99 latency due to expensive internal adjustments. The main reason\nfor short P99 latency is that the normalizing flow can alleviate the\nworst case, reducing the conflict differences between positions and\navoiding a large amount of conflicts in certain positions. The long\nP99 latency of proposed AFLI on the workloads LLT and FB also\nprove that without the transformation of NF, the learned index can\nbe an unbalanced tree due to locally large conflicts, thus querying\non those deep leaf nodes in the tree will introduce long latency.\nRead-Write Workloads. For read-write workloads (Figure 8 (b),\n(c), (f), and (g)), NFL can reduce the P99 latency by 26.64%, 45.05%,\n59.49%, and 65.31% compared to LIPP, ALEX, PGM-Index, and B-\nTree, respectively. The long tail latency on such read-write work-\nloads is mainly caused by two aspects, complex internal adjustments\nand querying on deep leaf nodes. The latter is the same as the case\non read-only workloads. As the amount of insertions increases,\nthe cost and frequency of internal adjustments for our proposed\nlearned indexes, LIPP, and ALEX also increases, thus resulting in\nlong P99 latency. The reduction on P99 latency for PGM-Index\nfrom read-heavy workloads to write-heavy workloads is due to the\npoor query performance and the high insertion performance of the\nLSM-Tree structure used in PGM-Index.\nWrite-Only Workloads. Figure 8 (d) and (h) show that NFL can\nreduce the P99 latency by 2.26%, 27.92%, and 50.48% on write-only\nworkloads compared to LIPP, ALEX, and B-Tree, respectively. Due\nto the LSM-Treeâ€™s high writing performance, NFL produce 32.09%\nhigher P99 latency than that of PGM-Index. ALEX performs expen-\nsive internal adjustments (e.g., merging or splitting nodes). Mean-\nwhile, the cost of the shifting operations in ALEX will increase\nas the number of keys in a ALEXâ€™s node increases. LIPP produces\ncomparable P99 latency to our proposed NFL, since both indexes\nadopt the precise placement without shifting. The difference be-\ntween LIPP and NFL is that LIPP directly builds a new node for\nconflicted keys while NFL uses a bucket to buffer the conflicted\nkeys first. This building operations is a little expensive compared\nto appending to a data array, thus NFL can produce a bit lower P99\nlatency than LIPP.\nP99.99 Latency and Max Latency. We also collect the P99.99\nlatency and the max latency in one run. Due to the space limitation,Table 3: Tail conflict degrees of workloads. The suffix \"(L)\"\nand \"(R)\" represent the results after the loading phase and\nrunning phase, respectively. Rows in orange are results after\nthe transformation of NF.\nLTD LLT LGN YCSB AMZN FB WIKI\nTail (L) 8 146 14 3 4 386 2\nTail (R) 7 147 13 3 4 454 1\nTail (L) 4 4 4 4 4 4 4\nTail (R) 4 5 4 4 4 4 4\nwe only present the results on the representative workloads. Figure\n9 (a) and (b) shows that our proposed NFL can produce the lowest\nlatency both on P99.99 latency and max latency, which also proves\nthe effectiveness of the NF.\n4.4 Other Results\n4.4.1 Conflict Degree. The conflict degree reflects the transfor-\nmation quality of NF. Table 3 shows that with the transformation\nof NF, the conflict degree can be significantly reduced. Although\nNF only uses 10% of bulk-loaded keys, after inserting around 100\nmillion new keys, the tail conflict degree is around 4. For datasets\nwith low conflict degrees (e.g., YCSB, WIKI), the NF increases the\nconflict degrees. This is due to the limitation of NF, which means\nthat in our setting, the transformation quality of the NF has almost\nreached to its upper limit. But if we can design a better NF, the\nconflict degrees can be further reduced.\n4.4.2 Bulk Loading Time. Figure 10 demonstrates the time cost\nof loading phase. For our proposed NFL, the time cost includes\nthe time cost of the online transformation of bulk-loaded keys and\nthe time cost of the bulk loading of the index. Our proposed AFLI\nachieves the fastest bulk loading compared to other baselines except\nfor PGM-Index. Although NFL takes 77% time to transform keys\nand requires 2.25x, 0.86x, 2.81x time to bulk load compared to LIPP,\nALEX, and B-Tree, NFL can make a better approximation of the\nCDF leading to a faster loading on the learned index. Figure 7 also\nproves that the built learned index can deliver high performance\nespecially for highly non-linear workloads. Compared to PGM-\nIndex, the bulk-loading time cost of NFL is much larger. However,\nthe overall performance of NFL is much higher than PGM-Index. In\naddition, the average time cost of NFL to bulk load 100 million keys\nis 13.42 seconds, which is acceptable and worthwhile considering\nthe high performance.\n4.4.3 Index Size. Figure 11 shows the normalized final index size\nof existing learned indexes after the running phase of write-heavy\nworkloads. Since the great improvements of the state-of-the-art\nlearned indexes benefit from precise placements which require lots\nof gaps [ 6] in the data array, we evaluate the overall index size,\nincluding the sum size of allocated gaps, rather than the model size\nin the learned index. The index size of NFL is 2.26x, 3.1x times than\nthe index size of ALEX and PGM-Index, respectively. However, the\nindex size of NFL is only 0.51 of LIPPâ€™s index size. Both NFL and\nLIPP create more space to support precise placements, but NFL\n11\n\nLTD LLT LGN YCSB02040 NFL-Index  NFL-Trans  AFLI \n  LIPP   ALEX  PGM-Index  B-TreeThroughput (million ops/sec)\nLTD LLT LGN YCSB051015202530Bulk Loading (s)\nAMZN FACE WIKI051015202530Bulk Loading (s)Figure 10: Bulk loading of NFL and Baselines.\nmakes a better trade-off between allocating more space and fitting\nlinear models better.\n5 RELATED WORK\n5.1 Learned index\nRecent work on the Learned Index [ 22] brings a new direction\nto the indexing field, which leverage machine learning models to\npredict the mapping between keys and positions. Inspired by [ 22], a\nseries works fully explore the potential of machine learning models\non the indexing problems. Both FITing-Tree [ 13] and PGM-Index\n[12] approximate the CDF using piece-wise linear functions to\nrestrict the prediction error in a given bound. Without a given\nerror bound , ALEX [ 6] places data in the predicted position of\na gapped array, which makes a better approximation of the CDF.\nTo support more precise placements, LIPP [ 42] defines the conflict\ndegree to reflect the approximation quality and directly create new\nnodes for the conflicted keys. LIPP [ 42] achieves the state-of-the-\nart performance compared to previous learned indexes. However,\nour work indicates that the performance of learned indexes can\nbe further improved by applying a distribution transformation on\nthe input keys, which fundamentally improve the approximation\nquality of the CDF. There are several works focusing on improving\nthe approximation by Gap Insertion [ 24], Log-Error Regression [ 11].\nThey donâ€™t consider the impact of different key distributions. Other\nworks investigate the effectiveness of learned indexes on multi-\ndimensions or spatial queries [ 7,23,30,39], construction efficiency\n[21], string index [ 40], multi-threads [ 38]. They are different from\nour scope. Other works also combine the learned index with modern\napplications [ 5,37], new hardware [ 3,27]. Our works can also be\nadapted to their scope.\n5.2 Normalizing Flows\nNormalizing flows are popularised in the context of variational\ninference [ 34] and density estimation [ 8]. The natural and most\nobvious use of normalizing flows is to perform density estimation,\nwhich is the same scope of the problem in learned indexes. The\nkey point of normalizing flows is how to design the generators,\nin terms of invertible transformation and efficient computations.\nDinh et al. introduced a coupling method to improve the expressive-\nness of the transformation [ 8]. The key of [ 8] is how to partition ğ‘¥,\nwhich inspires a series of work [ 9,17]. However, such a coupling\nmethod requires the dimension of input keys to be at least greater\nthan 2, which is not suitable in our problem. Another widely used\nLTD LLT LGN YCSB036912151821242730333639424548 NFL-Index  AFLI \n  LIPP  ALEX  PGM-IndexThroughput (million ops/sec)\nLTD LLT LGN YCSB02468Index Size(a) Write-Heavy\nLTD LLT LGN YCSB02468Index Size (b) Write-Only\nAMZN FACE WIKI02468Index Size\n(c) Write-Heavy\nAMZN FACE WIKI02468Index Size (d) Write-Only\nFigure 11: Index size of NFL and Baselines.\nflow structure is auto-regressive flow, first proposed by Kingma\net al. in IAF [ 18]. Huang et al. replace the affine univariate trans-\nformations of MAF [ 32] and IAF [ 18] with a more general class\nof invertible univariate transformations called monotonic neural\nnetworks, and achieve a better performance. B-NAF [ 2] improves\nthe structure of NAF by using a single feed-forward network to\nmodel the bijections, and becomes much more compact than NAF\nwhile remaining comparable universal approximation. For the first\ntime, we apply the state-of-the-art auto-regressive flows to improve\nthe approximation of learned indexes.\n6 CONCLUSIONS\nIn this paper, we address the approximation problem by applying a\ndistribution transformation. We present a normalizing-flow-learned\nindex framework to transform the key space for better approxi-\nmation on the CDF. We also introduce a 1D-friendly normalizing\nflow to achieve the distribution transformer. We further propose\na lightweight and precise learned index to support efficient index-\ning operations. Experimental results on representative workloads\ndemonstrate the effectiveness of our proposed framework and our\nproposed index.\nFor the future work, we plan to further investigate the poten-\ntial of using normalizing flows, including co-design on normaliz-\ning flows and learned indexes, using normalizing flows on multi-\ndimension indexing and so on. Besides, it is also worthwhile to\nfurther accelerate the inference of normalizing flows.\n12\n\nREFERENCES\n[1]Jie An, Siyu Huang, Yibing Song, Dejing Dou, Wei Liu, and Jiebo Luo. 2021.\nArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) . 862â€“871.\n[2] Nicola De Cao, Wilker Aziz, and Ivan Titov. 2019. Block Neural Autoregressive\nFlow. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial\nIntelligence (UAI) , Vol. 115. 1263â€“1273.\n[3] Leying Chen and Shimin Chen. 2021. How Does Updatable Learned Index Per-\nform on Non-Volatile Main Memory?. In Proceedings of the 37th IEEE International\nConference on Data Engineering Workshops (ICDEW) . 66â€“71.\n[4] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking cloud serving systems with YCSB. In Proceedings of\nthe 1st ACM Symposium on Cloud Computing (SoCC) . 143â€“154.\n[5] Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth,\nAndrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. 2020. From WiscKey\nto Bourbon: A Learned Index for Log-Structured Merge Trees. In Proceedings of\nthe 14th USENIX Symposium on Operating Systems Design and Implementation\n(OSDI) . 155â€“171.\n[6]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In Proceedings of the 2020 International Conference on Management of Data\n(SIGMOD) . 969â€“984.\n[7]Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed\nWorkloads. Proc. VLDB Endow. 14, 2 (2020), 74â€“86.\n[8]Laurent Dinh, David Krueger, and Yoshua Bengio. 2015. NICE: Non-linear\nIndependent Components Estimation. In Proceedings of the 3rd International\nConference on Learning Representations (ICLR) , Yoshua Bengio and Yann LeCun\n(Eds.).\n[9] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. 2017. Density estimation\nusing Real NVP. In Proceedings of the 5th International Conference on Learning\nRepresentations (ICLR) .\n[10] Wikimedia downloads. [n.d.]. http://dumps.wikimedia.org.\n[11] Martin Eppert, Philipp Fent, and Thomas Neumann. 2021. A Tailored Regression\nfor Learned Indexes: Logarithmic Error Regression. In Proceedings of the Fourth\nInternational Workshop on Exploiting Artificial Intelligence Techniques for Data\nManagement (aiDM) . 9â€“15.\n[12] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proc. VLDB Endow.\n13, 8 (2020), 1162â€“1175.\n[13] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-aware Index Structure. In Proceedings of the\n2019 International Conference on Management of Data (SIGMOD) . 1189â€“1206.\n[14] Georgios Giannikis, Gustavo Alonso, and Donald Kossmann. 2012. SharedDB:\nKilling One Thousand Queries With One Stone. Proc. VLDB Endow. 5, 6 (2012),\n526â€“537.\n[15] Georgios Giannikis, Darko Makreshanski, Gustavo Alonso, and Donald Koss-\nmann. 2014. Shared Workload Optimization. Proc. VLDB Endow. 7, 6 (2014),\n429â€“440.\n[16] Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron C. Courville.\n2018. Neural Autoregressive Flows. In Proceedings of the 35th International\nConference on Machine Learning (ICML) , Vol. 80. 2083â€“2092.\n[17] Diederik P. Kingma and Prafulla Dhariwal. 2018. Glow: Generative Flow with\nInvertible 1x1 Convolutions. In Advances in Neural Information Processing Systems\n31: Annual Conference on Neural Information Processing Systems 2018 (NeurIPS) .\n10236â€“10245.\n[18] Durk P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and\nMax Welling. 2016. Improved variational inference with inverse autoregressive\nflow. Advances in Neural Information Processing Systems 29 (2016), 4743â€“4751.\n[19] Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes.\nInProceedings of the 2nd International Conference on Learning Representations\n(ICLR) .\n[20] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2019. SOSD: A Benchmark for Learned\nIndexes. CoRR (2019).\n[21] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management (aiDM) . 5:1â€“5:5.\n[22] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data (SIGMOD) . 489â€“504.\n[23] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A\nLearned Index Structure for Spatial Data. In Proceedings of the 2020 International\nConference on Management of Data (SIGMOD) . 2119â€“2133.[24] Yaliang Li, Daoyuan Chen, Bolin Ding, Kai Zeng, and Jingren Zhou. 2021. A\nPluggable Learned Index Method via Sampling and Gap Insertion. CoRR (2021).\n[25] Intel Math Kernel Library. [n.d.]. https://www.intel.com/content/www/us/en/\ndeveloper/tools/oneapi/onemkl.html.\n[26] The C++ B-Tree library implemented by Google. [n.d.]. https://code.google.com/\narchive/p/cpp-btree.\n[27] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang.\n2021. APEX: A High-Performance Learned Index on Persistent Memory. CoRR\n(2021).\n[28] Darko Makreshanski, Georgios Giannikis, Gustavo Alonso, and Donald Koss-\nmann. 2018. Many-query join: efficient shared execution of relational joins on\nmodern hardware. VLDB J. 27, 5 (2018), 669â€“692.\n[29] Darko Makreshanski, Jana Giceva, Claude Barthels, and Gustavo Alonso. 2017.\nBatchDB: Efficient Isolated Execution of Hybrid OLTP+OLAP Workloads for\nInteractive Applications. In Proceedings of the 2017 ACM International Conference\non Management of Data (SIGMOD) . 37â€“50.\n[30] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. In Proceedings of the 2020 International Conference\non Management of Data (SIGMOD) . 985â€“1000.\n[31] OpenStreetMap on Amazon AWS. [n.d.]. https://registry.opendata.aws/osm/.\n[32] George Papamakarios, Iain Murray, and Theo Pavlakou. 2017. Masked Autore-\ngressive Flow for Density Estimation. In Advances in Neural Information Process-\ning Systems 30: Annual Conference on Neural Information Processing Systems 2017\n(NeurIPS) . 2338â€“2347.\n[33] Robin Rehrmann, Carsten Binnig, Alexander BÃ¶hm, Kihong Kim, Wolfgang\nLehner, and Amr Rizk. 2018. OLTPShare: The Case for Sharing in OLTP Work-\nloads. Proc. VLDB Endow. 11, 12 (2018), 1769â€“1780.\n[34] Danilo Jimenez Rezende and Shakir Mohamed. 2015. Variational Inference with\nNormalizing Flows. In Proceedings of the 32nd International Conference on Machine\nLearning (ICML) , Francis R. Bach and David M. Blei (Eds.), Vol. 37. 1530â€“1538.\n[35] Amazon sales rank data for print and kindle books. [n.d.]. https://www.kaggle.\ncom/ucffool/amazon-sales-rank-data-for-print-and-kindle-books.\n[36] Peter Van Sandt, Yannis Chronis, and Jignesh M. Patel. 2019. Efficiently Searching\nIn-Memory Sorted Arrays: Revenge of the Interpolation Search?. In Proceedings\nof the 2019 International Conference on Management of Data (SIGMOD) . 36â€“53.\n[37] Mihail Stoian, Andreas Kipf, Ryan Marcus, and Tim Kraska. 2021. PLEX: Towards\nPractical Learned Indexing. CoRR (2021).\n[38] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: a scalable learned index for multicore\ndata storage. In Proceedings of the 25th ACM SIGPLAN Symposium on Principles\nand Practice of Parallel Programming (PPoPP) . 308â€“320.\n[39] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned Index for\nSpatial Queries. In Proceedings of the 20th IEEE International Conference on Mobile\nData Management (MDM) . 569â€“574.\n[40] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. 2020. SIndex: a\nscalable learned index for string keys. In Proceedings of the 11th ACM SIGOPS\nAsia-Pacific Workshop on Systems (APSys) . 17â€“24.\n[41] Valentin Wolf, Andreas Lugmayr, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. 2021. DeFlow: Learning Complex Image Degradations From Unpaired\nData With Conditional Flows. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) . 94â€“103.\n[42] Jiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow. 14, 8\n(2021), 1276â€“1288.\n13",
  "textLength": 71050
}