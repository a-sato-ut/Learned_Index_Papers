{
  "paperId": "08c9e1e5dc1578fdf09ad9be1ecd6424a708582f",
  "title": "HTAP Databases: A Survey",
  "pdfPath": "08c9e1e5dc1578fdf09ad9be1ecd6424a708582f.pdf",
  "text": "1\nHTAP Databases: A Survey\nChao Zhang, Guoliang Li*, Fellow ,IEEE , Jintao Zhang, Xinning Zhang, Jianhua Feng\nAbstract —Since Gartner coined the term, Hybrid Transactional and Analytical Processing (HTAP), numerous HTAP databases have\nbeen proposed to combine transactions with analytics in order to enable real-time data analytics for various data-intensive applications.\nHTAP databases typically process the mixed workloads of transactions and analytical queries in a unified system by leveraging both a\nrow store and a column store. As there are different storage architectures and processing techniques to satisfy various requirements of\ndiverse applications, it is critical to summarize the pros and cons of these key techniques. This paper offers a comprehensive survey of\nHTAP databases. We mainly classify state-of-the-art HTAP databases according to four storage architectures: (a) Primary Row Store\nand In-Memory Column Store; (b) Distributed Row Store and Column Store Replica; (c) Primary Row Store and Distributed In-Memory\nColumn Store; and (d) Primary Column Store and Delta Row Store. We then review the key techniques in HTAP databases, including\nhybrid workload processing, data organization, data synchronization, query optimization, and resource scheduling. We also discuss\nexisting HTAP benchmarks. Finally, we provide the research challenges and opportunities for HTAP techniques.\nIndex Terms —HTAP databases, Data organization, Data synchronization, Query optimization, Resource scheduling, Benchmarks\n✦\n1 I NTRODUCTION\nHYbrid Transactional, and Analytical Processing\n(HTAP) was defined by Gartner [98], [35]. Since then,\nHTAP techniques have been deployed in various data-\nintensive applications, e.g., banking and finance [87], E-\ncommerce [137], and fraud detection [106]. The Gartner\nreport envisioned that, by 2024, HTAP techniques will be\nwidely adopted in numerous business applications that en-\ntail real-time data analytics. Compared with the traditional\ndata processing pipeline that processes transactions and\nanalytical queries separately, HTAP architecture enables a\nunified system that not only can handle online transactional\nprocessing (OLTP) efficiently, but also can perform online\nanalytical processing (OLAP) concurrently [35]. Such an ar-\nchitecture aims to eliminate the need for an explicit Extract-\nTransform-Load (ETL) process, thereby enabling real-time\ndata analytics on transaction data. For instance, HTAP\ndatabases allow entrepreneurs in retail applications to an-\nalyze the latest transaction data in real-time, and then they\ncan roll out promotional sales based on the gained insights.\nIn finance applications, HTAP databases enable vendors\nto process customer transactions efficiently while detecting\nfraudulent transactions simultaneously [99].\nOver the last decade, we have witnessed the emergence\nand evolvement of various HTAP databases [17], [24], [28],\n[38], [45], [57], [60], [74], [78], [83], [86], [102], [103], [108],\n[118], [137]. Since it is well recognized that a row store\nis ideal for OLTP workloads, and a column store is better\nsuited for OLAP workloads [32], [57], HTAP databases\nmainly adopt a dual-store architecture that leverages both a\nrow store and a column store. However, different categories\nof HTAP databases adopt disparate storage strategies and\nprocessing techniques to cater to various applications. For\ninstance, it depends on whether OLTP or OLAP has a\nhigher priority for the applications (e.g., OLTP is the first\ncitizen for banking scenarios while OLAP dominates the\nanalytical reporting). It also depends on the requirements of\navailability, scalability, performance, and data freshness [19]specified in the service level agreements (SLAs) [38] (e.g.,\nlarge-scale E-commerce applications require high scalability\nwhile banking and finance applications demand high data\nfreshness). Nevertheless, HTAP databases must strike a\ntrade-off between performance isolation and data freshness\nwhen handling the mixed workloads of OLTP and OLAP .\nThe main reason is that the two types of workloads exhibit\na completely different computing pattern and can intervene\nwith each other, i.e., OLTP workloads [121] are update-\nheavy and short-lived while OLAP workloads [119] are\nread-heavy and bandwidth-intensive. Consequently, HTAP\ndatabases cannot guarantee the metrics of query throughput\nand data freshness simultaneously, albeit with a unified\narchitecture. As different applications deliver disparate re-\nquirements, it is critical to study and understand the pros\nand cons of HTAP databases under various architectures.\nThere are five main challenges that HTAP databases\nneed to address, including (C1) hybrid workload process-\ning, (C2) data organization, (C3) data synchronization, (C4)\nquery optimization, and (C5) resource scheduling.\nChallenge 1. Hybrid Processing Challenge . The first\nchallenge is to process mixed workloads of OLTP and\nOLAP efficiently while maintaining a high data freshness.\nAs there is a trade-off between performance isolation and\ndata freshness, it is challenging for HTAP databases to bal-\nance performance and freshness. Hence, these systems must\nmake a compromise on either performance isolation or data\nfreshness, depending on the specific HTAP applications.\nChallenge 2. Data Organization Challenge . The second\nchallenge is to organize the data adaptively for HTAP\nworkloads with high throughput and low storage costs.\nThere are two rules of thumb: (i) the row store is suitable\nfor OLTP workloads and (ii) the column store is ideal for\nOLAP queries. However, storing a copy of the entire data\nfor both formats leads to high storage overhead (i.e., space\ncost and maintenance cost). Therefore, HTAP systems need\nto make wise decisions for data organization (e.g., row-wise\nor column-wise data layout) to reduce the storage overhead\nwhile delivering high system performance.arXiv:2404.15670v1  [cs.DB]  24 Apr 2024\n\n2\nHybrid ProcessingData OrganizationData SynchronizationQuery OptimizationResource SchedulingData GenerationExecution RuleMetricsFreshness-drivenWorkload-drivenDual-StoreCopy-on-WriteHybrid ScanRow Store with Column SelectionAdaptive Hybrid StoreHTAP IndexingIn-Memory Delta MergingLog-based Delta MergingCPU/GPU AccelerationHTAP ArchitecturesHTAP Techniques\nHTAP BenchmarksHTAP WorkloadPrimary Row Store + In-Memory Column StoreDistributed Row Store + Column Store ReplicaPrimary Row Store+Distributed In-Memory Column StorePrimary Column Store + Delta Row StoreMVCC\nFig. 1. An Overview of HTAP Architectures, Techniques, Benchmarks\nChallenge 3. Data Synchronization Challenge. The\nthird challenge is to decide when to synchronize the delta\ndata from the transactional store to the analytical store for\nhigh data freshness while keeping high throughput and\nscalability. On the one hand, immediately merging the delta\ndata to the analytical store can keep high data freshness.\nHowever, it can greatly affect the performance due to the\nmerging overhead. On the other hand, merging the delta\ndata on demand can improve the throughput but lead to low\ndata freshness. Hence, HTAP databases need to synchronize\nthe data adaptively.\nChallenge 4. Query Optimization Challenge . The\nfourth challenge is to optimize the queries with both a row\nstore and a column store. In the HTAP databases, a query\ncan be executed against either the row store or the column\nstore; thus the query optimizer must judiciously decide\nwhether the row-based execution or the columnar execution\nis more beneficial. However, it is challenging to generate\nan optimal query plan in a large search space. Therefore,\nthe HTAP optimizer must balance the trade-off between the\nplanning time and query execution latency.\nChallenge 5. Resource Scheduling Challenge. The fifth\nchallenge is to schedule the resources (e.g., CPU threads\nand memory) for OLTP and OLAP instances effectively\nfor high throughput and data freshness. On the one hand,\nassigning more resources to OLAP workloads favors high\nquery throughput but may block the OLTP threads due\nto the limited bandwidth. On the other hand, scheduling\nmore resources to OLTP workloads can accelerate transac-\ntion processing but may lead to low data freshness. Hence,\nthe HTAP resource scheduler must balance the trade-off\nbetween performance isolation and data freshness as well.In this work, we provide a comprehensive survey of\nHTAP databases in three aspects, including HTAP archi-\ntectures, techniques, and benchmarks. Figure 1 presents an\noverview of HTAP-related techniques. It gives a taxonomy\nof HTAP architectures and HTAP techniques, respectively.\nIt also presents four components of HTAP benchmarks. We\npay particular attention to how existing approaches address\nthe above-mentioned challenges, respectively.\n1.1 HTAP architectures\nWe mainly study HTAP databases that utilize row stores\nand column stores together to handle the mixed workloads\nof OLTP and OLAP efficiently in a single database system.\nBased on the storage strategies and processing paradigm,\nwe divide their architectures into four categories as follows:\n(1) Primary Row Store+In-Memory Column Store. This\ncategory of HTAP databases leverages a primary row store\nas the basis for OLTP workloads and processes OLAP\nworkloads with an in-memory column store. Updates are\nappended to the delta store, which will be periodically\nmerged with the column store. We review four represen-\ntatives: Oracle [57], SQL Server [60], and DB2 BLU [108].\n(2) Distributed Row Store+Column Store Replica. This cat-\negory relies on a distributed architecture to support HTAP .\nThe master node handles the read-write transactions and\nasynchronously replicates the logs to the slave nodes. The\nprimary storage relies on a distributed row store, and some\nslave nodes will be chosen as column-store servers for query\nacceleration. We introduce two representatives: TiDB [45]\nand F1 Lightning [136].\n(3) Primary Row Store+Distributed In-Memory Column\nStore. This type of database utilizes a primary with a dis-\ntributed in-memory column store (IMCS) to enable HTAP .\nWe present a representative: MySQL Heatwave [83].\n(4) Primary Column Store+Delta Row Store. This category\nof databases utilizes an in-memory column store as the\nbasis for OLAP , and handles OLTP with a delta row store,\nwhich will be eventually merged into the column store. We\nintroduce SAP HANA [118] and Hyper [86].\nOther HTAP architectures. We also review other types of\nHTAP architectures that complement the major archi-\ntectures, including (i) row-only HTAP architectures; (ii)\ncolumn-only HTAP architectures; (iii) Spark-based HTAP\narchitectures; and (iv) cloud-native HTAP architectures.\n1.2 HTAP techniques\nThis section takes a deep dive into the key techniques of\nHTAP databases, including hybrid workload processing,\ndata organization, data synchronization, query optimiza-\ntion, and resource scheduling.\n(1) Hybrid Workload Processing. There are three kinds of\nhybrid workload processing techniques, including (i) Multi-\nVersion Concurrency Control (MVCC) techniques [55], [54]\n(ii) Copy-on-Write (CoW) techniques [52], [9]; and (iii) Dual-\nstore based processing [57], [60], [83], [118], [45]. First,\nMVCC-based techniques process the hybrid workloads on\nthe same copy of multi-versioned data. Second, the CoW\ntechnique relies on snapshotting to support HTAP , where\nthe main process handles the transactions and the forked\n\n3\nprocesses perform the queries. Third, dual-store-based pro-\ncessing utilizes a transactional store for OLTP workloads,\nand employs an analytical store for OLAP workloads.\n(2) Data Organization. We introduce two types of data\norganization techniques, including (i) primary row store\nwith the selected column store and (ii) adaptive hybrid data\nstorage. Both methods adaptively generate the data storage\nbased on the given workloads. The former organizes the\ndata by selectively replicating the data from the row store to\nthe column store, including frequency-based heatmap [92]\nand integer programming [16]. The latter organizes each\ntable in a hybrid row and columnar format by vertically\nand horizontally partitioning the tables, including the cost-\nbased approaches [5], [12], a clustering approach [11], and a\ndeep-learning-based approach [2].\n(3) Data Synchronization (DS) There are two types of DS\ntechniques for various HTAP databases including (i) in-\nmemory delta merge [57], [60], [83], [108], [118] and (ii) log-\nbased delta merge [21], [45], [136]; The first category peri-\nodically merges the newly-inserted in-memory delta data to\nthe memory-based column store. The second category [45],\n[136] merges the deltas to the column store based on multi-\nlevel delta merging, including log shipping and replaying.\n(4) Query Optimization. We introduce three types of query\noptimization techniques, including (i) hybrid row/column\nscan [45], [60]; (ii) HTAP indexing [110], [127] and (iii)\nCPU/GPU acceleration for HTAP [9], [62]. The first type\n[83], [92] optimizes query plans by selecting the access path\nof a row store or a column store. The second type relies\non path-copying and multi-version indexing techniques to\nspeed up HTAP . The third type leverages heterogeneous\nCPU/GPU architecture to accelerate HTAP workloads.\n(5) Resource Scheduling. There are two types of scheduling\ntechniques: the workload-driven approaches [118], [120]\nand the freshness-driven approach [109]. The former adjusts\nthe parallelism of OLTP and OLAP threads based on the per-\nformance of executed workloads. The latter [109] switches\nthe execution modes on resource allocation and data ex-\nchange for OLTP and OLAP workloads by considering the\ndata freshness.\n1.3 HTAP benchmarks\nWe present state-of-the-art end-to-end benchmarks and\nmicro-benchmarks for evaluating HTAP databases. We in-\ntroduce four end-to-end HTAP benchmarks, including CH-\nbenchmark [26], HTAPBench [25], OLxPBench [51], HAT-\ntrick [80], and HyBench [141]. We focus on the key compo-\nnents, including data generation, HTAP workload, execu-\ntion rule, and metrics. In addition to the end-to-end bench-\nmarks, we will introduce three synthetic micro-benchmarks\nfor data organization [11], [12], [49].\n1.4 Contributions\nDifferences with existing surveys. In this paper, we focus\non fundamental techniques of HTAP databases [64]. We\nalso summarize the pros and cons of various architectures\nand techniques. ¨Ozcan et al. [93] discussed various HTAP\nsystems rather than the key techniques of HTAP databases.\nHieber et al. [44], [43] reviewed HTAP systems from several\ndimensions, including architecture, query handling, andconcurrency control. However, it lacked an in-depth anal-\nysis of HTAP databases and neglected many fundamental\nHTAP techniques, such as hybrid workload processing, data\norganization, data synchronization, and query optimization.\nCompared with a previous survey [140], this work has a\nsignificant amount of new content: (1) it gives a systematic\noverview and introduces five HTAP challenges in Section 1;\n(2) it reviews the evolution of HTAP databases in the\nhistory and introduces a trade-off between data freshness\nand performance isolation in Section 2; (3) it gives a more\ndetailed analysis on the architectures in Section 3, including\nthe cloud-native HTAP architectures; (4) it introduces a new\ntaxonomy of the key techniques and presents each type of\ntechniques in more details in Section 4, including the hybrid\nworkload processing; (5) it presents eight state-of-the-art\nHTAP benchmarks in Section 5, including OLxPBench [51],\nHATtrick [80], HyBench [141], and mOLxPBench [49]; (6)\nit presents six research directions with many new open\nproblems in Section 6, including HTAP for multi-model\ndata analytic [117], serving atop HTAP [79], [50], and cloud-\nnative HTAP techniques [39], [103], [122].\nTo summarize, we make the following contributions:\n1) We survey HTAP databases. We introduce a taxonomy\nof state-of-the-art HTAP databases according to their\nstorage architectures. We also discuss their pros and cons\non performance, scalability, and data freshness.\n2) We summarize HTAP techniques. We take a deep dive\ninto the key HTAP techniques concerning hybrid work-\nload processing, data organization, data synchronization,\nquery optimization, and resource scheduling.\n3) We review HTAP benchmarks. We introduce the state-of-\nthe-art benchmarks on HTAP databases. We present their\nschema, workloads, execution rules, and metrics.\n4) We provide new research challenges and discuss future\ndirections, including data organization for distributed\nHTAP databases, HTAP query optimization, and cloud-\nnative HTAP techniques.\n2 B ACKGROUND OF HTAP D ATABASES\nIn this section, we introduce the background of HTAP\ndatabases. We first review the evolution of HTAP databases\nby introducing four phases of HTAP development and ap-\nplications, and then we introduce a trade-off between data\nfreshness and performance isolation.\n2.1 The Evolution of HTAP Databases\nFigure 2 depicts a timeline of the HTAP databases between\n2010 and 2022. We place the systems in the year when they\nfirst released the HTAP functionality from the literature or\nfrom the publicly released material. By investigating these\nHTAP databases, we mainly classify the development of\nHTAP databases into four phases as follows:\nYear 2010-2014: Standalone Column-based HTAP databases.\nIn the first phase, HTAP databases mainly adopted stan-\ndalone column-based databases, representatives are SAP\nHANA [34], Hyrise [40], Hyper [52]. Since HTAP was not\nformally defined at that time, they named such a technique\nas ”hybrid OLTP&OLAP” [52] or ”OLxP” [84]. Back then,\n\n4\n2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010SAP HANAHyrise\nGreenplum TiDBF1 Lightning\nPelotonBatchDB HyPer\n\u000brow\f MemSQL Oracle DB2 BLU CalderaRateupDB SQL ServerMySQL Heatwave\n  HyPer\n(column)SingleStore PolarDB\n2022PolynesiaProteusPostgreSQL\nHyrise-newAlloyDBSnowﬂake\nStoneDB\nMarialDBCitus\nFig. 2. A Timeline of HTAP databases that first released the HTAP functionality in the literature\nthe applications they targeted were mainly analytical ap-\nplications with read-heavy transactional workloads, such as\nthe ERP applications [34].\nYear 2014-2019: Standalone Row-based HTAP databases. The\nyear 2014 is the time when the HTAP term was formally\ndefined by a Gartner report [98]. It initially defined HTAP\nas an application architecture that utilized in-memory com-\nputing technologies to enable hybrid processing on the\nsame in-memory data store. In 2018, Gartner extended the\nHTAP concept to ”In-Process HTAP” [35], which supported\nweaving hybrid workload processing techniques together as\nneeded to accomplish the business task. Such a new defini-\ntion indicated that HTAP is no longer limited to in-memory\ncomputing techniques, which significantly expanded the\nHTAP applications. During this period, major relational\ndatabases extended the primary row store with a column\nstore. To name a few, DB2 BLU [108], MemSQL [89], SQL\nServer [60], Oracle [57], PostgreSQL [84] and MarialDB [78].\nThe applications they targeted were medium-scale transac-\ntion processing applications with real-time data analytics,\nsuch as banking and finance services [87] and applications\nof fraud detection [107], [106].\nYear 2019-2022: Distributed HTAP databases. In the third\nphase, there emerged many distributed HTAP databases,\nsuch as SingleStore [103], Citus [28], F1 Lightning [136], and\nTiDB [45]. On the one hand, these databases embraced the\nNewSQL movement [95] by developing distributed SQL-\nbased transaction processing systems with high scalability\nand strong consistency. On the other hand, they caught\nup with the HTAP wave by expanding their capacity of\nscalability and consistency to HTAP , e.g., adding distributed\ncolumnar storage and unified data replication. Generally,\nthey are suitable for large-scale data-intensive applications\nsuch as E-commerce [137], Internet of Things (IoT) [46], and\nreal-time social media [69].\nYear 2022-present: Cloud-Native HTAP databases. As cloud\ndatabases are proliferating, there have been emerging many\ncloud-native HTAP databases [63]. Two representatives are\nAlloyDB [39] and Snowflake [122]. With the disaggregation\nof computing and storage, they enable HTAP with high\nelasticity, high availability, and multi-tenancy. First, since\ncompute and storage resources can be scheduled on demand\nindividually, they provide high elasticity for HTAP . Second,\nas the data is replicated across many availability zones and\nis backed by the scalable cloud service, cluster and node\nfailures can be recovered quickly. Third, as the resources are\nvirtualized and shared by multiple tenants, they are more\ncost-efficient. Since Gartner predicted [100] that the revenue\n0HPRU\\+LJK\u0003ZRUNORDG\u0003LVRODWLRQ/RZ\u0003GDWD\u0003IUHVKQHVV+LJK\u0003GDWD\u0003IUHVKQHVV/RZ\u0003ZRUNORDG\u0003LVRODWLRQ\u0003OLTP instanceOLAP instance2/732/$32/732/$3\nFig. 3. A trade-off between data freshness and performance isolation\nfrom the cloud databases will account for 50% of total DBMS\nmarket revenue in the near future, we believe cloud-native\nHTAP databases will find a wide spectrum of applications.\n2.2 A Trade-off between Freshness and Isolation\nData freshness. As OLTP workloads are updating the data,\nHTAP databases need to guarantee that the fresh data is\naccessed by the analytical queries. Hence, the data freshness\nrefers to the lag time between the analytics and transactions . One\nway of quantifying data freshness is to take the maximum\nvalue of the timestamp differences between the result sets\nof the OLAP client and OLTP client [141]. Additionally, the\ndata replication latency can also reflect the data freshness.\nParticularly, with the separated transactional store and an-\nalytical store, the fresh data is periodically replicated in the\nanalytical store. Since many approaches [45], [116], [136]\nmust merge the newly-updated data to the analytical store\nbefore performing the data analytics, it measures how fast\nthe recently committed transactions are synchronizing to the\nanalytical store such that the analyzed data is fresh.\nPerformance isolation. In HTAP databases, performance iso-\nlation [25], [26], [45], [105], [116] refers to the system’s capacity\nof reducing the performance degradation when processing the\ndynamic hybrid workload . In other words, it reflects how\nwell the systems can reduce the interference between OLTP\nand OLAP workloads when executing them concurrently.\nRegardless of the isolation architectures (in the same node\nor different nodes), such metrics directly reflect HTAP\ndatabases’ performance of handling hybrid workloads w.r.t.\nthe same number of OLTP/OLAP clients and the same scale\nfactor of a dataset.\nA Trade-off. HTAP databases must balance a trade-off be-\ntween data freshness and performance isolation. As shown\nin Figure 3, handling the hybrid workloads with separated\ninstances can provide high-performance isolation but may\nlead to low data freshness, hence the outdated results of\ndata analytics. For instance, TiDB [45] only degraded up\n\n5\nto 10% of the performance while it took up to 1000 ms to\napply the change logs for data replication. Handling the\nhybrid workloads in a unified memory space of a single\nsever favors high data freshness but it can greatly degrade\nthe performance. For instance, Hyper [86] took only mi-\ncroseconds to create a fresh snapshot, but it degraded up\nto 40% of the performance. To quantify the effectiveness\nof performance isolation, the degradation percentage [45],\n[105] of transactional/analytical throughput is mostly used.\nOne way of quantifying the performance isolation is to\ncompare the performance between a sequential execution\nand a hybrid execution with the hybrid workload[141]. The\nlower the metric is, the better the performance isolation is.\n3 HTAP A RCHITECTURES\nIn this work, we focus on the HTAP architectures with a\nrow store and a column store in a single DBMS. We do\nnot consider the loosely-coupled HTAP architectures [93]\nas those architectures employ multiple databases to form an\nHTAP solution that entails a costly ETL process. We classify\nthe tightly-coupled HTAP architectures into four categories:\n(a) Primary Row Store and In-Memory Column Store; (b)\nDistributed Row Store and Column Store Replica; (c) Pri-\nmary Row Store and Distributed In-Memory Column Store;\nand (d) Primary Column Store and Delta Row Store. Table\n1 presents their representative databases, OLTP paradigm,\nOLAP paradigm, and delta storage.\n3.1 Primary Row Store+In-Memory Column Store\nThis category of databases [57], [60], [108] leverages a\nprimary row store and an in-memory column store in a\nsingle node. It relies on a row store for handling the OLTP\nworkload, and the data will be persisted to the disk in a row-\nwise page store. Column store is optimized for OLAP work-\nload with compression techniques [1], [15] and standalone\ncolumnar scans [57], [60], [108]. An in-memory delta store is\nutilized to record the recent DML operations, which will be\nmerged into the column store periodically. For transaction\nprocessing, read/write transactions will be handled by the\nrow engine with the ACID guarantee based on MVCC; data\nupdates will also be recorded in the delta store. For analyti-\ncal processing, long-running queries are processed using the\ncolumnar scan, and the delta data that has not been merged\nshould be traversed to access the fresh data. This category\nof system has high analytical throughput because the OLAP\nworkloads are processed using an in-memory column store.\nData freshness is also high because the column store engine\ncan access the latest data in the main memory. However,\nlimited by memory capacity, the scalability of this category\nof system is low. In addition, because transactional and\nanalytical workloads are processed in the same machine,\nthe isolation of the system is low.\n3.1.1 Representatives of HTAP Databases\nWith architecture (a), Oracle in-memory dual-format\ndatabase [57], [92] combines the row-based buffer cache\nwith an in-memory column store (IMCS) to handle OLTP\nand OLAP workloads simultaneously. IMCS consists of in-\nmemory compression units (IMCUs), and each IMCU is im-\nmutable and can only be populated from the buffer; the datachanges are cached in the snapshot metadata unit (SMU),\nand each IMCU is associated with one SMU. To merge the\nupdates to the IMCU, it must create a new IMCU that incor-\nporates the data in the corresponding SMU. Another repre-\nsentative is SQL Server [60], where an in-memory row-based\nengine, called Hekaton [30], is integrated for handling OLTP\nworkloads. Its underlying storage is based on a disk-based\nrow store for logging the operations and persisting the data.\nIt also builds the column store index (CSI) for handling com-\nplex queries. Different from Oracle, the in-memory columns\nin CSI that are infrequently accessed will be compressed and\npersisted to the disk, and CSI is updatable. Besides, the data\nchanges are appended to the tail of the in-memory table, and\nthey are indexed by a tail index, e.g., a B-tree. By scanning\nthe tail index, data changes will be periodically merged into\nthe column store. DB2 also adopts such an architecture;\nit has been deeply integrated with an IMCS accelerator\ncalled DB2 BLU [108] to incorporate advanced column-\nbased techniques such as compression-based operations and\nsingle-instruction multiple-data (SIMD) instructions. BLU\nalso supports updating the column store.\n3.1.2 Challenges & Opportunities\nHTAP databases with architecture (a) need to improve\nthe scalability and performance isolation while maintaining\nhigh performance and high data freshness. The main chal-\nlenge is how to scale and isolate the OLAP workloads using\nthe column store. One possible solution is to scale the OLAP\nworkloads with a distributed in-memory architecture [125].\nIn addition, placing which columns into the column store\nwith a memory budget is also challenging as it is an NP-\nhard problem [16].\n3.2 Distributed Row Store+Column Store Replica\nThe second architecture is a distributed cluster including\na master node and multiple secondary nodes. The master\nnode handles the read/write transactions; the secondary\nnodes are read-only. Particularly, when handling the trans-\naction requests, the master node asynchronously replicates\nthe logs to the secondary nodes for data synchronization.\nThe primary node adopts a row store, certain secondary\nnodes will be chosen to adopt a column store for query\nacceleration. Transactions are handled in a distributed way\nfor high scalability; complex queries are performed in the\nsecondary nodes with a column store. For the pros, it has\nhigh performance isolation and high scalability as the mixed\nworkloads are processed on separated nodes. For the cons,\nthe data freshness is low since newly-updated data may not\nhave been merged into the column store.\n3.2.1 Representatives of HTAP Databases\nTwo representatives with architecture (a) are TiDB [45] and\nF1 Lightning [136]. TiDB is a Raft-based distributed HTAP\ndatabase [91], which consists of multiple row-based nodes,\ncalled TiKV nodes, among which a leader node handles\nthe read-write transactions. The leader node asynchronously\nsends the data to other follower nodes; the follower nodes\nonly serve the read transactions and consistently communi-\ncate with the leader. Only if the leader node has failures, a\nfollower node can become a leader by voting. For the data\n\n6\nNode 3Row StoreDiskMasterNode 2 Node 1MemoryNode 3(a) Primary Row Store+In-Memory Column Store(b) Distributed Row Store + Column Store Replica(c) Disk Row Store + Distributed Column Store(d) Primary Column Store + Delta Row StorePersistent StorageMemoryLogMergeColumn StoreDelta\nClientClientDiskColumn Store\nMemoryNode 1Partition 1Partition 2Partition 3MasterPartition 3Partition 1Partition 2Node 2Partition 2Partition 3Partition 1TransformRow StoreDeltaColumn StorePersistent StorageLogMerge Transform\nFig. 4. A Taxonomy of State-Of-The-Art HTAP Databases based on the Storage Architecture and Processing Paradigm\nTABLE 1\nA Classification of State-Of-The-Art HTAP Databases based on the Storage Architecture\nCategory HTAP databases OLTP Paradigm OLAP Paradigm Delta Store\nPrimary Row Store + In-Memory\nColumn StoreOracle [57],\nSQL Server [60]Standalone Row-wise\nMVCCStandalone Columnar\nScan with Delta TraverseIn-Memory Table\nDistributed Row Store + Column\nStore ReplicaTiDB [45]\nF1 Lightning [136]Distributed Row-wise\n2PC + PaxosDistributed Columnar\nScan with Log ReplayB-tree, Change\nLog\nPrimary Row Store + Distributed\nIn-Memory Column StoreMySQL Heatwave[83]Standalone Row-wise\nMVCCDistributed Columnar\nScan with Log ReplayChange Log\nPrimary Column Store + Delta\nRow StoreSAP HANA [118],\nHyper [86]Cache-based Row-wise\nMVCCIn-Memory Columnar\nScan with Delta TraverseIn-Memory\nDictionary\nstorage, the underlying data is partitioned into multiple\nrow-based regions. One or more servers will be selected\nas learner nodes that store columnar replicas for analytical\nprocessing. Learner nodes are read-only and do not partic-\nipate in voting. It adopts a global 2-Phase-Commit (2PC)\nprotocol [96] to handle distributed transactions based on a\nglobal time stamp. For analytical processing, it develops a\ncentralized cost-based optimizer that supports cross-engine\nquery processing where queries can be pushed down to\neither a row engine or a column engine. TiDB relies on\nthe Raft protocol for data replication [91]. The master node\nasynchronously replicates logs to the follower and learner\nnodes for log replaying, and it builds a delta merge tree to\ntrack the changes and merges them to the column store peri-\nodically. F1 Lightning [136] is another representative, which\nintegrates a data replication service into an OLTP engine\nthat is built on top of Spanner [27], a distributed OLTP\ndatabase with strong consistency and high scalability that\norganizes the row-based partitions in multiple regions. The\nlightning server contains a component, called changepump,\nwhich uses the change data capture mechanism to detect\nnew changes, then transforms them from row-wise format\nto columnar format and merges them into the storage. The\nmemory-resident deltas are row-wise B-tree, and Lightning\nperiodically checkpoints memory-resident deltas to disk.\nWhen the deltas are too large, Lightning merges and trans-\nforms them into the column store. The log-structured merge\n(LSM) reader merges both memory-resident deltas and disk-\nbased deltas by merging and collapsing. Particularly, merg-\ning deduplicates changes in the deltas and copies distinct\nversions to the new delta; collapsing combines multiple\nversions of the same key into a single version. F1 lightning\nadopts a Paxos-based 2PC protocol to handle distributed\ntransactions; a distributed query engine [58], F1 Query [113],\nis employed for query processing. For the specified times-tamp that is in the query window (normally 10 hours), the\npushdown evaluator reads the corresponding data from the\ncolumnar file in the lightning server, and then obtains other\ndata from Spanner.\n3.2.2 Challenges & Opportunities\nHTAP databases with architecture (b) need to increase the\ndata freshness due to the high overhead of log shipping,\ntransformation, and delta merging in the distributed ar-\nchitecture. The main challenge is how to efficiently merge\nthe delta files to the column store. Two possible solutions\nare to (1) develop a memory-based delta logging and ship-\nping [116] and (2) design new indexing techniques for delta\nmerging [114], [73]. Moreover, how to effectively organize\nthe data (e.g., data layout, data placement, and column\ncompression) for a distributed HTAP database is also a\nchallenging task [2], [3].\n3.3 Primary Row Store+Distributed IMCS\nThis kind of HTAP databases [83], [125] utilizes a row\nstore with a distributed in-memory column-store (IMCS) to\nsupport HTAP . Two main differences between architecture\n(a) and (c) are about the OLAP paradigm and delta store. For\nthe OLAP paradigm, the former one adopts the standalone\nin-memory columnar scan with delta traverse while the\nlatter one relies on distributed in-memory columnar scan\nwith log replay. Regarding the delta store, architecture (a)\nemploys the in-memory table while architecture (c) uses the\nchange log, so they have the different data synchronization\nmethods (See Section 4.3).\nWith architecture (c), the row engine processes the OLTP\nworkloads, and the IMCS handles the query processing. The\ncolumnar data is extracted from the row store, and the hot\ndata resides in IMCS and the cold data will be evicted to\ndisk. For the pros, it has high performance isolation as the\n\n7\nhybrid workloads are processed in different nodes. More-\nover, it has a high OLAP throughput and scalability because\nof the distributed IMCS. For the cons, it has medium or low\ndata freshness, depending on the deployment mode of the\nIMCS cluster (e.g., on-premise or on-cloud). In addition, it\nhas low horizontal scalability on OLTP due to a standalone\nrow store for transaction processing.\n3.3.1 Representatives of HTAP Databases\nMySQL Heatwave [83] is a representative that employs\narchitecture (c). Specifically, it tightly couples the MySQL\ndatabase with a distributed IMCS cluster, called Heatwave,\nto enable real-time analytics. Transactions are fully executed\nin the MySQL database. Columns that are frequently ac-\ncessed will be loaded into the Heatwave. When a complex\nquery comes in, it can be pushed down to Heatwave for\nquery acceleration. The columnar data is extracted from the\nMySQL database, and the hot data resides in the Heatwave\ncluster. For data synchronization, the latest transaction data\nwill be automatically transformed and merged to the col-\numn store in three cases: (i) every 200 milliseconds; (ii) when\nthe buffer size reaches 64 MB, or (iii) when the queries need\nto access the latest updated data. The heatwave cluster also\ndeveloped the auto-pilot service to automate the processes\nof data partition, query execution and scheduling.\n3.3.2 Challenges & Opportunities\nHTAP databases with architecture (c) need to increase the\ndata freshness due to the distributed in-memory column\nstore. The main challenge is how to balance the data fresh-\nness and OLAP throughput. Possible solutions are to design\n(1) cost functions or (2) ML models for column data man-\nagement, including column selection and compression [47].\n3.4 Primary Column Store+Delta Row Store\nThis category of databases utilizes the primary column store\nas the basis for OLAP , and handles OLTP with a delta row\nstore. The primary column store stores the whole data in\nthe main memory. Data updates are appended to the row-\nbased delta store. The system periodically merges the delta\ndata into the column store. For the pros, it has high data\nfreshness as the hybrid workloads are processed in the main\nmemory. Moreover, it has a high OLAP throughput because\nof the primary column store. For the cons, it has low OLTP\nscalability due to the delta-based row store. In addition, it\nhas low-performance isolation due to the standalone archi-\ntecture for hybrid workload processing.\n3.4.1 Representatives of HTAP Databases\nSAP HANA [34], [118] and Hyper [52], [86] are two rep-\nresentatives of architecture (d). SAP HANA divides the\nin-memory data store into three layers: L1-delta, L2-delta,\nand Main. The L1-delta keeps data updates in a row-wise\nformat. When a threshold is reached (e.g., 100k tuples), the\ndata in L1-delta is transformed and merged to L2-cache\nbased on a local dictionary. The L2-delta transforms the\ndata into columnar data, then merges the data into the\nmain column store based on a global dictionary. Finally,\nthe columnar data is persisted in the disk storage. For\nhigh data freshness, the OLAP client will traverse the deltarecords when scanning the column store. In addition, for\nnon-transformed records, it can use in-place updates for the\ndelta store. To update transformed records, it replaces an\nupdate operation with a delete and an insert operations on\nthe delta store. Hyper [52], [86] was initially based on a row\nstore. Now it has supported the architecture of a primary\ncolumn store with a delta row store. Specifically, it uses\nthe buffer to handle concurrent transactions based on an\nMVCC protocol. The version vector stores all the versions\nfor each unique tuple. Instead of periodically merging the\ndata in the version vector to the column store, Hyper\nadopts transaction-level garbage collection, where all of the\nversions that were generated by the transactions can be\nsafely removed after the transactions have been applied to\nthe column store. Finally, the columnar data is updated in-\nplace by applying all the committed transactions.\n3.4.2 Challenges & Opportunities\nFor HTAP databases with architecture (d), two main prob-\nlems need to be addressed. (1) they need to increase the\nOLTP scalability due to the delta-based transaction process-\ning. (2) they need to increase performance isolation due to\nthe unified memory pool for HTAP . The main challenge is\nhow to traverse the delta storage efficiently while keeping\nhigh throughput for HTAP . Although they have offered\ncertain scaling-out solutions [38], [82], these approaches\nneed to be further justified due to the centralized transaction\nscheduler.\n3.5 Applications of HTAP Architectures\nHTAP databases with the above-mentioned architectures\nhave their merits and demerits. Hence, “one HTAP database\ncannot fit all”, especially for different applications. By com-\nparing their pros and cons, we summarize the gained in-\nsights and give the recommendations as follows.\n(1) Architecture (a) is suitable for the applications that\nrequire high throughput and data freshness, but the de-\nmand for scalability is not high. Representatives are the\nbanking and finance services [87] that need to process and\nanalyze the customers’ transactions efficiently. Therefore,\nthese applications have a high requirement on the system\nthroughput. Since the number of target customers is almost\nfixed, these applications do not require high scalability.\n(2) Architecture (b) is ideal for applications that require\nhigh scalability and can have tolerable data freshness. A\nrepresentative is an E-commerce application with real-time\ndata analytics. Such applications need to process a large\nnumber of concurrent transactions from multiple customers,\nespecially for holidays, e.g., Double-Eleven in Alibaba [137].\nTherefore, the scalability requirement must be fulfilled. Nev-\nertheless, such applications do not force zero freshness as it\nis acceptable that there is any inconsistency in a short period\nof time, e.g., customer retention rate.\n(3) Architecture (c) is suitable for applications that re-\nquire high analytical throughput and scalability, but the\ndemand for data freshness is not high. A representative is\nthe IoT applications with real-time data analytics [46]. On\nthe one hand, these applications have a high requirement\nfor analytical throughput and scalability. Thus, a distributed\nin-memory column store is a good fit for such cases. On the\n\n8\nother hand, the data updates in these scenarios are rare, thus\na standalone row store is sufficient.\n(4) Architecture (d) is a good fit for applications that\nrequire high analytical throughput and high data freshness.\nA representative is real-time fraud detection [106], [107],\nwhich requires high data freshness as they can not toler-\nate any fraud, which could cause significant consequences.\nNevertheless, the scalability requirement is not high.\n3.6 Other Architectures\nOther than the four main types of HTAP architectures,\nthere are other HTAP architectures, including (1) row-only\nHTAP architectures; (2) column-only HTAP architectures;\n(3) Spark-based HTAP architectures; and (4) cloud-native\nHTAP architectures. We summarize them as follows:\n(1) Row-only HTAP architectures [52], [77] rely on the\npurely row store to enable HTAP . For instance, the first\nversion of Hyper [52] employed a copy-on-write mechanism\nto fork an OLAP process to operate on a separate snapshot.\nIts main process handles the transactions simultaneously,\nand both the OLTP and OLAP processes are based on the\nrow-based data store. BatchDB [77] leverages the primary-\nsecondary replication, where the primary row-wise replica\nis used for OLTP , and the secondary row-wise replica is in\ncharge of OLAP . It uses a batch-based propagation mecha-\nnism to synchronize the updates from the primary replica to\nthe secondary replica. This type of architecture relies heavily\non the row-based query processing in the main memory to\nprocess the hybrid workload. Thus, both the freshness and\nOLTP throughput are high. However, the major drawback\nis that the analytical throughput is much lower compared to\nthe column-based query processing.\n(2) Column-only HTAP architectures [9], [40], [62], [112]\nrely solely on a column store to support HTAP . For example,\nHyrise [40] initially employed an adaptive columnar layout\nto support HTAP , and the basic idea is to use narrower\ncolumn groups to handle OLAP workloads and leverage\nwider column groups to handle OLTP workloads. The latest\nversion of Hyrise [31] develops the chunk-based column\nstore, a PAX [4]-like data layout that horizontally divides\na table into partitions (a.k.a., row groups), and each par-\ntition is organized in columns. NoisePage [88] (previously\nnamed Peloton [11]), a self-driving columnar database [20],\n[75], [76], [94], also adopts such a data layout based on\nApache Arrow [8] format. Caldera [9] relies on the copy-on-\nwrite mechanism with CPU/GPU architecture where OLTP\nworkloads are handled by multi-threads of CPU and OLAP\nworkloads are executed with GPU in parallel. Another case\nis RateupDB [62], which also adopts the CPU/GPU archi-\ntecture. Different from Caldera [9], it relies on the primary-\nsecondary replication to enable HTAP . TiQue is a recent\nwork that adds the transaction logic in SQL and enables\nHTAP on top of a column store. The basic idea is to add\nthe delta table and transaction metadata for an analytical\ndatabase, thereby accelerating the transaction processing.\nOverall, column-only HTAP architectures have a high an-\nalytical throughput, but the data updates and data synchro-\nnization can significantly affect the system’s performance.\n(3) Spark-based HTAP architectures. Such systems [13],\n[81] support HTAP by combining an OLTP engine and anOLAP engine. The OLTP engine is dominated by columnar\ndatabases such as HBase [42], and the OLAP engine mainly\nuses a Spark [139] engine. Both engines share the data in\na distributed file system (such as HDFS [18]). For example,\nSplice Machine [124] and Phoenix [101] handle data updates\nbased on HBase, and leverage Spark for big data analytics.\nSnappyData [81] integrates a transactional engine, Apache\nGeode [37], into the Spark for processing streaming, trans-\nactional, and interactive analysis simultaneously. Wildfire\n[13] tightly couples the Spark with a distributed transaction\nengine. In the upper layer, it provides a unified interface\nto take as input the hybrid workloads. The middle layer\nschedules the OLTP workloads with Scala API to the trans-\nactional engines, and routes the Spark SQL queries to Spark\nexecutors. In the storage layer, they organize the data in the\nLSM-tree format with an indexing support [73]. The Spark-\nbased HTAP systems have high scalability and are suitable\nfor big data analytics with modest data updates. However,\nthe data freshness is low due to the shared file storage.\n(4) Cloud-Native HTAP architectures [22], [23], [39],\n[103], [122] rely on the disaggregation of compute and\nstorage to enable HTAP . For instance, AlloyDB [39] is a\nPostgreSQL-compatible cloud-native HTAP service. In its\ncompute layer, the compute nodes rely on machine learn-\ning to convert the in-memory data from the row format\nto columnar format for query acceleration. In its storage\nlayer, AlloyDB has developed an elastic log storage service\nthat organizes the data in shards and asynchronously ma-\nterializes the WAL records to fresh pages, and the dirty\npages in the compute layer are never flushed into the\nstorage [132]. Another example is SingleStore (the successor\nof MemSQL [89]), which proposes a unified table storage\nstructure based on a cloud-native architecture. It relies on\na distributed in-memory row store for handing updates.\nThe data is persisted to the disk-based columnar storage,\nand is organized as LSM trees in the storage layer; the\nsecondary hash indexes are built for accelerating point\nqueries. Snowflake [29], which is a cloud-native OLAP\ndatabase, has utilized its metadata storage to implement an\nHTAP solution, called Unistore [122], supporting real-time\ndata analytics. PolarDB-IMCI builds an in-memory column\nindex in a disagreggation architecture of PolarDB [23]. Data\nsynchronization is performed by replaying physical redo\nlogs from the shared storage. Cloud-native architectures\nempower HTAP with high elasticity, availability, and multi-\ntenancy. However, the data freshness is low due to the\nphilosophy of ”log is the database”, especially when the log\nhas not been replayed in the storage layer.\n4 HTAP T ECHNIQUES\nAs shown in Table 2, we summarize five types of HTAP\ntechniques, including (1) hybrid workload processing; (2)\ndata organization; (3) data synchronization; (4) query opti-\nmization; and (5) resource scheduling. It also presents their\nmain methods, key techniques, pros and cons. Figure 6\nillustrates an overview of the HTAP techniques. From a\ntop-down perspective, (1) OLTP and OLAP workloads are\nhandled by hybrid workload processing. (2) HTAP resource\nscheduling dynamically assigns the resources to the hybrid\nworkload. (3) Query optimization relies on row and column\n\n9TABLE 2\nAn Overview of HTAP Techniques\nTask Type Main Method Key Technique Pros Cons\nHybrid Workload\nProcessingMVCC-based HTAP Version Chains for OLTP and OLAP [55] High Freshness Long Version Chains\nCopy-on-Write based HTAP Snapshotting OLTP for OLAP [52] High Freshness Large Memory Size\nDual-Store based HTAP Separated Stores for OLTP and OLAP [57] High Isolation High Sync Cost\nData\nOrganizationPrimary Row Store with\nSelected Column StoreFrequency-based Heatmap [92] Arbitrary Queries Low Utility\nCost-Based Linear Programming [16] High Utility Single-Table Queries\nAdaptive Hybrid\nData StorageCost Functions [2], [5], [11], [12] High Efficiency Low Utility\nMachine Learning Models [2], [3] High Utility Training Overhead\nData\nSynchronizationIn-Memory Delta MergingThreshold-based Merging [57] Fast Insertion Slow Lookup\nDelete table based Merging [60] Fast Lookup Slow Insertion\nDictionary-based Merging [118] High Efficiency Low Scalability\nLog-based Delta MergingMulti-Level Delta Merging [45], [136] High Scalability High Merge Cost\nChange Data Capture [21], [83] High Isolation High Latency\nQuery\nOptimizationHybrid Row/Column ScanCost-based Execution [45], [32], [123] High Utility Large Search Space\nRule-based Execution [57], [60] High Efficiency Low Utility\nHTAP IndexingParallel Binary Tree [127] High Throughput Large Memory Size\nMulti-Version Partitioned B-tree [110] High Scalability Low Throughput\nCPU/GPU Acceleration CPU for OLTP , GPU for OLAP [9], [62] High AP Throughput Low Freshness\nResource\nSchedulingFreshness-Driven Method Execution Mode Switching [109] High Freshness Low Throughput\nWorkload-Driven Method Rule-Based Resource Assignment [120] High Throughput Low Freshness\n9LUWXDO\u00030HPRU\\2/732/$3&RS\\\u0003RQ\u0003ZULWH\n2/73\u00035HSOLFD\n2/$3\u00035HSOLFD'DWD\u00036\\QF,Q\u0003EDWFKHV2/732/$3\n\u000bD\f\u0003&RS\\\u0010RQ\u0010ZULWH\u0003+7$3\u000bE\f\u0003'XDO\u0010VWRUH\u0003+7$3\nFig. 5. Hybrid Processing based on Copy-on-Write and Dual-Store\nstores to execute the query. (4) Data synchronization merges\nthe updates from the row store to the column store. (5) Data\norganization adaptively organizes the hybrid storage.\n4.1 Hybrid Workload Processing\nHybrid workload processing aims to handle the mixed\nworkload under a unified architecture. It mainly consists of\nthree types of techniques: (1) MVCC-based HTAP; (2) copy-\non-write-based HTAP; and (3) dual-store-based HTAP .\n4.1.1 MVCC-based HTAP\nMulti-Version Concurrency Control (MVCC) is the most\nwidely adopted transaction management technique in ma-\njor relational DBMSs [135]. However, it is challenging to\nsupport HTAP solely based on MVCC. This is because\nHTAP workloads will lead to frequent version traversing\nand cleaning of stale data versions. Therefore, the main\nchallenge is how to reduce the resource contention and\nimprove the query performance by efficiently traversing and\nreclaiming the version chains [6]. To make the MVCC-based\ndatabases more HTAP friendly, Weaver [53] utilizes the frag\nskip lists to speed up the version chain lookup by using\ntwo pointers for each node, which can accelerate the in-\nchain version traversing and cross-chain version traversing,\nrespectively; Diva [54] separates the version searching and\nversion cleaning by maintaining a provisional index and\nperforming an interval-based version clearning separately,\nwhich results in a better HTAP performance. Note thatMVCC-based systems [135] can implement various isola-\ntion levels including read committed ,repeatable read ,snapshot\nisolation , and serializable snapshot isolation . The concurrency\ncontrol techniques should be different when it comes to\ndifferent isolation levels. As a result, HTAP techniques may\nvary slightly because the delta table should adapt to dif-\nferent isolation levels. As similar techniques of concurrency\ncontrol in OLTP can be applied to HTAP systems, existing\nHTAP techniques did not discuss these issues.\nPros and Cons. MVCC-based HTAP processing has a high\nfreshness as the analytical queries can access the latest visi-\nble data by traversing the version chains. However, when it\ncomes to the long version chains, it will incur a significant\noverhead for version traversing and cleaning.\n4.1.2 Copy-on-Write based HTAP\nThis category of techniques [9], [52], [127] relies on in-\nmemory techniques with the Copy-on-Write (CoW) mech-\nanism to support HTAP . The basic idea is to create snap-\nshots for handing OLAP workloads when the main storage\nencounters a write operation. As shown in Figure 5(a), a\nsnapshot is generated by forking the main OLTP process\nwith a consistent virtual memory.\nFor transaction processing, the main OLTP process uses\na single thread to execute the transactions in a sequential\nway without locking and latching [52], resulting in lock-free\ntransaction processing. Since the data is maintained in the\nmain memory, the OLTP process can execute transactions\nat the rate of tens of thousands per second. Moreover,\nCoW can also handle transactions using multiple cores [127]\nor multiple threads [52] where each thread processes the\nsequential transactions over a data partition.\nFor analytical processing, the forked OLAP process han-\ndles the queries with the up-to-date snapshots. Particularly,\nthe OLAP process can utilize the OS-enabled interface to\ncreate the updated pages on demand, which only takes\nseveral microseconds. With multiple threads, the queries\ncan be performed in parallel against a single snapshot or\n\n10\nmultiple snapshots [52]. Besides, the OLAP process can\nutilize GPUs to accelerate the query processing [9].\nPros and Cons. This type of technique enjoys high fresh-\nness as the OLAP process can always analyze the fresh data\nvia snapshotting. However, it suffers from large memory\nsize because each process will create a new snapshot, espe-\ncially for the write-heavy workload. What is more, it has low\nisolation due to the shared resources in the same instance.\n4.1.3 Dual-Store based HTAP\nThis type of techniques [10], [34], [45], [62], [57], [77], [83],\n[84] relies on a dual-store architecture to handle HTAP\nworkloads. Most of HTAP databases introduced in Section 3\nemploy such an approach by developing a dual-store with\nboth row and columnar format [45], [57], [83], [84], or purely\nrow format [77] and purely column format [34], [62], [86].\nAs shown in Figure 5(b), a main store is used for handling\nOLTP workloads, and a secondary store is employed for\nprocessing OLAP workloads. The recently updated data is\nsynchronized from the main store to the secondary store in\nbatches periodically and asynchronously.\nFor transaction processing, the main store relies on\nMVCC protocols [135] to process the transactions. Specifi-\ncally, each insert is first written to the log and the row store,\nthen is appended to the delta store. An update creates a new\nversion of a row with a new lifetime of a begin timestamp\nand an end timestamp, and the older version is marked as a\ndelete row in a delete bitmap.\nFor analytical processing, queries are performed using\ncolumn-oriented techniques [1] such as compression-aware\nprocessing [59], single-instruction multiple-data (SIMD) in-\nstructions [57], and vector processing [86]. Queries can\nalso be accelerated with new hardware such as heteroge-\nneous CPU/GPU processors, Processing-in-memory (PIM)\nchips [17], [72], and FPGAs [111]. To analyze the fresh data,\nthe query engine traverses the delta data [57], [60], or uses a\nmerge-on-read mechanism [45], [136] that merges the delta\ndata to the main store before analyzing the whole data.\nPros and Cons. Dual-store HTAP processing has high\nisolation as the workload can be processed with two sepa-\nrated stores using either logical isolation [57], [60], [77], [62],\n[39] or physical isolation [45], [83]. However, it has a high\nsynchronization cost as the delta data shall be converted and\nmerged to the secondary store.\n4.2 Data Organization\nData organization in HTAP databases requires choosing an\noptimized data layout, e.g., row-wise or column-wise data\nlayout for the mixed workload. As maintaining two copies\nof data for OLTP and OLAP workloads is costly, existing\nmethods seek to strike a trade-off between the throughput\nand storage cost. They fall into two categories of approaches:\nprimary row store with selected column store and adaptive\nhybrid data storage.\n4.2.1 Primary Row Store with Selected Column Store\nThe first category of methods persists the transaction data\nin the row store and chooses part of the attributes to be\nincluded in the column store. The major objective is to select\nthe beneficial columns (i.e., having high utility of query\n2/73\n&ROXPQ\n5RZ+7$3\u0003'DWDEDVHV2/$3OLTP Instances4XHU\\\u00032SWLPL]DWLRQ+7$3\u00035HVRXUFH\u00036FKHGXOLQJOLAP Instances\n'DWD\u00036\\QFKURQL]DWLRQ'DWD\u00032UJDQL]DWLRQFig. 6. The Key Techniques of HTAP Databases\nacceleration and low update cost) into the memory from the\nprimary row store. With such an approach, OLTP workloads\nare handled by the row store, and OLAP workload can be\naccelerated by in-memory columnar scan. It has two main\nmethods: (1) frequency-based heatmap and (2) cost-based\nlinear programming.\n(1) Frequency-Based Heatmap . The heatmap ap-\nproach [92] selects the columns based on the access fre-\nquency from the workload. Particularly, the frequently-\naccessed columns will be kept in the memory and rarely-\naccessed columns will be evicted to disk. Basically, it groups\nthe in-memory columns into three clusters based on their\naccess frequencies: hot, candidate, and cold columns. The\ncandidate columns are the columns of interests. If some of\nthem were frequently-accessed, then they would be marked\nas hot columns, which will be populated from the persistent\nrow store. The columns are marked as cold if they were not\ntouched during a time window, e.g., several days. If the cold\ncolumns have been populated, they are compressed and\nevicted to the persistent columns. As extracting the columns\nfrom the row store is expensive, the evicted columns can be\nloaded back if accessed later.\nPros and Cons. The pros of heatmap approach is that\nit supports arbitrary queries and is easy to implement.\nThe downside is that it may have a low utility as it only\nconsiders the access frequency and neglects the effect of\nvarious column combinations concerning different queries.\n(2) Cost-Based Linear Programming . Another approach\nis the cost-based linear programming [16], which formalizes\nthe in-memory column selection as a knapsack-style prob-\nlem, and then uses an integer linear programming (ILP)\nmethod to solve the problem. Given a set of queries, its\nobjective function is a cost function that sums over the scan\ncost of each query over the involved columns. The goal is to\nminimize the cost function with a set of columns subject to\na given constraint, e.g., the total column size is not greater\nthan a memory budget.\nPros and Cons. The ILP method can have high utility\nbecause the selected columns can reduce the cost of the\nqueries. However, it is unclear how the cost functions can\nestimate the cost of complex queries that involve multiple\ntables and complex operations.\n\n11\n4.2.2 Adaptive Hybrid Data Storage\nThis line of works [2], [5], [11], [12] organizes the data\nadaptively based on the given workloads and designed\ncost functions. Such approaches adopt a fine-grained hy-\nbrid storage scheme. For example, H2O [5] supports three\nstorage layouts: columnar layout, row-wise layout, and col-\numn groups (i.e., width-varying vertical partitions of the\ntables). Given a query workload, it evaluates the cost of\ndifferent storage schemes (including processing cost and\nconversion cost), and then selects an optimal storage so-\nlution. Casper [12] selects an optimal layout of columns\nbased on the mixed workload of read-only queries and\nupdates. It considers various column features, including the\nnumber of partitions, the size and range of each partition,\nthe sorting method (i.e., sorted or unsorted), the updating\nmethod (e.g., in-place or out-of-place), and the cache size.\nBy evaluating the cost of each scheme based on the defined\ncost function, it obtains a solution that minimizes the cost\nwith the SLA-aware constraints. Peloton [11] uses a flexible\nschema to logically divide a relational table into different\ntiles, where each tile is a sliced data block with vertical\nand horizontal partitions. With a clustering method, it phys-\nically materializes the tiles on disk based on the workload.\nBeyond the cost-based data organization in a standalone\nsystem, Proteus [2] leverages machine learning methods to\nselect the optimal mixed row/column storage based on a\ndistributed architecture. It considers a larger design space\nfor the storage, including data format (row-wise or column-\nwise), data placement (partial or full copy on a node), data\ncompression, and data tiering (memory or disk). Then, it\ncharacterizes the workload concerning the storage layout.\nFinally, it selects a generated storage schema through a\nlearned cost model. Its follow-up work, Tiresias [3], further\nextends Proteus to support automatic indexing.\nPros and Cons. The adaptive hybrid data layout has a\nlower storage cost and a higher throughput. However, such\nmethods have two main drawbacks. First, the hybrid stor-\nage increases the system’s complexity in query processing as\nmany execution rules need to be re-implemented. Second,\nthe transaction processing over hybrid storage leads to\nfrequent random accesses, resulting in multiple disk I/Os.\n4.3 Data Synchronization\nAs data resides in multiple replicas, efficiently synchroniz-\ning the latest transaction data, i.e., the deltas, to the read\nreplicas is required. In addition, since some read replicas\nadopt a columnar format, it is important to have a tailored\nmethod to merge the deltas to the column store efficiently.\nTo address such a problem, two kinds of synchronization\nmethods are proposed. Namely, in-memory delta merging\nand log-based delta merging.\n4.3.1 In-Memory Delta Merging\nThis type of technique synchronizes the data between the\nrow store and column store using the in-memory delta\nmerging. Particularly, there are three key techniques: (1)\nthreshold-based merging; (2) delete-table merging; and (3)\ndictionary-based merging.\n(1) Threshold-Based Merging. This technique periodi-\ncally merges the delta data to the column store, which hastwo steps. First, data updates (inserts/updates/deletes) are\nrecorded in a delta table that is normally implemented using\na heap table. Second, the delta data will be migrated to\nthe column store when its size reaches a threshold. Partic-\nularly, it can use a trickle-based mechanism that constantly\nmigrates the data in the background.\nPros and Cons. This method supports fast insertion as\nthe updates can be inserted into the heap table quickly.\nHowever, when the threshold has not been hit, it could slow\ndown the query processing. This is because the analytical\nquery will not only scan the columnar data (i.e., columnar\nscan) but also traverse the in-memory delta data that has not\nbeen merged in order to access the fresh data. Therefore, it\nhas a larger delta traversing overhead since the heap table\nis unordered, and the analytical query requires a full-table\nscan to access the heap table.\n(2) Delete-Table Merging. This method depicts the\ndelete table-based merging [60], which periodically merges\nthe delta data to the column store based on a delete table;\nthe delta store is an index-organized table (i.e., B-tree) that\nmaintains the latest transaction data. The delete table is\na bitmap that holds the row IDs (RIDs), where each one\nindicates a row’s location in the column store. The delta\nmerging consists of two phases. In the first phase, it assigns\na RID to each row from the delta store and inserts it into the\ndelete table. Then, it transforms the row-wise delta data to\nthe column store with the RIDs that are hidden by the delete\ntable. In the second phase, it removes the RIDs from the\ndelete table, and truncates the data in the delta store. Note\nthat the first phase and the second phase are committed as\na transaction, respectively. This is mainly for ensuring the\ndata consistency.\nPros and Cons. Delete-table merging supports fast lookup\nas the delta data is indexed. However, inserting the data has\nadditional overhead due to the data insertion to the index\nand deleting the table.\n(3) Dictionary-Based Merging. This method shows the\ndictionary-based merging [118], which organizes the delta\ndata in a columnar format and merges the delta to the\nprimary column store based on dictionaries. Particularly,\nit organizes the delta column by column, and maintains a\ndictionary with a data vector for each column. The delta\nmerging also consists of two phases. Firstly, new data is\nmerged to a delta column store with a local dictionary and\na data vector. Secondly, the local dictionaries are merged\ninto a primary column store with a global dictionary and a\nglobal data vector.\nPros and Cons. This method has high efficiency as both\nthe delta store and primary store are organized in a colum-\nnar format, and data is indexed using dictionaries. However,\nas each delta column is indexed by a local dictionary, the\nvolume of delta data grows drastically when the number of\ndata updates increases.\n4.3.2 Log-based Delta Merging\nThis type of technique records the delta data in the change\nlog, and then synchronizes the data between the row store\nand column store with log shipping and replaying. There are\ntwo kinds of log-based merging techniques: (1) multi-level\ndelta merging; and (2) change data capture mechanism;\n\n12\n(1) Multi-Level Delta Merging. This method [45], [136]\nmerges the deltas in the memory level and the deltas in the\ndisk level. It contains four levels from a top-down view:\nB+tree, memory level, delta space, and persistent storage.\nFirstly, data manipulation language (DML) operations, such\nas insert, delete, and update operations, are inserted into a\nB+ tree after committing the write-ahead log. Secondly, the\nwrite operations of a batch are appended to a small delta in\nthe memory, thus the merging process can be performed in\nbatches. Thirdly, the small deltas are compacted and merged\ninto larger delta files in the disk. These small deltas are\nmerged together in the order in which they were written,\nso these delta files are unordered. Nevertheless, the multi-\nversion, duplicate, and rolled-back records will be removed\nin the process of merging. Lastly, the unordered delta files\nwill be periodically merged into the persistent storage in\na columnar format. Particularly, the persistent storage or-\nganizes the data with the ordered chunks, where each one\ncovers a part of the range of the data. Since the delta files\nare out of order, the merge operation will produce a large\noverhead. Therefore, it will locate the data by searching over\nthe B+ tree, and then merge the data with the order.\nPros and Cons. Multi-level delta merging has high scala-\nbility as the delta data is organized in multiple stages with\npartitioned files. However, it has a high merging cost due to\nthe large I/O overhead.\n(2) Change Data Capture (CDC). CDC is another merg-\ning mechanism [21], [61], [83], which monitors the data\nupdates in the change log, and replicates the change log\nto the analytical store asynchronously. Generally, it treats\nthe log as the first citizen, and then migrates the valid log\nrecords to the column store.\nPros and Cons. The CDC mechanism has a high-\nperformance isolation as the OLTP and OLAP workloads are\nphysically isolated in different nodes or systems. However,\nit incurs high latency due to the log shipping and replaying.\n4.4 Query Optimization\nWe summarize three types of query optimization tech-\nniques: (1) hybrid row/column scan [45], [60]; (2) multi-\nversion indexing [127], [110] and (3) CPU/GPU acceleration\n[62], [9]. Next, we dive into each type of technique.\n4.4.1 Hybrid Row/Column Scan\nIn the HTAP databases, a complex query can be routed to\nperform against either the row store or the column store.\nWe call such an execution mode as hybrid row/column\nscan [45], [60], [92]. Moreover, a query can also be executed\nin a fine-grained way such that part of the operators are\nprocessed in the column store, and the rest of the operators\nare processed in the row store, and finally the results are\ncombined. For instance, the short-range or point queries\ncan be performed using B+ tree indexes in a row store;\ncolumn scans and complex aggregations can be processed\nusing SIMD scans in a column store; the query coordinator\nmerges the results from both execution engines to the final\nresults. Suppose a SQL query finds the license and color of\nthe vehicles registered in Beijing as follows:\nSELECT V .license, V .color\nFROM Register R, Vehicle VWHERE R.VID=V .ID and R.place =”BJ”\nSuch an SQL query contains a two-way join between the\nRegister and Vehicle tables with an equality predicate on the\nplace field. The logical plan is separated into a hybrid plan,\nwhich relies on a B+ tree to search for the qualified records\nin the Register table, then joins their VIDs with the IDs of\nthe Vehicle table in the column store, finally it returns the\nresults by projecting the columns of license and color in the\nVehicle table. Hence, the query execution can benefit from\nboth the index scan in the row store and the columnar scan.\nVarious interfaces for hybrid scan [39], [57], [60], [45]\nhave been developed. For instance, Oracle [57] can create\nan in-memory columnar table by altering the table with\n”INMEMORY” keyword, then any given SQL query can\nbe executed with a rule-based hybrid scan. SQL Server [60]\nsupports the hybrid scan by building a ”COLUMNSTORE\nINDEX” over the target attributes or tables, and then it\naccelerates the queries with cost-based columnar scans. Al-\nloyDB [39] performs a cost-based hybrid scan in the operator\ngranularity after enabling its columnar engine. TiDB [45]\nallows for creating ncolumnar replicas by altering a table T\nwith ”SET TIFLASH REPLICA n”, then the queries can be\nexecuted using distributed columnar scans. It also supports\nthe hint-based hybrid scan, which can force the access\npaths using hints. Recall the above SQL query. If a hint\n”read from storage(TIKV[Register], TIFLASH[Vehicle])” is\nadded to the SQL query, then the Register table will be\nprobed from the TIKV row store and the Vehicle table will\nbe scanned from the TIFLASH column store.\nThe key to hybrid row/column scans is to determine\nwhether a query or an operator should be executed against\nthe row or column store. However, it is not always straight-\nforward to generate an optimal plan for a more complex\nquery to which the plan space is large. Existing methods are\nmainly grouped into two types: (1) rule-based execution;\nand (2) cost-based execution.\n(1) Rule-based Execution. This type of method utilizes\nheuristic rules to execute the queries. They rely on two rules\nof thumb, namely, (i) the columnar scan is more efficient\nthan the index scan and row scan; and (ii) the index scan is\nmore efficient than the row scan. For instance, Oracle [92]\nperforms the hybrid scan by following a ”column first, row\nlater” principle: if some columns do not exist in the column\nstore, it scans them in the row store, and finally merges the\nresults from the columnar scan and row scan.\nPros and Cons. The rule-based methods have a high\nefficiency because of the short planning time by the rules of\nthumb. However, they may miss the optimal plan as they do\nnot explore the global plan space. For example, an index row\nscan may be more efficient than a columnar scan, depending\non the cost of the specific plans. Therefore, it is preferable to\nconsider the cost of candidate plans as well.\n(2) Cost-based Execution. This type of methods [45],\n[60], [102] selects the access path by comparing the cost\nof the candidate execution paths. For instance, TiDB [45]\nbuilds a cost model among the columnar scan, row scan, and\nindex scan, and it selects the access path with the minimum\ncost. Unfortunately, the cost model considers only scan\noperations. PolarDB-IMCI [134] utilizes a threshold-based\ncost model to select either the row scan or columnar scan.\n\n13\nIt relies on the row-based cost model, and if a query cost is\nbeyond a pre-defined threshold, the query is routed to the\ncolumn store. Metis [123] is a recent work that can generate\nHTAP-aware hybrid plans by considering the cost of data\nupdates and data synchronization. Its core contribution is\na new cost model that considers the delta scan, columnar\nscan, index scan, and row scan, which can guide the access\npath selection.\nPros and Cons. This method can produce query plans\nof high quality. However, existing cost functions are based\non independence and uniform distribution assumptions, so\nthe estimation may be inaccurate when these assumptions\ndo not hold. Besides, existing works lack a global and\ncomprehensive cost model for HTAP workloads.\n4.4.2 Multi-Version Indexing\nMulti-version indexing aims to accelerate HTAP through\nnew indexing methods. Two representatives are Parallel\nBinary Tree (P-Tree) [127] and Multi-Version Partitioned B-\nTree (MV-PBT) [110].\nThe main idea of P-Tree is to replicate the data paths\ninvolved in the latest transaction on the balanced binary\ntree, and it leverages multi-core processors to operate on\nthe data concurrently. Furthermore, P-Tree implements the\nsnapshot isolation level, so queries can also access snap-\nshot data visible on the index at the same time. The read\noperation obtains the pointer of the root node, and then\nfollows the index to find the visible versions of data on\nthe path; the update operation will create a new version\nof the root node, then copies all relevant paths and updates\nthe target node; both read and update operations can be\ncompleted in O(log n) time. P-Tree also supports nested\nmode across multiple tables, so queries can access cross-\ntable data through indexes without joins.\nMV-PBT utilizes a multi-version partitioned B-Tree to\nindex the updated data versions. The motivation is that\nthere could be long version chains for an MVCC-based\nHTAP database, so efficient indexing for the version chains\nis required. The main challenge is how to efficiently index\nmulti-version data of the same data and support fast queries\nof the latest visible version data. MV-PBT is based on the\npartitioned B-Tree that is divided based on the specified\nkey, and each partition has the same search key. All updates\nof the transaction will be written to the memory buffer;\nwhen the buffer is full, the data will be persisted to the\ncorresponding partitioned B-Tree.\nPros and Cons. P-Tree has a high efficiency as the data can\nbe read and updated concurrently. However, the downside\nis that it consumes large memory size and CPU resources\nas a single update leads to a copied path. Since different\nversions of the same data have been indexed into the\npartition B-Tree, it supports a fast search of visible versions\nof the data. Compared to the P-Tree, MV-PBT has a higher\nscalability as the data is indexed using a disk-based B-tree.\nHowever, it has a low throughput due to the row-based\nquery processing.\n4.4.3 CPU/GPU Acceleration\nThe heterogeneous integrated processor of CPU/GPU is\nalso an important technology for HTAP query optimiza-\ntion [9], [62], [138]. This type of technique utilizes the task-parallel nature of CPUs and the data-parallel nature of\nGPUs for handling OLTP and OLAP , respectively. Existing\nCPU/GPU methods for HTAP are based on a dual store to\nisolate the workload execution of OLTP and OLAP work-\nloads. That is, using CPU and transactional store for OLTP\nand using GPU and analytical store for OLAP . Therefore,\nthe kind of technique is also called heterogeneous HTAP\n(H2TAP) [9]. Particularly, the HTAP workload is classified\ninto OLAP workload and OLTP workload; OLAP is exe-\ncuted on the analytical store through the GPU, and OLTP\nis processed on the transactional store with CPU cores; the\ndata updated by the transactional storage can be synchro-\nnized to the analytical storage in batches. Ideally, the analyti-\ncal store will organize the data in a columnar format, and the\ntransaction store will use a row-based format. However, ex-\nisting methods only adopt purely column-based format [9],\n[62] due to the consideration of the engineering complexity.\nTherefore, this approach favors high OLAP throughput but\nhas a low OLTP throughput. It is worthwhile to mention\nthat the architecture and processing paradigm of H2TAP has\nmany variants and can be changed for different types of\nworkloads accordingly. For instance, some CPU cores can\nalso be deployed in the GPU chips to accelerate the short\nqueries in parallel [9]. Besides, recent work shows that\ntransactions can be accelerated using GPU [14], and read-\nonly queries can be executed in both CPU and GPU with a\nproper data placement strategy [138].\nPros and Cons. TheH2TAP method favors high analytical\nthroughput because queries can be accelerated by GPU or\nhybrid scan of CPU/GPU. However, it suffers from the low\nfreshness issue due to the low network bandwidth of PCIe\nbetween CPU and GPU.\n4.5 Resource Scheduling\nHTAP databases need to support the efficient execution\nof OLTP and OLAP workloads simultaneously, but the\nperformance degradation could be remarkable due to the\ndata synchronization and resource contention. Besides, data\nfreshness is another concern if OLAP cannot read the latest\nupdated data. Existing methods include freshness-driven\nscheduling and workload-driven scheduling.\n4.5.1 Freshness-Driven Scheduling\nThis method [109] switches the execution modes based on\na freshness threshold. Each execution mode adopts a par-\nticular strategy for resource allocation and data exchange.\nFor instance, the scheduler controls the execution of OLTP\nand OLAP in isolation for high throughput, and then pe-\nriodically synchronizes the data. Once the data freshness\nbecomes low, it switches to an execution mode where OLTP\nand OLAP share the same copy of data such that the queries\ncan access the fresh data directly.\nSuch a method works by varying three execution modes:\n(S1) Co-located execution with the OLTP instance and OLTP\ninstance, where the OLTP instance handles transactions and\ncan access the fresh data with the copy-on-write mechanism;\n(S2) Isolated execution with the OLTP instance and OLAP\ninstance, where OLTP instance handles transactions and\nOLAP instance processes the queries, and the delta data\nin OLTP instance is periodically synchronized to the OLAP\n\n14\ninstance; (3) hybrid execution with two OLTP instances and\nthe OLAP instance, where OLAP queries need to analyze\nthe base data in the OLAP instance and the delta data in\nOLTP instance simultaneously. For resource scheduling, the\nsystem executes S2 mode by default, which favors high-\nperformance isolation. When the data freshness is less than\nthe given threshold specified in the service-level agreement\n(SLA), it can jump to S1 or S3. There is a trade-off among S1,\nS2, and S3: S1 can analyze the fresh data immediately, but\nthe analytical capacity is limited; S3 can analyze the fresh\ndata with more CPU and storage resources, but it needs\nto access two instances for query processing. S2 handles\nthe hybrid workloads with better isolation, but the data\nfreshness is lower due to the latency of data exchange.\nPros and Cons. The freshness-driven scheduling strikes\na good balance between performance and freshness. How-\never, the system performance may fluctuate due to the\nfrequent mode switching. Therefore, it might be helpful to\ndesign some mechanisms for lazy switching.\n4.5.2 Workload-Driven Scheduling\nThis kind of methods [104], [105], [120] dynamically sched-\nules resources such as CPU, shared cache, and memory\nbandwidth, by monitoring the execution of the mixed work-\nload. Since the access patterns of OLTP and OLAP work-\nloads are different, the resource scheduling needs to adapt\nto their performance characteristics.\nFor CPU resources, workload-driven scheduling adjusts\nthe parallelism threads of OLTP and OLAP tasks. The initial\nnumber of threads of OLTP is set to the number of CPU\ncores, and the number of threads of OLAP is set to an aver-\nage parallelism based on the history statistic. The scheduling\nmethod adaptively adjusts the number of threads based on\nthe performance of executed workloads. For example, when\nCPU resource is saturated by OLAP threads, the task sched-\nuler can decrease the parallelism of OLAP while enlarging\nthe OLTP threads. When the monitoring process (e.g., a\nwatchdog [104]) detects a blocked transaction thread, it will\njoin it to a blocking queue and will try to restart the blocked\ntask. For shared cache and memory bandwidth resources, it\ncan also be dynamically adjusted by observing the workload\nexecution [120]. For example, when the OLAP throughput\ndecreases drastically in a hybrid execution, it indicates that\nthe OLTP execution affects the OLAP execution. Hence,\nmore shared cache, e.g., LLC cache, can be assigned to the\nOLAP instance. In addition, when the OLAP throughput\ndrops due to the synchronization process, it is necessary to\nallocate more resources to the synchronization process.\nPros and Cons. This method has high throughput as\nit regards the throughput metric as the first-class citizen.\nHowever, it has a low freshness as it does not consider the\nfreshness metric at all.\n5 HTAP B ENCHMARKS\nTable 3 summarizes eight state-of-the-art HTAP bench-\nmarks, including five end-to-end benchmarks and three\nmicro-benchmarks.5.1 CH-Benchmark\nCH-benchmark [26], a.k.a., TPC-CH [36], is a widely-used\nend-to-end HTAP benchmark that combines two classical\nTPC benchmarks, i.e., TPC-C [130] for benchmarking trans-\nactional processing systems, and TPC-H [131] for bench-\nmarking analytical reporting systems.\n(1)Data Schema . CH-benchmark unifies the schema\nof TPC-C and TPC-H in an application domain of retail\nbusiness, simulating the behaviors of wholesale suppliers\nthat process the customers’ orders and analyze the fresh\nsales data simultaneously. Specifically, it combines TPC-\nC’s nine tables and TPC-H’s eight tables to a schema of\n12 tables by merging the overlapping tables from TPC-H\n(i.e., customer, orders, lineitem, part) and by removing its\npartsupp table. It has adjusted the scaling model for data\ngeneration based on the number of warehouses.\n(2)Workloads Since the schema of TPC-C has not\nchanged, the CH-benchmark preserves all the five transac-\ntions of TPC-C for OLTP . For OLAP workloads, it has made\nseveral modifications on TPC-H’s. First, it has adjusted the\ntables’ names and join keys of the original 22 queries. Sec-\nond, it has reduced the arithmetic operations in the queries.\nThird, it has removed the refresh function of TPC-H as TPC-\nC has included the operations of data updates.\n(3)Execution Rule . The execution rule involves two\nsteps: (i) it first executes nstreams of OLTP workloads\nand mstreams of OLAP workloads in isolation, then (ii)\nit performs the mixed workloads with the same number of\nstreams in parallel. The main purpose is to evaluate how\nHTAP systems can handle the interference of two types\nof workloads by comparing the performance between the\nisolation mode and the hybrid mode.\n(4) Performance Metrics . CH-benchmark has a\nreference-based metric to measure performance by combin-\ning the metrics of tpmC and QphH. Particularly, tpmC is the\nnumber of transactions processed by the system per minute,\nand QphH is the number of queries handled by the system\nper hour. Two reference-based metrics are as follows:\nM(OLTP ) =tpmC\nQphH@tpmC (1)\nM(OLAP ) =tpmC\nQphH@QphH (2)\nwhere both metrics consist of two parts: the former part\nmeasures a ratio between tpmC and QphH, and the lat-\nter part is the referenced primary metric, i.e., tpmC for\nM(OLTP) and QphH for M(OLAP). Suppose M(OLTP) is\nused, and the execution rule follows the isolation-and-\nhybrid execution mode. For the isolated/sequential execu-\ntion, M(OLTP) is 2.5@5000 tpmC. Meanwhile, for the hybrid\nexecution, M(OLTP) becomes 3@5500 tpmC, indicating the\nOLTP throughput is increased with hybrid execution.\n5.2 HTAPBench\nHTAPBench [25] also combines TPC-C and TPC-H. It adopts\nthe same data schema and the same hybrid workloads in\nCH-benchmark. Its main contribution is two-fold. First, it\ncurates the parameters of OLAP queries with the concept of\na dynamic window. Second, it contains an execution rule for\n\n15\nTABLE 3\nAn Overview of HTAP Benchmarks\nBenchmark Type Domain/Task Benchmark/Workload Execution Rule Metrics\nCH-Benchmark [26]End-to-End\nBenchmarkRetail Business TPC-C+ TPC-HIsolated Execution+\nHybrid ExecutionReferenced\nThroughput\nHTAPBench [25]End-to-End\nBenchmarkRetail Business TPC-C+ TPC-HFixed OLTP workers+\nvaried OLAP workersReferenced\nThroughput\nOLxPBench [51]End-to-End\nBenchmarksRetail Business TPC-C+9 queries+5 txns Hybrid Execution Throughput\nBanking Small bank +4 queries+6 txns Hybrid Execution Throughput\nTelecom TATP+5 queries + 6 txns Hybrid Execution Throughput\nHATtrick [80]End-to-End\nBenchmarkRetail Business SSB + 3 txns Hybrid Execution2D Throughput\nFreshness\nHyBench [141]End-to-End\nBenchmarkOnline Finance18 txns+13 queries\n+12 mixed operationsHybrid ExecutionF-Score\nH-Score\nADAPT [11] Micro-benchmark Data Organization 1 insert + 4 select queries Hybrid Execution Throughput\nHAP [12] Micro-benchmark Data Organization 6 CRUD queries Hybrid Execution Throughput\nmOLxPBench [49] Micro-benchmark Data Synchronization 6 CRUD queries Hybrid Execution Tail Latency\ntargeting a fixed OLTP throughput by adaptively launching\nthe OLAP workers.\n(1)Workloads . HTAPBench employs the mixed work-\nload of TPC-C and TPC-H. However, since TPC-H adopts\nthe fixed parameters for the queries, the performance results\nacross runs are often incomparable in the hybrid execution\nmode. This is mainly because TPC-C’s workload may up-\ndate different parts of data during execution and change the\ndata distribution. To address such a problem, HTAPBench\nleverages the dynamic query generator, which utilizes a\ndensity consultant to generate the parameters for the ana-\nlytical queries to ensure the same query selectivity during\nexecution. Particularly, it curates the parameters based on\nthe DATE field to slide the time windows of queries for\naccessing the newly inserted data.\n(2)Execution Rules . Instead of scaling the query streams\nof OLTP and OLAP simultaneously, HTAPBench regards the\nOLTP throughput as the first citizen. That is, it targets a\nfixed OLTP throughput and evaluates how well the SUT\nsystems process the OLAP workloads while keeping the\nOLTP throughput and scaling the OLAP workers. Specifi-\ncally, according to the TPC-C’s specification that defines the\nmaximum of 1.286 tpmC per client, it computes the number\nof clients and warehouses for a target tpmC, then populates\nthe databases and launches the OLTP workload execution.\nThen, it gradually starts the OLAP workers based on a client\nbalancer, which monitors the throughput gap between the\nactual tpmC and the target tpmC, and periodically decides\nwhether or not to start an additional OLAP worker. Namely,\nif the current gap is larger than a margin and the resource\nis not saturated, then a new OLAP worker will be started.\nOtherwise, the balancer keeps monitoring.\n(3)Performance Metrics . Based on the execution rule,\nHTAPBench proposed a OLTP-oriented metric for measur-\ning the HTAP performance as follows:\nQpHpW =QphH\n#OLAPworkers@tpmC (3)\nwhere QpHpW denotes the number of processed Queries\nper Hour per Worker. The former part is a ratio between\nTPC-H’s metric and the total number of OLAP workers. The\nlatter part is the target tpmC.5.3 OLxPBench\nOLxPBench [51] is a composite HTAP benchmark suite. It\nconsists of three domain-specific HTAP benchmarks inher-\nited from three established OLTP benchmarks, respectively.\nNamely, Subenchmark is from the TPC-C benchmark [130]\nat a retail business scenario, Fibenchmark is from the Small-\nBank [7] benchmark at a bank scenario, and Tabenchmark\nis from the TATP [128] benchmark at a telecom scenario.\nOLxPBench makes two modifications. First, it contains new\nanalytical queries with the transactions to compose an\nHTAP workload. Second, it includes analytical transactions\nthat perform real-time queries inside the transactions.\n(1)Data Schema . The OLxPBench benchmarks have the\nschema of three original OLTP benchmarks. Particularly,\nSubenchmark preserves nine tables from TPC-C; Fibenchmark\nkeeps three tables from SmallBank, i.e., ACCOUNT, SAV-\nING, CHECKING; Tabenchmark includes all the five tables\nfrom TATP , which simulates a Home Location Register\n(HLR) database used by a mobile carrier.\n(2)Workloads and Execution Rules . Unlike CH-\nbenchmark where the analytical queries only operate on\nthe overlapping tables of TPC-C, OLxPBench follows se-\nmantic consistency, meaning that both queries and trans-\nactions shall cover all the tables. For instance, the history,\nwarehouse, and district tables will never be touched in\nthe queries of CH-benchmark, while OLxPBench contains\nseveral queries that analyze the records from these tables.\nMoreover, OLxPBench includes analytical transactions that\nare originally proposed by Gartner, which envisioned HTAP\ntransactions that include analytical operations. Specifically,\nOLxPBench implements such a transaction by adding a real-\ntime query to each transaction in all the three benchmarks.\nWith the new workload, OLxPBench offers three execution\nmodes: (1) a sequential execution mode of transactions and\nqueries. (2) a concurrent execution mode of mixed work-\nloads. and (3) an execution mode of analytical transactions.\n5.4 HATtrick\nHATtrick [80] is an end-to-end HTAP benchmark, which\ncombines an analytical benchmark, SSB [90], with a trans-\nactional component inspired by TPC-C. It has two contri-\nbutions to the performance metrics. First, it proposes the\n\n16\nmetric of the throughput frontier to capture the OLTP and\nOLAP throughput with a 2D visualization graph. Second, it\nuses the freshness score to quantify the transaction’s recency\nfor a query by the analytical client (A-client).\n(1)Data Schema . HATtrick modifies the SSB schema\nfrom two aspects. First, it adds a HISTORY table and up-\ndates the relations of CUSTOMER, SUPPLIER, and PART\nby adding new attributes that will be used by the transac-\ntions. Second, it incorporates a set of entries FRESHNESS j,\nwhere each entry stores an integer transaction number TXN-\nNUM for the i-th transactional client (T-client). TXNNUM is\nused to calculate the freshness score for the queries.\n(2)Workloads and Execution Rules . For analytical\nqueries, HATtrick contains all 13 queries of SSB with a mod-\nification to return the freshness entries. Each A-client per-\nforms a batch of 13 queries recursively with random orders.\nFor transactions, HATtrick defined three transactions based\non TPC-C, including two read-write transactions, NEW OR-\nDER and PAYMENT, and a read-only transaction, COUNT\nORDERS. Each T-client defined the ratio of transactions with\n48%, 48%, and 4%, respectively. Each transaction committed\nby the T-client jwill update the FRESHNESS jwith the\ntransaction ID accordingly. By configuring the number of\nT-clients and A-clients, HATtrick executes the workloads\nconcurrently.\n(3)Performance Metrics . HATtrick introduces two met-\nrics: throughput frontier and freshness score. Throughput\nfrontier is a 2D graph where the x-axis represents the\nthroughput of T-clients, and the y-axis depicts the through-\nput of A-clients. By fixing either the T-clients or A-clients\nwhile varying the number of the other types of clients,\nthe throughput frontier can be effectively computed. The\nplotted throughput frontiers can reflect the relationships\nbetween performance characteristics and workload interfer-\nence. For instance, if the frontier is below the diagonal line of\nthe bounding box, then the hybrid throughput is relatively\nlow, and workload contention is high. If the frontier is close\nto the bounding box, then the hybrid throughput is high,\nand the performance isolation is good. With a global clock,\nHATtrick defined the freshness score fAqof an analytical\nquery Aqas follows:\nfAq=max (0, ts\nAq−tfns\nAq) (4)\nwhere tfns\nAqis the commit time of the first transaction tfns\nthat is unseen by Aq.ts\nAqis the start time of Aq. Intuitively,\nthe smaller the score is, the fresher the data is. The larger\nthe score is, the analyzed data is more stale.\n5.5 HyBench\nAs existing benchmarks [25], [26], [51], [80] heavily rely\non traditional OLTP benchmarks (e.g., TPC-C) or OLAP\nbenchmarks (e.g., TPC-H), they fall short of providing rep-\nresentative HTAP data, workload, and metrics.\nHyBench [141] is a newly-emerged benchmark for HTAP\ndatabases, which features a new data generator, a multi-set\nworkload, and a unified metric. First, it contains a schema\nby simulating a realistic online finance HTAP scenario, and\nit provides a data generator based on a time-dependent\ngeneration phase and an anomaly generation phase. Regard-\ning the workload, it has three sets of workloads for OLTP ,OLAP , and OLXP , evaluating the performance of transaction\nprocessing, analytical processing, and hybrid processing,\nrespectively. To quantify the overall HTAP performance,\nit proposes a unified metric, H-Score, that combines the\nperformance of OLTP (TPS), OLAP (QPS), and OLXP (XPS)\nand data freshness.\n(1) Data Schema. Its schema is based on an online finance\napplication inspired by the real-world HTAP applications in\nthe field of finance technology (FinTech) [71], [129], [137].\nThe schema consists of eight tables, including CUSTOMER,\nCOMPANY, SAVINGACCOUNT, CHECKINGACCOUNT,\nTRANSFER, CHECKING, LOANAPP , and LOAN, simulat-\ning the widely-used finance activities such as saving, pay-\nment, and loan application. The data generation produces\nthe testing data based on a given scale factor (SF), and\nthe data size grows linearly as the SF increases. Instead of\nusing uniform data generation [80], [33], HyBench leverages\na time-dependent data generation to generate data in three\ntime ranges, enabling efficient and realistic data generation.\nAdditionally, it proposes an anomaly generation phase to\nproduce blocked accounts and illegal transactions, which\nsimulate realistic anomalies.\n(2) Workloads and Execution Rules. HyBench contains 18\noperational transactions, 13 analytical queries, and 12 mixed\noperations that include six analytical transactions (AT) and\nsix interactive queries (IQ), providing a rich set of workloads\nfor benchmarking HTAP databases. For instance, AT1 makes\na transfer while performing the risk controlling by analyzing\nif the target has any risks; IQ1 finds related transfers for\na blocked user on-the-fly. HyBench follows a three-phase\nexecution rule to execute the hybrid workload in a row,\nincluding an AP phase, a TP phase, and an XP phase, and\nthen it outputs a unified metric.\n(3) Performance Metrics. HyBench proposes two new\nmetrics tailored to HTAP databases, namely, F-Score and\nH-Score. F-Score is used to measure the data freshness by\nmeasuring the timestamp difference between the result set\nfrom the OLTP instance and the OLAP instance. Particularly,\nit is a general method that supports the cases of insert,\nupdate, and delete. H-Score is a unified metric that relies on\ngeometric mean to measure the overall HTAP performance.\nGiven the concurrency of TP workers and AP workers\n(n, m ), it is defined as follows:\nH-Score =SF∗3√TPS∗QPS∗XPS\nfs+ 1(5)\nwhere XPS = ATS+IQS, and ATS and IQS are analytical\ntransactions per second and interactive queries per second,\nrespectively; SF is the used scale factor; fsis the F-Score that\nis measured in seconds, and it is added with 1 to avoid that\nthe denominator is zero.\n5.6 Micro-Benchmarks\nOther than the end-to-end benchmarks, there are three syn-\nthetic micro-benchmarks [11], [12], [49] that were developed\nto evaluate the adaptive data layout in HTAP databases.\nParticularly, ADAPT [11] contains two synthetic tables with\n50 and 500 integer attributes, respectively. Each table owns\na primary key a0and has ten million tuples. The workload\noperates on either the narrow table or the wide table, i.e.,\n\n17\nthe table with 500 attributes. It contains an insert query,\nthree selection queries, and a self-join query. The selection\nqueries project a subset of the attributes ( a1, . . . a p) with a\nfilter on the primary key a0. The self-join query projects a\nsubset of the attributes with a theta-join on two random\nattributes, i.e., X.ai< Y.a j. HAP [12] was inspired by\nADAPT. For the data schema, it contains a narrow table of\n16 columns and a wide table of 160 columns. Regarding the\nworkload, it contains six queries, including a point query\nwith a projection, two aggregation queries with range filters,\nan insert query, a delete query, and an update query with an\nequality filter on the primary key. mOLxPBench [49] is a\nnew HTAP microbenchmark, which contains a single ITEM\ntable with 59 attributes and includes five queries and one\nupdate. A salient feature is that it can control the rate at\nwhich fresh data is generated and the scan range. Therefore,\nthe greater the rate and range are, the more data needs to be\nsynchronized, and the tail latency increases sharply.\n6 O PENPROBLEMS AND CHALLENGES\nData Organization for Distributed HTAP Databases. As\nvarious HTAP databases are going towards distributed ar-\nchitectures, the data organization poses many new chal-\nlenges. The first challenge is to decide which columns to\nkeep in memory, which on disk, and on which nodes. The\nsecond challenge is to decide whether the data is organized\nin a row-wise or column-wise or unified format [70], [112].\nThe third challenge is to decide which compression meth-\nods [15] should be used, and in which granularity (table\nor column or segment [31]). Proteus [2], [3] is a recent\nrepresentative that employed an offline learning method\nconsidering many factors to organize the data. However,\nthe offline learning method needs to be further justified\ndue to the high complexity of model learning and data\norganization. It might be helpful to address these challenges\nseparately to reduce the complexity. In addition, combining\nthe offline learning with lightweight online learning meth-\nods [65], [66], [67], [145], [146], [126] can also mitigate the\ntraining overhead.\nQuery Optimization in HTAP Databases. There are several\nopen problems for HTAP query optimization. The first one\nis about hybrid scans for analytical queries. As the existing\ninterface has limited functionality for hybrid scans (e.g.,\ndata cannot be exchanged between the row and column\ndata), it calls for more flexible and effective methods to\ngenerate hybrid plans. The second open problem is the\nFPGA-enabled HTAP . Recent works [111], [133] have shown\nsome promising results on such a task, but more HTAP\nworkloads need to be explored as many operators have yet\nto be implemented. Finally, how to incorporate the learned\nindexes [56], [68] for HTAP is also an open issue.\nHolistic Scheduling for HTAP. Existing freshness-driven\nscheduling [109] relies on a rule-based approach to control\nthe execution modes with different freshness settings. The\nworkload-driven approaches [118], [120] only adjust the\nnumber of OLTP and OLAP threads but do not consider the\nfreshness. Consequently, there still lacks a holistic schedul-\ning method that can orchestrate the workloads, resources,\nand freshness together. For example, if the current deltastore is too cumbersome, some OLAP queries may be sched-\nuled to OLTP instances for high freshness. Therefore, it is\npreferable to develop a holistic scheduling method that not\nonly captures the workload pattern for better performance,\nbut also satisfies the requirements of data freshness and cost.\nHTAP for Multi-Model Data Analytic. As other data mod-\nels such as graph models are calling for the support of\nHTAP [48], it is also promising to enable HTAP for multi-\nmodel data analytics. Gart [117] is a pioneering work that\nsupports the HTAP over row store, and then synchronizes\nthe delta logs to the graph store. Nevertheless, there still re-\nmain many opportunities, such as supporting HTAP-aware\nmulti-model queries [143], [144], [41] and supporting other\ndata models like semi-structured document [142].\nServing atop HTAP. Building data services on top of\nthe HTAP databases is an interesting direction to enable\nfreshness-driven data serving including real-time machine\nlearning (ML) based data analytics. Such a concept is also\ncalled HSTAP [50], [79], and how to efficiently and effec-\ntively train the ML over the incremental transaction data\nremains unexplored.\nCloud-Native HTAP Techniques. Cloud-native HTAP tech-\nniques [39], [103], [122] are just unfolding and bring many\nnew challenges for HTAP . First, since the compute and\nstorage are disaggregated, it is challenging to deliver a high\ndata freshness if the log in the storage layer has not been\nreplayed. Thus, how to guarantee the data freshness with\na low replication latency for the compute layer is an open\nproblem. Second, it is challenging to schedule the OLAP and\nOLTP workloads to meet various requirements of multiple\ntenants (e.g., throughput or freshness or cost). Hence, it\ncalls for SLA-aware HTAP scheduling methods. Third, as\nserverless computing [85], [97], [115] is becoming prevalent,\nit is still an open problem to utilize serverless computing to\nhandle the HTAP workloads.\n7 C ONCLUSION\nIn this paper, we review the recent advancement of HTAP\ndatabases. We classify the state-of-the-art HTAP databases\naccording to four storage architectures. We compare their\npros and cons, summarize the challenges and opportu-\nnities, and discuss the suitable applications. Since “one\nHTAP database cannot fit all”, we recommend choosing\ndifferent HTAP architectures to meet the requirements of\nspecific applications. Furthermore, we present their key\ntechniques regarding hybrid workload processing, data or-\nganization, data synchronization, query optimization, and\nresource scheduling, we then summarize the pros and cons\nof various techniques. We also compare and summarize\nthe state-of-the-art HTAP benchmarks, concerning domain\napplications, data schema, workload, execution rules, and\nmetrics. Finally, we discuss the research challenges and open\nproblems for HTAP techniques.\nACKNOWLEDGMENTS\nThis paper was supported by the National Key R&D Pro-\ngram of China (2023YFB4503600), NSF of China (61925205,\n62232009, 62102215), Huawei, BNRist, CCF-Huawei Popu-\nlus Grove Challenge Fund (CCF-HuaweiDBC202309). Guo-\nliang Li is the corresponding author.\n\n18\nREFERENCES\n[1] D. Abadi, P . A. Boncz, S. Harizopoulos, S. Idreos, and S. Madden.\nThe design and implementation of modern column-oriented\ndatabase systems. Found. Trends Databases , 5(3):197–280, 2013.\n[2] M. Abebe, H. Lazu, and K. Daudjee. Proteus: Autonomous\nadaptive storage for mixed workloads. In SIGMOD , pages 700–\n714. ACM, 2022.\n[3] M. Abebe, H. Lazu, and K. Daudjee. Tiresias: Enabling predictive\nautonomous storage and indexing. Proceedings of the VLDB\nEndowment , 15(11):3126–3136, 2022.\n[4] A. Ailamaki, D. J. DeWitt, M. D. Hill, and M. Skounakis. Weav-\ning relations for cache performance. In VLDB , pages 169–180.\nMorgan Kaufmann, 2001.\n[5] I. Alagiannis, S. Idreos, and A. Ailamaki. H2o: a hands-free\nadaptive store. In SIGMOD , pages 1103–1114, 2014.\n[6] A. Alhomssi and V . Leis. Scalable and robust snapshot isolation\nfor high-performance storage engines. Proceedings of the VLDB\nEndowment , 16(6):1426–1438, 2023.\n[7] M. Alomari, M. J. Cahill, A. D. Fekete, and U. R ¨ohm. The cost of\nserializability on platforms that use snapshot isolation. In ICDE ,\npages 576–585. IEEE Computer Society, 2008.\n[8] Apache Arrow. https://arrow.apache.org/, 2022.\n[9] R. Appuswamy, M. Karpathiotakis, D. Porobic, and A. Ailamaki.\nThe Case For Heterogeneous HTAP. In CIDR , 2017.\n[10] V . Arora, F. Nawab, D. Agrawal, and A. El Abbadi. Janus:\nA hybrid scalable multi-representation cloud datastore. TKDE ,\n30(4):689–702, 2017.\n[11] J. Arulraj, A. Pavlo, and P . Menon. Bridging the Archipelago\nbetween Row-stores and Column-stores for Hybrid Workloads.\nInSIGMOD , pages 583–598, 2016.\n[12] M. Athanassoulis, K. S. Bøgh, and S. Idreos. Optimal Column\nLayout for Hybrid Workloads. Proceedings of the VLDB Endow-\nment , 12(13):2393–2407, 2019.\n[13] R. Barber, C. Garcia-Arellano, R. Grosman, R. Mueller, V . Raman,\nR. Sidle, M. Spilchen, A. J. Storm, Y. Tian, P . T ¨oz¨un, et al. Evolving\ndatabases for new-gen big data applications. In CIDR , 2017.\n[14] N. Boeschen and C. Binnig. GaccO - A GPU-accelerated OLTP\nDBMS. In SIGMOD , pages 1003–1016. ACM, 2022.\n[15] M. Boissier. Robust and budget-constrained encoding configura-\ntions for in-memory database systems. Proceedings of the VLDB\nEndowment , 15(4):780–793, 2021.\n[16] M. Boissier, R. Schlosser, and M. Uflacker. Hybrid data layouts\nfor tiered HTAP databases with pareto-optimal data placements.\nInICDE , pages 209–220. IEEE, 2018.\n[17] A. Boroumand, S. Ghose, G. F. Oliveira, and O. Mutlu. Enabling\nhigh-performance and energy-efficient hybrid transactional/ana-\nlytical databases with hardware/software cooperation. In ICDE .\nIEEE, 2022.\n[18] D. Borthakur et al. Hdfs architecture guide. Hadoop apache project ,\n53(1-13):2, 2008.\n[19] M. Bouzeghoub. A Framework for Analysis of Data Freshness. In\nInternational workshop on Information quality in information systems ,\npages 59–67, 2004.\n[20] M. Butrovich, W. S. Lim, L. Ma, J. Rollinson, W. Zhang, Y. Xia, and\nA. Pavlo. Tastes great! less filling! high performance and accurate\ntraining data collection for self-driving database management\nsystems. In SIGMOD , pages 617–630. ACM, 2022.\n[21] D. Butterstein, D. Martin, K. Stolze, F. Beier, J. Zhong, and\nL. Wang. Replication at the speed of change - a fast, scalable\nreplication solution for near real-time HTAP processing. Proceed-\nings of the VLDB Endowment , 13(12):3245–3257, 2020.\n[22] W. Cao, F. Li, G. Huang, et al. Polardb-x: An elastic distributed\nrelational database for cloud-native applications. In ICDE , pages\n2859–2872. IEEE, 2022.\n[23] W. Cao, Y. Zhang, X. Yang, et al. Polardb serverless: A cloud\nnative database for disaggregated data centers. In SIGMOD ,\npages 2477–2489. ACM, 2021.\n[24] J. Chen, Y. Ding, Y. Liu, et al. Bytehtap: Bytedance’s HTAP system\nwith high data freshness and strong data consistency. Proceedings\nof the VLDB Endowment , 15(12):3411–3424, 2022.\n[25] F. Coelho, J. Paulo, R. Vilac ¸a, J. Pereira, and R. Oliveira. HTAP-\nBench: Hybrid Transactional and Analytical Processing Bench-\nmark. In Proceedings of the 8th ACM/SPEC on International Confer-\nence on Performance Engineering , pages 293–304, 2017.\n[26] R. Cole, F. Funke, L. Giakoumakis, et al. The Mixed Workload\nCH-benCHmark. In Proceedings of the Fourth International Work-\nshop on Testing Database Systems , pages 1–6, 2011.[27] J. C. Corbett, J. Dean, M. Epstein, et al. Spanner: Google’s\nglobally-distributed database. In OSDI , pages 251–264. USENIX\nAssociation, 2012.\n[28] U. Cubukcu, O. Erdogan, S. Pathak, S. Sannakkayala, and M. Slot.\nCitus: Distributed postgresql for data-intensive applications. In\nSIGMOD , pages 2490–2502, 2021.\n[29] B. Dageville, T. Cruanes, M. Zukowski, et al. The snowflake\nelastic data warehouse. In SIGMOD , pages 215–226. ACM, 2016.\n[30] C. Diaconu, C. Freedman, E. Ismert, P .-A. Larson, P . Mittal,\nR. Stonecipher, N. Verma, and M. Zwilling. Hekaton: SQL\nServer’s Memory-Optimized OLTP Engine. In SIGMOD , pages\n1243–1254, 2013.\n[31] M. Dreseler, J. Kossmann, M. Boissier, S. Klauck, M. Uflacker, and\nH. Plattner. Hyrise re-engineered: An extensible database system\nfor research in relational in-memory data management. In EDBT ,\npages 313–324, 2019.\n[32] A. Dziedzic, J. Wang, S. Das, B. Ding, V . R. Narasayya, and\nM. Syamala. Columnstore and B+ tree-Are Hybrid Physical\nDesigns Important? In SIGMOD , pages 177–190, 2018.\n[33] J. Fan, T. Liu, G. Li, J. Chen, Y. Shen, and X. Du. Relational data\nsynthesis using generative adversarial networks: A design space\nexploration. arXiv preprint arXiv:2008.12763 , 2020.\n[34] F. F ¨arber, N. May, W. Lehner, P . Große, I. M ¨uller, H. Rauhe, and\nJ. Dees. The SAP HANA Database–An Architecture Overview.\nIEEE Data Eng. Bull. , 35(1):28–33, 2012.\n[35] D. Feinberg. Setting the Record Straight: HTAP OPDBMS, 2018.\n[36] F. Funke, A. Kemper, and T. Neumann. Benchmarking hybrid\noltp&olap database systems. In BTW , volume P-180 of LNI, pages\n390–409. GI, 2011.\n[37] Geode. Performance is key. Consistency is a must, 2022.\n[38] A. K. Goel, J. Pound, N. Auch, P . Bumbulis, S. MacLean, F. F ¨arber,\nF. Gropengiesser, C. Mathis, T. Bodner, and W. Lehner. Towards\nScalable Real-Time Analytics: An Architecture for Scale-Out of\nOLxP Workloads. Proceedings of the VLDB Endowment , 8(12):1716–\n1727, 2015.\n[39] Google AlloyDB. AlloyDB for PostgreSQL, 2024.\n[40] M. Grund, J. Kr ¨uger, H. Plattner, A. Zeier, P . Cudre-Mauroux,\nand S. Madden. Hyrise: a main memory hybrid storage engine.\nProceedings of the VLDB Endowment , 4(2):105–116, 2010.\n[41] Q. Guo, C. Zhang, S. Zhang, and J. Lu. Multi-model query\nlanguages: taming the variety of big data. Distributed and Parallel\nDatabases , 42(1):31–71, 2024.\n[42] HBase. Apache HBase Reference Guide, 2016.\n[43] D. Hieber and G. Grambow. Hybrid transactional and analytical\nprocessing databases-state of research and production usage.\n[44] D. Hieber and G. Grambow. Hybrid transactional and analytical\nprocessing databases: A systematic literature review. In DATA\nANALYTICS , pages 90–98, 2020.\n[45] D. Huang, Q. Liu, Q. Cui, Z. Fang, X. Ma, F. Xu, L. Shen, L. Tang,\nY. Zhou, M. Huang, et al. TiDB: A Raft-based HTAP Database.\nProceedings of the VLDB Endowment , 13(12):3072–3084, 2020.\n[46] N. Jeba and S. Rathi. Effective data management and real-time\nanalytics in internet of things. Int. J. Cloud Comput. , 10(1/2):112–\n128, 2021.\n[47] H. Jiang, C. Liu, J. Paparrizos, A. A. Chien, J. Ma, and A. J.\nElmore. Good to the last bit: Data-driven encoding with codecdb.\nInSIGMOD , pages 843–856. ACM, 2021.\n[48] M. A. Jibril, A. Baumstark, and K.-U. Sattler. Adaptive update\nhandling for graph htap. Distributed and Parallel Databases , pages\n1–27, 2023.\n[49] G. Kang, S. Chen, and H. Li. Benchmarking htap databases\nfor performance isolation and real-time analytics. BenchCouncil\nTransactions on Benchmarks, Standards and Evaluations , 3(2):100122,\n2023.\n[50] G. Kang, L. Wang, S. Chen, and J. Zhan. Nhtapdb: Native htap\ndatabases. arXiv preprint arXiv:2302.09927 , 2023.\n[51] G. Kang, L. Wang, W. Gao, F. Tang, and J. Zhan. Olxpbench: Real-\ntime, semantically consistent, and domain-specific are essential in\nbenchmarking, designing, and implementing HTAP systems. In\nICDE , pages 1822–1834. IEEE, 2022.\n[52] A. Kemper and T. Neumann. Hyper: A hybrid oltp&olap main\nmemory database system based on virtual memory snapshots. In\nICDE , pages 195–206. IEEE, 2011.\n[53] J. Kim, K. Kim, H. Cho, et al. Rethink the scan in MVCC\ndatabases. In SIGMOD , pages 938–950. ACM, 2021.\n[54] J. Kim, J. Yu, J. Ahn, S. Kang, and H. Jung. Diva: Making MVCC\nsystems htap-friendly. In SIGMOD , pages 49–64. ACM, 2022.\n\n19\n[55] K. Kim, T. Wang, R. Johnson, and I. Pandis. ERMIA: fast memory-\noptimized database system for heterogeneous workloads. In\nSIGMOD , pages 1675–1687. ACM, 2016.\n[56] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The\ncase for learned index structures. In G. Das, C. M. Jermaine, and\nP . A. Bernstein, editors, SIGMOD , pages 489–504. ACM, 2018.\n[57] T. Lahiri, S. Chavan, M. Colgan, D. Das, A. Ganesh, M. Gleeson,\nS. Hase, A. Holloway, J. Kamp, T.-H. Lee, et al. Oracle Database\nIn-Memory: A Dual Format In-Memory Database. In ICDE , pages\n1253–1258. IEEE, 2015.\n[58] L. Lamport. Paxos made simple, fast, and byzantine. In A. Bui\nand H. Fouchal, editors, OPODIS , volume 3, pages 7–9. Suger,\nSaint-Denis, rue Catulienne, France, 2002.\n[59] H. Lang, T. M ¨uhlbauer, F. Funke, P . A. Boncz, T. Neumann, and\nA. Kemper. Data blocks: Hybrid OLTP and OLAP on compressed\nstorage using both vectorization and compilation. In SIGMOD ,\npages 311–326. ACM, 2016.\n[60] P .- ˚A. Larson, A. Birka, E. N. Hanson, W. Huang,\nM. Nowakiewicz, and V . Papadimos. Real-Time Analytical\nProcessing with SQL Server. VLDB , 8(12):1740–1751, 2015.\n[61] J. Lee, S. Moon, et al. Parallel Replication across Formats in SAP\nHANA for Scaling Out Mixed OLTP/OLAP workloads. VLDB ,\n10(12):1598–1609, 2017.\n[62] R. Lee, M. Zhou, C. Li, S. Hu, J. Teng, D. Li, and X. Zhang. The\nArt of Balance: A RateupDB Experience of Building a CPU/GPU\nHybrid Database Product. Proceedings of the VLDB Endowment ,\n14(12):2999–3013, 2021.\n[63] G. Li, H. Dong, and C. Zhang. Cloud databases: New techniques,\nchallenges, and opportunities. Proceedings of the VLDB Endow-\nment , 15(12):3758–3761, 2022.\n[64] G. Li and C. Zhang. HTAP databases: What is new and what is\nnext. In SIGMOD , pages 2483–2488. ACM, 2022.\n[65] G. Li and X. Zhou. Machine learning for data management: A\nsystem view. In ICDE , pages 3198–3201. IEEE, 2022.\n[66] G. Li, X. Zhou, and L. Cao. AI Meets Database: AI4DB and\nDB4AI. In SIGMOD , pages 2859–2866, 2021.\n[67] G. Li, X. Zhou, and L. Cao. Machine learning for databases. In\nAIMLSystems 2021: The First International Conference on AI-ML-\nSystems, Bangalore India, October 21 - 23, 2021 , pages 28:1–28:2.\nACM, 2021.\n[68] G. Li, X. Zhou, and L. Cao. Machine learning for databases.\nVLDB , 14(12):3190–3193, 2021.\n[69] Q. Li, A. Nourbakhsh, S. Shah, and X. Liu. Real-time novel event\ndetection from social media. In ICDE , pages 1129–1139. IEEE\nComputer Society, 2017.\n[70] T. Li, M. Butrovich, A. Ngom, W. S. Lim, W. McKinney, and\nA. Pavlo. Mainlining databases: Supporting fast transactional\nworkloads on universal columnar data file formats. Proceedings\nof the VLDB Endowment , 14(4):534–546, 2020.\n[71] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heteroge-\nneous graph neural networks for malicious account detection. In\nCIKM , pages 2077–2085. ACM, 2018.\n[72] Z. Lu, Q. Cao, H. Jiang, Y. Chen, J. Yao, and A. Pan. Fluidkv:\nSeamlessly bridging the gap between indexing performance and\nmemory-footprint on ultra-fast storage.\n[73] C. Luo, P . T ¨oz¨un, Y. Tian, R. Barber, V . Raman, and R. Sidle. Umzi:\nUnified multi-zone indexing for large-scale HTAP. In EDBT ,\npages 1–12. OpenProceedings.org, 2019.\n[74] Z. Lyu, H. H. Zhang, G. Xiong, G. Guo, H. Wang, J. Chen,\nA. Praveen, Y. Yang, X. Gao, A. Wang, et al. Greenplum: A hybrid\ndatabase for transactional and analytical workloads. In SIGMOD ,\npages 2530–2542, 2021.\n[75] L. Ma, D. V . Aken, A. Hefny, G. Mezerhane, A. Pavlo, and\nG. J. Gordon. Query-based workload forecasting for self-driving\ndatabase management systems. In SIGMOD , pages 631–645.\nACM, 2018.\n[76] L. Ma, W. Zhang, J. Jiao, W. Wang, M. Butrovich, W. S. Lim,\nP . Menon, and A. Pavlo. MB2: decomposed behavior modeling\nfor self-driving database management systems. In SIGMOD ,\npages 1248–1261. ACM, 2021.\n[77] D. Makreshanski, J. Giceva, C. Barthels, and G. Alonso. Batchdb:\nEfficient isolated execution of hybrid oltp+ olap workloads for\ninteractive applications. In SIGMOD , pages 37–50, 2017.\n[78] MariaDB. Deploy an HTAP Server with MariaDB ColumnStore\n5.5 and Community Server 10.6, 2021.\n[79] MatrixOne. HSTAP architecture. https://docs.matrixorigin.cn,\n2024.[80] E. Milkai, Y. Chronis, K. P . Gaffney, Z. Guo, J. M. Patel, and X. Yu.\nHow good is my HTAP system? In SIGMOD , pages 1810–1824.\nACM, 2022.\n[81] B. Mozafari, J. Ramnarayan, S. Menon, Y. Mahajan,\nS. Chakraborty, H. Bhanawat, and K. Bachhav. Snappydata:\nA unified cluster for streaming, transactions and interactice\nanalytics. In CIDR , volume 17, pages 8–11, 2017.\n[82] T. M ¨uhlbauer, W. R ¨odiger, et al. Scyper: A hybrid oltp&olap\ndistributed main memory database system for scalable real-time\nanalytics. In BTW , pages 499–502, 2013.\n[83] MySQL Heatwave. Real-time Analytics for MySQL Database\nService, 2024.\n[84] M. Nakamura, T. Tabaru, Y. Ujibashi, T. Hashida, M. Kawaba,\nand L. Harada. Extending postgresql to handle olxp workloads.\nInINTECH 2015 , pages 40–44, 2015.\n[85] V . R. Narasayya and S. Chaudhuri. Cloud Data Services: Work-\nloads, Architectures and Multi-Tenancy. Foundations and Trends\nin Databases , 10(1):1–107, 2021.\n[86] T. Neumann, T. M ¨uhlbauer, and A. Kemper. Fast Serializable\nMulti-Version Concurrency Control for Main-Memory Database\nSystems. In SIGMOD , pages 677–689, 2015.\n[87] Nikita Ivanov. How HTAP Enables Real-Time Banking Services\nAt Scale, 2021.\n[88] NoisePage. https://noise.page/, 2022.\n[89] D. P . A. Nugroho and H. A. Ismail. In-memory database and\nmemsql.\n[90] P . E. O’Neil, E. J. O’Neil, X. Chen, and S. Revilak. The star schema\nbenchmark and augmented fact table indexing. In TPCTC ,\nvolume 5895 of Lecture Notes in Computer Science , pages 237–252.\nSpringer, 2009.\n[91] D. Ongaro and J. K. Ousterhout. In search of an understandable\nconsensus algorithm. In USENIX ATC , pages 305–319. USENIX\nAssociation, 2014.\n[92] Oracle 21c. Automating Management of In-Memory Objects.\n[93] F. ¨Ozcan, Y. Tian, and P . T ¨oz¨un. Hybrid Transactional/Analytical\nProcessing: A Survey. In SIGMOD , pages 1771–1775, 2017.\n[94] A. Pavlo, G. Angulo, J. Arulraj, et al. Self-driving database\nmanagement systems. In CIDR . www.cidrdb.org, 2017.\n[95] A. Pavlo and M. Aslett. What’s really new with newsql? SIGMOD\nRec., 45(2):45–55, 2016.\n[96] D. Peng and F. Dabek. Large-scale incremental processing using\ndistributed transactions and notifications. In OSDI , pages 251–\n264. USENIX, 2010.\n[97] M. Perron, R. C. Fernandez, D. J. DeWitt, and S. Madden. Star-\nling: A scalable query engine on cloud functions. In SIGMOD ,\npages 131–141. ACM, 2020.\n[98] M. Pezzini, D. Feinberg, N. Rayner, and R. Edjlali. Hybrid\nTransaction/Analytical Processing Will Foster Opportunities For\nDramatic Business Innovation. Gartner , pages 4–20, 2014.\n[99] M. Pezzini, D. Feinberg, N. Rayner, and R. Edjlali. Real-time In-\nsights and Decision Making using Hybrid Streaming, In-Memory\nComputing Analytics and Transaction Processing. 2016.\n[100] M. Pezzini, D. Feinberg, N. Rayner, and R. Edjlali. Magic Quad-\nrant for Cloud Database Management Systems. Gartner (2021,\nDecember 13) , pages 1–37, 2021.\n[101] Phoenix. OLTP and operational analytics for Apache Hadoop.\n[102] PolarDB. PolarDB HTAP Real-Time Data Analysis Technology\nDecryption, 2021.\n[103] A. Prout, S. Wang, J. Victor, et al. Cloud-Native Transactions and\nAnalytics in SingleStore. In SIGMOD , pages 2340–2352, 2022.\n[104] I. Psaroudakis, T. Scheuer, N. May, and A. Ailamaki. Task\nscheduling for highly concurrent analytical and transactional\nmain-memory workloads. In ADMS , pages 36–45, 2013.\n[105] I. Psaroudakis, F. Wolf, N. May, T. Neumann, A. B ¨ohm, A. Aila-\nmaki, and K.-U. Sattler. Scaling Up Mixed Workloads: A Battle\nof Data Freshness, Flexibility, and Scheduling. In TPCTC , pages\n97–112. Springer, 2014.\n[106] X. Qiu, W. Cen, Z. Qian, Y. Peng, Y. Zhang, X. Lin, and J. Zhou.\nReal-time constrained cycle detection in large dynamic graphs.\nProceedings of the VLDB Endowment , 11(12):1876–1888, 2018.\n[107] J. T. S. Quah and M. Sriganesh. Real-time credit card fraud\ndetection using computational intelligence. Expert Syst. Appl. ,\n35(4):1721–1732, 2008.\n[108] V . Raman, G. Attaluri, R. Barber, N. Chainani, D. Kalmuk, V . Ku-\nlandaiSamy, J. Leenstra, S. Lightstone, S. Liu, G. M. Lohman, et al.\nDB2 with BLU Acceleration: So Much More Than Just A Column\nStore. VLDB , 6(11):1080–1091, 2013.\n\n20\n[109] A. Raza, P . Chrysogelos, A. C. Anadiotis, and A. Ailamaki. Adap-\ntive HTAP Through Elastic Resource Scheduling. In SIGMOD ,\npages 2043–2054, 2020.\n[110] C. Riegger, T. Vinc ¸on, R. Gottstein, and I. Petrov. MV-PBT: Multi-\nVersion Indexing for Large Datasets and HTAP Workloads. In\nEDBT , pages 217–228, 2020.\n[111] S. Roozkhosh, D. Hoornaert, J. H. Mun, et al. Relational memory:\nNative in-memory accesses on rows and columns. In EDBT ,\npages 66–79. OpenProceedings.org, 2023.\n[112] M. Sadoghi, S. Bhattacherjee, B. Bhattacharjee, and M. Canim.\nL-store: A real-time oltp and olap system. arXiv preprint\narXiv:1601.04084 , 2016.\n[113] B. Samwel, J. Cieslewicz, B. Handy, J. Govig, et al. F1 query:\nDeclarative querying at scale. Proceedings of the VLDB Endowment ,\n11(12):1835–1848, 2018.\n[114] H. Saxena, L. Golab, S. Idreos, and I. F. Ilyas. Real-Time LSM-\nTrees for HTAP Workloads. arXiv preprint arXiv:2101.06801 , 2021.\n[115] J. Schleier-Smith, V . Sreekanti, A. Khandelwal, et al. What\nserverless computing is and should become: the next phase of\ncloud computing. Commun. ACM , 64(5):76–84, 2021.\n[116] S. Shen, R. Chen, H. Chen, and B. Zang. Retrofitting High\nAvailability Mechanism to Tame Hybrid Transaction/Analytical\nProcessing. In OSDI , pages 219–238, 2021.\n[117] S. Shen, Z. Yao, L. Shi, L. Wang, L. Lai, Q. Tao, L. Su, R. Chen,\nW. Yu, H. Chen, et al. Bridging the gap between relational\n{OLTP }and graph-based {OLAP }. In 2023 USENIX Annual\nTechnical Conference (USENIX ATC 23) , pages 181–196, 2023.\n[118] V . Sikka, F. F ¨arber, W. Lehner, S. K. Cha, T. Peh, and C. Bornh ¨ovd.\nEfficient Transaction Processing in SAP HANA Database: The\nEnd of A Column Store Myth. In SIGMOD , pages 731–742, 2012.\n[119] U. Sirin and A. Ailamaki. Micro-architectural analysis of OLAP:\nlimitations and opportunities. Proceedings of the VLDB Endow-\nment , 13(6):840–853, 2020.\n[120] U. Sirin, S. Dwarkadas, and A. Ailamaki. Performance Charac-\nterization of HTAP Workloads. In ICDE , pages 1829–1834, 2021.\n[121] U. Sirin, P . T ¨oz¨un, D. Porobic, and A. Ailamaki. Micro-\narchitectural analysis of in-memory oltp. In SIGMOD , pages 387–\n402, 2016.\n[122] Snowflake Unistore. Getting Started with Transactional and\nAnalytical data in Snowflake, 2024.\n[123] H. Song, W. Zhou, F. Li, X. Peng, and H. Cui. Rethink query\noptimization in htap databases. Proceedings of the ACM on Man-\nagement of Data , 1(4):1–27, 2023.\n[124] Splice Machine. Defining HTAP, 2017.\n[125] StoneDB. A Real-time HTAP Database, 2022.\n[126] J. Sun, J. Zhang, Z. Sun, G. Li, and N. Tang. Learned cardinality\nestimation: A design space exploration and a comparative evalu-\nation. Proceedings of the VLDB Endowment , 15(1):85–97, 2021.\n[127] Y. Sun, G. E. Blelloch, W. S. Lim, and A. Pavlo. On Supporting\nEfficient Snapshot Isolation for Hybrid Workloads with Multi-\nVersioned Indexes. VLDB , 13(2), 2019.\n[128] TATP. TATP Benchmark Description (Version 1.0), 2009.\n[129] Tecent. Webank. www.webank.com, 2023.\n[130] Transaction Processing Performance Council. TPC-C, 2021.\n[131] Transaction Processing Performance Council. TPC-H, 2021.\n[132] A. Verbitski, A. Gupta, D. Saha, et al. Amazon aurora: De-\nsign considerations for high throughput cloud-native relational\ndatabases. In SIGMOD , pages 1041–1052. ACM, 2017.\n[133] T. Vinc ¸on, C. Kn ¨odler, L. Solis-Vasquez, et al. Near-data pro-\ncessing in database systems on native computational storage\nunder HTAP workloads. Proceedings of the VLDB Endowment ,\n15(10):1991–2004, 2022.\n[134] J. Wang, T. Li, H. Song, X. Yang, W. Zhou, F. Li, B. Yan, Q. Wu,\nY. Liang, C. Ying, et al. Polardb-imci: A cloud-native htap\ndatabase system at alibaba. Proceedings of the ACM on Management\nof Data , 1(2):1–25, 2023.\n[135] Y. Wu, J. Arulraj, J. Lin, R. Xian, and A. Pavlo. An empirical eval-\nuation of in-memory multi-version concurrency control. VLDB ,\n10(7):781–792, 2017.\n[136] J. Yang, I. Rae, J. Xu, et al. F1 lightning: Htap as a service.\nProceedings of the VLDB Endowment , 13(12):3313–3325, 2020.\n[137] Z. Yang, C. Yang, F. Han, et al. Oceanbase: A 707 million tpmc\ndistributed relational database system. Proceedings of the VLDB\nEndowment , 15(12):3385–3397, 2022.\n[138] B. W. Yogatama, W. Gong, and X. Yu. Orchestrating data place-\nment and query execution in heterogeneous CPU-GPU DBMS.\nProceedings of the VLDB Endowment , 15(11):2491–2503, 2022.[139] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and\nI. Stoica. Spark: Cluster computing with working sets. HotCloud ,\n10(10-10):95, 2010.\n[140] C. Zhang, G. Li, F. Hua, and J. Zhang. Survey of Key Techniques\nof HTAP Databases. Journal of Software , 34(2):761–785, 2022.\n[141] C. Zhang, G. Li, and T. Lv. HyBench: A New Benchmark for\nHTAP Databases. Proceedings of the VLDB Endowment , 17, 2024.\n[142] C. Zhang and J. Lu. Selectivity Estimation for Relation-Tree Joins.\nInSSDBM , pages 1–12, 2020.\n[143] C. Zhang and J. Lu. Holistic evaluation in multi-model databases\nbenchmarking. Distributed Parallel Databases , 39(1):1–33, 2021.\n[144] C. Zhang, J. Lu, P . Xu, and Y. Chen. UniBench: A Benchmark for\nMulti-model Database Management Systems. In TPCTC , volume\n11135, pages 7–23. Springer, 2018.\n[145] J. Zhang, C. Zhang, G. Li, and C. Chai. AutoCE: An Accurate\nand Efficient Model Advisor for Learned Cardinality Estimation.\nInICDE , pages 2621–2633. IEEE, 2023.\n[146] J. Zhang, C. Zhang, G. Li, and C. Chai. PACE: Poisoning Attacks\non Learned Cardinality Estimation. Proceedings of the ACM on\nManagement of Data , 2(1):1–27, 2024.\nChao Zhang is a postdoctoral researcher at\nTsinghua University. He was awarded the Ph.D.\ndegree in Computer Science at the University\nof Helsinki, Finland. He has given a tutorial on\nHTAP databases in SIGMOD 2022 and gave\na tutorial on cloud databases in VLDB 2022.\nHe serves as a PC member of SIGMOD 2024-\n2025, VLDB 2023 Tutorial, and ICDE 2023.\nHis research interests focus on heterogeneous\ndatabase management systems.\nGuoliang Li is an IEEE fellow, and a full pro-\nfessor at the Department of Computer Science,\nTsinghua University. His research interests in-\nclude database systems, large-scale data clean-\ning, and integration. He received the VLDB 2017\nEarly Research Contribution Award, TCDE 2014\nEarly Career Award, SIGMOD 2023 Best Pa-\npers, VLDB 2020 Best Papers, and ICDE 2018\nBest Paper. He served as a general chair of\nSIGMOD 2021, a demo chair of VLDB 2021, and\nan industry chair of ICDE 2022.\nJintao Zhang is a master student at Tsinghua\nUniversity. He received his bachelor’s degree\nin Computer Science at Xidian University. His\nresearch interests focus on the intersection be-\ntween database and machine learning.\nXinning Zhang is a master student at Tsinghua\nUniversity. He received his bachelor’s degree in\nComputer Science at Zhejiang University. His\nresearch interests focus on HTAP databases.\nJianhua Feng is a full professor at the Depart-\nment of Computer Science, Tsinghua University.\nHe received his bachelor’s degree in Computer\nScience at Tsinghua University. His research in-\nterests focus on cutting-edge database manage-\nment systems.",
  "textLength": 126240
}