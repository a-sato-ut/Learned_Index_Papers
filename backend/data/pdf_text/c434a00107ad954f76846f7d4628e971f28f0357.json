{
  "paperId": "c434a00107ad954f76846f7d4628e971f28f0357",
  "title": "Improved Learning-augmented Algorithms for k-means and k-medians Clustering",
  "pdfPath": "c434a00107ad954f76846f7d4628e971f28f0357.pdf",
  "text": "Improved Learning-augmented Algorithms for k-means and\nk-medians Clustering\nThy Nguyen∗, Anamay Chaturvedi∗, Huy Lê Nguy ˜ên∗\n{nguyen.thy2,chaturvedi.a,hu.nguyen} @northeastern.edu\nKhoury College of Computer Sciences,\nNortheastern University\nMarch 2, 2023\nAbstract\nWe consider the problem of clustering in the learning-augmented setting, where we are given a data\nset ind-dimensional Euclidean space, and a label for each data point given by an oracle indicating what\nsubsets of points should be clustered together. This setting captures situations where we have access to\nsome auxiliary information about the data set relevant for our clustering objective, for instance the labels\noutput by a neural network. Following prior work, we assume that there are at most an \u000b2(0;c)for some\nc<1fraction of false positives and false negatives in each predicted cluster, in the absence of which the\nlabels would attain the optimal clustering cost OPT . For a dataset of size m, we propose a deterministic k-\nmeans algorithm that produces centers with improved bound on clustering cost compared to the previous\nrandomized algorithm while preserving the O(dmlogm)runtime. Furthermore, our algorithm works even\nwhen the predictions are not very accurate, i.e. our bound holds for \u000bup to 1=2, an improvement over\n\u000bbeing at most 1=7in the previous work. For the k-medians problem we improve upon prior work by\nachieving a biquadratic improvement in the dependence of the approximation factor on the accuracy\nparameter\u000bto get a cost of (1 +O(\u000b))OPT , while requiring essentially just O(mdlog3m=\u000b)runtime.\n1 Introduction\nIn this paper we study k-means and k-medians clustering in the learning-augmented setting. In both these\nproblems we are given an input data set Pofmpoints ind-dimensional Euclidean space and an associated\ndistance function dist (\u0001;\u0001). The goal is to compute a set Cofkpoints in that same space that minimize the\nfollowing cost function:\ncost(P;C) =X\np2Pmin\ni2[k]dist(p;ci):\nIn words, the cost associated with a singular data point is its distance to the closest point in C, and the cost\nof the whole data set is the sum of the costs of its individual points.\nIn thek-means setting dist (x;y) :=kx\u0000yk2, i.e., the square of the Euclidean distance, and in the k-\nmedians setting we set dist (x;y) :=kx\u0000yk, although here instead of the norm of x\u0000y, we can in principle\nalso use any other distance function. These problem are well-studied in the literature of algorithms and\nmachine learning, and are known to be hard to solve exactly [Dasgupta, 2008], or even approximate well\nbeyond a certain factor [Cohen-Addad and Karthik C. S., 2019]. Although approximation algorithms are\nknown to exist for this problem and are used widely in practice, the theoretical approximation factors of\npractical algorithms can be quite large, e.g., the 50-approximation in Song and Rajasekaran [2010] and the\n∗Equal contribution. All three authors were supported in part by NSF CAREER grant CCF-1750716 and NSF grant\nCCF-1909314.\n1arXiv:2210.17028v2  [cs.LG]  1 Mar 2023\n\nO(lnk)-approximation in Arthur and Vassilvitskii [2006]. Meanwhile, the algorithms with relatively tight\napproximation factors do not necessarily scale well in practice [Ahmadian et al., 2019].\nTo overcome these computational barriers, Ergun et al. [2022] proposed a learning-augmented setting\nwhere we have access to some auxiliary information about the input data set. This is motivated by the\nfact that in practice we expect the dataset of interest to have exploitable structures relevant to the optimal\nclustering. For instance, a classiﬁer’s predictions of points in a dataset can help group similar instances\ntogether. This notion was formalized in Ergun et al. [2022] by assuming that we have access to a predictor\nin the form of a labelling P=P1[\u0001\u0001\u0001[Pk(all the points in Pihave the same label i2[k]), such that there\nexist an unknown optimal clustering P=P\u0003\n1[\u0001\u0001\u0001[P\u0003\nk, an associated set of centers C= (c\u0003\n1;:::;c\u0003\nk)that\nachieve the optimally low clustering cost OPT(P\ni2[k]cost(Pi;fc\u0003\nig) = OPT ), and a known label error rate\n\u000bsuch that:\njPi\\P\u0003\nij\u0015(1\u0000\u000b) max(jPij;jP\u0003\nij)\nInsimplerterms, theauxiliarypartitioning (P1;:::;Pk)isclosetosomeoptimalclustering: eachpredicted\ncluster has at most an \u000b-fraction of points from outside its corresponding optimal cluster, and there are at\nmost an\u000b-fraction of points in the corresponding optimal cluster not included in predicted cluster. The\npredictor, in other words, has at most \u000bfalse positive and false negative rate for each label.\nObserve that even when the predicted clusters Piare close to a set of true clusters P\u0003\niin the sense\nthat the label error rate \u000bis very small, computing the means or medians of Pican lead to arbitrarily bad\nsolutions. It is known that for k-means the point that is allocated for an optimal cluster should simply be the\naverage of all points in that cluster (this can be seen by simply diﬀerentiating the convex 1-mean objective\nand solving for the minimizer). However, a single false positive located far from the cluster can move this\nallocated point arbitrarily far from the true points in the cluster and drive the cost up arbitrarily high. This\nproblem requires the clustering algorithms to process the predicted clusters in a way so as to preclude this\npossibility.\nUsing tools from the robust statistics literature, the authors of Ergun et al. [2022] proposed a randomized\nalgorithm that achieves a (1 + 20\u000b)-approximation given a label error rate \u000b < 1=7and a guarantee that\neach predicted cluster has \n\u0000k\n\u000b\u0001\npoints. For the k-medians problem, the authors of Ergun et al. [2022] also\nproposed an algorithm that achieves a (1+\u000b0)-approximation if each predicted cluster contains \n\u0000n\nk\u0001\npoints\nand a label rate \u000bat mostO\u0010\n\u000b04\nklogk\n\u000b0\u0011\n;where the big-Oh notation hides some small unspeciﬁed constant,\nand\u000b0<1.\nThe restrictions for the label error rate \u000bto be small in both of the algorithms of Ergun et al. [2022] lead\nus to investigate the following question:\nIs it possible to design a k-means and a k-medians algorithm that achieve (1 +\u000b)-approximate clustering\nwhen the predictor is not very accurate?\n1.1 Our Contributions\nIn this work, we not only give an aﬃrmative answer to the question above for both the k-means and the\nk-medians problems, our algorithms also have improved bounds on the clustering cost, while preserving the\ntime complexity of the previous approaches and removing the requirement on a lower bound on the size of\neach predicted cluster.\nFor learning-augmented k-means, we modify the main subroutine of the previous randomized algorithm\nto get a deterministic method that works for all \u000b<1=2, which is the natural breaking point (as explained\nbelow). In the regime where the k-means algorithm of Ergun et al. [2022] applies, we get improve the\napproximation factor to 1 + 7:7\u000b. For the larger domain \u000b2[0;1=2), we derive a more general expression\nas reproduced in table 1. Furthermore, our algorithm has better bound on the clustering cost compared to\nthat of the previous approach, while preserving the O(mdlogm)runtime and not requiring a lower bound\non the size of each predicted cluster.\nOurk-medians algorithm improves upon the algorithm in Ergun et al. [2022] by achieving a (1 +O(\u000b))-\napproximation for \u000b < 1=2, thereby improving both the range of \u000bas well as the dependence of the ap-\nproximation factor on the label error rate from bi-quadratic to near-linear. For success probability 1\u0000\u000e,\nour runtime is O(1\n1\u00002\u000bmdlog3m\n\u000blogklog(k=\u000e)\n(1\u00002\u000b)\u000elogk\n\u000e), so we see that by setting \u000e= 1=poly(k), we have just a\nlogarithmic dependence in the run-time on k, as opposed to a polynomial dependence.\n2\n\nWork, Problem Approx. Factor Label Error Range Time Complexity\nErgun et al. [2022], k-Means 1 + 20\u000b (10 logmpm;1=7)O(mdlogm)\nAlgorithm 1, k-Means 1 +5\u000b\u00002\u000b2\n(1\u00002\u000b)(1\u0000\u000b)[0,1/2) O(mdlogm)\n1 + 7:7\u000b [0;1=7)\nErgun et al. [2022], k-Medians 1 +~O((k\u000b)1=4)small constantO(mdlog3m+\npoly(k;logm))\nAlgorithm 2, k-Medians 1 +7+10\u000b\u000010\u000b2\n(1\u0000\u000b)(1\u00002\u000b)[0;1=2)~O\u0010\n1\n1\u00002\u000bmd\nlog3mlog2k\n\u000e\u0001\nTable 1: Comparison of learning-augmented k-means and k-medians algorithms. We recall that mis the\ndata set size, dis the ambient dimension, \u000bis the label error rate, and \u000eis the failure probability (where\napplicable). The success probability of the k-medians algorithm of Ergun et al. [2022] is 1\u0000poly(1=k). The\n~Onotation hides some log factors to simplify the expressions.\nUpper bound on \u000b. Note that if the error label rate \u000bequals 1=2, then even for three clusters there is\nno longer a clear relationship between the predicted clusters and the related optimal clusters - for instance\ngiven three optimal clusters P\u0003\n1;P\u0003\n2;P\u0003\n3with equally many points, if for all i2[3], the predicted clusters\nPiconsist of half the points in P\u0003\niand half the points in P\u0003\n(i+1) mod 3, then the label error rate \u000b= 1=2is\nachieved, but there is no clear relationship between P\u0003\niandPi. In other words, it is not clear whether the\npredicted labels give us any useful information about an optimal clustering. In this sense, \u000b= 1=2is in a\nway a natural stopping point for this problem.\n1.2 Related Work\nThis work belongs to a growing literature on learning-augmented algorithms. Machine learning has been\nused to improve algorithms for a number of classical problems, including data structures [Kraska et al., 2018,\nMitzenmacher, 2018, Lin et al., 2022], online algorithms [Purohit et al., 2018], graph algorithms [Khalil et al.,\n2017, Chen et al., 2022a,b], computing frequency estimation [Du et al., 2021] , caching [Rohatgi, 2020, Wei,\n2020], and support estimation [Eden et al., 2021]. We refer the reader to Mitzenmacher and Vassilvitskii\n[2020] for an overview and applications of the framework.\nAnother relevant line of work is clustering with side information. The works Balcan and Blum [2008],\nAwasthi et al. [2014], Vikram and Dasgupta [2016] studied an interactive clustering setting where an oracle\ninteractively provides advice about whether or not to merge two clusters. Basu et al. [2004] proposed an\nactive learning framework for clustering, where the algorithm has access to a predictor that determines if two\npoints should or should not belong to the same cluster. Ashtiani et al. [2016] introduced a semi-supervised\nactive clustering framework where the algorithm has access to a predictor that answers queries whether two\nparticular points belong in an optimal clustering. The goal is to produce a (1 +\u000b)-approximate clustering\nwhile minimizing the query complexity to the oracle.\nApproximation stability, proposed in Balcan et al. [2013], is another assumption proposed to circumvent\nthe NP-hardness of approximation for k-means clustering. More formally, the concept of (c;\u000b)-stability\nrequires that every c-approximate clustering is \u000b-close to the optimal solution in terms of the fraction of\nincorrectly clustered points. This is diﬀerent from our setting, where at most an \u000bfraction of the points are\nincorrectly clustered and can worsen the clustering cost arbitrarily.\nGamlath et al. [2022] studies the problem of k-means clustering in the presence of noisy labels, where\nthe cluster label of each point created by either an adversarial or a random perturbation of the optimal\nsolution. Their Balanced Adversarial Noise Model assumes that the size of the symmetric diﬀerence between\nthe predicted cluster Piand optimal cluster P\u0003\niis bounded by \u000bjP\u0003\nij. The algorithm uses a subroutine\nwith runtime exponential in kanddfor a ﬁxed \u000b\u00141=4. In this work, we have diﬀerent assumptions on\nthe predicted cluster cluster Piand the optimal cluster P\u0003\ni. Moreover, our focus is on eﬃcient algorithms\npractical nearly linear-time algorithms that can scale to very large datasets for k-means and k-medians\nclustering.\n3\n\nAlgorithm 1 Deterministic Learning-augmented k-Means Clustering\nRequire: Data setPofmpoints, Partition P=P1[:::Pkfrom a predictor, accuracy parameter \u000b\nfori2[k]do\nforj2[d]do\nLet!i;jbe the collection of all subsets of (1\u0000\u000b)micontiguous points in Pi;j.\nIi;j argminZ2!i;jcost(Z;Z) = argminZ2!i;jP\nz2Zz2\u00001\njZj(P\nz0inZz0)2\nend for\nLetbci= (Ii;j)j2[d]\nend for\nReturnfbc1;:::;bckg\n2k-Means\nWe brieﬂy recall some notation for ease of reference.\nDeﬁnition 1. We make the following deﬁnitions:\n1. The given data set is denoted as P, andm:=jPj. The output of the predictor is a partition (P1;:::Pk)\nofP. Further,mi:=jPij.\n2. There exists an optimal partition (P\u0003\n1;:::;P\u0003\nk)and centers (c\u0003\n1;:::;c\u0003\nk)such thatP\ni2[k]cost(P\u0003\ni;c\u0003\ni) =\nOPT, the optimally low clustering cost for the data set P. Furthermore, m\u0003\ni:=jP\u0003\nij. For each cluster\ni2[k], denote the set of true positives P\u0003\ni\\Pi=Qi. Recall thatjQij\u0015(1\u0000\u000b) max(jPij;jP\u0003\nij), for\nsome\u000b<1=2.\n3. We denote the average of a set XbyX. For the sets XiandPiwe denote their projections onto the\nj-th dimension by Xi;jandPi;j, respectively.\nBefore we describe our algorithm, we recall why the naive solution of simply taking the average of each\ncluster provided by the predictor is insuﬃcient. Consider Pi, the set of points labeled iby the predictor.\nRecall that the optimal 1-means solution for this set is its mean, Pi. Since the predictor is not perfect,\nthere might exist a number of points in Pithat are not actually in P\u0003\ni. Thus, if the points in PinP\u0003\niare\nsigniﬁcantly far away from P\u0003\ni, they will increase the clustering cost arbitrary if we simply use Pias the\ncenter. The following well-known identity formalizes this observation.\nLemma 2 (Inaba et al. [1994]) .Consider a set X\u001aRdof sizenandc2Rd,\ncost(X;c) = min\nc02Rdcost(X;c0) +n\u0001kc\u0000Xk2= cost(X;X) +n\u0001kc\u0000Xk2:\nIdeally, wewouldliketobeabletorecovertheset Qi=Pi\\P\u0003\niandusetheaverageof Qiasthecenter. We\nknowthatjQinP\u0003\nij\u0014\u000bm\u0003\ni. Bylemma3, itisnothardtoshowthat cost(P\u0003\ni;Qi)\u0014\u0010\n1 +\u000b\n1\u0000\u000b\u0011\ncost(P\u0003\ni;P\u0003\ni) =\u0010\n1 +\u000b\n1\u0000\u000b\u0011\ncost(P\u0003\ni;c\u0003\ni), which also implies a\u0010\n1 +\u000b\n1\u0000\u000b\u0011\n- approximation for the problem.\nLemma 3. For any partition J1[J2of a setJ\u001aRof sizen, ifjJ1j\u0015(1\u0000\u0015)n, thenjJ\u0000J1j2\u0014\n\u0015\n(1\u0000\u0015)ncost(J;J).\nSince we do not have access to Qi, the main technical challenge is to ﬁlter out the outlier points in Pi\nand construct a center close to Qi. Minimizing the distance of the center to Qiimplies reducing the distance\ntoc\u0003\nias well as the clustering cost.\nOur algorithm for k-means, algorithm 1, iterates over all clusters given by the predictor and ﬁnds a set of\ncontiguous points of size (1\u0000\u000b)miwith the smallest clustering cost in each dimension. At the high level, our\nanalysis shows that the average of the chosen points, Ii;j, is not too far away from that of the true positives,\nQi;j. This also implies that the additive clustering cost of Ii;jwould not be too large. Since we can analyze\nthe clustering cost by bounding the cost in every cluster iand dimension j, for simplicity we will not refer\n4\n\nto a speciﬁc iandjwhen discussing the intuition of the algorithm. The proofs of the following lemmas and\ntheorem are included in the appendix.\nNote that there can be multiple optimal solutions in the optimization step. The algorithm can either be\nrandomized by choosing an arbitrary set, or can also be deterministic by always choosing the ﬁrst optimal\nsolution. Lemma 4 shows that the optimization step guarantees that Ii;jhas the smallest clustering cost\nwith respect to all sets of size (1\u0000\u000b)miinPi;j.\nLemma 4. For alli2[k];j2[d], let!0\ni;jbe the collection of all subsets of (1\u0000\u000b)mipoints inPi;j. Then\ncost(Ii;j;Ii;j) = min\nZ02!0\ni;jcost(Z0;Z0):\nSince we know that jQi;jj\u0015(1\u0000\u000b)mi, it can be shown from lemma 4 that the cost of the set Ii;jis\nsmaller than that of Qi;j. More precisely,\ncost(Ii;j;Ii;j)\u0014(1\u0000\u000b)mi\njQijcost(Qi;j;Qi;j): (1)\nWith this fact, we are ready to bound the clustering cost by bounding jIi;j\u0000Qi;jj2,\njIi;j\u0000Qi;jj2\u00142jIi;j\u0000Ii;j\\Qi;jj2+ 2jIi;j\\Qi;j\u0000Qi;jj2:\nUsing lemma 3, we can bound jIi;j\u0000Ii;j\\Qi;jj2andjIi;j\\Qi;j\u0000Qi;jj2respectively by cost(Ii;j;Ii;j)and\ncost(Qi;j;Qi;j). Combining this fact with eq. (1), we can bound, jIi;j\u0000Qi;jj2bycost(Qi;j;Qi;j).\nLemma 5. The following bound holds:\njIi;j\u0000Qi;jj2\u00144\u000b\n1\u00002\u000bcost(Qi;j;Qi;j)\njQij:\nNotice that lemma 5 also applies to any set in !i;jwith cost smaller than the expected cost of a subset of\nsize(1\u0000\u000b)midrawn uniformly at random from Qi;j. Instead of repeatedly sampling diﬀerent subsets of Qi;j\nand returning the one with the lowest clustering cost, the optimization step not only simpliﬁes the analysis\nof the algorithm, but also guarantees that we ﬁnd such a subset eﬃciently. This is the main innovation of\nthe algorithm.\nIn the notations of lemma 2, we can consider c=Ii;j;P\u0003\ni;j=X;m\u0003\ni=n. Thus, we want to bound\njP\u0003\ni;j\u0000Ii;jj2bycost(P\u0003\ni;j;P\u0003\ni;j)\nm\u0003\nito achieve a (1 +O(\u000b))-approximation. Recall that we bound jIi;j\u0000Qi;jj2by\ncost(Qi;j;Qi;j)\njQijin lemma 5. In lemma 6 we relate cost(Qi;j;Qi;j)tocost(P\u0003\ni;j;P\u0003\ni;j)as follows,\ncost(P\u0003\ni;j;P\u0003\ni;j)\u00151\u0000\u000b\n\u000bm\u0003\nijP\u0003\ni;j\u0000Qi;jj2+ cost(Qi;j;Qi;j)\nWe can then apply lemma 5 to bound jP\u0003\ni;j\u0000Ii;jj2bycost(P\u0003\ni;j;P\u0003\ni;j)\nm\u0003\ni.\nLemma 6. The following bound holds:\njP\u0003\ni;j\u0000Ii;jj2\u0014cost(P\u0003\ni;j;P\u0003\ni;j)\u0012\u000b\n1\u0000\u000b+4\u000b\n(1\u00002\u000b)(1\u0000\u000b)\u0013\n=m\u0003\ni\nApplying lemma 6 and lemma 2 to all i2[K];j2[d], we are able to bound the total clustering cost.\nTheorem 7. Algorithm 1 is a deterministic algorithm for k-means clustering such that given a data set\nP2Rm\u0002dand a partition (P1;:::;Pk)with error rate \u000b < 1=2, it outputs a\u0010\n1 +\u0010\n\u000b\n1\u0000\u000b+4\u000b\n(1\u00002\u000b)(1\u0000\u000b)\u0011\u0011\n-\napproximation in time O(dmlogm):\nCorollary 8. For\u000b\u00141=7, algorithm 1 achieves a clustering cost of (1 + 7:7\u000b)OPT.\n5\n\nAlgorithm 2 Learning-augmented k-Medians Clustering\nRequire: Data setPofmpoints, Partition P=P1[:::Pkfrom a predictor, accuracy parameter \u000b<1=2\nfori2[k]do\nLetR=2\n1\u00002\u000blog2k\n\u000e\nforj2[R]do\nSamplex\u0018Piu.a.r.\nLetP0\nibe thed\u000bmiepoints farthest from x\nbcj\ni median ofPinP0\ni.\nend for\nLetbcibe thebcj\niwith minimum cost\nend for\nReturnf^c1;:::; ^ckg\n3k-Medians\nIn this section we describe our algorithm for learning-augmented k-medians clustering and a theoretical\nbound on the clustering cost and the run-time. Our algorithm works for ambient spaces equipped with any\nmetric dist (\u0001;\u0001)for which it is possible to eﬃciently compute the geometric median, which is the minimizer\nof the 1-medians clustering cost. For instance, it is known from prior work [Cohen et al., 2016] that the\ngeometric median with respect to the `2-metric can be eﬃciently calculated, and appealing to this result as\na subroutine allows us to derive a guarantee for learning-augmented k-medians with respect to the `2norm.\nTheorem 9. (Cohen et al. [2016]) There is an algorithm that computes a (1 +\u000f)-approximation to the\ngeometric median of a set of size nind-dimensional Euclidean space with respect to the `2distance metric\nwith constant probability in O(ndlog3(n=\u000f))time.\nLooking ahead at the pseudocode of algorithm 2, we see that to eventually derive a bound on the time\ncomplexity, we would need to account for adjusting the success probability in the many calls to theorem 9.\nCorollary 10. It follows from theorem 9 that with probability 1\u0000\u000e\n2k, we have that for all j2[R],\ncost(PinP0\ni;bcj\ni)is a (1 +\r)-approximation to the optimal 1-median cost for PinP0\niwhile taking time\nO(midlog3(mi=\r) log(Rk=\u000e )).\nWe refer the reader to deﬁnition 1 for all notation that is undeﬁned in this section; the only additional\nnotation we introduce is the following deﬁnition.\nDeﬁnition 11. We make the following deﬁnitions:\n1. We denote the optimal clustering cost of PbyOPT, and the optimal 1-median clustering cost of P\u0003\ni\nbyOPTi, with which notation we have thatP\ni2[k]OPTi= OPT.\n2. We denote the distance dist (x;y)between two points by kx\u0000yk.\nWe now describe at a high-level a run of our algorithm. Algorithm 2 operates sequentially on each cluster\nestimate; for the cluster estimate Pi, it samples a point x2Piuniformly at random, and removes from Pi\nthed\u000bmie-many points that lie furthest from x. It then computes the median of the clipped set, which is\nwhere we appeal to an algorithm for the geometric median, for instance theorem 9 when the ambient metric\nfor the input data set is the `2metric. It turns out that this subroutine already gives us a good median\nfor the cluster P\u0003\niwith constant probability (lemma 14); to boost the success probability we repeat this\nsubroutine some R-many times (the exact expression is given in the pseudocode and justiﬁed in lemma 15),\nand pick the median with the lowest cost, denoted bci. Collecting the bciacrossi2[k], we get our ﬁnal\nsolutionf^c1;:::; ^ckg.\nAlthough our algorithm itself is relatively straightforward, the analysis turns out to be more involved.\nWe trace the proof at a high level in this section and mention the main steps, and defer all proofs to the\nappendix.\n6\n\nWe see that it would suﬃce to allocate a center that works well for the true cluster P\u0003\ni, but we only have\naccess to the set Piwith the promise that they have a signiﬁcant overlap (as characterized by \u000b). Fixing an\narbitrary true median c\u0003\ni, one key insight is that the “false\" points, i.e. points in PinP\u0003\niwill only signiﬁcantly\ndistort the median if they happen to lie far from c\u0003\ni. If there were a way to identify and remove these false\npoints which lie far from c\u0003\ni, then simply computing the geometric median of the clipped data set should\nwork well.\nBy a direct application of Markov’s inequality it is possible to show that a point xpicked uniformly at\nrandom will in fact lie at a distance on the order of the average clustering cost OPTi=miwith constant\nprobability, as formalized in lemma 12.\nLemma 12. With probability1\u00002\u000b\n2,kx\u0000c\u0003\nik\u00142OPTi=mi.\nAs we will condition on this good event holding, it will be convenient to introduce the notation E.\nDeﬁnition 13. We letEdenote the event that kx\u0000c\u0003\nik\u00142OPTi=mi.\nHaving identiﬁed a good point xto serve as a proxy for where the true median c\u0003\nilies, we need to ﬁgure\nout a good way to clip the data set so as to avoid false points which lie very far from c\u0003\ni. We observe\nthat since there are guaranteed to be at most d\u000bne-many false points, if we were to remove the d\u000bne-many\npoints that lie farthest from x(denotedP0\ni), then we either remove false points that lie very far from c\u0003\ni, or\ntrue points ( P\u0003\ni\\P0\ni) which are at the same distance from c\u0003\nias the remaining false points (the points in\nPin(P0\ni[P\u0003\ni). In particular, this implies that the impact of the remaining false points is roughly dominated\nby the clustering cost of an equal number of true points, and we are able to exploit this to show that the\nclustering cost of PinP0\niwith respect to its own median estimate bciis already close to that of the true center\nP\u0003\ni.\nLemma 14. Conditioned onE,cost(PinP0\ni;bcj\ni)\u0014(1 + 5\u000b)OPTi.\nSince the eventEthat the randomly sampled point xis close to a true median c\u0003\niis true only with\nconstant probability, we boost the success probability by running this subroutine some Rtimes and letting\nbcibe the median estimate with respect to which the respective clipped data set had the lowest clustering\ncost.\nLemma 15. ForR=O\u0010\n1\n(1\u00002\u000b)log\u00002k\n\u000e\u0001\u0011\nmany repetitions, with probability at least 1\u0000\u000e\n2k, we have that\ncost(PinP0\ni;bci)\u0014(1 + 5\u000b)OPTi.\nWe see from lemma 21 that the set PinP0\nidiﬀers from the true positives Pi\\P\u0003\niby sets of size at most\nd\u000bne. It follows that as long as the distance between bciandc\u0003\niis on the order of OPTi=n, they will not\ninﬂuence the clustering cost by more than an O(\u000bOPTi)additive term, which we will be able to absorb into\nthe(1 +O(\u000b))multiplicative approximation factor. We formalize this in lemma 16.\nLemma 16. Ifcost(PinP0\ni;bci)\u0014(1 + 5\u000b)OPT, thenkbci\u0000c\u0003\nik\u00142+5\u000b\n(1\u00002\u000b)OPTi\nn.\nWe ﬁnally put everything together to show that the clustering cost of the set of true points Pi\\P\u0003\niwith\nrespect to the estimate bciis only at most an additive O(\u000bOPTi)more than the cost with respect to the true\nmedianc\u0003\ni. The key technical point in the analysis is that we can only appeal to the fact that the cost of\nPinP0\niis close toOPTi, and we cannot directly reason about bciapart from appealing to lemma 16.\nLemma 17. With probability 1\u0000\u000e=k,cost(Pi\\P\u0003\ni;bci)\u0014cost(Pi\\P\u0003\ni;c\u0003\ni) +(5\u000b+10\u000b2)OPTi\n1\u00002\u000b.\nWe can now derive our main cost bound stated in lemma 18. Doing so only requires that we account for\nthe mislabelled points P\u0003\ninPiwhich were not accounted for during our clustering. Again, from lemma 16 it\nsuﬃces to appeal to the fact that the estimate bcilies within an O(\u000bOPTi=n)distance of the true median c\u0003\ni.\nLemma 18. With probability 1\u0000\u000e=k,cost(P\u0003\ni;^ci)\u0014(1 +c\u000b)OPTiforc=7+10\u000b\u000010\u000b2\n(1\u0000\u000b)(1\u00002\u000b).\nWe now formalize our main cost bound, success probability and run-time guarantees in theorem 19.\n7\n\nTheorem 19. There is an algorithm for k-medians clustering such that given a data set Pand a labelling\n(P1;:::;Pk)with error rate \u000b<1=2, it outputs a set of centers bC= (bc1;:::;bck)such thatP\ni2kcost(P\u0003\ni;bci)\u0014\n(1 +c\u000b)OPTiforc=7+10\u000b\u000010\u000b2\n(1\u0000\u000b)(1\u00002\u000b), and does so in time O\u0010\n1\n1\u00002\u000bmdlog3\u0000m\n\u000b\u0001\nlog\u0010\nklog(k=\u000e)\n(1\u00002\u000b)\u000e\u0011\nlog\u0000k\n\u000e\u0001\u0011\n.\nProof.We see from lemma 18 that by applying our subroutine for 1-median clustering on each labelled\npartitionPi, we get a center bciwith the promise that with probability 1\u0000\u000e\nk;cost(P\u0003\ni;bci) = (1 +c\u000b)OPTi.\nBy the union bound, it follows that with probability 1\u0000\u000e,P\ni2[k]cost(P\u0003\ni;bci)\u0014P\ni2k(1 +c\u000b)OPTi=\n(1 +c\u000b)OPT. SinceP=P\u0003\n1[\u0001\u0001\u0001[P\u0003\nk, it follows that cost(P;^C) = (1 +c\u000b)OPT.\nThe time taken to execute the 1-median clustering subroutine on partition PiisR(mid+O(milogmi) +\nO(midlog3(mi=\r) log(Rk=\u000e ))+mid). This is because we have Riterations, in each of which we ﬁrst compute\nthe distances of all mipoints from the sampled point xin timemid, followed by sorting the mimany points\nbytheirdistancesintime O(milogmi), followedby O(log(Rk=\u000e ))manyiterationsofthemediancomputation\nfor the clipped sets (wherein we appeal to corollary 10), followed by a calculation of the 1-median clustering\ncostachievedintime mid. Werecallthatweset R=O\u0010\n1\n1\u00002\u000blogk\n\u000e\u0011\n. Further, wenotethattheexpressionfor\nthe upper bound on the time complexity is convex in mi, so if we were to denote the value of this expression\non a set of size mibyT(mi);it follows thatP\ni2[k]T(mi)\u0014T\u0010P\ni2[k]mi\u0011\n=T(m). Putting everything\ntogether, we get that the net time complexity is O\u0010\n1\n1\u00002\u000bmdlog3\u0000m\n\u000b\u0001\nlog\u0010\nklog(k=\u000e)\n(1\u00002\u000b)\u000e\u0011\nlog\u0000k\n\u000e\u0001\u0011\n.\n4 Experiments\nIn this section, we evaluate algorithm 1 and algorithm 2 on real-world datasets. Our experiments were done\non a i9-12900KF processor with 32GB RAM. For all experiments, we ﬁx the number of points to be allocated\nk= 10, and report the average and the standard deviation error of the clustering cost over 20independent\nruns1.\nDatasets. We test the algorithms on the testing set of the CIFAR-10 dataset [Krizhevsky et al., 2009]\n(m= 104;d= 3072), thePHYdataset from KDD Cup 2004 [KDD Cup 2004], and the MNIST dataset\n[Deng, 2012] ( m= 1797;d= 64). For the PHY dataset , we take m= 104random samples to form our\ndataset (d= 50).\nPredictor description. For each dataset, we create a predictor by ﬁrst ﬁnding good k-means and\nk-medians solutions. Speciﬁcally, for k-means we initialize by kmeans++ and then run Lloyd’s algorithm\nuntil convergence. For k-medians, we use the \"alternating\" heuristic [Park and Jun, 2009] of the k-medoids\nproblem to ﬁnd the center of each cluster. In both settings, we use the label given to each point by the\nk-means and k-medians solutions to form the optimal partition (P\u0003\n1;:::;P\u0003\n10)(recall we set k= 10). In order\nto test the algorithms’ performance under diﬀerent error rates of the predictor, for each cluster i, we change\nthe labels of the \u000bmipoints closest to the mean (or median) to that of a random center. For every dataset,\nwe generate the set of corrupted labels (P1;:::;P 10)for\u000bfrom 0:1to0:5. Furthermore, we use the same\nset of optimal partition (P\u0003\n1;:::;P\u0003\n10)across all instances of the algorithms. By ﬁxing the optimal partition,\nwe can investigate the eﬀects of increasing \u000bon the clustering cost.\nGuessing the error rate. Note that in most situations, we will not have access to the error rate \u000band\nmust try out diﬀerent guesses of \u000bthen return the clustering with the best cost. For algorithm 1, algorithm 2,\nand thek-medians algorithm of Ergun et al. [2022], we iterate over 15 possible value of \u000buniformly spanning\nthe interval [0:1;0:5]. For thek-means algorithm of Ergun et al. [2022], the algorithm is deﬁned for \u000b<1=5\n(not to be confused with the assumption that \u000b<1=7for the bound on the clustering cost). Thus, the range\nis[0:1;1=5]for the algorithm.\nBaselines. We report the clustering costs of the initial optimalk-means and k-medians solution\n(P\u0003\n1;:::;P\u0003\n10)along with that of the naive approach of taking the average and geometric median of each\ngroup returned by the predictor , e.g., returning (P1;:::;P10)fork-means. The two baselines help us see\nhow much the clustering cost increases for diﬀerent error rate \u000b. The clustering cost of the algorithm without\ncorruption can also be seen as a lower bound on the cost of the learning-augmented algorithms. Following\nErgun et al. [2022], we use random sampling as another baseline. We ﬁrst randomly select a q-fraction of\n1The repository is hosted at github.com/thydnguyen/LA-Clustering.\n8\n\n0.1 0.2 0.3 0.4 0.5\nError rate8.48.68.89.09.29.49.69.8Clustering cost1e10\nOPT\nPredictor\nRandom Sampling\nErgun, Jon, et al.\nOurs(a) CIFAR-10\n0.1 0.2 0.3 0.4 0.5\nError rate1.151.201.251.301.351.401.451.501.55Clustering cost1e6\nOPT\nPredictor\nRandom Sampling\nErgun, Jon, et al.\nOurs (b) MNIST\n0.1 0.2 0.3 0.4 0.5\nError rate2.062.082.102.122.142.162.18Clustering cost1e11\nOPT\nErgun, Jon, et al.\nOurs(c) PHY\nFigure 1: Experimental comparison of algorithm 1 with prior work and baselines for k-Means\n0.1 0.2 0.3 0.4 0.5\nError rate2.922.942.962.983.003.023.043.063.08Clustering cost1e7\nOPT\nPredictor\nRandom Sampling\nErgun, Jon, et al.\nOurs\n(a) CIFAR-10\n0.1 0.2 0.3 0.4 0.5\nError rate48000490005000051000520005300054000Clustering costOPT\nPredictor\nRandom Sampling\nErgun, Jon, et al.\nOurs (b) MNIST\n0.1 0.2 0.3 0.4 0.5\nError rate4.254.504.755.005.255.505.75Clustering cost1e7\nOPT\nPredictor\nRandom Sampling\nErgun, Jon, et al.\nOurs (c) PHY\nFigure 2: Experimental comparison of algorithm 2 with prior work and baselines for k-Medians\npoints from each cluster for qvaried from 1%to50%. Then, we compute the means and the geometric me-\ndians of the sampled points to calculate the clustering cost. Finally, we return the clustering corresponding\nto the value of qwith the best cost.\nWe use the implementation provided in Ergun et al. [2022] for their k-means algorithm. Although both\nourk-medians algorithm and the algorithm in Ergun et al. [2022] use the approach in Cohen et al. [2016]\nas the subroutine to compute the geometric median in nearly linear time, we use Weiszfeld’s algorithm as\nimplemented in Pillutla et al. [2022], a well-known method to compute the geometric medians, for the k-\nmedians algorithms. To generate the predictions, we use Pedregosa et al. [2011], Scikit-Learn-Contrib [2021]\nfor the implementations of the k-means and k-medoids algorithms, and the code provided in Ergun et al.\n[2022] for the implementation of their k-means algorithm.\nFor algorithm 2, we can treat the number of rounds Ras a hyperparameter. We set R= 1; as shown\nbelow, this is already enough to achieve a good performance compared to the other approaches.\n4.1 Results\nIn Figure 1, we omit the Sampling and the Prediction approach for the PHY dataset as they have much\nlarger clustering cost than ours and the k-means algorithm in Ergun et al. [2022]. For the CIFAR-10 dataset,\nwe observe that the approach in Ergun et al. [2022] has slightly better clustering costs as \u000bincreases. For\nthe MNIST dataset, our approach has slightly improved costs across all values of \u000b. For the PHY dataset,\nobserve that algorithm 1 is comparable to the Ergun et al. [2022].\nIn summary, the mean clustering cost of the two learning-augmented algorithms are similar across the\ndatasets. It is important to note that our algorithm achieves similar clustering cost to that of Ergun et al.\n[2022] without any variance as it is a deterministic technique.\nFigure 2 shows that our our k-medians algorithm has the best clustering cost across all the datasets. We\nalso observe that the sampling approach outperforms the approach of Ergun et al. [2022] for the CIFAR-10\nand the MNIST datasets. This is expected since the latter algorithm sample a random subset of a ﬁxed size\nin each cluster while the baseline approach samples subsets of diﬀerent sizes and uses the one with the best\n9\n\ncost.\nReferences\nSanjoy Dasgupta. The hardness of k-means clustering . Department of Computer Science and Engineering,\nUniversity of California, San Diego, 2008.\nVincent Cohen-Addad and Karthik C. S. Inapproximability of clustering in lp metrics. In David Zuckerman,\neditor,60th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2019, Baltimore,\nMaryland, USA, November 9-12, 2019 , pages 519–539. IEEE Computer Society, 2019. doi: 10.1109/\nFOCS.2019.00040. URL https://doi.org/10.1109/FOCS.2019.00040 .\nMingjunSongandSanguthevarRajasekaran. Fastalgorithmsforconstantapproximationk-meansclustering.\nTrans. Mach. Learn. Data Min. , 3(2):67–79, 2010.\nDavid Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. Technical report,\nStanford, 2006.\nSara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward. Better guarantees for k-means and\neuclidean k-median by primal-dual algorithms. SIAM Journal on Computing , 49(4):FOCS17–97, 2019.\nJon C. Ergun, Zhili Feng, Sandeep Silwal, David Woodruﬀ, and Samson Zhou. Learning-augmented\n$k$-means clustering. In International Conference on Learning Representations , 2022. URL https:\n//openreview.net/forum?id=X8cLTHexYyY .\nTim Kraska, Alex Beutel, Ed H Chi, Jeﬀrey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 international conference on management of data , pages 489–504,\n2018.\nMichael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. Advances in\nNeural Information Processing Systems , 31, 2018.\nHonghao Lin, Tian Luo, and David Woodruﬀ. Learning augmented binary search trees. In International\nConference on Machine Learning , pages 13431–13440. PMLR, 2022.\nManish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions. Advances\nin Neural Information Processing Systems , 31, 2018.\nElias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization\nalgorithms over graphs. Advances in neural information processing systems , 30, 2017.\nJustin Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental graph algorithms via learned\npredictions. In International Conference on Machine Learning , pages 3583–3602. PMLR, 2022a.\nJustin Y Chen, Talya Eden, Piotr Indyk, Honghao Lin, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal,\nTal Wagner, David Woodruﬀ, and Michael Zhang. Triangle and four cycle counting with predictions\nin graph streams. In International Conference on Learning Representations , 2022b. URL https://\nopenreview.net/forum?id=8in_5gN9I0 .\nElbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the “learning” into learning-augmented\nalgorithms for frequency estimation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th\nInternational Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research ,\npages 2860–2869. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/du21d.html .\nDhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In Proceedings of the\nFourteenth Annual ACM-SIAM Symposium on Discrete Algorithms , pages 1834–1845. SIAM, 2020.\nAlexander Wei. Better and simpler learning-augmented online caching. In Approximation, Randomiza-\ntion, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM 2020) . Schloss\nDagstuhl-Leibniz-Zentrum für Informatik, 2020.\n10\n\nTalya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner. Learning-\nbased support estimation in sublinear time. arXiv preprint arXiv:2106.08396 , 2021.\nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. CoRR, abs/2006.09123, 2020.\nURL https://arxiv.org/abs/2006.09123 .\nMaria-Florina Balcan and Avrim Blum. Clustering with interactive feedback. In International Conference\non Algorithmic Learning Theory , pages 316–328. Springer, 2008.\nPranjal Awasthi, Maria Balcan, and Konstantin Voevodski. Local algorithms for interactive clustering. In\nInternational Conference on Machine Learning , pages 550–558. PMLR, 2014.\nSharad Vikram and Sanjoy Dasgupta. Interactive bayesian hierarchical clustering. In International Confer-\nence on Machine Learning , pages 2081–2090. PMLR, 2016.\nSugato Basu, Arindam Banerjee, and Raymond J Mooney. Active semi-supervision for pairwise constrained\nclustering. In Proceedings of the 2004 SIAM international conference on data mining , pages 333–344.\nSIAM, 2004.\nHassan Ashtiani, Shrinu Kushagra, and Shai Ben-David. Clustering with same-cluster queries. Advances in\nneural information processing systems , 29, 2016.\nMaria-Florina Balcan, Avrim Blum, and Anupam Gupta. Clustering under approximation stability. Journal\nof the ACM (JACM) , 60(2):1–34, 2013.\nBuddhima Gamlath, Silvio Lattanzi, Ashkan Norouzi-Fard, and Ola Svensson. Approximate cluster recovery\nfrom noisy labels. In Conference on Learning Theory , pages 1463–1509. PMLR, 2022.\nMary Inaba, Naoki Katoh, and Hiroshi Imai. Applications of weighted voronoi diagrams and randomization\nto variance-based k-clustering. In Proceedings of the tenth annual symposium on Computational geometry ,\npages 332–339, 1994.\nMichael B. Cohen, Yin Tat Lee, Gary L. Miller, Jakub Pachocki, and Aaron Sidford. Geometric median in\nnearly linear time. CoRR, abs/1606.05225, 2016. URL http://arxiv.org/abs/1606.05225 .\nAlex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.\nKDD Cup 2004. Kdd cup 2004. url=https://osmot.cs.cornell.edu/kddcup/index.html, 2004. Accessed:\n2022-09-28.\nLi Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal\nProcessing Magazine , 29(6):141–142, 2012.\nHae-Sang Park and Chi-Hyuck Jun. A simple and fast algorithm for k-medoids clustering. Expert systems\nwith applications , 36(2):3336–3341, 2009.\nKrishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust Aggregation for Federated Learning. IEEE\nTransactions on Signal Processing , 70:1142–1154, 2022. doi: 10.1109/TSP.2022.3153135.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,andE.Duchesnay.\nScikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011.\nScikit-Learn-Contrib. Scikit-learn-contrib/scikit-learn-extra: Scikit-learn contrib estimators, 2021. URL\nhttps://github.com/scikit-learn-contrib/scikit-learn-extra .\n11\n\n5 Appendix\n5.1 Missing proofs for k-Means\nLemma 3. For any partition J1[J2of a setJ\u001aRof sizen, ifjJ1j\u0015(1\u0000\u0015)n, thenjJ\u0000J1j2\u0014\n\u0015\n(1\u0000\u0015)ncost(J;J).\nProof.We knowjJ1j= (1\u0000x)n;jJ2j=xnfor somex\u0014\u0015. It follows that\nJ= (1\u0000x)J1+xJ2\n)jJ\u0000J1j=xjJ2\u0000J1j\nandjJ\u0000J2j= (1\u0000x)jJ2\u0000J1j\n)jJ\u0000J2j=1\u0000x\nxjJ\u0000J1j: (2)\nWe now observe that we can write\ncost(J;J) = cost(J1;J) + cost(J2;J):\nand recall the identity\ncost(Jb;J) = cost(Jb;Jb) +jJbj\u0001jJ\u0000Jbj2\nforb2f0;1g. It then follows that\ncost(J;J)\u0015jJ1j\u0001jJ\u0000J1j2+jJ2j\u0001jJ\u0000J2j2\n= (1\u0000x)njJ\u0000J1j2+xnjJ\u0000J2j2\n= (1\u0000x)njJ\u0000J1j2+(1\u0000x)2n\nxjJ\u0000J1j2\n=(1\u0000x)n\nxjJ\u0000J1j2\n\u0015(1\u0000\u0015)n\n\u0015jJ\u0000J1j2\n)jJ\u0000J1j2\u0014\u0015\n(1\u0000\u0015)ncost(J;J):\nLemma 4. For alli2[k];j2[d], let!0\ni;jbe the collection of all subsets of (1\u0000\u000b)mipoints inPi;j. Then\ncost(Ii;j;Ii;j) = min\nZ02!0\ni;jcost(Z0;Z0):\nProof.SupposeI0\ni;j= argminZ02!0\ni;jcost(Z0;Z0). IfI0\ni;j2!i;jthen we are done since we know:\ncost(Ii;j;Ii;j) = min\nZ2!i;jcost(Z;Z)\nIfI0\ni;j=2!i;j, letaandbbe the minimum point and maximum points in I0\ni;j. We know there exists a point\np2Pi;j\\(a;b)such thatp =2I0\ni;j. IfjI0\ni;jj= 2, then we have a contradiction since\ncost(I0\ni;j;I0\ni;j) = (b\u0000a)2=2>(b\u0000p)2=2 = cost(fb;pg;fb;pg)\nIfjI0\ni;jj\u00153, we know either aorbis the furthest point from I0\ni;jnfa;bgin the interval [a;b]. Supposeais\nsuch a point, consider Ki;j= (I0\ni;jna)[p. We have the following identity,\ncost(Ki;jnp;p) = cost(Ki;jnfp;bg;p) +jp\u0000bj2\n12\n\n= cost(I0\ni;jnfa;bg;p) +jp\u0000bj2\n= cost(I0\ni;jnfa;bg;I0\ni;jna;b) +jI0\ni;jnfa;bgj\u0001jp\u0000I0\ni;jnfa;bgj2+jp\u0000bj2\n<cost(I0\ni;jnfa;bg;I0\ni;jna;b) +jI0\ni;jnfa;bgj\u0001ja\u0000I0\ni;jnfa;bgj2+ja\u0000bj2\n= cost(I0\ni;jnfa;bg;a) +ja\u0000bj2\n= cost(I0\ni;jnfag;a):\nFor the inequality, we used the fact that ais the furthest point from I0\ni;jnfa;bgin the interval [a;b], and\nq2(a;b). We have,\ncost(Ki;j;Ki;j) =1\n(1\u0000\u000b)miX\ny1;y22Ki;jjy1\u0000y2j2\n=1\n(1\u0000\u000b)mi0\n@X\ny1;y22Ki;jnpjy1\u0000y2j2+ cost(Ki;jnp;p)1\nA\n=1\n(1\u0000\u000b)mi0\n@X\ny1;y22I0\ni;jnajy1\u0000y2j2+ cost(I0\ni;jna;p)1\nA\n<1\n(1\u0000\u000b)mi0\n@X\ny1;y22I0\ni;jnajy1\u0000y2j2+ cost(I0\ni;jna;a)1\nA\n=1\n(1\u0000\u000b)miX\ny1;y22I0\ni;jjy1\u0000y2j2\n= cost(I0\ni;j;I0\ni;j)\nHence, cost(Ki;j;Ki;j)<cost(I0\ni;j;I0\ni;j)and we have a contradiction.\nLemma 5. The following bound holds:\njIi;j\u0000Qi;jj2\u00144\u000b\n1\u00002\u000bcost(Qi;j;Qi;j)\njQij:\nProof.Consider the set Si;j=f(Qi;j\u0000q)2:q2Qi;jg. LetVi;jbe a subset of size (1\u0000\u000b)mdrawn uniformly\nat random from Qi;j. Since the sample mean is an unbiased estimator for the population mean, we know\n1\n(1\u0000\u000b)miE2\n4X\nq2Vi;j(Qi;j\u0000q)23\n5=Si;j=cost(Qi;j;Qi;j)\njQij:\nWe also know that,\nE2\n4X\nq2Vi;j(Qi;j\u0000q)23\n5=E\u0002\ncost(Vi;j;Qi;j)\u0003\n\u0015E\u0002\ncost(Vi;j;Vi;j)\u0003\n\u0015cost(Ii;j;Ii;j);\nwhereweusedthefactthat Ii;jisasubsetofsize (1\u0000\u000b)jPijwithminimum 1-meansclusteringcost(lemma4).\nThus, we have\ncost(Ii;j;Ii;j)\u0014(1\u0000\u000b)mi\njQijcost(Qi;j;Qi;j):\n13\n\nNow, in the notation of lemma 3, we set J=Ii;jandJ1=Ii;j\\P\u0003\ni;j. Since we have that jIi;jj= (1\u0000\u000b)mi\nandjIi;j\\P\u0003\ni;jj=jIi;j\\Qi;jj= (1\u0000jPi;jnQi;jj\n1\u0000mi)(1\u0000\u000b)mi, we can set \u0015=jPi;jnQi;jj\n1\u0000mi, and get that\njIi;j\u0000Ii;j\\Qi;jj2\u0014jPi;jnQi;jjcost(Ii;j;Ii;j)\n((1\u0000\u000b)mi\u0000jPi;jnQi;jj)(1\u0000\u000b)mi\n\u0014jPi;jnQi;jjcost(Qi;j;Qi;j)\n((1\u0000\u000b)mi\u0000jPi;jnQi;jj)jQij\n\u0014\u000bcost(Qi;j;Qi;j)\n(1\u00002\u000b)jQij;\nwhere we use the fact that jPi;jnQi;jj\u0014\u000bmi. Also, by lemma 3,\njQi;j\u0000Ii;j\\Qi;jj2\u0014\u000bmicost(Qi;j;Qi;j)\n(jQi;jj\u0000\u000bmi)jQi;jj\u0014\u000bcost(Qi;j;Qi)\n(1\u00002\u000b)jQij\nWe conclude the proof by noting that\njIi;j\u0000Qi;jj2\u00142jIi;j\u0000Ii;j\\Qi;jj2+ 2jIi;j\\Qi;j\u0000Qi;jj2:\nLemma 6. The following bound holds:\njP\u0003\ni;j\u0000Ii;jj2\u0014cost(P\u0003\ni;j;P\u0003\ni;j)\u0012\u000b\n1\u0000\u000b+4\u000b\n(1\u00002\u000b)(1\u0000\u000b)\u0013\n=m\u0003\ni\nProof.By eq. (2),\njP\u0003\ni;j\u0000P\u0003\ni;jnQi;jj2=(1\u0000z)2\nz2jP\u0003\ni;j\u0000Qi;jj2;\nwherez=jP\u0003\ni;jnQi;jj\njP\u0003\ni;jj\u0014\u000b. We have\ncost(P\u0003\ni;j;P\u0003\ni;j)\n= cost(P\u0003\ni;jnQi;j;P\u0003\ni;j) + cost(Qi;j;P\u0003\ni;j)\n= cost(P\u0003\ni;jnQi;j;P\u0003\ni;jnQi;j) +zm\u0003\nijP\u0003\ni;j\u0000P\u0003\ni;jnQi;jj2+ cost(Qi;j;Qi;j)\n+ (1\u0000z)m\u0003\nijP\u0003\ni;j\u0000Qi;jj2\n=1\u0000z\nzm\u0003\nijP\u0003\ni;j\u0000Qi;jj2+ cost(P\u0003\ni;jnQi;j;P\u0003\ni;jnQi;j) + cost(Qi;j;Qi;j)\n\u00151\u0000\u000b\n\u000bm\u0003\nijP\u0003\ni;j\u0000Qi;jj2+ cost(Qi;j;Qi;j):\nApplying lemma 5, we have\ncost(P\u0003\ni;j;P\u0003\ni;j)\u00151\u0000\u000b\n\u000bm\u0003\nijP\u0003\ni;j\u0000Qi;jj2+1\u00002\u000b\n4\u000b\u0001(1\u0000\u000b)m\u0003\nijIi;j\u0000Qi;jj2:\nBy Cauchy-Schwarz,\n\u0000\njP\u0003\ni;j\u0000Qi;jj+jIi;j\u0000Qi;jj\u00012\n\u0014\u0012\u000b\n1\u0000\u000b+4\u000b\n(1\u00002\u000b)(1\u0000\u000b)\u0013\n\u00121\u0000\u000b\n\u000bm\u0003\nijP\u0003\ni;j\u0000Qi;jj2+1\u00002\u000b\n4\u000b\u0001(1\u0000\u000b)m\u0003\nijIi;j\u0000Qi;jj2\u0013\n=m\u0003\ni\n\u0014cost(P\u0003\ni;j;P\u0003\ni;j)\u0012\u000b\n1\u0000\u000b+4\u000b\n(1\u00002\u000b)(1\u0000\u000b)\u0013\n=m\u0003\ni\nWe conclude the proof by the fact that jP\u0003\ni;j\u0000Ii;jj2\u0014\u0000\njP\u0003\ni;j\u0000Qi;jj+jIi;j\u0000Qi;jj\u00012.\n14\n\nTheorem 7. Algorithm 1 is a deterministic algorithm for k-means clustering such that given a data set\nP2Rm\u0002dand a partition (P1;:::;Pk)with error rate \u000b < 1=2, it outputs a\u0010\n1 +\u0010\n\u000b\n1\u0000\u000b+4\u000b\n(1\u00002\u000b)(1\u0000\u000b)\u0011\u0011\n-\napproximation in time O(dmlogm):\nProof.Recall that the k-means clustering cost can be written as the sums of the clustering cost in each\ndimension. For every i2[k], we have\nX\ni2[k]cost(P\u0003\ni;fbcjgk\nj=1)\u0014X\ni2[k]cost(P\u0003\ni;bci)\n=X\ni2[k]X\nj2[d]cost(P\u0003\ni;j;bci;j)\n=X\ni2[k]X\nj2[d]cost(P\u0003\ni;j;P\u0003\ni;j) +m\u0003\nijbci;j\u0000P\u0003\ni;jj\n=X\ni2[k]X\nj2[d]cost(P\u0003\ni;j;P\u0003\ni;j) +m\u0003\nijIi;j\u0000P\u0003\ni;jj\n\u0014X\ni2[k]X\nj2[d]\u0012\n1 +\u0012\u000b\n1\u0000\u000b+4\u000b\n(1\u00002\u000b)(1\u0000\u000b)\u0013\u0013\ncost(P\u0003\ni;j;P\u0003\ni;j)\n=\u0012\n1 +\u0012\u000b\n1\u0000\u000b+4\u000b\n(1\u00002\u000b)(1\u0000\u000b)\u0013\u0013X\ni2[k]cost(P\u0003\ni;c\u0003\ni):\nThe inequality is due to lemma 6.\nWe analyze the runtime of algorithm 1. Notice for every i2[k];j2[d], computing Ii;jinvolves sorting\nthe pointsPi;j, iterating from the smallest to the largest point, and taking the average of the interval in !i;j\nwith the smallest cost. This takes O(milogmi)time. Note thatP\ni2[K]mi=m. Thus, the total time over\nalli2[k]andj2[d]isO(dmlogm):\nCorollary 8. For\u000b\u00141=7, algorithm 1 achieves a clustering cost of (1 + 7:7\u000b)OPT.\nProof.We recall that the generic guarantee for \u000b<1=2is\ncost(P;fbc1;:::;bckg)\u0014\u0012\n1 +\u0012\u000b\n1\u0000\u000b+4\u000b\n(1\u00002\u000b)(1\u0000\u000b)\u0013\u0013\nOPT:\nWe see that for \u000b < 1=7,\u000b\n1\u0000\u000b\u00147\u000b\n6, and4\u000b\n(1\u00002\u000b)(1\u0000\u000b)\u001449\u00014\u000b\n30, so in sum the net approximation factor is\n1 + 7:7\u000b.\n5.2 Missing proofs for k-Medians\nLemma 12. With probability1\u00002\u000b\n2,kx\u0000c\u0003\nik\u00142OPTi=mi.\nProof.We observe that cost(Pi\\P\u0003\ni;c\u0003\ni)\u0014OPTi:It follows that Ex\u0018Pi\\P\u0003\ni[kx\u0000c\u0003\nik]\u0014OPTi\njPi\\P\u0003\nij\u0014OPTi\n(1\u0000\u000b)mi:\nBy Markov’s inequality,\nPr\u0012\nkx\u0000c\u0003\nik>(1 +\u000f)\u0001OPTi\n(1\u0000\u000b)mi\f\fx2Pi\\P\u0003\ni\u0013\n\u00141\n1 +\u000f\n)Pr\u0010\nkx\u0000c\u0003\nik\u0014(1+\u000f)OPTi\n(1\u0000\u000b)mi^x2Pi\\P\u0003\ni\u0011\nP(x2Pi\\P\u0003\ni)\u0015\u000f\n1 +\u000f\nPr\u0012\nkx\u0000c\u0003\nik\u0014(1 +\u000f)OPTi\n(1\u0000\u000b)mi\u0013\n\u0015\u000f\n1 +\u000fP(x2Pi\\P\u0003\ni)\n\u0015\u000f(1\u0000\u000b)\n1 +\u000f\nTo get the stated bound we set \u000f= 1\u00002\u000b.\n15\n\nLemma 14. Conditioned onE,cost(PinP0\ni;bcj\ni)\u0014(1 + 5\u000b)OPTi.\nWe ﬁrst deﬁne some notation for the sets of false positive and false negative points that occur in our\nproof for lemma 14, and prove a technical lemma relating the sets Pi\\P\u0003\niandPinP0\ni.\nDeﬁnition 20. We make the following deﬁnitions:\n1. LetE1denote the event that kx\u0000c\u0003\nik\u00142OPTi=n.\n2. LetAdenote the set of false negatives, i.e. P\u0003\ni\\P0\ni.\n3. LetBdenote the set of false positives, i.e. Pin(P0\ni[P\u0003\ni).\nTo bound the clustering cost of PinP0\ni, in terms of the cost of Pi\\P0\ni, we ﬁrst relate these two sets in\nterms of the false positives Band the false negatives A.\nLemma 21. We can write Pi\\P\u0003\ni= ((PinP0\ni)nB)[A(see deﬁnition 20 for the deﬁnitions of AandB).\nProof.To see this we observe that\nPinP0\ni= ((PinP0\ni)\\P\u0003\ni)[((PinP0\ni)nP\u0003\ni)\n= ((PinP0\ni)\\P\u0003\ni)[B\n)(PinP0\ni)\\P\u0003\ni= (PinP0\ni)nB:\nWe also have that\nPi\\P\u0003\ni= ((Pi\\P\u0003\ni)\\P0\ni)[((Pi\\P\u0003\ni)nP0\ni)\n)((Pi\\P\u0003\ni)nP0\ni) = (Pi\\P\u0003\ni)n((Pi\\P\u0003\ni)\\P0\ni)\n= (Pi\\P\u0003\ni)nA:\nSince (Pi\\P\u0003\ni)nP0\ni= (PinP0\ni)\\P\u0003\ni, we can identify the left hand sides in the last two displays and write\n(Pi\\P\u0003\ni)nA= (PinP0\ni)nB\n)Pi\\P\u0003\ni= ((PinP0\ni)nB)[A:\nwherein we use that A= (Pi\\P\u0003\ni)\\P0\ni.\nWe can now formalize our main argument showing that the clipped data set PinP0\nihas a clustering cost\nclose to that of the true cluster P\u0003\ni.\nProof of lemma 14. By lemma 21, we ﬁrst observe that\ncost(PinP0\ni;c\u0003\ni) = cost((Pi\\P\u0003\ni);c\u0003\ni)\u0000cost(A;c\u0003\ni) + cost(B;c\u0003\ni);\nwhereAandBare deﬁned as in deﬁnition 20. Again by lemma 21, PinP0\ni= (Pi\\P\u0003\ni)nA[B,A\u001aPi\\P\u0003\ni\nandB\\(Pi\\P\u0003\ni) =;, it follows that\njPinP0\nij=jPi\\P\u0003\nij\u0000jAj+jBj:\nFurther, we know that jPinP0\nij\u0014(1\u0000\u000b)jPijandjPi\\P\u0003\nij\u0015(1\u0000\u000b)jPij. It follows thatjBj\u0014jAj\u0014\u000bn.\nTherefore, for every false positive p2B, we can assign a unique corresponding false negative np2A\narbitrarily. We observe that every point in Ais farther from xthan every point in B, and so we can write\nknp\u0000c\u0003\nik\u0015knp\u0000xk\u0000kx\u0000c\u0003\nik\n\u0015kp\u0000xk\u0000kx\u0000c\u0003\nik\n\u0015kp\u0000c\u0003\nik\u00002kx\u0000c\u0003\nik\n\u0015kp\u0000c\u0003\nik\u00004OPTi\nn\n16\n\n)kp\u0000c\u0003\nik\u0014knp\u0000c\u0003\nik+4OPTi\nn:\nIt follows that\ncost(B;c\u0003\ni) =X\np2Bkp\u0000c\u0003\nik\n\u0014X\np2Bknp\u0000c\u0003\nik+4OPTi\nn\n\u0014cost(A;c\u0003\ni) + 4\u000bOPTi:\nReturning to our expression for cost(PinP0\ni;m\u0003\ni), we get that\ncost(PinP0\ni;c\u0003\ni) = cost((Pi\\P\u0003\ni)nA;c\u0003\ni) + cost(B;c\u0003\ni)\n= cost((Pi\\P\u0003\ni);c\u0003\ni)\u0000cost(A;c\u0003\ni) + cost(B;c\u0003\ni)\n\u0014cost((Pi\\P\u0003\ni);c\u0003\ni) + 4\u000bOPTi\n\u0014(1 + 4\u000b)OPTi:\nIt follows that the optimal clustering cost for the set PinP0\niis at most (1 + 4\u000b)OPTi, and hence that\ncost(PinP0\ni;bcj\ni)\u0014(1 +\r)(1 + 4\u000b)OPTi\u0014(1 + 5\u000b)OPTi, for suitably small \r\u0014\u000b\n1+4\u000b.\nLemma 15. ForR=O\u0010\n1\n(1\u00002\u000b)log\u00002k\n\u000e\u0001\u0011\nmany repetitions, with probability at least 1\u0000\u000e\n2k, we have that\ncost(PinP0\ni;bci)\u0014(1 + 5\u000b)OPTi.\nProof.The probabilityE1not holding for some bcj\niis at most (1\u00002\u000b)=2. The probability of E1not holding\nfor any of the bcj\niis(1\u0000(1\u00002\u000b)=2)R. It follows that for R=2\n1\u00002\u000bln\u00002k\n\u000e\u0001\n, the probability of E1not holding\nfor any of the mj\niis at most\n(1\u0000(1\u00002\u000b)=2)R\u0014exp(\u0000(1\u00002\u000b)=2)R\n\u0014exp (\u0000ln (2k=\u000e))\n\u0014\u000e\n2k:\nIt follows that with probability 1\u0000\u000e\n2k,E1holds for some bcj\niand consequently by the union bound\ncost(PinP0\ni;bci)\u0014(1 + 5\u000b)OPTiholds with probability 1\u0000\u000e\nk.\nLemma 16. Ifcost(PinP0\ni;bci)\u0014(1 + 5\u000b)OPT, thenkbci\u0000c\u0003\nik\u00142+5\u000b\n(1\u00002\u000b)OPTi\nn.\nProof.By the reverse triangle inequality we have that for every point p2P\u0003\ni\\(PinP0\ni),kbci\u0000pk \u0015\nkbci\u0000c\u0003\nik\u0000kp\u0000c\u0003\nik. Summing up across p, we get\nX\np2P\u0003\ni\\(PinP0\ni)kbci\u0000pk\u0015jP\u0003\ni\\(PinP0\ni)j\u0001kbci\u0000c\u0003\nik\u0000X\np2P\u0003\ni\\(PinP0\ni)kp\u0000c\u0003\nik\n(1 + 5\u000b)OPTi\u0015jP\u0003\ni\\(PinP0\ni)j\u0001kbci\u0000c\u0003\nik\u0000OPTi\n)jP\u0003\ni\\(PinP0\ni)j\u0001kbci\u0000c\u0003\nik\u0014((1 + 5\u000b) + 1)OPT i\n)kbci\u0000c\u0003\nik\u0014(2 + 5\u000b)OPTi\n(1\u00002\u000b)mi:\nLemma 17. With probability 1\u0000\u000e=k,cost(Pi\\P\u0003\ni;bci)\u0014cost(Pi\\P\u0003\ni;c\u0003\ni) +(5\u000b+10\u000b2)OPTi\n1\u00002\u000b.\n17\n\nProof.From corollary 10, we know that with probability 1\u0000\u000e\n2k, the following bound holds:\ncost(PinP0\ni;bci)\u0014(1 +\r)cost(PinP0\ni;c0\ni);\nwhere\r\u0014\u000b\n(1+4\u000b)andc0\niisanoptimal 1-medianfor PinP0\ni. Also, itfollowsbydeﬁnitionthat cost(PinP0\ni;c0\ni)\u0014\ncost(PinP0\ni;c\u0003\ni). Further, from lemma 15 and lemma 16 it follows that with probability 1\u0000\u000e\n2k,\nkbci\u0000c\u0003\nik\u0014(2 + 5\u000b)OPTi\n(1\u00002\u000b)mi:\nBy the union bound, both these events hold simultaneously with probability 1\u0000\u000e\nk. Conditioning on this\nbeing the case, since Pi\\P\u0003\ni= ((PinP0\ni)nB)[A, we can write\ncost(Pi\\P\u0003\ni;bci)\u0000cost(Pi\\P\u0003\ni;c\u0003\ni) = (cost(PinP0\ni;bci)\u0000cost(PinP0\ni;c\u0003\ni))\n+ (cost(B;c\u0003\ni)\u0000cost(B;bci))\n+ (cost(A;bci)\u0000cost(A;c\u0003\ni))\n\u0014(1 +\r)cost(PinP0\ni;c0\ni)\u0000cost(PinP0\ni;c0\ni)\n+jBj\u0001kbci\u0000c\u0003\nik+jAj\u0001kbci\u0000c\u0003\nik\n\u0014\r\u0001cost(PinP0\ni;c\u0003\ni) +jBj\u0001kbci\u0000c\u0003\nik+jAj\u0001kbci\u0000c\u0003\nik\n\u0014\u000bOPTi+ (\u000bmi+\u000bmi)\u0001(2 + 5\u000b)OPTi\n(1\u00002\u000b)mi\n\u0014\u000b+ 2\u000b(2 + 5\u000b)OPTi\n(1\u00002\u000b)\n=\u0000\n5\u000b+ 10\u000b2\u0001\nOPTi\n1\u00002\u000b:\nLemma 18. With probability 1\u0000\u000e=k,cost(P\u0003\ni;^ci)\u0014(1 +c\u000b)OPTiforc=7+10\u000b\u000010\u000b2\n(1\u0000\u000b)(1\u00002\u000b).\nProof.We have that\ncost(P\u0003\ni;bci) = cost(P\u0003\ni\\Pi;bci) + cost(P\u0003\ninPi;bci):\nWe bound the second summand as follows\ncost(P\u0003\ninPi;bci) = cost(P\u0003\ninPi;c\u0003\ni) +jP\u0003\ninPij\u0001(2 + 5\u000b)OPTi\n(1\u00002\u000b)ci\n\u0014cost(P\u0003\ninPi;c\u0003\ni) +\u000b(2 + 5\u000b)OPTi\n(1\u0000\u000b) (1\u00002\u000b):\nBounding the ﬁrst summand cost(P\u0003\ni\\Pi;bci)using the bound from above, we get\ncost(P\u0003\ni;bci) = cost(P\u0003\ni\\Pi;c\u0003\ni) +\u0000\n5\u000b+ 10\u000b2\u0001\nOPTi\n1\u00002\u000b\n+ cost(P\u0003\ninPi;c\u0003\ni) +\u000b(2 + 5\u000b)OPTi\n(1\u0000\u000b) (1\u00002\u000b)\n= cost(P\u0003\ni;c\u0003\ni) +\u0000\n5\u000b+ 10\u000b2\u00005\u000b2\u000010\u000b3+ 2\u000b+ 5\u000b2\u0001\nOPTi\n(1\u0000\u000b)(1\u00002\u000b)\n= cost(P\u0003\ni;c\u0003\ni) +\u0000\n7\u000b+ 10\u000b2\u000010\u000b3\u0001\nOPTi\n(1\u0000\u000b)(1\u00002\u000b):\n18\n\n0.2 0.4 0.6 0.8 1.0\nPortion of the dataset0.250.500.751.001.251.501.752.00Runtime in seconds\nErgun, Jon, et al.\nOurs(a) PHY\n0.2 0.4 0.6 0.8 1.0\nPortion of the dataset246810121416Runtime in seconds\nErgun, Jon, et al.\nOurs (b) CIFAR-10\nFigure 3: Runtime comparison of algorithm 1 with Ergun et al. [2022]\n0.2 0.4 0.6 0.8 1.0\nPortion of the dataset50100150200250Runtime in seconds\nErgun, Jon, et al.\nOurs\n(a) PHY\n0.2 0.4 0.6 0.8 1.0\nPortion of the dataset5101520253035Runtime in seconds\nErgun, Jon, et al.\nOurs (b) CIFAR-10\nFigure 4: Runtime comparison of algorithm 2 with Ergun et al. [2022]\n6 Experiments on runtime\nIn this section, we report the runtimes of our k-means and k-medians approaches and the methods in Ergun\net al. [2022]. We sample subsets of points from the CIFAR-10 and thePHYdatasets, and report the\nruntime (means and standard deviations) of the algorithms over 20 random runs. The subset sizes are varied\nfrom 20%to100%of the size of the datasets, kis ﬁxed at 10and\u000bis ﬁxed at:2\nFor k-means, we observe in ﬁg. 3 that the runtime of the two approaches are comparable, except for\nsubset sizes 80%and100%of CIFAR-10 where ours is slightly slower. This is expected since ﬁnding a\nsubset of size (1\u0000\u000b)miwith the best clustering cost in our algorithm and computing the shortest interval\ncontaining mi(1\u00005\u000b)=2points in the approach of Ergun et al. [2022] both involve sorting the points and\ntakesO(milogmi)time.\nWe observe similar trends in the k-medians setting in ﬁg. 4. This is also expected given that the runtimes\nof both algorithms are dominated by calls to compute the 1-median center of the ﬁltered points in each\npredicted cluster.\n19",
  "textLength": 51660
}