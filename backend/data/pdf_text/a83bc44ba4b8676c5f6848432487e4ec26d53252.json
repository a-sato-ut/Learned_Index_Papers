{
  "paperId": "a83bc44ba4b8676c5f6848432487e4ec26d53252",
  "title": "Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors",
  "pdfPath": "a83bc44ba4b8676c5f6848432487e4ec26d53252.pdf",
  "text": "arXiv:2506.05479v1  [cs.LG]  5 Jun 2025Learning-Augmented Algorithms for MTS with Bandit Access\nto Multiple Predictors\nMatei Gabriel Co¸ sa\nBocconi UniversityMarek Eli´ aˇ s\nBocconi University\nJune 9, 2025\nAbstract\nWe consider the following problem: We are given ℓheuristics for Metrical Task Systems\n(MTS), where each might be tailored to a different type of input instances. While processing an\ninput instance received online, we are allowed to query the action of only one of the heuristics\nat each time step. Our goal is to achieve performance comparable to the best of the given\nheuristics. The main difficulty of our setting comes from the fact that the cost paid by a\nheuristic at time tcannot be estimated unless the same heuristic was also queried at time\nt−1. This is related to Bandit Learning against memory bounded adversaries (Arora et al.,\n2012). We show how to achieve regret of O(OPT2/3) and prove a tight lower bound based on\nthe construction of Dekel et al. (2013).\n1 Introduction\nMetrical Task Systems (MTS) (Borodin et al., 1992) are a very broad class of online problems\ncapable of modeling problems arising in computing, production systems, power management, and\nrouting service vehicles. In fact, many fundamental problems in the field of Online Algorithms\nincluding Caching, k-server, Ski-rental, and Convex Body Chasing are special cases of MTS.\nMetrical Task Systems are also related to Online Learning from Expert Advice , see (Blum & Burch,\n2000) for the comparison of the two problems.\nIn MTS, we are given a description of a metric space ( M, d) and a starting point s0∈M\nbeforehand. The points in Mare traditionally called states and, depending on the setting, they\ncan represent actions, investment strategies, or configurations of some complex system. At each\ntime step t= 1, . . . , T , we receive a cost function ct:M→R+∪{0,+∞}. Seeing ct, we can decide\nto stay in our previous state st−1and pay its cost ct(st−1), or move to some (possibly cheaper)\nstate stand pay ct(st) +d(st−1, st), where the distance function drepresents the transition cost\nbetween two states. Our objective is to minimize the total cost paid over time.\nMTS is very hard in the worst case. This is due to its online nature ( sthas to be chosen\nwithout knowledge of ct+1, . . . , c T) and also due to its generality. The performance of algorithms is\nevaluated using the competitive ratio which is, roughly speaking, the ratio between the algorithm’s\ncost and the cost of the optimal solution computed offline. Denoting nthe number of points in the\nmetric space M, the best competitive ratio achievable for MTS is 2 n−1 and Θ( log2n) in the case\nof deterministic and randomized algorithms, respectively Borodin et al. (1992); Bartal et al. (2006);\nBubeck et al. (2022a, 2019). Note that nis usually very large or even infinite (e.g., M=Rd).\nThis worst-case hardness motivates the study of MTS and its special cases in the context of\nLearning-Augmented Algorithms (Lykouris & Vassilvitskii, 2021). Here, the algorithm can use\npredictions produced by an ML model in order to exploit specific properties of input instances.\nBy augmenting the algorithm with the ML model, we obtain a heuristic with an outstanding\nperformance, well beyond the classical worst-case lower bounds, on all inputs where the ML model\nperforms well. One of the key techniques used in this context is Combining Heuristics which is\nused to achieve robustness (Lykouris & Vassilvitskii, 2021; Wei, 2020; Antoniadis et al., 2023b),\n1\n\nadjust hyperparameters Antoniadis et al. (2021), and recognize the most suitable heuristic for the\ncurrent input instance (Emek et al., 2021; Anand et al., 2022; Antoniadis et al., 2023a).\nCombining heuristics. In this basic theoretical problem, we are given ℓheuristics H1, . . . , H ℓ\nwhich can be simulated on the input instance received online. We want to combine them into a\nsingle algorithm which achieves a cost comparable to the best of the heuristics on each individual\ninstance. The difficulty of the combination problem is given by the online setting: because we\nreceive the input online, we cannot tell beforehand which heuristic is going to perform better.\nMoreover, steps performed while following a wrong heuristic cannot be revoked.\nWe can combine a heuristic deploying an ML prediction model with a classical online algorithm\nin order to obtain a new algorithm that satisfies worst-case guarantees regardless of the performance\nof the ML model (this property is called robustness ). Similarly, we can use it to deploy a portfolio\nof very specialized ML models in a broader setting, where each model corresponds to a separate\nheuristic. The combination technique ensures that we achieve very good performance whenever at\nleast one of the models performs well on the current input instance.\nFull-feedback setting. Most of the previous works on Combining Heuristics query the state of\neach heuristic at each time step, observe their costs, and choose between their states, see (Wei,\n2020; Antoniadis et al., 2023b; Blum & Burch, 2000; Fiat et al., 1991). We call this the full-feedback\nsetting and, indeed, methods from Online Learning in the full-feedback setting can be directly\nintegrated in the combination algorithms. For example, Blum & Burch (2000) showed that by\nchoosing between heuristics using the HEDGE algorithm (Freund & Schapire (1997)) with a\nwell-chosen learning rate we can be (1 + ϵ)-competitive with respect to the best heuristic.1\nBandit-feedback setting. In this paper, we consider what can be seen as a bandit-feedback\nsetting of Combining Heuristics. At each time step, we are allowed to query the current state of\nonly one heuristic. This setting allows us to study the impact of restricting information about the\nheuristics on our ability to combine them effectively. Further motivation is the fact that querying\nall the heuristics is costly, especially if they utilize some heavy-weight prediction models. This\nissue was already considered by Emek et al. (2021) and Antoniadis et al. (2023a) whose works\ninspired our study. Note that the combination algorithm retains full access to the input instance.\nThis is required by any positive result for general MTS, since the cost functions ctusually do not\nsatisfy any natural properties such as Lipschitzness or convexity; see further discussion in Section 2.\nMoreover, ctis often easy to encode. For example, in Caching, ctis fully determined by the page\nrequested at time t.\nIn contrast to the full-feedback setting, classical learning methods cannot be directly integrated\nin combination algorithms operating in the bandit setting. The main reason is that the state si\nt\nof the heuristic Hiis not enough to estimate its cost at time t: the cost paid by Hiat time tis\nct(si\nt) +d(si\nt−1, si\nt), i.e., we cannot calculate this cost2unless we have queried Hiin both time\nsteps t−1 and t. We also consider the case where each heuristic needs to be bootstrapped for\nm−2≥0 time steps before we can see its state. This way, to receive the state of Hiin time steps\nt−1 and t, and to be able to calculate its cost at time t, we have to query Hiin the mtime steps\nt−m+ 1, . . . , t .\nA similar phenomenon occurs in the setting of Online Learning against Memory-Bounded\nAdversaries (Arora et al., 2012), which is our main theoretical motivation for considering non-zero\nbootsrapping time (i.e., m > 2). There, one needs to play an action aat least mtimes in a row\nto see the loss of the reference policy which plays aat each time step. The algorithm of Arora\net al. (2012) splits the time horizon into blocks of length larger than m. In the block i, they keep\n1In fact, one can achieve cost H∗+O(D√\nH∗), where H∗denotes the cost of the best heuristic and Dthe\ndiameter of the metric space M.\n2In fact, we cannot even estimate it. In Caching, k-server, and Convex Body Chasing, to give a few examples,\nthe cost of each state is either 0 or + ∞. Therefore, any algorithm with a finite cost is always located at some state\nstsuch that ct(st) = 0 and the difference in the performance of the algorithms can be seen only in the moving costs.\n2\n\nplaying the same action aiwhich allows them to observe the loss of the corresponding reference\npolicy after the first mtime steps in the block. However, this does not work in our setting. The\nadversary can set up the MTS instance in such a way that ct̸= 0 and the heuristics move only on\nthe boundaries of the blocks, so that the algorithm would not observe the cost of any heuristic.\nThis way, the algorithm cannot be competitive with respect to the best heuristic.\nA different bandit-like setting for Combining Heuristics for MTS was proposed by Antoniadis\net al. (2023a). In their setting, the queried predictor reports both its state and the declared moving\ncost incurred at time t. We discuss their contribution in Section 1.2. Their (1 + ϵ)-competitive\nalgorithm and absence of lower bounds motivated us to study regret bounds which can provide\nmore refined guarantees when the competitive ratio is close to 1. However, we have decided to\npursue these bounds in the more natural setting where the moving costs are not reported as it\ndoes not rely on the honesty of the predictors.3Since our algorithms use a subset of information\navailable in the setting of Antoniadis et al. (2023a), our upper bounds also apply to their setting.\n1.1 Our Results\nForm≥2, we say that an algorithm ALG hasm-delayed bandit access to heuristics H1, . . . , H ℓif\n(i) it can query at most one heuristic Hiat each time step tand (ii) the query yields the current\nstate of Hionly if Hiwas queried also in steps t−m+2, . . . , t , otherwise it yields an empty result.\nOur main result is an algorithm with the following regret.\nTheorem 1.1. LetD,ℓ, and m≥2be constant. Consider heuristics H1, . . . , H ℓfor an MTS with\ndiameter D. There is an algorithm ALG which, given an m-delayed bandit access to H1, . . . , H ℓ,\nsatisfies the following guarantee. The expected cost of ALG on any input Iis\nE[ALG] ≤OPT ≤0+O\u0000\nOPT2/3\n≤0\u0001\n,\nwhere OPT ≤0= minℓ\ni=1H(I)denotes the cost of the best heuristic on input I.\nWe prove this result in Section 3, where we state explicitly the dependence on D,m, and ℓ\nin the case when they are larger than a constant. We can interpret our result in terms of the\ncompetitive ratio. Since the regret term in Theorem 1.1 is sublinear in OPT ≤0, the expected cost\nof our algorithm is at most E[ALG]≤(1 + o(1))OPT ≤0. In other words, its competitive ratio\nwith respect to the best of the ℓheuristics converges to 1. Therefore, our algorithm can be used\nfor robustification. If we include a classical online algorithm which is ρ-competitive in the worst\ncase among H1, . . . , H ℓ, our algorithm will never be worse than (1 + o(1))ρ-competitive. However,\non input instances where some of the heuristics incur a very low cost, the algorithm can match\ntheir performance asymptotically. For example, if the cost of the best heuristic is only 1.01 times\nhigher than the offline optimum, our algorithm’s cost will be only (1 + o(1))·1.01 times the offline\noptimum.\nOur algorithm alternates between exploitation and exploration, as in the classical approach for\ntheMulti-Armed Bandit (MAB) setting, see e.g. (Slivkins, 2019). However, each exploration phase\ntakes mtime steps and we are unable to build an unbiased estimator of the loss vectors. Moreover,\nour algorithm sometimes needs to take improper steps, i.e., going to a state which is not suggested\nby any of the predictors. This is clearly necessary with m > 2, since we may receive empty answers\nto our queries. Such improper steps are crucial for achieving regret in terms of OPT ≤0even for\nm= 2.\nIn Section 6, we show that our algorithm can be adapted to the m-memory bounded setting for\nbandits, where it achieves a regret of O(T2/3) comparable to Arora et al. (2012), while slightly\nimproving the dependence on m. Note that Tcan be much larger than OPT ≤0.\nWe extend our result to a setting with a benchmark which can switch between the heuristics at\nmost ktimes, while the algorithm’s number of switches still remains unrestricted.\n3Alternatively, we could require predictors to certify their moving cost by providing their state in the previous\ntime step. This way, however, we end up querying the state of two predictors in a single time step.\n3\n\nTheorem 1.2. LetD,ℓ, and m≥2be constant and k≥1be a parameter. Consider heuristics\nH1, . . . , H ℓfor an MTS with diameter D. There is an algorithm ALG which, given an m-delayed\nbandit access to H1, . . . , H ℓ, satisfies the following guarantee. The cost of ALG on input Iwith\noffline optimum cost at least 2kis\nE[ALG] ≤OPT ≤k+˜O(k1/3OPT2/3\n≤k),\nwhere OPT ≤kdenotes the cost of the best combination of heuristics in hindsight that switches\nbetween heuristics at most ktimes on input I.\nAgain, we can interpret this regret bound as a competitive ratio converging to 1. In Section 5,\nwe show that the competitive ratio of our algorithm is below (1 + ϵ) for kas large as ˜Ω(ϵ3OPT ≤k)\nwhich is worse by a log-factor than the result of Antoniadis et al. (2023a) in their easier setting.\nIn Section 4, we show that our upper bound in Theorem 1.1 is tight up to a logarithmic factor\neven with 0 bootstrapping time ( m= 2). Our result is based on the construction of Dekel et al.\n(2013) for Bandits with Switching Costs . Their lower bound cannot be applied directly, since\nalgorithms in our setting have an advantage in being able to take improper actions and having\none-step look-ahead. Both of these advantages come from the nature of MTS and are indispensable\nin our setting, see Section 2.\nTheorem 1.3. For any algorithm ALG with 2-delayed bandit access to ℓpredictors, there is an\ninput instance Isuch that the expected cost of ALG is\nE[ALG] ≥OPT ≤0+˜Ω(OPT2/3\n≤0),\nwhere OPT ≤0= minℓ\ni=1H(I)denotes the cost of the best heuristic on input I.\nIn Section 4.1, we also show that the dependence on D, ℓandkin our bounds is (almost)\noptimal. In particular, our regret bound in Theorem 1.1 scales with ( Dkℓlnℓ)1/3m2/3and we show\nthat this dependence needs to be at least ( Dkℓ)1/3with m= 2.\n1.2 Related Work\nArora et al. (2012) introduced the problem of Bandit Learning against Memory-Bounded Adversaries.\nHere, the loss functions depend on the last µ+ 1 actions taken by the algorithm. This setting\ncaptures, for example, Bandits with Switching Costs (Cesa-Bianchi et al., 2013; Amir et al., 2022;\nRouyer et al., 2021). They propose an elegant algorithm with regret O(µT2/3) that partitions the\ntime horizon into blocks of equal size and let a classical online learning algorithm (e.g., EXP3)\nplay over the losses aggregated in each block. Their result was shown to be tight by Dekel et al.\n(2013) who provided a sophisticated lower bound construction showing that any algorithm suffers\nregret at least ˜Ω(T2/3) already for µ= 1. Our results are also related to Non-Stationary Bandits\nandDynamic Regret (Auer et al., 2002, Section 8).\nAntoniadis et al. (2023a) studied dynamic combination of heuristics for MTS. By reducing to\ntheLayered Graph Traversal problem (Bubeck et al., 2022b), they achieved a competitive ratio\nO(ℓ2) with respect to the best dynamic combination of ℓheuristics. Then, they focused on the\nscenario where the input instance is partitioned into kintervals and a different heuristic excels\nin each of the intervals. They provided bounds on how big kcan be to make (1 + ϵ)-competitive\nalgorithms possible. Finally, they also studied this question in the bandit-like setting which is\nstrictly easier compared to ours.\nFirst works on Combining Heuristics were by Fiat et al. (1990) for k-server, Fiat et al. (1991)\nfor Caching, and Azar et al. (1993); Blum & Burch (2000) for MTS. More recently, Emek et al.\n(2021) studied Caching with multiple predictors and achieved regret sublinear in T, while also\ntackling a bandit-like setting. Further results on other online problems are by Anand et al. (2022);\nDinitz et al. (2022); Bhaskara et al. (2020); Gollapudi & Panigrahi (2019); Almanza et al. (2021);\nWang et al. (2020); Kevi & Nguyen (2023).\n4\n\nLearning-augmented algorithms were introduced by Lykouris & Vassilvitskii (2021); Kraska\net al. (2018) who designed algorithms effectively utilizing unreliable machine-learned predictions.\nSince these two seminal works, many computational problems were studied in this setting, including\nCaching (Rohatgi, 2020; Wei, 2020), Scheduling (Lindermayr & Megow, 2022; Benomar & Perchet,\n2024b; Balkanski et al., 2023; Bamas et al., 2020), graph problems (Eberle et al., 2022; Bernardini\net al., 2022; Dong et al., 2025; Davies et al., 2023) and others. Several works consider algorithms\nusing the predictions sparingly (Im et al., 2022; Drygala et al., 2023; Sadek & Eli´ aˇ s, 2024; Benomar\n& Perchet, 2024a). See the survey (Mitzenmacher & Vassilvitskii, 2022) and the website by\n(Lindermayr & Megow, 2023).\nMetrical Task Systems were introduced by Borodin et al. (1992) who gave a tight competitive\nratio of 2 n−1 for deterministic algorithms ( nis the number of states). The best competitive ratio\nfor general MTS is Θ(log2n) by Bubeck et al. (2019) and Bubeck et al. (2022a).\n2 Notation and Preliminaries\nWe consider MTS instances with a bounded diameter and denote D=max s,s′∈Md(s, s′) the\ndiameter of the underlying metric space. For example, Din caching is equal to the size of the cache.\nAt each time step, the algorithm receives the cost function ctfirst, and then it chooses its new\nstate st, i.e. there is a 1-step look-ahead. This is standard in MTS definition and it is necessary\nfor existence of any competitive algorithm, since ctis potentially unbounded, see (Blum & Burch,\n2000, Section 2.3). We denote ∆dthed-dimensional probability simplex, and [ d] ={1, . . . , d }.\nm-delayed bandit access to heuristics. Given ℓheuristics H1, . . . , H ℓ, we denote si\ntthe state\nofHiat time tandft(i) =ct(si\nt) +d(si\nt−1, si\nt) the cost incurred by Hiat time t. Let m≥2 be a\nparameter. At each time t, the algorithm is allowed to query a single heuristic Hi. IfHiwas also\nqueried in time steps t−m+ 2, . . . , t , the result of the query is the state si\nt. Otherwise, the result\nis empty. While the access to the states of the heuristics is restricted, the algorithm has full access\nto the input instance which is not related to acquiring costly predictive information. Moreover,\nthe input instance can be often described in a very compact way. For example, ctin Caching is\ncompletely determined by the page requested at time t. Access to the input instance is necessary\nbecause the cost functions are not required to satisfy any natural assumptions (like boundedness,\nLipschitzness, convexity). For example, if the queried heuristic reports a state swith ct(s) = +∞,\nthe algorithm needs to know ctin order to choose a different state and avoid paying the infinite\ncost. Note that a similar situation can easily happen in Caching, k-server, Convex Body Chasing,\nor Convex Function Chasing.\nWe assume that ft(i)∈[0,2D]. This is without loss of generality for the following reason. First,\nwe can assume that at each time tthere is a state with zero cost, since subtracting minsct(s) from\nthe cost of each state affects the cost of any algorithm (including the offline optimum) equally.\nSecond, any predictor can be post-processed so that, in each time step where its cost is higher\nthan 2 D, it serves the request in the state with 0 cost and returns to the predicted state, paying at\nmost 2 Dfor the movement.\nBenchmarks and performance metrics. LetOPT ≤0=minℓ\ni=1PT\nt=1ft(i) be the static\noptimum, i.e., the cost of the best heuristic. For k >0, we define\nOPT ≤k= min i1,...,iTPT\nt=1\u0000\nct(sit\nt) +d(sit−1\nt−1, sit\nt)\u0001\n≥mini1,...,iTPT\nt=1ft(it)−kD,\nwhere the minimum is taken over all solutions i1, . . . , i Tsuch that the number of steps where\nit−1̸=itis at most k.\nWe evaluate the performance of our algorithms using expected pseudoregret regret (further\nabbreviated as regret ). For k≥0, we define\nRegk(ALG) = E[ALG] −OPT ≤k,\n5\n\nwhere ALG denotes the cost incurred by the algorithm on the given input instance with access\nto the given heuristics. We assume that the adversary is oblivious and has to fix the MTS input\ninstance and the solutions of the heuristics before seeing the random bits of the algorithm. We say\nthat an algorithm is ρ-competitive with respect to an offline algorithm OFF, ifE[ALG ]≤ρOFF +α\nholds on every input instances, where αis a constant independent on the input instance and we\nuseOFF to denote both the algorithm and its cost. If OFF is an offline optimal algorithm, we call\nρthe competitive ratio of ALG.\nRounding fractional algorithms. A fractional algorithm for MTS is an algorithm which, at\neach time t, produces a distribution pt∈∆Mover the states in M. These distributions do not yet\nsay much about the movement costs of the algorithm. But there is a standard way to turn such a\nfractional algorithm into a randomized algorithm for MTS.\nProposition 2.1. There is an online randomized algorithm for MTS which, receiving online a\nsequence of distributions p1, . . . , p T∈∆M, produces a solution s1, . . . , s T∈Mwith expected cost\nequal to\nE[ALG] =PT\nt=1\u0000\ncT\ntpt+ EMD( pt−1, pt)\u0001\n,\nwhere EMD denotes the Earth mover distance with respect to the metric space M.\nWe include the proof of this standard fact in Appendix B together with the following, very\nsimilar, proposition, where we overestimated the cost of switching between the states of two\nheuristics by D.\nProposition 2.2. There is an online randomized algorithm which, receiving online a sequence\nof distributions x1, . . . , x T∈∆ℓover the heuristics, queries at each time ta heuristic itwith\nprobability xt(it)such that\nE\u0002TX\nt=1ct(sit\nt) +d(sit−1\nt−1, sit\nt)\u0003\n≤TX\nt=1fT\ntxt+D\n2∥xt−1−xt∥1.\nBasic learning algorithms. We use the classical algorithms for online learning with expert\nadvice HEDGE (Freund & Schapire (1997)) and SHARE (Herbster & Warmuth (1998)). Both\nsatisfy the following property with ηbeing their learning rate. For both of them, the proof is\ncontained in (Blum & Burch, 2000), we discuss more details, as well as the learning dynamics in\nAppendix C.\nProperty 2.3. There is a parameter ηsuch that the following holds. Denoting xt−1andxtthe\nsolutions of the algorithm before and after receiving loss vector gt−1, we have\n∥xt−1−xt∥1≤ηgT\nt−1xt−1.\nWe use the bounds for HEDGE tuned for “small losses”, see (Cesa-Bianchi & Lugosi, 2006).\nProposition 2.4. Consider x1, . . . , x T∈∆ℓthe solution produced by HEDGE with learning rate\nηand denote γ:= 1−exp(−η). For any x∗∈∆ℓ, we have\nTX\nt=1gT\ntxt≤ηPT\nt=1gT\ntx∗+ lnℓ\n1−exp(−η)≤(1 +γ)TX\nt=1gT\ntx∗+lnℓ\nγ.\n3 Algorithm for m-Delayed Bandit Access to Heuristics\n3.1 Basic Approach and Comparison to Previous Works\nArora et al. (2012) use the following approach to limit the number of switches between arms (or\nheuristics): split the time horizon into blocks of length τand use some MAB algorithm to choose a\n6\n\nsingle arm (or heuristic) for each block which is then played during the whole block. The number\nof switches is then at most T/τ. This is a common approach to reduce the number of switches, see\n(Rouyer et al., 2021; Amir et al., 2022; Blum & Mansour, 2007). However, this approach does not\nlead to regret sublinear in OPT ≤0which can be much smaller than T. In order to have the number\nof switches T/τ≤o(OPT ≤0), we have to choose τ=ω(T/OPT ≤0) which can be ω(OPT ≤0) for\nsmall OPT ≤0. However, with blocks so large, already a single exploration of some arbitrarily bad\nheuristic would induce a cost of order τ≥ω(OPT ≤0).\nIn turn, our algorithm is more similar to the classical MAB algorithm alternating exploration\nand exploitation steps, see (Slivkins, 2019). However, there are three key differences and each of\nthem is necessary to achieve our performance guarantees:\n•Our algorithm makes improper steps (i.e., steps not taken by any of the heuristics);\n•We use MTS-style rounding to ensure bounded switching cost instead of independent random\nchoice at each time step;\n•Exploration steps are not sampled independently since our setting requires m≥2.\nIn particular, the last difference leads to more involving analysis. This is because we cannot\nassume that we have an unbiased estimator of the loss vector and consequently need to do extensive\nconditioning on past events. Moreover, the cost of only one of the time steps during each exploration\nphase can be directly charged to the expected loss of the internal full-feedback algorithm. We need\nto exploit the stability property of the internal full-feedback algorithm in order to relate the costs\nincurred during the steps of each exploration block.\nDuring each exploitation step t, our algorithm follows the advice of the exploited heuristic\nwhich is sampled from the algorithm’s internal distribution xtover the heuristics. Each exploration\nsteptis set up so that the algorithm discovers the cost of the heuristic Hetchosen uniformly at\nrandom and updates its distribution over the heuristics. However, the algorithm does not follow\nHet. Instead, it makes a greedy step from the last known state of the exploited heuristic.\n3.2 Description\nLet¯Abe an algorithm for the classical learning from expert advice in full feedback setting (e.g.\nHEDGE or SHARE), and ϵbe a parameter controlling our exploration rate. For each time step\nt= 1, . . . , T , we sample βt∈ {0,1}such that βt= 1 with probability ϵandetis chosen uniformly\nat random from {1, . . . , ℓ}. We set β0= 0 since the algorithm starts querying predictors from\nt= 1. Moreover, we assume all heuristics reside in s0att= 0. If tis an exploitation step ( t∈X)\nandβt= 0, the next step will be again exploitation. Otherwise, the algorithm skips mtime steps\nwhich are needed to bootstrap the explored heuristic Het+mand performs exploration in step\nt+m. At this latter step, the algorithm receives the cost ft+m(et+m) ofHet+mand uses it to\nupdate its distribution xt+m+1over the heuristics. This update is performed using the algorithm\n¯Areceiving as input a loss vector get+m\nt+mdefined as follows: get+m\nt+m(i) =ft+m(et+m)/2Difi=et+m\nand 0 otherwise. Thanks to the assumption that ft+m∈[0,2D]ℓ, we have gt+m∈[0,1]ℓ. Each\nexploration step is followed by an exploitation step. See the summary of this learning dynamics in\nAlgorithm 1. With m= 1, up to the scaling of the loss function, this dynamics would correspond\nto the classical algorithm for MAB which performs exploration with probability ϵand achieves\nregret O(T2/3) (Slivkins, 2019). Note that our setting requires m≥2.\nNow, it is enough to describe how to turn the steps of Algorithm 1 into a solution for the\noriginal MTS instance. First, we define xt+1=xtfor any t /∈E. Att= 1, we sample the exploited\nheuristic from the distribution x1and at each update of xt, we switch to a different heuristic with\nprobability1\n2∥xt−1−xt∥1using the procedure Round from Proposition 2.2 in order to ensure\nthat, at each time step t, we are following heuristic iwith probability xt(i). The state of the\nexploited heuristic is not known to us msteps before each exploitation step and another msteps\nafter. During these time steps, we make greedy steps from the last known state sit′\nt′of the heuristic\nHit′exploited at time t′. Namely, we choose st:=arg min s∈M(d(sit′\nt′, s) +ct(s)). This procedure is\nsummarized in Algorithm 2.\n7\n\nAlgorithm 1: Learning dynamics\n1Initialization:\n2β0:= 0, t:= 0, X:=∅,E:=∅\n3β1, . . . , β T∼Bernoulli( ϵ)\n4e1, . . . , e T∼U({1, . . . , ℓ})\n5x0is chosen by ¯A\n6while t≤Tdo\n7 addttoX // Exploitation step\n8 ifβt= 1then\n9 t:=t+m // Skip msteps\n10 addttoE // Exploration step\n11 xt+1chosen by ¯Aafter feedback get\nt\n12 t+ +\nAlgorithm 2: Producing solution for MTS\n1Input: x0, . . . , x Tproduced by Algorithm 1\n2i0∼U({1, . . . , ℓ})\n3fort= 1, . . . , T do\n4 ifxt̸=xt−1then it∼Round( it−1, xt−1, xt)\n5 else it:=it−1\n6 ifsit\ntis known then\n7 go to state st:=sit\ntand set bt:=st\n8 else /*mstates before and after exploration */\n9 setbt:=bt−1/* last successful query */\n10 st:= arg min s∈M(d(bt, s) +ct(s))\nLemma 3.1. Given x1, . . . , x T∈[0,1]ℓproduced by Algorithm 1, the expected cost of Algorithm 2\nis at most\u0000\n1 +O\u0000\nϵm2\u0001\u0001PT\nt=1\u0000\nfT\ntxt+D∥xt−1−xt∥1\u0001\n.\nA similar statement is proved in (Antoniadis et al., 2023a). We include our proof in Appendix A.\nChoice of hyperparameters. To achieve the regret bound in Theorem 3.9, we choose the\nparameter ϵ:= (Dℓlnℓ)1/3m−4/3OPT−1/3\n≤0for Algorithm 1 and the learning rate ηof HEDGE\nwhich is used as ¯Ais chosen based on γ:= (Dℓlnℓ)1/3m2/3OPT−1/3\n≤0, where γ= 1−exp(−η).\nWhile D, ℓ, m are usually known from the problem description, OPT ≤0can be guessed by doubling\nas described in Appendix D.\n3.3 Analysis\nWe introduce the following random variables which will be useful in our analysis. We define Xtas\nan indicator variable such that Xt= 1 if t∈Xand 0 otherwise. Similarly, Etis an indicator of\nt∈E. These variables are determined by β1, . . . , β T. We also define gt=Et·get\ntwhich is 0 for\nanyt /∈E. We consider a filtration F0⊆ F 1⊆ ··· ⊆ F T, where Ftis aσ-algebra generated by the\nrealizations of β1, . . . , β tande1, . . . , e t. Note that these realizations determine Xt′, Et′, gt′, and\nxt′+1for any t′≤t. Moreover, we have the following observations.\nObservation 3.2. For any t= 0, . . . , T −m, we have Et+m=βtXt.\nThis is because t+m∈Eif and only if t∈Xandβt= 1.\n8\n\nObservation 3.3. For any i= 1, . . . , m andt= 0, . . . , T −i, we have Xt+i≥Qi−1\nj=0(1−βt+j)Xt.\nThis holds because if Xt= 1 and βt=···=βt+i−1= 0, then also Xt+i= 1.\nObservation 3.4. Fori= 1, . . . , m andt=i, . . . , T , we have E[gt| Ft−i] =Etft/(2Dℓ).\nThis follows from E[gt| Ft−i] =E[Etget\nt| Ft−i] =E[βt−mXt−mget\nt| Ft−i] =Et·E[get\nt| Ft−i].\nUsing Observation 3.4, we estimate the cost of the optimal solution OPT ≤0.\nLemma 3.5. Letx∗∈∆ℓbe a solution minimizingPT\nt=1fT\ntx∗. We have\nE\"TX\nt=1gT\ntx∗#\n≤ϵ\n2DℓOPT ≤0.\nProof. By Observation 3.4, we have\nE[gT\ntx∗] =E\u0002\nE\u0002\ngT\ntx∗| Ft−1\u0003\u0003\n=E\u0014Et\n2DℓfT\ntx∗\u0015\n=E[βt−m]\n2DℓE[Xt−mfT\ntx∗]≤ϵ\n2DℓfT\ntx∗\nfor each t= 1, . . . , T . Summing over t, we get\nE\"TX\nt=1gT\ntx∗#\n≤ϵ\n2DℓTX\nt=1fT\ntx∗=ϵ\n2DℓOPT ≤0.\nThe next lemma will be used to bound the costs perceived by Algorithm 1 at time steps tsuch\nthatXt−m= 1.\nLemma 3.6. For each i= 0, . . . , m , we have\nE\"TX\nt=m+1gT\ntxt#\n=ϵ\n2DℓE\"TX\nt=m+1fT\ntxt−iXt−m#\n.\nProof. Fort=m+ 1, . . . , T , we have\nE[gT\ntxt] =E\u0002\nE\u0002\ngT\nt| Ft−1\u0003\nxt\u0003\n=E\u0014Et\n2DℓfT\ntxt\u0015\n=E[βt−m]\n2DℓE[fT\ntxtXt−m],\nwhere the second equality follows from Observation 3.4. To finish the proof, it is enough to note\nthatE[βt−m] =ϵandxtXt−m=xt−iXt−mfor any i= 0, . . . , m .\nBounding costs in time steps twhen Xt−m= 0 is more involving. In such case, there is Et−i= 1\nfor some i∈ {1, . . . , m }. We will use the following stability property.\nLemma 3.7. If¯Asatisfies Property 2.3, the following holds for every t≥m+1andi∈ {1, . . . , m }:\nE[fT\ntxtEt−i]≤E[fT\ntxt−iEt−i] +η\nℓE[fT\nt−ixt−iEt−i].\nProof of Lemma 3.7. We use the Cauchy-Schwarz inequality and Property 2.3:\nE[fT\ntxtEt−i] =E[(fT\nt(xt−xt−i) +fT\ntxt−i)Et−i]\n≤E[(∥ft∥∞∥xt−xt−i∥1+fT\ntxt−i)Et−i]\n≤η\nℓE[fT\nt−ixt−iEt−i] +E[fT\ntxt−iEt−i].\n9\n\nThe last inequality follows from ∥ft∥∞≤2Dand the following computation. Here, we use\nxt−i+1=xtwhenever Et−i= 1, Property 2.3, xt−iandEt−idepending only on time steps up to\nt−i−1, and Observation 3.4:\nE[∥xt−xt−i∥1·Et−i] =E[∥xt−i+1−xt−i∥1·Et−i]\n≤E[ηgT\nt−ixt−iEt−i]\n=E\u0002\nE\u0002\nηgT\nt−i| Ft−i−1\u0003\nxt−iEt−i\u0003\n=η\n2DℓE[fT\nt−ixt−iEt−i].\nThe following lemma is the core of our argument. In the proof, it decomposes the costs perceived\nby Algorithm 1 in each step tdepending on the value of Xt−m. IfXt−m= 1, such costs are easy\nto bound using Lemma 3.6. Those with Xt−m= 0 need to be charged to some other exploitation\nstep using the preceding stability lemma.\nLemma 3.8. If¯Asatisfies Property 2.3 then E\u0002PT\nt=1fT\ntxt\u0003\nis at most\n2m+\u0000\n1 +mϵ\n(1−ϵ)m+mηϵ\nℓ\u0001\n·2Dℓ\nϵE\u0002TX\nt=1gT\ntxt\u0003\n.\nProof. We start with an observation that for any t≥m+ 1, exactly one of the following holds:\neither Xt−m= 1 or the step t−mis skipped due to the exploration at time t−i(i.e., Et−i= 1)\nfor some i∈1, . . . , m . We can write\nE\u0002TX\nt=1fT\ntxt\u0003\n≤2m+E\u0002TX\nt=2m+1fT\ntxt·Xt−m\u0003\n+mX\ni=1E\u0002TX\nt=2m+1fT\ntxt·Et−i\u0003\n.\nBy Lemma 3.6, the first expectation in the right-hand side is at most2Dℓ\nϵE[PT\nt=1gT\ntxt]. In order\nto prove the lemma, it is therefore enough to show that, for each i= 1, . . . , m ,E\u0002PT\nt=2m+1fT\ntxt·\nEt−i\u0003\nis at most\n\u0012ϵ\n(1−ϵ)m+ηϵ\nℓ\u0013\n·2Dℓ\nϵE\u0002TX\nt=1gT\ntxt\u0003\n. (1)\nFirst, we use Lemma 3.7 to obtain\nTX\nt=2m+1E[fT\ntxtEt−i]≤TX\nt=2m+1E[fT\ntxt−iEt−i] (2)\n+η\nℓTX\nt=2m+1E[fT\nt−ixt−iEt−i].\nThe second term is easy to bound: by Observation 3.2, we have E[fT\nt−ixt−iEt−i] =E[βt−i−m]E[fT\nt−ixt−iXt−i−m].\nTherefore, we can write\nη\nℓTX\nt=2m+1E[fT\nt−ixt−iEt−i]≤ηϵ\nℓTX\nt=m+1E[fT\ntxtXt−m]. (3)\nUsing Observation 3.2, independence of Xt−i−mandβt−j−mforj= 1, . . . , i , and Observation 3.3,\n10\n\nwe get\nTX\nt=2m+1E[fT\ntxt−iEt−i] (1−ϵ)i\n=TX\nt=2m+1E[βt−i−m]E[fT\ntxt−iXt−i−m]E\u0014iY\nj=1(1−βt−j−m)\u0015\n≤TX\nt=2m+1E[βt−i−m]E[fT\ntxt−iXt−m].\nIn other words, we can bound the first term of (2) as\nTX\nt=2m+1E[fT\ntxt−iEt−i]≤ϵ\n(1−ϵ)iTX\nt=2m+1E[fT\ntxt−iXt−m]. (4)\nNow it is enough to apply Lemma 3.6 to the right-hand sides of (4)and(3). Equation (2)then\nimplies (1) which concludes the proof.\nTheorem 3.9. Algorithm 2 using HEDGE as ¯Awith m-delayed bandit access to ℓheuristics on\nany MTS input instance with diameter Dsuch that Dℓm≤o(OPT1/3\n≤0)satisfies the following regret\nbound:\nE[ALG] ≤OPT ≤0+O\u0000\n(Dℓlnℓ)1/3m2/3OPT2/3\n≤0\u0001\n.\nProof. By Lemma 3.1, E[ALG] is at most\n(1 +O(m2ϵ))\u0012TX\nt=1E[fT\ntxt] +TX\nt=1E[D∥xt−1−xt∥1]\u0013\n.\nThe first term in the parenthesis can be bounded using Lemma 3.8. The second term can be\nbounded using Property 2.3: denoting ηthe learning rate of ¯A, we have\nTX\nt=1E[D∥xt−1−xt∥1]≤DηTX\nt=1E[gT\ntxt].\nAltogether, using (1 −ϵ)m>1/2 and Dη≤mϵ·η·2Dℓ\nϵ, we get that E[ALG] is at most\n\u0000\n1+O(m2ϵ)\u0001\u0012\n2m+\u0000\n1+O(mϵ)(1+η)\u00012Dℓ\nϵTX\nt=1E[gT\ntxt]\u0013\n.\nNow we use the regret bound of ¯A(Proposition 2.4). Let Hi∗be the best heuristic and x∗∈[0,1]ℓ\nbe the vector such that x∗(i∗) = 1 and x∗(i) = 0 for every i̸=i∗. We have\n2Dℓ\nϵTX\nt=1E[gT\ntxt]≤(1 +γ)2Dℓ\nϵTX\nt=1E[gT\ntx∗] +2Dℓlnℓ\nϵγ\n≤(1 +γ) OPT ≤0+2Dℓlnℓ\nϵγ,\nwhere the second inequality used Lemma 5.2. In total, E[ALG] is at most\nOPT ≤0+O(m2ϵ+γ+m2ϵγ) OPT ≤0+O\u00122Dℓlnℓ\nϵγ\u0013\n.\nIt is enough to choose ϵ:= (Dℓlnℓ)1/3m−4/3OPT−1/3\n≤0andγ:= (Dℓlnℓ)1/3m2/3OPT−1/3\n≤0to get\nthe desired bound.\n11\n\n1\n1\n1\n1\n1\na1\nb1\na2\nb2\nr2\nr1\nM2\nM1Figure 1: With ℓ= 2,Mis the metric closure of this graph.\n4 Tight Lower Bound: Proof of Theorem 1.3\nIn this section, we prove a lower bound matching Theorem 3.9. Our proof is based on the\nconstruction of Dekel et al. (2013) for Bandits with Switching Costs, which is a special case of\nBandit Learning against a 1-Memory Bounded Adversary. In order to use their construction in our\nsetting, we need to address additional challenges due to the algorithm having more power in our\nsetting:\n1.The algorithm can see the full MTS input instance as it arrives online, and therefore it can\ntake actions on its own regardless of the advice of the queried heuristic.\n2.The algorithm (as it is common in MTS setting) has 1-step lookahead, i.e., it observes the\ncost function ctfirst and only then chooses the state st.\nNote that both properties are indispensable in our model, see Section 2, and any non-trivial positive\nresult would be impossible without them.\nWe create an MTS input instance circumventing the advantages of the algorithm mentioned\nabove. Our instance is generated at random and is oblivious to the construction of Dekel et al.\n(2013). However, we make a correspondence between the timeline in our MTS instance and in their\nBandit instance: our instance consists of blocks of length three, where each block corresponds to\na single time step in their Bandit instance. The construction of Dekel et al. (2013) itself comes\ninto play when generating the solutions of the heuristics. We create these solutions in such a way\nthat the cost of the heuristic Hican be related to the cost of the ith arm in (Dekel et al., 2013).\nMoreover, the state of Hidepends only on the loss of the ith arm at given time.\nProposition 4.1 (Dekel et al. (2013)) .There is a stochastic instance ℓ1, . . . , ℓ Tof Bandits with\nSwitching Costs such that the expected regret of any deterministic algorithm producing solution\ni1, . . . , i T∈[ℓ]is\nE\u0002TX\nt=1(ℓt(it) +1(it̸=it−1))\u0003\n−min\ni∈[ℓ]TX\nt=1ℓt(i)≥˜Ω(ℓ1\n3T2\n3).\nDescription of the MTS instance. The metric space Mconsists of ℓparts M1, . . . , M ℓ,\nwhere Mi={ri, ai, bi}. The distances are chosen as follows: For j̸=i, we have d(ri, rj) = 1,\nd(ri, aj) =d(ri, bj) = 2, and d(ri, ai) =d(ri, bi) = 1. We have d(ai, bj) = 2 if i=jand 3 otherwise.\nSee Figure 1 for an illustration.\nThe input sequence consists of blocks of length three. We choose the cost functions in block j\nas follows. For i= 1, . . . , ℓ , we choose σi\nj∈ {ai, bi}uniformly at random. For the first step of the\nblock j, we define the cost function c′\nj(s) such that c′\nj(s) = 0 if s∈ {r1, . . . , r ℓ}andc′\nj(s) = +∞\notherwise. In the second time step, we issue c′′\nj(s) = +∞ifs∈ {r1, . . . , r ℓ}andc′′\nj(s) = 0 otherwise.\nIn the third time step, we issue c′′′\nj(s) = 0 if s∈ {σ1\nj, . . . , σℓ\nj}andc′′′\nj(s) = +∞otherwise4.\n4We use infinite costs for a cleaner presentation. But choosing 2 D= 6 instead of + ∞does the same job, see\ndiscussion in Section 2.\n12\n\nHere is the intuition behind this construction. During block j, any reasonable algorithm stays in\ns′∈ {r1, . . . , r ℓ}in the first step and in s′′′∈ {σ1\nj, . . . , σℓ\nj}in the third step. Ideally, the algorithm\nwould move to s′′′already in the second step. However, it does not yet know σ1\nj, . . . , σℓ\njand\ntherefore needs the advice of the heuristics. In the first step of each block, the algorithm pays 1 if\nit is staying in the same part Miof the metric space (for returning to ri). If it is moving from Mj\ntoMi, it has to pay 2, i.e., an additional unit cost.\nDescription of the heuristics. For each i, the heuristic Hiremains in Mithroughout the\nwhole input instance. In the first step of the block t, it moves to the state ri. In the third step, it\nmoves to σi\nt. Its position in the second step is derived from the Bandit instance. With probability\n(1−ℓt(i)\n2), it is σi\nt, and with probabilityℓt(i)\n2it is the other point, i.e., {ai, bi}\\{σi\nt}. In the block t,\nthe heuristic Hipays 2 for the movements in the steps 1 and 2 and, with probabilityℓt(i)\n2, another\n2 for the movement in the step 3. Therefore, we have the following observation.\nObservation 4.2. The expected cost of the heuristic Hiis equal to 2T+PT\nt=1ℓt(i).\nWe consider the following special class of algorithms for our MTS instance called tracking\nalgorithms. This class has two important properties: firstly, any algorithm on our MTS instance\ncan easily be converted to a tracking algorithm without increasing its cost. Secondly, solutions\nproduced by tracking algorithms can be naturally converted online to the problem of Bandits with\nSwitching Cost.\nDefinition 4.3. Consider an algorithm Awith a bandit access to heuristics H1, . . . , H ℓ. We say\nthatAis atracking algorithm if, while processing the MTS instance described above, satisfies the\nfollowing condition in every block. Let Hibe the heuristic queried in the second step. Then the\nalgorithm resides in Miduring the whole block and moves to the state of Hiin its second step.\nLemma 4.4. Any algorithm Awith bandit access to H1, . . . , H ℓcan be converted to a tracking\nalgorithm ¯Awithout increasing its expected cost.\nProof. Consider a block such that the algorithm Amoves to another part of Min the second\nstep. The cost functions in the first two steps are deterministic, so we can simulate what part the\nalgorithm would go to in the second step and move there already in the first step for the same cost.\nConsider a block such that the algorithm moves to another part of Min the third step, i.e.,\nmoves from s∈ {ai, bi}tos′∈ {aj, bj}. This costs 3 in the third step and the subsequent move to\nrjin the beginning of the next block will cost 1. Instead, staying in Micosts at most 2 in the\nthird step and moving to rjin the beginning of the next block will cost 2.\nTherefore, we can make sure that the algorithm moves between different parts of Monly in the\nfirst step of each block.\nConsider the step 2 of block twhere Aqueries the state si\ntofHi. IfAmoves to s∈ {aj, bj},\nwhere j̸=i, its expected cost will be at least 1, since σj\ntis chosen from {aj, bj}uniformly at\nrandom.\nIfAmoves to s∈ {ai, bi} \\ {si\nt}, Then the expected cost of Ain step 3 will be\n\u0012\n1−ℓt(i)\n2\u0013\n2 = 2−ℓt(i)≥1≥ℓt(i) = 2ℓt(i)\n2,\nwhere the right-hand side corresponds to the cost of algorithm ¯Awhich moves to si\ntinstead.\nLemma 4.5. Consider a tracking algorithm A. For each block t, define itsuch that Ais located\natsit\ntin its second step. The expected cost of Ais2T+PT\nt=1ℓt(it) +PT\nt=11(it̸=it−1).\nProof. In each block t= 1, . . . , T , the algorithm pays 2 + 1(it̸=it−1) in the first two steps. In the\nthird step, it pays 2 only if sit\nt̸=σit\ntwhich happens with probabilityℓt(it)\n2.\n13\n\nTheorem 1.3 is derived from the following statement by Yao’s principle and from the fact that\nwe can pad the constructed instance with zero cost vectors to get an input instance of length higher\nthan OPT ≤0.\nTheorem 4.6. There is a stochastic instance Iof MTS of length 3Twith heuristics H1, . . . , H ℓ\nsuch that any deterministic algorithm with bandit access to H1, . . . , H ℓsuffers expected regret at\nleast ˜Ω(ℓ1/3T2/3).\nProof. LetIbe the input instance constructed above from ℓ1, . . . , ℓ Tin Proposition 4.1. Consider\na fixed deterministic algorithm A. Firstly, we consider queries made by A. The queries in the first\nand the third step of each block have a trivial answer which are independent of the loss sequence\nℓ1, . . . , ℓ T: ifAqueries Hi, then the answers are riandσi\ntrespectively. For each block t= 1, . . . , T ,\nwe denote itthe heuristic queried in its second step. Note that the result of the query depends\nonly on ℓt(it).\nWe can assume that Ais tracking, since making it tracking would only decrease its expected\ncost. Therefore, the expected cost of Acan be written as\nTX\nt=1E\u0002\n1(it̸=it−1) + 2 + 2 ·1(sit\nt̸=σi\nt)\u0003\n= 2T+TX\nt=1E\u0014\n1(it̸=it−1) + 2E[1(sit\nt̸=σi\nt)|ℓt(it)]\u0015\n= 2T+TX\nt=1E[1(it̸=it−1) +ℓt(it)].\nLetHi∗denote the best heuristic. Using Observation 4.2, the regret of Awith respect to Hi∗is\nequal to\nTX\nt=1E[1(it̸=it−1) +ℓt(it)]−TX\nt=1E[ℓt(i∗)].\nThis is equal to the expected regret of the strategy playing arms i1, . . . , i Ton the sequence ℓ1, . . . , ℓ T.\nBy Proposition 4.1, this regret is at least ˜Ω(ℓ1/3T2/3).\n4.1 Tight dependence on parameters kandD\nWe elaborate on the asymptotic dependence on kandDfor the lower bound with 2-delayed\nexploration starting from the construction in Section 4.\nThe optimality of the dependence on Dcan be seen from scaling. If we scale the input instance\nin Theorem 4.6 by a factor D, i.e. we multiply all distances in the metric space and all cost vectors\nbyD, the cost of any algorithm including OPT ≤0will be scaled by D. Therefore, the lower bound\nin Therorem 4.6 becomes\nD·E[ALG] ≤DOPT ≤0+D˜Ω(ℓ1/3OPT2/3\n≤0) = OPT′\n≤0+D1/3˜Ω(ℓ1/3(OPT′\n≤0)2/3),\nwhere OPT′\n≤0= OPT ≤0is the new value after the scaling.\nNow, we show tightness of kin Theorem 1.2.\nTheorem 4.7. There exists a stochastic instance Iof MTS of length 3Twith heuristics H1, . . . , H ℓ\nsuch that any deterministic algorithm with bandit access to H1, . . . , H ℓsuffers expected regret at\nleast ˜Ω((kℓ)1/3T2/3).\nProof. LetALG be any deterministic algorithm for MTS with bandit access to H1, . . . , H ℓ. We\nassume without loss of generality that there exists τ∈Nmultiple of 3 such that 3 ·T=k·τ. We\n14\n\nsplit the time horizon into ksegments of size τ. By Theorem 4.6, for each j∈[k] we have\nE\nτjX\nt=τ(j−1)+1\u0000\nfT\ntxt+d(xt, xt−1)\u0001\n≥c(H∗\nj) + Ω(( ℓ)1/3τ2/3),\nwhere c(H∗\nj) represents the cost of the best heuristic in the segment j. It follows that\nE[ALG] ≥kX\nj=1c(H∗\nj) + Ω( kℓ1/3τ2/3)\n≥OPT ≤k+Ω(( kℓ)1/3T2/3).\n5Upper Bound for m-Delayed Bandit Access to Heuristics\nagainst OPT ≤k\nIn what follows, we prove an upper bound for Algorithm 1 using SHARE as ¯Aagainst OPT ≤k.\nFirstly, SHARE satisfies the following performance bound found in (Cesa-Bianchi & Lugosi,\n2006).\nProposition 5.1. Consider x1, . . . , x T∈[0,1]ℓthe solution produced by SHARE with learning\nrateη, sharing parameter α, and denote γ:= 1−exp(−η). For any solution i1, . . . , i Tsuch that\nthe number of time steps where it−1̸=itis at most k, we have\nTX\nt=1gT\ntxt≤ln1\n1−γ\nγ(1−α)TX\nt=1gt(it) +kln (ℓ/α)\nγ(1−α).\nThe following is a generalization of Lemma 3.5 that holds for k≥0.\nLemma 5.2. Consider k≥0. Let i1, . . . , i T∈[ℓ]be a solution minimizingPT\nt=1\u0000\nct(sit\nt) +\nd(sit−1\nt−1, sit\nt)\u0001\nsuch that it−1̸=itholds in at most ktime steps t. For each t, we define xt∈[0,1]ℓ\nsuch that xt(it) = 1 andxt(i) = 0 for each i̸=it. We have\nE\"TX\nt=1gT\ntxt#\n≤ϵk\n2ℓ+ϵ\n2DℓOPT≤k.\nProof. By Observation 3.4, we have\nE[gT\ntxt] =E\u0002\nE\u0002\ngT\ntxt| Ft−1\u0003\u0003\n=E\u0014Et\n2DℓfT\ntxt\u0015\n=ϵ\n2DℓE[Xt−mfT\ntxt]≤ϵ\n2DℓfT\ntxt\nfor each t= 1, . . . , T . Let δt:=d(sit\nt−1, sit\nt)−d(sit−1\nt−1, sit\nt)≤D. Summing over t, we get\nE\"TX\nt=1gT\ntxt#\n≤ϵ\n2DℓTX\nt=1fT\ntxt\n=ϵ\n2DℓTX\nt=1\u0010\nct(sit\nt) +d(sit−1\nt−1, sit\nt) +δt\u0011\n≤ϵ\n2Dℓ(OPT ≤k+kD),\nsince δt>0 for at most ktime steps.\n15\n\nWe are now ready to prove the following upper bound.\nTheorem 5.3. Algorithm 2 with SHARE as ¯Aandm-delayed bandit access to ℓheuristics on any\nMTS instance with diameter Dwith offline optimum cost at least 2ksuch that Dℓm≤o(OPT1/3\n≤k)\nachieves Regk(ALG) at most\nO\u0012\n(Dℓk)1/3m2/3OPT2/3\n≤klnℓ1/3(OPT ≤k)2/3\n(Dk)2/3m4/3\u0013\n.\nProof. LetHi∗\n1, . . . , H i∗\nTbe the sequence of optimal heuristics such that i∗\nt̸=i∗\nt+1for at most k\nindices t∈[T−1]. Then we define x∗\nt∈[0,1]ℓsuch that x∗\nt(i∗\nt) = 1 and x∗\nt(i) = 0 for i̸=i∗\nt.\nProceeding analogously to the proof of Theorem 3.9 we have\nE[ALG] ≤\u0000\n1+O(m2ϵ)\u0001\u0012\n2m+\u0000\n1+O(mϵ)(1+η)\u00012Dℓ\nϵTX\nt=1E[gT\ntxt]\u0013\n.\nUsing the regret bound of ¯A(Proposition 5.1) followed by Lemma 5.2 we have\n2Dℓ\nϵTX\nt=1E[gT\ntxt]≤ln1\n1−γ\nγ(1−α)·2Dℓ\nϵE\"TX\nt=1gT\ntx∗\nt#\n+ 2Dℓkln (ℓ/α)\nϵγ(1−α)\n≤ln1\n1−γ\nγ(1−α)OPT ≤k+(1 + 2 ℓ)Dkln (ℓ/α)\nϵγ(1−α).\nBy setting α:= (Dℓk)/(ϵOPT ≤k) and γ:=p\nDℓk/ (ϵOPT ≤k) and considering that k≤OPT ≤k/2,\nwe obtain\n(1 +η)2Dℓ\nϵTX\nt=1E[gT\ntxt]≤OPT ≤k+O r\nDℓkOPT ≤k\nϵlnϵOPT ≤k\nDk!\n+O\u0012\nDℓklnϵOPT ≤k\nDk\u0013\n.\nwhere η= ln 1 /(1−γ) is the learning rate. It follows that the total expected cost is at most\nE[ALG] ≤OPT ≤k+O(m2ϵOPT ≤k) +O r\nDℓkOPT ≤k\nϵlnϵOPT ≤k\nDk!\n+O\u0012\nmϵDℓk lnϵOPT ≤k\nDk\u0013\n.\nFinally, by setting ϵ:= (OPT ≤k)−1/3(Dℓk)1/3m−4/3we obtain the desired bound.\nCorollary 5.4. Algorithm 2 is (1 +ϵ)-competitive against OPT ≤kforkas large as\nΩ\u0012ϵ3OPT ≤k\nDℓm2(lnZ)3\u0013\nwhere Z=ℓ1/3(OPT≤k)2/3/(D2/3m4/3).\n6 Upper Bound in the Setting of Arora et al. (2012)\nWe demonstrate how to use the solutions x1, . . . , x Tproduced by Algorithm 1 in order obtain\nsolutions in the setting of MAB against m-memory bounded adversaries introduced by Arora et al.\n(2012). To achieve this, we propose Algorithm 3 and analyze its performance. Note that Algorithm\n1 does not require any look-ahead, so it can be used directly in the setting of Arora et al. (2012).\nTo obtain a regret bound in this setting, we must relate the total cost incurred by Algorithm 3\ntoE[PT\nt=1fT\ntxt] which we can upper bound using Lemma 3.8.\n16\n\nAlgorithm 3: Producing solutions for MAB against m-memory bounded adversaries\n1Input: x1, . . . , x Tresulting from Algorithm 1\n2fort= 1, . . . , T do\n3 ifxt̸=xt−1then it∼Round( it−1, xt−1, xt)\n4 else it:=it−1\n5 playit\nLemma 6.1. Given x1, . . . , x T∈[0,1]ℓproduced by Algorithm 1, the expected cost of Algorithm 3\nis at most\nO(mϵT) +E\"TX\nt=1fT\ntxt#\nProof. We begin the proof by observing that E[lt(it)·Xt]≤E[fT\ntxt], since the algorithm plays it\nsampled from xtduring exploitation rounds. For the time steps leading up to an exploration round\nand the exploration step itself we can only say that the cost per round incurred is at most 1. We\nthus have\nE[ALG] = E\"TX\nt=1ℓt(it)#\n≤E\"TX\nt=1(1·(1−Xt) +ℓt(it)·Xt)#\n≤E\"TX\nt=1(1−Xt)#\n+E\"TX\nt=1fT\ntxt#\nIt remains to estimate the first term. By definition of Xt, Et, βtand Observation 3.2 we have:\nE\"TX\nt=1(1−Xt)#\n≤2m+m−1X\ni=0E\"TX\nt=2m+1Et−i#\n= 2m+m−1X\ni=0TX\nt=2m+1E[βt−i−m]·E[Xt−i−m]\n≤2m+m−1X\ni=0TX\nt=2m+1E[βt−i−m]\n≤O(mϵT).\nWe also need to link the real and perceived costs resulting from following an optimal policy.\nLemma 6.2. Letx∗∈∆ℓbe a solution minimizingPT\nt=1ℓT\ntx∗. We have\nE\"TX\nt=1gT\ntx∗#\n≤ϵ\n2DℓTX\nt=1ℓT\ntx∗\nProof. By Observation 3.4, we have\nE[gT\ntx∗] =E\u0002\nE\u0002\ngT\ntx∗| Ft−1\u0003\u0003\n=E\u0014Et\n2DℓfT\ntx∗\u0015\n=ϵ\n2DℓE[Xt−mfT\ntx∗]≤ϵ\n2DℓfT\ntx∗\n17\n\nfor each t= 1, . . . , T . The second equality holds because Et= 1 only if the same action was taken\nfor the last msteps. Summing over t, we get\nE\"TX\nt=1gT\ntx∗#\n≤ϵ\n2DℓTX\nt=1fT\ntx∗=ϵ\n2DℓTX\nt=1ℓT\ntx∗.\nPutting everything together, we may now prove the following theorem.\nTheorem 6.3. Consider ℓavailable arms and let m≥1be the memory bound of the adaptive\nadversary in the setting of Arora et al. (2012). Then Algorithm 3 achieves the following policy\nregret bound\nO\u0010\n(mℓlnℓ)1/3T2/3\u0011\nProof. We begin the proof by observing that Lemma 3.8 still holds in the setting of Arora et al.\n(2012) as it is based solely on the dynamics of Algorithm 1 and the stability assumption on ¯A.\nUsing Lemma 6.1 followed by Lemma 3.8 we have\nE[ALG ]≤O(mϵT) +E\"TX\nt=1fT\ntxt#\n≤O(mϵT) + 2m+\u0012\n1 +mϵ\n(1−ϵ)m+mηϵ\nℓ\u0013\n·2Dℓ\nϵE\"TX\nt=1gT\ntxt#\nArguing as in the proof of Theorem 3.9, we can use Proposition 2.4 and Lemma 6.2 to obtain\n2Dℓ\nϵE\"TX\nt=1gT\ntxt#\n≤(1 +γ)TX\nt=1ℓT\ntx∗+2Dℓlnℓ\nϵγ\nSincePT\nt=1ℓT\ntx∗≤T, it follows that\nE[ALG] −TX\nt=1ℓT\ntx∗≤O((mϵ+γ)T) +O\u0012Dℓlnℓ\nϵγ\u0013\n.\nSince the losses in the setting of Arora et al. (2012) are contained in [0 ,1]ℓ, we may drop the\ndependence on D. By choosing ϵ:= (ℓlnℓ)1/3m−2/3T−1/3andγ:= (mℓlnℓ)1/3T−1/3we get the\ndesired bound.\nAcknowledgements\nThis research was supported by Junior Researchers’ Grant awarded by Bocconi University thanks\nto the philanthropic gift of the Fondazione Romeo ed Enrica Invernizzi.\nReferences\nAlmanza, M., Chierichetti, F., Lattanzi, S., Panconesi, A., and Re, G. Online facility location with\nmultiple advice. In NeurIPS , pp. 4661–4673, 2021.\nAmir, I., Azov, G., Koren, T., and Livni, R. Better best of both worlds bounds for bandits\nwith switching costs. In Advances in Neural Information Processing Systems , volume 35, pp.\n15800–15810, 2022.\nAnand, K., Ge, R., Kumar, A., and Panigrahi, D. Online algorithms with multiple predictions. In\nICML , volume 162 of Proceedings of Machine Learning Research , pp. 582–598. PMLR, 2022.\n18\n\nAntoniadis, A., Coester, C., Eli´ as, M., Polak, A., and Simon, B. Learning-augmented dynamic\npower management with multiple states via new ski rental bounds. In NeurIPS , 2021.\nAntoniadis, A., Coester, C., Eli´ as, M., Polak, A., and Simon, B. Mixing predictions for online\nmetric algorithms. CoRR , abs/2304.01781, 2023a. doi: 10.48550/arXiv.2304.01781. URL\nhttps://doi.org/10.48550/arXiv.2304.01781 .\nAntoniadis, A., Coester, C., Eli´ aˇ s, M., Polak, A., and Simon, B. Learning-augmented dynamic\npower management with multiple states via new ski rental bounds. ACM Trans. Algorithms , 19(2),\napr 2023b. ISSN 1549-6325. doi: 10.1145/3582689. URL https://doi.org/10.1145/3582689 .\nArora, R., Dekel, O., and Tewari, A. Online bandit learning against an adaptive adversary: from\nregret to policy regret. In Proceedings of the 29th International Coference on International\nConference on Machine Learning , ICML’12, pp. 1747–1754, Madison, WI, USA, 2012. Omnipress.\nISBN 9781450312851.\nAuer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. The nonstochastic multiarmed bandit\nproblem. SIAM Journal on Computing , 32(1):48–77, 2002. doi: 10.1137/S0097539701398375.\nURL https://doi.org/10.1137/S0097539701398375 .\nAzar, Y., Broder, A. Z., and Manasse, M. S. On-line choice of on-line algorithms. In Proceedings\nof the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms , SODA ’93, pp. 432–440,\nUSA, 1993. Society for Industrial and Applied Mathematics. ISBN 0898713137.\nBalkanski, E., Perivier, N., Stein, C., and Wei, H.-T. Energy-efficient scheduling with predictions.\nInProceedings of the 37th International Conference on Neural Information Processing Systems ,\nNIPS ’23, Red Hook, NY, USA, 2023. Curran Associates Inc.\nBamas, ´E., Maggiori, A., Rohwedder, L., and Svensson, O. Learning augmented energy minimization\nvia speed scaling. In NeurIPS , 2020.\nBartal, Y., Bollob´ as, B., and Mendel, M. Ramsey-type theorems for metric spaces with applications\nto online problems. Journal of Computer and System Sciences , 72(5):890–921, August 2006.\nISSN 0022-0000. doi: 10.1016/j.jcss.2005.05.008. URL http://dx.doi.org/10.1016/j.jcss.\n2005.05.008 .\nBenomar, Z. and Perchet, V. Non-clairvoyant scheduling with partial predictions. In Proceedings of\nthe 41st International Conference on Machine Learning , volume 235 of Proceedings of Machine\nLearning Research , pp. 3506–3538, 21–27 Jul 2024a.\nBenomar, Z. and Perchet, V. Non-clairvoyant scheduling with partial predictions. In Salakhut-\ndinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F.\n(eds.), Proceedings of the 41st International Conference on Machine Learning , volume 235 of\nProceedings of Machine Learning Research , pp. 3506–3538. PMLR, 21–27 Jul 2024b. URL\nhttps://proceedings.mlr.press/v235/benomar24a.html .\nBernardini, G., Lindermayr, A., Marchetti-Spaccamela, A., Megow, N., Stougie, L., and Sweering,\nM. A universal error measure for input predictions applied to online graph problems. CoRR ,\nabs/2205.12850, 2022. doi: 10.48550/arXiv.2205.12850. URL https://doi.org/10.48550/\narXiv.2205.12850 .\nBhaskara, A., Cutkosky, A., Kumar, R., and Purohit, M. Online linear optimization with many\nhints. In NeurIPS , 2020.\nBlum, A. and Burch, C. On-line learning and the metrical task system problem. Mach. Learn. , 39\n(1):35–58, 2000. doi: 10.1023/A:1007621832648.\nBlum, A. and Mansour, Y. Learning, Regret Minimization, and Equilibria , pp. 79–102. Cambridge\nUniversity Press, 2007.\n19\n\nBorodin, A., Linial, N., and Saks, M. E. An optimal on-line algorithm for metrical task system. J.\nACM , 39(4):745–763, 1992. doi: 10.1145/146585.146588.\nBubeck, S., Cohen, M. B., Lee, J. R., and Lee, Y. T. Metrical task systems on trees via mirror\ndescent and unfair gluing. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on\nDiscrete Algorithms, SODA 2019 , pp. 89–97, 2019. doi: 10.1137/1.9781611975482.6. URL\nhttps://doi.org/10.1137/1.9781611975482.6 .\nBubeck, S., Coester, C., and Rabani, Y. The randomized k-server conjecture is false! CoRR ,\nabs/2211.05753, 2022a. doi: 10.48550/arXiv.2211.05753.\nBubeck, S., Coester, C., and Rabani, Y. Shortest paths without a map, but with an entropic\nregularizer. In FOCS , pp. 1102–1113. IEEE, 2022b.\nCesa-Bianchi, N. and Lugosi, G. Prediction, learning, and games . Cambridge University Press,\n2006. ISBN 978-0-521-84108-5. doi: 10.1017/CBO9780511546921. URL https://doi.org/10.\n1017/CBO9780511546921 .\nCesa-Bianchi, N., Dekel, O., and Shamir, O. Online learning with switching costs and other\nadaptive adversaries. ArXiv , abs/1302.4387, 2013.\nDavies, S., Moseley, B., Vassilvitskii, S., and Wang, Y. Predictive flows for faster ford-fulkerson. In\nProceedings of the 40th International Conference on Machine Learning , volume 202 of Proceedings\nof Machine Learning Research , pp. 7231–7248. PMLR, 23–29 Jul 2023.\nDekel, O., Ding, J., Koren, T., and Peres, Y. Bandits with switching costs: T2/3 regret. Proceedings\nof the forty-sixth annual ACM symposium on Theory of computing , 2013. URL https://api.\nsemanticscholar.org/CorpusID:9425790 .\nDinitz, M., Im, S., Lavastida, T., Moseley, B., and Vassilvitskii, S. Algorithms with prediction\nportfolios. In NeurIPS , 2022.\nDong, Y., Peng, P., and Vakilian, A. Learning-Augmented Streaming Algorithms for Approximating\nMAX-CUT. In 16th Innovations in Theoretical Computer Science Conference (ITCS 2025) ,\nvolume 325, pp. 44:1–44:24, Dagstuhl, Germany, 2025. Schloss Dagstuhl – Leibniz-Zentrum f¨ ur\nInformatik. ISBN 978-3-95977-361-4. doi: 10.4230/LIPIcs.ITCS.2025.44.\nDrygala, M., Nagarajan, S. G., and Svensson, O. Online algorithms with costly predictions.\nIn Ruiz, F., Dy, J., and van de Meent, J.-W. (eds.), Proceedings of The 26th International\nConference on Artificial Intelligence and Statistics , volume 206 of Proceedings of Machine\nLearning Research , pp. 8078–8101. PMLR, 25–27 Apr 2023. URL https://proceedings.mlr.\npress/v206/drygala23a.html .\nEberle, F., Lindermayr, A., Megow, N., N¨ olke, L., and Schl¨ oter, J. Robustification of online graph\nexploration methods. In AAAI , pp. 9732–9740. AAAI Press, 2022.\nEmek, Y., Kutten, S., and Shi, Y. Online paging with a vanishing regret. In ITCS , 2021.\nFiat, A., Rabani, Y., and Ravid, Y. Competitive k-server algorithms (extended abstract). In\nFOCS , 1990.\nFiat, A., Karp, R. M., Luby, M., McGeoch, L. A., Sleator, D. D., and Young, N. E. Competitive\npaging algorithms. Journal of Algorithms , 12(4):685–699, 1991. ISSN 0196-6774. doi: https:\n//doi.org/10.1016/0196-6774(91)90041-V. URL https://www.sciencedirect.com/science/\narticle/pii/019667749190041V .\nFreund, Y. and Schapire, R. E. A decision-theoretic generalization of on-line learning and an\napplication to boosting. Journal of Computer and System Sciences , 55(1):119–139, 1997. ISSN\n0022-0000. doi: https://doi.org/10.1006/jcss.1997.1504. URL https://www.sciencedirect.\ncom/science/article/pii/S002200009791504X .\n20\n\nGollapudi, S. and Panigrahi, D. Online algorithms for rent-or-buy with expert advice. In ICML ,\nvolume 97 of Proceedings of Machine Learning Research , pp. 2319–2327. PMLR, 2019.\nHerbster, M. and Warmuth, M. K. Tracking the best expert. Machine learning , 32(2):151–178,\n1998.\nIm, S., Kumar, R., Petety, A., and Purohit, M. Parsimonious learning-augmented caching. In\nICML , 2022.\nKevi, E. and Nguyen, K.-T. Online covering with multiple experts, 2023. URL https://arxiv.\norg/abs/2312.14564 .\nKomm, D., Kr´ aloviˇ c, R., Kr´ aloviˇ c, R., and M¨ omke, T. Randomized online computation with\nhigh probability guarantees. Algorithmica , 84(5):1357–1384, May 2022. ISSN 0178-4617. doi:\n10.1007/s00453-022-00925-z.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis, N. The case for learned index structures.\nInProceedings of SIGMOD’18 , pp. 489–504, 2018. doi: 10.1145/3183713.3196909.\nLattimore, T. and Szepesvari, C. Bandit algorithms. 2017. URL https://tor-lattimore.com/\ndownloads/book/book.pdf .\nLindermayr, A. and Megow, N. Permutation predictions for non-clairvoyant scheduling. In SPAA ,\npp. 357–368. ACM, 2022.\nLindermayr, A. and Megow, N. Algorithms with predictions. https:\n//algorithms-with-predictions.github.io , 2023. URL https://\nalgorithms-with-predictions.github.io . Online: accessed 2023-07-12.\nLykouris, T. and Vassilvitskii, S. Competitive caching with machine learned advice. J. ACM , 68\n(4):24:1–24:25, 2021.\nMitzenmacher, M. and Vassilvitskii, S. Algorithms with predictions. Commun. ACM , 65(7):33–35,\n2022. doi: 10.1145/3528087. URL https://doi.org/10.1145/3528087 .\nRohatgi, D. Near-optimal bounds for online caching with machine learned advice. In SODA , 2020.\nRouyer, C., Seldin, Y., and Cesa-Bianchi, N. An algorithm for stochastic and adversarial bandits\nwith switching costs. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International\nConference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp.\n9127–9135. PMLR, 18–24 Jul 2021.\nSadek, K. A. A. and Eli´ aˇ s, M. Algorithms for caching and MTS with reduced number of predictions.\nInThe Twelfth International Conference on Learning Representations , 2024.\nSlivkins, A. Introduction to multi-armed bandits. CoRR , abs/1904.07272, 2019. URL http:\n//arxiv.org/abs/1904.07272 .\nWang, S., Li, J., and Wang, S. Online algorithms for multi-shop ski rental with machine learned\nadvice. In NeurIPS , 2020.\nWei, A. Better and simpler learning-augmented online caching. In APPROX/RANDOM , 2020.\n21\n\nA Omitted Proofs from Section 3\nWe compare the cost of Algorithm 2 to the cost of a hypothetical algorithm A′which, at each time\nstept, is located at state sit\nt. At time step t, this algorithm pays cost C′\nt=ct(sit\nt) +d(sit−1\nt−1, sit\nt).\nAs a proxy for the cost of Algorithm 2, we define Ctin the following way. If st=sit\nt, we set\nCt:=C′\nt. Otherwise, we set\nCt:=d(sit−1\nt−1, st) +ct(st) +d(st, bt) +d(bt, sit\nt).\nThis way, the total cost of Algorithm 2 is at mostPT\nt=1Ct, since the cost of its movement\nd(bt, st+1)≤d(bt, sit\nt) +d(sit\nt, st+1) at time t+ 1 is split into CtandCt+1.\nThe following statement is part of the proof of Lemma 5.4 in Antoniadis et al. (2023a), we\ninclude it here for completeness.\nProposition A.1 (Antoniadis et al. (2023a)) .Consider t∈ {1, . . . , T }and denote τ≤tthe last\nstep such that sτ=siττ. Then Ct≤C′\nt+O(1)Pt\nt′=τ+1C′\nt′.\nProof. Ifτ=t, i.e., st=sit\nt, then we have Ct=C′\ntand the sumPt\nt′=τ+1C′\nt′is empty. Otherwise,\nτ < t and the following relations hold due to bt=bτ=siττ, triangle inequality, and the greedy\nchoice of stin Algorithm 2 respectively.\nd(bt, sit\nt) =d(bτ, sit\nt)\nd(sit−1\nt−1, st)≤d(sit−1\nt−1, bt) +d(bt, st)\nd(bt, st) +ct(st)≤d(bt, sit\nt) +ct(sit\nt).\nPlugging this in the definition of Ct, we get\nCt=d(sit−1\nt−1, st) +ct(st) +d(st, bt) +d(bt, sit\nt)\n≤d(bt, sit−1\nt−1) +d(bt, st) +ct(st) +d(st, bt) +d(bt, sit\nt)\n≤d(bt, sit−1\nt−1) + 2\u0000\nd(bt, st) +ct(st)\u0001\n+d(bt, sit\nt)\n≤[d(bt, sit−1\nt−1)] + 2[ d(bt, sit\nt) +ct(sit\nt)] + [d(bt, sit\nt)].\nBetween time steps τandt,A′has to traverse from bt=siττtosit−1\nt−1as well as sit\ntand pay ct(sit\nt).\nTherefore, each bracket in the equation above is bounded byPt\nt′=τ+1C′\nt′.\nProof of Lemma 3.1. By Proposition A.1, the cost of Algorithm 2 is at most\nTX\nt=1Ct≤TX\nt=1C′\nt+O(1)TX\nt=1tX\nτt+1C′\nt=TX\nt=1C′\nt+O(1)TX\nt=1(at−t)C′\nt,\nwhere τt:=max{t′|t≤tandst′=sit′\nt′}andat:=min{t′|t≥tandst′=sit′\nt′}. Therefore, it is\nenough to show that\nE\"TX\nt=1(at−t)C′\nt#\n≤O(ϵm2)E\"TX\nt=1C′\nt#\n.\nIfat> t, then st̸=sit\ntand there must be an exploration step within mtime steps before or after t.\nTherefore, we have\nE[(at−t)C′\nt]≤t+mX\nt′=t−mE[C′\ntEt′(at−t)]\n≤t+mX\nt′=t−mE[C′\ntE[Et′(at−t)| Ft]].\n22\n\nHere, we used that itand therefore C′\ntis determined only by random choices up to time t. Now,\nfor each t′=t−m, . . . , t +m, we have\nE[Et′(at−t)| Ft] =E[βt′−mXt′−m(at−t)| Ft]≤βt′−m(1 +TX\ni=1mi(2mϵ)i)≤mβt′−mO(1),\nbecause in order to have at−t=mi, we need to have βt′′= 1 in at least once in every block of\n2mtime steps between ttot+miandϵ <1/2. Therefore, we have\nE\"TX\nt=1(at−t)C′\nt#\n≤O(ϵm2)E\"TX\nt=1C′\nt#\nwhich concludes the proof.\nB Rounding Algorithm for MTS\nWe describe a procedure for rounding the solutions produced by fractional algorithms inspired by\nBlum & Burch (2000) and use it to prove Proposition 2.1 and Proposition 2.2.\nFor two distributions p, p′∈∆N,EMD (p, p′) =PN\ni=1max(0, p(i)−p′(i)). For every i, j∈[N],\nletτi,j≥0 represent the mass transferred from p(i) top′(j) such that p(i) =PN\nj=1τi,jand\nEMD (p, p′) =P\ni̸=jτi,j. Suppose a fractional algorithm chose i∈[N] at the previous time step\nwith the associated distribution p. At the next time step (with a new associated distribution p′), the\nalgorithm moves to i′∈[N] with probability τi,i′/p(i). This procedure is summarized in Algorithm\n4 which we also refer to as Round . Note that Algorithm 4 can be applied to distributions over the\nstates of a metric space (i.e., N=M) as well as distributions over heuristics/arms (i.e., N=ℓ).\nWe will not mention Nexplicitly when calling the procedure as a sub-routine for conciseness.\nAlgorithm 4: Round\n1Input: i, p, p′, N\n2\n3forj= 1, . . . , N do\n4 τi,j:= mass transferred from p(i) top′(j)\n5 setq(j) :=τi,j\np(i)\n6sample i′∼q\nProof of Property 2.1. Given p1, . . . , p T∈∆M, let s1, . . . , s Tbe the solutions produced after\niteratively calling Algorithm 4, i.e. st:=Round( st−1, pt1, pt)for every t∈[T]. Then for every\nt∈[T] we have\nE[ct(st) +d(st, st−1)] =|M|X\ni=1\nct(i)pt(i) +|M|X\nj=1,j̸=iτi,j\n=cT\ntpt+ EMD( pt−1, pt),\nfrom which we obtain the desired conclusion by summing over t= 1, . . . T .\nProof of Property 2.2. Given x1, . . . , x T∈∆ℓ, let i1, . . . , i Tbe the solutions produced after it-\neratively calling Algorithm 4, i.e. it:=Round( it−1, xt1, xt)for every t∈[T] and i0is selected\nuniformly at random. We subsequently define sit\ntas the state predicted by heuristic Hitat time t.\nIt follows that for every t∈[T] we have\nE[ct(sit\nt) +d(sit\nt, sit−1\nt−1)]≤fT\ntxt+D·EMD( xt, xt−1) =fT\ntxt+D\n2∥xt−1−xt∥1,\n23\n\nwhere we argued as in the proof of Property 2.1 and used the fact that the distance between any\ntwo states is bounded by the diameter D. By summing over t= 1, . . . T we obtain the desired\nconclusion.\nC Online Learning from Expert Advice\nC.1 Classical Algorithms\nIn this setting, a learner plays an iterative game against an oblivious adversary for Trounds. At\neach round t, the algorithm chooses among ℓexperts , incurs the cost associated to its choice, and\nthen observes the losses of all the experts at time t. In this full-feedback model, several successful\nalgorithms have been proposed and demonstrated to achieve strong performance guarantees. For\nan in-depth analysis of these classical algorithms, we invite the reader to see (Cesa-Bianchi &\nLugosi, 2006; Blum & Burch, 2000).\nHEDGE. The idea behind this algorithm is simple: associate to each expert a weight and then\nuse an exponential update rule for the weights after observing a new loss function. The weights are\nnormalized to produce a probability distribution over the expert set, from which the subsequent\naction is sampled. The learning dynamics are summarized in Algorithm 5.\nAlgorithm 5: HEDGE\n1Input: η, ℓ, T\n2w0(i) := 1 ∀i∈[ℓ]\n3fort= 1, . . . , T do\n4 Wt=P\ni∈[ℓ]wt(i)\n5 xt(i) =wt(i)/Wt∀i∈[ℓ]\n6 Play it∼xt\n7 Observe gt∈[0,1]ℓ\n8 Setwt+1(i) :=wt(i) exp(−η·gt−1(i))∀i∈[ℓ]\nSHARE. This algorithm starts from the same exponential update rule as HEDGE, but introduces\nan additional term. Given the reduction in the sum of the weights ∆, the SHARE algorithm adds\nto each weight a fixed fraction α·∆. The effect of this change is that information is shared across\nexperts, which makes the algorithm better at tracking the best moving expert. This allows SHARE\nto achieve good performance with respect to a benchmark that is a allowed to switch experts a\nlimited number of time. Algorithm 6 summarizes these dynamics.\nAlgorithm 6: SHARE\n1Input: η, α, ℓ, T\n2w0(i) := 1 ∀i∈[ℓ]\n3fort= 1, . . . , T do\n4 Wt=P\ni∈[ℓ]wt(i)\n5 xt(i) =wt(i)/Wt∀i∈[ℓ]\n6 Play it∼xt\n7 Observe gt∈[0,1]ℓ\n8 Compute ∆ :=P\ni∈[ℓ]wt(i)(1−exp(−η·gt(i)))\n9 Setwt+1(i) :=wt(i) exp(−η·gt−1(i)) +α·∆∀i∈[ℓ]\n24\n\nC.2 Proof of Property 2.3\nWe consider the weight vector wt∈[0,∞)ℓassociated to each expert and the loss vector gt−1∈[0,1]ℓ.\nThe distribution over the experts xt−1∈[0,1]ℓis obtained as xt−1(i) =wt−1(i)/Wt−1, where\nWt−1=Pℓ\nj=1wt−1(j).\nThe HEDGE algorithm with parameter η >0 uses the following update rule\nwt(i) :=wt−1(i)·exp (−η·gt−1(i))∀i∈[ℓ]. (5)\nThe SHARE algorithm with parameters η >0 and α∈[0,1/2] uses the following update rule\nwt(i) :=wt−1(i)·exp (−η·gt−1(i)) +α·∆/ℓ∀i∈[ℓ] (6)\nwhere ∆ :=Pl\nj=1(wt−1(j)−exp (−η·gt−1(j))wt−1(j)).\nThe following statement is part of the proof of Theorem 10 in Blum & Burch (2000), which we\ninclude here for completeness.\nLemma C.1. The SHARE algorithm satisfies the following\nX\ni:xt(i)<xt−1(i)(xt−1(i)−xt(i))≤X\ni:xt(i)<xt−1(i)xt−1(i)(1−exp (−η·gt−1(i))) (7)\nfor every t∈[T].\nProof. Using the update rule in (6) we have\nX\ni:xt(i)<xt−1(i)(xt−1(i)−xt(i)) =X\ni:xt(i)<xt−1(i)\u0012wt−1\nWt−1−wt−1·exp (−η·gt−1(i)) +α·∆/ℓ\nWt\u0013\n≤X\ni:xt(i)<xt−1(i)\u0012wt−1\nWt−1−wt−1·exp (−η·gt−1(i))\nWt\u0013\n≤X\ni:xt(i)<xt−1(i)\u0012wt−1\nWt−1−wt−1·exp (−η·gt−1(i))\nWt−1\u0013\nwhere the last inequality uses the observation that Wt≤Wt−1for all t∈[T].\nProof of Property 2.3. The proof for HEDGE follows immediately from Theorem 3 in Blum &\nBurch (2000). The result for SHARE is obtained by applying C.1, followed by the previous\nargument, since the right-hand side of (7) contains the update rule in (5). As a consequence, for\nboth HEDGE and SHARE, η(i.e., the learning rate parameter) directly appears in the expression\nof Property 2.3.\nD Estimating the Baseline Online\nThe bounds obtained in Theorem 3.9 and Theorem 5.3 require tuning parameters based on the\nvalues of OPT ≤0andOPT ≤k, respectively. These values are typically unknown a priori and we\nneed to estimate them online. In particular, our algorithm can observe the MTS input instance\nand, at each time t, compute the value of the offline optimal solution up to time t. Let OFF\ndenote the cost of the offline optimal solution to the given input instance. If at least one of the\nheuristics H1, . . . , H ℓachieves cost at most OFF, then OFF≤OPT ≤k≤ROFF. We use OFF as\nan estimate of OPT ≤kin order to tune the parameters of our algorithm and achieve regret bound\ndepending on R. Note that we do not need to know Rbeforehand and we can ensure that Ris\nbounded by including some classical online algortihm which is competitive in the worst case among\nH1, . . . , H ℓ.\n25\n\nWe use the guess and double trick which resembles the classical problem of estimating the time\nhorizon Tin a MAB setting, see Cesa-Bianchi & Lugosi (2006); Lattimore & Szepesvari (2017).\nHere, one starts with a prior estimate of the time horizon and an instance of some online algorithm\nwhose parameters are tuned based on this estimate. Whenever the estimate becomes smaller than\nthe index of the current iteration, the guess is doubled, a new instance of the online algorithm is\ncreated, and its parameters are set according to the new guess. In what follows, we show how to\nadapt this strategy to our setting. We focus on OPT ≤0, but a similar argument can be made for\nOPT ≤k.\nLetALG(ω) be Algorithm 2 configured with parameters ϵ:= (Dℓlnℓ)1/3m−4/3ω−1/3and\nγ:= (Dℓlnℓ)1/3m2/3ω−1/3. Here D, ℓ, m are known and ωis our estimate of the unknown\nOPT ≤0. Our strategy is to instantiate a sequence of algorithms ALG 1,ALG 2, . . . , ALG Nsuch that\nALG i=ALG(2iω) runs between rounds ai(included) and bi(excluded). The optimal heuristic\non interval ihas cost OPT i≤2iωby construction. Let OFF ibe the offline MTS optimal cost\ncomputed on the interval between aiandbi. Note that we can always compute this value at time t\nafter observing the sequence of local cost functions cai. . . , c bi−1. Suppose one of the heuristics\nisR-competitive with high probability w.r.t. the offline MTS optimum. Then it must be that\n2iω≤ROFF iwith high probability. This effectively provides a threshold for switching to the next\ninstance without knowledge of the true value of OPT i. It remains to show that this procedure,\nsummarized in Algorithm 7, guarantees a limited overhead w.r.t. to the bound that requires\nknowledge of OPT ≤0.\nAlgorithm 7: Doubling algorithm\n1Input: ω\n2\n3i= 0\n4t= 1\n5ALG 0:= ALG( ω)\n6while t≤Tdo\n7 ifOFF i> R·2iωthen\n8 i+ +\n9 ALG i:= ALG(2iω)\n10 Choose staccording to ALG i\n11 t+ +\nProposition D.1. LetD, ℓ, m be fixed and assume there exists ¯H∈ {H1, . . . , H ℓ}such that ¯His\nR-competitive with high probability. Then Algorithm 7 achieves regret O(R2/3OPT2/3\n≤0)with respect\ntoOPT ≤0.\nProof. By construction of the sequence of algorithms, we have that 2iω≤OPT iandPN\ni=1OPT i≤\nOPT ≤0. It follows that\nNX\ni=12iω=ω(2N−1)≤OPT≤0,\nwhich implies that N≤log(OPT ≤0). Moreover, we have\nReg = E[ALG] −OPT ≤0=NX\ni=0(E[ALG i]−OPT i)\n≤NX\ni=0O(OPT2/3\ni)≤O NX\ni=0(R·2i+2ω)2/3!\n≤O\u0010\nR2/3OPT2/3\n≤0\u0011\n,\n26\n\nwhere the first inequality comes from Theorem 1.1 and the second from the assumption in the\nhypothesis.\nThe previous result means that we can guarantee regret which is worse by a factor of R2/3w.r.t.\nto the bound which requires knowledge of OPT ≤0.Rdepends on the MTS variant being solved.\nFor general MTS, we can assume R≤2n−1 by including the classical deterministic algorithm\nby Borodin et al. (1992), or R=O(log2n) by including the algorithm by Bubeck et al. (2019).\nUsing the procedure of Komm et al. (2022), any algorithm for MTS which is R-competitive in\nexpectation can be made (1 + α)R-competitive with high probability for any α >0.\n27",
  "textLength": 72114
}