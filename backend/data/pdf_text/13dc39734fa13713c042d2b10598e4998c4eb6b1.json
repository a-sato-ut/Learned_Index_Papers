{
  "paperId": "13dc39734fa13713c042d2b10598e4998c4eb6b1",
  "title": "PolyFit: Polynomial-based Indexing Approach for Fast Approximate Range Aggregate Queries",
  "pdfPath": "13dc39734fa13713c042d2b10598e4998c4eb6b1.pdf",
  "text": "PolyFit: Polynomial-based Indexing Approach for Fast\nApproximate Range Aggregate Queriesâˆ—\nZhe Li1, Tsz Nam Chan2, Man Lung Yiu1, Christian S. Jensen3\nHong Kong Polytechnic University1, Hong Kong Baptist University2, Aalborg University3\nrichie.li@connect.polyu.hk,edisonchan@comp.hkbu.edu.hk,csmlyiu@comp.polyu.edu.hk,csj@cs.aau.dk\nABSTRACT\nRange aggregate queries find frequent application in data ana-\nlytics. In many use cases, approximate results are preferred over\naccurate results if they can be computed rapidly and satisfy ap-\nproximation guarantees. Inspired by a recent indexing approach,\nwe provide means of representing a discrete point dataset by\ncontinuous functions that can then serve as compact index struc-\ntures. More specifically, we develop a polynomial-based indexing\napproach, called PolyFit , for processing approximate range ag-\ngregate queries. PolyFit is capable of supporting multiple types\nof range aggregate queries, including COUNT, SUM, MIN and\nMAX aggregates, with guaranteed absolute and relative error\nbounds. Experimental results show that PolyFit is faster and more\naccurate and compact than existing learned index structures.\nACM Reference Format:\nZhe Li1, Tsz Nam Chan2, Man Lung Yiu1, Christian S. Jensen3. 2021.\nPolyFit: Polynomial-based Indexing Approach for Fast Approximate\nRange Aggregate Queries. In Proceedings of 24th International Conference\non Extending Database Technology (EDBT), Cyprus, Nicosia (EDBT2021),\n14 pages.\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n1 INTRODUCTION\nArange aggregate query [38] retrieves records in a dataset that be-\nlong to a given key range and then applies an aggregate function\n(e.g., SUM,COUNT ,MIN,MAX) to an attribute of those records. Range\naggregate queries are used in OLAP [ 38,69] and data analytics\napplications, e.g., for outlier detection [ 72,74], data visualization\n[24], and tweet analysis [ 54]. For example, network intrusion\ndetection systems [ 74] utilize range COUNT queries to monitor a\nnetwork for anomalous activities. Furthermore, applications with\nhuge numbers of users are expected to receive queries frequently.\nFor instance, Foursquare, with more than 50 million monthly\nactive users [ 4], helps users find the number of specific POIs\n(e.g., restaurants) within given regions [ 3]. In many application\nscenarios, users accept approximate results provided that (i) they\ncan be computed quickly and (ii) they are sufficiently accurate\n(e.g., within 5% error). We target such applications and focus on\nerror-bounded evaluation of range aggregate queries.\nA recent indexing approach represents the values of attributes\nin a dataset by continuous functions, which then serve to enable\ncompact index structures [ 28,44]. When compared to traditional\nindex structures, this approach is able to yield a smaller index\nsize and faster response time. The existing studies [ 28,44] focus\non computing exact results for point and range queries on 1-\ndimensional data. In contrast, we conduct a comprehensive\nstudy of approximate range aggregate queries, supporting\nmany aggregate functions and multi-dimensional data .\nâˆ—This work was supported by grant GRF 152050/19E from the Hong Kong RGC.\nÂ©2021 Association for Computing Machinery. Published in Proceedings of the\n24th International Conference on Extending Database Technology (EDBT), Nicosia,\nISBN 978-x-xxxx-xxxx-x/YY/MM on OpenProceedings.org.\nDistribution of this paper is permitted under the terms of the Creative Commons\nlicense CC-by-nc-nd 4.0.The idea that underlies our proposal for using functions to\nanswer approximate range aggregate queries may be explained\nas follows. Consider a stock market index (e.g., the Hong Kong\nHang Seng Index) at different times as a dataset Dconsisting of\nrecords of the form (index value, timestamp), where the former is\nour measure and the latter is our key that is used for specifying\nquery rangesâ€”see Figure 1(a). A user can find the average stock\nmarket index value in a specified time range [ğ‘™ğ‘,ğ‘¢ğ‘]by issuing\na range SUMquery (and divide by ğ‘¢ğ‘âˆ’ğ‘™ğ‘+1). We propose to\nconstruct the cumulative function of Das shown in Figure 1(b).\nIf we can approximate this function well by a polynomial function\nP(ğ‘¥)then the range SUMquery can be approximated as P(ğ‘¢ğ‘)âˆ’\nP(ğ‘™ğ‘), which takes ğ‘‚(1)time. As another example, the user may\nwish to find the maximum stock market index in a specified\ntime range. The timestamped index values in Dcan be modeled\nby the continuous function shown in Figure 1(c). Again, if we\ncan approximate this function well using a polynomial function\nP(ğ‘¥)then the range MAXquery can be answered quickly using\nmathematical tools, e.g., by applying differentiation to identify\nmaxima in P(ğ‘¥).\nRegarding the two-dimensional case, consider the dataset of\ntweetsâ€™ locations as shown in Figure 9(a) in Section 6, where\neach data point has a longitude (as key 1) and a latitude (as key\n2). Suppose that the user wishes to count the number of tweets\nin a geographical region. Our idea is to derive the cumulative\ncount function shown in Figure 9(b), and then approximate this\nfunction with a polynomial function P(ğ‘¥1,ğ‘¥2)(of two variables).\nThis enables us to answer a two-dimensional range COUNT query\ninğ‘‚(1)time.\nAnother difference between our work and existing studies [ 28,\n44] is the types of functions used for approximation. Our proposal\nuses piecewise polynomial functions, rather than piecewise linear\nfunctions [ 28,44]. As we will show in Section 4, using polynomial\nfunctions yields lower fitting errors than using linear functions.\nThus, our proposal leads to smaller index sizes and faster queries.\nThe key technical challenges are as follows. (1) How to find\npolynomial functions with low approximation error efficiently?\n(2) How to answer range aggregate queries with error guarantees?\n(3) How to support common aggregate functions (e.g., COUNT ,SUM,\nMIN,MAX) and multi-dimensional data?\nTo tackle these challenges, we develop a polynomial-based\nindexing approach ( PolyFit ) for processing approximate range\naggregate queries. Our contributions are summarized as follows.\nâ€¢To the best of our knowledge, this is the first study that\nutilizes polynomial functions to learn indexes that support\napproximate range aggregate queries.\nâ€¢PolyFit supports multiple types of range aggregate queries,\nincluding COUNT ,SUM,MINandMAXwith guaranteed deter-\nministic absolute and relative error bounds.\nâ€¢Experiment results show that PolyFit achieves significant\nspeedups, compared with the closest related works [ 28,44],\nand traditional exact/approximate methods. For instance,arXiv:2003.08031v7  [cs.DB]  10 Feb 2021\n\nEDBT2021, Nicosia, Cyprus Zhe Li1, Tsz Nam Chan2, Man Lung Yiu1, Christian S. Jensen3\n0 200000 400000 600000 800000\ntime (key)26000280003000032000HKG.Index (measure)Data\n(a) timestamped index values\n0 200000 400000 600000 800000\ntime (key)05000001000000150000020000002500000cumulative SUM (measure)DFsum(k)\nP(k) (b) function for range SUM queries\n0 200000 400000 600000 800000\ntime (key)26000280003000032000HKG.Index (measure)DFmax(k)\nP(k) (c) function for range MAX queries\nFigure 1: Stock market index values, 1-dimensional keys: discrete data points vs. continuous function\nfor the OpenStreetMap dataset with 100M records, our in-\ndex occupies only 4 MBytes and offers 5 ğœ‡ğ‘ query response\ntime (per 2-dimensional range COUNT query).\nThe rest of the paper is organized as follows. We first review\nthe related work in Section 2. Next, we introduce preliminaries in\nSection 3. Then, we present our index construction techniques in\nSection 4 and cover how to answer approximate range aggregate\nqueries in Section 5. Next, we extend our proposal to datasets\nwith two keys in Section 6. Lastly, we present experiments in\nSection 7 and conclude in Section 8.\n2 RELATED WORK\nRange aggregate queries are used frequently in analytics applica-\ntions and constitute important functionality in OLAP and data\nwarehousing [ 11,12,20,23,38,40,56,69]. Exact solutions are\nbased on prefix-sum arrays [ 38] or aggregate R-trees [ 59]. Due\nto the need for real-time performance in some applications (e.g.,\nğœ‡ğ‘ -level response time [ 74]), many proposals exist that aim to im-\nprove the efficiency of range aggregate queries. These proposals\ncan be classified as being either data-driven or query-driven. In\naddition, we also review some other studies, including learned\nindexes, and time series databases, which are also related to this\nwork.\nData-driven proposals build statistical models of a dataset\nfor estimating query selectivity or the results of range aggregate\nqueries. These models employ multi-dimensional histograms [ 39,\n52,57,68], data sampling [ 9,34,36,51,60,65], or kernel density\nestimation [ 32,33,37]. Although such proposals that compute\napproximate results are much faster than exact solutions, e.g.,\nachieving ms ( 10âˆ’3) level response time [ 61], they still do not of-\nfer real-time performance (e.g., ğœ‡ğ‘ level [ 74]). Furthermore, these\nproposals do not offer theoretical approximation error guaran-\ntees.\nThequery-driven approaches utilize query workloads to\nbuild statistical models of datasets. Typical methods include error-\nfeedback histograms [ 8,10,49], max-entropy histograms [ 55,\n64], and learning-based models [ 53,66]. In addition, Park et al.\n[61] explore the approach of using mixture probabilistic models.\nThese methods assume that new queries follow historical query\nworkload distributions. However, as one study [ 13] observes, this\nassumption may not always hold in practice. Further, even when\nthis assumption is valid, the number of queries that are similar to\nthose used for training may be much smaller if the queries follow\na power law distribution [ 73], which can cause poor accuracy\nand may render it impossible to obtain useful approximation\nerror guarantees for range aggregate queries.\nRecently, learning-based methods have been used to con-\nstruct more compact and effective index structure, that hold\npotential to accelerate database operations. Kraska et al. [ 44]propose the RMI index, which incorporates different machine\nlearning models, e.g., linear regression and deep-learning, to\nimprove the efficiency of range queries. Galakatos et al. [ 28]\ndevelop the FITing-tree, which is a segment-tree-like structure\n[22,71] that can significantly improve the efficiency of exact\npoint queries. Ferragina et al. [ 27] further support efficient up-\ndate operations for range queries. Wang et al. [ 70] extend this\nlearning-based approach to the spatial domain with their learned\nZ-order model that aims to support fast spatial indexing. How-\never, there are two main differences between these proposals and\nour proposal. First, they either support range queries [ 27,44,70]\nor point queries [ 28], but not range aggregate queries. Second,\nwe are the first to exploit polynomial functions to build index\nstructures for approximate range aggregate queries.\nIn the time series database community, some research stud-\nies utilize mathematical models to approximate time series data.\nRepresentative approaches include piecewise linear approxima-\ntion [ 25,41â€“43,50], discrete wavelet transform [ 15,62], discrete\nFourier transform [ 26,63], and their combinations [ 40,58]. How-\never, these studies focus on either time series similarity search\n(e.g., range or nearest neighbor queries) or time series compres-\nsion and they are not designed to answer the range aggregate\nqueries we target. Some of these studies also utilize piecewise\nlinear approximation [ 25,41,42,50] to approximate time-series,\nwhich we also do. In contrast, we achieve better performance\nby utilizing nonlinear (polynomial) functions to approximate\ncurves, which can reduce the number of segments dramatically.\nFurthermore, we can also support the segmentation of surfaces\n(e.g., Figure 9(b)), rather than only 1-D curves.\n3 PRELIMINARIES\nFirst, we define range aggregate queries and their approximate\nversions in Section 3.1. Then, we discuss the baselines for an-\nswering exact range aggregate queries in Section 3.2. Table 1\nsummarizes frequently used symbols in this paper.\nTable 1: Symbols\nSymbol Description\nD dataset\nğ‘› number of records in D\nğ‘…ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ range COUNT query\nğ‘…ğ‘ ğ‘¢ğ‘š range SUMquery\nğ‘…ğ‘šğ‘–ğ‘› range MINquery\nğ‘…ğ‘šğ‘ğ‘¥ range MAXquery\nğ¶ğ¹ğ‘ ğ‘¢ğ‘š cumulative function for range SUMquery\nğ·ğ¹ğ‘šğ‘ğ‘¥ key-measure function for range MAXquery\nP(ğ‘˜) polynomial function\nğ¼ interval\nğ‘‘ğ‘’ğ‘” degree of polynomial function\n\nPolyFit: Polynomial-based Indexing Approach for Fast Approximate Range Aggregate Queries EDBT2021, Nicosia, Cyprus\n3.1 Problem Definition\nWe focus on the setting that a range aggregate query specifies\nağ‘˜ğ‘’ğ‘¦attribute (for range selection) and a ğ‘šğ‘’ğ‘ğ‘ ğ‘¢ğ‘Ÿğ‘’ attribute for\naggregation. We shall consider the setting of two keys in Sec-\ntion 6. As such, the dataset Dis a set of(ğ‘˜ğ‘’ğ‘¦,ğ‘šğ‘’ğ‘ğ‘ ğ‘¢ğ‘Ÿğ‘’)records,\ni.e.,D={(ğ‘˜1,ğ‘š1),(ğ‘˜2,ğ‘š2),...,(ğ‘˜ğ‘›,ğ‘šğ‘›)}. For ease of discussion,\nwe assume that key values are distinct and measure values are\nnumerical. We leave the discussion of repeated keys and negative\nmeasure values in Appendices A.3 and A.4 [ 48]. Then we define\na range aggregate query as follows.\nDefinition 3.1. LetGbe an aggregate function (e.g., COUNT ,\nSUM,MIN,MAX) on a measure attribute. Given a dataset Dand a\nkey range[ğ‘™ğ‘,ğ‘¢ğ‘], we defineğ‘‰as the following multi-set\nğ‘‰={ğ‘š|(ğ‘˜,ğ‘š)âˆˆDâˆ§ğ‘™ğ‘â‰¤ğ‘˜â‰¤ğ‘¢ğ‘}\nand then define the result of the range aggregate query as\nğ‘…G(D,[ğ‘™ğ‘,ğ‘¢ğ‘])=G(ğ‘‰). (1)\nWe aim to develop efficient methods for obtaining an approx-\nimate result of ğ‘…G(D,[ğ‘™ğ‘,ğ‘¢ğ‘])with two types of error guaran-\ntees [ 29,30], namely the absolute error guarantee (cf. Problem 1)\nand the relative error guarantee (cf. Problem 2).\nProblem 1 ( ğ‘„ğ‘ğ‘ğ‘ ).Given an absolute error ğœ€ğ‘ğ‘ğ‘ and a range\naggregate query, we ask for an approximate result ğ´ğ‘ğ‘ğ‘ such that:\n|ğ´ğ‘ğ‘ğ‘ âˆ’ğ‘…G(D,[ğ‘™ğ‘,ğ‘¢ğ‘])|â‰¤ğœ€ğ‘ğ‘ğ‘  (2)\nProblem 2 ( ğ‘„ğ‘Ÿğ‘’ğ‘™).Given a relative error ğœ€ğ‘Ÿğ‘’ğ‘™and a range ag-\ngregate query, we ask for an approximate result ğ´ğ‘Ÿğ‘’ğ‘™such that:\n\f\f\f\fğ´ğ‘Ÿğ‘’ğ‘™âˆ’ğ‘…G(D,[ğ‘™ğ‘,ğ‘¢ğ‘])\nğ‘…G(D,[ğ‘™ğ‘,ğ‘¢ğ‘])\f\f\f\fâ‰¤ğœ€ğ‘Ÿğ‘’ğ‘™ (3)\n3.2 Baselines: Exact Methods\nWe proceed to discuss exact methods for answering range SUM\nqueries and range MAXqueries. These methods can be easily\nextended to support COUNT andMIN, respectively.\n3.2.1 Exact method for range SUMqueries .First, we de-\nfine the key cumulative function as ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘˜):\nğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘˜)=ğ‘…ğ‘ ğ‘¢ğ‘š(D,[âˆ’âˆ,ğ‘˜]). (4)\nThe additive property of ğ¶ğ¹ğ‘ ğ‘¢ğ‘š enables us to compute the\nexact result of the range SUMquery as:\nğ‘…ğ‘ ğ‘¢ğ‘š(D,[ğ‘™ğ‘,ğ‘¢ğ‘])=ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘¢ğ‘)âˆ’ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘™ğ‘). (5)\nThen, we discuss how to obtain the terms ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘™ğ‘)and\nğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘¢ğ‘)efficiently. Although ğ¶ğ¹ğ‘ ğ‘¢ğ‘šis a continuous function,\nit can be expressed by a discrete data structure in finite space.\nSpecifically, we presort dataset Din ascending key order and\nthen follow this order to construct a key-cumulative array of\nentries(ğ‘˜,ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘˜)). At query time, the terms ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘™ğ‘)and\nğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘¢ğ‘)are obtained by performing binary search on the\nabove key-cumulative array. This step takes ğ‘‚(logğ‘›)time.\nAs a remark, this key-cumulative array is similar to the prefix-\nsum array [ 38]. The difference is that our array allows floating-\npoint search keys, while the prefix-sum array does not.\n3.2.2 Exact method for range MAXqueries .First, we de-\nfine the key-measure function ğ·ğ¹ğ‘šğ‘ğ‘¥(ğ‘˜)in Equation 6 to capture\nthe data distribution in the dataset D. In the definition, we as-\nsume that each pair (ğ‘˜ğ‘–,ğ‘šğ‘–)inDis arranged in ascending order\nby the key.ğ·ğ¹ğ‘šğ‘ğ‘¥(ğ‘˜)=ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ ï£²\nï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³ğ‘š1 ifğ‘˜1â‰¤ğ‘˜<ğ‘˜2\n......\nğ‘šğ‘– ifğ‘˜ğ‘–â‰¤ğ‘˜<ğ‘˜ğ‘–+1\n......\nğ‘šğ‘› ifğ‘˜=ğ‘˜ğ‘›\nâˆ’âˆ otherwise(6)\nFigure 2(a) exemplifies the function ğ·ğ¹ğ‘šğ‘ğ‘¥(ğ‘˜).\nAn aggregate max-tree [ 59] (cf. Figure 2(b)) can be built to\nanswer range MAXqueries. In this tree, each internal node stores\ntwo entries, where each entry stores an interval and the max-\nimum measure within that interval (e.g., (ğ¼1,ğ‘š6)and(ğ¼2,ğ‘š7)\nare two entries of the root node ğ‘ğ‘Ÿğ‘œğ‘œğ‘¡). We then explain how\nto process the query ğ‘…ğ‘šğ‘ğ‘¥(D,[ğ‘™ğ‘,ğ‘¢ğ‘]), whose query range is\nindicated by the red line in Figure 2(a). In Figure 2(b), we start\nfrom the root of the tree. If the interval of an entry intersects\nwith the query range (e.g., ğ¼1andğ¼2in Figure 2(a)), we visit its\nchild nodes (e.g., ğ‘1andğ‘2). When the interval of an entry (e.g.,\nğ¼4andğ¼5in Figure 2a) is covered by the query range, we directly\nuse its stored aggregate value without visiting its child nodes\n(e.g., yellow nodes in Figure 2b). During the traversal, we keep\ntrack of the maximum measure seen so far. This procedure takes\nğ‘‚(logğ‘›)time as we check at most two branches per level.\nğ¼3 ğ¼4 ğ¼5ğ¼6ğ¼1 ğ¼2ğ‘«ğ‘­ğ’ğ’‚ğ’™(ğ’Œ)\nkey\nğ‘ğ‘Ÿğ‘œğ‘œğ‘¡\n(a)(key, measure) -pairs,\nred line denotes the max queryğ‘…ğ‘šğ‘ğ‘¥(ğ’Ÿ,[ğ‘™ğ‘,ğ‘¢ğ‘])ğ‘š1ğ‘š6ğ‘š12ğ‘š7\n(b) Hierarchical max -tree,\nyellow nodes are skipped during traversal2468\n010\n0 2 4 6 8 10 12 14 16â€¦â€¦â€¦â€¦ğ‘ğ‘Ÿğ‘œğ‘œğ‘¡\n(ğ¼1,ğ‘š6)(ğ¼2,ğ‘š7)\nğ‘1\n(ğ¼3,ğ‘š1)(ğ¼4,ğ‘š6)ğ‘2\n(ğ¼5,ğ‘š7)(ğ¼6,ğ‘š12)\nğ‘3\nâ€¦â€¦ğ‘4\nâ€¦â€¦ğ‘5\nâ€¦â€¦ğ‘6\nâ€¦â€¦\nFigure 2: Aggregate MAXtree\n4 INDEX CONSTRUCTION\nTraditional index structures (e.g., B-tree [ 21]) need to store ğ‘›\nkeys, where ğ‘›is the cardinality of the dataset D. Thus, the index\nsize grows linearly with the data size. To reduce the index size\ndramatically, we plan to index a limited number of functions\n(instead ofğ‘›keys).\nAs a case study, we compare existing fitting functions [ 28,\n44] with our fitting function (polynomial) on a real dataset (the\nHong Kong 40 Index in 2018 [ 5]) in Figure 3. The exact key-\nmeasure function ğ·ğ¹ğ‘šğ‘ğ‘¥(ğ‘˜)exhibits a complex shape. Observe\nthat linear functions, e.g., linear regression ğ¿ğ‘…(ğ‘˜)[44] and linear\nsegmentğ¹ğ¼ğ‘‡(ğ‘˜)[28], cannot accurately approximate the exact\nfunction. In this paper, we adopt the polynomial function P(ğ‘˜),\nwhich captures the nonlinear property1and achieves a better\napproximation of ğ·ğ¹max(ğ‘˜). In this example, P(ğ‘˜)is a degree-4\npolynomial function (blue dotted line).\nWe introduce our indexing framework in Figure 4. First, we\nconvert the dataset into the following exact function ğ¹(ğ‘˜)based\n1As a remark, other types of nonlinear functions (e.g., logarithmic and trigonometric\nfunctions) require higher computation cost than polynomial functions. Thus, we\nleave other types of nonlinear functions as future work.\n\nEDBT2021, Nicosia, Cyprus Zhe Li1, Tsz Nam Chan2, Man Lung Yiu1, Christian S. Jensen3\n0 20 40 60 80\ntime (key)250002600027000280002900030000310003200033000HKG.Index (measure)DFmax(k)\nFIT(k)\nP(k)\nLR(k)\nFigure 3: Curve fitting of the HKG 40 Index in 2018 [5]\non the aggregate function Gand the functions in Section 3.2.\nğ¹(ğ‘˜)=(\nğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘˜)ifG=SUM\nğ·ğ¹ğ‘šğ‘ğ‘¥(ğ‘˜)ifG=MAX(7)\nWe plan to compute an error-bounded approximation of ğ¹(ğ‘˜)\nby using a sequence of polynomial functions. In Section 4.1, we\nexamine how to find the best polynomial fitting of ğ¹(ğ‘˜)in a given\nkey interval ğ¼. Then, in Section 4.2, we propose a segmentation\nmethod forğ¹(ğ‘˜)in order to minimize the index size subject to a\ngiven deviation threshold. Finally, in Section 4.3, we discuss how\nto build an index for a sequence of polynomial functions.\n4.1 Polynomial Fitting in a Key Interval\nWe discuss how to find the best fitting polynomial function of\nğ¹(ğ‘˜)in a given key interval ğ¼. First, we express a polynomial\nfunction P(ğ‘˜)as follows:\nP(ğ‘˜)=ğ‘‘ğ‘’ğ‘”âˆ‘ï¸\nğ‘—=0ğ‘ğ‘—ğ‘˜ğ‘—, (8)\nwhereğ‘‘ğ‘’ğ‘”is the degree and each ğ‘ğ‘—is a coefficient. Note that\nthe choice of ğ‘‘ğ‘’ğ‘”entails tradeoffs between the fitting error and\nthe online query evaluation cost. We discuss the choice of ğ‘‘ğ‘’ğ‘”in\nSection 5.3.\nWe formulate the following optimization problem in order to\nminimize the fitting error between P(ğ‘˜)andğ¹(ğ‘˜).\nDefinition 4.1. Letğ¹(ğ‘˜)be the exact function and ğ¼be a given\nkey interval. Let ğ‘˜1,ğ‘˜2,Â·Â·Â·,ğ‘˜â„“be the keys ofDin intervalğ¼. We\naim to find polynomial coefficients, ğ‘0,ğ‘1,Â·Â·Â·,ğ‘ğ‘‘ğ‘’ğ‘”that minimize\nthe following error:\nğ¸(ğ¼)= min\nğ‘0,ğ‘1,...,ğ‘ğ‘‘ğ‘’ğ‘”âˆˆRmax\n1â‰¤ğ‘–â‰¤â„“|ğ¹(ğ‘˜ğ‘–)âˆ’P(ğ‘˜ğ‘–)| (9)\nThis is equivalent to the following linear programming prob-\nlem, where the coefficients ğ‘0,ğ‘1,Â·Â·Â·,ğ‘ğ‘‘ğ‘’ğ‘”andğ‘¡are variables.\nï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ ï£²\nï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³minimizeğ‘¡\nsubject to:\nâˆ’ğ‘¡â‰¤ğ¹(ğ‘˜1)âˆ’(ğ‘ğ‘‘ğ‘’ğ‘”ğ‘˜ğ‘‘ğ‘’ğ‘”\n1+...+ğ‘2ğ‘˜2\n1+ğ‘1ğ‘˜1+ğ‘0)â‰¤ğ‘¡\nâˆ’ğ‘¡â‰¤ğ¹(ğ‘˜2)âˆ’(ğ‘ğ‘‘ğ‘’ğ‘”ğ‘˜ğ‘‘ğ‘’ğ‘”\n2+...+ğ‘2ğ‘˜2\n2+ğ‘1ğ‘˜2+ğ‘0)â‰¤ğ‘¡\n...\nâˆ’ğ‘¡â‰¤ğ¹(ğ‘˜â„“)âˆ’(ğ‘ğ‘‘ğ‘’ğ‘”ğ‘˜ğ‘‘ğ‘’ğ‘”\nâ„“+...+ğ‘2ğ‘˜2\nâ„“+ğ‘1ğ‘˜â„“+ğ‘0)â‰¤ğ‘¡\nâˆ€ğ‘ğ‘–âˆˆR(10)\nIt takesğ‘‚(â„“2.5)time to solve the above linear programming\nproblem (Equation 10) [ 46]. In our experimental study, we adopt\nthe IBM CPLEX linear programming library as the LP Solver,\nwhich is believed to be the most reliable and efficient amongother implementations [ 31]. We discuss some subtle issues like\nprecision limitations in Section 5.3.\n4.2 Minimal Index Size with Bounded Error\nTo support approximate query evaluation (in Section 5), we re-\nquire that the fitting polynomial functions should satisfy a given\nerror constraint. However, a single polynomial function is un-\nlikely to fit accurately for the entire key domain. Thus, we pro-\npose to partition the key domain into intervals ğ¼1,ğ¼2,Â·Â·Â·,ğ¼â„so\nthat each interval ğ¼ğ‘–satisfies the following requirement:\nğ¸(ğ¼)â‰¤ğ›¿,\nwhereğ›¿is a given deviation threshold. For instance, in Figure 5,\nthe key domain is partitioned into two intervals ğ¼1andğ¼2so that\nthe best fitting polynomial function in each interval satisfies the\nerror requirement.\nTo achieve a small index size, we aim to minimize the number\nof intervals (i.e., â„in Figure 4). An existing dynamic program-\nming (DP) approach [ 47], though designed for piecewise linear\nfunctions, can be adapted to solve our partitioning problem of\nğ¹(ğ‘˜). However, this method takes ğ‘‚(ğ‘›2Ã—â„“2.5ğ‘šğ‘ğ‘¥)time2, where\nâ„“ğ‘šğ‘ğ‘¥ is the maximum number of keys covered by any interval.\nObviously, this method does not scale well with the data size ğ‘›.\nIn Section 4.2.1, we present a more efficient method, called\ngreedy segmentation (GS), to segment the exact function ğ¹(ğ‘˜). As\nwe show later, the time complexity of GS is ğ‘‚(ğ‘›Ã—â„“2.5ğ‘šğ‘ğ‘¥), which\nscales well with the data size ğ‘›. Then, in Section 4.2.2, we show\nthat GS is guaranteed to return the optimal solution.\n4.2.1 Greedy Segmentation (GS) Method .We present the\npseudo-code of the Greedy Segmentation (GS) method in Algo-\nrithm 1. It examines the key domain from left to right (line 2).\nIn each iteration, it expands the interval ğ¼by including the next\nkey (line 3), calls an LP solver on the interval ğ¼to obtain a fitting\nfunction Pğ‘›ğ‘œğ‘¤(line 4), and tests whether it fulfills the error re-\nquirement. When this test fails (i.e., ğ¸(ğ¼)>ğ›¿), we conclude that\nthe previous interval is a maximal interval and thus insert its\ncorresponding fitting function Pğ‘ğ‘Ÿğ‘’ğ‘£ into the result. The above\nprocedure is repeated until all keys are covered.\nAlgorithm 1 Greedy Segmentation (GS)\nInput: functionğ¹(ğ‘˜), degreeğ‘‘ğ‘’ğ‘”, deviation threshold ğ›¿\nOutput: sequence of polynomial functions SeqP\n1:SeqPâ†âˆ… ;ğ‘™â†1;Pğ‘ğ‘Ÿğ‘’ğ‘£â†ğ‘›ğ‘¢ğ‘™ğ‘™\n2:forğ‘¢â†2toğ‘›do\n3:ğ¼â†[ğ‘˜ğ‘™,ğ‘˜ğ‘¢] âŠ²the interval for polynomial function P\n4: Pğ‘›ğ‘œğ‘¤â†call LP solver on ğ¼ âŠ²Equation 10\n5: ifğ¸(ğ¼)>ğ›¿orğ‘¢=ğ‘›then âŠ²Equation 9\n6: insert Pğ‘ğ‘Ÿğ‘’ğ‘£ into SeqP\n7:ğ‘™â†ğ‘¢\n8: Pğ‘ğ‘Ÿğ‘’ğ‘£â†Pğ‘›ğ‘œğ‘¤\n9:return SeqP\nThe time complexity of GS is ğ‘‚(ğ‘›â„“2.5ğ‘šğ‘ğ‘¥)because it invokes\nğ‘‚(ğ‘›)calls to the LP solver, where each call takes ğ‘‚(â„“2.5ğ‘šğ‘ğ‘¥)time [ 46].\nWe further accelerate GS by applying an existing exponential\nsearch technique [14], which can reduce the number of LP calls\nper interval byâ„“\nlogâ„“times. With this technique, GS takes only 70\nseconds (cf. Section 7.2.2) to complete for a real dataset with 1\nmillion data points. This is acceptable for many data analytics\ntasks (with static datasets) in OLAP. In our experiments, we find\nthatâ„“ğ‘šğ‘ğ‘¥ usually ranges between hundreds and thousands, thus\n2Recall that the state-of-the-art linear programming solver [ 46] takesğ‘‚(â„“2.5\nğ‘šğ‘ğ‘¥)\ntime for each curve-fitting problem (cf. Equation 10).\n\nPolyFit: Polynomial-based Indexing Approach for Fast Approximate Range Aggregate Queries EDBT2021, Nicosia, Cyprus\ndatasetKey-cumulative function \nfor COUNT / SUM query\nKey-measure function \nfor MIN / MAX queryConstruction\nâ€¦\nğ¼1 ğ¼2ğ¿1ğ‘ˆ11stpolynomial 2ndpolynomial hthpolynomialindex\nPolyFit(ğ‘˜,ğ‘š)\nPreprocessing\nğ¿2ğ‘ˆ2\nğ¼â„ğ¿â„ğ‘ˆâ„\nFigure 4: Indexing framework for PolyFit , each leaf entry stores a polynomial function\nâ‰¤ğ›¿ â‰¤ğ›¿â„™à¯‚à°­(ğ‘˜)\nâ„™à¯‚à°®(ğ‘˜)\nfirst interval second interval ğ¼à¬µ ğ¼à¬¶ğ¹(ğ‘˜)\nFigure 5: Fitting ğ¹(ğ‘˜)with multiple polynomial functions,\nsubject to the deviation threshold ğ›¿\nthe termğ‘‚(â„“2.5ğ‘šğ‘ğ‘¥)is acceptable in practice. In Appendix [ 48], we\ndiscuss how to utilize parallel computation to further improve\nthe construction time.\n4.2.2 GS is Optimal .We first prove the following property\n(Lemma 4.2) of our curve fitting problem (cf. Definition 4.1).\nLemma 4.2. Letğ¼ğ‘™andğ¼ğ‘¢be two intervals, which contain two\nsets of keysğ‘†ğ‘™andğ‘†ğ‘¢, respectively. If ğ‘†ğ‘™âŠ†ğ‘†ğ‘¢, thenğ¸(ğ¼ğ‘™)â‰¤ğ¸(ğ¼ğ‘¢).\nProof. Recall that the value of ğ¸(ğ¼)(cf. Equation 9) is equal\nto the minimum value of the optimization problem (Equation 10).\nSinceğ‘†ğ‘™is a subset of ğ‘†ğ‘¢, the set of constraints for solving ğ¸(ğ¼ğ‘™)\nis also the subset of constraints for solving ğ¸(ğ¼ğ‘¢). Thus, for the\nminimization problem in Equation 10, the possible solution space\nforğ‘†ğ‘™is a superset of the possible solution space for ğ‘†ğ‘¢. Therefore,\nwe conclude that ğ¸(ğ¼ğ‘™)â‰¤ğ¸(ğ¼ğ‘¢). â–¡\nBased on Lemma 4.2, we then show that GS produces the\nfewest polynomial functions (cf. Theorem 4.3), i.e., the optimal\nsolution.\nTheorem 4.3. GS always produces the optimal number of func-\ntions (with respect to the given parameters ğ‘‘ğ‘’ğ‘”andğ›¿).\nProof. We denote the minimum key and the maximum key\nof an interval ğ¼byğ¼.minandğ¼.max, respectively.\nLetIâˆ—\nOPT=(ğ¼(1)\nOPT,ğ¼(2)\nOPT,Â·Â·Â·) andIâˆ—\nGS=(ğ¼(1)\nGS,ğ¼(2)\nGS,Â·Â·Â·) be two\nascending sequences of intervals for the optimal solution and\nour GS method, respectively (i.e., ğ¼(ğ‘–).max<ğ¼(ğ‘–+1).minforğ‘–=\n1,2,...,ğ‘›âˆ’1). Every interval ğ¼inIâˆ—\nOPTandIâˆ—\nGSmust satisfy ğ¸(ğ¼)â‰¤\nğ›¿. We now prove the theorem by mathematical induction.\nIn the base step, we consider the first interval in each sequence.\nSince both GS and OPT must cover the key domain, we have:\nğ¼(1)\nGS.min=ğ¼(1)\nOPT.min\nAccording to GS, the first interval ğ¼(1)\nGSis maximal, because a\nlonger interval would violate the deviation threshold ğ›¿. Thus, we\nhave:\nğ¼(1)\nGS.maxâ‰¥ğ¼(1)\nOPT.max (11)In the inductive step, assume that the first â„“intervals of the\ntwo sequences satisfy the following property:\nğ¼(â„“)\nGS.maxâ‰¥ğ¼(â„“)\nOPT.max (12)\nSinceIâˆ—\nOPTandIâˆ—\nGSare ascending sequences of intervals, Equa-\ntion 12 implies the following:\nğ¼(â„“+1)\nGS.minâ‰¥ğ¼(â„“+1)\nOPT.min (13)\nNow, we consider two cases for comparing ğ¼(â„“+1)\nGSandğ¼(â„“+1)\nOPT.\nCase 1:\nğ¼(â„“+1)\nGS.maxâ‰¥ğ¼(â„“+1)\nOPT.max\nIn this case, the first â„“+1intervals of GS cover all keys in the\nfirstâ„“+1intervals of OPT.\nCase 2:\nğ¼(â„“+1)\nGS.max<ğ¼(â„“+1)\nOPT.max (14)\nConsider the interval ğ¼â€²=[ğ¼(â„“+1)\nGS.min,ğ¼(â„“+1)\nOPT.max]. By using\nEquations 13 and 14, we obtain: ğ¼â€²âŠ‚ğ¼(â„“+1)\nOPT. By Lemma 4.2, we\nget:ğ¸(ğ¼â€²)â‰¤ğ¸(ğ¼(â„“+1)\nOPT). Sinceğ¸(ğ¼(â„“+1)\nOPT)â‰¤ğ›¿, we get:ğ¸(ğ¼â€²)â‰¤ğ›¿.\nObserve that ğ¼â€²has the same minimum key as ğ¼(â„“+1)\nGSbut a\nlarger maximum key than ğ¼(â„“+1)\nGS. Sinceğ¼â€²does not pass the error\ntest in GS, we get ğ¸(ğ¼â€²)>ğ›¿. This contradicts the statement\nğ¸(ğ¼â€²)â‰¤ğ›¿.\nTherefore, only the first case is true, and we have:\nğ¼(â„“+1)\nGS.maxâ‰¥ğ¼(â„“+1)\nOPT.max\nThis means GS always covers no fewer keys than OPT with\nthe same number of intervals. Thus, GS produces the optimal\nnumber of functions. â–¡\n4.3 Indexing of polynomial functions\nIn our experimental study, the number of intervals (for poly-\nnomial functions) ranges from 100 to 1000. We adopt existing\nindex structures on these intervals to support fast query eval-\nuation. Specifically, we employ an in-memory index called the\nSTX B-tree [ 6] to index intervals. In each internal node entry, we\nmaintain an additional attribute to store the aggregate value of\nits subtree. In each leaf node entry, we store an interval and its\ncorresponding polynomial model (in the form of coefficients). In\nsummary, this index is similar to the aggregate tree exemplified\nin Figure 2(b), except that we store polynomial models in leaf\nnodes.\n5 APPROXIMATE QUERY EVALUATION\nWe present our framework for answering approximate range\naggregate queries in Figure 6. The first step is to compute an\ninitial approximate result quickly by using our index ( PolyFit ).\nThen, we check whether the error condition is satisfied and refine\nthe approximate result if necessary. We discuss how to answer\nthe approximate range SUMquery and the approximate range MAX\n\nEDBT2021, Nicosia, Cyprus Zhe Li1, Tsz Nam Chan2, Man Lung Yiu1, Christian S. Jensen3\nquery in Sections 5.1 and 5.2, respectively. Finally, in Section 5.3,\nwe discuss how to tune our index parameters (e.g., ğ‘‘ğ‘’ğ‘”,ğ›¿ ) in order\nto optimize the query response time.\n5.1 Approximate range SUMQuery\nGiven the query range [ğ‘™ğ‘,ğ‘¢ğ‘], we propose to compute the ap-\nproximate result as:\nËœğ´ğ‘ ğ‘¢ğ‘š=Pğ¼ğ‘¢(ğ‘¢ğ‘)âˆ’Pğ¼ğ‘™(ğ‘™ğ‘), (15)\nwhereğ¼ğ‘™andğ¼ğ‘¢denote the intervals of Pthat contain the values\nğ‘™ğ‘andğ‘¢ğ‘, respectively.\nThen, we show the error conditions for ğ‘„ğ‘ğ‘ğ‘ (cf. Problem 1)\nandğ‘„ğ‘Ÿğ‘’ğ‘™(cf. Problem 2).\nError condition for ğ‘„ğ‘ğ‘ğ‘ \nGiven the absolute error ğœ€ğ‘ğ‘ğ‘ , we recommend to use the devia-\ntion threshold ğ›¿=ğœ€ğ‘ğ‘ğ‘ \n2in constructing PolyFit . With this setting,\nthe following lemma offers the absolute error guarantee for the\napproximate result Ëœğ´ğ‘ ğ‘¢ğ‘š(in Equation 15).\nLemma 5.1. Ifğ›¿=ğœ€ğ‘ğ‘ğ‘ \n2, then Ëœğ´ğ‘ ğ‘¢ğ‘š (in Equation 15) satisfies the\nabsolute error guarantee with respect to ğœ€ğ‘ğ‘ğ‘ .\nProof. Letğ¼ğ‘™andğ¼ğ‘¢be two intervals (in PolyFit ) which con-\ntainğ‘™ğ‘andğ‘¢ğ‘(of the query range [ğ‘™ğ‘,ğ‘¢ğ‘]), respectively. Based\non the deviation threshold guarantee in Section 4.2.2, we obtain:\n|ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘™ğ‘)âˆ’Pğ¼ğ‘™(ğ‘™ğ‘)|â‰¤ğ›¿\n|ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘¢ğ‘)âˆ’Pğ¼ğ‘¢(ğ‘¢ğ‘)|â‰¤ğ›¿\nBy combining them, we have:\nğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘¢ğ‘)âˆ’ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘™ğ‘)âˆ’2ğ›¿â‰¤Ëœğ´ğ‘ ğ‘¢ğ‘šâ‰¤ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘¢ğ‘)âˆ’ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘™ğ‘)+2ğ›¿\nBy using Equation 5, we have:\nğ‘…ğ‘ ğ‘¢ğ‘š(D,[ğ‘™ğ‘,ğ‘¢ğ‘])âˆ’2ğ›¿â‰¤Ëœğ´ğ‘ ğ‘¢ğ‘šâ‰¤ğ‘…ğ‘ ğ‘¢ğ‘š(D,[ğ‘™ğ‘,ğ‘¢ğ‘])+2ğ›¿\nSinceğ›¿=ğœ€ğ‘ğ‘ğ‘ \n2,Ëœğ´ğ‘ ğ‘¢ğ‘šsatisfies the absolute error guarantee ğœ€ğ‘ğ‘ğ‘ .\nâ–¡\nError condition for ğ‘„ğ‘Ÿğ‘’ğ‘™\nIn this scenario, there is no specific preference for setting the\ndeviation threshold ğ›¿when constructing PolyFit . The following\nlemma suggests a condition to test whether Ëœğ´ğ‘ ğ‘¢ğ‘šsatisfies the\nrelative error guarantee. If this test fails, we resort to the exact\nmethod (cf. Section 3.2.1) to obtain the exact result.\nLemma 5.2. IfËœğ´ğ‘ ğ‘¢ğ‘šâ‰¥2ğ›¿(1+1\nğœ€ğ‘Ÿğ‘’ğ‘™), then Ëœğ´ğ‘ ğ‘¢ğ‘š (in Equation 15)\nsatisfies the relative error guarantee with respect to ğœ€ğ‘Ÿğ‘’ğ‘™.\nProof. Like in the proof of Lemma 5.1, we can derive Equa-\ntions 16 and 17.\n|Ëœğ´ğ‘ ğ‘¢ğ‘šâˆ’ğ‘…ğ‘ ğ‘¢ğ‘š(D,[ğ‘™ğ‘,ğ‘¢ğ‘])|â‰¤ 2ğ›¿ (16)\nwhich also implies (by simple derivations):\nğ‘…ğ‘ ğ‘¢ğ‘š(D,[ğ‘™ğ‘,ğ‘¢ğ‘])â‰¥ Ëœğ´ğ‘ ğ‘¢ğ‘šâˆ’2ğ›¿ (17)\nSinceğ›¿andğœ€ğ‘Ÿğ‘’ğ‘™must be positive, the given condition Ëœğ´ğ‘ ğ‘¢ğ‘šâ‰¥\n2ğ›¿(1+1\nğœ€ğ‘Ÿğ‘’ğ‘™)implies that Ëœğ´ğ‘ ğ‘¢ğ‘š>2ğ›¿and2ğ›¿\nËœğ´ğ‘ ğ‘¢ğ‘šâˆ’2ğ›¿â‰¤ğœ€ğ‘Ÿğ‘’ğ‘™.\nDividing Equation 16 by Equation 17, we obtain the following\ninequality (under the condition Ëœğ´ğ‘ ğ‘¢ğ‘š>2ğ›¿).\n|Ëœğ´ğ‘ ğ‘¢ğ‘šâˆ’ğ‘…ğ‘ ğ‘¢ğ‘š(D,[ğ‘™ğ‘,ğ‘¢ğ‘])|\nğ‘…ğ‘ ğ‘¢ğ‘š(D,[ğ‘™ğ‘,ğ‘¢ğ‘])â‰¤2ğ›¿\nËœğ´ğ‘ ğ‘¢ğ‘šâˆ’2ğ›¿\nThis completes the proof because2ğ›¿\nËœğ´ğ‘ ğ‘¢ğ‘šâˆ’2ğ›¿â‰¤ğœ€ğ‘Ÿğ‘’ğ‘™.â–¡The overall query algorithm\nWe summarize the query algorithm for both types of error\nguarantees in Algorithm 2. The processing for ğ‘„ğ‘ğ‘ğ‘ is composed\nof two parts: index search ğ‘‡1(i.e., Lines 1-2) and function eval-\nuationğ‘‡2(i.e., Line 3). The processing for ğ‘„ğ‘Ÿğ‘’ğ‘™includesğ‘‡1,ğ‘‡2,\nand possible refinement ğ‘‡3(i.e., Lines 4-6). The time complex-\nity ofğ‘‡1,ğ‘‡2, andğ‘‡3areğ‘‚(log(|SeqP|)),ğ‘‚(ğ‘‘ğ‘’ğ‘”), andğ‘‚(log|D|)\nrespectively.\nAlgorithm 2 Query Processing for SUM (or COUNT)\nInput: SeqP(output from Algorithm 1), ğ‘™ğ‘,ğ‘¢ğ‘,D,ğ›¿,ğ‘„ğ‘¡ğ‘¦ğ‘ğ‘’\nOutput: Approximate query result ğ´\n1:Pğ¼ğ‘™â†index search Pfrom SeqPthat includes ğ‘™ğ‘\n2:Pğ¼ğ‘¢â†index search Pfrom SeqPthat includes ğ‘¢ğ‘\n3:Ëœğ´ğ‘ ğ‘¢ğ‘šâ†Pğ¼ğ‘¢(ğ‘¢ğ‘)âˆ’Pğ¼ğ‘™(ğ‘™ğ‘)\n4:ifğ‘„ğ‘¡ğ‘¦ğ‘ğ‘’=ğ‘„ğ‘Ÿğ‘’ğ‘™then\n5: ifËœğ´ğ‘ ğ‘¢ğ‘š fails the error condition of Lemma 5.2 then\n6: Ëœğ´ğ‘ ğ‘¢ğ‘šâ†perform refinement on D âŠ²Section 3.2.1\n7:return Ëœğ´ğ‘ ğ‘¢ğ‘š\n5.2 Approximate range MAXQuery\nThe query method described in Section 3.2.2 can be applied here,\nexcept that we employ the index described in Section 4.3.\nGiven the query range [ğ‘™ğ‘,ğ‘¢ğ‘], we propose to compute the\napproximate result as:\nËœğ´ğ‘šğ‘ğ‘¥=max{max\nğ‘˜âˆˆğ¼ğ‘™,ğ‘˜â‰¥ğ‘™ğ‘Pğ¼ğ‘™(ğ‘˜),max\nğ‘˜âˆˆğ¼ğ‘¢,ğ‘˜â‰¤ğ‘¢ğ‘Pğ¼ğ‘¢(ğ‘˜),\nmax\nğ‘ğ‘—.ğ¼âŠ†[ğ‘™ğ‘,ğ‘¢ğ‘]ğ‘ğ‘—.ğ‘šğ‘ğ‘¥}(18)\nwhereğ‘ğ‘—denotes an internal node of the index built on top of\nSeqP.ğ¼ğ‘™andğ¼ğ‘¢denote the intervals of Pthat contain the values\nğ‘™ğ‘andğ‘¢ğ‘, respectively.\nThe error conditions for ğ‘„ğ‘ğ‘ğ‘ andğ‘„ğ‘Ÿğ‘’ğ‘™are presented in Lem-\nmas 5.3 and 5.4 respectively. We omit their proofs; they are similar\nto the proofs of Lemmas 5.1 and 5.2.\nLemma 5.3. Ifğ›¿=ğœ€ğ‘ğ‘ğ‘ , then Ëœğ´ğ‘šğ‘ğ‘¥ (in Equation 18) satisfies the\nabsolute error guarantee ğœ€ğ‘ğ‘ğ‘ .\nLemma 5.4. IfËœğ´ğ‘šğ‘ğ‘¥â‰¥ğ›¿(1+1\nğœ€ğ‘Ÿğ‘’ğ‘™), then Ëœğ´ğ‘šğ‘ğ‘¥ (in Equation 18)\nsatisfies the relative error guarantee ğœ€ğ‘Ÿğ‘’ğ‘™.\nWe now discuss how to evaluate Equation 18 in greater detail.\nThe third term is contributed by the inner nodes of the aggregate\nR-tree whose intervals are covered by [ğ‘™ğ‘,ğ‘¢ğ‘]. Regarding the first\ntwo terms, it suffices to find the maximum values for Pğ¼ğ‘™(ğ‘˜)and\nPğ¼ğ‘¢(ğ‘˜)in regions[ğ‘™ğ‘,ğ‘ˆğ¼ğ‘™]and[ğ¿ğ¼ğ‘¢,ğ‘¢ğ‘], as shown in Figure 7,\nwhereğ‘ˆğ¼ğ‘™(ğ¿ğ¼ğ‘¢) is the upper (lower) end of the leaf node interval\nthatğ‘™ğ‘(ğ‘¢ğ‘) overlaps. These values (i.e., red dots) can be calculated\nby checking the border points and the zero derivative points.\nThe overall query algorithm\nWe conclude the query algorithm for both types of error guar-\nantees in Algorithm 3. The processing for ğ‘„ğ‘ğ‘ğ‘ consists of two\nparts: index search ğ‘‡1(i.e., Line 3) and function evaluation ğ‘‡2\n(Lines 8-9). The processing for ğ‘„ğ‘Ÿğ‘’ğ‘™includesğ‘‡1,ğ‘‡2, and possi-\nble refinement ğ‘‡3(i.e., Lines 10-12). The time complexities of ğ‘‡1\nandğ‘‡3are stillğ‘‚(log(|SeqP|))andğ‘‚(log|D|). However, for ğ‘‡2,\nthis includes calculating the zero derivative points within the\nintersection region. If the degree is between 1 and 5, closed-form\nequations exist, where the number of arithmetic operations in\nthese cases are summarized in Table 2. Starting from degree 6,\nthere is no closed-form equations, and thus require expensive\n\nPolyFit: Polynomial-based Indexing Approach for Fast Approximate Range Aggregate Queries EDBT2021, Nicosia, Cyprus\nCheck error condition\n(Problem 1/ Problem 2)ğ‘…(ğ·,[ğ¿,ğ‘ˆ])\nFail: Exact MethodPass\nâ€¦\nğ¼1 ğ¼2ğ¿1ğ‘ˆ11stpolynomial 2ndpolynomial hthpolynomialindex\nPolyFitğ¿2ğ‘ˆ2\nğ¼â„ğ¿â„ğ‘ˆâ„\nFigure 6: Querying framework for PolyFit\nâ€¦\nğ¼ğ‘™ğ¿ğ¼ğ‘™ğ‘ˆğ¼ğ‘™\nğ¼ğ‘¢ğ¿ğ¼ğ‘¢ğ‘ˆğ¼ğ‘¢index\nğ‘™ğ‘ ğ‘¢ğ‘\nâ„™ğ¼ğ‘™(ğ‘˜) â„™ğ¼ğ‘¢(ğ‘˜)\nFigure 7: The maximum measure values (red dots) for two\nleaf nodes, which include ğ‘™ğ‘andğ‘¢ğ‘\nnumerical evaluation methods like gradient descent [ 67]. In prac-\ntice, we recommend to use degrees up to 3 for the approximate\nrange MAXquery.\nAlgorithm 3 Query Processing for MAX (or MIN)\nInput: Aggregate R-tree ğ‘on SeqP,ğ‘™ğ‘,ğ‘¢ğ‘,D,ğ›¿,ğ‘„ğ‘¡ğ‘¦ğ‘ğ‘’\nOutput: Approximate query result ğ´\n1:Ëœğ´ğ‘šğ‘ğ‘¥â†âˆ’âˆ\n2:ifğ‘is an internal node then\n3: update Ëœğ´ğ‘šğ‘ğ‘¥ based on aggregate R-treeâ€™s mechanism\n4:else\n5: forleaf element Pinğ‘do\n6: ifP.ğ¼âˆ©[ğ‘™ğ‘,ğ‘¢ğ‘]â‰ âˆ…then âŠ²the interval Pcovered\n7: ğ¼âˆ—â†P.ğ¼âˆ©[ğ‘™ğ‘,ğ‘¢ğ‘]\n8: ğ›½â†{ğ‘¥âˆˆğ¼âˆ—|Pâ€²(ğ‘¥)=0} âŠ²zero derivative points\n9: Ëœğ´ğ‘šğ‘ğ‘¥â†max(Ëœğ´ğ‘šğ‘ğ‘¥,maxğ‘¥âˆˆğ›½P(ğ‘¥),P(ğ¼âˆ—.ğ‘™),P(ğ¼âˆ—.ğ‘¢)))\n10:ifğ‘is root node and ğ‘„ğ‘¡ğ‘¦ğ‘ğ‘’=ğ‘„ğ‘Ÿğ‘’ğ‘™then\n11: ifËœğ´ğ‘šğ‘ğ‘¥ fails the error condition of Lemma 5.4 then\n12: Ëœğ´ğ‘šğ‘ğ‘¥â†perform refinement on D âŠ²Section 3.2.2\n13:returnğ´\nTable 2: Number of arithmetic operations for calculating\nzero derivative points\ndegree 12 3 4 5\noperations 02up to 18 up to 261 up to 1612\n5.3 Tuning ğ‘‘ğ‘’ğ‘”andğ›¿\nWe discuss the effect of our index parameters (i.e., ğ‘‘ğ‘’ğ‘”,ğ›¿ ) on the\nquery response time and examine how to tune them.\nHow to tune the degree ğ‘‘ğ‘’ğ‘”?\nThe exact function ğ¹(ğ‘˜)is approximated by different polyno-\nmial functions with different degrees. For instance, in Figure 8,\nthe exact function ğ¹(ğ‘˜)is approximated, among others, by the\nfollowing functions (within the deviation threshold ğ›¿): (i) a piece-\nwise function ğº(ğ‘˜)with four pieces of degree-1 functions, or (ii) a\nsingle-piece function ğ»(ğ‘˜)of degree-4. Based on our experimen-\ntal findings (cf. Section 7.2.1), we recommend to set the degree\n-30-20-10 0 10 20 30 40\n-4 -3 -2 -1  0  1  2  3  4value\nkey kF(k)\nG(k): piecewise degree-1\nH(k): piecewise degree-4Figure 8: An example of degree selection\nto 2 or 3. In general, one could generate a random workload of\nqueries to measure the performance of an index, and then test\nthe performance of index structures using different degrees (e.g.,\nfrom 1 to 4).\nAs a remark, it is not practical to use large degree, due to\nthe limited precision of numeric data types in both the linear\nprogramming solver and the programming language [ 1,2]. For\nexample, IBM CPLEX uses ğœ…(kappa) as a statistical measurement\nof numerical difficulties. In our experiments, the ğœ…value of a\ndegree-4 polynomial (1E+10) is much higher than that of a degree-\n1 polynomial (1E+05).\nHow to tune ğ›¿?\nThe tuning of ğ›¿depends on the most frequent query type\nused in the given application. For ğ‘„ğ‘ğ‘ğ‘ (i.e., Problem 1), if all\nusers share the same absolute error threshold ğœ€ğ‘ğ‘ğ‘ , then it is\nused to derive the value of ğ›¿, according to Lemmas 5.1 and 5.3.\nOtherwise, we can select the value of ğ›¿such that it satisfies the\nerror requirements for the majority of users (e.g., 80%).\nForğ‘„ğ‘Ÿğ‘’ğ‘™(i.e., Problem 2), the processing includes three phases:\nindex search, function evaluation, and refinement (cf. Algorithms\n2 and 3). A large ğ›¿leads to fast index search but high refinement\nprobability. In contrast, a small ğ›¿leads to slow index search\nbut low refinement probability. Observe that refinement is often\nmore expensive than index search. We recommend to pick a\nsmallğ›¿such that most users avoid the refinement phase. In our\nexperiments, we examine different values of ğ›¿(e.g., 25, 50, 100,\n200, 500, and 1000) to identify the best setting in terms of the\nquery response time.\n6 EXTENSIONS: QUERIES WITH TWO KEYS\nPrevious sections consider range aggregate queries with a single\nkey (cf. Definition 3.1). We now discuss how to support range\naggregate queries with two keys (cf. Definition 6.1). Due to the\nspace limit, we only consider the COUNT query. In Appendix A.5\n[48], we discuss the case of more than two keys.\nDefinition 6.1. LetDbe a set of records (ğ‘¢,ğ‘£,ğ‘¤), whereğ‘¢,ğ‘£,\nandğ‘¤are the first key, the second key, and the measure, respec-\ntively. Given the query ranges [ğ‘™(1)\nğ‘,ğ‘¢(1)\nğ‘]and[ğ‘™(2)\nğ‘,ğ‘¢(2)\nğ‘]forğ‘¢\n\nEDBT2021, Nicosia, Cyprus Zhe Li1, Tsz Nam Chan2, Man Lung Yiu1, Christian S. Jensen3\nandğ‘£, respectively, we define the COUNT query as:\nğ‘…ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(D,[ğ‘™(1)\nğ‘,ğ‘¢(1)\nğ‘][ğ‘™(2)\nğ‘,ğ‘¢(2)\nğ‘])=COUNT(ğ‘‰) (19)\nwhereğ‘‰is the multi-set of measure values defined below:\nğ‘‰={ğ‘š:(ğ‘˜(1),ğ‘˜(2),ğ‘š)âˆˆD,ğ‘™(1)\nğ‘â‰¤ğ‘˜(1)â‰¤ğ‘¢(1)\nğ‘,ğ‘™(2)\nğ‘â‰¤ğ‘˜(2)â‰¤ğ‘¢(2)\nğ‘}\nWe build the following key-cumulative function to represent\nthe surface (cf. Figure 9), which is formulated in Definition 6.2.\nDefinition 6.2. The key-cumulative function with two keys for\nCOUNT query is defined as ğ¶ğ¹ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘¢,ğ‘£), where:\nğ¶ğ¹ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘¢,ğ‘£)=ğ‘…ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(D[âˆ’âˆ,ğ‘¢][âˆ’âˆ,ğ‘£]) (20)\n150\n 100\n 50\n 0 50 100 150\nlon (key1)50\n25\n0255075lat (key2)\nData\n(a) tweet locations as data points\nlongitude (key1)150\n100\n50\n050100150 latitude (key2)75\n50\n25\n0255075count (measure)\n02004006008001000cumulative count function (b) function for range COUNT queries\nFigure 9: Tweet locations, 2-dimensional keys: discrete\ndata points vs. continuous function\nThe following equation enables us to answer the COUNT query\nquickly.\nğ‘…ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(D[ğ‘™(1)\nğ‘,ğ‘¢(1)\nğ‘][ğ‘™(2)\nğ‘,ğ‘¢(2)\nğ‘])=ğ¶ğ¹ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘¢(1)\nğ‘,ğ‘¢(2)\nğ‘)âˆ’ğ¶ğ¹ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘™(1)\nğ‘,ğ‘¢(2)\nğ‘)\nâˆ’ğ¶ğ¹ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘¢(1)\nğ‘,ğ‘™(2)\nğ‘)+ğ¶ğ¹ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘™(1)\nğ‘,ğ‘™(2)\nğ‘)\nThen, we follow an idea similar to that used in Section 4.1\nand utilize the polynomial surface P(ğ‘¢,ğ‘£)to approximate the key\ncumulative function ğ¶ğ¹ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘¢,ğ‘£)with two keys, where:\nP(ğ‘¢,ğ‘£)=ğ‘‘ğ‘’ğ‘”âˆ‘ï¸\nğ‘–=0ğ‘‘ğ‘’ğ‘”âˆ‘ï¸\nğ‘—=0ğ‘ğ‘–ğ‘—ğ‘¢ğ‘–ğ‘£ğ‘—\nBy replacing ğ¹(ğ‘˜ğ‘–)andP(ğ‘˜ğ‘–)in Equation 9 with ğ¹(ğ‘¢ğ‘–,ğ‘£ğ‘–)and\nP(ğ‘¢ğ‘–,ğ‘£ğ‘–), respectively, we obtain a similar linear programming\nproblem for obtaining the best parameters ğ‘ğ‘–ğ‘—. However, unlike\nthe one-dimensional case, it takes at least ğ‘‚(ğ‘›2)to obtain the\nminimum number of segmentations when using the GS method\n(cf. Section 4.2.1), which is infeasible even for small-scale datasets\n(e.g., 10000 points). Instead, we propose a heuristics-based solu-\ntion that performs quad-tree-like segmentations. As illustrated\nin Figure 10, when a region does not fulfill the error guarantee\nğ›¿(e.g., white rectangles), it is decomposed into four smaller re-\ngions. This procedure terminates when all regions satisfy the\nerror guarantee ğ›¿.\n1stiteration 2nditeration 3rditeration\nâ€¦>ğ›¿\n>ğ›¿ <ğ›¿<ğ›¿<ğ›¿\n<ğ›¿<ğ›¿<ğ›¿\nFigure 10: Quad-tree based approach for obtaining the seg-\nmentation\nAfter building the PolyFit index structure, we utilize a similar\napproach in Section 5 to answer range aggregate queries with\ntheoretical guarantees (cf. Lemmas 6.3 and 6.4).Given the query range [ğ‘™(1)\nğ‘,ğ‘¢(1)\nğ‘]forğ‘¢and[ğ‘™(2)\nğ‘,ğ‘¢(2)\nğ‘]forğ‘£,\nwe propose to compute the approximate result as:\nËœğ´ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ =Pğ¼ğ‘¢ğ‘¢(ğ‘¢(1)\nğ‘,ğ‘¢(2)\nğ‘)âˆ’Pğ¼ğ‘™ğ‘¢(ğ‘™(1)\nğ‘,ğ‘¢(2)\nğ‘)\nâˆ’Pğ¼ğ‘¢ğ‘™(ğ‘¢(1)\nğ‘,ğ‘™(2)\nğ‘)+Pğ¼ğ‘™ğ‘™(ğ‘™(1)\nğ‘,ğ‘™(2)\nğ‘)(21)\nwhereğ¼ğ‘¢ğ‘¢,ğ¼ğ‘™ğ‘¢,ğ¼ğ‘¢ğ‘™, andğ¼ğ‘™ğ‘™denote the coverage regions of P\nthat(ğ‘¢(1)\nğ‘,ğ‘¢(2)\nğ‘),(ğ‘™(1)\nğ‘,ğ‘¢(2)\nğ‘),(ğ‘¢(1)\nğ‘,ğ‘™(2)\nğ‘), and(ğ‘™(1)\nğ‘,ğ‘™(2)\nğ‘)overlap,\nrespectively. These regions could be efficiently found with the\nsame quad-tree index used in construction.\nLemma 6.3. If we setğ›¿=ğœ€ğ‘ğ‘ğ‘ \n4, then Ëœğ´ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ satisfies the absolute\nerror guarantee ğœ€ğ‘ğ‘ğ‘ .\nLemma 6.4. IfËœğ´ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡â‰¥4ğ›¿(1+1\nğœ€ğ‘Ÿğ‘’ğ‘™), then Ëœğ´ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ satisfies the\nrelative error guarantee ğœ€ğ‘Ÿğ‘’ğ‘™.\nThe proofs of Lemma 6.3 and 6.4 are similar to those of Lemmas\n5.1 and 5.2, respectively.\n7 EXPERIMENTAL EVALUATION\nWe introduce the experimental setting in Section 7.1. Then, we\ninvestigate the performance of PolyFit in Section 7.2. Next, we\ncompare PolyFit and error-bounded competitors on real datasets\nin Section 7.3. After that, we compare the response time of PolyFit\nwith other heuristic methods in Section 7.4. Lastly, we compare\nthe construction times of all methods in Section 7.5.\n7.1 Experimental Setting\nWe use three real large-scale datasets (0.9M to 100M records) to\nevaluate the performance. They are summarized in Table 3. For\neach dataset, we randomly generate 1000 queries. In the single-\nkey case, we randomly choose two key values in the datasets as\nthe start and end points of each query interval. In the two-key\ncase, we randomly sample rectangles from the dataset as query\nregions. In our experiments, we focus on COUNT andMAXqueries.\nNevertheless, our methods are readily applicable to SUMandMIN\nqueries.\nTable 3: Datasets\nName Size Key(s) Measure Aggregate function\nHKI [5] 0.9M timestamp index value MAX\nTWEET [19] 1M latitude # of tweets COUNT\nOSM [7] 100M latitude, longitude # of records COUNT\nTable 4 summarizes different methods for supporting range ag-\ngregate queries. We classify these methods based on five features:\n(i) whether it provides absolute error guarantees (cf. Problem\n1 (ğ‘„ğ‘ğ‘ğ‘ )), (ii) whether it provides relative error guarantees (cf.\nProblem 2 (ğ‘„ğ‘Ÿğ‘’ğ‘™)), (iii) whether it supports queries with two keys\n(cf. Section 6), (iv) whether it supports the COUNT query, and (v)\nwhether it supports the MAXquery.\nWe first introduce the methods that can satisfy deterministic\nerror guarantees (i.e., those with âœ“orâ–³in theğ‘„ğ‘ğ‘ğ‘ andğ‘„ğ‘Ÿğ‘’ğ‘™\ncolumns in Table 4). The aR-tree [ 59] is a traditional tree-based\nmethod for answering exact COUNT andMAXqueries. The MRTree\n[45] extends the aR-tree by utilizing progressive lower and up-\nper bounds to answer approximate COUNT andMAXqueries with\nerror guarantees. In addition, both the aR-tree and the MRTree\ncan support the range aggregate queries with two keys. With\nsimple modifications, the learned-index methods, including RMI\n[44], FITing-tree [ 28], and PGM [ 27], can be extended to support\nrange aggregate queries with both absolute and relative error\nguarantees. However, they are unable to support queries with\ntwo keys and the MAXquery. Due to the space limitation, we cover\nthe modifications and parameter tuning in our technical report\n(cf. Appendix in [ 48]).PolyFit supports all these five features. By\n\nPolyFit: Polynomial-based Indexing Approach for Fast Approximate Range Aggregate Queries EDBT2021, Nicosia, Cyprus\nTable 4: Methods for range aggregate queries\nâœ“Directly supportâ–³Extend to support Ã—Cannot support\nMethod ğ‘„ğ‘ğ‘ğ‘ ğ‘„ğ‘Ÿğ‘’ğ‘™ 2 keys COUNT MAX\naR-tree [59]âœ“âœ“âœ“âœ“âœ“\nMRTree [45]âœ“âœ“âœ“âœ“âœ“\nRMI [44]â–³â–³Ã—âœ“Ã—\nFITing-tree [28] â–³â–³Ã—âœ“Ã—\nPGM [27]â–³â–³Ã—âœ“Ã—\nPolyFit (ours)âœ“âœ“âœ“âœ“âœ“\nHist [68]Ã—Ã—Ã—âœ“Ã—\nS-tree [6]Ã—Ã—Ã—âœ“Ã—\nS2 [35]Ã—Ã—âœ“âœ“Ã—\nVerdictDB [60] Ã—Ã—âœ“âœ“Ã—\nDBest [53]Ã—Ã—âœ“âœ“Ã—\nPLATO [50]Ã—Ã—Ã—âœ“Ã—\ndefault, we follow Lemmas 5.1, 5.3, and 6.3 to set the ğ›¿values in\nProblem 1 (ğ‘„ğ‘ğ‘ğ‘ ), for different absolute error threshold ğœ€ğ‘ğ‘ğ‘ . In\naddition, we adopt ğ›¿=100inPolyFit for the experiments with\ntwo keys in Problem 2 ( ğ‘„ğ‘Ÿğ‘’ğ‘™).\nWe then discuss the methods that are unable to fulifll the de-\nterministic error guarantee (i.e., the methods with Ã—in theğ‘„ğ‘ğ‘ğ‘ \nandğ‘„ğ‘Ÿğ‘’ğ‘™columns in Table 4). Hist [ 68] adopts the entropy-based\nhistogram for answering the COUNT query. The S-tree prebuilds\nthe STX B-tree [ 6] on top of a sampled subset of each dataset. S2\n[35] and VerdictDB [ 60] are sampling-based approaches that can\nonly provide probabilistic error guarantees. By default, we set the\nprobability to 0.9 in our experiments. Both DBest [ 53] and PLATO\n[50] are the state-of-the-art methods in approximate query pro-\ncessing and time series databases, respectively, that can be also\nadapted to answer approximate range aggregate queries. Since\nthese methods cannot provide deterministic error guarantees, we\nregard them as heuristic methods.\nWe implemented all methods in C++ and conducted experi-\nments on an Intel Core i7-8700 3.2GHz PC using WSL (Windows\n10 Subsystem for Linux).\n7.2 PolyFit Tuning\nIn this section, we investigate two research questions for PolyFit ,\nnamely (1) how does the degree ğ‘‘ğ‘’ğ‘”affect the query response\ntime of PolyFit ? (2) how does the degree ğ‘‘ğ‘’ğ‘”affect the construc-\ntion time of PolyFit ?\n7.2.1 Effect ofğ‘‘ğ‘’ğ‘”on the query response time .Recall\nthat we need to select the degree ğ‘‘ğ‘’ğ‘”in order to build PolyFit .\nIt is thus important to understand how this parameter affects\nthe query response time. Here, we use the form PolyFit -ğ‘‘ğ‘’ğ‘”to\nrepresent the degree ğ‘‘ğ‘’ğ‘”ofPolyFit . Figure 11 shows the trends for\nthe query response time for both COUNT (one key and two keys)\nandMAX(one key) queries, using the absolute error threshold\nğœ€ğ‘ğ‘ğ‘ =100. When we choose a larger degree ğ‘‘ğ‘’ğ‘”, the polynomial\nfunction can provide better approximation for ğ¹(ğ‘˜), and thus\nreduce the index size, which can reduce the response time for\neach query. However, the larger the degree ğ‘‘ğ‘’ğ‘”, the larger the\ncomputation time for each node in PolyFit . Therefore, we can\nfind that the response time increases (e.g., ğ‘‘ğ‘’ğ‘”=3and 4 in Figure\n11a), once we utilize a high degree ğ‘‘ğ‘’ğ‘”. By default, in subsequent\nexperiments, we choose deg = 2 for the COUNT query with a\nsingle key, and deg = 3 for the COUNT query with two keys and\nfor the MAX query.\n7.2.2 Effect ofğ‘‘ğ‘’ğ‘”on the construction time .We further\nexamine the construction time for PolyFit , varying the highest\ndegreeğ‘‘ğ‘’ğ‘”from 1 to 4 in the polynomial function (cf. Figure 12).\nSince a polynomial function with a higher degree can produceerror guarantee for a longer interval ğ¼, i.e.,ğ¸(ğ¼) â‰¤ğ›¿, the GS\nmethod needs to call the LP solver with longer intervals (cf. line 4\nin Algorithm 1), which can increase the construction time when\nusing polynomial functions with higher degree ğ‘‘ğ‘’ğ‘”.\n7.3 Comparing with Error-Bounded Methods\nIn this section, we test the response time of the different meth-\nods that can fulfill the absolute and relative error guarantees.\nHere, we adopt the default settings for these methods (cf. Section\n7.1) and use the datasets HKI, TWEET, and OSM for testing the\nperformance of COUNT (single key), MAX(single key), and COUNT\n(two keys) queries, respectively. For Problem 1 ( ğ‘„ğ‘ğ‘ğ‘ ), we fix the\nabsolute error ğœ€ğ‘ğ‘ğ‘ =100andğœ€ğ‘ğ‘ğ‘ =200for the experiments\nwith one key and two keys, respectively. For Problem 2 ( ğ‘„ğ‘Ÿğ‘’ğ‘™),\nwe fix the relative error ğœ€ğ‘Ÿğ‘’ğ‘™=0.01. Table 5 shows the response\ntime of different methods. Observe that PolyFit achieves the best\nperformance for all the types of queries. For the COUNT query\nwith two keys, PolyFit can achieve a speedup of at least two\norders of magnitude over the existing methods.\nTable 5: Response time (nanoseconds) for all methods\nwith error guarantees\nError guarantee ğ‘„ğ‘ğ‘ğ‘  ğ‘„ğ‘Ÿğ‘’ğ‘™\nQuery type COUNT MAX COUNT COUNT MAX COUNT\n# of keys 1 1 2 1 1 2\naR-tree 590 3592 357457 590 3592 357457\nMRTree 565 182 385391 335 138 98919\nRMI 568 n/a n/a 579 n/a n/a\nFITing-tree 135 n/a n/a 147 n/a n/a\nPGM 104 n/a n/a 118 n/a n/a\nPolyfit 68 63 5274 79 65 5299\nSensitivity of ğœ€ğ‘ğ‘ğ‘ forCOUNT query. We investigate how\nthe absolute error ğœ€ğ‘ğ‘ğ‘ affects the response times of different\nmethods. For the COUNT query with single key, we choose five\nabsolute error values for testing, which are 100, 200, 400, 1000,\nand 2000. Observe from Figure 13a that since PolyFit , FITing-\ntree, and PGM can provide more compact index structures for the\ndatasets, these methods can significantly improve the efficiency,\ncompared with the traditional index structures, i.e., the aR-tree\nand the MRTree. In addition, due to the better approximation with\nnonlinear polynomial functions ( ğ‘‘ğ‘’ğ‘”=2),PolyFit can achieve\n1.33x to 6x speedups, over the existing learned-index structures,\nincluding RMI, FITing-tree, and PGM. For the COUNT query with\ntwo keys, we choose 200, 400, 800, 2000, and 4000 as the absolute\nerror values for testing. Since the state-of-the-art learned index\nstructures (RMI, FITing-tree, and PGM) can only support queries\nwith a single key, we omit these methods in this experiment.\nFigure 13b shows that PolyFit achieves at least one order of\nmagnitude speedups compared with the existing methods (aR-\ntree and MRTree), which is due to its compact index structure\nand query processing method.\nSensitivity of ğœ€ğ‘Ÿğ‘’ğ‘™forCOUNT query. We proceed to test how\nthe relative error ğœ€ğ‘Ÿğ‘’ğ‘™affects the response time of the different\nmethods. In this experiment, we choose five relative error values,\nwhich are 0.005, 0.01, 0.05, 0.1, and 0.2. Based on the more compact\nindex structure, PolyFit is able to achieve better performance,\ncompared with the existing methods (cf. Figure 14a). For the\nCOUNT query with two keys, PolyFit significantly outperforms\nthe existing methods, i.e., the aR-tree and the MRTree, by at least\none order of magnitude (cf. Figure 14b).\nSensitivity of ğœ€ğ‘ğ‘ğ‘ andğœ€ğ‘Ÿğ‘’ğ‘™forMAXquery. In this experi-\nment, we proceed to investigate how the absolute error ğœ€ğ‘ğ‘ğ‘ and\n\nEDBT2021, Nicosia, Cyprus Zhe Li1, Tsz Nam Chan2, Man Lung Yiu1, Christian S. Jensen3\n 0 20 40 60 80 100 120\n1 2 3 4query response time (ns)\ndegreeQabs\nÎµabs=100\n 0 1000 2000 3000 4000 5000 6000 7000 8000\n1 2 3 4query response time (ns)\ndegreeQabs\nÎµabs=100\n 0 20 40 60 80 100 120\n1 2 3query response time (ns)\ndegreeQabs\nÎµabs=100\n(a)COUNT query (single key) (b) COUNT query (two keys) (c) MAXquery (single key)\nFigure 11: Running time for COUNT (single key), COUNT (two keys), and MAXqueries on TWEET, OSM, and HKI datasets,\nrespectively, varying the degree ğ‘‘ğ‘’ğ‘”ofPolyFit\n 10 100 1000\n1 2 3 4construction time (s)\ndegreePolyFit\n 1000 10000\n1 2 3 4construction time (s)\ndegreePolyFit\n(a)COUNT query (single key) (b) COUNT query (two keys)\nFigure 12: Index construction time of PolyFit forCOUNT\nquery with single key (using TWEET dataset) and two\nkeys (using OSM dataset), varying the degree ğ‘‘ğ‘’ğ‘”\n 10 100 1000\n0 500 1000 1500 2000query response time (ns)\nabsolute error thresholdQabs\naR-tree\nRMI\nMRTreeFITing-Tree\nPGM\nPolyFit-2\n 1000 10000 100000 1x106\n0 1000 2000 3000 4000query response time (ns)\nabsolute error thresholdQabs\naR-tree\nMRTree\nPolyFit-3\n(a)COUNT query (single key) (b) COUNT query (two keys)\nFigure 13: Response time for COUNT query in TWEET\ndataset (for single key) and OSM dataset (for two keys),\nvarying the absolute error ğœ€ğ‘ğ‘ğ‘ \n 10 100 1000\n 0.01  0.1query response time (ns)\nrelative error thresholdQrel\naR-tree\nRMI\nMRTreeFITing-Tree\nPGM\nPolyFit-2\n 1000 10000 100000 1x106\n 0.01  0.1query response time (ns)\nrelative error thresholdQrel\naR-tree\nMRTree\nPolyFit-3\n(a)COUNT query (single key) (b) COUNT query (two keys)\nFigure 14: Response time for COUNT query in TWEET\ndataset (for single key) and OSM dataset (for two keys),\nvarying the relative error ğœ€ğ‘Ÿğ‘’ğ‘™\n 10 100 1000 10000\n0 250 500 750 1000query response time (ns)absolute error thresholdQabs\naR-tree\nMRTree\nPolyFit-3\n 10 100 1000 10000\n 0.01  0.1query response time (ns)relative error thresholdQrel\naR-tree\nMRTree\nPolyFit-3\n(a)MAXquery, varying ğœ€ğ‘ğ‘ğ‘  (b)MAXquery, varying ğœ€ğ‘Ÿğ‘’ğ‘™\nFigure 15: Response time for MAXquery in HKI dataset\nrelative error ğœ€ğ‘Ÿğ‘’ğ‘™affect the efficiency performance of different\n 10 100 1000 10000\n 0.01  0.1  1  10query response time (ns)\nselectivity (%)Qabs\nMRTree\naR-tree\nRMIFITing-Tree\nPGM\nPolyFit-2\n 1000 10000 100000 1x106\n 0.01  0.1  1  10query response time (ns)\nselectivity (%)Qabs\naR-tree\nMRTree\nPolyFit-3(a)COUNT query (single key) (b) COUNT query (two keys)\nFigure 16: Response time for COUNT query in TWEET\ndataset (for single key) and OSM dataset (for two keys),\nvarying the selectivity of the query\n0200400600\n0 20 40 60 80 100query response time (ns)dataset records (million)Qabs\nFITing-Tree\nPGM\nPolyFit-2\n 1000 10000 100000 1x106\n0 20 40 60 80 100query response time (ns)dataset records (million)Qabs\naR-tree\nMRTree\nPolyFit-3\n(a)COUNT query (single key) (b) COUNT query (two keys)\nFigure 17: Response time for COUNT query in OSM dataset,\nvarying the dataset size\n 1 10 100 1000\n 0  100  200  300  400  500  600  700Polyfit-2\nÎ´ = 25\nÎ´ = 50\nÎ´ = 100\nÎ´ = 200\nÎ´ = 500\nÎ´ = 1000PGM\nÎ´ = 25\n50\nÎ´ = 100\nÎ´ = 200\nÎ´ = 500\nÎ´ = 1000FITing-Tree\nÎ´ = 25\nÎ´ = 50\nÎ´ = 100\nÎ´ = 200\nÎ´ = 500\nÎ´ = 1000MRTree\nÎ´ = 25 50 100 200 500 1000aR-Tree\nÎ´ = 25 - 1000\nRMI\nÎ´ = 25\nÎ´ = 50 - 200\nÎ´ = 500, 1000index size (KB)\nquery response time (ns)Qrel\nFigure 18: Trade-off between the query response time and\nindex size of COUNT query (single key) in TWEET dataset,\nforğ‘„ğ‘Ÿğ‘’ğ‘™withğœ€ğ‘Ÿğ‘’ğ‘™=0.01, varyingğ›¿from 25to1000\nmethods. Observe from Figure 15, PolyFit can achieve at least 2x\nspeedup, compared with other methods, even though the selected\nerror is small.\nSensitivity of the selectivity for COUNT query. We further\ntest the response time of the different methods, varying the selec-\ntivity of the COUNT query. Figure 16 shows that when we increase\nthe selectivity of the COUNT query (i.e., each query covers a larger\nregion), the query response time normally increases in different\nmethods. Since all methods for the COUNT query with a single\nkey have logarithmic time complexity, they are not sensitive to\nthe selectivity (cf. Figure 16a). Unlike the single key case, both\n\nPolyFit: Polynomial-based Indexing Approach for Fast Approximate Range Aggregate Queries EDBT2021, Nicosia, Cyprus\nthe existing methods aR-tree and MRTree are sensitive to the\nselectivity, compared with PolyFit (cf. Figure 16b).\nIn both cases, PolyFit achieves better performance across dif-\nferent selectivities. Since the methods MRTree, aR-tree, and RMI\nalways provide inferior efficiency in the single key case (cf. Fig-\nures 13a, 14a and 16a), compared with FITing-Tree, PGM and\nPolyFit , we omit their results in subsequent experiments.\nScalability to the dataset size. We proceed to test how the\ndataset size affects the efficiency of PolyFit and other methods. In\nthis experiment, we choose the largest dataset OSM (with 100M\nrecords) for testing. Here, we focus on solving Problem 1 ( ğ‘„ğ‘ğ‘ğ‘ )\nforCOUNT query, in which we adopt the default absolute errors,\ni.e.,ğœ€ğ‘ğ‘ğ‘ =100andğœ€ğ‘ğ‘ğ‘ =200for the cases in single key and two\nkeys, respectively, and choose the latitude attribute as the key.\nTo conduct this experiment, we choose five dataset sizes, which\nare 1M, 10M, 30M, and 100M. Figure 17 shows that PolyFit scales\nwell with the dataset size and outperforms other methods.\nTrade-off between the query response time and index\nsize. We proceed to investigate the trade-off between the query\nresponse time and index size of the different indexing methods. To\nconduct this experiment, we focus on Problem 2 ( ğ‘„ğ‘Ÿğ‘’ğ‘™) and choose\n25, 50, 100, 200, 500, and 1000 as values of ğ›¿for testing. In Figure\n18, since the changes to ğ›¿cannot affect the index construction\nmethods of the aR-tree and MRTree, parameter ğ›¿cannot affect\nthe index sizes of these two methods. We also notice that these\nindex structures consistently provide inferior performance in\nterms of index size and query response time, compared with\nthe FITing-tree, PGM, and the PolyFit methods. For the other\nmethods, we can observe that the smaller the ğ›¿, the larger the\nindex size and query response time. The reason is that smaller\nğ›¿values lead to more leaf nodes in the index structures in the\ndifferent methods (e.g., more intervals are generated by the GS\nmethod (cf. Algorithm 1) in PolyFit ). On the other hand, if ğ›¿is too\nlarge, it is easier for an online query to violate the error condition\nforğ‘„ğ‘Ÿğ‘’ğ‘™(i.e., Lemma 5.2), and thus the query response time can\nalso be larger. As such, all curves (except for the MRTree and\naR-tree methods) in Figure 18 resemble the â€œCâ€-shape. In general,\nPolyFit -2 offers a better trade-off compared with other methods.\n7.4 Comparing with Heuristic Methods\nWe compare the response time of PolyFit with other heuristic\nmethods, which cannot fulfill deterministic error guarantees, i.e.,\nğ‘„ğ‘ğ‘ğ‘ (cf. Problem 1) and ğ‘„ğ‘Ÿğ‘’ğ‘™(cf. Problem 2). In this experiment,\nwe adopt the default setting for the method PLATO [ 50], vary\nthe bin size for the method Hist and vary the sampling size for\nthe sampling-based methods, including S-tree, S2, VerdictDB,\nand DBest. Since S2 cannot achieve less than 100000ns query\nresponse time with 10% measured relative error, we omit the\nresult of S2 in Figure 19a. In addition, we only report the results\nof the heuristic methods DBest and VerdictDB in Figure 19b, as\nthe other heuristic methods cannot support COUNT queries with\ntwo keys (cf. Table 4). In these two figures, PolyFit yields the\nsmallest query response time with similar relative error.\n7.5 Comparing the Construction Time of All\nMethods\nWe proceed to investigate further how the construction times\nof all methods change across different dataset sizes. Here, we\nadopt the default degrees, i.e., ğ‘‘ğ‘’ğ‘”=2andğ‘‘ğ‘’ğ‘”=3, for the\npolynomial functions in the COUNT query with a single key and\ntwo keys, respectively. In Figure 20, PolyFit consistently achieves\nfaster construction time than Hist and DBest. Although PolyFit\nmay not achieve the fastest construction time, compared with\n 10 100 1000 10000 100000\n 0.01  0.1  1  10query response time (ns)\nmeasured relative error (%)DBest\nVerdictDB\nPLATO\nHist\nS-tree\nPolyFit-2\n 1000 10000 100000 1x106 1x107\n0.5 1.0 5.0 10.0query response time (ns)\nmeasured relative error (%)DBest\nVerdictDB\nPolyFit-3(a)COUNT query (single key) (b) COUNT query (two keys)\nFigure 19: Response time between PolyFit and the heuris-\ntic methods for COUNT query with single key and two keys\nin TWEET and OSM datasets, respectively\nsome methods (e.g., the aR-tree and the MRTree), PolyFit takes\nless than 150s and 2500s (with default ğ‘‘ğ‘’ğ‘”) in the construction\nstage with 1 million (TWEET) and 30 million records (OSM),\nrespectively, which are acceptable in practice where the datasets\nare static during data analytics tasks.\n 0.01 0.1 1 10 100 1000 10000 100000 1x106\n 0  5  10  15  20  25  30construction time (s)\ndataset size (million records)Hist\nDBest\nPolyFit-2RMI\nMRTree\naR-treePLATO\nFITing-tree\nPGM\n 0.01 0.1 1 10 100 1000 10000 100000 1x106\n 0  5  10  15  20  25  30construction time (s)\ndataset size (million records)DBest\nPolyFit-3MRTree\naR-tree\n(a)COUNT query (single key) (b) COUNT query (two keys)\nFigure 20: Index construction time of methods for COUNT\nquery with single key and two keys (using OSM dataset\nfor both settings), varying the dataset size\n8 CONCLUSION\nIn this paper, we study the range aggregate queries with two\ntypes of approximate guarantees, which are (1) absolute error\nguarantees (cf. Problem 1 ( ğ‘„ğ‘ğ‘ğ‘ )) and (2) relative error guarantees\n(cf. Problem 2 ( ğ‘„ğ‘Ÿğ‘’ğ‘™)). Unlike the existing methods, our work can\nefficiently support the most commonly used range aggregate\nqueries ( SUM,COUNT ,MIN,MAX), fulfill the error guarantees, and\nsupport the setting of two keys.\nIn order to improve the efficiency of computing these queries,\nwe utilize several polynomial functions to fit the data points and\nthen build the compact index structure PolyFit on top of these\npolynomial functions. An experimental study shows that PolyFit\ncan achieve significant speedups compared with existing learned-\nindex methods and other traditional exact/ approximate methods\nfor different query types. In particular, we can achieve at most\n5ğœ‡s query response time in a dataset with 30 million records,\nwhich cannot be achieved by the state-of-the-art methods.\nIn the future, we plan to further develop advanced techniques\nto improve the efficiency of constructing PolyFit , in order to\nhandle updates of records in large-scale datasets. In addition,\nwe aim to extend our methods to support other fundamental\nanalytics operations, including standard deviation, median, etc.\nMoreover, we plan to investigate how to utilize the idea of PolyFit\nto further improve the efficiency of other types of statistics and\nmachine learning models, e.g., kernel density estimation [ 16,18],\nand support vector machines [17, 18].\nREFERENCES\n[1]CPLEX performance tuning for linear programs. https://www.ibm.com/\nsupport/pages/node/397127#Item4.\n[2]Diagnosing ill conditioning. https://www.ibm.com/support/pages/node/\n397063.\n\nEDBT2021, Nicosia, Cyprus Zhe Li1, Tsz Nam Chan2, Man Lung Yiu1, Christian S. Jensen3\n[3] Foursquare API. https://developer.foursquare.com/.\n[4] Foursquare statistics. https://99firms.com/blog/foursquare-statistics/#gref/.\n[5]Hong Kong 40 Index 2018. https://www.dukascopy.com/swiss/english/\nmarketwatch/historical/. [Online; accessed 20-Dec-2019].\n[6]STX B+ Tree. https://panthema.net/2007/stx-btree/. [Online; accessed 11-Jan-\n2019].\n[7]OpenStreetMap dataset. https://registry.opendata.aws/osm/, 2019. [Online;\naccessed 19-May-2019].\n[8]A. Aboulnaga and S. Chaudhuri. Self-tuning histograms: Building histograms\nwithout looking at data. In SIGMOD , pages 181â€“192, 1999.\n[9]S. Agarwal, B. Mozafari, A. Panda, H. Milner, S. Madden, and I. Stoica. Blinkdb:\nqueries with bounded errors and bounded response times on very large data.\nInEuroSys , pages 29â€“42, 2013.\n[10] C. Anagnostopoulos and P. Triantafillou. Learning to accurately count with\nquery-driven predictive analytics. In BigData , pages 14â€“23, 2015.\n[11] M. Armbrust, R. S. Xin, C. Lian, Y. Huai, D. Liu, J. K. Bradley, X. Meng, T. Kaftan,\nM. J. Franklin, A. Ghodsi, et al. Spark SQL: Relational data processing in Spark.\nInSIGMOD , pages 1383â€“1394, 2015.\n[12] D. Bartholomew. MariaDB cookbook . Packt Publishing Ltd, 2014.\n[13] S. M. Beitzel, E. C. Jensen, A. Chowdhury, D. Grossman, and O. Frieder. Hourly\nanalysis of a very large topically categorized web query log. In SIGIR , pages\n321â€“328, 2004.\n[14] J. L. Bentley and A. C. Yao. An almost optimal algorithm for unbounded\nsearching. Inf. Process. Lett. , 5(3):82â€“87, 1976.\n[15] K. Chan and A. W. Fu. Efficient time series matching by wavelets. In ICDE ,\npages 126â€“133, 1999.\n[16] T. N. Chan, R. Cheng, and M. L. Yiu. QUAD: quadratic-bound-based kernel\ndensity visualization. In SIGMOD , pages 35â€“50. ACM, 2020.\n[17] T. N. Chan, L. H. U, R. Cheng, M. L. Yiu, and S. Mittal. Efficient algorithms for\nkernel aggregation queries. IEEE TKDE , pages 1â€“1, 2020.\n[18] T. N. Chan, M. L. Yiu, and L. H. U. KARL: fast kernel aggregation queries. In\nICDE , pages 542â€“553, 2019.\n[19] L. Chen, G. Cong, X. Cao, and K.-L. Tan. Temporal spatial-keyword top-k\npublish/subscribe. In ICDE , pages 255â€“266, 2015.\n[20] T. Condie, N. Conway, P. Alvaro, J. M. Hellerstein, J. Gerth, J. Talbot, K. Elmele-\negy, and R. Sears. Online aggregation and continuous query support in\nmapreduce. In SIGMOD , pages 1115â€“1118, 2010.\n[21] T. H. Cormen, C. Stein, R. L. Rivest, and C. E. Leiserson. Introduction to\nAlgorithms . McGraw-Hill Higher Education, 2nd edition, 2001.\n[22] M. de Berg, O. Cheong, M. J. van Kreveld, and M. H. Overmars. Computational\ngeometry: algorithms and applications, 3rd Edition . Springer, 2008.\n[23] K. Delaney. Inside Microsoft SQL Server 2000 . Microsoft Press, 2000.\n[24] A. Eldawy, M. F. Mokbel, S. Al-Harthi, A. Alzaidy, K. Tarek, and S. Ghani.\nSHAHED: A mapreduce-based system for querying and visualizing spatio-\ntemporal satellite data. In ICDE , pages 1585â€“1596, 2015.\n[25] H. Elmeleegy, A. K. Elmagarmid, E. Cecchet, W. G. Aref, and W. Zwaenepoel.\nOnline piece-wise linear approximation of numerical streams with precision\nguarantees. VLDB , 2(1):145â€“156, 2009.\n[26] C. Faloutsos, M. Ranganathan, and Y. Manolopoulos. Fast subsequence match-\ning in time-series databases. In SIGMOD , pages 419â€“429, 1994.\n[27] P. Ferragina and G. Vinciguerra. The PGM-index: a fully-dynamic compressed\nlearned index with provable worst-case bounds. PVLDB , 13(8):1162â€“1175,\n2020.\n[28] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska. Fiting-tree:\nA data-aware index structure. In SIGMOD , pages 1189â€“1206, 2019.\n[29] M. Garofalakis and P. B. Gibbons. Wavelet synopses with error guarantees.\nInSIGMOD , pages 476â€“487, 2002.\n[30] M. N. Garofalakis and P. B. Gibbons. Probabilistic wavelet synopses. ACM\nTrans. Database Syst. , 29:43â€“90, 2004.\n[31] J. L. Gearhart, K. L. Adair, R. J. Detry, J. D. Durfee, K. A. Jones, and N. Martin.\nComparison of open-source linear programming solvers. Sandia National\nLaboratories, SAND2013-8847 , 2013.\n[32] D. Gunopulos, G. Kollios, V. J. Tsotras, and C. Domeniconi. Approximating\nmulti-dimensional aggregate range queries over real attributes. In SIGMOD ,\npages 463â€“474, 2000.\n[33] D. Gunopulos, G. Kollios, V. J. Tsotras, and C. Domeniconi. Selectivity es-\ntimators for multidimensional range queries over real attributes. VLDBJ ,\n14(2):137â€“154, 2005.\n[34] P. J. Haas, J. F. Naughton, and A. N. Swami. On the relative cost of sampling\nfor join selectivity estimation. In PODS , pages 14â€“24, 1994.\n[35] P. J. Haas and A. N. Swami. Sequential sampling procedures for query size\nestimation. In SIGMOD , pages 341â€“350, 1992.\n[36] S. Han, H. Wang, J. Wan, and J. Li. An iterative scheme for leverage-based\napproximate aggregation. In ICDE , pages 494â€“505, 2019.\n[37] M. Heimel, M. Kiefer, and V. Markl. Self-tuning, GPU-accelerated kernel\ndensity models for multidimensional selectivity estimation. In SIGMOD , pages\n1477â€“1492, 2015.\n[38] C.-T. Ho, R. Agrawal, N. Megiddo, and R. Srikant. Range queries in OLAP data\ncubes. In SIGMOD , pages 73â€“88, 1997.\n[39] I. F. Ilyas, V. Markl, P. Haas, P. Brown, and A. Aboulnaga. CORDS: automatic\ndiscovery of correlations and soft functional dependencies. In SIGMOD , pages\n647â€“658, 2004.[40] S. K. Jensen, T. B. Pedersen, and C. Thomsen. ModelarDB: modular model-\nbased time series management with spark and cassandra. PVLDB , 11(11):1688â€“\n1701, 2018.\n[41] E. J. Keogh. Fast similarity search in the presence of longitudinal scaling in\ntime series databases. In ICTAI , pages 578â€“584, 1997.\n[42] E. J. Keogh, S. Chu, D. M. Hart, and M. J. Pazzani. An online algorithm for\nsegmenting time series. In ICDM , pages 289â€“296, 2001.\n[43] E. J. Keogh and M. J. Pazzani. An enhanced representation of time series which\nallows fast and accurate classification, clustering and relevance feedback. In\nKDD , pages 239â€“243, 1998.\n[44] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned\nindex structures. In SIGMOD , pages 489â€“504, 2018.\n[45] I. Lazaridis and S. Mehrotra. Progressive approximate aggregate queries with\na multi-resolution tree structure. In SIGMOD , pages 401â€“412, 2001.\n[46] Y. T. Lee and A. Sidford. Efficient inverse maintenance and faster algorithms\nfor linear programming. In FOCS , pages 230â€“249, 2015.\n[47] D. Leenaerts and W. van Bokhoven. Piecewise Linear Modeling and Analysis .\nSpringer US, 2013.\n[48] Z. Li, T. N. Chan, M. L. Yiu, and C. S. Jensen. Polyfit: Polynomial-based indexing\napproach for fast approximate range aggregate queries. CoRR , abs/2003.08031,\n2020.\n[49] L. Lim, M. Wang, and J. S. Vitter. Sash: A self-adaptive histogram set for\ndynamically changing workloads. In VLDB , pages 369â€“380, 2003.\n[50] C. Lin, E. Boursier, and Y. Papakonstantinou. Plato: approximate analytics\nover compressed time series with tight deterministic error guarantees. PVLDB ,\n13(7):1105â€“1118, 2020.\n[51] R. J. Lipton, J. F. Naughton, and D. A. Schneider. Practical selectivity estimation\nthrough adaptive sampling. In SIGMOD , pages 1â€“11, 1990.\n[52] C. A. Lynch. Selectivity estimation and query optimization in large databases\nwith highly skewed distribution of column values. In VLDB , pages 240â€“251,\n1988.\n[53] Q. Ma and P. Triantafillou. DBEst: Revisiting approximate query processing\nengines with machine learning models. In SIGMOD , pages 1553â€“1570, 2019.\n[54] A. Marcus, M. S. Bernstein, O. Badar, D. R. Karger, S. Madden, and R. C. Miller.\nProcessing and visualizing the data in tweets. SIGMOD Record , 40(4):21â€“27,\n2011.\n[55] V. Markl, P. J. Haas, M. Kutsch, N. Megiddo, U. Srivastava, and T. M. Tran.\nConsistent selectivity estimation via maximum entropy. VLDBJ , 16(1):55â€“76,\n2007.\n[56] B. Momjian. PostgreSQL: introduction and concepts . Addison-Wesley New York,\n2001.\n[57] M. Muralikrishna and D. J. DeWitt. Equi-depth multidimensional histograms.\nInSIGMOD , pages 28â€“36, 1988.\n[58] T. Palpanas, M. Vlachos, E. J. Keogh, and D. Gunopulos. Streaming time series\nsummarization using user-defined amnesic functions. IEEE TKDE , 20(7):992â€“\n1006, 2008.\n[59] D. Papadias, P. Kalnis, J. Zhang, and Y. Tao. Efficient OLAP operations in\nspatial data warehouses. In SSTD , pages 443â€“459, 2001.\n[60] Y. Park, B. Mozafari, J. Sorenson, and J. Wang. VerdictDB: Universalizing\napproximate query processing. In SIGMOD , pages 1461â€“1476, 2018.\n[61] Y. Park, S. Zhong, and B. Mozafari. Quicksel: Quick selectivity learning with\nmixture models. In SIGMOD , pages 1017â€“1033, 2020.\n[62] I. Popivanov and R. J. Miller. Similarity search over time-series data using\nwavelets. In ICDE , pages 212â€“221, 2002.\n[63] D. Rafiei. On similarity-based queries for time series data. In ICDE , pages\n410â€“417, 1999.\n[64] C. RÃ© and D. Suciu. Understanding cardinality estimation using entropy\nmaximization. In PODS , pages 53â€“64, 2010.\n[65] M. Riondato, M. Akdere, U. Ã‡etintemel, S. B. Zdonik, and E. Upfal. The vc-\ndimension of sql queries and selectivity estimation through sampling. In\nECML PKDD , pages 661â€“676, 2011.\n[66] F. Savva, C. Anagnostopoulos, and P. Triantafillou. Aggregate query prediction\nunder dynamic workloads. In BigData , pages 671â€“676, 2019.\n[67] I. N. Stewart. Galois theory . CRC Press, 2015.\n[68] H. To, K. Chiang, and C. Shahabi. Entropy-based histograms for selectivity\nestimation. In CIKM , pages 1939â€“1948, 2013.\n[69] J. S. Vitter and M. Wang. Approximate computation of multidimensional\naggregates of sparse data using wavelets. In SIGMOD , pages 193â€“204, 1999.\n[70] H. Wang, X. Fu, J. Xu, and H. Lu. Learned index for spatial queries. In MDM ,\npages 569â€“574, 2019.\n[71] A. Wasay, X. Wei, N. Dayan, and S. Idreos. Data canopy: Accelerating ex-\nploratory statistical analysis. In SIGMOD , pages 557â€“572, 2017.\n[72] E. Wu and S. Madden. Scorpion: Explaining away outliers in aggregate queries.\nPVLDB , 6(8):553â€“564, 2013.\n[73] X. Yu, Y. Xia, A. Pavlo, D. Sanchez, L. Rudolph, and S. Devadas. Sundial:\nharmonizing concurrency control and caching in a distributed oltp database\nmanagement system. PVLDB , 11(10):1289â€“1302, 2018.\n[74] X. Yun, G. Wu, G. Zhang, K. Li, and S. Wang. Fastraq: A fast approach to range-\naggregate queries in big data environments. IEEE Trans. Cloud Computing ,\n3(2):206â€“218, 2015.\n\nPolyFit: Polynomial-based Indexing Approach for Fast Approximate Range Aggregate Queries EDBT2021, Nicosia, Cyprus\nA APPENDIX\nA.1 Approximate Range Aggregate Queries\nvia Learn Index Methods\nThe learned index methods, including RMI [ 44], FITing-tree [ 28],\nand PGM [ 27], are originally designed for range and point queries.\nIn order to support range aggregate queries, e.g., SUMandMAX,\nwith both absolute error and relative error guarantees (cf. Prob-\nlem 1 and Problem 2), we follow the same mechanism of these\nindex structures to fit the curve of either ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘˜)orğ·ğ¹ğ‘šğ‘ğ‘¥(ğ‘˜)\ncf. Equation 7. Instead of finding the exact result for either range\nquery or point query, we utilize our querying methods (i.e.,\nLemma 5.1 to 5.4) to solve both Problem 1 and Problem 2.\nA.2 Tuning the RMI\nRMI [ 44] is a flexible learned index structure, which contains\nmany parameters for tuning this index, including: (1) types of\nmachine learning models, (2) the number of stages in RMI, (3)\nthe number of models for each stage in RMI. Here, we adopt\nthe TWEET dataset (cf. Table 3) to tune the parameters, so as to\nobtain the best performance for RMI.\nA.2.1 Model Selection .In [44], they adopt the neural net-\nwork (NN) with at most two hidden layers and linear regression\n(LR) for testing the performance. Table 6 summarizes the re-\nsponse time and measured relative error, using single model to\nfitğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘˜)in TWEET dataset for approximate SUMquery. Here,\nwe use 1:X:Y:1 to represent the NN architecture with two hidden\nlayers, i.e., X and Y neurons in the first and second hidden layers\nrespectively, where the first and last one denote the input and\noutput respectively. Similarly, we also use 1:X:1 to represent one\nhidden layer of the NN architecture.\nEven though NN model can generally provide accurate fitting\nto the curve ( ğ¶ğ¹ğ‘ ğ‘¢ğ‘š(ğ‘˜)in this experiment), the response time\ncan be much larger, compared with linear regression model. As\nan example, once we choose the shallow NN architecture 1:8:1,\nthe response time can achieve more than 100ns, which can be\nworse than the performance of FITing-tree (cf. Table 5). Due to\nthe inefficiency issue of highly non-linear NN model, we choose\nLR model for RMI.\nTable 6: Comparison of different machine learning mod-\nels\nModel NN Prediction time Measured\narchitecture (ns) relative error (%)\nLR n.a. 20 38\nNN 1:4:1 119 24.1\nNN 1:8:1 189 25.3\nNN 1:16:1 275 25.3\nNN 1:4:4:1 152 24.8\nNN 1:8:8:1 347 21.4\nNN 1:16:16:1 972 23.3\nA.2.2 Tuning RMI Structure .RMI utilize multiple models\n(e.g., LR) to obtain the approximate searching position (cf. Figure\n21). However, due to the large degree of flexibility for RMI, in-\ncluding the number of stages and the number of models for each\nstage in RMI, we only test some of the combinations for choosing\nthe structure of models, i.e., RMI structure.\nFigure 21: RMI structure (Cropped from [44])\nTheoretically, RMI can provide better performance with the\nlarge number of models, in which RMI can consume more mem-\nory resources. As such, we restrict the number of models in the\nleaf level (stage 3 in Figure 21 as an example) of RMI to be ap-\nproximately the same as PolyFit for the sake of fairness. Here,\nwe use 1â†’10â†’X to denote the three-stage RMI structure with\none model in stage 1, ten models in stage 2 and X models in\nstage 3. Similarly, we also use 1 â†’10â†’100â†’Y to denote the\nfour-stage RMI structure. Here, we vary X from 100 to 1000 and\nvary Y from 100 to 1000, by increasing X/Y 100 each time. In\nour experiment, we find that the RMI structure 1 â†’10â†’100\nâ†’1000 can normally provide the smallest query response time,\ncompared with other RMI structures. As such, we choose the\nRMI structure with 1 â†’10â†’100â†’1000 and LR for each model\nin our experiments (cf. Section 7).\nA.3 The Case of Repeated Keys\nIn this scenario, we assume that the records with repeated keys\ninDare arranged in the ascending order on measure, i.e.,\n(ğ‘˜ğ‘–,ğ‘šğ‘–),(ğ‘˜ğ‘–+1,ğ‘šğ‘–+1),Â·Â·Â·,(ğ‘˜ğ‘–+ğ‘¥,ğ‘šğ‘–+ğ‘¥), whereğ‘˜ğ‘–=ğ‘˜ğ‘–+1=...=\nğ‘˜ğ‘–+ğ‘¥. We propose to pre-process the dataset Das follows.\nForSUM,MIN, orMAXqueries, we propose to replace the repeated-\nkey records by a single pair (ğ‘˜ğ‘–,ğ‘¥), whereGis an aggregate\nfunction and ğ‘¥=G({ğ‘šğ‘–,ğ‘šğ‘–+1,Â·Â·Â·,ğ‘šğ‘–+ğ‘¥}).\nForCOUNT queries, we replace the repeated-key records by a\nsingle pair(ğ‘˜ğ‘–,ğ‘¥), whereğ‘¥=ğ¶ğ¹ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘˜ğ‘–)âˆ’ğ¶ğ¹ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘˜ğ‘–âˆ’1). Then,\nduring query evaluation, we execute a SUMquery instead of a\nCOUNT query.\nA.4 The Case of Negative Measure Values\nOur problem definitions in Section 3.1 are applicable to negative\nmeasure values. Nevertheless, the error conditions for query\nevaluation need to be examined and revised in order to preserve\ncorrectness.\nFor the absolute error guarantee, the error conditions (in Lem-\nmas 5.1, 5.3 and 6.3) are directly applicable to negative measure\nvalues.\nIn contrast, for the relative error guarantee, we need to revise\nthe error conditions in our lemmas with respect to negative\nmeasure values.\nFor example, we can replace Lemma 5.2 by the following:\nLemma A.1. IfËœğ´ğ‘ ğ‘¢ğ‘šâ‰¥2ğ›¿(1+1\nğœ€ğ‘Ÿğ‘’ğ‘™)orËœğ´ğ‘ ğ‘¢ğ‘šâ‰¤âˆ’2ğ›¿(1+1\nğœ€ğ‘Ÿğ‘’ğ‘™),\nthen Ëœğ´ğ‘ ğ‘¢ğ‘š satisfies the relative error guarantee with respect to ğœ€ğ‘Ÿğ‘’ğ‘™.\nSimilarly, we can replace Lemma 5.4 by the following:\nLemma A.2. IfËœğ´ğ‘šğ‘ğ‘¥â‰¥ğ›¿(1+1\nğœ€ğ‘Ÿğ‘’ğ‘™)or If Ëœğ´ğ‘šğ‘ğ‘¥â‰¤âˆ’ğ›¿(1+1\nğœ€ğ‘Ÿğ‘’ğ‘™),\nthen Ëœğ´ğ‘šğ‘ğ‘¥ satisfies the relative error guarantee with respect to ğœ€ğ‘Ÿğ‘’ğ‘™.\n\nEDBT2021, Nicosia, Cyprus Zhe Li1, Tsz Nam Chan2, Man Lung Yiu1, Christian S. Jensen3\nFinally, we can replace Lemma 6.4 by the following:\nLemma A.3. IfËœğ´ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡â‰¥4ğ›¿(1+1\nğœ€ğ‘Ÿğ‘’ğ‘™)orËœğ´ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡â‰¤âˆ’4ğ›¿(1+1\nğœ€ğ‘Ÿğ‘’ğ‘™)\nthen Ëœğ´ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ satisfies the relative error guarantee ğœ€ğ‘Ÿğ‘’ğ‘™.\nA.5 The Case of Multiple Keys\nWe now discuss how to extend the techniques in Section 6 to\nsupport queries with multiple ( ğ‘‘>2) keys.\nIt is straightforward to extend the key-cumulative surface (in\nFigure 9b) for the ğ‘‘-dimensional space.\nAccording to Ho et al. [ 38], the COUNT of any rectangular region\ncan be expressed as the sum (and the difference) of 2ğ‘‘precom-\nputed terms. This idea enables us to compute the result in ğ‘‚(2ğ‘‘)\ntime. However, the time complexity increases rapidly when ğ‘‘\nincreases.\nFurthermore, if we apply the quad-tree based partitioning ap-\nproach (in Figure 10) to fulfill the error guarantee ğ›¿, the required\nnumber of partitions (and models) grows exponentially with the\nincrease ofğ‘‘. The reason is that, at a high dimensionality, the\nkey-cumulative surface becomes more complicated, rendering it\nhard to approximate it well.\nA.6 The Case of Parallel Construction\nThe index construction process can be accelerated by parallel\ncomputation.\nFirst, we consider the single key scenario in Section 4. We may\napply a fast heuristics method (e.g., equi-width partitioning) to\ndivide the key domain into ğ‘šintervals:ğ¼1,ğ¼2,Â·Â·Â·,ğ¼ğ‘š. Then, we\ncreateğ‘šthreads, where the ğ‘—-th thread is used to run Algorithm 1\non theğ‘—-th intervalğ¼ğ‘—only. Since these ğ‘šintervals are disjoint, it\nis feasible to run these ğ‘šthreads in parallel. The speedup of this\nmethod is at most ğ‘štimes when compared to the single-thread\nexecution. Nevertheless, if the workload of these ğ‘šthreads are\nnot balanced, the speedup may become much lower than ğ‘š. In\nfuture, we will examine a fast heuristics method that can balance\nthe workload of threads.\nObserve that the above idea can also be extended to the two\nkeys scenario in Section 6. The only difference is that we use a\nheuristic method to partitioning the 2-dimensional domain into\nğ‘šdisjoint regions.",
  "textLength": 77965
}