{
  "paperId": "24e117cf47446d49c66c0c89eb3cc4f80cf7dc38",
  "title": "IRLI: Iterative Re-partitioning for Learning to Index",
  "pdfPath": "24e117cf47446d49c66c0c89eb3cc4f80cf7dc38.pdf",
  "text": "IRLI: Iterative Re-partitioning for Learning to Index\nGaurav Gupta* 1Tharun Medini* 1Anshumali Shrivastava2Alexander J Smola3\nAbstract\nNeural models have transformed the fundamental\ninformation retrieval problem of mapping a query\nto a giant set of items. However, the need for\nefﬁcient and low latency inference forces the com-\nmunity to reconsider efﬁcient approximate near-\nneighbor search in the item space. To this end,\nlearning to index is gaining much interest in recent\ntimes. Methods have to trade between obtaining\nhigh accuracy while maintaining load balance and\nscalability in distributed settings. We propose a\nnovel approach called IRLI (pronounced ‘early’ ),\nwhich iteratively partitions the items by learning\nthe relevant buckets directly from the query-item\nrelevance data. Furthermore, IRLI employs a su-\nperior power-of- k-choices based load balancing\nstrategy. We mathematically show that IRLI re-\ntrieves the correct item with high probability un-\nder very natural assumptions and provides supe-\nrior load balancing. IRLI surpasses the best base-\nline’s precision on multi-label classiﬁcation while\nbeing 5xfaster on inference. For near-neighbor\nsearch tasks, the same method outperforms the\nstate-of-the-art Learned Hashing approach Neu-\nralLSH by requiring only \u00191\n6thof the candidates\nfor the same recall. IRLI is both data and model\nparallel, making it ideal for distributed GPU im-\nplementation. We demonstrate this advantage by\nindexing 100 million dense vectors and surpassing\nthe popular FAISS library by >10% on recall.\n1. Introduction\nFor a given query q, the classical problem in information\nretrieval (IR) is to learn a function fthat mapsqto one\n(or few) of an extensive set (often hundreds of millions) of\ndiscrete item sets. Most practical applications in IR have\n*Equal contribution1Department of Electrical Engineer-\ning, Rice University, Houston, Texas2Department of Com-\nputer Science, Rice University, Houston, Texas3Amazon\nWeb Services, Palo Alto, California. Correspondence\nto: Gaurav Gupta <gaurav.gupta@rice.edu >, Tharun Medini\n<tharun.medini@rice.edu >.data with only the query-item relevance (like the classical\nquery-product purchase data (Nigam et al., 2019; Medini\net al., 2019)). Modern search engines often deploy a pipeline\nin which queries and items are ﬁrst embedded into a dense\nvector space. Here the information retrieval problem gets\nreduced to the Approximate Near Neighbor (ANN) search in\nthe embedding space. Analogs of this approach are explored\nin industry-scale works like SLICE (Jain et al., 2019) (Bing\nSearch) and DSSM (Nigam et al., 2019) (Amazon Search).\nApproximate Near Neighbor (ANN), being one of the most\nstudied algorithms in machine learning, is still far from\nbeing solved to a satisfactory extent in the context of infor-\nmation retrieval. In the past decade, learning-based solu-\ntion for ANN has shown signiﬁcant promise. Surprisingly,\nlearning-based ANN is precisely the same information re-\ntrieval problem of ﬁnding the function fthat mapsqto one\n(or few) of an extensive set (often hundreds of millions) of\ndiscrete neighbors. The hardness of the fact that we are\ndealing with a large item set remains the primary challenge.\nLearning to index has been a sought-after solution to\nthe fundamental challenge of large item space, particu-\nlarly in the context of Approximate Near Neighbor (ANN)\nsearch (Kraska et al., 2018). The idea behind learning to\nindex is to ﬁnd a function fthat maps the query qto a\nreasonably sized Bdiscrete partitions, where Bis much\nsmaller than the number of total items L. The hope is that if\nthe partitions are reasonably balanced, then the function f\nreduces the search space from LtoL\nB, which is manageable\nfor large enough B.\nSeveral approximate algorithms have been proposed for\nlearning to index. These algorithms reduce the search space,\nmainly using space partitioning (Dong et al., 2019) or graph-\nbased methods (Malkov and Yashunin, 2018), or by reduc-\ning the complexity of distance computation such as quantiza-\ntion and lookup table based methods (Johnson et al., 2019).\nGraph-based NNS methods are efﬁcient but limited to small-\nscale datasets. Due to their sequential nature, it is not trivial\nto parallelize the query and indexing process. Given this,\nmany commercial applications that deal with large-scale\ndata increasingly use partitioning based approaches.\nOverall, the current IR pipeline still struggles with two\nsigniﬁcant challenges - 1) Learning embedding from query-\nitem relevance is a pairwise training process leading to aarXiv:2103.09944v1  [cs.IR]  17 Mar 2021\n\nIRLI\nmassive amount of training samples and extended training\ntime. Additionally, negative sampling techniques have to\nbe employed to prevent degenerate solutions, which would\nonly exacerbate the problem for large output spaces. 2) The\npower-law distribution of data causes an imbalanced parti-\ntion of items into the hash buckets. As a result, frequently\nqueried items tend to coalesce in large numbers into a few\nbuckets, leaving the infrequent ones in the remaining buck-\nets. This imbalance leads to higher inference times as we\nquery heavy buckets more often than the lighter ones.\n1.1. Our contributions\n1)We propose a learning-to-index algorithm - IRLI, which\nlearns to partition and map together using a single neural\nnetwork via an alternative training and re-partitioning.\n2)We theoretically show that IRLI achieves superior recall\nwhile maintaining load-balance in convergence.\n3)We surpass the best Learned Indexing baselines on recall\nby probing<20% candidates. We outperform the best Ex-\ntreme Classiﬁcation baselines on precision while inferring\n5\u0002faster.\n4)We index a 100MM sample of the Deep-1B dataset by a\ntrivial distribution of the vectors across eight nodes. With a\n4 ms latency on CPU, we achieve a recall of 90%, which is\na good 10% over the popular FAISS library.\n2. Related works\nThere are several well-known approaches for partitioning\nlike k-means clustering (Wang and Su, 2011), locality sen-\nsitive hashing (LSH) (Shrivastava and Li, 2014; Andoni\net al., 2015; Lv et al., 2007) and tree-based methods like\nPCA trees (Sproull, 1991) and Randomised Partition trees\n(Dasgupta and Sinha, 2013). LSH in particular is a cheap\nand fast solution for high-dimensional data indexing. How-\never, both LSH and k-means generate partitions that have\nextremely skewed load for lop-sided distributed data. The\ntree-based methods on the other hand suffer from the curse\nof dimensionality. Additionally, their hierarchical design di-\nminishes their parallelism during the query and construction\nprocess.\nThe power law distribution issue in traditional indexing has\nwarranted research in load-balanced indexing schemes. An\nearly attempt in this regard is Balanced K-Means (Malinen\nand Fr ¨anti, 2014), which has high construction time dampen-\ning its scalability. A recent noteworthy work is NeuralLSH\n(NLSH) (Dong et al., 2019), which uses KaHIP (Sanders\nand Schulz, 2013), a balanced graph-partitioning algorithm.\nNLSH maps a query to the relevant partition/partitions viaa\nbrute force approach, using the partition-centers’ distance\nwith the query. However, the centroids do not always re-\nﬂect the higher-order statistics of the partitions. Sometimes,\nthey do get drifted by outliers within the paritions. NLSHlearns a model to rank the partitions generated by the k-NN\ngraph. Learning improves the mapping by training on the\ntrue query to partition afﬁnity.\nParabel: In the context of Information Retrieval, Para-\nbel (Prabhu et al., 2018) is one of the primary algorithms\nthat partitions the label space into roughly equal sets via\na balanced 2-means label tree, where the label vectors are\nconstructed using input instances. Subsequent improve-\nments like eXtremeText (Wydmuch et al., 2018) and Bon-\nsai (Khandagale et al., 2020) relax the 2-way partitioning to\nhigher orders of hierarchy.\nSLICE: Another recent work, SLICE, builds an ANN graph\non the label vectors obtained from a pre-trained network.\nIt maps a query to the common embedding space during\ninference and performs a random walk on the ANN graph\nto obtain the relevant labels.\nAll existing approaches decouple the partitioning step from\nthe learning step. Once a partition is created, it is ﬁxed\nfor the rest of the process while we map the query using\neither centroids, hashing, or a learned model. In many cases,\nthe partitioning process is an off-the-shelf algorithm (like\nKaHIP). Our work differs from the prior work primarily in\nthe fact that we alternatively learn both the mapping and\npartitions. We have a single model that maps the query to\nbuckets and also partitions the labels for subsequent train-\ning. To improve the candidate set precision, we repeat this\nprocess in few independent repetitions.\n3. Our Method: IRLI\nIterative Re-partitioning for Learning to Index (IRLI) begins\nwith a random-pooling based index initialization followed\nby an iterative process of alternating train and re-partition\nsteps. We train Rindependent such indexes and use them\nfor efﬁcient item retrieval. Figures 1 and 2 illustrate our\nalgorithm with a toy example of 5buckets.\nNotation: For a given datasetD, we denote a query vector\nbyxand its labels by \u0016y. LetNbe the total number of train\nvectors,dbe the input vector dimension, and Lbe the total\nnumber of labels. For the ANN scenario, L=N.R\nis the number of repetitions (independent indexes), Bis\nthe number of partitions in each repetition, and f(:)is the\nlearned deep-net model (we have Rsuch models).\n3.1. Initialization:\nWe initialise the partitioning randomly. For this, we use R2-\nuniversal hash functions hr: [L]![B]; r2f1;2;::;Rg.\nThe hash function hr(:)uniformly maps the Llabels intoB\nbuckets. As the pooling is randomised, the buckets contain\nan equal number of labels in expectation. If the application\nis near neighbor, the Nvectors gets pooled randomly R\n\nIRLI\nDeep Net\nDeep NetDeep Net\nDeep NetDeep Netb=2 p=0.6\nb=3 p=0.05b=1 p=0.15\nb=4 p=0.08\nb=5 p=0.121\n2\n3\n4p=0.7\ni\nL-2\nL-1\nL\nTrained \nDeep Netb=1\nb=2 Heavy\nb=3 Light\nb=4\nb=5\nInput \nVector1\n2\n3\n4\ni\nL-2\nL-1\nLb=1\nb=2\nb=3\nb=4\nb=5\nR=1R=15R=16\nR=15R=16 R=15R=16\nLabelsLabelsSoftmax \nbucket  probabilities\nInitial Random \nLabel PoolingTraining for \nBucket ScoresRe-partitioning Labelsp=0.65Step2 Step1Iterate Initialization\nFigure 1. We create IRLI index in 3 steps. First (left) (initialization step)- the labels are pooled randomly into Bbuckets using a 2-universal\nhash function. The above ﬁgure shows only ﬁve buckets (while we have a few thousands in practice). Second (middle) - We train R\nfully-connected networks on Ndata points, where any bucket containing at-least one true labels is positive. Third (right) : After training\nfor a few epochs, the labels are re-assigned to the buckets. For each label, we provide a representative input to the Rnetworks. We select\nthe top- Kbuckets and assign the label to the least occupied bucket (K=2 in the ﬁgure yields 2ndand3rdbuckets as the top-scored ones.\nLight-green bucket is the lesser occupied one, and hence we assign the label to the 3rdbucket). A larger Kensures perfect load balance,\nwhile a smaller Kensures higher precision and recall.\nnumber of times into Bpartitions. The ﬁrst part of ﬁgure 1\nshows the initialization.\n3.2. Alternative Training and Re-partitioning:\nTraining: We train a feed forward neural network to learn\nthe afﬁnityfrof a given point xtoBbuckets where B\u001cL.\nWe haveRindependent partitions and thereby Rindepen-\ndent neural networks ffrjr2[1;2;::;R ]g. We are effec-\ntively solving a classiﬁcation problem using the binary cross-\nentropy loss\nL(x;y;B ) =\u0000BX\nb=1yo;blog (pb)\u0000(1\u0000yo;b) log(1\u0000pb)\nby providing Bsoftmax scores ( pb) against the ground-truth\none hot value ( yo;b).yo;b= 1 if there is at-least one true\nlabel present in the bucket b, elseyo;b= 0.\nIn Extreme Multi-Label (XML scenario), the true labels\nare provided with the data. For the ANN datasets, we use\nthe 100 exact near neighbors to a query point (using the\ncorresponding distance metric) as labels. These neighbors\nhave to be generated beforehand.\nRe-partitioning : This is the critical step in IRLI as it\ncreates a partition with more relevant label afﬁnity than the\ncurrent partition. Let us ﬁrst deﬁne label afﬁnity for both theXML and ANN scenarios. The label afﬁnity in the absence\nof a label vector (XML scenario) is deﬁned as:\nDeﬁnition 1 For a given label land a network f(:)withB\noutputs, trained on a dataset x;y2D, the label afﬁnity is\ngiven by\nPl=X\n8l2yif(xi);f(:)2RB\nWhen the label vector is given (ANN scenario) the label\nafﬁnity is deﬁned as:\nDeﬁnition 2 For a given label embedding land a network\nf(:)withBoutputs, the label afﬁnity is given by\nPl=f(l);f(:)2RB\nPlease refer to section 4 for further analysis on the label\nafﬁnity behavior.\nOnce the training is done, a label ( l2 f1;2::Lg) is\nre-partitioned by assigning it to its top afﬁnity bucket\n(argmax P l), in each of the Rrepetitions. It is impor-\ntant to note that, for similar labels, the network will provide\nvery similar label afﬁnities.\nLoad Balancing : In general, a real dataset does not have a\nuniform distribution. Consequently, similarity-based parti-\ntioning leads to an unbalanced load across the buckets. To\n\nIRLI\nAlgorithm 1 IRLI Index Creation\nInput: data(xi;yi)2RN\u0002dand labelsli2f1::Lg\nforr= 1toRdo\nb=hr(li),li2f1::Lg#Initial assignment\nBucket(li) =b\nforepoch = 1toTdo\nLearn bucket scoring- fr(xi)2RB\nforli= 1toLdo\npli=g(li)#get label afﬁnity\nB= topK(pli) #choose top K buckets\nb=argmin Load (B)\nBucket(li) =b#re-assignment\nend for\nend for\nend for\n#Label afﬁnity pli=g(li)deﬁnition\nifli==xithen\npli=f(xi)#functiong(:)andf(:)is the same\nelse\npli=Sum(f(xi)8(xi;yi)s:t:l i2yi) #gives the ef-\nfective buckets’ probability score\nend if\novercome this, we select Ktop afﬁnity buckets for each\nlabel instead of 1. We choose the least occupied bucket\namong these top-scoring buckets and assign the label to\nit. This ensures that the label ﬁlls the lighter bucket ﬁrst\nto keep up with the load of the top buckets of similar la-\nbels. As we observe later in section 5, we will only need\na smallKto maintain a near-perfect load balance. For ex-\nample, on GloVe100 dataset, for B= 5000 , we only need\nk= 10 buckets to achieve a load variance of approx 3as\ncompared to 15for random bucket assignment (lower the\nload-variance, better the balance).\nAbsence of label embedding : In the case of ANN datasets,\nlabel vectors are given beforehand. On the other hand, if a\nlabel embedding is not given, we need an equivalent of it to\npass through the learned network and get the label afﬁnity.\nFor such cases, we retrieve the bucket scores for a label\nusing its data points (as shown in deﬁnition 1). For each\nlabell, we use the sum of the corresponding data vectors’\nsoftmax probability.\nWe re-assign labels once every few training epochs (once\nevery ﬁve epochs in our experiments). We alternate between\nthe training and re-partitioning steps until the number of\nnew assignments converges to zero.\n3.3. Inference/Query:\nAfter training, we store the trained models and inverted\nindexes for all Rrepetitions. During the query process, a\nvectorq2Rdis passed through Rtrained nets indepen-Algorithm 2 IRLI Index Query\nInput: Modelsfr(:), IRLI \u0005r;r2f1::Rg\nQuery point: q2Q\nforr= 1toRdo\nB[1 :m;r] =topm(fr(q))\nend for\nforb=B[1;1]toB[m;R ]do\n\u001e=\u001e[InvIndex (b)\nend for\nCandidate set = FrequentOnes (\u001e)\n0.7\nInput V ectorBucket \nProbabilities1 2 i nLabels\n0.1 0.09AggregationSet of \nCandidate Labels\n0.15 0.01 0.28 0.06 0.5 0.1 0.01 0.15 0.3 0.04 0.06 0.453 3 n-2iFrequency \nthreshold\nModel 1 Model 2 Model R\nFigure 2. IRLI query process. Here the query vector is passed\nthrough Rtrained models, and each one gives the probability\nscores over the corresponding buckets. Figure shows m= 1for\nillustration purpose. The top candidates are sorted based on the\naggregated scores of each label.\ndently in parallel, where each one gives a Bdimensional\nprobability vector.\nWe select the top- mbuckets (mis5\u000010ifBis around\n5000 ) from each model. This gives a total of m\u0002Rbuck-\nets to probe. A union of points/labels in these buckets is\nthe target candidate set. Additionally, we count each can-\ndidate’s frequency of occurrence in the total m\u0002Rsets.\nA higher frequency of a candidate label signiﬁes a higher\nrelevance to the query point. In the end, we keep only the\nhigher relevance candidates by ﬁltering and rejecting the\ncandidates below a certain frequency threshold from the\npool. Please refer to Figure 2 for an illustrative view of the\nquery process.\nTwo crucial points to note : 1) Our procedure will ensure\nthat every network has \u000ehigher probability of selecting a\nrelevant label than it can with learning on any predeﬁned ran-\ndom partitioning. We prove this in section 4. 2) Candidate\nset selection from Rrepetition and frequency-based ﬁlter-\ning exponentially decrease the variance of our true labels\nestimates. This is analyzed further in Appendix.\n\nIRLI\n4. Analysis\nIn this section, we theoretically analyze IRLI from two main\nperspectives. First, we show that the predicted probability\nof buckets corresponding to the relevant labels increases\nafter re-assigning the labels. Second, analogous to the pop-\nular power-of-2-choices, we show that the process of re-\nassigning to the least occupied of the top- Kbuckets is the\noptimal strategy to ensure load balance across the buckets.\nAs mentioned before, we have Lclasses being hashed to\nBbuckets using a universal hash function. This randomly\npartitions the classes into Bmeta-classes. We estimate the\ntop bucket probability maxPr (b=x)for a input vector x,\nwhereb2f1;2::Bg. Since each of the Rrepetitions is\nan instantiation of the same process, we only need an R-\nagnostic proof for the fact that re-assignment enhances the\nprediction probability of the most relevant bucket.\nTheorem 1 For a given dataset with x2RN\u0002d, and its\nlabell, the expected afﬁnity of the input query point xwith\nlincreases by a margin of \u000e>0after re-partitioning, i.e.,\nE\u0010\nP0\nx;h0(l)\u0011\n=E\u0000\nPx;h(l)\u0001\n+\u000e\nwherePx2RBis the bucket afﬁnity vector of x.\nThe increment in the query afﬁnity, results in increment of\nquality of the retrieved labels.\nProof Letxbe an input vector whose label set is \u0016y. Letpl\ndenote the probability of lbeing a true label to x.\nLet the current partitioning be given by a mapping h(l),\nwhereh(l)2f1;2;::;Bg. Also, assume l1;l22\u0016yand\nl3=2\u0016y.\nThe afﬁnity score of xforh(l1)thbucket is given by the\nsummation of probability of label l1and probability of other\nlabels in the same bucket, i.e.,\nPx;h(l1)=pl1+X\nk6=l11h(k)=h(l1)pk\n1is the indicator function. Now, let us reassign the la-\nbels as per section 3.2. Let the new partition be given by\nh0(:). Ifh(l1)6=h(l2)andh(l1) =h(l3), we want the\nre-partitioning to reverse this adversarial scenario, i.e., we\nexpect that\nh0(l1) =h0(l2)andh0(l1)6=h0(l3)\nLetZrepresent the event of l3being removed from l1’s\nbucket andl2being added to it.\nP0\nx;h0(l1)=pl1+X\nk6=l11h0(k)=h0(l1)pk+1Z(pl2\u0000pl3)Expected afﬁnity is given by-\nE\u0010\nPx;h0(l1)\u0011\n=E\u0000\nPx;h(l1)\u0001\n+E(Z)(pl2\u0000pl3)\nAsl1andl2are high scoring label of data point x,pl1=\npl2> pl3. Also E(Z)>0when there is a possible reas-\nsignment. and hence\nE\u0010\nPx;h0(l1)\u0011\n=E\u0000\nPx;h(l1)\u0001\n+\u000e\nwhere\u000e>0.\nThe main implication of the above theorem is that the bucket\ncontaining the relevant label pl1gets higher aggregated afﬁn-\nity as it will have other true labels with higher probability.\nIt is important to note that this afﬁnity increment is only\nmanifested after retraining on this new partitioning of the\nlabels. The re-partitioning alone does not affect any afﬁnity\nlearned by the neural net.\nThe increased afﬁnity directly increases the recall during\nthe evaluation. Given Rreps, the estimated afﬁnity of xis\ngiven by ^Px=1\nRPR\nr=1Pr\nx. With increasing Rthe error in\ncorrect label estimation also decreases exponentially.\nThe same holds for the ANN scenario where the labels are\nthe near true neighbors generated for the IRLI indexing.\nTheorem 2 Consider the process where at each step, a\nlabel is chosen independently and uniformly at random and\nis inserted into the index. Each new label linserted in the\nindex chooses K >K 0possible destination bins which are\nthe top-Kindices ofPl, and is placed in the least full of\nthese bins. For a sufﬁciently large t, the most crowded bin\nat timetcontains fewer thanlog(log( L)+f1(K))\nlog(K)+O(1) +\nf2(K)labels with high probability, where f1andf2are\nmonotonically decreasing functions of K.\nProof: Proof of this theorem is given in Appendix. It\ndraws parallels from the popular power-of-2-choices frame-\nwork (Mitzenmacher, 2001).\n5. Experiments\n5.1. Multi-label Classiﬁcation\nDatasets: We use the dense versions of Wiki-500K and\nAmazon-670K (Jain et al., 2019) datasets available on\nthe Extreme classiﬁcation repository (Bhatia et al., 2016).\nThe sparse versions of these datasets were scaled down\nand densiﬁed using XML-CNN (Liu et al., 2017) features.\nBoth the datasets have 512dimension vectors. Wiki-500K\nhas501;070 classes with 1;646;302 train points and\nAmazon-670K has 670;091 classes with 490;449 train\npoints.\n\nIRLI\nWiki-500K Amazon-670K\nMethod P@1 P@3 P@5 P@1 P@3 P@5\nMain BaselinesNeural Indexing (10 buckets) 60.77 46.09 43.49 35.56 32.68 31.02\nNeural Indexing (5 buckets) 60.69 45.78 43.15 35.13 32.20 30.58\nSLICE 59.89 39.89 30.12 37.77 33.76 30.7\nParabel 59.34 39.05 29.35 33.93 30.38 27.49\nOther BaselinesAnnexML 56.81 36.78 27.45 26.36 22.94 20.59\nPfastre XML 55.00 36.14 27.38 28.51 26.06 24.17\nSLEEC 30.86 20.77 15.23 18.77 16.5 14.97\nTable 1. Precision @1, @3, @5 for N-Index on Wiki-500K and Amazon-670K vs popular Extreme Classiﬁcation benchmarks.\nDataset Wiki-500K Amz-670K\nN-Index (m=10) 0.56 1.08\nN-Index (m=5) 0.47 0.76\nSLICE 1.37 3.49\nParabel 2.94 2.85\nPfastreXML 6.36 19.35\nTable 2. Inference speeds against the fastest Extreme Classiﬁcation\nbenchmarks.\nNetwork Parameters: Each of the R= 32 models has\nan input layer of 512 dimensions, a hidden layer of 1024\ndimensions, and an output layer of B= 20000 . We train\nthese networks for 30 epochs and re-assign the labels every\nﬁve epochs.\nMetrics: We measure precision at 1,3,5 (denoted by P@1,\nP@3,P@5). As there are no label vectors provided, we\nuse the corresponding points for each label to re-partition\n(as explained in section 3). Here we pay an additional\nre-partitioning cost of O(L).\nHardware and framework: The experiments were done\non a DGX machine with 8 NVIDIA-V100 GPUs. We train\nwith Tensorﬂow (TF) v1.14 library. We use TF Records\ndata streaming to reduce GPU idle time.\nBaselines : We compare IRLI with Parabel (Prabhu et al.,\n2018), SLICE (Jain et al., 2019), AnnexML (Tagami, 2017),\nPfast XML (Jain et al., 2016) and SLEEC (Bhatia et al.,\n2015).\nResults : Tables 1 and 2 provide the precision and inference\ntime comparison of 2 IRLI variants ( m= 5andm= 10 )\nwith all the baselines. We analyze the precision and runtime\nduring inference after selecting the top m= 5;10buckets\nfrom each of the 32independent indexes. While the best\nlabels are expected to be present in the topmost bucket, werelax this by querying the top 5/10 buckets per index. We\ncan observe that IRLI Index gives the best precision and\nruntime, beating all baselines for Wiki-500k dataset. On the\nAmazon-670K, it is faster and more precise on P@5 metric\nthan the baselines in comparison.\n5.2. Nearest Neighborhood Search\nDataset: We have used two million-scale datasets from\nANN benchmarks (Aum ¨uller et al., 2020)- Glove100\n(Pennington et al., 2014) and Sift1M. Glove100 has total\n1;183;514 points, each a 100 dimensional vector and\ntrained with angular distance metric. Sift-1M has exactly 1\nMM points, each a 128dimensional vector and trained with\nthe euclidean distance metric.\nHyper-parameters: Each of theRmodels are simple feed\nforward networks with an input layer of 100 or 128 (Glove\nvs SIFT), one hidden layer of 1024 neurons and and output\nlayer ofB= 5000 .\nHardware and framework: The experiments were done\non a DGX machine with 8 NVIDIA-V100 GPUs. We train\nwith Tensorﬂow (TF) v1.14 library. We use TF Records\ndata streaming to reduce GPU idle time.\nBaselines: We compare IRLI against the popular learned\npartitioning methods like Neural LSH, kmeans+Neural\nCatalyzer (Sablayrolles et al., 2018) and Cross-Polytype\nLSH. All baselines are measured with and without hierarchy\n(1-level and 2-level). Please refer to ﬁgure 3 for the detailed\ncomparison plots. For every baseline in the ﬁgure, a tag\nof1\u0000256denotes 1 level, 256 buckets, while a tag of\n2\u0000256denotes two levels, 256 buckets (65536 leaf nodes\neffectively).\nMetrics: Our metric of interest is the recall of the top 10\nneighbors for a particular candidate size. To be precise,\n\nIRLI\nFigure 3. Above: Comparison of IRLI with other partitioning\nmethods on Glove100 dataset: The red curve represents Re-\ncall10@10 vs the number of candidates (number of true distance\ncomputations) for IRLI. The blue lines represent different vari-\nants of the primary baseline NLSH. The best variant of NLSH (2\nlevel-256 bins, 65536 leaf nodes) has no reported recall at larger\ncandidate sizes. All baselines have results at both 1-level and\n2-level hierarchies. Balanced k-means, LSH and Catalyzer, are\nnoticeably worse than IRLI and NLSH. Below: Comparison of\nIRLI with other partitioning methods on SIFT128 dataset. All\nbaselines, including the SOTA NLSH have noticeably worse recall\nthan IRLI.\nboth ILRI and the baselines only provide a set of candidate\npoints within which we compute true distance computations\nto obtain the top 10 closest points. The intersection of this\nset with the true top-10 neighbors (recall10@10) is the\nprimary metric of our interest.\nResults Figure 3 show the comparison with Neural-LSH\nand other baselines. We can notice that the IRLI (red curve)\ncomfortably surpasses all baselines for a given candidate\nsize. The only baseline conﬁguration that comes close to\nIRLI is NLSH with a 2-level 256 bin conﬁguration, where\nthe2ndlevel has 65536 classes and uses k-means clustering\ninstead of a neural network prediction.5.2.1. A BLATION STUDY\nLoad-Balance: As mentioned earlier, K= 10 is an em-\npirical sweet spot for retaining precision while ensuring\nload-balance during bucket re-assignment. Table 3 shows\nthe standard deviation of the bucket load for Glove100 for\nvarious values of K. We start with a random partition of\npoints (using a 2-universal hash function) and train for 5\nepochs, after which we reassign the 1.2 M vectors as ex-\nplained before. We can observe that as Kincreases, each\nbucket has nearly the same number of candidates. How-\never, a larger Kmight compromise the relevance of buckets.\nHence we chose K= 10 as an appropriate trade-off.\nRandom Partition K=5 K=10 K=25\n15.3 17.08 2.66 0.46\nTable 3. Standard Deviation of load vs Kfor Glove-100 with B=\n5000 (mean = 236 :7). Larger Kgets better load balance while\nsmaller Kgets better precision and recall. Although K= 25\nachieves near perfect load balance, K= 10 is the practical choice\nfor all our experiments for better precision.\nEpoch-wise Performance: Figure 4 shows the epoch-\nwise recall for Glove100 dataset. We can observe that the\nrecall converges after epoch 25, justifying our choice to train\nfor30epochs. The improvement in recall as we increase\nRfrom 16 to 32 is signiﬁcant. Beyond R= 32 , we do not\nobserve any considerable gain with more repetitions.\nFigure 4. Epoch-wise R10@10 for Glove100, with K= 25 .\nFurther, keeping R= 32 and probing top m= 100 buckets\nfor a point in each repetition, we measure the number of\ncandidates that appear in at least 4 of the 32 repetitions. As\nwe train more, we expect this candidate set to increase in\nsize as we group more relevant candidates together. Table 4\nconﬁrms that trend.\n5.3. Data and Model Distributed KNN on 100M points\nThis section demonstrates the scalability of IRLI by parti-\ntioning a 100MM subsample of Deep-1B dataset (Babenko\n\nIRLI\nEpoch-5 10 15 20 25 30\n5011 19151 36252 43605 47072 48969\nTable 4. Candidates appearing atleast 4 of 32 repetitions.\nand Lempitsky, 2016) and achieving sub\u00005mslatency\nwith aR10@10 of96:16%. In the prior cases, each of the R\nindependent models catered to the entire set of labels/data\npoints. In this case, we distribute the 100MM points across\nP= 8disjoint nodes. This choice of Pnodes was made to\nmaximize the use of all GPUs on our machine.\nDataset: Deep-1B is an image indexing dataset compris-\ning of 1 billion 96-dimensional image descriptors. These de-\nscriptors were generated from the last layer of a pre-trained\nconvolutional neural network as discussed in (Babenko and\nLempitsky, 2016). Indexing datasets of this scale is an uphill\ntask and the primary GPU friendly algorithm that accom-\nplishes this is the popular FAISS library (Johnson et al.,\n2019).\nDeep-1B also provides 350 MM additional training vectors\nof which we subsample 10 MM vectors to train all models\nacross the 8 nodes.\nHyper-parameters and distribution details: As men-\ntioned earlier, each of the 8 nodes caters to only 12.5 MM\npoints. For each node, these 12.5 MM points are parti-\ntioned intoB= 20000 buckets. Unlike the previous cases\n(R= 32 ), we choose R= 4 for each node. This would\nagain lead to a total of P\u0003R= 32 models. Each model\nis a simple feed-forward network with an input dimension\nof 96 and a hidden layer with 1024 neurons. We train for\na total of 20 epochs and reassign the points to the buckets\nonce every 5 epochs.\nHardware and framework: We use a server equipped\nwith 64 CPU cores and 8 Quadro GPUs each with 48 GB\nmemory. However, our 32 models have a combined parame-\nter set of 660MM ﬂoat 32values requiring just 2:56GB of\nGPU memory (and 2x auxiliary momentum parameters for\nAdam optimizer). Like with the previous cases, we train\nusing Tensorﬂow v1.14.\nResults: Figures 5 and 6 compare IRLI against FAISS\n(Johnson et al., 2019) on GPU and CPU respectively (please\nnote the IRLI uses GPU only for the neural network pre-\ndiction. Rest of the aggregation process is done entirely\non CPU). We can observe that FAISS plateaus at 0.8 recall\n(corroborated by the reported result in (Johnson et al., 2019)\ntoo). FAISS does product quantization on the 96 dimensions\ndata vectors using 65536 clusters. In Figure 5, PQ32refers\nto a 32 splits of a data vector while PQ48refers to a 48 split.\nEach vector of the codebook is a byte long. Higher-order\nFigure 5. Recall vs Time comparison of 8-way distributed IRLI\n(red curve) with FAISS (blue and green) on GPU. BtSz stands for\nBatch Size. The trade-off of IRLI is governed m. We vary m\nfrom 1\u000020while keeping R= 4. The inference time ranges\nfrom 0.637 ms (lowest recall) to 4.958 ms (highest recall) per point\n(solid red curve). FAISS clearly plateaus on recall very soon while\nIRLI saturate at almost 100% as we afford more inference time.\nFigure 6. Recall vs Time comparison of 8-way distributed IRLI\n(red curve) with FAISS (blue and green) on CPU. As with ﬁgure 5,\nwe vary mfrom 1\u000020while keeping R= 4.\nsplits yield poor recall while PQ96(a quantized version of\nfull product computation) ran out of memory on our GPU.\nDuring inference, the recall vs. time trade-off is governed\nbym(the number of top-scoring buckets that we probe\namong theB). The red curve in ﬁgure 5 goes from an infer-\nence time of 0:637ms to 4:958ms withmranging from 1to\n20. The average candidate set size on which we compute\ntrue distances ranges from 19:8Kto349K(the union of\ncandidates across 8nodes).\n\nIRLI\nReferences\nAlexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razen-\nshteyn, and Ludwig Schmidt. Practical and optimal lsh\nfor angular distance. arXiv preprint arXiv:1509.02897 ,\n2015.\nMartin Aum ¨uller, Erik Bernhardsson, and Alexan-\nder Faithfull. Ann-benchmarks: A benchmark-\ning tool for approximate nearest neighbor algo-\nrithms. Information Systems , 87:101374, 2020. ISSN\n0306-4379. doi: https://doi.org/10.1016/j.is.2019.02.\n006. URL http://www.sciencedirect.com/\nscience/article/pii/S0306437918303685 .\nArtem Babenko and Victor Lempitsky. Efﬁcient indexing of\nbillion-scale datasets of deep descriptors. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 2055–2063, 2016.\nK. Bhatia, K. Dahiya, H. Jain, A. Mittal, Y . Prabhu,\nand M. Varma. The extreme classiﬁcation repos-\nitory: Multi-label datasets and code, 2016. URL\nhttp://manikvarma.org/downloads/XC/\nXMLRepository.html .\nKush Bhatia, Himanshu Jain, Purushottam Kar, Manik\nVarma, and Prateek Jain. Sparse local embeddings for\nextreme multi-label classiﬁcation. In Advances in neural\ninformation processing systems , pages 730–738, 2015.\nSanjoy Dasgupta and Kaushik Sinha. Randomized partition\ntrees for exact nearest neighbor search. In Conference on\nLearning Theory , pages 317–337. PMLR, 2013.\nYihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner.\nLearning space partitions for nearest neighbor search.\narXiv preprint arXiv:1901.08544 , 2019.\nDaniel G Horvitz and Donovan J Thompson. A general-\nization of sampling without replacement from a ﬁnite\nuniverse. Journal of the American statistical Association ,\n47(260):663–685, 1952.\nHimanshu Jain, Yashoteja Prabhu, and Manik Varma. Ex-\ntreme multi-label loss functions for recommendation, tag-\nging, ranking & other missing label applications. In\nProceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining ,\npages 935–944, 2016.\nHimanshu Jain, Venkatesh Balasubramanian, Bhanu Chun-\nduri, and Manik Varma. Slice: Scalable linear ex-\ntreme classiﬁers trained on 100 million labels for related\nsearches. In WSDM ’19, February 11–15, 2019, Mel-\nbourne, VIC, Australia . ACM, February 2019. Best Paper\nAward at WSDM ’19.Jeff Johnson, Matthijs Douze, and Herv ´e J´egou. Billion-\nscale similarity search with gpus. IEEE Transactions on\nBig Data , 2019.\nSujay Khandagale, Han Xiao, and Rohit Babbar. Bonsai:\ndiverse and shallow trees for extreme multi-label classiﬁ-\ncation. Machine Learning , 109(11):2099–2119, 2020.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and\nNeoklis Polyzotis. The case for learned index structures.\nInProceedings of the 2018 International Conference on\nManagement of Data , pages 489–504, 2018.\nJingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and Yim-\ning Yang. Deep learning for extreme multi-label text\nclassiﬁcation. In Proceedings of the 40th International\nACM SIGIR Conference on Research and Development\nin Information Retrieval , pages 115–124, 2017.\nQin Lv, William Josephson, Zhe Wang, Moses Charikar,\nand Kai Li. Multi-probe lsh: efﬁcient indexing for high-\ndimensional similarity search. In 33rd International Con-\nference on Very Large Data Bases, VLDB 2007 , pages\n950–961. Association for Computing Machinery, Inc,\n2007.\nMikko I Malinen and Pasi Fr ¨anti. Balanced k-means for clus-\ntering. In Joint IAPR International Workshops on Statisti-\ncal Techniques in Pattern Recognition (SPR) and Struc-\ntural and Syntactic Pattern Recognition (SSPR) , pages\n32–41. Springer, 2014.\nYu A Malkov and Dmitry A Yashunin. Efﬁcient and robust\napproximate nearest neighbor search using hierarchical\nnavigable small world graphs. IEEE transactions on\npattern analysis and machine intelligence , 42(4):824–\n836, 2018.\nTharun Kumar Reddy Medini, Qixuan Huang, Yiqiu Wang,\nVijai Mohan, and Anshumali Shrivastava. Extreme clas-\nsiﬁcation in log memory using count-min sketch: A case\nstudy of amazon search with 50m products. In Advances\nin Neural Information Processing Systems 32 , pages\n13265–13275. 2019.\nMichael Mitzenmacher. The power of two choices in ran-\ndomized load balancing. IEEE Transactions on Parallel\nand Distributed Systems , 12(10):1094–1104, 2001.\nPriyanka Nigam, Yiwei Song, Vijai Mohan, Vihan Laksh-\nman, Weitian Ding, Ankit Shingavi, Choon Hui Teo, Hao\nGu, and Bing Yin. Semantic product search. In Proceed-\nings of the 25th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining , pages 2876–\n2885, 2019.\n\nIRLI\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. Glove: Global vectors for word representation.\nInEmpirical Methods in Natural Language Processing\n(EMNLP) , pages 1532–1543, 2014. URL http://www.\naclweb.org/anthology/D14-1162 .\nYashoteja Prabhu, Anil Kag, Shrutendra Harsola, Rahul\nAgrawal, and Manik Varma. Parabel: Partitioned label\ntrees for extreme classiﬁcation with application to dy-\nnamic search advertising. In Proceedings of the 2018\nWorld Wide Web Conference , pages 993–1002, 2018.\nAlexandre Sablayrolles, Matthijs Douze, Cordelia Schmid,\nand Herv ´e J´egou. Spreading vectors for similarity search.\narXiv preprint arXiv:1806.03198 , 2018.\nPeter Sanders and Christian Schulz. Think locally, act glob-\nally: Highly balanced graph partitioning. In International\nSymposium on Experimental Algorithms , pages 164–175.\nSpringer, 2013.\nAnshumali Shrivastava and Ping Li. Densifying one permu-\ntation hashing via rotation for fast near neighbor search.\nInInternational Conference on Machine Learning , pages\n557–565. PMLR, 2014.\nRobert F Sproull. Reﬁnements to nearest-neighbor search-\ning in k-dimensional trees. Algorithmica , 6(1):579–589,\n1991.\nYukihiro Tagami. Annexml: Approximate nearest neighbor\nsearch for extreme multi-label classiﬁcation. In Proceed-\nings of the 23rd ACM SIGKDD international conference\non knowledge discovery and data mining , pages 455–464,\n2017.\nJuntao Wang and Xiaolong Su. An improved k-means clus-\ntering algorithm. In 2011 IEEE 3rd international con-\nference on communication software and networks , pages\n44–46. IEEE, 2011.\nMarek Wydmuch, Kalina Jasinska, Mikhail Kuznetsov,\nR´obert Busa-Fekete, and Krzysztof Dembczynski. A no-\nregret generalization of hierarchical softmax to extreme\nmulti-label classiﬁcation. In S. Bengio, H. Wallach,\nH. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-\nnett, editors, Advances in Neural Information Processing\nSystems , volume 31, pages 6355–6366. Curran Asso-\nciates, Inc., 2018. URL https://proceedings.\nneurips.cc/paper/2018/file/\n8b8388180314a337c9aa3c5aa8e2f37a-Paper.\npdf..\n\nAppendix\nLemma 1 Consider a process where we sample Kelements\nfrom a universe of Belements without replacement, with\narbitrary probabilities p1;p2;:::;p B. Denote the score Tof\nthis sample set to be the mean of the Ksampling probabili-\nties. The variance of Tis given by\n1\nK2KX\ni=1p2\ni(1\u0000pi)\nProof: Suppose we are to measure a characteristic xifor\ntheKelements in the sample.\nLet us deﬁne T=PK\ni=1xi\npi. Then, as shown in the theory\nof importance sampling (Horvitz and Thompson, 1952), we\nhave\nVar(T) =T2\u0000KX\ni=1x2\ni\npi\u0000KX\ni6=jxixj\npipj\nIn this theorem, by choosing xi=p2\ni\nK, we haveT=\n1\nKPK\ni=1piand\nVar(T) =(PK\ni=1pi)2\nK2\u0000PK\ni=1p3\ni\nK2\u0000PK\ni6=jpipj\nK2\n=)Var(T) =1\nK2(X\np2\ni\u0000X\np3\ni))\n=)Var(T) =1\nK2(X\np2\ni\u0000X\np3\ni))\nIt is easy to see that Var(T)is a monotonic decreasing\nfunction (say fm(K)) ofKas it goes down faster than\nO(1=K).\nImplications: Please recall that in IRLI, for any input, we\npick the top- Kbuckets of the Bbased on the probability\nscores. Hence, of all the B-choose-Kcombinations, the K-\ntuple that we pick has the maximum mean of probabilities\ncompared to every other K-tuple (vice-versa, the K-tuple\nhaving the maximum mean of probabilities is also the one\nwith all the top- Kbuckets).\nLemma 1 shows that the variance of the mean of the proba-\nbilities of the K-tuple decreases with K.This simulates a virtual random process of picking any tuple\nwith equal likelihood and leads to the following theorem.\nTheorem 2: Consider the process where at each step, a\nlabel is chosen independently and uniformly at random and\nis inserted into the index. Each new label linserted in the\nindex chooses K >K 0possible destination bins which are\nthe top-Kindices ofPl, and is placed in the least full of\nthese bins. For a sufﬁciently large t, the most crowded bin\nat timetcontains fewer thanlog(log( L)+f1(K))\nlog(K)+O(1) +\nf2(K)labels with high probability, where f1andf2are\nmonotonically decreasing functions of K.\nProof: Let the number of bins with load \u0015iat the end of\ntimeL(i.e., the end of the re-partitioning) to be less than\n\fi.\nGiven that we know #bins\u0015i(L)\u0014\fi, we need to ﬁnd an\nupper-bound #bins\u0015i+1(L)to ﬁnd the max of bins.\nIn the event of a collision, consider that each label stacks\nup the on the existing labels like a tower. The height\nof a label in that case is the number of labels below it.\nLet#labels\u0015i(t)represent the number of labels that have\nheight\u0015iafter totaltinsertions.\nPlease note that #labels\u0015i(t)is always higher than the\n#bins\u0015i(t), as each bin with\u0015ihas atleast one label with\nheight\u0015i.\nFor a new label to land at height \u0015i+ 1, allKbins (that\nwe pick) should have load of atleast i. With the assumption\nmade in (Mitzenmacher, 2001), where the Kbins are chosen\nrandomly, the probability of choosing Kbins that have\nheight\u0015iis at most\npi=\u0010\fi\nB\u0011K\nSince the number of bins with2L\nBcan atmost beB\n2, we have\n\f2L\nB\u0014B\n2.\nHowever, we select the top Kbins based on the maximum\nafﬁnity scores for a given query vector (instead of random\nKbins). In this case, for any sufﬁciently large K\u0015K0,\nthe probability piis atmost\npi\u0014\u0010\fi\nB\u0011K\n+\u000e\nwhere\u000eis monotonically decreasing function of K(by\n\nIRLI\ninvoking Lemma 1 ).\n\u000e=fm(K)\nPardon the abuse of notation, please don’t confuse this with\nthe\u000eused in Theorem 1.\nHence thetthlabel has height\u0015i+ 1 with probability\natmostpi. Number of labels that have height \u0015i+ 1is\natmostLpi. For a ﬁxed IRLI index parameters L>B . We\ncan safely assume that L=B\nc, wherec<1.\n\fi+1=Lpi=B\nc\u0010\u0010\fi\nB\u0011k\n+\u000e\u0011\nJust like the case of random Kselection, we can set \f2=c\u0014\nB=2 +\u000e. We now ﬁnd an expression for \f2=c+1using\ninduction\n\f2=c+1=B\nc\u00101\n2K+\u000e1\u0011\n\f2=c+2=B\nc\u00101\n2K2cK+\u000e2\u0011\nHere each\u000eiis a positive real number, monotonically de-\ncreasing inK. The\f2=c+iis given by\n\f2=c+i=B\nc\u00101\n2KicKi\u00001+\u000ei\u0011\nGoing by the deﬁnition of \fi, if\fi\u00141, thenirepresents\nthat maximum load across all the buckets. That happens\nwhenB\nc\u00101\n2KicKi\u00001+\u000ei\u0011\n\u00141\ni= logK\u0010\nlog\u0010L\n1\u0000L\u000ei\u0011\u0011\n\u0000logK\u0010\n1 +1\nKlogc\u0011\nThis can be further simpliﬁed into\ni\u0003= 2=c+i=log\u0010\nlogL+f1(K)\u0011\nlogK+f2(K) + 2L=B\nWheref1(K) =\u0000log(1\u0000L\u000ei)andf2(K) = logK(1 +\n1\nKlogc). As\u000eidecreases with K,f1(:)also monotonically\ndecreases.\nAnd using the fact that the derivative of\nlog(1 +logc\nx)\nlogx\nis negative for x>1, we can conclude that f2(:)is also a\ndecreasing function of K.Note that, when K=B, we are choosing least occupied\nbin from all Bbins. Irrespective of any picking up strategy,\nf1(B)will be zero. The bins will be most balanced. With\nincreasingK, the load balance increases. However, the\nchance of label reassignment to a high afﬁnity bin goes down\nand hence reduces the near-neighbor property of partitions.\nThe value of Kused in our experiments is an empirically\nfound optimum.",
  "textLength": 42621
}