{
  "paperId": "0539535989147bc7033f4a34931c7b8e17f1c650",
  "title": "The Case for Learned Index Structures",
  "pdfPath": "0539535989147bc7033f4a34931c7b8e17f1c650.pdf",
  "text": "The Case for Learned Index Structures\nTim Kraska\u0003\nMIT\nCambridge, MA\nkraska@mit.eduAlex Beutel\nGoogle, Inc.\nMountain View, CA\nalexbeutel@google.comEd H. Chi\nGoogle, Inc.\nMountain View, CA\nedchi@google.com\nJeffrey Dean\nGoogle, Inc.\nMountain View, CA\njeff@google.comNeoklis Polyzotis\nGoogle, Inc.\nMountain View, CA\nnpolyzotis@google.com\nAbstract\nIndexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record\nwithin a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted\narray, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research\npaper, we start from this premise and posit that all existing index structures can be replaced with other\ntypes of models, including deep-learning models, which we term learned indexes . The key idea is that\na model can learn the sort order or structure of lookup keys and use this signal to effectively predict\nthe position or existence of records. We theoretically analyze under which conditions learned indexes\noutperform traditional index structures and describe the main challenges in designing learned index\nstructures. Our initial results show, that by using neural nets we are able to outperform cache-optimized\nB-Trees by up to 70% in speed while saving an order-of-magnitude in memory over several real-world\ndata sets. More importantly though, we believe that the idea of replacing core components of a data\nmanagement system through learned models has far reaching implications for future systems designs and\nthat this work just provides a glimpse of what might be possible.\n1 Introduction\nWhenever efﬁcient data access is needed, index structures are the answer, and a wide variety of choices exist\nto address the different needs of various access patterns. For example, B-Trees are the best choice for range\nrequests (e.g., retrieve all records in a certain time frame); Hash-maps are hard to beat in performance for\nsingle key look-ups; and Bloom ﬁlters are typically used to check for record existence. Because of their\nimportance for database systems and many other applications, indexes have been extensively tuned over the\npast decades to be more memory, cache and/or CPU efﬁcient [36, 59, 29, 11].\nYet, all of those indexes remain general purpose data structures; they assume nothing about the data\ndistribution and do not take advantage of more common patterns prevalent in real world data. For example,\nif the goal is to build a highly-tuned system to store and query ranges of ﬁxed-length records over a set of\ncontinuous integer keys (e.g., the keys 1 to 100M), one would not use a conventional B-Tree index over\nthe keys since the key itself can be used as an offset, making it an O(1)rather thanO(logn)operation to\nlook-up any key or the beginning of a range of keys. Similarly, the index memory size would be reduced\n\u0003Work done while author was afﬁliated with Google.\n1arXiv:1712.01208v3  [cs.DB]  30 Apr 2018\n\nfromO(n)toO(1). Maybe surprisingly, similar optimizations are possible for other data patterns. In other\nwords, knowing the exact data distribution enables highly optimizing almost any index structure.\nOf course, in most real-world use cases the data do not perfectly follow a known pattern and the\nengineering effort to build specialized solutions for every use case is usually too high. However, we argue\nthat machine learning (ML) opens up the opportunity to learn a model that reﬂects the patterns in the data\nand thus to enable the automatic synthesis of specialized index structures, termed learned indexes , with low\nengineering cost.\nIn this paper, we explore the extent to which learned models, including neural networks, can be used\nto enhance, or even replace, traditional index structures from B-Trees to Bloom ﬁlters. This may seem\ncounterintuitive because ML cannot provide the semantic guarantees we traditionally associate with these\nindexes, and because the most powerful ML models, neural networks, are traditionally thought of as being\nvery compute expensive. Yet, we argue that none of these apparent obstacles are as problematic as they might\nseem. Instead, our proposal to use learned models has the potential for signiﬁcant beneﬁts, especially on the\nnext generation of hardware.\nIn terms of semantic guarantees, indexes are already to a large extent learned models making it surprisingly\nstraightforward to replace them with other types of ML models. For example, a B-Tree can be considered\nas a model which takes a key as an input and predicts the position of a data record in a sorted set (the data\nhas to be sorted to enable efﬁcient range requests). A Bloom ﬁlter is a binary classiﬁer, which based on\na key predicts if a key exists in a set or not. Obviously, there exists subtle but important differences. For\nexample, a Bloom ﬁlter can have false positives but not false negatives. However, as we will show in this\npaper, it is possible to address these differences through novel learning techniques and/or simple auxiliary\ndata structures.\nIn terms of performance, we observe that every CPU already has powerful SIMD capabilities and we\nspeculate that many laptops and mobile phones will soon have a Graphics Processing Unit (GPU) or Tensor\nProcessing Unit (TPU). It is also reasonable to speculate that CPU-SIMD/GPU/TPUs will be increasingly\npowerful as it is much easier to scale the restricted set of (parallel) math operations used by neural nets than a\ngeneral purpose instruction set. As a result the high cost to execute a neural net or other ML models might\nactually be negligible in the future. For instance, both Nvidia and Google’s TPUs are already able to perform\nthousands if not tens of thousands of neural net operations in a single cycle [ 3]. Furthermore, it was stated that\nGPUs will improve 1000\u0002in performance by 2025, whereas Moore’s law for CPUs is essentially dead [ 5].\nBy replacing branch-heavy index structures with neural networks, databases and other systems can beneﬁt\nfrom these hardware trends. While we see the future of learned index structures on specialized hardware, like\nTPUs, this paper focuses entirely on CPUs and surprisingly shows that we can achieve signiﬁcant advantages\neven in this case.\nIt is important to note that we do not argue to completely replace traditional index structures with\nlearned indexes. Rather, the main contribution of this paper is to outline and evaluate the potential\nof a novel approach to build indexes, which complements existing work and, arguably, opens up an\nentirely new research direction for a decades-old ﬁeld. This is based on the key observation that many\ndata structures can be decomposed into a learned model and an auxiliary structure to provide the same\nsemantic guarantees. The potential power of this approach comes from the fact that continuous functions,\ndescribing the data distribution, can be used to build more efﬁcient data structures or algorithms . We\nempirically get very promising results when evaluating our approach on synthetic and real-world datasets\nfor read-only analytical workloads. However, many open challenges still remain, such as how to handle\nwrite-heavy workloads, and we outline many possible directions for future work. Furthermore, we believe\nthat we can use the same principle to replace other components and operations commonly used in (database)\nsystems. If successful, the core idea of deeply embedding learned models into algorithms and data structures\ncould lead to a radical departure from the way systems are currently developed.\nThe remainder of this paper is outlined as follows: In the next two sections we introduce the general\n2\n\nBTreeKeypospos - 0pos + pagezise……pospos - min_errpos + max_er……Model (e.g., NN)(b) Learned Index(a) B-Tree IndexKeyFigure 1: Why B-Trees are models\nidea of learned indexes using B-Trees as an example. In Section 4 we extend this idea to Hash-maps and in\nSection 5 to Bloom ﬁlters. All sections contain a separate evaluation. Finally in Section 6 we discuss related\nwork and conclude in Section 7.\n2 Range Index\nRange index structure, like B-Trees, are already models: given a key, they “predict” the location of a value\nwithin a key-sorted set. To see this, consider a B-Tree index in an analytics in-memory database (i.e.,\nread-only) over the sorted primary key column as shown in Figure 1(a). In this case, the B-Tree provides a\nmapping from a look-up key to a position inside the sorted array of records with the guarantee that the key of\nthe record at that position is the ﬁrst key equal or higher than the look-up key. The data has to be sorted to\nallow for efﬁcient range requests. This same general concept also applies to secondary indexes where the\ndata would be the list of <key,record pointer> pairs with the key being the indexed value and the\npointer a reference to the record.1\nFor efﬁciency reasons it is common not to index every single key of the sorted records, rather only the\nkey of every n-th record, i.e., the ﬁrst key of a page. Here we only assume ﬁxed-length records and logical\npaging over a continuous memory region, i.e., a single array, not physical pages which are located in different\nmemory regions (physical pages and variable length records are discussed in Appendix D.2). Indexing only\nthe ﬁrst key of every page helps to signiﬁcantly reduce the number of keys the index has to store without\nany signiﬁcant performance penalty. Thus, the B-Tree is a model, or in ML terminology, a regression tree:\nit maps a key to a position with a min- and max-error (a min-error of 0 and a max-error of the page-size),\nwith a guarantee that the key can be found in that region if it exists. Consequently, we can replace the index\nwith other types of ML models, including neural nets, as long as they are also able to provide similar strong\nguarantees about the min- and max-error.\nAt ﬁrst sight it may seem hard to provide the same guarantees with other types of ML models, but it\nis actually surprisingly simple. First, the B-Tree only provides the strong min- and max-error guarantee\nover the stored keys, not for all possible keys. For new data, B-Trees need to be re-balanced, or in machine\nlearning terminology re-trained, to still be able to provide the same error guarantees. That is, for monotonic\nmodels the only thing we need to do is to execute the model for every key and remember the worst over- and\n1Note, that against some deﬁnitions for secondary indexes we do not consider the <key,record pointer> pairs as part\nof the index; rather for secondary index the data are the <key,record pointer> pairs. This is similar to how indexes are\nimplemented in key value stores [12, 21] or how B-Trees on modern hardware are designed [44].\n3\n\nunder-prediction of a position to calculate the min- and max-error.2Second, and more importantly, the strong\nerror bounds are not even needed. The data has to be sorted anyway to support range requests, so any error is\neasily corrected by a local search around the prediction (e.g., using exponential search) and thus, even allows\nfor non-monotonic models. Consequently, we are able to replace B-Trees with any other type of regression\nmodel, including linear regression or neural nets (see Figure 1(b)).\nNow, there are other technical challenges that we need to address before we can replace B-Trees with\nlearned indexes. For instance, B-Trees have a bounded cost for inserts and look-ups and are particularly good\nat taking advantage of the cache. Also, B-Trees can map keys to pages which are not continuously mapped to\nmemory or disk. All of these are interesting challenges/research questions and are explained in more detail,\ntogether with potential solutions, throughout this section and in the appendix.\nAt the same time, using other types of models as indexes can provide tremendous beneﬁts. Most\nimportantly, it has the potential to transform the logncost of a B-Tree look-up into a constant operation.\nFor example, assume a dataset with 1M unique keys with a value from 1M and 2M (so the value 1,000,009\nis stored at position 10). In this case, a simple linear model, which consists of a single multiplication and\naddition, can perfectly predict the position of any key for a point look-up or range scan, whereas a B-Tree\nwould require lognoperations. The beauty of machine learning, especially neural nets, is that they are able\nto learn a wide variety of data distributions, mixtures and other data peculiarities and patterns. The challenge\nis to balance the complexity of the model with its accuracy.\nFor most of the discussion in this paper, we keep the simpliﬁed assumptions of this section: we only\nindex an in-memory dense array that is sorted by key. This may seem restrictive, but many modern hardware\noptimized B-Trees, e.g., FAST [ 44], make exactly the same assumptions, and these indexes are quite common\nfor in-memory database systems for their superior performance [ 44,48] over scanning or binary search.\nHowever, while some of our techniques translate well to some scenarios (e.g., disk-resident data with very\nlarge blocks, for example, as used in Bigtable [ 23]), for other scenarios (ﬁne grained paging, insert-heavy\nworkloads, etc.) more research is needed. In Appendix D.2 we discuss some of those challenges and potential\nsolutions in more detail.\n2.1 What Model Complexity Can We Afford?\nTo better understand the model complexity, it is important to know how many operations can be performed in\nthe same amount of time it takes to traverse a B-Tree, and what precision the model needs to achieve to be\nmore efﬁcient than a B-Tree.\nConsider a B-Tree that indexes 100M records with a page-size of 100. We can think of every B-Tree node\nas a way to partition the space, decreasing the “error” and narrowing the region to ﬁnd the data. We therefore\nsay that the B-Tree with a page-size of 100 has a precision gain of1=100per node and we need to traverse\nin totallog100Nnodes. So the ﬁrst node partitions the space from 100Mto100M=100 = 1M, the second\nfrom 1Mto1M=100 = 10kand so on, until we ﬁnd the record. Now, traversing a single B-Tree page with\nbinary search takes roughly 50cycles and is notoriously hard to parallelize3. In contrast, a modern CPU can\ndo 8-16 SIMD operations per cycle. Thus, a model will be faster as long as it has a better precision gain than\n1=100per50\u00038 = 400 arithmetic operations. Note that this calculation still assumes that all B-Tree pages\nare in the cache. A single cache-miss costs 50-100 additional cycles and would thus allow for even more\ncomplex models.\nAdditionally, machine learning accelerators are entirely changing the game. They allow to run much\nmore complex models in the same amount of time and ofﬂoad computation from the CPU. For example,\n2The model has to be monotonic to also guarantee the min- and max-error for look-up keys, which do not exist in the stored set.\n3There exist SIMD optimized index structures such as FAST [ 44], but they can only transform control dependencies to memory\ndependencies. These are often signiﬁcantly slower than multiplications with simple in-cache data dependencies and as our experiments\nshow SIMD optimized index structures, like FAST, are not signiﬁcantly faster.\n4\n\nPos\nKeyFigure 2: Indexes as CDFs\nNVIDIA’s latest Tesla V100 GPU is able to achieve 120 TeraFlops of low-precision deep learning arithmetic\noperations (\u001960;000operations per cycle). Assuming that the entire learned index ﬁts into the GPU’s\nmemory (we show in Section 3.7 that this is a very reasonable assumption), in just 30 cycles we could execute\n1 million neural net operations. Of course, the latency for transferring the input and retrieving the result\nfrom a GPU is still signiﬁcantly higher, but this problem is not insuperable given batching and/or the recent\ntrend to more closely integrate CPU/GPU/TPUs [ 4]. Finally, it can be expected that the capabilities and the\nnumber of ﬂoating/int operations per second of GPUs/TPUs will continue to increase, whereas the progress\non increasing the performance of executing if-statements of CPUs essentially has stagnated [ 5]. Regardless\nof the fact that we consider GPUs/TPUs as one of the main reasons to adopt learned indexes in practice, in\nthis paper we focus on the more limited CPUs to better study the implications of replacing and enhancing\nindexes through machine learning without the impact of hardware changes.\n2.2 Range Index Models are CDF Models\nAs stated in the beginning of the section, an index is a model that takes a key as an input and predicts the\nposition of the record. Whereas for point queries the order of the records does not matter, for range queries\nthe data has to be sorted according to the look-up key so that all data items in a range (e.g., in a time frame)\ncan be efﬁciently retrieved. This leads to an interesting observation: a model that predicts the position given a\nkey inside a sorted array effectively approximates the cumulative distribution function (CDF). We can model\nthe CDF of the data to predict the position as:\np=F(Key)\u0003N (1)\nwherepis the position estimate, F(Key)is the estimated cumulative distribution function for the data to\nestimate the likelihood to observe a key smaller or equal to the look-up key P(X\u0014Key), andNis the total\nnumber of keys (see also Figure 2). This observation opens up a whole new set of interesting directions: First,\nit implies that indexing literally requires learning a data distribution. A B-Tree “learns” the data distribution\nby building a regression tree. A linear regression model would learn the data distribution by minimizing\nthe (squared) error of a linear function. Second, estimating the distribution for a dataset is a well known\nproblem and learned indexes can beneﬁt from decades of research. Third, learning the CDF plays also a key\nrole in optimizing other types of index structures and potential algorithms as we will outline later in this\npaper. Fourth, there is a long history of research on how closely theoretical CDFs approximate empirical\nCDFs that gives a foothold to theoretically understand the beneﬁts of this approach [ 28]. We give a high-level\ntheoretical analysis of how well our approach scales in Appendix A.\n5\n\n2.3 A First, Na ¨ıve Learned Index\nTo better understand the requirements to replace B-Trees through learned models, we used 200M web-server\nlog records with the goal of building a secondary index over the timestamps using Tensorﬂow [ 9]. We trained\na two-layer fully-connected neural network with 32 neurons per layer using ReLU activation functions; the\ntimestamps are the input features and the positions in the sorted array are the labels. Afterwards we measured\nthe look-up time for a randomly selected key (averaged over several runs disregarding the ﬁrst numbers) with\nTensorﬂow and Python as the front-end.\nIn this setting we achieved \u00191250 predictions per second, i.e., it takes \u001980;000nano-seconds (ns)\nto execute the model with Tensorﬂow, without the search time (the time to ﬁnd the actual record from the\npredicted position). As a comparison point, a B-Tree traversal over the same data takes \u0019300nsand binary\nsearch over the entire data roughly \u0019900ns. With a closer look, we ﬁnd our na ¨ıve approach is limited in a\nfew key ways:\n1.Tensorﬂow was designed to efﬁciently run larger models, not small models, and thus, has a signiﬁcant\ninvocation overhead, especially with Python as the front-end.\n2.B-Trees, or decision trees in general, are really good in overﬁtting the data with a few operations as they\nrecursively divide the space using simple if-statements. In contrast, other models can be signiﬁcantly\nmore efﬁcient to approximate the general shape of a CDF, but have problems being accurate at the\nindividual data instance level. To see this, consider again Figure 2. The ﬁgure demonstrates, that from\na top-level view, the CDF function appears very smooth and regular. However, if one zooms in to the\nindividual records, more and more irregularities show; a well known statistical effect. Thus models\nlike neural nets, polynomial regression, etc. might be more CPU and space efﬁcient to narrow down\nthe position for an item from the entire dataset to a region of thousands, but a single neural net usually\nrequires signiﬁcantly more space and CPU time for the “last mile” to reduce the error further down\nfrom thousands to hundreds.\n3.B-Trees are extremely cache- and operation-efﬁcient as they keep the top nodes always in cache\nand access other pages if needed. In contrast, standard neural nets require all weights to compute a\nprediction, which has a high cost in the number of multiplications.\n3 The RM-Index\nIn order to overcome the challenges and explore the potential of models as index replacements or optimizations,\nwe developed the learning index framework (LIF), recursive-model indexes (RMI), and standard-error-based\nsearch strategies. We primarily focus on simple, fully-connected neural nets because of their simplicity and\nﬂexibility, but we believe other types of models may provide additional beneﬁts.\n3.1 The Learning Index Framework (LIF)\nTheLIFcan be regarded as an index synthesis system; given an index speciﬁcation, LIFgenerates different\nindex conﬁgurations, optimizes them, and tests them automatically. While LIF can learn simple models\non-the-ﬂy (e.g., linear regression models), it relies on Tensorﬂow for more complex models (e.g., NN).\nHowever, it never uses Tensorﬂow at inference. Rather, given a trained Tensorﬂow model, LIFautomatically\nextracts all weights from the model and generates efﬁcient index structures in C++ based on the model\nspeciﬁcation. Our code-generation is particularly designed for small models and removes all unnecessary\noverhead and instrumentation that Tensorﬂow has to manage the larger models. Here we leverage ideas\nfrom [ 25], which already showed how to avoid unnecessary overhead from the Spark-runtime. As a result,\n6\n\nModel 1.1\nModel 2.1 Model 2.2 Model 2.3\nModel 3.1 Model 3.2 Model 3.3 Model 3.4…\n…Stage 3     Stage 2   Stage 1\nPositionKeyFigure 3: Staged models\nwe are able to execute simple models on the order of 30 nano-seconds. However, it should be pointed\nout that LIF is still an experimental framework and is instrumentalized to quickly evaluate different index\nconﬁgurations (e.g., ML models, page-sizes, search strategies, etc.), which introduces additional overhead in\nform of additional counters, virtual function calls, etc. Also besides the vectorization done by the compiler,\nwe do not make use of special SIMD intrinisics. While these inefﬁciencies do not matter in our evaluation as\nwe ensure a fair comparison by always using our framework, for a production setting or when comparing\nthe reported performance numbers with other implementations, these inefﬁciencies should be taking into\naccount/be avoided.\n3.2 The Recursive Model Index\nAs outlined in Section 2.3 one of the key challenges of building alternative learned models to replace B-Trees\nis the accuracy for last-mile search. For example, reducing the prediction error to the order of hundreds from\n100M records using a single model is often difﬁcult. At the same time, reducing the error to 10k from 100M,\ne.g., a precision gain of 100\u0003100 = 10000 to replace the ﬁrst 2 layers of a B-Tree through a model, is much\neasier to achieve even with simple models. Similarly, reducing the error from 10k to 100 is a simpler problem\nas the model can focus only on a subset of the data.\nBased on that observation and inspired by the mixture of experts work [ 62], we propose the recursive\nregression model (see Figure 3). That is, we build a hierarchy of models, where at each stage the model takes\nthe key as an input and based on it picks another model, until the ﬁnal stage predicts the position. More\nformally, for our model f(x)wherexis the key and y2[0;N)the position, we assume at stage `there\nareM`models. We train the model at stage 0, f0(x)\u0019y. As such, model kin stage`, denoted by f(k)\n`, is\ntrained with loss:\nL`=X\n(x;y)(f(bM`f`\u00001(x)=Nc)\n`(x)\u0000y)2L0=X\n(x;y)(f0(x)\u0000y)2\nNote, we use here the notation of f`\u00001(x)recursively executing f`\u00001(x) =f(bM`\u00001f`\u00002(x)=Nc)\n`\u00001(x). In\ntotal, we iteratively train each stage with loss L`to build the complete model. One way to think about the\ndifferent models is that each model makes a prediction with a certain error about the position for the keyand\nthat the prediction is used to select the next model, which is responsible for a certain area of the key-space\nto make a better prediction with a lower error. However, recursive model indexes do nothave to be trees.\n7\n\nAs shown in Figure 3 it is possible that different models of one stage pick the same models at the stage\nbelow. Furthermore, each model does not necessarily cover the same amount of records like B-Trees do\n(i.e., a B-Tree with a page-size of 100 covers 100 or less records).4Finally, depending on the used models\nthe predictions between the different stages can not necessarily be interpreted as positions estimates, rather\nshould be considered as picking an expert which has a better knowledge about certain keys (see also [62]).\nThis model architecture has several beneﬁts: (1) It separates model size and complexity from execution\ncost. (2) It leverages the fact that it is easy to learn the overall shape of the data distribution. (3) It effectively\ndivides the space into smaller sub-ranges, like a B-Tree, to make it easier to achieve the required “last mile”\naccuracy with fewer operations. (4) There is no search process required in-between the stages. For example,\nthe output of Model 1.1 is directly used to pick the model in the next stage. This not only reduces the\nnumber of instructions to manage the structure, but also allows representing the entire index as a sparse\nmatrix-multiplication for a TPU/GPU.\n3.3 Hybrid Indexes\nAnother advantage of the recursive model index is, that we are able to build mixtures of models. For example,\nwhereas on the top-layer a small ReLU neural net might be the best choice as they are usually able to learn a\nwide-range of complex data distributions, the models at the bottom of the model hierarchy might be thousands\nof simple linear regression models as they are inexpensive in space and execution time. Furthermore, we can\neven use traditional B-Trees at the bottom stage if the data is particularly hard to learn.\nFor this paper, we focus on 2 types of models, simple neural nets with zero to two fully-connected hidden\nlayers and ReLU activation functions and a layer width of up to 32 neurons and B-Trees (a.k.a. decision\ntrees). Note, that a zero hidden-layer NN is equivalent to linear regression. Given an index conﬁguration,\nwhich speciﬁes the number of stages and the number of models per stage as an array of sizes, the end-to-end\ntraining for hybrid indexes is done as shown in Algorithm 1\nAlgorithm 1: Hybrid End-To-End Training\nInput: int threshold, int stages[], NN complexity\nData: record data[], Model index[][]\nResult: trained index\n1M= stages.size;\n2tmp records[][];\n3tmp records[1][1] = all data;\n4fori 1toMdo\n5 forj 1tostages [i]do\n6 index[i][j] = new NN trained on tmp records[ i][j];\n7 ifi < M then\n8 forr2tmprecords[ i][j]do\n9 p= index[i][j] (r:key )/ stages[ i+ 1];\n10 tmp records[ i+ 1][p].add( r);\n11forj 1toindex [M]:size do\n12 index[ M][j].calc err(tmp records[ M][j]);\n13 ifindex [M][j]:max abserr > threshold then\n14 index[ M][j] = new B-Tree trained on tmp records[ M][j];\n15return index;\nStarting from the entire dataset (line 3), it trains ﬁrst the top-node model. Based on the prediction of this\ntop-node model, it then picks the model from the next stage (lines 9 and 10) and adds all keys which fall into\n4Note, that we currently train stage-wise and not fully end-to-end. End-to-end training would be even better and remains future\nwork.\n8\n\nthat model (line 10). Finally, in the case of hybrid indexes, the index is optimized by replacing NN models\nwith B-Trees if absolute min-/max-error is above a predeﬁned threshold (lines 11-14).\nNote, that we store the standard and min- and max-error for every model on the last stage. That has the\nadvantage, that we can individually restrict the search space based on the used model for every key. Currently,\nwe tune the various parameters of the model (i.e., number of stages, hidden layers per model, etc.) with a\nsimple simple grid-search. However, many potential optimizations exists to speed up the training process\nfrom ML auto tuning to sampling.\nNote, that hybrid indexes allow us to bound the worst case performance of learned indexes to the\nperformance of B-Trees. That is, in the case of an extremely difﬁcult to learn data distribution, all models\nwould be automatically replaced by B-Trees, making it virtually an entire B-Tree.\n3.4 Search Strategies and Monotonicity\nRange indexes usually implement an upperbound (key)[lowerbound (key)] interface to ﬁnd the position\nof the ﬁrst key within the sorted array that is equal or higher [lower] than the look-up key to efﬁciently\nsupport range requests. For learned range indexes we therefore have to ﬁnd the ﬁrst key higher [lower] from\nthe look-up key based on the prediction. Despite many efforts, it was repeatedly reported [ 8] that binary\nsearch or scanning for records with small payloads are usually the fastest strategies to ﬁnd a key within a\nsorted array as the additional complexity of alternative techniques rarely pays off. However, learned indexes\nmight have an advantage here: the models actually predict the position of the key, not just the region (i.e.,\npage) of the key. Here we discuss two simple search strategies which take advantage of this information:\nModel Biased Search: Our default search strategy, which only varies from traditional binary search in\nthat the ﬁrst middle point is set to the value predicted by the model.\nBiased Quaternary Search: Quaternary search takes instead of one split point three points with the\nhope that the hardware pre-fetches all three data points at once to achieve better performance if the data\nis not in cache. In our implementation, we deﬁned the initial three middle points of quaternary search as\npos\u0000\u001b;pos;pos +\u001b. That is we make a guess that most of our predictions are accurate and focus our\nattention ﬁrst around the position estimate and then we continue with traditional quaternary search.\nFor all our experiments we used the min- and max-error as the search area for all techniques. That is, we\nexecuted the RMI model for every key and stored the worst over- and under-prediction per last-stage model.\nWhile this technique guarantees to ﬁnd all existing keys, for non-existing keys it might return the wrong\nupper or lower bound if the RMI model is not monotonic. To overcome this problem, one option is to force\nour RMI model to be monotonic, as has been studied in machine learning [41, 71].\nAlternatively, for non-monotonic models we can automatically adjust the search area. That is, if the\nfound upper (lower) bound key is on the boundary of the search area deﬁned by the min- and max-error,\nwe incrementally adjust the search area. Yet, another possibility is, to use exponential search techniques.\nAssuming a normal distributed error, those techniques on average should work as good as alternative search\nstrategies while not requiring to store any min- and max-errors.\n3.5 Indexing Strings\nWe have primarily focused on indexing real valued keys, but many databases rely on indexing strings, and\nluckily, signiﬁcant machine learning research has focused on modeling strings. As before, we need to design\na model of strings that is efﬁcient yet expressive. Doing this well for strings opens a number of unique\nchallenges.\nThe ﬁrst design consideration is how to turn strings into features for the model, typically called tokeniza-\ntion. For simplicity and efﬁciency, we consider an n-length string to be a feature vector x2Rnwhere xiis\nthe ASCII decimal value (or Unicode decimal value depending on the strings). Further, most ML models\n9\n\nType52.45(4.00x)274(0.97x)198(72.3%)51.93(4.00x)276(0.94x)201(72.7%)49.83(4.00x)274(0.96x)198(72.1%)26.23(2.00x)277(0.96x)172(62.0%)25.97(2.00x)274(0.95x)171(62.4%)24.92(2.00x)274(0.96x)169(61.7%)13.11(1.00x)265(1.00x)134(50.8%)12.98(1.00x)260(1.00x)132(50.8%)12.46(1.00x)263(1.00x)131(50.0%)6.56(0.50x)267(0.99x)114(42.7%)6.49(0.50x)266(0.98x)114(42.9%)6.23(0.50x)271(0.97x)117(43.2%)3.28(0.25x)286(0.93x)101(35.3%)3.25(0.25x)291(0.89x)100(34.3%)3.11(0.25x)293(0.90x)101(34.5%)2nd stage models:   10k0.15(0.01x)98(2.70x)31(31.6%)0.15(0.01x)222(1.17x)29(13.1%)0.15(0.01x)178(1.47x)26(14.6%)0.76(0.06x)85(3.11x)39(45.9%)0.76(0.06x)162(1.60x)36(22.2%)0.76(0.06x)162(1.62x)35(21.6%)1.53(0.12x)82(3.21x)41(50.2%)1.53(0.12x)144(1.81x)39(26.9%)1.53(0.12x)152(1.73x)36(23.7%)3.05(0.23x)86(3.08x)50(58.1%)3.05(0.24x)126(2.07x)41(32.5%)3.05(0.24x)146(1.79x)40(27.6%)Learned IndexMap DataWeb DataLog-Normal DataSize (MB)Lookup (ns)Model (ns)Size (MB)Lookup (ns)Model (ns)Size (MB)Lookup (ns)Model (ns)Btreepage size: 5122nd stage models:   50k2nd stage models: 100k2nd stage models: 200kConfigpage size:   32page size:   64page size: 128page size: 256Figure 4: Learned Index vs B-Tree\noperate more efﬁciently if all inputs are of equal size. As such, we will set a maximum input length N.\nBecause the data is sorted lexicographically, we will truncate the keys to length Nbefore tokenization. For\nstrings with length n<N , we set xi= 0fori>n .\nFor efﬁciency, we generally follow a similar modeling approach as we did for real valued inputs. We\nlearn a hierarchy of relatively small feed-forward neural networks. The one difference is that the input is\nnot a single real value xbut a vector x. Linear models w\u0001x+bscale the number of multiplications and\nadditions linearly with the input length N. Feed-forward neural networks with even a single hidden layer of\nwidthhwill scaleO(hN)multiplications and additions.\nUltimately, we believe there is signiﬁcant future research that can optimize learned indexes for string\nkeys. For example, we could easily imagine other tokenization algorithms. There is a large body of research\nin natural language processing on string tokenization to break strings into more useful segments for ML\nmodels, e.g., wordpieces in translation [ 70]. Further, it might be interesting to combine the idea of sufﬁx-trees\nwith learned indexes as well as explore more complex model architectures (e.g., recurrent and convolutional\nneural networks).\n3.6 Training\nWhile the training (i.e., loading) time is not the focus of this paper, it should be pointed out that all of our\nmodels, shallow NNs or even simple linear/multi-variate regression models, train relatively fast. Whereas\nsimple NNs can be efﬁciently trained using stochastic gradient descent and can converge in less than one to\na few passes over the randomized data, a closed form solution exists for linear multi-variate models (e.g.,\nalso 0-layer NN) and they can be trained in a single pass over the sorted data. Therefore, for 200M records\ntraining a simple RMI index does not take much longer than a few seconds, (of course, depending on how\nmuch auto-tuning is performed); neural nets can train on the order of minutes per model, depending on the\ncomplexity. Also note that training the top model over the entire data is usually not necessary as those models\nconverge often even before a single scan over the entire randomized data. This is in part because we use\nsimple models and do not care much about the last few digit points in precision, as it has little effect on\nindexing performance. Finally, research on improving learning time from the ML community [ 27,72] applies\nin our context and we expect a lot of future research in this direction.\n3.7 Results\nWe evaluated learned range indexes in regard to their space and speed on several real and synthetic data sets\nagainst other read-optimized index structures.\n10\n\n3.7.1 Integer Datasets\nAs a ﬁrst experiment we compared learned indexes using a 2-stage RMI model and different second-stage\nsizes (10k, 50k, 100k, and 200k) with a read-optimized B-Tree with different page sizes on three different\ninteger data sets. For the data we used 2 real-world datasets, (1) Weblogs and (2) Maps [ 56], and (3) a\nsynthetic dataset, Lognormal. The Weblogs dataset contains 200M log entries for every request to a major\nuniversity web-site over several years. We use the unique request timestamps as the index keys. This dataset\nis almost a worst-case scenario for the learned index as it contains very complex time patterns caused by class\nschedules, weekends, holidays, lunch-breaks, department events, semester breaks, etc., which are notoriously\nhard to learn. For the maps dataset we indexed the longitude of \u0019200M user-maintained features (e.g.,\nroads, museums, coffee shops) across the world. Unsurprisingly, the longitude of locations is relatively linear\nand has fewer irregularities than the Weblogs dataset. Finally, to test how the index works on heavy-tail\ndistributions, we generated a synthetic dataset of 190M unique values sampled from a log-normal distribution\nwith\u0016= 0 and\u001b= 2. The values are scaled up to be integers up to 1B. This data is of course highly\nnon-linear, making the CDF more difﬁcult to learn using neural nets. For all B-Tree experiments we used\n64-bit keys and 64-bit payload/value.\nAs our baseline, we used a production quality B-Tree implementation which is similar to the stx::btree but\nwith further cache-line optimization, dense pages (i.e., ﬁll factor of 100% ), and very competitive performance.\nTo tune the 2-stage learned indexes we used simple grid-search over neural nets with zero to two hidden\nlayers and layer-width ranging from 4 to 32 nodes. In general we found that a simple (0 hidden layers) to\nsemi-complex (2 hidden layers and 8- or 16-wide) models for the ﬁrst stage work the best. For the second\nstage, simple, linear models, had the best performance. This is not surprising as for the last mile it is often\nnot worthwhile to execute complex models, and linear models can be learned optimally.\nLearned Index vs B-Tree performance: The main results are shown in Figure 4. Note, that the page\nsize for B-Trees indicates the number of keys per page not the size in Bytes, which is actually larger. As the\nmain metrics we show the size in MB, the total look-up time in nano-seconds, and the time to execution the\nmodel (either B-Tree traversal or ML model) also in nano-seconds and as a percentage compared to the total\ntime in paranthesis. Furthermore, we show the speedup and space savings compared to a B-Tree with page\nsize of 128 in parenthesis as part of the size and lookup column. We choose a page size of 128 as the ﬁxed\nreference point as it provides the best lookup performance for B-Trees (note, that it is always easy to save\nspace at the expense of lookup performance by simply having no index at all). The color-encoding in the\nspeedup and size columns indicates how much faster or slower (larger or smaller) the index is against the\nreference point.\nAs can be seen, the learned index dominates the B-Tree index in almost all conﬁgurations by being up\nto1:5\u00003\u0002faster while being up to two orders-of-magnitude smaller. Of course, B-Trees can be further\ncompressed at the cost of CPU-time for decompressing. However, most of these optimizations are orthogonal\nand apply equally (if not more) to neural nets. For example, neural nets can be compressed by using 4- or\n8-bit integers instead of 32- or 64-bit ﬂoating point values to represent the model parameters (a process\nreferred to as quantization). This level of compression can unlock additional gains for learned indexes.\nUnsurprisingly the second stage size has a signiﬁcant impact on the index size and look-up performance.\nUsing 10,000 or more models in the second stage is particularly impressive with respect to the analysis in\nx2.1, as it demonstrates that our ﬁrst-stage model can make a much larger jump in precision than a single\nnode in the B-Tree. Finally, we do not report on hybrid models or other search techniques than binary search\nfor these datasets as they did not provide signiﬁcant beneﬁt.\nLearned Index vs Alternative Baselines: In addition to the detailed evaluation of learned indexes\nagainst our read-optimized B-Trees, we also compared learned indexes against other alternative baselines,\nincluding third party implementations, under fair conditions. In the following, we discuss some alternative\nbaselines and compare them against learned indexes if appropriate:\n11\n\n Lookup Table w/ AVX searchFASTFixe-Size Btree w/ interpol. searchMultivariate Learned IndexTime199 ns189 ns280 ns105 nsSize16.3 MB1024 MB1.5 MB1.5 MBFigure 5: Alternative Baselines\nHistogram : B-Trees approximate the CDF of the underlying data distribution. An obvious question is\nwhether histograms can be used as a CDF model. In principle the answer is yes, but to enable fast data\naccess, the histogram must be a low-error approximation of the CDF. Typically this requires a large number\nof buckets, which makes it expensive to search the histogram itself. This is especially true, if the buckets\nhave varying bucket boundaries to efﬁciently handle data skew, so that only few buckets are empty or too full.\nThe obvious solutions to this issues would yield a B-Tree, and histograms are therefore not further discussed.\nLookup-Table : A simple alternative to B-Trees are (hierarchical) lookup-tables. Often lookup-tables\nhave a ﬁxed size and structure (e.g., 64 slots for which each slot points to another 64 slots, etc.). The advantage\nof lookup-tables is that because of their ﬁxed size they can be highly optimized using A VX instructions. We\nincluded a comparison against a 3-stage lookup table, which is constructed by taking every 64th key and\nputting it into an array including padding to make it a multiple of 64. Then we repeat that process one more\ntime over the array without padding, creating two arrays in total. To lookup a key, we use binary search on\nthe top table followed by an A VX optimized branch-free scan [ 14] for the second table and the data itself.\nThis conﬁguration leads to the fastest lookup times compared to alternatives (e.g., using scanning on the top\nlayer, or binary search on the 2nd array or the data).\nFAST : FAST [ 44] is a highly SIMD optimized data structure. We used the code from [ 47] for the\ncomparison. However, it should be noted that FAST always requires to allocate memory in the power of 2 to\nuse the branch free SIMD instructions, which can lead to signiﬁcantly larger indexes.\nFixed-size B-Tree & interpolation search : Finally, as proposed in a recent blog post [ 1] we created a\nﬁxed-height B-Tree with interpolation search. The B-Tree height is set, so that the total size of the tree is\n1.5MB, similar to our learned model.\nLearned indexes without overhead : For our learned index we used a 2-staged RMI index with a multi-\nvariate linear regression model at the top and simple linear models at the bottom. We used simple automatic\nfeature engineering for the top model by automatically creating and selecting features in the form of key,\nlog(key),key2, etc. Multivariate linear regression is an interesting alternative to NN as it is particularly well\nsuited to ﬁt nonlinear patterns with only a few operations. Furthermore, we implemented the learned index\noutside of our benchmarking framework to ensure a fair comparison.\nFor the comparison we used the Lognormal data with a payload of an eight-byte pointer. The results can\nbe seen in Figure 5. As can be seen for the dataset under fair conditions, learned indexes provide the best\noverall performance while saving signiﬁcant amount of memory. It should be noted, that the FAST index is\nbig because of the alignment requirement.\nWhile the results are very promising, we by no means claim that learned indexes will always be the best\nchoice in terms of size or speed. Rather, learned indexes provide a new way to think about indexing and\nmuch more research is needed to fully understand the implications.\n3.7.2 String Datasets\nWe also created a secondary index over 10M non-continuous document-ids of a large web index used as part\nof a real product at Google to test how learned indexes perform on strings. The results for the string-based\ndocument-id dataset are shown in Figure 6, which also now includes hybrid models. In addition, we include\nour best model in the table, which is a non-hybrid RMI model index with quaternary search, named “Learned\n12\n\nConfigLookup (ns)Model (ns)page size:   3213.11(4.00x)1247 (1.03x)643 (52%)page size:   646.56(2.00x)1280 (1.01x)500 (39%)page size: 1283.28(1.00x)1288 (1.00x)377 (29%)page size: 2561.64(0.50x)1398 (0.92x)330 (24%)1 hidden layer1.22(0.37x)1605 (0.80x)503 (31%)2 hidden layers2.26(0.69x)1660 (0.78x)598 (36%)t=128, 1 hidden layer1.67(0.51x)1397 (0.92x)472 (34%)t=128, 2 hidden layers2.33(0.71x)1620 (0.80x)591 (36%)t= 64, 1 hidden layer2.50(0.76x)1220 (1.06x)440 (36%)t= 64, 2 hidden layers2.79(0.85x)1447 (0.89x)556 (38%)Learned QS1 hidden layer1.22(0.37x)1155 (1.12x)496 (43%)Size(MB)BtreeLearned IndexHybrid IndexFigure 6: String data: Learned Index vs B-Tree\nQS” (bottom of the table). All RMI indexes used 10,000 models on the 2nd stage and for hybrid indexes we\nused two thresholds, 128 and 64, as the maximum tolerated absolute error for a model before it is replaced\nwith a B-Tree.\nAs can be seen, the speedups for learned indexes over B-Trees for strings are not as prominent. Part\nof the reason is the comparably high cost of model execution, a problem that GPU/TPUs would remove.\nFurthermore, searching over strings is much more expensive thus higher precision often pays off; the reason\nwhy hybrid indexes, which replace bad performing models through B-Trees, help to improve performance.\nBecause of the cost of searching, the different search strategies make a bigger difference. For example,\nthe search time for a NN with 1-hidden layer and biased binary search is 1102nsas shown in Figure 6. In\ncontrast, our biased quaternary search with the same model only takes 658ns, a signiﬁcant improvement. The\nreason why biased search and quaternary search perform better is that they take the model error into account.\n4 Point Index\nNext to range indexes, Hash-maps for point look-ups play a similarly important role in DBMS. Conceptually\nHash-maps use a hash-function to deterministically map keys to positions inside an array (see Figure 7(a)).\nThe key challenge for any efﬁcient Hash-map implementation is to prevent too many distinct keys from being\nmapped to the same position inside the Hash-map, henceforth referred to as a conﬂict . For example, let’s\nassume 100M records and a Hash-map size of 100M. For a hash-function which uniformly randomizes the\nkeys, the number of expected conﬂicts can be derived similarly to the birthday paradox and in expectation\nwould be around 33% or33M slots. For each of these conﬂicts, the Hash-map architecture needs to deal with\nthis conﬂict. For example, separate chaining Hash-maps would create a linked-list to handle the conﬂict (see\nFigure 7(a)). However, many alternatives exist including secondary probing, using buckets with several slots,\nup to simultaneously using more than one hash function (e.g., as done by Cuckoo Hashing [57]).\nHowever, regardless of the Hash-map architecture, conﬂicts can have a signiﬁcant impact of the perfor-\nmance and/or storage requirement, and machine learned models might provide an alternative to reduce the\nnumber of conﬂicts. While the idea of learning models as a hash-function is not new, existing techniques do\nnot take advantage of the underlying data distribution. For example, the various perfect hashing techniques\n[26] also try to avoid conﬂicts but the data structure used as part of the hash functions grow with the data size;\na property learned models might not have (recall, the example of indexing all keys between 1 and 100M).\nTo our knowledge it has not been explored if it is possible to learn models which yield more efﬁcient point\nindexes.\n13\n\nHash-\nFunctionKeyModelKey(a) Traditional Hash-Map (b) Learned Hash-MapFigure 7: Traditional Hash-map vs Learned Hash-map\n4.1 The Hash-Model Index\nSurprisingly, learning the CDF of the key distribution is one potential way to learn a better hash function.\nHowever, in contrast to range indexes, we do not aim to store the records compactly or in strictly sorted order.\nRather we can scale the CDF by the targeted size Mof the Hash-map and use h(K) =F(K)\u0003M, with key\nKas our hash-function. If the model Fperfectly learned the empirical CDF of the keys, no conﬂicts would\nexist. Furthermore, the hash-function is orthogonal to the actual Hash-map architecture and can be combined\nwith separate chaining or any other Hash-map type.\nFor the model, we can again leverage the recursive model architecture from the previous section. Obvi-\nously, like before, there exists a trade-off between the size of the index and performance, which is inﬂuenced\nby the model and dataset.\nNote, that how inserts, look-ups, and conﬂicts are handled is dependent on the Hash-map architecture.\nAs a result, the beneﬁts learned hash functions provide over traditional hash functions, which map keys to a\nuniformly distributed space depend on two key factors: (1) How accurately the model represents the observed\nCDF. For example, if the data is generated by a uniform distribution, a simple linear model will be able to\nlearn the general data distribution, but the resulting hash function will not be better than any sufﬁciently\nrandomized hash function. (2) Hash map architecture: depending on the architecture, implementation details,\nthe payload (i.e., value), the conﬂict resolution policy, as well as how much more memory (i.e., slots) will\nor can be allocated, signiﬁcantly inﬂuences the performance. For example, for small keys and small or no\nvalues, traditional hash functions with Cuckoo hashing will probably work well, whereas larger payloads or\ndistributed hash maps might beneﬁt more from avoiding conﬂicts, and thus from learned hash functions.\n4.2 Results\nWe evaluated the conﬂict rate of learned hash functions over the three integer data sets from the previous\nsection. As our model hash-functions we used the 2-stage RMI models from the previous section with 100k\nmodels on the 2nd stage and without any hidden layers. As the baseline we used a simple MurmurHash3-like\nhash-function and compared the number of conﬂicts for a table with the same number of slots as records.\nAs can be seen in Figure 8, the learned models can reduce the number of conﬂicts by up to 77% over\nour datasets by learning the empirical CDF at a reasonable cost; the execution time is the same as the model\nexecution time in Figure 4, around 25-40ns.\nHow beneﬁcial the reduction of conﬂicts is given the model execution time depends on the Hash-map\narchitecture, payload, and many other factors. For example, our experiments (see Appendix B) show that\nfor a separate chaining Hash-map architecture with 20 Byte records learned hash functions can reduce the\n14\n\n% Conflicts Hash Map% Conflicts ModelReductionMap Data35.3%07.9%77.5%Web Data35.3%24.7%30.0%Log Normal35.4%25.9%26.7%Figure 8: Reduction of Conﬂicts\nwasted amount of storage by up to 80% at an increase of only 13ns in latency compared to random hashing.\nThe reason why it only increases the latency by 13ns and not 40ns is, that often fewer conﬂicts also yield to\nfewer cache misses, and thus better performance. On the other hand, for very small payloads Cuckoo-hashing\nwith standard hash-maps probably remains the best choice. However, as we show in Appendix C, for larger\npayloads a chained-hashmap with learned hash function can be faster than cuckoo-hashing and/or traditional\nrandomized hashing. Finally, we see the biggest potential for distributed settings. For example, NAM-DB [ 74]\nemploys a hash function to look-up data on remote machines using RDMA. Because of the extremely high\ncost for every conﬂict (i.e., every conﬂict requires an additional RDMA request which is in the order of\nmicro-seconds), the model execution time is negligible and even small reductions in the conﬂict rate can\nsigniﬁcantly improve the overall performance. To conclude, learned hash functions are independent of the\nused Hash-map architecture and depending on the Hash-map architecture their complexity may or may not\npay off.\n5 Existence Index\nThe last common index type of DBMS are existence indexes, most importantly Bloom ﬁlters, a space efﬁcient\nprobabilistic data structure to test whether an element is a member of a set. They are commonly used to\ndetermine if a key exists on cold storage. For example, Bigtable uses them to determine if a key is contained\nin an SSTable [23].\nInternally, Bloom ﬁlters use a bit array of size mandkhash functions, which each map a key to one of\nthemarray positions (see Figure9(a)). To add an element to the set, a key is fed to the khash-functions and\nthe bits of the returned positions are set to 1. To test if a key is a member of the set, the key is again fed into\nthekhash functions to receive karray positions. If any of the bits at those kpositions is 0, the key is not a\nmember of a set. In other words, a Bloom ﬁlter does guarantee that there exists no false negatives , but has\npotential false positives .\nWhile Bloom ﬁlters are highly space-efﬁcient, they can still occupy a signiﬁcant amount of memory. For\nexample for one billion records roughly \u00191:76Gigabytes are needed. For a FPR of 0:01% we would require\n\u00192:23Gigabytes. There have been several attempts to improve the efﬁciency of Bloom ﬁlters [52], but the\ngeneral observation remains.\nYet, if there is some structure to determine what is inside versus outside the set, which can be learned, it\nmight be possible to construct more efﬁcient representations. Interestingly, for existence indexes for database\nsystems, the latency and space requirements are usually quite different than what we saw before. Given the\nhigh latency to access cold storage (e.g., disk or even band), we can afford more complex models while the\nmain objective is to minimize the space for the index and the number of false positives. We outline two\npotential ways to build existence indexes using lea\n5.1 Learned Bloom ﬁlters\nWhile both range and point indexes learn the distribution of keys, existence indexes need to learn a function\nthat separates keys from everything else. Stated differently, a good hash function for a point index is one with\nfew collisions among keys, whereas a good hash function for a Bloom ﬁlter would be one that has lots of\n15\n\nh1h2h3h1h2h3Model\nYesBloom \nﬁlterNo\nKey\nYes key1                       key2 key3 key1 key2 key3\nModel Model Model h1h2h3(a) Traditional Bloom-Filter Insertion (b) Learned Bloom-Filter Insertion (c) Bloom ﬁlters as a classiﬁcation problemFigure 9: Bloom ﬁlters Architectures\ncollisions among keys and lots of collisions among non-keys, but few collisions of keys and non-keys. We\nconsider below how to learn such a function fand how to incorporate it into an existence index.\nWhile traditional Bloom ﬁlters guarantee a false negative rate (FNR) of zero and a speciﬁc false positive\nrate (FPR) for any set of queries chosen a-priori [ 22], we follow the notion that we want to provide a speciﬁc\nFPR for realistic queries in particular while maintaining a FNR of zero. That is, we measure the FPR over a\nheldout dataset of queries, as is common in evaluating ML systems [ 30]. While these deﬁnitions differ, we\nbelieve the assumption that we can observe the distribution of queries, e.g., from historical logs, holds in\nmany applications, especially within databases5.\nTraditionally, existence indexes make no use of the distribution of keys nor how they differ from non-keys,\nbut learned Bloom ﬁlters can. For example, if our database included all integers xfor0\u0014x < n , the\nexistence index could be computed in constant time and with almost no memory footprint by just computing\nf(x)\u00111[0\u0014x<n ].\nIn considering the data distribution for ML purposes, we must consider a dataset of non-keys. In this\nwork, we consider the case where non-keys come from observable historical queries and we assume that\nfuture queries come from the same distribution as historical queries. When this assumption does not hold,\none could use randomly generated keys, non-keys generated by a machine learning model [ 34], importance\nweighting to directly address covariate shift [ 18], or adversarial training for robustness [ 65]; we leave this as\nfuture work. We denote the set of keys by Kand the set of non-keys by U.\n5.1.1 Bloom ﬁlters as a Classiﬁcation Problem\nOne way to frame the existence index is as a binary probabilistic classiﬁcation task. That is, we want to learn\na modelfthat can predict if a query xis a key or non-key. For example, for strings we can train a recurrent\nneural network (RNN) or convolutional neural network (CNN) [ 64,37] withD=f(xi;yi= 1)jxi2\nKg[f (xi;yi= 0)jxi2Ug . Because this is a binary classiﬁcation task, our neural network has a sigmoid\nactivation to produce a probability and is trained to minimize the log loss: L=P\n(x;y)2Dylogf(x) + (1\u0000\ny) log(1\u0000f(x)):\nThe output of f(x)can be interpreted as the probability that xis a key in our database. Thus, we can\nturn the model into an existence index by choosing a threshold \u001cabove which we will assume that the key\nexists in our database. Unlike Bloom ﬁlters, our model will likely have a non-zero FPR and FNR; in fact, as\nthe FPR goes down, the FNR will go up. In order to preserve the no false negatives constraint of existence\nindexes, we create an overﬂow Bloom ﬁlter. That is, we consider K\u0000\n\u001c=fx2Kjf(x)<\u001cgto be the set of\nfalse negatives from fand create a Bloom ﬁlter for this subset of keys. We can then run our existence index\nas in Figure 9(c): if f(x)\u0015\u001c, the key is believed to exist; otherwise, check the overﬂow Bloom ﬁlter.\nOne question is how to set \u001cso that our learned Bloom ﬁlter has the desired FPR p\u0003. We denote\nthe FPR of our model by FPR\u001c\u0011P\nx2~U1(f(x)>\u001c)\nj~Ujwhere ~Uis a held-out set of non-keys. We denote\nthe FPR of our overﬂow Bloom ﬁlter by FPRB. The overall FPR of our system therefore is FPRO=\n5We would like to thank Michael Mitzenmacher for valuable conversations in articulating the relationship between these\ndeﬁnitions as well as improving the overall chapter through his insightful comments.\n16\n\nFPR\u001c+ (1\u0000FPR\u001c)FPRB[53]6. For simplicity, we set FPR\u001c= FPRB=p\u0003\n2so that FPRO\u0014p\u0003. We\ntune\u001cto achieve this FPR on ~U.\nThis setup is effective in that the learned model can be fairly small relative to the size of the data. Further,\nbecause Bloom ﬁlters scale with the size of key set, the overﬂow Bloom ﬁlter will scale with the FNR. We\nwill see experimentally that this combination is effective in decreasing the memory footprint of the existence\nindex. Finally, the learned model computation can beneﬁt from machine learning accelerators, whereas\ntraditional Bloom ﬁlters tend to be heavily dependent on the random access latency of the memory system.\n5.1.2 Bloom ﬁlters with Model-Hashes\nAn alternative approach to building existence indexes is to learn a hash function with the goal to maximize\ncollisions among keys and among non-keys while minimizing collisions of keys and non-keys. Interestingly,\nwe can use the same probabilistic classiﬁcation model as before to achieve that. That is, we can create a\nhash function d, which maps fto a bit array of size mby scaling its output as d=bf(x)\u0003mcAs such, we\ncan usedas a hash function just like any other in a Bloom ﬁlter. This has the advantage of fbeing trained\nto map most keys to the higher range of bit positions and non-keys to the lower range of bit positions (see\nFigure9(b)). A more detailed explanation of the approach is given in Appendix E.\n5.2 Results\nIn order to test this idea experimentally, we explore the application of an existence index for keeping track of\nblacklisted phishing URLs. We consider data from Google’s transparency report as our set of keys to keep\ntrack of. This dataset consists of 1.7M unique URLs. We use a negative set that is a mixture of random (valid)\nURLs and whitelisted URLs that could be mistaken for phishing pages. We split our negative set randomly\ninto train, validation and test sets. We train a character-level RNN (GRU [ 24], in particular) to predict which\nset a URL belongs to; we set \u001cbased on the validation set and also report the FPR on the test set.\nA normal Bloom ﬁlter with a desired 1% FPR requires 2.04MB. We consider a 16-dimensional GRU\nwith a 32-dimensional embedding for each character; this model is 0.0259MB. When building a comparable\nlearned index, we set \u001cfor 0.5% FPR on the validation set; this gives a FNR of 55%. (The FPR on the test set\nis 0.4976%, validating the chosen threshold.) As described above, the size of our Bloom ﬁlter scales with the\nFNR. Thus, we ﬁnd that our model plus the spillover Bloom ﬁlter uses 1.31MB, a 36% reduction in size. If\nwe want to enforce an overall FPR of 0.1%, we have a FNR of 76%, which brings the total Bloom ﬁlter size\ndown from 3.06MB to 2.59MB, a 15% reduction in memory. We observe this general relationship in Figure\n10. Interestingly, we see how different size models balance the accuracy vs. memory trade-off differently.\nWe consider brieﬂy the case where there is covariate shift in our query distribution that we have not\naddressed in the model. When using validation and test sets with only random URLs we ﬁnd that we can save\n60% over a Bloom ﬁlter with a FPR of 1%. When using validation and test sets with only the whitelisted\nURLs we ﬁnd that we can save 21% over a Bloom ﬁlter with a FPR of 1%. Ultimately, the choice of negative\nset is application speciﬁc and covariate shift could be more directly addressed, but these experiments are\nintended to give intuition for how the approach adapts to different situations.\nClearly, the more accurate our model is, the better the savings in Bloom ﬁlter size. One interesting\nproperty of this is that there is no reason that our model needs to use the same features as the Bloom ﬁlter.\nFor example, signiﬁcant research has worked on using ML to predict if a webpage is a phishing page [ 10,15].\nAdditional features like WHOIS data or IP information could be incorporated in the model, improving\naccuracy, decreasing Bloom ﬁlter size, and keeping the property of no false negatives.\nFurther, we give additional results following the approach in Section 5.1.2 in Appendix E.\n6We again thank Michael Mitzenmacher for identifying this relation when reviewing our paper.\n17\n\n 0                   0.5                  1.0                  1.5                   212345Memory Footprint (Megabytes)BloomFilter \nW=128,E=32\nW=32,E=32\nW=16,E=32 \n0\nFalse Positive Rate (%)Figure 10: Learned Bloom ﬁlter improves memory footprint at a wide range of FPRs. (Here Wis the RNN\nwidth andEis the embedding size for each character.)\n6 Related Work\nThe idea of learned indexes builds upon a wide range of research in machine learning and indexing techniques.\nIn the following, we highlight the most important related areas.\nB-Trees and variants: Over the last decades a variety of different index structures have been proposed\n[36], such as B+-trees [ 17] for disk based systems and T-trees [ 46] or balanced/red-black trees [ 16,20] for\nin-memory systems. As the original main-memory trees had poor cache behavior, several cache conscious\nB+-tree variants were proposed, such as the CSB+-tree [ 58]. Similarly, there has been work on making use\nof SIMD instructions such as FAST [ 44] or even taking advantage of GPUs [ 44,61,43]. Moreover, many\nof these (in-memory) indexes are able to reduce their storage-needs by using offsets rather than pointers\nbetween nodes. There exists also a vast array of research on index structures for text, such as tries/radix-trees\n[19, 45, 31], or other exotic index structures, which combine ideas from B-Trees and tries [48].\nHowever, all of these approaches are orthogonal to the idea of learned indexes as none of them learn from\nthe data distribution to achieve a more compact index representation or performance gains. At the same time,\nlike with our hybrid indexes, it might be possible to more tightly integrate the existing hardware-conscious\nindex strategies with learned models for further performance gains.\nSince B+-trees consume signiﬁcant memory, there has also been a lot of work in compressing indexes,\nsuch as preﬁx/sufﬁx truncation, dictionary compression, key normalization [ 36,33,55], or hybrid hot/cold\nindexes [ 75]. However, we presented a radical different way to compress indexes, which—dependent on\nthe data distribution—is able to achieve orders-of-magnitude smaller indexes and faster look-up times and\npotentially even changes the storage complexity class (e.g., O(n)toO(1)). Interestingly though, some of\nthe existing compression techniques are complimentary to our approach and could help to further improve\nthe efﬁciency. For example, dictionary compression can be seen as a form of embedding (i.e., representing a\nstring as a unique integer).\nProbably most related to this paper are A-Trees [ 32], BF-Trees [ 13], and B-Tree interpolation search [ 35].\nBF-Trees uses a B+-tree to store information about a region of the dataset, but leaf nodes are Bloom ﬁlters\nand do not approximate the CDF. In contrast, A-Trees use piece-wise linear functions to reduce the number of\nleaf-nodes in a B-Tree, and [ 35] proposes to use interpolation search within a B-Tree page. However, learned\n18\n\nindexes go much further and propose to replace the entire index structure using learned models.\nFinally, sparse indexes like Hippo [ 73], Block Range Indexes [ 63], and Small Materialized Aggregates\n(SMAs) [ 54] all store information about value ranges but again do not take advantage of the underlying\nproperties of the data distribution.\nLearning Hash Functions for ANN Indexes: There has been a lot of research on learning hash functions\n[49,68,67,59]. Most notably, there has been work on learning locality-sensitive hash (LSH) functions to\nbuild Approximate Nearest Neighborhood (ANN) indexes. For example, [ 66,68,40] explore the use of\nneural networks as a hash function, whereas [ 69] even tries to preserve the order of the multi-dimensional\ninput space. However, the general goal of LSH is to group similar items into buckets to support nearest\nneighborhood queries, usually involving learning approximate similarity measures in high-dimensional input\nspace using some variant of hamming distances. There is no direct way to adapt previous approaches to learn\nthe fundamental data structures we consider, and it is not clear whether they can be adapted.\nPerfect Hashing: Perfect hashing [ 26] is very related to our use of models for Hash-maps. Like our CDF\nmodels, perfect hashing tries to avoid conﬂicts. However, in all approaches of which we are aware, learning\ntechniques have not been considered, and the size of the function grows with the size of the data. In contrast,\nlearned hash functions can be independent of the size. For example, a linear model for mapping every other\ninteger between 0 and 200M would not create any conﬂicts and is independent of the size of the data. In\naddition, perfect hashing is also not useful for B-Trees or Bloom ﬁlters.\nBloom ﬁlters: Finally, our existence indexes directly builds upon the existing work in Bloom ﬁlters\n[29,11]. Yet again our work takes a different perspective on the problem by proposing a Bloom ﬁlter\nenhanced classiﬁcation model or using models as special hash functions with a very different optimization\ngoal than the hash-models we created for Hash-maps.\nSuccinct Data Structures: There exists an interesting connection between learned indexes and succinct\ndata structures, especially rank-select dictionaries such as wavelet trees [ 39,38]. However, many succinct\ndata structures focus on H0 entropy (i.e., the number of bits that are necessary to encode each element in the\nindex), whereas learned indexes try to learn the underlying data distribution to predict the position of each\nelement. Thus, learned indexes might achieve a higher compression rate than H0 entropy potentially at the\ncost of slower operations. Furthermore, succinct data structures normally have to be carefully constructed for\neach use case, whereas learned indexes “automate” this process through machine learning. Yet, succinct data\nstructures might provide a framework to further study learned indexes.\nModeling CDFs: Our models for both range and point indexes are closely tied to models of the CDF.\nEstimating the CDF is non-trivial and has been studied in the machine learning community [ 50] with a few\napplications such as ranking [ 42]. How to most effectively model the CDF is still an open question worth\nfurther investigation.\nMixture of Experts: Our RMI architecture follows a long line of research on building experts for subsets\nof the data [ 51]. With the growth of neural networks, this has become more common and demonstrated\nincreased usefulness [ 62]. As we see in our setting, it nicely lets us to decouple model size and model\ncomputation, enabling more complex models that are not more expensive to execute.\n7 Conclusion and Future Work\nWe have shown that learned indexes can provide signiﬁcant beneﬁts by utilizing the distribution of data being\nindexed. This opens the door to many interesting research questions.\nOther ML Models: While our focus was on linear models and neural nets with mixture of experts, there\nexist many other ML model types and ways to combine them with traditional data structures, which are worth\nexploring.\n19\n\nMulti-Dimensional Indexes: Arguably the most exciting research direction for the idea of learned\nindexes is to extend them to multi-dimensional indexes. Models, especially NNs, are extremely good at\ncapturing complex high-dimensional relationships. Ideally, this model would be able to estimate the position\nof all records ﬁltered by any combination of attributes.\nBeyond Indexing: Learned Algorithms Maybe surprisingly, a CDF model has also the potential to\nspeed-up sorting and joins, not just indexes. For instance, the basic idea to speed-up sorting is to use an\nexisting CDF model Fto put the records roughly in sorted order and then correct the nearly perfectly sorted\ndata, for example, with insertion sort.\nGPU/TPUs Finally, as mentioned several times throughout this paper, GPU/TPUs will make the idea\nof learned indexes even more valuable. At the same time, GPU/TPUs also have their own challenges, most\nimportantly the high invocation latency. While it is reasonable to assume that probably all learned indexes\nwill ﬁt on the GPU/TPU because of the exceptional compression ratio as shown before, it still requires 2-3\nmicro-seconds to invoke any operation on them. At the same time, the integration of machine learning\naccelerators with the CPU is getting better [ 6,4] and with techniques like batching requests the cost of\ninvocation can be amortized, so that we do not believe the invocation latency is a real obstacle.\nIn summary, we have demonstrated that machine learned models have the potential to provide\nsigniﬁcant beneﬁts over state-of-the-art indexes, and we believe this is a fruitful direction for future\nresearch.\nAcknowledgements: We would like to thank Michael Mitzenmacher, Chris Olston, Jonathan Bischof and many others\nat Google for their helpful feedback during the preparation of this paper.\n20\n\nReferences\n[1]Database architects blog: The case for b-tree index structures. http://databasearchitects.\nblogspot.de/2017/12/the-case-for-b-tree-index-structures.html .\n[2]Google’s sparsehash documentation. https://github.com/sparsehash/sparsehash/\nblob/master/src/sparsehash/sparse_hash_map .\n[3]An in-depth look at google’s ﬁrst tensor processing unit\n(tpu). https://cloud.google.com/blog/big-data/2017/05/\nan-in-depth-look-at-googles-first-tensor-processing-unit-tpu .\n[4]Intel Xeon Phi. https://www.intel.com/content/www/us/en/products/\nprocessors/xeon-phi/xeon-phi-processors.html .\n[5]Moore Law is Dead but GPU will get 1000X faster by 2025. https://www.nextbigfuture.\ncom/2017/06/moore-law-is-dead-but-gpu-will-get-1000x-faster-by-2025.\nhtml .\n[6]NVIDIA NVLink High-Speed Interconnect. http://www.nvidia.com/object/nvlink.\nhtml .\n[7]Stanford DAWN cuckoo hashing. https://github.com/stanford-futuredata/\nindex-baselines .\n[8]Trying to speed up binary search. http://databasearchitects.blogspot.com/2015/\n09/trying-to-speed-up-binary-search.html .\n[9]M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,\net al. Tensorﬂow: A system for large-scale machine learning. In OSDI , volume 16, pages 265–283,\n2016.\n[10] S. Abu-Nimeh, D. Nappa, X. Wang, and S. Nair. A comparison of machine learning techniques for\nphishing detection. In eCrime , pages 60–69, 2007.\n[11] K. Alexiou, D. Kossmann, and P.-A. Larson. Adaptive range ﬁlters for cold data: Avoiding trips to\nsiberia. Proc. VLDB Endow. , 6(14):1714–1725, Sept. 2013.\n[12] M. Armbrust, A. Fox, D. A. Patterson, N. Lanham, B. Trushkowsky, J. Trutna, and H. Oh. SCADS:\nscale-independent storage for social computing applications. In CIDR , 2009.\n[13] M. Athanassoulis and A. Ailamaki. BF-tree: Approximate Tree Indexing. In VLDB , pages 1881–1892,\n2014.\n[14] Performance comparison: linear search vs binary search. https://dirtyhandscoding.\nwordpress.com/2017/08/25/performance-comparison-linear-search-vs-binary-search/ .\n[15] R. B. Basnet, S. Mukkamala, and A. H. Sung. Detection of phishing attacks: A machine learning\napproach. Soft Computing Applications in Industry , 226:373–383, 2008.\n[16] R. Bayer. Symmetric binary b-trees: Data structure and maintenance algorithms. Acta Inf. , 1(4):290–306,\nDec. 1972.\n21\n\n[17] R. Bayer and E. McCreight. Organization and maintenance of large ordered indices. In SIGFIDET\n(Now SIGMOD) , pages 107–141, 1970.\n[18] S. Bickel, M. Br ¨uckner, and T. Scheffer. Discriminative learning under covariate shift. Journal of\nMachine Learning Research , 10(Sep):2137–2155, 2009.\n[19] M. B ¨ohm, B. Schlegel, P. B. V olk, U. Fischer, D. Habich, and W. Lehner. Efﬁcient in-memory indexing\nwith generalized preﬁx trees. In BTW , pages 227–246, 2011.\n[20] J. Boyar and K. S. Larsen. Efﬁcient rebalancing of chromatic search trees. Journal of Computer and\nSystem Sciences , 49(3):667 – 682, 1994. 30th IEEE Conference on Foundations of Computer Science.\n[21] M. Brantner, D. Florescu, D. A. Graf, D. Kossmann, and T. Kraska. Building a database on S3. In\nSIGMOD , pages 251–264, 2008.\n[22] A. Broder and M. Mitzenmacher. Network applications of bloom ﬁlters: A survey. Internet mathematics ,\n1(4):485–509, 2004.\n[23] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, T. Chandra, A. Fikes, and\nR. Gruber. Bigtable: A distributed storage system for structured data (awarded best paper!). In OSDI ,\npages 205–218, 2006.\n[24] K. Cho, B. van Merrienboer, C ¸. G¨ulc ¸ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y . Bengio.\nLearning phrase representations using RNN encoder-decoder for statistical machine translation. In\nEMNLP , pages 1724–1734, 2014.\n[25] A. Crotty, A. Galakatos, K. Dursun, T. Kraska, C. Binnig, U. C ¸etintemel, and S. Zdonik. An architecture\nfor compiling udf-centric workﬂows. PVLDB , 8(12):1466–1477, 2015.\n[26] M. Dietzfelbinger, A. Karlin, K. Mehlhorn, F. Meyer auF der Heide, H. Rohnert, and R. E. Tarjan.\nDynamic perfect hashing: Upper and lower bounds. SIAM Journal on Computing , 23(4):738–761,\n1994.\n[27] J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and stochastic\noptimization. Journal of Machine Learning Research , 12(Jul):2121–2159, 2011.\n[28] A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic minimax character of the sample distribution\nfunction and of the classical multinomial estimator. The Annals of Mathematical Statistics , pages\n642–669, 1956.\n[29] B. Fan, D. G. Andersen, M. Kaminsky, and M. D. Mitzenmacher. Cuckoo ﬁlter: Practically better than\nbloom. In CoNEXT , pages 75–88, 2014.\n[30] T. Fawcett. An introduction to roc analysis. Pattern recognition letters , 27(8):861–874, 2006.\n[31] E. Fredkin. Trie memory. Commun. ACM , 3(9):490–499, Sept. 1960.\n[32] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska. A-tree: A bounded approximate\nindex structure. CoRR , abs/1801.10207, 2018.\n[33] J. Goldstein, R. Ramakrishnan, and U. Shaft. Compressing Relations and Indexes. In ICDE , pages\n370–379, 1998.\n22\n\n[34] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and\nY . Bengio. Generative adversarial nets. In NIPS , pages 2672–2680, 2014.\n[35] G. Graefe. B-tree indexes, interpolation search, and skew. In DaMoN , 2006.\n[36] G. Graefe and P. A. Larson. B-tree indexes and CPU caches. In ICDE , pages 349–358, 2001.\n[37] A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 ,\n2013.\n[38] R. Grossi, A. Gupta, and J. S. Vitter. High-order entropy-compressed text indexes. In SODA , pages\n841–850. Society for Industrial and Applied Mathematics, 2003.\n[39] R. Grossi and G. Ottaviano. The wavelet trie: Maintaining an indexed sequence of strings in compressed\nspace. In PODS , pages 203–214, 2012.\n[40] J. Guo and J. Li. CNN based hashing for image retrieval. CoRR , abs/1509.01354, 2015.\n[41] M. Gupta, A. Cotter, J. Pfeifer, K. V oevodski, K. Canini, A. Mangylov, W. Moczydlowski, and\nA. Van Esbroeck. Monotonic calibrated interpolated look-up tables. The Journal of Machine Learning\nResearch , 17(1):3790–3836, 2016.\n[42] J. C. Huang and B. J. Frey. Cumulative distribution networks and the derivative-sum-product algorithm:\nModels and inference for cumulative distribution functions on graphs. J. Mach. Learn. Res. , 12:301–348,\nFeb. 2011.\n[43] K. Kaczmarski. B + -Tree Optimized for GPGPU . 2012.\n[44] C. Kim, J. Chhugani, N. Satish, E. Sedlar, A. D. Nguyen, T. Kaldewey, V . W. Lee, S. A. Brandt, and\nP. Dubey. Fast: Fast architecture sensitive tree search on modern cpus and gpus. In SIGMOD , pages\n339–350, 2010.\n[45] T. Kissinger, B. Schlegel, D. Habich, and W. Lehner. Kiss-tree: Smart latch-free in-memory indexing\non modern architectures. In DaMoN , pages 16–23, 2012.\n[46] T. J. Lehman and M. J. Carey. A study of index structures for main memory database management\nsystems. In VLDB , pages 294–303, 1986.\n[47] V . Leis. FAST source. http://www-db.in.tum.de/leis/index/fast.cpp .\n[48] V . Leis, A. Kemper, and T. Neumann. The adaptive radix tree: Artful indexing for main-memory\ndatabases. In ICDE , pages 38–49, 2013.\n[49] W. Litwin. Readings in database systems. chapter Linear Hashing: A New Tool for File and Table\nAddressing., pages 570–581. Morgan Kaufmann Publishers Inc., 1988.\n[50] M. Magdon-Ismail and A. F. Atiya. Neural networks for density estimation. In M. J. Kearns, S. A.\nSolla, and D. A. Cohn, editors, NIPS , pages 522–528. MIT Press, 1999.\n[51] D. J. Miller and H. S. Uyar. A mixture of experts classiﬁer with learning based on both labelled and\nunlabelled data. In NIPS , pages 571–577, 1996.\n[52] M. Mitzenmacher. Compressed bloom ﬁlters. In PODC , pages 144–150, 2001.\n23\n\n[53] M. Mitzenmacher. A model for learned bloom ﬁlters and related structures. arXiv preprint\narXiv:1802.00884 , 2018.\n[54] G. Moerkotte. Small Materialized Aggregates: A Light Weight Index Structure for Data Warehousing.\nInVLDB , pages 476–487, 1998.\n[55] T. Neumann and G. Weikum. RDF-3X: A RISC-style Engine for RDF. Proc. VLDB Endow. , pages\n647–659, 2008.\n[56] OpenStreetMap database c\rOpenStreetMap contributors. https://aws.amazon.com/\npublic-datasets/osm .\n[57] R. Pagh and F. F. Rodler. Cuckoo hashing. Journal of Algorithms , 51(2):122–144, 2004.\n[58] J. Rao and K. A. Ross. Making b+- trees cache conscious in main memory. In SIGMOD , pages 475–486,\n2000.\n[59] S. Richter, V . Alvarez, and J. Dittrich. A seven-dimensional analysis of hashing methods and its\nimplications on query processing. Proc. VLDB Endow. , 9(3):96–107, Nov. 2015.\n[60] D. G. Severance and G. M. Lohman. Differential ﬁles: Their application to the maintenance of large\ndata bases. In SIGMOD , pages 43–43, 1976.\n[61] A. Shahvarani and H.-A. Jacobsen. A hybrid b+-tree as solution for in-memory indexing on cpu-gpu\nheterogeneous computing platforms. In SIGMOD , pages 1523–1538, 2016.\n[62] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large\nneural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 , 2017.\n[63] M. Stonebraker and L. A. Rowe. The Design of POSTGRES. In SIGMOD , pages 340–355, 1986.\n[64] I. Sutskever, O. Vinyals, and Q. V . Le. Sequence to sequence learning with neural networks. In NIPS ,\npages 3104–3112, 2014.\n[65] F. Tram `er, A. Kurakin, N. Papernot, D. Boneh, and P. McDaniel. Ensemble adversarial training: Attacks\nand defenses. arXiv preprint arXiv:1705.07204 , 2017.\n[66] M. Turcanik and M. Javurek. Hash function generation by neural network. In NTSP , pages 1–5, Oct\n2016.\n[67] J. Wang, W. Liu, S. Kumar, and S. F. Chang. Learning to hash for indexing big data;a survey. Proceedings\nof the IEEE , 104(1):34–57, Jan 2016.\n[68] J. Wang, H. T. Shen, J. Song, and J. Ji. Hashing for similarity search: A survey. CoRR , abs/1408.2927,\n2014.\n[69] J. Wang, J. Wang, N. Yu, and S. Li. Order preserving hashing for approximate nearest neighbor search.\nInMM, pages 133–142, 2013.\n[70] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey, M. Krikun, Y . Cao, Q. Gao,\nK. Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and\nmachine translation. arXiv preprint arXiv:1609.08144 , 2016.\n[71] S. You, D. Ding, K. Canini, J. Pfeifer, and M. Gupta. Deep lattice networks and partial monotonic\nfunctions. In NIPS , pages 2985–2993, 2017.\n24\n\n[72] Y . You, Z. Zhang, C. Hsieh, J. Demmel, and K. Keutzer. Imagenet training in minutes. CoRR,\nabs/1709.05011 , 2017.\n[73] J. Yu and M. Sarwat. Two Birds, One Stone: A Fast, Yet Lightweight, Indexing Scheme for Modern\nDatabase Systems. In VLDB , pages 385–396, 2016.\n[74] E. Zamanian, C. Binnig, T. Kraska, and T. Harris. The end of a myth: Distributed transaction can scale.\nPVLDB , 10(6):685–696, 2017.\n[75] H. Zhang, D. G. Andersen, A. Pavlo, M. Kaminsky, L. Ma, and R. Shen. Reducing the storage overhead\nof main-memory OLTP databases with hybrid indexes. In SIGMOD , pages 1567–1581, 2016.\n25\n\nA Theoretical Analysis of Scaling Learned Range Indexes\nOne advantage of framing learned range indexes as modeling the cumulative distribution function (CDF) of\nthe data is that we can build on the long research literature on modeling the CDF. Signiﬁcant research has\nstudied the relationship between a theoretical CDF F(x)and the empirical CDF of data sampled from F(x).\nWe consider the case where we have sampled i.i.d. Ndatapoints,Y, from some distribution, and we will use\n^FN(x)to denote the empirical cumulative distribution function:\n^FN(x) =P\ny2Y1y\u0014x\nN: (2)\nOne theoretical question about learned indexes is: how well do they scale with the size of the data N? In\nour setting, we learn a model F(x)to approximate the distribution of our data ^FN(x). Here, we assume we\nknow the distribution F(x)that generated the data and analyze the error inherent in the data being sampled\nfrom that distribution7. That is, we consider the error between the distribution of data ^FN(x)and our model\nof the distribution F(x). Because ^FN(x)is a binomial random variable with mean F(x), we ﬁnd that the\nexpected squared error between our data and our model is given by\nE\u0014\u0010\nF(x)\u0000^FN(x)\u00112\u0015\n=F(x)(1\u0000F(x))\nN: (3)\nIn our application the look-up time scales with the average error in the number of positions in the sorted\ndata; that is, we are concerned with the error between our model NF(x)and the key position N^FN(x). With\nsome minor manipulation of Eq. (3), we ﬁnd that the average error in the predicted positions grows at a rate\nofO(p\nN). Note that this sub-linear scaling in error for a constant-sized model is an improvement over the\nlinear scaling achieved by a constant-sized B-Tree. This provides preliminary understanding of the scalability\nof our approach and demonstrates how framing indexing as learning the CDF lends itself well to theoretical\nanalysis.\nB Separated Chaining Hash-map\nWe evaluated the potential of learned hash functions using a separate chaining Hash-map; records are stored\ndirectly within an array and only in the case of a conﬂict is the record attached to the linked-list. That is\nwithout a conﬂict there is at most one cache miss. Only in the case that several keys map to the same position,\nadditional cache-misses might occur. We choose that design as it leads to the best look-up performance even\nfor larger payloads. For example, we also tested a commercial-grade dense Hash-map with a bucket-based\nin-place overﬂow (i.e., the Hash-map is divided into buckets to minimize overhead and uses in-place overﬂow\nif a bucket is full [ 2]). While it is possible to achieve a lower footprint using this technique, we found that it\nis also twice as slow as the separate chaining approach. Furthermore, at 80% or more memory utilization the\ndense Hash-maps degrade further in performance. Of course many further (orthogonal) optimizations are\npossible and by no means do we claim that this is the most memory or CPU efﬁcient implementation of a\nHash-map. Rather we aim to demonstrate the general potential of learned hash functions.\nAs the baseline for this experiment we used our Hash-map implementation with a MurmurHash3-like\nhash-function. As the data we used the three integer datasets from Section 3.7 and as the model-based\nHash-map the 2-stage RMI model with 100k models on the 2nd stage and no hidden layers from the same\nsection. For all experiments we varied the number of available slots from 75% to125% of the data. That\nis, with 75% there are 25% less slots in the Hash-map than data records. Forcing less slots than the data\n7Learning F(x)can improve or worsen the error, but we take this as a reasonable assumption for some applications, such as data\nkeyed by a random hash.\n26\n\nDatasetSlotsHash TypeTime (ns)Empty SlotsSpaceModel Hash670.18GB0.21xRandom Hash520.84GBModel Hash530.35GB0.22xRandom Hash481.58GBModel Hash641.47GB0.60xRandom Hash492.43GBModel Hash780.64GB0.77xRandom Hash530.83GBModel Hash631.09GB0.70xRandom Hash501.56GBModel Hash772.20GB0.91xRandom Hash502.41GBModel Hash790.63GB0.79xRandom Hash520.80GBModel Hash661.10GB0.73xRandom Hash461.50GBModel Hash772.16GB0.94xRandom Hash462.31GBLog Normal75%100%125%75%100%125%75%100%125%MapWebFigure 11: Model vs Random Hash-map\nsize, minimizes the empty slots within the Hash-map at the expense of longer linked lists. However, for\nHash-maps we store the full records, which consist of a 64bit key, 64bit payload, and a 32bit meta-data ﬁeld\nfor delete ﬂags, version nb, etc. (so a record has a ﬁxed length of 20 Bytes); note that our chained hash-map\nadds another 32bit pointer, making it a 24Byte slot.\nThe results are shown in Figure 11, listing the average look-up time, the number of empty slots in GB and\nthe space improvement as a factor of using a randomized hash function. Note, that in contrast to the B-Tree\nexperiments, we do include the data size . The main reason is that in order to enable 1 cache-miss look-ups,\nthe data itself has to be included in the Hash-map, whereas in the previous section we only counted the extra\nindex overhead excluding the sorted array itself.\nAs can be seen in Figure 11, the index with the model hash function overall has similar performance\nwhile utilizing the memory better. For example, for the map dataset the model hash function only “wastes”\n0.18GB in slots, an almost 80% reduction compared to using a random hash function. Obviously, the moment\nwe increase the Hash-map in size to have 25% more slots, the savings are not as large, as the Hash-map is\nalso able to better spread out the keys. Surprisingly if we decrease the space to 75% of the number of keys,\nthe learned Hash-map still has an advantage because of the still prevalent birthday paradox.\nC Hash-Map Comparison Against Alternative Baselines\nIn addition to the separate chaining Hash-map architecture, we also compared learned point indexes against\nfour alternative Hash-map architectures and conﬁgurations:\nA VX Cuckoo Hash-map: We used an A VX optimized Cuckoo Hash-map from [7].\nCommercial Cuckoo Hash-map: The implementation of [ 7] is highly tuned, but does not handle all\ncorner cases. We therefore also compared against a commercially used Cuckoo Hash-map.\nIn-place chained Hash-map with learned hash functions: One signiﬁcant downside of separate chain-\ning is that it requires additional memory for the linked list. As an alternative, we implemented a chained\nHash-map, which uses a two pass algorithm: in the ﬁrst pass, the learned hash function is used to put items\ninto slots. If a slot is already taken, the item is skipped. Afterwards we use a separate chaining approach for\nevery skipped item except that we use the remaining free slots with offsets as pointers for them. As a result,\n27\n\nthe utilization can be 100% (recall, we do not consider inserts) and the quality of the learned hash function\ncan only make an impact on the performance not the size: the fewer conﬂicts, the fewer cache misses. We\nused a simple single stage multi-variate model as the learned hash function and implemented the Hash-map\nincluding the model outside of our benchmarking framework to ensure a fair comparison.\nType Time (ns) Utilization\nA VX Cuckoo, 32-bit value 31ns 99%\nA VX Cuckoo, 20 Byte record 43ns 99%\nComm. Cuckoo, 20Byte record 90ns 95%\nIn-place chained Hash-map with learned hash functions, record 35ns 100%\nTable 1: Hash-map alternative baselines\nLike in Section B our records are 20 Bytes large and consist of a 64bit key, 64bit payload, and a 32bit\nmeta-data ﬁeld as commonly found in real applications (e.g., for delete ﬂags, version numbers, etc.). For\nall Hash-map architectures we tried to maximize utilization and used records, except for the A VX Cuckoo\nHash-map where we also measured the performance for 32bit values. As the dataset we used the log-normal\ndata and the same hardware as before. The results are shown in Table 1.\nThe results for the A VX cuckoo Hash-map show that the payload has a signiﬁcant impact on the\nperformance. Going from 8 Byte to 20 Byte decreases the performance by almost 40%. Furthermore, the\ncommercial implementation which handles all corner cases but is not very A VX optimized slows down the\nlookup by another factor of 2. In contrast, our learned hash functions with in-place chaining can provide\nbetter lookup performance than even the cuckoo Hash-map for our records. The main take-aways from this\nexperiment is that learned hash functions can be used with different Hash-map architectures and that the\nbeneﬁts and disadvantages highly depend on the implementation, data and workload.\nD Future Directions for Learned B-Trees\nIn the main part of the paper, we have focused on index-structures for read-only, in-memory database systems.\nHere we outline how the idea of learned index structures could be extended in the future.\nD.1 Inserts and Updates\nOn ﬁrst sight, inserts seem to be the Achilles heel of learned indexes because of the potentially high cost\nfor learning models, but yet again learned indexes might have a signiﬁcant advantage for certain workloads.\nIn general we can distinguish between two types of inserts: (1) appends and (2) inserts in the middle like\nupdating a secondary index on the customer-id over an order table.\nLet’s for the moment focus on the ﬁrst case: appends. For example, it is reasonable to assume that for\nan index over the timestamps of web-logs, like in our previous experiments, most if not all inserts will be\nappends with increasing timestamps. Now, let us further assume that our model generalizes and is able to\nlearn the patterns, which also hold for the future data. As a result, updating the index structure becomes an\nO(1)operation; it is a simple append and no change of the model itself is needed, whereas a B-Tree requires\nO(logn)operations to keep the B-Tree balance. A similar argument can also be made for inserts in the\nmiddle, however, those might require to move data or reserve space within the data, so that the new items can\nbe put into the right place.\nObviously, this observation also raises several questions. First, there seems to be an interesting trade-off\nin the generalizability of the model and the “last mile” performance; the better the “last mile” prediction,\narguably, the more the model is overﬁtting and less able to generalize to new data items.\n28\n\nSecond, what happens if the distribution changes? Can it be detected, and is it possible to provide\nsimilar strong guarantees as B-Trees which always guarantee O(logn)look-up and insertion costs? While\nanswering this question goes beyond the scope of this paper, we believe that it is possible for certain models\nto achieve it. More importantly though, machine learning offers new ways to adapt the models to changes in\nthe data distribution, such as online learning, which might be more effective than traditional B-Tree balancing\ntechniques. Exploring them also remains future work.\nFinally, it should be pointed out that there always exists a much simpler alternative to handling inserts\nby building a delta-index [ 60]. All inserts are kept in buffer and from time to time merged with a potential\nretraining of the model. This approach is already widely used, for example in Bigtable [ 23] and many other\nsystems, and was recently explored in [32] for learned indexes.\nD.2 Paging\nThroughout this section we assumed that the data, either the actual records or the <key,pointer> pairs,\nare stored in one continuous block. However, especially for indexes over data stored on disk, it is quite\ncommon to partition the data into larger pages that are stored in separate regions on disk. To that end, our\nobservation that a model learns the CDF no longer holds true as pos= Pr(X < Key)\u0003Nis violated. In the\nfollowing we outline several options to overcome this issue:\nLeveraging the RMI structure: The RMI structure already partitions the space into regions. With small\nmodiﬁcations to the learning process, we can minimize how much models overlap in the regions they cover.\nFurthermore, it might be possible to duplicate any records which might be accessed by more than one model.\nAnother option is to have an additional translation table in the form of <first_key, disk-position> .\nWith the translation table the rest of the index structure remains the same. However, this idea will work best\nif the disk pages are very large. At the same time it is possible to use the predicted position with the min- and\nmax-error to reduce the number of bytes which have to be read from a large page, so that the impact of the\npage size might be negligible.\nWith more complex models, it might actually be possible to learn the actual pointers of the pages.\nEspecially if a ﬁle-system is used to determine the page on disk with a systematic numbering of the blocks on\ndisk (e.g., block1,...,block100 ) the learning process can remain the same.\nObviously, more investigation is required to better understand the impact of learned indexes for disk-based\nsystems. At the same time the signiﬁcant space savings as well as speed beneﬁts make it a very interesting\navenue for future work.\nE Further Bloom ﬁlter Results\nIn Section 5.1.2, we propose an alternative approach to a learned Bloom ﬁlter where the classiﬁer output is\ndiscretized and used as an additional hash function in the traditional Bloom ﬁlter setup. Preliminary results\ndemonstrate that this approach in some cases outperforms the results listed in Section 5.2, but as the results\ndepend on the discretization scheme, further analysis is worthwhile. We describe below these additional\nexperiments.\nAs before, we assume we have a model model f(x)![0;1]that maps keys to the range [0;1]. In this\ncase, we allocate mbits for a bitmap Mwhere we set M[bmf(x)c] = 1 for all inserted keys x2K. We can\nthen observe the FPR by observing what percentage of non-keys in the validation set map to a location in\nthe bitmap with a value of 1, i.e. FPRm\u0011P\nx2~UM[bf(x)mc]\nj~Uj. In addition, we have a traditional Bloom ﬁlter\nwith false positive rate FPRB. We say that a query qis predicted to be a key if M[bf(q)mc] = 1 and the\nBloom ﬁlter also returns that it is a key. As such, the overall FPR of the system is FPRm\u0002FPRB; we can\n29\n\ndetermine the size of the traditional Bloom ﬁlter based on it’s false positive rate FPRB=p\u0003\nFPR mwherep\u0003is\nthe desired FPR for the whole system.\nAs in Section 5.2, we test our learned Bloom ﬁlter on data from Google’s transparency report. We use\nthe same character RNN trained with a 16-dimensional width and 32-dimensional character embeddings.\nScanning over different values for m, we can observe the total size of the model, bitmap for the learned Bloom\nﬁlter, and the traditional Bloom ﬁlter. For a desired total FPR p\u0003= 0:1%, we ﬁnd that setting m= 1000000\ngives a total size of 2.21MB, a 27.4% reduction in memory, compared to the 15% reduction following the\napproach in Section 5.1.1 and reported in Section 5.2. For a desired total FPR p\u0003= 1% we get a total size of\n1.19MB, a 41% reduction in memory, compared to the 36% reduction reported in Section 5.2.\nThese results are a signiﬁcant improvement over those shown in Section 5.2. However, typical measures\nof accuracy or calibration do not match this discretization procedure, and as such further analysis would be\nvaluable to understand how well model accuracy aligns with it’s suitability as a hash function.\n30",
  "textLength": 97883
}