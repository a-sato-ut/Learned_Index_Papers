{
  "paperId": "e8f54f41469be8b205beaba4f8f42a949e64003f",
  "title": "Learning-based Sketches for Frequency Estimation in Data Streams without Ground Truth",
  "pdfPath": "e8f54f41469be8b205beaba4f8f42a949e64003f.pdf",
  "text": "JOURNAL OF L ATEX CLASS FILES 1\nLearning-based Sketches for Frequency Estimation\nin Data Streams without Ground Truth\nXinyu Yuan , Yan QiaoB,Member, IEEE, Meng Li ,Senior Member, IEEE, Zhenchun Wei ,Member, IEEE,\nCuiying Feng , Zonghui WangBand Wenzhi Chen ,Member, IEEE\nAbstract—Estimating the frequency of items on the high-\nvolume, fast data stream has been extensively studied in many\nareas, such as database and network measurement. Traditional\nsketches provide only coarse estimates under strict memory\nconstraints. Although some learning-augmented methods have\nemerged recently, they typically rely on offline training with\nreal frequencies or/and labels, which are often unavailable.\nMoreover, these methods suffer from slow update speeds, limiting\ntheir suitability for real-time processing despite offering only\nmarginal accuracy improvements. To overcome these challenges,\nwe propose UCL-sketch, a practical learning-based paradigm\nfor per-key frequency estimation. Our design introduces two key\ninnovations: (i) an online training mechanism based on equivalent\nlearning that requires no ground truth (GT), and (ii) a highly\nscalable architecture leveraging logically structured estimation\nbuckets to scale to real-world data stream. The UCL-sketch,\nwhich utilizes compressive sensing (CS), converges to an estimator\nthat provably yields a error bound far lower than that of prior\nworks, without sacrificing the speed of processing. Extensive ex-\nperiments on both real-world and synthetic datasets demonstrate\nthat our approach outperforms previously proposed approaches\nregarding per-key accuracy and distribution. Notably, under\nextremely tight memory budgets, its quality almost matches\nthat of an (infeasible) omniscient oracle. Moreover, compared\nto the existing equation-based sketch, UCL-sketch achieves an\naverage decoding speedup of nearly 500 times. To help further\nresearch and development, our code is publicly available at\nhttps://github.com/Y-debug-sys/UCL-sketch.\nIndex Terms—machine learning, frequency estimation, sketch,\ndata streams, self-supervised learning\nI. INTRODUCTION\nThe frequency or volume estimation of unending data\nstreams is a concern in many domains, starting with telecom-\nmunications but spreading to social networks, finance, and\nwebsite engine. In network fields, for example, professionals\nwant to keep track of the activity frequency to identify overall\nnetwork health and potential anomalies or changes in behavior,\nXinyu Yuan was with the School of Computer Science and Information En-\ngineering, Hefei University of Technology, Hefei, 230601, China. He is now\nwith the College of Computer Science and Technology, Zhejiang University,\nHangzhou, 310027, China (Email: yxy5315@gmail.com).\nYan Qiao and Meng Li are with the Key Laboratory of Knowledge Engi-\nneering with Big Data, Hefei University of Technology, Hefei, 230601, China.\nZhenchun Wei is with the School of Computer Science and Information En-\ngineering, Hefei University of Technology, Hefei, 230601, China. (Email:\n{qiaoyan,mengli,weizc}@hfut.edu.cn)\nZonghui Wang and Wenzhi Chen are with the College of Computer Sci-\nence and Technology, Zhejiang University, Hangzhou, 310027, China. (Email:\n{zhwang,chenwz}@zju.edu.cn)\nCuiying Feng is with the School of Information and Software Engineering,\nUniversity of Electronic Science and Technology of China, Chengdu, 610054,\nChina. (Email: cfeng19@uestc.edu.cn)\nLearned\nOracleBuckets\n& Sketch\nBuckets\n& SketchLearned\nSolverPast Streaming \nDatasetLabel\nOffline \nTraining\nkey\nfreqx…\n…y\nInsertion\nInsertionRecovery\nRecovery…\n…Input Data Stream\nInput Data StreamOnline \nTrainingSketch Counters\nSampleOur ApproachPrior Works\nICLR 2019, 2020\nICML 2021\nNeurIPS 2023Collect\nkey\nfreqx…\n…y\nFig. 1.Comparison between the previous learning-augmented sketches\nand our studied learning-based sketch:In our approach, we empower the\nsketch with learning technologies in the recovery phase to improve streaming\nthroughput. The model is online trained using just compressed counters in the\nsketch, which is much more practical and efficient than the prior works.\nwhich, however, is often challenging because the amount of\ninformation may be too large to store in an embedded device\nor to keep conveniently in fast storage [1]. As a consequence,\nsketch, which is a set of counters or bitmaps associated with\nhash functions, and a set of simple operations that record\napproximate information [2], has grown in popularity in the\ncontext of high-velocity data streams and limited compu-\ntational resources. Such an approximate algorithm is much\nfaster and more efficient, yet this comes at the expense of\nunsatisfactory accuracy and cover proportion, especially when\nfacing unbalanced stream characteristics, such as a Zipf or\nPower-law distribution [3], [4].\nOn the other hand, recent years have witnessed the inte-\ngration of deep learning technology with numerous classic\nalgorithms: index [5], bloom filter [6], [7], caching [8], graph\noptimization [9] and so on. In particular, the research about\nlearning-augmented streaming algorithms [10]–[15] is receiv-\ning significant attention due to the powerful potential of ma-\nchine learning (ML) to relieve or eliminate the binding of data\ncharacteristics and the sketch design. Their typical workflow\ninvolves training a heavy hitter oracle, which receives a key\nand returns a prediction of whether it will be heavy or not, thenarXiv:2412.03611v4  [cs.LG]  13 Oct 2025\n\nJOURNAL OF L ATEX CLASS FILES 2\ninserts the most frequent keys into unique buckets and applies\na sketch to the remaining keys. Although filtering heavy items\nhas been proven to improve the overall sketch performance on\nheavy-tailed distribution [4], [10], these offline and supervised\nmethods could hardly work in real-world applications. First, an\nunavoidable difficulty in designing algorithms in the learned\nsketch model is that ground truth like actual frequencies or\nlabels for which key is large are not known in advance [11].\nMoreover, since their models are only fitted on the past data,\nthe prediction performance of the oracle tends to deteriorate\nrapidly over time. That is to say, the neural network must\nbe retrained with new labeled datasets frequently, therefore,\nall the above-mentioned sketches face a common problem\nin terms of updating the out-of-date classifier [16]. Besides,\nthe idea of passing a deep model to reduce conflicts also\nincurs more insertion time and space cost, which may not be\nadvisable for hashing-intense situations by considering data\nstream is processed sequentially in only one pass.\nBuilding upon the limitations observed in the “hashing-\nenhanced” learning strategy, our study here is primarily mo-\ntivated by equation-based sketches [17]–[19], from the per-\nspective of compressed nature of sketching algorithms [20].\nSpecifically, they employed a compressive sensing (CS) ap-\nproach in thequery phaseto achieve a very low relative\nerror, given counter values and per-key aggregations. While\nthese works showed great applications of the CS theory [21]\nto sketch-based estimation, their sensing matrix constructing\nand iterative-optimization-style recovery operation to get all\nfrequencies of observed keys introduces considerable time and\nmemory complexity even with top-performing solvers [22].\nConsequently, one cannot help but pose the following\nintriguing question:Can we design a learning-based sketch\n(without ground truth) via the linear system by training and\nrecovering on the fly?\nThe comparison between the question-oriented approach in\nthis work and existing learned sketches is shown in Fig. 1.\nWithout slowing down the conventional sketch insertions,\nour approach continuously trains the ML model to recover\nper-key frequencies using only sketch counters. Although\npromising, the process raises the following two challenges: (1)\nThe first isself-supervision. Unsupervised learning, without\naccess to real frequencies or labels, is crucial for overcoming\nthe impracticalities associated with existing learning-based\nfrequency estimation algorithms. (2) The second isscalability\norcomplexity. Many modern streaming scenarios have evolved\ninto complex systems featuring tens of thousands of distinct\nitems, and the entire key space invariably includes some\nuncertain keys that will be observed in the future. To decrease\nthe complexity for per-key prediction, models dealing with\nsuch data need to be highly scalable as the size of streams\ngrows infinitely.\nTo address these challenges, we introduce a new frequency\nestimation framework calledUCL-sketch(Unsupervised\nCompressiveLearningSketch), which aims to integrate the\nadvantages of both equation-based and learned sketching ap-\nproaches. This framework achieves a significant improvement\nin practical feasibility and accuracy compared to learning-\naugmented algorithms, while maintaining substantially lowerquery overhead than equation-based competitors. A notable\ndistinction from prior works is that our model is entirely\nground-truth free, relying solely on downsampled frequen-\ncies for online training. This property endows UCL-sketch\nwith great flexibility and the capability for quick response\nto streaming distribution drift. To realize these benefits, we\ntheoretically and empirically demonstrate that the recovery\nfunction can be learned from compressed measurements alone\nusing anequivalent learningscheme, given per-key aggre-\ngations and keys set. Additionally, to mitigate the impact\nof large-scale and unbounded streams, we adopt the concept\noflogical bucketsto split and jointly learn multiple bucket-\nassociated mappings with shared parameters, leading to an\nefficient and expandable architecture. Experimental results\ndemonstrate the potential of our proposed algorithm through\ndetailed evaluations of the frequency estimation problem. Our\ncontributions can be summarized as follows:\n•We pioneer a new learning-based paradigm in frequency\nestimation by introducing UCL-sketch—a scalable, fast,\nand fully unsupervised framework that directly recovers\nper-key frequencies from sketch counters, advancing the\npracticality of existing learning-augmented sketches.\n•We present a comprehensive analysis of UCL-sketch. Our\ntheoretical results establish that its accuracy is fundamen-\ntally sound, and so high performance in practice can be\nachieved by just sampling sufficient “snapshots” of sketch\ncounters. We also provide rigorous justification for the\ndesign insights behind each of its key components.\n•Extensive empirical evaluations compare UCL-sketch,\nin terms of both quality and runtimes, to frequency\nestimation with state-of-the-art sketches using both pub-\nlicly available data and synthetic data. Our evaluation\ncovers small and large streams, different skewness of\ndata, and different memory usage. The main results\nshow that 1UCL-sketch achieves estimation quality\nalmost matching that of an infeasible oracle with perfect\nknowledge of future insertions, and its accuracy remains\nlargely unaffected by changes in storage resources and\nstream conditions. 2By invoking a learned solver in the\nquery time, UCL-sketch achieves runtimes 1-2 orders of\nmagnitude faster than running a (greedy) CS algorithm.\nWe organize the reminder of the paper as follows. Section II\nreviews related work on sketching algorithms. Section III\nintroduces the system model of equation-based sketches and\npresents the motivations behind this work. Section IV details\nthe design of the proposed frequency estimation framework.\nSection V provides a comprehensive theoretical analysis to\nsupport our design. Section VI evaluates the proposed frame-\nwork through extensive experiments on both real-world and\nsynthetic data streams. Finally, Section VII concludes the\npaper and discusses potential directions for future research.\nII. RELATEDWORK\nClassic Sketch. A sketch is a compact structure and solution\nwhich takes limited space to support approximate frequency\nqueries over high-speed data streams. The most representative\n\nJOURNAL OF L ATEX CLASS FILES 3\nData Plane Control Plane\nMonitor DeviceCentralized ServerKey Tracking Mechanism\nData Structure (Local Counters)\n()11k , v\n()11k , v\n()22k , v\n()13k , v\n() ijk , v\n…Hash Functions \n& Keys Set\n1c\n2c\n3c\n4c\n5c\n6c\n…\n…\nkc\nk1c+\nk2c+\nk3c+\nk4c+\nk5c+InsertQueryData Stream\nk6c+\nApplication\n2kcReport\nSampleConstruct\nEquation\nSolverMeasurement VectorSensing Matrix\nPer-key \nVolume \nVector\nFig. 2.The overall processing framework of equation-based sketch:In the data plane, it builds a local sketch to record the data stream and a key tracking\nmechanism for new item identification and reporting. After the centralized server receives sketch counters and keys from the monitor device, the control plane\ncan recover the frequencies through solving an under-constrained equation system.\nalgorithms [4], [23]–[28] include Count-Min Sketch (CM-\nsketch), Count Sketch (C-sketch), and Augmented Sketch (A-\nsketch). They adopt a common underlying structure which\nis essentially aw×darray of counters for preserving key\nfrequencies. Each of thedrows of the array is associated\nwith a hash function for mapping items towcounters, then\nthey consider the counts ofddifferent buckets array (e.g.\nminimum for CM-sketch [23], median for C-sketch [24])\nto which the data is mapped as the estimation. The CU-\nsketch [25] changed the insertion of the CM-sketch, which\nonly updates the value of the minimum bucket in each insertion\nprocess. The A-sketch [4] added a filter to the CM-sketch and\nexchanged data between the filter and the sketch to ensure\nthat hot keys are retained in the filter to reduce hash conflicts.\nElastic Sketch [27] adopts a pottery shards eviction strategy\nfor optimizing its replacement: If the inserted ID matches\nthe ID stored in the bucket, the positive vote count is in-\ncremented; otherwise, the negative vote count is incremented.\nWhen the ratio of negative to positive votes exceeds a certain\nthreshold, the stored ID is replaced with the inserted one.\nNitrosketch [28] introduces a sampling mechanism and multi-\nlevel control logic. Instead of sampling packets directly, it\nuses a sample counter array and applies geometric sampling\nto reduce per-packet processing, thereby saving on update\noperations of a sketch like C-sketch. For network-wide moni-\ntoring, UnivMon [26] builds on multi-layer sketches (e.g., C-\nsketch) with hierarchical sampling and inverse transformation\nto support accurate traffic analysis with low memory. Since\nclassic sketches have been proven to deliver high accuracy\nonly with impractical memory consumption, these algorithms\nare subject to an undesirable compromise between estimation\naccuracy and memory efficiency.\nEquation-based Sketch. Recent works have made progress\nin mitigating the trade-off by designing advanced query meth-\nods of sketch algorithms. [29] and [30] first disclose that sketch\nand compressive sensing are thematically related. The locality-\nsensitive sketch (LSS) [17] leverages the relationship between\nthe sketch with the compressed projection, then extends it to a\nK-means clustering method. The PR-sketch [18] recovers keys\nof streaming data by establishing linear equations. Specifically,\nit builds linear equations between counter values and per-key aggregations to improve accuracy, and records keys in\nthe recovery phase to reduce resource usage in the update\nphase. Finally, PR-sketch fixes the system by a conjugate gra-\ndient solver. The SeqSketch [19] and HistSketch [31] store a\nfew high-frequency items, then employ a compressed-sensing\napproach (i,e, OMP) to decode infrequent keys. By solving\nthe linear system, these equation-based sketches compensate\nfor the error introduced by counter sharing, and recovers the\ncomplete keys in the shared part with much higher accuracy\nthan the classical sketch. Unfortunately, such global sketches\nhave been shown to suffer from greatly increased time and\nmemory costs of estimation.\nLearning-based Sketch. In the last few years, machine\nlearning has taken the world by storm than ever before,\nwhich also motivates the design of learning-based frequency\nestimation algorithms [2]. [2] pioneered the idea of employing\nmachine learning to reduce the dependence of the accuracy of\nsketches on network traffic characteristics. TalentSketch [13]\napplies a long short-term memory (LSTM) model to network\nmeasurement tasks. In [10], the authors first proposed a\nlearned frequency estimation framework by using a trained\nclassifier (or oracle) to store hot and cold items separately.\nThe overall design resembles A-sketch, where cold items are\nstored in a sketch structure such as CM-sketch or C-sketch;\nhowever, the latter incorporates a data exchange mechanism.\n[11] adjusted the learning-augmented sketch, which uses a\nregression model to directly outputs the predicted frequency\nof hot keys rather than inserting them in unique buckets.\nThen a series of works have also studied theoretical analyses\nand optimizations under this framework [11]–[15]. However,\nthese hand-derived methods are excessively dependent on\noffline models and cannot handle dynamic data distribution.\nDifferently, UCL-sketch provides a new paradigm for learning-\nenhanced sketch design. It obtains accuracy close to original\nequation-based sketches while maintaining the efficient query\ncost by continuously adapting models without ground truth.\nIII. PRELIMINARIES\nA. Key Ideas of Equation-based Sketch\nWe follow prior studies that uncover the linear compression\nnature of equation-based sketch framework [17]–[19]. Before\n\nJOURNAL OF L ATEX CLASS FILES 4\ndelving into details, we define a subclass of sketches of\ninterest, termedlinear sketch: an insertion of a single key-value\npair updates only a fixed set of underlying counters. Typically,\nthe linear sketch comprises aninsertioncomponent that feeds\nthe key-value input to a compact structure that approximates\nthese key-value pairs with one or multiple hash-based counters\narrays, arecoverycomponent that inverses queried pairs from\nkey-aggregations based on the same set of hash functions.\nTheorem 1 establishes the equivalence between the sketch with\nthe linear system as follows:\nTheorem 1.The goal of frequency estimation based on a\nlinear sketch is equivalent to solving linear equations from the\ngiven keys, hash functions, and counters. Letx∈CNdenote\nthe vector of the streaming key-frequency sequence andy∈\nCMdenote sketch counters, the insertion process corresponds\ntoy=Ax, while the result of recovery phase corresponds to\nx=A†y+ (I−A†A)x,(1)\nwhereA∈CM×Nis an indicator matrix of mapping the\nvectorxto a buckets arrayy, andA†∈CN×Msatisfies\nAA†A≡A.\nRemark. The detailed proof of Theorem 1 can be found in\nAppendix, Considering Theorem 1, if we model a relation as\ndefining a vector or matrix, then the sketch of this is obtained\nby multiplying the data by a (fixed) matrix. In this regard,\na single update to the underlying volume has the effect of\nmodifying a single entry in the frequency vector. The linear\nsketch can then be updated by adding to it the result of\napplying the sensing matrix to this individual change alone.\nConsequently, the problem of frequency estimation reduces to\nsolving a linear inverse problem.\nAs shown in Fig. 2, the equation-based sketch needs to\nbuild a local linear sketch in the data plane and perform\nits original update operation. It deploys an additional key\ntracking mechanism to identify new keys and transfer them\nto the control plane. What distinguishes the design from other\nsketches is that it leverages an equation-based approach to\ncompensate for per-key error caused by counter sharing in the\nrecovery phase (or control plane), which can be concluded as\nthree steps:(i)Transform sketch counters to the measurement\nvectory.(ii)For all distinct stream items, construct a sketch\noperatorAbased on the hash functions that map them in the\nsketch.(iii)Fix the system of linear equations by an equation\nsolver. We now proceed to formally define the problem setting\nin the next subsection.\nB. Problem Statement\nSimply speaking, we formulate our frequency estimation\nalgorithms via compressive sensing (CS), like the equation-\nbased sketch introduced in Section III-A. Let a data stream\nof running lengthnbe a sequence ofntuples. Thet-th\ntuple is denoted as(k t, vt), wherek tis a data-item key used\nfor hashing andv tis a frequency value associated with the\nitem. For each item, the insertion process applies an update\nto sketch counters (vector)yof lengthM, with a sensing\nmatrixAdefined by hash functions and the key value. Then\ngiven the collectedyandA, the output of recovery phase isa list of estimated frequencies, i.e., ground-truth (GT) vector\nx. Also note that we assume the size of possible keys space\nN > Min this work, because keys are usually drawn from\na large domain (e.g, IP addresses, URLs) while available\nspace in data plane is limited, leading to an ill-posed system.\nFormally, the recovery phase builds an optimization problem:\nmax\nxlogp(x|y),s.t. y=Ax.\nC. Motivations\nLimitation of Baseline Solution.There have been many\nimplementations and extensions of equation-based sketch.\nAmong these sketching solutions, the PR-sketch [18] and\nSeqSketch [19] represent the most recent examples. These\nmethods involve key tracking mechanisms to collect distinct\nkeys in the stream and apply optimization techniques to\nsolve the linear system. However, the computational cost of\nstreaming problems grows significantly with the number of\nkeys, making iterative process of “decode” algorithms less\nfeasible for modern streams. As a result, they have remarkably\nincreased per-key accuracy as well as computation time and\npeak memory consumption at query time.\nImpact of a Learned Equation Solver.One idea here\nto eliminate the need for iterative optimization is a learned\nequation solver that directly maps measurementyto frequen-\nciesx. While such a deep solver incurs additional training\noverhead, this cost can be amortized in parallel with ongoing\nstream processing. In return, it significantly reduces query\nprocessing latency by enabling one-shot prediction, while pre-\nserving the strong estimation accuracy of traditional (baseline)\nsolvers—mirroring the success of learned approaches in the\nfield of compressive sensing [32]–[34].\nChallenges of Learning-based Solution.Building on the\nabove discussion, our primary objective is to develop equation-\nbased sketches that leverage learned solvers to automate per-\nkey frequency recovery. However, this goal introduces two\nkey challenges: (1) enabling online training without access to\nground-truth frequencies, as acquiring real-time exact counts\nfor all possible keys is impractical; and (2) maintaining\nalignment with large-scale data streams under strict constraints\non parameter overhead, especially as the solver’s complexity\ngrows over time. To address these issues, we present in the\nfollowing section a detailed description of our proposed self-\nsupervised learning framework for per-key frequency recovery.\nIV. METHODOLOGY\nIn this section, we present our method, called UCL-sketch,\nand elaborate on its design. At a high level, UCL-sketch in-\nherits the architectural framework of equation-based sketches\nbut replaces traditional iterative decoding algorithms with a\nlearned solver. To overcome the challenge of GT-free online\ntraining, we introduce the concept of equivalent learning and\ndevelop a solver trained using windowed sampled counters.\nMoreover, to support large-scale data streams with dynami-\ncally expanding key spaces, we incorporate position encoding\nwith parameter sharing, enabling logically unbounded scala-\nbility of the key space.\n\nJOURNAL OF L ATEX CLASS FILES 5\nkey new old\nx y…\n…\nxx yy\n…\nCM-sketchHF BF…\n…\n…………\n……0\n0\n1\n0\nw\nd\ns\n1k\nks\nbm\nData \nPlaneConrtol PlaneExchanged Keys\nOther Keys\nLearned \nSolver\n1,1c\n1,2c\n1,3c\n1,4c\n1,cw\n,1cd\n,2cd\n,3cd\n,4cd\n,cdw\n(a) Data structure of UCL-sketch\nkey new old key new oldkey new old key new old\nkey new old key new old\n2k\n3k8 2\n10 18 2\n10 1\n2 0\n2k\n3k…\n…\n…\n……\n…\n2k8 2\n10 1\n4 16 0\n10 1\n4 1\n3k\n1k\n4k\n3k\n1k\n+8\n+8\n6 0\n10 1\n4 1hash\n4k\n3k\n1k6 0\n10\n1 42\n+1\n+1\n4k\n3k\n1kBefore After\nCM-sketchHFBF\nHFBF\nHFBF\nHFBF\nHFBF\nHFBFSend\n2k\nSend\n5k\n5k BFExchangeCase 1.1\nCase 2\nCase 3Before After\nBefore After\n()1k , 2\n()4k ,6\n()5k ,1\nkey new old key new old\n2k\n3k\n1k\n8 2\n10 1\n2 18 2\n10 1\n4 1\n2k\n3k\n1k…\n…Before After\nHFBF\nHFBFCase 1.2\n()1k , 2\n1kCM-sketch\nCM-sketch CM-sketch CM-sketch CM-sketchCM-sketch CM-sketch (b) Examples of update processing by UCL-sketch\nFig. 3.The basic design of UCL-sketch:(a) UCL-sketch maintains a hash table, a CM-sketch, and a Bloom filter in the data plane, while deploying two\nkey sets in the control plane (storing keys evicted from the hash table and other keys, respectively), along with a learning-based solver. (b) During stream\nprocessing in the data plane, each incoming item is first checked against the hash table; if the insertion fails, the item is inserted into the CM-sketch instead,\nand its key is reported to the control plane if it was either recently evicted from the hash table or not found in the Bloom filter.\nA. Basic Design\n1) Key Ideas:To mitigate the extra bandwidth overhead\nused for transmitting keys, the UCL-sketch employs a Bloom\nFilter during the update phase, ensuring that each unique key\nis identified and reported at most once. Moreover, we filter\nhot keys twice, storing them separately in a hash table and\nan array, which has been proven beneficial for highly skewed\ndata streams [4], [19].\n2) Data Structure:Fig. 3(a) depicts the data structure of\nUCL-sketch. In the data plane, it has three types of data\nstructures: (1) a heavy filter (hash table)HFto track frequent\nkey pairs, (2) a sketch to record the remaining items, and\n(3) a Bloom FilterBFfor key identification. Each slot in the\nHF consists of three fields. In addition to a key identifier,\nthe slot contains two counters:new count, which tracks the\nvalues associated with the key, andold count, which records\nthe values not attributed to the key. As discussed in Section III,\nthe sketch used here can be any linear sketch. In this work,\nparticularly, we adopt the CM-sketch due to its simple and fast\ninsertion (see Appendix for other implementations). For the\ncontrol plane, apart from a learned solver, we maintain two\nnon-repeating and non-overlapping arrays to record inserted\nkeys and exchanged keys from HF in our sketch, respectively.\n3) Update Operation:The procedure of inserting an item\nin our UCL-sketch is very similar to previous equation-\nbased sketch, e.g. SeqSketch [19]. The main difference is\nthe exchanged keys that are supposed to be relatively hot are\nstored separately in preparation for subsequent training phase.\nAlgorithm 1 outlines the procedure of inserting a key-value\npair (k, v). We first compute its hash position in HF, then as\nFig. 3(b) shows, there are overall three cases:\nCase 1: The slot is empty or the existing entry has the same\nkey. We insert the item into the position or just increment the\nnew count by its value.\nCase 2: The current position does not have the same key\nand(new count - old count)≤0 after incrementing the old\ncount by the item’s value. We replace the existing entry with\nthe new item and evict the old entry into the sketch. Then we\ntransfer the exchanged key with the “hot” flag to the control\nplane, and update the BF.Algorithm 1Update Operation on the UCL-sketch\nRequire:key-value pair(k, v)\n1:i←hash(k);\n2:ifHF[i].key == Nullthen\n3:HF[i].key←k; HF[i].new←v; HF[i].old←0;\n4:else ifHF[i].key ==kthen\n5:HF[i].new←HF[i].new +v;\n6:else\n7:HF[i].old←HF[i].old +v;\n8:ifHF[i].new<HF[i].oldthen\n9:insert CM-sketch with (HF[i].key, HF[i].new);\n10:insert BF with HF[i].key;\n11:report hot key HF[i].key to control plane;\n12:HF[i].key←k; HF[i].new←v; HF[i].old←0;\n13:else\n14:insert CM-sketch with(k, v);\n15:ifk /∈BFthen\n16:insert BF withk;\n17:report cold keykto the control plane;\n18:end if\n19:end if\n20:end if\nCase 3: The current position does not have the same key\nand(new count - old count)>0 after incrementing the old\ncount by the item’s value. We insert the item into the sketch.\nAfter inserting the BF, only if it is identified as a new key,\nUCL-sketch sends the key to the control plane.\nGiven a heavy-tailed distribution of stream data, Case 1 con-\nstitutes a large portion while Case 2 represents the opposite.\nThus after filtering by the hash table, UCL-sketch is memory\nefficient as the number of exchanges is usually very small [4].\nB. Training Strategy\n1) Goal and intuitions:As mentioned in Section III-C, we\nconsider a challenging but reasonable setting in which only\nsampled measurement vectory, and the sketch sensing matrix\nAare available for on-line training our solverD:y=Ax→\nx. As shown in Eq. 1, the root problem is a non-trivial null\n\nJOURNAL OF L ATEX CLASS FILES 6\nServer Sample Point\nSketch Update Point\nFrequencyFrequency\n…\nKey IDKey IDFrequencyFrequency\n…\nKey IDKey IDLinear Sketching\nLinear Sketching\nFrequencyFrequency\n…\nKey IDKey IDFrequencyFrequency\n…\nKey IDKey ID\ny\nx\n1x\nAx\nx\nˆx\nD\nD\npT\nAx\nx\nx\nD\nx\ny\nAx\nˆx\nTimeTimeTest OccursUnbounded Frequency Set\n(a)\n(c)(d)\nPositive  \nTransformInverse  \nTransform\n(b)Past Data Training Data Future Data\nSliding Training Set\nc\npT\npT\nFig. 4.Main ideas of training strategy:(a) The unbounded set of frequency vectors is tolerant of certain Zipfian transformations. (b) Continually adapting the\nmodel using sampled counters in a sliding window for online training. (c) The learned solvery→xshould also be invariant to these natural transformations.\n(d) Illustration of our self-supervised equivalent loss design.\n……… …\n…\n……\n…… +\n1y\nmy\n1y\nmy\n1x\nlx\nkx\n1kx+… …\n…\n……\n1y\nmy\n1y\nmy\n… … …+ …\nReplace…\n1x\nlx\nBucket 1 Bucket i\nIx\n1kx+… …\n…\n……\n1y\nmy\n1y\nmy\n…\n1x\nlx\nBucket 1………\nBucket i\nIx\n1kx+…i\nUse Buckets Share Bucketst-1 t t-1 t t-1 t\n…\nFig. 5.Expansion design of a learned solver: Left:Redesigning. Whenever a new key emerges, the entire output layer learned on previous streams is\nreplaced and retrained.Center:Partial-redesigning with buckets. The keys are separated into independent buckets, so the solver is affected only by newly\nupdated bucket.Right:Non-redesigning with logical buckets. By sharing buckets, the solver can be adapted to varying key space, and greatly reduces the\nnumber of parameters.\nspace defined by(I−A†A)xwhileyonly provides the\ninformation of range space ofA, i.e., its pseudo-inverseA†.\nTherefore, a simple solver without additional constraints is not\nefficient enough to resolve the GT ambiguity. Our intuition for\nachieving unbiasedness is the distribution of item frequencies\nfollow approximate Zipf’s law, then the output of the solver\nshould be invariant to certain groups of transformations that\nallow us to learn beyond the range space.\n2) Online Training:First of all, we present an online\ntraining procedure for continually adapting the learned solver.\nWhen analyzing or processing continuous streaming data, one\nonly needs to keep track of recent stream because queries\nalways occur in the future, while the useful information\ncontained in past streaming data is diminishing over time.\nTherefore, as shown in Fig. 4 (b), our approach for dealing\nwith non-stationary data streams is to adopt a sliding window\n(SW) mechanism which retains a fixed number of sampled\n“snapshots” of sketch counters in memory, instead of training\non the entire history. The concept revolves around maintaining\na “window” that slides with time, capturing only the recent\nstate of the unbounded stream. The sample point depends on\nthe times of sketch updates, for instance, these counters are\ntransmitted to the control plane after every 1,000 insertions.\n3) Equivariant Learning:A straightforward practice of un-\nsupervised frequency recovery is to impose the measurement\nconsistency on the model, using a range space loss of formlike∥Ax−y∥2\n2. However, the sketch operatorA1has a null\nspace, which means the model converges freely to the biased\nsolutionA†y+H(y)withAH(y) = 0. This will cause\nunstable reconstructions without meeting ground truth data or\nprior information. According to CS theory, one direct way\nto alleviate the problem is “ℓ 1-minimization” by adding a\nregularization∥x∥1penalizes against the lack of sparsity [19].\nFortunately, it makes sense in streaming algorithms because\nheavy-tailed data such as network traffic exhibits high sparsity,\nbut it is not enough to eliminate the impact of null space, since\nthe accuracy of particular items is still unsatisfactory [35].\n0 200 400 600 800 1000\n/uni00000035/uni00000044/uni00000051/uni0000004e/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000056/uni0000004e/uni00000048/uni0000005a/uni00000051/uni00000048/uni00000056/uni00000056/uni00000003/uni00000020/uni00000003/uni00000014/uni00000011/uni00000013 \n/uni00000056/uni0000004e/uni00000048/uni0000005a/uni00000051/uni00000048/uni00000056/uni00000056/uni00000003/uni00000020/uni00000003/uni00000014/uni00000011/uni00000018  \n/uni00000056/uni0000004e/uni00000048/uni0000005a/uni00000051/uni00000048/uni00000056/uni00000056/uni00000003/uni00000020/uni00000003/uni00000015/uni00000011/uni00000013\nFig. 6. Example of the Zipfian distribution with\ndifferent skewness.In order to learn\nmore knowledge be-\nyond the range space\nofA, we draw on\nideas from the Zipf\nLaw prior of stream-\ning item frequencies,\nand this is a common\nand natural reoccur-\nring pattern in real-\nworld data [15]. At their simplest, zipfian models are based\non the assumption of a simple proportionality relationship:\n1Note that we do not need to explicitly maintain the 0-1 sparse sensing\nmatrixAin memory, although the control plane has sufficient space. Instead,\nwe generate its elements on demand.\n\nJOURNAL OF L ATEX CLASS FILES 7\nf(kj)∝1\nj, wheref(k j)is the frequency ofj-th keyk jin\nkeys set sorted by volume as shown in Fig. 6. Furthermore, we\nintroduce an additional assumption of “temporal smoothness”\nin the heavy-tailed distribution—a well-established principle\nin real-world measurement studies [36]. This assumption cap-\ntures the empirical observation that frequencies withinadja-\ncenttemporal sample points tend to express similar zipfian be-\nhavior: a few frequent items exhibit approximate proportional\ngrowth, while other infrequent items remain their original size.\nUnder this mild prior information, we define a group of\ntransformationsP=\b\np1,...,|P|\t\n, in which arbitraryp ican be\nsummarized in the following steps: Given a positive integer\ncwhich usually takes value around the sampling interval,p i\nallocates it proportionally based on the volume of exchanged\n(or hot) keys in the input frequency vector. Next, the transfor-\nmation randomly selects a small number of frequencies, i.e.\n5%from the remaining non-hot keys, and increments them by\nminimum update unit. As Fig. 4 (a) shows, for all possiblex\nin the unbounded frequency setX, the equivalent relationship\nTpx∈ X,∀p∈ Pholds, whereT p∈RN×Nis the corre-\nsponding transformation matrix ofp. Then, our learned solver\nDshould also capture such invariant proportional property\nof the target domain, that is,D(AT px) =T pD(Ax). This\nadditional constraint on the mapping allows the model to learn\nbeyond the range space (see details in Theorem 3). If the\nincremental component can be handled by the solver, then the\nambiguity in null space recovery can be effectively mitigated\nduring learning. As shown in Fig. 4 (c) and (d), the network\nweights are updated by minimizing the following objective:\narg min\nθEy∈AX,p∈P {∥AD(y)−y∥2\n2+λ∥D(y)∥1\n+∥D(AT pD(y))−T pD(y)∥2\n2},(2)\nwhere the first term enforces measurement consistency, the\nsecond term imposes sparse constraint, and the third term\nenforces system equivariance, andλis a trade-off coefficient.\nThe training pseudo-code is exhibited in Algorithm 2.\nC. Scalable Architecture\n1) Goal and intuitions:UCL-sketch is explicitly designed\nto allow for sketching large-scale data steam, where an un-\nknown number of new items arrive at the monitor device\nin sequence, rather than designed for a fixed or small key\nspace. Specifically, our goal here is to let the learned solver\ndynamically expand its capacity once new elements arrive,\nwhile achieving efficiency in parameters. The intuition for\ndesigning such a lifelong network is to incrementally adapt\nto new items while retaining acquired frequencies of previous\nitems, so parameter sharing is a good and natural choice.\n2) Network Expansion:A most naive way to design the\nnetwork for a sequence of items would be retraining the output\nlayer(s) every time a new item emerges. However, such retrain-\ning would incur significant costs for a deep neural network.\nInstead, we suggest dividing keys set into small buckets, where\neach bucket is maintained with its own parameters, thereby\nreducing the extra expanding overhead. Given the information\ncollected about the observed stream, this design transforms\nthe problem into a maximum-a-posteriori (MAP) estimate ofthe bucket-associated frequencies [22]. Unfortunately, it is still\nvery intensive in terms of memory usage since the network’s\nsize scales with the observed keys, under high-speed streams\nwhere the computational cost is a significant concern.\nNotice that the statistical properties of sketch-based estima-\ntion should be stationary over buckets, as it implies that the\nsimilar posteriori transformation can be applied at each key\n(or bucket) inserted in the sketch (see details in Theorem 4).\nThus, a better solution in such a case is sharing parameters\nacross these buckets. Specifically, we train a solver to model\nthe mapping(y, i)→x i, in whichidenotes the index of our\nselected bucket andx i⊂xis predicted frequencies in the\nlogical bucket. Borrowing ideas from literature in diffusion\nmodels [37], we learn the bucket-shared network using the\nsinusoidal position embedding [38]. Our model details can be\nfound in Appendix. After this bucket sharing and logification,\nthe network needs to update its weights by repeatedly forward\npropagation since the split changes the overall structure. For-\ntunately, in practice, this operation can be performed for all\nlogical buckets in parallel. Fig. 5 illustrates our dynamically\nscalable network architecture and shows what happens when\nthe set is partitioned into independent buckets.\n3) Query Operation:When querying an itemk, we initially\nlocate its index in the keys set such that we can determine the\nbucket id and relative position ofk. Then we query the hash\ntable in the data plane to obtain its filtered frequency, and\nreturn 0 if the key is not in it. The partial result and sampled\nsketch counters will be reported to the server together. With\nthe previously acquired position, we predict the remaining\nfrequency ofkby inputting counters and bucket id into the\nlearned solver. The final estimated frequency is a sum of the\ntwo parts. The process is depicted in Algorithm 3.\n4) Coping with Sketch Changes:In practice, sketch param-\neters are often updated, such as resizing memory or changing\nhash functions, which can render previously acquired knowl-\nedge invalid. We focus on the most challenging case (others\nare in the Appendix): switching the hash function midway\nthrough recording. A conventional sketch (e.g., CM-sketch)\nwould lose earlier traces and quickly fail, whereas UCL-sketch\nadapts by re-recording post-change keys while retaining and\nmarking the originals, effectively appending columns to the\nsketching matrix, as illustrated in Fig. 8. Online learning then\nenables rapid recovery despite the loss of prior knowledge.\nD. Putting It Together\nWe now put the basic design and our optimizations together,\nto build the final version of UCL-sketch. Fig. 7 gives overview\nof the complete version. Fig. 7 (a) shows the procedure\nof insertion process in data plane. Then in Fig. 7 (b) and\n(c), we construct the corresponding sketch sensing matrixA\nand present scalable inference of our bucket-wise network,\nrespectively. To enable all buckets to share a single model,\nthe model includes a bucket idi-indicator to guide the solver\nin selecting the corresponding logical bucket for estimation.\nAs for training, since no real frequency is sent to the control\nplane, as illustrated in Fig. 7 (d), our goal (or loss function)\nis to reconstruct per-key frequenciesxconforms to three\n\nJOURNAL OF L ATEX CLASS FILES 8\nAlgorithm 2One-Epoch Training Algorithm of UCL-sketch\nRequire:η,λ, sketching matrixA, exchanged key idsh,\nmeasurement setY, and number of keysn\nEnsure:trained modelD\n1:formeasurement vectoryinYdo\n2:x←Per Key Recovery(D, y, n);\n3:x′←Positive Transform(x, h, n);\n4:ˆx′←Per Key Recovery(D,Ax′, n);\n5:Gradient descent onη∇ θD(∥Ax−y∥2\n2+∥ˆx′−x′∥2\n2+λ∥x∥1);\n6:end for\n7:returnD;Algorithm 3Query Operation on the UCL-sketch\nRequire:learned modelD, keys setΩ, bucket lengthL,\ncurrent measurement vectory, and keyk\nEnsure:estimated frequencyx k\n1:position←Get Key Position(Ω, k);\n2:bucket id←position //L;\n3:inner id←position - bucket id×L;\n4:x←D(y, bucket id);\n5:xs\nk←x[inner id];\n6:xk←xs\nk+Heavy Filter Query(k);\n7:returnx k;\nkey new old\nx y…\n…\nxx yy\n…\nsketchHFBF…\n…\n…………\n……0\n0\n1\n0\n1k\nks(a) Insertion Process\n1,1c\n1,2c\n1,3c\n1,4c\n1,cw\n,1cd\n,2cd\n,3cd\n,4cd\n,cdw\n()11k , v\n()11k , v\n()22k , v\n()13k , v\n() ijk , v…filtered evicted\n1\n0\nexchanged keys other keys(b) Sensing Process (c) Scalable Solver\n(d) Online GT -free Training(e) Query Process\n1\n0\n0\n1\n0\n00\n1\n0\n1\n0\n00\n1\n0\n0\n0\n10\n0\n1\n0\n1\n00\n1\n0\n0\n0\n11\n0\n0\n0\n1\n0…\n…\n…\n…\n…\n…\nAhashingExchanged \nKeys\nOther Keys\n()yD\n1x\n2x\n3x\n4x\n5x\nnx…\n1key\n2key\n3key\n4key\n5key\nkeyn…i… ybucket\nid\ncounters\nvectorx\ny\nSliding datasetD\n,i1,…x+=c\nx\nxA\nˆx solver\nD\n,i1,…solverTransformation\nObjective:\n22\n2 1 2ˆ arg min ' '\nDx y x x x\n− + + −A\nExchanged \nKeys\nOther Keys\nkeyikey new old\nx y…\n…\nxx yy…\n1k\nks\nhf sketchfrequency f fi i i=+\nsketchfi\ny Dbucket\nidinner\nid\nhffi\nFig. 7. The overview of complete version of our UCL-sketch.\nConcat\nConcatSketching Matrix\n(before change)Sketching Matrix\n(after change)y\n(current) y\n(before) y\n(after) x\n(before) x\n(after) \nReset the Bloom Filter\nFig. 8. Scalable UCL-sketch is robust to changes in its underlying sketch.\nconstraints: measurement, sparsity and invariant proportional\nproperty of Zipfian distribution. To query the frequency of\nkeys, we sum up the results from the learned solver and the\nhash table as shown in Fig. 7 (e).\nV. THEORETICALANALYSIS\nIn this section, we analyze the proposed UCL-sketch, in-\ncluding complexity, keys coverage, error bound, and require-\nment for unbiased estimation during training. Due to space\nconstraints, we only list the conclusions and remarks here.\nDetailed proofs can be found in Appendix. For convenience,\nwe list the notations frequently used in this part in Table I.\nDefinition 1(s-restricted Isometry Constant [39]). For\nevery integers= 1,2, . . ., we define the s-restricted isometry\nconstantsσ sof a matrixAas the smallest quantity such that\n(1−σ s)∥x∥2\n2≤ ∥Ax∥2\n2≤(1 +σ s)∥x∥2\n2(3)\nfor alls-sparse vectors, where a vector is said to bes-sparse\nif it has at mostsnonzero entries.\nLemma 1 shows the complexities of memory space, and\nupdate time of UCL-sketch.TABLE I\nMAJOR NOTATIONS USED IN OUR ANALYSIS\nNotation Meaning\ne base of the natural logarithm\nF total frequencies stored in the entail system\nf filtered frequencies stored in the sketch part\nN total number of distinct items\nK total number of current distinct items\ns number of slots in HF\nb number of flag bits for each pair in HF\nmb number of bits in BF\n(εb, δb) coefficient and error probability in BF\n(εc, δc) coefficient and error probability in sketch\nw ⌈e/εc⌉, sketch array width\nd ⌈ln (1/δ c)⌉, sketch array depth\nkb log (1/ε bδb), number of hash functions in BF\nA sketching matrix of a linear sketch, i.e., CM-sketch\nx ground-truth frequency data vector\ny sketch counters measurement vector\nσs s-restricted isometry constant ofA\nxT vector satisfying{x(i) :i∈T; 0 :i /∈T}\n(·)ccomplement of a set(·)\nTp(·)matrix form of a transformation operationp (·)\nLemma 1.The space complexity of UCL-sketch is\nO\u0010\nmb+e\nεcln1\nδc+bs\u0011\n, and the time complexity of update\noperation isO\u0010\nlog1\nεbδb+ ln1\nδc\u0011\nin the data plane.\nLemma 2 guarantees the error bound for missing keys2in\nthe recovery phase of UCL-sketch.\nLemma 2.The keys coverage of the Bloom Filter in UCL-\n2For these missing keys, we report zeros as their volumes, as truncating low-\nfrequency estimates to 0 will not impact the system’s overall performance [15].\n\nJOURNAL OF L ATEX CLASS FILES 9\nsketch obeys\nPr (Y≥K y)≤K\nKy\u0012\n1−e−kbK\nmb\u0013\n,(4)\nwhere variableYdenotes the number of keys that are not\ncovered but viewed as covered.\nWe show UCL-sketch’s worst-case error bound of per-key\nrecovery (without equivalent loss) from sketch counters as\nshown in Theorem 2.\nTheorem 2.Letf= (f(1), f(2), . . . , f(n))be the real\nvolume vector of a stream that is stored in the sketch, where\nf(i)denotes the volume ofi-th distinct item. ConsiderT 0\nas the locations of theslargest volume off. Assume that\nthe reported volume vectorf∗is the optimal solution that\nminimizes the first two objective in Eqn. 2. Then the worst-\ncase frequency estimation error is bounded by\n∥f∗−f∥1≤2 +\u0000\n2√\n2 + 2\u0001\nσ2s\n1−\u0000√\n2 + 1\u0001\nσ2s\r\rfTc\n0\r\r\n1(5)\nRemark. We list theoretical comparison between UCL-\nsketch and six methods in Table II. Although some classic\nsketches have slightly lighter structure than ours, they obtain\nmore inaccurate error bound. Specifically, it is worthy noting\nthat1\nK\r\rfTc\n0\r\r\n1≪ε c∥f∥1in most scenarios due to heavy-\ntailed (sparse) property of real-world streams. It has also\nbeen proven in [27] that the bound of Elastic Sketch is\nlower than that of C-sketch and CM-sketch, e.g.,ε c∥f∥1≤\nεc∥F∥1≤εc∥F∥2. Therefore, our equation-based algorithm\nsignificantly outperforms these three competitors regarding es-\ntimation accuracy. Meanwhile, Univmon requireslogNtimes\nthe space in the update phase. Note that the effect of equivalent\nlearning was not considered in our analysis. Consequently, the\npractical performance achieved by our algorithms may surpass\nthe theoretical result, which has been substantiated by the\nempirical results presented in Section VI.\nTABLE II\nTHEORETICAL COMPARISON WITH STATE-OF-THE-ART SKETCHES\nAlgorithm Space Complexity Time Complexity Average Error Prob.\nCM-sketch O\u0010\ne\nεcln1\nδc\u0011\nO\u0010\nln1\nδc\u0011\nO\u0000\nεc∥F∥1\u0001\n1−δ c\nC-sketch O\u0010\ne\nε2cln1\nδc\u0011\nO\u0010\nln1\nδc\u0011\nO\u0000\nεc∥F∥2\u0001\n1−δ c\nElastic Sketch O\u0010\n−slogK\nln(1−ε bδb)+e\nε2cln1\nδc\u0011\nO\u0010\nln1\nδc\u0011\nO\u0000\nεc∥f∥1\u0001\n1−δ c\nUnivmon O\u0010\nlogN(e\nεcln1\nδc+slogN)\u0011\nO\u0010\nlogNlns\nδc\u0011\n/ /\nUCL-sketch O\u0010\nmb+e\nεcln1\nδc+bs\u0011\nO\u0010\nlog1\nεbδb+ ln1\nδc\u0011\nO\u0010\n1\nK\r\r\rfTc\n0\r\r\r\n1\u0011\n1\nIn the table, complexity analyses of all baselines are from [18].\nTheorem 3 gives the necessary condition of UCL-sketch\nto achieve full accuracy in the training set with our GT-free\nequivalent learning strategy.\nTheorem 3.A necessary condition for recovering the true\nvolume from compressed counters is that the following linear\nsystem has a unique solution:\nBx=\nA\nATp1\n...\nATp|P|\nx=\ny\ny1\n...\ny|P|\n,(6)wherey(·)is the measurement corresponding to the transfor-\nmationp (·). Rigorously, rank(B) =n.\nRemark. Due to the set invariance of zipfian streaming\ndata, Theorem 3 states the requirement to learn a deep solver\nwithout ground truth, is that numerous virtual sketch operators\n{AT p(i)}i=1,2,...,|P| have enough different range space to de-\ntermine unique per-key frequenciesx. That means the choice\nof transformation groupPin above system is not arbitrary,\nbut needs to be ranknor at least> mso that the model\nis guaranteed to learn from the null space ofA. Critically\nthough, much room beyondA†will be filled after a large\nnumber of random transformations during training. Thus, it is\npossible to train the solver from only sketch counters using our\nscheme, but also note that the groupPmight in some cases\nnot be sufficient since the targetxis rapidly and continuously\nextending in complex streams.\nIn Theorem 4, we give a unified MAP estimate result of\nall keys given the information data structure, which helps to\nexplain why the statistical property of sketch are similar over\ndifferent keys and buckets, creating the probability of sharing\nbucket-wise estimations in our deep solver.\nTheorem 4.Assume that the prior of eachk i-associated\nfrequency in per-key GT vectorxsatisfiesPr (x(i) =a) =\nN\u0000\na;µi, σ2\ni\u0001\n, then for∀k iand∥x∥1→ ∞, through asymp-\ntotic approximation, its posterior estimate based on sketch\ncountersyhas the following general form:\nˆx(i) =µi∥x∥2\n2+σ2\ni\u0010\nwPd\nj=1yj×H j(ki)−d∥x∥1\u0011\n∥x∥2\n2+σ2\nid(w−1)(7)\nwhereH jis the indep. hash function:Ω→ {1, . . . , w}.\nRemark. Theorem 4 can be easily extended to the estimation\nof combinations involving multiple keys (i.e., bucket). From\nEqn. 7, we see that once the system input (i.e.,yandA)\nis provided, all parameters are shared and fixed, except for\n(µi, σi)which is unknown for particular keys. Alternatively,\napproaches using neural network are able to consist of jointly\nlearning different estimation buckets with ai-indicator. While\nthe UCL-sketch can thus be viewed as a number of approx-\nimations, our empirical evaluation in Section VI shows that\nthe UCL-sketch yields highly accurate estimates, confirming\nthe validity of the shared logical buckets.\nVI. EXPERIMENTS\nWe next report key results compared to state-of-the-art\nmethods with both real-world and synthetic steam datasets.\nA. Experimental Setup\nDatasets.We conduct experiments on three real-world\ndatasets. The first isCAIDA[40], real traffic data collected\non a backbone link between Chicago and Seattle in 2018.\nWe form 13-byte keys with five fields: source and destination\nIP addresses, source and destination ports, and protocol. In\nour experiments, we use 1 million packets of it with around\n100K distinct keys. The second isKosarak[41], consists\nof anonymized click-stream data from a Hungarian online\nnews portal. We also extract a segment of data with a length\n\nJOURNAL OF L ATEX CLASS FILES 10\nof 1 million for our experiments, where about 25K unique\nkeys are in it. The third isRetail[42], which contains retail\nmarket basket data supplied by an anonymous Belgian retail\nsupermarket store. There are nearly 910K items in this stream,\nand we utilize the entire dataset comprising a total of 16K\nunique keys. Each key is 4-byte long in the above two datasets.\nTo evaluate the robustness of the proposed algorithm, we also\nsynthesize four datasets that satisfyZipf’s law, where the\nskewness varies in [1.2, 1.3, 1.4, 1.5] and keys with length\nof 4-byte distinguish items in these datasets. There are 2M\nelements in each dataset, with around 22K∼214K total\ndistinct items depending on the skewness.\nMetrics.For comparing the accuracy of frequency estima-\ntions, we leverageAverage Absolute Error (AAE)andAverage\nRelative Error (ARE). Additionally, we useWeighted Mean\nRelative Difference (WMRD)andEntropy Relative Errorto\nevaluate the accuracy of the per-key distribution. Formally,\nthe detailed descriptions are given below:\n(1)AAE: It equals1\nnPn\ni=1|f(i)−f∗(i)|, wheref(·)and\nf∗(·)are real and estimated frequency respectively.\n(2)ARE: It equals1\nnPn\ni=1|f(i)−f∗(i)|\nf(i), wheref(·)and\nf∗(·)are the same as those defined above.\n(3)WMRD[18]: It can be written asPz\ni=1|n(i)−n∗(i)|Pz\ni=1n(i)+n ∗(i)\n2,\nwherezis the maximum single-key frequency, andn(i)and\nn∗(i)are the real and estimated number of keys with frequency\nirespectively.\n(4)Entropy Absolute Error: We calculate\nthe entropyebased on a frequency set as\n−Pz\ni=1\u0010\ni×n(i)Pz\ni=1n(i)log2n(i)Pz\ni=1n(i)\u0011\n, then the absolute\nerror is|e−e∗|whereeande∗are the true and estimated\nentropy. Here we define0 log(0) = 0.\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0100200300400/uni00000024/uni00000024/uni00000028\n/uni00000026/uni00000030 /uni00000026/uni00000036 /uni0000002f/uni00000026/uni00000030 /uni0000002f/uni00000026/uni00000036 /uni00000028/uni00000036 /uni00000038/uni00000030 /uni00000031/uni00000036 /uni00000032/uni00000058/uni00000055/uni00000056\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c050100150200250/uni00000024/uni00000035/uni00000028\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0.51.01.52.0/uni0000003a/uni00000030/uni00000035/uni00000027\n0204060\n010203040\n(a) CAIDA Dataset\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0200400600/uni00000024/uni00000024/uni00000028\n/uni00000026/uni00000030 /uni00000026/uni00000036 /uni0000002f/uni00000026/uni00000030 /uni0000002f/uni00000026/uni00000036 /uni00000028/uni00000036 /uni00000038/uni00000030 /uni00000031/uni00000036 /uni00000032/uni00000058/uni00000055/uni00000056\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c050100150200250300/uni00000024/uni00000035/uni00000028\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0.500.751.001.251.501.752.00/uni0000003a/uni00000030/uni00000035/uni00000027\n0204060\n0102030\n(b) Kosarak Dataset\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0200400600800/uni00000024/uni00000024/uni00000028\n/uni00000026/uni00000030 /uni00000026/uni00000036 /uni0000002f/uni00000026/uni00000030 /uni0000002f/uni00000026/uni00000036 /uni00000028/uni00000036 /uni00000038/uni00000030 /uni00000031/uni00000036 /uni00000032/uni00000058/uni00000055/uni00000056\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c050100150200/uni00000024/uni00000035/uni00000028\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0.51.01.52.0/uni0000003a/uni00000030/uni00000035/uni00000027\n020406080\n05101520\n(c) Retail Dataset\nFig. 9. Performance comparison between our UCL-sketch and existing state-\nof-the-art sketches.Algorithm Comparisons.For comparison, we implement\nten existing frequency estimation algorithms3, such asCM-\nsketch (CM)[23],C-sketch (CS)[24],Elastic Sketch (ES)[27],\nUnivMon (UM)[26],Nitrosketch (NS)[28] and two ideally\nlearned sketches [10]:Learned CM-sketch (LCM)andLearned\nC-sketch (LCS). In addition, we compare our UCL-sketch with\nequation-based sketches, i.e.,FlowRadar[43],PR-sketch[18]\nandSeqSketch[19], in our ablational experiments (detailed in\nAppendix). We give them the same local memory of buckets\n&sketch as ours. To be more specific, the number of hash\nfunctions has been fixed at 4 across all methods. We fix\nlevels as 2 in the universal sketch of UnivMon. For Elastic\nSketch, we allocate 25%∼50%memory for its hash table. In\nparticular, the neural network oracle in two learned sketches\nis replaced with an ideal oracle that knows the identities\nof the heavy hitters, whose target domain depends on the\nnumber of its unique buckets which take up around 50%\nmemory. Also note that the memory of the learned oracle is\nnot computed in the memory cost reported in this section.\nFinally, the detailed description of our parameter settings can\nbe found in Appendix.\nTABLE III\nENTROPY ABSOLUTE ERROR ON DIFFERENT STREAMING DATA SETS\n(BOLDINDICATES BEST PERFORMANCE)\nDatasets Memory Ours CM CS LCM LCS ES UM NSCAIDA32KB 9.68 1550.48 1252.46 2475.28 1042.73 3969.5 1196.32 510.67\n64KB 7.54 543.85 550.84 806.84 380.76 1459.49 571.56 212.99\n96KB 4.51 284.80 385.78 394.74 341.08 774.28 429.74 131.79\n128KB 3.39 176.97 236.29 227.62 124.86 490.86 557.75 105.86\n160KB 3.12 122.43 185.47 168.27 71.54 455.09 291.04 100.44\n192KB 2.82 89.00 147.47 142.04 55.93 237.98 246.48 82.09\n224KB 2.90 67.45 120.76 121.82 45.29 146.81 194.93 68.26\n256KB 1.42 52.95 101.58 69.13 29.54 100.88 169.14 56.25Kosarak16KB 58.24 2620.64 3270.70 4506.12 3469.50 6537.66 1875.06 3575.26\n32KB 35.10 841.50 1438.20 1008.47 846.16 2268.03 1222.37 1532.43\n48KB 21.63 404.67 876.32 295.55 317.96 703.97 902.99 914.97\n64KB 16.85 231.51 608.17 196.10 202.46 643.72 565.21 625.38\n80KB 14.29 144.01 542.87 86.35 173.80 299.46 524.43 462.48\n96KB 13.19 98.37 430.44 60.62 119.25 274.27 483.68 361.92\n112KB 10.79 70.56 347.66 27.47 61.44 155.77 393.32 294.88\n128KB 10.18 52.63 247.36 23.44 34.50 144.44 654.84 288.81Retail16KB 69.16 3166.01 3468.11 5527.20 4129.77 7601.40 1766.46 3857.67\n32KB 41.02 1066.05 1569.19 1265.28 1097.03 2621.57 1197.95 1734.13\n48KB 27.14 511.37 984.25 346.69 415.05 850.06 894.09 1051.95\n64KB 22.20 291.21 690.76 211.31 254.86 742.85 582.79 740.24\n80KB 18.87 179.20 643.02 83.16 208.78 344.02 591.83 554.35\n96KB 15.07 119.71 516.00 50.66 130.63 282.78 494.88 441.95\n112KB 12.56 82.79 419.62 17.72 60.03 156.78 448.04 362.26\n128KB 10.56 59.45 305.66 13.67 32.82 129.85 773.58 358.09\nB. Performance Comparison\nEstimation Results on Real-world Datasets.First, we\ncompare the AAE, ARE, and WMRD metrics for all seven\nsketches, by varying the local memory budgets from 32KB\nto 256KB on CAIDA dataset and 16KB to 128KB on other\nrelatively smaller datasets. In the first two columns of Fig. 9,\nall methods achieve smaller AAE and ARE by increasing\nthe space budget, whereas the UCL-sketch nearly always\n3The implementation of all sketches (in Python) can be found in our\ncodebase, which is written mainly based on a C++ Github repository at\nhttps://github.com/N2-Sys/BitSense.\n\nJOURNAL OF L ATEX CLASS FILES 11\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0100200300400/uni00000024/uni00000024/uni00000028\n/uni00000026/uni00000030 /uni00000026/uni00000036 /uni0000002f/uni00000026/uni00000030 /uni0000002f/uni00000026/uni00000036 /uni00000028/uni00000036 /uni00000038/uni00000030 /uni00000031/uni00000036 /uni00000032/uni00000058/uni00000055/uni00000056\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0100200300/uni00000024/uni00000035/uni00000028\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0.51.01.52.0/uni0000003a/uni00000030/uni00000035/uni00000027\n0204060\n0204060\n(a) Skewness = 1.2\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c050100150200/uni00000024/uni00000024/uni00000028\n/uni00000026/uni00000030 /uni00000026/uni00000036 /uni0000002f/uni00000026/uni00000030 /uni0000002f/uni00000026/uni00000036 /uni00000028/uni00000036 /uni00000038/uni00000030 /uni00000031/uni00000036 /uni00000032/uni00000058/uni00000055/uni00000056\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c050100150200/uni00000024/uni00000035/uni00000028\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0.51.01.52.0/uni0000003a/uni00000030/uni00000035/uni00000027\n0102030\n01020\n(b) Skewness = 1.3\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c050100150200250/uni00000024/uni00000024/uni00000028\n/uni00000026/uni00000030 /uni00000026/uni00000036 /uni0000002f/uni00000026/uni00000030 /uni0000002f/uni00000026/uni00000036 /uni00000028/uni00000036 /uni00000038/uni00000030 /uni00000031/uni00000036 /uni00000032/uni00000058/uni00000055/uni00000056\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c050100150200/uni00000024/uni00000035/uni00000028\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0.51.01.52.0/uni0000003a/uni00000030/uni00000035/uni00000027\n0102030\n05101520\n(c) Skewness = 1.4\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c020406080100120/uni00000024/uni00000024/uni00000028\n/uni00000026/uni00000030 /uni00000026/uni00000036 /uni0000002f/uni00000026/uni00000030 /uni0000002f/uni00000026/uni00000036 /uni00000028/uni00000036 /uni00000038/uni00000030 /uni00000031/uni00000036 /uni00000032/uni00000058/uni00000055/uni00000056\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c020406080/uni00000024/uni00000035/uni00000028\n20 40 60 80 100 120\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0.51.01.52.0/uni0000003a/uni00000030/uni00000035/uni00000027\n051015\n0510\n(d) Skewness = 1.5\nFig. 10. Performance comparison between our UCL-sketch and existing state-\nof-the-art sketches on synthetic Zipfian datasets with different skewness.\nperforms the best. In particular, when the memory cost is\nlower than 64KB, UCL-sketch achieves 6∼20 times smaller\nerror rate, compared to the best algorithm. On the stream\ndata with much larger key space, i.e. CAIDA dataset, there\nis a more remarkable accuracy gap between our method and\nother sketches in all memory settings. For example, AREs\nof CM, CS, LCM, LCS, ES, UM, NS are 7.91 times, 6.98\ntimes, 16.39 times, 11.13 times, 19.11 times, 12.67 times, and\n8.11 times of that of ours on average, respectively. Despite\nthe additional BF to keep tracking unique items, the reason is\nthat existing algorithms cannot estimate per-key aggregations\naccurately without leveraging the linear system of sketching.\nRegarding frequency distribution alignment, we find that\nUCL-sketch still achieves better accuracy than the state-of-\nthe-art sketches. As shown in the last column in Fig. 9, we\nobserve that UCL-sketch maintains the WMRD value lower\nthan 0.55 in all settings on three datasets. Unfortunately, such\na stable performance does not persist in other competitors. We\nsee that they are significantly less precise than UCL-sketch,\nalthough the metric drops as memory increases in general.\nEven worse, the baselines only achieve WMRDs over 1.5\nwith 128KB of memory on CAIDA, because the trace set\ncontains too many keys to reliably measure the distribution\nof the stream with traditional point-wise methods, and their\ndesired resources exceed the hardware capacity. Moreover,\nwe list entropy relative errors of all estimation algorithmsTABLE IV\nENTROPY ABSOLUTE ERROR ON DIFFERENTZIPFIAN DATASETS\n(BOLDINDICATES BEST PERFORMANCE)\nSkewness Memory Ours CM CS LCM LCS ES UM NS\n1.232KB 6.23 1354.36 1869.68 2290.03 1008.36 3369.08 1405.47 1109.91\n64KB 2.87 502.69 724.66 830.31 369.63 1259.31 556.77 433.10\n96KB 3.56 277.20 431.75 448.20 497.78 675.75 537.01 250.64\n128KB 3.01 180.74 242.73 288.44 133.06 443.37 694.09 263.25\n160KB 3.74 129.94 206.26 185.51 94.36 428.60 330.40 226.99\n192KB 2.90 98.99 158.24 167.02 193.81 237.88 401.55 116.56\n224KB 2.70 78.23 126.25 156.93 78.60 156.00 206.78 175.68\n256KB 1.53 63.73 103.69 100.37 42.89 112.46 185.67 117.76\n1.332KB 3.26 592.36 1192.73 913.99 476.62 1569.04 1047.64 816.45\n64KB 1.41 201.90 435.70 294.59 159.95 509.99 391.56 296.26\n96KB 2.29 105.46 248.73 147.04 192.65 263.63 373.41 167.69\n128KB 1.60 66.54 141.37 88.00 52.50 164.57 416.50 167.98\n160KB 2.13 46.66 120.21 57.10 21.95 154.25 206.73 145.24\n192KB 1.76 34.75 89.10 50.18 17.72 80.89 161.93 115.41\n224KB 1.46 26.97 73.72 43.61 14.52 51.25 118.25 88.42\n256KB 0.81 21.50 58.37 25.51 10.30 35.84 102.04 70.00\n1.416KB 34.26 777.78 2015.95 1303.13 834.16 2106.11 1437.18 1528.59\n32KB 1.71 249.71 724.19 338.55 205.26 660.07 772.68 525.88\n48KB 3.65 126.36 394.16 116.05 83.62 213.24 517.90 278.60\n64KB 0.53 77.86 289.24 93.28 62.56 200.60 265.29 174.61\n80KB 0.30 52.84 188.41 48.16 76.59 100.88 216.73 131.69\n96KB 0.89 38.47 137.07 41.25 64.79 94.11 216.73 96.45\n112KB 1.26 29.64 115.87 23.69 39.95 58.06 195.21 81.31\n128KB 0.61 23.46 83.55 22.19 18.21 55.11 244.32 102.76\n1.516KB 24.69 357.16 1335.87 554.13 398.21 966.71 991.13 993.50\n32KB 2.94 105.53 460.32 117.42 83.63 286.85 467.95 339.90\n48KB 4.86 50.26 264.98 36.07 32.43 83.11 309.66 179.28\n64KB 4.25 29.70 149.75 26.13 22.49 73.99 162.53 103.97\n80KB 1.71 19.52 107.31 12.23 24.34 34.8 129.13 75.11\n96KB 4.02 13.76 78.85 9.47 19.31 29.52 129.13 56.80\n112KB 4.46 10.51 62.80 4.73 11.19 17.22 89.86 46.64\n128KB 0.79 7.98 46.58 4.22 5.88 15.01 159.09 58.71\nin Table III. As expected, similar trends to WMRD on the\nfrequency entropy can be observed in the table, where our\nalgorithm consistently achieves the smallest absolute error,\nsubstantially outperforming the second-best. Overall, UCL-\nsketch achieves both high performance in frequency query and\ndistributional accuracy.\nEstimation Results on Toy Zipfian Datasets.Fig. 10 plots\nthe average results regarding AAE, ARE, and WMRD, while\nfrequency entropy relative errors are listed in Table IV. The\nresults confirm the conclusion shown in the real-world results:\nWe observe that the performance of most methods increases\nas skewness rises. For example, in the case of skewness=1.2, a\nremark performance decline across all baselines. However, of\nparticular interest is an inversely proportional trend observed\nin UCL-sketch’s accuracy, as with the lowest skewness, the\nproposed algorithm can achieve even smaller deviation, e.g.\naverage entropy absolute errors never exceeding 9 in that\ncase, beating the other algorithms with a significant advantage.\nUpon further investigation, we have identified that this is\nbecause the heavy-tailed distribution leads to much fewer\ndistinct keys given the same total count, then the average effect\noffsets and even reverses the growth of our errors. Besides, we\nfind that UCL always achieves better accuracy than the state-\nof-the-art algorithm. When utilizing 64KB of memory, from\nFig. 10 (a), it becomes evident that UCL-sketch achieves a\nsubstantial reduction in AAE compared to the CM, CS, LCM,\nLCS, ES, UM and NS by factors of 10.94, 10.44, 22.48, 21.09,\n\nJOURNAL OF L ATEX CLASS FILES 12\n24.77, 22.35 and 12.98, respectively. In terms of WMRD, our\nexperimental results show that CM, CS, LCM, LCS, ES, UM\nand NS are 8.00, 7.67, 8.04, 7.71, 7.96, 7.88 and 7.92 times\nhigher than that of the UCL-sketch in average.\n/uni00000017/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000048 /uni0000001b/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000048 /uni00000014/uni00000016/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000048\n/uni0000002e/uni00000048/uni0000005c/uni00000003/uni00000036/uni0000004c/uni0000005d/uni000000480255075100125150175200/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni00000032/uni00000058/uni00000055/uni00000056\n/uni00000026/uni00000030\n/uni00000026/uni00000036/uni0000002f/uni00000026/uni00000030\n/uni0000002f/uni00000026/uni00000036\n/uni00000028/uni00000036/uni00000038/uni00000030\n/uni00000031/uni00000036\n(a) Stream Insertion Throughput\n/uni00000017/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000048 /uni0000001b/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000048 /uni00000014/uni00000016/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000048\n/uni0000002e/uni00000048/uni0000005c/uni00000003/uni00000036/uni0000004c/uni0000005d/uni000000480255075100125150175/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni00000032/uni00000058/uni00000055/uni00000056\n/uni00000026/uni00000030\n/uni00000026/uni00000036/uni0000002f/uni00000026/uni00000030\n/uni0000002f/uni00000026/uni00000036\n/uni00000028/uni00000036/uni00000038/uni00000030\n/uni00000031/uni00000036 (b) Per-key Query Throughput\nFig. 11. Results on processing speed comparison.\nProcessing Speed.We perform insertions of all items in a\nstream, record the total time used. We tested the C++ runtime\nof CM-sketch and proportionally scaled up all Python methods\naccordingly. We calculate million of operations per second\n(Mops) as throughput to measure the processing speed of\nvarious sketching algorithms. By using Kosarak with 64KB of\nmemory, we evaluate the throughput of UCL-sketch and other\nsolutions under different key sizes. Fig. 11 (a) provides the\ncomparison results of insertion processing speed, in which we\nrun a simple two-layer RNN model once before each update\noperation to simulate the actual practice of learning-augmented\nalgorithms. From the figure, it becomes evident that previous\nlearning-based sketch, i.e. LCM and LCS, fails to meet the\nrequirements of processing high-speed data streams, since the\npropagation time overwhelms the insertion time, encumbering\nthe efficiency of sketching. Meanwhile, ES achieves the high-\nest throughput through its lightweight data structure. Fig. 11\nalso shows that UCL-sketch which performed as the second\nfastest can achieve almost 50×speed of learning-augmented\nsketch due to the model-free update and heavy filter in the\ndata plane. The query processing throughput is shown in\nFig. 11 (b). The query items are uniformly sampled from the\nincoming stream, which means in the skewed data stream,\nhigh-frequency items are queried more frequently than low-\nfrequency ones. So, although the prediction time of neural\nnetworks exceeds that of hashing operations, our hash table\nis capable of answering most of the queries. As expected,\nFig. 11 (b) shows that UCL-sketch still maintains performance\ncomparable to that of the Nitrosketch and slightly inferior\nto C-sketch. Given the consistently slow processing speed of\nexisting learning-augmented sketches, our results attest the\npracticality of UCL-sketch.\nConvergence Rate.Next, we evaluate the convergence rate\nof UCL-sketch, a crucial factor since, after configuring or re-\nsetting the measurement framework, it is vital to quickly obtain\nan effective solver ready for query processing. As shown in\nFig. 12, the training loss on the CAIDA dataset drops to an ac-\nceptable level within just 30 iterations and fully converges after\napproximately 190 iterations. Each training step—including\nboth forward and backward passes—takes only milliseconds or\neven microseconds, enabling rapid convergence from scratch.\nThus, (re-)training UCL-sketch minimally affects the system’s\nroutine operation.\n/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000013\n/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni00000048/uni00000047/uni00000003/uni00000036/uni00000052/uni0000004f/uni00000059/uni00000048/uni00000055/uni00000003/uni00000038/uni00000053/uni00000047/uni00000044/uni00000057/uni00000048/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013(a) Original loss curve\n/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000013\n/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni00000048/uni00000047/uni00000003/uni00000036/uni00000052/uni0000004f/uni00000059/uni00000048/uni00000055/uni00000003/uni00000038/uni00000053/uni00000047/uni00000044/uni00000057/uni00000048/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000015/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018/uni0000002f/uni00000052/uni0000004a/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048 (b) Log-transformed loss curve\nFig. 12. Training loss curve of the 128KB UCL-sketch on the CAIDA dataset.\nC. Handling Changing Data Streams\nWe evaluate the robustness and generalization of UCL-\nsketch under temporal and spatial variations in data streams.\nMassively unforeseen items are not specifically addressed, as\nsuch scenarios are of lesser practical concern. Throughout\nthese experiments, we randomly sample 15% of the key set to\nsimulate variations in data stream behavior. Additionally, the\nexperimental setup uses the CAIDA dataset with 64 KB.\nTABLE V\n(ARE) PERFORMANCE DEGRADATION WITH CHANGING CONDITIONS\n# of Interval Temporal Fluctuation Factor Total Freq. Prop. of Hot Keys\nafter Change 2× 10× 20× 50× 90% 75% 50% 25%\nSW 0 0.04% 0.52% 1.58% 2.55% 0.06% 0.40% 1.26% 2.14%\nSW 1 0.02% 0.34% 1.08% 1.52% 0.03% 0.16% 0.71% 1.36%\nSW 2 0.01% 0.16% 0.57% 0.75% 0.01% 0.06% 0.27% 0.52%\nSW 4 0.00% 0.04% 0.08% 0.21% 0.00% 0.02% 0.04% 0.16%\nTemporal Fluctuation.We introduce various temporal fluc-\ntuations to the streaming data. For each selected key, we calcu-\nlate its occurrence in the sliding window, and then multiply it\nby 2, 10, 20, and 50 respectively. As shown in Table V, UCL-\nsketch exhibits no significant degradation in ARE under these\nfluctuating conditions. Even with a 50×scaling factor, the\nincrease in ARE remains below 3%. Furthermore, the learned\nsolver rapidly adapts to these fluctuations.\nSpatial Distribution.We redistribute these packets across\nIP addresses to simulate variations in their spatial distribution.\nSpecifically, we reassign the top 10% of hot keys to the\nselected ones, so that they subsequently represent 90%, 75%,\n50%, and 25% of the their (original) volume, respectively.\nTable V shows that UCL-sketch still consistently satisfies the\nnear-optimal ARE across all spatial distributions.\nD. Sensitivity Analysis\nWe measure the influence of some key parameter settings,\ni.e. size of the bucket and hidden dimension, on accuracy,\ndistribution, and resource usage. We use the Kosarak dataset\nin these experiments.\n/uni00000014/uni00000015/uni0000001b /uni00000015/uni00000018/uni00000019 /uni00000018/uni00000014/uni00000015 /uni00000014/uni00000013/uni00000015/uni00000017 /uni00000015/uni00000013/uni00000017/uni0000001b\n/uni00000025/uni00000058/uni00000046/uni0000004e/uni00000048/uni00000057/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b0.00.20.40.60.81.01.21.4/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000038/uni00000056/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c\nFig. 13. Total trainable parame-\nters w.r.t bucket length.The impact of bucket length.\nIn this experiment, we vary\nbucket length in [128, 256, 512,\n1024, 2056]. The results in\nFig. 15 show that a small size\nof bucket can achieve similar\nAREs, but it leads to notable\ndegradation in the entropy of es-\ntimated frequencies. Therefore,\n\nJOURNAL OF L ATEX CLASS FILES 13\nwe choose 512 or 1024 as its value in other experiments to\nmake a balance between overall performance and memory\nusage (see Fig. 13). In fact, users can set bucket length\naccording to the key space they are interested in, and a length\nof 1024 will be enough in general.\n/uni00000016/uni00000015 /uni00000019/uni00000017 /uni00000014/uni00000015/uni0000001b /uni00000015/uni00000018/uni00000019 /uni00000018/uni00000014/uni00000015\n/uni0000002b/uni0000004c/uni00000047/uni00000047/uni00000048/uni00000051/uni00000003/uni00000027/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni0000005101234/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000038/uni00000056/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c\nFig. 14. Total trainable parame-\nters w.r.t hidden dimension.The impact of hidden di-\nmension.We retrain models\nwith hidden dimensions from 32\nto 512 to obtain 5×2 groups of\nexperimental results in Fig. 16.\nAs shown in Fig. 16, the per-\nformance of frequency estima-\ntion improves with increasing\nthe hidden dimension, indicating\nthat the higher-dimensional representation is available, the\nbetter the model can be trained and the better results can\nbe achieved by the extracted feature. However, after the size\nreaches a certain value e.g. 128, the increase of dimension does\nnot boost the recovery performance obviously. Also note that\nthe memory overhead of the model exhibits exponential growth\nin high-dimensional (over 64) hidden spaces, which can be\nobserved in Fig. 14. We thus set it to 128 in all experiments.\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c163248648096112128/uni00000025/uni00000058/uni00000046/uni0000004e/uni00000048/uni00000057/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b128\n256\n512\n1024\n205602468\n(a) Average Relative Error\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c163248648096112128/uni00000025/uni00000058/uni00000046/uni0000004e/uni00000048/uni00000057/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b128\n256\n512\n1024\n2056010203040506070 (b) Entorpy Absolute Error\nFig. 15. Effects of the shared bucket length.\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c163248648096112128/uni0000002b/uni0000004c/uni00000047/uni00000047/uni00000048/uni00000051/uni00000003/uni00000027/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni0000005132\n64\n128\n256\n5120246810\n(a) Average Relative Error\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c163248648096112128/uni0000002b/uni0000004c/uni00000047/uni00000047/uni00000048/uni00000051/uni00000003/uni00000027/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni0000005132\n64\n128\n256\n5120102030405060 (b) Entorpy Absolute Error\nFig. 16. Effects of the hidden dimension in the learned solver.\nE. Abaltional Study\nIn Fig. 17 and Table VI, we compare the original UCL-\nsketch with its modified version to analyze the impact of each\ncomponent. Our used dataset is Kosarak and memory is 64KB.\nLearning version v.s. Non-learning version.Three tradi-\ntional versions of the decoding method are used for compar-\nison: OMP used in SeqSketch, LSQR [44], and Count-MinTABLE VI\nENTROPY ABSOLUTE ERROR WITH VIARIATES OFUCL-SKETCH\nViariates UCL-sketch w/o SA w/o EQ w/o SR OMP LSQR CM\nAbsolute Error 16.85 408.93 376.53 358.09 346.78 301.32 204.29\n020406080/uni00000024/uni00000024/uni00000028/uni00000032/uni00000058/uni00000055/uni00000056\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni00000024/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000028/uni00000034\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni00000035/uni00000032/uni00000030/uni00000033 /uni0000002f/uni00000036/uni00000034/uni00000035 /uni00000026/uni00000030\n0.00.51.01.5/uni0000003a/uni00000030/uni00000035/uni00000027\nFig. 17. Quantitative results of ablational studies.\n(CM). We also implemented a Pytorch-version OMP (OMP-\nGPU) to further enhance the algorithm’s parallelization. As\nshown in Fig. 17, we can find that the accuracy under our\nlearning-based algorithm is consistently better than that under\nnon-learning methods. CM performs the worst since it does\nnot utilize information from the linear system. Meanwhile,\nOMP is more precise than LSQR owing to its sparse greedy\nsolution. We then measure its inference time for different\nnumbers of keys. The left of Fig. 18 gives a recovery time\nof OMP or/and OMP-GPU that is over 60 seconds in some\ncases, which is much slower than ours which keeps the time\naround 0.5 seconds. At the same time, see the Appendix\nfor additional experiments comparing UCL-sketch to similar\nequation-based sketching methods. Importantly, UCL-sketch\nalso outperforms all other considered sketches on our real-\nworld datasets. Therefore, proposed learning technologies do\nboost the design of high-performance sketches.\nBasic training v.s. Optimized training.Also see Fig. 17\nfor the ablation results on training options, whose details are\nas follows.Without EQ: We retrain the solver by removing the\nthird term in Eqn. 2. Noticeable performance degradation is\nobserved in both AAE and WMRD when compared to ours.\nThis has indicated the effectiveness of equivalent learning\nfor handling the null-space ambiguity without ground truth.\nWithout SR: We remove the second loss term in Eqn. 2 and\nretrain the model. The accuracy is close to the situation without\nEQ. But it’s interesting to see that WMRD after discarding\nthe sparse regularization is slightly lower than the original\nversion. The reason may be the sparsity assumption limits the\nmodel’s ability to learn the heavy-tailed distribution weakly.\nHowever, Table VI shows that UCL-sketch offers a much\nbetter estimation of frequency entropy than the other two\nvariates for all memory sizes.\nScalable network v.s. Non-scalable network.To demon-\nstrate the impact ofscalable architecture (SA), we train a\nnetwork with unshared buckets. Surprisingly, the unshared\nversion does not achieve the best performance in Fig. 17.\nThis is because the parameter sharing acts as a form of\nregularization, preventing overfitting so that the solver is more\nlikely to generalize well to future counters, especially for very\n\nJOURNAL OF L ATEX CLASS FILES 14\n/uni00000014/uni0000002e /uni00000014/uni00000013/uni0000002e /uni00000015/uni00000013/uni0000002e /uni00000014/uni00000013/uni00000013/uni0000002e\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002e/uni00000048/uni0000005c/uni000000560200400600/uni00000035/uni00000048/uni00000046/uni00000052/uni00000059/uni00000048/uni00000055/uni0000005c/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c\n/uni00000032/uni00000058/uni00000055/uni00000056\n/uni00000032/uni00000030/uni00000033\n/uni00000032/uni00000030/uni00000033/uni00000010/uni0000002a/uni00000033/uni00000038\n/uni00000014/uni0000002e /uni00000014/uni00000013/uni0000002e /uni00000015/uni00000013/uni0000002e /uni00000014/uni00000013/uni00000013/uni0000002e /uni00000018/uni00000013/uni00000013/uni0000002e\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002e/uni00000048/uni0000005c/uni00000056050100150200250/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c\n/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000044/uni00000045/uni0000004f/uni00000048\n/uni00000031/uni00000052/uni00000051/uni00000010/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000044/uni00000045/uni0000004f/uni00000048\nFig. 18.Left:Recovery time.Right:Size of parameters.\nlarge-scale streams. A summary of the required memory is\nreported on the right of Fig. 18. Obviously, in the scalable net-\nwork, with the increase in the number of keys, the parameters\nsignificantly decrease. By setting 500K unique items, the non-\nscalable network takes up a memory of over 250MB, while\nours only requires a consumption of 1MB. This indicates that\nour compression scheme in parameters is very promising and\npractical, with an even better effect on estimation performance.\nVII. CONCLUSION\nIn this paper, we present the first GT-free learning-based\nfrequency estimation algorithm, called UCL-sketch which\nprovides a novel perspective for approximate measurement by\nleveraging the binding between sketch sensing and machine\nlearning. UCL-sketch extends the existing equation-based\nstreaming algorithms with a ML technique complementing\nsolver. It designs a training strategy to allow users to on-line\ntrain the deep solver without any ground truth. Using a scalable\nmodel architecture, UCL-sketch automatically and efficiently\nconfigures parameters for adapting to continuously expanding\ndata streams. Through extensive evaluation, the efficiency and\naccuracy demonstrate the power of our methodology.\nWe believe that our investigation of learning-based sketches\nfrom the CS perspective has but scratched the surface and\noutline below current limitations of our approach, as well as\nintriguing directions for future research. 1Our method’s in-\ntuition and proof partly rely on Zipf’s law. While experiments\nshow strong generalization, more realistic (learnable) priors\nmay improve accuracy and training speed. 2We analyzed the\nBloom Filter’s false positive rate for new keys, and it implies\nthat ensuring a very low number of missed keys requires an\nalmost linear increase in space. Designing a more compact\ndata structure is left for future work. Finally, we hope that\nthis work will spark more research in the area of learning\ncombinatorial sketching techniques.\nREFERENCES\n[1] G. Cormode, “Data sketching,”Communications of the ACM, vol. 60,\nno. 9, pp. 48–55, 2017.\n[2] T. Yang, L. Wang, Y . Shen, M. Shahzad, Q. Huang, X. Jiang, K. Tan,\nand X. Li, “Empowering sketches with machine learning for network\nmeasurements,” inProceedings of the 2018 Workshop on Network Meets\nAI & ML, 2018, pp. 15–20.\n[3] A. Kumar, M. Sung, J. Xu, and J. Wang, “Data streaming algorithms\nfor efficient and accurate estimation of flow size distribution,”ACM\nSIGMETRICS Performance Evaluation Review, vol. 32, no. 1, pp. 177–\n188, 2004.\n[4] P. Roy, A. Khan, and G. Alonso, “Augmented sketch: Faster and more\naccurate stream processing,” inProceedings of the 2016 International\nConference on Management of Data, 2016, pp. 1449–1463.[5] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” inProceedings of the 2018 international\nconference on management of data, 2018, pp. 489–504.\n[6] M. Mitzenmacher, “A model for learned bloom filters and optimizing\nby sandwiching,”Advances in Neural Information Processing Systems,\nvol. 31, 2018.\n[7] J. Rae, S. Bartunov, and T. Lillicrap, “Meta-learning neural bloom\nfilters,” inInternational Conference on Machine Learning. PMLR,\n2019, pp. 5271–5280.\n[8] T. Lykouris and S. Vassilvitskii, “Competitive caching with machine\nlearned advice,”Journal of the ACM (JACM), vol. 68, no. 4, pp. 1–25,\n2021.\n[9] E. Khalil, H. Dai, Y . Zhang, B. Dilkina, and L. Song, “Learning\ncombinatorial optimization algorithms over graphs,”Advances in neural\ninformation processing systems, vol. 30, 2017.\n[10] C.-Y . Hsu, P. Indyk, D. Katabi, and A. Vakilian, “Learning-based fre-\nquency estimation algorithms.” inInternational Conference on Learning\nRepresentations, 2019.\n[11] T. Jiang, Y . Li, H. Lin, Y . Ruan, and D. P. Woodruff, “Learning-\naugmented data stream algorithms,”ICLR, 2020.\n[12] E. Du, F. Wang, and M. Mitzenmacher, “Putting the “learning” into\nlearning-augmented algorithms for frequency estimation,” inInterna-\ntional Conference on Machine Learning. PMLR, 2021, pp. 2860–2869.\n[13] Y . Yan, F. Li, W. Wang, and X. Wang, “Talentsketch: Lstm-based sketch\nfor adaptive and high-precision network measurement,” in2022 IEEE\n30th International Conference on Network Protocols (ICNP). IEEE,\n2022, pp. 1–12.\n[14] X. Cheng, X. Jing, Z. Yan, X. Li, P. Wang, and W. Wu, “Alsketch:\nAn adaptive learning-based sketch for accurate network measurement\nunder dynamic traffic distribution,”Journal of Network and Computer\nApplications, vol. 216, p. 103659, 2023.\n[15] A. Aamand, J. Chen, H. Nguyen, S. Silwal, and A. Vakilian, “Im-\nproved frequency estimation algorithms with and without predictions,”\nAdvances in Neural Information Processing Systems, vol. 36, 2024.\n[16] S. Li, L. Luo, D. Guo, Q. Zhang, and P. Fu, “A survey of sketches in\ntraffic measurement: Design, optimization, application and implementa-\ntion,”arXiv preprint arXiv:2012.07214, 2020.\n[17] Y . Fu, D. Li, S. Shen, Y . Zhang, and K. Chen, “Clustering-preserving\nnetwork flow sketching,” inIEEE INFOCOM 2020-IEEE Conference on\nComputer Communications. IEEE, 2020, pp. 1309–1318.\n[18] S. Sheng, Q. Huang, S. Wang, and Y . Bao, “Pr-sketch: monitoring per-\nkey aggregation of streaming data with nearly full accuracy,”Proceed-\nings of the VLDB Endowment, vol. 14, no. 10, pp. 1783–1796, 2021.\n[19] Q. Huang, S. Sheng, X. Chen, Y . Bao, R. Zhang, Y . Xu, and G. Zhang,\n“Toward{Nearly-Zero-Error}sketching via compressive sensing,” in\n18th USENIX Symposium on Networked Systems Design and Implemen-\ntation (NSDI 21), 2021, pp. 1027–1044.\n[20] G. Cormode, “Sketch techniques for approximate query processing,”\nFoundations and Trends in Databases. NOW publishers, vol. 15, 2011.\n[21] G. Cormode and S. Muthukrishnan, “Towards an algorithmic the-\nory of compressed sensing,”Center for Discrete Math. and Comp.\nSci.(DIMACS), Tech. Rep. TR, vol. 25, p. 2005, 2005.\n[22] F. D. Dalt, S. Scherrer, and A. Perrig, “Bayesian sketches for volume\nestimation in data streams,”Proceedings of the VLDB Endowment,\nvol. 16, no. 4, pp. 657–669, 2022.\n[23] G. Cormode and S. Muthukrishnan, “An improved data stream summary:\nthe count-min sketch and its applications,”Journal of Algorithms,\nvol. 55, no. 1, pp. 58–75, 2005.\n[24] M. Charikar, K. Chen, and M. Farach-Colton, “Finding frequent items\nin data streams,” inInternational Colloquium on Automata, Languages,\nand Programming. Springer, 2002, pp. 693–703.\n[25] C. Estan and G. Varghese, “New directions in traffic measurement and\naccounting,” inProceedings of the 2002 conference on Applications,\ntechnologies, architectures, and protocols for computer communications,\n2002, pp. 323–336.\n[26] Z. Liu, A. Manousis, G. V orsanger, V . Sekar, and V . Braverman, “One\nsketch to rule them all: Rethinking network flow monitoring with\nunivmon,” inProceedings of the 2016 ACM SIGCOMM Conference,\n2016, pp. 101–114.\n[27] T. Yang, J. Jiang, P. Liu, Q. Huang, J. Gong, Y . Zhou, R. Miao,\nX. Li, and S. Uhlig, “Elastic sketch: Adaptive and fast network-wide\nmeasurements,” inProceedings of the 2018 Conference of the ACM\nSpecial Interest Group on Data Communication, 2018, pp. 561–575.\n[28] Z. Liu, R. Ben-Basat, G. Einziger, Y . Kassner, V . Braverman, R. Fried-\nman, and V . Sekar, “Nitrosketch: robust and general sketch-based\nmonitoring in software switches,”Proceedings of the ACM Special\nInterest Group on Data Communication, 2019.\n\nJOURNAL OF L ATEX CLASS FILES 15\n[29] G. M. Lee, H. Liu, Y . Yoon, and Y . Zhang, “Improving sketch recon-\nstruction accuracy using linear least squares method,” inProceedings\nof the 5th ACM SIGCOMM conference on Internet Measurement, 2005,\npp. 24–24.\n[30] Y . Lu, A. Montanari, B. Prabhakar, S. Dharmapurikar, and A. Kabbani,\n“Counter braids: A novel counter architecture for per-flow measure-\nment,”ACM SIGMETRICS Performance Evaluation Review, vol. 36,\nno. 1, pp. 121–132, 2008.\n[31] J. He, J. Zhu, and Q. Huang, “Histsketch: A compact data structure\nfor accurate per-key distribution monitoring,” in2023 IEEE 39th Inter-\nnational Conference on Data Engineering (ICDE). IEEE, 2023, pp.\n2008–2021.\n[32] K. Gregor and Y . LeCun, “Learning fast approximations of sparse\ncoding,” inProceedings of the 27th international conference on inter-\nnational conference on machine learning, 2010, pp. 399–406.\n[33] T. Pang, Y . Quan, and H. Ji, “Self-supervised bayesian deep learning for\nimage recovery with applications to compressive sensing,” inComputer\nVision–ECCV 2020: 16th European Conference, Glasgow, UK, August\n23–28, 2020, Proceedings, Part XI 16. Springer, 2020, pp. 475–491.\n[34] D. Chen, J. Tachella, and M. E. Davies, “Equivariant imaging: Learning\nbeyond the range space,” inProceedings of the IEEE/CVF International\nConference on Computer Vision, 2021, pp. 4379–4388.\n[35] L. Li, K. Xie, S. Pei, J. Wen, W. Liang, and G. Xie, “Cs-sketch:\nCompressive sensing enhanced sketch for full traffic measurement,”\nIEEE Transactions on Network Science and Engineering, 2023.\n[36] Y . Zhang, M. Roughan, W. Willinger, and L. Qiu, “Spatio-temporal\ncompressive sensing and internet traffic matrices,” inProceedings of\nthe ACM SIGCOMM 2009 conference on Data communication, 2009,\npp. 267–278.\n[37] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”\nAdvances in Neural Information Processing Systems, vol. 33, pp. 6840–\n6851, 2020.\n[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” inProceedings\nof the 31st International Conference on Neural Information Processing\nSystems, ser. NIPS’17. Red Hook, NY , USA: Curran Associates Inc.,\n2017, p. 6000–6010.\n[39] E. J. Candes and T. Tao, “Decoding by linear programming,”IEEE\ntransactions on information theory, vol. 51, no. 12, pp. 4203–4215,\n2005.\n[40] Caida, “Caida anonymized internet traces 2018 dataset,” http://www.\ncaida.org/data/passive/passivedataset.xml.\n[41] B. Ferenc, “Anonymized click-stream data,” http://fmi.uantwerpen.be/\ndata/kosarak.dat.gz.\n[42] T. Brijs, G. Swinnen, K. Vanhoof, and G. Wets, “Using association rules\nfor product assortment decisions: A case study,” inProceedings of the\nfifth ACM SIGKDD international conference on Knowledge discovery\nand data mining, 1999, pp. 254–260, http://fmi.uantwerpen.be/data/\nretail.dat.gz.\n[43] Y . Li, R. Miao, C. Kim, and M. Yu, “{FlowRadar}: A better{NetFlow}\nfor data centers,” in13th USENIX symposium on networked systems\ndesign and implementation (NSDI 16), 2016, pp. 311–324.\n[44] C. C. Paige and M. A. Saunders, “Lsqr: An algorithm for sparse linear\nequations and sparse least squares,”ACM Trans. Math. Softw., vol. 8,\nno. 1, p. 43–71, mar 1982.\n[45] E. J. Candes, “The restricted isometry property and its implications for\ncompressed sensing,”Comptes rendus. Mathematique, vol. 346, no. 9-\n10, pp. 589–592, 2008.\n[46] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nComputer Science, 2014.\n[47] Y . C. Pati, R. Rezaiifar, and P. S. Krishnaprasad, “Orthogonal matching\npursuit: Recursive function approximation with applications to wavelet\ndecomposition,” inProceedings of 27th Asilomar conference on signals,\nsystems and computers. IEEE, 1993, pp. 40–44.\n[48] D. C.-L. Fong and M. Saunders, “Lsmr: An iterative algorithm for sparse\nleast-squares problems,”SIAM Journal on Scientific Computing, vol. 33,\nno. 5, pp. 2950–2971, 2011.\n[49] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy,\nD. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J.\nvan der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J.\nNelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ˙I. Polat, Y . Feng, E. W.\nMoore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henrik-\nsen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro,\nF. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors, “SciPy 1.0:\nFundamental Algorithms for Scientific Computing in Python,”Nature\nMethods, vol. 17, pp. 261–272, 2020.APPENDIX\nA. Proofs\nIn this section, we prove the theoretical results in the paper\nby partially following [45] and [22].\nv\nv\nv+\n+\n1y\n2y\n3y\n4y\n5y\n6y\n()1 hash key 0i=\n()2 hash key 2i=0 1 2\n1y\n2y\n3y\n4y\n5y\n6y\n=\n\n1x\n2x\n3x\n4x\n5x\nnx…\n1key\n2key\n3key\n4key\n5key\nkeyn…1\n0\n0\n1\n0\n00\n1\n0\n1\n0\n00\n1\n0\n0\n0\n10\n0\n1\n0\n1\n00\n1\n0\n0\n0\n11\n0\n0\n0\n1\n0…\n…\n…\n…\n…\n…1 2 3 4 5…\n1\n2\n3\n4\n5\n6\nn\ny\nA\nx\nkeyi\n 1 0, 1Ai+=\n 4 2, 1Ai+=\n(a) Count-Min Sketch Sensing\n 4 2, 1Ai+=\nv\nv-\n+\n1y\n2y\n3y\n4y\n5y\n6y\n()1 hash key 0i=0 1 2\n1y\n2y\n3y\n4y\n5y\n6y\n=\n\n1x\n2x\n3x\n4x\n5x\nnx…-1\n0\n0\n1\n0\n00\n1\n0\n-1\n0\n00\n-1\n0\n0\n0\n10\n0\n-1\n0\n-1\n00\n1\n0\n0\n0\n11\n0\n0\n0\n-1\n0…\n…\n…\n…\n…\n…1 2 3 4 5…\n1\n2\n3\n4\n5\n6\nn\ny\nA\nx\n()2 hash key 2i=\n()3 hash key 1i=−\n 1 0, 1Ai+ =−\n()4 hash key 1i=\n1key\n2key\n3key\n4key\n5key\nkeyn…\nv\nkeyi\n(b) Count Sketch Sensing\nFig. 19. The sensing process of and C-sketch and CM-sketch based on the\nlinear hash operation.\nTheorem 1.The goal of frequency estimation based on a\nlinear sketch is equivalent to solving linear equations from the\ngiven keys, hash functions, and counters. Letx∈CNdenote\nthe vector of the streaming key-value sequence andy∈CM\ndenote sketch counters, the insertion process corresponds to\ny=Ax, while the result of recovery phase corresponds to\nx=A†y+ (I−A†A)x,(8)\nwhereA∈CM×Nis an indicator matrix of mapping the\nvectorxto a buckets arrayy, andA†∈CN×Msatisfies\nAA†A≡A.\nProof.Suppose that the hashing operation randomly maps\nincoming items to a bucket array uniformly at random. For\nan incoming key-value pair, the sketch selects one counter\nindexed by hashing the key with a hash function at each array.\nLetA[i, j] = 1or−1if thej-th key is mapped to thei-th\nbucket, and set other entries in this row vector to0s. Then the\ninsertion process for all key-value pairs can be equivalently\nrepresented as an algebraic equationy=Ax. For example,\nwe present the mapping matrices for CM-sketch and C-sketch\nin Fig. 19. Thus the approximated counters of a sketch can\nbe calculated as a decoding phase: we can obtain the general\nsolution of per-keyx, i.e.,x=A†y+ (I−A†A)t,∀t∈CN,\nwhere the first part is in the range-space ofAwhile the latter\nis in the null-space. One can justify this by left multiplying\neach side of the equation byA. Then\nx=A†Ax+ (I−A†A)t⇔x−t=A†A(x−t)(9)\nSinceA†A̸=I, we thus havet=x, which concludes the\nproof.□\n\nJOURNAL OF L ATEX CLASS FILES 16\nLemma 1.The space complexity of UCL-sketch is\nO\u0010\nmb+e\nεcln1\nδc+bs\u0011\n, and the time complexity of update\noperation isO\u0010\nlog1\nεbδb+ ln1\nδc\u0011\nin the data plane.\nProof.Since the bucket arrays of CM-sketch containw×\ndcounters, the total size of the heavy filter is fixed as\nb×s, and the Bloom Filter occupiesm bbits, the total\nspace complexity in the data plane isO(m b+wd+bs) =\nO\u0010\nmb+e\nεcln1\nδc+bs\u0011\n. For each item in the stream, it first\nrequires 1 hash operation to locate its slot in the heavy filter.\nThus, the time complexity of filtering is onlyO(1). Then the\nsketch hashes the keydtimes, and the Bloom Filter hashes\nthatk btimes. Therefore, the time complexity of insertion is\nO(k b+d+ 1) =O\u0010\nlog1\nεbδb+ ln1\nδc\u0011\n.□\nLemma 2.The keys coverage of the Bloom Filter in UCL-\nsketch obeys\nPr (Y≥K y)≤K\nKy\u0012\n1−e−kbK\nmb\u0013\n,(10)\nwhere variableYdenotes the number of keys that are not\ncovered but viewed as covered.\nProof.Suppose that independent hash functions uniformly\nmap keys to random bits, the probability that a certain bit\nwill still be0after one insertion is1−1\nmb. Consequently,\nthe probability that any bit of the Bloom Filter is1afterK i\ndistinct items have been seen is given by1−\u0010\n1−1\nmb\u0011kbKi\n.\nWe can use the identity\u0010\n1−1\nmb\u0011mb=1\nefor large\nmb→ ∞, then we have approximation\u0010\n1−1\nmb\u0011kbKi\n≈\n\u00001\ne\u0001kbKi/mb. Therefore, the false positive probability of an\nunobserved key is1−e−kbKi/mb. Also note thatE(Y) =\nKP\ni=1\u0000\n1−e−kbi/mb\u0001\n≤K\u0000\n1−e−kbK/m b\u0001\n. Now by Markov’s\ninequality, the bound can be derived as:Pr (Y≥K y)≤\nE(Y)\nKy≤K\nKy\u0000\n1−e−kbK/m b\u0001\n, which concludes the proof.□\nLemma 3.Given disjoint subsetsT a, Tb⊆ {1,2,3, . . .}\nwith|T a|,|Tb|=s, andxan arbitrary vector can be supported\non them, we have⟨Ax Ta, Ax Tb⟩ ≤σ 2s∥xTa∥2∥xTb∥2.\nProof.The core idea of the proof is Parallelogram Identity.\nFirst, without loss of generality, we may assume thatxandx′\nareunitvectors with disjoint support sets as described above.\nThen, by Definition 3, we have\n2(1−σ s+s′)≤ ∥Ax±Ax′∥2\n2≤2(1 +σ s+s′)\nThe parallelogram identity states that:\n|⟨Ax,Ax′⟩|=1\n4∥Ax+Ax′∥2\n2− ∥Ax−Ax′∥2\n2≤σs+s′\nThe vectorsxandx′can be replaced byx Taandx Tb,\nrespectively, yielding\n⟨Ax Ta,Ax Tb⟩ ≤ |⟨Ax Ta,Ax Tb⟩| ≤σ 2s∥xTa∥2∥xTb∥2\nThis completes the proof of Lemma 3.□\nTheorem 2.Letf= (f(1), f(2), . . . , f(n))be the real\nvolume vector of a stream that is stored in the sketch, where\nf(i)denotes the volume ofi-th distinct item. ConsiderT 0\nas the locations of theslargest volume off. Assume thatthe reported volume vectorf∗is the optimal solution that\nminimizes the first two objective in Eqn. 3. Then the worst-\ncase frequency estimation error is bounded by\n∥f∗−f∥1≤2 +\u0000\n2√\n2 + 2\u0001\nσ2s\n1−\u0000√\n2 + 1\u0001\nσ2s\r\rfTc\n0\r\r\n1(11)\nProof.Givenf=f∗+l, we start by dividing the residual\nvectorl: letT 1denote the locations of the largestsvalue\ninlTc\n0,T2denote the locations of the next largestsvalue in\nlTc\n0, and so on. By this definition, we can obtain\r\rlTj\r\r\n2=rP\nil2\nTj(i)≤q\nsmax\ni2\u0000\f\flTj(i)\f\f\u0001\n≤1√sP\ni\f\flTj−1(i)\f\f=\n1√s\r\rlTj−1\r\r\n1. This holds forj≥2, which derives the fol-\nlowing useful inequality:\nX\nj≥2\r\rlTj\r\r\n2≤1√sX\nj≥1\r\rlTj\r\r\n1=1√s\r\rlTc\n0\r\r\n1(12)\nSincef∗minimize∥f∗∥1subject toAf∗=Af, which\nintuitively means∥f∗∥1would not bigger than∥f∥1, we have\n∥f∥1≥ ∥f∗∥1and∥Af∗−Af∥2=∥Al∥2≈0. It gives\n∥fT0∥1+\r\rfTc\n0\r\r\n1=∥f∥1\n≥ ∥f∗∥1=∥f−l∥1=\r\r(f−l)T0\r\r\n1+\r\r\r(f−l)Tc\n0\r\r\r\n1\n≥ ∥f T0∥1−\r\rfTc\n0\r\r\n1+\r\rlTc\n0\r\r\n1− ∥l T0∥1\n⇔\r\rlTc\n0\r\r\n1≤2\r\rfTc\n0\r\r\n1+∥l T0∥1.(13)\nAlso note that∥l T0∥1≤√s∥lT0∥2≤√s∥lT0∪1∥2, which is\nis derived by\n1√s∥lT0∥1=vuut X\ni1√s|lT0(i)|!2\n≤vuuut\nsX\nj=11\ns\nX\nil2\nT0(i) =∥l T0∥2(14)\nusing Cauchy–Schwarz inequality. And following from Defi-\nnition 1, one can get(1−σ 2s)∥lT0∪1∥2\n2≤ ∥Al T0∪1∥2\n2. There-\nfore, we instead bound∥Al T0∪1∥2\n2as follows:\n∥AlT0∪1∥2\n2=⟨Al T0∪1, AlT0∪1⟩=\nAlT0∪1, A\u0000\nl−l Tc\n0∪1\u0001\u000b\n≤ |⟨Al T0∪1, Al⟩|+\f\f\nAlT0∪1, AlTc\n0∪1\u000b\f\f\n≤ ∥Al T0∪1∥2∥Al∥2+X\nj≥2\f\f\nAlT0∪1, AlTj\u000b\f\f\n≈0 +X\nj≥2\f\f\nAlT0+Al T1, AlTj\u000b\f\f\n≤X\nj≥2\f\f\nAlT0, AlTj\u000b\f\f+X\nj≥2\f\f\nAlT1, AlTj\u000b\f\f.\n(15)\nHere using the Lemma 3, we have\nX\nj≥2\f\f\nAlT0, AlTj\u000b\f\f≤σ2s∥lT0∥2X\nj≥2\r\rlTj\r\r\n2(16)\n\nJOURNAL OF L ATEX CLASS FILES 17\nThe proof for the upper bound ofP\nj≥2\f\f\nAlT1, AlTj\u000b\f\ffollows\nfrom the similar procedure, and thus applying Ineqn. 12,\n∥AlT0∪1∥2\n2≤σ2s(∥lT0∥2+∥l T1∥2)X\nj≥2\r\rlTj\r\r\n2\n≤√\n2σ2s∥lT0∪1∥2X\nj≥2\r\rlTj\r\r\n2≤r\n2\nsσ2s∥lT0∪1∥2\r\rlTc\n0\r\r\n1\n(17)\nwhere the first part of the second line is derived by\n∥lT0∥2+∥l T1∥2=q\n∥lT0∥2\n2+∥l T1∥2\n2+ 2∥l T0∥2∥lT1∥2\n≤r\n2\u0010\n∥lT0∥2\n2+∥l T1∥2\n2\u0011\n=√\n2∥lT0∪1∥2\n(18)\nThen recall that\n∥lT0∪1∥2\n2≤1\n1−σ 2s∥AlT0∪1∥2\n2≤p\n2/sσ 2s\n1−σ 2s∥lT0∪1∥2\r\rlTc\n0\r\r\n1\n⇔ ∥l T0∪1∥2≤p\n2/sσ 2s\n1−σ 2s\r\rlTc\n0\r\r\n1\n(19)\nwhich gives∥l T0∥1≤√s∥lT0∪1∥2≤√\n2σ2s\n1−σ 2s\r\rlTc\n0\r\r\n1. Now we\ncombine it with Ineqn. 13 to obtain the certain bound\n∥lT0∥1≤√\n2σ2s\n1−σ 2s\r\rlTc\n0\r\r\n1≤√\n2σ2s\n1−σ 2s\u0010\n2\r\rfTc\n0\r\r\n1+∥l T0∥1\u0011\n⇔ ∥l T0∥1≤2√\n2σ2s\n1−\u0000√\n2 + 1\u0001\nσ2s\r\rfTc\n0\r\r\n1.\n(20)\nFinally, the error bound of reported volumes is given by\n∥f∗−f∥1=∥l∥1=\r\rlTc\n0\r\r\n1+∥l T0∥1\n≤2\r\rfTc\n0\r\r\n1+ 2∥l T0∥1≤2 +\u0000\n2√\n2 + 2\u0001\nσ2s\n1−\u0000√\n2 + 1\u0001\nσ2s\r\rfTc\n0\r\r\n1,(21)\nwhich concludes the proof.□\nTheorem 3.A necessary condition for recovering the true\nvolume from compressed counters is that the following linear\nsystem has a unique solution:\nBx=\nA\nATp1\n...\nATp|P|\nx=\ny\ny1\n...\ny|P|\n,(22)\nwherey(·)is the measurement corresponding to the transfor-\nmationp (·). Rigorously, rank(B) =n.\nProof.Recall that the general form of the true frequency\nisx=A†y+ (I−A†A)xas Theorem 1 shows, thus for\np1, p2, . . . , p |P|∈ P,\nx=A†y+ (I−A†A)x\nx=\u0000\n(AT p1)†y1+ (I−(AT p1)†(AT p1)\u0001\nx\n.........\nx=\u0010\n(AT p|P|)†y|P|+ (I−(AT p|P|)†(AT p|P|)\u0011\nx.(23)\nStacking all these equations together into0 =\nA†y\n(AT p1)†y1\n...\n(AT p|P|)†y|P|\n−\nA†A\n(AT p1)†ATp1\n...\n(AT p|P|)†ATp|P|\nx(24)\nBy left multiplying each side of Eqn. 24 by\u0000\nA,AT p1,···,AT p|P|\u0001\n, we have that\nBx=\nA\nATp1\n...\nATp|P|\nx=\ny\ny1\n...\ny|P|\n,(25)\nand thereforeB∈R|P|m×nneeds to be of full ranknso\nthat the true frequencyxcan be accurately recovered from the\nnull space, which concludes the proof.□\nTheorem 4.Assume that the prior of eachk i-associated\nfrequency in per-key GT vectorxsatisfiesPr (x(i) =a) =\nN\u0000\na;µi, σ2\ni\u0001\n, then for∀k iand∥x∥1→ ∞, through asymp-\ntotic approximation, its posterior estimate based on sketch\ncountersyhas the following general form:\nˆx(i) =µi∥x∥2\n2+σ2\ni\u0010\nwPd\nj=1yj×H j(ki)−d∥x∥1\u0011\n∥x∥2\n2+σ2\nid(w−1)(26)\nwhereH jis the indep. hash function:Ω→ {1, . . . , w}.\nProof.We first introduce indicator variablesI j,ki,kfwhich\nare 1 if(k i̸=kf)∧(H j(ki) =H j(kf))and 0 otherwise.\nThen we have\nyj×H j(ki)=x(i) +X\nkf∈ΩIj,ki,kfx(f)(27)\nApparently,P\nkf∈ΩIj,ki,kfx(f)is a Gaussian random vari-\nable due to linear properties of the Gaussian distribution,\nand assumingx(f)∈Ωare independent and identically\ndistributed, we can get the following approximation\n\n\nE\u0000\nIj,ki,kfx(f)\u0001\n=x(f)\nw,\nV ar\u0000\nIj,ki,kfx(f)\u0001\n=w−1\nw2x2(f)(28)\nSo\nX\nkf∈ΩIj,ki,kfx(f)∼ N\u0012∥x∥1−x(i)\nw,w−1\nw2\u0000\n∥x∥2\n2−x2(i)\u0001\u0013\n(29)\nNow conditioned on the eventx(i) =a,\nyj×H j(ki)∼ N\u0012\na+∥x∥1−a\nw,w−1\nw2\u0000\n∥x∥2\n2−a2\u0001\u0013\n,(30)\nwhich also means\nPr\u0000\nyj×H j(ki)=vj,ki|x(i) =a\u0001\n=N\u0012\nyj×H j(ki);a+∥x∥1−a\nw,w−1\nw2\u0000\n∥x∥2\n2−a2\u0001\u0013(31)\n\nJOURNAL OF L ATEX CLASS FILES 18\nRecalling the problem definition section in the main text,\nthe sketching goalmax\nxlogp(x|y),s.t. y=Axcan also be\nreformulated in the form of MAP (Maximum A Posteriori):\nmax\nadX\nj=1log\u0000\nPr\u0000\nyj×H j(ki)=vj,ki|x(i) =a\u0001\u0001\n+ log (Pr (x i=a))(32)\nBy combining the prior and Eqn. 31, we can write\nˆx(i) = max\nadX\nj=1\u0012\n−1\n2log\u0010\n∥x∥2\n2−a2\u0011\u0013\n−(a−µ i)2\n2σ2\ni\n+dX\nj=1 \n−\u0000\nw(yj×H j−a)−(∥x∥1−a)\u00012\n2(w−1)(∥x∥2\n2−a2)!\n(33)\nNotice thata2≪ ∥x∥2\n2especially when∥x∥ 1→ ∞,∥x∥2\n2−\na2is thus dominated by∥x∥2\n2. Then by setting the term to\nthe constant and ignoring it, we have the following surrogate\napproximation result:\nˆx(i)≈max\naw2\n2(w−1)\u0010\n∥x∥2\n2−a2\u0011dX\nj=1\u0012\ny−a−∥x∥1−a\nw\u0013\n+w2\n2(w−1)\u0010\n∥x∥2\n2−a2\u0011·(w−1)(a−µ i)2\u0010\n∥x∥2\n2−a2\u0011\nw2σ2\ni\n(34)\nIt is also reasonable to drop the relatively constant coef-\nficientw2/2(w−1)\u0010\n∥x∥2\n2−a2\u0011\n, the formula can be thus\nfurther simplified to:\nˆx(i)≈max\na(w−1)(a−µ i)2\u0010\n∥x∥2\n2−a2\u0011\nw2σ2\ni\n+dX\nj=1\u0012\ny−a−∥x∥1−a\nw\u0013 (35)\nFinally, we take its derivative with respect toaand set it to\n0, then the maximum is attained when\nˆx(i)≈µi∥x∥2\n2+σ2\ni\u0010\nwPd\nj=1yj×H j(ki)−d∥x∥1\u0011\n∥x∥2\n2+σ2\nid(w−1)(36)\n□\nB. Model Details\nThe details of our neural network (NN) architecture are\nillustrated in Fig. 20. To recover an estimation bucket of\nlengthl, the size of the NN input measurements isd×w.\nThe measurement vector is first transformed to the shape\n(d, w). After projection by a linear block, each containing\ntwo fully connected layers (FCs) using ReLU Nonlinear, the\nsize of the tensor returns tod×h. Then, it will be fed\ninto the second linear block. We adopt this design with the\nfollowing motivations: (a) Considering the need to reduce the\ncomputational cost, the total number of parameters is several\ntimes less than that of its counterpart without transformations.\nLinear Block\n() ,b d w\n() ,,b d w\n() ,,b d h\n() ,b d hLinear BlockReshape\nLinear Block\nInstance \nNorm\nScale & ShiftEmbedding \nBlockBucket ID iDuring Inferencei = [1, 2,  3, …]During Training\nInput Counters\nN\n+\nReshape\nFully Connected Layer\nSigmoid \n\nEstimation Bucket\n(),blAdditional \nExtraction \nLayersFully Connected Layer\nReLU\nFully Connected Layer\nReLU\nEmbedding Block\nSinusoidol Embedding\nSiLU\nFully Connected Layer\nscale\n shiftNumber of rows in sketch\nNumber of columes in sketch\nHidden dimmension\nd\nw\nh\nLength of the bucket\nlFig. 20. Neural network architecture of the learned solver used in UCL-sketch.\n(b) Motivated by the studies in sketching algorithms, hash\n(row) independence should have a certain regularization effect\non the estimation process. Next, we integrate an embedding\nblock to learn the bucket offset with information in the\nsketch. For the bucket ID, we employ Sinusoidal embedding\nto represent eachias ah-dimensional vector, and then apply\none fully connected layer after activating by SiLU Nonlinear.\niis injected into the network withscale ix+shift i, wherexis\nthe shallow representation of our sampled counters. The fused\nfeature is then extracted by a series of layers, e.g. FCs. Finally,\nfeatures are projected back to the ground-truth domain with the\nSigmoid function, as the normalized frequency is guaranteed\nto be≤1.\nC. Implementation Details\nOur experiments run in a machine with one AMD 6-\nCore CPU (3.70 GHz), 32GB DRAM, and a single 12GB\nNVIDIA GeForce RTX 3060 GPU. For the learning-driven\npart, we used the PyTorch implementation. Besides, all these\nexperiments are repeated multiple times using different fixed\nrandom seeds, and then their average results are reported in\nthis paper.\nParameters of Baselines.The key details have already been\npresented at the beginning of the experimental section in the\nmain text. Here, we list the parameters of each baseline method\non the Retail dataset in detail in Table. VII, and the other\ndatasets follow similarly. We omit SeqSketch here, as its data\nstructure setup is essentially identical to ours.\nParameters of UCL-sketch.The parameters of the local\nsketch and hash table for each memory setting on Retail\ndataset are listed in Table VIII. As for the Bloom Filter,\nwe determine the maximum number of bits by setting the\ncoverage proportion over 99%according to Lemma 2 and\n\nJOURNAL OF L ATEX CLASS FILES 19\nTABLE VII\nKEY PARAMETERS OF BASELINES(#OF HASH FUNC.IS FIXED TO4)\nMemory Usage 16KB 32KB 48KB 64KB 80KB 96KB 112KB 128KB\nCount-Min Sketch (CM)\nWidth of sketch 1024 2048 3072 4096 5120 6144 7168 8192\nCount Sketch (CS)\nWidth of sketch 1024 2048 3072 4096 5120 6144 7168 8192\nLearned Count-Min Sketch (LCM)\n# of heavy buckets 512 1536 2048 3072 3584 4608 5632 6144\nWidth of sketch 512 1024 2048 2048 3096 3096 4096 4096\nLearned Count Sketch (LCS)\n# of heavy buckets 512 1536 2048 3072 3584 4608 5632 6144\nWidth of sketch 512 1024 2048 2048 3096 3096 4096 4096\nElastic Sketch (ES)\n# of buckets 50 50 50 75 70 90 92 100\n# of entries 20 38 30 50 50 64 65 75\nWidth of sketch 512 1024 2048 2048 3096 3096 4096 4096\nNitrosketch (NS)\nWidth of sketch 1024 2048 3072 4096 5120 6144 7168 8192\nUnivMon (UM)\nWidth of sketch 1024 1248 1536 2972 3560 4096 4580 5120\nPR-sketch\nWidth of sketch 900 1800 2700 3600 4500 5400 6300 7200\nFlowRadar\nWidth of sketch 810 1620 2430 3240 4060 4870 5680 6490\nfixingk bas 8. Regarding other datasets, when the space is\nsmall, the proportion of the Bloom Filter is larger (over 50%),\nbut there is a memory limit (for example, lower than 10KB\nfor Kosarak and Retail dataset). As the space becomes more\nabundant, we gradually increase the allocation to the hash\ntable. We provide detailed parameters for the learned solver of\nthis work in Table IX. We adopt the same key hyperparameter\nof neural network throughout the experiments. In particular,λ\nis the hyperparameter that reweights the sparse term in Eqn. 3.\nFor optimization, we use Adam optimizer [46] with default\n(β1, β2)for all the experiments.\nTABLE VIII\nPARAMETER CONFIGURATIONS OF THE LOCAL SKETCH AND HASH TABLE\nMemory Usage 16KB 32KB 48KB 64KB 80KB 96KB 112KB 128KB\n# of slots in HF 500 1500 2000 3000 3500 4500 5500 6000\nDepth of sketch 4 4 4 4 6 6 6 8\nWidth of sketch 512 512 1024 1024 1024 1024 1024 1024\nTABLE IX\nHYPERPARAMETERS SETTING AND OVERHEAD FOR LEARNING PARTS\nHyperparameter Setting value Refer range\nBucket length 512 [128, 256, 512, 1024, 2056]\nHidden dimension 128 [32, 64, 128, 256, 512]\nTrade-offλ0.1 0.05∼1\nTraining epoch 300 100∼500\nPatience 30 10∼50\nLearning rate 0.001 0.0001∼0.01\nBatch size 32 [8, 16, 32, 64, 128]\nSliding window length 128 [32, 64, 128, 256, 512]\nSampling interval 1000 [500, 1000, 2000]\nTraining time (per epoch) 0.2s\nInference time 0.3s∼0.5s\nTrainable parameters 0.75MB∼1.25MBData Normalization.Since there is no predefined end to\na stream, meaning that reliable statistics (mean, variance,\nand maximum) do not exist, the data normalization in our\nimplementation is operated on each individual sample in a\nbatch separately. Now recalling the sketch update procedure,\nwhen a streaming item arrives, its volume is added to one\ncounter in each row, where the counter is determined by\nhj,1≤j≤d. Therefore, counters in our sketch have\nthe following guarantee: any inserted frequency should≤\nscale := min\n1≤j≤dmax\n1≤i≤wsketch count [j, i]. All we need is\nto find the minimum of the maximum counts from all the\nrows, which can be done in linear time. Then we calculate\nthe instance-normalized measurementy′=y\nscale, and the\ninverse transformation for final estimations can be written as\nx= scale×x θ, in whichx θis the output of the learned solver.\nNormalization Denormalization \nMergeAccumulate\ni\niyyscale=\niix scale x=Counters\nyFrequencies\nx1\n2\n31\n2\n3 Stable Input & Output\nFig. 21. Our normalization mechanism enables the solver to maintain stable\nreconstruction under natural changes.\nD. Linear Sketch Case Study\nIn this section, we present several plug-and-play case\nstudies of our proposed framework, demonstrating that it is\nnot restricted to any specific base sketch design. Recall the\ndefinition of a linear sketch, we consider the following five\nrepresentative instances: 1Count-Min Sketch[23]: each\nupdate adds a positive increment (+∆) to fixed counters.\n2Count Sketch[24]: updates can be either positive or\nnegative (±∆), and counter entries may take negative values.\n3Augmented Sketch[4]: the data stream first enters the\nheavy part, and evicted flows are then inserted into a Count-\nMin sketch. 4FlowRadar[43]: similar to Count-Min but\nequipped with a Bloom Filter; the sketch width is fixed to\none. 5Elastic Sketch[27]: similar to Augmented Sketch,\nbut the heavy part is organized as a multi-layer structure.\nFor both training and inference, we only replace the sketch\nmatrixA(of the light part), without modifying the solver\narchitecture or learning pipeline. Next, the modifications we\nfocus on are solely structural optimizations of the data plane,\nas heavy part and hash style can be different. Taking Count\nSketch as an example, we only need to add an additional sign\nhash (for constructingAand recording) on top of the original\nUCL-sketch. Table X summarizes the implementation effort,\nmeasured in terms of additional lines of code (LoC), when\nadapting UCL-sketch to different linear sketches. From the\ntable, adapting UCL-sketch to various linear sketches incurs\nnegligible implementation effort: all variants require fewer\nthan 20 additional lines of code (due to the complex heavy part\n\nJOURNAL OF L ATEX CLASS FILES 20\nTABLE X\nIMPLEMENTATION COMPLEXITY ACROSS DIFFERENT LINEAR SKETCHES.\nLinear Sketch Add. LoC Recording Mod. Training Mod.\nCount-Min Sketch 0 No No\nCount Sketch 2∼5 Minor (hash) Minor (hash)\nAugmented Sketch 5∼10 Minor (HF) No\nFlowRadar 2∼5 Minor (arrays) No\nElastic Sketch 10∼20 Minor (HF) No\nof Elastic Sketch), and none require changes to the training\n(or/and testing) process. These results confirm the plug-and-\nplay nature of our framework—once trained, the learned solver\ncan be seamlessly reused across different sketch instances with\nonly lightweight modifications to the recording component.\nE. Robustness to Sketch Changes\nIn this part, we discuss the impact of sketch changes on\nour strategy by considering three different scenarios. Here,\none measurement period of a sketch is referred to as a cycle.\n1) Natural change between (or during) cycles:First, natu-\nral changes in the magnitude of frequencies (e.g., those caused\nby accumulating data over time or merging two sketches with\nidentical configurations) do not affect the final estimation\nof UCL-sketch after de-normalization. The robustness of the\nlearned solver to instance-level normalization stems from the\nfact that the values within the counters are always scaled\nproportionally. From Fig. 21, this invariance ensures that\nthe relative relationships among frequency estimates are pre-\nserved, rendering the normalization transparent to the sketch.\n2) Sudden change during current cycle:Second, consider\nthe scenario where critical components—such as hash func-\ntions—are modified mid-operation. In classical sketches (e.g.,\nCount-Min), this immediately invalidates all prior aggrega-\ntions, as the key-to-counter mapping changes fundamentally.\nThe entire sketch state thus becomes inconsistent and effec-\ntively unusable without a full reset. In contrast, our framework\naccommodates such transitions gracefully. As illustrated in\nFig. 22, we can retain the original sketch parameters (tagging\npre- and post-change data accordingly) and append new soft\nconstraints for the updated structure. This is equivalent to\nexpanding the sensing matrix with additional columns—an op-\neration naturally supported by our online learning mechanism.\nTwo temporal regimes emerge: (i) Shortly after the change\n– UCL-sketch continues to yield valid frequency estimates\nfor items inserted before the change. New items—particularly\nthose filtered via the hash table—have negligible impact on\nper-key estimation accuracy [15]. (ii) Long after the change\n– The solver fully adapts to the new hash functions, de-\nlivering accurate estimates for all items. The transition is\nrapid, typically completing within 5 seconds (see the related\nexperiment in the main text). By preserving history rather than\ndiscarding it, our approach offers markedly greater robustness\nthan conventional methods.\n3) Reset-induced change at cycle onset:Third, changes to\nsketch parameters are typically planned—for example, during\nsystem reconfiguration or shifts in traffic regimes. If a full\nreset is necessary, retraining is indeed required; however, the\nConcat\nConcatSketching Matrix\n(before change)Sketching Matrix\n(after change)y\n(current) y\n(before) y\n(after) x\n(before) x\n(after) \nLearned SolverCase 1: Short Time after Sketch Parameter Changes\nThe output of learned solver can ensure the reconstructed x (before) intact, while \nthe x (after) is small  (even z eroing it out would not materially affect the outcome ). \nTherefore,  the UCL -sketch is more robust than classical sketch (e.g., CM -sketch).\nCase 2: Long Time after Sketch Parameter Changes\nThe UCL -sketch gradually gets back on track through online learning, eventually \nachieving accurate estimation of all frequencies.y xReset the Bloom filterFig. 22. The overall framework under changes to the sketch during execution.\ncost is minimal thanks to our model’s lightweight architecture\n(see related experiment in the main text). During this brief\nretraining period (typically 1–5 seconds), the system can\nsafely revert to classical estimation methods, such as standard\nCount-Min decoding or linear reconstruction. This fallback\nis particularly effective immediately after a reset, when the\nnumber of active flows (|F| 1) is small, making the system\nless underdetermined. In such cases, classical methods can\nremain reasonably accurate, with estimation error bounded by\nεc|F|1. Whenever they consistently outperform UCL-sketch,\nthe system can seamlessly default to them.\nF . Comparison with Equation-based Sketches\nWe conduct additional analysis comparing UCL-sketch with\nthree other sketching algorithms equipped with a Bloom Filter:\nnamely, FlowRadar [43] SeqSketch [19], and PR-sketch [18].\nIn addition to the Bloom filter, FlowRadar maintains a\ncounting table where each cell consists of three fields:\nFlowXOR, FlowCount, and PacketCount. When updating (and\nhashing) a flow count, the PacketCount field is incremented.\nIf a new flow is inserted, its key is additionally XORed\nwith FlowXOR, and FlowCount is incremented by 1. During\ndecoding, the table is traversed to find cells where FlowCount\nequals 1; for these cells, the decoding key is XORed with\nFlowXOR, the PacketCount value is retrieved and then reset to\n0. Note that not all flows can be successfully decoded. For keys\nthat cannot be decoded but are recorded by the Bloom Filter,\nwe use Count-Min to estimate their corresponding frequencies.\nRegarding SeqSketch and PR-sketch, they are based on\nexactly the same principle (i.e., compressive sensing) as ours.\nThe main differences in data structures are that SeqSketch uses\na fractional sensing matrix, while PR-sketch does not employ\na hash table. Moreover, unlike our approach equipped with\na learning-based solver for decoding pre-key frequencies, the\nLEASTSQAREEQATIONSOLVERused in SeqSketch and PR-\nsketch is OMP [47] and LSMR [48], respectively. We imple-\nmented these two iterative optimization algorithms using the\nSciPy library [49]. For a fair comparison, we also implemented\ntheir parallel versions in PyTorch, called SeqSketch (G) and\nPR-sketch (G).\nThe first two columns in Fig. 23 present the performance\nof each method in terms of accuracy. There is a clear ranking\namong UCL-sketch, SeqSketch and PR-sketch on each real-\nworld dataset, where the learning-based algorithms consis-\n\nJOURNAL OF L ATEX CLASS FILES 21\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0100200300400500600/uni00000024/uni00000024/uni00000028\n/uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000035/uni00000044/uni00000047/uni00000044/uni00000055 /uni00000038/uni00000026/uni0000002f/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0100200300/uni00000024/uni00000035/uni00000028\n50 100 150 200 250\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0.51.01.52.0/uni0000003a/uni00000030/uni00000035/uni00000027\n0102030\n0510\n(a) CAIDA Dataset\n25 50 75 100 125\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0100200300/uni00000024/uni00000024/uni00000028\n/uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000035/uni00000044/uni00000047/uni00000044/uni00000055 /uni00000038/uni00000026/uni0000002f/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b\n25 50 75 100 125\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c050100150200/uni00000024/uni00000035/uni00000028\n25 50 75 100 125\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0.51.01.52.0/uni0000003a/uni00000030/uni00000035/uni00000027\n0102030\n2\n0246810\n(b) Kosarak Dataset\n25 50 75 100 125\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0100200300400500/uni00000024/uni00000024/uni00000028\n/uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000035/uni00000044/uni00000047/uni00000044/uni00000055 /uni00000038/uni00000026/uni0000002f/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b\n25 50 75 100 125\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c020406080100120/uni00000024/uni00000035/uni00000028\n25 50 75 100 125\n/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002e/uni00000025/uni0000000c0.51.01.52.0/uni0000003a/uni00000030/uni00000035/uni00000027\n(c) Retail Dataset\nFig. 23. Performance comparison of equation-based sketches.\nTABLE XI\nENTROPY ABSOLUTE ERROR OF EQUATION-BASED SKETCHES\nCAIDA Dataset\nMemory Usage 32KB 64KB 96KB 128KB 160KB 192KB 224KB 256KB\nFlowRadar 2430.44 729.56 560.10 291.34 226.56 184.73 93.94 69.60\nPR-sketch 30.73 46.53 54.08 53.26 69.28 84.98 99.4 126.53\nSeqSketch 33.49 40.49 27.29 34.81 26.49 21.57 14.95 12.06\nUCL-sketch 9.68 7.54 4.51 3.39 3.12 2.82 2.9 1.42\nKosarak Dataset\nMemory Usage 16KB 32KB 48KB 64KB 80KB 96KB 112KB 128KB\nFlowRadar 2908.92 1326.97 741.77 569.50 397.03 207.72 141.95 75.07\nPR-sketch 1290.96 1378.43 1057.47 1822.91 1029.92 1236.26 1567.18 2500.03\nSeqSketch 515.29 380.91 385.68 347.38 39.31 26.47 28.99 56.76\nUCL-sketch 58.24 35.10 21.63 16.85 14.29 13.19 10.79 10.18\nRetail Dataset\nMemory Usage 16KB 32KB 48KB 64KB 80KB 96KB 112KB 128KB\nFlowRadar 4638.81 2339.08 1058.84 631.51 380.87 245.74 159.66 109.21\nPR-sketch 1117.30 644.23 585.01 706.02 1793.55 758.57 1331.14 808.83\nSeqSketch 315.25 168.68 98.35 84.04 92.37 128.70 102.68 111.06\nUCL-sketch 69.16 41.02 27.14 22.20 18.87 15.07 12.56 10.56\ntently yield improvement over traditional optimization-based\nalgorithms. The measurement system is always undetermined,\nand the fact that traditional approaches rely solely on sparsity\nconstraints is the key reason. Also note that the performance\nof PR-sketch is highly unstable in comparison with SeqSketch,\nand even degrades as available memory increases. In addition\nto OMP being a more advanced—and consequently more\ncomputationally expensive—algorithm, we believe that the\nsparsity pattern ofA(where each column only contains\nexactly 4 non-zero entries, independent of the number ofcounters or rows) may be a key factor resulting in this\nphenomenon. Regarding FlowRadar, its accuracy is similar\nto that of traditional sketches and is even worse than the\nCM-sketch presented in the main text. This is because it\nis primarily designed for collaborative measurement across\nmultiple sketches in a network. This performance difference\narises as it allocates extra space for components like Bloom\nFilters and can decode only a very limited number of flows.\nThis phenomenon is most pronounced in the CAIDA dataset,\nhighlighting the superiority of compressive sensing.\nFrom Table XI and the third column of Fig. 23, it can\nbe observed that UCL-sketch exhibits a clear, step-change\nadvantage over other methods in the distribution of estimated\nfrequencies. As previously discussed, FlowRadar consistently\nperforms poorly in global distribution estimation, as it is\ndesigned to accurately decode only a small subset of (large)\nflows. Overall, PR-sketch ranks second to last in terms of\nWMRD and entropy absolute error, and its performance is\nstill unstable across evaluations. Although SeqSketch is com-\nparable to PR-sketch in terms of WMRD, it performs notably\nwell in entropy estimation. However, an obvious gap remains\nbetween SeqSketch and UCL-sketch.\n/uni00000038/uni00000026/uni0000002f/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000035/uni00000044/uni00000047/uni00000044/uni00000055 /uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b020406080100120140160/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000037/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000053/uni00000058/uni00000057/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000053/uni00000056/uni0000000c/uni00000035/uni00000048/uni00000057/uni00000044/uni0000004c/uni0000004f/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057\n/uni00000038/uni00000026/uni0000002f/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000035/uni00000044/uni00000047/uni00000044/uni00000055 /uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b020406080100120140/uni0000002e/uni00000052/uni00000056/uni00000044/uni00000055/uni00000044/uni0000004e/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057\n/uni00000038/uni00000026/uni0000002f/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000035/uni00000044/uni00000047/uni00000044/uni00000055 /uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b010203040/uni00000026/uni00000024/uni0000002c/uni00000027/uni00000024/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057\nFig. 24. Average results on insertion throughput comparison.\n/uni00000038/uni00000026/uni0000002f/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000035/uni00000044/uni00000047/uni00000044/uni00000055 /uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000003/uni0000000b/uni0000002a/uni0000000c/uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000003/uni0000000b/uni0000002a/uni0000000c100101/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000027/uni00000048/uni00000046/uni00000052/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni00000035/uni00000048/uni00000057/uni00000044/uni0000004c/uni0000004f/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057\n/uni00000038/uni00000026/uni0000002f/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000035/uni00000044/uni00000047/uni00000044/uni00000055 /uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000003/uni0000000b/uni0000002a/uni0000000c/uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000003/uni0000000b/uni0000002a/uni0000000c100101/uni0000002e/uni00000052/uni00000056/uni00000044/uni00000055/uni00000044/uni0000004e/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057\n/uni00000038/uni00000026/uni0000002f/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b /uni00000029/uni0000004f/uni00000052/uni0000005a/uni00000035/uni00000044/uni00000047/uni00000044/uni00000055 /uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000033/uni00000035/uni00000010/uni00000056/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000003/uni0000000b/uni0000002a/uni0000000c/uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000036/uni00000048/uni00000054/uni00000036/uni0000004e/uni00000048/uni00000057/uni00000046/uni0000004b/uni00000003/uni0000000b/uni0000002a/uni0000000c100101102103 /uni00000026/uni00000024/uni0000002c/uni00000027/uni00000024/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057\nFig. 25. Average results on decoding speed comparison.\nNext, we present the insertion throughput of these sketching\nalgorithms in Fig. 24. The high processing speed of UCL-\nsketch and SeqSketch comes from the fact that most insert\noperations occur in the hash table. PR-sketch follows, while\nFlowRadar requires additional XOR operations, resulting in\nthe lowest throughput. Moreover, as shown in Fig. 25, we\nmeasure their average decoding time. FlowRadar has the\nfastest decoding speed, as it only performs a few loop checks\non the underlying counters (and quickly exits the loop due\nto decoding failure), which is fundamentally different from\nthe compressed sensing approach used by other methods.\nNevertheless, the inference time of UCL-sketch also takes\nless than 0.5 seconds. Doubling the key space only slightly\nincreases the inference time. In contrast, the decoding speed\nof SeqSketch and PR-sketch is significantly slower compared\n\nJOURNAL OF L ATEX CLASS FILES 22\nto UCL-sketch. This disparity can be attributed to the inherent\ncomputational complexity associated with these traditional\ncompressed sensing techniques. Both OMP and LSMR require\niterative processes to reconstruct the original data from the\nsketch, leading to a substantial increase in processing time.\nEven when parallel acceleration techniques are applied to\nenhance the performance of SeqSketch and PR-sketch, their\ndecoding speeds still lag behind UCL-sketch. UCL-sketch,\nwith its ML-driven solver, thus emerges as a more efficient\nalternative, particularly in environments where high-speed data\nprocessing is essential.",
  "textLength": 134764
}