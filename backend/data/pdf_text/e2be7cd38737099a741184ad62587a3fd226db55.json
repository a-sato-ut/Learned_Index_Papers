{
  "paperId": "e2be7cd38737099a741184ad62587a3fd226db55",
  "title": "Learning Neural PDE Solvers with Convergence Guarantees",
  "pdfPath": "e2be7cd38737099a741184ad62587a3fd226db55.pdf",
  "text": "Published as a conference paper at ICLR 2019\nLEARNING NEURAL PDE S OLVERS WITH CONVER -\nGENCE GUARANTEES\nJun-Ting Hsieh*\nStanford\njunting@stanford.eduShengjia Zhao*\nStanford\nsjzhao@stanford.eduStephan Eismann\nStanford\nseismann@stanford.edu\nLucia Mirabella\nSiemens\nlucia.mirabella@siemens.comStefano Ermon\nStanford\nermon@stanford.edu\nABSTRACT\nPartial differential equations (PDEs) are widely used across the physical and com-\nputational sciences. Decades of research and engineering went into designing\nfast iterative solution methods. Existing solvers are general purpose, but may be\nsub-optimal for speciﬁc classes of problems. In contrast to existing hand-crafted\nsolutions, we propose an approach to learn a fast iterative solver tailored to a\nspeciﬁc domain. We achieve this goal by learning to modify the updates of an\nexisting solver using a deep neural network. Crucially, our approach is proven to\npreserve strong correctness and convergence guarantees. After training on a sin-\ngle geometry, our model generalizes to a wide variety of geometries and boundary\nconditions, and achieves 2-3 times speedup compared to state-of-the-art solvers.\n1 I NTRODUCTION\nPartial differential equations (PDEs) are ubiquitous tools for modeling physical phenomena, such\nas heat, electrostatics, and quantum mechanics. Traditionally, PDEs are solved with hand-crafted\napproaches that iteratively update and improve a candidate solution until convergence. Decades of\nresearch and engineering went into designing update rules with fast convergence properties.\nThe performance of existing solvers varies greatly across application domains, with no method\nuniformly dominating the others. Generic solvers are typically effective, but could be far from\noptimal for speciﬁc domains. In addition, high performing update rules could be too complex to\ndesign by hand. In recent years, we have seen that for many classical problems, complex updates\nlearned from data or experience can out-perform hand-crafted ones. For example, for Markov chain\nMonte Carlo, learned proposal distributions lead to orders of magnitude speedups compared to hand-\ndesigned ones (Song et al., 2017; Levy et al., 2017). Other domains that beneﬁted signiﬁcantly\ninclude learned optimizers (Andrychowicz et al., 2016) and learned data structures (Kraska et al.,\n2018). Our goal is to bring similar beneﬁts to PDE solvers.\nHand-designed solvers are relatively simple to analyze and are guaranteed to be correct in a large\nclass of problems. The main challenge is how to provide the same guarantees with a potentially\nmuch more complex learned solver. To achieve this goal, we build our learned iterator on top of an\nexisting standard iterative solver to inherit its desirable properties. The iterative solver updates the\nsolution at each step, and we learn a parameterized function to modify this update. This function\nclass is chosen so that for any choice of parameters, the ﬁxed point of the original iterator is pre-\nserved. This guarantees correctness, and training can be performed to enhance convergence speed.\nBecause of this design, we only train on a single problem instance; our model correctly generalizes\nto a variety of different geometries and boundary conditions with no observable loss of performance.\nAs a result, our approach provides: (i) theoretical guarantees of convergence to the correct station-\nary solution, (ii) faster convergence than existing solvers, and (iii) generalizes to geometries and\nboundary conditions very different from the ones seen at training time. This is in stark contrast with\nexisting deep learning approaches for PDE solving (Tang et al., 2017; Farimani et al., 2017) that are\nlimited to speciﬁc geometries and boundary conditions, and offer no guarantee of correctness.\n1arXiv:1906.01200v1  [math.NA]  4 Jun 2019\n\nPublished as a conference paper at ICLR 2019\nOur approach applies to any PDE with existing linear iterative solvers. As an example application,\nwe solve the 2D Poisson equations. Our method achieves a 2-3 \u0002speedup on number of multiply-\nadd operations when compared to standard iterative solvers, even on domains that are signiﬁcantly\ndifferent from our training set. Moreover, compared with state-of-the-art solvers implemented in\nFEniCS (Logg et al., 2012), our method achieves faster performance in terms of wall clock CPU\ntime. Our method is also simple as opposed to deeply optimized solvers such as our baseline in\nFEniCS (minimal residual method + algebraic multigrid preconditioner). Finally, since we utilize\nstandard convolutional networks which can be easily parallelized on GPU, our approach leads to an\nadditional 30\u0002speedup when run on GPU.\n2 B ACKGROUND\nIn this section, we give a brief introduction of linear PDEs and iterative solvers. We refer readers to\nLeVeque (2007) for a thorough review.\n2.1 L INEAR PDE S\nLinear PDE solvers ﬁnd functions that satisfy a (possibly inﬁnite) set of linear differential equations.\nMore formally, let F=fu:Rk!Rgbe the space of candidate functions, and A:F!F be\na linear operator; the goal is to ﬁnd a function u2F that satisﬁes a linear equation Au=f,\nwherefis another function Rk!Rgiven by our problem. Many PDEs fall into this framework.\nFor example, heat diffusion satisﬁes r2u=f(Poisson equation), where r2=@2\n@x2\n1+\u0001\u0001\u0001+@2\n@x2\nk\nis the linear Laplace operator; umaps spatial coordinates (e.g. in R3) into its temperature, and f\nmaps spatial coordinates into the heat in/out ﬂow. Solving this equation lets us know the stationary\ntemperature given speciﬁed heat in/out ﬂow.\nUsually the equation Au=fdoes not uniquely determine u. For example, u= constant for\nany constant is a solution to the equation r2u= 0. To ensure a unique solution we provide\nadditional equations, called “boundary conditions”. Several boundary conditions arise very naturally\nin physical problems. A very common one is the Dirichlet boundary condition, where we pick some\nsubsetG\u001aRkand ﬁx the values of the function on Gto some ﬁxed value b,\nu(x) =b(x);for allx2G\nwhere the function bis usually clear from the underlying physical problem. As in previous literature,\nwe refer toGas the geometry of the problem, and bas the boundary value . We refer to the pair (G;b)\nas the boundary condition . In this paper, we only consider linear PDEs and boundary conditions that\nhave unique solutions.\n2.2 F INITE DIFFERENCE METHOD\nMost real-world PDEs do not admit an analytic solution and must be solved numerically. The ﬁrst\nstep is to discretize the solution space FfromRk!RintoDk!R, where Dis a discrete subset\nofR. When the space is compact, it is discretized into an n\u0002n\u0002n\u0001\u0001\u0001(kmany) uniform Cartesian\ngrid with mesh width h. Any function inFis approximated by its value on the nkgrid points. We\ndenote the discretized function as a vector uinRnk. In this paper, we focus on 2D problems ( k= 2),\nbut the strategy applies to any dimension.\nWe discretize all three terms in the equation Au=fand boundary condition (G;b). The PDE\nsolution uis discretized such that ui;j=u(xi;yj)corresponds to the value of uat grid point\n(xi;yj). We can similarly discretize fandb. In linear PDEs, the linear operator Ais a linear com-\nbination of partial derivative operators. For example, for the Poisson equation A=r2=P\ni@2\n@x2\ni.\nTherefore we can ﬁrst discretize each partial derivative, then linearly combine the discretized partial\nderivatives to obtain a discretized A.\nFinite difference is a method that approximates partial derivatives in a discretized space, and as\nmesh width h!0, the approximation approaches the true derivative. For example,@2\n@x2ucan\nbe discretized in 2D as@2\n@x2u\u00191\nh2(ui\u00001;j\u00002ui;j+ui+1;j), the Laplace operator in 2D can be\n2\n\nPublished as a conference paper at ICLR 2019\ncorrespondingly approximated as:\nr2u=@2u\n@x2+@2u\n@y2\u00191\nh2(ui\u00001;j+ui+1;j+ui;j\u00001+ui;j+1\u00004ui;j) (1)\nAfter discretization, we can rewrite Au=fas a linear matrix equation\nAu=f (2)\nwhereu;f2Rn2, andAis a matrix in Rn2\u0002n2(these aren2dimensional because we focus on 2D\nproblems). In many PDEs such as the Poisson and Helmholtz equation, Ais sparse, banded, and\nsymmetric.\n2.3 B OUNDARY CONDITION\nWe also need to include the boundary condition u(x) =b(x)for allx2 G. If a discretized\npoint (xi;yj)belongs toG, we need to ﬁx the value of ui;jtobi;j. To achieve this, we ﬁrst deﬁne\ne2f0;1gn2to be a vector of 0’s and 1’s, in which 0 indicates that the corresponding point belongs\ntoG. Then, we deﬁne a “reset” matrix G=diag(e), a diagonal matrix Rn2!Rn2such that\n(Gu)i;j=\u001aui;j(xi;yj)62G\n0 (xi;yj)2G(3)\nIntuitivelyG”masks” every point in Gto0. Similarly, I\u0000Gcan mask every point not in Gto0.\nNote that the boundary values are ﬁxed and do not need to satisfy Au=f. Thus, the solution uto\nthe PDE under geometry Gshould satisfy:\nG(Au) =Gf\n(I\u0000G)u= (I\u0000G)b(4)\nThe ﬁrst equation ensures that the interior points (points not in G) satisfyAu=f, and the second\nensures that the boundary condition is satisﬁed.\nTo summarize, (A;G;f;b;n)is our PDE problem , and we ﬁrst discretize the problem on an n\u0002n\ngrid to obtain (A;G;f;b;n ). Our objective is to obtain a solution uthat satisﬁes Eq. (4), i.e. Au=f\nfor the interior points and boundary condition ui;j=bi;j;8(xi;yj)2G.\n2.4 I TERATIVE SOLVERS\nAlinear iterative solver is deﬁned as a function that inputs the current proposed solution u2Rn2\nand outputs an updated solution u0. Formally it is a function \t :Rn2!Rn2that can be expressed\nas\nu0= \t(u) =Tu+c (5)\nwhereTis a constant update matrix and cis a constant vector. For each iterator \tthere may be\nspecial vectors u\u00032Rn2that satisfyu\u0003= \t(u\u0003). These vectors are called ﬁxed points.\nThe iterative solver \tshould map any initial u02Rn2to a correct solution of the PDE problem.\nThis is formalized in the following theorem.\nDeﬁnition 1 (Valid Iterator) .An iterator \tis valid w.r.t. a PDE problem (A;G;f;b;n )if it satisﬁes:\na)Convergence: There is a unique ﬁxed point u\u0003such that \tconverges to u\u0003from any\ninitialization:8u02Rn2;limk!1\tk(u0) =u\u0003.\nb)Fixed Point: The ﬁxed point u\u0003is the solution to the linear system Au=funder boundary\ncondition (G;b).\nConvergence: Condition (a) in Deﬁnition 1 is satisﬁed if the matrix Tisconvergent , i.e.Tk!0as\nk!1 . It has been proven that Tis convergent if and only if the spectral radius \u001a(T)<1(Olver,\n2008):\n3\n\nPublished as a conference paper at ICLR 2019\nTheorem 1. (Olver, 2008, Prop 7.25) For a linear iterator \t(u) =Tu+c,\tconverges to a\nunique stable ﬁxed point from any initialization if and only if the spectral radius \u001a(T)<1.\nProof. See Appendix A.\nIt is important to note that Condition (a) only depends on Tand not the constant c.\nFixed Point: Condition (b) in Deﬁnition 1 contains two requirements: satisfy Au=f, and the\nboundary condition (G;b). To satisfyAu=fa standard approach is to design \tby matrix splitting:\nsplit the matrix AintoA=M\u0000N; rewriteAu=fasMu=Nu+f(LeVeque, 2007). This\nnaturally suggests the iterative update\nu0=M\u00001Nu+M\u00001f (6)\nBecause Eq. (6) is a rewrite of Au=f, stationary points u\u0003of Eq. (6) satisfy Au\u0003=f. Clearly,\nthe choices of MandNare arbitrary but crucial. From Theorem 1, we must choose Msuch that\nthe update converges. In addition, M\u00001must easy to compute (e.g., diagonal).\nFinally we also need to satisfy the boundary condition (I\u0000G)u= (I\u0000G)bin Eq.4. After each\nupdate in Eq. (6), the boundary condition could be violated. We use the “reset” operator deﬁned in\nEq. (3) to “reset” the values of ui;jtobi;jbyGu+ (I\u0000G)b.\nThe ﬁnal update rule becomes\nu0=G(M\u00001Nu+M\u00001f) + (I\u0000G)b (7)\nDespite the added complexity, it is still a linear update rule in the form of u0=Tu+cin Eq. (5):\nwe haveT=GM\u00001Nandc=GM\u00001f+ (1\u0000G)b. As long as Mis a full rank diagonal matrix,\nﬁxed points of this equation satisﬁes Eq. (4). In other words, such a ﬁxed point is a solution of the\nPDE problem (A;G;f;b;n ).\nProposition 1. IfMis a full rank diagonal matrix, and u\u00032Rn2\u0002n2satisﬁes Eq. (7), then u\u0003\nsatisﬁes Eq. (4).\n2.4.1 J ACOBI METHOD\nA simple but effective way to choose Mis the Jacobi method, which sets M=I(a full rank\ndiagonal matrix, as required by Proposition 1). For Poisson equations, this update rule has the\nfollowing form,\n^ui;j=1\n4(ui\u00001;j+ui+1;j+ui;j\u00001+ui;j+1) +h2\n4fi;j (8)\nu0=G^u+ (1\u0000G)b (9)\nFor Poisson equations and any geometry G, the update matrix T=G(I\u0000A)has spectral radius\n\u001a(T)<1(see Appendix B). In addition, by Proposition 1 any ﬁxed point of the update rule Eq.(8,9)\nmust satisfy Eq. (4). Both convergence and ﬁxed point conditions from Deﬁnition 1 are satisﬁed:\nJacobi iterator Eq.(8,9) is valid for any Poisson PDE problem.\nIn addition, each step of the Jacobi update can be implemented as a neural network layer, i.e., Eq.\n(8) can be efﬁciently implemented by convolving uwith kernel 0 1=4 0\n1=4 0 1=4\n0 1=4 0!\nand adding\nh2f=4. The “reset” step in Eq. (9) can also be implemented as multiplying uwith G and adding the\nboundary values (1\u0000G)b.\n2.4.2 M ULTIGRID METHOD\nThe Jacobi method has very slow convergence rate (LeVeque, 2007). This is evident from the update\nrule, where the value at each grid point is only inﬂuenced by its immediate neighbors. To propagate\ninformation from one grid point to another, we need as many iterations as their distance on the grid.\nThe key insight of the Multigrid method is to perform Jacobi updates on a downsampled (coarser)\ngrid and then upsample the results. A common structure is the V-cycle (Briggs et al., 2000). In each\n4\n\nPublished as a conference paper at ICLR 2019\nV-cycle, there are kdownsampling layers followed by kupsampling layers, and multiple Jacobi\nupdates are performed at each resolution. The downsampling and upsampling operations are also\ncalled restriction andprolongation , and are often implemented using weighted restriction and linear\ninterpolation respectively. The advantage of the multigrid method is clear: on a downsampled grid\n(by a factor of 2) with mesh width 2h, information propagation is twice as fast, and each iteration\nrequires only 1/4 operations compared to the original grid with mesh width h.\n3 L EARNING FAST AND PROVABLY CORRECT ITERATIVE PDE S OLVERS\nA PDE problem consists of ﬁve components (A;G;f;b;n). One is often interested in solving the\nsame PDE classAunder varying f, discretization n, and boundary conditions (G;b). For example,\nsolving the Poisson equation under different boundary conditions (e.g., corresponding to different\nmechanical systems governed by the same physics). In this paper, we ﬁx Abut varyG;f;b;n, and\nlearn an iterator that solves a class of PDE problems governed by the same A. For a discretized\nPDE problem (A;G;f;b;n )and given a standard (hand designed) iterative solver \t, our goal is to\nimprove upon \tand learn a solver \bthat has (1) correct ﬁxed point and (2) fast convergence (on\naverage) on the class of problems of interest. We will proceed to parameterize a family of \bthat\nsatisﬁes (1) by design, and achieve (2) by optimization.\nIn practice, we can only train \bon a small number of problems (A;fi;Gi;bi;ni). To be useful,\n\bmust deliver good performance on every choice of G;f;b , and different grid sizes n. We show,\ntheoretically and empirically, that our iterator family has good generalization properties: even if\nwe train on a single problem (A;G;f;b;n ), the iterator performs well on very different choices of\nG;f;b , and grid size n. For example, we train our iterator on a 64\u000264square domain, and test on\na256\u0002256L-shaped domain (see Figure 1).\n3.1 F ORMULATION\nFor a ﬁxed PDE problem class A, let\tbe a standard linear iterative solver known to be valid. We\nwill use more formal notation \t(u;G;f;b;n )as\tis a function of u, but also depends on G;f;b;n .\nOur assumption is that for any choice of G;f;b;n (but ﬁxed PDE class A),\t(u;G;f;b;n )is valid.\nWe previously showed that Jacobi iterator Eq.(8,9) have this property for the Poisson PDE class.\nWe design our new family of iterators \bH:Rn2!Rn2as\nw= \t(u;G;f;b;n )\u0000u\n\bH(u;G;f;b;n ) = \t(u;G;f;b;n ) +GHw(10)\nwhereHis a learned linear operator (it satisﬁes H0 = 0 ). The term GHw can be interpreted as\na correction term to \t(u;G;f;b;n ). When there is no confusion, we neglect the dependence on\nG;f;b;n and denote as \t(u)and\bH(u).\n\bHshould have similar computation complexity as \t. Therefore, we choose Hto be a convolutional\noperator, which can be parameterized by a deep linear convolutional network. We will discuss the\nparameterization of Hin detail in Section 3.4; we ﬁrst prove some parameterization independent\nproperties.\nThe correct PDE solution is a ﬁxed point of \bHby the following lemma:\nLemma 1. For any PDE problem (A;G;f;b;n )and choice of H, ifu\u0003is a ﬁxed point of \t, it is a\nﬁxed point of \bHin Eq. (10).\nProof. Based on the iterative rule in Eq. (10), if u\u0003satisﬁes \t(u\u0003) =u\u0003thenw= \t(u\u0003)\u0000u\u0003=0.\nTherefore, \bH(u\u0003) = \t(u\u0003) +GH0=u\u0003.\nMoreover, the space of \bHsubsumes the standard solver \t. IfH= 0, then \bH= \t. Furthermore,\ndenote \t(u) =Tu+c, then ifH=T, then sinceGT=T(see Eq. (7)),\n\bH(u) = \t(u) +GT(\t(u)\u0000u) =T\t(u) +c= \t2(u) (11)\nwhich is equal to two iterations of \t. Computing \trequires one convolution T, while computing\n\bHrequires two convolutions: TandH. Therefore, if we choose H=T, then \bHcomputes two\niterations of \twith two convolutions: it is at least as efﬁcient as the standard solver \t.\n5\n\nPublished as a conference paper at ICLR 2019\n3.2 T RAINING AND GENERALIZATION\nWe train our iterator \bH(u;G;f;b;n )to converge quickly to the ground truth solution on a set\nD=f(Gl;fl;bl;nl)gM\nl=1of problem instances. For each instance, the ground truth solution u\u0003is\nobtained from the existing solver \t. The learning objective is then\nmin\nHX\n(Gl;fl;bl;nl)2DEu0\u0018N (0;1)k\bk\nH(u0;Gl;fl;bl;nl)\u0000u\u0003k2\n2 (12)\nIntuitively, we look for a matrix Hsuch that the corresponding iterator \bHwill get us as close as\npossible to the solution in ksteps, starting from a random initialization u0sampled from a white\nGaussian.kin our experiments is uniformly chosen from [1;20], similar to the procedure in (Song\net al., 2017). Smaller kis easier to learn with less steps to back-propagate through, while larger\nkbetter approximates our test-time setting: we care about the ﬁnal approximation accuracy after a\ngiven number of iteration steps. Combining smaller and larger kperforms best in practice.\nWe show in the following theorem that there is a convex open set of Hthat the learning algorithm\ncan explore. To simplify the statement of the theorem, for any linear iterator \b(u) =Tu+cwe\nwill refer to the spectral radius (norm) of \bas the spectral radius (norm) of T.\nTheorem 2. For ﬁxedG;f;b;n , the spectral norm of \bH(u;G;f;b;n )is a convex function of H,\nand the set of Hsuch that the spectral norm of \bH(u;G;f;b;n )<1is a convex open set.\nProof. See Appendix A.\nTherefore, to ﬁnd an iterator with small spectral norm, the learning algorithm only has to explore\na convex open set. Note that Theorem 2 holds for spectral norm , whereas validity requires small\nspectral radius in Theorem 1. Nonetheless, several important PDE problems (Poisson, Helmholtz,\netc) are symmetric, so it is natural to use a symmetric iterator, which means that spectral norm is\nequal to spectral radius. In our experiments, we do not explicitly enforce symmetry, but we observe\nthat the optimization ﬁnds symmetric iterators automatically.\nFor training, we use a single grid size n, a single geometry G,f= 0, and a restricted set of boundary\nconditionsb. The geometry we use is a square domain shown in Figure 1a. Although we train on a\nsingle domain, the model has surprising generalization properties, which we show in the following:\nProposition 2. For ﬁxedA;G;n and ﬁxedH, if for some f0;b0,\bH(u;G;f 0;b0;n)is valid for the\nPDE problem (A;G;f 0;b0;n), then for all fandb, the iterator \bH(u;G;f;b;n )is valid for the\nPDE problem (A;G;f;b;n ).\nProof. See Appendix A.\nThe proposition states that we freely generalize to different fandb. There is no guarantee that\nwe can generalize to different Gandn. Generalization to different Gandnhas to be empirically\nveriﬁed: in our experiments, our learned iterator converges to the correct solution for a variety of\ngrid sizesnand geometries G, even though it was only trained on one grid size and geometry.\nEven when generalization fails, there is no risk of obtaining incorrect results. The iterator will\nsimply fail to converge. This is because according to Lemma 1, ﬁxed points of our new iterator is\nthe same as the ﬁxed point of hand designed iterator \t. Therefore if our iterator is convergent, it is\nvalid.\n3.3 I NTERPRETATION OF H\nWhat isHtrying to approximate? In this section we show that we are training our linear function\nGH to approximate T(I\u0000T)\u00001: if it were able to approximate T(I\u0000T)\u00001perfectly, our iterator\n\bHwill converge to the correct solution in a single iteration.\nLet the original update rule be \t(u) =Tu+c, and the unknown ground truth solution be u\u0003\nsatisfyingu\u0003=Tu\u0003+c. Letr=u\u0003\u0000ube the current error, and e=u\u0003\u0000\t(u)be the new error\nafter applying one step of \t. They are related by\ne=u\u0003\u0000\t(u) =u\u0003\u0000(Tu+c) =T(u\u0003\u0000u) =Tr (13)\n6\n\nPublished as a conference paper at ICLR 2019\nIn addition, let w= \t(u)\u0000ube the update \tmakes. This is related to the current error rby\nw= \t(u)\u0000u=Tu+c\u0000u+ (u\u0003\u0000Tu\u0003\u0000c) =T(u\u0000u\u0003) + (u\u0003\u0000u) = (I\u0000T)r(14)\nFrom Eq. (10) we can observe that the linear operator GH takes as input \t’s updatew, and tries\nto approximate the error e:GHw\u0019e. If the approximation were perfect: GHw =e, the iterator\n\bHwould converge in a single iteration. Therefore, we are trying to ﬁnd some linear operator R,\nsuch thatRw=e. In fact, if we combine Eq. (13) and Eq. (14), we can observe that T(I\u0000T)\u00001is\n(uniquely) the linear operator we are looking for\nT(I\u0000T)\u00001w=e (15)\nwhere (I\u0000T)\u00001exists because \u001a(T)<1, so all eigenvalues of I\u0000Tmust be strictly positive.\nTherefore, we would like our linear function GH to approximate T(I\u0000T)\u00001.\nNote that (I\u0000T)\u00001is a dense matrix in general, meaning that it is impossible to exactly achieve\nGH=T(I\u0000T)\u00001with a convolutional operator H. However, the better GH is able to approximate\nT(I\u0000T)\u00001, the faster our iterator converges to the solution u\u0003.\n3.4 L INEAR DEEPNETWORKS\nIn our iterator design, His a linear function parameterized by a linear deep network without non-\nlinearity or bias terms. Even though our objective in Eq. (12) is a non-linear function of the param-\neters of the deep network, this is not an issue in practice. In particular, Arora et al. (2018) observes\nthat when modeling linear functions, deep networks can be faster to optimize with gradient descent\ncompared to linear ones, despite non-convexity.\nEven though a linear deep network can only represent a linear function, it has several advantages. On\nann\u0002ngrid, each convolution layer only requires O(n2)computation and have a constant number\nof parameters, while a general linear function requires O(n4)computation and have O(n4)param-\neters. Stacking dconvolution layers allows us to parameterize complex linear functions with large\nreceptive ﬁelds, while only requiring O(dn2)computation and O(d)parameters. We experiment on\ntwo types of linear deep networks:\nConv model. We modelHas a network with 3\u00023convolutional layers without non-linearity or\nbias. We will refer to a model with klayers as “Conv k”, e.g. Conv3 has 3 convolutional layers.\nU-Net model. The Conv models suffer from the same problem as Jacobi: the receptive ﬁeld grows\nonly by 1 for each additional layer. To resolve this problem, we design the deep network counter-\npart of the Multigrid method. Instead of manually designing the sub-sampling / super-sampling\nfunctions, we use a U-Net architecture (Ronneberger et al., 2015) to learn them from data. Because\neach layer reduces the grid size by half, and the i-th layer of the U-Net only operates on (2\u0000in)-sized\ngrids, the total computation is only increased by a factor of\n1 + 1=4 + 1=16 +\u0001\u0001\u0001<4=3\ncompared to a two-layer convolution. The minimal overhead provides a very large improvement\nof convergence speed in our experiments. We will refer to Multigrid and U-Net models with k\nsub-sampling layers as Multigrid kand U-Netk, e.g. U-Net2 is a model with 2 sub-sampling layers.\n4 E XPERIMENTS\n4.1 S ETTING\nWe evaluate our method on the 2D Poisson equation with Dirichlet boundary conditions, r2u=\nf. There exist several iterative solvers for the Poisson equation, including Jacobi, Gauss-Seidel,\nconjugate-gradient, and multigrid methods. We select the Jacobi method as our standard solver \t.\nTo reemphasize, our goal is to train a model on simple domains where the ground truth solutions\ncan be easily obtained, and then evaluate its performance on different geometries and boundary\nconditions. Therefore, for training, we select the simplest Laplace equation, r2u= 0, on a square\ndomain with boundary conditions such that each side is a random ﬁxed value. Figure 1a shows an\n7\n\nPublished as a conference paper at ICLR 2019\n(a) Square domain.\n (b) L-shape domain.\n (c) Cylinders domain.\n(d) Poisson equation\nin the square domain.\nFigure 1: The ground truth solutions of examples in different settings. We only train our models on\nthe square domain, and we test on all 4 settings.\nexample of our training domain and its ground truth solution. This setting is also used in Farimani\net al. (2017) and Sharma et al. (2018).\nFor testing, we use larger grid sizes than training. For example, we test on 256\u0002256grid for a model\ntrained on 64\u000264grids. Moreover, we designed challenging geometries to test the generalization\nof our models. We test generalization on 4 different settings: (i) same geometry but larger grid,\n(ii) L-shape geometry, (iii) Cylinders geometry, and (iv) Poisson equation in same geometry, but\nf6= 0. The two geometries are designed because the models were trained on square domains and\nhave never seen sharp or curved boundaries. Examples of the 4 settings are shown in Figure 1.\n4.2 E VALUATION\nAs discussed in Section 2.4, the convergence rate of any linear iterator can be determined from the\nspectral radius \u001a(T), which provides guarantees on convergence and convergence rate. However, a\nfair comparison should also consider the computation cost of H. Thus, we evaluate the convergence\nrate by calculating the computation cost required for the error to drop below a certain threshold.\nOn GPU, the Jacobi iterator and our model can both be efﬁciently implemented as convolutional\nlayers. Thus, we measure the computation cost by the number of convolutional layers. On CPU,\neach Jacobi iteration u0\ni;j=1\n4(ui\u00001;j+ui+1;j+ui;j\u00001+ui;j+1)has 4 multiply-add operations,\nwhile a 3\u00023convolutional kernel requires 9 operations, so we measure the computation cost by\nthe number of multiply-add operations. This metric is biased in favor of Jacobi because there is\nlittle practical reason to implement convolutions on CPU. Nonetheless, we report both metrics in\nour experiments.\n4.3 C ONV MODEL\nTable 1 shows results of the Conv model. The model is trained on a 16\u000216square domain, and\ntested on 64\u000264. For all settings, our models converge to the correct solution, and require less\ncomputation than Jacobi. The best model, Conv3, is \u00185\u0002faster than Jacobi in terms of layers, and\n\u00182:5\u0002faster in terms of multiply-add operations.\nAs discussed in Section 3.2, if our iterator converges for a geometry, then it is guaranteed to converge\nto the correct solution for any fand boundary values b. The experiment results show that our model\nnot only converges but also converges faster than the standard solver, even though it is only trained\non a smaller square domain.\n4.4 U-N ETMODEL\nFor the U-Net models, we compare them against Multigrid models with the same number of subsam-\npling and smoothing layers. Therefore, our models have the same number of convolutional layers,\nand roughly 9=4times the number of operations compared to Multigrid. The model is trained on a\n64\u000264square domain, and tested on 256\u0002256.\nThe bottom part of Table 1 shows the results of the U-Net model. Similar to the results of Conv\nmodels, our models outperforms Multigrid in all settings. Note that U-Net2 has lower computation\n8\n\nPublished as a conference paper at ICLR 2019\nTable 1: Comparisons between our models and the baseline solvers. The Conv models are compared\nwith Jacobi, and the U-Net models are compared with Multigrid. The numbers are the ratio between\nthe computation costs of our models and the baselines. None of the values are greater than 1, which\nmeans that all of our models achieve a speed up on every problem and both performance metric\n(convolutional layers and multiply-add operations).\nModel Baseline Square L-shape Cylinders Square-Poisson\nlayers / ops layers / ops layers / ops layers / ops\nConv1 Jacobi 0.432 / 0.702 0.432 / 0.702 0.432 / 0.702 0.431 / 0.701\nConv2 Jacobi 0.286 / 0.524 0.286 / 0.524 0.286 / 0.524 0.285 / 0.522\nConv3 Jacobi 0.219 / 0.424 0.219 / 0.423 0.220 / 0.426 0.217 / 0.421\nConv4 Jacobi 0.224 / 0.449 0.224 / 0.449 0.224 / 0.448 0.222 / 0.444\nU-Net2 Multigrid2 0.091 / 0.205 0.090 / 0.203 0.091 / 0.204 0.079 / 0.178\nU-Net3 Multigrid3 0.220 / 0.494 0.213 / 0.479 0.201 / 0.453 0.185 / 0.417\n256 512 1024\nDimension101102Time (s)\nSquare\nFEniCS\nOurs\n(a) Square domain.\n256 512 1024\nDimension101102Time (s)\nL-shape\nFEniCS\nOurs\n (b) L-shape domain.\n256 512 1024\nDimension101102Time (s)\nCylinders\nFEniCS\nOurs\n (c) Cylinders domain.\nFigure 2: CPU runtime comparisons of our model with the FEniCS model. Our method is compara-\nble or faster than the best solver in FEniCS in all cases. When run on GPU, our solver provides an\nadditional 30\u0002speedup.\ncost compared with Multigrid2 than U-Net3 compared to Multigrid 3. This is because Multigrid2 is\na relatively worse baseline. U-Net3 still converges faster than U-Net2.\n4.5 C OMPARISON WITH FENICS\nThe FEniCS package (Logg et al., 2012) provides a collection of tools with high-level Python and\nC++ interfaces to solve differential equations. The open-source project is developed and maintained\nby a global community of scientists and software developers. Its extensive optimization over the\nyears, including the support for parallel computation, has led to its widespread adaption in industry\nand academia (Alnæs et al., 2015).\nWe measure the wall clock time of the FEniCS model and our model, run on the same hardware.\nThe FEniCS model is set to be the minimal residual method with algebraic multigrid preconditioner,\nwhich we measure to be the fastest compared to other methods such as Jacobi or Incomplete LU\nfactorization preconditioner. We ignore the time it takes to set up geometry and boundary conditions,\nand only consider the time the solver takes to solve the problem. We set the error threshold to be\n1 percent of the initial error. For the square domain, we use a quadrilateral mesh. For the L-shape\nand cylinder domains, however, we let FEniCS generate the mesh automatically, while ensuring the\nnumber of mesh points to be similar.\nFigure 2 shows that our model is comparable or faster than FEniCS in wall clock time. These\nexperiments are all done on CPU. Our model efﬁciently runs on GPU, while the fast but complex\nmethods in FEniCS do not have efﬁcient GPU implementations available. On GPU, we measure an\nadditional 30\u0002speedup (on Tesla K80 GPU, compared with a 64-core CPU).\n9\n\nPublished as a conference paper at ICLR 2019\n5 R ELATED WORK\nRecently, there have been several works on applying deep learning to solve the Poisson equation.\nHowever, to the best of our knowledge, previous works used deep networks to directly generate the\nsolution; they have no correctness guarantees and are not generalizable to arbitrary grid sizes and\nboundary conditions. Most related to our work are (Farimani et al., 2017) and (Sharma et al., 2018),\nwhich learn deep networks to output the solution of the 2D Laplace equation (a special case where\nf= 0). (Farimani et al., 2017) trained a U-Net model that takes in the boundary condition as a\n2D image and outputs the solution. The model is trained by L1 loss to the ground truth solution\nand an adversarial discriminator loss. (Sharma et al., 2018) also trained a U-net model but used a\nweakly-supervised loss. There are other related works that solved the Poisson equation in concrete\nphysical problems. (Tang et al., 2017) solved for electric potential in 2D/3D space; (Tompson et al.,\n2017) solved for pressure ﬁelds for ﬂuid simulation; (Zhang et al., 2018) solved particle simulation\nof a PN Junction.\nThere are other works that solve other types of PDEs. For example, many studies aimed to use\ndeep learning to accelerate and approximate ﬂuid dynamics, governed by the Euler equation or the\nNavier-Stokes equations (Guo et al., 2016; Yang et al., 2016; Chu & Thuerey, 2017; Kutz, 2017).\n(Eismann et al., 2018) use Bayesian optimization to design shapes with reduced drag coefﬁcients in\nlaminar ﬂuid ﬂow. Other applications include solving the Schrodinger equation (Mills et al., 2017),\nturbulence modeling (Singh et al., 2017), and the American options and Black Scholes PDE (Sirig-\nnano & Spiliopoulos, 2018). A lot of these PDEs are nonlinear and may not have a standard linear\niterative solver, which is a limitation to our current method since our model must be built on top of\nan existing linear solver to ensure correctness. We consider the extension to different PDEs as future\nwork.\n6 C ONCLUSION\nWe presented a method to learn an iterative solver for PDEs that improves on an existing standard\nsolver. The correct solution is theoretically guaranteed to be the ﬁxed point of our iterator. We show\nthat our model, trained on simple domains, can generalize to different grid sizes, geometries and\nboundary conditions. It converges correctly and achieves signiﬁcant speedups compared to standard\nsolvers, including highly optimized ones implemented in FEniCS.\n7 A CKNOWLEDGEMENTS\nThis research was supported by NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145),\nAFOSR (FA9550-19-1-0024), Siemens, and JP Morgan.\n10\n\nPublished as a conference paper at ICLR 2019\nREFERENCES\nMartin S Alnæs, Jan Blechta, Johan Hake, August Johansson, Benjamin Kehlet, Anders Logg, Chris\nRichardson, Johannes Ring, Marie E Rognes, and Garth N Wells. The fenics project version 1.5.\nArchive of Numerical Software , 3(100):9–23, 2015.\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\nBrendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient\ndescent. In Advances in Neural Information Processing Systems , 2016.\nSanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit\nacceleration by overparameterization. arXiv preprint arXiv:1802.06509 , 2018.\nWilliam L Briggs, Steve F McCormick, et al. A multigrid tutorial , volume 72. Siam, 2000.\nMengyu Chu and Nils Thuerey. Data-driven synthesis of smoke ﬂows with cnn-based feature de-\nscriptors. ACM Transactions on Graphics (TOG) , 36(4):69, 2017.\nStephan Eismann, Daniel Levy, Rui Shu, Stefan Bartzsch, and Stefano Ermon. Bayesian optimiza-\ntion and attribute adjustment. In Proc. 34th Conference on Uncertainty in Artiﬁcial Intelligence ,\n2018.\nAmir Barati Farimani, Joseph Gomes, and Vijay S Pande. Deep learning the physics of transport\nphenomena. arXiv preprint arXiv:1709.02432 , 2017.\nStanley P Frankel. Convergence rates of iterative treatments of partial differential equations. Math-\nematical Tables and Other Aids to Computation , 4(30):65–75, 1950.\nXiaoxiao Guo, Wei Li, and Francesco Iorio. Convolutional neural networks for steady ﬂow ap-\nproximation. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining , pp. 481–490, 2016.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data ,\npp. 489–504. ACM, 2018.\nJ Nathan Kutz. Deep learning in ﬂuid dynamics. Journal of Fluid Mechanics , 814:1–4, 2017.\nRandall J LeVeque. Finite difference methods for ordinary and partial differential equations: steady-\nstate and time-dependent problems , volume 98. Siam, 2007.\nDaniel Levy, Matthew D Hoffman, and Jascha Sohl-Dickstein. Generalizing hamiltonian monte\ncarlo with neural networks. arXiv preprint arXiv:1711.09268 , 2017.\nAnders Logg, Kent-Andre Mardal, and Garth Wells. Automated solution of differential equations\nby the ﬁnite element method: The FEniCS book . Springer, 2012. ISBN 978-3-642-23098-1. doi:\n10.1007/978-3-642-23099-8.\nKyle Mills, Michael Spanner, and Isaac Tamblyn. Deep learning and the schr ¨odinger equation.\nPhysical Review A , 96(4):042113, 2017.\nPeter J Olver. Numerical solution of ordinary differential equations. Numerical Analysis Lecture\nNotes , 2008.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-\ncal image segmentation. In International Conference on Medical image computing and computer-\nassisted intervention , pp. 234–241. Springer, 2015.\nRishi Sharma, Amir Barati Farimani, Joe Gomes, Peter Eastman, and Vijay Pande. Weakly-\nsupervised deep learning of heat transport via physics informed loss. arXiv preprint\narXiv:1807.11374 , 2018.\nAnand Pratap Singh, Shivaji Medida, and Karthik Duraisamy. Machine-learning-augmented predic-\ntive modeling of turbulent separated ﬂows over airfoils. AIAA Journal , pp. 2215–2227, 2017.\n11\n\nPublished as a conference paper at ICLR 2019\nJustin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial\ndifferential equations. Journal of Computational Physics , 375:1339–1364, 2018.\nJiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-mc: Adversarial training for mcmc. In\nAdvances in Neural Information Processing Systems , 2017.\nWei Tang, Tao Shan, Xunwang Dang, Maokun Li, Fan Yang, Shenheng Xu, and Ji Wu. Study\non a poisson’s equation solver based on deep learning technique. In IEEE Electrical Design of\nAdvanced Packaging and Systems Symposium (EDAPS) . IEEE, 2017.\nJonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, and Ken Perlin. Accelerating eulerian\nﬂuid simulation with convolutional networks. In International Conference on Machine Learning ,\n2017.\nCheng Yang, Xubo Yang, and Xiangyun Xiao. Data-driven projection method in ﬂuid simulation.\nComputer Animation and Virtual Worlds , 27(3-4):415–424, 2016.\nZhongyang Zhang, Ling Zhang, Ze Sun, Nicholas Erickson, Ryan From, and Jun Fan. Solving\npoisson’s equation using deep learning in particle simulation of pn junction. arXiv preprint\narXiv:1810.10192 , 2018.\n12\n\nPublished as a conference paper at ICLR 2019\nA P ROOFS\nTheorem 1. For a linear iterator \t(u) =Tu+c,\tconverges to a unique stable ﬁxed point from\nany initialization if and only if the spectral radius \u001a(T)<1.\nProof. Suppose\u001a(T)<1, then (I\u0000T)\u00001must exist because all eigenvalues of I\u0000Tmust be strictly\npositive. Let u\u0003= (I\u0000T)\u00001c; thisu\u0003is a stationary point of the iterator \t, i.e.u\u0003=Tu\u0003+c. For\nany initialization u0, letuk= \tk(u0). The errorek=u\u0003\u0000uksatisﬁes\nTek= (Tu\u0003+c)\u0000(Tuk+c) =u\u0003\u0000uk+1=ek+1)ek=Tke0(16)\nSince\u001a(T)<1, we knowTk!0ask!1 (LeVeque, 2007), which means the error ek!0.\nTherefore, \tconverges to u\u0003from anyu0.\nNow suppose \u001a(T)\u00151. Let\u00151be the largest absolute eigenvalue where \u001a(T) =j\u00151j\u00151, and\nv1be its corresponding eigenvector. We select initialization u0=u\u0003+v1, thene0=v1. Because\nj\u00151j\u00151, we havej\u0015k\n1j\u00151, then\nTke0=\u0015k\n1v16!k!10\nHowever we know that under a different initialization ^u0=u\u0003, we have ^e0= 0, soTk^e0= 0.\nTherefore the iteration cannot converge to the same ﬁxed point from different initializations u0and\n^u0.\nProposition 1 IfMis a full rank diagonal matrix, and u\u00032Rn2\u0002n2satisﬁes Eq. (7), then u\u0003\nsatisﬁes Eq. (4).\nProof of Proposition 1. Letu\u0003be a ﬁxed point of Eq. (7) then\nGu\u0003+ (I\u0000G)u\u0003=G(M\u00001Nu\u0003+M\u00001f) + (I\u0000G)b\nThis is equivalent to\n(I\u0000G)u\u0003= (I\u0000G)b\nG(u\u0003\u0000M\u00001Nu\u0003\u0000M\u00001f) = 0(17)\nThe latter equation is equivalent to GM\u00001(Au\u0003\u0000f) = 0 . IfMis a full rank diagonal matrix, this\nimpliesG(Au\u0003\u0000f) = 0 , which isGAu\u0003=Gf. Therefore, u\u0003satisﬁes Eq.(4).\nTheorem 2. For ﬁxedG;f;b;n , the spectral norm of \bH(u;G;f;b;n )is a convex function of H,\nand the set of Hsuch that the spectral norm of \bH(u;G;f;b;n )<1is a convex open set.\nProof. As before, denote \t(u) =Tu+c. Observe that\n\bH(u;G;f;b;n ) =Tu+c+GH(Tu+c\u0000u) = (T+GHT\u0000GH)u+GHc +c (18)\nThe spectral norm k\u0001k2is convex with respect to its argument, and (T+GHT\u0000GH)is linear\ninH. Thus,kT+GHT\u0000GHk2is convex in Has well. Thus, under the condition that kT+\nGHT\u0000GHk2<1, the set ofHmust be convex because it is a sub-level set of the convex function\nkT+GHT\u0000GHk2.\nTo prove that it is open, observe that k\u0001k2is a continuous function, so kT+GHT\u0000GHk2is\na continuous map from Hto the spectral radius of \bH. If we consider the set of Hsuch that\nkT+GHT\u0000GHk2<1, this set is the preimage of (\u0000\u000f;1)for any\u000f>0. As(\u0000\u000f;1)is open, its\npreimage must be open.\nProposition 2. For ﬁxedA;G;n and ﬁxedH, if for some f0;b0,\bH(u;G;f 0;b0;n)is valid for\nthe PDE problem (A;G;f 0;b0;n), then for all fandb, the iterator \bH(u;G;f;b;n )is valid for\nthe PDE problem (A;G;f;b;n ).\n13\n\nPublished as a conference paper at ICLR 2019\nProof. From Theorem 1 and Lemma 1, our iterator is valid if and only if \u001a(T+GHT\u0000GH)<1.\nThe iterator T+GHT\u0000GH only depends on A;G , and is independent of the constant cin Eq.\n(18). Thus, the validity of the iterator is independent with fandb. Thus, if the iterator is valid for\nsomef0andb0, then it is valid for any choice of fandb.\nB P ROOF OF CONVERGENCE OF JACOBI METHOD\nIn Section 2.4.1, we show that for Poisson equation, the update matrix T=G(I\u0000A). We now\nformally prove that \u001a(G(I\u0000A))<1for anyG.\nFor any matrix T, the spectral radius is bounded by the spectral norm: \u001a(T)\u0014 kTk2, and the\nequality holds if Tis symmetric. Since (I\u0000A)is a symmetric matrix, \u001a(I\u0000A) =kI\u0000Ak2. It\nhas been proven that \u001a(I\u0000A)<1(Frankel, 1950). Moreover, kGk2= 1. Finally, matrix norms\nare sub-multiplicative, so\n\u001a(T)\u0014kG(I\u0000A)k2\u0014kGk2kI\u0000Ak2<1 (19)\n\u001a(T)<1is true for any G. Thus, the standard Jacobi method is valid for the Poisson equation under\nany geometry.\n14",
  "textLength": 41902
}