{
  "paperId": "17d680178329f7a6b6daad12b013272a396e8844",
  "title": "FlexFlood: Efficiently Updatable Learned Multi-dimensional Index",
  "pdfPath": "17d680178329f7a6b6daad12b013272a396e8844.pdf",
  "text": "FlexFlood: Efficiently Updatable\nLearned Multi-dimensional Index\nFuma Hidaka\nThe University of Tokyo\nhidaka@hal.t.u-tokyo.ac.jpYusuke Matsui\nThe University of Tokyo\nmatsui@hal.t.u-tokyo.ac.jp\nAbstract\nA learned multi-dimensional index is a data structure that efficiently answers\nmulti-dimensional orthogonal queries by understanding the data distribution using\nmachine learning models. One of the existing problems is that the search per-\nformance significantly decreases when the distribution of data stored in the data\nstructure becomes skewed due to update operations. To overcome this problem, we\npropose FlexFlood, a flexible variant of Flood. FlexFlood partially reconstructs the\ninternal structure when the data distribution becomes skewed. Moreover, FlexFlood\nis the first learned multi-dimensional index that guarantees the time complexity of\nthe update operation. Through experiments using both artificial and real-world data,\nwe demonstrate that the search performance when the data distribution becomes\nskewed is up to 10 times faster than existing methods. We also found that partial\nreconstruction takes only about twice as much time as naive data updating.\n1 Introduction\nFiltering, scanning, and updating of data are fundamental operations for databases, and various data\nstructures have been studied to perform these operations efficiently. In the real world, we often need\nto handle multi-dimensional data, and Kd-tree and its variants [ 5,20,31] are typical data structures\nfor handling them. These data structures are widely used in real-world applications [3, 23, 22].\nRecently, there has been active research to improve data structures by learning the distribution of data\nand queries with machine learning models. Such data structures are called learned index [ 24]. One of\nthe significant challenges in learned multi-dimensional indexes [ 32,12,25,19,8] is that many do not\nsupport data update operations. Even if they do, none describe the time complexity for updating.\nTherefore, we proposed a flexible variant of Flood (FlexFlood) that supports efficient data updating by\nadaptively modifying the internal structure of the existing learned multi-dimensional index, Flood [ 32].\nWe proved that the amortized time complexity of updating is O(DlogN)under two assumptions that\nthe data increases at an approximately constant pace and that the training results of the ML model\nsatisfy certain conditions. Here, Dis the dimensionality of the data, and Nis the total number of data.\nFurthermore, experiments using multiple artificial and real-world datasets confirm that the “certain\nconditions” can be sufficient. The original Flood’s search speed slows down as we update data, but\nFlexFlood remains fast. As a result, FlexFlood is up to 10 times faster than Flood after many updates.\nThe source code for FlexFlood is available at https://github.com/mti-lab/FlexFlood.\n2 Related Work\nMany classical data structures have been proposed for handling multi-dimensional data. For example,\ntree-based structures include Kd-tree [ 5,16], R-tree [ 20,4], and Oct-tree [ 28]. Grid File [ 33] is also\nproposed as a space-partitioning structure. Additionally, there is the Z-order curve algorithm [ 31],\nwhich reduces multi-dimensional data into one dimension using a special sorting technique.\nMachine Learning for Systems Workshop at (NeurIPS 2024).arXiv:2411.09205v2  [cs.DS]  15 Nov 2024\n\nLearned index [ 24,13,2] is a data structure that incorporates machine learning models into classical\ndata structures such as B-tree [ 1], Hash Map [ 10], and Bloom Filter [ 6]. Learned indexes improve\nperformance by taking advantage of the distribution of data and queries. Recently, learned indexes\nhave been actively researched. For example, several learned Bloom Filters have been proposed [ 29,\n26, 30, 34, 35]. They achieved a better memory/accuracy trade-off than the original Bloom Filter.\nLearned indexes are vulnerable to data update operations in general. This is because if the data\ndistribution is distorted by updating operations, the accuracy of the machine learning model will\ndecrease, and search performance will decrease, too. To address this problem, learned indexes that\nsupport efficient updating operations have been proposed [11, 15, 14, 37].\nThe above learned indexes can handle only one-dimensional data. On the other hand, learned\nindexes for multi-dimensional data have also been proposed. For example, Flood [ 32], Tsunami [ 12],\nLisa [ 25], RLR-tree [ 19], Waffle [ 8], and so on [ 2]. Flood and Tsunami do not support data updating\noperations. Lisa, RLR-tree and Waffle support them, but there are no discussion of time complexity.\n3 Preliminary\nFirst, we define the problem setting. N D -dimensional vectors v1, . . . , vNare given, where vn∈RD.\nWe denote the d-th dimension of vnasvn[d]∈R. Initially, we construct a index based on the N\nvectors. Then, we process Qqueries sequentially. Queries are provided in the following formats:\n•Search Query: l∈RDandr∈RDare provided. landrrepresent the endpoints of the\ndiagonal of the search range hyper-rectangle. We enumerate all vectors vncontained within\nthe data structure, such that for all d∈ {1,2, . . . , D },l[d]≤vn[d]≤r[d]holds.\n•Insert Query: A D-dimensional vector v∈RDis provided. We add it if this vector is not in\nthe data structure. We do nothing if the same vector already exists within the data structure.\n•Erase Query: A D-dimensional vector v∈RDis provided. We remove it if this vector\nexists within the data structure. We do nothing if the vector is not in the data structure.\nNext, we explain the Flood algorithm [ 32], which is the basis of the proposed method. Flood divides\ntheD-dimensional space into approximately equal parts by (D−1)-dimensional grid cells. Given\nD-dimensional input vectors, Flood assigns them to the corresponding cell. Within each cell, Flood\nkeeps vectors sorted using the value of the D-th dimension that is not used for the grid division. The\nparameters of Flood are the sort dimension and the number of cell partitions. Flood optimizes these\nparameters using a gradient descent method with a random forest regression model [7, 36].\nFlood uses sorted arrays to hold data in cells, but updating data on a sorted array is very expensive.\nWe can make Flood updatable by replacing sorted arrays with B-trees. (Appendix A discussed this\noverhead.) However, if the data distribution changes, the grid partitioning at the initialization becomes\nmeaningless and the updatable Flood’s search speed slows down greatly. Therefore, we propose an\nefficient data updating algorithm to solve this problem, using the updatable Flood as a baseline.\n4 Proposed Method: FlexFlood\nBased on the updatable Flood, we propose FlexFlood, a data updating algorithm that ensures fast\nsearch even if data distribution changes. Figure 1 is an overview of FlexFlood. Remember Flood\nconstructs cells so that for each axis d, the total number of data in the cell with the same d-dimensional\nvalue isN\nxd(an equal number of data for each cell). Here, xdis the number of cell partitions on axis d.\nWhen the number of vectors in each cell deviates far fromN\nxddue to data updating, the search of the\nupdatable Flood slows down greatly. Therefore, FlexFlood re-partitions around the cells as follows.\n(Appendix B discussed thresholds for re-partitioning in more detail.)\n• If the cell contains more than2N\nxdvectors, we split the cell into two.\n•If the cell contains less thanN\n3xdvectors and the neighboring cell contains less than7N\n6xdvectors, we merge the two cells.\n•If the cell contains less thanN\n3xdvectors and the neighboring cell contains more than7N\n6xdvectors, we equalize the number of vectors in the two cells.\n2\n\n1SPCMFN\u0001\u0007\u00010VS\u0001NFUIPE1SPCMFN\u0001w*G\u0001XF\u0001VQEBUF\u0001EBUB\r\u0001'MPPE`T\u0001TFBSDI\u0001TQFFE\u0001JT\u0001SFEVDFE\u00010VS\u0001NFUIPE\u0001w*G\u0001UIF\u0001EJTUSJCVUJPO\u0001JT\u0001TLFXFE\r\u0001QBSUJBMMZ\u0001SFQBSUJUJPO\u0001DFMMT\n73FQBSUJUJPO6QEBUFFigure 1: Overview of our method ( D= 2): Even if N= 27 data are equally divided into x1= 3\ncells at the initialization, the distribution can become skewed due to data updating, and the updatable\nFlood slows down. We aim to ensure the search performance by partially re-partitioning the cells.\nRe-partitioning in this way keeps the number of vectors in the cell constant and keeps the search\nspeed fast. However, since the above re-partition requires a large amount of movement of data within\nthe cells, it is not obvious whether the update operation can be performed efficiently.\nWe therefore analyzed the time complexity of this algorithm. Based on the insight that re-partitions\nwith very high computational costs occur only with sufficiently low frequency, we found that\nthe computational complexity of the updating operation is not large when considered in terms of\namortization. Under the two assumptions that (1) the data increases at a pace that can be regarded\nas constant and (2)QD\nd=1xdPD\nd=1xd≤DNlogN, we proved that the updating operation of\nFlexFlood is amortized at O(DlogN). (See Appendix C for detailed analysis.)\nThe update operation of the updatable Flood is O(logN), which is Dtimes faster than ours. Therefore,\nwe can interpret FlexFlood as an algorithm that ensures high search performance even when the data\ndistribution is skewed, instead of sacrificing up to Dtimes the complexity of the update operations.\n5 Experiment\nWe evaluate FlexFlood. The runtime environment is Intel Core i7-11800H, 8 cores, 2.3 GHz, 32 GB\nmemory. The four data structures used for comparison and their implementations are shown below.\n• Self-Balancing Kd-tree (SB-Kdtree) [5, 16]: We implemented it in C++.\n• R-tree [20]: We used C++ boost::geometry::index::rtree [17].\n•Updatable Flood: We implemented Flood in C++, then we replaced Flood’s sorted arrays\nwith B-trees published by Google [18].\n• FlexFlood: We added cell re-partitioning algorithm to the updatable Flood.\nAs a dataset, we used (1) normal distribution, (2) Stock Price dataset [ 21], and (3) Open Street Map\ndataset [9]. See Appendix D for details on data and query generation methods.\n6 Result\nFigures 2 illustrate the cumulative query processing time for each data structure for each dataset. The\nnumber of cell partitions and whether the conditionQD\nd=1xdPD\nd=1xd≤DNlogNwas satisfied or\nnot in each dataset are shown in Table 1.\nFirst, looking at Table 1, we can see that the assumption (2) was always satisfied within the range of\nthe experiments conducted here. Therefore, we can assume that the amortized time complexity of the\ndata updating operation of FlexFlood is O(DlogN)for practical purposes.\nWe then turn focus on Figures 2a, 2b, 2c, the results of the update queries. Comparing FlexFlood\nwith the classical data structures such as SB-Kdtree and R-tree, FlexFlood processes update queries\nabout 1.1 to 2.9 times faster. FlexFlood is better than the SB-Kdtree and R-tree for all datasets.\nComparing FlexFlood with the updatable Flood, FlexFlood requires at most 2.0 times longer runtime.\nRemember that in theory, the cell re-partitioning algorithm takes about Dtimes longer runtime than\nthe updatable Flood’s data updating operation in the worst case. In light of this, we believe that\n3\n\nTable 1: Number of cell partitions and whether the conditional expression was satisfied or not\nDataset {xd}D\nd=1QD\nd=1xdPD\nd=1xdDNlogN Assumption (2)\n3D Normal Distribution {21,17,1} 1.4·1045.0·106✓\n4D Stock Price {19,19,1,39} 1.1·1061.4·107✓\n5D Open Street Map {17,17,13,23,1}6.1·1061.0·108✓\n0.00 0.25 0.50 0.75 1.00\n# Queries 1e60.000.250.500.751.00Cumulative Time [s]SB-Kdtree\nR-tree\nUpdatable Flood\nFlexFlood (Ours)\n(a) Normal Distribution Update\n0 1 2 3\n# Queries 1e6024Cumulative Time [s](b) Stock Price Update\n0.0 0.5 1.0 1.5 2.0\n# Queries 1e601234Cumulative Time [s] (c) Open Street Map Update\n0.00 0.25 0.50 0.75 1.00\n# Queries 1e601234Cumulative Time [s]\n(d) Normal Distribution Search\n0 1 2 3\n# Queries 1e602040Cumulative Time [s] (e) Stock Price Search\n0.0 0.5 1.0 1.5 2.0\n# Queries 1e6050010001500Cumulative Time [s] (f) Open Street Map Search\nFigure 2: Experimental results: The upper panel shows the update queries, and the lower panel shows\nthe results for the search queries. (Lower is better.)\nFlexFlood is not only theoretically guaranteed to be computationally feasible but also fast enough for\npractical use (e.g., FlexFlood could be D= 5times slower than the updatable Flood for 5D Open\nStreet Map dataset, but Figure 2c shows that FlexFlood is only about 2.0 times slower in practice).\nFinally, we refer to Figures 2d, 2e, 2f, the results of the search queries. Comparing FlexFlood with\nthe updatable Flood, FlexFlood processes search queries about 3.3 to 10 times faster, outperforming\nthe updatable Flood on all datasets. Comparing FlexFlood with SB-Kdtree and R-tree, FlexFlood\nprocesses queries on the normal distribution dataset and the Stock Price dataset about 1.2 to 12\ntimes faster. On the Open Street Map dataset, however, FlexFlood is slower than the R-tree although\nFlexFlood is faster than SB-Kdtree. Regarding this result, the benchmark paper [ 27] pointed out that\nthe learned indexes perform poorly compared to classical data structures on the Open Street Map\ndataset because it lacks local structure, making them difficult to learn. Figure 2f also shows that the\nslope of the curve near the origin for the updatable Flood is steeper than that of SB-Kdtree and R-tree.\nIn light of this fact, we interpret this result as consistent with the results of previous studies.\n7 Conclusion\nBy adaptively reconstructing the internal structure of Flood, we proposed FlexFlood, which supports\nefficient data updating. Experimental results show that FlexFlood does not reduce the search speed\nand has advantages over classical data structures. Furthermore, we proved that the amortized time\ncomplexity of data updating is O(DlogN)under two experimentally valid assumptions.\nOn the other hand, FlexFlood loses the optimality guarantees regarding the sort dimension and the\nnumber of cell divisions after the data update. Therefore, it may be possible to ensure even faster\nsearch by periodically relearning the distribution. (Appendix E discusses this in more detail.)\n4\n\nReferences\n[1]Database architects blog: The case for b-tree index structures. http://databasearchitects.\nblogspot.com/2017/12/the-case-for-b-tree-index-structures.html .\n[2]Abdullah Al-Mamun, Hao Wu, Qiyang He, Jianguo Wang, and Walid G. Aref. A survey of\nlearned indexes for the multi-dimensional space, 2024.\n[3]Amazon AWS. Amazon redshift engineering’s advanced table design playbook: Compound and\ninterleaved sort keys. 2016. https://aws.amazon.com/jp/blogs/big-data/amazon-\nredshift-engineerings-advanced-table-design-playbook-compound-and-\ninterleaved-sort-keys/ .\n[4]Norbert Beckmann, Hans-Peter Kriegel, Ralf Schneider, and Bernhard Seeger. The r*-tree: An\nefficient and robust access method for points and rectangles. In Proceedings of the 1990 ACM\nSIGMOD International Conference on Management of Data , pages 322–331, 1990.\n[5]Jon Louis Bentley. Multidimensional binary search trees used for associative searching. Com-\nmunications of the ACM , 18(9):509–517, 1975.\n[6]Burton H Bloom. Space/time trade-offs in hash coding with allowable errors. Communications\nof the ACM , 13(7):422–426, 1970.\n[7] Leo Breiman. Random forests. Machine learning , 45:5–32, 2001.\n[8]Dalsu Choi, Hyunsik Yoon, Hyubjin Lee, and Yon Dohn Chung. Waffle: in-memory grid index\nfor moving objects with reinforcement learning-based configuration tuning system. Proceedings\nof the VLDB Endowment , 15(11):2375–2388, 2022.\n[9]Contributers. Open street map. https://download.geofabrik.de/ . Viewed 10 January\n2024, Licence: ODbL 1.0.\n[10] Martin Dietzfelbinger, Anna Karlin, Kurt Mehlhorn, Friedhelm Meyer Auf Der Heide, Hans\nRohnert, and Robert E Tarjan. Dynamic perfect hashing: Upper and lower bounds. SIAM\nJournal on Computing , 23(4):738–761, 1994.\n[11] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang,\nBadrish Chandramouli, Johannes Gehrke, Donald Kossmann, et al. Alex: an updatable adap-\ntive learned index. In Proceedings of the 2020 ACM SIGMOD International Conference on\nManagement of Data , pages 969–984, 2020.\n[12] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. Tsunami: A learned\nmulti-dimensional index for correlated data and skewed workloads. Proceedings of the VLDB\nEndowment , 14(2):74–86, 2020.\n[13] Paolo Ferragina and Giorgio Vinciguerra. Learned data structures. In Recent Trends in\nLearning From Data: Tutorials from the INNS Big Data and Deep Learning Conference\n(INNSBDDL2019) , pages 5–41. Springer, 2020.\n[14] Paolo Ferragina and Giorgio Vinciguerra. The pgm-index: a fully-dynamic compressed learned\nindex with provable worst-case bounds. Proceedings of the VLDB Endowment , 13(8):1162–\n1175, 2020.\n[15] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim Kraska. Fiting-\ntree: A data-aware index structure. In Proceedings of the 2019 ACM SIGMOD International\nConference on Management of Data , pages 1189–1206, 2019.\n[16] Igal Galperin and Ronald L Rivest. Scapegoat trees. In Proceedings of the fourth annual\nACM-SIAM Symposium on Discrete algorithms , pages 165–174, 1993.\n[17] Barend Gehrels, Bruno Lalande, Mateusz Loskot, Adam Wulkiewicz, and Oracle and/or its affil-\niates. boost::geometry::index::rtree. https://beta.boost.org/doc/libs/1_82_0/libs/\ngeometry/doc/html/geometry/reference/spatial_indexes/boost__geometry_\n_index__rtree.html . Viewed 18 September 2024, License: Boost Software License 1.0.\n5\n\n[18] Google. cpp-btree. https://code.google.com/archive/p/cpp-btree/wikis/\nUsageInstructions.wiki . Viewed 30 January 2024, License: Apache License 2.0.\n[19] Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, and Sheng Wang. The rlr-tree: A\nreinforcement learning based r-tree for spatial data. Proceedings of the ACM on Management of\nData , 1(1):1–26, 2023.\n[20] Antonin Guttman. R-trees: A dynamic index structure for spatial searching. In Proceedings of\nthe 1984 ACM SIGMOD International Conference on Management of Data , pages 47–57, 1984.\n[21] Evan Hallmark. Daily historical stock prices (1970 - 2018). https://www.kaggle.com/\ndatasets/ehallmar/daily-historical-stock-prices-1970-2018 . Viewed 10 Jan-\nuary 2024, License: Unknown.\n[22] IBM. The spatial index. https://www.ibm.com/docs/en/informix-servers/12.10?\ntopic=data-spatial-index .\n[23] Adrian Ionescu. Processing petabytes of data in seconds with databricks delta.\nhttps://www.databricks.com/blog/2018/07/31/processing-petabytes-of-\ndata-in-seconds-with-databricks-delta.html .\n[24] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 ACM SIGMOD International Conference on\nManagement of Data , pages 489–504, 2018.\n[25] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. Lisa: A learned index structure\nfor spatial data. In Proceedings of the 2020 ACM SIGMOD international Conference on\nManagement of Data , pages 2119–2133, 2020.\n[26] Qiyu Liu, Libin Zheng, Yanyan Shen, and Lei Chen. Stable learned bloom filters for data\nstreams. Proceedings of the VLDB Endowment , 13(12):2355–2367, 2020.\n[27] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra, Alfons\nKemper, Thomas Neumann, and Tim Kraska. Benchmarking learned indexes. Proceedings of\nthe VLDB Endowment , 14(1):1–13, 2020.\n[28] Donald Meagher. Octree encoding: A new technique for the representation, manipulation and\ndisplay of arbitrary 3-d objects by computer. 1980.\n[29] Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching.\nProceedings of the NeurIPS , pages 462–471, 2018.\n[30] Michael Mitzenmacher. Partitioned learned bloom filters. Proceedings of the ICLR , 2021.\n[31] Guy M Morton. A computer oriented geodetic data base and a new technique in file sequencing.\n1966.\n[32] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. Learning multi-\ndimensional indexes. In Proceedings of the 2020 ACM SIGMOD International Conference on\nManagement of Data , pages 985–1000, 2020.\n[33] Jürg Nievergelt, Hans Hinterberger, and Kenneth C Sevcik. The grid file: An adaptable,\nsymmetric multikey file structure. ACM Transactions on Database Systems (TODS) , 9(1):38–71,\n1984.\n[34] Atsuki Sato and Yusuke Matsui. Fast partitioned learned bloom filters. Proceedings of the\nNeurIPS , 2023.\n[35] Atsuki Sato and Yusuke Matsui. Fast construction of partitioned learned bloom filter with\ntheoretical guarantees. arXiv preprint arXiv:2410.13278 , 2024.\n[36] scikit-learn developers. Randomforestregressor. https://scikit-learn.org/dev/\nmodules/generated/sklearn.ensemble.RandomForestRegressor.html . Viewed 28\nOctober 2024, License: BSD License.\n[37] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao Xing. Updatable\nlearned index with precise positions. Proceedings of the VLDB Endowment , 14(8):1276–1288,\n2021.\n6\n\nSB-Kdtree R-tree Flood Updatable\nFlood051015Time / Query ( s)\n(a) Normal Distribution\nSB-Kdtree R-tree Flood Updatable\nFlood0204060Time / Query ( s)\n (b) Stock Price\nSB-Kdtree R-tree Flood Updatable\nFlood0100200300Time / Query ( s)\n (c) Open Street Map\nFigure 3: Processing time per search query.\nA Sorted Array vs B-tree\nThe updatable Flood replaces the Flood’s sorted arrays with B-trees, which slows down the search.\nTherefore, we conducted comparative experiments on workloads where no update queries existed.\nThe data structures used for the comparison are (1) Self-Balancing Kd-tree (SB-Kdtree), (2) R-tree,\n(3) Flood, and (4) updatable Flood. Note that FlexFlood is exactly the same as the updatable Flood\nif there are no update queries. The dataset used for the experiments are (1) normal distribution,\n(2) Stock Price dataset, and (3) Open Street Map dataset. We initialized each data structure, and\nperformed 104search queries to measure the processing time per search query.\nThe experimental results are in Figure 3. The search speed of the updatable Flood is about 1.5 to 2.7\ntimes slower than that of Flood. Therefore, we should use regular Flood for workloads where it is\nknown in advance that there will be no update queries at all. However, for many datasets, updatable\nFlood achieves faster search than classical data structures. Therefore, we believe that updatable Flood\nis worth using for workloads where update queries are likely to be present.\nB Threshold for Re-partitioning\nThe thresholds for the re-partitioning algorithm introduced in Section 4 are hyperparameters. In this\nsection, we discuss the intuitive reasons for determining the thresholds as in Section 4 and how the\nperformance changes when the thresholds are varied.\nB.1 Threshold Selection Criteria\nWe explain why the threshold for “split” in Section 4 is set to2N\nxdand the threshold for “merge” and\n“equalize” toN\n3xd. The purpose of cell re-partitioning is to maintain the number of data in cells at\na baseline valueN\nxdwhen the number of data in a particular cell increases or decreases too much.\nSince “split” halves the number of data in a cell, it is efficient to perform a “split” when the number\nof data in a cell is2N\nxd. For the same reason, it seems intuitive that “merge” and “equalize” should\nbe performed when the number of data in a cell reachesN\n2xd. However, the number of data in the\nneighboring cell is greater thanN\n2xdand less than2N\nxd. Therefore, the number of data in the cell after\n“merge” and “equalize” is considered to be more thanN\nxd. For these reasons, we selectedN\n3xdas the\nthreshold for “merge” and “equalize”, which is slightly smaller thanN\n2xd. This is expected to bring\nthe number of data in the cell after “merge” and “equalize” closer toN\nxd.\nAlso, we explain why the threshold for switching between “merge” and “equalize” is set to7N\n6xd. This\nis because7N\n6xd=N\n3xd+2N\nxd\n2. When “merge” or “equalize” is performed, the number of data in the\nneighboring cell is betweenN\n3xdand2N\nxd. Therefore, we switch between “merge” and “equalize” at\nthe intermediate value.\n7\n\nB.2 Performance Variation with Thresholds\nIn order to confirm the appropriateness of the thresholds in Section 4, we describe the results of\nthe experiments in which FlexFlood’s performance changed when the thresholds were changed.\nWe conducted the experiments by varying the coefficients ofN\nxdon the threshold of “split” in\nthe range {1.5,1.6,1.7,1.8,1.9,2.0,2.1,2.2,2.3,2.4,2.5}and on the threshold of “merge” and\n“equalize” in the range {0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6}. The threshold for\nswitching between “merge” and “equalize” was set to a value halfway between the two thresholds for\nre-partitioning. The experimental settings are the same as in Section 5.\nFigures 4 shows the results of the experiments in which search time and update time were measured\nseparately. Red squares in the heatmap indicate a long search/update time, while blue squares indicate\na short time. That is, blue squares mean better performance. In more detail, we calculated the\npercentage change compared to the search/update time when the threshold was set as section 4.\nFigures 4 shows that there are few thresholds that are blue squares in the heatmaps for both search\nand update. Therefore, we believe that the thresholds of Section 4 are one of the best practical values\nof hyperparameters.\nC Amortized Time Complexity Analysis\nWe prove that the amortized time complexity of FlexFlood’s update operation is O(DlogN)under\ntwo assumptions. Let Xdenote the total number of cells, that is,QD\nd=1xd=X.\nC.1 Worst Time Complexity of Vector Insertion or Erasion\nFirst, we show that the worst time complexity of vector insertion or erasion is O(logN). Insertion or\nerasion of a vector v∈RDconsists of two steps: (1) identifying the cell that should contain vand\n(2) inserting or erasing vfor a B-tree within the cell. For (1), for each dimension d∈ {1,2, . . . , D },\nwe can perform a binary search on the set of cell boundary coordinates to determine where v[d]\nshould be placed. This takes O(PD\nd=1logxd) =O(logQD\nd=1xd) =O(logX). We can do (2)\nwith worst O(logN)because we only insert or erase vinto the B-tree that holds the data in the\nidentified cell. The total computational cost of inserting or erasing vectors is O(logX+ log N).\nHere, X < N is considered to be valid unless the learning of the distribution is very unsuccessful.\nThis is because X≥Nmeans that the number of cells is larger than the total number of data, which\nis obviously wasteful. Table 1 also shows that this is valid. Therefore, the worst time complexity of\nvector insertion or erasion is O(logN).\nC.2 Worst Time Complexity of Cell Re-partition\nNext, we discuss the worst time complexity of cell re-partition. Since cell re-partition occurs\nindependently for each axis d∈ {1,2, . . . , D }, we consider each axis dindependently and sum up\nlater. We assume the following two conditions.\n• The number of data Nincreases at an approximately constant rate.\n•QD\nd=1xdPD\nd=1xd≤DNlogN\nWe define the first assumption as increasing the number of data by ∆∈[0,1]per updating query.\nFrom a micro perspective, of course, each update operation either increases or decreases the data count\nby one. However, from a macro perspective, we assume that after performing kupdate operations,\nthe number of data Ncan be approximated as N=N0+k∆using the initial number of data N0.\nWe discuss the worst-case time complexity of the “split” operation. An overview of the “split” is\nshown in Figure 5. “Split” first inserts empty cells (B-trees) in the appropriate location, and then\ndistributes the data equally between the old and new cells. We can implement the insertion of empty\ncells by simultaneously inserting newX\nxdcells while sliding (at most) Xcells that already exist. Thus,\nthe insertion of empty cells costs O(X\nxd+X) =O(X). For the data distribution, the erase from old\nB-trees and insert into new B-trees are performed2N\nxdtimes in total (Remember the definition of the\n“split”. Since ∆≥0holds, the “split” occurs when the number of data of the target cells is exactly\n8\n\n1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5\nSplit Threshold Coefficient0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6Merge/Equalize Threshold Coefficient\n10.0\n7.5\n5.0\n2.5\n0.02.55.07.510.0\nPercentage Change from Main Experiments(a) Normal Distribution Search\n1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5\nSplit Threshold Coefficient0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6Merge/Equalize Threshold Coefficient\n40\n30\n20\n10\n010203040\nPercentage Change from Main Experiments (b) Normal Distribution Update\n1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5\nSplit Threshold Coefficient0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6Merge/Equalize Threshold Coefficient\n40\n30\n20\n10\n010203040\nPercentage Change from Main Experiments\n(c) Stock Price Search\n1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5\nSplit Threshold Coefficient0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6Merge/Equalize Threshold Coefficient\n40\n30\n20\n10\n010203040\nPercentage Change from Main Experiments (d) Stock Price Update\n1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5\nSplit Threshold Coefficient0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6Merge/Equalize Threshold Coefficient\n20\n15\n10\n5\n05101520\nPercentage Change from Main Experiments\n(e) Open Street Map Search\n1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5\nSplit Threshold Coefficient0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6Merge/Equalize Threshold Coefficient\n40\n30\n20\n10\n010203040\nPercentage Change from Main Experiments (f) Open Street Map Update\nFigure 4: Heatmaps show percentage changes compared to experimental results in Section 6. Blue\nsquares mean better performance.\n9\n\n\u0011\u0014\u0014\u0017\u0013\u0013\u0012\u0012ᶃ\u0001*OTFSU\u0001OFX\u0001DFMMTᶄ\u0001%JTUSJCVUF\u0001EBUB\u0013\u001a\u0017\u0012\u0014\u0018\u0016\u0013\u0015\n\u0013\u001a\u0017\u0018\u0016\u0013\u0015\u0018\u0017\u0012\u0019\n\u0012\u0019\u0012\u0015\u0012\u0015\u0012\u0017\n\u0012\u0017\u0012\u0014\u0012\u0014\u0013\u0013\u0011\u0013\u0012\u0014\u0013\u0015\u0014\u0012\u0012\u0012\u0011\u0014\u0014\u0017\u0011\u0013\u0013\u0012\u0012\u0014\u0012\u0014\u0011\u0013\u0013\u0011\u0012\u0014\u0013\u0015\u0011\u0014\u0012\u0012\u0012\u0013\u0012\u0011\u0014\u0014\u0015\u0013\u0013\u0013\u0012\u0012\u0014\u0012\u0012\u0013\u0013\u0013\u0011\u0012\u0014\u0013\u0013\u0013\u0014\u0012\u0012\u0012\u0013\u0012TQMJUFigure 5: Overview of “split” ( D= 3, x1= 8, x2= 3, X= 24, N= 48 ): The cells outlined in\nred are subject to “split” because they contain 13 data in total. This value is larger than the “split”\ncondition: 2·N\nx1= 2·48\n8= 12 . We insertX\nx1=24\n8= 3new cells (B-trees), and distribute the 12\ndata points evenly between the old and new cells, with 6 points in each.\n2N\nxd, and the number of B-tree operations cannot be more than2N\nxd). Therefore, the time complexity\nof operating B-trees for data distribution is O(N\nxdlogN). Thus, the worst time complexity of the\n“split” is O(X+N\nxdlogN). Considering the “merge” and the “equalize” in the same way, the worst\ntime complexities are O(X+N\nxdlogN), O(N\nxdlogN), respectively.\nC.3 Amortized Time Complexity of Cell Re-partition\nHowever, under the assumption that N=N0+k∆, we can show that a re-partition occurs at most\nonce every O(N\nxd). When N=N0+k∆, we insert(1+∆) k\n2times and erase(1−∆)k\n2times. “Splits”\ndue to intensive insertions occur only(1+∆) k\n2·xd−2∆\nNtimes at most. This is because the “split”\noccurs when the number of data in the cell reaches exactly2N\nxd(because ∆≥0holds), and once\nthe “split” is executed, the number of data in the cell is reset toN\nxd. So, we needN\nxd−2∆consecutive\nintensive insertions to execute the “split” again (CheckN\nxd+N\nxd−2∆= 2·N+N\nxd−2∆∆\nxd). In the same\nway, we can say that the “merge” and the “equalize” by intensive erasion occur only(1−∆)k\n2·3xd+∆\n2Ntimes at most. Moreover, even for cells that are not erased at all, the number of data in the cell may\nreach the threshold of “merge” or “equalize” because Nincreases. About this case, we can prove\nthat the number of computations is maximized when the “merge” or “equalize” occurs immediately\nwhen the number of data reaches the threshold. Therefore, the “merge” and the “equalize” due to\nincreasing data can occur only xd·k\n2N\n∆times at most. From the above, the upper bound on the\nnumber of cell re-partitions that occur after kupdate operations isk((5+∆) xd−3∆−5∆2)\n4N. That is, the\ncell re-partitioning occurs at most once every4N\n(5+∆) xd−3∆−5∆2=O(N\nxd)times.\nRemember the worst-case complexity of the cell re-partitioning is O(X+N\nxdlogN). This happens\nonce every O(N\nxd)times, so the amortized time complexity of the cell re-partitioning is O(X\nNxd+\nlogN). Since we have considered each axis dindependently, the amortized time complexity of\nthe overall cell re-partitioning is O(X\nNPD\nd=1xd+DlogN)by summing them up. Here, from the\n10\n\nsecond assumptionQD\nd=1xdPD\nd=1xd≤DNlogN,X\nNPD\nd=1xd< DlogNholds. Therefore, it is\nproved that the amortized time complexity of the cell re-partitioning is O(DlogN).\nC.4 Limitation\nWe proved that the amortized time complexity of the FlexFlood’s update operation is O(DlogN)\nwhen N≃N0+k∆andQD\nd=1xdPD\nd=1xd≤DNlogNare assumed to hold. It remains to be\ndiscussed how realistic these assumptions are. In the real world, data basically tends to increase.\nHowever, for example, there may be cases where periods of rapid data growth alternate with periods\nof slower growth on an annual cycle. It is very important to observe how FlexFlood performs in such\ncases. If the performance drops significantly, we should develop a hybrid approach that balances these\nperiods. Furthermore, althoughQD\nd=1xdPD\nd=1xd≤DNlogNalways held in our experiment,\nit may not hold for some datasets. We should observe how much the update speed decreases due\nto the breakdown of this assumption. Furthermore, it would be interesting to identify common\ncharacteristics among datasets where this assumption breaks down.\nD Dataset Details\nWe describe the details of the three dataset used in the experiments. In addition, we also introduce\nthe method for generating queries.\nD.1 Normal Distribution Dataset\nDataset 1 is a Normal Distribution Dataset. We tested D= 3 as the number of dimensions of\nthe normal distribution. As the initial data, we independently generated 105data. Each data was\ngenerated independently for each axis according to a normal distribution with µ= 3·108,σ= 108.\nNote that µis the mean and σis the standard deviation of the normal distribution.\nThe number of queries we generated is 2·106, with an update and search queries alternating every\n104queries. The i-th query is an update query if ⌊i\n104⌋ ≡0 (mod 2) , and a search query otherwise.\n(Note that ⌊x⌋denotes the largest integer less than or equal to x.)\nWhen we generated search queries, we first generated a hyper-rectangle, a search region. The length\nof one side of the hyper-rectangle was independently set to a random value less than 3·108. We\nplaced that hyper-rectangle uniformly at random inside the hyper-rectangle whose diagonals are\n(0,0,0)and(109,109,109), and we used this as a search query.\nThe update query has a 50% chance of being selected as an insert query and a 50% chance of being\nselected as an erase query. We generated the insertion queries according to a normal distribution with\nµ= 3·108+ 4·108·i\n2·106,σ= 108independently for each axis when the query was the i-th from\nthe first. This simulates the gradual change of the data distribution. We generate deletion queries by\nrandomly selecting one of the data currently in the data structure.\nD.2 Stock Price Dataset\nDataset 2 is the Stock Price Dataset [ 21] used by [ 12]. we tested D= 4as the number of dimensions.\nOf the approximately 2·107of data included in the dataset, we randomly chose 2·106data and used\nthem. Each data has four attributes: lowest price, highest price, volume, and date.\nOut of 2·106data points, we took 2·105with the oldest dates as the initial data. We generated\n7.2·106queries, with the search and update query order matching Dataset 1. Hyper-rectangles were\ngenerated randomly for search queries, searching approximately 0.1 %of the data per query. Insert\nqueries were generated by selecting the oldest data not yet in the structure, and erase queries by\nselecting the oldest data in the structure. This simulated keeping the last 2·105data points and\nevaluated each data structure’s robustness to real-world data distribution shifts over time.\n11\n\n0.00 0.25 0.50 0.75 1.00\n# Queries 1e6012Cumulative Time [s]SB-Kdtree\nR-tree\nUpdatable Flood\nFlood10000\nFlood20000\nFlexFlood (Ours)(a) Normal Distribution Update\n0 1 2 3\n# Queries 1e605101520Cumulative Time [s] (b) Stock Price Update\n0.0 0.5 1.0 1.5 2.0\n# Queries 1e60204060Cumulative Time [s] (c) Open Street Map Update\n0.00 0.25 0.50 0.75 1.00\n# Queries 1e602468Cumulative Time [s]\n(d) Normal Distribution Search\n0 1 2 3\n# Queries 1e602040Cumulative Time [s] (e) Stock Price Search\n0.0 0.5 1.0 1.5 2.0\n# Queries 1e6050010001500Cumulative Time [s] (f) Open Street Map Search\nFigure 6: Experimental results: The upper panel shows the update queries, and the lower panel shows\nthe results for the search queries. (Lower is better.)\nD.3 Open Street Map Dataset\nDataset 3 is the Open Street Map Dataset [ 9] used by [ 32]. We tested D= 5 as the number of\ndimensions. Of the approximately 108of data, we randomly extracted 2·106data and used them.\nEach data set has five attributes: ID, version, date, latitude, and longitude.\nFirst, we clustered the dataset into two groups of 106each using K-means, and used one cluster as the\ninitial data. We generated 2·106queries, with the search and update query order matching Dataset 1.\nWe generated hyper-rectangles randomly for search queries so that approximately 0.3 %of the data\nwere searched per query. Insert queries were generated by randomly selecting data from the cluster\nnot selected as the initial data, while erase queries were generated by randomly selecting data from\nthe initial cluster.\nE Re-initialization\nThere is a delta-buffer method according to the survey paper on learned multi-dimensional indexes [ 2].\nIt stores data updating in a small array and periodically merges them with the data structure. We\ncompare the performance of FlexFlood with that of a delta-buffered version of Flood.\nIn designing the delta-buffer Flood, it is computationally too expensive to incorporate re-learning\nof parameters (sort dimension and the number of cell partitions). In our implementation, learning\nthe distribution took 32 seconds for the normal distribution dataset, 46 seconds for the Stock Price\ndataset, and 138 seconds for the Open Street Map dataset. Since there are about 106update queries in\nour workload, we need to re-learn more than 102times even if we re-learn every 104times. The total\nre-learning time would then be about 104seconds ≃3 hours, which is too long as shown in Figures 2.\nTherefore, we adopt the method of re-initializing the data structure without changing the parameters.\nIn this experiment, we used Flood10000, which re-initializes the entire structure after every 104data\nupdates, and used Flood20000 with re-initialization after every 2·104updates, in addition to the four\ndata structures described in Section 5. As noted in Appendix D, search queries and update queries\ncome alternately, 104each. Thus, Flood10000 is re-initialized for each set of update queries, and\nFlood20000 stores the first set of update queries in the buffer and re-initializes after the second set.\nFigures 6 illustrate the cumulative query processing time for each data structure for each dataset.\nLooking at the update query results (Figures 6a, 6b, 6c), Flood10000 and Flood20000 are slower\n12\n\nthan all the compared data structures. Especially, both are more than five times slower than FlexFlood.\nFrom this result, the delta buffer Flood is undesirable when there are many update queries.\nWe then turn our attention to the search query results (Figures 6d, 6e, 6f). Flood10000 has the fastest\nsearch speed of all data structures. This is because Flood10000 re-initializes every update query\nset, so Flood10000 can achieve virtually the same performance as Flood, and the original Flood is\nextremely fast, as discussed in Appendix A. On the other hand, Flood20000 is slower than FlexFlood\nfor normal distribution dataset and Stock Prices dataset, but achieves a very fast search speed for\nOpen Street Map dataset. This is likely due to the large amount of data being searched in the Open\nStreet Map dataset (See Appendix D). The time spent searching the internal structure of the Flood\nhas become the rate-limiting step, making the time spent reading from the buffer relatively negligible.\nTherefore, delta-buffer Flood is worth considering in situations where an extremely large amount of\ndata is being searched, or when spending a relatively long time on updates is not an issue. On the\nother hand, we believe that FlexFlood excels in scenarios where high update speed is required or\nwhen the amount of data being searched is relatively small.\n13",
  "textLength": 40573
}