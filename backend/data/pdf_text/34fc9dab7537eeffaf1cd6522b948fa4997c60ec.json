{
  "paperId": "34fc9dab7537eeffaf1cd6522b948fa4997c60ec",
  "title": "Faster Global Minimum Cut with Predictions",
  "pdfPath": "34fc9dab7537eeffaf1cd6522b948fa4997c60ec.pdf",
  "text": "Faster Global Minimum Cut with Predictions\nBenjamin Moseley∗\nCarnegie Mellon University\nmoseleyb@andrew.cmu.eduHelia Niaparast†\nCarnegie Mellon University\nhniapara@andrew.cmu.edu\nKaran Singh\nCarnegie Mellon University\nkaransingh@cmu.edu\nAbstract\nGlobal minimum cut is a fundamental combinatorial optimization problem with wide-\nranging applications. Often in practice, these problems are solved repeatedly on families\nof similar or related instances. However, the de facto algorithmic approach is to solve each\ninstance of the problem from scratch discarding information from prior instances.\nIn this paper, we consider how predictions informed by prior instances can be used to\nwarm-start practical minimum cut algorithms. The paper considers the widely used Karger’s\nalgorithm and its counterpart, the Karger-Stein algorithm. Given good predictions, we\nshow these algorithms become near-linear time and have robust performance to erroneous\npredictions. Both of these algorithms are randomized edge-contraction algorithms. Our\nnatural idea is to probabilistically prioritize the contraction of edges that are unlikely to be\nin the minimum cut.\n1 Introduction\nMachine learning is driving scientific breakthroughs. While this has transformed many areas,\nthere remain domains where machine learning holds immense, yet unrealized potential. One such\narea is the reimagining of computer science foundations through machine learning, particularly\nfor designing faster discrete algorithms .\nA rapidly growing body of work, collectively referred to as algorithms with predictions ,\nfocuses on leveraging machine-learned predictions to overcome worst-case performance barriers.\nIn recent years, hundreds of papers have explored this model, mainly applying it to improve\nquality of solutions produced by online algorithms. The primary challenge in the online set-\nting is uncertainty. Hence, machine learning is naturally suited to this setting. Lykouris and\nVassilvitskii [2021] provide a theoretical framework that characterizes the competitive ratio\nof algorithms based on the quality of machine-learned predictions. Subsequent research has\napplied this model to achieve higher-quality solutions and break worst-case lower bounds in\nvarious problems, including caching [Im et al., 2022, Lykouris and Vassilvitskii, 2021], binary\nsearch Dinitz et al. [2024], scheduling [Lindermayr and Megow, 2022, Lattanzi et al., 2020, Im\net al., 2023], and clustering [Lattanzi et al., 2021]. For a comprehensive overview, see the survey\nby Mitzenmacher and Vassilvitskii [2022].\nInterestingly, the work of Kraska et al. [2018], which arguably initiated this line of work,\nhad a different goal in mind and emphasized improvements in running time , a direction that\nremains underexplored compared to advancements in solution quality. Their empirical results\n∗Supported in part by Google Research Award, an Infor Research Award, a Carnegie Bosch Junior Faculty\nChair, NSF grants CCF-2121744 and CCF-1845146.\n†Supported in part by NSF CCF-1845146.\n1arXiv:2503.05004v1  [cs.DS]  6 Mar 2025\n\nhighlight the significant potential of machine learning to accelerate algorithms and motivate\nexploration of the algorithmic possibilities of using machine learning for runtime improvements.\nTraditionally, computer science solves problems from scratch, with running times analyzed\nusing worst-case scenarios. However, in practice, many problems are repeatedly solved over\ntime. The conventional worst-case model often discards valuable information shared between\ninstances. Given that problem instances frequently exhibit similarities, machine learning offers\nan opportunity to learn a warm-starting state that can enhance algorithmic performance. The\ncommunity has begun to investigate theoretical guarantees for algorithms that use machine-\nlearned warm starts to achieve runtime improvements. Dinitz et al. [2021] initiated this line\nof inquiry by showing how predicted dual variables could accelerate the Hungarian algorithm\nfor bipartite matching. Building on this idea, researchers have developed runtime-improving\nalgorithms for discrete combinatorial optimization problems, such as shortest paths [Chen et al.,\n2022, McCauley et al., 2025], maximum flow [Davies et al., 2023], list labeling data structures\n[McCauley et al., 2024a], and dynamic graph algorithms [McCauley et al., 2024b].\nDespite these advances, this area remains underdeveloped, with significant open questions\nregarding how machine-learned predictions can improve algorithmic runtimes.\nGlobal Minimum Cut. This paper focuses on the global minimum cut problem. Consider\nan undirected graph G= (V, E, w ) with vertex set Vand edge set E, where each edge ehas\na nonnegative weight w(e). We will use mto denote the number of edges and nto denote the\nnumber of vertices. For an edge subset E′⊆E, letw(E′) be the sum of weights of edges in E′,\nand for a subset V′⊆Vof vertices, let δ(V′) be the edges between V′andV\\V′. The goal\nis to find a partition ( S, T) of the vertex set that minimizes w(δ(S)) =P\ne∈E∩(S×T)w(e), thus\nminimizing the total weight of the edges that cross the partition. Occasionally, we will refer to\nthe minimum cut by the edges it contains, rather than by the partition that induces it.\nThis problem has been extensively studied in the literature [Gomory and Hu, 1961, Hao\nand Orlin, 1994, Stoer and Wagner, 1997, Karger, 1993, Karger and Stein, 1996, Karger, 2000,\nGabow, 1995, Kawarabayashi and Thorup, 2018, Henzinger et al., 2020, Saranurak, 2021, Li and\nPanigrahi, 2020, Li, 2021, Henzinger et al., 2024, Chekuri et al., 1997], and has wide-reaching\napplications, e.g., network design and network reliability [Colbourn, 1987], information retrieval\n[Botafogo, 1993], and compiler design for parallel languages [Chatterjee et al., 1995]. Following\na sequence of breakthrough results, the fastest known algorithms for this problem run in near-\nlinear O(mpoly log n) time, even when constrained to be deterministic [Henzinger et al., 2024].\nHowever, known near-linear time algorithms are primarily of theoretical interest and have not\nbeen implemented due to their complexity. In fact, popular graph libraries [Siek et al., 2001,\nHagberg et al., 2008] resort to algorithms that are much slower in theory but easier to implement.\nKarger’s and Karger-Stein Algorithms. Karger’s algorithm [Karger, 1993] and its ex-\ntension, the Karger-Stein algorithm [Karger and Stein, 1996], are two renowned randomized\nalgorithms for finding the global minimum cut. They are frequently used as algorithmic bench-\nmarks [Chekuri et al., 1997]. The practical relevance of Karger’s algorithm draws from its\nsimplicity and its highly parallelizable nature. Given an unweighted graph G= (V, E), at each\niteration, Karger’s algorithm picks an edge e∈Euniformly at random and contracts its end-\npoints, keeping parallel edges but removing self-loops. Once there are only two vertices left, the\npartition of the vertex set formed by these two “metavertices” is returned as a candidate for\nthe minimum cut. This algorithm can be easily extended to weighted graphs, where an edge\neis picked with probability w(e)/w(E), and instead of adding parallel edges, the edge weights\nare summed upon contraction.\nIt can be shown that the cut reported by Karger’s algorithm is actually a minimum cut of\nthe graph with probability at least Ω(1 /n2). Therefore, by repeating Karger’s algorithm O(n2)\ntimes and retaining the best cut across all runs, the algorithm recovers the true minimum cut\n2\n\nwith constant probability. Each run of Karger’s algorithm can be performed in O(m) time; thus\nthe total runtime is O(mn2). However, given a sufficient number of parallel processors, each of\nthese runs can be performed in parallel with zero intermittent communication.\nThe Karger-Stein algorithm considerably boosts the probability of obtaining the minimum\ncut in any given trial to Ω(1 /logn) at the cost of O(n2logn) runtime per trial. The key\nobservation is that the probability that the minimum cut survives a random edge contraction\ndecreases severely the fewer vertices there are left. Therefore, repeated contractions when there\nare too many vertices remaining as in Karger’s algorithm are wasteful. Instead, starting with n\nvertices, Karger-Stein contracts edges at random until there are about n/√\n2 vertices left, and\nthen returns the better of the cuts received by making two independent recursive calls on the\nresultant graph. The net runtime of O(n2log2n) is nearly optimal for dense graphs.\nThe natural questions we ask are: How can Karger’s algorithm and the Karger-Stein algo-\nrithm be improved using predictions? How error-resilient are the resultant prediction-augmented\nalgorithms? What is the right measure of error in such predictions?\n1.1 Results\nThis paper improves the runtime performance of Karger’s algorithm and the Karger-Stein al-\ngorithm using predictions. The first key question is: what should be predicted? The idea is to\npredict which edges are more or less likely to be in the global minimum cut. Of course, these\npredictions could be erroneous. We introduce new variants of these algorithms that robustly\nuse these predictions.\nPredictions. LetC∗⊆Ebe a minimum cut in G. Since a cut ultimately is a subset of edges\nthat cross some partition of the set of vertices, let us begin by considering binary predictions\nfor each edge e∈Eindicating whether or not e∈C∗. LetbC⊆Ebe the predicted minimum\ncut. Note that bCmay not necessarily be a cut.\nClearly, any edge in the symmetric set-difference bC△C∗constitutes an error. The majority\nof error measures considered in the algorithms with predictions literature are symmetric (e.g.,\nin Mitzenmacher and Vassilvitskii [2022], Dinitz et al. [2021]); they penalize equally for over-\nand under- prediction. However, an important feature of our work is to disentangle the impact\nof these two types of errors on the algorithm’s runtime. Concretely, the prediction error can be\ndivided into false negatives ( η), and false positives ( ρ). We define ηas the ratio of the weight\nof the edges in the minimum cut but not in the prediction to the weight of the minimum cut\nandρas the ratio of the weight of the edges in the prediction but not in the minimum cut to\nthe weight of the minimum cut:\nη:=w(C∗\\bC)\nw(C∗), ρ :=w(bC\\C∗)\nw(C∗).\nWe will see that a false negative is far more costly than a false positive in boosting the runtime\nof Karger-like algorithms.\nIn fact, we prove and state our results in a more general setting that cleanly generalizes\nthe above definitions to real-valued predictions. Here, each edge ehas an associated prediction\npe∈[0,1], representing the probabilistic prediction of its inclusion in C∗. Now, ηandρare\ndefined as:\nη:=P\ne∈C∗(1−pe)w(e)\nw(C∗), ρ :=P\ne∈E\\C∗pew(e)\nw(C∗).\nIn the analysis, we will only use that ηandρare valid upper bounds for the quantities\ndefined above, and, for simplicity, we assume that ρis at least 1. We note that if there is more\nthan one global minimum cut, ηandρcan be defined with respect to any fixed minimum cut.\n3\n\nDue to this, our run-time guarantees hold with respect to the minimum cut that gives the best\nrun-time in terms of ηandρ.\nBoosted Karger’s Algorithm. Our intuitive approach to taking advantage of predictions\nis to tweak the graph so that Karger’s algorithm has a higher chance of contracting the edges\nthat are not predicted to be in the minimum cut, so that the minimum cut has a higher chance\nof surviving.\nA reasonable guess a priori is that the amount of computational work needed to compute\nthe minimum cut (and hence, the runtime) scales linearly in the quality of predictions, e.g.\nknowing about half of the edges in the minimum cut (given no false positives) reduces the total\nwork by a factor of half. However, we show that the improvement is far more stark, and such\npredictions can eliminate an entire factor of nfrom the runtime. Thus, even for fixed prediction\nquality, the multiplicative speed-up grows with the size of instance.\nWe prove the following theorem, demonstrating the potential for significant improvement in\nthe running time of Karger’s algorithm, provided that the predictions are not too erroneous.\nTheorem 1.1. For a suitable setting of parameters, given predictions measured by ηandρ\nas defined above, the Boosted Karger’s algorithm (Algorithm 1) outputs a minimum cut with\nprobability at least\nΩ\u00121\nn2ηρ2(1−η)\u0013\n.\nLet us compare this to the Ω(1 /n2) probability of recovering the true minimum cut in the\nstandard Karger’s algorithm. Regardless of the value of η, which is always in [0 ,1], the result\nin Theorem 1.1 is better than Karger’s algorithm as long as ρ≤o(n). Thus, the result shows\nremarkable resiliency to error: Even if none of the minimum cut edges is in the prediction,\nand the predicted edges are almost ntimes as many as the minimum cut, the probability of\nrecovering the minimum cut is no worse than Karger’s algorithm.\nTo see the utility of this result, consider when the error is small, e.g. if ρis a constant, then\nthe probability of success of Boosted Karger’s algorithm is Ω\u0000\n1/n2η\u0001\n, which is significantly\nbetter than that of Karger’s algorithm, Ω\u0000\n1/n2\u0001\n.\nBoosting the Karger-Stein Algorithm. For Karger-Stein, it is important first to carve\nout the possible regime of improvement. For dense graphs, that is, if m= Θ( n2), Karger-Stein\nis already nearly optimal. For sparse graphs, the best one may hope for is a near-linear runtime\nofO(m). Therefore, depending on the quality of the predictions, one may hope to interpolate\nthese. This is what our results deliver.\nAs we will see, our earlier result relied on improving the minimum cut’s probability of\nsurviving a single random edge contraction. The Karger-Stein analysis is not directly well\nsuited to make use of this effect. Instead, we adapt a variant, here eponymously termed FPZ,\nintroduced in Fox et al. [2019], who obtain Karger-Stein-style guarantees for finding minimum\ncuts in hypergraphs. Their analysis was greatly simplified recently by Karger and Williamson\n[2021], which we borrow. The difference in FPZ vs. Karger-Stein is that the former executes\na single edge-contraction in each step, but makes a random number of recursive calls; the\npropensity of these is closely tied to the survival probability of a minimum cut during an edge\ncontraction. Here, in addition to tweaking the graph so that random edge contractions are more\nlikely to contract edges outside the predicted set, we modify the propensity for these recursive\ncalls. In the end, we prove the following.\nTheorem 1.2. For a suitable setting of parameters, given predictions measured by ηandρas\ndefined above, the Boosted FPZ algorithm runs in time\n(\nO(m1−ηn2ηlogn) ifρ=O(√m),and\nO(ρ2(1−η)n2ηlogn)otherwise.\n4\n\nIt outputs a minimum cut with probability at least Ω(1\nlogn).\nFor a large and forgiving regime of false positives, when ρ=O(√m), the running time\nmultiplicatively interpolates that of Karger-Stein and a near-linear time algorithm, depending\non the value of η. For small η, it is almost linear-time. In fact, the improvement over Karger-\nStein persists regardless of the value of ηas long as ρ≤o(n).\nWe note that our running-time analysis, along with the underlying data structures support-\ning the implementation, differs from prior work. Importantly, our recurrence analysis is careful\nas to how many edges are processed in each iteration, effectively amortizing the total work done\nacross multiple levels of recursion.\nLearning Near-Optimal Predictions. We also give a learning algorithm to learn near-\noptimal predictions from solutions to past instances. Specifically, given a distribution over\ngraphs, from which the learning algorithm can draw samples, we show how near-optimal pre-\ndictions minimizing average runtime over the distribution may be computed in polynomial time\nand sample complexity.\nExperiments. We conduct three sets of experiments ranging from synthetic to real datasets.\nLimitations. One limitation of our approach is that the setting of the parameters needed\nfor the theoretical results depends on the knowledge of ηandρ(or at least on upper bounds\nfor them). Given a family of instances, it might be possible to conservatively estimate these\nparameters, but we do not pursue this here. However, in our experiments, we do not assume\naccess to such information and apply the same problem-agnostic parameters uniformly across\ninstances. Our empirical results strongly suggest that the algorithms are insensitive to these\nparameters for a wide-ranging degree of errors, and this might not be a limitation in practice.\n2 Boosted Karger’s Algorithm\nWe discuss the Boosted Karger’s algorithm, and we prove an improved lower bound for its\nprobability of success. The algorithm has two parameters, a scalar and a threshold. The\nalgorithm boosts the edges in E\\bC, meaning it multiplies the weights of the edges that fall\noutside the predicted set by a large scalar and then performs random edge contractions similarly\nas in Karger’s algorithm provided there is a sufficient number of vertices remaining. At this\npoint, each remaining vertex (or properly, metavertex ) corresponds to a subset of the original\nvertex set. When fewer vertices remain than the specified threshold, our algorithm reverts to\nthe standard (i.e., not boosted ) Karger’s algorithm on the remaining metavertices.\nAlgorithm 1 Boosted Karger’s Algorithm\n1:Input: graph G= (V, E, w ), predictions {pe}e∈E.\n2:Parameters: scalar B, threshold t.\n3:Build GB= (V, E, w B), where wB(e) = (1 + ( B−1)(1−pe))w(e).\n4:while there are > tvertices left in GBdo\n5: Pick an edge ¯ ewith probability wB(¯e)/P\ne∈E(GB)wB(e) and contract it.\n6:end while\n7:Define G′= (V′, E′, w) := subgraph of Ginduced by the remaining tmetavertices in GB.\n8:while there are at least 3 vertices left in G′do\n9: Pick an edge ¯ ewith probability w(¯e)/P\ne∈E(G′)w(e) and contract it.\n10:end while\n11:return the set of edges in Gbetween the two remaining metavertices.\n5\n\nWe refer to the steps on lines 3-6 of the algorithm above as the first phase , and the execution\nof the standard Karger’s algorithm on lines 7-10 as the second phase . For brevity of notation,\nwe define the sequence\nqi:= 1−1 + (B−1)η\nBi/2−(B−1)(ρ+ (1−η)).\nWe begin the analysis by establishing the survival probability of a fixed minimum cut during\na single randomized edge contraction.\nLemma 2.1. Fix a weighted graph G, and let C∗⊆E(G)be a minimum cut in G, with respect\nto which ηandρare defined. Now consider a weighted graph G′= (V, E, w )with kvertices\nobtained by a sequence of edge contractions starting from Gsuch that no edge from C∗has been\ncontracted in any of these contractions. Let wB(e) = (1 + ( B−1)(1−pe))w(e)for all edges e\ninE. Then the probability that none of the edges in C∗is contracted in a single randomized\nedge contraction in G′, where an edge eis chosen with probability wB(e)/wB(E), is at least qk,\nas long as k≥2ρ+ 2.\nProof. Let us start by observing that the probability that C∗remains intact after a random\nedge contraction is 1 −wB(C∗)/wB(E). Now we have\nwB(C∗) =X\ne∈C∗(1 + ( B−1)(1−pe))w(e) =w(C∗) (1 + ( B−1)η).\nAdditionally, for the surviving edges E, we have\nwB(E) =X\ne∈E(1 + ( B−1)(1−pe))w(e)\n=X\ne∈E(B−(B−1)pe)w(e)\n≥Bw(E)−(B−1)(ρw(C∗) + (1 −η)w(C∗)),\nwhere the last derivation is an inequality for the sole reason that not all of the original false\npositive edges may have survived by this stage, that is, in earlier contractions used to arrive at\nG′. Note that by now, since each (meta) vertex vcorresponds to a cut in the original graph\nG,w(δ(v))≥w(C∗), where δ(v) is the set of edges incident on v. Since every edge has two\nvertices, 2 w(E) =P\nv∈Vw(δ(v)). Therefore, we have\nwB(E)≥B\u0012kw(C∗)\n2\u0013\n−(B−1) (ρw(C∗) + (1 −η)w(C∗))\n=w(C∗)\u0012Bk\n2−(B−1) (ρ+ (1−η))\u0013\n.\nWe can now write\n1−wB(C∗)\nwB(E)≥1−w(C∗) (1 + ( B−1)η)\nw(C∗) (Bk/2−(B−1) (ρ+ (1−η)))=qk.\nNext, we state and prove the following elementary inequality that will be used to prove the\nmain result of this section.\nLemma 2.2. For all t≥2ρ+ 2, it holds\nnY\ni=t+1qi≥\u0012t−2ρ−2\nn\u00132(η+1−η\nB)\n.\n6\n\nProof. We first apply the inequality 1 −x≥e−x\n1−x, which holds for all x <1, to get\nnY\ni=t+1\u0012\n1−1 + (B−1)η\nBi/2−(B−1)(ρ+ (1−η))\u0013\n≥exp \n−nX\ni=t+11 + (B−1)η\nB(i/2−1)−(B−1)ρ!\n= exp \n−n−2X\ni=t−11 + (B−1)η\nBi/2−(B−1)ρ!\n.\nNote that for a non-decreasing function f, we haveRU\nL−1f(x)dx≤PU\ni=Lf(i). Therefore, we\ncan write\nexp \n−n−2X\ni=t−11 + (B−1)η\nBi/2−(B−1)ρ!\n≥exp\u0012\n−Zn−2\nt−21 + (B−1)η\nBx/2−(B−1)ρdx\u0013\n.\nThe condition t≥2ρ+ 2 ensures that fis non-decreasing in the desired interval. From here,\nwe just need to carry out the calculations and simplify the expressions:\nexp\u0012\n−Zn−2\nt−22 + 2( B−1)η\nBx−2(B−1)ρdx\u0013\n= exp\u0012\n−(2 + 2( B−1)η)Zn−2\nt−2dx\nBx−2(B−1)ρ\u0013\n= exp\u0012\n−\u00122 + 2( B−1)η\nB\u0013\nln (Bx−2(B−1)ρ)|n−2\nt−2\u0013\n=\u0012B(t−2)−2(B−1)ρ\nB(n−2)−2(B−1)ρ\u0013\u0010\n2+2(B−1)η\nB\u0011\n=\u0012B(t−2−2ρ) + 2ρ\nBn−2B−2(B−1)ρ\u00132(η+1−η\nB)\n≥\u0012B(t−2−2ρ)\nBn\u00132(η+1−η\nB)\n.\nWe are now ready to prove the following theorem:\nTheorem 2.3. LetC∗⊆Ebe a minimum cut in the weighted input graph G= (V, E, w ), with\nrespect to which ηandρare defined. Then, assuming t≥2ρ+ 2, the probability that none of\nthe edges of C∗are contracted in the first phase of Algorithm 1 is at least\u0010\nt−2ρ−2\nn\u00112(η+1−η\nB).\nProof. LetEibe the event that none of the edges of C∗are contracted in step iof the algorithm.\nAt the start of step i, there are n−i+ 1 remaining vertices. Now, by Lemma 2.1, we have\nPr(Ei|{Ej}j<i)≥qn−i+1.\nLetAbe the event that none of the edges of C∗are contracted in the first phase of the\nalgorithm, that is, until there are at least tvertices left. Then, we have\nPr(A) = Pr( E1∩E2∩ ··· ∩ En−t)\n= Pr( E1) Pr(E2|E1)···Pr(En−t|E1E2···En−t−1)\n≥nY\ni=t+1qi.\nTo conclude the claim, we invoke Lemma 2.2.\nTheorem 1.1 can now be obtained from Theorem 2.3 by choosing tto be the smallest integer\nexceeding 3 ρ+2 and any B= Ω(log n), because in the second phase Karger’s algorithm ensures\nthat the minimum cut has at least Ω(1 /t2) probability of continued survival.\n7\n\n3 Boosting the Karger-Stein Algorithm\nIn this section, we present a variant of the FPZ algorithm due to Fox et al. [2019] that utilizes\npredictions to improve over the running time of Karger-Stein.\nThe standard FPZ algorithm is the following. In the algorithm, q′\nn:= 1−2/nis a lower\nbound on the probability that a fixed minimum cut C∗remains intact after a single random\nedge contraction on nvertices, assuming none of its edges have been contracted thus far.\nAlgorithm 2 FPZ(G, n)\n1:Input: graph G= (V, E, w ) with nvertices.\n2:Parameters: branching factor q′\nn.\n3:ifn= 2then\n4:return the set of edges in Gbetween the two remaining metavertices.\n5:end if\n6:Pick an edge ¯ ewith probability ∝w(¯e), that is, with probability w(¯e)/P\ne∈Ew(e).\n7:Contract ¯ einGto produce G′.\n8:C1←FPZ(G′, n−1).\n9:With probability q′\nn,\n10: return C1.\n11:Otherwise\n12: C2←FPZ(G, n).\n13: return the cut from {C1, C2}with the smaller weight.\nThe boosted variant is as follows.\nAlgorithm 3 BoostedFPZ (G, n, p )\n1:Input: graph G= (V, E, w ) with nvertices, predictions {pe}e∈E.\n2:Parameters: scalar B, threshold t, branching factor qn.\n3:ifn= 2then\n4:return the set of edges in Gbetween the two remaining metavertices.\n5:else if n≤tthen\n6:return FPZ(G, n). // In other words, run the standard ( non-boosted ) FPZ algorithm.\n7:end if\n8:LetwB(e) := (1 + ( B−1)(1−pe))w(e) for all e∈E.\n9:Pick an edge ¯ ewith probability wB(¯e)/P\ne∈EwB(e) and contract it to produce G′.\n10:C1←BoostedFPZ (G′, n−1, p).\n11:With probability qn,\n12: return C1.\n13:Otherwise\n14: C2←BoostedFPZ (G, n, p ).\n15: return the cut from {C1, C2}with the smaller weight.\nIn the algorithm above, qnrepresents a lower bound on the probability that a fixed minimum\ncutC∗remains intact after a single random boosted edge contraction on nvertices, assuming\nnone of its edges have been contracted thus far. Given prediction pe∈[0,1] for each edge e, we\napply our previous idea of reweighting the edges with parameter Bto encourage the contraction\nof edges that lie outside the predicted set. We also add a switching point tto the algorithm,\nas we did before. Whenever fewer than tvertices remain in the graph, where t≥3ρ+ 2, the\nalgorithm then invokes the standard FPZ algorithm. This time, we use\nqn:= 1−1 + (B−1)η\nBn/2−(B−1) (ρ+ (1−η)),\n8\n\nwhere ηandρare defined with respect to C∗, for n > t , and qn:= 1−2/nforn≤t. We\nrefer to this modified version of the FPZ algorithm as the Boosted FPZ. First, we establish the\nfollowing lower bound on the success probability of the Boosted FPZ algorithm.\nTheorem 3.1. For any threshold tsatisfying t≥3ρ+2, the probability that Algorithm 3 returns\na minimum cut is Ω\u0010\n1\nlogt+ηlog(n/t)\u0011\n.\nProof. LetC∗be the minimum cut in Gthat is used to define ηandρ, and let P(i) denote\na lower bound on the probability that the algorithm returns C∗, given that all edges of C∗\nhave survived contractions up to the point where ivertices are left. Once there are fewer than\ntremaining vertices, Algorithm 3 proceeds identically to the standard FPZ algorithm. Thus,\nutilizing the result from Karger and Williamson [2021], we have that P(t) = 1 /(2Ht−2).\nFor the first phase, that is, when there are at least tremaining vertices, we will be able to\nreuse the following recurrence for P(n) from Karger and Williamson [2021], although the value\nof the branching factor (compare q′\nnandqn) is now different.\nP(n) =q2\nnP(n−1) + (1 −qn)(1−(1−P(n))(1−qn·P(n−1))).\nAs observed in Lemma 2.1, qnis a lower bound on the probability that C∗survives yet another\nrandomized edge contraction. The recurrence is derived by noting that with probability qn, line\n12 is executed, after which the probability of returning a minimum cut is at least qn·P(n−1).\nWith the remaining probability 1 −qn, both paths to computing the minimum cut through\nrecursive calls on instances of size nandn−1 must fail for the algorithm to miss the minimum\ncut.\nThis recurrence can be simplified to:\n1\nP(n)=1\nP(n−1)+ 1−qn.\nUnrolling the recurrence, we get\n1\nP(n)=1\nP(t)+nX\ni=t+1(1−qi)\n= 2Ht−2 +nX\ni=t+11 + (B−1)η\nBi/2−(B−1) (ρ+ (1−η))\n≤2Ht−2 +Zn\nt1 + (B−1)η\nBi/2−(B−1) (ρ+ (1−η))di\n= 2Ht−2 +2 (1 + ( B−1)η)\nBln\u0012Bn/2−(B−1)(ρ+ (1−η))\nBt/2−(B−1)(ρ+ (1−η))\u0013\n=O\u0010\nlogt+ηlogn\nt\u0011\n,\nwhere we use the fact thatRU\nL−1f(x)dx≤PU\ni=Lf(i) holds for any non-decreasing function f,\nand in particular for f(x) =−1/x. This concludes the proof.\nNext, we will analyze the running time of the algorithm. Let us take a moment to revisit\na textbook implementation of Karger’s algorithm that runs in near-linear time using Kruskal’s\nalgorithm. Recall, Kruskal’s algorithm is typically used for finding minimum spanning trees.\nIn the unweighted case, we start by creating a uniformly random permutation of all edges\nand processing them sequentially. Throughout the algorithm, we use a union-find data structure\nto check which nodes have been merged. This is used in the standard fast implementation of\nthe traditional Karger’s algorithm. When processing an edge in this order, like in Kruskal’s\n9\n\nalgorithm, any edge with both endpoints in the same connected component is discarded. If\nan edge’s endpoints are in different components, the union-find data structure merges these\nnodes. The partition of vertices formed just before merging the last two connected components,\nis returned as the minimum cut. This approach can also be extended to the weighted case, e.g.,\nusing the Gumbel trick.\nA similar implementation can be performed for the Boosted FPZ algorithm. This will in\nturn enable the efficient run-time of our algorithm. The implementation of each random edge\ncontraction must be carried out in two cases, each utilizing a different data structure. The data\nstructure used is based on the number of remaining vertices, n.\nIfn > t , a union-find data structure is used and the edges are sampled lazily without\nreplacement, with probabilities proportional to their boosted weights. This is done until an\nedge is found whose endpoints belong to different components. For sampling, a categorical\ndistribution over edges can be maintained online, for example, using a red-black tree, while\nallowing sampling without replacement in O(logm) time. To make recursive calls on the same\ngraph, it is too inefficient to copy the graph and run recursive calls separately. Instead, the\nalgorithm runs one call and then later returns to a possibly second recursive call, in a depth-first\nmanner over the recursion tree. We utilize a union-find data structure with deletions, which\nalso takes O(logm) time in the worst case per operation [Alstrup et al., 2014].\nIfn≤t, we switch to an adjacency list data structure, which allows a random edge con-\ntraction in time proportional to the number of remaining vertices, as suggested in Karger and\nWilliamson [2021]. When switching between these two regimes, we prune the list of remaining\nedges in O(mlogm) time to ensure that there are at most t2remaining edges. Once we are in\nthe second phase, Algorithm 3 is identical to the FPZ algorithm, the total run-time thereafter\nisO(t2logt).\nProof of Theorem 1.2. LetT(k, ℓ) be an upper bound on the expected running time of the\nalgorithm on any call with kvertices and ℓedges left to process. Since we switch to the\nstandard FPZ algorithm at tvertices, for all ℓ′, we have T(t, ℓ′) =O(t2logt), which is the\nexpected running time of the FPZ [Karger and Williamson, 2021]. Note that, as mentioned\nabove, we can assume ℓ′≤t2in this case.\nThen, we have the following recurrence for T(k, ℓ), carefully considering the number of edges\nprocessed in each iteration, via the union-find data structure, to carry out one edge contraction.\nThe recursive expression can be explained as follows: in any call, the algorithm processes ℓ−ℓ′\nedges for some ℓ′, taking ( ℓ−ℓ′)·O(logn) time. The algorithm then makes a recursive call on\nk−1 vertices and ℓ′edges. Furthermore, with probability (1 −qk), the algorithm repeats itself\non the input graph. For the analysis, we take the maximum over all possible values of ℓ′to\nconsider the worst case for the algorithm.\nT(k, ℓ)≤max\n1≤ℓ′<ℓ{T(k−1, ℓ′) + (1 −qk)T(k, ℓ) + (ℓ−ℓ′)·O(logn)}.\nThis inequality can be simplified to:\nT(k, ℓ)≤1\nqkmax\n1≤ℓ′<ℓ{T(k−1, ℓ′) + (ℓ−ℓ′)·O(logn)}.\nUnfolding the right-hand side, we get:\nT(n, m)≤max\nℓi(nX\ni=t+1ℓi·O(logn)Q\ni≤j≤nqj:nX\ni=t+1ℓi≤m)\n+O \nt2logtQ\nt+1≤j≤nqj!\n≤O(mlogn+t2logt)Q\nt+1≤j≤nqj.\n10\n\nUsing Lemma 2.2 to lower bound the product of {qn}, by setting B= Ω(log n), for any t≥3ρ+2,\nwe get\nT(n, m) =O\u0012mlogn+t2logt\n(t/n)2η\u0013\n.\nSetting t= max {⌈3ρ+ 2⌉,√m}concludes the claim.\n4 Learning Near-Optimal Predictions\nIn this section, we describe how near-optimal predictions can be learned from past data. For-\nmally, we assume that there is an unknown fixed distribution Don weighted graphs that share\nthe same set Vof vertices, from which a number of samples are drawn independently. Given\nthese samples, our goal is to learn near-optimal predictions p∗∈[0,1](V\n2)that minimize the\nexpected runtime of the Boosted Karger’s algorithm with respect to D.\nLetC∗(G) denote a minimum cut in G. For a prediction p, letη(G, p) and ρ(G, p) denote\nthe false negative and false positive with respect to C∗(G) and p, respectively. Note that we\ncan assume, without loss of generality, that 0 ≤wG(e)≤1 for all e∈E(G). Otherwise, we\ncan scale all edge weights so that they are within the interval [0 ,1], and this would not change\nC∗, η,andρ. Since the edge sets of the sampled graphs may differ, we will assume the vector\nof predictions pis defined over\u0000V\n2\u0001\n.\nWe have established that the expected running time of the Boosted Karger’s algorithm\nis at most R(G, p) := n2η(G,p)ρ(G, p)2(1−η(G,p)). A natural strategy for learning near-optimal\npredictions is to compute predictions that minimize this running time upper bound averaged\nover collected samples. However, R(G, p) is nonconvex in p. Instead, we aim to optimize\nU(G, p) :=n2η(G,p)˜ρ(G, p)2. Here ˜ ρis a variation of ρdefined as:\n˜ρ(G, p) :=(1−w∗(G))⊤p\nwG(C∗(G)),\nwhere w∗(G) is the characteristic weight vector of the minimum cut C∗(G), that is, it is a\nvector in [0 ,1](V\n2), with its entry corresponding to eequal to wG(e) ife∈C∗(G), and zero\notherwise. Note that ˜ ρ(G, p)≥ρ(G, p) for all p, and therefore U(G, p) is a valid upper bound\nonR(G, p). It is instructive to compare ρand ˜ρfor unweighted graphs. For unweighted graphs,\nρ(G, p) =P\ne∈E(G)\\C∗(G)pe/wG(C∗(G)) and ˜ ρ(G, p) =P\ne∈(V\n2)\\C∗(G)pe/wG(C∗(G)). Thus,\nthe principal difference between the two is that ˜ ρ(G, p) additionally accounts for erroneous\npredictions that correspond to missing edges.\nUnfortunately, U(G, p) is also not convex in p(see Proposition A.1). Our key observation\nis that upon replacing 1⊤p, which appears naturally in the definition of ˜ ρ, with a free variable,\nthe resultant analogue of U(G, p) becomes convex in p(see Proposition A.3). Since 1⊤pis an\ninstance-independent scalar, its best value can be estimated through a grid search, in addition\nto running copies of online gradient descent on pcorresponding to all possible values of the free\nvariable in the grid. In Appendix A, we prove:\nTheorem 4.1. For any ε, δ > 0, there exists an algorithm with the following properties. Given\npoly(n,1/Cmin,log(1/εδ))/ε2i.i.d samples from any distribution D, satisfying that Cminis a\nlower bound on the size of the minimum cut of any graph in D’s support, the algorithm runs in\npoly(n,1/ε,1/Cmin,log(1/δ))time and outputs a prediction ¯p∈[0,1](V\n2)such that, with proba-\nbility at least 1−δ, we have\nEG∼D[U(G,¯p)]−arg min\np∈[0,1](V\n2)EG∼D[U(G, p)]≤ε.\n11\n\nThe time and sample complexity above scale with 1 /Cmin. Such dependencies occur regularly\nwhile learning real-valued functions without uniformly bounded derivatives (see, e.g., Chapter\n4 in Hazan et al. [2016]). For unweighted graphs, one may always assume that Cmin≥1.\n5 Experiments\nWe aim to demonstrate that the theoretical advantages presented in the previous sections also\ntranslate to improved empirical performance. We perform three sets of experiments1. The first\ninvolves synthetic settings where we can explicitly control the fidelity of predictions to study the\nperformance of the algorithm quantitatively. The second is a setting where Karger’s algorithm is\nused to repeatedly find minimum cuts on a family of instances that organically arise from trying\nto solve traveling salesperson (TSP) instances. We conclude with experimental comparisons on\nreal data.\n5.1 Controlled Experiments on Synthetic Graphs\nIn the following set of experiments, we explicitly control the prediction quality and measure the\nperformance of the proposed algorithm against Karger’s algorithm on a family of synthetically\ngenerated graphs. We are especially interested in:\n1. How the Boosted Karger’s algorithm compares to the original Karger’s algorithm, espe-\ncially on instances where the latter requires many repetitions to succeed.\n2. How the asymmetric error measures ηandρaffect the number of trials that the Boosted\nKarger’s algorithm needs to find the minimum cut.\nA bipartite graph Gwith nvertices is built as follows. Each partite set has n/2 vertices, and\nthe edges consist of random perfect matchings. First, krandom perfect matchings are added\nto the graph, and then an arbitrary vertex is picked and ℓof its incident edges are randomly\nchosen and removed from the graph. These instances are designed to be difficult for Karger-\nlike algorithms because they contain many near-minimum-cut-sized cuts, each of which has a\nhealthy probability of survival via random edge contractions.\nTo generate predictions, we first compute the true minimum cut C∗onG. Now for any given\nηandρ, we pick two random subsets Cη⊆C∗andCρ⊆E\\C∗, such that w(Cη) =ηw(C∗)\nandw(Cρ) =ρw(C∗). Our predicted edge set is bCη,ρ=C∗\\Cη∪Cρ.\nWe build Gwith n= 600 , k= 100 , ℓ= 10. We note the number of trials that Karger’s\nalgorithm needs on Gto find the minimum cut. This is our baseline. Next, we fix a value of\nρ∈ {0,10,100}, and for each value of η∈ {0,0.05,0.1, . . . , 1}, we measure the number of trials\nBoosted Karger’s needs to find the minimum cut with input ( G,bCη,ρ) with ( B, t) = ( n,2). In\nFigure 1, this process is repeated 30 times.\nWe can see that the Boosted Karger’s algorithm outperforms Karger’s algorithm by two\norders of magnitude when η≤0.5 and ρ∈ {0,10}. Furthermore, even for ρ= 100, indicating\nespecially poor prediction quality, since the predicted set of edges is about a hundred times as\nnumerous as the size of the minimum cut, Boosted Karger’s algorithm is better by one order of\nmagnitude when η∈[0,0.6].\n5.2 Minimum Cut Instances from the TSP LP\nIn the second set of experiments, we explore instances in which predictions appear naturally.\nCutting plane algorithms for the traveling salesperson problem (TSP) proceed by repeatedly\n1The Python implementation of the experiments is available at https://github.com/helia-niaparast/global-\nminimum-cut-with-predictions.\n12\n\n0.0 0.2 0.4 0.6 0.8 1.0\n0100200300400500Number of Repetitions = 0\nKarger\nBoosted\n0.0 0.2 0.4 0.6 0.8 1.0\n0100200300400500600Number of Repetitions = 10\nKarger\nBoosted\n0.0 0.2 0.4 0.6 0.8 1.0\n010002000300040005000Number of Repetitions = 100\nKarger\nBoostedFigure 1: A controlled experimental comparison of the number of repetitions Boosted Karger’s\nalgorithm needs to find the mincut vs. the standard Karger’s algorithm for different quality of\npredictions, as parameterized by ηandρ.\nidentifying subtour elimination constraints in the subtour linear program relaxation for TSP,\nthe search for which can be recast as finding global minimum cuts (see, e.g., Chekuri et al.\n[1997]). We use this practical use case of Karger’s algorithm to evaluate the performance of the\nBoosted Karger’s algorithm.\nThe subtour elimination approach to TSP starts by minimizingP\ne∈Ewexesubject toP\ne∈N(v)xe= 2 for all nodes v∈Vand 0 ≤xe≤1 for all e∈E, to obtain an initial so-\nlution x0. This linear program is known as the subtour relaxation. Then, a new graph G0is\nbuilt with the same set of vertices and edges as the original, but with the difference that the\nweight of edge einG0isx0\ne. Note that if the entire vector x0is integral, then x0represents a\nHamiltonian cycle, and the size of the minimum cut in G0is 2.\nThe issue is that the edges maybe fractional and the goal is to find a constraint to add to\nthe program based on x0. Upon finding a minimum cut in G0, if its size is smaller than 2, the\nfollowing subtour elimination constraints are added to the above LP and the LP is solved again.\nX\ne={u,v}\nu,v∈S1xe≤ |S1| −1,andX\ne={u,v}\nu,v∈S2xe≤ |S2| −1,\nwhere ( S1, S2) is the vertex partition for the minimum cut found in G0. This process is repeated\nuntil the minimum cut in the current graph is of weight 2. Thus, global minimum cuts are used\nto generate constraints for the subtour relaxation.\nTo begin, we first construct a TSP instance. A random graph G= (V, E) with nvertices is\nbuilt as follows. The vertices are partitioned into two subsets SandTof equal size, and the\nedge set consists of a number of random cycles. First, krandom Hamiltonian cycles are added\ntoGwith the guarantee that each cycle crosses the partition ( S, T) in exactly two edges. Then,\nkrandom cycles of length n/2 are added to each of SandT. Finally, εksmaller random cycles,\neach having a random length between 3 and n/2−1, are added to G, making sure that these\ncycles do not cross the partition.\nWe obtain a sequence of graphs G0, G1, . . . , G ℓ, for which we want to find the minimum cut.\nWe predict that none of the edges with integer weights appear in the minimum cut. Therefore,\nfor each graph Gi, the predicted minimum cut bCiis the set of all edges with fractional weights.\nThese are natural and easily computable predictions.\nWe build Gwith n= 500 , k= 50 , ε= 0.5, and construct G0, . . . , G ℓ. Then, we do the\nfollowing steps for each i∈[ℓ]. On each minimum cut instance we obtain, we measure the\nnumber of iterations needed to produce the minimum cut for Karger’s and for Boosted Karger\non (Gi,bCi). We set ( B, t) = (log n,2). These steps are repeated 10 times.\nIn Figure 2A, we evaluate both algorithms and observe that the Boosted Karger’s algorithm\nconsistently outperforms Karger’s algorithm. In particular, it achieves an order-of-magnitude\n13\n\nimprovement on the harder instances where Karger’s algorithm requires many repetitions to\nfind the minimum cut.\n0 2000 4000 6000 8000\nNumber of Repetitions0255075100125150175200Number of Instances Solved\nKarger\nBoosted\nKarger Boosted50100150200250300350400Number of Repetitions\nk = 250sanr400-0-7\nKarger Boosted101520253035Number of Repetitions\nk = 55bn-mouse_brain_1\nKarger Boosted100150200250300Number of Repetitions\nk = 300frb30-15-5\nFigure 2: In Figure 2A, on the left, we compare the number of minimum cuts arising from the\nsubtour TSP that Karger’s and Boosted Karger’s algorithms solved within a given number of\nrepetitions. On the right, in Figure 2B, is demonstrated the number of repetitions needed to\nrecover the minimum cut on three real graph datasets.\n5.3 Real Datasets\nFinally, we compare the performance of the Boosted Karger’s algorithm and the standard variant\non three real datasets from Rossi and Ahmed [2015]. For each dataset, the predictions are\nobtained by first randomly sampling half of the edges and then performing kparallel runs of\nKarger’s algorithm on the sampled edges. The predicted edge set is formed by the union of the\nedges of the kcuts found by Karger’s algorithm. As a heuristic, we pick kto be close to the\nminimum degree of the graph. This process is repeated 100 times in Figure 2B. We observe\nthat for all three datasets the Boosted Karger’s algorithm requires discernibly fewer number of\ntrials to find the minimum cut.\n6 Conclusion\nWe explored how predictions about the minimum cut can be impactful in boosting the perfor-\nmance of Karger’s and the Karger-Stein algorithms. Furthermore, we empirically demonstrated\nthat the Boosted Karger’s algorithm outperforms Karger’s algorithm even when predictions\nhave a relatively high error. The paper shows Karger’s algorithm can naturally be improved\nwith predictions, and a natural direction for future research is to explore how predictions may\nbe applied to speed up other combinatorial optimization problems.\nReferences\nStephen Alstrup, Mikkel Thorup, Inge Li Gørtz, Theis Rauhe, and Uri Zwick. Union-find with\nconstant time deletions. ACM Trans. Algorithms , 11(1):6:1–6:28, 2014. doi: 10.1145/2636922.\nURL https://doi.org/10.1145/2636922 .\nRodrigo A Botafogo. Cluster analysis for hypertext systems. In Proceedings of the 16th annual\ninternational ACM SIGIR conference on Research and development in information retrieval ,\npages 116–125, 1993.\n14\n\nSiddhartha Chatterjee, John R Gilbert, Robert Schreiber, and Thomas J Sheffler. Array distri-\nbution in data-parallel programs. In Languages and Compilers for Parallel Computing: 7th\nInternational Workshop Ithaca, NY, USA, August 8–10, 1994 Proceedings 7 , pages 76–91.\nSpringer, 1995.\nChandra Chekuri, Andrew V Goldberg, David R Karger, Matthew S Levine, and Clifford Stein.\nExperimental study of minimum cut algorithms. In SODA , volume 97, pages 324–333, 1997.\nJustin Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental graph al-\ngorithms via learned predictions. In International Conference on Machine Learning , pages\n3583–3602. PMLR, 2022.\nCharles J Colbourn. The combinatorics of network reliability . Oxford University Press, Inc.,\n1987.\nSami Davies, Benjamin Moseley, Sergei Vassilvitskii, and Yuyan Wang. Predictive flows for\nfaster ford-fulkerson. In International Conference on Machine Learning , pages 7231–7248.\nPMLR, 2023.\nMichael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii.\nFaster matchings via learned duals. Advances in neural information processing systems , 34:\n10393–10406, 2021.\nMichael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, Aidin Niaparast, and Sergei\nVassilvitskii. Binary search with distributional predictions. In Amir Globersons, Lester\nMackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang,\neditors, Advances in Neural Information Processing Systems 38: Annual Conference on Neu-\nral Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, Decem-\nber 10 - 15, 2024 , 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\na4b293979b8b521e9222d30c40246911-Abstract-Conference.html .\nKyle Fox, Debmalya Panigrahi, and Fred Zhang. Minimum cut and minimum k-cut in hy-\npergraphs via branching contractions. In Proceedings of the Thirtieth Annual ACM-SIAM\nSymposium on Discrete Algorithms , pages 881–896. SIAM, 2019.\nHN Gabow. A matroid approach to finding edge connectivity and packing arborescences. Jour-\nnal of Computer and System Sciences , 2(50):259–273, 1995.\nRalph E Gomory and Tien Chung Hu. Multi-terminal network flows. Journal of the Society for\nIndustrial and Applied Mathematics , 9(4):551–570, 1961.\nAric Hagberg, Pieter J Swart, and Daniel A Schult. Exploring network structure, dynamics,\nand function using networkx. Technical report, Los Alamos National Laboratory (LANL),\nLos Alamos, NM (United States), 2008.\nJX Hao and James B Orlin. A faster algorithm for finding the minimum cut in a directed graph.\nJournal of Algorithms , 17(3):424–446, 1994.\nElad Hazan et al. Introduction to online convex optimization. Foundations and Trends ®in\nOptimization , 2(3-4):157–325, 2016.\nMonika Henzinger, Satish Rao, and Di Wang. Local flow partitioning for faster edge connectivity.\nSIAM Journal on Computing , 49(1):1–36, 2020.\nMonika Henzinger, Jason Li, Satish Rao, and Di Wang. Deterministic near-linear time mini-\nmum cut in weighted graphs. In Proceedings of the 2024 Annual ACM-SIAM Symposium on\nDiscrete Algorithms (SODA) , pages 3089–3139. SIAM, 2024.\n15\n\nSungjin Im, Ravi Kumar, Aditya Petety, and Manish Purohit. Parsimonious learning-augmented\ncaching. In International Conference on Machine Learning , pages 9588–9601. PMLR, 2022.\nSungjin Im, Ravi Kumar, Mahshid Montazer Qaem, and Manish Purohit. Non-clairvoyant\nscheduling with predictions. ACM Transactions on Parallel Computing , 10(4):1–26, 2023.\nDavid R Karger. Global min-cuts in rnc, and other ramifications of a simple min-cut algorithm.\nInSoda, volume 93, pages 21–30. Citeseer, 1993.\nDavid R Karger. Minimum cuts in near-linear time. Journal of the ACM (JACM) , 47(1):46–76,\n2000.\nDavid R Karger and Clifford Stein. A new approach to the minimum cut problem. Journal of\nthe ACM (JACM) , 43(4):601–640, 1996.\nDavid R Karger and David P Williamson. Recursive random contraction revisited. In Sympo-\nsium on Simplicity in Algorithms (SOSA) , pages 68–73. SIAM, 2021.\nKen-ichi Kawarabayashi and Mikkel Thorup. Deterministic edge connectivity in near-linear\ntime. Journal of the ACM (JACM) , 66(1):1–50, 2018.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 international conference on management of data ,\npages 489–504, 2018.\nSilvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online schedul-\ning via learned weights. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on\nDiscrete Algorithms , pages 1859–1877. SIAM, 2020.\nSilvio Lattanzi, Benjamin Moseley, Sergei Vassilvitskii, Yuyan Wang, and Rudy Zhou. Robust\nonline correlation clustering. Advances in Neural Information Processing Systems , 34:4688–\n4698, 2021.\nJason Li. Deterministic mincut in almost-linear time. In Proceedings of the 53rd Annual ACM\nSIGACT Symposium on Theory of Computing , pages 384–395, 2021.\nJason Li and Debmalya Panigrahi. Deterministic min-cut in poly-logarithmic max-flows. In\n2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS) , pages\n85–92. IEEE, 2020.\nAlexander Lindermayr and Nicole Megow. Permutation predictions for non-clairvoyant schedul-\ning. In Proceedings of the 34th ACM Symposium on Parallelism in Algorithms and Architec-\ntures, pages 357–368, 2022.\nThodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice.\nJournal of the ACM (JACM) , 68(4):1–25, 2021.\nSamuel McCauley, Ben Moseley, Aidin Niaparast, and Shikha Singh. Online list labeling with\npredictions. Advances in Neural Information Processing Systems , 36, 2024a.\nSamuel McCauley, Benjamin Moseley, Aidin Niaparast, and Shikha Singh. Incremental topo-\nlogical ordering and cycle detection with predictions. In Proceedings of the 41st International\nConference on Machine Learning , pages 35240–35254. PMLR, 2024b.\nSamuel McCauley, Benjamin Moseley, Aidin Niaparast, Helia Niaparast, and Shikha Singh.\nIncremental approximate single-source shortest paths with predictions, 2025. URL https:\n//arxiv.org/abs/2502.08125 .\n16\n\nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. Communications\nof the ACM , 65(7):33–35, 2022.\nRyan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph an-\nalytics and visualization. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial\nIntelligence , 2015. URL http://networkrepository.com .\nThatchaphol Saranurak. A simple deterministic algorithm for edge connectivity. In Symposium\non Simplicity in Algorithms (SOSA) , pages 80–85. SIAM, 2021.\nJeremy G Siek, Lie-Quan Lee, and Andrew Lumsdaine. The Boost Graph Library: User Guide\nand Reference Manual, The . Pearson Education, 2001.\nMechthild Stoer and Frank Wagner. A simple min-cut algorithm. Journal of the ACM (JACM) ,\n44(4):585–591, 1997.\n17\n\nA Learning Near-Optimal Predictions\nWe begin by proving the following two propositions, which motivate our learning algorithm.\nProposition A.1. The function U(G, p)is not convex in p.\nProof. The hessian of U(G, p) w.r.t pis as follows:\n∇2U(G, p) =2n2η(G,p)\nwG(C∗(G))2 \n\u0000\n1−w∗(G)−2 lnn·˜ρ(G, p)·w∗(G)\u0001\n×\u0000\n1−w∗(G)−2 lnn·˜ρ(G, p)·w∗(G)\u0001T\n−2\u0000\nlnn·˜ρ(G, p)·w∗(G)\u0001\u0000\nlnn·˜ρ(G, p)·w∗(G)\u0001T!\n.\nBeing a difference of two rank-one matrices, the hessian is not positive semi-definite. Con-\ncretely, consider the following simple example demonstrating that ∇2U(G, p) is not positive\nsemi-definite, which means that U(G, p) is not convex in p. Consider the following graph\non 3 vertices with the edge weights written next to them. Let p({1,2}) = 1 /ln 3, and\np({2,3}) =p({1,3}) = 0.\n13\n2 0.60.7\nThen, we have\n∇2U(G, p) =2n2η(G,p)\nwG(C∗(G))2\n−0.16−0.4−0.4\n−0.4 1 1\n−0.4 1 1\n,\nwhich is not positive semi-definite.\nNevertheless, we will succeed in minimizing U(G, p) over p. To build towards this, consider\nthe following function formed by replacing 1⊤pinU(G, p) by a free variable b.\nDefinition A.2. Forb≥0, define\nUb(G, p) :=n2η(G,p)\u0012b− ⟨w∗(G), p⟩\nwG(C∗(G))\u00132\n,and Kb:={p∈[0,1](V\n2):1Tp≤b}.\nWe have the following proposition.\nProposition A.3. For all b≥0,Ub(G, p)is convex in pover Kb.\nProof. The hessian of Ub(G, p) w.r.t pis as follows:\n∇2Ub(G, p) =n2η(G,p)\nwG(C∗(G))2· \n4 ln2n\u0012b− ⟨w∗(G), p⟩\nwG(C∗(G))\u00132\n+ 8 ln n\u0012b− ⟨w∗(G), p⟩\nwG(C∗(G))\u0013\n+ 2!\n×w∗(G)w∗(G)T.\nThe coefficient of w∗(G)w∗(G)Tin the expression above is non-negative whenever p∈Kb.\nTherefore, the hessian is positive semi-definite on the interior of Kb, which means Ub(G, p) is\nconvex over Kb.\nNow, we describe our proposed algorithm to compute a prediction ¯ pgiven a polynomial\nnumber of i.i.d samples drawn from D, and then analyze it to prove Theorem 4.1.\n18\n\nThe Learning Algorithm. To begin, we draw Ti.i.d samples G1, . . . , G TfromD. We\ndiscretize the range of possible values for b(which represents 1⊤p), i.e., [0 ,\u0000n\n2\u0001\n], into equally\nsized intervals, and optimize pover each of them separately. Let Bbe the set of discrete values\nconsidered for b; we will specify the resolution of the grid Blater.\nFor each b∈ B, we perform online gradient descent on the sequence {Ub(Gt,·)}T\nt=1of convex\nfunctions over the convex body Kbto obtain the set of vectors {pb\nt}T\nt=1⊆Kb, as stated below:\npb\nt+1= Π Kbh\npb\nt−ηt∇Ub(Gt, pb\nt)i\n,\nwhere Π Kb[x] = arg miny∈Kb∥x−y∥2. Let ¯ pb:=1\nTPT\nt=1pb\ntfor all b∈ B.\nNext, we draw T′new i.i.d samples G′\n1, . . . , G′\nT′fromDand compute1\nT′PT′\nt=1Ub(G′\nt,¯pb) for\neach b∈ B. Let b′= arg minb∈B1\nT′PT′\nt=1Ub(G′\nt,¯pb). The algorithm outputs ¯ pb′. The values of\nηt,T,T′, and the cardinality of Bwill be determined in the analysis.\nWe start the analysis with the following guarantee:\nTheorem A.4 (Theorem 3.1 in Hazan et al. [2016]) .For a fixed b∈ B, let Qbe an upper\nbound on ∥∇Ub(Gt, p)∥2for all (t, p)∈[T]×Kb, and let Dbe an upper bound on ∥p−q∥2for\nallp, q∈Kb. The iterates produced by Online Gradient Descent with step sizes ηt=D/Q√\nt\nguarantee that:\nTX\nt=1Ub(Gt, pb\nt)−min\np∈KbTX\nt=1Ub(Gt, p)≤3\n2QD√\nT.\nTo use the above theorem, note that\n∇Ub(G, p) =−2n2η(G,p)\nwG(C∗(G))\"\nlnn\u0012b− ⟨w∗(G), p⟩\nwG(C∗(G))\u00132\n+\u0012b− ⟨w∗(G), p⟩\nwG(C∗(G))\u0013#\nw∗(G).\nLetCmin:= inf G∈supp(D)wG(C∗(G)). Then, the guarantee in Theorem A.4 is obtained by\nsetting Q= 2n7lnn/C3\nmin,D=n; these valid upper bounds on size of the gradients and the\ndiameter can be readily verified.\nFor each b∈ B, let Regretb\nT:=PT\nt=1Ub(Gt, pb\nt)−minp∈KbPT\nt=1Ub(Gt, p). Now, we utilize\nthe following theorem:\nTheorem A.5 (Theorem 9.5 in Hazan et al. [2016]) .For a fixed b∈ Band any δ >0, given T\ni.i.d samples drawn from D, with probability at least 1−δ, we have\nEG∼D[Ub(G,¯pb)−Ub(G, p∗\nb)]≤Regretb\nT\nT+s\n8 log\u00002\nδ\u0001\nT,\nwhere p∗\nb= arg minp∈KbEG∼D[Ub(G, p)].\nLetp∗= arg minp∈KEG∼D[U(G, p)], where K= [0,1](V\n2), and let b∗=1Tp∗. Let ˜b∈ Bbe\nthe smallest element in Bthat is larger than or equal to b∗, and suppose ˜b−b∗≤∆. Let L(G, p)\nbe the Lipschitz constant of n2η(G,p)\u0010\nb−⟨w∗(G),p⟩\nwG(C∗(G))\u00112\nas a function of b. Then, we have\nEG∼D[U˜b(G, p∗)]≤EG∼D[Ub∗(G, p∗)] +L∆ =EG∼D[U(G, p∗)] +L∆,\nwhere Lis an upper bound on L(G, p) for all GinD’s support and p∈K. Note that ∆ and L\ncan be chosen such that ∆ ≤n2/|B|andL≤2n4/C2\nmin.\nLetMbe an upper bound on |Ub(G, p)−Ub(G, q)|for all b∈ B,p, q∈Kb, and GinD’s\nsupport. We can pick Msuch that M≤n6/C2\nmin. Then, by the Chernoff-Hoeffding inequality\n19\n\nand union bound, if T′= Θ\u0010\u0000M\nε\u00012log\u0010\n|B|\nδ\u0011\u0011\n, then with probability at least 1 −δ, for all b∈ B,\nwe have \f\f\f\f\fEG∼Dh\nUb(G,¯pb)i\n−1\nT′T′X\nt=1Ub(G′\nt,¯pb)\f\f\f\f\f≤ε.\nTherefore, using the fact that b′is chosen to minimize the empirical average, we have\nEG∼Dh\nUb′(G,¯pb′)i\n≤1\nT′T′X\nt=1Ub′(G′\nt,¯pb′) +ε≤1\nT′T′X\nt=1U˜b(G′\nt,¯p˜b) +ε≤EG∼Dh\nU˜b(G,¯p˜b)i\n+ 2ε.\nFinally, by Theorem A.5, we have\nEG∼Dh\nU˜b(G,¯p˜b)i\n≤EG∼Dh\nU˜b(G, p∗\n˜b)i\n+Regret˜b\nT\nT+s\n8 log\u00002\nδ\u0001\nT.\nPutting everything together, we can write\nEG∼Dh\nUb′(G,¯pb′)i\n≤EG∼Dh\nU˜b(G,¯p˜b)i\n+ 2ε\n≤EG∼Dh\nU˜b(G, p∗\n˜b)i\n+Regret˜b\nT\nT+s\n8 log\u00002\nδ\u0001\nT+ 2ε\n≤EG∼Dh\nU˜b(G, p∗)i\n+Regret˜b\nT\nT+s\n8 log\u00002\nδ\u0001\nT+ 2ε\n≤EG∼D[U(G, p∗)] +L∆ +Regret˜b\nT\nT+s\n8 log\u00002\nδ\u0001\nT+ 2ε.\nTherefore, we need |B|= Θ( n6/εC2\nmin) and T= Θ\u0012\nmax\u001a\u0010\nQD\nε\u00112\n,1\nε2log1\nδ\u001b\u0013\nto obtain the\npromised guarantee.\n20",
  "textLength": 54752
}