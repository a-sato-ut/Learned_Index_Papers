{
  "paperId": "457ea3e552c87a2a8e991fe526d6f7b9cc2fedc9",
  "title": "An Empirical Analysis of Deep Learning for Cardinality Estimation",
  "pdfPath": "457ea3e552c87a2a8e991fe526d6f7b9cc2fedc9.pdf",
  "text": "An Empirical Analysis of Deep Learning\nfor Cardinality Estimation\nJennifer Ortiz†, Magdalena Balazinska†, Johannes Gehrke‡, S. Sathiya Keerthi+\nUniversity of Washington†, Microsoft‡, Criteo Research+\nABSTRACT\nWe implement and evaluate deep learning for cardinality estimation\nby studying the accuracy, space and time trade-offs across several\narchitectures. We find that simple deep learning models can learn\ncardinality estimations across a variety of datasets (reducing the error\nby 72% - 98% on average compared to PostgreSQL). In addition,\nwe empirically evaluate the impact of injecting cardinality estimates\nproduced by deep learning models into the PostgreSQL optimizer.\nIn many cases, the estimates from these models lead to better query\nplans across all datasets, reducing the runtimes by up to 49% on\nselect-project-join workloads. As promising as these models are, we\nalso discuss and address some of the challenges of using them in\npractice.\n1 INTRODUCTION\nQuery optimization is at the heart of relational database management\nsystems (DBMSs). Given a SQL query, the optimizer automatically\ngenerates an efficient execution plan for that query. Even though\nquery optimization is an old problem [62], it remains a challenging\nproblem today: existing database management systems (DBMSs)\nstill choose poor execution plans for many queries [ 40]. Cardinality\nestimation is the ability to estimate the number of tuples produced\nby a subquery. This is a key component in the query optimization\nprocess. It is especially challenging with complex queries that con-\ntain many joins, where cardinality estimation errors propagate and\namplify from the leaves to the root of the query plan. One problem\nis that existing DBMSs make simplifying assumptions about the\ndata (e.g., inclusion principle, uniformity or independence assump-\ntions) when estimating the cardinality of a subquery. When these\nassumptions do not hold, cardinality estimation errors occur, leading\nto sub-optimal plan selections [ 40]. To accurately estimate cardinali-\nties, optimizers must be able to capture detailed data distributions\nand correlations across columns. Capturing and processing this in-\nformation, however, imposes space and time overheads and adds\ncomplexity.\nTo support cardinality estimation, DBMSs collect statistics about\nthe data. These statistics typically take the form of histograms or\nsamples. Because databases contain many tables with many columns,\nthese statistics rarely capture all existing correlations. The manual\nprocess of selecting the best statistics to collect can help but requires\nexpertise both in database systems and in the application domain.\nRecently, thanks to dropping hardware costs and growing datasets\navailable for training, deep learning has successfully been applied\nto solving computationally intensive learning tasks in other domains.\nThe advantage of these type of models comes from their ability to\nlearn unique patterns and features of the data that are difficult to\nmanually find or design [26].Given this success, we ask the following fundamental question:\nShould we consider using deep learning for query optimization?\nCan a deep learning model actually learn properties about the data\nand learn to capture correlations that exist in the data? What is the\noverhead of building these models? How do these models compare\nto other existing machine learning techniques? In this work, we\nimplement a variety of deep learning architectures to predict query\ncardinalities. Instead of relying on basic statistics and formulas to\nestimate cardinalities, we train a model to automatically learn impor-\ntant properties of the data to more accurately infer these estimates.\nIn this paper, we seek to understand the fundamental capabilities of\ndeep neural networks for this application domain. For this reason,\nwe focus on the performance of basic deep learning architectures.\nOur community has recently started to consider the potential of\ndeep learning techniques to solve database problems [ 75]. There\nstill is, however, limited understanding of the potential and impact\nof these models for query optimization. Previous work has demon-\nstrated the potential of using deep learning as a critical tool for\nlearning indexes [ 37], improving query plans [ 46], and learning car-\ndinalities specifically through deep set models [ 34], but we argue\nthat that the accuracy should not be the only factor to consider when\nevaluating these models. We also need to consider their overheads,\nrobustness, and impact on query plan selection. We need a system-\natic analysis of the benefits and limitations of various fundamental\narchitectures.\nIn this experimental study, we focus on the trade-offs between\nthe size of the model (measured by the number of trainable pa-\nrameters), the time it takes to train the model, and the accuracy of\nthe predictions. We study these trade-offs for several datasets. Our\ngoal is to understand the overheads of these models compared to\nPostgreSQL’s optimizer. To do this, we build several simple neural\nnetwork as well as recurrent neural network models and vary the\ncomplexity by modifying the network widths and depths. We train\neach model separately and compare the overheads of these models\nto PostgreSQL and tree ensembles (based on off-the-shelf machine\nlearning models).\nTo summarize, we contribute the following:\n•We show how deep neural networks and recurrent neural net-\nworks can be applied to the problem of cardinality estimation\nand describe this process in Section 3.\n•We comparatively evaluate neural networks, recurrent neural\nnetworks, tree ensembles, and PostgreSQL’s optimizer on\nthree real-world datasets in Section 4. For a known query\nworkload, we find that, compared to PostgreSQL, simple deep\nlearning models that are similar in space improve cardinality\npredictions by reducing the error by up to 98%. These models,\nhowever, come with high training overheads. We also find\nthat, although tree ensembles usually require a larger amountarXiv:1905.06425v2  [cs.DB]  12 Sep 2019\n\nof space, they are fast to train and are more accurate than the\ndeep learning models under certain settings.\n•In Section 4.3, we study these models in more detail by eval-\nuating the robustness of these models with respect to query\nworkload changes. We find that tree ensembles are more sen-\nsitive to these changes compared to the neural network and\nrecurrent neural network models.\n•In Section 4.4, we visualize the embeddings from the models\nto understand what they are learning.\n•Finally, we study these models from a practical perspective\nin Section 5. We evaluate how predictions from these models\nimprove query plan selection. We find that these models can\nhelp the optimizer select query plans that lead from 7% to\n49% faster query executions.\n2 BACKGROUND AND PROBLEM\nSTATEMENT\nMany optimizers today use histograms to estimate cardinalities.\nThese structures can efficiently summarize the frequency distribution\nof one or more attributes. For single dimensions, histograms split the\ndata using equal-sized buckets (equi-width) or buckets with equal\nfrequencies (equi-depth). To minimize errors, statistics about each\nbucket are also stored including but not limited to the number of\nitems, average value, and mode [19].\nThese histograms are especially relevant in cases where there are\nsimple single query predicates. For more complex predicates, the\nsystem extracts information from these histograms in conjunction\nwith “magic constants” to make predictions [ 40]. Optimizers typi-\ncally do not build or use multidimensional histograms or sampling\ndue to the increased overheads [ 24,79]. As the estimates from these\noptimizers are not theoretically grounded, propagating these esti-\nmates through each intermediate result of a query plan can result in\nhigh cardinality errors, leading to sub-optimal query plans.\nIn this paper, we use PostgreSQL’s optimizer as representative\nof this class because it is a mature optimizer available in a popular\nopen source system.\nOur goal in this paper is to apply deep learning to the cardinality\nestimation problem and compare the performance of this approach\nempirically to that of a traditional query optimizer.\nWe consider the following scenario: A database system is de-\nployed at a customer’s site. The customer has a database Dand\na query workload Q. Both are known. We compare the following\napproaches:\n•Traditional: In the pre-processing phase, we build his-\ntograms on select attributes in D. We select those attributes\nfollowing standard best practices given the workload Q. Sim-\nple best practices include collecting statistics for all frequently\njoined columns and for non-indexed columns frequently ref-\nerenced as a selection predicate, particularly if the column\ncontains very skewed data[ 2]. We then measure the accuracy\nof cardinality estimates on queries in Q(and queries similar\ntoQ) and the overhead of creating and storing the histograms.\nWe measure both the time it takes to build the histograms and\nthe space that the histograms take.\n•Deep Neural Networks: In the pre-processing phase, we\nexecute all queries in Qto compute their cardinalities. Weuse the results to train deep neural networks. We encode all\nqueries in Qinto inputs for the models, and evaluate how\naccurately these models are able to learn the function be-\ntween the input, Xand the cardinality value, Y. As above, we\nmeasure the overhead of building, storing and accuracy of\ncardinality estimates for queries in Qand queries not in Qbut\nsimilar to those in Q. To compare different architectures, we\nbuild several models by varying the width and depth.\nAs a simplifying assumption, in this paper, we focus on select-\nproject-join queries and only use single-sided range predicates as\nselection predicates. The join predicates consist of primary key and\nforeign key relationships between tables, as defined by their schema.\n3 MACHINE LEARNING-BASED\nCARDINALITY ESTIMATION\nThe first contribution of this paper is to articulate how to map the\ncardinality estimation problem into a learning problem. We show\nthe mapping for three types of models: neural networks, recurrent\nneural networks, and tree ensembles.\nFor ease of illustration, in this section, we use a simple running\nexample comprising a database Dwith three relations, D:{A,B,C}.\nEach relation has two attributes where relation Acontains{a1,a2},\nrelation Bhas attributes{b1,b2}, and relation Chas attributes{c1,c2}.\nIn this database, there are two possible join predicates. Attribute\na2is a foreign key to primary key attribute b1, while b2serves as a\nforeign key to primary key attribute c1.\n3.1 Neural Networks\nDeep learning models are able to approximate a non-linear function,\nf[26]. These models define a mapping from an input Xto an output\nY, through a set of learned parameters across several layers with\nweights,θ. Each layer contains a collection of neurons, which help\nexpress non-linearity. During training, the behavior of the inner\nlayers are not defined by the input data X, instead these models\nmust learn how to tune the weights to produce the correct output.\nSince there is no direct interaction between the layers and the input\ntraining data, these layers are called hidden layers [26].\nTraining occurs through a series of steps. First, during forward\npropagation, a fixed-sized input Xis fed into the network through\nthe input layer. This input is propagated through each layer through\na series of weights [ 67] until it reaches the final output, Y. This\nprocess is illustrated in Figure 1. After a forward pass, the back-\npropagation step then evaluates the error of the network and through\ngradient descent, modifies the weights to improve errors for future\npredictions.\nThere are several architectures we could consider for the model.\nAs shown in Figure 1, a neural network can have a different number\nof layers (depth) and a different number of hidden units in each\nindividual layer (width). Determining the correct number of hidden\nunits is currently an active area of research and does not have strong\ntheoretical principles [ 26]. Although a network with only a single\nwide hidden layer is capable of learning a complex function, deep\nnetworks are able to use a smaller number of training parameters to\nachieve the same goal. Unfortunately, deep networks are difficult to\ntrain and to optimize [ 26]. In this work, we focus on evaluating a\nvariety of network architectures. We focus on simple architectures\n2\n\n...!\"depthwidthFigure 1: Illustration of a Deep Neural Network: The input con-\nsists of an input X. This is then fed into a network with nhidden\nlayers, which then makes a prediction for the cardinality of the\nquery, Y.\n100.110000q:  SELECT * FROM AWHERE a1<= 23IselpredIrelationABCa1a2b1b2c1c2\nFigure 2: Query Encoding for Selections: We encode a selection\nby specifying the underlying relations in the query and all selec-\ntion predicate values.\ncomprising a small number of fully connected layers. We vary the\nwidth and the depth of the network. More complex architectures are\npossible [ 34] and are also interesting to study. Our goal, however, is\nto understand the performance of basic architectures first.\nGiven a model, a query qand a fixed dataset D, we define an\nencoding for the input, X. The input Xshould contain enough infor-\nmation for the model to learn a useful mapping. There are several\nways to represent a query as an input vector. The encoding deter-\nmines how much information we provide the network. In this work,\nwe define Xas a concatenation of three single dimensional vectors:\nIrelations ,Iselpred , andIjoinpred . To explain this encoding, we\nfirst describe how to encode selection queries.\nModeling Selection Queries With selection queries, the goal\nis to have the network learn the distribution of each column and\ncombinations of columns for a single relation. To encode a selection\nquery, we provide the model with information about which relation\ninDwe are applying the selections to, along with the attribute values\nused in the selection predicates. We encode the relation using vector\nIrelations as a binary one-hot encoding. Each element in Irelations\nrepresents a relation in D. If a relation is referenced in q, we set the\ndesignated element to 1, otherwise we set it to 0.\nWe encode the selection predicates in qusing vectorIselpred .\nAs described in Section 2, selection predicates are limited to single-\nsided range predicates. Each element in this vector holds the selec-\ntion value for one attribute. The vector includes one element for\neach attribute of each relation in D. As an example, assume we have\nthe following query: SELECT * FROM AWHERE a1≤23. In\n110.11110010q:  SELECT * FROM A,BWHERE a1<= 23anda2=b1IselpredIrelationABCa1a2b1b2c1c2a2=b1b2=c1IjoinpredFigure 3: Query Encoding for Joins+Selections: We encode a\njoin+selection query based on the joined relations, the selection\npredicate values and join predicates.\nthis case, we set the corresponding element for a1inIselpred as23.\nOtherwise, if there is no selection on an attribute, we set the element\nwith the maximum value of the attribute’s domain. This captures the\nfact that we are selecting all values for that attribute.\nNeural networks are highly sensitive to the domain of the input\nvalues. Having unnormalized values in the input will highly impact\nthe error surface for gradient descent, making the model difficult\nto train. Instead, we encode these selection predicates as values\nranging from 0 to 1, where the value represents the percentile of the\nattribute’s active domain as shown in Figure 2. The output of the\nmodel is also normalized to represent the percentage of tuples that\nremain after the selection query is applied to the relation. Using this\ntype of normalization, instead of learning query cardinalities, we are\nin fact learning predicate selectivities.\nModeling Join Queries Introducing queries that contain both\njoins and selections requires the model to learn a more complex\noperation. Joins essentially apply a cartesian product across a set of\nrelations followed by additional filters that correspond to the equality\njoin and selection predicates. We encode existing join predicates\nwith the vectorIjoinpred using a binary one-hot encoding. As we\nnow include joins, the output Ynow represents the fraction of tuples\nselected from the join result. Hence, once again, the model will learn\nthe selectivity of the join operation.\nWe illustrate this encoding using an example in Figure 3. Given\nthe following query from our running example dataset, SELECT ∗\nFROM A,BWHERE a1≤23anda2=b1, we show the encoding\nin the figure. ForIrelations , there are three possible elements, one\nfor each relation in D. For this query, we only set the elements\ncorresponding to relations AandBto 1. The vectorIselpred contains\nthe encoding for the selection predicates. Since relation Cis not\nreferenced in q, we set all of its attributes in Iselpred to 0. We set\nto.1the element corresponding to attribute a1, as it represents the\npercentile of the active domain for a1. The rest of the attributes from\nAandBare set to 1, as we are not filtering any values from these\nattributes. Finally, the vector Ijoinpred encodes the join predicate\na2=b1with a 1.\n3.2 Recurrent Neural Networks\nIf we focus on left-deep plans, we can model queries as sequence\nof operations, and we can leverage that structure when learning\na model [ 51]. Recurrent neural networks (RNN) in particular are\ndesigned for sequential data such as time-series data or text se-\nquences [ 67]. Compared to neural networks where the input is a\nsingle vector X, the input to RNNs is a sequence with ttimesteps,\nX={x1,x2, ...,xt}. For each timestep t, the model receives two\n3\n\n!\"#\"...!$!%ℎ$ℎ\"ℎ%''#$#%Figure 4: Illustration of a Recurrent Neural Network: The in-\nput consists of a sequence of inputs {x1,x2, ...,xt}. Each input,\nalong with the hidden state of the previous timestep, is fed into\nthe network to make a prediction, yi.\n100.11000000q:  SELECT * FROM A,BWHERE a1<= 23anda2=b1IselpredIrelationABCa1a2b1b2c1c2a2=b1b2=c1Ijoinpred!\"01000110010IselpredIrelationABCa1a2b1b2c1c2a2=b1b2=c1Ijoinpred!#(selection on A)(join with B)1\n211\n22\nFigure 5: Query Encoding for the RNN Model: In this example,\nthe input consists of two inputs: {x1,x2}. The first input repre-\nsents the subquery that scans and filters relation A. The second\nrepresents the join with relation B.\ninputs: xtandht−1, where ht−1is the generated hidden state from\nthe previous timestep [ 67]. With these inputs, the model generates a\nhidden state for the current timestep t, where ht=f(ht−1,xt)and\nfrepresents an activation function. Given this feedback loop, each\nhidden state contains traces of not only the previous timestep, but all\nthose preceding it as well. RNNs can either have a single output Yfor\nthe final timestep (a many-to-one architecture) or they can have one\noutput for each timestep (many-to-many) where Y={y1,y2, ...,yt}.\nIn our context, we model queries as a series of actions, where each\naction represents a query operation (i.e. a selection or a join) in a\nleft-deep query plan corresponding to the query. With a sequential in-\nput, RNNs incrementally generate succinct representations for each\ntimestep, which are known as hidden states , and which represent a\nsubquery. Recurrent neural networks rely on these hidden states to\ninfer context from previous timesteps. More importantly, htis not a\nmanually specified feature vector, but it is the latent representation\nthat the model learns itself. We illustrate how these hidden states\nare generated in Figure 4. Information from each hidden state, htis\nfed into the next timestep, t+1through shared weights, w. In our\ncontext, hidden representations are useful, as they capture important\ndetails about the underlying intermediate result. The information\nlearned at the hidden state is highly dependent on the input and\noutput of the network.\nTo generate the input for this model, we concatenate three input\nvectors for each action xi. That is, for each action xi, we concatenate\nvectorsIrelation i,Iselpred iandIjoinpred i. In Figure 5, we show\nthe representation for our running example, SELECT ∗FROM A,B\n!\"#$%&≤(#$%)≤(TrueFalseTrueFalse#$%*≤(TrueFalse+,--\"…+,--/!=1(!\",…,!/)5(67,8779\":)Figure 6: Illustration of a Random Forest Model: Before train-\ning, each tree is provided with a bootstrapped sample of train-\ning points, B({(xi,yi)}N\ni=1). During inference, each xiis evalu-\nated against the criteria in each node (where each xi[j]repre-\nsents an attribute in xi). The predictions from each tree are ag-\ngregated for the final prediction, Y.\n!\"##${('(,*()}(-$.{('(,*(−0$'()}(-$.\n0$(')!\"##101('){('(,*(−023$'()}(-$.!\"##202(')...4=60$7+6017+...+602(7)\nFigure 7: Illustration of a Gradient Boosted Tree Model: The\nfirst tree in the sequence is provided with Ndata points from\nthe training set. This tree learns the function p1(x). Subsequent\ntrees are built incrementally based on the residual errors of the\nprevious tree.\nWHERE a1≤23anda2=b1. We break down this query into two\noperations: the scan and selection on relation A, followed by a join\nwith relation B. Alternatively, we could have the first action represent\nthe scan on relation B(with no selections applied), followed by a\njoin and selection with relation A.\n3.3 Tree Ensembles\nIn addition to deep learning models, we also include an analysis\nof tree ensembles. In particular, we compare bagging andboosting\napproaches. Ensemble methods aim to combine several decision\nmodels together to generate a single strong learner.\nBagging As a form of bagging, we study random forest models,\nas they are fast to build and are well suited for regression and clas-\nsification problems [ 20]. A random forest model is a combination\nof predictions from several independently trained trees as shown in\nFigure 6. During training, each tree takes as input a bootstrapped\nsample of Ntraining points, B({(xi,yi)}N\ni=1)where xiandyirepre-\nsent the input and label respectively. Each tree generated partitions\n4\n\nthe input space into subregions, where each of these subregions\neither contains a linear model [ 57] or a constant to make a predic-\ntion [ 11]. Finding these subregions requires finding an optimal set\nof splits, which is computationally infeasible. Instead, these models\nuse a greedy optimization to incrementally grow the trees one node\nat a time. To help generalize the model and to help add randomness,\neach tree generated uses a bootstrapped dataset from the training\ndata [ 21]. Based on the predictions from all these trees, the random\nforest model uses a function F(usually the mean) to compute a final\nprediction Y.\nBoosting Another variant of ensemble models is known as boost-\ning, which involves training multiple models in a sequence. For\nAdaBoost, a well-known boosting algorithm, each training data\npoint iis initialized with a weight coefficient. These weight coeffi-\ncients are adjusted according to the performance of the previously\ntrained model [ 11]. Data points with higher inaccuracies are given a\ngreater weight, to emphasize fixing these errors in the next model.\nTo implement a boosting technique in this work, we use gradient\nboosted trees. Instead of increasing the weights for each data point\nas seen in AdaBoost, the gradient boosted tree approach fits the next\nmodel based on the residual errors made by the current model [ 54].\nWe illustrate this approach in Figure 7. Initially, a single tree is built\nbased on Ndata points from the training set: {(xi,yi)}N\ni=1. Given\nthe errors observed from this initial tree, the algorithm iteratively\nimproves the errors by minimizing the residual errors in the next\ntree. The algorithm stops once it reaches a limit on the number of\nestimators or once the training objective does not change. As shown\nin the figure, to compute Ygiven X, the predictions from all the trees\nare summed up (given a shrinkage parameter, ϵ).\n4 MEASUREMENT ANALYSIS\nIn this section, we evaluate all models and their architecture vari-\nants on three datasets. We start with a description of the experimental\nsetup, which includes the implementation of the models and gener-\nation of the training datasets. We then evaluate the accuracy, time,\nspace trade-offs in Section 4.2, followed by a study of the robust-\nness of the models in Section 4.3 and a look into their latents in\nSection 4.4.\n4.1 Experimental Setup\nDatasets: We evaluate the models on three datasets:\n•IMDB : The Internet Movie Data Base is a real dataset that\ncontains a wide variety of information about actors, movies,\ncompanies, etc. This dataset has 21 relations. The dataset is\nbased on the 3.6GB snapshot from Leis et. al. [39].\n•DMV : This dataset contains 6 relations (61MB) and is based\non a real-world dataset from a Department of Motor Vehi-\ncles [ 33]. Relations include accidents, owners, cars, location,\ndemographics and time.\n•TPC-H (skewed) : This is a standard benchmark dataset with\n8 relations and a scale factor of 1 (1GB). We adjust the skew\nfactor to z=1[18].\nModel Architectures: For the recurrent and neural networks,\nwe build several models that vary in width (w) and depth (d). To\nminimize the number of possible architecture combinations, weassume that all layers within a model have the same width. We\nannotate the models as a pair ( x,y), where xrepresents the width\nandyrepresents the depth. For example, (100w, 1d) represents a\nmodel with 100 hidden units in a single hidden layer. For the random\nforest models, we vary the the depth and the number of trees from 1\nto 500. We similarly vary the number of trees, depth and shrinkage\nfor the gradient boosted trees. We compare these models to estimates\nfrom PostgreSQL version 9.6 [ 56]. To fairly compare PostgreSQL to\nthese models in terms of space, we modify the PostgreSQL source\nto allow for a larger number of bins in each histogram. For each\nrelation in each dataset, we collect statistics from each join predicate\ncolumn and each selection column. We vary the number of bins from\nthe default size (100 bins) up to 100K.\nTraining Data: For each dataset, we generate various training\nsets with different levels of query complexity. We define three com-\nplexity levels: 2Join,4Joinand 6Join.2Joinis the case where we\ngenerate a training set with joins that consist of any 2 relations in the\ndataset, 4Joinrepresents joins with 4 relations, and 6Joinrepresents\njoins with 6 relations. In addition, for each dataset, we manually\nselect a set of columns as candidates for selection predicates. We se-\nlect columns with small discrete domain sizes, as these are generally\nthe columns that contain more semantically meaningful information\nabout the data, unlike columns that contain a sequence of identifiers.\nAs we generate the workload, selection predicate values are ran-\ndomly drawn from the domains of the selected candidate columns.\nWe generate 100K training samples for each query complexity and\neach dataset. We randomly select the desired number of tables and\npick the selection columns from the joined relations. For the RNN,\nbecause it requires an input for each timestep, we extend these train-\ning sets by adding more training samples for all the subqueries. For\nexample, for a query that joins six relations, we extend the training\nset with additional examples representing the subquery after each\nintermediate join. For each query complexity training set, we select\n1K samples to serve as the test set.\nHyperparameter Tuning: We tune each model architecture for\neach dataset. We separate 10 %of the training data as the validation\ndata. We run a basic grid search over the learning rate and batch size.\nA larger batch size (although faster to train, especially on a GPU)\nmight lead to sub-optimal results, while a small batch size is more\nsusceptible to noise. Larger learning rates also have the tendency\nto oscillate around the optimum, while smaller learning rates might\ntake a long time to train. We set the number of epochs to 500 for\nall learning rate and batch parameter combinations. Based on the\ncombination that leads to the lowest learning rate, we continue to\ntrain for more epochs as long as the validation loss keeps decreasing.\nWe stop the training once the validation loss plateaus or increases.\nModel Implementation Details: The neural network is imple-\nmented in Tensorflow [ 3] and is implemented as a residual network\nwith leaky RELU activation functions, as it is a default recommenda-\ntion to use in modern neural networks [ 26]. Weights are initialized\nfrom a random normal distribution with a small standard deviation.\nBiases are initialized to .01. The input Xis normalized as explained\nin Section 3.1 and centered using a StandardScaler. The output Y\nis log transformed and also normalized with a StandardScaler. The\nmodel’s goal is to minimize the mean squared error between the\nreal outputs and the predictions. We use the AdamOptimizer as the\n5\n\nFigure 8: CDF of PostgreSQL absolute errors with storage budget: For each curve, we show the knee, k, which defines the split\nbetween Easy(PostgreSQL) and Hard(PostgreSQL).\n(a) IMDB with Storage Budget\nCDF Percentiles\n25% 50% 75%\nPostgreSQL 7.8M 13.8M 29.7M\nNN (100w,1d) 1.25M 2.97M 6.6M\nRNN (100w,1d) .71M 1.49M 3.67M\nAverage Absolute Errors\n2Join 4Join 6Join\nPostgreSQL 6.8M 12.8M 31.3M\nNN (100w,1d) .80M 4.1M 7.0M\nRNN (100w,1d) .58M 2.2M 4.1M\nAverage Relative Errors\n2Join 4Join 6Join\nPostgreSQL .39 .75 .95\nNN (100w,1d) .04 .22 .20\nRNN (100w,1d) .03 .11 .13\n(b) DMV with Storage Budget\nCDF Percentiles\n25% 50% 75%\nPostgreSQL 5.2K 6.8K 10.3K\nNN (500w,1d) .50K 1.4K 4.1K\nRNN (100w,1d) .76K 2.1K 5.1K\nAverage Absolute Errors\n2Join 4Join 6Join\nPostgreSQL 9.4K 9.4K 8.9K\nNN (100w,1d) 4.9K 3.0K 3.1K\nRNN (100w,1d) 7.9K 2.9K 4.0K\nAverage Relative Errors\n2Join 4Join 6Join\nPostgreSQL .10 .20 .23\nNN (100w,1d) .03 .03 .04\nRNN (100w,1d) .06 .02 .07\n(c) TPCH with Storage Budget\nCDF Percentiles\n25% 50% 75%\nPostgreSQL 1.2M 1.8M 2.9M\nNN (100w,1d) .01M .02M .05M\nRNN (100w,1d) .01M .03M .06M\nAverage Absolute Errors\n2Join 4Join 6Join\nPostgreSQL 2.9M 2.2M 1.8M\nNN (100w,1d) 35K 40K 32K\nRNN (100w,1d) 27K 60K 41K\nAverage Relative Errors\n2Join 4Join 6Join\nPostgreSQL .99 .99 .99\nNN (100w,1d) .01 .02 .01\nRNN (100w,1d) .01 .03 .02\nFigure 9: Error Analysis for all Models : We show the curve for Hard(PostgreSQL) and show the corresponding errors from the best\nmodels below the storage budget. Below each graph, we show tables detailing the percentiles, the average absolute error and average\nrelative error.\n6\n\n% Queries Easy\n(Models)\nIMDB DMV TPC-H\nNN(100w,1d) RNN(100w,1d) NN(500w,1d) RNN(100w,1d) NN(100w,1d) RNN(100w,1d\nEasy(PostgreSQL) 99.5% 99.8% 90.5% 94.5% 100% 100%\nHard(PostgreSQL) 71.4% 83.5% 75.6% 69.4% 100% 100%\nTable 1: Percentage of Queries that are Easy for the Models: For each Easy(PostgreSQL) query batch, we find the percentage of\nqueries that are also easy for the models. We also show the percentage of queries that are easy based on the Hard(PostgreSQL) batch\noptimizer for the model. The recurrent neural network is also im-\nplemented in Tensorflow. For deep recurrent neural networks, we\nuse a ResidualWrapper around each layer, to mimic the residual\nimplementation of the neural networks. Both the neural network and\nrecurrent neural networks are run on a GPU on p2.xlarge instances\non Amazon AWS [ 6]. The Random Forest model is based on an im-\nplementation from sklearn’s RandomForestRegressor module [ 55].\nFinally, the gradient boosted tree model is based on sklearn’s Gradi-\nentBoostingRegressor [55].\n4.2 Learning Cardinalities for Selections + Joins\nIn this section, we vary the architecture of the models and evaluate\nthem on the three datasets. We study the trade-offs (space, time, and\naccuracy) for these models.\nFirst, we evaluate the prediction accuracy for each model. As\ndescribed in Section 2, we make the assumption that the query\nworkload is known in advance (we relax this assumption later in\nthis section). In this case, the models overfit to a specific set of\nqueries. As a result, training overfit models helps us study how\neffectively these models are able to compress information. For each\nquery complexity, we train six neural network (NN) and six recurrent\nneural network models (RNN) based on the following widths and\ndepths: (100w, 1d), (100w, 5d), (500w, 1d), (500w, 5d), (1000w, 1d),\n(1000w, 5d). We separately train random forest models and gradient\nboosted tree models with 1, 5, 50 and 500 trees. Larger models\ngenerally use up more space, but result in more accurate cardinality\npredictions.\nTo make this analysis comparable to PostgreSQL, we first limit\nthe storage budget for the models to be no more than the storage\nbudget for PostgreSQL histograms. We compute the size of a model\nas the size of all its parameters. For the NN and RNN models, we\nthus measure the number of trainable variables and for PostgreSQL,\nwe measure the number of parameters used in the pд_stats table.\nWe compare PostgreSQL cardinality estimates to those produced\nby models that are smaller in size compared to PostgreSQL’s his-\ntograms. We specifically study the PostgreSQL scenario where each\nhistograms builds at most 1K bins. Setting the number of bins to 1K\nfor PostgreSQL results in 13385 parameters for the DMV dataset,\n15182 parameters for the TPC-H dataset and 44728 parameters for\nthe IMDB dataset. We purposely set PostgreSQL as the storage up-\nper bound size. Given these storage budgets, we then select the best\nneural network architecture, the best recurrent network architecture,\nthe best random forest model and best gradient boosted tree model.\nIf no model meets the budget, we do not display them on the graphs.\nIf more than one model architecture meets the storage budget, wedisplay the best model, where the best model is defined as the one\nwith the lowest median error.\nLimited Storage CDFs and Outlier Analysis: For PostgreSQL,\nas for other relational DBMSs, cardinality estimation is easy for\nsome queries and hard for others. As expected PostgreSQL yields\nmore accurate predictions for queries with a low complexity, particu-\nlarly those with no selection predicates. To help distinguish between\nthese “easy” and “hard” queries (labeled as Easy(PostgreSQL) and\nHard(PostgreSQL)), we plot the absolute errors from PostgreSQL\nas a cumulative distribution (cdf) as shown in Figure 8. We use the\nknee ( k) of the cdf curve to split the queries into an “easy” category\n(those with errors less than the knee, k) and a “hard” category (those\nwith errors greater than the knee k). For the TPC-H dataset, the\ndistribution of errors is wide. To ensure we retain enough queries in\nthe Hard(PostgreSQL) category (for later more in-depth analysis),\nwe compute kand half the corresponding error.\nWe first focus on the Hard(PostgreSQL) queries. These are the\nmore interesting queries to study as these are the queries for which\nwe seek to improve cardinality estimates. We plot the distribution of\nerrors for the best performing models for each dataset in Figure 9.\nOverall, both types of models outperform PostgreSQL on all three\ndatasets. We also find the performance of both types of models to be\nsimilar.\nFirst, in Figure 9a, we show the cdf for the Hard(PostgreSQL)\nqueries from the IMDB dataset. From the entire set of IMDB queries,\n11% of the queries fall in the Hard(PostgreSQL) category. The y-\naxis represents the percentage of queries and the x-axis represents\nthe absolute error. In addition to the PostgreSQL error curve, we\nshow the cdf for the corresponding queries from the best neural\nnetwork and recurrent neural network models. We do not show the\ntree ensemble models here, as the models do not meet the storage\nbudget. Both the neural network and recurrent neural network have\ncomparable cardinality estimation errors. On average, the neural\nnetwork reduces estimation error by 72%, while the recurrent neural\nnetwork reduces the error by 80%. Below Figure 9a, we include\nadditional details that show the percentiles of the model cdfs, the\naverage absolute error for each query complexity, and the average\nrelative error.\nIn Figure 9b, we show the cdf for Hard(PostgreSQL) from the\nDMV dataset. Approximately 10% of the queries are labeled as hard\nfor PostgreSQL. The NN reduces the errors by 75% on average\nand the RNN by 73%. As shown in the tables below the figure, the\ncomplexity of the queries does not heavily impact the average error.\nIn fact, the relative errors for the NN across all query complexities\nhave a small standard deviation ( σ=.004), compared to IMDB\n(σ=.08). Compared to DMV , the IMDB dataset contains several\n7\n\nQueries with Highest Errors from 2Join\nBest NN per Dataset Best RNN per Dataset\nIMDB(cast_info,role_type) where role_id<= 11 [1.9M]\n(cast_info,role_type) where role_id <= 10 [1.9M]\n(cast_info,role_type) where role_id <= 8 [1.9M]\n(cast_info,title) where kind_id <= 1,production_year <=\\\n2019,role_id<= 4 [1.5M]\n(movie_info,info_type) [1.3M]\n(cast_info,role_type) where role_id <= 7 [1.2M]\n(cast_info,role_type) where role_id <= 6 [1.1M]\n(cast_info,title) where kind_id <= 4,production_year <=\\\n2019,role_id<= 6 [1.0M](cast_info,name) where role_id <= 9 [2.1M]\n(cast_info,name) where role_id<= 11 [2.1M]\n(cast_info,role_type) where role_id <= 11 [1.9M]\n(cast_info,role_type) where role_id <= 9 [1.8M]\n(cast_info,role_type) where role_id <= 7 [1.3M]\n(cast_info,name) where role_id <= 8 [1.2M]\n(cast_info,role_type) where role_id <= 7 [1.1M]\n(cast_info,title) where kind_id <= 4,production_year <=\\\n2019,role_id<= 6 [.9M]\nDMV(accidents,time) where year<= 2005 and month <= 9 [11K]\n(accidents,time) where month <= 9 and year <= 2005 [11K]\n(accidents,time) where year <= 2003 and month <= 6 [10K]\n(accidents,time) where year <= 2000 and month <= 6 [10K]\n(accidents,location) where year <= 2001 [7K]\n(accidents,location) where year <= 2000 [7K]\n(car,accidents) where year <= 2005 [6K]\n(car,accidents) where year <= 2004 [6K](accidents,time) where year <= 2005 and month <= 9 [33K]\n(accidents,location) where year <= 2003 [18K]\n(car,accidents) where year <= 2005 [18K]\n(car,accidents) where year <= 2003 [18K]\n(accidents,time) where year <= 2005 and month <= 9 [17K]\n(accidents,time) where year <= 2003 and month <= 6 [10K]\n(accidents,time) where year <= 2000 and month <= 6 [10K]\n(accidents,location) where year <= 2002 [8K]\nTPC-H(lineitem,orders) l_linenumber <= 7 and l_quantity <=16 [116K]\n(lineitem,orders) l_linenumber <= 6 and l_quantity<= 16 [109K]\n(lineitem,orders) l_linenumber <= 5 and l_quantity <= 35 [98K]\n(lineitem,orders) l_linenumber <= 7 and l_quantity <= 34 [84K]\n(lineitem,orders) l_linenumber <= 7 and l_quantity <= 27 [65K]\n(lineitem,orders) l_linenumber <= 2 and l_quantity <= 17 [61K]\n(lineitem,orders) l_linenumber <= 6 and l_quantity <= 27 [61K]\n(lineitem,orders) l_linenumber <= 7 and l_quantity <= 23 [59K](lineitem,orders) where l_linenumber <= 7 and l_quantity <= 34 [89K]\n(lineitem,orders) where l_linenumber<= 6 and l_quantity <= 38 [75K]\n(lineitem,orders) where l_linenumber <= 5 and l_quantity <= 35 [72K]\n(lineitem,orders) where l_linenumber <= 6 and l_quantity <= 28 [62K]\n(lineitem,orders) where l_linenumber <= 7 and l_quantity <= 22 [55K]\n(lineitem,orders) where l_linenumber <= 5 and l_quantity <= 28 [52K]\n(lineitem,orders) where l_linenumber <= 4 and l_quantity <= 28 [51K]\n(lineitem,orders) where l_linenumber <= 7 and l_quantity <= 23 [50K]\nTable 2: The 2JoinQueries with the Highest Errors from the NN and RNN Models: For each dataset, we show the top eight queries\nwith the highest absolute errors.\n(a) role_id\n (b) info_type\n (c) production_year\n (d) company_type\n (e) kind_id\n(f) year (accidents)\n (g) month\n (h) year (car)\n (i) age\n(j) l_quantity\n (k) l_linenumber\n (l) p_size\n (m) c_nation\nFigure 10: Distributions for all Selection Columns: First row shows all distributions from the IMDB relation. Second shows distribu-\ntions from DMV , and the third shows TPC-H.\n8\n\n(a) IMDB Unlimited\nCDF Percentiles\n25% 50% 75%\nPostgreSQL 7.8M 13.9M 27.5M\nNN (500w,5d) .30M .97M 2.6M\nRNN (100w,1d) .6M 1.4M 3.6M\nRandom Forest (1) 1e-6M 6e-6M .2M\nBoosted Trees (1) 8e-7M 2e-6M 8e-6M\nAverage Absolute Errors\n2Join 4Join 6Join\nPostgreSQL 6.8M 12.8M 31.3M\nNN (500w,5d) .28M .45M 4.1M\nRNN (100w,1d) .58M 2.0M 4.1M\nRandom Forest (1) 1e-6 .21M 1.0M\nBoosted Trees (1) 1e-6 1e-4 7e-4\nAverage Relative Errors\n2Join 4Join 6Join\nPostgreSQL .38 .80 .95\nNN (500w,5d) .01 .03 .11\nRNN (100w,1d) .03 .11 .13\nRandom Forest (1) 3e-8 .01 .03\nBoosted Trees (1) 3e-8 9e-5 1e-4\n(b) DMV Unlimited\nCDF Percentiles\n25% 50% 75%\nPostgreSQL 5.28K 6.82K 10.3K\nNN (500w,5d) .16K .46K 1.0K\nRNN (500w,5d) .02K 1.1K 3.2K\nRandom Forest (1) 3e-6K 6e-6K 1e-5K\nBoosted Trees (1) 3e-6K 5e-6K 1e-5K\nAverage Absolute Errors\n2Join 4Join 6Join\nPostgreSQL 9.5K 9.3K 8.9K\nNN (500w,5d) .4K .9K 1.3K\nRNN (500w,5d) 1.1K 3.3K 2.8K\nRandom Forest (1) 8e-5 .06K .29K\nBoosted Trees (1) 8e-5 2e-3 9e-3\nAverage Relative Errors\n2Join 4Join 6Join\nPostgreSQL .10 .20 .23\nNN (500w,5d) .002 .01 .01\nRNN (500w,5d) .006 .03 .04\nRandom Forest (1) 7e-8 .0008 .007\nBoosted Trees (1) 7e-8 6e-6 4e-5\n(c) TPCH Unlimited\nCDF Percentiles\n25% 50% 75%\nPostgreSQL 1.2M 1.8M 2.9M\nNN (1000w,5d) .01M .02M .04M\nRNN (500w,5d) .01M .02M .05M\nRandom Forest (1) 4e-8M 8e-8M 1e-7M\nBoosted Trees (1) 3e-8M 7e-8M 1e-7M\nAverage Absolute Errors\n2Join 4Join 6Join\nPostgreSQL 2.9M 2.2M 1.8M\nNN (1000w,5d) 16K 41K 28K\nRNN (500w,5d) 29K 59K 23K\nRandom Forest (1) 9e-4 3K 15K\nBoosted Trees (1) 9e-4 1e-3 4e-2\nAverage Relative Errors\n2Join 4Join 6Join\nPostgreSQL .99 .99 .99\nNN (1000w,5d) .007 .02 .01\nRNN (500w,5d) .01 .02 .01\nRandom Forest (1) 4e-8 .001 .009\nBoosted Trees (1) 4e-8 9e-8 4e-6\nFigure 11: Error Analysis for all Models : We show the curve for Hard(PostgreSQL) and show the corresponding errors from the\nbest models with an unlimited storage budget. Below each graph, we show tables detailing the percentiles, the average absolute error\nand average relative error.\nmany-to-many primary/foreign key relationships, so joining relations\nsignificantly increases the size of the final join result.\nWe also observe a significant error reduction in Figure 9c (TPC-\nH), where the NN improves estimates by 98% and the RNN by 97%.\nFor TPC-H, 30% of the queries are hard for PostgreSQL.\nTable 1 shows the percentage of queries that are easy for\nthe models given that they are either Easy(PostgreSQL) or\nHard(PostgreSQL) for PostgreSQL. In the case of Hard(PostgreSQL)\nqueries, 70% or more become easy with the models. For the\nEasy(PostgreSQL) queries, the simple NN and RNN models also\nfind a majority of these queries to be easy (>90%). For IMDB and\nDMV , there are some queries from the Easy(PostgreSQL) batch that\nthe models find to be hard. We highlight some of these hard queries\nbelow:\n•From the IMDB dataset, approximately 0.4% of the\nEasy(PostgreSQL) queries are hard for the NN, and we find\nthat the query with the highest error is one with an abso-\nlute error of 8.9M. This query joins the name ,cast_inf o ,\nrole_type, and char _name relations and has a selection\npredicate on role_id<=8. For the RNN, the query withthe highest error is similar. It joins the name ,cast_inf o ,\nrole_type, and char _name relations, with a selection predi-\ncate on role_id<=4.\n•For the DMV dataset, approximately 10.5% of the queries are\nhard for NN, and 6.5% are hard for the RNN. The query with\nthe highest error for the NN is one that joins all relations\ncar,demoдraphics ,location ,time,owner , and accidents\nand has several selection predicates: aдe_demoдraphics <=\n89,month _time <=12,year_accidents <=2004. For\nthe RNN, the query with the highest absolute error also\njoins all relations and has selection predicates with sim-\nilar values, aдe_demoдraphics <=93,month _time <=\n9,year_accidents <=2005.\nFrom the Hard(PostgreSQL) queries, there are more queries that\nremain difficult for the models compared to Easy(PostgreSQL).\nThese hard queries consist of joins of 6 relations (the most com-\nplex queries we have in the test set) and up to 5 selection predicates.\nUnderstanding why the NN or RNN fail to accurately predict the\ncardinality for specific queries is challenging as there are several\nfactors to consider. For example, the error could be caused by a\n9\n\n(a) Space vs. Error for all models\n(b) Time vs Error for all models\nFigure 12: Trade-offs between Error, Space and Time: We show the absolute error, space and time for each model and for PostreSQL\nfor different number of bins. The horizontal line represents the space and time for the hash table model.\nspecific join or perhaps a combination of selection attributes. To gain\na better understanding of these errors, we now only focus on the\nqueries with a low complexity (i.e. those from the 2Jointest set). In\nTable 2, we take the Easy(PostgreSQL) queries and show the queries\nwith the highest errors from the 2Joinset. For succinctness, we\nannotate each query with the names of the relations it joins (relations\nare listed in parenthesis) and its selection predicates. We further add\nthe absolute error of the query in brackets.\nFor IMDB, the hardest queries for the NN and RNN are similar.\nThese queries consist of joins with cast_inf o and either role_type\norname . All queries also have a selection predicate on the role_id\ncolumn with values between 6 and 11. Figure 10 shows the value\ndistributions for different attributes. The first row shows all selection\ncolumns for IMDB, the second for DMV , and third for TPC-H. The\nx-axis in each graph represents the column value and the y-axis\nrepresents the frequency of the value. In Figure 10a, we show the\ndistribution of the role_idcolumn. The red bars represent the values\nfor which we see the highest errors for the NN and RNN models,based on Table 2. Compared to the other existing selection attributes,\nrole_idcomes from the largest relation in the dataset, cast_inf o .\nWe generally observe that the models have the highest errors for\ncolumns that belong to the largest relations and specifically at the\npoints where the distribution is irregular.\nFor the DMV dataset, the hardest queries are those that contain the\naccidents relation and join with time orlocation . These queries have\nselection predicates on the yearandmonth columns. We highlight\nthe selection predicate values in Figure 10f and Figure 10g. We note\nthat there is a one-to-one mapping between the accidents andtime\nrelation, so the distribution for these columns does not change due\nto the join. This is also the case for the join between accidents and\nlocation . The year column in the accidents relation has a high skew\nand the models have the highest errors for the more frequent values.\nThe accidents relation also happens to be the largest relation in the\nDMV dataset.\nFor the TPC-H dataset, most of the errors come from the join\nbetween lineitem andorders . These contain selection predicates\n10\n\n(a) IMDB Remove Values\nproduction _year\n(b) DMV Remove Selection Values\nyear\n(c) TPC-H Remove Selection\nl_quantit y\nFigure 13: Removing 10% selection predicate values across all datasets\non both the l_linenumber andl_quantity. The pearson correlation\nfor these two attributes is low ( .0002) so these are independent\nattributes. We highlight the values with highest errors in Figure 10j\nand Figure 10k. We note that the l_quantityin particular has an\nirregular distribution, and also belongs to the relation with the highest\nnumber of tuples in the dataset, lineitem .\nUnlimited Storage CDFs and Outlier Analysis In Figure 11,\nwe show similar graphs across all datasets, but with an unlimited\nstorage budget. The goal here is to understand how more complex\nmodels compare against the simpler ones from Figure 9. Given this\nunlimited budget, we now include the tree ensemble models. The\nPostgreSQL estimates do not significantly change even with 100K\nbins, which implies that adding finer granularity to the histograms\ndoes not significantly improve estimates. Among all the models, the\ntrees (in particular, the gradient boosted trees) have the lowest errors\noverall across all query complexities and across all datasets.\nTime vs Space vs Accuracy Trade-offs In Figure 12, we show\nthe error, space and time trade-offs for each model. First, in Fig-\nure 12a we compare the error and space. On the y-axis, we show\nthe absolute error between the predicted value and the real value on\na log scale. On the x-axis we show the space of each model on a\nlog scale. Each point represents the median error and the error bars\nrepresent the 25th and 75th percentiles. For all datasets, all variants\nof PostgreSQL have the highest errors and increasing histogram\nbin granularity does not significantly improve performance. Neu-\nral networks and recurrent neural networks are fairly competitive\nin terms of absolute error. In Section 5, we study whether deeper\nmodels actually learn more context about the relations compared to\nthe shallower ones. Models that are deeper are much larger in terms\nof space, with small error improvement over simpler models.\nIn Figure 12b, we compare the accuracy to the time (in seconds) it\ntakes to train each model. We do not include the time it takes to run\nthe hyperparameter tuning and we do not include the time it takes to\nrun the training queries.\nGiven their large sizes, an important question is whether the\nmodels improve upon simply keeping the entire query workload\nin a hash table (with query features as keys and cardinalities asvalues). To answer this question we plot the overhead of such a hash\ntable. Given that our training data consists of only 100K samples\nfor each query complexity, our goal is to understand whether the\ndeep learning models can actually compress information and still\nprovide a good accuracy. For the hash table model, we assume that\neach feature for each training example is equivalent to one weight\nwhen measuring space. To measure time, we measure the time it\ntakes to populate the hash table. We mark this implementation in the\ngraphs as a vertical dashed line. For this model, the error is 0 for\neach query.\nFor each dataset, all variants of the tree ensemble models result\nin the lowest errors. For the random forest model, trees with the\nlowest error are those with a single decision tree. Since we build\nthese models to overfit to a specific query workload, using a single\ndecision tree results in the lowest error. Once more decision trees\nare introduced, the error is higher as these models no longer overfit\nand attempt to generalize over the training set. For the boosted trees\nmodels, generating more trees incrementally lowers the residual\nerrors, and as a result, does not impact the overfitting. This also\ndepends on how we tune the boosted tree models. Since our goal is\nto overfit, the best boosted tree models are those that contain a high\ndepth and high shrinkage rate. These results suggest that for overfit\nworkloads, the ensemble models are able to build these models\nquickly and more accurately compared to the deep learning models.\nThe deep learning models are able to save in space and although\nthey are not as accurate as the trees, they can still improve errors in\nsome cases by an order of magnitude compared to PostgreSQL.\n4.3 Model Robustness\nIn this section, we study how robust these models are in the face\nofunknown queries. That is, instead of overfitting each model to\na specific set of queries, we remove some query samples from the\ntraining data. We focus on the most challenging, the 6Joinqueries.\nWe evaluate the most complex models for the RNN and NN (1000w,\n5d) as these perform favorably for the 6Joinset. We also select the\nbest performing version of the random forest model and booted tree\nmodels.\n11\n\n(a) IMDB Remove Join\n(b) TPC-H Remove Join\nFigure 14: Removing joins from the training workload\nRemoving Selections In the first row of Figure 13, we re-\nmove 10% of values from three columns (one from each dataset):\nproduction _year(IMDB) in Figure 13a, year(DMV) in Figure 13b\nandl_quantity(TPC-H) in Figure 13c. As shown in Figure 13a, for\nthe IMDB dataset, the accuracy of the tree ensembles outperforms\nthe other models. For both the DMV and TPC-H dataset (shown\nin columns 2 and 3) of Figure 13, the NN and RNN models turn\nout to be more robust compared to the trees. In these graphs, we\nalso included the accuracy of the hash table model implementation.\nSince the data points in the test set are not included in the training\nset, we use a nearest neighbor approach to find the closest sample\nthat exists in the training set (stored in the hash table). We use the\nnearest neighbor implementation from sklearn [ 55] which uses the\nminkowski distance metric. In many cases, the hash model performs\nsimilarly to the tree ensemble models, except for IMDB, where the\nhash model is not as accurate. For the IMDB dataset, we generate\n100K random query samples uniformly from the set of all possible\nqueries, but unlike the other two datasets, 100K queries doesn’t fully\ncover the set of all possible queries for this dataset. As a result, the\nnearest neighbor is not always as close for this database as it is for\nthe other two.\nRemoving Joins In Figure 14a and Figure 14b, we remove a\njoin with a specific combination of tables from the IMDB andTPC-H 6Joindatasets. During training, the models observe how\ncertain tables join with each other, but they never see the specific\ncombinations we remove. In Figure 14a, we remove the join be-\ntween relations:{complete _cast,aka_title,movie_inf o _idx,title,\nmovie_companies ,movie_link}from the training set. The queries\nshown in the figure correspond to the test set, which includes the\nremoved combination of tables with random selection predicates.\nFor the IMDB dataset, we observe that the tree models rely heavily\non features fromIselpred . We found that the IMDB dataset contains\ncombinations of tables in the training data that are very similar (and\nyield the same cardinality) as the combination of tables we removed\nfrom the training. The hash table model has the worst accuracy, since\nthe nearest neighbor at times selects queries with selection predicates\non the same values but different underlying tables.\nIn Figure 14b, we observe a similar trend. For this experiment, we\nremove a join from the TPC-H dataset with the relations: {customer ,\nlineitem ,partsupp ,nation ,part,orders}. For this dataset, the RNN\nand NN models are more accurate compared to the tree ensembles\nand the hash table model.\nUltimately, these graphs show that the deep learning models\nare generally more robust in scenarios where we are not simply\noverfitting the models to known training data.\n4.4 Model Latents\nOne challenge of training deep neural networks is the difficulty to\nunderstand what the models are actually learning. As discussed in\nSection 4.3, the tree models are easily interpretable as we can track\npath of decision splits to understand how the model is able to predict\nthe outcome given the input. For neural networks, diagnosing why\na model arrives at a specific answer is a harder problem. There are\nseveral existing approaches, which include masking or altering the\ninput to measure the predication change and studying hidden unit\nactivation values [32, 63, 80].\nWe study the activation values of the hidden layers for the NN\nand RNN models. During training, these models take the input, X,\nand propagate it through a series of transformations that represent\nthe data at different levels of abstraction [ 26]. Taking a close look\nat the activation values (also referred to as latent representations or\nembeddings ) can help diagnose what the model is learning from the\ninputs. For example, if we cluster training samples based on their\nlatents, we can determine whether models are in fact generating\nsimilar representations for queries that are semantically similar.\nWe use the t-SNE technique to cluster latents, which is a dimen-\nsionality reduction technique that is often used to visualize high-\ndimensional data [ 73]. This approach has an objective function that\nminimizes the KL-divergence between a distribution that measures\npairwise similarities of the objects in high-dimensional space and a\ndistribution that measures the pairwise similarities of the correspond-\ning low-dimensional points [ 73]. Compared to principal component\nanalysis (PCA), t-SNE is non-linear and relies on probabilities.\nWe cluster latent vectors from the (100w, 1d) NN model for the\n6Jointraining set from each dataset. In Figure 15, we reduce the\ndimensionality of the latents from the (100w, 1d) model on the TPC-\nH dataset (100 hidden units total) down to three dimensions, which\nis the highest number of dimensions allowed for t-SNE. In the figure,\nthere are four clusters, each representing different sets of joins:\n12\n\nCluster 1Cluster 2\nCluster 3Cluster 4Figure 15: Clustering dimensionality-reduced latents for the\nNN (100w, 1d) model on the TPC-H dataset.\n•Cluster 1: customer,lineitem,nation,orders,partsupp,region\n•Cluster 2: customer,lineitem,orders,part,partsupp,supplier\n•Cluster 3: customer,lineitem,nation,orders,partsupp,supplier\n•Cluster 4: customer,lineitem,nation,orders,part,partsupp\nFor t-SNE, the distance between clusters is irrelevant, the more\nimportant factor is the relevance among the points that are clustered\ntogether. For the DMV dataset and IMDB, the clusters do not repre-\nsent combinations of relations, but we observe that queries that are\nnear each other share similar selection predicate values.\nFor the RNN (100w, 1d) model, we find that clusters are deter-\nmined based on the sequence of operations. Recall, during training,\nthe RNN learns to predict cardinalities for different join sequences,\nas a result of observing many queries. We observe that the resulting\nclusters represent queries that end with similar operations. For exam-\nple, one cluster contains combinations of relations orders ,lineitem ,\npartsupp , but always ends the sequence with joins on either the\nsupplier andpart relation or customer andsupplier . We find that\ncomplex models (1000w, 5), also show a similar trend. This is ac-\ntually a side-effect of RNNs, as more recent actions have a heavier\ninfluence on the content that exists in the hidden states. More specif-\nically, it is difficult to learn long-term dependencies as the gradient\nis much smaller compared to short-term interactions [26].\nAs an additional experiment, we cluster the latents from queries\nthat have not been included in the training. Ideally, although these\nqueries have never been observed by the model, they should cluster\nwith similar training queries. We focus on the TPC-H join removal\nscenario, originally shown in Figure 14b. When we cluster the latents\nfrom the (1000w, 5d) NN model, the queries that were not included\nin the training are clustered separately from the rest. This seems to\nimply that the NN does not learn the interactions between subqueries.\nThis is not the case for the RNN, as queries that are left out of training\nare clustered together with queries that have similar subqueries. For\nexample, a query that joins relations lineitem ,orders ,partsupp ,\ncustomer ,part, and nation , is clustered together with queries that\ncontain relations lineitem ,orders ,partsupp ,customer ,part, and\nsupplier .5 PRACTICAL CONSIDERATIONS\nIn this section, we study two additional practical considerations.\nIn Section 4, we evaluated the accuracy of cardinality estimates pro-\nduced by the different models. In this section, we evaluate whether\nthe cardinality estimate improvements lead to faster query execution\nplans. Additionally, in earlier sections, we showed the trade-offs\nbetween prediction error and time overhead due to model training.\nWe did not consider the time that it takes to execute the training\nqueries. To minimize this overhead, we discuss and consider using\nactive learning as a way to reduce the time spent generating training\nsets.\n5.1 Impact on Query Plans\nWe evaluate the impact of these models starting with a simple RNN\nmodel (100w, 1d) and going to a more complex one (1000w, 5d). We\nuse the RNN, as query optimization requires evaluating cardinalities\nfor several possible subqueries that could exist in the final plan. We\nevaluate the performance benefit for queries with 6 relations for each\nof the three datasets. As we collect the subquery cardinalities from\nthe RNN, these estimates are then fed into a version of PostgreSQL\nmodified to accept external cardinality estimates [14].\nIn Figure 16, we show the performance impact of these improved\ncardinalities compared to the default cardinality estimates from\nPostgreSQL. First, for the IMDB dataset, we show the performance\nimprovement for 50 queries in Figure 16a. The runtimes for these\nqueries range from <1sec up to 200sec. The simple RNN model\nimproves the performance of 54% of the queries, while the complex\nmodel improves 60% of the queries. For the simpler model, query\n22 is an outlier where the model’s estimates actually slows down the\nquery considerably (from 2 seconds up to 39 seconds). In contrast,\nthere is no significant slow down on any query for the complex\nmodel.\nFor the DMV dataset, both the simple model and complex model\nimprove the performance for 76% of the queries and there is no\nsignificant slow down for any query. We should note, however, that\na majority of the query runtimes in this dataset range from 1 to 3\nseconds. Finally, for the TPC-H dataset, the complex model improves\n90% of the queries. The simpler model also makes a significant\nimprovement, speeding up 84% of the queries. The query execution\ntimes for this dataset range from 20 to 120 seconds.\n5.2 Reducing the Training Time\nBuilding a model can be time consuming depending on the size of\nthe model, the training time, and the amount of time that it takes to\ncollect the training samples. To train the models shown in Section 4,\nwe needed to run a large set of random queries to collect their\nground-truth cardinalities, the output Y, for the models. Depending\non the complexity of these queries, running them and collecting\nthese labels can be time consuming. This process can be parallelized,\nbut it comes with a resource cost.\nModels can be trained in several ways. One approach to reducing\nthe time to collect training samples, is to train the model in an online\nfashion. That is, as the user executes queries while using the system,\nthe model can train on only those queries. The learning happens in an\nincremental fashion, and updates the model after observing a batch\n13\n\n(a) IMDB Impact\n (b) DMV Impact\n (c) TPC-H Impact\nFigure 16: Query execution time speed-ups thanks to cardinality estimates from simple or complex RNN\nof samples. This approach can work well if the user executes similar\nqueries. Online learning can also be fast and memory efficient, but\nthe learning may experience a drift [ 25], where the model’s decision\nboundary changes largely depends on the latest samples it observes.\nAlternatively, instead of relying on a user to provide query sam-\nples, we can use a technique known as active learning . Active learn-\ning selects the best sample of candidates to improve a model’s\nobjective and to train as effectively as possible [ 35]. It is ideal in\nsettings where labeled examples are expensive to obtain [15].\nActive learning works through a series of iterations. In each itera-\ntion, it determines unlabeled points to add to the training sample to\nimprove the model. Given a large pool of unlabeled samples, active\nlearning will select the unlabeled sample that should be annotated\nto improve the model’s predictions. In our context, given a large\npool of unlabeled queries, active learning should help narrow down\nwhich queries to execute next.\nThere are various existing active learning methods. Common\ntechniques include using uncertainty sampling, query-by-committee\n(QBC), and expected model change [ 35]. In this work, we focus\non using QBC [ 64]. After each active learning iteration, QBC first\nbuilds a committee of learners from the existing training dataset via\nbootstrapping [ 78]. Each learner in the committee makes a prediction\nfor all the samples in the unlabeled pool. The sample with the highest\ndisagreement is labeled and added to the training pool. For regression\ntasks, this disagreement can be measured by the variance in the\npredictions across the learners [60].\nTraditionally, active learning only adds a single informative sam-\nple in each data sampling iteration [ 13]. More recently, batch-model\nAL (BMAL), where multiple samples are labeled in each iteration\nhas become more prevalent, as labeling in bulk or in parallel has been\nmore accessible in recent years [ 16]. As shown in work by Wu et.\nal. [78] careful attention must be placed in picking out diverse points\nwith BMAL, as models might disagree on a batch that contains very\nsimilar points, leading to suboptimal results.\nWe use BMAL in the following experiment and run three different\nmethods to help select the unlabeled points for each iteration:\n(1)QBC : after each iteration, we train an ensemble of models\nand select the top Kpoints with the highest disagreement(2)QBC+Clustering : we train an ensemble of models, but pick\nout the top Kdiverse set of points through clustering, which\nis based on the technique from [78] for linear regression\n(3)Random : we select a random sample of points from the\nunlabeled pool\nFor each dataset, we use training samples from the 2Join,4Join,\nand 6Joinset along with all their subqueries, for a total of 600K\nsamples for the model. We run two experiments. In the first experi-\nment, we start with a small number of training samples (100) and\nsetK=100. For the second experiment, we start with a larger sample\n(1000) and set K=1000. As the number of training samples is small,\nwe include regularization to prevent overfitting.\nIn Figure 17, we show the loss of each technique for three active\nlearning iterations on each dataset. We show the results for both\nexperiments ( K=100 and K=1000). Each point represents the average\nloss for three separate runs. For each graph, we also include the loss\nfor the case where all samples are labeled (labeled as “all training”).\nIn general, we find that with small training sets, QBC and\nQBC+Clustering result in a lower loss, particularly at the end of\nthe first iteration. For subsequent iterations, the random technique\nperforms just as well and in some cases even better, as in the TPC-H\ndataset for example. QBC is competitive, but it often overfits as\nshown by the cases where the loss increases (IMDB and TPC-H).\nThis is expected, as BMAL techniques are known to select a distinct\nset of points to improve the loss more effectively.\nWhen the training set is larger ( K=1000), all techniques perform\nsimilarly, negating the immediate benefit of active learning. Never-\ntheless, adding fewer points rather than the entire training set can\nstill reach a loss that is approximately an order of magnitude away\nfrom the loss that includes all the training data.\n6 RELATED WORK\nLearning Optimizers Leo [ 68], was one of the first approaches to\nautomatically adjust an optimizer’s estimates based on past mistakes.\nThis requires successive runs of similar queries to make adjustments.\nSimilarly, in the effort of using a self-correcting loop, others\nhave proposed a “black-box” approach to cardinality estimation\n14\n\nFigure 17: Active Learning\nby grouping queries into syntactic families [ 44]. Machine learning\ntechniques are then used to learn the cardinality distributions of these\nqueries based on features describing the query attributes, constants,\noperators and aggregates. They specifically focus on applications that\nhave fixed workloads do not require fine-grained, sub-plan estimates.\nWork by Marcus et al. [46] uses a deep reinforcement learning\ntechnique to find optimal join orders to improve query latency on a\nfixed database. They use cost estimates from PostgreSQL to boot-\nstrap the learning and continuously improve the accuracy of the\nmodel’s rewards during training. Related work by Sanjay et al. [38],\nalso uses deep reinforcement learning to improve query plans, but\nthey assume perfect cardinality predictions for base relations.\nNeural Networks and Cardinality Estimation Liuet al. [43]\nuse neural networks to solve the cardinality estimation problem, but\nfocus on selection queries only. Hasan et al. [28] also only focus\non selectivity estimation, but show that deep learning models are\nparticularly successful at predicting query cardinalities with a large\nnumber of selection predicates.\nWork by Kipf et al. [34] proposes a new deep learning approach\nto cardinality estimation by using a multi-set convolutional network.\nCardinality estimation does improve, but they do not show improve-\nment of query plans. In addition, our work explores the space, time,\naccuracy of these models across a variety of datasets. Work by Wolt-\nmann et al. [76] propose building specialized neural network models\nthat focus on a specific part of the schema (i.e. a join between two\nrelations). These local models are beneficial as they reduce the query\nsample space required for training, and thus, reduce the training time.\nIn our work, we propose to use active learning as an approach to help\nreduce the number of training samples. Work by Dutt et al. [23] also\ncompare tree ensembles and neural network models for cardinality\nestimation, but only focus on selectivities for single relations.\nWork by Kraska et al. [37] uses a mixture of neural networks to\nlearn the distribution of an attribute with a focus on building fast\nindexes. In SageDB [ 36], this work is extended towards building a\nnew system that learns the underlying structure of the data to provide\noptimal query plans. In their work, they state that one key aspect\nin successfully improving these query plans is through cardinalityestimation. They are currently working on a hybrid model-based\napproach to cardinality estimation, where they balance between\nlooking for a model that can learn the distribution of the data and\na model that can capture the extreme outliers and anomalies of the\ndata.\nWuet al. [77] learn several models to predict the cardinalities\nfor a variety of template subgraphs in a dataset instead of building\none large model. Input features include filters and parameters for\nthe subgraph, but they do not featurize information about the dataset\n(i.e. the relations). Thus, their models cannot make predictions for\nunobserved subgraph templates.\n7 CONCLUSION\nWe show the promise of using deep learning models to predict query\ncardinalities. In our study we found that even simple models can\nimprove the runtimes of several queries across a variety of datasets.\nAcknowledgements This project was supported in part by the\nGraduate Opportunities and Minority Achievement Program (GO-\nMAP) fellowship, NSF grant IIS-1524535 and Teradata.\nREFERENCES\n[1]An empirical analysis of deep learning for cardinality estimation (anonymized\ntech-report). https://tinyurl.com/y2999vbz.\n[2]Statistics collection recommendations - Teradata. http://knowledge.teradata.com/\nKCS/id/KCS015023.\n[3]Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey\nDean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Man-\njunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,\nBenoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan\nYu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning.\nIn12th USENIX Symposium on Operating Systems Design and Implementation\n(OSDI 16) , pages 265–283, 2016.\n[4]Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, and Bohan Zhang. Automatic\ndatabase management system tuning through large-scale machine learning. In\nProceedings of the 2017 ACM International Conference on Management of Data,\nSIGMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017 , pages 1009–\n1024, 2017.\n[5] AlphaGo. https://deepmind.com/research/alphago/.\n[6] Amazon AWS. http://aws.amazon.com/.\n[7]K. Arulkumaran et al. Deep reinforcement learning: A brief survey. IEEE Signal\nProcessing Magazine , 2017.\n15\n\n[8]Ron Avnur et al. Eddies: Continuously adaptive query processing. SIGMOD\nRecord , 2000.\n[9]Shivnath Babu et al. Adaptive query processing in the looking glass. In CIDR\n2015 .\n[10] Debabrota Basu, Qian Lin, Weidong Chen, Hoang Tam V o, Zihong Yuan, Pierre\nSenellart, and Stéphane Bressan. Cost-model oblivious database tuning with\nreinforcement learning. In DEXA , pages 253–268, 2015.\n[11] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information\nScience and Statistics) . Springer-Verlag New York, Inc., Secaucus, NJ, USA,\n2006.\n[12] Nicolas Bruno, Surajit Chaudhuri, and Luis Gravano. STHoles: A multidimen-\nsional workload-aware histogram. In SIGMOD Conference , 2001.\n[13] W. Cai, M. Zhang, and Y . Zhang. Batch mode active learning for regression with\nexpected model change. IEEE Transactions on Neural Networks and Learning\nSystems , 28(7):1668–1681, July 2017.\n[14] Walter Cai, Magda Balazinska, and Dan Suciu. Pessimistic cardinality estimation:\nTighter upper bounds for intermediate join cardinalities. In SIGMOD , 2019.\n[15] Wenbin Cai, Ya Zhang, and Jun Zhou. Maximizing expected model change for\nactive learning in regression. 2013 IEEE 13th International Conference on Data\nMining , pages 51–60, 2013.\n[16] S. Chakraborty, V . Balasubramanian, and S. Panchanathan. Adaptive batch mode\nactive learning. IEEE Transactions on Neural Networks and Learning Systems ,\n26(8):1747–1760, Aug 2015.\n[17] Surajit Chaudhuri. An overview of query optimization in relational systems.\nPODS, 1998.\n[18] Surajit Chaudhuri and Vivek Narasayya. Program for TPC-H data generation with\nskew.\n[19] Graham Cormode, Minos N. Garofalakis, Peter J. Haas, and Chris Jermaine.\nSynopses for massive data: Samples, histograms, wavelets, sketches. Foundations\nand Trends in Databases , 4(1-3):1–294, 2012.\n[20] Antonio Criminisi and Jamie and Shotton. Decision Forests: A Unified Framework\nfor Classification, Regression, Density Estimation, Manifold Learning and Semi-\nSupervised Learning , volume 7, pages 81–227. NOW Publishers, foundations\nand trendsÂ ˝o in computer graphics and vision: vol. 7: no 2-3, pp 81-227 edition,\nJanuary 2012.\n[21] Misha Denil, David Matheson, and Nando De Freitas. Narrowing the gap: Random\nforests in theory and in practice. In Eric P. Xing and Tony Jebara, editors, Pro-\nceedings of the 31st International Conference on Machine Learning , volume 32\nofProceedings of Machine Learning Research , pages 665–673, Bejing, China,\n22–24 Jun 2014. PMLR.\n[22] Anshuman Dutt and Jayant R. Haritsa. Plan bouquets: query processing without\nselectivity estimation. In International Conference on Management of Data,\nSIGMOD 2014, Snowbird, UT, USA, June 22-27, 2014 , pages 1039–1050, 2014.\n[23] Anshuman Dutt, Chi Wang, Azade Nazi, Srikanth Kandula, Vivek Narasayya, and\nSurajit Chaudhuri. Selectivity estimation for range predicates using lightweight\nmodels. Proc. VLDB Endow. , 12(9):1044–1057, May 2019.\n[24] Todd Eavis et al. Rk-hist: An R-tree based histogram for multi-dimensional\nselectivity estimation. In CIKM ’07 .\n[25] João Gama, Indr ˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid\nBouchachia. A survey on concept drift adaptation. ACM Comput. Surv. , 46(4):44:1–\n44:37, March 2014.\n[26] Ian Goodfellow et al. Deep Learning . MIT Press, 2016. http://www.\ndeeplearningbook.org.\n[27] Daniel Halperin et al. Demonstration of the Myria big data management service.\nInSIGMOD , pages 881–884, 2014.\n[28] Shohedul Hasan, Saravanan Thirumuruganathan, Jees Augustine, Nick Koudas,\nand Das Gautam. Multi-attribute selectivity estimation using deep learning.\n[29] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical\nLearning . Springer Series in Statistics. Springer New York Inc., New York, NY ,\nUSA, 2001.\n[30] Max Heimel, Martin Kiefer, and V olker Markl. Self-tuning, GPU-accelerated\nkernel density models for multidimensional selectivity estimation. In Proceedings\nof the 2015 ACM SIGMOD International Conference on Management of Data ,\nSIGMOD ’15, 2015.\n[31] Tin Kam Ho. Random decision forests. In Proceedings of the Third International\nConference on Document Analysis and Recognition (Volume 1) - Volume 1 , ICDAR\n’95, pages 278–, Washington, DC, USA, 1995. IEEE Computer Society.\n[32] Minsuk Kahng, Pierre Y . Andrews, Aditya Kalro, and Duen Horng Chau. Ac-\ntivis: Visual exploration of industry-scale deep neural network models. IEEE\nTransactions on Visualization and Computer Graphics , 24:88–97, 2018.\n[33] Martin Kiefer, Max Heimel, Sebastian Breß, and V olker Markl. Estimating\njoin selectivities using bandwidth-optimized kernel density models. PVLDB ,\n10(13):2085–2096, 2017.\n[34] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter A. Boncz, and\nAlfons Kemper. Learned cardinalities: Estimating correlated joins with deep\nlearning. In CIDR 2019, 9th Biennial Conference on Innovative Data Systems\nResearch, Asilomar, CA, USA, January 13-16, 2019, Online Proceedings , 2019.[35] Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning\nfrom data. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural Information Processing\nSystems 30 , pages 4225–4235. Curran Associates, Inc., 2017.\n[36] Tim Kraska, Mohammad Alizadeh, Alex Beutel, Ed H. Chi, Ani Kristo, Guillaume\nLeclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. Sagedb: A learned\ndatabase system. In CIDR 2019, 9th Biennial Conference on Innovative Data\nSystems Research, Asilomar, CA, USA, January 13-16, 2019, Online Proceedings ,\n2019.\n[37] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case\nfor learned index structures. In Proceedings of the 2018 International Conference\non Management of Data , SIGMOD ’18, pages 489–504, New York, NY , USA,\n2018. ACM.\n[38] Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph M. Hellerstein, and Ion\nStoica. Learning to optimize join queries with deep reinforcement learning.\n[39] Viktor Leis et al. Cardinality estimation done right: Index-based join sampling. In\nCIDR 2017 .\n[40] Viktor Leis et al. How good are query optimizers, really? Proc. VLDB Endow. ,\n2015.\n[41] Timothée Lesort et al. State representation learning for control: An overview.\nCoRR , 2018.\n[42] Timothée Lesort, Mathieu Seurin, Xinrui Li, Natalia Díaz Rodríguez, and David\nFilliat. Unsupervised state representation learning with robotic priors: a robustness\nbenchmark. CoRR , 2017.\n[43] Henry Liu, Mingbin Xu, Ziting Yu, Vincent Corvinelli, and Calisto Zuzarte. Car-\ndinality estimation using neural networks. In Proceedings of the 25th Annual\nInternational Conference on Computer Science and Software Engineering , CAS-\nCON ’15, pages 53–59, Riverton, NJ, USA, 2015. IBM Corp.\n[44] Tanu Malik, Randal C. Burns, and Nitesh V . Chawla. A Black-Box Approach to\nQuery Cardinality Estimation. In CIDR 2007 , pages 56–67, 2007.\n[45] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. Re-\nsource management with deep reinforcement learning. In Proceedings of the 15th\nACM Workshop on Hot Topics in Networks , HotNets ’16, 2016.\n[46] Ryan Marcus and Olga Papaemmanouil. Towards a hands-free query optimizer\nthrough deep learning. In CIDR 2019, 9th Biennial Conference on Innovative Data\nSystems Research, Asilomar, CA, USA, January 13-16, 2019, Online Proceedings ,\n2019.\n[47] V olodymyr Mnih et al. Human-level control through deep reinforcement learning.\nNature , 2015.\n[48] Guido Moerkotte. In Building Query Compilers , 2014.\n[49] Kristi Morton, Magdalena Balazinska, and Dan Grossman. Paratimer: A progress\nindicator for MapReduce DAGs. In Proceedings of the 2010 ACM SIGMOD\nInternational Conference on Management of Data , SIGMOD ’10, 2010.\n[50] Patrick O’Neil, Elizabeth O’Neil, and Xuedong Chen. Star schema benchmark.\nhttp://www.cs.umb.edu/~poneil/StarSchemaB.PDF.\n[51] Jennifer Ortiz, Magdalena Balazinska, Johannes Gehrke, and S. Sathiya Keerthi.\nLearning state representations for query optimization with deep reinforcement\nlearning. In Proceedings of the Second Workshop on Data Management for End-\nTo-End Machine Learning , DEEM’18, pages 4:1–4:4, New York, NY , USA, 2018.\nACM.\n[52] Jennifer Ortiz, Victor Teixeira de Almeida, and Magdalena Balazinska. Changing\nthe face of database cloud services with personalized service level agreements. In\nCIDR 2015, Seventh Biennial Conference on Innovative Data Systems Research,\nAsilomar, CA, USA, January 4-7, 2015, Online Proceedings , 2015.\n[53] Jennifer Ortiz et al. Perfenforce demonstration: Data analytics with performance\nguarantees. In SIGMOD , 2016.\n[54] Terence Parr and Jeremy Howard. How to explain gradient boosting, 2018.\nhttps://explained.ai/gradient-boosting/.\n[55] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel,\nM. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine\nlearning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011.\n[56] PostgreSQL. https://www.postgresql.org/.\n[57] J. Ross Quinlan. Learning with continuous classes. 1992.\n[58] Tilmann Rabl, Michael Frank, Hatem Mousselly Sergieh, and Harald Kosch. A\ndata generator for cloud-scale benchmarking. In TPCTC’10 , pages 41–56.\n[59] Raghu Ramakrishnan and Johannes Gehrke. Database Management Systems .\nMcGraw-Hill, Inc., New York, NY , USA, 3 edition, 2003.\n[60] Jakub Repicky. Active learning in regression tasks, 2017.\n[61] B. Rosman and S. Ramamoorthy. What good are actions? accelerating learning us-\ning learned action priors. In 2012 IEEE International Conference on Development\nand Learning and Epigenetic Robotics (ICDL) , 2012.\n[62] P. Griffiths Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and T. G.\nPrice. Access path selection in a relational database management system. In\nProceedings of the 1979 ACM SIGMOD International Conference on Management\nof Data , SIGMOD ’79, 1979.\n[63] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-\ntam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep\n16\n\nnetworks via gradient-based localization. 2017 IEEE International Conference on\nComputer Vision (ICCV) , pages 618–626, 2017.\n[64] H. S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings\nof the Fifth Annual Workshop on Computational Learning Theory , COLT ’92,\npages 287–294, New York, NY , USA, 1992. ACM.\n[65] Noam Shazeer et al. Outrageously large neural networks: The sparsely-gated\nmixture-of-experts layer. CoRR , 2017.\n[66] David Silver. UCL Course on Reinforcement Learning, 2015.\n[67] Sandro Skansi. Introduction to Deep Learning - From Logical Calculus to Artificial\nIntelligence . Undergraduate Topics in Computer Science. Springer, 2018.\n[68] Michael Stillger, Guy M. Lohman, V olker Markl, and Mokhtar Kandil. Leo -\ndb2’s learning optimizer. In Proceedings of the 27th International Conference on\nVery Large Data Bases , VLDB ’01, pages 19–28, San Francisco, CA, USA, 2001.\nMorgan Kaufmann Publishers Inc.\n[69] Richard S. Sutton and Andrew G. Barto. Reinforcement learning I: Introduction,\n1998.\n[70] Richard S. Sutton et al. Reinforcement learning I: Introduction, 2016.\n[71] Csaba Szepesvari. Algorithms for reinforcement learning. Morgan and Claypool\nPublishers, 2009.\n[72] Kostas Tzoumas et al. A reinforcement learning approach for adaptive query\nprocessing. In A DB Technical Report , 2008.\n[73] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne, 2008.\n[74] Jingjing Wang, Tobin Baker, Magdalena Balazinska, Daniel Halperin, Brandon\nHaynes, Bill Howe, Dylan Hutchison, Shrainik Jain, Ryan Maas, Parmita Mehta,\nDominik Moritz, Brandon Myers, Jennifer Ortiz, Dan Suciu, Andrew Whitaker,\nand Shengliang Xu. The Myria big data management and analytics system and\ncloud services. In CIDR 2017, 8th Biennial Conference on Innovative Data\nSystems Research , 2017.\n[75] Wei Wang et al. Database Meets Deep Learning: Challenges and Opportunities.\nSIGMOD Record , 2016.\n[76] Lucas Woltmann, Claudio Hartmann, Maik Thiele, Dirk Habich, and Wolfgang\nLehner. Cardinality estimation with local deep learning models. In Proceedings of\nthe Second International Workshop on Exploiting Artificial Intelligence Techniques\nfor Data Management , aiDM ’19, pages 5:1–5:8, New York, NY , USA, 2019.\nACM.\n[77] Chenggang Wu, Alekh Jindal, Saeed Amizadeh, Hiren Patel, Wangchao Le, Shi\nQiao, and Sriram Rao. Towards a learning optimizer for shared clouds. PVLDB ,\n12(3):210–222, 2018.\n[78] Dongrui Wu. Pool-based sequential active learning for regression. IEEE transac-\ntions on neural networks and learning systems , 2018.\n[79] Wentao Wu et al. Sampling-based query re-optimization. SIGMOD 2016.\n[80] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional\nnetworks. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,\neditors, Computer Vision – ECCV 2014 , pages 818–833, Cham, 2014. Springer\nInternational Publishing.\n17",
  "textLength": 85130
}