{
  "paperId": "61a93b9078077fcbedcaeaa747a2f29b4347414e",
  "title": "Spitz",
  "pdfPath": "61a93b9078077fcbedcaeaa747a2f29b4347414e.pdf",
  "text": "Spitz: A Veriﬁable Database System\nMeihui Zhang1Zhongle Xie2Cong Yue2Ziyue Zhong1\n1Beijing Institute of Technology2National University of Singapore\nmeihui zhang@bit.edu.cn, zhongle@comp.nus.edu.sg, yuecong@comp.nus.edu.sg, ziyue zhong@bit.edu.cn\nABSTRACT\nDatabases in the past have helped businesses maintain and\nextract insights from their data. Today, it is common for\na business to involve multiple independent, distrustful par-\nties. This trend towards decentralization introduces a new\nand important requirement to databases: the integrity of the\ndata, the history, and the execution must be protected. In\nother words, there is a need for a new class of database sys-\ntems whose integrity can be veri\fed (or veri\fable databases).\nIn this paper, we identify the requirements and the design\nchallenges of veri\fable databases. We observe that the main\nchallenges come from the need to balance data immutabil-\nity, tamper evidence, and performance. We \frst consider\napproaches that extend existing OLTP and OLAP systems\nwith support for veri\fcation. We next examine a clean-slate\napproach, by describing a new system, Spitz, speci\fcally\ndesigned for e\u000eciently supporting immutable and tamper-\nevident transaction management. We conduct a preliminary\nperformance study of both approaches against a baseline\nsystem, and provide insights on their performance.\nPVLDB Reference Format:\nMeihui Zhang, Zhongle Xie, Cong Yue, Ziyue Zhong. Spitz: A\nVeri\fable Database System. PVLDB , 13(12): 3449-3460, 2020.\nDOI: https://doi.org/10.14778/3415478.3415567\n1. INTRODUCTION\nTraditional database systems are indispensable for busi-\nnesses.They excel at storing, processing, and performing an-\nalytics over business transactions. Recent digital optimiza-\ntion and transformation have enabled businesses to transact\ndirectly with each other, without relying on a central party.\nAs a result, multiple parties can access a shared database.\nSince the parties are mutually distrustful, the underlying\ndatabase must consider support for auditing, tamper evi-\ndence, and dispute resolution in its design. For instance,\nit must maintain a trusted data history and allow users to\nverify the integrity of both current and historical data.\nThis work is licensed under the Creative Commons Attribution-\nNonCommercial-NoDerivatives 4.0 International License. To view a copy\nof this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. For\nany use beyond those covered by this license, obtain permission by emailing\ninfo@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 13, No. 12\nISSN 2150-8097.\nDOI: https://doi.org/10.14778/3415478.3415567Blockchains demonstrate one practical design for database\nsystems with strong integrity [21]. Public blockchain sys-\ntems, such as Ethereum, support secure peer-to-peer ap-\nplications through smart contracts. Private blockchains,\nsuch as Hyperledger Fabric, target business settings and\nachieve higher performance than public ones. Blockchains\nhave drawn interests from banks and regulators, with the\nprospect of o\u000bering digital currency and digital banking.\nCombined with recent advances in 5G, AI and IoT, block-\nchains are expected to speed up the transformation and fur-\nther disrupt the e-commerce and \fnancial industries.\nTo make a database that can be accessed by potentially\nmalicious parties trustworthy, it must be veri\fable . A veri-\n\fable database system protects integrity of the data, of its\nprovenance, and of its query execution. More speci\fcally,\nany tampering such as changing the data content, changing\na historical record, or modifying query results, can be de-\ntected. We note that the demand for veri\fcation is on the\nrise due to the requirements imposed by the regulators on\nvarious business sectors, investment and banking in partic-\nular.\nThe \frst requirement in the design of a veri\fable database\n(VDB) is data immutability, which is necessary for main-\ntaining trusted provenance. Immutability means data is\nonly written once and never deleted. It is not a new con-\ncept. It has been used in NoSQL systems such as HBase [4],\nCouchDB [1] and RethinkDB [8] to achieve more e\u000ecient\nconcurrency, due to the fact that no synchronization of ac-\ncesses is needed. It is also used in Resilient Distributed\nDatasets (RDDs)[9] for lineage and fault tolerance. The\nsecond requirement of a veri\fable database is query veri\f-\nability. It means the query results contain integrity proofs\nfor both the data and query execution. More speci\fcally, a\nuser can detect if either the data or the query execution has\nbeen tampered.\nIn this paper, we discuss four challenges in realizing the\ndesign of veri\fable databases. The \frst challenge is in stor-\nage management, as data immutability requires managing\nthe ever-increasing volume of data. Consider a typical health-\ncare analytic application, in which health data needs to be\nkept for the lifetime of a patient, and each diagnosis, lab test,\nprescription, etc., is appended to the patient pro\fle. Disease\nand procedure coding standards evolve over time, e.g., from\nICD-9-CM to ICD-10 in recent years. Such changes in clas-\nsi\fcation and coding standards require updates or mapping\nonto the existing medical record. To ensure good data prove-\nnance, the data must be immutable and a new version of the\ndatabase, i.e., a snapshot, is appended. The data volume isarXiv:2008.09268v1  [cs.DB]  21 Aug 2020\n\n 0 200 400 600 800 1000\n10 20 30 40 50 60Storage (KB)\n#VersionsStorage-ForkBase\nStorageFigure 1: Data storage improved by deduplication.\nincreasing with time, and therefore its management needs\nto be e\u000ecient and reliable. Let us consider another exam-\nple where an immutable database stores 10 WIKI pages of\n16 KB each initially. We create a new version when updat-\ning a page, while keeping the previous versions. Figure 1\nshows the space utilized with an increasing number of ver-\nsions. Clearly, the space utilization increases substantially\nwith the number of immutable versions, and the use of an\ne\u000ecient multi-version storage engine such as ForkBase [51]\nhelps to reduce it. This highlights the importance of storage\ne\u000eciency for a database that is forever increasing in size.\nThe second challenge is to provide e\u000ecient access methods\nfor querying immutable data. While some existing database\nsystems archive historical data and support temporal query\nprocessing, they have not been designed to support \\perma-\nnent\" immutability. In VDB, data is never deleted; query\nprocessing and frequent searching on older versions of the\ndata will be prohibitively expensive if e\u000ecient storage lay-\nout and indexes are not supported. This may entail scan-\nning of a substantial portion of the database for answering\nveri\fcation queries.\nThe third challenge is to minimize performance overhead\nof veri\fcation. VDB must generate integrity proofs whose\ncost can be signi\fcant. Blockchains, for example, have poor\ntransaction throughput due to their protocols for guaran-\nteeing security in the Byzantine environment. The perfor-\nmance gap between traditional databases and blockchains is\nsigni\fcant due to their di\u000berent design focus. As a result, a\nveri\fable database must adopt a hybrid blockchain-database\napproach in order to strike a better balance between perfor-\nmance and security.\nThe fourth challenge is the need to support both OLTP\nand OLAP workloads, as illustrated by the emergence of\nHTAP database systems. The former requires serializabil-\nity which is important for applications such as e-commerce.\nMost existing OLTP systems adopt optimistic concurrency\ncontrol (OCC), instead of pessimistic concurrency control,\nbecause of its simplicity and high performance. In contrast,\nanalytic queries in OLAP do not require the strict ordering\nprovided by serializability. Existing OLAP systems adopt\nmulti-version concurrency control (MVCC) to achieve data\nconsistency with high performance. Most existing works on\nveri\fable queries focus on OLTP workloads. While general\nOLAP queries can be made veri\fable, for example by using\nfully-homomorphic encryption, they involve complex cryp-\ntographic operations and incur signi\fcant overhead [40, 47].\nFinancial\nTransactions\nLogistic\nOrders\nMedical\nRecords\nVerification Digital Token\nTransferringCredit\nReportsVerifiable \nDatabase  System\nVerifiable Data Structure\nMulti -version Store Key-Value Store Row Store Columnar Store\nVersion\nManagementE-commerce\nEvents\nFigure 2: Veri\fable database system overview.\nTherefore, it is challenging to support both veri\fable OLTP\nand OLAP queries with practical performance.\nIn addition to the four challenges above, we note that\nVDB must also aim for deployability. It is often costly\nto either add a new database system into an existing in-\nfrastructure, or to replace an existing database with a new\none. In particular, for a business with consolidated soft-\nware stacks, data conversion is necessary to move data to\nthe new database. Furthermore, users may \fnd the system\ndi\u000ecult to use if the veri\fable database adopts unfamiliar\nprogramming models or interface.\nIn this paper, we discuss two approaches to realize an\ne\u000ecient VDB. The \frst approach is to extend existing sys-\ntems, and the second is to design a new system from scratch.\nFigure 2 shows the second approach and how it \fts with\nexisting business applications. The new system uses tamper-\nevident structures for veri\fcation, and e\u000ecient version man-\nagement for performance.\nWe make the following contributions in this paper:\n\u000fWe identify the requirements and design challenges of\ne\u000ecient veri\fable databases.\n\u000fWe discuss two approaches for realizing an e\u000ecient ver-\ni\fable database: by extending existing systems, and a\nnew clean-slate design called Spitz.\n\u000fWe perform an experimental study on Spitz and com-\npare it with a baseline. The results show that Spitz\ncan achieve good performance, despite overhead from\nveri\fcation and additional data structures.\n\n\u000fWe discuss various future research topics, including the\nintegration of learning methods onto VDB and version\nmanagement of the machine learning pipeline.\nThe rest of this paper is organized as follows. In Sec-\ntion 2, we present existing works and systems that are re-\nlated to VDB processing. In Section 3, we discuss the re-\nsearch challenges and opportunities of VDB. We next de-\nscribe the challenges in the approach of extending existing\nOLTP and OLAP system to implement VDB in Section 4.\nIn Section 5, we present the system architecture of Spitz. We\nthen present an experimental study in Section 6, and com-\npare our systems against a baseline implementation based\non a commercial service. We discuss the promising synergy\nbetween VDB and AI in Section 7, before concluding in Sec-\ntion 8.\n2. VERIFIABLE DATABASES\nIn this section, we survey existing works and systems with\nveri\fcation features.\n2.1 Veriﬁable Database\nData integrity is important in outsourced database as\nthird-party service providers can be malicious. In partic-\nular, applications running on top of an outsourced database\nrequire the data and query results from the providers to be\nveri\fable, that is, tampering of data and query execution\ncan be securely detected. One way to achieve veri\fabil-\nity is using veri\fable computation techniques. Benabbas et\nal. [16] present a delegation scheme on veri\fable database\nminimizing the resources required by the clients of veri\fable\ndatabase. Guo et al. [25] improve the update e\u000eciency using\na long polynomial for public keys and a short polynomial\nfor private keys. Miao et al. [36] enable e\u000ecient keyword\nsearch for VDB using enhanced vector commitment while\nHVDB [65] supports hierarchical veri\fcation by building a\nvector commitment tree. SNARKs [40] can support arbi-\ntrary computation tasks, but requiring an expensive setup\nphase, and incurring signi\fcant overhead. Ben-Sasson et\nal. [15] improve upon SNARKs by bounding the complexity\nof the setup phase to size of the database and the query com-\nplexity. More recently, Zhang et al. [63] propose a system\ncalled vSQL that uses an interactive protocol to support ver-\ni\fable SQL queries. However, vSQL is limited to relational\ndatabases with a \fxed schema.\nAnother way to achieve veri\fability is by using authenti-\ncation data structures such as Merkle trees. Li et al. [29]\npropose and evaluate authentication index structures com-\nbining Merkle trees and B+-trees. Yang et al. [57] propose\nintegrity-protected MR-tree for spatial data. ServeDB [54]\nproposes a Merkle tree index based on hierarchical cube en-\ncoding that supports e\u000ecient multi-dimensional queries. Se-\ncurity conscious applications enforce data integrity against\nmalicious modi\fcations not only from external attackers,\nbut also from malicious insiders and cloud hosting opera-\ntors. As a solution, SUNDR [30] cryptographically protects\nall \fle system contents and proposes a fork consistency pro-\ntocol to detect data tampering.\nMore recent systems, namely VeritasDB [49] and Con-\ncerto [14], leverage trusted hardware to speed up veri\fca-\ntion. In particular, both store Merkle tree data inside SGX\nenclaves. Veritas stores the roots of the trees, and Concertouses memory veri\fcation technique to avoid contention in-\nside the enclaves.\n2.2 Out-of Blockchain Database\nBlockchains, which was originally designed for cryptocur-\nrencies, is now being used as a general-purpose transac-\ntional system. Being a distributed data processing system, a\nblockchain system shares some similarities with a distributed\ndatabase system. However, its focus is security, whereas the\ndatabase's focus is performance. The design space of both\nsystems can be viewed along four dimensions: replication,\nconcurrency, storage and sharding. A recent work [45] pro-\nvides an extensive and in-depth comparison of blockchain\nversus database. It shows that along the four design dimen-\nsions, di\u000berent choices lead to di\u000berent performance. We are\nseeing a trend of merging these two systems into a design\nthat is secure, e\u000ecient, and can be readily adopted by ap-\nplications such as logistic, digital banking, and digital asset\nmanagement.\nOne step toward realizing a hybrid blockchain-database\nis to support rich data queries on blockchains [22, 35, 43,\n10]. A simple approach is to join the network as a full node\nand then execute the query. However, running a full node\nis expensive. vChain [56] addresses this problem by em-\nbedding an aggregate and constant-size authentication data\nstructure, constructed with multiset accumulator, in each\nblock header. This allows users to run a light node to query\nwith integrity guarantee. TrustDBle [23] proposes a secure\nand scalable OLTP engine that provides veri\fable ACID-\ncompliant transactions on shared data using trusted hard-\nware.\nBlockchainDB [22], Veritas [24], FalconDB [42], and Lin-\neageChain [44, 46] are recent systems that use blockchain as\na veri\fable storage and add database features on top of it.\nWe now discuss these systems in more detail.\nBlockchainDB adopts a simple key-value data model, and\nexposes Put/Get/Verify operations to clients. It consists\nof a database layer and a storage layer. The former con-\ntrols the consistency level of requests so that clients can\nchoose the balance of result staleness and performance. The\nstorage layer serves as the uni\fed interface to the under-\nlying blockchains. It translates requests from the database\nlayer into blockchain transactions and monitors the trans-\naction status. When a client invokes Verify , a blockchain\nnode would contact other peers to check whether the corre-\nsponding transaction is committed in the ledger. A node in\nBlockchainDB does not hold the complete copy of the state.\nInstead, the states are partitioned to multiple blockchains.\nVeritas shares a similar goal and vision with BlockchainDB,\nbut di\u000bers in three aspects. First, it targets complex data\nmodels, i.e., relational model. Second, it employs Trusted\nExecution Environments (TEEs) such as Intel SGX as the\ntrustable veri\fers that consume the database logs for the\ntransaction validation. The validation results, in the form\nof veri\fers' votes, are persisted in the blockchain. As a re-\nsult, when there is a dispute, any party can resolve it by\nreconciling the database log with the votes on the ledger.\nThird, Veritas does not support partitioning, as it does not\nstore all states on the blockchain.\nInstead of checking the ledger for log validation, FalconDB\norganizes database records into an authenticated data struc-\nture, such as a Merkle tree, which enables a succinct in-\ntegrity proof on a database record. FalconDB employs an\n\nincentive model allowing clients to selectively challenge the\ntransaction results from a suspicious server. If the server\ncannot provide proof of correctness, it will be penalized.\nFalconDB also supports authenticated queries on a tempo-\nral data model, so that users may access data snapshot with\nrespect to a particular block.\nLineageChain [44, 46] is a \fne-grained, secure, and e\u000e-\ncient provenance system built on top of ForkBase [51, 32]\nand FabricSharp[3]. It provides provenance information to\nsmart contracts through simple interfaces to enable a new\nclass of blockchain applications whose execution logics de-\npend on provenance information at runtime. LineageChain\ncaptures provenance during contract execution and stores it\nin a Merkle tree implemented in ForkBase, and provides a\nnovel skip list index to support e\u000ecient provenance queries.\n2.3 Ledger Database\nAmazon o\u000bers Quantum Ledger Database (QLDB) [11],\na cloud service that provides data immutability and veri\f-\nability. QLDB consists of blocks organized in a hash chain\ncalled journal. Changes to the data, including insert, update\nand delete, are collected into blocks and appended to the\njournal. A Merkle tree is built upon the entire journal. To\nsupport e\u000ecient query, the journal is materialized to user-\nde\fned tables for the latest data and history data. QLDB\naims to provide the high performance of database systems\nwith integrity guarantees for data and historical data ver-\nsions. Similarly, Oracle Blockchain Table [13] o\u000bers append-\nonly veri\fable tables by implementing a centralized ledger\nmodel. MongoDB [12] supports veri\fable change history by\nstoring document collections in a hash chain.\nDatomic [2] is a distributed immutable database system\ndesigned to be ACID compliant, with datom as its database\nbuilding block. It is a form of key-value store, and Datoms\nare collected to form an entity. It makes use of key-value\nstores such as Amazon DynamoDB for managing the data,\nand allows users to obtain a historic snapshot of the database\nvia its APIs and query language. Immudb [6] is a recently\nreleased immutable tamper-evident open source database\nsystem. Due to the demand for VDB, we foresee active\ndevelopment in such kind of database systems. However,\nwith no deletion, the database size will grow over time, and\nquery processing e\u000eciency and scalability could become ma-\njor concerns.\n3. CHALLENGES AND OPPORTUNITIES\nIn this section, we discuss the research challenges in im-\nplementing an e\u000ecient VDB. In particular, the requirements\nof immutability and veri\fability have implications on stor-\nage and indexing, query veri\fcation, and concurrency con-\ntrol mechanisms. We discuss the methodologies from recent\nworks that provide building blocks for VDB.\n3.1 Storage and Indexing\nVDB requires an immutable and tamper-evident storage\nengine. In particular, the storage must support integrity\nproof generation, and have an e\u000ecient version management\nmechanism.\nForkBase [51], a storage with Git-like version control and\nbranch management, and Merkle-based directed acyclic graph\n(DAG) data structure, provides a good starting point. Fork-\nBase supports collaborative analytics, and content-based datadeduplication mechanism that signi\fcantly reduces data vol-\nume in the physical storage. Furthermore, it supports e\u000e-\ncient version querying.\nAs in traditional databases, indexes are necessary for fast\nretrieval and location of records. Recent Merkle tree-based\nindexes, namely Merkle Patricia Trie (MPT) [53], Merkle\nBucket Tree (MBT) [5], and Pattern-Oriented-Split Tree\n(POS-Tree) [51], support e\u000ecient queries on immutable data.\n[59] contains a comprehensive analysis of these indices, show-\ning that MPT, MBT, and POS-Tree are di\u000berent instances\nof Structurally Invariant and Reusable Indexes (SIRI) [51],\nand that POS-tree has better overall performance. In ad-\ndition to these indices which are designed for query veri\f-\nability, other indices are needed to further speed-up data\nretrieval. Since versions can be modeled as temporal or his-\ntorical data, indexes such as the historical R-tree[37], and\nrolling index Bx-tree [26] could be adapted to support the\nmulti-dimensional and single-dimensional queries. We envi-\nsion that the need for fast querying of historical data will\nlead to new, innovative indexes.\n3.2 Veriﬁcation\nQuery veri\fability in VDB means that the user who sends\nthe query can verify the integrity of the result, that is the\ndata and execution have not been tampered with. We dis-\ncuss here di\u000berent approaches to achieve veri\fability.\nClient-side veri\fcation vs Server-side veri\fcation.\nWhen the data is outsourced to a third party, the users\nthemselves must verify some proofs provided by the third\nparty. However, veri\fcation can be expensive for the users,\nespecially when running on low-power devices. Trusted hard-\nware, such as Intel SGX, can help mitigate this cost for user,\nby supporting server-side veri\fcation. In particular, the\nhardware performs veri\fcation securely at the servers, by\nrunning veri\fcation inside trusted execution environments,\nand output only succinct proofs that can be veri\fed cheaply\nby the user. However, the secure hardware has limited re-\nsources that can lead to signi\fcant performance overhead.\nFurthermore, existing secure hardware are vulnerable to side-\nchannel attacks that compromise their security.\nOnline veri\fcation vs Deferred veri\fcation. With\nrespect to the timing of the veri\fcation, there are two ap-\nproaches: online, and deferred veri\fcation. In the former,\nthe data must be committed after the veri\fcation succeeds,\nwhich is useful when recovery from malicious tampering is\ncostly. In the latter, veri\fcation is done over a batch of\ntransactions, therefore achieving higher throughput than the\nformer.\nVeri\fcation via encryption. One way to protect data\nintegrity is by using authenticated encryption. Users can\nencrypt data using private key and store the ciphertexts\non untrusted storage. Data tampering can be detected di-\nrectly with the authentication tag. The limitation of this\napproach is that it restricts computation (or queries) on the\nciphertexts. Encryption schemes with various support for\ncomputation on ciphertexts exist, but they have trade-o\u000b in\nsecurity and computation. All of these schemes have signif-\nicant performance overhead.\nVeri\fcation via authentication data structure. Au-\nthentication data structures, which are based on Merkle\ntrees, provide data integrity with low cost. In this struc-\nture, the leaf nodes contain cryptographic hashes of the data\nblocks, while the non-leaf nodes contain the hashes of their\n\nOLTP Service\nOLAP ServiceLedgerRead/Write\nETL\nRead/WriteProof\nProofTransaction\nTransactionResult + Proof\nResult + ProofFigure 3: Non-intrusive design.\nchild nodes. The hash of the root node is called the \\digest\"\nof the data. The integrity proof consists of the hashes of the\nnodes from the corresponding leaf to the root of the tree.\nThe new digest is recalculated recursively and equality is\nchecked with the previously saved digest.\n3.3 Concurrency Control\nMany outsourced or cloud databases are multi-tenant.\nApplications running on top of a multi-tenant database may\nrequire di\u000berent ACID isolation levels. The database of-\nten has \fxed the transaction isolation levels at the time of\ndeployment, therefore applications have to implement their\nown levels for their needs, which increases the complexity of\nthe system. This problem can be mitigated by using per-\ntenant database architecture, but this approach does not\nscale well.\nConsider as an example an e-commerce system with cus-\ntomer credits. On the one hand, the purchases of the items\nmust occur in sequence to prevent double spending or ship-\nping out-of-stock items. In other words, the transaction\nschedule needs to be serializable, which can be implemented\nusing optimistic concurrency control (OCC) or multi-version\nconcurrency control (MVCC) with abortion on read-write\ncon\ricts. On the other hand, the analysis report or sta-\ntus checking on the system may not require strict isolation.\nSuch queries are mostly processed as read-only workloads,\nand many of them require near real-time responses. For\nexample, read committed isolation will be su\u000ecient to ex-\necute query \\getting all items with stock-level lower than\n50\". In this case, it is unnecessary to abort the query when\nread-write con\ricts occur.\nA common approach to achieving high performance for\nweak isolation is to \fx the isolation to a weak level (e.g.,\nread committed), and implement customized logic to han-\ndle stricter level in the applications. Such design involves\nlocks, checking pre-images of data and sometimes reversions,\ntherefore complicating the application logic and incurring\nlarge overhead. By providing \rexible isolation levels in the\nRead/Write + ProofOLTP Service\nOLAP ServiceETL\nRead/Write + ProofLedger\nLedgerTransaction\nTransactio nResult + Proof\nResult + ProofFigure 4: Intrusive design.\nunderlying database, it allows for performance optimization\nand lets users focus more on the application logic.\n4. EXTENDING OLTP/OLAP TO VDB\nVDB can be implemented by adding a veri\fable ledger\nto an existing database system. The ledger supports im-\nmutable data and veri\fable queries. Here we discuss the\nchallenges of integrating such a ledger to OLTP and OLAP\nsystems.\nThere are two designs for integration, as shown in Fig-\nure 3 and Figure 4. The blue arrows, rectangles and cylin-\nders depict a typical data processing \row, where the data is\ncollected by OLTP and analyzed by OLAP systems.\nNon-intrusive design. As shown in Figure 3, a ledger is\nattached without modifying the architecture of the original\ndatabase systems. However, additional steps are added dur-\ning transaction processing. The OLTP and OLAP systems\ngenerate integrity proofs from an independent ledger. On\nthe one hand, this design minimizes disruption to existing\nsystems, as it does not require changes to existing data. On\nthe other hand, it incurs considerable performance overhead,\ndue to the interaction with the ledger.\nIntrusive design. Another design, as depicted in Fig-\nure 4, is to embed the ledger into an existing database sys-\ntem. This eliminates communication with an outside ledger,\nby generating the integrity proof inside the database. While\nreducing performance overhead compared to the other de-\nsign, it incurs signi\fcant cost in data migration. In particu-\nlar, data must be moved to the new system, which may be\ntoo costly for users with large amounts of data.\nAnother approach is to integrate the ledger with a hy-\nbrid transactional/analytical processing (HTAP) system. A\nHTAP system is designed to unify e\u000ecient processing of op-\nerational and analytical workloads in the same database. In\nthe HTAP system, no data migration from OLTP system to\nOLAP system is necessary. Existing OLTP systems are be-\ning converted to HTAP by exploiting in-memory processing\n[61] and both columnar and row storage structures. Some\n\nA\nAB AC\nACIJ ABCADIB\nBA\nForkBaseCell Store\n LedgerStorage\nLayerControl\nLayer\nIndexes\nRequest Handler\nAuditorRead Result + Proof\nGet Proof Proof\nTM\nProcessor…\nRequest Handler\nAuditorWrit e Result + Proof\nUpdate Ledger Proof\nTM\nProcessorFigure 5: System architecture.\nrecent NewSQL systems also adopt HTAP in their design\nand implementation.\n5. SYSTEM ARCHITECTURE\nIn this section, we discuss the system architecture of Spitz,\na distributed database designed from scratch that supports\nboth OLTP and OLAP workloads with veri\fable ledgers.\nAs shown in Figure 5, the system consists of two layers: the\ncontrol layer , and the storage layer .\nThe control layer consists of multiple processor nodes that\naccept and process requests from a global message queue1.\nEach node has three main components: a request handler,\nan auditor, and a transaction manager (TM). The request\nhandler accepts query requests and returns the results with\nthe corresponding proofs. The auditor communicates with\nthe ledger in the storage layer to keep track of data changes.\nThe transaction manager controls the execution of the queries\nin the storage.\nThe storage layer features a distributed storage engine,\nnamely ForkBase. Built on top of ForkBase is a virtual\ncell store, as opposed to row or column store in traditional\ndatabases. The system maps each cell to a universal key\nconsisting of the column id, primary key, timestamp, and the\nhash of its value. There are multiple index structures built\ninto the storage layer to support veri\fable query processing.\n1Similar to other distributed systems, the coordination, as\nwell as the resource management, is done by a master node.Ledger. This structure consists of a sequence of hashed\nblocks. Each block tracks the modi\fcation of the records,\nquery statements, metadata and the root node of the in-\ndexes on the entire dataset. The block and the data can be\nveri\fed using the Merkle tree structure built on top of the\nentire ledger. Section 5.3 discussed more details regarding\nveri\fcation and proof generation.\nIndex. Spitz uses a B+-tree for query processing. The\ninput of the index is the requested keys, and the output is\nthe matched data cell. This structure is e\u000ecient for both\npoint and range queries.\nInverted Index. When processing analytical queries,\nthe system uses an inverted index to quickly locate the rows\nto fetch data. Such an index uses the value recorded in each\ncell as index key and the universal key of the correspond-\ning cell as value. The structure of the inverted list varies\naccording to the type of the data stored in the cell. For\ninstance, for numeric type, the system uses a skip list to\nbetter support range query, whereas for string type, it uses\na radix tree to reduce space consumption.\n5.1 Query Processing\nThe processor nodes handle both read, write, and mixed\nworkload. Spitz supports both SQL and a self-de\fned JSON\nschema.\nWrite workload. There are four steps in handling a\nwrite workload. (1) The request handler collects a transac-\ntion from the message queue. (2) The auditor checks the\nwrite operations and updates the ledger. The ledger records\n\nthe changes and returns a proof to the auditor. (3) The pro-\ncessor traverses the B+-tree index and performs the write\noperations to the cell store. (4) The processor collects the\nresults, combines them with the proof, and sends back to\nthe user through the request handler.\nRead workload. The processing of read workload fol-\nlows similar steps. (1) The request handler receives a trans-\naction from the message queue. (2) The processor collects\nthe results by traversing corresponding inverted indexes and\nretrieving the cell store. (3) The processor visits the ledger\nvia the auditor, getting the proofs of the results. The proof\ngeneration is done by the ledger using the universal keys\nand the internal nodes of inverted index. (4) The processor\ncombines the results and the proofs as responses and the\nrequest handler returns them to the user.\nSpitz uses a HTAP design to overcome the data movement\nbetween OLTP and OLAP systems. Similar to the intrusive\ndesign in Figure 4, it requires users to replace the underly-\ning database systems, which might be highly tangled with\ntheir business. However, it should be highlighted that Spitz\ncan be used as an individual ledger by solely waking up the\nauditor in the processor. Thus, the system can be applied\ninto a non-intrusive design shown in Figure 3 as a short-\nterm transition plan of integrating Spitz into the real-world\nbusiness. Ultimately, users should use Spitz as a standalone\nand complete database system to cover and develop their\nbusiness.\n5.2 Concurrency Control\nConcurrency control in each processor node can be imple-\nmented in the same way as in traditional database systems.\nHowever, in our design, cells are multi-versioned. There-\nfore, to achieve serializability guarantee, concurrency control\nmechanisms based on MVCC, including MVCC with 2PL\n[18], MVCC with timestamp ordering (T/O) [17], MVCC\nwith OCC [31], are more suitable.\nSince each processor node processes transactions indepen-\ndently, it is necessary to keep the data in the indexes and\nthe virtual storage consistent across di\u000berent nodes. The so-\nlution is to add distributed transactions to each node, and\nfollow the two-phase commit (2PC) protocol to coordinate\neach transaction so that transactions committed by di\u000berent\nnodes can be made serializable. The challenge in achieving\nserializability in distributed setting is to \fgure out the order\nof transactions in the equivalently serial schedule.\nOne approach to achieving serializability is to rely on a\nglobal timestamp service, like Timestamp Oracle [41], to\nallocate the timestamps upon a transaction starts and com-\nmits. We then order transactions based on their start times-\ntamps. In the prepare phase of 2PC, each transaction with\nread/write and write/write con\rict with this order will abort.\nHowever, there are two limitations. First, the timestamp al-\nlocation service can become the bottleneck. Second, the\nabort rate can be high in a write-intensive workload. To\naddress the \frst limitation, we can adopt the hybrid logic\ntimestamp scheme that allocates timestamps by each indi-\nvidual node and still has serializability guarantee [28, 50].\nFor the second limitation, it is possible to adopt the com-\nbination of OCC and MVCC by dynamically adjusting the\ntransaction order to reduce abort rates [19, 34], and ver-\nifying the transactions in batch to reduce the veri\fcation\ncost [20]. These approaches need further investigation and\nevaluation.5.3 Proof and Veriﬁcation\nSpitz o\u000bers timely detection of malicious data tampering\nby using an authentication data structure, namely the ledger\nshown in Figure 5. Clients can use the digest of the ledger\nto perform veri\fcation locally. Since changes to ledger are\nserializable, during the transaction processing, only the data\ncommitted before the transaction can be veri\fed. After the\nprocessing, clients can get the data and the proof of this\ntransaction as described in Section 5.1, along with other\nmetadata of the authentication tree structure if applicable.\nTo verify the correctness of the results, clients can recal-\nculate the digest with the received proof and compare it\nwith the previous digest saved locally. If they match, it\nmeans the data has not been modi\fed during the period be-\ntween the veri\fcation and when the digest is generated. To\nimprove veri\fcation throughput, we use a deferred scheme,\nwhich means the transactions are veri\fed asynchronously in\nbatch.\n6. EXPERIMENTAL STUDY\nIn this section, we describe the prototype of Spitz and\npresent its preliminary evaluation results. The full-scale im-\nplementation of Spitz is in progress and a thorough perfor-\nmance study will be conducted in the future.\n6.1 Implementation\nFirst of all, we implement a baseline system to emulate a\ncommercial product based on the features described online\nand testing provided by the website. The newly inserted or\nmodi\fed records are collected into blocks and appended to a\nledger implemented by a Merkle tree. The ledger is used for\nveri\fcation purposes, shadowing the nodes of a typical B+-\ntree for query key searching. Furthermore, the appended\nblocks are materialized to indexed views for fast query pro-\ncessing. To perform a read query, users can directly fetch\nthe data with meta information using the indexed views,\nwhich can be veri\fed against the ledger.\nFor the prototype of Spitz, we modify the latest version of\nForkBase and forgo irrelevant functionalities such as branch\nmanagement. In particular, we implement the ledger by\nadopting index from Structurally Identical and Reusable In-\ndexes (SIRI) family for both query and veri\fcation. Each\nblock in the ledger stores a historical index instance, natu-\nrally composing a version of the ledger, and the nodes be-\ntween instances can be shared, bene\fting from SIRI prop-\nerties.\nFor comparison purpose, we also build an immutable key-\nvalue store (KVS) using ForkBase. It is the same as Spitz in\nterms of indexing, except that it does not maintain a ledger\nor provide veri\fability. Therefore, by comparing the two\nsystems, we can focus on the maintenance and veri\fcation\ncost of the ledger storage implemented in Spitz.\n6.2 Evaluation\nWe evaluate the performance of the systems with read-\nonly and write-only workloads. The number of records,\nwhich consist of di\u000berent key-value pairs, vary from 10,000\nto 1,280,000. The length of the key ranges from 5 to 12\nbytes while the size of the value is 20 bytes. The experi-\nments are conducted on a server with Ubuntu 14.04, which\nis equipped with 6 cores Intel Xeon Processor E5-1650 pro-\ncessor (3.5GHz) and 32GB RAM.\n\n 0 50 100 150 200 250 300 350\n1 2 4 8 16 32 64 128Throughput (x103 Ops/s)\n#Records (x104)Immutable KVS\nSpitz\nSpitz-verifyBaseline\nBaseline-verify(a) Read\n 0 10 20 30 40 50 60 70 80 90\n1 2 4 8 16 32 64 128Throughput (x103 Ops/s)\n#Records (x104)Immutable KVS\nSpitz\nSpitz-verifyBaseline\nBaseline-verify (b) Write\nFigure 6: Basic operations in single-thread setup.\n6.2.1 Basic Operations\nWe \frst evaluate the performance of read-only and write-\nonly workloads in a single-thread setup. We vary the initial\ndatabase size from 10,000 to 1,280,000 records, and execute\nread-only and write-only workloads on di\u000berent systems.\nFigure 6(a) shows the results for read-only workloads.\nThe immutable KVS performs the best without maintaining\nany veri\fable data structures. The baseline implementation\nand Spitz have comparable performance when the number\nof records becomes large, as the index traversal becomes a\ndominant factor in query processing. When the veri\fcation\non the integrity of the queried results is enabled, plotted\nas Spitz-verify and Baseline-verify in the \fgure, the read\nperformance for Spitz is approximately half of that with-\nout the veri\fcation while the baseline operations per second\ndrops by almost two orders of magnitude. If compared di-\nrectly, Spitz achieves 7x operations per second than that\nof the baseline. The major reason of such phenomenon is\nthat Spitz can store the proofs of the results and the value\nof the target nodes in a uni\fed index, namely the ledger\nimplemented via SIRI. To compare, the baseline needs to\nvisit the B+-index \frst, and uses the resultant nodes to get\nthe proof from the ledger. Figure 6(b) shows the results for\nwrite-only workloads. Similarly, thanks to the uni\fed index\nstructure, Spitz has operations per second comparable to\nthe immutable KVS with and without veri\fcation while the\nperformance of the baseline system is much worse because\nof maintaining multiple indexed views.\n6.2.2 Range Query\nIn this section, we evaluate the performance of analyti-\ncal workloads with range queries. Such workloads are com-\nmonly submitted by data scientists to retrieve a group of\nrecords for analysis or further aggregation. We initialize the\ndatabase with 10,000 to 1,280,000 records for di\u000berent runs.\nThe selection conditions of the range query are set on the\nprimary key and the selectivity of the query is \fxed at 0.1%.\nFigure 7 depicts the operations per second in all systems.\nAs can be seen, the performance of the range query is worse\nthan the performance of point query shown in Figure 6(a)\nby 25% to 90% for all cases. This is due to the additional\nnodes needed to be traversed and scanned when the query\nis processed. Meanwhile, the operations per second drops\nfast when the total number of records increases because the\n 0 50 100 150 200\n1 2 4 8 16 32 64 128Throughput (x103 Ops/s)\n#Records (x104)Immutable KVS\nSpitz\nSpitz-verifyBaseline\nBaseline-verifyFigure 7: Range query performance.\nsystems fetch increasing number of records with \fxed selec-\ntivity for all cases.\nThe baseline stores the ledger and the index of the data\nseparately, and hence, the retrieval of the proofs cannot ben-\ne\ft from the optimizations used in range query processing.\nThat is, the retrieval on the proofs of resultant records, in-\nstead of being fetched in a batch by scanning keys with the\ngiven interval, must be processed by searching the digest in\nthe ledger individually. In contrast, for Spitz, thanks to the\nuse of the uni\fed index structure described in Section 6.2.1,\nproof retrieval can leverage the traversal on the index of the\ndata { the proofs of the resultant records are returned si-\nmultaneously when the resultant records are scanned and\nselected. Consequently, for queries with veri\fcation of data\nintegrity enabled, Spitz outperforms the baseline by up to\ntwo orders of magnitude, a gap much larger than the results\nshown in Figure 6(a).\n6.2.3 Non-intrusive Design vs Spitz\nIn this section, we evaluate the performance of a non-\nintrusive design of VDB and compare it with Spitz. We\nset up an immutable key-value store using ForkBase as the\nunderlying system, which interacts with the ledger shown\n\n 0 50 100 150 200 250\n1 2 4 8 16 32 64 128Throughput (x103 Ops/s)\n#Records (x104)Spitz\nSpitz-verifyNon-intrusive\nNon-intrusive-verify(a) Read\n 0 10 20 30 40 50 60 70 80 90\n1 2 4 8 16 32 64 128Throughput (x103 Ops/s)\n#Records (x104)Spitz\nSpitz-verifyNon-intrusive\nNon-intrusive-verify (b) Write\nFigure 8: Non-intrusive design vs. Spitz.\nin Figure 3.2To support data veri\fcation, we deploy Spitz\non the same server as the Ledger database in the \fgure. In\nthe case of read workloads, the client obtains the queried\nresults from the underlying database and the proofs from\nthe ledger as responses, while in the case of write workloads,\nthe submitted data are committed in both the underlying\nand ledger database atomically. To verify the results, the\nclient uses the proof from the Ledger database, calculates\nthe digest of the returned results, and compares it with the\nprevious digests as described in Section 5.3.\nThe experiment is conducted with read-only and write-\nonly workloads and the results are shown in Figure 8. As can\nbe seen, the non-intrusive design incurs signi\fcant overhead\nby maintaining two systems, i.e., the underlying database\nand the Ledger database. Speci\fcally, for read-only work-\nloads, the performance of Spitz is 6x higher than the non-\nintrusive design when the veri\fcation of data integrity is\nenabled. The huge performance gain comes from a sim-\npler process \row: the request can be processed within a\nsingle system in Spitz, while in the non-intrusive design,\nit must be sent to the underlying database \frst to obtain\nthe results, and passed to the Ledger database to retrieve\nthe proofs. Obviously, the interactions between the Ledger\ndatabase and the underlying database inevitably introduce\nadditional cost on network communication, query planning,\netc. For write workloads, Spitz produces 3x higher number\nof operations per second than the non-intrusive design.\nIn summary, the results show that the prototype of Spitz\nachieves better performance than the non-intrusive design.\nWithout doubt, its performance could further be improved\nwith indexes and optimization strategies speci\fcally designed\nfor VDB.\n7. AI AND VDB - A SYNERGY\nWe have discussed the design and implementation of a\nVDB that can support new database applications. This new\ntrend in database coincides with the rapid development of\narti\fcial intelligence (AI). AI has been proven successful in\na wide range of applications, could automate many tasks,\n2ForkBase can be treated as a HTAP system here therefore\nwe do not initialize separate OLAP and OLTP system as\nshown in the \fgure.and it is often better at modeling complex situations than\nhumans.\nTo meet the demand for complex analytics, database sys-\ntems have enabled the addition of machine (or deep) learning\nlibraries to construct end-to-end analytics pipelines. With\nthe evolution of dataset due to versioning, machine learning\nmodels used in the analytics pipeline may exhibit concept\ndrift behavior, which causes the model to become less accu-\nrate over time. Therefore, iterative analytics component up-\ndates may become necessary, and the relationship between\ncomponent and data versions need to be maintained for ver-\ni\fcation on the analytical results.\nFrom system performance perspective, deep learning can\nbe used to enhance database performance and usability, and\nvice versa, deep learning can bene\ft from the e\u000ecient data\nmanagement and performance provided by databases (Apache\nSINGA [38] for example). Earlier works [52] have discussed\nthe symbiotic relationship between databases and machine\nlearning. In the following, we shall continue this discussion\nby making a case for the merging of VDB and AI.\n7.1 AI for VDB\nAI can make VDB more intelligent. Currently, VDB de-\nsign is based on empirical methodologies, which might not\nhave a good performance. As pointed out in [52], AI may\nhelp to improve VDBs performance in several aspects.\n\u000fLearning-based data structure. A number of solutions\ninvestigate how to enhance existing indexes or design\nnew indexes for better storage and query e\u000eciency,\ne.g., learned B+tree [27], secondary index [55].\n\u000fLearning-based transaction management. AI could be\nused to predict the future transaction workload [33]\nand schedule the transactions such that the through-\nput is maximized with an acceptable abort rate [48].\n\u000fLearning-based performance tuning. To avoid man-\nual tuning of the memory allocation or I/O control,\nrecent works [62] apply reinforcement learning to au-\ntomatically tune database con\fgurations according to\nworkload changes.\n\u000fLearning-based query optimization. To optimize the\nqueries, existing works [39, 58] deploy deep neural net-\nworks such as convolutional neural networks (CNNs),\n\nFigure 9: Veri\fable federated analytical query pro-\ncessing.\nrecurrent neural networks (RNNs) and their variants\nto estimate the cardinality and cost.\nThough many learning-based techniques for general DB\nhave been proposed, more e\u000bort is needed to adapt them\nfor VDB, since VDB has di\u000berent data structure, storage\nand transaction management requirements.\n7.2 VDB for AI\nVDB can make AI (-based analytics) more reliable. One\nkey feature of VDB is that it maintains historical data.\nHence, a natural question might be: can VDB support an-\nalytical queries? For example, a client (e.g., a hospital) has\nalready outsourced its database to a cloud hosting company\nfor processing online transactions (e.g., medical records).\nIt may then also want the VDB to process some analyti-\ncal queries (even some machine learning model) for speci\fc\nevaluations, such that it does not need to download the data\nand execute locally. However, the analytical query support\nin VDB is limited (e.g. [7]). More importantly, the analytics\nresult should be veri\fable, ensuring that it is computed from\ncorrect data; otherwise, it may result in a wrong decision,\nand lead to huge loss (e.g., could be peoples health or even\nlife in medical domain).\nThere are several works [64, 63] that support verifying\narbitrary SQL queries over outsourced databases, but with\nlow performance. Recent works [56, 60] on veri\fable speci\fc\nqueries over blockchain (can be viewed as a special kind of\nVDB) are relatively e\u000ecient, but they only support range\nqueries. It is necessary to investigate how to e\u000eciently com-\npute complex analytical queries on VDB.\nFinally, it is possible to consolidate multiple clients VDB\nto provide federated analytics (as illustrated in Figure 9).\nFor example, a few hospitals want to have a more precise\nand comprehensive analysis of a disease. The integrity of\nthe data and queries are important in these use cases. At\nthe same time, each client should not be able to break the\ncon\fdentiality of the other clients data.\nIn summary, AI and VDB could bene\ft from each other:\nAI could improve the performance of VDB, and VDB couldensure the trustworthiness of the analytical results (or AI\nmodels).\n8. CONCLUSIONS\nWith recent digital optimization and transformation, more\nand more businesses are transacting directly with each other.\nThe current pandemic further speeds up the transformation\nand adoption of online business processes. This trend in-\ntroduces a new important requirement to database systems:\nthe integrity of the data, the history, and the execution must\nbe protected. This gives rise to a new class of database\nsystems that support the veri\fcation of the transactional\nintegrity.\nIn this paper, we discuss the requirements and challenges\nof veri\fable databases. We present approaches to extend ex-\nisting systems to support veri\fcation, and an initial design\nand prototype of our ongoing development of a new VDB\nsystem called Spitz. We conduct an experimental study\nand show that Spitz is able to provide a better performance\nthan a baseline system. As future works, we will continue\nto implement the system and present a comprehensive sys-\ntem design and a thorough performance study. We will also\nstudy and possibly introduce new indexes, concurrency con-\ntrol mechanisms and query processing strategies for VDB.\nAcknowledgments\nMeihui Zhang would like to thank the VLDB Endowment\nAwards Selection Committee for the 2020 VLDB Early Ca-\nreer Research Contribution Award, and the nominators for\nthe nomination. After checking with the Chair of the Awards\nSelection Committee on the invited paper requirements, Mei-\nhui decided to report an ongoing system development, which\nis being built upon system components and works developed\nby her and collaborators. For the works that led to the\naward, Meihui would like to thank her mentors, colleagues,\ncollaborators, research assistants and students for their con-\ntributions. Meihui would also like to thank her ex-dean,\nProf. Heyan Huang and current dean Prof. Guoren Wang,\nfor their support and guidance.\nFor this paper, Meihui, Zhongle, Cong and Ziyue would\nlike to thank Anh Dinh, Qian Lin, Wei Lu, Beng Chin Ooi,\nPingcheng Ruan, and Yuncheng Wu for their discussions,\ncontributions and proof reading. The research of Cong Yue\nand Zhongle Xie are supported by Singapore Ministry of Ed-\nucation Academic Research Fund Tier 3 under MOEs o\u000ecial\ngrant number MOE2017-T3-1-007.\n9. REFERENCES\n[1] Couchdb. https://couchdb.apache.org/ .\n[2] Datomic. https://www.datomic.com/ .\n[3] Fabricsharp. https:\n//www.comp.nus.edu.sg/ ~dbsystem/fabricsharp .\n[4] Hbase. https://hbase.apache.org/ .\n[5] Hyperledger. https://www.hyperledger.org .\n[6] immudb. https://github.com/codenotary/immudb .\n[7] Querying your data - amazon quantum ledger\ndatabase (amazon qldb).\nhttps://docs.aws.amazon.com/qldb/latest/\ndeveloperguide/working.userdata.html .\n[8] Rethinkdb. https://rethinkdb.com/ .\n[9] Spark. https://spark.apache.org/ .\n\n[10] Wolk SwarmDB{decentralized database service for\nweb3. https://laptrinhx.com/wolk-swarmdb-\ndecentralized-database-services-for-web3-\n4011398543/ , 2017.\n[11] Amazon quantum ledger database (qldb).\nhttps://aws.amazon.com/qldb/ , 2019.\n[12] Implementing cryptographically veri\fable change\nhistory using mongodb.\nhttps://github.com/mongodb-labs/ledger , 2020.\n[13] Oracle blockchain tables. https:\n//docs.oracle.com/en/database/oracle/oracle-\ndatabase/20/ftnew/oracle-blockchain-table.html ,\n2020.\n[14] A. Arasu, K. Eguro, R. Kaushik, D. Kossmann,\nP. Meng, V. Pandey, and R. Ramamurthy. Concerto:\nA high concurrency key-value store with integrity. In\nProceedings of the International Conference on\nManagement of Data, SIGMOD , pages 251{266.\nACM, 2017.\n[15] E. Ben-Sasson, A. Chiesa, E. Tromer, and M. Virza.\nSuccinct non-interactive zero knowledge for a von\nneumann architecture. In Proceedings of the 23rd\nUSENIX Security Symposium , pages 781{796.\nUSENIX Association, 2014.\n[16] S. Benabbas, R. Gennaro, and Y. Vahlis. Veri\fable\ndelegation of computation over large datasets. In\nAdvances in Cryptology - CRYPTO 2011 , volume\n6841, pages 111{131. Springer, 2011.\n[17] P. A. Bernstein and N. Goodman. Multiversion\nconcurrency control - theory and algorithms. ACM\nTransactions on Database Systems , 8(4):465{483,\n1983.\n[18] P. A. Bernstein, V. Hadzilacos, and N. Goodman.\nConcurrency Control and Recovery in Database\nSystems . Addison-Wesley, 1987.\n[19] C. Boksenbaum, M. Cart, J. Ferri\u0013 e, and J. Pons.\nCerti\fcation by intervals of timestamps in distributed\ndatabase systems. In Tenth International Conference\non Very Large Data Bases,VLDB , pages 377{387.\nMorgan Kaufmann, 1984.\n[20] B. Ding, L. Kot, and J. Gehrke. Improving optimistic\nconcurrency control through transaction batching and\noperation reordering. PVLDB , 12(2):169{182, 2018.\n[21] T. T. A. Dinh, R. Liu, M. Zhang, G. Chen, B. C. Ooi,\nand J. Wang. Untangling blockchain: A data\nprocessing view of blockchain systems. IEEE\nTransactions on Knowledge and Data Engineering,\nTKDE , 30(7):1366{1385, 2018.\n[22] M. El-Hindi, C. Binnig, A. Arasu, D. Kossmann, and\nR. Ramamurthy. Blockchaindb - A shared database on\nblockchains. PVLDB , 12(11):1597{1609, 2019.\n[23] M. El-Hindi, S. Karrer, G. Doci, and C. Binnig.\nTrustDBle: Towards trustable shared databases. In\nThird International Symposium on Foundations and\nApplications of Blockchain , 2020.\n[24] J. Gehrke, L. Allen, P. Antonopoulos, A. Arasu,\nJ. Hammer, J. Hunter, R. Kaushik, D. Kossmann,\nR. Ramamurthy, S. T. V. Setty, J. Szymaszek, A. van\nRenen, J. Lee, and R. Venkatesan. Veritas: Shared\nveri\fable databases and tables in the cloud. In 9th\nBiennial Conference on Innovative Data Systems\nResearch, CIDR , 2019.[25] Z. Guo, H. Li, C. Cao, and Z. Wei. Veri\fable\nalgorithm for outsourced database with updating.\nCluster Computing , 22:5185{5193, 2019.\n[26] C. S. Jensen, D. Lin, and B. C. Ooi. Query and\nupdate e\u000ecient b+-tree based indexing of moving\nobjects. In Proceedings of the Thirtieth International\nConference on Very Large Data Bases, VLDB , pages\n768{779. Morgan Kaufmann, 2004.\n[27] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nProceedings of the International Conference on\nManagement of Data, SIGMOD , pages 489{504.\nACM, 2018.\n[28] S. Kulkarni, M. Demirbas, D. Madeppa,\nA. Bharadwaj, and M. Leone. Logical physical clocks\nand consistent snapshots in globally distributed\ndatabases. In The 18th International Conference on\nPrinciples of Distributed Systems , 2014.\n[29] F. Li, M. Hadjieleftheriou, G. Kollios, and L. Reyzin.\nDynamic authenticated index structures for\noutsourced databases. In Proceedings of the\nInternational Conference on Management of Data,\nSIGMOD , pages 121{132. ACM, 2006.\n[30] J. Li, M. N. Krohn, D. Mazi\u0012 eres, and D. E. Shasha.\nSecure untrusted data repository (SUNDR). In 6th\nSymposium on Operating System Design and\nImplementation (OSDI) , pages 121{136. USENIX\nAssociation, 2004.\n[31] H. Lim, M. Kaminsky, and D. G. Andersen. Cicada:\nDependably fast multi-core in-memory transactions.\nInProceedings of the International Conference on\nManagement of Data, SIGMOD , pages 21{35. ACM,\n2017.\n[32] Q. Lin, K. Yang, T. T. A. Dinh, Q. Cai, G. Chen,\nB. C. Ooi, P. Ruan, S. Wang, Z. Xie, M. Zhang, and\nO. Vandans. Forkbase: Immutable, tamper-evident\nstorage substrate for branchable applications. In 36th\nIEEE International Conference on Data Engineering,\nICDE , pages 1718{1721. IEEE, 2020.\n[33] L. Ma, D. V. Aken, A. Hefny, G. Mezerhane, A. Pavlo,\nand G. J. Gordon. Query-based workload forecasting\nfor self-driving database management systems. In\nProceedings of the International Conference on\nManagement of Data, SIGMOD , pages 631{645.\nACM, 2018.\n[34] H. A. Mahmoud, V. Arora, F. Nawab, D. Agrawal,\nand A. E. Abbadi. Maat: E\u000bective and scalable\ncoordination of distributed transactions in the cloud.\nPVLDB , 7(5):329{340, 2014.\n[35] T. McConaghy, R. Marques, A. M uller, D. D. Jonghe,\nT. McConaghy, G. McMullen, R. Henderson,\nS. Bellemare, and A. Granzotto. Bigchaindb: A\nscalable blockchain database. Whitepaper , 2018.\n[36] M. Miao, J. Wang, S. Wen, and J. Ma. Publicly\nveri\fable database scheme with e\u000ecient keyword\nsearch. Information Sciences , 475:18{28, 2019.\n[37] M. A. Nascimento and J. R. O. Silva. Towards\nhistorical r-trees. In Proceedings of the ACM\nSymposium on Applied Computing, SAC , pages\n235{240. ACM, 1998.\n[38] B. C. Ooi, K. Tan, S. Wang, W. Wang, Q. Cai,\nG. Chen, J. Gao, Z. Luo, A. K. H. Tung, Y. Wang,\n\nZ. Xie, M. Zhang, and K. Zheng. SINGA: A\ndistributed deep learning platform. In Proceedings of\nthe 23rd Annual ACM Conference on Multimedia\nConference,MM , pages 685{688. ACM, 2015.\n[39] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi.\nAn empirical analysis of deep learning for cardinality\nestimation. CoRR , abs/1905.06425, 2019.\n[40] B. Parno, J. Howell, C. Gentry, and M. Raykova.\nPinocchio: Nearly practical veri\fable computation. In\nIEEE Symposium on Security and Privacy, SP , pages\n238{252. IEEE Computer Society, 2013.\n[41] D. Peng and F. Dabek. Large-scale incremental\nprocessing using distributed transactions and\nnoti\fcations. In 9th USENIX Symposium on\nOperating Systems Design and Implementation, OSDI ,\npages 251{264. USENIX Association, 2010.\n[42] Y. Peng, M. Du, F. Li, R. Cheng, and D. Song.\nFalconDB: Blockchain-based collaborative database.\nInProceedings of the International Conference on\nManagement of Data, SIGMOD , pages 637{652.\nACM, 2020.\n[43] B. M. Platz, A. Filipowski, and K. Doubleday.\nFlureedb: a practical decentralized database.\nWhitepaper , 2017.\n[44] P. Ruan, G. Chen, A. Dinh, Q. Lin, B. C. Ooi, and\nM. Zhang. Fine-grained, secure and e\u000ecient data\nprovenance for blockchain. PVLDB , 12(9):975{988,\n2019.\n[45] P. Ruan, G. Chen, T. T. A. Dinh, Q. Lin, D. Loghin,\nB. C. Ooi, and M. Zhang. Blockchains and distributed\ndatabases: a twin study. CoRR , abs/1910.01310, 2019.\n[46] P. Ruan, A. Dinh, Q. Lin, M. Zhang, G. Chen, and\nB. C. Ooi. Revealing every story of data in blockchain\nsystems. ACM SIGMOD Record , special issue for\nSIGMOD Research Highlight Award, 2020.\n[47] S. T. V. Setty, V. Vu, N. Panpalia, B. Braun, A. J.\nBlumberg, and M. Wal\fsh. Taking proof-based veri\fed\ncomputation a few steps closer to practicality. In\nProceedings of the 21th USENIX Security Symposium ,\npages 253{268. USENIX Association, 2012.\n[48] Y. Sheng, A. Tomasic, T. Sheng, and A. Pavlo.\nScheduling OLTP transactions via machine learning.\nCoRR , abs/1903.02990, 2019.\n[49] R. Sinha and M. Christodorescu. Veritasdb: High\nthroughput key-value store with integrity. IACR\nCryptology ePrint Archive , 2018:251, 2018.\n[50] R. Taft, I. Sharif, A. Matei, N. VanBenschoten,\nJ. Lewis, T. Grieger, K. Niemi, A. Woods, A. Birzin,\nR. Poss, P. Bardea, A. Ranade, B. Darnell, B. Gruneir,\nJ. Ja\u000bray, L. Zhang, and P. Mattis. Cockroachdb: The\nresilient geo-distributed SQL database. In Proceedings\nof the International Conference on Management of\nData, SIGMOD , pages 1493{1509. ACM, 2020.\n[51] S. Wang, T. T. A. Dinh, Q. Lin, Z. Xie, M. Zhang,\nQ. Cai, G. Chen, B. C. Ooi, and P. Ruan. Forkbase:\nAn e\u000ecient storage engine for blockchain and forkable\napplications. PVLDB , 11(10):1137{1150, 2018.\n[52] W. Wang, M. Zhang, G. Chen, H. V. Jagadish, B. C.\nOoi, and K. Tan. Database meets deep learning:\nChallenges and opportunities. SIGMOD Record ,45(2):17{22, 2016.\n[53] D. D. Wood. Ethereum: A secure decentralised\ngeneralised transaction ledger. 2014.\n[54] S. Wu, Q. Li, G. Li, D. Yuan, X. Yuan, and C. Wang.\nServeDB: Secure, veri\fable, and e\u000ecient range queries\non outsourced database. In 35th IEEE International\nConference on Data Engineering, ICDE , pages\n626{637. IEEE, 2019.\n[55] Y. Wu, J. Yu, Y. Tian, R. Sidle, and R. Barber.\nDesigning succinct secondary indexing mechanism by\nexploiting column correlations. In Proceedings of the\nInternational Conference on Management of Data,\nSIGMOD , pages 1223{1240. ACM, 2019.\n[56] C. Xu, C. Zhang, and J. Xu. vchain: Enabling\nveri\fable boolean range queries over blockchain\ndatabases. In Proceedings of the International\nConference on Management of Data, SIGMOD , pages\n141{158. ACM, 2019.\n[57] Y. Yang, S. Papadopoulos, D. Papadias, and\nG. Kollios. Authenticated indexing for outsourced\nspatial databases. VLDB J. , 18(3):631{648, 2009.\n[58] Z. Yang, E. Liang, A. Kamsetty, C. Wu, Y. Duan,\nP. Chen, P. Abbeel, J. M. Hellerstein, S. Krishnan,\nand I. Stoica. Deep unsupervised cardinality\nestimation. PVLDB , 13(3):279{292, 2019.\n[59] C. Yue, Z. Xie, M. Zhang, G. Chen, B. C. Ooi,\nS. Wang, and X. Xiao. Analysis of indexing structures\nfor immutable data. In Proceedings of the\nInternational Conference on Management of Data,\nSIGMOD , pages 925{935. ACM, 2020.\n[60] C. Zhang, C. Xu, J. Xu, Y. Tang, and B. Choi.\nGem2-tree: A gas-e\u000ecient structure for authenticated\nrange queries in blockchain. In 35th IEEE\nInternational Conference on Data Engineering, ICDE ,\npages 842{853. IEEE, 2019.\n[61] H. Zhang, G. Chen, B. C. Ooi, K. Tan, and M. Zhang.\nIn-memory big data management and processing: A\nsurvey. IEEE Transactions on Knowledge and Data\nEngineering, TKDE , 27(7):1920{1948, 2015.\n[62] J. Zhang, Y. Liu, K. Zhou, G. Li, Z. Xiao, B. Cheng,\nJ. Xing, Y. Wang, T. Cheng, L. Liu, M. Ran, and\nZ. Li. An end-to-end automatic cloud database tuning\nsystem using deep reinforcement learning. In\nProceedings of the International Conference on\nManagement of Data, SIGMOD , pages 415{432.\nACM, 2019.\n[63] Y. Zhang, D. Genkin, J. Katz, D. Papadopoulos, and\nC. Papamanthou. vSQL: Verifying arbitrary SQL\nqueries over dynamic outsourced databases. In IEEE\nSymposium on Security and Privacy, SP , pages\n863{880. IEEE Computer Society, 2017.\n[64] Y. Zhang, J. Katz, and C. Papamanthou. Integridb:\nVeri\fable SQL for outsourced databases. In\nProceedings of the 22nd ACM SIGSAC Conference on\nComputer and Communications Security , pages\n1480{1491. ACM, 2015.\n[65] Z. Zhang, X. Chen, J. Li, X. Tao, and J. Ma. HVDB:\na hierarchical veri\fable database scheme with scalable\nupdates. Journal of Ambient Intelligence and\nHumanized Computing , 10(8):3045{3057, 2019.",
  "textLength": 62320
}