{
  "paperId": "feb51d6133e8c869ecd2ca4325ded0304eb4bc59",
  "title": "COLE: A Column-based Learned Storage for Blockchain Systems",
  "pdfPath": "feb51d6133e8c869ecd2ca4325ded0304eb4bc59.pdf",
  "text": "COLE: A Column-based Learned Storage for Blockchain Systems (Technical\nReport)\nCe Zhang∗, Cheng Xu∗, Haibo Hu†, Jianliang Xu∗\n∗Hong Kong Baptist University†Hong Kong Polytechnic University\nAbstract\nBlockchain systems suffer from high storage costs as ev-\nery node needs to store and maintain the entire blockchain\ndata. After investigating Ethereum’s storage, we find that\nthe storage cost mostly comes from the index, i.e., Merkle\nPatricia Trie (MPT). To support provenance queries, MPT\npersists the index nodes during the data update, which adds\ntoo much storage overhead. To reduce the storage size, an\ninitial idea is to leverage the emerging learned index tech-\nnique, which has been shown to have a smaller index size\nand more efficient query performance. However, directly ap-\nplying it to the blockchain storage results in even higher\noverhead owing to the requirement of persisting index nodes\nand the learned index’s large node size. To tackle this, we\npropose COLE, a novel column-based learned storage for\nblockchain systems. We follow the column-based database\ndesign to contiguously store each state’s historical values,\nwhich are indexed by learned models to facilitate efficient\ndata retrieval and provenance queries. We develop a series\nof write-optimized strategies to realize COLE in disk envi-\nronments. Extensive experiments are conducted to validate\nthe performance of the proposed COLE system. Compared\nwith MPT, COLE reduces the storage size by up to 94% while\nimproving the system throughput by 1 .4×-5.4×.\n1 Introduction\nBlockchain, as the backbone of cryptocurrencies and decen-\ntralized applications [38,52], is an immutable ledger built on a\nset of transactions agreed upon by untrusted nodes. It employs\ncryptographic hash chains and consensus protocols for data\nintegrity. Users can retrieve historical data from blockchain\nnodes with integrity assurance, also known as provenance\nqueries. However, all nodes are required to store the complete\ntransactions and ledger states, leading to amplified storage ex-\npenses, particularly as the blockchain continues to grow. For\nexample, the Ethereum blockchain requires about 16TB stor-\nage as of December 2023, with an annual growth of around\n4TB [1]. This storage requirement may compel the resource-\nlimited nodes to retain only the data of a few recent blocks,\nwhich restricts the ability to support data provenance. The\nnodes that maintain the complete data may also leave the\nnetwork due to the rapidly increasing storage size, which\npotentially affects system security.\nExtension Node\nBranch Node\nLeaf Nodeaddr valueFigure 1: An Example of Merkle Patricia Trie\nTo tackle the storage issue, we investigate Ethereum’s\nindex, Merkle Patricia Trie (MPT), to identify the storage\nbottleneck. MPT combines Patricia Trie with Merkle Hash\nTree (MHT) [37] to ensure data integrity. During data up-\ndates, its index nodes are persisted to support provenance\nqueries. Figure 1 shows an example of an MPT storing\nthree state addresses across two blocks. Each node is aug-\nmented with a digest from its content and child nodes (e.g.,\nh(n1) =h(a1|h(n2))). The root hash secures data integrity\nthrough the collision-resistance of the cryptographic hash\nfunction and the hierarchical structure. With each new block,\nMPT retains obsolete nodes from the preceding block. For\nexample, in block i+1, updating address a11e67with v′\n3\nintroduces new nodes n′\n1,n′\n2,n′\n4, while old nodes n1,n2,n4\nendure. This setup allows historical data retrieval from any\nblock (e.g., for address a11e67in block i, value v3is retrieved\nby traversing nodes n1,n2, and n4).\nHowever, this approach adds too much storage overhead\ndue to duplicating nodes along the update path (e.g., n1,n2,n4\nandn′\n1,n′\n2,n′\n4in Figure 1). Consequently, most storage over-\nhead comes from the index rather than the underlying data. In\na preliminary experiment with 10 million transactions under\nthe SmallBank workload [17], we observed that the underly-\ning data contributes only 2.8%of the total storage. Thus, a\nmore compact index supporting data integrity and provenance\nqueries is imperative.\nRecently, a novel indexing technique, learned index [15,20,\n26, 54], has emerged and shows notably smaller index size\nand faster query speed. The improved performance comes\nfrom the substitution of the directing keys in index nodes with\na learned model. For instance, consider a key-value database\nwith linear key distribution: (1,v1),(2,v2),···,(n,vn). In a\n1arXiv:2306.10739v3  [cs.DB]  17 Jan 2024\n\ntraditional B+-tree with fanout f, this leads to O(n\nf)nodes\nandO(logfn)levels, resulting in O(n)storage costs and\nO(logfn·log2f)query times. Conversely, using a simple\nlinear model y=xenables accurate data positioning with just\nO(1)storage and O(1)query times. Although this example\nmay not perfectly reflect real-world applications, it highlights\nthat the learned index outperforms traditional indexes signifi-\ncantly when the model effectively learns the data.\nIn view of the advantages of the learned index, one may\nwant to apply it to blockchain storage to improve performance.\nHowever, the current learned indexes do not support both\ndata integrity and provenance queries required by blockchain\nsystems. A naive approach is to combine the learned index\nwith MHT [37] and make the index nodes persistent, as in\nMPT. Nonetheless, this is not feasible due to the larger node\nsize of the learned index. The fanout of such a node is mainly\ndictated by data distribution. In favorable cases, only a few\nmodels are needed to index data, leading to a node fanout\ncomparable to data magnitude. Thus, persisting learned index\nnodes might incur even higher storage overhead than MPT.\nOur evaluation in Section 8 shows that a learned index with\npersistent nodes is 5×to31×larger than MPT. Furthermore,\nas blockchain systems require durable disk-based storage and\noften involve frequent data updates, the learned index should\nbe optimized for both disk and write operations. Therefore, a\nblockchain-friendly learned index needs to be proposed.\nIn this paper, we propose COLE, a novel column-based\nlearned storage for blockchain systems that overcomes the\nlimitations of current learned indexes and supports prove-\nnance queries. The key challenge in adapting learned indexes\nto blockchains is the need for node persistence, which may\nlead to substantial storage overhead. COLE tackles this issue\nwith an innovative column-based design, inspired by column-\nbased databases [4, 36]. In this design, each ledger state is\ntreated as a “column”, with different versions of a state stored\ncontiguously and indexed using learned models within the\nlatest block’s index. This enables efficient data updates as ap-\npend operations with associated version numbers (i.e., state’s\nblock heights). Moreover, historical data queries no longer\ntraverse previous block indexes, but utilize the learned in-\ndex in the most recent block. The column-based design also\nsimplifies model learning and reduces disk IOs.\nTo handle frequent data updates and enhance write effi-\nciency in COLE, we propose adopting the log-structured\nmerge-tree (LSM-tree) [33,41] maintenance approach to man-\nage the learned models. This involves inserting updates into\nan in-memory index before merging them into on-disk levels\nthat grow exponentially. For each on-disk level, we design\na disk-optimized learned model that can be constructed in\nastreaming way, which enables efficient data retrieval with\nminimal IO cost. To guarantee data integrity, we construct\nanm-ary complete MHT for the blockchain data in each on-\ndisk level. The root hashes of the in-memory index and all\nMHTs combine to create a root digest that attests to the en-tire blockchain data. However, recursive merges during write\noperations can lead to long-tail latency in the LSM-tree ap-\nproach. To alleviate this issue, we further develop a novel\ncheckpoint-based asynchronous merge strategy to ensure the\nsynchronization of the storage among blockchain nodes.\nTo summarize, this paper makes the following contribu-\ntions:\n•To the best of our knowledge, COLE is the first column-\nbased learned storage that combines learned models with\nthe column-based design to reduce storage costs for\nblockchain systems.\n•We propose novel write-optimized and disk-optimized de-\nsigns to store blockchain data, learned models, and Merkle\nfiles for realizing COLE.\n•We develop a new checkpoint-based asynchronous merge\nstrategy to address the long-tail latency problem for data\nwrites in COLE.\n•We conduct extensive experiments to evaluate COLE’s\nperformance. The results show that compared with MPT,\nCOLE reduces storage size by up to 94% and improves\nsystem throughput by 1.4×-5.4×. Additionally, the pro-\nposed asynchronous merge decreases long-tail latency by\n1-2 orders of magnitude while maintaining a comparable\nstorage size.\nThe rest of the paper is organized as follows. We present\nsome preliminaries about blockchain storage in Section 2.\nSection 3 gives a system overview of COLE. Section 4 designs\nthe write operation of COLE, followed by an asynchronous\nmerge strategy in Section 5. Section 6 describes the read\noperations of COLE. Section 7 presents a complexity analysis.\nThe experimental evaluation results are shown in Section 8.\nSection 9 discusses the related work. Finally, we conclude\nour paper in Section 10.\n2 Blockchain Storage Basics\nIn this section, we give some necessary preliminaries to intro-\nduce the proposed COLE. Blockchain is a chain of blocks that\nmaintains a set of states and records the transactions that mod-\nify these states. To establish a consistent view of the states\namong mutually untrusted blockchain nodes, a consensus pro-\ntocol is utilized to globally order the transactions [7, 38, 45].\nThe transaction’s execution program is known as smart con-\ntract. A smart contract can store states, each of which is iden-\ntified by a state address addr . In Ethereum [52], both the\nstate address addr and the state value value are fixed-sized\nstrings. Figure 2 shows an example of the block data struc-\nture. The header of a block consists of (i) Hprev_blk, the hash\nof the previous block; (ii) T S, the timestamp; (iii) πcons, the\nconsensus protocol related data; (iv) Htx, the root digest of\nthe transactions in the current block; (v) Hstate, the root digest\nof the states. The block body includes the transactions, states,\nand their corresponding Merkle Hash Tree (MHTs).\nMHT is a prevalent hierarchical structure to ensure data\n2\n\n... ...\n... ...Figure 2: Block Data Structure\nintegrity [37]. In the context of blockchain, MHT is built\nfor the transactions of each block and the ledger states. Fig-\nure 2 shows an example of an MHT of a block’s transac-\ntions. The leaf nodes are the hash values of the transactions\n(e.g., h1=h(tx1)). The internal nodes are the hash values\nof their child nodes (e.g., h5=h(h1||h2)). MHT enables the\nproof of existence for a given transaction. For example, to\nprove tx3, the sibling hashes along the search path (i.e., h4\nandh5, shaded in Figure 2) are returned as the proof. One\ncan verify tx3by reconstructing the root hash using the proof\n(i.e., h(h5||h(h(tx3)||h4))) and comparing it with the one in\nthe block header (i.e., Htx). Apart from being used in the\nblockchain, MHT has also been extended to database indexes\nto support result integrity verification for different queries.\nFor example, MHT has been extended to Merkle B+-tree\n(MB-tree) by combining the Merkle structure with B+-tree,\nto support trustworthy queries in relational databases [29].\nThe blockchain storage uses an index to efficiently main-\ntain and access the states [50, 52]. Besides the write and\nread operations that a normal index supports, the index of the\nblockchain storage should also fulfill the two requirements\nwe mentioned before: (i) ensuring the integrity of the indexed\nblockchain states, (ii) supporting provenance queries that en-\nable blockchain users to retrieve historical state values with\nintegrity assurance. With these requirements, the index of the\nblockchain storage should support the following functions:\n•Put(addr,value ): insert the state with the address addr\nand the value value to the current block;\n•Get(addr): return the latest value of the state at address\naddr if it exists, or returns nilotherwise;\n•ProvQuery (addr,[blkl,blku]): return the provenance\nquery results {value}and a proof π, given the address\naddr and the block height range [blkl,blku];\n•VerifyProv (addr,[blkl,blku],{value},π,Hstate): verify\nthe provenance query results {value}w.r.t. the address,\nthe block height range, the proof, and Hstate, where Hstate\nis the root digest of the states.\nEthereum employs Merkle Patricia Trie (MPT) to index\nblockchain states. In Section 1, we have shown how MPT\nimplements Put(·)andProvQuery (·)using Figure 1 and the\naddress a11e67. We now explain the other two functions\nusing the same example. Get(a11e67)finds a11e67’s latest\nvalue v′\n3by traversing n′\n1,n′\n2,n′\n4under the latest block i+\naddr valueMerkle File Value File Index FileMB-tree In-Mem:\nOn-Disk:\nblkFigure 3: Overview of COLE\n1. After ProvQuery (a11e67,[i,i])gets v3and the proof π=\n{n1,n2,n4,h(n3)}in block i,VerifyProv (·)is used to verify\nthe integrity of v3by reconstructing the root digest using the\nnodes from n4ton1inπand checks whether the reconstructed\none matches the public digest Hiin block iand whether the\nsearch path in πcorresponds to the address a11e67.\n3 COLE Overview\nThis section presents COLE, our proposed column-based\nlearned storage for blockchain systems. We first give the\ndesign goals and then show how COLE achieves these goals.\n3.1 Design Goals\nWe aim to achieve the following design goals for COLE:\n•Minimizing storage size. To scale up the blockchain sys-\ntem, it is important to reduce the storage size by leveraging\nthe learned index and column-based design.\n•Supporting the requirements of blockchain storage.\nAs blockchain storage, it should ensure data integrity and\nsupport provenance queries as mentioned in Section 2.\n•Achieving efficient writes in a disk environment. Since\nblockchain is write-intensive and all data needs to be\npreserved on disk, the system should be write-optimized\nand disk-optimized to achieve better performance.\n3.2 Design Overview\nFigure 3 shows the overview of COLE. Following the column-\nbased design [4,36], we adopt an analogy between blockchain\nstates and database columns. Each state’s historical versions\nare contiguously stored in the index of the latest block. When\na state is updated in a new block, the state and its version\nnumber (i.e., block height) are appended to the index where\nall of the state’s historical versions are stored. For indexing\nhistorical state values, we use a compound key Kin the form\nof⟨addr,blk⟩, where blkis the block height when the value of\naddr was updated. In Figure 3, when block i+1updates the\nstate at address k3(highlighted in red), a new compound key\nofk3,K′\n3← ⟨k3,i+1⟩, is created. Then, the updated value v′\n3\nindexed by K′\n3is inserted into COLE. With the column-based\ndesign, v′\n3is stored next to k3’s old version v3. Compared with\n3\n\nthe MPT in Figure 1, the cumbersome node duplication along\nthe update path (e.g., n1,n2,n4andn′\n1,n′\n2,n′\n4) is avoided to\nsave the storage overhead.\nTo mitigate the high write cost associated with learned\nmodels for indexing blockchain data in a column-based de-\nsign, we propose using the LSM-tree maintenance strategy in\nCOLE. It structures index storage into levels of exponentially\nincreasing sizes. New data is initially added to the first level.\nWhen the level reaches its pre-defined maximum capacity,\nall the data in that level is merged into a sorted run in the\nnext level. This merge operation can occur recursively until\nthe capacity requirement is no longer violated. The first level,\noften highly dynamic, is typically stored in memory, while\nother levels reside on disk. COLE employs Merkle B+-tree\n(MB-tree) [29] for the first level and disk-optimized learned\nindexes for subsequent levels. We choose MB-tree over MPT\nfor the in-memory level due to its better efficiency in com-\npacting data into sorted runs and flushing them to the first\non-disk level.\nEach on-disk level contains a fixed number of sorted runs,\neach of which is associated with a value file, an index file, and\na Merkle file:\n•Value file stores blockchain states as compound key-value\npairs, which are ordered by their compound keys to facili-\ntate the learned index.\n•Index file helps locate blockchain states in the value file\nduring read operations. It uses a disk-optimized learned\nindex, inspired by PGM-index [20], for efficient data re-\ntrieval with minimal IO cost.\n•Merkle file authenticates the data stored in the value file.\nIt is an m-ary complete MHT built on the compound key-\nvalue pairs.\nNote that since the model construction and utilization require\nnumerical data types, we convert a compound key into a big\ninteger using the binary representation of the address and\nthe block height. For example, given a compound key K←\n⟨addr,blk⟩, its big integer is computed as binary (addr)×\n264+blk, where blkis a 64-bit value. Moreover, to ensure\ndata integrity, root hashes of both the in-memory MB-tree\nand the Merkle files of each on-disk run are combined to\ncreate a root_hash_list . The root digest of states, stored in the\nblock header, is computed from this list. This list is cached in\nmemory to expedite root digest computation.\nWith this design, to retrieve the state value of address addr q\nat a block height blkq, a compound key Kq← ⟨addr q,blkq⟩is\nemployed. The process entails a level-wise search within\nCOLE, initiated from the first level. The MB-tree or the\nlearned indexes in other levels are traversed. The search ceases\nupon encountering a compound key Kr← ⟨addr r,blkr⟩\nwhere addr r=addr qandblkr≤blkq, at which point the cor-\nresponding value is returned. For retrieving the latest value\nof a state, the procedure remains similar but with the search\nkey set to ⟨addr q,max_int⟩, where max_intis the maximum\ninteger. That is, the search is stopped as long as a state valueAlgorithm 1: Write Algorithm\n1Function Put( addr,value )\nInput: State address addr , value value\n2blk←current block height; K← ⟨addr,blk⟩;\n3Insert⟨K,value⟩into the MB-tree in L0;\n4ifL0contains B compound key-value pairs then\n5 Flush the leaf nodes in L0toL1as a sorted run;\n6 Generate files FV,FI,FHfor this run;\n7 i←1;\n8 while Licontains T runs do\n9 Sort-merge all the runs in LitoLi+1as a new run;\n10 Generate files FV,FI,FHfor the new run;\n11 Remove all the runs in Li;\n12 i←i+1;\n13 Update Hstatewhen finalizing the current block;\nwith the queried address addr qis found.\n4 Write Operation of COLE\nWe now detail the write operation of COLE. As mentioned in\nSection 3.2, COLE organizes the storage using an LSM-tree,\nwhich consists of an in-memory level and multiple on-disk\nlevels. The in-memory level has a capacity of Bstates in\nthe form of compound key-value pairs. Once this capacity is\nreached, the in-memory level is flushed to the disk as a sorted\nrun. Similarly, when the first on-disk level reaches its capacity\nofTsorted runs, they are merged into a new run in the next\nlevel. This merging process continues for subsequent disk\nlevels, with the size of each run growing exponentially with a\nratio of T. That is, level ihas a maximum of B·Tistates.\nAlgorithm 1 shows COLE’s write operation. It starts by\ncalculating a compound key for the state using the address\nand the current block height (Line 2). The compound key-\nvalue pair is inserted into the in-memory level L0indexed by\nthe MB-tree (Line 3). As L0fills up, it is flushed to the first\non-disk level L1as a sorted run (Line 5). The value file FV\nis generated by scanning compound key-value pairs in the\nMB-tree’s leaf nodes (Line 6). At the same time, the index\nfileFIand the Merkle file FHare constructed in a streaming\nmanner (see Section 4.1, Section 4.2 for details). When on-\ndisk level Lifills up (i.e., with Truns), all the runs in Li\nare merge-sorted as a new run in the next level Li+1, with\ncorresponding three files generated (Lines 8 to 11). This level-\nmerge process continues recursively until a level does not fill\nup. The blockchain’s state root digest Hstateis computed by\nhashing the concatenation of the root hash of L0’s MB-tree\nand root hashes of runs in other levels, stored in root_hash_list ,\nwhen finalizing the current block (Line 13).\nExample. Figure 4 shows an example of the insertion of\ns10. For clarity, we show only the states and the value files but\nomit the index files and Merkle files. Assume B=2andT=3.\nThe sizes of the runs in L1andL2are2and6, respectively.\nAfter s10is inserted into in-memory level L0, the level is full\n4\n\nValue File MB-tree\n1 2Before\ninserting \nAfter\ninserting Figure 4: An Example of Write Operation\nand its states are flushed to L1as a sorted run (step 1⃝). This\nincurs L1reaching the maximum number of runs. Thus, all\nthe runs in L1are next sort-merged as a new run, placed in L2\n(step 2⃝). Finally, L0andL1are empty and L2has two runs,\neach of which contains six states.\nA common optimization technique to speed up read opera-\ntions is to integrate a Bloom filter into the in-memory MB-tree\nand each run in the on-disk levels. We incorporate the Bloom\nfilter into COLE with careful consideration. First, they should\nbe built upon the addresses of the underlying states rather\nthan their compound keys to facilitate read operations. Sec-\nond, since the Bloom filters may produce false positives, if\nthey indicate that an address exists, we further resort to the\nnormal read process of the corresponding MB-tree or the disk\nrun to ensure the search correctness. We will elaborate on\ntheir usage during the read operation in Section 6. Moreover,\nthe Bloom filters should be incorporated alongside the root\nhashes of each run when computing the states’ root digest.\nThis is needed to verify the result integrity during provenance\nqueries.\n4.1 Index File Construction\nAn index file consists of the models that can be used to locate\nthe positions of the states’ compound keys in the value file. In-\nspired by PGM-index [20], we start by defining an ε-bounded\npiecewise linear model (or model for short) as follows.\nDefinition 1 (ε-Bounded Piecewise Linear Model) .The\nmodel is a tuple of M=⟨sl,ic,kmin,pmax⟩, where slandic\nare the slope and intercept of the linear model, kminis the\nfirst key in the model, and pmaxis the last position of the data\ncovered by the model.\nGiven a model, one can predict a compound key K’s posi-\ntion prealin a file, if K≥kmin. The predicted position ppred\nis calculated as ppred=min(K·sl+ic,pmax), which satisfies\n|ppred−preal| ≤ε. Since files are often organized into pages,\nwe set εas half the number of models that can fit into a single\ndisk page to generate the models in a disk-friendly manner.\nAs will be shown, this reduces the IO cost by ensuring that\nat most two pages need to be accessed per model during read\noperations.Algorithm 2: Learn Models from a Stream\n1Function BuildModel( S,ε)\nInput: Input stream S, error bound ε\nOutput: A stream of models {M}\n2kmin←/0,pmax←/0,glast←/0;\n3Init an empty convex hull H;\n4foreach ⟨K,preal⟩ ←Sdo\n5 ifkmin=/0then kmin←K;\n6 Add⟨BigNum (K),preal⟩toH;\n7 Compute the minimum parallelogram Gthat covers H;\n8 ifG.height ≤2εthen\n9 pmax←preal,glast←G;\n10 else\n11 Compute slope sland intercept icfrom glast;\n12 M← ⟨sl,ic,kmin,pmax⟩;\n13 yieldM;\n14 kmin←K;\n15 Init a new convex hull Hwith⟨BigNum (K),preal⟩;\nPositionEnclosed parallelogram Edge of convex hull\nPosition\nFigure 5: An Example of Model Learning\nTo compute models from a stream of compound keys and\ntheir corresponding positions, we treat each compound key\nand its position as a point’s coordinates. Upon the arrival of a\nnew compound key, we convert it into a big integer using the\nbinary representation of the address and the block height as\nmentioned in Section 3.2. Next, we find the smallest convex\nshape containing all the existing input points, which is known\nas a convex hull. Note that this convex hull can be computed\nincrementally in a streaming fashion [40]. Then, we find the\nminimal parallelogram that covers the convex hull, with one\nside aligned to the vertical axis (i.e., the position axis). If the\nparallelogram’s height stays under 2ε, all existing inputs can\nfit into a single model. In this case, we try to include the next\ncompound key in the stream for model construction. However,\nif the current parallelogram fails to meet the height criteria, the\nslope and intercept of the central line in the parallelogram will\nbe used to build a model that covers all existing compound\nkeys except the current one. After this, a new model will be\nbuilt, starting from the current compound key. We summarize\nthe algorithm in Algorithm 2.\nExample. Figure 5 shows an example of model learning\nfrom a stream. Assume states s1tos3form a convex hull, with\nits minimal parallelogram satisfying the height criterion (i.e.,\nbelow 2ε). After state s4is added, the parallelogram’s height\nremains within 2ε(see Figure 5(a)), indicating that states\ns1tos4can be fit into one model. However, after the next\nstate s5is added, the parallelogram’s height exceeds 2ε(see\n5\n\nAlgorithm 3: Index File Construction\n1Function ConstructIndexFile( S,ε)\nInput: Input stream Sof compound key-position pairs\nOutput: Index file FI\n2Create an empty index file FI;\n3Invoke BuildModel (S,ε)and write to FI;\n4n←# of pages in FI;\n5while n>1do\n6 S← {⟨M.kmin,pos⟩ |foreach ⟨M,pos⟩ ∈FI[−n:]};\n7 Invoke BuildModel (S,ε)and append to FI;\n8 n←# of pages in FI−n;\n9return FI;\nFigure 5(b)). Thus, the slope and intercept of the previous\nparallelogram’s central line (highlighted in red) are used to\nbuild a model for s1tos4, with s5reserved for the next model.\nAlgorithm 3 shows the overall procedure of index file gen-\neration. During flush or sort-merge operations in Algorithm 1,\nordered compound keys and state values are generated and\nwritten streamingly into the value file. Meanwhile, another\nstream consisting of compound keys and their positions is cre-\nated and used to generate models with Algorithm 2 (Line 3).\nOnce the models are yielded by Algorithm 2, they are immedi-\nately written to the index file, constituting the bottom layer of\nthe run’s learned index. Then, we recursively build the upper\nlayers of the index until the top layer can fit into a single disk\npage (Lines 4 to 8). Specifically, for each layer, we scan lower-\nlayer models (denoted as FI[−n:]) to create a compound key\nstream using kminin each model and their index file positions\n(Line 6). Similar to the bottom layer, we use Algorithm 2\non the stream to create models and instantly write them to\nthe index file (Line 7). This results in the sequential storage\nof models across layers in a bottom-up manner. The index\nfile remains valid from its construction until the next level\nmerge operation thanks to the LSM-tree-based maintenance\napproach, which avoids costly model retraining.\n4.2 Merkle File Construction\nA Merkle file stores an m-ary complete MHT that authen-\nticates the compound key-value pairs in the corresponding\nvalue file. The related index file’s learned models are excluded\nfrom authentication, as they solely enhance query efficiency\nand do not affect blockchain data integrity. For the m-ary com-\nplete MHT, the bottom layer consists of hash values of every\ncompound key-value pair in the value file. The hash values in\nan upper layer are recursively computed from every mhash\nvalues in the lower layer, except that the last one might be\ncomputed from less than mhash values in the lower layer.\nDefinition 2 (Hash Value) .A hash value in the bottom layer of\nthe MHT is computed as hi=h(Ki∥value i), where Ki,value i\nare the corresponding compound key and value, ∥is the con-\ncatenation operator, and h(·)is a cryptographic hash function\nsuch as SHA-256. A hash value in an upper layer of the MHTAlgorithm 4: Merkle File Construction\n1Function ConstructMerkleFile( S,n,m)\nInput: Input stream Sof compound key-value pairs,\nstream size n, fanout m\nOutput: Merkle file FH\n2Nnodes←[n,⌈n\nm⌉,⌈n\nm2⌉,···,1],d← |Nnodes|;\n3layer_offset [0]←0;\n4layer_offset [i]←∑i−1\n0Nnodes[i−1],∀i∈[1,d−1];\n5Create a merkle file FHwith size ∑d−1\ni=0Nnodes[i];\n6Create a cache Cwith dnumber of buffers;\n7foreach ⟨K,value⟩ ←Sdo\n8 h′←h(K∥value ), append h′toC[0];\n9 foreach iin0tod−2do\n10 if|C[i]|=mthen\n11 h′←h(C[i]), append h′toC[i+1];\n12 FlushC[i]toFHat offset layer_offset [i];\n13 layer_offset [i]←layer_offset [i]+m;\n14 else break ;\n15 foreach iin0tod−1do\n16 ifC[i]is not empty then\n17 h′←h(C[i]), append h′toC[i+1];\n18 FlushC[i]toFHat offset layer_offset [i];\n19 return FH;\nis computed as hi=h(h1\ni∥h2\ni∥···∥ hm∗\ni), where m∗≤mand\nhj\niis the corresponding j-th hash in the lower layer.\nSimilar to Algorithm 3, we streamingly generate the Merkle\nfile. However, instead of layer-wise construction, we concur-\nrently build all MHT layers to reduce IO costs, as shown in\nAlgorithm 4. Note that the size of the input stream of com-\npound key-value pairs nis known in advance since the size\nof a value file is determined by the level of its correspond-\ning run. Thus, the MHT has ⌈logmn⌉+1layers, containing\nn,⌈n\nm⌉,⌈n\nm2⌉,···,1hash values (Line 2). Layer offsets can\nalso be computed (Lines 3 to 4). For concurrent construction,\n⌈logmn⌉+1buffers are maintained, one per layer. Upon the\narrival of a new compound key-value pair, its hash value is\ncomputed and added to the bottom layer’s buffer (Line 8).\nWhen a buffer fills with mhash values, an upper layer’s hash\nvalue is created and added to its buffer (Line 11). Next, the\nbuffered hash values in the current layer are flushed to the\nMerkle file, followed by incrementing the offset (Lines 12\nto 13). This process recurs in upper layers until a layer with\nless than mbuffered hash values is encountered. Once the\ninput stream is fully processed, any remaining non-empty\nbuffers will hold fewer than mhash values. If so, we’ll initi-\nate this process by taking a buffer from the lowest layer and\niteratively generating hash values. Each hash value is added\nto the upper layer before flushing the buffer to the Merkle file\n(Lines 15 to 18).\nExample. Figure 6 shows an example of a 2-ary MHT with\nstates s1tos4. According to the MHT’s structure, Nnodes=\n[4,2,1]andlayer_offset = [0,4,6]. Assume that s1,s2are al-\nready added. In this case, FHhash1,h2and cache C[1]con-\n6\n\n123Flush\nFlush FlushMHT Hash V aluesFigure 6: An Example of Merkle File Construction\ntains h12, where h1,h2are the hash values of s1,s2andh12is\nderived from h1,h2(Figure 6(a)). Meanwhile, layer_offset [0]\nhas been updated to 2. After s3,s4are added, their hashes\nh3,h4will be inserted to cache C[0], resulting in C[0]having\n2hash values. Thus, h34derived from h3,h4will be added\ninto cache C[1]andh3,h4are then flushed to FHat offset 2\n(step 1⃝). Since C[1]also has 2hash values so the derived h14\nis added to cache C[2]andh12,h34are flushed to FHat offset\nlayer_offset [1] =4(step 2⃝). Finally, h14inC[2]is flushed to\nFHat offset layer_offset [2] =6(step 3⃝).\n4.3 Discussions\nAs discussed earlier, COLE adopts the LSM-tree-based main-\ntenance approach to optimize data writes and disk operations\nunder the column-based design. However, it also comes with\nsome tradeoffs. The presence of multiple levels can impact\nread performance, as retrieving a state requires traversing mul-\ntiple levels until a satisfactory result is found. Additionally,\nthe merge operation complicates the process of state rewind,\nas data cannot be deleted in-place. Therefore, COLE does\nnot support blockchain forking and is designed to work with\nblockchains that do not fork [5, 22, 53].\nWe next discuss the ACID properties in COLE. COLE\nachieves atomicity by maintaining root_hash_list in an atomic\nmanner. During the level merge process, root_hash_list is\nupdated atomically only after constructing all three files in\nthe new level, followed by removing the old level files. This\nensures data consistency as the old level files remain in-\ntact and are referenced by root_hash_list even during a node\ncrash. Concurrency control is not required due to the write-\nserializability guarantee of the consensus protocol. Data in-\ntegrity is ensured using Merkle-based structures for each level.\nFor durability, COLE uses transaction logs as the Write Ahead\nLog since they are agreed upon by the consensus protocol.\nIn case of a crash, COLE recovers by replaying transactions\nsince the last checkpoint. A checkpoint is created when the in-\nmemory MB-tree is flushed to the first disk level and cleared.\nAt this time point, all the data in the system is safely stored\non the disk. After a crash, COLE reverts to the last check-\npoint, discards all the files in the unfinished merge levels,and starts fresh with an empty in-memory MB-tree. It then\nreplays all unprocessed transactions and restarts the aborted\nlevel merges.\n5 Write with Asynchronous Merge\nAlgorithm 1 may trigger recursive merge operations during\nsome writes (e.g., steps 1⃝and 2⃝in Figure 4). As a result, it\ncan introduce long-tail latency and cause all future operations\nto stall. This issue is known as write stall , which leads to\nperiodic drops in application throughput to near-zero levels\nand dramatic fluctuations in system performance. A com-\nmon solution is to make the merge operations asynchronous\nby moving them to separate threads. However, the existing\nasynchronous merge solution is not suitable for blockchain\napplications. Since different nodes in the blockchain network\ncould have drastically different computation capabilities, the\nstorage structure will become out-of-sync among nodes when\napplying asynchronous merges. This will result in different\nHstate’s and break the requirement of the blockchain protocol.\nTo address these challenges, we design a novel asyn-\nchronous merge algorithm for COLE, which ensures the syn-\nchronization of the storage across blockchain nodes. The\nalgorithm introduces two checkpoints, start and commit ,\nwithin the asynchronous merge process for each on-disk\nlevel. By synchronizing the checkpoints, we ensure consistent\nblockchain storage and thus Hstateagreed by the network. To\nfurther minimize the possibility of long-tail latency due to\ndelays at the commit checkpoint, we propose to make the\ninterval between the start checkpoint and the commit check-\npoint proportional to the size of the run. This ensures that the\nmajority of the nodes in the network can complete the merge\noperation before reaching the commit checkpoint.\nTo realize our idea, we propose to have each level of COLE\ncontain two groups of runs as shown in Figure 7. Each group’s\ndesign is identical to the one discussed in Section 4. Specifi-\ncally, the in-memory level now contains two groups of MB-\ntree, each with a capacity of Bstates. Similarly, each on-disk\nlevel contains two groups of up to Tsorted runs. Level ican\nhold a maximum of 2·B·Tistates. The two groups in each\nlevel have two mutually exclusive roles, namely writing and\nmerging . The writing group accepts newly created runs from\nthe upper level. On the other hand, the merging group gen-\nerates a new run from its own data and adds to the writing\ngroup of the next level in an asynchronous fashion.\nAlgorithm 5 shows the write operation in COLE with asyn-\nchronous merge. First, new state values are inserted into the\ncurrent writing group of in-memory level L0(Lines 2 to 4).\nThe levels in COLE are then traversed from smaller to larger.\nWhen a level is full, we commit the previous merge operation\nin the current level and start a new merge operation in a new\nthread. To accommodate slow nodes in the network, we check\nif the previous merging thread of the current level exists and is\nstill in progress, and wait for it to finish if necessary (Line 9).\n7\n\nUncommitted FileFigure 7: Asynchronous Merge\nCommitted V alue File Uncommitted File MB-tree Figure 8: An Example of Asynchronous Merge\nAlgorithm 5: Write Algorithm with Asynchronous Merge\n1Function Put( addr,value )\nInput: State address addr , value value\n2blk←current block height; K← ⟨addr,blk⟩;\n3w0←GetL0’s writing group;\n4Insert⟨K,value⟩into the MB-tree of w0;\n5i←0;\n6while wibecomes full do\n7 mi←GetLi’s merging group;\n8 ifmi.merge_thread exists then\n9 Wait for mi.merge_thread to finish;\n10 Add the root hash of the generated run from\nmi.merge_thread to root_hash_list ;\n11 Remove the root hashes of the runs in mifrom\nroot_hash_list ;\n12 Remove all the runs in mi;\n13 Switch miandwi;\n14 mi.merge_thread ←start thread do\n15 ifi=0then\n16 Flush the leaf nodes in mitoLi+1’s writing group a\nsorted run;\n17 Generate files FV,FI,FHfor the new run;\n18 else\n19 Sort-merge all the runs in mitoLi+1’s writing\ngroup a new run;\n20 Generate files FV,FI,FHfor the new run;\n21 i←i+1;\n22 Update Hstatewhen finalizing the current block;\nThe previous merge operation is committed by adding the root\nhash of the newly generated run to root_hash_list (Line 10),\nwhile obsolete run hashes are removed from root_hash_list\n(Line 11) and the obsolete runs in the merging group are\nalso removed (Line 12). The above procedure ensures the\ncommit checkpoint occurs simultaneously across nodes in\nthe network, which is essential to synchronize the blockchain\nstates and the corresponding root digest. Following this, the\nroles between the two groups in the current level are switched\n(Line 13). This means that future write operations will be di-\nrected to the vacated space of the new writing group, whereas\nthe merge operation will be performed on the new merging\ngroup, which is now full. The latter starts a new merge thread,\nwhose procedure is similar to that of Algorithm 1 (Lines 14\nto 20). Lastly, when finalizing the current block, Hstate isupdated using stored root hashes in root_hash_list (Line 22).\nExample. Figure 8 shows an example of the asynchronous\nmerge from level LitoLi+1, where T=3. The uncommit-\nted files are denoted by dashed boxes. Figure 8(a) shows\nCOLE’s structure before Li’s commit checkpoint, when Li’s\nwriting group wibecomes full. In case mi’s merging thread\n(denoted by the purple arrow) is not yet finished, we wait for\nit to finish. Then, during Li’s commit checkpoint, wi+1.R1’s\nroot hash is added to root_hash_list and all runs in mi(i.e.,\nmi.R0,mi.R1,mi.R2) are removed (Figure 8(b)). Next, miand\nwi’s roles are switched. Finally, a new thread will be started\n(denoted by the blue arrow) to merge all runs in mitoLi+1’s\nwriting group as the third run w i+1.R2(Figure 8(c)).\nSoundness Analysis . Next, we show our proposed asyn-\nchronous merge operation is sound. Specifically, the following\ntwo requirements are satisfied.\n•The blockchain states’ root digest Hstateis always synchro-\nnized among nodes in the blockchain network regardless\nof how long the underlying merge operation takes.\n•The interval between the start checkpoint and the commit\ncheckpoint for each level is proportional to the size of the\nruns to be merged.\nThe first requirement ensures blockchain states are solely\ndetermined by the current committed states and are indepen-\ndent of individual node performance variations. The second\nrequirement minimizes the likelihood of nodes waiting for\nmerge operations of longer runs. We now prove that our algo-\nrithm complies with the requirements.\nProof Sketch. It is trivial to show that the first requirement is\nsatisfied as the update of root_hash_list (hence Hstate) occurs\noutside the asynchronous merge thread, making the update\nofHstatefully synchronous and deterministic. For the second\nrequirement, the interval between the start checkpoint and\nthe commit checkpoint in any level equals the time taken to\nfill up the writing group in the same level. Since the latter\ncontains those runs to be merged in this level, the interval is\nproportional to the size of the runs.\n6 Read Operations of COLE\nIn this section, we discuss the read operations of COLE, in-\ncluding the get query and the provenance query with its veri-\n8\n\nAlgorithm 6: Get Query\n1Function Get( addr )\nInput: State address addr\nOutput: State latest value value\n2Kq← ⟨addr,max_int⟩;\n3foreach gin{L0’s writing group, L0’s merging group }do\n4 ⟨K′,state′⟩ ←SearchMBTree (g,Kq);\n5 ifK′.addr =addr then return state′;\n6foreach level i in{1,2,...}do\n7 RS←{Ri,j|Ri,j∈Li’s writing group ∧committed };\n8 RS←RS+{Ri,j|Ri,j∈Li’s merging group };\n9 foreach Ri,jinRSdo\n10 ⟨⟨K′,state′⟩,pos′⟩ ←SearchRun (Ri,j,Kq);\n11 ifK′.addr =addr then return state′;\n12 return nil;\nfication function. We assume that COLE is implemented with\nthe asynchronous merge.\n6.1 Get Query\nAlgorithm 6 shows the get query process. As mentioned in\nSection 3.2, getting a state’s latest value requires a special\ncompound key Kq=⟨addr q,max_int⟩. Owing to the tem-\nporal order of COLE’s levels, we perform the search from\nsmaller levels to larger levels, until a satisfied state value is\nfound. This involves searching both the writing and merging\ngroups’ MB-trees in the in-memory level L0as both of them\nare committed (Lines 3 to 5). Then, in each on-disk level, a\nsearch is performed in the committed writing group’s runs,\nfollowed by the merging group’s runs (Lines 6 to 11). Note\nthat the runs in the same group will be searched in the order\nof their freshness. For the example in Figure 7, we search\nthe MB-trees in w0andm0, followed by the runs in the order\nofw1.R1,w1.R0,m1.R2,m1.R1,m1.R0,w2.R0,···, while the\nuncommitted w1.R2,w2.R1are skipped. The search halts once\nthe satisfied state is found.\nTo search an on-disk run, we use Algorithm 7. First, if the\nqueried address addr qis not in the run’s bloom filter B, the\nrun is skipped (Line 2). Otherwise, models in the index file\nFIare used to find Kq. The search starts from the top layer of\nmodels, stored on the last page of FI. The model covering Kq\nis found by binary searching kminof each model in this page\n(Line 4). Then, a recursive query on models in subsequent\nlayers is conducted from top to bottom (Lines 5 to 7). Upon\nreaching the bottom layer, the corresponding model is used\nto locate the state value in the value file FV(Line 8).\nFunction QueryModel (·)in Algorithm 7 shows the pro-\ncedure of using a learned model Mto locate the queried\ncompound key Kq. If the model covers Kq, it predicts the\nposition pos predof the queried data (Line 12). With the error\nbound of the model 2εequaling the page size, the predicted\npage id is computed as pos pred/2ε(Line 13). The correspond-\ning page Pis fetched and the first and last models are checked\nwhether they cover Kq. If not, the adjacent page is fetched asAlgorithm 7: Search a Run\n1Function SearchRun( FI,FV,B,Kq)\nInput: Index file FI, value file FV, bloom filter B,\ncompound key Kq=⟨addr q,blkq⟩\nOutput: Queried state sand its position pos\n2ifaddr q/∈Bthen return ;\n3Kq←BigNum (Kq);\n4P←FI’s last page; M←BinarySearch (P,Kq);\n5⟨M,pos⟩ ←QueryModel (M,FI,Kq);\n6while pos is notpointing to the bottom models do\n7 ⟨M,pos⟩ ←QueryModel (M,FI,Kq);\n8return QueryModel (M,FV,Kq);\n9Function QueryModel( M,F,Kq)\nInput: Model M, query file F, compound key Kq\nOutput: Queried data and its position in F\n10⟨sl,ic,kmin,pmax⟩ ←M;\n11 ifKq<kminthen return ;\n12 pospred←min(Kq·sl+ic,pmax);\n13 page pred←pospred/2ε;\n14P←F’s page at page pred;\n15 ifKq<P[0].kthen\n16 P←F’s page at page pred−1;\n17 else if Kq>P[−1].kthen\n18 P←F’s page at page pred+1;\n19 return BinarySearch (P,Kq);\nP(Lines 15 to 18). This process involves at most two pages\nfor prediction, hence minimizing IO. Finally, a binary search\ninPlocates the queried data (Line 19).\n6.2 Provenance Query\nA provenance query resembles a get query but with no-\ntable distinctions. Unlike a get query, a provenance query\ninvolves a range search based on the queried block height\nrange. This entails computing two boundary compound keys,\nKl=⟨addr,blkl−1⟩andKu=⟨addr,blku+1⟩, with offsets\nadjusted by one to prevent the omission of valid results. More-\nover, a provenance query provides Merkle proofs to authenti-\ncate the results.\nSpecifically, during the search of MB-trees in L0, in ad-\ndition to retrieving satisfactory results, Merkle paths are in-\ncluded in the proof using a similar approach mentioned in\nSection 2. For the runs of the on-disk levels, we search in the\nsame order as those described in Algorithm 6. Klis used as\nthe search key when applying the learned models to find the\nfirst query result in each run. Then, the value file is scanned\nsequentially until a state beyond Kuis reached.1Afterwards,\na Merkle proof is computed upon the first and last results’\npositions pos l,pos uof each run. Since the states in the value\nfile and their hash values in the Merkle file share the same\nposition, the Merkle paths of the hash values at pos landpos u\nare used as the Merkle proof. To compute the Merkle path,\n1For simplicity, we assume that addr is in the bloom filter B. If not, Bis\nalso added as the proof to prove that addr is not in the run.\n9\n\nCost MPT COLE COLE w/ async-merge\nStorage size O(n·dMPT) O(n)\nWrite IO cost O(dMPT) O(dCOLE)\nWrite tail latency O(1) O(n) O(1)\nWrite memory footprint O(1) O(T+m·dCOLE)O(T·dCOLE +m·d2\nCOLE)\nGet query IO cost O(dMPT) O(T·dCOLE·Cmodel)\nProv-query IO cost O(dMPT) O(T·dCOLE·Cmodel+m·d2\nCOLE)\nProv-query proof size O(dMPT) O(m·d2\nCOLE)\nTable 1: Complexity Comparison\nwe traverse the MHT in the Merkle file from bottom to top.\nNote that given a hash value’s position posat layer i, we\ncan directly compute its parent hash value’s position in the\nMerkle file as ⌊(pos−∑i−1\n0⌈n\nmi⌉)/m⌋+∑i\n0⌈n\nmi⌉. Due to the\nspace limitation, the detailed procedure of the provenance\nquery is given in Appendix A.\nOn the user’s side, the verification algorithm works as fol-\nlows: (1) use each MB-tree’s results and their corresponding\nMerkle proof to reconstruct the MB-tree’s root hash; (2) use\neach searched run’s results and their corresponding Merkle\nproof to reconstruct the run’s root hash; (3) use the recon-\nstructed root hashes to reconstruct the states’ root digest and\ncompare it with the published one, Hstate, in the block header;\n(4) check the boundary results of each searched run against\nthe compound key range [Kl,Ku]to ensure no missing results.\nIf all these checks pass, the results are verified.\n7 Complexity Analysis\nIn this section, we analyze the complexity in terms of storage,\nmemory footprint, and IO cost. To ease the analysis, we as-\nsume nas the total historical values, Tas the level size ratio,\nBas the in-memory level’s capacity, and mas COLE’s MHT\nfanout. Table 1 shows the comparison of MPT, COLE, and\nCOLE with the asynchronous merge.\nWe first analyze the storage size. Since MPT duplicates\nthe nodes of the update path for each insertion, its storage\nhas a size of O(n·dMPT), where dMPT is the height of the\nMPT. COLE completely removes the node duplication, thus\nachieving an O(n)storage size.\nNext, we analyze the write IO cost. MPT takes O(dMPT)\nto write the nodes in the update path, while COLE takes\nO(dCOLE)for the worst case when all levels are merged,\nwhere dCOLE is the number of levels in COLE. Similar to\nthe traditional LSM-tree’s write cost [13], the level merge in\nCOLE takes an amortized O(1)IO cost to write the value\nfile, the index file, and the Merkle file. The number of levels\ndCOLE is⌈logT(n\nB·T−1\nT)⌉, which is logarithmic to n. Note that\nnormally dCOLE <dMPT since dMPT depends on the data’s\nkey size, which can be large (e.g., when having a 256-bit key,\nmaximum dMPT is 64 under hexadecimal base while COLE\nhas only a few levels following the LSM-tree).\nRegarding the write tail latency, MPT has a constant cost\nsince there is no write stall during data writes. On the other\nhand, COLE may experience the write stall in the worst case,\nwhich requires waiting for the merge of all levels and resultsin the reading and writing of O(n)states. The asynchronous\nmerge algorithm removes the write stall by merging the levels\nin background threads and reduces the tail latency to O(1).\nAs for the write memory footprint, MPT has a constant\ncost since the update nodes are computed on the fly and can\nbe removed from the memory after being flushed to the disk.\nFor COLE, we consider the case of merging the largest level\nas this is the worst case. The sort-merge takes O(T)memory\nand the model construction takes constant memory [40]. Con-\nstructing the Merkle file takes O(m·dCOLE)since there are\nlogarithmic layers of cache buffers and each buffer contains\nmhash values. To sum up, COLE takes O(T+m·dCOLE)\nmemory during a write operation. For COLE with the asyn-\nchronous merge, the worst case is that each level has a merg-\ning thread, thus requiring dCOLE times of memory compared\nwith the synchronous merge, i.e., O(T·dCOLE+m·d2\nCOLE).\nWe finally analyze the read operations’ costs, including\nthe get query IO cost, the provenance query IO cost, and the\nproof size of the provenance query. MPT’s costs are all linear\nto the MPT’s height, O(dMPT). For COLE, Truns in each\nlevel should be queried, where we assume that each run takes\nCmodel to locate the state. Therefore, the cost of the get query\nisO(T·dCOLE·Cmodel). To generate the Merkle proof during\nthe provenance query, an additional O(m·d2\nCOLE)is required\nsince there are multiple layers of MHT in all levels and O(m)\nhash values are retrieved for each MHT’s layer. The proof\nsize is O(m·d2\nCOLE)for a similar reason.\n8 Evaluation\nIn this section, we first describe the experiment setup, includ-\ning comparing baselines, implementation, parameter settings,\nworkloads, and evaluation metrics. Then, we present the ex-\nperiment results.\n8.1 Experiment Setup\n8.1.1 Baselines\nWe compare COLE with the following baselines:\n•MPT : It is used by Ethereum to index the blockchain\nstorage. The structure is made persistent as mentioned in\nSection 1.\n•LIPP : It applies LIPP [54], the state-of-the-art learned\nindex supporting in-place data writes, to the blockchain\nstorage without our column-based design. LIPP retains the\nnode persistence strategy to support provenance queries.\n•Column-based Merkle Index (CMI) : It uses the column-\nbased design with traditional Merkle indexes rather than\nthe learned index. It adopts a two-level structure. The\nupper index is a non-persistent MPT whose key is the\nstate address and the value is the root hash of the lower\nindex. The lower index follows the column-based design,\nusing an MB-tree to store the state’s historical values in a\n10\n\nParameters Value\n# of generated blocks 102,103,104,105\nSize ratio T 2,4,6,8,10,12\nCOLE’s MHT fanout m 2,4,8,16,32,64\nTable 2: System Parameters\ncontiguous fashion [29].\n8.1.2 Implementation and Parameter Setting\nWe implement COLE and the baselines in Rust program-\nming language. The source code is available at https:\n//github.com/hkbudb/cole . We use the Rust Ethereum\nVirtual Machine (EVM) to execute transactions, simulat-\ning blockchain data updates and reads [2]. Transactions are\npacked into blocks, each containing 100 transactions. Ten\nsmart contracts are initially deployed and repeatedly invoked\nwith transactions. Big number operations mentioned in Sec-\ntion 3.2 are implemented using the ruglibrary [3]. Baselines\nutilize RocksDB [18] as the underlying storage, while COLE\nuses simple files for data storage as enabled by our design.\nWe set ε=23based on the page size (4KB) and the com-\npound key-pair size (88 bytes). By default, the size ratio T\nand the MHT fanout mof COLE are set to 4. Following the\ndefault configuration of RocksDB, its memory budget is set\nto 64MB. The in-memory capacity Bis set to the number of\nstates that can fit within the same memory budget. Table 2\nshows all the parameters where the default settings are high-\nlighted in bold font. All experiments are run on a machine\nequipped with an Intel i7-10710U CPU, 16GB RAM, and\nSamsung SSD 256GB.\n8.1.3 Workloads and Evaluation Metrics\nThe experiment evaluation includes two parts: the overall per-\nformance of transaction executions and the performance of\nprovenance queries. For the first part, SmallBank and KVS-\ntore from Blockbench [17] are used as macro benchmarks to\ngenerate the transaction workload. SmallBank simulates the\naccount transfers while KVStore uses YCSB [9] for read/write\ntests. YCSB involves a loading phase where base data is gen-\nerated and stored, followed by a running phase for read/update\noperations. A transaction that reads/updates data is denoted\nas a read/write transaction. We set 105transactions as the\nbase data and vary read/update ratios to simulate different\nscenarios: (i) Read-Write with equal read/write transactions;\n(ii) Read-Only with only read transactions; and (iii) Write-\nOnly with all write transactions. The overall performance is\nevaluated in terms of the average transaction throughput, the\ntail latency, and the storage size.\nTo evaluate provenance queries, we use KVStore to simu-\nlate the workload including frequent data updates. We initially\nwrite 100 states as the base data and then continuously gen-\nerate write transactions to update the base data’s states. For\n10−1101103105\n102103104105Storage Size (MB)\nBlock HeightMPT\nCOLELIPP\nCOLE*CMI\n✖ ✖✖\n100101102103104105\n102103104105Throughput (TPS)\nBlock HeightMPT\nCOLELIPP\nCOLE*CMI\n✖ ✖✖Figure 9: Performance vs. Block Height (SmallBank)\n10−1101103105\n102103104105Storage Size (MB)\nBlock HeightMPT\nCOLELIPP\nCOLE*CMI\n✖ ✖ ✖✖\n100101102103104105\n102103104105Throughput (TPS)\nBlock HeightMPT\nCOLELIPP\nCOLE*CMI\n✖ ✖ ✖✖\nFigure 10: Performance vs. Block Height (KVStore)\neach query, we randomly select a key from the base data and\nvary the block height range (e.g., 2,4,···,128), which fol-\nlows [44]’s setting. The evaluation metrics include (i) CPU\ntime of the query executed on the blockchain node and veri-\nfied by the query user and (ii) proof size.\n8.2 Experimental Results\n8.2.1 Overall Performance\nFigures 9 and 10 show the storage size and throughput of\nCOLE and all baselines under the SmallBank and KVStore\nworkloads, respectively. We denote COLE with the asyn-\nchronous merge as COLE*.\nWe make several interesting observations. First, COLE\nsignificantly reduces the storage size compared to MPT as\nthe blockchain grows. For example, at a block height of 105,\nthe storage size decreases by 94% and 93% for SmallBank\nand KVStore, respectively. This is due to COLE’s elimina-\ntion of the need to persist internal data structures via the\ncolumn-based design, and its use of storage-efficient learned\nmodels for indexing. Moreover, COLE outperforms MPT in\nthroughput, achieving a 1.4×-5.4×improvement, thanks to\nits learned index. COLE* performs slightly worse than COLE\ndue to the overhead of the asynchronous merge.\nSecond, using the learned index without the column-based\ndesign ( LIPP ) even increases the blockchain storage. At a\nblock height of 102, the storage size of LIPP already exceeds\nMPT ’s by 5×(for SmallBank) and 31×(for KVStore). This\nhappens because the learned index often generates larger in-\ndex nodes that must be persisted with each new block, leading\nto increased storage and significant IO operations. Conse-\nquently, LIPP ’s throughput is significantly worse than MPT .\nWe are not able to report the results of LIPP for the block\nheight above 103for SmallBank and 102for KVStore as the\nexperiment could not be finished within 24 hours.\n11\n\n100101102103104105\nRO RW WOThroughput (TPS)\nWorkloads\n (a) 104 Block HeightMPT COLE COLE*\n100101102103104105\nRO RW WOThroughput (TPS)\nWorkloads\n (b) 105 Block HeightMPT COLE COLE*Figure 11: Throughput vs. Workloads (KVStore)\nThird, extending MPT with the column-based design ( CMI)\ndoes not significantly change the storage size. The addi-\ntional storage of the lower-level MB-tree and the use of the\nRocksDB backend largely negate the benefit of removing\nnode persistence. Additionally, refreshing Merkle hashes of\nall nodes in the index update path, which entails both read\nand write IOs, further impacts performance. Consequently,\nthe throughput of CMI is7×and22×worse than MPT for\nSmallBank and KVStore, respectively, at a block height of\n104. The experiments of CMI cannot scale beyond a block\nheight of 104.\nOverall, with a unique combination of the learned index,\ncolumn-based design, and write-optimized strategies, COLE\nand COLE* not only achieve the smallest storage requirement\nbut also gain the highest system throughput.\n8.2.2 Impact of Workloads\nWe use KVStore to evaluate the impact of different workloads,\nnamely Read-Only (RO), Read-Write (RW), and Write-Only\n(OW), in terms of the system throughput. As shown in Fig-\nure 11, the throughputs of all systems decrease with more\nwrite operations in the workload. The performance of MPT\ndegrades by up to 93% while that of COLE and COLE* de-\ngrades by up to 87%. This shows that the LSM-tree-based\nmaintenance approach helps optimize the write operation.\nWe omit LIPP andCMI in Figure 11 since they cannot scale\nbeyond a block height of 103and 104, respectively.\n8.2.3 Tail Latency\nTo assess the effect of the asynchronous merge, Figure 12\nshows the box plot of the latency of SmallBank and KVStore\nworkloads at block heights of 104and105. The tail latency is\ndepicted as the maximum outlier. As the blockchain grows,\nCOLE* decreases the tail latency by 1-2 orders of magnitude\nfor both workloads. This shows that the asynchronous merge\nstrategy will become more effective when the system scales\nup for real-world applications. Owing to the asynchronous\nmerge overhead, COLE* incurs slightly higher median latency\nthan COLE, but it still outperforms MPT.\n8.2.4 Impact of Size Ratio\nFigure 13 shows the system throughput and latency box plot\nunder 105block height using the SmallBank benchmark with\n100101102103104105\n104105Latency (ms)\nBlock Height (SmallBank)MPT COLE COLE*\n100101102103104105\n104105Latency (ms)\nBlock Height (KVStore)MPT COLE COLE*Figure 12: Latency Box Plot\n100101102103104105\n2 4 6 8 10 12Throughput (TPS)\nSize RatioCOLE COLE*\n100101102103104105\n2 4 6 8 10 12Latency (ms)\nSize RatioCOLE COLE*\nFigure 13: Impact of Size Ratio\nvarying size ratio T. As the size ratio increases, the through-\nput remains stable, while the tail latency shows a U shape.\nWe observe that T=6andT=4are the best settings for\nCOLE and COLE*, respectively, with the lowest tail latency.\nMeanwhile, with an increasing size ratio, the median latency\nof both COLE and COLE* increases.\n8.2.5 Provenance Query Performance\nWe now evaluate the performance of provenance queries by\nquerying historical state values of a random address within\nthe latest qblocks. With the current block height fixed at 105,\nwe vary qfrom 2to128.LIPP andCMIare omitted here since\nthey cannot scale at 105block height. Figure 14 shows that\nMPT ’s CPU time and proof size grow linearly with qwhile\nthose of COLE and COLE* grow only sublinearly. This is\nbecause MPT requires to query each block inside the queried\nrange. In contrast, COLE and COLE*’s column-based design\noften locates query results within contiguous storage of each\nrun, hence reducing the number of index traversals during the\nquery and shrinking the proof size by sharing ancestor nodes\nin the Merkle path. COLE and COLE*’s proof sizes surpass\nthat of MPT when the query range is small due to limited\nsharing capabilities within a small query range.\n9 Related Work\nIn this section, we briefly review the related works on learned\nindexes and blockchain storage management.\n9.1 Learned Index\nLearned index has been extensively studied in recent years.\nThe original learned index [26] only supports static data while\nPGM-index [20], Fiting-tree [21], ALEX [15], LIPP [54], and\nLIFOSS [61] support dynamic data using different strate-\ngies. All these works are designed and optimized for in-\n12\n\n100101102103104\n248163264128CPU Time (us)\nBlock Height RangeMPT COLE COLE*\n100101102103\n248163264128Proof Size (KB)\nBlock Height RangeMPT COLE COLE*Figure 14: Prov-Query Performance vs. Query Range\nmemory databases. Bourbon [10] uses the PGM-based models\nto speed up the lookup in the WiscKey system, which is a\npersistent key-value store. [27] investigates how existing dy-\nnamic learned indexes perform on-disk and shows the design\nchoices. Some other learned indexes are proposed for more\ncomplex application scenarios like spatial data [23, 32, 47],\nmulti-dimensional data [16, 39], and variable-length string\ndata [51]. Moreover, [30, 34] consider designing learned in-\ndexes for concurrent systems. [64] proposes a persistent\nlearned index that is specifically designed for the NVM-only\narchitecture with concurrency control. More recently, [31] de-\nsigns a scalable RDMA-oridented learned key-value store for\ndisaggregated memory systems. Nevertheless, existing works\ncannot be directly applied to blockchain storage since they do\nnot take into account disk-optimized storage, data integrity,\nand provenance queries simultaneously.\n9.2 Blockchain Storage Management\nPioneering blockchain systems, such as Bitcoin [38] and\nEthereum [52], use MPT and store it using simple key-value\nstorage like RocksDB [18], which implements the LSM-tree\nstructure. While many works propose to optimize the generic\nLSM-tree for high throughput and low latency [12 –14,46,60],\nand some propose orthogonal designs that could potentially\nbe incorporated into COLE, they are not specifically designed\nto meet the unique integrity and provenance requirements\nof blockchain systems. On the other hand, a large body\nof research has been carried out to study alternative solu-\ntions to reduce blockchain storage overhead. Several stud-\nies [11, 19, 24, 25, 62] consider using sharding techniques to\nhorizontally partition the blockchain storage and each par-\ntition is maintained by a subset of nodes, thus reducing the\noverall storage overhead. Distributed data storage [43, 59] or\nmoving on-chain states to off-chain nodes [6, 8, 48, 56, 58]\nhas also been proposed to reduce each blockchain node’s stor-\nage overhead. Besides, ForkBase [50] proposes to optimize\nblockchain storage by deduplicating multi-versioned data and\nsupporting efficient fork operations. [28] employs a vector\ncommitment protocol and multi-level authenticated trees to\nreduce I/O costs for blockchain storage. To the best of our\nknowledge, COLE is the first work that targets the index itself\nto address the blockchain storage overhead.\nAnother related topic is to support efficient queries in\nblockchain systems. LineageChain [44] focuses on prove-nance queries in the blockchain. Verifiable boolean range\nqueries are studied in vChain and vChain+ [49, 55], where\naccumulator-based authenticated data structures are designed.\nGEM2-tree [63] explores query processing in the context\nof on-chain/off-chain hybrid storage. FalconDB [42] com-\nbines the blockchain and the collaborative database to support\nSQL queries with a strong security guarantee. [57] studies\nthe authenticated spatial and keyword queries in blockchain\ndatabases. iQuery [35] supports intelligent blockchain analyti-\ncal queries and guarantees the trustworthiness of query results\nby using multiple service providers. While all these works fo-\ncus on proposing additional data structures to process specific\nqueries, COLE focuses on improving the performance of the\ngeneral blockchain storage system.\n10 Conclusion\nIn this paper, we have designed COLE, a novel column-based\nlearned storage for blockchain systems. COLE follows the\ncolumn-based database design to contiguously store each\nstate’s historical values using an LSM-tree approach. Within\neach run of the LSM-tree, a disk-optimized learned index has\nbeen designed to facilitate efficient data retrieval and prove-\nnance queries. Moreover, a streaming algorithm has been\nproposed to construct Merkle files that are used to ensure\nblockchain data integrity. In addition, a new checkpoint-based\nasynchronous merge strategy has been proposed to tackle the\nlong-tail latency issue for data writes in COLE. Extensive\nexperiments show that, compared with the existing systems,\nthe proposed COLE system reduces the storage size by up\nto 94% and improves the system throughput by 1.4×-5.4×.\nAdditionally, the proposed asynchronous merge decreases the\nlong-tail latency by 1-2 orders of magnitude while maintain-\ning a comparable storage size.\nFor future work, we plan to extend COLE to support\nblockchain systems that undergo forking, where the states of\na forked block can be rewound. We will investigate efficient\nstrategies to remove the rewound states from storage. Fur-\nthermore, since the column-based design stores blockchain\nstates contiguously, compression techniques can be applied\nto take advantage of similarities between adjacent data. We\nwill study how to incorporate compression strategies into the\nlearned index.\nAcknowledgments\nThis work is supported by Hong Kong RGC Grants (Project\nNo. 12200022, 12201520, C2004-21GF). Jianliang Xu is the\ncorresponding author.\n13\n\nReferences\n[1]Ethereum full node sync (archive) chart. https://\netherscan.io/chartsync/chainarchive , 2023.\n[2]Ethereum virtual machine. https://github.com/\nrust-blockchain/evm , 2023.\n[3] rug library. https://docs.rs/rug , 2023.\n[4]Daniel J Abadi, Peter A Boncz, and Stavros Harizopou-\nlos. Column-oriented database systems. Proceedings of\nthe VLDB Endowment , pages 1664–1665, 2009.\n[5]Elli Androulaki, Artem Barger, Vita Bortnikov, Chris-\ntian Cachin, Konstantinos Christidis, Angelo De Caro,\nDavid Enyeart, Christopher Ferris, Gennady Laventman,\nYacov Manevich, et al. Hyperledger fabric: a distributed\noperating system for permissioned blockchains. In Pro-\nceedings of the thirteenth EuroSys conference , pages\n1–15, 2018.\n[6]Dan Boneh, Benedikt Bünz, and Ben Fisch. Batching\ntechniques for accumulators with applications to iops\nand stateless blockchains. In Annual International Cryp-\ntology Conference , pages 561–586, 2019.\n[7]Miguel Castro and Barbara Liskov. Practical byzantine\nfault tolerance and proactive recovery. ACM Transac-\ntions on Computer Systems (TOCS) , pages 398–461,\n2002.\n[8]Alexander Chepurnoy, Charalampos Papamanthou,\nShravan Srinivasan, and Yupeng Zhang. Edrax: A cryp-\ntocurrency with stateless transaction validation. Cryp-\ntology ePrint Archive , 2018.\n[9]Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu\nRamakrishnan, and Russell Sears. Benchmarking cloud\nserving systems with ycsb. In Proceedings of the 1st\nACM symposium on Cloud computing , pages 143–154,\n2010.\n[10] Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan\nAlagappan, Brian Kroth, Andrea Arpaci-Dusseau, and\nRemzi Arpaci-Dusseau. From WiscKey to bourbon: A\nlearned index for Log-Structured merge trees. In OSDI ,\npages 155–171, 2020.\n[11] Hung Dang, Tien Tuan Anh Dinh, Dumitrel Loghin, Ee-\nChien Chang, Qian Lin, and Beng Chin Ooi. Towards\nscaling blockchain systems via sharding. In Proceedings\nof the 2019 international conference on management of\ndata, pages 123–140, 2019.\n[12] Niv Dayan, Manos Athanassoulis, and Stratos Idreos.\nMonkey: Optimal navigable key-value store. In ACM\nSIGMOD , pages 79–94, New York, NY , USA, 2017.[13] Niv Dayan and Stratos Idreos. Dostoevsky: Better space-\ntime trade-offs for lsm-tree based key-value stores via\nadaptive removal of superfluous merging. In ACM SIG-\nMOD , pages 505–520, 2018.\n[14] Niv Dayan, Tamar Weiss, Shmuel Dashevsky, Michael\nPan, Edward Bortnikov, and Moshe Twitto. Spooky:\ngranulating lsm-tree compactions correctly. PVLDB ,\npages 3071–3084, 2022.\n[15] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang,\nJaeyoung Do, Yinan Li, Hantian Zhang, Badrish Chan-\ndramouli, Johannes Gehrke, Donald Kossmann, et al.\nALEX: an updatable adaptive learned index. In ACM\nSIGMOD , pages 969–984, 2020.\n[16] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and\nTim Kraska. Tsunami: A learned multi-dimensional in-\ndex for correlated data and skewed workloads. PVLDB ,\npage 74–86, 2020.\n[17] Tien Tuan Anh Dinh, Ji Wang, Gang Chen, Rui Liu,\nBeng Chin Ooi, and Kian-Lee Tan. Blockbench: A\nframework for analyzing private blockchains. In ACM\nSIGMOD , pages 1085–1100, 2017.\n[18] Siying Dong, Andrew Kryczka, Yanqin Jin, and Michael\nStumm. Rocksdb: Evolution of development priorities\nin a key-value store serving large-scale applications.\nACM Trans. Storage , pages 1–32, 2021.\n[19] Muhammad El-Hindi, Carsten Binnig, Arvind\nArasu, Donald Kossmann, and Ravi Ramamurthy.\nBlockchainDB: A shared database on blockchains.\nPVLDB , pages 1597–1609, 2019.\n[20] Paolo Ferragina and Giorgio Vinciguerra. The PGM-\nindex: a fully-dynamic compressed learned index with\nprovable worst-case bounds. PVLDB , pages 1162–1175,\n2020.\n[21] Alex Galakatos, Michael Markovitch, Carsten Binnig,\nRodrigo Fonseca, and Tim Kraska. Fiting-tree: A data-\naware index structure. In ACM SIGMOD , pages 1189–\n1206, 2019.\n[22] Yossi Gilad, Rotem Hemo, Silvio Micali, Georgios Vla-\nchos, and Nickolai Zeldovich. Algorand: Scaling byzan-\ntine agreements for cryptocurrencies. In Proceedings\nof the 26th symposium on operating systems principles ,\npages 51–68, 2017.\n[23] Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng\nWang, and Sheng Wang. The rlr-tree: A reinforcement\nlearning based r-tree for spatial data. In ACM SIGMOD ,\n2023.\n14\n\n[24] Suyash Gupta, Sajjad Rahnama, Jelle Hellings, and Mo-\nhammad Sadoghi. ResilientDB: Global scale resilient\nblockchain fabric. PVLDB , page 868–883, 2020.\n[25] Zicong Hong, Song Guo, Enyuan Zhou, Wuhui Chen,\nHuawei Huang, and Albert Zomaya. Gridb: Scaling\nblockchain database via sharding and off-chain cross-\nshard mechanism. PVLDB , pages 1685–1698, 2023.\n[26] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and\nNeoklis Polyzotis. The case for learned index structures.\nInACM SIGMOD , pages 489–504, 2018.\n[27] Hai Lan, Zhifeng Bao, J Shane Culpepper, and Renata\nBorovica-Gajic. Updatable learned indexes meet disk-\nresident dbms-from evaluations to design choices. Pro-\nceedings of the ACM on Management of Data , pages\n1–22, 2023.\n[28] Chenxing Li, Sidi Mohamed Beillahi, Guang Yang,\nMing Wu, Wei Xu, and Fan Long. {LVMT }: An ef-\nficient authenticated storage for blockchain. In 17th\nUSENIX Symposium on Operating Systems Design and\nImplementation (OSDI 23) , pages 135–153, 2023.\n[29] Feifei Li, Marios Hadjieleftheriou, George Kollios, and\nLeonid Reyzin. Dynamic authenticated index structures\nfor outsourced databases. In ACM SIGMOD , pages 121–\n132, 2006.\n[30] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo.\nFINEdex: a fine-grained learned index scheme for scal-\nable and concurrent memory systems. PVLDB , pages\n321–334, 2021.\n[31] Pengfei Li, Yu Hua, Pengfei Zuo, Zhangyu Chen, and\nJiajie Sheng. ROLEX: A scalable RDMA-oriented\nlearned Key-Value store for disaggregated memory sys-\ntems. In 21st USENIX Conference on File and Storage\nTechnologies (FAST 23) , pages 99–114, 2023.\n[32] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang\nPan. LISA: A learned index structure for spatial data.\nInACM SIGMOD , pages 2119–2133, 2020.\n[33] Yinan Li, Bingsheng He, Qiong Luo, and Ke Yi. Tree\nindexing on flash disks. In IEEE ICDE , pages 1303–\n1306, 2009.\n[34] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Min-\nhas, and Tianzheng Wang. Apex: a high-performance\nlearned index on persistent memory. PVLDB , pages\n597–610, 2021.\n[35] Lingling Lu, Zhenyu Wen, Ye Yuan, Binru Dai, Peng\nQian, Changting Lin, Qinming He, Zhenguang Liu, Jian-\nhai Chen, and Rajiv Ranjan. Iquery: A trustworthy and\nscalable blockchain analytics platform. IEEE Transac-\ntions on Dependable and Secure Computing , 2022.[36] Raghav Mehra, Nirmal Lodhi, and Ram Babu. Column\nbased nosql database, scope and future. International\nJournal of Research and Analytical Reviews , pages 105–\n113, 2015.\n[37] Ralph C Merkle. A certified digital signature. In Con-\nference on the Theory and Application of Cryptology ,\npages 218–238, 1989.\n[38] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic\ncash system. Decentralized Business Review , page\n21260, 2008.\n[39] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and\nTim Kraska. Learning multi-dimensional indexes. In\nACM SIGMOD , pages 985–1000, 2020.\n[40] Joseph O’Rourke. An on-line algorithm for fitting\nstraight lines between data ranges. Communications\nof the ACM , pages 574–578, 1981.\n[41] Patrick O’Neil, Edward Cheng, Dieter Gawlick, and Eliz-\nabeth O’Neil. The log-structured merge-tree (lsm-tree).\nActa Informatica , pages 351–385, 1996.\n[42] Yanqing Peng, Min Du, Feifei Li, Raymond Cheng, and\nDawn Song. Falcondb: Blockchain-based collaborative\ndatabase. In ACM SIGMOD , pages 637–652, 2020.\n[43] Xiaodong Qi, Zhao Zhang, Cheqing Jin, and Aoying\nZhou. Bft-store: Storage partition for permissioned\nblockchain via erasure coding. In IEEE ICDE , pages\n1926–1929, 2020.\n[44] Pingcheng Ruan, Gang Chen, Tien Tuan Anh Dinh, Qian\nLin, Beng Chin Ooi, and Meihui Zhang. Fine-grained,\nsecure and efficient data provenance on blockchain sys-\ntems. PVLDB , pages 975–988, 2019.\n[45] Fahad Saleh. Blockchain without waste: Proof-of-stake.\nSSRN Electronic Journal , 2018.\n[46] Subhadeep Sarkar, Dimitris Staratzis, Zichen Zhu, and\nManos Athanassoulis. Constructing and analyzing the\nlsm compaction design space. PVLDB , pages 2216–\n2229, 2021.\n[47] Yufan Sheng, Xin Cao, Yixiang Fang, Kaiqi Zhao,\nJianzhong Qi, Gao Cong, and Wenjie Zhang. Wisk:\nA workload-aware learned index for spatial keyword\nqueries. ACM SIGMOD , pages 1–27, 2023.\n[48] Alin Tomescu, Ittai Abraham, Vitalik Buterin, Justin\nDrake, Dankrad Feist, and Dmitry Khovratovich. Ag-\ngregatable subvector commitments for stateless cryp-\ntocurrencies. In International Conference on Security\nand Cryptography for Networks , pages 45–64, 2020.\n15\n\n[49] Haixin Wang, Cheng Xu, Ce Zhang, Jianliang Xu, Zhe\nPeng, and Jian Pei. vChain+: Optimizing verifiable\nblockchain boolean range queries. In IEEE ICDE , pages\n1927–1940, 2022.\n[50] Sheng Wang, Tien Tuan Anh Dinh, Qian Lin, Zhon-\ngle Xie, Meihui Zhang, Qingchao Cai, Gang Chen,\nBeng Chin Ooi, and Pingcheng Ruan. Forkbase: an\nefficient storage engine for blockchain and forkable ap-\nplications. PVLDB , pages 1137–1150, 2018.\n[51] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo\nChen. SIndex: a scalable learned index for string keys.\nInProceedings of the 11th ACM SIGOPS Asia-Pacific\nWorkshop on Systems , pages 17–24, 2020.\n[52] Gavin Wood. Ethereum: A secure decentralised gener-\nalised transaction ledger, 2014.\n[53] Gavin Wood. Polkadot: Vision for a heterogeneous\nmulti-chain framework. White Paper , pages 2327–4662,\n2016.\n[54] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang,\nYu Chen, and Chunxiao Xing. Updatable learned in-\ndex with precise positions. PVLDB , pages 1276–1288,\n2021.\n[55] Cheng Xu, Ce Zhang, and Jianliang Xu. vChain: En-\nabling verifiable boolean range queries over blockchain\ndatabases. In ACM SIGMOD , pages 141–158, 2019.\n[56] Cheng Xu, Ce Zhang, Jianliang Xu, and Jian Pei. Slim-\nChain: scaling blockchain transactions through off-\nchain storage and parallel processing. PVLDB , pages\n2314–2326, 2021.\n[57] Hao Xu, Bin Xiao, Xiulong Liu, Li Wang, Shan Jiang,\nWeilian Xue, Jianrong Wang, and Keqiu Li. Empower-\ning authenticated and efficient queries for stk transaction-\nbased blockchains. IEEE Transactions on Computers ,\n2023.\n[58] Zihuan Xu and Lei Chen. L2chain: Towards high-\nperformance, confidential and secure layer-2 blockchain\nsolution for decentralized applications. PVLDB , pages\n986–999, 2022.\n[59] Zihuan Xu, Siyuan Han, and Lei Chen. Cub, a consensus\nunit-based storage scheme for blockchain system. In\nIEEE ICDE , pages 173–184, 2018.\n[60] Jinghuan Yu, Sam H Noh, Young-ri Choi, and Chun Ja-\nson Xue. ADOC: Automatically harmonizing dataflow\nbetween components in Log-StructuredKey-Value\nstores for improved performance. In 21st USENIX Con-\nference on File and Storage Technologies (FAST 23) ,\npages 65–80, 2023.Algorithm 8: Provenance Query\n1Function ProvQuery( addr,[blkl,blku])\nInput: State address addr , block height range [blkl,blku]\nOutput: Result set R, proof π\n2Kl← ⟨addr,blkl−1⟩;Ku← ⟨addr,blku+1⟩;\n3foreach gin{L0’s writing group, L0’s merging group }do\n4 ⟨R′,π′⟩ ←SearchMBTree (g,[Kl,Ku]);\n5 R.add(R′);π.add(π′);\n6 ifmin({r.blk|r∈R′})<blklthen\n7 π.add(remaining of root_hash_list );\n8 return ⟨R,π⟩;\n9foreach level i in{1,2,...}do\n10 RS←{Ri,j|Ri,j∈\nLi’s writing group ∧Ri,jis committed };\n11 RS←RS+{Ri,j|Ri,j∈Li’s merging group };\n12 foreach Ri,jinRSdo\n13 ⟨⟨K′,state′⟩,posl⟩ ←SearchRun (Ri,j,Kl);\n14 posu←posl;\n15 while FV[posu].k≤Kudo\n16 R.add(FV[posu]);\n17 posu←posu+1;\n18 π.add(MHT proof w.r.t. poslto pos u);\n19 ifK′.blk<blklthen\n20 π.add(remaining of root_hash_list );\n21 return ⟨R,π⟩;\n22 return ⟨R,π⟩;\n[61] Tong Yu, Guanfeng Liu, An Liu, Zhixu Li, and Lei Zhao.\nLIFOSS: a learned index scheme for streaming scenar-\nios.World Wide Web , pages 1–18, 2022.\n[62] Mahdi Zamani, Mahnush Movahedi, and Mariana\nRaykova. Rapidchain: Scaling blockchain via full shard-\ning. In ACM CCS , pages 931–948, 2018.\n[63] Ce Zhang, Cheng Xu, Jianliang Xu, Yuzhe Tang, and\nByron Choi. GEM2-Tree: A gas-efficient structure for\nauthenticated range queries in blockchain. In IEEE\nICDE , pages 842–853, 2019.\n[64] Zhou Zhang, Zhaole Chu, Peiquan Jin, Yongping Luo,\nXike Xie, Shouhong Wan, Yun Luo, Xufei Wu, Peng\nZou, Chunyang Zheng, et al. PLIN: a persistent learned\nindex for non-volatile memory with high performance\nand instant recovery. PVLDB , pages 243–255, 2022.\nA Appendix\nA.1 Provenance Query Algorithm\nAlgorithm 8 shows the procedure of the provenance query.\nFirst, we compute two boundary compound keys Kl=\n⟨addr,blkl−1⟩,Ku=⟨addr,blku+1⟩(Line 2). The offsets\nby one are needed to ensure that no valid results will be miss-\ning. Then, similar to the get query, we traverse both MB-trees\ninL0to find the results in the query range (Line 4). At the\n16\n\n 0 20 40 60 80 100 120\n2 4 816 32 64CPU Time (us)\nMHT FanoutCOLE COLE*\n 0 10 20 30\n2 4 816 32 64Proof Size (KB)\nMHT FanoutCOLE COLE*Figure 15: Impact of COLE’s MHT Fanout\nsame time, the corresponding MB-tree paths are added to π\nas the Merkle proof (Line 5). If we find a state whose block\nheight is smaller than blkl, we stop the search since all states\nin the following levels must be even older than blkl(Lines 6\nto 8). Otherwise, we continue to search the on-disk runs in\nthe same order as those described in Algorithm 6. We use Kl\nas the search key when applying the learned models to find\nthe first query result in each run (Line 13). Afterwards, we\nsequentially scan the value file until the state is outside of the\nquery range based on Ku(Lines 14 to 17). A Merkle proof is\ncomputed accordingly based on the position of the first and\nthe last results in the value file of this run. This proof is added\ntoπ(Line 18). Similar to the in-memory level, we apply an\nearly stop when we find a state’s block height is smaller than\nblkl(Line 19). Finally, the root hashes of the unsearched runs\ninroot_hash_list are added to π(Line 20).\nA.1.1 Impact of COLE’S MHT Fanout\nFigure 15 shows the CPU time and proof size under 105block\nheight and q=16when varying COLE’s MHT fanout m. We\nobserve a U-shaped trend for both the CPU time and proof\nsize with the increasing fanout. The reason is that as the fanout\nincreases, the MHT height decreases, resulting in shorter CPU\ntime and smaller proof size. However, the size of each node\nof MHT increases, which may lead to longer CPU time and\nlarger proof size (as shown in Table 1). We find that setting\nm=4 yields the best results for both COLE and COLE*.\n17",
  "textLength": 79283
}