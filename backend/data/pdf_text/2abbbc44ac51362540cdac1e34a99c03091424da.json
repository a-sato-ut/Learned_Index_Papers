{
  "paperId": "2abbbc44ac51362540cdac1e34a99c03091424da",
  "title": "Triangle and Four Cycle Counting with Predictions in Graph Streams",
  "pdfPath": "2abbbc44ac51362540cdac1e34a99c03091424da.pdf",
  "text": "Published as a conference paper at ICLR 2022\nTRIANGLE AND FOUR CYCLE COUNTING WITH PRE-\nDICTIONS IN GRAPH STREAMS\nJustin Y. Chen, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, and Sandeep Silwal\u0003\nComputer Science and Artiﬁcial Intelligence Laboratory\nMassachusetts Institute of Technology\nCambridge, MA 02139, USA\nfjustc, indyk, shyamsn, ronitt, silwal g@mit.edu\nHonghao Lin, David P. Woodruff, Michael Zhang\nComputer Science Department\nCarnegie Mellon University\nPittsburgh, PA 15213, USA\nfhonghaol, dwoodruf, jinyaoz g@andrew.cmu.edu\nTal Wagner\nMicrosoft Research\nRedmond, WA 98052, USA\ntal.wagner@gmail.comTalya Eden\nMassachusetts Institute of Technology and Boston University\nCambridge, MA 02139, USA\nteden@mit.edu\nABSTRACT\nWe propose data-driven one-pass streaming algorithms for estimating the number\nof triangles and four cycles, two fundamental problems in graph analytics that are\nwidely studied in the graph data stream literature. Recently, Hsu et al. (2019a)\nand Jiang et al. (2020) applied machine learning techniques in other data stream\nproblems, using a trained oracle that can predict certain properties of the stream\nelements to improve on prior “classical” algorithms that did not use oracles. In\nthis paper, we explore the power of a “heavy edge” oracle in multiple graph edge\nstreaming models. In the adjacency list model, we present a one-pass triangle\ncounting algorithm improving upon the previous space upper bounds without such\nan oracle. In the arbitrary order model, we present algorithms for both triangle and\nfour cycle estimation with fewer passes and the same space complexity as in previ-\nous algorithms, and we show several of these bounds are optimal. We analyze our\nalgorithms under several noise models, showing that the algorithms perform well\neven when the oracle errs. Our methodology expands upon prior work on “clas-\nsical” streaming algorithms, as previous multi-pass and random order streaming\nalgorithms can be seen as special cases of our algorithms, where the ﬁrst pass or\nrandom order was used to implement the heavy edge oracle. Lastly, our experi-\nments demonstrate advantages of the proposed method compared to state-of-the-\nart streaming algorithms.\n1 I NTRODUCTION\nCounting the number of cycles in a graph is a fundamental problem in the graph stream model\n(e.g., Atserias et al. (2008); Bera & Chakrabarti (2017); Seshadhri et al. (2013); Kolountzakis et al.\n(2010); Bar-Yossef et al. (2002); Kallaugher et al. (2019)). The special case of counting triangles\nis widely studied, as it has a vast range of applications. In particular, it provides important insights\ninto the structural properties of networks (Prat-P ´erez et al., 2012; Farkas et al., 2011), and is used\nto discover motifs in protein interaction networks (Milo et al., 2002), understand social networks\n(Foucault Welles et al., 2010), and evaluate large graph models (Leskovec et al., 2008). See Al Hasan\n& Dave (2018) for a survey of these and other applications.\n\u0003All authors contributed equally.\n1arXiv:2203.09572v1  [cs.DS]  17 Mar 2022\n\nPublished as a conference paper at ICLR 2022\nBecause of its importance, a large body of research has been devoted to space-efﬁcient streaming\nalgorithms for (1 +\u000f)-approximate triangle counting. Such algorithms perform computation in one\nor few passes over the data using only a sub-linear amount of space. A common difﬁculty which\narises in all previous works is the existence of heavy edges , i.e., edges that are incident to many\ntriangles (four cycles). As sublinear space algorithms often rely on sampling of edges, and since a\nsingle heavy edge can greatly affect the number of triangles (four cycles) in a graph, sampling and\nstoring these edges are often the key to an accurate estimation. Therefore, multiple techniques have\nbeen developed to determine whether a given edge is heavy or not.\nRecently, based on the observation that many underlying patterns in real-world data sets do not\nchange quickly over time, machine learning techniques have been incorporated into the data stream\nmodel via the training of heavy-hitter oracles. Given access to such a learning-based oracle, a wide\nrange of signiﬁcant problems in data stream processing — including frequency estimation, esti-\nmating the number of distinct elements, Fp-Moments or (k;p)-Cascaded Norms — can all achieve\nspace bounds that are better than those provided by “classical” algorithms, see, e.g., Hsu et al.\n(2019a); Cohen et al. (2020); Jiang et al. (2020); Eden et al. (2021); Du et al. (2021). More gen-\nerally, learning-based approaches have had wide success in other algorithmic tasks, such as data\nstructures (Kraska et al., 2018; Ferragina et al., 2020; Mitzenmacher, 2018; Rae et al., 2019; Vaidya\net al., 2021), online algorithms (Lykouris & Vassilvtiskii, 2018; Purohit et al., 2018; Gollapudi &\nPanigrahi, 2019; Rohatgi, 2020; Wei, 2020; Mitzenmacher, 2020; Lattanzi et al., 2020; Bamas et al.,\n2020), similarity search (Wang et al., 2016; Dong et al., 2020) and combinatorial optimization (Dai\net al., 2017; Balcan et al., 2017; 2018a;b; 2019). See the survey and references therein for additional\nworks (Mitzenmacher & Vassilvitskii, 2020).\nInspired by these recent advancements, we ask: is it possible to utilize a learned heavy edge oracle\nto improve the space complexity of subgraph counting in the graph stream model? Our results\ndemonstrate that the answer is yes.\n1.1 O URRESULTS AND COMPARISON TO PREVIOUS THEORETICAL WORKS\nWe present theoretical and empirical results in several graph streaming models, and with several\nnotions of prediction oracles. Conceptually, it is useful to begin by studying perfect oracles that\nprovide exact predictions. While instructive theoretically, such oracles are typically not available in\npractice. We then extend our theoretical results to noisy oracles that can provide inaccurate or wrong\npredictions. We validate the practicality of such oracles in two ways: by directly showing they can\nbe constructed for multiple real datasets, and by showing that on those datasets, our algorithms attain\nsigniﬁcant empirical improvements over baselines, when given access to these oracles.\nWe proceed to a precise account of our results. Let G= (V;E)denote the input graph, and let n,\nm, andTdenote the number of vertices, edges, and triangles (or four-cycles) in G, respectively.\nThere are two major graph edge streaming models: the adjacency list model and the arbitrary order\nmodel. We show that training heavy edge oracles is possible in practice in both models, and that such\noracles make it possible to design new algorithms that signiﬁcantly improve the space complexity of\ntriangle and four-cycle counting, both in theory and in practice. Furthermore, our formalization of\na heavy edge prediction framework makes it possible to show provable lower bounds as well. Our\nresults are summarized in Table 1.\nIn our algorithms, we assume that we know a large-constant approximation of Tfor the purposes of\nsetting various parameters. This is standard practice in the subgraph counting streaming literature\n(e.g., see (Braverman et al., 2013, Section 1), (McGregor et al., 2016, Section 1.2) for an extensive\ndiscussions on this assumption). Moreover, when this assumption cannot be directly carried over in\npractice, in Subsection F.3 we discuss how to adapt our algorithms to overcome this issue.\n1.1.1 P ERFECT ORACLE\nOur ﬁrst results apply for the case that the algorithms are given access to a perfect heavy edge oracle.\nThat is, for some threshold \u001a, the oracle perfectly predicts whether or not a given edge is incident to\nat least\u001atriangles (four cycles). We describe how to relax this assumption in Section 1.1.2.\nAdjacency List Model All edges incident to the same node arrive together. We show:\n2\n\nPublished as a conference paper at ICLR 2022\nTable 1: Our results compared to existing theoretical algorithms. \u0001E(\u0001V) denotes the maximum\nnumber of triangles incident to any edge (vertex), and \u0014denotes the arboricity of the graph.\nProblem Previous Results (no oracle) Our Results\nTriangle,\nAdjacencyeO(\u000f\u00002m=p\nT), 1-pass (McGregor et al., 2016)eO(min(\u000f\u00002m2=3=T1=3;\n\u000f\u00001m1=2)), 1-pass\nTriangle,\nArbitraryeO(\u000f\u00002m3=2=T), 3-pass (McGregor et al., 2016)\neO(\u000f\u00002m=p\nT), 2-pass (McGregor et al., 2016)\neO(\u000f\u00002(m=p\nT+m\u0001E=T)), 1-pass\n(Pagh & Tsourakakis, 2012) O(\u000f\u00001(m=p\nT+m1=2)),\neO(\u000f\u00002(m=T2=3+m\u0001E=T+mp\u0001V=T)), 2-pass 1-pass\n(Kallaugher & Price, 2017)\neO(poly(\u000f\u00001)(m\u0001E=T+mp\u0001V=T)), 1-pass\n(Jayaram & Kallaugher, 2021)\neO(poly(\u000f\u00001)m\u0014=T ), multi-pass (Bera & Sheshadhri, 2020)\n4-cycle,\nArbitraryeO(\u000f\u00002m=T1=4), 3-pass (McGregor & V orotnikova, 2020) eO(T1=3+\u000f\u00002m=T1=3),\neO(\u000f\u00002m=T1=3), 3-pass (V orotnikova, 2020) 1-pass\nTheorem 1.1. There exists a one-pass algorithm, Algorithm 1, with space complexity1\neO(min(\u000f\u00002m2=3=T1=3;\u000f\u00001m1=2))in the adjacency list model that, using a learning-based ora-\ncle, returns a (1\u0006\u000f)-approximation to the number Tof triangles with probability at least27=10.\nAn overview of Algorithm 1 is given in Section 2, and the full analysis is provided in Appendix B.\nArbitrary Order Model In this model, the edges arrive in the stream in an arbitrary order. We\npresent a one-pass algorithm for triangle counting and another one-pass algorithm for four cycle\ncounting in this model, both reducing the number of passes compared to the currently best known\nspace complexity algorithms. Our next result is as follows:\nTheorem 1.2. There exists a one-pass algorithm, Algorithm 4, with space complexity\neO(\u000f\u00001(m=p\nT+pm))in the arbitrary order model that, using a learning-based oracle, returns a\n(1\u0006\u000f)-approximation to the number Tof triangles with probability at least 7=10.\nAn overview of Algorithm 4 is given in Section 3, and full details are provided in Appendix C.\nWe also show non-trivial space lower bounds that hold even if appropriate predictors are available.\nIn Theorem C.2 in Appendix C.3, we provide a lower bound for this setting by giving a construction\nthat requires \n(min(m=p\nT;m3=2=T))space even with the help of an oracle, proving that our result\nis nearly tight in some regimes. Therefore, the triangle counting problem remains non-trivial even\nwhen extra information is available.\nFour Cycle Counting. For four cycle counting in the arbitrary order model, we give Theorem 1.3\nwhich is proven in Appendix D.\nTheorem 1.3. There exists a one-pass algorithm, Algorithm 5, with space complexity eO(T1=3+\n\u000f\u00002m=T1=3)in the arbitrary order model that, using a learning-based oracle, returns a (1\u0006\u000f)-\napproximation to the number Tof four cycles with probability at least 7=10.\nTo summarize our theoretical contributions, for the ﬁrst set of results of counting triangles in the\nadjacency list model, our bounds always improve on the previous state of the art due to McGregor\net al. (2016) for all values of mandT. For a concrete example, consider the case that T= \u0002(pm).\n1We use eO(f)to denoteO(f\u0001polylog(f)).\n2The success probability can be 1\u0000\u000eby running log(1=\u000e)copies of the algorithm and taking the median.\n3\n\nPublished as a conference paper at ICLR 2022\nIn this setting previous bounds result in an eO(m3=4)-space algorithm, while our algorithm only\nrequireseO(pm)space (for constant \u000f).\nFor the other two problems of counting triangles and 4-cycles in the arbitrary arrival model, our\nspace bounds have an additional additive term compared to McGregor et al. (2016) (for triangles)\nand V orotnikova (2020) (for 4-cycles) but importantly run in a single pass rather than multiple\npasses. In the case where the input graph has high triangles density, T= \n(m=\u000f2), our space\nbound is worse due to the additive factor. When T=O(m=\u000f2), our results achieve the same\ndependence on mandTas that of the previous algorithms with an improved dependency in \u000f.\nMoreover, the case T\u0014m=\u000f2is natural for many real world datasets: for \u000f= 0:05, this condition\nholds for all of the datasets in our experimental results (see Table 2). Regardless of the triangle\ndensity, a key beneﬁt of our results is that they are achieved in a single pass rather than multiple\npasses. Finally, our results are for general graphs, and make no assumptions on the input graph\n(unlike Pagh & Tsourakakis (2012); Kallaugher & Price (2017); Bera & Sheshadhri (2020)). Most\nof our algorithms are relatively simple and easy to implement and deploy. At the same time, some of\nour results require the use of novel techniques in this context, such as the use of exponential random\nvariables (see Section 2).\n1.1.2 N OISY ORACLES\nThe aforementioned triangle counting results are stated under the assumption that the algorithms are\ngiven access to perfect heavy edge oracles. In practice, this assumption is sometimes unrealistic.\nHence, we consider several types of noisy oracles. The ﬁrst such oracle, which we refer to as a\nK-noisy oracle, is deﬁned below (see Figure 3 in the Supplementary Section C.2).\nDeﬁnition 1.1. For an edge e=xyin the stream, deﬁne Neas the number of triangles that contain\nbothxandy. For a ﬁxed constant K\u00151and for a threshold \u001awe say that an oracle O\u001ais a\nK-noisy oracle if for every edge e,1\u0000K\u0001\u001a\nNe\u0014Pr[O\u001a(e) = HEAVY ]\u0014K\u0001Ne\n\u001a.\nThis oracle ensures that if an edge is extremely heavy or extremely light, it is classiﬁed correctly\nwith high probability, but if the edge is close to the threshold, the oracle may be inaccurate. We\nfurther discuss the properties of this oracle in Section G.\nFor this oracle, we prove the following two theorems. First, in the adjacency list model, we prove:\nTheorem 1.4. Suppose that the oracle given to Algorithm 1 is a K-noisy oracle as deﬁned in\nDeﬁnition 1.1. Then with probability 2=3, Algorithm 1 returns a value in (1\u0006p\nK\u0001\u000f)T, and\nuses space at most eO(min(\u000f\u00002m2=3=T1=3;K\u0001\u000f\u00001m1=2)).\nHence, even if our oracle is inaccurate for edges near the threshold, our algorithm still obtains an\neffective approximation with low space in the adjacency list model. Likewise, for the arbitrary order\nmodel, we prove in Theorem C.1 that the O(\u000f\u00001(m=p\nT+pm))1-pass algorithm of Theorem 1.2\nalso works when Algorithm 4 is only given access to a K-noisy oracle.\nThe proof of Theorem 1.4 is provided in Appendix B.1, and the proof of Theorem C.1 is provided\nin Appendix C.2. We remark that Theorems 1.4 and C.1 automatically imply Theorems 1.1 and 1.2,\nsince the perfect oracle is automatically a K-noisy oracle.\n(Noisy) Value Oracles In the adjacency list model, when we see an edge xy, we also have access\nto all the neighbors of either xory, which makes it possible for the oracle to give a more accurate\nprediction. For an edge xy, letRxydenote the number of triangles fx;z;ygso thatxprecedesz\nandzprecedesyin the stream arrival order. Formally, Rxy=jz:fx;y;zg2\u0001andx<sz <syj\nwherex<sydenotes that the adjacency list of xarrives before that of yin the stream.\nMotivated by our empirical results in Section F.7, it is reasonable in some settings to assume we\nhave access to oracles that can predict a good approximation to Rxy. We refer to such oracles as\nvalue oracles .\nIn the ﬁrst version of this oracle, we assume that the probability of approximation error decays\nlinearly with the error from above but exponentially with the error from below.\nDeﬁnition 1.2. Given an edge e, an(\u000b;\f)value-prediction oracle outputs a random value p(e)\nwhere E[p(e)]\u0014\u000bRe+\f, andPr[p(e)<Re\n\u0015\u0000\f]\u0014Ke\u0000\u0015for some constant Kand any\u0015\u00151.\n4\n\nPublished as a conference paper at ICLR 2022\nFor this variant, we prove the following theorem.\nTheorem 1.5. Given an oracle with parameters (\u000b;\f), there exists a one-pass algorithm, Algo-\nrithm 2, with space complexity O(\u000f\u00002log2(K=\u000f)(\u000b+m\f=T ))in the adjacency list model that\nreturns a (1\u0006\u000f)-approximation to the number of triangles Twith probability at least 7=10.\nIn the second version of this noisy oracle, we assume that the probability of approximation error\ndecays linearly with the error from both above and below. For this variant, we prove that we can\nachieve the same guarantees as Theorem 1.5 up to logarithmic factors (see Theorem B.1). The\nalgorithms and proofs for both Theorem 1.5 and Theorem B.1 appear in Appendix B.2.\nExperiments We conduct experiments to verify our results for triangle counting on a variety of\nreal world networks (see Table 2) in both the arbitrary and adjacency list models. Our algorithms\nuse additional information through predictors to improve empirical performance. The predictors\nare data dependent and include: memorizing heavy edges in a small portion of the ﬁrst graph in a\nsequence of graphs, linear regression, and graph neural networks (GNNs). Our experimental results\nshow that we can achieve up to 5xdecrease in estimation error while keeping the same amount of\nedges as other state of the art empirical algorithms. For more details, see Section 4. In Section F.7,\nwe show that our noisy oracle models are realistic for real datasets.\nRelated Empirical Works On the empirical side, most of the focus has been on triangle counting\nin the arbitrary order model for which there are several algorithms that work well in practice. We\nprimarily focus on two state-of-the-art baselines, ThinkD (Shin et al., 2018) and WRS (Shin, 2017).\nIn these works, the authors compare to previous empirical benchmarks such as the ones given in\nStefani et al. (2017); Han & Sethu (2017); Lim & Kang (2015) and demonstrate that their algorithms\nachieve superior estimates over these benchmarks. There are also other empirical works such as\nAhmed et al. (2017) and Ahmed & Dufﬁeld (2020) studying this model but they do not compare\nto either ThinkD or WRS. While these empirical papers demonstrate that their algorithm returns\nunbiased estimates, their theoretical guarantees on space is incomparable to the previously stated\nspace bounds for theoretical algorithms in Table 1. Nevertheless, we use ThinkD and WRS as part\nof our benchmarks due to their strong practical performance and code accessibility.\nImplicit Predictors in Prior Works The idea of using a predictor is implicit in many prior works.\nThe optimal two pass triangle counting algorithm of McGregor et al. (2016) can be viewed as an\nimplementation of a heavy edge oracle after the ﬁrst pass. This oracle is even stronger than the K-\nnoisy oracle as it is equivalent to an oracle that is always correct on an edge eifNeeither exceeds\nor is under the threshold \u001aby a constant multiplicative factor. This further supports our choice of\noracles in our theoretical results, as a stronger version of our oracle can be implemented using one\nadditional pass through the data stream (see Section G). Similarly, the optimal triangle counting\nstreaming algorithm (assuming a random order) given in McGregor & V orotnikova (2020) also\nimplicitly deﬁnes a heavy edge oracle using a small initial portion of the random stream (see Lemma\n2:2in McGregor & V orotnikova (2020)). The random order assumption allows for the creation of\nsuch an oracle since heavy edges are likely to have many of their incident triangle edges appearing\nin an initial portion of the stream. We view these two prior works as theoretical justiﬁcation for our\noracle deﬁnitions. Lastly, the WRS algorithm also shares the feature of deﬁning an implicit oracle:\nsome space is reserved for keeping the most recent edges while the rest is used to keep a random\nsample of edges. This can be viewed as a speciﬁc variant of our model, where the oracle predicts\nrecent edges as heavy.\nPreliminaries. G= (V;E)denotes the input graph, and n,mandTdenote the number of ver-\ntices, edges and triangles (or four-cycles) in G, respectively. We use N(v)to denote the set of neigh-\nbors of a node v, and \u0001to denote the set of triangles. In triangle counting, for each xy2E(G),\nwe recall that Nxy=jz:fx;y;zg 2 \u0001jis the number of triangles incident to edge xy, and\nRxy=jz:fx;y;zg2\u0001;x <sz <syjis the number of triangles adjacent to xywith the third\nvertexzof the triangle between xandyin the adjacency list order. Table A summarizes the notation.\n2 T RIANGLE COUNTING IN THE ADJACENCY LISTMODEL\nWe describe an algorithm with a heavy edge oracle, and one with a value oracle.\nHeavy Edge Oracle. We present an overview of our one-pass algorithm, Algorithm 1, with a space\ncomplexity of eO(min(\u000f\u00002m2=3=T1=3;\u000f\u00001m1=2)), given in Theorem 1.1. We defer the pseudocode\n5\n\nPublished as a conference paper at ICLR 2022\nand proof of the theorem to Appendix B. The adaptations and proofs for Theorems 1.4, 1.5 and B.1\nfor the various noisy oracles appear in Appendix B.1 and Appendix B.2.\nOur algorithm works differently depending on the value of T. We ﬁrst consider the case that\nT\u0015(m=\u000f)1=2. Assume that for each sampled edge xyin the stream, we can exactly know the\nnumber of triangles Rxythis edge contributes to T. Then the rate at which we would need to sample\neach edge would be proportional to pnaive\u0019\u000f\u00002\u0001E=T, where \u0001Eis the maximum number of tri-\nangles incident to any edge. Hence, our ﬁrst idea is to separately consider light and non-light edges\nusing the heavy edge oracle. This allows us to sample edges that are deemed light by the oracle\nat a lower rate, p1, and compute their contribution by keeping track of Rxyfor each such sampled\nedge. Intuitively, light edges offer us more ﬂexibility and thus we can sample them with a lower\nprobability while ensuring the estimator’s error does not drastically increase. In order to estimate\nthe contribution due to non-light edges, we again partition them into two types: medium and heavy,\naccording to some threshold \u001a. We then use an observation from McGregor et al. (2016), that since\nfor heavy edges Rxy> \u001a, it is sufﬁcient to sample from the entire stream at rate p3\u0019\u000f\u00002=\u001a, in\norder to both detect if some edge xyis heavy and if so to estimate Rxy.\nTherefore, it remains to estimate the contribution to Tdue to medium edges (these are the edges\nthat are deemed non-light by the oracle, and also non-heavy according to the sub-sampling above).\nSince the number of triangles incident to medium edges is higher than that of light ones, we have\nto sample them at some higher rate p2> p1. However, since their number is bounded, this is still\nspace efﬁcient. We get the desired bounds by correctly setting the thresholds between light, medium\nand heavy edges.\nWhenT < (m=\u000f)1=2our algorithm becomes much simpler. We only consider two types of edges,\nlight and heavy, according to some threshold T=\u001a. To estimate the contribution due to heavy edges\nwe simply store them and keep track of their Rxyvalues. To estimate the contribution due to light\nedges we sub-sample them with rate \u000f\u00002\u0001\u0001E=T=\u000f\u00002=\u001a. The total space we use is eO(\u000f\u00001pm),\nwhich is optimal in this case. See Algorithm 1 for more details of the implementation.\nValue-Based Oracle. We also consider the setting where the predictor returns an estimate p(e)of\nRe, and we assume Re\u0014p(e)\u0014\u000b\u0001Re, where\u000b\u00151is an approximation factor. We relax this\nassumption to also handle additive error as well as noise, but for intuition we focus on this case.\nThe value-based oracle setting requires the use of novel techniques, such as the use of exponential\nrandom variables (ERVs). Given this oracle, for an edge e, we compute p(e)=ue, wereueis a\nstandard ERV . We then store the O(\u000blog(1=\u000f))edgesefor whichp(e)=ueis largest. Since we\nare in the adjacency list model, once we start tracking edge e=xy, we can also compute the\ntrue valueReof triangles that the edge eparticipates in (since for each future vertex zwe see,\nwe can check if xandyare both neighbors of z). Note that we track this quantity only for the\nO(\u000blog(1=\u000f))edges that we store. Using the max-stability property of ERVs, maxeRe=ueis equal\nin distribution to T=u, whereuis another ERV . Importantly, using the density function of an ERV ,\none can show that the edge efor whichRe=ueis largest is, with probability 1\u0000O(\u000f3), in our list\nof theO(\u000blog(1=\u000f))largestp(e)=uevalues that we store. Repeating this scheme r=O(1=\u000f2)\ntimes, we obtain independent estimates T=u1;:::;T=ur, whereu1;:::;urare independent ERVs.\nTaking the median of these then gives a (1\u0006\u000f)-approximation to the total number Tof triangles.\nWe note that ERVs are often used in data stream applications (see, e.g., Andoni (2017)), though to\nthe best of our knowledge they have not previously been used in the context of triangle estimation.\nWe also give an alternative algorithm, based on subsampling at O(logn)scales, which has worse\nlogarithmic factors in theory but performs well empirically.\n3 T RIANGLE COUNTING IN THE ARBITRARY ORDER MODEL\nIn this section we discuss Algorithm 4 for estimating the number of triangles in an arbitrary order\nstream of edges. The pseudo-code of the algorithm as well as omitted proofs for the different\noracles are given in Supplementary Section C. Here we give the intuition behind the algorithm. Our\napproach relies on sampling the edges of the stream as they arrive and checking if every new edge\nforms a triangle with the previously sampled edges. However, as previously discussed, this approach\nalone fails if some edges have a large number of triangles incident to them as “overlooking” such\nedges might lead to an underestimation of the number of triangles. Therefore, we utilize a heavy\nedge oracle, and refer to edges that are not heavy as light. Whenever a new edge arrives, we ﬁrst\n6\n\nPublished as a conference paper at ICLR 2022\nquery the oracle to determine if the edge is heavy. If the edge is heavy we keep it, and otherwise\nwe sample it with some probability. As in the adjacency list arrival model case, this strategy allows\nus to reduce the variance of our estimator. By balancing the sampling rate and our threshold for\nheaviness, we ensure that the space requirement is not too high, while simultaneously guaranteeing\nthat our estimate is accurate.\nIn more detail, our algorithm works as follows. First, we set a heaviness threshold \u001a, so that if we\npredict an edge eto be part of \u001aor more triangles, we label it as heavy. We also set a sampling\nparameterp. We letHbe the set of edges predicted to be heavy and let SLbe a random sample of\nedges predicted to be light. Then, we count three types of triangles. The ﬁrst counter, `1, counts the\ntriangles \u0001 = (e1;e2;e), where the ﬁrst two edges seen in this triangle by the algorithm, represented\nbye1ande2, are both in SL. Note that we only count the triangle if e1ande2,were both in SLat\nthe timeearrives in the stream. Similarly, `2counts triangles whose ﬁrst two edges are in SLand\nH(in either order), and `3counts triangles whose ﬁrst two edges are in H. Finally, we return the\nestimate`=`1=p2+`2=p+`3. Note that if the ﬁrst two edges in any triangle are light, they will\nboth be inSLwith probability p2, and if exactly one of the ﬁrst two edges is light, it will be in SL\nwith probability p. Therefore, we divide `1byp2and`2bypso that`is an unbiased estimator.\n4 E XPERIMENTS\nWe now evaluate our algorithm on real and synthetic data whose properties are summarized in\nTable 2 (see Appendix F for more details).\nTable 2: Datasets used in our experiments. Snapshot graphs are a sequence of graphs over time (the\nlength of the sequence is given in parentheses) and temporal graphs are formed by edges appearing\nover time. The listed values for n(number of vertices), m(number of edges), and T(number of\ntriangles) for Oregon and CAIDA are approximated across all graphs. The Oregon and CAIDA\ndatasets come from Leskovec & Krevl (2014); Leskovec et al. (2005), the Wikibooks dataset comes\nfrom Rossi & Ahmed (2015), the Reddit dataset comes from Leskovec & Krevl (2014); Kumar et al.\n(2018), the Twitch dataset comes from Rozemberczki et al. (2019), the Wikipedia dataset comes\nfrom Rossi & Ahmed (2015), and the Powerlaw graphs are sampled from the Chung-Lu-Vu random\ngraph model with expected degree of the i-th vertex proportional to 1=i2(Chung et al., 2003).\nName Type Predictor n m T\nOregon Snapshot (9) 1st graph \u0018104\u00182:2\u0001104\u00181:8\u0001104\nCAIDA 2006 Snapshot (52) 1st graph \u00182:2\u0001104\u00184:5\u0001104\u00183:4\u0001104\nCAIDA 2007 Snapshot (46) 1st graph \u00182:5\u0001104\u00185:1\u0001104\u00183:9\u0001104\nWikibooks Temporal Preﬁx \u00181:3\u0001105\u00183:9\u0001105\u00181:8\u0001105\nReddit Temporal Regression \u00183:6\u0001104\u00181:2\u0001105\u00184:1\u0001105\nTwitch - GNN \u00186:5\u0001103\u00185:7\u0001104\u00185:4\u0001104\nWikipedia - GNN \u00184:8\u0001103\u00184:6\u0001104\u00189:5\u0001104\nPowerlaw Synthetic EV \u00181:7\u0001105\u0018106\u00183:9\u0001107\nWe now describe the edge heaviness predictors that we use (see also Table 2). Our predictors adapt\nto the type of dataset and information available for each dataset. Some datasets we use contain\nonly the graph structure (nodes and edges) without semantic features, thus not enabling us to train a\nclassical machine learning predictor for edge heaviness. In those cases we use the true counts on a\nsmall preﬁx of the data (either 10% of the ﬁrst graph in a sequence of graphs, or a preﬁx of edges\nin a temporal graph) as predicted counts for subsequent data. However, we are able to create more\nsophisticated predictors on three of our datasets, using feature vectors in linear regression or a Graph\nNeural Network. Precise details of the predictors follow.\n\u000fSnapshot : For Oregon / CAIDA graph datasets, which contain a sequence of graphs, we use\nexact counting on a small fraction of the ﬁrst graph as the predictor for all the subsequent graphs .\nSpeciﬁcally, we count the number of triangles per edge, Ne, on the ﬁrst graph for each snapshot\ndataset. We then only store 10% of the top heaviest edges and use these values as estimates for edge\nheaviness in all later graphs. If a queried edge is not stored, its predicted Nevalue is 0.\n7\n\nPublished as a conference paper at ICLR 2022\n\u000fPreﬁx : In the WikiBooks temporal graph, we use the exact Necounts on the ﬁrst half of the graph\nedges (when sorted by their timestamps) as the predicted values for the second half.\n\u000fLinear Regression : In the Reddit Hyperlinks temporal graph, we use a separate dataset\n(Kumar et al., 2019) that contains 300-dimensional feature embeddings of subreddits (graph\nnodes). Two embeddings are close in the feature space if their associated subreddits have\nsimilar sub-communities. To produce an edge f(e)embedding for an edge e=uvfrom\nthe node embedding of its endpoints, f(u)andf(v), we use the 602-dimensional embedding\n(f(u);f(v);k(f(u)\u0000f(v)k1;k(f(u)\u0000f(v)k2). We then train a linear regressor to predict Ne\ngiven the edge embedding f(e). Training is done on a preﬁx of the ﬁrst half of the edges.\n\u000fLink Prediction (GNN) : For each of the two networks, we start with a graph that has twice as\nmany edges as listed in Table 2, ( \u00181:1\u0001105edges for Twitch and 9:2\u0001104edges for Wikipedia).\nWe then randomly remove 50% of the total edges to be the training data set, and use the remaining\nedges as the graph we test on. We use the method proposed in Zhang & Chen (2018) to train a link\nprediction oracle using a Graph Neural Network (GNN) that will be used to predict the heaviness\nof the testing edges. For each edge that arrives in the stream of the test edges, we use the predicted\nlikelihood of forming an edge given by the the neural network to the other vertices as our estimate\nforNuv, the number of triangles on edge uv. See Section F.2 for details of training methodology.\n\u000fExpected Value (EV) : In the Powerlaw graph, the predicted number of triangles incident to each\nNeis its expected value, which can be computed analytically in the CLV random graph model.\nBaselines. We compare our algorithms with the following baselines.\n\u000fThinkD and WRS (Arbitrary Order): These are the state of the art empirical one-pass algorithms\nfrom Shin et al. (2018) and Shin (2017) respectively. The ThinkD paper presents two versions of\nthe algorithm, called ‘fast’ and ‘accurate’. We use the ‘accurate’ version since it provides better\nestimates. We use the authors’ code for our experiments (Shin et al., 2020; Shin, 2020).\n\u000fMVV (Arbitrary Order and Adjacency List): We use the one pass streaming algorithms given in\nMcGregor et al. (2016) for the arbitrary order model and the adjacency list model.\nError measurement. We measure accuracy using the relative error j1\u0000eT=Tj, whereTis the true\ntriangle count and eTis the estimate returned by an algorithm. Our plots show the space used by an\nalgorithm (in terms of the number of edges) versus the relative error. We report median errors over\n50independent executions of each experiment, \u0006one standard deviation.\n4.1 R ESULTS FOR ARBITRARY ORDER TRIANGLE COUNTING EXPERIMENTS\nIn this section, we give experimental results for Algorithm 4 which approximates the triangle count\nin arbitrary order streams. Note that we need to set two parameters for Algorithm 4: p, which is the\nedge sampling probability, and \u001a, which is the heaviness threshold. In our theoretical analysis, we\nassume knowledge of a lower bound on Tin order to set pand\u001a, as is standard in the theoretical\nstreaming literature. However, in practice, such an estimate may not be available; in most cases,\nthe only parameter we are given is a space bound for the number of edges that can be stored. To\nremedy this discrepancy, we modify our algorithm slightly by setting a ﬁxed fraction of space to use\nfor heavy edges ( 10% of space for all of our experiments) and setting pcorrespondingly to use up\nthe rest of the space bound given as input. See details in Supplementary Section F.3.\nOregon and CAIDA In Figures 1(a) and 1(b), we display the relative error as a function of in-\ncreasing space for graph #4in the dataset for Oregon and graph #30 for CAIDA 2006 . These\nﬁgures show that our algorithm outperforms the other baselines by as much as a factor of 5 . We do\nnot display the error bars for MVV and WRS for the sake of visual clarity, but they are comparable\nto or larger than both ThinkD and our algorithm. A similar remark applies to all ﬁgures in Figure\n1. As shown in Figure 2, these speciﬁc examples are reﬂective of the performance of our algorithm\nacross the whole sequence of graphs for Oregon and CAIDA. We also show qualitatively similar\nresults for CAIDA 2007 in Figure 4(a) in Supplementary Section F.4.\nWe also present accuracy results over the various graphs of the Oregon and CAIDA datasets. We\nﬁx the space to be 10% of the number of edges (which varies across graphs). Our results for the\nOregon dataset are plotted in Figure 2(a) and the results for CAIDA 2006 and2007 are plotted in\n8\n\nPublished as a conference paper at ICLR 2022\n0 5103\n1104\n1.5104\n2104\nSpace0.000.050.100.150.200.25Relative Error\nOregon, Graph#4\nOur Alg\nThinkD\nMVV\nWRS\n(a) Oregon\n01104\n2104\n3104\n4104\nSpace0.000.050.100.150.200.250.30Relative Error\nCaida-2006, Graph#30\nOur Alg\nThinkD\nMVV\nWRS (b) CAIDA 2006\n00.5104\n1104\n1.5104\n2104\n2.5104\nSpace0.000.050.100.150.200.250.300.350.40Relative Error\nReddit Hyperlinks Graph\nOur Alg\nThinkD\nMVV\nWRS (c) Reddit\n1104\n2104\n3104\n4104\n5104\n6104\nSpace0.000.050.100.150.200.250.300.350.40Relative Error\nWikibooks Graph\nOur Alg\nThinkD\nMVV\nWRS\n(d) Wikibooks\n0 1104\n2104\n3104\n4104\nSpace0.000.020.040.060.080.100.120.14Relative Error\nWikipedia Graph\nOur Alg\nThinkD\nMVV\nWRS (e) Wikipedia\n1104\n2104\n3104\n4104\n5104\n6104\nSpace0.000.020.040.060.080.100.120.140.16Relative Error\nPowerlaw Graph\nOur Alg\nThinkD\nMVV\nWRS (f) Powerlaw\nFigure 1: Error as a function of space in the arbitrary order model.\n2 3 4 5 6 7 8 9\nGraph #0.000.050.100.150.200.25Relative Error\nOregon, Space = 0.1m\nOur Alg\nThinkD\nWRS\n(a) Oregon\n0 10 20 30 40 50\nGraph #0.000.050.100.150.200.25Relative Error\nCaida-2006, Space = 0.1m\nOur Alg\nThinkD\nWRS (b) CAIDA 2006\n0 10 20 30 40\nGraph #0.000.020.040.060.080.100.120.140.16Relative Error\nCaida-2007, Space = 0.1m\nOur Alg\nThinkD\nWRS (c) CAIDA 2007\nFigure 2: Error across snapshot graphs with space 0:1min the arbitrary order model.\nFigures 2(b) and 2(c), respectively. These ﬁgures illustrate that the quality of the predictor remains\nconsistent over time even after a year has elapsed between when the ﬁrst and the last graphs were\ncreated as our algorithm outperforms the baselines on average by up to a factor of 2.\nReddit Our results are displayed in Figure 1(c). All four algorithms are comparable as we vary\nspace. While we do not improve over baselines in this case, this dataset serves to highlight the fact\nthat predictors can be trained using node or edge semantic information (i.e., features).\nWikibooks and Powerlaw For Wikibooks, we see in Figure 1(d) that our algorithm is outperform-\ning ThinkD and WRS by at least a factor of 2, and these algorithms heavily outperform the MVV\nalgorithm. Nonetheless, it is important to note that our algorithm uses the exact counts on the ﬁrst\nhalf of the edges (encoded in the predictor), which encode a lot information not available to the\nbaselines. Thus the takeaway is that in temporal graphs, where edges arrive continuously over time\n(e.g., citation networks, road networks, etc.), using a preﬁx of the edges to form noisy predictors can\nlead to a signiﬁcant advantage in handling future data on the same graph. Our results for Powerlaw\nare presented in Figure 1(f). Here too we see that as the space allocation increases, our algorithm\noutperforms ThinkD, MVV , and WRS.\nTwitch and Wikipedia In Figure 1(e), we see that three algorithms, ours, MVV , and WRS, are\nall comparable as we vary space. The qualitatively similar result for Twitch is given in Figure 4(b).\nThese datasets serve to highlight that predictors can be trained using modern ML techniques such as\nGraph Neural Networks. Nevertheless, these predictors help our algorithms improve over baselines\nfor experiments in the adjacency list model (see Section below).\nResults for Adjacency List Experiments Our experimental results for adjacency list experiments,\nwhich are qualitatively similar to the arbitrary order experiments, are given in full detail in Sections\nF.5 and F.6.\n9\n\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGMENTS\nJustin is supported by the NSF Graduate Research Fellowship under Grant No. 1745302 and Math-\nWorks Engineering Fellowship. Sandeep and Shyam are supported by the NSF Graduate Research\nFellowship under Grant No. 1745302. Ronitt was supported by NSF awards CCF-2006664, DMS\n2022448, and CCF-1740751. Piotr was supported by the NSF TRIPODS program (awards CCF-\n1740751 and DMS-2022448), NSF award CCF-2006798 and Simons Investigator Award. Talya is\nsupported in part by the NSF TRIPODS program, award CCF-1740751 and Ben Gurion University\nPostdoctoral Scholarship. Honghao Lin and David Woodruff would like to thank for partial support\nfrom the National Science Foundation (NSF) under Grant No. CCF-1815840.\nREFERENCES\nNesreen Ahmed and Nick Dufﬁeld. Adaptive shrinkage estimation for streaming graphs.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances\nin Neural Information Processing Systems , volume 33, pp. 10595–10606. Curran Asso-\nciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n780261c4b9a55cd803080619d0cc3e11-Paper.pdf .\nNesreen K. Ahmed, Nick Dufﬁeld, Theodore L. Willke, and Ryan A. Rossi. On sampling from\nmassive graph streams. Proc. VLDB Endow. , 10(11):1430–1441, August 2017. ISSN 2150-\n8097. doi: 10.14778/3137628.3137651. URL https://doi.org/10.14778/3137628.\n3137651 .\nMohammad Al Hasan and Vachik S Dave. Triangle counting in large networks: a review. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge Discovery , 8(2):e1226, 2018.\nAlexandr Andoni. High frequency moments via max-stability. In 2017 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing, ICASSP 2017, New Orleans, LA, USA, March\n5-9, 2017 , pp. 6364–6368. IEEE, 2017.\nAlexandr Andoni, Collin Burns, Ying Li, Sepideh Mahabadi, and David P. Woodruff. Streaming\ncomplexity of svms. In APPROX-RANDOM , 2020.\nMartin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations . Cam-\nbridge University Press, 1999. doi: 10.1017/CBO9780511624216.\nAlbert Atserias, Martin Grohe, and Daniel Marx. Size bounds and query plans for relational joins.\nIn49th Annual IEEE Symposium on Foundations of Computer Science , 2008.\nMaria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-theoretic\nfoundations of algorithm conﬁguration for combinatorial partitioning problems. In Conference\non Learning Theory , pp. 213–274. PMLR, 2017.\nMaria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch. In\nInternational Conference on Machine Learning , 2018a.\nMaria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design,\nonline learning, and private optimization. In 2018 IEEE 59th Annual Symposium on Foundations\nof Computer Science (FOCS) , pp. 603–614. IEEE, 2018b.\nMaria-Florina Balcan, Travis Dick, and Manuel Lang. Learning to link. arXiv preprint\narXiv:1907.00533 , 2019.\nEtienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning aug-\nmented algorithms. In Advances in Neural Information Processing Systems , 2020.\nZiv Bar-Yossef, Ravi Kumar, and D. Sivakumar. Reductions in streaming algorithms, with an ap-\nplication to counting triangles in graphs. In 13th Annual ACM-SIAM Symposium on Discrete\nAlgorithms , 2002.\nSuman K. Bera and Amit Chakrabarti. Towards tighter space bounds for counting triangles and\nother substructures in graph streams. In Symposium on Theoretical Aspects of Computer Science\n(STACS 2017) , 2017.\n10\n\nPublished as a conference paper at ICLR 2022\nSuman K. Bera and C. Sheshadhri. How the degeneracy helps for traingle counting in graph streams.\nInACM Symposium on Principles of Database Systems , June 2020.\nVladimir Braverman, Rafail Ostrovsky, and Dan Vilenchik. How hard is counting triangles in the\nstreaming model? In International Colloquium on Automata, Languages, and Programming , pp.\n244–254. Springer, 2013.\nFan Chung, Linyuan Lu, and Van Vu. The spectra of random graphs with given expected degrees. In-\nternet Math. , 1(3):257–275, 2003. URL https://projecteuclid.org:443/euclid.\nim/1109190962 .\nEdith Cohen, Oﬁr Geri, and Rasmus Pagh. Composable sketches for functions of frequencies:\nBeyond the worst case. In International Conference on Machine Learning . PMLR, 2020.\nHanjun Dai, Elias Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial op-\ntimization algorithms over graphs. In Advances in Neural Information Processing Systems , pp.\n6351–6361, 2017.\nMichael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster\nmatchings via learned duals. In Advances in Neural Information Processing Systems , 2021. URL\nhttps://arxiv.org/abs/2107.09770 .\nYihe Dong, Piotr Indyk, Ilya P Razenshteyn, and Tal Wagner. Learning space partitions for nearest\nneighbor search. ICLR , 2020.\nElbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the “learning” into learning-\naugmented algorithms for frequency estimation. In International Conference on Machine Learn-\ning, 2021.\nTalya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner.\nLearning-based support estimation in sublinear time. In International Conference on Learning\nRepresentations , 2021. URL https://openreview.net/forum?id=tilovEHA3YS .\nIlles J Farkas, Imre Der ´enyi, A-L Barab ´asi, and Tamas Vicsek. Spectra of” real-world” graphs:\nBeyond the semicircle law. In The Structure and Dynamics of Networks , pp. 372–383. Princeton\nUniversity Press, 2011.\nPaolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. Why are learned indexes so effective? In\nInternational Conference on Machine Learning , pp. 3123–3132. PMLR, 2020.\nBrooke Foucault Welles, Anne Van Devender, and Noshir Contractor. Is a ”friend” a friend? investi-\ngating the structure of friendship networks in virtual worlds. In CHI 2010 - The 28th Annual CHI\nConference on Human Factors in Computing Systems, Conference Proceedings and Extended Ab-\nstracts , pp. 4027–4032, June 2010. ISBN 9781605589312. doi: 10.1145/1753846.1754097. 28th\nAnnual CHI Conference on Human Factors in Computing Systems, CHI 2010 ; Conference date:\n10-04-2010 Through 15-04-2010.\nSreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice.\nInInternational Conference on Machine Learning , pp. 2319–2327. PMLR, 2019.\nHeitor Murilo Gomes, Jesse Read, Albert Bifet, Jean Paul Barddal, and Jo ˜ao Gama. Machine\nlearning for streaming data: state of the art, challenges, and opportunities. SIGKDD Explor. ,\n21:6–22, 2019.\nGuyue Han and Harish Sethu. Edge sample and discard: A new algorithm for counting triangles\nin large dynamic graphs. In Proceedings of the 2017 IEEE/ACM International Conference on\nAdvances in Social Networks Analysis and Mining 2017 , ASONAM ’17, pp. 44–49, New York,\nNY , USA, 2017. Association for Computing Machinery. ISBN 9781450349932. doi: 10.1145/\n3110025.3110061. URL https://doi.org/10.1145/3110025.3110061 .\nChen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In International Conference on Learning Representations , 2019a. URL https:\n//openreview.net/forum?id=r1lohoCqY7 .\n11\n\nPublished as a conference paper at ICLR 2022\nChen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In International Conference on Learning Representations , 2019b.\nZachary Izzo, Sandeep Silwal, and Samson Zhou. Dimensionality reduction for wasserstein\nbarycenter. In Advances in Neural Information Processing Systems , 2021.\nRajesh Jayaram and John Kallaugher. An optimal algorithm for triangle counting in the stream.\nInApproximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques\n(APPROX/RANDOM 2021) . Schloss Dagstuhl-Leibniz-Zentrum f ¨ur Informatik, 2021.\nTanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodruff. Learning-augmented\ndata stream algorithms. In International Conference on Learning Representations , 2020. URL\nhttps://openreview.net/forum?id=HyxJ1xBYDH .\nJohn Kallaugher and Eric Price. A hybrid sampling scheme for triangle counting. In Philip N. Klein\n(ed.), Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,\nSODA 2017, Barcelona, Spain, Hotel Porta Fira, January 16-19 , pp. 1778–1797. SIAM, 2017.\nJohn Kallaugher, Andrew McGregor, Eric Price, and Sofya V orotnikova. The complexity of count-\ning cycles in the adjacency list streaming model. In Proceedings of the 38th ACM SIGMOD-\nSIGACT-SIGAI Symposium on Principles of Database Systems , PODS ’19, pp. 119–133, New\nYork, NY , USA, 2019. Association for Computing Machinery. ISBN 9781450362276. doi:\n10.1145/3294052.3319706. URL https://doi.org/10.1145/3294052.3319706 .\nMihail N. Kolountzakis, Gary L. Miller, Richard Peng, and Charalampos E. Tsourakakis. Efﬁcient\ntriangle counting in large graphs via degree-based vertex partitioning. Lecture Notes in Computer\nScience , pp. 15–24, 2010. ISSN 1611-3349. doi: 10.1007/978-3-642-18009-5 3. URL http:\n//dx.doi.org/10.1007/978-3-642-18009-5_3 .\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data\n(SIGMOD) , pp. 489–504. ACM, 2018.\nSrijan Kumar, William L Hamilton, Jure Leskovec, and Dan Jurafsky. Community interaction and\nconﬂict on the web. In Proceedings of the 2018 World Wide Web Conference on World Wide Web ,\npp. 933–943. International World Wide Web Conferences Steering Committee, 2018.\nSrijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in tem-\nporal interaction networks. In Proceedings of the 25th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining , pp. 1269–1278. ACM, 2019.\nSilvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online schedul-\ning via learned weights. In Proceedings of the 31st Annual ACM-SIAM Symposium on Discrete\nAlgorithms , pp. 1859–1877. SIAM, 2020.\nJure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http:\n//snap.stanford.edu/data , June 2014.\nJure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graphs over time: Densiﬁcation laws,\nshrinking diameters and possible explanations. In Proceedings of the Eleventh ACM SIGKDD\nInternational Conference on Knowledge Discovery in Data Mining , KDD ’05, pp. 177–187,\nNew York, NY , USA, 2005. Association for Computing Machinery. ISBN 159593135X. doi:\n10.1145/1081870.1081893. URL https://doi.org/10.1145/1081870.1081893 .\nJure Leskovec, Lars Backstrom, Ravi Kumar, and Andrew Tomkins. Microscopic evolution of social\nnetworks. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining , KDD ’08, pp. 462–470, New York, NY , USA, 2008. Association for\nComputing Machinery. ISBN 9781605581934. doi: 10.1145/1401890.1401948. URL https:\n//doi.org/10.1145/1401890.1401948 .\nYongsub Lim and U Kang. Mascot: Memory-efﬁcient and accurate sampling for counting local\ntriangles in graph streams. In Proceedings of the 21th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , KDD ’15, pp. 685–694, New York, NY , USA, 2015.\nAssociation for Computing Machinery. ISBN 9781450336642. doi: 10.1145/2783258.2783285.\nURL https://doi.org/10.1145/2783258.2783285 .\n12\n\nPublished as a conference paper at ICLR 2022\nMario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian mixture\nmodels at scale via coresets. Journal of Machine Learning Research , 18(160):1–25, 2018. URL\nhttp://jmlr.org/papers/v18/15-506.html .\nThodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned advice. In\nInternational Conference on Machine Learning , pp. 3296–3305. PMLR, 2018.\nM. Mahoney. Large text compression benchmark., 2011.\nAndrew McGregor and Sofya V orotnikova. Triangle and four cycle counting in the data stream\nmodel. PODS’20, pp. 445–456, New York, NY , USA, 2020. Association for Computing Machin-\nery. ISBN 9781450371087. doi: 10.1145/3375395.3387652. URL https://doi.org/10.\n1145/3375395.3387652 .\nAndrew McGregor, Sofya V orotnikova, and Hoa T. Vu. Better algorithms for counting triangles in\ndata streams. In ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems ,\nJune 2016.\nR. Milo, S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon. Network motifs: Simple\nbuilding blocks of complex networks. Science , 298(5594):824–827, 2002. ISSN 0036-8075. doi:\n10.1126/science.298.5594.824. URL https://science.sciencemag.org/content/\n298/5594/824 .\nMichael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. In\nAdvances in Neural Information Processing Systems , 2018.\nMichael Mitzenmacher. Scheduling with predictions and the price of misprediction. In ITCS , 2020.\nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. arXiv preprint\narXiv:2006.09123 , 2020.\nRasmus Pagh and Charalampos E. Tsourakakis. Colorful triangle counting and a mapreduce im-\nplementation. Inf. Process. Lett. , 112(7):277–281, 2012. doi: 10.1016/j.ipl.2011.12.007. URL\nhttps://doi.org/10.1016/j.ipl.2011.12.007 .\nArnau Prat-P ´erez, David Dominguez-Sal, Josep M Brunat, and Josep-Lluis Larriba-Pey. Shaping\ncommunities out of triangles. In Proceedings of the 21st ACM international conference on Infor-\nmation and knowledge management , pp. 1677–1681, 2012.\nManish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions.\nInAdvances in Neural Information Processing Systems , pp. 9661–9670, 2018.\nJack Rae, Sergey Bartunov, and Timothy Lillicrap. Meta-learning neural bloom ﬁlters. In Interna-\ntional Conference on Machine Learning , pp. 5271–5280, 2019.\nPiyush Rai, Hal Daum ´e, and Suresh Venkatasubramanian. Streamed learning: One-pass svms.\nArXiv , abs/0908.0572, 2009.\nDhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In Proceed-\nings of the 31st Annual ACM-SIAM Symposium on Discrete Algorithms , pp. 1834–1845. SIAM,\n2020.\nRyan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph analytics\nand visualization. In AAAI , 2015. URL http://networkrepository.com .\nBenedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding, 2019.\nC. Seshadhri, Ali Pinar, and Tamara G. Kolda. Fast triangle counting through wedge sampling. In\nthe International Conference on Data Mining (ICDM) , 2013.\nKijung Shin. Wrs: Waiting room sampling for accurate triangle counting in real graph streams.\n2017 IEEE International Conference on Data Mining (ICDM) , pp. 1087–1092, 2017.\nKijung Shin. Wrs: Waiting room sampling for accurate triangle counting in real graph streams.\nhttps://github.com/kijungs/waiting_room , 2020.\n13\n\nPublished as a conference paper at ICLR 2022\nKijung Shin, Jisu Kim, Bryan Hooi, and Christos Faloutsos. Think before you discard: Accurate\ntriangle counting in graph streams with deletions. In Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases , pp. 141–157. Springer, 2018.\nKijung Shin, Jisu Kim, Bryan Hooi, and Christos Faloutsos. Think before you discard: Accu-\nrate triangle counting in graph streams with deletions. https://github.com/kijungs/\nthinkd , 2020.\nLorenzo De Stefani, Alessandro Epasto, Matteo Riondato, and Eli Upfal. Tri `Est: Counting local and\nglobal triangles in fully dynamic streams with ﬁxed memory size. ACM Trans. Knowl. Discov.\nData , 11(4), June 2017. ISSN 1556-4681. doi: 10.1145/3059194. URL https://doi.org/\n10.1145/3059194 .\nKapil Vaidya, Eric Knorr, Tim Kraska, and Michael Mitzenmacher. Partitioned learned bloom ﬁlter.\nInInternational Conference on Learning Representations , 2021.\nSofya V orotnikova. Improved 3-pass algorithm for counting 4-cycles in arbitrary order streaming.\nCoRR , abs/2007.13466, 2020. URL https://arxiv.org/abs/2007.13466 .\nJun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data - a\nsurvey. Proceedings of the IEEE , 104(1):34–57, 2016.\nAlexander Wei. Better and simpler learning-augmented online caching. In Approximation, Ran-\ndomization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM\n2020) . Schloss Dagstuhl-Leibniz-Zentrum f ¨ur Informatik, 2020.\nDavid P. Woodruff. Sketching as a tool for numerical linear algebra. Found. Trends Theor. Comput.\nSci., 10:1–157, 2014.\nMuhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Samy Bengio,\nHanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol `o Cesa-Bianchi, and Roman Garnett\n(eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural\nInformation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr ´eal, Canada ,\npp. 5171–5181, 2018.\n14\n\nPublished as a conference paper at ICLR 2022\nA N OTATIONS TABLE\nNotation Deﬁnition\nG =\n(V;E)a graphGwith vertex set Vand edge set E\nn number of triangles\nm number of edges\nT number of triangles (or 4-cycles)\n\u000f approximation parameter\n<stotal ordering on the vertices according to their arrival in\nthe adjacency list arrival model\n\u0001 set of triangles in G\n\u0003 set of 4-cycles in G\nN(v) set of neighbors of node v\nNxynumber of triangles (4-cycles) incident to edge xy, i.e.,\njfzj(x;y;z )2\u0001gj(jfw;zj(x;y;w:z )2\u0003gj)\nRxynumber of triangles (x;z;y )wherexprecedeszand\nzprecedesyin the adjacency list arrival model, i.e.,\njfzj(x;z;y )2\u0001;x<sz<sygj\n\u0001E maximum number of triangles incident to any edge\n\u0001V maximum number of triangles incident to any vertex\nO\u001a heavy edge oracle with threshold \u001a\nB O MITTED PROOFS FROM SECTION 2\nIn this section, we prove correctness and bound the space of Algorithm 1, thus proving Theorem 1.1.\nIn the adjacency list model, recall that we have a total ordering on nodes <sbased on the stream\nordering, where x<syif and only if the adjacency list of xarrives before the adjacency list of yin\nthe stream. For each xy2E(G), we deﬁneRxy=jz:fx;y;zg2\u0001andx<sz <syj. Note that\nify<sx, thenRxy= 0. It holds thatP\nxy2ERxy=T.\nWe ﬁrst give a detailed overview of the algorithm. We ﬁrst consider the case when T\u0015(m=\u000f)1=2.\nLet\u001a= (mT)1=3. We deﬁne the edge xyto be heavy ifRxy\u0015\u001a,light ifRxy\u0014T\n\u001a, and medium if\nT\n\u001a<Rxy<\u001a. We train the oracle to predict, upon seeing xy, whetherRxyis light or not. We deﬁne\nH;M , andLas the set of heavy, medium, and light edges, respectively. Deﬁne TL=P\nxy2LRxy.\nWe deﬁneTHandTMsimilarly. SinceP\nxy2ERxy=T=TL+TM+TH, it sufﬁces to estimate\neach of these three terms.\nFor the light edges, as shown in Lemma B.2, if we sample edges with rate p1\u0019\u000f\u00002=\u001a, with\nhigh probability we obtain an estimate of TLwithin error\u0006\u000fT. For the heavy edges, we recall an\nobservation in McGregor et al. (2016): for an edge xywhereRxyis large, even if we do not sample\nxydirectly, if we sample from the stream (at rate p3\u0019\u000f\u00002=\u001a), we are likely to sample some edges\nin the setjxz:fx;y;zg2\u0001andx<sz<syj. We refer to the sampled set of these edges as Saux.\nFurther, we will show that not only can we use Sauxto recognize whether Rxyis large, but also to\nestimateRxy(Lemma B.3). What remains to handle is the medium edges. Since medium edges have\nhigher values of Rxythan light edges, we sample with a larger rate, p2, to reduce variance. However,\nwe show there cannot be too many medium edges, so we do not pay too much extra storage space.\nThe overall space we use is eO(\u000f\u00002m2=3=T1=3).\nWhenT <(m=\u000f)1=2our algorithm becomes much simpler. Let \u001a=pm=\u000f. We deﬁne the edge xy\nto be heavy ifRxy\u0015T=\u001a, and light otherwise (so in this case all the medium edges become heavy\n15\n\nPublished as a conference paper at ICLR 2022\nedges). We directly store all the heavy edges and sub-sample with rate \u000f\u00002=\u001ato estimateTL. The\ntotal space we use is eO(\u000f\u00001pm), which is optimal in this case. See Algorithm 1 for more details of\nthe implementation.\nAlgorithm 1 Counting Triangles in the Adjacency List Model\n1:Input: Adjacency list edge stream and an oracle Othat outputs HEAVY ifRxy\u0015T=\u001a and\nLIGHT otherwise.\n2:Output: An estimate of the triangle count T.\n3:InitializeAl;Am;Ah= 0,SL;SM;Saux=;\n4:ifT\u0015(m=\u000f)1=2then\n5: Set\u001a= (mT)1=3,p1=\u000b\u000f\u00002=\u001a,p2= min(\f\u000f\u00002\u001a=T; 1),p3=\r\u000f\u00002logn=\u001a.\n6:else\n7: Set\u001a=m1=2=\u000f,p1= 0,p2= 1,p3=\r\u000f\u00002=\u001a.f\u000b,\f, and\rare large enough constants. g\n8:while seeing edges adjacent to vdo\n9: for allab2SL[SMdo\n10: ifa;b2N(v)thenCab=Cab+ 1.fUpdate the counter for all the sampled light and\nmedium edgesg\n11: for allav2Sauxdo\n12:Bav= 1fseenavtwiceg.\n13: foreach incident edge vudo\n14: W.p.p3:Saux=fvug[Saux,Bvu= 0\n15: ifO(uv)outputs LIGHT then\n16: ifuv2SLthenAl=Al+Cuv.fFinished counting Ruvg\n17: elsew.p.p1,SL=fvug[SL\n18: elsefuvis either MEDIUM orHEAVYg\n19: # :=jfz:uz2Saux;Buz= 1;z2N(v)gj.\n20: if#\u0015p3\u001athenfuvisHEAVYg\n21: Ah=Ah+ # .\n22: else ifuv2SMthen\n23: Am=Am+Cuv.fFinished counting Ruvg\n24: else\n24: With probability p2,SM=fvug[SM.\n25:return:Al=p1+Am=p2+Ah=p3.\nRemark B.1. In the adjacency list model, we assume the oracle can predict whether Rxy\u0015T\ncor not, where Rxyis dependent on the node order. This may sometimes be impractical. However,\nobserve that Nxy\u0015RxyandPNxy= 3T. Hence, our proof still holds if we assume what the\noracle can predict is whether Nxy\u0015T\nc, which could be a more practical assumption.\nRecall the following notation from Section 2. We deﬁne the edge xyto be heavy ifRxy\u0015\u001a,light if\nRxy\u0014T\n\u001a, and medium ifT\n\u001a<Rxy<\u001a. We deﬁneH;M , andLas the set of heavy, medium, and\nlight edges, respectively. Deﬁne TL=P\nxy2LRxy. We deﬁne THandTMsimilarly. Also, recall\nthat sinceP\nxy2ERxy=T=TL+TM+TH, it sufﬁces to estimate of these three terms.\nLemma B.2. LetAbe an edge set such that for each edge xy2A,Rxy\u0014T\nc, for some parameter\nc, and letSbe a subset of Asuch that every element in Ais included in Swith probability p=1\n\u001c\nfor\u001c=\u000f2\u0001c=\u000b; so that 1=\u001c=\u000b\u000f\u000021\ncindependently, for a sufﬁcient large constant \u000b. Then, with\nprobability at least 9=10, we have\n\u001cX\nxy2SRxy=X\nxy2ARxy\u0006\u000fT\n16\n\nPublished as a conference paper at ICLR 2022\nProof. We letY=\u001cP\ni2SRi=\u001cP\ni2AfiRi, wherefi= 1wheni2Sandfi= 0otherwise. It\nfollows that E[Y] =TA. Then,\nVar[Y] =E[Y2]\u0000E[Y]2\n=\u001c20\n@E[X\ni2AfiR2\ni+X\ni6=jfifjRiRj]1\nA\u0000T2\nL\n\u0014\u001c20\n@E[X\ni2AfiR2\ni] +X\ni;j1\n\u001c2RiRj1\nA\u0000T2\nL\n=\u001cX\ni2AR2\ni:\nTo bound this, we notice that Ri\u0014T\ncandP\niRi=TA\u0014T, so\n\u001cX\ni2AR2\ni\u0014\u001c\u0001max\ni2ARi\u0001X\ni2ARi\u0014\u001c\u0001T\nc\u0001T=\u001c\ncT2=1\n\u000b\u000f2T2:\nBy Chebyshev’s inequality, we have that\nPr[jY\u0000TAj>\u000fT ]\u00141\n\u000b\u000f2T2\n\u000f2T2\u00141\n\u000b:\nThe following lemma shows that for the heavy edges, we can use the edges we have sampled to\nrecognize and estimate them.\nLemma B.3. Letxybe an edge in E. Assume we sample the edges with rate p3=\f\u000f\u00002logn=\u001ain\nthe stream for a sufﬁciently large constant \f, and consider the set\n# =jxzhas been sampled :fx;y;zg2\u0001andx<sz<syj:\nThen we have the following: if Rxy\u0015\u001a\n2, with probability at least 1\u00001\nn5, we havep\u00001\n3\u0001# =\n(1\u0006\u000f)Rxy. IfRxy<\u001a\n2, with probability at least 1\u00001\nn5, we havep\u00001\n3\u0001#<\u001a.\nProof. We ﬁrst consider the case when Rxy\u0015\u001a\n2.\nFor each edge i2jxz:fx;y;zg2\u0001andx <sz <syj, letXi= 1 ifihas been sampled, or\nXi= 0 otherwise. We can see the Xiare i.i.d. Bernoulli random variables and E[Xi] =p3. By a\nChernoff bound we have\nPr\"\njX\niXi\u0000p3Rxyj\u0015\u000fp3\u0001Rxy#\n\u00142 exp\u0000\n\u0000\u000f2p3\u0001Rxy=3\u0001\n\u00141\nn5:\nTherefore, we have\nPr\u0002\njp\u00001\n3\u0001#\u0000Rxyj\u0015\u000fRxy\u0003\n=Pr\"\njp\u00001\n3\u0001X\niXi\u0000Rxyj\u0015\u000fRxy#\n\u00141\nn5:\nAlternatively, when Rxy<\u001a\n2, we have\nPr\u0014\njp\u00001\n3\u0001#\u0000Rxyj\u00151\n2\u001a\u0015\n\u00141\nn5;\nwhich means p\u00001\n3\u0001#<\u001awith probability at least 1\u00001\nn5.\nWe are now ready to prove Theorem 1.1, restated here for the sake of convenience.\n17\n\nPublished as a conference paper at ICLR 2022\nTheorem 1.1. There exists a one-pass algorithm, Algorithm 1, with space complexity3\neO(min(\u000f\u00002m2=3=T1=3;\u000f\u00001m1=2))in the adjacency list model that, using a learning-based ora-\ncle, returns a (1\u0006\u000f)-approximation to the number Tof triangles with probability at least47=10.\nProof. We ﬁrst consider the case when T\u0015(m=\u000f)1=2.\nFor each edge xy2H, from Lemma B.3 we have that with probability at least 1\u00001\nn5,#=p3=\n(1\u0006\u000f)Rxyin Algorithm 1. If xy =2H, then with probability 1\u00001\nn5,xywill not contribute to Ah.\nTaking a union bound, we have that with probability at least 1\u00001=n3,\nAh=p3= (1\u0006\u000f)X\nxy2HRxy= (1\u0006\u000f)TH:\nWe now turn to deal with the light and medium edges. For a light edge xy, it satisﬁesRxy\u0014T=\u001a\nand therefore we can invoke Lemma B.2 with the parameter cset to\u001a. For a medium edge xy, it\nsatisﬁesRxy\u0014\u001a=T=(T=\u001a), and therefore we can invoke Lemma B.2 with the parameter cset to\nT=\u001a. Hence, we have that with probability 4=5,\nAm=p2=TM\u0006\u000fT;Al=p1=TL\u0006\u000fT :\nPutting everything together, we have that with probability 7=10,\njAh=p3+Am=p2+Al=p1\u0000Tj\u00143\u000fT:\nNow we analyze the space complexity. We note that there are at most \u001amedium edges. Hence, the\nexpected total space we use is O(mp3+\u001ap2+mp1) =O(\u000f\u00002m2=3=T1=3logn)5.\nWhenT <(m=\u000f)1=2, as described in Section 2, we only have two classes of edges: heavy and light.\nWe will save all the heavy edges and use sub-sampling to estimate TL. The analysis is almost the\nsame.\nB.1 N OISY ORACLES FOR THE ADJACENCY LISTMODEL\nProof of Theorem 1.4. We ﬁrst consider the case that T < (m=\u000f)1=2. Recall that at this time, our\nalgorithms becomes that we save all the heavy edges xysuch thatp(e)\u0015\u000fT=pmand sampling the\nlight edges with rate pl=\u000f\u00001=pm. We deﬁneLas the set of deemed light edges,Has the set of\ndeemed heavy edges andL=jLj,H=jHj. Let\u001a=\u000fT=pm, then we have the condition that for\nevery edgee,\n1\u0000K\u0001\u001a\nRe\u0014Pr[O\u001a(e) = HEAVY ]\u0014K\u0001Re\n\u001a:\nWe will show that, (i)E[H]\u0014(K+ 1)pm\n\u000f,(ii)Var[AL=p1]\u0014(4K+ 2)\u000f2T2. Under the above\nconditions we can get that with probability at least 9=10we can get a (1\u0006O(p\nK)\u0001\u000f)-approximation\nofTby Chebyshev’s inequality.\nFor(i), we divideH=Hh[Hl, where the edges in Hhare indeed heavy edges, and the edges\ninHlare indeed lightedges. Then, it is easy to see that jHhj\u0014T=\u001a. For every light edges, the\nprobability that it is included in Hlis at mostK\u0001Re\n\u001a, hence we have\nE[jHhj]\u0014K\u0001X\ne2H(Re\n\u001a)\u0014K\u0001T\n\u001a=Kpm\n\u000f\nwhich implies E[H]\u0014(K+ 1)pm\n\u000f.\nFor(ii), we also divideL=Ll[Lhsimilarly. Then we have Var[AL]\u00142(Var[X] +Var[Y]),\nwhereX=P\ne2Ll;ehas been sampledRe,Y=P\ne2Lh;ehas been sampledRe. Similar to the proof in\nLemma B.2, we have\nVar[X]\u0014p1X\ne2LR2\ne\u0014p1T\n\u001a\u001a2=p1T\u001a:\n3We use eO(f)to denoteO(f\u0001polylog(f)).\n4The success probability can be 1\u0000\u000eby running log(1=\u000e)copies of the algorithm and taking the median.\n5Using Markov’s inequality, we have that with probability at least 9=10, the total space we use is less than\n10times the expected total space.\n18\n\nPublished as a conference paper at ICLR 2022\nNow we bound Var[Y]. For every heavy edge we have that Pr[e2Lh]\u0014K\u0001\u001a\nRe. Then, condition\non the oracle O\u001a, we have\nE[Var[YjO\u001a]]\u0014p1KX\ne2H\u001a\nReR2\ne\u0014p1KX\ne2H\u001aRe\u0014p1K\u001aT :\nAlso, we have\nVar[E[YjO\u001a]]<p1KX\ne2H\u001a\nReR2\ne\u0014p1K\u001aT :\nFrom Var[Y] =E[Var[YjO\u001a]] +Var[E[YjO\u001a]]we know Var[Y]\u00142p1K\u001aT , which means\nVar[Al=p1]\u00141\np1(4K+ 2)\u001aT= (4K+ 2)\u000f2T2.\nWhenT\u0015(m=\u000f)1=2, recall that the oracle O\u001aonly needs to predict the edges that are medium\nedges or light edges. So, we can use the same way to bound the expected space for the deemed\nmedium edges and the variance of the deemed light edges.\nB.2 A V ALUE PREDICTION ORACLE FOR THE ADJACENCY LISTMODEL\nIn this section, we consider two types of value-prediction oracles, and variants of Algorithm 1 that\ntake advantage of these oracles. Recall that given an edge xy,Rxy=jz:fx;y;zg2\u0001andx<s\nz<syj.\nDeﬁnition B.4. A value-prediction oracle with parameter (\u000b;\f)is an oracle that, for any edge e,\noutputs a value p(e)for which E[p(e)]\u0014\u000bRe+\f, and Pr[p(e)<Re\n\u0015\u0000\f]\u0014Ke\u0000\u0015for some\nconstantKand any\u0015\u00151.\nOur algorithm is based on the following stability property of exponential random variables (see\ne.g. Andoni (2017))\nLemma B.5. Letx1;x2;:::;xn>0be real numbers, and let u1;u2;:::unbe i.i.d. standard\nexponential random variables with parameter \u0015= 1. Then we have that the random variable\nmax(x1\nu1;:::;xn\nun)\u0018x\nu, wherex=P\nixiandu\u0018Exp(1) .\nLemma B.6. Letr=O(ln(1=\u000e)=\u000f2)and\u000f\u00141=3. If we takerindependent samples X1;X2;:::;Xr\nfrom Exp(1) and letX= median(X1;X2;:::;Xr), then with probability 1\u0000\u000e,X2[ln 2(1\u0000\n\u000f);ln 2(1 + 5\u000f)].\nProof. We ﬁrst prove the following statement: with probability 1\u0000\u000e,F(X)2[1=2\u0000\u000f;1=2 +\u000f],\nwhereFis the cumulative density function of a standard exponential random variable.\nConsider the case when F(X)\u00141=2\u0000\u000f. We use the indicator variable ZiwhereZi= 1 if\nF(Xi)\u00141=2\u0000\u000forZi= 0 otherwise. Notice that when F(X)\u00141=2\u0000\u000f, at least half of the Zi\nare1, soZ=P\niZi\u0015r=2. On the other hand, we have E[Z] =r=2\u0000r\u000f, by a Chernoff bound.\nWe thus have\nPr[jZ\u0000E[Z]j\u0015\u000fr]\u00142e\u0000\u000f2r=3\u0014\u000e=2:\nTherefore, we have with probability at most \u000e=2,F(X)\u00141=2\u0000\u000f. Similarly, we have that with\nprobability at most \u000e=2,F(X)\u00151=2 +\u000f. Taking a union bound, we have that with probability at\nleast1\u0000\u000e,F(X)2[1=2\u0000\u000f;1=2 +\u000f].\nNow, condition on F(X)2[1=2\u0000\u000f;1=2 +\u000f]. Note thatF\u00001(x) =\u0000ln(1\u0000x), so if we consider\nthe two endpoints of F(X), we have\nF\u00001(1=2\u0000\u000f) =\u0000ln(1=2 +\u000f)\u0015(1\u0000\u000f) ln 2;\nand\nF\u00001(1=2 +\u000f) =\u0000ln(1=2\u0000\u000f)\u0014(1 + 5\u000f) ln 2:\nwhich indicates that X2[ln 2(1\u0000\u000f);ln 2(1 + 5\u000f)].\nThe algorithm utilizing this oracle is shown in Algorithm 2. As described in Section 2, here we runs\nO(1\n\u000f2)copies of the subroutine and takes a medium of them. For each subroutine, we aim to output\nthe maximum value maxeRe=ue. We will show that with high probability for each subroutine,\n19\n\nPublished as a conference paper at ICLR 2022\nthis maximum will be at least T=log(K=\u000f). Hence, we only need to save the edge e, such that\n(Re+\f)=ueis larger than T=log(K=\u000f). However, one issue here is we need the information of T.\nWe can remove this assumption when \f= 0using the following way: we will show that with high\nprobability, the total number of edges we save is less than H=O(1\n\u000f2log2(K=\u000f)(\u000b+m\f=T )), so\nevery time when a new edge comes, we can search the minimum value Msuch that the total number\nof edgesethat in at least one subroutine (p(e) +\f)=ue> M is less than H(we count an edge\nmultiple times if it occurs in multiple subroutines), then use Mas the threshold. We can see Mwill\nincrease as the new edge arriving and it will be always less than H.\nAlgorithm 2 Counting Triangles in the Adjacency List Model with a Value Prediction Oracle\n1:Input: Adjacency list edge stream and an value prediction oracle with parameter (\u000b;\f).\n2:Output: An estimate of the number Tof triangles.\n3:InitializeX ; andH=O(1\n\u000f2log2(K=\u000f)(\u000b+m\f=T )), and letcbe some large constant.\n4:fori= 0toc\u000f\u00002dofInitialize sets for c\u000f\u00002copies of samples, where Sistores edges, Qistores\nall the exponential random variables for each sampled edge, and Aistores the current maximum\nof each sampleg\n5:Si ;;Qi ;, andAi 0.\n6:while seeing edges adjacent to yin the stream do\n7: fori= 0toc\u000f\u00002dofUpdate the counters for each sample g\n8: For allab2Si: ifa;b2N(y)thenCi\nab Ci\nab+ 1\n9: foreach incident edge yxdo\n10: fori= 0toc\u000f\u00002do\n11: ifxy2SithenfFinished counting Cxy, update the current maximum g\n12: Ai max(Ai;Ci\nxy=ui\nxy).\n13: elsefPutyxinto the sample sets temporarily g\n14: Let^Ryxbe the predicted value of Ryx.\n15: ^Ryx ^Ryx+\f.\n16: Generateui\nyx\u0018Exp(1) . SetQi Qi[fui\nyxg;Si Si[fyxg.\n17: SetMto be the minimal integer such thatP\nisi\u0014H, wheresi=jfe2Ej^Re=ui\ne\u0015Mgj.\n18: fori= 0toc\u000f\u00002dofAdjust the samples to be within the space limit g\n19: LetQi Qinfui\nabg;Si Sinfabgif^Rab=ui\nab<M for allab2E.\n20:fori= 0toc\u000f\u00002do\n21:X X[fAig\n22:return: ln 2\u0001median(X).\nWe are now ready to prove Theorem 1.5. We ﬁrst recall the theorem.\nTheorem 1.5. Given an oracle with parameters (\u000b;\f), there exists a one-pass algorithm, Algo-\nrithm 2, with space complexity O(\u000f\u00002log2(K=\u000f)(\u000b+m\f=T ))in the adjacency list model that\nreturns a (1\u0006\u000f)-approximation to the number of triangles Twith probability at least 7=10.\nProof. It is easy to see that for the i-th subroutine, the value f(i) = maxeRe=ui\neis the sample from\nthe distribution T=u, whereu\u0018Exp(1) . And if the edge ei, for which the true value Rei=ui\neiis the\nmaximum value among E, is included in Si, then the output of the i-th subroutine will be a sample\nfromT=u. Intuitively, the prediction value p(e)will help us ﬁnd this edge.\nWe will ﬁrst show that with probability at least 9=10the following events will happen:\n•(i)f(i)\u0015c1T=log(1=\u000f)for alli, wherec1is a constant.\n•(ii)Letsibe the number of the edge ein thei-th subroutine such that (p(e) +\f)=ui\ne\u0015\nc2\n1T=log2(K=\u000f), thenP\nisi\u0014c2(1\n\u000f2log2(K=\u000f)(\u000b+m\f=T ))for some constant c2.\n•(iii)p(ei) +\f\u0015c1f(i)=log(K=\u000f)for alli.\n20\n\nPublished as a conference paper at ICLR 2022\nFor(i), recall thatf(i)\u0018T=u; whereu\u0018Exp(1) . Hence, we have\nPr[f(i)<c1T=log(1=\u000f)] =e\u0000log(1=\u000f)=c1\u00141\n100c1\n\u000f2:\nTaking an union bound we get that with probability at least 99=100,(i)will happen. For each edge\ne, we have\nPr[p(e) +\f <Re=\u0015]<Ke\u0000\u0015;\nsimilarly we can get that with probability at least 99=100,(iii)will happen.\nFor(ii), we ﬁrst bound the expectation of si. For each edge e, we have\nPr\u0002\n(p(e) +\f)=ui\ne\u0015c2\n1T=log2(K=\u000f)\u0003\n=Pr[ue\u0014c2\n1(p(e) +\f) log2(K=\u000f)=T]\n\u0014c2\n1(p(e) +\f) log2(K=\u000f)=T\n\u0014c2\n1(\u000bRe+ 2\f) log2(K=\u000f)=T :\nHence, we have\nE[si] =X\nec2\n1(\u000bRe+ 2\f) log2(K=\u000f)\nT=O(log2(K=\u000f)(\u000b+m\f=T )):\nUsing Markov’s inequality, we get that with probability at least 99=100,(ii)will happen. Finally,\ntaking a union bound, we can get with probability at least 9=10, all the three events will happen.\nNow, conditioned on the above three events, we will show that all the subroutine iwill outputf(i).\nFor the subroutine i, from (ii)we know that Mwill be always smaller than O(T=log2(K=\u000f)), and\nfrom (i)and(iii)we get that (p(ei)+\f)=ui\nei\u0015\n(T=log2(K=\u000f)). Hence, the edge eiwill be saved\nin the subroutine i. From Lemma B.6 we get that with probability at least 7=10, we can ﬁnally get a\n(1\u0006\u000f)-approximation of T.\nWe note that when \f= 0, the space complexity will become O(1\n\u000f2log2(K=\u000f)\u000b), and in this case\nwe will not need a lower bound of T.\nWe will also consider the following noise model.\nDeﬁnition B.7. A value-prediction oracle with parameter (\u000b;\f)is an oracle that, for any edge e,\noutputs a value p(e)for which Pr[p(e)>\u0015\u000bRe+\f]\u0014K\n\u0015, andPr[p(e)<Re\n\u0015\u0000\f]\u0014K\n\u0015for some\nconstantKand any\u0015\u00151.\nThe algorithm for this oracle is shown in Algorithm 3.\nWe now state our theorem.\nTheorem B.1. Given an oracle with parameter (\u000b;\f), there exists a one-pass algorithm, Algo-\nrithm 3, with space complexity O(K\n\u000f2\u0000\n\u000b(logn)3log logn+m\f=T\u0001\n)in the adjacency list model\nthat returns a (1\u0006p\nK\u0001\u000f)-approximation to Twith probability at least 7=10.\nProof. Deﬁneq(e) =p(e) +\f, it is easy to see that from the condition we can get that\nPr[q(e)>2\u0015\u000bRe]\u0014K1\n\u0015whenRe>2\f;Pr\u0014\nq(e)<Re\n\u0015\u0015\n\u0014K1\n\u0015\nWe deﬁne the set Eisuch thate2Eiif and only if q(e)2Ii. We can see E[Ai=pi] =P\ne2EiRe.\nNow, we bound Var[Ai=pi]wheni\u00151.\nLetX=P\ne2Ei;Re\u00142i+1\fReandY=P\ne2Ei;Re>2i+1\fRe, then we have Var[Ai]\u00142(Var[X]+\nVar[Y]).\nSimilar to the proof in Lemma B.2, we can get that\nVar[X]\u0014piX\ne2Ei;Re\u00142i+1\fR2\ne\u0014pi(2i+1\f)2T\n2i+1\f=pi2i+1\fT:\n21\n\nPublished as a conference paper at ICLR 2022\nAlgorithm 3 Counting Triangles in the Adjacency List Model with a Value Prediction Oracle\n1:Input: Adjacency list edge stream and an value prediction oracle with parameter (\u000b;\f).\n2:Output: An estimate of the number Tof triangles.\n3:InitializeSj\ni ; andAj\ni 0wherei=O(logn)andj=O(log logn).p0=c\u000f\u000022i\f=T\nandpi=c\u000f\u000022i\f(logn)2=Tfor some constant c. DeﬁneI0= [0;2\f)andIi= [2i\f;2i+1\f),\nH 0.\n4:while seeing edges adjacent to yin the stream do\n5: For allab2Sj\ni: ifa;b2N(y)thenCab Cab+ 1\n6: foreach incident edge yxdo\n7: ifxy2Sj\nithen\n8:Aj\ni Aj\ni+Cyx.\n9: else\n10: Let^Ryxbe the predicted value of Ryx.\n11: Searchisuch that (^Ryx+\f)2Ii\n12: For eachjletSj\ni Sj\ni[fyxgwith probability pi.\n13:fori= 0toO(logn)do\n14:H H+ median(Aj\ni)=pi\n15:return:H.\nForVar[Y], recall that for an edge esuch thatRe\u00152i+1\f, we have Pr[e2Ei]\u0014Kq(e)\nRe\u0014\nK2i+1\f\nRe. Then, condition on the oracle O\u0012, we have\nE[Var[YjO\u0012]]\u0014piX\nRe\u00152i+1\fK2i+1\f\nReR2\ne=piK2i+1\fX\nRe\u00152i+1\fRe=piK2i+1\fT:\nAnd\nVar[E[YjO\u0012]]<piX\nRe\u00152i+1\fK2i+1\f\nReR2\ne=piK2i+1\fT:\nFrom Var[Y] =E[Var[YjO\u0012]] +Var[E[YjO\u0012]]we get that Var[Y]\u00142piK2i+1\fT, from which\nwe can get that Var[Ai=pi] =1\np2\niVar[Ai] =O(K(\u000f=logn)2T2). Using Chebyshev’s inequality\nand taking a median over O(log logn)independent trials, we can get Xi= median(Aj\ni)satisﬁes\njXi\u0000P\ne2EiRej\u0014p\nK(\u000f=logn)Twith probability 1\u0000O(1=logn). A similar argument also\nholds forI0, which means that after taking a union bound, with probability 9=10, the output of the\nalgorithm 3 is a (1\u0006p\nK\u0001\u000f)- approximation of T.\nNow we analyze the space complexity of Algorithm 3.\nFori\u00151, we have\nE[jEij] =E2\n4X\nRe\u00142i\u00001\f=\u000b[e2Ei] +X\nRe>2i\u00001\f=\u000b[e2Ei]3\n5\u0014E2\n4X\nRe\u00142i\u00001\f=\u000b[e2Ei]3\n5+T\u000b\n2i\u00001\f\n\u0014X\nRe\u00142i\u00001\f=\u000b2KRe\np(e)+T\u000b\n2i\u00001\f\u0014X\nRe\u00142i\u00001\f=\u000bKRe\n2i\u00001\f+T\u000b\n2i\u00001\f\u0014(K+ 1)T\u000b\n2i\u00001\f;\nandjE0j \u0014m, hence we have that the total expectation of the space isP\nipiE[jEij]=\nO(K\n\u000f2\u0000\n\u000b(logn)3log logn+m\f=T\u0001\n).\nC O MITTED PROOFS FROM SECTION 3\nWe ﬁrst present Algorithm 4, and then continue to prove Theorem 1.2. In the following algorithm,\nfor two sets A;B of edges, we let wA;Brepresent the set of pairs of edges (u;v), whereu2A;v2\nB, andu;vshare a common vertex.\n22\n\nPublished as a conference paper at ICLR 2022\nAlgorithm 4 Counting Triangles in the Arbitrary Order Model\n1:Input: Arbitrary order edge stream and an oracle O\u001athat outputs HEAVY ifNxy>\u001aand LIGHT\notherwise.\n2:Output: An estimate of the triangle count T.\n3:Initialize\u001a= max\b\u000fTpm;1\t\n,p=Cmax\b1\n\u000fp\nT;\u001a\n\u000f2\u0001T\t\nfor a constant C.\n4:Initialize`1;`2;`3= 0, andSL;H=;.\n5:forevery arriving edge ein the stream do\n6: ifthe oracleO\u001aoutputs HEAVY then\n7: AddetoH\n8: else\n9: AddetoSLwith probability p.\n10: foreachw2wSL;SLdo\n11: if(e;w)is a triangle then\n12: Increment`1=`1+ 1.\n13: foreachw2wSL;Hdo\n14: if(e;w)is a triangle then\n15: Increment`2=`2+ 1.\n16: foreachw2wH;H do\n17: if(e;w)is a triangle then\n18: Increment`3=`3+ 1.\n19:return:`=`1=p2+`2=p+`3.\nC.1 P ROOF OF THEOREM 1.2\nRecall that we ﬁrst assume that Algorithm 4 is given access to a perfect oracle O\u001a. That is, for every\nedgee2E,\nO\u001a(e) =\u001a\nLIGHT ifTe\u0014\u001a\nHEAVY ifTe>\u001a;\nwhereTeas the number of triangles incident to the edge e.\nTheorem 1.2. First, we assume that T\u0015\u000f\u00002. If not, our algorithm can just store all of the edges,\nsincem\u0014\u000f\u00001\u0001m=p\nT:\nFor each integer i\u00151;letE(i)denote the set of edges that are part of exactly itriangles, and let\nEi=jE(i)j. Since each triangle has 3edges, we have thatP\ni\u00151i\u0001E(i) = 3T:\nLetT1denote the set of triangles whose ﬁrst two edges in the stream are light (according to O\u001a).\nFor every triangle tiinT1, let\u001fidenote the indicator of the event that the ﬁrst two edges of ti\nare sampled to SL, and let`1=P\ni\u001fi. Since each \u001fiis1with probability p2,Ex[\u001fi] =p2, and\nVar[\u001fi] =p2\u0000p4\u0014p2. For any two variables \u001fi;\u001fj;they must be uncorrelated unless the triangles\nti;tjshare a light edge that is among the ﬁrst two edges of tiand among the ﬁrst two edges of tj.\nMoreover, in this case, Cov[\u001fi;\u001fj]\u0014Ex[\u001fi\u001fj] =P(\u001fi=\u001fj= 1):This event only happens if\nthe ﬁrst two edges of both tiandtjare sampled in SL, but due to the shared edge, this comprises 3\nedges in total, so P(\u001fi=\u001fj= 1) =p3. Hence, if we deﬁne t12\nias the ﬁrst two edges of tiin the\nstream for each triangle ti,\nEx[`1] =p2jT1j;\nand\nVar[`1]\u0014X\nti2T1p2+X\nti;tj2T1\nt12\ni\\t12\nj6=;p3\u0014p2jT1j+p3X\nTe\u0014\u001aT2\ne\n=p2jT1j+p3\u0001\u001aX\nt=1t2\u0001E(t)\u0014(p2+ 3p3\u0001\u001a)\u0001T :\nThe ﬁrst line follows by adding the covariance terms; the second line follows since each light edge e\nhas at mostT2\nepairs (ti;tj)intersecting at e; the third line follows by summing over t=Te, ranging\nfrom 1to\u001a, instead over e; and the last line follows sinceP\nt\u00151t\u0001E(t) = 3T.\n23\n\nPublished as a conference paper at ICLR 2022\nNow, letT2denote the set of triangles whose ﬁrst two edges in the stream are light and heavy (in\neither order). For every triangle t= (e1;e2;e3)inT2,e1;e2are sampled to SL[Hwith probability\np. Also all other triangles have no chance to contribute to `2. Hence, Ex[`2] =p\u0001T2. Also,\ntwo triangles ti;tj2T 2that intersect on an edge ehave Cov(\u001fi;\u001fj)6= 0 only ifeis light and\ne2t12\ni;t12\nj. In this case.\nCov(\u001fi;\u001fj)\u0014Ex[\u001fi\u001fj]\u0014p;\nsince ifeis added toSL(which happens with probability p) then\u001fi=\u001fj= 1as the other edges in\nt12\ni;t12\njare heavy and are added to Hwith probability 1. Therefore,\nVar[`2] =X\nti2T2p+X\nti;tj2T2\nt12\ni\\t12\nj=e;O\u001a(e)=LIGHTp\u0014(p+ 3p\u0001\u001a)\u0001T;\nby the same calculations as was done to compute Var[`1]:\nFinally, letT3denote the set of triangles whose ﬁrst two edges in the stream are heavy. Then\n`3=jT3j.\nNow, using the well known fact that Var[X+Y]\u00142(Var[X]+Var[Y])for any (possibly correlated)\nrandom variables X;Y; we have that\nVar[p\u00002`1+p\u00001`2+`3]\u00142\u0000\np\u00004(p2+ 3p3\u001a)T+p\u00002(p+ 3p\u001a)T\u0001\n\u00144T(p\u00002+ 3p\u00001\u001a);\nsince`3has no variance. Moreover, Ex[p\u00002`1+p\u00001`2+`3] =jT1j+jT2j+jT3j=T. So, by\nChebyshev’s inequality, since `=`1=p2+`2=p+`3;\nPr[j`\u0000Tj>\u000fT ]<Var[`]\n\u000f2\u0001T2<4(p\u00002+ 3p\u00001\u001a)\n\u000f2\u0001T:\nTherefore, setting p=C\u0001maxn\n1=(\u000f\u0001p\nT);\u001a=(\u000f2\u0001T)o\nfor some ﬁxed constant Cimplies that,\nwith probability at least 2=3,`is a(1\u0006\u000f)-multiplicative estimate of T.\nFurthermore, the expected space complexity is O(mp+H) =O(mp+T=\u001a). Setting\u001a=\nmaxf\u000fT=pm;1gimplies that the space complexity is O(\u000f\u00001m=p\nT+\u000f\u00002m=T +\u000f\u00001pm\u0001T=T) =\nO(\u000f\u00001m=p\nT+\u000f\u00001pm\u0001T=T), since we are assuming that T\u0015\u000f\u00002. Hence, assuming that Tis a\nconstant factor approximation of T, the space complexity is O(\u000f\u00001m=p\nT+\u000f\u00001pm):\nC.2 P ROOF OF THEOREM 1.2 FOR THE K-NOISY ORACLE\nIn this section, we prove Theorem 1.2 for the case that the given oracle is a K-noisy oracle, as\ndeﬁned in Deﬁnition 1.1. That is, we prove the following:\nTheorem C.1. Suppose that the oracle in Algorithm 4 is a K-noisy oracle as deﬁned in Deﬁni-\ntion 1.1 for a ﬁxed constant K. Then with probability 2=3, Algorithm 4 returns `2(1\u0006\u000f)T, and\nuses space at most O\u0010\n\u000f\u00001\u0010\nm=p\nT+pm\u0011\u0011\n.\nRecall that in Deﬁnition 1.1, we deﬁned a K-noisy oracle if the following holds. For every edge e,\n1\u0000K\u0001\u001a\nNe\u0014Pr[O\u001a(e) = HEAVY ]\u0014K\u0001Ne\n\u001a. We ﬁrst visualize this model. To visualize this error\nmodel, in Figure 3 we have plotted Neversus the rangeh\n1\u0000K\u0001\u001a\nNe;K\u0001Ne\n\u001ai\nforK= 2. We set\nNeto vary on a logarithmic scale for clarity. For example, if Neexceeds the threshold \u001aby a factor\nof2, there is no restriction on the oracle output, whereas if Neexceeds\u001aby a factor of 4, then the\noracle must classify the edge as heavy with probability at least 0:5. In contrast, the blue piece-wise\nline shows the probability Pr[O\u001a(e) = HEAVY ]if the oracle O\u001ais a perfect oracle.\nProof of Theorem C.1. We follow the proof of Theorem 4.1 from the main paper. Let T1be the set\nof triangles such that their ﬁrst two edges in the stream are light according to the oracle. Let T2\nbe the set of triangles such that their ﬁrst two edges in the stream are determined light andheavy\naccording to the oracle. Finally, let T3be the set of triangles for which their ﬁrst two edges are\n24\n\nPublished as a conference paper at ICLR 2022\nFigure 3: Plot of Ne, the number of triangles containing edge e, versus the allowed oracle probability\nrange Pr[O\u001a(e) = HEAVY ], shaded in orange. The blue piece-wise line shows the probability\nPr[O\u001a(e) = HEAVY ]if the oracle O\u001ais a perfect oracle.\ndetermined heavy by the oracle. Furthermore, we deﬁne \u001fifor each triangle ti,t(e)for each edge\ne, andE(i);Eifor eachi\u00151as done in Theorem 4.1. Finally, we deﬁne L(i)as the set of deemed\nlight edges that are part of exactly itriangles, and L(i) =jL(i)j:\nFirst, note that the expectation (over the randomness of O\u001a) ofL(i)is at mostE(i)\u0001K\u0001\u001a\ni, since\nour assumption on Pr[O\u001a(e) = heavy ]tells us that Pr[O\u001a(e) = light]\u0014K\u0001\u001a\niift(e) =i.\nTherefore, by the same computation as in Theorem 4.1, we have that, conditioning on the oracle,\nEx[`1jO\u001a] =p2jT1]and\nVar[`1jO\u001a] =X\nti2T1p2+X\nti;tj2T1\nt12\ni\\t12\nj6=;p3\n\u0014p2jT1j+p3X\nelightt(e)2\n=p2jT1j+p3\u0001X\nt\u00151t2\u0001L(t)\nBut since ExO\u001a[L(t)]\u0014K\u0001\u001a\nt\u0001E(t)for allt, we have that\nExO\u001a[Var[`1jO\u001a]]\u0014ExO\u001a\u0014\np2\u0001jT1j+p3\u0001X\nt\u00151t2L(t)\u0015\n=p2jT1j+K\u001a\u0001p3X\nt\u00151t\u0001E(t)\n\u0014(p2+ 3Kp3\u0001\u001a)\u0001T:\nA similar analysis to the above shows that Ex[`2] =p\u0001jT2jand that\nExO\u001a[Var[`2jO\u001a]]\u0014ExO\u001a\u0014\np\u0001jT2j+p\u0001X\nt\u00151t2L(t)\u0015\n=pjT2j+K\u001a\u0001pX\nt\u00151t\u0001E(t)\n\u0014(p+ 3Kp\u0001\u001a)\u0001T:\nHence, as in Theorem 4.1, we have Ex[`jO\u001a] =TandExO\u001a[Var[`jO\u001a]]\u00144T\u0001(p\u00002+ 3Kp\u00001\u001a):\nThus, Ex[`] =Ex[Ex[`jO\u001a]] =TandVar[`] =ExO\u001a[Var[`jO\u001a]]+VarO\u001a[Ex[`jO\u001a]]\u00144T\u0001(p\u00002+\n3Kp\u00001\u001a)by the laws of total expectation/variance. Therefore, since Kis a constant, the variance is\n25\n\nPublished as a conference paper at ICLR 2022\nthe same as in Theorem 4.1, up to a constant. Therefore, we still have that `2(1\u0006O(\u000f))\u0001Twith\nprobability at least 2=3:\nFinally, the expected space complexity is bounded by\nmp+X\ne2mPr[O\u001a(e) =heavy ]\u0014mp+X\nt\u00151E(t)\u0001K\u0001t\n\u001a\n=mp+K\n\u001a\u0001X\nt\u00151E(t)\u0001t\n=O(mp+T=\u001a);\nsinceP\nt\u00151E(t)\u0001t= 3TandK=O(1). Hence, setting \u001aandpas in the proof of Theorem 4.1\nimplies that the returned value is a (1\u0006\u000f)-approximation of T, and the the space complexity is\nO(\u000f\u00001(m=p\nT+pm))as before.\nC.3 L OWER BOUND\nIn this section, we prove a lower bound for algorithms in the arbitrary order model that have access\nto a perfect heavy edge oracle. Here heavy means Nuv\u0015T=cfor a pre-determined threshold c, and\nwe assumec=o(m)andT=c> 1, as otherwise the threshold will be too small or too close to the\naverage to give an accurate prediction. The following theorem shows an \n(min(m=p\nT;m3=2=T))\nlower bound even with such an oracle.\nTheorem C.2. Suppose that the threshold of the oracle c=O(mq), where 0\u0014q <1. Then for\nanyTandm,T\u0014m, there exists m0= \u0002(m)andT0= \u0002(T)such that any one-pass arbi-\ntrary order streaming algorithm that distinguishes between m0edge graphs with 0andT0triangles\nwith probability at least 2=3requires \n(m=p\nT)space. For any Tandm,T= \n(m1+\u000e)where\nmax(0;q\u00001\n2)\u0014\u000e<1\n2, there exists m0= \u0002(m)andT0= \u0002(T)such that any one-pass arbitrary\norder streaming algorithm that distinguishes between m0edge graphs with 0andT0triangles with\nprobability at least 2=3requires \n(m3=2=T)space.\nWhenT\u0014m, we consider the hubs graph mentioned in Kallaugher & Price (2017).\nDeﬁnition C.3 (Hubs Graph, Kallaugher & Price (2017)) .The hubs graph Hr;dconsists of a single\nvertexvwith2rdincident edges, and dedges connecting disjoint pairs of v’s neighbors to form\ntriangles.\nIt is easy to see that in Hr;d, each edge has at most one triangle. Hence, there will not be any heavy\nedges of this kind in the graph. In Kallaugher & Price (2017), the authors show an \n(rp\nd)lower\nbound for the hubs graph.\nLemma C.4 (Kallaugher & Price (2017)) .Givenrandd, there exist two distributions G1;G2on\nsubgraphsG1andG2ofHr;d, such that any algorithm which distinguishes them with probability at\nleast 2=3requires \n(rp\nd)space, where Gihas\u0002(rd)edges andT(G1) =d;T(G2) = 0 .\nNow, givenTandm, whereT\u0014m, we letr= \u0002(m=T )andd=T. We can see Hr;dhas\u0002(m)\nedges andTtriangles, and we need \n(rp\nd) = \n(m=p\nT)space.\nWhenT >m , we consider the following \n(m3=2=T)lower bound in Bera & Chakrabarti (2017).\nLetHbe a complete bipartite graph with bvertices on each side (we denote the two sides of vertices\nbyAandB). We add an additional Nvertex blocks V1;V2;:::VNwith eachjVij=d. Alice has\nanN-dimensional binary vector x. Bob has an N-dimensional binary vector y. Bothxandyhave\nexactlyN=3coordinates that are equal to 1. Then, we deﬁne the edge sets\nEAlice=[\ni:xi=1ffu;vg;u2A;v2Vig\nand\nEBob=[\ni:yi=1ffu;vg;u2B;v2Vig:\n26\n\nPublished as a conference paper at ICLR 2022\nLet the ﬁnal resulting graph be denoted by G= (V;E)whereV=VH[V1[:::[VNand\nE=EH[EAlice[EBob.\nIn Bera & Chakrabarti (2017), the authors show by a reduction to the DISJN=3\nNproblem in commu-\nnication complexity, that we need \n(N)space to distinguish the case when Ghas0triangles from\nthe case when Ghas at leastb2dtriangles.\nGivenmandT,T= \u0002(m1+\u000e)where 1\u0014\u000e < 1=2, as shown in Bera & Chakrabarti (2017),\nwe can setb=Nsandd=Ns\u00001wheres= 1=(1\u00002\u000e). This will make m= \u0002(N2s)and\nT= \u0002(N3s\u00001), which will make the \n(m3=2=T)lower bound hold. Note that at this time each\nedge will have an O(1\nm)-fraction of triangles or an O(1\nm1\u00001\n2s)-fraction of triangles. Assume the\nthreshold of the oracle c=O(mq). When\u000e\u0015max(0;q\u00001\n2), there will be no heavy edges in the\ngraph.\nTheorem C.2 follows from the above discussion.\nD F OUR-CYCLE COUNTING IN THE ARBITRARY ORDER MODEL\nIn the 4-cycle counting problem, for each xy2E(G), deﬁneNxy=jz;w:fx;y;z;wg2\u0003jas\nthe number of 4-cycles attached to edge xy.\nIn this section, we present the one-pass eO(T1=3+\u000f\u00002m=T1=3)space algorithm behind Theorem 1.3\nand the proof of the theorem.6\nWe begin with the following basic idea.\n• Initialize: C 0;S ;. On the arrival of edge uv:\n–With probability p,S fuvg[S.\n–C C+jffw;zg:uw;wz andzv2Sgj.\n• ReturnC=p3.\nFollowing the analysis in V orotnikova (2020), we have\nVar[C]\u0014O(Tp3+T\u0001Ep5+T\u0001Wp4);\nwhere \u0001Eand\u0001Wdenote the maximum number of 4-cycles sharing a single edge or a single\nwedge.\nThere are two important insights from the analysis: ﬁrst, this simple sampling scheme can output\na good estimate to the number of 4-cycles on graphs that do not have too many heavy edges and\nheavy wedges (the deﬁnitions of heavy will be shown later). Second, assuming that we can store all\nthe heavy edges, then at the end of the stream we can also estimate the number of 4-cycles that have\nexactly one heavy edge but do not have heavy wedges.\nFor the 4-cycles that have heavy wedges, we use an idea proposed in McGregor & V orotnikova\n(2020); V orotnikova (2020): for any node pair uandv, if we know their number of common neigh-\nbors (denoted by k), we can compute the exact number of 4-cycles with u;vas a diagonal pair (i.e.,\nthe four cycle contains two wedges with the endpoints u;v), and this counts to\u0000k\n2\u0001\n. Furthermore,\nifkis large (in this case we say all the wedges with endpoints u;vare heavy), we can detect it and\nobtain a good estimation to kby a similar vertex sampling method mentioned in Section 3. We can\nthen estimate the total number of 4-cycles that have heavy wedges with our samples.\nAt this point, we have not yet estimated the number of 4-cycles having more than one heavy edge\nbut without heavy wedges. However, McGregor & V orotnikova (2020) shows this class of 4-cycles\nonly takes up a small fraction of T, and hence we can get a (1\u0006\u000f)-approximation to Teven without\ncounting this class of 4-cycles.\nThe reader is referred to Algorithm 5 for a detailed version of our one-pass, 4-cycle counting algo-\nrithm. Before the stream, we randomly select a node set S, where each vertex is in Swith probability\n6We assume1\nT1=6\u0014O(\u000f), which is the same assumption as in previous work\n27\n\nPublished as a conference paper at ICLR 2022\nAlgorithm 5 Counting 4-cycles in the Arbitrary Order Model\n1:Input: Arbitrary Order Stream and an oracle that outputs TRUE ifNxy\u0015T=\u001a and FALSE\notherwise.\n2:Output: An estimate of the number Tof 4-cycles.\n3:InitializeAl 0,Ah 0,Aw 0EL ;EH ;,ES ;, andW ;. Set\u001a T1=3,\np \u000b\u000f\u00002logn=\u001a for a sufﬁciently large \u000b, and LetSbe a random subset of nodes such that\neach node is in Swith probability p.\n4:while seeing edge uvin the stream do\n5: ifu2Sorv2Sthen\n6:ES fuvg[ES.\n7: ifthe oracle outputs FALSE then\n8: With probability p,EL fuvg[EL.\n9: Find pair (w;z)such thatuw;wz; andzv2EL, letD D[f(u;w;z;v )g.\n10: else\n11:EH fuvg[EH.\n12:foreach node pair (u;v)do\n13: letq(u;v)be the number of wedges with center in Sand endpoints uandv.\n14: ifq(u;v)\u0015pT1=3then\n15:Aw Aw+\u0000q(u;v)\n2\u0001\n.\n16:W W[f(u;v)g.\n17:foreach 4-cycle dinDdo\n18: ifthe end points of wedges in dare not inWthen\n19:Al Al+ 1\n20:foreach edgeuvinEHdo\n21: foreach 4-cycle dformed with uvande2ELdo\n22: ifthe end points of wedges in dare not inWthen\n23:Ah Ah+ 1\n24:return:Al=p3+Ah=p3+Aw.\npindependently, and we later store all edges that are incident to Sduring the stream. In the mean-\ntime, we deﬁne the edge uvto be heavy ifNuv\u0015T2=3and train the oracle to predict whether uv\nis heavy or not when we see the edge uvduring the stream. Let p=eO(\u000f\u00002=T1=3). We store all\nthe heavy edges and sample each light edge with probability pduring the stream. Upon seeing see\na light edge uv, we look for the 4-cycles that are formed by uvand the light edges that have been\nsampled before, and then record them in set D. At the end of the stream, we ﬁrst ﬁnd all the node\npairs that share many common neighbors in Sand identify them as heavy wedges. Then, for each\n4-cycled2D, we check if dhas heavy wedges, and if so, remove it from D. Finally, for each\nheavy edgeuvindicated by the oracle, we compute the number of 4-cycles that are formed by uv\nand the sampled light edges, and without heavy wedges. This completes all parts of our estimation\nprocedure.\nWe now prove Theorem 1.3, restated here for the sake of convenience.\nTheorem 1.3. There exists a one-pass algorithm, Algorithm 5, with space complexity eO(T1=3+\n\u000f\u00002m=T1=3)in the arbitrary order model that, using a learning-based oracle, returns a (1\u0006\u000f)-\napproximation to the number Tof four cycles with probability at least 7=10.\nProof. From the Lemma 3 in V orotnikova (2020), we know that with probability at least 9=10, we\ncan get an estimate Awsuch that\nAw=Tw\u0006\u000fT:\nWhereTwis the number of the 4-cycles that have at least one heavy wedge. We note that if a 4-\ncycle has two heavy wedges, it will be counted twice. However, V orotnikova (2020) shows that this\ndouble counting is at most O(T2=3) =O(\u000f)T.\nFor the edge sampling algorithm mentioned in D, from the analysis in V orotnikova (2020), we have\nVar[Al=p3]\u0014O(T=p3+T\u0001E=p+T\u0001W=p2):\n28\n\nPublished as a conference paper at ICLR 2022\nNotice that in our algorithm, the threshold of the heavy edges and heavy wedges are Nuv\u0015T2=3\nandNw\u0015T1=3, respectively, which means Var[Al\np3] =O(\u000f2T2). Hence, we can get an estimate Al\nsuch that with probability at least 9=10,\nAl=Tl\u0006\u000fT ;\nwhereTlis the number of 4-cycles that have no heavy edges. Similarly, we have with probability at\nleast9=10,\nAh=Th\u0006\u000fT ;\nwhereThis the number of 4-cycles that have at most one heavy edge.\nOne issue is that the algorithm has not yet estimated the number of 4-cycles having more than one\nheavy edge, but without heavy wedges. However, from Lemma 5.1 in McGregor & V orotnikova\n(2020) we get that the number of this kind of 4-cycles is at most O(T5=6) =O(\u000f)T. Hence putting\neverything together, we have\njAl=p3+Ah=p3+Aw\u0000Tj\u0014O(\u000f)T ;\nwhich is what we need.\nNow we analyze the space complexity. The expected number of light edges we sample is\nO(mp) =eO(\u000f\u00002m=T1=3)and the expected number of nodes in Sand edges in ESisO(2mp) =\neO(\u000f\u00002m=T1=3). The expected number of 4-cycles we store in DisO(Tp3) =eO(\u000f\u00006). We store\nall the heavy edges, and the number of heavy edges is at most O(T1=3). Therefore, the expected\ntotal space iseO(T1=3+\u000f\u00002m=T1=3).\nE R UNTIME ANALYSIS\nIn this section, we verify the runtimes of our Algorithms 1, 2, 3, 4, and 5.\nProposition E.1. Algorithm 1 runs in expected time at most eO\u0000\nmin(\u000f\u00002m5=3=T1=3;\u000f\u00001m3=2)\u0001\nin\nthe setting of Theorem 1.1, or eO\u0000\nmin(\u000f\u00002m5=3=T1=3;K\u0001\u000f\u00001m3=2)\u0001\nin the setting of Theorem 1.4.\nProof. For each edge ab2SL[SM, we check whether we see both vaandvb(lines 9-10) when\nlooking at edges adjacent to v. So, this takes time jSLj+jSMjper edge in the stream. We similarly\ncheck for each edge in Saux(lines 11-12), so this takes time jSauxj. Finally, for each edge vu(line\n13), we note that lines 14-17 can trivially be implemented in O(1)time, along with 1call to the\noracle. The remainder of the oracle simply involves looking through the set SauxandSMto search\nor count for elements. Thus, the overall runtime is O(jSLj+jSMj+jSauxj)per stream element, so\nthe runtime is O(m\u0001(jSLj+jSMj+jSauxj)).\nThis is at most O(m\u0001S), whereSis the space used by the algorithm. So, the run-\ntime iseO\u0000\nmin(\u000f\u00002m5=3=T1=3;\u000f\u00001m3=2\u0001\nin the setting of Theorem 1.1, and is at most\neO\u0000\nmin(\u000f\u00002m5=3=T1=3;K\u0001\u000f\u00001m3=2)\u0001\nin the setting of Theorem 1.4.\nProposition E.2. Algorithm 2 runs in time eO(\u000f\u00002(\u000b+m\f=T )m).\nProof. For each edge ab2Sifor each 0\u0014i\u0014c\u000f\u00002, we check whether we see both yaandyb\nin the stream when looking at edges adjacent to y. So, lines 7-8 take time O(PjSij)for each edge\nwe see in the stream. By storing each Si(and eachQi) in a balanced binary search tree or a similar\ndata structure, we can search for xy2Siin timeO(logn)in line 11, and it is clear that lines\n12-16 take time O(logn)(since insertion into the data structure for Qi;Sican take time O(logn)).\nSince we do this for each ifrom 0toc\u000f\u00002and for each incident edge yxwe see, in total we spend\nO(\u000f\u00002logn+PjSij)time up to line 16. The remainder of the lines take time O(\u000f\u00002\u0001mlogn),\nsince the slowest operation is potentially deleting an edge from each Siand a value from each Qi\nup tomtimes (for each edge).\nOverall, the runtime is eO(\u000f\u00002\u0001m+m\u0001PjSij) =eO(m\u0001S), whereSis the total space used by the\nalgorithm. Hence, the runtime is eO(\u000f\u00002(\u000b+m\f=T )m).\n29\n\nPublished as a conference paper at ICLR 2022\nProposition E.3. Algorithm 3 runs in time eO(K\u000f\u00002(\u000b+m\f=T )m).\nProof. For each edge ab2Sj\nifor each 1\u0014i\u0014O(logn);1\u0014j\u0014O(log logn), we check whether\nwe see both yaandybin the stream when looking at edges adjacent to y. So, line 5 takes time\nO(PjSj\nij)for each edge we see in the stream. By storing each Sj\niin a balanced binary search tree\nor a similar data structure, we can search for xy2Siin timeO(logn)in line 7, and it is clear that\nlines 8-12 take time O(logn)(since insertion into the data structure for Qj\nican take time O(logn)).\nSince we do this for each ifrom 0toc\u000f\u00002and for each incident edge yxwe see, in total we spend\npolylogn\u0001O(PjSij)time up to line 12. Finally, lines 13-15 take time poly logn.\nOverall, the runtime is eO(m\u0001PjSij) =eO(m\u0001S), whereSis the total space used by the algorithm.\nHence, the runtime is eO(K\u000f\u00002(\u000b+m\f=T )m).\nProposition E.4. Algorithm 4 runs in time eO\u0010\n\u000f\u00001\u0001m2=p\nT+\u000f\u00001\u0001m3=2\u0011\n:\nProof. For each edge ein the stream, ﬁrst we check the oracle, and we either add etoHor toSLwith\nprobabilityp(lines 6-9), which take O(1)time. The remaining lines (lines 10-18) involve operations\nthat takeO(1)time for each wwhich represents a pair (e1;e2)of edges where e1;e22SL[H\nand(e;e1;e2)forms a triangle. So, the runtime is bounded by the amount of time it takes to ﬁnd all\ne1;e22SL[Hsuch that (e;e1;e2)forms a triangle. If the edge e= (u;v), we just ﬁnd all edges\ninSL[Hadjacent tou, and all edges in SL[Hadjacent tov. Then, we sort these edges based on\ntheir other endpoint and match the edges if they form triangles. So, the runtime is eO(jSLj+jHj)\nper edgeein the stream. Finally, line 19 takes O(1) time.\nOverall, the runtime is eO(m\u0001(jSLj+jHj)) =eO(m\u0001S)whereSis the total space of the algorithm.\nSo, the runtime is eO\u0010\n\u000f\u00001\u0001m2=p\nT+\u000f\u00001\u0001m3=2\u0011\n:\nProposition E.5. Algorithm 5 runs in time eO\u0000\n\u000f\u00002=T1=3\u0001(n3+m2)\u0001\n.\nProof. We note that for each edge uvin the stream (line 4), the code in the loop (lines 5-11) can\nbe implemented using 1oracle call and eO(jELj)time. The only nontrivial step here is to ﬁnd pairs\n(w;z)such thatuw;wz;zv are all inEL. However, by storing ELin a balanced binary search tree\nor similar data structure, one can enumerate through each edge wz2ELand determine if uwand\nzvare inELinO(logjELj)time. So, lines 4-11 take time eO(jELj)per stream element.\nNext, lines 12-16 can easily be implemented in time O(n2\u0001jSj), lines 17-19 can be easily imple-\nmented in time eO(jDj)if the vertices in Ware stored properly, and lines 20-23 can be done in\neO(jEHj\u0001jELj)time. The last part is true since we check each uv2EHande= (u0;v0)2EL,\nand then check if u;u0;v0;vform a 4-cycle by determining if u;u0andv;v0are inEL[EH(which\ntakes timeO(logjELj+ logjEHj)time assuming EL;EHare stored in search tree or similar data\nstructure).\nOverall, we can bound the runtime as eO(m\u0001jELj+n2\u0001jSj+jDj+jEHj\u0001jELj) =eO(m\u0001jELj+n2\u0001jSj).\nAs each edge is in ELwith probability at most pand each edge is in Swith probability at most p,\nthe total runtime is eO\u0000\n(m2+n3)\u0001p\u0001\n=eO\u0000\n\u000f\u00002=T1=3\u0001(n3+m2)\u0001\n.\nF A DDITIONAL EXPERIMENTAL RESULTS\nAll of our graph experiments were done on a CPU with i5 2.7 GHz dual core and 8 GB RAM or\na CPU with i7 1.9 GHz 8 core and 16GB RAM. The link prediction training was done on a single\nGPU.\nF.1 D ATASET DESCRIPTIONS\n•Oregon: 9graphs sampled over 3months representing a communication network of inter-\nnet routers Leskovec & Krevl (2014); Leskovec et al. (2005).\n30\n\nPublished as a conference paper at ICLR 2022\n•CAIDA: 98graphs sampled approximately weekly over 2years, representing a communi-\ncation network of internet routers Leskovec & Krevl (2014); Leskovec et al. (2005).\n•Reddit Hyperlinks: Network where nodes represent sub communities (subreddits) and\nedges represent posts that link two different sub communities Leskovec & Krevl (2014);\nKumar et al. (2018).\n•WikiBooks: Network representing Wikipedia users and pages, with editing relationships\nbetween them Rossi & Ahmed (2015).\n•Twitch: A user-user social network of gamers who stream in a certain language. Vertices\nare the users themselves and the links are mutual friendships between them. Rozemberczki\net al. (2019)\n•Wikipedia: This is a co-occurrence network of words appearing in the ﬁrst million bytes\nof the Wikipedia dump. Mahoney (2011)\n•Synthetic Power law: Power law graph sampled from the Chung-Lu-Vu (CLV) model with\nthe expected degree of the ith vertex proportional to 1=i2Chung et al. (2003). To create this\ngraph, the vertices are ‘revealed’ in order. When the jth vertex arrives, the probability of\nforming an edge between the jth vertex and ith vertex for i<j is proportional to 1=(ij)2.\nF.2 D ETAILS ON LINK PREDICTION ORACLE\nOur oracle for the Twitch and Wikipedia graphs is based on Link Prediction, where the task is to\nestimate the likelihood of the existence of edges or to ﬁnd missing links in the network. Here we\nuse the method and code proposed in Zhang & Chen (2018). For each target link, it will extract\na local enclosing subgraph around a node pair, and use a Graph Neural Network to learn general\ngraph structure features for link prediction. For the Twitch network, we use all the training links\nas the training data for the graph neural network, while for the Wikipedia network, we use 30% of\nthe training links as the training data due to memory limitations. For the Twitch network, our set of\nlinks that we will try to predict are between two nodes that have a common neighbor in the training\nnetwork, but do not form a link in the training network. This is about 3.8 million pairs, and we\ncall this the candidate set. We do this for memory considerations. For the remaining node pairs,\nwe set the probability they form a link to be 0. For the Wikipedia network, we randomly select a\nlink candidate set of size 20times the number of edges (about 1million pairs) from the entire set of\ntesting links. These link candidate sets will be used by the oracle to determine heaviness. Then, we\nuse this network to do our prediction for all links in our candidate sets for the two networks.\nNow we are ready to build our heavy edge oracle. For the adjacency list model, when we see the\nedgeuvin the stream, we know all the neighbors of u, and hence, we only need to predict the\nneighbors of v. Let deg(v)be the degree of vin the training graph. The training graph and testing\ngraph are randomly split. Hence, we can use the training set to provide a good estimation to the\ndegree ofv. Next, we choose the largest deg(v)edges incident to vfrom the candidate set, in terms\nof their predicted likelihood given by the the neural network, as our prediction of N(v), which leads\nto an estimate of Ruv. For the arbitrary order model, we use the same technique as above to predict\nN(u)andN(v), and estimate Nuv.\nF.3 P ARAMETER SELECTION FOR ALGORITHM 2\nWe need to set two parameters for Algorithm 2: p, which is the edge sampling probability, and\n\u0012, which is the heaviness threshold. In our theoretical analysis, we assume knowledge of a lower\nbound onTin order to set pand\u0012, as is standard in the theoretical streaming literature. However, in\npractice, such an estimate may not be available; in most cases, the only parameter we are given is a\nspace bound for the number of edges that can be stored. To remedy this discrepancy, we modify our\nalgorithm in experiments as follows.\nFirst, we assume we only have access to the stream, a space parameter Zindicating the maximum\nnumber of edges that we are allowed to store, and an estimate of m, the number of edges in the\nstream. Given Z, we need to designate a portion of it for storing heavy edges, and the rest for\nstoring light edges. The trade-off is that the more heavy edges we store, the smaller our sampling\nprobability of light edges would be. We manage this trade off in our implementation by reserving\n0:3\u0001Zof the edge ‘slots’ for heavy edges. The constant 0:3isﬁxed throughout all our experiments.\n31\n\nPublished as a conference paper at ICLR 2022\nWe then set p= 0:7\u0001Z=m . To improve the performance of our algorithm, we optimize for space\nusage by always insuring that we are storing exactly Zspace (after observing the ﬁrst Zedges of the\nstream). To do so and still maintain our theoretical guarantees, we perform the following procedure.\nCall the ﬁrst Zedges of the stream the early phase and the rest of the edges the late phase.\nWe always keep edges in the early phase to use our space allocation Zand also keep track of the\n0:3-fraction of the heaviest edges. After the early phase is over, i.e., more than Zedges have passed\nin the stream, if a new incoming edge is heavier than the lightest of the stored heavy edges, we\nreplace the least heavy stored edge with the new arriving edge and re-sample the replaced edge with\nprobabilityp. Otherwise, if the new edge is not heavier than the lightest stored edge, we sample the\nnew incoming edge with probability p. If we exceed Z, the space threshold, we replace one of the\nlight edges sampled in the early phase. Then similarly as before, we re-sample this replaced edge\nwith probability pand continue this procedure until one of the early light edges has been evicted. In\nthe case that there are no longer any of the light edges sampled in the early phase stored, we replace\na late light edge and then any arbitrary edge (again performing the re-sampling procedure for the\nevicted edge).\nNote that in our modiﬁcation, we only require our predictor be able to compare the heaviness be-\ntween two edges, i.e., we do not need the predictor to output an estimate of the number of triangles\non an edge. This potentially allows for more ﬂexibility in the choice of predictors.\nIf the given space bound meets the space requirements of Theorem C.1, then the theoretical guaran-\ntees of this modiﬁcation simply carry over from Theorem 1.2: we always keep the heaviest edges\nand always sample light edges with probability at least p. In case the space requirements are not\nmet, the algorithm stores the most heavy edges as to reduce the overall variance.\nF.4 A DDITIONAL FIGURES FROM ARBITRARY ORDER TRIANGLE COUNTING EXPERIMENTS\nAdditional ﬁgures for our arbitrary order triangle counting experiments are given in Figures 4 and 5.\n01104\n2104\n3104\n4104\n5104\nSpace0.000.050.100.150.200.25Relative Error\nCaida-2007, Graph#25\nOur Alg\nThinkD\nMVV\nWRS\n(a) CAIDA 2007\n01104\n2104\n3104\n4104\n5104\nSpace0.000.020.040.060.080.10Relative Error\nTwitch Graph\nOur Alg\nThinkD\nMVV\nWRS (b) Twitch\nFigure 4: Error as a function of space for various graph datasets.\nF.5 E XPERIMENTAL DESIGN FOR ADJACENCY LISTEXPERIMENTS\nWe now present our adjacencly list experiments. At a high level overview, similarly to the arbitrary\norder experiments, for our learning-based algorithm, we reserve the top 10% of the total space for\nstoring the heavy edges. To do this in practice, we can maintain the heaviest edges currently seen so\nfar and evict the smallest edge in the set when a new heavy edge is predicted by the oracle and we no\nlonger have sufﬁcient space. We also consider a multi-layer sub-sampling version of the algorithm\nin Section B.2. Here we use more information from the oracle by adapting the sub-sampling rates\nof edges based on their predicted value. For more details, see Section F.5. Our results are presented\nin Figure 6 (with additional plots given in Figure 7). Our algorithms soundly outperform the MVV\nbaseline for most graph datasets. We only show the error bars for the multi-layer algorithm and\nMVV for clarity. Additional details follow.\nWe use the same predictor for Nxyin Section 4 as a prediction for Rxy. The experiment is done\nunder a random vertex arrival order. For the learning-based algorithm, suppose Zis the maximum\nnumber of edges that we are allowed to store. we set the k=Z=10edges with the highest predicted\nRuvvalues to be the heavy edges, and store them during the stream (i.e., we use 10% of the total\n32\n\nPublished as a conference paper at ICLR 2022\n01104\n2104\n3104\n4104\nSpace0.000.050.100.150.200.250.30Relative Error\nCaida-2006, Graph#20\nOur Alg\nThinkD\nMVV\nWRS\n(a) CAIDA 2006 , Graph #20\n01104\n2104\n3104\n4104\nSpace0.000.050.100.150.200.250.30Relative Error\nCaida-2006, Graph#40\nOur Alg\nThinkD\nMVV\nWRS (b) CAIDA 2006 , Graph #40\n0 5103\n1104\n1.5104\n2104\nSpace0.000.050.100.150.200.25Relative Error\nOregon, Graph#3\nOur Alg\nThinkD\nMVV\nWRS\n(c) Oregon, Graph #3\n0 5103\n1104\n1.5104\n2104\nSpace0.000.050.100.150.200.25Relative Error\nOregon, Graph#7\nOur Alg\nThinkD\nMVV\nWRS (d) Oregon, Graph #7\nFigure 5: Error as a function of space for various graph datasets.\nspace for storing the heavy edges). For the remaining edges, we use a similar sampling-based\nmethod. Note that it is impossible to know the k-heaviest edges before we see all the edges in the\nstream. However, in the implementation we can maintain the kheaviest edges we currently have\nseen so far, and evict the smallest edge in the set when a new heavy edge is predicted by the oracle.\nWe also consider the multi-layer sub-sampling algorithm mentioned in Section B.2, which uses more\ninformation from the oracle. We notice that in many graph datasets, most of the edges having very\nfew number of triangles attached to. Taking an example of the Oregon and CAIDA graph, only\nabout 3%-5% of edges will satisfy Re\u00155under a random vertex arrival order. Hence, for this\nedges, intuitively we can estimate them using a slightly smaller space.\nFor the implementation of this algorithm(we call it multi-layer version), we use 10% of the total\nspace for storing the top k=Z=10edges, and 70% of the space for sub-sampling the edges that the\noracle predict value is very tiny(the threshold may be slightly different for different datasets, like\nfor the Oregon and CAIDA graph, we set the threshold to be 5). Then, we use 20% of the space for\nsub-sampling the remaining edges, for which we call them the medium edges.\nF.6 F IGURES FROM ADJACENCY LISTTRIANGLE COUNTING EXPERIMENTS\n1104\n2104\n3104\n4104\nSpace0.000.020.040.060.080.100.120.14Relative Error\nCaida-2006 #30\nOur Alg\nOur Alg(multi-layer)\nMVV\n(a) CAIDA 2006\n0.5104\n1104\n1.5104\n2104\n2.5104\n3104\nSpace0.000.020.040.060.080.10Relative Error\nWikipedia\nOur Alg\nOur Alg(multi-layer)\nMVV (b) Wikipedia\n1104\n2104\n3104\n4104\n5104\n6104\nSpace0.000.050.100.150.200.250.30Relative Error\nPowerlaw\nOur Alg\nOur Alg(multi-layer)\nMVV (c) Powerlaw\nFigure 6: Error as a function of space in the adjacency list model.\nAdditional ﬁgures from the adjacency list triangle counting experiments are shown in Figure 7. They\nare qualitatively similar to the results presented in Figure 6 as the multi-layer sampling algorithm is\nsuperior over the MVV baseline for all of our datasets.\n33\n\nPublished as a conference paper at ICLR 2022\n4103\n8103\n1.2104\n1.6104\n2104\nSpace0.000.020.040.060.080.100.120.14Relative Error\nOregon #4\nOur Alg\nOur Alg(multi-layer)\nMVV\n(a) Oregon\n1104\n2104\n3104\n4104\n5104\nSpace0.000.020.040.060.080.100.120.14Relative Error\nCaida-2007 #25\nOur Alg\nOur Alg(multi-layer)\nMVV (b) CAIDA 2007\n1104\n2104\n3104\n4104\n5104\n6104\nSpace0.00.10.20.30.40.50.6Relative Error\nWikibooks\nOur Alg\nOur Alg(multi-layer)\nMVV (c) Wikibooks\n0.5104\n1104\n1.5104\n2104\n2.5104\n3104\nSpace0.000.020.040.060.080.10Relative Error\nTwitch\nOur Alg\nOur Alg(multi-layer)\nMVV\n(d) Twitch\nFigure 7: Error as a function of space for various graph datasets.\nF.7 A CCURACY OF THE ORACLE\nIn this section, we evaluate the accuracy of the predictions the oracle gives in our experiments.\nValue Prediction Oracle : We use the prediction of Nxyin Section 4 as a value prediction for\nRxy(x\u0014sy), under a ﬁxed random vertex arrival order. The results are shown in Figure 8. For a\nﬁxed approximation factor k, we compute the failure probability \u000eof the value prediction oracle as\nfollows:\u000e= #=m, wheremis the number of total edges and #equals to the number of edges e\nthatp(e)\u0015k\u000bRe+\forp(e)\u00141\nkRe\u0000\f, respectively. Here we set \u000b= 1;\f= 10 for all graph\ndatasets.\nWe can see for the smaller side, there are very few edges esuch thatp(e)\u00141\nkRe\u0000\f. This meets\nthe assumption of the exponential decay tail bound of the error. For the larger side, it also meets the\nassumption of the linear decay tail bound on most of the graph datasets.\nF.8 D ETAILS ON ORACLE TRAINING OVERHEAD\nThe overhead of the oracles used in our experiments vary from task to task. For the important\nuse case illustrated by the Oregon and CAIDA datasets in which we are interested in repeatedly\ncounting triangles over many related streams, we can pay a small upfront cost to create the oracle\nwhich can be reused over and over again. Thus, the time complexity of building the oracle can be\namortized over many problem instances, and the space complexity of the oracle is relatively small\nas we only need to store the top 10% of heavy edges from the ﬁrst graph (a similar strategy is used\nin prior work on learning-augmented algorithms in Hsu et al. (2019b)). To give more details, for\nthis snapshot oracle, we simply calculate the Nevalues for all edges only in the ﬁrst graph. We then\nkeep the heaviest edges to form our oracle. Note that this takes polynomial time to train. The time\ncomplexity of using this oracle in the stream is as follows: when an edge in the stream comes, we\nsimply check if it’s among the predicted heavy edges in our oracle. This is a simple lookup which\ncan even be done in constant time using hashing.\nFor learning-based oracles like the linear regression predictor, we similarly need to pay an upfront\ncost to train the model, but the space required to store the trained model depends on the dimension\nof the edge features. For the Reddit dataset with \u0018300features, this means that the storage cost\nfor the oracle is a small fraction of the space of the streaming algorithm. Training of this oracle\ncan be done in polynomial time and can even be computed in a stream via sketching and sampling\n34\n\nPublished as a conference paper at ICLR 2022\n2 4 6 8 10\nApproximation Factor k0.0000.0050.0100.0150.0200.0250.030Failure Probability\nOregon\nlarger side\nsmaller side\n(a) Oregon\n2 4 6 8 10\nApproximation Factor k0.0000.0050.0100.0150.0200.0250.0300.0350.040Failure Probability\nCAIDA-2006\nlarger side\nsmaller side (b) CAIDA 2006\n2 4 6 8 10\nApproximation Factor k0.0000.0050.0100.0150.0200.0250.0300.0350.040Failure Probability\nCAIDA-2007\nlarger side\nsmaller side (c) CAIDA 2007\n2 4 6 8 10\nApproximation Factor k0.0000.0020.0040.0060.0080.010Failure Probability\nWikibooks\nlarger side\nsmaller side\n(d) Wikibooks\n2 4 6 8 10\nApproximation Factor k0.0000.0250.0500.0750.1000.1250.1500.1750.200Failure Probability\nTwitch\nlarger side\nsmaller side (e) Twitch\n2 4 6 8 10\nApproximation Factor k0.000.010.020.030.040.05Failure Probability\nWikipedia\nlarger side\nsmaller side (f) Wikipedia\n2 4 6 8 10\nApproximation Factor k0.000.050.100.150.200.250.300.35Failure Probability\nRandom\nlarger side\nsmaller side\n(g) Powerlaw\nFigure 8: Failure probability as a function of approximation factor kfor various graph datasets.\ntechniques which reduce the number of constraints from m (number of edges) to roughly linear in\nthe number of features.\nOur expected value oracle exploits the fact that our input graph is sampled from the CLV random\ngraph model. Given this, we can explicitly calculate the expected value of Nefor every edge which\nrequires no training time and nearly constant space. For training details for our link prediction\nmodel, see Section F.2. Note that in general, there is a wide and rich family of predictors which can\nbe trained using sublinear space or in a stream such as regression Woodruff (2014), classiﬁcation for\nexample using SVMs Andoni et al. (2020); Rai et al. (2009) and even deep learning modelsGomes\net al. (2019).\nG I MPLICIT PREDICTOR IN PRIOR WORKS\nWe prove that the ﬁrst pass of the two pass triangle counting Algorithm given in Section 3:1of\nMcGregor et al. (2016), satisﬁes the conditions of the K-noisy oracle in Deﬁnition 1.1. Therefore,\nour work can be seen as a generalization of their approach when handling multiple related data sets,\nwhere instead of performing two passes on each data set and using the ﬁrst of which to train the\nheavy edge oracle, we perform the training once according to the ﬁrst related dataset, and we get a\none pass algorithm for all remaining sets.\nWe ﬁrst recall the ﬁrst pass of (McGregor et al., 2016, Section 3.1):\n1. Sample each node zof the graph with probability p=C\u000f\u00002logm=\u001a for some large con-\nstantC > 0. LetZbe the set of sampled nodes.\n35\n\nPublished as a conference paper at ICLR 2022\n2. Collect all edges incident on the set Z.\n3. For any edge e=fu;vg, letet(e) =jfz2Z:u;v2N(z)gjand deﬁne the oracle as\noracle (e) =\u001a\nLIGHT ifet(e)<p\u0001\u001a\nHEAVY ifet(e)\u0015p\u0001\u001a:\nLemma G.1. For any edge e= (u;v),oracle (e) = LIGHT impliesNe\u00142\u001aandoracle (e) =\nHEAVY impliesNe>\u001a=p\n2with failure probability at most 1=n10.\nProof. For any edge e, it follows that et(e)\u0018Bin(Ne;p). Therefore if Ne>2\u001a,\nPr[et(e)<p\u0001\u001a]\u0014exp(\u0000\n(p\u0001\u001a))\u00141=n10\nby pickingClarge enough in the deﬁnition of p. The other case follows similarly.\nLemma G.2. The expected space used by the above oracle is O(pm).\nProof. Each vertex is sampled in Zwith probability pand we keep all of its incident edges. There-\nfore, the expected number of edges saved is O(pP\nvdv) =O(pm).\nNote that the oracle satisﬁes the conditions of the K-noisy oracle in Deﬁnition 1.1. For example, if\nNe\u0015C0\u001aforC0\u001d1, Deﬁnition 1.1 only assumes that we incorrectly classify ewith probability\n1=C0, whereas the oracle presented above incorrectly classiﬁes ewith probability exp(\u0000C0)=n10\nwhich is much smaller than the required 1=C0.\nH L EARNABILITY RESULTS\nIn this section, we give formal learning bounds for efﬁcient predictor learning for edge heaviness.\nIn particular, we wish to say that a good oracle or predictor for edge heaviness and related graph\nparameters can be learned efﬁciently using few samples if we observe graph instances drawn from\na distribution. We can view the result of this section, which will be derived via the PAC learning\nframework, as one formalization of data driven algorithm design. Our results are quite general but\nwe state simple examples throughout the exposition for clarity. Our setting is formally the following.\nSuppose there is an underlying distribution Dwhich generates graph instances H1;H2;\u0001\u0001\u0001all on\nnvertices. Note that this mirrors some of our experimental setting, in particular our graph datasets\nwhich are similar snapshots of a dynamic graph across time.\nOur goal is to efﬁciently learn a good predictor famong some family of functions F. The input\nof eachfis a graph instance Hand the output is a feature vector in kdimensions. The feature\nvector represents the prediction of the oracle and can encapsulate a variety of different meanings.\nOne example is when k=jEjandfoutputs an estimate of edge heaviness for all edges. Another\nis whenk <<jEjandfoutputs the id’s of the kheaviest edges. We also think of each input\ninstanceHas encoded in a vector in Rpforp\u0015\u0000n\n2\u0001\n(for example, each instance is represented as an\nadjacency matrix). Note that we allow for p>\u0000n\n2\u0001\nif for example, each edge or vertex for H\u0018D\nalso has an endowed feature vector.\nTo select the ‘best’ f, we need to precisely deﬁne the meaning of best. Note that in many settings,\nthis involves a loss function which captures the quality of a solution. Indeed, suppose we have a loss\nfunctionL:f\u0002H!Rwhich represents how well a predictor fperforms on some input H. An\nexample ofLcould be squared error from the output of fto the true edge heaviness values of edges\ninH. Note that such a loss function clearly optimizes for predicting the heavy edges well.\nOur goal is to learn the best function f2F which minimizes the following objective:\nEH\u0018D[L(f;H)]: (1)\nLetf\u0003be such the optimal f2F, and assume that for each instance Hand eachf2F,f(H)can\nbe computed in time T(p;k). For example, suppose graphs drawn from Dpossess edge features in\nRd, and that our family Fis parameterized by a single vector \u00122Rdand represents linear functions\n36\n\nPublished as a conference paper at ICLR 2022\nwhich outputs the dot product of each edge feature with \u0012. Then it is clear that T(p;k)is a (small)\npolynomial in the relevant parameters.\nOur main result is the following.\nTheorem H.1. There is an algorithm which after poly (T(p;k);1=\u000f)samples, returns a function ^f\nthat satisﬁes\nEH\u0018D[L(^f;H)]\u0014EH\u0018D[L(f\u0003;H)] +\u000f\nwith probability at least 9=10.\nWe remark that one can boost the probability of success to 1\u0000\u000eby taking additional log(1=\u000e)\nmultiplicative samples.\nThe above theorem is a PAC-style bound which shows that only a small number of samples are\nneeded in order to ensure a good probability of learning an approximately-optimal function ^f. The\nalgorithm to compute ^fis the following: we simply minimize the empirical loss after an appropriate\nnumber of samples are drawn, i.e., we perform empirical risk minimization. This result is proven\nby Theorem H.3. Before introducing it, we need to deﬁne the concept of pseudo-dimension for a\nfunction class which is the more familiar VC dimension, generalized to real functions.\nDeﬁnition H.2 (Pseudo-Dimension, Deﬁnition 9Lucic et al. (2018)) .LetXbe a ground set and F\nbe a set of functions from Xto the interval [0;1]. Fix a setS=fx1;\u0001\u0001\u0001;xng\u001aX , a set of real\nnumbersR=fr1;\u0001\u0001\u0001;rngwithri2[0;1]and a function f2F. The setSf=fxi2Sjf(xi)\u0015\nrigis called the induced subset of Sformed byfandR. The setSwith associated values Ris\nshattered byFifjfSfjf2Fgj = 2n. The pseudo-dimension ofFis the cardinality of the largest\nshattered subset of X(or1).\nThe following theorem relates the performance of empirical risk minimization and the number of\nsamples needed, to the notion of pseudo-dimension. We specialize the theorem statement to our\nsituation at hand. For notational simplicity, we deﬁne Abe the class of functions in fcomposed\nwithL:\nA:=fL\u000ef:f2Fg:\nFurthermore, by normalizing, we can assume that the range of Lis equal to [0;1].\nTheorem H.3 (Anthony & Bartlett (1999)) .LetDbe a distribution over graph instances and A\nbe a class of functions a:H![0;1]with pseudo-dimension dA. Consider ti.i.d. samples\nH1;H2;:::;HtfromD:There is a universal constant c0, such that for any \u000f>0, ift\u0015c0\u0001dA=\u000f2;\nthen we have \f\f\f\f\f1\nttX\ni=1a(Hi)\u0000EH\u0018Da(H)\f\f\f\f\f\u0014\u000f\nfor alla2A with probability at least 9=10:\nThe following corollary follows from the triangle inequality.\nCorollary H.4. Consider a set of tindependent samples H1;:::;HtfromDand let ^abe a function\ninAwhich minimizes1\ntPt\ni=1a(Hi). If the number of samples tis chosen as in Theorem H.3, then\nEH\u0018D[^a(H)]\u0014EH\u0018D[a\u0003(H)] + 2\u000f\nholds with probability at least 9=10.\nThe main challenge is to bound the pseudo-dimension of our given function class A. To do so, we\nﬁrst relate the pseudo-dimension to the VC dimension of a related class of threshold functions. This\nrelationship has been fruitful in obtaining learning bounds in a variety of works such as Lucic et al.\n(2018); Izzo et al. (2021).\nLemma H.5 (Pseudo-dimension to VC dimension, Lemma 10in Lucic et al. (2018)) .For any\na2A, letBabe the indicator function of the region on or below the graph of a, i.e.,Ba(x;y) =\nsgn(a(x)\u0000y). The pseudo-dimension of Ais equivalent to the VC-dimension of the subgraph class\nBA=fBaja2Ag .\nFinally, the following theorem relates the VC dimension of a given function class to its computa-\ntional complexity, i.e., the complexity of computing a function in the class in terms of the number\nof operations needed.\n37\n\nPublished as a conference paper at ICLR 2022\nLemma H.6 (Theorem 8:14in Anthony & Bartlett (1999)) .Letw:R\u000b\u0002R\f!f0;1g, determining\nthe class\nW=fx!w(\u0012;x) :\u00122R\u000bg:\nSuppose that any wcan be computed by an algorithm that takes as input the pair (\u0012;x)2R\u000b\u0002R\f\nand returnsw(\u0012;x)after no more than rof the following operations:\n•arithmetic operations +;\u0000;\u0002;and=on real numbers,\n•jumps conditioned on >;\u0015;<;\u0014;=;and=comparisons of real numbers, and\n•output 0;1,\nthen the VC dimension of WisO(\u000b2r2+r2\u000blog\u000b).\nCombining the previous results allows us prove Theorem H.1. At a high level, we are instantiating\nLemma H.6 with the complexity of computing any function in the function class A.\nProof of Theorem H.1. First by Theorem H.3 and Corollary H.4, it sufﬁces to bound the pseudo-\ndimension of the class A=L\u000eF. Then from Lemmas H.5, the pseudo-dimension of Ais the VC\ndimension of threshold functions deﬁned by A. Finally from Lemma H.6, the VC dimension of the\nappropriate class of threshold functions is polynomial in the complexity of computing a member of\nthe function class. In other words, Lemma H.6 tells us that the VC dimension of BAas deﬁned in\nLemma H.5 is polynomial in the number of arithmetic operations needed to compute the threshold\nfunction associated to some a2A. By our deﬁnition, this quantity is polynomial in T(p;k). Hence,\nthe pseudo-dimension of Gis also polynomial in T(p;k)and the result follows.\nNote that we can consider initializing Theorem H.1 with speciﬁc predictions. If the family of oracles\nwe are interested in is efﬁcient to compute (which is the case of the predictors we employ in our\nexperiments), then Theorem H.1 assures us that only polynomially many samples are required (in\nterms of the computational complexity of our function class), to be able to learn a nearly optimal\noracle. Furthermore, computing the empirical risk minimizer needed in Theorem H.1 is also efﬁcient\nfor a wide verity of function classes. For example in practice, we can simply use gradient descent\nor stochastic gradient descent for a range of predictor models, such as regression or even general\nneural networks.\nWe remark that our learnability result is in similar in spirit to one given in the recent learning-\naugmented paper Dinitz et al. (2021). There, they derive sample complexity learning bounds for\nthe different algorithmic problem of computing matchings in a graph (not in a stream). Since they\nspecialize their analysis to a speciﬁc function class and loss function, their bounds are tighter com-\npared to the possibly loose polynomial bounds we have stated. However, our analysis above is\nmore general as it allows for a variety of predictors and loss functions to measure the quality of the\npredictions.\n38",
  "textLength": 124032
}