{
  "paperId": "6ac386b9f77c3e4d84c06ec8b66475b1a6eada67",
  "title": "Spreading vectors for similarity search",
  "pdfPath": "6ac386b9f77c3e4d84c06ec8b66475b1a6eada67.pdf",
  "text": "Published as a conference paper at ICLR 2019\nSPREADING VECTORS FOR SIMILARITY SEARCH\nAlexandre Sablayrollesy,?, Matthijs Douzey, Cordelia Schmid?, and Herv ´e J´egouy\nyFacebook AI Research?Inria\nABSTRACT\nDiscretizing multi-dimensional data distributions is a fundamental step of modern\nindexing methods. State-of-the-art techniques learn parameters of quantizers on\ntraining data for optimal performance, thus adapting quantizers to the data. In this\nwork, we propose to reverse this paradigm and adapt the data to the quantizer: we\ntrain a neural net which last layer forms a ﬁxed parameter-free quantizer, such as\npre-deﬁned points of a hyper-sphere. As a proxy objective, we design and train a\nneural network that favors uniformity in the spherical latent space, while preserving\nthe neighborhood structure after the mapping. We propose a new regularizer\nderived from the Kozachenko–Leonenko differential entropy estimator to enforce\nuniformity and combine it with a locality-aware triplet loss. Experiments show that\nour end-to-end approach outperforms most learned quantization methods, and is\ncompetitive with the state of the art on widely adopted benchmarks. Furthermore,\nwe show that training without the quantization step results in almost no difference\nin accuracy, but yields a generic catalyzer that can be applied with any subsequent\nquantizer. The code is available online1.\n1 I NTRODUCTION\nRecent work (Kraska et al., 2017) proposed to leverage the pattern-matching ability of machine\nlearning algorithms to improve traditional index structures such as B-trees or Bloom ﬁlters, with\nencouraging results. In their one-dimensional case, an optimal B-Tree can be constructed if the\ncumulative density function (CDF) of the indexed value is known, and thus they approximate this\nCDF using a neural network. We emphasize that the CDF itself is a mapping between the indexed\nvalue and a uniform distribution in [0;1]. In this work, we wish to generalize such an approach to\nmulti-dimensional spaces. More precisely, as illustrated by Figure 1, we aim at learning a function\nthat maps real-valued vectors to a uniform distribution over a d-dimensional sphere, such that a ﬁxed\ndiscretizing structure, for example a ﬁxed binary encoding (sign of components) or a regular lattice\nquantizer, offers competitive coding performance.\nOur approach is evaluated in the context of similarity search, where methods often rely on various\nforms of learning machinery (Gong et al., 2013; Wang et al., 2014b); in particular there is a substantial\nbody of literature on methods producing compact codes (J ´egou et al., 2011a). Yet the problem of\njointly optimizing a coding stage and a neural network remains essentially unsolved, partly because\nFCcatalyzerdiscretization\nBN\nRELU\nFC\nBN\nRELU\nFC\nx2Rdc(x)f(x)`2\nFigure 1: Our method learns a network that encodes the input space Rdinto a code c(x). It is\nlearned end-to-end, yet the part of the network in charge of the discretization operation is ﬁxed in\nadvance, thereby avoiding optimization problems. The learnable function f, namely the “catalyzer”,\nis optimized to increase the quality of the subsequent coding stage.\n1https://github.com/facebookresearch/spreadingvectors\n1arXiv:1806.03198v3  [stat.ML]  30 Aug 2019\n\nPublished as a conference paper at ICLR 2019\ninput \u0015= 0 \u0015= 0:01 \u0015= 0:1 \u0015!1\nFigure 2: Illustration of our method, which takes as input a set of samples from an unknown\ndistribution. We learn a neural network that aims at preserving the neighborhood structure in the input\nspace while best covering the output space (uniformly). This trade-off is controlled by a parameter\n\u0015. The case\u0015= 0keeps the locality of the neighbors but does not cover the output space. On the\nopposite, when the loss degenerates to the differential entropic regularizer ( \u0015!1 ), the neighbors\nare not maintained by the mapping. Intermediate values offer different trade-offs between neighbor\nﬁdelity and uniformity, which is proper input for an efﬁcient lattice quantizer (depicted here by the\nhexagonal lattice A2).\nit is difﬁcult to optimize through a discretization function. For this reason, most efforts have\nbeen devoted to networks producing binary codes, for which optimization tricks exist, such as soft\nbinarization or stochastic relaxation, which are used in conjunction with neural networks (Liong\net al., 2015; Jain et al., 2017). However it is difﬁcult to improve over more powerful codes such as\nthose produced by product quantization (J ´egou et al., 2011a), and recent solutions addressing product\nquantization require complex optimization procedures (Klein & Wolf, 2017; Ozan et al., 2016).\nIn order to circumvent this problem, we propose a drastic simpliﬁcation of learning algorithms\nfor indexing. We learn a mapping such that the output follows the distribution under which the\nsubsequent discretization method, either binary or a more general quantizer, performs better. In other\nterms, instead of trying to adapt an indexing structure to the data, we adapt the data to the index.\nOur technique requires to jointly optimize two antithetical criteria. First, we need to ensure that\nneighbors are preserved by the mapping, using a vanilla ranking loss (Usunier et al., 2009; Chechik\net al., 2010; Wang et al., 2014a). Second, the training must favor a uniform output. This suggests a\nregularization similar to maximum entropy (Pereyra et al., 2017), except that in our case we consider\na continuous output space. We therefore propose to cast an existing differential entropy estimator\ninto a regularization term, which plays the same “distribution-matching” role as the Kullback-Leiber\nterm of variational auto-encoders (Doersch, 2016).\nAs a side note, many similarity search methods are implicitly designed for the range search problem\n(ornear neighbor , as opposed to nearest neighbor (Indyk & Motwani, 1998; Andoni & Indyk, 2006)),\nthat aims at ﬁnding all vectors whose distance to the query vector is below a ﬁxed threshold. For\nreal-world high-dimensional data, range search usually returns either no neighbors or too many. The\ndiscrepancy between near– and nearest– neighbors is signiﬁcantly reduced by our technique, see\nSection 3.3 and Appendix C for details.\nOur method is illustrated by Figure 2. We summarize our contributions as follows:\n\u000fWe introduce an approach for multi-dimensional indexing that maps the input data to an output\nspace in which indexing is easier. It learns a neural network that plays the role of an adapter for\nsubsequent similarity search methods.\n\u000fFor this purpose we introduce a loss derived from the Kozachenko-Leonenko differential entropy\nestimator to favor uniformity in the spherical output space.\n\u000fOur learned mapping makes it possible to leverage spherical lattice quantizers with competitive\nquantization properties and efﬁcient algebraic encoding.\n\u000fOur ablation study shows that our network can be trained without the quantization layer and used\nas a plug-in for processing features before using standard quantizers. We show quantitatively\nthat our catalyzer improves performance by a signiﬁcant margin for quantization-based (OPQ\n(Ge et al., 2013)) and binary (LSH (Charikar, 2002)) method.\nThis paper is organized as follows. Section 2 discusses related works. Section 3 introduces our neural\nnetwork model and the optimization scheme. Section 4 details how we combine this strategy with\nlattice assignment to produce compact codes. The experimental section 5 evaluates our approach.\n2\n\nPublished as a conference paper at ICLR 2019\n2 R ELATED WORK\nGenerative modeling. Recent models such as Generative Adversarial Networks (GANs) (Goodfel-\nlow et al., 2014) or Variational Auto-Encoders (V AEs) (Kingma & Welling, 2013) learn a mapping\nbetween an isotropic Gaussian distribution and the empirical distribution of a training set. Our\napproach maps an empirical input distribution to a uniform distribution on the spherical output space.\nAnother distinction is that GANs learn a unidirectional mapping from the latent code to an image\n(decoder), whereas V AEs learn a bidirectional mapping (encoder - decoder). In our work, we focus\non learning the encoder, whose goal is to pre-process input vectors for subsequent indexing.\nDimensionality reduction and representation learning. There is a large body of literature on the\ntopic of dimensionality reduction, see for instance the review by Van Der Maaten et al. (2009). Rele-\nvant work includes self-organizing maps (Kohonen et al., 2001), the stochastic neighbor embedding\n(Hinton & Roweis, 2003) and the subsequent t-SNE approach (van der Maaten & Hinton, 2008),\nwhich is tailored to low-dimensional spaces for visualisation purposes. Both works are non-linear\ndimensionality reduction aiming at preserving the neighborhood in the output space.\nLearning to index and quantize. The literature on product compact codes for indexing is most\nrelevant to our work, see Wang et al. (2014b; 2016) for an overview of the topic. Early popular high-\ndimensional approximate neighbor methods, such as Locality Sensitive Hashing (Indyk & Motwani,\n1998; Gionis et al., 1999; Charikar, 2002; Andoni & Indyk, 2006), were mostly relying on statistical\nguarantees without any learning stage. This lack of data adaptation was subsequently addressed by\nseveral works. The Iterative quantization (ITQ) (Gong et al., 2013) modiﬁes the coordinate system to\nimprove binarization, while methods inspired by Vector Quantization and compression (J ´egou et al.,\n2011a; Babenko & Lempitsky, 2014; Zhang et al., 2015; Jain et al., 2016; Norouzi & Fleet, 2013)\nhave gradually emerged as strong competitors for estimating distances or similarities with compact\ncodes. In particular, Norouzi et al. (2012) learn discrete indexes with neural networks using a ranking\nloss. While most of these works aim at reproducing target (dis-)similarity, some recent works directly\nleverage semantic information in a supervised manner with neural networks (Liong et al., 2015; Jain\net al., 2017; Klein & Wolf, 2017; Sablayrolles et al., 2017).\nLattices, also known as Euclidean networks, are discrete subsets of the Euclidean space that are of\nparticular interest due to their space covering and sphere packing properties (Conway & Sloane, 2013).\nThey also have excellent discretization properties under some assumptions about the distribution,\nand most interestingly the closest point of a lattice is determined efﬁciently thanks to algebraic\nproperties (Ran & Snyders, 1998). This is why lattices have been proposed (Andoni & Indyk, 2006;\nJ´egou et al., 2008) as hash functions in LSH. However, for real-world data, lattices waste capacity\nbecause they assume that all regions of the space have the same density (Paulev ´e et al., 2010). In this\npaper, we are interested in spherical lattices because of their bounded support.\nEntropy regularization appears in many areas of machine learning and indexing. For instance,\nPereyra et al. (2017) argue that penalizing conﬁdent output distributions is an effective regularization.\nCuturi (2013) use entropy regularization to speed up computation of optimal transport distances.\nZhang et al. (2017) match the output statistics with the ﬁrst and second moment of a uniform\ndistribution, while Bojanowski & Joulin (2017), in an unsupervised learning context, spread the\noutput by enforcing input images to map to points drawn uniformly on a sphere. Most recent works on\nbinary hashing use some form of entropic regularization: Liong et al. (2015) maximize the marginal\nentropy of each bit, and SUBIC (Jain et al., 2017) extends this idea to one-hot codes.\n3 O UR APPROACH : LEARNING THE CATALYZER\nOur proposal is inspired by prior work for one-dimensional indexing (Kraska et al., 2017). However\ntheir approach based on unidimensional density estimation can not be directly translated to the multi-\ndimensional case. Our strategy is to train a neural network fthat maps vectors from a din-dimensional\nspace to the hypersphere of a dout-dimensional space Sdout.\n3.1 K OLEO: DIFFERENTIAL ENTROPY REGULARIZER\nLet us ﬁrst introduce our regularizer, which we design to spread out points uniformly across Sdout.\nWith the knowledge of the density of points p, we could directly maximize the differential entropy\n3\n\nPublished as a conference paper at ICLR 2019\n���� ���� ���� ���� ��\n�����������������������������������\n��������\n������\n���� ���� ���� ����\n������������������������������������\n��������\n������\nFigure 3: Histograms of the distance between a query point and its 1st (resp. 100th) nearest neighbors,\nin the original space (left) and after our catalyzer (right). In the original space, the two histograms\nhave a signiﬁcant overlap, which means that a 100-th nearest neighbor for a query has often a distance\nlower that the 1st neighbor for another query. This gap is signiﬁcantly reduced by our catalyzer.\n\u0000R\np(u) log(p(u))du. Given only samples (f(x1);:::;f (xn)), we instead use an estimator of the\ndifferential entropy as a proxy. It was shown by Kozachenko and Leononenko (see e.g. (Beirlant\net al., 1997)) that deﬁning \u001an;i= min j6=ikf(xi)\u0000f(xj)k, the differential entropy of the distribution\ncan be estimated by\nHn=\u000bn\nnnX\ni=1log(\u001an;i) +\fn; (1)\nwhere\u000bnand\fnare two constants that depend on the number of samples nand the dimensionality\nof the datadout. Ignoring the afﬁne components, we deﬁne our entropic regularizer as\nLKoLeo =\u00001\nnnX\ni=1log(\u001an;i): (2)\nThis loss also has a satisfactory geometric interpretation: closest points are pushed away, with a\nstrength that is non-decreasing and concave. This ensures diminishing returns: as points get away\nfrom each other, the marginal impact of increasing the distance becomes smaller.\n3.2 R ANK PRESERVING LOSS\nWe enforce the outputs of the neural network to follow the same neighborhood structure as in the\ninput space by adopting the triplet loss (Chechik et al., 2010; Wang et al., 2014a)\nLrank= max\u0000\n0;kf(x)\u0000f(x+)k2\u0000kf(x)\u0000f(x\u0000)k2\u0001\n; (3)\nwherexis a query,x+a positive match, x\u0000a negative match. The positive matches are obtained\nby computing the kposnearest neighbors of each point xin the training set in the input space. The\nnegative matches are generated by taking the kneg-th nearest neighbor of f(x)in(f(x1);:::;f (xn)).\nIn order to speed up the learning, we compute the kneg-th nearest neighbor of every point in the\ndataset at the beginning of each epoch and use these throughout the epoch. Note that we do not need\nto use a margin, as its effect is essentially superseded by our regularizer. Our overall loss combines\nthe triplet loss and the entropy regularizer, as\nLmodel =Lrank+\u0015LKoLeo; (4)\nwhere the parameter \u0015\u00150controls the trade-off between ranking quality and uniformity.\n3.3 D ISCUSSION\nChoice of\u0015.Figure 2 was produced by our method on a toy dataset adapted to the disk as the output\nspace. Without the KoLeo regularization term, neighboring points tend to collapse and most of the\noutput space is not exploited. If we quantize this output with a regular quantizer, many V oronoi cells\n4\n\nPublished as a conference paper at ICLR 2019\nFigure 4: Impact of the regularizer on the output distri-\nbution. Each column corresponds to a different amount\nof regularization (left: \u0015= 0, middle:\u0015= 0:02, right:\n\u0015= 1). Each line corresponds to a different random\nprojection of the empirical distribution, parametrized by\nan angle in [0;2\u0019]. The marginal distributions for these\ntwo views are much more uniform with our KoLeo regu-\nlarizer, which is a consequence of the higher uniformity\nin the high-dimensional latent space.\nare empty and we waste coding capacity. In contrast, if we solely rely on the entropic regularizer, the\nneighbors are poorly preserved. Interesting trade-offs are achieved with intermediate values of \u0015.\nQualitative evaluation of the uniformity. Figure 3 shows the histogram of the distance to the\nnearest (resp. 100thnearest) neighbor, before applying the catalyzer (left) and after (right). The\noverlap between the two distributions is signiﬁcantly reduced by the catalyzer. We evaluate this\nquantitatively by measuring the probability that the distance between a point and its nearest neighbor\nis larger than the distance between another point and its 100thnearest neighbor. In a very imbalanced\nspace, this value is 50%, whereas in a uniform space it should approach 0%. In the input space, this\nprobability is 20:8%, and it goes down to 5:0%in the output space thanks to our catalyzer.\nVisualization of the output distribution. While Figure 2 illustrates our method with the 2D disk as\nan output space, we are interested in mapping input samples to a higher dimensional hyper-sphere.\nFigure 4 proposes a visualization of the high-dimensional density from a different viewpoint, with\nthe Deep1M dataset mapped in 8dimensions. We sample 2planes randomly in Rdoutand project the\ndataset points (f(x1);:::;f (xn))on them. For each column, the 2 ﬁgures are the angular histograms\nof the points with a polar parametrization of this plane. The area inside the curve is constant and\nproportional to the number of samples n. A uniform angular distribution produces a centered disk,\nand less uniform distributions look like unbalanced potatoes.\nThe densities we represent are marginalized, so if the distribution looks non-uniform then it is\nnon-uniform in dout-dimensional space, but the reverse is not true. Yet one can compare the results\nobtained for different regularization coefﬁcients, which shows that our regularizer has a strong\nuniformizing effect on the mapping, ultimately resembling that of a uniform distribution for \u0015= 1.\n4 C ATALYZER WITH DISCRETIZATION\nIn this section we describe how our method interplays with discretization, at training and at search\ntime. We consider two parameter-free coding methods: binarization and deﬁning a ﬁxed set of\npoints on the unit sphere provided by a lattice spherical quantizer. A key advantage of a ﬁxed coding\nstructure like ours is that compressed-domain distance computations between codes do not depend on\nexternal meta-data. This is in contrast with quantization-based methods like product quantization,\nwhich require centroids to be available at search time.\n4.1 B INARIZATION\nBinary features are obtained by applying the sign function to the coordinates. We relax this\nconstraint at train time by replacing the sign with the identity function, and the binarization is used\nonly to cross-validate the regularization parameter on the validation set.\n4.2 L ATTICES\nAs discussed by Paulev ´e et al. (2010), lattices impose a rigid partitioning of the feature space, which\nis suboptimal for arbitrary distributions, see Figure 2. In contrast, lattices offer excellent quantization\nproperties for a uniform distribution (Conway & Sloane, 2013). Thanks to our regularizer, we are\ncloser to uniformity in the output space, making lattices an attractive choice.\n5\n\nPublished as a conference paper at ICLR 2019\nWe consider the simplest spherical lattice, integer points of norm r, a set we denote Sr\nd. Given a\nvectorx2Rdin, we compute its catalyzed features f(x), and ﬁnd the nearest lattice point on Sr\nd\nusing the assignment operation, which formally minimizes q(f(x)) = min c2Sr\ndkr\u0002f(x)\u0000ck2\n2:\nThis assignment can be computed very efﬁciently (see Appendix B for details). Given a query\nyand its representation f(y), we approximate the similarity between yandxusing the code:\nkf(y)\u0000f(x)k2\u0019kf(y)\u0000q(f(x))=rk2, This is an asymmetric comparison, because the query\nvectors are not quantized (J ´egou et al., 2011a).\nWhen used as a layer, it takes a vector in Rdand returns the quantized version of this vector in the\nforward pass, and passes the gradient to the previous layer in the backward pass. This heuristic is\nreferred to as the straight-through estimator in the literature, and is often used for discretization steps,\nseee.g., van den Oord et al. (2017).\n5 E XPERIMENTS\nThis section presents our experimental results. We focus on the class of similarity search methods\nthat represents the database vectors with a compressed representation (Charikar, 2002; J ´egou et al.,\n2011a; Gong et al., 2013; Ge et al., 2013; Norouzi & Fleet, 2013), which enables to store very large\ndataset in memory (Lv et al., 2004; Torralba et al., 2008).\n5.1 E XPERIMENTAL SETUP\nAll experiments have two phases. In the ﬁrst phase (encoding), all vectors of a database are encoded\ninto a representation ( e.g.32, 64 bits). Encoding consists in a vector transformation followed by\na quantization or binarization stage. The second phase is the search phase: a set of query vectors\nis transformed, then the codes are scanned exhaustively and compared with the transformed query\nvector, and the top-k nearest vectors are returned.\nDatasets and metrics. We use two benchmark datasets Deep1M and BigAnn1M. Deep1M consists\nof the ﬁrst million vectors of the Deep1B dataset (Babenko & Lempitsky, 2016). The vectors\nwere obtained by running a convnet on an image collection, reduced to 96 dimensions by principal\ncomponent analysis and subsequently `2-normalized. We also experiment with the BigAnn1M (J ´egou\net al., 2011b), which consists of SIFT descriptors (Lowe, 2004). Both datasets contain 1M vectors\nthat serve as a reference set, 10k query vectors and a very large training set of which we use 500k\nelements for training, and 1M vectors that we use a base to cross-validate the hyperparameters dout\nand\u0015. We also experiment on the full Deep1B and BigAnn datasets, that contain 1 billion elements.\nWe evaluate methods with the recall at kperformance measure, which is the proportion of results that\ncontain the ground truth nearest neighbor when returning the top kcandidates (for k2f1;10;100g).\nTraining. For all methods, we train our neural network on the training set, cross-validate doutand\u0015\non the validation set, and use a different set of vectors for evaluation. In contrast, some works carry\nout training on the database vectors themselves (Muja & Lowe, 2014; Malkov & Yashunin, 2016;\nGong et al., 2013), in which case the index is tailored to a particular ﬁxed set of database vectors.\n5.2 M ODEL ARCHITECTURE AND OPTIMIZATION\nOur model is a 3- layer perceptron, with ReLU non-linearity and hidden dimension 1024 . The ﬁnal\nlinear layer projects the dataset to the desired output dimension dout, along with`2-normalization. We\nuse batch normalization (Ioffe & Szegedy, 2015) and train our model for 300 epochs with Stochastic\nGradient Descent, with an initial learning rate of 0:1and a momentum of 0:9. The learning rate is\ndecayed to 0:05(resp. 0:01) at the 80-th epoch (resp. 120-th).\n5.3 S IMILARITY SEARCH WITH LATTICE VECTOR QUANTIZERS\nWe evaluate the lattice-based indexing proposed in Section 4, and compare it to more conventional\nmethods based on quantization, namely PQ (J ´egou et al., 2011a) and Optimized Product Quantization\n(OPQ) (Ge et al., 2013; Norouzi & Fleet, 2013). We use the Faiss (Johnson et al., 2017) implementa-\ntion of PQ and OPQ and assign one byte per sub-vector (each individual quantizer has 256 centroids).\nFor our lattice, we vary the value of rto increase the quantizer size, hence generating curves for\n6\n\nPublished as a conference paper at ICLR 2019\n 0 0.2 0.4 0.6 0.8 1\n 16  32  64  1281-recall at 10\nbits per indexed vectord=16\nd=24\nd=32\nd=40\nPQ\nOPQ\n 0 0.2 0.4 0.6 0.8 1\n 16  32  64  1281-recall at 10\nbits per indexed vectord=16\nd=24\nd=32\nd=40\nPQ\nOPQ\nFigure 5: Comparison of the performance of the product lattice vsOPQ on Deep1M (left) and\nBigAnn1M (right). Our method maps the input vectors to a dout-dimensional space, that is then\nquantized with a lattice of radius r. We obtain the curves by varying the radius r.\neach value of dout. Figure 5 provides a comparison of these methods. On both datasets, the lattice\nquantizer strongly outperforms PQ and OPQ for most code sizes.\nImpact of the hyperparameters. Varying the rank parameters kposandknegdid not impact signif-\nicantly the performance, so we ﬁxed them respectively to kpos= 10 andkneg= 50 . For a ﬁxed\nnumber of bits, varying the dimension doutis a trade-off between a good representation and an easily\ncompressible one. When doutis small, we can use a large rfor a very small quantization error,\nbut there are not enough dimensions to represent the degrees of freedom of the underlying data. A\nlargerdoutallows for better representations but suffers from a coarser approximation. Figure 5 shows\nthat for low bitrates, small dimensions perform better because the approximation quality dominates,\nwhereas for higher bitrates, larger dimensions are better because the representation quality dominates.\nSimilarly, the regularizer \u0015needs to be set to a large value for small dimensions and low bitrates, but\nhigher dimensions and higher bitrates require lower values of \u0015(cf. Appendix A for details).\nLarge-scale experiments. We experiment with the full Deep1B (resp. BigAnn) dataset, that contains\n1 billion vectors, with 64 bits codes. At that scale, the recall at 10 drops to 26.1% for OPQ and to\n37.8% for the lattice quantizer (resp. 21.3% and 36.5%). As expected, the recall performance is\nlower than for the 1million vectors database, but the precision advantage of the lattice quantizer is\nmaintained at large scale.\nComparison to the state of the art. Additive quantization variants (Babenko & Lempitsky, 2014;\nMartinez et al., 2018; Ozan et al., 2016) are currently state-of-the art encodings for vectors in terms of\nDeep1M BigAnn1M Encoding time\nRecall1@ 1 10 100 1 10 100 (1M vectors)\nOPQ (Ge et al., 2013) 15.6 50.3 88.1 20.8 63.6 95.3 5.5 s\nCatalyst + OPQ 21.2 62.8 93.4 24.9 71.1 97.0 11.4 s\nLSQ (Martinez et al., 2018) 20.3 61.4 94.3 28.4 76.2 98.7 122.1 s\nPCA + Lattice 12.2 42.5 81.6 19.0 60.6 93.5 5.3 s\nCatalyst + Lattice 22.6 66.9 95.2 28.4 75.8 98.3 8.5 s\nCatalyst + Lattice + end2end 22.8 67.5 95.5 28.7 76.2 98.2 8.5 s\nTable 1: Comparison of different ﬂavors of the catalyst: with a lattice quantizer (with or without\nend-to-end training), and with OPQ. All results use 64 bits per code. All timings are for BigAnn1M\nare on a 2.2 GHz machine with 40 threads. The encoding times associated with the catalyzer include\nthe forward pass through our neural network. Note, our lattice-based coding scheme is the only one\nnot requiring external meta-data once the compact code is produced.\n7\n\nPublished as a conference paper at ICLR 2019\nDeep1M BigAnn1M\nbits per vector 16 32 64 128 16 32 64 128\nLSH 0.8 4.9 14.6 32.2 0.3 2.6 9.5 24.4\nITQ 1.0 7.3 21.0 n/a 1.5 8.5 22.8 41.8\nCatalyzer + sign 2.7 11.4 25.5 46.8 2.6 11.1 28.3 52.2\nTable 2: Performance (1-recall at 10, %) with LSH, on Deep1M and BigAnn1M, as a function of the\nnumber of bits per index vector. All results are averaged over 5 runs with different random seeds.\nOur catalyzer gets a large improvement in binary codes over LSH and ITQ.\naccuracy. However, their encoding stage involves an iterative optimization process that is prohibitively\nslow for practical use cases. For example, Competitive quantization’s reported complexity is 15 \u0002\nslower than OPQ. Table 1 compares our results with LSQ (Martinez et al., 2018), a recent variant\nthat is close to the state of the art and for which open-source code is available. We show that our\nCatalyst + Lattice variant method is 14 \u0002times faster for an accuracy that is competitive or well\nabove that of LSQ. To our knowledge, this is the ﬁrst time that such competitive results are reported\nfor a method that can be used in practice at a large scale. Our search time is a bit slower: computing\n1M asymmetric distances takes 7.5 ms with the Catalyzer+Lattice instead of 4.9 ms with PQ. This is\ndue to our decoding procedure, which does not rely on precomputed tables as used in PQ.\n5.4 A UNIVERSAL CATALYZER ?\nAblation study. As a sanity check, we ﬁrst replace our catalyzer by a PCA that reduces the\ndimensionality to the same size as our catalyzer, followed by `2-normalization. This signiﬁcantly\ndecreases the performance of the lattice quantizer, as can be seen in Table 1.\nWe also evaluate the impact of training end-to-end, compared to training without the quantization\nlayer. Table 1 shows that end-to-end training has a limited impact on the overall performance for 64\nbits, sometimes even decreasing performance. This may be partly due to the approximation induced\nby the straight-through estimation, which handicaps end-to-end training. Another reason is that the\nKoLeo regularizer narrows the performance gap induced by discretization. In other terms, our method\ntrained without the discretization layer trains a general-purpose network (hence the name catalyzer ),\non which we can apply any binarization or quantization method. Table 1 shows that OPQ is improved\nwhen applied on top of catalyzed features, for example increasing the recall@10 from 63:6to71:1.\nBinary hashing. We also show the interest of our method as a catalyzer for binary hashing, compared\nto two popular methods (Charikar, 2002; Gong et al., 2013):\nLSH maps Euclidean vectors to binary codes that are then compared with Hamming distance. A\nset ofmﬁxed projection directions are drawn randomly and isotropically in din, and each vector is\nencoded into mbits by taking the sign of the dot product with each direction.\nITQ is another popular hashing method, that improves LSH by using an orthogonal projection that is\noptimized to maximize correlation between the original vectors and the bits.\nTable 2 compares our catalyzer to LSH and ITQ. Note that a simple sign function is applied to the\ncatalyzed features. The catalyzer improves the performance by 2-9 percentage points in all settings,\nfrom 32 to 128 bits.\n6 C ONCLUDING REMARKS\nWe train a neural network that maps input features to a uniform output distribution on a unit hyper-\nsphere, making high-dimensional indexing more accurate, in particular with fast and rigid lattice\nquantizers or a trivial binary encoding. To the best of our knowledge, this is the ﬁrst work on\nmulti-dimensional data that demonstrates that it is competitive to adapt the data distribution to\na rigid quantizer, instead of adapting the quantizer to the input data. This has several beneﬁts:\nrigid quantizers are fast at encoding time; and vectors can be decoded without carrying around\ncodebooks or auxiliary tables. We open-sourced the code corresponding to the experiments at\nhttps://github.com/facebookresearch/spreadingvectors .\n8\n\nPublished as a conference paper at ICLR 2019\nREFERENCES\nAlexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate nearest neighbor\nin high dimensions. In Symposium on the Foundations of Computer Science , 2006.\nArtem Babenko and Victor Lempitsky. Additive quantization for extreme vector compression. In\nConference on Computer Vision and Pattern Recognition , 2014.\nArtem Babenko and Victor Lempitsky. Efﬁcient indexing of billion-scale datasets of deep descriptors.\nInConference on Computer Vision and Pattern Recognition , 2016.\nJan Beirlant, E J. Dudewicz, L Gyor, and E.C. Meulen. Nonparametric entropy estimation: An\noverview. International Journal of Mathematical and Statistical Sciences , 1997.\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In International\nConference on Machine Learning , 2017.\nMoses Charikar. Similarity estimation techniques from rounding algorithms. In ACM symposium on\nTheory of computing , 2002.\nGal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio. Large scale online learning of image\nsimilarity through ranking. Journal of Machine Learning Research , 2010.\nJohn Horton Conway and Neil James Alexander Sloane. Sphere packings, lattices and groups .\nSpringer Science & Business Media, 2013.\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in\nNeural Information Processing Systems . 2013.\nCarl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908 , 2016.\nTiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. Optimized product quantization for approximate\nnearest neighbor search. In Conference on Computer Vision and Pattern Recognition , 2013.\nArisitides Gionis, Piotr Indyk, and Rajeev Motwani. Similarity search in high dimension via hashing.\nInInternational Conference on Very Large DataBases , 1999.\nYunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. Iterative quantization: A\nprocrustean approach to learning binary codes for large-scale image retrieval. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 2013.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural\nInformation Processing Systems . 2014.\nGeoffrey Hinton and Sam Roweis. Stochastic neighbor embedding. In Advances in Neural Information\nProcessing Systems , 2003.\nPiotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of\ndimensionality. In ACM symposium on Theory of computing , 1998.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International Conference on Machine Learning , 2015.\nHimalaya Jain, Patrick P ´erez, R ´emi Gribonval, Joaquin Zepeda, and Herv ´e J´egou. Approximate\nsearch with quantized sparse representations. In European Conference on Computer Vision ,\nOctober 2016.\nHimalaya Jain, Joaquin Zepeda, Patrick Perez, and Remi Gribonval. SUBIC: A supervised, structured\nbinary code for image search. In International Conference on Computer Vision , 2017.\nHerv ´e J´egou, Laurent Amsaleg, Cordelia Schmid, and Patrick Gros. Query adaptative locality\nsensitive hashing. In International Conference on Acoustics, Speech, and Signal Processing , 2008.\nHerv ´e J´egou, Matthijs Douze, and Cordelia Schmid. Product Quantization for Nearest Neighbor\nSearch. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2011a.\n9\n\nPublished as a conference paper at ICLR 2019\nHerv ´e J´egou, Romain Tavenard, Matthijs Douze, and Laurent Amsaleg. Searching in one billion\nvectors: re-rank with source coding. In International Conference on Acoustics, Speech, and Signal\nProcessing , 2011b.\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. Billion-scale similarity search with gpus. arXiv\npreprint arXiv:1702.08734 , 2017.\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114 , 2013.\nBenjamin Klein and Lior Wolf. In defense of product quantization. arXiv preprint arXiv:1711.08589 ,\n2017.\nDonald E Knuth. The art of computer programming, volume 4: Generating all combinations and\npartitions, fascicle 3, 2005.\nT. Kohonen, M. R. Schroeder, and T. S. Huang (eds.). Self-Organizing Maps . 2001.\nTim Kraska, Alex Beutel, Ed H. Chi, Jeff Dean, and Neoklis Polyzotis. The case for learned index\nstructures. arXiv preprint arXiv:1712.01208 , 2017.\nVenice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, Jie Zhou, et al. Deep hashing for compact\nbinary codes learning. In Conference on Computer Vision and Pattern Recognition , 2015.\nDavid G. Lowe. Distinctive image features from scale-invariant keypoints. International journal of\nComputer Vision , 2004.\nQin Lv, Moses Charikar, and Kai Li. Image similarity search with compact data structures. In\nInternational Conference on Information and Knowledge , November 2004.\nYu A Malkov and DA Yashunin. Efﬁcient and robust approximate nearest neighbor search using\nhierarchical navigable small world graphs. arXiv preprint arXiv:1603.09320 , 2016.\nJulieta Martinez, Shobhit Zakhmi, Holger H Hoos, and James J Little. LSQ++: Lower running time\nand higher recall in multi-codebook quantization. In Proceedings of the European Conference on\nComputer Vision (ECCV) , 2018.\nMarius Muja and David G. Lowe. Scalable nearest neighbor algorithms for high dimensional data.\nIEEE Transactions on Pattern Analysis and Machine Intelligence , 2014.\nMohammad Norouzi and David J. Fleet. Cartesian k-means. In Conference on Computer Vision and\nPattern Recognition , 2013.\nMohammad Norouzi, David J Fleet, and Ruslan R Salakhutdinov. Hamming distance metric learning.\nInAdvances in Neural Information Processing Systems . 2012.\nE. C. Ozan, S. Kiranyaz, and M. Gabbouj. Competitive quantization for approximate nearest neighbor\nsearch. IEEE Transactions on Knowledge and Data Engineering , 2016.\nLo¨ıc Paulev ´e, Herv ´e J´egou, and Laurent Amsaleg. Locality sensitive hashing: A comparison of hash\nfunction types and querying mechanisms. Pattern recognition letters , 2010.\nGabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, and Geoffrey Hinton. Regularizing\nneural networks by penalizing conﬁdent output distributions. arXiv preprint arXiv:1701.06548 ,\n2017.\nMoshe Ran and Jakov Snyders. Efﬁcient decoding of the Gosset, Coxeter-Todd and the Barnes-Wall\nlattices. In International Symposium on Information Theory , 1998.\nAlexandre Sablayrolles, Matthijs Douze, Nicolas Usunier, and Herv ´e J´egou. How should we evaluate\nsupervised hashing? In International Conference on Acoustics, Speech, and Signal Processing ,\n2017.\nAntonio Torralba, Rob Fergus, and Yair Weiss. Small codes and large image databases for recognition.\nInConference on Computer Vision and Pattern Recognition , 2008.\n10\n\nPublished as a conference paper at ICLR 2019\nNicolas Usunier, David Buffoni, and Patrick Gallinari. Ranking with ordered weighted pairwise\nclassiﬁcation. In International Conference on Machine Learning , 2009.\nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning.\nInAdvances in Neural Information Processing Systems . 2017.\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine\nLearning Research , 2008.\nLaurens Van Der Maaten, Eric Postma, and Jaap Van den Herik. Dimensionality reduction: a\ncomparative review. Journal of Machine Learning Research , 2009.\nJiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen,\nand Ying Wu. Learning ﬁne-grained image similarity with deep ranking. In Conference on\nComputer Vision and Pattern Recognition , 2014a.\nJingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. Hashing for similarity search: A\nsurvey. arXiv preprint arXiv:1408.2927 , 2014b.\nJun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data: a\nsurvey. Proceedings of the IEEE , 2016.\nTing Zhang, Guo-Jun Qi, Jinhui Tang, and Jingdong Wang. Sparse composite quantization. In\nConference on Computer Vision and Pattern Recognition , June 2015.\nXu Zhang, Felix X. Yu, Sanjiv Kumar, and Shih-Fu Chang. Learning spread-out local feature\ndescriptors. In International Conference on Computer Vision , 2017.\n11\n\nPublished as a conference paper at ICLR 2019\nAPPENDIX A V ALUES OF THE REGULARIZATION PARAMETER\nThe optimal value of the regularizer \u0015decreases with the dimension, as shown by Table 3.\ndout\u0015\n16 0.05\n24 0.02\n32 0.01\n40 0.005\nTable 3: Optimal values of the regularization parameter \u0015for Deep1M, using a ﬁxed radius of r= 10 .\nAPPENDIX B F AST DISCRETIZATION WITH A LATTICE ON THE SPHERE .\nWe consider the set of integer points z= (z1;:::;z d)2Zdsuch thatPd\ni=1z2\ni=r2, that we denote\nSr\nd. This set is the intersection of the hyper-cubic lattice Zdwith the hyper-sphere of radius r. For\nexample to extract a 64\u0000bit representation in 24D we use r2= 79 . Quantizing a vector y2Rd\namounts to solving the following optimization problem:\nargmin\nz2Sr\ndky\u0000zk2=argmax\nz2Sr\ndyz>: (5)\nAtoms. We deﬁne a “normalization” function Nof vectors: it consists in taking the absolute value\nof their coordinates, and sorting them by decreasing coordinates. We call “atoms” the set of vectors\nthat can be obtained by normalizing the vectors of Sr\nd.\nFor example, the atoms of Sp\n10\n8are:\n(3 1 0 0 0 0 0 0\n2 2 1 1 0 0 0 0\n2 1 1 1 1 1 1 0(6)\nAll vectors of Sr\ndcan be represented as permutations of an atom, with sign ﬂips. Figure 6 reports the\nnumber of vectors of Sk\ndand the corresponding number of atoms.\nEncoding and enumerating. To solve Equation 5, we apply the following steps:\n1. normalize ywithN, store the permutation \u001bthat sorts coordinates of jyj\n2. exhaustively search the atom z0that maximizes N(y)>z0\n3. apply the inverse permutation \u001b\u00001that sortsytoz0to obtainz00\n4. the nearest vector (z1;::;z d)iszi= sign(yi)z00\ni8i= 1::d.\nTo encode a vector of z2Sr\ndwe proceed from N(z):\n1.each atom is assigned a range of codes, so zis encoded relative to the start of N(z)’s range\n2.encode the permutation using combinatorial number systems Knuth (2005). There are d!\npermutations, but the permutation of equal components is irrelevant, which divides the\nnumber combinations. For example atom (2;2;1;1;0;0;0;0)is the normalized form of\n8!=(2!2!4!) = 240 vectors ofSp\n10\n8.\n3. encode the sign of non-zero elements. In the example above, there are 4 sign bits.\nDecoding proceeds in the reverse order.\nEncoding 1M vectors takes about 0.5 s on our reference machine, which is faster than PQ (1.9 s). In\nother terms, he quantization time is negligible w.r.t. the preprocessing by the catalyzer.\n12\n\nPublished as a conference paper at ICLR 2019\n��������������������������\n������������������������������������������������\n��\n���������������\n���������������������������������������������������������\n��\nFigure 6: Number of atoms of the hyper-sphere of Sr\n24. (linear scale), and the corresponding number\nof points on the hyper-sphere (log scale).\nAPPENDIX C E PSILON -SEARCH\nFigure 7 shows how our method achieves a better agreement between range search and k-nearest\nneighbors search on Deep1M. In this experiment, we consider different thresholds \"for the range\nsearch and perform a set of queries for each \". Then we measure how many vectors we must return,\non average, to achieve a certain recall in terms of the nearest neighbors in the original space. Without\nour mapping, there is a large variance on the number of results for a given \". In contrast, after the\nmapping it is possible to use a unique threshold to ﬁnd most neighbors.\n�����������������������\n�� ���� ���� ���� ���� ���������������������������\n�����������������\n��������������������\n������������\n��������������������\nFigure 7: Agreement between nearest neigh-\nbor and range search: average number of re-\nsults per query for given values of \"(indicated\non the curve), and corresponding recall values.\nFor example: to obtain 80% recall, the search\nin the original space requires to set \"= 0:54,\nwhich returns 700 results per query on aver-\nage, while in the transformed space \"= 0:38\nreturns just 200 results. Observe the much\nbetter agreement in the latent spherical space.\n13",
  "textLength": 42630
}