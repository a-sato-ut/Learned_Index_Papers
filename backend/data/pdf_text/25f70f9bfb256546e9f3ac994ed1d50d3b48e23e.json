{
  "paperId": "25f70f9bfb256546e9f3ac994ed1d50d3b48e23e",
  "title": "Qd-tree: Learning Data Layouts for Big Data Analytics",
  "pdfPath": "25f70f9bfb256546e9f3ac994ed1d50d3b48e23e.pdf",
  "text": "Qd-tree: Learning Data Layouts for Big Data Analytics\nZongheng Yang§∗, Badrish Chandramouli, Chi Wang, Johannes Gehrke†, Yinan Li,\nUmar Farooq Minhas, Per-Åke Larson, Donald Kossmann, Rajeev Acharya†\nMicrosoft Research†Microsoft§University of California, Berkeley\nzongheng@cs.berkeley.edu,{badrishc,chiw,johannes,yinali}@microsoft.com,\n{ufminhas,a-palars,donaldk,rajeevac}@microsoft.com\nABSTRACT\nCorporations today collect data at an unprecedented and\naccelerating scale, making the need to run queries on large\ndatasets increasingly important. Technologies such as colum-\nnar block-based data organization and compression have\nbecome standard practice in most commercial database sys-\ntems. However, the problem of best assigning records to data\nblocks on storage is still open. For example, today’s systems\nusually partition data by arrival time into row groups, or\nrange/hash partition the data based on selected fields. For\na given workload, however, such techniques are unable to\noptimize for the important metric of the number of blocks ac-\ncessed by a query. This metric directly relates to the I/O cost,\nand therefore performance, of most analytical queries. Fur-\nther, they are unable to exploit additional available storage\nto drive this metric down further.\nIn this paper, we propose a new framework called a query-\ndata routing tree , orqd-tree , to address this problem, and pro-\npose two algorithms for their construction based on greedy\nand deep reinforcement learning techniques. Experiments\nover benchmark and real workloads show that a qd-tree can\nprovide physical speedups of more than an order of magni-\ntude compared to current blocking schemes, and can reach\nwithin 2×of the lower bound for data skipping based on\nselectivity, while providing complete semantic descriptions\nof created blocks.\nACM Reference Format:\nZongheng Yang, Badrish Chandramouli, Chi Wang, Johannes\nGehrke, Yinan Li, Umar Farooq Minhas, Per-Åke Larson, Donald\nKossmann, Rajeev Acharya. 2020. Qd-tree: Learning Data Layouts\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request\npermissions from permissions@acm.org.\nSIGMOD’20, June 14–19, 2020, Portland, OR, USA\n©2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-6735-6/20/06. . . $15.00\nhttps://doi.org/10.1145/3318464.3389770for Big Data Analytics. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data (SIGMOD’20), June\n14–19, 2020, Portland, OR, USA. ACM, New York, NY, USA, 16 pages.\nhttps://doi.org/10.1145/3318464.3389770\n1 INTRODUCTION\nThe last decade has seen a huge surge in the volume of data\ncollected for big data analytics. This trend has in turn driven\nup interest in building high-performance analytics systems\nthat can answer queries over terabytes of data in seconds.\nThe first generation of analytics systems heavily leveraged\nfine-grained indexes such as B-Trees to accelerate query\nprocessing. However, more recently, the trend has shifted to\nusing scan-oriented data processing strategies that exploit\nthe high sequential bandwidth of modern storage devices.\nIn an effort to reduce the amount of data read from disk,\ntoday’s analytics systems typically split up the data into\nchunks or data blocks in main memory or secondary storage.\nFurther, they build an in-memory min-max index that stores\nthe minimum and maximum values per block, per field, and\nuse it to only retrieve blocks relevant to a query. A retrieved\nblock is fully scanned, and other blocks are entirely skipped.\nWhile tremendous progress has been made in organizing\nthe data carefully within each block, with techniques such as\ncolumnar organization and compression, the problem of best\nassigning records to data blocks is relatively less explored.\nFor example, today’s production systems usually partition\ndata by arrival time into blocks called row groups , or use\nhash orrange partitioning of data based on selected fields.\nSuch techniques are unable to optimize for the important\nmetric of number of blocks accessed (or rows touched, in\ncase of variable sized blocks) by a given query workload.\nThis metric directly relates to the I/O cost, and therefore\nperformance, of the workload.\nWe desire blocks with two properties: (1) a semantic de-\nscription , a precise description of its contents as a predicate\nover table fields; and (2) completeness , a guarantee that a block\ncontains alltuples matching the given description. These\nproperties are desirable for accelerating block retrieval and\nfor using blocks as a local cache or partial view over remote\n∗Research done during internship at Microsoft Research.arXiv:2004.10898v1  [cs.DB]  22 Apr 2020\n\ndata. Further, they allow us to consider advanced data lay-\nouts where a row is stored in more than one block, thereby\nexploiting additional storage budget (Sec. 6). Such data redun-\ndancy can significantly reduce the number of blocks accessed\nby a query, but only if we have the completeness property\nfor blocks. Without the completeness guarantee, all blocks\noverlapping with a query would need to be scanned, even if\na single block contains all the tuples needed for that query.\nRecent research on row-grouping [ 45] proposes the use of\ndata mining and clustering techniques to create data blocks;\nthe blocks have semantic descriptions based on a feature\nbitmap vector, but the resulting blocks lack completeness\n(Sec. 2 has more details).\n1.1 Our Solution\nIn this paper, we propose a new framework to address the\nproblem of data organization and query routing (Sec. 2 and 3).\nFig. 1 depicts our overall system architecture, targeting a\ntraditional data warehouse scenario where data is stored on\ndisk. At the heart of the system is a data structure we call\naquery-data routing tree (orqd-tree ). Briefly, a qd-tree is a\nbinary tree where each node corresponds to some sub-space\nof the entire high-dimensional data space. The root of the\ntree corresponds to the entire data space. We perform a cut\nof a node’s data space in order to create its children.\nWe can use a qd-tree for both block generation and query\nprocessing. Given a qd-tree and a dataset, we can route each\ntuple in the dataset through the qd-tree to assign them to data\nblocks of a minimum size (large blocks may be physically\nstored as multiple segments on storage). In other words, the\nleaves of the qd-tree correspond to data blocks. Each leaf has\na semantic description, a predicate pbased on a conjunction\nof the cuts made to the data space as we traverse the routing\ntree from root to leaf. Data blocks created in this manner are\nalso complete: a data block can be described as “all tuples\nthat match predicate p”. Further, given a set of qd-tree based\nblocks and an incoming query, we can use the qd-tree in\nconjunction with traditional min-max indexes to quickly\nlocate and scan all blocks relevant to the query.\nConstructing the optimal qd-tree for a given dataset and\nworkload is a hard problem. Our first approach (Sec. 4) uses a\nnew greedy heuristic where, starting from the root, each cut\nis made based on locally available information. This provides\nrouting trees of high quality with new approximation guar-\nantees, but is unable to fully exploit long-term knowledge\nof tree quality. At the other extreme is dynamic program-\nming (DP) or memoized search, which can find the optimal\nsolution, but is infeasible given our large search space. In-\nstead, by exploiting deep reinforcement learning (RL) for\ntheir construction, we show (Sec. 5) that we can explore alarger search space and exploit any implicit lower dimension-\nality of data during qd-tree construction, thereby producing\nrouting trees that significantly outperform the state-of-the-\nart, while still providing complete semantic descriptions for\nblocks. Our RL solution can be regarded as an approximate\nand accelerated memoized search method [ 38,39], leading\nto higher efficiency than DP but with optimality close to\nDP. Deep RL-based qd-tree also forms a general framework:\nwe show in Sec. 6 that it can easily be extended with newer\ntypes of cuts, data overlap, and data replication for even\nbetter block skipping.\nWe perform a detailed evaluation (Sec. 7) on a standard\nbenchmark workload (TPC-H) and two real workloads. Our\nexperiments show that qd-tree -based data layouts can yield\nphysical speedups, compared to current block construction\nschemes, of up to 14×per workload (or up to 180×per query),\nand reach within 2×of the lower bound for data skipping\nbased on selectivity (the optimal solution, being a hard prob-\nlem, is unknown). We cover related work in Sec. 8 and con-\nclude the paper in Sec. 9.\n1.2 Contributions\nTo summarize, we make the following contributions:\n•We introduce the qd-tree data structure and show how it\ncan be used for data partitioning.\n•We propose a greedy construction scheme for qd-tree ,\nwhich builds the tree top-down starting from the root, and\noffers theoretical approximation guarantees.\n•To overcome the limitations of the greedy approach, we\nintroduce Woodblock , a deep reinforcement learning al-\ngorithm that learns to construct high-quality qd-trees .\n•We discuss the generality of our tree and learning frame-\nwork via potential extensions that are enabled by a qd-\ntree’s semantic description and completeness properties.\n•Through a detailed evaluation on benchmarks and real\nworkloads, we show that a learned qd-tree exhibits excel-\nlent data skipping properties.\nFinally, note that one can view a qd-tree as a powerful\nworkload-guided index for modern block-based big data ana-\nlytics. It can express non-trivial block assignment strategies\nand can locate relevant blocks quickly during query pro-\ncessing. The layout within each block is orthogonal to the\nqd-tree itself, affecting only its cost function. For instance, a\nqd-tree can support columnar scan-optimized layouts as well\nas row-oriented layouts (possibly indexed) in each block.\n2 PRELIMINARIES\n2.1 Problem Definition\nGiven a set of tuples V, we aim to partition them into multiple\nblocks, such that the number of tuples required to scan for a\n\nonlineofﬂinelearned treeQd-tree ConstructorGreedy / Deep RLQueries\nData RouterQuery RouterData BlockscandidatecutsBlock IDs samplelearned treeDBMSDataFigure 1: System Architecture.\nworkload is minimized, or equivalently, the number of tuples\nthat can be skipped for the workload is maximized. Consider\na partitioningP={P1, . . . , Pk}over V, i.e.,Pis a set of\ndisjoint subsets of Vwhose union is V. Each subset Piis\ncalled a block. We use C(Pi)to denote the number of tuples\nthat can be skipped when we execute all the queries in a\nworkload W={q1, . . . , qm}. For a scan-oriented system, it\ncan be calculated as:\nC(Pi)=|Pi|Õ\nq∈WS(Pi,q) (1)\nwhere S(P,q)is a binary function indicating whether parti-\ntionPcan be skipped when processing query q. The defini-\ntion of Sdepends on the type of meta information maintained\nat each block. The most common type of meta information\nis the max-min filters, i.e., the maximal and minimal values\nof each dimension over all the tuples in a block. For this case,\nS(P,q)=1if the hypercube defined by the max-min filters\nintersects with the range of query q.\nGiven a workload W, the overall effectiveness of a parti-\ntioningPis measured by the total number of tuples skipped\nC(P)=Í\nPi∈PC(Pi). Without constraints on block size, the\nnumber can be trivially maximized by putting every tuple\nin an individual block. For reasons like I/O batching and\ncolumnar compression, a real system requires blocks to have\ncertain minimal size, e.g., 1 million tuples in SQL Server [ 25].\nWe use bto refer to this minimal size.\nThe partitioning problem is formulated as follows.\nProblem 1 (MaxSkip Partitioning). Given a set Vof\ntuples, a workload Wof queries, a skipping function S, and a\nminimal block size b, find a partitioning Pto maximize C(P),\ns.t.|Pi|≥bfor all Pi∈P.\nThis formulation is appropriate for static data. To handle\ndynamically ingested data, it is desirable to learn a parti-\ntioning function from offline data, and apply the function to\nonline data ingestion to save data reshuffling cost.\nProblem 2 (Learned MaxSkip Partitioning). Given a\nsetVof tuples, a workload Wof queries, a skipping function\nS, and a minimal block size b, find a partitioning function F,such that for the next Vtuples ingested, the partitioning P\ngenerated by F(V)maximizes C(P).\nIn general, no partitioning function is guaranteed to work\nfor future unseen data. In this work, we focus on the scenario\nwhere the current Vtuples have the same distribution as the\nnext Vtuples. Therefore, solving Problem 2 is reduced to\nsolving Problem 1 in addition with a descriptive partitioning\nfunction, such that any new tuple can be mapped to a right\npartition identifier. For efficiently ingesting data, we also\ndesire the partitioning function to be lightweight to compute.\n2.2 Current Approaches\n2.2.1 Date Partitioning. In this basic partitioning scheme,\nwe partition data by time of ingestion. The skipping function\nS(P,q)=1if query q’s date range intersects with partition\nP, and is 0otherwise.\n2.2.2 Bottom-up Row Grouping. This technique was pro-\nposed by Sun et al. [ 45], and uses feature-based data skip-\nping. Basically, each feature fiis a predicate over the data.\nMfeatures are extracted from the workload in the beginning\nusing frequent pattern mining. Each block has a bitmap of\nlength M, indicating whether predicate fi,i∈[M]is satisfied\nby any tuple in this block. If the i-th bit for this block is 0, i.e.,\nno tuple satisfies fi, then we can skip all queries subsumed\nby (i.e., stricter than) fi. Sun et al.’s problem formulation\nis slightly different, requiring each partition to have equal\nsize. They name the problem Balanced MaxSkip Partitioning,\nand prove its NP-hardness by reduction from hypergraph\nbisection. Using the same reduction technique, we can prove\nthat Problem 1 is NP-hard.\nSun et al.’s solution uses bottom-up clustering and is ac-\ntually a solution to Problem 1, rather than the Balanced\nMaxSkip Partitioning problem. This is because the output of\nthat algorithm has varying block sizes, and the sizes are no\nsmaller than b. The algorithm converts tuples into unique\nbinary feature vectors, and record the weight of each unique\nfeature vector (row weight), as well as the number of queries\nsubsumed by each feature (column weight). Initially every\nunique feature vector is in its own block. Then blocks are\nmerged greedily using a heuristic criterion: in each iteration,\na heuristic penalty is calculated for all pairs of blocks; and\nthe pair with lowest penalty is chosen to be merged into a\nnew block. Once the size of a block reaches b, it does not\nfurther merge with other blocks. Hence, merging eventually\nstops with every block having size no smaller than b.\nThis solution is shown to be more effective than date\npartitioning and simple multi-dimensional range partition-\ning. There are several drawbacks of that approach. First, the\nheuristic penalty criterion used in the greedy algorithm only\nmatches the optimization objective when the query sets sub-\nsumed by all features are disjoint. In general that assumption\n\nB1 B2 B3 B4mem=10GB? cpu<5%?cpu<10%?\nFigure 2: An example qd-tree with four leaf blocks.\nis not true. So choosing the pair of blocks minimizing the\npenalty does not necessarily maximize C(P). Second, no\ntheoretical guarantee is provided by the greedy merging al-\ngorithm. Third, the complexity of the algorithm is quadratic\nto the number of unique feature vectors, which can be as\nlarge as the number of tuples and grows exponentially with\nrespect to the number of features. Practical application of this\nalgorithm requires using a small number of features, which\nposes an additional challenge of selecting a good, small set\nof features. Last, while each block can be described using\nthe “OR” of all the feature bitmap vectors contained in that\nblock, such description is not complete. For example, there\ncan be two blocks with identical bitmap description, and a\nnew tuple does not have a deterministic destination partition\nusing this description.\n3QD-TREE\nAqd-tree describes how a high-dimensional data space is cut.\nEach node corresponds to a subspace of an N-dimensional\ntable, modeled as a discrete hypercube, ([0,|Dom i|),∀i∈\n[0,N)). Each node logically holds all records that belong to\nits hypercube. The root of the tree, ([0,|Dom i|),∀i), repre-\nsents the whole table. We assume that the domain of each di-\nmension is known, and its attribute values are in [0,|Dom i|).\nEach internal node nhas two children, where the left child\nsatisfies a particular predicate p—attached to node n—and the\nright child satisfies ¬p. For now, we assume each predicate\nto be a simple unary form, (attr, op, literal) , where opis a\nnumeric comparison, but the framework supports arbitrary\npredicates as well. We call predicate pacuton node n.\nqd-tree differs from the classical k-d tree [ 5].k-d tree can\nbe seen as a simple form of qd-tree , in that they typically\ncome with heuristics such as assuming cuts to be unary, cuts\nalternating among dimensions, and cuts points chosen as\neach dimension’s median value. qd-tree does not assume\nthese construction heuristics.\nExample. Figure 2 shows an example qd-tree on two\ncolumns, (cpu,mem) . The root is cut with predicate cpu <\n10%. The resultant two children are cut with mem =10GB\nandcpu <5%, respectively. In our implementation, the liter-\nals, e.g., “ 10%”, are dictionary-encoded as integers.\nWe next describe the usage of qd-tree in data routing and\nquery processing. We later present algorithms to construct\nqd-tree in Sections 4 and 5.Fields of node n Definition\nn.range Hypercube describing the node’s sub-\nspace. 2N-dimensional array.\nn.categorical_mask Map: categorical column i’s name→\n|Dom i|-dim of bits. 0 means that value\nis not present.\nTable 1: Semantic description of a qd-tree node.\n3.1 Routing Data\nOur overall strategy is to use a qd-tree to assign data to\nblocks on storage. The routing of data to blocks is carried\nout as follows. Each record “arrives” at the root and is re-\ncursively routed down. At each node, the tagged predicate\npis evaluated; if p(record)is true, it is routed to the left,\notherwise to the right. Each record uniquely lands in a leaf\ndue to the binary split ( por¬p). Each leaf thus represents\na set of physical blocks to be persisted. Records are stored\nwith an additional block ID ( BID) field to denote the block\nthey belong to, and the dataset is partitioned by this field.\nIn practice, we route large batches of records at a time,\ntaking advantage of vectorized instructions. Further, threads\ncan load different batches of records in parallel (assuming\nthe appends at the leaves are protected with locking).\n3.2 Semantic Description of Nodes\nAs mentioned above, each allowed cut (predicate) is of the\nform (attr, op, literal) . We allow each operator to be range\ncomparisons,{<,≤, >,≥}, or equality comparisons, {=,IN}.\nWe now describe what node metadata we need to store to\nprocess each cut. Table 1 presents a summary.\nHandling range comparisons is straightforward, as we\nonly need to restrict a parent’s hypercube description. For\nexample, Figure 2’s root node has the hypercube\nroot.range:[0,MAX cpu),[0,MAX mem),\nand the cut on this node, cpu <10%, produces two restricted\nversions for its left and right child,\nleft.range:[0,10%),[0,MAX mem)\nright.range:[10%,MAX cpu),[0,MAX mem)\nHandling equality comparisons , i.e.,=andIN, requires\nstoring additional metadata. We assume that these predicates\nare only issued to categorical columns. Each node stores, for\neach categorical column i, a|Dom i|-dimensional bit vector,\nrepresenting the distinct values of this column. If 1 is present\nat a position, the value that corresponds to that position may\nappear under the node’s subspace; otherwise, if 0, that value\ndefinitively does not appear. It is then straightforward to\nprocess =andINcuts, by simply keeping (for the left child,\nsince it satisfies the cut) or zeroing out (for the right child) the\ncorresponding slots in the bit vector. For example, consider\n\na categorical column, priority∈{LOW ,MED ,HIGH}. The\nroot is initialized with\nroot.categorical_mask: (priority→[1,1,1])\nsince any of the three values may potentially appear. If we\ncut the root with priority=MED (say, second value in the\nordered domain), the left and right child would have the\nfollowing categorical masks:\nleft.categorical_mask: (priority→[1,1,1])\nright.categorical_mask: (priority→[1,0,1])\nbecause the right child must satisfy ¬(priority=MED).\nOverall, this scheme is similar to “dictionary filtering” in\npopular persistent formats such as Parquet.\nThus,(range ,categorical_mask)make up a node’s seman-\ntic description. We make an optimization in the case of when\ndata has fully been routed down a qd-tree . In this scenario,\nwe can freeze the tree and replace each leaf’s range with a\nmin-max index over the leaf’s records. The min-max index\nserves to “tighten” the range hypercube.\nEach node of the qd-tree has a semantic description as\ndescribed. Further, based on our routing strategy, the blocks\nassigned to each leaf together have the completeness prop-\nerty, i.e., for a given leaf’s semantic description, every record\nsatisfying the description is stored at the corresponding leaf.\n3.3 Query Processing\nA simple way to process queries is to directly execute them\non a dataset partitioned by the block ID ( BID) field introduced\nbyqd-tree (Sec. 3.1). In this case, which requires no inter-\nvention during query processing, the traditional partition-\npruning [ 11,16,30] block-level indexes (e.g., min-max) are\nused for actual block-skipping on a best-effort basis. For fur-\nther effectiveness, we instead intercept queries submitted\nby users and augment them to effectively use qd-tree for\npartition pruning as follows. Queries are routed through the\nqd-tree and augmented with a BID IN (...) clause that lists\nthe pruned set of block IDs. Modern databases can use this\nexplicit predicate to prune blocks, without modifications to\nthe database internals. If desired, the query routing func-\ntionality can also be integrated into the DBMS to make the\nprocess entirely transparent.\nTo obtain the BIDlist, we loop over each leaf descrip-\ntion, check whether the query logically intersects with the\nleaf subspace, and return the IDs of all intersecting leaves.\nConcretely, for any (unary) range predicate (recall from last\nsection, these include {<,≤, >,≥}), we perform a simple in-\nterval intersection check against each leaf.range . For any\nequality predicate ( =andIN), we check the corresponding\nbit vector slot in leaf.categorical_mask . Alternatively, we\ncould also “route” the query down the tree to reach a set\nof leaves; however, we find scanning leaf metadata to beefficient enough, especially when leaf metadata is grouped\ntogether for fast access.\nBuilding on top of checks for predicates , the intersection\nchecks for queries are natural extensions. We allow a query to\nbe arbitrary conjunction or disjunction of unary predicates\n(and of lower-level conjuncts/disjuncts). The intersection\nlogic for AND is simply that it intersects if all of its conjuncts\ndo. Likewise, an ORintersects if any of its disjuncts does.\n3.4 Choosing Candidate Cuts\nPrior to discussing algorithms to construct qd-tree , we de-\nscribe choosing the set of allowed cuts. This set serves as the\nsearch space for the construction algorithms.\nWe opt for a simple treatment. Since we are given a target\nworkload Wof queries, we simply parse them through a stan-\ndard SQL planner and take all pushed-down unary predicates\nas allowed cuts. For example, from a target query:\nSELECT ... FROM R\nWHERE (R.a < 10 OR R.b > 90) AND (R.c IN (0,4))\nthree cuts are extracted: (1) R.a < 10 , (2) R.b > 90 , and\n(3)R.c IN (0,4) . We find that our algorithms can easily\nhandle a few hundreds to low thousands of candidate cuts.\n4 GREEDY CONSTRUCTION OF QD-TREE\nThe construction of a qd-tree is an NP-hard combinatorial\noptimization problem. Greedy algorithms are a typical fam-\nily of solutions that are usually efficient and make locally\noptimal choices. Hence, we start by proposing a greedy algo-\nrithm to construct the qd-tree . We begin with all the tuples\nin a single block, i.e., the qd-tree has a single root node that\ncontains all the tuples. In each iteration, we split a leaf node\nwhose size is larger than 2binto two child nodes, and make\nsure the two children have size at least b. When choosing the\ncut for a node, we use the one that maximizes C(T), i.e., the\nnumber of tuples skipped by the partitioning PTinduced by\nqd-tree T. The idea is similar to decision tree construction,\nexcept that in decision tree learning, the predicate is chosen\nusing a different criterion such as information gain.\nTo present the algorithm, we define an action a=(p,n)as\napplying cut pto node nin aqd-tree T. The result of action\nais denoted as T⊕a=T⊕(p,n). InT⊕a, node nbecomes\nthe parent of two child nodes: the left child npcontains all\nthe tuples in nsatisfying p, and the right child n¬pcontains\nall the tuples in nsatisfying¬p.\nOur algorithm is presented in Algorithm 1. The main com-\nputation is to choose the cut pthat maximizes the greedy\ncriterion C(Tt−1⊕(p,n))for each node n. For each level of the\ntree, the cost to executed the for loop is bounded by O(|V||P|).\nThe total cost of the while loop is bounded by O(|V||P|d),\nwhere dis the final depth of the tree. log2|V|\nb≤d<|V|/b. In\n\nAlgorithm 1 Greedy construction of qd-tree\nInput : Tuple set V, min block size b, workload W, candi-\ndate cut set P\nInitializatioin : SetT0←V,t←1,CanSplit←True\nwhile CanSplit do\nCanSplit←False\nforeach node n∈Tt−1on the last level do\nifn.size≥2bthen\np←arg max p∈P,|np|≥b,|n¬p|≥bC(Tt−1⊕(p,n))\nifC(Tt−1⊕(p,n))>C(Tt−1)then\nTt←Tt−1⊕(p,n)\nt←t+1,CanSplit←True\nReturn Tt−1\nthe worst case, when the tree is least balanced, the complex-\nity is quadratic to|V|. In the balanced case, the complexity\ngrows as|V|log|V|. If the number of partitions |V|/bis a\nconstant, then the cost is linear to |V|.\nApproximation Guarantee\nUnder certain generic assumptions, we can prove that the\ngreedy algorithm has a multiplicative offline approximation\nguarantee, and an additive online approximation guarantee.\nTo the best of our knowledge, this the first algorithm with\nsuch guarantees. Our proof technique defines a new notion\nof submodularity: tree submodularity. We are unaware of\nnotions of similar kind in other tree construction algorithms.\nLettbe the number of leaf nodes in a qd-tree . The total\nnumber of nodes is 2t−1. While the result of the greedy\nalgorithm is invariant to the order of leaf splitting (which leaf\nto split at each iteration), we fix the order to be top-down,\nleft-to-right for ease of analysis. In each iteration i, a cut pi\nis chosen for the leftmost node nion level li. We define this\naction as ai=(pi,ni). To characterize the properties of C(T),\nit is useful to define an encoding of the qd-tree .\nDefinition 1 (Action Encoding). An action encoding\nA(T)={ai}t−1\n1of a qd-tree is the sequence of cuts chosen\nfollowing the top-down, left-to-right order. We denote the tree\nafter iteration rasTr, and Tr=T0⊕a1⊕···⊕ ar=T0⊕Ar(T).\nAfter applying action ai, a leaf node is split into two child\nnodes. By definition, T=Tt−1=T0⊕At−1(T).\nDefinition 2 (Tree Submodularity). Given two nodes n\nandn′inqd-tree T, and actions a=(p,n),a′=(p,n′). Ifn′is\nan ancestor of n, letT′beTminus all descendants of n′. We say\naqd-tree space is tree-submodular if for any such T,n,n′,ain\nthe space, C(T⊕a)−C(T)≤C(T′⊕a′)−C(T′).\nThis property means that applying a cut in qd-tree has a\ndiminishing return as the tree grows deeper. Let Q(p)⊆W\nbe the set of queries that can be skipped by p. We have thefollowing sufficient condition for the qd-tree space to be\ntree-submodular.\nLemma 1. Theqd-tree space is tree-submodular if the con-\njunction of cuts p1andp2cannot skip any query besides Q(p1)∪\nQ(p2)for all candidate cuts p1andp2.\nFor example, if the workload Wonly consists of conjunc-\ntive range queries, and each cut is a range predicate, the\nabove condition is satisfied.\nTheorem 2. If the qd-tree space is tree-submodular, the\ngreedy top-down construction algorithm produces a qd-tree T\nwhose overall skipping capacity C(T)is no worse than:\n(a)OPT−2|V|\nb\u0000C(T)−C(T−1)\u0001\n(b)\u0012\n1−b\n|V|blog2e\n2|V|\u0013\nOPT\nwhere OPT refers to the skipping capacity of the optimal\nqd-tree T∗, and T−1refers to the sub qd-tree inTby removing\nall the leaf nodes.\nBound (a) is an online bound because it depends on the\nalgorithm output C(T)andC(T−1). Bound (b) is an offline\nbound independent of the algorithm output. For space rea-\nsons, we defer the proofs to our technical report [49].\n5QD-TREE USING DEEP RL\nThe greedy technique presented above makes locally op-\ntimal choices which may lead to global suboptimality. At\nthe other extreme is dynamic programming (DP), or equiva-\nlently, memoized search. It can find the optimal solution, but\nis infeasible given our large high-dimensional search space,\nleading to a need for approximate DP [ 38,39]. In this paper,\nwe propose leveraging deep RL to perform an approximate,\naccelerated, and incremental memoized search.\nWoodblock is our deep RL agent that constructs routing\ntrees optimized for a target dataset and workload. At a high\nlevel, the algorithm repeatedly constructs many trees, ini-\ntially making random cuts (i.e., randomly sampling a tree\nfrom the set of all valid trees), and gradually learns to iden-\ntify better cuts through rewards. After attempting a fixed\nnumber of trees or if a timeout is reached, the best tree found\nis deployed. This approach brings several key benefits:\n•Instead of remembering all the exact search states and the\noptimality (reward) for them, it featurizes the states and\nuses a model to predict the reward under the states.\n•Instead of enumerating all the follow-up actions of a search\nstate to observe the reward from each of them, it samples\na subset of such actions and updates the model from the\nobserved rewards.\n•It can incrementally produce better trees, letting us deploy\nsolutions quickly based on time or CPU budgets.\nWe next start with motivating arguments to illustrate the\nabove intuition, and then present Woodblock in detail.\n\n0 20 40 60 80 100\ncpu01diskQ1: cpu<10 OR cpu>90\nQ2: disk<0.01Figure 3: A dataset with disjunctive queries. Regions se-\nlected by Q1/Q2 are shown in grey/blue. The candidate cuts\nare: {cpu<10, cpu>90, disk<0.01}. The first two cuts cannot\nskip any query, so Greedy opts for the third cut, resulting in\na scan ratio of 50.5%. Woodblock is not limited by the forms\nof queries; it produces a layout with a scan ratio of 10.4%, a\n4.8×improvement. Discussion in Section 5.1.\n5.1 Motivation for RL\nRouting tree construction presents several unique challenges\nthat we argue are good fit for RL.\nFirst, the exact goodness of a tree is only measurable after\nthe whole tree is completed. Typically, a tree is completed\nafter dozens or hundreds of cuts. Thus, when deciding what\ncut to make, we either approximate its benefit at that single\nstep(e.g., a greedy criterion), or we randomly sample a cut\nfrom some (learned, gradually refined) distribution, and then\naccurately attribute benefits of each decision once the true\ngoodness is calculated. We will show long-term considera-\ntion leads to higher quality trees than greedy consideration.\nRL methods are thus a natural fit because they study the\noptimization of long-term, cumulative rewards.\nSecond, an RL method does not make assumptions on\nthe query or data distribution. It requires only a black-box\nlearning signal, the skipping quality of a tree. To be concrete,\nwe now present a microbenchmark to showcase the potential\nadvantage due to this generality.\nIn Figure 3, we plot a simple dataset with two columns,\n(cpu, disk) . We draw cpu∼Unif[0,100)anddisk∼Unif[0,1).\nQuery 1 is a disjunctive query on cpu(perhaps looking for\nanomalies at either ends), and Query 2 is a unary filter on\ndisk. Recall from last section that the optimality proofs of\nour Greedy construction relies on the tree-submodularity.\nWhen the query workload contains disjunctive range queries\nand the candidate cuts are only simple range predicates,\ntree submodularity is not satisfied. Our Greedy algorithm is\nforced to choose the cut on disk—since, the two cuts on cpu\nprovide zero skipping capability (making either cut cannot\nskip Q1 nor Q2), whereas the cut on disk provides a non-zero\ngain. This results in a layout of the following two blocks:\n•Block 1: disk <0.01\n•Block 2: disk≥0.01Thus, Q1 has to scan the large portion of unselected records\nin the middle. Our deep RL agent, Woodblock , is able to\nproduce a 4.8×better partitioning:\n•Block 1: disk <0.01\n•Block 2:(disk≥0.01)∧(cpu >90)\n•Block 3:(disk≥0.01)∧(cpu <10)\n•Block 4:(disk≥0.01)∧(cpu≤90)∧(cpu≥10)\nHence, under this layout, both Q1 and Q2 can skip block 4,\nwhich contains a majority of records. This showcases the\npower of RL as a black-box optimization method.\nLastly, a large search space needs to be navigated. The\nnumber of candidate cuts can be large, potentially O(100)\norO(1000). Further, the number of data dimensions can be\nin the dozens or hundreds. Deep RL (compared to classical\nRL) methods have shown successes in tackling such high-\ndimensional problems For instance, OpenAI Five [ 34], a deep\nRL agent for successfully playing Dota, considers an action\nspace of O(1000)dimensions and an O(20000)-dimensional\nobservation space. Our Woodblock agent uses the same\nscalable learning algorithm as OpenAI Five, taking advantage\nof recent algorithmic advances.\nWe next describe the detailed design of Woodblock , a\ndeep RL agent that learns to construct qd-trees .\n5.2 Woodblock : the Deep RL agent\nTo apply any RL algorithm, we first need to define the tree\nconstruction Markov Decision Process (MDP). The state\nspace ,S, is defined to be any subspace of the entire data\nspace of the relation under optimization. The action space ,\nA, is the set of allowed cuts. Taking an action (cut) on a state\n(node) produces two new states, which we append into a\nqueue for exploration. The queue is initialized with a root\nstate (the root node) when starting each tree construction.\nTheWoodblock agent, at its core, consists of two learn-\nable networks parameterized by θ: (1) the policy network ,\nπθ:S→A, takes a state and emits a probability distribu-\ntion over the action space (“given a node, how good are the\ncuts?”), and (2) the value network ,Vθ:S→R, estimates the\nexpected cumulative reward from a given state.\nSequentially, the agent (1) takes a node noff the explo-\nration queue, (2) evaluates its current policy, πθ(n), (3) sam-\nples an action from this output distribution, (4) applies the\nsampled action (cut) on node nto produce new nodes. We use\nProximal Policy Optimization (PPO) [ 43] as the underlying\nlearning algorithm, a variation in the policy gradient family\nof methods. This update rule is used as a black-box subrou-\ntine and is not fundamental to the design of Woodblock .\nIntuition. We start each episode (the construction of one\ntree) with the root state (the singleton tree with a root node).\nThe agent takes actions and transition into next state(s).\nOnce a stopping condition is reached, described next, the\n\nepisode is ended and we obtain a completed qd-tree . We\ncalculate the reward of this episode, i.e., the data skipping\nratio achieved by the tree, and invoke PPO for gradient up-\ndates toθ. With the updated behaviors, the agent starts the\nnext episode. Through repeatedly constructing qd-tree , the\nRL agent becomes better. At the beginning of training, a\nrandomly initialized θimplies random behavior: random\ncuts are made and the skipping ratios would not excel. How-\never, as more trees (episodes) are explored, the refined θis\nencouraged to make cuts that achieve higher skipping ratios.\nWe next discuss algorithmic details.\n5.2.1 Stopping Condition. To prevent an endless sequence\nof cuts, we must define an appropriate stopping condition.\nA naive condition would be stopping after a pre-determined\nnumber of cuts are made. This is problematic, because we\ndo not know a priori the number of cuts required to achieve\ngood data skipping for a given dataset-workload.\nInstead, we connect back to Problem 1’s requirement where\neach leaf block must contain at least brecords. The agent is\nallowed to make a cut pon current node n, if the resultant\nchildren (approximately) contain more than brecords.\nThis approximation is achieved by testing the cut on a\ndata sample. First, at algorithm initialization, we take a data\nsample of ratio sfrom the dataset (we find s=0.1%tos=1%\ngenerally work well). All episodes (the exploration of trying\nout different trees) reuse the fixed sample. The data sample is\nassigned to the root node. Next, as a cut is made, we evaluate\nthe cut on the data sample, and obtain a subset of records\nthat satisfy it and a subset that does not. If both have more\nthan s·brecords, we call the cut legal and allow it to proceed.\nLastly, if a current node nhas no legal cuts in action space,\nwe stop cutting on it and form a leaf.\n5.2.2 Reward Calculation. After a tree Tis produced, Wood-\nblock calculates rewards for all actions taken. The rewards\nserve as important learning signals: they allow the agent to\nlearn to distinguish profitable cuts.\nFirst, we define S(n), the number of skipped records under\nnode nacross all queries:\nS(n):=(\nC(n.records) ifnis a leaf\nS(n.left)+S(n.left)otherwise\nRecall from Equation 1 that C(·)refers to the number of\nrecords skipped across the workload. Here, n.records is the\nset of records routed to node nduring tree construction—\nthus a subset of the small data sample we take. Since the\nsample is small, this ensures reward calculation is efficient.\nWe then assign a reward Rfor every action taken, i.e., for\nevery intermediate node nand corresponding cut p:\nR((n,p)):=S(n)/(|W|·|n.records|)Namely, we normalize number of skipped records under\nnto[0,1]by scaling with 1/(|W|·|n.records|), where the\ndenominator is the maximum number of skipped records\npossible under n(the best case of all queries skipping all its\nrecords). The PPO update rule is then invoked with a list of\nstate-action-reward tuples.\n5.2.3 Implementation. This section discusses detailed im-\nplementation of the networks. The policy network πθand\nthe value network Vθhave shared weights, which are two\nfully-connected layers, 512 units each, with ReLU activation.\nEach network has its own output layer: for the policy net-\nwork, it is a|A|-dimensional linear projection; for the value\nnetwork it is a scalar projection.\nEach state is featurized as the concatenation of n.range\nandn.categorical_mask . Due to the potentially large val-\nues in the former component, we binary-encode both vectors\n(i.e., these vectors are encoded in bits). The action space is a\ndiscrete categorical distribution, usually with a dimensional-\nity in a few hundreds.\nWe found that neural network computation is not a bot-\ntleneck in our setup. Routing records and calculation of re-\nwards, on the other hand, take up a significant portion of\ntree construction time. We therefore only use CPUs for our\nagent (although using a GPU for neural network computa-\ntion yields a slight overall speedup).\n5.2.4 Related Work. Our formulation of using deep RL to\nlearn a tree under custom quality metrics is inspired by Neu-\nroCuts [ 27], a deep RL algorithm to construct packet classifi-\ncation trees. We compare the two work below.\nFirst, Woodblock adopts their overall approach of tree-\nstructured MDP: each node nis treated as an independent\nstate, and receives a normalized reward as if the agent is\nasked to start constructing a tree from that node. This means\nthe agent is tasked to solve each subproblem independently.\nSecond, while NeuroCuts assumes no knowledge of work-\nload, Woodblock optimizes a target data-workload pair. This\naffects the choice of the actions. Their action space includes\ngeneric actions (e.g., “cut dim K in N equal parts”) while we\nobtain our actions from the unary predicates of the target\nworkload. We find that such generic actions do not make\nsense in our setting—the domain of an attribute can be large,\nand cutting at non query-aligned literals is suboptimal.\nThird, we make specific optimizations for our data analyt-\nics setting. A data sample is required for us to calculate in-\nvalid cuts and respect the layout constraints. We also propose\nspecial treatment of categorical predicates in featurization.\nWe find our RL-based solution to already achieve within\n20% of the true dataset selectivity, which is itself a lower\nbound for the optimal solution, for the TPC-H benchmark\n(Sec. 7). Therefore, we do not consider alternate DP approxi-\nmation or optimization techniques [38, 39] in this work.\n\n6 FRAMEWORK EXTENSIONS\nHaving described two algorithms to construct a qd-tree , we\nare now in a position to discuss extensions to our framework.\n6.1 Advanced Cuts\nThus far we have assumed the candidate cuts are single-\ncolumn predicates (Section 3.4). They are desirable because\ntheir simplicity allows for fast evaluation during tree con-\nstruction. Nevertheless, qd-tree can be extended to support\nbinary cuts of the form (attr1, op, attr2) . Recall from Table 1\nthere are two components of a node’s semantic description,\nand neither of them can describe a binary cut. We append a\nnew component to each node n’s description:\nn.adv_cuts: a bit vector of size |AC|\nwhere the constant |AC|denotes the number of advanced cuts\nto support and is specified for each workload a priori. Each\nposition icorresponds to “does this node contain records\nthat satisfy advanced cut i”, with zero indicating no and\none indicating potentially yes. This is the same semantics as\ncategorical_mask .\nFor instance, the TPC-H workload contains non-join bi-\nnary filters such as:\n•AC0:c_nationkey = s_nationkey\n•AC1:l_shipdate < l_commitdate\n•AC2:l_commitdate < l_receiptdate\nA vector of(0,1,1)thus indicates the first condition is defi-\nnitely not met (i.e., it describes a subspace of records whose\nc_nationkey does notequate s_nationkey ).\nLastly, the same mechanism also handles LIKE predicates\nor even stateless UDFs (with the caveat that, clearly, the\ncost of evaluating the predicates depends on their inherent\ncomplexities). The user can impose a limit on the maximum\nnumber of advanced cuts to support.\n6.2 Data Overlap\nWith the abundance of cheap storage in the cloud, one de-\nsirable feature for an analytics system is to trade space for\npotentially faster execution time. A fruitful line of work has\ndedicated to this problem, e.g., materialized views, which\nwe review in related work. We now discuss how qd-tree can\nalso naturally support duplicating data.\nFigure 4 shows a 2D synthetic dataset and four queries.\nNaively invoking either Greedy or Woodblock to construct\naqd-tree for this dataset-workload is suboptimal. Any se-\nquence of cuts—recall, the cut points are query literals, i.e.,\nany of the edges in the figure—would lead to 4 blocks: one\nwith N+1 record, and three with N records. (This is due to\nthe binary nature of the cuts.) The three blocks would have\nto fetch the singleton record they need from the first block.\nHence, a total of 3Nextra tuples are read.\nAttribute 1Attribute 2N NN\nNQ1\nQ2\nQ3\nQ4Figure 4: A scenario where significant data skipping is\ngained by replicating a single record. Each query selects N+1\nrecords. The queries only overlap in the one tuple placed\nat the center. If the space is naively cut in a binary fashion,\n3 out of 4 queries each reads Nextra tuples. By handling\noverlap, qd-tree replicates the singleton record to all four\nN-record regions, so no queries touch unnecessary records.\nWe extend qd-tree construction to handle such data over-\nlap cases as follows. Observe that the reason the “lucky”\n(N+1)-record block is not further cut is due to the minimum\nblock size constraint, b. We can instead launch either of our\nconstruction algorithms with a relaxed cutting condition: al-\nlowing one of the children to have size smaller than b. With\nthis change the lucky block would be further cut into an\nN-record one and a block with the singleton record.\nOnce such a qd-tree is constructed, we loop through all\nproduced leaves, and partition them into two sets: those with\nsize≥band those with size <b(the original constraint). We\nthen replicate each block in the small-size set to any of its\nneighbor blocks in the large-size set. We define two blocks\nto be neighbors if their hypercubes have N−1dimension\nboundaries in common and the intervals at the remaining\ndimension are adjacent. This ensures that, with minor modifi-\ncations to node metadata, the semantic descriptions preserve\ncompleteness. With this scheme, our algorithms could reach\nthe optimal partitioning for the scenario in Figure 4 at virtu-\nally no extra storage cost.\n6.2.1 Data and Query Routing. Data routing occurs as be-\nfore, with a row routed to all matching blocks. Rows landing\nin a replicated block are simply copied to every replica. For\nquery processing, the candidate set of blocks to be consid-\nered includes all blocks that overlap with the query rectan-\ngle. However, we can leverage the completeness property of\nblocks to prune out blocks that are redundant. For example,\na query that asks only for the centre rectangle (with the sin-\ngleton record) in the example above does not have to fetch\nall blocks, even though the min-max index for all four blocks\nincludes it . This is because of our semantic descriptions and\ncompleteness properties: we can prune away the other blocks\nbecause the first block completely covers the query rectangle.\nWith overlap, the set of blocks scanned when evaluating a\nquery may contain duplicate rows. To eliminate them, when\nscanning block ID i, we can simply ignore tuples that match\n\nthe semantic description of any selected block with ID <i. A\ndetailed evaluation of these strategies is left as future work.\n6.3 Data Replication: two-tree approach\nFull-copy data replication is a complimentary approach to\noverlap in utilizing extra storage. The extension for qd-tree\nconstruction to support such replication is natural.\nFirst, we learn (using Greedy or RL) a qd-tree ,T1, opti-\nmized for the full workload W. Then, we can in fact build a\nsecond qd-tree ,T2, tailored for the queries that experience the\nworst skippability under tree T1. The second tree is a logical\ncopy of the entire dataset. When constructing the second\ntree, we modify the reward function for RL (and the greedy\ncriterion for greedy), by accounting for the existence of the\nfirst tree. For each query q∈W, we choose one of the two\ntrees which maximizes the skippability for q, and calculate\nthe number of skipped tuples Cqusing that tree. Then we\nsum up the Cq’s for all q∈W. This change naturally guides\nthe construction of the second tree to focus on the queries\nwith low skippability by T1. Additionally, the first tree T1can\nbere-built and re-optimized with T2fixed, and iterate. Since\nthe revised reward function keeps increasing and is upper\nbounded, this process eventually converges. The idea may\nalso be extended to more than two trees if needed. Exploring\nsuch extensions is a rich area for future work.\n7 EVALUATION\nWe now experimentally evaluate our designs. We highlight\nkey findings from the evaluation:\n•(Table 2) qd-tree -based layouts enable excellent data skip-\nping ( 1.8×–61×over the state-of-the-art).\n•Greedy construction of qd-tree produces high skipping\nratios; yet, Woodblock , using a general-purpose RL algo-\nrithm, can still produce up to 8.5×gains due to its opti-\nmization of the long-term objective.\n•(Sections 7.4, 7.5) qd-tree provides 1.6×–14×physical ex-\necution speedup over a state-of-the-art baseline across\nexecution engines, storage formats, and workloads.\n7.1 Setup and Metrics\nWe implement qd-tree as a lightweight Python library. It\nincludes a lightweight AST and associated query-processing\nlogic. We use the vectorized arithmetics in numpy and pan-\ndas.Woodblock , the RL agent, is implemented using Ray\nRLlib [ 26,31], a scalable reinforcement learning library. We\nevaluate all systems using both logical and physical metrics.\nLogical: Access Percentage. We report %tuples accessed\nfor the whole workload achieved by various partitioners.\nThis metric is lower bounded by the true workload selectivity,\nand lower values indicate greater potential I/O savings.Physical: Query Runtime. We report end-to-end execu-\ntion time as queries are issued over several analytics systems:\n•Single-Node Spark (v2.4): All layouts execute with Spark\nover Parquet files stored on disk (HDD). After a qd-tree\nis constructed, each leaf block is converted into a Parquet\nfile. Other baseline layouts are stored as Parquet files as\nwell in comparable number of blocks.\n•Commercial DBMS: We use a commercial standalone black-\nbox optimized DBMS. We model a scenario where parti-\ntioning (e.g., by tenant ID, query ID, or TPC-H month)\nis used for parallelism at a higher level, by limiting each\nindividual query to a single degree of parallelism. We first\nload the datasets under different layouts into the system,\nwhich uses its own binary columnar storage format on a\nlocal SSD.\n•Distributed Spark: We use a 4-node Spark cluster hosted\non Microsoft Azure, each with 8 vCPUs, 56GB RAM, and\nan SSD. All data is stored as Parquet files (as with single-\nnode Spark) on Azure Storage, a remote blob store. The\nqd-tree is cached at the driver for query rewrites.\nWe ensure that all layouts have a comparable number of\nblocks. To eliminate caching effects we clear the OS buffer\ncache before each query run. To evaluate the benefit of qd-\ntree query routing, we execute queries using explicit BID\nfilters (Section 3.3; the default), or without (called no route ).\n7.2 Workloads\nWe evaluate on (1) TPC-H and (2) two real-world workloads-\nfrom a large commercial software vendor.\nTPC-H. We generated TPC-H with a scale factor (SF) of\n1000. Following prior work [ 45], we denormalize the TPC-H\nschema for the purpose of obtaining a table that many filters\ntouch1. Due to the uniform nature of TPC-H data, we apply\nall partitioning techniques to an one-month partition of the\ndataset; the month-partition totals 77M tuples, 68 columns,\nand 85GB. For queries, we include alltemplates that touch\ntheline_item fact table2. This includes the same 8 templates\n(q3,q5,q6,q8,q10,q12,q14,q19) that [ 45] uses, as well as 7\nadditional templates: q1,q4,q7,q9,q17,q18,q21. We use 10\nrandom seeds to generate each template, resulting in a total\nof 150 queries. The overall scan selectivity is 21.3%.\nReal Datasets. Our first real dataset, called ErrorLog-\nInt, consists of error logs collected from internal customers\nof a large software vendor. The error logs correspond to ker-\nnel crash dump reports and are collected in real-time and\nloaded into a data warehouse for analysis. These error logs\n1Our technique can layout each table in the database independently using\npredicates over that table. Jointly optimizing the layout of multiple tables\nfor complex join queries is left for future work.\n2Our goal is to find a table with a rich set of filters. To achieve this, we\ndenormalize all tables and include all templates that touch the fact table.\n\ncontain information such as a categorical event type (e.g.,\ndevice crash, live kernel event, etc.) with 8distinct values,\nOS build date, OS version (string), client ingest date, and\nentry validity (a boolean). The dataset has 50columns. We\ncollect a sample of around one week of logs, amounting to\n100million records and 85GB of raw data. The dataset is also\nassociated with a query workload imposed by automated\nsystems via an API and users through a user interface and\ntranslated into stored procedures over the data. We extract\nthe predicates that are pushed to storage and extract a set of\n1000 queries over 5dimensions that represent a majority of\nthe workload. The overall workload selectivity is 0.0005 %;\nthus, individual queries return very few results on average,\nusually less than 100. All queries are of the form of INpredi-\ncates over the categorical data, along with date ranges and\nLIKE and equality predicates over the string fields.\nOur second real dataset, called ErrorLog-Ext , is also a\ncrash dump log, but is collected from external customers (ap-\nplications) around the world. This dataset is fundamentally\ndifferent from the earlier one, collected over 15days with 81\nmillion rows ( 85GB), has more dimensions ( 58) and a much\nlarger categorical domain of around 3600 distinct values. We\nalso use 1000 queries, which return more results on average,\nwith an overall scan selectivity of 0.0697% .\n7.3 Approaches\nWe compare layouts produced by different algorithms, span-\nning from heuristics used in industry practice to the state-\nof-the-art approach in literature:\n•Random baseline (TPC-H): a partitioner that simply shuf-\nfles records into fixed-size blocks.\n•Range baseline (real workloads): range-partitioning on an\ningest time column (the default scheme deployed for the\nreal workloads).\n•Bottom-Up [ 45]: state-of-the-art row-grouping approach\nbased on clustering, described in Sec. 2.\n•Greedy-constructed qd-tree (Sec. 4).\n•Woodblock -constructed qd-tree (Sec. 5).\nWe ensure Bottom-Up, Greedy, and Woodblock have the\nsame search space: the same set of candidate cuts is fed to\nthe latter two approach, which is also fed to Bottom-Up\nas input to their feature selection procedure. The feature\nselection procedure first performs a topological sort of the\nfeatures according to the subsumption relationship, and then\nselect features one by one. The frequency of each feature\nis initialized as the number of queries subsumed by that\nfeature. At each iteration, a feature not subsumed by any\nothers is chosen, and the frequency of all the other features\nis discounted if the they subsume common queries with the\nchosen feature. A feature will not be chosen if its frequency\nis below a threshold. We configure Bottom-Up to use up toWorkload Baseline Bottom-Up Greedy (ours) RL (ours)\nTPC-H 56% 46.1% 26.3% 25.8%\nErrLog-Int 100% 5.6%∗3.1% 0.4%\nErrLog-Ext 100% 12.2%∗1.7% 0.2%\nTable 2: Logical I/O costs: percentage of tuples accessed un-\nder different layout schemes, compared to full scan. Base-\nlineis a random shuffler for TPC-H, and range partitioning\non an “ingest time” column for the two ErrorLog workloads.\n(∗Results of BU+, our tuned version. The untuned version\nfares at 100% and 96.9%, respectively.)\n15 features, which follows the number reported in [45]. We\nsetb, the minimum number of records per block, to 100K for\nTPC-H and 50K for the two ErrorLog workloads.\n7.4 TPC-H\nTable 2 shows the percentage of tuples accessed for different\nlayouts on the TPC-H workload. Overall, qd-tree layouts\nprovide up to 1.8×savings compared to Bottom-Up.\n7.4.1 Physical execution. We report the execution runtime\nof TPC-H on (1) a distributed Spark cluster, and (2) a single-\nnode commercial DBMS.\nDistributed Spark. Figure 5a shows the mean runtime per\ntemplate on distributed Spark (each template has 10 random\ninstances). In total, qd-tree yields a speedup of 1.6×(closely\nmatching the logical ratio in Table 2). When excluded tem-\nplates that need to scan all data, the speedup is 2.6×.\nThe top three templates where qd-tree exhibits the most\nabsolute runtime reduction are q21,q5,q19. For q21, the ad-\nvanced cut l_commitdate < l_receiptdate allows many blocks\nto be skipped, substantially speeding up a self-join. q5has fil-\nters on supplier’s r_name , a categorical with diverse literals—\nqd-tree yields a 16.8×speedup on this template. q19is an OR\nof three complex 6-filter blocks; qd-tree is able to optimize\nfor this complex template and provides 5.5×speedup over\nBottom-Up. Bottom-Up is faster only on q1andq18, both of\nwhich require the full month worth of data.\nCommercial DBMS. We move to a second execution en-\ngine, the commercial DBMS, to investigate whether qd-tree\nstill provides physical runtime execution. Results are shown\nin Figure 5b. Over all templates, qd-tree has a speedup of 1.3×\nand when excluding the scan-all templates, the speedup is\n1.7×. Consistent with SparkSQL results, templates q21,q5,q19\nexhibit large speedups of 1.3×,8.6×,9.1×respectively. Rel-\native ratios for other templates are also consistent, which\nsuggests that the benefits from improved layouts carry over.\nPerformance of data and query routing. Figure 6a re-\nports the throughput of routing records through a qd-tree\n(i.e., ingestion). We vary the number of ingestion threads on\n\n1345678910121417181921\nTemplate010203040Runtime (s)bottom-up\nqd-tree(a) Distributed Spark\n1345678910121417181921\nTemplate050100Runtime (s)bottom-up\nqd-tree (b) DBMS\nFigure 5: TPC-H execution runtimes, grouped by each template.\n1248163264\nNum Threads0200K400Krecords/s\n(a) Data routing\n0 5 10 15\nLatency (ms/query)0.000.250.500.751.00CDF (b) Query routing\nFigure 6: Performance of routing data and queries.\nthe same machine; linear scalability is achieved with up to\n16 threads, and at 64 threads, our prototype implementation\nin Python (which uses vectorized operations when possible)\ncan reach 400K records/second. Higher throughput can be\nreached still, by either an optimized implementation in a\ncompiled language, or by scaling out to multiple nodes.\nFigure 6b shows the latency CDF for routing all 150 queries—\nthe time it takes to check each query against a qd-tree to\ndetermine intersecting blocks (Section 3.3). The maximum\ntime it takes for a query is less than 16ms, with most un-\nder 10ms. The latencies for checking against the semantic\ndescriptions are not homogeneous, because queries have\nvarying number of filters (of varying complexity).\nRobustness. We generated 10×more queries (100 queries\nper template) using distinct random seeds from before. This\nenlarged “test” set includes substantially more query liter-\nals that the qd-tree did not use for construction. The mean\nruntime of these 1500 queries on the same qd-tree layout as\nFigure 5a is 7776ms, compared to that figure’s 7752ms, the\nmean of the 150 “train” queries. This suggests that qd-tree is\nrobust for this templated workload with unseen literals.\n7.5 ErrorLogs\nWe now discuss ErrorLog-Int and ErrorLog-Ext. Table 2 re-\nports the logical metrics for all approaches.\nThe default range-partitioner (“Baseline”) accesses all tu-\nples. Further, we found that the original feature selection\nmethod in Bottom-Up ends up choosing a predicate with\nvery high frequency but also very high selectivity as a fea-\nture. It prunes other predicates due to the frequency discount\nused in the original paper, and its skipping power is poor.\nBU+QD-Tree no route0250050007500Runtime (s)8890\n627 753(a) ErrorLog-Int\nBU+QD-Tree no route01000020000Runtime (s)19325\n3859 4126 (b) ErrorLog-Ext\n0 25 50 75 100 125 150 175\nSpeedup0%25%50%75%100%% of QueriesErrorLog-Int\nErrorLog-Ext\n(c) CDF of per-query speedups over BU+\nFigure 7: ErrorLog execution runtimes.\nThus, the access ratio is close to 100% . This a weakness of\nfrequency-based feature selection without taking selectivity\ninto account. We tuned it by ignoring predicates with selec-\ntivity > 10%. This tuning, which we call BU+, improves the\naccess ratio of Bottom-Up to 5–12%(listed in Table 2).\nOur greedy qd-tree , on the other hand, achieves excellent\nskipping capability, accessing only 3.1% and 1.7% of tuples\nfor ErrorLog-Int and ErrorLog-Ext, respectively. The RL qd-\ntreeimproves them to 0.4% and 0.2% respectively, an up to\n8.5×improvement.\n7.5.1 Physical execution. We measure the actual execution\nruntime for both real datasets on an optimized single-node\nSparkSQL instance. Each workload consists of 1000 queries.\nFigures 7(a) and (b) show the aggregate runtimes.\nWe find that qd-tree dominates Bottom-Up+with a 14×\nlower runtime for ErrorLog-Int. On ErrorLog-Ext, the speedup\nbecomes 5×because of its higher selectivity. For qd-tree , we\nreport runtimes using qd-tree routing (adding BID IN (...)\npredicates) and using the default partition pruning ( no route ).\nOn SparkSQL with Parquet, we observe that qd-tree -based\nrouting is better than no route by16% for ErrorLog-Int and\n6.4% for ErrorLog-Ext.\nFigure 7(c) shows the CDF of per-query speedups. We\nobserve that 50%of queries have a speedup of at least 25×\n\n0 1000 200025%30%35%40%Scan RatioTPC-H\n0 1000 20000.15%0.20%0.25%0.30%\nElasped Time (sec)ErrorLog-ExtFigure 8: Learning curve of Woodblock . On TPC-H, most\nquality improvement is learned in the first ∼10 minutes;\non ErrorLog-Ext, high quality is achieved immediately ( .3%)\nand continuously improved when given more time budget.\nThe trend for ErrorLog-Int is similar (not shown).\n(ErrorLog-Int) and 20×(ErrorLog-Ext), respectively. Thus,\nthe excellent skipping benefits reflected in the logical metrics\ntranslate well into physical runtime reduction.\nLastly, we executed a subset of the ErrorLog-Int queries\non the commercial DBMS. Briefly, the trends remain consis-\ntent as before. Query execution with qd-tree query routing\nis∼400×faster than the range-partitioned baseline which\nneeds to touch all blocks. Unlike Parquet, however, we found\nthat no route performed significantly worse than qd-tree\nrouting. We believe this is due to a lack of block-level in-\ndexes (dictionaries) for categorical fields, which prevents\ncategorical predicates from pruning blocks in no route .\n7.6 Time to Produce Layouts\nWhile quality of data layout is the primary metric of concern\nin this study, we next discuss the wall-clock time required\nto produce good data layouts.\nTPC-H . Bottom-Up took 71 minutes to produce its layout\ndue to large number of records and queries. It produces a\nlayout only on termination. In contrast, the RL agent Wood-\nblock produces trees immediately and continuously, with\nquality improving over time. Figure 8 plots the learning\ncurves of Woodblock . There are several key takeaways.\nFirst, at random initialization, Woodblock immediately\nproduces partitioning trees with a scan ratio of ∼39%. This,\nin fact, significantly exceeds the quality of the Random parti-\ntioner reported in Table 2 ( 56%). The reason is that, at initial-\nization Woodblock produces a tree drawn randomly from\nthe search space , which is defined with a candidate cut set ex-\ntracted from query workload. Leveraging these informative\ncuts is much better than disregarding workload information.\nSecond, Woodblock enables the user to explicitly trade-\noff computation time vs. the quality of layouts produced. As\nthe agent constructs more trees, it learns to bias the cuts that\nare observed to more profitable, but it also keeps a non-zero\nprobability for exploration. We see that most of the improve-\nment is learned in the first 10 minutes. Further speedups may\nresult from (1) implementing our tree library in a native lan-\nguage rather than in Python, and (2) switching Woodblock ’s\nlearning algorithm to a distributed learner [14].\nTree Depth01020Num Cutssn_name (127)\nl_shipmode (95)\nl_quantity (75)\nl_returnflag (61)\nl_discount (57)\nAC 1 (42)\np_container (34)\nsr_name (29)\ncn_name (20)p_brand (14)\nl_receiptdate (8)\nl_shipdate (5)\nAC 0 (2)\ncr_name (1)\np_size (1)\np_type (1)\nAC 2 (1)Figure 9: AWoodblock -produced top-performing qd-tree\nfor TPC-H. The number after each legend indicates the total\nnumber of cuts on that column (or advanced cut).\nErrorLogs. A uniform workload like TPC-H is, in fact, a\nmore challenging task for RL because the uniform distribu-\ntion of data and queries has higher entropy. On both real\nErrorLog workloads, Woodblock produces top-performing\ntrees within 30 seconds (Fig. 8). The run time is much short-\nened, due to the abundant correlations to be exploited in\nthe real-world data/workload. With the existence of corre-\nlations, exploration of the space is significantly faster. On\nErrorLog-Int, Greedy and Bottom-Up finished in 12 min-\nutes and 432 minutes, respectively. On ErrorLog-Ext, these\nnumbers became 12 minutes and 565 minutes, respectively.\n7.7 Interpreting Learned qd-trees\nTo gain insights, we now analyze a top-performing qd-tree\n(constructed by Woodblock ) found for the TPC-H workload.\nFigure 9 plots the dimensions cut across tree levels. We\nmake a few observations. First, the variety of cuts is high : 8\ncolumns are cut at least 20 times throughout all tree levels.\nBoth categorical (e.g., l_shipmode ) and numerical columns\n(e.g., l_discount ) contribute to skipping. This indicates fine-\ngrained cutting, as is done by RL-learned qd-tree , is ben-\neficial. Second, advanced cuts are leveraged (AC; Sec. 6.1),\nindicating their skippability benefits in complex workloads.\nThe cuts made at the root and the first two children are:\n•p_container IN (LG CASE,LG BOX,LG PACK,LG PKG) ;\n•(First two children) AC0:c_nationkey = s_nationkey .\nThe next level’s cuts involve l_shipdate andp_brand . Over-\nall, it is clear that existing partitioning techniques (hash or\nrange) do not equate the sophisticated combination of cuts\nproduced by a qd-tree layout.\n8 RELATED WORK\nPhysical Design & Partitioning. Traditionally, data ware-\nhouses employ partitioning for the purpose of scaling out\ncomputation and load balancing. Data is either chunked into\nblocks based on arrival time, or partitioned using range or\nhash partitioning schemes [ 25] or their improvements [ 6,51].\nOur technique may be applied within partitions created by\nsuch static schemes. Automated physical design tools pro-\nvide auto-tuning capabilities based on what-if analyses and\n\nintegrated into the query optimizer [ 1,2,15,18,41]. Au-\ntoAdmin [ 1,2,33,36,48,52] optimizes a database and its\nphysical design using machine learning and data mining.\nSome systems perform partitioning based on workload ac-\ncess patterns [ 2,3,9], while other systems are based on\ngraph-based workload modeling techniques [ 10,40,44]. Sun\net al. [45, 46] (discussed in this paper) extract features from\neach workload operation based on its predicates. Casper [ 4]\noffers a general partitioning design tool based on navigat-\ning a three-way tradeoff between read performance, update\nperformance, and memory utilization.\nAdaptive Physical Design. The cost of periodic automated\nphysical design can be amortized in an online pay-as-you-go\nfashion. This line of research can roughly be categorized\ninto two approaches. Online analysis [ 7,8,42] uses available\nsystem cycles between query execution and offline analysis\nto optimize physical design. Database cracking [ 21,22] im-\nmediately starts executing queries, and treats each query as a\nhint to reorganize parts of the data during query processing,\nusually based on sort attributes. This approach optimizes\nand adapts the data layout over time.\nIn contrast, a qd-tree performs data layout using fine-\ngrained descriptions based on a workload set. Our technique\nlearns at coarse-grained time boundaries and emits a syn-\nthesized qd-tree data structure to enable data layout. We\ndo not react to individual query execution, but our result-\ning data structure may be used for continuous ingestion\nor bulk loading. We find this approach to be suitable for\nmodern data formats such as Parquet, where incremental re-\norganization is expensive. An interesting direction of future\nwork is to integrate cracking with qd-tree . Since a qd-tree\nrepresents a way to layout data, cracking would allow us to\nincrementally refine the qd-tree over time. A possible ap-\nproach, left as future work, would be to include the cost of\ndata re-organization into the qd-tree cost model, so that we\ncan enable efficient incremental re-organization over time.\nLearned Databases. There is increasing interest in automat-\ning core database functionality and design decisions. This\nline of research leverages recent advancements in deep learn-\ning algorithms and scalable hardware (GPU) to improve data-\nbase systems. Closest to our work is a proposal of learned\npartition adviser using deep RL [ 19]; it focuses on replica-\ntion and coarse-grained partitioning (e.g., hash) along entire\nattribute(s), unlike qd-tree which partitions based on a rich\nset of fine-grained candidate cuts. In this space, machine\nlearning has also been used to revisit tuning [ 47], workload\nforecasting [ 28], data structures and indexes [ 12,20,23,32],\nand query optimization [ 13,24,29,50]. Our qd-tree may\nbe viewed as a learned physical design or indexing tool: it\noptimizes for scan-based workloads, common in big data\nanalytics, to minimize the I/O cost of block accesses.Traditional Indexing. Database indexing is a well-studied\nspace. For one dimensional indexing, B-Trees form the state\nof the art. Multi-dimensional indexes such as k-d tree [ 5] and\nR-tree [ 17] have been proposed to index data over more than\none dimension, but they do not adapt to high dimensional\ndata or the specific query workload. These indexes are not\na good fit for analytical workloads due to the cost of each\nindex lookup. Modern systems instead use scan-oriented pro-\ncessing over columnar row-groups. Our work may be viewed\nas workload-aware multi-dimensional indexing adapted to\nthe needs of analytical scan-based workloads.\nPartition Pruning. Most scan-oriented databases employ\nindexing over the blocks to make it easy to skip blocks. Ex-\namples include min-max based pruning, also known as small\nmaterialized aggregates (SMA) [ 30], zone maps [ 16], and\ndata skipping [ 45,46]. Here, the system maintains the data\ndistribution information for each block. Depending on the\nquery predicates, these values can be used to determine that\na given block might not be needed for a given query. They\nare commonly employed in systems such as Oracle [ 35], Post-\ngres [ 37], Microsoft SQL Server [ 25], and Snowflake [ 11]. The\nmost popular SMA is the min-max index, which maintains\nthe minimum and maximum value for each field, per block.\nSnowflake also maintains SMAs for auto-detected columns\nin semi-structured data. They are used for simple predicates\nas well as more complex predicates, such as IN. We leverage\nsuch block-level indexes in this paper as well. During query\nprocessing, we use a combination of tree routing and SMA\nindexes to achieve maximum skippability.\n9 CONCLUSION\nRunning queries at interactive speeds on large datasets is\nincreasingly important. In this paper, we address the problem\nof best assigning records to data blocks on storage, with\na goal to optimize for the important metric of number of\nblocks accessed by a query. This metric directly relates to\nthe I/O cost, and therefore performance, of most analytical\nqueries. Current techniques based on hash and time-based\npartitioning or clustering are unable to exploit the workload\nfully and provide blocks with complete semantic descriptions,\nwhich is useful for caching, exploiting additional storage, etc.\nWe propose a new framework called a query-data routing\ntree, orqd-tree , to address this gap. Further, we develop two\nnovel algorithms for qd-tree construction: one based on a\ngreedy approach and the other based on deep reinforcement\nlearning. Experiments over benchmark and real workloads\nshow that a qd-tree can provide physical speedups of more\nthan an order of magnitude compared to current blocking\nschemes, and can reach within 2×of the lower bound for\ndata skipping based on selectivity, while providing complete\nsemantic descriptions of created blocks.\n\nREFERENCES\n[1]Sanjay Agrawal, Nicolas Bruno, Surajit Chaudhuri, and Vivek R\nNarasayya. 2006. AutoAdmin: Self-Tuning Database Systems Technol-\nogy. IEEE Data Eng. Bull. 29, 3 (2006), 7–15.\n[2]Sanjay Agrawal, Vivek Narasayya, and Beverly Yang. 2004. Integrating\nVertical and Horizontal Partitioning into Automated Physical Data-\nbase Design. In Proceedings of the 2004 ACM SIGMOD International\nConference on Management of Data (SIGMOD ’04) . ACM, New York,\nNY, USA, 359–370. https://doi.org/10.1145/1007568.1007609\n[3]Joy Arulraj, Andrew Pavlo, and Prashanth Menon. 2016. Bridging\nthe archipelago between row-stores and column-stores for hybrid\nworkloads. In Proceedings of the 2016 International Conference on Man-\nagement of Data . 583–598.\n[4]Manos Athanassoulis, Kenneth S Bøgh, and Stratos Idreos. 2019. Op-\ntimal column layout for hybrid workloads. Proceedings of the VLDB\nEndowment 12, 13 (2019), 2393–2407.\n[5]Jon Louis Bentley. 1975. Multidimensional binary search trees used\nfor associative searching. Commun. ACM 18, 9 (1975), 509–517.\n[6]Bishwaranjan Bhattacharjee, Sriram Padmanabhan, Timothy Malke-\nmus, Tony Lai, Leslie Cranston, and Matthew Huras. 2003. Efficient\nQuery Processing for Multi-Dimensionally Clustered Tables in DB2.\nInVLDB .\n[7]Nicolas Bruno and Surajit Chaudhuri. 2006. To tune or not to tune?:\na lightweight physical design alerter. In Proceedings of the 32nd in-\nternational conference on Very large data bases . VLDB Endowment,\n499–510.\n[8]Nicolas Bruno and Surajit Chaudhuri. 2007. An online approach to\nphysical design tuning. In 2007 IEEE 23rd International Conference on\nData Engineering . IEEE, 826–835.\n[9]Craig Chasseur and Jignesh M Patel. 2013. Design and evaluation\nof storage organizations for read-optimized main memory databases.\nProceedings of the VLDB Endowment 6, 13 (2013), 1474–1485.\n[10] Carlo Curino, Evan Jones, Yang Zhang, and Sam Madden. 2010. Schism:\na workload-driven approach to database replication and partitioning.\nProceedings of the VLDB Endowment 3, 1-2 (2010), 48–57.\n[11] Benoit Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov,\nArtin Avanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Mar-\ntin Hentschel, Jiansheng Huang, et al .2016. The snowflake elastic\ndata warehouse. In Proceedings of the 2016 International Conference on\nManagement of Data . 215–226.\n[12] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do,\nYinan Li, Hantian Zhang, Badrish Chandramouli, Johannes Gehrke,\nDonald Kossmann, David Lomet, and Tim Kraska. 2020. ALEX: An Up-\ndatable Adaptive Learned Index. In Proceedings of the 2020 International\nConference on Management of Data .\n[13] Anshuman Dutt, Chi Wang, Azade Nazi, Srikanth Kandula, Vivek\nNarasayya, and Surajit Chaudhuri. 2019. Selectivity estimation for\nrange predicates using lightweight models. Proceedings of the VLDB\nEndowment 12, 9 (2019), 1044–1057.\n[14] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad\nMnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dun-\nning, Shane Legg, and Koray Kavukcuoglu. 2018. IMPALA: Scalable\nDistributed Deep-RL with Importance Weighted Actor-Learner Archi-\ntectures. In Proceedings of the 35th International Conference on Machine\nLearning (Proceedings of Machine Learning Research) , Vol. 80. PMLR,\n1407–1416.\n[15] S. Finkelstein, M. Schkolnick, and P. Tiberio. 1988. Physical Database\nDesign for Relational Databases. ACM Trans. Database Syst. 13, 1\n(March 1988), 91âĂŞ128. https://doi.org/10.1145/42201.42205\n[16] Goetz Graefe. 2009. Fast loads and fast queries. In International Confer-\nence on Data Warehousing and Knowledge Discovery . Springer, 111–124.[17] Antonin Guttman. 1984. R-trees: A dynamic index structure for spatial\nsearching . Vol. 14. ACM.\n[18] Theo Härder. 1976. Selecting an optimal set of secondary indices.\nInConference of the European Cooperation in Informatics . Springer,\n146–160.\n[19] Benjamin Hilprecht, Carsten Binnig, and Uwe Röhm. 2019. Towards\nlearning a partitioning advisor with deep reinforcement learning. In\nProceedings of the Second International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management . 1–4.\n[20] Stratos Idreos, Niv Dayan, Wilson Qin, Mali Akmanalp, Sophie Hilgard,\nAndrew Ross, James Lennon, Varun Jain, Harshita Gupta, David Li,\net al.2019. Design Continuums and the Path Toward Self-Designing\nKey-Value Stores that Know and Learn. In CIDR .\n[21] Stratos Idreos, Martin L Kersten, Stefan Manegold, et al .2007. Database\nCracking. In CIDR , Vol. 7. 68–78.\n[22] Stratos Idreos, Stefan Manegold, Harumi Kuno, and Goetz Graefe. 2011.\nMerging what’s cracked, cracking what’s merged: adaptive indexing\nin main-memory column-stores. Proceedings of the VLDB Endowment\n4, 9 (2011), 586–597.\n[23] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis.\n2018. The case for learned index structures. In Proceedings of the 2018\nInternational Conference on Management of Data . ACM, 489–504.\n[24] Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein,\nand Ion Stoica. 2018. Learning to optimize join queries with deep\nreinforcement learning. arXiv preprint arXiv:1808.03196 (2018).\n[25] Per-Åke Larson, Cipri Clinciu, Campbell Fraser, Eric N. Hanson,\nMostafa Mokhtar, Michal Nowakiewicz, Vassilis Papadimos, Susan L.\nPrice, Srikumar Rangarajan, Remus Rusanu, and Mayukh Saubhasik.\n2013. Enhancements to SQL server column stores. In Proceedings of\nthe ACM SIGMOD International Conference on Management of Data,\nSIGMOD 2013, New York, NY, USA, June 22-27, 2013 . 1159–1168.\n[26] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox,\nKen Goldberg, Joseph E. Gonzalez, Michael I. Jordan, and Ion Stoica.\n2018. RLlib: Abstractions for Distributed Reinforcement Learning. In\nInternational Conference on Machine Learning (ICML) .\n[27] Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica. 2019. Neural Packet\nClassification. In Proceedings of the ACM Special Interest Group on Data\nCommunication (SIGCOMM ’19) . ACM, New York, NY, USA, 256–269.\nhttps://doi.org/10.1145/3341302.3342221\n[28] Lin Ma, Dana Van Aken, Ahmed Hefny, Gustavo Mezerhane, Andrew\nPavlo, and Geoffrey J Gordon. 2018. Query-based workload forecasting\nfor self-driving database management systems. In Proceedings of the\n2018 International Conference on Management of Data . 631–645.\n[29] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad\nAlizadeh, Tim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019.\nNeo: A Learned Query Optimizer. PVLDB 12, 11 (2019), 1705–1718.\n[30] Guido Moerkotte. 1998. Small materialized aggregates: A light weight\nindex structure for data warehousing. (1998).\n[31] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov,\nRichard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul,\nMichael I Jordan, et al .2018. Ray: A distributed framework for emerg-\ning AI applications. In 13th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 18) . 561–577.\n[32] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska.\n2020. Learning Multi-dimensional Indexes. In Proceedings of the 2020\nInternational Conference on Management of Data .\n[33] Matthaios Olma, Manos Karpathiotakis, Ioannis Alagiannis, Manos\nAthanassoulis, and Anastasia Ailamaki. 2017. Slalom: Coasting\nthrough raw data via adaptive partitioning and indexing. Proceed-\nings of the VLDB Endowment 10, 10 (2017), 1106–1117.\n[34] OpenAI. 2018. OpenAI Five. https://blog.openai.com/openai-five/.\n[35] Oracle. 2019. https://oracle.com/.\n\n[36] Andrew Pavlo, Carlo Curino, and Stanley Zdonik. 2012. Skew-aware\nautomatic database partitioning in shared-nothing, parallel OLTP sys-\ntems. In Proceedings of the 2012 ACM SIGMOD International Conference\non Management of Data . 61–72.\n[37] PostgreSQL. 2019. https://www.postgresql.org/.\n[38] Warren B Powell. 2007. Approximate Dynamic Programming: Solving\nthe curses of dimensionality . Vol. 703. John Wiley & Sons.\n[39] Warren B Powell. 2016. Perspectives of approximate dynamic pro-\ngramming. Annals of Operations Research 241, 1-2 (2016), 319–356.\n[40] Abdul Quamar, K Ashwin Kumar, and Amol Deshpande. 2013. SWORD:\nscalable workload-aware data placement for transactional workloads.\nInProceedings of the 16th International Conference on Extending Data-\nbase Technology . 430–441.\n[41] Jun Rao, Chun Zhang, Nimrod Megiddo, and Guy Lohman. 2002. Au-\ntomating Physical Database Design in a Parallel Database. In Pro-\nceedings of the 2002 ACM SIGMOD International Conference on Man-\nagement of Data (SIGMOD ’02) . ACM, New York, NY, USA, 558–569.\nhttps://doi.org/10.1145/564691.564757\n[42] Karl Schnaitter, Serge Abiteboul, Tova Milo, and Neoklis Polyzotis.\n2006. Colt: continuous on-line tuning. In Proceedings of the 2006 ACM\nSIGMOD international conference on Management of data . 793–795.\n[43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and\nOleg Klimov. 2017. Proximal policy optimization algorithms. arXiv\npreprint arXiv:1707.06347 (2017).\n[44] Marco Serafini, Rebecca Taft, Aaron J Elmore, Andrew Pavlo, Ashraf\nAboulnaga, and Michael Stonebraker. 2016. Clay: Fine-grained adaptive\npartitioning for general database schemas. Proceedings of the VLDB\nEndowment 10, 4 (2016), 445–456.\n[45] Liwen Sun, Michael J. Franklin, Sanjay Krishnan, and Reynold S. Xin.\n2014. Fine-grained partitioning for aggressive data skipping. In Inter-\nnational Conference on Management of Data, SIGMOD 2014, Snowbird,UT, USA, June 22-27, 2014 . 1115–1126.\n[46] Liwen Sun, Michael J. Franklin, Jiannan Wang, and Eugene Wu. 2016.\nSkipping-oriented Partitioning for Columnar Layouts. PVLDB 10, 4\n(2016), 421–432. https://doi.org/10.14778/3025111.3025123\n[47] Dana Van Aken, Andrew Pavlo, Geoffrey J Gordon, and Bohan Zhang.\n2017. Automatic database management system tuning through large-\nscale machine learning. In Proceedings of the 2017 ACM International\nConference on Management of Data . 1009–1024.\n[48] Eugene Wu and Samuel Madden. 2011. Partitioning techniques for\nfine-grained indexing. In 2011 IEEE 27th International Conference on\nData Engineering . IEEE, 1127–1138.\n[49] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke,\nYinan Li, Umar Farooq Minhas, Per-Åke Larson, Donald Kossmann,\nand Rajeev Acharya. 2020. Qd-tree: Learning Data Layouts for Big\nData Analytics . Technical Report. Microsoft Research, https://aka.ms/\nqdtree-tr.\n[50] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan\nDuan, Xi Chen, Pieter Abbeel, Joseph M Hellerstein, Sanjay Krish-\nnan, and Ion Stoica. 2019. Deep Unsupervised Cardinality Estimation.\nProceedings of the VLDB Endowment 13, 3, 279–292.\n[51] Jingren Zhou, Nicolas Bruno, and Wei Lin. 2012. Advanced Partitioning\nTechniques for Massively Distributed Computation. In Proceedings of\nthe 2012 ACM SIGMOD International Conference on Management of\nData (SIGMOD ’12) . ACM, New York, NY, USA, 13–24.\n[52] Daniel C Zilio, Jun Rao, Sam Lightstone, Guy Lohman, Adam Storm,\nChristian Garcia-Arellano, and Scott Fadden. 2004. DB2 design advisor:\nintegrated automatic physical database design. In Proceedings of the\nThirtieth international conference on Very large data bases-Volume 30 .\n1087–1097.",
  "textLength": 83009
}