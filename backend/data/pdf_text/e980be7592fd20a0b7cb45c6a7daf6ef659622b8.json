{
  "paperId": "e980be7592fd20a0b7cb45c6a7daf6ef659622b8",
  "title": "Hash Adaptive Bloom Filter",
  "pdfPath": "e980be7592fd20a0b7cb45c6a7daf6ef659622b8.pdf",
  "text": "Hash Adaptive Bloom Filter\nRongbiao Xie1yMeng Li1yZheyu Miao2Rong Gu1\u0003He Huang3Haipeng Dai1\u0003Guihai Chen1\nState Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu 210023, CHINA1,\nZhejiang University, Hangzhou, Zhejiang 310058, CHINA2,\nSchool of Computer Science and Technology, Soochow University, Suzhou, Jiangsu 215006, CHINA3\nfrongbiaoxie, menson g@smail.nju.edu.cn, fgurong, haipengdai, gchen g@nju.edu.cn,\npredatory@zju.edu.cn, huangh@suda.edu.cn\nAbstract —Bloom ﬁlter is a compact memory-efﬁcient prob-\nabilistic data structure supporting membership testing, i.e., to\ncheck whether an element is in a given set. However, as Bloom\nﬁlter maps each element with uniformly random hash functions,\nfew ﬂexibilities are provided even if the information of negative\nkeys (elements are not in the set) are available. The problem gets\nworse when the misidentiﬁcation of negative keys brings different\ncosts. To address the above problems, we propose a new H ash\nAdaptive B loom F ilter (HABF) that supports the customization\nof hash functions for keys. The key idea of HABF is to customize\nthe hash functions for positive keys (elements are in the set)\nto avoid negative keys with high cost, and pack customized hash\nfunctions into a lightweight data structure named HashExpressor.\nThen, given an element at query time, HABF follows a two-\nround pattern to check whether the element is in the set.\nFurther, we theoretically analyze the performance of HABF and\nbound the expected false positive rate. We conduct extensive\nexperiments on representative datasets, and the results show that\nHABF outperforms the standard Bloom ﬁlter and its cutting-edge\nvariants on the whole in terms of accuracy, construction time,\nquery time, and memory space consumption (Note that source\ncodes are available in [1]).\nI. I NTRODUCTION\nMembership testing problem refers to testing whether an\nitem is in a given set. It is a fundamental problem in numerous\napplications such as big data applications and databases, where\nthe query latency, memory consumption, and accuracy are the\nprimary performance indicators. To address the problem, a\nlightweight probabilistic data structure named Bloom ﬁlter,\nwith a bit vector of length mas the underlying data structure,\nis proposed [2]. To insert an item into Bloom ﬁlter, the item\nis mapped by khash functions to kbits in the bit vector, and\nallkmapped bits are set to 1. To query an item, the item\nis mapped by khash functions, and it is considered to be a\nmember if all kmapped bits are 1. Due to the compact space\nefﬁciency and satisfactory accuracy, Bloom ﬁlter has been the\ncommon practice in many applications. For example, it is used\nto avoid unnecessary I/O overhead [3] base on log-structured\nmerge (LSM) tree [4] ( e.g., LevelDB [5] and RocksDB [6]) in\nsome key-value databases; to reduce communication cost in a\ndistributed database [7], and to prevent Distributed Denial-of-\nService (DDoS) in network security [8].\nHowever, there is a small probability for Bloom ﬁlter to\nmistakenly identify a negative key ( i.e., a key is not in the\nset) as a positive key ( i.e., a key is in the set), which is called\nyR. Xie and M. Li are the co-ﬁrst authors. \u0003R. Gu and H. Dai are the\ncorresponding authors.false positive [9]. Targeting at decreasing the number of false\npositives, a lot of techniques [9]–[15] have been developed\nover the past decades. However, these works shared a similar\ntechnical path, i.e., reducing the false positives by leveraging\nthe randomness (in terms of hash function mapping) while ig-\nnoring the availability of negative keys in many systems [16]–\n[20]. For example, for intrusion detection, malicious IP address\nstatistics can be obtained from access logs or some well-known\nonline real-time Blacklists such as URIBL [21]; and for LSM-\ntree-based key-value databases [5], [6], the frequently failed\nqueries with heavy I/O overhead can be cached to reduce extra\ndisk accesses. Unfortunately, such negative key information is\nhardly utilized [9]–[15].\nRecently, Bloom ﬁlters empowered by machine learning\n(ML) techniques [16]–[20] are proposed to take advantage of\nthe keys information (including negative keys) by introducing\nin learned models. However, they suffer from the explosive\ngrowth of latency, for query and insert operations ( e.g.,400\u0002\nof standard Bloom ﬁlter [19]), which is quite intolerant for\nmany latency-sensitive applications. Not to mention the extra\ncomputation overhead incurred by the time-consuming training\nphase of learned models, which although can be alleviated\nwith hardware ( e.g., GPU and TPU). Therefore, how to build\na practical ﬁlter that takes advantage of the negative keys\ninformation remains unknown.\nBesides, we consider one further problem, that is, in many\nreal-world systems, the misidentiﬁcation of negative keys ( i.e.,\nfalse positives) brings cost, which may even be highly skewed\nfrom one key to another [22]–[24]. For example, Internet\ntrafﬁcs is highly skewed and concentrates on some popular\nﬁles [25], and popular ﬁles will bring more communication\ncosts than unpopular ﬁles. In LevelDB, accessing data in\ndifferent levels incurs signiﬁcantly different I/O costs from\ndisk accessing [26]. Besides, some cost information can be or\nis already being monitored [22]–[27]. However, Bloom ﬁlter\ncannot directly utilize such cost information because it treats\nall keys equally by sharing identical ﬁxed khash functions.\nNote that the situation may even get worse if the shared hash\nfunctions are not uniformly random or even skewed.\nBefore diving into our proposed solutions, we formally\ndeﬁne the problem considered in this paper as follows: Suppose\nthe positive key set is denoted by S, the negative key set is\ndenoted byO, the global hash function set is H, the number\nof hash functions is k, and for a certain key e, the cost of eis\n\u0002(e). Our problem is how to build a Bloom ﬁlter so that the\noverall cost of false positives from Ois minimized?arXiv:2106.07037v1  [cs.DB]  13 Jun 2021\n\nStandard Bloom \nfiltereee\nHashExpressorYes② Yes② \nYes④ Yes④ \nNo② No② Customized hash functions③ Customized hash functions③ Initial hash functions① Initial hash functions① Fig. 1: Architecture of HABF\nTo address the problem, we propose a new solution, i.e.,\ncustomizing hash functions for each key in Bloom ﬁlter\nindividually according to the given negative keys and their\ncosts information. To be speciﬁc, we aim to select a hash\nfunction set of size kfor each key from Hto construct a\nBloom ﬁlter so that the overall cost of false positives from Ois\nminimized. Besides, the hash customization mechanism avoids\nthe performance degradation from hash function skewness.\nIn this paper, we propose a novel structure named Hash\nAdaptive Bloom Filter (HABF), which consists of two parts\nincluding a standard Bloom ﬁlter and a novel lightweight hash\ntable named HashExpressor, as shown in Fig. 1. The key idea\nof HABF is to customize and store the hash functions at\nconstruction time and then obtain customized hash functions\nat query time. Note that storing hash functions for each key\nis non-trivial as it consumes large memory. Instead, HABF\nallocates initial hash functions for each key, and then adjusts\nthe hash functions for only a small portion of positive keys\nthat incur false positives. During the construction time, we\nﬁrst allocate all keys with krandom initial hash functions\nfrom the global hash functions collection that is available in\nTable II, and then optimize hash function selections for positive\nkeys with our proposed T wo-P hase J oint O ptimization (TPJO)\nalgorithm, which is greedy-based but with performance bound.\nAfter obtaining the optimal hash function selections, we pack\nthem into the previously mentioned lightweight hash table\nHashExpressor. During the query time, a key eﬁrst applies\ninitial hash functions to check whether it is positive. If yes, e\nis believed to be positive. If no, we query a new set of hash\nfunctions from HashExpressor and check with Bloom ﬁlter\nagain. If yes, eis also believed to be positive; otherwise, eis\nconsidered to be negative. Following such a two-round pattern,\nHABF has no false negatives as the standard Bloom ﬁlter.\nAs far as we know, there is no prior work on customizing\nhash functions for keys to address the deﬁned problem above.\nThe most related work is to group keys into disjoint subsets\nand use a different hash function set for each subset [13] to\ndecrease the number of bits equals 1and optimize false positive\nrate (FPR). In a sense, it is only a special case of customizing\nhash functions. Considering that HABF customizes hash func-\ntions according to negative keys and their cost, the information\nneeds to be known during construction time.\nChallenges. In this paper, we are mainly faced with three\nchallenges. The ﬁrst challenge is how to customize hash\nfunctions for positive keys to minimize the overall cost of our\nproblem, as a brute-force search brings exponential complexity.\nTo address the challenge, we propose a performance-boundedgreedy-based algorithm named TPJO to ﬁnd the optimal hash\nfunctions. The second challenge is how to store the optimal\nhash functions of adjusted keys without incurring heavy space\noverhead. To address the challenge, we design a hash table\nnamed HashExpressor by sharing the same space. The third\nchallenge is how to ensure that HABF inherits the nice query\nperformance of the standard Bloom ﬁlter, i.e., no false negative\nrate (FNR) and a small FPR. Considering that each key in\nHABF is mapped with initial hash functions or customized\nhash functions in HashExpressor, to address this challenge,\nthe query of HABF follows a two-round pattern. A key is\nnegative if and only if it is checked not to be in the set after\nthe two-round query.\nContributions. Our principal contributions can be summa-\nrized as follows. Firstly, we consider the scenarios where the\ninformation of negative keys and cost can be obtained, and\nwe propose a novel framework named HABF. Secondly, we\ntheoretically analyze the performance of HABF and bound\nthe expected false positive rate. Thirdly, we evaluate the\nproposed framework on representative datasets to validate its\neffectiveness and efﬁciency. The results show that our HABF\nachieves high accuracy and low cost under the scenarios that\nthe negative keys and their costs information can be obtained\nwhen using the same space size.\nThe rest of this paper is organized as follows. We ﬁrst\nreview related works in Section II. Then we present the\narchitecture of HABF together with the construction/query\nprocedure in Section III. Next, we give the theoretical analysis\nin Section IV. After that, we present our experimental result\nin Section V. Finally, we conclude our work in Section VI.\nII. R ELATED WORK\nIn this section, we ﬁrst review the standard Bloom ﬁlter [2],\nand then three types of variants closely related to our work.\nBloom ﬁlter. The standard Bloom ﬁlter [2] has a bit array\nas the underlying data structure, and supports membership\ntesting query. Bloom ﬁlter provides a one-side error guarantee,\ni.e., small FPR and zero FNR. To be speciﬁc, if a key is\nindicated to be absent in the set by the query result, it is\ndeﬁnitely not in the set (zero FNR). In contrast, if the key\nis indicated to be in the set, it is actually not in the set\nwith a small error probability (FPR). Given the number of\nbits allocated for each key (bits-per-key) b, the FPR can be\nformulated as (1\u0000e\u0000k\nb)k[12] and achieves its minimum\nvalue of 0:6185bwhenk=ln2\u0001b. Unfortunately, as Bloom\nﬁlter shares kidentical hash functions across all keys, it is\ninsensitive to the information of negative keys and cost.\nHash function/ﬁngerprint-based. Gosselin-Lavigne et al.\nevaluated different hash functions and selected several optimal\nones in terms of FPR as the default functions for Bloom ﬁlter\n[28]. However, they only aimed at seeking hash functions with\nbetter implementations. Hao et al. proposed to group keys\ninto disjoint subsets and used a different set of hash functions\nfor each subset [13]. In contrast, we can achieve ﬁne-grained\nhash functions customization for each key. For static datasets,\nBroder et al. proposed to store a ﬁngerprint of each key in its\ncorresponding hash location [29] by designing a perfect hash\nfunction to achieve optimal memory usage. The ﬁngerprint is\n\ngenerated by a hash function, and a key is considered to be\npositive only when its ﬁngerprint is matched. Nonetheless, the\nconstruction incurs heavy computation overhead. Recently, a\nnew ﬁlter named Xor ﬁlter [9] is proposed with optimal mem-\nory usage. However, no further performance gain is achieved\nby it when negative keys and costs are known.\nCost-based. Considering the cost of different keys, Bruck\net al. proposed Weighted Bloom ﬁlter (WBF) to reduce the\noverall cost by setting the number of hash functions for each\nkey according to its cost. [27]. However, when it comes to\nthe query phase, it relies heavily on the cost to calculate the\nnumber of used hash functions for each key, which accordingly\nincurs large additional memory consumption and high query\nlatency from storing and retrieving cost information. Zhong et\nal.also studied how to adjust the number of hash functions\nbased on cost of keys, and posed it as a constrained nonlinear\ninteger programming problem together with two polynomial-\ntime approximation solutions [30]. Similarly, this method\nincurs heavy space overhead to store the optimized number of\nhash functions, and high query latency when retrieving them.\nElasticBF considers a different case where data (key and value)\nare stored in the multi-level LSM tree [4]. To relieve the I/O\ncost brought by accessing hot data in different levels, ElasticBF\nproposes to construct multiple small Bloom ﬁlters for each\nlevel and dynamically load the ﬁlter into memory as needed\nto achieve a ﬁne-grained and elastic control on memory usage\n[26]. However, ElasticBF only aims at cutting down I/O cost\nrather than the overall cost brought by FPR of Bloom ﬁlter.\nLearning-based. Kraska et al. ﬁrst proposed Learned\nBloom ﬁlter (LBF) by incorporating a machine learning (ML)\nmodel to improve space utilization from evident characteristics\nof data distribution [16]. Mitzenmacher proposed to add an\ninitial Bloom ﬁlter before ML model to improve the perfor-\nmance of LBF, which named Sandwiched Learned Bloom ﬁlter\n(SLBF) [17]. Dai et al. proposed Adaptive Learned Bloom\nﬁlter (Ada-BF) to score keys by ML model and tune the\nnumber of hash functions according to the score [18]. Un-\nder incremental workloads, Bhattacharya et al. proposed two\nvariants of LBF for supporting updates [20], one is Classiﬁer-\nAdaptive LBF (CA-LBF) by retraining ML model, the other\nis Index-Adaptive LBF (IA-LBF) by sacriﬁcing memory. With\nan elaborately trained learned model, existing learning-based\nworks could achieve remarkable performance in terms of FPR\nbut at the cost of prolonged training time and query latency.\nBesides, they are not sensitive to cost distribution.\nIII. H ASH ADAPTIVE BLOOM FILTER\nIn this section, we ﬁrst present the model of customizing\nhash functions for each key and formulate the optimization\nproblem. Then we provide the problem observation and our\ndesign insight. Next, we describe the architecture of HABF in\ndetail and the TPJO algorithm is further proposed to optimize\nthe hash function selections. Finally, the Zero-FNR query\nprocedure is provided, followed by the FPR analysis.\nA. Problem Formulation\nLetUdenote the universal key set. Meanwhile, Sis a col-\nlection of positive keys in UandOis a collection of negative\nkeys inU. Note thatSandOare disjoint. Let \u0002denote theTABLE I: Notations\nNotations Deﬁnitions\nS;O Collection of positive keys, negative keys in U\nes;eo Key inS,O\nm Number of bits in Bloom ﬁlter\nk Number of hash functions used by keys\n\u0002(e) Cost of key e\n\u001e(e)k-size hash function subset selected from Hfor\nkeye\nH Global hash functions, H=fh1;h2;:::;hjHjg\nH0 Initial hash function selection\n! Number of cells in HashExpressor\nC[i]ithcell in HashExpressor\nf Uniﬁed hash function of HashExpressor\neck;eopk collision key, optimized key\nV;\u0000 Two runtime-index structures\nV[i];\u0000[i]ithunit inV,ithbucket in\u0000\nFbf;F\u0003\nbf False positive rate of Bloom ﬁlter in HABF\nbefore and after optimization\ncost distribution of keys, i.e.,\u0002(e)is the cost of key e. Given\nthe set of global hash functions H=fh1;h2;\u0001\u0001\u0001;hjHjg. Our\nproblem is how to select a hash function subset \u001e(e)of sizek\nfromHfor each key to minimize the overall cost brought by\nfalse positives of keys from O. To measure the performance\nacross different algorithms, we deﬁne the normalized cost from\nfalse positives as a weighted FPR, namely,\nWeightedFPR =P\ne2O\u0002(e)\u0001Q\nh2\u001e(e)\u001b(h(e))\nP\ne2O\u0002(e); (1)\nwhere\u001b(i)is the value of ithbit in Bloom ﬁlter. In particular,\nwhen\u0002is uniform, the weighted FPR is equivalent to tradi-\ntional FPR. For quick reference, we summarize the notations\nused throughout this paper in Table I.\nB. Observation and Design Insight\nTo optimize Equation (1), a straight design is to go through\nall possible hash function subsets for each key, and choose\nthe one with the optimal weighted FPR. However, such a\nbrute-force method is time-consuming and incurs heavy space\noverhead, i.e., storing hash functions for each key. Besides,\nwe may use machine learning (ML) models to approximate\nand store the optimal hash function subset for each key,\nwhile the ML model needs to be elaborately trained and\nheavy computation overhead for training will inevitably be\nintroduced. Therefore, these designs are impractical.\nFurther, we observe that if the hash function subset of each\nnegative key is ﬁxed, the weighted FPR is only determined\nby the bits equal 1, which are set by (inserted) positive keys.\nInspired by this, we randomly choose a set of hash functions as\ntheinitial hash functions fromHfor each (positive/negative)\nkey and then adjust hash functions for certain positive keys to\nprevent them from conﬂicting with negative keys. Therefore,\nthe majority of (unadjusted) keys stick to the initial hash\nfunctions while the (adjusted) positive keys switch to new\nhash functions. Thus, we only need to store the hash functions\nof (adjusted) keys, rather than that of the universal keys. Let\nH0=fh0\n1;\u0001\u0001\u0001;h0\nkgdenote initial hash functions .\n\n2 fields endbit , hashindex endbit , hashindex\nw cells(a) Structure:\n0, h4 0, h9 0, h2 1, h7 ... 0, 0 0, h4 0, h9 0, h2 1, h7 ... 0, 0f\n {h4, h2, h7}H0e(c) Query:\n0, h4 0, h9 0, h2 1, h7 ... 0, 0f\n {h4, h2, h7}H0e(c) Query:0, 0 0, h2 0, 0 ...fe(b) Insertion:\n {h7, h4, h2}\n {h7, h4, h2} {h7, h4, h2}\nh4h2\n0, 0 0, h2 0, 0 ...fe(b) Insertion:\n {h7, h4, h2}\n {h7, h4, h2} {h7, h4, h2}\nh4h2Cell 1Cell 2Cell 3Cell 4Cell 5 ... Cell w Cell 1Cell 2Cell 3Cell 4Cell 5 ... Cell w\nh4h2\nh9Fig. 2: HashExpressor structure and operations\nC. Architecture\nAs shown in Fig. 1, HABF consists of a standard Bloom ﬁl-\nter and a data structure named HashExpressor. At construction\ntime, HABF customizes the hash functions for each positive\nkey to reduce weighted FPR and stores the customized hash\nfunctions into HashExpressor. At query time, it follows a two-\nround pattern by using H0ﬁrst, and if the query with H0fails,\nthen using hash function subset retrieved from HashExpressor.\nAs shown in Fig. 2(a), HashExpressor is a probabilistic\nstructure composed of !cells, each of which is a 2-tuple:\nhendbit;hashindex i. Theendbit ﬁeld indicates whether the\nqueried hash function subset comes from an adjusted positive\nkey. Thehashindex ﬁeld stores the index of a hash function\nfromH. LetC[i]be theithcell of HashExpressor, C[i]:endbit\nandC[i]:hashindex be theendbit andhashindex ofC[i],\nrespectively. Here, C[i]is empty if both C[i]:endbit and\nC[i]:hashindex are zero. Now, we introduce the two basic\noperations of HashExpressor, i.e., Insertion and Query.\n1) Insertion. For each key eand its hash function subset\n\u001e(e), we ﬁrstly initialize all hash functions in \u001e(e)to be invalid\n(not being inserted already). Then, HashExpressor maps eto\nthe cellC[f(e)]with a predeﬁned hash function f, and there\nare three cases for cell C[f(e)]:\nCase 1: ifC[f(e)]is empty, we randomly choose an invalid\nhash function hfrom\u001e(e)and markhas valid.\nCase 2: ifC[f(e)]is not empty and C[f(e)]:hashindex\nis an invalid hash function in \u001e(e), we mark h=\nC[f(e)]:hashindex in\u001e(e)as valid.\nCase 3:\u001e(e)is failed to be inserted.\nIfC[f(e)]falls into Case 1 or 2, we repeat the above\nmapping procedure but with another hash function h,i.e.,\nmappingeto the next Cell C[h(e)]. The above procedure\nrepeats until all hash functions in \u001e(e)are marked as valid\nand theendbit of cell mapped in the last time will be set to 1.\nThen, we insert the hash functions in \u001e(e)into HashExpressor\nin the order of marking valid. For example, as shown in Fig.\n2(b), when inserting \u001e(e) =fh7;h4;h2g,eis ﬁrst mapped to\nan empty cellh0;0iwithf, we randomly mark h4as valid.\nNegative key?\nNo\nCan be inserted?Valid F ‘(es)\nNoUpdate BF Successful\nFailedYes\nYesAll adjustments failed\nInsertionPhase  -Ⅰ\nPhase  - ⅡStandard Bloom \nfilter\nHashExpressorAdjust F (es) of es \nconflicting with eo to F ‘(es)eoOFig. 3: Procedure of TPJO\nNext,eis mapped to cellh0;h2iwithh4, thus we mark h2as\nvalid. At last, eis mapped to another empty cell h0;0iwith\nh2, we markh7as valid and set the endbit to1. Finally, we\ninsert\u001e(e)into HashExpressor in the order of fh4;h2;h7g.\n2) Query. To retrieve the hash function set \u001e(e)for a keye,\nHashExpressor maps eto the Cellc1with the predeﬁned hash\nfunctionf. Ifc1isempty ,ehas not adjusted hash functions\nand the query procedure fails, \u001e(e) =H0. Otherwise, we store\nhash function hc1fromc1:hashindex into\u001e(e), and then map\neto the next cell c2withhc1. The procedure repeats until the\nsize of\u001e(e)reacheskand theendbit of the last mapped cell\nckis1. If so,\u001e(e) =fhc1;hc2;:::;hckgor\u001e(e) =H0. For\nexample, as shown in Fig. 2(c), we set k= 3. Ifeis mapped\nalong the solid line, eis ﬁrst mapped toh0;h4iwithfand we\ngeth4, theneis mapped toh0;h2iwithh4and we geth2. At\nlast,eis mapped toh1;h7iwithh2and we get h7, since the\nendbit of the last cell is 1, so\u001e(e) =fh4;h2;h7g. However, if\neis mapped along the dotted line, eis mapped to an empty cell\nwithh9, so\u001e(e) =H0. Note that HashExpressor may suffer\nfrom insertion failure when two different keys are mapped to\nthe same cell but can not share the cell space to store hash\nfunctions. To make HashExpressor more compact, we propose\na two-phase joint optimization algorithm to tackle the insertion\nprocedure of HashExpressor together with the optimization\nprocedure of hash function selection simultaneously.\nD. Two-phase Joint Optimization\nIn this subsection, we introduce the proposed T wo-P hase\nJoint O ptimization (TPJO) algorithm, including a phase of\nadjusting hash functions for positive keys ( phase -I) and a\nphase of inserting the adjusted results into HashExpressor\n(phase -II).\nWe ﬁrst describe the high-level design of TPJO algorithm.\nAs shown in Fig. 3, we initialize standard Bloom ﬁlter by\ninserting all positive keys esinSwithH0. Inphase -I,\nfor each key eoinO, we judge whether eois tested to\nbe a negative key. If yes, there is no need to optimize eo.\nOtherwise, we adjust \u001e(es)ofesto\u001e0(es), whereesconﬂicts\nwitheo. If theneocan be tested to be negative, we denote\n\n m units unit 1unit 2unit 3unit 4unit 5 unit m ... unit 1unit 2unit 3unit 4unit 5 unit m ...\n2 fieldssingleflag , keyid singleflag , keyid\n2 fieldssingleflag , keyid1, e s unit 1 1, NULL unit 1\n0, e 5 unit 4 1, e 5 unit 4\n0, e 4 unit m 0, e 4 unit m\n m units unit 1unit 2unit 3unit 4unit 5 unit m ...\n2 fieldssingleflag , keyid1, e s unit 1 1, NULL unit 1\n0, e 5 unit 4 1, e 5 unit 4\n0, e 4 unit m 0, e 4 unit m\nes\nhg\nhjhlFig. 4: Data structure of V\neopkeopkeopk\nhg\nm bucketshjhl\ne5, e7, …, e4, eopk e3, e10, …, e2, eopk e10, e2, …, e6, eopkbucket 1bucket 2bucket 3bucket 4 bucket m ...\nFig. 5: Data structure of \u0000\n\u001e0(es)as valid. Inphase -II, we test whether the valid \u001e0(es)\ncould be inserted into HashExpressor. If yes, we insert \u001e0(es)\ninto HashExpressor and update the Bloom ﬁlter. Otherwise,\nthe insertion in phase -IIfails, and then we obtain a new\n\u001e0(es)inphase -I. Wheneois always tested as a positive\nkey whatever \u001e0(es)is or all valid \u001e0(es)cannot be inserted\ninto HashExpressor, the optimization of eofails. Besides, the\nprobability of insertion failure for HashExpressor will increase\nas the number of inserted keys increases. Therefore, in phase -\nI, we ﬁrst turn to optimize the negative keys with high cost.\nWe now introduce how to implement TPJO algorithm in\ndetail. For a key eoinO, according to whether eoconﬂicts\nwith keys in S, we divideeointo two types: collision key eck\nand optimized key eopk. We ﬁrst deﬁne two runtime auxiliary\ndata structures: one is the index of bits in Bloom ﬁlter that are\nonly mapped by a single positive key in Sand only once, we\ndenote it as V; the other is the index of bits that are mapped\nby optimized key eopk, and we denote it as \u0000.\nTo avoid performance degradation caused by too many\nadjustment operations, we consider adjusting hash functions of\npositive keys from V. Letmbe the number of bits in Bloom\nﬁlter. As shown in Fig. 4, Vis composed of munits, and\neach unit corresponds to one bit in Bloom ﬁlter with the same\nposition and is used to store a 2-tuple:hsingleflag;keyid i.\nLetV[i]be theithunit inV,V[i]:singleflag andV[i]:keyid\nrepresent the singleflag andkeyid ofV[i], respectively.\nV[i]:singleflag indicates whether V[i]is mapped by positive\nkeys at most once, V[i]:keyid is used to store the identiﬁer\n(e.g., a pointer in C++) of eswhich is mapped to V[i]ﬁrst.\nWe initialize the value of V[i]:singleflag to1andV[i]:keyid\ntoNULL . To construct V, we randomly insert all positive\nkeys inSintoV. For a certain key esinS,esneeds to be\ninsertedk=j\u001e(es)jtimes since it has khash functions, and\nwhenesis inserted into a unit u, there are three cases:\nCase 1: Ifu:singleflag = 1 andu:keyid =NULL , the\nidentiﬁer of esis inserted into u:keyid ,e.g., unit 1in Fig. 4,\nwhich changes from h1;NULLitoh1;esi.Algorithm 1: Conﬂict Detection\nInput: Bucket\u0017\nOutput: Conﬂict optimized key set \u0010\u0017.\n1foreopk2\u0017do\n2count = 0\n3 forh2\u001e(eopk)do\n4 if\u0000[h(eopk)]6=\u0017andV[h(eopk)]:keyid6=NULL\nthencount ++endif\n5 end\n6 ifcount isk\u00001then Addeopkto\u0010\u0017endif\n7end\n8return\u0010\u0017.\nVH0\nunit 4 ... unit 4 ...eckeckeck\neseses\nG Hc\nbucket 2 ... bucket 2 ...\nCollision queue\nConflict dectionIf belongs to xck\nIf not exists s (hc (es)) = 1\nFig. 6: Procedure of phase -I\nCase 2: Ifu:singleflag = 1 andu:keyid6=NULL , we\nsetu:singleflag = 0.e.g., unit 4in Fig. 4, which changes\nfromh1;e5itoh0;e5i.\nCase 3: Ifu:singleflag = 0, no changes to u,e.g., unit\nmin Fig. 4.\nThe latter two cases indicate that unit uwill be mapped at\nleast twice by the positive keys in S. Then, we use \u0000to gather\nalleopks which change to collision keys due to the update of\n\u001e(es)in Fig. 3. As shown in Fig. 5, \u0000is composed of m\nbuckets and \u0000[i]represents the ithbuckets in\u0000. Each bucket\ncorresponds to the bit in Bloom ﬁlter with the same position,\nand stores identiﬁers of all eopks mapped to it. For each bucket\n\u0017, we conduct conﬂict detection for \u0017in Algorithm 1. If the\nbit in Bloom ﬁlter corresponding to \u0017changes from 0to1,\nthis operation gathers all conﬂicting eopks in\u0017as set\u0010\u0017.\nNext, we describe how to select hash functions for \u001e(es)\ninphase -Ito speciﬁcally optimize ecks. As shown in Fig.\n6, Collision Queue (abbreviated as CQ below) represents the\nqueue composed of ecks to be optimized, which are arranged in\ndescending order of cost. When optimizing a certain collision\nkeyeck,eckis ﬁrst mapped to VbyH0to obtain units that\nmeet the following conditions:\nsingleflag = 1^keyid6=NULL:\nLet\u0018ckdenote the set of these units, for any u2\u0018ck, it is\nonly mapped once by a single positive key, and we get esby\nu:keyid . Lethube the hash function where esis mapped to\nubyhu, andHcbe the candidate hash functions set, namely\nHc=H\u0000\u001e(es), we conduct an adjustment operation: using\none hash function in Hcto replacehuin\u001e(es).\nIf there exists a hash function hcinHcwhere\u001b(hc(es)) =\n1,eckcan be optimized directly by replacing huwithhc\nwithout generating new collision keys. Otherwise, we map\n\nestojHcjbuckets of\u0000and conduct conﬂict detection for\neach bucket. When detecting bucket \u0017, if\u0010\u00176=;, we call\n\u0017conﬂict after adjustment , which means adding hcto\u001e(es)\nwill makeeopk2\u0010\u0017become a collision key. For convenience,\nwe also denote \u0002(\u0017)as the overall cost of all conﬂicting\noptimized keys in bucket \u0017. If there is a bucket that is not\nconﬂict after adjustment , we can easily use the mapped hash\nfunction to replace huin\u001e(es). Otherwise, we denote \u00170as the\nbucket with the largest non-negative value of (\u0002(eck)\u0000\u0002(\u00170)).\nTo minimize the weighted FPR in Equation (1), we choose\nthe hash function mapped to \u00170to replacehuinphase -I.\nIn particular, if all buckets are conﬂict after adjustment , and\n\u0002(eck)<\u0002(\u0017)for any bucket \u0017, there is no need to optimize\neckas it will bring more cost.\nFor convenience, we deﬁne es2\u0018ckifes=u:keyid;u2\n\u0018ck. If we can optimize eckand insert\u001e(es)into HashEx-\npressor successfully, we insert eckinto\u0000and update V.\nSpeciﬁcally, for updating V, we reset unit uand insertes\ninto a new unit by the exchanged hash function. Besides, if\nthe adjustment generates new collision keys in phase -I, we\ninsert them into the tail of CQ.\nExample: As shown in Fig. 7, we set k= 3 ,H=\nfh1;h2;h3;h4;h5;h6g,H0=fh1;h2;h3g. When optimizing\na collision key e1,e1is ﬁrst mapped to three units in V, the\nsingleflag ofh1;e7iis1, which means it is only inserted by\ne7once. Therefore, we consider adjusting the hash functions\nofe7. Let\u001e(e7) =H0andh2be the hash function of e7to be\nmapped toh1;e7i, we use hash functions in Hc=fh4;h5;h6g\nto replaceh2of\u001e(e7). We assume that only \u001b(h4(e7)) = 1 ,\nso one selection of \u001e(e7)isfh1;h3;h4g. Then we use h5and\nh6to mape7to two buckets and conduct conﬂict detection\nrespectively. For the ﬁrst bucket, we assume there is no\nconﬂiction for already optimized keys after adjustment, which\nindicatesfh1;h3;h5gis also a selection for \u001e(e7). For the\nsecond bucket, we assume e2is conﬂicted after adjustment\nand\u0002(e2)> \u0002 (e1), sofh1;h3;h6gis not a selection.\nTherefore, to optimize e1, there are two candidate adjustment\nselections for e7, and if both of them can not be stored, then\ne1fails to be optimized. Otherwise, among the two choices\n(i.e.,fh1;h3;h4gandfh1;h3;h5g), we store the one with\nmaximized overlap (with hash functions already stored in\nHashExpressor) into HashExpressor.\n (h4 (e7)) = 1Vh1\n0, e6 1, e7 0, e5 ... 0, e6 1, e7 0, e5 ...e1e1\nG e2, e4 e2, e8 ... e2, e4 e2, e8 ...h2\ne7\nh5 h6\n{h1, h3, h5}{h1, h3, h4}h3\nFig. 7: An example for optimizing a collision key\nE. Zero-FNR Query\nAs mentioned before, HashExpressor is a lightweight hash\ntable, and it has a zero FNR and a small FPR. Speciﬁcally,\nletckrepresent the last mapped cell in HashExpressor whenquerying a certain key e. Ifehas been inserted into HashEx-\npressor,ewill deﬁnitely get its hash function selection (zero\nFNR). Otherwise, \u001e(e) =H0. However, if all cells mapped\nbyeare not empty due to conﬂicts and ck:endbit = 1 during\nquery,ewill be misjudged as an inserted key and the queried\n\u001e(e)6=H0which means HashExpressor has a small FPR.\nTo make HABF provide the same query pattern ( i.e.,\nzero FNR) as Bloom ﬁlter, we propose a two-round query\nmechanism as shown in Fig. 1. To be speciﬁc, eﬁrst uses\nH0to check whether it is positive. If yes, eis identiﬁed as\na positive key. If no, we query \u001e(e)from HashExpressor and\nconduct second-round query by using \u001e(e)to check again. If\nyes,eis also identiﬁed to be positive otherwise eis negative.\nNext, we analyze how this two-round query pattern\nachieves zero FNR in HABF. For a certain key esinS,\nthere are two possible cases: 1) if eshas not been inserted\ninto HashExpressor, i.e.,\u001e(es) =H0,eswill be correctly\ntested to be positive by the ﬁrst-round query, and 2) if eshas\nbeen inserted into HashExpressor, namely \u001e(e)6=H0, since\nHashExpressor has zero FNR, eswill get\u001e(e)correctly, and\neswill be also tested to be positive by the second-round query.\nTherefore, HABF achieves Zero-FNR Query.\nF . FPR Analysis\nBased on the two-round query pattern, we analyze the FPR\nof HABF, which is denoted as Fhabf. LetF\u0003\nbfrepresent the\nFPR of Bloom ﬁlter after optimization and Fhrepresent the\nFPR of HashExpressor. For a certain key eoinO, we discuss\nhoweowill be correctly tested to be negative. If eois tested to\nbe negative by H0in the ﬁrst-round query, HashExpressor will\nquery its\u001e(eo)in the second-round query. If HashExpressor\ngives the correct result, namely the queried \u001e(eo) =H0,\nBloom ﬁlter will test eobyH0again, theneowill be ﬁnally\ntested to be negative. Otherwise, if HashExpressor gives an\nincorrect result \u001e0(eo), Bloom ﬁlter will test eowith\u001e0(eo).\nIfeocan be tested to be negative by \u001e0(eo),eowill be ﬁnally\ntested to be negative, too. So Fhabf can be expressed as\nFhabf= 1\u0000(1\u0000F\u0003\nbf)(1\u0000Fh+Fh\u0001(1\u0000F\u0003\nbf))\n=F\u0003\nbf+Fh\u0001F\u0003\nbf\u0000Fh(F\u0003\nbf)2: (2)\nForFh, given a HashExpressor with !cells, iftkeys have\nbeen inserted into HashExpressor, there are at most tcells of\nwhichendbit is set to 1. For simpliﬁcation, we assume that\ntheendbit s of these cells are evenly distributed, then when\nquerying a key e, for the last mapped cell ck, the probability\nofck:endbit being 1 is less than or equal tot\n!. Therefore,\nFh\u0014t\n!, and we can derive that Fhabf\u0014(!+t)\n!\u0001F\u0003\nbf. In fact,\nin the actual optimization process of HABF, if we set tto be\nmuch smaller than !, then we have Fhabf\u0019F\u0003\nbf.\nG. Discussion: Fast Construction and Query.\nConsidering that there is much hash function computa-\ntion during the optimization of HABF, heavy computation\noverhead will inevitably be introduced if a quiet number of\nhash functions are used. Inspired by [12], we employ double\nhashing for some scenarios. To be speciﬁc, we reduce hash\nfunction calculation by simulating a new hash value from\ntwo previously calculated hash values h1(x)andh2(x),e.g.,\n\nsimulated hash values gi(x) =h1(x) +ih2(x)fori=\n0;:::;k\u00001. Note that the double-hashing technique may lead\nto performance degradation [31]. However, targeting at higher\n(query/construction) throughput, we provide a fast version\nHABF with double hashing and denote it as f-HABF . Further,\nfor faster construction in some scenarios, f-HABF speeds up\nthe procedure at the expense of sacriﬁcing partial hash function\nselections by disabling \u0000which contains complex operations\nfor accuracy.\nIV. T HEORETICAL ANALYSIS\nIn this section, we theoretically analyze the performance of\nHABF. We give the theoretical bound for the expected number\nof collision keys that HABF can optimize in CQ. Then, we\nderive the formula of the theoretical bound of F\u0003\nbf.\nA. Analysis for Probability P\u0018\nInphase -I, to avoid performance degradation caused by\ntoo many adjustment operations, we only adjust the hash\nfunctions of positive keys from units in Vthat are inserted\nonly once. For a certain Collision Key eck, these positive keys\nconstitute the set \u0018ck. For any unit uinV, we ﬁrst analyze\nthe probability P\u0018thatu2\u0018ck.\nTheorem 4.1: Ifeckis a collision key, bis the number of\nbits allocated for each key, when eckis mapped to a unit uin\nV, for the probability P\u0018thatubelongs to\u0018ck, we have\nE(P\u0018)>k\nb\nek\nb\u00001: (3)\nProof: For a certain hash function h, we assume that h\nsatisﬁes a distribution p(pmay be non-uniform). For any\nkeye, the probability p(u)thatuis mapped by eusingh\nis determined by its distribution p. LetP1(u)represent the\nprobability that uis only inserted once while all positive keys\nare inserted into Vwithktimes. We assume that the distribu-\ntion ofP1(u)is approximately constant during optimization.\nFor convenience, we deﬁne p2H0ifpis the distribution of\nthe corresponding hash function in H0. Moreover, we assume\nthat the hash functions are independent of each other. Then,\nP1(u)can be expressed as\nP1(u) =jSj\u0001(X\np2H0p(u)Y\np02H0;p06=p(1\u0000p0(u)))\n\u0001(Y\np2H0(1\u0000p(u)))jSj\u00001\n>jSj\u0001X\np2H0p(u)(Y\np2H0(1\u0000p(u)))jSj: (4)\nLetP0(u)represent the probability that uis empty, then\nP0(u) = 1\u0000(Q\np2H0(1\u0000p(u)))jSj. As the units mapped by\neckare inserted at least once, P\u0018(u)can be expressed as a\nconditional probability:\nP\u0018(u) =P1(u)\n1\u0000P0(u)>jSj\u0001P\np2H0p(u)\n1\n(Q\np2H0(1\u0000p(u)))jSj\u00001: (5)Lemma 4.1:8p2H0,0\u0014p(u)\u00141, we have\nY\np2H0(1\u0000p(u))\u00151\u0000X\np2H0p(u): (6)\nLemma 4.2:8x2[0;1],f(x) =jSj\u0001x\n1\n(1\u0000x)jSj\u00001is convex.\nDue to space limitations, the proofs of Lemma 4.1 and\nLemma 4.2 are detailed in the appendix. Let x=P\np2H0p(i), as\nper Lemma 4.1, P\u0018(u)>jSj\u0001x\n1\n(1\u0000x)jSj\u00001=f(x). As per Lemma\n4.2,f(x)is convex, by Jensen inequality [32], we get\nE(P\u0018) =E(P\u0018(u))>E(f(x))\u0015f(E(x)): (7)\nFor any hash function distribution p,E(p(u)) =1\nm, and\nE(x) =E(P\np2H0p(i)) =P\np2H0E(p(i)) =k\nm, so we have\nE(P\u0018)>jSj\u0001k\nm\n(1\u0000k\nm)(\u0000m\nk)\u0001k\nm\u0001jSj\u00001\u0019k\nb\nek\nb\u00001: (8)\nThis completes the proof.\nB. Analysis for F\u0003\nbf\nLetFbfrepresent the FPR of Bloom ﬁlter before optimiza-\ntion, and since HABF only optimizes the ecks, which means\nF\u0003\nbf\u0014Fbf. Lettbe the number of collision keys optimized\nby HABF. Thus for F\u0003\nbf, we can derive that\nF\u0003\nbf=Fbf\u0000t\njOj: (9)\nWe ﬁrst analyze \u0018ckbeforeF\u0003\nbf, for8eck2CQ,eckis\nﬁrst mapped to kunits inV, as per Theorem 4.1, we have\nE(j\u0018ckj) =k\u0001E(P\u0018). Whenk\u00152,E(j\u0018ckj)>2g(1)>\n1:164. We assume that at least one unit belongs to \u0018ck, namely\nj\u0018ckj\u00151(k\u00152) and we consider the worst case of j\u0018ckj= 1.\nLetuckbe the single unit in \u0018ckandeskbe the key in S\ncorresponding to uck:keyid . We denote Pcas the probability\nthateskcan adjust its hash function in phase -IandPsas the\nprobability that \u001e(esk)can be inserted into HashExpressor, Pc\nandPsare independent of each other. For the probability Pck\nthateckcan be optimized, we have\nPck=Pc\u0001Ps: (10)\nFor eacheckinCQ,phase -Iprovides multiple adjustment\nschemes (esand\u001e(es)) to be inserted into cells in HashEx-\npressor until one of them can be inserted. We assume that the\ndistribution of the inserted cells in HashExpressor will tend\nto be approximately uniform. If tcollision keys have been\noptimized, we have\nPs(t)>k\u00001Y\ni=0(1\u0000kt+i\n!)>(1\u0000kt+k\n!)k: (11)\nLetP0\ncbe the probability that \u001e(esk)can be adjusted to a\nvalid\u001e0(esk)when all keys in Oare inserted into \u0000, not just\nthe optimized keys as mentioned before. It is easy to see that\nPc\u0015P0\nc, andP0\ncis not related to t. Due to space limitations,\nthe analysis of P0\ncis detailed in the appendix.\n\nTheorem 4.2: IfTis the size of CQ andtis the number\nof Collsion Keys optimized by HABF, we have\nE(t)>T\u0001P0\nc(!\u0000k2)\n!+T\u0001P0c\u0001k2: (12)\nProof: We denote HABF0as the HABF that changes\noperations as follows: no matter whether eckis optimized\nsuccessfully or not, we insert a virtual positive key with kran-\ndomly selected hash functions into HashExpressor. Let E0(t)\nbe the expected number of collision keys that can be optimized\nby HABF0. It can be seen intuitively that E(t)\u0015E0(t).\nNext, we analyze E0(t). LetP(i)be the probability that\ntheithcollision key in CQ is optimized by HABF0. As per\nEquation (10), we have\nP(i+1)=Pck(i)\u0015P0\nc\u0001Ps(i)>P0\nc(1\u0000k(i+ 1)\n!)k:(13)\nIt is easy to prove that function g0(i) = (1\u0000k(i+1)\n!)kis a\nconvex function, and P0\ncis not related to ias mentioned before.\nBy the Jensen inequality, we have\nE(P(i+1))>P0\nc\u0001E(g0(i))>P0\nc\u0001g0(E(i)): (14)\nFor HABF0, the number of inserted keys in HashExpressor is\nequal to the number of optimized collision keys, E(i) =E0(t),\nthen we have\nE(P(i+1))>P0\nc\u0001g0(E0(t)): (15)\nLemma 4.3: For a random variable Xi,0\u0014i\u0014n, the\nvalue ofXiis 0 or 1, the probability expectation of Xi= 1\nisE(pi),8i;j2N;0\u0014i;j\u0014n;i6=j,XiandXjare\nindependent of each other, we have\nE(nX\ni=0Xi) =nX\ni=0E(pi): (16)\nIt is easy to prove Lemma 4.3 by mathematical induction.\nAs per Equation (11) and Equation (13), P(i+1)is only\ndetermined by i, so80\u0014\u000b;\f\u0014n;\u000b6=\f,P(\u000b)andP(\f)are\nindependent of each other. By Lemma 4.3, we get\nE0(t) =TX\ni=0E(P(i))>T\u0001P0\nc\u0001g0(E0(t)): (17)\nAs per Lemma 4.1, g0(E0(t)) = (1\u0000k(E0(t)+1)\n!)k\u00151\u0000\nk2(E0(t)+1)\n!, we haveE0(t)>T\u0001P0\nc(1\u0000k2(E0(t)+1)\n!), then\nE(t)\u0015E0(t)>T\u0001P0\nc(!\u0000k2)\n!+T\u0001P0c\u0001k2: (18)\nThis completes the proof.\nBased on Theorem 4.2 and Equation (9), we get\nE(F\u0003\nbf) =E(Fbf)\u0000E(t)\njOj\n<E(Fbf)\u0000T\u0001P0\nc(!\u0000k2)\njOj(!+T\u0001P0c\u0001k2)(19)\n2 4 6 8 10\nNumber of Hash Functions k00.511.522.5 FPR (%)Real value\nTheoretic bound(a) Varying number of hash functions\n4 6 810 12\nBits Per Key b051015 FPR (%)Real value\nTheoretic bound (b) Varying bits-per-key\nFig. 8: Real value and theoretic bound\nC. Experimental Veriﬁcation\nTo validate the upper bound of the expected false positive\nrate of HABF in Equation (19), we conduct experiments to\nverify the theoretical bound of F\u0003\nbf. As shown in Fig. 8(a),\nwe set bits-per-key b= 10 and vary the number kof hash\nfunctions from 2to10. In Fig. 8(b), we set k= 4 and varyb\nfrom 4to13. The results show that the theoretical upper bound\nperfectly holds as it is always larger than the real value.\nV. E XPERIMENTAL RESULTS\nIn this section, we conduct experiments to validate the\nperformance of HABF.\nA. Experimental Setup\nThe comparison algorithms can be divided into two types:\n1) Non-learned ﬁlter. We choose a standard Bloom ﬁlter\n(BF) and Xor ﬁlter (Xor) [9] as baselines. Given bits-per-key\nb, we set the number of hash functions k=ln2\u0001bto minimize\nFPR for BF, and set the number of bits of the ﬁngerprint\ntobb\n1:23+32\njSjcfor Xor. The optimized implementation comes\nfrom [33]. Besides, under the skewed cost distribution, we\nalso compare HABF with Weighted Bloom ﬁlter (WBF).\nConsidering WBF relies on cost information during the query,\nthus we cache some keys with high costs in memory for WBF.\n2) Learned ﬁlter. Learned ﬁlter refers to the set of the\nstate-of-the-art works based on learned index [16], including\nLearned Bloom ﬁlter (LBF) [16], Sandwiched Learned Bloom\nﬁlter (SLBF) [17], and Adaptive Learned Bloom ﬁlter (AdaBF)\n[18], which incorporate machine learning (ML) models as\nthe underlying data structures. For the deep-learning model,\nwe implemented a 16-dimensional character-level RNN (GRU\n[34], in particular) and a six-layer fully connected neural\nnetwork [35], both of which have a 32-dimensional embedding\nlayer. Considering that the current learning models generally\nuse GPU to train the model, we also compare the learning\nmodel algorithms in the GPU environment, which we denote\nas LBF (GPU), SLBF (GPU), and AdaBF (GPU).\nImplementation: We implement our algorithm and non-\nlearned ﬁlter algorithms in C++ and compiled using g++ with -\nO3 optimization, and learned ﬁlter algorithms are implemented\nin Keras [36], which is a deep learning platform. We summa-\nrize all used hash functions and their implementations in Table\nII. If not speciﬁed, we set the default hash function used by\nf-HABF and other algorithms to XXH 128. All the programs\nrun on a server with Intel(R) Xeon(R) Gold 6248 CPU with\n10cores running at 2:5GHZ, 106GB memory, and two Tesla\nV100 SXM2 GPUs with 32GB memory. The source codes of\nall algorithms are available in [1].\n\nTABLE II: Hash function set\nHash function Implementation\nxxHash [37]\nCityHash [38]\nMurmurHash [39]\nSuperFast, crc32, FNV [40]\nBOB, OAAT [41]\nDEK, Hsieh, PYHash, BRP, TWMX,\n[42] APHash, NDJB, DJB, BKDR, PJW,\nJSHash, RSHash, SDBM, ELF\nB. Metrics\nWe use the following metrics: (1) weighted FPR; (2)\nconstruction time; (3) query latency; and (4) construction\nmemory consumption. Weighted FPR is deﬁned in Equation\n(1),i.e., suppose the false positive key set from OisO0, then\nWeightedFPR =P\ne02O0\u0002(e0)P\ne2O\u0002(e); (20)\nwhere\u0002(e)is the cost of e. In particular, if \u0002is a uniform\ndistribution function, weighted FPR is equivalent to traditional\nFPR. Moreover, the construction time refers to the time to build\nﬁlters, the query latency refers to the time to conduct member-\nship testing per key, and the construction memory consumption\nrefers to the memory footprint during construction. To achieve\na head-to-head comparison, we set the same bits-per-key for\nevery ﬁlter and thus all ﬁlters use the same space.\nC. Datasets\nWe use the following two datasets in the experiments:\n1) Shalla’s Blacklists. Shalla’s Blacklists [43] is a URL\ndataset with evident characteristics and available in [44]. The\ndataset consists of 2:927 million keys, including 1,491,178\npositive keys and 1,435,527negative keys. For simplicity, we\ncall this dataset Shalla for short if no confusion arises.\n2) YCSB. YCSB is a benchmark [45] for key-value\ndatabases, and we modiﬁed YCSB’s uniform generator to\ngenerate 24,074,812keys, including 12,500,611positive keys\nand11,574,201negative keys. The key schema consists of a 4-\nbyte preﬁx and a 64-bit integer without evident characteristics.\nFor cost distribution, since all keys in both datasets initially\nhave no cost, we generate Zipf [46] distributions with various\nskewness factors (from 0to3:0). In particular, if the skewness\nfactor is 0, the cost distribution is uniform. Moreover, for\neach skewness factor, we randomly shufﬂe the generated Zipf\ndistribution 10times and apply it to each dataset, and then\ncalculate the average weighted FPR.\nD. Parameter Performance Evaluation\nWe ﬁrst evaluate the overall performance of HABF. Let \u00011\nand\u00012be the space size of HashExpressor and Bloom ﬁlter,\nand we deﬁne the space allocation ratio as \u0001=\u00011\n\u00012. Given the\ntotal space size, the performance of HABF is determined by\nthe following three parameters: (1) space allocation ratio \u0001; (2)\nnumber of hash functions k; and (3) cell size of HashExpressor.\nHere, we ﬁrst use Shalla with uniform cost distribution to show\nhow the three parameters affect the performance of HABF.\n0.1 0.3 0.5 0.7 0.900.10.20.30.4 Weighted FPR(%)Weighted FPR vs. \nWeighted FPR vs. k2345678k\n00.10.20.30.4\n (a) Weighted FPR vs. \u0001andk\n1.25 1.75 2.25 2.75 3.25\nSpace Size (MB)0246 Weighted FPR(%)Cell size = 3\nCell size = 4\nCell size = 5 (b) Weighted FPR vs. cell size\nFig. 9: Parameter performance evaluation\n1)Effect of\u0001.We set the space size \u00011+\u00012= 2MB\nand vary\u0001from 0to1. The results in Fig. 9(a) show that\nwhen\u0001is low, the failure probability of \u001e0(e)to be inserted\ninto HashExpressor increases and the weighted FPR is high. In\nparticular, if \u0001= 0, HABF is equivalent to the standard Bloom\nﬁlter. When \u0001is high, there will be lots of Collision Keys\nin Bloom ﬁlter, the probability of HABF optimization failure\nincreases and the weighted FPR is high. The optimal value \u0001\nis0:25, which means that the space allocation ratio between\nHashExpressor and Bloom ﬁlter is 1:4in this case and will be\nused as the default parameter throughout the experiments.\n2)Effect ofk.We set the space size \u00011+\u00012= 2MB and\nvarykfrom 2to8. As shown in Fig. 9(a), HABF achieves\nthe best performance when k= 3;4;5. Ifk<3, the weighted\nFPR increases since the number of hash functions applied to\ncheck a given key decreases. As kbecomes large and k >\n5, the adjusted hash function sets that could be inserted into\nHashExpressor will decrease a lot. k= 3 is a modest choice,\nand we set it by default in the following experiments.\n3)Effect of cell size. The size of a cell is determined by the\nnumber of bits in hashindex . If cell size equals \u000b, each cell\ncan represent at most 2\u000b\u00001\u00001hash functions, which is equal\nto the numberjHjof global hash functions. In our work, we\nprovide 22kinds of hash functions and the maximum size of a\ncell is 5. In this setting of experiments, we vary the space size\nfrom 1:25MB to 3:25MB and compare the performance when\ncell size equals 3,4, and 5. Fig. 9(b) shows that the weighted\nFPR is minimized when the cell size equals 4. We use this\nsetting by default in the following experiments.\nE. Weighted FPR vs. Space Under Uniform Distribution\nIn this experiment, we set the cost distribution of datasets\nto be uniform. According to the deﬁnition of weighted FPR\nin Equation (20), the value of cost for each key is normalized\nto1. We compare the weighted FPRs of HABF and f-HABF\nwith that of BF, Xor, LBF, Ada-BF, SLBF, and WBF.\n1)When the key schema has evident characteristics, HABF\nwill use less space if a low weighted FPR is required. For\nShalla, we vary the space size from 1:25MB to 3:25MB.\nAs shown in Fig. 10(a), HABF always outperforms the non-\nlearned ﬁlters regarding weighted FPR with the same space\nsize. As shown in Fig. 10(b), since the URL blacklist has\nevident characteristics, learned ﬁlters can use only a small\nspace to correctly judge a large part of the keys. At this\ntime, learned ﬁlters will consume a small space to achieve\nthe same weighted FPR. But with lower requirements for\nweighted FPR, learned ﬁlters need more space than HABF.\nWhen increasing the space size to 1:5MB, the weighted FPR\n\n1.25 1.75 2.25 2.75 3.25\nSpace size (Mb)00.511.52Weighted FPR (%)HABFf-HABF\nXorBF(a) vs. Non-learned ﬁlter (Shalla)\n1.25 1.75 2.25 2.75 3.25\nSpace size (Mb)00.511.52Weighted FPR (%)HABFf-HABF\nLBFAda-BF\nSLBF (b) vs. Learned ﬁlter (Shalla)\n12.5 17.5 22.5 27.5 32.5\nSpace size (Mb)00.511.5 Weighted FPR (%)HABFf-HABF\nXorBF (c) vs. Non-learned ﬁlter (YCSB)\n12.5 17.5 22.5 27.5 32.5\nSpace size (Mb)00.511.5 Weighted FPR (%)HABFf-HABF\nLBFAda-BF\nSLBF (d) vs. Learned ﬁlter (YCSB)\nFig. 10: Weighted FPR on uniform distribution\n1.25 1.75 2.25 2.75 3.25\nSpace size (Mb)00.511.52Weighted FPR (%)HABFf-HABF\nXorBF\nWBF\n(a) vs. Non-learned ﬁlter (Shalla)\n1.25 1.75 2.25 2.75 3.25\nSpace size (Mb)00.511.52Weighted FPR (%)HABFf-HABF\nLBFAda-BF\nSLBF (b) vs. Learned ﬁlter (Shalla)\n12.5 17.5 22.5 27.5 32.5\nSpace size (Mb)00.511.5 Weighted FPR (%)HABFf-HABF\nXorBF\nWBF (c) vs. Non-learned ﬁlter (YCSB)\n12.5 17.5 22.5 27.5 32.5\nSpace Size (MB)00.511.5 Weighted FPR (%)HABFf-HABF\nLBFAda-BF\nSLBF (d) vs. Learned ﬁlter (YCSB))\nFig. 11: Weighted FPR on skewed distribution\n/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000048/uni0000000f/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000024/uni00000028 /uni0000003a/uni00000051/uni00000054/uni00000039/uni00000024/uni00000028 /uni0000002e/uni00000024/uni00000028/uni00000023/uni00000046/uni00000043/uni0000000f/uni00000024/uni00000028/uni00000035/uni0000002e/uni00000024/uni00000028\n/uni00000002/uni00000002/uni00000002/uni00000002/uni00000002101102103104105106/uni00000025/uni00000051/uni00000050/uni00000055/uni00000056/uni00000054/uni00000057/uni00000045/uni00000056/uni0000004b/uni00000051/uni00000050/uni00000002/uni00000036/uni0000004b/uni0000004f/uni00000047/uni0000000a/uni00000050/uni00000055/uni00000011/uni0000004d/uni00000047/uni0000005b/uni0000000b/uni00000025/uni00000032/uni00000037 /uni00000029/uni00000032/uni00000037\n(a) Construction time (Shalla)\n/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000048/uni0000000f/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000024/uni00000028 /uni0000003a/uni00000051/uni00000054/uni00000039/uni00000024/uni00000028 /uni0000002e/uni00000024/uni00000028/uni00000023/uni00000046/uni00000043/uni0000000f/uni00000024/uni00000028/uni00000035/uni0000002e/uni00000024/uni00000028\n/uni00000002/uni00000002/uni00000002/uni00000002/uni00000002101102103104105106/uni00000025/uni00000051/uni00000050/uni00000055/uni00000056/uni00000054/uni00000057/uni00000045/uni00000056/uni0000004b/uni00000051/uni00000050/uni00000002/uni00000036/uni0000004b/uni0000004f/uni00000047/uni0000000a/uni00000050/uni00000055/uni00000011/uni0000004d/uni00000047/uni0000005b/uni0000000b/uni00000025/uni00000032/uni00000037 /uni00000029/uni00000032/uni00000037 (b) Construction time (YCSB)\n/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000048/uni0000000f/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000024/uni00000028 /uni0000003a/uni00000051/uni00000054/uni00000039/uni00000024/uni00000028 /uni0000002e/uni00000024/uni00000028/uni00000023/uni00000046/uni00000043/uni0000000f/uni00000024/uni00000028/uni00000035/uni0000002e/uni00000024/uni00000028\n/uni00000002/uni00000002/uni00000002/uni00000002/uni00000002101102103104105106107108/uni00000033/uni00000057/uni00000047/uni00000054/uni0000005b/uni00000002/uni0000002e/uni00000043/uni00000056/uni00000047/uni00000050/uni00000045/uni0000005b/uni00000002/uni0000000a/uni00000050/uni00000055/uni00000011/uni0000004d/uni00000047/uni0000005b/uni0000000b/uni00000025/uni00000032/uni00000037 /uni00000029/uni00000032/uni00000037 (c) Query time (Shalla)\n/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000048/uni0000000f/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000024/uni00000028 /uni0000003a/uni00000051/uni00000054/uni00000039/uni00000024/uni00000028 /uni0000002e/uni00000024/uni00000028/uni00000023/uni00000046/uni00000043/uni0000000f/uni00000024/uni00000028/uni00000035/uni0000002e/uni00000024/uni00000028\n/uni00000002/uni00000002/uni00000002/uni00000002/uni00000002101102103104105106107108/uni00000033/uni00000057/uni00000047/uni00000054/uni0000005b/uni00000002/uni0000002e/uni00000043/uni00000056/uni00000047/uni00000050/uni00000045/uni0000005b/uni00000002/uni0000000a/uni00000050/uni00000055/uni00000011/uni0000004d/uni00000047/uni0000005b/uni0000000b/uni00000025/uni00000032/uni00000037 /uni00000029/uni00000032/uni00000037 (d) Query time (YCSB)\nFig. 12: Construction and query time\nof BF, Xor, LBF, Ada-BF and SLBF is 1:73%,1:56%,0:54%,\n0:51%, and 0:44%, respectively, while HABF achieves 0:36%\nand f-HABF achieves 0:55%.\n2)When the key schema is approximately random, HABF\nhas the smallest weighted FPR for all our space settings. For\nYCSB, we vary the space size from 12:5MB to 32:5MB. As\nshown in Fig. 10(c) and Fig. 10(d), the weighted FPR of HABF\nchanges from 3:46\u000210\u00003to3:63\u000210\u00006, and the weighted\nFPR of f-HABF is around 1:5\u0002than HABF on average, while\nthe weighted FPR of BF, Xor, LBF, Ada-BF, and SLBF change\nfrom 1:78\u000210\u00002to2:83\u000210\u00005,1:57\u000210\u00002to1:59\u000210\u00005,\n7:04\u000210\u00003to1:08\u000210\u00004,3:13\u000210\u00002to1:42\u000210\u00004, and\n6:81\u000210\u00003to1:72\u000210\u00005, respectively. The randomness of\nthe key schema characteristics increases the difﬁculty of ﬁtting\nML model, and the performance of algorithms relying on the\nprediction score [18] of the ML model will be greatly affected.\nThere is a signiﬁcant gap in performance between the two\ndatasets for Ada-BF. By adding a Bloom ﬁlter in the beginning\nto reduce the impact of ML model errors, the performance of\nSLBF will be less affected.\nF . Weighted FPR vs. Space Under Skewed Distribution\nIn this experiment, we vary space size as in the previous\nexperiment and set the cost distribution of datasets to be Zipf\nwith skewness 1:0. Consequently, the weighted FPR is mostly\ncontributed by the false positives of keys with high cost.\nHABF always has the smallest weighted FPR under all the\nspace settings. For Shalla, as shown in Fig. 11(a), compared\nwith non-learned ﬁlter, the weighted FPR of HABF changes\nfrom 8:67\u000210\u00003to2:56\u000210\u00006and the weighted FPR of\nf-HABF changes from 1:37\u000210\u00002to3:86\u000210\u00006, while theweighted FPR of BF, Xor, and WBF change from 2:81\u000210\u00002\nto7:49\u000210\u00005,2:67\u000210\u00002to2:74\u000210\u00005, and 1:83\u000210\u00002to\n8:81\u000210\u00005, respectively. As shown in Fig. 11(b), compared\nwith learned ﬁlter, the weighted FPR of LBF, Ada-BF, and\nSLBF change from 9:78\u000210\u00003to2:3\u000210\u00004,1:72\u000210\u00002\nto2:13\u000210\u00005, and 8:81\u000210\u00003to4:05\u000210\u00005, respectively.\nIt shows that HABF performs better under the skewed cost\ndistribution. For YCSB, as shown in Fig. 11(c) and Fig.\n11(d), compared with both non-learned and learned ﬁlters, the\nweighted FPR of HABF reaches the range from 1:99\u000210\u00003to\n1:97\u000210\u00006. While for other algorithms, the lowest weighted\nFPR changes from 5:80\u000210\u00003to5:14\u000210\u00006.\nG. Effect of Skewness\n00.61.21.82.4 3\nSkewness00.511.52Weighted FPR (%)HABFf-HABF\nBFXor\nFig. 13: Varying skewnessWe further study how\nthe skewness of dataset af-\nfects the weighted FPR as\nshown in Fig. 13. Here we\nuse Shalla dataset and set\nthe space size to 1:5MB,\nand we show how the\nweighted FPR changes as\nthe skewness increases from\n0:0to3:0for HABF, f-\nHABF, BF, and Xor. When the skewness is 0, the weighted\nFPRs follows Fig. 10(a). When the skewness \u00150:9, the\nweighted FPRs of HABF and f-HABF continue to decrease\nsteadily but for BF and Xor, the weighted FPRs show great\nﬂuctuations. The reason is that, as the skewness increases, once\na key with high cost is misjudged, the weighted FPR increases\na lot. Therefore, BF and Xor hardly show any performance\ngain as they are insensitive to the cost distribution.\n\n12.5 17.5 22.5 27.5 32.5\nSpace size (Mb)00.511.5 Weighted FPR (%)HABF BF\nBF(City64) BF(XXH128)(a) On uniform distribution (YCSB)\n12.5 17.5 22.5 27.5 32.5\nSpace size (Mb)00.511.5 Weighted FPR (%)HABF BF\nBF(City64) BF(XXH128) (b) On skew distribution (YCSB)\nFig. 14: Bloom ﬁlter with different implementations\n/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000048/uni0000000f/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000024/uni00000028 /uni0000003a/uni00000051/uni00000054/uni00000039/uni00000024/uni00000028 /uni0000002e/uni00000024/uni00000028/uni00000023/uni00000046/uni00000043/uni0000000f/uni00000024/uni00000028/uni00000035/uni0000002e/uni00000024/uni00000028\n/uni00000002/uni00000002/uni00000002/uni00000002/uni00000002/uni00000012/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni0000002f/uni00000047/uni0000004f/uni00000051/uni00000054/uni0000005b/uni00000002/uni00000028/uni00000051/uni00000051/uni00000056/uni00000052/uni00000054/uni0000004b/uni00000050/uni00000056/uni00000002/uni0000000a/uni00000029/uni00000024/uni0000000b/uni00000025/uni00000032/uni00000037 /uni00000029/uni00000032/uni00000037(a) Shalla\n/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000048/uni0000000f/uni0000002a/uni00000023/uni00000024/uni00000028/uni00000024/uni00000028 /uni0000003a/uni00000051/uni00000054/uni00000039/uni00000024/uni00000028 /uni0000002e/uni00000024/uni00000028/uni00000023/uni00000046/uni00000043/uni0000000f/uni00000024/uni00000028/uni00000035/uni0000002e/uni00000024/uni00000028\n/uni00000002/uni00000002/uni00000002/uni00000002/uni00000002/uni00000012/uni00000017/uni00000013/uni00000012/uni00000013/uni00000017/uni0000002f/uni00000047/uni0000004f/uni00000051/uni00000054/uni0000005b/uni00000002/uni00000028/uni00000051/uni00000051/uni00000056/uni00000052/uni00000054/uni0000004b/uni00000050/uni00000056/uni00000002/uni0000000a/uni00000029/uni00000024/uni0000000b/uni00000025/uni00000032/uni00000037 /uni00000029/uni00000032/uni00000037 (b) YCSB\nFig. 15: Memory footprint of construction\nH. Discussion for Bloom ﬁlter with different implementations\nSince the performance of the Bloom ﬁlter will be af-\nfected by different implementations of the hash function. We\nimplement three versions of Bloom ﬁlter: BF by using k\ndifferent hash functions in Table II. BF (City64) by using\nCityHash (64bit version) and BF (XXH128) by using xxHash\n(128bit version). For the latter two implementations, we use\ndifferent seeds to generate khash values for accuracy. As\nshown in Fig. 14, the dataset is set to YCSB, under the uniform\ndistribution, since the cost of each key is the same, the three\nversions of the Bloom ﬁlter ( i.e.BF, BF (City64) and BF\n(XXH128)) are nearly consistent. Under skewed distribution,\nwe set the cost distribution to skewness 1:0, all Bloom ﬁlter\nimplementations have ﬂuctuated. It demonstrates that even\nwith advanced hash functions like City64 and XXH128, they\nstill can’t effectively reduce weighted FPR and are not sensitive\nto the skew cost distribution.\nI. Construction and Query Time\nIn this part, we compare the construction time and the\nquery time in nanoseconds per key. We ﬁx the space size for\neach algorithm, i.e.,1:5MB for Shalla and 15MB for YCSB.\n1)The construction time per key of HABF and f-HABF\nare around 19\u0002and2:7\u0002larger than that of BF , respectively.\nOn the Shalla dataset, as shown in Fig. 12(a), for HABF,\nthe construction time per key is 1,411ns; for f-HABF, it\nis around 205ns; for BF, it is around 68ns; for Xor, it\nis around 158ns; for WBF, it is around 245ns; while for\nlearned ﬁlters, CPU-based LBF, Ada-BF, and SLBF are around\n36,430ns,38,743ns, and 32,470ns, respectively. LBF (GPU),\nAda-BF (GPU), and SLBF (GPU) take 25,686ns,24,123ns,\nand20,728ns, respectively. On the YCSB dataset, as shown in\nFig. 12(b), HABF, f-HABF, BF, Xor ,and WBF take 1,480ns,\n193ns,84ns,188ns, and 325ns, respectively. LBF (GPU),\nAda-BF (GPU), and SLBF (GPU) take 11,636ns,11,730ns,\nand12,300ns, respectively, while the construction time of\nCPU-based learning models is all above 90,000ns. The con-\nstruction of the learning model is highly dependent on GPU\nespecially for massive data, but for the machines without GPU,\nthe application of learning models is heavily limited. Our fast\nversion, i.e., f-HABF achieves the same order of construction\nspeed as BF and Xor.\n2)The query time of HABF and f-HABF per key is around\n5:35\u0002and1:15\u0002than that of BF , respectively. Fig. 12(c) and\n12(d) show the average query time of all algorithms in two\ndatasets. For Shalla, to query one key, HABF, f-HABF, BF,\nand Xor take 338ns,67ns,52ns, and 48ns, respectively. For\nYCSB, HABF takes 336ns; f-HABF, BF, and Xor take 82ns,79ns, and54ns, respectively. This result indicates the potential\nof the application of HABF in real-time query scenarios. The\nquery time of LBF, Ada-BF, and SLBF are all above 500\u0002\nlarger than that of HABF due to computational complexity\nof ML model, and using GPU to query a key may increase\nthe query time due to the transmission of data between CPU\nand GPU. For WBF, it will traverse the cached cost list when\nquerying a key, which shows that WBF will lead to poor query\nperformance with the size of the cost list increasing.\nJ. Construction Memory Consumption\nIn this part, we ﬁx the space size of each algorithm i.e.,\n1:5MB for Shalla and 15MB for YCSB, and compare the CPU\nmemory footprint during construction. Moreover, we also give\nCPU memory usage for some algorithms using GPU; note that\nhere we allocate all memory of two GPUs to these algorithms.\nThe construction memory consumption of HABF and f-\nHABF is around 6:1\u0002and3:6\u0002than that of BF , which is lower\nthan all learning models. For Shalla, as shown in Fig. 15(a),\nHABF, f-HABF, BF, Xor and WBF consume 0:79GB,0:46GB,\n0:13GB,0:20GB and 0:58GB, respectively. LBF, Ada-BF, and\nSLBF consume 2:59GB,2:78GB, and 2:68GB, respectively.\nDue to the process of dealing and loading data to GPU, LBF\n(GPU), Ada-BF (GPU), and SLBF (GPU) consume more CPU\nmemory which is 3:55GB,3:56GB, and 3:51GB, respectively.\nFor YCSB, as shown in Fig. 15(b), HABF, f-HABF, BF, Xor\nand WBF consumes 7:569GB,4:394GB,1:23GB,1:781GB,\nand2:708GB, respectively. CPU-based and GPU-based learn-\ning models consume memory of 9:88GB and 10:44GB on\naverage, respectively. The reason for extra memory for HABF\nduring construction is that HABF needs to maintain negative\nkeys and two runtime auxiliary data structures.\nVI. C ONCLUSION\nIn this paper, we study the problem of how to customize\nthe hash functions for positive keys to minimize the overall\ncost of the misidentiﬁed negative keys when the information\nof negative keys and their costs are available. We propose a\nnovel framework Hash Adaptive Bloom Filter (HABF), which\nconsists of a standard Bloom ﬁlter, and a novel lightweight\nhash table named HashExpressor for storing the customized\nhash functions. Then, at query time, to provide a one-side\nerror guarantee, HABF follows a two-round pattern to check\nwhether a key is in the set. Besides, to optimize hash function\nselections for positive keys, a greedy-based but performance-\nbounded TPJO algorithm is proposed. Extensive experiments\nshow that HABF outperforms the standard Bloom ﬁlter and its\nvariants on the whole in terms of accuracy, construction time,\nquery time, and memory space consumption.\n\nACKNOWLEDGMENT\nWe thank the reviewers for their thoughtful suggestions.\nThis work was supported in part by the National Natural Sci-\nence Foundation of China under Grant 61872178, in part by the\nNatural Science Foundation of Jiangsu Province under Grant\nNo. BK20181251, in part by the open research fund of Key\nLab of Broadband Wireless Communication and Sensor Net-\nwork Technology (Nanjing University of Posts and Telecom-\nmunications), Ministry of Education, in part by the Key\nResearch and Development Project of Jiangsu Province under\nGrant No. BE2015154 and BE2016120, in part by the National\nNatural Science Foundation of China under Grant 61832005,\nand 61672276, in part by the Collaborative Innovation Center\nof Novel Software Technology and Industrialization, Nanjing\nUniversity, in part by the Jiangsu High-level Innovation and\nEntrepreneurship (Shuangchuang) Program, and in part by the\nNational Natural Science Foundation of China (NO.62072230,\nU1811461) and Alibaba Innovative Research Project.\nREFERENCES\n[1] “The source codes of our and other related algorithms.” https://github.\ncom/njulands/HashAdaptiveBF.\n[2] B. H. Bloom, “Space/time trade-offs in hash coding with allowable\nerrors,” Communications of ACM , pp. 422–426, 1970.\n[3] R. Sears and R. Ramakrishnan, “blsm: a general purpose log struc-\ntured merge tree,” in Proceedings of the International Conference on\nManagement of Data . ACM, 2012.\n[4] P. O’Neil, E. Cheng, D. Gawlick, and E. O’Neil, “The log-structured\nmerge-tree (lsm-tree),” Acta Informatica , pp. 351–385, Springer, 1996.\n[5] “Leveldb. a fast and lightweight key/value database library,” 2011, http:\n//code.google.com/p/leveldb/.\n[6] “A facebook fork of leveldb which is optimized for ﬂash and big\nmemory machines,” 2013, https://rocksdb.org/.\n[7] L. F. Mackert and G. M. Lohman, “R* optimizer validation and\nperformance evaluation for distributed queries,” in Proceedings of Inter-\nnational Conference on Very Large Data Bases . VLDB Endowment,\n1986.\n[8] B. Xiao, W. Chen, and Y . He, “A novel approach to detecting ddos\nattacks at an early stage,” Journal of Supercomputing , pp. 235–248,\nSpringer, 2006.\n[9] T. M. Graf and D. Lemire, “Xor ﬁlters: Faster and smaller than bloom\nand cuckoo ﬁlters,” Journal of Experimental Algorithmics , pp. 1–16,\nACM, 2020.\n[10] S. Cohen and Y . Matias, “Spectral Bloom ﬁlters,” in Proceedings of the\nInternational Conference on Management of Data . ACM, 2003.\n[11] D. Guo, J. Wu, H. Chen, Y . Yuan, and X. Luo, “The dynamic Bloom\nﬁlters,” Transactions on Knowledge and Data Engineering , pp. 120–\n133, IEEE, 2009.\n[12] A. Kirsch and M. Mitzenmacher, “Less hashing, same performance:\nbuilding a better bloom ﬁlter,” in Proceedings of European Symposium\non Algorithms . Springer, 2006.\n[13] F. Hao, M. Kodialam, and T. Lakshman, “Building high accuracy Bloom\nﬁlters using partitioned hashing,” in Proceedings of the international\nconference on Measurement and modeling of computer systems . ACM,\n2007.\n[14] F. Deng and D. Raﬁei, “Approximately detecting duplicates for stream-\ning data using stable Bloom ﬁlters,” in Proceedings of the international\nconference on Management of data . ACM, 2006.\n[15] M. Mitzenmacher, “Compressed bloom ﬁlters,” Transactions on Net-\nworking , pp. 604–612, IEEE, 2002.\n[16] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The\ncase for learned index structures,” in Proceedings of the International\nConference on Management of Data . ACM, 2018.\n[17] M. Mitzenmacher, “A model for learned Bloom ﬁlters and optimizing by\nsandwiching,” in Advances in Neural Information Processing Systems .\nCurran Associates, Inc., 2018.[18] Z. Dai and A. Shrivastava, “Adaptive learned Bloom ﬁlter (Ada-BF):\nEfﬁcient utilization of the classiﬁer,” arXiv preprint , 2019.\n[19] J. W. Rae, S. Bartunov, and T. P. Lillicrap, “Meta-Learning Neural\nBloom Filters,” in Proceedings of International Conference on Machine\nLearning . ACM, 2019.\n[20] A. Bhattacharya, S. Bedathur, and A. Bagchi, “Adaptive learned bloom\nﬁlters under incremental workloads,” in Proceedings of India Joint\nInternational Conference on Data Science and Management of Data .\nACM, 2020.\n[21] “Uribl, Realtime URI Blacklist.” http://uribl.com/.\n[22] B. Babcock and C. Olston, “Distributed top-k monitoring,” in Proceed-\nings of the International Conference on Management of Data . ACM,\n2003.\n[23] G. Cormode and S. Muthukrishnan, “What’s hot and what’s not:\ntracking most frequent items dynamically,” Transactions on Database\nSystems , pp. 249–278, ACM, 2005.\n[24] F. Wu, M.-H. Yang, B. Zhang, and D. H. Du, “Ac-key: Adaptive caching\nfor lsm-based key-value stores,” in Proceedings of Annual Technical\nConference . USENIX Association, 2020.\n[25] L. Breslau, P. Cao, L. Fan, G. Phillips, and S. Shenker, “Web caching\nand zipf-like distributions: Evidence and implications,” in Proceedings\nof International Conference on Computer Communications . IEEE,\n1999.\n[26] Y . Li, C. Tian, F. Guo, C. Li, and Y . Xu, “Elasticbf: elastic bloom\nﬁlter with hotness awareness for boosting read performance in large\nkey-value stores,” in Proceedings of Annual Technical Conference .\nUSENIX Association, 2019.\n[27] J. Bruck, J. Gao, and A. Jiang, “Weighted Bloom ﬁlter,” in Proceedings\nof International Symposium on Information Theory . IEEE, 2006.\n[28] M. A. Gosselin-Lavigne, H. Gonzalez, N. Stakhanova, and A. A.\nGhorbani, “A performance evaluation of hash functions for ip reputation\nlookup using Bloom ﬁlters,” in Proceedings of International Conference\non Availability, Reliability and Security . IEEE, 2015.\n[29] A. Broder and M. Mitzenmacher, “Network applications of Bloom\nﬁlters: A survey,” Internet mathematics , pp. 485–509, 2004.\n[30] M. Zhong, P. Lu, K. Shen, and J. Seiferas, “Optimizing data popularity\nconscious Bloom ﬁlters,” in Proceedings of symposium on Principles\nof distributed computing . ACM, 2008.\n[31] P. C. Dillinger, “Adaptive approximate state storage,” Ph.D. dissertation,\nNortheastern University, 2010.\n[32] G. H. Hardy, J. E. Littlewood, G. P ´olya, and D. Littlewood, Inequalities .\nCambridge university press, 1952.\n[33] “Fastﬁlter.” https://github.com/FastFilter/fastﬁlter cpp.\n[34] K. Cho, B. Van Merri ¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y . Bengio, “Learning phrase representations using\nrnn encoder-decoder for statistical machine translation,” arXiv preprint\narXiv:1406.1078 , 2014.\n[35] J. J. Hopﬁeld, “Neural networks and physical systems with emergent\ncollective computational abilities,” in Proceedings of the national\nacademy of sciences . National Acad Sciences, 1982.\n[36] “Keras, a Deep Learning API,” https://keras.io/.\n[37] “xxhash,” https://github.com/Cyan4973/xxHash.\n[38] “Cityhash,” https://github.com/google/cityhash.\n[39] “Murmurhash.” https://sites.google.com/site/murmurhash/.\n[40] “Smhasher,” https://github.com/rurban/smhasher.\n[41] “R. jenkins.” http://www.burtleburtle.net/bob/hash/doobs.html.\n[42] K. Lovett, “Miscellaneous hash functions.” http://www.\ncall-with-current-continuation.org/eggs/hashes.html.\n[43] “Shalla’s blacklists,” http://www.shallalist.de/index.html.\n[44] K. Singhal and P. Weiss, “DeepBloom,” https://github.com/karan1149/\nDeepBloom/tree/master/data.\n[45] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears,\n“Benchmarking cloud serving systems with YCSB,” in Proceedings of\nsymposium on Cloud computing . ACM, 2010.\n[46] D. M. Powers, “Applications and explanations of Zipf’s law,” in\nProceedings of Association for Computational Linguistics . ACL, 1998.",
  "textLength": 70872
}