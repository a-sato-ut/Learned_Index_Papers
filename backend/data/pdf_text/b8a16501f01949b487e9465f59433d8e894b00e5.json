{
  "paperId": "b8a16501f01949b487e9465f59433d8e894b00e5",
  "title": "A Learned Index for Exact Similarity Search in Metric Spaces",
  "pdfPath": "b8a16501f01949b487e9465f59433d8e894b00e5.pdf",
  "text": "1\nA Learned Index for Exact Similarity Search in\nMetric Spaces\nY ao Tian, Tingyun Y an, Xi Zhao, Kai Huang, and Xiaofang Zhou, Fellow, IEEE\nAbstract â€”Indexing is an effective way to support efï¬cient query processing in large databases. Recently the concept of learned index ,\nwhich replaces or complements traditional index structures with machine learning models, has been actively explored to reduce\nstorage and search costs. However, accurate and efï¬cient similarity query processing in high-dimensional metric spaces remains to be\nan open challenge. In this paper, we propose a novel indexing approach called LIMS that uses data clustering, pivot-based data\ntransformation techniques and learned indexes to support efï¬cient similarity query processing in metric spaces. In LIMS, the\nunderlying data is partitioned into clusters such that each cluster follows a relatively uniform data distribution. Data redistribution is\nachieved by utilizing a small number of pivots for each cluster. Similar data are mapped into compact regions and the mapped values\nare totally ordinal. Machine learning models are developed to approximate the position of each data record on disk. Efï¬cient algorithms\nare designed for processing range queries and nearest neighbor queries based on LIMS, and for index maintenance with dynamic\nupdates. Extensive experiments on real-world and synthetic datasets demonstrate the superiority of LIMS compared with traditional\nindexes and state-of-the-art learned indexes.\nIndex Terms â€”Learned Index, Multi-dimension, Metric Space.\nF\n1 I NTRODUCTION\nSIMILARITY search is one of the fundamental operations\nin the era of big data. It ï¬nds objects from a large\ndatabase within a distance threshold to a given query object\n(called range queries ) or the top- kmost similar to the query\nobject (called knearest neighbor queries , orkNN queries ),\nbased on certain similarity measures or distance functions.\nFor example, in spatial databases, a similarity query can be\nused to ï¬nd all restaurants within a given range in terms\nof the Euclidean distance . In image databases, a similarity\nquery can be used to ï¬nd the top 10 most similar images\nto a given image in terms of the Earth moverâ€™s distance [1].\nTo accommodate a wide range of data types and distance\nfunctions, we consider similarity search in the context of\nmetric spaces in this paper. A metric space is a generic\nspace that makes no requirement of any particular data\nrepresentation, but only a distance function that satisï¬es the\nfour properties, namely non-negativity, identity, symmetry\nand triangle inequality (Deï¬nition 1, Section 3). A number\nof metric-space indexing methods have been proposed in\nthe literature to accelerate similarity query processing [2],\n[3], [4], [5], [6]. However, these indexing methods that are\nbased on tree-like structures are increasingly challenged by\nthe rapidly growing volume and complexity of data. On\nthe one hand, query processing with such indexes requires\ntraversing many index nodes ( i.e.,nodes on the path from\nthe root node to a leaf node) in the tree structure, which\n\u000fY. Tian, X. Zhao, K. Huang and X.F. Zhou are with the Department of\nComputer Science and Engineering, Hong Kong University of Science\nand Technology, Clear Water Bay, Kowloon, Hong Kong.\nE-mail: ytianbc@cse.ust.hk, fxizhao, ustkhuangg@ust.hk, zxf@cse.ust.hk.\n\u000fT.Y. Yan is with the Cyberspace Institute of Advanced Technology,\nGuangzhou University, Guangzhou, China.\nE-mail: tingyun yan@e.gzhu.edu.cn.\nManuscript received December 28, 2021; revised 5 April 2022; published\nonline xx xx xx.can be time-consuming. On the other hand, tree-like indexes\nimpose non-negligible storage pressure on datasets that\nstore complex and large objects, such as image and audio\nfeature data.\nIn recent years, the concept of learned index [7] has been\ndeveloped to provide a new perspective on indexing. By\nenhancing or even replacing traditional index structures\nwith machine learning models that can reï¬‚ect the intrinsic\npatterns of data, a learned index can look up a key quickly\nand save a lot of memory space required by traditional\nindex structures at the same time. The original idea is\nlimited to the one-dimensional case where data is sorted\nin an in-memory dense array. Directly adapting this idea\nfor a multi-dimensional case is unattractive, since multi-\ndimensional data has no natural sort order. Several multi-\ndimensional learned index structures have been proposed\nto address this issue [8], [9], [10], [11], [12], [13], [14],\n[15] (detailed discussions refer to Section 2). Despite the\nsigniï¬cant success of these learned indexes compared with\ntraditional indexing methods, they still have some limita-\ntions. First, the existing learned index structures do not\nsupport similarity search in metric spaces. The metric space\nhas neither coordinate structure nor dimension information\n(Remark 1, Section 3), so the numbering rules ( e.g., z-order\n[16]) and speciï¬c pruning strategies designed for vector\nspaces are not applicable. The triangle inequality is the\nonly property we can utilize to reduce the search space.\nThe generality of metric space provides an opportunity to\ndevelop uniï¬ed indexing methods, while it also presents a\nsigniï¬cant challenge to develop an efï¬cient learned index-\ning method. Second, the existing learned multi-dimensional\nindex structures suffer from the phenomenon called curse\nof dimensionality . By integrating machine learning models\ninto traditional multi-dimensional indexes, these learned\nindexes are restricted to certain types of data space partition-arXiv:2204.10028v2  [cs.DB]  29 Jul 2022\n\n2\ning ( e.g., grid partitioning), which inevitably leads to rapid\nperformance degradation when the number of dimensions\ngrows. Third, the time to train a machine learning model\nthat can well approximate complex data distributions is\ntypically very long, which makes learned indexes difï¬cult to\nadapt to frequent insertion/deletion operations and query\npattern changes. Finally, some existing learned indexes [10]\ncan only return approximate query results, i.e.,there may\nexist false negatives in the result set, because of errors\ncaused by machine learning models.\nTo address the aforementioned limitations, we develop a\nnovel disk-based learned index structure for metric spaces,\ncalled LIMS, to facilitate exact similarity queries ( i.e.,point,\nrange andkNN queries). In contrast to the coordinate-based\ndata partitioning, LIMS adopts a distance-based clustering\nstrategy to group the underlying data into a number of\nsubsets so as to decompose complex and potentially corre-\nlated data into clusters with simple and relatively uniform\ndistributions. LIMS selects a small set of pivots for each\ncluster and utilizes the distances to the pivots to perform\ndata redistribution. This reduces the dimensionality of the\ndata to the number of pivots adopted. By using a proper\npivot-based mapping, LIMS organizes similar objects into\ncompact regions and imposes a total order over the data.\nSuch an organization can signiï¬cantly reduce the number\nof distance computations and page accesses during query\nprocessing. In order to further boost the search performance,\nLIMS follows the idea of learned index , using several simple\npolynomial regression models to quickly locate data records\nthat might match the query ï¬ltering conditions. Further-\nmore, LIMS can be partially reconstructed quickly due to its\nindependent index structure for each cluster, which makes\nLIMS adaptable to changes. As we will show later, LIMS\nsigniï¬cantly outperforms other multi-dimensional learned\nindexes and traditional indexes in terms of the average\nquery time and the number of page accesses, especially\nwhen processing high dimensional data.\nThe main contributions of this paper include:\n\u000fWe design LIMS, the ï¬rst learned index structure for\nmetric spaces, to facilitate exact similarity search.\n\u000fEfï¬cient algorithms for processing point, range and\nkNN queries are proposed, enabling a uniï¬ed solution\nfor searching complex data in a representation-agnostic\nway. An update strategy is also proposed for LIMS.\n\u000fTo the best of our knowledge, no experiment evaluation\nbetween different learned indexes has been performed.\nIn this paper, we compare four multi-dimensional\nlearned indexes. Extensive experiments on real-world\nand synthetic data demonstrate the superiority of LIMS.\nThe rest of the paper is organized as follows. Section 2 re-\nviews related work. Section 3 introduces the basic concepts\nand formulates the research problem. Section 4 describes the\ndetails of LIMS. LIMS-based similarity query algorithms are\ndiscussed in Section 5. Section 6 reports the experimental\nresults. Section 7 concludes the paper.\n2 R ELATED WORK\nWe focus on reviewing learned multi-dimensional indexes\nhere. Good surveys of various traditional metric-space in-\ndexing methods can be found in [2], [3], [4], [5], [6].The idea of learned index is that indexes can be regarded\nas models which take a key as the input and output the\nposition of the corresponding record. If such a â€œblack-boxâ€\nmodel can be learned from data, a query can be processed\nby a function invocation in O(1)time instead of traversing\na tree structure in O(logn)time. RMI is the ï¬rst to explore\nhow to enhance or replace classic index structures with\nmachine learning models [7]. It assumes that data is sorted\nand kept in an in-memory dense array. In light of this, a\nmachine learning model essentially is to learn a cumulative\ndistribution function (CDF). RMI consists of a hierarchy\nof models, where internal nodes in the hierarchy are the\nmodels responsible for predicting the child model to use,\nand a leaf model predicts the position of record. Since RMI\nutilizes the distribution of data and requires no comparison\nin each node, it provides signiï¬cant beneï¬ts in terms of\nstorage consumption and query processing time. For the\nsake of quickly correcting errors caused by machine learning\nmodels and supporting range queries, RMI is limited to\nindex key-sorted datasets, which makes a direct application\nof RMI to multi-dimensional data infeasible because there is\nno obvious ordering of points. Even if these points are em-\nbedded into an ordered space, guaranteeing the correctness\nand efï¬ciency of a query ( e.g., range query and kNN query)\nremains a challenging task.\nZM index is the ï¬rst effort to apply the idea of learned\nindex to multi-dimensional spaces [8]. It adopts the z-order\nspace ï¬lling curve [16] to establish the ordering relationship\nfor all points and then invokes RMI to support point and\nrange queries. The correctness is guaranteed by a nice\ngeometric property of the z-order curve, i.e., monotonic\nordering. However, ZM index needs to check many irrel-\nevant points during the reï¬nement phase, which would get\nworse for high dimensional spaces. It does not support kNN\nqueries and index updates.\nRecursive spatial model index (RSMI) builds on RMI\nand ZM [10]. It develops a recursive partitioning strategy to\npartition the original space, and then groups data according\nto predictions. This results in a learned point grouping,\nwhich is different from RMI that ï¬xes the data layout ï¬rst\nand trains a model to estimate positions. For each partition,\nRSMI ï¬rst maps points into the rank space and then invokes\nZM to support point, range and kNN queries. However, the\ncorrectness of range and kNN queries can not be guaran-\nteed. In addition, since RSMI is still based on space ï¬lling\ncurves, the good performance of RSMI is conï¬ned to low\ndimension spaces.\nLISA [9], a learned index structure for spatial data can\neffectively reduces the number of false positives compared\nwith ZM, by 1) partitioning the original space into grid\ncells based on the data distribution; 2) ordering data with\na partially monotonic mapping function and rearranging\ndata layout according to mapped values; 3) decomposing\na large query range into multiple small ones. LISA has a\ndynamic data layout as RSMI, but the correctness of range\nandkNN queries can be guaranteed by the monotonicity\nof the models. However, its advantage in low scan over-\nhead comes with the costly checking procedure and high\nindex construction time. And the grid-based partitioning\nstrategy makes it unsuitable for high dimensional spaces.\nBesides, LISA-based kNN query processing suffers from\n\n3\nmany repeated page accesses due to doing range queries\nwith increasing radius from the scratch.\nSimilar to LISA, Flood [13] also partitions data space into\ngrid cells along dimensions such that for each dimension,\nthe number of points in each partition is approximately the\nsame. Flood assumes a known query workload and utilizes\nsample queries to learn an optimal combination of indexing\ndimensions and the number of partitions. Once these are\nlearned, Flood maintains a table to record the position of\nthe ï¬rst point in each cell. At query time, Flood invokes\nRMI for each dimension to identify the cells intersecting the\nquery and looks up the cell table to locate the corresponding\nrecords. However, it cannot efï¬ciently adapt to correlated\ndata distribution and skewed query workloads. Tsunami\n[14] extends Flood by utilizing query skew to partition data\nspace into some regions, and then further dividing each re-\ngion based on data correlations. However, simply choosing\na subset of dimensions could degrade the performance as\ndimensionality increases. These studies are not discussed\nfurther as we do not assume a known query workload.\nA multi-dimensional learned (ML) index [11] combines\nthe idea of iDistance [17] and RMI. It ï¬rst partitions data\ninto clusters, and then identiï¬es the cluster center as the ref-\nerence point. After all data points are represented in a one-\ndimensional space based on the distances to the reference\npoint, RMI can be applied. Different from iDistance, ML\nuses a scaling value rather than a constant to stretch the data\nrange. However, points along a ï¬xed radius have the same\nvalue after the transformation, leading to many irrelevant\npoints to be checked. ML does not support data updates.\nNote that ML cannot be directly applied in metric spaces\nsince reference points selection is realized by the KMeans\nalgorithm [18]. The reference point that is the mean of points\nin the clusters may not be in the dataset. It is not always\npossible to create â€œartiï¬cialâ€ objects in metric datasets [19].\nDifferent from ï¬nding a sort order over multi-\ndimensional data and then learning the CDF, a reinforce-\nment learning based R-tree for spatial data (RLR-Tree) [20]\nuses machine learning techniques to improve on the classic\nR-tree index. Instead of relying on hand-crafted heuristic\nrules, RLR-Tree models two basic operations in R-tree, i.e.,\nchoosing a subtree for insertion and splitting a node, as\nMarkov decision process [21], so reinforcement learning mod-\nels can be applied. Because it does not need to modify\nthe basic structure of the R-tree and query processing al-\ngorithms, it is easier to be deployed in the current databases\nsystems than the learned indexes. However, due to the\ncurse of dimensionality, the minimum bounding rectangle\n(MBR) for a leaf node (even in an optimal R-tree) can be\nnearly as large as the entire data space, such that the R-tree\nbecomes ineffective. Similar to RLR-Tree, Qd-Tree [15] uses\nthe reinforcement learning to optimize the data partition-\ning strategy of kd-tree based on a given query workload,\nand suffers from the same problem. These studies are not\ndiscussed further since they are out of our scope.\n3 B ACKGROUND\nIn this section, we ï¬rst introduce some basic concepts and\nthen the formal deï¬nition of learned index for exact similar-\nity search in metric spaces is presented. Table 1 lists the key\nnotations and acronyms used in this paper.TABLE 1: List of key notations\nNotation Description\nP The dataset\nU; dist; d The data space, distance metric, dimensionality\np,q A data point, a query point\nr Query radius\nk The number of nearest neighbors\nK The number of clusters\nm The number of pivots\nN The number of super rings\nCi Theith cluster\nO(i)\njThejth pivots in ith cluster\ndistmax(i)\njThe distance of the furthest object in ith cluster from\nthejth pivot\ndistmin(i)\njThe distance of the nearest object in ith cluster from\nthejth pivot\nDeï¬nition 1 (Metric space) .A metric space is a pair (U;dist ),\nwhere Uis a set of objects and dist :U\u0002U![0;1)is a\nfunction so that8p1;p2;p32U, the following holds:\n\u000fnon-negativity: dist(p1;p2)\u00150;\n\u000fidentity:dist(p1;p2) = 0 iffp1=p2;\n\u000fsymmetry:dist(p1;p2) =dist(p2;p1);\n\u000ftriangle inequality: dist(p1;p3)\u0014dist(p1;p2)+dist(p2;p3).\nRemark 1. Metric space is generic because it only requires the\ndistance function satisfying the above properties. Vector space\nRdwith the Euclidean distance is a special metric space, where\nadditional properties, e.g., the dimension and coordinates, are\nspeciï¬ed, which can be used to accelerate the search.\nIn this paper, we consider three types of exact similarity\nqueries in metric spaces: the range query, point query, and\nkNN query.\nDeï¬nition 2 (Range query) .Given a setP\u0012U, a query object\nq2U, and a query radius r\u00150, a range query returns all\nobjects inPwithin the distance rofq,i.e.,range (q;r) =fp2\nPjdist(p;q)\u0014rg.\nRemark 2. Point query is a special case of range query with\nr= 0 and an arbitrary metric. In this case, we say p=q.\nDeï¬nition 3 (kNN query) .Given a setP\u0012U, a query object\nq2U, and a positive integer k, akNN query returns a set of k\nobjects, denoted as kNN(q;k), such that8p2kNN(q;k);p02\nPnkNN(q;k);dist (q;p)\u0014dist(q;p0):\nExample 1. Consider a word dataset P=f\\fame \";\\gain\";\n\\aim\";\\ACM \"gassociated with the edit distance [22]. A range\nqueryrange (\\game \";2)returns all words in Pwithin the edit\ndistance 2 to \\game \",i.e.,f\\fame \";\\gain\"g. The 1NN query\nkNN(\\game \";1)returns the nearest neighbor of \\game \",i.e.,\nf\\fame \"g.\nPROBLEM STATEMENT . Let (U;dist )be a metric space and\nP=fp1;p2;:::;p ng\u0012Ube a set of objects. The learned\nindex for exact similarity search in metric spaces is to\nlearn an index structure for Pso that point query, range\nquery and kNN query can be processed accurately and\nefï¬ciently. In addition, the index structure is supposed to\nsupport insertion and deletion operations.\n\n4\nâ„›ğ’«!(#)â€¦ğ‘‘ğ‘–ğ‘ ğ‘¡),ğ‘‚!\"â€¦â€¦ğ‘‘ğ‘–ğ‘ ğ‘¡%,ğ‘‚#\"ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘‚!(\")ğ‘‚\"(\")ğ‘‚#(\")ğ‘‘ğ‘–ğ‘ ğ‘¡_ğ‘šğ‘–ğ‘›!(\")ğ‘‘ğ‘–ğ‘ ğ‘¡_ğ‘šğ‘ğ‘¥!(\")ğ‘‘ğ‘–ğ‘ ğ‘¡_ğ‘šğ‘–ğ‘›\"(\")ğ‘‘ğ‘–ğ‘ ğ‘¡_ğ‘šğ‘ğ‘¥\"(\")ğ‘‘ğ‘–ğ‘ ğ‘¡_ğ‘šğ‘–ğ‘›#(\")ğ‘‘ğ‘–ğ‘ ğ‘¡_ğ‘šğ‘ğ‘¥#(\")â„›ğ’«#(#)â„›ğ’«,(#)â„›ğ’«(#)page1page2page3â€¦â€¦ğ‘‘ğ‘–ğ‘ ğ‘¡),ğ‘‚\"\"â€¦â€¦ğ‘‘ğ‘–ğ‘ ğ‘¡),ğ‘‚#\"â€¦â€¦â„³)â€¦ğ‘Ÿğ‘–ğ‘‘!(#)ğ‘Ÿğ‘–ğ‘‘#(#)ğ‘Ÿğ‘–ğ‘‘,(#)LIMSvaluesğ‘¶ğŸ(ğŸ)ğ‘¶ğŸ(ğŸ)ğ‘¶ğŸ(ğŸ)ğ‘¶ğŸ‘(ğŸ)ğ‘¶ğŸ(ğŸ)ğ‘¶ğŸ‘(ğŸ)â„›ğ’«,(#)\n(a)Dataclusteringandpivotsinthemetricspace(b)Indexstructureforclusterğ¶&Clusterğ‘ªğŸ\nClusterğ‘ªğŸ(c)Anexampleofrankpredictionmodel\nFig. 1: LIMS index structure\n4 LIMS\nIn this section, we ï¬rst give an overview of the index\nstructure of LIMS and then present everything needed to\nbuild LIMS. LIMS-based query processing will be discussed\nin Section 5.\n4.1 Overview\nLIMS consists of three parts: the data clustering and pivot\nselection, the pivot-based mapping function and associated\nbinary relationship, as well as rank prediction models. Fig.\n1 gives an overview of LIMS index structure in a metric\nspace associated with the Euclidean distance, although other\nmetric spaces also apply for LIMS. LIMS ï¬rst partitions\nthe underlying data into a set of clusters, e.g., 2 clusters in\nFig. 1(a), so that each of them follows a relatively uniform\ndata distribution, and then a set of data-dependent pivots\nfor each cluster are picked, e.g., 3 pivotsO(1)\n1;O(1)\n2and\nO(1)\n3for cluster C1(detailed discussions refer to Section\n4.3). Then, LIMS maintains a learned index for each clus-\nter separately. Since the index structure is same for each\ncluster, we take cluster C2for example. LIMS computes the\ndistances from each object in the cluster to the well-chosen\npivots, e.g.,dist(\u0001;O(2)\n3)in Fig. 1(b). The maximum and\nminimum distances from each pivot to the corresponding\nobjects, e.g.,distmax(2)\n3anddistmin(2)\n3are stored so as to\nsupport efï¬cient queries. Since objects are sorted by distance\nvalues, LIMS can learn a series of rank prediction models,\ne.g.,RP(2)\n3in Fig. 1(c), for quick computation of the rank\nof an object given its distance to the pivot. After that, a\nwell-deï¬ned pivot-based mapping function Mis called to\ntransform each object into an ordered set (Deï¬nition 6, 7,\nSection 4.2). We call elements in this set LIMS values . Finally,\nwe physically maintain all data objects sequentially on disk\nin ascending order of their LIMS values and the relationship\nbetween LIMS values and the addresses of data objects in\ndisk pages can be learned by another rank prediction model,\ne.g.,RP(2)(detailed discussions refer to Section 4.2).\nIn what follows, we ï¬rst focus on the speciï¬c learned\nindex structure for each cluster, and then turn back to\nclustering and pivot selection methods. In other words, we\nassume that the data space has been partitioned, and the\npivots in each cluster have been determined.4.2 Index Structure\nSuppose that Kclusters, sayfC1;C2;:::;C Kg, andmpiv-\nots for each cluster, say fO(i)\n1;O(i)\n2;:::;O(i)\nmg,i= 1;:::;K ,\nhave been determined. Then, for each cluster Ci;i=\n1;:::;K and pivot O(i)\nj;j= 1;:::;m , all data objects\n(unique identiï¬ers) are sorted in ascending order of their\ndistances to the pivot. Based on msorted lists in Ci, LIMS\nlearnsmrank prediction models. For model reuse, we\ndeï¬ne it formally as follows:\nDeï¬nition 4 (Rank) .LetAbe a ï¬nite multiset drawn from an\nordered setB. For any element x2A, we deï¬ne the rank of xas\nthe number of elements smaller than x,i.e.,\nrank (x) =jfx02Ajx0<xgj: (1)\nExample 2. LetA=f1:5;1:5;1:8;1:8;2:0gbe a multiset of\ndistance values to a given pivot sorted in ascending order. Then,\nrank (1:5) = 0;rank (1:8) = 2;rank (2:0) = 4 .\nDeï¬nition 5 (Rank Prediction Model) .LetAbe a ï¬nite mul-\ntiset drawn from an ordered set B. Rank prediction model RP:\nB![0;+1)is a function learned from f(x;rank (x))gx2A, so\nthat it can predict the rank for any element x2B,i.e.,\nrank (x)\u0019RP (x): (2)\nRemark 3. In the strict sense, there is no deï¬nition of rank for\nelementx2BnA. What we want to express here is the number\nof elements in Asmaller than x. Without confusion, we still use\nrank for simplicity.\nA series of rank prediction models RP(i)\nj(a.k.a. one-\ndimensional learned indexes) can be trained as follows:\nletD(i)\nj=fdist(p;O(i)\nj)gp2Ci, then the training set is\n~D(i)\nj=f(x;rank (x))gx2D(i)\nj. LetRP(i)\njbe a polynomial\nfunction of x, and the loss function Lbe the squared error as\nfollows:\nL=X\n~D(i)\nj\u0010\nRP(i)\nj(x)\u0000rank (x)\u00112\n: (3)\nThen, the rank prediction models can be determined by\nminimizingLloss using the gradient descent . For example, if\nthe degree of polynomial is 2, then RP(i)\nj(x) =ax2+bx+c,\nwherea;b;c are parameters to be learned. After RP(i)\njis\ntrained, we can get the approximate rank for any object\np2Uwith the distance in D(i)\njand the error can be easily\n\n5\ncorrected by exponential search in O(logerr)time, where\nerr is the difference between the estimated rank and the\ncorrect rank. For object p2Uwith the distance not in D(i)\nj,\nwe can also useRP(i)\njand exponential search to ï¬nd its\ncorresponding rank, i.e.,the rank of the ï¬rst element larger\nthandist(p;O(i)\nj)inD(i)\njwith the same time complexity.\nIn order to accelerate query processing, to be introduced\nin Section 5, LIMS divides ranks of data objects into Nequal\nparts, i.e.,makes the data in the cluster covered by Nsuper\nrings as evenly as possible. Fig. 2 gives two examples in\na metric space with the Euclidean distance. In Fig. 2(a),\nthe number of pivots and rings are speciï¬ed as 1 and 3,\nrespectively, so we partition all data objects in the cluster\ninto 3 rings w.r.t. the pivotO(1)\n1such that each ring includes\nabout 5 data objects. In Fig. 2(b), the number of pivots and\nrings are speciï¬ed as 2 and 3, respectively, so we partition\nall data objects into 6 rings, where 3 w.r.t. the pivotO(1)\n1and\n3w.r.t.O(1)\n2. In this way, LIMS effectively avoids that lots\nof objects whose ring IDs deï¬ned in Equation (4) are same,\nwhile a few objects have different ring IDs.\nğ‘¶ğŸ(ğŸ)\n(a)m= 1; N= 3\nğ‘¶ğŸ(ğŸ)ğ‘¶ğŸ(ğŸ)ğ‘%ğ‘&ğ‘' (b)m= 2; N= 3\nFig. 2: Examples of data partitioning\nThe ring ID, i.e.,which ring the data object is located,\ndenoted asrid(i)\nj, can be computed by:\nrid(i)\nj(p) =6664rank\u0010\ndist\u0010\np;O(i)\nj\u0011\u0011\nl\njD(i)\njj=Nm7775: (4)\nWhen the above steps are completed, each data object is\nequipped with mring IDs. Then, LIMS designs a novel\npivot-based mapping function Mto transform all data ob-\njects in the metric space into ordered sets with an associated\nbinary relation\u0014. The formal deï¬nitions are as follows.\nDeï¬nition 6 (Pivot-based mapping function) .Given a clus-\nterCi,i= 1;:::;K , and its corresponding pivots O(i)\nj,\nj= 1;:::;m , letfrid(i)\n1;:::;rid(i)\nmgbe a set ofmring ID\nfunctions. Then, for any data object p2U, we deï¬ne a pivot-\nbased mapping function Mas follows:\nM(p) =rid(i)\n1(p)\brid(i)\n2(p)\b\u0001\u0001\u0001\brid(i)\nm(p): (5)\nWe callM(p)LIMS value of p.\nExample 3. Consider the cluster in Fig. 2(b). The ring IDs of\np1;p2andp3arerid1(p1) = 2 andrid2(p1) = 2 ,rid1(p2) = 1\nandrid2(p2) = 2 ,rid1(p3) = 0 andrid2(p3) = 1 , respectively.\nThe ring IDs of other data objects can be computed similarly.\nAccording to the deï¬nition of pivot-based mapping function,\nM(p1) = 2\b2;M(p2) = 1\b2andM(p3) = 0\b1.In order to build a learned index structure on LIMS\nvalues, we need to impose a binary relationship \u0014between\nLIMS values as follows:\nDeï¬nition 7 (Binary Relation\u0014).LetSbe a multiset of LIMS\nvalues in cluster Ciwithmpivots,i= 1;:::;K . Then,Scan\nbe ordered as follows:\nrid(i)\n1(p)\b\u0001\u0001\u0001\brid(i)\nm(p)\u0014rid(i)\n1(p0)\b\u0001\u0001\u0001\brid(i)\nm(p0)(6)\nif and only if condition (1)or(2)is satisï¬ed.\n1)8j2f1;:::;mg\nrid(i)\nj(p) =rid(i)\nj(p0); (7)\n2)9k2f1;:::;mgsuch that\nrid(i)\nj(p) =rid(i)\nj(p0)forj <k andrid(i)\nk(p)<rid(i)\nk(p0);\n(8)\nwhere \\ = \" and\\<\"in conditions is the order of natural\nnumbers.\nItâ€™s straightforward to prove that the binary relation \u0014in\nDeï¬nition 7 is well-deï¬ned, i.e.,it is reï¬‚exive, antisymmetric\nand transitive [23], so (S;\u0014)is an ordered set. In our imple-\nmentation, we use the concatenation of ring IDs as LIMS\nvalue, which satisï¬es conditions in Deï¬nition 7 obviously.\nExample 4. Reconsider Example 3, LIMS values of p1;p2and\np3are sorted as follows: M(p3)<M(p2)<M(p1).\nNow that all objects in the metric space are trans-\nformed into the corresponding ordered sets, we can sort\nthem sequentially in ascending order of LIMS values, and\nstore data in a number of disk pages with each page\nfully utilized. To quickly locate the addresses of data ob-\njects, LIMS learns a rank prediction model. Speciï¬cally, let\nD(i)=fM(p)gp2Cibe a multiset of LIMS values and\n~D(i)=f(x;rank (x))gx2D(i), then a rank prediction model\nRP(i)can be learned based on ~D(i). Similar to the loss\nfunction in Equation (3), we still try to minimize the squared\nerror. AfterRP(i)is trained, we can get the approximate\nrank (address) for any object p2Uwith the LIMS value\nM(p)2D(i). The error can be easily corrected by ex-\nponential search that stops when the ï¬rst occurrence of\nM(p)is found. For object p2Uwith the LIMS value not\ninD(i), we can also use RP(i)and exponential search to\nï¬nd its corresponding rank, i.e.,the position where the ï¬rst\noccurrence of the element larger than M(p)inD(i).\n4.3 Data Clustering and Pivot Selection\nAs mentioned in [7], one challenge of replacing traditional\ntree-like indexes with learned indexes is that it is difï¬cult\nto approximate complex data distributions with a single\nmodel. If we construct a learned index on the whole dataset\ndirectly, the function to be learned would be very steep\nin regions where the data objects are dense, while very\ngentle for sparse regions. Such a complicated relationship\ncan be ï¬t by a neural network, but it will incur expensive\nquery costs in practice. Based on the observation that real-\nlife data are usually clustered and correlated [24], LIMS\ngroups the underlying data into a number of clusters and\nmaintains a learned index for each cluster instead. This\n\n6\nstrategy gives two advantages: ï¬rst, the data distribution of\neach cluster becomes simpler, which simpliï¬es the model\nto be learned ( e.g., a polynomial function); second, simple\nmodels have lower query and (re-)construction costs. In\nthis paper, we simply adopt the k-center algorithm [25], a\nsimple yet effective algorithm that guarantees to return a 2-\napproximate optimal centroid set. We can follow the same\nsteps to build LIMS on top of other clustering algorithms\nsuch as the kMeans [18], which can potentially further\nimprove our approach. Different from a general clustering\nproblem where the number of clusters can be ï¬‚exible, the\nnumber of clusters used for indexing affects the index\nstructure and search performance. Therefore, we propose a\nstatistic to determine the number of clusters, to be discussed\nin detail in Section 5.4. For now, we assume that the number\nof clusters has been determined.\nOnce the clusters are obtained, LIMS picks a few data\nobjects, named pivots [26], for each cluster and computes\ndistances from each data object in the cluster to pivots.\nThis strategy gives three advantages: ï¬rst, we can use\nthe triangle inequality on these pre-computed distances to\nprune the search space; second, learned indexes can be\nbuilt naturally on these 1-dimensional distance values; third,\nre-distributing data with reference to well-chosen pivots\nmay effectively ease the curse of dimensionality because\nmetric search performance depends critically on the intrinsic\ndimension, a property relying on the data distribution itself,\nas opposed to the dimension where the data is represented\n[2], [19], [27], [28]. For example, the intrinsic dimension\nof a plane is two no matter if it is embedded in a high\ndimensional space. While LIMS is not dependent on the un-\nderlying pivot selection method, the number and locations\nof pivots have an inï¬‚uence on the retrieval performance.\nThe more high-quality pivots there are, the more informa-\ntion they provide and the higher pruning power for query\nprocessing. However, the time taken for checking pruning\nconditions also increases (For LIMS, it mainly refers to the\ncost of generating search intervals, to be discussed in detail\nin Section 5). Extensive methods for pre-deï¬ning an optimal\nset of pivots have been proposed. A good survey can be\nfound in [29]. In our implementation, we adopt the farthest-\nï¬rst-traversal (FFT) algorithm [25] because of its linear time\nand space complexity.\n5 LIMS- BASED QUERY PROCESSING\nIn this section, we proceed to present query processing\nalgorithms for point, range and kNN queries using LIMS.\nSection 5.3 explains dynamic updates. Section 5.4 discusses\nthe choice of K. Without loss of generality, we assume there\nis no two objects in Pare totally same.\n5.1 Range Query\nGiven a query object q, query radius rand dataset P, a range\nquery is to retrieve all objects in Pwithin the distance rof\nq,i.e.,range (q;r) =fp2Pjdist(p;q)\u0014rg. Algorithm 1\noutlines the range query processing, which consists of 4 sub-\nprocedures. TriPrune (Line 1-6): prune irrelevant clusters by\ntriangle inequality; AreaLocate (Line 7-18): determine affected\nareas of relevant clusters with the help of rank predictionAlgorithm 1: Range Query\nInput: q: a query object; r: a query radius\nOutput:S: objects in Psatisfying dist(p; q)\u0014r\n1Letflag[K]be an array of all TRUE ,\nridmin[K][m]; ridmax[K][m]be arrays of all 0;\n2foreach cluster Cido\n3 for each pivot O(i)\njdo\n4 ifdist(O(i)\nj; q)> dist max(i)\nj+rOR\ndist(O(i)\nj; q)< dist min(i)\nj\u0000rthen\n5 flag[i] =FALSE ;\n6 break ;\n7foreach TRUE cluster Cido\n8 foreach pivot O(i)\njdo\n9 rmin maxfdist(O(i)\nj; q)\u0000r; dist min(i)\njg;\n10 rmax minfdist(O(i)\nj; q) +r; dist max(i)\njg;\n11 rank0\nmin; rank0\nmax RP(i)\nj(rmin);RP(i)\nj(rmax);\n12 rank min ExpSearch (rank0\nmin; rmin);\n13 rank max ExpSearch (rank0\nmax; rmax);/*assume\nrank min< rank max , otherwise discard Ci*/;\n14 ridmin[i][j] rid(i)\nj(rank min);\n15 ifD(i)\nj[rank max] =rmax then\n16 ridmax[i][j] rid(i)\nj(rank max);\n17 else\n18 ridmax[i][j] rid(i)\nj(rank max\u00001);\n19generate LIMS-value ranges Rbased on ridmin and\nridmax ;\n20foreach range I2Rdo\n21 lbound0; ubound0 RP(i)(I:left );RP(i)(I:right );\n22 lbound ExpSearch (lbound0; I:left );\n23 ubound ExpSearch (ubound0; I:right );\n24 ifD(i)[ubound ] =I:right then\n25 ubound ExpSearch 2(ubound; I:right );\n26 else\n27 ubound ubound\u00001;\n28 /* assume lbound < ubound , otherwise discard I*/;\n29 add toPall unvisited pages from blbound= \ncto\nbubound= \nc;\n30add toSall objects saved in Psatisfying dist(p; q)\u0014r;\n31returnS\nfunctionsRP(i)\nj;IntervalGen (Line 19): generate search inter-\nvals on LIMS values; PosLocate (Line 20-31): locate positions\nof records in the disk by rank prediction functions RP(i).\nTriPrune (Line 1-6). The algorithm starts by computing the\ndistances between the query qand the pivots, and then\nutilizes triangle inequality property in the metric space to\nprune a number of irrelevant clusters and thus accelerate\nthe search. Speciï¬cally, according to triangle inequality, an\nobjectpin clusterCimay fall into the query range, it must\nsatisfy the following: 8j2f1;:::;mg,\ndist(O(i)\nj;q)\u0000r\u0014dist(O(i)\nj;p)\u0014dist(O(i)\nj;q) +r: (9)\nRecall that LIMS also maintains both maximum distance\ndistmax(i)\njand minimum distance distmin(i)\njof the clus-\nter, hence, for any object pin the cluster, we must have:\n8j2f1;:::;mg,\ndistmin(i)\nj\u0014dist(O(i)\nj;p)\u0014distmax(i)\nj: (10)\nCombing Equations (9) and (10), we can derive that a cluster\nCi;i= 1;:::;K is needed to be further checked if and only\nif the following condition (Line 4) is satisï¬ed:\n\n7\nm^\nj=1[dist(O(i)\nj;q)\u0014distmax(i)\nj+r\n^dist(O(i)\nj;q)\u0015distmin(i)\nj\u0000r]:(11)\nAreaLocate (Line 7-18). For a cluster Cithat needs to be\nsearched, LIMS ï¬rst determines the affected areas in the\nmetric space according to the following equations:\nrmin(i)\nj= maxfdist(O(i)\nj;q)\u0000r;distmin(i)\njg; (12)\nrmax(i)\nj= minfdist(O(i)\nj;q) +r;distmax(i)\njg; (13)\nwherej= 1;:::;m . Then, LIMS invokes corresponding\nrank prediction models RP(i)\njto predict the min and max\nranks of affected areas and the error is ï¬xed via exponential\nsearch (Line 11-13). Min ring ID can be easily calculated by\ncallingrid(i)\njfunction, while the max ring ID is divided to\n2 cases in order to narrow down search range as much as\npossible (Line 14-18).\nIntervalGen (Line 19). Instead of doing intersection of sev-\neral candidate sets in the metric space via costly distance\ncomputations, LIMS reduces the search space by doing\nintersection of LIMS-value intervals directly. Speciï¬cally, for\na relevant cluster Ci;i= 1;:::;K , letL(i)\nj=fridmin[i][j];\nridmin[i][j] + 1;:::;ridmax[i][j]g,j= 1;:::;m\u00001and\nL(i)\nm=fridmin[i][m]; ridmax[i][m]g. Depth-ï¬rst search\n(DFS) is run on a directed acyclic graph (DAG) composed\nof vertexes[m\nj=1L(i)\njand fully connected edges from L(i)\nj\ntoL(i)\nj+1,j= 1;:::;m\u00001, to ï¬nd all paths from L(i)\n1to\nL(i)\nm. These paths form a total ofQm\u00001\nj=1jL(i)\njjLIMS-value\nsearch ranges. It is through IntervalGen that the number of\ndata objects to be accessed is signiï¬cantly reduced, while the\npruning cost remains low. Here is an example of this step.\nExample 5. Consider a cluster with m= 3 pivots andN= 10\nrings. Suppose it is relevant to a range query such that the mini-\nmum and maximum ring IDs of affected region w.r.t. the 1st, 2nd\nand 3rd pivots are ridmin 1= 2,ridmax 1= 4,ridmin 2=\n6,ridmax 2= 8,ridmin 3= 1,ridmax 3= 5, respectively,\ni.e.,L1=f2;3;4g;L2=f6;7;8gandL3=f1;5g. Then,\nLIMS-value search ranges can be computed by running DFS on\nthe DAG shown in Figure 3. The ï¬nal search ranges are the union\nof[2\b6\b1;2\b6\b5],[2\b7\b1;2\b7\b5],[2\b8\b1;2\b8\b5],\n[3\b6\b1;3\b6\b5],[3\b7\b1;3\b7\b5],[3\b8\b1;3\b8\b5],\n[4\b6\b1;4\b6\b5],[4\b7\b1;4\b7\b5]and[4\b8\b1;4\b8\b5].\nPosLocate (Line 20-31). For each search range, the position\nof lower bound in the disk can be easily located via rank\nprediction model RP(i)and exponential search. \nis the\nmaximum number of objects each page can hold. The upper\nbound is divided into 2 cases to make sure the results\ncorrect. For example, itâ€™s possible that different objects have\nthe same LMIS value, so we ï¬nd the last occurrence of\nthe LIMS value via another exponential search, denoted\nasExpSearch 2, to guarantee no objects missed ( i.e., no\nfalse negatives). Finally, all retrieved objects are further\nreï¬ned in the reï¬nement step (Line 30), where exact distance\ncomputations are performed.\nCorrectness. To prove that Algorithm 1 offers exact answers\nfor a range query, we need to show that 1) all data objects\n2 3 4\n67 8\n1 5ğ¿1\nğ¿2\nğ¿3Fig. 3: An illustration of ï¬nding LIMS-value search ranges\nin the result set satisfy dist(p;q)\u0014r,i.e.,no false positives;\nand 2) no objects satisfying dist(p;q)\u0014rare missed i.e.,\nno false negatives. Obviously, no false positives can be\nreturned because a ï¬nal reï¬nement step is applied, where\nthe exact distance computations are performed to guarantee\nall data objects in the result set satisfy dist(p;q)\u0014r.\nWe prove there are no false negatives by contradiction.\nAssume that there exists an object p2Cithat satisï¬es\ndist(p;q)\u0014rbut is not returned. According to the triangle\ninequality, we know that 8j2f1;:::;mg,dist(O(i)\nj;q)\u0014\ndist(O(i)\nj;p)+dist(p;q)\u0014distmax(i)\nj+r:Similarly, we can\nderivedist(O(i)\nj;q)\u0015distmin(i)\nj\u0000r. It indicates that the\nclusterCi(and thusp) will not be pruned in TriPrune step\n(Equation (11)). Since dist(O(i)\nj;p)\u0014dist(O(i)\nj;q) +rand\ndist(O(i)\nj;p)\u0014distmax(i)\nj, we know that dist(O(i)\nj;p)\u0014\nminfdist(O(i)\nj;q) +r;distmax(i)\njg=rmax(i)\nj(Equation\n(13)). Similarly, we can derive dist(O(i)\nj;p)\u0015rmin(i)\nj(Equa-\ntion (12)). Although the rank prediction models RP(i)\njhave\nan error, the error is ï¬xed by exponential search. Thus,\nwe haveridmin[i][j]\u0014rid(i)\nj(p)\u0014ridmax[i][j]after\nAreaLocate step. Denote by l(i)\njthe value in L(i)\njsatisfying\nl(i)\nj=rid(i)\nj(p), thenM(p)can be written as l(i)\n1\b\u0001\u0001\u0001\bl(i)\nm.\nAccording to the procedure of generating LIMS-value search\nranges in IntervalGen step, there must exist a range [l(i)\n1\b\n\u0001\u0001\u0001\bl(i)\nm\u00001\bridmin[i][m];l(i)\n1\b\u0001\u0001\u0001\bl(i)\nm\u00001\bridmax[i][m]].\nBased on the binary relation in Deï¬nition 7, we know that\nM(p)falls in the above search range, and thus in the\nunion of all search ranges after IntervalGen step. The rank\nprediction modelRP(i)inPosLocate step may have an error,\nbut the error is ï¬xed by exponential search. Hence, exact dis-\ntance computations between all objects in the search ranges\nand the query object are performed. pwill be returned,\nwhich leads to a contraction. Therefore, Algorithm 1 can\nanswer the range query correctly.\nQuery Cost. TriPrune takesO(mKD )time, where Drepre-\nsents the cost of distance computation. The cost of AreaLocate\ndepends on rank prediction models. We use O(RP)and\nO(logerr)to denote the prediction cost of RP(i)\njorRP(i),\nand the cost of ï¬xing error incurred by models via expo-\nnential search, respectively. Hence, we need O(mK(RP+\nlogerr))time to locate affected areas of relevant clusters.\nThe cost of IntervalGen is from running DFS on DAG, which\ntakesO(jVj+jEj)time, wherejVjis the number of ver-\ntexes andjEjis the number of edges. Similar to AreaLocate\nsubprocedure, PosLocate takesO(jRj(RP+ logerr))time,\nwherejRj is the number of LIMS-value search ranges.\nGenerally,jRj>jEj>jVj. In addition, we need to access\ndisk pages inPto reï¬ne and retrieve ï¬nial result, which\n\n8\nClusterğ‘ªğŸ\nClusterğ‘ªğŸğ’’ğ‘¶ğŸ(ğŸ)ğ‘¶ğŸ‘(ğŸ)ğ‘¶ğŸ(ğŸ)ğ’’ğ‘¶ğŸ(ğŸ)ğ‘¶ğŸ(ğŸ)ğ‘¶ğŸ‘(ğŸ)ğ’’â„›ğ’«!(#)â„›ğ’«#(#)â„›ğ’«,(#)\nâ„›ğ’«(#)\nğ‘Ÿğ‘ğ‘›ğ‘˜\nâ€¦â€¦â€¦â€¦LIMSvaluesLIMSvaluesğ‘Ÿğ‘ğ‘›ğ‘˜\nDataondisk(a)(b)(c)ğ‘¶ğŸ(ğŸ)ğ‘¶ğŸ(ğŸ)ğ‘¶ğŸ‘(ğŸ)ğ‘¶ğŸ(ğŸ)ğ‘¶ğŸ‘(ğŸ)ğ‘¶ğŸ(ğŸ)ğ‘Ÿ&'(#(\")ğ‘Ÿ&)*#(\")ğ‘Ÿğ‘ğ‘›ğ‘˜\nğ‘Ÿ&'(\"(\")ğ‘Ÿ&+,\"(\")ğ‘Ÿğ‘ğ‘›ğ‘˜\nğ‘Ÿ&'(!(\")ğ‘Ÿ&+,!(\")â€¦\nFig. 4: An example of range query based on LIMS\ntakesO(jPj\nD)time. Therefore, the overall query time is\nO((mK+jRj)(RP+ logerr) + (mK+jPj\n)D).\nExample 6. Figure 4 is an example of LIMS-based range query.\nIt is straightforward to see that cluster C1does not satisfy\ndist(O(1)\n2;q)\u0014distmax(1)\n2+r, and thus can be discarded from\nfurther processing directly, while cluster C2cannot be discarded.\nAs forC2, LIMS ï¬rst inputs the min and max boundaries ( i.e.,\npurple dashed lines in Figure 4(a)) of affected areas into corre-\nsponding rank prediction models RP(2)\njand ring ID functions\nrid(2)\nj,j= 1;2;3. Then, LIMS runs DFS on DAG to transform\nthe intersection of several candidate sets in the metric space (the\ngrey region in Figure 4(a)) to the intersection of intervals (the\ngrey region in Figure 4(b)), to signiï¬cantly reduce the number of\ndistance computations and page accesses. Next, positions of objects\non disk are estimated by the rank prediction function RP(2).\nFinally, candidate objects are retrieved and a reï¬nement step is\napplied (Figure 4(c)).\n5.2 kNN Query\nIn LIMS, akNN query is processed by conducting a series of\nrange queries with increasing search radius r= \u0001r,2\u0001\u0001r,\n3\u0001\u0001r;::: untilknearest neighbors are found, where \u0001ris\na given small initial radius. Algorithm 2 outlines the LIMS-\nbasedkNN query. Given a query qandk, a range query\nwith a radius r= \u0001ris issued at the beginning. To answer\nthis query, LIMS invokes Algorithm 1 (Line 6). Qis a max\npriority queue to record the candidate knearest neighbors. If\nthe distance from the retrieved object pto the query object q\nis smaller than the current furthest distance in Q, LIMS will\nextract the object with the maximum distance value from\nQand insertpintoQ(Line 11-14). The search stops if and\nonly if the furthest object in the current priority queue falls\nwithin the current query range and further expansion of\nthe query radius does not change the answer set (Line 4-\n8). Otherwise, it enlarges the query radius by \u0001r(Line 3)\nand invokes Algorithm 1 till the termination conditions are\nsatisï¬ed. LIMS also maintains an array to record whether\na page has been processed. If so, it will be skipped during\nthe next range query to avoid repeated accesses (Line 10).\nThe number of range query calls depends on \u0001rand the\ndistance between qand thek-th NN in the dataset, which\ncan be determined by ddist(Q[1];q)\n\u0001re+ 1.Algorithm 2: kNN Query\nInput: q: a query point; k: a positive integer; \u0001r: a positive\nnumber\nOutput:Q: topknearest neighbors to q\n1r 0; flag FALSE ,Qis a max priority queue on k\nobjects initialized to 1;\n2while flag =FALSE do\n3 r r+ \u0001r;\n4 ifdist(Q[1]; q)< r then\n5 flag TRUE ;\n6 callrange (q; r)to get pages setP;\n7 callcheck (P);\n8 returnQ;\n9procedure check (P):\n10foreach unvisited page P2P do\n11 foreach point p2Pdo\n12 ifdist(Q[1]; q)> dist (p; q)then\n13 Extract-Max(Q);\n14 Insert(Q; p);\nRemark 4. Intuitively, the initial radius \u0001raffects the number of\nrange query calls, and thus the query efï¬ciency. We observed that\na small \u0001rwould not degrade the query performance severely. As\nwe know, the extra cost of using range queries with increasing\nradius to answer a kNN query mainly comes form 1) traversing\nthe index multiple times and 2) accessing the same pages multiple\ntimes. In LIMS, a query is processed by a function invocation in\nO(1)time instead of O(logn)time in tree-like indexes, which\nmakes the cost of multiple traverses negligible. Besides, LIMS\nwould not access visited pages. However, a too large initial radius\ncan degrade the query performance. Therefore, we recommend a\nsmall initial search radius, which can be simply estimated based on\ndistances between pairs of data objects sampled from the dataset.\n5.3 Updates\nLIMS allows both insertions and deletions of data. To sup-\nport the efï¬cient insertion, LIMS maintains a sorted array\nfor each cluster in ascending order of distance values to\nthe centroid. Given an object pto be inserted, LIMS ï¬rst\nruns a point query to ï¬nd if there is a page containing\np. If so, the algorithm terminates immediately. Otherwise,\nLIMS ï¬nds the cluster closest to pand inserts pinto the\nsorted array of this cluster. All newly inserted data are\narranged in a number of pages with each page fully utilized.\n\n9\nDuring query processing, LIMS uses the triangle inequality\nand exponential search to retrieve these inserted objects\nmatching query ï¬lters. Given an object pto be deleted, LIMS\nï¬rst runs a point query for p. Ifpis found, it is marked\nas â€˜deletedâ€™. Then, LIMS updates maximum and minimum\ndistances to each pivot of the cluster pbelongs to. Due to\nthe space limitation, we omit the pseudocode here. Such\nan easy update strategy is effective and efï¬cient because\n1) LMIS partitions the data space into many clusters that\namortize the additional search time on the sorted arrays; 2)\nLIMS maintains an index for each cluster separately, which\nallows for partially rebuilding the index, i.e.,retraining rank\nprediction models for certain clusters, especially if deletions\ninvalidate a cluster or insertions result in too much over-\nlapping between some clusters. The procedure of retraining\nrank prediction models is the same as that of training them;\n3) the short index construction (reconstruction) time of LIMS\nensures the feasibility of this strategy in practice (Section 6).\n5.4 Last Piece\nThe last piece of LIMS is to determine the number of\nclusters. The optimal number is related not only to the\ndata distribution but also to the query workload. However,\nthe query workload is not available during data clustering\nand we do not assume a known query workload in this\npaper. Therefore, an alternative should be developed to pre-\ndeï¬ne the number of clustering parameter K. Recall that\nthe goal of clustering in LIMS is to decompose complex and\npotentially correlated data into a few clusters. Ideally, these\nclusters are independent and each can be accurately ï¬t by a\nlinear function. However, in most real-world use cases, the\ndata do not follow such a perfect pattern. On the one hand,\nthe overlapping between clusters may incur extra pruning\nand reï¬nement costs. On the other hand, uneven intra-\ncluster distribution incurs more arithmetic and comparison\noperations. In order to pick a Kto avoid or reduce such\noverhead, we introduce overlap rate (OR) and mean absolute\nerror (MAE) to evaluate the goodness of clustering. OR\nquantiï¬es the extent of overlapping among clusters. It can\nbe computed as:\nOR=1\nK(K\u00001)KX\ni=1X\ni06=ir(i;i0)\ndistmax(i)\n1: (14)\nWithout confusion, we use distmax(i)\n1to represent the dis-\ntance of the furthest object in ith cluster from the centroid,\nandr(i;i0)is the length of the overlapping area computed as:\nr(i;i0)= minfdist(O(i)\n1;O(i0)\n1) +distmax(i0)\n1;distmax(i)\n1g\n\u0000maxf(dist(O(i)\n1;O(i0)\n1)\u0000distmax(i0)\n1;distmin(i)\n1g:\n(15)\nMAE quantiï¬es the quality of the linear regression ï¬t for\neach cluster. It can be computed as:\nMAE =1\nmjPjKX\ni=1mX\nj=1X\n~D(i)\njja(i)\njx+b(i)\nj\u0000rank (x)j;(16)\nwherejPjis the cardinality of dataset, RP(i)\nj(x) =a(i)\njx+\nb(i)\njare linear rank prediction models learned from ~D(i)\nj.TABLE 2: Summary of datasets\nDatasets Cardinality Dim. Ins. Metric\nColor 1,281,167 32 4.2 L2-norm\nForest 565,892 6 1.5 L2-norm\nGaussMix 5,10,20,40,60,80M 2,4,8 ,12,16 6.2 L2-norm\nSkewed 10M 2,4,8 ,12,16 5.6 L1-norm\nSignature 100K 65 36 Edit distance\nWe model the overhead as OR+\u0015MAE , where\u0015>0is\na user-deï¬ned weight. Inspired by the elbow method [30],\nwe choose the elbow orknee of a curve as the clustering\nnumber to use, i.e.,a point where adding more clusters will\nnot give much better modeling of the data. The number\nestimated by the techniques described above turns out to\nbe very close to the optimal number of clusters observed in\npractice, as shown in Section 6.\n6 E XPERIMENTS\nIn this section, we present the results of an in-depth exper-\nimental study on LIMS. We implement LIMS1and associ-\nated similarity search algorithms in C++. All experiments\nare conducted on a computer running 64-bit Ubuntu 20.04\nwith a 2.30 GHz Intel(R) Xeon(R) Gold 5218 CPU, 254 GB\nRAM, and an 8.2 TB hard disk.\n6.1 Experimental Settings\nDatasets. We employ two real-world datasets, namely, Color\nHistogram2and Forest Cover Type3following the experi-\nmental settings of ML index [11]. Color Histogram contains\n1,281,167 32-dimensional image features extracted from the\nImageNet4dataset. Forest Cover Type is collected by US\nGeological Survey and US Forest Service. It includes 565,892\nrecords, each of which has 12 cartographic variables. We\nextract 6 quantitative variables of them as our data object.\nFollowing the experimental settings of iDistance [17], we\ngenerate 2, 4, 8, 12, 16-dimensional GaussMix datasets. Every\ndataset contains up to 80 million points (5.36GB in size)\nsampled from 150 normal distributions with the standard\ndeviation of 0.05 and randomly determined means. Default\nsettings are underlined. Without loss of generality, L2norm\nis utilized to the above datasets. Following the experimental\nsettings of RSMI [10], we create 2, 4, 8, 12, 16-dimensional\nSkewed datasets. They are generated from uniform data\nby raising the values in each dimension to their powers,\ni.e.,a randomly generated data point are converted from\n(x1;x2;:::;x d)to(x1;x2\n2;:::;xd\nd). The size of each dataset\nis 10 million and L1norm is employed. Without loss of\ngenerality, all the data values of the above datasets are\nnormalized to the range [0;1]. Following the experimental\nsettings in [31], we also generate a Signature dataset, where\neach object is a string with 65 English letters. We ï¬rst obtain\n25 â€˜anchor signaturesâ€™ whose letters are randomly chosen\nfrom the alphabet. Then, each anchor produces a cluster\nwith 4,000 objects, each of which is obtained by randomly\n1. https://github.com/learned-index/LIMS\n2. https://image-net.org/download-images\n3. https://www.kaggle.com/c/forest-cover-type-prediction/data\n4. http://image- net.org/download- images\n\n10\n50 75 100\nK51015202530A verage query time (ms)GaussMix 8d Skewed 8d Forest Color Signature T ime Pages\n0 20 40 60 80 100 120 140 160\nK0.00.51.01.52.0Overhead\n(a) Estimated effect of K\n50 75 100 125 150\nK51015202530A verage query time (ms)\n03691215\n# Page Accesses (x100) (b) Effect of Kon range query\n23456\nm51015202530A verage query time (ms)\n03691215\n# Page Accesses (x100) (c) Effect of mon range query\n10 20 40 80 120\nN51015202530A verage query time (ms)\n03691215\n# Page Accesses (x100) (d) Effect of Non range query\nFig. 5: Effect of parameters\nchangingxpositions in the corresponding anchor signature\nto other random letters, where xis uniformly distributed\nin the range [1;30]. The edit distance is used to compute the\ndistance between two signatures. Table 2 summarizes the\nstatistics of the datasets.\nCompetitors. We compare LIMS with three representative\nmulti-dimensional learned indexes as mentioned in Section\n2,i.e., ZM [8], ML [11], LISA [9], and three traditional\nindexes, i.e.,R\u0003-tree [32], M-tree [33] and SPB-tree [34] [35].\nZM and ML are in-memory indexes, so we adapt them\nto the disk by storing data in ascending order of their\nz-order/mapped values in a number of pages with each\npage fully utilized. For LISA, no open-source C++ code\nis available, so we implement it following Python version\nimplementation. For R\u0003-tree, M-tree and SPB-tree, we use\nthe original implementations. In addition, to study the\neffectiveness of learning components in LIMS, we design\na method called non-learned index for metric spaces (N-\nLIMS) by replacing rank prediction models in LIMS with\nthe traditional B+-trees [36]. All competitors are conï¬gured\nto use a ï¬xed disk page size of 4KB.\nEvaluation Metrics. Four metrics are used to evaluate the\nperformance of indexes: the average number of page ac-\ncesses, the average query time, indexing time and index size.\nWe randomly select 200 objects from each dataset and repeat\neach experiment 20 times to get average results.\n6.2 Effect of Parameters\nWe ï¬rst study the effect of parameters, including the num-\nber of clusters K, the number of pivots mand the number of\nringsN, to optimize LIMS-based similarity search, as sum-\nmarized in Fig. 5. Only one parameter varies whereas the\nothers are ï¬xed to their default values in every experiment.\nBy default, the selectivity of range query, i.e.,the fraction\nof objects within the query range from the total number of\nobjects, is set to 0.01%. The kofkNN query is 5. Degrees of\nRP(i)\njandRP(i)are 20 and 1. mandNare 3 and 20. K\nis determined according to the method described in Section\n5.4. We set K= 100 for 10M 8dGaussMix and 8dSkewed ,\nK= 50 forColor Histogram ,Forest Cover Type and Signature .\n6.2.1 Effect of K\nFig. 5(a) plots the criterion OR+\u0015MAE versusKon both\nreal and synthetic data, where K= 20;30;:::; 150 and\n\u0015is set to 1=maxfMAE (K)g. We can see that different\ndatasets have different elbow points. In order to show the\nestimation is close to the actual optimal number of clusters,\nwe also plot the actual average query time and the numberof page accesses for range query by varying K. Due to the\nspace limitation, we only report the performance on 10M 8 d\nGaussMix dataset in Fig. 5(b). It can be observed that query\ntime decreases slowly after K= 100 , which is consistent\nwith the recommended choice of Kin Fig. 5(a). Therefore,\nwe setK= 100 for this dataset.\n6.2.2 Effect of m\nFig. 5(c) reports the query performance on 10M 8 dGaussMix\ndataset by varying the number of pivots m. We can see\nthat increasing the number of pivots always reduces (or at\nleast does not increase) the number of page accesses. This is\nexpected because the intersection of metric regions deï¬ned\nby more pivots is always smaller than (or at least equal to)\nthe intersection of metric regions deï¬ned by fewer pivots.\nAs discussed in Section 4.3, the more pivots, the stronger\npruning ability. This observation can also be proven using\nEquation (11). However, the average query time decreases\nwhen using up to four pivots, and then increases progres-\nsively. That is because the cost for ï¬ltering unqualiï¬ed\nobjects grows as well with more pivots. The best number\nof pivots for a metric index is a trade-off between ï¬lter cost\nand scanning cost. Therefore, the default value for mis set\nto 3 unless otherwise stated.\n6.2.3 Effect of N\nFig. 5(d) reports the query performance on 10M 8 dGaussMix\ndataset by varying the number of rings N. For the same\nreason as above, the average query time presents a down\nand up trend whereas the lowest value turns out at N= 20 .\n6.3 Range Query Performance\nIn this subsection, we study the performance of LIMS, ML,\nLISA, ZM, R\u0003-tree, M-tree and SPB-tree on range query from\ndifferent angles, as summarized in Fig. 6, 7 and 8.\n6.3.1 Performance with dimensionality\nThe ï¬rst set of experiments studies the average query time\nand the number of page accesses under different dimen-\nsionalities. Fig. 6(a)(b) and Fig. 6(c)(d) report the results\nonSkewed and GaussMix datasets, respectively. From the\nï¬gures, we have the following observations: 1) The average\nquery time of all methods increases with dimensionality,\nbut LIMS, ML and SPB-tree grow much slower than others,\nsuggesting that data clustering and pivot-based data trans-\nformation techniques are effective in alleviating the curse\nof dimensionality. The coordinate-based methods, i.e.,LISA,\nZM and R\u0003-tree, degrade rapidly with dimensionality and\neven do not work, hence we do not report their results\n\n11\n50 75 100\nK51015202530A verage query time (ms)LIMS ML LISA ZM R*-tree SPB-tree LIMS ML LISA ZM R*-tree SPB-tree\n2 4 8 12 16\nDimension2022242628210A verage query time (ms)\n(a) Query time on Skewed\n2 4 8 12 16\nDimension101102103104105# Page Accesses (b) # Page accesses on Skewed\n2 4 8 12 16\nDimension2022242628A verage query time (ms) (c) Query time on GaussMix\n2 4 8 12 16\nDimension101102103104# Page Accesses (d) # Page accesses on GaussMix\nFig. 6: Range query performance with dimensionality\n0.1 0.5 1 2 4\nSelectivity (%)0510152025303540A verage query time (ms)\n(a) Query time on Forest Cover Type\n0.1 0.5 1 2 4\nSelectivity (%)0510152025# Page Accesses (x100) (b) # Page accesses on Forest\n0.05 0.1 0.2 0.4 0.8\nSelectivity (%)100150200250300350A verage query time (ms) (c) Query time on Color\n0.05 0.1 0.2 0.4 0.8\nSelectivity (%)0100200300# Page Accesses (x100) (d) # Page accesses on Color\nFig. 7: Range query performance with selectivity\nafter 8d. 2) LIMS is slightly slower than LISA when 2dand\nwhen 4donGaussMix , but LIMS offers the best performance\nin a higher range of dimensions on both datasets. The\nreason is that metric-space indexes can only use the four\ndistance properties to prune the search space, while LISA\ncan easily locate the cells that overlap with the query range\nby coordinates. Fewer assumptions about the data result\nin poorer pruning and slightly larger query costs in the\nlow-dimensional case. However, the ï¬ltering cost of LISA\nincreases exponentially with dimensionality and it does not\nwork when the dimension is greater than 8. Note that on 8d\nGaussMix , even though LIMS has higher numbers of page\naccesses than LISA, it is still faster due to the low ï¬lter\ncost. On Skewed , LIMS begins to have a lead advantage\nsince 4dbecause LIMS can be applied to a metric space\nwith any distance metrics naturally ( e.g.,L1norm here) and\nguarantees a few false positives and fast query response.\n3) LIMS is always better than ML in both the query time\nand page accesses. This is intuitive since ML transforms\ndifferent objects with equi-distance from the pivot into\nthe same 1-dimensional value, while LIMS integrates the\npruning abilities from multiple pivots. In addition, with\nthe help of a well-deï¬ned pivots-based mapping, LIMS\nmaps nearby data into the compact region, which further\nreduces the search region, and thus fewer objects to be\naccessed in the reï¬nement step. 4) LIMS outperforms the\nlearned index, ZM, by over an order of magnitude. This is\nbecause LIMS uses clusters and LIMS values to organize\nobjects into compact regions, while ZM uses z-order curve\nto organize data, which incurs too many false positives, and\nthus large page accesses and distance computations. 5) LIMS\nis always better than traditional indexes. The main reason\nis that query processing with a traditional index requires\ntraversing many tree nodes multiple times, which is time-\nconsuming. M-tree is omitted since it is considerably worse\nthan others. 6) The performance of all indexes on GaussMix\nis better than that on Skewed due to the simple distribution.6.3.2 Performance with selectivity\nThe second set of experiments studies the average query\ntime and the number of page accesses with different se-\nlectivity. Fig. 7(a)(b) show the results on Forest dataset by\nvarying selectivity from 0.1% to 4%. Fig. 7(c)(d) show the\nresults on Color dataset by varying the selectivity from 0.05%\nto 0.8%.Non these small datasets is cut in half. From\nthe ï¬gures, we have the following observations: 1) both\nquery time and page accesses among indexes grow with the\nselectivity since more objects are queried. 2) LIMS achieves\nbetter performance under all selectivities by at least 1.7X and\nup to 4.4X faster. Consistent high efï¬ciency of LIMS shows\nthe robustness for range queries in varying settings. 3) On\nColor Histogram , the advantage in fewer page accesses shows\nthe potential advantages that LIMS can achieve over ML for\nprocessing similarity search with costly distance metrics.\n6.3.3 Performance in metric spaces\nThe third set of experiments studies the average query time\nand the number of page accesses in a metric space. We\ncompare LIMS with SPB-tree and M-tree. We also extend ML\nto the metric dataset by replacing the KMeans in ML with\nthe k-center algorithm. Other competitors are omitted since\nthey are not applicable for metric spaces. Fig. 8(a)(b) report\nthe results on Signature dataset. Clearly, LIMS has a decided\nadvantage over all competitors under all selectivities, where\nLIMS is around 20X faster than M-tree and the page accesses\nare at least 12X fewer. This is expected because of expensive\nand unavoidable costs to traverse the tree structure in tradi-\ntional index structures, and poor pruning powers in ML.\n6.4 kNN Query Performance\nIn this subsection, we study the performance of LIMS,\nML, LISA, R\u0003-tree, M-tree and SPB-tree on kNN query, as\nsummarized in Fig. 8, 9, and 10. ZM is excluded because it\ndoes not support kNN query.\n\n12\n50 75 100\nK51015202530A verage query time (ms)LIMS ML LISA R*-tree SPB-tree M-T ree LIMS ML LISA R*-tree SPB-tree M-T ree\n0.1 0.5 1 2 4\nSelectivity (%)242526272829A verage query time (ms)\n(a) Time of range queries\n0.1 0.5 1 2 4\nSelectivity (%)101102103# Page Accesses (b) # Page accesses of range queries\n1 5 25 50 100\nK23252729A verage query time (ms) (c) Time of kNN queries\n1 5 25 50 100\nSelectivity (%)101102103# Page Accesses (d) # Page accesses of kNN queries\nFig. 8: Range and kNN query performance on Signature\n2 4 8 12 16\nDimension2âˆ’22022242628A verage query time (ms)\n(a) Query time on Skewed\n2 4 8 12 16\nDimension101102103104# Page Accesses (b) # Page accesses on Skewed\n2 4 8 12 16\nDimension2âˆ’220222426A verage query time (ms) (c) Query time on GaussMix\n2 4 8 12 16\nDimension101102103104# Page Accesses (d) # Page accesses on GaussMix\nFig. 9: kNN query performance with dimensionality\n1 5 25 50 100\nk024681012A verage query time (ms)\n(a) Query time on Forest\n1 5 25 50 100\nk0.00.51.01.52.02.53.0# Page Accesses (x100) (b) # Page accesses on Forest\n1 5 25 50 100\nK153045607590A verage query time (ms) (c) Query time on Color\n1 5 25 50 100\nK050100150# Page Accesses (x100) (d) # Page accesses on Color\nFig. 10: kNN query performance with k\n6.4.1 Performance with dimensionality\nFig. 9(a)(b) and Fig. 9(c)(d) report the average query time\nand the number of page accesses on Skewed and Gauss-\nMix datasets, respectively. From the ï¬gures, we have the\nfollowing observations: 1) R\u0003-tree works best in low di-\nmensions, but it is less effective than LIMS with increased\ndimensionality. 2) The relative performance of LIMS and\nML onkNN queries is similar to their performance on\nrange queries, since both techniques follow a similar search\nregion expansion paradigm. 3) Even though LISA uses a\nmodel learned from the data distribution to estimate the\ninitial radius, it still suffers from too many unnecessary page\naccesses. The reason is that once the number of retrieved\nobjects is smaller than k, LISA will issue a range query\nwith a larger radius from the scratch, leading to the same\npage accessed repeatedly. When the estimated initial radius\nis large, it also incurs many page accesses.\n6.4.2 Performance with k\nFig. 10(a)(b) and Fig. 10(c)(d) show the performance when\nvarying the value of kinf1;5;25;50;100gonForest and\nColor datasets, respectively. LIMS is the fastest except for\n1NN on Forest , where LISA is slightly faster owing to a\nproper radius estimation. If setting \u0001rin LIMS to the\nvalue recommended by LISA. LIMS can achieve better\nperformance ( 0:11ms v.s.0:12ms). However, the trainingtime of model used to estimate \u0001rin LISA is long, which\nis not favorable for frequent insertion/deletion operations\nand query pattern changes. As kgrows, LISA becomes not\ncomparable due to the aforementioned reasons. The results\nof SPB-tree on Color is omitted because it is drastically\nslower. Consistent high efï¬ciency of LIMS indicates that it\nscales to large kvalues.\n6.4.3 Performance in metric spaces\nFig. 8(c)(d) present the performance on Signature dataset.\nAs expected, LIMS again yields the fastest query time and\nfewest page accesses. The curve becomes gentle after k= 5\nbecause many signatures share the same edit distance to a\ngiven signature.\n6.5 Indexing Time and Index Size\nIn this subsection, we report the indexing time and index\nsize. Due to the space limitation, we only report the results\non8d10M GaussMix and Signature . As Fig. 11(a) shown,\nLIMS does not suffer from long training time, a common\ndilemma faced by learned indexes. On 10M 8 dGaussMix ,\nLIMS is 15.5X faster than LISA ( 368sv.s.1:6h), which makes\nLIMS easy to rebuild and ensures simple update operations\neffective in practice. As described in Section 5.3, when\nthe query performance does not degrade severely, we can\npartially rebuild LIMS by retraining rank prediction models\n\n13\nGaussMix Signature\nDistribution101102103Indexing time (s)LIMS ML LISA ZM R*-tree SPB M-tree\nGaussMix Signature\nDistribution101102103Indexing time (s)\n(a) Indexing time\nGaussMix Signature\nDistribution101102103Index size (MB) (b) Index size\nFig. 11: Indexing time and index size\nfor some clusters. Retraining the index of a cluster only takes\nan average of 0:5s. LIMS has a longer construction time than\nZM because of expensive distance computations in metric\nspaces. However, LIMS has a decided advantage over ZM\nregardless of the dataset or dimension. Fig. 11(b) shows the\nindex size. The index size of LIMS is only 1/3 that of R\u0003-\ntree on GaussMix and 1/23 that of ML on Signature , respec-\ntively, since traditional indexes have to store a large number\nof internal nodes and ML have to store multiple stages\nof learned models. LIMS has a slightly larger index size\nthan other learned indexes on GaussMix because LIMS is a\nmetric-space index that needs to store many pre-computed\ndistances between pivots and data objects. However, it is\naccepted because a few extra distance values are relatively\ninsigniï¬cant compared to the complex and large data, such\nas images and audio. We expect the indexing time and index\nsize to be smaller when using fewer pivots.\n0K 5K 10K 20K 40K\nInsertions51015202530A verage query time (ms)\n0612182430\n# Page Accesses (x100)T ime Pages\n0K 5K 10K 20K 40K\nInsertions51015202530A verage query time (ms)\n03691215\n# Page Accesses (x100)\n(a)GaussMix\n0K 5K 10K 20K 40K\nInsertions51015202530A verage query time (ms)\n0612182430\n# Page Accesses (x100) (b)Skewed\nFig. 12: Range query performance after insertions\n6.6 Updates\nIn this subsection, we examine the impact of data updates.\nWithout loss of generality, we assume that the distribution\nof underlying data does not change greatly over time. Fig.\n12(a)(b) report the average query time and the number of\npage accesses for range queries after inserting 5K, 10K,\n20K and 40K objects into 10M 8 dGaussMix and Skewed\ndatasets, respectively. As expected, insertions cause query\ntime increase since there are more points to query, and\nthe index becomes less optimal. However, the rate of per-\nformance degradation is slow and steady. For example,\nonSkewed , after 40K insertions, LIMS still achieves 3.4X\nspeedup compared to the second best index ( 28:05msv.s.\n94:68ms). Retraining can be triggered when the query per-\nformance deteriorates beyond a user-speciï¬ed threshold. We\nalso studied the impact of deletion operations, but omit\nthose due to negligible inï¬‚uence on query performance and\nspace constraints.6.7 Ablation Study\nFinally, we make a comparison of LIMS and N-LIMS to\nshow the effectiveness of learning components in LIMS.\nSince the only difference between two methods is whether to\nuse B+-trees or the rank prediction models and exponential\nsearch to locate the start and end of a range query, both\nhave the same number of page accesses (I/O cost). Fig. 13(a)\nreports the average CPU time of range query by varying the\ncardinality of 8 dGaussMix dataset. From the ï¬gure, we have\nthe following observations: 1) LIMS outperforms N-LIMS on\nall data sizes. The reason is that rank prediction models in\nLIMS achieve a good grasp of the data distribution so that\na query can be processed by a function invocation in O(1)\ntime instead of traversing B+-trees inO(logn)time, which\nimplies the effectiveness of the machine learning model.\n2) The advantage of LIMS becomes more obvious when\nthe data size increases. This is because the query cost of\nLIMS does not directly depend on the cardinality, which\nallows LIMS scalable to very large data sets. To present\nthe whole picture for a fair comparison, we also report the\naverage query time (CPU time + I/O time) of LIMS and N-\nLIMS in Fig. 13(b). As expected, LIMS again offers the best\nquery performance. Furthermore, N-LIMS is still faster than\nthe second best competitor ( 10:45msv.s.12:21ms), which\nfurther conï¬rms the superiority of LIMS index structure.\n0 10 20 30 40 50 60 70 80 90\nCardinality036912151821A verage CPU time (ms)LIMS N-LIMS\n0 10 20 30 40 50 60 70 80 90\nCardinality036912151821A verage CPU time (ms)\n(a) CPU time\n0 10 20 30 40 50 60 70 80 90\nCardinality010203040506070A verage query time (ms) (b) Query time\nFig. 13: Comparison of LIMS and N-LIMS\n7 C ONCLUSIONS\nAs a universal abstraction of various data types, metric\nspaces and associated similarity search play an important\nrole in many real-life applications. In this paper, we have de-\nveloped LIMS, a novel learned index structure, for efï¬cient\nexact similarity search query processing in metric spaces.\nLIMS takes advantage of compact-partitioning methods,\npivot-based techniques and the idea of learned index to\norganize and access multi-dimensional data. Our extensive\nexperimental results illustrate that LIMS can respond sig-\nniï¬cantly better to the problem of â€˜curse of dimensionalityâ€™\ncompared with other learned index structures. It also has a\nclear advantage over traditional indexes. In the future, we\nplan to take this work further by considering query work-\nload information such that workload-aware optimization\ncan be made.\nACKNOWLEDGMENTS\nThis research is partially supported by Natural Science\nFoundation of China (Grant# 62072125) and is conducted\nin the JC STEM Lab of Data Science Foundations funded by\nThe Hong Kong Jockey Club Charities Trust.\n\n14\nREFERENCES\n[1] Y. Rubner, C. Tomasi, and L. J. Guibas, â€œA metric for distributions\nwith applications to image databases,â€ in ICCV , pp. 59â€“66, 1998.\n[2] E. Ch Â´avez, G. Navarro, R. A. Baeza-Yates, and J. L. Marroqu Â´Ä±n,\nâ€œSearching in metric spaces,â€ ACM Comput. Surv. , vol. 33, no. 3,\npp. 273â€“321, 2001.\n[3] M. L. Hetland, â€œThe basic principles of metric indexing,â€ in Swarm\nintelligence for multi-objective problems in data mining , pp. 199â€“232,\n2009.\n[4] D. Rachkovskij, â€œDistance-based index structures for fast similar-\nity search,â€ Cybernetics and Systems Analysis , vol. 53, no. 4, pp. 636â€“\n658, 2017.\n[5] P . Zezula, G. Amato, V . Dohnal, and M. Batko, Similarity Search -\nThe Metric Space Approach , vol. 32 of Advances in Database Systems .\nKluwer, 2006.\n[6] L. Chen, Y. Gao, X. Song, Z. Li, X. Miao, and C. S. Jensen,\nâ€œIndexing metric spaces for exact similarity search,â€ CoRR ,\nvol. abs/2005.03468, 2020.\n[7] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, â€œThe\ncase for learned index structures,â€ in SIGMOD , pp. 489â€“504, 2018.\n[8] H. Wang, X. Fu, J. Xu, and H. Lu, â€œLearned index for spatial\nqueries,â€ in MDM , pp. 569â€“574, 2019.\n[9] P . Li, H. Lu, Q. Zheng, L. Yang, and G. Pan, â€œLISA: A learned\nindex structure for spatial data,â€ in SIGMOD , pp. 2119â€“2133, 2020.\n[10] J. Qi, G. Liu, C. S. Jensen, and L. Kulik, â€œEffectively learning spatial\nindices,â€ PVLDB , vol. 13, no. 11, pp. 2341â€“2354, 2020.\n[11] A. Davitkova, E. Milchevski, and S. Michel, â€œThe ml-index: A\nmultidimensional, learned index for point, range, and nearest-\nneighbor queries,â€ in EDBT , pp. 407â€“410, 2020.\n[12] T. Kraska, M. Alizadeh, A. Beutel, E. H. Chi, A. Kristo, G. Leclerc,\nS. Madden, H. Mao, and V . Nathan, â€œSagedb: A learned database\nsystem,â€ in CIDR , 2019.\n[13] V . Nathan, J. Ding, M. Alizadeh, and T. Kraska, â€œLearning multi-\ndimensional indexes,â€ in SIGMOD , pp. 985â€“1000, 2020.\n[14] J. Ding, V . Nathan, M. Alizadeh, and T. Kraska, â€œTsunami: A\nlearned multi-dimensional index for correlated data and skewed\nworkloads,â€ PVLDB , vol. 14, no. 2, pp. 74â€“86, 2020.\n[15] Z. Yang, B. Chandramouli, C. Wang, J. Gehrke, Y. Li, U. F. Minhas,\nP . Larson, D. Kossmann, and R. Acharya, â€œQd-tree: Learning data\nlayouts for big data analytics,â€ in SIGMOD , pp. 193â€“208, 2020.\n[16] J. A. Orenstein and T. H. Merrett, â€œA class of data structures for\nassociative searching,â€ in PODS , pp. 181â€“190, 1984.\n[17] H. V . Jagadish, B. C. Ooi, K. Tan, C. Yu, and R. Zhang, â€œiDistance:\nAn adaptive B+-tree based indexing method for nearest neighbor\nsearch,â€ TODS , vol. 30, no. 2, pp. 364â€“397, 2005.\n[18] R. Xu and D. C. W. II, â€œSurvey of clustering algorithms,â€ IEEE\nTrans. Neural Networks , vol. 16, no. 3, pp. 645â€“678, 2005.\n[19] C. T. Jr., R. F. S. Filho, A. J. M. Traina, M. R. Vieira, and C. Faloutsos,\nâ€œThe omni-family of all-purpose access methods: a simple and\neffective way to make similarity search more efï¬cient,â€ VLDBJ ,\nvol. 16, no. 4, pp. 483â€“505, 2007.\n[20] T. Gu, K. Feng, G. Cong, C. Long, Z. Wang, and S. Wang, â€œThe rlr-\ntree: A reinforcement learning based r-tree for spatial data,â€ CoRR ,\nvol. abs/2103.04541, 2021.\n[21] A. Feinberg, â€œMarkov decision processes: Discrete stochastic dy-\nnamic programming (martin l. puterman),â€ SIAM , vol. 38, no. 4,\np. 689, 1996.\n[22] G. Navarro, â€œA guided tour to approximate string matching,â€\nACM Comput. Surv. , vol. 33, no. 1, pp. 31â€“88, 2001.\n[23] B. SCHROEDER, Ordered Sets: An Introduction with Connections\nfrom Combinatorics to Topology . BIRKHAUSER, 2018.\n[24] K. Chakrabarti and S. Mehrotra, â€œLocal dimensionality reduction:\nA new approach to indexing high dimensional spaces,â€ in VLDB ,\npp. 89â€“100, 2000.\n[25] D. S. Hochbaum and D. B. Shmoys, â€œA best possible heuristic for\nthek-center problem,â€ Math. Oper. Res. , vol. 10, no. 2, pp. 180â€“184,\n1985.\n[26] E. Ch Â´avez, G. Navarro, R. A. Baeza-Yates, and J. L. Marroqu Â´Ä±n,\nâ€œSearching in metric spaces,â€ ACM Comput. Surv. , vol. 33, no. 3,\npp. 273â€“321, 2001.\n[27] B. Pagel, F. Korn, and C. Faloutsos, â€œDeï¬‚ating the dimensionality\ncurse using multiple fractal dimensions,â€ in ICDE , pp. 589â€“598,\n2000.\n[28] K. L. Clarkson et al. , â€œNearest-neighbor searching and metric space\ndimensions,â€ Nearest-neighbor methods for learning and vision: theory\nand practice , pp. 15â€“59, 2006.[29] Y. Zhu, L. Chen, Y. Gao, and C. S. Jensen, â€œPivot selection algo-\nrithms in metric spaces: a survey and experimental study,â€ VLDBJ ,\nvol. 31, no. 1, pp. 23â€“47, 2022.\n[30] R. L. Thorndike, â€œWho belongs in the family?,â€ Psychometrika ,\nvol. 18, no. 4, pp. 267â€“276, 1953.\n[31] Y. Tao, M. L. Yiu, and N. Mamoulis, â€œReverse nearest neighbor\nsearch in metric spaces,â€ TKDE , vol. 18, no. 9, pp. 1239â€“1252, 2006.\n[32] N. Beckmann, H. Kriegel, R. Schneider, and B. Seeger, â€œThe R*-tree:\nAn efï¬cient and robust access method for points and rectangles,â€\ninSIGMOD , pp. 322â€“331, 1990.\n[33] P . Ciaccia, M. Patella, and P . Zezula, â€œM-tree: An efï¬cient access\nmethod for similarity search in metric spaces,â€ in VLDB , pp. 426â€“\n435, 1997.\n[34] L. Chen, Y. Gao, X. Li, C. S. Jensen, and G. Chen, â€œEfï¬cient\nmetric indexing for similarity search,â€ in ICDE , pp. 591â€“602, IEEE\nComputer Society, 2015.\n[35] L. Chen, Y. Gao, X. Li, C. S. Jensen, and G. Chen, â€œEfï¬cient metric\nindexing for similarity search and similarity joins,â€ IEEE Trans.\nKnowl. Data Eng. , vol. 29, no. 3, pp. 556â€“571, 2017.\n[36] T. Bingmann, â€œTLX: Collection of sophisticated C++ data\nstructures, algorithms, and miscellaneous helpers,â€ 2018.\nhttps://panthema.net/tlx, retrieved Oct. 7, 2020.\nYao Tian is currently a PhD student with the\nDepartment of Computer Science and Engineer-\ning, Hong Kong University of Science and Tech-\nnology (HKUST), supervised by Prof. Xiaofang\nZhou. She received her MSc degree in School\nof Mathematical Science from Zhejiang Univer-\nsity in 2020. Her research interests include the\nlearned index and approximate query process-\ning in high dimensional spaces.\nTingyun Yan is currently a postgraduate student\nin Cyberspace Institute of Advanced Technology,\nGuangzhou University, supervised by Prof. Xiao-\nfang Zhou. He received the Bachelor degree in\nSoftware College from Northeastern University\nin 2020. His research interests include spatial\nindex and learned index.\nXi Zhao is currently a research assistant at\nHKUST, under the supervision of Prof. Xiaofang\nZhou. He received the Master degree in Com-\nputer Science from Huazhong University of Sci-\nence and Technology, China, in 2021. His re-\nsearch interests include the approximate query\nprocessing in high dimensional spaces, exact\ntrajectory similarity search and exact textual sim-\nilarity search.\nKai Huang is a Postdoc at the Department\nof Computer Science and Engineering, HKUST,\nunder the supervision of Prof. Xiaofang Zhou. He\nreceived his PhD degree in School of Computer\nScience from Fudan University in 2020, and\nBEng degree in Software Engineering from East\nChina Normal University in 2014. His research\ninterests include graph database and privacy-\naware data management.\nXiaofang Zhou is Otto Poon Professor of En-\ngineering and Chair Professor of Computer Sci-\nence and Engineering at the Hong Kong Univer-\nsity of Science and Technology. He received his\nBachelor and Master degrees in Computer Sci-\nence from Nanjing University, in 1984 and 1987\nrespectively, and his PhD degree in Computer\nScience from the University of Queensland in\n1994. His research is focused on ï¬nding effec-\ntive and efï¬cient solutions for managing, inte-\ngrating, and analysing very large amount of com-\nplex data for business, scientiï¬c and personal applications. His research\ninterests include spatial and multimedia databases, high performance\nquery processing, web information systems, data mining, data quality\nmanagement, and machine learning. He is a Fellow of IEEE.",
  "textLength": 76895
}