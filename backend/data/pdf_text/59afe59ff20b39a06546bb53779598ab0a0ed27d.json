{
  "paperId": "59afe59ff20b39a06546bb53779598ab0a0ed27d",
  "title": "Double Coverage with Machine-Learned Advice",
  "pdfPath": "59afe59ff20b39a06546bb53779598ab0a0ed27d.pdf",
  "text": "Double Coverage with Machine-Learned Advice\nAlexander Lindermayrâˆ—Nicole Megowâˆ—Bertrand Simonâ€ \nAbstract\nWe study the fundamental online ğ‘˜-server problem in a learning-augmented setting. While in the\ntraditional online model, an algorithm has no information about the request sequence, we assume that\nthere is given some advice (e.g. machine-learned predictions) on an algorithmâ€™s decision. There is,\nhowever, no guarantee on the quality of the prediction and it might be far from being correct.\nOur main result is a learning-augmented variation of the well-known Double Coverage algorithm\nforğ‘˜-server on the line (Chrobak et al., SIDMA 1991) in which we integrate predictions as well as\nour trust into their quality. We give an error-dependent competitive ratio, which is a function of a\nuser-dened condence parameter, and which interpolates smoothly between an optimal consistency,\nthe performance in case that all predictions are correct, and the best-possible robustness regardless\nof the prediction quality. When given good predictions, we improve upon known lower bounds for\nonline algorithms without advice. We further show that our algorithm achieves for any ğ‘˜an almost\noptimal consistency-robustness tradeo, within a class of deterministic algorithms respecting local and\nmemoryless properties.\nOur algorithm outperforms a previously proposed (more general) learning-augmented algorithm. It is\nremarkable that the previous algorithm crucially exploits memory, whereas our algorithm is memoryless .\nFinally, we demonstrate in experiments the practicability and the superior performance of our algorithm\non real-world data.\n1 Introduction\nTheğ‘˜-server problem is one of the most fundamental online optimization problems. Manasse et al. [39, 40]\nintroduced it in 1988 as a generalization of other online problems, such as the prominent paging problem,\nand since then, it has been a corner stone for developing new models and techniques. We follow this line\nand investigate the ğ‘˜-server problem in the recently evolving framework of learning-augmented online\ncomputation.\nWe consider the ğ‘˜-server problem on the line , in which there are given ğ‘˜distinct servers ğ‘ 1Â•Â”Â”Â”Â•ğ‘ ğ‘˜located\nat initial positions on the real line. A sequence of requests ğ‘Ÿ1Â•Â”Â”Â”Â•ğ‘Ÿğ‘›2Ris revealed online one-by-one, that\nis, an algorithm only knows the current (unserved) request, serves it and only then sees the next request; it\nhas no knowledge about future requests. To serve a request, (at least) one of the servers has to be moved to\nthe requested point. The cost of serving a request is dened as the distance traveled by the server(s). The\ntask is to give an online strategy of minimum total cost for serving a request sequence.\nIn standard competitive analysis, an online algorithm Ais calledğœ‡-competitive if for every instance ğ¼,\nthere is some constant ğ‘depending only on the initial conguration such that AÂ¹ğ¼Âº\u0014ğœ‡\u0001O/p.sc/t.scÂ¹ğ¼ÂºÂ¸ğ‘,\nâˆ—Faculty of Mathematics and Computer Science, University of Bremen, Germany. {linderal,nmegow}@uni-bremen.de\nâ€ IN2P3 Computing Center, CNRS, Villeurbanne, France. bertrand.simon@cc.in2p3.frarXiv:2103.01640v2  [cs.LG]  16 Nov 2021\n\nwhereAÂ¹ğ¼Âºdenotes the cost of Aonğ¼whereas O/p.sc/t.scÂ¹ğ¼Âºis the cost of an optimal solution that can be\nobtained when having full information about ğ¼in advance.\nManasse et al. [40] gave a strong lower bound which rules out any deterministic online algorithm with\na competitive ratio better than ğ‘˜. They also stated the famous k-server conjecture in which they conjecture\nthat there is a ğ‘˜-competitive online algorithm for the ğ‘˜-server problem in any metric space and for any ğ‘˜.\nThe conjecture has been proven to be true for special metric spaces such as the line [17], considered in this\npaper, the uniform metric space (paging problem) [47] and tree metrics [18]. For the ğ‘˜-server problem on\nthe line, Chrobak et al. [17] devised the D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc algorithm and proved a best possible competitive\nratioğ‘˜. For a given request, D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc moves the (at most) two adjacent servers towards the\nrequested point until the rst of them reaches that point.\nThe past decades have witnessed a rapid advancement of machine learning (ML) methods, which\nnowadays can be expected to predict oftenâ€”but not alwaysâ€”uncertain data with good accuracy. The lack\nof guarantees on the predictions and the need for trustable performance guarantees lead to the area of\nlearning-augmented online algorithms . This recently emerging research area investigates online algorithms\nthat have access to predictions, e.g., on parts of the instance or the algorithmâ€™s execution, while not making\nany assumption on the quality of the predictions. Formally, we assume that a prediction has a certain\nqualityğœ‚\u00150. In the context of learning theory one may think of the lossof a prediction with respect to the\nground truth. Accordingly, ğœ‚=0refers loosely speaking to the case where the prediction was correct. In the\neld of learning-augmented algorithm this quantity is called prediction error . An algorithm does not know\nwhat quality a prediction has, but we can use it in the analysis to measure an algorithmâ€™s performance\ndepending on ğœ‚. If a learning-augmented algorithm is ğœ‡Â¹ğœ‚Âº-competitive for some function ğœ‡, we say that the\nalgorithm is ğ›¼-consistent ifğ›¼=ğœ‡Â¹0Âºandğ›½-robust ifğœ‡Â¹ğœ‚Âº\u0014ğ›½for any prediction with prediction error ğœ‚[44].\nVery recently, Antoniadis et al. [3] proposed learning-augmented online algorithms for general metrical\ntask systems, a generalization of our problem. Their algorithm relies on simulating several online algorithms\nin parallel and keeping track of their solutions and cost. This technique crucially employs additional memory\nwhich can be a serious drawback in practice when decisions must be made without access to the history.\nIn this work, we introduce memory-constrained learning-augmented algorithms for the ğ‘˜-server\nproblem on the line. An algorithm Ais intuitively memory-constrained , if the decision for the next move\nofAonly depends on the current situation (server positions, request and prediction). It is especially\nindependent of previous requests. However, as the algorithm is allowed to move a server to any point of\nthe real line, it could use its position to encode any information at a negligible cost. This issue is often\naddressed by forbidding algorithms to move several servers per request (hence, restricting to so-called lazy\nalgorithms) which leads to the classical memoryless property, although variations of this denition exist [28].\nA downside of this restriction is that deterministic memoryless algorithms cannot be competitive, and\nthere is no distinction between the type of information gathered by D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc and unconstrained\ninformation encoding. This dierence has been nevertheless acknowledged by informally considering\nD/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc as memoryless [26], although noting immediately that such a denition for a non-lazy\nalgorithm is cumbersome. In order to allow the behavior of D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc , we formally dene memory-\nconstrained algorithms as algorithms allowed to move several servers, making decisions independently\nof previous requests, but with an erasable memory: for any set of ğ‘˜distinct points and any starting\nconguration, there exists a nite sequence of requests among these ğ‘˜points after which each point\ncontains exactly one server. We will refer to such a sequence as a force to theseğ‘˜points. This denition is\nquite general as it allows to pre-move some servers as D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc does, and even allows information\nencoding, but provides a possibility to erase any information gathered. The algorithms we design will not\nabuse information encoding, but our lower bounds will hold in this context.\n2\n\nFurther related work The past few years have exhibited several demonstrations of the power of learning-\naugmented algorithms improving on traditional online algorithms. Studied online problems include\ncaching [3, 37, 45, 51], paging [24], ski rental [9, 20, 44, 50, 52], TCP acknowledgement [8, 9], bin packing [2],\nscheduling [6,7,22,31,43,44,52], secretary problems [5,19], linear search [1], matching [30,32], sorting [36],\nonline covering problems [8], and possibly more by now. Learning-augmented algorithms have proven\nto be successful also in other areas, e.g., to speed up search queries [29], in revenue optimization [41], to\ncompute low rank approximations [23], frequency estimation [21] and bloom lters [42].\nMore than a decade ago, Mahdian et al. [38] demonstrated performance improvements for online\nallocation algorithms when there is access to an accurate solution estimation. They further bounded the\ncase where the estimation is inaccurate. While these bounds essentially correspond to consistency and\nrobustness, they did not precisely measure the prediction quality. Yet they introduced a parameter to\nexpress the tradeo between both bounds. In the recent eld of learning-augmented algorithms, Kumar et\nal. [44] initiated the use of a similar parameter ğœ†2Â»0Â•1Â¼. It can be interpreted as an algorithmâ€™s indicator\nof trust in the given predictions: smaller ğœ†indicates stronger trust and gives a higher priority to a better\nconsistency at the cost of a worse robustness, and vice versa. Such parameterized consistency-robustness\ntradeo has become standard for expressing the performance of learning-augmented algorithms when\naiming for constant factors [2, 5, 8, 9, 22, 44, 50, 52].\nAs mentioned, Antoniadis et al. [3] provide a general learning-augmented framework for any metrical\ntask systems which includes the ğ‘˜-server problem. Applied to the line metric, they devise a learning-\naugmented algorithm that crucially requires memory and obtains a 9-consistent and 9ğ‘˜-robust algorithm.\nTheğ‘˜-server problem has been studied also in the context of reinforcement learning (RL), originating\nat [25] and including hierarchical RL learning [33] as well as deep RL learning [35].\nThe classical online ğ‘˜-server problem without access to predictions has been studied extensively, also\nin general metric spaces. The best known deterministic algorithm is the W/o.sc/r.sc/k.scF/u.sc/n.sc/c.sc/t.sc/i.sc/o.sc/n.sc algorithm [27]\nwith a competitive ratio of 2ğ‘˜\u00001. For several special metric spaces there are even tighter bounds known\nfor this algorithm [11, 53]. When allowing randomization, a Î©Â¹logğ‘˜Âlog logğ‘˜Âºlower bound holds [10] and\naÂ¹logğ‘˜ÂºOÂ¹1Âº-competitive randomized algorithm is conjectured [26]. Restricting further to memoryless\nrandomized algorithms increases the lower bound on the competitive ratio exponentially to ğ‘˜[26] and\nsome recent eorts focus on a more general variant in this setting [15].\nThe power of D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc goes beyond its optimality for the ğ‘˜-server problem in tree metrics [18].\nRecently, Buchbinder et al. [14] showed that it is a best possible deterministic algorithm for the more\ngeneralğ‘˜-taxi problem, even in general metric spaces using an embedding into hierarchically separated trees.\nOur contribution We design learning-augmented memory-constrained online algorithms for the ğ‘˜-\nserver problem on the line. Firstly, we dene some more notation and the precise prediction model. We\ndenote a serverâ€™s name as well as its position on the line by ğ‘ ğ‘–, forğ‘–2f1Â•2Â”Â”Â”Â•ğ‘˜g. A conguration ğ¶ğ‘¡=\nÂ¹ğ‘ 1Â•Â”Â”Â”Â•ğ‘ ğ‘˜Âº2Rğ‘˜is a snapshot of the server positions at a certain point in time. For a given instance, a\nğ‘˜-server algorithm outputs a sequence of congurations ğ¶1Â•Â”Â”Â”Â•ğ¶ğ‘›(also called schedule ) such that for\neveryğ‘¡=1Â•Â”Â”Â”Â•ğ‘› , we haveğ‘Ÿğ‘¡2ğ¶ğ‘¡. We denote the initial conguration by ğ¶0. The objective function can\nbe expressed asÃğ‘›\nğ‘¡=1ğ‘‘Â¹ğ¶ğ‘¡\u00001Â•ğ¶ğ‘¡Âº, whereğ‘‘Â¹ğ¶ğ‘¡\u00001Â•ğ¶ğ‘¡Âºdenotes the cost for moving the servers from ğ¶ğ‘¡\u00001toğ¶ğ‘¡.\nWe assume w.l.o.g. ğ‘ 1\u0014Â”Â”Â”\u0014ğ‘ ğ‘˜, as server overtakings can be uncrossed without increasing the total cost.\nWe employ a prediction model that predicts algorithmic choices of an optimal algorithm, that is\npredicting which server should serve a certain request. Given an instance ğ¼composed of the request\nsequenceğ‘Ÿ1Â•Â”Â”Â”Â•ğ‘Ÿğ‘›, we dene a prediction forğ¼as a sequence of indices ğ‘1Â•Â”Â”Â”Â•ğ‘ğ‘›from the setf1Â•Â”Â”Â”Â•ğ‘˜g.\nIfğ‘ 1Â•Â”Â”Â”Â•ğ‘ ğ‘˜are the servers of some learning-augmented algorithm, we call ğ‘ ğ‘ğ‘¡thepredicted server for the\n3\n\nğ‘¡-th request. We call the algorithm that simply follows the predictions F/t.scP, that is, it serves each request by\nthe predicted server (to simplify computations, we still remove overtakings as mentioned above, which\nis equivalent to relabel servers by their position order). We denote its cost by F/t.scPÂ¹ğ¼Âº. We dene the\nprediction error ğœ‚=F/t.scPÂ¹ğ¼Âº\u0000O/p.sc/t.scÂ¹ğ¼Âºas quality measure for our predictions. Note that this error denition\nis independent of our algorithm.\nOur main result is a parameterized algorithm for the ğ‘˜-server problem on the line with an error-\ndependent performance guarantee thatâ€”when having access to good-quality predictionsâ€”beats the known\nlower bound for deterministic online algorithms.\nTheorem 1. Letğœ†2Â»0Â•1Â¼. We deneğ›½Â¹ğ‘˜Âº=Ãğ‘˜\u00001\nğ‘–=0ğœ†\u0000ğ‘–, forğœ†Â¡0, andğ›½Â¹ğ‘˜Âº=1, forğœ†=0. Further, let\nğ›¼Â¹ğ‘˜Âº=(\n1Â¸2ğœ†Â¸2ğœ†2Â¸Â”Â”Â”Â¸2ğœ†Â¹ğ‘˜\u00001ÂºÂ2ifğ‘˜is odd\n1Â¸2ğœ†Â¸2ğœ†2Â¸Â”Â”Â”Â¸2ğœ†ğ‘˜Â2\u00001Â¸ğœ†ğ‘˜Â2ifğ‘˜is evenÂ”\nLetğœ‚denote the total prediction error and O/p.sc/t.sc the cost of an optimal solution. Then, there exists a learning-\naugmented memory-constrained online algorithm for the ğ‘˜-server problem on the line with a competitive ratio\nof at most\nminn\nğ›¼Â¹ğ‘˜Âº\u0010\n1Â¸ğœ‚\nO/p.sc/t.sc\u0011\nÂ•ğ›½Â¹ğ‘˜Âºo\nÂ”\nIn particular, the algorithm is ğ›¼Â¹ğ‘˜Âº-consistent and ğ›½Â¹ğ‘˜Âº-robust, forğœ†Â¡0.\nInterpreting both bounds as functions of ğœ†2Â»0Â•1Â¼illustrates that ğ›¼Â¹ğ‘˜Âºinterpolates monotonously\nbetween 1andğ‘˜whileğ›½Â¹ğ‘˜Âºgrows from ğ‘˜asğœ†decreases. This matches our expectation on a learning-\naugmented online algorithm, as it improves in consistency but loses in robustness compared to the best\npossible online algorithm. From another perspective, for a xed value of ğœ†,ğ›¼Â¹ğ‘˜Âºis bounded by a constant\n(equal to 1Â¸2\n1\u0000ğœ†Âºwhich highlights the algorithm consistency but this comes at the price of an exponential\ndependency on ğ‘˜forğ›½Â¹ğ‘˜Âº.\nTo show this result, we design an algorithm that carefully balances between (i) the wish to simply follow\nthe predictions ( F/t.scP) which is obviously optimal if the predictions are correct, i.e. is 1-consistent, and (ii) the\nbest possible online algorithm when not having access to (good) predictions D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc [17], which\nisğ‘˜-robust. An additional challenge is to preserve the memory-constrained property. We achieve this, by\ngeneralizing the classical D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc [17] in an intuitive way. Essentially, our algorithm L/a.sc/m.sc/b.sc/d.sc/a.scDC\nincludes the information about predicted servers and our trust into them by varying server speeds.\nThe analysis of our algorithm is tight. On the technical side, our analysis builds on the powerful potential\nfunction method , as does the analysis of the classical D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc [17]. While L/a.sc/m.sc/b.sc/d.sc/a.scDC is quite\nsimple (a precise denition follows), the analysis is much more intricate and requires a careful re-design for\nthe learning-augmented setting. Our main technical contribution is the denition and analysis of dierent\nparameterized potential functions for proving robustness and consistency, that capture the dierent speeds\nfor moving servers and the accordingly more dicult tracing of the server moves.\nWe remark that our performance bound also holds (with an additional factor of 2on the error) using\nthe error measure of Antoniadis et al. [3] for our problem [34]. Their error denition sums up the distances\nbetween the congurations of O/p.sc/t.sc andF/t.scP after every request, thus, it may seem more intuitive as\nserver positions are compared instead of solution costs. However, our error denition allows to establish\nlearnability results and also simplies some analyses.\nWhile our result is tailored to the ğ‘˜-server problem, the framework by Antoniadis et al. [3] is designed\nfor more general metrical task systems. Interestingly, one of their methods is a deterministic combination of\nD/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc andF/t.scP, we refer to it as F/t.scP&DC. It is shown that F/t.scP&DCis9-consistent and 9ğ‘˜-robust.\n4\n\nOur methods dier substantially. While F/t.scP&DCcarefully tracks states and costs of the simulated individual\nalgorithms, L/a.sc/m.sc/b.sc/d.sc/a.scDC is a simple algorithm that only requires knowledge of the current conguration.\nFurther, L/a.sc/m.sc/b.sc/d.sc/a.scDC has a better performance for ğ‘˜ÂŸ20and an appropriate parameter ğœ†(e.g.,ğ‘˜=19and\nğœ†=0Â”83), but does not oer such a good tradeo for larger ğ‘˜. Actually, this is unavoidable for a certain\nclass of memory-constrained algorithms, that includes L/a.sc/m.sc/b.sc/d.sc/a.scDC .\nIndeed, we complement our main result with an almost matching lower bound on the consistency-\nrobustness tradeo. We construct a non-trivial bound for the class of memory-constrained algorithms\nthat satisfy an additional locality property; its precise denition is formulated in Section 5. Intuitively, the\nlocality property enforces an algorithm to achieve a better competitive ratio for a subinstance served by\nfewer servers. Other locality restrictions have been required before to establish lower bounds, e.g., for\nmatching on the line, see [4].\nTheorem 2. Letğœ†2Â¹0Â•1Â¼,ğœŒÂ¹ğ‘˜Âº=Ãğ‘˜\u00001\nğ‘–=0ğœ†ğ‘–andğ›½Â¹ğ‘˜Âº=Ãğ‘˜\u00001\nğ‘–=0ğœ†\u0000ğ‘–. LetAbe a learning-augmented locally-\nconsistent and memory-constrained deterministic online algorithm for the ğ‘˜-server problem on the line. Then,\nifAisğœŒÂ¹ğ‘˜Âº-consistent, it is at least ğ›½Â¹ğ‘˜Âº-robust.\nAlgebraic transformations (see Lemma 20) show that ğ›¼Â¹ğ‘˜ÂºÂŸ2ğœŒÂ¹ğ‘˜Âº, which implies that L/a.sc/m.sc/b.sc/d.sc/a.scDC\nachieves a tradeo within a factor of at most 2of the optimal consistency-robustness tradeo (among\nlocally-consistent and memory-constrained algorithms). For ğ‘˜=2,L/a.sc/m.sc/b.sc/d.sc/a.scDC achieves the optimal tradeo\n(among memory-constrained algorithms).\nWe demonstrate the power of our approach in empirical experiments on real-world data. We show that\nfor a reasonable choice of ğœ†our method outperforms the classical online algorithm D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc as\nwell as the algorithm in [3] for nearly all prediction errors.\nFinally, we address the learnability of our predictions, even though this is not the focus of our work.\nWe show that a static prediction sequence is PAC-learnable [48, 49]. We show a bound on the sample\ncomplexity that is polynomial in the number of requests, ğ‘›, and the number of servers, ğ‘˜, and we give a\nlearning algorithm with a polynomial running time in ğ‘›Â•ğ‘˜and the number of samples.\n2 Algorithm and Roadmap for the Analysis\nThe Algorithm L/a.sc/m.sc/b.sc/d.sc/a.scDC We generalize the classical D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc [17] by including the infor-\nmation about predicted servers as well as our trust into this advice, in an intuitive way. If a request ğ‘Ÿğ‘¡\nappears between two servers, the one closer to the predicted server ğ‘ğ‘¡moves by a greater distance towards\nthe requestâ€”as if it traveled at a higher speed.\nFormally, we dene L/a.sc/m.sc/b.sc/d.sc/a.scDC for a givenğœ†2Â»0Â•1Â¼as follows. If ğ‘Ÿğ‘¡ÂŸğ‘ 1orğ‘Ÿğ‘¡Â¡ğ‘ ğ‘˜, then L/a.sc/m.sc/b.sc/d.sc/a.scDC\nonly moves the closest server. Otherwise, we have ğ‘ ğ‘–ÂŸğ‘Ÿğ‘¡ÂŸğ‘ ğ‘–Â¸1. Ifğ‘ğ‘¡\u0014ğ‘–, then L/a.sc/m.sc/b.sc/d.sc/a.scDC movesğ‘ ğ‘–with\nspeed 1 and ğ‘ ğ‘–Â¸1with speedğœ†towardsğ‘Ÿğ‘¡until one server reaches the request. If ğ‘ğ‘¡\u0015ğ‘–Â¸1, the speeds of ğ‘ ğ‘–\nandğ‘ ğ‘–Â¸1are swapped. Hence, L/a.sc/m.sc/b.sc/d.sc/a.scDC equals F/t.scP(with shortcuts) for ğœ†=0, and D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc\nforğœ†=1. Using nonintegral values for ğœ†gives an algorithm that interpolates between both.\nPotential Function Analysis The analysis of our algorithm builds on the powerful potential function\nmethod , as does the analysis of the classical D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc [17].\nOur potential analysis follows the well-known interleaving moves technique [12]. To compare two\nalgorithmsAandBin terms of competitiveness, we simulate both in parallel on some instance ğ¼. Then, we\nemploy a potential function Î¦which maps at every time ğ‘¡the state of both algorithms (i.e. the algorithms\ncurrent congurations) to a value Î¦ğ‘¡\u00150, the potential at time ğ‘¡. We dene Î”Î¦ğ‘¡=Î¦ğ‘¡\u0000Î¦ğ‘¡\u00001. LetÎ”Bğ‘¡Â¹ğ¼Âº\n5\n\nresp.Î”Ağ‘¡Â¹ğ¼Âºdenote the costAresp.Bcharges for serving the request at time ğ‘¡and letğœ‡Â¡0. For every\nrequestğ‘Ÿğ‘¡, we assume that rst Bserves the request, and second A. If\n(i) the move ofBincreases Î¦by at mostğœ‡\u0001Î”Bğ‘¡Â¹ğ¼Âº, whereas\n(ii) the move ofAdecreases Î¦by at least Î”Ağ‘¡Â¹ğ¼Âº,\nwe can use a telescoping sum argument to conclude AÂ¹ğ¼Âº\u0014ğœ‡\u0001BÂ¹ğ¼ÂºÂ¸Î¦0. Note that ifBis the optimal\nalgorithm,ğœ‡is equal to the competitive ratio of AsinceÎ¦0only depends on ğ¶0.\nTo show an error-dependent competitive ratio in the learning-augmented setting, we follow three steps.\nWe show rst that the cost of L/a.sc/m.sc/b.sc/d.sc/a.scDC is close to the cost of F/t.scP, that is A/l.sc/g.scÂ¹ğ¼Âº\u0014ğ›¼Â¹ğ‘˜Âº\u0001F/t.scPÂ¹ğ¼ÂºÂ¸ğ‘\nfor someğ‘Â¡0and for every instance ğ¼. Note that this corresponds to the consistency case as F/t.scPis the\noptimal algorithm if ğœ‚=0. Second we plug in the denition of our prediction error ğœ‚to bound the cost\nofF/t.scPby the cost of the xed optimal solution (xed with respect to the denition of ğœ‚) andğœ‚. Combining\nboth results yields the rst part of the competitive ratio of Theorem 1. Lastly we prove a robustness bound,\ni.e. a general bound independent of the prediction, on the cost of L/a.sc/m.sc/b.sc/d.sc/a.scDC with respect to O/p.sc/t.sc. All\nadditive constants in the competitive ratios only depend on the initial conguration of the servers, being\nzero if all servers start at the same position.\nThe potential functions we use to analyze L/a.sc/m.sc/b.sc/d.sc/a.scDC are inspired by the potential function in the\nclassical analysis of D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc [17]. It is composed of a matching part Î¨, summing the distances\nbetween the server positions of an algorithm and the reference algorithm ( O/p.sc/t.sc,F/t.scP) and a spreadness part Î˜,\nsumming the distances between an algorithms server positions. To incorporate the more sophisticated\nserver moves at dierent speeds, we introduce multiplicative coecients to both parts. The main technical\ncontribution lies in identifying the proper weights and performing the much more involved analysis.\nLower Bounds for L/a.sc/m.sc/b.sc/d.sc/a.scDC In Appendix A we show that our analysis is tight.\nLemma 3. L/a.sc/m.sc/b.sc/d.sc/a.scDC is at leastğ›¼Â¹ğ‘˜Âº-consistent and ğ›½Â¹ğ‘˜Âº-robust.\nOrganization of the paper For ease of exposition, we rst consider the setting of 2servers in Section 3.\nThen, we extend the techniques to the general setting in Sections 4 and 5 while maintaining the same\nstructure as for ğ‘˜=2. We illustrate and discuss the results of computational experiments in Section 6, and,\nnally, talk about PAC learnability of our predictions in Section 7.\n3 Full Analysis for Two Servers\n3.1 Error-dependent Competitive Ratio of L/a.sc/m.sc/b.sc/d.sc/a.scDC\nWe show the theoretical guarantees of L/a.sc/m.sc/b.sc/d.sc/a.scDC claimed in Theorem 1 restricted to two servers. We denote\nthe cost of L/a.sc/m.sc/b.sc/d.sc/a.scDC for some instance ğ¼byA/l.sc/g.scÂ¹ğ¼Âº, and the cost for serving a request ğ‘Ÿğ‘¡byÎ”A/l.sc/g.scğ‘¡Â¹ğ¼Âº.\nIfğ‘¡is clear from the context then we omit the index.\nTheorem 4. For any parameter ğœ†2Â»0Â•1Â¼,L/a.sc/m.sc/b.sc/d.sc/a.scDC has a competitive ratio of at most\nmin\u001a\nÂ¹1Â¸ğœ†Âº\u0010\n1Â¸ğœ‚\nO/p.sc/t.sc\u0011\nÂ•1Â¸1\nğœ†\u001b\nÂ”\nThus, it isÂ¹1Â¸ğœ†Âº-consistent andÂ¹1Â¸1Âğœ†Âº-robust.\n6\n\nWe follow the three-step approach outlined in the previous section. The denition of ğœ‚immediately\ngives for any instance ğ¼and prediction with error ğœ‚thatF/t.scPÂ¹ğ¼Âº=O/p.sc/t.scÂ¹ğ¼ÂºÂ¸ğœ‚. With Lemmata 5 and 6 this\nimplies Theorem 4. We rstly compare the algorithm to F/t.scP.\nLemma 5. For any instance ğ¼andğœ†2Â»0Â•1Â¼, there is some ğ‘\u00150that only depends on the initial conguration\nsuch that A/l.sc/g.scÂ¹ğ¼Âº\u0014Â¹ 1Â¸ğœ†Âº\u0001F/t.scPÂ¹ğ¼ÂºÂ¸ğ‘.\nProof. Letğ¼be an arbitrary instance and let servers start at positions ğ‘ 0\n1andğ‘ 0\n2. Ifğœ†=0,L/a.sc/m.sc/b.sc/d.sc/a.scDC only\nshortcuts F/t.scPâ€™s moves, hence A/l.sc/g.scÂ¹ğ¼Âº\u0014F/t.scPÂ¹ğ¼Âº. Now assume that ğœ†Â¡0. Letğ‘ 1Â•ğ‘ 2beL/a.sc/m.sc/b.sc/d.sc/a.scDC â€™s servers\nandğ‘¥0\n1Â•ğ‘¥0\n2beF/t.scPâ€™s servers. We simulate ğ¼in parallel for both algorithms. At every time ğ‘¡, we map the\ncongurations of both algorithms to a non-negative value using the potential function\nÎ¦=1Â¸ğœ†\nğœ†\u0000jğ‘ 1\u0000ğ‘¥0\n1jÂ¸jğ‘ 2\u0000ğ‘¥0\n2j\u0001\n|                             {z                             }\nÎ¨(matching part)Â¸ jğ‘ 1\u0000ğ‘ 2j|  {z  }\nÎ˜(spreadness part)Â”\nSuppose that a new request arrives. First, F/t.scP serves the request. Assume that ğ‘¥0\n1moves and charges\ncostÎ”F/t.scP. Since L/a.sc/m.sc/b.sc/d.sc/a.scDC remains in its previous conguration, jğ‘¥0\n1\u0000ğ‘ 1jincreases by at most Î”F/t.scP,\nandÎ¦increases by at most Â¹1Â¸ğœ†ÂºÂğœ†\u0001Î”F/t.scP. Second, L/a.sc/m.sc/b.sc/d.sc/a.scDC moves. Assume by scaling the instance\nthat the algorithm serves the request after exactly one time unit, i.e., the fast server moves distance 1and\nthe slow server distance ğœ†. We distinguish whether the request is between the algorithmâ€™s servers or not,\nand prove in each case that Î¦decreases by at least 1Âğœ†\u0001Î”A/l.sc/g.sc.\n(a)Suppose the request is not between the servers ğ‘ 1andğ‘ 2; say, it is left of ğ‘ 1. Then L/a.sc/m.sc/b.sc/d.sc/a.scDC moves\nonlyğ‘ 1andÎ”A/l.sc/g.sc=1. Eitherğ‘¥0\n1orğ‘¥0\n2covers the request, hence moving ğ‘ 1decreases Î¨byÂ¹1Â¸ğœ†ÂºÂğœ†\nwhile it increases Î˜by1. Thus,\nÎ”Î¦\u0014\u00001Â¸ğœ†\nğœ†Â¸1=\u00001\nğœ†=\u00001\nğœ†\u0001Î”A/l.sc/g.scÂ”\n(b)Suppose the request is between ğ‘ 1andğ‘ 2, and suppose that ğ‘ 1is predicted. L/a.sc/m.sc/b.sc/d.sc/a.scDC moves both\nservers and Î”A/l.sc/g.sc=1Â¸ğœ†. This means that ğ‘¥0\n1already covers the request. Thus, moving ğ‘ 1towards\nthe request decreases Î¨byÂ¹1Â¸ğœ†ÂºÂğœ†, whileğ‘ 2increases Î¨by at mostÂ¹1Â¸ğœ†ÂºÂğœ†\u0001ğœ†. Also, Î˜decreases\nby1Â¸ğœ†. We can conclude that\nÎ”Î¦\u00141Â¸ğœ†\nğœ†Â¹\u00001Â¸ğœ†Âº\u0000Â¹1Â¸ğœ†Âº=\u00001\nğœ†Â¹1Â¸ğœ†Âº=\u00001\nğœ†\u0001Î”A/l.sc/g.scÂ”\nSumming over all rounds, we obtain A/l.sc/g.scÂ¹ğ¼Âº\u0014Â¹ 1Â¸ğœ†ÂºF/t.scPÂ¹ğ¼ÂºÂ¸ğœ†jğ‘ 0\n1\u0000ğ‘ 0\n2j. \u0003\nFinally, we give a robustness guarantee for L/a.sc/m.sc/b.sc/d.sc/a.scDC â€™s performance independently of the prediction\nquality.\nLemma 6. For any instance ğ¼andğœ†2Â¹0Â•1Â¼, there is some ğ‘\u00150that only depends on the initial conguration\nsuch that A/l.sc/g.scÂ¹ğ¼Âº\u0014Â¹ 1Â¸1Âğœ†Âº\u0001O/p.sc/t.scÂ¹ğ¼ÂºÂ¸ğ‘.\nThe proof of this claim is similar to the proof of Lemma 5 with the crucial dierence that the reference\nalgorithm is unknown. Hence, the multiplicative factor is larger but relative to the optimal solution and,\nthus, independent of the prediction error.\n7\n\nProof. Letğ¼be an arbitrary instance and let ğœ†2Â¹0Â•1Â¼. Letğ‘ 1Â•ğ‘ 2beL/a.sc/m.sc/b.sc/d.sc/a.scDC â€™s servers and ğ‘¥1Â•ğ‘¥2the\nservers of an optimal algorithm. We dene\nÎ¦=Â¹1Â¸ğœ†ÂºÂ¹jğ‘ 1\u0000ğ‘¥1jÂ¸jğ‘ 2\u0000ğ‘¥2jÂº|                               {z                               }\nÎ¨Â¸jğ‘ 1\u0000ğ‘ 2j|  {z  }\nÎ˜Â”\nUpon arrival of a request, rst the optimal algorithm moves and Î¦increases by at most Â¹1Â¸ğœ†Âº\u0001Î”O/p.sc/t.sc.\nSecond L/a.sc/m.sc/b.sc/d.sc/a.scDC moves and, by scaling the instance, we assume that the request is served after exactly\none time unit. We distinguish whether the request is between the algorithmâ€™s servers or not, and show that\nin each case Î¦decreases by at least ğœ†\u0001Î”A/l.sc/g.sc.\n(a)Let the request be not between the servers, say on the left of ğ‘ 1. Eitherğ‘¥1orğ‘¥2covers the request,\nhence moving ğ‘ 1decreases Î¨by1Â¸ğœ†while it increases Î˜by1. Thus,\nÎ”Î¦\u0014\u0000Â¹ 1Â¸ğœ†ÂºÂ¸1=\u0000ğœ†=\u0000ğœ†\u0001Î”A/l.sc/g.scÂ”\n(b)Let the request be between ğ‘ 1andğ‘ 2, and suppose that ğ‘ 1is predicted. The request is covered by ğ‘¥1\norğ‘¥2. In the worst case ( ğ‘¥2covers the request), moving ğ‘ 1towards the request increases Î¨by at\nmost 1Â¸ğœ†, whileğ‘ 2decreases Î¨only byÂ¹1Â¸ğœ†Âºğœ†. Also, Î˜decreases by 1Â¸ğœ†. Put together,\nÎ”Î¦\u0014Â¹1Â¸ğœ†ÂºÂ¹1\u0000ğœ†Âº\u0000Â¹1Â¸ğœ†Âº=\u0000ğœ†Â¹1Â¸ğœ†Âº=\u0000ğœ†\u0001Î”A/l.sc/g.scÂ” \u0003\n3.2 Optimality of L/a.sc/m.sc/b.sc/d.sc/a.scDC : the Consistency-Robustness Tradeo\nWe now show that L/a.sc/m.sc/b.sc/d.sc/a.scDC is optimal for two servers, in the sense that no memory-constrained algorithm\ncan achieve a better robustness-consistency tradeo. As we target memory-constrained algorithms, at any\ntime, we can use force requests, cf., Section 1, to enforce the algorithm to place its servers at prescribed\nlocations.\nTheorem 7. LetAbe a learning-augmented memory-constrained algorithm for the 2-server problem on the\nline and letğœ†2Â¹0Â•1Â¼. IfAisÂ¹1Â¸ğœ†Âº-consistent, it is at least Â¹1Â¸1Âğœ†Âº-robust.\nProof. Letğœ†2Â¹0Â•1Â¼andAbe aÂ¹1Â¸ğœ†Âº-consistent, memory-constrained algorithm for the 2-server problem\non the line. This means for every instance ğ¼,AÂ¹ğ¼Âº\u0014Â¹ 1Â¸ğœ†Âº\u0001O/p.sc/t.scÂ¹ğ¼ÂºÂ¸ğœˆifğœ‚=0, whereğœˆdepends on the\ninitial conguration. Let ğ‘Â•ğ‘andğ‘be consecutive points on the line at position \u00001,0andğ¿\u00151Â¸1Âğœ†,\nandÂ¹ğ‘Â•ğ‘Âºthe algorithmâ€™s initial conguration.\nConsider the instance ğ¼1which is composed of a force to Â¹ğ‘Â•ğ‘Âº, followed by arbitrarily many alternating\nrequests atğ‘andğ‘. Clearly, an optimal solution for instance ğ¼1is to move the right server to ğ‘and then\nimmediately back to ğ‘with a total cost of 2ğ¿.\nAssume thatAgets this optimal solution as prediction. Amoves one server to ğ‘for the rst request.\nSince the consistency implies that AÂ¹ğ¼1Âº\u0014Â¹ 1Â¸ğœ†ÂºO/p.sc/t.sc, at some point in time Ahas to move the right\nserver toğ‘. Denote the instance which ends at this point in time by ğ¼. Note thatAÂ¹ğ¼1Âº\u0015AÂ¹ğ¼Âº. Letğ‘›ğ¿\ndenote the number of times in instance ğ¼where the left server moves from ğ‘toğ‘and back to ğ‘(cost of 2).\nSince the right server pays at least ğ¿for moving from ğ‘toğ‘, we concludeAÂ¹ğ¼Âº\u00152ğ‘›ğ¿Â¸2ğ¿. The consistency\nofAleads to 2ğ‘›ğ¿Â¸2ğ¿\u0014Â¹1Â¸ğœ†Âº2ğ¿Â¸ğœˆ, which means ğ‘›ğ¿\u0014ğœ†ğ¿Â¸ğœˆÂ2.\nWe now construct another instance ğ¼ğœ”by concatenating ğœ”copies of instance ğ¼, each starting by the\nforce toÂ¹ğ‘Â•ğ‘Âº. We call such a copy an iteration , and in each iteration we use the same predictions as in\ninstanceğ¼.Ahas to pay at least ğ¿for the force, as the right server was previously on ğ‘, and thenAfollows\n8\n\nğ‘ 1ğ‘ 2ğ‘ 3ğ‘ 4\u0001\u0001\u0001ğ‘ ğ‘˜\u00002ğ‘ ğ‘˜\u00001ğ‘ ğ‘˜â„“=0â„“=1â„“=2 â„“=2â„“=1â„“=0\nâ„“=0\nâ„“=1 â„“=3â„“=2â„“=1\nFigure 1: Visualization of all incident ğ›¿ğ‘–ğ‘—-weights of the servers ğ‘ 1andğ‘ 2. The thickness (resp. color) of an arc indicates\nthe inuence of the corresponding distance in Î¦.\nthe same behavior as in ğ¼in each iteration. So AÂ¹ğ¼ğœ”Âº\u0015ğœ”\u0001Â¹2ğ‘›ğ¿Â¸2ğ¿Âº. Another solution for instance ğ¼ğœ”is\nto move the right server to ğ‘in the beginning with cost ğ¿and leave it there, while the left server alternates\nbetweenğ‘andğ‘. Hence, O/p.sc/t.scÂ¹ğ¼ğœ”Âº\u0014ğ¿Â¸ğœ”\u00012Â¹ğ‘›ğ¿Â¸1Âº. Indeed,ğ‘is requested ğ‘›ğ¿Â¸1times per iteration: ğ‘›ğ¿\nwhereAuses the left server and one where it uses the right server. The ratio is then\nAÂ¹ğ¼ğœ”Âº\nO/p.sc/t.scÂ¹ğ¼ğœ”Âº\u0015ğœ”\u0001Â¹2ğ‘›ğ¿Â¸2ğ¿Âº\nğ¿Â¸ğœ”\u00012Â¹ğ‘›ğ¿Â¸1Âºğœ”!1\u0000\u0000\u0000\u0000!2ğ‘›ğ¿Â¸2ğ¿\n2Â¹ğ‘›ğ¿Â¸1Âº=1Â¸ğ¿\u00001\nğ‘›ğ¿Â¸1\u00151Â¸ğ¿\u00001\nğœ†ğ¿Â¸ğœˆ\n2Â¸1ğ¿!1\u0000\u0000\u0000\u0000! 1Â¸1\nğœ†Â•\nwhich implies that Ais at leastÂ¹1Â¸1Âğœ†Âº-robust. \u0003\n4 The General Case with ğ‘˜Servers: Upper Bound\nWe present two lemmas which imply Theorem 1. The novelty lies in designing appropriate potential\nfunctions that capture the server movements at dierent speeds. This takes substantially more technical\ncare than in the 2-server case but builds on the same ideas.\nIn the rst step of the analysis, we compare the performance of L/a.sc/m.sc/b.sc/d.sc/a.scDC andF/t.scP.\nLemma 8. For every instance ğ¼andğœ†2Â»0Â•1Â¼, there is some ğ‘Â¡0that only depends on the initial conguration\nsuch that A/l.sc/g.scÂ¹ğ¼Âº\u0014ğ›¼Â¹ğ‘˜Âº\u0001F/t.scPÂ¹ğ¼ÂºÂ¸ğ‘.\nLetğ¼be an arbitrary instance. Note that ğœ†=0implies A/l.sc/g.scÂ¹ğ¼Âº \u0014 F/t.scPÂ¹ğ¼ÂºasL/a.sc/m.sc/b.sc/d.sc/a.scDC can only\nshortcut F/t.scPâ€™s moves. So, we now assume that ğœ†2Â¹0Â•1Â¼. We dene a new potential function Î¦as follows.\nLetğ‘ 1Â•Â”Â”Â”Â•ğ‘ ğ‘˜be the servers of L/a.sc/m.sc/b.sc/d.sc/a.scDC and letğ‘¥0\n1Â•Â”Â”Â”Â•ğ‘¥0\nğ‘˜be the servers of F/t.scP. For 1\u0014ğ‘–ÂŸğ‘—\u0014ğ‘˜\nandâ„“=minfğ‘—\u0000ğ‘–Â•ğ‘˜\u0000Â¹ğ‘—\u0000ğ‘–Âºg\u0000 1we deneğ›¿ğ‘–ğ‘—=ğœ†â„“, see Figure 1. Then,\nÎ¦=ğ›¼Â¹ğ‘˜Âº\nğœ†\u0001ğ‘˜âˆ‘ï¸\nğ‘–=1jğ‘ ğ‘–\u0000ğ‘¥0\nğ‘–j\n|                 {z                 }\nÎ¨Â¸âˆ‘ï¸\nğ‘–ÂŸğ‘—ğ›¿ğ‘–ğ‘—jğ‘ ğ‘–\u0000ğ‘ ğ‘—jÂ”\n|            {z            }\nÎ˜\nIntuitively, the leading coecient of Î¨comes from the targeted competitive ratio. Then, in Î˜, the coecient\nin front of each term depends on the number of interleaving servers. Following the idea of Lemma 3, when\nL/a.sc/m.sc/b.sc/d.sc/a.scDC moves a server by a distance of 1 as in O/p.sc/t.sc, its neighbor moves by a distance of ğœ†. Hence,\n9\n\ncorrecting the position of this neighbor means that the next server moves by a distance ğœ†2. Therefore,\nthis geometric decrease in the consequences of a movement also appears in the expression of Î˜. The\nsymmetric increase when ğ‘—\u0000ğ‘–grows is more dicult to explain intuitively, but is required to compensate\nthe modications of Î¨. The coecients of Î˜are illustrated in Figure 1.\nWe carefully analyze in Appendix B how the potential changes when F/t.scPandL/a.sc/m.sc/b.sc/d.sc/a.scDC move servers.\nFurther, we give a robustness guarantee for L/a.sc/m.sc/b.sc/d.sc/a.scDC for any error.\nLemma 9. For any instance ğ¼andğœ†2Â¹0Â•1Â¼, there is some ğ‘\u00150that only depends on the initial conguration\nsuch that A/l.sc/g.scÂ¹ğ¼Âº\u0014ğ›½Â¹ğ‘˜Âº\u0001O/p.sc/t.scÂ¹ğ¼ÂºÂ¸ğ‘.\nProving the general upper bound on the competitive ratio, independent of the prediction error, is much\nmore intricate than in the two-server case and than the consistency proof. Again, our key ingredient is\na carefully chosen potential function Î¦. We generalize the function used for the consistency bound even\nfurther by rening the weights, in particular, adding server-dependent weights to the term Î¨measuring\nthe distance between the positions of the algorithmâ€™s servers and the optimal servers.\nLetğœ†2 Â¹0Â•1Â¼. Fixğ‘˜, letğ›½=ğ›½Â¹ğ‘˜Âº=Ãğ‘˜\u00001\nğ‘–=0ğœ†\u0000ğ‘–, and letğ‘ 1Â•Â”Â”Â”Â•ğ‘ ğ‘˜be the servers of L/a.sc/m.sc/b.sc/d.sc/a.scDC and\nletğ‘¥1Â•Â”Â”Â”Â•ğ‘¥ğ‘˜be the servers of an optimal solution. The potential function is\nÎ¦=ğ›½ğ›¾ ğ‘˜âˆ‘ï¸\nğ‘–=1ğœ”ğ‘–jğ‘ ğ‘–\u0000ğ‘¥ğ‘–j!\n|                  {z                  }\nÎ¨Â¸âˆ‘ï¸\nğ‘–ÂŸğ‘—ğ›¿ğ‘–ğ‘—jğ‘ ğ‘–\u0000ğ‘ ğ‘—j\n|           {z           }\nÎ˜Â”\nWe specify the weights in this function as follows. For a pair of servers ğ‘ ğ‘–Â•ğ‘ ğ‘—with 1\u0014ğ‘–ÂŸğ‘—\u0014ğ‘˜,\nletâ„“=minfğ‘—\u0000ğ‘–Â•ğ‘˜\u0000Â¹ğ‘—\u0000ğ‘–Âºg\u0000 1andğ›¿ğ‘–ğ‘—=Â¹ğœ†â„“Â¸ğœ†ğ‘˜\u00002\u0000â„“ÂºÂÂ¹1Â¸ğœ†ğ‘˜\u00002ÂºÂ”\nThe intuition of the weights in the spreadness part Î˜is the same as in the consistency potential function\nabove. However, the new weights ğœ”ğ‘–in the matching part Î¨(dened below) require the more complex\nweightsğ›¿ğ‘–ğ‘—compared to the simpler ğœ†â„“weights.\nFurther, we dene ğ‘‘dğ‘˜Â2e=0ifğ‘˜is odd and for all 1\u0014ğ‘–\u0014bğ‘˜Â2clet\nğ‘‘ğ‘–=ğ‘‘ğ‘˜Â¸1\u0000ğ‘–=2\n1Â¸ğœ†ğ‘˜\u00002ğ‘˜\u00001\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–\u00001ğœ†â„“Â”\nWe demonstrate in the appendix that these values correspond to the change of Î˜when a server of L/a.sc/m.sc/b.sc/d.sc/a.scDC\nmoves. Letğ›¾=ğ‘‘1ÂÂ¹ğ›½\u00001Âº,ğœ”1=ğœ”ğ‘˜=1and for 2\u0014ğ‘–\u0014dğ‘˜Â2ewe dene the server-individual weights\nğœ”ğ‘–=ğœ”ğ‘˜Â¸1\u0000ğ‘–=8>>>>>> <\n>>>>>>:2ğœ†Ãğ‘–Â2\u00001\nğ‘—=1ğ‘‘2ğ‘—\u00002Ãğ‘–Â2\u00001\nğ‘—=1ğ‘‘2ğ‘—Â¸1Â¸ğœ†ğ‘‘ğ‘–Â¸Â¹2Â¸ğœ†Âºğ›¾\nğ›½ğ›¾ğœ†ifğ‘–is even, and\n2ğœ†ÃÂ¹ğ‘–\u00001ÂºÂ2\nğ‘—=1ğ‘‘2ğ‘—\u00002ÃÂ¹ğ‘–\u00003ÂºÂ2\nğ‘—=1ğ‘‘2ğ‘—Â¸1\u0000ğ‘‘ğ‘–Â¸ğ›¾\nğ›½ğ›¾ifğ‘–is odd.\nWe nally prove Lemma 9 in Appendix B by exhaustively reviewing all possible moves and bounding\nthe corresponding change of Î¦. Establishing a constant upper bound of the ğœ”-weights yields a general\nupper bound on the increase of Î¦independently of the choice of the optimal solutionâ€™s server. We further\nchoose the scaling parameter ğ›¾such that the decrease of Î¦exactly matches the required lower bound for\nthe case where the request is outside of the convex hull of L/a.sc/m.sc/b.sc/d.sc/a.scDC â€™s servers. The remaining cases are\nsplit among the possible locations where a request can appear between two servers of L/a.sc/m.sc/b.sc/d.sc/a.scDC , and\n10\n\nwe show in each case that Î¦decreases enough. Intuitively, the ğœ”values are dened such that a wrong\nprediction gives a tight bound on the decrease of Î¦forL/a.sc/m.sc/b.sc/d.sc/a.scDC â€™s move, while a correct prediction still\nguarantees a loose bound.\n5 The Consistency-Robustness Tradeo\nIn this section we give a bound on the consistency-robustness tradeo, as stated in Theorem 2. Our\nbound holds for memory-constrained algorithms that satisfy a certain locality property, which includes\nL/a.sc/m.sc/b.sc/d.sc/a.scDC . Informally, we require that a ğ‘˜-server algorithm with a certain consistency ğœ‡Â¹ğ‘˜Âºshall have a\nconsistency ğœ‡Â¹ğ‘˜0Âºon a sub-instance that it serves with ğ‘˜0ÂŸğ‘˜servers. The rationale is to prevent the mere\npresence of additional unused workers to allow the algorithm to perform poorly on a subinstance served\nby few servers, as ğœ‡Â¹ğ‘˜0ÂºÂŸğœ‡Â¹ğ‘˜Âº. Hence, such algorithms are expected to present a better performance on\na modied instance where some extreme servers are removed and side-eects due to their presence are\nsimulated. In the following, we make this intuition precise and sketch our worst-case construction.\nGiven an algorithm Awhich isğœ‡Â¹ğ‘˜Âº-consistent for the ğ‘˜-server problem, we dene the notion of\nlocally-consistent . Given an instance of the ğ‘˜-server problem served by algorithm A, consider any\nsubsetğ‘†0ofğ‘˜0consecutive servers. We construct an instance ğ¼0of theğ‘˜0-server problem based on ğ¼andğ‘†0:\nIf a request of ğ¼is predicted to be served by a server in ğ‘†0then this request is replicated in ğ¼0. Otherwise, ğ¼0\nrequests the position of the closest server among ğ‘†0afterAserved this request in ğ¼(in order to take into\naccount side-eects due to additional servers in the original instance). Let F/t.scPÂ¹ğ¼0Âºbe the cost of solving ğ¼0\nfollowing the original predictions of ğ¼0(using the closest server among ğ‘†0if a server outside of ğ‘†0was\ninitially predicted). An algorithm is locally-consistent if its total cost on ğ¼restricted to the servers in ğ‘†0is at\nmostğœ‡Â¹ğ‘˜0Âº\u0001F/t.scPÂ¹ğ¼0ÂºÂ¸ğ‘, whereğ‘can be upper bounded based only on the initial conguration. We further\nrequire that if the initial and nal congurations dier by a total distance of ğœ€, thenğ‘=ğ‘‚Â¹ğ‘˜0ğœ€Âº. Note that\nL/a.sc/m.sc/b.sc/d.sc/a.scDC is locally-consistent as its behavior in ğ¼restricted to the servers in ğ‘†0is equal to its behavior\ninğ¼0withğ‘˜0servers.\nThe proof of Theorem 2 generalizes ideas from the 2-server case (Section 3.2) in a highly non-trivial\nway. We only sketch the main idea and refer to Appendix C for details. Let Abe a memory-constrained and\nlocally-consistent deterministic algorithm. We construct an instance that starts with ğ‘˜equidistant servers.\nFirst, a point far on the right is requested. Then the initial server locations are requested following specic\nrules until the rightmost server comes back. Predictions correspond to the server initially at the point\nrequested. The consistency of Alimits the possible cost paid before the rightmost server comes back. The\nlocally-consistent denition allows, with technical care, to link the distance traveled by two neighboring\nservers: the left one travels a total distance at most ğœ†times the right one (plus negligible terms). An oine\nsolution can aord to initially shift all servers to the right, and then move only the leftmost server, which\nAcould not move much. We then repeat this instance, and use the memory-constrained and deterministic\ncharacteristics ofAto eliminate constant costs and show the desired robustness lower bound, again with\ntechnical care.\n6 Experiments\nWe supplement our theoretical results by empirically comparing our learning-augmented algorithm L/a.sc/m.sc/b.sc/hyphen.sc\n/d.sc/a.scDC with the classical online algorithm ignoring predictions D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc [17] and the previously\nproposed prediction-based algorithm F/t.scP&DC[3] on real world data. We generate instances with 1000\nrequests based on the BrightKite-Dataset [16], which is composed of sequences of coordinates of app\n11\n\n0 2 4 6 8\nEta / Opt1.01.52.02.53.03.54.0Empirical competitive ratio\nFtP&DC\nDC\nLDC (  = 0.00)\nLDC (  = 0.10)\nLDC (  = 0.50)\n(a) Results for ğ‘˜=2.\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\nEta / Opt1.01.21.41.61.82.02.2Empirical competitive ratio\nFtP&DC\nDC\nLDC (  = 0.00)\nLDC (  = 0.10)\nLDC (  = 0.50)\n (b) Results for ğ‘˜=10.\nFigure 2: Non-lazy algorithms: means over all empirical competitive ratios per prediction quality.\ncheck-ins. This dataset was used previously to evaluate and compare learning-augmented algorithms for\ncaching problems [3, 37]. We further generate predictions in a semi-random fashion aiming for large and\nevenly distributed prediction errors. All algorithms are implemented in lazy and non-lazy variants.\nThe results for non-lazy implementations are displayed in Figure 2. They show well that, for a reasonable\nchoice ofğœ†(0Â”1\u0014ğœ†\u00140Â”5),L/a.sc/m.sc/b.sc/d.sc/a.scDC outperforms both D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc andF/t.scP&DCfor almost all\ngenerated relative prediction errors. This is true even if laziness is allowed as we show in Appendix E. We\ngive also more details on the generation of instances and predictions, as well as an overview over all results.\n7 PAC Learnability of Predictions\nWhile our results show the applicability of untrusted predictions, it is a natural question whether such\npredictions are actually learnable.\nIn Appendix D, we show that for our model a static prediction sequence is PAC learnable in an agnostic\nsense using empirical risk minimization. That is, given an unknown distribution over request sequences\nwhich we can sample, we can nd a prediction that is close to the best possible prediction for this distribution\nin terms of prediction error using a bounded number of samples.\nTheorem 10. For anyğœ–Â•ğ›¿2 Â¹0Â•1Âº, a known initial conguration ğ¶0and any distribution Dover the\nsequences of ğ‘›requests of known extent, there exists an algorithm which, given an i.i.d. sample of Dof\nsizeğ‘š2O\u0010\n1\nğœ–2\u0001Â¹ğ‘›logğ‘˜\u0000logğ›¿Âºğœ‚2\nmax\u0011\n, returns a prediction ğœğ‘2H in polynomial time depending on ğ‘˜,ğ‘›\nandğ‘š, such that with probability of at least Â¹1\u0000ğ›¿Âºit holds Eğœ\u0018DÂ»ğœ‚ğœÂ¹ğœğ‘ÂºÂ¼\u0014Eğœ\u0018DÂ»ğœ‚ğœÂ¹ğœ\u0003ÂºÂ¼Â¸ğœ–, whereğœ\u0003=\narg minğœ2HEğœ\u0018DÂ»ğœ‚ğœÂ¹ğœÂºÂ¼.\nWe remark that a pre-computed static prediction does not include information about the partially\nrevealed input. Thus, this is a rather weak prediction and may not help L/a.sc/m.sc/b.sc/d.sc/a.scDC much. The existence of\nanadaptive prediction policy which can be eciently learned remains an open question. Such a policy\nwould provide much more valuable information to our learning-augmented online algorithm.\n12\n\n8 Conclusion\nWe show the power of (untrusted) predictions in designing online algorithms for the ğ‘˜-server problem\non the line. Our algorithm generalizes the classical D/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc algorithm [17] in an intuitive way\nand admits a (nearly) tight error-dependent competitive analysis, based on new potential functions, and\noutperforms other methods from the literature. While we can show PAC learnability for static predictions,\nwe leave open whether possibly more powerful adaptive prediction models are learnable.\nClearly, it would be interesting to see whether our results generalize to more general metric spaces than\nthe line. In fact, in a related version we show that our upper bounds for the 2-server problem can be extended\nto tree metrics [34] and we expect that an extension to ğ‘˜servers is possible. However, for more general\nmetrics our current approach seems not to generalize well. Further, we focused on memory-constrained\nalgorithms, leaving open a more precise quantication of the power of memory. Finally, the recent success\non randomized k-server algorithms [13] raises the question whether and how randomized algorithms can\nbenet from (ML) predictions.\n13\n\nReferences\n[1]Spyros Angelopoulos. Online search with a hint. In ITCS , volume 185 of LIPIcs , pages 51:1â€“51:16.\nSchloss Dagstuhl - Leibniz-Zentrum fÃ¼r Informatik, 2021.\n[2]Spyros Angelopoulos, Christoph DÃ¼rr, Shendan Jin, Shahin Kamali, and Marc P. Renault. Online\ncomputation with untrusted advice. In ITCS , volume 151 of LIPIcs , pages 52:1â€“52:15. Schloss Dagstuhl\n- Leibniz-Zentrum fÃ¼r Informatik, 2020.\n[3]Antonios Antoniadis, Christian Coester, Marek EliÃ¡s, Adam Polak, and Bertrand Simon. Online metric\nalgorithms with untrusted predictions. In ICML , volume 119 of Proceedings of Machine Learning\nResearch , pages 345â€“355. PMLR, 2020.\n[4]Antonios Antoniadis, Carsten Fischer, and Andreas TÃ¶nnis. A collection of lower bounds for online\nmatching on the line. In Latin American Symposium on Theoretical Informatics , pages 52â€“65. Springer,\n2018.\n[5]Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online matching\nproblems with machine learned advice. In NeurIPS , 2020.\n[6]Yossi Azar, Stefano Leonardi, and Noam Touitou. Flow time scheduling with uncertain processing\ntime. In STOC , pages 1070â€“1080. ACM, 2021.\n[7]Ã‰tienne Bamas, Andreas Maggiori, Lars Rohwedder, and Ola Svensson. Learning augmented energy\nminimization via speed scaling. In NeurIPS , 2020.\n[8]Ã‰tienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning augmented\nalgorithms. In NeurIPS , 2020.\n[9]Soumya Banerjee. Improving online rent-or-buy algorithms with sequential decision making and ML\npredictions. In NeurIPS , 2020.\n[10] Yair Bartal, BÃ©la BollobÃ¡s, and Manor Mendel. Ramsey-type theorems for metric spaces with applica-\ntions to online problems. J. Comput. Syst. Sci. , 72(5):890â€“921, 2006.\n[11] Yair Bartal and Elias Koutsoupias. On the competitive ratio of the work function algorithm for the\nk-server problem. Theor. Comput. Sci. , 324(2-3):337â€“345, 2004.\n[12] Allan Borodin and Ran El-Yaniv. Online computation and competitive analysis . Cambridge University\nPress, 1998.\n[13] SÃ©bastien Bubeck, Michael B. Cohen, Yin Tat Lee, James R. Lee, and Aleksander Madry. k-server via\nmultiscale entropic regularization. In STOC , pages 3â€“16. ACM, 2018.\n[14] Niv Buchbinder, Christian Coester, and Joseph (Se) Naor. Online k-taxi via double coverage and\ntime-reverse primal-dual. In IPCO , volume 12707 of Lecture Notes in Computer Science , pages 15â€“29.\nSpringer, 2021.\n[15] Ashish Chiplunkar and Sundar Vishwanathan. Randomized memoryless algorithms for the weighted\nand the generalized k-server problems. ACM Trans. Algorithms , 16(1):14:1â€“14:28, 2020.\n14\n\n[16] Eunjoon Cho, Seth A. Myers, and Jure Leskovec. Friendship and mobility: user movement in location-\nbased social networks. In KDD , pages 1082â€“1090. ACM, 2011.\n[17] Marek Chrobak, Howard J. Karlo, T. H. Payne, and Sundar Vishwanathan. New results on server\nproblems. SIAM J. Discret. Math. , 4(2):172â€“181, 1991.\n[18] Marek Chrobak and Lawrence L. Larmore. An optimal on-line algorithm for k-servers on trees. SIAM\nJ. Comput. , 20(1):144â€“148, 1991.\n[19] Paul DÃ¼tting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. Secretaries with advice. In\nEC, pages 409â€“429. ACM, 2021.\n[20] Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice.\nInICML , volume 97 of Proceedings of Machine Learning Research , pages 2319â€“2327. PMLR, 2019.\n[21] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In ICLR , 2019.\n[22] Sungjin Im, Ravi Kumar, Mahshid Montazer Qaem, and Manish Purohit. Non-clairvoyant scheduling\nwith predictions. In SPAA , pages 285â€“294. ACM, 2021.\n[23] Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In NeurIPS , pages\n7400â€“7410, 2019.\n[24] Zhihao Jiang, Debmalya Panigrahi, and Kevin Sun. Online algorithms for weighted paging with\npredictions. In ICALP , volume 168 of LIPIcs , pages 69:1â€“69:18. Schloss Dagstuhl - Leibniz-Zentrum fÃ¼r\nInformatik, 2020.\n[25] Manoel Leandro L Junior, AD Doria Neto, and Jorge D Melo. The k-server problem: a reinforcement\nlearning approach. In IJCNN , 2005.\n[26] Elias Koutsoupias. The k-server problem. Comput. Sci. Rev. , 3(2):105â€“118, May 2009.\n[27] Elias Koutsoupias and Christos H. Papadimitriou. On the k-server conjecture. J. ACM , 42(5):971â€“983,\n1995.\n[28] Elias Koutsoupias and David Scot Taylor. The CNN problem and other k-server variants. Theoretical\nComputer Science , 324(2):347 â€“ 359, 2004. Online Algorithms: In Memoriam, Steve Seiden.\n[29] Tim Kraska, Alex Beutel, Ed H. Chi, Jerey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. In SIGMOD Conference , pages 489â€“504. ACM, 2018.\n[30] Ravi Kumar, Manish Purohit, Aaron Schild, Zoya Svitkina, and Erik Vee. Semi-online bipartite\nmatching. In ITCS , volume 124 of LIPIcs , pages 50:1â€“50:20. Schloss Dagstuhl - Leibniz-Zentrum fÃ¼r\nInformatik, 2019.\n[31] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online scheduling via\nlearned weights. In SODA , pages 1859â€“1877. SIAM, 2020.\n[32] Thomas Lavastida, Benjamin Moseley, R. Ravi, and Chenyang Xu. Learnable and instance-robust\npredictions for online matching, ows and load balancing. To appear at ESA , abs/2011.11743, 2021.\n15\n\n[33] M. Leandro Costa, C. A. Araujo Padilha, J. Dantas Melo, and A. Duarte Doria Neto. Hierarchical\nreinforcement learning and parallel computing applied to the k-server problem. IEEE Latin America\nTransactions , 14(10):4351â€“4357, 2016.\n[34] Alexander Lindermayr. Learning-augmented online algorithms for the 2-server problem on the line\nand generalizations. Masterâ€™s thesis, University of Bremen, Germany, 2020.\n[35] Ramon Augusto Sousa Lins, AdriÃ£o Duarte DÃ³ria Neto, and Jorge Dantas de Melo. Deep reinforcement\nlearning applied to the k-server problem. Expert Syst. Appl. , 135:212â€“218, 2019.\n[36] Pinyan Lu, Xuandi Ren, Enze Sun, and Yubo Zhang. Generalized sorting with predictions. In Symposium\non Simplicity in Algorithms (SOSA) , pages 111â€“117. SIAM, 2021.\n[37] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. In\nICML , volume 80 of Proceedings of Machine Learning Research , pages 3302â€“3311. PMLR, 2018.\n[38] Mohammad Mahdian, Hamid Nazerzadeh, and Amin Saberi. Allocating online advertisement space\nwith unreliable estimates. In EC, pages 288â€“294. ACM, 2007.\n[39] Mark S. Manasse, Lyle A. McGeoch, and Daniel Dominic Sleator. Competitive algorithms for on-line\nproblems. In STOC , pages 322â€“333. ACM, 1988.\n[40] Mark S. Manasse, Lyle A. McGeoch, and Daniel Dominic Sleator. Competitive algorithms for server\nproblems. J. Algorithms , 11(2):208â€“230, 1990.\n[41] Andres MuÃ±oz Medina and Sergei Vassilvitskii. Revenue optimization with approximate bid predictions.\nInNIPS , pages 1858â€“1866, 2017.\n[42] Michael Mitzenmacher. A model for learned bloom lters and optimizing by sandwiching. In NeurIPS ,\npages 462â€“471, 2018.\n[43] Michael Mitzenmacher. Scheduling with predictions and the price of misprediction. In ITCS , volume\n151 of LIPIcs , pages 14:1â€“14:18. Schloss Dagstuhl - Leibniz-Zentrum fÃ¼r Informatik, 2020.\n[44] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ML predictions. In\nNeurIPS , pages 9684â€“9693, 2018.\n[45] Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In SODA , pages\n1834â€“1845. SIAM, 2020.\n[46] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning - From Theory to Algorithms .\nCambridge University Press, 2014.\n[47] Daniel Dominic Sleator and Robert Endre Tarjan. Amortized eciency of list update and paging rules.\nCommun. ACM , 28(2):202â€“208, 1985.\n[48] Leslie G. Valiant. A theory of the learnable. Commun. ACM , 27(11):1134â€“1142, 1984.\n[49] VN Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to\ntheir probabilities. Theory of Probability & Its Applications , 16(2):264â€“280, 1971.\n16\n\n[50] Shufan Wang, Jian Li, and Shiqiang Wang. Online algorithms for multi-shop ski rental with machine\nlearned advice. In NeurIPS , 2020.\n[51] Alexander Wei. Better and simpler learning-augmented online caching. In APPROX/RANDOM , volume\n176 of LIPIcs , pages 60:1â€“60:17. Schloss Dagstuhl - Leibniz-Zentrum fÃ¼r Informatik, 2020.\n[52] Alexander Wei and Fred Zhang. Optimal robustness-consistency trade-os for learning-augmented\nonline algorithms. In NeurIPS , 2020.\n[53] Wenming Zhang and Yongxi Cheng. A new upper bound on the work function algorithm for the\nk-server problem. J. Comb. Optim. , 39(2):509â€“518, 2020.\n17\n\nA Proofs for Section 2\nLemma 3. L/a.sc/m.sc/b.sc/d.sc/a.scDC is at leastğ›¼Â¹ğ‘˜Âº-consistent and ğ›½Â¹ğ‘˜Âº-robust.\nProof. We give two separate instances for consistency and robustness.\n(i)Considerğ‘˜servers initially at positions 0, 1,\u00001,2,\u00002, ... and the request sequence of length ğ‘˜Â¸1\nat positions 0Â”5,0,1,\u00001,2,\u00002, .... There is a solution of cost 1that only moves the server that is\ninitially at 0.\nL/a.sc/m.sc/b.sc/d.sc/a.scDC serves the rst request by moving the optimal server from 0to0Â”5and additionally the\none from 1to1\u0000ğœ†Â2. With the second request, the rst server is moved back to 0, having moved a\ntotal distance of 1, and the server from \u00001moves to\u00001Â¸ğœ†Â2. For the third request, the server from\noriginal position 1returns to this position, etc. Each server moves back to its initial position ğ‘–after\nmoving a total distance of ğœ†jğ‘–j. Repeating this example gives the lower bound on the consistency.\n(ii)Considerğ‘˜servers initially at positions ğ›½Â¹ğ‘–Âº=Ãğ‘˜\u00001\nğ‘–=0ğœ†\u0000ğ‘–, forğ‘–2f1Â•Â”Â”Â”Â•ğ‘˜g, and the request sequence\nof lengthğ‘˜Â¸1at positions 0,ğ›½Â¹1Âº,ğ›½Â¹2Âº, . ..,ğ›½Â¹ğ‘˜Âº. There is a solution of cost 2that only moves the\nserver that is initially at 1. Consider predictions corresponding always to the rightmost server at the\nhighest position.\nL/a.sc/m.sc/b.sc/d.sc/a.scDC serves the second request by moving both servers from 0 and ğ›½Â¹2Âºtoğ›½Â¹1Âºas the closest\nserver moves by a distance of 1 and the furthest server, which is predicted, moves by a distance of 1Âğœ†.\nSimilarly, for each request except the last one, both servers neighboring the request end up serving\nthe request simultaneously. So the ğ‘–-th server moves by a total distance of 2Âğœ†ğ‘–\u00001. Repeating this\nexample gives the lower bound on the robustness. \u0003\nB Proofs for Section 4\nThe Consistency Bound\nLemma 8. For every instance ğ¼andğœ†2Â»0Â•1Â¼, there is some ğ‘Â¡0that only depends on the initial conguration\nsuch that A/l.sc/g.scÂ¹ğ¼Âº\u0014ğ›¼Â¹ğ‘˜Âº\u0001F/t.scPÂ¹ğ¼ÂºÂ¸ğ‘.\nBefore proving Lemma 8, we need a few preliminary results. Let ğ¼be an arbitrary instance. Note\nthatğœ†=0implies A/l.sc/g.scÂ¹ğ¼Âº\u0014F/t.scPÂ¹ğ¼ÂºasL/a.sc/m.sc/b.sc/d.sc/a.scDC can only shortcut F/t.scPâ€™s moves, so we now assume\nthatğœ†2Â¹0Â•1Â¼.\nObservation 11. For everyğ‘˜Â¡3, we haveğ›¼Â¹ğ‘˜Âº\nğœ†=ğ›¼Â¹ğ‘˜\u00002ÂºÂ¸1\nğœ†Â¸1.\nProof. We prove this statement depending on the parity of ğ‘˜. Ifğ‘˜is odd,ğ‘˜\u00002is also odd. By denition\nofğ›¼Â¹ğ‘˜Âº,\n1Â¸1\nğœ†Â¸ğ›¼Â¹ğ‘˜\u00002Âº=1\nğœ†Â¸2Â¸2Â¹ğ‘˜\u00003ÂºÂ2âˆ‘ï¸\nğ‘–=1ğœ†ğ‘–=1\nğœ†Â¸2Â¹ğ‘˜\u00001ÂºÂ2âˆ‘ï¸\nğ‘–=1ğœ†ğ‘–\u00001=ğ›¼Â¹ğ‘˜Âº\nğœ†Â”\n18\n\nğ‘ 1ğ‘ 2ğ‘ 3ğ‘ 4\u0001\u0001\u0001ğ‘ ğ‘˜\u00002ğ‘ ğ‘˜\u00001ğ‘ ğ‘˜â„“=0â„“=1â„“=2 â„“=2â„“=1â„“=0\nâ„“=0\nâ„“=1 â„“=3â„“=2â„“=1\nFigure 3: Visualization of all incident ğ›¿ğ‘–ğ‘—-weights of the servers ğ‘ 1andğ‘ 2. The thickness (resp. color) of an arc indicates\nthe inuence of the corresponding distance in Î¦.\nIfğ‘˜is even,ğ‘˜\u00002is also even, and we conclude\n1Â¸1\nğœ†Â¸ğ›¼Â¹ğ‘˜\u00002Âº=1\nğœ†Â¸2Â¸2Â¹ğ‘˜\u00002ÂºÂ2\u00001âˆ‘ï¸\nğ‘–=1ğœ†ğ‘–Â¸ğœ†Â¹ğ‘˜\u00002ÂºÂ2\n=1\nğœ†Â¸2ğ‘˜Â2\u00001âˆ‘ï¸\nğ‘–=1ğœ†ğ‘–\u00001Â¸ğœ†Â¹ğ‘˜\u00002ÂºÂ2\n=ğ›¼Â¹ğ‘˜Âº\nğœ†Â”\n\u0003\nWe dened our potential function Î¦as follows. Let ğ‘ 1Â•Â”Â”Â”Â•ğ‘ ğ‘˜be the servers of L/a.sc/m.sc/b.sc/d.sc/a.scDC and\nletğ‘¥0\n1Â•Â”Â”Â”Â•ğ‘¥0\nğ‘˜be the servers of F/t.scP. For 1\u0014ğ‘–ÂŸğ‘—\u0014ğ‘˜andâ„“=minfğ‘—\u0000ğ‘–Â•ğ‘˜\u0000Â¹ğ‘—\u0000ğ‘–Âºg\u0000 1we deneğ›¿ğ‘–ğ‘—=ğœ†â„“.\nThen,\nÎ¦=ğ›¼Â¹ğ‘˜Âº\nğœ†\u0001ğ‘˜âˆ‘ï¸\nğ‘–=1jğ‘ ğ‘–\u0000ğ‘¥0\nğ‘–j\n|                 {z                 }\nÎ¨Â¸âˆ‘ï¸\nğ‘–ÂŸğ‘—ğ›¿ğ‘–ğ‘—jğ‘ ğ‘–\u0000ğ‘ ğ‘—jÂ”\n|            {z            }\nÎ˜\nThe analysis of L/a.sc/m.sc/b.sc/d.sc/a.scDC requires evaluating the evolution of Î¦after each request. The following lemma\ncharacterizes how a move of L/a.sc/m.sc/b.sc/d.sc/a.scDC inuences Î˜.\nLemma 12. Letğ‘–\u0014bğ‘˜Â2c. Ifğ‘ ğ‘–moves from ğ‘toğ‘Â¸ğ‘¥,Î˜changes by\nÂ¹\u0000ğ‘¥Âº\u0001 \n1Â¸ğ›¼Â¹ğ‘˜\u00002Âº\u00002ğ‘–\u00002âˆ‘ï¸\nğ‘—=0ğœ†ğ‘—!\nÂ”\nProof. Assume w.l.o.g. that ğ‘¥=\u00001, that is, server ğ‘ ğ‘–moves one unit to the left. Consider servers ğ‘ ğ‘—,ğ‘ 0\nğ‘—\nsuch thatğ‘—0Â¸â„“=ğ‘–=ğ‘—\u0000â„“for some 1\u0014â„“\u0014ğ‘–\u00001. Sinceğ›¿ğ‘–ğ‘—0=ğ›¿ğ‘–ğ‘—we observe that the changes to the\ntermsğ›¿ğ‘–ğ‘—jğ‘ ğ‘–\u0000ğ‘ ğ‘—jandğ›¿ğ‘–ğ‘—0jğ‘ ğ‘–\u0000ğ‘ 0\nğ‘—jofÎ˜cancel out. Hence, as ğ‘–\u0014bğ‘˜Â2c, the change of Î˜due to the move of ğ‘ ğ‘–\nis equal toÃğ‘˜\nğ‘—=2ğ‘–ğ›¿ğ‘–ğ‘—. We now prove the statement depending on the parity of ğ‘˜.\n19\n\n(i) Ifğ‘˜is odd,ğ‘˜\u00002ğ‘–Â¸1is even. By denition,\nğ‘˜âˆ‘ï¸\nğ‘—=2ğ‘–ğ›¿ğ‘–ğ‘—=ğ‘˜âˆ‘ï¸\nğ‘—=2ğ‘–ğœ†minfğ‘—\u0000ğ‘–Â•ğ‘˜\u0000Â¹ğ‘—\u0000ğ‘–Âºg\u00001=2Â¹ğ‘˜\u00001ÂºÂ2Â¸ğ‘–âˆ‘ï¸\nğ‘—=2ğ‘–ğœ†ğ‘—\u0000ğ‘–\u00001=2Â¹ğ‘˜\u00001ÂºÂ2\u00001âˆ‘ï¸\nğ‘—=ğ‘–\u00001ğœ†ğ‘—\n=2Â¸2Â¹ğ‘˜\u00003ÂºÂ2âˆ‘ï¸\nğ‘—=1ğœ†ğ‘—\u00002ğ‘–\u00002âˆ‘ï¸\nğ‘—=0ğœ†ğ‘—=1Â¸ğ›¼Â¹ğ‘˜\u00002Âº\u00002ğ‘–\u00002âˆ‘ï¸\nğ‘—=0ğœ†ğ‘—Â”\n(ii)Ifğ‘˜is even,ğ‘˜\u00002ğ‘–Â¸1is odd, and there is a single term where the minimum in the denition of ğ›¿ğ‘–ğ‘—in\nachieved by both conditions. Hence,\nğ‘˜âˆ‘ï¸\nğ‘—=2ğ‘–ğœ†minfğ‘—\u0000ğ‘–Â•ğ‘˜\u0000Â¹ğ‘—\u0000ğ‘–Âºg\u00001=2ğ‘˜Â2\u00001Â¸ğ‘–âˆ‘ï¸\nğ‘—=2ğ‘–ğœ†ğ‘—\u0000ğ‘–\u00001Â¸ğœ†ğ‘˜Â2\u00001=2ğ‘˜Â2\u00002âˆ‘ï¸\nğ‘—=ğ‘–\u00001ğœ†ğ‘—Â¸ğœ†ğ‘˜Â2\u00001\n=2Â¸2ğ‘˜Â2\u00002âˆ‘ï¸\nğ‘—=1ğœ†ğ‘—Â¸ğœ†ğ‘˜Â2\u00001\u00002ğ‘–\u00002âˆ‘ï¸\nğ‘—=0ğœ†ğ‘—\n=1Â¸ğ›¼Â¹ğ‘˜\u00002Âº\u00002ğ‘–\u00002âˆ‘ï¸\nğ‘—=0ğœ†ğ‘—Â” \u0003\nProof of Lemma 8. Suppose that the next request appears. First F/t.scPmoves some server ğ‘¥0\nğ‘–towards the\nrequest, and the distance to ğ‘ ğ‘–increases by at most Î”F/t.scP. Since this move only aects Î¨,Î”Î¦\u0014ğ›¼Â¹ğ‘˜ÂºÂğœ†\u0001Î”F/t.scP.\nSecond L/a.sc/m.sc/b.sc/d.sc/a.scDC moves. We distinguish whether the request is between two servers or not, and assert\nfor both cases Î”Î¦\u0014\u00001Âğœ†\u0001Î”A/l.sc/g.sc.\n(a)Let the request be located w.l.o.g on the left of ğ‘ 1. Thus,ğ‘ 1moves towards it and charges cost Î”A/l.sc/g.sc.\nThe fact that some server ğ‘¥0\nğ‘—must already be on ğ‘Ÿğ‘¡implies with Lemma 12 that\nÎ”Î¦\u0014\u0000ğ›¼Â¹ğ‘˜Âº\nğœ†Î”A/l.sc/g.scÂ¸ \n1Â¸ğ›¼Â¹ğ‘˜\u00002Âº\u000021\u00002âˆ‘ï¸\nğ‘—=0ğœ†ğ‘—!\nÎ”A/l.sc/g.scÂ”\nRearranging and using Observation 11 gives the claimed bound, that is\n\u0012\n\u0000ğ›¼Â¹ğ‘˜\u00002Âº\u00001\nğœ†\u00001Â¸1Â¸ğ›¼Â¹ğ‘˜\u00002Âº\u0013\nÎ”A/l.sc/g.sc=\u00001\nğœ†Î”A/l.sc/g.scÂ”\n(b)Let the request be between ğ‘ ğ‘–andğ‘ ğ‘–Â¸1. Assume w.l.o.g. that F/t.scPserves it with ğ‘¥0\nğ‘—andğ‘—\u0014ğ‘–. For ease of\nexposition, we assume that ğ‘ ğ‘–travels distance 1andğ‘ ğ‘–Â¸1distanceğœ†. Hence, Î”A/l.sc/g.sc=1Â¸ğœ†. Sinceğ‘—\u0014ğ‘–,\nwe know that ğ‘¥0\nğ‘–must be located on the right of ğ‘¥0\nğ‘—. Hence, the distance between ğ‘ ğ‘–andğ‘¥0\nğ‘–decreases\nby1, but the distance between ğ‘ ğ‘–Â¸1andğ‘¥0\nğ‘–Â¸1increases by at most ğœ†. Thus, Î”Î¨\u0014ğ›¼Â¹ğ‘˜ÂºÂğœ†\u0001Â¹ğœ†\u00001Âº. The\nchange of Î˜is clearly bounded from above by the case where ğ‘ ğ‘–moves distance ğœ†andğ‘ ğ‘–Â¸1moves\ndistance 1forğ‘–Â¸1\u0014bğ‘˜Â2c. Combining Lemma 12 for both servers gives\nÎ”Î˜=1Â¸ğ›¼Â¹ğ‘˜\u00002Âº\u00002ğ‘–\u00001âˆ‘ï¸\nğ‘—=0ğœ†ğ‘—\u0000ğœ† \n1Â¸ğ›¼Â¹ğ‘˜\u00002Âº\u00002ğ‘–\u00002âˆ‘ï¸\nğ‘—=0ğœ†ğ‘—!\n=\u00001\u0000ğœ†Â¸ğ›¼Â¹ğ‘˜\u00002Âº\u0000ğœ†ğ›¼Â¹ğ‘˜\u00002ÂºÂ”\n20\n\nUsing this and Observation 11, we can bound the increase of the potential by\nÎ”Î¦\u0014ğ›¼Â¹ğ‘˜Âº\nğœ†Â¹ğœ†\u00001Âº\u00001\u0000ğœ†Â¸ğ›¼Â¹ğ‘˜\u00002Âº\u0000ğœ†ğ›¼Â¹ğ‘˜\u00002Âº\n=\u0000ğ›¼Â¹ğ‘˜Âº\nğœ†Â¸ğ›¼Â¹ğ‘˜\u00002Âº=\u00001\u00001\nğœ†\n=\u00001\nğœ†Î”A/l.sc/g.scÂ” \u0003\nThe Robustness Bound\nLemma 9. For any instance ğ¼andğœ†2Â¹0Â•1Â¼, there is some ğ‘\u00150that only depends on the initial conguration\nsuch that A/l.sc/g.scÂ¹ğ¼Âº\u0014ğ›½Â¹ğ‘˜Âº\u0001O/p.sc/t.scÂ¹ğ¼ÂºÂ¸ğ‘.\nWe start by dening a potential function Î¦. Letğœ†2 Â¹0Â•1Â¼. Fixğ‘˜, letğ›½=ğ›½Â¹ğ‘˜Âº=Ãğ‘˜\u00001\nğ‘–=0ğœ†\u0000ğ‘–, and\nletğ‘ 1Â•Â”Â”Â”Â•ğ‘ ğ‘˜be the servers of L/a.sc/m.sc/b.sc/d.sc/a.scDC and letğ‘¥1Â•Â”Â”Â”Â•ğ‘¥ğ‘˜be the servers of an optimal solution. The\npotential function is\nÎ¦=ğ›½ğ›¾ ğ‘˜âˆ‘ï¸\nğ‘–=1ğœ”ğ‘–jğ‘ ğ‘–\u0000ğ‘¥ğ‘–j!\n|                  {z                  }\nÎ¨Â¸âˆ‘ï¸\nğ‘–ÂŸğ‘—ğ›¿ğ‘–ğ‘—jğ‘ ğ‘–\u0000ğ‘ ğ‘—j\n|           {z           }\nÎ˜Â”\nWe specify the weights in this function as follows. For a pair of servers ğ‘ ğ‘–Â•ğ‘ ğ‘—with 1\u0014ğ‘–ÂŸğ‘—\u0014ğ‘˜,\nletâ„“=minfğ‘—\u0000ğ‘–Â•ğ‘˜\u0000Â¹ğ‘—\u0000ğ‘–Âºg\u0000 1and\nğ›¿ğ‘–ğ‘—=ğœâ„“=ğœ†â„“Â¸ğœ†ğ‘˜\u00002\u0000â„“\n1Â¸ğœ†ğ‘˜\u00002Â”\nThe intuition of the weights in the spreadness part Î˜is the same as in the consistency potential function.\nHowever, the new weights ğœ”ğ‘–in the matching part Î¨(dened below) require the more complex weights ğ›¿ğ‘–ğ‘—\ncompared to the simpler ğœ†â„“weights.\nFurther, we dene ğ‘‘dğ‘˜Â2e=0ifğ‘˜is odd and for all 1\u0014ğ‘–\u0014bğ‘˜Â2clet\nğ‘‘ğ‘–=ğ‘‘ğ‘˜Â¸1\u0000ğ‘–=2\n1Â¸ğœ†ğ‘˜\u00002ğ‘˜\u00001\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–\u00001ğœ†â„“Â”\nLetğ›¾=ğ‘‘1ÂÂ¹ğ›½\u00001Âº,ğœ”1=ğœ”ğ‘˜=1and for 2\u0014ğ‘–\u0014dğ‘˜Â2ewe dene the server-individual weights\nğœ”ğ‘–=ğœ”ğ‘˜Â¸1\u0000ğ‘–=8>>>>>> <\n>>>>>>:2ğœ†Ãğ‘–Â2\u00001\nğ‘—=1ğ‘‘2ğ‘—\u00002Ãğ‘–Â2\u00001\nğ‘—=1ğ‘‘2ğ‘—Â¸1Â¸ğœ†ğ‘‘ğ‘–Â¸Â¹2Â¸ğœ†Âºğ›¾\nğ›½ğ›¾ğœ†ifğ‘–is even, and\n2ğœ†ÃÂ¹ğ‘–\u00001ÂºÂ2\nğ‘—=1ğ‘‘2ğ‘—\u00002ÃÂ¹ğ‘–\u00003ÂºÂ2\nğ‘—=1ğ‘‘2ğ‘—Â¸1\u0000ğ‘‘ğ‘–Â¸ğ›¾\nğ›½ğ›¾ifğ‘–is odd.\nThis nishes the denition of the potential function Î¦. To prove a robustness guarantee for L/a.sc/m.sc/b.sc/d.sc/a.scDC ,\nwe show bounds on the change of Î¦when the algorithms ( L/a.sc/m.sc/b.sc/d.sc/a.scDC andO/p.sc/t.sc) move their servers. To that\nend, several preliminary results will become handy. We rst observe that the values ğ‘‘1Â•Â”Â”Â”Â•ğ‘‘ğ‘˜correlate\nwith the change of Î˜when L/a.sc/m.sc/b.sc/d.sc/a.scDC moves a server.\n21\n\nObservation 13. Letğ‘–\u0014bğ‘˜Â2c. If serverğ‘ ğ‘–moves from ğ‘toğ‘Â¸ğ‘¥,Î˜changes byÂ¹\u0000ğ‘¥Âº\u0001ğ‘‘ğ‘–.\nProof. Assume w.l.o.g. that ğ‘¥=\u00001, that is, the server ğ‘ ğ‘–moves one unit to the left. Consider servers ğ‘ ğ‘—Â•ğ‘ 0\nğ‘—\nsuch thatğ‘—0Â¸â„“=ğ‘–=ğ‘—\u0000â„“for some 1\u0014â„“\u0014ğ‘–\u00001. Sinceğ›¿ğ‘–ğ‘—0=ğ›¿ğ‘–ğ‘—we observe that the changes to the\ntermsğ›¿ğ‘–ğ‘—jğ‘ ğ‘–\u0000ğ‘ ğ‘—jandğ›¿ğ‘–ğ‘—0jğ‘ ğ‘–\u0000ğ‘ 0\nğ‘—jofÎ˜cancel out. Hence, as ğ‘–\u0014bğ‘˜Â2c, it suces to consider the distances\nofğ‘ ğ‘–to serversğ‘ ğ‘—withğ‘—\u00152ğ‘–. Therefore,\nÎ”Î˜=ğ‘˜âˆ‘ï¸\nğ‘—=2ğ‘–ğ›¿ğ‘–ğ‘—=(Ãğ‘˜Â2\u00002\nâ„“=ğ‘–\u000012ğœâ„“Â¸ğœğ‘˜Â2\u00001ifğ‘˜is even, andÃÂ¹ğ‘˜\u00003ÂºÂ2\nâ„“=ğ‘–\u000012ğœâ„“ ifğ‘˜is oddÂ”\nThe denition of ğœâ„“implies that this is indeed equal to ğ‘‘ğ‘–. \u0003\nNext, we give several algebraic transformations of ğ›¾.\nLemma 14. The following statements are true:\n(i)ğ›¾=2ğœ†ğ‘˜\u00001ÂÂ¹1Â¸ğœ†ğ‘˜\u00002Âº.\n(ii) For all 1\u0014ğ‘–\u0014bğ‘˜Â2c, it holdsÂ¹1Â¸ğœ†Âºğ›¾=ğœ†ğ‘–Â¸1ğ‘‘ğ‘–\u0000ğœ†ğ‘–ğ‘‘ğ‘–Â¸1.\n(iii) Ifğ‘˜is even, it holds ğ›¾=ğœ†ğ‘‘1Â¸Â¹1Â¸ğœ†ÂºÃğ‘˜Â2\nğ‘—=2Â¹\u00001Âºğ‘—\u00001ğ‘‘ğ‘—.\nProof. (i) Since\nğœ†ğ‘˜\u00001Â¹ğ›½\u00001Âº=ğœ†ğ‘˜\u00001ğ‘˜\u00001âˆ‘ï¸\nâ„“=1ğœ†\u0000â„“=ğ‘˜\u00002âˆ‘ï¸\nâ„“=0ğœ†â„“Â•\nwe conclude by the denition of ğ›¾andğ‘‘1that\nğ›¾=ğ‘‘1\nğ›½\u00001=2\nÂ¹1Â¸ğœ†ğ‘˜\u00002ÂºÂ¹ğ›½\u00001Âºğ‘˜\u00002âˆ‘ï¸\nâ„“=0ğœ†â„“=2\n1Â¸ğœ†ğ‘˜\u00002ğœ†ğ‘˜\u00001Â”\n(ii) Simplifying the right-hand side gives\nğœ†ğ‘–Â¸1ğ‘‘ğ‘–\u0000ğœ†ğ‘–ğ‘‘ğ‘–Â¸1=2ğœ†ğ‘–\n1Â¸ğœ†ğ‘˜\u00002 \nğœ†ğ‘˜\u00001\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–\u00001ğœ†â„“\u0000ğ‘˜\u00002\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–ğœ†â„“!\n=2ğœ†ğ‘–\n1Â¸ğœ†ğ‘˜\u00002 ğ‘˜\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–ğœ†â„“\u0000ğ‘˜\u00002\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–ğœ†â„“!\n=2ğœ†ğ‘–\n1Â¸ğœ†ğ‘˜\u00002\u0010\nğœ†ğ‘˜\u0000ğ‘–Â¸ğœ†ğ‘˜\u0000ğ‘–\u00001\u0011\n=2\u0012ğœ†ğ‘˜Â¸ğœ†ğ‘˜\u00001\n1Â¸ğœ†ğ‘˜\u00002\u0013\n=Â¹1Â¸ğœ†Âº\u00122ğœ†ğ‘˜\u00001\n1Â¸ğœ†ğ‘˜\u00002\u0013\nÂ”\nThen, Lemma 14(i) concludes the proof.\n(iii) Assume that ğ‘˜is even. The right-hand side is equal to\nÂ¹\u00001Âºğ‘˜Â2\u00001ğœ†ğ‘‘ğ‘˜Â2Â¸ğ‘˜Â2\u00001âˆ‘ï¸\nğ‘—=1Â¹\u00001Âºğ‘—\u00001Â¹ğœ†ğ‘‘ğ‘—\u0000ğ‘‘ğ‘—Â¸1ÂºÂ”\nBy Lemma 14(ii),\nÂ¹\u00001Âºğ‘˜Â2\u00001ğœ†ğ‘‘ğ‘˜Â2Â¸Â¹1Â¸ğœ†Âºğ‘˜Â2\u00001âˆ‘ï¸\nğ‘—=1Â¹\u00001Âºğ‘—\u00001ğ›¾\nğœ†ğ‘—Â•\n22\n\nwhich is equal to\n1\nğœ†ğ‘˜Â2\u00001 \nÂ¹\u00001Âºğ‘˜Â2\u00001ğœ†ğ‘˜Â2ğ‘‘ğ‘˜Â2Â¸ğ‘˜Â2\u00002âˆ‘ï¸\nğ‘—=0Â¹\u00001Âºğ‘˜Â2\u0000ğ‘—Â¹ğœ†ğ‘—Â¸ğœ†ğ‘—Â¸1Âºğ›¾!\nÂ”\nWe proceed by applying a telescoping sum argument. Since ğ‘˜Â2\u0000Â¹ğ‘˜Â2\u00002Âº=2, the last term of\nthe sumğœ†ğ‘˜Â2\u00001ğ›¾is positive. Similarly, the rst term ğœ†0ğ›¾has the same sign as Â¹\u00001Âºğ‘˜Â2\u00000=\u0000Â¹\u00001Âºğ‘˜Â2\u00001.\nThe remaining terms of the sum cancel out. Thus, it remains\n1\nğœ†ğ‘˜Â2\u00001\u0010\nğœ†ğ‘˜Â2\u00001ğ›¾Â¸Â¹\u00001Âºğ‘˜Â2\u00001Â¹ğœ†ğ‘˜Â2ğ‘‘ğ‘˜Â2\u0000ğ›¾Âº\u0011\nÂ”\nBy denition, ğ‘‘ğ‘˜Â2=2ğœ†ğ‘˜Â2\u00001ÂÂ¹1Â¸ğœ†ğ‘˜\u00002Âº. Hence,ğœ†ğ‘˜Â2ğ‘‘ğ‘˜Â2is equal toğ›¾by Lemma 14(i). We conclude\nthat the expression is indeed equal to ğ›¾.\n\u0003\nThese preliminary results enable us to prove two more involved observations about the weights chosen\nfor our potential function. The proofs are deferred to Appendix B. The rst observation is important for all\ncases where a request appears between two servers. Recall the denition of Î¦. Ifğ‘ ğ‘–moves with speed ğœ†\nandğ‘ ğ‘–Â¸1with speed 1, the changes to Î¨(increase or decrease) are scaled by ğ›½ğ›¾ğœ†ğœ”ğ‘–regardingğ‘ ğ‘–andğ›½ğ›¾ğœ”ğ‘–Â¸1\nregardingğ‘ ğ‘–Â¸1. Ifğ‘–is even, we can easily use the denition of ğœ”, since the denominators cancel. However,\nifğ‘–is odd, we use the following alternative representation of the ğœ”-weights.\nObservation 15. For2\u0014ğ‘–\u0014dğ‘˜Â2e,ğœ”ğ‘–is equal to\n8>>>>>> <\n>>>>>>:2ğœ†Ãğ‘–Â2\nğ‘—=1ğ‘‘2ğ‘—\u00001\u00002Ãğ‘–Â2\u00001\nğ‘—=1ğ‘‘2ğ‘—\u0000ğ‘‘ğ‘–\u0000ğ›¾\nğ›½ğ›¾ifğ‘–is even, and\n2ğœ†ÃÂ¹ğ‘–\u00001ÂºÂ2\nğ‘—=1ğ‘‘2ğ‘—\u00001\u00002ÃÂ¹ğ‘–\u00001ÂºÂ2\nğ‘—=1ğ‘‘2ğ‘—Â¸ğœ†ğ‘‘ğ‘–Â¸ğœ†ğ›¾\nğ›½ğ›¾ğœ†ifğ‘–is odd.\nProof. We rst note that for every 1\u0014ğ‘—\u0014bğ‘˜Â2c\u00001, applying Lemma 14(ii) with ğ‘—andğ‘—Â¸1yields\nğœ†ğ‘‘ğ‘—\u0000ğ‘‘ğ‘—Â¸1=1Â¸ğœ†\nğœ†ğ‘—ğ›¾=ğœ†2ğ‘‘ğ‘—Â¸1\u0000ğœ†ğ‘‘ğ‘—Â¸2Â” (1)\nWe now prove the statement separately for all even and all odd values of 2\u0014ğ‘–\u0014dğ‘˜Â2eby induction.\nAs induction base for the even case, we rst prove the claim for ğ‘–=2. Indeed,\nğœ”2=ğœ†ğ‘‘2Â¸Â¹2Â¸ğœ†Âºğ›¾\nğ›½ğ›¾ğœ†=ğ‘‘2Â¸Â¹2Â¸ğœ†Âºğ›¾Âğœ†\nğ›½ğ›¾=2ğœ†ğ‘‘1\u0000ğ‘‘2\u0000ğ›¾\nğ›½ğ›¾Â”\nNote that the last equality derives from Lemma 14(ii). Now assume that ğ‘–Â¡2is even. The induction\nhypothesis for ğ‘–\u00002yields in this case\nğ›½ğ›¾ğœ†\u0001ğœ”ğ‘–\u00002=2ğœ†2ğ‘–Â2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00001\u00002ğœ†ğ‘–Â2\u00002âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u0000ğœ†ğ‘‘ğ‘–\u00002\u0000ğœ†ğ›¾Â” (2)\n23\n\nWe want to prove that ğ›½ğ›¾ğœ†\u0001ğœ”ğ‘–is equal to\n2ğœ†2ğ‘–Â2âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00001\u00002ğœ†ğ‘–Â2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u0000ğœ†ğ‘‘ğ‘–\u0000ğœ†ğ›¾Â•\nwhich can be rearranged to\n2ğœ†2ğ‘–Â2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00001\u00002ğœ†ğ‘–Â2\u00002âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u0000ğœ†ğ‘‘ğ‘–\u00002\u0000ğœ†ğ›¾\u0000ğœ†ğ‘‘ğ‘–\u0000ğœ†ğ‘‘ğ‘–\u00002Â¸2ğœ†2ğ‘‘ğ‘–\u00001Â”\nReplacing the right side of (2) in the above expression yields\nğ›½ğ›¾ğœ†\u0001ğœ”ğ‘–\u00002\u0000ğœ†ğ‘‘ğ‘–\u0000ğœ†ğ‘‘ğ‘–\u00002Â¸2ğœ†2ğ‘‘ğ‘–\u00001Â”\nSince (1) gives 2Â¹1Â¸ğœ†2Âºğ‘‘ğ‘–\u00001=2ğœ†Â¹ğ‘‘ğ‘–\u00002Â¸ğ‘‘ğ‘–Âº, and by the denition of ğœ”ğ‘–\u00002, this can be rewritten to\n2ğœ†ğ‘–Â2\u00002âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00002ğ‘–Â2\u00002âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸1Â¸ğœ†ğ‘‘ğ‘–\u00002Â¸Â¹2Â¸ğœ†Âºğ›¾\u00002ğ‘‘ğ‘–\u00001Â¸ğœ†ğ‘‘ğ‘–\u00002Â¸ğœ†ğ‘‘ğ‘–\n=2ğœ†ğ‘–Â2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00002ğ‘–Â2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸1Â¸Â¹2Â¸ğœ†Âºğ›¾Â¸ğœ†ğ‘‘ğ‘–Â•\nwhich is indeed equal to ğ›½ğ›¾ğœ†\u0001ğœ”ğ‘–by denition.\nAs induction base for the odd case, we start by proving the claim for ğ‘–=3, that is\nğœ”3=2ğœ†ğ‘‘2\u0000ğ‘‘3Â¸ğ›¾\nğ›½ğ›¾=2ğœ†2ğ‘‘2\u0000ğœ†ğ‘‘3Â¸ğœ†ğ›¾\nğ›½ğ›¾ğœ†=2ğœ†ğ‘‘1\u00002ğ‘‘2Â¸ğœ†ğ‘‘3Â¸ğœ†ğ›¾\nğ›½ğ›¾ğœ†Â”\nIn the last equality we used that 2Â¹1Â¸ğœ†2Âºğ‘‘2=2ğœ†Â¹ğ‘‘1Â¸ğ‘‘3Âºby(1). Now assume that ğ‘–Â¡3is odd. By induction\nhypothesis for ğ‘–\u00002,\nğ›½ğ›¾ğœ†\u0001ğœ”ğ‘–\u00002=2ğœ†Â¹ğ‘–\u00001ÂºÂ2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00001\u00002Â¹ğ‘–\u00001ÂºÂ2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸ğœ†ğ‘‘ğ‘–\u00002Â¸ğœ†ğ›¾Â” (3)\nConsider the claimed expression for ğ›½ğ›¾ğœ†\u0001ğœ”ğ‘–, that is\n2ğœ†Â¹ğ‘–\u00001ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00001\u00002Â¹ğ‘–\u00001ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸ğœ†ğ‘‘ğ‘–Â¸ğœ†ğ›¾Â•\nwhich we can rearrange to\n2ğœ†Â¹ğ‘–\u00001ÂºÂ2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00001\u00002Â¹ğ‘–\u00001ÂºÂ2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸ğœ†ğ‘‘ğ‘–\u00002Â¸ğœ†ğ›¾Â¸ğœ†ğ‘‘ğ‘–\u00002Â¸ğœ†ğ‘‘ğ‘–\u00002ğ‘‘ğ‘–\u00001Â”\nReplacing the right side of (3) in the above expression gives\nğ›½ğ›¾ğœ†\u0001ğœ”ğ‘–\u00002Â¸ğœ†ğ‘‘ğ‘–\u00002Â¸ğœ†ğ‘‘ğ‘–\u00002ğ‘‘ğ‘–\u00001Â”\n24\n\nNoting that (1)gives 2Â¹1Â¸ğœ†2Âºğ‘‘ğ‘–\u00001=2ğœ†Â¹ğ‘‘ğ‘–\u00002Â¸ğ‘‘ğ‘–Âºyields together with the denition of ğœ”ğ‘–\u00002the equivalent\nexpression\n2ğœ†2Â¹ğ‘–\u00001ÂºÂ2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00002ğœ†Â¹ğ‘–\u00003ÂºÂ2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸1\u0000ğœ†ğ‘‘ğ‘–\u00002Â¸ğœ†ğ›¾Â¸2ğœ†2ğ‘‘ğ‘–\u00001\u0000ğœ†ğ‘‘ğ‘–\u00002\u0000ğœ†ğ‘‘ğ‘–\n=2ğœ†2Â¹ğ‘–\u00001ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00002ğœ†Â¹ğ‘–\u00003ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸1\u0000ğœ†ğ‘‘ğ‘–Â¸ğœ†ğ›¾Â”\nSince this is by denition equal to ğ›½ğ›¾ğœ†\u0001ğœ”ğ‘–, we can also conclude this case. \u0003\nThe second observation is an upper and a lower bound of the ğœ”-weights regardless of the corresponding\nserver. The lower bound is necessary to show that Î¦\u00150, while we use the upper bound to give an easy\nupper bound on the increase of the potential when the optimal solution moves, independently of its chosen\nserver.\nObservation 16. The valuesğœ”1Â•Â”Â”Â”Â•ğœ”ğ‘˜are at least 0and at most 1.\nProof. By denition, ğœ”1=ğœ”ğ‘˜=1. We now show this property for ğœ”ğ‘–depending on whether 2\u0014ğ‘–\u0014ğ‘˜\u00001\nis even or odd.\nAssume that ğ‘–is even. By denition, the numerator of ğœ”ğ‘–is equal to\n2ğœ†ğ‘–Â2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00002ğ‘–Â2\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸1Â¸ğœ†ğ‘‘ğ‘–Â¸Â¹2Â¸ğœ†Âºğ›¾Â”\nUsing the denition of ğ‘‘ğ‘–and Lemma 14(i) gives\n2\n1Â¸ğœ†ğ‘˜\u00002 \nğœ†ğ‘–Â2\u00001âˆ‘ï¸\nğ‘—=1ğ‘˜\u00001\u00002ğ‘—âˆ‘ï¸\nâ„“=2ğ‘—\u000012ğœ†â„“\u0000ğ‘–Â2\u00001âˆ‘ï¸\nğ‘—=1ğ‘˜\u00002\u00002ğ‘—âˆ‘ï¸\nâ„“=2ğ‘—2ğœ†â„“Â¸ğœ†ğ‘˜\u00001\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–\u00001ğœ†â„“!\nÂ¸Â¹2Â¸ğœ†Âºğ›¾\n=2\n1Â¸ğœ†ğ‘˜\u00002 ğ‘–Â2\u00001âˆ‘ï¸\nğ‘—=12Â¹ğœ†ğ‘˜\u00002ğ‘—Â¸ğœ†ğ‘˜\u00001\u00002ğ‘—ÂºÂ¸ğ‘˜\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–ğœ†â„“!\nÂ¸Â¹2Â¸ğœ†Âºğ›¾\n=2\n1Â¸ğœ†ğ‘˜\u00002 ğ‘˜\u00002âˆ‘ï¸\nâ„“=ğ‘˜\u0000ğ‘–Â¸12ğœ†â„“Â¸ğ‘˜\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–ğœ†â„“!\nÂ¸Â¹2Â¸ğœ†Âºğ›¾\n=2\n1Â¸ğœ†ğ‘˜\u00002 ğ‘˜\u00001âˆ‘ï¸\nâ„“=ğ‘˜\u0000ğ‘–Â¸12ğœ†â„“Â¸ğ‘˜\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–ğœ†â„“!\nÂ¸ğœ†ğ›¾\nSinceğ›½ğ›¾ğœ†=ğœ†ğ›¾Â¸ğœ†ğ‘‘1\u00150, we conclude that ğœ”ğ‘–\u00150. Further, using the fact thatÃğ‘˜\u00001\nâ„“=ğ‘˜\u0000ğ‘–Â¸1ğœ†â„“\u0014Ãğ‘–\u00001\nâ„“=1ğœ†â„“yields\n2\n1Â¸ğœ†ğ‘˜\u00002 ğ‘˜\u00001âˆ‘ï¸\nâ„“=ğ‘˜\u0000ğ‘–Â¸12ğœ†â„“Â¸ğ‘˜\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–ğœ†â„“!\nÂ¸ğœ†ğ›¾\u00142\n1Â¸ğœ†ğ‘˜\u00002ğ‘˜\u00001âˆ‘ï¸\nâ„“=1ğœ†â„“Â¸ğœ†ğ›¾=ğœ†ğ‘‘1Â¸ğœ†ğ›¾Â•\nand we conclude that ğœ”ğ‘–\u00141.\n25\n\nAssume that ğ‘–is odd. By denition, the numerator of ğœ”ğ‘–is equal to\n2ğœ†Â¹ğ‘–\u00001ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00002Â¹ğ‘–\u00003ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸1\u0000ğ‘‘ğ‘–Â¸ğ›¾Â”\nUsing denitions gives\n2\n1Â¸ğœ†ğ‘˜\u00002 \nğœ†Â¹ğ‘–\u00001ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘˜\u00001\u00002ğ‘—âˆ‘ï¸\nâ„“=2ğ‘—\u000012ğœ†â„“\u0000Â¹ğ‘–\u00003ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘˜\u00002\u00002ğ‘—âˆ‘ï¸\nâ„“=2ğ‘—2ğœ†â„“\u0000ğ‘˜\u00001\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–\u00001ğœ†â„“!\nÂ¸ğ›¾\n=2\n1Â¸ğœ†ğ‘˜\u00002 Â¹ğ‘–\u00001ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘˜\u00002ğ‘—âˆ‘ï¸\nâ„“=2ğ‘—2ğœ†â„“\u0000Â¹ğ‘–\u00003ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘˜\u00002\u00002ğ‘—âˆ‘ï¸\nâ„“=2ğ‘—2ğœ†â„“\u0000ğ‘˜\u00001\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–\u00001ğœ†â„“!\nÂ¸ğ›¾\n=2\n1Â¸ğœ†ğ‘˜\u00002 Â¹ğ‘–\u00001ÂºÂ2âˆ‘ï¸\nğ‘—=12Â¹ğœ†ğ‘˜\u00002ğ‘—Â¸ğœ†ğ‘˜\u00001\u00002ğ‘—ÂºÂ¸ğ‘˜\u00001\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–\u000012ğœ†â„“\u0000ğ‘˜\u00001\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–\u00001ğœ†â„“!\nÂ¸ğ›¾\n=2\n1Â¸ğœ†ğ‘˜\u00002 ğ‘˜\u00002âˆ‘ï¸\nâ„“=ğ‘˜\u0000ğ‘–2ğœ†â„“Â¸ğ‘˜\u00001\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–\u00001ğœ†â„“!\nÂ¸ğ›¾Â”\nSinceğ›½ğ›¾=ğ›¾Â¸ğ‘‘1\u00150, we conclude that ğœ”ğ‘–\u00150. Further, using the fact thatÃğ‘˜\u00002\nâ„“=ğ‘˜\u0000ğ‘–ğœ†â„“\u0014Ãğ‘–\u00002\nâ„“=0ğœ†â„“yields\n2\n1Â¸ğœ†ğ‘˜\u00002 ğ‘˜\u00002âˆ‘ï¸\nâ„“=ğ‘˜\u0000ğ‘–2ğœ†â„“Â¸ğ‘˜\u00001\u0000ğ‘–âˆ‘ï¸\nâ„“=ğ‘–\u00001ğœ†â„“!\nÂ¸ğ›¾\u00142\n1Â¸ğœ†ğ‘˜\u00002ğ‘˜\u00002âˆ‘ï¸\nâ„“=0ğœ†â„“Â¸ğ›¾=ğ‘‘1Â¸ğ›¾Â•\nand we conclude that ğœ”ğ‘–\u00141. \u0003\nBefore proving formally our robustness bound by exhaustively reviewing all possible moves and\nbounding the corresponding changes of Î¦, we give some intuition.\nWe choose the scaling parameter ğ›¾such that the decrease of Î¦exactly matches the required lower\nbound for the case where the request is outside the convex hull of L/a.sc/m.sc/b.sc/d.sc/a.scDC â€™s servers. The remaining\ncases are split among the possible locations where a request can appear between two servers of L/a.sc/m.sc/b.sc/d.sc/a.scDC ,\nand we show in each case that Î¦decreases enough. The denition of the ğœ”values ensures that a wrong\nprediction gives a tight bound on the decrease of Î¦forL/a.sc/m.sc/b.sc/d.sc/a.scDC â€™s move, while a correct prediction still\nguarantees a loose bound.\nProof of Lemma 9. Note that Observation 16 implies Î¦\u00150. Suppose that the next request arrives. First\nthe optimal solution increases due to Observation 16 the potential by at most ğ›½ğ›¾Î”O/p.sc/t.sc while L/a.sc/m.sc/b.sc/d.sc/a.scDC\nremains in its previous conguration. Second L/a.sc/m.sc/b.sc/d.sc/a.scDC moves. In the remaining proof we demonstrate\nthat the potential decreases by at least ğ›¾Î”A/l.sc/g.sc, which proves the Lemma. We look at the following set of\nexhaustive cases that occur when L/a.sc/m.sc/b.sc/d.sc/a.scDC makes its move. Assume by scaling that in each case the fast\nserver moves distance 1.\n(a)Let the request w.l.o.g. be on the left of ğ‘ 1. Hence, Î”A/l.sc/g.sc=1, andÎ˜increases by ğ‘‘1due to Observa-\ntion 13. Since ğ‘¥1cannot be on the right side of the request, the potential changes by\nÎ”Î¦=\u0000ğ›½ğ›¾Â¸ğ‘‘1=\u0000Â¹ğ‘‘1\u0000ğ›¾ÂºÂ¸ğ‘‘1=\u0000ğ›¾Î”A/l.sc/g.scÂ”\n26\n\nThe remaining cases tackle the situations where the request is located between the two servers ğ‘ ğ‘–andğ‘ ğ‘–Â¸1.\nWithout loss of generality we only look at those cases where ğ‘–\u0014 bğ‘˜Â2c, since the others hold by the\nsymmetry of the line and by the symmetry of Î¦.\n(b)Let1\u0014ğ‘–\u0014dğ‘˜Â2e\u00001and suppose that ğ‘ ğ‘–is predicted while the optimal solution serves the request\nwithğ‘¥ğ‘—for someğ‘—Â¡ğ‘–. Note that Î”A/l.sc/g.sc=1Â¸ğœ†. The change of Î¦is at most\nÎ”Î¦\u0014ğ›½ğ›¾Â¹ğœ”ğ‘–\u0000ğœ†ğœ”ğ‘–Â¸1Âº\u0000ğ‘‘ğ‘–Â¸ğœ†ğ‘‘ğ‘–Â¸1Â”\nBy using the denition of ğœ”ğ‘–ifğ‘–is odd and Observation 15 if ğ‘–is even, this is equal to\nğ‘‘ğ‘–\u0000ğœ†ğ‘‘ğ‘–Â¸1\u0000Â¹1Â¸ğœ†Âºğ›¾\u0000ğ‘‘ğ‘–Â¸ğœ†ğ‘‘ğ‘–Â¸1=\u0000ğ›¾Â¹1Â¸ğœ†Âº=\u0000ğ›¾Î”A/l.sc/g.scÂ”\n(c)Let1\u0014ğ‘–\u0014dğ‘˜Â2e\u00001and suppose that ğ‘ ğ‘–Â¸1is predicted while the optimal solution serves the request\nwithğ‘¥ğ‘—for someğ‘—\u0014ğ‘–. The change of Î¦is at most\nÎ”Î¦\u0014ğ›½ğ›¾Â¹ğœ”ğ‘–Â¸1\u0000ğœ†ğœ”ğ‘–Âº\u0000ğœ†ğ‘‘ğ‘–Â¸ğ‘‘ğ‘–Â¸1Â”\nBy using the denition of ğœ”ğ‘–ifğ‘–is even and Observation 15 if ğ‘–is odd, this is equal to\nğœ†ğ‘‘ğ‘–\u0000ğ‘‘ğ‘–Â¸1\u0000Â¹1Â¸ğœ†Âºğ›¾\u0000ğœ†ğ‘‘ğ‘–Â¸ğ‘‘ğ‘–Â¸1=\u0000ğ›¾Â¹1Â¸ğœ†Âº=\u0000ğ›¾Î”A/l.sc/g.scÂ”\n(d)Let1\u0014ğ‘–\u0014dğ‘˜Â2e\u00001and suppose that ğ‘ ğ‘–is predicted while the optimal solution serves the request\nwithğ‘¥ğ‘—for someğ‘—\u0014ğ‘–. The change of Î¦is at most\nÎ”Î¦\u0014ğ›½ğ›¾Â¹ğœ†ğœ”ğ‘–Â¸1\u0000ğœ”ğ‘–Âº\u0000ğ‘‘ğ‘–Â¸ğœ†ğ‘‘ğ‘–Â¸1Â”\nBy using the denition of ğœ”ğ‘–ifğ‘–is odd and Observation 15 if ğ‘–is even, this is equal to\n\u0000ğ‘‘ğ‘–Â¸ğœ†ğ‘‘ğ‘–Â¸1Â¸Â¹1Â¸ğœ†Âºğ›¾\u0000ğ‘‘ğ‘–Â¸ğœ†ğ‘‘ğ‘–Â¸1\n=\u0000ğ›¾Â¹1Â¸ğœ†ÂºÂ¸2Â¹ğœ†ğ‘‘ğ‘–Â¸1\u0000ğ‘‘ğ‘–Â¸Â¹1Â¸ğœ†Âºğ›¾Âº\n=\u0000ğ›¾Â¹1Â¸ğœ†ÂºÂ¸2Â¹ğœ†ğ‘‘ğ‘–Â¸1Â¸ğœ†2ğ‘‘ğ‘–\u0000ğœ†2ğ‘‘ğ‘–\u0000ğ‘‘ğ‘–Â¸Â¹1Â¸ğœ†Âºğ›¾ÂºÂ”\nApplying Lemma 14(ii) yields\n\u0000ğ›¾Â¹1Â¸ğœ†ÂºÂ¸2\u0010\nÂ¹ğœ†2\u00001Âºğ‘‘ğ‘–\u0000Â¹1Â¸ğœ†Âºğ›¾\nğœ†ğ‘–\u00001Â¸Â¹1Â¸ğœ†Âºğ›¾\u0011\n\u0014\u0000ğ›¾Â¹1Â¸ğœ†Âº=\u0000ğ›¾Î”A/l.sc/g.scÂ”\n(e)Let1\u0014ğ‘–\u0014dğ‘˜Â2e\u00001and suppose that ğ‘ ğ‘–Â¸1is predicted while the optimal solution serves the request\nwithğ‘¥ğ‘—for someğ‘—Â¡ğ‘–. The change of Î¦is at most\nÎ”Î¦\u0014ğ›½ğ›¾Â¹ğœ†ğœ”ğ‘–\u0000ğœ”ğ‘–Â¸1Âº\u0000ğœ†ğ‘‘ğ‘–Â¸ğ‘‘ğ‘–Â¸1Â”\nBy using the denition of ğœ”ğ‘–ifğ‘–is even and Observation 15 if ğ‘–is odd, we can conclude\n\u0000ğœ†ğ‘‘ğ‘–Â¸ğ‘‘ğ‘–Â¸1Â¸Â¹1Â¸ğœ†Âºğ›¾\u0000ğœ†ğ‘‘ğ‘–Â¸ğ‘‘ğ‘–Â¸1\n=\u0000Â¹1Â¸ğœ†Âºğ›¾Â¸2Â¹Â¹1Â¸ğœ†Âºğ›¾\u0000ğœ†ğ‘‘ğ‘–Â¸ğ‘‘ğ‘–Â¸1ÂºÂ”\nUsing Lemma 14(ii) gives\n\u0000Â¹1Â¸ğœ†Âºğ›¾Â¸2\u0010\nÂ¹1Â¸ğœ†Âºğ›¾\u0000Â¹1Â¸ğœ†Âºğ›¾\nğœ†ğ‘–\u0011\n\u0014\u0000ğ›¾Â¹1Â¸ğœ†Âº=\u0000ğ›¾Î”A/l.sc/g.scÂ”\n27\n\nIfğ‘˜is even, there are two additional cases which occur when the request is located between the two middle\nserversğ‘ ğ‘˜Â2andğ‘ ğ‘˜Â2Â¸1. Note that these cases cannot be covered by the previous ones, since the ğœ”-weights\nof the servers on both sides of the request are equal.\n(f)Let the request be between ğ‘ ğ‘˜Â2andğ‘ ğ‘˜Â2Â¸1, and suppose that ğ‘ ğ‘˜Â2is predicted while the optimal\nsolution serves ğ‘Ÿwithğ‘¥ğ‘—for someğ‘—Â¡ğ‘˜Â2. The change of Î¦is at most\nÎ”Î¦\u0014ğ›½ğ›¾Â¹ğœ”ğ‘˜Â2\u0000ğœ†ğœ”ğ‘˜Â2Âº\u0000ğœ†ğ‘‘ğ‘˜Â2\u0000ğ‘‘ğ‘˜Â2Â” (4)\nFor the rest of this case, we distinguish two cases according to the parity of ğ‘˜Â2, and show that Î”Î¦\u0014\n\u0000ğ›¾Î”A/l.sc/g.sc.\nIfğ‘˜Â2is even , (4) is by Observation 15 and the denition of ğœ”ğ‘˜Â2equal to\n2ğœ†ğ‘˜Â4âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00001\u00002ğ‘˜Â4\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u0000ğ‘‘ğ‘˜Â2\u0000ğ›¾\n\u0000 \n2ğœ†ğ‘˜Â4\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00002ğ‘˜Â4\u00001âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸1Â¸ğœ†ğ‘‘ğ‘˜Â2Â¸Â¹2Â¸ğœ†Âºğ›¾!\n\u0000ğœ†ğ‘‘ğ‘˜Â2\u0000ğ‘‘ğ‘˜Â2Â”\nNoting that 2ğœ†Ãğ‘˜Â4\nğ‘—=1ğ‘‘2ğ‘—\u00001=2ğœ†ğ‘‘1Â¸2ğœ†Ãğ‘˜Â4\u00001\nğ‘—=1ğ‘‘2ğ‘—Â¸1gives\n\u0000Â¹3Â¸ğœ†Âºğ›¾Â¸2 \nğœ†ğ‘‘1Â¸Â¹1Â¸ğœ†Âºğ‘˜Â2âˆ‘ï¸\nğ‘—=2Â¹\u00001Âºğ‘—\u00001ğ‘‘ğ‘—!\nÂ”\nWe can conclude that this is equal to \u0000ğ›¾Â¹1Â¸ğœ†Âºby Lemma 14(iii).\nSimilarly, ifğ‘˜Â2is odd , (4) is by Observation 15 and the denition of ğœ”ğ‘˜Â2equal to\n2ğœ†Â¹ğ‘˜Â2\u00001ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00002Â¹ğ‘˜Â2\u00003ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸1\u0000ğ‘‘ğ‘˜Â2Â¸ğ›¾\n\u0000 \n2ğœ†Â¹ğ‘˜Â2\u00001ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—\u00001\u00002Â¹ğ‘˜Â2\u00001ÂºÂ2âˆ‘ï¸\nğ‘—=1ğ‘‘2ğ‘—Â¸ğœ†ğ‘‘ğ‘˜Â2Â¸ğœ†ğ›¾!\n\u0000ğœ†ğ‘‘ğ‘˜Â2\u0000ğ‘‘ğ‘˜Â2Â”\nNoting that 2ğœ†ÃÂ¹ğ‘˜Â2\u00001ÂºÂ2\nğ‘—=1ğ‘‘2ğ‘—\u00001=2ğœ†ğ‘‘1Â¸2ğœ†ÃÂ¹ğ‘˜Â2\u00003ÂºÂ2\nğ‘—=1ğ‘‘2ğ‘—Â¸1yields\nğ›¾\u0000ğœ†ğ›¾Â¸2 \n\u0000ğœ†ğ‘‘1Â¸Â¹1Â¸ğœ†Âºğ‘˜Â2âˆ‘ï¸\nğ‘—=2Â¹\u00001Âºğ‘—ğ‘‘ğ‘—!\n=ğ›¾\u0000ğœ†ğ›¾\u00002 \nğœ†ğ‘‘1Â¸Â¹1Â¸ğœ†Âºğ‘˜Â2âˆ‘ï¸\nğ‘—=2Â¹\u00001Âºğ‘—\u00001ğ‘‘ğ‘—!\nÂ”\nThis is equal to\u0000ğ›¾Â¹1Â¸ğœ†Âºby Lemma 14(iii).\n(g)Let the request be between ğ‘ ğ‘˜Â2andğ‘ ğ‘˜Â2Â¸1, and suppose that ğ‘ ğ‘˜Â2is predicted while the optimal\nsolution serves ğ‘Ÿwithğ‘¥ğ‘—for someğ‘—\u0014ğ‘˜Â2. The change of Î¦is at most\nÎ”Î¦\u0014ğ›½ğ›¾Â¹ğœ†ğœ”ğ‘˜Â2\u0000ğœ”ğ‘˜Â2Âº\u0000ğœ†ğ‘‘ğ‘˜Â2\u0000ğ‘‘ğ‘˜Â2Â•\nwhich is bounded from above by the previous case. Hence, Î”Î¦\u0014\u0000ğ›¾Î”A/l.sc/g.sc. \u0003\n28\n\nğ‘1 ğ‘2 ğ‘3 ğ‘4 ğ‘‘1 ğ‘‘2 ğ‘‘3\nforce toÂ¹ğ‘1Â•ğ‘2Â•ğ‘4Âº\nğ‘3\nğ‘2\nğ‘3\nğ‘2\nğ‘1\nğ‘2\nğ‘3\nğ‘2\nğ‘3\nFigure 4: Instance ğ¼forğ‘˜=3. The prediction is drawn green. The blue moves indicate an exemplary schedule of an\nalgorithm.\nC Proofs for Section 5\nThis section is dedicated to the proof of Theorem 2, which we restate below. The proof is a generalization\nof the one proposed in Section 3.2 for two servers. However, for proving the general case we need a more\nsophisticated construction rule and a more involved argumentation.\nTheorem 2. Letğœ†2Â¹0Â•1Â¼,ğœŒÂ¹ğ‘˜Âº=Ãğ‘˜\u00001\nğ‘–=0ğœ†ğ‘–andğ›½Â¹ğ‘˜Âº=Ãğ‘˜\u00001\nğ‘–=0ğœ†\u0000ğ‘–. LetAbe a learning-augmented locally-\nconsistent and memory-constrained deterministic online algorithm for the ğ‘˜-server problem on the line. Then,\nifAisğœŒÂ¹ğ‘˜Âº-consistent, it is at least ğ›½Â¹ğ‘˜Âº-robust.\nLetğœ†2Â¹0Â•1Â¼. Recall that ğœŒÂ¹ğ‘˜Âº=Ãğ‘˜\u00001\nğ‘–=0ğœ†ğ‘–. LetAbe ağœŒÂ¹ğ‘–Âº-consistent locally-consistent and memory-\nconstrained deterministic online algorithm for the ğ‘–-server problem on the line, for all ğ‘–\u0014ğ‘˜. The objective\nis to show thatAis then at least ğ›½Â¹ğ‘˜Âº-robust, with ğ›½Â¹ğ‘˜Âº=Ãğ‘˜\u00001\nğ‘–=0ğœ†\u0000ğ‘–.\nLetğ‘1\u0014Â”Â”Â”\u0014ğ‘ğ‘˜Â¸1be points on the line with inter-distances ğ‘‘1Â•Â”Â”Â”Â•ğ‘‘ğ‘˜, where for 1\u0014ğ‘–\u0014ğ‘˜\u00001,ğ‘‘ğ‘–=1,\nandğ‘‘ğ‘˜Â¡1is arbitrarily large. See Figure 4 for an illustration. We also dene an arbitrarily small constant\nğœ€Â¡0and say that a server covers a pointğ‘ğ‘–if it is at most a distance ğœ€away from it. We refer to smaller\npositions on the line as left. Letğ‘ƒ:=fğ‘1Â•Â”Â”Â”Â•ğ‘ğ‘˜g. In the following we inductively construct an instance. In\ntheir initial conguration, i.e.at timeğ‘¡=0, theğ‘˜servers,ğ‘ 1Â•Â”Â”Â”Â•ğ‘ ğ‘˜, are located at ğ‘1Â•Â”Â”Â”Â•ğ‘ğ‘˜. We assume that\nservers never overpass each other to simplify the notations. Then, we force the servers to ğ‘1Â•Â”Â”Â”Â•ğ‘ğ‘˜\u00001Â•ğ‘ğ‘˜Â¸1\n(see the memory-constrained denition). The instance terminates when Aplacesğ‘ ğ‘˜to coverğ‘ğ‘˜. At any\ntimeğ‘¡Â¡0, the next requested point ğ‘Ÿğ‘¡is the leftmost point ( i.e.the point with the smallest index) which\nisnot covered by any server ofA. Ifğ‘1is not covered and ğ‘ 1is on the left of ğ‘2\u0000ğœ€thenğ‘Ÿğ‘¡is the second\nleftmost uncovered point. If ğ‘1is not covered and ğ‘ 1coversğ‘2, but did not serve it since leaving ğ‘1, thenğ‘Ÿğ‘¡\nisğ‘2andğ‘Ÿğ‘¡Â¸1(next in time) is ğ‘1. At any time ğ‘¡Â¡0, we denote the instance composed of ğ‘Ÿ1Â•Â”Â”Â”Â•ğ‘Ÿğ‘¡byğ¼ğ‘¡.\nAt every point in time, we give Athe prediction that suggests serving a request at some point ğ‘ğ‘–with\nthe serverğ‘ ğ‘–. An exception is the rst request, where ğ‘ ğ‘˜is predicted (note that the rst request is always\nlocated atğ‘ğ‘˜Â¸1). We now show that this construction rule is well-dened.\nLemma 17. The construction ends after a nite number of requests.\nProof. For the sake of contradiction, assume that the construction does not end after a nite number of\nrequests. Hence, every request ğ‘Ÿexcept the rst one must be in the set ğ‘ƒ, and by construction, no server\n29\n\ncoveredğ‘Ÿin the previous conguration. Thus, the server that serves ğ‘Ÿmust have been moved with some\ncost at least ğœ€, which implies that Ahas unbounded cost.\nNow consider anyinnite instance ğ¼ğ‘ƒwhich starts with a request at ğ‘ğ‘˜Â¸1followed by requests contained\ninğ‘ƒ. An optimal solution for ğ¼ğ‘ƒis to serve the rst request with ğ‘ ğ‘˜and then to move it immediately back\nto the setğ‘ƒ, such that every point in ğ‘ƒcontains a server. Hence, the total cost of an optimal solution is\nconstant. Therefore the consistency of Awould be innite, as the prediction given to Acorresponds to\nthe optimal solution, which is a contradiction. \u0003\nDue to Lemma 17, we assume for the rest of this section that the construction ends after ğ‘›steps, and\nwe deneğ¼=ğ¼ğ‘›, see Figure 4.\nWe rst focus on the cost that Acharges for ğ¼. Letğ·ğ‘–be the distance traveled by the server ğ‘ ğ‘–inA.\nUsing the locally-consistent denition, we show the following relation between ğ·ğ‘–â€™s:\nLemma 18. For allğ‘–\u0014ğ‘˜, forğœ€small enough, we have ğ·1\u0014ğœ†ğ‘–\u00001ğ·ğ‘–Â¸ğ‘‚ğ‘˜Â¹ğœ€ğ·ğ‘–Â¸ğ‘‘1Âº, where the notation ğ‘‚ğ‘˜Â¹\u0001Âº\ntreatsğ‘˜as a constant.\nProof. Letğ‘–2f2Â•3Â•Â”Â”Â”Â•ğ‘˜gand assume by induction that the relation is true for all ğ‘—ÂŸğ‘–. Note that it is\ntrivial forğ‘–=1.\nWe denote byAğ‘–Â¹ğ¼Âºthe cost ofArestricted to the ğ‘–leftmost servers. Consider the ğ‘–leftmost servers\nand we apply the locally-consistent property of Aon these servers. Let ğ¼0be the corresponding instance\nonğ‘–servers, where requests not served by fğ‘ 1Â•Â”Â”Â”Â•ğ‘ ğ‘–gare replaced by requests to the new position of ğ‘ ğ‘–.\nConsider the algorithm F/t.scPservingğ¼0following the initial predictions as in the locally-consistent\ndenition. There are two types of requests: a point ğ‘â„“forâ„“ÂŸğ‘–is served at no cost by ğ‘ â„“, and any other\nrequest is served by ğ‘ ğ‘–. The objective is to upper bound F/t.scPÂ¹ğ¼0Âºbyğ·ğ‘–plus negligible terms. Consider all\nrequests dierent from ğ‘ğ‘–served byğ‘ ğ‘–inF/t.scP, and letğ‘Ÿ1andğ‘Ÿ2be two consecutive requests in this set (there\ncan be other requests not belonging to this set between ğ‘Ÿ1andğ‘Ÿ2). These requests are based on requests\nofğ¼outside offğ‘1Â”Â”Â”ğ‘ğ‘–g, which means that each of these points (except ğ‘1) is covered by a server of A\nbefore the request, and that ğ‘ ğ‘–also went to ğ‘Ÿ1andğ‘Ÿ2inA, at the time at which they are requested in ğ¼0. A\ntechnical diculty here is that ğ‘ ğ‘–does not need to be exactly atğ‘ğ‘–before these requests: it can be within a\ndistance ofğœ€. There are several cases to analyze.\nâ€¢Ifğ‘ğ‘–is not requested between ğ‘Ÿ1andğ‘Ÿ2, then F/t.scPpays the shortest path between ğ‘Ÿ1andğ‘Ÿ2, so at most\nhow muchğ‘ ğ‘–travels inA.\nâ€¢Ifğ‘ ğ‘–goes onğ‘ğ‘–betweenğ‘Ÿ1andğ‘Ÿ2inA, then F/t.scPalso pays at most how much ğ‘ ğ‘–travels inA.\nâ€¢Ifğ‘ğ‘–is requested between ğ‘Ÿ1andğ‘Ÿ2andğ‘ ğ‘–does not go on ğ‘ğ‘–inA, we focus on the subinstance ğ¼\u0003\nstarting from the request ğ‘Ÿ1and ending just before ğ‘Ÿ2is requested. Let F/t.scPÂ¹ğ¼\u0003Âº,Ağ‘–Â¹ğ¼\u0003Âºandğ·\u0003\nâ„“be the\nrestrictions of F/t.scPÂ¹ğ¼Âº,Ağ‘–Â¹ğ¼Âºandğ·â„“toğ¼\u0003. Note that F/t.scPÂ¹ğ¼\u0003Âº\u0014ğ·\u0003\nğ‘–Â¸ğœ€asF/t.scPmovesğ‘ ğ‘–toğ‘Ÿ1then back\ntoğ‘ğ‘–whereasAneeds only to move ğ‘ ğ‘–toğ‘Ÿ1and then near ğ‘ğ‘–. The objective is now to show that this\nadditive term ğœ€is negligible compared to Ağ‘–Â¹ğ¼\u0003Âº, for which we need a further case distinction.\nâ€“Ifğ‘Ÿ1is at least a distancepğœ€away from ğ‘ğ‘–, then F/t.scPÂ¹ğ¼\u0003Âºmovesğ‘ ğ‘–by a distance which is\nclose toğ·\u0003\nğ‘–. Specically, we have ğ·\u0003\nğ‘–\u00152pğœ€\u00002ğœ€\u0015pğœ€forğœ€small enough, and the relation\nF/t.scPÂ¹ğ¼\u0003Âº\u0014ğ·\u0003\nğ‘–Â¸ğœ€implies F/t.scPÂ¹ğ¼\u0003Âº\u0014Â¹ 1Â¸pğœ€Âºğ·\u0003\nğ‘–.\nâ€“Ifğ‘Ÿ1is at most a distancepğœ€away fromğ‘ğ‘–, we get F/t.scPÂ¹ğ¼\u0003Âº\u00142pğœ€Â¸ğœ€and we distinguish two\ncases which are slightly dierent if ğ‘–=2orğ‘–Â¡2.\n30\n\nâˆ—Ifğ‘–Â¡2then the cost ofAğ‘–onğ¼\u0003is at leastğ´ğ‘–Â¹ğ¼\u0003Âº\u0015ğ·\u0003\nğ‘–\u00001Â¡ğ‘‘ğ‘–\u00001\u0000ğœ€=1\u0000ğœ€asğ‘ğ‘–must have\nbeen served by ğ‘ ğ‘–\u00001(previously located near ğ‘ğ‘–\u00001) if it was not served by ğ‘ ğ‘–. We therefore\nobtain F/t.scPÂ¹ğ¼\u0003Âº\u00143pğœ€\u0001ğ·\u0003\nğ‘–\u00001.\nâˆ—Ifğ‘–=2, the dierence is that ğ‘ 1may be initially located anywhere between ğ‘1andğ‘2.ğ‘ 1\nservesğ‘ğ‘–=ğ‘2when it is requested (as this case assumes ğ‘ 2does not serve ğ‘2inğ¼\u0003), and\nthen must serve ğ‘1by the denition of the instance ğ¼. Therefore, the cost of Ağ‘–onğ¼\u0003is at\nleastğ´ğ‘–Â¹ğ¼\u0003Âº\u0015ğ·\u0003\n1\u0015ğ‘‘1=1. We thus obtain F/t.scPÂ¹ğ¼\u0003Âº\u00143pğœ€\u0001ğ·\u0003\n1.\nSumming over all subinstances, we obtain the following inequality:\nF/t.scPÂ¹ğ¼0Âº\u0014Â¹ 1Â¸pğœ€Âºğ·ğ‘–Â¸3pğœ€\u0001ğ‘–\u00001âˆ‘ï¸\nâ„“=1ğ·â„“\u0014ğ·ğ‘–Â¸3pğœ€\u0001Ağ‘–Â¹ğ¼ÂºÂ”\nAs the initial and nal congurations are identical up to a distance of ğ‘‘1forğ‘ 1andğœ€for other servers,\nthe locally-consistent property for ğ¼0yields\nAğ‘–Â¹ğ¼Âº\u0014ğœŒÂ¹ğ‘–Âºğ·ğ‘–Â¸3pğœ€ğœŒÂ¹ğ‘–Âº\u0001Ağ‘–Â¹ğ¼ÂºÂ¸ğ‘‚Â¹ğœ€ğ‘˜2Â¸ğ‘‘1ğ‘˜ÂºÂ”\nForğœ€small enough, we have 3pğœ€ğœŒÂ¹ğ‘–ÂºÂŸ1Â2, which implies that Ağ‘–Â¹ğ¼Âº\u00142ğœŒÂ¹ğ‘–Âºğ·ğ‘–Â¸ğ‘‚Â¹ğœ€ğ‘˜2Â¸ğ‘‘1ğ‘˜Âº. Using\nthis new bound on Ağ‘–Â¹ğ¼Âºon the right-hand side of the above inequality leads to the following:\nAğ‘–Â¹ğ¼Âº\u0014ğœŒÂ¹ğ‘–Âºğ·ğ‘–Â¸ğ‘‚Â¹ğœ€ğ‘˜2Â¸ğ‘‘1ğ‘˜Â¸pğœ€ğœŒÂ¹ğ‘–Âº2ğ·ğ‘–Âº\nğ‘–âˆ‘ï¸\nâ„“=1ğ·â„“\u0014ğ·ğ‘–Â¸Â¹ğœŒÂ¹ğ‘–Âº\u00001Âºğ·ğ‘–Â¸ğ‘‚Â¹ğœ€ğ‘˜2Â¸ğ‘‘1ğ‘˜Â¸pğœ€ğœŒÂ¹ğ‘–Âº2ğ·ğ‘–Âº\nğ‘–\u00001âˆ‘ï¸\nâ„“=1ğ·â„“\u0014Â¹ğœŒÂ¹ğ‘–Âº\u00001Âºğ·ğ‘–Â¸ğ‘‚Â¹ğœ€ğ‘˜2Â¸ğ‘‘1ğ‘˜Â¸pğœ€ğœŒÂ¹ğ‘–Âº2ğ·ğ‘–Âº\nWe now use the induction hypothesis to lower bound ğ·â„“byğœ†1\u0000â„“ğ·1Â¸ğ‘‚ğ‘˜Â¹ğœ€ğ·ğ‘–Â¸ğ‘‘1Âºand replace ğœŒÂ¹ğ‘–Âº\nby its expression, before dividing all sides byÃğ‘–\u00002\nâ„“=0ğœ†\u0000â„“. We use the notation ğ‘‚ğ‘˜Â¹\u0001Âºto avoid detailing the\nirrelevant dependencies on ğ‘˜, note thatğœŒÂ¹ğ‘–Âºdepends only on ğœ†andğ‘˜so does not appear inside the notation\nğ‘‚ğ‘˜Â¹\u0001Âº.\nğ‘–\u00001âˆ‘ï¸\nâ„“=11\nğœ†â„“\u00001ğ·1\u0014ğ‘–\u00001âˆ‘ï¸\nâ„“=1ğœ†â„“ğ·ğ‘–Â¸ğ‘‚ğ‘˜Â¹ğœ€ğ·ğ‘–Â¸ğ‘‘1Âº\nğ·1\u0014ğœ†ğ‘–\u00001ğ·ğ‘–Â¸ğ‘‚ğ‘˜Â¹ğœ€ğ·ğ‘–Â¸ğ‘‘1Âº\n\u0003\nWe build the instance ğ¼ğœ”repeating the instance ğ¼ğœ”times, starting directly by the force to ğ‘1Â•Â”Â”Â”Â•ğ‘ğ‘˜\u00001Â•ğ‘ğ‘˜Â¸1,\nsee Figure 5. The predictions for each iteration correspond to the predictions dened in instance ğ¼. We now\nbound the optimal cost for this instance.\nLemma 19. O/p.sc/t.scÂ¹ğ¼ğœ”Âº\u00142ğ‘‘ğ‘˜Â¸ğœ”\u0001Â¹ğ·1Â¸2Ãğ‘˜\u00001\nğ‘–=2ğ‘‘ğ‘–Âº.\n31\n\nğ‘1 ğ‘2 ğ‘3 ğ‘4 ğ‘‘1 ğ‘‘2 ğ‘‘3\nforce toÂ¹ğ‘1Â•ğ‘2Â•ğ‘4Âº\nğ‘3\nğ‘2\nğ‘3\nğ‘2\nğ‘1\nğ‘2\nğ‘3\nğ‘2\nğ‘3\nforce toÂ¹ğ‘1Â•ğ‘2Â•ğ‘4Âº\nğ‘3\nÂ”Â”Â”Iteration\nFigure 5: Instance ğ¼ğœ”forğ‘˜=3. The alternative (better) solution is drawn orange. The prediction and the exemplary\nmoves of the algorithm are the same as in instance ğ¼for each iteration.\nProof. Consider the following schedule for ğ¼ğœ”: at each iteration, move ğ‘˜\u00001servers toğ‘2Â•Â”Â”Â”Â•ğ‘ğ‘˜Â¸1and\nalternate between ğ‘1andğ‘2withğ‘ 1. We now analyze how many alternations we need to do in each iteration.\nBy denition of the instance, ğ‘1is only requested if ğ‘ 1has servedğ‘2since it last left ğ‘1. Therefore, the\ndistance traveled by ğ‘ 1equalsğ·1. At the end of the iteration, we move back the ğ‘˜\u00002middle servers, giving\nthe target cost. \u0003\nProof of Theorem 2. AsAis memory-constrained, its behavior on each iteration of ğ¼is identical, ğ‘ ğ‘˜is at\nğ‘ğ‘˜initially, then the ğ‘˜servers are forced to the points ğ‘1Â•Â”Â”Â”Â•ğ‘ğ‘˜\u00001Â•ğ‘ğ‘˜Â¸1before continuing the requests.\nThereforeAmust pay at least ğ‘‘ğ‘˜to serve the rst force operation, and then must make the same decisions\nin all iterations.\nUsing Lemma 19, the competitive ratio of Afor instance ğ¼ğœ”is therefore at least\nAÂ¹ğ¼ğœ”Âº\nO/p.sc/t.scÂ¹ğ¼ğœ”Âº\u0015ğœ”\u0001Ãğ‘˜\nğ‘–=1ğ·ğ‘–\n2ğ‘‘ğ‘˜Â¸ğœ”\u0001Â¹ğ·1Â¸2Ãğ‘˜\u00001\nğ‘–=2ğ‘‘ğ‘–Âº\nğœ”!1\u0000\u0000\u0000\u0000!Ãğ‘˜\nğ‘–=1ğ·ğ‘–\nğ·1Â¸2Ãğ‘˜\u00001\nğ‘–=2ğ‘‘ğ‘–Â”\nConsiderğ‘‘ğ‘˜arbitrarily large (but still small compared to ğœ”). Ifğ·1is bounded by a constant, then the\ncompetitive ratio is unbounded, so Ais not robust. Otherwise, the terms ğ‘‘ğ‘–become negligible compared\ntoğ·1, and we show that the limit of the competitive ratio is lower bounded by the desired robustness\nexpression, using Lemma 18 (which implies that ğ‘‘1is also negligible compared to any ğ·ğ‘–):\nAÂ¹ğ¼ğœ”Âº\nO/p.sc/t.scÂ¹ğ¼ğœ”Âºğ‘‘ğ‘˜!1\u0000\u0000\u0000\u0000\u0000!Ãğ‘˜\nğ‘–=1ğ·ğ‘–\nğ·1\u0015ğ‘˜\u00001âˆ‘ï¸\nğ‘–=01\nğœ†ğ‘–Â¸ğ‘‚ğ‘˜Â¹ğœ€Â¸ğ‘‘1\nğ·ğ‘–Â¸1Âºğœ€!0\u0000\u0000\u0000!ğ‘˜\u00001âˆ‘ï¸\nğ‘–=0ğœ†\u0000ğ‘–Â” \u0003\nIn the following we show that the consistency of L/a.sc/m.sc/b.sc/d.sc/a.scDC is best possible up to a factor of 2.\nLemma 20. For everyğœ†2Â»0Â•1Â¼,ğ›¼Â¹ğ‘˜ÂºÂŸ2ğœŒÂ¹ğ‘˜Âº.\n32\n\nProof. First note that for ğœ†=1,ğ›¼Â¹ğ‘˜Âº=ğ‘˜=ğœŒÂ¹ğ‘˜Âº. Now suppose that ğœ†ÂŸ1. Applying the formula for the\nnite geometric series gives\nğœŒÂ¹ğ‘˜Âº=1\u0000ğœ†ğ‘˜\n1\u0000ğœ†Â”\nWe now prove the result based on the parity of ğ‘˜. Assume that ğ‘˜is even. Recall that\nğ›¼Â¹ğ‘˜Âº=1Â¸2ğ‘˜Â2\u00001âˆ‘ï¸\nğ‘–=1ğœ†ğ‘–Â¸ğœ†ğ‘˜Â2=1Â¸2ğœ†\u0000ğœ†ğ‘˜Â2\n1\u0000ğœ†Â¸ğœ†ğ‘˜Â2\nand, thus,\nğ›¼Â¹ğ‘˜Âº\nğœŒÂ¹ğ‘˜Âº=Â¹1Â¸ğœ†ğ‘˜Â2ÂºÂ¹1\u0000ğœ†ÂºÂ¸2Â¹ğœ†\u0000ğœ†ğ‘˜Â2Âº\n1\u0000ğœ†ğ‘˜=1Â¸ğœ†\u0000ğœ†ğ‘˜Â2\u0000ğœ†ğ‘˜Â2Â¸1\n1\u0000ğœ†ğ‘˜ÂŸ2Â”\nAssume that ğ‘˜is odd, then\nğ›¼Â¹ğ‘˜Âº=1Â¸2Â¹ğ‘˜\u00001ÂºÂ2âˆ‘ï¸\nğ‘–=1ğœ†ğ‘–=1Â¸2ğœ†\u0000ğœ†Â¹ğ‘˜Â¸1ÂºÂ2\n1\u0000ğœ†Â•\nand we conclude that\nğ›¼Â¹ğ‘˜Âº\nğœŒÂ¹ğ‘˜Âº=1\u0000ğœ†Â¸2Â¹ğœ†\u0000ğœ†Â¹ğ‘˜Â¸1ÂºÂ2Âº\n1\u0000ğœ†ğ‘˜=1Â¸ğœ†\u00002ğœ†Â¹ğ‘˜Â¸1ÂºÂ2\n1\u0000ğœ†ğ‘˜ÂŸ2Â”\n\u0003\nD PAC Learnability of Predictions\nWe show that our predictions are PAC learnable in an agnostic sense with a sample complexity polynomial\nin the number of requests and we give an ecient learning algorithm. Let Dbe an unknown distribution\nof sequences of ğ‘›requests represented by points in the interval Â»0Â•1Â¼. Here we assume a bounded line as a\nmetric (scaled toÂ»0Â•1Â¼), which is a restriction but natural in most applications. Further, we assume that we\ncan sample i.i.d. sequences from D.\nLetH=f1Â•Â”Â”Â”Â•ğ‘˜gğ‘›denote a hypothesis class containing all possible static predictions, i.e., the set\nof allğ‘˜-server solutions for request sequences of length ğ‘›. Letğ¶0be a known initial conguration. The\nprediction error for a prediction ğœ2H on a request sequence ğœis dened as ğœ‚ğœÂ¹ğœÂº=F/t.scPÂ¹ğœÂ•ğœÂº\u0000O/p.sc/t.scÂ¹ğœÂº,\nwhere F/t.scPÂ¹ğœÂ•ğœÂºis the total cost of following the prediction ğœon the sequence ğœstarting inğ¶0, and O/p.sc/t.scÂ¹ğœÂº\nis the cost of an optimal solution on ğœstarting inğ¶0. Then,ğœ‚ğœÂ¹ğœÂº\u0014ğœ‚max\u0014ğ‘›for all possible sequences ğœ\nand for allğœ2H.\nWe argue that we can use a classical empirical risk minimization (ERM) learning method, see, e.g., [46].\nThe ERM method uses a training set ğ‘†=fğœ1Â•Â”Â”Â”Â•ğœğ‘šgof i.i.d. samples from D. Then, it determines a\npredictionğœğ‘2H that minimizes the empirical error ğœ‚ğ‘†Â¹ğœÂº=1\nğ‘šÃğ‘š\nğ‘—=1ğœ‚ğœğ‘—Â¹ğœÂº. Since our hypothesis class\nis nite and the error function bounded, classical results imply that our predictions are PAC learnable in\nan agnostic sense with a polynomial sample complexity. Further, we show that the problem of nding\nthe prediction minimizing the empirical error within the training set can be reduced to an oine ğ‘˜-server\nproblem on a modied request sequence eğœof lengthğ‘›, where the distance between the â„“th andğ‘–th request\nineğœis given by1\nğ‘šÃğ‘š\nğ‘—=1ğ‘‘Â¹ğœğ‘—Â¹â„“ÂºÂ•ğœğ‘—Â¹ğ‘–ÂºÂº. This problem can be solved eciently [17].\n33\n\nTheorem 10. For anyğœ–Â•ğ›¿2 Â¹0Â•1Âº, a known initial conguration ğ¶0and any distribution Dover the\nsequences of ğ‘›requests of known extent, there exists an algorithm which, given an i.i.d. sample of Dof\nsizeğ‘š2O\u0010\n1\nğœ–2\u0001Â¹ğ‘›logğ‘˜\u0000logğ›¿Âºğœ‚2\nmax\u0011\n, returns a prediction ğœğ‘2H in polynomial time depending on ğ‘˜,ğ‘›\nandğ‘š, such that with probability of at least Â¹1\u0000ğ›¿Âºit holds Eğœ\u0018DÂ»ğœ‚ğœÂ¹ğœğ‘ÂºÂ¼\u0014Eğœ\u0018DÂ»ğœ‚ğœÂ¹ğœ\u0003ÂºÂ¼Â¸ğœ–, whereğœ\u0003=\narg minğœ2HEğœ\u0018DÂ»ğœ‚ğœÂ¹ğœÂºÂ¼.\nProof. Since the hypothesis class His nite withjHj=ğ‘˜ğ‘›, and our non-negative error function is bounded\nbyğœ‚max, classical results, see e.g. [46], imply that His agnostically PAC-learnable using the ERM algorithm\nwith a sample complexity of\nğ‘š\u0014\u00182 logÂ¹2jHjÂğ›¿Âºğœ‚2\nmax\nğœ–2\u0019\n2O\u0012Â¹ğ‘›logğ‘˜\u0000logğ›¿Âºğœ‚2\nmax\nğœ–2\u0013\nÂ”\nThat is, given a sample of size at least ğ‘š, the ERM algorithm outputs with a probability of at least Â¹1\u0000ğ›¿Âºa\npredictionğœğ‘such that Eğœ\u0018DÂ»ğœ‚ğœÂ¹ğœğ‘ÂºÂ¼\u0014Eğœ\u0018DÂ»ğœ‚ğœÂ¹ğœ\u0003ÂºÂ¼Â¸ğœ–holds, where ğœ\u0003=arg minğœ2HEğœ\u0018DÂ»ğœ‚ğœÂ¹ğœÂºÂ¼.\nIt remains to describe an ecient implementation of the ERM algorithm for our setting. Let ğ‘†=\nfğœ1Â•Â”Â”Â”Â•ğœğ‘šgbe a sample drawn i.i.d. from D. We assume that this can be done in polynomial time in ğ‘š.\nFor a sequence ğœğ‘—2ğ‘†letğœğ‘—Â¹ğ‘–Âºbe the position of the ğ‘–th request in ğœğ‘—. We further dene for 1\u0014â„“\u0014ğ‘–\u0014ğ‘›\nand1\u0014ğ‘˜0\u0014ğ‘˜the distance functions ğ›¿ğ‘—Â¹â„“Â•ğ‘–Âº=jğœğ‘—Â¹â„“Âº\u0000ğœğ‘—Â¹ğ‘–Âºjandğ›¾ğ‘—Â¹ğ‘˜0Â•ğ‘–Âº=jğ¶0Â¹ğ‘˜0Âº\u0000ğœğ‘—Â¹ğ‘–Âºj. The empirical\nerror of a prediction ğœis in our setting dened as\nğœ‚ğ‘†Â¹ğœÂº=1\nğ‘šğ‘šâˆ‘ï¸\nğ‘—=1ğœ‚ğœğ‘—Â¹ğœÂº=1\nğ‘šğ‘šâˆ‘ï¸\nğ‘—=1F/t.scPÂ¹ğœğ‘—Â•ğœÂº\u0000O/p.sc/t.scÂ¹ğœğ‘—ÂºÂ”\nThe ERM algorithm outputs the prediction ğœğ‘2H that minimizes ğœ‚ğ‘†Â¹ğœÂºas a function over H. Since\niterating over all predictions in Htakes exponential time, we compute ğœğ‘dierently. To do so, we rst\nobserve that1\nğ‘šÃğ‘š\nğ‘—=1O/p.sc/t.scÂ¹ğœğ‘—Âºis independent of ğœ, thus minimizing ğœ‚ğ‘†Â¹ğœÂºcan be reduced to minimizing\n1\nğ‘šğ‘šâˆ‘ï¸\nğ‘—=1F/t.scPÂ¹ğœğ‘—Â•ğœÂº=1\nğ‘šğ‘šâˆ‘ï¸\nğ‘—=1ğ‘˜âˆ‘ï¸\nğ‘˜0=1ğ‘›âˆ‘ï¸\nğ‘–=1ğœ‰ğœ\nğ‘˜0Â•ğ‘–\u0001ğ›¾ğ‘—Â¹ğ‘˜0Â•ğ‘–ÂºÂ¸ğ‘–âˆ‘ï¸\nâ„“=1ğœ’ğœ\nğ‘˜0Â•ğ‘–Â•â„“\u0001ğ›¿ğ‘—Â¹â„“Â•ğ‘–Âº\n=ğ‘˜âˆ‘ï¸\nğ‘˜0=1ğ‘›âˆ‘ï¸\nğ‘–=1ğœ‰ğœ\nğ‘˜0Â•ğ‘–\u0001 \n1\nğ‘šğ‘šâˆ‘ï¸\nğ‘—=1ğ›¾ğ‘—Â¹ğ‘˜0Â•ğ‘–Âº!\nÂ¸ğ‘–âˆ‘ï¸\nâ„“=1ğœ’ğœ\nğ‘˜0Â•ğ‘–Â•â„“\u00011\nğ‘šğ‘šâˆ‘ï¸\nğ‘—=1ğ›¿ğ‘—Â¹â„“Â•ğ‘–ÂºÂ• (5)\nwhereğœ’ğœ\nğ‘˜0Â•ğ‘–Â•â„“2f0Â•1gindicates (i.e. is equal to 1) that server ğ‘˜0serves theğ‘–th request of ğœğ‘—directly after\ntheâ„“th request of ğœğ‘—inğœandğœ‰ğœ\nğ‘˜0Â•ğ‘–2f0Â•1gindicates that the ğ‘–th request of ğœğ‘—is the rst one that server ğ‘˜0\nserves inğœ.\nWe now demonstrate that we can eciently compute a prediction ğœ2H that minimizes (5). Indeed,\nobserve that (5)is equal to the total cost of the solution ğœfor theğ‘˜-server instance that starts in ğ¶0\nand serves a sequence eğœof lengthğ‘›, where the distance between the â„“th andğ‘–th request in eğœis given\nbyğ›¿0Â¹â„“Â•ğ‘–Âº=1\nğ‘šÃğ‘š\nğ‘—=1ğ›¿ğ‘—Â¹â„“Â•ğ‘–Âºand the distance between the ğ‘–th request in eğœand the initial position of server ğ‘˜0\nis given byğ›¾0Â¹ğ‘˜0Â•ğ‘–Âº=1\nğ‘šÃğ‘š\nğ‘—=1ğ›¾ğ‘—Â¹ğ‘˜0Â•ğ‘–Âº. But this means that any optimal solution eğœfor this instance also\nminimizes (5). Clearly, eğœ2H, and an optimal solution for a k-server instance with known distance functions\ncan be computed in OÂ¹ğ‘˜ğ‘›2Âºtime using a min-cost ow algorithm [17]. \u0003\n34\n\nD/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc L/a.sc/m.sc/b.sc/d.sc/a.scDC (0.1) L/a.sc/m.sc/b.sc/d.sc/a.scDC (0.5) F/t.scP&DC\nnon-lazy lazy non-lazy lazy non-lazy lazy non-lazy lazy\nğ‘˜=2 1.60 1.03 1.31 1.10 1.35 1.035 1.70 1.23\nImprovement 35% 16% 22% 27%\nğ‘˜=10 1.63 1.29 1.35 1.19 1.43 1.16 2.14 1.63\nImprovement 21% 12% 19% 24%\nğ‘˜=50 1.63 1.29 1.45 1.27 1.44 1.17 2.29 1.66\nImprovement 21% 12% 19% 28%\nTable 1: Relative improvements of the largest mean empirical competitive ratio for any bin due to lazy implementations.\nE Experiments\nThis section gives a detailed overview over the empirical experiments. The simulation software is written\nin Rust (version 1.51.0, 2018 edition). We executed all experiments in Ubuntu 18.04.5 on a machine with\ntwo AMD EPYC ROME 7542 CPUs (64 cores in total) and 1.96 TB RAM.\nWe implemented F/t.scP&DC[3] with the hyperparameter ğ›¾equal to 1. The instances are based on the\nBrightKite-Dataset [16]. We extract sequences with a length of 1000 checkins, normalize the scaling of\nlatitudes to the interval Â»0Â•4000Â¼, and use these values as the positions of the requests on the line. All\nservers start at the same initial random position.\nWe generate predictions in a semi-random fashion. Fix two parameters ğ‘, the number of bins, andğ‘, the\nbin size , and an instance. Our goal is to generate evenly distributed predictions, i.e., in each bin ğ‘–2f1Â•Â”Â”Â”Â•ğ‘g\nthere are at least ve predictions with relative error between Â¹ğ‘–\u00001Âºğ‘andğ‘–ğ‘. Additionally, we use an optimal\nsolution of the instance as the perfect prediction.\nGiven those parameters and an instance, we iteratively sample many predictions with an increasing\nnumber of wrong choices with respect to the optimal solution. While this procedure does not nd all\npredictions, especially these with the largest relative error, it gives a good tradeo between running time\nand range of prediction error. We set ğ‘=10andğ‘as high as we nd for at least 40instances these evenly\ndistributed predictions. Other instances are discarded.\nThe results for ğ‘˜=2,ğ‘˜=10andğ‘˜=50are displayed in Figures 6 to 8. We rst observe that\nfor a reasonable choice of ğœ†(0Â”1\u0014ğœ†\u00140Â”5)L/a.sc/m.sc/b.sc/d.sc/a.scDC outperforms F/t.scP&DCthroughout almost all\ngenerated relative prediction errors in both lazy and non-lazy settings. This is also the case compared\ntoD/o.sc/u.sc/b.sc/l.sc/e.scC/o.sc/v.sc/e.sc/r.sc/a.sc/g.sc/e.sc with the exception of its strong performance for ğ‘˜=2in the lazy implementation.\nFurther, all algorithms except L/a.sc/m.sc/b.sc/d.sc/a.scDC forğœ†=0Â”0improve by a lazy implementation. This is no surprise,\nas this is the only algorithm that only moves a single server in the non-lazy setting, so there are no\npostponed moves that can possibly be improved by a lazy implementation. The actual improvements of\nthe largest mean empirical ratio for any bin of all algorithms which we discovered in our experiments are\ngiven in Table 1. Observe that L/a.sc/m.sc/b.sc/d.sc/a.scDC benets more from the lazy implementation when ğœ†gets closer\nto 1, whereas the improvements for F/t.scP&DCare between 24% and 28%. We suspect that F/t.scP&DConly\nmakes few expensive resets in our instances, while L/a.sc/m.sc/b.sc/d.sc/a.scDC benets from many cheap improvements.\n35\n\n0 2 4 6 8\nEta / Opt1.01.52.02.53.03.54.0Empirical competitive ratio\nFtP&DC\nDC\nLDC (  = 0.00)\nLDC (  = 0.10)\nLDC (  = 0.50)\n(a) non-lazy\n0 2 4 6 8\nEta / Opt1.001.051.101.151.201.251.301.35Empirical competitive ratio\nFtP&DC\nDC\nLDC (  = 0.00)\nLDC (  = 0.10)\nLDC (  = 0.50)\n (b) lazy\nFigure 6: Results for ğ‘˜=2andğ‘=1.\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\nEta / Opt1.01.21.41.61.82.02.2Empirical competitive ratio\nFtP&DC\nDC\nLDC (  = 0.00)\nLDC (  = 0.10)\nLDC (  = 0.50)\n(a) non-lazy\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\nEta / Opt1.01.11.21.31.41.51.61.71.8Empirical competitive ratio\nFtP&DC\nDC\nLDC (  = 0.00)\nLDC (  = 0.10)\nLDC (  = 0.50)\n (b) lazy\nFigure 7: Results for ğ‘˜=10andğ‘=2.\n0 5 10 15 20 25\nEta / Opt1.01.21.41.61.82.02.2Empirical competitive ratio\nFtP&DC\nDC\nLDC (  = 0.00)\nLDC (  = 0.10)\nLDC (  = 0.50)\n(a) non-lazy\n0 5 10 15 20 25\nEta / Opt1.01.21.41.61.82.02.2Empirical competitive ratio\nFtP&DC\nDC\nLDC (  = 0.00)\nLDC (  = 0.10)\nLDC (  = 0.50)\n (b) lazy\nFigure 8: Results for ğ‘˜=50andğ‘=3.\n36",
  "textLength": 89265
}