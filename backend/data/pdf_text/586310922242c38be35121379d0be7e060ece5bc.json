{
  "paperId": "586310922242c38be35121379d0be7e060ece5bc",
  "title": "Leveraging Soft Functional Dependencies for Indexing Multi-dimensional Data",
  "pdfPath": "586310922242c38be35121379d0be7e060ece5bc.pdf",
  "text": "COAX: Correlation-Aware Indexing\non Multidimensional Data with Soft Functional Dependencies\nAli Hadian1, Behzad Ghaffari1, Taiyi Wang2, Thomas Heinis1\n1Imperial College London,2Johns Hopkins University\nABSTRACT\nRecent work proposed learned index structures, which learn the\ndistribution of the underlying dataset to improve performance.\nThe initial work on learned indexes has shown that by learning\nthe cumulative distribution function of the data, index structures\nsuch as the B-Tree can improve their performance by one order\nof magnitude while having a smaller memory footprint.\nIn this paper, we present COAX, a learned index for multidi-\nmensional data that, instead of learning the distribution of keys,\nlearns the correlations between attributes of the dataset. Our\napproach is driven by the observation that in many datasets, val-\nues of two (or multiple) attributes are correlated. COAX exploits\nthese correlations to reduce the dimensionality of the datasets.\nMore precisely, we learn how to infer one (or multiple) at-\ntributeğ¶ğ‘‘from the remaining attributes and hence no longer\nneed to index attribute ğ¶ğ‘‘. This reduces the dimensionality and\nhence makes the index smaller and more efficient.\nWe theoretically investigate the effectiveness of the proposed\ntechnique based on the predictability of the FD attributes. We fur-\nther show experimentally that by predicting correlated attributes\nin the data, we can improve the query execution time and reduce\nthe memory overhead of the index. In our experiments, we reduce\nthe execution time by 25% while reducing the memory footprint\nof the index by four orders of magnitude.\n1 INTRODUCTION\nMultidimensional data plays a crucial role in data analytics appli-\ncations.Indexing multidimensional data, however, is challenging\nas the curse of dimensionality hits: with every additional dimen-\nsion indexed, the performance of the index degrades. A promising\napproach to tackle the curse of dimensionality is using machine\nlearning techniques and exploit patterns in data distribution for\nmore efficient indexing. For example, learned indexes automat-\nically model and use the distribution of the underlying data to\nlocate the data records [ 5,11,15,21,22,28] and the idea has been\nextended to indexing multidimensional data [28].\nIn this paper we develop a new class of learned indexes for\nmultidimensional data that uses dependencies between attributes\nof either the full or a subset of the dataset to improve perfor-\nmance of the index structure. Our approach is motivated by the\nobservation that it is common in real-world datasets for two or\nmore attributes of the data to correlate. We argue that by tak-\ning learned indexes to the multidimensional case, in addition\nto learning from CDF of the data, we can also learn from rela-\ntionships between attributes of the data such as the correlation\nbetweenğ‘–ğ‘‘andğ‘¡ğ‘–ğ‘šğ‘’ğ‘ ğ‘¡ğ‘ğ‘šğ‘ , a common case in real-world datasets;\nor between flight distance and flight time in an airline dataset.\nThe idea of learning the relationship between attributes of data\nhas already been used to improve estimating the selectivity of\na given query and thus to improve query optimizers [ 18] and\none-dimensional secondary indexes [ 35]. We take this idea a step\nfurther to indexing multidimensional data.More precisely, we develop models that explain correlations\nin the attributes of the data and we show that a multidimensional\nindex does not need to index every dimension if some attributes\nare correlated. The index only needs to store one dimension per\neach group of correlated attributes, thereby effectively reducing\nthe dimensionality of the dataset. In case a query targets an\nattributeğ¶ğ‘‘that is not indexed but there is another indexed\nattributeğ¶ğ‘¥correlated to ğ¶ğ‘‘(ğ¶ğ‘¥â†’ğ¶ğ‘‘), we use our model to\ncheck which range of values in ğ¶ğ‘¥correlate with the query and\nrun a translated query over the indexed attribute instead. As\nwe show experimentally, the suggested approach significantly\nshrinks the memory footprint of the index, by four orders of\nmagnitudes depending on the number of the FDs and their degree\nof correlation, while improving the overall lookup time of the\nindexes by 25%.\n2 RELATED WORK\nOur ideas build on model-based indexes, including learned in-\ndexes and interpolation search, as well as spatial indexes.\nRecently, it was suggested that models driven from data can\nimprove the lookup time of indexes and reduce memory footprint\nin what are called learned indexes [ 22]. In a learned index, the\nCDF of the key distribution is learned by fitting a model; then\nthe learned model is used as a replacement for the conventional\nindex structures (such as B+trees) for finding the location of\nthe query results. Index learning frameworks such as the RMI\nmodel [ 22] are capable of learning arbitrary models, although\na further theoretical study [ 7] have shown that models such as\nlinear splines are effective for most real-world datasets. Spline-\nbased learned indexes include Piecewise Geometric Model index\n(PGM-index) [ 9], Fiting-tree [ 11], Model-Assisted B-tree (MAB-\ntree) [ 16], Radix-Spline [ 21], Interpolation-friendly B-tree (IF-\nBtree) [ 15] and others [ 25,32]. A comprehensive comparison of\nlearned indexing approaches can be found here [8].\nCOAX is inspired by hybrid learned indexes that combine\nmachine learning with traditional indexes structures, includ-\ning RadixSpline [ 21], FITing-tree [ 11], Interpolation-Friendly\nB-tree [ 15], and MADEX [ 16]. Hybrid learned indexes have been\nwell explored in the one-dimensional case. For example, RadixS-\npline [ 21] uses radix-trees as the top-level model while FITing-\ntree [ 11] and IFB-tree [ 15] use B+-tree as the top-level index\nstructure. Then they use a series of piecewise linear functions\nin the leaf level. Furthermore, adaptability and updatability of\nlearned indexes have been explored in a similar area [ 5,14]. Fi-\nnally, in the multivariate area, learning from a workload has also\nshown interesting results [17, 28].\nBesides the main trend in learned indexes (focusing on range\nindexing), machine learning has also inspired other indexing\nand retrieval tasks. This includes bloom filters [ 4,27], inverted\nindexes [ 36], conjunctive Boolean intersection[ 30], learned in-\ndexing for strings [ 34], rule-matching [ 23,37], and computing\nlist intersections [3].\nAnother direction that we exploit in this paper is the statisti-\ncal dependency between the attributes (data columns), i.e., thearXiv:2006.16393v3  [cs.DB]  2 Feb 2021\n\nQuery\nLearned\nSoft-FD\nIndex \nOutlier\nIndexMerged outputFigure 1: The suggested index structure: a primary index\n(top) indexes only one dimension from each group of cor-\nrelated attributes, while a small outlier index handles the\noutliers and guarantees retrieval of all results.\nnotion of soft functional dependencies (Soft FDs) [18,35], which is\na relaxed definition of FDs. A soft FD between X and Y means\nthat the value of X determines the value of Y with a high proba-\nbility, i.e., we can predict Y using X but there might be errors in\nprediction, for example, from Flight departure time we can deter-\nmine the approximate arrival time. Soft FDs have been mainly\nused for selectivity estimation in query optimization [ 18,19,24],\ndesigning materialized views and indexes [ 20], data integration\nfrom multiple data sources [ 33], and reducing the number of sec-\nondary indexes [ 35]. In this work, however, we aim to design an\nindex structure that uses soft FDs to decrease memory footprint\nand accelerate query execution in a multidimensional index .\nIn contrast to the related work like HERMIT, which is based\non unclustered secondary indexes, COAX builds a multidimen-\nsional primary index, which is more efficient for range indexing\nas it does not require heavy join operations for multidimen-\nsional queries. Moreover, related work only considers cases with\na small percentage of outliers (the records that do not follow\nthe dependency pattern) while COAX consists of two indexes\nwhere the outliers are organised as a separate multidimensional\nindex, hence supporting datasets with a much â€˜softerâ€™ functional\ndependency, i.e., a considerable number of outliers (e.g., 25%).\nFinally, previous work on soft-FD requires the DBA to manu-\nally introduce all soft-functional dependencies and handcraft the\nindex layout for the soft-FD, which is very time-consuming for\nthe DBA. COAX, on the other hand, automatically detects the\ncorrelated columns and works with any multidimensional index\nstructure.\n3 APPROACH OVERVIEW\nWe are interested in exploiting correlations in a multidimensional\ndataset to build a more efficient index. The key components of\nCOAX, illustrated in Figure 1, are as follows:\nâ€¢Query translation. The core idea of COAX is that to build an\nğ‘›-dimensional index on a data where ğ‘šattributes are highly\ncorrelated with the other ğ‘›âˆ’ğ‘šattributes (i.e., ğ‘šdependent\nattributes), we can build a multidimensional index on the ğ‘›âˆ’ğ‘š\nattributes which results in a much smaller multidimensional\nindex. If a query constraint targets one of the dependent at-\ntributes, COAX transforms the constraint into another con-\nstraint on one of the indexed attributes. Query translation uses\na model that predicts the dependent attributes using a set of\nindexed attributes , along with the tolerance margins.\nâ€¢Learning the correlations. COAX detects whether a func-\ntional dependency exists between two or more attributes, and\nevaluates whether the dependency can be effectively modelled\nin the presence of outliers.\nâ€¢Learned Soft-FD index. We apply a pre-processing step to\nour data. We use the learned correlations to separate data\nthat agrees with the learned dependency from the remainingpoints that are regarded as outliers. We create a primary index\non the data points that have up to a certain deviation from\nthe learned correlation (i.e., those that are within a certain\ntolerance margin around the fitted line). The primary index\nonly indexes one attribute per each set of correlated attributes.\nFor the rest of the correlated attributes, a model is learned\nto represent the correlation between indexed attributes and\nlearned attributes is used to execute queries. If a query targets\na dependent attribute ğ¶ğ‘‘that is not indexed but is correlated\nwith an indexed attribute ğ¶ğ‘¥, then the learned correlation\nmodel converts query constraints targeting ğ¶ğ‘‘to an equivalent\nconstraint on ğ¶ğ‘¥.\nâ€¢Outlier index. Data points that do not fall within the tolerance\nmargin of the soft FD model are excluded from the primary\nindex and are indexed with all dimensions in an outlier index ,\nwhich is a typical multidimensional index structure.\n4 COAX QUERY TRANSLATION\nLet us consider the simple case where we want to answer queries\non two attributes ğ¶1andğ¶2:\nSELECT âˆ—FROM t b l WHERE ğ‘ğ‘™ğ‘œğ‘¤\n1<ğ¶1<ğ‘â„ğ‘–ğ‘”â„\n1\nANDğ‘ğ‘™ğ‘œğ‘¤\n2<ğ¶2<ğ‘â„ğ‘–ğ‘”â„\n2;\nWe define a query by a rectangle characterised by its lower-\nmost leftmost point (ğ‘ğ‘™ğ‘œğ‘¤\n1, ğ‘ğ‘™ğ‘œğ‘¤\n2)and uppermost rightmost point\n(ğ‘â„ğ‘–ğ‘”â„\n1, ğ‘â„ğ‘–ğ‘”â„\n2). Note that with this setting, we can express the\ncase where, for example, only the first dimension is queried by\ndefiningğ‘â„ğ‘–ğ‘”â„\n2=âˆandğ‘ğ‘™ğ‘œğ‘¤\n2=âˆ’âˆ. Similarly, we can express\npoint queries by defining the lower and upper points to be equal.\nSuppose that the attributes to be indexed are correlated, i.e.,\nthere is a soft functional dependency between one attribute, say\nğ¶ğ‘¥(theindexed attribute), and another dependent attribute ğ¶ğ‘‘. In\nthis case, a range index can be built on only one of the attributes\n(ğ¶ğ‘¥), and the query constraint that targets a non-indexed at-\ntributeğ¶ğ‘‘can be mapped to equivalent query constraints on\nthe indexed attribute using the prediction model and its error\nbounds. To do this without loss of accuracy, we need an oracle\ndependency function ğœ“:ğ¶ğ‘¥â†’ğ¶ğ‘‘that calculates the value of\nğ¶ğ‘‘based on a given ğ¶ğ‘¥for each row in the dataset. Because in\npractice finding such an oracle function may prove impossible\nfor a softfunctional dependency, we must relax this concept and\nallow our model to instead estimate an approximate value for ğ¶ğ‘‘,\ni.e.,Ë†ğœ“:ğ¶ğ‘¥â†’ğ¶ğ‘‘\nSimilar to learned range indexes [ 22], using an approximation\nwithout any bounds is impractical since we want to avoid scan-\nning the entire dataset. We must therefore define a tight error\nbound for the approximation. To do so, we argue that once a\nsignificant majority of the values of the dependent attribute ğ¶ğ‘‘\nare very close to the values predicted by Ë†ğœ“model. For any point\n(ğ‘ğ‘¥,ğ‘ğ‘‘)âˆˆ(ğ¶ğ‘¥,ğ¶ğ‘‘)in our primary index we have:\nğ‘ğ‘‘âˆˆ[Ë†ğœ“(ğ‘ğ‘¥)âˆ’ğœ–ğ¿ğµ,ğœ“(ğ‘ğ‘¥)+ğœ–ğ‘ˆğµ] (1)\nWhereğœ–ğ¿ğµandğœ–ğ‘ˆğµare the the lower bound and upper bound\nerror margins, or as illustrated graphically, the distances at which\nthe data separators have been drawn in both directions. We then\nkeep the records that fall between these bounds and leave outliers\naside to be inserted into the outlier index. Figure 1 shows the\nprocess of defining the two parallel lines characterising the lower\nand upper error bounds of a linear regression model, and the role\nof our primary and outlier index.\n\ncd\ncx\n1\ncd\ncx\n1Figure 2: Query execution on the primary index. Query\nconstraints targeting the dependent attributes ğ¶ğ‘‘are\nmapped to equivalent constraints on the indexed attribute\nğ¶ğ‘¥using the model and the error bounds. The final query\nconstraint is the intersection of the two constraints on ğ¶ğ‘¥\nFigure 3: An error margin can be defined by considering\nthe the correlation model of the data (left) and the density\nof the data records around the model (right)\nTo create a primary index on ğ¶ğ‘¥andğ¶ğ‘‘, we only need to sort\nthe rows based on the ğ¶ğ‘‘attribute. To answer a range query on\nboth dimensions, we need to (1) calculate which range of values\nin the indexed dimension corresponds to the queried range for\nthe missing dimension and (2) scan the intersection of the queried\nranges projected on the indexed dimension. Put differently, be-\ncause the data in the primary index fits in within the lower and\nupper bound estimates of the model, we can tighten the lower\nand upper bounds of our query for each of the correlated dimen-\nsions making the overall scanned range smaller. As illustrated in\nFigure 2, we need to find the intersection of the query rectangle\nwith our lower and upper thresholds. We then scan the records\nin the indexed attribute ( ğ¶ğ‘‘) between the more selective bounds\n(drawn as solid vertical lines in Figure 2), i.e.:\nh\nğ‘šğ‘ğ‘¥\u0010\nË†ğœ“(ğ‘ğ‘™ğ‘œğ‘¤\n1),ğ‘ğ‘™ğ‘œğ‘¤\n2âˆ’ğœ–ğ¿ğµ\u0011\n,ğ‘šğ‘–ğ‘›\u0010\nË†ğœ“(ğ‘â„ğ‘–ğ‘”â„\n1),ğ‘â„ğ‘–ğ‘”â„\n2+ğœ–ğ‘ˆğµ\u0011i\n(2)\n5 TRAINING THE SOFT-FD MODELS\nDetecting the soft-FDs and learning a soft-FD model that is com-\nputationally expensive. To tackle this, COAX uses a fraction of\nkeys rather than the full key set to train the model and evaluate\nthe efficiency of adopting a potential soft-FD. More precisely,\nCOAX only considers centres of dense areas in a sample drawn\nfrom the dataset. We overlay a multidimensional grid on the key\nspace and count the number of records in each cell. We then\nfilter out any cells that do not reach a threshold in their count\nand consider our training data to be the weighted centres of the\nremaining cells. This is depicted in Figure 3.\nAfter the training set (bucket centres) is found, we recursively\nconsider unique pairs of attributes and use a Monte Carlo sampler\nto check whether a linear model fits the training records (Algo-\nrithm 1) to learn correlations between multiple attributes. If two\nattributes are found to be correlated, we save the resulting pair\nalong with their model parameters. In the final step, we merge all\ngroups that have an attribute in common and pick one attribute\nin each group to be the predictor responsible for estimating the\nremaining attributes in its group.Algorithm 1: Splitting Data\nInput: Centred attribute values: ğ¶ğ‘¥,ğ¶ğ‘‘[ğ‘]\nResult: Model parameters ( ğ‘š,ğ‘),ğ‘ğ‘Ÿğ‘–ğ‘šğ‘ğ‘Ÿğ‘¦ _ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ ,\nğ‘œğ‘¢ğ‘¡ğ‘™ğ‘–ğ‘’ğ‘Ÿ _ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥\nğ¶ğ‘¥_ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ =ğ¶ğ‘¥.sample(ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ _ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ )\nğ¶ğ‘‘_ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ =ğ¶ğ‘‘.sample(ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ _ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ )\nğ‘¤ğ‘¥=ğ¶ğ‘¥_ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ .max() /ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ _ğ‘â„ğ‘¢ğ‘›ğ‘˜ğ‘ \nğ‘¤ğ‘‘=ğ¶ğ‘‘_ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ .max() /ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ _ğ‘â„ğ‘¢ğ‘›ğ‘˜ğ‘ \nğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ğ‘  = [ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ _ğ‘â„ğ‘¢ğ‘›ğ‘˜ğ‘  ][ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ _ğ‘â„ğ‘¢ğ‘›ğ‘˜ğ‘  ]\nforğ‘–â†0toğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ _ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ do\nğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ğ‘  [ğ¶ğ‘¥[ğ‘–] /ğ‘¤ğ‘¥][ğ¶ğ‘‘[ğ‘–] /ğ‘¤ğ‘‘] += 1\nend\nğ¶ğ‘¥_ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› =ğ¶ğ‘‘_ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› = []\nforğ‘–â†0toğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ _ğ‘â„ğ‘¢ğ‘›ğ‘˜ğ‘  do\nforğ‘—â†0toğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ _ğ‘â„ğ‘¢ğ‘›ğ‘˜ğ‘  do\nifğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ğ‘  [ğ‘–][ğ‘—]>ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ then\nğ¶ğ‘¥_ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› += [i *ğ‘¤ğ‘¥+ 0.5ğ‘¤ğ‘¥] *ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ğ‘  [ğ‘–][ğ‘—]\nğ¶ğ‘‘_ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› += [j *ğ‘¤ğ‘‘+ 0.5ğ‘¤ğ‘‘] *ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡ğ‘  [ğ‘–][ğ‘—]\nend\nend\nend\nğ‘š,ğ‘ = linear_regress( ğ¶ğ‘¥_ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ,ğ¶ğ‘‘_ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› )\nğ‘‘ğ‘–ğ‘ ğ‘ğ‘™ğ‘ğ‘ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘  =(ğ¶ğ‘‘âˆ’(ğ‘šÂ·ğ¶ğ‘¥âˆ’ğ‘))\nforğ‘–â†0toğ‘do\nifâˆ’ğœ–ğ¿ğµ<ğ‘‘ğ‘–ğ‘ ğ‘ğ‘™ğ‘ğ‘ğ‘’ğ‘šğ‘’ğ‘›ğ‘¡ğ‘ [ğ‘–]<ğœ–ğ‘ˆğµthen\nğ‘ğ‘Ÿğ‘–ğ‘šğ‘ğ‘Ÿğ‘¦ _ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ .insert(ğ¶ğ‘¥[ğ‘–],ğ¶ğ‘‘[ğ‘–])\nelse\nğ‘œğ‘¢ğ‘¡ğ‘™ğ‘–ğ‘’ğ‘Ÿ _ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ .insert(ğ¶ğ‘¥[ğ‘–],ğ¶ğ‘‘[ğ‘–])\nend\nend\nThe accuracy and runtime of the learning step can be adjusted\nby tuning parameters given in our proposed algorithm. To in-\ncrease the accuracy of the model, we can draw a larger learning\nsample from the dataset, group them in smaller cells and define\na lower cell acceptance threshold. This, however, increases the\nnumber of records processed by the regression algorithm and\ncan degrade training run time.\nThe bucketing step significantly reduces the number of records\nthe regression algorithm has to consider without impeding the\nmethodâ€™s ability to generalise. This reduces the training time\nbut more importantly, it provides an added benefit; recall that\nwe have already inserted our sample records in a grid index (to\nuse centre points of its denser cells to define our training set),\nwe can maintain the trained index for later use. As new records\nare added, a sample of new items can be inserted in the existing\ngrid index without having to populate it from scratch. This in\ncombination with the fact that we have used a Bayesian method\nfor learning the regression model, can help supporting updates\non the index, as we can use the previous gradient and intersect\nand continuously adjust our existing model.\n6 INDEX IMPLEMENTATION\nTo evaluate differences in performance when predicting dimen-\nsions, we need to build an index on indexed dimensions . We im-\nplement our index on top of Grid Files [ 29] with a number of\nmodifications. In particular, we choose boundaries for each cell\nbased on quantiles along each dimension and use the same num-\nber of grid lines for each attribute. Addresses for all cells are\nsorted using the original ordering of attributes in the dataset.\n\n0 5000 10000 15000 20000\npage length0500100015002000count (a) Non-uniform distribution of page sizes in 2D grid layout\n(b) 2D Index Layout\n (c) Learned 1D Grid\nFigure 4: Reducing index dimensionality allows having\nmore accurate grids for the remaining predictor dimen-\nsions\nFurthermore, each cell stores records in a contiguous block of\nvirtual memory in a row store format. Finally, rows within each\npage are sorted based on a given function similar to the approach\nproposed in Flood [ 28]. Sorting the rows inside pages means\nthat we can reduce the dimensionality of the grid by one. This\nis because instead of having grid lines for the particular sorted\nattribute, we can use binary search to locate items (or a scan\nbetween two bounding binary searches in a range query).\nNote that picking grid lines based on the distribution of the\ndataset does not mean that we have regular cells. Although do-\ning so does reduce the standard deviation in cell lengths for\nnon-uniform datasets, bucket lengths are still allowed to grow\narbitrarily large. Figure 4a shows the variation in cell lengths in\none of our experiments.\nThe combination of predicting attributes and having a sorted\ndimension means that for a dataset with ğ‘›dimensions and ğ‘š\npredicted attributes, we only need an index with ğ‘›âˆ’ğ‘šâˆ’1di-\nmensions. Due to this reduced dimensionality, we will show in\nsection 8.2.4 that the resulting index makes much more efficient\nuse of memory.\n7 THEORETICAL ANALYSIS\nIn this section, we analyze how the performance of the soft-\nFD model is affected by the margin size and how its memory\nfootprint compares with a multidimensional index (square grid)\nthat performs the same number of comparisons.\n7.1 Effect of Margin Size\nFigure 5 compares the area scanned by the soft-FD model, with\nthe area of the actual result set. We define the following regions:\nâ€¢B-box (shown as a green box) is the index boundaries ): the\narea that contains all data points (records) restricted by two\nborderlines and hence supported by the Soft-FD index. Any\ndata point outside of B-box needs to be handled by the outlier\nindex.\nâ€¢R-box is the Result set , which is shown as a red parallelogram.\nâ€¢S-Box is the Scanned area, illustrated by a blue parallelogram.\nFor the index to return all results, we need have R-boxâŠ‚S-box .\nğ‘‹!\"#$%ğ‘Œ!\"#$%â„!ğ‘™!ğ‘!ğ‘…\"#$ğ‘†\"#$ğ‘¦=ğ‘ğ‘¥+\tğœ€ğ‘¦=ğ‘ğ‘¥âˆ’\tğœ€B-boxFigure 5: Illustrating the notation. B-box defines the\nboundaries of the index (the range of records indexed by\nthe Soft-FD index). R-box (red parallelogram) is the result\nset. S-box (blue parallelogram) is the area scanned. The\nrange query(â„ğ‘¦,ğ‘™ğ‘¦)is shown with red dashed lines.\nBoth S-box and R-box are within the green rectangle with\ndashed lines.\nWithout loss of generality, we assume that the margin has\nthe same length from both sides, i.e., ğœ–ğ¿ğµ=ğœ–ğ‘ˆğµ=ğœ–. Also, we\nconsider linear models only, and assume that B-box is symmetric,\nwhich means that Ë†ğœ“(ğ‘¥)=ğ‘ğ‘¥. Therefore, the two boundary lines\nare defined as ğ‘¦=ğ‘ğ‘¥+ğœ€andğ‘¦=ğ‘ğ‘¥âˆ’ğœ€. Note that, if the model is\nnon-symmetric ( Ë†ğœ“(ğ‘¥)=ğ‘ğ‘¥+ğ‘,ğ‘â‰ 0) or ifğœ–ğ¿ğµ=ğœ–ğ‘ˆğµ, the areas\nof B-box, S-Box, and R-box are the same and we transform the\nproblem to a symmetric equivalent with a parallel movement.\nThe query constraints can be on the X-axis, the Y-axis, or\nboth. The constraints on either of the axes are translated to\nits equivalent in the other axis (considering the Â±ğœ€boundaries)\nand eventually define some S-box parallelogram as explained in\nSection 4. Therefore, without loss of generality, here we consider\na range query with constraints on the Y-axis only while other\nqueries can be translated to this case. Given the pre-defined range\n(â„ğ‘¦,ğ‘™ğ‘¦), i.e., we want to find all points (ğ‘¥ğ‘–,ğ‘¦ğ‘–)whereğ‘¦ğ‘–âˆˆ[ğ‘ğ‘¦,ğ‘™ğ‘¦].\nLet us denote the data range on the X-axis (the indexed attribute)\nasğ‘‹ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ and the range on the Y-axis (the dependent attribute)\nasğ‘Œğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ .\nWe defineğ‘ğ‘¦as the range of the query ğ‘ğ‘¦=â„ğ‘¦âˆ’ğ‘™ğ‘¦. The area of\nR-box (the red parallelogram) is denoted as ğ‘†ğ‘Ÿ. R-box is the result\nset, and we ideally expect the soft-FD index to scan exactly the\nR-box region (i.e., the red parallelogram in Figure 5). Following\nthe equations of boundary lines ğ‘¦=ğ‘ğ‘¥+ğœ€andğ‘¦=ğ‘ğ‘¥âˆ’ğœ€and\nthe definition of ğœ€above,ğ‘†ğ‘Ÿcan be computed using the base of\nthe R-Box (ğµğ‘ğ‘ ğ‘’ğ‘Ÿ) and its height ( ğ»ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘Ÿ):\nğ‘†ğ‘Ÿ=ğµğ‘ğ‘ ğ‘’ğ‘ŸÂ·ğ»ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘Ÿ=(â„ğ‘¦+ğœ€\nğ‘âˆ’â„ğ‘¦âˆ’ğœ€\nğ‘)Â·ğ‘ğ‘¦=ğ‘ğ‘¦Â·2ğœ–\nğ‘(3)\nFor the Soft-FD model, we define the scanned area ğ‘†ğ‘ as the\narea of the S-box (the blue parallelogram in Figure 5). ğ‘†ğ‘ is like-\nwise estimated using the base and height of the S-box:\nğ‘†ğ‘ =ğµğ‘ğ‘ ğ‘’ğ‘ Â·ğ»ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘ =2ğœ€(2ğœ€+ğ‘ğ‘¦)\nğ‘(4)\nWe define the effectiveness of the Soft-FD model as the ratio\nbetweenğ‘†ğ‘Ÿ, the effective scanning area in ideal cases, and ğ‘†ğ‘ , the\nactual area that the Soft-FD index needs to scan:\n\neffectiveness =ğ‘†ğ‘Ÿ\nğ‘†ğ‘ =ğ‘ğ‘¦\n2ğœ€+ğ‘ğ‘¦(5)\nEffectiveness is the ratio of the red area and blue area in Fig-\nure 5, which is affected by the width of the B-box ( 2ğœ–). It can be\nconcluded from Equation 5 that when the margin is tighter and\nğœ€gets closer to 0, effectiveness gets closer to 1, which means the\narea that the Soft FD index estimates for scanning match the ac-\ntual result set, i.e., S-Box fits R-Box and ğ‘†ğ‘Ÿ=ğ‘†ğ‘ . Similarly, when\nğœ€gets larger, the ratio gets smaller, which means that the Soft-FD\nmodel is less effective. We assumed that the data distribution is\nclose to uniform, otherwise the PDF of the distribution has to be\nconsidered in Equation 5.\n7.2 The Capacity of Linear Soft-FD Models\nIn this section, we provide a stochastic analysis to estimate the\ncapacity of a soft-FD index. If the segmentation number n is large\nenough, the number of data points in the original data set and\nthe number of keys in the new sequence are of the same order of\nmagnitude. Thus, we can estimate the capacity of a soft-FD index\nwith the properties of the new sequence. Our key results are:\nâ€¢The expected number of keys covered by a linear segment\n(Theorem 7.1)\nâ€¢The number of segments needed to cover a stream of length\nn (Theorem 7.4), which is the capacity of a linear model for a\nsoft-FD index.\nWe also provide additional conclusions, including Theorem 7.2\nwhich helps us explain the necessary conditions for the optimal\nvalue of Theorem 7.1. The proofs of all theorems is provided in\nthe Appendix.\nFerragina et al. [ 7] suggest a stochastic process to measure\nthe capacity of learned indexes by making an approximate as-\nsumption of a stochastic process for the position-key sequence\nand designed reasonable distributions over X-axis (Key axis). We\nbuild on their thinking with the help of our CSM model, and we\ncan directly assume the gap distribution of the Y-axis data, so\nthat we no longer need the swapping of the coordinate. In other\nwords, by discretizing the continuous value-value sequence via\nthe CSM model, we generalized their stochastic sequence model.\nIn practice, we assume that the sequence gaps {ğ‘”ğ‘–}are random\nvariables which are sampled from the i.i.d. distribution ğºğ‘–with\nprobability density function ğ‘“ğº, and then we can make assump-\ntions on the mean and variance of the gap distribution, with\nmean E[ğºğ‘–]=ğœ‡and variance ğ‘‰ğ‘ğ‘Ÿ[ğºğ‘–]=ğœ2.\nTheorem 7.1. In the soft-FD model, if a single linear segment\nhas slopeğ‘=ğœ‡and the margin parameter ğœ€is sufficiently larger\nthan the variance of the gap distribution ğœ, the expected number of\nkeys covered by this linear segment isğœ€2\nğœ2\nTheorem 7.2. Given the same assumptions of Theorem 7.1, the\nexpected number of keys covered by a single linear segment can be\nexpressed as a function of margin parameter ğœ€and the variance\nof the gap distribution ğœ. The maximum value of this function, i.e.\nthe largest expected number of keys covered is achieved when the\nslope isğœ‡.\nTheorem 7.3. Given the assumptions and symbols of Theorem\n7.1, the variance of the number of keys covered by a single linear\nsegment is2ğœ€4\n3ğœ4\nEven though this paper mainly discusses a linear soft-FD\nmodel (in terms of a linear model with a margin of constantAirline OSM\nCount 80M 105M\nKey Type float float\nDimensions 8 4\nCorrelated Dimensions (3, 3) 2\nIndexed Dimensions (Soft-FD Index) 2-4 3\nPrimary Index Ratio 92% 73%\nTable 1: Dataset characteristics\nwidth), one can use more complicated non-linear methods to\nmodel soft functional dependencies that cannot be modelled\nwith a linear model. Among non-linear models, we specifically\nconsider linear splines, which are recently shown to be very ef-\nfective in learned indexes for modelling the value-to-position\nmappings [ 9,11,21]. Linear splines still consist of linear models,\nwhere each linear model is used in a specific region. Theorem 7.4\nsuggests the number of segments that a linear spline model re-\nquires for modelling soft-FDs with a maximum error of ğœ–.\nTheorem 7.4. Following the basic assumptions of Theorem 7.1,\ngiven a data stream of length n and the margin parameter ğœ€, the\nnumber of segments s needed to cover the stream can be computed\nfrom the results of Theorem 7.1 and Theorem 7.2, which converges\ntoğ‘›Â·ğœ2\nğœ€2\n8 EVALUATION\n8.1 Experimental Setup\n8.1.1 Implementation and Runtime Environment. We imple-\nment the online section of our index in C and compile it using\nClang 10.0.0. All of the indexes in our experiments run in a single\nthread and use single-precision floating point values. We build\nthe soft-FD model using Python and the pymc3 library [ 2]. We\nrun our experiments on an Intel Core i5-8210Y CPU running at\n3.6 GHz (L1: 128KB, L2: 512KB, L3: 4MB) and 8GB of RAM.\n8.1.2 Datasets. We run experiments on two real-world datasets:\nOpen Street Map (OSM) : we use 4 dimensions of the OSM data\nfor the US Northeast region [ 1] which contains 105M records;\nTheğ¼ğ‘‘andğ‘‡ğ‘–ğ‘šğ‘’ğ‘ ğ‘¡ğ‘ğ‘šğ‘ attributes in the OSM dataset are corre-\nlated and its ğ¿ğ‘ğ‘¡ğ‘–ğ‘¡ğ‘¢ğ‘‘ğ‘’ andğ¿ğ‘œğ‘›ğ‘”ğ‘–ğ‘¡ğ‘¢ğ‘‘ğ‘’ coordinates contain multiple\ndense areas. For this dataset we group ( ğ¼ğ‘‘,ğ‘‡ğ‘–ğ‘šğ‘’ğ‘ ğ‘¡ğ‘ğ‘šğ‘ ) for the\ncase of learned index.\nAirlines : data from US Airlines flights from 2000 up to 2009,\nwhich has 8 attributes and 80M records. The airline dataset is\nmore interesting for our experiments because it contains many\ncorrelated dimensions. Example grouping in this dataset in our ex-\nperiments usually consists of ( ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ ,ğ‘‡ğ‘–ğ‘šğ‘’ğ¸ğ‘™ğ‘ğ‘ğ‘ ğ‘’ğ‘‘ ,ğ´ğ‘–ğ‘Ÿğ‘‡ğ‘–ğ‘šğ‘’ )\nand (ğ´ğ‘Ÿğ‘Ÿğ‘‡ğ‘–ğ‘šğ‘’ ,ğ·ğ‘’ğ‘ğ‘‡ğ‘–ğ‘šğ‘’ ,ğ‘†ğ‘â„ğ‘’ğ‘‘ğ‘¢ğ‘™ğ‘’ğ‘‘ğ´ğ‘Ÿğ‘Ÿğ‘‡ğ‘–ğ‘šğ‘’ ). We can reduce\nthe grid dimensionality to 2-4(depending on the tolerated error\nthreshold). Table 1 summarises the key aspects of the datasets.\nWe generate the queries by picking a random record from the\ndata. Then, we find the K nearest records and take the minimum\nand maximum values corresponding to each dimension. Our\nrange queries are rectangles and target all attributes in the index.\n8.1.3 Baselines. We compare our suggested method with the\nR-Tree , arguably the most broadly used index for multidimen-\nsional data, and two grid structures as baselines: the uniform grid\nand column files.\n\nUniform grid : or equivalently the full grid, is a hash struc-\nture that breaks down each attribute into uniformly sized grid\ncells between their minimum and maximum values. The address\nfor each cell is stored independently and no adjacent cells are\n\"shared/merged\" explicitly. In memory, addresses for all cells are\nsorted using the original ordering of attributes in the dataset.\nFurthermore, each cell stores points in a contiguous block of\nvirtual memory in a row store format.\nColumn files : Essentially a non uniform grid, uses the CDF\nof the data to align/arrange its cell boundaries and sorts data\nwithin each cell based on one of the attributes in the data, thus\nreducing the dimensionality of the index by one. In a lookup on\nthe sorted dimension, we use binary search in each cell to get\nthe range that needs to be scanned. Column files is similar to the\napproach [ 28] with the difference that it does not assume that the\nquery workload is known and hence uses the data distribution\nto arrange and align the grid layout.\nFull scan : Every item in the dataset is checked against queries.\n8.2 Results\n8.2.1 Tuning. In this experiment we measure and compare\nthe execution time for all indexes. We use the configuration\nthat performs best for each index. This configuration consists of\nchunk size for the full grid, chunk size and sort dimension for\nthe column files and COAX, and the node capacity (non-leaf and\nleaf capacity) of the R-Tree. For example, we evaluated different\nnode capacities between 2 to 32 for the for the R-Tree index and\nthe best performance for each experiment (i.e., point queries and\nindividually for each selectivity rate in range queries) was used.\nThe best node size for R-Tree is between 8 and 12.\nDue to memory constraints, we limit any index that would\nrequire more memory overhead for its index directory than mem-\nory occupied by the underlying data itself.\nWe evaluate our results using range queries and point queries\nthat are drawn randomly from each dataset. We define a point\nquery as a range query where the lower bound and upper bound\nin the matching hyper rectangle are equal.\n8.2.2 Point and range queries. As seen in Figure 6, COAX\noutperforms both the R-Tree and full grid. The main drawback\nin the case of full grid is the higher index dimensionality and the\nfact that it is limited in terms of how many cells it can use. An\nexample illustration for this in 2D is shown in Figure 4. This is\nbecause with a skewed dataset, most grid cells become empty or\nvery small in size. In addition, in comparison to Column Files,\nCOAX benefits from a smaller number of memory lookups. The\ndecreased total number of cells in COAX also translates to binary\nsearch on larger ranges in each cell, which makes COAX even\nmore efficient.\n8.2.3 Effect of selectivity in range queries. In addition to this,\nwe run the same experiment with the range queries with selec-\ntivity sizes. In this experiment we use the airline data for the\nyear 2008 only. The results, shown in Figure 7, indicate that the\nLearned Gird does not lose performance on larger/shorter queries.\nWe can check whether the query intersects with the primary, the\noutlier, or both indexes; and run it against the appropriate in-\ndexes. For queries with larger selectivities, it is hence more likely\nfor both indexes to be invoked which results in more invocation\nand bigger overhead for the outlier index.\n8.2.4 Memory Usage. Figure 8 plot the range query perfor-\nmance against memory overhead in the case of COAX, attribute\nAirline (range) Airline (point) OSM (range) OSM (point)10âˆ’310âˆ’210âˆ’1100101102run time (ms)COAX (primary)\nCOAX (outliers)R- Tree\nFull GridFull ScanFigure 6: Query runtime performance on airline and OSM\ndata for range and point queries. Note the log scale.\n35K 150K 750K 1.5M\na\nverage query selectivity (points)02468101214run time (ms)COAX (primary)\nCOAX (outliers)R-Tree Column Files\nFigure 7: Runtime comparison for range queries with dif-\nferent selectivity values on airline data for year 2008 (7M)\nfiles and the R-Tree. The results show that all grid indexes have\na sweet spot. This is because as we increase the number of cells,\nalthough we are likely to scan fewer items, after a certain point\nthe increased pointer lookups starts to hurt performance mainly\nbecause there is diminishing returns in reducing the actual items\nconsidered when increasing the grid size. This effect is illustrated\nin Figure 4.\n9 CONCLUSIONS & FUTURE WORK\nIn this paper we address the degradation of performance/query\nexecution time for every additionally indexed dimension. Instead\nof indexing all dimensions, COAX learns the soft-FDs, i.e., the cor-\nrelation between different attributes. By doing so, we no longer\nhave to index the dependent attributes and can thus reduce the\nnumber of dimensions indexed, thereby accelerating query exe-\ncution and reducing memory overhead. In case the dependent\nattribute is queried for, we use the model (as well as the outlier\nindex) to find a starting point for scanning the data. As we show\nexperimentally, our approach uses substantially less memory and\nalso executes queries faster. COAX shrinks the memory footprint\nof the index by four orders of magnitude, while reducing the\nquery execution time by 25%.\nOur idea can be extended to the case of non-linear models for\nthe dependency and the margins, including a mixture of models.\nAlso, COAX can be extended to support updates. We leave this\nas future work.\nREFERENCES\n[1] [n.d.]. OpenStreetMap Data. https://download.geofabrik.de. 2020-12-22.\n[2][n.d.]. Probabilistic Programming in Python: Bayesian Modeling and Proba-\nbilistic Machine Learning with Theano. https://github.com/pymc-devs/pymc3.\n2020-12-22.\n[3]Naiyong Ao, Fan Zhang, Di Wu, Douglas S Stones, Gang Wang, Xiaoguang\nLiu, Jing Liu, and Sheng Lin. 2011. Efficient parallel lists intersection and\n\n102104106108\nm\nemory overhead (bytes)0.001.002.003.004.005.006.00runtime (ms)COAX (primary)\nCOAX (outliers)COAX (total)\nColumn FilesR-Tree  (a) Airlines (7M)\n102103104105106107108\nmemory overhead (bytes)0.050.100.150.200.25runtime (ms)COAX (primary)\nCOAX (outliers)COAX (total)\nColumn FilesR-Tree\n(b) OSM (9M)\nFigure 8: Runtime vs memory footprint tradeoff for COAX\nand competitors. Note the log scale\nindex compression algorithms using graphics processing units. PVLDB 4, 8\n(2011), 470â€“481.\n[4]Zhenwei Dai and Anshumali Shrivastava. 2019. Adaptive learned Bloom filter\n(Ada-BF): Efficient utilization of the classifier. arXiv:1910.09131 (2019).\n[5]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan\nLi, Hantian Zhang, Badrish Chandramouli, Johannes Gehrke, and Donald\nKossmann. 2020. ALEX: an updatable adaptive learned index. In SIGMOD .\n[6]Paul Embrechts, Claudia KlÃ¼ppelberg, and Thomas Mikosch. 2013. Modelling\nextremal events: for insurance and finance . Vol. 33. Springer.\n[7]Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2020. Why are learned\nindexes so effective?. In ICML .\n[8]Paolo Ferragina and Giorgio Vinciguerra. 2020. Learned data structures. Recent\nTrends in Learning From Data (2020), 5â€“41.\n[9]Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-\ndynamic compressed learned index with provable worst-case bounds. PVLDB\n13, 8 (2020), 1162â€“1175.\n[10] Vanel Steve Siyou Fotso, Engelbert Mephu Nguifo, and Philippe Vaslin. 2019.\nGrasp heuristic for time series compression with piecewise aggregate approx-\nimation. RAIRO-Operations Research 53, 1 (2019), 243â€“259.\n[11] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and\nTim Kraska. 2019. FITing-Tree: A Data-aware Index Structure. (2019).\n[12] Crispin W Gardiner. 1985. Handbook of stochastic methods . Vol. 3. Springer.\n[13] Chonghui Guo, Hailin Li, and Donghua Pan. 2010. An improved piecewise\naggregate approximation based on statistical features for time series mining.\nInKSEM . Springer, 234â€“244.\n[14] Ali Hadian and Thomas Heinis. 2019. Considerations for handling updates in\nlearned index structures. In AIDM .\n[15] Ali Hadian and Thomas Heinis. 2019. Interpolation-friendly B-trees: Bridging\nthe Gap Between Algorithmic and Learned Indexes. In EDBT .\n[16] Ali Hadian and Thomas Heinis. 2020. MADEX: Learning-augmented Algo-\nrithmic Index Structures. In AIDB .\n[17] Ali Hadian, Ankit Kumar, and Thomas Heinis. 2020. Hands-off Model Integra-\ntion in Spatial Index Structures. In AIDB .\n[18] Ihab F Ilyas, Volker Markl, Peter Haas, Paul Brown, and Ashraf Aboulnaga.\n2004. CORDS: automatic discovery of correlations and soft functional depen-\ndencies. In SIGMOD . 647â€“658.\n[19] Hideaki Kimura, George Huo, Alexander Rasin, Samuel Madden, and Stanley B\nZdonik. 2009. Correlation maps: a compressed access method for exploiting\nsoft functional dependencies. PVLDB 2, 1 (2009), 1222â€“1233.\n[20] Hideaki Kimura, George Huo, Alexander Rasin, Samuel R Madden, and Stan-\nley B Zdonik. 2010. CORADD: Correlation Aware Database Designer for\nMaterialized Views and Indexes. PVLDB 3, 1 (2010).\n[21] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons\nKemper, Tim Kraska, and Thomas Neumann. 2020. RadixSpline: A Single-Pass\nLearned Index. (2020).[22] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In SIGMOD . 489â€“504.\n[23] Zhao Lucis Li, Chieh-Jan Mike Liang, Wei Bai, Qiming Zheng, Yongqiang\nXiong, and Guangzhong Sun. 2019. Accelerating rule-matching systems with\nlearned rankers. In USENIX Annual Technical Conference . 1041â€“1048.\n[24] Hai Liu, Dongqing Xiao, Pankaj Didwania, and Mohamed Y Eltabakh. 2016.\nExploiting soft and hard correlations in big data query optimization. PVLDB\n9, 12 (2016), 1005â€“1016.\n[25] Anisa Llavesh, Utku Sirin, Robert West, and Anastasia Ailamaki. 2019. Ac-\ncelerating B+tree Search by Using Simple Machine Learning Techniques. In\nAIDB .\n[26] Jaume Masoliver, Miquel Montero, and Josep PerellÃ³. 2005. Extreme times in\nfinancial markets. Physical Review E 71, 5 (2005).\n[27] Michael Mitzenmacher. 2018. A model for learned bloom filters, and optimizing\nby sandwiching. In NIPS . 462â€“471.\n[28] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020.\nLearning Multi-dimensional Indexes. In SIGMOD . 985â€“1000.\n[29] JÃ¼rg Nievergelt, Hans Hinterberger, and Kenneth C Sevcik. 1981. The grid file:\nAn adaptable, symmetric multi-key file structure. In ECI. Springer, 236â€“251.\n[30] Harrie Oosterhuis, J Shane Culpepper, and Maarten de Rijke. 2018. The\npotential of learned index structures for index compression. In ADCS . 1â€“4.\n[31] Sidney Redner. 2001. A guide to first-passage processes . Cambridge Uni. Press.\n[32] Naufal Fikri Setiawan, Benjamin IP Rubinstein, and Renata Borovica-Gajic.\n2020. Function Interpolation for Learned Index Structures. In ADC .\n[33] Daisy Zhe Wang, Xin Luna Dong, Anish Das Sarma, Michael J. Franklin, and\nAlon Y. Halevy. 2009. Functional Dependency Generation and Applications in\nPay-As-You-Go Data Integration Systems. In WEBDB .\n[34] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. 2020. SIndex:\na scalable learned index for string keys. In APSys .\n[35] Yingjun Wu, Jia Yu, Yuanyuan Tian, Richard Sidle, and Ronald Barber. 2019.\nDesigning succinct secondary indexing mechanism by exploiting column\ncorrelations. In SIGMOD . 1223â€“1240.\n[36] Wenkun Xiang, Hao Zhang, Rui Cui, Xing Chu, Keqin Li, and Wei Zhou. 2018.\nPavo: A RNN-Based Learned Inverted Index, Supervised or Unsupervised?\nIEEE Access 7 (2018), 293â€“303.\n[37] Wangda Zhang, Mengdi Lin, and Kenneth A Ross. 2020. Efficient Search over\nGenomic Short Read Data. In SSDBM .\nA APPENDIX OVERVIEW\nIn the appendix we provide a model that theoretically proves the\ncapacity of linear dependencies in the Soft-FD model through\nthe approximation of new sequences and stochastic analysis.\nThen, we include proofs for the theorems in Section 7 using\nthe suggested theoretical model. We also include a comparison\nbetween our soft-FD index and Grid search.\nB THE CENTER-SEQUENCE MODEL (CSM)\nIn this section, we compute the space cost of linear dependen-\ncies and the capacity of the soft-FD to efficiently model complex\nfunctional dependencies in data. We focus on linear soft-FD mod-\nels and determine 1) what is the capacity of a linear model for\nmodeling soft FDs with a specific maximum error (margin), and\n2) if a single linear model cannot handle the data efficiently, how\nmany segments are needed to model the soft FDs across the entire\nrange of the primary attribute (i.e., across ğ‘‹ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ in Figure 5).\nFollowing the theoretical supports of the effect of the learned\nindex, we were also inspired to make a theoretical evaluation of\nthe capacity and the performance of the Soft-FD model. Earlier\nstudies [ 10,13] suggest a method of constructing a new sequence\nby mean values or centers is widely used in the approximation\nof the original data set, since the two have the same statistical\nproperties but the new sequence ignores some noise. Therefore,\ncombined with the similar stochastic analysis method used in [ 7],\nwe aim to construct a new sequence to give a theoretical support\nfor the effectiveness of the Soft-FD model.\nWe first model the dataset as a sequence of intervals using\nthe proposed Center-Sequence Model (CSM). Then, we perform\nstochastic analysis on the CSM-based representation of data to\nestimate the expected number of keys covered by a linear segment\nand the number of segments needed to cover a sequence of n\nmultidimensional records.\n\n(3,ğ‘¦!)(4,ğ‘¦\")(1,ğ‘¦#)(2,ğ‘¦$)(5,ğ‘¦%)(6,ğ‘¦&)m#m$m!m\"m%m&Figure 9: Segmentation of the B-box using the CSM model\nB.1 Representing Data with CSM\nNow we suggest the Center-Sequence Model, which represents\nthe data as a sequence containing intervals where the data points\nare expected to be in the centre of each interval. The new rep-\nresentation approximates the original data, such that the data\nand its interval-based representation have similar statistical char-\nacteristics. The CSM model is inspired by a recent study [ 7] on\nlearned index structures which proposed using time series ap-\nproximation and stochastic analysis to compute the upper and\nlower limits to estimate the capacity and the cost of using the\npiece-wise linear approximation (PLA) to fit the data points to\na model in PLA-compatible learned index structure such as the\nPGM-index [ 9]. The CSM representation makes it more straight-\nforward to exploit stochastic analysis methods that we use for\nestimating the space cost of the soft-FD index and the effective-\nness of linear and spline Soft-FD models. In fact, approximating\ndata using sequences (or sub-sequences, in case of linear spline\nsoft-FD indexes) is very common in related work [ 7,10]. Based\non a stochastic analysis we can give a very good estimation of\nthe effectiveness of linear correlations and help us simplify our\nmodel.\nB.2 Constructing new sequences to\napproximate the original data sequences\nWe explain how the interval representation is built from the\noriginal data. We consider the two-dimensional case and assume\nthat the data set is relatively large and contains N points. We split\nthe B-box along the X-axis into n parts (n smaller parallelograms)\nusing intervals of the same length ğ‘‘ğ‘¥. Whenğ‘‘ğ‘¥converges to 0,\nğ‘›converges toâˆ, hence we have smaller boxes where each box\nis expected to contain several original points.\nLet us denote the ğ‘˜thinterval as[ğ‘¡ğ‘˜,ğ‘¡ğ‘˜+1], as shown in Fig-\nure 9. Within these ğ‘›separated parts of B-box, if we connect the\ncentroids of these sequences to form an equally spaced sequence,\nthe new sequence will be {[(ğ‘¡ğ‘–+ğ‘¡ğ‘–+1)/2,ğ‘¦ğ‘–],ğ‘–âˆˆ[1,ğ‘]}, where\nğ‘¦ğ‘–is the mean value of all points in the ğ‘–thsegment along the Y\naxis. Let us denote the number of points located in the ğ‘˜thpart\nasğ‘›ğ‘–, and the Y values of these original data points as ğ‘Œğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™ .\nğ‘¦ğ‘–=1\nğ‘›ğ‘–âˆ‘ï¸\nğ‘Œğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™âˆˆğ‘–thintervalğ‘Œğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™ (6)\n(3,ğ‘¦!)(4,ğ‘¦\")\n(1,ğ‘¦#)(2,ğ‘¦$)(5,ğ‘¦%)EmptyFigure 10: Skewed data cases\nWe can rewrite the new sequence as {(ğ‘–,ğ‘¦ğ‘–),ğ‘–âˆˆğ‘}. In order\nto make our subsequent analysis more convenient, the starting\npoint of the entire sequence is set as (0,0), i.e. when ğ‘–=0,ğ‘¦ğ‘–=0.\nFor a precise CSM model, the original data points must be\ndistributed around the centers. If we know the distribution in\nadvance, we can precisely embed the distribution in the model.\nHowever, in a general case where we cannot make any assump-\ntions about data distribution, the number of intervals n should\nbe big so that the original data can be precisely approximated by\nthe CSM model. We use the midpoints of the intervals along the\nX axis, say[ğ‘šğ‘–,ğ‘šğ‘–+1], to create equally spaced sequences.\nB.3 Prerequisite of the efficiency of the model\nThe CSM model assumes that the data distribution on the pri-\nmary attribute (X-axis) is not extremely skewed and is close to\nuniform. Also, it assumes that the original data is large enough\nsuch that it can be accurately approximated by the interval model.\nTo measure how close the data distribution is to a uniform dis-\ntribution, we use the Kullback-Leibler (KL) Divergence. In our\ncase, letâ€™s denote the set of X coordinates of all points is ğ‘‹ğ‘¡, and\nthen we need to define a new set to store all unique values in ğ‘‹ğ‘¡\nthat are not repeated, and we call it the unique set of ğ‘‹ğ‘¡, denoted\nasğ‘‹ğ‘¢, and the number of unique values in the unique set ğ‘‹ğ‘¢is\nğ‘ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ . Since we have ğ‘ƒuniform,ğ‘–=1/ğ‘ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’,âˆ€ğ‘–âˆˆğ‘, and given\nğ‘–âˆˆ[1,ğ‘ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’],ğ‘‹ğ‘–âˆˆğ‘‹ğ‘¢,ğ‘ƒğ‘–=ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘‹ğ‘˜=ğ‘‹ğ‘–)/ğ‘ğ‘–,âˆ€ğ‘‹ğ‘˜âˆˆğ‘‹ğ‘¡,\nKL-divergence could be expressed as\nğ·ğ¾ğ¿(ğ‘ƒ||ğ‘ƒuniform)=ğ‘ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’âˆ‘ï¸\nğ‘–ğ‘ƒğ‘–ğ‘™ğ‘œğ‘”(ğ‘ƒğ‘–\nğ‘ƒuniform,ğ‘–) (7)\nKL-Divergence provides a distance between the distribution\nof the original points along the X axis and a uniform distribution.\nLet us denote the marginal distribution function of X axis is ğ‘ƒ(ğ‘‹)\nand the distribution function of a typical uniform distribution is\nğ‘ƒğ‘¢ğ‘›ğ‘–ğ‘“ğ‘œğ‘Ÿğ‘š(ğ‘‹). Whenğ‘ƒ(ğ‘‹)=ğ‘ƒğ‘¢ğ‘›ğ‘–ğ‘“ğ‘œğ‘Ÿğ‘š(ğ‘‹), their KL distance is\n0; otherwise, the greater the difference, the greater the distance.\nSince our model is under a nearly uniform distribution hypothesis,\nwe can test that when KL distance becomes smaller and closer to\n0, our model will perform better.\nFigure 10 shows a worst-case scenario for the CSM model to\nrepresent the original data sets. Since the data is skewed and the\nX values are far from a uniform distribution, some segments are\nempty, hence the CSM model will no longer be equally spaced.\n\nB.4 Relationship between new sequence and\noriginal one.\nThe goal is to show how far the original data points are from\nthese centers. So let us assume a distribution model, i.e., we can\nassume all original points follow the distribution ğ‘(ğ‘¦ğ‘–,ğœğ‘–), where\ntheğ‘¦ğ‘–is the corresponding centers of the original points. The\nidea is equivalent to making assumptions on the generation rules\nof the original sequence given the new sequence. Then we can\ndirectly define the noise bar/ margin bar of these centers and\nsay that all points are limited with high possibilities in a certain\nrange around their centers. Put formally,\nâˆ€ğ‘‹ğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™âˆˆ[ğ‘šğ‘–,ğ‘šğ‘–+1],ğ‘ƒ[ğ‘Œğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™âˆˆ[ğ‘¦ğ‘–âˆ’2ğœğ‘–,ğ‘¦ğ‘–+2ğœğ‘–]]=95%\n(8)\nLet us define ğ‘ğ‘ğ‘Ÿğ‘š=ğ‘šğ‘ğ‘¥ğ‘–(ğœğ‘–),ğ‘–âˆˆğ‘, then we can say it is\nvery possible (p = 95%) that almost all original data points are\nlimited in the[ğ‘ğ‘¥+ğœ€+2ğ‘ğ‘ğ‘Ÿğ‘š,ğ‘ğ‘¥+ğœ€âˆ’2ğ‘ğ‘ğ‘Ÿğ‘š]before the fist Exit\nTime of new sequence (i, ğ‘¦ğ‘–), iâˆˆN\nC PROOF OF THEOREM 7.1\nProof. Based on the CSM model and the reasonable assump-\ntions made, our data points should be limited by linear segments,\neach of which consists of two boundary lines. Given the i.i.d.\nincrements, the new sequence should be described as a plain sto-\nchastic process. When considering the expected numbers of keys\n(We call the values along Y-axis of the new sequence as \"keys\")\ncovered by a linear segment, we only need to find the Mean First\nExit Time (MFET) of the new sequence {(ğ‘–,ğ‘¦ğ‘–),ğ‘–âˆˆğ‘}.\nAs mentioned in [ 12], the Mean First Exit Time is defined from\nthe spatial domain ğ·asğ‘–âˆ—:=ğ‘–ğ‘›ğ‘“{ğ‘–â‰¥0,ğ‘‹ğ‘–âˆ‰ğ·}. Thus, in our\ncase, the MFET can be written as ğ‘–âˆ—=ğ‘–ğ‘›ğ‘“{ğ‘–âˆˆğ‘|ğ‘¦ğ‘–>ğ‘ğ‘–+ğœ€âˆ¨ğ‘¦ğ‘–<\nğ‘ğ‘–âˆ’ğœ€}\nTo make our formula more intuitive and concise, we use new\nsymbols to simplify the stochastic sequence using the approach\nsuggested by Ferragina et al. [ 7], where we consider ğ‘¦ğ‘–=Ãğ‘–\nğ‘—=1ğºğ‘—,\nand defineğ‘Šğ‘—=ğºğ‘—âˆ’ğ‘. Thus, we have ğ‘ğ‘–=ğ‘¦ğ‘–âˆ’ğ‘Â·ğ‘–=Ãğ‘–\nğ‘—=1(ğºğ‘—âˆ’\nğ‘)=Ãğ‘–\nğ‘—=1ğ‘Šğ‘—.\nThe statistical properties of the transformed stochastic pro-\ncessğ‘ğ‘–and its increments ğ‘Šğ‘–can be easily computed based on\nFerragina et al. [ 7], Also, following the Central Limit Theorem,\nequation 3.1.(3) [ 7], the conditions mentioned in Masoliver et\nal. [26] and Redner [ 31], we know that when ğœâ‰ªğœ€, the trans-\nformed stochastic process converges to a Brownian motion in\ncontinuous time.\nAccording to [ 12], for a driftless Brownian motion, the Mean\nFirst Exit Time (MFET) out of an interval [âˆ’ğ›¿/2,ğ›¿/2]isğ‘‡(ğ‘¥0)=\n(ğ›¿/2)2âˆ’ğ‘¥2\n0\nğœ2, whereğ‘¥0denotes the initial position.\nIn our case, ğ‘¥0=0andğ›¿=2ğœ€, thus we can easily conclude\nthatğ‘€ğ¹ğ¸ğ‘‡ =ğœ€2\nğœ2. The result means that for the new sequence,\nthe expected numbers of keys covered by a linear segment is\nğ‘€ğ¹ğ¸ğ‘‡ =ğœ€2\nğœ2. As we mentioned above, it is also reasonable to\nuse this result to estimate the expected number of data points\ncovered in the original Soft-FD model.\nâ–¡\nD PROOF OF THEOREM 7.2\nProof. Following the proof of Theorem 7.1, we want to ex-\npress the expected number of keys covered by a single linear\nsegment as a function of margin parameter ğœ€and the variance\nğ‘¦=ğ‘ğ‘¥+\tğœ€ğ‘¦=ğ‘ğ‘¥âˆ’\tğœ€(ğ‘–âˆ—,ğ‘¦\"âˆ—)(ğ‘–âˆ—,ğ‘\"âˆ—)ğ‘¦=ğœ€ğ‘¦=âˆ’ğœ€(a) An example random walk\n(ğ‘–âˆ—,ğ‘\"âˆ—)ğ‘¦=ğœ€ğ‘¦=âˆ’ğœ€\n(b) The corresponding transformed random walk\nFigure 11: Random walk illustration. The specific se-\nquence model here refers to the key-position sequence\nproposed by Ferragina et al. [7]. The difference between\nour model and the model in [7] is that for Soft-FDs we\ndo not need to exchange the coordinate plane of the new\nstochastic sequence constructed by the CSM model, and\nthe model is applicable for any unskewed value-value se-\nquences. (a) shows the sequence of the centroids of the in-\ntervals, which are estimated using the CSM method men-\ntioned in section B. We treat this sequence as a random\nwalk. (b) shows the transformed random walk mentioned\nin section 7.2, which is used in the stochastic analysis.\nof the gap distribution ğœ. It can be easily shown that the trans-\nformed stochastic process ğ‘ğ‘–has increments with mean ğ‘‘=ğœ‡âˆ’ğ‘\nand variance ğœ2. Based on prior analyses [ 7], when introducing\nthe drift coefficient, the MFET out of an interval [âˆ’ğ›¿/2,ğ›¿/2]for\na Brownian motion with drift coefficient ğ‘‘â†’0and diffusion\nrateğœcan be written as\nğ‘‡(0)=ğœ€\nğ‘‘ğ‘¡ğ‘ğ‘›â„(ğœ€ğ‘‘\nğœ2) (9)\nWhen we introduce our model and the slope ğ‘, it is easy to see\nthat the maximum of ğ‘‡(0)is achieved for ğ‘‘=0, i.e., whenğ‘=ğœ‡.\nâ–¡\nE PROOF OF THEOREM 7.3\nProof. Now that we have the expected value of the Mean\nFirst Exit time, we need to know the second moment of MFET to\ncompute the variance. Following Gardiner [ 12], Equation 5.2.140-\n156), the second moment ğ‘‡2(ğ‘¥)of the exit time of a Brownian\n\nmotion with diffusion rate ğœstarting at x is the solution of the\npartial differential equation âˆ’2ğ‘‡(ğ‘¥)=ğœ2\n2ğœ•2ğ‘‡2(ğ‘¥)\nğœ•ğ‘¥2.\nWhereğ‘‡(ğ‘¥)=(ğ›¿/2)2âˆ’ğ‘¥2\nğœ2 is the MFET. Thus, we have\nğ‘‡2(ğ‘¥)=âˆ«\nâˆ’4\nğœ2ğ‘‡(ğ‘¥)ğ‘‘ğ‘¥ (10)\nwith boundary conditions ğ‘‡2(Â±ğ›¿/2)=0. By solving for ğ‘‡2(ğ‘¥),\nthe second moment of the exit time is ğ‘‡2(0)=5ğœ€4\n3ğœ4when setting\nğ‘¥0=0andğ›¿=2ğœ€. Therefore,\nğ‘‰ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘ğ‘’ =ğ‘‡2(0)âˆ’[ğ‘‡(0)]2=2ğœ€4\n3ğœ4(11)\nâ–¡\nF PROOF OF THEOREM 7.4\nProof. Consider that one single linear segment cannot cover\nall of the data, so we can assume the total number of segments\nneeded to be s given a stream of stochastic sequence with length\nn. Suggested by Ferragina et al. [7], we similarly define\nğ‘ (ğ‘›):=ğ‘šğ‘ğ‘¥{ğ‘˜|ğ‘˜â‰¥1,ğ‘ .ğ‘¡.ğ‘†ğ‘˜â‰¤ğ‘›} (12)\nwhereğ‘†ğ‘˜=ğ‘–âˆ—\n1+ğ‘–âˆ—\n2+...+ğ‘–âˆ—\nğ‘˜.\nRecall that a renewal process is an arrival process in which the\ninterarrival intervals are positive, independent and identically\ndistributed (i.i.d.) random variables, we notice that {ğ‘ (ğ‘›)}ğ‘›â‰¥0is\narenewal counting process . Therefore, based on [ 6],E[ğ‘ (ğ‘›)]=\nğœ†ğ‘›+ğ‘‚(1)asğ‘›â†’âˆ ,ğ‘‰ğ‘ğ‘Ÿ[ğ‘ (ğ‘›)]=ğœ’2ğœ†3ğ‘›+ğ‘‚(ğ‘›)asğ‘›â†’âˆ , and\nğ‘ (ğ‘›)/ğ‘›â†’ğœ†. In our case, it holds:\n1\nğœ†=ğœ€2\nğœ2andğœ’2=2ğœ€4\n3ğœ4(13)\nHence, almost surely ğ‘ (ğ‘›)/ğ‘›â†’ğœ†=ğœ2\nğœ€2. Thus, we also have\nğ‘ (ğ‘›)â†’ğ‘›Â·ğœ2\nğœ€2Given this result, we conclude that the number of\nsegments s converges to ğ‘‚(ğ‘›/ğœ€2).\nâ–¡\nG COMPARISON WITH THE GRID INDEX\nTo understand the effect of the proposed index on memory foot-\nprint reduction, we compare it with a multidimensional index,\nspecifically a grid index. For the sake of simplicity, we compare\nthe Soft-FD index against a square grid and do not consider\nworkload-aware or distribution-based optimizations on grids.\nConsider a grid index with N dimensions. If the data has soft\nfunctional dependencies between some dimensions, we can par-\ntition N dimensions into primary andsoft-dependent attributes\nwhere for each attribute in the soft-dependent set, say Y, there\nis an attribute from the primary dimensions, say X, such that\nğ‘‹â†’ğ‘Œis a soft FD. Let us denote ğ‘˜ththe (ğ‘˜thpair of X-Y soft func-\ntional dependency. ğ‘ğ‘˜is the slope of the fitted curve of Soft-FD\nmodel.\nA grid index, on the other hand, creates an N-dimensional\narray of cells where each cell stores points for specific ranges in\neach dimension. Figure 12 shows how a 2D grid processes the\nrange query ğ‘¥âˆˆ[ğ‘™ğ‘¥,â„ğ‘¥],ğ‘¦âˆˆ[ğ‘™ğ‘¦,â„ğ‘¦]. The grid scans 14 cells for\nprocessing the query (empty cells still require a lookup). The soft\nFD index, however, translates the Y-axis constraints ğ‘¦âˆˆ[ğ‘™ğ‘¦,â„ğ‘¦]\nto X-axis constraints ğ‘¥âˆˆ[ğ‘™â€²ğ‘¥,â„â€²ğ‘¥]and the final constrains on the X-\naxis is the intersection of the two ranges ( [ğ‘™â€²ğ‘¥,â„â€²ğ‘¥]âˆ©[ğ‘™â€²ğ‘¥,â„â€²ğ‘¥]). In the\nexample shown in Figure 12, the X-axis range inferred from the\nY-axis,[ğ‘™â€²ğ‘¥,â„â€²ğ‘¥], is a smaller range than the actual constraints on\nğ‘‹which is[ğ‘™ğ‘¥,â„ğ‘¥]. Therefore, the soft-FD index scans a smaller\nâ„!ğ‘™!ğ‘!ğ‘‹\"#$%&ğ‘Œ\"#$%&ğ‘†!(soft-FD)ğ‘(â„(ğ‘™(X limits inferred by the soft-FD model)\nğ‘(â„(ğ‘™(ğ‘†GridActual query limitsFigure 12: An example Square-Grid. The area that the grid\nscans for the range query {(ğ‘¥,ğ‘¦)|ğ‘™ğ‘¥â‰¤ğ‘¥â‰¤â„ğ‘¥,ğ‘™ğ‘¦â‰¤ğ‘¦â‰¤â„ğ‘¦)}\nis marked with a red rectangle.\narea (ğ‘†ğ‘ ) than the grid ( ğ‘†Grid). In general, ğ‘†ğ‘ may be smaller or\nbigger than ğ‘†Griddepending on the query constraints, the size of\nthe grid cells, and the margins of the soft-FD index.\nWe analyze how reducing the dimensions affects the index\nstructures. We are interested to see \"how many cells does the\n2-D grid need to scan, compared to a soft-FD index that keeps\nthe records in a contiguous array, assuming that both indexes\nscan areas of roughly the same size\". Therefore, we consider a\ngrid index in which the number of cells is specifically chosen\nsuch that the area the grid scans equals a constant value t times\nthe scanned area in the Soft-FD model, i.e., ğ‘†ğºğ‘Ÿğ‘–ğ‘‘=ğ‘¡Â·ğ‘†ğ‘ . When\nt=1, it means we have exactly the same scanned area between\nthe Soft-FD and grid models.\nThe number of cells that a square grid index scans can be\nestimated as the ratio between the whole area and the scanned\narea.\nğ‘›ğ‘˜=ğ‘†ğ‘¤â„ğ‘œğ‘™ğ‘’\nğ‘¡Â·ğ‘†ğ‘ =(ğ‘Œğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’\nğ‘¡Â·ğœ€Â·ğ‘‹ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’\n(2ğœ€+ğ‘ğ‘¦)/ğ‘ğ‘˜) (14)\nWe defineğ‘Ÿğ‘˜as the ratio between length and the width of\ns-box:\nğ‘Ÿğ‘˜=Length(B-box)\nWidth(B-Box)=âˆšï¸ƒ\nğ‘Œ2ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’+ğ‘‹2ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’\n2ğœ€/âˆšï¸ƒ\n1+ğ‘2\nğ‘˜(15)\nThen, we can compute the scanned cells in grid as:\nğ‘›ğ‘˜=ğ‘‹ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’Â·ğ‘Œğ‘…ğ‘ğ‘›ğ‘”ğ‘’Â·ğ‘ğ‘˜\nğ‘¡Â·((1+ğ‘2\nğ‘˜)(ğ‘Œ2ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’+ğ‘‹2ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’)\nğ‘Ÿ2\nğ‘˜+ğ‘ğ‘¦(âˆšï¸ƒ\n1+ğ‘2\nğ‘˜)(âˆšï¸ƒ\nğ‘Œ2ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’+ğ‘‹2ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’\nğ‘Ÿğ‘˜)\n=ğ‘‹ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’Â·ğ‘Œğ‘…ğ‘ğ‘›ğ‘”ğ‘’Â·ğ‘ğ‘˜\nğ‘¡Â·(ğ‘“(ğ‘Ÿğ‘˜|ğ‘ğ‘˜,ğ‘‹,ğ‘Œ)2+ğ‘ğ‘¦ğ‘“(ğ‘Ÿğ‘˜|ğ‘ğ‘˜,ğ‘‹,ğ‘Œ))\nWhereğ‘“(ğ‘Ÿğ‘˜|ğ‘ğ‘˜,ğ‘‹,ğ‘Œ)=(âˆšï¸ƒ\n1+ğ‘2\nğ‘˜)(âˆšï¸ƒ\nğ‘Œ2ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’+ğ‘‹2ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’)/ğ‘Ÿğ‘˜is a\nfunction ofğ‘Ÿğ‘˜. Given the distribution of data points and a fixed lin-\near dependence, when ğ‘Ÿğ‘˜increases,ğ‘“(ğ‘Ÿğ‘˜|ğ‘ğ‘˜,ğ‘‹,ğ‘Œ)decreases, and\nthenğ‘›ğ‘˜increases. It can be easily imagined that when ğ‘Ÿğ‘˜â†’âˆ ,\nğ‘›ğ‘˜will get close to infinity as well. Thus, for K pairs of attributes\nin total (K is O(d), d denotes the total dimensions), we may need\nto createğ‘›1Â·ğ‘›2...Â·ğ‘›ğ¾=ğ‘›ğ¾cells in total. It can be easily con-\ncluded that if ğ‘Ÿğ‘˜is larger, i.e., the B-box has a narrow margin,\ntherefore and equivalent grid index needs a large number of cells\nto reach the almost same space cost as Soft-FD model.",
  "textLength": 56369
}