{
  "paperId": "7220b4431a46075cbe1a23a8ed12a88d1e0db643",
  "title": "Secretaries with Advice",
  "pdfPath": "7220b4431a46075cbe1a23a8ed12a88d1e0db643.pdf",
  "text": "arXiv:2011.06726v1  [cs.DS]  13 Nov 2020Secretaries with Advice\nPaul D¨ utting∗Silvio Lattanzi†Renato Paes Leme‡Sergei Vassilvitskii§\nAbstract\nThe secretary problem is probably the purest model of decisi on making under uncertainty. In this\npaper we ask which advice can we give the algorithm to improve its success probability?\nWe propose a general model that uniﬁes a broad range of proble ms: from the classic secretary problem\nwith no advice, to the variant where the quality of a secretar y is drawn from a known distribution and\nthe algorithm learns each candidate’s quality on arrival, t o more modern versions of advice in the form\nof samples, to an ML-inspired model where a classiﬁer gives u s noisy signal about whether or not the\ncurrent secretary is the best on the market.\nOur main technique is a factor revealing LP that captures all of the problems above. We use this\nLP formulation to gain structural insight into the optimal p olicy. Using tools from linear programming,\nwe present a tight analysis of optimal algorithms for secret aries with samples, optimal algorithms when\nsecretaries’ qualities are drawn from a known distribution , and a new noisy binary advice model.\n∗Google Research Z¨ urich, Switzerland, duetting@google.com\n†Google Research Z¨ urich, Switzerland, silviol@google.com\n‡Google Research New York, USA, renatoppl@google.com\n§Google Research New York, USA, sergeiv@google.com\n\n1 Introduction\nThe secretary problem captures one of the purest forms of decis ion making under uncertainty: ncandidates\narrive one at a time, and must either be accepted or rejected on th e spot. The goal is to design a stopping\npolicy that maximizes the probability of selecting the best candidate.\nSome additional information is necessary to ﬁnd policies that have pr ovable performance guarantees. A\nstandard assumption is that candidates arrive in random order, in w hich case one can achieve a 1/e≈0.36\ncompetitive ratio by hiring the ﬁrst candidate who is better than the ﬁrstn/eapplicants.\nHowever, pure random arrival is not the only plausible paradigm, and previous work considered other\ninformation augmentation schemes. For instance, Gilbert and Most eller [16] posit that each candidate has a\nqualityin[0 ,1], which isdrawnindependently fromsomeﬁxedand knowndistributio n. Gilbert andMosteller\nclaimed the optimality of a certain sequence of decreasing threshold s, and showed numerically that this leads\nto a winning probability of ≈0.58 [16]. Follow-up work formally showed that this policy is optimal and\nthat the winning probability of the optimal policy converges to e−c+(ec−c−1)/integraltext∞\n1x−1e−cxdx≈0.580164\nwherec≈0.804352 is the unique solution to/summationtext∞\nj=1cj/(j!j) = 1 [6, 17, 33].\nAnother natural form of advice are samples. Indeed there has be en a ﬂurry of recent work on stopping\nproblems, especiallytheprophetinequalityvariety, withlimitedinform ationaboutanunderlyingdistribution\nin the form of samples (e.g., [4, 9, 10, 11, 21, 32]). In a model popular ized by Kaplan et al. [21], for example,\nan adversary chooses k+nnumbers. A random subset of kof these numbers are revealed to the decision\nmaker (as samples) at the outset. Afterwards, the decision make r gets to inspect the remaining nnumbers\nin an online fashion, and in random order.\nWhileequallywellmotivatedfairlylittleisknownaboutthesesamplingmod elsforthesecretaryobjective.\nThe only exemption is an elegant paper by Correa et al. [9], which studie s a variant of the Kaplan et\nal. model with an additional independence assumption that serves t o increase the mathematical tractability\nof the problem. In this variant of the model, an adversary chooses nnumbers, and each number is marked\nindependently with probability pas a sample. In this setting, they show that the optimal policy for an yn\nandpis a threshold policy, obtain closed form solutions for thresholds whe nn→ ∞, and use this to recover\nthe optimal approximation guarantees of the classic no advice mode l of 1/e(whenp→0) and of the known\ndistributions setting of ≈0.58 (when p→1).\nA diﬀerent kind of information augmentation occurs frequently in re al job markets in the form of recom-\nmendation letters. Suppose each candidate comes with a letter tha t makes claims about the candidate being\nthe best in the pool. Obviously if the recommender is known to be 100% accurate, the problem becomes\neasy. However, if the recommender can be wrong with some probab ility, the choice of the optimal policy is\nless obvious.\nAll ofthese scenarios—assumptionson randomarrival, availabilityof quality scores, and recommendation\nletters—can be seen as a kind of “advice” given to the algorithm. In t his work we take a general view of\nthis problem, and explore secretary problems with advice.\n1.1 Our Contribution\nWe unify disparate information augmentation settings for the secr etary problem into a single framework that\nis rich enough to capture the classic random arrival model [13], the G ilbert-Mosteller i.i.d. setting [16], its\nMarkovian generalizations [2, 12, 18, 34], and the recently introduc ed sample-based variants [9, 21].\nIn all of these settings, previous workpainstakingly showed existe nce and optimality of threshold policies,\nwhich essentially specify the minimum hiring bar for every time step. We identify structural properties of\nthe advice that explain the optimality of such policies as well as give an a lgorithm for quickly computing\nthe optimum thresholds.\nTo demonstrate the utility of the framework we investigate a new ma chine-learning inspired advice for\nthis problem, prove that the optimal policy is a threshold policy, and g ive tight bounds on its performance.\nSetup Consider a setting where each secretary ihas a rank ri. In the generic case without ties, the ranks\nform a permutation of [ n] ={1,...,n}; we assume that higher is better so that ri=nis the secretary we\n1\n\nmax/summationdisplay\ni/summationdisplay\nsz(i,s)·a(i,s) s.t.\nz(i,s)≤1−/summationdisplay\nj<i/summationdisplay\ns′z(j,s′)·c(i,s,j,s′)\nz(i,s)≥0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglemin/summationdisplay\ni/summationdisplay\nsu(i,s) s.t.\nu(i,s)+/summationdisplay\nj>i/summationdisplay\ns′u(j,s′)·c(j,s′,i,s)≥a(i,s)\nu(i,s)≥0\nFigure 1: Factor revealing LP for secretaries with advice and its dua l.\nseek to hire. We pair each secretary with an abstract signal sifrom some signal space S, and assume that\nranks and signals are drawn from a known joint distribution F.\nThe crux of the problem is that although the algorithm knows this dist ribution, it cannot observe the\nrank of the arriving secretary directly. Instead, it can observe t he relative order of the secretaries that have\narrived so far, along with their signals, and use this information to de duce the likelihood of the current\nsecretary being the best.\nTechniques We identify two properties of signalling schemes, Non-Filtering (NF) andHistory-Irrelevance\n(HI),that allowus towrite downafactorrevealinglinearprogramthat qu antiﬁesthe performanceofthe best\npolicy. The two properties restrict the joint distribution, F, on the signals and the ranks. Roughly speaking,\nNon-Filtering implies that future signals do not change the informatio n state availableto the algorithm when\nprocessingaparticularcandidate. History-Irrelevancestatest hat, conditionedonthe currentcandidatebeing\nbest so far, the probability of the candidate being globally optimum is in dependent of previously seen signals.\nWe describe these precisely in Section 2.2.\nWe show the factor revealing LP and its dual in Figure 1. The linear pro gram is a generalization of the\none proposed by Buchbinder et al. [7] for the classic secretary pro gram, with additional considerations to\ncapture the eﬀect of the signals. In the primal, the variable z(i,s) denotes the probability of accepting a\ncandidate with signal sat timeiprovided that it is the best candidate seen thus far. The coeﬃcient sa(i,s)\nandc(i,s,j,s′) depend only on the joint distribution, F. In particular, a(i,s) is the probability of seeing the\ntop rank element in position iwith signal si, andc(i,s,j,s′) is the probability that the candidate in position\njwith signal s′is the best candidate thus far, given that candidate in position i > jwith signal sis also\nlocally optimum.\nNext, we characterize the policies captured by our linear program f ormulation. Here we show that under\nthe NF and HI restrictions any stopping policy is captured by the prim al, and any solution to the primal\ncan be converted to a stopping rule.\nProposition A (Restatement of Proposition 9) .For any policy for the secretary problem with a signaling\nscheme satisfying NF and HI there is a set of values z(i,s)≥0such that the objective of the factor revealing\nlinear program corresponds to the probability that the high est ranked secretary is selected.\nThe converse is also true:\nProposition B (Restatement of Proposition 10) .For any signaling scheme satisfying NF and HI, given\nfeasible values z(i,s)≥0there is a policy for the secretary problem which picks the hi ghest ranked secretary\nwith probability equal to the objective of the factor reveal ing linear program.\nWe delve deeper to understand the kinds of optimal policies generat ed by the LP. Let memoryless policies\nbe those that make the hiring decision at time ibased only on the signal of the secretary i, andnoton the\nsignals of previous candidates. Memoryless policies are a natural cla ss, since their implementation does not\nrequire tracking previously seen signals, and can be computed in con stant space.\nWe prove that any policy captured by the LP in Figure 1 is a memoryless policy.\nTheorem C (Restatement of Theorem 8) .If a signaling structure satisﬁes Non-Filtering and Histor y-\nIrrelevance, then the optimal signaling policy is a memoryl ess policy.\n2\n\n1/e0.58\n200 100kP(winning)1/e0.58\n1p\nFigure 2: Performance of the optimal policy for secretary with sam ples. Both plots are for n= 20. On\nthe left, the x-axis shows kand the y-axis shows the probability of winning. On the right, the x-axis is\np=k/(k+n) and the y-axis is probability of winning.\nAn important subclass of memoryless policies are threshold policies. These policies associate an earliest\nacceptance time for every signal, and accept the ﬁrst locally optimu m candidate whose signal passes the test.\nWe characterize algorithmically when threshold policies are optimal un der HI and NF: They are optimal\nprecisely when the dual to the linear program can be solved optimally b y the greedy (backward induction)\nalgorithm and satisﬁes a natural monotonicity assumption.\nProposition D (Restatement of Proposition 12) .Assume NF and HI. If the greedy solution u(i,s)for\ni∈Nands∈ Sconstructed via backwards induction solution is an optimal solution to the dual LP and for\nalls∈S,u(i,s)is non-decreasing in i, then the optimal policy is a threshold policy.\nand\nProposition E (Restatement of Proposition 13) .Assume NF and HI. If the optimal policy is a threshold\npolicy, then the greedy backwards induction solution is opt imal for the dual LP.\nWe also show that the monotonicity assumption in Proposition D is requ ired: there are cases where the\ngreedy algorithm leads to an optimal solution, but the solution violate s monotonicty, and the optimal policy\nis not a threshold policy.\nA key advantage of our approach is that it allows us to characterize the family of optimal policies fairly\neasily, in contrast with previouswork where ad hoc lowerbounds tec hniques were required. Furthermore, our\nfactor-revealing linear program allows us to derive explicitly optimal p olicies and their success probability\nfor several problems. We can obtain such results by a careful bac kward induction argument on the dual\nof our factor-revealing linear program. For example, in this way we c an recover the optimal policy for the\nGilbert and Mosteller setting, and for our binary setting with advice.\nApplications We highlight some of the results that can be derived using our framew ork.\nFirst we focus on secretary algorithms for the sampling model of Ka plan et al. [21]. In this model an\nadversary chooses n+knumbers, a random subset of kof these numbers are presented to the algorithm\nassamples at the outset, the remaining nnumbers are presented to the algorithm one-by-one, in random\norder. This model smoothly interpolates between the classic secre tary problem with no advice ( k= 0) and\nthe known distributions setting of Gilbert and Mosteller ( k→ ∞).\nIn Section 5 we show how to capture this problem within our LP approa ch; as a consequence we obtain\nan LP formulation for the Gilbert-Mosteller model. We use the dual to show the optimality of thresholding\npolicies for all number of samples kand all numbers of candidates n. The dual then has the interpretation\nthat theu(i,s) variablesare the contribution to the optimal winning probability fro m secretary iwhen seeing\nsignals. The dual also gives rise to an eﬃcient poly-time algorithm for comput ing the optimal policy and\nthe corresponding winning probability. See Figure 2 for an exemplary set of these bounds.\nFurthermore, we obtain exact solutions to the dual for all nandk, and thus the thresholds used by the\noptimal policy. These can be used to derive exact analytic expressio ns of the optimal winning probability\nin the asymptotic regime when n→ ∞. We do this for the Gilbert-Mosteller setting, and thus recover the\naforementioned analytic expression of the winning probability.\n3\n\np 0.5 11/e\n01Competitive Ratio\np 0.5 1n/e\n0ntN\ntY\nFigure 3: Left plot shows the performance of the optimal secreta ry with advice policy as a function of p.\nThe right plot shows the thresholds tYandtNas a function of p.\nOur results for this setting strengthen and extend the results of both [9] and [16]. We use our framework\nto give an LP-duality based proof of the optimality of thresholding fo r all values of nandk(without the\nsimplifying independence assumption of [9]), we obtain an eﬃcient (po ly-time) algorithm for computing the\noptimal policy and winning probability for all values of nandk, and derive closed formulas for the thresholds\nused by the optimal policy in both the asymptotic and non-asymptot ic regime.\nWe then give an example of a new kind of advice that can be easily analyz ed in this framework. Suppose\neach secretary arrives with a binary signal indicating whether the c andidate is globally optimum, but the\nsignal is incorrect with some probability 1 −p. Anecdotally such a signal can model recommendation letter\nwriters that claim that a candidate is “best in their class.”\nFormally, we consider the random order arrival model, and restrict the signal space to S={Y,N}. For\nthe best candidate the signal is Ywith probability p; for each other candidate the signal is Nalso with\nprobability p. In Section 6 we consider the extended case where the probability o f error is diﬀerent for two\nclasses, capturing potential false positive and false negative trad e-oﬀs in a real life classiﬁer.\nWe show that the optimum solution in this setting is a threshold policy wit h two thresholds:\ntY≈n/parenleftbigg1\np−1/parenrightbigg1/p\ne1−1/pandtN≈ne1−1/p.\nIn other words, the optimum policy waits the ﬁrst tYsteps; then accepts any candidate with a Ysignal\nthat is better than all previous candidates until time tN, and then accepts any candidate that is better\nthan all already rejected candidates. Note that when p=1/2, and thus the signal provides no additional\ninformation, we recover Dynkin’s classic policy. However, as p grows , the thresholds tYandtNdiverge, and\nthe competitive ratio grows to ≈(1/p−1)(1/p−1)·e(1−1/p).\nWe plot both the growth of the competitive ratio as well as the two th resholdtYandtNin Figure 3.\n1.2 Related Work\nOurworkiscloselyrelatedtothreebroadactiveresearchdirection s. First,ourproblemformulationisinspired\nand aligned with the nascent area of algorithms with machine-learned advice. Second, our technique can\nbe seen as a unifying framework that emphasizes the role of advice in the secretary and prophets literature.\nFinally, on a technical level, our work is related to the literature using factor revealing LPs in approximation\nand online algorithms. In the following we brieﬂy review the most relate d papers in those three areas.\nAlgorithms with ML Advice Traditionally, the design and analysis of algorithms has focused on pr ov-\nable guarantees for worst-case inputs. A growing body of work ex plores how “machine learned advice” can\nbe leveraged. Thanks to its practical applications several problem s have been studied through this lens.\n4\n\nExamples range from building better data structures [22, 28], to imp roved competitive and approximation\nratios for several online tasks [23, 24, 26, 29, 30, 31], to cases w here advice has been used to speed-up algo-\nrithms [1, 5] or to reduce their space complexity [19]. Our work can b e seen as a formalization of the classic\nsecretary problem in this general framework.\nSecretaries and Prophets Literature The secretary problem is one of the most studied problems in\nonline optimization. The classic formulation of the problem, introduce d by Dynkin [13], succeeds with\nprobability approaching 1 /e. The guarantee of 1 /eis known to be best possible, even when the values of the\nsecretaries are drawn from an unknown distribution (e.g., [15]).\nThe same objective has also been studied in the setting where the ncandidates are i.i.d. draws from\na known distribution (which can w.l.o.g. be the uniform distribution on [0,1 ]) [6, 16, 17, 33]. The optimal\nalgorithm sets a decreasing sequence of thresholds, and it can be s hown that it accepts the best secretary\nwith probability at least ≈0.58. A recent paper by Esfandiari et al. [14] has considered a non- i.i.d variant\nof this problem, and shows how to obtain a 1 /eapproximation with a single threshold; and provides an\nexample of non-i.i.d. distributions where this is best possible.\nAnother popular generalization of the Gilbert-Mosteller model (bec ause of its application in ﬁnance) are\nrandom walk models. An early example is [18]. More recent results inclu de [2, 12, 34]. The main take away\nfrom this line of work is known as the “bang-bang principle”: if the ran dom walk is balanced then you might\nas well stop immediately, if it’s biased upward you should wait until the e nd, if it’s biased downwards you\nshould accept immediately.\nAnother interesting line of work in this area studies the secretary a nd prophet problems in the presence\nof a limited number of samples [9, 10, 11, 21, 32]. Most relevant in this c ontext are the Kaplan et al. paper\n[21], which is the model we adopt here, and the paper by Correa et al. [9] as it is the only prior work that\nlooks at the secretary objective.\nOur work oﬀers a unifying lens that captures all these problems as s ecretary problems with advice; and\nextends the known LP formulation for the classic secretary proble m to all of the other problems. It in\nparticular enables structural insights about the form the optimal policy takes, e.g, when and why backward\ninduction yields optimal solutions.\nFactor Revealing LPs Factor revealing LPs have been used in a number of algorithmic analys es. They\nwere introduced in the context of designing approximation algorithm s for the facility location problem, in\nconjunction withthe dual ﬁttingtechnique [20]. Thetechnique hasb een extended tostronglyfactorrevealing\nLPs by [25], who used it to analyze the KVV ranking algorithm for bipart ite matching in the random order\nmodel. Another variant called tradeoﬀ revealing LPs was introduced in [27] to analyze a greedy algorithm\nfor the the AdWords problem.\nIn the context of secretary problems, there are two main precur sors: The ﬁrst is [7], which describes a\nLP that recovers the optimal 1 /eapproximation guarantee for the classic secretary problem. The s econd\none is [8], which extends this formulation to the ( j,k)-secretary problem. In this variant of the problem the\nalgorithm is allowed to retain jelements and the goal is to maximize the expected number of element s that\nare among the kbest secretaries.\nOur LP formulation is inspired by [7], but is much more general, and in par ticular enables—for the ﬁrst\ntime—a uniﬁed treatment of the two classic secretary problems thr ough the lens of LPs.\nAdditional Related Work In parallel to this work, Antonianidis et al. [3] have considered the va lue\nmaximization variant of the secretary problem with advice (so a diﬀer ent objective than we consider here).\nThe techniques and results of that paper are very diﬀerent from t hose in this paper.\nThey consider, for example, the single choice problem, and as advice the anticipated quality ρ∗of the\nbest secretary. They seek bounds that are at least α >1/ewhen the advice is accurate (as measured by the\nEuclidean distance) and at least β <1/ewhen the advice is inaccurate. They obtain qualitatively similar\nresults for more general combinatorial allocation problems, such a s bipartite matching.\n5\n\nThey do not characterize optimal policies and how their performanc e decays as the advice gets worse,\nand they also don’t provide a general framework for studying diﬀer ent forms of advice.\n2 Model of Secretaries with Advice\nIn the original secretary problem, the algorithm’s goal is hiring the b est secretary from a set of ncandidates.\nThere is a total order on the candidates which is not known in advanc e. Candidates arrive in random order,\nand upon arrival, the algorithm is able compare the candidate with eac h of the previously seen options. The\nalgorithm must then irrevocably decide to either hire that secretar y or pass, in which case that secretary is\nno longer available.\nMathematically, we can describe the problem as follows: Let ( r1,...,r n) be a permutation of [ n] =\n{1,...,n}. We refer to rias therankof thei-th arriving secretary. We say that i/{ollowsequalj(i.e. secretary iis at\nleast as good as j) whenever ri≥rj. Note that the best secretary has value n. The algorithm has no access\nto the ranks. Instead, at time i, it can only see the relative comparisons i/{ollowsequaljand/ori/precedesequaljforj < i. The\ngoal is to maximize the probability with which we stop at the maximum.\nWhile we described the model without the possibility of ties, in some of o ur applications it will be natural\nto allow for ties. In that case we will assume that ( r1,...,r n) is a vector with ri∈[n], and we will require\nthatri=nfor at least one secretary i. Our goal will then be to stop at an isuch that ri=n(of which\nthere may be more than one).\nWe will use /precedesequal1..ito represent the partial order induced on the ﬁrst ielements.\nBest-so-far Event We deﬁne a probabilistic event that will play a key role in the analysis and deﬁnitions\nbelow. Let the best-so-far eventTibe:\nTi={i/{ollowsequalj;∀j < i} (1)\nIt is important to observe that since the goal of the algorithm is to p ick the best secretary, one can assume\nwithout loss of optimality that the algorithm only picks secretary iif it is the best-so-far.\nAdvice We augment the secretary problem with an extra signal sifor each arriving secretary. Let Sbe\nthe space of signals. We assume that ( r1,r2,...,r n,s1,...,s n) is drawn from a known joint distribution. In\nthe no ties case, r1,...,r nwill just be a permutation of [ n] andsi∈ Sfor alli. With ties, we require that\nri∈[n],si∈ S, andri=nfor some i.\nIn each period ithe algorithm observes both the signal siand the relative comparisons i/{ollowsequaljori/precedesequaljfor\nj < i. The algorithm knows the joint distribution of ranks and signals, but it cannot observe ranks directly,\nit is limited to computing induced ranks.\nThe algorithm decides in each step iwhether to stop or proceed. As before, the goal of the algorithm is\nto select the best candidate. Note that if there are two secretar ies with the top rank, we can pick either one.\n2.1 Examples of Secretary Problems with Advice\nIt is useful to keep some concrete examples in mind:\nExample 1 (Secretaries without advice [13]) .IfS={0}and(r1,...,r n)are distributed as a random\npermutation, we are back at the original secretary problem. The best optimal strategy for this problem\n(Dynkin’s algorithm) ﬁnds the optimal secretary with proba bility1/e≈0.37.\nExample 2 (Gilbert-Mosteller [16]) .Consider a ﬁxed known distribution Fover the real numbers. Let\nS=R,siis an independent sample from Fandrirepresent the ranks induced by si, i.e.,ri=kifsiis\nthek-th largest value of among (s1,...,s n). This stochastic version of the secretary problem is studie d by\nGilbert and Mosteller who show that with this extra informat ion the algorithm can hire the best secretary\nwith probability ≈0.58.\n6\n\nA non-i.i.d. version of Gilbert-Mosteller model was studie d by Esfandiari et al. [14] who show that when\nsi∼Fithen it is possible to choose the optimal secretary with prob ability1/e; and this is best possible in the\nworst case. (Note that this does not follow from Dynkin’s alg orithm since the ranking induced by the random\ndraws is no longer uniform random.)\nExample 3 (Markovian stopping) .A generalization of the Gilbert-Mosteller setting is the fo llowing Marko-\nvian stopping problem: consider a Markov chain on space S, i.e., a stochastic process s1,...,s nwhere\nP[si|s1,...,s i−1] =P[si|si−1]and letribe the ranks induced by signals assuming there is a total orde ring\ndeﬁned on S.\nMarkovian stopping problems are popular in ﬁnance, where th ey serve as proxies for investment problems.\nHlynka and Sheahan [18], for example, study a simple “symmet ric” random walk. The process starts on day\nzero with a reward of zero. Then on each of ndays, with equal probability, either the reward is increase d by\none or it is decreased by one. The goal is to maximize the proba bility with which the process is stopped at\nthe maximum reward of all days. They show that all strategies that skip a ﬁxed number of t≥0days, and\nthen accept the ﬁrst reward from day t+1onwards that is the highest so far actually achieve the exact same\nwinning probability.\nSubsequent work has identiﬁed this as the indiﬀerence case o f what has become to be known as the “bang\nbang principle” [2, 12, 34]: In a random walk that is started a t zero and in which the probability to move up\nby one is pand the probability to move down by one is 1−pit is best to stop immediately when p <1/2and\nto wait until the end when p >1/2.\nExample 4 (Secretaries with Samples [9, 21]) .An adversary writes down n+knumbers. A random subset\nof sizekis chosen and revealed to the algorithm as samples. Afterwar ds, the remaining nnumbers are\npresented to the algorithm in an online fashion, in random or der.\nThe algorithm can observe the relative order of all secretar ies it has seen so far. So in addition to\nobserving the best so far event Ti, the algorithm learns about the relative rank of the current secretary among\nthe samples. The signal space is thus S={0,...,k}, wheresi=jmeans that jof the samples are worse\nthan the current secretary.\nThe rank riof a hirable secretary is its relative rank among the nhirable secretaries. So ri=nis the\nrank of the best secretary, and ri= 1is the rank of the worst secretary.\nExample 5 (Binary Classiﬁer) .A natural ML-advice model is a classiﬁer that given a secreta ry predicts\nwhether it is the best secretary or not. The input to the machi ne learned system is a candidate with all of\ntheir features, and the output is a binary classiﬁcation: Y( es) or N(o).\nIn ML it’s common practice to evaluate the quality of a binary classiﬁer using the following four metrics\naccuracy, precision, recall, and speciﬁcity . It will be more convenient for us to express our results in te rms\nofrecall(a.k.a.sensitivity ) andspeciﬁcity . Such metrics are depicted in Figure 4 as a function of True\nPositive (TP), True Negative (TN), False Positive (FP) and F alse Negative (FN).\nFNTP\nTNFPActual\nY NPredictedY\nNRecall:p=TP\nTP+FN\nSpeciﬁcity: p′=TN\nFP+TN\nFigure 4: Parameters of the binary classiﬁer\nThis translates to a signal space S={Y,N}and two parameters p,p′∈[1\n2,1]corresponding to recall and\n7\n\nspeciﬁcity respectively. We will assume that (r1,...,r n)is a random permutation and that:\nP[si=Y|ri=n] =p P[si=N|ri=n] = 1−p\nP[si=Y|ri=r] = 1−p′P[si=N|ri=r] =p′\nfor every r < n. Essentially the advice suggest to hire the best secretary w ith probability pand suggest to hire\nany other secretary with probability 1−p′. Whenp=p′=1\n2we are back in the original secretary problem\nsince the signal is clearly useless. When p=p′= 1the ML model is perfect and we can hire the optimal\nsecretary by simply following its suggestion, i.e., hiring whenever it says Yand not hiring otherwise.\n2.2 Signal Structure Properties\nThe joint distribution in all of the examples we described above has tw o properties that will be key to our\nanalysis: Non-Filtering andHistory Irrelevance . As we note below in Example 11 these are not universal\nand rule out certain types of advice.\nDeﬁnition 6 (Non-Filtering) .We say that a signaling structure satisﬁes Non-Filtering (N F) if given any\nj < iwe have:\nP[/precedesequal1..j,s1,...,s j|Tj,sj] =P[/precedesequal1..j,s1,...,s j|Tj,sj,Ti,si]\nwhereTiis the best-so-far event deﬁned in equation (1).\nThe property essentially means that if at both iandjwe see secretaries that are the best-so-far the\ninformation we see later in idoes not aﬀect the conditional distribution of information the algor ithm gets\nat timej.\nNote that it holds in Examples 2, 3 simply by the Markovian property: c onditioned on Tj,sj, we have\nthat (/precedesequal1..j,s1,...,s j) and (sj,sj+1,...,s n) are independent. Since si,Tiis a function of ( sj,sj+1,...,s n),\nproperty NF automatically follows. A similar, but more delicate argume nt which we defer to Section 5, can\nbe used to argue that Example 4 also satisﬁes NF. In Example 5 if P′(N) =p′andP′(Y) = 1−p′we have:\nP[/precedesequal1..j,s1,...,s j|Tj,sj,Ti,si] =1\n(j−1)!j−1/productdisplay\nt=1P′(st)\nsince after we know that jis the best so far, each element before jcannot be the top ranked element and\nhence must have its signals drawn from P′.\nNon-Filtering captures the eﬀect of future arrivals on the informa tion state available to the algorithm.\nIn contrast, history irrelevance imposes structure on the relatio nship between signals previously observed.\nDeﬁnition 7 (History-Irrelevance) .We say that a signaling structure satisﬁes History-Irrelev ance (HI) if\nconditioned on (Ti,si)the variable (≺1..i,s1,...,s i)representing the information available at round iand\nthe event ri=nare independent.\n(≺1..i,s1,...,s i) (Ti,si) ri=n\nFigure 5: Graphical model representing property HI\nIn graphical model language, we can say that the variables and eve nt above satisfy the graphical model\nin Figure 5. It is useful to observe some implications of this fact. The ﬁrst implication is that in order to\ndetermine the probability that each iis the highest ranked secretary, it is enough to look at its signal and\nto whether it is the highest ranked so far:\nP[ri=n|≺1..i,s1,...,s i] =P[ri=n|Ti,si] (2)\n8\n\nA second implication is that if Xis an event that depends only on the the information available at perio di\n(i.e., depends only on ( ≺1..i,s1,...,s i)) then:\nP[X|si,ri=n] =P[X|Ti,si] (3)\nsinceri=nimpliesTi.\nAgain it is simple to see that all the example discussed satisfy it. For Ex amples 2 and 3 it again follows\nfrom the same argument: ( /precedesequal1..j,s1,...,s i) and (si,si+1,...,s n) are conditionally independent given Ti,si\nandri=ndepends only on ( si,si+1,...,s n) whenever Tiholds. For Example 5 we can explicitly compute\nthe probability P[ri=n|/precedesequal1..i,s1,...,s i] as follows: if iis not the best so far, then this probability is zero.\nSubject to Tithe probability isp\np+(n−i)(1−p′)ifsi=Yand1−p\n1−p+(n−i)p′ifsi=N. Hence, conditioned on\n(Ti,si) the probability of ri=nis independent of ( /precedesequal1..i,s1,...,s i). For Example 4 we again defer the\ndiscussion to Section 5.\n3 Memoryless Policies and LP Formulation\nNon-Filtering (NF) and History-Irrelevance(HI) are naturalproperties of a signallingscheme. In this section\nwe investigate which policies are optimal under Non-Filtering (NF) and History-Irrelevance (HI).\nMemoryless Policies A generic policy is a map from the available information at period i, which consists\nof (/precedesequal1..i,s1,...,s i) to a stopping probability. In general the optimal generic policy can be quite complicated,\nspecially when there are lots of correlations among signals. An intere sting class of policies are the one where\nthe decision is based on the advice given for the current secretary only and where the decision does not\ndepend on the relative order among the ﬁrst i−1 elements, but instead depends only on whether iis the\nbest-so-far or not, i.e., whether we are in the event Ti. Putting it all together, we say that a memoryless\npolicyis a map that outputs the probability of stopping in each round iconditioned on Tiandsi.\nMemoryless policies are well-motivated and a natural assumption in ML applications. The resulting\npolicies are simpler, and require less space. They oﬀer increased priv acy and may even be a legal necessity\n(e.g., when data protection laws regulate which data may be stored a nd for how long).\nUnder the following natural conditions (satisﬁed by Examples 1, 2, 3 , 4, and 5) the optimal signaling\npolicy is memoryless:\nTheorem 8. If a signaling structure satisﬁes Non-Filtering and Histor y-Irrelevance, then the optimal sig-\nnaling policy is a memoryless policy.\nEven under these nice conditions, it is not trivial to show the theore m above. As one tries to modify a\ngeneric policy to make it be oblivious to the history, one needs also to a ccount for probability of not having\nstopped at any given point which is complicated to track.\nLP Formulation and Proof of Theorem 8 The main tool we will use is a linear programming formula-\ntion for the secretary problem with advice subject to the NF and HI properties. Our formulation generalize\nthe linear program of Buchbinder, Jain, and Singh [7] to our more cha llenging setting.\nAssumethesignalingstructuresatisﬁesNFandHIandconsiderage nericpolicymapping( /precedesequal1..i,s1,...,s i)\nto a stopping probability for each i. For each i∈[n] ands∈ S, we deﬁne:\nz(i,s) =P[iis picked |Ti,si=s]\nWe furthermore deﬁne for each i∈[n] ands∈ S,a(i,s) =P[ri=n,si=s] and for each i,j∈[n] withi > j\nands,s′∈ S,c(i,s,j,s′) =P[Tj,sj=s′|Ti,si=s].\nWe will show that if z(i,s) is “feasible” (to be deﬁned soon), then it contains all the relevant data to\nreconstruct the policy.\nWhich constraints must z(i,s) satisfy in order for us to be able to recover a policy from it? Besides the\ntrivial requirement that z(i,s)≥0 the other requirement we will ask is that there is enough probability left\n9\n\nto choose iwheniarrives. Even if iis the top among the ﬁrst ielements, an earlier element may have been\nchosen preventing us from choosing ilater on. In other words, we have:\nP[iis picked |Ti,si=s]+/summationdisplay\nj<iP[jis picked |Ti,si=s]≤1.\nSince each j < iis only picked in the event Tj(jis the best so far) then we can re-write the expression\nabove using the law of total probability as:\nP[iis picked |Ti,si=s]+/summationdisplay\nj<i/summationdisplay\ns′∈SP[jis picked |Tj,sj=s′,Ti,si=s]P[Tj,sj=s′|Ti,si=s]≤1.\nNow we can apply property NF to argue that:\nP[jis picked |Tj,sj=s′,Ti,si=s] =P[jis picked |Tj,sj=s′].\nSince the fact that jis picked depends only on ( /precedesequal1..j,s1,...,s j) and the conditional distribution of ( /precedesequal1..j\n,s1,...,s j) is the same given Tj,sjorTj,sj,Ti,siwe obtain the last display equation. Substituting it above\nand replacing the deﬁnition of z(i,s) andc(i,s,j,s′) we obtain:\nz(i,s)≤1−/summationdisplay\nj<i/summationdisplay\ns′∈Sz(j,s′)·c(i,s,j,s′). (4)\nIt is important to note that the term c(i,s,j,s′) =P[Tj,sj=s′|Ti,si=s] depends only on the joint\ndistribution of ( r1,...,r n,s1,...,s n) and not on the policy itself.\nWe can also write the performance of the policy in terms of the z(i,s) variables:\nObj=n/summationdisplay\ni=1E/precedesequal1..i,s1,...,si[P[iis picked ∧ri=n|/precedesequal1..i,s1,...,s i]]\n=n/summationdisplay\ni=1E/precedesequal1..i,s1,...,si[P[iis picked |/precedesequal1..i,s1,...,s i]·P[ri=n|/precedesequal1..i,s1,...,s i]]\n=n/summationdisplay\ni=1E/precedesequal1..i,s1,...,si[P[iis picked |/precedesequal1..i,s1,...,s i]·P[ri=n|Ti,si]]\n=n/summationdisplay\ni=1Esi[P[iis picked |Ti,si]·P[ri=n|si]],\nwhere the ﬁrst equality follows from independence, the second fro m HI (equation (2)) and the third follows\nfrom the law of conditional probability. Substituting z(i,s) anda(i,s) we get:\nObj=n/summationdisplay\ni=1/summationdisplay\ns∈Sz(i,s)·a(i,s). (5)\nWe note again that the term a(i,s) =P[ri=n,si=s] depends only on the signaling structure and not on\nthe policy itself. We showed the following statement:\nProposition 9. For any policy for the secretary problem with a signaling sch eme satisfying NF and HI\nthere is a set of values z(i,s)≥0satisfying (4)such that the objective in equation (5)corresponds to the\nprobability that the highest ranked secretary is selected.\nThe converse is also true:\nProposition 10. Assume again that the signaling structure satisﬁes NF and HI . Given values z(i,s)≥0\nsatisfying (4)then there is a policy for the secretary problem which picks t he highest ranked secretary with\nprobability equal to the objective deﬁned in (5).\n10\n\nProof.Consider the policy that upon seeing secretary iwith signal schooses that secretary with probability:\nq(i,s) =z(i,s)\n1−/summationtext\nj<i/summationtext\ns′∈Sz(j,s′)·P[Tjandsj=s′|Tiandsi=s](6)\nifiis the best-so-far (in other words, if Tihappens) and zero otherwise. Now we need to argue that the\nprobability that this policy chooses the highest ranked secretary is equal to the objective in equation (5).\nBefore we do that, we show that under the reconstructed policy, the probability that we pick the i-th\nsecretary conditioned on Ti,si=sis indeed z(i,s). We will show that recursively. Assume for now it is true\nfor allj < i. Under the constructed policy the probability that we pick iis the probability that we reach\nthat step without picking any of the previous secretaries times the probability we choose iat that step:\nP[iis picked |Ti,si=s] =P[reach step i|Ti,si=s]·q(i,s)\n=/parenleftbigg\n1−/summationdisplay\nj<iP[jis picked |Ti,si=s]/parenrightbigg\n·q(i,s).\nWe can now use the induction hypothesis to evaluate the probability t hatjis picked:\nP[jis picked |Ti,si=s] =/summationdisplay\ns′∈SP[jis picked |Tj,sj=s′,Ti,si=s]·P[Tj,sj=s′|Ti,si=s]\n=/summationdisplay\ns′∈SP[jis picked |Tj,sj=s′]·P[Tj,sj=s′|Ti,si=s]\n=/summationdisplay\ns′∈Sz(j,s′)·P[Tj,sj=s′|Ti,si=s],\nwhere the last equality follows from the induction hypothesis. Now ta king the two previous display equations\ntogether and substituting the formula for q(i,s) we obtain that:\nP[iis picked |Ti,si=s] =z(i,s).\nEquipped with that we can now bound the performance of the policy:\nAlg=n/summationdisplay\ni=1P[ri=n]·/parenleftBigg/summationdisplay\ns∈SP[si=s|ri=n]·P[iis picked |si=s,ri=n]/parenrightBigg\n.\nSince the probability that the algorithm picks idepends only on ( /precedesequal1..i,s1,...,s i) we can use property HI\n(equation (3)) to get that:\nP[iis picked |si=s,ri=n] =P[iis picked |Ti,si=s].\nPutting it all together we get that:\nAlg=n/summationdisplay\ni=1P[ri=n]·/parenleftBigg/summationdisplay\ns∈SP[si=s|ri=n]·z(i,s)/parenrightBigg\n=Obj,\nas claimed.\nWe now can show the proof of Theorem 8 as a corollary:\nProof of Theorem 8. Given any policy use Proposition 9 to obtain variables z(i,s). Now use Proposition 10\nto convert this back to a policy. Observe that the policy we obtain fr om this transformation uses only si,Ti\nwhen deciding whether to stop. Hence, it is an memoryless policy.\n11\n\nSince all of the examples in Section 2 satisfy the NF and HI conditions, Theorem 8 implies that they all\nhave an LP formulation, and moreover, they all have optimal memor yless policies.\nWe conclude this section with an example in which the signalling scheme do es not satisfy NF and HI,\nand the optimal policy is not memoryless.\nExample 11. Fix some integer mbetween1andnand consider two signals S={T,B}such that si=Tif\nri> mandsi=Botherwise. In other words, the signal indicates whether we a re in the top (T) or bottom\n(B) of the distribution. This structure violates NF.\nThe optimal policy in that case is obvious: ignore all bottom elements and treat the top elements as a\nstandard instance of the secretary problem with n−melements. Dynkin’s policy on the reduced instance\nwould say that we don’t pick until we have seen at (n−m)/etop elements and after that we pick the best so\nfar. Such policy is not memoryless as it needs to remember how many top elements appeared up to a certain\npoint.\n4 Optimality of Threshold Policies\nWe showed that under NF and HI the optimal policy is memoryless. Her e we investigate when the optimal\nstrategy has the even simpler form of a threshold strategy. It will also be a good opportunity to study the\nstructure of the dual LP.\nThreshold Policies The optimal policy for the secretary problem without advice (Examp le 1) is to wait\nuntil we have seen n/esecretaries and then pick the ﬁrst secretary that is the best so f ar. This is a special\ncase of what we call a threshold policy , which conceptually is just a further restriction of memoryless polic ies.\nRecall that we deﬁned the best-so-far eventTias:\nTi={i/{ollowsequalj;∀j < i}.\nNow we say that a stopping time τis a threshold policy if there is a threshold function t:S →[n] such that:\nτ= min{is.t.Tiandi≥t(si)}.\nDynkin’s algorithm for the secretary problem without advice has t(∅) =n/e. It is useful to see what a\nthreshold policy would look like for Yes/No advice (Example 5). A thres hold policy in that case should\nspecify threshold t(N) andt(Y). Assume for now that t(Y)≤t(N) (we will prove it should be the case\nlater). Then the policy would say:\n•No item is picked for i < t(Y).\n•For items arriving t(Y)≤i < t(N) they are chosen if they are the best so far and the advice is Yes.\n•For items arriving t≥t(N) they are chosen if they are the best so far regardless of the adv ice.\nForp= 1 the optimal policy is clearly a threshold policy with t(Y) = 1 and t(N) =n. Forp=1\n2the optimal\npolicy is the same as in Dynkin’s algorithm: t(Y) =t(N) =n/e.\nCharacterization via Dual LP We will see that whether or not the optimal policy is a threshold policy\nis closely related to our ability to solve the dual of the LP in the previous section with a greedy algorithm.\nMore precisely, we will show that under NF and HI the optimal policy is a threshold policy if we can solve\nthe dual with the greedy algorithm and the resulting solution satisﬁe s a natural monotonicity assumption.\nThis will be the case in both of our case studies. The reverse implicatio n is always true: If the optimal policy\nis a threshold policy, then we can ﬁnd it with the greedy algorithm.\nTo state our result more formally, let’s ﬁrst recall the LP from the p revious section, and let’s also derive\nits dual. Under NF and HI we have the following primal-dual pair:\n12\n\nmax/summationdisplay\ni/summationdisplay\nsz(i,s)·a(i,s) s.t.\nz(i,s)≤1−/summationdisplay\nj<i/summationdisplay\ns′z(j,s′)·c(i,s,j,s′)\nz(i,s)≥0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglemin/summationdisplay\ni/summationdisplay\nsu(i,s) s.t.\nu(i,s)+/summationdisplay\nj>i/summationdisplay\ns′u(j,s′)·c(j,s′,i,s)≥a(i,s)\nu(i,s)≥0\nwherea(i,s) andc(i,s,j,s′) are coeﬃcients in [0 ,1] that depend only on the joint distribution of signals and\nranks. In particular, a(i,s) =P[si=s,ri=n] andc(i,s,j,s′) =P[Tj,sj=s′|Ti,si=s].\nFurthermore note that since the coeﬃcients care derived from a probability distribution then:\n/summationdisplay\ns′c(i,s,j,s′)≤1. (7)\nConsider solving the dual LP with the following greedy algorithm: Set u(n,s) =a(n,s) for alls. Then\nfori < nassuming that we have set u(j,s′) forj > iand alls′, set\nu(i,s) = max\n\n0,a(i,s)−/summationdisplay\nj>i/summationdisplay\ns′u(j,s′)·c(j,s′,i,s)\n\n. (8)\nIt is clear that this leads to a feasible dual solution.\nWe show:\nProposition 12. Assume NF and HI. If the greedy solution u(i,s)fori∈[n]ands∈ Sconstructed via\nequation (8) is an optimal solution to the dual LP and for all s∈S,u(i,s)is non-decreasing in i, then the\noptimal policy is a threshold policy.\nProof.We proceed in two steps:\nStep 1:For now, assume that a(i,s) isgeneric. That is, in equation (8) we have a(i,s)−/summationtext\nj>i/summationtext\ns′u(j,s′)·\nc(j,s′,i,s)/ne}ationslash= 0. In that case for each pair ( i,s) we have that either (a) u(i,s) = 0 and the dual constraint is\nnon-tight; or (b) u(i,s)>0 and the dual constraint is tight. Deﬁne the threshold t(s) to be the smallest i\nsuch that u(i,s)>0. Then, because u(i,s) is non-decreasing in i, for every i < t(s) we are in case (a) and\nfor every i≥t(s) we are in case (b).\nBy complementary slackness we have that in case (a) since the dual constraint is not tight, we must\nhavez(i,s) = 0 and hence the probability of picking igiven signal sand that iis the best so far should be\nq(i,s) = 0 (by equation (6)). In case (b) since u(i,s)>0 the primal constraint should be tight and hence the\nprobability of picking ishould be 1 (again by equation (6)). This shows that the policy must b e a threshold\npolicy.\nStep 2: The only missing part now is to argue what happens if a(i,s) is not generic. (This will be a technical\nand not particularly algorithmic argument. The reader that doesn’t care about corner cases may want to\nskip it). In that case we can appeal to a perturbation argument: c onsideraǫ(i,s) =a(i,s)+ǫ·n(i,s) where\nn(i,s) is a random perturbation and ǫis a small number (that we will send to zero). Then almost surely\nwe will have that the LP will be generic (in the sense of Step 2). Hence the optimal solution zǫwill be a\nthreshold policy. Take now a sequence of ǫt→0 and consider the threshold policy solutions zǫt. Since the\nsolutions zǫtlive in a compact space, they must converge to some feasible solution z∗in the limit (passing to\na subsequence if necessary). This solution must also be a threshold policy since the set of threshold policies\nis closed. To see that this is an optimal solution to the unperturbed L P, note that the perturbation only\naﬀects the objective function, hence z∗is feasible. Also note that for any feasible point zwe have that\n/an}bracketle{taǫ,zǫ/an}bracketri}ht ≥ /an}bracketle{taǫ,z/an}bracketri}ht. Taking the limit as ǫ→0 we get: /an}bracketle{ta,z∗/an}bracketri}ht ≥ /an}bracketle{ta,z/an}bracketri}htfor all feasible z, hencez∗is a solution\nto the unperturbed LP.\n13\n\nFor the reverse direction we have:\nProposition 13. Assume NF and HI. If the optimal policy is a threshold policy, then the greedy backwards\ninduction solution (equation (8)) is optimal for the dual LP.\nProof.Lett∗(s) be the thresholds in the optimal policy and let ube any solution to the dual LP. Then\nthe optimal primal solution is such that z(i,s) = 0 for i < t∗(s) and the primal ( i,s)-constraint is tight for\ni≥t∗(s).\nThis means in particular that for i < t∗(s) the primal ( i,s) constraint is slack, so by complementary\nslackness we must have u(i,s) = 0. For i≥t∗(s) we have z(i,s)>0 which means that the ( i,s)-constraint\nneeds to be tight (again by complementary slackness). It means in p articular that for i≥t(s) we must have\nu(i,s) =a(i,s)−/summationtext\nj>i/summationtext\ns′u(j,s)·c(j,s′,i,s).\nNow we argue that u(i,s) must be equal to the solution obtained by backwards induction in eq uation\n(8). If not, let ( i,s) be index with largest isuch they diﬀer. Note that either the dual solution uand the\noptimal solution are either zero or tight in each coordinate. If a(i,s)−/summationtext\nj>i/summationtext\ns′u(j,s)·c(j,s′,i,s)>0 then\nneither can be tight so both must be zero. If a(i,s)−/summationtext\nj>i/summationtext\ns′u(j,s)·c(j,s′,i,s)≤0 then both need to be\nzero as well. So they must also agree on ( i,s).\nAn example of a setting that satisﬁes HI and NF, but where the optim al policy is not a threshold policy\nis the following:\nExample 14. Consider a Markovian stopping problem with n= 4transitions given by the chain in Figure\n6. The ﬁrst state is s1= 1. The optimal policy is to stop at the second element if s2= 2and otherwise to\nwait until the last element. This policy is memoryless but no t a threshold policy since we stop at s= 2at\ni= 2but don’t stop at the same signal at i= 3.\n12\n02\n03\n00.5\n0.5\ni\nFigure 6: Markovian stopping problem where the optimal policy is not a threshold policy\nIn fact, in this example, the greedy algorithm yields an opti mal solution to the dual LP (it sets u(2,2) =\n1/2andu(4,3) = 1/2and so its objective value is 1), but it violates the monotonicity assumption that is\nrequired for Proposition 12 to hold.\n5 Application: Secretary with Samples\nAs our ﬁrst case study we consider the secretary problem with sam ples. For concreteness, we focus on the\nmodel of Kaplan et al. [21] (Example 4). Recall that in this model an ad versary picks n+knumbers. Then\nkof these numbers chosen uniformly at random are shown to the algo rithm as samples. Afterwards, the\nremaining nvalues are presented in random order.\n14\n\nLP Formulation As signals we consider: s= “how many elements in the sample set are smaller than the\ncurrent element”. So the signal space is in S={0,1,2,...k}.\nNow we need to calculate a(i,s) andc(i,s,j,s′) and argue that both NF and HI are satisﬁed. The way\nto do that is based on the following way to sample:\n•Start with a total order on kelements. Let’s imagine that those are blue.\n•Now we will add nblack elements one by one in the following way:\nFort= 1,...,n, there are k+tpositions where we can insert the t-th ﬁrst black element. Insert in a\nrandom position.\nThis is the same sampling process as sampling the entire permutation a nd marking kof those blue and\nrevealingtheotheronesinrandomorder. Thesamplingprocedured ecides/precedesequal1..j,s1,...,s jbeforedecidingthe\nposition of later elements i > jimplying NF. Similarly, since the remaining elements are inserted after j, the\nrelative order and signals of elements before jis irrelevant to whether jwill be the top element conditioned\nonTj,sj, therefore verifying HI. Now, from this sampling process we can co mpute the coeﬃcients:\na(i,s) =P(ri=n,si=s) =P(si=s)·P(ri=n|si=s) =1\nk+1·n−1/productdisplay\nt=1s+t\nk+t+1.\nTo see that imagine inserting ﬁrst element i(it must be inserted in one of the k+1 positions) and then\ninserting every other element and see what positions are available so that it is still the top so far.\nForc(i,s,j,s′) =P(Tj,sj=s′|Ti,si=s) whereTimeans that iis the top so far. Note that this is only\nnon-zero if j≤iands′≤s. Subject to that we can again imagine inserting the elements one by o ne. We\nget:\nc(i,s,j,s′) =1\ns+1·j−1/productdisplay\nt=1s′+t\ns+t+1\nThe idea is again very similar. Start by inserting jand then insert all the elements between 1 and j−1.\nPlugging these formulas for the coeﬃcients a(i,s) andc(i,s,j,s′) into our generic LP, we obtain the\nfollowing primal-dual pair:\nmax/summationdisplay\ni/summationdisplay\nsz(i,s)·1\nk+1·n−1/productdisplay\nt=1s+t\nk+t+1s.t.\nz(i,s)≤1−/summationdisplay\nj<i/summationdisplay\ns′≤sz(j,s′)·1\ns+1·j−1/productdisplay\nt=1s′+t\ns+t+1\nz(i,s)≥0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglemin/summationdisplay\ni/summationdisplay\nsu(i,s) s.t.\nu(i,s)+/summationdisplay\nj>i/summationdisplay\ns′≥su(j,s′)·1\ns′+1·i−1/productdisplay\nt=1s+t\ns′+t+1\n≥1\nk+1·n−1/productdisplay\nt=1s+t\nk+t+1\nu(i,s)≥0\nSee Figure 2 for plots of the success probability of the optimal policy forn= 20 and varying konce as a\nfunction of kand once as a function of p=k/(k+n).\nOptimalityof Thresholding Beforeweshowoptimalityofthresholding,weestablishthefollowingle mma\nthat will allow us to work with expressions arising in the dual (which can be thought as a form of discrete\nintegration):\nLemma 15. Given integers a < bandu < vthe following equality holds:\nv/summationdisplay\ns=ub/productdisplay\nt=a+1(s+t) =1\nb−a+1/bracketleftBiggb/productdisplay\nt=a(v+1+t)−b/productdisplay\nt=a(u+t)/bracketrightBigg\n15\n\nProof.For some integer swe have:\nb/productdisplay\nt=a(s+1+t)−b/productdisplay\nt=a(s+t) = [(s+b+1)−(s+a)]b/productdisplay\nt=a+1(s+t) = (b−a+1)b/productdisplay\nt=a+1(s+t)\nThe result is then obtained by telescoping the above equality for s=utov.\nWe can now prove:\nTheorem 16. For every nand every kthe optimal policy for secretaries with samples is a thresho ld policy.\nProof Sketch (Full Proof in Appendix A). We prove the theorem through proposition Proposition 12. We\nneed to show that the greedy solution in equation (8) is optimal and n on-decreasing in i. Monotonicity fol-\nlowsfrom observingthat a(i,s) =1\nk+1·/producttextn−1\nt=1s+t\nk+t+1does not depend on iandc(j,s′,i,s) =1\ns′+1·/producttexti−1\nt=1s+t\ns′+t+1\nis non-increasing in i.\nFor optimality assume that the optimal solution u(i,s) does not satisfy the recursion that deﬁnes the\ngreedy algorithm. Then there must be a largest index i∗and a signal s∗for which the equation is violated.\nNow we consider changing u(i,s) tou(i,s)+∆(i,s) where ∆( i∗,s∗) =−δfor\nδ:=u(i∗,s∗)−max\n\n0,a(i∗,s∗)−/summationdisplay\nj>i∗/summationdisplay\ns′≥su(j,s′)·c(j,s′,i∗,s∗)\n\n\nand ∆(i,s) = 0 for all other i≥i∗. Fori < i∗we deﬁne ∆( i,s) recursively as follows\n∆(i,s) =−i∗/summationdisplay\nj=i+1/summationdisplay\ns′≥s∆(j,s′)·c(j,s′,i,s).\nFurthermore, deﬁne\nRi:=/summationdisplay\nj≥i/summationdisplay\ns≤s∗∆(j,s)\nand note that Riis the cumulative change to the objective function. We can now prov e by backward\ninduction that (see full proof in Appendix A for the details):\n(i) Fori=i∗: ∆( i∗,s∗) =−δfors=s∗, and\n∆(i∗,s) = 0 for all s/ne}ationslash=s∗.\nFori < i∗: ∆( i,s) =δ·1\ns∗+1i∗−2/productdisplay\nt=1s+t\ns∗+t+1for alls≤s∗, and\n∆(i,s) = 0 for all s > s∗.\n(ii) For all i≤i∗:Ri=−δ·i−1\ni∗−1≤0.\nIn fact, by the deﬁnition of Riand (i), we have\nRi=\ni∗−1/summationdisplay\nj=is∗/summationdisplay\ns=0∆(j,s)\n−δ=δ·\ni∗−1/summationdisplay\nj=is∗/summationdisplay\ns=01\ns∗+1i∗−2/productdisplay\nt=1s+t\ns∗+t+1\n−δ\n16\n\n=δ·i∗−1/summationdisplay\nj=i/parenleftBigg/summationtexts∗\ns=0/producttexti∗−2\nt=1(s+t)/producttexti∗−1\nt=1(s∗+t)/parenrightBigg\n−δ=/parenleftbiggi∗−i\ni∗−1−1/parenrightbigg\nδ=−δ·i−1\ni∗−1≤0,\nwhere the last equality follows from Lemma 15.\nWe now use (i) and (ii) to argue that the operation preserves feasib ility and only improves the objective:\nWe have chosen the recursion for the ∆( i,s) to satisfy the ﬁrst constraint of the dual. From (i) we get\nthat all the ∆( i,s) are non-negative, so we also satisfy the non-negativity constra ints. From (ii) we get that\nR1≤0 so we only decrease the objective.\nBy repeatedly applying this operation we can conclude that the gree dy solution is an optimal solution,\njust as we claimed.\nAn immediate implication of Theorem 16 is an eﬃcient (poly-time) algorith m for computing the optimal\npolicy, and the winning probability of that policy.\nExplicit solution to dual Nextwederiveanexplicit(non-recursive)formulafortheoptimald ualsolution.\nTheorem 17. The following is an explicit solution to the dual recursion:\nFori=nand alls:\nu(n,s) =1\nk+1·n−1/productdisplay\nt=1s+t\nk+t+1.\nFori < nand alls:\nu(i,s) = max/braceleftBigg\n0,1\nk+1·i−1/productdisplay\nt=1/parenleftbiggs+t\nk+t+1/parenrightbigg\n·/parenleftBiggn−1/productdisplay\nt=i/parenleftbiggs+t\nk+t+1/parenrightbigg\n−\n/summationtextn−i\nj=1/parenleftBig/parenleftbign−i\nj/parenrightbig\n·1\nj·/producttextj\nℓ=1(k−s+ℓ)·/producttextn−i−j\nℓ=1(s+i−1+ℓ)/parenrightBig\n/producttextn−1\nt=i(k+t+1)/parenrightBigg/bracerightBigg\n,\nwhere we use the convention that/producttextb\nt=axt= 1ifb < a.\nWe note that the dual solution has the following natural interpreta tion: The positive term is the proba-\nbility that secretary iwith signal sis the best over all. This is the winning probability if we would accept.\nThe negative terms are the probability with which we would win if we would pick the ﬁrst secretary among\nthe remaining n−isecretaries that is better than the current one.\nProof of Theorem 17. For the proof we can ignore the max {0,˙}. We prove the claim by induction. The base\ncase (i=nand alls) holds by deﬁnition. Now let’s do the inductive step. Assume the claim h olds for all\ni′> iand alls. Then we can use the induction hypothesis to obtain\nu(i,s) =1\nk+1·n−1/productdisplay\nt=1/parenleftbiggs+t\nk+t+1/parenrightbigg\n−1\nk+1·n−1/productdisplay\nt=1/parenleftbigg1\nk+t+1/parenrightbigg\n·i−1/productdisplay\nt=1(s+t)·k/summationdisplay\ns′=sT(i,s′),\nwhere\nT(i,s′) = (n−i)n−1/productdisplay\nt=i+1(s′+t)−n−1/summationdisplay\nz=i+1\nn−z/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−z\nj/parenrightbigg\n·1\nj·j/productdisplay\nℓ=1(k−s′+ℓ)·n−j−1/productdisplay\nt=i+1(s′+t)/parenrightBigg\n\n= (n−i)n−1/productdisplay\nt=i+1(s′+t)−n−i−1/summationdisplay\nj=1/parenleftBiggn−j/summationdisplay\nz=i+1/parenleftbigg/parenleftbiggn−z\nj/parenrightbigg\n·1\nj/parenrightbigg\n·j/productdisplay\nℓ=1(k−s′+ℓ)·n−j−1/productdisplay\nt=i+1(s′+t)/parenrightBigg\n17\n\n= (n−i)n−1/productdisplay\nt=i+1(s′+t)−n−i−1/summationdisplay\nj=1/parenleftBigg\n(n−i)·/parenleftbiggn−i−1\nj/parenrightbigg\n·1\nj(j+1)·j/productdisplay\nℓ=1(k−s′+ℓ)·n−j−1/productdisplay\nt=i+1(s′+t)/parenrightBigg\n.\nIn particular, to establish the claim it suﬃces to show that\nk/summationdisplay\ns′=sT(i,s′) =n−i/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i(s+t)/parenrightBigg\n.\nWe prove this identity by backward induction over s. For the base s=kcase we need to show\nT(i,k) =n−i/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j!·n−j−1/productdisplay\nt=i(k+t)/parenrightBigg\n.\nIndeed, we have\nn−i/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j!·n−j−1/productdisplay\nt=i(k+t)/parenrightBigg\n=n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j!·/parenleftbigg\n(k+n−j)−(n−i−j)/parenrightbigg\n·n−j−1/productdisplay\nt=i+1(k+t)/parenrightBigg\n+(n−i−1)!\n= (n−i)·n−1/productdisplay\nt=i+1(k+t)+n−i−1/summationdisplay\nj=2/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j!·(k+n−j)·n−j−1/productdisplay\nt=i+1(k+t)/parenrightBigg\n+(n−i−1)!\n−n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j!·(n−i−j)·n−j−1/productdisplay\nt=i+1(k+t)/parenrightBigg\n= (n−i)·n−1/productdisplay\nt=i+1(k+t)+n−i−2/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·1\nj+1·(j+1)!·(k+n−j−1)·n−j−2/productdisplay\nt=i+1(k+t)/parenrightBigg\n+(n−i−1)!\n−n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j!·(n−i−j)·n−j−1/productdisplay\nt=i+1(k+t)/parenrightBigg\n= (n−i)·n−1/productdisplay\nt=i+1(k+t)+n−i−2/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·1\nj+1·(j+1)!·n−j−1/productdisplay\nt=i+1(k+t)/parenrightBigg\n+(n−i−1)!\n−n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j!·(n−i−j)·n−j−1/productdisplay\nt=i+1(k+t)/parenrightBigg\n= (n−i)·n−1/productdisplay\nt=i+1(k+t)+n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·1\nj+1·(j+1)!·n−j−1/productdisplay\nt=i+1(k+t)/parenrightBigg\n−n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j!·(n−i−j)·n−j−1/productdisplay\nt=i+1(k+t)/parenrightBigg\n= (n−i)·n−1/productdisplay\nt=i+1(k+t)−n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j!·(n−i−j)−/parenleftbiggn−i\nj+1/parenrightbigg1\nj+1·(j+1)!/parenrightbiggn−j−1/productdisplay\nt=i+1(k+t)/parenrightBigg\n= (n−i)·n−1/productdisplay\nt=i+1(k+t)−n−i−1/summationdisplay\nj=1/parenleftBigg\n(n−i)·/parenleftbiggn−i−1\nj/parenrightbigg\n·1\nj(j+1)·j!·n−j−1/productdisplay\nt=i+1(k+t)/parenrightBigg\n=T(i,k),\n18\n\nas claimed.\nThe argument for the inductive step is similar, but technically a bit mor e involved. We defer the details\nto Appendix B.\nGilbert and Mosteller as the limit when k → ∞In the model of Gilbert and Mosteller (Example 2)\neach secretary is associated with a sample from a known distribution F. The distribution can without loss\nof generality be thought as the uniform distribution over [0 ,1] since the algorithm can always process the\nquantiles qi=F−1(si)∼U[0,1]. Quantiles can be seen as a limit of the secretary with samples model with\nk→ ∞by taking q=s/(k+1) where s∈ {0,...,k+1}and take the limit k→ ∞. In the limit, q∈[0,1]\ncorresponds to the quantile of the secretary. One can then take the limit of the LP coeﬃcients:\na(i,q) =qn−1dqandc(i,q,j,q′) =1\nq/parenleftbiggq′\nq/parenrightbiggj−1\ndq′\nand obtain the following functional optimization problem in the limit:\nmax/summationdisplay\ni/integraldisplay\nsz(i,q)·qn−1dqs.t.\nz(i,q)≤1−/summationdisplay\nj<i/integraldisplay\nq′≤qz(j,q′)·1\nq·/parenleftbiggq′\nq/parenrightbiggj−1\ndq′\nz(i,q)≥0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglemin/summationdisplay\ni/integraldisplay\nsu(i,q)dqs.t.\nu(i,q)+/summationdisplay\nj>i/integraldisplay\nq′≥qu(j,q′)·1\nq′·/parenleftbiggq\nq′/parenrightbiggi−1\ndq′≥qn−1\nu(i,q)≥0\nSince this is a limit of secretaries with samples, we can again obtain a dua l solution via backwards induction,\nwhich has a particularly nice form:\nCorollary 18 (From Theorem 16) .The optimal solution u(i,s)to the dual LP found by the greedy backward\ninduction algorithm satisﬁes\nu(i,s) =sn−1fori=nand alls, and\nu(i,s) = max/braceleftBigg\n0,si−1·/parenleftBigg\nsn−i−n−i/summationdisplay\nk=11\nk/parenleftbiggn−i\nk/parenrightbigg\nsn−i−k(1−s)k/parenrightBigg/bracerightBigg\nfori < nand alls.\nAn immediate consequence of Corollary18 is that the optimal policy ca n be determined by setting s∗\nn= 0\nand ﬁnding for each i < nthes∗\nisuch that\n(s∗\ni)n−i−n−i/summationdisplay\nk=11\nk/parenleftbiggn−i\nk/parenrightbigg\n(s∗\ni)n−i−k(1−(s∗\ni))k= 0, (9)\nand to then accept secretary iwith signal sif it is the best so far and s≥s∗\ni. We note that this is precisely\nhow Gilbert and Mosteller [16] deﬁne the optimal policy (Equation (3b- 1) on p. 53).\nWinning Probability More generally, the explicit dual solution (in Theorem 17 and Corollary 18) allows\nto deduce thresholds in signal space (as we just did for Gilbert and M osteller), both in the non-asymptotic\nand asymptotic regimes. For Gilbert and Mosteller, for example, it is k nown that solutions to equation (9)\nin the asymptotic regime satisfy bi→1 andi(1−bi)→c, wherebifori=n−1,...,0 is the threshold\nwhen there are isecretaries after the current one, and this can be used to obtain analytic expressions for the\nasymptotic winning probability [17, 33].\n19\n\n6 Application: Advice from a Binary Classiﬁer\nWe will now use the technology developed in the previous sections to d erive the optimal policy for the\nsecretary problem with advice from a binary classiﬁer (from the intr o, and deﬁned formally in Section 2).\nRecall that in this example secretaries have uniform random ranks, and that for the top secretary (rank =\nn) we receive advice Ywith probability pand advice Nwith probability 1 −p. For all other secretaries we\nreceive advice Nwith probability p′and advice Ywith probability 1 −p′. In ML speak, pcorresponds to\nthe precision of the advice and p′to its speciﬁcity.\nNotation To avoid polluting the expressions with too many parenthesis, in this s ection we will abbreviate\nthe primal z(i,s) and the dual u(i,s) usingzs\niandus\ni. Moreover, we will use the notation:\n¯zj= (1−p′)zY\nj+p′zN\nj.\nLP Formulation Since Example 5 satisﬁes NF and HI we can instantiate our general line ar programming\nformulation to the binary classiﬁer case. Using the notation establis hed above, we can write for nperiods\nand parameters pandp′:\nmax/summationdisplay\ni1\nn/bracketleftbig\npzY\ni+(1−p)zN\ni/bracketrightbig\ns.t.\nzs\ni≤1−/summationdisplay\nj<i1\nj/bracketleftbig\n(1−p′)zY\nj+p′zN\nj/bracketrightbig\nzs\ni∈[0,1]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglemin/summationdisplay\ni/parenleftbig\nuY\ni+uN\ni/parenrightbig\ns.t.\nuY\ni+/summationdisplay\nj>i1−p′\ni·/parenleftbig\nuY\nj+uN\nj/parenrightbig\n≥p\nn\nuN\ni+/summationdisplay\nj>ip′\ni·/parenleftbig\nuY\nj+uN\nj/parenrightbig\n≥1−p\nn\nuY\ni,uN\ni≥0\nOptimality of Thresholding We start by using the dual LP to show:\nTheorem 19. The optimal policy for the binary advice problem is a thresho ld policy.\nProof Sketch (Full Proof in Appendix C). The proof uses Proposition 12, by which it suﬃces to shows that\nthe greedy algorithm yields an optimal and monotone solution to the d ual LP. Our argument follows the\nsame blueprint asthe proofof Theorem16. Monotonicityfollowsfro m the properties ofthe coeﬃcients a(i,s)\nandc(j,s′,i,s). For optimality we use the same basic construction, with appropria tely adjusted inductive\nclaims.\nThe Optimal Threshold Policy Next we use the fact that the optimal policy is a threshold policy to\nobtain a closed-form understanding of the optimal policy and the pe rformance it achieves.\nLettNandtYdenote the thresholds (in time) after which we start accepting the best-ranking secretary\nso far conditioned on the advice being No ( N) or Yes ( Y). Write Opt(n,p,p′) for the optimal solution to the\nprimal LP, and let OPT(p,p′) = inf n≥1Opt(n,p,p′).\nTheorem 20. With advice from a binary classiﬁer with recall pand speciﬁcity p′, the optimal policy is a\nthreshold policy that has two thresholds tY≤tNfrom which onwards it accepts the best secretary so far upon\nreceiving advice YorN, and the optimal objective is:\nOpt(p,p′)≥lim\nn→∞Opt(n,p,p′) =/parenleftbigg1−p′\np/parenrightbigg(1−p′)/p\n·ep/p′−1/p′.\nThe two thresholds are tY=n·(1−p′\np)1/p′·ep/p′−1/p′andtN=n·ep/p′−1/p′.\n20\n\np 0.5 11/e\n01Competitive Ratio\np 0.5 1n/e\n0ntN\ntY\nFigure 7: Left plot shows the performance of the optimal secreta ry with advice policy. The right plot shows\nthe thresholds tYandtN. In both cases, solid lines indicate p=pand the dashed lines show p′= (1+p)/2.\nAs we see, higher accuracy (dashed lines) leads to increased compe titive ratio.\nNote how with p=p′= 1/2 (no advice), both thresholds become tY=tN=n/erecovering Dynkin’s\nalgorithm. If p=p′= 1 (perfect advice) then the thresholds become tY= 0 and tN=nwhich means the\npolicy should always pick whenever the signal is Yand never pick if the signal is N. Figure 7 shows how the\napproximation and thresholds vary as a function of pfor the symmetric case p=p′, and asymmetric case\np′= (1+p)/2.\nProof of Theorem 20. Throughout the proof we will assume nis large and we will approximate Riemann\nsums by integrals/summationtextn\ni=11\nnf(i\nn)≈/integraltext1\n0f(x)dx. Whenever we use the symbol ≈what we really mean is an\nequality up to terms that vanish in the limit as n→ ∞which we choose to omit to prevent making the\nnotation too ugly. As an example, whenever a,b= Θ(n) we will approximate the harmonic sums/summationtextb\nj=a1\njby\nlog(b/a).\nTheorem 19 gives a recipe on how to calculate the threshold using the dual LP. Recall the dual LP for\nbinary advice from the beginning of this section. For notational con venience we will work with the following\nvariant, in which we moved the 1 /nterm from the constraints to the objective:\nmin/summationdisplayuY\ni\nn+uN\ni\nns.t.uY\ni+/summationdisplay\nj>i1−p′\ni/bracketleftbig\nuY\nj+uN\nj/bracketrightbig\n≥panduN\ni+/summationdisplay\nj>ip′\ni/bracketleftbig\nuY\nj+uN\nj/bracketrightbig\n≥1−pandua\ni≥0.\nBy the structure of the dual and a backward induction argument w e have that for i≥tNthe dual has\nthe following form:\nuY\ni=p−n−1/summationdisplay\nj=i1−p′\njanduN\ni= (1−p)−n−1/summationdisplay\nj=ip′\nj.\nThe threshold tNis deﬁned as the ﬁrst time in which uN\nibecomes zero. It is convenient at this point to\napproximate the harmonic sums in uN\niby a logarithm:\nuN\ni≈1−p−p′log/parenleftBign\ni/parenrightBig\n,\nwhich vanishes at tN=n·ep/p′−1/p′. Now for i < tNwe update only uY\ni, then the formula becomes:\nuY\ni=p−1−p′\ni/bracketleftbigg\nuY\ni+1+(p−uY\ni+1)i+1\n1−p′/bracketrightbigg\n=i+p′\niuY\ni+1−p\ni.\nSolving for the recursion we get the following:\nuY\ni=uY\ntN·tN−1/productdisplay\nj=i/parenleftbigg\n1+p′\nj/parenrightbigg\n−tN−1/summationdisplay\nj=ip\njj−1/productdisplay\nk=i/parenleftbigg\n1+p′\nk/parenrightbigg\n.\n21\n\nUsing 1+ x≈e−xsincex= 1/jforj= Ω(n) and then approximating harmonic sums by logs, we get:\nuY\ni≈uY\ntN·ep′log(tN/i)−tN−1/summationdisplay\nj=ip\njep′log(j/i)=uY\ntN·/parenleftbiggtN\ni/parenrightbiggp′\n−tN−1/summationdisplay\nj=ip\nj/parenleftbiggj\ni/parenrightbiggp′\n.\nFor the last term we observe it can be interpreted as a Riemann sum a nd therefore approximated as the\ncorresponding integral:\ntN−1/summationdisplay\nj=ip\nj/parenleftbiggj\ni/parenrightbiggp′\n≈p\nip′/integraldisplaytN\nixp′−1dx=p\nip′1\np′[tp′\nN−ip′] =p\np′/bracketleftBigg/parenleftbiggtN\ni/parenrightbiggp′\n−1/bracketrightBigg\n.\nThus,\nuY\ni=p\np′+/parenleftbigg\nuY\ntN−p\np′/parenrightbigg/parenleftbiggtN\ni/parenrightbiggp′\n=p\np′−/parenleftbigg1−p′\np′/parenrightbigg/parenleftbiggtN\ni/parenrightbiggp′\nvanishing when:\ni=tN/parenleftbigg1−p′\np/parenrightbigg1/p′\n=:tY.\nNow we can sum the dual to get the dual objective:\nDualObj ≈1\nn/integraldisplaytN\ntY/parenleftBigg\np\np′−1−p′\np′·/parenleftbiggtN\nx/parenrightbiggp′/parenrightBigg\ndx+1\nn/integraldisplayn\ntN/parenleftBig\n1−log/parenleftBign\nx/parenrightBig/parenrightBig\ndx\n=tN−tY\nnp\np′−1\nn·1−p′\np′·tY/parenleftBig\ntN\ntY/parenrightBigp′\n−tN\np′−1+tN\nnlog/parenleftbiggn\ntN/parenrightbigg\n=tN−tY\nnp\np′+tN\nnlog/parenleftbiggn\ntN/parenrightbigg\n−1\nnp′/bracketleftBig\ntN−tp′\nNt1−p′\nY/bracketrightBig\n=1\nnp′tp′\nNt1−p′\nY−p\np′tY\nn\n=/parenleftbigg1−p′\np/parenrightbigg(1−p′)/p′\n·ep/p′−1/p′.\nThis concludes the proof.\n7 Discussion and Future Work\nIn this work we present a unifying view of multiple versions of the class ic secretary problem. The linear\nprogram that is at the heart of our analysis precisely isolates the eﬀ ect of the signalling scheme from the\ncombinatorialstructureofthe problem: the jointdistributiononr anksand signalsonlyaﬀectsthecoeﬃcients\nof the simple set of constraints. The formulations that are captur ed by our analysis are quite diverse, as we\ndemonstrate through the examples in Section 2, and is it remarkable that all of these can be solved optimally\nby memoryless policies.\nAs we saw in Example 11, however, some settings require simple, but n ot memoryless policies. One\npossible direction for future work is to explore counting policies, that do not rely on the permutation of\nsignals observed, but only on their histograms. Another approach is to consider advice that does not satisfy\nNF or HI, but still has suﬃcient structure to reason about its eﬃca cy.\nMore broadly, we hope that the advice lens can be used to abstract the modeling assumptions and ﬁnd\nformal connections between related problems in other areas.\n22\n\nReferences\n[1] D. Alabi, A. T. Kalai, K. Ligett, C. Musco, C. Tzamos, and E. Viterc ik. Learning to prune: Speeding\nup repeated computations. In Proceedings of the 2019 Conference on Learning Theory , pages 30–33,\n2019.\n[2] P. Allaart. A general ‘bang-bang’ principle for predicting the max imum of a random walk. Journal of\nApplied Probability , 47(4):1072–1083, 2010.\n[3] A. Antoniadis, T. Gouleakis, P. Kleer, and P. Kolev. Secretary an d online matching problems with\nmachine learned advice. CoRR, abs/2006.01026, 2020. (Accepted to NeurIPS’20).\n[4] P. D. Azar, R. Kleinberg, and S. M. Weinberg. Prophet inequalities with limited information. In\nProceedings of the 25th ACM-SIAM Symposium on Discrete Algo rithms, pages 1358–1377, 2014.\n[5] M. Balcan, T. Dick, T. Sandholm, and E. Vitercik. Learning to bran ch. InProceedings of the 35th\nInternational Conference on Machine Learning , pages 353–362, 2018.\n[6] B. A. Berezovskiy and A. V. Gnedin. The best choice problem (in ru ssian).Nauka, Moscow , 1984.\n[7] N. Buchbinder, K. Jain, and M. Singh. Secretary problems via linea r programming. Mathematics of\nOperations Research , 39(1):190–206, 2014.\n[8] T.H. Chan, F. Chen, and S.H.-C. Jiang. Revealingoptimalthresh oldsforgeneralizedsecretaryproblem\nviacontinuouslp: Impacts on online k-item auction and bipartite k-ma tchingwith randomarrivalorder.\nInProceedings of the 26th ACM-SIAM Symposium on Discrete Algo rithms, pages 1169–1188, 2015.\n[9] J. Correa, A. Cristi, L. Feuilloley, T. Osterwijk, and A. Tsigonias- Dimitriadis. The secretary problem\nwith independent sampling. In Proceedings of the 31st SIAM Symposium on Discrete Algorith ms, 2020.\nForthcoming.\n[10] J. R. Correa, A. Cristi, B. Epstein, and J. A. Soto. The two-sid ed game of googol and sample-based\nprophet inequalities. In Proceedings of the 30th ACM-SIAM Symposium on Discrete Algo rithms, pages\n2066–2081, 2020.\n[11] J. R. Correa, P. D¨ utting, F. A. Fischer, and K. Schewior. Pro phet inequalities for I.I.D. random\nvariables from an unknown distribution. In Proceedings of the 20th ACM Conference on Economics and\nComputation , pages 3–17, 2019.\n[12] J. Du Toit and G. Peskir. Selling a stock at the ultimate maximum. The Annals of Applied Probability ,\n19:983–1014, 2009.\n[13] E. B. Dynkin. The optimum choice of the instant for stopping a ma rkov process. Soviet Mathematics ,\n4:627–629, 1963.\n[14] H. Esfandiari, M. Hajiaghayi, B. Lucier, and M. Mitzenmacher. P rophets, secretaries, and maximizing\nthe probability of choosing the best. In Proceedings of the 23rd International Conference on Artiﬁc ial\nIntelligence and Statistics , pages 3717–3727, 2020.\n[15] T. S. Ferguson. Who solved the secretary problem? Statistical Science , 4:282–289, 1989.\n[16] J. P. Gilbert and F. Mosteller. Recognizing the maximum of a seque nce.Journal of the American\nStatistical Association , 61:35–73, 1966.\n[17] A. V. Gnedin. On the full information best-choice problem. Journal of Applied Probability , 33:678–87,\n1996.\n23\n\n[18] M. Hlynka and J. Sheahan. The secretary problem for a random walk.Stochastic Processes and their\nApplications , 28(2):317 – 325, 1988.\n[19] P. Indyk, A. Vakilian, and Y. Yuan. Learning-based low-rank ap proximations. In Proceedings of the\n2019 Conference on Neural Information Processing Systems , pages 7400–7410, 2019.\n[20] K. Jain, M. Mahdian, E. Markakis, A. Saberi, and V. V. Vazirani. G reedy facility location algorithms\nanalyzed using dual ﬁtting with factor-revealing lp. Journal of the ACM , 50(6):795–824, 2003.\n[21] H. Kaplan, D. Naori, and D. Raz. Competitive analysis with a sample and the secretary problem. In\nProceedings of the 30th ACM-SIAM Symposium on Discrete Algo rithms, pages 2082–2095, 2020.\n[22] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The c ase for learned index structures. In\nProceedings of the 2018 International Conference on Manage ment of Data , pages 489–504, 2018.\n[23] S. Lattanzi, T. Lavastida, B. Moseley, and S. Vassilvitskii. Online scheduling via learned weights. In\nProceedings of the 14th ACM-SIAM Symposium on Discrete Algo rithms, pages 1859–1877, 2020.\n[24] T. Lykouris and S. Vassilvitskii. Competitive caching with machine le arned advice. In Proceedings of\nthe 35th International Conference on Machine Learning , pages 3302–3311, 2018.\n[25] M. Mahdian and Q. Yan. Online bipartite matching with random arriv als: an approach based on\nstrongly factor-revealing LPs. In Proceedings of the 43rd ACM Symposium on Theory of Computing ,\npages 597–606, 2011.\n[26] A. M. Medina and S. Vassilvitskii. Revenue optimization with approx imate bid predictions. In Pro-\nceedings of the 2017 Annual Conference on Neural Informatio n Processing Systems , pages 1858–1866,\n2017.\n[27] A. Mehta, A. Saberi, U. U. Vazirani, and V. V. Vazirani. Adwords and generalized online matching.\nJournal of the ACM , 54(5):22–40, 2007.\n[28] M. Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. In Proceedings of\nthe 2018 Annual Conference on Neural Information Processin g Systems , pages 462–471, 2018.\n[29] M. Mitzenmacher. Queues with small advice. CoRR, abs/2006.15463, 2020.\n[30] M. Purohit, Z. Svitkina, and R. Kumar. Improving online algorithm s via ML predictions. In Proceedings\nof the 2018 Annual Conference on Neural Information Process ing Systems , pages 9684–9693, 2018.\n[31] D. Rohatgi. Near-optimal bounds for online caching with machine learned advice. In Proceedings of the\n30th Annual ACM-SIAM Symposium on Discrete Algorithms , pages 1834–1845, 2020.\n[32] A.Rubinstein, J.Z.Wang,andS.M.Weinberg. Optimalsingle-ch oiceprophetinequalitiesfromsamples.\nInProceedings of the 11th Innovations in Theoretical Compute r Science Conference , volume 151, pages\n60:1–60:10, 2020.\n[33] S. Samuels. Exact solutions for the full information best choice problem. Purdue University Statistics\nMimeo Series , page 81-87, 1982.\n[34] S. C. P. Yam, S. P. Yung, and W. Zhou. Two rationales behind the ‘buy-and-hold or sell-at-once’\nstrategy. Journal of Applied Probability , 46(3):651–668, 2009.\n24\n\nA Full Proof of Theorem 16\nFull Proof of Theorem 16. We prove the theorem through Proposition 12. We need to show tha t the greedy\nsolution obtained via backwards induction is optimal and non-decrea sing ini. The greedy algorithms yields:\nu(n,s) =1\nk+1·n−1/productdisplay\nt=1s+t\nk+t+1and\nu(i,s) = max\n\n0,1\nk+1·n−1/productdisplay\nt=1s+t\nk+t+1−/summationdisplay\nj>i/summationdisplay\ns′≥su(j,s′)·1\ns′+1·i−1/productdisplay\nt=1s+t\ns′+t+1\n\nfori < nand alls.\nMonotonicityfollowsfromobservingthat a(i,s) =1\nk+1·/producttextn−1\nt=1s+t\nk+t+1doesnotdependon iandc(j,s′,i,s) =\n1\ns′+1·/producttexti−1\nt=1s+t\ns′+t+1is non-increasing in i.\nIt remains to show optimality. Suppose the optimal dual solution doe s not satisfy the recursion that\ndeﬁnes the greedy algorithm. Then there must be a largest index i∗and a signal s∗for which the equation\nis violated. Now we consider changing u(i,s) tou(i,s)+∆(i,s) where ∆( i∗,s∗) =−δfor\nδ:=u(i∗,s∗)−max\n\n0,a(i∗,s∗)−/summationdisplay\nj>i∗/summationdisplay\ns′≥su(j,s′)·c(j,s′,i∗,s∗)\n\n\nand ∆(i,s) = 0 for all other i≥i∗. Fori < i∗we deﬁne ∆( i,s) recursively as follows\n∆(i,s) =−i∗/summationdisplay\nj=i+1/summationdisplay\ns′≥s∆(j,s′)·c(j,s′,i,s).\nFurthermore, deﬁne\nRi:=/summationdisplay\nj≥i/summationdisplay\ns≤s∗∆(j,s)\nand note that Riis the cumulative change to the objective function.\nWe can now prove by backward induction that:\n(i) Fori=i∗: ∆( i∗,s∗) =−δfors=s∗, and\n∆(i∗,s) = 0 for all s/ne}ationslash=s∗\nFori < i∗: ∆( i,s) =δ·1\ns∗+1i∗−2/productdisplay\nt=1s+t\ns∗+t+1for alls≤s∗, and\n∆(i,s) = 0 for all s > s∗\nLet’s ﬁrst do the base case. For i=i∗the claim holds by deﬁnition. For i=i∗−1, we have\n∆(i∗−1,s) =−∆(i∗,s∗)·c(i∗,s∗,i∗−1,s) =δ·1\ns∗+1·i∗−2/productdisplay\nt=1s+t\ns∗+t+1,\nby the deﬁnition of ∆( i∗−1,s), ∆(i∗,s∗), andc(i∗,s∗,i∗−1,s).\nLet’s do the inductive step. For this assume the claim holds for i′> iand verify the inductive claim for\ni:\n∆(i,s)\n25\n\n=−i∗−1/summationdisplay\nj=i+1/summationdisplay\ns′≥s/parenleftbigg\n∆(j,s′)·c(j,s′,i,s)/parenrightbigg\n+δ·c(i∗,s∗,i,s)\n=−i∗−1/summationdisplay\nj=i+1s∗/summationdisplay\ns′=s/parenleftBigg\nδ·1\ns∗+1·i∗−2/productdisplay\nt=1/parenleftbiggs′+t\ns∗+t+1/parenrightbigg\n·1\ns′+1·i−1/productdisplay\nt=1/parenleftbiggs+t\ns′+t+1/parenrightbigg/parenrightBigg\n+δ·1\ns∗+1·i−1/productdisplay\nt=1/parenleftbiggs+t\ns∗+t+1/parenrightbigg\n=−δ/producttexti∗−1\nt=1(s∗+t)·i∗−1/summationdisplay\nj=i+1/parenleftBiggi−1/productdisplay\nt=1(s+t)·s∗/summationdisplay\ns′=s/parenleftBigg/producttexti∗−2\nt=1(s′+t)\n/producttexti\nt=1(s′+t)/parenrightBigg/parenrightBigg\n+δ·1\ns∗+1·i−1/productdisplay\nt=1/parenleftbiggs+t\ns∗+t+1/parenrightbigg\n=−δ/producttexti∗−1\nt=1(s∗+t)·i∗−1/summationdisplay\nj=i+1/parenleftBiggi−1/productdisplay\nt=1(s+t)·1\ni∗−i−1·/parenleftBigg/producttexti∗−1\nt=1(s∗+t)/producttexti\nt=1(s∗+t)−/producttexti∗−2\nt=1(s+t)/producttexti−1\nt=1(s+t)/parenrightBigg/parenrightBigg\n+δ·1\ns∗+1·i−1/productdisplay\nt=1/parenleftbiggs+t\ns∗+t+1/parenrightbigg\n=−δ/producttexti∗−1\nt=1(s∗+t)·i−1/productdisplay\nt=1(s+t)·/parenleftBigg/producttexti∗−1\nt=1(s∗+t)/producttexti\nt=1(s∗+t)−/producttexti∗−2\nt=1(s+t)/producttexti−1\nt=1(s+t)/parenrightBigg\n+δ·1\ns∗+1·i−1/productdisplay\nt=1/parenleftbiggs+t\ns∗+t+1/parenrightbigg\n=−δ·1\ns∗+1·i−1/productdisplay\nt=1/parenleftbiggs+t\ns∗+t+1/parenrightbigg\n+δ·1\ns∗+1·i∗−2/productdisplay\nt=1/parenleftbiggs+t\ns∗+t+1/parenrightbigg\n+δ·1\ns∗+1·i−1/productdisplay\nt=1/parenleftbiggs+t\ns∗+t+1/parenrightbigg\n=δ·1\ns∗+1·i∗−2/productdisplay\nt=1/parenleftbiggs+t\ns∗+t+1/parenrightbigg\n,\nwhere the second equation uses the inductive hypothesis and the f ourth equality holds by Lemma 15.\nHaving established (i), we next show:\n(ii) For all i≤i∗:Ri=−δ·i−1\ni∗−1≤0\nIn fact, by the deﬁnition of Riand (i), we have\nRi=\ni∗−1/summationdisplay\nj=is∗/summationdisplay\ns=0∆(j,s)\n−δ=δ·\ni∗−1/summationdisplay\nj=is∗/summationdisplay\ns=01\ns∗+1i∗−2/productdisplay\nt=1s+t\ns∗+t+1\n−δ\n=δ·i∗−1/summationdisplay\nj=i/parenleftBigg/summationtexts∗\ns=0/producttexti∗−2\nt=1(s+t)/producttexti∗−1\nt=1(s∗+t)/parenrightBigg\n−δ\n=/parenleftbiggi∗−i\ni∗−1−1/parenrightbigg\nδ=−δ·i−1\ni∗−1≤0,\nwhere the last equality follows from Lemma 15.\nWe now use (i) and (ii) to argue that the operation preserves feasib ility and only improves the objective:\nWe have chosen the recursion for the ∆( i,s) to satisfy the ﬁrst constraint of the dual. From (i) we get\nthat all the ∆( i,s) are non-negative, so we also satisfy the non-negativity constra ints. From (ii) we get that\nR1≤0 so we only decrease the objective.\nBy repeatedly applying this operation we can conclude that the gree dy solution is an optimal solution,\njust as we claimed.\nB Inductive Step in Proof of Theorem 17\nFor the inductive step we will assume the claim is true for s+1.\n26\n\nWe want to show that:\nk/summationdisplay\ns′=sT(i,s′) =n−i/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i(s+t)/parenrightBigg\n.\nFrom the deﬁnition of T(i,s′) and the induction hypothesis we know that\nk/summationdisplay\ns′=sT(i,s′) =T(i,s)+/summationdisplay\ns′=s+1T(i,s′)\n=\n(n−i)n−1/productdisplay\nt=i+1(s+t)−n−i−1/summationdisplay\nj=1/parenleftBigg\n(n−i)·/parenleftbiggn−i−1\nj/parenrightbigg\n·1\nj(j+1)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n\n+\nn−i/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j/productdisplay\nℓ=1(k−(s+1)+ℓ)·n−j−1/productdisplay\nt=i((s+1)+t)/parenrightBigg\n.\nWe have:\nn−i/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i(s+t)/parenrightBigg\n=n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·(s+i)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n+1\nn−i·n−i/productdisplay\nℓ=1(k−s+ℓ)\n=n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·(s+n−j−n+j+i)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n+1\nn−in−i/productdisplay\nℓ=1(k−s+ℓ)\n= (k−s+1)·(n−i)·n−1/productdisplay\nt=i+1(s+t)+n−i−1/summationdisplay\nj=2/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·(s+n−j)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n−n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·(n−j−i)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n+1\nn−i·n−i/productdisplay\nℓ=1(k−s+ℓ)\n= (k−s+1)·(n−i)·n−1/productdisplay\nt=i+1(s+t)+n−i−2/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·1\nj+1·j+1/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n−n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·(n−j−i)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n+1\nn−in−i/productdisplay\nℓ=1(k−s+ℓ)\n= (k−s+1)·(n−i)·n−1/productdisplay\nt=i+1(s+t)+n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·1\nj+1·j+1/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n−n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·(n−j−i)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n= (k−s+1)·(n−i)·n−1/productdisplay\nt=i+1(s+t)+n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·1\nj+1·(k−s+j+1)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n−n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·(n−j−i)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n27\n\n= (k−s+1)·(n−i)·n−1/productdisplay\nt=i+1(s+t)+n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·1\nj+1·(k−s)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n+\n+n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n−n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·(n−j−i)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n= (k−s)·(n−i)·n−1/productdisplay\nt=i+1(s+t)+n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·1\nj+1·(k−s)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n+\n+(n−i)·n−1/productdisplay\nt=i+1(s+t)+n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n−n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg1\nj·(n−j−i)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n= (k−s)·(n−i)·n−1/productdisplay\nt=i+1(s+t)+n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·1\nj+1·(k−s)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n+\n+(n−i)·n−1/productdisplay\nt=i+1(s+t)−n−i−1/summationdisplay\nj=1/parenleftBigg\n(n−i)/parenleftbiggn−i−1\nj/parenrightbigg1\nj(j+1)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n.\nWe furthermore have that\n(k−s)·(n−i)·n−1/productdisplay\nt=i+1(s+t)+n−i−1/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj+1/parenrightbigg\n·1\nj+1·(k−s)·j/productdisplay\nℓ=1(k−s+ℓ)·n−j−1/productdisplay\nt=i+1(s+t)/parenrightBigg\n= (k−s)·(n−i)·n−1/productdisplay\nt=i+1(s+t)+n−i/summationdisplay\nj=2/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j−1/productdisplay\nℓ=0(k−s+ℓ)·n−j/productdisplay\nt=i+1(s+t)/parenrightBigg\n= (k−s)·(n−i)·n−1/productdisplay\nt=i+1(s+t)+n−i/summationdisplay\nj=2/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j/productdisplay\nℓ=1(k−(s+1)+ℓ)·n−j−1/productdisplay\nt=i((s+1)+t)/parenrightBigg\n=n−i/summationdisplay\nj=1/parenleftBigg/parenleftbiggn−i\nj/parenrightbigg\n·1\nj·j/productdisplay\nℓ=1(k−(s+1)+ℓ)·n−j−1/productdisplay\nt=i((s+1)+t)/parenrightBigg\n,\nwhich completes the proof.\nC Full Proof of Theorem 19\nFull Proof of Theorem 19. From Proposition 12 we know that it suﬃces to show that the greedy algorithm\nﬁnds an optimal solution us\nito the dual LP, and that the us\niare non-decreasing in i.\nFor ease of reference, let’s recall the equation that deﬁnes the g reedy algorithm (equation (8)). For i=n\nand allsthe greedy algorithm sets us\nn=a(n,s) and for i < nand allsit sets\nus\ni= max/parenleftbigg\n0,a(i,s)−/summationdisplay\nj>i/summationdisplay\ns′us′\nj·ˆc(i,s)/parenrightbigg\n. (10)\nwherea(i,Y) =p/n,a(i,N) = (1−p)/N, ˆc(i,Y) = (1−p′)/i, and ˆc(i,N) =p′/i.\n28\n\nMonotonicity of the us\niconstructed via equation (10) follows from the facts that a(i,s) does not depend\noni, and that ˆ c(i,s) is decreasing in i.\nFor optimality assume that we have a dual solution that is not of the f orm above. Take the ( i∗,s∗) point\nwith largest possible i∗such that equation (10) is not an equality. We will show how to change the solution\nto make it an equality without hurting feasibility or increasing the dual objective.\nSpeciﬁcally, consider changing us\nitous\ni+∆(i,s) where ∆( i∗,s∗) =−δfor\nδ:=u(i∗,s∗)−max/parenleftbigg\n0,a(i∗,s∗)−/summationdisplay\nj>i∗/summationdisplay\ns′us′\nj·ˆc(i∗,s∗)/parenrightbigg\nand ∆(i,s) = 0 for all other i≥i∗. Fori < i∗we can deﬁne ∆( i,s) recursively as follows:\n∆(i,s) =−i∗/summationdisplay\nj=i+1/summationdisplay\ns′∆(j,s′)·ˆc(i,s).\nWe will show (by a backwards recursion) that for each i < i∗we have:\n(i) ∆(i,s)≥0 and (ii) Ri:=/summationdisplay\nj≥i/summationdisplay\ns∆(j,s)≤0\nLet’s ﬁrst check that base case i=i∗−1. For this case, we have:\n∆(i∗−1,s) =−ˆc(i∗−1,s)·∆(i∗,s∗) = ˆc(i∗−1,s)·δ≥0\nwhich shows (i). For (ii), note that:\nRi∗−1=−δ+/summationdisplay\nsˆc(i∗−1,s)·δ.\nSince ˆc(i,s) represent probability distributions, we have that/summationtext\nsˆc(i,s)≤1. This directly implies that\nRi∗−1≤0.\nFor the induction step, note that\n∆(i,s) =−ˆc(i,s)·Ri+1.\nSinceRi+1≤0 then ∆( i,s)≥0 showing (i). For (ii) observe that:\nRi=/summationdisplay\ns∆(i,s)+Ri+1=−Ri+1·/summationdisplay\nsˆc(i,s)+Ri+1=Ri+1(1−/summationdisplay\nsˆc(i,s))≤0\nsince ˆc(i,s) is a probability distribution.\nNow that we established (i) and (ii) observe that ( ii) fori= 1 implies that the objective function doesn’t\nincrease, so if the original solution was optimal, the transformed so lution will also be optimal. By (i) we\nestablish that us\ni+ ∆(i,s) is still non-negative. Finally, note that ∆( i,s) is deﬁned precisely to make the\nconstraints in the dual LP feasible. Hence, by applying this transfo rmation repeatedly starting form any\noptimal solution to the dual, we arrive at an optimal solution of the du al that is obtained by the greedy\nalgorithm in equation (10).\n29",
  "textLength": 89867
}