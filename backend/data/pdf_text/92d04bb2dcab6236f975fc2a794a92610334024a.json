{
  "paperId": "92d04bb2dcab6236f975fc2a794a92610334024a",
  "title": "A Critical Analysis of Recursive Model Indexes",
  "pdfPath": "92d04bb2dcab6236f975fc2a794a92610334024a.pdf",
  "text": "A Critical Analysis of Recursive Model Indexes\nMarcel Maltry\nSaarland Informatics Campus\nmarcel.maltry@bigdata.uni-saarland.deJens Dittrich\nSaarland Informatics Campus\njens.dittrich@bigdata.uni-saarland.de\nABSTRACT\nThe recursive model index (RMI) has recently been introduced as a\nmachine-learned replacement for traditional indexes over sorted\ndata, achieving remarkably fast lookups. Follow-up work focused\non explaining RMIâ€™s performance and automatically configuring\nRMIs through enumeration. Unfortunately, configuring RMIs in-\nvolves setting several hyperparameters, the enumeration of which\nis often too time-consuming in practice. Therefore, in this work,\nwe conduct the first inventor-independent broad analysis of RMIs\nwith the goal of understanding the impact of each hyperparameter\non performance. In particular, we show that in addition to model\ntypes and layer size, error bounds and search algorithms must be\nconsidered to achieve the best possible performance. Based on our\nfindings, we develop a simple-to-follow guideline for configuring\nRMIs. We evaluate our guideline by comparing the resulting RMIs\nwith a number of state-of-the-art indexes, both learned and tradi-\ntional. We show that our simple guideline is sufficient to achieve\ncompetitive performance with other learned indexes and RMIs\nwhose configuration was determined using an expensive enumera-\ntion procedure. In addition, while carefully reimplementing RMIs,\nwe are able to improve the build time by 2.5x to 6.3x.\n1 INTRODUCTION\nMachine learning and artificial intelligence are taking the world by\nstorm. Research areas that were believed to have been researched to\ncompletion have been revisited with exciting new results, showing\nthat considerable improvements are still possible ifwe factor in\nwisdom from the machine learning world. Notable examples in-\nclude natural language processing and compute vision which were\ncompletely revolutionized in the past decade by variants of deep\nlearning. In the database world, we witnessed a surge of similar\nre-exploration endeavors in the past five years. Notable recent ex-\namples of works in that space include cardinality estimation [ 18,29],\nauto-tuning [ 10,28], and indexing [ 14,16,17,21,23]. We believe\nthat indexing is the most surprising result of these three areas be-\ncause both cardinality estimation and auto-tuning are optimization\nproblems and thus have a natural proximity to machine learning.\nThe connection to indexing becomes evident when we examine a\nspecial case of indexing.\nProblem Statement. Given a sorted, densely packed array ğ´of\nkeys and a query ğ‘„asking for a particular key ğ‘¥ğ‘–that may or may\nnot exist in that array, return the array index ğ‘–of that keyğ‘¥ğ‘–.\nIn other words, we are looking for a function that assigns to each\nkey its position in the sorted array. Traditionally, this function is\nimplemented by a suitable algorithm like binary search or a data\nstructure like a B-tree. In contrast, Kraska et al .[23] observe that\nthis function can be learned through regression, effectively making\nthe indexing problem a machine learning task. Based on this obser-\nvation, Kraska et al .[23] present the recursive model index (RMI)\nas first learned index with remarkable results in terms of lookupperformance. We wanted to understand the performance benefits of\nRMIs early on and therefore tried to reproduce the results. However,\nwe quickly encountered several issues.\nHyperparameter configuration. Configuring RMIs involves set-\nting several hyperparameters. Unfortunately, the exact configura-\ntions with which the remarkable results were obtained were not\nreported and in some cases even described misleadingly. The use\nof neural networks is mentioned frequently throughout the experi-\nmental evaluation of the original paper. However, the low model\nevaluation times reported in Fig. 4 strongly suggest that none of the\nbest-performing configurations actually uses neural networks. In\npersonal communication with the first author in August 2019, we\nlearned that linear models should be preferred over neural networks\nin most cases. In our experience, there is still a misconception in the\ncommunity today that RMIs internally use neural networks. Subse-\nquent studies [ 20,25] involving inventors of the RMI investigated\nthe performance benefits of learned indexes over traditional indexes.\nHowever, hyperparameter configurations for the reported results\nwere obtained by a time-consuming enumeration process [ 26]. As\na result, similar to the original paper [ 23], the studies neither show\nhow the choice of hyperparameters affects performance, nor do they\ngive advice for configuring RMIs in practice besides enumeration.\nClosed source. The source code of the original paper was never\nmade available. A so-called reference implementation [ 25] was\npublished in December 2019, two years after the preprint [ 22].\nHowever, that implementation differs from the descriptions in the\noriginal paper in terms of model types and error bounds.\nGoals: We pursue the following objectives with this paper.\nâ€¢Conduct the first inventor-independent detailed analysis of RMIs\nto understand the impact of each hyperparameter on prediction\naccuracy, lookup time, and build time.\nâ€¢Develop a clear and simple guideline for database architects on\nhow to configure RMIs with good lookup performance.\nâ€¢Provide a clean and easily extensible implementation of RMIs.\nContributions: We make the following contributions to achieve\nthese goals.\n(1) Learned Tree-Structured Indexes. We revisit in detail recur-\nsive model indexes [ 23] and explain how they are trained and what\nhyperparameters to consider (Section 2). We provide a detailed\noverview on the design dimensions of learned indexes and the\nalready large body of work in that space (Section 3).\n(2) Hyperparameter Analysis. We present our experimental setup (Sec-\ntion 4) and conduct a set of extensive experiments to analyze the\nimpact of each hyperparameter on predictive accuracy and search\ninterval size (Section 5), lookup performance (Section 6), and build\ntime (Section 7).\n(3) Configuration Guideline. Based on our findings, we develop\na simple guideline to configure RMIs in practice (Section 8).\n(4) Comparison with Other Indexes. We compare the RMIs re-\nsulting from our guideline in terms of lookup time and build timearXiv:2106.16166v2  [cs.DB]  22 Nov 2021\n\nğ‘“0\n0\nğ‘“0\n1ğ‘“1\n1Â·Â·Â·\nğ‘“0\n2ğ‘“1\n2ğ‘“2\n2Â·Â·Â·Layerğ‘™2Layerğ‘™1Layerğ‘™0Lookup key:ğ‘¥=42\n2729323536373942444649 ... ...\nerror interval\n[posâ€“err ,pos+err ]pos\nFigure 1: A three-layer RMI that is evaluated on key 42 yield-\ning estimated position pos(prediction). Based on pos, the\nsorted array is searched for the key (error correction).\nwith a number of learned indexes like ALEX [ 14], PGM-index [ 16],\nRadixSpline [ 21], and the reference implementation of RMIs [ 26], as\nwell as state-of-the-art traditional indexes like B-tree [ 12], ART [ 24],\nand Hist-Tree [13] (Section 9).\n2 RECURSIVE MODEL INDEXES\nIn this section, we recap recursive model indexes, how to perform a\nlookup, how they are trained, and what their hyperparameters are.\n2.1 Core Idea\nRMIs are based on the observation that the position of a key in\na sorted array can be computed using the cumulative distribution\nfunction (CDF) of the data. Let ğ·be a dataset consisting of ğ‘›=|ğ·|\nkeys. Further, let ğ‘‹be a random variable that takes each keyâ€™s\nvalue with equal probability and let ğ¹ğ‘‹be the CDF of ğ‘‹. Then, the\npositionğ‘–of each key ğ‘¥ğ‘–âˆˆğ·in the sorted array is computed as:\nğ‘–=ğ¹ğ‘‹(ğ‘¥ğ‘–)Â·ğ‘›=ğ‘ƒ(ğ‘‹â‰¤ğ‘¥ğ‘–)Â·ğ‘› (1)\nNote that in the context of learned indexes, the term CDF is fre-\nquently used synonymously for a mapping from key to position\nin the sorted array instead of its statistical definition of a mapping\nfrom key to the probability that a random variable will take a value\nless than or equal to that key. In the following, we submit to the\nformer interpretation.\nThe core idea of an RMI is to approximate the CDF of a dataset\nby means of a hierarchical, multi-layer model. Consider Figure 1 for\nan example three-layer RMI. Each model in an RMI approximates\na segment of the CDF, all models of a layer together approximate\nthe entire CDF. An RMI is a directed acyclic graph (DAG), i.e., in\ncontrast to a tree, a node (or model) in an RMI may have multiple\ndirect predecessors. We denote the ğ‘–-th layer of a ğ‘˜-layered RMI\nbyğ‘™ğ‘–where 0â‰¤ğ‘–â‰¤ğ‘˜âˆ’1and refer to the ğ‘—-th model of the ğ‘–-th\nlayer byğ‘“ğ‘—\nğ‘–. The first layer ğ‘™0of an RMI always consists of a single\nroot modelğ‘“0\n0. Each subsequent layer may consist of an arbitrary\nnumber of models. The number of models of a layer ğ‘™ğ‘–is denoted\nby|ğ‘™ğ‘–|and called the size of the layer.2.2 Index Lookup\nA lookup is performed in two steps: (1) Prediction : We evaluate the\nRMI on a given key yielding a position estimate. (2) Error correction :\nWe search the key in the area around the estimated position in the\nsorted array to compensate for estimation errors. We discuss both\nsteps in more detail below.\nPrediction. Consider again Figure 1 that shows an index lookup for\nkey 42. We start by evaluating the root model ğ‘“0\n0on key 42 yielding\na position estimate. Based on this estimate, model ğ‘“0\n1in the next\nlayerğ‘™1is chosen for evaluation. This iterative process is continued\nuntil the position estimate posof the last layer is obtained.\nDefinition (Prediction) : Letğ‘…be ağ‘˜-layer RMI trained on dataset ğ·\nconsisting of ğ‘›=|ğ·|keys. Let us denote the value ğ‘restricted to\nthe interval[ğ‘,ğ‘]by\nâŸ¦ğ‘âŸ§ğ‘\nğ‘:=max\u0000ğ‘,min(ğ‘,ğ‘)\u0001. (2)\nThe predicted position for key ğ‘¥of layerğ‘™ğ‘–is recursively defined as\nğ‘“ğ‘–(ğ‘¥)=ï£±ï£´ï£´ï£´ ï£²\nï£´ï£´ï£´ï£³ğ‘“0\n0(ğ‘¥) ğ‘–=0\nğ‘“j\nâŸ¦|ğ‘™ğ‘–|Â·ğ‘“ğ‘–âˆ’1(ğ‘¥)/ğ‘›âŸ§|ğ‘™ğ‘–|âˆ’1\n0k\nğ‘–(ğ‘¥)0<ğ‘–<ğ‘˜(3)\nIntuitively, to determine the model in layer ğ‘™ğ‘–that is evaluated on\nkeyğ‘¥, the estimate ğ‘“ğ‘–âˆ’1(ğ‘¥)of the previous layer is scaled to the\nsize of the current layer. Note that ğ‘“ğ‘–âˆ’1(ğ‘¥)might be less than 0 or\ngreater than ğ‘›âˆ’1. Thus, the result is restricted to [0,|ğ‘™ğ‘–|âˆ’1]to\nevaluate to a valid model index. The predicted position for key ğ‘¥of\nRMIğ‘…is the output of layer ğ‘™ğ‘˜âˆ’1:\nğ‘…(ğ‘¥)=ğ‘“ğ‘˜âˆ’1(ğ‘¥). (4)\nError correction. Based on the estimate ğ‘…(ğ‘¥)obtained by evalu-\nating the RMI, the sorted array is searched for the key. In Figure 1,\nthe position estimate for key 42 points to key 37 in the sorted array.\nSince 37 <42, we have to search to the right of 37 to find 42. To\nfacilitate the search, an RMI may store error bounds that limit the\nsize of the interval that has to be searched. The RMI guarantees that\nif a key is present, then it can be found within the provided error\nbounds. A simple way of achieving this is to store the maximum\nabsolute error errof the RMI. The left and right search bounds,\ni.e., the error interval, is set to [ğ‘…(ğ‘¥)âˆ’err,ğ‘…(ğ‘¥)+err]. If the key\nexists, it must be within these bounds. We search the interval for\nkeyğ‘¥using an appropriate algorithm like binary search.\n2.3 Training Algorithm\nThe goal of the training process is to minimize the prediction error.\nThe training algorithm is shown in Listing 1. Its core idea is to\nperform a top-down layer-wise bulk loading. We start by assigning\nall keys to the root model (line 4). Then, the root model is trained\non those keys (line 7). Afterwards, the keys are assigned to the\nnext-layer models based on the root modelâ€™s estimates (lines 9â€“11).\nWe proceed by training the models of the next layer on the keys\nthat were assigned to them. This process is repeated for each layer\nuntil the last layer has been trained. Finally, if desired, error bounds\ncan be computed on the trained RMI (after line 11).\n2\n\nListing 1 RMI Training Algorithm.\nInput: Datasetğ·, number of layers ğ‘˜, array of layer sizes ğ‘™\nOutput: RMIğ‘…\n1:procedure BuildRMI (ğ·,ğ‘˜,ğ‘™)\n2:ğ‘…:=Array2D() âŠ²Initialize dynamic array to store models.\n3: keys :=Array2D() âŠ²Initialize dynamic array to store each modelâ€™s keys.\n4: keys[0,0]:=ğ· âŠ²Assign all keys to the root model.\n5: forğ‘–â†0toğ‘˜âˆ’1do\n6: forğ‘—â†0toğ‘™[ğ‘–]âˆ’1do\n7: ğ‘…[ğ‘–,ğ‘—]:=TrainModel (keys[i, j]) âŠ²Train modelğ‘—of layerğ‘–.\n8: ifğ‘–<ğ‘˜âˆ’1then âŠ²Check whether current layer is not last layer.\n9: for allğ‘¥inkeys[ğ‘–,ğ‘—]do\n10: ğ‘:=GetModelIndex (ğ‘¥,ğ‘…[ğ‘–,ğ‘—],ğ‘™[ğ‘–+1],|ğ·|)\n11: keys[ğ‘–+1,ğ‘].add(ğ‘¥)âŠ²Assign keyğ‘¥to next-layer model ğ‘.\n12: returnğ‘…\n13:function GetModelIndex (ğ‘¥,ğ‘“,ğ‘,ğ‘› )\n14: returnj\nâŸ¦ğ‘Â·ğ‘“(ğ‘¥)/ğ‘›âŸ§ğ‘âˆ’1\n0k\nâŠ²Compute model index according to Equation (3).\n2.4 Hyperparameters\nRMIs offer a high degree of freedom in configuration and tuning. In\nthe following, we briefly describe each hyperparameter. We provide\na set of possible configurations for each parameter in Section 4.2\nwhen describing the experimental setup.\nModel types. Model types are crucial to the predictive quality of\nRMIs. While simple models, e.g., linear regression, are small and\nfast to train and evaluate, complex models, e.g., neural networks,\nmight offer higher accuracy, but are slow to train and evaluate.\nLayer count. The number of layers ğ‘˜determines the depth of an\nRMI. While a deeper RMI might distribute the keys more evenly\nover the last-layer models, deeper RMIs are larger in size and take\nlonger to train and evaluate.\nLayer sizes. The size of a layer defines the number of models\nin that layer. A higher number of models leads to more accurate\npredictions since the segments that the models have to cover are\nsmaller.\nError bounds. Error bounds facilitate the error correction by lim-\niting the size of the interval that has to be searched. Error bounds\ncan be chosen on different granularities or be omitted altogether.\nSearch algorithm. Depending on the error bounds, several search\nalgorithms may be applied to perform error correction, e.g., binary\nsearch, linear search, or exponential search.\n3 RELATED WORK\nThe introduction of learned indexes by Kraska et al .[23] caused\nboth excitement and criticism within the database community. Early\ncriticism mainly focused on the lack of efficient updates, the rel-\natively weak baselines, and the absence of an open-source imple-\nmentation [ 11,27]. Later, Crotty [ 13] claimed that the performance\nadvantages of learned indexes are primarily due to implicit as-\nsumptions on the data such as sortedness and immutability. Subse-\nquently published learned indexes addressed some of these weak-\nnesses [ 14,16,17]. Nevertheless, RMI remains one of the fastest\nindexes in experimental evaluations [13, 20, 21, 25].\n3.1 Learned Indexes\nExisting learned indexes commonly approximate the CDF. These\nindexes most notably differ in (1) the type of model they use to\napproximate the CDF, (2) whether they are trained bottom-up orTable 1: Overview of learned indexes.\nIndex Model Training Updates Open-Source\nRMI [23] Multiple top-down ( âœ“)\nFITing-tree [17] PLA bottom-up âœ“\nALEX [14] PLA top-down âœ“ âœ“\nPGM-index [16] PLA bottom-up âœ“ âœ“\nRadixSpline [21] LS bottom-up âœ“\ntop-down, and (3) whether they support updates. Table 1 gives an\noverview of existing learned indexes.\nFITing-tree. FITing-tree [ 17] models the CDF using piecewise lin-\near approximation (PLA). During training, a dataset is first divided\ninto variable-sized segments by a greedy algorithm in a single pass\nover the data. The segments are created in such a way that their\nlinear approximation satisfies a user-defined error bound. Segments\nare then indexed by bulk loading them into a B-tree. A lookup con-\nsists of traversing the B-tree to find the segment that contains the\nkey, computing an estimated position based on the linear approx-\nimation of the segment, and searching the key within the error\nbounds around the estimated position. FITing-tree supports inserts,\neither in-place by shifting existing keys within the segment or using\na buffering strategy, where each segment has a buffer that is merged\nwith the other keys in the segment whenever the buffer is full. Un-\nfortunately, at the time of writing, no open-source implementation\nof FITing-tree was available which kept us from including it in our\nexperiments.\nALEX. ALEX [ 14] uses a variable-depth tree structure to approxi-\nmate the CDF with linear models. Internal nodes are linear models\nwhich, given a key, determine the child node. Leaf nodes hold the\ndata, the distribution of which is again approximated by a linear\nmodel. During a lookup the tree is traversed until a leaf node is\nreached, then a position is predicted using the leafâ€™s linear model,\nand finally, the key is searched using exponential search. Like RMI,\nALEX is trained top-down, however, ALEX has a dynamic structure\nthat is controlled by a cost model, which decides how to split nodes.\nALEX supports inserts by splitting or expanding full nodes.\nPGM-index. PGM-index [ 16] also approximates the CDF by means\nof PLA. Similar to FITing-tree, PGM-index starts by computing seg-\nments that satisfy an error bound. However, in contrast to FITing-\nTree, PGM-index creates a PLA-model that is optimal in the number\nof segments. Each segment is represented by the smallest key in\nthat segment and a linear function that approximates the segment.\nAfterwards, this process is continued recursively bottom-up by\nagain creating a PLA-model on the smallest keys of each segment.\nThe recursion is terminated as soon as a single segment is left. So\nunlike ALEX, each path from the root model to a segment is of\nequal length. A lookup is an iterative process where on each level\nof the PGM-index (1) a linear model predicts the next-layer seg-\nment containing the key, (2) the correct segment is searched within\nthe error bounds around the prediction using binary search, and\n(3) the process is continued for the next-layer segment until the\nsorted array of keys is reached. Ferragina and Vinciguerra [ 16] also\nintroduce variants of PGM-index that support updates (dynamic\nPGM-index) and compression on the segment level (compressed\nPGM-index). The size of PGM-index depends on the number of\nsegments required to satisfy the used-defined error bound.\n3\n\nRadixSpline. In contrast to the aforementioned learned indexes,\nRadixSpline [ 21] approximates the CDF using a linear spline. The\nlinear spline is fit in a single pass over the data and to satisfy a\nuser-defined error bound. The resulting spline points are inserted\ninto a radix table that maps keys to the smallest spline point with\nthe same prefix. The size of the radix table depends on the user-\ndefined prefix length. A lookup consists of finding the spline points\nsurrounding the lookup key using the radix table, performing linear\ninterpolation between the spline points to obtain an estimated\nposition, and applying binary search in the error interval around\nthe estimated position to find the key. Like RMI, RadixSpline has a\nfixed number of layers and does not support updates.\n3.2 Experiments and Analysis\nMarcus et al .[26] published an open-source implementation of\nRMIs along with an automatic optimizer in December 2019. The\nreference implementation differs in some respects from the original\ndescription [ 23]. For instance, model types like B-tree nodes and\nneural networks are missing and error bounds are determined on a\ndifferent granularity. Given a dataset, the optimizer uses exhaus-\ntive enumeration to determine a set of pareto-optimal (in terms of\nlookup time and index size) two-layer RMI configurations consist-\ning of first-layer model type, second-layer model type, and second-\nlayer size. Instead of blindly performing this costly enumeration,\nour work aims to understand the impact of each hyperparameter\nand to develop a simple guideline. Further, in addition to model\ntypes and layer sizes, we also consider error bounds and search\nalgorithms when configuring RMIs.\nKipf et al .[20] introduced the Search On Sorted Data (SOSD)\nbenchmark, a benchmarking framework for learned indexes. Be-\nsides providing a variety of index implementations, they supply\nfour real-world datasets. In their preliminary analysis, the authors\nconclude that RMI and RadixSpline are able to outperform tradi-\ntional indexes including ART [ 24], FAST [ 19], and B-trees while\nbeing significantly smaller in size. The authors also state that the\nlack of efficient updates, long building times, and the need for\nhyperparameter tuning are notable drawbacks of learned indexes.\nAs a follow-up, Marcus et al .[25] conduct a more detailed exper-\nimental analysis of learned indexes based on the framework and\ndatasets from SOSD [ 20]. The authors perform a series of experi-\nments to explain the superior performance of learned indexes and\nconclude that a combination of fewer cache misses, branch misses,\nand instructions account for most of the improved performance\ncompared to traditional indexes. Further, the authors show that\nlearned indexes are pareto-optimal in terms of size and lookup\nperformance independently of dataset and key size.\nBoth aforementioned studies [ 20,25] involve inventors of the\nRMI and aim to explain the performance of learned indexes in\ngeneral. Since the evaluated RMI configurations were obtained\nusing the optimizer [ 26], the studies neither show the impact of\nincorrectly configuring an RMI, nor do the studies provide advice\non how to configure RMIs outside of using the optimizer. In contrast,\nto the best of our knowledge, we conduct the first independent and\nholistic analysis of RMIs that directly compares configurations and\naims to explain their performance.Ferragina et al .[15] take a theoretical approach at understand-\ning the benefits of learned indexes, specifically of indexes based on\nPLA. The authors show that for a number of distributions, PGM-\nindex [ 16], while achieving the same query time complexity as\nB-trees, offers improved space complexity. To support their theo-\nretical results, the authors conduct several experiments both on\nsynthetic and real-world datasets. The theoretical results build a\nsolid foundation for further research. However, since RMIs are\nneither limited to PLA nor do RMIs aim to construct the optimal\nnumber of segments, the results cannot be transferred to RMIs.\n4 EXPERIMENTAL SETUP\nIn this section, we introduce the implementation, hyperparameter\nsettings, datasets, and workload used in our experiments and base-\nlines considered for comparison. All experiments are conducted on\na Linux machine with an IntelÂ®XeonÂ®CPU E5-2620 v4 (2.10 GHz,\n20 MiB L3) and 4x8 GiB DDR4 RAM. Our code is compiled with\nclang-12.0.1 , optimization level -O2, and executed single-threaded.\n4.1 Implementation\nOur implementation of RMIs is written in C ++. RMI classes have\na fixed number of layers and model types are passed as template\narguments. This implies that all models in a layer are of the same\ntype. Training algorithms of the model types are adapted from\nthe reference implementation [ 26]. When assigning keys to the\nnext-layer models, the reference implementation always copies\nkeys to a new array. We optimized the training process based on\nthe observation that the models considered here are monotonic\nand will never create overlapping segments. Thus, when assigning\nkeys to next-layer models, we simply store iterators on the sorted\narray of the first and last key of each segment. We then train the\nnext-layer models by passing them the respective iterators and\nthereby avoid copying the keys. Further, instead of training all\nmodels on a mapping from key to position in the sorted array, we\ntrain inner layers on a mapping from key to next-layer model index\nwhich is obtained by scaling the position to the size of the next\nlayer similar to Equation (3). In other words, we train inner layers\ndirectly on a targeted equal-width segmentation. This approach\nsaves a multiplication and division during lookup that are otherwise\nrequired for computing the model index from the position estimate.\nOur entire codebase is available on GitHub.1\n4.2 Hyperparameters\nIn the following, we give a list of hyperparameter configurations\nevaluated in our experiments and briefly compare them against\nthose considered by the reference implementationâ€™s optimizer [ 26].\nModel types. Table 2a lists the model types considered in our eval-\nuation. Linear regression (LR) is a linear model that minimizes the\nmean squared error (MSE). Linear spline (LS) and cubic spline (CS)\nfit a linear respectively cubic spline segment through the leftmost\nand rightmost data points. Radix (RX) eliminates the common prefix\nand maps keys to their most significant bits. Models most notably\ndiffer in three respects.\n(1)Built time. LS, CS, and RX are fast to build from the leftmost\nand rightmost key. LR, a regression method, is built on all keys.\n1https://github.com/BigDataAnalyticsGroup/analysis-rmi\n4\n\nTable 2: Evaluated hyperparameter configurations.\n(a) Model types\nAbrv. Method Formula\nLR L inear R egression ğ‘“(ğ‘¥)=ğ‘ğ‘¥+ğ‘\nLS L inear S pline ğ‘“(ğ‘¥)=ğ‘ğ‘¥+ğ‘\nCS C ubic S pline ğ‘“(ğ‘¥)=ğ‘ğ‘¥3+ğ‘ğ‘¥2+ğ‘ğ‘¥+ğ‘‘\nRX R adix ğ‘“(ğ‘¥)=(ğ‘¥â‰ªğ‘)â‰«ğ‘(b) Error bounds\nAbrv. Method Granularity\nLInd L ocal Ind ividual [23] max +/- error per model\nLAbs L ocal Abs olute [26] max abs error per model\nGInd G lobal Ind ividual max +/- error per RMI\nGAbs G lobal Abs olute max abs error per RMI\nNB N o Bounds [23] -(c) Search algorithms\nAbrv. Method\nBin Bin ary Search\nMBin M odel-biased Bin ary Search [23]\nMLin M odel-biased Lin ear Search\nMExp M odel-biased Exp onential Search [23]\n(2)Evaluation time. RX is the fastest to evaluate with only two bit\nshifts. LR and LS are equally fast to evaluate, CS is the slowest.\n(3)Predictive quality. LS and CS are spline techniques whose pre-\ndictive quality is based on how representative the leftmost and\nrightmost keys are. LR minimizes the error across all keys. RX\nis radix-based and therefore only used for segmentation.\nIn addition to the four models listed, the optimizer [ 26] considers\nradix tables and a specialized variant of linear regression (see Sec-\ntion 9.1) for the first layer and cubic splines for the second layer.\nWe decided to evaluate a smaller set of model types to analyze\nthe impact of model types in general. Since the optimizer always\nrecommends LR for the second layer, we only consider LR and LS\nfor the second layer.\nLayer count. Like the optimizer [ 26], we only consider two-layer\nRMIs. It was previously reported that in most cases two layers\nare sufficient to accurately approximate a CDF [ 25,26], which we\nverified for the considered datasets in preliminary experiments. We\nplan to explore multi-layer RMIs as part of future work.\nLayer size. We cover the same wide range of second layer sizes\nbetween 26and 225in power of two steps like the optimizer [26].\nError bounds. We consider five different variants of error bounds\nlisted in Table 2b, which differ in the granularity of the stored\nbounds in two respects. (1) Error bounds might either be computed\nfor each last-layer model (local) or for the entire RMI (global). Global\nbounds, while being more memory efficient, are prone to outliers\nas the single largest error determines the search interval size of\nall lookups. Local bounds are more robust against outliers as an\noutlier only affects the respective model. (2) We can either store the\nmaximum absolute error (absolute) or both the maximum positive\nand negative error individually (individual). While the former is\nagain more space efficient, the latter allows for tighter bounds,\nespecially, if a model either overestimates or underestimates the\nactual position. Additionally, we might not store any bounds (NB).\nBoth local individual (LInd) and NB were suggested by Kraska\net al.[23]. The reference implementation supports local absolute\nbounds (LAbs) and NB, but the optimizer [ 26] always recommends\nLAbs.\nSearch algorithm. The evaluated search algorithms are listed in\nTable 2c. We generally distinguish between two types of search al-\ngorithms: (1) search algorithms that only consider the error bounds\nand (2) search algorithms that also utilize the estimated position\n(model-biased) [ 23]. Standard binary search is an example of the first\ntype of search algorithm. We search the key in the interval between\nthe two error bounds and ignore the position estimate. However,\nbinary search can be adjusted to become model-biased. Instead of\nchoosing the middle element of the interval as first comparisonpoint, we pick the estimated position. Similarly, linear search and\nexponential search can be tweaked to become model-biased. Instead\nof searching the interval from left to right, we start the search from\nthe estimated position and search to the left or right, depending\non whether the prediction is an overestimation or an underestima-\ntion. The search is stopped once it is certain that the key cannot\nbe found anymore. Initially, we also considered standard linear\nsearch and exponential search for our experiments but both always\nperformed worse than their model-biased counterparts. Note that\nnot all combinations of error bounds and search algorithms make\nsense, e.g., in the case of absolute error bounds, model-biased binary\nsearch and standard binary search are essentially the same as the\nestimate will be the center of the interval. Further, model-biased lin-\near and exponential search do not require bounds. Previous studies\ncompared binary [ 20,23,25], linear, and interpolation search [ 25].\nModel-biased variants of linear and exponential search have not\nbeen studied in the context of RMIs so far.\n4.3 Datasets\nLearned indexes are know to adapt well to artificial data sampled\nfrom statistical distributions [ 25]. Therefore, we use the four real-\nworld datasets from the SOSD benchmark [ 20]. Each dataset con-\nsists of 200M 64-bit unsigned integer keys. The CDFs of the four\ndatasets are depicted in Figure 2, zoom-ins show a segment of 100\nconsecutive keys and indicate the amount of noise in the dataset.\nbooks : keys represent the popularity of books on Amazon.\nfb: keys represent Facebook user ID. This dataset contains a small\nnumber of extreme outliers, which are several orders of magnitude\nlarger than the rest of the keys, at the upper end of the key space.\nThese outliers were not plotted in previous studies [20, 25].\nosmc: keys represent cell IDs on OpenStreetMap. This dataset has\nclusters that are artifacts of projecting two-dimensional data into\none-dimensional space [25].\nwiki: keys are edit timestamps on Wikipedia, contains many dupli-\ncates.\n0 4 8\nKey 1e18012Position1e8books\n0.0 0.8 1.6\nKey 1e19fb\n0.0 0.5 1.0\nKey 1e19osmc\n1.0 1.1 1.2\nKey 1e9wiki\nFigure 2: CDFs of four real-world datasets from SOSD [20].\nZoom-ins show segments of 100 consecutive keys.\n5\n\nTable 3: Overview of the considered baselines.\nMethod Type Hyperparameters Source\nRMI [23] Learned model types, layer size [7]\nALEX [14] Learned sparsity [1]\nPGM-index [16] Learned max error [5]\nRadixSpline [21] Learned radix width, max error [6]\nB-tree [12] Tree sparsity [9]\nHist-Tree [13] Tree num bins, max error [2]\nART [24] Trie sparsity [8]\nBinary search Search - [3]\n4.4 Workload\nFor the lookup performance, we consider lower bound queries,\ni.e., for a given key, the index returns an iterator to the smallest\nelement in the sorted array that is equal to or greater than the\nkey. The sorted array is kept in memory and we perform 20M\nlookups per run, the keys of which are sampled from the sorted\narray uniformly at random with a fixed seed. Reported execution\ntimes are the average execution time of the median of three runs.\n4.5 Baselines\nIn Section 9, we compare our RMI implementation against a num-\nber of baselines listed in Table 3 for which we use the referenced\nopen-source implementations. Due to our focus on ranking the\nperformance of RMIs, we consider all publicly available learned\nindexes but only some representatives of traditional indexes.\nLearned indexes. ALEX [ 14], PGM-index [ 16], and RadixSpline [ 21]\nare learned indexes discussed in Section 3.1. The index size of PGM-\nindex and RadixSpline is varied based on the maximum error. Ad-\nditionally, RadixSpline provides a parameter to adjust the size of\nthe radix table that is used to index the spline points. Since we do\nnot consider update performance here, we use the standard vari-\nant of PGM-index, which does not support updates. ALEX does\nnot provide any parameters itself, so we vary its size by adjusting\nthe number indexed keys (sparsity) by inserting only every ğ‘˜-th\nkey. In addition, we also consider the reference implementation of\nRMIs [26] that is configured using its integrated optimizer.\nTraditional indexes. B-tree [ 12] and ART [ 24] are traditional in-\nmemory index structures. We vary the size of B-tree and ART\nby adjusting the number of keys that are inserted. Therefore, we\nuse an implementation of ART that supports lower bound queries\nfrom SOSD [ 20]. The recently published Hist-Tree [ 13] is a tree-\nstructured index. Each inner node in a Hist-Tree is a histogram\nthat partitions the data into equal-width bins. Like learned indexes,\nHist-Tree exploits that the data is sorted. Hist-Tree provides two\ntuning parameters: the number of bins determines the size of inner\nnodes and the maximum error defines a threshold for the size of a\nterminal node. We use an implementation of a Compact Hist-Tree\nthat does not support updates in favor of lookup performance [2].\nBinary search. We also consider standard binary search over the\nsorted array without any index as provided by std::lower_bound .\n5 PREDICTIVE ACCURACY ANALYSIS\nIn this section, we analyze the impact of hyperparameters on the\npredictive accuracy of RMIs. Our analysis is divided into three parts.\nSegmentation (Section 5.1): We investigate how root models of\n0 4 8\nKey 1e18012Estimated\nposition1e8books\n0.0 0.8 1.6\nKey 1e19fb\n0.0 0.6 1.2\nKey 1e19osmc\n1.0 1.1 1.2\nKey 1e9wikiCDF CS LR LS RXFigure 3: CDF approximations by root models of different\ntypes based on which keys are segmented.\n212220\n# of segments0%50%100%Percentage of\nempty segments\nbooks\n212220\n# of segments\nfb\n212220\n# of segments\nosmc\n212220\n# of segments\nwikiCS LR LS RX\nFigure 4: Percentage of empty segments when segmenting\nthe keys with different first-layer models.\ndifferent types divide the keys into segments.\nPosition Prediction (Section 5.2): We analyze how accurately dif-\nferent combinations of models approximate the CDF.\nError Bounds (Section 5.3): We examine how different types of\nerror bounds limit the error interval to be searched.\n5.1 Segmentation\nAn RMI divides the keys into segments based on its root modelâ€™s\napproximation of the CDF. Assuming a root model correctly pre-\ndicts the position of each key, each segment would consist of the\nsame number of keys. Therefore, RMIs aim for an equal-depth seg-\nmentation by design. This approach to segmentation has a crucial\nweakness: it ignores whether the resulting segments can be accu-\nrately approximated by the next-layer models. In contrast, other\nlearned indexes like PGM-index [ 16] and RadixSpline [ 21], which\nare built bottom-up, explicitly create segments that meet a certain\nerror tolerance. Consequently, the quality of an RMIâ€™s segmentation\ncannot be assessed independently of the next layer. In the follow-\ning, we address two problems that may occur when segmenting\nkeys in an RMI: (1) empty segments , which do not contain any keys,\nand (2) large segments , which contain significantly more keys than\nothers. Figure 3 shows the CDFs and the corresponding root model\napproximations.\nEmpty segments. Since there is a second-layer model for every\nsegment, empty segments increase the size of an RMI without im-\nproving the prediction accuracy. Thus, we should aim for as few\nempty segments as possible. Figure 4 shows the percentage of empty\nsegments of each model type on each dataset for a varying number\nof segments. We generally observe that the percentage of empty\nsegments increases with an increasing number of segments. The\nmore accurately a model approximates the CDF, the fewer empty\nsegments it creates. For instance, CS produces empty segments on\nbooks only after a high number of segments is reached. In contrast,\nradix predictions often do not cover the full range of positions,\ne.g., on wiki, leaving the segments associated with the non-covered\n6\n\n212220\n# of segments102104106108Size of largest\nsegment\nbooks\n212220\n# of segments\nfb\n212220\n# of segments\nosmc\n212220\n# of segments\nwikiCS LR LS RXFigure 5: Size of the largest segment when segmenting the\nkeys with different first-layer models.\npositions empty. The clustered distribution of osmc dataset causes\npercentages to be generally higher and to increase more quickly\nsince the keys are distributed over a small number of segments. Due\nto the few extreme outliers that strongly affect the CDF approxima-\ntion of fb, all models map the majority of keys to the same position,\ncausing all of these keys to be assigned to the same segment. In-\ncreasing the number of segments gradually removes the outliers\nfrom this segment, but the segment will continue to contain most\nkeys.\nLarge segments. Large segments potentially follow a more com-\nplex distribution and are more difficult to approximate by the\nsecond-layer models. Therefore, large partitions may negatively\naffect the prediction quality of an RMI. Figure 5 shows the number\nof keys that reside in the largest segment. Again, the more accu-\nrate a model approximates the CDF, the more evenly the keys are\ndistributed over the segments. Logically, the average segment size\ndecreases as the number of segments increases. However, this does\nnot necessarily apply to the largest segment. For LR, the size of the\nlargest partition often remains near-constant. The reason for this is\nthat LR may produce estimates outside the range of valid positions.\nThese out-of-range predictions are then clamped to either the first\nor last valid position. All keys whose prediction is clamped will be\nassigned to the same segment. Increasing the number of segments\nonly decreases the size of these segments until the segments consist\nexclusively of keys whose prediction had to be clamped. CS, LS, and\nRX do not produce estimates outside the range of valid positions\nand therefore do not exhibit this problem. As discussed before, on\nfb, almost all keys reside in a single segment, regardless of the\nnumber of segments and type of the root models. As we will see\nin subsequent experiments, the inability of the considered model\ntypes to segment datasets with extreme outliers is the main reason\nfor inaccurate predictions, large error intervals, and slow lookups\nonfb.\nSummary. When choosing a first-layer model type for segmenta-\ntion, empty and large segments should be avoided. In our experi-\nments, LS and CS produced the most uniform segments. RX tends to\nproduce many empty segments. LR often creates large segments at\nthe upper and lower end of the key space due to clamping. If none\nof the models satisfactorily segments the keys as with fb, more\ncomplex models must be considered.\n5.2 Position Prediction\nTo analyze the impact of model types on prediction accuracy, we\ntrain RMIs of all combinations of first-layer and second-layer model\n100101102103104Median absolute error\nbooks\n books\n101103105107Median absolute error\nfb\n fb\n100101102103104105106Median absolute error\nosmc\n osmc\n28211214217220223\n# of segments100101102103104105106Median absolute error\nwiki\n28211214217220223\n# of segments\nwikiCSLR\nCSLS\nLRLR\nLRLS\nLSLR\nLSLS\nRXLR\nRXLS\nFigure 6: Median absolute error of RMIs with different com-\nbinations of first-layer and second-layer models.\ntypes with different second-layer sizes on the four datasets. In Fig-\nure 6, we report the median absolute error over all keys as a measure\nof deviation between predicted position and actual position. We\ndecided against reporting the mean absolute error due to variances\ncaused by high errors on the large partitions when segmenting\nwith LR. In the remainder, we refer to an RMI that uses RX and LR\nin the first and second layer, respectively, as RX â†¦â†’LR.\nAs expected, RMIs with more segments and thus more second-\nlayer models generally produce more accurate predictions. On both\nthebooks and wiki dataset, RMIs with more than 219second-layer\nmodels even achieve errors in single digits. The osmc and fbdataset\nare more difficult to approximate. The osmc dataset has a clustered\ndistribution that results in a high number of empty segments, mak-\ning non-empty segments larger on average. Additionally, these\nsegments often have a significant amount of noise and cannot be\napproximated precisely with the models considered here. Similarly,\nthe large prediction error of fbcan also be attributed to the single\nlarge segment. The sudden drop in prediction error between 215\nand 217segments is due to fewer of the outliers being assigned to\nthe large segment anymore. Although the distribution within that\n7\n\n101102103104105106Median search\ninterval size\nbooks (LSLR)\n books (RXLS)\n102104106108Median search\ninterval size\nosmc (LSLR)\n osmc (RXLS)\n103\n102\n101\n100101102103\nIndex size [MiB]101102103104105106107Median search\ninterval size\nwiki (LSLR)\n103\n102\n101\n100101102103\nIndex size [MiB]\nwiki (RXLS)\nGAbs GInd LAbs LIndFigure 7: Comparison of error interval sizes for different er-\nror bounds for two example model combinations.\nlarge segment is close to uniform, it still contains a considerable\namount of noise that leads to the persistent high prediction error.\nComparing the different RMI configurations, RMIs with LR, LS,\nand CS as root model achieve similar errors while RX performs\nslightly worse. This indicates that in terms of prediction accuracy,\nRX is less suitable for segmentation. Regarding the second-layer\nmodels, LR always achieves lower errors than LS. This is expected\nsince LR is the only regression model and minimizes the MSE.\nSummary. For the first layer, a segmentation that distributes the\nkeys over many models is a prerequisite for high prediction accu-\nracy. For the second layer, regression models like LR achieve higher\naccuracy than spline models since regression models minimize the\nprediction error. Increasing the second-layer size of an RMI further\nimproves its accuracy. Overall, LS â†¦â†’LR and CSâ†¦â†’LR achieve good\naccuracy across datasets, except for fbdue to poor segmentation.\n5.3 Error Bounds\nError bounds facilitate correcting prediction errors by limiting the\nsize of interval that has to be searched during a lookup. To evaluate\nthe impact of different error bounds, we again train RMIs with\nall combinations of first-layer and second-layer model type and\nvarying second-layer size. For each configuration, we compute error\nbounds of different types and record the error interval sizes over all\nkeys. In Figure 7, we report the median error interval size, i.e., the\nmedian number of keys that have to be searched during a lookup.\nHere, we only show two combinations of models and omit fbas the\nsize of the error interval remains near constant due to inaccurate\npredictions. The complete experimental results can be found in\nFigure 16 in Appendix A.Global bounds consistently lead to significantly larger error in-\ntervals than local bounds, despite the fact that at a similar index\nsize, global bounds allow for more second-layer models and achieve\non average more accurate predictions. Global bounds, however, are\nprone to single bad predictions, whereas local bounds are more\nrobust because they refer to only one model. LInd and LAbs achieve\nsimilar error interval sizes. Spline models, which tend to either\noverestimate or underestimate, profit from LInd. LR, which often\nachieves similar positive and negative errors, works better with\nLAbs as LAbs allows for more second-layer models at a similar size.\nSummary. Considering RMIs of similar size, local bounds consis-\ntently result in smaller error intervals than global bounds. For the\npreferred second-layer model type LR, LAbs achieves smaller error\nintervals due to more second-layer models at a similar index size.\n6 LOOKUP TIME ANALYSIS\nIn this section, we analyze the impact of hyperparameters on the\nlookup performance of RMIs. Our analysis is divided into two parts.\nModel Types (Section 6.1): We investigate the lookup performance\nof different combinations of first and second-layer model types.\nError Correction (Section 6.2): We analyze the impact of error\nbounds and search algorithms on lookup performance.\n6.1 Model Types\nTo evaluate the impact of model types on lookup performance, we\ntrain RMIs of all combinations of first-layer and second-layer model\ntype with varying second-layer sizes. We use no bounds and model-\nbiased exponential search (NB+MExp) for error correction as this\nconfiguration relies solely on the predictive power of the RMI and\nthus most clearly illustrates the differences between the various\ncombinations of model types. In Figure 8, we report the average\nlookup time of each configuration. The dashed horizontal lines are\nthe average time for obtaining a key using binary search.\nFor a fixed index size, the lookup times of different models within\na dataset often differ only slightly, e.g., on osmc and books , all com-\nbinations of models have similar lookup times. However, lookup\ntimes vary significantly across different datasets. This observation\nis consistent with the prediction errors we saw in Section 5.2. The\nreason for this is that lookup time consists of evaluation time and\nerror correction time. The error correction time accounts for the\nmajority of lookup time and is determined by the prediction error.\nHowever, balancing evaluation time and error correction time is a\ntrade-off that has to be carefully considered. In our experiments,\nwe only consider relatively simple models that are fast to evaluate\nand, as a result, there are only minor differences in evaluation time.\nIn preliminary experiments, we also considered neural networks,\nwhich achieved higher prediction accuracy, but the faster error cor-\nrection was overshadowed by a significantly higher evaluation time\nultimately resulting in considerably slower lookups. Of the mod-\nels considered here, CS is the slowest to evaluate. We can observe\nthe impact of its slower evaluation time compared to LS on books\nwhere despite CSâ†¦â†’LR being slightly more accurate than LS â†¦â†’LR,\nLSâ†¦â†’LR achieves faster lookups. Differences in evaluation time are\nparticularly noticeable when the error correction time is relatively\nshort which often is the case for configurations that are larger in\nsize.\n8\n\n0200400600800Lookup time [ns]\nbooks\n books\n02505007501000125015001750Lookup time [ns]\nfb\n fb\n0200400600800100012001400Lookup time [ns]\nosmc\n osmc\n103\n102\n101\n100101102\nIndex size [MiB]02004006008001000Lookup time [ns]\nwiki\n103\n102\n101\n100101102\nIndex size [MiB]\nwikiCSLR\nCSLS\nLRLR\nLRLS\nLSLR\nLSLS\nRXLR\nRXLS\nFigure 8: Comparison of lookup time for different combina-\ntions of models with NB+MExp for error correction.\nSummary. Prediction accuracy is a strong indicator for lookup\nperformance as it determines the error correction time. Therefore,\nmodels like CSâ†¦â†’LR and LSâ†¦â†’LR that achieve good accuracy across\ndatasets should be chosen. However, the more accurate the predic-\ntions are, the more important become differences in evaluation time\nand models that are slightly less accurate but faster to evaluate have\nan advantage. Increasing the second-layer size improves accuracy\nand causes the lookup time to converge.\n6.2 Error Correction\nNext, we examine the impact of eight combinations of error bounds\nand search algorithms for error correction on lookup time. We\nconsider the following combinations. NB is evaluated with MLin\nand MExp as both search algorithms do not use bounds. GInd and\nLInd are evaluated with MBin and Bin. GAbs and LAbs are evaluated\nwith Bin only as MBin and Bin are the same in case of absolute\nbounds. In Figure 9, we report the average lookup time. Here, we\nagain show only two combinations of models and omit fbbecause\nlookup performance could not be significantly improved compared\nto Figure 8 in this experiment. The complete experimental results\ncan be found in Figure 17 in Appendix A.\n0200400600800Lookup time [ns]\nbooks (LSLR)\n books (RXLS)\n0200400600800100012001400Lookup time [ns]\nosmc (LSLR)\n osmc (RXLS)\n103\n102\n101\n100101102103\nIndex size [MiB]02004006008001000Lookup time [ns]\nwiki (LSLR)\n103\n102\n101\n100101102103\nIndex size [MiB]\nwiki (RXLS)\nGAbs+Bin\nGInd+BinGInd+MBin\nLAbs+BinLInd+Bin\nLInd+MBinNB+MExp\nNB+MLinFigure 9: Comparison of lookup time for different error cor-\nrections using two combinations of models as examples.\nWe observe that either a configuration with local bounds or\nwithout any bounds performs best. Local bounds generally perform\nbetter than global bounds, which is consistent with our observation\nfrom Section 5.3. Nevertheless, binary search mitigates differences\nin search interval size drastically, e.g., global and local bounds\nperform almost identical with LS â†¦â†’LR on books , although the search\ninterval sizes differ by more than an order of magnitude. LInd\nand LAbs perform almost identical with a maximum performance\ndifference of factor 1.1x. Similar to what we saw in Section 5.3,\nLS works better with LInd as it tends to either overestimate or\nunderestimate, LR works better with LAbs as its loss function often\ncauses the maximum overestimation and underestimation to be\nsimilar. Considering LInd, there is hardly any difference between\nBin and MBin.\nSimilar to what we saw in Section 6.1 with respect to model\ntypes, the choice of error bounds not only affects error correction\ntime but also evaluation time as error bounds induce overhead for\ncomputing the search intervalâ€™s limits. Hence, RMIs without error\nbounds are faster to evaluate. The faster evaluation is particularly\nnoticeable when the RMI achieves a high prediction accuracy and\nthus fast error correction. In these cases, NB+MExp performs better\nthan configurations with bounds as can be seen with books and\nwiki. To further analyze when to use NB+MExp over configura-\ntions with bounds, we also recorded the mean log 2error as an\nestimate of the number of search steps required by MExp. Starting\nat an mean log 2error of around 7 to 10, NB+MExp is faster than\nLAbs+Bin. NB+MLin requires even lower errors to be similarly fast\nas NB+MExp.\n9\n\n102\n100102\nIndex size [MiB]0246810Build time [s]\nbooks (NB)CSLR\nLRLR\nLSLR\nRXLR\n(a) Layer 1 type\n102\n100102\nIndex size [MiB]0246810Build time [s]\nbooks (NB)LSLS\nLSLR\nRXLS\nRXLR\n (b) Layer 2 type\n102\n100102\nIndex size [MiB]0246810Build time [s]\nbooks (LSLR)\nGAbs\nGIndLAbs\nLIndNB (c) Error bound\nFigure 10: Build times when varying hyperparameters.\nSummary. The best combination of error bounds and search algo-\nrithm depends on the predictive accuracy of the RMI. If the mean\nlog2error is sufficiently small, NB+MExp performs best due to\nRMIs without bounds being faster to evaluate. For larger errors,\nconfiguration with local bounds such as LAbs+Bin perform better.\n7 BUILD TIME ANALYSIS\nIn this section, we analyze the build time of our implementation\nof RMIs and compare it with the reference implementation [ 26].\nRecall that the build process of a two-layer RMI consists of four\nsteps: (1) training the root model, (2) creating segments, (3) training\nthe second-layer models, and (4) computing error bounds. Figure 10\nshows build times on books . Except for minor caching effects on\nlarge configurations, the build time is independent of the dataset.\nThe complete experimental results can be found in Figure 18 in Ap-\npendix A. We discuss each aspect that affects build time individually\nbelow.\nFirst-layer type. Consider Figure 10a for a build time comparison\nof different root models. Models in general and root models in\nparticular not only differ in training time, which affects step (1),\nbut also in evaluation time, which affects steps (2) and (4). The\nmost notable difference between the models in terms of training\ntime is whether a model considers all keys, like LR, or a constant\nnumber of keys, like LS, CS, and RX. Since the evaluation time of\nLR and LS is the same, the difference in build time in Figure 10a can\nbe attributed entirely to the training time of the root model. Like\nLS, RX also considers only two keys for training. Here, the faster\nbuild time of RX is caused by the faster evaluation of RX during\nsegmentation. CS is faster than LR because it again only considers\na constant number of keys but slower than LS because training and\nevaluation are slightly slower.\nSecond-layer type. Consider Figure 10b for a build time compari-\nson of different second-layer models. Analogously to the root model\ntype, the second-layer model type affects training time and eval-\nuation time. Second layers consisting of LS models takes about\ntwo seconds less to train than second layers consisting of LR mod-\nels. Note that in this example, the second layer is never evaluated\nbecause we do not compute bounds. Otherwise, evaluation time\nwould be the same for LR and LS.\nError bounds. Consider Figure 10c for a build time comparison of\ndifferent error bounds. Computing error bounds requires evaluating\nthe RMI on every key plus the actual computation of the bounds.\nThis additional effort explains the difference in built time between\n102\n100102\nIndex size [MiB]010203040Build time [s]\nbooks (NB)LSLR (ours)\nRXLS (ours)\nLSLR (ref)\nRXLS (ref)\n(a) No bounds\n102\n100102\nIndex size [MiB]010203040Build time [s]\nbooks (LAbs)LSLR (ours)\nRXLS (ours)\nLSLR (ref)\nRXLS (ref)\n (b) Local absolute bounds\nFigure 11: Build times of our implementation (ours) and the\nreference implementation (ref) with NB and LAbs.\nNB and configurations with bounds. The difference between indi-\nvidual configurations with bounds is mainly due to branch misses\nwhen calculating the bounds. At similar index size, local bounds trig-\nger more branch misses than global bounds and individual bounds\ntrigger more branch misses than absolute bounds.\nIndex size. Consider again the RMI configuration without bounds\nin Figure 10c. The build time remains almost constant as long as\nthe entire RMI fits in cache (20 MiB). Once the RMI no longer fits in\ncache, the build time increases due to cache misses. Next, consider\nthe configurations with bounds in Figure 10c. Here, the previously\ndescribed branch and cache misses add up and the build time already\nincreases for configurations that are smaller than the cache size.\nThe increase in build time is less pronounced if a configuration\nproduces many empty segments due to less cache misses.\nReference implementation. Figure 11 shows build times of our\nimplementation (ours) and the reference implementation (ref). Fig-\nure 11a and Figure 11b compare configurations with NB and LAbs,\nrespectively. Build times for both types of bounds are almost iden-\ntical for the reference implementation because the reference im-\nplementation always computes bounds during training and only\ndecides later whether these computed bounds are kept or discarded.\nConsidering only configurations with LAbs, our implementation\nimproves build times by 2.5x to 6.3x. We attribute this improvement\nto our optimized segmentation for monotonous root models that\navoids copying keys as described in Section 4.1.\nSummary. RMIs can be built in a matter of seconds. For a given\ncombination of models, the build time remains almost constant as\nlong as the RMI fits in the cache. The computation of error bounds\nleads to additional cache and branch misses, which negatively im-\npact build times.\n8 RMI GUIDELINE\nBased on our findings from the previous sections, we present a\ncompact guideline for configuring RMIs. Our guideline does not\nguarantee to always provide the fastest lookups but it is easy to\nfollow and achieves competitive lookup performance. Given a max-\nimum allowed index size budget , we propose to configure RMIs as\nfollows.\nModels types. LSâ†¦â†’LR with the maximum second-layer size that\nis allowed by the budget . CS and LS both segment most datasets\nwell, but we choose LS as it is slightly faster to train and evaluate.\nAlthough more accurate predictions can be obtained with CS, CS\n10\n\n101\n101103\nIndex size [MiB]200400600800Lookup time [ns]\nbooks\n101\n101103\nIndex size [MiB]25050075010001250\nosmc\n101\n101103\nIndex size [MiB]0200400600800\nwikiRMI (fastest) RMI (guideline)Figure 12: Comparison of lookup time of fastest configura-\ntion and guideline configuration.\nis only faster for small RMIs, where the improvement in search\ntime outweighs the longer evaluation time. LR as second-layer\nmodel minimizes the error and thus always performs better than\nLS. Larger RMIs generally achieve smaller errors and thus perform\nbetter, which is why we choose the maximum number of second-\nlayer models within the budget .\nError correction. LAbs+Bin or NB+MExp. Our experiments show\nthat LAbs+Bin performs better than NB+MExp until a certain error\nthreshold is reached. This error threshold is hardware-dependent\nand must be determined empirically once. We use the mean log 2\nerror as measure of error to estimate the number of search steps\nwith exponential search and determine the error threshold to be 5.8.\nWhenever the mean log 2error of our RMI with NB is below the\nthreshold, we use NB+MExp and LAbs+Bin otherwise.\nFigure 12 compares the lookup times of configurations obtained\nby our guideline with the fastest configurations. As before, we omit\nfbas none of the considered models segments fbwell. We consider\nsize budgets between 2 KiB and 1 GiB. Our guideline is on average\nonly 2.0% slower than the fastest configuration with a maximum\nperformance decline of 11.3% on wiki.\nImplementing our guideline requires training at most two RMIs:\n(1) Train an RMI with LS â†¦â†’LR and NB that is within budget .\n(2) Compute the mean log 2error of the RMI.\n(3)If the error is above the threshold, train and use an RMI with\nLAbs within budget . Otherwise, use the already trained RMI.\nLimitations. In order to be simple and induce as little overhead\nas possible, our guideline neglects some aspects that are required\nfor optimal configuration. (1) Our guideline uses fixed model types.\nWhile LSâ†¦â†’LR works well for datasets without outliers, a more\nsuitable first-layer model must be sought for datasets with outliers.\n(2) Our guideline only chooses between LAbs+Bin and NB+MExp\nbased on a rough estimate of expected search steps. In some cases,\nother error correction strategies are slightly faster.\n9 COMPARISON WITH OTHER INDEXES\nIn this section, we compare our guideline for configuring RMIs\nwith the indexes introduced in Section 4.5 and vary the parameters\nlisted in Table 3 to obtain indexes of different sizes. Configurations\nof our RMI implementation are chosen based on our guideline.\nConfiguration of the reference implementation are chosen based\non its optimizer [26].\n9.1 Lookup Time\nWe first compare lookup times with respect to index size. During\na lookup, each index yields a search range, either through error\n020040060080010001200Lookup time [ns]\nbooks\n fb\n102\n100102104\nIndex size [MiB]020040060080010001200Lookup time [ns]\nosmc\n102\n100102104\nIndex size [MiB]\nwikiRMI (ours)\nRMI (ref)ALEX\nPGM-indexRadixSpline\nHist-TreeB-tree\nARTFigure 13: Comparison of lookup times w.r.t. index size.\nbounds or level of sparsity. We use binary search to find keys in\nthat search range. In Figure 13, we report average lookup times.\nFor indexes with multiple hyperparameters, i.e., RadixSpline and\nHist-Tree, we show pareto-optimal configurations in terms of index\nsize and lookup time for better readability. As a result, the number\nof data points shown differs across dataset. Hist-Tree and ART do\nnot support duplicates and are therefore not evaluated on wiki.\nOverall, our results are consistent with previous reports [ 4,20,25].\nLet us first consider the traditional indexes. Hist-Tree is the\nfastest index on all datasets except wiki, but Hist-Tree needs index\nsizes of 100 MiB and more to reach its full potential. The best-\nperforming configurations of Hist-Tree use a high branching factor\nresulting in few levels while achieving search intervals of less than\n64 keys. B-tree is the only index whose performance is completely\nindependent of the data distribution but also the slowest index,\nbarely beating binary search. ART is always faster than B-tree but\nnoticeably slower than all learned indexes except ALEX.\nThe performance of learned indexes highly depends on the data\ndistribution. Up to a certain index size from which Hist-Tree outper-\nforms the other indexes, learned indexes achieve the fastest lookup\ntimes. This implies that learned indexes work particularly well for\nsmaller index sizes. On books ,fb, and wiki either our implementa-\ntion of RMIs or the reference implementation dominates the other\nlearned indexes. On osmc, both PGM-index and RadixSpline perform\nbetter than RMIs. ALEX is clearly the slowest learned index, which\ncan be attributed to its more complex adaptive structure.\nLet us now compare our RMI implementation and the refer-\nence implementation [ 26]. On books and wiki, our implementation\ndominates the reference implementation despite using our simple\nguideline. There are two reasons for this. (1) Unlike how the opti-\nmizer is described [ 26], the publicly available implementation [ 7]\ndoes not consider evaluation time in its optimization process and\ninstead chooses configurations that achieve the smallest mean log 2\nerror. While this results in selecting the configuration with the\nfastest error correction time, it does not guarantee to select the\n11\n\nBinary search\nRMI (ours)\nRMI (ref)\nALEX\nPGM-index\nRadixSpline\nHist-Tree\nB-tree\nART0200400600800Lookup time [ns]books\nBinary search\nRMI (ours)\nRMI (ref)\nALEX\nPGM-index\nRadixSpline\nHist-Tree\nB-tree\nARTfb\nBinary search\nRMI (ours)\nRMI (ref)\nALEX\nPGM-index\nRadixSpline\nHist-Tree\nB-tree\nARTosmc\nBinary search\nRMI (ours)\nRMI (ref)\nALEX\nPGM-index\nRadixSpline\nHist-Tree\nB-tree\nARTx xwikiEvaluation SearchFigure 14: Comparison of evaluation time and search time\nfor the best-performing configuration of each index.\nconfiguration with the fastest lookup time. The configurations cho-\nsen by our guideline consistently have fast evaluation times at the\ncost of potentially slower error correction. (2) The optimizer of the\nreference implementation always picks LAbs. Our experiments in\nSection 6.2 show that for accurate RMIs, NB+MExp performs better,\nwhich is considered by our guideline. On osmc, no implementation\ndominates the other. Here, RMIs are never sufficiently accurate\nfor our guidelines to deviate from LAbs+Bin. Thus, differences in\nperformance are solely due to the choice of models. On fb, the\nreference implementation clearly dominates our implementation.\nAs discussed before, LS is not sufficient hor segmenting datasets\nwith extreme outliers. Here, the reference implementation chooses\na variant of LR that ignores the lowest and highest 0.01% of keys\nfor segmentation. This approach, while effectively eliminating the\noutliers in fbfrom the segmentation process, only works if there\nare at most 0.01% of outliers at either end of the key space. We did\nnot include this model type in our evaluation because we believe\nthat a more robust solution to segmentation should be sought.\nLet us now examine the composition of lookup time from evalu-\nation time (evaluating the model or traversing the tree) and search\ntime (searching within the error interval or data page). Figure 14\nshows the lookup time of the best-performing configuration of each\nindex divided into evaluation time and search time. There is a trade-\noff between fast evaluation and fast search. RMIs clearly prioritize\nfast evaluation: The evaluation leads to the correct segment in a\nfixed number of steps, but the RMI does not provide any guarantees\non the prediction accuracy. Adding more segments continuously\nimproves the lookup performance because more segments hardly\nincrease the evaluation time while improving the search time. If\nthe evaluation time of our implementation is faster than that of the\nreference implementation, it is because our configuration does not\nuse bounds. In contrast, PGM-index and RadixSpline prioritize fast\nerror correction: Both indexes cap the maximum error at the cost\nof a slower evaluation that requires traversing multiple layers or\nperforming intermediate searches. At some point, the improved\nsearch time of a smaller maximum error does not compensate the\nlonger evaluation time and the lookup performance decreases. Thus,\ndespite fewer hyperparameters than RMIs, configuring PGM-index\nand RadixSpline optimally is an elaborate task.\nSummary. Learned indexes perform well even at small index sizes.\nOverall, Hist-Tree is the fastest evaluated index, but it requires\nsizes of 100 MiB and more to beat learned indexes. Other traditional\nindexes perform significantly worse on sorted data.\n051015202530Build time [s]\nbooks\n fb\n102\n100102104\nIndex size [MiB]051015202530Build time [s]\nosmc\n102\n100102104\nIndex size [MiB]\nwikiRMI (ours)\nRMI (ref)ALEX\nPGM-indexRadixSpline\nHist-TreeB-tree\nARTFigure 15: Comparison of build times w.r.t. index size.\n9.2 Build Time\nNext, we compare build times with respect to index size. In Fig-\nure 15, we report build times which refer to the index configurations\nevaluated in terms of lookup time in Section 9.1. We show the raw\nbuild times without the time required to determine hyperparame-\nters, e.g., by running the reference implementationâ€™s optimizer [ 26]\nor determining pareto-optimal configurations of RadixSpline and\nHist-Tree. Some indexes require data preparation to be built. For\ninstance, ALEX, B-tree, and ART are not only built on the keys but\nalso explicitly require the positions to which these keys should be\nmapped. Since these preparation steps could be circumvented by a\nspecialized implementation, we do not consider it part of the build\ntime.\nThe index size of B-tree, ART, and ALEX is determined by the\nlevel of sparsity. In contrast to learned indexes, these indexes are\nbuilt on a subset of the keys and therefore provide fast build times\nespecially at smaller index sizes. With an increasing number of keys,\nthe structure of these indexes becomes more complex, e.g., more\nlevels are introduced, and the build time increases. In contrast, RMI,\nPGM-index, and RadixSpline are always built on the entire dataset.\nThis means that their build times are higher from the outset. RMI\nand RadixSpline have a fixed number of layers. Therefore, their\nbuild time is hardly impacted by the data distribution and only\nincreases once the index no longer fits into cache. The sudden de-\ncrease in build time of RMIs on books and wiki is caused by the\nguideline choosing an RMI configuration without bounds which is\nfaster to build. PGM-index, on the other hand, has a variable num-\nber of layers. Depending on the data distribution and the desired\nerror, more layers have to be trained leading to a steeper increase\nin build times compared to RadixSpline and RMI. Causes for the\ndifferences in build time between our RMI implementation and the\nreference implementation [ 26] were already discussed in Section 7.\nThe reference implementationâ€™s jumps in build time are caused by\nvarying build times for different model types chosen by its opti-\nmizer. Hist-Tree exhibits similar build times to the learned indexes.\n12\n\nHowever at larger sizes, its built time quickly increases due to the\nincreasing depth of the Hist-Tree.\nSummary. The benefits in terms of lookup performance of learned\nindexes come at the cost of significantly higher build times com-\npared to traditional indexes. Thus, the improvement of build times\nshould be a priority of future work.\n10 CONCLUSION AND FUTURE WORK\nWe provided an extensible open-source implementation of RMIs\nand conducted a comprehensive hyperparameter analysis of RMIs\nin terms of prediction accuracy, lookup time, and build time. Based\non this analysis, we developed a simple-to-follow guideline for\nconfiguring RMIs, which achieves competitive performance. In ad-\ndition, we were able to improve the build time of RMIs by exploiting\nthe monotonicity of models, thereby avoiding the copying of keys\nwhen assigning them to the second-layer models. In the future,\nwe plan to extend our implementation to also support multi-layer\nRMIs and additional model types. We would also like to address the\nproblem of segmenting datasets with extreme outliers.\nREFERENCES\n[1][n.d.]. ALEX: A library for building an in-memory, Adaptive Learned indEX.\nhttps://github.com/microsoft/ALEX. (accessed: 2021-11-08).\n[2][n.d.]. CHT: Implementation of the compact \"Hist-Tree\". https://github.com/\nstoianmihail/CHT. (accessed: 2021-11-08).\n[3][n.d.]. cppreference.com: std::lower_bound. https://en.cppreference.com/w/cpp/\nalgorithm/lower_bound. (accessed: 2021-11-08).\n[4][n.d.]. Learned Index Leaderboard. https://learnedsystems.github.io/\nSOSDLeaderboard. (accessed: 2021-11-08).\n[5][n.d.]. PGM-index: State-of-the-art learned data structure. https://github.com/\ngvinciguerra/PGM-index. (accessed: 2021-11-08).\n[6][n.d.]. RadixSpline: A Single-Pass Learned Index. https://github.com/\nlearnedsystems/radixspline. (accessed: 2021-11-08).\n[7][n.d.]. RMI: The recursive model index, a learned index structure. https://github.\ncom/learnedsystems/RMI. (accessed: 2021-11-08).\n[8][n.d.]. SOSD: A Benchmark for Learned Indexes. https://github.com/\nlearnedsystems/SOSD. (accessed: 2021-11-08).\n[9][n.d.]. TLX: A Collection of Sophisticated C++ Data Structures, Algorithms, and\nMiscellaneous Helpers. https://github.com/tlx/tlx. (accessed: 2021-11-08).\n[10] Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, and Bohan Zhang. 2017.\nAutomatic Database Management System Tuning Through Large-scale Machine\nLearning. In Proceedings of the 2017 ACM International Conference on Management\nof Data, SIGMOD Conference 2017, Chicago, IL, USA, May 14-19, 2017 , Semih\nSalihoglu, Wenchao Zhou, Rada Chirkova, Jun Yang, and Dan Suciu (Eds.). ACM,\n1009â€“1024.\n[11] Peter Bailis, Kai Sheng Tai, Pratiksha Thaker, and Matei Zaharia. [n.d.]. Donâ€™t\nThrow Out Your Algorithms Book Just Yet: Classical Data Structures That Can\nOutperform Learned Indexes. https://dawn.cs.stanford.edu/2018/01/11/index-\nbaselines/. (accessed: 2021-11-08).\n[12] Rudolf Bayer and Edward M. McCreight. 1970. Organization and Maintenance of\nLarge Ordered Indexes. In Record of the 1970 ACM SIGFIDET Workshop on Data\nDescription and Access, November 15-16, 1970, Rice University, Houston, Texas, USA\n(Second Edition with an Appendix) . ACM, 107â€“141.\n[13] Andrew Crotty. 2021. Hist-Tree: Those Who Ignore It Are Doomed to Learn. In\n11th Conference on Innovative Data Systems Research, CIDR 2021, Virtual Event,\nJanuary 11-15, 2021, Online Proceedings . www.cidrdb.org.\n[14] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In Proceedings of the 2020 International Conference on Management of Data,\nSIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020 ,\nDavid Maier, Rachel Pottinger, AnHai Doan, Wang-Chiew Tan, Abdussalam\nAlawini, and Hung Q. Ngo (Eds.). ACM, 969â€“984.\n[15] Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2020. Why Are Learned\nIndexes So Effective?. In Proceedings of the 37th International Conference on Ma-\nchine Learning, ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine\nLearning Research) , Vol. 119. PMLR, 3123â€“3132.[16] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proc. VLDB Endow.\n13, 8 (2020), 1162â€“1175.\n[17] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-aware Index Structure. In Proceedings of\nthe 2019 International Conference on Management of Data, SIGMOD Conference\n2019, Amsterdam, The Netherlands, June 30 - July 5, 2019 , Peter A. Boncz, Stefan\nManegold, Anastasia Ailamaki, Amol Deshpande, and Tim Kraska (Eds.). ACM,\n1189â€“1206.\n[18] Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian\nKersting, and Carsten Binnig. 2020. DeepDB: Learn from Data, not from Queries!\nProc. VLDB Endow. 13, 7, 992â€“1005.\n[19] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D. Nguyen,\nTim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey. 2010. FAST:\nfast architecture sensitive tree search on modern CPUs and GPUs. In Proceedings\nof the ACM SIGMOD International Conference on Management of Data, SIGMOD\n2010, Indianapolis, Indiana, USA, June 6-10, 2010 , Ahmed K. Elmagarmid and\nDivyakant Agrawal (Eds.). ACM, 339â€“350.\n[20] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2019. SOSD: A Benchmark for Learned\nIndexes. CoRR abs/1911.13014.\n[21] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management, aiDM@SIGMOD 2020, Portland,\nOregon, USA, June 19, 2020 , Rajesh Bordawekar, Oded Shmueli, Nesime Tatbul,\nand Tin Kam Ho (Eds.). ACM, 5:1â€“5:5.\n[22] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2017.\nThe Case for Learned Index Structures. arXiv:1712.01208v1 [cs.DB]\n[23] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA,\nJune 10-15, 2018 , Gautam Das, Christopher M. Jermaine, and Philip A. Bernstein\n(Eds.). ACM, 489â€“504.\n[24] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive radix\ntree: ARTful indexing for main-memory databases. In 29th IEEE International\nConference on Data Engineering, ICDE 2013, Brisbane, Australia, April 8-12, 2013 ,\nChristian S. Jensen, Christopher M. Jermaine, and Xiaofang Zhou (Eds.). IEEE\nComputer Society, 38â€“49.\n[25] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. Proc. VLDB Endow. 14, 1 (2020), 1â€“13.\n[26] Ryan Marcus, Emily Zhang, and Tim Kraska. 2020. CDFShop: Exploring and\nOptimizing Learned Index Structures. In Proceedings of the 2020 International\nConference on Management of Data, SIGMOD Conference 2020, online conference\n[Portland, OR, USA], June 14-19, 2020 , David Maier, Rachel Pottinger, AnHai\nDoan, Wang-Chiew Tan, Abdussalam Alawini, and Hung Q. Ngo (Eds.). ACM,\n2789â€“2792.\n[27] Thomas Neumann. 2017. The Case for B-Tree Index Structures.\nhttp://databasearchitects.blogspot.com/2017/12/the-case-for-b-tree-index-\nstructures.html. (accessed: 2021-11-08).\n[28] Andrew Pavlo, Gustavo Angulo, Joy Arulraj, Haibin Lin, Jiexi Lin, Lin Ma,\nPrashanth Menon, Todd C. Mowry, Matthew Perron, Ian Quah, Siddharth San-\nturkar, Anthony Tomasic, Skye Toor, Dana Van Aken, Ziqi Wang, Yingjun Wu,\nRan Xian, and Tieying Zhang. 2017. Self-Driving Database Management Sys-\ntems. In 8th Biennial Conference on Innovative Data Systems Research, CIDR 2017,\nChaminade, CA, USA, January 8-11, 2017, Online Proceedings . www.cidrdb.org.\n[29] Lucas Woltmann, Claudio Hartmann, Maik Thiele, Dirk Habich, and Wolfgang\nLehner. 2019. Cardinality estimation with local deep learning models. In Pro-\nceedings of the Second International Workshop on Exploiting Artificial Intelligence\nTechniques for Data Management, aiDM@SIGMOD 2019, Amsterdam, The Nether-\nlands, July 5, 2019 , Rajesh Bordawekar and Oded Shmueli (Eds.). ACM, 5:1â€“5:8.\nA COMPLETE EXPERIMENTAL RESULTS\nComplete experimental results on the impact of error bounds on er-\nror interval sizes as introduced in Section 5.3 are shown in Figure 16.\nComplete experimental results on the impact of error bounds and\nsearch algorithm on lookup time as introduced in Section 6.2 are\nshown in Figure 17. Complete experimental results on the impact\nof model type and error bounds on build time as introduced in\nSection 7 are shown in Figure 18.\n13\n\n102104106108Median search\ninterval size\nbooks (CSLR)\n fb (CSLR)\n osmc (CSLR)\n wiki (CSLR)\n102104106108Median search\ninterval size\nbooks (CSLS)\n fb (CSLS)\n osmc (CSLS)\n wiki (CSLS)\n102104106108Median search\ninterval size\nbooks (LRLR)\n fb (LRLR)\n osmc (LRLR)\n wiki (LRLR)\n102104106108Median search\ninterval size\nbooks (LRLS)\n fb (LRLS)\n osmc (LRLS)\n wiki (LRLS)\n102104106108Median search\ninterval size\nbooks (LSLR)\n fb (LSLR)\n osmc (LSLR)\n wiki (LSLR)\n102104106108Median search\ninterval size\nbooks (LSLS)\n fb (LSLS)\n osmc (LSLS)\n wiki (LSLS)\n102104106108Median search\ninterval size\nbooks (RXLR)\n fb (RXLR)\n osmc (RXLR)\n wiki (RXLR)\n103\n102\n101\n100101102103\nIndex size [MiB]102104106108Median search\ninterval size\nbooks (RXLS)\n103\n102\n101\n100101102103\nIndex size [MiB]\nfb (RXLS)\n103\n102\n101\n100101102103\nIndex size [MiB]\nosmc (RXLS)\n103\n102\n101\n100101102103\nIndex size [MiB]\nwiki (RXLS)\nGAbs GInd LAbs LIndFigure 16: Comparison of error interval sizes for different error bounds.\n14\n\n0200400600800Lookup time [ns]\nbooks (CSLR)\n02505007501000125015001750\nfb (CSLR)\n0200400600800100012001400\nosmc (CSLR)\n02004006008001000\nwiki (CSLR)\n0200400600800Lookup time [ns]\nbooks (CSLS)\n02505007501000125015001750\nfb (CSLS)\n0200400600800100012001400\nosmc (CSLS)\n02004006008001000\nwiki (CSLS)\n0200400600800Lookup time [ns]\nbooks (LRLR)\n02505007501000125015001750\nfb (LRLR)\n0200400600800100012001400\nosmc (LRLR)\n02004006008001000\nwiki (LRLR)\n0200400600800Lookup time [ns]\nbooks (LRLS)\n02505007501000125015001750\nfb (LRLS)\n0200400600800100012001400\nosmc (LRLS)\n02004006008001000\nwiki (LRLS)\n0200400600800Lookup time [ns]\nbooks (LSLR)\n02505007501000125015001750\nfb (LSLR)\n0200400600800100012001400\nosmc (LSLR)\n02004006008001000\nwiki (LSLR)\n0200400600800Lookup time [ns]\nbooks (LSLS)\n02505007501000125015001750\nfb (LSLS)\n0200400600800100012001400\nosmc (LSLS)\n02004006008001000\nwiki (LSLS)\n0200400600800Lookup time [ns]\nbooks (RXLR)\n02505007501000125015001750\nfb (RXLR)\n0200400600800100012001400\nosmc (RXLR)\n02004006008001000\nwiki (RXLR)\n103\n102\n101\n100101102103\nIndex size [MiB]0200400600800Lookup time [ns]\nbooks (RXLS)\n103\n102\n101\n100101102103\nIndex size [MiB]02505007501000125015001750\nfb (RXLS)\n103\n102\n101\n100101102103\nIndex size [MiB]0200400600800100012001400\nosmc (RXLS)\n103\n102\n101\n100101102103\nIndex size [MiB]02004006008001000\nwiki (RXLS)\nGAbs+Bin GInd+Bin GInd+MBin LAbs+Bin LInd+Bin LInd+MBin NB+MExp NB+MLinFigure 17: Comparison of lookup time for different error correction strategies.\n15\n\n0246810Build time [s]\nbooks (CSLR)\n fb (CSLR)\n osmc (CSLR)\n wiki (CSLR)\n0246810Build time [s]\nbooks (CSLS)\n fb (CSLS)\n osmc (CSLS)\n wiki (CSLS)\n0246810Build time [s]\nbooks (LRLR)\n fb (LRLR)\n osmc (LRLR)\n wiki (LRLR)\n0246810Build time [s]\nbooks (LRLS)\n fb (LRLS)\n osmc (LRLS)\n wiki (LRLS)\n0246810Build time [s]\nbooks (LSLR)\n fb (LSLR)\n osmc (LSLR)\n wiki (LSLR)\n0246810Build time [s]\nbooks (LSLS)\n fb (LSLS)\n osmc (LSLS)\n wiki (LSLS)\n0246810Build time [s]\nbooks (RXLR)\n fb (RXLR)\n osmc (RXLR)\n wiki (RXLR)\n103\n102\n101\n100101102103\nIndex size [MiB]0246810Build time [s]\nbooks (RXLS)\n103\n102\n101\n100101102103\nIndex size [MiB]\nfb (RXLS)\n103\n102\n101\n100101102103\nIndex size [MiB]\nosmc (RXLS)\n103\n102\n101\n100101102103\nIndex size [MiB]\nwiki (RXLS)\nGAbs GInd LAbs LInd NBFigure 18: Comparison of build time for different error bounds.\n16",
  "textLength": 79041
}