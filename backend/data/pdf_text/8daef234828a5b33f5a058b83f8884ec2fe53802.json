{
  "paperId": "8daef234828a5b33f5a058b83f8884ec2fe53802",
  "title": "PANDA: Facilitating Usable AI Development",
  "pdfPath": "8daef234828a5b33f5a058b83f8884ec2fe53802.pdf",
  "text": "PANDA: Facilitating Usable AI Development\nJinyang Gaoy, Wei Wangy, Meihui Zhang\u0005, Gang Chenz, H.V . Jagadish\u0003,\nGuoliang Li\b, Teck Khim Ngy, Beng Chin Ooiy, Sheng Wangy, Jingren Zhou?\nyNational University of Singapore\u0005Beijing Institute of TechnologyzZhejiang University\n\u0003University of Michigan\bTsinghua University\nAlibaba Group\ny{jinyang.gao, wangwei, ngtk, ooibc, wangsh}@comp.nus.edu.sg,\u0005meihui_zhang@bit.edu.cn\nzcg@zju.edu.cn,\u0003jag@umich.edu,\bliguoliang@tsinghua.edu.cn,\njingren.zhou@alibaba-inc.com\nAbstract\nRecent advances in artiﬁcial intelligence (AI) and machine learning have created\na general perception that AI could be used to solve complex problems, and in\nsome situations over-hyped as a tool that can be so easily used. Unfortunately, the\nbarrier to realization of mass adoption of AI on various business domains is too\nhigh because most domain experts have no background in AI. Developing AI appli-\ncations involves multiple phases, namely data preparation, application modeling,\nand product deployment. The effort of AI research has been spent mostly on new\nAI models (in the model training stage) to improve the performance of benchmark\ntasks such as image recognition. Many other factors such as usability, efﬁciency\nand security of AI have not been well addressed, and therefore form a barrier to\ndemocratizing AI. Further, for many real world applications such as healthcare\nand autonomous driving, learning via huge amounts of possibility exploration is\nnot feasible since humans are involved. In many complex applications such as\nhealthcare, subject matter experts (e.g. Clinicians) are the ones who appreciate\nthe importance of features that affect health, and their knowledge together with\nexisting knowledge bases are critical to the end results. In this paper, we take a\nnew perspective on developing AI solutions, and present a solution for making\nAI usable. We hope that this resolution will enable all subject matter experts (eg.\nClinicians) to exploit AI like data scientists.\n1 Introduction\nRecent advances in artiﬁcial intelligence (AI) and machine learning provide many opportunities in\nimproving various applications, business practices and models. For example, AI-based solutions\ndriven by Big Data have achieved human-level performance in computer vision and speech processing\nbenchmarks. The availability of data has caused the rapid development of new models, whose success\nfurther fuels the interest for exploiting data in decision making. It is therefore not surprising that we\nsee an increasing desire to exploit of AI in application areas such as ﬁnance and healthcare.\nHowever, there is a signiﬁcant barrier realizing the mass adoption of AI applications. Developing an\nAI application involves multiple phases, namely data preparation, application modeling, and product\ndeployment. In fact, the effort of AI researchers was spent mostly on new AI models to improve the\nperformance of benchmark tasks, e.g. the ImageNet competition[ 39]. Many other factors such as\nusability, efﬁciency and security of AI have not been well addressed, and therefore form a barrier to\ndemocratizing AI.\nWe have been involved in developing the basic research, understanding and interpretation of require-\nments, to deployment and validation of several such applications. One example is the healthcare\nPreprint. Work in progress.arXiv:1804.09997v1  [cs.AI]  26 Apr 2018\n\nEMR (electronic medical record) application, where we worked with the clinicians, developed the\nmodel [ 55], validated the model, and integrated the application onto the production system after\nvalidation. Figure 1 shows the development pipeline of a healthcare AI systems such as disease\nprogression modelling. Compared to devising a new algorithm on a standard benchmark problem, we\nface the following challenges: 1) There is no standard dataset for any application based on EMR. Data\nare biased, irregular and too noisy to be directly used as input for any ready-made model. Exploration\nfor new features could be very helpful for the speciﬁed application. 2) The usage of model and\nparameter setting strongly depends on a detailed application. Finding a suitable solution requires\nboth strong domain knowledge and machine learning background. 3) High stakes applications require\nstrong reliability for the deployed product.\nData Acquisition:\nHospital Data\nGenome Data\nMedical KB\nCT/MRI ImagesIntegration& \nAugmentation:\nCollaborate Analytics\nGAN Data Cleaning\nKB Data Enrichment\nImage AugmentationUnderstanding& \nInterpretation:\nEMR Bias Resolving\nEMR Imputation\nEMR Embedding\nEMR Pattern Mining\nMedical Logic MiningApplication \nDeployment:\nStandard Model Pool\nExample Models\nAdaptive Regularizer\nKB Hashing Model\nBagging & Evaluation\nExtensive Raw DataCleaned Data with \nRich SemanticsExtracted Effective \nFeature SetsMedical Insights\nFigure 1: AI Development pipeline for Healthcare.\nBased on our experience and observations, we examine the life-cycle of an AI application to locate\nthe speciﬁc research topics related to the barrier from the perspective of AI and Big Data researchers,\nand application developers and data scientists.\nTraining\nDeploymentData \nPreparation \nUsability\nEfficiency \nEffectiveness\nSecurity\nFigure 2: Development life-cycle of an AI application.\nSuccinctly, the development process of an AI application consists of three main stages (Figure 2):\n1) data preparation including acquisition, cleaning, labeling, integration and analytics; 2) speciﬁed\nmodel design for the given application; 3) product deployment which provides reliable service\nefﬁciently. Like the development of other applications, e.g. database applications, we need support\nfrom algorithms, models, tools and systems to ease the processing, reduce the cost, improve the\nperformance and ensure the security of each stage of the development. In the remainder of this paper,\nwe shall discuss each stage to analyze the challenges of realizing these goals while keeping in mind\nour goals: ease of use, effectiveness, efﬁciency, scalability and security.\n2\n\nModel…Training data Test data at day 1 Test data at day 100\nModel\nModel\nModelModel\n…\nCloud…Hospital 1\nHospital 2\nHospital nFigure 3: Collaborative analysis by integrating data from multiple hospitals.\n2 Data Preparation\nIndustrial AI applications are often based on simple yet effective standard learning models. However,\nthe development procedure is not trivial and often painful because of the quantity and quality of data.\nUnlike well-studied benchmark problems which come with a pre-deﬁned training set, the training\ndata of a real AI application is often not well pre-deﬁned, cleaned or integrated. Data is so important\nfor AI applications that it is referred to as the new oil. Currently, most datasets are created manually\nby domain experts or via crowdsourcing. The cleaning, integration, labeling and analyzing procedures\nare tedious and expensive for a large dataset.\n2.1 Visualization and Interaction\nTools with good usability can improve both efﬁciency and effectiveness. However, an easy-to-use\nsystem requires a lot of engineering work on the interface and functionalities.\nWe highlight the role of data visualization. A well-designed data visualization tool greatly assists the\ndomain experts in reviewing the data, exploring the existing large-scale data, performing collaborative\nannotations, and effectively offering their expertise. Some new types of data visualization tools\nhave been developed for this purpose, e.g. the interactive visualization tool with the collaborative\nannotation and recommendation functionalities. In addition, some crowdsourcing platforms allow\nthe embedded Hypertext Markup Language to visualize the data in the micro-tasks. Other related\nresearch work, including the approximate visualization, the auto-ranked visualization, collaborative\nexploration, resolution reduction, explore by example, result recommendations, are extensively\nstudied to make the input of domain experts’ knowledge more friendly. Efﬁciency of the visualization\ntools is also worthy of research, especially for big datasets, as it greatly affects the user experience.\nIntelligent questioning modules must be designed to ask the right questions to extract useful informa-\ntion from domain experts who may not be conversant in AI techniques. Such kind of modules will\nnot only serve to bridge the knowledge gap between domain experts and AI practitioners, they will\nalso serve as gate keepers to ensure the validity of the data. The statistical distribution and bias of the\ndata will have signiﬁcant impact on the success of AI systems as most of them are data-driven.\n2.2 Cost-sensitive Acquisition\nWe factorize the efﬁciency of data preparation into two parts following the efﬁciency deﬁnition of a\ntraining system [ 14]: hardware efﬁciency for the speed of processing one single (or batch) sample;\nstatistical efﬁciency related to the total number of samples to process. The effectiveness refers to the\nquality of the pre-processed data and the performance of the model trained using the data.\nTo reduce the time spent for each sample, only those that require domain knowledge and can reduce\nthe uncertainty of learned model should be presented to the domain experts. The interaction with\ndomain experts may entail in all the processes in the pipeline where there exist uncertainties. The\nchoice of step where interaction should be invoked and how much effort should be spent in it have to\nbe optimized in a quantitative manner.\nTo reduce the total number of samples to process, we need to maximize the quantity of information\ninstilled per sample. Then we can stop the processing early once we have enough knowledge about\nthe data. The accuracy of the returned result is a critical issue. One way to increase the accuracy is\nto collect answers from different domain experts. However, in this case, some algorithms (e.g., the\nmajority voting) have to be designed to reconcile the answers.\n3\n\nFigure 4: Drag-and-drop interface for model construction.\n2.3 Data Privacy\nWith the rapidly growing complexity of AI tasks, AI systems require more extensive cooperation\namong data providers and end-users. Data from multiple sources are required to be integrated and\nmanaged as a data ecosystem. We need a storage system to support collaborative analytics where\ndifferent organizations having similar applications could share the common data processing ﬂow\nwhile maintaining the conﬁdentiality of the data. For example, to train an accurate model for medical\nimage analysis, e.g thoracic disease identiﬁcation based on x-ray images [ 29], hospitals have to\ncollaborate to construct a large labeled image dataset (Figure 3). In such a scenario, part of the\ndata and its processing ﬂow are required to be shared while the security for some other data and\ntheir relevant application model should be strictly protected. We have developed a rich semantic\ndata management and storage system call ForkBase [ 46] based on the principle of immutability,\nsharing and security. Immutability ensures the traceability of data provenance. Sharing and security\nproperties can facilitate the development for collaborative analytics.\n3 Application Modelling\nThere is plenty of research on model architectures and training algorithms. However, implementing\nthose ideas requires expertise knowledge about AI. Moreover, model selection and training conﬁgura-\ntion are typically done by experts with years of experience. All these together create a big barrier for\nAI application developers.\n3.1 Model Selection\nThere are three different levels of AI developers, namely, AI researchers, AI beginners and domain\nexperts. To enable all developers to train models efﬁciently and effectively, research on programming\nabstraction, resource management and user-system interaction is necessary.\nFor AI researchers, they are able to construct their own models using open-source libraries like\nTensorﬂow [ 1]. However, it is still tedious for them to tune many hyper-parameters of the training\nalgorithms, including learning rate, total number of training iterations, etc. In addition, they have to\nmanage many intermediate models and results. In fact, the checkpoint ﬁles for model parameters\ngenerated during the training are large for big models such as VGG [ 41]. A tool with distributed\nhyper-parameter search, e.g. based on Bayesian optimization [ 42] or random search [ 4], is desired.\nA model management database with model compression would save a lot of space and time for\ndevelopers.\n4\n\nDomain \nexperts\nAI \nbeginners\nAI \nresearchersFigure 5: Optimization scope for different AI developers.\nFor AI beginners, a simple, ﬂexible, and extensible interface or programming abstraction is vital\nfor them to get started. Many open source libraries with good programming abstractions have been\ndeveloped, including Keras1and scikit-learn2, which are widely used by students to learn data\nscience and deep learning. A more convenient interface for beginners would be like drag-and-drop or\nplug-and-play on web pages as shown in Figure 4, which sends the models back to the servers for\ntraining and tuning automatically.\nFor domain experts, they know the data well. However, they may have little knowledge about the AI\nmodels and training algorithms. Therefore, it would be better to just let them prepare the data and\nspecify the task. To implement such a system, we need to provide built-in models and model selection\nalgorithms. In fact, many AI applications share the similar models. For example, convolutional\nneural networks (CNN) [ 25] are the backbone models for image classiﬁcation tasks, including vehicle\nclassiﬁcation, ﬂower classiﬁcation, food classiﬁcation, etc. We can also implement other popular\nmodels (like LSTM [ 19], CapsuleNet[ 40]) as built-in models and share them for different applications.\nThere are also multiple models for the same task. For example, InceptionNet [ 44], ResNet [ 16]\nand SqueezeNet [ 20] are all CNN models for image classiﬁcation. However, they have different\ncharacteristics, where some models are more accurate but more resource hungry. Model selection is a\nresearch problem [ 28], which trades off between efﬁciency (i.e. speed and memory) and effectiveness\n(i.e. accuracy).\nThe features related to usability for the three types of AI developers are summarized in Figure 5. The\nfeatures from the inner circles beneﬁt developers in the same circle and in the outer circles.\n3.2 Cost-sensitive Modelling\nThe recent resurgence of AI is mainly driven by deep learning, which expands traditional machine\nlearning models with more complex structures to increase their capability of modeling data. From\nthe statistics of a famous visual recognition challenge, ILSVRC, the number of layers of the annual\nwinning model increases from 8 layers in 2012 to 152 layers in 2015. For example, the number of\nlayers of deep convolutional neural networks (CNN) have reached one thousand [ 16]. DeepForest [ 57]\nmodel stacks multiple random forests together. Models for text comprehension including question\nanswering, typically combine many recurrent neural networks with attention modeling [ 49]. New\nmodels, like CapsuleNet [ 40], are also very complex in terms of the operations and number of\nparameters. At the same time, the training dataset size is also increasing sharply. On the one hand, big\ndatasets are required by complex models to avoid over-ﬁtting. On the other hand, big datasets need\nlarge models to capture the complex data regularities. Thereafter, datasets and models are affected by\neach other, and both grow in size and complexity. We do see better performance (i.e accuracy) as a\nconsequence. However, we also notice the efﬁciency cost in terms of computation, memory and disk\ncost. Model compression [ 20] replaces some complex structures in the model architecture with simple\nones. For instance, fully connected layers in CNNs are replaced with fully convolutional layers[33].\nSquared convolution ﬁlters are factorized into 1-dimensional convolution ﬁlters [ 44]. Bottleneck\nconvolution layers are also widely used [ 44]. Architecture optimization (or search) for efﬁciency\n(without deteriorating the performance) is now mainly done based on experience and trial-and-error.\n1https://keras.io/\n2http://scikit-learn.org/\n5\n\nTo train such a cost sensitive model selected by the above process, reducing the high demand on\ntraining data processing cost is also an active research problem. There are three directions in general:\nreducing the processing cost per visit of training sample; reducing the number of training samples;\nand reducing the number of visits per training sample (i.e. number of iterations); Feature hashing\nand embedding methods [ 5,13] could be used to reduce the cost per training sample. Few-shot\nlearning [ 11], meta-learning and transfer learning [ 38] are major solutions towards reducing the\nnumber of samples need to be trained. Adaptive and importance based sampling methods [ 2,12] lead\nto faster convergence and less visits per training sample. In some extreme cases, samples which are\nnot informative could be even removed from the training process without sacriﬁcing model accuracy.\nGenetic algorithm, reinforcement learning and model-based optimization [ 31] may help make some\nprogress. Researchers have been constructing more and more complex models in recent years.\nHowever, simple models usually have the advantages of good interpretation. Hence, new models\nwith simple structures and comparable performance, are also worth to investigate. Convergence\nacceleration that reduces the total number of training iterations is a difﬁcult problem. Approaches for\nsome special models have been proposed, e.g. increasing the batch-size[53]. It remains challenging\nfor (asynchronous) distributed training due to gradient staleness [ 51] caused by communication delay.\nTrading off between efﬁciency and effectiveness has never been easy. In practice, what to optimize\ndepends on the expectation of the users or the requirement of the applications. Notwithstanding, it is\nalso related to fairness or resource saving. Typically, the improvement at the ﬁnal stage of training\nis usually very minor, e.g. from accuracy 99% to 99.2%. When the hardware resource is shared by\nmultiple tenants as investigated by [ 28], the cluster administrator can stop such instances to release\nthe GPU to train other users’ models. When the model is running on cloud platforms e.g. Amazon\nEC2, the running time is directly related to the fees. User expectation, application requirement, cost\nand the fairness are metrics to consider for the stopping criteria.\n3.3 Auto-tuning Models based on Knowledge-bases\nBuilding domain speciﬁc knowledge base has been widely accepted as the foundation of conducting\ndomain speciﬁc analytics. However, there is no golden standard as to what kind of knowledge base\nshould be constructed and how they should be utilized to improve the analytic model. Currently, a\nknowledge base is mostly used in simple tasks such as manual analytics and visualization. There\nis no doubt that a domain speciﬁc knowledge base should be a valuable resource for all kinds of\napplications. However, it is still not clear how it can directly beneﬁt applications based on complex\nmodels such as deep learning. Intuitively, using domain speciﬁc knowledge base to improve a machine\nlearning model is a paradox: knowledge base records how entities/features are related (usually in\nqualitative manner), while machine learning models tend to learn those relations from the training\ndata (usually in quantitative manner). The main challenge of applying a knowledge base is how\nto balance the qualitative relations and quantitative relations. We believe that using the qualitative\nrelations from knowledge base as a prior distribution (i.e. regularization term) could be a simple,\ngeneral, feasible solution. Nowadays, typical regularization methods are mostly acting in quantitative\nmanner. For the healthcare system mentioned in Section 1, we designed a regularization term based\non healthcare domain knowledge. However, the domain knowledge used there is limited to ontology\nknowledge, and the regularization method designed is limited to a certain kind of classiﬁcation task.\nLogically, using knowledge for regularization requires research from two areas. One is to build a\nknowledge base that can clearly describe qualitative relations among features/samples. Another is to\ndesign regularization methods that can work in a qualitative manner.\n4 Model Deployment\nModels are trained ofﬂine and then deployed on cloud platforms, dedicated servers or edge devices\nfor online predictions. Most research focuses on model training. In fact, the deployment process\nis not any simpler than training. It involves much engineering work, e.g. fault tolerance and load\nbalance. These are also interesting research topics.\n6\n\n4.1 Reliability and Interpretability\nAI applications must go through a sequence of checks and validations before deployment. Once an\napplication is deployed, we still need to monitor the performance, scale the throughput according\nto the demands, keep the load balance and recover nodes from failures. A one-step deployment\nservice that combines and automates all these operations together is helpful. Besides automation,\nwe highlight the importance of reliability and interpretability of models for the usability of model\ndeployment.\nVertical domains like healthcare and ﬁnance have demanding requirements on the reliability of\ndeployed applications. A simple solution is to monitor the performance and switch the working\nmode to human mode when AI is uncertain about some requests. Most machine learning models are\nsoft margin based and have a self-evaluation for its accuracy (e.g. the Softmax outputs in logistics\nregression and most deep neural networks). However, this self-evaluation is only accurate when\nthere is no concept drifting and the data characteristic exactly matches with the model assumptions.\nFor example, for an application that is based on Naive Bayes model, when the input features appear\nare highly correlated, the real accuracy will drop signiﬁcantly while its self-evaluation will have\nalmost 100% conﬁdence about its prediction. Therefore, the self-evaluation may not be reliable.\nDesigning a robust model to monitor the system performance is thus necessary. For example, we\nmay continuously check the data distribution to see if the characteristic matches with the model\nassumptions. We can also collect feedback from users to evaluate the performance of the deployed\nmodel.\nIn addition to reliability, interpretation is also important. For example, doctors often ask the question—\n“how is the prediction generated?\". Explanations are essential for the democratization of AI on\ncritical applications. Most complex machine learning models work like a black-box. Even for their\ndesigners, it is difﬁcult to know the exact reason for every decision or prediction. Using black-box\nsystems to do critical decisions or predictions could bring users a sense of distrust, violate regulation\nrequirements and put the domain practitioners in a competitive relationship with AI solutions. All\nthese factors are harmful for the success of AI on these valuable applications. Explanations could\nsigniﬁcantly reduce outside resistance and hence ease the usage of AI systems. In healthcare\napplications, researchers working on computational phenotyping [ 18,45,56] are trying to ﬁnd out\nthe explainable risk factors from the models for healthcare problems. There is a trend of research on\nmodel interpretation [3, 23, 27, 43].\nWe aim to design a set of general mechanisms to make AI solutions more understandable for model\ndesigners, domain experts, regulators and end-users. For model designers, the explanation could help\nthem to reﬁne the model architecture and training process. For domain experts, the explanation could\nbring more insights and hence enhance the cooperative relationship. For regulators, the explanations\ncould help them solve legal issues and build accountability systems. For end-users, explanations\nincrease the quality of service and promote trust.\nWorking towards this direction, we use a neural network as a research prototype, evaluate the\nimportance and meaning for each neuron and analyze how they interact. Without loss of generality,\nthis evaluation framework can be extended to any machine learning models whose data transformation\nprocess can be described as a graph (e.g. PGM and topic modeling). We conduct the evaluation via\na novel concept called neuron saliency, which measures neuron efﬁciency in neural networks. By\nestimating neuron saliency, we are able to ﬁnd out whether the basic unit of neural networks, namely\nthe neuron, is contributing to the success of these models or other neurons. We ﬁrst unify the neural\nnetworks in neuron representation and introduce dropout optimization for neural networks. Then two\nmethods are proposed to estimate neuron saliency efﬁciently by dropout and gradient information\nrespectively. Based on the neuron saliency, algorithms for optimizing the training of neural networks\nare developed, and in the meantime, a novel algorithm for model compression by dropping low\nsaliency neurons is introduced.\n4.2 Cost-sensitive Deployment\nEfﬁciency or latency is more critical for the deployment stage than other stages as this stage is online.\nFor example, because of the high requirement on latency (less than 1ms) during database querying,\nthe learned index [ 24] has to replace the inference code from a Tensorﬂow implementation with\nhand-crafted but well optimized code, even for a simple neural network model. Optimization should\n7\n\nModel…Training data Test data at day 1 Test data at day 100\nModel\nModel\nModelModel\n…\nCloud…Hospital 1\nHospital 2\nHospital nFigure 6: Illustration of data distribution drift.\nbe conducted from every strata [ 3,50] including hardware, compiler, code, algorithm and models.\nGPU is excellent for training, but it costs extra time to transfer data from CPU to GPU. Hence,\nFPGA [ 15] has been applied as a replacement. To make it usable for non-FPGA programmers, we\nneed a library with optimized operations (e.g. convolution for deep learning models) on FPGA, and a\ntool to convert the model trained on GPU to work on FPGA. Compilers like XLA3and Weld[ 37] are\ndesigned to optimize AI and data analytic operations. Model compression that reduces the memory\nand computation cost for deploying models on small devices is a hot research topic [ 7]. The Tensor\nTrain line of work is an example of such kind of model reduction effort [ 36]. The challenge is how to\ncompress the model without sacriﬁcing the accuracy. A more challenging but preferred solution is to\ndesign a new and simpler model directly to replace current big models. For example, it is desirable to\nreplace CNNs with a new model with good interpretability, less computation and memory cost.\nIn terms of effectiveness, the most obvious issue is the change of data distribution as illustrated by\nFigure 6. When the data distribution is evolving, the model should adjust to keep its performance.\nContinuous learning in nowadays deep learning context could be very challenging. If a single model\nis used for the prediction, the only choice here is to design a transfer learning or online learning\nmodel that can leverage the online data to reﬁne itself. It is still not clear how a model training using\nstochastic gradient descent can be efﬁciently updated or trained with stream data with performance\nguarantee. An alternative solution that is commonly adopted in practice is ensemble modeling.\nInstead of using a single prediction model that may suffer from over-ﬁtting and change of data\ndistribution, ensemble modeling is a more robust solution since the ﬁnal prediction are based on\nthe output of several different models. However, simply averaging the results of multiple models\ncan only result in a static robust model (i.e. less sensitive to over-ﬁtting) but it still cannot adapt to\nthe change of data distribution such as concept drift. In many cases the best model that should be\ntrusted depends on the data distribution of the online incoming data. To get the best performance\nfrom multiple models, inference based on real-time feedback is strongly required. For both solutions,\nwe have to optimize the cost in terms of power consumption and storage as the target devices may be\nmobile phones or IoTs.\n4.3 Security\nNowadays, many applications are deployed on cloud platforms, e.g. Amazon EC2. Users submit their\nrequest to the cloud platforms for processing and then receive the prediction results. For such cases,\nwe need to protect both the request (or query) data and the model to avoid leaking training data. To\nprotect the request data, we have to encrypt it. Therefore, the models must accept encrypted data as\ninput and generate encrypted predictions. Similar to the approaches for training over encrypted data,\ninference [ 17,52] over encrypted data is mainly based on homomorphic encryption. Considering\nthat the efﬁciency problem is more critical for inference than training, approaches with fast inference\nspeed is necessary. To protect the model, we typically add noise to prevent users from inferring some\nproperties of the training data. For example, users can infer the membership of a certain data sample\nbased on the prediction accuracy and conﬁdence. In particular, if the model is over-ﬁtting on the\ntraining data and is very conﬁdent about a test data sample, it is likely that this sample is included in\nthe training dataset. However, adding noise into the prediction results would affect the accuracy of the\nmodel from the user’s perspective. A research direction is to train a model with good generalization\n3https://www.tensorﬂow.org/performance/xla/\n8\n\n+ \nVisualization  \n SINGA  \nData Storage  CPU -GPU Cluster  \nForkBase  Infrastructure  Data Analysis  \nPipeline  \niDat \nDICE  Raw Data  \nCohAna  CDAS  \nepiC Cohort Analysis  Machine/Deep Learning  Crowdsourcing  \nData Integration  Big Data  \nProcessing  Application  Finance  Healthcare  Security  Location -based Services  Figure 7: An end-to-end analytics system stack.\nability such that it performs equally well on both training and testing data. Then, we cannot infer the\nmembership of the test data. In fact, it is a shared research goal from the perspective of security and\nmachine learning training.\n5 PANDA Solution\nWe have been developing systems towards resolving the issues discussed in this paper. We shall now\ndiscuss the PANDA architecture that we believe can address the issues highlighted in this paper.\n5.1 Basic End-to-end Analytics Stack\nFigure 7 shows the current stack of our systems. Healthcare is one of our primary applications.\nWe are collaborating with multiple local hospitals, who give us the data and help to validate our\nresults. CDAS [ 32] is a crowdsourcing system used by doctors to add their knowledge into the\ndata, e.g. by labeling. DICE is a system for data integration that cleans raw EMR data based on\nexpert deﬁned rules. epiC [ 22] is our large scale batch processing engine. ForkBase4[46] is our data\nstorage engine designed with rich semantics and three key properties, namely immutability, sharing\nand security. After labeling, cleaning and pre-processing, the data is fed into training or analytics\nengines. Apache SINGA [ 47] is a deep learning platform initiated by us, which focuses on memory\nand speed efﬁciency optimization. On top of SINGA, we have a platform called Raﬁki [ 48], which\nprovides training and deployment services for analytics tasks. CohAna [ 21] is a cohort analysis\nengine designed for tasks like customer churn analysis. iDat is a visualization tool for presenting the\nanalytics results and users to explore the results. Other applications include ﬁnance data analytics\nand cyber-security.\n5.2 Speciﬁc Challenges as Plug-ins\nIn the PANDA system, we aim to ease the development of AI applications when facing the aforemen-\ntioned challenges. However, not all of them are shared by every application. Different applications\ncould beneﬁt from different workﬂows. Therefore, instead of building the solution for each challenge\nas a ﬁxed step in the pipeline, we propose to build them as optional plug-ins.\nEach plug-in consists of three basic components. We always build one simple default solution which\nis general and data insensitive. To resolve the speciﬁed challenges, a detector is developed to examine\nwhether the inputs follow certain data characteristics. If the answer is true, a speciﬁc solution, which\nis typically data-driven, will be applied to replace the default solution.\nWe use the knowledge-based regularization module in the pipeline as an illustrative example. There\nwill be a detector to examine whether the parameter set are correlated to a concept set. Once such\nconnection is veriﬁed, the regularization term will be constructed based on the relations of concepts\nin the knowledge-base. Meanwhile, the default solution is just a simple L2-norm, which will be\napplied to most parameter sets that are without any meaningful relation to existing concepts.\n4ForkBase is the second version of UStore [ 9], which has evolved substantially since the ﬁrst implementation\n9\n\n5.3 Key Modules\n5.3.1 Application Driven Data Exploration\nWe propose to build an automatic feature and sample exploration model. Given a budget and a pricing\nmodel for the dataset, the target of this model is to ﬁnd an iterative and explorative data acquisition\nstrategy to obtain the subset of data which has a price lower than the budget and leads to the best\napplication performance. This problem can be viewed as a generalized process of active learning,\nwhich only optimizes the application performance by selecting data samples. However, if the data\nacquisition model can decide the set of features they should query for data samples, there are more\nopportunities to further reduce the data acquisition cost than simply applying active learning. This\nprocess is also different from feature engineering which aims to ﬁlter out noisy features, since in\nour exploration model the ﬁlter is not only cost sensitive, but also is designed to ﬁlter out as many\nunnecessary features as possible.\nData quality management is also necessary, but acts as a fundamental module to support the data\nexploration process. Data exploration can be viewed as a set of cost-sensitive schemes for data\nacquisition, where the data quality is managed in a proactive manner. Typical data quality management\nconsists of evaluations of data quality (or system performance) and optimizations (e.g. data cleaning)\nfor data quality based on the given dataset. However, its achievement is typically limited if there\nexist fundamental imperfections in the given dataset. Instead of passively recovering noisy features\nor labels and inferring them based on uninformative samples, data exploration looks for indicative\nfeatures and learns only from representative and valuable samples.\n5.3.2 Data Driven Model Selection\nData quality affects the model performance directly. However, although the data is well pre-processed,\nthe model may still perform poorly. This is because, when applying a machine learning model to a\ndataset to solve an application, we are actually making a set of assumptions on the characteristics of\nthe dataset. For example, using Naive Bayes model means we are assuming that the features of a data\nsample should be independent when given the labels, and using logistics regression means we are\nassuming that the labels are generated based on a linear combination of all its features. In terms of\nregularization, using L2-norm regularization means we are assuming a Gaussian prior over the model\nparameters learned through the datasets. If there is a mismatch between the real characteristics of the\ndataset and the model assumptions, the performance suffers. This is the critical reason why we design\nor select different machine learning models for different applications. Based on the above intuition,\navoiding the mismatch between data characteristics and model assumption could be an efﬁcient way\nto automate model designing and tuning.\nFor each model or processing step, we propose to build a matching evaluation to test if the input data\nfollow the model assumption. If there is a mismatch, data transformations need to be applied. For\nexample, if the model assumes the data should be linearly separable (e.g. SVM), but the data are\nnot linearly separable (this evaluation can be easily obtained from the optimization result of SVM),\ntwo choices should be recommended instead of directly applying SVM: using feature engineering to\npre-process the data to make it linearly separable; or applying kernel SVM which does not assume\nthat the data are linearly separable. The examination of the matching between data characteristics and\nmodel assumption could signiﬁcantly ease the model designing and tuning process or even automate\nit, since making the exact match is exactly one of the principles of model designing.\n5.3.3 Reliable Answers for High Stake Applications\nFor most of the complex data analytics and decision making problems such as ﬁnancial investment [ 8],\nmedical treatment [ 30] and self-driving system [ 6], learning algorithms, while in progressive de-\nvelopment, are widely believed to be able to surpass human performance in the near future. Such\nemerging “high stakes” applications of AI pose exacting demands on the reliability of deployed\nsolutions. However, most of these applications rely on prevailing deep neural network models [ 26].\nWhile these models may provide high prediction accuracy in the general case, they may be vulnerable\nto unexpected egregious errors [ 10,34,35], particularly when being applied to data points that\nare not well-represented in the training set. In some cases, the deep learning models are no better\nthan random guesses on regions lacking of training points, and yet predict with high conﬁdence.\nFor high stakes applications, every decision matters and such irresponsible actions are deﬁnitely\n10\n\nprohibited. Unfortunately, most deep learning models act like a black-box without much explanation,\nand are hard to understand even for domain experts [ 54]. It is not practical to prevent such failures\nby manually examining the logic inside a deep learning solution. Consequently, developing a deep\nlearning solution with reliable behaviour has attracted a great deal of interest.\nInstead of answering all the problems, we propose that a deep learning model should only be\nresponsible to answer problems which it has been trained to answer, i.e. problems that lie in the\nreliable region. The reliable region is deﬁned as a data distribution generalized from the training\nset where the deep learning model can achieve as good performance as when tested on the training\nset. Reliable deep learning solutions are useful for many high stakes applications. For example,\na model for CT image classiﬁcation with 90% accuracy cannot be applied onto any other clinical\nsystem, where the typical minimum requirement is 95%. As an alternative, by applying a model that\nprovides 99% accuracy inside a reliable region which covers half of the patient images, the workload\nof radiologists could be effectively reduced by half. Developing an all-weather strategy for ﬁnancial\ninvestment to signiﬁcantly outperform the market is usually not practical. However, if we can build a\nmodel which signiﬁcantly outperforms the market within a small reliable region, safe arbitrage can\nbe done when the opportunity arises (i.e. market state falls within a reliable region).\n6 Conclusions\nThere is no doubt that AI technologies will have great success in many vertical domains in the next\nfew years. However, the mass production of AI poses many challenges for the current data analytics\npipeline and other support system infrastructure, especially for critical decision making in a domain\nspeciﬁc problem. In this paper, we review some challenges with respect to the issue of usability,\nefﬁciency and effectiveness and security in data preparation, training and product delivery phases of\nan AI application. Compared to the great success achieved in recent benchmark problem (e.g. CV\nand NLP) modeling, these challenges are not well addressed by current AI research but play a vital\nrole in practical domain speciﬁc AI solution development. We summarize several research directions\nand discuss some preliminary methods. We are developing an AI platform called PANDA to resolve\nthe aforementioned issues and support fast development of domain speciﬁc applications. We hope to\nmake AI more usable, explainable, and scalable.\nReferences\n[1]M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,\nM. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker,\nV . Vasudevan, P. Warden, M. Wicke, Y . Yu, and X. Zheng. Tensorﬂow: A system for large-scale\nmachine learning. In OSDI , 2016.\n[2]G. Alain, A. Lamb, C. Sankar, A. Courville, and Y . Bengio. Variance reduction in sgd by\ndistributed importance sampling. arXiv preprint arXiv:1511.06481 , 2015.\n[3]P. Bailis, K. Olukoton, C. Ré, and M. Zaharia. Infrastructure for usable machine learning: The\nstanford dawn project. arXiv preprint arXiv:1705.07538 , 2017.\n[4] J. Bergstra and Y . Bengio. Random search for hyper-parameter optimization. JMLR , 2012.\n[5]X. Cai, J. Gao, K. Y . Ngiam, B. C. Ooi, Y . Zhang, and X. Yuan. Medical concept embedding\nwith time-aware attention. In IJCAI , 2018.\n[6]C. Chen, A. Seff, A. Kornhauser, and J. Xiao. Deepdriving: Learning affordance for direct\nperception in autonomous driving. In ICCV , 2015.\n[7]Y . Cheng, D. Wang, P. Zhou, and T. Zhang. A survey of model compression and acceleration\nfor deep neural networks. arXiv preprint arXiv:1710.09282 , 2017.\n[8]X. Ding, Y . Zhang, T. Liu, and J. Duan. Deep learning for event-driven stock prediction. In\nIJCAI , 2015.\n[9]A. Dinh, J. Wang, S. Wang, G. Chen, W.-N. Chin, Q. Lin, B. C. Ooi, P. Ruan, K.-L. Tan, Z. Xie,\net al. Ustore: a distributed storage with rich semantics. arXiv preprint arXiv:1702.02799 , 2017.\n[10] A. Fawzi, D. Moosavi, M. Seyed, and P. Frossard. Robustness of classiﬁers: from adversarial to\nrandom noise. In NIPS , 2016.\n11\n\n[11] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. TPAMI , 2006.\n[12] J. Gao, H. Jagadish, and B. C. Ooi. Active sampler: Light-weight accelerator for complex data\nanalytics at scale. arXiv preprint arXiv:1512.03880 , 2015.\n[13] J. Gao, W.-c. Lee, Y . Shen, and B. C. Ooi. Cuckoo feature hashing: Dynamic weight sharing\nfor sparse analytics. In IJCAI , 2018.\n[14] S. Hadjis, C. Zhang, I. Mitliagkas, D. Iter, and C. Ré. Omnivore: An optimizer for multi-device\ndeep learning on cpus and gpus. arXiv preprint arXiv:1606.04487 , 2016.\n[15] S. Han, J. Kang, H. Mao, Y . Hu, X. Li, Y . Li, D. Xie, H. Luo, S. Yao, Y . Wang, et al. Ese: efﬁcient\nspeech recognition engine with compressed lstm on fpga. arXiv preprint arXiv:1612.00694 ,\n2016.\n[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR ,\n2016.\n[17] E. Hesamifard, H. Takabi, and M. Ghasemi. Cryptodl: Deep neural networks over encrypted\ndata. arXiv preprint arXiv:1711.05189 , 2017.\n[18] J. C. Ho, J. Ghosh, and J. Sun. Marble: high-throughput phenotyping from electronic health\nrecords via sparse nonnegative tensor factorization. In SIGKDD , 2014.\n[19] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput. , 1997.\n[20] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer. Squeezenet:\nAlexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint\narXiv:1602.07360 , 2016.\n[21] D. Jiang, Q. Cai, G. Chen, H. Jagadish, B. C. Ooi, K.-L. Tan, and A. K. Tung. Cohort query\nprocessing. PVLDB , 2016.\n[22] D. Jiang, G. Chen, B. C. Ooi, K.-L. Tan, and S. Wu. epic: an extensible and scalable system for\nprocessing big data. PVLDB , 2014.\n[23] P. W. Koh and P. Liang. Understanding black-box predictions via inﬂuence functions. arXiv\npreprint arXiv:1703.04730 , 2017.\n[24] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures.\narXiv preprint arXiv:1712.01208 , 2017.\n[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In NIPS , 2012.\n[26] Y . LeCun, Y . Bengio, and G. Hinton. Deep learning. Nature , 2015.\n[27] G. Li, M. Zhang, G. Chen, and B. C. Ooi. Towards a uniﬁed graph model for supporting data\nmanagement and usable machine learning. IEEE Data Eng. Bull. , 2017.\n[28] T. Li, J. Zhong, J. Liu, W. Wu, and C. Zhang. Ease. ml: towards multi-tenant resource sharing\nfor machine learning workloads. PVLDB , 2018.\n[29] Z. Li, C. Wang, M. Han, Y . Xue, W. Wei, L.-J. Li, and F.-F. Li. Thoracic disease identiﬁcation\nand localization with limited supervision. arXiv preprint arXiv:1711.06373 , 2017.\n[30] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian, J. A. van der\nLaak, B. van Ginneken, and C. I. Sánchez. A survey on deep learning in medical image analysis.\nMedical image analysis , 2017.\n[31] C. Liu, B. Zoph, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang, and K. Murphy.\nProgressive neural architecture search. arXiv preprint arXiv:1712.00559 , 2017.\n[32] X. Liu, M. Lu, B. C. Ooi, Y . Shen, S. Wu, and M. Zhang. Cdas: a crowdsourcing data analytics\nsystem. PVLDB , 2012.\n[33] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation.\nInCVPR , 2015.\n[34] D. Moosavi, M. Seyed, A. Fawzi, and P. Frossard. Deepfool: a simple and accurate method to\nfool deep neural networks. In CVPR , 2016.\n[35] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High conﬁdence\npredictions for unrecognizable images. In CVPR , 2015.\n12\n\n[36] A. Novikov, D. Podoprikhin, A. Osokin, and D. Vetrov. Tensorizing Neural Networks. In NIPS ,\n2015.\n[37] S. Palkar, J. Thomas, D. Narayanan, A. Shanbhag, R. Palamuttam, H. Pirk, M. Schwarzkopf,\nS. Amarasinghe, S. Madden, and M. Zaharia. Weld: Rethinking the interface between data-\nintensive applications. arXiv preprint arXiv:1709.06416 , 2017.\n[38] S. J. Pan and Q. Yang. A survey on transfer learning. TKDE , pages 1345–1359, 2010.\n[39] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\nA. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV , 2015.\n[40] S. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing between capsules. In NIPS , 2017.\n[41] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\n[42] J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning\nalgorithms. In NIPS , 2012.\n[43] I. Stoica, D. Song, R. A. Popa, D. Patterson, M. W. Mahoney, R. Katz, A. D. Joseph, M. Jordan,\nJ. M. Hellerstein, J. E. Gonzalez, et al. A berkeley view of systems challenges for ai. arXiv\npreprint arXiv:1712.05855 , 2017.\n[44] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. A. Alemi. Inception-v4, inception-resnet and the\nimpact of residual connections on learning. In AAAI , 2017.\n[45] T. Tran, T. D. Nguyen, D. Phung, and S. Venkatesh. Learning vector representation of med-\nical objects via emr-driven nonnegative restricted boltzmann machines (enrbm). Biomedical\nInformatics , 2015.\n[46] S. Wang, T. T. A. Dinh, Q. Lin, Z. Xie, M. Zhang, Q. Cai, G. Chen, W. Fu, B. C. Ooi, and\nP. Ruan. Forkbase: An efﬁcient storage engine for blockchain and forkable applications. arXiv\npreprint arXiv:1802.04949 , 2018.\n[47] W. Wang, G. Chen, T. T. A. Dinh, J. Gao, B. C. Ooi, K. Tan, and S. Wang. SINGA: putting\ndeep learning in the hands of multimedia users. In ACM MM , 2015.\n[48] W. Wang, S. Wang, J. Gao, M. Zhang, G. Chen, T. K. Ng, and B. C. Ooi. Raﬁki: Machine\nlearning as an analytics service system. arXiv preprint arXiv:1804.06087 , 2018.\n[49] W. Wang, N. Yang, F. Wei, B. Chang, and M. Zhou. Gated self-matching networks for reading\ncomprehension and question answering. In ACL, 2017.\n[50] W. Wang, M. Zhang, G. Chen, H. V . Jagadish, B. C. Ooi, and K.-L. Tan. Database meets deep\nlearning: Challenges and opportunities. SIGMOD Rec. , 2016.\n[51] J. Wei, W. Dai, A. Kumar, X. Zheng, Q. Ho, and E. P. Xing. Consistent bounded-asynchronous\nparameter servers for distributed ml. arXiv preprint arXiv:1312.7869 , 2013.\n[52] P. Xie, M. Bilenko, T. Finley, R. Gilad-Bachrach, K. Lauter, and M. Naehrig. Crypto-nets:\nNeural networks over encrypted data. arXiv preprint arXiv:1412.6181 , 2014.\n[53] Y . You, I. Gitman, and B. Ginsburg. Scaling sgd batch size to 32k for imagenet training. arXiv\npreprint arXiv:1708.03888 , 2017.\n[54] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In ECCV ,\n2014.\n[55] K. Zheng, W. Wang, J. Gao, K. Y . Ngiam, B. C. Ooi, and J. W. L. Yip. Capturing feature-level\nirregularity in disease progression modeling. In CIKM , 2017.\n[56] J. Zhou, F. Wang, J. Hu, and J. Ye. From micro to macro: data driven phenotyping by\ndensiﬁcation of longitudinal electronic medical records. In SIGKDD , 2014.\n[57] Z.-H. Zhou and J. Feng. Deep forest: Towards an alternative to deep neural networks. arXiv\npreprint arXiv:1702.08835 , 2017.\n13",
  "textLength": 49522
}