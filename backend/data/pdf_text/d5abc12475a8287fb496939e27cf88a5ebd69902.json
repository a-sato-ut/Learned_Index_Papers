{
  "paperId": "d5abc12475a8287fb496939e27cf88a5ebd69902",
  "title": "EXMA: A Genomics Accelerator for Exact-Matching",
  "pdfPath": "d5abc12475a8287fb496939e27cf88a5ebd69902.pdf",
  "text": "arXiv:2101.05314v1  [cs.AR]  13 Jan 2021EXMA: A Genomics Accelerator for Exact-Matching\nLei Jiang\nIndiana University Bloomington\njiang60@iu.eduFarzaneh Zokaee\nIndiana University Bloomington\nfzokaee@iu.edu\nAbstract —Genomics is the foundation of precision medicine,\nglobal food security and virus surveillance. Exact-match i s one of\nthe most essential operations widely used in almost every st ep of\ngenomics such as alignment, assembly, annotation, and comp res-\nsion. Modern genomics adopts Ferragina-Manzini Index (FM-\nIndex) augmenting space-efﬁcient Burrows-Wheeler transf orm\n(BWT) with additional data structures to permit ultra-fast exact-\nmatch operations. However, FM-Index is notorious for its po or\nspatial locality and random memory access pattern. Prior wo rks\ncreate GPU-, FPGA-, ASIC- and even process-in-memory (PIM) -\nbased accelerators to boost FM-Index search throughput. Th ough\nthey achieve the state-of-the-art FM-Index search through put,\nthe same as all prior conventional accelerators, FM-Index P IMs\nprocess only one DNA symbol after each DRAM row activation,\nthereby suffering from poor memory bandwidth utilization.\nIn this paper, we propose a hardware accelerator, EXMA, to\nenhance FM-Index search throughput. We ﬁrst create a novel\nEXMA table with a multi-task-learning (MTL)-based index to\nprocess multiple DNA symbols with each DRAM row activation.\nWe then build an accelerator to search over an EXMA table.\nWe propose 2-stage scheduling to increase the cache hit rate of\nour accelerator. We introduce dynamic page policy to improv e\nthe row buffer hit rate of DRAM main memory. We also\npresent CHAIN compression to reduce the data structure size\nof EXMA tables. Compared to state-of-the-art FM-Index PIMs ,\nEXMA improves search throughput by 4.9×, and enhances\nsearch throughput per Watt by 4.8×.\nIndex Terms —Domain-Speciﬁc Hardware Accelerator, Ge-\nnomics, Exact-Matching\nI. I NTRODUCTION\nBecause of the huge advancement of sequencing technolo-\ngies such as Illumina [1], PacBio SMRT [2], and Oxford\nNanopore [3], sequencing a entire human genome requires\nonly<1day. The big genomic data has been a cornerstone\nto enabling personalized healthcare [4], and ensuring glob al\nfood security [5]. Recently, genome sequencing becomes a\npowerful tool to ﬁght virus outbreaks, e.g., Ebola [6], Zika [7]\nand COVID-19 [8].\nHowever, it is challenging to process and analyze huge\nvolumes of genomic data generated by high throughput se-\nquencers that scale faster than Moore’s Law [9]. For instanc e,\nthousands of USB-drive-size Oxford Nanopore Minion se-\nquencers are deployed to monitor virus outbreaks [6]–[8] in the\nwild by generating several terabytes data per day. Analyzin g\na single genome may take hundreds of CPU hours [10],\n[11] even on high-end servers. To overcome the looming\ncrisis of big genomic data, the application-speciﬁc hardwa re\nacceleration has become essential for genomics.\nThis work was partially supported by the National Science Fo undation\n(NSF) through awards CCF-1908992 and CCF-1909509.al i gnmentassembl yal i gnmentassembl yal i gnmentassembl yannotatecompress0%20%40%60%80%100%PacBi o Nanoporeexecuti on\nti me breakdown  FM-I ndex  DynPro  OtherI l l umi na\nFig. 1. Execution time breakdown of human genome analysis ( DynPro\nmeans dynamic programming).\nA genome sequencing pipeline [4] sequences organic\ngenomes, archives genomic data, analyzes genome sequences ,\nand generates genetic variants that can used for patient tre at-\nment. Therefore, the latency of genome sequencing is a matter\nof life and death. Read alignment [12], which aligns reads,\ni.e., small DNA fragments, against a long genome reference,\nis identiﬁed as one of the most time-consuming steps [10],\n[11], [13]–[15] in genome analysis. Read alignment adopts\ntheseed-and-extend paradigm [12], [16], and thus includes\ntwo major stages, i.e., seeding and seed extension . During\nseeding, parts of each read are mapped to their exactly match ed\npositions, i.e., seeds, of the long reference by hash tables [10],\n[11], [17] or Ferragina-Manzini Index (FM-Index) [12], [16 ].\nSeed extension pieces together a larger sequence with seeds\nand edit distance errors, i.e., insertions, deletions (ind els) and\nsubstitutions, by dynamic programming [18]–[20].\nState-of-the-art read alignment applications such as BWA-\nMEM [12], MA [16] and SOAP [21] use FM-Index to build\nsuper-maximal exact matches (SMEMs) during seeding, since\nit augments the space-efﬁcient Burrows-Wheeler transform\n(BWT) [22] with accessory data structures that permit ultra -\nfast exact-match operations. SMEMs generated by FM-Index\nguarantee each seed does not overlap other seeds and has the\nmaximal length that cannot be further extended. Compared to\nhash tables, FM-Index reduces not only the number of errors\nin output genome mappings but also the durations of seed\nextension substantially [23].\nBesides read alignment, FM-Index is widely used for exact-\nmatch operations in other time-consuming steps of genome\nanalysis such as genome assembly [24], annotation [25]\nand compression [26]. Figure 1 shows the execution time\nbreakdown of various genome analysis applications on a\nhuman genome1. On average, FM-Index searches cost 31%\n∼81%of the execution time of these genome analysis\napplications . Since Illumina, Nanopore and PacBio genome\nsequencers generate reads having different lengths and err or\nrates, aligning and assembling these reads require differe nt\n1The experimental methodology is elaborated in §V.\n\namounts of time for FM-Index searches. The reads produced\nby Illumina machines have lower error rates, so an Illumina\ndataset invokes FM-Index searches more frequently.\nHowever, FM-Index is notorious for its poor spatial localit y\nand random memory access pattern [27]. The kernel of con-\nventional FM-Index search is pointer chasing. After activa ting\none DRAM row, FM-Index processes only one DNA symbol,\nthereby greatly decreasing DRAM bandwidth utilization. Al -\nthough a recent algorithmic work, LISA [28], uses a learned\nindex [29] to search multiple DNA symbols after each row\nactivation, the learned index accuracy is low. LISA has to\nsearch many unnecessary entries, and thus achieves only mod -\nerate search throughput improvement. Beyond CPUs [12] and\nGPUs [21], prior work creates FPGA [13], [30]-, ASIC [31]-\n, and even processing-in-memory (PIM) [14], [15]-based de-\nsigns to accelerate conventional FM-Index searches proces sing\nonly one DNA symbol after each row activation. Therefore,\nthese accelerators are fundamentally limited by the poor\noff-chip memory bandwidth utilization. Instead of searchi ng\nmultiple DNA symbols after each row activation, a recent\nDRAM PIM, MEDAL [15], achieves the state-of-the-art search\nthroughput by enabling DRAM chip-level parallelism, where\neach chip can independently activate a partial row to proces s\na DNA symbol. However, we observe that there are a lot\nof conﬂicts on the DDR4 address bus shared by all chips\nin a rank. The shared address bus seriously limits search\nthroughput of MEDAL.\nIn this paper, we propose an algorithm and hardware co-\ndesigned accelerator, EXMA , to process EXact-MAtch oper-\nations during genome analysis. Our contributions are summa -\nrized as follows:\n•An EXMA table with a MTL-based index – We propose\na novel data structure, EXMA table, that can process k\nDNA symbols, i.e., a k-mer, in a DRAM row in each\nFM-Index search iteration. We further present a multi-task -\nlearning (MTL)-based index to accelerate searches over an\nEXMA table. The MTL-based index trained with multiple k-\nmers uses less neural network parameters, but obtains highe r\naccuracy over learning to search each k-mer independently.\n•A hardware accelerator – We build an accelerator to search\nan EXMA table with a MTL-based index. We present a 2-\nstage scheduling to increase the hit rate of on-chip caches o f\nour accelerator for the table and its index. We also propose\ndynamic page policy to improve the row buffer hit rate\nof DRAM main memory. At last, we introduce CHAIN\ncompression to greatly reduce the data structure size of an\nEXMA table.\n•Search throughput and throughput per Watt – We eval-\nuated and compare EXMA to prior CPU-, GPU-, FPGA-,\nASIC-, and PIM-based FM-Index accelerators. Compared to\nthe state-of-the-art DRAM PIM MEDAL, EXMA improves\nsearch throughput by 4.9×, and enhances search throughput\nper Watt by 4.8×.reference CT GG AA GG A\nCT TG\nGC AG\nGG AAGA GG\nGG AG\nGG AAreads sequencing error genetic variation \n(a) Read alignmentreference CT GG\nseed CT G\nC TG query 1query 0\nnot seed\n(b) seed-&-extend\nFig. 2. Read alignment\nII. B ACKGROUND\nA. Read Alignment\nSeed-and-Extend : As one of the bottlenecks in genome\nanalysis, read alignment may consume hundreds of CPU\nhours [10], [11], [13]. During read alignment, DNA reads\ngenerated by various sequencing machines, e.g., Illumina,\nPacBio SMRT, and Oxford Nanopore, are mapped to a pre-\nexisting genome reference, as shown in Figure 2(a). Read\nalignment is complicated by the fact that there are genetic\nvariations in the human population, and sequencing machine s\nalso introduce sequencing errors [32]. The overall variati on\nof human population has been estimated as 0.1%[9], while\nthe sequencing error rate of various sequencing machines is\n0.2%∼30% [32]. To reduce sequencing errors, a sequencing\nmachine produces 30∼50reads to cover every position in\nthe genome. As Figure 2(b) shows, read alignment adopts\nthe seed-and-extend paradigm [12], [16], [24], [33]–[35] t o\naccommodate sequencing errors and genetic variations. Dur ing\nseeding, a read is divided into multiple smaller parts that a re\naligned against the reference. If a part is exactly matched,\nit becomes a seed. The computationally expensive Smith-\nWaterman algorithm [10], [11], [13] is invoked only around\nseeds to handle sequencing errors and genetic variations.\nExact-Match Operation : The alphabet/summationtextof DNA includes\nA,C,GandT. Given a genome reference G ∈/summationtext∗of length\n|G|and a query Q∈/summationtext∗of length |Q|, the seeding, aka\nexact-match problem, is to ﬁnd all occurrences of QinG.\nA na¨ ıve algorithm of exhausting all possible positions for Q\nwill take O(|G||Q|)comparisons, which is infeasible for large\ngenome. It is possible to use a hash table [10], [11], [17]\nto support exact-match operations with hundreds of gigabyt es\nDRAM, but the hash-table-based seeding not only degrades\ngenome mapping quality but also prolongs seed extension du-\nrations [23]. State-of-the-art alignment algorithms [12] , [16],\n[21] use FM-Index for seeding. To search a query Qover\nthe reference genome G, FM-Index occupies O(|G|log(|G|))\nDRAM space and does O(|Q|)comparisons during a search.\nB. FM-Index\n1) Burrows-Wheeler Transform\nFM-index is built upon BWT [22]. To compute the BWT\nof a genome reference G, we can list all its circularly shifted\nsequences. For instance, Figure 3(a) shows G=CATAGA $,\nwhere$indicates the end of the sequence and it is the lexico-\ngraphically smallest symbol. Circularly shifted sequence s of\nG=CATAGA $can be listed as $CATAGA ,A$CATAG ,\n..., andCATAGA $, which can be sorted in the lexico-\ngraphical order to form a Burrows-Wheeler (BW)-matrix.\nThe last column of the BW-matrix is the BWT of G, i.e.,\n\n0   6   $  C  A  T  A  G  A\n1   5   A  $  C  A  T  A  G\n2   3   A  G  A  $  C  A  T\n3   1   A  T  A  G  A  $  C\n4   0   C  A  T  A  G  A  $ \n5   4   G  A  $  C  A  T  A\n6   2   T  A  G  A  $  C  AiSA BW matrix \n(a) BWTBWT=AGTC$AA 0  0  0  0  0 1  1  0  0  0 \n2  1  0  1  0 \n3  1  0  1  1 \n4  1  1  1  1 \n6  2  1  1  1 \n7  3  1  1  1 5  1  1  1  1 i A C G T\n1  4  5  6 A C G T(b) Occ(s,i)  table \n(c) Count(s)// BWT: the genome reference BWT; \n//Q : the query; \n// |Q|: the query length; \n0: low= 0;                high= MAX_OCC; \n1:  for (i=|Q|-1; i ≥ 0; i--) { \n2:  low  = Count(Q[i])+Occ(Q[i], low); \n3:  high = Count(Q[i])+Occ(Q[i], high); \n4:   if (low ≥ high) return ;\n5: } \n6:  for ( i=low; i < high; i++) { \n7:  final=SA[i];   //  reference positions \n8: } \n(d) Backward searchquery: TAG \nref: CATAGA \n(e) Example(0,7)→(5,6) Iter 0 for G \nIter 1 for A \n(5,6)→(2,3) \nIter 2 for T \n(2,3)→(6,7) \n→ SA[6]=2 \n(f) FM-IndexACGT\n1456Bucket-0\nBWT: AGTC \nMarker \nACGT\n2567Bucket-1\nBWT: $AA-\nMarker \nFig. 3. FM-Index overview: (a) the BWT of a genome reference G=CATAGA $; (b) anOcc(s,i)table; (c) a Count(s)table; (d) backward search; (e)\na search example; and (f) a bucket-based data structure\nBWT(G) =AGTC$AA. The sub-sequence ending with $\nin each row of the BW-matrix is a sufﬁx of G, which can\nbe denoted by an integer (SA in Figure 3(a)) recording its\nstarting position in the reference. For example, ATAGA $is\nSA[3] = 1 , which means it starts from the position 1 of G.\n2) FM-index\nThe data structure and search algorithm of FM-Index can\nbe summarized as:\n•Occ and Count . FM-index searches are implemented with\ntwo functions Occ(s,i)andCount(s)over the BWT of\nG. As Figure 3(b) exhibits, Occ(s,i)returns the number of\nsymbolsin the BWT from the position 0to the position\ni−1, e.g.,Occ(C,5) = 1 , which means that there is only\n1Cfrom the position 0 to the position 4 of BWT(G) =\nAGTC$AA.Count(s)shown in Figure 3(c) computes the\nnumber of symbols in the BWT that are lexicographically\nsmaller than the symbol s, e.g.,Count(T) = 6 , which indi-\ncates that there are 6 symbols in BWT(G) =AGTC$AA\nlexicographically smaller than T.\n•Backward Search . An exact-match operation is imple-\nmented by backward search, whose algorithm can be viewed\nin Figure 3(d). The interval (low,high )covers a range\nof indices in the BW-matrix where the sufﬁxes have the\nsame preﬁx. The pointer low locates the index in the BW-\nmatrix where the pattern is ﬁrst found as a preﬁx, while\nthe pointer high provides the index after the one where the\npattern is last found. At ﬁrst, low andhigh are initialized\nto the minimum and maximum indexes of the Occ table\nrespectively. And then, they iterate from the lastsymbol in\na queryQto the ﬁrst. The pointer posis updated by\nCount(Q[i])+Occ(Q[i],pos) (1)\nwhereQ[i]indicates the ithsymbol in the query Q. The\npointerposcan below orhigh , as shown from the lines 2\nto 3 in Figure 3(d). The computations of low andhigh\narepointer chasing and thus suffer from poor spatial\nlocality [14], [15], [30]. Finally, the interval (low,high )\ngives the range of indexes in the BW-matrix where the\nsufﬁxes have the target query as a preﬁx. These indexes\nare converted to reference genome positions using SA.\nFigure 3(e) illustrates an example of searching a query TAG\nin the reference G=CATAGA $. Before a search happens,(low,high )is initialized to (0,7). In the iteration 0, the last\nsymbolGis processed, and then (low,high )is updated to\n(5,6). After three iterations, (low,high )eventually equals\n(6,7). By looking up SA[6] = 2 in Figure 3(a), we ﬁnd that\nthe query TAG in reference G=CATAGA $starts from\nthe position 2.\n•Bucket-based Storage . BothCount(s)andOcc(s,i)can\nbe pre-calculated and stored. However, the storage overhea d\nofOcc(s,i)is proportional to the genome reference length\n|G|, and thus signiﬁcant. To keep the storage overhead in\ncheck, the Occ(s,i)values are sampled into buckets of\nwidthdshown in Figure 3(f) ( d= 4). TheOcc(s,i)values\nare stored each dpositions as markers to reduce the storage\noverhead by a factor of d. The omitted Occ(s,i)values\ncan be reconstructed by summing the previous marker and\nthe number of symbol sfrom the remaining positions in\nthe BWT bucket. To simplify searches, Count(s)values of\neach symbol are added to corresponding markers. Markers\nand BWT buckets are interleaved to build a FM-Index.\n3) Multi-step FM-Index\ni AA AC TT TG ... \n0 \n1\n2\n3\nf0f1 f15 f14...4\n5\n60 0 0 0... \n0 1 0 0... \n1 1 0 0... \n2 1 0 0... \n2 1 0 1... \n2 1 1 1... \n3 1 1 1... \n... ... ... ... ... \n|G|\nFig. 4. A 2-step table.During an iteration of the FM-Index\nbackward search, two memory accesses\nforlow andhigh are issued for each\nsymbol in a query Q. Totally, 2|Q|mem-\nory accesses are required for an exact\nmatch operation of a query. The FM-\nIndex backward search performance is\nseriously limited by random memory ac-\ncesses [14], [15], [30], since each access\nopens a DRAM row but fetches only 64B.k-step FM-\nIndex [36] is proposed to reduce the number of memory\naccesses to2|Q|\nkby updating a k-mer, i.e., kDNA symbols,\nin each search iteration. The idea of k-step FM-Index is to\nenlarge the alphabet size from ΣtoΣk. For instance, if\nk= 2, instead of single DNA symbols, as Figure 4 shows,\nthe enlarged alphabet includes 16 2-mers: AA,AC,...,TT.\nWe can construct a BWT with the enlarged alphabet and its\ncorresponding FM-Index to perform k-step backward search\nin the same way. The trade-off for k-step FM-Index is the\nincrease in its size, which is calculated as\nF=⌈log2(|G|)⌉·|G|·|Σ|k\n8d+|G|·⌈log2(|Σ|k+1)⌉\n8(2)\n\n[G$,0] \nm0\nm1m2\n... m3m4 mineural networks \n(a) IP-BWT (b) ExampleIter 1 for TA \n[TA,1]→6 \n[TA,7]→6 \n(c) Learned Index[CA, 6]  [GA, 0] [TA,5] ... i\n6  T  A  5 0  $  C  3 \n1  A  $  4  \n2  A  G  1 \n3  A  T  2 \n4  C  A  6 \n5  G  A  0 ❷ \n❸ ❹[GT,6]→1 ❺ [G$,0]→Iter 0 for G \n❶query: TAG  → TA, G \n6+1=7 \n→ SA[6]=2 IP-BWT Nk-mer \nFig. 5. LISA: (a) an IP-BWT array; (b) a search example; (c) a l earned index.\nwherekis the number of DNA symbols updated in each search\niteration. The size of multi-step FM-Index exponentially i n-\ncreases with an enlarging k.\n4) Learned Indexes for Sequence Analysis\nTo support multi-step searches with smaller DRAM over-\nhead, a recent work proposes Learned Indexes for Sequence\nAnalysis (LISA) [28] consisting of an Index-Paired BWT (IP-\nBWT) array and a learned index.\n•IP-BWT . In Figure 5(a), each entry of the IP-BWT is a\npair of [k-mer,N], where k-mer is the ﬁrst ksymbols of\nthe corresponding BW-matrix row, and Nis the row number\nof the sequence with the ﬁrst kand the last |G|−ksymbols\nswapped in the BW-matrix. For example, if k= 2, thek-\nmer of the row 0 of the IP-BWT can be derived from the\nrow 0 of the BW-matrix, $CATAGA , shown in Figure 3(a)\nusing only the ﬁrst 2 symbols $C. By sweeping the ﬁrst 2\nsymbols and the other 5 symbols of $CATAGA , we can\nhaveATAGA $C, which is the row 3 of the BW-matrix. So\nthe row 0 of the IP-BWT is [ $C, 3].\n•Backward Search . The backward search of LISA ﬁnds the\nlower bound position of a [ k-mer,N] pair in the IP-BWT.\nSince the IP-BWT is sorted, LISA adopts binary search\nfor backward searches. As Figure 5(b) shows, to search the\nqueryTAG , we ﬁrst break it into TA andG, since each\niteration can process a 2-mer. In the ﬁrst iteration, we star t\nwithG. The padding algorithm [28] of LISA converts Gto\nG$forlow andGTforhigh .low andhigh are initialized\nto 0 and 6 respectively. ❶To search [ G$, 0], a binary search\nis performed over the IP-BWT. ❷During the binary search,\nG$is ﬁrst compared against AT, i.e., the row 3 of the IP-\nBWT. ❸BecauseG$> AT , the binary search goes to the\nrow 5, i.e., GA of the IP-BWT. ❹Finally, it ends with the\nrow 4 of the IP-BWT, i.e., CA.❺SinceG$> CA , the\nnewlow is calculated as 6+1 = 7 .high can be computed\nin the same way. Each search iteration requires log2(|G|)\ncomparisons due to binary search.\n•Learned Index . To reduce the number of comparisons\nduring binary searches, LISA adopts a learned index [29],\ni.e., a model hierarchy consisting of multiple neural netwo rk\nmodels, as shown in Figure 5(c), where miis the neural\nnetwork model i. The learned index enables LISA to do\nonly one comparison during each iteration in the best case.\nTo search [G$,0]by the learned index, we can traverse down\nlower-level neural network models based on the output ofthe higher-level neural network models. Finally, a leaf neu ral\nnetwork model predicts the position of [G$,0]in the IP-\nBWT. However, if the predicted position does not contain\n[G$,0], a linear search over the IP-BWT starts from the\npredicted position to ﬁnd its actual position.\nIII. R ELATED WORK AND DESIGN MOTIVATION\nIt is challenging to achieve high-throughput and power-\nefﬁcient FM-Index searches by state-of-the-art FM-Index a l-\ngorithms and accelerators.\nA. Algorithm Inefﬁciency\nIntractable Size of k-step FM-Index . We recorded the row\nIDs of 200 consecutive 1-step FM-Index search iterations in\nFigure 6(a), where 197 different rows are accessed. Because 1-\nstep FM-Index processes only 1 DNA symbol in each iteration,\nin most cases, searching a DNA symbol by 1-step FM-Index\nrequires one row activation. Prior accelerators [14], [15] for\n1-step FM-Index expect no row buffer hit and thus adopt\nclose-page policy. Though k-step FM-Index can search kDNA\nsymbols by activating a row, as Figure 6(b) shows, its data\nstructure size exponentially increases with the step numbe r\nk. For instance, 5-step FM-Index costs 105GB, while 6-step\nFM-Index occupies 374GB. As a result, 5-step FM-Index\n(FM-5 ) improves search throughput by only 1.21×over 1-\nstep FM-Index, as shown in Figure 6(d). Further enlarging th e\nstep number of k-step FM-Index ( FM-6 ) decreases its search\nthroughput improvement, due to more TLB misses.\nWeakness of LISA Learned Index . LISA can search\nkDNA symbols after each row activation by its IP-BWT.\nMoreover, as Figure 6(b) shows, the size of LISA linearly\nincreases with the step number k. However, the learned\nindex of LISA suffers from low accuracy and low cache\nhit rate . First, the LISA learned index has to index |G|IP-\nBWT entries, where |G|is the length of reference genome.\nFor a human genome, |G|= 3G. When the learned index\nof LISA predicts a wrong position, LISA has to linearly\nsearch up to |G|IP-BWT entries. As Figure 6(c) describes,\non average, LISA has to search ∼3Kextra IP-BWT en-\ntries during each iteration, due to the low accuracy of its\nlearned index. Consequently, compared to 1-step FM-Index,\n21-step LISA ( LISA-21 ) improves search throughput by only\n2.15×in Figure 6(d). But if LISA has a perfect learned\nindex (LISA-21P ) which always predicts the right position,\ncompared to 1-step FM-Index, LISA-21P improves search\nthroughput by 5.1×. Further increasing the step number of\nLISA (LISA-32 ) also introduces more TLB misses, thereby\nachieving smaller search throughput improvement. Second,\ntraversing the learned index’s hierarchical models is also\npointer chasing. The LISA learned index consumes ∼1.5GB.\nIf we assume a perfect cache (100% hit) for LISA-21P\n(LISA-21PC ),LISA-21PC improves search throughput by\n8.53×over 1-step FM-Index.\nAlgorithmic Takeaways .(1) Implementing k-step search\nwith moderately enlarged data structure is important for\nFM-Index. However, a larger step number, i.e., k, may not\nnecessarily lead to better search throughput. (2) A more\n\n0 50 100 150 20001x1072x1073x1074x1075x1076x1077x107Row i d\nFM-I ndex accesses\n(a) Random FM-Index accesses0 5 10 15 20 25 300326496128 Si ze (GB) \nstep # FM-I ndex\n LI SA\n(b) DRAM overhead v.s. step #102103104105\n25%50%75%mean\nmi nErrormax\n(c) Inaccurate LISA-21FM-4FM-5FM-6LI SA-11LI SA-21LI SA-32LI SA-21PLI SA-23PC246810 Norm.  throughput\n(d) Throughput on CPU baseline\nFig. 6. The inefﬁciency of prior FM-Index algorithms (A huma n genome dataset is used). In Figure 6(c), we present the maxi mal and minimal errors, the\nmean of errors, and the 25th,50th,75thpercentiles of errors. In Figure 6(d), FM-X means X-step conventional FM-Index; LISA-X means X-step LISA;\nLISA-XP means X-step LISA with a 100% accurate index; and LISA-XPC denotes X-step LISA with a 100% accurate index and a 100% hit c ache.\nRAS 0RAS 1\nCAS 0CAS 1\nR-A0R-A1 C-A0C-A1RAS \nCAS \nADDR RAS 2\nR-A2 C-A2CAS 20 12 34 56 78 910 11 12 13 14 15 16 17 18 CK \nChip 0\nChip 1\nChip 2data 0\ndata 1\ndata 219 \nRAS 3 RAS 3 delay!!! \nR-A3\nChip3\nFig. 7. The address bus bottleneck of MEDAL.\naccurate learned index reduces linear search overhead to\nimprove search throughput. (3) A higher cache hit rate of\nlearned index also improves search throughput by reducing\nredundant memory accesses to learned index.\nB. Hardware Incapacity\nFPGAs and ASICs . Most prior FM-Index hardware ar-\ntifacts such as CPUs [12], GPUs [21], FPGAs [13], and\nASICs [37] can accelerate only 1-step FM-Index searches. A\nrecent FPGA design [30] supports 2-step FM-Index searches,\nwhilek-step LISA is built merely on CPUs. Existing FM-\nIndex application-speciﬁc accelerators including FPGAs [ 13],\n[30], and ASICs [37] can search only 1 or 2 DNA symbols\nafter each DRAM row activation. Therefore, they cannot full y\nexploit the maximal DRAM bandwidth.\nProcessing-In-Memories . Though recent works [14], [15],\n[38], [39] propose PIM accelerators to process FM-Index\nsearches, they cannot fully utilize the available DRAM band -\nwidth either. Most NVM-based PIMs [14], [38], [39] focus\non processing simple arithmetic computations of FM-Index b y\nNVM arrays, but ignore optimizing external memory accesses .\nFor instance, a ReRAM-based FM-Index PIM, FindeR [14],\nhas only 2.6GB memory arrays, so it still suffers from low\nDRAM bandwidth utilization when fetching FM-Index buckets\nthat cannot ﬁt into its internal arrays from external DRAMs.\nOnly a DRAM PIM, MEDAL [15], modiﬁes its DRAM\nmain memory for higher FM-Index search throughput. Instead\nof processing multiple DNA symbols during a DRAM row\nactivation, MEDAL enables chip-level parallelism where ea ch\nchip in a rank can independently process a DNA symbol by\nopening its partial row. In ideal case, 16 chips in a rank\ncan enlarge FM-Index search throughput by 16×, and each\nchip has only 1/16 row size. However, we ﬁnd the DDR4\naddress bus shared by all chips in a rank seriously limits sea rch\nthroughput of MEDAL. The DDR4 address bus is only 17-\nbit wide [40]. During each access, the row activation and the\ncolumn access serially pass their addresses via the same 17-AA ... 0base \nAC f0+1 \nTG ... \nTTTC MAX \nΣ   f i+1 013 increment \n2 ... 3612 \n1 ... 10 13 MAX If1-1\nMAX xf14 -1 4 ... ... MAX xf0-1 xf0-2\n5 ... 921 MAX xf15-1 26 xf15-2|G|fi=0\n15 Σ\nΣ   f i+1014b0\nFig. 8. A 2-step EXMA table ( MAX means the end of increments of a\nk-mer; and fiindicates the number of increments of the ithk-mer).\nbit bus. As Figure 7 shows, MEDAL can sequentially activate\na 1/16 partial row in chip0∼2. But when activating a 1/16\npartial row in chip3, its row address ( R-A3) and the column\naddress of chip0(C-A0) compete for the same address bus.\nThe activation in chip3has to be delayed to CK12. And idle\nbubbles appear in the DDR4 data bus. Because of address bus\nconﬂicts, although MEDAL claims a 68×search throughput\nimprovement over a multi-core CPU baseline, we observe it\nimproves search throughput by only 11×.\nIV. EXMA\nWe ﬁrst create a row-buffer-friendly alternative to FM-\nIndex, EXMA table, to process multiple DNA symbols in each\nsearch iteration. And then, we present a Multi-task-Learni ng\n(MTL)-based index to accelerate searches over an EXMA\ntable. Compared to LISA learned index, the MTL-based index\nis more accurate, although it has less parameters.\nA. EXMA Table\nData Structure . The major reason why the learned index\nof LISA is not accurate is that it has to index |G|IP-BWT\nentries. To reduce the problem size for a learned index, we\npropose a novel data structure, EXMA table. In each row of\ntheOcc table, only one k-mer can increase its value by one.\nFor instance, in the 2-step Occ table shown in Figure 4, only\n“AC” increases its value from 0 to 1 in the row 1. Based on\nthis observation, as Figure 8 shows, for each k-mer, an EXMA\ntable stores only the row numbers of the Occ table, where its\nvalue increases. For example, for “AA”, its value increases in\nthe row 2, 3, 6, and others. We store these row number as\ntheincrement s of “AA”. Totally, we have f0“AA”s,f1“AC”s,\n..., andf15“TT”s in the Occ table, where f0,...,f 15are\nnon-negative integers and/summationtext15\ni=0fi=|G|. So in the EXMA\ntable, each 2-mer has fiincrements, where 0≤i≤15. We\nadd a symbol MAX to indicate the end of the increments\nof ak-mer, where MAX=|G|+ 1. The increments of an\n\nAAACTTmAA mAC \nmTT pos \n... increments\n(a) Learned indexk-mer, pos \nAAAC TT... \n(b) MTL index\nFig. 9. MTL-based EXMA index.\nEXMA table has the space complexity of O(|G|log(|G|)). For\na 3G-base human genome, the increments occupy 12GB. We\ncan consecutively store the increments of all k-mers to take\nadvantage of the row buffer locality. Each k-mer needs a base\nto point to its ﬁrst increment. For instance, the base of “AC”\nisf0+1indicating its ﬁrst increment is in the position f0+1.\nTotally, a k-step EXMA table stores 4kbases, each of which\nis related to a k-mer. Even if a k-mer has no increment, e.g.,\n“TC”, its base is set to MAX=|G|+1. The bases of a k-step\nEXMA table have the space complexity of O(4klog(|G|)).\nBackward Search . Each iteration in a backward search\ncomputes Equation 1. The entire Count table costs only\nseveral bytes, so the bottleneck is the Occ table lookups.\nTo compute Occ(k-mer,pos)in each search iteration, we\ncompare posagainst all increments of the k-mer and ﬁnd the\nﬁrst increment larger than pos, whereposcan beloworhigh .\nFor instance, to compute Occ(AA,4), we ﬁrst read the base\nof “AA”, which is 0. And then, we initialize a counter and\nstart a linearly search from the position 0 of “AA” to MAX .\nIf an increment is smaller than 4, we increase the counter by\n1. When 6 is found, we stop, since 6>4. At last, the counter\nvalue is 2, i.e., Occ(AA,4) = 2 .\nNa¨ ıve Adoption of Learned Index . Since all increments\nof eachk-mer are sorted, similar to LISA, we can adopt\nlearned index [29] to perform only one comparison to compute\nOcc(k-mer,pos)in the best case. We build a learned index\nmodel hierarchy for each k-mer that has >256 increments.\nAs Figure 9(a) shows, similar to LISA [28], to build learned\nindexes, we use a ﬁxed ratio between the number of parameters\nof a learned index model and the number of increments\nthat need to be indexed. As a result, if a k-mer has more\nincrements, its learned index model has more parameters.\nFor instance, the model of “TT” ( mTT) owns more weights\nand biases than that of “AA” ( mAA), since “TT” has more\nincrements. To compute Occ(AA,pos)in a search iteration,\nwe ﬁrst read all the parameters of mAA, and input only pos\nto the model. If the prediction is not correct, we start a line ar\nsearch from the predicted position to ﬁnd the correct positi on.\nHowever, since an EXMA table have to index totally |G|\nincrements, the learned index of EXMA does not have higher\naccuracy than that of LISA.\nStep # of an EXMA Table . We tuned the step number k\nof an EXMA table to balance the DRAM overhead and search\nthroughput. For a genome reference G, the size of increments\nin an EXMA table is constant, while the size of bases in\nthe table is proportional to 4k. Although a small khas few\nbases, the search throughput is low. For instance, for a 3G-\nbase human genome, if k= 2, the bases require only 32-byte.\nBut each time, only a 2-mer can be processed by a search8 91011121314151617020406080100Si ze (GB)\nstep # SA      i ndex\n i ncre  base\n(a) DRAM overhead v.s. step #LI SA-21EXMA-14EXMA-15EXMA-16EXMA-17EXMA-15M123456Norm.  throughput\n(b) Throughput on CPU baseline\nFig. 10. The trade-off of an EXMA table (EXMA-X means X-step E XMA).\n(a) 15 “A”s\n\u0002\u0003 \n\u0001\u0003 \n\u0002\u0001 \n\u0001\u0002 \n\u0002\u0001 \n\u0001\u0002 \n\u0001\u0003 \u0001\u0002 \n\u0001\u0002 \n\u0002\u0003 \n\u0001\u0002 \u0001\u0002 \n\u0001\u0002 \n\u0001\u0002 \n\u0002\u0003 \u0001\u0001\b\u0004\t\u0005\u0007\u0005\b\n\f \u0002\u0003\u0006\u000b\u0005\f \n(b) 7 “AC”s and “A” (c) 7 “AT”s and “G”\nFig. 11. Increment distributions of 15-mers.\niteration. In contrast, a large kimproves the search throughput\nbut signiﬁcantly increases the number of bases and thus the\nsize of an EXMA table. For a human genome, as Figure 10(a)\nshows, a 15-step EXMA table ( 15) costs 29.5GB, while a 16-\nstep EXMA ( 16) occupies 41.5GB. Increasing kfrom 15 to 16\nincreases 12GB DRAM overhead. As Figure 10(b) describes,\nEXMA (EXMA-15 ) achieves its best search throughput with\n15-step. Compared to LISA-21 ,EXMA-15 degrades search\nthroughput by 7.3%, since it has a smaller step number.\nB. Multi-task-Learning Index for EXMA\nMTL-based Index . To more accurately approximate the\ncumulative distribution function of increments of each k-\nmer in an EXMA table with less parameters, we propose\na Multi-Task-Learning [41]–[43] (MTL)-based index for the\nincrements of an EXMA table. As Figure 11 shows, the\nincrements of various k-mers in15-EXMA exhibit similar\nrandom distributions. Based on Stein’s paradox [44], it is m ore\naccurate to approximate many independent random distribu-\ntions using samples from all of them rather than approximati ng\nthem separately. The MTL-based index is trained with the\nincrements of multiple k-mers to obtain superior accuracy\nover learning the increments of each k-mer independently.\nWe adopt the hard-parameter-sharing MTL [43] that shares\na subset of parameters between the learned index models of\nk-mers having different numbers of increments. For instance ,\nas Figure 9(b) shows, “AA” uses the smallest model ( mAA) to\nindex its increments, since it has the fewest increments amo ng\nall 2-mers. “TT” has more increments, and thus a larger model\n(mTT) which contains the entire mAA. Besides mAA,mTT\ncomprises more levels of nodes to index its increments more\naccurately. Compared to the na¨ ıve learned index, we add mor e\nneurons in the hidden layers of each non-leaf node of a MTL-\nbased index to accommodate two inputs, i.e., posandk-mer.\nBut the size of a MTL-based index is smaller that of the na¨ ıve\nlearned index, since the k-mers share most parameters of a\nMTL-based index.\nInference . The MTL-based index of an EXMA table effec-\ntively approximates Occ(k-mer,pos)as\np=F(k-mer,pos)∗fk-mer (3)\n\n01\n2-256256-1K1K-4K4K-16K16K-64K64K-256K256K-1M>1M10- 610- 510- 410- 310- 210- 1100101102\ni ncrement #0. 000004%Percentage (%)24.8% \n1\n2-256256-1K1K-4K4K-16K16K-64K64K-256K256K-1M>1M0%10%20%30%40%\n(b) (a) i ncrement #ti me breakdownsearch\nFig. 12. Proﬁling EXMA-15 : (a) increment #; (b) search time breakdown.\nwherepis the predicted position of pos in the increments\nof thek-mer;F(.,.)is the neural network model hierarchy\nto estimate the probability to observe an increment ≤pos;\nfk-mer is the number of increments of the k-mer, and can be\nstored along the k-mer base. After each inference, we read\nbothpandp+1 to check whether the prediction is correct.\nIf not, we start a linear search to ﬁnd the correct position.\nTherefore, the accuracy of a MTL-based index decides search\nthroughput of FM-Index, but has no impact on the quality of\nﬁnal DNA mapping . A MTL-based index model hierarchy is\na tree structure. To keep the index size in check, we deploy\nsimple linear regression models [45] as leaf nodes in the mod el\nhierarchy of the MTL-based index. A linear regression model\ncontains only one weight and one bias. Each non-leaf node\nis a neural network having a fully-connected layer, each of\nwhich contains 10 neurons with sigmoid activation.\nTraining . The MTL-based index is built for the increments\nofp k-mers{Ti}p\ni=1. For ak-mer{Ti}, its training dataset\nincludesfiincrements {xi,j}fi\nj=1and their positions {yi,j}fi\nj=1.\nThe learning function for the k-merTiis deﬁned as wT\nix+bi.\nBased on [42], [43], [46], we formulate the loss function to\nlearn the relations between k-mers as\nmin\nW,bp/summationdisplay\ni=1βi\nfifi/summationdisplay\nj=1l(wT\nixi,j+bi,yi,j) (4)\nwhereW= (w1,...,wp);b= (b1,...,b p)T;l(·,·)means\nthe cross-entropy loss function; βiis the importance of k-\nmer{Ti}. We trained the MTL-based index to minimize this\nequation by an Adam optimizer. Similar to LISA [28], the\ntraining and testing of a MTL-based index use the same\ndataset, an EXMA table of a reference. Training a MTL-based\nindex for a reference typically takes 1∼2days. Once a MTL-\nbased index is trained, billions of exact match operations c an\nhappen on its reference.\nl earn-256K l earn-1M MTL-256K MTL-1M10-1100101102103104Errors\nFig. 13. Prediction errors of learned and MTL indexes.\nMTL-based Index Performance . We proﬁled the perf-\normance of EXMA-15 equipped with a na¨ ıve learned index\nin Figure 12. As Figure 12(a) shows, 2.5E-5 %and 4E-6 %\nof 15-mers have 64K ∼256K and >1M increments, respec-\ntively. However, searching the 15-mers having 64K ∼256K\nand>1M increments consumes 36% and 20.5% of the\nsearch time respectively, as shown in Figure 12(b). This\nis because the na¨ ıve learned index predicts a lot wrongEXMA de/comprss \nqueue infer. engine PE PE \nPE PE buffer ... \n... ... ... \n2-stage sch ❶ W, bias \n❹k-mer, index $ \nk-mer ❷base base $ pred. pos \nincrmnts ❻\nW, bias k-mer, pos ❺\nbase❸k-mer MTL index \nDRAM cmpr. \ncmpr. basesEXMA table \n... pos \nmem. ctrl \n❼DMA ctrl \n page mgn \naccelerator \nFig. 14. The architecture of EXMA accelerator.\npositions, and the linear search overhead is large. The pre-\ndiction errors of the na¨ ıve learned index for the 15-mers\nhaving 64K ∼256K (learn-256K ) and>1M (learn-1M )\nincrements are shown in Figure 13. On average, we have\nto search 917 and 2133 more increments to ﬁnd the correct\none for the 15-mers having 64K ∼256K and >1M increments\nrespectively. The MTL-based index greatly improves index\nprediction accuracy by simultaneously learning from multi ple\n15-mers having similar amounts of increments. The MTL-\nbased index ( MTL) further reduces the mean of prediction\nerrors to 45 and 182 for the 15-mers having 64K ∼256K and\n>1M increments respectively. As a result, the MTL-based\nindex (EXMA-15M ) improves search throughput by 75% over\nLISA-21 with only a half number of parameters, as shown\nin Figure 10(b).\nC. EXMA Accelerator\nWe propose a hardware accelerator to process search op-\nerations over an EXMA table using a MTL-based index. We\nintegrate two caches for the bases and the MTL-based index\nof an EXMA table to reduce unnecessary DRAM accesses.\nAnd then, we present EXMA scheduling to improve cache\nhit rate. We also introduce dynamic page policy to improve\nDRAM row buffer hit rate during searching the increments of\nan EXMA table. At last, we create CHAIN compression to\nreduce the EXMA table size.\n1) Accelerator Architecture\nThe architecture of our EXMA accelerator is shown in\nFigure 14. The kernel of the EXMA accelerator is an inference\nengine computing predictions of a MTL-based index. We\nadopt the state-of-the-art Tangram neural network acceler a-\ntor [47] as the inference engine. The Tangram accelerator\nconsists of a number of processing elements (PEs) organized\nin a 2D array. Each PE includes a simple ALU for multiply-\naccumulate (MAC) operations and a small register ﬁle of\n32B. A larger SRAM buffer is shared by all PEs. We add\ntwo small caches for the bases and MTL-based index of an\nEXMA table. Data fetched and stored by the accelerator goes\nthrough a de/compression unit (§IV-C4) that de/compresses\nthe bases and increments of an EXMA table. We integrate a\nscheduling queue into the EXMA accelerator to implement 2-\nstage EXMA scheduling (§IV-C2). At last, the dynamic page\nmanagement (§IV-C3) switching between open and close page\npolicies is implemented in the CPU memory controller.\n2) EXMA Scheduling\nPoor Locality of MTL-based Index . The conventional\nﬁrst-ready ﬁrst-come-ﬁrst-serve (FR-FCFS) policy is adop ted\nby almost all accelerators [13]–[15], [30], [37], [38] to\nschedule FM-Index searches. However, FR-FCFS signiﬁcantl y\ndegrades the hit rates of our base cache and index cache.\n\n(a) base arrayAAAA \nAAAC \nTTTG \nTTTT\n(b) MTL index\n... ... 0x0000 0000 \n0x000C 0011 4-mer base \n0x1E00 00FD \n0x1FFF 00D1 m2m0\n... m1\nm4 m3 m18 \n121 215 1034-mer, pos \n... \n(c) FR-FCFSmiss 4-mer \nR3TTTT R0\nR1\nR229 pos \n998 \nAAAA \n1 TTTG \n99 AAAC base cache \nmiss \nmiss AAAA AAAC TTTG TTTT \nmiss TTTG TTTT miss \nmiss \nhit m0,m 2,m 18 \nmiss m0,m 1,m 3m0,m 1,m 3index cache \nFig. 15. The low cache hit rate of MTL-based index.\n(b) the 2nd stageR0R2\nR1\nR3miss \nhit \nmiss m0,m 1,m 3\nhit m0,m 2,m 18m0,m 2,m 18 index cache \nmiss \nR0R1\nR3\nR2base cache \nhit \nmiss AAAA AAAC AAAA AAAC \nhit TTTG TTTT\n(a) the 1st stage\nFig. 16. The 2-stage EXMA scheduling.\nWe show an example of FR-FCFS in Figure 15, where there\nare 4 FM-Index requests in the form of [k-mer,pos], i.e.,\nR0= [TTTT,998],R1= [AAAA,29],R2= [TTTG,1]\nandR3= [AAAC,99]. As Figure 15(a) shows, all bases\nare stored consecutively in the lexicographical order of k-\nmers. Each base occupies 4-byte. So the bases of AAAA\nandAAAC (TTTC andTTTT ) are stored in the same 64-\nbyte. The MTL-based index used by four requests is shown\nin Figure 15(b). To predict the increments of pos 1 and 29\n(99 and 998), the MTL index nodes of m0,m1andm3\n(m0,m2andm18) are required. Assume the base cache can\ncontain only one 64-byte line, while the index cache can stor e\nthree index nodes. Four requests are scheduled by FR-FCFS\nin Figure 15(c). When R0arrives, both caches are empty and\nthus have a miss. And then, the bases of “TTTG” and “TTTT”\nare stored in the base cache, while the index nodes of m0,m2\nandm18used byR0are stored in the index cache. For R1,\nboth caches also suffer from a miss. The bases of “AAAA”\nand “AAAC” are fetched to the base cache, while m0,m1and\nm3used byR1are installed into the index cache. For R2, the\nbase cache has a miss, but the index cache has a hit. At last,\nboth cache have a miss for R3. Totally, four misses happen in\nthe base cache, while three misses occur in the index cache.\n2-Stage Scheduling . We propose a 2-stage EXMA schedul-\ning technique to enhance the hit rates of the base and index\ncaches. Unlike FR-FCFS scheduling requests based on their\naddresses and order, EXMA re-orders the requests according\nto their data including k-mers and positions ( pos). In the\nﬁrst stage, EXMA sorts the FM-Index requests based on their\nk-mers. By consecutively issuing FM-Index requests in the\nlexicographic order of their k-mers, the hit rate of the base\ncache increases, since the bases of the k-mers sorted in the\nlexicographic order tend to have stronger spatial locality . As\nFigure 16(a) shows, the ﬁrst stage of EXMA scheduling issues\nfour requests in the order of R1,R3,R2, andR0. The base\ncache has two hits and two misses, but the index cache has all\nfour misses. This is why EXMA needs to do the second stage\nof scheduling. During the second stage of EXMA scheduling,\nfour requests are ranked according to their posvalues. Through\nconsecutively computing inferences of MTL indexes of therequests having small differences between their pos values,\nthe index cache can expect more hits. This is because the\nsmaller difference the pos values of two requests exhibit,\nthe more likely these two requests use the same MTL index\nnodes during searches. As Figure 16(b) shows, the index cach e\nhas two misses and two hits. Totally, the 2-stage EXMA\nscheduling has 4 misses and 4 hits.\nImplementation . Our 2-stage EXMA scheduling is imple-\nmented with the scheduling queue that is a content-addressa ble\nmemory (CAM). A CAM can perform sorting operations [48].\nThek-mer and posof a request can be stored in one row of\nthe CAM. Each DNA symbol in a k-mer is denoted by 3 bits,\nsince we need to encode $, A, C, T and G. Requests can be\nsorted in the CAM based on their k-mers or posvalues.\n3) Dynamic Page Policy\nDynamic Page Policy . Prior FM-Index accelerators [13]–\n[15], [30], [37], [38] adopt close-page policy in their DRAM\nmain memories. They always pre-charge a DRAM row after\neach access, since conventional 1-step FM-Index searches\nhave little spatial locality, as shown in Figure 6(a). On the\ncontrary, our EXMA table stores the increments of a k-mer\nconsecutively in DRAM rows. As the algorithm of FM-Index\nbackward search (line 2 ∼3 in Figure 3(d)) indicates, each\niteration issues two requests searching the same k-mer but with\ndifferent position values. In a search iteration, we comput e\nOcc(k-mer,low )andOcc(k-mer,high ). Both search the\nincrements of the same k-mer that are very likely stored in the\nsame row. So our accelerator asks the CPU memory controller\n(MC) to keep the DRAM row open after the ﬁrst request in\na search iteration is processed, since we expect the second\nrequest can hit in the row buffer. However, the row will be\npre-charged, after the second is processed.\nImplementation . The dynamic page policy can be imple-\nmented in the CPU MC. When searching a request, if there\nis another request pending in the scheduling queue of the\naccelerator searches the same k-mer, the CPU MC keeps\nthe DRAM row open after the ongoing request completes.\nOtherwise, it pre-charges that DRAM row. The CPU MC\nmaintains a register to indicate whether all rows are closed ,\nand another register to record which row is open for each bank .\n4) CHAIN compression\nB∆I. The state-of-the-art cache compression technique,\nB∆I [49], breaks a 64-byte cache line into eight 8-byte data\nsections. As Figure 17(a) shows, to compress the cache line,\nB∆I records the ﬁrst data section ( data0) and stores only\nthe difference ( ∆i) between each data section ( datai) and\ndata0. To decompress a data section, B ∆I simply calculates\n\ndata0Δ1...Δ2 Δ7datai=data0+Δi\n(a) B∆I\nincr 1 incr 0 incr 2incr 3\n- - -\nincr 0Δ1Δ2Δ3\nincr i=incr 0+    Δ j\nj=1ΣiΔi=incr i - incr i-1\n+++\nincr 1 incr 0 incr 2incr 3compress \ndecompress \n(b) Compression and decompression of CHAIN\nFig. 17. The CHAIN compression.\ndatai=data0+∆i. B∆I typically reduces data size of SPEC-\nCPU2006 applications by ∼50%. In these applications, data\nsections in a cache line are not sorted.\nCHAIN . Since both increments and bases of each k-mer are\nsorted and stored consecutively in a DRAM row, we believe\nthey are more compressible than the data of SPEC-CPU2006\napplications. Therefore, we propose a novel compression te ch-\nnique, CHAIN, to more aggressively compress an EXMA\ntable. As Figure 17(b) describes, to compress the increment s\nin a 64B memory line, CHAIN ﬁrst stores the ﬁrst increment\n(incr0). And then, it stores the value difference ( ∆i) between\ntwo consecutive increments. So we have ∆i=incri−incri−1.\nTo decompress an increment incriin a 64B memory line,\nCHAIN simply computes incri=incr0+/summationtexti\nj=1∆j. Bases\nof an EXMA table can be (de)compressed in the same way.\nImplementation . The CHAIN compression and decompres-\nsion require only 64-bit adders. Multiple adders concurren tly\noperate for the CHAIN compression, while the CHAIN de-\ncompression requires only one adder for accumulations. The\nCHAIN decompression slightly prolongs FM-Index search\nlatency but greatly increases FM-Index search throughput.\n5) Putting all together\nAs Figure 14 describes, ❶after receiving FM-Index requests\nfrom the CPU, the accelerator stores them in its scheduling\nqueue and performs the ﬁrst stage scheduling. ❷Based on\ntheirk-mers, the accelerator checks whether the bases of\nthe requests stored in the queue are in the base cache or\nnot.❸If misses occur, DRAM accesses are issued to fetch\nthe bases. Otherwise, the accelerator does the second stage\nscheduling. ❹The accelerator checks whether the MTL index\nnodes required by the requests in the queue are in the index\ncache or not, according to both k-mer and posvalues. ❺If\nmisses happen, DRAM accesses are issued to fetch MTL index\nnodes. Otherwise, the inference engine computes with MTL\nindex nodes. ❻Until the inference of a leaf node is ﬁnished,\nthe accelerator issues a DRAM access to read the increment\nin the predicted position. If the increment is correct, the\ncomputation of Occ(k-mer,pos)is completed. Otherwise, it\nlinearly searches DRAM rows to ﬁnd the correct increment. ❼\nAll DRAM accesses from the EXMA accelerator are managed\nby its DMA controller communicating with the CPU MC,\nwhich decides whether to pre-charge opened rows based on\nthe requests in the scheduling queue of the accelerator.TABLE I\nTHE HARDWARE CONFIGURATION OF EXMA.\nComponent Description Area (mm2)Energy/Op ( pJ)\nInfer. engine 48×8PE arrays 0.512 0.25\nSch. queue SRAM, 128-bit ×256 0.023 1.9\nIndex cache SRAM, 32KB, 16-way 0.084 2.62\nBase cache eDRAM, 1MB, 8-way 0.667 17.2\nDe/compress 32 64-bit adders 0.091 0.21\nSch. & row 2-stage sch. & dyn. page 0.035 1.02\nDMA ctrl adopted from [52] 0.21 3.42\nEXMA accelerator: area 1.62 mm2, and leakage 223.8 mW\nCPU 2.5GHz , 16-core, 40MB LLC, 64 LLC MSHRs\nDRAM DDR4-2400, 384GB, 4 channels, 3 DIMMs / channel,\nmain 4 ranks / DIMM, 2 bank groups / rank, 16 chips / rank,\nmemory 2 banks / bank group, FR-FCFS, close-page, row size 2KB\nsystem 2 chips / data buffer, tRCD -tCAS -tRP: 16-16-16\n6) System Integration\nOur EXMA is connected to a CPU processor as a loosely-\ncoupled non-coherent accelerator [50], [51] by a Network-on-\nChip (NoC). EXMA accesses DRAM via two DMA-dedicated\nplanes of the NoC, bypassing the cache hierarchy of the CPU.\nThe EXMA data region must be ﬂushed from the CPU cache\nhierarchy before FM-Index searches start. We chose the non-\ncoherent model [50] for better performance, since the memor y\nfootprint of FM-Index searches is always larger than the CPU\nLLC capacity.\nD. Design Overhead\nThe training overhead of a MTL-based index is shown\nin the Section of Training in §IV-B. The hardware over-\nhead of the EXMA accelerator is summarized in Table I.\nFrom [47], we adopted the inference engine, which runs at\n800MHz and is synthesized with Synopsys 28nm generic\nlibrary. We quantized the model hierarchy of MTL index\nwith 8-bit without accuracy degradation. A PE has an 8-bit\nMAC ALU and a 32B register ﬁle. The inference engine\ncontains 4 8×8PE arrays, each of which has a 16KB shared\nSRAM buffer and an activation unit. The EXMA accelerator\nalso includes a scheduling queue (SRAM CAM) with 512\n128-bit entries, a 32KB 16-way SRAM index cache, and\na 1MB 8-way eDRAM base cache. We modeled the area\nand power of memory components including registers, buffer s\nand caches by CACTI [53]. We used the same DDR4-2400\nDRAM main memory conﬁguration as the recent FM-Index\nPIM MEDAL [15]. The EXMA accelerator connects to four\nDRAM channels, with a total 384GB capacity. We adopted\nDRAMPower [54] to model the power consumption of our\nDDR4 main memory.\nV. E XPERIMENTAL METHODOLOGY\nSimulation . We used gem5-aladdin [55] to model our CPU\nbaseline and our EXMA accelerator. The conﬁguration of our\nCPU baseline is shown in Table I. We used McPAT [56] to\ncalculate the power consumption of the CPU processor. We\nimplemented the main memory system using DRAMsim2 [57].\nAccelerator Baselines . Besides CPU, we also ran the FM-\nIndex search kernel on an Nvidia Tesla P100 GPU [58],\na Stratix-V FPGA [30], and a 28nm ASIC design [37].\nWe compared EXMA against recent FM-Index PIM designs,\n\nhuman pi cea pi nus gmean05101520253035throughput EXMA-15  EX-acc  EX-2stage  EXMAnorm.  search\nFig. 18. The search throughput of EXMA (norm. to CPU).\nMEDAL [15] and FindeR [14]. We used gem5-gpu [59] to\nsimulate the GPU, and gem5-aladdin to model the computing\nunits of FPGA, ASIC and PIMs. All their DRAM main\nmemories are implemented by DRAMsim2. The power data\nof the GPU, FPGA, ASIC and PIMs are adopted from [14],\n[15], [30], [37], [58]. The power of DRAM main memories is\nalso modeled by DRAMPower.\nWorkloads . To evaluate EXMA, we adopted FM-Index-\nbased genome analysis applications: BWA-MEM [12] for short\nread alignment, MA [16] for long read alignment, SGA [24]\nfor read assembly, ExactWordMatch [25] for annotation and\na reference-based compression algorithm [26]. SGA for long\nreads uses the FM-Index-based error correcting scheme [33]\nto reduce errors.\nDatasets . For alignment, annotation and compression, we\nused human (human, 3G-bp), picea glauca (picea, 20G-bp),\nand pinus lambertiana (pinus, 31G-bp) genomes as reference\ngenomes. To study short reads, we adopted DWGSim [60] to\ngenerate 101-bp short reads with 50×coverage. To evaluate\nlong reads, we created long reads (with length of 1K-bp) by\nPBSIM [61]. The error proﬁles of reads is summarized in\nthe format of (name, mismatch%, insertion%, deletion%, tot al\nerror%), i.e., (Illumina, 0.18%, 0.01%, 0.01%, 0.2%) [62],\n(PacBio, 1.50%, 9.02%, 4.49%, 15.01%), and (ONT 2D,\n16.50%, 5.10%, 8.40%, 30.0%) [10].\nSchemes . The schemes we studied can be summarized as:\n•CPU: We ranLISA-21 for FM-Index searches in genome\napplications on our CPU baseline. We also applied B ∆I [49]\ncompression on LISA data for three datasets.\n•EXMA-15 :EXMA-15 with the MTL-based index and\nCHAIN compression is used to replace LISA-21 inCPU.\n•EX-acc : We ranEXMA-15 on the EXMA accelerator.\n•EX-2stage : 2-stage scheduling is added to EX-acc .\n•EXMA : Dynamic page policy is enabled on EX-2stage .\nVI. R ESULTS AND ANALYSIS\nThroughput Comparison against CPU. As Figure 18\nshows, we compare FM-Index search throughput of EXMA\nandCPU by running the seeding of short read alignment, since\nFM-Index searches consume 99% of the seeding time in short\nread alignment. Compared to CPU, on average, EXMA-15\nimproves search throughput by 80%. Our MTL-based index\nachieves high accuracy on picea, since the increment distri bu-\ntions of its different k-mers are more similar to each other.\nEX-acc improves search throughput by 7.25×overCPU.\nOur EXMA accelerator can support more concurrent search\noperations, while CPU has only a limited number of LLC\nMSHRs. Compared to CPU,EX-2stage increases search\nthroughput by 15×. Pinus with EX-2stage has the smallest\nthroughput improvement. Because the size of the pinus MTL-al i gnmentassembl yal i gnmentassembl yal i gnmentassembl yannotatecompressgmean123456 norm.  speedup human  pi cea  pi nusPacBi o Nanopore I l l umi na\nFig. 19. The speedup of EXMA in genome analysis (norm. to CPU).\nbased index is the largest among 3 datasets, and thus its inde x\ncache has the lowest hit rate. On average, EXMA increases\nsearch throughput by 23.6×overCPU.\nPerformance Comparison against CPU. We report and\ncompare the speedup achieved by EXMA in various genome\napplications in Figure 19, where we list 3 sets of “alignment\nand assembly” for reads generated by Illumina, Nanopore,\nand PacBio respectively. For each application, although EXMA\nobtains smaller FM-Index search throughput improvement on\nlarger datasets (Figure 18), e.g., pinus, EXMA improves the\napplication performance more signiﬁcantly on larger datas ets.\nThis is because CPU consumes a larger portion of the execu-\ntion time of a genome analysis application to perform FM-\nIndex searches when processing larger datasets that introd uce\nmore TLB and data cache misses. EXMA achieves larger\nperformance improvement on alignment and assembly for Illu -\nmina, annotation, and compression, since FM-Index searche s\ndominate the execution of these applications. On average,\nEXMA improves the performance of genome applications by\n2.5× ∼3.2×, when processing various datasets.hum\npi c\npi n\nhum\npi c\npi n\nhum\npi c\npi n\nhum\npi c\npi n\nhum\npi c\npi n\nhum\npi c\npi n\nhum\npi c\npi n\nhum\npi c\npi n\nhum\npi c\npi n0. 00. 20. 40. 60. 8 DRAM-chi p  DRAM-I O  EXMA-dyn  EXMA-l eak  CPU\nPacBi o Nanopore I l l umi na\ngmean assem norm.  energy\nal i gn assemal i gn assemal i gn compreanno\nFig. 20. The energy reduction of EXMA in genome analysis (nor m. to\nCPU. DRAM-chip/IO indicates the energy of DRAM chips/DDR4 inte rface.\nEXMA-leak/dyn is the static/dynamic energy of the EXMA acce lerator).\nEnergy Comparison against CPU. We compare the energy\nreduction obtained by EXMA in various genome applications\nin Figure 20, where we list 3 sets of “align(ment) and\nasse(mbly)” for reads generated by Illumina, Nanopore, and\nPacBio respectively. On average, EXMA reduces total energy\nconsumption of genome analysis by 61%∼70% when\nprocessing different datasets. The major part of the energy\nreduction comes from voiding using the CPU processor dur-\ning FM-Index searches. The more time FM-Index searches\nconsume in a genome analysis application, the more energy\nreductionEXMA can achieve in that application. On average,\nthe EXMA accelerator consumes only <3% of the total\nenergy consumption of various genome applications. The vas t\nmajority of energy consumption is consumed by the DRAM\nmain memory and the CPU handling non-FM-Index-search\nparts in genome analysis applications.\nComparison against Accelerators . We evaluated EXMA\nand compare it against various hardware accelerators inclu ding\na GPU [58], a FPGA [30], an ASIC [37], and two PIMs [14],\n[15] when processing pinus in Table II. Not all accelerators\ncan run the whole genome applications, so we use “million\n\nASI CGPUMEDALEXMA20%40%60%80%100%BW uti l i zati on\nFig. 21. Bandwidth utilization2D3D4D2A4A8A256E512E1KE512KB1MB2MB0. 40. 60. 81. 01. 2\nEXMA\nMEDALbase cache CAM-(E)ntri es (A)rray #throughputnorm.  search(D)I MM #\nFig. 22. Design space exploration (norm. to EXMA )ori gi nal B∆I \norgi nal CHAI N080160240320\nEXMA-15si ze (GB) BWT  i ncr\n base  I nd\nLI SA-21\nFig. 23. CHAIN on pinus\nTABLE II\nTHE COMPARISON OF ACCELERATORS WHEN PROCESSING PINUS .\nGPU FPGA [30] ASIC [37] MEDAL [15] FindeR [14] EXMA\nAlgorithm LISA-21 FM-2 FM-1 FM-1 FM-1 EXMA-15\nMem (GB) 384 384 384 384 384 384\nAcc Power (W) 182 11 9.4 0.011 0.28 0.89\nMem Power (W) 72 72 72 72 72 72\nMbase/s 157 96 34 102 93 504\nMbase/s/Watt 0.61 0.6 0.42 1.42 1.28 6.9\nbase per second” (Mbase/s) as the performance metric to eval -\nuate only FM-Index search throughput. Different accelerat ors\nadopt different search algorithms. We implemented LISA-21\non the Tesla P100 GPU. The FPGA design [30] supports FM-2\n(conventional 2-step) searches. EXMA performs EXMA-15\nsearches. The other accelerators can conduct only FM-1\nsearches. Since the capacity of internal memories of the GPU\n(16GB HBM) and the PIM FindeR (2.6GB ReRAM) is smaller\nthan thepinus data size. We provide the same DDR4 main\nmemory conﬁguration shown in Table I to all accelerators.\nThe power values of both the accelerator (Acc Power) and\nthe DDR4 main memory (Mem Power) are listed in Ta-\nble II. The search performance is decided by two factors, i.e .,\nthe memory bandwidth utilization, and the number of DNA\nsymbols searched during each iteration. For the bandwidth\nutilization, only EXMA supports dynamic page policy, while\nthe other use only close page policy. MEDAL can support\nchip-level parallelism, but its search throughput is limit ed by\nthe address bus. So EXMA achieves the highest bandwidth\nutilization. For the number of DNA symbols searched during\neach iteration, only the GPU and EXMA can process >2DNA\nsymbols in each iteration. But the low accuracy of learned\nindex makes the GPU to search many unnecessary IP-BWT\nentries to ﬁnd the correct one. Therefore, EXMA obtains the\nbest search throughput. The throughput per Watt of the GPU\nand FPGA designs is low, since the power consumption of\ntheir computing parts is not trivial. For the PIMs and EXMA,\nthe power consumption is dominated by the DRAM main\nmemory. Compared to the PIM MEDAL, EXMA improves\nsearch throughput per Watt by 4.8×.\nBandwidth utilization . Figure 21 shows the comparison of\nbandwidth utilization, which is deﬁned as the ratio between\nthe data capacity fetched from DRAM and total DRAM band-\nwidth. ASIC using FM-1 has only 26% of the total DRAM\nbandwidth, since it uses close-page policy and fetches only\n64B after activating a 2KB row. GPU implementing LISA-21\nfetches entire rows from host memory, so it achieves higher\nbandwidth utilization. MEDAL increases bandwidth utiliza tion\nby activating each individual chips. However, due to conﬂic ts\non the address bus, MEDAL uses only 67% of the DRAM\nbandwidth. In contrast, EXMA obtains 91% bandwidth utiliza -tion by switching between close-page and open-page policie s.\nDIMM number . We studied the sensitivity of EXMA and\nMEDAL to the DIMM number in Figure 22. With 2 DIMMs,\nEXMA improves search throughput by 29% over MEDAL. By\nhaving 3 DIMMs, EXMA linearly scales its throughput up by\n40%, since a single EXMA accelerator can maintain all the\nDIMMs. However, MEDAL increases its throughput by only\n14.5% with 3 DIMMs. Each MEDAL PIM accelerator sits on\na DIMM. More DIMMs bring more ranks. MEDAL suffers\nfrom non-trivial inter-DIMM communication overhead. When\nthe number of DIMM increases to 4, the search throughput of\nneither EXMA nor MEDAL increases signiﬁcantly. The data\nbus (bandwidth utilization) of EXMA is saturated, while the\naddress bus of MEDAL is saturated.\nPE Array number . We show search throughput of EXMA\nwith a varying number of PE arrays in Figure 22. Two PE\narrays of EXMA already achieve 89% of the search throughput\nof the conﬁguration with four PE arrays. This is because the\ncomputations of MTL-based indexes are not intensive. Furth er\nincreasing the PE array number to 8 increases search through -\nput by only 3% over the conﬁguration with four PE arrays.\nSo we selected 4 PE arrays in our baseline conﬁguration.\nCAM & Cache . We explored search throughput of EXMA\nwith varying CAM and base cache sizes in Figure 22. We use\na CAM consisting of 256, 512, and 1024 entries to serve as the\nscheduling queue of EXMA. A 256-entry CAM cannot fully\nsatisfy 2-stage scheduling and scheduling for dynamic page\npolicy, and thus achieves only 77% of search throughput of\nthe conﬁguration with a 512-entry CAM. Further increasing\nthe CAM entry number to 1K improves search throughput\nby only 2% over the conﬁguration with a 512-entry CAM.\nCompared to the index cache, the search throughput is more\nsensitive to the capacity of the base cache, since the global\nbuffer and register ﬁle of PE arrays can temporarily store\nMTL-based index nodes. We tried 512KB, 1MB and 2MB\nfor the base cache. The search throughput stops increasing\nsigniﬁcantly, when the base cache capacity reaches 1MB. So\nwe selected a 512-entry CAM and a 1MB base cache in our\nbaseline conﬁguration.\nCHAIN . We show the compression result of CHAIN on\npinus in Figure 23. Since the size of the IP-BWT table of\nLISA-21 is proportional to its step number, the total data size\nofLISA-21 (original) is 2.2×larger than that of EXMA-15\n(original). After B ∆I compresses the data size of LISA-21\nto 50%, i.e., 152GB. On the contrary, CHAIN compresses the\ndata size of EXMA-15 to only 25%, i.e., 40GB. We observed\nsimilar compression rates of B ∆I and CHAIN on the other\ngenome datasets.\n\nVII. C ONCLUSION\nThough state-of-the-art genome analysis adopts FM-Index\nto process exact-matches, FM-Index is notorious of random\nmemory access patterns. In this paper, we ﬁrst present a\nrow-buffer-friendly and highly-compressible EXMA table w ith\na MTL-based index to process multiple DNA symbols by\nactivating a DRAM row during each search iteration. And\nthen, we build a hardware accelerator to process FM-Index\nsearches on a EXMA table. Compared to the state-of-the-art\nFM-Index PIM MEDAL, EXMA improves search throughput\nby4.9×, and enhances search throughput per Watt by 4.8×.\nREFERENCES\n[1] M. Schirmer, U. Z. Ijaz, R. D’Amore, N. Hall, W. T. Sloan, a nd\nC. Quince, “Insight into biases and sequencing errors for am plicon\nsequencing with the illumina miseq platform,” Nucleic acids research ,\nvol. 43, no. 6, pp. e37–e37, 2015.\n[2] J. J. Mosher, B. Bowman, E. L. Bernberg, O. Shevchenko, J. Kan,\nJ. Korlach, and L. A. Kaplan, “Improved performance of the pa cbio\nsmrt technology for 16s rdna sequencing,” Journal of microbiological\nmethods , vol. 104, pp. 59–60, 2014.\n[3] M. Eisenstein, “Oxford nanopore announcement sets sequ encing sector\nabuzz,” Nature Biotechnology , vol. 30, no. 4, pp. 295–297, 2012.\n[4] J. D. Merker, A. M. Wenger, T. Sneddon, M. Grove, Z. Zappal a,\nL. Fresard, D. Waggott, S. Utiramerur, Y . Hou, K. S. Smith et al. ,\n“Long-read genome sequencing identiﬁes causal structural variation in a\nmendelian disease,” Genetics in Medicine , vol. 20, no. 1, p. 159, 2018.\n[5] X. Ma, M. Mau, and T. F. Sharbel, “Genome editing for globa l food\nsecurity,” Trends in biotechnology , 2017.\n[6] T. Hoenen, A. Groseth, K. Rosenke, R. J. Fischer, A. Hoene n, S. D.\nJudson, C. Martellaro, D. Falzarano, A. Marzi, R. B. Squires et al. ,\n“Nanopore sequencing as a rapidly deployable ebola outbrea k tool,”\nEmerging infectious diseases , vol. 22, no. 2, p. 331, 2016.\n[7] J. Quick, N. D. Grubaugh, S. T. Pullan, I. M. Claro, A. D. Sm ith,\nK. Gangavarapu, G. Oliveira, R. Robles-Sikisaka, T. F. Roge rs, N. A.\nBeutler et al. , “Multiplex pcr method for minion and illumina sequencing\nof zika and other virus genomes directly from clinical sampl es,”nature\nprotocols , vol. 12, no. 6, p. 1261, 2017.\n[8] N. Zhu, D. Zhang, W. Wang, X. Li, B. Yang, J. Song, X. Zhao, B . Huang,\nW. Shi, R. Lu et al. , “A novel coronavirus from patients with pneumonia\nin china, 2019,” New England Journal of Medicine , 2020.\n[9] S. Canzar and S. L. Salzberg, “Short read mapping: An algo rithmic\ntour,” Proceedings of the IEEE , vol. 105, no. 3, pp. 436–458, 2017.\n[10] Y . Turakhia, G. Bejerano, and W. J. Dally, “Darwin: A gen omics co-\nprocessor provides up to 15,000x acceleration on long read a ssembly,” in\nACM Architectural Support for Programming Languages and Op erating\nSystems , 2018.\n[11] D. Fuijiki, A. Subramaniyan, T. Zhang, Y . Zheng, R. Das, D. Blaauw,\nand S. Narayanasamy, “Genax: A genome sequencing accelerat or,” in\nIEEE/ACM International Symposium on Computer Architectur e, 2018.\n[12] H. Li, “Aligning sequence reads, clone sequences and as sembly contigs\nwith bwa-mem,” arXiv preprint arXiv:1303.3997 , 2013.\n[13] M. C. F. Chang, Y . T. Chen, J. Cong, P. T. Huang, C. L. Kuo,\nand C. H. Yu, “The smem seeding acceleration for dna sequence\nalignment,” in IEEE International Symposium on Field-Programmable\nCustom Computing Machines FCCM , 2016, pp. 32–39.\n[14] F. Zokaee, M. Zhang, and L. Jiang, “Finder: Acceleratin g fm-index-\nbased exact pattern matching in genomic sequences through r eram\ntechnology,” in International Conference on Parallel Architectures and\nCompilation Techniques , 2019, pp. 284–295.\n[15] W. Huangfu, X. Li, S. Li, X. Hu, P. Gu, and Y . Xie, “Medal: S calable\ndimm based near data processing accelerator for dna seeding algorithm,”\ninIEEE/ACM International Symposium on Microarchitecture , 2019, p.\n587–599.\n[16] M. Schmidt, K. Heese, and A. Kutzner, “Accurate high thr oughput align-\nment via line sweep-based seed processing,” Nature communications ,\nvol. 10, no. 1, pp. 1–10, 2019.\n[17] A. Nag, C. Ramachandra, R. Balasubramonian, R. Stutsma n, E. Gi-\nacomin, H. Kambalasubramanyam, and P.-E. Gaillardon, “Gen cache:\nLeveraging in-cache operators for efﬁcient sequence align ment,” inIEEE/ACM International Symposium on Microarchitecture , 2019, pp.\n334–346.\n[18] R. Kaplan, L. Yavits, R. Ginosar, and U. Weiser, “A resis tive cam\nprocessing-in-storage architecture for dna sequence alig nment,” IEEE\nMicro , 2017.\n[19] A. Madhavan, T. Sherwood, and D. Strukov, “Race logic: A hardware\nacceleration for dynamic programming algorithms,” in IEEE/ACM In-\nternational Symposium on Computer Architecture , 2014.\n[20] E. Rucci, C. Garcia, G. Botella, A. De Giusti, M. Naiouf, and M. Prieto-\nMatias, “Accelerating smith-waterman alignment of long dn a sequences\nwith opencl on fpga,” in International Conference on Bioinformatics and\nBiomedical Engineering . Springer, 2017, pp. 500–511.\n[21] R. Luo, T. Wong, J. Zhu, C.-M. Liu, X. Zhu, E. Wu, L.-K. Lee , H. Lin,\nW. Zhu, D. W. Cheung et al. , “Soap3-dp: fast, accurate and sensitive\ngpu-based short read aligner,” PloS one , vol. 8, no. 5, p. e65632, 2013.\n[22] M. Burrows and D. J. Wheeler, “A block-sorting lossless data compres-\nsion algorithm,” 1994.\n[23] N. Ahmed, K. Bertels, and Z. Al-Ars, “A comparison of see d-and-\nextend techniques in modern dna read alignment algorithms, ” in IEEE\nInternational Conference on Bioinformatics and Biomedici ne, 2016, pp.\n1421–1428.\n[24] J. T. Simpson and R. Durbin, “Efﬁcient de novo assembly o f large\ngenomes using compressed data structures,” Genome research , vol. 22,\nno. 3, 2012.\n[25] J. Healy, E. E. Thomas, J. T. Schwartz, and M. Wigler, “An notating large\ngenomes with exact word matches,” Genome research , vol. 13, no. 10,\npp. 2306–2315, 2003.\n[26] P. Prochazka and J. Holub, “Compressing similar biolog ical sequences\nusing fm-index,” in Data Compression Conference , 2014, pp. 312–321.\n[27] A. Chacon, S. Marco-Sola, A. Espinosa, P. Ribeca, and J. C. Moure,\n“Boosting the fm-index on the gpu: Effective techniques to m itigate\nrandom memory access,” IEEE/ACM Transactions on Computational\nBiology and Bioinformatics , vol. 12, no. 5, pp. 1048–1059, 2015.\n[28] D. Ho, J. Ding, S. Misra, N. Tatbul, V . Nathan, V . Md, and T . Kraska,\n“Lisa: Towards learned dna sequence search,” in Workshop on Systems\nfor ML at NeurIPS 2019 , 2019.\n[29] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis , “The\ncase for learned index structures,” in ACM International Conference\non Management of Data , 2018, p. 489–504.\n[30] J. Arram, T. Kaplan, W. Luk, and P. Jiang, “Leveraging fp gas for acceler-\nating short read alignment,” IEEE/ACM Transactions on Computational\nBiology and Bioinformatics , 2017.\n[31] Y . C. Wu, C. H. Chang, J. H. Hung, and C. H. Yang, “A 135-mw\nfully integrated data processor for next-generation seque ncing,” IEEE\nTransactions on Biomedical Circuits and Systems , 2017.\n[32] M. A. Quail, M. Smith, P. Coupland, T. D. Otto, S. R. Harri s, T. R.\nConnor, A. Bertoni, H. P. Swerdlow, and Y . Gu, “A tale of three\nnext generation sequencing platforms: comparison of ion to rrent, paciﬁc\nbiosciences and illumina miseq sequencers,” BMC genomics , vol. 13,\nno. 1, p. 341, 2012.\n[33] J. R. Wang, J. Holt, L. McMillan, and C. D. Jones, “Fmlrc: Hybrid long\nread error correction using an fm-index,” BMC bioinformatics , vol. 19,\nno. 1, p. 50, 2018.\n[34] Y .-T. Huang and Y .-W. Huang, “An efﬁcient error correct ion algorithm\nusing fm-index,” BMC bioinformatics , vol. 18, no. 1, p. 524, 2017.\n[35] H. Li, “Exploring single-sample snp and indel calling w ith whole-\ngenome de novo assembly,” Bioinformatics , vol. 28, no. 14, pp. 1838–\n1844, 2012.\n[36] A. Chac´ on, J. C. Moure, A. Espinosa, and P. Hern´ andez, “n-step fm-\nindex for faster pattern matching,” Procedia Computer Science , vol. 18,\npp. 70–79, 2013.\n[37] Y . Wang, X. Li, D. Zang, G. Tan, and N. Sun, “Accelerating fm-index\nsearch for genomic data processing,” in International Conference on\nParallel Processing , 2018.\n[38] S. Angizi, J. Sun, W. Zhang, and D. Fan, “Aligns: A proces sing-in-\nmemory accelerator for dna short read alignment leveraging sot-mram,”\ninACM/IEEE Design Automation Conference , 2019, pp. 1–6.\n[39] S. Angizi, J. Sun, W. Zhang, and D. Fan, “Pim-aligner: A p rocessing-in-\nmram platform for biological sequence alignment,” in Design, Automa-\ntion Test in Europe Conference & Exhibition , 2020, pp. 1265–1270.\n[40] JEDEC, “Ddr4 sdram standard jesd79-4c,” 2020. [Online ]. Available:\nhttps://www.jedec.org/standards-documents/docs/jesd 79-4a\n[41] S. Ruder, “An overview of multi-task learning in deep ne ural networks,”\nCoRR , vol. abs/1706.05098, 2017.\n\n[42] A. Kendall, Y . Gal, and R. Cipolla, “Multi-task learnin g using uncer-\ntainty to weigh losses for scene geometry and semantics,” in IEEE\nconference on computer vision and pattern recognition , 2018, pp. 7482–\n7491.\n[43] H. Bilen and A. Vedaldi, “Integrated perception with re current multi-task\nneural networks,” in Advances in neural information processing systems ,\n2016, pp. 235–243.\n[44] C. Stein, “Inadmissibility of the usual estimator for t he mean of a\nmultivariate normal distribution,” Stanford University, Tech. Rep., 1956.\n[45] M. H. Kutner, C. J. Nachtsheim, J. Neter, W. Li et al. ,Applied linear\nstatistical models . McGraw-Hill Irwin New York, 2005, vol. 5.\n[46] A. Argyriou, T. Evgeniou, and M. Pontil, “Multi-task fe ature learning,”\ninAdvances in neural information processing systems , 2007, pp. 41–48.\n[47] M. Gao, X. Yang, J. Pu, M. Horowitz, and C. Kozyrakis, “Ta ngram:\nOptimized coarse-grained dataﬂow for scalable nn accelera tors,” in ACM\nInternational Conference on Architectural Support for Pro gramming\nLanguages and Operating Systems , 2019, pp. 807–820.\n[48] I. Okabayashi, H. Kotani, and H. Kadota, “A proposed str ucture of a 4\nmbit content-addressable and sorting memory,” in Symposium on VLSI\nCircuits Digest of Technical Papers , 1990, pp. 109–110.\n[49] G. Pekhimenko, V . Seshadri, O. Mutlu, M. A. Kozuch, P. B. Gibbons,\nand T. C. Mowry, “Base-delta-immediate compression: Pract ical data\ncompression for on-chip caches,” in IEEE International Conference on\nParallel Architectures and Compilation Techniques , 2012.\n[50] K. Bhardwaj, M. Havasi, Y . Yao, D. M. Brooks, J. M. H. Loba to, and\nG. Wei, “Determining optimal coherency interface for many- accelerator\nsocs using bayesian optimization,” IEEE Computer Architecture Letters ,\nvol. 18, no. 2, pp. 119–123, 2019.\n[51] D. Giri, P. Mantovani, and L. P. Carloni, “Accelerators and coherence:\nAn soc perspective,” IEEE Micro , vol. 38, no. 6, pp. 36–45, 2018.\n[52] G. Ma and H. He, “Design and implementation of an advance d dma\ncontroller on amba-based soc,” in IEEE International Conference on\nASIC , 2009, pp. 419–422.\n[53] N. P. Jouppi, A. B. Kahng, N. Muralimanohar, and V . Srini vas, “Cacti-\nio: Cacti with off-chip power-area-timing models,” IEEE Transactions\non Very Large Scale Integration Systems , vol. 23, no. 7, pp. 1254–1267,\n2014.\n[54] K. Chandrasekar, C. Weis, Y . Li, B. Akesson, N. Wehn, and K. Goossens,\n“Drampower: Open-source dram power & energy estimation too l,”\n2012. [Online]. Available: https://github.com/tukl-msd /DRAMPower\n[55] Y . S. Shao, S. L. Xi, V . Srinivasan, G.-Y . Wei, and D. Broo ks,\n“Co-designing accelerators and soc interfaces using gem5- aladdin,” in\nIEEE/ACM International Symposium on Microarchitecture (M ICRO) ,\n2016, pp. 1–12.\n[56] S. Li, J. H. Ahn, R. D. Strong, J. B. Brockman, D. M. Tullse n, and\nN. P. Jouppi, “Mcpat: an integrated power, area, and timing m odeling\nframework for multicore and manycore architectures,” in IEEE/ACM\nInternational Symposium on Microarchitecture , 2009, pp. 469–480.\n[57] P. Rosenfeld, E. Cooper-Balis, and B. Jacob, “Dramsim2 : A cycle\naccurate memory system simulator,” IEEE computer architecture letters ,\nvol. 10, no. 1, pp. 16–19, 2011.\n[58] NVIDIA, “Nvidia tesla p100,” 2016. [Online]. Availabl e:\nhttps://images.nvidia.com/content/pdf/tesla/whitepa per/pascal-architecture-whitepaper.pdf\n[59] J. Power, J. Hestness, M. Orr, M. Hill, and D. Wood, “gem5 -gpu:\nA heterogeneous cpu-gpu simulator,” Computer Architecture Letters ,\nvol. 13, no. 1, Jan 2014.\n[60] H. Li, B. Handsaker, A. Wysoker, T. Fennell, J. Ruan, N. H omer,\nG. Marth, G. Abecasis, and R. Durbin, “The sequence alignmen t/map\nformat and samtools,” Bioinformatics , vol. 25, no. 16, pp. 2078–2079,\n2009.\n[61] Y . Ono, K. Asai, and M. Hamada, “Pbsim: Pacbio reads simu la-\ntor—toward accurate genome assembly,” Bioinformatics , vol. 29, no. 1,\npp. 119–121, 2012.\n[62] M. Schirmer, R. D’Amore, U. Z. Ijaz, N. Hall, and C. Quinc e, “Illumina\nerror proﬁles: resolving ﬁne-scale variation in metagenom ic sequencing\ndata,” BMC bioinformatics , vol. 17, no. 1, p. 125, 2016.",
  "textLength": 74599
}