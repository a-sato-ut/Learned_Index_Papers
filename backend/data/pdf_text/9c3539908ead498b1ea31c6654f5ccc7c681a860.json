{
  "paperId": "9c3539908ead498b1ea31c6654f5ccc7c681a860",
  "title": "Improved Frequency Estimation Algorithms with and without Predictions",
  "pdfPath": "9c3539908ead498b1ea31c6654f5ccc7c681a860.pdf",
  "text": "Improved Frequency Estimation Algorithms with and\nwithout Predictions\nAnders Aamand\nMIT\naamand@mit.eduJustin Y. Chen\nMIT\njustc@mit.eduHuy Lê Nguyê ˜n\nNortheastern University\nhu.nguyen@northeastern.edu\nSandeep Silwal\nMIT\nsilwal@mit.eduAli Vakilian\nTTIC\nvakilian@ttic.edu\nAbstract\nEstimating frequencies of elements appearing in a data stream is a key task in large-\nscale data analysis. Popular sketching approaches to this problem (e.g., CountMin\nand CountSketch) come with worst-case guarantees that probabilistically bound\nthe error of the estimated frequencies for any possible input. The work of Hsu\net al. (2019) introduced the idea of using machine learning to tailor sketching\nalgorithms to the specific data distribution they are being run on. In particular, their\nlearning-augmented frequency estimation algorithm uses a learned heavy-hitter\noracle which predicts which elements will appear many times in the stream. We\ngive a novel algorithm, which in some parameter regimes, already theoretically\noutperforms the learning based algorithm of Hsu et al. without the use of any pre-\ndictions. Augmenting our algorithm with heavy-hitter predictions further reduces\nthe error and improves upon the state of the art. Empirically, our algorithms achieve\nsuperior performance in all experiments compared to prior approaches.\n1 Introduction\nIn frequency estimation, we stream a sequence of elements from [n] :={1, . . . , n }, and the goal is to\nestimate fi, the frequency of the ith element, at the end of the stream using low-space. Frequency\nestimation is one of the central problems in data streaming with a wide range of applications from\nnetworking (gathering important monitoring statistics [ 31,62,46]) to machine learning (NLP [ 33],\nfeature selection [ 3], semi supervised learning [ 58]). CountMin (CM) [ 20] and CountSketch (CS)\n[14] are arguably the most popular and versatile of the algorithms for frequency estimation, and are\nimplemented in many popular packages such as Spark [63], Twitter Algebird [10], and Redis.\nStandard approaches to frequency estimation are designed to perform well in the worst-case due to\nthe multitudinous benefits of worst-case guarantees. However, algorithms designed to handle any\npossible input do not exploit special structure of the particular distribution of inputs they are used\nfor. In practice, these patterns can be described by domain experts or learned from historical data.\nFollowing the burgeoning trend of combining machine learning and classical algorithm design, [ 36]\ninitiated the study of learning-augmented frequency estimation by extending the classical CM and CS\nalgorithms in a simple but effective manner via a heavy-hitters oracle. During a training phase, they\nconstruct a classifier (e.g. a neural network) to detect whether an element iis “heavy” (e.g., whether\nfiis among the most frequent items). After such a classifier is trained, they scan the input stream,\nand apply the classifier to each element i. If the element is predicted to be heavy, it is allocated a\nunique bucket, so that an exact value of fiis computed. Otherwise, the stream element is inputted\ninto the standard sketching algorithms.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2312.07535v1  [cs.DS]  12 Dec 2023\n\nThe advantage of their algorithm was analyzed under the assumption that the true frequencies follow\na heavy-tailed Zipfian distribution. This is a common and natural reoccurring pattern in real world\ndata where there are a few very frequent elements and many infrequent elements. Experimentally,\n[36] showed several real datasets where the Zipfian assumption (approximately) held and useful\nheavy-hitter oracles could be trained in practice. Our paper is motivated by the following natural\nquestions and goals in light of prior works:\nCan we design better frequency estimation algorithms (with and without predic-\ntions) for heavy-tailed distributions?\nIn particular, we consider the setting of [ 36] where the underlying data follow a heavy-tailed distri-\nbution and investigate whether sketching algorithms can be further tailored for such distributions.\nBefore tackling this question, we must tightly characterize the benefits–and limitations–of these\nexisting methods, which is another goal of our paper:\nGive tight error guarantees for CountMin and CountSketch, as well as their\nlearning-augmented variants, on Zipfian data.\nLastly, any algorithms we design must possess worst case bounds in the case that either the data does\nnot match our Zipfian (or more generally, heavy-tailed) assumption or the learned predictions have\nhigh error, leading to the following ‘best of both worlds’ goal:\nDesign algorithms which exploit heavy tailed distributions and ML predictions but\nalso maintain worst-case guarantees.\nWe addresses these challenges and goals and our contributions can be summarized as follows:\n•We give tight upper and lower bounds for CM and CS, with and without predictions, for heavy\ntailed distributions. A surprising conclusion from our analysis is that (for a natural error metric)\na constant number of rows is optimal for both CM and CS. In addition, our theoretical analysis\nshows that CS outperforms CM, both with and without predictions, validating the experimental\nresults of [36].\n•We go beyond CM and CS based algorithms to give a better frequency estimation algorithm for\nheavy tailed distributions, with and without the use of predictions. We show that our algorithms can\ndeliver up to a logarithmic factor improvement in the error bound over CS and its learned variant.\nIn addition, our algorithm has worst case guarantees.\n•Prior learned approaches require querying an oracle for every element in the stream. In contrast,\nwe obtain a parsimonious version of our algorithm which only requires a limited number of queries\nto the oracle. The number of queries we use is approximately equal to the given space budget.\n•Lastly, we evaluate our algorithms on two real-world datasets with and without ML based predic-\ntions and show superior empirical performance compared to prior work in all cases.\n1.1 Preliminaries\nNotation and Estimation Error The stream updates an ndimensional frequency vector and\nevery stream element is of the form (i,∆)where i∈[n]and∆∈Rdenotes the update on the\ncoordinate. The final frequency vector is denoted as f∈Rn. LetN=P\ni∈[n]fidenote the sum of\nall frequencies. To simplify notation, we assume that f1≥f2≥. . .≥fn.˜fidenotes the estimate of\nthe frequency fi. Given estimates {˜fi}i∈[n], the error of a particular frequency is |˜fi−fi|. We also\nconsider the following notion of overall weighted error as done in [36]:\nWeighted Error: =1\nNX\ni∈[n]fi· |˜fi−fi|. (1)\nThe weighted error can be interpreted as measuring the error with respect to a query distribution\nwhich is the same as the actual frequency distribution. As stated in [ 36], theoretical guarantees of\nfrequency estimation algorithms are typically phrased in the traditional (ε, δ)-error formulations.\nHowever as argued in there, the simple weighted objective (1)is a more holistic measure and does\nnot require tuning of two different parameters, and is thus more natural from an ML perspective.\n2\n\nZipfian Stream We also work under the common assumption that the frequencies follow the Zipfian\nlaw, i.e., the ith largest frequency fiis equal to A/ifor some parameter A. Note we know Aat the\nend of the stream since the stream length is A·Hn. By rescaling, we may assume that A= 1without\nloss of generality. We will make this assumption throughout the paper.\nCountMin (CM) For parameters kandB, which determine the total space used, CM uses k\nindependent and uniformly random hash functions h1, . . . , h k: [n]→[B]. Letting Cbe an array of\nsize[k]×[B]we let C[ℓ, b] =P\nj∈[n][hℓ(j) =b]fj. When querying i∈[n]the algorithm returns\n˜fi= min ℓ∈[k]C[ℓ, hℓ(i)]. Note that we always have that ˜fi≥fi.\nCountSketch (CS) In CS, we again have the hash functions hias above as well as sign functions\ns1, . . . , s k: [n]→ {− 1,1}. The array Cof size [k]×[B]is now tracks C[ℓ, b] =P\nj∈[n][hℓ(j) =\nb]sℓ(j)fj. When querying i∈[n]the algorithm returns the estimate ˜fi=median ℓ∈[k]sℓ(i)·\nC[ℓ, hℓ(i)].\nLearning-Augmented Sketches [ 36]Given a base sketching algorithm (either CM or CS) and\na space budget B, the corresponding learning-augmented algorithm (learned CM or learned CS)\nallocates a constant fraction of the space Bto the base sketching algorithm and the rest of the space\nto store items identified as heavy by a learned predictor. These items predicted to be heavy-hitters are\nstored in a separate table which maintains their counts exactly, and their updates are not sent to the\nsketching algorithm.\n1.2 Summary of Main Results and Paper Outline\nOur analysis, both of CM and CS, our algorithm, and prior work, is summarized in Table 1.\nAlgorithm Weighted Error Uses Predictions? Reference\nCountMin (CM) Θ\u0010\nlogn\nB\u0011\nNo Theorem B.1\nCountSketch (CS) Θ\u00001\nB\u0001\nNo Theorem C.4\nLearned CountMin Θ\u0010\nlog(n/B)2\nBlogn\u0011\nYes [36]\nLearned CountSketch Θ\u0010\nlog(n/B)\nBlogn\u0011\nYes Theorem D.1\nOur (Without predictions) O\u0010\nlogB+poly(log log n)\nBlogn\u0011\nNo Theorem 2.1\nOur (Learned version) O\u0010\n1\nBlogn\u0011\nYes Theorem 3.1\nTable 1: Bounds are stated assuming that the total space is Bwords of memory. Weighted error\nmeans that element iis queried with probability proportional to 1/i. Moreover, the table considers\nnormalized frequencies, so that fi= 1/i.\nSummary of Theoretical Results We interpret Table 1. Bdenotes the space bound, which is the\ntotal number of entries used in the CM or CS tables. First note that CS achieves lower weighted\nerror compared to CM, proving the empirical advantage observed in [ 36]. However, the learned\nversion of CS only improves upon standard CS in the regime B=n1−o(1). While this setting does\nappear sometimes in practice [ 33,36] (referred to as high-accuracy regime), for CS, learning gives\nno asymptotic advantage in the low space regime.\nOn the other hand, in the low space regime of B=poly(logn), our algorithm, without predictions,\nalready archives close to a logarithmic factor improvement over even learned CS. Furthermore, our\nlearning-augmented algorithm achieves a logarithmic factor improvement over classical CS across\nall space regimes, whereas the learned CS only achieves a logarithmic factor improvement in the\nregime B=n1−o(1).Furthermore, our learned version outperforms or matches learned CS in all\nspace regimes.\n3\n\nOur learning-augmented algorithm can also be made parsimonious in the sense that we only query\nthe heavy-hitter oracle ˜O(B)times. This is desirable in large-scale streaming applications where\nevaluating even a small neural network on every single element would be prohibitive.\nRemark 1.1. We remark that all bounds in this paper are proved by bounding the expected error\nwhen estimating the frequency of a single item, E[|˜fi−fi|], then using linearity of expectation. While\nwe specialized our bounds to a query distribution which is proportional to the actual frequencies in\n(1), our bounds can be easily generalized to anyquery distribution by simply weighing the expected\nerrors of different items according to the given query distribution.\nSummary of Empirical Results We compare our algorithm without prediction to CS and our\nalgorithm with predictions to that of [ 36] on synthetic Zipfian data and on two real datasets corre-\nsponding to network traffic and internet search queries. In all cases, our algorithms outperform the\nbaselines and often by a significant margin (up to 17xin one setting). The improvement is especially\npronounced when the space budget is small.\nOutline of the Paper Our paper is divided into roughly two parts. One part covers novel and\ntight analysis of the classical algorithms CountMin (CM) and CountSketch (CS). The second part\ncovers our novel algorithmic contributions which go beyond CM and CS. The main body of our\npaper focuses on our novel algorithmic components, i.e. the second part, and we defer our analysis\nof the performance of CountMin (CM) and CountSketch (CS), with and without predictions, to the\nappendix: in Section B we give tight analysis of CM for a Zipfian frequency distribution. In Section\nC we give the analogous bounds for CS. Lastly, Section D gives tight bounds for CS with predictions.\nSection 2 covers our better frequency estimation without predictions while Section 3 covers the\nlearning-augmented version of the algorithm, as well as its extentions.\n1.3 Related Works\nFrequency Estimation While there exist other frequency estimation algorithms beyond CM and\nCS (such as [ 51,48,21,40,49,11] ) we study hashing based methods such as CM [ 20] and CS [ 14]\nas they are widely employed in practice and have additional benefits, such as supporting insertions\nand deletions , and have applications beyond frequency estimation, such as in machine learning\n(feature selection [3], compressed sending [13, 25], and dimensionality reduction [61, 18] etc.).\nLearning-augmented algorithms The last few years have witnessed a rapid growth in using\nmachine learning methods to improve “classical” algorithmic problems. For example, they have\nbeen used to improve the performance of data structures [ 42,52], online algorithms [ 47,56,32,\n5,60,43,1,6,4,22,34], combinatorial optimization [ 41,7,43,53,23,16], similarity search and\nclustering [ 59,24,30,54,57]. Similar to our work, sublinear constraints, such as memory or sample\ncomplexity, have also been studied under this framework [36, 38, 39, 19, 27, 28, 15, 44, 57].\n2 Improved Algorithm without Predictions\nWe first present our frequency estimation algorithm which does not use any predictions. Later, we\nbuild on top of it for our final learning-augmented frequency estimation algorithm.\nThe main guarantees of of the algorithm is the following:\nTheorem 2.1. Consider Algorithm 1 with space parameter B≥lognupdated over a Zipfian stream.\nLet{ˆfi}n\ni=1denote the estimates computed by Algorithm 2. The expected weighted error (1)is\nEh\n1\nN·Pn\ni=1fi· |fi−ˆfi|i\n=O\u0010\nlogB+poly(log log n)\nBlogn\u0011\n.\nAlgorithm and Proof intuition: LetB′=B/log log n. At a high level, we show that for\nevery i≤B′, we execute line 10 of Algorithm 2 and the error satisfies |1/i−ˆfi| ≈1/B′(recall\nin the Zipfian case, the ith largest frequency is fi= 1/i). On the other hand, for i≥B′, we\nshow that (with sufficiently high probability) line 8 of Algorithm 2 will be executed, resulting in\n|1/i−ˆfi|=|1/i−0|= 1/i.\n4\n\nAlgorithm 1 (Not augmented) Frequency update algorithm\n1:Input: Stream of updates to an ndimensional vector, space budget B\n2:procedure UPDATE\n3: T←Θ(log log n)\n4: forj= 1toT−1do\n5: Sj←CountSketch table with 3rows andB\n6Tcolumns\n6: end for\n7: ST←CountSketch table with 3rows andB\n6columns\n8: forstream element (i,∆)do\n9: Input (i,∆)in each of the TCountSketch tables Sj\n10: end for\n11:end procedure\nAlgorithm 2 (Not augmented) Frequency estimation algorithm\n1:Input: Index i∈[n]for which we want to estimate fi\n2:procedure QUERY\n3: forj= 1toT−1do\n4: ˆfj\ni←estimate of the ith frequency given by table Sj\n5: end for\n6: ˜fi←Median (ˆf1\ni, . . . , ˆfT−1\ni)\n7: if˜fi< O((log log n))/Bthen\n8: Return 0\n9: else\n10: Return ˆfT\ni, the estimate given by table ST\n11: end if\n12:end procedure\nIt might be perplexing at first sight why we wish to set the estimate to be 0, but this idea has solid\nintuition: it turns out the additive error of standard CountSketch with B′columns is actually of the\norder 1/B′. Thus, it does not make sense to estimate elements whose true frequencies are much\nsmaller than 1/B′using CountSketch. A challenge is that we do not know a priori which elements\nthese are. We circumvent this via the following reasoning: if CountSketch itself outputs ≈1/B′as\nthe estimate, then either one of the following must hold:\n•The element has frequency 1/i≪1/B′, in which case we should set the estimate to 0to obtain\nerror 1/i, as opposed to error 1/B′−1/i≈1/B′.\n•The true element has frequency ≈1/B′in which case either using the output of the CountSketch\ntable or setting the estimate to 0both obtain error approximately O(1/B′), so our choice is\ninconsequential.\nIn summary, the output of CountSketch itself suggests whether we should output an estimated\nfrequency as 0. We slightly modify the above approach with O(log log n)repetitions to obtain\nsufficiently strong concentration, leading to a robust method to identify small frequencies. The proof\nformalizes the above plan and is given in full detail in Section E.\nBy combining our algorithm with predictions, we obtain improved guarantees.\n3 Improved Learning-Augmented Algorithm\nTheorem 3.1. Consider Algorithm 3 with space parameter B≥lognupdated over a Zipfian stream.\nSuppose we have access to a heavy-hitter oracle which correctly identifies the top B/2heavy-hitters\nin the stream. Let {ˆfi}n\ni=1denote the estimates computed by Algorithm 4. The expected weighted\nerror (1)isEh\n1\nN·Pn\ni=1fi· |fi−ˆfi|i\n=O\u0010\n1\nBlogn\u0011\n.\n5\n\nAlgorithm 3 (Learning-augmented) Frequency update algorithm\n1:Input: Stream of updates to an ndimensional vector, space budget B, access to a heavy-hitter\noracle which correctly identifies the top B/2heavy-hitters\n2:procedure UPDATE\n3: T←O(log log n)\n4: forj= 1toT−1do\n5: Sj←CountSketch table with 3rows andB\n12Tcolumns\n6: end for\n7: ST←CountSketch table with 3rows andB\n12columns\n8: forstream element (i,∆)do\n9: ifiis a top B/2heavy-hitter then\n10: Maintain the frequency of iexactly\n11: else\n12: Input (i,∆)in each of the TCountSketch tables Sj\n13: end if\n14: end for\n15:end procedure\nAlgorithm 4 (Learning-augmented) Frequency estimation algorithm\n1:Input: Index i∈[n]for which we want to estimate fi\n2:procedure QUERY\n3: ifiis a top B/2heavy-hitter then\n4: Output the exact maintained frequency of i\n5: else\n6: Return ˆfi←output of Alg. 2 using the CountSkech tables created in Alg.3\n7: end if\n8:end procedure\nAlgorithm and Proof Intuition: Our final algorithm follows a similar high-level design pattern\nused in the learned CM algorithm of [ 36]: given an oracle prediction, we either store the frequency of\nheavy element directly, or input the element into our algorithm from the prior section which does not\nuse any predictions.\nThe workhorse of our analysis is the proof of Theorem 2.1. First note that we obtain 0error for\ni < B/ 2. Thus, all error comes from indices i≥B/2. Recall the intuition for this case from\nTheorem 2.1: we want to output 0as our estimates as this results in lower error than the additive error\nfrom CS. The same analysis as in the proof of Theorem 2.1 shows that we are able to detect small\nfrequencies and appropriately output an estimate from either the Tth CS table or output 0.\n3.1 Parsimonious Learning\nIn Theorem 3.1, we assumed access to a heavy-hitter oracle which we can use on every single stream\nelement to predict if it is heavy. In practical streaming applications, this will likely be infeasible.\nIndeed, even if the oracle is a small neural network, it is unlikely that we can query it for every single\nelement in a large-scale streaming application. We therefore consider the so called parsimonious\nsetting with the goal of obtaining the same error bounds on the expected error but with an algorithm\nthat makes limited queries to the heavy-hitter oracle. This setting has recently been explored for other\nproblems in the learning-augmented literature [37, 9, 26].\nOur algorithm works similarly to Algorithm 3 except that when an element (i,∆)arrives, we only\nquery the heavy-hitter oracle with some probability p(proportional to ∆). We will choose pso that\nwe in expectation only query ˜O(B)elements, rather than querying the entire stream. To be precise,\nwhenever an item arrives, we first check if it is already classified as one of the top B/2heavy-hitters\nin which case, we update its exact count (from the point in time where was classified as heavy).\nOtherwise, we query the heavy-hitter oracle with probability p. In case the item is queried and is\nindeed one of the top B/2heavy-hitters, we start an exact count of that item. An arriving item which\n6\n\nis not used as a query for the heavy-hitter oracle and was not earlier classified as a heavy-hitter is\nprocessed as in Algorithm 3.\nQuerying for an element, we first check if it is classified as a heavy-hitter and if so, we use the\nestimate from the separate lookup table. If not, we estimate its frequency using Algorithm 4. With\nthis algorithm, the count of a heavy-hitter will be underestimated since it may appear several times in\nthe stream before it is used as a query for the oracle and we start counting it exactly. However, with\nour choice of sampling probability, with high probability it will be sampled sufficiently early to not\naffect its final count too much. We present the pseudocode of the algorithm as well as the precise\nresult and its proof in Appendix G.\n3.2 Algorithm variant with worst case guarantees\nIn this section we discuss a variant of our algorithm with worst case guarantees. To be more precise,\nwe consider the case where the actual frequency distribution is not Zipfian. The algorithm we discuss\nis actually a more general case of Algorithm 2 and in fact, it completely recovers the asymptotic error\nguarantees of Theorem 2.1 (as well as Theorem 4 if we use predictions).\nRecall that Algorithm 2 outputs 0when the estimated frequency is below T/B forT=O(log log n).\nThis parameter has been tuned to the Zipfian case. As stated in Section 2, the main intuition for\nthis parameter is that it is of the same order as the additive error inherent in CountSketch, which we\ndiscuss now. Denote by fPthe frequency vector where we zero out the largest Pcoordinates. For\nevery frequency, the expected additive error incurred by a CountSketch table with B′columns is\nO(∥fB′∥2/√\nB′). In the Zipfian case, this is equal to O\u0010∥fB′∥2√\nB′\u0011\n=O\u00001\nB′\u0001\n, which is exactly the\nthreshold we set1. Thus, our robust variant simply replaces this tuned parameter O(T/B)with an\nestimate of O(∥fB′∥2/√\nB′)where B′=B/T . We given an algorithm which efficiently estimates\nthis quantity in a stream. Note this quantity is only needed for the query phase.\nLemma 3.2. With probability at least 1−exp (Ω ( B)), Algorithm 6 outputs an estimate Vsatisfying\nΩ\u0010\n∥f3B′∥2\n2/B′\u0011\n≤V≤O\u0012\r\r\rfB′/10\r\r\r2\n2/B′\u0013\n.\nThe algorithm and analysis are given in Section H. Replacing the threshold in Line 7of Algorithm\n2 with the output of Algorithm 6 (more precisely the square root of the value) readily gives us the\nfollowing worst case guarantees. Lemma 3.3 states that the expected error of the estimates outputted\nby Algorithm 2 using B, regardless of the true frequency distribution, is no worse than that of a\nstandard CountSketch table using slightly smaller O(B/log log n)space.\nLemma 3.3. Suppose B≥logn. Let{ˆfi}n\ni=1denote the estimates of Algorithm 2 using B/2space\nand with Line 7replaced by the square root of the estimate of Algorithm 6, also using B/2space.\nSuppose the condition of Lemma 3.2 holds. Let {ˆf′\ni}n\ni=1denote the estates computed by a CountSketch\ntable withcB\nlog log ncolumns for a sufficiently small constant c. Then, E[|ˆfi−fi|]≤E[|ˆf′\ni−fi|].\nRemark 3.1. The learned version of the algorithm automatically inherits any worst case guarantees\nfrom the unlearned (without predictions) version. This is because we only set aside half the space to\nexplicitly track the frequency of some elements, which has worst case guarantees, while the other half\nis used for the unlearned version, also with worst case guarantees.\n4 Experiments\nWe experimentally evaluate our algorithms with and without predictions on real and synthetic\ndatasets and demonstrate that the improvements predicted by theory hold in practice. Comprehensive\nadditional figures are given in Appendix J.\nAlgorithm Implementations In the setting without predictions, we compare our algorithm to\nCountSketch (CS) (which was shown to have favorable empirical performance compared to CountMin\n(CM) in [ 36] and better theoretical performance due to our work). In the setting with predictions, we\ncompare the algorithm of [ 36], using CS as the base sketch and dedicated half of the space for items\n1Recall B′=B/T in Algorithm 2.\n7\n\n100101102103104105106\nSorted Elements101103105FrequencyCAIDA Log-Log Frequencies\n100101102103104105\nSorted Elements100101102103FrequencyAOL Log-Log FrequenciesFigure 1: Log-log plots of the sorted frequencies of the first day/minute of the CAIDA/AOL datasets.\nBoth data distributions are heavy-tailed with few items accounting for much of the total stream.\n500 1000 1500 2000 2500 3000\nSpace2468Weighted Error1e11\n CAIDA Day #20\nCS\nCS (nonneg)\nOurs (C=1.0)\nOurs (C=2.0)\nOurs (C=5.0)\n0 10 20 30 40 50\nMinute1.52.02.53.03.54.04.5Weighted Error1e11\n Space: 750.0\nCS\nCS (nonneg)\nOurs (C=5)\nFigure 2: Comparison of weighted error without predictions on the CAIDA dataset. The left plot\ncompares the performance of various algorithms (including our algorithm with different choices\nofC) for a fixed dataset and varying space. The right plot compares algorithms over time across\nseparate streams for each minute of data for a specific choice of space being 750.\nwhich are predicted to be heavy by the learned oracle. For all implementations, we use three rows in\nthe CS table and vary the number of columns. We additionally augment each of these baselines with a\nversion that truncates all negative estimated frequencies to zero as none of our datasets include stream\ndeletions. This simple change does not change the asymptotic (ε, δ)classic sketching guarantees but\ndoes make a big difference when measuring empirical weighted error.\nWe implement a simplified and practical version of our algorithm which uses a single CS table. If the\nmedian estimate of an element is below a threshold of Cn/w for domain size n, sketch width w(a\nthird of the total space), and a tunable constant C, the estimate is instead set to 0. As all algorithms\nuse a single CS table as the basic building block with different estimation functions, for each trial we\nrandomly sample hash functions for a single CS table and only vary the estimation procedure used.\nWe evaluate algorithms according the weighted error as in Equation (1) but also according to\nunweighted error which is simply the sum over all elements of the absolute estimation error, given byP\ni|fi−˜fi|. Space is measured by the size of the sketch table, and all errors are averaged over 10\nindependent trials with standard deviations shown shaded in.\nDatasets We compare our algorithm with prior work on three datasets. We use the same two real-\nworld datasets and predictions from [ 36]: the CAIDA and AOL datasets. The CAIDA dataset [ 12]\ncontains 50 minutes of internet traffic data. For each minute of data, the stream is formed of the IP\naddresses associated with packets going through a Tier1 ISP. A typical minute of data contains 30\nmillion packets accounted for by 1 million IPs. The AOL dataset [ 55] contains 80 days of internet\nsearch queries with a typical day containing ≈3·105total queries and ≈105unique queries. As\nshown in Figure 1, both datasets approximately follow a power law distribution. For both datasets, we\nuse the predictions from prior work [ 36] formed using recurrent neural networks. We also generate\nsynthetic data following a Zipfian distribution with n= 107elements and where the ith element has\nfrequency n/i.\nResults Across the board, our algorithm outperforms the baselines. On the CAIDA and AOL\ndatasets without predictions, our algorithm consistently outperforms the standard CS with up to 4x\nsmaller error with space 300. This gap widens when we compare our algorithm with predictions\n8\n\n500 1000 1500 2000 2500 3000\nSpace0.20.40.60.81.01.21.4Weighted Error1e12\n CAIDA Day #20\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1.0)\nOurs (C=2.0)\nOurs (C=5.0)\n0 10 20 30 40 50\nMinute1234567Weighted Error1e11\n Space: 750.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=5)Figure 3: Comparison of weighted error with predictions on the CAIDA dataset.\n500 1000 1500 2000 2500 3000\nSpace0.00.51.01.52.02.53.03.5Weighted Error1e7\n AOL Day #50\nCS\nCS (nonneg)\nOurs (C=1.0)\nOurs (C=2.0)\nOurs (C=5.0)\n0 10 20 30 40 50 60 70 80\nDay0.51.01.52.02.53.0Weighted Error1e7\n Space: 750.0\nCS\nCS (nonneg)\nOurs (C=1)\nFigure 4: Comparison of weighted error without predictions on the AOL dataset.\nto that of [ 36] with a gap of up to 17xwith space 300. In all cases, the performance of CS and\n[36] is significantly improved by the simple trick of truncating negative estimates to zero. However,\nour algorithm still outperforms these “nonneg” baselines. The longitudinal plots which compare\nalgorithms over time show that our algorithm consistently outperforms the state-of-the-art with and\nwithout predictions.\nIn the case of the CAIDA dataset, predictions do not generally improve the performance of any\nof the algorithms. This is consistent with the findings of [ 36] where the prediction quality for the\nCAIDA dataset was relatively poor. However, for the AOL which has a more accurate learned oracle,\nour algorithm in particular is significantly improved when augmented with predictions. Intuitively,\nthe benefit of our algorithm comes from removing error due to noise for low frequency elements.\nConversely, good predictions help to obtain very good estimates of high frequency elements. In\ncombination, this yields very small total weighted error.\nIn Appendix J, we display comprehensive experiments of the performance of the algorithms across\nthe CAIDA and AOL datasets with varying space and for both weighted and unweighted error as\nwell as results for synthetic Zipfian data. In all cases, our algorithm outperforms the baselines. On\nsynthetic Zipfian, the gap between our algorithm and the non-negative CS for weighted error is\nrelatively small compared to that for the real datasets. While we mainly focus on weighted error in\nthis work, the benefits of our algorithm are even more significant for unweighted error as setting\nestimates below the noise floor to zero is especially impactful for this error measure. In general, we\nsee the trend, matching our theoretical results, that as space increases, the gap between the different\nalgorithms shrinks as the estimates of the base CS become more accurate.\nAcknowledgements\nWe are grateful to Piotr Indyk for insightful discussions. Anders Aamand is supported by DFF-\nInternational Postdoc Grant 0164-00022B from the Independent Research Fund Denmark and a\nSimons Investigator Award. Justin Chen is supported by an NSF Graduate Research Fellowship under\nGrant No. 174530. Huy Nguyen is supported by NSF Grants 2311649 and 1750716.\n9\n\n500 1000 1500 2000 2500 3000\nSpace0.00.51.01.52.02.53.03.5Weighted Error1e7\n AOL Day #50\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1.0)\nOurs (C=2.0)\nOurs (C=5.0)\n0 10 20 30 40 50 60 70 80\nDay0.00.51.01.52.02.53.0Weighted Error1e7\n Space: 750.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1)Figure 5: Comparison of weighted error with predictions on the AOL dataset.\nReferences\n[1]Anders Aamand, Justin Y . Chen, and Piotr Indyk. (Optimal) Online Bipartite Matching with\nDegree Information. In Advances in Neural Information Processing Systems , volume 35, 2022.\n[2]Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the\nfrequency moments. In Proceedings of the twenty-eighth annual ACM symposium on Theory of\ncomputing , pages 20–29, 1996.\n[3]Amirali Aghazadeh and Ryan Spring and Daniel LeJeune and Gautam Dasarathy and Anshumali\nShrivastava and Richard G. Baraniuk. Mission: Ultra large-scale feature selection using count-\nsketches. In International Conference on Machine Learning , pages 80–88. PMLR, 2018.\n[4]Keerti Anand, Rong Ge, and Debmalya Panigrahi. Customizing ml predictions for online\nalgorithms. In International Conference on Machine Learning , pages 303–313, 2020.\n[5]Spyros Angelopoulos, Christoph Dürr, Shendan Jin, Shahin Kamali, and Marc Renault. Online\ncomputation with untrusted advice. In 11th Innovations in Theoretical Computer Science\nConference (ITCS 2020) . Schloss Dagstuhl-Leibniz-Zentrum für Informatik, 2020.\n[6]Antonios Antoniadis, Christian Coester, Marek Eliáš, Adam Polak, and Bertrand Simon. Online\nmetric algorithms with untrusted predictions. ACM Transactions on Algorithms , 19(2):1–34,\n2023.\n[7]Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch.\nInInternational Conference on Machine Learning , pages 353–362, 2018.\n[8]George Bennett. Probability inequalities for the sum of independent random variables. Journal\nof the American Statistical Association , 57(297):33–45, 1962.\n[9]Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, and Manish Purohit. Logarithmic regret from\nsublinear hints. In Advances in Neural Information Processing Systems 34: Annual Conference\non Neural Information Processing Systems 2021 , pages 28222–28232, 2021.\n[10] Oscar Boykin, Avi Bryant, Edwin Chen, ellchow, Mike Gagnon, Moses Nakamura, Steven\nNoble, Sam Ritchie, Ashutosh Singhal, and Argyris Zymnis. Algebird. https://twitter.\ngithub.io/algebird/ , 2016.\n[11] Vladimir Braverman, Stephen R Chestnut, Nikita Ivkin, Jelani Nelson, Zhengyu Wang, and\nDavid P Woodruff. Bptree: an l2 heavy hitters algorithm using constant memory. In Proceedings\nof the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems ,\npages 361–376, 2017.\n[12] CAIDA. Caida internet traces, chicago. http://www.caida.org/data/monitors/passive-equinix-\nchicago.xml, 2016.\n[13] Emmanuel J Candès, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact\nsignal reconstruction from highly incomplete frequency information. IEEE Transactions on\ninformation theory , 52(2):489–509, 2006.\n[14] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams.\nInInternational Colloquium on Automata, Languages, and Programming , pages 693–703.\nSpringer, 2002.\n10\n\n[15] Justin Y . Chen, Talya Eden, Piotr Indyk, Honghao Lin, Shyam Narayanan, Ronitt Rubinfeld,\nSandeep Silwal, Tal Wagner, David P. Woodruff, and Michael Zhang. Triangle and four cycle\ncounting with predictions in graph streams. In 10th International Conference on Learning\nRepresentations, ICLR , 2022.\n[16] Justin Y . Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental graph\nalgorithms via learned predictions. In International Conference on Machine Learning, ICML ,\nvolume 162 of Proceedings of Machine Learning Research , pages 3583–3602, 2022.\n[17] Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the\nsum of observations. Annals of Mathematical Statistics , 23(4):493–507, 1952.\n[18] Kenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input\nsparsity time. Journal of the ACM (JACM) , 63(6):1–45, 2017.\n[19] Edith Cohen, Ofir Geri, and Rasmus Pagh. Composable sketches for functions of frequencies:\nBeyond the worst case. In Proceedings of the 37th International Conference on Machine\nLearning , 2020.\n[20] Graham Cormode and Shan Muthukrishnan. An improved data stream summary: the count-min\nsketch and its applications. Journal of Algorithms , 55(1):58–75, 2005.\n[21] Erik D Demaine, Alejandro López-Ortiz, and J Ian Munro. Frequency estimation of internet\npacket streams with limited space. In Esa, volume 2, pages 348–360. Citeseer, 2002.\n[22] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Ali Vakilian, and Nikos Zarifis. Learning\nonline algorithms with distributional advice. In International Conference on Machine Learning ,\npages 2687–2696, 2021.\n[23] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvit-\nskii. Faster matchings via learned duals. Advances in neural information processing systems ,\n34:10393–10406, 2021.\n[24] Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning sublinear-time indexing\nfor nearest neighbor search. arXiv preprint arXiv:1901.08544 , 2019.\n[25] David L Donoho. Compressed sensing. IEEE Transactions on information theory , 52(4):1289–\n1306, 2006.\n[26] Marina Drygala, Sai Ganesh Nagarajan, and Ola Svensson. Online algorithms with costly\npredictions. In Proceedings of The 26th International Conference on Artificial Intelligence\nand Statistics , volume 206 of Proceedings of Machine Learning Research , pages 8078–8101.\nPMLR, 2023.\n[27] Elbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the “learning\" into learning-\naugmented algorithms for frequency estimation. In Proceedings of the 38th International\nConference on Machine Learning , pages 2860–2869, 2021.\n[28] Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner.\nLearning-based support estimation in sublinear time. In 9th International Conference on\nLearning Representations, ICLR , 2021.\n[29] Paul Erd ˝os. On a lemma of littlewood and offord. Bulletin of the American Mathematical\nSociety , 51(12):898–902, 1945.\n[30] Jon C. Ergun, Zhili Feng, Sandeep Silwal, David P. Woodruff, and Samson Zhou. Learning-\naugmented k-means clustering. In 10th International Conference on Learning Representations,\nICLR , 2022.\n[31] Cristian Estan and George Varghese. New directions in traffic measurement and accounting:\nFocusing on the elephants, ignoring the mice. ACM Transactions on Computer Systems (TOCS) ,\n21(3):270–313, 2003.\n[32] Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert\nadvice. In Proceedings of the 36th International Conference on Machine Learning , pages\n2319–2327, 2019.\n[33] Amit Goyal, Hal Daumé III, and Graham Cormode. Sketch algorithms for estimating point\nqueries in nlp. In Proceedings of the 2012 joint conference on empirical methods in natural\nlanguage processing and computational natural language learning , pages 1093–1103, 2012.\n11\n\n[34] Anupam Gupta, Debmalya Panigrahi, Bernardo Subercaseaux, and Kevin Sun. Augmenting\nonline algorithms with ε-accurate predictions. Advances in Neural Information Processing\nSystems , 35:2115–2127, 2022.\n[35] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of\nthe American Statistical Association , 58(301):13–30, 1963.\n[36] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In 7th International Conference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019.\n[37] Sungjin Im, Ravi Kumar, Aditya Petety, and Manish Purohit. Parsimonious learning-augmented\ncaching. In International Conference on Machine Learning , pages 9588–9601. PMLR, 2022.\n[38] Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In\nAdvances in Neural Information Processing Systems , pages 7400–7410, 2019.\n[39] Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodruff. Learning-augmented\ndata stream algorithms. In International Conference on Learning Representations , 2020.\n[40] Richard M Karp, Scott Shenker, and Christos H Papadimitriou. A simple algorithm for finding\nfrequent elements in streams and bags. ACM Transactions on Database Systems (TODS) ,\n28(1):51–55, 2003.\n[41] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial\noptimization algorithms over graphs. In Advances in Neural Information Processing Systems ,\npages 6348–6358, 2017.\n[42] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data ,\npages 489–504, 2018.\n[43] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online schedul-\ning via learned weights. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on\nDiscrete Algorithms , pages 1859–1877. SIAM, 2020.\n[44] Yi Li, Honghao Lin, Simin Liu, Ali Vakilian, and David Woodruff. Learning the positions in\ncountsketch. In 11th International Conference on Learning Representations, ICLR , 2023.\n[45] John Edensor Littlewood and Albert C Offord. On the number of real roots of a random\nalgebraic equation. ii. In Mathematical Proceedings of the Cambridge Philosophical Society ,\nvolume 35, pages 133–148. Cambridge University Press, 1939.\n[46] Zaoxing Liu, Antonis Manousis, Gregory V orsanger, Vyas Sekar, and Vladimir Braverman.\nOne sketch to rule them all: Rethinking network flow monitoring with univmon. In Proceedings\nof the 2016 ACM SIGCOMM Conference , pages 101–114, 2016.\n[47] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice.\nInInternational Conference on Machine Learning , pages 3302–3311, 2018.\n[48] Gurmeet Singh Manku and Rajeev Motwani. Approximate frequency counts over data streams.\nInVLDB’02: Proceedings of the 28th International Conference on Very Large Databases , pages\n346–357. Elsevier, 2002.\n[49] Ahmed Metwally, Divyakant Agrawal, and Amr El Abbadi. Efficient computation of frequent\nand top-k elements in data streams. In International Conference on Database Theory , pages\n398–412. Springer, 2005.\n[50] Gregory T Minton and Eric Price. Improved concentration bounds for count-sketch. In\nProceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms , pages\n669–686. Society for Industrial and Applied Mathematics, 2014.\n[51] Jayadev Misra and David Gries. Finding repeated elements. Science of computer programming ,\n2(2):143–152, 1982.\n[52] Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching. In\nAdvances in Neural Information Processing Systems , pages 464–473, 2018.\n[53] Michael Mitzenmacher. Scheduling with predictions and the price of misprediction. In 11th\nInnovations in Theoretical Computer Science Conference (ITCS 2020) . Schloss Dagstuhl-\nLeibniz-Zentrum für Informatik, 2020.\n12\n\n[54] Thy Nguyen, Anamay Chaturvedi, and Huy Le Nguyen. Improved learning-augmented algo-\nrithms for k-means and k-medians clustering. 2023.\n[55] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. A picture of search. In Proceedings of the\n1st international conference on Scalable information systems , pages 1–es, 2006.\n[56] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions.\nInAdvances in Neural Information Processing Systems , pages 9661–9670, 2018.\n[57] Sandeep Silwal, Sara Ahmadian, Andrew Nystrom, Andrew McCallum, Deepak Ramachandran,\nand Seyed Mehran Kazemi. Kwikbucks: Correlation clustering with cheap-weak and expensive-\nstrong signals. In The Eleventh International Conference on Learning Representations , 2023.\n[58] Partha Talukdar and William Cohen. Scaling graph-based semi supervised learning to large\nnumber of labels using count-min sketch. In Artificial Intelligence and Statistics , pages 940–947.\nPMLR, 2014.\n[59] Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data -\na survey. Proceedings of the IEEE , 104(1):34–57, 2016.\n[60] Alexander Wei and Fred Zhang. Optimal robustness-consistency trade-offs for learning-\naugmented online algorithms. Advances in Neural Information Processing Systems , 33:8042–\n8053, 2020.\n[61] David P Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and\nTrends® in Theoretical Computer Science , 10(1–2):1–157, 2014.\n[62] Minlan Yu, Lavanya Jose, and Rui Miao. Software defined traffic measurement with opensketch.\nInNSDI , volume 13, pages 29–42, 2013.\n[63] Matei Zaharia, Reynold S Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave,\nXiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J Franklin, et al. Apache spark:\na unified engine for big data processing. Communications of the ACM , 59(11):56–65, 2016.\n13\n\nA Organization of the Appendix\nIn Section B, we give tight bounds for CM with Zipfians, as well as tight bounds for CS (in Section\nC) and its learning augmented variants (in Section D). Our results for CM and CS, with and without\npredictions, can be summarized in Table 2. We highlight the results of these sections are presented\nassuming that we use a total ofBbuckets. With khash functions, the range of each hash functions is\ntherefore [B/k]. We make this assumption since we wish to compare the expected error incurred by\nthe different sketches when the total sketch size is fixed.\nSections E and F contain the proofs of Theorems 2.1 and 3.1, respectively. Section H contains omitted\nproofs of Section 3.2.\nIn Section J, we include additional experimental results.\nNotation We use the bracket [·]notation for the indicator function. a≲bdenotes a≤Cbfor some\nfixed positive constant C.\nk= 1 k >1\nCount-Min (CM) Θ\u0010\nlogn\nB\u0011\n[36] Θ\u0010k·log(kn\nB)\nB\u0011\nLearned Count-Min (L-CM) Θ\u0010log2(n\nB)\nBlogn\u0011\n[36] Ω\u0010log2(n\nB)\nBlogn\u0011\n[36]\nCount-Sketch (CS) Θ\u0010\nlogB\nB\u0011\nΩ\u0010\nk1/2\nBlogk\u0011\nandO\u0010\nk1/2\nB\u0011\nLearned Count-Sketch (L-CS) Θ\u0010logn\nB\nBlogn\u0011\nΩ\u0010logn\nB\nBlogn\u0011\nTable 2: This table summarizes our and previously known results on the expected frequency estimation\nerror of Count-Min (CM), Count-Sketch (CS) and their learned variants (i.e., L-CM and L-CS) that\nusekfunctions and overall space k×B\nkunder Zipfian distribution. For CS, we assume that kis odd\n(so that the median of kvalues is well defined).\nB Tight Bounds for Count-Min with Zipfians\nFor both Count-Min and Count-Sketch we aim at analyzing the expected value of the variableP\ni∈[n]fi· |˜fi−fi|where fi= 1/iand˜fiis the estimate of fioutput by the relevant sketching\nalgorithm. Throughout this paper we use the following notation: For an event Ewe denote by [E]the\nrandom variable in {0,1}which is 1if and only if Eoccurs. We begin by presenting our improved\nanalysis of Count-Min with Zipfians. The main theorem is the following.\nTheorem B.1. Letn, B, k ∈Nwithk≥2andB≤n/k. Let further h1, . . . , h k: [n]→[B]\nbe independent and truly random hash functions. For i∈[n]define the random variable ˜fi=\nminℓ∈[k]\u0010P\nj∈[n][hℓ(j) =hℓ(i)]fj\u0011\n. For any i∈[n]it holds that E[|˜fi−fi|] = Θ\u0012\nlog(n\nB)\nB\u0013\n.\nReplacing BbyB/k in Theorem B.1 and using linearity of expectation we obtain the desired bound\nfor Count-Min in the upper right hand side of Table 2. The natural assumption that B≤n/k simply\nsays that the total number of buckets is upper bounded by the number of items.\nTo prove Theorem B.1 we start with the following lemma which is a special case of the theorem.\nLemma B.2. Suppose that we are in the setting of Theorem B.1 and further that2n=B. Then\nE[|˜fi−fi|] =O\u00121\nn\u0013\n.\nProof. It suffices to show the result when k= 2since adding more hash functions and corresponding\ntables only decreases the value of |˜fi−fi|. Define Zℓ=P\nj∈[n]\\{i}[hℓ(j) =hℓ(i)]fjforℓ∈[2]and\n2In particular we dispose with the assumption that B≤n/k.\n14\n\nnote that these variables are independent. For a given t≥3/nwe wish to upper bound Pr[Zℓ≥t].\nLets < t be such that t/sis an integer, and note that if Zℓ≥tthen either of the following two events\nmust hold:\nE1: There exists a j∈[n]\\ {i}withfj> sandhℓ(j) =hℓ(i).\nE2: The set {j∈[n]\\ {i}:hℓ(j) =hℓ(i)}contains at least t/selements.\nTo see this, suppose that Zℓ≥tand that E1does not hold. Then\nt≤Zℓ=X\nj∈[n]\\{i}[hℓ(j) =hℓ(i)]fj≤s|{j∈[n]\\ {i}:hℓ(j) =hℓ(i)}|,\nso it follows that E2holds. By a union bound,\nPr[Zℓ≥t]≤Pr[E1] + Pr[ E2]≤1\nns+\u0012n\nt/s\u0013\nn−t/s≤1\nns+\u0010es\nt\u0011t/s\n.\nChoosing s= Θ(t\nlog(tn))such that t/sis an integer, and using t≥3\nn, a simple calculation yields\nthatPr[Zℓ≥t] =O\u0010\nlog(tn)\ntn\u0011\n. Note that |˜fi−fi|= min( Z1, Z2). AsZ1andZ2are independent,\nPr[|˜fi−fi| ≥t] =O\u0012\u0010\nlog(tn)\ntn\u00112\u0013\n, so\nE[|˜fi−fi|] =Z∞\n0Pr[Z≥t]dt≤3\nn+O Z∞\n3/n\u0012log(tn)\ntn\u00132\ndt!\n=O\u00121\nn\u0013\n.\nWe can now prove the full statement of Theorem B.1.\nProof of Theorem B.1. We start out by proving the upper bound. Let N1= [B]\\ {i}andN2=\n[n]\\([B]∪{i}). Letb∈[k]be such thatP\nj∈N1fj·[hb(j) =hb(i)]is minimal. Note that bis itself\na random variable. We also define\nY1=X\nj∈N1fj·[hb(j) =hb(i)],andY2=X\nj∈N2fj·[hb(j) =hb(i)].\nThen,|˜fi−fi| ≤Y1+Y2. Using Lemma B.2, we obtain that E[Y1] =O(1\nB). ForY2we observe that\nE[Y2|b] =X\nj∈N2fj\nB=O \nlog\u0000n\nB\u0001\nB!\n.\nWe conclude that\nE[|˜fi−fi|]≤E[Y1] +E[Y2] =E[Y1] +E[E[Y2|b]] =O \nlog\u0000n\nB\u0001\nB!\n.\nNext we prove the lower bound. We have already seen that the main contribution to the error\ncomes from the tail of the distribution. As the tail of the distribution is relatively “flat” we can\nsimply apply a concentration inequality to argue that with probability Ω(1) , we have this asymptotic\ncontribution for each of the khash functions. To be precise, for j∈[n]andℓ∈[k]we define\nX(j)\nℓ=fj·\u0000\n[hℓ(j) =hℓ(i)]−1\nB\u0001\n. Note that the variables (X(j)\nℓ)j∈[n]are independent. We also\ndefine Sℓ=P\nj∈N2X(j)\nℓforℓ∈[k]. Observe that |X(j)\nℓ| ≤fj≤1\nBforj≥B,E[X(j)\nℓ] = 0 , and\nthat\nVar[Sℓ] =X\nj∈N2f2\nj\u00121\nB−1\nB2\u0013\n≤1\nB2.\n15\n\nApplying Bennett’s inequality(Theorem I.1 of Appendix I), with σ2=1\nB2andM= 1/Bthus gives\nthat\nPr[Sℓ≤ −t]≤exp (−h(tB)).\nDefining Wℓ=P\nj∈N2fj·[hℓ(j) =hℓ(i)]it holds that E[Wℓ] = Θ\u0012\nlog(n\nB)\nB\u0013\nandSℓ=Wℓ−\nE[Wℓ], so putting t=E[Wℓ]/2in the inequality above we obtain that\nPr[Wℓ≤E[Wℓ]/2] = Pr[ Sℓ≤ −E[Wℓ]/2]≤exp\u0010\n−h\u0010\nΩ\u0010\nlogn\nB\u0011\u0011\u0011\n.\nAppealing to Remark I.1 and using that B≤n/k the above bound becomes\nPr[Wℓ≤E[Wℓ]/2]≤exp\u0010\n−Ω\u0010\nlogn\nB·log\u0010\nlogn\nB+ 1\u0011\u0011\u0011\n= exp( −Ω(log k·log(log k+ 1))) = k−Ω(log(log k+1)). (2)\nBy the independence of the events (Wℓ> E[Wℓ]/2)ℓ∈[k], we have that\nPr\u0014\n|˜fi−fi| ≥E[Wℓ]\n2\u0015\n≥(1−k−Ω(log(log k+1)))k= Ω(1) ,\nand so E[|˜fi−fi|] = Ω( E[Wℓ]) = Ω\u0012\nlog(n\nB)\nB\u0013\n, as desired.\nRemark B.1. We have stated Theorem B.1 for truly random hash functions but it suffices with\nO(logB)-independent hashing to prove the upper bound. Indeed, the only step in which we require\nhigh independence is in the union bound in Lemma B.2 over the\u0000n\nt/s\u0001\nsubsets of [n]of size t/s. To\noptimize the bound we had to choose s=t/log(tn), so that t/s= log( tn). As we only need to\nconsider values of twitht≤Pn\ni=1fi=O(logn), in fact t/s=O(logn)in our estimates. Finally,\nwe applied Lemma B.2 with n=Bso it follows that O(logB)-independence is enough to obtain\nour upper bound.\nC (Nearly) Tight Bounds for Count-Sketch with Zipfians\nIn this section we proceed to analyze Count-Sketch for Zipfians either using a single or more hash\nfunctions. We start with two simple lemmas which for certain frequencies (fi)i∈[n]of the items\nin the stream can be used to obtain respectively good upper and lower bounds on E[|˜fi−fi|]in\nCount-Sketch with a single hash function. We will use these two lemmas both in our analysis of\nstandard and learned Count-Sketch for Zipfians.\nLemma C.1. Letw= (w1, . . . , w n)∈Rn,η1, . . . , η nBernoulli variables taking value 1with\nprobability p, and σ1, . . . , σ n∈ {− 1,1}independent Rademachers, i.e., Pr[σi= 1] = Pr[ σi=\n−1] = 1 /2. LetS=Pn\ni=1wiηiσi. Then, E[|S|] =O\u0000√p∥w∥2\u0001\n.\nProof. Using that E[σiσj] = 0 fori̸=jand Jensen’s inequality E[|S|]2≤E[S2] =\nE\u0002Pn\ni=1w2\niηi\u0003\n=p∥w∥2\n2, from which the result follows.\nLemma C.2. Suppose that we are in the setting of Lemma C.1. Let I⊂[n]and let wI∈Rnbe\ndefined by (wI)i= [i∈I]·wi. Then\nE[|S|]≥1\n2p(1−p)|I|−1∥wI∥1.\nProof. LetJ= [n]\\I,S1=P\ni∈Iwiηiσi, andS2=P\ni∈Jwiηiσi. LetEdenote the event that S1\nandS2have the same sign or S2= 0. Then Pr[E]≥1/2by symmetry. For i∈Iwe denote by Ai\nthe event that {j∈I:ηj̸= 0}={i}. Then Pr[Ai] =p(1−p)|I|−1and furthermore AiandEare\nindependent. If Ai∩Eoccurs, then |S| ≥ |wi|and as the events (Ai∩E)i∈Iare disjoint it thus\nfollows that E[|S|]≥P\ni∈IPr[Ai∩E]· |wi| ≥1\n2p(1−p)|I|−1∥wI∥1.\nWith these tools in hand, we proceed to analyse Count-Sketch for Zipfians with one and more hash\nfunctions in the next two sections.\n16\n\nC.1 One hash function\nBy the same argument as in the discussion succeeding Theorem B.1, the following theorem yields\nthe desired result for a single hash function as presented in Table 2.\nTheorem C.3. Suppose that B≤nand let h: [n]→[B]ands: [n]→ {− 1,1}be truly random\nhash functions. Define the random variable ˜fi=P\nj∈[n][h(j) =h(i)]s(j)fjfori∈[n]. Then\nE[|˜fi−s(i)fi|] = Θ\u0012logB\nB\u0013\n.\nProof. Leti∈[n]be fixed. We start by defining N1= [B]\\ {i}andN2= [n]\\([B]∪ {i})and\nnote that\n|˜fi−s(i)fi| ≤\f\f\f\f\f\fX\nj∈N1[h(j) =h(i)]s(j)fj\f\f\f\f\f\f+\f\f\f\f\f\fX\nj∈N2[h(j) =h(i)]s(j)fj\f\f\f\f\f\f:=X1+X2.\nUsing the triangle inequality E[X1]≤1\nBP\nj∈N1fj=O(logB\nB). Also, by Lemma C.1, E[X2] =\nO\u00001\nB\u0001\nand combining the two bounds we obtain the desired upper bound. For the lower bound we\napply Lemma C.2 with I=N1concluding that\nE[|˜fi−s(i)fi|]≥1\n2B\u0012\n1−1\nB\u0013|N1|−1X\ni∈N1fi= Ω\u0012logB\nB\u0013\n.\nC.2 Multiple hash functions\nLetk∈Nbe odd. For a tuple x= (x1, . . . , x k)∈Rkwe denote by median xthe median of the\nentries of x. The following theorem immediately leads to the result on CS with k≥3hash functions\nclaimed in Table 2.\nTheorem C.4. Letk≥3be odd, n≥kB, and h1, . . . , h k: [n]→[B]ands1, . . . , s k: [n]→\n{−1,1}be truly random hash functions. Define ˜fi=median ℓ∈[k]\u0010P\nj∈[n][hℓ(j) =hℓ(i)]sℓ(j)fj\u0011\nfori∈[n]. Assume that3k≤B. Then\nE[|˜fi−s(i)fi|] = Ω\u00121\nB√\nklogk\u0013\n,and E[|˜fi−s(i)fi|] =O\u00121\nB√\nk\u0013\nThe assumption n≥kBsimply says that the total number of buckets is upper bounded by the number\nof items. Again using linearity of expectation for the summation over i∈[n]and replacing Bby\nB/k we obtain the claimed upper and lower bounds of√\nk\nBlogkand√\nk\nBrespectively. We note that\neven if the bounds above are only tight up to a factor of logkthey still imply that it is asymptotically\noptimal to choose k=O(1), e.g. k= 3. To settle the correct asymptotic growth is thus of merely\ntheoretical interest.\nIn proving the upper bound in Theorem C.4, we will use the following result by Minton and Price\n(Corollary 3.2 of [50]) proved via an elegant application of the Fourier transform.\nLemma C.5 (Minton and Price [ 50]).Let{Xi:i∈[n]}be independent symmetric random variables\nsuch that Pr[Xi= 0]≥1/2for each i. LetX=Pn\ni=1Xiandσ2=E[X2] = Var[ X]. For ε <1\nit holds that Pr[|X|< εσ] = Ω( ε)\nProof of Theorem C.4. IfB(and hence k) is a constant, then the results follow easily\nfrom Lemma C.1, so in what follows we may assume that Bis larger than a sufficiently large\nconstant. We subdivide the exposition into the proofs of the upper and lower bounds.\n3This very mild assumption can probably be removed at the cost of a more technical proof. In our proof it\ncan even be replaced by k≤B2−εfor any ε= Ω(1) .\n17\n\nUpper bound Define N1= [B]\\ {i}andN2= [n]\\([B]∪ {i}). Let for ℓ∈[k],X(ℓ)\n1=P\nj∈N1[hℓ(j) =hℓ(i)]sℓ(j)fjandX(ℓ)\n2=P\nj∈N2[hℓ(j) =hℓ(i)]sℓ(j)fjand let X(ℓ)=X(ℓ)\n1+\nX(ℓ)\n2.\nAs the absolute error in Count-Sketch with one pair of hash functions (h, s)is always upper bounded\nby the corresponding error in Count-Min with the single hash function h, we can use the bound\nin the proof of Lemma B.2 to conclude that Pr[|X(ℓ)\n1| ≥t] =O(log(tB)\ntB), when t≥3/B. Also\nVar[X(ℓ)\n2] = (1\nB−1\nB2)P\nj∈N2f2\nj≤1\nB2, so by Bennett’s inequality (Theorem I.1) with M= 1/B\nandσ2= 1/B2and Remark I.1,\nPr[|X(ℓ)\n2| ≥t]≤2 exp ( −h(tB))≤2 exp\u0012\n−1\n2tBlog (tB+ 1)\u0013\n=O\u0012log(tB)\ntB\u0013\n,\nfort≥3\nB. It follows that for t≥3/B,\nPr[|X(ℓ)| ≥2t]≤Pr[(|X(ℓ)\n1| ≥t)] + Pr( |X(ℓ)\n2| ≥t)] =O\u0012log(tB)\ntB\u0013\n.\nLetCbe the implicit constant in the O-notation above. If |˜fi−s(i)fi| ≥2t, at least half of the\nvalues (|X(ℓ)|)ℓ∈[k]are at least 2t. Fort≥3/Bit thus follows by a union bound that\nPr[|˜fi−s(i)fi| ≥2t]≤2\u0012k\n⌈k/2⌉\u0013\u0012\nClog(tB)\ntB\u0013⌈k/2⌉\n≤2\u0012\n4Clog(tB)\ntB\u0013⌈k/2⌉\n. (3)\nIfα=O(1)is chosen sufficiently large it thus holds that\nZ∞\nα/BPr[|˜fi−s(i)fi| ≥t]dt= 2Z∞\nα/(2B)Pr[|˜fi−s(i)fi| ≥2t]dt\n≤4\nBZ∞\nα/2\u0012\n4Clog(t)\nt\u0013⌈k/2⌉\ndt\n≤1\nB2k≤1\nB√\nk.\nHere the first inequality uses Equation (3) and a change of variable. The second inequality uses that\u0010\n4Clogt\nt\u0011⌈k/2⌉\n≤(C′/t)2k/5for some constant C′followed by a calculation of the integral. Now,\nE[|˜fi−s(i)fi|] =Z∞\n0Pr[|˜fi−s(i)fi| ≥t]dt,\nso for our upper bound it therefore suffices to show thatRα/B\n0Pr[|˜fi−s(i)fi| ≥t]dt=O\u0010\n1\nB√\nk\u0011\n.\nFor this we need the following claim:\nClaim C.6. LetI⊂Rbe the closed interval centered at the origin of length 2t, i.e., I= [−t, t].\nSuppose that 0< t≤1\n2B. For ℓ∈[k],Pr[X(ℓ)∈I] = Ω( tB).\nProof. Note that Pr[X(ℓ)\n1= 0]≥Pr[V\nj∈N1(hℓ(j)̸=hℓ(i))] = (1 −1\nB)N1= Ω(1) . Secondly\nVar[X(ℓ)\n2] = (1\nB−1\nB2)P\nj∈N2f2\nj≤1\nB2. Using that X(ℓ)\n1andX(ℓ)\n2are independent and Lemma C.5\nwithσ2= Var[ X(ℓ)\n2], it follows that Pr[X(ℓ)∈I] = Ω\u0010\nPr[X(ℓ)\n2∈I]\u0011\n= Ω(tB).\nLet us now show how to use the claim to establish the desired upper bound. For this let 0< t≤1\n2B\nbe fixed. If |˜fi−s(i)fi| ≥t, at least half of the values (X(ℓ))ℓ∈[k]are at least tor at most −t.\nLet us focus on bounding the probability that at least half are at least t, the other bound being\nsymmetric giving an extra factor of 2in the probability bound. By symmetry and Claim C.6,\nPr[X(ℓ)≥t] =1\n2−Ω(tB). For ℓ∈[k]we define Yℓ= [X(ℓ)≥t], and we put S=P\nℓ∈[k]Yℓ.\n18\n\nThenE[S] =k\u00001\n2−Ω(tB)\u0001\n. If at least half of the values (X(ℓ))ℓ∈[k]are at least tthenS≥k/2.\nBy Hoeffding’s inequality (Theorem I.3) we can bound the probability of this event by\nPr[S≥k/2] = Pr[ S−E[S] = Ω( ktB)] = exp( −Ω(kt2B2)).\nIt follows that Pr[|˜fi−s(i)fi| ≥t]≤2 exp(−Ω(kt2B2)). Thus\nZα/B\n0Pr[|˜fi−s(i)fi| ≥t]dt≤Z 1\n2B\n02 exp(−Ω(kt2B2))dt+Zα/B\n1\n2B2 exp(−Ω(k))dt\n≤1\nB√\nkZ√\nk/2\n0exp(−t2)dt+2αexp(−Ω(k))\nB=O\u00121\nB√\nk\u0013\n.\nHere the second inequality used a change of variable. The proof of the upper bound is complete.\nLower Bound Fixℓ∈[k]and let M1= [Blogk]\\ {i}andM2= [n]\\([Blogk]∪ {i}). Write\nS:=X\nj∈M1[hℓ(j) =hℓ(i)]sℓ(j)fj+X\nj∈M2[hℓ(j) =hℓ(i)]sℓ(j)fj:=S1+S2.\nWe also define J:={j∈M1:hℓ(j) =hℓ(i)}. LetI⊆Rbe the closed interval around sℓ(i)fiof\nlength1\nB√\nklogk. We now upper bound the probability that S∈Iconditioned on the value of S2. To\nease the notation, the conditioning on S2has been left out in the notation to follow. Note first that\nPr[S∈I] =|M1|X\nr=0Pr[S∈I| |J|=r]·Pr[|J|=r]. (4)\nFor a given r≥1we now proceed to bound Pr[S∈I| |J|=r]. This probability is the same as\nthe probability that S2+P\nj∈Rσjfj∈I, where R⊆M1is a uniformly random r-subset and the\nσj’s are independent Rademachers. Suppose that we sample the elements from Ras well as the\ncorresponding signs (σi)i∈Rsequentially, and let us condition on the values and signs of the first\nr−1sampled elements. At this point at mostBlogk√\nk+ 1possible samples for the last element in R\ncan cause that S∈I. Indeed, the minimum distance between distinct elements of {fj:j∈M1}is\nat least 1/(Blogk)2and furthermore Ihas length1\nB√\nklogk. Thus, at most\n1\nB√\nklogk·(Blogk)2+ 1 =Blogk√\nk+ 1\nchoices for the last element of Rensure that S∈I. For 1≤r≤(Blogk)/2we can thus upper\nbound\nPr[S∈I| |J|=r]≤Blogk√\nk+ 1\n|M1| −r+ 1≤2√\nk+2\nBlogk≤3√\nk.\nNote that µ:=E[|J|]≤logkso for B≥6, it holds that\nPr[|J| ≥(Blogk)/2]≤Pr\u0014\n|J| ≥µB\n2\u0015\n≤Pr\u0014\n|J| ≥µ\u0012\n1 +B\n3\u0013\u0015\n≤exp (−µh(B/3)) = k−Ω(h(B/3)),\nwhere the last inequality follows from the Chernoff bound of Theorem I.2. Thus, if we assume\nthatBis larger than a sufficiently large constant, then Pr[|J| ≥Blogk/2]≤k−1. Finally,\nPr[|J|= 0] = (1 −1/B)Blogk≤k−1. Combining the above, we can continue the bound in (4)as\nfollows.\nPr[S∈I]≤Pr[|J|= 0] +(Blogk)/2X\nr=1Pr[S∈I| |J|=r]·Pr[|J|=r]\n+|M1|X\nr=(Blogk)/2+1Pr[|J|=r] =O\u00121√\nk\u0013\n, (5)\n19\n\nwhich holds even after removing the conditioning on S2. We now show that with probability\nΩ(1) at least half the values (X(ℓ))ℓ∈[k]are at least1\n2B√\nklogk. Let p0be the probability that\nX(ℓ)≥1\n2B√\nklogk. This probability does not depend on ℓ∈[k]and by symmetry and (5),p0=\n1/2−O(1/√\nk). Define the function f:{0, . . . , k } →Rby\nf(t) =\u0012k\nt\u0013\npt\n0(1−p0)k−t.\nThen f(t)is the probability that exactly tof the values (X(ℓ))ℓ∈[k]are at least1\nB√\nklogk. Using that\np0= 1/2−O(1/√\nk), a simple application of Stirling’s formula gives that f(t) = Θ\u0010\n1√\nk\u0011\nfor\nt=⌈k/2⌉, . . . ,⌈k/2 +√\nk⌉when kis larger than some constant C. It follows that with probability\nΩ(1) at least half of the (X(ℓ))ℓ∈[k]are at least1\nB√\nklogkand in particular\nE[|˜fi−fi|] = Ω\u00121\nB√\nklogk\u0013\n.\nFinally we handle the case where k≤C. It follows from simple calculations (e.g., using Lemma C.2)\nthatX(ℓ)= Ω(1 /B)with probability Ω(1) . Thus this happens for all ℓ∈[k]with probability Ω(1)\nand in particular E[|˜fi−fi|] = Ω(1 /B), which is the desired for constant k.\nD Learned Count-Sketch for Zipfians\nWe now proceed to analyze the learned Count-Sketch algorithm. In Appendix D.1 we estimate the\nexpected error when using a single hash function and in Appendix D.2 we show that the expected\nerror only increases when using more hash functions. Recall that we assume that the number of\nbuckets Bhused to store the heavy hitters that Bh= Θ( B−Bh) = Θ( B).\nD.1 One hash function\nBy taking B1=Bh= Θ( B)andB2=B−Bh= Θ( B)in the theorem below, the result on L-CS\nfork= 1claimed in Table 2 follows immediately.\nTheorem D.1. Leth: [n]\\[B1]→[B2]ands: [n]→ {− 1,1}be truly random hash functions\nwhere n, B 1, B2∈Nand4n−B1≥B2≥B1. Define the random variable ˜fi=Pn\nj=B1+1[h(j) =\nh(i)]s(j)fjfori∈[n]\\[B1]. Then\nE[|˜fi−s(i)fi|] = Θ \nlogB2+B1\nB1\nB2!\nProof. LetN1= [B1+B2]\\([B1]∪{i})andN2= [n]\\([B1+B2]∪{i}). LetX1=P\nj∈N1[h(j) =\nh(i)]s(j)fjandX2=P\nj∈N2[h(j) =h(i)]s(j)fj. By the triangle inequality and linearity of\nexpectation,\nE[|X1|] =O \nlogB2+B1\nB1\nB2!\n.\nMoreover, it follows directly from Lemma C.1 that E[|X2|] =O\u0010\n1\nB2\u0011\n. Thus\nE[|˜fi−s(i)fi|]≤E[|X1|] +E[|X2|] =O \nlogB2+B1\nB1\nB2!\n,\n4The first inequality is the standard assumption that we have at least as many items as buckets. The second\ninequality says that we use at least as many buckets for non-heavy items as for heavy items (which doesn’t\nchange the asymptotic space usage).\n20\n\nas desired. For the lower bound on Eh\f\f\f˜fi−s(i)fi\f\f\fi\nwe apply Lemma C.2 with I=N1to obtain\nthat,\nEh\f\f\f˜fi−s(i)fi\f\f\fi\n≥1\n2B2\u0012\n1−1\nB2\u0013|N1|−1X\ni∈N1fi= Ω \nlogB2+B1\nB1\nB2!\n.\nCorollary D.2. Leth: [n]\\[Bh]→[B−Bh]ands: [n]→ {− 1,1}be truly random hash functions\nwhere n, B, B h∈NandBh= Θ( B)≤B/2. Define the random variable ˜fi=Pn\nj=Bh+1[h(j) =\nh(i)]s(j)fjfori∈[n]\\[Bh]. Then E[|˜fi−s(i)fi|] = Θ(1 /B).\nRemark D.1. The upper bounds of Theorem D.1 and Corollary D.2 hold even without the assumption\nof fully random hashing. In fact, we only require that handsare2-independent. Indeed Lemma C.1\nholds even when the Rademachers are 2-independent (the proof is the same). Moreover, we need hto\nbe2-independent as we condition on h(i)in our application of Lemma C.1. With 2-independence the\nvariables [h(j) =h(i)]forj̸=iare then Bernoulli variables taking value 1with probability 1/B2.\nD.2 More hash functions\nWe now show that, like for Count-Sketch, using more hash functions does not decrease the expected\nerror. We first state the Littlewood-Offord lemma as strengthened by Erd ˝os.\nTheorem D.3 (Littlewood-Offord [ 45], Erd ˝os [29]).Leta1, . . . , a n∈Rwith|ai| ≥1fori∈[n].\nLet further σ1, . . . , σ n∈ {− 1,1}be random variables with Pr[σi= 1] = Pr[ σi=−1] = 1 /2and\ndefine S=Pn\ni=1σiai. For any v∈Rit holds that Pr[|S−v| ≤1] =O(1/√n).\nSetting B1=Bh= Θ( B)andB2=B−B2= Θ( B)in the theorem below gives the final bound\nfrom Table 2 on L-CS with k≥3.\nTheorem D.4. Letn≥B1+B2≥2B1,k≥3odd, and h1, . . . , h k: [n]\\[B1]→[B2/k]and\ns1, . . . , s k: [n]\\[B1]→ {− 1,1}be independent and truly random. Define the random variable\n˜fi=median ℓ∈[k]\u0010P\nj∈[n]\\[B1][hℓ(j) =hℓ(i)]sℓ(j)fj\u0011\nfori∈[n]\\[B1]. Then\nE[|˜fi−s(i)fi|] = Ω\u00121\nB2\u0013\n.\nProof. Like in the proof of the lower bound of Theorem C.4 it suffices to show that for each i\nthe probability that the sum Sℓ:=P\nj∈[n]\\([B1]∪{i})[hℓ(j) =hℓ(i)]sℓ(j)fjlies in the interval\nI= [−1/(2B2),1/(2B2)]isO(1/√\nk). Then at least half the (Sℓ)ℓ∈[k]are at least 1/(2B2)with\nprobability Ω(1) by an application of Stirling’s formula, and it follows that E[|˜fi−s(i)fi|] =\nΩ(1/B2).\nLetℓ∈[k]be fixed, N1= [2B2]\\([B2]∪ {i}), and N2= [n]\\(N1∪ {i}), and write\nSℓ=X\nj∈N1[hℓ(j) =hℓ(i)]sℓ(j)fj+X\nj∈N2[hℓ(j) =hℓ(i)]sℓ(j)fj:=X1+X2.\nNow condition on the value of X2. Letting J={j∈N1:hℓ(j) =hℓ(i)}it follows by Theorem D.3\nthat\nPr[Sℓ∈I|X2] =O\nX\nJ′⊆N1Pr[J=J′]p\n|J′|+ 1\n=O\u0010\nPr[|J|< k/2] + 1 /√\nk\u0011\n.\nAn application of Chebyshev’s inequality gives that Pr[|J|< k/ 2] = O(1/k), soPr[Sℓ∈I] =\nO(1/√\nk). Since this bound holds for any possible value of X2we may remove the conditioning and\nthe desired result follows.\nRemark D.2. The bound above is probably only tight for B1= Θ( B2). Indeed, we know that it\ncannot be tight for all B1≤B2since when B1becomes very small, the bound from the standard\nCount-Sketch with k≥3takes over — and this is certainly worse than the bound in the theorem.\nIt is an interesting open problem (that requires a better anti-concentration inequality than the\nLittlewood-Offord lemma) to settle the correct bound when B1≪B2.\n21\n\nE Proof of Theorem 2.1\nIn this section we give the complete proof of Theorem 2.1. We need the following special case of a\nresult about the behaviour of CountSketch, proved in the prior sections.\nTheorem E.1 (Theorem C.4) .Letˆfibe the estimate of the ith frequency given by a 3×B/3\nCountSketch table. There exists a universal constant Csuch that the following two inequalities hold:\nPr\u0012\n|fi−ˆfj\ni| ≥C\nB\u0013\n≤1\n2, (6)\n∀t≥3/B, Pr\u0010\n|fi−ˆfj\ni| ≥t\u0011\n≤C\u0012log(tB)\ntB\u00132\n. (7)\nProof of Theorem 2.1. Case 1: i > B/ log log n. Recall that ˆfj\nidenotes the estimate of the ith\nfrequency given by table Sjin Algorithm 2. Furthermore, ˜fi←Median (ˆf1\ni, . . . , ˆfT−1\ni)denotes the\nmedian of the estimates of the first T−1tables in Algorithm 2. From Theorem E.1, we have that for\nevery fixed j,\nPr\u0012\n|fi−ˆfj\ni| ≥2Clog log n\nB\u0013\n≤1\n4\nand so it follows that\nPr\u0012\n|fi−˜fi| ≥2Clog log n\nB\u0013\n≤exp(−Ω(T))≤1\n(logn)100(8)\nby adjusting the constant in front of T. We let 2Cbe the constant for the Onotation in line 7of\nAlgorithm 2. Now consider the expected value of |ˆfi−1/i|, where the expectation is taken over the\nrandomness used by the CountSketch tables of Algorithm 2. By conditioning on the event that we\neither output 0or output the estimate of the Tth table, we have\nE\u00141\ni·\f\f\f\fˆfi−1\ni\f\f\f\f\u0015\n≲Pr(We output 0)·1\ni2+ Pr( We output estimate of table T)·1\niB\nwhere we have used the first inequality in Theorem E.1 in the above inequality and ≲denotes\ninequality up to a constant factor. We have bounded the second probability in Equation (8)which\ngives\nE\u00141\ni·\f\f\f\fˆfi−1\ni\f\f\f\f\u0015\n≲1\ni2+1\niB·(logn)99. (9)\nCase 2: i≤B/(log log n)4. We employ the more refined tail bound for Count Sketch stated in the\nsecond inequality of Theorem E.1.\nFor any ismaller than B/(log log n)4, we have that for any fixed j,\nPr\u0012\nˆfj\ni≤2Clog log n\nB\u0013\n≤Pr\u0012\n|fi−ˆfj\ni| ≥1\n2i\u0013\n≲\u0012log(B′/i)·i\nB′\u00132\nwhere B′=B/(4T) = Θ( B/log log n). It follows that\nPr\u0012\n˜fi≤2Clog log n\nB\u0013\n≤T·Pr\u0012\n|fi−ˆf1\ni| ≤2Clog log n\nB\u0013\n≲T·\u0012log(B′/i)·i\nB′\u00132\n.\nTherefore, for i≤B/(log log n)4, we again have\nE\u00141\ni·\f\f\f\fˆfi−1\ni\f\f\f\f\u0015\n≲Pr(We output 0)·1\ni2+ Pr( We output estimate of table T)·1\niB\n≲(log log n)3·\u0012log(B/i)\nB\u00132\n+1\niB.\n22\n\nWe can summarize this case as:\nE\u00141\ni·\f\f\f\fˆfi−1\ni\f\f\f\f\u0015\n≲(log log n)3·\u0012log(B/i)\nB\u00132\n+1\niB. (10)\nPutting everything together. Equation (9) gives us\n1\nlogn·X\ni>B/ log log nE\u00141\ni·\f\f\f\fˆfi−1\ni\f\f\f\f\u0015\n≲1\nlognX\ni>B/ log log n1\ni2+1\nB·(logn)100nX\ni=11\ni\n≲log log n\nBlogn.\nEquation (10) gives us\n1\nlogn·X\ni≤B/(log log n)4E\u00141\ni·\f\f\f\fˆfi−1\ni\f\f\f\f\u0015\n≲1\nlognX\ni≤B/(log log n)4 \n(log log n)3·\u0012log(B/i)\nB\u00132\n+1\niB!\n≲(log log n)3\nB2lognZB/(log log n)4\n1log(B/x)2dx+logB\nBlogn\n≲(log log n)3\nB2logn·B(log2(log log n))\n(log log n)4+logB\nBlogn\n≲logB\nBlogn\nwhere the second to last inequality follows from the indefinte integralR\nlog2(c/x)dx=xlog2(c/x)+\n2xlog(c/x) + 2x+Constant.\nFinally, we deal with the remaining case: ibetween B/log log nandB/(log log n)4. For these i’s,\nthe worst case error happens when we set their estimates to 0, incurring error 1/i, as opposed to\nincurring error O(1/B)if we used the estimate of table T:\n1\nlogn·X\nB/(log log n)4≤i≤B/log log nE\u00141\ni·\f\f\f\fˆfi−1\ni\f\f\f\f\u0015\n≲1\nlognX\nB/(log log n)4≤i≤B/log log n1\ni2\n≲(log log n)4\nBlogn.\nCombining our three estimates completes the proof.\nF Proof of Theorem 3.1\nProof of Theorem 3.1. We summarize the intuition and give the full proof. Recall the workhorse of\nour analysis is the proof of Theorem 2.1. First note that we obtain 0error for i < B/ 2. Thus, all our\nerror comes from indices i≥B/2. Recall the intuition for this case from the proof of Theorem 2.1:\nwe want to output 0as our estimates. Now the same analysis as in Case 1of Theorem 2.1 gives us\nthat the probability we use the estimate of table Tcan be bounded by say1\n(logn)100. Thus, similar to\nEquation (9), we have\nE\u00141\ni·\f\f\f\fˆfi−1\ni\f\f\f\f\u0015\n≲Pr(We output 0)·1\ni2+ Pr( We output estimate of table T)·1\niB\n≲1\ni2+1\niB·(logn)99.\n23\n\nThus, our total error consists of only one part of the total error calculation of Theorem 2.1:\n1\nlogn·X\ni>BE\u00141\ni·\f\f\f\fˆfi−1\ni\f\f\f\f\u0015\n≲1\nlognX\ni>B1\ni2+1\nB·(logn)100nX\ni=11\ni\n≲1\nBlogn,\nas desired.\nAlgorithm 5 Parsimonious frequency update algorithm\n1:Input: Stream of updates to an ndimensional vector, space budget B, access to a heavy hitter\noracle which correctly identifies the top B/2heavy hitters.\n2:procedure UPDATE\n3: T←O(log log n)\n4: forj= 1toT−1do\n5: Sj←CountSketch table with 4rows andB\n16Tcolumns\n6: end for\n7: ST←CountSketch table with 4rows andB\n16columns\n8: forstream element (i,∆)do\n9: ifiis already classified as a top B/2heavy hitter then\n10: Maintain the count of iexactly (from the point of time it was detected as heavy).\n11: else\n12: Query the heavy hitter oracle with probability p= min\u0000\n1, CB(logn)2∆\u0001\n13: ifigets queried and is classified as a top B/2heavy hitter then\n14: Maintain the count of iexactly (from this point of time).\n15: else\n16: Input (i,∆)in each of the TCountSketch tables Sj\n17: end if\n18: end if\n19: end for\n20:end procedure\nG Parsimonious learning\nIn this appendix, we state our result on parsimonious learning precisely. We consider the modification\nto Algorithm 3 where whenever an element (i,∆)arrives, we only query the heavy hitter oracle with\nprobability p= min\u0000\n1, γB(logn)2∆\u0001\nforγa sufficiently large constant5. To be precise, when an\nitemiarrives, we first check if it is already classified as a top B/2heavy hitter. If so, we update\nits exact count (from the first point of time where it was classified as heavy). If not, we query the\nheavy hitter oracle with probability p. In case igets queried and classified as one of the top B/2\nheavy hitters, we store its count exactly (from this point of time). Otherwise, we input it to the\nCountSketch tables Sjsimilarly to Algorithm 1 and Algorithm 3. Algorithm 5 shows the pseudocode\nfor the update procedure of our parsimonious learning algorithm. The query procedure is similar\nto Algorithm 4. We now state our main result on our parsimonious learning algorithm, namely that it\nachieves the same expected weighted error bound as in Theorem 3.1.\nTheorem G.1. Consider Algorithm 5 with space parameter B≥lognupdated over a Zipfian\nstream. Suppose the heavy-hitter oracle correctly identifies the top B/2heavy hitters in the stream.\nLet{ˆfi}n\ni=1denote the estimates computed by Algorithm 4. The expected weighted error (1)is\nEh\n1\nN·Pn\ni=1fi· |fi−ˆfi|i\n=O\u0010\n1\nBlogn\u0011\n.The algorithm makes O(B(logn)3)queries to the heavy\nhitter oracle in expectation.\n5This sampling probability depends on the length of the stream which is likely unknown to us. We will\ndiscuss how this assumption can be removed shortly.\n24\n\nProof of Theorem G.1. Introducing some notation, we denote the stream ((x1,∆1), . . . , (xm,∆m)).\nLetting Si={j∈[m]|xj=i}, we then have thatP\nj∈Si∆j=fj= 1/j. Then, whenever\nan element (xj,∆j)arrives, the algorithm queries the heavy hitter oracle with probability pj=\nmin\u0000\n1, CγB (logn)2∆j\u0001\n.\nLet us first consider the expected error when estimating the frequency of a heavy hitter i≤B/2. Let\nj0∈Sibe minimal such thatP\nj∈Si,j≤j0∆j≥1\nBlogn. Since iis a heavy hitter with total frequency\nfi≥2/B, such a j0exists. If there exists j∈Siwithj≤j0such that pj= 1, then iwill be\nclassified as a heavy hitter by time j0with probability 1. Otherwise, the probability that iis not\nclassified as a heavy hitter by time j0is upper bounded by\nY\nj∈Si,j≤j0(1−pj)≤exp\n−X\nj∈Si,j≤j0pj\n= exp\n−γB(logn)2X\nj∈Si,j≤j0∆j\n\n≤exp(−γlogn) =n−γ.\nUnion bounding over the B/2top heavy hitters we find that with high probability in nthey are indeed\nclassified as heavy at the first point of time where they have appeared with weight at least1\nBlogn. In\nparticular, with the same high probability the error when estimating each of the top B/2heavy hitters\nis at most1\nBlognand so,\nE\n1\nN·B/2X\ni=1fi· |fi−ˆfi|\n=O\u00121\nBlogn\u0013\n.\nLet us now consider the light elements i > B/ 2. Such an element is never classified as heavy and\nconsequently is estimated using the CountSketch tables Sjas in Algorithm 2. Denoting by Ethe\nevent that we output 0(that is, the median of the first T−1CountSketch tables is small enough) and\nbyEcthe event that we output the estimate from table T, as in Appendix F, we again have\nE\u00141\ni·\f\f\f\fˆfi−1\ni\f\f\f\f\u0015\n≲Pr(E)·1\ni2+ Pr( Ec)·1\niB≤1\ni2+ Pr( Ec)·1\niB.\nHere, the bound of O(1/B)on the expected error of table Tholds even though the B/2heavy hitters\nmight appear in table T. The reason is with high probability, these heavy hitters appear with weight at\nmost1\nBlognand conditioned on this event, we can plug into Lemma C.1 to get that the expected error\nis still O(1/B). It remains to bound Pr(Ec). Again, from Lemma C.1, it follows that the expected\nerror of each of the first T−1tables is at most C2 log log n\nBfor a sufficiently large constant C(even\nincluding the contribution from the heavy hitters), and so by Markov’s inequality,\nPr\u0012\n|fi−ˆfj\ni| ≥2Clog log n\nB\u0013\n≤1\n4\nand again,\nPr\u0012\n|fi−˜fi| ≥2Clog log n\nB\u0013\n≤exp(−Ω(T))≤1\n(logn)100.\nThus, we can bound,\nE\u00141\ni·\f\f\f\fˆfi−1\ni\f\f\f\f\u0015\n≲1\ni2+·1\n(logn)100iB.\nRecalling that N=Hnand summing over i≥B/2we get that\nE\n1\nN·nX\ni=B/2+1fi· |fi−ˆfi|\n=O\u00121\nBlogn+log(n/B)\nB(logn)100\u0013\n=O\u00121\nBlogn\u0013\n,\nas desired. The expected number of queries to the heavy hitter oracle is\nmX\nj=1pj≤nX\ni=1X\nj∈SiγB(logn)2∆j=nX\ni=1γB(logn)2fi=O(B(logn)3).\n25\n\nRemark G.1. We note that Algorithm 5 makes use of the length of the stream to set p. Usually we\nwould not know the length of the stream but at the cost of an extra log-factor in the number of queries\nmade to the oracle, we can remedy this. Indeed, the query probability is p= min\u0010\n1,γB(logn)3\nm\u0011\nwhere mis the length of the stream. If we instead increase the query probability after we have seen j\nstream elements to pj= min\u0010\n1,γB(logn)3\nj\u0011\n, we obtain the same bound on the expected weighted\nerror. Indeed, we will only detect the heavy hitters earlier. Moreover, the expected number of queries\nto the oracle is at mostmX\nj=1γB(logn)3\nj=O\u0000\nB(logn)3logm\u0001\n.\nH Omitted Proofs of Section 3.2\nIn this section, we discuss a version of our algorithm using a worst case estimate of the tail of the\ndistribution, generalizing the value O(AT/B )designed for Zipfian distributions. The algorithm\nBasic-Tail-Sketch is essentially the classic AMS sketch [ 2] with c=O(1)counters for the\nelements whose hash value is 1. It is easy to see that the final algorithm, Algorithm 6 uses O(B)\nwords of space.\nAlgorithm 6 Estimating the tail of the frequency vector f\n1:Input: Stream of updates to an ndimensional vector f, space budget O(B)\n2:procedure TAIL-ESTIMATOR\n3: Initialize Bindependent copies of Basic-Tail-Sketch\n4: Update each copy of Basic-Tail-Sketch with updates from the stream\n5: for1≤i≤Bdo\n6: Vi←value outputted by ith copy of Basic-Tail-Sketch after stream ends\n7: end for\n8: Return V←theB/3-th largest value among {Vi}B\ni=1\n9:end procedure\nAlgorithm 7 Auxilliary algorithm for Algorithm 6\n1:Input: Stream of updates to an ndimensional vector f\n2:procedure BASIC -TAIL-SKETCH\n3: T←Θ(log log n)\n4: B′←Θ(B/T)\n5: h: [n]→[B′](4-wise independent hash function)\n6: c←32\n7: for1≤j≤cdo\n8: sj: [n]→ {± 1}(4-wise independent hash function)\n9: end for\n10: Keep track of the sum1\ncPc\nj=1\u0010P\ni:h(i)=1fisj(i)\u00112\n11:end procedure\nWe now show that V, the output of Algorithm 6, satisfies V≈ ∥fΘ(B′)∥2\n2/B′, which is of the same\norder as the threshold value used in line 7of Algorithm 4, generalizing the Zipfian case.\nProof of Lemma 3.2. We analyze one copy of the sketch V1, starting with the upper bound.\nLetabe the number of elements i∈[B′/10]such that h(i) = 1 . Because E[a] = 1/10, by Markov’s\ninequality, we have a≤9/10with probability at least 8/9. Next, let Wj=P\ni>B′/10:h(i)=1fisj(i).\nWe have\n26\n\nE[W2\nj] =X\ni≥B′/10f2\ni·[h(i) = 1] =\r\r\rfB′/10\r\r\r2\n2/B′\nBy Markov’s inequality, we have1\n4P\njW2\nj≤9\r\r\rfB′/10\r\r\r2\n2/B′with probability 8/9. By the union\nbound, V2\n1≤9\r\r\rfB′/10\r\r\r2\n2/Bwith probability at least 7/9.\nNext, we show the lower bound. Let X1=P\ni:h(i)=1min\u0000\nf2\ni, f2\n3B′\u0001\nandY1=P\ni:h(i)=1f2\ni.\nObserve that X1≤Y1. We have\nE\nh[X1] =∥f3B′∥2\n2/B′+ 3f2\n3B′\nV ar(X1) =B′−1\nB′2 X\ni>3B′f4\ni+ 3Bf4\n3B′!\nBy Chebyshev’s inequality,\nPrh\nX1≤ ∥f3B′∥2\n2/(3B′)i\n≤B′−1\nB′2\u0000P\ni>3B′f4\ni+ 3B′f4\n3B′\u0001\n\u0010\n2∥f3B′∥2\n2/(3B′) + 3f2\n3B′\u00112\n≤(B′−1)\u0000P\ni>3Bf4\ni+ 3B′f4\n3B′\u0001\n4∥f3B′∥4\n2/9 + 9 B′2f2\n3B′+ 4B′∥f3B′∥2\n2f2\n3B′\n≤1\n3.\nThus, with probability at least 2/3, we have Y1≥ ∥f3B′∥2\n2/(3B′). Next we can bound V1in terms\nofY1using the standard analysis of the AMS sketch. Let Zj=P\ni:h(i)=1fisj(i).\nE\ns\u0002\nZ2\nj|h\u0003\n=Y1\nE\ns\u0002\nZ4\nj|h\u0003\n=X\ni:h(i)=1f4\ni+ 6X\ni<j:h(i)=h(j)=1f2\nif2\nj= 3Y2\n1−2X\ni:h(i)=1f4\ni.\nBy the Chebyshev’s inequality, Pr [V1≤Y1/2]≤2Y2\n1/c\nY2\n1/4≤8\nc≤1\n4. By the union bound, we have\nV1≥ ∥f3B∥2\n2/(6B′)with probability at least 5/12.\nThe lemma follows from applying the Chernoff bound to the independent copies V1, . . . , V B.\nGiven the estimator V, we can output 0for elements whose squared estimated frequency is below V.\nLemma H.1. LetEbe the event that Vis accurate, which holds with probability 1−exp (Ω ( B)). If\nf2\ni≤ ∥f3B′∥2\n2/(12B′)then with probability 1−exp (Ω ( B))−1/polylog (n), the algorithm outputs\n0. Iff2\ni≥ ∥f3B′∥2\n2/(12B′)then with constant probability,\n\r\r\rf2\ni−ˆf2\ni\r\r\r≤O\u0012\r\r\rfΩ(B′)\r\r\r2\n2/B′\u0013\nProof. Observe that the error in the comparison between the threshold Vand˜fiis bounded by\nVplus the estimation error of ˜fi. By the standard analysis of the CountSketch, with probability\n1−exp (Ω ( T)),\f\f\ff2\ni−˜f2\ni\f\f\f≤O\u0012\r\r\rfΩ(B′)\r\r\r2\n2/B′\u0013\n27\n\nThus, if f2\ni≤ ∥f3B′∥2\n2/(12B′)then with probability 1−exp (Ω ( B))−1/polylog (n), we have\nV≥˜fiand the algorithm outputs 0.\nOn the other hand, consider f2\ni≥ ∥f3B′∥2\n2/(12B′). First, consider the case when the algorithm\noutputs 0. Except for a failure probability of exp (Ω ( B)) + 1 /polylog (n), it must be the case that\nf2\ni=O\u0010\n∥f3B′∥2\n2/(12B′)\u0011\nso we have |f2\ni−0|=O\u0010\n∥f3B′∥2\n2/(12B′)\u0011\n. Next, consider the case\nwhen the algorithm outputs the answer from ST. The correctness guarantee of this case follows from\nthe standard analysis of CountSketch, which guarantees that for a single row of CountSketch with B\ncolumns, with constant probability,\f\f\ff2\ni−˜f2\ni\f\f\f≤O\u0012\r\r\rfΩ(B)\r\r\r2\n2/B\u0013\n.\nProof of Lemma 3.3. Note that we are assuming Lemma 3.2 is satisfied, which happens with\nprobability 1−1/poly(n). For elements with true frequencies less than O(∥fB′∥2/√\nB′)for\nB′=O(B/log log n), we either we either use the last CS table in Algorithm 2 or we set the\nestimate to be 0. In either case, the inequality holds as O(∥fB′∥2/√\nB′)is the expected error of a\nstandard 1×B′CS table.\nFor elements with frequency larger than O(∥fB′∥2/√\nB′), we ideally want to use the last CS table in\nAlgorithm 2. In such a case, we easily satisfy the desired inequality since we are using a CS table\nwith even more columns. But there is a small probability we output 0. We can easily handle this\nas follows. Let fi=ℓ∥fB′∥2/√\nB′be the frequency of element iforℓ≥Cfor a large constant C.\nAny fixed CS table with B′columns gives us expected error ∥fB′∥2/√\nB′, so the probability that it\nestimates the frequency of fito be smaller than ∥fB′∥2/√\nB′is at most 1/Ω(ℓ)by a straightforward\napplication of Markov’s inequality. Since we take the median across Θ(log log n)different CS tables\nin Algorithm 2, a standard Chernoff bound implies that the probability the median estimate is smaller\nthanO(|fB′∥2/√\nB′)is at most (1/ℓ)Ω(log log n). In particular, the expected error of our estimate is\nat most ≪\u0010\nℓ∥fB′∥2/√\nB′\u0011\n·1/ℓ=O(∥fB′∥2/√\nB′), which can be upper bounded by the expected\nerror of CS table with cB/log log ncolumns for a sufficiently small c, completing the proof.\nI Concentration bounds\nIn this appendix we collect some concentration inequalities for reference in the main body of the\npaper. The inequality we will use the most is Bennett’s inequality. However, we remark that for\nour applications, several other variance based concentration result would suffice, e.g., Bernstein’s\ninequality.\nTheorem I.1 (Bennett’s inequality [ 8]).LetX1, . . . , X nbe independent, mean zero random variables.\nLetS=Pn\ni=1Xi, and σ2, M > 0be such that Var[S]≤σ2and|Xi| ≤Mfor all i∈[n]. For any\nt≥0,\nPr[S≥t]≤exp\u0012\n−σ2\nM2h\u0012tM\nσ2\u0013\u0013\n,\nwhere h:R≥0→R≥0is defined by h(x) = (x+ 1) log( x+ 1)−x. The same tail bound holds on\nthe probability Pr[S≤ −t].\nRemark I.1. Forx≥0,1\n2xlog(x+1)≤h(x)≤xlog(x+1). We will use these asymptotic bounds\nrepeatedly in this paper.\nA corollary of Bennett’s inequality is the classic Chernoff bounds.\nTheorem I.2 (Chernoff [ 17]).LetX1, . . . , X n∈[0,1]be independent random variables and\nS=Pn\ni=1Xi. Letµ=E[S]. Then\nPr[S≥(1 +δ)µ]≤exp(−µh(δ)).\nEven weaker than Chernoff’s inequality is Hoeffding’s inequality.\nTheorem I.3 (Hoeffding [ 35]).LetX1, . . . , X n∈[0,1]be independent random variables. Let\nS=Pn\ni=1Xi. Then\nPr[S−E[S]≥t]≤e−2t2\nn.\n28\n\nJ Additional Experiments\nIn this section, we display figures for synthetic Zipfian data and additional figures for the CAIDA and\nAOL datasets.\n500 1000 1500 2000 2500 3000\nSpace012345Weighted Error1e11\n Zipfian Data (n=1000000)\nCS\nCS (nonneg)\nOurs (C=1.0)\nOurs (C=2.0)\nOurs (C=5.0)\n500 1000 1500 2000 2500 3000\nSpace0.00.51.01.52.02.53.0Unweighted Error1e10\n Zipfian Data (n=1000000)\nCS\nCS (nonneg)\nOurs (C=1.0)\nOurs (C=2.0)\nOurs (C=5.0)\nFigure 6: Comparison of weighted and unweighted error without predictions on Zipfian data.\n0 10 20 30 40 50\nMinute0.20.40.60.81.01.21.4Weighted Error1e12\n Space: 150.0\nCS\nCS (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute23456789Weighted Error1e11\n Space: 300.0\nCS\nCS (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute1.52.02.53.03.54.04.5Weighted Error1e11\n Space: 750.0\nCS\nCS (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute1.01.52.02.53.0Weighted Error1e11\n Space: 1500.0\nCS\nCS (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute0.81.01.21.41.6Weighted Error1e11\n Space: 3000.0\nCS\nCS (nonneg)\nOurs (C=5)\nFigure 7: Comparison of weighted errors without predictions on the CAIDA dataset\n29\n\n0 10 20 30 40 50\nMinute0.51.01.52.0Weighted Error1e12\n Space: 150.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute0.20.40.60.81.01.2Weighted Error1e12\n Space: 300.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute1234567Weighted Error1e11\n Space: 750.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute1.01.52.02.53.03.54.0Weighted Error1e11\n Space: 1500.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute0.751.001.251.501.752.002.25Weighted Error1e11\n Space: 3000.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=5)Figure 8: Comparison of weighted errors with predictions on the CAIDA dataset\n30\n\n0 10 20 30 40 50\nMinute01234Unweighted Error1e10\n Space: 150.0\nCS\nCS (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute0.00.51.01.52.02.53.0Unweighted Error1e10\n Space: 300.0\nCS\nCS (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute0.20.40.60.81.01.21.41.6Unweighted Error1e10\n Space: 750.0\nCS\nCS (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute0.20.40.60.81.0Unweighted Error1e10\n Space: 1500.0\nCS\nCS (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute23456Unweighted Error1e9\n Space: 3000.0\nCS\nCS (nonneg)\nOurs (C=5)Figure 9: Comparison of unweighted errors without predictions on the CAIDA dataset\n31\n\n0 10 20 30 40 50\nMinute0246Unweighted Error1e10\n Space: 150.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute01234Unweighted Error1e10\n Space: 300.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute0.00.51.01.52.02.5Unweighted Error1e10\n Space: 750.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute0.20.40.60.81.01.21.4Unweighted Error1e10\n Space: 1500.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=5)\n0 10 20 30 40 50\nMinute2468Unweighted Error1e9\n Space: 3000.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=5)Figure 10: Comparison of unweighted errors with predictions on the CAIDA dataset\n32\n\n0 10 20 30 40 50 60 70 80\nDay0.20.40.60.81.0Weighted Error1e8\n Space: 150.0\nCS\nCS (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay12345Weighted Error1e7\n Space: 300.0\nCS\nCS (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.51.01.52.02.53.0Weighted Error1e7\n Space: 750.0\nCS\nCS (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.250.500.751.001.251.501.75Weighted Error1e7\n Space: 1500.0\nCS\nCS (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.20.40.60.81.01.2Weighted Error1e7\n Space: 3000.0\nCS\nCS (nonneg)\nOurs (C=1)Figure 11: Comparison of weighted errors without predictions on the AOL dataset\n33\n\n0 10 20 30 40 50 60 70 80\nDay02468Weighted Error1e7\n Space: 150.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay012345Weighted Error1e7\n Space: 300.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.00.51.01.52.02.53.0Weighted Error1e7\n Space: 750.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.000.250.500.751.001.251.501.75Weighted Error1e7\n Space: 1500.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.20.40.60.81.01.2Weighted Error1e7\n Space: 3000.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1)Figure 12: Comparison of weighted errors with predictions on the AOL dataset\n34\n\n0 10 20 30 40 50 60 70 80\nDay012345Unweighted Error1e7\n Space: 150.0\nCS\nCS (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.00.51.01.52.02.53.0Unweighted Error1e7\n Space: 300.0\nCS\nCS (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.000.250.500.751.001.251.50Unweighted Error1e7\n Space: 750.0\nCS\nCS (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.00.20.40.60.81.0Unweighted Error1e7\n Space: 1500.0\nCS\nCS (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0123456Unweighted Error1e6\n Space: 3000.0\nCS\nCS (nonneg)\nOurs (C=1)Figure 13: Comparison of unweighted errors without predictions on the AOL dataset\n35\n\n0 10 20 30 40 50 60 70 80\nDay012345Unweighted Error1e7\n Space: 150.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.00.51.01.52.02.53.0Unweighted Error1e7\n Space: 300.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.000.250.500.751.001.251.501.75Unweighted Error1e7\n Space: 750.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay0.00.20.40.60.81.0Unweighted Error1e7\n Space: 1500.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1)\n0 10 20 30 40 50 60 70 80\nDay01234567Unweighted Error1e6\n Space: 3000.0\nHsu et al.\nHsu et al. (nonneg)\nOurs (C=1)Figure 14: Comparison of unweighted errors with predictions on the AOL dataset\n36",
  "textLength": 86557
}