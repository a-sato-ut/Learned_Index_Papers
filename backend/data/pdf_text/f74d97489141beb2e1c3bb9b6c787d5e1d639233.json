{
  "paperId": "f74d97489141beb2e1c3bb9b6c787d5e1d639233",
  "title": "SuRF: Identification of Interesting Data Regions with Surrogate Models",
  "pdfPath": "f74d97489141beb2e1c3bb9b6c787d5e1d639233.pdf",
  "text": " \n \n \n \n \n \nSavva, F. , Anagnostopoulos, C.  and Triantafillou, P.  (2020) SuRF: Identification of \nInteresting Data Regions with Surrogate M odels. In: 36th IEEE International Conference \non Data Engineering (IEEE ICDE), Dallas, TX, USA, 20 -24 April 2020, pp. 1321 -1332. \nISBN 9781728129037  (doi: 10.1109/ICDE48307.2020.00118 ) \n \nThere may be differences between this v ersion and the publish ed version. You are \nadvised to consult the  publisher’s version if  you wish to cite from it.     \nhttp://eprints.gla.ac.uk/ 209812 / \n          \n \nDeposit ed on 11 February 2020  \n \n \nEnlighten – Research publications by members of the University of Glasgow  \nhttp://eprints.gla.ac.uk   \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\nSuRF: Identiﬁcation of Interesting Data Regions\nwith Surrogate Models\nFotis Savva\nUniversity of Glasgow, UK\nf.savva.1@research.gla.ac.ukChristos Anagnostopoulos\nUniversity of Glasgow, UK\nchristos.anagnostopoulos@glasgow.ac.ukPeter Triantaﬁllou\nUniversity of Warwick, UK\np.triantaﬁllou@warwick.ac.uk\nAbstract —Several data mining tasks focus on repeatedly in-\nspecting multidimensional data regions summarized by a statistic.\nThe value of this statistic (e.g., region-population sizes, order\nmoments) is used to classify the region’s interesting-ness. These\nregions can be naively extracted from the entire dataspace –\nhowever, this is extremely time-consuming and compute-resource\ndemanding. This paper studies the reverse problem: analysts\nprovide a cut-off value for a statistic of interest and in turn\nour proposed framework efﬁciently identiﬁes multidimensional\nregions whose statistic exceeds (or is below) the given cut-off\nvalue (according to user’s needs). However, as data dimensions\nand size increase, such task inevitably becomes laborious and\ncostly. To alleviate this cost, our solution, coined SuRF (SUrrogate\nRegion Finder), leverages historical region evaluations to train\nsurrogate models that learn to approximate the distribution\nof the statistic of interest. It then makes use of evolutionary\nmulti-modal optimization to effectively and efﬁciently identify\nregions of interest regardless of data size and dimensionality.\nThe accuracy, efﬁciency, and scalability of our approach are\ndemonstrated with experiments using synthetic and real-world\ndatasets and compared with other methods.\nIndex Terms —Surrogate model estimation, statistical learning,\nswarm intelligence, evolutionary multimodal optimization.\nI. I NTRODUCTION\nConsider Exploratory Data Analysis (EDA) whereby an-\nalysts engage in repeatedly selecting regions in their data\nand subsequently summarizing them by extracting statistics\n[15]. For instance, analyzing spatial data one might ﬁlter\nout all data points except the ones of a speciﬁc district\nand then measure the number of data points within that\nregion to infer the interesting-ness of it. Multiple meth-\nods/algorithms/visualizations implicitly adopt this process and\nare part of an analyst’s toolbox. A problem with this approach\nis that the task of mining regions of interest is a tedious and\nlaborious process and in the worst case has exponential com-\nplexity. The interesting-ness of a region can also be measured\nby comparing its extracted statistic with a given threshold by\nthe analysts. Regions whose statistics are greater/less than a\ngiven threshold are deemed more interesting.\nThis new analytics task (query) studied here represents\na natural and intuitive way for identifying interesting data\nregions primarily because the associated threshold is often\nknown implicitly/explicitly. For instance, when the task is to\nidentify geographic regions which have an index less/more\nthan a national average or for tasks in which readings beyond\na threshold are deemed harmful (eg pollution levels or innuclear reactors). The same task is useful, for instance, in\ncluster analysis [26] when deciding which clusters to prune,\nin detecting regions of interest in fMRI scans [25] (where\nonly the regions that are ‘activated’ are shown), when trying\nto identify landmarks [29] based on tracking data, etc. For\nthese reasons, we believe mining regions that exceed a statistic\nthreshold is in many cases more appropriate than having the\nanalyst ask for the top\u0000kregions of interest.\nA. Use Case Examples\nLet 2-dimensional spatial coordinates describe the locations\nof Crime Incidents (or any data points with spatial dimensions\nlike trafﬁc congestion and pollution levels in urban areas, etc.).\nProactively identifying regions which contain a pre-deﬁned\nnumber of data points within them can advise analysts as to\nwhich areas are worth looking into. For instance, a region\nhaving more crime incidents than a global threshold or having\nhigher average deprivation/crime-index indicator could suggest\nlack of infrastructure, policing or social/economic disparities\ncompared to other regions. Identifying such regions is non-\ntrivial – we will show that a naive approach has exponential\ncomplexity.\nUse cases are not restricted to density of data points within\nregions. Consider, for example, data from activity trackers.\nAnalysts may wish to ﬁnd time-frames (regions/ranges in\ntime) with high ratio of a speciﬁc activity (e.g., sitting,\nstanding, cardio, etc). These constitute crucial information\nabout the activity patterns of a user. If other attributes are also\nincorporated, (e.g., GPS coordinates, geo-spatial readings from\naccelerometers, etc.) the analysts can learn when & where an\nactivity occurs most often, along with what type of readings\nindicate the activity is taking place. Note that these regions of\ninterest demarcate boundaries in multidimensional space – this\nis of high value to analysts as it makes them easy to interpret.\nAs a use case in high dimensional data spaces, consider\nMachine Learning (ML) classiﬁcation, where analysts are\ninterested in ﬁnding regions with a high ratio of certain\nclasses, which implicitly suggest classiﬁcation boundaries.\nThis task cannot be performed visually unless dimensionality\nreduction is employed, which does not guarantee ﬁne-grained\nand accurate results and may suggest regions which are no\nlonger interpretable.\n\nB. Contributions\nWe study a new analytics task (query) whereby data regions\nof interest are identiﬁed, given a threshold of a statistic of\nregions. We contribute a learning & optimization methodology\nto efﬁciently and accurately process such peranalyst queries.\nWe formulate this, as an optimization problem which can be\nof multimodal nature (as multiple regions matching the analyst\nrequest can exist). We identify the back-end data/analytics\nsystem as being a bottleneck in examining the validity of the\nproposed regions. To alleviate this key problem, we propose\nthe use of ML models to learn from past evaluations and\napproximate the behavior of the back-end system, i.e., to ﬁnd\nsurrogate models that replace the back-end data system for this\ntask. We then use these models in an evolutionary multimodal\noptimization for identifying the regions of interest per analyst\nrequest. Concretely, the paper provides the following technical\ncontributions:\n\u000fWe formalize the task of mining interesting regions based\non statistics given a cut-off value and provide objective\nfunctions for optimization.\n\u000fWe propose the use of multimodal multiple-swarm opti-\nmization algorithm to locate multiple regions of interest\n\u000fWe adopt statistical learning for approximating the back-\nend system via past function evaluations. Both ML-driven\napproximation and evolutionary optimization alleviate the\ninherent complexity of the considered task.\n\u000fFinally, we provide extensive experimental results eval-\nuating and comparing SuRF and the various algorithmic\nstrategies with other methods.\nThe rest of the paper is organized as follows: Section\nII formalizes the problem of ﬁnding interesting regions and\ndescribes a baseline algorithm. Section III deﬁnes the op-\ntimization problem and introduces evolutionary multimodal\noptimization for solving it. Section IV describes the type\nof surrogate model needed to approximate the behavior of\nthe back-end analytics system. Finally Section V contains a\ncomprehensive list of experiments and results that assess the\naccuracy and efﬁciency of SuRF.\nII. P ROBLEM DEFINITION & R ATIONALE\nDeﬁnition 1: (Data Vector ) Let a= (a1;:::;a d)>2Rd\ndenote a multivariate random data vector. A dataset Bis a\ncollection of Ndata vectorsfakgN\nk=1.\nDeﬁnition 2: (Statistic Region) We deﬁne a statistic region\nin ad-dimensional vector space via the (2d+ 1)-dimensional\ninformation vector q= [x;l;y]>, where x= [x1;:::;x d]>2\nRdis the region center point of the hyper-rectangle with side\nlengths l= [l1;:::;l d]>2Rd\n+across theddimensions.\nA statistic region qover datasetBis associated with the\nsubsetD \u0012 B encompassing vectors asuch thatfa2\nDjVd\ni=1(xi\u0000li\u0014ai\u0014xi+li)g. The component y=f(x;l)\ndenotes a statistical mapping f:Rd\u0002Rd\n+7!RoverD\nfrom the hyper-rectangle [x;l]to a statistic of interest y2R,\ni.e., scalaryis the statistic extracted from the data vectors\ninD. This can be (not limited to) e.g., number of vectors inD, i.e.,y=f(x;l) =jDj, or the average \u0016aiof dimension\nai, i.e.,y=f(x;l;i) =1\njDjPjDj\nk=1ai;k,ak2 D . Note\nthat in the case of the average \u0016aithei-th dimension is not\npart of the deﬁned hyper-rectangle and the deﬁnition becomes\nfa2DjV\nj\u0000i2d(xj\u0000lj\u0014aj\u0014xj+lj)g.\nDeﬁnition 3: (Surrogate Model) Given a region q, the cor-\nresponding mapping freturns a local statistic applied to data\nvectors inD.fdepends on the conditional data distribution\np(ajx;l)deﬁned by the hyper-rectangle [x;l]. In addition, f\ncan be considered as an aggregate function that summarizes\nthe data vectors contained in D. There is no restriction to\nthe nature of fas it can be decomposable ( COUNT ,SUM)\nor non-decomposable ( MEDIAN ). The actual evaluation of f\nis computationally expensive as the complete data subset D\nhas to be identiﬁed, given region qout of all data points.\nTherefore, we rest on a surrogate model ^fto approximate\nf, i.e.,f\u0019^fgiven any random q. The surrogate model ^f\nmust be inexpensive to evaluate and has to approximate the\ntrue function fwith high accuracy. More details of how ^fis\nobtained are given at Section IV.\nImagine a data set Bthat holds a number of recorded\nincidents, as in our example of Crime Incidents in Section I-A.\nA random data vector arepresents a recorded incident in 2D\nspatial coordinates. A statistic region deﬁned by qholds a sub-\nsetDof the recorded incidents and is summarized by statistic\ny, which for our purpose, represents the number of recorded\nincidents within the given region. Hence, a surrogate model\nis a function ^fthat approximates the true underlying function\nand produces this statistic for different regions. Through our\nformulation we will refer to this example for clarity.\nProblem 1: Given a user requested threshold yR2R, seek\nthekunknown regions fqkgover the vectorial space of B\nsuch that their statistics fykgare less (or greater) than yR.\nThat is, ﬁnd the kunknown regionsfqkgdeﬁned by [xk;lk]:\nfqk2R2d+1:yk=f(xk;lk)<y R;8kg: (1)\nFor instance, given a threshold yR= 600 we seek to ﬁnd\nan arbitrary number of regions enclosing more/less than 600\nincidents. Note: we adopt (yk> y R)in the case where the\nsought statistics are all greater than yR. The regions are deﬁned\nas hyper-rectangles as it is an established method [5], [12],\n[16], [28] of portraying which regions are interesting in an\ninterpretable manner.\nTo avoid the inherent computationally heavy task of eval-\nuating all possible (not trivially countable) sub-regions that\nsatisfy (1), we approximate a solution to Problem 1 using\nsurrogate models f^fgover a data setB. Evidently, this\napproach introduces approximation of the evaluation of (1)\nbyreplacingf(xk;lk)with ^f(xk;lk). We also, deﬁne an\nobjective function that helps us ﬁnd multiple regions by ﬁnding\nlocal -optima. In the optimization function, we incorporate\nregion size deﬁned by l, to penalize large regions, as an\narbitrarily large region might not be informative enough. For\ninstance, if we seek regions with a number of incidents larger\nthanyRwithyR<jBj. Then a region covering all data vectors\n\n(all recorded incidents) is the optimal result. By factoring\nin the region size we allow the analyst to choose to focus\ntheir attention on larger/smaller areas. The objective function\nis deﬁned as:\nJ(x;l) =yR\u0000f(x;l)\u0010Qd\ni=1li\u0011c: (2)\nEq. (2) indicates that the result of the objective is inversely\nproportional to a region’s size. A single global optimal solution\nmaximizing the objective at Eq. (2) would be an inﬁnitesimal\nbox surrounding a single point with the greatest difference\ngiven byyR\u0000f(x;l). Indeed this would be a valid solution and\nmight be of interest to the analyst. However, as we will later\nshow there could be multiple local optimal solutions meeting\nthe constraints (introduced at (3)) and maximizing (2). Hence,\nwe are not interested in ﬁnding one global solution to the given\nobjective, but many. This is also motivated by the analysts need\nas they would be interested in identifying all regions that are\nabove/less than a threshold to compare underlying causes or\nperform further analysis. To allow the user some ﬂexibility in\nterms of region sizes, we introduce a tuning scalar parameter\nc, which allows the user to restrict solutions to smaller/larger\nareas. Hence, we seek the region(s):\n[x\u0003;l\u0003] = arg max\n[x;l]2R2dJ(x;l)s.t.f(x;l)<y R: (3)\nIn the case f(x;l)> y R, we maximize\u0000J(x;l). In the\nremainder we use (3) without loss of generality. To avoid\ncomputational burden we take the logarithm of (2) obtaining:\nJ(x;l) = log(J(x;l)) = log(yR\u0000f(x;l))\u0000ck\u0018k1; (4)\nwhere \u0018= [log(l1);:::; log(ld)]>andk\u0018k1=Pd\ni=1log(li)\nis theL1norm of the log-vector of l= [l1;:::;l d]>.\nAn interesting property arises from (4) as the logarithm is\nundeﬁned for negative values. Thus, the objective implicitly\nrejects regions in which yR\u0000f(x;l)<0conforming to the\nconstraint of ﬁnding regions less than yR(and vice versa for\nf(x;l)> y R), as will be shown in our experiments. In (4),\nc >0is theL1regularization parameter limiting the size of\n\u0018(and of l) coefﬁcients and results in ﬁnding ﬁne-grained\nregions (in size), as discussed later.\nA. Baseline Complexity\nBefore elaborating on our computationally efﬁcient approx-\nimate solution of Problem 1, we ﬁrst report on a baseline\nsolution. The computational complexity of mining the k\nregions in (1) grows exponentially with data dimensionality\ndand sizeN. It is not trivial to ﬁnd exact solutions given\ncontinuous data domains of (if not all) different dimensions\ninB. Given continuous (real-valued) attributes xi, one way\nof solving Problem 1 is to perform an exhaustive search.\nInitially, we could discretize the data using a ﬁnite number\nof multidimensional center points to obtain several nregions\nfx1\u0016x2:::\u0016xng;xi2Rd(\u0016denotes the point-\nwise inequality between values of the same dimension). This\ndiscretization yields an approximate solution, as the optimalcenter for a region could lie in-between the proposed centers.\nIn addition, the arbitrary size of the regions adds another level\nof complexity to the exhaustive search as we have to consider\nnregions with varying sizes across dimensions, such that\nfl1\u0016l2;:::\u0016lmg, which again is an approximate size of the\noptimal region. Thus, to obtain potential regions via exhaustive\nsearch yields asymptotic complexity of O((n\u0002m)d). We\nthen have to evaluate the result for each of the obtained\nregions using (4). Since the objective in (4) entails the eval-\nuation offoverD\u0012B , the baseline complexity becomes\nO((n\u0002m)d\u0002N), assuming that fcan be computed in a single\npass overBin linear time. As dimensions dand data vectors\nNgrow, the task becomes prohibitively costly. Hence, we now\nshow how to leverage evolutionary multi-modal optimization\nalgorithms and surrogate models to reduce the complexity for\nthe mining task at hand.\nIII. O PTIMIZATION & V IABLE SOLUTIONS\nGiven that the baseline complexity of solving this task\nbecomes exponential we seek alternatives. Our task is to\nmaximize the objective in (4) in an efﬁcient manner. We ﬁrst\ndiscuss the form of the objective which will help us identify\ncandidate optimization algorithms.\nThe solution space of the objective in (4) might have a\nunique (optimum) or multiple solutions (local optima) given an\narbitraryyR. Based on Problem 1, given a yR, the probability\nof ﬁnding a viable region is\nPff(x;l)>y Rg= 1\u0000FY(yR); (5)\nwhereFYis the cumulative distribution function (CDF) of y.\nSince, limyR!+1FY(yR) = 1 , it indicates that the objective\nfunction will have less viable solutions because Pff(x;l)>\nyRg!0, i.e., the probability of a viable solution diminishes.\nIn the case f(x;l)< y R, we obtain limyR!\u00001Pff(x;l)<\nyRg= lim yR!\u00001FY(yR) = 0 . Hence, with an appropriate\nyR, i.e., strictly non-zero probability (5), we expect to ﬁnd\nmultiple regions (local optimal) satisfying (1), i.e., k\u00151\nregions. It is highly plausible that given an appropriate yR,\nmultiple regions kexist satisfying f(xk;lk)> y R. Therefore\nwe make use of a multimodal optimization algorithm capable\nof ﬁnding all the possible solutions for Problem 1.\nA. Multimodal Evolutionary Optimization\nDue to the multimodal nature of Problem 1, we cannot\nadopt optimization methods which return a single optimal so-\nlution (=region) given yR. Therefore, we cast our optimization\nproblem as an evolutionary multimodal optimization problem\n[19] adopting methodologies from Swarm Intelligence. We\nadopt the Glowworm Swarm Optimization (GSO), which\nis a multimodal variant of the well-known Particle Swarm\nOptimization (PSO) method [17]. Both GSO and PSO methods\nare computationally light providing near-optimal solutions\n(regions in our context) in the face of non-differentiable ﬁtness\nobjective functions. Notably, GSO optimizes multimodal ﬁt-\nness functions as it converges towards multiple local-optima,\nthus considered a good candidate optimizer for our problem.\n\nGSO makes use of particles , which are represented as\nmultidimensional candidate solutions in the solution space.\nThe particles move around the solution space and eventu-\nally converge to local-optima. A candidate solution particle\np= [x;l]2R2drefers to a region deﬁned by [x;l]in the (2d)-\ndimensional solution space. The ﬁtness objective function that\nwe use for GSO is the objective Jin (4), which encapsulates\nthe function f. However, given an arbitrary yR, our method\navoids the evaluation of fover all the possible viable solutions.\nThe ﬁtness function of GSO becomes the objective ^Jderived\nfrom (4) by replacing fwith the estimate ^f. Hence, the\nsolutions are evaluated using ^Jgiven ^f.\nIn short, GSO initializes a number of particles fpigat\nrandom positions in R2d. Each particle piis associated with\naluciferin value`iemulating glowworms. The glowworms\nhave an adaptive neighborhood that helps them identify their\nneighbours and move towards other particles that have higher\nluciferin values. Because their movements are based only on\nlocal interactions and in small neighbourhoods it allows the\nswarm of glowworms to partition into disjoint groups and\nconverge to multiple local optima. The GSO algorithm is\nexecuted iteratively with discrete steps t=f1;2;:::gand is\nsplit into two phases. The ﬁrst phase updates the luciferin `i(t)\nat steptfor each particle pi= [xi;li]in the swarm using:\n`i(t) = (1\u0000\u001a)`i(t\u00001) +\r^J(xi;li) (6)\nThe factor\u001ain (6) is the luciferin decay, which reduces\nattraction to particles that are not moving towards local-\noptima. The factor \rin (6) is the luciferin enhancement and\nincreases attraction of particles close to local-optima dictated\nby the current evaluation of ^J. The second phase updates the\n(position) vector piof each particle w.r.t to a neighbourhood of\nparticlesNi(t) =fpj:kpi\u0000pjk2\u0014ri(t)^`j(t)>`i(t)gin\nwhich the selected neighbours have higher luciferin values and\nare within a current radius ri(t)inL2(Euclidean) distance.\nGSO then adapts the (position) vector pitowards a neighbour\npj2Ni(t)with the maximum selection probability:\nPfpjg=`j(t)\u0000`i(t)P\nk2Ni(t)`k(t)\u0000`i(t)(7)\nFig. 1 illustrates the ﬁnal (converged) positions of the particles\nover a 2-dim. region space. The x-axis denotes the center of\nregionxand the y-axis denotes the side length l. Hence each\nparticle is a region deﬁned over this space, with the intensity of\nthe color at Figure 1 being the value of the objective function\n(4) across the space. The ﬁnal positions are illustrated as red\n“x” and the slightly shaded blue dots are previous positions\nheld by those particles. In this example, 84% of the particles\nhave converged to regions satisfying the constraint set here\n(f(x;l)>1080 ),yR= 1080 . As witnessed a large number\nof particles have converged to the objective’s peaks which\nsuggest better regions. Indeed the regions at the bottom (the\npeaks) constitute pre-deﬁned ground-truth regions (explained\nin our evaluation section). There are also particles that seem\nstationary as they are in a space undeﬁned by our objective(4), where ( f(x;l)<1080 ). We also explain this in more\ndetail in our Evaluation section.\n0.00 0.25 0.50 0.75 1.00\nx10.00.20.40.60.81.0l1\n0.81.62.43.24.04.85.66.47.2\nFig. 1. Final positions of particles (optimal regions) in the 2-dim. region\nsolution space. The objective’s (4) value is the color’s intensity with the peaks\nshown at the bottom of the plot.\nB. Constraining the Regions Solution Space\nWe introduce surrogate models in our problem to expe-\ndite the process of evaluating viable regions. However, the\nsurrogate models are not restricted within a speciﬁc domain.\nAlthough the underlying fis essentially undeﬁned in regions\nwith no data points in B, surrogate models are not. The\npurpose of ML models, is to generalize to unknown regions.\nHence, even if the function fis undeﬁned in areas with no data\npoints, ^fwill still return a result. If the surrogate model is not\nprovided with training examples denoting where the function\nis undeﬁned then the obtained result might not reﬂect reality.\nTherefore, we adapt our algorithm to account for this fact.\nThe particles in GSO are initially randomly spread across\nthe solution space. Again, the valid solution space (space\nwhere data points and thus regions exist) is not reﬂected and\nparticles only have their neighbours’ luciferin values to guide\nthem. These are inherently associated with the ﬁtness value\n^J, which then goes back to our initial concern about the\nvalidity of the surrogate models. To alleviate this, we ﬁrst\napproximate the distribution of the data points pA(a)(over a\nsample for large-scale datasets) in Badopting Kernel Density\nEstimation (KDE) [11] and then we obtain the probability\nof a region containing any number of data points, i.e., from\nx\u0000ltox+l. We use this as a guide for particles when\nselecting which direction to explore. Therefore, given (7),\nwe alternate the selection probability by multiplying with the\ndensity (probability) of data points around the particle pj’s\ndata component xj:\nP0fpjg=Pfpjg\u0001Rxj+lj\nxj\u0000ljpA(a)da\nP\nk2NiPfpkg\u0001Rxk+lk\nxk\u0000lkpA(a)da(8)\nC. Complexity of Multimodal Optimization\nAs reported earlier, the baseline complexity of our problem\nisO((n\u0002m)d\u0002N). By adopting GSO and surrogate models\n^f, we expedite this process, obtaining viable solution(s) in\nO(TL2d), whereTis the number of iterations and Lis the\nnumber of particles for GSO. As a rule of thumb, GSO requires\nless thanT\u0019100 iterations and L\u0019100 glowworms to\nconverge (evidenced also in our experiments shown later). On\n\nthe contrary, using the naive approach with just n=m= 6\nandd= 5, one needs to evaluate more than 6\u0001107possible\nregions over Ndata points. On the other hand, GSO has to\nexecute only 100\u0002100 = 104evaluations, just 0:016% of\nthe evaluations needed by the baseline approach.1Therefore,\nby using GSO, the complexity is now of polynomial nature\nas not all parameter values, spanning uniformly across the\nentire domain space, have to be examined. In addition, the\nuse of surrogate models has eliminated the need to examine\nNdata points as the regions no longer have to be evaluated\nusingf. In the next section, we report on how to approximate\nfusing ML models. This gives a near-constant time (w.r.t\nthe chosen model) performance for evaluating obtaining the\nregion’s statistic y.\nIV. S URROGATE MODEL ESTIMATE\nWe could approximate fvia various ML models2trained\nto associate a region [x;l]with its corresponding statistic\ny=f(x;l)using a set of past function fevaluations\ninQ=fqm= [xm;lm;ym]gM\nm=1. Using these training\nexamples, ML models approximate the actual f. In general,\nML algorithms try to minimize the Expected Prediction Error\n(EPE) min ^fE[(f(x;l)\u0000^f(x;l))2]which is estimated using an\nout-of-sample dataset different from Q. They also try to ﬁnd\nmodels which are complex enough to minimize this EPE and\nsimple enough to ensure good generalizability to never before\nseen examples: they tune what is called the Bias-Variance\ntrade-off to ensure the derived model is neither under-ﬁtting\nnor over-ﬁtting [11]. However, our task is to approximate the\nbehavior of the actual fapplied over regions of data subsets\ninB. Hence our primary concern is notto generalize well\nto new examples. Instead, it is to ﬁnd a surrogate model ^f,\nwhich follows the trend offover random regions given an\narbitraryyR. In other words, our desideratum of ^fis that\ngiven a random region [x;l], if the statistic y=f(x;l)and\nf(x;l)< y Rthen (and only then) the estimate ^y=^f(x;l),\nand ^f(x;l)< y R. That is both fand ^fshould agree on\nthe constraint < y Rfor any random region. This, clearly by\ndeﬁnition, does not imply that jy\u0000^yjis desired to be as small\nas possible (i.e., minimizing the prediction error). Instead, we\nwould like to obtain a model ^fsuch that whenever y < y R\nholds then, ^y < y Rholds true, too. Surely, if ^fminimizes\nthe EPE then we may statistically expect that the two above-\nmentioned conditions hold true. Nonetheless, both conditions\ncan hold true even if it is not the case that y\u0019^y. To reﬂect\nthis objective, we would require to ﬁnd an estimate ^f, which\nminimizes the L2norm difference of gradients at any region:\nmin\n^fE[kr^f\u0000rfk2] (9)\nMinimizing the gradient difference we expect that a surrogate\nmodel ^fresembles the behavior of the true underlying function\n1Note: Although the complexity contains T\u0002L2, the number of region\nevaluations by the algorithm is, in fact, T\u0002L[19].\n2We restrict to a single class of ML models in our experimentation, however\nthis is not necessary and alternative ML models could be employed.f. However, a number of problems arise if we seek to\nminimize (9). We have no way of knowing if the true function\nfis differentiable and we also do not restrict our choice\nof ML models to differentiable ones. We could approximate\nthe gradient using a ﬁnite number of training samples that\nare equally spaced in ( x;l). But this would mean that we\ncannot take advantage of past function evaluations, issued by\nanalysts/applications, as an assumption that these examples are\nequally spaced is invalid.\nIn this paper, we do not use a speciﬁc class of ML models\nthat minimizes (9) and is left as our future work for further\ninvestigation. Nevertheless, we adopt conventional ML models\nminimizing the EPE, which can be directly used for providing\nrobust (in terms of predictability) surrogate estimate model ^f.\nV. P ERFORMANCE EVALUATION\nIn our evaluation, we seek to answer the following:\n1) What is the impact on accuracy, for ﬁnding inter-\nesting regions per user/application request using ML-\napproximated surrogate models ^f?\n2) What are the performance beneﬁts of SuRF over the\nbaseline approach and other methods?\n3) How is the efﬁciency and accuracy affected by SuRF-\nGSO, ML-approximate surrogate models and objective\nfunctions?\nWe begin by outlining the implementation details & setup,\ndiscussing our methodology and establish evaluation metrics\nin Section V-A. We showcase the accuracy of SuRF in compar-\nison to other methods using a variety of synthetic datasets in\nSection V-B. A qualitative analysis over real datasets, showing\nthe applicability of SuRF is presented in Section V-C. The\nperformance beneﬁts of SuRF are discussed in Section V-D.\nThe aforementioned sections provide the answers to questions\n(1) and (2). Finally, we answer question (3) by evaluating\nthe sensitivity of objective functions, GSO and surrogate ML\nmodels in Sections V-F, V-G, and V-H, respectively.\nA. Implementation Details & Setup\nWe implemented our algorithms using scikit-learn [24] and\nadopted the XGBoost (XGB) [8] ML model for our ML-\napproximated surrogate models ^f. We implemented Glow-\nWorm [19] as our optimization algorithm. We performed our\nexperiments using Python 3.5 running on a desktop machine\nwith an Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz and\n16GB RAM. The surrogate models used for both synthetic and\nreal datasets were trained using a set of past function evalu-\nations executed across the data space with centers xselected\nuniformly at random and region side lengths lset to cover\n1%\u000015% (uniformly) of the data domain.3The surrogate\nmodels were hyper-tuned using Grid-Search [24] with K-fold\ncross validation. A sensitivity analysis for surrogate models is\ndiscussed at Section V-H.\n3Please note that uniformly sampling regions across the data space with\nuniform lengths is not the same as obtaining training examples that are equally\nspaced across the complete domain in both xandl\n\nMethods : We evaluate the effectiveness and efﬁciency on\nmining interesting regions of four different methods: (i) Our\nframework SuRF which is the ML-approximated surrogate\nmodel used with the GSO (ii) Naive is the baseline method\ndescribed in Section II-A4, (iii)f+GlowWorm is the GSO\noptimization coupled with the true underlying function which\naccesses data to evaluate the objective function described in\n(4), and (iv) PRIM, is an implementation of the algorithm\nin [12], which is obtained from [1]. PRIM is used to ﬁnd\nregions which maximize the result of an output variable. We\nhave found it performs good on our task as well.\nSynthetic Datasets: We have created 20synthetic datasets\nto compare the methods outlined above. The size of the\ndatasets can be arbitrary and it is deﬁned within each experi-\nment. The synthetic datasets have Ground Truth (GT) regions,\nwhich are purposely either more dense than the rest of the\ndataset, or have relatively higher yvalues (for the purposes\nof testing for other statistics). The GT regions are hyper-\nrectangles constraining a region in all dimensions. Concretely\nwe vary the following settings: number of GT regions k=\nf1;3g, statistic type for yis either: (i) ‘density’ referring to\nnumber of data points in subset Dor (ii) ‘aggregate’ referring\nto average value of a certain dimension of data points in\nsubsetD, data dimensions d2 f1;2;3;4;5g. Each dataset\nis characterized by a variation of these settings. Note that the\nstatistic could be any other type, e.g., variance, high-order\nmoments. Figure 2 shows four different datasets with varying\nsettings. The sub-ﬁgures on the left show data points afor\nsettingd= 1, as only the dimension a1is to be used to\nbound the data space. The dimension a1has areas with higher\nvalues foraiand thus the average y=1\njDjPjDj\nm=1ai;mover\nthe highlighted GT regions bounded on a1is higher. On the\nother hand, the sub-ﬁgures on the right show the corresponding\ndatasets for the density statistic. The region is bounded by\nbotha1anda2and for the highlighted (green rectangle) GT\narea the density of data points is higher. The number of GT\nregionsk= 3 is evident at the bottom sub-ﬁgures, in which\nmultiple regions exist for both statistics, and k= 1 at the top\nsub-ﬁgures.\nOur goal for each synthetic dataset is to estimate the GT\nboundaries as close as possible. Let R(x;l)be the hyper-\nrectangle area corresponding to a random region [x;l]2R2d\nwith coordinates: [x\u0000l;x+l]. We use a popular metric adopted\nin data mining, the Intersection over Union (IoU), also known\nas the Jaccard Index i.e., a ratio where the numerator is the\narea of overlap between the bounding box (hyper-rectangle)\nR(xk;lk)of the region [xk;lk]mined from any of the outlined\nmethods and the ground-truth bounding box G(x0;l0)corre-\nsponding to the GT region [x0;l0]. The denominator is the area\nof union, i.e., the area encompassed by both the R(xk;lk)and\nthe GT bounding box G(x0;l0), thus, we obtain:\nIoU=R(xk;lk)\\G(x0;l0)\nR(xk;lk)[G(x0;l0); (10)\n4As the number of function evaluations becomes un-manageable we restrict\nthe discretisation to n=m= 6where\\and[in (10) are adopted as the overlap and union\noperators over (hyper)-rectangles. One might notice that region\ndimensionality is not exceedingly high (we experiment up\nto2d= 10 dimensions in the region solution space for\nd= 5 data dimensionality). Indeed, at ﬁrst we conducted\nexperiments by producing synthetic datasets U(0;1)d;d\u001d5,\nresulting to searching for regions in signiﬁcantly higher than\n10-dimensional spaces. However, due to the effects of curse\nof dimensionality and as mentioned by Friedman et al. [11],\nregions (and data points) become increasingly sparse and, thus,\nthe mined regions were returning no data points, thus, no\ninteresting regions . The expected length lof a hyper-cube to\nretrieve a fraction of data points in unit volume in Rdis given\nbyE[r] =r1\nd[11]. Thus, as dimensionality dincreases, the\nexpected length becomes much larger, covering most of the\ndata domain. Hence, the notion of ﬁnding interesting regions\nbecomes meaningless as we would essentially return regions\ncovering most of the data domain. Even though we set the\nsynthetic datasets’ dimensionality up to 5, we highlight the\nfact that our algorithm deals with 2ddimensions as our regions\nare expressed as vectors in R2d(region solution space).\n05αi\nAggregate\nRegionsd= 1\n0.00.51.0\nα2\nDensity\nRegionsd= 2\n0.00 0.25 0.50 0.75 1.00\nα1−505αi\n0.00 0.25 0.50 0.75 1.00\nα10.00.51.0\nα2\nFig. 2. Synthetic Ground Truth Regions (shaded green) for statistic type\n‘aggregate‘ and d= 1(left) and ground truth regions (green rectangles) for\nstatistic type ‘density‘ and d= 2 (right), with both a single ground truth\nregion k= 1 (top) and multiple regions k= 3 (bottom).\nReal Datasets: We use the Crimes [2] and Human\nActivity datasets [4] publicly available online. As ground-\ntruth regions do not exist for these datasets, we use them to\nconduct a qualitative analysis experiment, testing the applica-\nbility and effectiveness of SuRF to ﬁnd regions of interest\nfor ﬁxedyR. Speciﬁcally, we train surrogate models using\nfunction evaluations obtained uniformly across the data space\nwith varying lengths and, then, try to ﬁnd regions of interest\ngivenyR. Finally, we analyze the obtained regions and conﬁrm\nthat they match to true regions in those datasets. Parameter c\nfor objective (2) was set to 4.\nB. Accuracy of Interesting Region Identiﬁcation\nAll experiments for assessing the accuracy of the inter-\nesting region identiﬁcation were performed on the constraint\nf(x;l)> yRwithyRset to the value close to the extracted\nstatistic given by the GT regions. Speciﬁcally yR= 2 for\naggregate statistics and yR= 1000 for the density statistic. As\n\nstated, the surrogate models were trained using past function\nevaluations, the number of past function evaluations varied\nas the number of dimensions increases ( 300\u0000300K) to\naccount for the fact that more training examples are required to\nsufﬁciently learn a much larger space. The GSO parameters\nwere dynamically adjusted to reach convergence outlined in\nSection V-G. The objective’s parameter was set to c= 4. For\nPRIM, minimum support for the sub-boxes was set to 0:01\nand the threshold for aggregate statistics to 2. For Naive as\nthe number of queries becomes prohibitively large we resort\nto a subset of the total queries that are to be generated.\nNevertheless, this is still a good approximation for the method\noutlined at Section (II-A) and serves as a good baseline. As\nthe synthetic dataset size in this experiment is not important\nwe create synthetic datasets of 7;500\u000012;500points. Bigger\ndatasets will merely scale the responses. For all algorithms,\nwe obtain the average IoU per dataset by obtaining all the\nproposed regions given by the algorithms and assessing their\nIoU with the GT regions.\nFigure 3 shows the average IoU over all settings used.\nAs dimensionality increases, the IoU decreases for all meth-\nods across all settings. It is worth mentioning that our\nmethod is identical to the true underlying function method\n(f+GlowWorm) without incurring any of the costs associ-\nated with computing the exact results of the statistics. This\nleads us to believe that the error attributed to the use of\nan approximation is minimal and, thus, it can be safely\nused to identify interesting regions with no signiﬁcant use of\ncomputational resources. From all sub-ﬁgures, we can deduce\nthat dimensionality plays a crucial role in making this task\nmore challenging. We see a drop in IoU as d > 3, one\ncontributing factor is that the GT regions cover a much smaller\nspace in higher dimensions. Given a ﬁxed side length of\nl= 0:3in uniform space U(0;1), then the ratio of space\ncovered in d= 1 can be obtained by 0:3d= 0:31. Asd\nincreases then the ratio of space covered becomes much less\nand thus the possibility of fully intersecting with other hyper-\nrectangles is relatively small. For instance the ratio of covered\nspace (by the GT) in d= 3is2:7%of the total space covered\nby the unit hyper-cube.\nFor the aggregate statistic and k= 1 (top-left sub-ﬁgure\nof Figure 3), PRIM outperforms all other methods and is\ninitially invariant by the increase in dimensions. However,\nfor the density statistic (right column in Figure 3), PRIM is\nunable to spot the GT regions as it is not applicable in such\ndomains. PRIM constructs sub-boxes (hyper-rectangles) by\npeeling across a speciﬁc dimension. It sequentially generates\nsmaller sub-boxes Buntil the support of current box \fB(i.e.,\n\fB=jBj; the number of points belonging in B) is below a\nuser-speciﬁed threshold \f0. PRIM tries to identify sub-boxes\nwith minimum support \f0, that maximize the average response\nvalue of a selected attribute. Formally, PRIM’s objective is:\nmax\nBE[f(a)ja2B^\fB=\f0]: (11)\nThe density of a box Bis deﬁned by the support to volume\nratio:jBjQd\nili, where the denominator is the volume of the sub-box. To this end, there is neither a way to specify density\nas the response variable, nor PRIM takes into consideration\nthe volume of sub-boxes. In addition, PRIM progressively\nremoves sub-boxes such that the expectation in (11) is greater\nthan what it was before the removal of the sub-box. In case\nwhere two sub-boxes BiandBjprovide similar gains w.r.t.\n(11), then the one with less support\fBi< \fBjis removed.\nHowever, in the case of the density statistic and, precisely\nbecause PRIM does not consider the region covered by the\nsub-boxes, a sub-box with higher density might be removed.\nOf course this should not be considered as a problem of PRIM\nas we are testing it in settings that was not designed to operate.\nIts primary use case is to maximize the average response of an\nattribute by enclosing small sub-boxes in d-dimensional space.\nPRIM also performed less than the rest methods for the\naggregate statistic and k= 3 multiple regions (bottom-left) in\nFigure 3)5. In general, we are able to get satisfactory IoU with\ntheNaive method, but as we will exhibit in our performance\nsection, its efﬁciency deteriorates as datasets grow in size and\ndimension.\n0.00.20.40.60.81.0IoUType: Aggregate - Regions: k=1SuRF Naive PRIM f+GlowWorm\nType: Density - Regions: k=1\n1 2 3 4 5\nDimensions0.00.20.40.60.81.0IoUType: Aggregate - Regions: k=3\n1 2 3 4 5\nDimensionsType: Density - Regions: k=3\nFig. 3. Average IoU: (Top-Left) for aggregate statistic and k= 1 GT region;\n(Top-Right) for density statistic and k= 1 GT region; (Bottom-Left) for\naggregate statistic and k= 3 GT regions; (Bottom-Right) for density statistic\nandk= 3 GT regions.\nFigure 4 shows the average IoU along with the standard\ndeviation for multiple/single regions (left) and different statis-\ntic/aggregate types (right). For multiple regions, Figure 4(left)\nwe note that PRIM has the relatively largest standard deviation\nand largest decrease in accuracy as we switch from 1 GT\nregions to 3.\nIn addition, all other methods seem to be identical, with\na decrease experienced from 1 GT region to 3 GT regions.\nOn the other hand, the statistic type (density or aggregate)\nin Figure 4(right) does not affect accuracy, apart for PRIM’s,\nwhich as stated is not able to ﬁnd regions under this setting.\nGiven our experiments, it is safe to conclude that SuRF is able\nto detect multiple regions of interest under different types of\nstatistics.\nC. Qualitative Analysis over Real Datasets\nWe also run a set of experiments over real datasets to exem-\nplify the use cases of SuRF. Using the approach described, we\n5The IoU for k= 3 is obtained by averaging IoU’s for 3 GT regions.\n\nk=1 k=3\nMultiple Regions0.00.20.40.6IoUSuRF\nNaive\nPRIM\nf+GlowWorm\nAggregate Density\nAggregate Type0.00.20.40.6IoUSuRF\nNaive\nPRIM\nf+GlowWormFig. 4. (Left) Average IoU for multiple regions; (right) Average IoU for\ndifferent statistics.\nexamine whether SuRF can indeed identify regions of interest\nexperimenting with Crimes [2] and Human Activity [4]\nreal datasets. SuRF was trained using synthetically generated\npast region evaluations. We use SuRF over Crimes to identify\nregions where the crime index is over the 3rdquartile of a\nrandom set of regions, i.e., yR=Q3with ^f(x;l)>yR. Figure\n5 shows the number of crimes over X-Y spatial coordinates.\nThe higher the intensity of the color, the higher the crime rate\nis within the given area. We plot the corresponding density\nvalues obtained by the surrogate model ^f(\u0001), shown at Figure\n5(left), and note that it is a coarse grained approximation to\nthe true density values shown on the right. However, optimiz-\ning the objective function using the surrogate model is still\nsufﬁcient to propose accurate regions in a matter of seconds.\nThe regions shown at Figure 5(left) are the regions that SuRF\nidentiﬁed as complying with the constraint ^f(x;l)> y R.\nFigure 5(right), shows the same regions over the true density\nvalues with 100% of the proposed regions complying with\nf(x;l)>yR. This means that the obtained region deﬁned by\n(x;l)complied with the constraint >yRat both the surrogate\n^fand the true function f. Thus, SuRF using approximate\nsurrogate models and GSO is able to pin-point regions of\ninterest in the true data space, complying with the user request\nf(x;l)> yR;yR=Q3. Moreover, the regions identiﬁed are\nhighly parsimonious as the regions denote boundaries in X-Y\nCoordinates.\n0.00 0.25 0.50 0.75 1.00\nX Coordinate0.00.20.40.60.81.0Y CoordinateSurrogate Model  f̂⋅)\n0200400600800\n0.00 0.25 0.50 0.75 1.00\nX Coordinate0.00.20.40.60.81.0Y CoordinateTrue Function f̂⋅)\n0100200300400\nFig. 5. On the left, identiﬁed regions by approximate surrogate function ^f\nmatch to regions identiﬁed by the true function fshown to the right.\nFurthermore, the Human Activity dataset reports the\nvalues for gyrometers and accelerometers. Using the parame-\nters(X, Y, Z) from the accelerometers, we used SuRF to\nidentify regions with high ratio for a speciﬁc activity; for this\nexperiment we used the human activity stand . This proac-\ntively suggests classiﬁcation boundaries which the analystsTABLE I\nCOMPARATIVE ASSESSMENT OF DIFFERENT METHODS .\nData size N 105106107\nMethod ddim. Time (sec)\nSuRF 1 1.28 1.28 1.3\n2 1.4 1.4 1.4\n3 1.35 1.35 1.35\n4 1.63 1.63 1.64\n5 1.68 1.68 1.69\nNaive 1 0.01 0.16 1.94\n2 3.22 33.72 341.7\n3 115.49 1221.6 - (22%)\n4 - (66%) - (6%) - (0.5%)\n5 - (1%) - (0.1%) - (0.01%)\nf+GlowWorm 1 4.71 51.9 601.32\n2 26.7 280.14 2856.02\n3 26.46 289.5 2808.42\n4 27.1 293.62 2981.81\n5 30.21 320.03 -\nPRIM 1 0.15 0.4 4.8\n2 0.2 1.9 32.2\n3 0.56 9.3 46.3\n4 0.9 9.5 160.5\n5 1.28 7.36 282.6\ncan adopt to build a baseline classiﬁer, or further investigate\nthe identiﬁed region. SuRF was able to identify regions with\nratio of 33% for activity stand . Notably, the empirical CDF\n^FY, whereYis the ratio of data points with activity= stand ,\nshowed that the probability of obtaining yR= 0:3was equal\ntoP(f(x;l)>yR) = 1\u0000^FY(0:3) = 0:0035 . This denotes a\nhighly unlikely event and also shows that regions with higher\nratios are not easy to identify. This denotes the capability of\nSuRF to mine interesting regions even for cases where the\nusers’ requests correspond to highly unlikely regions.\nD. Models Comparison\nWe present a comparative assessment with other methods\nto showcase the efﬁciency and scalability of SuRF in terms\nof data size and dimensionality. We also demonstrate the\nexponential complexity of the considered problem. The perfor-\nmance results are shown in Table I. As shown in Table I, the\nNaive method is efﬁcient with low dimensional data ( d= 1).\nForNaive , we keptm=n= 6, therefore the number\nof function evaluations executed were just (6\u00026)1= 36\nford= 1 . However, there is an exponential increase in\ntime asdincreases, and with N= 107data points, Naive\ntimes out. The ratio included denotes the number of regions\nexamined before exceeding the time limit, which was set\nto3000 seconds. The same trend appears in f+GlowWorm\nshowing an exponential increase in the amount of time it takes\nto mine interesting regions. The GSO parameters were set to\nT= 100 andL= 100 for bothf+GlowWorm and SuRF ,\nwith initial swarm neighborhood range r0= 3 and constants\n\r= 0:6;\u001a= 0:4as in [19]. For these experiments, we keep\nGSO’s parameters ﬁxed to explore the effects of dimension-\nalitydand data size N. At (V-G) we investigate the impact\nof GSO’s parameters on efﬁciency. PRIM is not affected as\nmuch and performs well across all conﬁgurations except when\n\nthe dimensions dand data points Nbecome sufﬁciently large.\nOn the other hand, SuRF only takes a few seconds across all\nconﬁgurations. Given the same dimesionality dand a varying\ndataset size N, SuRF’s performance remains constant (scales\nvery well) as SuRF does not actually access any data during\nthe mining process. Of course SuRF’s surrogate models are\ntrained before hand for separate statistics. The models will\nbe trained once on a number of past region evaluations and\nthen successively be used for different statistics, thresholds\nand by different users. Each new request does not need to\nre-train the model and the overhead for training the surrogate\nmodels of SuRF is incurred once. Note: It is worth mentioning\nthat all datasets were loaded in memory for performing these\nexperiments. For larger datasets in size Nthat do not ﬁt in\nmemory the methods in comparison would have to perform\nmultiple disk accesses, thus, incurring signiﬁcantly higher\ncosts in solving the discussed mining task. In addition, as\nstated in [12], PRIM is not equipped to work with disk-access\nand a common remedy would be to sample the dataset. On the\ncontrary, SuRF models are light enough, to always be loaded\nin memory and make no use of data at all. For SuRF, it does\nnot matter if the data are stored on a disk or remote data center.\nE. Training Surrogate Models\nIn this experiment we measure the overhead required to train\nthe surrogate models on a varying number of queries. The\nresults are shown at Figure 6. Using GridSearchCV by [24],\nwe are able to ﬁnd optimal parameters for our model of choice.\nFor GridSearchCV , we pre-specify a range of parameter values\nfor the parameters of XGBoost. We hypertune the parameters:\n(i)learning_rate 2[0:1;0:01;0:001], (ii) max_depth\n2[3;5;7;9], (iii) n_estimators2[100;200;300] and, (iv)\nreg_lambda2[1;0:1;0:01;0:001]. As expected, this takes\nmore time than only training the models with their values pre-\nspeciﬁed, as witnessed at Figure 6. This is because 3\u00024\u0002\n3\u00024 = 144 combinations have to be tested on large sets of\ntraining examples. We could possibly reduce the number of\nparameter values, to be tested, to increase efﬁciency. However,\nwe run the risk of not getting adequate approximations to f.\nSurely, this should not be a problem to the analysts as the\nmodels will only be trained once. In addition, the models could\nbe trained in a central location on more powerful clusters to\nexpedite this process and then subsequently be used by the\nanalysts.\n10 52 94136 178 220 262 304 346 388\nQueries (103)10−1100101102103104Time (log(s))\nHypertuning\nFalse\nTrue\nFig. 6. Training overhead shown in log-scale (y-axis) as the number of queries\n(x-axis) increase.F . Sensitivity on Optimization Functions\nWe compare the effectiveness of the optimization objectives\noutlined in (2) and (4) and present the results in Figure 7. The\ntop sub-ﬁgures refer to the objective function in (4) and the\nbottom sub-ﬁgures refer to objective function in (2). We used\nthe synthetic data set with d= 1 andk= 3 to be able to\nvisualise the objectives and demonstrate the multimodality in\nthe optimization process. Regarding the objective (4), the use\nof logarithms explicitly impose that for regions not adhering\nto the constraint on yR, the regions become invalid and the\ncorresponding objective function undeﬁned. Hence, the white\narea in Figure 7(top) corresponds to those areas. Using this\nobjective, GSO is able to successfully isolate glowworms\ninitialized at those areas and eventually adjust their radii to\nreach glowworms in the valid solution space only. On the\nother hand, if objective (2) was to be adopted, the glowworms\ncould have formed neighbourhoods in what they would believe\nare local optima, where in reality, those regions would be\ninvalid. In addition, we conduct an experiment to study the\nsensitivity of the objective on parameter ” c”. We keep a ﬁxed\nnumber of solutions spread uniformly across the solution space\nand gradually increase the value of parameter c. We used the\nsynthetic data set with d= 1andk= 1. The results are shown\nat Figure 8. On the y-axis we plot the number of solutions that\nare within a given radius ( 0:2) of the peak ( global optima ).\nAs is evident, the number of viable solutions decreases as c\nacts as a regularization variable on the region’s size that is to\nbe accepted.\n0.00.51.0l1c=1 c=2 c=3 c=4\n0.00 0 .25 0 .50 0 .75 1 .00\nx10.00.51.0l1\n0.00 0 .25 0 .50 0 .75 1 .00\nx10.00 0 .25 0 .50 0 .75 1 .00\nx10.00 0.25 0.50 0.75 1.00\nx1−7−6−5−4−3−2−1012\n−2.0−1.4−0.8−0.30.220.781.331.892.443.00\nFig. 7. 2-dim. region solution space examined by (top) objective Jin (4)\nand by (bottom) objective Jin (2) as the optimization parameter cincreases.\n0.0 0.5 1.0 1.5 2.0\nc0.050.100.150.20Viable Solutions (%)\nFig. 8. Sensitivity of parameter con identifying viable solutions.\n\nG. SuRF-GSO Algorithm Sensitivity\nWe have conducted experiments to evaluate the compu-\ntational efﬁciency of GSO and also examined its rate of\nconvergence across different dimensions and parameter set-\ntings. Please note that the Dimensions parameter has been\ndoubled to reﬂect the fact that SuRF (and GSO) operate at\n2ddimensions per our deﬁnition for regions at Def.2. GSO\nspeciﬁc parameters such as \r,\u001aare constant adopted from\nthe respective paper [19]. The results are shown in Figures\n9 & 10. Experimentally, we have found that the number of\nglowworms and neighbourhood radius ( r0in GSO parameters)\nhave to be adjusted to account for the enlarged region solution\nspace. We increase glowworms using L= 50dand radius\nr0= (1\u00001\n21\nL)1\ndadopted from [11] Section 2, Equation\n(2.24). Although the number of needed iterations does vary\nacross settings,as witnessed at Figure 9, the average number\nof iterations across all settings is 63. Making GSO a robust and\nefﬁcient algorithm for converging to the various local-optima\nof the mining task, over different dimensions dand number\nof multiple regions k. Moreover, the average performance\nfor varying number of iterations and glowworms is shown at\nFigure 10. In Figure 10(left), we increase dimensionality dand\nnumber of glowworms Las we keep the number of iterations\nT= 100 constant to measure the impact on performance for\na varying number of glowworms. This has minimal effect on\nthe total run-time as it takes no more than 15seconds for\nGSO’s process to complete (still better than the best competitor\nshown at V-D). The same holds for the number of iterations\nin Figure 10(right). Although the average number of iterations\nrequired to reach convergence is estimated to be 63and no\nsetting required more than T= 250 iterations, we measured\nthe performance for up to T= 400 iterations with L= 100 .\nNo more than 10seconds is required for the largest number\nof iterations to ﬁnish. It appears that both parameters cause\nan almost linear increase in time for the same number of\ndimensions even if the stated complexity was O(TL2d). This\nis because the number of glowworms is small enough so that\nthe time required is still driven by the prediction time from\nthe approximate ^f(x;l)instead of the increase in glowworms.\n5.05.56.0E[J]k=3\nDimensions=2\n45k=3\nDimensions=4\n24k=3\nDimensions=6\n024k=3\nDimensions=8\n−10k=3\nDimensions=10\n0 100 200\nIterations4.55.05.5E[J]k=1\nDimensions=2\n0 100 200\nIterations345k=1\nDimensions=4\n0 100 200\nIterations24k=1\nDimensions=6\n0 100 200\nIterations02k=1\nDimensions=8\n0 100 200\nIterations−2−10k=1\nDimensions=10\nFig. 9. Expected convergence rates vs iterations Tfor different dimensionality\ndwithk2f1;3gmultiple regions.\n2 4 6 8 10\nDimensions051015Time (sec)Glowworms (L)\n100 200 300 400 500\n2 4 6 8 10\nDimensions0246810Time (sec)Iterations\n100 200 300 400Fig. 10. SuRF-GSO mining performance over dimensionality dfor (left)\ndifferent number of glowworms Land (right) iterations T\nH. SuRF-Surrogate Model Sensitivity\nIn this experiment we evaluate the sensitivity of the sur-\nrogate models. Speciﬁcally we examine how the number of\ntraining samples and out-of-sample generalization error affect\nthe accuracy of the model. In addition, we evaluate how the\ncomplexity of the model affects the accuracy of the model\nand its ability to ﬁnd obtain good IoU. Figure 11 (left) shows\na negative correlation between IoU and Root Mean Squared\nError (RMSE) obtained from the ML-trained surrogate models\nusing XGB. For this experiment we use the dataset with\nadensity static, dimensions d= 3 and single GT region\nk= 1. As the out-of-sample test error (measured by RMSE)\nincreases, the accuracy for IoU drops. This is evidenced by an\nestimated regression line along with 95% conﬁdence interval\nand Pearson’s Correlation estimated at \u00000:57. Therefore, it is\nimportant to ﬁnd ML models that can also act as good statistic\nestimators. In addition, Figure 11 (right) shows how cross-\nvalidated error decreases as the number of training examples\nfor approximating a surrogate function increases. For each\nML model at different dimensions, we stop training when no\nfurther improvement is measured w.r.t. RMSE. We use datasets\nwith varying dimensions using the density statistic and single\nregionk= 1. The shaded area refer to the error’s standard\ndeviation. We note that by \u00181;000 training examples, i.e.,\nfunction evaluations, and sufﬁcient hyper-tuning of parame-\nters, the ML models are able to learn the association between\nregion vectors [x;l]and statistic values ywell enough. In our\nregion identiﬁcation accuracy experiments, we examine the\nIoU behaviour up to 5-dim. hyper-rectangles corresponding\nto 10-dim. vectors; recall region is [x;l]with x2x2Rd\nandl2Rd\n+. Hence, the XGB ML models need to learn\nusing 2\u0002d-dim. vectors. The number of examples is not at\nall hard to obtain as in reality multi-dimensional regions are\nextracted from datasets by a plethora of business intelligence\napplications . One could also assume that the past function\nevaluations can be obtained,manually by SuRF, at a regular\ndowntime of the system (where trafﬁc load is low). We also\nanalyze the impact of the XGB-ML model complexity on\nRMSE and IoU reﬂected by the maximum depth in regression\ntrees in XGB. The results for both training and cross-validation\nsteps are shown in Figure 12. As expected, RMSE drops as ML\nmodel complexity is increased. Although not initially evident,\n\nIoU has a tendency to increase when model complexity\nincreases. However, this might deter the analysts to training a\nmore complicated model as it is evident that they would be\nable to get a good enough approximation with relatively less\ncomplex models.\n0 50 100 150 200 250\nRMSE0.100.150.20IoU\nCorrelation Coef : -0.57\n101102103104105\nTraining examples100200300400500600700800RMSEDimensions = 2\nDimensions = 4\nDimensions = 6\nDimensions = 8\nDimensions = 10\nFig. 11. (Left) Correlation of IoU and RMSE; (right) number of training\nexamples needed to minimize RMSE of XGB ML-approximate surrogate\nmodel over different dimensionality d.\n5 10 15\nMax depth050100150200250300RMSETraining score\nCross-validation\nscore\n5 10 15\nMax depth0.00.20.40.60.81.0IoUTraining score\nCross-validation\nscore\nFig. 12. (Left) RMSE vs ML model complexity (max depth in trees in XGB);\n(right) IoU vs ML model complexity (max depth in trees in XGB).\nVI. R ELATED WORK\nIdentifying interesting regions can be traced back to Fried-\nman et al. [12] who were interested in ﬁnding regions in d-\ndimensional spaces that would maximize/minimize a depen-\ndent variable y. Their algorithm processed data sequentially\nto generate smaller regions and included a pruning step in the\nend. The computational cost of their algorithm is prohibitive\nwhen considering large data sets with respect to dimensionality\nand number of points. Their objective is different from ours\nas we do not seek regions that would maximize/minimize y\nbut regions that satisfy the conditions listed at (3). Our task is\nalso loosely coupled with the objective of Subspace Clustering\n(SC) [23]. The algorithms proposed for SC aim to identify\nclusters in low-dimensional sub-spaces by pruning regions\nand dimensions using some evaluation criteria often being the\nsupport of a given region. Other measures of interestingness\nhave also been proposed [28] with the underlying metric still\nbeing the number of points. Such methods rely on partitioning\nschemes, evaluating the density of the found regions and\npruning/merging until converging to a region of interest. This\nis not ideal as the complexity is often exponential w.r.t the\nnumber of dimensions [28] as also experimentally evidenced\nby our Naive/Baseline solution. In addition, although we\nconsider the density of regions as one example use case, in\ngeneral, we are interested in regions satisfying the constraint\noutlined in (3) for any given statistic. Thus, our objective issubstantially different, but we regard it as equally important for\ndata mining practitioners. Furthermore, there is large body of\nwork on Subgroup Discovery (SD) [5], [7], [13], of course the\nlist is not exhaustive. The purpose of SD is to ﬁnd subsets of\ndata that show an interesting behaviour with respect to a given\ninteresting-ness/quality function. It is similar to SC, however\nSD generalizes the notion of interesting-ness to subsets of data\n(potentially across all dimensions) of various data types, i.e\nnominal, binary, numeric etc. Depending on the data type an\nanalogous quality function is employed. Multiple algorithms,\nboth exhaustive [6] and approximate [5] have been developed\nfor this task, however to our knowledge most algorithms are\ndata-driven and do not share our approach to this problem. By\ndata-driven we mean that they employ algorithms that work\ndirectly with the underlying data and try to extract subgroups\nby repeatedly performing region evaluations. We believe that\nthis is costly as data sets become larger.\nIn other contexts, ﬁnding interesting regions was explored\nfor categorical attributes by the construction of OLAP cubes\n[27] on deﬁned dimensions and hierarchies. As we consider\nthe problem of identifying regions in continuous attributes\nthis approach could not be leveraged as also mentioned in\n[27], in which they direct to other techniques for continuous\ndata. Alternative formulations, such as posing this problem as\nﬁnding the top-k regions [22], could also be leveraged and are\nconsidered to be complementary to our approach. In the case\nof,top-k formulation, the user has to supply the number of\nregions, this is often ad-hoc and as evidenced by our examples\nat Section I a threshold is more intuitive. Hence, each approach\ncan be used in cases when one of the values ( kor threshold)\nis known. in addition, the complexity of any top-k algorithm\ninevitably depends on N(the number of data items), d, and\nk. In intended applications, Nwill be very large and (as we\nargued before, so will be k). In contrast, our approach manages\nto offer performance independent of N, which is likely to\npay off big dividends for big data deployments. Also, note\nthat for the multi-modal case in our experiments, if all top- k\nregions were to be concentrated in one of those regions (if y\nwas to be slightly higher for one of the regions) then a top- k\napproach would effectively identify just one of the regions.\nLastly, spatial indexing [14], [22] could also be considered as\nthe indexes produce hyper-rectangular regions over data points\nwhich is one of our requirements. However, their goal is not\nto locate interesting regions with respect to a given statistic\nbut to group data points together for efﬁcient access. Hence,\nthe produced regions only take in mind the spatial distance of\ndata points and produce ﬁxed regions which the user can use.\nHence, for any subsequent region that a user wants to identify\nbased on a threshold or a region size requirement the regions\nproduced by these methods are ﬁxed.\nFinding regions has been considered in other domains [10],\n[20], [29], showing an interest for such methods, with different\nobjectives and algorithms which consider smaller data sets\nN < 200;000. SuRF is used with an arbitrarily large number\nof data points Nas effectively makes no use of the underlying\ndatabase system; instead, SuRF uses ML models to perform\n\ncomputations over surrogate models.\nMachine Learning is increasingly being used to do heavy\nlifting in data computation where faster and more light-weight\nmodels can be leveraged to perform a variety data-intensive\ntasks. For instance, the authors of [3], [9] trained ML models\nusing past query evaluations to estimate the cardinality of\ndata points returned, given unseen queries without performing\ncomputations over the data set. In addition other areas include\n: settings where an estimate of the result of a given aggregate\nquery is requested [21] and for light-weight and efﬁcient\nindexes over data [18]. Our approach can similarly exploit past\nissued queries for mining signiﬁcant statistical information\nover the underlying data. However, the core objective and\ncontext are deﬁnitely not the same. We do not wish to\ntrain ML models that could answer unseen queries with an\narbitrary low error. Instead, we approximate the underlying\ntrue function fusing an accurate estimate ^f, which we then\nuse to inexpensively evaluate (4) and solve (3).\nVII. C ONCLUSIONS\nWe propose SuRF, a solution based on multimodal evolu-\ntionary optimization and Machine Learning which efﬁciently\nmines regions of interest in multidimensional datasets. Specif-\nically, the regions are associated with a statistic of interest y\ncomputed using the data points included in a region. Thus, the\nproblem of locating regions of interest is formulated as ﬁnding\nregions complying with y > y Rory < y R, whereyRis a\nuser deﬁned threshold. Given this constraint an optimization\nproblem is introduced which yields multimodal solution space.\nSuRF leverages the Glowworm Swarm Optimization built for\nthis class of optimization problems. SuRF also leverages ML\nmodels to approximate functions for predicting statistic yover\ninteresting regions. Therefore, by resorting to these algorithms,\nSuRF locates regions of interest 150x faster than the best\ncompetitor and more than 3orders of magnitude than the\nworse, with minimal impact in accuracy. To our knowledge,\nthe problem of ﬁnding interesting regions by fusing multi-\nmodal optimization with ML has not been investigated before.\nSuRF is a promising approach in solving a laborious and often\nmanual mining task.\nVIII. A CKNOWLEDGEMENT\nThis work is partially funded by the European Union\nEU/H2020 Marie Sklodowska-Curie Action Individual Fel-\nlowship (MSCA-IF) under the project INNOV ATE (Grant No.\n745829). The ﬁrst author is funded by an EPSRC scholarship,\ngrant number 1804140.\nREFERENCES\n[1] Prim - implementation. URL: https://github.com/Project-Platypus/PRIM.\n[2] Crimes - 2001 to present. URL: https://data.cityofchicago.org/Public-\nSafety/Crimes-2001-to-present/ijzp-q8t2, 2018. Accessed: 2018-08-10.\n[3] C. Anagnostopoulos and P. Triantaﬁllou. Query-driven learning for\npredictive analytics of data subspace cardinality. ACM Transactions\non Knowledge Discovery from Data (TKDD) , 11(4):47, 2017.\n[4] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz. A public\ndomain dataset for human activity recognition using smartphones. In\nEsann , 2013.[5] M. Atzmueller. Subgroup discovery. Wiley Interdisciplinary Reviews:\nData Mining and Knowledge Discovery , 5(1):35–49, 2015.\n[6] M. Atzmueller and F. Puppe. Sd-map–a fast algorithm for exhaustive\nsubgroup discovery. In European Conference on Principles of Data\nMining and Knowledge Discovery , pages 6–17. Springer, 2006.\n[7] A. Belfodil, A. Belfodil, and M. Kaytoue. Anytime subgroup discovery\nin numerical domains with guarantees. In Joint European Conference\non Machine Learning and Knowledge Discovery in Databases , pages\n500–516. Springer, 2018.\n[8] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system.\nInProceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining , pages 785–794. ACM, 2016.\n[9] A. Dutt, C. Wang, A. Nazi, S. Kandula, V . Narasayya, and S. Chaudhuri.\nSelectivity estimation for range predicates using lightweight models.\nProceedings of the VLDB Endowment , 12(9):1044–1057, 2019.\n[10] J. Eckstein, P. L. Hammer, Y . Liu, M. Nediak, and B. Simeone. The\nmaximum box problem and its application to data analysis. Computa-\ntional Optimization and Applications , 23(3):285–298, 2002.\n[11] J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical\nlearning , volume 1. Springer series in statistics New York, 2001.\n[12] J. H. Friedman and N. I. Fisher. Bump hunting in high-dimensional\ndata. Statistics and Computing , 9(2):123–143, 1999.\n[13] H. Grosskreutz and S. R ¨uping. On subgroup discovery in numerical\ndomains. Data mining and knowledge discovery , 19(2):210–226, 2009.\n[14] A. Guttman and M. Stonebraker. A dynamic index structure for spatial\nsearching. In Proceedings of the 13th ACM SIGMOD International\nConference on Management of Data , pages 47–57, 1983.\n[15] S. Idreos, O. Papaemmanouil, and S. Chaudhuri. Overview of data\nexploration techniques. In Proceedings of the 2015 ACM SIGMOD\nInternational Conference on Management of Data , pages 277–281.\nACM, 2015.\n[16] A. E. Jaffe, P. Murakami, H. Lee, J. T. Leek, M. D. Fallin, A. P. Feinberg,\nand R. A. Irizarry. Bump hunting to identify differentially methylated\nregions in epigenetic epidemiology studies. International journal of\nepidemiology , 41(1):200–209, 2012.\n[17] J. Kennedy. Particle swarm optimization. Encyclopedia of machine\nlearning, Springer , pages 760–766, 2010.\n[18] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The\ncase for learned index structures. In Proceedings of the 2018 ACM\nSIGMOD International Conference on Management of Data , pages 489–\n504. ACM, 2018.\n[19] K. Krishnanand and D. Ghose. Glowworm swarm optimization for\nsimultaneous capture of multiple local optima of multimodal functions.\nSwarm intelligence , 3(2):87–124, 2009.\n[20] B. Liu, L.-P. Ku, and W. Hsu. Discovering interesting holes in data. In\nIJCAI (2) , pages 930–935, 1997.\n[21] Q. Ma and P. Triantaﬁllou. Dbest: Revisiting approximate query\nprocessing engines with machine learning models. In Proceedings of the\n2019 International Conference on Management of Data , pages 1553–\n1570. ACM, 2019.\n[22] N. Mamoulis, S. Bakiras, and P. Kalnis. Evaluation of top-k olap queries\nusing aggregate r–trees. In International Symposium on Spatial and\nTemporal Databases , pages 236–253. Springer, 2005.\n[23] L. Parsons, E. Haque, and H. Liu. Subspace clustering for high\ndimensional data: a review. ACM SIGKDD Explorations Newsletter ,\n6(1):90–105, 2004.\n[24] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-\nplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. Scikit-learn: Machine learning in Python. Journal of Machine\nLearning Research , 12:2825–2830, 2011.\n[25] R. A. Poldrack. Region of interest analysis for fMRI. Social Cognitive\nand Affective Neuroscience , 2(1):67–70, 03 2007.\n[26] Rui Xu and D. Wunsch. Survey of clustering algorithms. IEEE\nTransactions on Neural Networks , 16(3):645–678, May 2005.\n[27] S. Sarawagi, R. Agrawal, and N. Megiddo. Discovery-driven exploration\nof olap data cubes. In International Conference on Extending Database\nTechnology , pages 168–182. Springer, 1998.\n[28] K. Sequeira and M. Zaki. Schism: A new approach for interesting\nsubspace mining. In Fourth IEEE International Conference on Data\nMining (ICDM’04) , pages 186–193, 2004.\n[29] M. R. Uddin, C. Ravishankar, and V . J. Tsotras. Finding regions of\ninterest from trajectory data. In 2011 IEEE 12th MDM International\nConference on Mobile Data Management , volume 1, pages 39–48, 2011.",
  "textLength": 69534
}