{
  "paperId": "395e65ea10ab82ab9010c50245dc6edcf7515e13",
  "title": "A Lazy Approach for Efficient Index Learning",
  "pdfPath": "395e65ea10ab82ab9010c50245dc6edcf7515e13.pdf",
  "text": "arXiv:2102.08081v2  [cs.DB]  18 Feb 2021A Lazy ApproachforEﬀicientIndexLearning\nGuanli Liu\nUniversity ofMelbourne\nguanli@student.unimelb.\nedu.auLarsKulik\nUniversity ofMelbourne\nlkulik@unimelb.edu.auXingjun Ma\nDeakin University\ndaniel.ma@deakin.edu.auJianzhong Qi\nUniversity ofMelbourne\njianzhong.qi@unimelb.\nedu.au\nABSTRACT\nLearned indices usingneuralnetworkshavebeenshowntoout -\nperform traditional indices such as B-trees in both query ti me\nandmemory.However,learningthedistributionofalargeda taset\ncan beexpensive, and updating learned indices is diﬃcult,t hus\nhindering theirusageinpracticalapplications.Inthispa per,we\naddresstheeﬃciencyandupdateissuesoflearnedindicesth rough\nagilemodelreuse .Wepre-trainlearnedindices over asetofsyn-\nthetic (rather than real) datasets and propose a novel appro ach\nto reuse these pre-trained models for a new (real) dataset. T he\nsynthetic datasetsarecreatedtocover alargerangeofdiﬀe rent\ndistributions.Given a new dataset D/u1D447,weselectthelearnedin-\ndex of asynthetic dataset similarto D/u1D447,toindexD/u1D447.Weshow\na bound over theindexing error when a pre-trained index is se -\nlected. We further show how our techniques can handle data\nupdates and bound the resultant indexing errors. Experimen tal\nresults on synthetic and real datasets conﬁrm the eﬀectiven ess\nand eﬃciency of ourproposed lazy(modelreuse) approach.\n1 INTRODUCTION\nLearnedindices using neural networks have been shown to out-\nperform traditional indices such as B-trees in both query ti me\nand memory [5, 10, 13]. Given a dataset (e.g., a database tabl e),\nan index is a structure that maps the index key /u1D45D./u1D458/u1D452/u1D466.altof a data\npoint/u1D45D(e.g., a data record) to its storage address /u1D45D./u1D44E/u1D451/u1D451/u1D45F. The\nidea of learned indices is to train a machine learning model F\n(e.g., aneuralnetwork) toapproximatethemappingfrom /u1D45D./u1D458/u1D452/u1D466.alt\nto/u1D45D./u1D44E/u1D451/u1D451/u1D45F. Previous work has shown that such learned indices\ncanbesimplerandmorequery-eﬃcientthantraditionalindi ces.\nThe trained model Fcan predict /u1D45D./u1D44E/u1D451/u1D451/u1D45Fwith a bounded error\nrange[/u1D452/u1D45F/u1D45F/u1D459,/u1D452/u1D45F/u1D45F/u1D462],i.e.,thedatapoint /u1D45Dcanbefoundintherange\nof[F(/u1D45D./u1D458/u1D452/u1D466.alt)+/u1D452/u1D45F/u1D45F/u1D459,F(/u1D45D./u1D458/u1D452/u1D466.alt)+/u1D452/u1D45F/u1D45F/u1D462][5].\nWhile learned indices have eﬃcient query procedures, they\nare proneto slowbuilding and updates,since machine learni ng\nmodelsareexpensivetotrain,andoncetrained,theyarediﬃ cult\ntoupdate.Evenwithsimplemodelssuchaslinearsplines,cu bic\nsplines, or linear regression, a learned index such as the recur-\nsivemodelindex (RMI)[10]istwoordersofmagnitudeslowerto\nbuildthanaB-tree[13].Techniquesthatlearnindicesinas ingle\npass such as RadixSpline [8]canbebuiltfaster,buttheytend to\nproducesub-optimal indices of large sizes and lower query e ﬃ-\nciency.Thehighcostsinmodeltrainingalsopreventtheret rain-\ning of learned indices for every data update. Existing learn ed\nindices [4–6] avoid model retraining by storing newly inser ted\npoints into additional structures, which inevitably adds q uery\nprocessing costs.This limits the applicabilityof learned indices\nin dynamic scenarios where there are frequent dataset creat ion\nor updates, which is common in practice, for example, to inde x\nsenor data ordatafrom scientiﬁc studies(simulations) [19 ].\n© 2021 Copyright held by the owner/author(s). Published in P roceedings of the\nACM Conference, July 2017, ISBN 978-x-xxxx-xxxx-x/YY/MM o n OpenProceed-\nings.org.\nDistributionof this paper is permitted under the terms of th e CreativeCommons\nlicense CC-by-nc-nd4.0.Table 1: Two example datasets D/u1D446andD/u1D447.\nD/u1D4460.10.20.30.50.60.70.80.80.91.0\nD/u1D4470.10.20.250.30.40.50.60.80.91.0\nInthis paper,weaim toaddress theeﬃciency issues intrain-\ning and updatinglearned indices withouthindering their qu ery\neﬃciency.Oursolutionisinspiredby domainadaptation [1].Given\namodelM/u1D446trainedonanexisting(source)dataset D/u1D446,domain\nadaptation reuses M/u1D446for a new (target) dataset D/u1D447by ﬁne-\ntuningM/u1D446overD/u1D447. This avoids training a new model on D/u1D447\nfrom scratch,which canbeextremely time-consuming.\nA key requirement for successful adaptationof M/u1D446toD/u1D447is\nthatD/u1D446andD/u1D447should have similar distributions [2, 12]. Oth-\nerwise,M/u1D446may yield large errors on D/u1D447. This is important in\nour problem as we aim to further skip ﬁne-tuning on D/u1D447, to\nachieve fast updates. This motivates us to generate synthet ic\ndatasets to cover a wide range diﬀerent data distributions a nd\npre-train reusable indices on such datasets. Our dataset ge nera-\ntionisbasedonthe cumulativedistributionfunction (CDF).Given\na new datasetD/u1D447,we measure theCDF similarity between D/u1D447\nand the synthetic datasets. We select a model pre-trained on a\nsynthetic datasetsimilar to D/u1D447as theindex modelfor D/u1D447.\n0.0 0.2 0.4 0.60.70.8 1.00.000.250.500.751.00S(x)\nS\nT\nFigure1: CDFsof D/u1D446andD/u1D4471 (bestviewedin color).\nTable1andFig.1illustratetwoexampledatasets(bothsort ed\nin ascending order) and their corresponding CDFs. We denote\nthe CDFs ofD/u1D446andD/u1D447as/u1D450/u1D451/u1D453/u1D446(·)and/u1D450/u1D451/u1D453/u1D447(·), respectively.\nIn this toy example, an index model M/u1D446is learned to predict\nthe rank /u1D45D./u1D45F/u1D44E/u1D45B/u1D458(or percentile) of point /u1D45D∈ D/u1D446based on its\nsearch key /u1D45D./u1D458/u1D452/u1D466.alt, that is,/u1D45D./u1D45F/u1D44E/u1D45B/u1D458≈M/u1D446(/u1D45D./u1D458/u1D452/u1D466.alt)and/u1D45D./u1D44E/u1D451/u1D451/u1D45F≈\nM/u1D446(/u1D45D./u1D458/u1D452/u1D466.alt)·|D/u1D446|. This eﬀectively learns /u1D450/u1D451/u1D453/u1D446(/u1D45D). For/u1D45D∈D/u1D446,\n/u1D450/u1D451/u1D453/u1D446(/u1D45D)measurestheprobabilityofavaluelessthanorequalto\n/u1D45D,which is alsotherank of /u1D45DinD/u1D446.When reusingM/u1D446forD/u1D447,\ntheadditionalpredictionerrorsintroducedcanbebounded with\nrespect to the dissimilarity between /u1D450/u1D451/u1D453/u1D446(·)and/u1D450/u1D451/u1D453/u1D447(·). This\nsuggests that, if we can generate synthetic datasets that co ver\na suﬃciently large area of the space of all possible CDFs, the\nlearned indices on the synthetic datasets can be reused for a ny\nnew dataset with bounded prediction errors. Since the CDF of\na dataset with /u1D45Bpoints takes /u1D442(/u1D45B)time to compute, we further\nproposeahistogrambasedapproximationofCDF,withbounde d\nerrors,toreducethecomputationtimetoonly /u1D442(log/u1D45B).\nWe use a model reuse threshold /u1D716∈(0,1]to help determine\nwhethertoreuseapre-trainedmodelforanewdataset /u1D437/u1D447.When\nthe CDF similarity between D/u1D447and a synthetic dataset D/u1D446is\ngreaterthanorequalto /u1D716,wereusethemodel M/u1D446pre-trainedon\n\nD/u1D446toindexD/u1D447.Basedon /u1D716,wefurtherderivethemaximumad-\nditional predictionerror of M/u1D446onD/u1D447, and we derive thenum-\nberofsynthetic datasetstobegenerated.Sinceourmodelre use\nprocedureisfastandﬂexible,wecallit agilemodelreuse .Follow-\ningasimilaridea,weadaptagilemodelreusetohandleupdat es.\nWhen the similarity between the CDFs of a dataset D/u1D447and its\nupdated versionD′\n/u1D447is greater than or equal to /u1D716, we can reuse\nmodelM/u1D447trainedonD/u1D447withoutre-training.\nTo showcase the applicability of our agile model reuse tech-\nnique,weintegrateitintotheRMIlearnedindex [10].Wesho w\nthatagilemodelreusecansigniﬁcantlyreducethetraining time\nof the sub-models in RMI. We then propose a new index struc-\nturenamed recursivemodelreusetree (RMRT)withbuilt-inagile\nmodelreuse support.RMRTis designed tobeadaptivetodiﬀer -\nentdatadistributions:itbuildssub-modelswithmorelaye rsfor\nmore dense regions of a dataset. This is particularly useful for\nskewed data,which has not beenaddressed inRMI.\nIn summary,ourkey contributionsare:\n(1) We propose an agile model reuse technique to pre-train\na set of models on synthetic datasets and adaptively se-\nlect the most suitablemodel to index a new dataset with\nrespect to a model reuse threshold /u1D716. We show how to\nboundtheadditionalmodelpredictionerrorgiven /u1D716.\n(2) We propose a new index structure named RMRT, which\nhasbuilt-inagilemodelreusesupportandadaptivelybuild s\nan unbalanced hierarchical structure for better indexing\nofskewed data.\n(3) Extensiveexperimentsonsyntheticandrealdatasetssh ow\nthatagilemodelreusecanacceleratethebuildingtimeof\nneural network-based learned indices by two orders of\nmagnitude,whileretainingthelookupeﬃciency.Further,\nour agile model reuse based index RMRT is faster than\nRMI-based structures to build, while it outruns all base-\nlinemodelsin lookupperformance.\n2 RELATED WORK\nAlearnedindex[3–6,10,11]learnsamappingfromasearchke y\nto the storage address of a data point with a machine learning\nmodel.Due to limits onthe learning capacity of a single mode l,\nexisting learned indices such as RMI [10] build a hierarchy o f\nmodelstoindexlargedatasets.Theideaissimilartothatof tradi-\ntionalhierarchicalindices:top-levelmodelspredictpar titionsof\nthedatapoints(i.e.,thelower-level modelinwhichapoint isin-\ndexed),whileleaf-levelmodelspredictthestoragelocati ons.The\ntrainingandupdatesofahierarchical learnedindex canbev ery\nexpensive,especiallywhenneuralnetworksareused.Follo w-up\nstudies aim to bound the prediction error of the learned mode l.\nForexample,PGM[5]buildsahierarchicallearnedindexbot tom\nup,withaworst-casepredictionerrorbound /u1D716oneverylearned\nmodel.Thebuilding timeofsuch learned indices is alsohigh .\nUpdatehandling. Updatesmaychangethedatadistribution\nfrom which an index model is learned and amplify the model\nprediction error. Existing studies have focused on handlin g in-\nsertions,sincepointsdeletedcanbesimplyﬂaggedas“remo ved”\nwith a light impact on query processing. For query correctne ss,\nonemayupdatethepredictionerrorboundsto /u1D452/u1D45F/u1D45F/u1D459−/u1D456and/u1D452/u1D45F/u1D45F/u1D462+/u1D456\nafter/u1D456insertions. Tighterboundsareachieved bykeeping track\nof the error bound drifts for a number of reference points [7].\nAt query time, the closest reference points on both sides of t he\nquerypointarefetched,andtheirerrorbounddriftsareuse dto\nestimate the updated error bounds with a linear interpolati on.\nPGM [5] uses two diﬀerent strategies to handle insertions. F ortime series data insertion, it can either add a new point to th e\nlastmodeloraddanewmodeltohandlethenewpoint.Forarbi-\ntraryinsertion,itappliesthe logarithmicmethod [14]andbuilds\naseriesofPGMindicesfortheinsertions.Alltheseindices need\nextra structuresto handle updates,which impact thequerye ﬃ-\nciency.\nDomain adaptation. The idea of domain adaptation is to\nadapt a model pre-trained on a dataset D/u1D446for a new problem\nwith a diﬀerent dataset D/u1D447. A key step is to measure the simi-\nlaritybetween thedistributionsof D/u1D446andD/u1D447.The/u1D43F1distance\nis a often used [12].It does not suit our problem becauseit ca n-\nnot help bound the index prediction error on D/u1D447. Thediscrep-\nancy[2] is another a measure. It is designed based on testing\nwhether the training loss diﬀers signiﬁcantly on D/u1D446andD/u1D447.\nThis is inapplicablebecausewerequire ahighly eﬃcient tes t to\ndetermine online whether a model can be reused for D/u1D447. Typi-\ncaldomainadaptationtechniquesalsoﬁne-tunethepre-tra ined\nmodel onD/u1D447, while we skip this step for eﬃciency considera-\ntions.\n3 AGILEMODEL REUSE\nGivenaneworupdateddataset D/u1D447,weaimtoconstructalearned\nindexM/u1D447forD/u1D447with a high eﬃciency.\nWe ﬁrst present an overview of our agile model reuse tech-\nnique.Wethendetailitskeycomponents,includingdataset simi-\nlaritymeasurement,syntheticdatasetgeneration,modela dapta-\ntion,anderrorbounding.Wewillalsoshowcasetheapplicab ility\nofourtechniqueonanexisting and anovel learned indices.\nAlgorithm 1: AgileModelReuse\nInput:D/u1D447,Q/u1D440/u1D443\nOutput:M/u1D447\n1for<D/u1D446,M/u1D446>∈Q/u1D440/u1D443do\n2/u1D451/u1D456/u1D460/u1D461←/u1D450/u1D44E/u1D459_/u1D451/u1D456/u1D460/u1D461/u1D44E/u1D45B/u1D450/u1D452(D/u1D446,D/u1D447);\n3if/u1D451/u1D456/u1D460/u1D461≤1−/u1D716then\n4M/u1D447←/u1D44E/u1D451/u1D44E/u1D45D/u1D461_/u1D45A/u1D45C/u1D451/u1D452/u1D459(M/u1D446,D/u1D446,D/u1D447);\n5 returnM/u1D447;\n6TrainmodelM/u1D447overD/u1D447;\n7M/u1D447./u1D45A/u1D44E/u1D465_/u1D44E/u1D44F/u1D460_/u1D452/u1D45F/u1D45F←M/u1D447.calc_err(D/u1D447);\n8Q/u1D440/u1D443./u1D452/u1D45B/u1D45E/u1D462/u1D452/u1D462/u1D452(<D/u1D447,M/u1D447>,M/u1D447./u1D45A/u1D44E/u1D465_/u1D44E/u1D44F/u1D460_/u1D452/u1D45F/u1D45F);\n9returnM/u1D447;\nAgile model reuseoverview. Algorithm 1 summarizes our\nagile(i.e., fastandﬂexible) modelreusetechnique.Wepre -train\nmodels on synthetic datasets (detailed later) which are reu sed\ntoindexD/u1D447.Thepre-trained modelsareorganizedinapriority\nqueueQ/u1D440/u1D443. Each entry inQ/u1D440/u1D443contains the information of a\nsynthetic datasetD/u1D446and its corresponding trained model M/u1D446.\nThe trained models are sorted by their error bounds in ascend -\ningorder.Algorithm1traverses Q/u1D440/u1D443(line1),calculatesthedis-\ntance(dissimilarity)between D/u1D447andeachsyntheticdataset D/u1D446\n(line 2), and ﬁnds the ﬁrst model where the distance is smalle r\nthan or equal to the model reuse threshold /u1D716∈ (0,1](line 3).\nIf such a model is found, the model and its error bounds are\nadaptedbasedonthedatasetdistance(line4,detailedlate r),and\nthe adapted model is returned as M/u1D447(line 5). Otherwise, we\ntrainanewmodelM/u1D447forD/u1D447(line6)andobtaintheerrorrange\n(/u1D452/u1D45F/u1D45F/u1D462−/u1D452/u1D45F/u1D45F/u1D459, line 7). We enqueue and return the model (lines 8\nand 9).\nWe use/u1D716to control the dataset similarity in model reuse. A\nsmaller/u1D716allows the algorithm to return a model earlier, which\nmay not have a high similarity with D/u1D447and may lead to low\nprediction accuracy and high query costs. In contrast, a lar ger\n/u1D716can cost more time in traversing Q/u1D440/u1D443but gain a more ﬁtted\n\nmodel with high prediction accuracy and low query costs.As /u1D716\nincreases in range (0,1], the requirement for agile model reuse\nis getting higher. Weelaborateontheeﬀect of /u1D716inSection5.\nDatasetsimilaritymeasurement. AmodelM/u1D446fordataset\nD/u1D446eﬀectively learns a CDF of D/u1D446. To reuseM/u1D446onD/u1D447, it is\nimportant that the CDFs of D/u1D446andD/u1D447are similar. We thus\ndeﬁne thesimilaritybetween D/u1D446andD/u1D447bytheir CDFs.\nDeﬁnition3.1(Similaritybetweentwodatasets). Giventwodatasets\nD/u1D446andD/u1D447,theirsimilarity isdeﬁnedbythemaximumdistance\nbetween their CDFs:\n/u1D460/u1D456/u1D45A(D/u1D446,D/u1D447)=1−sup\n/u1D465|/u1D450/u1D451/u1D453/u1D446(/u1D465)−/u1D450/u1D451/u1D453/u1D447(/u1D465)| (1)\nHere, sup/u1D465|/u1D450/u1D451/u1D453/u1D446(/u1D465)−/u1D450/u1D451/u1D453/u1D447(/u1D465)|is the maximum gap between\n/u1D450/u1D451/u1D453/u1D446(/u1D465)and/u1D450/u1D451/u1D453/u1D447(/u1D465).Weuse/u1D460/u1D456/u1D45A(D/u1D446,D/u1D447)and/u1D451/u1D456/u1D460/u1D461(D/u1D446,D/u1D447)=\n1−/u1D460/u1D456/u1D45A(D/u1D446,D/u1D447)to denote the similarity and the distance (dis-\nsimilarity) between D/u1D446andD/u1D447,respectively.\nThissimilaritymetricisalsobasedonthe Kolmogorov–Smirnov\n(KS) test [9], which takes /u1D442(|D/u1D446|+|D/u1D447|)time to compute, as-\nsuming that both datasets are sorted already. This may be too\nexpensiveforonlinecomputationforlargedatasets.Wepre sent\nan approximatesimilarity metric forfaster computation.\nOurapproximatesimilaritymetricuses relativefrequencyhis-\ntograms(“histograms”forshort)thatdiscretizethedatadomain\nintobinsandrecordrelativefrequencies(i.e.,percentag es)ofthe\ndatapointsineachbin.Ahistogramisadiscreteapproximat ion\nof theprobability density function (PDF) of a dataset. We use it\ntocomputeanapproximationoftheCDFandtocomputeanap-\nproximationof /u1D451/u1D456/u1D460/u1D461(D/u1D446,D/u1D447), denotedby /u1D451/u1D456/u1D460/u1D461ℎ(D/u1D446,D/u1D447).\nAlgorithm2summarizesthecomputationprocess,whichtake s\nasinputhistogramsof D/u1D446andD/u1D447with/u1D45A(asystemparameter)\nbins each, denoted by /u1D43B/u1D446and/u1D43B/u1D447. We use /u1D43B/u1D446[/u1D456]and/u1D43B/u1D447[/u1D456]to\ndenote the /u1D456-th bins and their relative frequencies. The sum of\nthe probabilities of ﬁrst /u1D456bins of/u1D43B/u1D446and/u1D43B/u1D447are denoted by /u1D443/u1D446\nand/u1D443/u1D447,i.e.,/u1D443/u1D446=/summationtext.1/u1D456\n/u1D457=0/u1D43B/u1D446[/u1D456]and/u1D443/u1D447=/summationtext.1/u1D456\n/u1D457=0/u1D43B/u1D447[/u1D456].\nThe algorithm computes /u1D451/u1D456/u1D460/u1D461ℎ(D/u1D446,D/u1D447)(/u1D451/u1D456/u1D460/u1D461ℎfor short) by\nlooping through the bins (lines 2 to 4). In the /u1D456-th iteration ( /u1D456∈\n[0,/u1D45A−1]),itcomputes /u1D43B/u1D446[/u1D456]+/u1D443/u1D446.Thisisthemaximum /u1D450/u1D451/u1D453/u1D446(/u1D465)\nfor any/u1D465∈(/u1D456\n/u1D45A,/u1D456+1\n/u1D45A](in our synthetic datasets, /u1D465∈[0,1]), be-\ncause/u1D443/u1D446has accumulated the probabilities for /u1D465≤/u1D456\n/u1D45Awhile\n/u1D43B/u1D446[/u1D456]furtheradds theprobabilityfor /u1D465∈(/u1D456\n/u1D45A,/u1D456+1\n/u1D45A]. Meanwhile,\n/u1D443/u1D447is the minimum /u1D450/u1D451/u1D453/u1D447(/u1D465)for any/u1D465∈(/u1D456\n/u1D45A,/u1D456+1\n/u1D45A]. Thus,∀/u1D465∈\n(/u1D456\n/u1D45A,/u1D456+1\n/u1D45A]:\n/u1D43B/u1D446[/u1D456]+/u1D443/u1D446−/u1D443/u1D447≥/u1D450/u1D451/u1D453/u1D446(/u1D465)−/u1D450/u1D451/u1D453/u1D447(/u1D465)\n/u1D43B/u1D447[/u1D456]+/u1D443/u1D447−/u1D443/u1D446≥/u1D450/u1D451/u1D453/u1D447(/u1D465)−/u1D450/u1D451/u1D453/u1D446(/u1D465)(2)\nAfter going through allbins, wehave:\n/u1D451/u1D456/u1D460/u1D461ℎ(D/u1D446,D/u1D447)≥|/u1D450/u1D451/u1D453/u1D447(/u1D465)−/u1D450/u1D451/u1D453/u1D446(/u1D465)|,∀/u1D465∈(0,1](3)\nThus,/u1D451/u1D456/u1D460/u1D461ℎ(D/u1D446,D/u1D447)≥/u1D451/u1D456/u1D460/u1D461(D/u1D446,D/u1D447).\nAlgorithm 2: Histogram-based-Distance\nInput:/u1D43B/u1D446,/u1D43B/u1D447\nOutput:/u1D451/u1D456/u1D460/u1D461ℎ\n1/u1D451/u1D456/u1D460/u1D461ℎ←0,/u1D443/u1D446←0,/u1D443/u1D447←0;\n2for/u1D456∈[0,/u1D45A−1]do\n3/u1D451/u1D456/u1D460/u1D461ℎ←max{/u1D43B/u1D446[/u1D456]+/u1D443/u1D446−/u1D443/u1D447,/u1D43B/u1D447[/u1D456]+/u1D443/u1D447−/u1D443/u1D446,/u1D451/u1D456/u1D460/u1D461ℎ};\n4/u1D443/u1D446←/u1D443/u1D446+/u1D43B/u1D446[/u1D456],/u1D443/u1D447←/u1D443/u1D447+/u1D43B/u1D447[/u1D456];\n5return/u1D451/u1D456/u1D460/u1D461ℎ;\nUsing histograms to discretize CDFs reduces the similarity\ncomputationtimeto /u1D442(log|D/u1D447|+/u1D45A), i.e.,/u1D442(log|D/u1D447|)timefor\n/u1D43B/u1D447computationand /u1D442(/u1D45A)timeforAlgorithm2.Histogram /u1D43B/u1D446\nis pre-computedsince D/u1D446is known. Its costis omittedhere.Synthetic dataset generation. We aim to generate a small\nnumber of datasets with CDFs that can be similar to those of a\nlargenumber ofreal datasets,as boundedbythreshold /u1D716.\nWeﬁrstgenerate aset ofCDFs tocover thespaceofpossible\nCDFs. We discretize the CDF space, such that it can be covered\nby limited CDFs given threshold /u1D716. As shown in Fig. 2a, after\ndatanormalization,allCDFs lieina [0,1]2space. AnyCDF can\nbe seen as a curve that starts at (0,0)and travels to(1,1)in a\nnon-deceasing manner (in the CDF value dimension). We dis-\ncretethisspacewithagrid,whereeachrowhasaheightof1 −/u1D716\n(/u1D716=0.8intheﬁgure),andeachcolumnhasawidthof ⌈1/(1−/u1D716)⌉.\nConsiderthesetLofpolylineseachstartingfrom (0,0)andtrav-\neling to(1,1)via the grid vertices in a non-deceasing manner\n(in the CDF value dimension, e.g., the colored lines). Strai ght-\nforwardly. given any CDF, there must be a polyline /u1D459∈Lsuch\nthatthedistancebetween /u1D459and theCDF is boundedby1 −/u1D716(cf.\nFig. 2b).\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized key0.00.20.40.60.81.0CDF value\n(a) CDFsof syntheticdata0.00.20.40.60.81.0\nNormalized key0.00.20.40.60.81.0\namzn\namzn-syn\nosm\nosm-syn\nwiki\nwiki-syn\n(b) CDFsofreal andsyntheticdata\nFigure2: CDFspacediscretization.\nTheCDFsinLcorrespondtohistogramswith /u1D45A=⌈1/(1−/u1D716)⌉\nwhereeachbinhasaprobabilityvaluein {0,1−/u1D716,2(1−/u1D716),...,1}.\nTo limit the bin value combinations and hence the number of\nCDFs(syntheticdatasets)generated,welimittheprobabil ityvalue\nof each bin to be within {0,(1−/u1D716)/2,1−/u1D716}, and we use /u1D45A=\n⌈2/(1−/u1D716)⌉bins in the histogram heuristically. Our synthetic\ndatasets hence will not cover the most skewed CDFs (e.g., the\nblack polylines in Fig. 2a). However, when a target dataset D/u1D447\nis matched by a synthetic dataset, their CDF similarity may b e\nwithin(1−/u1D716)/2ratherthan1−/u1D716,whichimprovesthequeryper-\nformance.Ourtotalnumberofhistogramsgeneratedis:/summationtext.1/u1D45A\n/u1D456=0(/u1D436/u1D456/u1D45A·\n/u1D436⌊(1−/u1D456(1−/u1D716))/((1−/u1D716)/2)⌋\n/u1D45A−/u1D456), where the two combination terms rep-\nresent the numbers of bins with probability values of (1−/u1D716)\nand(1−/u1D716)/2, respectively. Once a histogram is generated, we\ngenerate a synthetic dataset of /u1D45B/u1D460key values ( /u1D45B/u1D460=100 in our\nexperiments) based on the histogram, where the data range is\n[0,1], and random key values are generated for each bin. This\nprocedureis shownthebeeﬀective and eﬃcient empirically.\nModel adaptation. When a modelM/u1D446pre-trained onD/u1D446\nhas been selected to index D/u1D447, we need to adapt M/u1D446based on\nthe data domains of D/u1D446andD/u1D447. This is becauseM/u1D446will not\nwork properlyona domainover which itwas not trained,even\nif the CDFs ofD/u1D446andD/u1D447share a similar shape. Let the data\nranges ofD/u1D446andD/u1D447be[/u1D465/u1D460\n/u1D446,/u1D465/u1D452\n/u1D446]and[/u1D465/u1D460\n/u1D447,/u1D465/u1D452\n/u1D447], and their data\nstorage position ranges be [/u1D466.alt/u1D460\n/u1D446,/u1D466.alt/u1D452\n/u1D446]and[/u1D466.alt/u1D460\n/u1D447,/u1D466.alt/u1D452\n/u1D447], respectively.\nModelM/u1D446has been trained to take a search key in [/u1D465/u1D460\n/u1D446,/u1D465/u1D452\n/u1D446]as\nthe input and predict an storage position in [/u1D466.alt/u1D460\n/u1D446,/u1D466.alt/u1D452\n/u1D446]. Here, we\nassumethatM/u1D446predictsthestoragepositionofpoint /u1D45Ddirectly\nrather than its rank (or percentile), i.e., /u1D45D./u1D44E/u1D451/u1D451/u1D45F≈ M/u1D446(/u1D45D./u1D458/u1D452/u1D466.alt)\n(instead ofM/u1D446(/u1D45D./u1D458/u1D452/u1D466.alt)·|D/u1D446|as shown in Section 1). This sim-\npliﬁes the discussion but does not impact our key ﬁndings. To\nadaptM/u1D446forD/u1D447, wetake a search key in [/u1D465/u1D460\n/u1D447,/u1D465/u1D452\n/u1D447], map it into\n[/u1D465/u1D460\n/u1D446,/u1D465/u1D452\n/u1D446],andfeedthemappedvalueinto M/u1D446forprediction.The\npredictedoutputneedstobemappedbackinto [/u1D466.alt/u1D460\n/u1D447,/u1D466.alt/u1D452\n/u1D447]forD/u1D447.\n\nLet/u1D446Δ/u1D465=/u1D465/u1D452\n/u1D446−/u1D465/u1D460\n/u1D446\n/u1D465/u1D452\n/u1D447−/u1D465/u1D460\n/u1D447and/u1D446Δ/u1D466.alt=/u1D466.alt/u1D452\n/u1D447−/u1D466.alt/u1D460\n/u1D447\n/u1D466.alt/u1D452\n/u1D446−/u1D466.alt/u1D460\n/u1D446. The input mapping\nis done by a linear transformation T/u1D456/u1D45B(/u1D465)=/u1D44E1·/u1D465+/u1D44F1where\n/u1D44E1=/u1D446Δ/u1D465and/u1D44F1=/u1D465/u1D460\n/u1D446−/u1D465/u1D460\n/u1D447·/u1D446Δ/u1D465.Thisisanaﬃnetransformation\nthat maps the data range (i.e., T/u1D456/u1D45B(/u1D465/u1D460\n/u1D447)=/u1D465/u1D460\n/u1D446andT/u1D456/u1D45B(/u1D465/u1D452\n/u1D447)=/u1D465/u1D452\n/u1D446)\nwithout changing the distribution. Similarly, the output m ap-\nping is done byT/u1D45C/u1D462/u1D461(/u1D466.alt)=/u1D44E2·/u1D466.alt+/u1D44F2where/u1D44E2=/u1D446Δ/u1D466.altand\n/u1D44F2=/u1D466.alt/u1D460\n/u1D447−/u1D466.alt/u1D460\n/u1D446·/u1D446Δ/u1D466.alt. The mappings may incur extra costs (ﬂoat-\ning point calculation), which can be mitigated by adjusting the\nparameters ofM/u1D446.We usealinear modelas anexample.\nL/e.sc/m.sc/m.sc/a.sc3.2. Inputandoutputmappingsforalinearmodel M/u1D446\nincurnoadditionalpredictioncosts.\nP/r.sc/o.sc/o.sc/f.sc.LetM/u1D446be a linear model /u1D466.alt=/u1D44E/u1D465+/u1D44F, where/u1D44Eand/u1D44F\nare parameters.Theoutput ˜/u1D466.alt′ofM/u1D446is (withinput mapping):\n˜/u1D466.alt′=M/u1D446/parenleftbigT/u1D456/u1D45B(/u1D465)/parenrightbig=/u1D44E·T/u1D456/u1D45B(/u1D465)+/u1D44F\n=/u1D44E·/u1D446Δ/u1D465·/u1D465−/u1D44E·/u1D465/u1D460\n/u1D447·/u1D446Δ/u1D465+/u1D44E·/u1D465/u1D460\n/u1D446+/u1D44F(4)\nAfter outputmapping, theﬁnal predictionoutput ˜ /u1D466.altis:\n˜/u1D466.alt=T/u1D45C/u1D462/u1D461(˜/u1D466.alt′)=(˜/u1D466.alt′−/u1D466.alt/u1D460\n/u1D446)·/u1D446Δ/u1D466.alt+/u1D466.alt/u1D460\n/u1D447=/u1D44E·/u1D446Δ/u1D465·/u1D446Δ/u1D466.alt·/u1D465\n+(−/u1D44E·/u1D465/u1D460\n/u1D447·/u1D446Δ/u1D465+/u1D44E·/u1D465/u1D460\n/u1D446+/u1D44F−/u1D466.alt/u1D460\n/u1D446)·/u1D446Δ/u1D466.alt+/u1D466.alt/u1D460\n/u1D447(5)\nThus, we can adapt M/u1D446to a new linear model /u1D466.alt=/u1D44E′/u1D465+/u1D44F′for\nD/u1D447where/u1D44E′=/u1D44E·/u1D446Δ/u1D465·/u1D446Δ/u1D466.altand/u1D44F′=(−/u1D44E·/u1D465/u1D460\n/u1D447·/u1D446Δ/u1D465+/u1D44E·/u1D465/u1D460\n/u1D446+\n/u1D44F−/u1D466.alt/u1D460\n/u1D446)·/u1D446Δ/u1D466.alt+/u1D466.alt/u1D460\n/u1D447.Inputandoutputmappings canbecombined\nwith linear modelswithoutextra predictioncosts. /square\nSimilar results can be derived for other models (e.g., neura l\nmodels).Weomitthedetails duetothespace limit.\nErrorbounding. Recallthepredictionerrorsof M/u1D446overD/u1D446,\n/u1D452/u1D45F/u1D45F/u1D459and/u1D452/u1D45F/u1D45F/u1D462. Given a query key /u1D465∈D/u1D446, the position of /u1D465is\nboundedin[M/u1D446(/u1D465)+/u1D452/u1D45F/u1D45F/u1D459,M/u1D446(/u1D465)+/u1D452/u1D45F/u1D45F/u1D462].Afterinputandoutput\nmappings, we alsoneed toadjusttheerror boundsfor D/u1D447.\nT/h.sc/e.sc/o.sc/r.sc/e.sc/m.sc 3.3. LetM/u1D446be a model trained on D/u1D446with predic-\ntion error bounds /u1D452/u1D45F/u1D45F/u1D459and/u1D452/u1D45F/u1D45F/u1D462. Let/u1D451/u1D456/u1D460/u1D461be the distance between\nD/u1D446andD/u1D447.Theerror boundsof M/u1D446overD/u1D447of size/u1D45B/u1D447are:\n/u1D452/u1D45F/u1D45F′\n/u1D459=−/u1D451/u1D456/u1D460/u1D461·/u1D45B/u1D447+/u1D452/u1D45F/u1D45F/u1D459·/u1D446Δ/u1D466.alt (6)\n/u1D452/u1D45F/u1D45F′\n/u1D462=/u1D451/u1D456/u1D460/u1D461·/u1D45B/u1D447+/u1D452/u1D45F/u1D45F/u1D462·/u1D446Δ/u1D466.alt (7)\nP/r.sc/o.sc/o.sc/f.sc.For any/u1D465∈ D/u1D447and anyT/u1D456/u1D45B(/u1D465) ∈ D/u1D446, let/u1D466.altand\n/u1D466.alt′bethestorage positionsof the correspondingdata points,r e-\nspectively.Then,M/u1D446/parenleftbigT/u1D456/u1D45B(/u1D465)/parenrightbigis boundedby:\n/u1D466.alt′−/u1D452/u1D45F/u1D45F/u1D459≥M/u1D446/parenleftbigT/u1D456/u1D45B(/u1D465)/parenrightbig(8)\nAfter output mapping, T/u1D45C/u1D462/u1D461/parenleftbigM/u1D446/parenleftbigT/u1D456/u1D45B(/u1D465)/parenrightbig/parenrightbigis the predicted posi-\ntionofM/u1D446overD/u1D447, which is boundedby:\n/u1D466.alt−/u1D452/u1D45F/u1D45F′\n/u1D459≥T/u1D45C/u1D462/u1D461/parenleftBig\nM/u1D446/parenleftbigT/u1D456/u1D45B(/u1D465)/parenrightbig/parenrightBig\n(9)\nSince mappings aremonotonic,Inequality(8) is rewritten a s:\nT/u1D45C/u1D462/u1D461(/u1D466.alt′−/u1D452/u1D45F/u1D45F/u1D459)=T/u1D45C/u1D462/u1D461(/u1D466.alt′)−T/u1D45C/u1D462/u1D461(/u1D452/u1D45F/u1D45F/u1D459)+/u1D44F′≥T/u1D45C/u1D462/u1D461/parenleftBig\nM/u1D446/parenleftbigT/u1D456/u1D45B(/u1D465)/parenrightbig/parenrightBig\n(10)\nwhere/u1D44F′istheinterceptionof T/u1D45C/u1D462/u1D461(·).Given/u1D451/u1D456/u1D460/u1D461asthedistance\nbetweenD/u1D447andD/u1D446,afterinputandoutputmappings,wehave\n|T/u1D45C/u1D462/u1D461(/u1D466.alt′)−/u1D466.alt|≤/u1D451/u1D456/u1D460/u1D461·/u1D45B/u1D447,i.e.,/u1D466.alt+/u1D451/u1D456/u1D460/u1D461·/u1D45B/u1D447≥T/u1D45C/u1D462/u1D461(/u1D466.alt′).Combining\nwith Inequality (10), we have /u1D466.alt+/u1D451/u1D456/u1D460/u1D461·/u1D45B/u1D447−T/u1D45C/u1D462/u1D461(/u1D452/u1D45F/u1D45F/u1D459)+/u1D44F′≥\nT/u1D45C/u1D462/u1D461/parenleftBig\nM/u1D446/parenleftbigT/u1D456/u1D45B(/u1D465)/parenrightbig/parenrightBig\n.Giventhisinequality,tosatisfyInequality(9),\nwe enforce /u1D466.alt−/u1D452/u1D45F/u1D45F′\n/u1D459≥/u1D466.alt+/u1D451/u1D456/u1D460/u1D461·/u1D45B/u1D447−T/u1D45C/u1D462/u1D461(/u1D452/u1D45F/u1D45F/u1D459)+/u1D44F′. Thus,\n/u1D452/u1D45F/u1D45F′\n/u1D459≤−/u1D451/u1D456/u1D460/u1D461·/u1D45B/u1D447+/u1D452/u1D45F/u1D45F/u1D459·/u1D446Δ/u1D466.alt, where/u1D446Δ/u1D466.altis the slopeofT/u1D45C/u1D462/u1D461(·).\nLetting/u1D452/u1D45F/u1D45F′\n/u1D459=−/u1D451/u1D456/u1D460/u1D461·/u1D45B/u1D447+/u1D452/u1D45F/u1D45F/u1D459·/u1D446Δ/u1D466.altwillensurequerycorrectness.\nSimilarly, wecanderive /u1D452/u1D45F/u1D45F′/u1D462=/u1D451/u1D456/u1D460/u1D461·/u1D45B/u1D447+/u1D452/u1D45F/u1D45F/u1D462·/u1D446Δ/u1D466.alt./square\nLearnedindiceswithagilemodelreuse. Weshowcasethe\napplicability of agile model reuse over existing learned in dices\nby building a two-layer RMI [10], as shown in Fig. 3. We ﬁrst\ncompute /u1D460/u1D456/u1D45A(D1\n/u1D446,D0,0)between the full dataset D0,0to be in-\ndexedandasyntheticdataset D1\n/u1D446,whichhasapre-trainedmodelFigure3: Building RMI with agilemodelreuse( /u1D716=0.8).\nM1\n/u1D446inQ/u1D440/u1D443.Suppose /u1D460/u1D456/u1D45A(D1\n/u1D446,D0,0)=0.9≥/u1D716=0.8.Then,M1\n/u1D446\nis reused overD0,0. ForD1,0, a subset to be indexed by a child\nmodelinRMI,wecannotﬁndasynthetic datasetwithasimilar -\nitygreaterthanorequalto0.8.Wetrainamodel M1,0overD1,0\nandputitintoQ/u1D440/u1D443forreuselater.Fortheothersubset D1,1,we\nﬁnd another synthetic dataset D2\n/u1D446with/u1D460/u1D456/u1D45A(D2\n/u1D446,D1,1)=0.85≥\n0.8.ItsmodelM2\n/u1D446isreusedoverD1,1,whichcompletestheRMI.\nRecursiveModelReuseTree. InRMI,thenumberoflayers\nand the number of models in each layer is ﬁxed. If the data is\nskewed,thecardinalityofthesubsetsassigned todiﬀerent mod-\nelscanvaryconsiderably,resultinginhighpredictionerr orsand\nsearch costsonsomemodels.Toaddress this issue, wedesign a\nlearned index structure with built-in agile model reuse sup port\nnamed the recursivemodelreuse tree (RMRT).\nSupposethat the models used in RMRT have the same learn-\ningcapacity(e.g.,neuralnetworksofthesamestructure), which\ncanﬁtatmost /u1D441pointseach.When |D/u1D447|is greater than /u1D441,we\nﬁrst learn a model M0,0to predict the points into /u1D435partitions\nwhere/u1D435isasystemparameter.Then,recursively,forpointspre-\ndicted to partition /u1D456, we learn another model M1,/u1D456to partition\nthem.Thisprocesscontinues,untileachpartitionhasatmo st/u1D441\npoints,which is indexed by a learned model.Agile modelreus e\nis applied whenever a model is needed in this process. Fig. 4a\ngives an example with /u1D441=4 and/u1D435=2. ModelM0,0predicts\ntwo partitions(i.e., subsets) D1,0andD1,1that contain theﬁrst\nfourand the lasteight points,respectively. Further parti tioning\nis needed forD1,1. A modelM1,1is learned for this, creating\ntwopartitions ofsize /u1D441each. Thepartitioning thenstops.\n(a) RMRT( /u1D441=4,/u1D435=2) (b) Insertionhandling\nFigure4: RMRT andinsertion handling examples\n4 UPDATEHANDLING\nWefocusoninsertions.Deletionscanbeimplementedsimply as\na pointqueryand marking thequeriedpointas “deleted”.\nTo handle insertions, we examine their impact on the CDF.\nAs shown in Fig. 4b, when data point 5 is inserted, only the\nCDF ofD1,0is impacted in the second layer of the recursive\nmodel. ForD1,1which is not impacted, we can simply add 1 to\nitsmodelerrorbounds.For D1,0,wehavetocheck whetherthe\nreused modelM1,0can still meet the similarity bound deﬁned\nby threshold /u1D716. To enable eﬃcient checks, we propose a bound\nonthemaximumnumberofinsertionswithoutrequiringmodel\nupdates.\nL/e.sc/m.sc/m.sc/a.sc 4.1. LetDbe a dataset with cardinality /u1D45BDandM/u1D437\nbe a model overD. Let/u1D460/u1D456/u1D45Abe the similarity between Dand the\ndatasetfromwhich M/u1D437istrained,whichcanbe Dorasynthetic\n\ndataset.Iftherearelessthan(/u1D460/u1D456/u1D45A−/u1D716)\n1+/u1D716−/u1D460/u1D456/u1D45A·/u1D45BDinsertionsonD,wecan\nstill reuse modelM/u1D437for the resultant dataset D′.\nP/r.sc/o.sc/o.sc/f.sc.After/u1D45B/u1D456insertions, the CDFs /u1D450/u1D451/u1D453/u1D437(/u1D465)and/u1D450/u1D451/u1D453/u1D437′(/u1D465)\nmay become diﬀerent. In the worst case, all new data points\nare inserted at thesameposition,where thediﬀerence betwe en\n/u1D450/u1D451/u1D453/u1D437(/u1D465)and/u1D450/u1D451/u1D453/u1D437′(/u1D465)is bounded by /u1D451/u1D456/u1D460/u1D461(D,D′)≤/u1D45B/u1D456\n/u1D45B/u1D456+/u1D45BD. Re-\ncallthatM/u1D437isreusedwith /u1D460/u1D456/u1D45A≥/u1D716ortrainedonD(/u1D460/u1D456/u1D45A=1≥\n/u1D716). We can use the gap /u1D460/u1D456/u1D45A−/u1D716as a buﬀer to accommodate the\nCDFdriftcausedbytheinsertions.Accordingtothetransit ivity\nof inequalities, if/u1D45B/u1D456\n/u1D45B/u1D456+/u1D45BD≤/u1D460/u1D456/u1D45A−/u1D716, thereis no need to rebuild a\nmodel overD′, since the impact on the CDF of Dby the inser-\ntionscannotexceedtheerrorbound /u1D460/u1D456/u1D45A−/u1D716.Thus,wecanderive\naboundonthenumberofinsertions /u1D45B/u1D456as/u1D45B/u1D456≤(/u1D460/u1D456/u1D45A−/u1D716)\n1+/u1D716−/u1D460/u1D456/u1D45A·/u1D45BD/square\nAccording to Lemma 4.1, insertions can be handled without\nmodel rebuilt when their number does not exceed the bound.\nWhen a new data point is inserted, we ﬁnd the target insertion\nposition through a point query and obtain the corresponding\nmodelM.Ifthenumberofinsertionson Mhasnotexceededthe\nbound, the insertion is completed. Otherwise, we only rebui ld\nthemodelindexing theinserted datapoint.\n5 EXPERIMENTS\nAll experiments are done on a 64-bit machine with a 3.60 GHz\nIntel i9 CPU, RTX 2080Ti GPU, 64 GB RAM, and a 1 TB hard\ndisk.Weuse PyTorch1.4[15]anditsC++APIstoimplementthe\nlearned indices based on GPU. The linear and neural network\nmodels are implemented using Scikit-learn [17] andPyTorch,re-\nspectively.\nCompetitors. Wecomparewithbothtraditionalandlearned\nindices:1)traditional BTree[18]whichisaC++basedin-memory\nB+treefromtheSTXB+Treepackage;2) RMI[10]whichisthe\nlinear RMI model from the SOSD benchmark [13]; 3) RMI-NN\nwhich is ourimplementation oftheneural network RMImodel;\n4)PGM[5] which is a piecewise geometric model index; and\n5)RS[8]which is asingle-pass learned index.\nProposed models. We studythe performance of the follow-\ning proposed and adapted models:1) RMI-MR which is the lin-\nearRMImodelenhancedbyagilemodelreuse;2) RMI-NN-MR\nwhich is the neural RMI model enhanced by agile model reuse;\nand 3)RMRTwhich is ourproposedlearned index.\nImplementationdetails. ForBTree,RMI,PGM,and RS,we\nuse their published source code and default conﬁgurations. For\nRMI-MR, we adapt the original model training code to include\nagile model reuse. For neural network based models includin g\nRMI-NN, RMI-NN-MR and RMRT, we use feedforward neural\nnetworks each with one hidden layer of four neurons. We use\n/u1D441=106astheRMRTmodelsizethreshold,whichshowsstrong\nempirical performance. Weset thedefaultvaluefor /u1D716to0.9.\nWe summarize the number of synthetic datasets (each with\n100points) andthetimetopre-trainmodelsontheminTable2 .\nNotethatweuse /u1D45A=12<⌈2/(1−/u1D716)⌉when/u1D716=0.9.Asaresult,\nthe number of datasets generated is bounded in 10,000; all pr e-\ntrainedmodelscanbeloadedinmemorywithinasecond(30MB\nin size); and the total model comparison time to build an inde x\nin any oftheexperiments is alsowithina second.\nTable 2: Summaryof SyntheticDatasets.\n/u1D716 0.50.60.70.80.9\nNumberof bins( /u1D45A) 4571012\nNumberof datasets 19959878,9531,221\nTotalmodel training time (s) 2.18.863.5839.5109.1Datasets. Following SOSD [13], we use four real datasets:\namzn(default) – an Amazon book popularity dataset, face–\na Facebook user ID dataset, osm– an OpenStreetMap cell ID\ndataset,and wiki–aWikipediaedittimestampdataset.Wefur-\nther generate skew datasets from uniform databy raising a ke y\nvalue/u1D465to its powers /u1D465/u1D6FC(/u1D6FC=1,3,5,7,9), following [16]. Each\ndatasetcontains 200millionunsigned 64-bitinteger keys.\nPerformancemetrics. Wereporttheindexbuildtime,lookup\n(i.e., pointquery) time,and update(insertion) time.\n5.1 Results\nIndexbuildtimeonrealdatasets. Thefourrealdatasetshave\ndiﬀerent distributions. They lead to diﬀerent index build t imes\nas shown in Fig. 5a. BTreeis the fastest to build and is little im-\npacted by the data distribution, due to its simple building p ro-\ncedure. With agile model reuse, RMI-NN-MR is two orders of\nmagnitude faster than RMI-NN on all four datasets, while RMI -\nMR is also consistently faster than its non-model-reusing c oun-\nterpart RMI. Our RMRT further outperforms RMI-NN-MR and\nPGM,andtheadvantageisupto74%(3.0svs.11.7sonface) and\n32% (5.0s vs. 7.4s on face) than PGM, respectively. RS is fast er\nthanRMRT,duetoitssingle-passprocedure,butitslookupt ime\nissubstantiallyhigherthanthatofRMRT,whichisdetailed next.\n100101102103104\namzn faceosmwikiBuild time (s)\nDatasetBTree\nPGM\nRS\nRMRTRMI\nRMI-MR\nRMI-NN\nRMI-NN-MR\n(a) Indexbuildtime 150 200 250 300 350 400 450 500 550 600\namznfaceosmwikiLookup time (ns)\nDatasetBTree\nPGM\nRS\nRMRTRMI\nRMI-MR\nRMI-NN\nRMI-NN-MR\n(b) Lookuptime\nFigure5: Build andlookuptimeon realdatasets.\nLookuptimeonrealdatasets. Eachrealdatasethas10mil-\nlion random lookup keys, and we report their average lookup\ntime in Fig. 5b. RMRT is the fastest over all four datasets. On\namzn, RMRT (189 ns) is 46%, 35%, 14%, 7%, 52%, 38%, and 20%\nfaster thanBTree(534ns), RMI-NN (440ns), RMI(221 ns), RMI -\nMR (205 ns), RMI-NN-MR (401 ns), PGM (308 ns), and RS (236\nns), respectively. RMI-MR and RMI-NN-MR have better perfor -\nmance than RMI and RMI-NN over amzn, face, and wiki, since\nthesethreedatasetsarewelldistributedandcanbeﬁttedby the\npre-trained models. For osm, it diﬀers more from the synthet ic\ndatasets,wherethereusedmodelhasincreasedlookupcosts .We\nfurther note that RS index stores spline points, the number o f\nwhich are a decisive factor in RS lookup time. To obtain the\nlookup performance shown here, the index size of RS is about\nanorder ofmagnitude larger thanthatof RMRT.\n100101102103\n1 3 5 7 9Build time (s)\nSkewnessBTree\nPGM\nRSRMI\nRMI-MR\nRMI-NNRMI-NN-MR\nRMRT\n(a) Indexbuildtime 150 200 250 300 350 400 450 500 550\n1 3 5 7 9Lookup time (ns)\nSkewness\n(b) Lookuptime\nFigure6: Buildand lookuptimeon skewdatasets.\nIndex build time on skewed datasets. Since our RMRT\ntargets skewed data, we further test the indices on syntheti c\ndatawithincreasingskewness.AsFig.6ashows,theindexbu ild\ntimesarelessimpactedbydataskewness.Thisisconsistent with\n\ntheresultsonrealdatasets,wheretheindexbuildtimesare also\nsimilaracrossdiﬀerentdatasets.RMI-NN-MRandRMIagaino ut-\nperformRMI-NNandRMI,respectively,whileourRMRTisonly\nslightlyslowerthanBTreeandthesingle-pass learnedinde x RS.\nLookup time on skewed datasets. As Fig. 6b shows, our\nRMRT again yields the best lookup performance on all skewed\ndatasets (skewness = 1 denotes uniform data), and its perfor -\nmance is stable as data skewness increases, conﬁrming its ca -\npabilitytoadapttoskeweddata.Incontrast,thefourRMI-b ased\nindices have fast increasing lookup times when the data skew -\nnessincreases,asanalyzedinSection3.BTree,PGM,andRSa re\nalsolessimpactedbecausetheirdesignsarebasedonworst- case\nscenarios.\nIndex build time under varying /u1D716.Table 3 shows that, as\n/u1D716increases, the build times of both RMI-NN-MR and RMRT in-\ncrease. Thisisbecausealarger /u1D716requiresahigher similarityfor\nmodel reuse, thus more datasets are examined. For our RMRT,\nthe build time decreases initially. This is because a small /u1D716(/u1D716=\n0.5)cannotﬁtthedatasetswell,whichcreatesunevenpartiti ons\nthat take more models to ﬁt. As /u1D716increases beyond /u1D716=0.6, the\nbuildtimeofRMRTrisesagain.NotethatRMI-MRshowsasim-\nilar trend toRMI-NN-MR and is omittedforconciseness.\nLookuptimeundervarying /u1D716.Table3alsoshowsthat,as /u1D716\nincreases, the lookup times of both models decrease. This is be-\ncause better-ﬁtted models are selected for larger /u1D716which bring\nshorter search ranges. We see that the beneﬁt in lookup out-\nweighs theextraindex buildingcostswhen using alarger /u1D716.\nTable3:Build,lookupandinsertiontimeundervarying /u1D716.\n/u1D716 0.50.60.70.80.9\nBuild(s)RMI-NN-MR 7.67.98.311.814.4\nRMRT 4.23.74.14.34.4\nLookup (ns)RMI-NN-MR 572570470466401\nRMRT 349321274223189\nInsertion(ns)RMI-NN-MR 99106110124132\nRMRT 105113115121124\nUpdatetimeunderundervarying /u1D716.Table3furthershows\nthe times for inserting 100% more points (following the dist ri-\nbution of amzn, same below). We see that the insertion times\nincrease with /u1D716. This is becausethe boundon thenumber of in-\nsertions before model rebuilding is inversely proportiona l to/u1D716\n(Lemma 4.1),i.e., a larger /u1D716triggers rebuilds moreeagerly.\nUpdatetimeundervaryinginsertionratios. Next,wetest\ntheimpactof thenumber ofpointsinserted.RMI,RMI-NN,and\nRSarestaticindicesandareomittedforthisexperiment.Fo rthe\nfanout(maximumerrorinPGM),weuse210forPGMand213for\nRMRT and RMI-NN-MR, respectively. As shown in Fig. 7a, the\ninsertiontimesofBTree,RMI-NN-MR,andRMRTincreasewith\nthe insertion ratio. For PGM, the insertion time rises in sta ges.\nIt has higher insertion costs than RMI-NN-MR in most cases\n(from 10% to 90%). This is because PGM uses the logarithmic\nmethod [14] which builds and merges (hence the cost jumps) a\nsetofPGMsforinsertions.ForRMI-NN-MR,whentheinsertio n\nratioiswithin80%,thecostincreasesslowlybecausemostp oints\nareinserteddirectly.Thecostsincreasefasterwhenthein sertion\nratioexceeds 80%,which exceedstheinsertionboundandlea ds\nto more rebuilds. For RMRT, the insertion time is more stable .\nThisisbecauseeachRMRTsub-modelindexesarelativelysma ll\nsetofdatapointsandhasarelativelysmallrebuildbound.M odel\nrebuild is triggered steadily as new data pointsareinserte d.\nUpdate time undervarying branching parameters. Due\ntotheparameterlimitationofdynamicPGM,wevarythefanou t\n(i.e., maximum error in PGM) from 26to 213. As Fig. 7b shows, 90 110 130 150 170 190\n102030405060708090100Response time (ns) \nInsertions (%)BTree\nPGMRMI-NN-MR\nRMRT\n(a) Varyinginsertionratio 100 120 140 160 180\n 26  27  28  29  210  211  212  213 Response time (ns)\nBranching parameter\n(b) Varyingbranchingparameter\nFigure7: Insertiontimeresults.\nPGMoutperformsRMI-NN-MRwhenitsmaximumerror(fanout)\nis larger than28becausea larger fanout forthe RMI learned in-\ndicesmeans fewer datapointsineach model.Thecardinality of\neach model in the second layer is smaller, and thebound for in -\nsertionis also smaller (w.r.t.,Lemma 4.1),such that rebui ld hap-\npensmoreoften.ForRMRT,theinsertionperformancebecome s\nbetter as the fanout increases. This is because it can adapti vely\ndividetheunderlyingmodelsandprovidemorepositionsfor di-\nrectinsertions withoutfrequentmodelrebuilds.\n5.2 Discussion\nWenoteseveraldirectionstobeexplorednext.Wehaveomitt ed\nthesortingcosts in index building,since theseare shared b y all\nindices. It would be interesting to further optimize these c osts\nwith a learning-based technique. Our CDF similarity approx i-\nmationconsiders themaximumdistancebetweentheCDFs. An\nalternative is to take the average distance. How to bound the\nsearch range inthis caseis an interesting challenge.\n6 CONCLUSIONS\nWe proposed to reuse pre-trained models for indexing new (or\nupdated) datasets to address building and update eﬃciency i s-\nsues in learned indices. We also proposed a similarity metri c to\nmeasurethedistributiondiﬀerencebetweentwodatasets.B ased\non this metric, our agile model reuse algorithm can eﬃcientl y\nselectthemostsuitablepre-trainedmodeltoindexanew(or up-\ndated)dataset.Weshowhowthepredictionerroroftheselec ted\npre-trained model is bounded on the new (or updated) dataset .\nWe demonstrate the eﬀectiveness of the proposedalgorithm b y\napplying it on the RMI learned indices [10] and our proposed\nlearnedindexRMRT.Experimentalresultsonsyntheticdata sets\nandfourrealdatasetsshowthatouragilemodelreusetechni que\ncanimprovethebuildingandupdatetimeoflearnedindicess ub-\nstantiallywith littleimpactonthelookupperformance.\nREFERENCES\n[1] S.Ben-David,J.Blitzer,K.Crammer,A.Kulesza,F.Pere ira,andJ.W.Vaughan.\n2010. A Theory of Learning from Diﬀerent Domains. Machine Learning 79,\n1-2(2010), 151–175.\n[2] C.Cortesand M.Mohri.2011. DomainAdaptation inRegres sion.InInterna-\ntional Conferenceon AlgorithmicLearning Theory . 308–323.\n[3] A. Davitkova, E. Milchevski, and S. Michel. 2020. The ML- Index: A Multidi-\nmensional, Learned Index for Point, Range, and Nearest-Nei ghbor Queries.\nInEDBT.407–410.\n[4] J.Ding,U.F.Minhas,J.Yu,C.Wang,J.Do, Y.Li,H.Zhang, B.Chandramouli,\nJ.Gehrke,D.Kossmann,D.Lomet,andT.Kraska.2020. ALEX:A nUpdatable\nAdaptiveLearned Index. In SIGMOD.969–984.\n[5] P. Ferragina and G. Vinciguerra. 2020. The PGM-Index: A F ully-Dynamic\nCompressedLearned Index with ProvableWorst-Case Bounds. PVLDB13, 8\n(2020), 1162–1175.\n[6] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T . Kraska. 2019.\nFITing-Tree:A Data-AwareIndex Structure.In SIGMOD.1189–1206.\n[7] A. Hadian and T. Heinis. 2019. Considerations for Handli ng Updates in\nLearned Index Structures. In International Workshop on Exploiting Artiﬁcial\nIntelligence Techniquesfor DataManagement . 3:1–3:4.\n\n[8] A. Kipf,R.Marcus,A. v.Renen, M.Stoian, A. Kemper,T. Kr aska,and T.Neu-\nmann.2020. RadixSpline:ASingle-PassLearnedIndex.In InternationalWork-\nshoponExploitingArtiﬁcialIntelligenceTechniquesforD ataManagement .5:1–\n5:5.\n[9] Kolmogorov-SmirnovTest.2020. https://en.wikipedia.org/wiki/Kolmogorov-Smirnov_te st.\nAccessed: 2021-01-06.\n[10] T. Kraska,A. Beutel, E. H. Chi,J. Dean, and N. Polyzotis . 2018. The Casefor\nLearned IndexStructures.In SIGMOD.489–504.\n[11] X. Li, J. Li, and X. Wang. 2019. ASLM: Adaptive Single Lay er Model for\nLearned Index. In DASFAA.80–95.\n[12] Y. Mansour, M. Mohri, and A. Rostamizadeh. 2009. Domain Adaptation:\nLearningBounds and Algorithms. In COLT.\n[13] R.Marcus,A.Kipf, A. v.Renen, M.Stoian, S. Misra,A. Ke mper,T.Neumann,\nand T. Kraska. 2020. Benchmarking Learned Indexes. PVLDB14, 1 (2020),\n1–13.\n[14] M.H.Overmars.1983. TheDesignofDynamicDataStructures .LectureNotes\ninComputerScience, Vol. 156. Springer.\n[15] PyTorch.2016. https://pytorch.org . Accessed: 2021-01-06.\n[16] J. Qi, Y. Tao, Y. Chang, and R. Zhang. 2018. Theoreticall y Optimal and Em-\npirically Eﬃcient R-trees with Strong Parallelizability. PVLDB11, 5 (2018),\n621–634.\n[17] Scikit-learn.2007. https://scikit-learn.org . Accessed: 2021-01-06.\n[18] STX B+ Tree.2007. https://panthema.net/2007/stx-btree . Accessed: 2021-01-\n06.\n[19] J.C.Thibault,D.R.Roe,J.C.Facelli,andT.E.Cheatha mIII.2014. DataModel,\nDictionaries,and DesiderataforBiomolecularSimulation DataIndexing and\nSharing. Journal of Cheminformatics 6, 1(2014), 4:1–4:23.",
  "textLength": 46536
}