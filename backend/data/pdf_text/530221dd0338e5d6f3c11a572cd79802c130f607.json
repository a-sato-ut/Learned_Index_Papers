{
  "paperId": "530221dd0338e5d6f3c11a572cd79802c130f607",
  "title": "PLATON: Top-down R-tree Packing with Learned Partition Policy",
  "pdfPath": "530221dd0338e5d6f3c11a572cd79802c130f607.pdf",
  "text": "253PLATON: Top-down R-tree Packing with Learned Partition\nPolicy\nJINGYI YANG, Nanyang Technological University, Singapore\nGAO CONG, Nanyang Technological University, Singapore\nThe exponential growth of spatial data poses new challenges to the performance of spatial databases. Spatial\nindexes like R-tree greatly accelerate the query performance and can be effectively constructed through\npacking, i.e., loading all data into the index at once. However, existing R-tree packing methods rely on a set\nof fixed heuristic rules, which may not be suitable for different data distributions and workload patterns. To\naddress the limitations of existing R-tree packing methods, we propose PLATON , a top-down R-tree packing\nmethod with learned partition policy that explicitly optimizes the query performance with regard to the\ngiven data and workload instance. We develop a learned partition policy based on Monte Carlo Tree Search\nand carefully make design choices for the MCTS exploration strategy and simulation strategy to improve\nalgorithm convergence. We propose a divide and conquer strategy and two optimization techniques, early\ntermination and level-wise sampling, to drastically reduce the MCTS algorithmâ€™s time complexity and make\nit a linear-time algorithm. Experiments on both synthetic and real-world datasets demonstrate the superior\nperformance of PLATON over existing R-tree variants and recently proposed learned/workload-aware spatial\nindexes.\nCCS Concepts: â€¢Information systems â†’Database management system engines .\nAdditional Key Words and Phrases: Spatial index, Spatial query processing, Learned index, Monte carlo tree\nsearch\nACM Reference Format:\nJingyi Yang and Gao Cong. 2023. PLATON: Top-down R-tree Packing with Learned Partition Policy. Proc. ACM\nManag. Data 1, 4 (SIGMOD), Article 253 (December 2023), 26 pages. https://doi.org/10.1145/3626742\n1 INTRODUCTION\nSpatial data is rapidly generated from mobile GPS sensors through location-based apps like Uber\nand Google Reviews, from embedded GPS sensors in vehicles, vessels, and airplanes, and from a\nvast number of Lidar sensors with the increasing adoption of self-driving technology. With the\nexponential growth of spatial data, there is an increasing need for database systems to efficiently\nmanage and analyse spatial data. One of the key components of a spatial database is the spatial\nindex, a data structure that facilitates spatial query processing. Under the ongoing trend of applying\nmachine learning techniques to enhance database systems, several learning-based methods have\nbeen proposed to improve the spatial index performance. Existing work on machine learning for\nspatial indexes can be categorized into two classes: learned spatial indexes and ML-enhanced\nspatial indexes. Learned spatial indexes use machine learning models to map spatial coordinates to\nstorage locations [ 40,46,47,53], while ML-enhanced spatial indexes [ 10,21,26] learn to enhance\ncertain operations, e.g., insertion and search operations, of traditional spatial indexes like R-tree.\nAuthorsâ€™ addresses: Jingyi Yang, jyang028@e.ntu.edu.sg, Nanyang Technological University, Singapore; Gao Cong, gaocong@\nntu.edu.sg, Nanyang Technological University, Singapore.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\nÂ©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n2836-6573/2023/12-ART253\nhttps://doi.org/10.1145/3626742\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n\n253:2 Jingyi Yang and Gao Cong\nLearned spatial indexes have several limitations. Firstly, depending on the index design, some\nlearned spatial indexes may produce approximate results for certain types of query, e.g., KNN\nquery [ 47,53]. Secondly, existing learned spatial indexes only support point data, and cannot handle\ndata with geometry. Thirdly, most learned indexes model the cumulative density function (CDF) of\nthe data [ 40,53], which makes them vulnerable to data poisoning attacks [ 35]. As a consequence, it\nwould be a long way for real-world systems to integrate learned spatial indexes. In contrast, the\nML-enhanced spatial indexes can be more easily deployed into real-world systems, as they are built\non top of popular spatial indexes like R-tree, which are already widely used in DBMS. Moreover, as\nthe basic structure and properties of R-tree are unchanged, existing query processing algorithms\nremain applicable.\nExisting ML-enhanced spatial indexes [ 21,27] construct an R-tree through one-by-one insertion,\nand use ML techniques to optimize the ğ‘â„ğ‘œğ‘œğ‘ ğ‘’ğ‘†ğ‘¢ğ‘ğ‘¡ğ‘Ÿğ‘’ğ‘’ andğ‘ ğ‘ğ‘™ğ‘–ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’ process. However, in real-\nworld applications, R-tree packing, or bulk-loading is usually performed at index construction,\nbecause packing constructs an R-tree with better space utilization and query performance compared\nto one-by-one insertion. Moreover, packing is performed during the periodic reload of clustered\nindexes to optimize the index performance. As we will discuss, existing methods for R-tree packing\nsuffer from two major limitations, which motivate the use of machine learning techniques to\nenhance R-tree packing.\nTwo classes of methods have been proposed for R-tree packing, bottom-up methods and top-\ndown methods. Bottom-up [ 12,15,17,30,31,33,39,48,49] methods build the R-tree from leaf nodes\nupward by sorting the data objects in a heuristic order with a good clustering property. Top-down\nmethods [ 25,38] build the R-tree through recursive partitioning, with a partition policy that greedily\noptimizes a heuristic cost function, e.g., the sum of the area of node minimum bounding rectangles\n(MBRs). The first limitation of existing methods is that both bottom-up and top-down methods\nare not adaptive to different data distributions and workload patterns, as they rely on a fixed set\nof heuristic rules. Bottom-up methods use a heuristic sort order, while top-down methods use a\nheuristic cost function. The use of heuristic rules results in the constructed R-tree only performing\nwell for certain data distributions and workload patterns, while performing poorly for others. The\nsecond limitation is that existing top-down packing methods ignore the dependencies between the\npartition decisions on different nodes, which leads to a partition policy that only makes locally\noptimal partitions. We will discuss the two limitations in detail in Section 2.4.\nIn this work, we propose PLATON : top-down R-tree Packing with Learned p ArTitiONpolicy,\nwhich addresses the two limitations of the existing packing methods. PLATON adopts a top-down\npacking framework and explicitly optimizes the query performance with regard to the given data\nand workload instance, so that it is adaptive to any data distribution and workload pattern.\nWe prove in Section 3 that optimal R-tree packing with regard to the given data and workload\ninstance, is an NP-hard problem. While it is possible to design greedy partition policies that optimize\na heuristic function with regard to the given data and workload, such an approach, like existing\ntop-down methods, ignores the dependencies between the partition decisions on different nodes.\nTo identify the partition policy that optimizes query performance while avoiding locally optimal\npartition actions, we propose to learn a partition policy using an effective and lightweight RL\ntechnique, Monte Carlo Tree Search (MCTS), which maximizes the long-term reward of the partition\nactions.\nThe partition problem setup, however, poses unique challenges to the design of the MCTS\nalgorithm, specifically, to the design of the exploration strategy and simulation strategy in MCTS.\nAs the return of a state across different rollouts usually has a high variance, the standard Upper\nConfidence Bound for Trees (UCT) selection policy no longer works well. Meanwhile, due to a\nlong action sequence and a huge state space, the standard random rollout policy leads to slow\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:3\nconvergence. We carefully design these two policies to address the challenges, as we will discuss in\nSection 5.4.\nAnother main obstacle in learning a partition policy using MCTS is that the MCTS algorithm\nhas a time complexity of ğ‘‚(ğ‘˜Â·ğ‘2logğ‘)(ğ‘is the size of data, k is the number of MCTS iterations),\nwhich is not scalable to large datasets. To address this challenge, we develop a divide and conquer\nstrategy, which drastically reduces the size of the state space. We further propose two optimization\ntechniques, early termination and level-wise sampling, to reduce the time complexity to ğ‘‚(ğ‘˜Â·ğ‘).\nMoreover, we consider the case when no training workload is available, and propose to learn from\na synthetic query workload following the data distribution.\nContributions. This work makes the following contributions.\nâ€¢We propose PLATON , a top-down R-tree packing method adaptive to different data distributions\nand workload patterns. We formulate the optimal R-tree packing problem with regard to the\ngiven data and workload instance, and prove its NP-hardness. (Sec. 3) To take into account the\ndependencies between the partition decisions at different nodes, we develop a learned partition\npolicy based on Monte Carlo Tree Search and carefully designed the selection policy and rollout\npolicy for our problem to achieve better convergence. (Sec. 5)\nâ€¢We propose a divide and conquer strategy, and two optimization techniques to reduce the MCTS\nalgorithm complexity from higher than quadratic time to linear time. (Sec. 6)\nâ€¢When a workload is not available at the time of index construction, we propose to optimize the\nI/O cost of a query workload that follows the data distribution. (Sec. 7)\nâ€¢We integrate PLATON into two real-world systems, libspatialindex and PostgreSQL, and conduct\ncomprehensive experiments to show that PLATON significantly outperforms existing R-tree\nvariants and recently proposed learned spatial indexes.\n2 BACKGROUND\nWe first provide the preliminaries in Section 2.1. We then review two classes of existing methods\nfor R-tree packing, bottom-up methods and top-down methods, in Section 2.2 and Section 2.3\nrespectively. We discuss the limitations of existing methods in Section 2.4.\n2.1 Preliminaries\nConsider a setD={ğ‘…1,...ğ‘…ğ‘}ofğ‘(hyper-)rectangle data objects in a ğ‘‘-dimensional Euclidean\nspace. R-tree packing considers the problem of constructing a balanced R-tree ğ‘‡fromDwith a\nnode capacity ğµ.\nWe label the levels of the R-tree in a bottom-up manner, with leaf nodes being level 1and\nroot nodes being level âŒˆlogğµğ‘âŒ‰. Each node at level ğ‘¡indexes at most ğµğ‘¡data objects. For ease of\npresentation, we use ğ‘‘=2in the following discussion, but the discussion can be generalized to any\nğ‘‘>2.\n2.2 Bottom-up R-tree Packing\nBottom-up methods [ 12,15,17,31,33,39,48,49] start by packing data objects into leaf nodes, and\nthen repeatedly build the tree upward by packing lower-level nodes into nodes one level higher. To\nachieve good query performance and create tree nodes with minimal overlap, bottom-up methods\ntypically rely on a heuristic sort order with a good clustering property, i.e., after sorting the data\nobjects in the heuristic order, neighboring objects are spatially close. Example heuristic sort orders\ninclude Hilbert ordering [17, 31, 33], Sort-Tile-Recursive [39], as well as space-filling curve-based\nordering in rank space [ 48]. Bottom-up methods first sort data objects in the heuristic sort order,\nand pack every ğµconsecutive objects into a leaf node. This process then repeats over the Minimum\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:4 Jingyi Yang and Gao Cong\nBounding Rectangles (MBRs) of nodes at level ğ‘¡to create nodes at level ğ‘¡+1. Bottom-up packing\nfinishes when the root node of the tree is built. We note that a special case is the Priority R-tree [ 12]\n(PR-tree), which does not rely on a sort order. However, the construction of PR-tree nodes also\ninvolves heuristic rules.\n2.3 Top-down R-tree packing\nTop-down methods [ 25,38] start with the root node, and construct the R-tree downward by\nrecursively partitioning higher-level nodes into nodes one level lower. To illustrate the top-down\npacking process, we first define several key concepts.\nDefinition 2.1 (Partially Constructed Tree). A partially constructed tree is a ğµ-ary tree that represents\nan intermediate state during the top-down R-tree packing process. A partially constructed tree node\nğ‘›corresponds to one or more R-tree nodes and is described by two fields, ğ‘›.ğ‘‘ğ‘ğ‘¡ğ‘ andğ‘›.ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ .ğ‘›.ğ‘‘ğ‘ğ‘¡ğ‘\nrepresents the set of data to be indexed, while ğ‘›.ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ represents the level of the corresponding R-tree\nnodes.\nğ‘¦ğ‘¥ğ‘¦!\"#=0.4ğ‘›$.ğ‘‘ğ‘ğ‘¡ğ‘ğ‘›%.ğ‘‘ğ‘ğ‘¡ğ‘ğ‘›&.ğ‘‘ğ‘ğ‘¡ğ‘ğ‘¦ğ‘¥ğ‘¥!\"#=0.3ğ‘›$.ğ‘‘ğ‘ğ‘¡ğ‘ğ‘›'.ğ‘‘ğ‘ğ‘¡ğ‘ğ‘¦ğ‘¥\tğ‘›(ğ‘›(.ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›\tğ‘!(ğ‘¥\"#$,9)ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›\tğ‘%\t(ğ‘¦\"#$,9)\tğ‘›(\tğ‘›$\tğ‘›'\tğ‘›(\tğ‘›$\tğ‘›&\tğ‘›%3Level2\nFig. 1. Top-down R-tree packing example, ğ‘=27,ğµ=3.\nDefinition 2.2 (Partition Action). A partition action ğ‘on a partially constructed tree node ğ‘›partitions\nğ‘›.ğ‘‘ğ‘ğ‘¡ğ‘ into two subsets, and creates two new nodes. Each partition action ğ‘is described by a pair\nof fields(ğ‘‘ğ‘–ğ‘š,ğ‘ğ‘œğ‘ ), whereğ‘‘ğ‘–ğ‘š denotes the dimension along which to partition the data, and ğ‘ğ‘œğ‘ \ndenotes the position of the partition. To build a balanced tree, at least one child nodeâ€™s size should\nbe an integer multiple of the corresponding R-tree nodeâ€™s size, i.e., one of the child nodes ğ‘›â€²satisfies\n|ğ‘›â€².ğ‘‘ğ‘ğ‘¡ğ‘|=ğ‘–Ã—ğµğ‘›â€².ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™, whereğ‘–is a positive integer.\nExample. Figure 1 shows the first three partially constructed trees during top-down packing. The\ninitial partially constructed tree only consists of a single root node ğ‘›0.ğ‘›0.ğ‘‘ğ‘ğ‘¡ğ‘ =D, andğ‘›0.ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ =3.\nThe first partition acts on the root node ğ‘›0of the initial state, with ğ‘‘ğ‘–ğ‘š=ğ‘¥ğ‘šğ‘–ğ‘›andğ‘ğ‘œğ‘ =9. We\nfirst sortğ‘›0.ğ‘‘ğ‘ğ‘¡ğ‘ by theirğ‘¥ğ‘šğ‘–ğ‘›values, and partition them into two sets. The first 9 objects with\nthe smallest ğ‘¥ğ‘šğ‘–ğ‘›form nodeğ‘›1, while the rest 18 objects form node ğ‘›2. Bothğ‘›1andğ‘›2are at level\n2, as they correspond to R-tree nodes at level 2. The second partition acts on the node ğ‘›2with\nğ‘‘ğ‘–ğ‘š=ğ‘¦ğ‘šğ‘–ğ‘›andğ‘ğ‘œğ‘ =9. Similarly, we create two new nodes ğ‘›3andğ‘›4. Both nodes are at the same\nlevel asğ‘›2, and we remove ğ‘›2from the tree.\nPartition Policy. At each step, there are multiple candidate partition actions for the given partially\nconstructed tree node ğ‘›. Specifically, along each dimension, there are âŒˆ|ğ‘›.ğ‘‘ğ‘ğ‘¡ğ‘|\nğµğ‘›.ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™âˆ’1âŒ‰possible partition\npositions given the constraints in Definition 2. Assuming ğ‘possible dimensions, e.g.,ğ‘¥ğ‘šğ‘–ğ‘›,ğ‘¦ğ‘šğ‘–ğ‘›,\nto sort the data, this gives us a total of ğ‘Â·âŒˆ|ğ‘›.ğ‘‘ğ‘ğ‘¡ğ‘|\nğµğ‘›.ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™âˆ’1âŒ‰candidate partition actions. The key to\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:5\ntop-down packing is the partition policy, which chooses a partition action among all the candidates\nat each step, so that the constructed R-tree has good query performance. Existing top-down packing\nmethods like TGS [ 25] proposed partition policies that greedily minimize a heuristic cost function,\ne.g.,the sum of the area of node MBRs.\nTop-down packing starts with a partially constructed tree of a single node ğ‘›0, which corresponds\nto the root of the R-tree. ğ‘›0is initialized with ğ‘›0.ğ‘‘ğ‘ğ‘¡ğ‘ =ğ·,ğ‘›0.ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ =âŒˆlogğµğ‘âŒ‰. The first partition\nactionğ‘1is chosen by the partition policy, and acts on the root ğ‘›0, which bi-partitions ğ‘›0.ğ‘‘ğ‘ğ‘¡ğ‘ to\nform two nodes at the next level. We then recursively choose and perform partition actions on the\nnewly created partially constructed tree nodes, until all leaf nodes of the R-tree are created.\n2.4 Limitation of Existing Methods\nLimitation 1: Not adaptive to different data distributions and workload patterns. One\ncommon limitation of both classes of packing methods is that they rely on heuristic rules during\ntree construction, which only work well for certain data distributions and workload patterns, and\nfail for the others.\nAs mentioned, bottom-up packing methods rely on a heuristic sort order with a good clustering\nproperty in order to work well. However, this is not always the case, as no heuristic sort order\nworks for all data distribution and workload patterns. The left part of Figure 2 shows a packing\nexample using ZR [ 48], the most recently proposed bottom-up packing method. It first maps the\ndata objects from the euclidean space to the rank space, and then sort them using a space-filling\ncurve, e.g,Z-curve. With the heuristic sort order, ZR packs {ğ‘…2,ğ‘…3,ğ‘…4},{ğ‘…8,ğ‘…1,ğ‘…5},{ğ‘…7,ğ‘…6,ğ‘…9}\ninto three leaf nodes. We note that the leaf node {ğ‘…8,ğ‘…1,ğ‘…5}has a huge MBR that almost spans\nthe entire data space, because although ğ‘…8andğ‘…1are neighbors under the heuristic sort order,\nthey are far apart spatially. As a result, this leaf node needs to be accessed for queries that do\nnot intersect with any object in the node. For instance, given the query ğ‘, the node{ğ‘…8,ğ‘…1,ğ‘…5}is\naccessed, although the query only intersects with ğ‘…2andğ‘…3, leading to 2 leaf node I/O. An R-tree\nwith better performance on the given data and query is shown on the right of Figure 2, in which\ncase the leaf node I/O is 1.\nğ‘¦ğ‘¥ğ‘…!ğ‘…\"ğ‘…#ğ‘…$ğ‘…%ğ‘…&ğ‘…'ğ‘…(ğ‘…)\nğ‘¦ğ‘¥ğ‘…!(48)ğ‘…\"(21)ğ‘…#(80)ğ‘…$(57)ğ‘…%(54)ğ‘…&(42)ğ‘…'(12)ğ‘…((15)ğ‘…)(3)ZRA better R-Treeğ‘ğ‘ğ‘\nFig. 2. Example failure of bottom-up packing, ğ‘=9,ğµ=3.\nTop-down methods explicitly optimize a heuristic cost function like the sum of the area of\nnode MBRs. However, such heuristics may not be effective for all data distributions and workload\npatterns. As shown in Figure 3, certain data distributions and workload patterns can make the\nheuristic cost function fail. The left part of the figure shows the four leaf nodes created using TGS.\nAs TGS makes its partition decisions by greedily minimizing the sum of the area of node MBRs, it\npacks the data points into leaf nodes with the shape of long and thin stripes. Given the query ğ‘,\nwhich spans a large range along the y-axis, the result R-tree has poor performance because every\nleaf node needs to be visited. An R-tree with better performance on the given data and query is\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:6 Jingyi Yang and Gao Cong\nshown on the right of Figure 3. Only 1 leaf node I/O is needed for the same query ğ‘. Note that\nbottom-up methods can also fail due to the heuristic nature of the sort order used. A detailed\nexample can be found in the appendix.\nğ‘¦ğ‘¥ğ‘…!ğ‘…\"ğ‘…#ğ‘…$ğ‘…%ğ‘…&ğ‘…'ğ‘…(\nğ‘¥ğ‘¦ğ‘…!ğ‘…\"ğ‘…#ğ‘…$ğ‘…%ğ‘…&ğ‘…'ğ‘…(\nTGSA better R-Treeğ‘ğ‘\nFig. 3. Example failure of TGS, ğ‘=8,ğµ=2.\nLimitation 2: Ignoring the dependencies between partition decisions. Existing top-down\npacking methods greedily choose the partition that optimizes a heuristic cost function, and the\npartition decisions are independently made at each step. However, the partition decisions are not\nindependent of one another, because the partition on a node can affect subsequent partitions on\nits lower-level child nodes. Ignoring the dependencies between the partitions on the parent nodes\nand its child nodes results in locally optimal partition decisions and R-tree structures that are not\noptimized with regard to the cost function.\nOur work aims at addressing the limitations of existing methods and designing a packing strategy\nthat optimizes the index performance for any given data distribution and workload patterns.\n3 OPTIMAL R-TREE PACKING\nIn this section, we define the problem of optimal R-tree packing with regard to the given data and\nworkload instance, and show the NP-hardness of the problem.\nProblem: Optimal R-tree Packing with regard to data and workload. Given a setD=\n{ğ‘…1,...ğ‘…ğ‘}ofğ‘(hyper-)rectangle data objects and a workload W={ğ‘1,...ğ‘ğ‘€}ofğ‘€window\nqueries in a ğ‘‘-dimensional Euclidean space. Our goal is to construct a balanced R-tree ğ‘‡fromD\nwith a node capacity ğµ, so that the query performance is optimized for workload W. Specifically,\nwe consider the case of constructing a disk-based R-tree, where each node is stored on a disk page.\nThe I/O cost ğ¶(ğ‘‡,W) is measured by the total number of nodes accessed by the queries.\nTheorem 3.1. Optimal R-tree packing with regard to the workload I/O cost ğ¶(ğ‘‡,W) is NP-hard.\nProof. To prove the theorem, we prove for the case of a 2-level R-tree. Optimal packing\nof a 2-level R-tree is equivalent to finding the optimal partitioning of Dinto leaf nodes ğ‘ƒ=\n{ğ‘1,ğ‘2,...,ğ‘ğ‘™},ğµ/2â‰¤|ğ‘ğ‘–|â‰¤ğµ, such that for a given weight function ğ‘¤:ğ‘ğ‘–â†’R+, the sum of the\nweights is minimized. In our case, ğ‘¤(ğ‘)=Ã\nğ‘âˆˆW 1â„ğ‘ğ‘ ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘(ğ‘,ğ‘),i.e., the weight of a partition is\nthe number of query windows it overlaps. We prove the NP-hardness by a reduction from the\noptimal partitioning problem without considering the workload [ 11], which shows that for the\nweight function ğ‘¤(ğ‘)=ğ‘ğ‘Ÿğ‘’ğ‘(ğ‘€ğµğ‘…(ğ‘)), the optimal partitioning problem is NP-hard.\nAn instance of the optimal partitioning problem [ 11] consists of a dataset Dof N rectangles. The\nproblem finds a partitioning ğ‘ƒ={ğ‘1,ğ‘2,...,ğ‘ğ‘™},ğµ/2â‰¤|ğ‘ğ‘–|â‰¤ğµ, that minimizesÃğ‘™\nğ‘–=1ğ‘ğ‘Ÿğ‘’ğ‘(ğ‘€ğµğ‘…(ğ‘ğ‘–)).\nWe assume that all rectangles have integer coordinates. We reduce this instance of optimal parti-\ntioning to our problem with the same dataset D, and construct a workload such that we have an\nequivalent weight functions. We construct our query workload Wto be the set of 1Ã—1squares that\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:7\npacks the data space, i.e.,W={(ğ‘¥,ğ‘¦,ğ‘¥+1,ğ‘¦+1)|ğ‘¥âˆˆ[ğ‘‹ğ‘šğ‘–ğ‘›,ğ‘‹ğ‘šğ‘ğ‘¥âˆ’1],ğ‘¦âˆˆ[ğ‘Œğ‘šğ‘–ğ‘›,ğ‘Œğ‘šğ‘ğ‘¥âˆ’1],ğ‘¥,ğ‘¦âˆˆZ},\nwhereğ‘‹ğ‘šğ‘–ğ‘›,ğ‘‹ğ‘šğ‘ğ‘¥,ğ‘Œğ‘šğ‘–ğ‘›,ğ‘Œğ‘šğ‘ğ‘¥are the minimal and maximal coordinates of the input data along\neach dimension. It is easy to see that ğ‘¤(ğ‘)=Ã\nğ‘âˆˆW 1â„ğ‘ğ‘ ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘(ğ‘,ğ‘)=ğ‘ğ‘Ÿğ‘’ğ‘(ğ‘€ğµğ‘…(ğ‘)). As a result,\na solution to the optimal partition problem [ 11] is a solution to the optimal partition problem with\nregard to dataDand the constructed workload W, and vice versa. This completes the proof.\n4 OVERVIEW OF PLATON\nPLATON aims at exploiting the advantage of the top-down R-tree packing framework, while\naddressing its limitations. We summarize the design goals and our solutions as follows.\n1.Goal: Optimizing query performance . Optimal R-tree packing with regard to a given data\nand workload instance is NP-hard. While existing top-down packing methods propose partition\npolicies that greedily optimize a heuristic cost function, such policies lead to locally optimal\npartitions.\nSolution: To address this issue, we frame top-down packing as an MDP and leverage an effective,\nlightweight RL technique, Monte Carlo Tree Search (MCTS) to learn a partition policy that\noptimizes the long-term reward.\n2.Goal: Designing exploration and simulation strategy for better convergence . The partition\nproblem setup makes it hard for MCTS to converge due to a huge state space and the high variance\nof simulation returns across different rollouts.\nSolution: We carefully design the MCTS exploration strategy and simulation strategy to achieve\nbetter convergence. We incorporate domain knowledge of R-tree packing into MCTS simulation\nby proposing a greedy rollout policy.\n3.Goal: Efficiently learning the partition policy . While the learned partition policy achieves\ngood query performance, the MCTS algorithm, despite being lightweight compared with many\nother RL techniques, still has a high time complexity, which makes it hard to scale to large\ndatasets.\nSolution: We propose a divide and conquer strategy, and two optimization techniques, early\ntermination and level-wise sampling, which drastically reduce the time complexity of the MCTS\nalgorithm and makes it a linear-time algorithm.\nTop-down Packing\nLearned Partition Policy\nMCTSContructed R-treeData\nQuery\nWorkload\nFig. 4. Workflow of PLATON.\nWorkflow of PLATON. Figure 4 shows the high-level workflow of PLATON . The inputs include\nthe dataset and a training workload representative of the workload patterns. We construct an\nR-tree on the data in a top-down manner. At each step, the partition action is chosen based on\nthe learned policy using MCTS, which optimizes the query performance for the given data and\nworkload instance. This process is repeated until the R-tree is fully constructed.\nInsertion and Deletion. Like all prior packed R-trees, PLATON is compatible with existing R-tree\ninsertion and deletion methods, e.g., the methods used in R*-tree or learning-based methods like\nRLR-tree [26] .\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:8 Jingyi Yang and Gao Cong\n5 LEARNING PARTITION POLICY\nWe first discuss the motivation for learning the partition policy in Section 5.1. We then discuss how\nto frame top-down packing as a MDP in Section 5.2. We propose to learn a partition policy using\nMonte Carlo Tree Search (MCTS) in Section 5.3. We discuss our design of MCTS exploration and\nsimulation strategy in Section 5.4.\n5.1 Motivation for Learned Partition Policy\nDue to the NP-hardness of the optimal R-tree packing problem, one can only design approximate\nalgorithms for the problem. To develop a partition policy that minimizes the I/O cost, a natural\nidea is to adopt an approach similar to TGS [ 25], which chooses the partition action that greedily\noptimizes a heuristic at each step. However, this approach ignores the dependencies between the\npartition decisions at different nodes. As the partition on a node can affect subsequent partitions\non its lower-level child nodes, the greedy partition policy only makes locally optimal partition\ndecisions by choosing the partition with the highest immediate reward.\nWe identify that choosing the partition action at each step is a sequential decision-making\nprocess. To avoid making locally optimal decisions, we consider a class of algorithms that optimizes\nthe long-term rewards in sequential decision-making, reinforcement learning (RL) [52].\n5.2 Top-down Packing as an MDP\nAs discussed in Section 5.1, choosing the partition action at each step is a typical sequential decision-\nmaking process. We now discuss how we can frame it as a Markov Decision Process (MDP). We\ndefine the components of our MDP as follows. State space : The state space ğ‘†, is defined as the set of\nall possible partially constructed R-trees, and a state ğ‘ âˆˆğ‘†is represented by a partially constructed\nR-tree. A state is terminal if the corresponding R-tree is fully constructed. Action space : Without\nloss of generality, we define the action space ğ´for a state, i.e., partially constructed R-tree, to be\nthe set of possible partition actions on the left-most unpartitioned node. As there may be multiple\nunpartitioned nodes for a partially constructed R-tree, we fix a node to partition so that we reduce\nthe action space. Transition probability : The state transition function ğ‘ƒğ‘is deterministic. Taking\nan actionğ‘on a stateğ‘ transits into a new partially constructed tree state ğ‘ â€²with 100% probability.\nReward function : The reward function of our MDP should be designed in a way so that the\nobjective of our MDP is equivalent to minimizing the I/O cost of the given workload. However, the\nI/O cost of an R-tree can only be computed when the R-tree is fully constructed. If the reward is\nonly associated with the last action, the extremely sparse reward makes it hard to learn a good\npolicy. To address this problem, we process a way to break down the objective into a series of\nrewards associated with each partition action. We first transform the objective of minimizing the\nworkload I/O cost into the equivalent objective of maximizing the number of skipped page access.\nConsider an R-tree ğ‘‡and a workloadW, the number of skipped page access ğ‘†(ğ‘‡,W) refers to the\ntotal number of disk pages one avoid accessing due to the R-tree ğ‘‡when executing the workload\nW.\nğ‘†(ğ‘‡,W)=âˆ‘ï¸\nğ‘âˆˆğ‘Šâˆ‘ï¸\nğ‘›âˆˆğ‘‡1Â¬â„ğ‘ğ‘ ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘(ğ‘›,ğ‘) (1)\nwhereâ„ğ‘ğ‘ ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘(ğ‘›,ğ‘)computes whether the MBR of a partially constructed tree node ğ‘›\noverlaps query ğ‘. It is easy to see that maximizing the number of skipped page access ğ‘†(ğ‘‡,W) is\nequivalent to minimizing the I/O cost ğ¶(ğ‘‡,W). While we can only calculate the I/O cost when\nthe R-tree is fully constructed, we can define each partition actionâ€™s contribution to the number of\nskipped page access.\nTo see how a partition action leads to skipped page access, we first look at an example in Figure 5.\nThe partition action ğ‘1creates two partially constructed tree nodes, ğ‘›1andğ‘›2.ğ‘›1overlaps with the\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:9\nğ‘¦ğ‘¥ğ‘›!.ğ‘‘ğ‘ğ‘¡ğ‘ğ‘›\".ğ‘‘ğ‘ğ‘¡ğ‘\nğ‘¦ğ‘¥\tğ‘›#ğ‘›#.ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›\tğ‘!(ğ‘¥\"#$,9)ğ‘ğ‘\tğ‘›#\tğ‘›!\tğ‘›\"ğ’©!\"ğ’©!!ğ’©!#ğ’©#ğ’©!ğ’©$ğ’©%ğ’©&ğ’©'ğ’©(ğ’©)ğ’©*â€¦ğ‘¦ğ‘¥ğ’©!ğ’©\"ğ’©#ğ’©$ğ’©%ğ’©&ğ’©'ğ’©(ğ’©)ğ’©\"\"ğ’©\"*ğ’©\"%ğ‘Skipped due to ğ‘!Node access for query ğ‘Constructed R-tree\nFig. 5. How partition actions lead to skipped page access.\nqueryğ‘, whileğ‘›2does not overlap with query ğ‘. Becauseğ‘›2does not overlap with query ğ‘, any\nnode created by further partitioning ğ‘›2does not overlap with query ğ‘either. As a result, we can\nskip all the subtrees that index ğ‘›2.ğ‘‘ğ‘ğ‘¡ğ‘ when executing query ğ‘,i.e., the two subtrees rooted at N11\nandN12in the constructed R-tree. In this case, ğ‘1leads to a total of 2Ã—(1+3)=8skipped disk\npages.\nDefinition 5.1 (Reward of a partition action). Assume partition action ğ‘on partially constructed tree\nstateğ‘ partitions node ğ‘›, and creates two new nodes ğ‘›1andğ‘›2, the reward of the action is defined as:\nğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘(ğ‘ ,ğ‘)=âˆ‘ï¸\nğ‘âˆˆW1Â¬â„ğ‘ğ‘ ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘(ğ‘,ğ‘›)Â·( 1â„ğ‘ğ‘ ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘(ğ‘,ğ‘›1)\nÂ·ğ‘ğ‘ğ‘”ğ‘’ğ‘†ğ‘–ğ‘§ğ‘’(ğ‘›1)+ 1â„ğ‘ğ‘ ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘(ğ‘,ğ‘›2)Â·ğ‘ğ‘ğ‘”ğ‘’ğ‘†ğ‘–ğ‘§ğ‘’(ğ‘›2))(2)\nwhereğ‘ğ‘ğ‘”ğ‘’ğ‘†ğ‘–ğ‘§ğ‘’(ğ‘›)=(|ğ‘›.ğ‘‘ğ‘ğ‘¡ğ‘|/ğµğ‘›.ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™)Â·ğµğ‘›.ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™âˆ’1\nğµâˆ’1computes the number of disk pages to store\nthe subtree(s) that indexes ğ‘›.ğ‘‘ğ‘ğ‘¡ğ‘ in the fully constructed tree. For each query, we sum up the size\nof nodes that do not overlap with the query. This value is then summed over all queries in the\nworkload to produce the reward of a partition action. To avoid redundant reward computation,\npage skipping should only be counted if the node ğ‘›before partitioning overlaps with the query.\nThe objective of an MDP is to find the optimal policy function ğœ‹that maximizes the expected\nreturnğ‘‰ğœ‹(ğ‘ )of a stateğ‘ following the policy. The expected return ğ‘‰ğœ‹(ğ‘ )is the expected discounted\nsum of rewards with decay factor ğ›¾. Under our formulation, we set ğ›¾=1, so maximizing the\nexpected return is equivalent to maximizing the total number of skipped page access.\n5.3 Monte Carlo Tree Search algorithm\nTo find the partition policy that optimizes the I/O cost while taking the dependencies between the\npartition decisions into consideration, we propose to use Monte Carlo Tree Search (MCTS), an\neffective and lightweight search algorithm. We first motivate our use of MCTS over other common\nfamilies of RL techniques like dynamic programming, value-based RL ( e.g.Q-learning), and policy\ngradient methods.\nâ€¢Efficiency and Effectiveness. Due to the large state space, a dynamic programming algorithm\nfor our problem has exponential complexity. Value-based (typically temporal difference methods)\nand policy gradient methods require neural networks to approximate the value/policy function.\nThey are slow to train (several days for 1 million records) even with lightweight models and\nhardly converge for our problem. In contrast, MCTS is a statistical anytime algorithm [ 16],i.e., it\ncan work under any given amount of computing power, while more computing power leads to\nbetter solutions to the problem. MCTS achieves much better convergences under limited time\nand computing resources compared to value-based and policy gradient methods.\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:10 Jingyi Yang and Gao Cong\nÎ”=1Î”=1ğ‘£!Selection(ğ‘¥!\"#,9)(ğ‘¥!\"#,18)ğœ™0.70.50.8ğ‘£!Expansionğœ™0.70.50.8ğ‘£!Simulationğœ™0.70.50.8ğ‘£!Backpropagationğœ™0.70.50.9(ğ‘¦!$%,18)RolloutPolicyRolloutPolicy1.0(ğ‘¦!\"#,9)(ğ‘¥!\"#,9)(ğ‘¥!\"#,18)(ğ‘¦!\"#,9)(ğ‘¥!\"#,9)(ğ‘¥!\"#18)(ğ‘¦!\"#,9)(ğ‘¥!\"#,9)(ğ‘¥!\"#,18)(ğ‘¦!\"#,9)(ğ‘¦!$%,18)(ğ‘¦!$%,18)ğ‘£\"ğ‘£\"ğ‘£\"ğœ™ğœ™ğ‘£#ğ‘£#ğ‘£$ğ‘£%ğ‘£$ğ‘£#ğ‘£$ğ‘£#ğ‘£$ğ‘£%ğ‘£%ğ‘£%\nFig. 6. Overview of MCTS.\nâ€¢Free of Tuning. RL techniques that use neural networks to learn the value/policy function usually\nrequire hyper-parameter tuning during training. As the optimal set of hyper-parameters differs\nacross datasets, the time-consuming tuning process makes it costly to apply these techniques to\nnew data. In comparison, MCTS is free of hyper-parameter tuning, and therefore much easier to\napply on new instances.\nMCTS is a simulation-based search algorithm that explores the state space in the form of a search\ntree. Each search tree node ğ‘£corresponds to a state ğ‘ ,i.e., partially constructed R-tree. We use\nğ‘£(ğ‘ )to represent the search tree node corresponding to the state ğ‘ , andğ‘ (ğ‘£)to represent the state\ncorresponding to the search tree node ğ‘£. Note that a search tree is not the R-tree being constructed.\nDuring the search process, MCTS leverages Monte Carlo methods to evaluate the action value\nfunctionğ‘„(ğ‘ ,ğ‘)of state-action pairs, i.e., the expected return from state ğ‘ by taking action ğ‘. In our\nproblem, the Q-values represent the expected return from a partially constructed R-tree by taking a\nparticular partition action. The Q-values are approximated by averaging the returns from a number\nof simulated episodes, or rollouts. By leveraging the approximated Q-values of explored states\nand actions, MCTS progressively builds a search tree that focuses on the branches with the most\npromising states. We maintain two values for each search tree node ğ‘£,ğ‘(ğ‘£)andğ‘„(ğ‘£).ğ‘(ğ‘£)is the\nnumber of times a state is visited, and ğ‘„(ğ‘£)is the approximated return of the state from the Monte\nCarlo evaluations. Given that ğ‘£.ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ transit toğ‘£with actionğ‘,ğ‘„(ğ‘£)effectively approximates the\naction value function ğ‘„(ğ‘ (ğ‘£.ğ‘ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡),ğ‘).\nMCTS involves an iterative process. In each iteration, the search tree grows by one node and the\napproximated returns of the explored states are updated. As the number of iterations increases, the\napproximated returns of the states gradually approach the expected returns. An iteration consists\nof four steps: Selection, Expansion, Simulation and Backpropagation. The number of iterations is\ndependent on the computational budget. We use the example in Figure 6 to show how the search\ntree evolves over an iteration. For simplicity, we only show ğ‘„(ğ‘£)for each search tree node ğ‘£, andğœ™\ndenotes no approximated return available.\nâ€¢Selection. MCTS starts from the root ğ‘£0, and recursively descends into the child nodes following\nağ‘‡ğ‘Ÿğ‘’ğ‘’ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦ to find the most promising search tree node whose child nodes will be explored. In\nthe figure, assuming the ğ‘‡ğ‘Ÿğ‘’ğ‘’ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦ simply finds the child node ğ‘£with the largest approximated\nreturnğ‘„(ğ‘£). MCTS starts from the search tree root, and descends into node ğ‘£1with the partition\naction(ğ‘¥ğ‘šğ‘–ğ‘›,9), which has the highest approximated return among the three child nodes. Since\nğ‘£1has unexplored child nodes, we select ğ‘£1to be the node to expand.\nâ€¢Expansion. MCTS randomly adds one of the selected nodeâ€™s child ğ‘£ğ‘™to the search tree. In the\nfigure, we choose a random partition action from the state ğ‘ (ğ‘£1),e.g.,(ğ‘¦ğ‘šğ‘ğ‘¥,9), and act on it. We\nadd the result state to the search tree as node ğ‘£ğ‘™.\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:11\nâ€¢Simulation. MCTS performs a rollout from the new node ğ‘£ğ‘™to approximate its return. A rollout\nis a Monte Carlo evaluation of the node ğ‘£ğ‘™, where a sequence of actions is taken following a\nğ‘…ğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦ until a terminal state is reached. The return of the actions from the rollout, Î”, is\nobtained. In the figure, we perform a sequence of random actions from ğ‘£ğ‘™, until the R-tree is fully\nconstructed. We record the return Î”=1.\nâ€¢Backpropagation. After obtaining the return Î”from a rollout, we use it to update the ğ‘(ğ‘£)\nandğ‘„(ğ‘£)values of nodes on the path from ğ‘£ğ‘™toğ‘£0. In the figure, we update the approximated\nreturn of the nodes ğ‘£ğ‘™andğ‘£1with the return Î”=1from the current rollout. ğ‘„(ğ‘£ğ‘™)becomes 1,\nandğ‘„(ğ‘£1)increases to 0.9.\nAn overview of PLATON with MCTS is shown in Algorithm 1. We start by initializing a partially\nconstructed R-tree ğ‘‡with a single root node (line 1). We maintain three variables, ğ‘ğ‘ğ‘›ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡ tracks\nif the tree can be further partitioned, ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ tracks the tree level currently being partitioned,\nandğ‘›ğ‘’ğ‘¥ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’ records the next node to partition (line 2). We partition the tree level-by-level,\nstarting from the top. As long as the tree can be partitioned (line 3), we iterate through nodes at\nğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ , and finds the left-most unpartitioned node ğ‘›ğ‘’ğ‘¥ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’ (lines 4â€“7). If it is found, we\nrun MCTS to find a good partition action (line 9). Specifically, we first create a search tree root\nwith the current partially constructed R-tree ğ‘‡(line 15), and then run MCTS for ğ‘˜iterations (lines\n16â€“19). After ğ‘˜iterations, the action that leads to the state with the highest approximate return is\nchosen and performed on ğ‘›ğ‘’ğ‘¥ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’ (line 10). If no unpartitioned node is found at ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ , we\ndecreaseğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ by 1 (line 11-12). The algorithm returns the fully constructed R-tree when\nall bottom-level nodes are partitioned.\nAlgorithm 1 PLATON with MCTS\nInput: dataD, workloadW, node capacity ğµ, # of iterations ğ‘˜\nOutput: packed R-tree ğ‘‡\n1:ğ‘‡â†{ğ‘›0},ğ‘›0.ğ‘‘ğ‘ğ‘¡ğ‘ =D,ğ‘›0.ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ =âŒˆlogğµğ‘âŒ‰\n2:ğ‘ğ‘ğ‘›ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡â†ğ‘‡ğ‘Ÿğ‘¢ğ‘’,ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ â†âŒˆlogğµğ‘âŒ‰,ğ‘›ğ‘’ğ‘¥ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’â†ğœ™\n3:while canSplit do\n4:ğ‘ğ‘ğ‘›ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡â†ğ¹ğ‘ğ‘™ğ‘ ğ‘’\n5: foreach nodeğ‘›âˆˆğ‘‡at levelğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ do\n6: if|ğ‘›.ğ‘‘ğ‘ğ‘¡ğ‘|>ğµğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ¿ğ‘’ğ‘£ğ‘’ğ‘™then\n7: ğ‘ğ‘ğ‘›ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡â†ğ‘‡ğ‘Ÿğ‘¢ğ‘’ ,ğ‘›ğ‘’ğ‘¥ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’â†ğ‘›\n8: ifcanSplit then\n9:ğ‘= MCTS(ğ‘‡,ğ‘˜)\n10: perform partition ğ‘onğ‘›ğ‘’ğ‘¥ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’\n11: else ifğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ >1then\n12:ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ¿ğ‘’ğ‘£ğ‘’ğ‘™â†ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ¿ğ‘’ğ‘£ğ‘’ğ‘™âˆ’1,ğ‘ğ‘ğ‘›ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡â†ğ‘‡ğ‘Ÿğ‘¢ğ‘’\n13:returnğ‘‡\n14:procedure MCTS (ğ‘‡,ğ‘˜)\n15: Initialize search tree root ğ‘£0with stateğ‘‡\n16: forğ‘–=1toğ‘˜do\n17:ğ‘£ğ‘™â†ğ‘‡ğ‘Ÿğ‘’ğ‘’ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦(ğ‘£0)\n18: Î”â†ğ‘…ğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦(ğ‘£ğ‘™)\n19:ğµğ‘ğ‘ğ‘˜ğ‘ˆğ‘(ğ‘£ğ‘™,Î”)\n20: returnğ‘(ğµğ‘’ğ‘ ğ‘¡ğ¶â„ğ‘–ğ‘™ğ‘‘(ğ‘£0))\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:12 Jingyi Yang and Gao Cong\n5.4 MCTS Policy Designs\nChallenges. In order for MCTS to return good actions, the exploration strategy and simulation\nstrategy, i.e., selection policy and rollout policy, must be carefully designed. There are unique\nchallenges in designing the policies for our problem step. For the selection policy, as the return of a\nstate across different rollouts usually has a high variance, the standard Upper Confidence Bound for\nTrees (UCT) algorithm no longer works well. For the rollout out policy, due to a long action sequence\nand a huge state space, random rollouts lead to slow convergence. We now discuss in detail the\ndesign choices we make for the selection policy and rollout policy to address the challenges.\nSelection Policy. At the selection stage, the algorithm follows a ğ‘‡ğ‘Ÿğ‘’ğ‘’ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦ to find the most\npromising search tree node to expand. The key challenge in designing the selection policy is the\nexploitation-exploration dilemma, i.e., how to balance the choice between states of high approx-\nimated returns and unexplored states. A popular choice is the Upper Confidence Bound for Trees\n(UCT) algorithm. When selecting the search tree nodes, UCT considers both the average return\nfrom previous rollouts, which prioritize nodes with high returns, and an upper confidence bound\nterm, which prioritize less frequently visited child states.\nWhile the UCT policy works for many problems, using the average return in node selection\ndoes not work well for our problem setup. This is because the return of a state has a high variance\nacross different rollouts, and a good partition choice followed by some bad partition choices can\nlead to an extremely low return. If the average return is used for node selection, rollouts with high\nreturns may be averaged out by a few rollouts with low returns, making it hard to identify good\npartition actions. To address this issue, we propose a new selection policy.\nğ‘£â† arg max\nğ‘£â€²âˆˆğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘›ğ‘œğ‘“ ğ‘£ğ‘„ğ‘šğ‘ğ‘¥(ğ‘£â€²)+ğ‘âˆšï¸„\n2 lnğ‘(ğ‘£)\nğ‘(ğ‘£â€²)(3)\nThe first term is the maximal return from previous rollouts, and the second term is a confidence\nbound that makes the selection to prioritize less frequently visited child states. With the new design,\nthe selection policy considers the most successful previous experience instead of the average\nexperience, which prevents promising states from being overlooked during selection. We present\nourğ‘‡ğ‘Ÿğ‘’ğ‘’ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦ in Algorithm 2 .\nAlgorithm 2 ğ‘‡ğ‘Ÿğ‘’ğ‘’ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦(ğ‘£)\n1:whileğ‘£is non-terminal do\n2: ifğ‘£not fully expanded then\n3: randomly choose untried action ğ‘âˆˆğ´(ğ‘ (ğ‘£))\n4: add a new child ğ‘£â€²toğ‘£\n5: returnğ‘£â€²\n6: else\n7:ğ‘£â†arg maxğ‘£â€²âˆˆğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘›ğ‘œğ‘“ ğ‘£ğ‘„ğ‘šğ‘ğ‘¥(ğ‘£â€²)+ğ‘âˆšï¸ƒ\n2 lnğ‘(ğ‘£)\nğ‘(ğ‘£â€²)\nSubsequently, as our ğ‘‡ğ‘Ÿğ‘’ğ‘’ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦ makes the selection based on the maximum return from previous\niterations, we record the maximum ğ‘„-value for each state during Backpropagation.\nRollout Policy. The standard choice of ğ‘…ğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦ in MCTS is selecting a random action from\nthe action space. However, due to a long action sequence and a huge state space, random rollouts\nlead to slow convergence. To address the problem, we propose to incorporate domain knowledge of\nR-tree packing into the simulation using a greedy rollout policy. The greedy rollout policy chooses\nthe action that leads to the highest immediate reward, i.e., the partition with the most page skipping.\nThe greedy rollout policy makes better partitions than random rollouts, and the return obtained\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:13\nfrom the rollouts more closely approximates the optimal return. Thus, we need fewer iterations for\nMCTS to converge and find a good solution. We present our ğ‘…ğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦ in Algorithm 3.\nAlgorithm 3 ğ‘…ğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦(ğ‘£)\n1:Î”=ğ¶ğ‘¢ğ‘šğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ (ğ‘£)\n2:ğ‘ =ğ‘ (ğ‘£)\n3:whileğ‘ is non-terminal do\n4:ğ‘ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ =arg maxğ‘âˆˆğ´ğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘(ğ‘ ,ğ‘)\n5:ğ‘ â†ğ‘“(ğ‘ ,ğ‘ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)\n6: Î”â†Î”+ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘(ğ‘ ,ğ‘ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)\n7:return Î”\nComplexity. The complexity of PLATON (Algorithm 1) consists of two parts, the complexity of\nrunning the MCTS algorithm (line 15-20) at each partitioning step, as well as the complexity of the\ntop-down construction of the index on disk given the learned partitions. The running time of the\ntop-down construction of the tree is ğ‘‚((ğ‘/ğµ)log2ğ‘)[12]. We next focus on the time complexity of\nthe MCTS algorithm. We first consider the complexity of a single MCTS iteration. The complexity\nof the selection and expansion step is ğ‘‚(ğ‘˜)(line 17). For the simulation step (line 18), we need to\nconstruct the entire R-tree using the greedy rollout policy. Computing and performing the greedy\npartition action for a single partially constructed tree node ğ‘›incurs a complexity of ğ‘‚(ğµÂ·|ğ‘›.ğ‘‘ğ‘ğ‘¡ğ‘|).\nAs we construct the entire tree, we add up this complexity for all nodes. We first add up the\ncomplexity for all nodes at the same level, which gives us a complexity of ğ‘‚(ğµÂ·ğ‘)because the\nsum of the node sizes at the same level is ğ‘‚(ğ‘). We then add up the complexity for all logğµğ‘\nlevels of partially constructed R-tree nodes, which leads to a time complexity of ğ‘‚(ğµÂ·ğ‘logğ‘)for\nthe simulation step. The complexity of the backpropagation step is ğ‘‚(ğ‘˜)(line 19). As a result, the\ncomplexity of an MCTS iteration is dominated by that of the simulation step and is ğ‘‚(ğ‘logğ‘).\nWe then multiply the per-iteration complexity by the number of iterations ğ‘˜and the total number\nof actionsğ‘/ğµ, which gives a time complexity of ğ‘‚(ğ‘˜Â·ğ‘2logğ‘)for the MCTS algorithm.\nTo summarize, PLATON â€™s complexity is the sum of the complexity of the top-down construction\nof the index, ğ‘‚((ğ‘/ğµ)log2ğ‘), and the complexity of the MCTS algorithm, ğ‘‚(ğ‘˜Â·ğ‘2logğ‘). We note\nthat in practice, the total running time is dominated by the running time of the MCTS algorithm.\n6 OPTIMIZATION OF MCTS\nThe MCTS algorithm has a running time that dominates the total running time. In this section, we\npropose to reduce the running time of MCTS with a divide and conquer strategy. Together with\ntwo optimization techniques, early termination and level-wise sampling, we are able to drastically\nreduce the time complexity of the MCTS algorithm to linear time.\nThe main reason of the high complexity is the long action sequence. For example, for a three-level\nR-tree with node size ğµ=100, there is a total of 9999 actions. To make the first partition decision,\nthe MCTS algorithm needs to consider all 9998 follow-up actions, which results in the long running\ntime of the rollouts. We first make the observation that the optimal partition decision on the two\nchild nodes of a partially constructed tree node does not depend on each other. Formally speaking,\nif we denote the optimal total return from partitioning partially constructed tree node ğ‘›and its\nchild nodes as ğ‘‰âˆ—(ğ‘›), we have the following:\nğ‘‰âˆ—(ğ‘›)=max\nğ‘âˆˆğ´(ğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘(ğ‘›,ğ‘)+ğ‘‰âˆ—(ğ‘›1)+ğ‘‰âˆ—(ğ‘›2)), (4)\nwhereğ‘›1andğ‘›2are the two partially constructed tree nodes created from a partition action a on\nnodeğ‘›. The equation above identifies the optimal substructure in our problem: in order to find the\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:14 Jingyi Yang and Gao Cong\nMCTS\tğ‘›!\tğ‘›\"\tğ‘›#MCTS\tğ‘›!\tğ‘›\"\tğ‘›#\tğ‘›$\tğ‘›%\tğ‘›!\tğ‘›\"\tğ‘›#\tğ‘›$\tğ‘›%\tğ‘›&\tğ‘›'\tğ‘›!\tğ‘›\"\tğ‘›#\tğ‘›$\tğ‘›%\tğ‘›&\tğ‘›'MCTSMCTSMCTSOriginal MCTS algorithmDivide and Conquer\nFig. 7. Illustration of divide and conquer.\nsequence of actions that maximizes the total return from a partially constructed tree node ğ‘›, we\nonly need to independently solve the subproblems of finding the sequence of actions that maximizes\nthe total return from the two child nodes ğ‘›1andğ‘›2. In other words, to construct the subtree rooted\natğ‘›with the maximal return, we break it down into two subproblems of constructing the subtrees\nrooted atğ‘›1andğ‘›2with the maximal return respectively.\nWith the optimal substructure identified, we can leverage a divide and conquer strategy to reduce\nthe algorithm complexity. For the partition decision on a node ğ‘›, instead of running MCTS on the\noriginal state space, we consider a new state space consisting of smaller trees, i.e., the subtrees\nrooted atğ‘›. The new state space is equivalent to the state space we have for constructing an R-tree\nfromğ‘›.ğ‘‘ğ‘ğ‘¡ğ‘ . To run MCTS on this new state space, we create a new partially constructed tree with\na single root node ğ‘›, initialize a search tree with this partially constructed tree state, and perform\nthe MCTS iterations to find a good action, as if we are constructing a new R-tree from ğ‘›.ğ‘‘ğ‘ğ‘¡ğ‘ .\nFigure 7 illustrates how divide and conquer helps make the MCTS algorithm run faster. The dashed\nboxes denote the states that form the search tree root of MCTS. Previously, we always run MCTS\non the original partially constructed tree state space to find the partition actions for nodes ğ‘›1and\nğ‘›2. As a result, to find the partition action for ğ‘›1, MCTS also considers the partition actions on ğ‘›2\nand its child nodes at the simulation step, leading to slow rollouts. With divide and conquer, to\nfind the partition action on ğ‘›1/ğ‘›2, we only need to run MCTS algorithm on the new state space\nof subtrees rooted at ğ‘›1/ğ‘›2, and the action sequence is much shorter. This greatly accelerates the\nrollouts.\nAlgorithm 4 shows the pseudocode of MCTS with the divide and conquer strategy. We maintain\na queue of unpartitioned nodes (line 2). We repeatedly pop nodes from the queue and perform\npartitions as long as the queue is not empty (line 3). Given a node ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’ , instead of running\nMCTS on the original state space, we run MCTS on the state space of subtrees rooted at ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’\nto get the partition action ğ‘(line 5). After obtaining the two newly created nodes ğ‘›1andğ‘›2from\nthe partition (line 6), we add them to the queue as long as they are not leaf nodes (line 7).\nEarly termination. With the divide and conquer strategy, to carry out a rollout, we still need to\nconstruct the entire subtree using the greedy policy, which involves a long action sequence if the\npartition is on a high-level node. To further speed up the rollouts, we propose early termination:\ninstead of using the greedy policy to construct the entire subtree, ğ‘…ğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦ will terminate as\nlong as all the next-level partially constructed tree nodes are created. For example, if we are finding\nthe optimal partition action on a partially constructed tree node ğ‘›at levelğ‘¡with sizeğµğ‘¡, a rollout\nwill terminate as long as all ğµchild nodes at level ğ‘¡âˆ’1are created. This further reduces the number\nof steps to run for each rollout.\nLevel-wise sampling. To accelerate the training of RL, we learn on a sample set of data. Intuitively,\none can learn the R-tree partition policy on a data sample, where the size of the partially constructed\ntree nodes scales by the sample rate. However, the inherent structure of the R-tree makes it hard to\ndirectly learn from a data sample. Consider building an R-tree on a data set of size 1000,000, with\nnode capacity ğµ=100. If we perform top-down packing on a sample of 5%of the data, then each\nleaf node on average contains 100Ã—5%=5data object. As a result, the leaf node MBRs are no\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:15\nAlgorithm 4 PLATON with Divide and Conquer\nInput: datasetD, workloadW, node capacity ğµ, # of iterations ğ‘˜\nOutput: packed R-tree ğ‘‡\n1:ğ‘‡â†{ğ‘›0},ğ‘›0.ğ‘‘ğ‘ğ‘¡ğ‘ =D,ğ‘›0.ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ =âŒˆlogğµğ‘âŒ‰\n2:ğ‘„ğ‘¢ğ‘’ğ‘¢ğ‘’ ={ğ‘›0}\n3:while !ğ‘„ğ‘¢ğ‘’ğ‘¢ğ‘’.ğ‘’ğ‘šğ‘ğ‘¡ğ‘¦()do\n4:ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’ =ğ‘„ğ‘¢ğ‘’ğ‘¢ğ‘’.ğ‘ğ‘œğ‘()\n5:ğ‘= MCTS(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’,ğ‘˜ )\n6:ğ‘›1,ğ‘›2â†ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘ğ‘œğ‘‘ğ‘’,ğ‘)\n7: addğ‘›1andğ‘›2toğ‘„ğ‘¢ğ‘’ğ‘¢ğ‘’ if not leaf node\n8:returnğ‘‡\n9:procedure MCTS (ğ‘›,ğ‘˜)\n10:ğ‘›â€²=ğ‘†ğ‘ğ‘šğ‘ğ‘™ğ‘’(ğ‘›,ğ‘Ÿğ‘ğ‘¡ğ‘’ =ğ‘ /ğµğ‘›.ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™);\n11:ğ‘‡ğ‘›ğ‘’ğ‘¤={ğ‘›â€²}\n12: Initialize search tree root ğ‘£0with stateğ‘‡ğ‘›ğ‘’ğ‘¤\n13: forğ‘–=1toğ‘˜do\n14:ğ‘£ğ‘™â†ğ‘‡ğ‘Ÿğ‘’ğ‘’ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦(ğ‘£0)\n15: Î”â†ğ‘…ğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ğ‘ƒğ‘œğ‘™ğ‘–ğ‘ğ‘¦(ğ‘£ğ‘™); # Early termination\n16:ğµğ‘ğ‘ğ‘˜ğ‘ˆğ‘(ğ‘£ğ‘™,Î”)\n17: returnğ‘(ğµğ‘’ğ‘ ğ‘¡ğ¶â„ğ‘–ğ‘™ğ‘‘(ğ‘£0))\nlonger representative of the original leaf node MBRs when the R-tree is built on the full dataset,\nand the rewards derived from the samples may deviate significantly from that of the original data.\nWhile a small sample rate makes leaf node MBRs no longer representative, for higher-level\nnodes, it is possible to apply a small sample rate. Under the same example, with a sample rate of\n5%, nodes at level 2on average contain 1002Ã—5%=500data objects, which can still form node\nMBRs that are sufficiently representative of the original node MBRs. By taking into consideration\nthe varying node sizes at different levels, we develop a level-wise sampling technique that applies\na different sampling rate to partially constructed tree nodes at different levels when running the\nMCTS algorithm. The core idea of level-wise sampling is that after applying data sampling, bottom-\nlevel tree nodes at the terminal states have a fixed size ğ‘ , which ensures that the result node MBRs\nare still representative of the original node MBRs. Level-wise sampling applies a sample rate of\nğ‘ /ğµğ‘™when running the MCTS algorithm to partition a partially constructed R-tree node into child\nnodes at level ğ‘™. Level-wise sampling makes the MCTS algorithm more efficient using samples,\nwithout compromising the accurate calculation of rewards.\nComplexity. We now analyze the time complexity of the MCTS algorithm with the proposed\noptimization (Algorithm 4). Similar to the analysis in Section 5, we compute the complexity of the\nsimulation step in an MCTS iteration (line 15). With early termination, we compute and perform the\ngreedy partition actions for ğ‘‚(ğµ)steps. At each step, the size of a node is ğ‘‚(ğµÂ·ğ‘ )after level-wise\nsampling (line 10) , and therefore it takes ğ‘‚(ğµ2Â·ğ‘ )to compute and perform the greedy action\nfor the sampled node. We multiply the two terms and obtain a complexity of ğ‘‚(ğµ3Â·ğ‘ )for the\nsimulation step. The complexity of an MCTS iteration is therefore also ğ‘‚(ğµ3Â·ğ‘ ). We then multiply\nthe per-iteration complexity by the number of iterations ğ‘˜and the total number of actions ğ‘/ğµ,\nwhich gives us a time complexity of ğ‘‚(ğ‘˜Â·ğµ2Â·ğ‘ Â·ğ‘). As discussed earlier, ğ‘ is a constant in level-wise\nsampling, and therefore the complexity of the MCTS algorithm is effectively ğ‘‚(ğ‘˜Â·ğ‘). Note that the\ntop-down construction of the tree in PLATON still has a complexity of ğ‘‚((ğ‘/ğµ)log2ğ‘). However,\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:16 Jingyi Yang and Gao Cong\nat the data scale this work concerns, the total running time is always dominated by that of the\nMCTS algorithm. Thus reducing the complexity of the MCTS algorithm to linear with regard to\nthe input size ğ‘makes PLATON scalable to large datasets.\n7 PACKING WITHOUT WORKLOAD\nWhen a workload is not available at the time of index construction, we propose to learn a partition\npolicy that optimizes the I/O cost of a workload that follows the data distribution. We first provide\nthe intuition on why this can lead to a well-packed R-tree.\nğ‘¦ğ‘¥ğ‘…!ğ‘…\"ğ‘…#ğ‘…$ğ‘…%ğ‘…&ğ‘…'ğ‘…(ğ‘…)ğ‘$ğ‘#ğ‘\"ğ’©!ğ’©\"ğ’©#ğ‘…\"ğ‘…!ğ‘…#ğ‘…$ğ‘…%ğ‘…&ğ‘…'ğ‘…(ğ‘…)Optimal R-tree w.r.t.the workload\nFig. 8. Motivating example of optimizing for a synthetic query workload following the data distribution.\nIntuition. Consider the example in Figure 8, where we need to pack 9 data points into a 2-level\nR-tree with node capacity ğµ=3. The data points are distributed over three clusters. We have a\nquery workloadW={ğ‘1,ğ‘2,ğ‘3}containing three window queries, and the centers of the query\nwindows follow the data distribution. The R-tree that minimizes the I/O cost of the workload Wis\nshown on the right of Figure 8, which leads to an I/O cost of 2 for each query. Note that this R-tree\nessentially packs the data objects in the same cluster into the same leaf node, which is the ideal\nR-tree given no specific assumption on the workload pattern.\nThe reason why our proposed objective works is as follows. An R-tree that minimizes the I/O\ncost of a query workload would pack data objects overlapping the same query window into the\nsame leaf node or subtree. As we sample the queries from the data distribution, the constructed\nR-tree packs each data object into the same leaf node or subtree as its neighbors, achieving a good\nclustering property. Moreover, the generated workload covers the entire data space, therefore the\nconstructed R-tree would perform well for queries accessing different regions of the data space.\nAnother problem is how to set the height and width of the query windows. A naive approach is\nto uniformly sample window height and width from a range to form query windows of different\nsizes. For example, to generate query window sizes between 0.0001% and1%of the unit square, we\ncan simply sample from the range [0.001,0.1]. However, uniform sampling results in an unbalanced\nportion of large query windows. In fact, only 10%of samples are from [0.001,0.01], while 90%of\nsamples are from[0.01,0.1]. To remedy this, we sample the height and width from a log-uniform\ndistribution, i.e., the log of the sampled values are uniformly distributed. As a result, there will be\nas many samples from [0.001,0.01]as samples from[0.01,0.1], creating a much more balanced\nratio between small query windows and large query windows.\n8 EXPERIMENTS\n8.1 Experimental Settings\nDatasets. We use three synthetic datasets and three real-world datasets for our experiments. Each\nsynthetic dataset contains 10 million data objects generated within a unit square. We generate\nsynthetic datasets following three types of data distributions: Uniform, Skew, Gaussian.\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:17\nâ€¢Uniform (UNI): rectangle data whose centers are uniformly distributed over the unit square. The\nheight and width of the rectangles are uniformly sampled from [0,0.001].\nâ€¢Skew: point data with a distribution that is highly skewed over one dimension. Following previous\nwork on R-tree packing [ 12,48], we first generate uniformly distributed points over the unit\nsquare, and then squeeze the ğ‘¦-dimension by raising ğ‘¦toğ‘¦9.\nâ€¢Gaussian (GAU): rectangle data following Gaussian distribution. We generate the center of the\nrectangles by sampling each dimensionâ€™s value from N(0,1). We then normalize sampled values\nto ensure the centers fall within the unit square. The height and width of the rectangles are\nuniformly sampled from [0,0.001].\nThe real-world datasets are from US Census Bureau TIGER/Line Shapefiles [ 6] and OpenStreetMap [ 4].\nA summary of the real-world datasets is in Table 1. The Area Water dataset and the OSM Parks\ndataset are pre-processed and published by SpatialHadoop [ 22]. We use four rectangle datasets and\ntwo point datasets.\nName Type Cardinality Description\nArea Water(AW) Polygon 2.3M US Area Hydrography\nOSM Parks(PARK) Polygon 10M Parks around the world\nOSM India(IND) Point 100M Landmarks in India\nTable 1. Summary of real-world datasets.\nWorkloads. We generate workloads with different patterns to train PLATON and to evaluate\ndifferent methods. For each workload pattern, we generate a training workload and a test workload,\nboth of which contain 10000 queries. For the synthetic datasets, we consider two aspects of the\nworkload patterns:\nâ€¢aspect ratio : We generate 4 workloads with aspect ratio of 10, 100, 1000, and 10000. The queries\nfollow the uniform distribution and have a fixed size of 0.001% .\nâ€¢size: We generate 5 workloads with query size of 0.001% ,0.005% ,0.01%,0.05%and0.1%of the\nentire data space. The generated queries follow the uniform distribution and have random shapes.\nFor real-world datasets, we generate workloads with different patterns using the concept of\ndecimal degree [1]. Decimal degree is an alternative unit for geographic coordinates, and different\ndecimal degree scales have different interpretations in the physical world. For each real-world\ndataset, we generate four workloads with decimal degree scales of 0.001, 0.01, 0.1, and 1, which\ncorrespond to street level, town level, city level, and country level in the real world. The centers of\nthe query windows are sampled from the data distribution. One advantage of generating workload\nin this way is that the queries have actual meaning. For example, the generated workloads contain\nqueries that answer \"How many rivers cross XXX town of US\", or \"Find all parks in XXX city\".\nBaseline Methods. We compare PLATON with two classes of methods: R-tree variants and\nlearned/workload-aware spatial indexes. The R-tree variants include R-tree constructed from\nrepeated insertion (R*-tree), and four R-tree packing methods: STR [ 39], TGS [ 25], PR-tree [ 12],\nand a rank space SFC-based method HRR [ 48]. The four methods are the best-performing methods\nreported in previous works. The four learned/workload-aware indexes1are learned index RSMI [ 47]\nand LISA [ 40], ML-enhanced index RLR-tree [ 26], and workload-aware index Waffle [ 44]. RLR-tree\nconstructs an R-tree through repeated insertion instead of packing. Note that RSMI, LISA, and\n1We do not include RW-tree in the experiments as the source code is not available after communication with the authors.\nWe note that RW-tree reported up to 1.24 Ã—speedup over R*-tree, while PLATON achieves up to 3.17 Ã—speedup.\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:18 Jingyi Yang and Gao Cong\nWaffle only support point data. We find that the released code of RSMI only works on the OSM\nIndia dataset with small data size, i.e., 10 million. Therefore, we exclude RSMI from the experiments\non other datasets and scalability. We use PLATON-D to denote the learned partition policy from\nthe data distribution.\nImplementation. We implement PLATON on top of two real-world systems: libspatialindex [ 2], a\npopular open source library of spatial indexes, as well as PostgreSQL 14.3 [ 9], with the PostGIS [ 8]\nextension for spatial index. Our implementations use C++ and Python. We also implement all R-tree\nvariants on top of both libspatialindex and PostgreSQL. For learned/workload-aware indexes, we\nuse the code released by the authors [ 3,5,7]. We aligned the implementation of PLATON and the\nbaseline methods and used the same encoding to store the records. Specifically, for each record, we\nuse 4 double (8 bytes each) to store the MBR coordinate values, and 4 bytes to store the address,\nwhich leads to a total of 4Ã—8+4=36bytes per record. All experiments are conducted on a Ubuntu\nserver with a 20-core E5-2698 v4 @ 2.20GHz CPU.\nMetric. We measure the performance of different methods based on both query I/O cost and query\nlatency.\nI/O Cost. For PLATON and R-tree variants, we obtain the I/O cost from the libspatialindex\nimplementation. We include the I/O cost of the intermediate nodes when comparing PLATON with\nthe R-tree variants. When comparing PLATON with learned/workload-aware indexes, we assume\nthe intermediate nodes of the tree-based indexes, i.e.,PLATON , RLR and Waffle, are cached in the\nmemory, because disk-based learned indexes, i.e., RSMI and LISA, assume their models are cached\nin the memory. We normalize the I/O cost by an I/O lower bound, which is calculated by assuming\nthe result data objects of each query are packed in a minimal subtree. The lower bound includes\nI/Os of the intermediate nodes if the I/O cost being compared includes intermediate nodes, and\nvice versa.\nQuery Latency. We obtain the query latency from PostgreSQL. We normalize the results by\nPLATON â€™s query latency.\nParameters. We set the page size to 4KB by default. With 4KB pages, the libspatialindex imple-\nmentation of PLATON and the R-tree variants, as well as all learned/workload-aware methods,\nhave a node capacity ğµof\u00044096\n36\u0005\n=113. For the PostgreSQL implementation of PLATON and the\nR-tree variants, as PostgreSQL reserves 44 bytes for metadata per page, the node capacity is given\nby\u00044096âˆ’44\n36\u0005\n=112. We set the level-wise sampling parameter ğ‘ =ğµ.\n8.2 Comparison with R-tree variants\nWe evaluate the performance of PLATON in comparison with existing R-tree variants for different\ntypes of workloads. Below we report several major findings from the results.\n(F1) PLATON significantly outperforms all existing R-tree variants in terms of both query\nI/O and query latency across different workload patterns. Figure 9 shows the overall result of\nPLATON and existing R-tree variants, in terms of normalized I/O and normalized query latency,\nrespectively. We observe that PLATON significantly outperforms all existing R-tree variants in terms\nof both query I/O and query latency. The performance improvement is a result of PLATON â€™s ability\nto adapt to both the data distributions and workload patterns. Compared to the best-performing\nbottom-up and top-down methods, i.e., STR and TGS, PLATON achieves a speedup of up to 2.70Ã—\nand3.80Ã—respectively in terms of query I/O. The query latency result in general aligns with the\nquery I/O result, with PLATON achieving a speedup of up to 2.03Ã—and2.32Ã—compared to STR and\nTGS. We notice that the performance gaps are slightly reduced in terms of query latency due to\nthe existence of the database buffer during query execution. For the remaining experiment results,\nwe follow previous work and only report query I/O. Figures 10 - 12 show the performance of\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:19\n 0 2 4 6 8 10 12 14\nUNI SKEW GAU AW PARK INDNormalized I/O\n 0 0.5 1 1.5 2 2.5 3 3.5 4\nUNI SKEW GAU AW PARK INDNormalized query latencyPR\nR*\nSTR\nTGS\nHRR\nPLATON-D\nPLATON\nFig. 9. Query performance of PLATON and R-tree variants, query size 0.001%, decimal degree 0.001.\n 0 2 4 6 8 10 12 14 16 18\n10 100 1000 10000Normalized I/O\nAspect ratioUniform\n 0 10 20 30 40 50 60\n10 100 1000 10000Normalized I/O\nAspect ratioSkew\n 0 2 4 6 8 10 12\n10 100 1000 10000Normalized I/O\nAspect ratioPR\nR*\nSTR\nTGS\nHRR\nPLATON-D\nPLATONGaussian\nFig. 10. Query I/O of PLATON and R-tree variants on workloads with varying query aspect ratios.\nPLATON and existing R-tree variants on workloads with varying aspect ratios, sizes, and decimal\ndegree scales respectively. We now discuss the observations for each type of workload pattern.\n(1)Window query with varying aspect ratios. PLATON achieves the biggest performance improve-\nment on workloads with a large aspect ratio. This shows the benefit of adapting to both the data\ndistribution and workload distribution, especially when the workload has a special pattern. Com-\npared to the best-performing baseline method, PLATON achieves a speedup of up to 2.90Ã—,16.31Ã—,\nand2.15Ã—on the Uniform, Skew, and Gaussian dataset respectively.\n(2)Window query with varying sizes. PLATON achieves the biggest performance improvement\non workloads with a small query size. Compared to the best-performing baseline method, PLA-\nTON achieves a speedup of up to 1.52Ã—,2.70Ã—, and 1.35Ã—on the Uniform, Skew, and Gaussian\ndataset respectively.\n(3)Window query with varying decimal degree scales on real-world dataset. We note that among the\nexisting R-tree variants, the best-performing method is different for each dataset, which shows that\nno heuristic rule works well for all data distributions. Meanwhile, PLATON consistently outperforms\nall existing R-tree variants across the datasets. Compared to the best-performing baseline method,\nPLATON achieves a speedup of up to 1.18Ã—,1.31Ã—, and 1.29Ã—on the Area Water, OSM Parks, and\nOSM India dataset respectively.\n(F2) If there is no workload available, by training from a synthetic workload following\ndata distribution, PLATON still significantly outperforms all existing R-tree variants\nacross different workload patterns. From Figure 9, we can see that compared to existing R-tree\nvariants, PLATON-D achieves the best performance in 4 out of 6 datasets and top-2 performance in\n5 out of 6 datasets in terms of query I/O. From Figure 10 and Figure 11, we observe that PLATON-\nDoutperforms all existing R-tree variants on the Skew dataset and Gaussian dataset for different\nworkload patterns. The performance of PLATON-D is most noticeable on real workload datasets.\nFigure 12 shows that PLATON-D achieves similar performance as PLATON across all three real-\nworld datasets, even outperforming PLATON for smaller decimal degree scales. This demonstrates\nthe effectiveness of optimizing for a synthetic query workload following data distribution.\n(F3) PLATON significantly outperforms all existing R-tree variants for KNN queries.\nFigure 13 shows the performance of PLATON and R-tree variants for K-Nearest-Neighbor (KNN)\nqueries. We sample the query points from the data distribution and conduct experiments for ğ¾=\n1,5,25,125,625. We observe that although PLATON is optimized for window query performance, it\noutperforms all existing R-tree variants for KNN queries.\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:20 Jingyi Yang and Gao Cong\n 0 2 4 6 8 10\n0.001% 0.005% 0.001% 0.05% 0.1%Normalized I/O\nSizeUniform\n 0 2 4 6 8 10 12\n0.001% 0.005% 0.001% 0.05% 0.1%Normalized I/O\nSizeSkew\n 0 1 2 3 4 5 6 7 8\n0.001% 0.005% 0.001% 0.05% 0.1%Normalized I/O\nSizePR\nR*\nSTR\nTGS\nHRR\nPLATON-D\nPLATONGaussian\nFig. 11. Query I/O of PLATON and R-tree variants on workloads with varying query sizes.\n 0 0.5 1 1.5 2 2.5 3 3.5 4\n0.001 0.01 0.1 1Normalized I/O\nDecimal degreeArea Water\n 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5\n0.001 0.01 0.1 1Normalized I/O\nDecimal degreeOSM Parks\n 0 1 2 3 4 5\n0.001 0.01 0.1 1Normalized I/O\nDecimal degreePR\nR*\nSTR\nTGS\nHRR\nPLATON-D\nPLATONOSM India\nFig. 12. Query I/O of PLATON and R-tree variants on workloads with varying decimal degree scales.\n12345\n1 5 25 125 625I/O\nKPR R* STR TGS HRR PLATON\nIndia\n1*1052*1053*105\n1 5 25 125 625I/O\nKIndia\n2*1054*1056*1058*1051*106\n1 5 25 125 625I/O\nKIndia\nFig. 13. Query I/O of PLATON and R-tree variants on KNN queries.\n(F4)PLATON significantly outperforms all existing R-tree packing methods for spatial join\nqueries. Table 2 shows the performance of PLATON and R-tree variants for spatial join queries. We\nperform spatial join over the OSM Parks and Area Water dataset to find all pairs of (park, water area)\nthat intersects with each other. We implement two classic spatial join algorithms [ 32], index-nested\nloop join [ 23] and hierarchical traversal [ 28]. For the index-nested loop join, we only build an R-tree\nindex on the OSM Parks dataset and loops over the Area Water dataset to find the parks intersecting\neach water area. For the hierarchical traversal method, we build R-tree index on both datasets.\nFrom the results, we observe that with Index-nested loop join, PLATON outperforms all existing\nR-tree variants; with hierarchical traversal, PLATON underperforms R*-tree but outperforms all\nexisting R-tree packing methods. One interesting finding is that R*-tree outperforms all existing\nR-tree packing methods on spatial join.\nMethod R* PR STR TGS HRR PLATON\nIndex-nested loop ( Â·107) 1.24 2.75 1.49 1.37 1.31 1.00\nHierarchical traversal ( Â·105)1.42 3.84 2.14 2.24 2.40 1.73\nTable 2. Query I/O of PLATON and R-tree variants on spatial join queries.\n8.3 Comparison with Learned/Workload-aware Spatial Indexes\nFigure 14 shows the performance of PLATON in comparison with learned/workload-aware indexes\non the two point datasets. Note that RSMI, LISA, and Waffle only support point data. We only\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:21\nevaluate RSMI on the OSM India dataset as the released code does not work on the synthetic skew\ndataset. We observe that PLATON outperforms all baseline methods across all query sizes on the\ntwo datasets, with a speedup of up to 2.30Ã—and1.42Ã—compared to the best performing baseline\nWaffle.\n 0 1 2 3 4 5 6 7\n0.001 0.01 0.1 1Normalized I/O\nDecimal degreeRSMI LISA RLR Waffle PLATON\nOSM India\n 0 5 10 15 20\n0.001% 0.005% 0.01% 0.05%Normalized I/O\nSizeSkew\n 0 1 2 3 4 5 6 7\n0.001 0.01 0.1 1Normalized I/O\nDecimal degreeOSM India\nFig. 14. Query I/O of PLATON in comparison with learned/workload-aware indexes.\n8.4 Does MCTS optimizes the long-term reward?\nWe investigate whether MCTS optimizes the long-term reward through comparison with a greedy\npartition policy that maximizes the immediate reward at each step. Table 3 shows PLATONâ€™s\nspeedup over the greedy partition policy on different datasets. We observe the PLATON significantly\noutperforms the greedy partition policy across all datasets, with the most speedup observed on\nlarge datasets. This shows the effectiveness of MCTS in avoiding locally optimal decisions and\noptimizing the long-term reward.\nDataset UNI SKEW GAU AW PARK IND\nSpeedup 1.04 1.13 1.22 1.07 1.17 1.69\nTable 3. PLATONâ€™s speedup over the greedy partition policy.\n8.5 Effect of Optimization Techniques\nWe conduct experiments to evaluate the effect of the proposed optimization techniques. The left\nplot in Figure 15 shows the log-log plot of the training time of PLATON with data sizes ranging\nfrom 100K to 100 million on the OSM India dataset. From the figure, we observe that the training\ntime of PLATON w/o optimization scales super-linearly with regard to the dataset size. With the\nproposed optimization, the training time is drastically reduced, and becomes linear with regard to\nthe data size.\n8.6 Scalability\nWe conduct experiments to evaluate whether PLATON is scalable to larger datasets. We generate\nfour data samples with sizes ranging from 25 million to 100 million from the OSM India dataset.\nThe right plot in Figure 15 shows the overall construction time (including training time) of\nPLATON in comparison with the learned index LISA. We observe that the construction of PLATON is\nmuch faster than LISA for large data sizes, and is only slower than LISA for 25 million records. The\nconstruction time of PLATON scales linearly with regard to the data size, which is consistent with\nour analysis in Section 6. We note that existing R-tree variants and workload-aware indexes like\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:22 Jingyi Yang and Gao Cong\n 0.1 1 10\n 0.1  1  10  100Training time (hours)\nDataset size (million)PLATON w/o optimization\nPLATON\n 0 2 4 6 8 10 12 14 16\n25 50 75 100Construction time (hours)\nDataset size (million)PLATON LISA\nFig. 15. Scalability results.\nWaffle have a shorter construction time, i.e., several minutes, on such data scales as they do not\nrequire training. As the bulk-loading of an index is typically performed in an offline manner, the\ntraining overhead of PLATON is acceptable.\nWe also conduct experiments to evaluate whether PLATON keeps the performance gain on larger\ndatasets. Figure 16 shows the query I/O of PLATON and baseline methods with varying data sizes.\nFrom the left chart, we observe that PLATON consistently outperforms all R-tree variants, as well\nas all learned/workload-aware indexes as the data size increases.\n 0 1 2 3 4 5 6 7\n4KB 8KB 12KB 16KBNormalized I/O\nPage sizePR R* STR TGS HRR PLATON LISA RLR Waffle\nR-tree variants\n 1 1.5 2 2.5 3 3.5 4 4.5 5\n25 50 75 100Normalized I/O\nData size (million)R-tree variants\n 1 1.2 1.4 1.6 1.8 2 2.2 2.4\n25 50 75 100Normalized I/O\nData size (million)Learned/Workload-aware indexes\nFig. 16. Query I/O of PLATON and baseline methods with varying data sizes.\n8.7 Effect of pages size\nFigure 17 shows the performance of PLATON and baseline methods with varying page sizes. We\nconduct experiments on the OSM India dataset for page size of 4KB, 8KB, 12KB, and 16KB, which\ncorresponds to node capacity of 113, 227, 341, and 455 respectively. From the results, we observe that\nPLATON consistently outperforms existing R-tree variants and learned/workload-aware indexes as\nthe page size varies.\n 0 1 2 3 4 5 6 7\n4KB 8KB 12KB 16KBNormalized I/O\nPage sizePR R* STR TGS HRR PLATON LISA RLR Waffle\nR-tree variants\n 1 1.5 2 2.5 3 3.5 4\n4KB 8KB 12KB 16KBNormalized I/O\nPage sizeR-tree variants\n 1 1.2 1.4 1.6 1.8 2 2.2 2.4\n4KB 8KB 12KB 16KBNormalized I/O\nPage sizeLearned/Workload-aware indexes\nFig. 17. Query I/O of PLATON and baseline methods with varying page sizes.\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:23\n8.8 Effect of dimensionality\nWe conduct experiments to evaluate the applicability and scalability of PLATON in higher dimen-\nsions. We generate hyper-rectangle datasets of size 10 million in up to 6-dimensional space. The\nrectangle centers follow uniform distribution, and the edge length in each dimension is uniformly\nsampled from (0, 0.001]. The query windows have a fixed size of 0.001% of the data space and\nhave random shapes. The left plot in Figure 18 shows the query I/O of PLATON and the R-tree\nvariants as the dimension increases. We observe that while the performance of existing R-tree\nvariants drastically degrades as the number of dimensions increases, PLATON remains highly\nefficient in higher dimensions. The right plot in the figure shows the training time of PLATON as\nthe dimension varies. We observe that the training time of PLATON is linear with regard to the\nnumber of dimensions.\n 0 50 100 150 200 250\n2 3 4 5 6Normalized I/O\nDimensionPR R* STR TGS HRR PLATON\n 0 50 100 150 200 250\n2 3 4 5 6Normalized I/O\nDimension\n 0 2 4 6 8 10\n2 3 4 5 6Training time (hours)\nDimension\nFig. 18. Effect of dimensionality\n8.9 Workload shifts\nFigure 19 shows the performance of PLATON when the test workload shifts from the training\nworkload in terms of workload characteristics. We evaluate two workload shift scenarios: shift in\nquery aspect ratio and shift in query size. In both scenarios, we train PLATON on a default workload,\ni.e., query aspect ratio of 10 and decimal degree scale of 0.1 respectively, and evaluate its performance\non workloads with a range of aspect ratios and sizes. We denote PLATON trained on the default\nworkload as PLATON-S . We compare the performance of PLATON under workload shifts with the\nbest-performing baseline methods, the workload-aware index Waffle, as well as PLATON-D . From\nthe left plot, we observe that PLATON trained on the default workload consistently outperform\nthe baseline methods as the query aspect ratio changes, while PLATON-D underperforms Waffle.\nFrom the right plot, we observe that PLATON-D achieves the best performance for all query sizes.\nPLATON trained on the default workload slightly underperforms PLATON-D , but still outperforms\nWaffle. This shows that PLATON is robust to workload shifts.\n 1 1.2 1.4 1.6 1.8 2 2.2\n0.001 0.01 0.1 1Normalized I/O\nDecimal degreePLATON-S PLATON-D Waffle\n01020304050\n10 100 1000 10000Normalized I/O\nAspect ratio\n 1 1.2 1.4 1.6 1.8 2 2.2\n0.001 0.01 0.1 1Normalized I/O\nDecimal degree\nFig. 19. Performance of PLATON under workload shifts\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:24 Jingyi Yang and Gao Cong\n9 RELATED WORK\nR-Tree Variants. R-tree [ 29] was first proposed by Antonin Guttman in 1984 and has since been\nextensively studied by the research community. R+-tree [51], R*-tree [13], and RR*-tree [14] were\nproposed to improve the insertion and deletion operation and produce a better tree structure.\nMeanwhile, a number of works have studied the problem of R-tree packing [ 12,15,17,25,30,31,\n33,38,39,48,49], which constructs an R-tree with better space utilization and query performance.\nA detailed discussion of existing R-tree packing methods can be found in Section 2. Waffle [ 44]\ncombines the concepts of data partitioning from R-tree with space partitioning and proposes an\nindex structure with square-like, non-overlapping nodes.\nMachine Learning for Spatial Index. The initial idea of learned index [ 37] was to replace 1-d\nindex structures like B-tree with a machine learning model that learns the cumulative density\nfunction (CDF) of the data. The model then maps the search key to the storage id based on the\nlearned CDF. Several variants [ 19,24,34,41] have been proposed to achieve better efficiency\nand robustness. Learned spatial indexes extended the idea to multi-dimensional spatial data. ZM\nindex [ 53] was proposed for 2-d spatial data points. It linearizes the spatial points using a space-\nfilling curve, e.g., Z-curve, and learns to model the CDF of the z-order values. RSMI [ 47] improved\nover ZM index by introducing a rank space-based ordering and a recursive partitioning strategy.\nInstead of relying on space-filling curves, LISA [ 40] was proposed to directly learn a mapping from\nspatial data points to 1-d value. Flood [ 45] proposed a grid-based index for multi-dimensional data\nthat adapts to a particular dataset and workload. Tsunami [ 20] addressed the performance issue of\nFlood in case of correlated data and skewed workload. Note that Flood and Tsunami mainly focus\non in-memory indexes, and they do not support spatial queries like KNN queries. Therefore, we do\nnot compare Flood and Tsunami with the spatial indexes.\nMore recently, several methods were proposed to enhance traditional spatial indexes like R-tree\nwith machine learning techniques. RLR-Tree [ 27] and RW-tree [ 21] were proposed to improve\nR-tree construction through one-by-one insertion, and they learn to optimize the chooseSubtree and\nsplitNode process. \"AI+R\"-tree [ 10] was proposed to enhance the range query processing algorithm\nof R-tree for a given data and workload instance, by leveraging machine learning techniques.\nInstance-optimized Databases. The concept of Instance-optimized Databases [ 36] refers to\ndata systems that self-adjust to a given workload and data distribution to provide unprecedented\nperformance. A number of instance-optimized data components have been proposed, including\ninstance-optimized index [ 10,20,45], data layouts [ 18,54], query optimizers [ 42,43] and query\nscheduler [50].\n10 CONCLUSION\nThis work considers the problem of R-tree packing, such that the query performance is optimized\nfor a given data and workload instance. We propose PLATON , a top-down packing method that\nleverages Monte Carlo Tree Search to learn an optimal partition policy. We propose a divide and\nconquer strategy, and two optimization techniques to reduce the MCTS algorithm complexity and\nderive a linear-time algorithm. Extensive experiments on both synthetic and real-world datasets\nshow that PLATON outperforms both existing R-tree variants and learned/workload-aware indexes.\nACKNOWLEDGMENTS\nThis research is supported in part by MOE Tier-2 grant MOE-T2EP20221-0015.\nREFERENCES\n[1] [n. d.]. Decimal Degrees. https://en.wikipedia.org/wiki/Decimal_degrees.\n[2] [n. d.]. libspatialindex 1.9.3. https://libspatialindex.org/en/latest/index.html.\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\nPLATON: Top-down R-tree Packing with Learned Partition Policy 253:25\n[3] [n. d.]. LISA Implementation. https://github.com/pfl-cs/LISA.\n[4] [n. d.]. OpenStreetMap. https://www.openstreetmap.org.\n[5] [n. d.]. RSMI Implementation. https://github.com/Liuguanli/RSMI.\n[6][n. d.]. TIGER/Line Shapefiles. https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-\nfile.html.\n[7] [n. d.]. Waffle Implementation. https://gitlab.com/moinmoti/waffle.\n[8] 2023. PostGIS. https://postgis.net/.\n[9] 2023. PostgreSQL. https://www.postgresql.org/.\n[10] Abdullah-Al Abdullah-Al-Mamun, Ch. Md. Rakin Haider, Jianguo Wang, and Walid G. Aref. 2022. The â€œAI + Râ€ - tree:\nAn Instance-optimized R - tree. In 2022 23rd IEEE International Conference on Mobile Data Management (MDM) . 9â€“18.\nhttps://doi.org/10.1109/MDM55031.2022.00023\n[11] Daniar Achakeev, Bernhard Seeger, and Peter Widmayer. 2012. Sort-based query-adaptive loading of r-trees. In\nProceedings of the 21st ACM international conference on Information and knowledge management . 2080â€“2084.\n[12] Lars Arge, Mark De Berg, Herman Haverkort, and Ke Yi. 2008. The priority R-tree: A practically efficient and worst-case\noptimal R-tree. ACM Transactions on Algorithms (TALG) 4, 1 (2008), 1â€“30.\n[13] Norbert Beckmann, Hans-Peter Kriegel, Ralf Schneider, and Bernhard Seeger. 1990. The R*-tree: An efficient and\nrobust access method for points and rectangles. In Proceedings of the 1990 ACM SIGMOD international conference on\nManagement of data . 322â€“331.\n[14] Norbert Beckmann and Bernhard Seeger. 2009. A revised R*-tree in comparison with related index structures. In\nProceedings of the 2009 ACM SIGMOD International Conference on Management of data . 799â€“812.\n[15] Stefan Berchtold, Christian BÃ¶hm, and Hans-Peter Kriegel. 1998. Improving the query performance of high-dimensional\nindex structures by bulk load operations. In International Conference on Extending Database Technology . Springer,\n216â€“230.\n[16] Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen,\nStephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. 2012. A survey of monte carlo tree search\nmethods. IEEE Transactions on Computational Intelligence and AI in games 4, 1 (2012), 1â€“43.\n[17] David J DeWitt, Navin Kabra, Jun Luo, Jignesh M Patel, and Jie-Bing Yu. 1994. Client-server paradise. In VLDB , Vol. 94.\nCiteseer, 558â€“569.\n[18] Jialin Ding, Umar Farooq Minhas, Badrish Chandramouli, Chi Wang, Yinan Li, Ying Li, Donald Kossmann, Johannes\nGehrke, and Tim Kraska. 2021. Instance-optimized data layouts for cloud analytics workloads. In Proceedings of the\n2021 International Conference on Management of Data . 418â€“431.\n[19] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang, Badrish Chandramouli,\nJohannes Gehrke, Donald Kossmann, et al .2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data . 969â€“984.\n[20] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020. Tsunami: A learned multi-dimensional index\nfor correlated data and skewed workloads. arXiv preprint arXiv:2006.13282 (2020).\n[21] Haowen Dong, Chengliang Chai, Yuyu Luo, Jiabin Liu, Jianhua Feng, and Chaoqun Zhan. 2022. RW-Tree: A Learned\nWorkload-aware Framework for R-tree Construction. In 2022 IEEE 38th International Conference on Data Engineering .\nIEEE.\n[22] Ahmed Eldawy and Mohamed F Mokbel. 2015. Spatialhadoop: A mapreduce framework for spatial data. In 2015 IEEE\n31st international conference on Data Engineering . IEEE, 1352â€“1363.\n[23] R Elmasri, Shamkant B Navathe, R Elmasri, and SB Navathe. 2000. Fundamentals of Database Systems</Title.\n[24] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic compressed learned index with\nprovable worst-case bounds. Proceedings of the VLDB Endowment 13, 8 (2020), 1162â€“1175.\n[25] YvÃ¡n J GarcÃ­a R, Mario A LÃ³pez, and Scott T Leutenegger. 1998. A greedy algorithm for bulk loading R-trees. In\nProceedings of the 6th ACM international symposium on Advances in geographic information systems . 163â€“164.\n[26] Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, and Sheng Wang. 2021. A Reinforcement Learning Based\nR-Tree for Spatial Data Indexing in Dynamic Environments. arXiv preprint arXiv:2103.04541 (2021).\n[27] Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, and Sheng Wang. 2023. The RLR-Tree: A Reinforcement\nLearning Based R-Tree for Spatial Data. Proceedings of the ACM on Management of Data 1, 1 (2023), 1â€“26.\n[28] Oliver Gunther. 1993. Efficient computation of spatial joins. In Proceedings of IEEE 9th International Conference on Data\nEngineering . IEEE, 50â€“59.\n[29] Antonin Guttman. 1984. R-trees: A dynamic index structure for spatial searching. In Proceedings of the 1984 ACM\nSIGMOD international conference on Management of data . 47â€“57.\n[30] M Hammar, HJ Haverkort, et al .2002. Box-trees and R-trees with near-optimal query time. Discrete & Computational\nGeometry 28, 3 (2002), 291â€“312.\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.\n\n253:26 Jingyi Yang and Gao Cong\n[31] Herman Haverkort and Freek V Walderveen. 2008. Four-dimensional Hilbert curves for R-trees. Journal of Experimental\nAlgorithmics (JEA) 16 (2008), 3â€“1.\n[32] Edwin H Jacox and Hanan Samet. 2007. Spatial join techniques. ACM Transactions on Database Systems (TODS) 32, 1\n(2007), 7â€“es.\n[33] Ibrahim Kamel and Christos Faloutsos. 1993. On packing R-trees. In Proceedings of the second international conference\non Information and knowledge management . 490â€“499.\n[34] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and Thomas Neumann.\n2020. RadixSpline: a single-pass learned index. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management . 1â€“5.\n[35] Evgenios M Kornaropoulos, Silei Ren, and Roberto Tamassia. 2020. The Price of Tailoring the Index to Your Data:\nPoisoning Attacks on Learned Index Structures. arXiv preprint arXiv:2008.00297 (2020).\n[36] Tim Kraska. 2021. Towards instance-optimized data systems. Proceedings of the VLDB Endowment 14, 12 (2021).\n[37] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018. The case for learned index structures. In\nProceedings of the 2018 international conference on management of data . 489â€“504.\n[38] Taewon Lee and Sukho Lee. 2003. OMT: Overlap Minimizing Top-down Bulk Loading Algorithm for R-tree.. In CAISE\nShort paper proceedings , Vol. 74. 69â€“72.\n[39] Scott T Leutenegger, Mario A Lopez, and Jeffrey Edgington. 1997. STR: A simple and efficient algorithm for R-tree\npacking. In Proceedings 13th international conference on data engineering . IEEE, 497â€“506.\n[40] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A learned index structure for spatial data. In\nProceedings of the 2020 ACM SIGMOD international conference on management of data . 2119â€“2133.\n[41] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang. 2021. APEX: A High-Performance\nLearned Index on Persistent Memory. arXiv preprint arXiv:2105.00683 (2021).\n[42] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Alizadeh, and Tim Kraska. 2021. Bao: Making\nlearned query optimization practical. In Proceedings of the 2021 International Conference on Management of Data .\n1275â€“1288.\n[43] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh, Tim Kraska, Olga Papaemmanouil, and\nNesime Tatbul. 2019. Neo: A learned query optimizer. In VLDB .\n[44] Moin Hussain Moti, Panagiotis Simatis, and Dimitris Papadias. 2022. Waffle: A Workload-Aware and Query-Sensitive\nFramework for Disk-Based Spatial Indexing. Proceedings of the VLDB Endowment 16, 4 (2022), 670â€“683.\n[45] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learning multi-dimensional indexes. In\nProceedings of the 2020 ACM SIGMOD International Conference on Management of Data . 985â€“1000.\n[46] Varun Pandey, Alexander van Renen, Andreas Kipf, Ibrahim Sabek, Jialin Ding, and Alfons Kemper. 2020. The case\nfor learned spatial indexes. In Proceedings of the 2nd International Workshop on Applied AI for Database Systems and\nApplications (AIDBâ€™20) .\n[47] Jianzhong Qi, Guanli Liu, Christian S Jensen, and Lars Kulik. 2020. Effectively learning spatial indices. Proceedings of\nthe VLDB Endowment 13, 12 (2020), 2341â€“2354.\n[48] Jianzhong Qi, Yufei Tao, Yanchuan Chang, and Rui Zhang. 2018. Theoretically optimal and empirically efficient r-trees\nwith strong parallelizability. Proceedings of the VLDB Endowment 11, 5 (2018), 621â€“634.\n[49] Nick Roussopoulos and Daniel Leifker. 1985. Direct spatial search on pictorial databases using packed R-trees. In\nProceedings of the 1985 ACM SIGMOD international conference on Management of data . 17â€“31.\n[50] Ibrahim Sabek, Tenzin Samten Ukyab, and Tim Kraska. 2022. LSched: A Workload-Aware Learned Query Scheduler for\nAnalytical Database Systems. In Proceedings of the 2022 International Conference on Management of Data (Philadelphia,\nPA, USA) (SIGMOD â€™22) . Association for Computing Machinery, New York, NY, USA, 1228â€“1242. https://doi.org/10.\n1145/3514221.3526158\n[51] Timos K. Sellis, Nick Roussopoulos, and Christos Faloutsos. 1987. The R+-Tree: A Dynamic Index for Multi-Dimensional\nObjects. In Proceedings of the 13th International Conference on Very Large Data Bases (VLDB â€™87) . Morgan Kaufmann\nPublishers Inc., San Francisco, CA, USA, 507â€“518.\n[52] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction . MIT press.\n[53] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned index for spatial queries. In 2019 20th IEEE\nInternational Conference on Mobile Data Management (MDM) . IEEE, 569â€“574.\n[54] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li, Umar Farooq Minhas, Per-Ã…ke Larson,\nDonald Kossmann, and Rajeev Acharya. 2020. Qd-tree: Learning data layouts for big data analytics. In Proceedings of\nthe 2020 ACM SIGMOD International Conference on Management of Data . 193â€“208.\nReceived 16 April 2023; revised 20 July 2023; accepted 24 August 2023\nProc. ACM Manag. Data, Vol. 1, No. 4 (SIGMOD), Article 253. Publication date: December 2023.",
  "textLength": 91377
}