{
  "paperId": "91996caa93f4d322411470463bf6050bb2b90295",
  "title": "Compressibility Measures and Succinct Data Structures for Piecewise Linear Approximations",
  "pdfPath": "91996caa93f4d322411470463bf6050bb2b90295.pdf",
  "text": "Compressibility Measures and Succinct Data\nStructures for Piecewise Linear Approximations\nPaolo Ferragina/envel⌢pe\nDepartment L’EMbeDS, Sant’Anna School of Advanced Studies, Pisa, Italy\nDepartment of Computer Science, University of Pisa, Italy\nFilippo Lari1/envel⌢pe\nDepartment of Computer Science, University of Pisa, Italy\nAbstract\nWe study the problem of deriving compressibility measures forPiecewise Linear Approximations\n(PLAs), i.e., error-bounded approximations of a set of two-dimensionalincreasingdata points using\na sequence of segments. Such approximations are widely used tools in implementing manylearned\ndata structures, which mix learning models with traditional algorithmic design blocks to exploit\nregularities in the underlying data distribution, providing novel and effective space-time trade-offs.\nWe introduce the first lower bounds to the cost of storing PLAs in two settings, namely\ncompressionandindexing. We then compare these compressibility measures to known data structures,\nand show that they are asymptotically optimal up to a constant factor from the space lower bounds.\nFinally, we design the first data structures for the aforementioned settings that achieve the space\nlower bounds plus small additive terms, which turn out to besuccinctin most practical cases. Our\ndata structures support the efficient retrieval and evaluation of a segment in the (compressed) PLA\nfor a givenx-value, which is a core operation in any learned data structure relying on PLAs.\nAs a result, our paper offers the first theoretical analysis of the maximum compressibility achiev-\nable by PLA-based learned data structures, and provides novel storage schemes for PLAs offering\nstrong theoretical guarantees while also suggesting simple and efficient practical implementations.\n2012 ACM Subject ClassificationTheory of computation →Design and analysis of algorithms;\nTheory of computation→Data compression; Information systems→Information retrieval\nKeywords and phrasesPiecewise Linear Approximations, Succinct Data Structures, Lower Bounds\nDigital Object Identifier10.4230/LIPIcs..2016.23\nFundingThe work of P.F. was partially supported by the Alfred P. Sloan Foundation with the\ngrant #G-2025-25193 ( sloan.org ). The work of both authors has been supported by the Next-\nGenerationEU – National Recovery and Resilience Plan (Piano Nazionale di Ripresa e Resilienza,\nPNRR) – Project: “SoBigData.it - Strengthening the Italian RI for Social Mining and Big Data\nAnalytics” – Prot. IR0000013 – Avviso n. 3264 del 28/12/2021; and by the European Union – Horizon\n2020 Program under the scheme “INFRAIA-01-2018-2019 – Integrating Activities for Advanced\nCommunities”, Grant Agreement n. 871042, “SoBigData++: European Integrated Infrastructure\nfor Social Mining and Big Data Analytics” (http://www.sobigdata.eu).\n1 Introduction\nThe rapid growth of data has led to the development of novel techniques for designing data\nstructures that leverage regularities in the underlying data distribution to deliver faster or\nmore succinct solutions to proper query operations. This novel line of research, known as\nlearned data structures[ 14], integrates machine learning models with traditional algorithmic\ntechniques to achieve improved space occupancy and query time performance.\n1Corresponding author\n©Paolo Ferragina and Filippo Lari;\nlicensed under Creative Commons License CC-BY 4.0\n42nd Conference on Very Important Topics (CVIT 2016).\nEditors: John Q. Open and Joan R. Access; Article No.23; pp.23:1–23:19\nLeibniz International Proceedings in Informatics\nSchloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, GermanyarXiv:2509.07827v3  [cs.DS]  11 Sep 2025\n\n23:2 CompressibilityMeasuresandSuccinctDataStructuresforPiecewiseLinearApproximations\nA widely used tool in the field of learned data structures is the so-calledPiecewise\nLinear Approximation(PLA), an error-bounded approximation of a two-dimensional set\nof points using a sequence of segments [ 19]. The common use of PLAs is due to their\nability to capture linear trends in the input data, which frequently occur in many practical\napplications, while using little space and providing fast inference time. In this work, we focus\non PLAs arising in two settings: (1) thecompressionsetting, where the input sequence of\npoints is increasing in both coordinates but the x-values are also consecutive; and (2) the\nindexingsetting, where instead the y-values of the input sequence are consecutive. These\ntwo settings are general enough to cover a broad range of applications, including rankand\nselectdictionaries [ 7,2], time series compression [ 13], string indexing [ 23], one-dimensional\nindexing [ 14,12], multi-dimensional indexing [ 16,4], range minimum queries [ 8], monotone\nminimal perfect hashing [9], and range filtering [22], to mention a few.\nPrior work concerning the study of theoretical properties of PLAs has primarily focused\non providingupper boundson the optimal number of segments ℓas a function of the sequence\nlengthnand the maximum allowed error ε, that is the maximum vertical distance between the\ny-coordinate of any input point and itscoveringsegment. Such a relationship is fundamental\nfrom a theoretical perspective since it can explain the surprising performance of learned\ndata structures and also allow a theoretical comparison with their classical counterparts. In\nthis context, it is worth mentioning the pioneering study of Ferragina et al. [ 10] showing\nthat in the indexing case under the assumption that the gaps between consecutive keys are\nindependently and identically distributed random variables, with constant mean and variance,\nthe optimal number of segments ℓisΘ(n/ε2)with high probability. Later works [ 24,3]\nprovided better upper bounds, however, their PLAs were restricted to be formed byhorizontal\nsegments.\nAs far aslower boundsare concerned, we mention the work of Zeighami and Shahabi [ 25]\nwhich was the first to limit from below the model size (e.g., the number of segments ℓin a\nPLA or the number of neurons in a neural network) of any learned data structure achieving\na given desired accuracy εfor a wide range of problems arising in database systems such as\none- and multi-dimensional indexing, cardinality estimation, and range-sum estimation.\nIn our work, we assume that the segments havearbitrary slopesand the number of\nsegmentsℓforming an optimal PLA is known (e.g., computed by [ 19]), and thus provide\nthe first information-theoretic lower bound to the storage complexity of that PLA in both\nthe compression and the indexing settings (see Theorems 6 and 8). We then compare two\nrepresentative data structures against these lower bounds, namely the LA-vector by Boffa et\nal. [2] (for the compression setting) and the PGM-index by Ferragina & Vinciguerra [ 12] (for\nthe indexing setting), showing that both are optimal up to a constant factor. Finally, we\nconclude our work by designing two novel data structures for the aforementioned compression\nand indexing settings, that provide the same asymptotic query time as the LA-vector and\nthe PGM-index, but with small additive terms to the introduced lower bounds.\n2 Background\nWe assume the transdichotomous word RAM model where the word size wmatches the\nlogarithm of the problem size and arithmetic, logic, and bitwise operations, as well as access\ntow-bit memory cells, take O(1)time. To simplify, we denote with logxthe logarithm in\nbase2ofx, and we assume that logarithms hide their ceiling and thus return integers.\nGiven a static sorted sequence S= 1≤x 1< x 2<···< x n≤uofnelements drawn\nfrom a finite integer universe Uof sizeu, we consider the following fundamental operations:\n\nP. Ferragina and F. Lari 23:3\nrank (S,x), which for x∈Ureturns the number of S’s elements that are smaller or equal\ntox, i.e.|{x i∈S|xi≤x}|.\nselect (S,i), which for i∈ { 1,2,...,n}, returns the element in Sof position i, i.e.\nrank(S,x) =i. We assumeselect(S,0) = 0.\nIfSis represented with its characteristic bit vector BS, namely, a bit vector of length\nusuch thatBS[i] = 1ifi∈S, andBS[i] = 0otherwise, then supporting such operations\nis equivalent to support the rank 1(BS,x), which yields the number of1s before position x,\nandselect 1(BS,i), which gives the position of the i-th1inBS. These operations have been\nstudied extensively with many practical and theoretical results, and are at the core of any\nsuccinct or compressed data structure (see [ 17] for a comprehensive treatment). For our\npurposes, we briefly mention that they can be supported in O(1)time while using only o(u)\nadditional bits on top of the underlying bit vector.\nThe information-theoretic lower bound to store SisB(u,n) =log/parenleftbigu\nn/parenrightbig\n. Such value is\nrelated to the zero-order empirical entropy of its characteristic bit vector BS, defined as\nuH0(BS) =nlogu\nn+ (u−n )logu\nu−n. In fact, with simple arithmetic manipulations one\nobtainsB(u,n) =uH0(BS)−O(logu). The best upper bound to represent Swith the\nminimum redundancy with respect to B(u,n), and while still being able to perform rankand\nselectin optimal constant time, is given by the following data structure due to Pătraşcu [ 20]:\n▶Theorem1(Succincter).For any c>0, a monotone sequence Sofnpositive integers drawn\nfrom a universe of size ucan be stored using B(u,n) +O(u/logc(u/c)) +O(u3/4polylogu )\nbits and supportrankandselectinO(c)time.\nTheorem 1 implies constant time rankandselectoperations inB(u,n) +O(u/logcu)\nspace, which essentially matches the lower bound of Pătraşcu & Viola [ 21]. To further reduce\nthe space redundancy on top of B(u,n), one must allow for slower rankorselectoperations.\nA prominent example is given by the Elias-Fano encoding scheme [ 6,5], which attains the\nfollowing space-time trade-off for the aforementioned operations [17]:\n▶Theorem 2(Elias-Fano encoding).A monotone sequence Sofnpositive integers drawn\nfrom a universe of size ucan be stored using2 n+nlogu\nn+o(n)bits while supporting select\ninO(1)time and rank inO(min{logu\nn,logn})time.\nIndeed, the space usage of Theorem 2 can be expressed in terms ofB(u,n)as follows:\n2n+nlogu\nn+o(n) =uH 0(BS) +O(n) =B(u,n) +O(n+ logu)(1)\nThus obtaining a lower redundancy term with respect to Theorem 1 whenever Sis\nvery sparse, which, however, requires giving up the constant time rankoperation. The\nliterature offers other trade-offs (e.g., [ 18]) that could be adopted in our solution, leading to\ncorresponding time-space bounds. However, for the sake of presentation, in the remainder of\nthe paper we focus on the results provided in Theorems 1 and 2.\nA new line of research has recently started to investigate novel methods for designing\nalgorithms and data structures that leverage regularities in the underlying data to deliver\nfaster or more succinct solutions to a wide variety of problems. Most of these methods rely on\na technique known asPiecewise Linear Approximation(PLA), which involves approximating\na sequence of two-dimensional points using a set of segments.\n▶Definition 3.Given a set of ntwo-dimensional data points P⊆R2, a Piecewise Linear\nApproximation of Pwith maximum error ε≥1is the smallest set of non-overlapping segments\n{s1,s2,...,sℓ}that entirely covers P, ensuring that every point in Plies within a maximum\nvertical distanceεfrom its corresponding segment.\n\n23:4 CompressibilityMeasuresandSuccinctDataStructuresforPiecewiseLinearApproximations\nA PLA can always be computed optimally, hence producing the minimum number of\nsegmentsℓin time proportional to the sequence length using an algorithm by O’Rourke [ 19].\nHere, we focus on PLAs computed from monotone sequences of two-dimensional data\npoints, in which a point( xi,yi)is smaller than( xi+1,yi+1)because it has a smaller abscissa\nxi<xi+1and a smaller ordinate yi<yi+1. In particular, we consider two specific scenarios:\n(1) the compression setting, where PLAs are computed on sequences having consecutive\nx-values (i.e., xi+1=xi+ 1, typically x1= 1), and (2) the indexing setting, in which PLAs\nare computed on sequences having consecutive y-values (i.e. yi+1=yi+ 1, typically y1= 1).\nAs already mentioned in the introduction, despite the names, such scenarios are quite general\nand encompass a wide range of applications. To get a practical sense of the usage of PLAs\nin the design of learned data structures, we briefly outline two prominent examples.\nInthedesignoflearnedandcompressed rankandselectdictionaries[ 2], theinputsequence\nS= 1≤x1<x2<···<x n≤u, is first mapped to a Cartesian plane representation where\nthex-axis corresponds to the sequence of positions1 ,2,...,n, while the y-axis represents\nthe sequence values, namely xis. This transformation converts Sinto a set of monotonically\nincreasing two-dimensional points: {(i,xi)|xi∈S}. The key innovation of this approach is\nthattheCartesianplanerepresentationrevealsinherentregularitiesintheunderlyingsequence\nS. A PLA is then computed from such a sequence of points, and compression is achieved by\nstoring for each point( i,xi)the absolute difference between xiand the approximation of the\nsegment for position iusing just log (2ε+ 1) bits. Performing a select (S,i)operation then\nreduces to first finding the segment covering the abscissa value i, computing the approximate\nsequence value from such segment in i, and finally adding the correction term stored in\npositionito retrieve the exact value. A rank (S,x)operation can be naively solved by binary\nsearching on theselectvalues, even if faster alternatives do exist [2].\nIn the design of learned one-dimensional indexes [ 12], given the input sequence S=\n1≤x 1< x 2<···< x n≤u, one performs the same mapping above onto a Cartesian\nplane representation. In this case, the x-axis contains the sequence values, while the y-axis\ncorresponds to their respective positions1 ,2,...,n. Again, this transformation converts S\ninto a set of monotonically increasing two-dimensional data points: {(xi,i)|xi∈S}. The\ndata structure then consists of a PLA computed from such a set of points and a search for a\ngiven keyxproceeds in three steps. First, the segment covering xis identified through a\nbinary search on the first keys covered by each segment. Second, the approximate position i\nofxis obtained from such a segment. Third, since the underlying sequence is monotonically\nincreasing and the approximate position iofxis correct up to an error ε, it iscorrected\nvia a (binary) search in the interval[ i−ε,i +ε]. Such queries can be sped up by applying\nthe same construction recursively to the sequence of first keys covered by each segment\nuntil a single one remains (see e.g. [ 12,14]). This approach implements the index as a\nhierarchical structure, forming a tree of linear models. Consequently, searches involve a\ntop-down traversal of the tree, trading faster queries for an increased space usage.\n3 Related Work\nIn the following, we review existing methods for the lossless compression of PLAs, focusing\non thepredict (x)operation, which, given a value lying on the x-axis, identifies the segment\ncoveringxand computes the correspondingy-valuepredictedby that segment.\nFerragina & Vinciguerra introduced the PGM-index [ 12], the first solution in the indexing\ncase based on the optimal PLA computed by the O’Rourke algorithm [ 19], i.e., the one\nconsisting of the minimum number of segments ℓproviding an ε-approximation of the input\n\nP. Ferragina and F. Lari 23:5\npoints. In particular, they designed two compression methods for the segments. The first\none hinges on the observation that, in the indexing setting, the intercepts are monotonically\nincreasing. Denoting with βithe intercept of the i-th segment and with yiits first covered\ny-value, this easily follows since βi∈[yi−ε,yi+ε]andβi+1∈[yi+1−ε,yi+1+ε], but\nyi+ε≤yi+1−εas in this particular case a segment covers at least2εkeys [12, Lemma 2].\nExploiting this, they convert the monotone sequence of floating-point intercepts into integers\nand store them with the data structure of Okanohara & Sadakane [ 18]. But, they stored\nuncompressed the first covered keys and slopes of each segment. For the sake of comparison,\nwe provide the space usage for the PLA composing just the first layer of the PGM index.\nSince the universe of the underlying sequence is u, and each slope can be expressed as a\nrational number with a numerator and denominator respectively of lognandlogubits, they\nobtained the following result which we specialize via two approaches, one based on binary\nsearch (as proposed in [ 12]), and one using the data structure of Pătraşcu (see Theorem 1):\n▶Lemma 4(PGM-index [ 12]).The PLA composing the first layer of the PGM-index can be\nstored using:\nℓ(1.92 + logn2\nℓ+ 2logu +o(1))bits, while supporting the predictoperation in O(logℓ)\ntime via binary search.\nℓ(1.92 + logn2\nℓ+logu +o(1)) + log/parenleftbigu\nℓ/parenrightbig\n+O(u/logcu)bits for any constant c>0, while\nsupporting thepredictoperation inO(c)time.\nThe second compression method targets the segment’s slopes, and is based on the\nobservation that the O’Rourke algorithm does not compute a single segment but a whole\nfamily of segments whose slopes identify an interval of reals. The compressor then works\ngreedily (and optimally) by trying to assign the same slope to most segments of the PLA,\nthus reducing the number of slopes to be fully represented. This is very effective on some\nreal-world datasets, however, the authors did not provide an analysis of its space occupancy.\nMoving to the compression setting, Boffa et al. introduced the LA-vector [ 2], the\nfirst learned and compressed rankandselectdictionary based on the optimal PLA. Their\ncompression scheme for segments hinges on the assumption that the sequence of intercepts is\nmonotonically increasing, which is not necessarily the case in general. This is because, in the\ncompression setting, segments may cover fewer than2 εpoints, as the y-value can increase by\nmore than one between consecutive x-values. Ferragina & Lari [ 8] recently showed how to\nwaive that assumption with a negligible impact on space occupancy, obtaining the following\nbound (we drop the cost of cnbits for storing the correction vector Cin Theorem 3.2 of [ 2],\nand we add the solution based on the data structure of Pătraşcu):\n▶Lemma 5(LA vector, Theorem 2.3 in [ 2] and Theorem 5 in [ 8]).The PLA of the LA-vector\ncan be stored using\nℓ(2logu\nℓ+logn\nℓ+ 6 + 2 log (2ε+ 1) +o(1))bits, while supporting the predictoperation\ninO(logℓ)time via binary search.\nℓ(2logu\nℓ+ 4 + 2 log (2ε+ 1) +o(1)) + log/parenleftbign\nℓ/parenrightbig\n+O(n/logcn)bits for any constant c>0,\nwhile supporting thepredictoperation inO(c).\nLastly, Abrav and Medvedev proposed the PLA-index [ 1] in a Bioinformatics application\nwhere thex- andy-values of the underlying sequence are increasing but not necessarily\ncontiguous. They took an orthogonal approach to PLA compression by modifying the\nO’Rourke algorithm, enforcing each segment to start from the ending point of the previous\none. This allows them to derive a more compact encoding, which may produce a suboptimal\n\n23:6 CompressibilityMeasuresandSuccinctDataStructuresforPiecewiseLinearApproximations\nx1= 1x2−1x2 x3y2β2y′\n2γ2\ny2+ε\ny2−εy′\n2+ε\ny′\n2−ε\npositionsvalues\n(a)x1x2 x′\n2x3β2y2y3−1γ2\ny2+ε\ny2−εy3−1 +ε\ny3−1−ε\nvaluespositions\n(b)\nFigure 1The key parameters of PLAs arising in the two considered settings. Figure 1a shows an\nexample in the compression case. The values on the x-axis are consecutive, the first segment always\nbegins at position1, and each segment starts at an x-value immediately after the ending of the\nprevious one. Intercepts and the last y-values of each segment stay within a range of size2 ε+ 1from\nthe first and last covered values of the sequence. Notice how the intercepts do not necessarily form\nan increasing sequence. Figure 1b considers the indexing case, which is similar to the compression\nsetting except that the values on the y-axis are consecutive. Hence, the last covered y-value of a\nsegment is implicitly determined by the first covered y-value of the next one, and the intercepts βis\nform a monotone sequence as yi+ε≤y i+1−εsince a segment covers at least2 εkeys [12, Lemma 2].\nnumber of segments ℓ′≥ℓ. As their approach does not guarantee optimality and targets\ndifferent sequence types, we defer comparison to the journal version.\nWe conclude by noticing that none of these results focused on studying lower bounds to the\ncost of storing PLAs (possibly in compressed form). Therefore, the first novel contribution of\nour work lies in deriving the first compressibility measures for PLAs in the two aforementioned\nsettings. It will turn out that the storage schemes for PLAs of the LA-vector and the PGM-\nindex are asymptotically optimal up to a constant factor. Hence, our second contribution is\nto design the first data structures for the compression and indexing settings that achieve the\nspace lower bounds plus small additive terms, which aresuccinctin most practical cases.\n4 Compressibility Measures for PLAs\nWe begin by observing that, in general, the segments of a PLA, whether in the compression\nor indexing setting, have floating-point components. As discussed in Section 3, some existing\ncompression schemes assume that segments either start or end at an integer y-coordinate to\nexploit integer compression techniques. In our context, we work under the same assumption\nfor two reasons: first, when establishing a lower bound on the space required to represent a\nPLA, it is important to note that the class of PLAs with floating-point components is larger\nthan that of PLAs with integer components. Consequently, any lower bound proven for the\nlatter also applies to the former. Second, as proved by Ferragina & Lari [ 8] this reduction\nhas a negligible impact on the approximation error of the resulting PLA, and in fact, it only\nincreases that error by a constant3. Therefore, without loss of generality, we can safely\nrestrict our analysis to PLAs having segments with integer starting and ending ordinates.\nUnder this assumption, given the optimal number of segments ℓ, the approximation error\nε, the universe size u, and the length of the sequence n, we consider the problem of counting\n\nP. Ferragina and F. Lari 23:7\nthe number of possible PLAs that can be constructed using these parameters, in both the\ncompression and indexing cases. Then, by a simple information-theoretic argument, taking\nthe logarithm of such quantities gives a lower bound on the number of bits required to\nrepresent any PLA in the considered settings, and this provides a compressibility measure\nthat any compression scheme should strive to achieve. Starting from PLAs arising in the\ncompression setting, we prove the following theorem:\n▶Theorem 6.Let C(ℓ,ε,u,n, y)be the set of PLAs of ℓsegments having maximum error\nε, covering a monotone sequence of nelements drawn from a universe of size uin the\ncompression setting, and havingy=y 1≤y2≤···≤y ℓas the first coveredy-values. Then:\n|C(ℓ,ε,u,n,y)|=/parenleftbiggn−ℓ−1\nℓ−1/parenrightbigg/parenleftbiggu+ℓ−1\nℓ/parenrightbigg/parenleftiggℓ−1/productdisplay\ni=1(yi+1−yi+ 1)/parenrightigg\n(2ε+ 1)2ℓ\nProof.Let us denote with xithe first position covered by the i-th segment, and let us start\nby constraining each xiin such a way as to obtain a valid PLA with error ε. Since the first\nsegment always starts from the first position, it is x1= 1. Each segment covers at least two\nelements because there is always a segment connecting two points without incurring any\nerror (i.e., the one connecting them), hence xi+1−xi≥2(recall that in the compression\nsetting thex-values are strictly increasing and contiguous).\nToeaseourcounting, weintroducethefollowingchangeofvariables ¯xi=xi−(i−1). Inthis\nway, ¯x2=x2−1≥2and ¯xℓ=xℓ−(ℓ−1)≤n−ℓ, sincex2≥3andxℓ≤n− 1. Additionally,\n¯xi+1−¯xi≥1(since the ¯xis are distinct) and thus xi+1−xi=¯xi+1−¯xi+ 1≥2. Therefore,\nchoosing the first position of each segment while simultaneously satisfying the previous\nconstraint on the xis, is equivalent to choosing ℓ−1distinct values from {2,3,...,n−ℓ} .\nThe total number of ways to perform this choice is then/parenleftbign−ℓ−1\nℓ−1/parenrightbig\n.\nLetβiandyidenote, respectively, the intercept of the i-th segment and its first covered\ny-value (see Figure 1a). Choosing βiis equivalent to first selecting yiand then choosing a\nvalue within the interval[ yi−ε, yi+ε], which contains2 ε+ 1elements. In the compression\nsetting, the input sequence may contain repeated values, so consecutive segments can share\nthe same first covered y-value (i.e., yi+1=yi). Thus, choosing the first y-value of each of\ntheℓsegments amounts to selecting an increasing sequence of ℓelements from{1,2,...,u}.\nThe number of such choices is/parenleftbigu+ℓ−1\nℓ/parenrightbig\n. Since each βican then be any value of the interval\n[yi−ε, yi+ε], the total number of possible selections for the ℓintercepts is/parenleftbigu+ℓ−1\nℓ/parenrightbig\n(2ε+ 1)ℓ.\nLastly, let γibe they-value given by the i-th segment evaluated on its last covered\nposition (see Figure 1a). We perform the same reasoning as for the intercepts. Let y′\nibe\nthe value of the sequence on the last position covered by the i-th segment. Choosing γiis\nequivalent to first selecting y′\niand then choosing any of the values inside[ y′\ni−ε,y′\ni+ε]. Now,\nsince we are dealing with monotonically increasing sequences, to obtain a valid PLA, we need\nthaty′\ni≥yiandy′\ni≤yi+1for every1≤i<ℓ, notice that y′\nℓ=u. Consequently, each y′\nican\nbe independently chosen from the interval[ yi,yi+1], and each γiis then selected inside the\ninterval[y′\ni−ε,y′\ni+ε], therefore, the total number of ways to choose the ℓlasty-values of\neach segment is/parenleftig/producttextℓ−1\ni=1(yi+1−yi+ 1)/parenrightig\n(2ε+ 1)ℓ.\nCombining these quantities, our claim follows.◀\nTheorem 6 suggests the following lower bound on the number of bits that any lossless\ncompressor for PLAs in the compression setting must output.\n\n23:8 CompressibilityMeasuresandSuccinctDataStructuresforPiecewiseLinearApproximations\n▶Corollary 7.The minimum number of bits to represent any PLA fromC(ℓ,ε,u,n,y)is:\nBC(ℓ,ε,u,n,y) = log/parenleftbiggn−ℓ−1\nℓ−1/parenrightbigg\n+ log/parenleftbiggu+ℓ−1\nℓ/parenrightbigg\n+ℓ−1/summationdisplay\ni=1log (yi+1−yi+ 1) + 2ℓlog (2ε+ 1)\nNext, we focus on PLAs arising in the indexing setting. Due to space limitations, we\npresent only the main result here and defer the full proof to Appendix A.1. We only mention\nthat the proof follows the same strategy as that of Theorem 6, with the only difference that\nin the indexing setting, each segment (except possibly the last one) covers at least2 εkeys\n(see [12, Lemma 2]), thus reducing the possible choices for the first covered x- andy-values.\n▶Theorem 8.LetI(ℓ,ε,u,n, x)be the set of PLAs of ℓsegments having maximum error ε,\ncovering a monotone sequence of nelements drawn from a universe of size uin the indexing\nsetting, and havingx=x 1<x2<···<x ℓas the first coveredx-values. Then:\n|I(ℓ,ε,u,n,x)|=/parenleftbiggu−ℓ(2ε−1)\nℓ/parenrightbigg/parenleftbiggn−ℓ(2ε−1)−1\nℓ−1/parenrightbigg/parenleftiggℓ−1/productdisplay\ni=1(xi+1−xi−1)/parenrightigg\n(2ε+ 1)2ℓ\nTheorem 8 gives the following lower bound on the number of bits that any lossless\ncompressor has to produce when applied to PLAs in the indexing setting.\n▶Corollary 9.The minimum number of bits to represent any PLA fromI(ℓ,ε,u,n,x)is:\nBI(ℓ,ε,u,n,x) = log/parenleftbiggu−ℓ(2ε−1)\nℓ/parenrightbigg\n+ log/parenleftbiggn−ℓ(2ε−1)−1\nℓ−1/parenrightbigg\n+\n+ℓ−1/summationdisplay\ni=1log (xi+1−xi−1) + 2ℓlog (2ε+ 1)\nAs already mentioned, unlike in the compression setting, segments always cover at least\n2εkeys (except possibly the last one). Therefore, the lower bound of Corollary 9 can be\nmade independent from the choice of the first covered x-values, by noting that xi+1−xi≥2ε.\nNevertheless, this more general form of the lower bound comes at the cost of underestimating\nthe minimum number of bits required to store any PLA in the indexing case when segments\nspan more than2εelements.\nHaving established lower bounds on the space required to represent lossless any PLA in\nboththecompressionandindexingsettings, wenowshiftourfocustodesigningdatastructures\nthat use space close to BC(ℓ,ε,u,n, y)andBI(ℓ,ε,u,n, x)while efficiently supporting the\ncore operation of any learned data structure hinging on PLAs (e.g., [ 12,2,22,7,13,8]),\nnamely, returning they-valuepredictedby the segment covering a givenx-value.\n5 Succinct Data Structures for PLAs\nWe begin by establishing constraints on the parameters of a PLA that are commonly observed\nin many practical applications. Specifically, we assume that ϵ=O(1),ℓ=o(n)andn=o(u).\nNext, we proceed by formalizing the core operation performed on any PLA in both the\ncompression and indexing settings. In particular, we define the predict (x)operation, which,\ngiven a value lying on the x-axis, identifies the segment covering xand computes the\ncorresponding y-valuepredictedby that segment. Designing data structures that efficiently\n\nP. Ferragina and F. Lari 23:9\nsupport this operation while achieving space usage close to the lower bounds of Section 4\nis of both practical and theoretical interest since it can help to understand what the limits\nof PLA-based learned data structures are in terms of the redundancy needed to store and\nquery the PLA itself, and possibly improve the performances of such data structures.\nAt the end of Section 2, we commented on the relation between the current literature\nand our lower bounds, highlighting that the storage schemes for PLAs of the LA-vector\nand the PGM-index arecompact[ 17]. Due to space constraints, we sketch the main result\nhere; please refer to the appendix for the complete proofs. Starting with the LA-vector (see\nSection5.1), itturnsoutthatthestoragespaceofLemma5canbeexpressedas BC(ℓ,ε,u,n, y)\nwith an additive term that is either O(ℓlogu\nℓ)orO(ℓlogu\nℓ+n\nlogcn)depending if queries\nare answered with a binary search or with the data structure of Pătraşcu [ 20]. In the\nfirst case, the overhead is always O(BC(ℓ,ε,u,n, y)), while in the second case, that bound\nholds only when ℓ=ω(n/(logulogcn)). As a result, the storage space of the LA-vector\nis asymptotically optimal up to a constant factor, and thus is compact. Moving to the\nPGM-index (see Appendix A.2), the additive term on top of the lower bound BI(ℓ,ε,u,n, x)\nis eitherΘ( ℓlogu )orO(ℓlogu +u\nlogcu), depending on how queries are answered. In the\nfirst case, the overhead is alwaysΘ( BI(ℓ,ε,u,n, x)), while in the second case that bound\nholds only when ℓ= Ω(u/logc+1u). Therefore, the storage space of the PGM-index is also\ncompact.\nMotivated by this, the following sections will describe the design of the first data structures\nfor the compression and indexing settings, achieving small additive terms to the lower bounds\nof Section 4, which turn out to be succinct in most practical cases, and most importantly\nprovide the same asymptotic query time of the LA-vector and the PGM-index.\n5.1 Using space close toB C(ℓ, ε, u, n,y)\nGiven a PLA{s1,s2,...,sℓ}∈C (ℓ,ε,u,n, y), we represent a generic segment sias a tuple\nsi= (xi,βi,γi,yi,y′\ni)wherexiis the first covered x-value of the input sequence, βiis the\nintercept,γiis the lasty-value given by the segment, yiandy′\niare respectively the first and\nlasty-values of the input sequence covered by the i-th segment (see Figure 1a). To encode\neach segment, we begin by storing a bit sequence Xcontaining the first x-value covered\nby each segment. Specifically, Xencodes in unary the deltas between the first x-values of\nconsecutive segments (i.e., between xi+1andxi), shifted by a constant to further reduce\nspace usage. Such a sequence is defined as follows:\nX= 0x2−x1−210x3−x2−21...0xℓ−xℓ−1−21. The sequence is well-defined since xi+1−xi≥2.\nThe length of Xis/summationtextℓ−1\ni=1(xi+1−xi−1) =xℓ−x1−(ℓ−1), which is upper bounded by\nn−ℓ− 1sincex1= 1andxℓ≤n− 1, and contains ℓ−1ones. The i-th element of X\ncan be accessed by noticing thatx 1= 1and for anyi>1,x i=select 1(X,i−1) +i.\nNext, we store an integer sequence Yencoding the first covered y-values of each segment.\nGiven our assumption that ℓ=o(n), both the XandYsequences are sparse. Since they\nare also monotonically increasing by construction, we represent them using the Elias–Fano\nencoding scheme2. By Theorem 2, the space in bits required to storeXandYis:\n(ℓ−1) log/parenleftbiggn−ℓ−1\nℓ−1/parenrightbigg\n+ℓlog/parenleftigu\nℓ/parenrightig\n+ 4ℓ+o(ℓ)(2)\n2We turnXinto an integer sequence by interpreting each unary code as an integer.\n\n23:10 CompressibilityMeasuresandSuccinctDataStructuresforPiecewiseLinearApproximations\nNotingthat log/parenleftbigu+ℓ−1\nℓ/parenrightbig\n=log/parenleftbigu\nℓ/parenrightbig\n+O(ℓ)(since/parenleftbigu+ℓ−1\nℓ/parenrightbig\n=/parenleftbigu\nℓ/parenrightbig\n·/parenleftig/producttextℓ−1\ni=1(u+i)//producttextℓ−1\ni=1(u−i)/parenrightig\nand the second factor can be bounded by cℓ−1for some constant c>1) and applying the\nElias-Fano space bound from Equation 1, we can rewrite Equation 2 in the following form:\nlog/parenleftbiggn−ℓ−1\nℓ−1/parenrightbigg\n+ log/parenleftbiggu+ℓ−1\nℓ/parenrightbigg\n+O(ℓ+ logu)(3)\nNext we consider the sequence of last covered y-values of each segment, i.e. y′\n1,y′\n2,...,y′\nℓ−1,\nomittingy′\nℓas it is always equal to u. Since each y′\ni∈[yi,yi+1], we store the relative values\nusing just log (yi+1−yi+ 1)bits and concatenate their representation in a single bit vector\nB, of size|B|=/summationtextℓ−1\ni=1log (yi+1−yi+ 1)bits. To access each y′\ni, we maintain an auxiliary\narrayP, where each piis the starting index of the binary representation of y′\niinsideB.\nNotice that this technique resembles thedense pointersmethod of Ferragina & Venturini [ 11].\nArrayPis both sparse and monotonically increasing, hence we store it with the Elias-Fano\nencoding scheme using the following number of bits (recall Theorem 2):\n2(ℓ−1) + (ℓ−1) log/parenleftigg/summationtextℓ−1\ni=1log (yi+1−yi+ 1)\nℓ−1/parenrightigg\n+o(ℓ)(4)\nUsing Jensen’s inequality, and noticing that/summationtextℓ−1\ni=1(yi+1−yi+ 1) =yℓ+ℓ−1which is at\nmostu+ℓ−1sinceyℓ≤u, we obtain that|P|=O(ℓlog logu\nℓ)bits. Therefore, the sequence\nof the last covered y-values is encoded as the bit vector Band the Elias-Fano encoding of\nthe arrayP, using the following number of bits:\n/parenleftiggℓ−1/summationdisplay\ni=1log (yi+1−yi+ 1)/parenrightigg\n+O(ℓlog logu\nℓ)(5)\nLastly, since each segment is an ε-approximation for the points it covers, it holds that\n|βi−yi|≤2εand|γi−y′\ni|≤2ε. Therefore we store each intercept βiand each last y-value\nγias deltas with respect to yiandy′\niinside two arrays∆ βand∆γusing2ℓlog (2ε+ 1) bits.\nCombining this with Equations 3 and 5, and recalling Corollary 7, the space usage of our\nstorage scheme is BC(ℓ,ε,u,n, y) +O(ℓlog logu\nℓ+logu)bits. That is succinct space, since\nO(ℓlog logu\nℓ+logu)⊆o(BC(ℓ,ε,u,n, y))becauseBC(ℓ,ε,u,n, y)includes a term that is\nΘ(ℓlogu\nℓ), and we consider the case thatℓ= Ω(logu/log logu).\nWithout introducing any extra space on top of this representation, performing a predict (x)\nquery proceeds as detailed in Algorithm 1. For simplicity, we assume that the segment\ncoveringxis never the first one nor the last one. Corner cases can be handled in constant\ntime without altering the query time.\nBecause of Theorem 2, all the selectoperations on the Elias-Fano encoded sequences\nX,Y, andPtakeO(1)time. Reading the binary representation of numbers from B, as\nwell as accessing the deltas inside∆ βand∆γ, requires a O(1)number of bitwise and\nbit-shifting operations (see [ 17, Chapter 3] for the technical details), hence O(1)time in\nthe transdichotomous word RAM model we are assuming. The most expensive part of\nAlgorithm 1 is then the predecessor search on X, which dominates the cost of a predict\nquery, making the overall timeO(logℓ).\nTo lower the cost of the predecessor search, and thus achieve faster query times, one\nmust allow for a larger additive term to the lower bound BC(ℓ,ε,u,n, y). To address this, we\n\nP. Ferragina and F. Lari 23:11\nAlgorithm 1Performs a predictoperation on our compressed storage scheme for PLAs.\nInput :The arrays of first coveredx-values andy-valuesXandY, the bit-vector\nB, the array of offsetsP, the array of deltas∆ βand∆γ, and a valuex.\nOutput:A valueysuch thatpredict(x) =y.\ni=pred(X, x);// predecessor search onXusing a binary search\nxi=select 1(X,i−1) +i;\nxi+1=select 1(X,i) + (i+ 1);\nyi=select 1(Y, i);\nsi=select 1(P, i);\nei=select 1(P, i+ 1)−1;\ny′\ni=read(B, s i, ei);// read the bit sequenceB[s i,ei]representingy′\ni\nβi=yi+ ∆β[i];\nγi=y′\ni+ ∆γ[i];\nreturn/floorleftig\n(x−xi)(γi−βi)\n(xi+1−xi)/floorrightig\n+βi;\ndirectly store the sequence of first covered x-values asX=x2,x3,...,xℓ, whose universe is\nbounded byn−1and containsℓ−1values (recallx 1= 1in the compression setting).\nThis sequence allows the predecessor of any given position (and, consequently, the index\nof the segment covering that position) to be computed via a simple rankoperation. Xis\nthen stored using Theorem 1, which guarantees that for any constant c>0, bothrankand\nselectoperations takeO(c)time while requiringlog/parenleftbign−1\nℓ−1/parenrightbig\n+O/parenleftig\nn\nlogcn/parenrightig\nbits of space.\nSince we assumed thatℓ=o(n), we refactor the termlog/parenleftbign−1\nℓ−1/parenrightbig\nas follows:\nlog/parenleftbiggn−1\nℓ−1/parenrightbigg\n= log/parenleftbiggn−ℓ−1\nℓ−1/parenrightbigg\n+ log/parenleftigg/producttextℓ\ni=1(n−i)\n/producttext2ℓ−1\ni=ℓ(n−i)/parenrightigg\n≤log/parenleftbiggn−ℓ−1\nℓ−1/parenrightbigg\n+ℓlog/parenleftbiggn−1\nn−2ℓ+ 1/parenrightbigg\n= log/parenleftbiggn−ℓ−1\nℓ−1/parenrightbigg\n+O(ℓ)\nWhere the inequality is obtained by upper-bounding each term in the numerator by n−1\nand lower-bounding each term in the denominator byn−2ℓ+ 1. The space usage is then:\nBC(ℓ,ε,u,n,y) +O(ℓlog logu\nℓ+n\nlogcn)(6)\nWe now show that the space usage of this second data structure is succinct. To this\nend, we use the well-known upper and lower bounds on the logarithm of the binomial\n(i.e.,klogm\nk≤log/parenleftbigm\nk/parenrightbig\n≤klog (em\nk), wheree≈ 2.718is the Euler’s number, and thus\nlog/parenleftbigm\nk/parenrightbig\n= Θ(klogm\nk)), to rewrite the lower bound of Corollary 7 as follows:\nBC(ℓ,ε,u,n,y) = Θ(ℓ(logu\nℓ+ logn\nℓ)) = Θ(ℓlogu\nℓ)(7)\nWhere theΩ-term comes from the first two additive terms in the formula of BC(ℓ,ε,u,n, y)\nand the approximation of the logarithm of the binomial coefficient written above. The O()-\nterm comes similarly and from upper bounding the summation in Corollary 7 using Jensen’s\ninequality, as already done in Section 5.1.\n\n23:12 CompressibilityMeasuresandSuccinctDataStructuresforPiecewiseLinearApproximations\nFocusing on the additive term of Equation 6, it is easy to see that O(ℓlog logu\nℓ+n\nlogcn)⊆\no(ℓlogu\nℓ+n\nlogcn), byconsidering ℓ=ω(n/(logulogcn))whichimpliesthat ℓlogu\nℓ=ω(n\nlogcn).\nTherefore, from Equation 7, it is O(ℓlog logu\nℓ+n\nlogcn)⊆o(BC(ℓ,ε,u,n, y))for those values\nofℓthat areω(n/(logulogcn))and do not violate our initial assumption thatℓ=o(n).\nAs a result, we proved the following theorem, which gives data structures able to store\nany PLA in the compression setting using space close to the lower bound of Corollary 7 while\nsupporting efficientpredictqueries.\n▶Theorem 10.There exist data structures storing any PLA fromC(ℓ,ε,u,n)as follows:\nUsingBC(ℓ,ε,u,n, y) +O(ℓlog logu\nℓ+logu)bits of space, while supporting predictin\nO(logℓ)time.\nUsingBC(ℓ,ε,u,n, y)+O(ℓlog logu\nℓ+n/logcn)bits of space for any constant c>0, while\nsupportingpredictinO(c)time.\nWe conclude by comparing Theorem 10 with the storage scheme for PLAs of the LA-vector\n(see Lemma 5). We start by considering the one solving the predictqueries with a binary\nsearch on the first covered positions, reporting its space usage in bits here to facilitate\ncomparison:\nℓ(2 logu\nℓ+ logn\nℓ+ 6 + 2 log (2ε+ 1) +o(1))(8)\nWe notice that the term2 ℓlog (2ε+ 1) is shared by both Corollary 7 and Equation 8,\nhence we only focus on the remaining parts. Since we assumed that ℓ=o(n), the terms ℓlogu\nℓ\nandℓlogn\nℓappearing in Equation 8 can be respectively bounded as log/parenleftbigu+ℓ−1\nℓ/parenrightbig\n+O(ℓ+logu)\nandlog/parenleftbign−ℓ−1\nℓ−1/parenrightbig\n+O(ℓ+logn)(see Section 2 and 5.1). Consequently, Equation 8 can be\nrewritten in the following form, which is closer to the one of Corollary 7.\nlog/parenleftbiggn−ℓ−1\nℓ−1/parenrightbigg\n+ log/parenleftbiggu+ℓ−1\nℓ/parenrightbigg\n+ℓlogu\nℓ+ 2ℓlog (2ε+ 1) +O(ℓ+ logu)(9)\nLastly, the summation/summationtextℓ−1\ni=1log (yi+1−yi+ 1)appears in the lower bound of Corollary 7,\nbut not in Equation 9 which in turns contain also the additive termsℓlogu\nℓ+O(ℓ+ logu).\nNow, the minimum of the summation is ℓ−1, and this occurs when each segment covers\nexactly two positions. We rule out the case in which it is zero, as this would force yi+1=yi,\nthus only considering PLAs formed by a single segment. In this way, the gap between the\nlower bound and the storage space of the LA-vector is thusΘ( ℓlogu\nℓ) = Θ(BC(ℓ,ε,u,n, y)).\nHence, the storage scheme for PLAs of the LA-vector with the correction of Ferragina & Lari\niscompact, because it is up to a constant factor from the optimal. Therefore, our first data\nstructure in Theorem 10 offers an even smaller additive term to the optimal space usage,\nwhile having the same asymptotic query time. Specifically, assuming ℓ= Ω( logu/log logu ),\nit exponentially reduces the overhead, fromO(ℓlogu\nℓ)toO(ℓlog logu\nℓ).\nSimilarly, an analogous result holds for the two data structures described in Lemma 5\nand Theorem 10, which answer queries in O(c)time for any constant c>0. In this case, it\nis sufficient to notice that term due to the data structure of Pătraşcu is shared by both, and\nthe additive term of the LA-vector becomes O(ℓlogu\nℓ+n\nlogcn). Therefore, using the same\nargument we used for proving the succinctness of the second data structure in Theorem 10,\nit is possible to show that for ℓ=ω(n/logulogcn)the storage scheme of the LA-vector is\nagain compact, while ours is succinct, and thus offering a lower overhead to the optimal\nspace usage without altering its asymptotic query time.\n\nP. Ferragina and F. Lari 23:13\nIn conclusion, we remark that, beyond its theoretical appeal, the first data structure of\nTheorem10isalsopractical, asitleveragestechniqueswithhighlyengineeredimplementations,\nsuch as the Elias-Fano encoding of monotone sequences [ 15] and arrays of fixed- and variable-\nsize elements [ 17]. Indeed, under the same assumption that ℓ= Ω( logu/log logu ), our\nstorage scheme asymptotically requires just O(log logu\nℓ)bits per segment over the theoretical\nminimum, while still providing practically fast predictqueries. Conversely, the second data\nstructure is mostly theoretical, and it aims at showing how close we can approach the lower\nbound of Corollary 7 while still supporting (optimal) constant timepredictqueries.\n5.2 Using space close toB I(ℓ, ε, u, n,x)\nThe compression and indexing settings are closely related, the only difference lies in which\naxis is constrained to have consecutive values. Indeed, this similarity can also be observed in\nthe two lower bounds of Corollary 7 and 9 sharing the same form. As a result, the design\nof a data structure that achieves space usage close to the lower bound for the indexing\nsetting, while supporting efficient predictqueries, builds upon the same ideas and algorithmic\ntechniques introduced in Section 5.1. Due to space constraints, we present only the main\nresult in this section, deferring the complete proof to Appendix A.3.\n▶Theorem 11.There exist data structures storing any PLA fromI(ℓ,ε,u,n)as follows:\nUsingBI(ℓ,ε,u,n, x) +O(ℓlog logu\nℓ+logu)bits of space, while supporting the predict\noperation inO(logℓ)time.\nUsingBI(ℓ,ε,u,n, x)+O(ℓlog logu\nℓ+u/logcu)bits of space for any constant c>0, while\nsupporting thepredictoperation inO(c)time.\nThe storage scheme of the PGM-index is optimal up to a constant factor. Considering\nthe first data structure of Lemma 4, it can be expressed as BI(ℓ,ε,u,n, x) + Θ(ℓlogu ). As a\nresult, Theorem 11 introduces the first data structure storing any PLA in the indexing setting\nwithin lower-order terms to BI(ℓ,ε,u,n, x), under the assumption that ℓ= Ω( logu/log logu ),\nwith the same asymptotic query time (see Appendix A.2 for a detailed comparison).\nThe same observations of Section 5.1 regarding the practicality of our data structures\nhold. The first one is the most practical, while the second is just theoretical and shows how\nclose the lower bound can be approached while answering queries in (optimal) constant time.\n6 Conclusions\nIn this work, we addressed the problem of deriving the first compressibility measures for\nPLAs of monotone sequences in two distinct settings:compressionandindexing, which are\ngeneral enough to cover a broad range of applications. The measures we obtained provide a\ntheoretical foundation for understanding the space requirements of any learned data structure\nrelying on PLAs in any of its components.\nBased on our compressibility measures, we analyzed two representative data structures\nfor storing PLAs, namely the one of the LA-vector (compression setting) and the one of the\nPGM-index (indexing setting), showing that they are both optimal up to a constant factor.\nMotivated by this study, we proposed two novel data structures for the above settings,\nwhich turn out to be succinct (thus optimal up to lower-order terms) in most practical cases\nwhile retaining the same asymptotic query time of the LA-vector and PGM-index. Overall,\nthey offer strong theoretical guarantees and suggest efficient practical implementations,\nproviding valuable tools for practitioners and researchers in learned data structures.\n\n23:14 CompressibilityMeasuresandSuccinctDataStructuresforPiecewiseLinearApproximations\nFuture works could investigate an extension of our analysis toPiecewise Non-Linear\nApproximations, for which successful applications already exist [ 13]. Another interesting\ndirection, complementary to our work, is the implementation of the proposed data structures\nwith a thorough experimental evaluation of their space-time trade-offs.\nReferences\n1Md. Hasin Abrar and Paul Medvedev. Pla-index: A k-mer index exploiting rank curve linearity.\nIn24th International Workshop on Algorithms in Bioinformatics, WABI 2024, September\n2-4, 2024, Royal Holloway, London, United Kingdom, volume 312 ofLIPIcs, pages 13:1–13:18.\nSchloss Dagstuhl - Leibniz-Zentrum für Informatik, 2024. doi:10.4230/LIPICS.WABI.2024.13 .\n2Antonio Boffa, Paolo Ferragina, and Giorgio Vinciguerra. A learned approach to design\ncompressed rank/select data structures.ACM Trans. Algorithms, 18(3):24:1–24:28, 2022.\ndoi:10.1145/3524060.\n3Luis Alberto Croquevielle, Guang Yang, Liang Liang, Ali Hadian, and Thomas Heinis. Beyond\nlogarithmic bounds: Querying in constant expected time with learned indexes. In28th\nInternational Conference on Database Theory, ICDT 2025, March 25-28, 2025, Barcelona,\nSpain, volume 328 ofLIPIcs, pages 19:1–19:21. Schloss Dagstuhl - Leibniz-Zentrum für\nInformatik, 2025.doi:10.4230/LIPICS.ICDT.2025.19.\n4Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. Tsunami: A learned\nmulti-dimensional index for correlated data and skewed workloads.Proc. VLDB Endow.,\n14(2):74–86, 2020.doi:10.14778/3425879.3425880.\n5Peter Elias. Efficient storage and retrieval by content and address of static files.J. ACM,\n21(2):246–260, 1974.doi:10.1145/321812.321820.\n6Robert M. Fano. On the number of bits required to implement an associative memory.\nTechnical report, Massachusetts Institute of Technology, Project MAC, Computer Structures\nGroup, 1971.\n7Paolo Ferragina, Marco Frasca, Giosuè Cataldo Marinò, and Giorgio Vinciguerra. On nonlinear\nlearned string indexing.IEEE Access, 11:74021–74034, 2023. doi:10.1109/ACCESS.2023.\n3295434.\n8Paolo Ferragina and Filippo Lari. FL-RMQ: A learned approach to range minimum queries. In\n36th Annual Symposium on Combinatorial Pattern Matching, CPM 2025, June 17-19, 2025,\nMilan, Italy, volume 331 ofLIPIcs, pages 7:1–7:23. Schloss Dagstuhl - Leibniz-Zentrum für\nInformatik, 2025.doi:10.4230/LIPICS.CPM.2025.7.\n9Paolo Ferragina, Hans-Peter Lehmann, Peter Sanders, and Giorgio Vinciguerra. Learned\nmonotone minimal perfect hashing. In31st Annual European Symposium on Algorithms,\nESA 2023, September 4-6, 2023, Amsterdam, The Netherlands, volume 274 ofLIPIcs, pages\n46:1–46:17. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2023. doi:10.4230/LIPICS.\nESA.2023.46.\n10Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. On the performance of learned data\nstructures.Theor. Comput. Sci., 871:107–120, 2021.doi:10.1016/J.TCS.2021.04.015.\n11Paolo Ferragina and Rossano Venturini. A simple storage scheme for strings achieving entropy\nbounds.Theor. Comput. Sci., 372(1):115–121, 2007.doi:10.1016/J.TCS.2006.12.012.\n12Paolo Ferragina and Giorgio Vinciguerra. The PGM-index: a fully-dynamic compressed\nlearned index with provable worst-case bounds.Proc. VLDB Endow., 13(8):1162–1175, 2020.\ndoi:10.14778/3389133.3389135.\n13Andrea Guerra, Giorgio Vinciguerra, Antonio Boffa, and Paolo Ferragina. Learned compression\nof nonlinear time series with random access. InProc. 41st IEEE International Conference on\nData Engineering (ICDE), 2025.\n14Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. InProceedings of the 2018 International Conference on Management of Data,\n\nP. Ferragina and F. Lari 23:15\nSIGMOD Conference 2018, Houston, TX, USA, June 10-15, 2018, pages 489–504. ACM, 2018.\ndoi:10.1145/3183713.3196909.\n15Danyang Ma, Simon J. Puglisi, Rajeev Raman, and Bella Zhukova. On elias-fano for rank\nqueries in fm-indexes. In31st Data Compression Conference, DCC 2021, Snowbird, UT, USA,\nMarch 23-26, 2021, pages 223–232. IEEE, 2021.doi:10.1109/DCC50243.2021.00030.\n16Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. Learning multi-\ndimensional indexes. InProceedings of the 2020 International Conference on Management of\nData, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020,\npages 985–1000. ACM, 2020.doi:10.1145/3318464.3380579.\n17Gonzalo Navarro.Compact Data Structures - A Practical Approach. Cambridge University\nPress, 2016.\n18Daisuke Okanohara and Kunihiko Sadakane. Practical entropy-compressed rank/select\ndictionary. InProceedings of the Nine Workshop on Algorithm Engineering and Exper-\niments, ALENEX 2007, New Orleans, Louisiana, USA, January 6, 2007. SIAM, 2007.\ndoi:10.1137/1.9781611972870.6.\n19Joseph O’Rourke. An on-line algorithm for fitting straight lines between data ranges.Commun.\nACM, 24(9):574–578, 1981.doi:10.1145/358746.358758.\n20Mihai Pătraşcu. Succincter. In49th Annual IEEE Symposium on Foundations of Computer\nScience, FOCS 2008, October 25-28, 2008, Philadelphia, PA, USA, pages 305–313. IEEE\nComputer Society, 2008.doi:10.1109/FOCS.2008.83.\n21Mihai Pătraşcu and Emanuele Viola. Cell-probe lower bounds for succinct partial sums.\nInProceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms,\nSODA 2010, Austin, Texas, USA, January 17-19, 2010, pages 117–122. SIAM, 2010. doi:\n10.1137/1.9781611973075.11.\n22Kapil Vaidya, Tim Kraska, Subarna Chatterjee, Eric R. Knorr, Michael Mitzenmacher, and\nStratos Idreos. SNARF: A learning-enhanced range filter.Proc. VLDB Endow., 15(8):1632–\n1644, 2022.doi:10.14778/3529337.3529347.\n23Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. Sindex: a scalable learned\nindex for string keys. In Taesoo Kim and Patrick P. C. Lee, editors,APSys ’20: 11th ACM\nSIGOPS Asia-Pacific Workshop on Systems, Tsukuba, Japan, August 24-25, 2020, pages 17–24.\nACM, 2020.doi:10.1145/3409963.3410496.\n24Sepanta Zeighami and Cyrus Shahabi. On distribution dependent sub-logarithmic query time\nof learned indexing. InInternational Conference on Machine Learning, ICML 2023, 23-29\nJuly 2023, Honolulu, Hawaii, USA, volume 202 ofProceedings of Machine Learning Research,\npages 40669–40680, 2023. URL:https://proceedings.mlr.press/v202/zeighami23a.html.\n25Sepanta Zeighami and Cyrus Shahabi. Towards establishing guaranteed error for learned\ndatabase operations. InThe Twelfth International Conference on Learning Representations,\nICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL: https://\nopenreview.net/forum?id=6tqgL8VluV.\n\n23:16 CompressibilityMeasuresandSuccinctDataStructuresforPiecewiseLinearApproximations\nA Detailed Proofs\nA.1 Complete Proof of Theorem 8\nWe restate Theorem 8 here to facilitate the presentation.\n▶Theorem 12.Let I(ℓ,ε,u,n, x)be the set of PLAs of ℓsegments having maximum error ε,\ncovering a monotone sequence of nelements drawn from a universe of size uin the indexing\nsetting, and havingx=x 1<x2<···<x ℓas the first coveredx-values. Then:\n|I(ℓ,ε,u,n,x)|=/parenleftbiggu−ℓ(2ε−1)\nℓ/parenrightbigg/parenleftbiggn−ℓ(2ε−1)−1\nℓ−1/parenrightbigg/parenleftiggℓ−1/productdisplay\ni=1(xi+1−xi−1)/parenrightigg\n(2ε+ 1)2ℓ\nProof.Let us denote with xithe firstx-value covered by the i-th segment. We start by\nnoticing that a segment always covers at least2 εpoints (elements) in the indexing setting.\nThis is because, in the indexing setting, points have monotonically increasing contiguous\ny-values and so the horizontal segment y=i+εis always an ε-approximation of any\nsubsequence of2 εconsecutive elements, see [ 12, Lemma 2]. Hence, it is xi+1−xi≥2εand\nxℓ≤u−2ε+ 1.\nWe perform a similar change of variables as in the proof of Theorem 6 by defining\n¯xi=xi−(2ε−1)(i−1). This guarantees that xi+1−xi=¯xi+1−¯xi+ 2ε−1≥2εbecause\n¯xi+1−¯xi≥1. Moreover, ¯x1≥1and ¯xℓ=xℓ−(2ε−1)(ℓ−1)≤u−ℓ (2ε−1), since\nxℓ≤u− 2ε+ 1. Therefore, choosing the ℓstarting keys of each segment correspond to\nselectingℓdistinct elements from {1,2,...,u−ℓ (2ε−1)}. Hence, the total number of ways\nto perform this selection is/parenleftbigu−ℓ(2ε−1)\nℓ/parenrightbig\n.\nNext, since the x-values are not consecutive in the indexing case, we are free to choose\nthe last covered x-value of each segment, which we denote by x′\ni(see Figure 1b). To ensure\na valid PLA, we require that x′\ni≥xi+ 1andx′\ni≤xi+1−1for every1≤i<ℓ, withx′\nℓ=u.\nSince each x′\nican be independently chosen from the interval[ xi+ 1,xi+1−1], the total\nnumber of possible choices is/producttextℓ−1\ni=1(xi+1−xi−1). Notice that this quantity is always larger\nthan zero becausex i+1−xi≥2εand we assumedε≥1.\nMoving to the y-axis (which represents the positions of the keys), let yibe the position\nof the first key covered by the i-th segment and βiits intercept (see Figure 1b). Choosing βi\ncan be reduced to first selecting yi, and then choosing any value in the interval[ yi−ε,yi+ε]\nwhose size is2 ε+ 1. As already mentioned, in the indexing case, each segment covers at least\n2εkeys, hence yi+1−yi≥2εandyℓ≤n− 2ε+ 1. Noticing that y1= 1and performing the\nchange of variables ¯yi=yi−(2ε−1)(i−1), we obtain that ¯y2≥2and ¯yℓ≤n−ℓ (2ε−1)\nsincey2≥2ε+ 1andyℓ≤n− 2ε+ 1. It follows that choosing the ℓintercepts is equivalent\nto first choosing ℓ−1distinct elements in {2,3,...,n−ℓ (2ε−1)}and, then, for each of\nthem, selecting a value inside an interval of size2 ε+ 1. Therefore, the total number of ways\nto perform this choice is/parenleftbign−ℓ(2ε−1)−1\nℓ−1/parenrightbig\n(2ε+ 1)ℓ.\nTo conclude the proof, we need to consider for every i-th segment its slope, and thus\nthey-value on its last covered x-value. Let us denote this value by γi. Since we are in the\nindexing setting, we know that the y-values of the sequence are monotonically increasing\nand contiguous, so the last covered y-value isyi+1−1, see Figure 1b, and thus γican\nassume any value within the interval[ yi+1−1−ε,yi+1−1 +ε]to make that segment a valid\nϵ-approximation. Therefore, we introduce a(2 ε+ 1)ℓfactor to the previous quantity, and\nthus our claim follows.◀\nWe conclude by noting that, unlike in the compression setting, in the indexing case each\nsegment covers at least2 εelements (except possibly the last one). Consequently, a lower\n\nP. Ferragina and F. Lari 23:17\nbound to the counting of Theorem 12 can be obtained by replacing xi+1−xiwith2ε. This\nyields a formula that is independent of the sequence of first coveredx-values.\n▶Corollary 13.Let I(ℓ,ε,u,n )be the set of PLAs of ℓsegments having maximum error ε,\ncovering a monotone sequence of nelements drawn from a universe of size uin the indexing\nsetting, then:\n|I(ℓ,ε,u,n)|=/parenleftbiggu−ℓ(2ε−1)\nℓ/parenrightbigg/parenleftbiggn−ℓ(2ε−1)−1\nℓ−1/parenrightbigg\n(2ε−1)ℓ−1(2ε+ 1)2ℓ\nClearly, for any choice ofx, we have |I(ℓ,ε,u,n )|≤|I (ℓ,ε,u,n, x)|. Therefore, a lower\nbound based on Corollary 13 is more general, as it depends only on the sequence length n,\nthe universe size u, the maximum error ε, and the number of segments ℓ. However, this\ncomes at the cost of underestimating the minimum number of bits required to store any PLA\nin the indexing setting when segments cover more than2εelements.\nA.2 PGM-index Space Usage in Terms ofB I(ℓ, ε, u, n,x)\nAs in Section 5.1, we start by considering the PGM-index storage scheme for PLAs of\nLemma 4 solving predictqueries with a binary search on the first covered keys, reporting its\nspace usage in bits here to facilitate comparison:\nℓ(1.92 + logn\nℓ+ logn+ 2 logu+o(1))(10)\nSince we assumed ℓ=o(n)andε=O(1), the terms ℓlogn\nℓandℓloguin Equation 10 can\nbe respectively bounded as log/parenleftbign−ℓ(2ε−1)−1\nℓ−1/parenrightbig\n+O(ℓ+logn)and log/parenleftbigu−ℓ(2ε−1)\nℓ/parenrightbig\n+O(ℓlogℓ +\nlogu)(see Section 2 and Appendix A.3). Noticing that, since εis a positive constant,1 .92ℓ\nis equal to2 ℓlog (2ε+ 1)− Θ(ℓ), moreover there are some sparing terms of ℓlogu(because\nof the factor2) and ℓlognin Equation 10. Hence, Equation 10 can be rewritten in the\nfollowing form that reminds the lower bound of Corollary 9.\nlog/parenleftbiggn−ℓ(2ε−1)−1\nℓ−1/parenrightbigg\n+ log/parenleftbiggu−ℓ(2ε−1)\nℓ/parenrightbigg\n+ 2ℓlog (2ε+ 1) + Θ(ℓ(logu+ logn))(11)\nAgain, the summation/summationtextℓ−1\ni=1log (xi+1−xi−1)appearing in Corollary 9 but not in\nEquation 11 can be handled using the same argument as in Section 5.1. Therefore, the space\nusage of the storage scheme for PLAs of the PGM-index can be expressed as follows:\nBI(ℓ,ε,u,n,x) + Θ(ℓ(logu+ logn))\nNotice thatΘ( ℓ(logu +logn)) = Θ(ℓlogu )̸⊆o(BI(ℓ,ε,u,n, x)), because of the presence\ninBI(ℓ,ε,u,n, x)of terms such as ℓlogn\nℓandℓlogu\nℓ. Therefore, the storage scheme for PLAs\nof the PGM-index iscompact, but not succinct. Instead, our data structure of Theorem 11 is\nsuccinct, offering a significantly smaller overhead to the optimal space usage while providing\nthe same asymptotic query time.\nFinally, the same result can be achieved when considering the data structure of Lemma 4,\nanswering queries in O(c)time for any given constant c>0. Such a data structure is compact\nforℓ= Ω(u/logc+1n), while ours is succinct and provides the same query time.\n\n23:18 CompressibilityMeasuresandSuccinctDataStructuresforPiecewiseLinearApproximations\nA.3 Using Space Close toB I(ℓ, ε, u, n,x)\nGiven a PLA{s1,s2,...,sℓ}∈I (ℓ,ε,u,n ), we proceed similarly as in the compression setting.\nWe represent a generic segment sias a tuplesi= (xi,x′\ni,βi,γi,yi).xiandx′\niare the first\nand last covered x-values,βiis the intercept, γiis the lasty-value given by the segment,\nwhileyiis the first covered y-value. To encode each segment, To encode each segment, we\nfirst store two bit sequences, XandY, which respectively represent the first x-value and\nthe lasty-value covered by each segment. The sequence Xencodes, in unary, the deltas\nbetween the x-values of consecutive segments (i.e., xi+1andxi), shifted by a constant to\nfurther reduce space usage. This representation enables constant time access to any value via\na simpleselectoperation. An identical strategy is applied to the sequence Y, which encodes\nthe deltas between successivey-values. Such sequences are detailed as follows.\nX= 0x1−110x2−x1−2ε1...0xℓ−xℓ−1−2ε1. The sequence is well defined since x1≥1and\nxi+1−xi≥2ε. The length of Xisx1+/summationtextℓ−1\ni=1(xi+1−xi−2ε+ 1) =xℓ−(ℓ−1)(2ε−1),\nwhich is indeed upper bounded by u−ℓ (2ε−1)sincexℓ≤u− 2ε+ 1, and contains ℓones.\nThei-th element ofXcan be accessed by noticing thatx i=select 1(X,i) + 2ε(i−1).\nY= 0y2−y1−2ε1...0yℓ−yℓ−1−2ε1. The sequence is well defined since yi+1−yi≥2ε. The\nlength ofYis/summationtextℓ−1\ni=1(yi+1−yi−2ε+1) =yℓ−y1−(ℓ−1)(2ε−1), which is upper bounded\nbyn−ℓ(2ε−1)−1asy1= 1andyℓ≤n− 2ε+1, and contains ℓ−1ones. Any element of Y\ncan be retrieved by noticing that y1= 1and for any i>1,yi=select 1(Y,i− 1)+2ε(i−1).\nAgain, given our assumption that ℓ=o(n), both sequences are sparse. Since they are also\nmonotonically increasing by construction, we represent them using the Elias–Fano encoding\nscheme. By Theorem 2, using the same argument as in the compression setting, the space\nrequired to storeXandYis:\nlog/parenleftbiggu−ℓ(2ε−1)\nℓ/parenrightbigg\n+ log/parenleftbiggn−ℓ(2ε−1)−1\nℓ−1/parenrightbigg\n+O(ℓ+ logu)(12)\nNext,wefocusonthesequenceoflastcoveredkeysofeachsegment, namely x′\n1,x′\n2,...,x′\nℓ−1,\nwe do not consider x′\nℓas it is always equal to u. Since each x′\ni∈[xi+ 1,xi+1−1], we exploit\nthe same technique as in the compression setting using the following number of bits:\n/parenleftiggℓ−1/summationdisplay\ni=1log (xi+1−xi−1)/parenrightigg\n+O(ℓlog logu\nℓ)(13)\nUnlike the compression setting, the y-values are consecutive; hence, the position of the\nlast covered value of the i-th segment is implicitly given by yi+1−1. Moreover, since each\nsegment is a ε-approximation for the points it covers, |βi−yi|≤2εand|γi−(yi+1−1)|≤2ε.\nTherefore, we store each intercept βiand lasty-valueγias deltas with respect to yiand\nyi+1−1inside two arrays∆ βand∆γusing2ℓlog (2ε+ 1) bits. Notice that, since yℓ+1is\nnot available, we store γℓdirectly using only O(logn)bits. Combining this with Equation 12\nand 13, and recalling Corollary 9, the space usage of our storage scheme can be expressed as\nfollows:\nBI(ℓ,ε,u,n,x) +O(ℓlog logu\nℓ+ logu)(14)\nThatissuccinctspace,since O(ℓlog logu\nℓ+logu)⊆o(BI(ℓ,ε,u,n, x))becauseBI(ℓ,ε,u,n, x)\nincludes a term that isΘ( ℓlogu\nℓ), and we consider the case that ℓ= Ω( logu/log logu ).\n\nP. Ferragina and F. Lari 23:19\nWithout adding any extra space on top of this representation, performing a predict (x)query\nproceeds exactly as in the compression setting. Hence, after computing the index iof the\nsegment covering x, we retrieve its the parameters xi,βi,γi, andx′\niusing a procedure that\nclosely resembles the one already detailed in Section 5.1, thus returning:\npredict(x) =/floorleftbigg\n(x−xi)(γi−βi)\n(x′\ni−xi)/floorrightbigg\n+βi\nThe cost of a predictis again dominated by the predecessor search on X, which overall\nmakes the query time O(logℓ). Still, it is possible to reduce this time by directly storing\nthe sequence X=x1,x2,...,xℓ, whose universe is bounded by u−(2ε−1)and contains\nℓones. The index of the segment covering a given value xcan be computed with a rank\noperation, and it allows access to any xiusing aselectoperation. Applying Theorem 1 on\nXguarantees that for any constant c>0, bothrankandselectoperations take O(c)time\nwhile requiring the following space usage in bits:\nlog/parenleftbiggu−(2ε−1)\nℓ/parenrightbigg\n+O/parenleftbiggu\nlogcu/parenrightbigg\nSince we assumed that ε=O(1)andℓ=o(n), henceℓ=o(u), we refactor the term\nlog/parenleftbigu−(2ε−1)\nℓ/parenrightbig\nas follows:\nlog/parenleftbiggu−(2ε−1)\nℓ/parenrightbigg\n= log/parenleftbiggu−ℓ(2ε−1)\nℓ/parenrightbigg\n+ log\n/producttextℓ(2ε−1)−1\ni=(2ε−1)(u−i)\n/producttextℓ(2ε−1)−1\ni=(2ε−1)(u−i−ℓ)\n\n≤log/parenleftbiggu−ℓ(2ε−1)\nℓ/parenrightbigg\n+ (ℓ−1)(2ε−1) log/parenleftbiggu−(2ε−1)\nu−(ℓ+ 1)(2ε−1) + 1/parenrightbigg\n= log/parenleftbiggu−ℓ(2ε−1)\nℓ/parenrightbigg\n+O(ℓ)\nWhere the inequality is obtained by upper-bounding each term in the numerator by\nu−(2ε−1)and lower-bounding each term in the denominator by u−(ℓ+ 1)(2ε−1) + 1.\nThus, the overall space usage can be expressed as:\nBI(ℓ,ε,u,n,x) +O(ℓlog logu\nℓ+u\nlogcu)(15)\nAgain, this is succinct space since O(ℓlog logu\nℓ+u\nlogcu)⊆o(BI(ℓ,ε,u,n, x))as long as\nℓ= Ω(u/logcu)(the proof is similar to the one in Section 5.1). As a result, we proved the\nequivalent of Theorem 10 in the indexing setting, which gives the first data structures able to\nstore any PLA in such a scenario using space close to the information-theoretic lower bound\nof Corollary 9 while still supporting efficientpredictoperations.\n▶Theorem 14.There exist data structures storing any PLA fromI(ℓ,ε,u,n)as follows:\nUsingBI(ℓ,ε,u,n, x) +O(ℓlog logu\nℓ+logu)bits of space, while supporting the predict\noperation inO(logℓ)time.\nUsingBI(ℓ,ε,u,n, x)+O(ℓlog logu\nℓ+u/logcu)bits of space for any constant c>0, while\nsupporting thepredictoperation inO(c)time.\nFinally, we redirect to Section 5.2 for a detailed discussion on how our data structures\nrelate to the storage scheme for PLAs of the PGM-index of Lemma 4.",
  "textLength": 64710
}