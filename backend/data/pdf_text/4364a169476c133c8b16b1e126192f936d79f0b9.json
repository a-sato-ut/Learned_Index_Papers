{
  "paperId": "4364a169476c133c8b16b1e126192f936d79f0b9",
  "title": "Theoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability",
  "pdfPath": "4364a169476c133c8b16b1e126192f936d79f0b9.pdf",
  "text": "Theoretical Analysis of Learned Database Operations under Distribution Shift\nthrough Distribution Learnability\nSepanta Zeighami1Cyrus Shahabi2\nAbstract\nUse of machine learning to perform database op-\nerations, such as indexing, cardinality estimation,\nand sorting, is shown to provide substantial perfor-\nmance benefits. However, when datasets change\nand data distribution shifts, empirical results also\nshow performance degradation for learned mod-\nels, possibly to worse than non-learned alterna-\ntives. This, together with a lack of theoretical un-\nderstanding of learned methods undermines their\npractical applicability, since there are no guaran-\ntees on how well the models will perform after\ndeployment. In this paper, we present the first\nknown theoretical characterization of the perfor-\nmance of learned models in dynamic datasets,\nfor the aforementioned operations. Our results\nshow novel theoretical characteristics achievable\nby learned models and provide bounds on the per-\nformance of the models that characterize their\nadvantages over non-learned methods, showing\nwhy and when learned models can outperform the\nalternatives. Our analysis develops the distribu-\ntion learnability framework and novel theoretical\ntools which build the foundation for the analysis\nof learned database operations in the future.\n1. Introduction\nGiven a fixed dataset, learned database operations (machine\nlearning models learned to perform database operations such\nas indexing, cardinality estimation and sorting) have been\nshown to outperform non-learned methods, providing speed-\nups and space savings both empirically (Kraska et al., 2018;\nKipf et al., 2018; Kristo et al., 2020) and, for the case of in-\ndexing, theoretically (Zeighami & Shahabi, 2023; Ferragina\net al., 2020). For dynamic datasets (e.g., when new points\ncan be inserted into the dataset), significant empirical ben-\nefits are also often observed when using learned methods.\n1University of California, Berkeley. Work done while at USC’s\nInfolab2University of Southern California. Correspondence to:\nSepanta Zeighami <zeighami@berkeley.edu >.\nProceedings of the 41stInternational Conference on Machine\nLearning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).However, an important caveat accompanying these results is\nthat, especially when data distribution changes, models’ per-\nformance may deteriorate after new insertions (Ding et al.,\n2020; Negi et al., 2023; Wang et al., 2021), possibly to worse\nthan non-learned methods (Wongkham et al., 2022). This,\ncombined with the lack of a theoretical understanding of the\nbehavior of the learned models as datasets change, poses\na critical hurdle to their deployment in practice. It is theo-\nretically unclear why and when learned models outperform\nnon-learned methods, and, until this paper, no theoretical\nwork shows any advantage in using the learned methods\nin dynamic datasets and under distribution shift. The goal\nof this paper is to theoretically understand the capabilities\nof learned models for database operations, show why and\nwhen they outperform non-learned alternatives and provide\ntheoretical guarantees on their performance.\nWe specifically study learned solutions for three fundamen-\ntal database operations: indexing, cardinality estimation and\nsorting. Our main focus is the study of learned indexing\nand cardinality estimation in the presence of insertions from\na possibly changing data distribution, while we also study\nlearned sorting (in static scenario) to show the broader ap-\nplicability of our developed theoretical tools. In all cases,\na learned model, ˆf(x;θ)is used to replace a specific data\noperation, fD(x), that takes an input xand calculates a de-\nsired answer from the dataset D. For cardinality estimation,\nfD(x)returns the number of points in the database Dthat\nmatch the query x, and for indexing fD(x)returns the true\nlocation of xin a sorted array. The model ˆf(x;θ)is trained\nto approximate fD(x), and an accurate approximation leads\nto efficiency gains when using the model (e.g., for learned\nindexing, if ˆf(x;θ)gives an accurate estimate of location\nofxin a sorted array, a local search around the estimated\nlocation efficiently finds the exact location). In the presence\nof insertions, the ground-truth fD(x)changes as the dataset\nchanges (e.g., the cardinality of some queries increase as\nnew points are inserted). Thus, as more points are inserted\n(not only due to distribution shift, but exacerbated by it),\nthe accuracy of ˆf(x;θ)worsens. A common solution is to\nperiodically retrain ˆfto ensure consistent accuracy. This,\nhowever, increases insertion cost when insertions trigger a\n(computationally expensive) model retraining.\n1arXiv:2411.06241v1  [cs.LG]  9 Nov 2024\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nLearned Operation Query Complexity Insertion Complexity Space Complexity\nIndexing TX\nnlog log n+ log δnTX\nnlog log n+ log δn+BX\nnlog2logn n logn†\nCE,d-dim, ϵ= Ω(√n) TX\nn max{δn\nϵ,1}BX\nn SX\nn\nCE,1-dim TX\nϵ2+ log n max{δϵ,1}BX\nϵ2+ log nn\nϵ2SX\nϵ2+n\nϵ2logn\nSorting TX\nnnlog log n†SX√n+nlogn\nSorting , appx. known dist. TXn SX\nTable 1. Summary of results for data sampled from a distribution learnable class X(CE: cardinality estimation, †: for simplicity assuming\nSX\nn,BX\nnare at most linear in data size, see Theorem 4.1 and Theorem 4.5 for general cases).\nTheoretically, the relationship between accuracy change and\nnew data insertion has not been well understood, leading to\na lack of meaningful theoretical guarantees for learned meth-\nods in the presence of insertions. The only existing guaran-\ntees are by the PGM index (Ferragina & Vinciguerra, 2020),\nwhich achieves a worst-case insertion time of O(logn)with\nworst-case query time of O(log2n). Although experimental\nresults show PGM often outperforms B-trees in practice\n(Ferragina & Vinciguerra, 2020; Wongkham et al., 2022),\nthe theoretical guarantees are worse than those of a B-tree\n(that supports both insertions and queries in O(logn)). Such\ntheoretical guarantees do not meaningfully characterize the\nindex’s performance in practice nor show why and when the\nlearned model performs better (or worse) than B-trees.\nIn this paper, we present the first known theoretical char-\nacterization of the performance of learned models for in-\ndexing and cardinality estimation in the presence of inser-\ntions, painting a thorough picture of why and when they\noutperform non-learned alternatives for these fundamental\ndatabase operations. Our analysis develops the notion of\ndistribution learnability , a characteristic of data distribu-\ntions that helps quantify learned database operation’s perfor-\nmance for data form such distributions. Using this notion,\nour results are distribution dependent (as one expects bounds\non learned operations should be), without making unnec-\nessary assumptions about data distribution. Our developed\ntheoretical framework builds a foundation for the analysis\nof learned database operations in the future. To show its\nbroader applicability, we present a theoretical analysis of\nlearned sorting, showing its theoretical characteristics and\nproving why and when it outperforms non-learned methods.\n1.1. Summary of Results\nTable 1 summarizes our results in the following setting.\nSuppose ndata points are sampled independently from\ndistributions χ1, ..., χ n, and let the distribution class X=\n{χ1, ..., χ n}. The points are inserted one by one into a\ndataset. Didenotes the dataset after iinsertion. Our goal\nis to efficiently answer cardinality estimation and indexing\nqueries on Diaccurately for any i, i.e., as new points are\nbeing inserted. We denote distribution shift byδ∈[0,1]\n(defined based on, and often equal, to total variation dis-\ntance) where δ= 0means no distribution shift. Table 1 also\ncontains results for sorting, where the goal is to sort the fixedarrayDn, and the reported results are the time and space\ncomplexity of doing so. For sorting only, we assume the\nsamples are i.i.d. All results are expected complexities, with\nthe expectation over sampling of the data, and the insertion\ncomplexity is amortized over ninsertions. To obtain our\nresults, we develop a novel theoretical framework, dubbed\ndistribution learnability . We provide an informal discussion\nof the framework before discussing the results.\nDistribution Learnability . At a high level, distribution\nlearnability means we can model a data distribution well .\nThis notion allows us to state our results in the form “ if\nwe can model a data distribution well, learned database\noperations will perform well on data from that distribution ”.\nThen, if one indeed proves that “ we can model the data dis-\ntribution χwell”, our result immediately implies “ learned\ndatabase operations will perform well on data coming from\nχ”. Crucially, our Theorem 3.5 shows that purely function\napproximation results (independent of the application of\nlearned databases) imply distribution learnability, enabling\nus to utilize function approximation results to show the\nbenefits of learned database operations.\nMore concretely (but still informally), we say a distribution\nclassXisdistribution learnable with parameters TX\nn,SX\nn,\nBX\nn, if given a set of observations, Dn, from distributions in\nX, there exists a learning algorithm that returns an accurate\nmodel, ˆf, of the distributions in X, and that ˆfcan be evalu-\nated in TX\nnoperations, and takes space at most SX\nnto store.\nFurthermore, the learning algorithm takes time n× BX\nnto\nlearn ˆf, where BX\nnis the amortized training time. The no-\ntion is related to statistical estimation, but we also utilize it\nto characterize time and space complexity of modeling.\nResults in Table 1 are stated for data sampled from any distri-\nbution learnable class X. For illustration, we summarize the\nresults for two specific distribution classes: (1) distributions,\nXρ, with p.d.f bounded between 0< ρ1andρ2<∞, and\n(2) distributions, Xc, where the data distribution is known\nand probability of events can be calculated efficiently (e.g.,\ndistribution is known to be uniform or piece-wise polyno-\nmial). The first case formulates a realistic scenario for the\ndata distribution (experimentally shown by Zeighami &\nShahabi (2023)), while the second case presents a best case\nscenario for the learned models, showing what is possible\n2\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nin favorable circumstances. Lemma 3.6 shows Xρ, (and\ntrivially) Xcare distribution learnable, deriving the corre-\nsponding values for TX\nn,SX\nnandBX\nn(See Table 2 for exact\nvalues). Next, we discuss Table 1 for XρandXc, where we\nsubstitute the values of TX\nn,SX\nnandBX\nnfrom Lemma 3.6\nforXρandXc, and discuss the resulting complexities.\nIndexing . After substituting the complexities in the first row\nof Table 1 we obtain that for XρandXc, query and insertion\ncomplexities are O(log log n+ log( δn)). To understand\nthis result, consider the simple scenario with δ= 0, where\ninserted items are sampled form a fixed distribution, and\nthus frequent model updates are not necessary. The result\nshows that a learned model performs insertions and queries\ninO(log log n), showing their superiority over non-learned\nmethods that perform queries and insertions in O(logn).\nNonetheless, when there is a distribution shift, model per-\nformance worsens. In the worst-case and when δ= 1, we\nsee no advantage to using learned models over non-learned\nmethods. This is not surprising, since learned models use\ncurrent observations to make prediction about the future, and\nif the future distribution is drastically different, one should\nnot be able to gain from using the current observations.\nCardinality Estimation . First, consider the second row\nin Table. 1, showing performance of learned models for\ncardinality estimation in high dimensions but when error is\nat least√n. Substituting the complexities in this row, for Xc,\nwe obtain that learned models perform insertions and queries\ninO(1)time and space in this setting. This is significant,\ngiven that a non-learned method such as sampling (and more\nbroadly ϵ-approximations (Mustafa & Varadarajan, 2017)),\neven in this accuracy regime, needs space exponential in\ndimensionality (Wei & Yi, 2018). Nonetheless, modeling\nin high dimensions is difficult, and consequently this result\nrequires the accuracy to be at least√n. Moreover, even\nforϵ≥√nbut for more general distribution class of Xρ,\nour results show that learned methods will also take space\nexponential in dimensionality (which is broadly needed,\neven for neural networks (Petersen & V oigtlaender, 2018),\nwithout further assumptions). We also mention that√n\nhas a statistical significance (see Sec. 3 for discussion), and\nappears in our analysis throughout. Second, we show that\nin 1-dimension (the third row of Table 1), learned models\nperform cardinality estimation queries effectively, where for\nXc, a learned model can perform queries and insertions in\nO(logn)while taking space O(n\nϵ2logn). This result also\nshows that a learned approach outperforms the non-learned\n(and worst-case optimal) method discussed in (Wei & Yi,\n2018) that takes space O(n\nϵlogϵn)to answer queries.\nSorting . Substituting complexities in the fourth row of\nTable 1, we obtain O(nlog log n)time complexity for Xρ,\nusing a method that is a variation of Kristo et al. (2020) that\nlearns to sort through sampling. Our framework applies tothis method because its study needs to consider the gener-\nalization of a model learned from samples (similar to how\nmodels need to generalize to a new dataset after insertions).\nMoreover, last row of Table 1 shows that, if we (approxi-\nmately) know the data distribution, and the distribution can\nbe efficiently evaluated and stored, we can sort an array in\nO(n)(TXis independent of data size), showing benefits of\nusing data distribution to perform database operations.\nTo conclude, our results in Table 1 are more general than the\ntwo distribution classes discussed above. A major contribu-\ntion of this paper is developing the distribution learnability\nframework that allows us to orthogonally study the two prob-\nlems of modeling a data distribution ( the modeling problem ),\nand how learned models can be used to perform database\noperations with theoretical guarantees ( the model utilization\nproblem ). Table 1 summarizes our contributions to the latter\nproblem, while our results connecting distribution learnabil-\nity to function approximation concepts (Theorem 3.5) is our\ncontribution to the former. The rest of this paper discusses\nour developed framework and results in more detail, but for\nthe sake of space, formal discussion, proofs, and a detailed\ndiscussion of the related work are differed to the appendix.\n2. Preliminaries\n2.1. Problem Setting\nSetup . We study performing database operations on a possi-\nbly changing d-dimensional dataset. We either consider the\nsetting when ndata points are inserted one by one (dynamic\nsetting), or that we are given a fixed set of ndata points\n(static setting). We define Dias the dataset after iinsertions,\nand the final dataset, Dn, is often denoted as D. We study\nindexing, cardinality estimation and sorting operations.\nFor indexing, the goal is to build an index to store and\nfind items in a 1-dimensional dataset. The index supports\ninsertions and queries. That is, after iinsertions, for any\ni, we can retrieve items from the dataset Di, where the\nquery is either an exact match query or a range query. For\ncardinality estimation, the dataset is d-dimensional, and we\nsupport insertions and axis-parallel queries. That is, after i\ninsertions, for any i, we would like to estimate the number of\nitems in the dataset Dithat match a query q, where qdefines\nan axis-parallel hyper-rectangle. Finally, the goal of sorting\nis to sort a fixed 1-dimensional array, D, of size n. Indexing\nand sorting always return exact results (i.e., array has to be\nfully sorted after the operation), while cardinality estimation\naccepts an error of ϵfor the query answer estimates.\nData Distribution and Distribution Shift . We consider\nthe case that the i-th data point is sampled independently\nfrom a distribution χi, and denote by D∼χthis sam-\npling procedure, where χ={χ1, ..., χ n}. We say Dwas\nsampled from a distribution class Xifχi∈X∀i. We\nuse total variation to quantify distribution shift. We say D\n3\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nwas sampled from a distribution χwith distribution shift\nδ, when max χi,χj∈χ∥χi−χj∥TV, where ∥χi−χj∥TV\ndenoted the total variation (TV) distance between χiand\nχj. We also define total variation of a distribution set χ\nasTV(χ) = supχi,χj∈χ∥χi−χj∥TV. TV is a number\nbetween 0 and 1 with δ= 1the maximum distribution shift\nandδ= 0the case with no distribution shift.\nProblem Definition . We study the performance of learned\nmodels when performing the above data operations. Assume\nan algorithm takes at most TI(D)operations to perform n\ninsertions from a dataset D, at most TQ(D)to perform\nany query and has S(D), space overhead (excluding the\nspace to store the data). We study amortized expected inser-\ntion time defined as1\nnED∼χTI(D), expected query time,\nED∼χTQ(D), and storage space, ED∼χS(D).\n2.2. Learned Database Operations\nOperation Functions . LetfD(x)be an operation function ,\ndefined as a function that takes an input xand outputs the\nanswer, calculated from the database D, for some desired\noperation. In this paper, fDis either the cardinality function,\ncD(x), that takes a query, x, as input and outputs the number\nof points in Dthat match x, or the rank function, rD(x)\nthat takes a 1-dimensional query as input and returns the\nnumber of elements in Dsmaller than x. The rank function\nis used in sorting and indexing, because rD(x)is the index\nofxifDwas stored in a sorted array. We use the notation\nfD∈ {rD, cD}(orf∈ {r, c}) to refer to both functions,\nrDandcD(for instance, fD≥0is equivalent to the two\nindependent statements that rD≥0andcD≥0).\nWe also define distribution operation function ,fχ, for an\noperation f, defined as fχ(x) =ED∼χ[fD(x)], ifDis\nsampled from a distribution χ. Note that distribution op-\neration function depend only on the data distribution (and\nnot observed dataset). For instance,1\nnrχis the c.d.f of data\ndistribution, χifDis sampled i.i.d from χ, and similarly\n1\nncχ(x)is the probability that a sample from χfalls in an\naxis-parallel rectangle defined by x. We call ED∼χ[cD(x)]\ndistribution cardinatliy function.\nLearned Database Operations with Insertions . Learned\ndatabase operations learn a model ˆfthat approximates fD\nwell, and use the learned model to obtain an estimate of\nthe operation output (for sorting and indexing, a refinement\nstep ensures exact result, through either local binary search\nor lightweight sorting). However, as new data points are\ninserted and the dataset changes, the ground truth answers to\noperations change, thereby increasing the model error. Note\nthat model answers are scaled to current data size (i.e., if ˆf\nwas trained on a dataset of size i, and tested on a dataset of\nsizej, we reportj\niˆfas answers), but this does not stop the\nerror from increasing. Thus, to guarantee the error is below\na threshold, one needs to update the models as the datasetschange, which is often done by periodically retraining the\nmodels. Model retraining contributes to insertion cost in\nthe database. To keep the insertion cost low, one needs\nto minimize retraining frequency. Meanwhile, infrequent\nretraining increases error (and, for indexing, query time).\nFinding a suitable balance between insertion time, accuracy\nand query time is a subject in much of our theoretical study.\n3. Analysis through Distribution Learnability\nOur goal is to ensure that a model ˆftrained to perform\noperations f,f∈ {r, c}, has bounded error. We first discuss\na lower bound on the error of models in the presence of\ninsertions, which motivates our analysis framework.\nLower Bound on Model Generalization. Consider a\nmodel ˆf, trained after the i-th insertion and using dataset\nDi. Assume the model is not retrained after kfurther in-\nsertions so that ˆfis used to answer queries for dataset Dj,\nj=i+k. The following lemma shows a lower bound on\nthe expected maximum generalization error of the model to\ndataset Dj, defined as supxED∼χ[|j\niˆf(x)−fDj(x)].\nTheorem 3.1. Consider any model ˆftrained after the i-th\ninsertion and on dataset Di. For any integer j > i + 2and\nafter performing k=j−inew insertions we have\nsup\nxEDj∼χ[|j\niˆf(x)−fDj(x)|]≥√\nk\n4,\nwhenDjis i.i.d from any continuous distribution χ.\nTheorem 3.1 states that the expected error of a single fixed\nmodel, no matter how good the model is when it is trained,\nafter kinsertions, will increase to Ω(√\nk)on some input.\nConsequently, to achieve an error at most ϵ, we have to\nretrain the model at least every (4ϵ)2insertions. For any\nconstant error ϵ, this impliesn\n(4ϵ)2=O(n)model retraining\nis needed when inserting ndata points. Model retraining\nfor many practical choices costs O(n)(to go over the data\nat least once), so that amortized insertion cost, i.e., insertion\ncost per insertion , must be at least O(n). This is signifi-\ncantly larger than non-learned methods, e.g., for indexing\nB-trees support insertions in O(logn).\nNonetheless, the√\nkbarrier (and consequently a heavy in-\nsertion cost) can be avoided, as is often done in practice,\nbypartial retraining. A common example is arranging a\nset of models in a tree structure and retraining parts of the\ntree structure as new data is inserted. This avoids a full\nretraining every O(ϵ2)insertions, but makes smaller neces-\nsary adjustments throughout that are cheap to make. Thus,\nTheorem 3.1 provides a theoretical justification for many\npractical design choices in existing work (Ding et al., 2020;\nZeighami et al., 2023; Galakatos et al., 2019) that partition\nthe space and train multiple models, utilizing data structures\nbuilt around multiple models to perform operations. We\nalso note that such approaches often come with repartition-\ning and tree balancing as new data is inserted to adjust the\n4\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\ncreated partitions after observing new points. Indeed such\nrepartitioning is also necessary in the presence of insertions.\nGiven that for a fixed set of partitions the number of points\nper partition will grow linearly in data size, Theorem 3.1\ncan be used to show the error per partition will remain large\nunless partitions are recreated and adjusted as new points\nare observed.\nThe error in Theorem 3.1 is independent of total data size,\nj, and only depends on k. This is because we make no\nassumptions on model capacity, and consequently, when the\nmodel is trained on the dataset, Diof size i, the training\nerror can be zero. Thus, error on Djonly depends on how\nwell the trained model on Digeneralizes to Dj, which,\nintuitively, only depends on the difference between Diand\nDj. Theorem 3.1 quantifies this difference in terms of k.\nAnalysis Framework Overview . In light of Theorem 3.1\nand existing practical modeling choices that use a set of\nmodels to perform an operation, analyzing database oper-\nations in the presence of insertion can be divided into two\ncomponents: (1) how well a model can learn a set of ob-\nservations ( the modeling problem ), and (2) how a set of\nmodels can be used to perform operations in the presence of\ninsertions ( the model utilization problem ). Our framework\nallows studying the two separately, as discussed next.\n3.1. The Modeling Problem\nOur analysis is divided into studying the problem of mod-\neling and the problem of model utilization. We introduce\nthe notion of distribution learnability to abstract away the\nmodeling problem when studying the utilization problem.\nRoughly speaking, if a distribution class is distribution\nlearnable , we can use observations from the class to model\ntheir distribution operation functions well. In other words, if\na distribution class is distribution learnable, we have a solu-\ntion to the modeling problem, and thus, we can focus on the\nmodel utilization problem. Meanwhile, the modeling prob-\nlem is reduced to showing distribution learnablity. In this\nsection, we define distribution learnability and discuss how\nwe can prove a distribution class is distribution learnable.\n3.1.1. D EFINING DISTRIBUTION LEARNABILITY\nA distribution class is distribution learnable if there exists an\nalgorithm that returns a good model of the data distribution\ngiven an observed dataset. Formally,\nDefinition 3.2. A distribution class X, is said to be dis-\ntribution learnable for an operation f,f∈ {r, c}, with\nparameters TX\nn,SX\nnandBX\nn, if for any χ⊆X, there exists\nan algorithm that takes a set of observations, D∼χ, of\nsizenas input and returns a model ˆfsuch that:\n•(Accuracy ) IfDis sampled from χ, for some χ⊆X,\nwe have that\nPD∼χ[∥fχ−ˆf∥∞≥ϵ]≤κ1e−κ2(ϵ√n−1)2\n,For any ϵ≥√nand universal constants κ1,κ2>0;\n•(Inference Complexity ) It takes TX\nnnumber of opera-\ntions to evaluate ˆfand space SX\nnto store it; and\n•(Training Complexity ) Each call to the algorithm costs\nBX\nnamortized number of operations.\nThat a distribution class is distribution learnable for opera-\ntionfmeans that observations from the distribution class\ncan be used to model the expected value of fto a desired\naccuracy, and that distribution dependent parameters TX\nn,\nSX\nnandBX\nn, characterize the computational complexity of\nthe modeling (amortized number of operations is total num-\nber of operations divided by n, sonBX\nnis total number of\noperations). We make two remarks regarding the definition.\nRemark 3.3.The accuracy requirement for distribution\nlearnability is defined so that, with high probability, the\nmodel error is at most O(√n). This is due to Theorem 3.1,\nwhich shows the expected generalization error, after ninser-\ntions, will be Ω(√n)as dataset changes because of inser-\ntions and irrespective of modeling accuracy. Thus, having\nmodeling error lower than O(√n)will not improve the\ngeneralization error, but will increase inference complexity\n(larger models will be needed to improve accuracy). Mean-\nwhile, due to the inherent Ω(√n)error, an extra modeling\nerror of√nonly increases generalization error by a constant\nfactor, thus not changing any of our results asymptotically.\nWe also note that, given that Def. 3.2 allows modeling er-\nrors to occur, using distribution learnability to return exact\nresults (i.e., for indexing and sorting) requires designing fall-\nback strategies to ensure correctness even in the presence of\nmodeling error. This is done in our results in Sec. 4, where\nmodel outputs are adjusted by some lightweight non-learned\nmethod (e.g., exponential search for indexing and merge\nsort for sorting) to ensure correctness. On the other hand,\nfor cardinality estimation, where the goal is to obtain fast\nestimates and errors can be tolerated in practice, our results\nin Sec. 4 use the modeling error formulation in Def. 3.2\nto present methods that answers queries with guaranteed\nbounds on error.\nRemark 3.4.Distribution learnability for the distribution\nclass,Xis defined so that we can characterize the compu-\ntational complexity of modeling data from X. Such charac-\nterization is important because different modeling choices\nare beneficial for different distributions. For instance, a\nlinear model may be sufficient to model data from uniform\ndistribution but not for a Gaussian distribution. The defi-\nnition allows us to distinguish between simple distribution\nclasses where we can create models that are fast to evaluate\n(e.g., linear models for uniform distribution), from more\ncomplex distribution classes that may need more complex\nmodels with higher runtime and space complexity (e.g.,\nneural networks for complex distributions). This is done\nthrough parameters TX\nn,SX\nnandBX\nn.\n5\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nDistribution class TX\nnSX\nn nBX\nn\nXρ 1 ρ√nlogn ρ√nlogn\nXl logl llogn n logn\nXc 1 1 1\nTable 2. Asymptotic complexities of some distribution learnable\nclasses for rank function defined in Lemma 3.6\n3.1.2. P ROVING DISTRIBUTION LEARNABILITY\nFor a distribution class, X, to be distribution learnable for\nan operation f, we need to be able to model distribution\noperation functions in that class using some model class F.\nIntuitively, Fneeds to have enough representation power\nto model distributions in X, and we need to be able to ef-\nfectively optimize over Fto find the good representations\ngiven an input (i.e., Fisoptimizable ). The following the-\norem shows that if these two properties are true, then the\ndistribution class is indeed distribution learnable. For the\nsake of space, we only state our results here informally (for-\nmal statement is in Sec. C), as a formal statement requires\nmaking enough representation power andopitimizability\nconcrete, which diverts from our main discussion.\nTheorem 3.5 (Informal) .LetXbe a distribution class\nwhose operation functions belong to some function class G.\nThat is, fχ∈ Gfor all χ∈X, for an operation f. Assume\nanother function class, F, has enough representation power\nto represent G, and is optimizable. Then, Xis distribution\nlearnable for operation f.\nTheorem 3.5 can be broadly used to translate function ap-\nproximation results to distribution learnability. For instance,\nTaylor’s theorem shows that infinitely differentiable func-\ntions can be approximated by polynomials to arbitrary ac-\ncuracy (i.e., polynomials have enough representation power\nto represent infinitely differentiable functions), and the ex-\nchange algorithm (Powell, 1981) shows that we can find\nthe best polynomial approximating a function (i.e., shows\noptimizability for polynomials). These together with Theo-\nrem 3.5 imply that distributions with infinitely differentiable\noperation functions are distribution learnable. Nonetheless,\nthe time complexity of function approximation is important\nwhen deciding what function class to choose for model-\ning purposes in database applications. For instance, the\nexchange algorithm, although converges, can take too long\nto find polynomials that model functions with a desired ac-\ncuracy (Powell, 1981). Our next result uses Theorem 3.5\nto show distribution learnability using piecewise linear and\npiecewise constant models that show better time/space com-\nplexity. We first discuss learnability for rank operations.\nLemma 3.6. LetXρbe the set of distributions with p.d.f\nbounded by ρ,Xlthe set of distributions with piecewise\nlinear c.d.f with at most lpieces and Xca distribution the\nc.d.f of which can be stored and evaluated in constant time.\nXρ,Xl,Xcare distribution learnable for rank operation\nwith parameters shown in Table 2.Lemma 3.6 presents results for multiple distribution classes.\nXρformulates a realistic scenario (experimentally shown\nby (Zeighami & Shahabi, 2023)). Xcshows the ideal sce-\nnario for learned models, where the data distribution is easy\nto model, and is included to show a best-case scenario for\nour results when using learned models. Piece-wise linear\nmodels have been used for the purpose of indexing (Fer-\nragina & Vinciguerra, 2020; Galakatos et al., 2019), and\nXlis included to study their theoretical properties for the\ndistribution class where they are well suited. Next, consider\ndistribution learnability for cardinality operation.\nLemma 3.7. LetXρbe the set of distributions for which\nthe distribution cardinatliy function has gradient bounded\nbyρ, and let Xcbe a countable set of cdistributions for\nwhich distribution cardinatliy function can be stored and\nevaluated in constant time. XρandXcare distribution\nlearnable for cardinality estimation where the same param-\neters as Table 2 hold for Xc. ForXρ, we have BX\nnandSX\nn\nasO(√\n2d(ρ√n)2dlogn), while TX\nn=O(1).\nAs before, we have included Xcto show a best-case scenario\nfor learned models. Nonetheless, cardinality estimation is\na problem in high dimensions where modeling is difficult.\nThe exponential behavior in Lemma 3.7 for Xρis required\nfor different modeling choices, including neural networks\n(Petersen & V oigtlaender, 2018; Yarotsky, 2018). To reduce\ncomplexity, stricter assumptions on data distribution are\noften justified. For example, attributes may be correlated\nand only fall in a small part of the space. A common as-\nsumption using neural networks is that data is supported\non a low dimensional manifold (Pope et al., 2021), which\ntogether with results showing that neural networks can ap-\nproximate data on low dimensional manifolds well (Chen\net al., 2019), yields that neural networks can avoid space\ncomplexity exponential in dimensionality. This is an ac-\ntive area of research orthogonal to our work, and our results\nshow how learned database operations can benefit from such\napproximation theoretic results as they become available.\n3.2. The Model Utilization Problem\nOur results in Sec. 4 thoroughly discuss how learned models\ncan perform different database operations for distribution\nlearnable classes. Here, we provide a brief overview of the\ngeneral methodology and state required definitions.\nTypical methods used in practice to perform database opera-\ntions partition the domain and model different parts of the\ndomain separately. Each partition can be denoted by a set\nRof the space it covers. The model in each partition can be\nseen as a model of the conditional distribution of the data,\nwhere the original data distribution is conditioned on the set\nR. As such, to effectively model the data in a partition, we\nneed to be able to model the conditional distribution for the\npartition. This means not only the original data distribution,\nbut also the conditional data distributions need to be distri-\n6\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nbution learnable. We formalize our notion of conditional\ndistribution to be able to formalize this statement.\nLetRbe a set s.t. R ⊂ 2D(i.e.,Ris a set of subsets of\nthe data domain). Then, for any R∈ R withPX∼χ(X∈\nR)>0, we define χ|Ras the data distribution with c.d.f\nFχ|R(x) =PX∼χ(X≤x|X∈R). In this paper, unless\notherwise stated, Ris the set of axis-parallel rectangles,\nwhere R= (rmin,rmax)withrmin,rmax∈[0,1]ddefine\ntwo corners of the hyper rectangle. We define the normal-\nized conditional distribution,¯χ|R, as the distribution with\nc.d.fF¯χ|R(x) =Fχ|R((rmax−rmin)x+rmin). The nor-\nmalization scales the domain of the conditional distribution\nback to [0, 1]d, and helps standardize our modeling dis-\ncussion. We define the closure of a distribution class X,\ndenoted by ¯X, as the set {¯χ|R,∀χ∈X, R∈ R} . That is, ¯X\ncontains not only Xbut all the other distributions obtained\nby distributions χ∈Xconditioned under sets R∈ R. Of-\nten, we need the distribution class ¯X, and not only X, to be\ndistribution learnable. ¯XandXcan be (but not necessarily\nare) the same set. An example is the uniform distribution,\nwhere conditioning the distribution over any interval yields\nanother uniform distribution over the interval.\n4. Results\n4.1. Indexing Dynamic Data\nWe show the following result for dynamic indexing.\nTheorem 4.1. Suppose D∼χforχ⊆Xfor some dis-\ntribution class XwithTV(χ)≤δ, and that ¯Xis distri-\nbution learnable. There exists a learned index into which\nthendata points of Dcan be inserted in O(TX\nnlog log n+\nlogδ√n+BX\nnlog2logn)expected amortized time, that can\nbe queried in O(TX\nnlog log n+log δ√n)expected time and\ntakes space O(nlogn+Plog log n\ni=0n1929iSX\nn29i).\nThe term TX\nnlog log nis due to making log log ncalls to\nthe distribution model, and BX\nnroughly refers to the need to\nrebuild a model every ninsertions. For example, without\ndistribution shift (i.e., δ= 0), one can answer queries and\nperform insertions with O(log log n)model calls, while\nevery ninsertions incurs extra BX\nncost for model rebuilding.\nDistribution shift increases both insertion and query time by\nO(logδ√n). In the worst case, having δ= 1, we recover\nthe traditional O(logn)insertion and query time. That is,\nour results show no gain from modeling when distribution\nshift is too severe. This is as expected. If data distribution\nchanges too much, one cannot use the current knowledge\nof data distribution to locate future elements. By systemati-\ncally handling the distribution shift, we show that a learned\nmethod can provide robustness in such scenarios.\nThe data structure that achieves the bound is a tree structure\nwith a distribution model used in each node to find the\nnode’s child to traverse given a query or insertion. The\nstructure can be thought of as a special case of Alex (Dinget al., 2020), with specific tree height, fanount and split\nmechanism to ensure the desired gaurantees. All elements\nare stored at leaf nodes, and the traversal to the leaf nodes\nis similar to B-trees but using learned models to choose the\nchild. Using Lemma 3.6 we can specialize Theorem 4.1 for\nspecific distribution classes.\nCorollary 4.2. LetXρ1,ρ2be the class of distributions with\nbounded p.d.f. That is, for all χ∈Xρ1,ρ2and denoting by\ngχthe p.d.f of χ, we have 0< ρ1≤gχ(x)≤ρ2<∞,∀x.\nSuppose D∼χforχ⊆Xfor some distribution class\nX⊆Xρ1,ρ2withTV(χ)≤δ. There exists a learned index\nthat supports insertions in O(log log n+log δ√n)expected\namortized time, queries in O(log log n+log δ√n)expected\ntime and takes space O(ρ1\nρ2nlogn).\nCorollary 4.2 shows a learned index that performs insertions\nand answers queries in O(log log n+ log( δn)), while non-\nlearned methods take O(logn). Thus, when distribution\nshift is not severe, a learned method can outperform non-\nlearned methods, while large distribution shift ( δ= 1) leads\nto same bounds as non-learned methods. Corollary 4.1\nstrictly generalizes results in (Zeighami & Shahabi, 2023)\nto the setting with insertions and data distribution change.\nWe note that one can consider the data structure used in the\nproof of Theorem 4.1 (and consequently Corollary 4.2) as\na method for switching between learned and non-learned\nindexes when the distribution shift grows. Recall that proof\nof Theorem 4.1 uses a tree of learned models as an index.\nWhen there is no distribution shift, this tree is traversed only\nusing learned models. However, when the distribution shift\nis large, the tree structure needs to be adjusted frequently\n(i.e., nodes are split frequently), and these adjustments are\nstored in a non-learned data structure. Thus, when the distri-\nbution shift is large, the tree traversal becomes more reliant\non non-learned data structures.\n4.2. Cardinality Estimation\nFor cardinality estimation, designing learned models that\nanswer queries with arbitrary accuracy is more challenging\ndue to the high dimensionality of the problem. The curse\nof dimensionality is a well-understood phenomenon for\nnon-learned methods, leading to approaches that take space\nexponential in dimensionality (Cormode et al., 2012; Wei &\nYi, 2018). We first show that this is not the case when using\nlearned models if an error of Ω(√n)is tolerable.\nTheorem 4.3. Suppose D∼χforχ⊆Xfor a distri-\nbution learnable class XwithTV(χ)≤δ. There exists\na learned cardinality estimator that answers queries with\nexpected error ϵforϵ= Ω(√n)supports insertions in\nO(max{δ√n\nϵ,1}BX\nn), queries in O(TX\nn)and takes space\nO(SX\nn).\nTheorem 4.3 states that we can use a distribution model\nto answer queries for any expected error Ω(√n). Conse-\nquently, when we can effectively model a data distribu-\n7\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\ntion, we can answer queries to accuracy at least√nwith-\nout having an exponential space blowup. Comparing this\nwith random sampling, and more broadly ϵ-approximations,\nthat need at least√nlogd−1(√n)data samples to answer\nqueries with accuracy√n(Wei & Yi, 2018; Matou ˇsek &\nNikolov, 2015), we show a clear advantage to learned mod-\nels over such non-learned methods in this accuracy regime.\nTheorem 4.3 uses a single distribution model that is periodi-\ncally retrained with insertion. The frequency of retraining\ndepends on distribution shift. If δ≤1√n, the error caused\nby distribution shift is on a similar scale as error due to ran-\ndomness. Thus, the distribution shift does not significantly\naffect insertion time. On the other hand, in the worst case\nwhen δ= 1, we need to retrain the model every1\nϵinsertions,\nwhich can be significant depending on retraining cost.\nError of Ω(√n)is not necessarily too large. Indeed, ex-\npected query answer for a fixed query with probability p\nisn×p, so error, relative to the expected query answer is\nO(√n\nnp) =O(1√n)and goes to zero as data size increases.\nNonetheless, one may wish to answer queries more accu-\nrately. Below, we discuss how to achieve this in one dimen-\nsion. Appendix D.6.3, presents Lemma D.1 that shows how\nideas in one dimension can be extended to high dimensions,\nbut nevertheless, only achieves space complexity exponen-\ntial in dimensionality, similar to non-learned methods.\nArbitrary Accuracy in One Dimension . In one dimension,\nwe show the following is possible using learned models.\nTheorem 4.4. Suppose D∼χforχ⊆Xfor a distribution\nclassXwithTV(¯χ)≤δ, and that Xis distribution learn-\nable. There exists a learned cardinality estimator that an-\nswers queries with expected error ϵfor any ϵ >0supports\ninsertions in O(max{deltaϵ, ϵ2}BX\nϵ2) + log n), queries in\nO(logn+TX\nϵ2)and takes space O(n\nϵ2SX\nϵ2+n\nϵ2logn).\nTheorem 4.4 shows that we can effectively answer queries\nto any accuracy in one dimension using learned models.\nImportantly, the result shows that if the data distribution can\nbe modeled space-efficiently (e.g., whenever SX\nϵ2≤logn),\nthen a learned approach outperforms non-learned (and worst-\ncase optimal) method discussed in (Wei & Yi, 2018) that\ntakes space O(n\nϵlogn)to answer queries with accuracy ϵ.\nThe learned model that achieves the bound in Theorem 4.4\nuses a combination of materialized answers and model es-\ntimates to answer queries. Given that a model can be at\nbest accurate to√nif the dataset contains npoints, the\nalgorithm divides up the data domain inton\nϵ2intervals, each\ncontaining ϵ2points, so that a model for each interval will\nhave accuracy ϵ. Meanwhile, the algorithm materializes\nquery answers that span multiple intervals so that errors do\nnot accumulate when answering such queries. The mate-\nrialization is done through a B-tree like structure, where\neach node stores the exact number of points inserted into\nit. Because, in our construction, we build several modelseach for a subset of the space, it is not enough that the total\nvariation between the distributions is bounded, but also that\nthe total variation after conditioning is bounded ( ¯χis the\nclosure of χunder conditioning defined in Sec. 3.2).\n4.3. Sorting\nSorting involves only a fixed array, while the operations\nwe discussed so far consider a dataset that changes due\nto insertions. Our discussion here shows that the distribu-\ntion learnability framework can be beneficial for analyzing\nlearned database operations beyond insertions.\nTo see why our framework applies to learned sorting, first\nrecall the existing learned sorting algorithm of Kristo et al.\n(2020), which sorts an array by first sampling a subset of the\narray, learning a model to predict the correct location using\nthe sample (the sample is sorted by an existing algorithm for\nthe purpose of training), and then using the learned model\nto predict the item locations in the original array. Here,\nsimilar to the case of learned operations with insertions, the\nproblem isn’t (only) how well we can learn a model, but\nalso how well a model learned from a sample of the array\nwill generalize to the complete array. Thus, we need to both\nstudy a modeling problem and a model utilization problem,\nand we can do so using distribution learnability. Finally,\nsince a function that sorts an array is the rank function, our\ndiscussion on distribution learnability for the rank operation\nalready covers the modeling problem.\nBefore stating our results, we also note that one can sort a\nfixed array by iteratively inserting its element into a learned\nindex. Thus, Theorem 4.1 has already provided a method for\nsorting an array using machine learning. Our result below\npresents another learned method for sorting an array. This is\nanalogous to how both B-trees and merge sort can be used to\nsort a fixed array. The result below can be seen as a means\nof extending merge sort with machine learning.\nTheorem 4.5. Suppose an array consists of npoints sam-\npled i.i.d. from a distribution learnable class X. There\nexists a learned sorting method that sorts the array in\nO(TX√nnlog log n+√nBX√n+Plog log n\ni=0n191\n2(4\n5)iBX\nn1\n2(4\n5)i)\nand space O(SX√n+nlogn)\nWe note that√nBX√n+Plog log n\ni=0n191\n2(4\n5)iBX\nn1\n2(4\n5)i)isO(n)\nif model training takes linear in data size, so that the runtime\nisO(TX√nnlog log n). This is independent of training time,\nBX\nn, because, due to sampling, training is done on much\nsmaller arrays than the original data. Thus, Theorem 4.5 pro-\nvides a time complexity for sorting similar to Theorem 4.1,\nboth showing that for efficient modeling choices, one can\nsort an array in O(nlog log n)model calls.\nThe algorithm that achieves the bound in Theorem 4.5 is\nsimilar to Kristo et al. (2020), which first samples a subset\nof the original array, uses it to build a distribution model,\n8\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nand uses the model to sort the original array. Due to model-\ning errors, the resulting attempt using the model will only\nbe a partially sorted array. Unlike Kristo et al. (2020) that\nuses insertion sort to fully sort the partially sorted array, we\nuse a merge sort like approach to recursively sort the array.\nThis is because, to reduce the asymptotic complexity below\nO(nlogn), the sample needs to be of size o(n). However,\nthe generalization error of a model trained on a sample of\nsizeo(n)would be too large to allow insertion sort to be\neffective. Partitioning the partially sorted array and recur-\nsively sorting each portion allows us to sort the array while\nperforming O(TX√nnlog log n)operations.\nFinally, the lower bound of Theorem 3.1 does not apply to\nsorting, and a natural question is if it is possible to do better\nthanO(nlog log n). The following result shows that under\nstronger assumptions on data distribution, this is possible.\nTheorem 4.6. Suppose an array consists of npoints sam-\npled i.i.d. from a distribution χ, and assume we have a\nmodel ˆrs.t.,∥ˆr−rχ∥∞≤ϵ0, that can be evaluated in Tχ\nand takes space Sχ. There exists an algorithm that sorts the\narray in O(Tχnlogϵ0)taking space O(Sχ+n).\nThe theorem shows if we know the data distribution very\naccurately, then we can sort the data very efficiently. Tχ\ncan be O(1), e.g., if the data c.d.f was a polynomial, so\nthat we can sort an array in O(n). This is because the data\ndistribution provides a good indicator of the location of the\nitem in the sorted array. The algorithm that achieves this\ncan be seen as a special case of merge sort, where instead\nof dividing the array into 2, we divide the array, using ˆr, to\nup to ngroups, and recursively sort each.\nThe difference between Theorems 4.6 and 4.5 is how accu-\nrate of a model of data distribution we have access to. Theo-\nrem 4.5 effectively assumes that data distribution can only\nbe modeled to accuracy O(√n), which is too large to allow\nfixing model errors in sorting with a single pass over the\narray. On the other hand, Theorem 4.6 assumes the model\nis correct to within a constant accuracy. As a result, a single\niteration over the partially sorted array fixes any potential\ninversions and yields the O(n)complexity. Nonetheless,\nknowing the data distribution to a constant accuracy can be\nimpractical, because it requires further knowledge about the\ndata distribution beyond merely observing its samples.\n5. Conclusion and Future Work\nWe have presented a thorough theoretical analysis of learned\nindexing and cardinality estimation in the presence of inser-\ntions from a possibly changing data distribution. Our results\ncharacterize learned models’ performance, and show when\nand why they can outperform their non-learned counterparts.\nWe have developed the distribution learnability analysis\nframework that provides a systematic tool for analyzing\nlearned database operations. Our results enhance our un-\nderstanding of learned database operations and provide themuch-needed theoretical guarantees on their performance\nfor robust practical deployment.\nWe believe our theoretical tools will pave the way for a\nbroader theoretical understanding of various learned meth-\nods. Nonetheless, there are several aspects that require\nfurther research to allow for a broader applicability of our\nresults to real-world databases. First, we have established\ndistribution learnability for distributions with bounded p.d.f\nand piecewise-linear c.d.f, but demonstrating distribution\nlearnability for a broader range of real-world data distribu-\ntions is needed to cover a more comprehensive set of real-\nworld data distributions. There are two aspects that require\nfurther research. There is a need to identify and formally\ncharacterize the distribution classes from which real-world\ndatasets originate. This task is challenging, especially for\nhigh-dimensional data. For instance, in the case of images,\nit is commonly believed that they lie on a low-dimensional\nmanifold. It is essential to validate if such assumptions also\nhold true for real-world tabular datasets. Moreover, we need\nto establish distribution learnability for such distribution\nclasses using appropriate modeling choices. The best mod-\neling choice may vary depending on the distribution class,\nnecessitating further research to determine the most effec-\ntive modeling choices for specific distribution classes. Other\nfuture work includes incorporating deletions and updates,\nwhere we believe statistical tools developed here can be uti-\nlized, but formalizing the notion of data distribution in the\npresence of deletions/updates, and the relationship between\ninsertions and deletions/updates require further research. Fi-\nnally, considering query distribution, and analyzing other\ndatabase operations are other future directions.\nAcknowledgements\nThis research has been funded by NSF grant IIS-2128661\nand NIH grant 5R01LM014026. Opinions, findings, con-\nclusions, or recommendations expressed in this material\nare those of the author(s) and do not necessarily reflect the\nviews of any sponsors, such as NSF and NIH.\nImpact Statement\nThis paper presents work whose goal is to theoretically un-\nderstand the use of Machine Learning in database systems.\nDeveloping such theoretical results can improve trust and\nusability of ML in database systems, leading to a more\nwidespread adoption in practice. This can help reduce\ncost, lower energy consumption, and improve usability for\ndatabase systems. There can be various societal conse-\nquences for this, including the creation of more data-driven\napplications.\n9\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nReferences\nAgarwala, A., Das, A., Juba, B., Panigrahy, R., Sharan, V .,\nWang, X., and Zhang, Q. One network fits all? modular\nversus monolithic task formulations in neural networks.\nInInternational Conference on Learning Representations ,\n2021. URL https://openreview.net/forum?\nid=uz5uw6gM0m .\nAndersson, A. and Mattsson, C. Dynamic interpolation\nsearch in o (log log n) time. In Automata, Languages and\nProgramming: 20th International Colloquium, ICALP 93\nLund, Sweden, July 5–9, 1993 Proceedings 20 , pp. 15–27.\nSpringer, 1993.\nBerend, D. and Kontorovich, A. A sharp estimate of the bi-\nnomial mean absolute deviation with applications. Statis-\ntics & Probability Letters , 83(4):1254–1259, 2013.\nChen, M., Jiang, H., Liao, W., and Zhao, T. Efficient ap-\nproximation of deep relu networks for functions on low\ndimensional manifolds. Advances in neural information\nprocessing systems , 32, 2019.\nCormode, G., Garofalakis, M., Haas, P. J., and Jermaine,\nC. Synopses for massive data: Samples, histograms,\nwavelets, sketches. Found. Trends Databases , 4(1–3):\n1–294, January 2012. ISSN 1931-7883. doi: 10.1561/\n1900000004. URL https://doi.org/10.1561/\n1900000004 .\nDing, J., Minhas, U. F., Yu, J., Wang, C., Do, J., Li, Y .,\nZhang, H., Chandramouli, B., Gehrke, J., Kossmann, D.,\net al. Alex: an updatable adaptive learned index. In\nProceedings of the 2020 ACM SIGMOD International\nConference on Management of Data , pp. 969–984, 2020.\nFerragina, P. and Vinciguerra, G. The pgm-index: a fully-\ndynamic compressed learned index with provable worst-\ncase bounds. Proceedings of the VLDB Endowment , 13\n(8):1162–1175, 2020.\nFerragina, P., Lillo, F., and Vinciguerra, G. Why are learned\nindexes so effective? In International Conference on\nMachine Learning , pp. 3123–3132. PMLR, 2020.\nGalakatos, A., Markovitch, M., Binnig, C., Fonseca, R.,\nand Kraska, T. Fiting-tree: A data-aware index structure.\nInProceedings of the 2019 International Conference on\nManagement of Data , pp. 1189–1206, 2019.\nHilprecht, B., Schmidt, A., Kulessa, M., Molina, A., Ker-\nsting, K., and Binnig, C. Deepdb: Learn from data, not\nfrom queries! Proceedings of the VLDB Endowment , 13\n(7), 2019.\nHu, X., Liu, Y ., Xiu, H., Agarwal, P. K., Panigrahi, D., Roy,\nS., and Yang, J. Selectivity functions of range queriesare learnable. In Proceedings of the 2022 International\nConference on Management of Data , SIGMOD ’22, pp.\n959–972, New York, NY , USA, 2022. Association for\nComputing Machinery. ISBN 9781450392495. doi: 10.\n1145/3514221.3517896. URL https://doi.org/\n10.1145/3514221.3517896 .\nJanson, S. On the probability that a binomial variable is\nat most its expectation. Statistics & Probability Letters ,\n171:109020, 2021.\nKipf, A., Kipf, T., Radke, B., Leis, V ., Boncz, P., and Kem-\nper, A. Learned cardinalities: Estimating correlated joins\nwith deep learning. CIDR 2019, 9th Biennial Conference\non Innovative Data Systems Research , 2018.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis,\nN. The case for learned index structures. In Proceedings\nof the 2018 International Conference on Management of\nData , pp. 489–504, 2018.\nKristo, A., Vaidya, K., C ¸etintemel, U., Misra, S., and\nKraska, T. The case for a learned sorting algorithm. In\nProceedings of the 2020 ACM SIGMOD International\nConference on Management of Data , pp. 1001–1016,\n2020.\nLu, Y ., Kandula, S., K ¨onig, A. C., and Chaudhuri, S. Pre-\ntraining summarization models of structured datasets for\ncardinality estimation. Proceedings of the VLDB Endow-\nment , 15(3):414–426, 2021.\nMa, Q. and Triantafillou, P. Dbest: Revisiting approximate\nquery processing engines with machine learning models.\nInProceedings of the 2019 International Conference on\nManagement of Data , pp. 1553–1570, 2019.\nMatou ˇsek, J. and Nikolov, A. Combinatorial discrep-\nancy for boxes via the gamma 2 norm. In 31st Inter-\nnational Symposium on Computational Geometry (SoCG\n2015) . Schloss Dagstuhl-Leibniz-Zentrum fuer Infor-\nmatik, 2015.\nMehlhorn, K. and Tsakalidis, A. Dynamic interpolation\nsearch. Journal of the ACM (JACM) , 40(3):621–634,\n1993.\nMustafa, N. H. and Varadarajan, K. R. Epsilon-\napproximations and epsilon-nets. Chapter 47 in Hand-\nbook of Discrete and Computational Geometry, 3rd edi-\ntion, 2017.\nNegi, P., Marcus, R., Kipf, A., Mao, H., Tatbul, N.,\nKraska, T., and Alizadeh, M. Flow-loss: Learning car-\ndinality estimates that matter. Proc. VLDB Endow. , 14\n(11):2019–2032, jul 2021. ISSN 2150-8097. doi: 10.\n14778/3476249.3476259. URL https://doi.org/\n10.14778/3476249.3476259 .\n10\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nNegi, P., Wu, Z., Kipf, A., Tatbul, N., Marcus, R., Mad-\nden, S., Kraska, T., and Alizadeh, M. Robust query\ndriven cardinality estimation under changing workloads.\nProceedings of the VLDB Endowment , 16(6):1520–1533,\n2023.\nO’Rourke, J. An on-line algorithm for fitting straight lines\nbetween data ranges. Communications of the ACM , 24\n(9):574–578, 1981.\nPerl, Y . and Reingold, E. M. Understanding the complexity\nof interpolation search. Information Processing Letters ,\n6(6):219–222, 1977.\nPerl, Y ., Itai, A., and Avni, H. Interpolation search—a\nlog log n search. Communications of the ACM , 21(7):\n550–553, 1978.\nPetersen, P. and V oigtlaender, F. Optimal approximation\nof piecewise smooth functions using deep relu neural\nnetworks. Neural Networks , 108:296–330, 2018.\nPeterson, W. W. Addressing for random-access storage.\nIBM journal of Research and Development , 1(2):130–\n146, 1957.\nPope, P., Zhu, C., Abdelkader, A., Goldblum, M., and Gold-\nstein, T. The intrinsic dimension of images and its impact\non learning. arXiv preprint arXiv:2104.08894 , 2021.\nPowell, M. J. D. Approximation theory and methods . Cam-\nbridge university press, 1981.\nShorack, G. R. and Wellner, J. A. Empirical processes with\napplications to statistics . SIAM, 1986.\nWang, X., Qu, C., Wu, W., Wang, J., and Zhou, Q. Are we\nready for learned cardinality estimation? Proceedings of\nthe VLDB Endowment , 14(9):1640–1654, 2021.\nWasan, M. T. Parametric estimation. McGraw-Hill; , 1970.\nWei, Z. and Yi, K. Tight space bounds for two-dimensional\napproximate range counting. ACM Transactions on Algo-\nrithms (TALG) , 14(2):1–17, 2018.\nWongkham, C., Lu, B., Liu, C., Zhong, Z., Lo, E., and Wang,\nT. Are updatable learned indexes ready? Proceedings of\nthe VLDB Endowment , 15(11):3004–3017, 2022.\nWu, P. and Cong, G. A unified deep model of learning\nfrom both data and queries for cardinality estimation.\nInProceedings of the 2021 International Conference on\nManagement of Data , pp. 2009–2022, 2021.\nYang, Z., Liang, E., Kamsetty, A., Wu, C., Duan, Y ., Chen,\nX., Abbeel, P., Hellerstein, J. M., Krishnan, S., and Stoica,\nI. Deep unsupervised cardinality estimation. Proceedings\nof the VLDB Endowment , 13(3):279–292, 2019.Yang, Z., Kamsetty, A., Luan, S., Liang, E., Duan, Y ., Chen,\nX., and Stoica, I. Neurocard: one cardinality estimator\nfor all tables. Proceedings of the VLDB Endowment , 14\n(1):61–73, 2020.\nYao, A. C. and Yao, F. F. The complexity of searching an\nordered random table. In 17th Annual Symposium on\nFoundations of Computer Science (sfcs 1976) , pp. 173–\n177. IEEE Computer Society, 1976.\nYarotsky, D. Optimal approximation of continuous func-\ntions by very deep relu networks. In Conference on learn-\ning theory , pp. 639–649. PMLR, 2018.\nZeighami, S. and Shahabi, C. On distribution depen-\ndent sub-logarithmic query time of learned indexing.\nInProceedings of the 40th International Conference\non Machine Learning , Proceedings of Machine Learn-\ning Research, pp. 40669–40680. PMLR, 23–29 Jul\n2023. URL https://proceedings.mlr.press/\nv202/zeighami23a.html .\nZeighami, S. and Shahabi, C. Towards establishing guaran-\nteed error for learned database operations. In The Twelfth\nInternational Conference on Learning Representations ,\n2024.\nZeighami, S., Shahabi, C., and Sharan, V . Neurosketch: Fast\nand approximate evaluation of range aggregate queries\nwith neural networks. Proceedings of the ACM on Man-\nagement of Data , 1(1):1–26, 2023.\nA. Related Work\nA large and growing body of work has focused on using\nmachine learning to speed up database operations, among\nthem, learned indexing (Galakatos et al., 2019; Kraska et al.,\n2018; Ferragina & Vinciguerra, 2020; Ding et al., 2020),\nlearned cardinality estimation (Kipf et al., 2018; Wu &\nCong, 2021; Hu et al., 2022; Yang et al., 2019; 2020; Lu\net al., 2021; Negi et al., 2021) and learned sorting (Kristo\net al., 2020). Most existing work focus on improving mod-\neling choices, with various modeling choices such as neural\nnetworks (Zeighami et al., 2023; Kipf et al., 2018; Kraska\net al., 2018), piece-wise linear approximation (Ferragina &\nVinciguerra, 2020), sum-product networks (Hilprecht et al.,\n2019) and density estimators (Ma & Triantafillou, 2019).\nExisting results show significant empirical benefits in static\ndatasets, while performance often deteriorates in dynamic\ndatasets and in the presence of distribution shift (Wang et al.,\n2021; Wongkham et al., 2022). Our theoretical results help\nexplain such observations and provide a theoretical frame-\nwork for analysis of the operations under different modeling\nchoices.\n11\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nOn the theory side, no existing study meaningfully char-\nacterizes performance of learned models in the dynamic\nsetting or studies learned sorting. In the static setting,\n(Zeighami & Shahabi, 2023; Ferragina et al., 2020) study\nquery time of learned indexing. Ferragina et al. (2020)\nshows learned models can provide constant factor improve-\nments under an assumption on the distribution of the gap\nbetween observations, and Zeighami & Shahabi (2023)\nshows a learned model can answer queries in O(log log n)\nquery time if the p.d.f of data distribution is non-zero and\nbounded. Our results strictly generalize the latter to the dy-\nnamic setting, in the presence of insertions from a possibly\nchanging distribution, and also show that more generally,\nO(TX\nnlog log n+ log δn)query time is possible for any\ndistribution learnable class X. Moreover, Zeighami et al.\n(2023) presents a special case of our Theoerem 4.3 for car-\ndinality estimation on static datasets for distributions where\nthe operation distribution function is Lipschitz continuous.\nOur result strictly generalizes Zeighami et al. (2023) to the\ndynamic setting with distribution change and any distribu-\ntion learnable class X. Orthogonal to our work, Zeighami\n& Shahabi (2024) study lower bounds on the model size\nneeded to perform various database operations with a de-\nsired accuracy and (Hu et al., 2022; Agarwala et al., 2021)\nstudy the number of training samples needed to achieve a\ndesired accuracy for different database operations.\nFinally, we draw a broader connection between our work,\nlearned indexing and interpolation search. A large body of\nearly work focused on interpolation search Peterson (1957);\nPerl et al. (1978); Perl & Reingold (1977); Yao & Yao\n(1976); Mehlhorn & Tsakalidis (1993), proposed by (Pe-\nterson, 1957) which uses linear interpolation to estimate\nthe location of a query in an array. It has been shown that\nthis search algorithm achieves O(log log n)query time on\nuniformly distributed arrays (Yao & Yao, 1976; Perl & Rein-\ngold, 1977), with extensions to cover smooth distribution\nclasses and dynamic data in (Andersson & Mattsson, 1993;\nMehlhorn & Tsakalidis, 1993). Indeed, interpolation search\ncan be seen as an early example of a model-based search,\nwhere linear models are used to estimate item locations. Us-\ning the terminology introduced in this paper and given that\nuniform distribution is distribution learnable using linear\nmodels (c.d.f of the uniform distribution is a linear function),\ntheO(log log n)query time can be seen as a special case\nof our results. Overall, interpolation search can be seen as\na special case of learned indexing, where learned indexing\nallows for more complex data-driven modeling choices that\ncan be useful for a broader class of data distributions.\nB. Formalized Setup and Operations\nWe are interested in performing database operations on a\npossibly changing dataset. We assume data records are d-dimensional points in the range [0,1](otherwise, the data\ndomain can be scaled and shifted to this range). We either\nconsider the setting when ndata points are inserted one by\none into the dataset, or that we are given a fixed set of n\ndata points. We refer to the former as the dynamic setting\nand the latter as the static setting. We define Dias the\ndataset Di∈[0,1]i×d, i.e., a dataset consisting of irecords\ninserted so far and in ddimensions with each attribute in the\nrange [0, 1], where ianddare integers greater than or equal\nto 1.Dnis the dataset after the last insertion, and is often\ndenoted as D.Di:jdenotes the dataset of points inserted\nafter the i-th insertion until the j-th (i.e., Dj\\Di). We\nuseDito refer to the i-th record of a dataset (which is a d-\ndimensional vector) and Di,jto refer to the j-th element of\nDi. Ifd= 1(i.e.,Dis 1-dimensional), then Diis the i-th\nelement of D(and is not a vector). We study the following\ndatabase operations.\nIndexing . The goal is to use an index to store and find items\nin a1-dimensional dataset. The index supports insertions\nand queries. nitems are inserted into the index one by one.\nAfter inserting kitems, for any 1≤k≤n, we would like to\nretrieve items from the dataset based on a query q∈[0,1].\nThe query is either an exact match query or a range query.\nAn exact match query returns the point in the database that\nexactly matches the query q(orNULL if there is none) while\na range query [q, q′]returns all the elements in the dataset\nthat fall in the range [q, q′], forq, q′∈[0,1].\nCardinality Estimation . Used often for query optimiza-\ntion, the goal is to find how many records in the dataset\nmatch a range query, where the query specifies lower and\nupper bound conditions on the values of each attribute.\nSpecifically, the query predicate q= (c1, ..., c d, r1, ..., r d),\nspecifics the condition that the i-th attribute is in the interval\n[ci, ci+ri], forci, ri∈[0,1]. Data records can be inserted\ninto the data set one by one. After the insertion of k-th item,\nfor any 1≤k≤n, we would like to obtain an estimate\nof the cardinality of query q. We expect that the answers\nare within error ϵof the true answers. That is, if c(q)is\nthe true cardinality of qandˆc(q)is an estimate, we expect\n|c(q)−ˆc(q)| ≤ϵ. This guarantee has to hold throughout,\nand as new elements are inserted in the dataset.\nSorting . The goal is to sort a fixed array of size n. That is,\nwe are given a one-dimensional array, D, and the goal is to\nreturn an array, D′, which has the same elements as Dbut\nordered so that D′\ni≤D′\ni+1. Unlike indexing and cardinality\nestimation, sorting assumes a fixed given array that needs to\nbe sorted. Although indexing can often be used to sort an\narray (e.g., inserting elements one by one into a binary tree\nsorts a fixed array), we study the problem of sorting more\nbroadly and explore other learned solutions to the problem\nbeyond indexing (e.g., analogous to how merge sort can also\nbe used to sort an array).\n12\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nC. Distribution Learnability Through\nFunction Approximation\nWe first formalize representation power and optimizablity,\nand then present a formal statement for Theorem 3.5.\nRepresentation Power . Consider using a function class Fto\napproximate another function class G(e.g., neural networks\nto approximate real-valued functions). Consider some hy-\nperparameter, ϑ, that controls the representation power and\ninference complexity in F, and denote by Fϑis the subset\nofFwith hyperparameter ϑ. For instance, ϑcan be the\nnumber of learnable parameters of a neural network, the\nmaximum degree of a polynomial, or the number of pieces\nin a piecewise approximation. In all such cases, larger ϑim-\nplies better representation power but also higher inference\ntime and/or space complexity. Assume we have access to a\nrepresentation complexity function αF→G(ϵ), that given a\nmaximum error ϵreturns the smallest value of ϑsuch that\nfor any g∈ Gthere exists an f∈ Fϑwith∥f−g∥∞. The\nfunction αF→G(ϵ)determines the required model complex-\nity ofF, in terms of ϑ, to represent all elements of Gwith\nerror at most ϵ. For instance, such a function for neural\nnetworks approximating real-valued functions will show the\nminimum number of neural network parameters needed to\napproximate all real-valued functions to error at most ϵwith\na neural network. We say that a function class, Fhas the\nrepresentation power to model Gif there exists a representa-\ntion complexity function αF→G(ϵ)for all ϵ >0. Finally, let\nτF→G(ϵ)andσF→G(ϵ)respectively be the maximum time\nand space complexity of performing a model forward pass\nfor functions in Fϑforϑ=αF→G(ϵ).\nOptimizability . We say a function class Fis optimizable\nwith an algorithm Aif given any function hand a hyperpa-\nrameter value ϑ,A(h, ϑ)returns an approximately optimal\nrepresentation of hinFθ. Formally, for ˆh=A(h, ϑ)and if\nh∗= arg min ˆh∈Fθ∥h−ˆh∥,∥h∗−h∥∞≥κ∥ˆh−h∥∞for a\nconstant κ≤1. Letβ(ϑ)be the maximum time complexity\nofA.\nWe note that although optimizability as defined broadly\nabove is sufficient to show distribution learnability, it is\nnot necessary. Here, we discuss two qualifications to the\ndefinition that make proving optimizability simpler, specifi-\ncally for database operations. First, it is only necessary to\nhave optimizability for h∈fDfor all possible Dand for\na desired operation function f(since we will only use A\nto model operation functions). This can simplify the opti-\nmizability requirement depending on the operation function\nconsidered. For example, when showing opimizability for\nrank operations, we only need an Athat returns approxi-\nmately optimal estimates for input functions that are non-\ndecreasing (since all rank functions are non-decreasing).\nSecond, when Ais used on h=fDn, we can allow ad-ditive error of O(1√n). That is, we only need to show\n1\nκ∥h∗−h∥∞+κ′\n√n≥ ∥ˆh−h∥∞forκ′≥0andκ≤1\nuniversal constants.\nTheorem C.1. Assume a function class, F, is optimizable\nwith an algorithm A, and that Fhas enough representation\npower to represent G. LetXbe a distribution class with\nfχ∈ G for all χ∈X. Then, Xis distribution learnable\nwithTX\nn=τF→G(1√n),SX\nn=σF→G(1√n), andBX\nn=\nβ(αF→G(1√n)).\nD. Proofs\nThe high-level idea behind most of our theoretical results\nis to use the relationship between query answers and distri-\nbution properties. Overall, many statistical tools have been\ndeveloped that relate the properties of an observed dataset to\nthe data distribution (e.g., studying the relationship between\nsample mean and distribution mean). In statistics, such tools\nhave been used to describe the population using observed\nsamples. Our proofs often use such tools to do the opposite,\nthat is, use the properties of the data distribution to de-\nscribe observed samples. Indeed, that is the intuition behind\nlearned database operations, that if the data distribution can\nbe efficiently modeled, then it can be used to answer queries\nabout the observed samples (i.e., the database) efficiently.\nOur proposed distribution learnability framework allows us\nto state this more formally. It allows us to assume that we\ncan indeed model the data distribution efficiently. Then, the\nanalysis can focus on utilizing statistical tools to character-\nize the relationship between the observed sample and the\ndata distribution. Having access to an accurate model of the\ndata distribution, we use existing statistical tools to analyze\nits error. However, a main challenge in the case of learned\ndatabase operations is to balance accuracy and efficiency.\nThus, our theoretical study includes designing data struc-\ntures and algorithms that can utilize modeling capacities\nwhile performing operations as efficiently as possible.\nD.1. Proof of Theorem 3.1\nWe would like to bound ED∼χ[∥ˆf−fDj∥]. Note that both\nˆfandfDjare random variable (since ˆfdepends on fDi).\nFirst, consider\nfDj(q) =1\njX\nk∈[j]IDk∈q=i\njfDi(q) +j−i\njfDi:j(q),\nSo that the error is\nED∼χ[∥ˆf−i\njfDi(q) +j−i\njfDi:j(q)∥]\n=EDi∼χ[EDi:j∼χ[∥ˆf−i\njfDi(q)−j−i\njfDi:j(q)∥|Di]].\nNow consider EDi:j∼χ[∥ˆf−i\njfDi(q)−j−i\njfDi:j(q)∥|Di].\n13\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nGiven Di,ˆf−i\njfDi(q)is a fixed quantity. Furthermore,\nrecall that arg min cE[|X−c|] =Med(X)for any random\nvariable X, where Med(X)is a median of X(Wasan, 1970).\nTherefore, for any query,\nEDi:j∼χ[∥ˆf−i\njfDi(q)−j−i\njfDi:j(q)∥|Di]\n≥j−i\njEDi:j∼χ[Med(fDi:j(q))−fDi:j(q)].\nObserve that fDi:j(q)∼Binomial (j−i,Pp∼χ(Ip∈q))\nand consider any query such that (j−i)Pp∼χ(Ip∈q)is an\ninteger, which exists as long as the c.d.f of the distribu-\ntion is continuous. For such queries, we have Med(X) =\n(j−i)Pp∼χ(Ip∈q)since mean and median of binomial dis-\ntributions where (j−i)Pp∼χ(Ip∈q)is an integer are equal\n(Janson, 2021). Let pq=Pp∼χ(Ip∈q). Using the bound\non the binomial mean absolute deviation in Berend & Kon-\ntorovich (2013), we have, when j−i≥2and for any query\ns.t.1\n(j−i)≤pq≤1−1\nj−i,\np\n(j−i)pq(1−pq)√\n2≤EDi:j∼χ[|(j−i)pq−fDi:j(q)|].\nMoreover, setting pq=⌊j−i\n2⌋\nj−i, we have\np\n(j−i)pq(1−pq)√\n2≥√j−i\n4.\nD.2. Proof of Theorem 3.5 (formally Theorem C.1)\nFirst, we use the algorithm A(due to optimizability)\nto construct the algorithm in definition 3.2 as ˆf=\n1\nnA(fD, αF→G(1√n))forf∈ {r, c}given an input dataset,\nD, of size n. Letϑ=αF→G(1\nϵ).\nNote that since Fhas enough representation power to repre-\nsentG, and since by assumption {χ∈X, fχ} ⊆ G , we have\nthat, for any χthere exists ˆfχ∈ Fϑs.t.∥ˆfχ−fχ∥∞≤1√n.\nFurthermore, since we approximately optimally find ˆf, we\nhave|ˆf(x)−1\nnfD(x)| ≤1\nκ|ˆfχ(x)−1\nnfD(x)|+κ′\n√n. Now,\nto analyze accuracy of ˆf, observe that, for any input xwe\nhave\n|ˆf(x)−1\nnfD(x)| ≤1\nκ|ˆfχ(x)−1\nnfD(x)|+κ′\n√n\n≤1\nκ|ˆfχ(x)−fχ(x)|+\n1\nκ|fχ(x)−1\nnfD(x)|+κ′\n√n\n≤1\nκ√n+1\nκ|fχ(x)−1\nnfD(x)|+κ′\n√n.We also have\n|ˆf(x)−fχ(x)| ≤ |ˆf(x)−1\nnfD(x)|+|fχ(x)−1\nnfD(x)|,\nSo that ,\nn|ˆf(x)−fχ(x)| ≤√n\nκ+2\nκ|nfχ(x)−fD(x)|+√nκ′.\nBy Hoeffeding’s inequality, we have\nP(|nfχ(x)−fD(x)| ≥ϵ′)≤e−2(ϵ′\n√n)2\n, (1)\nSo that\nP(n|fχ(x)−ˆf(x)| ≥2\nκϵ′+√n(1\nκ+κ′))≤e−2(ϵ′\n√n)2\n,\n(2)\nAnd therefore, for some universal constant κ2andϵ=\nΩ(√n),\nP(n|fχ(x)−ˆf(x)| ≥ϵ)≤e−κ2(ϵ√n−1)2\n. (3)\nD.3. Proof of Lemma 3.6\nFor each distribution class, we show optimizability and rep-\nresentation power of some function class Fthat can be used\nto model the distribution class, which combined with Theo-\nrem C.1 shows the desired result for both Lemmas 3.6 and\n3.7. Then, for each class, we discuss modeling complexities.\nDistribution learnability for Xρ. LetFbe the class of\npiecewise constant functions with uniformly spaced pieces\nand let Gbe the class of real-valued differentiable functions\n[0,1]d→Rwith gradient bounded by ρ. Consider the num-\nber of pieces to use for approximation as a hyperparameter.\nOptimizability . Given the number of pieces, the function\nthat creates the minimum infinity norm is to place a con-\nstant at the mid-point of maximum and minimum values\nin each interval. That is, for an interval I⊆[0,1]d, the\nconstant approximating gover with the lowest infinity norm\nIis1\n2(min x∈Ig(x) + max x∈If(x))). Note that this func-\ntion has error at most max x∈Ig(x)−1\n2(min x∈Ig(x) +\nmax x∈If(x))) =1\n2(max x∈If(x)−minx∈Ig(x)). For\nefficiency purposes, instead of the optimal solution, we\nlet the constant for the piece responsible for Ibeg(p)for\nsome p∈I. Note that for all x∈I|g(p)−g(x)| ≤\n|max x∈Ig(x)−minx∈Ig(x)|, so that this construction\ngives us a1\n2-approximation of the optimal solution.\nRepresentation Power . Define αF→G(ϵ) =√\ndρ\nϵ. We show\nthat for any g∈ G and any ϵ > 0, there is a function\nˆf∈ Fα(ϵ)s.t.∥ˆf−g∥∞≤ϵ. This function is the optimal\nsolution as constructed above. To see why the error is at\nmost ϵ, consider a partition over Iwith j-th dimension\n[pj,i, pj,i+1], where pj,i+1−pj,i=ϵ\nρ, and let x1andx2be\nthe two points in Ithat, respectively, achieve the minimum\nand maximum of ginI. For any point in x∈I, our function\napproximator answer ˆf(x) =1\n2(g(x1) +g(x2)). We have\n14\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nthat\n|ˆf(x)−g(x)|=|1\n2(g(x1) +g(x2))−g(x)|\n≤max{g(x2)−g(x), g(x)−g(x1)}\n≤ ∥g′(x)∥2∥x2−x∥2\n≤ρ(√\ndϵ√\ndρ) =ϵ.\nModel Complexity . The inference time, TX\nn, is constant inde-\npendent of the number of pieces used. The space complexity\nis the number of pieces multiplied by the space to store each\nconstant. Given that gconsists of integers between 0 to n,\nSX\nncan be stored in O(√\nd(ρ√n)dlogn). Finally, for rank\noperation, the algorithm that outputs the function optimizer\nmakes ρ√ncalls to g, so that building the function approxi-\nmator can be done in O(ρ√nlogn), assuming the data is\nsorted (so that each call to gtakes O(logn)). For cardinality\nestimation there are O(√\nd(ρ√n)dlogn)calls to the cardi-\nnality function, where each call in the worst case takes O(n)\n(this can optimized by building high dimensional indexes).\nThus, in this case, BX\nn=O(√\nd(ρ√n)dlogn)\nDistribution learnability for Xl. LetFandGbe the class\nof piecewise linear functions with at most lpieces (not nec-\nessarily uniformly spaced pieces). Trivially, Fhas enough\nrepresentation power to represent G, thus, it remains to show\noptimizability and model complexity.\nOptimizability . The PLA algorithm, P(ϵ)by (O’Rourke,\n1981), used in PGM index (Ferragina & Vinciguerra, 2020),\nis able to find the piecewise linear solution with the smallest\nnumber of pieces given an error ϵ. Here, we want to achieve\nthe opposite, i.e., given a number of pieces find piecewise\nlinear approximation with smallest error. Note that ϵis in\nthe range 0to1, and we can do a binary search on the val-\nues of ϵ, for each calling P(ϵ)until we find the smallest ϵ\nwhere |P(ϵ)| ≤l. Note that since suboptimality of O(1√n)\ninϵis allowed, wee can discretize [0,1]to√ngroups, and\nonly do binary search over this discrete set, which takes\nO(log(√n))calls to P(ϵ), and each call takes O(n)oper-\nations (Ferragina & Vinciguerra, 2020) on a sorted array,\nso that Fis optimizable to with the algorithm running in\nO(nlogn)\nModel Complexity . The learning time nBX\nn=O(nlogn)\nis discussed above. The algorithm always returns lpieces\nwhich can be evaluated in TX\nn=O(logl)time. Note the\neach linear piece can be adjusted to cover an interval starting\nand ending at points in the dataset (so the interval can be\nstored as pointers to corresponding dataset item). Moreover,\nthe beginning and end of each line can be adjusted to be\nan integer (since the rank function only returns integers),\nsimilar to (Ferragina & Vinciguerra, 2020), so that the lines\ncan be stored in SX\nn=O(llogn).Algorithm 1 Dynamic Learned Index Query\nRequire: New element to be inserted in tree rooted at N\nEnsure: Balanced data structure\n1:procedure QUERY (q,N)\n2: ifNis a leaf node then\n3: return BINARY SEARCH (N.content )\n4: ˆi←N.ˆf(p)\n5: i←EXPSEARCH (p,ˆi, N.content )\n6: j←BINARY SEARCH (p, N.children [i])\n7: return QUERY (q,N.children [i][j])\nAlgorithm 2 Dynamic Learned Index Insertions\nRequire: New element, pto be inserted in tree rooted at N\nEnsure: Index with pinserted\n1:procedure INSERT (p,N)\n2: N.counter ++\n3: ifN.children isNULL then\n4: INSERT CONTENT (p,N.content )\n5: return\n6: ˆi←N.ˆf(p)\n7: i←EXPSEARCH (p,ˆi, N.content )\n8: j←BINARY SEARCH (p, N.children [i])\n9: INSERT (p,N.children [i][j])\n10: ifN.counter =N.maxpoints then\n11: A←the sorted array in the index rooted at N\n12: ifNhas no parent then\n13: return REBUILD (A)\n14: P←parent of N\n15: ip←index of NinP.children\n16: Remove Nfrom P.children [ip]\n17: N1←REBUILD (A[:N.maxpoints /2])\n18: N2←REBUILD (A[N.maxpoints /2 :])\n19: Insert N1andN2inP.children [ip]\nDistribution Learnability for Xc. Trivially, the class F\ncontaining the distribution operation function has enough\napproximation power for Xcand is optimizable with\nSX\nn,TX\nn,BX\nnallO(1).\nD.4. Proofs for Learned Indexing\nD.4.1. I NDEX OPERATIONS\nThe index supports two operations, QUERY andINSERT ,\nwhich are presented in Algs. 1 and 2. Recall that Ais an\nalgorithm defined in Definition 3.2 and exists due to distri-\nbution learnability of X. The index builds a tree structure\nsimilar to (Ding et al., 2020), with each node containing\na model, and a set of children. An overview of the tree\narchitecture is shown in Fig. 1. Each node can be seen to\ncover a subarray of the original indexed array. If the size of\nthe covered subarray is kelements, then the node will have√\nkchildren, where the subarray of size kis equally divided\nbetween the children (so each child covers√\nkelements).\n15\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nAlgorithm 3 Procedure for for Rebuilding Root\nRequire: A sorted array, A\nEnsure: A learned index rooted at a new node N\n1:procedure REBUILD (A)\n2: N←new node\n3: k← |A|\n4: N.counter ←k\n5: ifk≤κthen\n6: N.content ←A\n7: return N\n8: N.maxpoints ←2k\n9: N.ˆf=A(A)\n10: N.content ←A[::√\nk]\n11: foriin√\nkdo\n12: Nc←REBUILD (A[ik: (i+ 1)k])\n13: N.children .append (Nc)\n14: return N\nle v elle v el 0le v el 1le v el 2arr a y inde x e dle af nodes ha v e c onstan t siz eS ub arr a y c o v er e d b y a node\nFigure 1. Structure of the learned dynamic index\nThe root node covers the entire nelements of the array, and\ntherefore has√nchildren. As can be seen, the number\nof children of the nodes decreases as we go down the tree.\nA node won’t have any children if its covered subarray is\nsmaller than some constant c. Moreover, shown as black\nelements in the figure, each parent node stores the mini-\nmum value of the subarray covered by each of its children\nin an array called the node’s content . Thus, the node’s\ncontent can be used to traverse the tree. When a node’s\nmodel predicts which child the node should travel, the node\nfirst checks with its content to make sure it is the correct\nnode. This is done by doing an exponential search on the\nnode’s content .\nDuring insertions, each node keeps a counter of the number\nof points inserted through it. If a node is at level i, then at\nmostk2−ielements are allowed in the node for i >0, where\nkis the size of the dataset at the time of construction of the\ncurrent root node (root node’s are periodically rebuilt). If the\nnumber of insertions reaches k2−i, the node splits. When\na node splits the subarray it covers is split into two, and an\nentirely new subtree is built for each half of the subarray,\nrebuilding all models. To avoid splits affecting parent nodes,\nas Fig. 2 shows, the newly created node is appended to the\nlist of children (we’ll discuss how exactly this is done later).\n( a ) bef or e inser tion(b ) af ter inser tionFigure 2. Insertion Causing a split in index\nFinally, the root is rebuilt every time its size doubles.\nTo support the splitting discussed above, the children are\narranged in a two dimensional array N.children . We\nrefer to children pointed to in N.children [i]as children\nin the i-th child slot. Each child slot contains a sorted list of\nat least one, but a variable number, of children, where the\nlist is kept sorted using binary trees. To find which node to\ntraverse, we first find the correct node slot with the help of\nthe learned model, and then use the binary tree in the node\nslot to find the correct child. Leaf nodes have content\nstoring the data. The index keeps a counter at each node,\nand periodically rebuilds the tree rooted at a node. Thus, the\ntwo operations are performed as follows.\nQuery . Performing queries on an index with root node Nis\nsimilar to performing queries with a B-tree, where nodes are\nrecursively traversed until reaching a leaf node. The only\ndifference is how we decide which child to search. This\nis done by, for each non-leaf node, first asking a model to\nestimate which child to search. Then, the model estimate is\ncorrected by performing a local exponential search. Since a\nchild might have been split, the search then uses the binary\ntree at the correct node slot to find the correct child.\nInsertions . Insertions first traverses the tree similar to the\nqueries, with the extra addition that a counter in each node\nis incremented if a new element is inserted in that node. If\nthe counter of a node passes the maximum size of the node,\nthe node is split into two, and the parent meta data for the\ncorresponding node slot is updated. The only exception is\nthe root node, which does not split, but triggers a full rebuild\nof the entire tree.\nD.4.2. Q UERY TIME\nQueries are performed by recursively searching each node,\nwhere a single node per level is queried. Consider the num-\nber of operations performed at the i-th level. Each level per-\nforms a model inference, exponential search on the node’s\ncontent and a binary search on the node’s extension. We\nconsider each separately.\nModel Inference . A model at the i-th level is built on at\nmost n1\n2ielements. Thus, the inference time is O(TX\nn29i).\nExponential Search . The time complexity of the expo-\nnential search step depends on the accuracy of the model\nestimate. We show that, on expectation, the time complexity\n16\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nis constant.\nNote that neither the content of each node, nor its model\ngets modified by insertions, unless a node is rebuilt. Thus,\nwe only need to show the statement for right after the node\nconstruction. Assume the node is constructed for some array\nAwith|A|=kfor some integer k.\nAssume the i-th element of Ais a sample originally obtained\nfrom the distribution χi, letR= [l, u]define the range the\nelements Ahave fallen into passed to the algorithm. The\nelements of Aare independent samples from conditional\ndistributions χ1|R, ..., χ k|R, respectively. Let χR=\n{χ1|R, ..., χ k|R}.\nApplying the extension of the DKW bound to independent\nbut non-identical random variables (Shorack & Wellner,\n1986, Chapter 25.1) we have that\nP(∥krχR−rA∥∞≥r\nk\n2ϵ)≤2e−ϵ2+1. (4)\nFurthermore, for ˆr, the model of rχR=1\nkPk\ni=1rχi|R,\nobtained from Aand by the accuracy requirement of distri-\nbution learnability, we have for any ϵ≥√κ2(1√\nk−1)\nP(∥krχR−ˆr∥∞≥√\nk(ϵ√κ2+ 1))≤κ1e−ϵ2.(5)\nBy union bound on Ineq. 4 and 5, and the triangle inequality,\nfor any ϵ≥√\nk, we have\nP(∥ˆr−rA∥∞≥ϵ)≤κ′\n1e−κ′\n2(ϵ√\nk−1)2\n, (6)\nwhereκ′\n1= 2e+κ1andκ′\n2=2κ2\n(√\n2+√κ2)2.\nNow let N(q)be the number of operations by performed\nby exponential search for a query q. Recall that the content\nof the node is Ac=A[::√\nk]and that we use ˆrAc=\n⌈1√\nkˆr(q)⌉as the start location to start searching for qinAc\nwith exponential search. We have the true location of qin\nAcisrAc(q) =⌈1√\nkrA(q)⌉. Observe that, for any q\n∥rA−ˆr∥∞>(ˆrAc(q)−rAc(q)| −2)√\nk, (7)\nand that it is easy to see that for exponential search we have\n|ˆrAc(q)−rAc(q)| ≥2N(q)\n2−1, (8)\nCombining which we get, for any query q,\n∥rA−ˆr∥∞>(2N(q)\n2−1−2)√\nk.\nSo that, for any i,N(q)≥iimplies that ∥rA−ˆr∥∞>(|2i\n2−1−2)√\nkThus, we have\nEA∼χ[N(q)] =2 logkX\ni=1PA∼χ(N(q)≥i)\n≤2 logkX\ni=1PA∼χ(∥rA−ˆr∥∞>(|2i\n2−1−2)√\nk)\n≤5 +κ′\n12 logkX\ni=6e−κ′\n2(2i\n2−1−3)2=O(1)\nThus, the expected time searching performing exponential\nsearch is O(1).\nSearching Node’s Extension . Next, we study the expected\ntime for searching the additional list added to the nodes. Let\nLbe the size of the list. Note that the lists are created for\nall the nodes except the root. Moreover, a non-root node at\nlevelihas capacity cn1\n2i, it will split every cn1\n2iinsertions\ninto the parent, with each split adding an element to the\nparent’s extension list. Furthermore, the node gets rebuilt\nafter every k=n1\n2i−1insertions into its parent. Thus, if kN\nelements out of n1\n2i−1get inserted into a node slot N, the\nnumber of splits for that slot will be ⌊kN\ncn1\n2i⌋.\nAssume the knew insertions into the parent, NpofN\nsince the last rebuild of the parent were from r.v.s with\ndistribution χ1, ..., χ k. Given that they fall in Np, their\nconditional distribution is χ1|R, ..., χ k|RforRdefin-\ning an interval for which the node NPwas built. Let\nχ′\nR={χ1|R, ..., χ k|R}. Furthermore, let χRbe the\noriginal distribution the model of in the parent node was\ncreated based on. Let A′be the set of kinsertions and let\nAbe the set of points based on which the parent of Nwas\nbuilt. The number of insertions out of the knew insertions\ninto the j-th node slot is rA′(Nj)−rA′(Nj−1). To study\nthis quantity, recall that the j-th slot was created so that\nrA(Nj)−rA(Nj−1) =n1\n2i. (9)\nThus, we first relate rAandr′\nA.\nThe elements of AandA′are independent so that apply-\ning the extension of the DKW bound to independent but\nnon-identical random variables (Shorack & Wellner, 1986,\nChapter 25.1) for both AandA′we have\nP(∥krχ′\nR−rA′∥∞≥r\nk\n2ϵ)≤2e−ϵ2+1, (10)\nP(∥krχR−rA∥∞≥r\nk\n2ϵ)≤2e−ϵ2+1. (11)\nMoreover, assume TV( χi|R, χ j|R)≤δfor all i, j, we\nhave that ∥rχ−rχ′∥∞≤δ. Combining this with Ineq. 10\n17\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nand 11 and using the triangle inequality, we have\nP(∥rA′−rA∥∞≥√\n2kϵ+δk)≤4e−ϵ2+1. (12)\nFinally, combining Ineq. 12 with Eq. 9, and recalling that\nk=n2−i+1implies that\nP(rA′(Nj)−rA′(Nj−1)≥n2−i(1 + 2√\n2ϵ) + 2δn2−i+1)\n≤4e−ϵ2+1.\nAnd therefore\nP(⌊rA′(Nj)−rA′(Nj−1)\nn2−i ⌋ ≥2√\n2ϵ+ 2δn2−i)≤4e−ϵ2+1,\nWhere ⌊rA′(Nj)−rA′(Nj−1)\nn2−i ⌋is the number of splits of the\nj-th node slot. Let Sjdenote this random variable. We have\nE[Sj] =kX\ni=0P(Sj≥i)\n≤2δn2−i+k−2δn2−i\nX\ni=0P(Sj≥i+ 2δn2−i)\n≤2δn2−i+k−2δn2−i\nX\ni=04e−i2\n8+1=O(δn2−i)\nFinally, we are interested in E[log(Sj)]≤log(E[Sj]) =\nO(log(n1\n2iδ)).\nTotal Query Time . Thus, the expected time to search\na node at the i-th level to find its children is O(TX\nn29i+\nlog(n−2iδ)). Thus, the total time to search the tree is\nO(Plog log n\ni=1TX\nn29i+ log( n−2iδi)). We can bound this as\nO(TX\nnlog log n+ log( n¯δ)), where ¯δ= min {δ, δlog log n\nc }.\nD.4.3. I NSERTION TIME\nNote that insertion time is equal to query time plus the total\ncost of rebuilds. Next, we calculate the cost of rebuilds.\nLetT(N)be the amortized cost of inserting Nelements\ninto a tree that currently has Nelements and was just rebuilt\nat its root, so that NT(N)will be the total insertion cost\nfor the Nelements. Note that the amortized cost of all n\ninsertions starting from a tree with one 1 element is at most\n1\nnPlogn\ni=0n\n2iT(n\n2i)≤T(n)P1\n2i=O(T(n)).\nThus, we only need to study T(n). Note that when inserting\nnelements into a tree that currently has nelements and was\njust rebuilt at its root, the height of the tree remains constant\nthroughout insertions. Furthermore, at the i-th level, i≥0,\nthere will be at mostn\nn2−irebuilds and each rebuild costs\nlog log nX\nj=i2n2−i\nn2−jBX\n2n29j.Thus, we have the amortized cost of all rebuilds is\n1\nnlog log nX\ni=0n\nn2−ilog log nX\nj=i2n2−i\nn2−jBX\n2n29j=\nO(log log nX\ni=0(i+ 1)BX\n2n29i\nn2−i).\nThus, the total cost of insertions is O(TX\nnlog log n+\nlog(¯δn) +BX\nn\nnlog2logn)).\nD.4.4. S PACE OVERHEAD\nAfter ninsertions, we will have log log nlevels. Right after\nthe root was rebuilt, level ihas at mostn\nn2−imodels. If\nnfurther insertions are performed, each level will have at\nmostn\nn2−inew models. Thus, the total size of the models\nat level iis at most 2n\nn2−iSX\nn29i, and thus the total size of\nall models is O(nPlog log n\ni=0SX\nn29i\nn2−i). Furthermore, the total\nnumber of nodes in the tree is O(n), and for each node we\nstore a pointer to it and its lower and upper bounds, as well\nas a counter, which can be done in O(logn). Thus, the total\nspace consumption is O(n(logn+Plog log n\ni=0SX\nn29i\nn2−i)).\nD.5. Proof of Corollary 4.2\nObserve that for any distribution in X, we have that¯χ|R\nfor any interval Rhas p.d.f at mostρ2\nρ1. According to\nLemma 3.6, distributions with p.d.f at mostρ2\nρ1are dis-\ntribution learnable. Substituting the complexities proves\nCorollary.\nD.6. Proofs for Cardinality Estimation\nD.6.1. H IGHDIMENSIONS (THEOREM 4.3)\nConstruction . By distribution learnablility we have an\nalgorithm Athat builds a model that we use for estimation.\nTo prove the lemma, we use Aand periodically rebuild\nmodels to answer queries. Specifically, Ais called every\nkinsertions, where if δ≥2κ√n,k=ϕ−κ\n2κδ√n, and when\nδ≤2κ√n,k=n×min{(ϕ−κ\nκ(1+2κ))2,1}. That is, if currently\nthere are npoints inserted, and we insert n′new points,\nforn′< k, the algorithm will answer queries as (n+n′)ˆc.\nHowever, when n′=k, the algorithm rebuilds the model\nand starts answering queries using the new model.\nQuery Time and Space Consumption . We use a single\nmodel with no additional data structure, so query time is\nO(TX\nn)and space complexity is O(SX\nn).\nInsertion Complexity . To analyze the cost of insertions,\nfirst consider, T(n), the total number of operations when\nwe insert nnew elements in a data structure that already\nhasnelements. Consider the two cases where δ≥2κ√n\n18\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nandδ≤2κ√\n2. In the first case, we have the we rebuild the\nmodel everyϕ−κ\n2κδ√ninsertions, so that there are at most\nn\nϕ−κ\n2κδ√n=2κδ\nϕ−κ√nrebuilds. If δ≥2κ√\n2, we rebuild the\nmodel every ρntimes, so that the total number of rebuilds\nis1\nρ. Ensuring that ϕ≥κ+ 1, we have that1\nρ≤κ2(2κ+\n1)2. In either case, each rebuild costs O(BX\n2n)and besides\nrebuilds insertions takes constant time. Thus, if δ≥2κ√n,\nT(n) =O(n+δ\nϕ√nBX\n2n)and if δ≤2κ√\n2we have T(n) =\nO(n+BX\n2n).\nNext, to analyze the total runtime of starting from\n0 elements and inserting nnew elements, we have\nthat the amortized insertion is1\nnPlogn\ni=1T(n\n2i). Now\nifδ≥2κ√n, this is O(1\nnPlogn\ni=1n\n2i+δ\nϕpn\n2iBX\n2n\n2i) =\nO(1\nnPlogn\ni=1δ\nϕ√\nn2iBX\n2n\n2i) =O(n+δ\nϕ√nBX\n2n). Furthermore,\nifδ≤2κ√n, this is O(n+BX\n2n). Thus, the amortized insertion\ncost is O(max{δ\nϕ√n,1\nn}BX\nn)\nAccuracy . We show that if δ≥2κ√n, rebuilding the model\neveryϕ−κ\n2κδ√n, and when δ≤2κ√nrebuilding the model\nevery ρnforρ= min {(ϕ−κ\nκ(1+2κ))2,1}insertions is suffi-\ncient to answer queries with error at most ϕ√n, whenever\nϕ≥κ+ 1.\nAssume a model was built using dataset Di, i.e., after i\ninsertions. We study the error in answering after knew\ninsertions, so that the goal is to answer queries on Dj,\nj=i+k. Letχandχ′be the distributions so that Di∼χ\nandDi:j∼χ′.\nConsider a model, ˆcthat was built on DiusingA, so we\nhave\nP(i∥ˆc−cχ∥∞≥√\ni(ϵ√κ2+ 1))≤κ1e−ϵ2(13)\nWe are interested\n∥jˆc−cDj∥=∥iˆc+kˆc−cDi−cDi:j∥\n≤ ∥iˆc−cDi∥+∥kˆc−cDi:j∥. (14)\nFor the first term, by Hoeffding’s inequality we have\nP(|icχ(q)−cDi(q)| ≥√\niϵ)≤e−2ϵ2, (15)\nWhich combined with Ineq. 13 gives\nP\u0010\n|iˆc(q)−cDi(q)| ≥√\ni((1 +1√κ2)ϵ+ 1)\u0011\n(16)\n≤(1 +κ1)e−2ϵ2.\nFor the second term, again by Hoeffding’s inequality we\nhave\nP(|kcχ(q)−cDi:j(q)| ≥√\nkϵ)≤e−2ϵ2, (17)\nWe also have that ∥χ−χ′∥ ≤ δ, which combined withIneq. 17 gives\nP(|kcχ(q)−cDi:j(q)| ≥√\nkϵ+kδ)≤e−2ϵ2,\nAnd therefore, using Ineq. 13, we have\nP\u0010\n|kˆc(q)−cDi:j(q)| ≥√\nkϵ+k√\ni(ϵ√κ2+ 1) + kδ\u0011\n≤(κ1+ 1)e−2ϵ2. (18)\nCombining Ineq. 14, 16 and 18, we have\nP\u0010\n|jˆc(q)−cDj(q)| ≥(√\ni+√\ni\nκ2+√\nk+k√iκ2)ϵ+\nk√iκ2+√\ni+kδ\u0011\n≤2(κ1+ 1)e−2ϵ2\nAs such, we have E[|jˆc(q)−cDj(q)|−(k√\niκ2+√\ni+kδ)\n√\ni+√\ni\nκ2+√\nk+k√\niκ2]≤\nκ3, for some universal constant κ3so that, E[|jˆc(q)−\ncDj(q)|]≤k√iκ2+√\ni+kδ+κ3√\ni+κ3√\ni\nκ2+κ3√\nk+κ3k√iκ2.\nAssuming k≤i, we have\nE[|jˆc(q)−cDj(q)|]≤κ(√\ni+√\nk+kδ),\nFor some universal constant κ.\nNow if δ≥2κ√\niwe let k=ϕ−κ\n2κδ√\niand otherwise set k=ρi\nforρ= min {(ϕ−κ\nκ(1+2κ))2,1}.\nFirst, consider the case where δ≥1√j. Consider the error\nϵ=ϕ√j≥ϕ√\ni, so it suffices to show that the error is at\nmost ϕ√\ni. Let k=ϕ−κ\n2κδ√\ni. Substituting this in, we want\nto showq\nκ(ϕ−κ)\n2δ√\ni−ϕ−κ\n2√\ni≤0. Indeed, for δ≥2κ√\ni,\nwe haveκ(ϕ−κ)\n2δ√\ni≤(ϕ−κ)\n4i, so that\nr\nκ(ϕ−κ)\n2δ√\ni−ϕ−κ\n2√\ni≤1\n2p\n(ϕ−κ)i−ϕ−κ\n2√\ni\n=1\n2p\n(ϕ−κ)i(1−p\nϕ−κ)\n≤0\nWhich proves E[|jˆc(q)−cDj(q)|]≤ϕ√jwhenever δ≥\n2κ√\niandϕ−κ≥1.\nIfδ≤2κ√\ni, we set k=ρiforρ= min {(ϕ−κ\nκ(1+2κ))2,1}. We\nhave\nκ(√\ni+p\nρi+ρiδ)≤κ√\ni(1 +√ρ+ 2√ρκ)\nSo that we need to ensure 1 +√ρ(1 + 2κ)≤ϕ\nκ.Observe\nthat√ρ≤ϕ−κ\nκ(1+2κ)implies the above, so that setting ρas\nabove proves the result in this case.\n19\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nD.6.2. O NEDIMENSION (THEOREM 4.4)\nConstruction . We build a B-tree like data structure. How-\never, in addition to the content of each node, each node also\nkeeps a counter of the number of elements inserted into the\nnode. After each insertion, if a leaf node has more than k\nelements, the node is split into two, for k=ϵ2\n4(κ+1)2. Thus,\nleaf nodes cover betweenk\n2tokelements, while the rest of\nthe tree has its own fanout B. Leaf nodes do not store the\nelements associated with them, but build models to answer\nqueries. To answer a query, the tree is traversed similar to\ntypical range query answering with a B-tree. However, if a\nnode is fully covered in a range, then the number of inser-\ntions in the node is used to answer queries. Otherwise, the\nnode is recursively searched until reaching a leaf node. We\nwill have at most 2leaf nodes reached that will be partially\ncovered by the query range. Finally, the model of each node\nare constructed by using the construction in Theorem 4.3\nwithϕ=κ+ 1.\nQuery Time and Space Consumption . Each query will\ntakeO(logn+2TX\nk)where lognis due to the tree traversal\nand2TX\nkfor the two model inferences needed. Moreover,\nthe total space consumption is O(n\nkSX\nk+n\nklogn)\nInsertion Complexity . Each leaf node will start with\nk\n2elements and will be split whenever it reaches k\nelements. Insertion of thek\n2elements in a node\ncost O(k+ max {δ√\nk,1}BX\nk). Given ninsertions,\nwe have2n\nkinsertions ofk\n2elements in the nodes,\nso that the amortized cost of rebuilds is O(1\nnn\nk(k+\nmax{δ√\nk,1}BX\nk)) =O(max{δ√\nk,1\nk}BX\nk)). Furthermore,\ntraversing the tree nodes costs O(logn)per insertion, so that\namortized insertion cost is O(max{δ√\nk,1\nk}BX\nk) + log n) =\nO(max{δ\nϵ,1\nϵ2}BX\nϵ2) + log n).\nAccuracy . Setting ϕ=κ+ 1in Theorem 4.3 and having\nk≤ϵ2\n4(1+κ)2ensures that the expected error of the each\nmodel is at most ϕ√\nk=ϵ\n2. Since each query is answered\nby making two model calls, the total expected error for\nanswering queries is at most ϵas required.\nD.6.3. H IGH DIMENSION WITH ANY ACCURACY\nHere we also discuss how we can use models to answer\nqueries to arbitrary accuracy in high dimensions. We note\nthat, as we see here, building data structures to answer\nqueries is high dimension is difficult. We discuss this result\nonly in the static setting.\nLemma D.1. There exists a learned model that can an-\nswer cardinality estimation query with error up to ϵwith\nquery time O(TX\n(ϵ\n2d)2+ (4d2\nϵ2)dΠiki)and space complexity\nO(4d2n\nϵ2SX\n(ϵ\n2d)2+(4d2n\nϵ2)d), where kiis the cardinality of the\nquery in the i-th dimension.Algorithm 4 Cardinality Estimation with Grid\nRequire: Query q, dimension to refine, i, set of models,\nM, and set of partition points S\nEnsure: Estimate to cardinatliy of q\n1:procedure QUERY (q,i,S,M)\n2: ifi= 0then\n3: return use grid to answer q\n4: il←index of q[i][0]inS[i]\n5: iu←index of q[i][1]inS[i]\n6: ifiu=ilthen\n7: return M[i][il](q)\n8: qu←q\n9: qu[i][0]←S[iu]\n10: ql←q\n11: ql[i][1]←S[il+ 1]\n12: ifiu=il+ 1then\n13: return M[i][il](ql) +M[i][iu](qu)\n14: q[i][0]←S[il+ 1]\n15: q[i][1]←S[iu]\n16: return QUERY (q,i−1,S,M)+M[i][il](ql) +\nM[i][iu](qu)\nCardinality of a query in the i-th dimension is the number of\npoints the would query we only consider the i-th dimension.\nConstruction . Assume we would like to obtain accuracy ϵ.\nWe build a grid and materialize the exact result in each cell.\nThen, for queries, where part of a query partially overlaps a\ncell, we also build models to answer queries. Thus, a query\nis decomposed into parts that fully contain cell and parts\nthat don’t which are answered by models.\nSplit the i-th dimension into k=4d2n\nϵ2partitions, with each\npartition containing (ϵ\n2d)2points. Let S[i] ={si\n1, ..., si\nk}\nbe the partition points in the i-th dimension, that is, for\nalljwe have for P[i, j] ={p∈D, si\nj≤pi< si\nj+1},\n|P[i, j]|= (ϵ\n2d)2. Using theorem 4.3 to build a model for\neach set of points in P, we have that the expected error of\neach is O(ϵ\n2d). The models are stored in M, with M[i][j]\ndenoting the model corresponding to j-th partition in the\ni-th dimension. Now, to answer a query, we first decompose\nit into 2d+ 1queries. 2dof the queries are answered by\nmodels, which reduce the original query to one that matches\nall facets of the grid cells. Then, the grid cells are used to\nanswer the final query, and the answer is combined with the\nmodel estimates to find final query answer estimate.\nThis is presented in Alg. 4. The decomposition of the query\nis done by recursively moving the upper and lower facets of\nthe query hyperretangle in the d-th dimensions to aligh with\nthe grid cell. Thus, in the d-th dimensions, if the closest grid\npartition points, respectively larger and smaller than q[d][0]\nandq[d][1](the lower and upper bound of the query in d-th\ndimension) are siandsj, we decompose the query into three\nqueries: q1,q2andq3, all the same as qbutq1[d][1] = si,\n20\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nAlgorithm 5 Learned Sorting\nRequire: An array Aof length nto be sorted\nEnsure: The sorted array\n1:procedure SORT(A)\n2: ifn≤κthen\n3: return MERGE SORT(A)\n4: S←random sample of Aof size√n\n5: S←MERGE SORT(S)\n6: k←n1\n8\n7: ˆf← A(S)\n8: B←array of size k\n9: Bmin, Bmax←arrays tracking min/max B[i]∀i\n10: foriinndo\n11: B[⌊kˆf(A[i])\nn⌋].append (A[i])\n12: Update Bmin,Bmax for bucket ⌊kˆf(A[i])\nn⌋\n13: foriink−2do\n14: ifBmax[i]> B min[i+ (2κ+ 1)] then\n15: return MERGE SORT(A)\n16: foriinkdo\n17: if|B[i]| ≥(2κ+ 1)n4\n5then\n18: return MERGE SORT(A)\n19: else\n20: B[i]←SORT(B[i])\n21: return MERGE (B) ▷Alg. 6\nq2[d][0] = sjandq3[d][0] = si,q3[d][1] = sj. Then,\nlearned models are used to answer q1andq2, while q3is\nfurther recursively decomposed along its d−1-th dimension\n(and after full decomposition is answered using the grid).\nNote that q1andq2can now be answered using models,\nbecause by grid construction, they fall in a part of the space\nwith at most (ϵ\n2d)2points.\nAccuracy . Grid cells are exact, and, as discussed above\neach model is built on a dataset of size at most (ϵ\n2d)2, so\nthat it will have expected error O(ϵ\n2d). Thus, combining the\nerror of the 2dqueries, the total model error is ϵas desired.\nQuery Time and Space Complexity . There are 2dmodel\ncalls, each model call costing TX\n(ϵ\n2d)2. Furthermore, if the\ni-th dimension of the query covers kipoints, then total\nof at most4d2ki\nϵ2partitions in the i-th dimension inter-\nsect the query, so that the total number of cells traversed\nwill be (4d2\nϵ2)dΠiki. Thus, total query time is O(TX\n(ϵ\n2d)2+\n(4d2\nϵ2)dΠiki). Furthermore, the total cost of storing the mod-\nels is4d2n\nϵ2SX\n(ϵ\n2d)2, and the cost of the grid is (4d2n\nϵ2)d. Thus,\ntotal space complexity is O(4d2n\nϵ2SX\n(ϵ\n2d)2+ (4d2n\nϵ2)d).Algorithm 6 Merge Step\nRequire: An array of sorted buckets\nEnsure: Buckets merged into a sorted array\n1:procedure MERGE (B)\n2: As←empty array of size n\n3: As[:len(B[1])]←B[1]\n4: j←len(B[1])\n5: forb←2tokdo ▷Merges As[:j]withB[b]\n6: j←j+len(B[b]) ▷Iterator for As\n7: i←len(B[b]) ▷Iterator for B[b]\n8: while i >0do\n9: ifB[b][i]> A s[j]then\n10: As[j+i]←B[b][i]\n11: i--\n12: else\n13: As[j+i]←As[j]\n14: j--\n15: j←j+len(B[b])\n16: return As\nD.7. Sorting\nD.7.1. U SING LEARNED MODEL (THEOREM 4.5)\nAlgorithm. The algorithm is presented in Alg. 5. A sample\nof the array is first created and a model is built using the\nsample. Then, using the model, the array is split into n1\n8\nbuckets, where we theoretically show, using such a number\nof buckets, based on the accuracy of the model and with\nhigh probability, merging the buckets can be done by in\nlinear time (because there will be limited overlap between\nthe buckets) and each bucket will not be too big. Indeed,\nwe first make sure the two properties mentioned before hold\n(otherwise the algorithm quits and reverts to merge sort),\nand then proceed to merge the buckets.\nCorrectness . If all the created buckets are sorted, the merge\nstep simply merges them and thus returns a sorted array\ncorrectly. At the base case, merge sort is used, so the buckets\nwill be sorted correctly. Thus, using the invariant above, the\nalgorithm is correct.\nTime Complexity . Consider sorting an array A∼χof size\nn. We take a subset S, of size√nfromAwithout checking\nthe elements to preserve the i.i.d assumption. We sort them\nand use the algorithm Ato obtain a model ˆr. We have that\nP(∥ˆr−rχ∥∞≥√nϵ1)≤κ1e−κ2(ϵ1n−1\n4−1)2.(19)\nNote that\n∥ˆr−rA∥∞≤ ∥ˆr−rχ∥∞+∥rχ−rA∥,\nAnd, by DWK,\nP(∥rA−rχ∥∞≥ϵ3)≤2e−2(ϵ3√n)2\n.\nSetϵ1=n1\n4(q\nlog log n\nκ2+ 1) andϵ3=q\nnlog log n\n2, we\n21\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nhave\nP(∥ˆr−rχ∥∞≥√nϵ1or∥rA−rχ∥ ≥ϵ3)\n≤κ1e−log log n+ 2e−log log n\n=κ1+ 2\nlogn\nThus,\nP(∥ˆr−rA∥ ≥√nϵ1+ϵ3)≤κ1+ 2\nlogn\nand thus, whenever log log n≥2and forκ=1√\n2+1√κ2\nP(∥ˆr−rA∥∞≥κn3\n4p\nlog log n)≤κ1+ 2\nlogn.\nTo simplify, observe that n3/4√log log n≤n4/5forn≥e\nso that\nP(∥ˆr−rA∥∞≥κn4\n5)≤κ1+ 2\nlogn.\nRecall that we use kbuckets and consider the i-th bucket.\nThe elements, x, assigned to it must havei\nk≤1\nnˆr(x)<\ni+1\nk. Combining this with the above, we have that whenever\n∥ˆr−rA∥∞≤κn4\n5holds, for the elements xin the i-\nth bucket, we must have1\nnˆr(x)≥i\nk−κn−1\n5and that\nrA(x)≤i+1\nk+κn−1\n5. Therefore, we must have rA(x)∈\n[in\nk−κn4\n5,(i+1)n\nk+κn4\n5]. There are at mostn\nk+ 2κn4\n5\nelements in this set. Setting k=n1\n5, we have that whenever\n∥ˆr−rA∥∞≤κn4\n5holds, all buckets will have at most\n(2κ+ 1)n4\n5elements. Thus, the probability that a bucket\nwill have more than (2κ+ 1)n4\n5elements is at mostκ1+2\nlogn.\nFurthermore, the largest element in the i-th bucket or be-\nfore will have rA(x)<(i+ 1 +κ)n4\n5and the smallest\nelement in the i+ 2κ+ 1-th bucket or after will have\nrA(x)≥(i+ 1 +κ)n4\n5, so that the content of buckets up\ntoiare less than the content of the buckets from i+ 2κ+ 1\nonwards. Thus, whenever ∥ˆr−rA∥∞≤κn4\n5holds, if\nall the buckets are sorted, then the algorithm takes at mostPk\ni=1P2κ+1\nj=0|Bi−j|number of operations to merge the\nsorted array, which is O(n), where |Bi|is the number of\nelements in the i-th bucket.\nFinally, let Tnbe the expected number of operations it takes\nto sort an array with nelements i.i.d sampled from some a\ndistribution learnable class. Recall that we do merge sort\nif∥ˆr−rA∥∞≤κn4\n5does not hold, and recursively sort\nthe array if it does. Thus, whenever we recursively sort an\narray, the array will have at most (2κ+ 1)n4\n5i.i.d elements,\ndistributed from a conditional distribution of the originalAlgorithm 7 Sorting Using Distribution Model\nRequire: An unsorted array Aof size n\nEnsure: A sorted array\n1:procedure SORT(A)\n2: ifn≤10then\n3: return MERGE SORT(A)\n4: A′←new array, A′[i]initialized as linked list, ∀i\n5: fori←1tondo\n6: i′← ⌈ˆr(A[i])⌉\n7: A′[i′].append (A[i])\n8: return MERGE (B) ▷Alg. 6\ndistribution. Thus, we have\nT(n)≤O(BX√n+TX√nn) +P(merge sort )nlogn+\nP(recursively sort )n1\n5T((2κ+ 1)n4\n5)\n≤O(BX√n+TX√nn) +κ+ 2\nlognnlogn+\nn1\n5T((2κ+ 1)n4\n5)\n=O(TX√nnlog log n+\n√nBX√n+log log nX\ni=0n191\n2(4\n5)iBX\nn1\n2(4\n5)i).\nSpace Comlexity . First, observe that we only create one\nmodel at a time, so the maximum size used for modeling is\nSX√n+√nlogn. Moreover, the depth of recursion is at most\nO(log log n), and the overhead of storing Bis dominated\nby the first of recursion, whose overhead is O(n1\n8logn+\nO(nlogn)), giving overall space overhead of O(SX√n+\nnlogn)\nD.7.2. U SING DATA DISTRIBUTION (THEOREM 4.6)\nAlg. 7 shows how to sort an array given an approximate\nmodel of the data distributionˆ[r]. The algorithm is very\nsimilar to Alg. 7, but uses nbuckets and merges each bucket\nusing merge sort (and thus no recursive sorting of the buck-\nets).\nCorrectness . The algorithm creates buckets, sorts them\nindependently. The sort is done by merge sort so it is correct,\nand thus merging the sorted buckets creates a sorted array.\nRunning time LetT(A)be the number of operations\nthe algorithm performs on an array A, and let T(n) =\nEA∼χn[T(A)]be the expected run time of the algorithm on\nan input of size n.\nFirst, assume we use ⌊nrχ(x)⌋to map an element xto a\nlocation in the array S. After the mapping, we study the\nexpected time to sort the elements in S[i:j]where j=i+k\nandk > 0. Note that the probability that an element is\nmapped to location [i:j], i.e.,Px∼χ(i\nn≤rχ(x)<j\nn), is\n22\n\nTheoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability\nk\nn. LetNi:j=|S[i:j]|be the number of elements mapped\ntoS[i:j]. We have that\nPA∼χn(Ni:j=z) =C(n, z)(k\nn)z(1−z\nn)n−z.\nThus, we have\nEA∼χn[Ni:jlog(Ni:j)] =nX\nz=1PA∼χn(Ni:j=z)[zlog(z)]\n=nX\nz=1C(n, z)(k\nn)z(1−k\nn)n−zzlogz\n=4ekX\ni=1C(n, i)(k\nn)i(1−k\nn)n−iilogi\n+nX\ni=4ekC(n, i)(k\nn)i(1−k\nn)n−iilogi\nFor the first part of the summation, we have\n4ekX\ni=1C(n, i)(k\nn)i(1−k\nn)n−iilogi\n≤log(4ek)4ekX\ni=1C(n, i)(k\nn)i(1−k\nn)n−ii\n≤klog(4ek).\nFor the second part, we have\nnX\ni=4ekC(n, i)(k\nn)i(1−k\nn)n−iilogi\n≤nX\ni=4ek1√\ni(en\ni)i(k\nn)i(1−k\nn)n−iilogi\n=nX\ni=4ek(ekn\ni(n−k))i(1−k\nn)n√\nilogi\n≤nX\ni=4ek(2ek\ni)i(1−k\nn)n√\nilogi\n≤nX\ni=4ek(1\n2)ii\n≤2.\nSo that\nEA∼χn[Ni:jlog(Ni:j)]≤(j−i) log(4 e(j−i)) + 2 .\nNow recall that we use ⌊ˆr⌋with error ∥ˆr−nrχ∥∞≤ϵ\nto map the elements to an array S′. Thus, if, for any x,\n⌊nrχ(x)⌋=j,ˆr∈ {j− ⌊ϵ⌋, ..., j +⌈ϵ⌉}. Let ¯Ni:jbe the\nnumber of elements mapped to positions [i:j]using ˆr.\nNote that ¯Ni:j≤Ni−ϵ:j+ϵ. Thus, dividing S′into groups\nofϵand sorting each separately, we have that the total costof sorting the groups is\nn\nϵX\nj=0EA∼χn[¯Njϵ:(j+1)ϵlog(¯Njϵ:(j+1)ϵ)]\n≤n\nϵX\nj=0EA∼χn[N(j−1)ϵ:(j+2)ϵlog(N(j−1)ϵ:(j+2)ϵ)]\n≤O(ϵlogϵ)\nFinally, note that rχis a non-decreasing function. Therefore,\nifx≤y, we have ⌊nrχ(x)⌋ ≤ ⌊ nrχ(y)⌋. Given that\n∥ˆr−nrχ∥∞≤ϵ, we have that if x≤y,ˆr(y)≥ˆr(x)−2ϵ.\nConsequently, if xis mapped to the i-th group by ˆr, all\nelements in the j-th group with j < i−2are less than x.\nThis means, to merge the sorted groups, we start with the\nfirst group and iteratively merge the next group with the\nmerged array so far. Performing each merge from the end of\nthe two sorted arrays (as done in merging using learned data\ndistribution), each merge will cost at most 3ϵ, so the total\ncost of merging all then\nϵsorted groups is O(n\nϵϵ) =O(n).\nPutting everything together,r when each model call costs\nTX\nn, we have that expected time complexity of the algorithm\nisO(nTX\nn+nlogϵ). Moverover the space overhead of\nthe algorithm is O(nlogn+SX\nn)where the lognfactor is\nto keep a pointer to the elements of the array (instead of\ncopying them).\n23",
  "textLength": 110245
}