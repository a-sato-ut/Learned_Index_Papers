{
  "paperId": "225c57b1c0eb44837f61f7bbfc5a61699ea4f1d5",
  "title": "Why Are Learned Indexes So Effective but Sometimes Ineffective?",
  "pdfPath": "225c57b1c0eb44837f61f7bbfc5a61699ea4f1d5.pdf",
  "text": "Why Are Learned Indexes So Effective but Sometimes Ineffective?\nQiyu Liu\nSouthwest University\nqyliu.cs@gmail.comSiyuan Han\nHKUST\nshanaj@connect.ust.hkYanlin Qi\nHIT Shenzhen\nyanlinqi7@gmail.comJingshu Peng\nByteDance\njingshu.peng@bytedance.com\nJin Li\nHarvard University\njinli@g.harvard.eduLonglong Lin\nSouthwest University\nlonglonglin@swu.edu.cnLei Chen\nHKUST & HKUST (GZ)\nleichen@cse.ust.hk\nABSTRACT\nLearned indexes have attracted significant research interest due\nto their ability to offer better space-time trade-offs compared to\ntraditional B+-tree variants. Among various learned indexes, the\nPGM-Index based on error-bounded piecewise linear approximation\nis an elegant data structure that has demonstrated provably superior\nperformance over conventional B+-tree indexes. In this paper, we\nexplore two interesting research questions regarding the PGM-\nIndex:â¶Why are PGM-Indexes theoretically effective? andâ·Why\ndo PGM-Indexes underperform in practice? For question â¶, we first\nprove that, for a set of ğ‘sorted keys, the PGM-Index can, with\nhigh probability, achieve a lookup time of ğ‘‚(log logğ‘)while using\nğ‘‚(ğ‘)space. To the best of our knowledge, this is the tightest\nbound for learned indexes to date. For question â·, we identify that\nquerying PGM-Indexes is highly memory-bound, where the internal\nerror-bounded search operations often become the bottleneck. To\nfill the performance gap, we propose PGM++, a simple yet effective\nextension to the original PGM-Index that employs a mixture of\ndifferent search strategies, with hyper-parameters automatically\ntuned through a calibrated cost model. Extensive experiments on\nreal workloads demonstrate that PGM++ establishes a new Pareto\nfrontier. At comparable space costs, PGM++ speeds up index lookup\nqueries by up to 2.31Ã—and1.56Ã—when compared to the original\nPGM-Index and state-of-the-art learned indexes.\nPVLDB Reference Format:\nQiyu Liu, Siyuan Han, Yanlin Qi, Jingshu Peng, Jin Li, Longlong Lin,\nand Lei Chen. Why Are Learned Indexes So Effective but Sometimes\nIneffective?. PVLDB, 14(1): XXX-XXX, 2020.\ndoi:XX.XX/XXX.XX\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/qyliu-hkust/bench_search.\n1 INTRODUCTION\nIndexes are fundamental components of DBMS and big data en-\ngines to enable real-time analytics [ 30,35]. An emerging research\ntendency is to directly learn the storage layout of sorted data by\nusing simple machine learning (ML) models, leading to the concept\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XX\nSorted Key Set \n(b) Learned Index(a) B+-tr ee Index\n-bounded Last-\nMile Sear ch\nQuery key \nML\nModel Internal\nModel Sear chQuery key \nPointersFigure 1: (a) A conventional B+-tree index. (b) A learned index\nwith a â€œlast-mileâ€ maximum search error ğœ–.\nofLearned Index [8,11,18,45,50,52]. Compared to traditional\nindexes like B+-tree variants [ 6,14,19], learned indexes have been\nshown to reduce the memory footprint by 2â€“3 orders of magnitude\nwhile achieving comparable index lookup performance.\nSimilar to B+-trees or other binary search tree (BST) variants,\nlearned indexes address the classical problem of Sorted Dictionary\nIndexing [7]. Given an array of ğ‘sorted keysK={ğ‘˜1,Â·Â·Â·,ğ‘˜ğ‘},\nthe objective of learned indexes is to find a projection function\n(i.e., an ML model) ğ‘“(ğ‘˜)âˆˆN+that maps an arbitrary query key ğ‘˜\nto its corresponding index in the sorted array K(i.e., its position\non storage). However, ML models inherently produce prediction\nerrors. As illustrated in Figure 1, the maximum prediction error over\nKis denoted by ğœ–. To ensure the correctness of an index lookup\nquery for a search key ğ‘˜, an exact â€œlast-mileâ€ search, typically a\nstandard binary search, must be performed within the error range\n(i.e.,[ğ‘“(ğ‘˜)âˆ’ğœ–,ğ‘“(ğ‘˜)+ğœ–]). To balance model accuracy with complexity,\nlearned indexes such as Recursive Model Index (RMI) [ 18] and PGM-\nIndex [ 11] opt to stack simple models, such as linear models or\npolynomial splines, in a hierarchical structure, thereby achieving a\nbalance between the model complexity and fitting accuracy.\nAmong the various published learned indexes [ 8,11,18,45,50,\n52], the PGM-Index [ 11] stands out as a simple yet elegant struc-\nture that has been proven to be theoretically more efficient than\na B+-tree. As depicted in Figure 2, the PGM-Index is a multi-level\nstructure constructed by recursively fitting error-bounded piecewise\nlinear approximation models (ğœ–-PLA). Searching in a PGM-Index is\nperformed through a sequence of error-bounded search operations\nin a top-down manner. Recent theoretical analysis [ 10] indicates\nthat, compared to a B+-tree with fanout ğµ, the PGM-Index, however,\ncan reduce memory footprint by a factor of ğµ, while preserving the\nsame logarithmic index lookup complexity (i.e., ğ‘‚(logğ‘)).\nIntuitively, the PGM-Index is structured as a hierarchy of line\nsegments, where the index height is a key factor in determining the\nlookup time complexity. Existing results [ 10,11] suggest that thearXiv:2410.00846v1  [cs.DB]  1 Oct 2024\n\nkeyidx\nsegidata point\nline segmentLevel-2\n(Root Segment)\nLevel-1\nLevel-0\n(Leaf Segments)\nSorted Keys......\n... ...(a) PGM-Index (b) -PLAFigure 2: A toy example of a 3-level PGM-Index with ğœ–ğ‘–=1(i.e., internal search error range) and ğœ–â„“=4(i.e., last-mile search\nerror range). Processing a lookup query on such PGM-Index involves in total three linear function evaluations, two internal\nsearch operations in the range 2Â·ğœ–ğ‘–+1, and one â€œlast-mileâ€ search operation on the sorted data array in the range 2Â·ğœ–â„“+1.\nheight of a PGM-Index built on ğ‘sorted keys should be ğ‘‚(logğ‘).\nHowever, our empirical investigations reveal that PGM-Indexes are\nhighly flat, with over 99% of the total index space cost attributed\nto the segments at the bottom level. This observation implies that\nthe height of the PGM-Index grows more slowly than ğ‘‚(logğ‘),\npotentially at a sub-logarithmic rate. Motivated by this, we pose\nthe following research question.\nQ1: Why Are PGM-Indexes So Effective in Theory? To an-\nswer this question, we establish new theoretical results for PGM-\nIndexes. With high probability (w.h.p.), the index lookup time can\nbe bounded by ğ‘‚(log2logğºğ‘)=ğ‘‚(log logğ‘)using linear space\nofğ‘‚(ğ‘/ğº), whereğºis a constant determined by data distribu-\ntion characteristics and the error constraint ğœ–. To the best of our\nknowledge, this work presents the tightest bound for learned index\nstructures compared to existing theoretical analyses [10, 49].\nInterestingly, BSTs can be viewed as a â€œmaterializedâ€ version of\nthe binary search algorithm, whose time complexity is ğ‘‚(logğ‘).\nAs an analog, the PGM-Index with piecewise linear approximation\nmodels can be regarded as a â€œmaterializedâ€ version of the interpola-\ntion search algorithm, whose time complexity is ğ‘‚(log logğ‘)[27,\n33], aligning with our theoretical findings.\nDespite its theoretical superiority, recent benchmarks [ 22,44]\nshow that the PGM-Index falls short of practical performance expec-\ntations, often underperforming compared to well-optimized RMI\nvariants [17, 18]. This leads to our second research question.\nQ2: Why Are PGM-Indexes Ineffective in Practice? Our inves-\ntigation into extensive benchmark results across various hardware\nplatforms reveals that PGM-Indexes are memory-bound. The in-\nternal error-bounded search operation, often implemented as a\nstandard binary search (e.g., std::lower_bound in C++), becomes\na bottleneck when processing an index lookup query. According to\nour benchmark (Section 5), less than 1%of the internal segments\naccount for over 80% of the total index lookup time.\nTo improve search efficiency, we propose a hybrid internal search\nstrategy that combines the advantages of linear search and highly\noptimized branchless binary search by properly setting search range\nthresholds. Additionally, as illustrated in Figure 2, constructing\na PGM-Index necessitates two hyper-parameters ğœ–ğ‘–andğœ–â„“, the\nerror thresholds for internal index traversal and last-mile search on\nthe data array, respectively. We find that the ğœ–â„“primarily controls\nthe overall index size, while both ğœ–ğ‘–andğœ–â„“influence the index\nlookup efficiency. Based on theoretical analysis and experimental\nobservations, we develop a cost model that is finely calibrated usingbenchmark data. Leveraging this cost model, we further introduce\nan automatic hyper-parameter tuning strategy to better balance\nindex lookup efficiency with index size.\nIn summary, our technical contributions are as follows. â¶New\nBound. We prove the sub-logarithmic index lookup time of the\nPGM-Index (i.e., ğ‘‚(log logğ‘)). This result tightens the previous\nlogarithmic bound on the PGM-Index and further validates its prov-\nable performance superiority compared to conventional tree-based\nindexes. â·Simple Methods. We introduce PGM++, a simple yet\neffective improvement to the PGM-Index by replacing the costly\ninternal search operations. We further devise an automatic param-\neter tuner for PGM++, guided by an accurate cost model. â¸New\nPareto Frontier. Extensive experimental studies on real and syn-\nthetic data show that, with a comparable index memory footprint,\nPGM++ robustly outperforms the original PGM-Index and opti-\nmized RMI variants [ 22,50] by up to 2.31Ã—and1.56Ã—, respectively.\nFor example, even on a resource-constrained device like MacBook\nAir 2024 [ 21], PGM++ achieves index lookup time of <400 ns on\n800 million keys, using only 0.28 MB of memory.\nThe remainder of this paper is structured as follows. Section 2\nintroduces the basis of learned indexes, followed by the micro-\nbenchmark setup details in Section 3. Section 4 presents our core\ntheoretical analysis of the PMG-Index. Section 5 explores the rea-\nsons behind the PGM-Indexâ€™s underperformance in practice. In\nSection 6, we introduce PGM++, an optimized PGM-Index vari-\nant featuring hybrid error-bounded search and automatic hyper-\nparameter tuning. Section 7 reports the experimental results. Sec-\ntion 8 surveys and discusses related works, and finally, Section 9\nconcludes the paper and discusses future studies.\n2 PRELIMINARIES\nWe first overview the basis of learned indexes ( â–·Section 2.1) and\nthen elaborate on the details of existing theoretical results ( â–·Sec-\ntion 2.2). Table 1 summarizes the major notations.\n2.1 Learned Index\nGiven a set of ğ‘sorted keysK={ğ‘˜1,ğ‘˜2,Â·Â·Â·,ğ‘˜ğ‘}and an index\nsetI={1,2,Â·Â·Â·,ğ‘}, the goal of learned indexes is to find a map-\nping function ğ‘“(ğ‘˜) âˆˆN+such thatğ‘“can project a search key\nğ‘˜âˆˆK to its corresponding index rank(ğ‘˜)âˆˆI with controllable\nerror. Intuitively, learning ğ‘“is equivalent to learning a cumulative\ndistribution function (CDF) scaled by the data size ğ‘. The model\nselection considerations for ğ‘“are threefold:\n2\n\nTable 1: Summary of major notations.\nNotation Description\nK a set ofğ‘sorted keys\nrank(ğ‘˜) the sorting index of a key ğ‘˜inK\nğœ–ğ‘– the internal search error parameter of PGM-Index\nğœ–â„“ the last-mile search error parameter of PGM-Index\nğ‘”ğ‘– the difference between ğ‘˜ğ‘–andğ‘˜ğ‘–âˆ’1(a.k.a. gap)\nğœ‡,ğœ2the mean and variance of gap distribution\n(ğ‘ ,ğ‘,ğ‘) a line segment â„“(ğ‘¥)=ğ‘Â·(ğ‘¥âˆ’ğ‘ )+ğ‘\nğ»ğ‘ƒğºğ‘€ the height of a PGM-Index\nâ¶Compactness : the model ğ‘“should be compact to reduce mem-\nory footprint, and model inference using ğ‘“must not introduce\nsignificant computational overhead;\nâ·Error-Boundness : the model ğ‘“should be error-bounded, en-\nsuring that an exact last-mile search can correct prediction errors,\ni.e.,|ğ‘“(ğ‘˜)âˆ’rank(ğ‘˜)|â‰¤ğœ–forâˆ€ğ‘˜âˆˆK;\nâ¸Monotonicity : to ensure the correctness of querying keys out-\nsideK,ğ‘“(ğ‘˜1)â‰¤ğ‘“(ğ‘˜2)should hold for any ğ‘˜1â‰¤ğ‘˜2.\nSince running deep learning (DL) models usually require a heavy\nruntime like PyTorch [ 31] or TensorFlow [ 37] that are costly and\nless flexible, existing learned index designs favor stacking simple\nmodels, such as linear functions [ 8,11,45], polynomial splines [ 18],\nand radix splines [ 17]. Among these learned index structures, the\nPGM-Index [ 11] employs the error-bounded piecewise linear ap-\nproximation ( ğœ–-PLA) to strike a balance between the model com-\nplexity and prediction accuracy, which is defined as follows.\nDefinition 2.1 ( ğœ–-PLA). Given a univariate set X={ğ‘¥1,Â·Â·Â·,ğ‘¥ğ‘},\na corresponding target set Y={ğ‘¦1,Â·Â·Â·,ğ‘¦ğ‘}, and an error con-\nstraintğœ–, anğœ–-PLA on the point set in Cartesian space (X,Y)=\n{(ğ‘¥ğ‘–,ğ‘¦ğ‘–)}ğ‘–=1,Â·Â·Â·,ğ‘is defined as,\nğ‘“(ğ‘¥)=ï£±ï£´ï£´ï£´ï£´ï£´ ï£²\nï£´ï£´ï£´ï£´ï£´ï£³ğ‘1Â·(ğ‘¥âˆ’ğ‘ 1)+ğ‘1 ifğ‘ 1â‰¤ğ‘¥<ğ‘ 2\nğ‘2Â·(ğ‘¥âˆ’ğ‘ 2)+ğ‘2 ifğ‘ 2â‰¤ğ‘¥<ğ‘ 3\nÂ·Â·Â· Â·Â·Â·\nğ‘ğ‘šÂ·(ğ‘¥âˆ’ğ‘ ğ‘š)+ğ‘ğ‘š ifğ‘ ğ‘šâ‰¤ğ‘¥<+âˆ(1)\nsuch that forâˆ€ğ‘–=1,2,Â·Â·Â·,ğ‘, it always holds that |ğ‘“(ğ‘¥ğ‘–)âˆ’ğ‘¦ğ‘–|â‰¤ğœ–.\nTheğ‘–-th segment in Eq. (1)can be expressed by a tuple ğ‘ ğ‘’ğ‘”ğ‘–=\n(ğ‘ ğ‘–,ğ‘ğ‘–,ğ‘ğ‘–)whereğ‘ ğ‘–is the segment starting point, ğ‘ğ‘–is the slope,\nandğ‘ğ‘–is the intercept. To ensure the monotonic requirement, the\nsegments in Eq. (1)should satisfy two conditions: (a) ğ‘ğ‘–â‰¥0for\nğ‘–=1,Â·Â·Â·,ğ‘š, and (b)ğ‘ ğ‘–<ğ‘ ğ‘—forâˆ€1â‰¤ğ‘–<ğ‘—â‰¤ğ‘š. We then extend\nthe original PGM-Index definition [ 11] by separating the error\nparameters for internal search and last-mile search.\nDefinition 2.2 ((ğœ–ğ‘–,ğœ–â„“)-PGM-Index [ 11]).Given a sorted key set\nK={ğ‘˜1,ğ‘˜2,Â·Â·Â·,ğ‘˜ğ‘}and two error parameters ğœ–ğ‘–andğœ–â„“(ğœ–ğ‘–,ğœ–â„“âˆˆ\nN+), an(ğœ–ğ‘–,ğœ–â„“)-PGM-Index is a multi-level structure where the\nbottom level (a.k.a., the leaf level or level-0) is an ğœ–â„“-PLA and the\nremaining levels (a.k.a., internal levels) are ğœ–ğ‘–-PLA(s). The structure\ncan be constructed in a bottom-up manner:\nâ¶Leaf Level : anğœ–â„“-PLA constructed on (K,I={1,Â·Â·Â·,ğ‘}).\nâ·Internal Levels : for theğ‘—-th level (ğ‘—â‰¥1), letSğ‘—âˆ’1denote the set\nof segments in the (ğ‘—âˆ’1)-th level (i.e., the previous level), and let\nKğ‘—âˆ’1={ğ‘ ğ‘’ğ‘”.ğ‘ |ğ‘ ğ‘’ğ‘”âˆˆSğ‘—âˆ’1}andIğ‘—âˆ’1={1,2,Â·Â·Â·,|Kğ‘—âˆ’1|}. Then,\ntheğ‘—-th level is an ğœ–ğ‘–-PLA constructed on dataset (Kğ‘—âˆ’1,Iğ‘—âˆ’1).\nâ¸Root Level : the internal level consisting of a single line segment.Table 2: Summary of theoretical results. For our result, ğºis\na constant that depends on data distribution characteristics\nand the pre-specified error bound ğœ–.\nResults Base Model Lookup Time Space Cost\nICMLâ€™20 [10] Linear ğ‘‚(logğ‘)ğ‘‚(ğ‘/ğœ–2)\nICMLâ€™23 [49] Constant ğ‘‚(log logğ‘)ğ‘‚(ğ‘logğ‘)\nOurs Linear ğ‘‚(log logğ‘)ğ‘‚(ğ‘/ğº)\nSpecifically, we denote the segments in the bottom level as leaf\nsegments and the remaining segments as internal segments (corre-\nsponding to the subscripts of ğœ–â„“andğœ–ğ‘–, respectively). The following\nexample illustrates the PGM-Index lookup query processing.\nExample 2.3 (PGM-Index Lookup). Figure 2 illustrates a 3-level\nPGM-Index with ğœ–ğ‘–=1andğœ–â„“=4. Given a query key ğ‘˜, an index\nlookup query is performed in a top-down manner from the root\nlevel to the bottom level as follows:\nâ¶TheInternal Index Traversal phase starts from the root level\nand finds the appropriate line segment in each level until reaching\nthe bottom level (depicted by the red path in Figure 2). Specifically,\nletğ‘ ğ‘’ğ‘”ğ‘—=(ğ‘ ğ‘—,ğ‘ğ‘—,ğ‘ğ‘—)denote the segment in the ğ‘—-th level during\nthe traversal. The next segment in the (ğ‘—âˆ’1)-th level to be traversed\nis found by searching ğ‘˜within range ğ‘ğ‘—Â·(ğ‘˜âˆ’ğ‘ ğ‘—)+ğ‘ğ‘—Â±ğœ–ğ‘–.\nâ·TheLast-Mile Search phase performs an exact search on the raw\nsorted keys (i.e.,K) within the range Âœrank(ğ‘˜)Â±ğœ–â„“where Âœrank(ğ‘˜)=\nğ‘0Â·(ğ‘˜âˆ’ğ‘ 0)+ğ‘0is the predicted rank and ğ‘ ğ‘’ğ‘”0=(ğ‘ 0,ğ‘0,ğ‘0)is the\nleaf segment found during the internal index traversal phase.\nRecall that the index construction procedures introduced in Def-\ninition 2.2 guarantees that the maximum errors for internal index\ntraversal and last-mile search cannot exceed ğœ–ğ‘–andğœ–â„“, respectively.\nThus, the aforementioned lookup processing ensures the correct\nlocation (i.e., rank(ğ‘˜)) must be found for an arbitrary query key ğ‘˜.\n2.2 Existing Theoretical Results\nFrom Section 2.1, two key questions need to be addressed to deter-\nmine the space and time complexities of the PGM-Index. â¶How\nmany line segments are required to satisfy the error constraint for an\nğœ–-PLA model? andâ·What is the height (i.e., the number of layers)\nof a PGM-Index? In this section, we review the related theoretical\nstudies [ 10,49] regarding these two questions, with major results\nsummarized in Table 2.\nThe original PGM-Index [ 11] first provides a straightforward\nlower bound to determine the index height.\nTheorem 2.4 (PGM-Index Lower Bound [ 11]).Given a consec-\nutive chunk of 2ğœ–+1sorted keys{ğ‘˜ğ‘–,Â·Â·Â·,ğ‘˜ğ‘–+2ğœ–}âŠ†K , there exists a\nhorizontal line segment â„“(ğ‘¥)=ğ‘–+ğœ–such that|â„“(ğ‘˜ğ‘—)âˆ’ğ‘—|â‰¤ğœ–holds\nforğ‘—=ğ‘–,Â·Â·Â·,ğ‘–+2ğœ–, implying that each line segment in an ğœ–-PLA can\ncover at least 2ğœ–+1keys.\nRecall the recursive construction process in Definition 2.2, w.l.o.g.,\na PGM-Index with ğœ–ğ‘–=ğœ–â„“=ğœ–has a height of ğ‘‚(logğœ–ğ‘)=\nğ‘‚(logğ‘). Thus, the index lookup takes time ğ‘‚(logğ‘Â·log2ğœ–)=\nğ‘‚(logğ‘)asğœ–can be regarded as a pre-specified constant.\nFerragina et al. [ 10] further tighten the results in Theorem 2.4\nby showing that the expected segment coverage is proportional to\nğœ–2. Suppose that the key set to be indexed K={ğ‘˜1,ğ‘˜2,Â·Â·Â·,ğ‘˜ğ‘}\nis a materialization of a random process ğ‘˜ğ‘–=ğ‘˜ğ‘–âˆ’1+ğ‘”ğ‘–forğ‘–â‰¥2\n3\n\nTable 3: Summary of three micro-benchmark platforms. For platforms X86-1 and ARMwhose CPU chips adopt the â€œbig.LITTLEâ€\narchitecture [ 3], the hardware statistics of the performance cores (i.e., P-core) are reported. The reported L1/L2/L3 sizes represent\nthe actual cache size that a physical core can access. Notably, for the Apple M3 chip, only L1 cache and L2 cache are available.\nPlatform OS Compiler CPU Frequency Memory L1 L2 L3 (LLC)\nX86-1 Ubuntu 20.04 g++ 11 Intel Core i7-13700K 5.30 GHz (P-core) 32 GB DDR4 64 KiB 256 KiB 16 MB\nX86-2 CentOS 9.4 g++ 11 AMD EPYC 7413 3.60 GHz 1 TB DDR4 64 KiB 1 MB 256 MB\nARM macOS 14.4.1 clang++ 15 Apple M3 4.05 GHz (P-core) 16 GB LPDDR5 320 KiB 16 MB N.A.\nwhereğ‘”ğ‘–â€™s are i.i.d. random variables (r.v.) following some unknown\ndistribution. We term the r.v. ğ‘”ğ‘–as the â€œgapâ€ and denote ğœ‡=E[ğ‘”ğ‘–]\nandğœ2=Var[ğ‘”ğ‘–]as its mean and variance. These distribution\ncharacteristics are crucial in determining the expected number of\nsegments to satisfy the error constraints.\nTheorem 2.5 (Expected Line Segment Coverage [ 10]).Given\na set of sorted keys K={ğ‘˜1,ğ‘˜2,Â·Â·Â·,ğ‘˜ğ‘}and an error parameter ğœ–,\nlet the gap be ğ‘”ğ‘–=ğ‘˜ğ‘–âˆ’ğ‘˜ğ‘–âˆ’1. If the condition ğœ–â‰«ğœ/ğœ‡holds, with\nhigh probability, the expected number of keys in Kcovered by a line\nsegmentâ„“(ğ‘¥)=ğœ‡Â·(ğ‘¥âˆ’ğ‘˜1)+1is given by\nE\u0002\nmin\b\nğ‘–âˆˆN+||â„“(ğ‘˜ğ‘–)âˆ’ğ‘–|>ğœ–\t\u0003\n=ğœ‡2ğœ–2/ğœ2, (2)\nwhereâ„“(ğ‘˜ğ‘–)=ğœ‡Â·(ğ‘˜ğ‘–âˆ’ğ‘˜1)+1is the predicted index for a key ğ‘˜ğ‘–.\nBy constructing a special line segment with slope ğœ‡, Theorem 2.5\nestablishes the relationship between the expected segment cover-\nage and the error constraint ğœ–. Based on Theorem 2.5, for a set of\nğ‘sorted keys, the expected number of segments1of a one-layer\nğœ–-PLA can be derived as ğ‘ğœ2/ğœ–2ğœ‡2. In the practical PGM-Index\nimplementation, an optimalğœ–-PLA fitting algorithm [ 26] is adopted\ntominimize the number of line segments while ensuring the error\nconstraintğœ–is met. Thus, the expected number of segments can be\nthen bounded by ğ‘‚(ğ‘ğœ2/ğœ–2ğœ‡2).\nCombining the results in Theorem 2.4 and Theorem 2.5, Ferrag-\nina et al. [ 10] conclude that a PGM-Index with ğœ–ğ‘–=ğœ–â„“=ğœ–using\nğ‘‚(ğ‘/ğœ–2)space can handle lookup queries in ğ‘‚(logğ‘)time with\nhigh probability. By setting ğœ–=Î˜(ğµ), a PGM-Index can achieve\nthe same logarithmic index lookup complexity of a B+-tree while\nreducing the space complexity from B+-treeâ€™s ğ‘‚(ğ‘/ğµ)toğ‘‚(ğ‘/ğµ2).\nIn addition to [ 10], a recent study [ 49] also delves into the theo-\nretical aspects of learned index. They demonstrate that a Recursive\nModel Index [ 18] using piece-wise constant functions as base models\ncan achieve a sub-logarithmic lookup complexity of ğ‘‚(log logğ‘)\nat the cost of super-linear space, specifically ğ‘‚(ğ‘logğ‘).\nOur Results. Inspired by the findings in [ 49], we reasonably specu-\nlate that PGM-Indexes, utilizing ğœ–-PLA as base models, can achieve\nthe same sub-logarithmic lookup time complexity with reduced\nspace overhead, given that a constant function can be regarded\nas a special case of a piecewise linear function. As summarized in\nTable 2, our analysis in Section 4 concludes that, w.h.p., the PGM-\nIndex can search a query key in ğ‘‚(log logğ‘)time while requiring\nonly linear space ğ‘‚(ğ‘/ğº), whereğºis a constant related to the\nerror parameter ğœ–and gap distribution characteristics.\n3 MICROBENCHMARK SETTING\nTo ensure consistency in presentation, this section outlines the mi-\ncrobenchmark setups, including the hardware platforms, datasets,\n1The conclusion is drawn hastily as, in general, 1/E[ğ‘‹]â‰ E[1/ğ‘‹]for an arbitrary\nrandom variable ğ‘‹. A more rigorous proof can be found in Theorem 4 of [10].Table 4: Statistics of benchmark datasets. â„ğ·is the distribu-\ntion hardness ratio. ğ¶ğ‘œğ‘£is the observed segment coverage to\nfit a PLA model with an error bound of ğœ–=16.\nDataset Category #Keys Raw Size â„ğ·ğ¶ğ‘œğ‘£\nfb Real 200 M 1.6 GB 3.88 94\nwiki Real 200 M 1.6 GB 1.77 877\nbooks Real 800 M 6.4 GB 5.39 101\nosm Real 800 M 6.4 GB 1.91 129\nand query workloads. The remainder of this paper adopts this mi-\ncrobenchmark to either motivate or validate the theoretical findings\nand proposed methodologies.\nPlatforms. We perform the subsequent experiments on three plat-\nforms with different architectures: â¶X86-1 is an Ubuntu desk-\ntop equipped with an Intel Â©Coreâ„¢i7-13700K CPU (5.30 GHz,\nP-core) and 64 GB of memory; â·X86-2 is a CentOS server with 2\nAMD Â©EPYC â„¢7413 CPUs (3.60 GHz) and 1 TB of memory; and â¸\nARMis a Macbook Air laptop with an Apple Silicon M3 CPU (4.05\nGHz, P-core) and 16 GB of unified memory, which offers higher\nmemory bandwidth compared to the X86platforms. As we will\ndiscuss in Section 5, searching a PGM-Index is highly memory-\nbound, and factors such as cache latency and memory bandwidth\ncan significantly affect query performance2. Table 3 summarizes\nthe specifications of the benchmark platforms.\nIn addition, all the experiments are written in C++ and com-\npiled using g++ 11.4 on X86-1 andX86-2 and clang++ 15 on ARM.\nThe complete microbenchmark implementation and experimental\nresults are publicly available at [28].\nBenchmark Datasets. We adopt 4 real datasets from SOSD [ 22]\nthat have been widely evaluated in previous studies [ 8,17,44,45,\n50]. Specifically, â¶fbis a set of user IDs randomly sampled from\nFacebook [ 33];â·wiki is a set of edit timestamp IDs committed\nto Wikipedia [ 42];â¸books is the dataset of book popularity from\nAmazon; and â¹osmis a set of cell IDs from OpenStreetMap [ 25].\nWe also generate 3 synthetic datasets by sampling from uniform,\nnormal, and log-normal distributions, following a process similar\nto [22,52]. All keys are stored as 64-bit unsigned integers ( uint64_t\nin C++), and Table 4 summarizes the dataset statistics.\nTo quantify the difficulty of indexing a dataset, we define the\ndistribution hardness ratio asâ„ğ·=ğœ2/ğœ‡2whereğœ‡andğœ2represent\nthe mean and variance of the gap distribution for a dataset. Accord-\ning to Theorem 2.5, a higher â„ğ·implies a harder dataset to learn, as\nmore segments are required to meet the error constraint ğœ–. However,\nas illustrated in Figure 3, extreme values can easily influence â„ğ·,\nleading to an overestimation of the necessary segment count. For\nexample, on dataset osm, the original hardness ratio â„ğ·=1.27Ã—106,\nand according to Theorem 2.5, the expected segment coverage for\n2Typical access latencies for L1 cache, last level cache (LLC), and main memory are 1\nns, 20 ns, and 100 ns, respectively.\n4\n\nfb fb wiki wiki osm osm books books\ndataset1011041071010101310161019gap value\n2.86e7\n3.881288\n1.771.27e6\n5.392.861.91Raw Gaps Clipped Gaps Hardness (sigma/mu)^2Figure 3: Gap distributions for 4 real datasets. In the box\nplots, the horizontal lines and the star marks refer to the\nmedians and means of data, respectively.\nanğœ–-PLA withğœ–=16can be estimated as,\nğœ‡2Â·ğœ–2\nğœ2=ğœ–2\nâ„ğ·=162\n1.27Ã—106â‰ˆ0.0002, (3)\nwhich is far away from 129, the observed segment coverage.\nTo mitigate this, we clip the observed gaps at the 1%- and 99%-\nquantiles and re-calculate â„ğ·based on the clipped gaps (as reported\nin columnâ„ğ·of Table 4). After removing the extreme gaps, the\nrevisedâ„ğ·=5.39on dataset osm, and the corresponding estimated\nsegment coverage is 162/5.39â‰ˆ71.3, which is much closer to\nthe observed value. Notably, accurately estimating the segment\ncoverage is vital to building an effective cost model. The estimator\nbased on clipped gaps remains too coarse , as it fails to capture local\ndata variations. In Section 6.2, we will develop a more fine-grained\ncoverage estimator based on adaptive data partition.\nQuery Workloads. Similar to the settings in [ 11,17,18,50], our\nwork focuses on in-memory read-heavy workloads. Given a key\nsetK, we generate the query workload by randomly sampling ğ‘†\n(by defaultğ‘†=5,000) keys fromK. To simulate different access\npatterns, we sample lookup keys from two distributions: â¶Uniform ,\nwhere every key in Khas an equal likelihood of being sampled;\nandâ·Zipfan , where the probability of sampling the ğ‘–-th key inK\nis given byğ‘(ğ‘–)=ğ‘–ğ›¼/Ãğ‘\nğ‘—=1ğ‘—ğ›¼. For the Zipfan workload, by default,\nwe set the parameter ğ›¼=1.3such that over 90% of index accesses\noccur within the range of (0,103].\n4 WHY ARE PGM-INDEXES SO EFFECTIVE?\nIn this section, we first motivate the necessity of an index height\nlower than ğ‘‚(logğ‘)through benchmark results ( â–·Section 4.1).\nThen, we establish a tighter sub-logarithmic bound ( â–·Section 4.2).\nFinally, a case study on uniformly distributed keys is provided to\nfurther validate our theoretical analysis ( â–·Section 4.3).\n4.1 Motivation Experiments\nWe construct the PGM-Indexes and B+-trees using various config-\nurations, with index statistics summarized in Table 5. Intuitively,\na B+-tree with a fan-out of ğµ=ğœ–can be considered analogous to\na PGM-Index where ğœ–ğ‘–=ğœ–â„“=ğœ–, since a B+-tree index guarantees\nthe search key to be located within a data block of size ğµ.\nAs shown in Table 5, we first fix ğµ=ğœ–=16while varying the\ninput data size across {103,104,Â·Â·Â·,109}using synthetic uniform\nkeys. As the data size ğ‘increases, the height of a B+-tree index\n(ğ»ğµ) follows a logarithmic growth pattern, adhering to the formula\nğ»ğµ=âŒˆ1+logğµğ‘+1\n2âŒ‰. On the other hand, the PGM-Index heightTable 5: Statistics of PGM-Index ( ğœ–ğ‘–=ğœ–â„“=ğœ–) and B+-tree\n(fan-outğµ=ğœ–) under different configurations. ğœ–is fixed to\n8 when varying data size ğ‘(synthetic uniform keys), and\nğ‘is fixed to 800M when varying ğœ–(real dataset books ). The\nratio in percentage refers to the proportion of leaf segments\ncontributing to the total index memory footprint.\nğ‘PGM\nHeightLeaf\nSegmentsInternal\nSegments% over\nTotalB+-tree\nHeight\n1032 2 2 50.0% 4\n1042 16 2 88.9% 6\n1052 140 2 98.6% 7\n1062 1,388 2 99.9% 8\n1073 13,918 12 99.9% 9\n1083 139,376 109 99.9% 10\n1094 1,394,003 1,049 99.9% 11\nğœ–(ğµ)PGM\nHeightLeaf\nSegmentsInternal\nSegments% over\nTotalB+-tree\nHeight\n8 4 16,859,902 46,572 99.7% 11\n16 4 7,943,403 4,100 99.9% 9\n32 3 2,464,229 272 99.99% 7\n64 3 797,152 60 99.99% 6\n128 3 267,966 25 99.99% 6\n256 3 81,340 12 99.99% 5\n512 3 22,684 7 99.99% 5\n(ğ»ğ‘ƒğºğ‘€ ) grows at a much slower, sub-logarithmic rate. Besides, on\ndataset books , when varying ğœ–within{22,23,Â·Â·Â·,210}, the results\nconsistently demonstrate that ğ»ğ‘ƒğºğ‘€â‰ªğ»ğµholds across all ğœ–con-\nfigurations. Moreover, the decrease in ğ»ğ‘ƒğºğ‘€ relative toğœ–is also\nnotably slower than that of ğ»ğµ. In addition to the index height, we\nalso report the numbers of leaf and internal segments in Table 5.\nUnlike B+-trees or other BST variants, the PGM-Index exhibits a\nhighly â€œflatâ€ structure, with most of the line segments (up to 99.99% )\nlocated at the bottom level, aligning with its slow height growth.\nNotably, the results obtained from different datasets and addi-\ntionalğœ–configurations are similar and therefore omitted here due\nto page limitations. The complete results are available at [28].\n4.2 Theoretical Analysis\nIn this section, we aim to provide a new bound tightening the\nprevious results. The road map for establishing our theoretical\nresults is outlined below.\nâ¶Lemma 4.1 and Lemma 4.2 provide a lower bound for the expected\nsegment coverage in an arbitrary level of the PGM-Index;\nâ·Theorem 4.3 derives the PGM-Index height as ğ‘‚(log logğ‘), in-\ndicating sub-logarithmic growth w.r.t. the data size ğ‘;\nâ¸Theorem 4.4 concludes the space and time complexities of the\nPGM-Index as summarized in Table 2.\nNotably, unless explicitly stated otherwise, the subsequent anal-\nyses adhere to the core assumptions regarding gaps from Theo-\nrem 2.5 [ 10], that is, the gaps are i.i.d. random variables following\nsome unknown distribution with expectation ğœ‡and variance ğœ2.\nBesides, as discussed in [ 10], the â€œi.i.d.â€ assumption can be further\nrelaxed to weakly correlated random variables without affecting the\ncorrectness of theoretical results.\nLemma 4.1 (Expected Coverage Recursion). Given a set of ğ‘\nsorted keysK={ğ‘˜1,Â·Â·Â·,ğ‘˜ğ‘}and an error parameter ğœ–, let a random\n5\n\nvariableğ¶ğ‘–denote the number of keys in the (ğ‘–âˆ’1)-th level that a\nsegment in the ğ‘–-th level can cover (i.e., satisfying the error constraint\nğœ–). Specifically, ğ¶0denotes the leaf segment coverage (i.e., level-0) on\nthe input key setK. Then, the following recursion holds for E[ğ¶ğ‘–],\nE[ğ¶ğ‘–]=ğœ‡2Â·ğœ–2\nğœ2Â·E[ğ¶0Â·ğ¶1Â·Â·Â·ğ¶ğ‘–âˆ’1]. (4)\nProof. According to the law of total expectation [5],\nE[ğ¶ğ‘–]=âˆ«\nÂ·Â·Â·âˆ«\nE[ğ¶ğ‘–|ğ¶0=ğ‘0,Â·Â·Â·,ğ¶ğ‘–âˆ’1=ğ‘ğ‘–âˆ’1]Ã—\nğ‘“(ğ‘0,Â·Â·Â·,ğ‘ğ‘–âˆ’1)ğ‘‘ğ‘0Â·Â·Â·ğ‘‘ğ‘ğ‘–âˆ’1,(5)\nwhereğ‘“(ğ‘0,Â·Â·Â·,ğ‘ğ‘–âˆ’1)is the joint probability density function of\nrandom variables ğ¶0,Â·Â·Â·,ğ¶ğ‘–âˆ’1.\nSuppose that ğ‘”â€™s are the gaps of the original key set K. When\nfixingğ¶0,Â·Â·Â·,ğ¶ğ‘–âˆ’1toğ‘0,Â·Â·Â·,ğ‘ğ‘–âˆ’1, as illustrated in Figure 4, w.l.o.g.,\nan arbitrary gap in the ğ‘–-th level, denoted by ğ‘”(ğ‘–), should be the\nsum ofğ‘0Â·ğ‘1Â·Â·Â·ğ‘ğ‘–âˆ’1consecutive gaps on the raw key set K3.\nThus, according to Theorem 2.5, on a key set with gaps as ğ‘”(ğ‘–), the\nexpected segment coverage (conditioned on ğ¶0,Â·Â·Â·,ğ¶ğ‘–âˆ’1) in the\nğ‘–-th level for an ğœ–-PLA should be,\nE[ğ¶ğ‘–|ğ¶0=ğ‘0,Â·Â·Â·,ğ¶ğ‘–âˆ’1=ğ‘ğ‘–âˆ’1]=E\u0002\nğ‘”(ğ‘–)\u00032Â·ğœ–2\nVar\u0002\nğ‘”(ğ‘–)\u0003\n=EhÃğ‘—+ğ‘0Â·ğ‘1Â·Â·Â·ğ‘ğ‘–âˆ’1\nğ‘—â€²=ğ‘—ğ‘”ğ‘—â€²i2\nÂ·ğœ–2\nVarhÃğ‘—+ğ‘0Â·ğ‘1Â·Â·Â·ğ‘ğ‘–âˆ’1\nğ‘—â€²=ğ‘—ğ‘”ğ‘—â€²i\n=(ğ‘0Â·ğ‘1Â·Â·Â·ğ‘ğ‘–âˆ’1)Â·ğœ‡2Â·ğœ–2\nğœ2,(6)\nwhereğœ‡andğœ2are the mean and variance of the gaps on the original\nkey setK. Taking Eq. (6) into the integral in Eq. (5), we have,\nE[ğ¶ğ‘–]=ğœ‡2Â·ğœ–2\nğœ2âˆ«\nÂ·Â·Â·âˆ«ğ‘–âˆ’1Ã–\nğ‘—=0ğ‘ğ‘—Â·ğ‘“(ğ‘0,Â·Â·Â·,ğ‘ğ‘–âˆ’1)ğ‘‘ğ‘0Â·Â·Â·ğ‘‘ğ‘ğ‘–âˆ’1\n=ğœ‡2Â·ğœ–2\nğœ2Â·E[ğ¶0Â·ğ¶1Â·Â·Â·ğ¶ğ‘–âˆ’1].(7)\nThus, we have the statement in Lemma 4.1. â–¡\nLemma 4.2 (Expected Coverage of Level- ğ‘–).The following\nlower bound holds for E[ğ¶ğ‘–],\nE[ğ¶ğ‘–]â‰¥\u0012ğœ‡2Â·ğœ–2\nğœ2\u00132ğ‘–\n. (8)\nProof. We prove Lemma 4.2 using mathematical induction.\nâ¶Base Case ( ğ‘–â€²=0):According to Theorem 2.5, E[ğ¶0]=ğœ‡2ğœ–2/ğœ2,\nsatisfying the inequality in Eq. (8).\nâ·Inductive Step: Assume that the lower bound in Eq. (8)holds\nforğ‘–â€²=ğ‘–âˆ’1, i.e.,\nE[ğ¶ğ‘–âˆ’1]â‰¥\u0012ğœ‡2Â·ğœ–2\nğœ2\u00132ğ‘–âˆ’1\n. (9)\nThen, for the case of ğ‘–â€²=ğ‘–, according to Lemma 4.1, we have,\nE[ğ¶ğ‘–]=ğœ‡2Â·ğœ–2\nğœ2Â·E[ğ¶0Â·ğ¶1Â·Â·Â·ğ¶ğ‘–âˆ’1]\nâ‰¥ğœ‡2Â·ğœ–2\nğœ2Â·E[ğ¶0Â·ğ¶1Â·Â·Â·ğ¶ğ‘–âˆ’2]Â·E[ğ¶ğ‘–âˆ’1],(10)\n3Here, we assume that all line segments within the same level exhibit equal coverage.\nA more rigorous analysis can be established by using concentration bounds like\nChebyshevâ€™s inequality or Chernoff bound [5], which is omitted here for brevity.\nNew Gap in the\ni-th Level\nGaps in the\n( i-1)-th LevelFigure 4: Illustration of gaps for the next level. Suppose ğºis\nthe segment coverage for the current level. The new gap in\ntheğ‘–-th level is ğ‘”(ğ‘–)=Ãğ‘—+ğºâˆ’1\nğ‘—â€²=ğ‘—+1ğ‘”(ğ‘–âˆ’1)\nğ‘—â€²whereğ‘”(ğ‘–âˆ’1)\nğ‘—â€²is theğ‘—â€²-th\ngap in the(ğ‘–âˆ’1)-th level.\nconsidering that ğ¶ğ‘–âˆ’1is positively correlated with ğ¶0Â·ğ¶1Â·Â·Â·ğ¶ğ‘–âˆ’2.\nBy the inductive hypothesis (i.e., Eq. (9)), we have,\nE[ğ¶ğ‘–]â‰¥E[ğ¶ğ‘–âˆ’1]2â‰¥\u0012ğœ‡2Â·ğœ–2\nğœ2\u00132ğ‘–\n, (11)\nwhich satisfies the lower bound for ğ‘–â€²=ğ‘–. Thus, by induction, we\nconclude that Lemma 4.2 holds for all ğ‘–. â–¡\nTheorem 4.3 (PGM-Index Height). Given a setKofğ‘sorted\nkeys, denote the constant ğº=ğœ‡2ğœ–2/ğœ2, w.h.p., the height of a PGM-\nIndex with error parameter ğœ–ğ‘–=ğœ–â„“=ğœ–is bounded by\nğ»ğ‘ƒğºğ‘€ =ğ‘‚(log2logğºğ‘)=ğ‘‚(log logğ‘). (12)\nProof. Here we only provide an intuitive proof sketch due to the\npage limit. A more rigorous proof can be established by employing\na similar technique as introduced in Theorem 4 of [10].\nAccording to Definition 2.2, the construction of a PGM-Index ter-\nminates when the current level consists of exactly one line segment\n(i.e., reaching the root level). Intuitively, the index height ğ»ğ‘ƒğºğ‘€\ncan be solved by letting\nğ‘\nÃğ»ğ‘ƒğºğ‘€âˆ’1\nğ‘–=0E[ğ¶ğ‘–]=ğ‘‚(1). (13)\nAccording to Theorem 4.2, we have,\nğ»ğ‘ƒğºğ‘€âˆ’1Ã–\nğ‘–=0E[ğ¶ğ‘–]â‰¥ğ»ğ‘ƒğºğ‘€âˆ’1Ã–\nğ‘–=0ğº2ğ‘–â‰¥ğºÃğ»ğ‘ƒğºğ‘€âˆ’1\nğ‘–=02ğ‘–â‰¥ğº2ğ»ğ‘ƒğºğ‘€. (14)\nThus, Eq. (13) can be solved by ğ»ğ‘ƒğºğ‘€=ğ‘‚(log2logğºğ‘).â–¡\nTheorem 4.4 (Space and Time Complexity). Given a setKof\nğ‘sorted keys, a PGM-Index with ğœ–ğ‘–=ğœ–â„“=ğœ–can process an index\nlookup query in ğ‘‚(log logğ‘)time usingğ‘‚(ğ‘/ğº)space.\nProof. According to Definition 2.2 and Example 2.3, querying a\nPGM-Index requires ğ»ğ‘ƒğºğ‘€ times search operations, each within a\nrange of 2Â·ğœ–+1. According to Theorem 4.3, the total index lookup\ntime should be ğ‘‚(ğ»ğ‘ƒğºğ‘€Â·log2(2Â·ğœ–+1))=ğ‘‚(log logğ‘).\nWe further analyze the space complexity of a PGM-Index, specif-\nically the total number of line segments required to satisfy the error\nconstraintğœ–. According to Definition 2.2 and Lemma 4.2, the â„-th\nlevel contains at most ğ‘/Ãâ„\nğ‘–=0ğº2ğ‘–line segments. Thus, the upper\nbound on the total number of segments can be derived as,\nğ»ğ‘ƒğºğ‘€âˆ’1âˆ‘ï¸\nâ„=0ğ‘\nÃâ„\nğ‘–=0ğº2ğ‘–â‰¤ğ»ğ‘ƒğºğ‘€âˆ’1âˆ‘ï¸\nâ„=0ğ‘\nğºâ„+1â‰¤ğ‘Â·1âˆ’1\nğºğ»ğ‘ƒğºğ‘€\nğºâˆ’1\nâ‰¤ğ‘\nğºâˆ’1=ğ‘‚(ğ‘/ğº),(15)\nconsidering thatÃâ„\nğ‘–=0ğº2ğ‘–â‰¥Ãâ„\nğ‘–=0ğº20â‰¥ğºâ„+1.â–¡\n6\n\nTable 6: PGM-Index statistics on 10 million synthetic uniform\nkeys with different ranges.\nKey Range ğœ–Height Segments Memory ğ¶ğ‘œğ‘£\n[0,108]\nğœ‡=10\nğœ2=100.194 4 129,503 2,078 KiB 77\n8 3 37,732 604 KiB 265\n16 3 10,224 163 KiB 978\n32 2 2,666 42 KiB 3,751\n[0,109]\nğœ‡=100\nğœ2=10007.74 3 129,659 2,080 KiB 77\n8 3 37,601 602 KiB 266\n16 3 10,124 162 KiB 988\n32 2 2,665 42 KiB 3,752\n[0,1010]\nğœ‡=1000\nğœ2=9997504 3 129,586 2,079 KiB 77\n8 3 37,597 602 KiB 266\n16 3 10,217 164 KiB 979\n32 2 2,646 42 KiB 3,779\n4.3 Case Study: Uniform Keys\nPreviously, we assume that gaps are drawn from an unkonwn distri-\nbution. To further validate the correctness of our theoretical results,\nwe now provide a case study on uniformly distributed keys.\nGiven a key setK, assume that all keys ğ‘˜âˆˆK are i.i.d. samples\ndrawn from a uniform distribution U(ğ›¼,ğ›½). In this case, the ğ‘–-th gap\nonKcan be defined as ğ‘”ğ‘–=ğ‘˜(ğ‘–)âˆ’ğ‘˜(ğ‘–âˆ’1)whereğ‘˜(ğ‘–)andğ‘˜(ğ‘–âˆ’1)are\ntheğ‘–-th and(ğ‘–âˆ’1)-thorder statistics ofK(i.e., theğ‘–-th and(ğ‘–âˆ’1)-th\nsmallest values inK). Then, for an arbitrary ğ‘–=2,Â·Â·Â·,ğ‘, it can be\nshown thatğ‘”ğ‘–follows a beta distribution, ğ‘”ğ‘–âˆ¼(ğ›½âˆ’ğ›¼)Â·Beta(1,ğ‘),\nwith the following mean and variance,\nE[ğ‘”ğ‘–]=ğ›½âˆ’ğ›¼\nğ‘+1,Var[ğ‘”ğ‘–]=(ğ›½âˆ’ğ›¼)2Â·ğ‘\n(ğ‘+1)2Â·(ğ‘+2)â‰ˆ(ğ›½âˆ’ğ›¼)2\n(ğ‘+1)2. (16)\nAccording to Eq. (16), the constant ğº=ğœ‡2Â·ğœ–2\nğœ2=ğœ–2, which is interest-\ningly independent of the original key distribution. By Theorem 2.4\nand Theorem 4.4, this result implies that, for uniformly distributed\nkeys, a PGM-Index should have the same index height and memory\nfootprint as long as ğ‘andğœ–remain unchanged.\nTable 6 reports the statistics for PGM-Indexes constructed on\nthree synthetic uniform key sets with different ranges. The empiri-\ncal results further validate the correctness of the aforementioned\nanalysis, given that the index height and segment count remain\nconsistent across different data ranges, depending solely on the\nvalue of error constraint ğœ–.\nExtension to Arbitrary Key Distributions. Suppose that the\nkeysğ‘˜1,Â·Â·Â·,ğ‘˜ğ‘areğ‘i.i.d. random samples drawn from an ar-\nbitrary distribution with cumulative distribution function ğ¹(ğ‘¥)\nand density function ğ‘“(ğ‘¥). As theğ‘–-th gapğ‘”ğ‘–=ğ‘˜(ğ‘–)âˆ’ğ‘˜(ğ‘–âˆ’1), the\ndistribution characteristics like mean and variance of ğ‘”ğ‘–can be\nderived by evaluating the joint density function ğ‘“ğ‘˜(ğ‘–âˆ’1),ğ‘˜(ğ‘–)(ğ‘¥,ğ‘¦)of\ntwo consecutive order statistics ğ‘˜(ğ‘–âˆ’1)andğ‘˜(ğ‘–)[5]. For example,\nthe expectation E[ğ‘”ğ‘–]can be derived as,\nE[ğ‘”ğ‘–]=âˆ¬\n(ğ‘¦âˆ’ğ‘¥)Â·ğ‘“ğ‘˜(ğ‘–âˆ’1),ğ‘˜(ğ‘–)(ğ‘¥,ğ‘¦)ğ‘‘ğ‘¥ğ‘‘ğ‘¦,\nğ‘“ğ‘˜(ğ‘–âˆ’1),ğ‘˜(ğ‘–)(ğ‘¥,ğ‘¦)=ğ‘!Â·ğ¹(ğ‘¥)ğ‘–âˆ’2(1âˆ’ğ¹(ğ‘¦))ğ‘âˆ’ğ‘–ğ‘“(ğ‘¥)ğ‘“(ğ‘¦)\n(ğ‘–âˆ’2)!(ğ‘âˆ’ğ‘–)!.(17)\nNotably, in most cases, no closed-form solution exists for Eq. (17).\nThus, an empirical CDF (ECDF) based on random sampling can be\napplied to obtain a provably accurate approximation according to\nthe DKW bound [9].\nCPU CyclesSeg-bounded\nSear chSeg-bounded\nSear chSeg -bounded Sear ch\nLinear  Segment EvaluationMEM READ CMP MEM READ CMPCompar e(x, Data[mid]) Access Data[mid]\n...\n<10 ns>200 ns\n...Figure 5: Illustration of the CPU cycles used for searching an\n(ğœ–ğ‘–,ğœ–â„“)-PGM-Index with a standard binary search algorithm\nfor the internal error-bounded search operation.\n5 WHY ARE PGM-INDEXES INEFFECTIVE?\nThe theoretical findings in Section 4 reveal that PGM-Indexes can\nachieve the best space-time trade-off among existing learned in-\ndexes. However, according to recent benchmarks [ 22,44], an opti-\nmized RMI [18] consistently outperforms the PGM-Index by 20%â€“\n40%. Motivated by this, in this section, we aim to answer another\ncritical question: Why do PGM-Indexes underperform in practice?\nA Simple Cost Model. We begin by introducing a simplified cost\nmodel for an arbitrary (ğœ–ğ‘–,ğœ–â„“)-PGM-Index. Recalling the PGM-Index\nstructure as shown in Figure 2, the total index lookup time for a\nsearch key ğ‘˜can be modeled as the summation of the internal\nsearch cost with error constraint ğœ–ğ‘–and the last-mile search cost\nwith error constraint ğœ–â„“, i.e.,\nğ¶ğ‘œğ‘ ğ‘¡=ğ¶ğ‘œğ‘ ğ‘¡ internal+ğ¶ğ‘œğ‘ ğ‘¡ last-mile\n=(ğ»ğ‘ƒğºğ‘€âˆ’1)Â·ğ¶ğ‘†(ğœ–ğ‘–)+ğ¶ğ‘†(ğœ–â„“)+ğ»ğ‘ƒğºğ‘€Â·ğ¶ğ¿,(18)\nwhereğ»ğ‘ƒğºğ‘€ is the index height, ğ¶ğ‘†(ğœ–)represents the search cost\nwithin the range of 2Â·ğœ–+1, andğ¶ğ¿is the overhead to evaluate a\nlinear function ğ‘¦=ğ‘Â·ğ‘¥+ğ‘.\nBottleneck: Error-bounded Search. According to Theorem 2.4,\nthe index height ğ»ğ‘ƒğºğ‘€ =ğ‘‚(log logğ‘), implying that very few\ninternal searches are required (generally fewer than 5for 1 billion\nkeys). Additionally, as depicted in Figure 5, our benchmark results\nacross various datasets and platforms indicate that evaluating a lin-\near function typically takes less than 10 ns . In contrast, performing\na search with ğœ–=64takes time more than 200 ns by adopting a\nstandard binary search implementation (e.g., std::lower_bound ).\nBased on this observation, the cost model in Eq. (18)can be simpli-\nfied by neglecting the segment evaluation overhead, i.e.,\nğ¶ğ‘œğ‘ ğ‘¡â‰ˆ(ğ»ğ‘ƒğºğ‘€âˆ’1)Â·ğ¶ğ‘†(ğœ–ğ‘–)+ğ¶ğ‘†(ğœ–â„“). (19)\nThe revised cost model reveals that searching a PGM-Index is\ndominated by performing ğ»ğ‘ƒğºğ‘€ times error-bounded searches,\nwhich are generally known as memory-bound operations [ 43]. As\ndepicted in Figure 5, an ğœ–-bounded binary search typically involves\nâŒˆlog2(2Â·ğœ–+1)âŒ‰comparisons and memory accesses. Each comparison\ngenerally requires a few nanoseconds, whereas each memory access,\nif cache missed, can take approximately 100 nanoseconds due to\nthe asymmetric nature of the memory hierarchy.\nComparison to RMI. We then investigate why RMI practically\noutperforms the PGM-Index. As illustrated in Figure 6, the major\nstructural difference between RMI and PGM-Index lies in their\ninternal search mechanisms. For RMI, the model prediction ğ‘“ğ‘–,ğ‘—(ğ‘˜)\ndirectly serves as the model index for the next level (i.e., the (ğ‘–+1)-th\nlevel), thereby bypassing the costly internal error-bounded search\nused in PGM-Index. To ensure lookup correctness, the models in\n7\n\n20313449506772 10 858891 77 ... ...Query key \n111213141516171819202122 ... ...Keys\nRankLevel-0 Level-1 Level-2Figure 6: Illustration of a 3-layer RMI [ 18].ğ‘“ğ‘–,ğ‘—denotes the\nğ‘—-th model in the ğ‘–-th layer. The path in red denotes the index\ntraversal from the root model ğ‘“0,0. Notably, the root level is\nspecified as level-0, opposite to the PGM-Index.\nthe bottom layer materialize the maximum search error to perform\na last-mile error-bounded search, similar to the PGM-Index.\nTable 7 presents the detailed overheads when querying RMI and\nPGM-Index. Consistent with previous benchmark results [ 22], an\noptimized RMI implementation [ 32] outperforms the PGM-Index\nin terms of total index lookup time across all datasets. Specifically,\nfor PGM-Index, the internal search time ğ‘‡ğ‘–accounts for 69%â€“81%\nof the total index lookup overhead; in contrast, for RMI, this ratio\nis as low as 19%â€“27% , supporting our earlier claim.\nIs RMI the Best Choice? While RMI generally outperforms the\nPGM-Index, its design poses a critical limitation: RMI is hard to\nguarantee a maximum error before index construction, making its\nperformance highly data-sensitive . As shown in Table 7, RMIâ€™s maxi-\nmum error ranges from 63to3.1Ã—105, resulting in high worst-case\nlast-mile search overhead. Such â€œunpredictabilityâ€ also raises the\nchallenge of building an accurate cost model for RMI-like indexes,\nwhich is crucial for practical DBMS to perform effective cost-based\nquery optimization [ 13]. Moreover, given the identified bottleneck\nin querying a PGM-Index, a natural idea is to accelerate the costly\ninternal error-bounded search operation. In Section 6, we demon-\nstrate how a simple hybrid branchless search strategy can make\nthe â€œineffectiveâ€ PGM-Index outperform RMI.\n6 PGM++: OPTIMIZATION TO PGM-INDEX\nThis section introduces PGM++, a simple yet effective variant of\nthe PGM-Index by incorporating a hybrid error-bounded search\nstrategy (â–·Section 6.1) and an automatic parameter tuner based\non well-calibrated cost models ( â–·Section 6.2).\n6.1 Hybrid Search Strategy\nAs the error-bounded search operation is identified as the bottle-\nneck in querying the PGM-Index, our optimized structure, named\nPGM++, employs a hybrid search strategy to replace the standard\nbinary search. To start, we discuss the impact of branch misses in\nstandard binary search implementation.\nBranch Prediction and Branch Miss. Modern CPUs rely on so-\nphisticated branch predictors to enhance pipeline parallelism by\nforecasting the outcomes of conditional jump instructions (e.g., JLE\nandJAEinstructions in the X86architecture). These predictors are\nhighly effective for simple, repetitive tasks such as for loops or\npointer chasing, where the pattern of execution is predictable [ 12].\nHowever, in the case of standard binary search implementations,\nsuch as the widely used std::lower_bound , branching exhibits aTable 7: Query processing details. For PGM-Index, ğœ–ğ‘–=16and\nğœ–â„“=32. For RMI, we adopt CDFShop [ 24] to find an optimized\nRMI structure with a comparable space to the PGM-Index.\nData Index SizeMax\nErr.Internal\nTimeLast-mile\nTimeTotal\nfbPGM 16.1 MB 32 675 ns 300 ns 975 ns\nRMI 24.0 MB 568 185 ns 614 ns 799 ns\nwikiPGM 1.3 MB 32 606 ns 270 ns 876 ns\nRMI 1.0 MB 63 95 ns 317 ns 412 ns\nbooksPGM 37.6 MB 32 887 ns 208 ns 1095 ns\nRMI 40.0 MB 302 159 ns 429 ns 588 ns\nosmPGM 44.4 MB 32 824 ns 212 ns 1036 ns\nRMI 96.0 MB 311K 146 ns 636 ns 782 ns\nT* lower_bound (T *d, T k, size_t n) {\n    size_t lo = 0, hi = n - 1;\n    while (lo < hi) {\n        size_t mid = (lo + hi) / 2;\n        if (d[mid] >= k) lo = mid;\n        else hi = mid + 1;\n    }\n    return d + lo;\n} JAE/JLE  InstructionFetch\nDecode\nExecute\nWrite BackCPU Cycles\nStageCPU Cycles1 2 3Instruction Flow Abandoned\n(a) Branchy sear ch with branch missing . (b) Branchless sear ch with CMOV  instruction.Branch Instruction\ne.g. JLE on x86\nRe-build Pipeline\nFetch\nDecode\nExecute\nWrite Back\nStage\nT* lower_bound_brl (T *d, T k, size_t n) {\n    T *base = d; size_t l = n;\n    while (l > 1) {\n        size_t half = l / 2;\n        base += (base[half -1] < k) * half;\n        l -= half;\n    }\n    return base;\n}CMOV  InstructionPipeline Stall\nFigure 7: Illustration of the CPU pipeline status for executing\n(a) standard binary search ( std::lower_bound ) and (b) branch-\nless binary search enabled by CMOV instruction.\nrandom pattern , leading to a high branch miss rate of approximately\n50% [34]. As depicted in Figure 7(a), a branch miss stalls the en-\ntire CPU pipeline until the branch condition is resolved (e.g., the\ncomparison d[mid]>=k in line 5 of function lower_bound ).\nBranchless Binary Search. A simple optimization [ 34] to the\nstandard binary search is to remove the branches by conditional\nmove instructions (e.g., CMOV onX86andMOVGE onARM), which\nallow both sides of a branch to execute and keeps the valid one\nbased on the evaluated condition. As illustrated in Figure 7(b),\neliminating branches (function lower_bound_brl ) maximizes the\nCPU pipeline utilization, yielding up to a 51% reduction in total\nsearch time. Notably, CMOV is not the â€œsilver bulletâ€ as it disables\nthe native branch predictor and incurs extra overhead due to its\nintrinsic complexity. On large datasets (>LLC size), the performance\ngap between branchy and branchless searches diminishes as the\nmemory access latency dominates the total overhead. However,\nsuch extra overhead is negligible particularly when the search range\nfits within the L2 cache, making CMOV performance-worthy in PGM-\nIndex (usually ğœ–â‰¤1024).\nBenchmark Results. Figure 8 presents the benchmark result for\nbranchy binary search ( std::lower_bound from STL), branchless\nbinary search (similar to lower_bound_brl in Figure 7(b)), and lin-\near scan, tested on synthetic uint64_t key sets of varying sizes.\nThe results indicate that, on both ARMandX86platforms, branch-\nless search demonstrates superior performance across a wide range\nof data sizes, excluding very small sets (e.g., ğ‘â‰¤16), where the\n8\n\n20232629212215218221\ndata size0100200300400500600700800latency (ns)\nL1 Cache\nL2 CacheLinear Scan\nBranchlessstd::lower_bound(a) Platform ARM.\n20232629212215218221\ndata size0100200300400500600700800latency (ns)\nL1 Cache\nL2 Cache\nL3 CacheLinear Scan\nBranchlessstd::lower_bound (b) Platform X86-1 .\nFigure 8: Latency w.r.t. data size for linear search, binary\nsearch ( std::lower_bound ), and branchless binary search.\nlinear scan is more efficient. Compared to std::lower_bound , our\nbranchless search implementation achieves a performance improve-\nment of approximately 1.2Ã—to1.6Ã—.\nIt is noteworthy that other search algorithms, like k-ary search\nand interpolation search [ 34], are not included in this comparison.\nThis is because, the search range in the PGM-Index is typically small\n(e.g.,ğœ–â‰¤1024 ), where more advanced search algorithms do not\nconsistently outperform a branchless binary search. Additionally,\nwe do not consider architecture-aware optimizations like SIMD and\nmemory pre-fetching, as our work is intended to provide a detailed\ntheoretical and experimental revisit of the original PGM-Index [ 11].\nThe simple hybrid search strategy, as described below, is sufficient\nto showcase the potential of PGM-Index.\nHybrid Search. Based on the above discussion and benchmark\nresults, our PGM++ adopts the following hybrid search operator:\nhybrid _search =\u001a\nlinear _scan if Search Rangeâ‰¤ğ›¿\nlower _bound _brl if Search Range >ğ›¿\nwhereğ›¿is a threshold to switch to linear search (8 on ARM/X86-1 ,\nand 16 on X86-2 ). Then, as illustrated in Figure 9, PGM++ processes\nan index lookup query as follows. Stepâ¶: Starting from the root\nlayer, identify the layer ğ‘™where the next layerâ€™s segment count\nexceedsğ›¿and skip all the layers before ğ‘™.Stepâ·: Starting from layer\nğ‘™, perform internal searches (using hybrid_search ) with error bound\nğœ–ğ‘–until reaching the bottom layer. Stepâ¸: Perform last-mile search\non sorted keys (using hybrid_search ) with error bound ğœ–â„“. Notably,\nthe specific search strategy for each layer can be determined at\ncompile time, without introducing any extra runtime overhead.\nRecalling our theoretical findings in Section 4, the height of PGM-\nIndex grows at a sub-logarithmic rate of ğ‘‚(log logğ‘), leading to an\nextremely flathierarchical structure where the non-bottom layers\ncontain very few line segments. Due to the structural invariance\nof PGM-Index (as defined in Definition 2.2), instead of recursively\nsearching from the root, PGM++ skips all layers until reaching the\nfirst layer whose next layer is considered dense (segment count\n>ğ›¿). This strategy, outlined in Step â¶, effectively reduces search\noverhead, particularly in a cold-cache environment.\n6.2 Calibrated Cost Model\nTo efficiently and effectively determine the error bounds for internal\nsearch (ğœ–ğ‘–) and last-mile search ( ğœ–â„“), we first develop cost models\nthat estimate the space and time overheads without the need for\nphysically constructing the PGM-Index.\nSpace Cost Model. According to Section 4.1 and Table 5, the space\noverhead of a PGM-Index is predominantly determined by the\nnumber of segments in the bottom layer (denoted as ğ¿), which\naccounts for up to >99.9%of the total space cost. Therefore, to\nSegment Cnt: 1\nSegment Cnt: 3\nSegment Cnt: 9,312\nSegment Cnt: 2,120,486Skip\nlinear_scan\nRaw Data Size: 200,000,000lower_bound_brl\nlower_bound_brllower_bound_brlSear ch Key\nInternal\nSegments\nLeaf\nSegmentsLinear  Scan\nThreshold: 16\nDecided at\nCompile TimeFigure 9: A toy example of the hybrid search strategy.\nsimplify the space cost model, we focus solely on the leaf segments\nand ignore the internal segments. According to the results in [ 10]\n(i.e., Theorem 2.5), ğ¿âˆğ‘ğœ2/ğœ–2\nâ„“ğœ‡2, whereğœ‡andğœ2refer to the mean\nand variance of gaps on the input sorted keys, respectively.\nHowever, as discussed in Section 3, this estimation, which re-\nlies on the global gap distribution, is often too coarse for practical\ndatasets due to the inherent heterogeneity in gap distributions. To\ndevelop a more fine-grained cost model, we partition the gaps into\na set of consecutive and disjoint chunks P(withÃ\nğ‘ƒâˆˆP|ğ‘ƒ|=ğ‘)\nby using a kernel-based change-point detection algorithm [ 2]. As-\nsuming that gaps are identically distributed within each partition\nğ‘ƒâˆˆP, the refined estimator for ğ¿becomes:\nğ¿(ğœ–â„“)âˆâˆ‘ï¸\nğ‘ƒâˆˆPğ‘ğ‘ƒğœ2\nğ‘ƒ/ğœ–2\nâ„“ğœ‡2\nğ‘ƒ, (20)\nwhereğ‘ğ‘ƒ,ğœ‡ğ‘ƒ, andğœ2\nğ‘ƒrepresent the size, mean, and variance of\ngaps within partition ğ‘ƒâˆˆP, respectively. The total space cost of\nan(ğœ–ğ‘–,ğœ–â„“)-PGM-Index is then given by ğ‘€=ğ¿(ğœ–â„“)Â·sizeof(ğ‘ ğ‘’ğ‘”),\nwhere sizeof(ğ‘ ğ‘’ğ‘”)is the number of bytes required to encode a line\nsegmentğ‘ ğ‘’ğ‘”=(ğ‘ ,ğ‘,ğ‘). Typically, sizeof(ğ‘ ğ‘’ğ‘”)=24foruint64_t\nkeys and double slope and intercept.\nTime Cost Model. According to the discussions in Section 5, the\nmajority of the index lookup overhead comes from the recursively\ninvoked error-bounded search operations. As we adopt a hybrid\nsearch strategy, the simplified cost model introduced in Eq. (19)can\nbe further refined as follows,\nğ¶ğ‘œğ‘ ğ‘¡(ğœ–ğ‘–,ğœ–â„“)=ğ¶ğ‘œğ‘ ğ‘¡ internal+ğ¶ğ‘œğ‘ ğ‘¡ last-mile (21a)\nğ¶ğ‘œğ‘ ğ‘¡ last-mile =âŒˆlog2(2Â·ğœ–â„“+1)âŒ‰Â·ğ¶miss (21b)\nğ¶ğ‘œğ‘ ğ‘¡ internal =(ğ»ğ‘ƒğºğ‘€âˆ’1)Â·(ğ¶ğ‘†(ğœ–ğ‘–)+ğ¶segment) (21c)\nğ¶ğ‘†(ğœ–ğ‘–)=(\nğ¶linear if2Â·ğœ–ğ‘–+1â‰¤ğ›¿\nâŒˆlog2(2Â·ğœ–ğ‘–+1)âŒ‰Â·ğ¶hit if2Â·ğœ–ğ‘–+1>ğ›¿(21d)\nğ»ğ‘ƒğºğ‘€âˆlog2logğœ‡2ğœ–2\nğ‘–/ğœ2ğ¿(ğœ–â„“) (21e)\nwhere (a) constants ğ¶missandğ¶last-mile are the memory access costs\nwhen missing or hitting L1/L2 cache; (b) constant ğ¶segment refers\nto the overhead of evaluating a linear function ğ‘¦=ğ‘Â·ğ‘¥+ğ‘; (c)\nconstantğ¶linear is the cost of performing a linear search within\nthe range of 2Â·ğœ–ğ‘–+1; and (d)ğ¿(ğœ–â„“)is the count of leaf segments\nestimated by Eq. (20). Notably, all constants in the cost model are\nestimated by probe datasets for each platform.\nIn Eq. (21b) , we assume a cold-cache environment, as the raw\nkey setKis large enough and the access to Kis too random for\nhardware prefetchers to be effective. Conversely, in Eq. (21d) , we as-\nsume a hot-cache environment, since the non-bottom layers contain\nvery few segments, making it highly likely for these segments to be\ncache-resident after processing a few queries. It is noteworthy that\n9\n\n481632641282565121024\nlast-mile error \n4\n8\n16\n32\n64\n128\n256\n512\n1024internal error i\n450500550600650700750800\n481632641282565121024\nlast-mile error \n4\n8\n16\n32\n64\n128\n256\n512\n1024internal error i\n650700750800\n2223242526272829210\ninternal error i\n500600700avg. lookup cost\n=22\n=24\n=28\n(a) Lookup cost on wiki.\n2223242526272829210\ninternal error i\n650700750800850avg. lookup cost\n=22\n=24\n=28\n (b) Lookup cost on books .\nFigure 10: Observed index lookup overhead (unit: ns) of\nPGM++ on x86-1 w.r.t. different combinations of (ğœ–ğ‘–,ğœ–â„“).\nwhen index data can be well-cached by the CPU, the segment com-\nputation overhead becomes non-negligible. That is why Eq. (21c)\nincludes an additional term ğ¶segment .\nPGM-Index Parameter Tuning. With the space and time cost\nmodels, the two error parameters, ğœ–ğ‘–andğœ–â„“, can be automatically\nconfigured by minimizing the potential lookup cost while satisfy-\ning a pre-specified space constraint. Stepâ¶: Given a rough index\nstorage budget ğµ, according to Eq. (20), ğœ–â„“can be estimated by\neğœ–â„“=âˆšï¸‚\nsizeof(ğ‘ ğ‘’ğ‘”)\nğµâˆ‘ï¸\nğ‘ƒâˆˆPğ‘ğ‘ƒğœ2\nğ‘ƒ/ğœ‡2\nğ‘ƒ. (22)\nStepâ·: With a determined ğœ–â„“=eğœ–â„“,ğœ–ğ‘–can be derived by minimizing\nthe index search overhead as formulated in Eq. (21a), i.e.,\neğœ–ğ‘–=arg minğœ–ğ‘–âˆˆEğ¶ğ‘œğ‘ ğ‘¡(ğœ–ğ‘–,eğœ–â„“), (23)\nwhereEis the set of possible values for ğœ–ğ‘–(E={2ğ‘—|ğ‘—=2,Â·Â·Â·,10}\nin our implementation). Intuitively, to minimize Eq. (21a) ,ğœ–ğ‘–should\nneither be too large nor too small. According to the cost model,\na largerğœ–ğ‘–increases the overhead of ğ¶ğ‘†(ğœ–ğ‘–)in Eq. (21d) , while a\nsmallerğœ–ğ‘–results in more layers to traverse (i.e., ğ»ğ‘ƒğºğ‘€ in Eq. (21e) ).\nNotably, although Eq. (23)has an analytical solution by solving\nğœ•ğ¶ğ‘œğ‘ ğ‘¡\nğœ•ğœ–ğ‘–=0, in practice, we simply enumerate all possible ğœ–ğ‘–âˆˆEto\nfind the optimal value, as Eis typically a small set ( |E|<10).\nFigure 10 reports the results of the observed index lookup costs\nw.r.t. different values of ğœ–ğ‘–andğœ–â„“. When fixing ğœ–â„“, the time cost\nw.r.t.ğœ–ğ‘–exhibits a â€œUâ€-shaped pattern, consistent with our earlier\nanalysis based on the established cost model.\nTakeaways. Our cost model for PGM++ can be easily extended to\nanyPGM-Index variants like [ 11,52]. In contrast to existing cost\nmodels for learned indexes (mostly based on RMI) like [ 50], our cost\nmodel is workload-independent , relying solely on gap distribution\ncharacteristics and platform-aware cost constants. These features\nenhance the robustness of parameter tuning, as the cost is optimized\nforallqueries rather than being tailored to a specific workload.\n7 EXPERIMENTAL STUDY\nIn this section, we present the major benchmark results to answer\nthe vital question that whether PGM++ is capable of reversing the\nâ€œineffectiveâ€ scenario of PGM-Indexes. The experimental setups\nhave been detailed in Section 3.7.1 Overall Evaluation\nBaseline and Implementation. We implement and evaluate three\nlearned indexes: â¶RMI, the optimized recursive model index [ 18,\n22],â·PGM , the original PGM-Index implementation [ 11,29], and\nâ¸PGM++ , our optimized PGM-Index variant. For RMI, we adopt\nCDFShop [ 24] to produce a set of optimal RMI configurations under\nvarious index sizes. For PGM , we construct 9Ã—9PGM-Indexes with\n(ğœ–ğ‘–,ğœ–â„“)âˆˆEÃ—E andE={22,Â·Â·Â·,210}. Then, for each ğœ–â„“âˆˆE, the\nfastest PGM-Index is reported. Similarly, for PGM++ , we adopt the\nPGM-Index configuration tuned by cost models (Section 6.2) for\neachğœ–â„“âˆˆE. For PGM andPGM++ , according to Eq. (20), eachğœ–â„“\ncorresponds to an index storage budget.\nWe do not consider other PGM or RMI variants, such as the\ncache-efficient RMI [ 50] or the IO-efficient PGM-Index [ 52]. This\nis because this work primarily aims at exploring the theoretical\naspect and performance bottlenecks inherent in the PGM-Index.\nOur findings, however, possess a broader applicability, as they can\nbe generalized to anyPGM-like indexes. This study also excludes\nnon-learned baselines like B+-tree variants as they have been exten-\nsively compared in previous learned index benchmarks like [ 22,44].\nOverall Evaluation. Figure 11 presents the trade-offs between\nindex lookup overhead and storage cost across all seven datasets\nand three platforms on Uniform query workloads. The results show\nthat, in terms of index lookup time, PGM++ consistently outper-\nforms PGM by a factor of 1.2Ã—âˆ¼2.2Ã—with the same index size,\nsupporting our bottleneck analysis for PGM-Indexes (Section 5). In\ncontrast to the optimized RMI, our PGM++ addresses the costly\ninternal index traversal through a hybrid search strategy, generally\ndelivering better or, in some cases, comparable lookup efficiency,\nachieving speedups of up to 1.56Ã—. An outlier case is on dataset\nnormal ,RMI significantly outperforms PGM++ andPGM . The rea-\nson is that the optimized RMI, based on CDFShop [ 24], adopts\nnon-linear models (with the best RMI uses cubic splines), which\ncan fit normal keys very well (maximum error <4). However, on\nother datasets, especially complex real-world datasets, RMI fell\nshort in fitting the data with constrained error limits, leading to\ncostly last-mile search overhead as discussed in Section 5.\nSpace-time Trade-off. In most cases, PGM++ offers the best space-\ntime trade-off. However, interestingly, unlike RMI, whose perfor-\nmance improves with increased index memory usage, PGM++ ex-\nhibits an â€œirregularâ€ pattern in its time-space relationship. This is\nbecause PGM++ is specifically optimized for query efficiency at\na given storage budget. Leveraging accurate cost models, our pa-\nrameter tuner can find configurations to provide competitive query\nefficiency, even under limited space constraints. For example, on\ndataset wiki ,PGM++ uses just 0.1 MB of memory to outperform\nanRMI with over 100 MB space.\nInfluence of Architecture. From Figure 11, the comparison re-\nsults vary across different platforms. For dataset osm, compared to\nPGM ,PGM++ achieves an average speedup ratio of 1.78Ã—onx86-1\nandarm. However, such a speedup decreases to 1.32Ã—on platform\nx86-2 . This is because the memory access latency on x86-2 is much\nhigher than that on x86-1 , which reduces the improvement brought\nby adopting the hybrid search strategy.\nEffects of Workloads. We also evaluate an extreme query work-\nload, Zipfan , though the results are not included in this paper due\n10\n\n1061071084008001200x86-1\n latency (ns)\nfb\n105106107108\nwiki\n106107108\nosm\n105107109\nbooks\n104106108\nuniform\nPGM\n104105\nnormal\nPGM++\n104105106107\nlognormal\nRMI\n10610710850010001500x86-2\n latency (ns)\n105106107108\n106107108\n105107109\n104106108\n104105\n104105106107\n106107108\nsize (bytes)50010001500arm\n latency (ns)\n105106107108\nsize (bytes)\n106107108\nsize (bytes)\n104106108\nsize (bytes)\n104106108\nsize (bytes)\n104105\nsize (bytes)\n104105106107\nsize (bytes)\nFigure 11: Space and time tradeoffs for seven datasets on three platforms (workload: Uniform ).\nto space limits. Queries sampled from a Zipfan distribution exhibit a\nhighly long-tail pattern, where the first 1K elements are frequently\naccessed (Section 3). Under this workload, PGM++ ,PGM , and RMI\nall achieve lower query latencies by up to 1.77Ã—,2.13Ã—, and 4.58Ã—,\nrespectively, compared to their performance on Uniform workloads.\nRMI shows the most substantial gains, as the last-mile search cost\ndominates the total index lookup time ( >90%). This phase benefits\ngreatly from the spatial locality inherent in the Zipfan workload,\nwhere frequently accessed memory is more likely to be cached.\n7.2 Cost Model and Parameter Tuner\nSpace Cost Model. According to Section 6.2, the leaf segment count\n(ğ¿) dominates the PGM-Index space cost. Here, we evaluate three\ndifferent segment count estimators: (a) SIMPLE , which directly\napplies Theorem 2.5 on the entire gap distribution; (b) CLIP , which\napplies Theorem 2.5 on the gaps excluding extreme values (<0.01-\nquantile or >0.99-quantile); and (c) ADAP , which partitions gaps\ninto disjoint chunks and aggregates the segment count estimated\nfor each chunk (as in Eq. (20)).\nAs shown in Figure 12, compared to the true segment count\n(TRUE ),ADAP consistently achieves accurate estimations across\nall seven datasets, nearly overlapping the TRUE line. In addition,\nexcluding uniformly distributed datasets (e.g., books anduniform ),\nSIMPLE performs the worst, validating our discussion in Section 3\nthat extreme gap values significantly affect estimation accuracy.\nNotably, CLIP also delivers accurate results on real datasets fb,\nwiki , and osm. This is because, on these datasets, the gaps are\nnearly identically distributed after removing the extreme values,\nthus better satisfying the requirement of using Theorem 2.5.\nTime Cost Model. For each pair of(ğœ–ğ‘–,ğœ–â„“)âˆˆEÃ—E , whereE=\n{2ğ‘—|ğ‘—=2,3,Â·Â·Â·,10}, we estimate the index lookup overhead as\nğ¶ğ‘œğ‘ ğ‘¡(ğœ–ğ‘–,ğœ–â„“)using the time cost model (i.e., Eq. (21a) â€“Eq. (21e) ), and\nthen physically construct the corresponding (ğœ–ğ‘–,ğœ–â„“)-PGM-Index to\nmeasure the actual lookup time (averaged over a given workload).\nFigure 13 visualizes the relationship between the true index\nlookup overhead and the cost modelâ€™s estimation. The closer the\npoints in Figure 13 are to the line ğ‘¦=ğ‘¥, the more accurate theestimation. From the results, our cost model closely approximates\nthe true index lookup overhead, especially for the three synthetic\ndatasets uniform ,normal , and lognormal . This is because syn-\nthetic datasets strictly follow i.i.d. gaps assumptions, leading to\nmore precise estimates of the index height (Eq. (21e) ), which signif-\nicantly affects the total time cost estimation (Eq. (21c)).\nParameter Tuning Strategy. We finally evaluate PGM++â€™s pa-\nrameter tuner as introduced in Section 6.2. For a given ğœ–â„“, which is\ndirectly solved given a pre-specified storage budget (Eq. (22)), we\nrecord the index lookup overhead for PGM-Indexes with different\nğœ–ğ‘–configurations: (a) ğ‘‡PGM++ , whereğœ–ğ‘–is automatically tuned using\nour cost model, (b) ğ‘‡rand, whereğœ–ğ‘–is randomly selected, and (c) ğ‘‡opt,\nwhich is the optimal time cost by testing all possible ğœ–ğ‘–values.\nFigure 14 reports the relative index lookup overhead w.r.t. dif-\nferentğœ–â„“settings (i.e., ğ‘‡PGM++/ğ‘‡optâˆ’1andğ‘‡rand/ğ‘‡optâˆ’1). From\nthe results, across all datasets and ğœ–â„“settings (i.e., storage budgets),\nPGM++â€™s automatic parameter tuning strategy consistently finds a\nbetterğœ–ğ‘–to reduce the index lookup overhead. Specifically, in 46%\nof cases, PGM++ successfully picks the optimalğœ–ğ‘–, and in 91% of\ncases, PGM++ finds a configuration that is only <10%worse than\nthe optimal one in terms of actual index lookup overhead.\nTakeaways. The experimental results reveal that in over 90% of\ncases, PGM++â€™s parameter tuner identifies a near-optimal index\nconfiguration, introducing less than 10% extra index lookup over-\nhead. In addition, our parameter tuner is much more efficient than\nCDFShop [ 24] designed for optimizing RMI structures (requiring\n<1Âµsv.s. >10 minutes). This is because instead of physically con-\nstructing the index, our method only depends on gap distribution\ncharacteristics, which can be pre-computed and re-used.\n8 RELATED WORK\nLearned Indexes. Indexing one-dimensional sorted keys has been\na well-explored topic for decades. While mainstream tree-based\nindexes (e.g., B+-tree [ 6], FAST [ 14], ART [ 4], Wormhole [ 47],\nHOT [ 4], etc.) are widely adopted in commercial DBMS, a new\nclass of data structure, known as learned index , has recently gained\nsignificant attention in both academia and industry [ 8,11,18,36,\n11\n\n22242628210\n1021071012segment cnt\nfb\n22242628210\nwiki\n22242628210\nosm\n22242628210\nbooks\nCLIP\nSIMPLE\n22242628210\nuniform\nTRUE\nADAP\n22242628210\nnormal\n22242628210\nlognormalFigure 12: Evaluation of the space cost model (Eq. (20)). For eachğœ–â„“, we compare three leaf segment count estimators: (a) SIMPLE ,\n(b)CLIP , and (c) ADAP .TRUE refers to the actual observed leaf segment count.\n0.0 0.5 1.0\nest. cost0.00.51.0observed costfb\n0.0 0.5 1.0\nest. costwiki\n0.0 0.5 1.0\nest. costosm\n0.0 0.5 1.0\nest. costbooks\n0.0 0.5 1.0\nest. costuniform\n0.0 0.5 1.0\nest. costnormal\n0.0 0.5 1.0\nest. costlognormal\nFigure 13: Evaluation of the time cost model (Eq. (21a)â€“Eq. (21e)). We plot the true index lookup costs (normalized) against the\nestimated costs (normalized) on platform x86-1 , where each point corresponds to a unique pair of (ğœ–ğ‘–,ğœ–â„“)configuration.\n22242628210\n0.00.20.40.6diff. to bestfb\n22242628210\nwiki\n22242628210\nosm\n22242628210\nbooks\nPGM++\nRandom\n22242628210\nuniform\n22242628210\nnormal\n22242628210\nlognormal\nFigure 14: Evaluation of parameter tuning. The y-axis is the relative difference compared to the optimal configuration when\nfixingğœ–â„“. Red bars and blue bars refer to the ğœ–ğ‘–settings selected by using PGM++â€™s cost model and randomly picking, respectively.\n40,45,46,50,52,53]. Intuitively, learned indexes directly fit the\nCDF over sorted keys with controllable error to perform an error-\nbounded last-mile search. By properly organizing the model struc-\nture, learned indexes offer the potential for superior space-time\ntrade-offs compared to conventional tree-based indexes [22, 44].\nExisting learned indexes can be roughly categorized as either\nRMI-like [ 18] or PGM-like [ 11], based on whether the error-bounded\nsearch occurs during the index traversal phase. This work delves\ndeeply into the theoretical and empirical aspects of the PGM-Index,\nhighlighting its potential to be practically embedded into real DBMS.\nLearned Index Theories. Unlike tree-based indexes, which are\nsupported by well-established theoretical foundations, the effec-\ntiveness of learned indexes has largely been demonstrated through\nempirical results . Ferragina et al. [ 10,11] first prove that the ex-\npected time and space complexities of a PGM-Index with error\nconstraintğœ–onğ‘keys should be ğ‘‚(logğ‘)andğ‘‚(ğ‘/ğœ–2), respec-\ntively. In parallel, another recent work [ 49] focuses on an RMI\nvariant with piece-wise constant models, achieving an index lookup\ntime ofğ‘‚(log logğ‘)but using super-linear space ofğ‘‚(ğ‘logğ‘).\nIn this work, we tighten the results of [ 10] by achieving a sub-\nlogarithmic time complexity of ğ‘‚(log logğ‘)while maintaining\nlinear space,ğ‘‚(ğ‘/ğœ–2), for PGM-Indexes. To the best of our knowl-\nedge, this is the tightest bound among all existing learned indexes.\nLearned Index Cost Model. Modeling the space and time over-\nheads of an index structure is crucial for both index parameter\nconfiguration and DBMS query optimization. Existing learned in-\ndexes mainly adopt a workload-based cost model, which assumes\nprior knowledge of the query distribution [ 24,50]. In contrast, by\nextending the theoretical results, we establish a cost model forPGM-like indexes without anyassumptions on query workloads.\nAs our cost model is simple, parameter tuning based on it is much\nmore efficient than workload-driven approaches, making it more\nfeasible to be integrated into practical DBMS.\nAI4DB. Beyond learned indexing, recent advancements in AI are\nreshaping traditional approaches on decades-old data management\nchallenges, such as query planning [ 23,48,54], cardinality estima-\ntion [ 16,39], approximate query processing [ 20,38], SQL genera-\ntion [15, 41], DBMS configuration [1, 51], etc.\n9 CONCLUSION AND FUTURE WORK\nThis work provides a thorough theoretical and experimental revisit\nto the PGM-Index. We establish a new bound for the PGM-Index by\nshowing the ğ‘‚(log logğ‘)index lookup time while using ğ‘‚(ğ‘/ğº)\nspace. We further identify that costly internal error-bounded search\noperations have become a bottleneck in practice. Based on such\nfindings, we propose PGM++, a simple yet effective PGM-Index vari-\nant, by improving the internal search subroutine and configuring\nindex hyper-parameters based on accurate cost models. Extensive\nexperimental results demonstrate that PGM++ speeds up index\nlookup queries by up to 2.31Ã—and1.56Ã—compared to the original\nPGM-Index and the optimized RMI implementation, respectively.\nFuture Work. â¶Our theoretical results inherit the i.i.d. assump-\ntion on gaps from previous analyses. In our future work, we aim\nto relax this assumption to demonstrate that the sub-logarithmic\nbound still holds for weakly correlated data. â·To further accelerate\nPGM++, we plan to fully exploit architecture-aware optimizations\nlike memory pre-fetching and SIMD. Additionally, we will release\na GPU-accelerated version of PGM++.\n12\n\nREFERENCES\n[1]Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, and Bohan Zhang. 2017.\nAutomatic Database Management System Tuning Through Large-scale Machine\nLearning. In SIGMOD Conference . ACM, 1009â€“1024.\n[2]Sylvain Arlot, Alain Celisse, and ZaÃ¯d Harchaoui. 2019. A Kernel Multiple\nChange-point Algorithm via Model Selection. J. Mach. Learn. Res. 20 (2019),\n162:1â€“162:56.\n[3] biglittle [n.d.]. big.LITTLE: Balancing Power Efficiency and Performance. https:\n//www.arm.com/en/technologies/big-little. Accessed: 2024-06-12.\n[4] Robert Binna, Eva Zangerle, Martin Pichl, GÃ¼nther Specht, and Viktor Leis. 2018.\nHOT: A Height Optimized Trie Index for Main-Memory Database Systems. In\nSIGMOD Conference . ACM, 521â€“534.\n[5] Kai Lai Chung. 2000. A course in probability theory . Elsevier.\n[6] Douglas Comer. 1979. The Ubiquitous B-Tree. ACM Comput. Surv. 11, 2 (1979),\n121â€“137.\n[7]Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein.\n2022. Introduction to algorithms . MIT press.\n[8]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In SIGMOD Conference . ACM, 969â€“984.\n[9] Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz. 1956. Asymptotic minimax\ncharacter of the sample distribution function and of the classical multinomial\nestimator. The Annals of Mathematical Statistics (1956), 642â€“669.\n[10] Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2020. Why Are Learned\nIndexes So Effective?. In ICML (Proceedings of Machine Learning Research) ,\nVol. 119. PMLR, 3123â€“3132.\n[11] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proc. VLDB Endow.\n13, 8 (2020), 1162â€“1175.\n[12] John L Hennessy and David A Patterson. 2011. Computer architecture: a quanti-\ntative approach . Elsevier.\n[13] Matthias Jarke and JÃ¼rgen Koch. 1984. Query Optimization in Database Systems.\nACM Comput. Surv. 16, 2 (1984), 111â€“152.\n[14] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D.\nNguyen, Tim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey.\n2010. FAST: fast architecture sensitive tree search on modern CPUs and GPUs.\nInSIGMOD Conference . ACM, 339â€“350.\n[15] Hyeonji Kim, Byeong-Hoon So, Wook-Shin Han, and Hongrae Lee. 2020. Natural\nlanguage to SQL: Where are we today? Proc. VLDB Endow. 13, 10 (2020), 1737â€“\n1750.\n[16] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter A. Boncz, and\nAlfons Kemper. 2019. Learned Cardinalities: Estimating Correlated Joins with\nDeep Learning. In CIDR . www.cidrdb.org.\n[17] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In aiDM@SIGMOD . ACM, 5:1â€“5:5.\n[18] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD Conference . ACM, 489â€“504.\n[19] Justin J. Levandoski, David B. Lomet, and Sudipta Sengupta. 2013. The Bw-Tree:\nA B-tree for new hardware platforms. In ICDE . IEEE Computer Society, 302â€“313.\n[20] Qingzhi Ma, Ali Mohammadi Shanghooshabad, Mehrdad Almasi, Meghdad Kur-\nmanji, and Peter Triantafillou. 2021. Learned Approximate Query Processing:\nMake it Light, Accurate and Fast. In CIDR . www.cidrdb.org.\n[21] macbook2024 [n.d.]. MacBook Air (13-inch, M3, 2024) - Technical Specifications.\nhttps://support.apple.com/en-us/118551. Accessed: 2024-06-12.\n[22] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. Proc. VLDB Endow. 14, 1 (2020), 1â€“13.\n[23] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Al-\nizadeh, and Tim Kraska. 2021. Bao: Making Learned Query Optimization Practical.\nInSIGMOD Conference . ACM, 1275â€“1288.\n[24] Ryan Marcus, Emily Zhang, and Tim Kraska. 2020. CDFShop: Exploring and\nOptimizing Learned Index Structures. In SIGMOD Conference . ACM, 2789â€“2792.\n[25] openstreetmap [n.d.]. OpenStreetMap. https://www.openstreetmap.org/. Ac-\ncessed: 2024-06-12.\n[26] Joseph Oâ€™Rourke. 1981. An On-Line Algorithm for Fitting Straight Lines Between\nData Ranges. Commun. ACM 24, 9 (1981), 574â€“578.\n[27] Yehoshua Perl, Alon Itai, and Haim Avni. 1978. Interpolation Search - A Log Log\nN Search. Commun. ACM 21, 7 (1978), 550â€“553.\n[28] pgm++ [n.d.]. PGM++. https://github.com/qyliu-hkust/bench_search. Accessed:\n2024-06-12.\n[29] pgm [n.d.]. PGM-Index. https://github.com/gvinciguerra/PGM-index. Accessed:\n2024-06-12.\n[30] postgresql [n.d.]. PostgreSQL. https://www.postgresql.org/docs/current/indexes.\nhtml. Accessed: 2024-06-12.\n[31] pytorch [n.d.]. PyTorch. https://pytorch.org/. Accessed: 2024-06-12.[32] rmi [n.d.]. rmi. https://github.com/learnedsystems/RMI/. Accessed: 2024-06-12.\n[33] Peter Van Sandt, Yannis Chronis, and Jignesh M. Patel. 2019. Efficiently Searching\nIn-Memory Sorted Arrays: Revenge of the Interpolation Search?. In SIGMOD\nConference . ACM, 36â€“53.\n[34] Lars-Christian Schulz, David Broneske, and Gunter Saake. 2018. An Eight-\nDimensional Systematic Evaluation of Optimized Search Algorithms on Modern\nProcessors. Proc. VLDB Endow. 11, 11 (2018), 1550â€“1562.\n[35] sparksql [n.d.]. Spark SQL. https://spark.apache.org/sql/. Accessed: 2024-06-12.\n[36] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: a scalable learned index for multicore\ndata storage. In PPoPP . ACM, 308â€“320.\n[37] tensorflow [n.d.]. TensorFlow. https://www.tensorflow.org/. Accessed: 2024-06-\n12.\n[38] Saravanan Thirumuruganathan, Shohedul Hasan, Nick Koudas, and Gautam\nDas. 2020. Approximate Query Processing for Data Exploration using Deep\nGenerative Models. In ICDE . IEEE, 1309â€“1320.\n[39] Xiaoying Wang, Changbo Qu, Weiyuan Wu, Jiannan Wang, and Qingqing Zhou.\n2021. Are We Ready For Learned Cardinality Estimation? Proc. VLDB Endow. 14,\n9 (2021), 1640â€“1654.\n[40] Zhaoguo Wang, Haibo Chen, Youyun Wang, Chuzhe Tang, and Huan Wang.\n2022. The Concurrent Learned Indexes for Multicore Data Storage. ACM Trans.\nStorage 18, 1 (2022), 8:1â€“8:35.\n[41] Nathaniel Weir, Prasetya Ajie Utama, Alex Galakatos, Andrew Crotty, Amir\nIlkhechi, Shekar Ramaswamy, Rohin Bhushan, Nadja Geisler, Benjamin HÃ¤t-\ntasch, Steffen Eger, Ugur Ã‡etintemel, and Carsten Binnig. 2020. DBPal: A Fully\nPluggable NL2SQL Training Pipeline. In SIGMOD Conference . ACM, 2347â€“2361.\n[42] wikidata [n.d.]. Wikidata. https://www.wikidata.org/wiki/Wikidata:Main_Page.\nAccessed: 2024-06-12.\n[43] Samuel Williams, Andrew Waterman, and David A. Patterson. 2009. Roofline:\nan insightful visual performance model for multicore architectures. Commun.\nACM 52, 4 (2009), 65â€“76.\n[44] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and\nTianzheng Wang. 2022. Are Updatable Learned Indexes Ready? Proc. VLDB\nEndow. 15, 11 (2022), 3004â€“3017.\n[45] Jiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. Proc. VLDB Endow. 14, 8\n(2021), 1276â€“1288.\n[46] Shangyu Wu, Yufei Cui, Jinghuan Yu, Xuan Sun, Tei-Wei Kuo, and Chun Jason\nXue. 2022. NFL: Robust Learned Index via Distribution Transformation. Proc.\nVLDB Endow. 15, 10 (2022), 2188â€“2200.\n[47] Xingbo Wu, Fan Ni, and Song Jiang. 2019. Wormhole: A Fast Ordered Index for\nIn-memory Data Management. In EuroSys . ACM, 18:1â€“18:16.\n[48] Xiang Yu, Chengliang Chai, Guoliang Li, and Jiabin Liu. 2022. Cost-based or\nLearning-based? A Hybrid Query Optimizer for Query Plan Selection. Proc.\nVLDB Endow. 15, 13 (2022), 3924â€“3936.\n[49] Sepanta Zeighami and Cyrus Shahabi. 2023. On Distribution Dependent Sub-\nLogarithmic Query Time of Learned Indexing. In ICML (Proceedings of Machine\nLearning Research) , Vol. 202. PMLR, 40669â€“40680.\n[50] Jiaoyi Zhang and Yihan Gao. 2022. CARMI: A Cache-Aware Learned Index with a\nCost-based Construction Algorithm. Proc. VLDB Endow. 15, 11 (2022), 2679â€“2691.\n[51] Ji Zhang, Yu Liu, Ke Zhou, Guoliang Li, Zhili Xiao, Bin Cheng, Jiashu Xing,\nYangtao Wang, Tianheng Cheng, Li Liu, Minwei Ran, and Zekang Li. 2019. An\nEnd-to-End Automatic Cloud Database Tuning System Using Deep Reinforce-\nment Learning. In SIGMOD Conference . ACM, 415â€“432.\n[52] Jiaoyi Zhang, Kai Su, and Huanchen Zhang. 2024. Making In-Memory Learned\nIndexes Efficient on Disk. Proceedings of the ACM on Management of Data 2, 3\n(2024), 1â€“26.\n[53] Zhou Zhang, Zhaole Chu, Peiquan Jin, Yongping Luo, Xike Xie, Shouhong Wan,\nYun Luo, Xufei Wu, Peng Zou, Chunyang Zheng, Guoan Wu, and Andy Rudoff.\n2022. PLIN: A Persistent Learned Index for Non-Volatile Memory with High\nPerformance and Instant Recovery. Proc. VLDB Endow. 16, 2 (2022), 243â€“255.\n[54] Rong Zhu, Wei Chen, Bolin Ding, Xingguang Chen, Andreas Pfadler, Ziniu Wu,\nand Jingren Zhou. 2023. Lero: A Learning-to-Rank Query Optimizer. Proc. VLDB\nEndow. 16, 6 (2023), 1466â€“1479.\n13",
  "textLength": 75800
}