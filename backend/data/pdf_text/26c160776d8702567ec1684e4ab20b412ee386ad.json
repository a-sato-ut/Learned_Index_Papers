{
  "paperId": "26c160776d8702567ec1684e4ab20b412ee386ad",
  "title": "Neural packet classification",
  "pdfPath": "26c160776d8702567ec1684e4ab20b412ee386ad.pdf",
  "text": "Neural Packet Classification\nEric Liang1, Hang Zhu2, Xin Jin2, Ion Stoica1\n1UC Berkeley,2Johns Hopkins University\nekl@berkeley.edu, hzhu@jhu.edu, xinjin@cs.jhu.edu, istoica@cs.berkeley.edu\nABSTRACT\nPacket classification is a fundamental problem in computer\nnetworking. This problem exposes a hard tradeoff between\nthe computation and state complexity, which makes it par-\nticularly challenging. To navigate this tradeoff, existing so-\nlutions rely on complex hand-tuned heuristics, which are\nbrittle and hard to optimize.\nIn this paper, we propose a deep reinforcement learning\n(RL) approach to solve the packet classification problem.\nThere are several characteristics that make this problem a\ngood fit for Deep RL. First, many existing solutions itera-\ntively build a decision tree by splitting nodes in the tree.\nSecond, the effects of these actions (e.g., splitting nodes) can\nonly be evaluated once the entire tree is built. These two\ncharacteristics are naturally captured by the ability of RL to\ntake actions that have sparse and delayed rewards. Third,\nit is computationally efficient to generate data traces and\nevaluate decision trees, which alleviate the notoriously high\nsample complexity problem of Deep RL algorithms. Our so-\nlution, NeuroCuts, uses succinct representations to encode\nstate and action space, and efficiently explore candidate de-\ncision trees to optimize for a global objective. It produces\ncompact decision trees optimized for a specific set of rules\nand a given performance metric, such as classification time,\nmemory footprint, or a combination of the two. Evaluation\non ClassBench shows that NeuroCuts outperforms existing\nhand-crafted algorithms in classification time by 18% at the\nmedian, and reduces both classification time and memory\nfootprint by up to 3 Ã—.\n1 INTRODUCTION\nPacket classification is one of the fundamental problems in\ncomputer networking. The goal of packet classification is to\nmatch a given packet to a rule from a set of rules, and to do\nso while optimizing the classification time and/or memory\nfootprint. Packet classification is a key building block for\nmany network functionalities, including firewalls, access\ncontrol, traffic engineering, and network measurements [ 13,\n30,55]. As such, packet classifiers are widely deployed by\nenterprises, cloud providers, ISPs, and IXPs [1, 30, 48].\nExisting solutions for packet classification can be divided\ninto two broad categories. Solutions in the first category are\nhardware-based. They leverage Ternary Content-Addressable\nMemories (TCAMs) to store all rules in an associative mem-\nory, and then match a packet to all these rules in parallel [ 24].As a result, TCAMs provide constant classification time, but\ncome with significant limitations. TCAMs are inherently\ncomplex, and this complexity leads to high cost and power\nconsumption. This makes TCAM-based solutions prohibitive\nfor implementing large classifiers [55].\nThe solutions in the second category are software based.\nThese solutions build sophisticated in-memory data structuresâ€”\ntypically decision treesâ€”to efficiently perform packet classi-\nfication [ 30]. While these solutions are far more scalable than\nTCAM-based solutions, they are slower, as the classification\noperation needs to traverse the decision tree from the root\nto the matching leaf.\nBuilding efficient decision trees is difficult. Over the past\ntwo decades, researchers have proposed a large number of\ndecision tree based solutions for packet classification [ 13,\n30,41,47,55]. However, despite the many years of research,\nthese solutions have two major limitations. First, they rely\non hand-tuned heuristics to build the tree. Examples include\nmaximizing split entropy [ 13], balancing splits with custom\nspace measures [ 13], special handling for wildcard rules [ 47],\nand so on. This makes them hard to understand and optimize\nover different sets of rules. If a heuristic is too general, it\ncannot take advantage of the characteristics of a particular\nset of rules. If a heuristic is designed for a specific set of rules,\nit typically does not achieve good results on another set of\nrules with different characteristics.\nSecond, these heuristics do not explicitly optimize for a\ngiven objective (e.g., tree depth). They make decisions based\non information (e.g., the difference between the number of\nrules in the children, the number of distinct ranges in each\ndimension) that is only loosely related to the global objective.\nAs such, their performance can be far from optimal.\nIn this paper, we propose a learning approach to packet\nclassification. Our approach has the potential to address\nthe limitations of the existing hand-tuned heuristics. In par-\nticular, our approach learns to optimize packet classifica-\ntion for a given set of rules and objective, can easily in-\ncorporate pre-engineered heuristics to leverage their do-\nmain knowledge, and does so with little human involve-\nment. The recent successes of deep learning in solving noto-\nriously hard problems, such as image recognition [ 23] and\nlanguage translation [ 51], have inspired many practition-\ners and researchers to apply deep learning, in particular,\nand machine learning, in general, to systems and network-\ning problems [ 4,6,16,34,35,54,62,64,65]. While in some\n1arXiv:1902.10319v1  [cs.NI]  27 Feb 2019\n\nof these cases there are legitimate concerns about whether\nmachine learning is the right solution for the problem at\nhand, we believe that deep learning is a good fit for our prob-\nlem. This is notable since, when an efficient formulation is\nfound, learning-based solutions have often outperformed\nhand-crafted alternatives [22, 37, 44].\nThere are two general approaches to apply learning to\npacket classification. The first is to replace the decision tree\nwith a neural network, which given a packet will output the\nrule matching that packet. Unfortunately, while appealing,\nthis end-to-end solution has a major drawback: it does not\nguarantee the correct rule is always matched. While this\nmight be acceptable for some applications such as traffic\nengineering, it is not acceptable for others, such as access\ncontrol. Another issue is that large rule sets will require\ncorrespondingly large neural network models, which can\nbe expensive to evaluate without accelerators such as GPUs.\nThe second approach, and the one we take in this paper, is to\nuse deep learning to build a decision tree. Recent work has\napplied deep learning to optimize decision trees for machine\nlearning problems [ 21,39,59]. These solutions, however, are\ndesigned for machine learning settings that are different\nthan packet classification, and aim to maximize accuracy.\nIn contrast, decision trees for packet classification provide\nperfect accuracy by construction, and the goal is to minimize\nclassification time and memory footprint.\nOur solution uses deep reinforcement learning (RL) to\nbuild efficient decision trees. There are three characteristics\nthat makes RL a particularly good fit for packet classification.\nFirst, the natural solution to build a decision tree is to start\nwith one node and recursively split (cut) it. Unfortunately,\nthis kind of approach does not have a greedy solution. When\nmaking a decision to cut a node, we do not know whether\nthat decision was a good one (i.e., whether it leads to an\nefficient tree) before we finish building the actual tree. RL\nnaturally captures this characteristic as it does not assume\nthat the impact of a given decision on the performance objec-\ntive is known immediately. Second, unlike existing heuristics\nwhich take actions that are only loosely related to the per-\nformance objective, the explicit goal of an RL algorithm is to\ndirectly maximize the performance objective. Third, unlike\nother RL domains such as as robotics, for our problem it is\npossible to evaluate an RL model quickly (i.e., a few seconds\nof CPU time). This alleviates one of the main drawbacks of\nRL algorithms: the non-trivial learning time due to the need\nto evaluate a large number of models to find a good solution.\nBy being able to evaluate each model quickly (and, as we will\nsee, in parallel) we significantly reduce the learning time.\nTo this end, we design NeuroCuts, a deep RL solution for\npacket classification that learns to build efficient decision\ntrees. There are three technical challenges to formulate this\nproblem as an RL problem. First, the tree is growing during\nPrioritySrcIPDstIPSrcPortDstPortProtocol210.0.0.010.0.0.0/16***1**[0,1023][0,1023]TCP0*****Figure 1: A packet classifier example. Real-world clas-\nsifiers can have 100K rules or more.\nthe execution of the algorithm, as existing nodes are split.\nThis makes it very difficult to encode the decision tree, as\nRL algorithms require a fixed size input. We address this\nproblem by noting that the decision of how to split a node in\nthe tree depends only on the node itself; it does not depend\non the rest of the tree. As such, we do not need to encode the\nentire tree; we only need to encode the current node. The\nsecond challenge is in computing dense rewards to accelerate\nthe learning process; here we exploit the branching structure\nof the problem to provide denser feedback for tree size and\ndepth. The final challenge is that training for very large sets\nof rules can take a long time. To address this, we leverage\nRLlib [31], a distributed RL library.\nIn summary, we make the following contributions.\nâ€¢We show that the packet classification problem is a good\nfit for reinforcement learning (RL).\nâ€¢We present NeuroCuts, a deep RL solution for packet clas-\nsification that learns to build efficient decision trees.\nâ€¢We show that NeuroCuts outperforms state-of-the-art so-\nlutions, improving classification time by 18% at the median\nand reducing both time and memory usage by up to 3 Ã—.\nThe code for NeuroCuts is open source and is available at:\nhttps://github.com/xinjin/neurocuts-code\n2 BACKGROUND\nIn this section, we provide background on the packet classi-\nfication problem, and summarize the key ideas behind the\ndecision tree based solutions to solve this problem.\n2.1 Packet Classification\nA packet classifier contains a list of rules. Each rule speci-\nfies a pattern on multiple fields in the packet header. Typi-\ncally, these fields include source and destination IP addresses,\nsource and destination port numbers, and protocol type.\nThe ruleâ€™s pattern specifies which packets match the rule.\nMatching conditions include prefix based matching (e.g.,\nfor IP addresses), range based matching (e.g., for port num-\nbers), and exact matching (e.g., for protocol type). A packet\nmatches a rule if each field in the packet header satisfies\nthe matching condition of the corresponding field in the\nrule, e.g., the packetâ€™s source/destination IP address matches\nthe prefix of the source/destination address in the rule, the\npacketâ€™s source/destination port number is contained in\n2\n\nNeural Packet Classification\nthe source/destination range specified in the rule, and the\npacketâ€™s protocol type matches the ruleâ€™s protocol type.\nFigure 1 shows a packet classifier with three rules. The\nfirst rule matches all packets with source address 10.0.0.1 and\nthe destination addresses sharing prefix 10.0.0.0/16. Other\nfields are unspecified (i.e., they are â‹†) meaning that the rule\nmatches any value in these fields. The second rule matches all\nTCP packets with source and destination ports in the range\n[0, 1023], irrespective of IP addresses (as they are â‹†). Finally,\nthe third rule is a default rule that matches all packets. This\nguarantees that any packet matches at least one rule.\nSince rules can overlap, it is possible for a packet to match\nmultiple rules. To resolve this ambiguity, each rule is assigned\na priority. A packet is then matched to the highest priority\nrule. For example, packet (10.0.0.0, 10.0.0.1, 0, 0, 6) matches all\nthe three rules of the packet classifier in Figure 1. However,\nsince the first rule has the highest priority, we match the\npacket to the first rule only.\n2.2 Decision Tree Algorithms\nPacket classification is similar to the point location problem\nin a multi-dimensional geometric space: the fields in the\npacket header we are doing classification on (e.g., source\nand destination IP addresses, source and destination port\nnumbers, and protocol number) represent the dimensions in\nthe geometric space, a packet is represented as a point in this\nspace, and a rule as a hypercube. Unfortunately, the point\nlocation problem exhibits a hard tradeoff between time and\nspace complexities [14].\nThe packet classification problem is then equivalent to\nfinding all hypercubes that contains the point corresponding\nto a given packet. In particular, in a d-dimensional geometric\nspace with nnon-overlapping hypercubes and when d>3,\nthis problem has either (i)a lower bound of O(loÐ´n)time\nandO(nd)space, or(ii)a lower bound of O(loÐ´dâˆ’1n)time\nandO(n)space [ 14]. The packet classification problem allows\nthe hypercubes (i.e., rules) to overlap, and thus is at least as\nhard as the point location problem [ 14]. In other words, if we\nwant logarithmic computation time, we need space that is\nexponential in the number of dimensions (fields), and if we\nwant linear space, the computation time will be exponential\nin the logarithm of the number of rules. Given that for packet\nclassification d=5, neither of these choices is attractive.\nNext, we discuss two common techniques employed by\nexisting solutions to build decision trees for packet classifi-\ncation: node cutting andrule partition .\nNode cutting. Most existing solutions for packet classifi-\ncation aim to build a decision tree that exhibits low classi-\nfication time (i.e., time complexity) and memory footprint\n(i.e., space complexity) [ 55]. The main idea is to split nodes\nin the decision tree by â€œcuttingâ€ them along one or more\nR1R0R4R2R5R3XYR0,R1, R2, R3, R4,R5R1,R3,R4R0,R1,R4R1,R2,R4R1,R4,R5R3,R4R1R4R0,R1R4R1,R2R4,R5R1,R5(a) Packet classifier.(b) Decision tree.Figure 2: Node cutting.\nR0R2R5R3XY(a) Partition 1.\n(b) Partition 2.R1R4XYR0,R2,R3, R5R3R0R2R5\nR1, R4R1R4\nFigure 3: Rule partition.\ndimensions. Starting from the root which contains all rules,\nthese algorithms iteratively split/cut the nodes until each\nleaf contains fewer than a predefined number of rules. Given\na decision tree, classifying a packet reduces to walk the tree\nfrom the root to a leaf, and then chose the highest priority\nrule associated with that leaf.\nFigure 2 illustrates this technique. The packet classifier\ncontains six rules (R0 to R5) in a two-dimensional space.\nFigure 2(a) shows each rule as a rectangle in the space, and\nrepresents the cuts as dashed lines. Figure 2(b) shows the\ncorresponding decision tree for this packet classifier. The\nroot of the tree contains all the six rules. First, we cut the\nentire space (which represents the root) into four chunks\nalong dimension x. This leads to the creation of four children.\nIf a rule intersects a childâ€™s chunk, it is added to that child.\nFor example, R1, R3 and R4 all intersect the first chunk (i.e.,\nthe first quarter in this space), and thus they are all added to\nthe first rootâ€™s child. If a rule intersects multiple chunks it is\nadded to each corresponding child, e.g., R1 is added to all the\nfour children. Next, we cut the chunk corresponding to each\nof the four children along dimension y. As a result, each of\nthe nodes at the first level will end up with two children.\nRule partition. One challenge with \"blindly\" cutting a node\nis that we might end up with a rule being replicated to a large\nnumber of nodes [ 55]. In particular, if a rule has a large size\nalong one dimension, cutting along that dimension will result\n3\n\nin that rule being added to many nodes. For example, rule\nR1 in Figure 2(a) has a large size in dimension x. Thus, when\ncutting along dimension x, R1 will end up being replicated\nat every node created by the cut. Rule replication can lead to\ndecision trees with larger depths and sizes, which translate\nto higher classification time and memory footprint.\nOne solution to address this challenge is to first partition\nrules based on their \"shapes\". Broadly speaking, rules with\nlarge sizes in a particular dimension are put in the same set.\nThen, we can build a separate decision tree for each of these\npartitions. Figure 3 illustrates this technique. The six rules\nin Figure 2 are grouped into two partitions. One partition\nconsists of rules R1 and R4, as both these rules have large\nsizes in dimension x. The other partition consists of the other\nfour rules, as these rules have small sizes in dimension x.\nFigure 3(a) and Figure 3(b) show the corresponding decision\ntrees for each partition. Note that the resulting trees have\nlower depth, and smaller number of rules per node as com-\npared to the original decision tree in Figure 2(b). To classify\na packet, we classify it against every decision tree, and then\nchoose the highest priority rule among all rules the packet\nmatches in all decision trees.\nSummary. Existing solutions build decision trees by em-\nploying two types of actions: node cutting and rule partition.\nThese solutions mainly differ in the way they decide (i) at\nwhich node to apply the action, (ii) which action to apply,\nand (iii) how to apply it (e.g., along which dimension(s) to\npartition).\n3 A LEARNING-BASED APPROACH\nIn this section, we describe a learning-based approach for\npacket classification. We motivate our approach, discuss the\nformulation of classification as a learning problem, and then\npresent our solution.\n3.1 Why Learn?\nThe existing solutions for packet classification rely on hand-\ntuned heuristics to build decision trees. Unfortunately, this\nleads to two major limitations.\nFirst, these heuristics often face a difficult trade-off be-\ntween performance andcost. Tuning such a heuristic for a\ngiven set of rules is an expensive proposition, requiring con-\nsiderable human efforts and expertise. Worse yet, when given\na different rule set, one might have to do this all over again.\nAddressing this challenge has been the main driver of a long\nline of research over the past two decades [ 13,30,41,47,55].\nOf course, one could build a general heuristic for a large\nvariety of rule sets. Unfortunately, such a solution would not\nprovide the best performance for a given set of rules.\nSecond, existing algorithms do not directly optimize for a\nglobal objective. Ideally, a good packet classification solutionshould optimize for (i)classification time, (ii)memory foot-\nprint, or(iii)a combination between the two. Unfortunately,\nthe existing heuristics do not directly optimize for any of\nthese objectives. At their core, these heuristics make greedy\ndecisions to build decision trees. At every step, they decide\non whether to cut a node or partition the rules based on\nsimple statistics (e.g., the size of the rules in each dimen-\nsion, number of unique ranges in each dimension), which\nare poorly correlated with the desired objective. As such, the\nresulting decision trees are often far from being optimal.\nAs we will see, a learning-based approach can address\nthese limitations. Such an approach can learn to generate\nan efficient decision tree for a specific set of rules without\nthe need to rely on hand-tuned heuristics. This is not to say\nthese heuristics do not have value; in fact they often contain\nkey domain knowledge that we show can be leveraged and\nimproved on by the learning algorithm.\n3.2 What to Learn?\nClassification is a central task in machine learning literature.\nThe recent success of using deep neural networks (DNNs) for\nimage recognition, speech recognition and language transla-\ntion has been single-handedly responsible for the recent AI\n\"revolution\" [10, 23, 51].\nAs such, one natural solution for packet classification\nwould be to replace a decision tree with a DNN. In particular,\nsuch DNN will take as input the fields of a packet header and\noutput the rule matching that packet. Related to our problem,\nprior work has shown that DNN models can be effectively\nused to replace B-Trees for indexing [22].\nHowever, this solution has two drawbacks. First, a DNN-\nbased classifier does not guarantee 100% accuracy. This is\nbecause training a DNN is fundamentally a stochastic pro-\ncess. Second, given a DNN packet classification result, it\nis expensive to verify whether the result is correct or not.\nUnlike the recently proposed learned index solution to re-\nplace B-Trees [ 22], the rules in packet classification are multi-\ndimensional and overlap with each other. If a rule matches a\npacket, we still need to check other rules to see if this rule\nhas the highest priority among all matched rules.\nTo avoid these drawbacks, in this paper we propose to\nlearn building decision trees for a given set of rules. Since\nthe result is still a decision tree, we can guarantee correct-\nness, and it will be easy to deploy the classifier with existing\nsystems (hardware and software) compared to a DNN.\n3.3 How to Learn?\nIn this section, we show that the problem of building decision\ntrees maps naturally to RL. As illustrated in Figure 4(a), an\nRL system consists of an agent that repeatedly interacts\nwith an environment. The agent observes the state of the\n4\n\nNeural Packet Classification\nAgentEnvironmentactionAtstateStrewardRtRt+1St+1\n(a)\nStateAgentNeural NetworkEnvironmentPacket ClassifierDecision Tree\nactionAtstateStrewardRtRt+1St+1\n(b)\nFigure 4: (a) Classic RL system. An agent takes an ac-\ntion, At, based on the current state of the environment,\nSt, and applies it to the environment. This leads to a\nchange in the environment state ( St+1) and a reward\n(Rt+1). (b) NeuroCuts as an RL system.\nenvironment, and then takes an action that might change\nthe environmentâ€™s state. The goal of the agent is to compute\napolicy that maps the environmentâ€™s state to an action in\norder to optimize a reward. As an example, consider an agent\nplaying chess. In this case, the environment is the board, the\nstate is the position of the pieces on the board, an action is\nmoving a piece on the board, and the reward could be 1if\nthe game is won, and âˆ’1, if the game is lost.\nThis simple example illustrates two characteristics of RL\nthat are a particularly good fit to our problem. First, rewards\naresparse , i.e., not every state has associated a reward. For\ninstance, when moving a piece we do not necessary know\nwhether that move will result in a win or loss. Second, the\nrewards are delayed ; we need to wait until the end of the\ngame to see whether the game was won or lost.\nTo deal with large state and action spaces, recent RL so-\nlutions have employed DNNs to implement their policies.\nThese solutions, called Deep RL, have achieved remarkable\nresults matching humans at playing Atari games [ 37], and\nbeating the Go world champion [ 46]. These results have en-\ncouraged researchers to apply Deep RL to networking and\nsystems problems, from routing, to congestion control, to\nvideo streaming, and to job scheduling [ 4,6,16,34,35,54,62,64,65]. Building a decision tree can be easily cast as an\nRL problem: the environmentâ€™s state is the current decision\ntree, an action is either cutting a node or partitioning a set of\nrules, and the reward is either the classification time, memory\nfootprint, or a combination of the two. While in some cases\nthere are legitimate concerns about whether Deep RL is the\nright solution for the problem at hand, we identify several\ncharacteristics that make packet classification a particularly\ngood fit for Deep RL.\nFirst, when we take an action, we do not know for sure\nwhether it will lead to a good decision tree or not; we only\nknow this once the tree is built. As a result, the rewards in\nour problem are both sparse anddelayed . This is naturally\ncaptured by the RL formulation.\nSecond, the explicit goal of RL is to maximize the reward.\nThus, unlike existing heuristics, our RL solution aims to\nexplicitly optimize the performance objective, rather than\nusing local statistics whose correlation to the performance\nobjective can be tenuous.\nThird, one potential concern with Deep RL algorithms\nis sample complexity. In general, these algorithms require\na huge number of samples (i.e., input examples) to learn a\ngood policy. Fortunately, in the case of packet classification\nwe can generate such samples cheaply. A sample, or rollout,\nis a sequence of actions that builds a decision tree with the\nassociated reward(s) by using a given policy. The reason we\ncan generate these rollouts cheaply is because we can build\nall these trees in software, and do so in parallel. Contrast this\nwith other RL-domains, such as robotics, where generating\neach rollout can take a long time and requires expensive\nequipment (i.e., robots).\n4 NEUROCUTS DESIGN\n4.1 NeuroCuts Overview\nWe introduce the design for NeuroCuts, a new Deep RL\nformulation of the packet classification problem. Given a\nrule set and an objective function (i.e., classification time,\nmemory footprint, or a combination of both), NeuroCuts\nlearns to build a decision tree that minimizes the objective.\nFigure 4(b) illustrates the framing of NeuroCuts as an RL\nsystem: the environment consists of the set of rules and\nthe current decision tree, while the agent uses a model (im-\nplemented by a DNN) that aims to select the best cut or\npartition action to incrementally build the tree. A cut action\ndivides a node along a chosen dimension (i.e., one of SrcIP ,\nDstIP ,SrcPort ,DstPort , and Protocol ) into a number of\nsub-ranges (i.e., 2, 4, 8, 16, or 32 ranges), and creates that\nmany child nodes in the tree. A partition action on the other\nhand divides the rules of a node into disjoint subsets (e.g.,\nbased on the coverage fraction of a dimension), and creates a\nnew child node for each subset. The available actions for the\n5\n\ncurrent node are advertised by the environment at each step,\nthe agent chooses among them to generate the tree, and over\ntime the agent learns to optimize its decisions to maximize\nthe reward from the environment. Figure 5 visualizes the\nlearning process of NeuroCuts.\n4.2 NeuroCuts Training Algorithm\nRecall that the goal of an RL algorithm is to compute a policy\nto maximize rewards from the environment. Referring again\nto Figure 4, the environment defines the action space Aand\nstate space S. The agent starts with an initial policy, evalu-\nates it using multiple rollouts, and then updates it based on\nthe results (rewards) of these rollouts. Then, it repeats this\nprocess until satisfied with the reward.\nWe first consider a strawman formulation of decision tree\ngeneration as a single Markov Decision Process (MDP). In\nthis framing, a rollout begins with a tree consisting of a\nsingle node. This is the initial state, s0âˆˆS. At each step t,\nthe agent executes an action atâˆˆAand receives a reward\nrt; the environment transitions from the current state stâˆˆS\nto the next state st+1âˆˆS(i.e., the updated tree and next\nnode to process). The goal is to maximize the total reward\nreceived by the agent, i.e.,Ã\ntÎ³trtwhereÎ³is a discounting\nfactor used to prioritize more recent rewards.\nDesign challenges. While at a high level this RL formula-\ntion seems straightforward, there are three key challenges we\nneed to address before we have a realizable implementation.\nThe first is how to encode the variable-length decision tree\nstate stas an input to the neural network policy. While it is\npossible to flatten the tree, say, into an 1-dimensional vector,\nthe size of such a vector would be very large (i.e., hundreds\nof thousands of units). This will require both a very large\nnetwork model to process such input, and a prohibitively\nlarge number of samples.\nWhile recent work has proposed leveraging recurrent neu-\nral networks (RNNs) and graph embedding techniques [ 58,\n60,61] to reduce the input size, these solutions are brittle in\nthe face of large or dynamically growing graph structures\n[66]. Rather than attempting to solve the state representa-\ntion problem to deal with large inputs, in NeuroCuts we\ninstead take advantage of the underlying structure of packet\nclassification trees to design a simple and compact state rep-\nresentation. This means that when the agent is deciding how\nto split a node, it only observes a fixed-length representation\nof the node. All needed state is encoded in the representation;\nno other information about the rest of the tree is observed.\nThe second challenge is how to deal with the sparse and\ndelayed rewards incurred by the node-by-node process of\nbuilding the decision tree. While we could in principle return\na single reward to the agent when the tree is complete, itwould be very difficult to train an agent in such an envi-\nronment. Due to the long length of tree rollouts (i.e., many\nthousands of steps), learning is only practical if we can com-\npute meaningful dense rewards .1Such a dense reward for an\naction would be based on the statistics of the subtree it leads\nto (i.e., its depth or size).2Unfortunately, it is not possible to\ncompute this until the subtree is complete. To handle this, we\ntake the somewhat unusual step of only computing rewards\nfor the rollout when the tree is completed, and setting Î³=0,\neffectively creating a series of 1-step decision problems sim-\nilar to contextual bandits [ 25]. However, unlike the bandit\nsetting, these 1-step decisions are connected through the\ndynamics of the tree building process.\nAnother way of looking at the dense reward problem is\nthat the process of building a decision tree is not really se-\nquential but tree-structured (i.e., it is more accurately modeled\nas a branching decision process [ 8,18,40]), and we need to\naccount for the reward calculations accordingly. In such a\n\"branching\" formulation, Î³>0, but the rewards of an action\nare computed as an aggregation over multiple child states\nproduced by an action. For example, cutting a node produces\nmultiple child sub-nodes, and the reward calculation may\ninvolve a sumor aminover each childâ€™s future rewards, de-\npending on whether we are optimizing for tree size or depth.\nThe 1-step decision problem and branching decision process\nformulations of NeuroCuts are roughly equivalent; in the\nimplementation section we describe how we adapt standard\nRL algorithms to run NeuroCuts.\nThe final challenge is how to scale the solution to large\npacket classifiers. The decision tree for a packet classifier\nwith 100K rules can have hundreds of thousands of nodes.\nThe size of the tree impedes training along several dimen-\nsions. Not only does it take more steps to finish building a\ntree, but the execution time of each action increases as there\nare more rules to process. The space of trees to explore is\nalso larger, requiring the use of larger network models and\ngenerating more rollouts to train.\nState representation. One key observation is that the ac-\ntion on a tree node only depends on the node itself, so it is\nnot necessary to encode the entire decision tree in the en-\nvironment state. Our goal to optimize a global performance\nobjective over the entire tree suggests that we would need\nto make decisions based on the global state. However, this\ndoes not mean that the state representation needs to encode\nthe entire decision tree. Given a tree node, the action on that\nnode only needs to make the best decision to optimize the\n1Note that just returning -1 or - cutSize for each step would not be a\nparticularly useful dense reward.\n2The rewards for NeuroCuts correspond to the true problem objective; we\ndo not do \"reward engineering\" since that would bias the solution.\n6\n\nNeural Packet Classification\n010000200003000040000Protocol DstPort SrcPort DstIP SrcIP\n050100150200\n0200400600800\n025005000750010000\n(a) NeuroCuts starts with a randomly initialized policy that generates poorly shaped trees (left, truncated). Over\ntime, it learns to reduce the tree depth and develops a more coherent strategy (center). The policy converges to a\ncompact depth-12 tree (right) that specializes in cutting SrcIP ,SrcPort , and DstPort .\n010000200003000040000(b) In comparison, HiCuts produces\na depth-29 tree for this rule set that\nis 15Ã—larger and 3Ã—slower in clas-\nsification time.\nFigure 5: Visualization of NeuroCuts learning to split the fw5_1k ClassBench rule set. The x-axis denotes the tree\nlevel, and the y-axis the number of nodes at the level. The distribution of cut dimensions per level of the tree is\nshown in color.\nFigure 6: The NeuroCuts policy is stochastic, which en-\nables it to effectively explore many different tree vari-\nations during training. Here we visualize four random\ntree variations drawn from a single policy trained on\nthe acl4_1k ClassBench rule set.\nsub-tree rooted at that node. It does not need to consider\nother tree nodes in the decision tree.\nFormally, given tree node n, lettnandsndenote nâ€™s clas-\nsification time and memory footprint, respectively, and Tn\nandSnbe the classification time and memory footprint of\nthe entire sub-tree rooted at node n, respectively. Then, for\na cut action, we have the following equations:\nTn=tn+max iâˆˆchildren(n)Ti (1)\nSn=sn+sum iâˆˆchildren(n)Si (2)\nSimilarly, for a partition action, we have\nTn=tn+sum iâˆˆchildren(n)Ti (3)\nSn=sn+sum iâˆˆchildren(n)Si (4)\nAn action, a, taken on node nonly needs to optimize the\nsub-tree rooted at naccording to the following expression,\nVn=argmaxaâˆˆAâˆ’(cÂ·Tn+(1âˆ’c)Â·Sn), (5)\nwhere cis a coefficient capturing the tradeoff between classi-\nfication time and memory footprint. The negation is needed\nsince we want to minimize time and space complexities.\nWhen câˆˆ{0,1}, it is easy to see that if at every tree node\nnwe take the action that optimizes Vn, then, by induction, we\nend up optimizing Vr, where ris the root of the tree. In otherwords, we end up optimizing the global objective (reward)\nfor the entire decision tree. For 0<c<1this optimization\nbecomes approximate, but we find empirically that ccan\nstill be used to interpolate between the two objectives. It is\nimportant to note here that while the state representation\nonly encodes current node n, action ataken for node nisnot\nlocal, as it optimizes the entire sub-tree rooted at n.\nIn summary, we only need to encode the current node as\nthe input state of the agent. This is because the environment\nbuilds the tree node-by-node, node actions need only con-\nsider their own state, and each node contains a subset of the\nrules of its parent (i.e., rules contained in some subspace of\nits parent space). Therefore, nodes in the tree can be com-\npletely defined by the ranges they occupy in each dimension.\nGiven ddimensions, we use 2dnumbers to encode a tree\nnode, which indicate the left and right boundaries of each\ndimension for this node. The state also needs to describe the\npartitioning at the node, which can be handled in a similar\nway. We defer a full description of the NeuroCuts state and\naction representations to Appendix A.\nTraining algorithm. We use an actor-critic algorithm to\ntrain the agentâ€™s policy [ 19]. This class of algorithms have\nbeen shown to provide state-of-the-art results in many use\ncases [ 5,36,43], and can be easily scaled to the distributed\nsetting [ 7]. We also experimented with Q-learning [ 38] based\napproaches, but found they did not perform as well.\nAlgorithm 1 shows the pseudocode of the NeuroCuts algo-\nrithm, which executes as follows. NeuroCuts starts with the\nroot node of the decision tree, sâˆ—. The end goal is to learn an\noptimized stochastic policy function Ï€(a|s;Î¸)(i.e., the actor).\nNeuroCuts first initializes all the parameters (line 1-6), and\nthen runs for Nrollouts to train the policy and the value\nfunction (line 7-23). After each rollout, it reinitializes the\ndecision tree to the root node (line 9). It then incrementally\nbuilds the tree by repeatedly selecting and applying an action\n7\n\non each non-terminal leaf node (line 11-13) according to the\ncurrent policy. A terminal leaf node is a node in which the\nnumber of rules is below a given threshold.\nMore specifically, NeuroCuts traverses the tree nodes in\ndepth-first-search (DFS) order (line 13), i.e., it recursively\ncuts the child of the current node until the node becomes a\nterminal leaf. Note that the DFS order is not essential. It is\nused to give a way for the agent to find a tree node to cut.\nOther orders, such as the breadth-first-search (BFS), can be\nused as well. After the decision tree is built, the gradients\nare reset (line 14), and then the algorithm iterates over all\nthe tree nodes to aggregate the gradients (line 15-21). Finally,\nNeuroCuts uses the gradients to update the parameters of\nthe actor and critic networks (line 22), and proceeds to the\nnext rollout (line 23).\nThe first gradient computation (line 19) corresponds to\nthat for the policy gradient loss . This loss defines the direction\nto updateÎ¸to improve the expected reward. An estimation\nof the state value V(s;Î¸v)is subtracted from the rollout re-\nward Rto reduce the gradient variance [ 20].Vis trained\nconcurrently to minimize its prediction error (line 21). Fig-\nure 5 visualizes the learning process of NeuroCuts to build a\ndecision tree. The NeuroCuts policy is stochastic, enabling it\nto effectively explore many different tree variations during\ntraining, as illustrated in Figure 6.\nIncorporating existing heuristics. NeuroCuts can easily\nincorporate additional heuristics to improve the decision\ntrees it learns. One example is adding rule partition actions.\nIn addition to the cut action, in our NeuroCuts implementa-\ntion we also allow two types of partition actions:\n(1)Simple : the current node is partitioned along a single\ndimension using a learned threshold.\n(2)EffiCuts : the current node is partitioned using the\nEffiCuts partition heuristic [55].\nScaling out to handle large packet classifiers. The pseu-\ndocode in Algorithm 1 is for a single-threaded implementa-\ntion of NeuroCuts. This is sufficient for small classifiers. But\nfor large classifiers with tens or hundreds of thousands of\nrules, parallelism can significantly improve the speed of train-\ning. In Figure 7 we show how Algorithm 1 can be adapted to\nbuild multiple decision trees in parallel.\nHandling classifier updates. Packet classifiers are often\nupdated by network operators based on application require-\nments, e.g., adding access control rules for new devices. For\nsmall updates of only a few rules, NeuroCuts modifies the\nexisting decision tree to reflect the changes. New rules are\nadded to the decision tree according to the existing struc-\nture; deleted rules are removed from the terminal leaf nodes.Algorithm 1 Learning a tree-generation policy using an\nactor-critic algorithm.\nInput: The root node sâˆ—where a tree always grows from.\nOutput: A stochastic policy function Ï€(a|s;Î¸)that outputs a branching\naction aâˆˆA given a node state s, and a value function V(s;Î¸v)that\noutputs a value estimate for a node state.\nMain routine:\n1:// Initialization\n2: Randomly initialize the model parameters Î¸,Î¸v\n3: Maximum number of rollouts N\n4: Coefficient câˆˆ[0,1]that trades off classification time vs. space\n5: Reward scaling function f(x)âˆˆ{x,log(x)}\n6:nâ†0\n7:// Training\n8:while n<Ndo\n9: sâ†Reset(sâˆ—)\n10: // Build a tree using the current policy\n11: while s,Null do\n12: aâ†Ï€(a|s;Î¸)\n13: sâ†GrowTreeDFS(s,a)\n14: Reset gradients dÎ¸â†0anddÎ¸vâ†0\n15: for(s,a)âˆˆTreeIterator(sâˆ—)do\n16: // Compute the future rewards for the given action\n17: Râ†âˆ’( cÂ·f(Time(s))+(1âˆ’c)Â·f(Space(s)))\n18: // Accumulate gradients wrt. policy gradient loss\n19: dÎ¸â†dÎ¸+âˆ‡Î¸logÏ€(a|s;Î¸)(Râˆ’V(s;Î¸v))\n20: // Accumulate gradients wrt. value function loss\n21: dÎ¸vâ†dÎ¸v+âˆ‚(Râˆ’V(s;Î¸v))2/âˆ‚Î¸v\n22: Perform update of Î¸using dÎ¸andÎ¸vusing dÎ¸v.\n23: nâ†n+1\nSubroutines :\nâ€¢Reset (s): Reset the tree sto its initial state.\nâ€¢GrowTreeDFS (s, a): Apply action ato tree node s, and return the\nnext non-terminal leaf node in the tree in depth-first traversal order.\nâ€¢TreeIterator (s): Non-terminal tree nodes of the subtree sand\ntheir taken action.\nâ€¢Time (s): Upper-bound on classification time to query the subtree s.\nIn non-partitioned trees this is simply the depth of the tree.\nâ€¢Space (s): Memory consumption of the subtree s.\nPolicy Evaluation \nPolicy Evaluation \nPolicy Evaluation Improve ðœƒ, ðœƒv via \nstochastic gradient \ndescent Concatenate \ntree rollouts \nBroadcast new values of ðœƒ\nFigure 7: NeuroCuts can be parallelized by generating\ndecision trees in parallel from the current policy.\nWhen enough small updates accumulate or a large update is\nmade to the classifier, NeuroCuts re-runs training.\n8\n\nNeural Packet Classification\n5 IMPLEMENTATION\nDeep RL algorithms are notoriously difficult to reproduce\n[15]. For a practical implementation, we prioritize the ability\nto(i)leverage off-the-shelf RL algorithms, and (ii)easily\nscale NeuroCuts to enable parallel training of policies.\nDecision tree implementation. We implement the deci-\nsion tree data structure for NeuroCuts in Python for ease of\ndevelopment. To ensure minor implementation differences\ndo not bias our results, we use this same data structure to\nimplement each baseline algorithm (e.g., HiCuts, EffiCuts,\netc.), as well as to implement NeuroCuts.\nBranching decision process environment. As discussed\nin Section 4, the branching structure of the NeuroCuts en-\nvironment poses a challenge due to its mismatch with the\nMDP formulation assumed by many RL algorithms. A typical\nRL environment defines a transition function Pa(st+1|st)and\na reward function Ra(s,sâ€²). The first difference is that the\nstate transition function in NeuroCuts returns multiple child\nstates, instead of a single state., i.e., (st,at)â†’{ s0\nt+1, ...,sk\nt+1}.\nSecond, the final reward for NeuroCuts is computed by aggre-\ngating across the rewards of child states. More precisely, for\nthe cut action we use maxaggregation for classification time\nandsumaggregation for memory footprint. For the partition\naction, we use sumaggregation for both metrics.\nThe recursive dependence of the NeuroCuts reward calcu-\nlation on all descendent state actions means that it is difficult\nto flatten the tree structure of the environment into a single\nMDP, which is required by existing off-the-shelf RL algo-\nrithms. Rather than attempting to flatten the NeuroCuts\nenvironment, our solution is to instead treat the NeuroCuts\nenvironment as a series of independent 1-step decision prob-\nlems, each of which yields an â€œimmediateâ€ reward. The actual\nreward for these 1-step decisions is calculated once the rele-\nvant sub-tree rollout is complete.\nFor example, consider a NeuroCuts tree rollout from a root\nnode s1. Based on Ï€Î¸the agent decides to take action a1to\nsplit s1intos2,s3, and s4. Of these child nodes, only s4needs\nto be further split (via a2), into s5ands6, which finishes the\ntree. The experiences collected from this rollout consist of\ntwo independent 1-step rollouts: ( s1,a1) and ( s4,a2). Taking\nthe time-space coefficient c=1and discount factor Î³=1\nfor simplicity, the total reward Rfor each rollout would be\nR=2andR=1respectively.\nMulti-agent implementation. Since these 1-step decisions\nare logically independent of each other, NeuroCuts execu-\ntion can be realized as a multi-agent environment, where\neach nodeâ€™s 1-decision problem is taken by an independent\nâ€œagentâ€ in the environment. Since we want to learn a single\npolicy,Ï€Î¸, for all states, the agents must be configured toshare the same underlying stochastic neural network policy.\nThis ensures all experiences go towards optimizing the single\nshared policy Ï€Î¸. When using an actor critic algorithm to\noptimize the policies of such agents, the relevant loss calcu-\nlations induced by this multi-agent realization are identical\nto those presented in Algorithm 1.\nThere are several ways to implement the 1-step formula-\ntion of NeuroCuts while leveraging off-the-shelf RL libraries.\nIn Algorithm 1 we show standalone single-threaded pseu-\ndocode assuming a simple actor-critic algorithm is used. In\nour experiments, we use the multi-agent API provided by\nRay RLlib [ 31], which implements parallel simulation and\noptimization of such RL environments.\nPerformance. We found that NeuroCuts often converges to\nits optimal solution within just a few hundred rollouts. The\nsize of the rule set does not significantly affect the number\nof rollouts needed for convergence, but affects the running\ntime of each rollout. For smaller problems (e.g., 1000 rules),\nthis may be within a few minutes of CPU time. The compu-\ntational overhead for larger problem scales with the size of\nthe classifier, i.e., linearly with the number of rules that must\nbe scanned per action taken to grow the tree. The bulk of\ntime in NeuroCuts is spent executing tree cut actions. This is\nlargely an artifact of our Python implementation, which iter-\nates over each rule present in a node on each cut action. An\noptimized C++ implementation of the decision tree would\nfurther reduce the training time.\n5.1 Optimizations\nRollout truncation. During the initial phase of learning,\nthe unoptimized policy will create excessively large trees.\nSince NeuroCuts does not start learning until a tree is com-\nplete, it is necessary to truncate rollouts to speed up the\ninitial phase of training. For larger classifiers, we found it\nnecessary to allow rollouts of up to 15000 actions in length.\nDepth truncation. Since valid solutions never involve trees\nof depth greater than a few hundred, we also truncate trees\nonce they reach a certain depth. In our experience, depth\ntruncation is only a factor early on in learning; NeuroCuts\nquickly learns to avoid creating very deep trees.\nProximal Policy Optimization. For better stability and\nmore sample-efficient learning, in our experiments we choose\nto use Proximal Policy Optimization (PPO) [ 43]. PPO imple-\nments an actor-critic style loss with entropy regularization\nand a clipped surrogate objective, which enables improved\nexploration and sample efficiency. We report the PPO hyper-\nparameters we used in Appendix B. It is important to note\nhowever that this particular choice of RL algorithm is not\nfundamental to NeuroCuts.\n9\n\n6 EVALUATION\nIn the evaluation, we seek to answer the following questions:\n(1)How does NeuroCuts compare to the state-of-the-art\napproaches in terms of classification time and memory\nfootprint? (Section 6.1 and 6.2)\n(2)Beyond tabula rasa learning, can NeuroCuts effectively\nincorporate and improve upon pre-engineered heuris-\ntics? (Section 6.3)\n(3)How much influence does the time-space coefficient c\nhave on the performance of NeuroCuts? (Section 6.4)\nFor the results presented in the next sections, we evalu-\nated NeuroCuts using the range of hyperparameters shown\nin Appendix B. We did not otherwise perform extensive\nhyperparameter tuning; in fact we use close to the default\nhyperparameter configuration of the PPO algorithm. The\nnotable hyperparameters we swept over include:\nâ€¢Allowed top-node partitioning (none, simple, and the\nEffiCuts heuristic), which strongly biases NeuroCuts\ntowards learning trees optimized for time (none) vs\nspace (EffiCuts), or somewhere in the middle (simple).\nâ€¢The max number of timesteps allowed per rollout be-\nfore truncation. It must be large enough to enable solv-\ning the problem, but not so large that it slows down\nthe initial phase of training.\nâ€¢We also experimented with values for the time-space\ntradeoff coefficient câˆˆ{0,0.1,0.5,1}. When c<1, we\nused log(x)as the reward scaling function to simplify\nthe combining of the time and space rewards.\nWe ran NeuroCuts on m4.16xl AWS machines, with four\nCPU cores used per NeuroCuts instance to speed up the\nexperiment. Because the neural network model and data\nsizes produced by NeuroCuts are quite small (e.g., in contrast\nto image observations from Atari games), the use of GPUs is\nnot necessary. Our main training bottleneck was the Python\nimplementation of the decision tree. We ran each NeuroCuts\ninstance for up to 10 million timesteps (i.e., up to a couple\nthousand generated trees in total), or until convergence.\nWe compare NeuroCuts with four hand-tuned algorithms:\nHiCuts [ 13], HyperCuts [ 47], EffiCuts [ 55], and CutSplit [ 30].\nWe use the standard benchmark, ClassBench [ 52], to generate\npacket classifiers with different characteristics and sizes. We\nuse two metrics: classification time (tree depth) and memory\nfootprint (bytes per rule).\nWe find that NeuroCuts significantly improves over all\nbaselines in classification time while also generating signif-\nicantly more compact trees. NeuroCuts is also competitive\nwhen optimizing for memory, with a 25% median space im-\nprovement over EffiCuts without compromising in time.6.1 Time-Optimized NeuroCuts\nIn Figure 8, we compare the best time-optimized trees gener-\nated by NeuroCuts against HiCuts, HyperCuts, EffiCuts, and\nCutSplit in the ClassBench classifiers. NeuroCuts provides\na 20%, 38%, 52% and 56% median improvement over HiCuts,\nHyperCuts, EffiCuts, and CutSplit respectively. NeuroCuts\nalso does better than the minimum of all baselines in 70% of\nthe cases, with a median all-baseline improvement of 18%,\naverage improvement of 12%, and best-case improvement\nof 58%. These time-optimized trees generally correspond to\nNeuroCuts runs with either no partitioning action or the\nsimple top-node partitioning action.\n6.2 Space-Optimized NeuroCuts\nWe again compare NeuroCuts against the baselines in Fig-\nure 9, this time selecting the most space-optimized trees and\ncomparing the memory footprint (bytes per rule). As ex-\npected, NeuroCuts does significantly better than HiCuts and\nHyperCuts since it can learn to leverage the partition action.\nNeuroCutâ€™s space-optimized trees show a 40% median and\n44% mean improvement over EffiCuts. In our experiments\nNeuroCuts does not usually outperform CutSplit in memory\nfootprint, with a 26% higher median memory usage com-\npared to CutSplit, though the best case improvement is still\n3Ã—(66%) over all baselines .\nSeparately, we also note that the memory footprints of\nthe best time-optimized trees generated by NeuroCuts are\nsignificantly lower than those generated by HiCuts and Hy-\nperCuts, with a >100 Ã—median space improvement along\nwith the better classification times reported in Section 6.1.\nHowever, these time-optimized trees are not competitive\nin space with the space-optimized NeuroCuts, EffiCuts and\nCutSplit trees.\n6.3 Improving on EffiCuts\nIn Figure 10 we examine a set of 36 NeuroCuts trees (one tree\nfor each ClassBench classifier) generated by NeuroCuts with\nthe EffiCuts partition action. This is in contrast with the prior\nexperiments that selected trees optimized for either space or\ntime alone. On this 36-tree set, there is a median space im-\nprovement of 29% relative to EffiCuts; median classification\ntime is about the same. This shows that NeuroCuts is able\nto effectively incorporate and improve on pre-engineered\nheuristics such as the EffiCuts top-level partition function.\nSurprisingly, NeuroCuts is able to outperform EffiCuts de-\nspite the fact that NeuroCuts does not use multi-dimensional\ncut actions. When we evaluate EffiCuts with these cut types\ndisabled, the memory advantage of NeuroCuts widens to\n10\n\nNeural Packet Classification\nClassification Time\n0204060\nacl1 1kacl2 1kacl3 1kacl4 1kacl5 1kacl1 10kacl2 10kacl3 10kacl4 10kacl5 10kacl1 100kacl2 100kacl3 100kacl4 100kacl5 100kfw1 1kfw2 1kfw3 1kfw4 1kfw5 1kfw1 10kfw2 10kfw3 10kfw4 10kfw5 10kfw1 100kfw2 100kfw3 100kfw4 100kfw5 100kipc1 1kipc2 1kipc1 10kipc2 10kipc1 100kipc2 100kHiCuts HyperCuts EffiCuts CutSplit NeuroCuts\nFigure 8: Classification time (tree depth) for HiCuts, HyperCuts, EffiCuts, and NeuroCuts (time-optimized). We\nomit four entries for HiCuts and HyperCuts that did not complete after more than 24 hours.\nFigure 9: Memory footprint (bytes per rule) used for HiCuts, HyperCuts, EffiCuts, and NeuroCuts (space-\noptimized). We omit four entries for HiCuts and HyperCuts that did not complete after more than 24 hours.\n67% at the median. This suggests that NeuroCuts could fur-\nther improve its performance if we also incorporate multi-\ndimensional cut actions via parametric action encoding tech-\nniques [ 9]. It would also be interesting to, besides adding\nactions to NeuroCuts, consider postprocessing steps such as\nresampling that can be used to further improve the stochastic\npolicy output.\n6.4 Tuning Time vs Space\nFinally, in Figure 11 we sweep across a range of values of c\nfor NeuroCuts with the simple partition method and log(x)\nreward scaling. We plot the ClassBench median of the best\nclassification times and bytes per rule found for each clas-\nsifier. We find that classification time improves by 2Ã—as\ncâ†’1, while the number of bytes per rule improves 2 Ã—ascâ†’0. This shows that cis effective in controlling the\ntradeoff between space and time.\n7 RELATED WORK\nPacket classification. Packet classification is a long-standing\nproblem in computer networking. Decision-tree based algo-\nrithms are a major class of algorithmic solutions. Existing so-\nlutions rely on hand-tuned heuristics to build decision trees.\nHiCuts [ 13] is a pioneering work in this space. It cuts the\nspace of each node in one dimension to create multiple equal-\nsized subspaces to separate rules. HyperCuts [ 47] extends Hi-\nCuts by allowing cutting in multiple dimensions at each node.\nHyperSplit [ 41] combines the advantages of rule-based space\ndecomposition and local-optimized recursion to guarantee\nworst-case classification time and reduce memory footprint.\n11\n\nSpace Improvement (1 - a/b) 00.250.50.751(a) NeuroCuts can build on the EffiCuts partitioner to generate trees up to\n10Ã—(90%) more space efficient than EffiCuts. In this experiment NeuroCuts\ndid as well or better than EffiCuts on all 36 rule sets.\nTime Improvement (1 - a/b)-0.4-0.200.20.4\n(b) NeuroCuts with the EffiCuts partitioner generates trees with about the\nsame time efficiency as EffiCuts.\nFigure 10: Sorted rankings of NeuroCutsâ€™ improve-\nment over EffiCuts in the ClassBench benchmark.\nHere NeuroCuts is run with only the EffiCuts par-\ntition method allowed. Positive values indicate im-\nprovements.\n1040\n0 1Median classification time Median bytes per rule\nvalue of c\nFigure 11: The classification time improves by 2 Ã—as\nthe time-space coefficient câ†’1, and conversely, num-\nber of bytes per rule improves 2 Ã—ascâ†’0.\nEffiCuts [ 55] introduces four heuristics, including separable\ntrees, tree merging, equal-dense cuts and node co-location, to\nreduce rule replication and imbalance cutting. CutSplit [ 30]\nintegrates equal-sized cutting and equal-dense cutting to op-\ntimize decision trees. Besides decision-tree based algorithms,\nthere are also other algorithms proposed for packet classifica-\ntion, such as tuple space search [ 49], RFC [ 12] and DCFL [ 53].\nThese algorithms are not as popular as decision-tree based\nalgorithms, because they are either too slow or consume too\nmuch memory. There are also solutions that exploit special-\nized hardware such as TCAMs, GPUs and FPGAs to support\npacket classification [ 17,24,32,33,42,48,50,57]. Compared\nto existing work, NeuroCuts is an algorithmic solution thatapplies Deep RL to generate efficient decision trees, with the\ncapability to incorporate and improve on existing heuristics\nas needed.\nDecision trees for machine learning. There have been\nseveral proposals to use deep learning to optimize the per-\nformance of decision trees for machine learning problems\n[21,39,59]. In these settings, the objective is maximizing\ntest accuracy. In contrast, packet classification decision trees\nprovide perfect accuracy by construction, and the objective\nis minimizing classification time and memory usage.\nStructured data in deep learning. There have many re-\ncent proposals towards applying deep learning to process and\ngenerate tree and graph data structures [ 11,58,60,61,63,66].\nNeuroCuts sidesteps the need to explicitly process graphs,\ninstead exploiting the structure of the problem to encode\nagent state into a compact fixed-length representation.\nDeep reinforcement learning. Deep RL leverages the mod-\neling capacity of deep neural networks to extend classical\nRL to domains with large, high-dimensional state and ac-\ntion spaces. DQN [ 37,38,56] is one of the earliest successes\nof Deep RL, and shows how to learn control policies from\nhigh-dimensional sensory inputs and achieve human-level\nperformance in Atari 2600 games. A3C, PPO, and IMPALA\n[7,36,43] scale actor-critic algorithms to leverage many\nparallel workers. AlphaGo [ 44], AlphaGo Zero [ 46] and Al-\nphaZero [ 45] show that Deep RL algorithms can achieve\nsuperhuman performance in many challenging games like\nGo, chess and shogi. Deep RL has also been applied to many\nother domains like natural language processing [ 29] and\nrobotics [ 26â€“28]. NeuroCuts works in a discrete environ-\nment and applies Deep RL to learn decision trees for packet\nclassification.\nDeep learning for networking and systems. Recently\nthere has been an uptake in applying deep learning to net-\nworking and systems problems [ 4,6,16,34,35,54,62,64,65].\nNAS [ 62] utilizes client computation and deep neural net-\nworks to improve the video quality independent to the avail-\nable bandwidth. Pensieve [ 35] generates adaptive bitrate al-\ngorithms using Deep RL without relying on pre-programmed\nmodels or assumptions about the environment. Valadarsky\net al. [54] applies Deep RL to learn network routing. Chin-\nchali et al. [4] uses Deep RL for traffic scheduling in cellular\nnetworks. AuTO [ 3] scales Deep RL for datacenter-scale traf-\nfic optimization. There are also many solutions that apply\ndeep reinforcement learning to congestion control [ 6,16,64]\nand resource management [ 34]. We explore the application\nof Deep RL to packet classification, and propose a new al-\ngorithm to learn decision trees with succinct encoding and\nscalable training mechanisms.\n12\n\nNeural Packet Classification\n8 CONCLUSION\nWe present NeuroCuts, a simple and effective Deep RL for-\nmulation of the packet classification problem. NeuroCuts\nprovides significant improvements on classification time and\nmemory footprint compared to state-of-the-art algorithms. It\ncan easily incorporate pre-engineered heuristics to leverage\ntheir domain knowledge, optimize for flexible objectives, and\ngenerates decision trees which are easy to test and deploy\nin any environment.\nWe hope NeuroCuts can inspire a new generation of learning-\nbased algorithms for packet classification. As a concrete ex-\nample, NeuroCuts currently optimizes for the worst-case\nclassification time or memory footprint. By considering a\nspecific traffic pattern, NeuroCuts can be extended to other\nobjectives such as average classification time. This would\nallow NeuroCuts to not only optimize for a specific classifier\nbut also for a specific traffic pattern in a given deployment.\n13\n\nREFERENCES\n[1]Florin Baboescu, Sumeet Singh, and George Varghese. 2003. Packet\nclassification for core routers: Is there an alternative to CAMs?. In IEEE\nINFOCOM .\n[2]Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,\nJohn Schulman, Jie Tang, and Wojciech Zaremba. 2016. Openai gym.\narXiv preprint arXiv:1606.01540 (2016).\n[3]Li Chen, Justinas Lingys, Kai Chen, and Feng Liu. 2018. AuTO: Scaling\nDeep Reinforcement Learning for Datacenter-Scale Automatic Traffic\nOptimization. In ACM SIGCOMM .\n[4]Sandeep Chinchali, Pan Hu, Tianshu Chu, Manu Sharma, Manu Bansal,\nRakesh Misra, Marco Pavone, and Sachin Katti. 2018. Cellular network\ntraffic scheduling with deep reinforcement learning. In AAAI .\n[5]Ekin Dogus Cubuk, Barret Zoph, Dandelion ManÃ©, Vijay Vasudevan,\nand Quoc V. Le. 2018. AutoAugment: Learning Augmentation Policies\nfrom Data. CoRR (2018).\n[6]Mo Dong, Tong Meng, Doron Zarchy, Engin Arslan, Yossi Gilad,\nBrighten Godfrey, and Michael Schapira. 2018. PCC Vivace: Online-\nLearning Congestion Control. In USENIX NSDI .\n[7]Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan,\nVolodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley,\nIain Dunning, et al .2018. IMPALA: Scalable distributed Deep-RL\nwith importance weighted actor-learner architectures. arXiv preprint\narXiv:1802.01561 (2018).\n[8]Kousha Etessami and Mihalis Yannakakis. 2005. Recursive Markov\ndecision processes and recursive stochastic games. In International\nColloquium on Automata, Languages, and Programming . Springer, 891â€“\n903.\n[9]Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen\nHe, Zachary Kaden, Vivek Narayanan, and Xiaohui Ye. 2018. Horizon:\nFacebookâ€™s Open Source Applied Reinforcement Learning Platform.\narXiv preprint arXiv:1811.00260 (2018).\n[10] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013.\nSpeech recognition with deep recurrent neural networks. In ICASSP .\n[11] Arthur Guez, ThÃ©ophane Weber, Ioannis Antonoglou, Karen Simonyan,\nOriol Vinyals, Daan Wierstra, RÃ©mi Munos, and David Silver. 2018.\nLearning to search with MCTSnets. arXiv preprint arXiv:1802.04697\n(2018).\n[12] Pankaj Gupta and Nick McKeown. 1999. Packet classification on\nmultiple fields. SIGCOMM CCR (1999).\n[13] Pankaj Gupta and Nick McKeown. 1999. Packet classification using\nhierarchical intelligent cuttings. In Hot Interconnects .\n[14] Pankaj Gupta and Nick McKeown. 2001. Algorithms for packet classi-\nfication. (2001).\n[15] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina\nPrecup, and David Meger. 2017. Deep Reinforcement Learning that\nMatters. CoRR (2017).\n[16] Nathan Jay, Noga H Rotman, P Godfrey, Michael Schapira, and Aviv\nTamar. 2018. Internet Congestion Control via Deep Reinforcement\nLearning. arXiv preprint arXiv:1810.03259 (2018).\n[17] Kirill Kogan, Sergey Nikolenko, Ori Rottenstreich, William Culhane,\nand Patrick Eugster. 2014. SAX-PAC (scalable and expressive packet\nclassification). In SIGCOMM CCR .\n[18] AN Kolmogorov and NA Dmitriev. 1947. Stochastic branching pro-\ncesses. In Doklady Akademi Nauk SSSR , Vol. 56. 7â€“10.\n[19] Vijay R Konda and John N Tsitsiklis. 2000. Actor-critic algorithms. In\nAdvances in neural information processing systems .\n[20] Vijay R Konda and John N Tsitsiklis. 2000. Actor-critic algorithms. In\nAdvances in neural information processing systems . 1008â€“1014.\n[21] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel\nRota Bulo. 2015. Deep Neural Decision Forests. In The IEEE Interna-\ntional Conference on Computer Vision (ICCV) .[22] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis.\n2018. The case for learned index structures. In SIGMOD .\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet\nclassification with deep convolutional neural networks. In Advances\nin neural information processing systems .\n[24] Karthik Lakshminarayanan, Anand Rangarajan, and Srinivasan Venkat-\nachary. 2005. Algorithms for advanced packet classification with\nternary CAMs. In SIGCOMM CCR .\n[25] John Langford and Tong Zhang. 2008. The epoch-greedy algorithm\nfor multi-armed bandits with side information. In Advances in neural\ninformation processing systems . 817â€“824.\n[26] Ian Lenz, Honglak Lee, and Ashutosh Saxena. 2015. Deep learning for\ndetecting robotic grasps. The International Journal of Robotics Research\n(2015).\n[27] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. 2016.\nEnd-to-end training of deep visuomotor policies. The Journal of Ma-\nchine Learning Research (2016).\n[28] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre\nQuillen. 2018. Learning hand-eye coordination for robotic grasping\nwith deep learning and large-scale data collection. The International\nJournal of Robotics Research (2018).\n[29] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan\nJurafsky. 2016. Deep reinforcement learning for dialogue generation.\narXiv preprint arXiv:1606.01541 (2016).\n[30] Wenjun Li, Xianfeng Li, Hui Li, and Gaogang Xie. 2018. CutSplit: A\nDecision-Tree Combining Cutting and Splitting for Scalable Packet\nClassification. In IEEE INFOCOM .\n[31] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox,\nKen Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. 2018.\nRLlib: Abstractions for distributed reinforcement learning. In ICML .\n[32] Alex X Liu, Chad R Meiners, and Yun Zhou. 2008. All-match based\ncomplete redundancy removal for packet classifiers in TCAMs. In IEEE\nINFOCOM .\n[33] Yadi Ma and Suman Banerjee. 2012. A smart pre-classifier to reduce\npower consumption of TCAMs for multi-dimensional packet classifi-\ncation. In ACM SIGCOMM .\n[34] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kan-\ndula. 2016. Resource management with deep reinforcement learning.\nInACM SIGCOMM HotNets Workshop .\n[35] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. 2017. Neural\nadaptive video streaming with pensieve. In ACM SIGCOMM .\n[36] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex\nGraves, Timothy Lillicrap, Tim Harley, David Silver, and Koray\nKavukcuoglu. 2016. Asynchronous methods for deep reinforcement\nlearning. In ICML .\n[37] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioan-\nnis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing\natari with deep reinforcement learning. arXiv preprint arXiv:1312.5602\n(2013).\n[38] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,\nJoel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, An-\ndreas K Fidjeland, Georg Ostrovski, et al .2015. Human-level control\nthrough deep reinforcement learning. Nature (2015).\n[39] Mohammad Norouzi, Maxwell Collins, Matthew A Johnson, David J\nFleet, and Pushmeet Kohli. 2015. Efficient non-greedy optimization of\ndecision trees. In Advances in Neural Information Processing Systems .\n1729â€“1737.\n[40] Stanley R Pliska. 1976. Optimization of multitype branching processes.\nManagement Science 23, 2 (1976), 117â€“124.\n[41] Yaxuan Qi, Lianghong Xu, Baohua Yang, Yibo Xue, and Jun Li. 2009.\nPacket classification algorithms: From theory to practice. In IEEE IN-\nFOCOM .\n14\n\nNeural Packet Classification\n[42] Yun R Qu, Hao H Zhang, Shijie Zhou, and Viktor K Prasanna. 2015. Op-\ntimizing many-field packet classification on FPGA, multi-core general\npurpose processor, and GPU. In ACM/IEEE Symposium on Architectures\nfor Networking and Communications Systems .\n[43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and\nOleg Klimov. 2017. Proximal policy optimization algorithms. arXiv\npreprint arXiv:1707.06347 (2017).\n[44] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,\nGeorge Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,\nVeda Panneershelvam, Marc Lanctot, et al .2016. Mastering the game\nof Go with deep neural networks and tree search. Nature (2016).\n[45] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan\nKumaran, Thore Graepel, et al .2018. A general reinforcement learning\nalgorithm that masters chess, shogi, and Go through self-play. Science\n(2018).\n[46] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis\nAntonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker,\nMatthew Lai, Adrian Bolton, et al .2017. Mastering the game of Go\nwithout human knowledge. Nature (2017).\n[47] Sumeet Singh, Florin Baboescu, George Varghese, and Jia Wang. 2003.\nPacket classification using multidimensional cutting. In ACM SIG-\nCOMM .\n[48] Ed Spitznagel, David Taylor, and Jonathan Turner. 2003. Packet classi-\nfication using extended TCAMs. In IEEE ICNP .\n[49] Venkatachary Srinivasan, Subhash Suri, and George Varghese. 1999.\nPacket classification using tuple space search. In SIGCOMM CCR .\n[50] Weibin Sun and Robert Ricci. 2013. Fast and flexible: parallel packet\nprocessing with GPUs and click. In ACM/IEEE Symposium on Architec-\ntures for Networking and Communications Systems .\n[51] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to\nsequence learning with neural networks. In Advances in neural infor-\nmation processing systems .\n[52] David E Taylor and Jonathan S Turner. 2005. ClassBench: A packet\nclassification benchmark. In IEEE INFOCOM .\n[53] David E Taylor and Jonathan S Turner. 2005. Scalable packet classifica-\ntion using distributed crossproducing of field labels. In IEEE INFOCOM .\n[54] Asaf Valadarsky, Michael Schapira, Dafna Shahaf, and Aviv Tamar.\n2017. Learning to route with deep rl. In NIPS Deep Reinforcement\nLearning Symposium .\n[55] Balajee Vamanan, Gwendolyn Voskuilen, and T. N. Vijaykumar. 2010.\nEffiCuts: Optimizing Packet Classification for Memory and Through-\nput. In ACM SIGCOMM .\n[56] Hado Van Hasselt, Arthur Guez, and David Silver. 2016. Deep Rein-\nforcement Learning with Double Q-Learning. In AAAI .\n[57] Matteo Varvello, Rafael Laufer, Feixiong Zhang, and TV Lakshman.\n2016. Multilayer packet classification with graphics processing units.\nIEEE/ACM Transactions on Networking (2016).\n[58] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014.\nKnowledge Graph Embedding by Translating on Hyperplanes.. In\nAAAI .\n[59] Zheng Xiong, Wenpeng Zhang, and Wenwu Zhu. 2017. Learning\ndecision trees with reinforcement learning. In NIPS Workshop on Meta-\nLearning .\n[60] Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang Zhang, Qiang\nYang, and Stephen Lin. 2007. Graph embedding and extensions: A\ngeneral framework for dimensionality reduction. IEEE transactions on\npattern analysis and machine intelligence (2007).\n[61] Jianchao Yang, Shuicheng Yang, Yun Fu, Xuelong Li, and Thomas\nHuang. 2008. Non-negative graph embedding. In CVPR .[62] Hyunho Yeo, Youngmok Jung, Jaehong Kim, Jinwoo Shin, and Dongsu\nHan. 2018. Neural adaptive content-aware internet video delivery. In\nUSENIX OSDI .\n[63] Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec.\n2018. Graph Convolutional Policy Network for Goal-Directed Molecu-\nlar Graph Generation. arXiv preprint arXiv:1806.02473 (2018).\n[64] Yasir Zaki, Thomas PÃ¶tsch, Jay Chen, Lakshminarayanan Subrama-\nnian, and Carmelita GÃ¶rg. 2015. Adaptive congestion control for\nunpredictable cellular networks. In SIGCOMM CCR .\n[65] Ying Zheng, Ziyu Liu, Xinyu You, Yuedong Xu, and Junchen Jiang.\n2018. Demystifying Deep Learning in Networking. In ACM SIGCOMM\nAPNet Workshop .\n[66] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and\nMaosong Sun. 2018. Graph Neural Networks: A Review of Methods\nand Applications. arXiv preprint arXiv:1812.08434 (2018).\nA NEUROCUTS ACTION AND\nOBSERVATION SPACES\nNeuroCuts action and observation spaces described in Ope-\nnAI Gym format [ 2]. Actions are sampled from two cate-\ngorical distributions that select the dimension and action to\nperform on the dimension respectively. Observations are en-\ncoded in a one-hot bit vector (278 bits in total) that describes\nthe node ranges, partitioning info, and action mask (i.e., for\nprohibiting partitioning actions at lower levels).\nA.1 Action Space\nTuple(Discrete(NumDims),\nDiscrete(NumCutActions + NumPartitionActions))\nA.2 Observation Space\nBox(low=0, high=1, shape=(278,))\nA.3 Observation Components\n(BinaryString( RanÐ´edim\nmin) + BinaryString( RanÐ´edim\nmax) +\nOneHot( Partitiondim\nmin) + OneHot( Partitiondim\nmax))\nâˆ€dimâˆˆ{SrcIP ,DstIP ,SrcPort ,DstPort ,Protocol}+\nOneHot(EffiCutsPartitionID) + ActionMask\nWhen not using the EffiCuts partitioner, the Partitiondim\nrule dimension coverage thresholds are set to one of the\nfollowing discrete levels: 0%, 2%, 4%, 8%, 16%, 32%, 64%, and\n100%.\nWe note that the set of rules for the packet classifier are\nnotpresent in the observation space. NeuroCuts learns to\naccount for packet classifier rules implicitly through the\nrewards it gets from the environment.\nB NEUROCUTS HYPERPARAMETERS15\n\nHyperparameter Value\nTime-space coefficient c <set by user>\nTop-node partitioning {none, simple, EffiCuts}\nReward scaling function f {x,log(x)}\nMax timesteps per rollout {1000, 5000, 15000}\nMax tree depth {100, 500}\nMax timesteps to train 10000000\nMax timesteps per batch 60000\nModel type fully-connected\nModel nonlinearity tanh\nModel hidden layers [512, 512]\nWeight sharing between Î¸,Î¸vtrue\nLearning rate 0.00005\nDiscount factor Î³ 1.0\nPPO entropy coefficient 0.01\nPPO clip param 0.3\nPPO VF clip param 10.0\nPPO KL target 0.01\nSGD iterations per batch 30\nSGD minibatch size 1000\nTable 1: NeuroCuts hyperparameters. Values in curly\nbraces denote a set of values searched over during\nevaluation. We found that the most sensitive hyper-\nparameter is the top-node partitioning, which greatly\naffects the structure of the search problem. It is also\nimportant to ensure that the rollout timestep limit\nand model used are sufficiently large for the problem\n(we found that using 256-unit hidden layers slightly\ndegraded learning for larger classifiers, and more se-\nverely so at 64-units).\n16",
  "textLength": 72562
}