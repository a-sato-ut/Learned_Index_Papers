{
  "paperId": "9faf9029981a0b92abdb983a60f2af96eca3aea2",
  "title": "Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues",
  "pdfPath": "9faf9029981a0b92abdb983a60f2af96eca3aea2.pdf",
  "text": "Received July 27, 2020, accepted August 8, 2020, date of publication August 17, 2020, date of current version August 28, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3017135\nVideo Big Data Analytics in the Cloud: A\nReference Architecture, Survey, Opportunities,\nand Open Research Issues\nAFTAB ALAM\n , IRFAN ULLAH\n , AND YOUNG-KOO LEE\n , (Member, IEEE)\nDepartment of Computer Science and Engineering, Kyung Hee University (Global Campus), Yongin 1732, South Korea\nCorresponding author: Young-Koo Lee (yklee@khu.ac.kr)\nThis work was supported by the Institute for Information and Communications Technology Promotion Grant through the Korean\nGovernment (MSIT) (SIAT CCTV Cloud Platform) under Grant R7120-17-1007.\nABSTRACT The proliferation of multimedia devices over the Internet of Things (IoT) generates an\nunprecedented amount of data. Consequently, the world has stepped into the era of big data. Recently, on the\nrise of distributed computing technologies, video big data analytics in the cloud has attracted the attention of\nresearchers and practitioners. The current technology and market trends demand an ef\u001ccient framework\nfor video big data analytics. However, the current work is too limited to provide a complete survey of\nrecent research work on video big data analytics in the cloud, including the management and analysis of\na large amount of video data, the challenges, opportunities, and promising research directions. To serve this\npurpose, we present this study, which conducts a broad overview of the state-of-the-art literature on video\nbig data analytics in the cloud. It also aims to bridge the gap among large-scale video analytics challenges,\nbig data solutions, and cloud computing. In this study, we clarify the basic nomenclatures that govern the\nvideo analytics domain and the characteristics of video big data while establishing its relationship with\ncloud computing. We propose a service-oriented layered reference architecture for intelligent video big data\nanalytics in the cloud. Then, a comprehensive and keen review has been conducted to examine cutting-edge\nresearch trends in video big data analytics. Finally, we identify and articulate several open research issues\nand challenges, which have been raised by the deployment of big data technologies in the cloud for video big\ndata analytics. To the best of our knowledge, this is the \u001crst study that presents the generalized view of the\nvideo big data analytics in the cloud. This paper provides the research studies and technologies advancing\nthe video analyses in the era of big data and cloud computing.\nINDEX TERMS Big data, intelligent video analytics, cloud-based video analytics system, video analytics\nsurvey, deep learning, distributed computing, intermediate results orchestration, cloud computing.\nI. INTRODUCTION\nVideos are generated and uploaded regularly to the cloud.\nMany sources include CCTV, smartphones, drones, etc., are\nactively contributing to video generation leads to the evolu-\ntion of Intelligent Video Analytics (IV A) and management\nsystems. IV Ais a domain that uses advanced computer vision\ntechnologies to process and extract insights from streaming\nor stored videos automatically. Over the past two decades,\nIV A is extensively rising from real-world needs driving by a\nbroader range of application domains ranging from security\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Xin Luo\n .and surveillance to transportation and healthcare. The IV A\nmarket is anticipated to rise from 4.9 billion USD in 2020 to\n11.7 billion USD by 2025 at a compound annual growth rate\nof 19.0% [1].\nVideo management and services providers such as Face-\nbook [2], YouTube [3], and Net\u001dix [4] are considered as\nvaluable sources of large-scale video data. Along with these,\nvarious leading industrial organizations have successfully\ndeployed video management and analytics platforms that\nprovide more bandwidth and high-resolution cameras col-\nlecting videos at scale and has become one of the lat-\nest trends in the video surveillance industry. For example,\nmore than 400 hours of videos are uploaded in a minute\nVOLUME 8, 2020This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/152377\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\non Youtube [5], and more than one hundred and seventy\nmillion video surveillance cameras have been installed in\nchina only [6]. It has been reported that the data gen-\nerated by various IoT devices will see a growth rate\nof 28.7% over the period 2018-2025, where surveillance\nvideos are the majority shareholder [7].\nSuch an enormous video data is considered as ``big data''\nbecause a variety of sources generates a large volume of\nvideo data at high velocity that holds high Value. Even though\n65% of the big data shares hold by surveillance videos are\nmonitored, but still, a signi\u001ccant portion of video data has\nbeen failed to notice [8]. That neglected data contain valuable\ninformation directly related to real-world situations. Video\ndata provide information about interactions, behaviors, and\npatterns, whether its traf\u001cc or human patterns. However,\nhandling such a large amount of complex video data is not\nworthwhile utilizing traditional data analytical approaches.\nTherefore, more comprehensive and sophisticated solutions\nare required to manage and analyses such large-scale unstruc-\ntured video data.\nDue to the data-intensive and resources hungry nature of\nlarge scale video data processing, extracting the insights from\nthe video is a challenging task. A considerable size of video\ndata poses signi\u001ccant challenges for video management and\nmining systems that require powerful machines to deal with\nlarge-scale video data. Moreover, a \u001dexible solution is nec-\nessary to store and mine this large volume of video data\nfor decision making. However, large-scale video analytics\nbecomes a reality due to the popularity of big data and cloud\ncomputing technologies.\nCloud computing is an infrastructure for providing con-\nvenient and ubiquitous remote access to a shared pool of\ncon\u001cgurable computing resources. These resources can be\nmanaged with minimal management effort or service [9]. Big\ndata technologies, such as Hadoop or Spark echo system,\nare software platforms designed for distributed computing\nto process, analyze, and extract the valuable insights from\nlarge datasets in a scalable and reliable way. The cloud\nis preferably appropriate to offer the big data computation\npower required for the processing of these large datasets. [10],\nAmazon web service [11], Microsoft Azure [12], and Oracle\nBig Data Analytics [13] are some examples of video big data\nanalytics platforms. Large-scale video analytics in the cloud\nis a multi-disciplinary area, and the next big thing in big\ndata, which opens new research avenues for researchers and\npractitioners.\nThis work aims to conduct a comprehensive study on the\nstatus of large scale video analytics in the cloud-computing\nenvironment while deploying video analytics techniques.\nFirst, this study builds the relationship between video big\ndata and cloud computing and de\u001cnes the terminologies that\ngovern the study. Then service-oriented and a layered refer-\nence architecture have been proposed for large-scale video\nanalytics in the cloud while focusing on architectural proper-\nties like reliability, scalability, fault-tolerance, extensibility,\nand intermediate results orchestration. Further, an intensivesurvey has been conducted to project the current research\ntrends in video analytics that encompass the taxonomy of\nvideo analytics approaches, and cloud-based scholarly and\nindustrial study. Finally, open research issues and challenges\nare discussed, with a focus on proposed architecture, i.e., the\ndeployment of an array of computer vision algorithms for\nlarge-scale videos in the cloud.\nA. VIDEO BIG DATA, CLOUD COMPUTING, AND THEIR\nRELATIONSHIP\nThe term big data appeared and popularized by\nJohn R. Masey in the late 1990s [14], which refers to a large\nvolume of data that are impractical to be stored, processed\nand analyzed using traditional data management and process-\ning technologies [15]. The data can be unstructured, semi-\nstructured, and structured data, but mostly unstructured data\nis considered. The de\u001cnition of big data evolved and has\nbeen described in terms of three, four, or \u001cve characteristics.\nIn literature, among these characteristics, three are shared,\ni.e., Volume, Velocity, and Variety, while the others are\nVeracity and Value [16]\u0015[19]. Various video stream sources\ngenerate a considerable amount of unstructured video data on\na regular bases and becoming a new application \u001celd of big\ndata. The data generated by such sources are further subject to\ncontextual analysis and interpretation to uncover the hidden\npatterns for decision-making and business purposes.\nIn the context of a large volume of video data, we specialize\nthe generic big data characteristics. The size of data is referred\nto as Volume [20], but the majority of the shares, i.e., 65%, are\nheld only by surveillance videos. The type of data generated\nby various sources such as text, picture, video, voice, and logs\nare known as Variety [20]. The video data are acquired from\nmultimodal video stream sources, e.g., IP-Camera, depth\ncamera, body-worn camera, etc., and from different geolo-\ncations, which augments the Variety property. The pace of\ndata generation and transmission is known as Velocity [21].\nThe video data also possess the Velocity attribute, i.e., the\nVideo Stream Data Source (VSDS) primarily produce video\nstream 24/7 and acquired by the data center storage servers.\nVeracity can be de\u001cned as the diversity of quality, accuracy,\nand trustworthiness of the data [22]. Video data are acquired\ndirectly from real-world domains and meet the Veracity char-\nacteristic. The Value refers to contextual analysis to extract\nthe signi\u001ccant values for decision-making and business pur-\npose [23], [24]. Video data has high Value because of its direct\nrelation with real-word. Automatic criminal investigation,\nillegal vehicle detection, and abnormal activity recognition\nare some of the examples of Value extraction. Almost all the\nbig data properties are dominated by the video data, which\nencourage us to give birth to Video Big Data.\nThese \u001cve characteristics impose many challenges on the\norganizations when embracing video big data analytics. Stor-\ning, scaling, and analyzing are some apparent challenges\nassociated with video big data. To cope with these chal-\nlenges, converged and hyper-converged infrastructure and\nsoftware-de\u001cned storage are the most convenient solutions.\n152378 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nDistributed databases, data processing engines, and machine\nlearning libraries have been introduced to overcome video\nbig data management, processing, and analysis issues, respec-\ntively.\nThese big data technologies are deployed over a computer\ncluster to process and manage a massive amount of video\ndata in parallel. A computer cluster may consist of few to\nhundreds, and even thousands of nodes work together as a\nsingle integrated computing resource, on different parts of\nthe same program [25], [26]. Deploying an indoor computer\ncluster is an option for big data technologies, but hardware\ncost and maintenance issues are associated with it. An alter-\nnative solution can be cloud computing that elegantly reduces\nthe costs associated with the management of hardware and\nsoftware resources [27].\nTypically, cloud services are provided on-demand in a\n``pay-as-you-go'' manner for the conveniences of end-users\nand organizations [28]. Cloud computing follows the philoso-\nphy of the ``as-a-Service'' and offers its ``services'' according\nto different models, for example, Infrastructure-as-a-Service\n(IaaS), Platform-as-a-Service (PaaS), Software-as-a-Service\n(SaaS) [9].\nUnder IaaS (e.g., Amazon's EC2), the cloud service\nprovider facilitates and allows the consumers to provision\nfundamental computing resources and deploy arbitrary soft-\nware. In PaaS, the service provider provides a convenient\nplatform enabling customers to develop, run, and manage\napplications without considering the complexities of building\nand maintaining the infrastructure. The examples of PaaS\nare Google's Apps Engine and Microsoft Azure. In SaaS,\napplications (e.g., email, docs, etc.) are deployed on cloud\ninfrastructure by service providers and allow the consumer\nto subscribe. These applications can be easily accessed\nfrom various client devices using a thin client or program\ninterfaces [9].\nB. RESEARCH OBJECTIVES AND CONTRIBUTIONS\nThis paper presents a detailed survey and review of\ncloud-based large-scale IV A. We also propose big data\ntechnological solutions for the challenges faced by IV A\nresearchers and practitioners. The contributions of this paper\nare listed below:\n\u000fWe standardize the basic nomenclatures that govern the\nIV Adomain. The term video big data has been coined\nand clari\u001ced how it inherits the big data characteristics\nwhile establishing its relationship with cloud computing.\n\u000fWe propose a distributed, layered, service-oriented, and\nlambda style [29] inspired reference architecture for\nlarge-scale IV Ain the cloud. Each layer of the proposed\nCloud-based Video Analytics System (CV AS) has been\nelaborated technologically, i.e., layerwise available big\ndata technological alternatives. The base layer of the\nproposed architecture, i.e., video big data curation layer,\nis based on the notion of Intermediate Results (IR)\norchestration, which can play a signi\u001ccant role in theoptimization of the IV Apipeline. Under the proposed\narchitecture, we perform a thorough investigation of\nscalable traditional video analytics and deep learning\ntechniques and tools on distributed infrastructures along\nwith popular computer vision benchmark datasets.\n\u000fTo the best of our knowledge, this is a \u001crst surveying\nvideo big data analytics, our research targets the most\nrecent approaches that encompass broad IV Aresearch\ndomains like content-based video retrieval, video sum-\nmarization, semantic-based approaches, and surveil-\nlance and security.\n\u000fWe also investigate how the researchers exploit big data\ntechnologies for large-scale video analytics and show\nhow the industrial IV Asolutions are provided.\n\u000fIn this study, we further write real word IV Aapplication\nareas, which projects the signi\u001ccance of big data and\ncloud computing in IV A.\n\u000fFinally, we identify the research gap and list several\nopen research issues and challenges. These research\nissues encompass orchestration and optimization of IV A\npipeline, big dimensionality, online learning on video\nbig data, model management, parameter servers, and\ndistributed learning, evaluation issues and opportunities,\nIV Aservices statistics maintenance and ranking in the\ncloud, IV A-as-a-Service (IV AaaS) and cost model, video\nbig data management, and privacy, security and trust.\nThe remaining paper is organized as follows. Section II\nandIIIis about recent studies, and scope and nomenclature,\nrespectively. Section IVdiscusses the proposed CV AS. Lit-\nerature review has been presented in Section VandVI. The\nIV Aapplications can be seen in Section VII. In Section VIII\nseveral video big data challenges and opportunities are dis-\ncussed. Finally, Section IXconcludes this study.\nII. RECENT STUDIES\nVarious studies have been conducted that discus video ana-\nlytics, which can be classi\u001ced as IV Aand Big data, as shown\nin Table 1. In the former class, the surveys overlook the big\ndata characteristics and challenges of video analysis in the\ncloud. The majority of the work focuses on a speci\u001cc domain\nofIV Aand the application of video analyses. For instance,\na comprehensive survey was presented by Liu et al. [30],\nand Olatunji et al. [31], where they have discussed IV Aand\nits applications in the context of surveillance conventionally.\nContext-Based Video Retrieval (CBVR) was reviewed by\nHuet al. [32], Patel et al. [33], and Haseyama et al. [34].\nSimilarly, vehicle surveillance systems in Intelligent Trans-\nportation System (ITS), abnormal behavior analytics,\nin surveillance videos were studied by Tian et al. [35],\nand [36], respectively.\nFurthermore, big data studies were conducted from differ-\nent perspectives, i.e., cloud computing, ML, mining, multi-\nmedia, and ITS. Hashem et al. [16] and Agrawal et al. [39]\npresented research issues and challenges on big data analytics\nin the cloud computing. Tsai et al. [38] and Khan et al. [37]\nVOLUME 8, 2020 152379\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nTABLE 1. Recent surveys related to video big data analytics.\nfocus on big techniques and general applications along with\nchallenges. Zhou et al. [40], Che et al. [41], and Pouyan-\nfaret al. [5] studied and reported the research issues of\nbig data practices concerning ML techniques, data mining\nalgorithms, and multimedia data respectively. Zhu et al. [42]\npresented a comprehensive study on big data analytics in\nITS, analytical methods and platforms, and categories of big\ndata video analytics. In the study of big data, IV Ahas been\noverlooked, and minimal discussion can be found in the study\nof Khan et al. [37], Tsai et al. [38], Pouyanfar et al. [5],\nZhuet al. [42], H. and Zahid et al. [43].\nFrom the literature, it is clear that the recent studies ignored\nlarge-scale unstructured video analytics in the cloud in the\ndiscussion of big data. Some surveys focus on big data\nmanagement and its related tools, while others have limited\ninvestigation of IV A in a particular context. They also do\nnot consider the growing unstructured videos as video big\ndata. Unlike existing work, this paper provides a compre-\nhensive assessment of state-of-the-art literature and proposes\nan in-depth distributed cloud computing-based IV Areference\narchitecture.\nIII. SCOPE AND NOMENCLATURE\nWe clarify some nomenclature being used and scoping this\nstudy. The \u001crst one is IV A, which is ``any surveillance\nsolution that utilizes video technologies to automatically\nmanipulate and/or perform actions on live or stored video\nimages [44]''. The IV A services are implemented through\nhardware called Video Analytics System (V AS) [30]. V AS\nassist acquires videos continuously and monitors unblink-\ningly. The V AS falls into four categories, i.e., Embedded\nVideo Analytics System (EV AS), On-site Video Analytics\nSystem (OV AS), Fog-based Video Analytics System (FV AS),\nandCV AS.\nEV AS embeds IV Asolutions and performs video analysis\ndirectly on the edge device, e.g., camera or encoder, and can\nFIGURE 1. Geo-distributed video analytics infrastructure.\nproduce alerts in case of abnormality. EV AS provides very\nplain video analytics solutions and can simultaneously per-\nform two or three rules on its stream and cannot accomplish\ncomplex algorithms such as \u001cre detection, facial recognition,\nor cross video stream analytics. Under OV AS, small and\nmiddle-sized companies consist of networked or wireless\ncameras, a network router, a system running the video analyt-\nics and management software (e.g., IBM smart surveillance\nsystem [45], and Zoneminder [46]), and a storage device.\nAll the cameras send the video data for analytics against\ncontextual video analytics algorithms and warn the operator if\nanomaly detected. OV AS has many limitations, e.g., mainte-\nnance, software up-gradation, expensive hardware, scalabil-\nity, and unable to deal with large-scale video data.\nWhen IV Asolutions are provided in fog and cloud comput-\ning environment, then it is called FV AS, and CV AS, respec-\ntively. In such environments the IV A solutions are made\navailable under the as-a-Service (aaS) paradigm, i.e., IV AaaS.\nInFV AS, the IV Asolutions are geographically distributed and\ncon\u001cgured near the edge devices, i.e., video stream sources,\nto meet the strict real-time IV Arequirements of large-scale\nvideo analytics, which must address latency, bandwidth, and\nprovisioning challenges. Whereas the CV AS is more suitable\nfor of\u001dine IV Abecause of the relatively high response time\n152380 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nFIGURE 2. (a) A generic IVA service pipeline. (b) Hierarchal representation of video units.\nand latency. The hierarchy and relation among CV AS, FV AS,\nandEV AS is shown in Fig. 1. The batch video data analyt-\nics are performed in the cloud, while the real-time IV A is\nperformed in the fog. The EV AS can play a passive role in\nthe proposed architecture, e.g., feed the video streams to the\nCV AS if motion is detected.\nThe scope of this paper is FV AS, and CV AS, i.e., the\nreal-time and batch IV Asolutions are deployed in the fog and\ncloud computing environment, respectively, while utilizing\nbig data computing technology. However, for the ease of\nunderstandability, throughout this paper, we use the notion\nofCV AS.\nTo analyze a video in CV AS, a video undergoes through\ndifferent phases, as shown in Fig. 2a. In Fig. 2a,Video\nSource are the sources which either generate videos streams\nfrom sources connected directly to real-world domains such\nas IP-camera or can be already acquired videos in the form\nof datasets residing in a \u001cle system. If IV Ain the cloud are\nperformed on real-time video streams, then we called it Real-\ntime IV A (RIV A) andBatch IV A (BIV A) if performed on\nbatch videos.\nTheIngest phase implement interfaces to acquire\nvideos from the Video Source. In the context of IV A,\nthe acquired videos can be represented as a hierarchy,\nas shown in Fig. 2b. An acquired video from a Video\nSource may be decomposed into its constituent units either\nin the temporal or spatial domain. In a video, a frame\nrepresents a single image, whereas a shot denotes a con-\nsecutive sequence of frames recorded by a single camera.\nA scene is semantically related shots in a sequence that\ndepicts a high-level story. A collection of scenes composes a\nsequence/story. Frames and shots are low-level temporal fea-\ntures suitable for machines, while scenes and sequence/story\nare considered to be the high-level features that are suitable\nfor human perceptions. Such constituent units are further\nsubject to low, mid, or/and high-level processing. In low-level\nprocessing, primitive operations (in Transformations\nphase) are performed e.g., noise reduction, histogram equal-\nizer. The Infer phase encompasses mid and high-level\nprocessing. The mid-level processing extracts features from\nthe sequence of frames, e.g., segmentation, description, clas-\nsi\u001ccation, etc. The high-level processing, make sense of an\nensemble of recognized objects; perform the cognitive func-\ntions normally associated with vision. Finally, the extractedinformation can be persisted to the data store and/or published\nto the end-user.\nFurthermore, the basic unit of an IV Aservice pipeline is an\nalgorithm, e.g., encoder, feature extractor, classi\u001cer, etc. The\ninput of an algorithm can be a Video Source, keyframes,\nor features. Similarly, the output of an IV A algorithm can\nbe high, mid, or low-features. Throughout this study, all\npossible outputs of IV Aalgorithms are termed as IR. Multiple\nalgorithms can be pipelined to build a domain-speci\u001cc IV A\nservice. The input and output of an IV Aservice are restricted\nto the Video Source andIR, respectively. The User rep-\nresents the stakeholder of the CV AS, such as administrator,\nconsumer, IV A researchers, and practitioners. A Domain\nis a speci\u001cc real-word environment, e.g., street, shop, road\ntraf\u001cc, etc., for which an IV Aservice needs to be built for\nautomatic monitoring. Domain knowledge facilities IV Ain\ndiscovering interesting patterns from domain video streams.\nThe combination of software and hardware constitutes s dis-\ntributed System (cloud environment) where IV Aservice and\nalgorithms can run fast. Nevertheless, upgrading existing IV A\nalgorithms to distributed architecture requires customization\nof how IV Aalgorithms should be implemented and deployed.\nMoreover, the speci\u001cc needs of IV Amay require the design\nand development of new system architecture. All the symbols\nbeing used in this paper are listed below.\nLIST OF ABBREVIATIONS\naaS as-a-Service.\nACID Atomicity, Consistency, Isolation, Durability.\nAFS Adaptive Feature Scaling.\nAI Arti\u001ccial Intelligence.\nB2B Business-to-Business.\nB2C Business-to-Customer.\nBIV A Batch IV A.\nC2C Customer-to-Customer.\nCAP Consistency, Availability, Partition Tolerance.\nCBVR Context-Based Video Retrieval.\nCCDG Controlled Cyclic Dependency Graph.\nCNN Convolutional Neural Network.\nCV AS Cloud-based Video Analytics System.\nDAG Directed Acyclic Graph.\nDBDS Distributed Big Datastore.\nDFS Distributed File System.\nDMBM Distributed Message Broker Manager.\nVOLUME 8, 2020 152381\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nDPDS Distributed Persistent Data Store.\nEV AS Embedded Video Analytics System.\nFV AS Fog-based Video Analytics System.\nFVSA Fused Video Surveillance Architecture.\nHDFS Hadoop File System.\nHOG Histogram of Oriented Gradients.\nIaaS Infrastructure-as-a-Service.\nIoT Internet of Things.\nIR Intermediate Results.\nIRM Intermediate Results Manager.\nISDDS Immediate Structured Distributed Data Store.\nITS Intelligent Transportation System.\nIV A Intelligent Video Analytics.\nIV AAaaS IV A-Algorithm-as-a-Service.\nIV AaaS IV A-as-a-Service.\nIV AS IV A services.\nKCL Knowledge Curation Layer.\nL-CV AS Lambda CV AS.\nLBP Local Binary Pattern.\nLVSM Lifelong Video Stream Monitor.\nML Machine Learning.\nOV AS On-site Video Analytics System.\nPaaS Platform-as-a-Service.\nPCA Principal Component Analysis.\nQoS Quality of Service.\nRDBMS Relational Database Management System.\nRIV A Real-time IV A.\nRNN Recurrent neural network.\nRVSAS Real-time Video Stream Acquisition and\nSynchronization.\nSaaS Software-as-a-Service.\nSIFT Scale Invariant Feature Transform.\nSVM Support Vector Machine.\nUPDDS Unstructured Persistent Distributed Data Store.\nV AS Video Analytics System.\nVBDCL Video Big Data Curation Layer.\nVBDML Video Big Data Mining Layer.\nVBDPL Video Big Data Processing Layer.\nVSAC Video Stream Analytics Consumer.\nVSAS Video Stream Acquisition Service.\nVSCS Video Stream Consumer Service.\nVSDS Video Stream Data Source.\nWSL Web Service Layer.\nIV. LAMBDA CVAS: A REFERENCE ARCHITECTURE\nIn this section, we brie\u001dy presents the technical details of the\nproposed CV AS (called Lambda CV AS (L-CV AS)) and the\ntechnical detail of each layer in the consecutive sub-sections.\nFig. 3 presents the proposed reference cloud-based layered\narchitecture for distributed RIV A and BIV A. The proposed\nL-CV AS architecture consists of \u001cve layers i.e., Video Big\nData Curation Layer (VBDCL), Video Big Data Processing\nLayer (VBDPL), Video Big Data Mining Layer (VBDML),\nKnowledge Curation Layer (KCL), and Web Service Layer\n(WSL).VBDCL is the foundation layer and is responsible for\nlarge-scale big data management throughout the life-cycle of\nIV A, i.e., from data acquisition to early persistence to archival\nand deletion [47]. VBDPL is responsible for distributed video\npre-processing, feature extraction, etc. The VBDML deploys\nIV Aalgorithms on the top of distributed processing engines\nintending to produce high-level semantics from the processed\nsequence of frames. On top of the VBDML layer, the KCL\nlayer has been designed to link the low-level features in\nspatial and temporal relation across videos in a multi-stream\nenvironment. KCL deploys a generic video ontology. The\nKCL layer maps the extracted IR to the video ontology to\nbridge the semantic gap between the low-level features in\nEuclidean space and temporal relation across videos while\nutilizing semantic rich queries. The proposed architecture\nincorporates top-notch functionalities of the above four layers\ninto a simple uni\u001ced role base WSL, which enables the\nL-CV AS users to manage, built, and deploy a wide array of\ndomain-speci\u001cc near RIV A andBIV A services.\nAll the layers are made available as aaS. These IV AaaS are\nprovided to the domain experts and allow them to pipelined in\na speci\u001cc context to built an IV Aservice. These IV Aservices\nare made available as IV AaaS to which users can subscribe\nVideo Sources.\nFunctionalities like security, scalability, load-balancing,\nfault-tolerance, and performance are mandatory and com-\nmon to all the layers, which are shown as a cross-cutting\nin Fig. 3. The cloud infrastructure provides the underlying\nhardware and software under IaaS, on which the L-CV AS can\nbe deployed. The cloud infrastructure is out of the scope of\nthis paper. It has already been studied in detail in the context\nof big data by [16], [36], [39]. However, in the discussion\nofIV Ain the cloud, some resources like CPU, GPU, FPGA,\nHDD, and SSD can be considered.\nA. VIDEO BIG DATA CURATION LAYER\nEffective data management is key to extract insights from the\ndata. It is a petascale storage architecture that can be accessed\nin a highly ef\u001ccient and convenient manner. We design the\nVBDCL forL-CV AS to ef\u001cciently manage video big data.\nL-CV AS's data storage stack consists of three main compo-\nnents: Real-time Video Stream Acquisition and Synchroniza-\ntion (RVSAS), Distributed Persistent Data Store (DPDS), and\nVBDCL Business Logic.\n1) REAL-TIME VIDEO STREAM ACQUISITION AND\nSYNCHRONIZATION\nThe real-time video stream needs to be collected from the\nsource device and forwarded to the executors for on-the-\u001dy\nprocessing against the subscribed IV A service. Handling a\ntremendous amount of video streams, both processing and\nstorage are subject to lose [51]. To handle, large-scale video\nstream acquisition in real-time, to manage the IR, anomalies,\nand the communication among RIV A services, we design the\nRVSAS component while assuming a distributed messaging\nsystem.\n152382 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nFIGURE 3. Lambda CVAS: A reference architecture for real-time and batch intelligent video analytics in the cloud.\nDistributed Message Broker, also known as message-\noriented-middleware [52], is an independent application that\nis responsible for buffering, queuing, routing, and deliveringthe messages to the consumers being received from the mes-\nsage producer [53]. Message broker should be able to handle\npermission control and failure recovery. A message broker\nVOLUME 8, 2020 152383\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nTABLE 2. Popular message-oriented middlewares.\ngenerally supports routing methods like direct worker queue,\nand/or publish-subscribe [54]. Similarly, the message con-\nsumer component receives the messages from the message\nbroker either periodically (cron-like consumer) or continu-\nously (daemon-like consumer). Generally and for the sake\nof scalability, message consumers are deployed on separate\nservers independently of message producers [54]. Some pop-\nular distributed messaging systems are shown in table 2.\nRVSAS provides client APIs on the top of a dis-\ntributed messaging system for the proposed framework. The\nRVSAS component is responsible for handling and collect-\ning real-time video streams from device-independent video\ndata sources. Once the video stream is acquired, then it\nis sent temporarily to the distributed broker server. The\nworker system, on which an IV A service is con\u001cgured,\ne.g., activity recognition, reads the data from the dis-\ntributed broker and process. The RVSAS component is com-\nposed of \u001cve sub-modules, i.e., Distributed Message Bro-\nker Manager (DMBM), Video Stream Acquisition Service\n(VSAS), Video Stream Consumer Service (VSCS), Interme-\ndiate Results Manager (IRM), and Lifelong Video Stream\nMonitor (LVSM).\na: DISTRIBUTED MESSAGE BROKER MANAGER\nDMBM are used to manage the queues in the distributed mes-\nsage broker cluster considering RIV A services. Three types\nof queues, RIVA_ID, RIVA_IR_ID, and RIVA_A_ID as\nshown in Fig. 4, are automatically generated by the DMBM\nmodule on the distributed message broker when a new RIV A\nservice is created. Here RIVA, ID,IR, and Astands for RIV A\nservice, unique identi\u001cer of the service, IR, and Anomalies,\nrespectively. These queues are used to hold the actual video\nstream being acquired by VSAS, IRproduced by an algo-\nrithm, and anomalies detected by the video analytics services.\nb: VIDEO STREAM ACQUISITION SERVICE\nVSAS module is used to provide interfaces to VSDS and\nacquires large-scale streams from device-independent video\ndata sources for on-the-\u001dy processing. If a particular video\nstream source is subscribed against an RIV A service, then the\nVSAS gets the con\u001cguration metadata from the Data Source\nFIGURE 4. Real-time Video Stream Acquisition and Sync.\nDS in Immediate Structured Distributed Data Store (ISDDS)\nand con\u001cgure the source device for video streaming. After\nsuccessful con\u001cguring the source device, VSAS decodes the\nvideo stream, detects the frames, and then performs some\nnecessary operations on each frame such as meta-data extrac-\ntion and frame resizing, which is then converted to a formal\nmessage. These messages are then serialized in the form of\nmini-batches, compressed, and sent to the Distributed Broker.\nIf a video-stream source ``C 1'' is subscribed to the RIV A\nservice ``S 1'' then the VSAS will rout the mini-batch of the\nvideo stream to queue RIVA_1 in the Broker Cluster as\nshown in Fig. 4.\nc: VIDEO STREAM CONSUMER SERVICE\nAs the acquired video streams are now residing in the dis-\ntributed broker in different queues in the form of mini-\nbatches. To process these mini-batches of the video stream,\nwe have different groups of computer cluster know as Video\nStream Analytics Consumer (VSAC) Cluster. On each clus-\nter, three types of client APIs are con\u001cgured, i.e., RIV A\nservices, VSCS, and LVSM. Each VSAC cluster has different\ndomain-speci\u001cc RIV A services where the VSCS are com-\nmon for all. The VSCS assists the RIV A service to read the\nmini-batches of the video stream from the respective queue\nin the distributed broker for analytics, as shown in Fig. 4.\nTheVSCS module has two main functions. First, this module\n152384 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nallows RIV A service to read the mini-batches of the video\nstream from the respective queue in the distributed broker.\nThe second task is to save the consumed unstructured video\nstreams and its meta-data to the row video space in the DPDS\nand to the Data Source DS meta-store, respectively.\nd: INTERMEDIATE RESULTS MANAGER\nDuring the IV Aservice life-cycle, a sequence of algorithms\nare executed. Thus the output of an algorithm can be the input\nof another algorithm. The IRdemands proper management in\nthe distributed environment because one algorithm may be on\none computer while the other may be on another computer.\nThus we design IRM that sends and gets the IRto and from\nthe topic RIVA_IR_ID in the distributed broker cluster.\nSimilarly, this module is also responsible for reading the IR\nfrom the respective queue and persists to the IRdata store for\nfuture use so that to avoid recomputation.\ne: LIFELONG VIDEO STREAM MONITOR\nThe domain-speci\u001cc RIV A service process the video stream\nfor anomalies or abnormal activities. If anomalies are\ndetected, then it is sent to the distributed broker queue\n(i.e.,RIVA_A_ID) by using the LVSM instance. To gen-\nerate noti\u001ccation base response, LVSM follow standard\nobserver-based concept [55]. Based on this approach,\ntheLVSM module reads the anomalies from the respective\ndistributed broker queue, i.e., RIVA_A_ID and notify the\nclients in near real-time and simultaneously persisted to the\nISDDS.\n2) DISTRIBUTED PERSISTENCE DATA STORE\nThe second component of the VBDCL isDPDS. The DPDS\ncomponent provides the facilities of permanent and dis-\ntributed big-data persistence storage of both structured and\nunstructured data. The DPDS provides abstraction in two\nlevels on the acquired video data ie. ISDDS andUnstruc-\ntured Persistent Distributed Data Store (UPDDS). The phi-\nlosophy behind DPDS and two levels of abstraction in the\ncontext of L-CV AS is many folds. From the users' perspec-\ntive, the CV AS demands geo-based, real-time, low latency,\nand random read-write access to the data in the cloud.\nSimilarly, the DPDS should also provide high-performance\nlocality-based access to the data when the other layers deploy\ndata-intensive IV Aservices. To meet such a diverse amount\nof requirements of the DPDS component, technologically,\naDistributed File System (DFS) andDistributed Big Data-\nstore (DBDS) can be leveraged.\na: IMMEDIATE STRUCTURED DISTRIBUTED DATASTORE\nTheISDDS is provided to manage large-scale structured data\nin the distributed environment over DBDS. Because of the\ndata-intensive operation and according to the requirements of\nthe other layer, technologically, a distributed big data store,\ncan be deployed. The ISDDS hosts \u001cve types of data. The\ndetailed description of each type of data has been described\nin this section.L-CV AS provides role-based access to its user. L-CV AS\nuser logs and the respective role information are maintained\nthrough the User Pro\u001cle and Logs meta-store. The pro-\nposed framework manages two types of video data sources\nthrough the Data Source meta-store. These are video data\nsources, for example, IP-cameras, Kinect, body-worn cam-\neras, etc., and batch video datasets. The former one can be\nsubscribed to RIV A service while the later one is eligible for\nBIV A services. The meta-information of these sources, along\nwith access rights, are managed through the Data Source\nmeta-store. Administrator and developer roles can develop,\ncreate, and deploy video analytics algorithms through the\nL-CV AS. Similarly, different IV Aalgorithms can be pipelined\ninto an IV A service. The management of video analytics\nalgorithms and services is managed through Video Analytics\nAlgorithm and Service meta-store, respectively. As stated that\ninIV Apipelining environment, the output of one IV Aalgo-\nrithm can be the output of another algorithm. In this context,\nwe design a general container called IRdatastore to persist\nand index the output of an RIV A algorithm, and services.\nThis datastore is signi\u001ccant and can play a vital role in IV A\npipeline optimization, and fast content-based searching and\nretrieval. Finally, the L-CV AS users are allowed to subscribe\nto the data sources to the IV Aservices. The subscription infor-\nmation is maintained through the Subscription meta-store,\nand the anomalies are maintained through the Anomalies\nmeta-store.\nTheISDDS Data Model of the L-CV AS demands an ef\u001c-\ncient distributed data store. The distributed data store should\nhave the ability of horizontal scalability, high availability,\npartition tolerance, consistency, and durability. Furthermore,\nthe data store should ful\u001cll the read/write access demands\nof the BIV A operations, and RIV A, interaction, and visual-\nization. It is a fact that traditional relational databases have\nlittle or no ability to scale-out to accommodate the growing\ndemands of the big data, and resultantly new distributed\ndata stores have emerged. The distributed data stores can be\ngrouped into two major categories, i.e., NoSQL (Not Only\nSQL) and NewSQL (excluding graph data stores).\nNoSQL is a schema-free data store designed to support\nmassive data storage across distributed servers [62], [63].\nThe features of NoSQL data stores include horizontal scal-\nability, data replication, distributed indexing, simple API,\n\u001dexibility, and consistency [64]. NoSQL lacks true Atom-\nicity, Consistency, Isolation, Durability (ACID) transactions,\nunlike Relational Database Management System (RDBMS).\nIn the context of Consistency, Availability, Partition Toler-\nance (CAP) theorem [65], it has to compromise on either\nconsistency or availability while choosing partition tolerance.\nNoSQL can further be categorized as Document, Key-value,\nand Extensible stores. A key-value data store is respon-\nsible for storing values and indexes for searching. Docu-\nment datastore is used for document storage, indexing, and\nretrieval. Extensible data store stores extensible records that\ncan be partitioned vertically and horizontally across the\nnodes.\nVOLUME 8, 2020 152385\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nTABLE 3. Popular distributed file systems.\nTABLE 4. Popular distributed data stores.\nNewSQL data stores provide the characteristics of both\nNoSQL and RDBMS: ACID transactional consistency of\nrelational databases with facilities of SQL; and the scalabil-\nity and performance of NoSQL. MySQL Cluster, VoldDB,\nClustrixDB are examples of NewSQL. Some of the popular\nNewSQL and NoSQL datastores are shown in Table 4along\nwith the respective properties. The `A' and `C' in CAP is not\nequal to the `A' and `C' in ACID [66].\nb: UNSTRUCTURED PERSISTENT DISTRIBUTED DATASTORE\nTheUPDDS component built on the top of the DFS such as\nHadoop File System (HDFS) that facilitates permanent and\ndistributed big-data storage. The data are stored and mapped\nsystematically according to the business logic of the proposed\nsystem. The UPDDS component is designed to effectively\nand dynamically manage the data workload in the life-cycle\nof an IV Aservice. Upon new registration with the L-CV AS,a formal User Space is created on the top of DFS. The User\nSpace is managed through the proper hierarchical directory\nstructure and special read and writes access are granted to the\nowner. All the directories are synchronized and mapped in the\ncorresponding user pro\u001cle logs. Under the User Space, three\ntypes of distributed directories are created, i.e., Raw Video\nSpace, Model Space, and Project Space. The hierarchical\nstructure of the User Space in DFS is shown in Fig. 5.\nRaw Video Space is used for the management of the video\ndata. Raw Video Space is further divided into two types\nof video spaces. The \u001crst type is a batch video which has\nbeen uploaded to the L-CV AS for batch analytics, where\nthe second type is acquired and persisted from the VSDS.\nThe entire acquired stream is time-stamped on persistence.\nThe granularity level of raw streaming videos is maintained\nthrough video data sources. IV Alife-cycle may need different\nmodels for training and testing purposes. The Model Space is\n152386 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nFIGURE 5. Hierarchical structure of the User Space in DFS.\nprovided to facilitate the developers to manage the training\nand testing model according to the deployed IV Aalgorithm.\nSimilarly, a developer can develop a new algorithm or a video\nanalytics service. The Project Space is provided to manage\nthe source code of the respective developer and practitioners.\nTheUPDDS is supposed to be designed on the top of DFS.\nIn this context, diverse types of open source DFS have been\ndeveloped. The implementation quality of big data applica-\ntions is relative to the storage tier's \u001cle system. From an archi-\ntectural perspective, working and handling large volumes\nand throughput of data is challenging. Commonly, big data\nsolutions exploit a cluster of computers, ranging from few to\nhundreds of computers, connected through high-speed net-\nworks while deploying specialized distributed data and man-\nagement software. In distributed data-intensive applications,\nlarge-scale data is always moving across the cluster and thus\ndemands a distributed, scalable, reliable, and fault-tolerant\n\u001cle system [72] known as DFS. DFS is a \u001cle system that\nprovides access to replicated \u001cles across multiple hosts on a\ncomputer network, ensuring performance, data locality, high\navailability, scalability, reliability, security, uniform access,\nand fault-tolerance. Currently, various DFS are available and\nmay differ in terms of performance, fault-tolerance, content\nmutability, and read/write policy. Some state-of-the-art pop-\nularDFS, which can be used for UPDDS, along with a short\ndescription, are shown in Fig 3.\nc: ACTIVE AND PASSIVE DATA READERS AND WRITER\nThis module gives read-write access to the underlying data\nsecurely according to the business logic of the VBDCL Busi-\nness Logic and according to the registered user access rights.\nThis sub-module is composed of four types of readers and\nwriters, i.e., ISDDS Active Data Reader, ISDDS Passive Data\nReader, UPDDS Active Data Reader, and UPDDS Passive\nWriter. For real-time, read-write operation over the data resid-\ning in the ISDDS andUPDDS, such as CRUD operation, \u001cle\ncreation, video stream writing to DFS, etc. the active data\nreader and writer are used. In the context of of\u001dine analytics\nover the bulk of videos while using distributed processing\nengines. The Passive Data Reader and Writer (PDRW) is\nprovided to allow processing engines to load the bulk of data\nand persist the same to the ISDDS andUPDDS.\n3) VBDCL BUSINESS LOGIC\nVBDCL Business Logic provides the actual business logic.\nIt implements six different modules, i.e., User Manager, DataSource Manager, Model Manager, (R/B)IV A Algorithm and\nService Manager, (R/B)IV A Service Discovery and Subscrip-\ntion Manager, Video Ontology Manager, Cluster Manage-\nment and Monitoring.\nThe User Manager module encapsulates all the\nuser-related operations such as new user account cre-\nation, access role assignment, and session manage-\nment. Through the Data Source Manager Model\nManager modules, the user can manage the VSDS, video\ndata uploading, and model management. The (R/B)IVA\nAlgorithm and Service Manager are built to man-\nage, develop, and deploy new IV A algorithms and ser-\nvices, respectively. The former one is provided aaSto the\nL-CV AS developers, while the latter one is provided aaS\nto the consumers. The developer role can create and pub-\nlish a new video analytics algorithm. The algorithm is then\nmade available aaSto other developers and can use it.\nOnce IV Aservices are created, then the L-CV AS users are\nallowed to subscribe to the streaming video data sources and\nbatch data against the provided RIV A andBIV A services,\nrespectively.\nSimilarly, the Ontology Manager allows the devel-\noper to get the IRfor decision making. The ontology man-\nager provides a secure way of getting the IRand maps it\naccording to ontology. This module also allows the user to\nmanage the functionalities of the KCL. Finally, Cluster\nManagement and Monitoring allows the administra-\ntor to monitor the health of the cluster.\nB. DISTRIBUTED INTELLIGENT VIDEO ANALYTICS\nIV A performs the complex tasks of extracting signi\u001ccant\nknowledge and information of interest from the video data,\ni.e., structural patterns, behavior patterns, content char-\nacteristics, event patterns [73], [74] and their relation-\nship in the form of classi\u001ccation and clustering. Extract-\ning knowledge and information from video big data is\na processing-intensive task. Value extraction from video\nbig data is behind the capabilities of traditional tools and\ndemands for technological solutions to meet the processing\nrequirements.\nTherefore, video big data analytics is preferably performed\nin the distributed environment in a parallel manner while\nutilizing distributed scale-out computing technologies [75],\nsuch as MapReduce and Apache Spark. Distributed IV Anot\nonly signi\u001ccantly improves the performance, but also reduces\nthe analytics cost. Based on the generic IV Alife-cycle and\nmotivated by scikit-learn [76], the distributed IV Aare divided\ninto two layers, i.e., VBDPL, and VBDML. These two layers\nare further elaborated in the following sub-sections.\n1) VIDEO BIG DATA PROCESSING LAYER\nIV Arequires video data pruning and strong feature extraction.\nWith such intentions, the VBDPL layer consists of three\ncomponents, i.e., Video Preprocessing, Feature Extractor, and\nDimensionality Reduction is designed.\nVOLUME 8, 2020 152387\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\na: VIDEO PREPROCESSING\nThe quality of data plays an active and signi\u001ccant role\nin solving a problem with ML. Raw videos have an\nunstructured format and contain noise/uncertainties, mak-\ning it unsuitable for knowledge and information mining.\nVideo Preprocessing component is designed with\nthe same objectives and is supposed to deploy several\ndistributed video preprocessing operations including frame\nextraction [77], frame-resizing, frame-conversion from RGB\nto grayscale [78], shot boundary detection [79], segmen-\ntation [80], transcoding [81], and many more. In the \u001crst\nstep, frames are extracted from a video for processing. Sev-\neral frame selection algorithms are available, for example,\nkeyframe extraction. The number of frames to be extracted\nis dependent on the user objective and task-dependent.\nCandidate frames can be all frames, step frames (every sec-\nond frame, \u001cfth frame, etc.) or keyframes [82]. The\nspatial operations highly depend on the scenario and objec-\ntive. Spatial operations include frame resizing (for reducing\ncomputational complexity), corrections (brightness, contrast,\nhistogram equalization, cropping, keyframes), mode (RGB,\nGrayscale, etc.), and many other operations. Segmentation\nis used for various purposes, such as partitioning video into\nsemantically related chunks.\nb: FEATURE EXTRACTOR\nTheFeature Extractor component implements dis-\ntributed feature extraction algorithms. The performance of\nMLis highly dependent upon the type of data representa-\ntion or features [83]. Features represent the characteristics\nof classes in the dataset and have a heavy impact on the\nMLalgorithm's generalizability and performance. The data\nfeatures that used to train MLmodels have a huge in\u001duence\non the performance of the algorithm. Inappropriate or irrele-\nvant features affect the performance of the algorithm. Thus,\nfeature extraction extracts the features from the raw videos\nthat can be interpreted by the MLalgorithm [84], [85]. In this\ncontext, several feature extraction algorithms have been intro-\nduced for video data. These feature extraction approaches can\nbe categorized into static features of keyframes [86]\u0015[88],\nobject features [89], [90], dynamic/motion feature extrac-\ntion [84], [91], [92], trajectory-based features extrac-\ntion [84], [85], [93], [94], and deep learning-based feature\nextraction [95]\u0015[103].\nc: FEATURE SELECTION AND DIMENSIONALITY REDUCTION\nFeature selection and dimensionality reduction reduce the\nsize of the features. Large sizes of feature sets are expensive\nin terms of time for training and/or performing classi\u001ccation\nacquired by trained classi\u001cers. For example, Principal Com-\nponent Analysis (PCA) and its variants are used to reduce the\nsize of features. During feature selection, most relevant fea-\ntures are selected by discarding irrelevant and weak features.\nThe performance of MLclassi\u001cers is also directly related\nto the quality of features; GIGO (garbage in garbage out).Inappropriate or partially relevant features can negatively\naffect model performance. Therefore, only a limited set of\nfeatures should be selected and used for training classi\u001cers.\nThis is what precisely the purpose of this component is and\ndeploy different algorithms in this context. Similarly, some\nfeature reduction techniques available that selects the speci\u001cc\nset of limited features in real-time. For example, Online\nFeature Selection selects and inputs a speci\u001cc number of\nsmall features to the classi\u001cers in real-time [104]. In order\nto accelerate the training process, [105] used non-linear and\ngroup-based feature selection techniques, based on Adaptive\nFeature Scaling (AFS), to process the data with substan-\ntial dimension sizes. [106] proposed an unsupervised fea-\nture reduction technique that selects extremely relevant fea-\ntures and indicates suitable weights to the distinctive feature\ndimensions.\n2) VIDEO BIG DATA MINING LAYER\nThe VBDML utilizes diverse types of machine-learning\nalgorithms, i.e., supervised, semi-supervised, and unsuper-\nvised algorithms to \u001cnd different type of information from\nthe videos [73], [74]. In this context, VBDML layer hosts\nthree types of components, i.e., Classi\u001ccation, Regression,\nClustering.\nClassi\u001ccation component provides various MLalgorithms,\ne.g., Support Vector Machine (SVM), Nearest Neighbors,\nRandom Forest, Decision Tree, Nave Bayes, etc., that identi-\n\u001ces that a particular object in a video frame belongs to which\ncategory while using prede\u001cned classes. The Regression\ncomponent includes different algorithms, e.g., Linear Regres-\nsion, Decision Tree Regression, Logistic Regression, and\nmany more, predicting a continuous-valued attribute associ-\nated with objects rather than discrete values. The Clustering\ncomponent encapsulates algorithms, e.g., K-Mean, spectral\nclustering, etc., that produces groups of data depending upon\nthe similarity of data items.\nL-CV AS has the ability of BIV A andRIV A. In this con-\ntext, the VBDML should support batch learning and online\nlearning. The former case considers the complete training\ndata to learn and generates models. The batch-learning algo-\nrithm is expected to generalize, that usually does not perform\nwell in the real environment. Unlike batch learning, online\nlearning continuously learns from new input without mak-\ning any statistical assumptions about the data [107]. In the\ncontext of model generalization, online learning is expected\nto work well by accurately predicting the prede\u001cned set of\ninputs [107]. Online learning is used in the environment when\ncontinuous learning from the data is required to learn new\npatterns instead of batch learning.\n3) DISTRIBUTED DEEP LEARNING FOR IVA\nHandcrafted features, e.g., Scale Invariant Feature Trans-\nform (SIFT) [108], Local Binary Pattern (LBP) [109], His-\ntogram of Oriented Gradients (HOG) [110], etc., generates\nhigh dimensional features vectors and resultantly facing the\nissue of scalability. Recently, Convolutional Neural Network\n152388 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nTABLE 5. Comparision of popular deep learning architectures.\n(CNN) based approaches have shown performance superior-\nity in tasks like optical character recognition [111], and object\ndetection [112]. The motive of the deep learning is to scale\nthe training in three dimensions, i.e., size and complexity\nof the models [113], proportionality of the accuracy to the\namount of training data [114], and the hardware infrastructure\nscalability [115]. Results of deep learning are so promising\nthat soon the deep learning will give equivalent or higher\nperformance compared to humans when trained over large\ndata sets [112]. A CNN or ConvNet is a type of neural\nnetwork that can recognize visual patterns directly from the\npixels of images with less preprocessing. CNN based video\nclassi\u001ccation methods have been proposed in the literature\nto learn features from raw pixels from both short video and\nstill images [96], [97], [116], [117]. In the proposed L-CV AS\nframework, both the VBDPL, and VBDML are capable to\ndeploy deep-learning approaches for distributed IV A.\nSince on the dawn of deep learning, various open-source\narchitecture have been developed. Some of the well-known\nand state-of-the-art CNN architectures are LeNet-5 [118],\nAlexNet [117], ZFNet [119], GoogleNet [120], VGGNet\n[121], and ResNet [122]. The comparison of these architec-\ntures can be found in Table 5. Similarly, several frameworks\nhave been developed to eliminate the need for a manual\nde\u001cnition of gradient propagation. Table 6summarizes these\nlibraries along with the comparisons.\nTensorFlow is a popular deep learning library designed\nforMLand deep learning. It supports the deployment ofcomputation on both CPUs and GPUs. TensorFlow allows\nthe fast implementation of deep neural networks on the\ncloud. TensorFlow is also suitable for other data-driven\nresearch purposes and is equipped with TensorBoard (a visu-\nalization tool). Higher-level programming interfaces such\nas Luminoth, Kera, and TensorLayer were built on the\ntop of TensorFlow. Caffe2, developed by Berkeley AI\nResearch, is another library to build their deep learning\nmodels ef\u001cciently along with GPUs' support in a dis-\ntributed environment. PyTorch, maintained by Facebook,\nis a scienti\u001cc computing framework with wide support\nfor machine learning models and algorithms. PyTorch\noffers rich pre-trained models that can be easily reused.\nMXNET is a deep learning library suitable for fast numer-\nical computation for both single and distributed ecosys-\ntems. Likewise, some more deep learning libraries have\nbeen developed, such as CNTK, Deeplearning4j, Blocks,\nGluon, and Lasagne, which can also be employed cloud\nenvironment.\nBig DL models training with large-scale training data is a\nchallenging task. For example, S Gao at al. [129] utilized six\nlearning algorithms, i.e., biogeography-based optimization,\nparticle swarm optimization, genetic algorithm, ant colony\noptimization, evolutionary strategy, and population-based\nincremental learning, for the best combination of neural net-\nwork user-de\u001cned parameters during training. It is a hectic\njob for a single system when the training datasets are large.\nDistributed infrastructure with multiple computing nodes\nVOLUME 8, 2020 152389\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nTABLE 6. Summary of popular deep learning libraries.\nFIGURE 6. Scalable deep learning utilizing model distribution.\n(equipped with powerful GPUs) is the best option, but it\nleads to numerous challenges. First is the effective utiliza-\ntion of resources (costly GPU stalling preventions). Second,\nthe resources are shared among different users in the cloud\nfor cost reduction and elasticity. Such challenges are attract-\ning the attention of researchers. There are three approaches,\ni.e., model, data, and pipeline distribution for leveraging\ndistributed computing in distributed computing [130], [131].\nIn the former, the DL model is partitioned in logical frag-\nments, and loaded to different worker agents for training,\nas shown in Fig. 6.\nIn the second approach, the deep learning model is repli-\ncated to all the cluster's worker-agents, as shown in Fig. 7.\nThe training dataset is partitioned into non-overlapping sub-\ndataset, and each sub-dataset are loaded to the different\nworker-agents of the cluster. Each worker-agent executes the\ntraining on its sub-dataset of training data. The model param-\neters are synchronized among the cluster worker-agents\nto updates the model parameters. The data distribution\napproached naturally \u001cts in the distribute computing MapRe-\nduce paradigm [131]. The MapReduce splits the input based\non some prede\u001cned parameters. The map tasks, then, pro-\ncess these chunks in parallel a manner. After processing,\nthe output is shuf\u001ded for relevance and is directed to map\ntasks for generating intermediate results. The output from\nthe map tasks is shuf\u001ded for relevance and is given as input\nto the reduce tasks for generating intermediate results. The\nFIGURE 7. Scalable deep learning utilizing data distribution.\nFIGURE 8. Deep learning with pipelining (Figure motivated by [132]).\nintermediate results are combined to produce the complete\nresult. Hadoop and Spark require and process data natu-\nrally distributed in the manner and popular research trend\nnowadays.\nIn the third case, the DL model is partitioned, and each\nworker-agent loads a different segment of the DL model for\ntraining, as shown in Fig. 8. The training data are given to\nthe worker-agents that carry the input layer of the DL model.\nIn the forward pass, the output signal is computed, which\nis transmitted to the worker-agents that hold the next layer\nof the DL model. In the backpropagation pass, gradients are\ncalculated starting at the workers that carry the DL model's\noutput layer, propagating to the workers that hold the input\nlayers of the DL model [132].\n4) BIG DATA ENGINES, ML LIBRARIES, AND IVA\nThe VBDPL, and VBDML are assumed to be built on the\ntop of distributed computing engines. This section overview\nsome latest big data engines that can be utilized for scale-out\nIV A.\n152390 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nTABLE 7. Comparison of big data engines.\nHadoop MapReduce [144] is a distributed programming\nmodel, developed based on GFS [72], for data-intensive\ntasks. Apache Spark follows a similar programming model\nlike MapReduce but extends it with Resilient Distributed\nDatasets (RDDs), data sharing abstraction [134]. Hadoop's\nMapReduce operations are heavily dependent on the hard\ndisk while Spark is based on in-memory computation, which\nmakes Spark a hundred times faster than Hadoop [134], [145].\nSpark support interactive operations, Directed Acyclic Graph\n(DAG) processing, and process streaming data in the form of\nmini-batches in near real-time [146]. Apache Spark is batch\ncentric and treats stream processing as a special case, lack-\ning support for cyclic operations, memory management, and\nwindows operators. Such issues of Spark has been elegantly\naddressed by Apache Flink [135]. Apache Flink treats batch\nprocessing as a special and does not use micro-batching.\nSimilarly, Apache Storm and Samza is another prominent\nsolution focused on working with large data \u001dow in real-time.\nA brief description and comparison of the open-source big\ndata frameworks are listed in Table 7.\nTo achieve scalability, big data techniques can be exploited\nby existing video analytics modules. The VBDPL is not\nprovided by default and needs its implementation on the top\nof these big data engines. However, The MLapproaches can\nbe categorized into two classes in the context of VBDML.\nOne class re-implements the existing MLtask by providing a\nmiddleware layer to run them on a big data platform. This\ngeneral type of middleware layer provides general primi-\ntives/operations that assists in various learning tasks. Users\nwho want to try different MLalgorithms in the same frame-\nwork can take bene\u001cts from it. In the second class, the indi-\nvidual video analytics and MLalgorithm are executed on a\nbig data platform that is directly built on top of a big data\nengine for better scalability.\nSpark MLlib [142], Mahout [138], FlinKML [135] are\nlist of some open-source MLpackages built on the top ofHadoop, Spark and Flink, respectively, that support many\nscalable learning algorithms. For deep learning, various\nopen-source libraries have been develop including Tensor-\nFlow [123], DeepLearning4J [139], Keras [140], Caffe [124],\nH20 [147], BigDL [141], and PyTorch [125]. All these\nlibraries provide support for various MLalgorithms and fea-\nture engineering. These libraries introduce an independent\nlayer between front-end algorithms and a back-end engine to\nfacilitate the migration from one big data engine to another,\nas shown in Table 7. These algorithms can be used to process\nlarge datasets, just like processing it on a single machine\nby providing the distributed environment abstraction and\noptimization.\n5) COMPUTER VISION BENCHMARK DATASETS\nIn the advancement of IV A, public datasets always play\na vital and active role. Over the year, server benchmark\ndatasets have emerged. Some of the recent popular datasets\nare listed in Table 8. ImageNet [148] is one of the signif-\nicant datasets in deep learning and is utilized for training\nneural networks such as ResNet, AlexNet, and GoogleNet.\nSome more datasets have been developed aiming human\naction and motion recognition, including [150], [150],\n[152], [155], [156]. Google released YouTube-8M [159]\nand consisting of eight million diverse types of automat-\nically labeled videos. Deng A et al. [158] proposed the\nHowTo100M dataset comprising of web videos with nar-\nrated instructions. Dataset like MediaEval2015 [153], and\nTrecvid2016 [150] are designed to support CBVR related\nresearch. For sports IV A, the Sports-1M dataset karpa-\nthy2014large is proposed and composed of 487 classes\nalong with ground truth. YACVID [154] is a labeled image\nsequence dataset for benchmarking video surveillance algo-\nrithms. All these benchmark datasets are used for differ-\nentIV A, such as action recognition, event detection, and\nclassi\u001ccation.\nVOLUME 8, 2020 152391\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nTABLE 8. Popular computer vision benchmark datasets.\nC. KNOWLEDGE CURATION LAYER\nVideos low-level processing produces feature descriptors\nthat summarize characteristics of data quantitatively. The\nhigh-level analytics is more associated with the visual data\nunderstanding and reasoning. The features descriptors work\nas input to the high-level analytics and generate abstract\ndescriptions about contents. The dif\u001ccult problem is to\nbridge the semantic gap between the low-level features and\nhigh-level concepts suitable for human perception [160].\nWith the same indentations, the KCL layer has been pro-\nposed under L-CV AS architecture, on the top of VBDML,\nwhich map the IR(both online and of\u001dine) into the video\nontology in order to allow domain-speci\u001cc semantic video\nand complex event analysis. The KCL is composed of\n\u001cve components, i.e., Video Ontology Vocabulary, Video\nOntology, Semantic Web Rules, FeatureOnto Mapper, and\nSPARQL queries.\nVideo Ontology Vocabulary standardizes the\nbasic terminology that governs the video ontology, such as\nconcept, attributes, objects, relations, video temporal rela-\ntion, video spatial relation, and events. Video Ontology\nis a generic semantic-based model for the representation\nand organization of video resources that allow the L-CV AS\nusers for contextual complex, event analysis, reasoning,\nsearch, and retrieval. Semantic Web Rules express\ndomain-speci\u001cc rules and logic for reasoning. When videos\nare classi\u001ced and tagged by the VBDML then the respective\nIRare persistent to VBDCL and also mapped to the Video\nOntology while using the FeatureOnto Mapper.\nFinally, SPARQL based semantic rich queries are allowed\nfor knowledge graph, complex event reasoning, analysis, and\nretrieval.\nD. WEB SERVICES LAYER\nFinally, to provide the functionality of the proposed L-CV AS\nover the web, it incorporates top-notch functionality into\nsimple uni\u001ced role-based web services. The Web Service\nLayer is built on the top of VBDCL Business Logic.\nSequence diagrams for IV Aalgorithm and service creation is\nshown in Fig. 10. Whereas, role-based use case diagram of\nthe proposed platform is shown in Fig. 9.\nFIGURE 9. Lambda CVAS user roles and use case diagram.\nE. IVA SERVICE EXAMPLE SCENARIOS\nWe show two example scenarios, i.e., how to develop BIV A\nand RIV A services under L-CV AS. Hadoop and Spark\nMapReduce type operations naturally \u001ct in a BIV A. Fig. 11,\nshows the block diagram along with sample script for dis-\ntributed BIV A, i.e., object classi\u001ccation, where a DFS is\ncon\u001cgured to read and store video \u001cles, e.g., in a standard\nvideo \u001cle format such as MPEG, A VI, H.264, etc. First,\nthe videos are loaded, the distributed video transcoding (a\npreprocessing algorithm under VBDPL) are performed. For\nexample, a user uploads a MPEG or other video \u001cles to\ntheDFS. The transcoder algorithm \u001crst split the \u001cle into\nimage frames, which may be throttled to key-value frames,\nand converts them into a sequence \u001cle format. These frames\nare then mapped, and features are extracted (utilizing some\nfeature extractor from VBDPL). The features are then classi-\n\u001ced (using a classi\u001ccation algorithm of VBDPL). For each\n152392 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nFIGURE 10. The sequence diagram for the IVA algorithm and service\ncreation. In the Actor lifeline, AandDrepresent Admin andDeveloper,\nrespectively. These two roles are allowed to create a new video analytics\nalgorithm and service.\ndetected object, a key-value and coordinates of where the\nobject is located within the image are computed. For each\ndetected object in each frame, the map element process the\nframe. The map operation generates and provides output\nas composite visual value pair that includes the visual key,\na time-stamp that identi\u001ces the frame, and the coordinates.\nThe map stages then send the output in the form of a compos-\nite key-value pair to the respective reduce stages. The reduce\nstages provide the output to an output stage, which is persisted\nin the IR-DS of ISDDS.\nFIGURE 11. BIVA service example pipeline under Lambda CVAS.\nFIGURE 12. RIVA service example pipeline under Lambda CVAS.\nThe \u001dow for a single RIV A along with an example\npseudo-code is shown in Fig. 12. The VSAS component sends\nthe video stream to the queue in the broker cluster. Then the\nconsumer service (VSCS) is used to extract the mini-batch\nof video streams from the queue and process it. Once the\nmini-batch is consumed, then it is transcoded, features are\nextracted for classi\u001ccation, and \u001cnally, the classi\u001ccation\nresults are persisted to the required destination (IR-DS and/or\ndashboard).\nF. EXECUTION SCENARIOS\nL-CV AS follows the lambda architecture style [29], and the\nexecution scenarios undergo through two types of execu-\ntion scenarios, i.e., Streaming Execution Scenario, and Batch\nExecution Scenario. These two scenarios aim to execute a\nmassive amount of real-time video stream and batch videos\nagainst the subscribed IV Aservices. In literature, these exe-\ncution scenarios are referred to as Speed Layer, and Batch\nLayer respectively [29]. The data of both the scenarios are\nmanaged through a common layer called Serving Layer.\nL-CV AS components are deployed on various types of clus-\nters in the cloud, and each cluster is subject to scale-out on-\ndemand. Fig. 13illustrates these execution scenarios, and the\nexplanation is given in the following subsections.\n1) STREAMING EXECUTION SCENARIO\nL-CV AS is supposed to deploy a pool of contextual RIV A ser-\nvices that are made available to the user for the subscription.\nVOLUME 8, 2020 152393\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nFIGURE 13. Lambda CVAS: Streaming and Batch execution scenarios.\nOnce a video stream data source is subscribed to service in\nthe pool of contextual RIV A services, then the life-cycle of\nStreaming Execution Scenario encompasses through differ-\nent stages while using distinct L-CV AS components. For the\nease of understandability, these components are deployed on\nsix types of computing clusters in the cloud, which are labeled\nexplicitly as P, V, K, S, N, I, as shown\nin Fig. 13.\nThe cluster P hosts VSAS and provides interfaces to\nexternal VSDS. On con\u001cguration, the video streams are cap-\ntured and transformed into a proper message, which is then\ngrouped into micro-batch, compressed, and loaded to the\nrespective queue in the cluster K.The cluster K deploys the distributed messaging sys-\ntem, where the acquired video streams, IR, and anomalies\nproduced by LVSM are buffered. In this context, the cluster\nK is composed of a set of three types of queues for each\nservice, i.e., RIVA_ID, RIVA_IR_ID, and RIVA_A_ID,\nas described in IV-A1.\nThe mini-batches of video streams residing in the dis-\ntributed broker's RIVA_ID queue need to be persisted\nto the UPDDS andISDDS data stores. For this purpose,\nthe cluster `V' deploys three types of L-CV AS modules,\ni.e.,VSCS, Video Processor andPersistence. The\n\u001crst module allows the cluster V to read the video stream\nmini-batches from RIVA_ID topics in the Cluster K. The\n152394 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\ncluster Vthen processes the consumed video data, encodes\nit, and extracts the metadata from the video. Finally, the video\nstream persistence module saves the video data and the\nrespective metadata to the UPDDS andISDDS, respectively.\nThe cluster S is responsible for processing the video\nstream in near real-time while using the IV Aservices. Dif-\nferent stream processing engines, e.g., Apache Spark Stream,\ncan be used for RIV A. The cluster S deploys four types of\nL-CV AS modules. The \u001crst module is VSCS and is used to\nconsume the video streams from the RIVA_ID queue in the\ncluster K. The second type of module is RIV A services.\nTheRIV A service is the actual video stream analytics service\nthat analyzes the videos. The video RIV A service is loaded\naccording to the RIV A services subscription contract made\nby the L-CV AS user. The RIV A services can be pipelined, and\ntheIRmight need other applications in the multi-subscription\nscenario. Thus the IRproducer/subscriber is used to send and\nreceive the IRaccording to the application logic to and from\ntheIRqueue in Cluster K.\nThe fourth type of module is LVSM producer. The con-\ntextual IV A services (IV AS) instance deployed in the cluster\nSshould have some domain-speci\u001cc goal and can produce\nanomalies if analyzed any. The L-CV AS support real-time\nanomalies delivery system. The IV AS sent the anomalies\ncontinuously to the LVSM producer and the LVSM pro-\nducer to the respective anomalies queue RIVA_A_ID in the\ncluster K.\nThe cluster group I read the IRfrom the RIVA_A_ID\nqueue in cluster K continuously and sent it to the ISDDS's\nIRdata store for persistence. If the subscribed service is using\nthe ontology then the IRare also mapped to the VidOnto triple\nresiding in the Knowledge Curation Server T.\nThe \u001cnal type of cluster in the Streaming Execution\nScenario is cluster N and is known as Anomalies\nNotification Cluster. This cluster aims to read\nanomalies from the RIVA_A_ID queue in cluster K and\nsend the same to the ISDDS for persistence and also delivered\nin real-time to the video stream source owner in the form of\nalerts.\n2) BATCH EXECUTION SCENARIO\nTheL-CV AS architecture is also equipped with BIV A. Unlike\ntheRIV A, the BIV A is analyzed as an of\u001dine manner. The\nexecution time of of\u001dine analytics is proportional to the\nvideo dataset size and the subscribed BIV A service compu-\ntation complexity. The Batch execution life-cycle undergoes\nthrough three types of clusters, i.e., R, M, B.\nThe cluster R allows the L-CV AS user to upload the\nbatch video dataset to the L-CV AS cloud and con\u001cgure three\ntypes of L-CV AS modules. The \u001crst type of service is Batch\nVideo Acquisition Service, which is used to acquire batch\nvideo datasets. Once uploaded to the node buffer, the batch\ndataset is processed by the activated Video Processor to\nextract the metadata from the batch videos. After process-\ning the batch video dataset and the respective metadata are\npersisted to the UPDDS andISDDS, respectively. Similarly,the cluster M works the same way as that of cluster R,\nbut this one is responsible for model management.\nIn the batch video analytics, the supporting layers deploy\nvarious contextual multi-domain of\u001dine BIV A services. This\ncluster loads the instance of the RIV A services as per user\ncontract and processes the videos in an of\u001dine manner. Once\nsubscribed, this cluster loads the batch video data set and\nmodel from the UPDDS. Similarly, the IRand anomalies\nare maintained in the ISDDS. The acquired video streams\nresiding in the UPDDS is also illegible for of\u001dine analytics.\nFinally, the Web Server W deploys the Web User Inter-\nface (as described in IV-D), i.e., allow the users to interact\nwith L-CV AS.\nV. IVA; CONSTITUENTS, AND PREDOMINANT TRENDS\nThis section review the existing IV A literature and can\nbe classi\u001ced into four classes under the umbrella of IV A,\ni.e.,CBVR, IV ASurveillance and Security, Video Summa-\nrization, and Semantics Approaches, as shown in Fig. 14.\nWe also show that how L-CV AS can be used under these\napplication areas.\nA. CONTENT-BASED VIDEO RETRIEVAL\nCBVR has applications from video browsing to intelligent\nmanagement of video surveillance and analysis. To uphold\nadvancement in CBVR, since 2001, the National Institute of\nStandards and Technology has been sponsoring the annual\nText Retrieval Conference Video Retrieval Evaluation [161],\n[162]. The CBVR is an active research area, and several sur-\nveys are available, i.e., [32], [162]\u0015[165]. However, here we\ndiscuss some of the scalable CBVR system being proposed\nin the literature.\nIn literature, some researchers tried to exploit distributed\ncomputing technology for the development of large-scale\nCBVR systems. Shang et al. [166] utilize the time-oriented\ntemporal structure of videos and the relative gray-level inten-\nsity distribution of the frames as a feature base. Their method\nis expensive in terms of parallel processing due to the video\nsemantics, and then all the frames of a video must be pro-\ncessed within the same execution environment. Hence this\napproach is challenging to parallelize accurately and ef\u001c-\nciently even for the state-of-the-art big-data frameworks.\nWang et al. [167] proposed a novel MapReduce frame-\nwork called Multimedia and Intelligent Computing Cluster\nfor near-duplicate video retrieval for large-scare multimedia\ndata processing by joining the computing power of CPU's\nand GPU's to speed up the video data processing. They\nextract the keyframes using uniform sampling, store the\nkeyframes to HDFS, perform local feature extraction using\nthe Hessian-Af\u001cne detector [168] to detect interest points.\nK-means clustering over the feature vectors is utilized to\ngenerate visual words following the BoF [169] model, thus\ngenerating BoF-based feature vectors. Ding et al. [170] used\nbig data processing technologies to design a human retrieval\nsystem on extensive surveillance video data called SurvSurf.\nMotion-based segments called M-clop were detected, which\nVOLUME 8, 2020 152395\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nFIGURE 14. IVA taxonomy based on literature.\nwere utilized to remove redundant videos. Hadoop MapRe-\nduce framework was used to process M-clips for human\ndetection and motion feature extraction. Vision algorithms\nwere accelerated by processing only sub-areas with signi\u001c-\ncant motion vectors rather than entire frames. Further, a dis-\ntributed data store called V-BigTable on top of HBase was\ndesigned to structuralize M-clips' semantic information and\nenables large-scale M-clips retrieval. They stated that Surv-\nSurf outperforms the baseline solutions in terms of computa-\ntional time and with acceptable human retrieval accuracy.\nAuthors in [171] proposed Marlin for video big data simi-\nlarity search. They used parallel computing to extract features\nfrom the acquired video micro-batches, which are then per-\nsisted in a distributed feature indexer. The proposed indexer\nwas able to index incremental updates and real-time queries.\nThey designed a \u001cne-grained resource allocation with a\nresource-aware data abstraction layer over streaming videos\nto upsurge the system throughput. They reported Marlin\nachieved 25X and 23X speedup against the sequential feature\nextraction algorithm and similarity search, respectively. The\nchallenge of extracting distinctive features is addressed by\nLvet al. [172] for the ef\u001cciency of extraction closely related\nvideos from the large scale data based on local and global\nfeatures utilizing Spark. To balance precision and ef\u001cciency,\nthey introduced a multi-feature based distributed system,\nincluding local and global features. They combined local\nfeature SIFT, Local Maximal Occurrence, and global feature\nColor Name. Lastly, they developed the system in a dis-\ntributed environment based on Apache Spark. Further, M. N.\nKhan et al. [173] proposed FALKON for large-scale CBVR\nthat utilized distributed deep learning on top of Apache Spark\nfor accuracy, ef\u001cciency, fault tolerance, and scalability. Moti-\nvated by the fact that Apache Spark, by default, does not pro-\nvide native video data structure, they developed a wrapper on\nthe top of Spark's RDD called VidRDD. Utilizing VidRDD,\n\u001crst, they performed structural analysis on the videos, and\nthen index the extracted deep spatial and temporal features\nin their designed distributed indexer. Finally, they evaluatetheir proposed system and show performance improve-\nment in terms of scalability and accuracy. Likewise, Lin\nFC.et al. [174] put forward a cloud-based face video retrieval\nsystem while utilizing deep learning. First, pre-processing\noperations like termination of blurry images, and face align-\nment are performed. Then the re\u001cned dataset is constructed\nand used to pre-train the CNN models, i.e., ArcFace, FaceNet,\nand VGGFace for face recognition. The results of these three\nmodels are compared, and the ef\u001ccient one was chosen for\nthe retrieval system development. The input query in their\nproposed system is a person's name. If the system detects a\nnew person, it performs enrolling that person. Finally, times-\ntamped results are returned against a query. A prototype of the\nproposed face retrieval system was implemented and reported\nits recognition accuracy and computational time.\n1) CBVR UNDER L-CVAS ARCHITECTURE\nL-CV AS provides an elegant and \u001dexible six steps solution\nfor the implementation and customization of scalable video\nindexing and retrieval, as shown in Fig. 15. First, the VSAS\ncomponent acquires the video streams from VSDS in the\nform of mini-batches, and in the case of batch analytics, video\ndata is loaded from the RAW DS and feeds to the VBDPL.\nIn the second step, VBDPL perform pre-processing oper-\nations and feature extraction. The former one encom-\npasses structural exploration of a video, i.e., video scenes,\nshots detection, frames, and keyframes extraction, while the\nlatter one extracts the low-level features. These low-level\nfeatures can be keyframes' static features (texture-based,\ncolor-based, and shape-based), object features, and/or motion\nfeatures (trajectory-based, statistics-based, and objects'\nspatial relationships-based). The extracted features are\nthen handed over to the VBDML for classi\u001ccation and\nannotation.\nIn the third step, the semantic and high dimensional video\nfeature vectors' indexes make the representative index for\npersisted video sequences in IR-DS. The IR-DS is synchro-\nnized with RAW-DS.\n152396 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nFIGURE 15. Components of CBVR under the Lambda CVAS architecture.\nIn the fourth step, through the WSL, the users are\nallowed to query the desired videos. In literature for video\nretrieval, various types of queries have been utilized for\nvideo retrieval. The queries can be categorized as Query by\nExample, Sketch, Objects, Keyword, and Natural\nLanguage. In theQuery by Example, similar videos or\nimages to the given sample video or image is extracted using\nfeature similarity. In Query by Sketch, the features are\nextracted from users drawn sketches (sketch represent the\nrequired videos) and compared against the features gener-\nated from the stored videos [175]. In Query by Object,\nthe user give an object image then the CBVR system search\nfor all occurrences of the object in the video database [176].\nLikewise, the last two approaches use Keywords and\nNatural Language [177] as query. Combination-based\nQuery combines different types of queries for CBVR, was\nalso adopted by some researchers such as [178], [179].\nIn the \u001cfth step, the CBVR system search for similar videos\nagainst the user query. For similarity measure approaches\ncan be classi\u001ced as feature, text, Ontology and\ncombination based matching. The similarity measure\ndepends on the type of query. Feature matching based simi-\nlarity measure is the average distance between the features of\nthe corresponding frames [180]. Query uses uses low-level\nfeatures for extracting relevant videos. The bene\u001ct of this\napproach is that the video similarity can be found easily from\nthe features but is not appropriate for semantic similarity,\nwhich is conversant to users. In text matching based\nsimilarity measure, the name of each concept is matched\nwith given query terms so that to \u001cnd the videos that sat-\nisfy the query. The best example of this approach is that of\nSnoek et al. [181]. This approach is simple, but the query\ntext must include the relevant concepts to get pleasing search\nresults. In Ontology-Based Matching, the similarity\nbetween between semantic concepts is measured utilizing\nontology. Query descriptions are enriched from knowledge\nsources, such as the ontology of concepts. Adding extra\nconcepts can improve the retrieval results [182] but can\nalso decline search results unexpectedly. This approached is\nfurther explained in section V-D. Combination-based\nmatching ``leverages semantic concepts from a train-\ning collection by learning the combination strategies [86],and query-class-dependent combination models [179]'' [88].\nUp to some extent, using this approach, the concept weights\ncan be automated, and hidden semantic concepts can be\nhandled but are dif\u001ccult to learn query combination models.\nFinally, the ranked result-set is presented to the user\nfor browsing. To increase ef\u001cciency and to optimize the\nresults of the CBVR system, many researchers use the\nobtained relevance feedback from the user. This feedback\ncan be categorized as explicit, implicit, and pseudo\nfeedback. In the \u001crst case, users are asked to select rel-\nevant videos from the previous results actively [183]. The\nexplicit feedback approach obtains better results than\nthe other two approaches, but direct user involvement is\nrequired. In the second case, the retrieval results are re\u001cned\nby exploiting the user's interaction with the system, i.e., click-\ning pattern [184]. In the third case, there is no involve-\nment of the users. The user's feedback is produced through\npositive (closely related to the query sample) and negative\nsample (different from the query sample) from the previous\nretrieval. These samples are directed to the system for the\nnext search. Yan et al. [185], and Hauptmann et al. [186]\napproaches are based on pseudo-relevance feedback\napproach. This approach substantially reduces the user inter-\naction, but the semantic gap between low and high-level\nfeatures obtained from different videos does not always agree\nwith the similarities between the user-de\u001cned videos.\nB. REAL-TIME INTELLIGENT VIDEO ANALYSIS AND\nSURVEILLANCE\nIn the context of growing security concerns, the surveil-\nlance meant to criminality and intrusion detection. Video\nsurveillance is not limited to these, but it encapsulates all\naspects of monitoring to capture the dynamics of diverse\napplication areas, e.g., transportation, healthcare, retail, and\nservice industries. A generic use case for RIV A has been\nshown in section IV-E, Fig. 12. Likewise, domain-speci\u001cc\nRIV A services for security and surveillance can be created\nunder L-CV AS architecture. In this section, we discuss recent\nliterature on how the researchers advancing the activity and\nbehavior analytics in the video streams with the aim of\nintrusion and crime detection, scene monitoring, and resource\ntracking.\n1) VIDEO SEGMENTATION FOR ACTION RECOGNITION\nA combination of numerous actions, objects, and scenes\nforms complex events [187]. Video analytics against complex\nevents is a nontrivial task. Complex event detection demands\nthe association of multiple semantic concepts because it is\nalmost impossible to capture the complex event through a\nsingle event class label [188]. Video segmentation is required\nto mine informative segments regarding the event happening\nin the video. For effective event detection in video segmen-\ntation, it is vital to take into account the temporal relations\nbetween key segments in a particular event. The event videos\nhold intra-class variation, and several training videos are\nrequired to consider all possible instances of event classes.\nVOLUME 8, 2020 152397\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nSong and Wu [189] suggested a methodology to extract key\nsegments automatically for event detection in videos by learn-\ning from a loosely tagged pool of web videos and images.\nFor the positions of key segments and content depiction in\nthe video, they used an adaptive latent structural SVM model\nand semantic concepts, correspondingly. They also developed\ntwo types of models, i.e., Temporal Relation Model and\nSegment-Event Interaction Model, for the temporal relations\nbetween video segments, and for evaluating the correlation\nbetween key segments and events respectively. They adapted\nlabeled videos and images from the Web into their model\nand employed N adjacent point sample consensus [190]\nfor noise elimination. A knowledge-base was produced by\nZhang et al. [191] to decrease the semantic gap between\ncomplex events. To effectively model event-centric semantic\nconcepts, they used a large-scale of web images for learning\nnoise-resistant classi\u001cers.\nAction recognition in videos encompasses both segmen-\ntation and classi\u001ccation. Action recognition can be tackled\nby sliding windows and aggregation in sequential as well\nas isolated manner, or by performing both tasks parallel.\n[188], [192] are some of the good examples of video segmen-\ntation based event detection. The related literature on video\nsegmentation based action recognition can be classi\u001ced as\naction segmentation, a depth-based approach, and\ndeep learning-based motion recognition.\nThe most popular action segmentation model is dynamic\ntime warping scheme [193]. Action segmentation\nfrom videos using an appearance-based method considers a\ncomparison between the start and end frames of contigu-\nous actions. Quantity of movement [194] and KNN along\nwith HOG [195] are commonly used to identify the start\nand stop frames of actions. Similarly, for action recogni-\ntion,depth-based approaches have been developed\nwhich consist of binary range-sample feature [196], captur-\ning local motion and geometry information [197], histogram\noriented 4D normal [198], and combination of depth motion\nmap and HOG [199]. [200]\u0015[202] have applied deep learn-\ning approaches to depth-based action recognition methods.\nBesides, for motion recognition, deep learning has been\nutilized in many ways. One is a suboptimal way, in which\nvideo represented as still images is fed to the channel of a\nCNN [96], [116]. Sometimes video as compact images are\ngiven as input to the already trained CNN to achieve good\nperformance [203]. Video in the form of a sequence of images\nis input to the Recurrent neural network (RNN) for sequential\nparsing of video frames for long as well as short-term pat-\nterns [204], [205]. In order to introduce a temporal dimension,\nvideo is regarded as a volume and substitutes the 2D \u001clters of\nCNN with 3D \u001clters [206], [207].\nIn many application areas, e.g., resource tracking, action\nrecognition, human behavior recognition, and traf\u001cc control,\nobject tracking and motion detection in videos are vital. The\nprocess of detecting moving objects in videos is known as\nobject tracking. For object tracking, initially, the foreground\ninformation is extracted in videos, and then the backgroundmodeling of the scene is captured using a background sub-\ntraction algorithm, i.e., IAGMM [208], [209]. To increase the\naccuracy, subsequently, shadow elimination algorithms are\napplied to the foreground frame [210]. A connected compo-\nnent algorithm is used to determine the bounding box of an\nobject. To ensure frame-to-frame matching of the detected\nobject, a method such as adaptive mean shift [211] can be\nused for comparison. Factors like size and distance are used\nto object matching between frames. Finally, the occlusion\nscheme is used to detect and resolve occlusion. On the\nother hand, motion detection can be detected via foreground\nimages extracted by the Gaussian Mixture Model background\nand connected component algorithm for noise removal. The\narea of detection is re\u001cned using a connected component\nalgorithm and produces the bounding box information of\nthe moving objects [208]. The output of the detection is\na binary mask representing the moving object for each\nframe in a particular sequence. Object detection for moving\nobjects is challenging, especially in the case of shadows and\ncloud movement [212]. The related literature for moving\nobject detection and classi\u001ccation can be categorized as\nastationary camera with a moving object\nandmoving objects with moving camera.\na: MOVING OBJECT DETECTION WITH STATIONARY\nCAMERA\nIn \u001cxed camera video, the background image pixels in\neach frame remain the same, and thus simple background\nsubtraction techniques are required. The object detection\napproaches using a \u001cxed camera can be grouped as feature-\nbased, motion-based, classi\u001cer-based, and template-based\nmodels [213]. Categorization of object tracking in videos\ninto point tracking, kernel tracking, and silhouette tracking\nas well as feature-based, region-based, and contour-based\nwas performed by [213], [214] respectively. Unlike a \u001cxed\ncamera, moving object detection with a moving camera is\nrelatively challenging because of camera motion and back-\nground modeling for generating foreground and background\npixel fails [215].\n\u000fTrajectory classi\u001ccation involves computing long\ntrajectories for feature point and discriminating tra-\njectories that belong to different objects from those\nbackgrounds using the clustering method. Some rec-\nommended algorithms include compensating long term\nmotion based on \u001dow optic technique [216], bag-of word\nclassi\u001cer, and pre-trained CNN method for detecting\nmoving object trajectories [217].\n\u000fInbackground modeling based methods, for each\nsequence, the frame-by-frame background is created\nutilizing the motion compensation method. Some pop-\nular algorithms are Mixture of Gaussian [218], com-\nplex homography [219], gaussian-based method [220],\nadaptiveMoG [221], multi-layer homography trans-\nform [222], thresholding [223], and CNN-based\nmethod [224] for background modeling.\n152398 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\n\u000fInextension of background subtraction method, low\nrank and sparse matrix decomposition method for static\ncamera [225] are extended to moving camera. If there\nexists coherency between a set of image frames, then a\nlow-rank representation of the matrix created by these\nframes contains the coherency, and the sparse matrix\nrepresentation contains the outliers, which represents the\nmoving object in these frames. Low rank and sparse\ndecomposition involve segmenting moving objects from\nthe \u001cxed background by applying principal component\npursuit. It is a valuable technique in background mod-\neling. Mathematical formula and optimization of this\nmethod can be found in [226].\n2) BEHAVIOR ANALYSIS\nUsually, a camera is mounted nearby the digital displays to\nanalyze and understand human behavior by investigating user\ninterfacing with digital display [227]. Commercial tools have\nbeen developed to analyze audience behavior using video\nanalytics while considering parameters like age, gender, dis-\ntance from the display, and sight and spent time. The obtained\ndata can then be used to improve advertising campaigns in\ncombination with sales data [228].\nRecently, crowd analytic, i.e., human detection and track-\ning, have attracted the attention of the researches. The explo-\nration of both group and individual behavior to govern\nabnormality scope the crowd analysis. Congestion analysis,\nmotion detection, tracking, and behavior analysis are the\nmain attributes of crowd analytics. While performing crowd\nanalysis, factors like terrain features, geometrical informa-\ntion, and crowd \u001dow can be considered.\nFor analysis of the crowded scene, motion features\nare vital, and can be categorized as flow-based\nfeatures, local spatiotemporal features,\nandtrajectory features [229]. These features have\napplications in crowd behavior recognition, abnormality\ndetection in-crowd, and motion pattern segmentation.\na: FLOW-BASED FEATURES\nThe \u001dow-based features are pixel-level features, and in lit-\nerature, different schemes have been proposed [230], [231],\nwhich be further classi\u001ced as optical flow, partial\nflow, and streak flow. Optical flow technique\nencompasses computing pixel-wise motion between succes-\nsive frames and can handle multi-camera objection motion.\nIt has been applied to detection crowd motion and crowd\nsegmentation [232]. This approach, optical \u001dow, is unable to\ncapture spatiotemporal attributes of the \u001dow and long-range\ndependencies. Particle flow contains moving a grid\nof particles with the optical \u001dow and providing trajectories\nthat maps a particle's initial position to its future or current\nposition. It has an application in crowd segmentation and\ndetection of abnormal behavior [229]. Optical \u001dow is unable\nto handle spatial changes. Mehran et al. [231] proposed\nstreak flow to overcome the shortcomings of particle\n\u001dow and to analyzing crowd video while computing motion\u001celd. This approach, streak \u001dow, captures motion informa-\ntion similar to particle \u001dow; changes in the \u001dow is faster and\nperforms well in dynamic motion \u001dow.\nb: LOCAL SPATIOTEMPORAL FEATURES\nFlow-based features fail on the very crowded scene, and\nresultantly local spatiotemporal features techniques are\ndeveloped, which are 2D patches or 3D cubes representa-\ntion of the scene. Spatiotemporal features can be catego-\nrized as spatiotemporal gradients, and motion\nhistogram. To capture steady-state motion behavior, Kratz\nand Nishino [233] used a spatiotemporal motion pattern\nmodel and con\u001crmed the detection of abnormal activities.\nOn the other hand, motion histogram considers motion\ninformation within the local region. It is not appropriate\nfor crowd analysis because it takes a substantial amount\nof time, and is subject to error. However, some improve-\nments have been shown in the literature to motion histogram,\ne.g., [230], [234].\nc: TRAJECTORY FEATURES\nTrajectory features signify tracks in videos. The distance\nbetween object-based motion features can be extracted from\nthe trajectories of objects and can be utilized to analyze crowd\nactivities. The failure to obtain a full trajectory in dense crowd\nleads to the concepts of tracklet. The tracklets are extracted\nfrom the dense region and enforce the spatiotemporal corre-\nlation between them to detect patterns of behavior. Tracklet is\na fragment of a trajectory obtained within a short period, and\nthe occurrence of occlusion leads to closure. Tracklets have\nbeen used for human action recognition [31], [235] and for\nthe representation of motion in crowded scenes [236], [237].\n3) ANOMALY DETECTION\nAnomaly detection an application area of crowd behavior\nanalysis and is domain-dependent. Anomaly in a video occurs\nwhen the analyzed pattern drifts from the normal in a training\nvideo. The related literature of anomaly detection can be\ncategorized into three, i.e., trajectory-based, global\npattern-based, and grid pattern-based method\nof anomaly detection.\na: TRAJECTORY-BASED METHOD OF ANOMALY DETECTION\nIn trajectory-based anomaly detection, objects are formed\nfrom the segment scenes, and then the object is followed in\nthe video. A trajectory is caused by the tracked object, which\ndescribes the behavior of the object [238]. For the evalua-\ntion of abnormality in trajectory-based methods have been\nused i.e., single-class SVM [239], zone-based analysis [240],\nsemantic tracking [241], String kernels clustering [242],\nSpatiotemporal path search [243], and deep learning-based\napproach [244] have been used.\nb: GLOBAL PATTERN-BASED METHOD OF ANOMALY\nDETECTION\nIn a global pattern-based technique, the video sequence\nis analyzed in whole, i.e. low, or medium-level features\nVOLUME 8, 2020 152399\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nare extracted from video using Spatiotemporal gradients\nor optical \u001dow methods [245]. The technique is suitable\nfor crowd analysis because it does not individually track\neach object in the video but is challenging while locat-\ning the position where the anomaly occurred. Approached\nused in the global pattern-based method are Gaussian\nMixture Model [246], energy model [247], SFM [248],\nstationary-map [249], Gaussian regression [250], PCA\nmodel [251], global motion-map [252], motion in\u001duence\nmap [253], and salient motion map [254].\nc: GRID PATTERN-BASED METHOD OF ANOMALY\nDETECTION\nIn a grid pattern-based method, splits frames into blocks\nand individually analyze pattern on a block-level basis [255].\nIf ignoring inter-object connections that lead to process-\ning ef\u001cciency. Spatiotemporal anomaly maps, local fea-\ntures probabilistic framework, joint sparsity model, mix-\ntures of dynamic textures with Gaussian Mixture Model,\nlow-rank and sparse decomposition, cell-based texture anal-\nysis, sparse coding and deep networks are used in evaluating\ngrid pattern-based methods [256].\nC. VIDEO SUMMARIZATION\nVideo big data are facing the challenge of sparsity and redun-\ndancy, i.e., hours of videos with less meaningful information,\nwhich creates many issues for viewing, mining, browsing,\nand storing videos. It has motivated researchers to \u001cnd ways\nto shorten hours of videos and led to the \u001celd of video sum-\nmarization. Video summarization is a process of generating\na shorter video of the original one without spoiling the capa-\nbility to comprehend the meaning of the whole video [257].\nThe video summarization can be classi\u001ced as Static Video\nAbstracts, Dynamic Video Skimming, and Video Synopsis.\n1) STATIC VIDEO ABSTRACT\nThese approaches include a video table of contents, a sto-\nryboard, and a pictorial video summary. For example, Xie\nand Wu [258] propose an algorithm to generate a video sum-\nmary for broadcasting news videos automatically. An af\u001cnity\npropagation-based clustering algorithm is used to group the\nextracted keyframes into clusters, aiming to keep the relevant\nkeyframes that distinguish one scene from the others and\nremove redundant keyframes. J. Wu et al. [259] were moti-\nvated by the notion from high-density peaks search clustering\nalgorithm. They proposed a clustering algorithm by incorpo-\nrating signi\u001ccant properties of video to gather similar frames\ninto clusters. Finally, all clusters' centers were presented as\na static video summary. Bhaumik et al. [260] proposed a\nsummarization technique where they detect keyframes from\neach shot that eliminates redundancy at the intra-shot and\ninter-shot levels. For frames redundancy elimination, SURF\nand GIST feature descriptors were extracted for computing\nthe similarity between the frames. The quality of the sum-\nmaries obtained by using SURF and GIST descriptors are also\ncompared in terms of precision and recall.Similarly, Zhang et al. [261] propose a subset\nselection technique that leverages supervision in the\nform of human-created summaries to perform automatic\nkeyframe-based video summarization. They were motivated\nby the intuition that similar videos share similar summary\nstructures. The fundamental notion is to nonparametrically\ntransfer summary structures from annotated videos to unseen\ntest videos. Concretely, for each fresh video, they \u001crst com-\npute the frame-level similarity between annotated and test\nvideos. Then the summary structures are encoded in the\nannotated videos with kernel matrices made of binarized\npairwise similarity among their frames. Those structures\nare then combined into a kernel matrix that encodes the\nsummary structure for the test video. Finally, the summary\nis decoded by feeding the kernel matrix to a probabilistic\nmodel called the determinantal point process to extract a\nglobally optimal subset of frames. M. Gygli et al. [262] used\na supervised approach to learn the importance of the global\ncharacteristics in summary by extracting deep features of\nvideo frames. J. Mohan et al. [263] proposed a technique that\nutilizes sparse autoencoders. Motion vectors have been used\nfor the elimination of redundant frames, and then high-level\nfeatures are extracted from frames using sparse autoencoders.\nThese high-level feature vectors are then clustered using the\nK-means algorithm. The frames closest to the centroid of\neach cluster are selected as keyframes of the input video.\nJi, Zhong, et al. [264] employed tag information, i.e., titles\nand descriptions, as the side information for the generation\nof summarization. A sparse auto-encoder was used as the\nprimary model to generate the \u001cnal summary, where the input\nand output were multiple videos and keyframes set, respec-\ntively. They fused the visual and tag information to guide\nvisual features, which constrained the sparse auto-encoder\nto select the candidate keyframes.\n2) DYNAMIC VIDEO SKIMMING\nA summary video is formed from the video segments of the\noriginal video to remove redundancy or to summarize based\non object action or events. As an example of object-based\nskimming, Peker et al. [265] proposed a video skimming\nalgorithm while utilizing face detection on broadcast video\nprograms. In the algorithm, the attention was given to faces,\nas they establish the focus of most consumer video pro-\ngrams. Ngo et al. [266] represent a video as a complete\nundirected graph and exploit the normalized cut algorithm\nto form optimal graph clusters. One-shot was taken from\neach cluster of visually alike shots to remove duplicate shots.\nXiao et al. [267] mine frequent patterns from a video. A video\nshot importance evaluation model is utilized to choose useful\nvideo shots to create a video summary. For personal videos,\nGaoet al. [268] developed a video summarization technique,\nwhich encompasses a two-level redundancy detection proce-\ndure. First, they terminated redundant video content with the\nhierarchical agglomerative cluster method at the shot level.\nThen parts of shots were selected, based on the ranking of the\nscenes and keyframes, to generate an initial video summary.\n152400 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nFinally, to terminate the redundant information, a repetitive\nframe segment detection step was utilized. They veri\u001ced\nthe proposed technique through a prototype while using TV\ndatasets (movies and cartoons videos) and reported the per-\nformance in terms of compression ratio (81%) and recall\n(87.4%). In [269], for user-generated video summarization,\nboth the representativeness and the quality of the selected\nsegments from an original video were considered. They stated\nthat user-generated videos contain semantic and emotional\ncontent, and its preservation is vital. They have designed a\nscheme to pick representative segments that include consis-\ntent semantics and emotions for the whole video. To ensure\nthe quality of the summary, they computed quality measures,\ni.e., motion and lighting conditions, and integrate them with\nthe semantic and emotional clues for segment selection.\nJi, Zhong, et al. [270] addresses the issue of supervised\nvideo summarization by formulating it as a sequence-to-\nsequence learning problem, where the input and output is a\nsequence of original video frames and a keyshot sequence,\nrespectively. The notion is to learn a deep summarization net-\nwork with attention mechanism to mimic the way of selecting\nthe human keyshots. The proposed framework was called\nattentive encoder-decoder networks for video summarization.\nThey utilized the BiLSTM encoder for encoding the con-\ntextual information among the input video frames. For the\ndecoder, two attention-based LSTM networks, are explored\nby using additive and multiplicative objective functions,\nrespectively. The results demonstrate the superiority of the\nproposed framework against the state-of-the-art approaches,\nwith remarkable improvements. J. Wu et al. [271] were moti-\nvated by the fact that multi-video summarization is signif-\nicant for video browsing and proposed a technique where\nmulti-video summarization was formulated as a graph prob-\nlem. They also introduced a dynamic graph convolutional net-\nwork to measure the importance and relevance of each video\nshot locally as well as globally. They adopted two approaches\nto address the inherent class imbalance issue of video summa-\nrization. Additionally, a diversity regularization to encourage\nthe model to generate a diverse summary was introduced.\nThe results demonstrate the effectiveness of our proposed\nmodel in generating a representative summary for multiple\nvideos with encouraging diversity. Z. Sheng-hua et al. [272]\nproposed a deep learning-based dynamic video summariza-\ntion model. First, they addressed the issue of the imbalanced\nclass distribution in video summarization. The over-sampling\nalgorithm is used to balance the class distribution on train-\ning data. They proposed two-stream deep architecture with\ncost-sensitive learning to handle the class imbalance problem\nin feature learning. RGB images are utilized to represent\nthe appearance of video frames in the spatial stream. Likely,\nmulti-frame motion vectors with deep learning framework\nare introduced to represent and extract temporal information\nof the input video. Moreover, they stated that the proposed\nmethod highlights the video content with the active level of\narousal in effective computing tasks and can automatically\npreserve the connection between consecutive frames.\nFIGURE 16. Simultaneous activities display in a video [273].\n3) VIDEO SYNOPSIS\nIn this approach, activities from the stated time interval are\ncollected and moved in time to form a smaller video synop-\nsis showing maximum activity, as shown in Figure 16. The\nnotion of video synopsis was pioneered by [273] in 2006 and\nproposed a two-phase approach, i.e., online and of\u001dine. The\nformer phase includes the queuing of the generated activities.\nThe later phase started after selecting a time interval of video\nsynopsis with tube readjustment, background formation, and\nobject stitching. A global energy function was de\u001cned and\nencompassing activity, temporal consistency, and collision\ncost. Then the simulated annealing method was applied for\nenergy minimization. The video synopsis domain was further\nresearched in single and multi-view scenarios. Some recent\nexamples of a single-view are [274]\u0015[276]. He et al. [274],\n[275] brought advancement in activity collision analysis\nby describing collision statuses between activities such as\ncollision-free, colliding in the same direction and opposite\ndirections. They also offered a graph-based optimization\ntechnique by considering these collision states to improve\nthe activity density and put activity collisions at the center\nof their optimization strategy. Baskurt and Samet [276] con-\ncentrated on rising robustness of object detection by suggest-\ning an adaptive background generation method. In another\nstudy, Baskurt and Samet [277] planned the object track-\ning method speci\u001ced for video synopsis requirements. Their\napproach focused on long term tracking to represent each\ntarget with just one activity in video synopsis. Single view\nscalable approaches for video synopsis were projected by\nLinet al. [278] while utilizing distributed computing technol-\nogy. Their proposed video synopsis approach encompasses\nsteps like object detection, tracking, classi\u001ccation, and opti-\nmization, which were performed in a distributed environ-\nment. Ahmed, S. A. [279] proposed a query-based method\nto generate a synopsis of long videos. Objects were tracked\nand utilized deep learning for objects classi\u001ccation (e.g.,\ncar, bike, etc.). Through unsupervised clustering, they iden-\nti\u001ced regions in the surveillance scene. The source and the\nVOLUME 8, 2020 152401\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\ndestination represented spatiotemporal object trajectories.\nFinally, user queries were allowed to generate video synopsis\nby smoothly blending the appropriate tubes over the back-\nground frame through energy minimization.\nFurther, the examples of multi-view video analytics are that\nof [280], [281]. Zhu et al. [280] proposed a framework to gen-\nerate a uni\u001ced synopsis of multi-view videos. The synopsis is\nvisualized by mapping multiple views to a common ground\nplane. Multiple camera activities were allied via trajectory\nmatching in overlapping camera views. The process of pro-\nducing a synopsis requires a balance among minimizing the\nsynopsis length, maximizing the information coverage, and\nreducing the collision among objects' track that are presented\nconcurrently. Likewise, Mahapatra et al. [281] proposed a\nmulti-camera approach for an overlapping camera network\nand modeled the synopsis generation as a scheduling prob-\nlem. They utilized three distinct methods, i.e., table-driven\napproach, contradictory binary graph coloring approach, and\nsimulated annealing. Action recognition modules were inte-\ngrated to recognize signi\u001ccant actions, i.e., walking, running,\nbending, jumping, hand-shaking, and one or both hands wav-\ning. The inclusion of such essential actions can help in the\nsynopsis length reduction while preserving the value. The\nsynopsis length was further reduced while utilizing a fuzzy\ninference system that computes the visibility score for each\nobject tracking. They stated that through the contradictory\nbinary graph coloring approach, they achieved a maximum\nreduction in synopsis length. Zhang, Z. et al. [282], tried to\naddress the issue of video synopsis by joint object-shifting\nand camera view-switching to show multiple synopsis results\nmore compactly and understandably. The input videos were\nsynchronized and grouped the same object in different videos\ntogether. Then they shifted the grouped objects with respect\nto the time axis to gain multiple synopsis videos. They con-\nstructed a simultaneous object-shifting and view-switching\noptimization framework to achieve encouraging synopsis\nresults. To address uni\u001ced optimization, they further pre-\nsented an optimization strategy composed of graph cuts and\ndynamic programming.\nVIDEO SUMMARIZATION aaS UNDER L-CVAS\nFig. 17 shows the \u001dow of the video summarization \u001dow\nunder the proposed L-CV AS architecture. Through the WSL,\nthe user \u001crst subscribes to the video data-source to the video\nsummarization service. Then the user preferences allow the\nusers to set the parameters required for video summary ser-\nvice and personalization. The summary parameters encom-\npass granularity level, type of summary to be performed\n(e.g., overview, highlights, synopsis, etc.) and any other as\nper the video summary service scenario. The personaliza-\ntion can be in terms of speci\u001cc features of the video, like\npeople, objects, events, etc. Through the VBDCL, videos\nare acquired from the video data-source and sent to the\nVBDPL for pre-processing. In pre-processing, video units are\nextracted (segmentation, shots, frame extraction, etc.) as per\nthe requirements of the video summarization service. Then\nFIGURE 17. Generic flow of video summarization aaS under Lambda\nCVAS architecture.\nmultiple low and high-level features, such as motion, color,\naesthetics, semantics, etc., are extracted from the video units.\nThe extracted features from the basic video units are input\nto the VBDPL for object/activity identi\u001ccation and cluster-\ning. Once done then the next step is video summarization.\nThe video summarization phase deploys the actual logic,\ni.e., video unit selection, and redundancy removal. The video\nunit selection and redundancy removal decide which video\nunits should be included in the video summary based on unit\nsigni\u001ccance, summary length, and other user's parameters.\nThis block also removes similar video units within the video\nsummary to achieve the best possible video summary, cover-\ning the required details in the original video. Finally, the sum-\nmary results are delivered to the respective user through the\nWSL.\nD. SEMANTIC-BASED VIDEO ANALYSIS\nTo bridge the semantic gap and to allow the machine\nto understand the visual data, Semantic Web technolo-\ngies can be incorporated [160]. Semantic Web unlocked\na new avenue for knowledge-based computer vision while\nenabling data exchange between video analysis systems\nin an open and extensible manner. The scienti\u001cc commu-\nnity has exploited the Semantic Web concept for intel-\nligent video analytics to bridge the so-called semantic\ngap between the low-level features and high-level human-\nunderstandable concepts. State-of-the-art scholarly work can\nroughly be classi\u001ced as semantic-based low-level, mid-level,\nhigh-level analysis, and semantic-based video search and\nretrieval.\n1) SEMANTIC-BASED LOW-LEVEL ANALYSIS\nSemantic-based low-level analysis refers to the formaliza-\ntion of the extracted objects of interest from the videos.\nIt then performs reasoning on formalization such as detec-\ntion and tracking across domain-speci\u001cc videos [283], [284].\nDasiopoulou et al. [285] proposed a multimedia ontology\nfor domain-speci\u001cc video analysis. In semantic concepts,\nthey consider object attributed, low-level features, spatial\nobject relations, and the processing approaches while de\u001cn-\ning F-logic rules for reasoning that govern the applica-\ntion of analysis methods. Garca et al. [286] proposed a\nknowledge-based framework for video object segmentation,\nwhere relationships among analysis phases are utilized. The\nmain contribution is to provide a detailed description of the\nscene at low, mid, and high semantic levels through an ontol-\nogy. The notion is to offer the semantic rich description of a\n152402 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nscene via an ontology that includes occurrences in the scene\nfrom high to low semantic level, controls iterative decisions\non every stage. The low-level analysis modules (background\nsubtraction, short-term change detection, point or region\ntracking, etc.) are provided with a structure to collaborate and\nachieve consistently, and contextual results. The results of the\nvision algorithms are mapped to the ontology, representing\nlow-level scene occurrences. The following stages build Point\nHypothesis Maps and Region Hypothesis Maps. These were\nthe most probable occurrences of each point and region. The\npoints and regions are coded according to the ScenePoint\nand SceneRegion hierarchies of the analysis ontology. The\nquality of the results was evaluated through a feedback path.\nGomez et al. [287] proposed a computer vision framework\nfor surveillance and consists of two layers, i.e., the tracking\nlayer and a context layer. The proposed framework relies on\nan ontology-based representation of the scene in combination\nwith contextual information and sensor data. The notion is the\napplication of logical reasoning initiating from the acquired\ndata from a classical tracker intending to construct an onto-\nlogical model of the objects and the activities happening in\nthe area of observation. Reasoning procedures were utilized\nto detect and predict tracking errors, sending feedback to the\ntracker to adjust the low-level image processing algorithms.\nVision operation, like movement detection, blob-track asso-\nciation, and track and trajectory generation, were performed\nin the tracking layer. The context layer was supposed to\nproduce a high-level interpretation of the scene. RACER\nreasoner [288] was utilized for scene interpretation since it\nallows abductive reasoning. Abductive rules are de\u001cned in\nthe proposed framework to interpret what is happening in the\nscene from the primary tracking data.\n2) SEMANTIC-BASED MID AND HIGH-LEVEL ANALYSIS\nAtomic events such as loitering, fall, direction changes, group\nformations, and separations [289], and ``complex'' events\nsuch as aggressions, \u001cghts, thefts, and other general suspi-\ncious events [290], [291] falls in mid and high-level video\nanalytics, respectively.\nThe utilization of semantic technology for video event\nrepresentation in surveillance videos was initiated by Video\nEvent Representation Language [292]. The main idea is to\nmodel simple events in a hierarchical framework intended\nfor detecting complex events. They stated that the sequence\nof simple events (car door opening, leaving a car, car door\nclosing, walking and opening a building door, and entering a\nbuilding) forms complex events (a person arrived through a\ncar and entered a building). They used Allen's interval algebra\nto handle temporal relationships between subevents. They\nclari\u001ced their proposed application through the detection\nof an example event in a surveillance video, i.e., accessing\na secure zone by entering behind an authorized individ-\nual. For complex events recognition in surveillance videos,\nSnidaro et al. [293] proposed an ontology and is composed\nof three high-level concepts, i.e., background, entities, and\nevents. The event class, high-level in subclasses, describessimple events, spatial events, and transitive events, allow-\ning to show how complex events can be described through\nsimple events sequencing. The event concept is composed\nof sub-concepts that represent simple events, spatial events,\nand transitive events. The focus was on complex events,\nwhich can be achieved by sequencing simple events. San-\nMiguel et al. [294] propose an ontology for representing the\nprior knowledge related to video event analysis. Such knowl-\nedge is described in terms of scene related entities (Object,\nEvent, and Context), and system-related entities. The key\ncontribution of the work is the integration of different types of\nknowledge in an ontology for detecting the objects and events\nin a video scene. In the same direction, Greco et al. [295]\nproposed a hybrid approach for simple abnormal (person\nfalling) and complex abnormal events (person aggression)\nrecognition using semantic web technologies. They mod-\neled the extracted general tracking information to the pro-\nposed tracking ontology for advanced reasoning. The data\nfrom the videos were obtained using the tracking compo-\nnent (frames, bounding box), knowledge about the scene\n(static and dynamic objects, occluding objects), Situations\nand Events (people leaving the scene, falling ground, \u001cght-\ning). For event detection, SPIN rules and functions are used\nwhile SPARQL queries are employed for analytics tasks. The\nsystem has proven to successfully recognize mid-level events\n(ex. people falling to the ground) and high-level events (ex.\nperson being attacked) on the PETS2016 dataset.\nResearches also utilized semantic technologies aiming to\naddress the issue of human activity recognition in daily living.\nIn this context, Chen et al. [296] introduced a method for\nactivity recognition while using ontological model, represen-\ntation, and reasoning. They analyzed the nature and charac-\nteristics of daily life activities and modeling related concepts\nthrough ontologies. The authors describe the algorithms of\nactivity recognition making full use of the reasoning power\nof semantic modeling and representation. They claimed that\nthe proposed ontological models for daily life activities\ncould easily be customized, deployed, and scaled up. Like-\nwise, [297], an approach exploiting the synergy between\nthe semantic technologies and tracking methods have been\npresented for object labeling. The work aims to augment and\ncomprehend situation awareness, as well as critical alerting\nconditions. The unmanned aerial vehicles with an embedded\ncamera were utilized to recognize moving and stationary\nobjects along with relations between them. Contextual infor-\nmation was used for abnormal event detection. A prototype\nwas designed and used a drone to capture videos on the\nUniversity of Salerno. They stated that the proposed system\ncould recognize an abnormal event by means of SWRL rules\nassociated with mid-level activities, such as ball kicking by a\nhuman and a car passing through the same road.\n3) SEMANTIC-BASED VIDEO RETRIEVAL\nSome researchers exploited semantic technologies for video\nsearch and retrieval purposes. In this regard, Yao et al. [298]\nproposed the image to text framework to extract events from\nVOLUME 8, 2020 152403\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nTABLE 9. Scholarly state-of-the-art-work CVAS.\nimages (or video frames) and then provide semantic and\ntext annotations. The And-or-Graph incorporates vocabular-\nies of visual elements like objects and scenes along with\nstochastic image grammar that identi\u001ces semantic relations\namong the visual elements. In this way, low-level image\nfeatures are linked with high-level concepts, and the parsed\nimage can be transformed into semantic metadata to form\nthe textual description. Video contents are expressed in both\nOWL and text format, and then the users are allowed to\nsearch images and video clips through keyword searching\nand semantic-based querying. Xue et al. [299] proposed an\nontology-based content archive and retrieval framework for\nsurveillance videos. A surveillance ontology was proposed\nthat represents semantic information of video clips as a\nresource ontology. Such an ontology models the basic feature\ndescription in the low level, the video object description in\nthe mid-level, and event description in the high-level. The\nproposed system was tested for object and event retrieval,\nsuch as walking and car parking.\nFurthermore, Xu et al. in [300] and [301] propose a method\nto annotate video traf\u001cc events while considering theirspatial and temporal relations. They introduced a hierarchi-\ncal semantic data model called structural video description,\nwhich consists of three layers, i.e., pattern recognition layer\n(ontological representation from the video the extracted video\nconcepts), video resources layer (links video resources with\ntheir semantic relations), and demands layer (retrieval inter-\nface). They de\u001cned various concepts in the ontology, such\nas persons, vehicles, and traf\u001cc signs that can be used to\nannotate and represent video traf\u001cc events. Besides, the spa-\ntial and temporal relationships between objects in an event\nare de\u001cned. As a case study, an application to annotate and\nsearch traf\u001cc events is considered. Sah et al. [302] proposed\na multimedia standard-based semantic metadata model and\nannotate globally inter-operable data about abnormal crowd\nbehaviors from surveillance videos. Similar efforts are made\nby Sobhani et al. [303] and proposed an advanced intelligent\nforensic retrieval system by taking advantage of an ontologi-\ncal knowledge representation while considering the UK riots\nin 2011 as a use case. Similarly, A. Alam et al. [304] proposed\na layered architecture for large-scale distributed intelligent\nvideo retrieval while exploiting deep-learning and semantic\n152404 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\napproaches called IntelliBVR. The base layer is responsible\nfor large-scale video data curation. The second and third\nlayers are supposed to process and annotate videos, respec-\ntively while using deep learning on the top of a distributed\nin-memory computing engine. Finally, the knowledge cura-\ntion layer, where the extracted low-level and high-level fea-\ntures are mapped to the proposed ontology, can be searched\nand retrieved using semantic rich queries. Finally, they pro-\njected the effectiveness of IntelliBVR through experimental\nevaluation.\nVI. STATE-OF-THE-ART CVAS\nIn this section, we review the start-of-the-art CV AS. This dis-\ncussion also supports our claim on the relationship between\nvideo big-data analytics and cloud computing. The discussion\nis further divided into two subsections, i.e., Scholarly CV AS\nand Industrial CV AS.\nA. SCHOLARLY CVAS\nIn this subsection, we explore academic research trends (sum-\nmarized in table 9) that how scienti\u001cc community investigate\nand proposed cloud-based IV Asolutions while utilizing big\ndata technologies. In this direction, Ajiboye, S.O. et al. [305]\nstated that the network video recorder is already equipped\nwith intelligent video processing capabilities but complained\nabout its limitations, i.e., isolation, and scalability. To resolve\nsuch issues, they proposed a general high-level theoretical\narchitecture called Fused Video Surveillance Architecture\n(FVSA). The design goals of the FVSA were cost reduc-\ntion, unify data mining, public safety, and scalable IV A.\nTheFVSA architecture consists of four-layer, i.e., Applica-\ntion layer (responsible for system administration and user\nmanagement), Services Layer (for storage and analytics),\nNetwork Layer, and Physical Layer (physical devices like\ncamera, etc.). They guaranteed the compatibility of FVSA\nwith the hierarchical structure of computer networks and\nemerging technologies. Likewise, Lin, C.-F. et al. [306]\nimplemented a prototype of a cloud-based video recorder\nsystem under glsIaaS while using big data technologies like\nHDFS and Map Reduce. They showed the scalable of video\nrecording, backup, and monitoring features only without\nimplementing any video analytics services. Similarly, Liu,\nX.et al. [307] also came out with a cloud platform for large\nscale video analytics and management. They stated that the\nexisting work failed to design a versatile video management\nplatform in a user-friendly way and to effectively use Hadoop\nto tune the performance of video processing. They success-\nfully develop a cloud platform and the same big data tech-\nnologies, i.e., Hadoop and MapReduce. They also managed to\ndevelop three video processing services, i.e., video summary,\nvideo encoding and decoding, and background subtraction.\nTan, H. et al. [105] used Hadoop and MapReduce for fast\ndistributed video processing and analytics. They developed\ntwo video analytics services, i.e., face recognition and motion\ndetection, by using JavaCV. Furthermore, Ryu, C., et al. [308]\nproposed a cloud video analytics framework using HDFS andMapReduce along with OpenCV [309] and FFmpeg for video\nanalytics. They implemented face recognition and tracking\nalgorithm and reported the scalability of the system and the\naccuracy of the algorithm. Ali M. et al. [310] proposed\nan edge enhanced stream analytics system for video big\ndata called RealEdgeStream. They tried to investigate video\nstream analytics issues by offering \u001cltration and identi\u001cca-\ntion phases to increase the value and to perform analytics\non the streams, respectively. The stages are mapped onto\navailable in-transit and cloud resources using a placement\nalgorithm to satisfy the Quality of Service constraints rec-\nognized by a user. They demonstrate that for a 10K element\ndata streams, with a frame rate of 15-100 per second, the job\ncompletion took 49% less time and saves 99% bandwidth\ncompared to a centralized cloud-only based approach.\nWhite et al. [311] researched MapReduce for IV A\nservices, which comprises classi\u001cer training, clustering,\nsliding windows, bag-of-features, image registration, and\nbackground subtraction. However, experiments were per-\nformed for the k-means clustering and Gaussian back-\nground subtraction only. Tan and Chen [105] presented\nface detection, motion detection, and tracking using\nMapReduce-based clusters on Apache Hadoop. They uti-\nlized JavaCV since Hadoop is developed and designed for\nJava. Pereira, R. el al. [312] proposed a cloud-based dis-\ntributed architecture for video compression based on the\nSplit-Merge technique while using the MapReduce frame-\nwork. They stated that they optimized the Split-Merge\ntechnique against two synchronization problems. The \u001crst\noptimization problem was split and merge video fragments\nwithout loss in synchronization, whereas the second opti-\nmization problem is the synchronization between audio and\nvideo can be greatly affected, since the frame size of each one\nmay not be equal. Similarly, Liu et al. [313] used Hadoop and\nMapReduce for video sharing and transcoding purposes.\nZhang, W. et al. [51] proposed a cloud-based architecture\nfor large scale intelligent video analytics called BiF. BiF\ncombines the merits of RIV A andBIV A while exploiting\ndistributed technologies like storm and MapReduce, respec-\ntively. BiF architecture considered non-functional architec-\ntural properties and constraints, i.e., usability, scalability,\nreliability, fault tolerance, data immutability, re-computation,\nstoring large objects, batch processing capabilities, streaming\ndata access, simplicity and consistency. The BiF architecture\nconsists of four main layers, i.e., data collection layer, batch\nlayer, real-time layer, and serving layer. The data collection\nlayer collects the streaming video frames from the input video\nsources (camera). The data collection layer forwards the\nvideo frames to the batch layer and streaming layer for batch\nprocessing and real-time analytics, respectively. The service\nlayer is to query both batch views and real-time views and\nintegrate them to answer queries from a client. To evaluate\nthe performance of the BiF architecture, they developed a\nvideo analytics algorithm, which was able to detect and count\nfaces for a speci\u001cc interval of time from the input source.\nDuring the evaluation, they showed that BiF is ef\u001ccient in\nVOLUME 8, 2020 152405\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nterms of scalability and fault tolerance. Zhang et al. [319]\nintroduced Apache Kafka and Spark Streaming framework\nfor ef\u001ccient real-time video data processing. They also pro-\nposed a \u001cne-grained online video stream task management\nscheme to boost resource utilization and experimented with\nlicense plate extraction and human density analysis.\nAzher et al. [318] proposed CV AS for RIV A and BIV A\nwhile using Spark Stream and Spark, respectively. They\nimplemented IV A services such as human action recognition\nand face recognition services, respectively. In another work,\nAzher et al. [320] proposed a novel feature descriptor to\nrecognize human action on Spark while utilized the Spark\nMLlib [142] to recognize the action from the feature vector\ngenerated by ALMD [320]. Wang et al. [321] also performed\nhuman action recognition on Spark. The aim was to speed\nup some key processes, including trajectory-based feature\nextraction, Gaussian Mixture Model generation, and Fisher\nVector encoding. Distributed video processing called stream-\ning video engine is also introduced in [322] for distributed\nIV A framework at Facebook scale against three major chal-\nlenges, i.e., low latency, application-oriented \u001dexibility, and\nrobustness to faults and overload.\nZhang et al. [316], [317] stated that the historical video\ndata could be used with the updated video stream to know\nthe current status of an activity, e.g., status of traf\u001cc on the\nroad, and to predict future. To make it possible, they pro-\nposed a video cloud-based service-oriented layered architec-\nture called Depth Awareness Framework and consists of four\nlayers, i.e., data retrieval layer, of\u001dine video analytics layer,\nonline video processing layer, and domain service layer. The\ndata service layer is supposed to hander large-scale video data\nand Webcam Stream. The of\u001dine layer is used to perform\nthe operation on the batch videos, whereas online processing\noccurs in a real-time video processing layer. On the top of\nthe proposed cloud platform, they implemented deep con-\nvolution neural network for obtaining in-depth raw context\ndata inside the big video, and a deep belief network-based\nmethod to predict workload status of different cloud nodes,\nas part of knowledge on a system running status. They pre-\npared a dataset consisting of seven traf\u001cc videos, each of\nsize 2GB. During the evaluation, they stated the improvement\nin object prediction accuracy, fault tolerance, and scalabil-\nity. Zhang et al. [323] performed pedestrian recognition on\nreal-time video data using deep learning. Here, the CNN net-\nwork is improved to \u001cne-CNN, which consists of a nine-layer\nneural network. Moreover, the Apache Storm framework,\nalong with a GPU-based scheduling procedure, is presented.\nB. INDUSTRIAL CVAS\nVarious leading industrial organizations have successfully\ndeployed CV AS. Some of the most popular are brie\u001dy\ndescribed in the following subsections.\n1) GOOGLE VISION\nOn March 8, 2017, at the Google Cloud Next confer-\nence in San Francisco, Google announced the release oftheIV A REST API [327]. The API lets the developer\nrecognize objects in videos automatically and can detect\nand tag scene changes. Furthermore, it enables the users\nto search and discover the unstructured video contents by\nproviding information about entities (20,000 labels). Its\nmain features are label detection, explicit content detection,\nshot change detection, and regionalization [330]. It exploits\ndeep-learning models and is built on the top of the Ten-\nsorFlow framework. The Google IV AAPIs is targeting the\nunstructured video content analytics rather than surveil-\nlance and security. The application domains of the API\ncan be large media organizations that want to build their\nmedia catalogs or \u001cnd easy ways to manage crowd-sourced\ncontent. It can also be helpful for product recommenda-\ntions, medical-image analysis, fraud detection, and many\nmore.\n2) IBM CVAS\nIn April 2017 at the National Association of Broad-\ncasters Show, IBM announced CV AS services [331].\nBlueChasm, [328] development team, came up with a pro-\ntotype app, known as ``VideoRecon,'' that combines IV Avia\nIBM Watson and IBM cloud stack. The IBM CV AS service\ncan extract metadata like keywords, concepts, visual imagery,\ntone, and emotional context from video data. The IBM CV AS\nallows the users to upload video footage to the IBM Clever-\nsafe object storage [332] and subscribe to a service. When\nan object or event of interest is detected, the VideoRecon\nservice creates a tag along with a timestamp of the point\nin the video when either the object was recognized or the\nevent occurred. The tags are then stored in the IBM Cloudant\nfully managed NoSQL JSON document store [333] for future\nuse.\n3) AZURE CVAS\nMicrosoft Azure, a cloud computing service launched\nin 2010, started media services that enable developers to build\nscalable media management and delivery applications [329].\nMedia Services is based on REST APIs that enable the\nusers to securely manage video or audio content for both\non-demand and live streaming delivery to clients. Recently,\nthey provide CV AS APIs to the customers for (R/B)IV A (as\nshown in Table 10).\n4) CITILOG CVAS\nCitilog [324], also known as CT-Cloud, provides intelligent\nvideo analytics and surveillance solutions in the domain\nof transportation. Citilog provides services like automatic\nincident detection, traf\u001cc data collection (vehicle counting,\nclassi\u001ccation, average speed, occupancy and levels of ser-\nvice), interaction control, video management, and license\nplate recognition. The Citilog is an open platform, providing\nAPIs, widgets for quick development of services. According\nto the Citilog, they process approximately 32000 hours of\nvideo data and detects about \u001cve incidents per minute from\n900 sites worldwide.\n152406 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nTABLE 10. Industrial CVAS.\n5) CHECK-VIDEO CVAS\nCheck-Video [325], founded in 1998, is one of the initiators in\nthe domain of IV A. The company offers cameras, recorders,\nand a cloud video management solution for security deal-\ners, integrators, and end-users. Initially, they used to pro-\nvide OV AS, but with the advancement of cloud technology,\nCheckVideo launched CV AS. They provide domain-speci\u001cc\nintelligent video analytics RIV A solution. The main features\nCheck-Video are RIV A, a video search engine, cloud video\nstorage, and an alert system. The provided services can be\ncategorized as basic analytics, object classi\u001ccation, and busi-\nness analytics (see table 10). According to the company's,\nthey have successfully analyzed 108,458,000 hours of video\nand detected 61,233,000 events per month.\n6) INTELLI-VISION CVAS\nIntelli-Vision [326], founded in 2002, is a leading and notable\ncompany in the \u001celd of Arti\u001ccial Intelligence (AI) and deep\nlearning-based video analytics and video cloud platform.\nThey are exploiting state-of-the-art technology in the area\nofAIfor security and monitoring purposes while targeting\nmultiple business domains, including home, retail, trans-\nportation, and advanced driver assistance systems for cars.\nIntelli-Vision's analytics adds the ``Brains Behind the Eyes''\nfor cameras by analyzing the video content, extracting meta-\ndata, sending out real-time alerts, and providing intelligence\non the video. Currently, they are providing a wide range of\nvideo analytics services in the domains mentioned above,\nranging from object left to night vision and enhancements\n(see table 10). In Feb 2018, in a press release, the Intelli-\nVision stated that they have successfully deployed four mil-\nlion cameras worldwide, which have been subscribed to\nvarious IV Aservices.VII. IVA APPLICATIONS\nIV Aat scale drives many application domains ranging from\nsecurity and surveillance to self-driving and healthcare. Many\napplication areas of video big data analytics are shown, which\nproject the signi\u001ccant role of big data and cloud computing\ninIV A.\nA. TRAFFIC AND TRANSPORTATION\nIV Ahas been extensively used in traf\u001cc control and trans-\nportation, e.g., lane traf\u001cc counts, incident detection, ille-\ngal u-turn, and many more. One of the main reason for\ndeaths and injuries are traf\u001cc-related misfortunes [334].\nProactive analytics is required to predict abnormal events\nso that to minimize or avoid such accidents. In this direc-\ntion, VisonZero [335] has been developed and deployed suc-\ncessfully. In transportation, another application is vehicle\ntracking where chasing of a license plate, overspeeding, and\ncollision cause analysis can be obtained by analyzing video\ndata. Kestrel [336] is a vehicle tracking system and uses\ninformation from various non-overlapping cameras to detect\nvehicle path. Gao et al. [337] used an automatic particle\n\u001cltering algorithm to track the vehicle and monitor its illegal\nlane changes. Chen et al. citechen2009machine used hidden\nMarkov models to determine the traf\u001cc density state proba-\nbilistically. Incident detection framework based on generative\nadversarial networks were proposed in [338].\nB. INTELLIGENT VEHICLE AND SELF-DRIVING CARS\nCurrently, the term self-driving cars mean that the vehicle\nexploits computer vision for safe and intelligent driving while\nassisting the driver. In an intelligent vehicle, different sensors\nand high de\u001cnition cameras (cameras for vehicle cabin, for-\nward roadway, and the instrument cluster) are integrated with\nVOLUME 8, 2020 152407\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nthe vehicle, which generates multi-model data, and the same\nis sent to the cloud for real-time analytics [339], [340]. In this\ncontext, video analytics is vital with optimum algorithm\naccuracy. Researchers have developed several algorithms,\nincluding pedestrian detection, traf\u001cc light detection, and\nother driver assistance system. For example, Wang et al. [341]\nproposed a method for pedestrian detection in urban traf\u001cc\nconditions using a multilayer laser sensor mounted onboard\na vehicle. An algorithm was proposed by Tsai et al. [342] to\ndetect three condition changes: missing, tilted, and blocked\nsigns, using GPS data, and video log images. An innovative\nCNN-based visual processing model is proposed in [343] to\nautomatically detect traf\u001cc signs and dramatically reduces\nthe sign inventory workload. Driver decision making was\nimproved in taking the right turn in left-hand traf\u001cc at a sig-\nnalized intersection utilizing simulation [344]. They used an\nin-car video assistant system to present the driver's occluded\nview when the driver's view is occluded by truck. An effort\nfor driver body tracking and activity analysis, posture recog-\nnition, and action predication, have been studied in [345],\n[346] respectively.\nC. HealthCare\nRecently, video big data analytics is reshaping the health-\ncare industry and yet another vital application area that\ndemands special focus while deploying video analytics.\nSurveillance video streams can help understand the tracked\nperson's behavior, such as monitoring elderly citizens or\nblind people against fall detection or detecting any possible\nthreat. Fleck and Strasser reported a prototype 24/7 system\ninstalled in a home for assisted living for several months\nand shows quite promising performance [347]. Zhou et al.\nstudied how video analytics can be used in eldercare to\nassist the independent living of elders and improve the ef\u001c-\nciency of eldercare practice [348]. Some more studies were\ndone [349]\u0015[352] to analyze activities, recognize posture, and\nto detect falls or other substantial events. A smart gym can\nexploit the video cam stream to determine frequently used\nequipment, the duration of exercise, and time spent on a\npiece of particular equipment, which is useful for real-time\nassessment.\nD. SMART CITY SECURITY (IoT) AND SURVEILLANCE\nIn many organizations ranging from large enterprises to\nschools, home and law enforcement agencies where security\nis becoming an essential concern and is turning the security\ncenters to video analytics to keep their premises safe. Law\nenforcement agents can use a body-worn camera to identify\ncriminals in real-time while transmitting the video stream to\nthe video analytic cloud. IV AaaS can provide customized ser-\nvices that can adjust quickly to changing needs and demands.\nPeople detection, and tracking [353], motion detection, intru-\nsion detection [354], line crossing [355], object lift, loitering\n[356], and license plate recognition [357] are the example of\nvideo analytics services for security. For security in subway\nstations, Krausz [358] developed as a surveillance systemto detect dangerous events. Shih et al. [359] tried to extract\nthe color features of an employee's uniform to recognize the\nentry legality in a restricted area. In the context of security,\nabandoned object detection is indispensable and can lead to a\nterrorist attack. In the future, video analytics applications are\ndeveloping fast, and they are changing the way the security\nindustry works.\nE. AUGMENTED REALITY AND PERSONAL DIGITAL\nASSISTANCE\nAmong the many, one aspect of the augmented reality is\nvisual, where devices like special glasses, helmets, or goggles\nare utilized for the projection of additional information or\ninteractive experience of the surrounding real-world environ-\nment. The visual aspect of augmented reality may encom-\npass complex IV Aand demand powerful hardware. Likewise,\nvision-based digital assistants is a rising technology (e.g.,\npersonal robot Jibo) that could deeply alter our regular activ-\nities while offering personalized and interactive experiences.\nSuch devices could be of\u001doaded to the CV AS for low latency\ncomplex IV Auninterruptedly.\nF. RETAIL, MANAGEMENT, AND BUSINESS INTELLIGENCE\nANALYSIS\nLarge-scale\nproducts, services, and staff management while adjusting to\nconsumer demands can be challenging without timely and\nup-to-date information. Smart Retail solution takes advan-\ntage of smart cameras combined with IV A to gather data\non store operations and customer trends. Dwell analysis,\nface recognition, queue management, customer count, cus-\ntomer matrics, consumer traf\u001cc map, are some of the exam-\nple services in this context [360]. Gaze analysis provides a\nmeans to learn customers' interest in merchandise by follow-\ning their attention [361], [362] on a store display. Actions\nlike reaching or grabbing products were analyzed by [363],\n[364] to understand customers' interest. Emotion analysis\ncan identify customers' views regarding product and inter-\naction with the company's representative [334]. IV A can\nalso be used for business intelligence analysis to answer\nqueries like ``number of people visited per unite time?''\nor ``customer interest in items?'' while utilizing the same\nsecurity infrastructure. Such information is bene\u001ccial for\nretailers in improving customer experience and marketing\nstrategies.\nVIII. RESEARCH ISSUES, OPPORTUNITIES, AND FUTURE\nDIRECTIONS\nIntelligent video big data analytics in the cloud opens new\nresearch avenues, challenges, and opportunities. This section\nprovides in-depth detail about such research challenges,\nwhich has been summarized in Table 11).\nA. IVA ON VIDEO BIG DATA\nBig data analytics engines are the general-purpose engine\nand are not mainly designed for big video analytics. Con-\n152408 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nTABLE 11. Open research issues in video big data analytics in the cloud.\nsequently, video big data analytics is challenging over such\nengines and demand optimization. Almost all the engines\nare inherently lacking the support of elementary video data\nstructures and processing operations. Further, such engines\nare also not optimized, especially for iterative IV A and depen-\ndency among processes.\nOptimizing cluster resource allocations among multiple\nworkloads of iterative algorithms often involves an approx-\nimation of their runtime, i.e., predicting the number of\niterations and the processing time of each iteration [365].\nBy default, Hadoop lacks iterative job support but can be han-\ndled through speculative execution. However, Spark supports\nnot only MapReduce and fault tolerance but also cache data\nin memory between iterations. IV Aon video big data creates\nan immense space for the research community to further\ncrack in this direction. The research community is already\ntrying to develop basic video processing and IV A support\nover big data, but it is still the beginning. How to optimize\nsuch engines for iterative IV A? It also allows us to research\nwhether the exiting distributed computing engines ful\u001cll the\ndemands of the IV Aon video big data or need a specialized\none.\nFurthermore, the focus of the existing research on IV A\nare velocity, volume, velocity, but the veracity and value\nhave been overlooked. One promising direction in addressing\nvideo big data veracity is to research methods and techniques\ncapable of accessing the credibility of video data sources so\nthat untrustworthy video data can be \u001cltered. Another way\nis to come up with novel MLmodels that can make infer-\nences with defective video data. Likewise, users' assistance\nis required to comprehend IV Aresults and the reason behind\nthe decision to realize the value of video big data in decision\nsupport. Thus, understandable IV Acan be a signi\u001ccant future\nresearch area.B. IVA AND HUMAN-MACHINE COORDINATION\nIV Aon video big data grants a remarkable opportunity for\nlearning with human-machine coordination for numerous\nreasonsV\n\u000fIV A on video big data in cloud demands researchers\nand practitioners mastering both IV A and distributed\ncomputing technologies. Bridging both the worlds for\nmost analysts is challenging. Especially in an educa-\ntional environment, where the researcher focuses more\non the understanding, con\u001cguration, and tons of param-\neters rather than innovation and research contribution.\nThus there is a growing need to design such CV AS that\nprovide high-level abstractions to hide the underlying\ncomplexity.\n\u000fIVS service to become commercially worthwhile and to\nachieve pervasive recognition, consumer lacking tech-\nnical IV A knowledge. The consumers should be able\nto con\u001cgure, subscribe, and maintain IV Aservices with\ncomfort.\n\u000fIn traditional IV A, consumers are usually passive. Fur-\nther, research is required to build more interactive IV A\nservices that assist consumers in gaining insight into\nvideo big data. An ef\u001ccient interactive IV A service\ndepends on the design of innovative interfacing prac-\ntices based on an understanding of consumer abilities,\nbehaviors, and requirements [366]. The interactive IV A\nservices will learn from the consumer and decrease the\nneed for administration by a specialist. It will also enable\nconsumers to design custom IV Aservices to meet the\ndomain-speci\u001cc requirement.\nC. ORCHESTRATION AND OPTIMIZATION OF IVA PIPELINE\nL-CV AS is a service-oriented architecture, i.e., (R/B)IV AaaS.\nThe real-time and batch work\u001dow are deeply dependent on\nVOLUME 8, 2020 152409\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nthe messaging middleware (Table 2) and distributed process-\ning engines (Table 7). In L-CV AS, the dynamic (R/B)IV A\nservice creation and multi-subscription environment demand\nthe optimization and orchestration of the IV Aservice pipeline\nwhile guarantees opportunities for further research.\nIn literature, two types of scheduling techniques have\nbeen presented for real-time scheduling, i.e., static and\ndynamic [367], [368]. Static approaches are advantageous if\nthe number of services and subscription sources is known\npriorly, but this is not the case with L-CV AS. The suitabil-\nity of dynamic methods is reasonable but is expensive in\nterms of resource utilization. Likewise, the main issue in the\nBIV A service work\u001dow on video big data is the data parti-\ntioning, scheduling, executing, and then integrate numerous\npredictions. The BIV A service work\u001dow can be affected by\nthe data \u001dow feature of the underlying big data engine (as\nshown in Table 7). As Hadoop map-reduce lacking the loops\nor chain of stages, and Spark support DAG style of chain-\ning, whereas Flink supports a Controlled Cyclic Dependency\nGraph (CCDG).\nIn the map-reduce infrastructure, a slowdown predictor can\nbe utilized to improve the agility and timeliness of scheduling\ndecisions [369]. Spark and Flink can accumulate a sequence\nof algorithms into a single pipeline but need research to exam-\nine its behavior in dynamic service creation and subscrip-\ntion environment. Further, concepts from the \u001celd of query\nand queuing optimization can be utilized while considering\nmessaging middleware and distributed processing engines to\norchestrate and optimization of IV Aservice Pipeline.\nD. IVA AND BIG DIMENSIONALITY\nThe VSDS multi-modality can produce diverse types\nof data streams. Similarly, the IV A algorithm, devel-\noper, and IRhas a triangular relationship. An array\nof algorithms can be deployed, generating varied sorts\nof multi-dimensional features from the acquired data\nstreams. The high-dimensionality factor poses many intrin-\nsic challenges for data stream acquisition, transmission,\nlearner, pattern recognition problems, indexing, and retrieval.\nIn literature, it has been referred to as a ``Big Dimensionality''\nchallenge [370].\nVSDS variety leads to key challenges in acquiring and\neffectively processing the heterogeneous data. Most existing\nIV Aapproaches can consider a speci\u001cc input, but in many\ncases, for a single IV Agoal, different kinds, and formats can\nbe considered.\nWith growing features dimensionality, current algorithms\nquickly become computationally in\u001dexible and, therefore,\ninapplicable in many real-time applications [371]. Dimension\nreduction approaches are still going to be a hot research topic\nbecause of data diversity, increasing volume, and complexity.\nEffect-learning algorithms for \u001crst-order optimization, online\nlearning, and paralleling computing will be more preferred.\nSimilarly, designing a generic, ef\u001ccient, and scalable\nmulti-level distributed data model for indexing and retrieving\nmulti-dimensional features is becoming tougher than everbecause of the exponential growth and speed of video data.\nConsidering varied situations, requirements, and parame-\nters such as complex data type indexing (such as objects,\nwhich contains multiple types of data), multi-dimensional\nfeatures (require different feature matching scheme for each\ntype), cross scheme matching (e.g., spatiotemporal, spatial-\nobject, object-temporal, etc.), giant search space, incremental\nupdates, on the \u001dy indexing, and concurrent query processing\ndemands further investigation. It gives the research commu-\nnity the opportunities to optimize existing hashing schemes\nand indexing structures such as R-trees [372], M-tree [373],\nX-tree [374], locality-sensitive hashing [375] etc. on the top\nof big data engines. Log-structured merge-tree [376] based\ndistributed data stores (see Table 4) can be leveraged to\nimprove multi-dimensional query performances. Addition-\nally, MLclassi\u001ccation models can be used to capture the\nsemantics by inspecting the association between features and\nthe context among them to better index multi-dimensional\ndata. Such methods make them more precise and effective\nthan the non-ML methods [377]. Thus, MLclassi\u001ccation\nmodels (including neural networks) can be used to capture the\nsemantics by inspecting the association between features and\nthe context among them to better index multi-dimensional\ndata, that make them more precise and effective than the\ntraditional indexing approaches [377].\nE. ONLINE LEARNING ON VIDEO BIG DATA\nThe value of RIV A is dependent on the velocity of the video\nstreams, i.e., newness and relatedness to ongoing happenings.\nThough existing big data de-facto standards are lacking to\ndeal with the changing streams [378]. The RIV A services\nmust address continuous and changing video streams. In this\ncontext, online learning can be utilized, representing a group\nof learning algorithms for constructing a predictive model\nincrementally from a sequence of data, e.g., Fourier Online\nGradient Descent and Nystrom Online Gradient Descent\nalgorithms [379]. In this context, Nallaperuma et al. [380]\nproposed ITSplatform utilizing unsupervised online learn-\ning and deep learning approaches. It gives further research\nopportunities by involving data fusion from heterogeneous\ndata sources [380].\nF. MODEL MANAGEMENT\nL-CV AS architecture is designed to deploy an array of\nIV A algorithms, i.e., both by administrator and devel-\nopers. An algorithm might hold a list of parame-\nters. The model selection process encompasses feature\nengineering (feature selection IV-B1), IV A algorithm\nselection, and hyperparameter tuning. Feature engineering is\na laborious activity and is in\u001duenced by many key factors,\ne.g., domain-speci\u001cc regulations, time, accuracy, video data,\nandIV Aproperties, which resultantly slow and hinder explo-\nration. IV Aalgorithm selection is the process of choosing a\nmodel that \u001cxes the hypothesis space of prediction function\nexplored for a given application [381]. This process of IV A\nalgorithm selection is reliant on technical and non-technical\n152410 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\naspects, which enforce the IV A developer to try manifold\ntechniques at the cost of time and cloud resources. Hyper-\nparameter is vital as they govern the trade-offs between\naccuracy and performance. IV Aanalysts usually do ad-hoc\nmanual tuning by iteratively choosing a set of values or using\nheuristics such as grid search [381]. From IV A analysts'\nperspective, model selection is an expensive job in terms of\ntime and resources that bringing down the video analytics\nlifecycle [382]. Model selection is an iterative and investiga-\ntive process that generally creates an endless space, and it is\nchallenging for IV Aanalysts to know a priori which com-\nbination will produce acceptable accuracy/insights. In this\ndirection, theoretical design trade-offs are presented by\nArun et al. [383], but further research is required that how\nto shape a uni\u001ced framework that acts as a foundation for a\nnovel class of IV Aanalytics while building the procedure of\nmodel selection easier and quicker.\nG. PARAMETER SERVERS AND DISTRIBUTED LEARNING\nDeveloping a model (such as Stochastic Gradient Descent)\nfor video big data analytics in a distributed environment car-\nries an intrinsic issue of sharing and updating high-dimension\nparameters that can easily run into orders billions to trillions.\nThe Parameter Server notion has been introduced to address\nthis issue, aiming to store the parameters of an MLmodel\nsuch as the weights of a neural network and serve them\nto clients. Parameter Server proposes a new framework for\nbuilding distributed MLalgorithms, and encompass diverse\ndesign goals, e.g., ef\u001ccient communication, \u001dexible consis-\ntency, elasticity when adding resources, resource utilization,\nand ease of use. In literature, recently, various studies tried\nto optimize Parameter Server. PS2 [384] builds the param-\neter server on top of Spark. SketchML [385] compresses\nthe gradient values by a sketch-based method. FlexPS [386]\nintroduces a multi-stage abstraction to support \u001dexible par-\nallel control. The Parameter Server can be optimized further\nagainst the stated design goals and need further investigation.\nH. EVALUATION ISSUES AND OPPORTUNITIES\nHaralick [387] initiated the discussion of IV Aperformance\nevaluation followed by dedicated workshops [388], and jour-\nnals [389], [390]. As a result, performance evaluation tools\n(ViPER5), and datasets (ETISEO [391], TrecVID [392], i-\nLIDS [393]) were introduces. It is a fact that traditional IV A\nhas an established set of prediction accuracy based metrics\nfor performance evaluation that is ranging from accuracy,\nerror rate, and precision to optimization and estimation error.\nSome more evaluation parameters are adopted from big data\nanalytics when IV Ais tried on distributed computing, e.g.,\nscalability, fault tolerance, memory usage, throughput, etc.\n[394] (as shown in Table 9. The amalgamation of two types\nof matrices might not be enough.\nIV A services provided by a system like L-CV AS has to\naccomplish predictably through an intractable number of\nscenarios and environmental circumstances, meeting require-\nments that vary according to the situation, domain, and user.ForL-CV AS's consumer, the IV Aservices work as a black\nbox where the signi\u001ccant metrics relate to overall system\nperformance, such as false alarms, accuracy, and detection\nrate. However, from the developer and researcher perspective,\nL-CV AS consists of numerous computer vision algorithms,\nwith complex interfaces among them. A proper performance\nevaluation matrix is required for IV Aservice developers to\ncomprehend these relations and to revolutionize and address\nnovel IV Aservices. Another critical issue is how to guaran-\ntee accurate and predictable IV Aservice performance when\nporting technology between distributed algorithm develop-\nment environments and deployment code environments with\nhardware-speci\u001cc optimizations. These shortcomings result\nin algorithmic alterations that can in\u001duence the performance\nofIV Aservices.\nIV Aperformance evaluation is goal-oriented, and the fac-\ntors should be determined carefully. Many key factors that\nin\u001duence the performance of video big data analytics uti-\nlizing distributed computing engines in the cloud are listed\nbelow (not limited to).\n\u000fVSDS: holds diverse types of parameters, i.e., video type\n(color, grey-scale, infrared, omnidirectional, depth map,\netc.), property (frame-rate, \u001celd-depth), and quality (res-\nolution, pixel depth) of the generated video as generated\nby the camera.\n\u000fVSDSand messaging middleware parameters: encom-\npass VSDS connection, frames reducing and transfor-\nmation, messaging queue (broker server), compression\nartifacts, mini-batch size, and possibly the involvement\nof internal and external network.\n\u000fVSDSenvironmental parameters: some features of the\ncon\u001cguration remain constant in a given use case, but\ndiffer between con\u001cguration, possibly in\u001duencing per-\nformance. These parameters comprise camera location\n(mounting height, angle, indoor or outdoor), mounting\ntype (still or in motion), camera view (roads, water,\nfoliage), and weather (sun, cloud, rain, snow, fog, wind).\n\u000fDistributed processing environment: the video process-\ning hardware (FPGA, CPU, GPU, etc.), and network\ncommunication channels potentially impose additional\nlimitations in terms of locality, speed, and memory.\n\u000fBig data analytics engines: The nature and characteris-\ntics of big data engines affect the performance of the\nIV A, such as data \u001dow, windowing, computation model,\netc. Further, big video analytics in the cloud demand\ncomplex trade-offs between different evaluation criteria.\nIn order to comprehend, assume the intricate trade-offs\nbetween accuracy and response time. Iterative tasks have\nan inverse relation with fault tolerance concerning scala-\nbility (e.g., MapReduce is high fault-tolerant but lacking\niteration). Similarly, non-iterative IV Aalgorithms scale\nbetter than iterative at the cost of performance degrada-\ntion.\n\u000fIV A service parameters: Application parameters are\ndomain-speci\u001cc parameters, e.g., vehicles, carts,\nVOLUME 8, 2020 152411\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nhumans, etc., tolerable miss detection and false alarm\nrates and their desired trade-off, IV A type, and max\nacceptable latency.\n\u000fComputation and communication trade-off: IV AAlgo-\nrithms and services in the distributed environment\nshould be developed and designed wisely intending to\nminimize computation time, which is associated with\ndata locality and loading.\nDiverse types of factors affect video big data analytics\nperformance in the cloud, constructing a comprehensive eval-\nuation of all use cases are almost near impossible. It fur-\nther provides opportunities for the researchers to design a\nframework that provides a uni\u001ced and generic framework\nthat can be adapted by any CV AS. Investigating these issues\nwould signi\u001ccantly contribute to the academic and industrial\ncommunities interested in building IV Aalgorithms, services,\nandCV AS.\nI. IVA ALGORITHM, MODEL, AND SERVICES STATISTICS\nMAINTENANCE, RANKING, AND RECOMMENDATION\nL-CV AS architecture is designed under the Customer-to-\nCustomer (C2C) business model. In L-CV AS, a user can\ndevelop and deploy an IV A algorithm, model, or service\n(here collectively we call it IV Aservice) that can be either\nextended, utilized, or subscribed by other users. The commu-\nnity members run such architecture, and rapidly, the number\nofIV Aservices can be reached to tons of domain-dependent\nor independent IV Aservices. This scenario develops a com-\nplex situation for the users, i.e., which IV A service (when\nsharing the parallel functionalities) in a speci\u001cc situation,\nespecially during service discover. Against each IV Aservice,\nthere is a list of Quality of Service (QoS) parameters. Some\nof these QoS parameters (not limited to) are user trust, sat-\nisfaction, domain relevance, security, usability, availability,\nreliability, documentation, latency, response time, resource\nutilization, accuracy, and precision.\nSuch types of IV A services against the QoS parame-\nters lead to the 0-1 knapsack issue. In this direction, one\npossible solution is utilizing multi-criteria decision-making\napproaches. It gives further opportunities to the research\ncommunity to investigate how to rank and recommend IV A\nalgorithms, models, and services. Similarly, it can lead to\na high-dimensional sparse matric [395]. In this direction,\nresearch is required on how to utilize such parameters for IV A\nservices recommendation.\nJ. IVAaaS AND COST MODEL\nRecently, cloud-based analytics platforms are the key means\nfor enterprises to provide services on the pay-as-you-go cost\nmodel. Existing cost metrics usually are determined to utilize\nhardware usage comprising processing (CPU, GPU), disk\nspace, and memory usage. These prices are often static or\ndynamic [396]. The example of the former one is Amazon's\nEC2, which offers tiered levels of service. In later cases,\nthe cost model is used to determine the price of the serviceusing analytics. This takes into account factors such as peak\nhours and opponent cost model etc. The hardware cost is\nusually minimal compared to the cost of software such as\nL-CV AS where the cost of IV Aanalytics valued more.\nL-CV AS is supposed to provide IV A-Algorithm-as-a-\nService (IV AAaaS) andIV AaaS in the cloud while adopting\ntheC2C business model. Unfortunately, current SaaS cost\nmodels might not be applicable because of the involvement\nof diverse types of parameters that drastically affect the cost\nmodel. Such parameters are, business model (Business-to-\nBusiness (B2B), Business-to-Customer (B2C), and C2C),\nunite of video, user type (developer, researcher, and con-\nsumers), services (IV AAaaS andIV AaaS), service subscrip-\ntion (algorithm, IV Aservice, single, multiple, dependent or\nindependent), cloud resource utilization, user satisfaction,\nQoS, location, service subscription duration, and cost model\nfairness. The addition of further parameters is subject to dis-\ncussion, but the listed are the basic that govern L-CV AS cost\nmatrix. Additionally, the cost model demands further research\nand investigations to develop an effective price scheme for\nIV Aservices while considering the stated parameters.\nK. VIDEO BIG DATA MANAGEMENT\nDespite video big data pose high value, but its management,\nindexing, retrieval, and mining are challenging because of\nits volume, velocity, and unstructuredness. IV A has been\ninvestigated over the years (section V) but still evolving and\nneed to address diverse types of issues such as:\n\u000fIn the context of video big data management, the main\nissue is the extraction of semantic concepts from primi-\ntive features. A general domain-independent framework\nis required that can extract semantic features, analyze\nand model the multiple semantics from the videos by\nusing the primitive features. Further, semantic event\ndetection is still an open research issue because of the\nsemantic gap and the dif\u001cculty of modeling temporal\nand multi-modality features of video streams. The tem-\nporal information is signi\u001ccant in the video big data\nmining mainly, in pattern recognition.\n\u000fMotion analysis is vital, and further research is required\nfor moving objects analysis, i.e., object tracking, han-\ndling occlusion, and moving objects with statics cam-\neras [213], moving cameras [215], and multiple camera\nfusion.\n\u000fLimited research is available on CBVR (see\nsection V-A) while exploiting distributed computing.\nFurther study is required to consider different features\nranging from local to global spatiotemporal features\nutilizing and optimizing deep learning and distributed\ncomputing engines.\n\u000fFor video retrieval, semantic-based approaches have\nbeen utilized because of the semantic gap between the\nlow-level features and high-level human-understandable\nconcepts. Ontology adds extra concepts that can improve\nthe retrieval results [182] but can also lead to unexpected\n152412 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\ndeterioration of search results. In this context, a hybrid\napproach can be fruitful and need to design different\nquery planes that can ful\u001cll diverse queries in complex\nsituations.\n\u000fInsuf\u001ccient research is available on graph-based video\nbig data retrieval and analysis, opening doors for further\ninvestigation. The researchers can conduct studies to\nanswer questions like: the formation of the video graph,\ntuning the similar value and its effect on the graph\nformation, studying properties, and meta-analytic of the\nformed video graph.\n\u000fHow the reinforcement learning and real-time feedback\nquery expansion technique [397] can be exploited to\nimprove the retrieval results?\n\u000fRecently, video query engines have been introduced to\nretrieve and analyze video at scale [398]\u0015[400]. A spe-\ncial focus of the database community is required to\ndesign, implement, optimize, and operationalize such\nvideo query engines.\nL. PRIVACY, SECURITY AND TRUST\nVideo big data, acquisition, storage, and subscriptions to\nshared IV A in the cloud become mandatory, which leads to\nprivacy concerns. For the success of such platforms, privacy,\nsecurity, and trust are always central. In literature, the word\n`trust' is commonly used as a general term for `security' and\n`privacy' [401]. Trust is a social phenomenon where the user\nhas expectations from the IV Aservice provider and willing\nto take action (subscription) on the belief based on evidence\nthat the expected behavior occurs [402], [403]. In the cloud\nenvironment, security and privacy are playing an active role\nin the trust-building. To ensure security, the CV AS should\noffer different levels of privacy control. The phenomena of\nprivacy and security are valid across VSDS, storage secu-\nrity, multi-level access controls, and privacy-aware IV Aand\nanalysis. We list some research directions that can provide\nopportunities for cloud security specialists.\n\u000fThe video big data volume, variety, and velocity boost\nsecurity threats. Recently, disputes and news were cir-\nculating regarding the misuse of user-generated content\nand hacking the cameras in an unauthorized manner. The\nCV AS vendor, in collaboration with or under the law\nenforcement agencies, must come up with new rules,\nlaws, and agreements, which can differ from country\nto country. Utilizing such policies, the CV AS vendor\nshould ensure that all the IV Aservice, subscription, and\nstorage level agreements are adequately followed. The\npolicies can be researched whether it offers adequate\nprotection for individuals' data while performing video\nbig data analytics and public monitoring.\n\u000fUnlike other data, videos are more valuable for the\nowner and can be a direct threat, e.g., live broadcast,\nblackmailing, etc. Likewise, the focus of traditional\nprivacy approaches is data management that becomes\nabsolute when it comes to data security. Novel algo-rithms are required to secure user's data both for shared\nIV A, storage to make the video stream acquisition more\nsecure.\n\u000fIn the context of security, the blockchain (popularized\nby Bitcoin) [404] has been studied and operationalized\nacross academia and industry evenly. A blockchain is\na modi\u001ccation resilient cryptography technique known\nas a distributed ledger, where records are linked and\nmanaged by a decentralized peer-to-peer network [404].\nThe blockchains techniques are still in early-stage and\ncan be researched further to form a novel automated\nsecurity system for CV AS.\n\u000fMLtechniques have been matured over the years and\nhave been successfully utilized for security, i.e., model-\ning attack patterns with their distinctive features. How-\never, change in features in case of sophisticated attacks\nmay lead to security failure. MLcould enhance the per-\nformance of security solutions to alleviate the dangers of\nthe existing cyberattacks. The research community can\nfurther investigate how MLtechniques, especially deep\nlearning, can be deployed to analyze logs produced by\nnetwork traf\u001cc, IV Aprocesses, and users to recognize\ndoubtful activities.\nIX. CONCLUSION\nIn the recent past, the number of public surveillance cameras\nhas increased signi\u001ccantly, and an enormous amount of visual\ndata is produced at an alarming rate. Such large-scale video\ndata pose the characteristics of big data. Video big data offer\nopportunities to the video surveillance industry and permits\nthem to gain insights in almost real-time. The deployment\nof big data technologies such as Hadoop, Spark, etc., in the\ncloud under aaSparadigm to acquire, persist, process and\nanalyze a large amount of data has been in service from last\nfew years. This approach has changed the context of infor-\nmation technology and has turned the on-demand service\nmodel's assurances into reality.\nThis paper provides an extensive study on intelligent video\nbig data in the cloud. First, we de\u001cne basic terminologies\nand establish the relation between video big data analytics\nand cloud computing. A comprehensive layered architecture\nhas been proposed for intelligent video big data analytics in\nthe cloud under the aaSmodel called L-CV AS. VBDCL is\nthe base layer that allows the other layers to develop IV A\nalgorithms and services. This layer is based on the concept of\nIRorchestration and takes care of data curations throughout\nthe life cycle of an IV Aservice. The VBDPL is in-charge of\npre-processing and extracting the signi\u001ccant features from\nthe raw videos. The VBDML is accountable for producing\nthe high-level semantic result from the features generated\nby the VBDML. The KCL deploys video ontology and cre-\nates knowledge based on the extracted higher-level features\nobtained from VBDML. When all these layers are pipelined\nin a speci\u001cc context, it becomes an IV Aservice to which the\nusers can subscribe to video data sources under the IV AAaaS\nparadigm.\nVOLUME 8, 2020 152413\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\nFurthermore, to show the signi\u001ccance and recent research\ntrends of IV A in the cloud, a broad literature review has been\nconducted. The research issues, opportunity, and challenges\nbeing raised by the uniqueness of the proposed L-CV AS,\nand the triangular relation among video big data analyt-\nics, distributed computing technologies, and cloud has been\nreported.\nREFERENCES\n[1] M INC. (2012). Video Analytics Market. Accessed: Dec. 20, 2019.\n[Online]. Available: https://github.com/apache/rocketmq\n[2] Facebook.Com. (2020). Facebook. Accessed: Dec. 20, 2019. [Online].\nAvailable: https://www.facebook.com\n[3] Youtube.com. (2019). Youtube Statistics. Accessed: Dec. 20, 2019.\n[Online]. Available: https://www.youtube.com/about/press/\n[4] Net\u001dix.Com. (2020). Net\u001dix. Accessed: Dec. 20, 2019. [Online]. Avail-\nable: https://www.net\u001dix.com/\n[5] S. Pouyanfar, Y. Yang, S.-C. Chen, M.-L. Shyu, and S. S. Iyengar,\n``Multimedia big data analytics: A survey,'' ACM Comput. Surv., vol. 51,\nno. 1, p. 10, Jan. 2018, doi: 10.1145/3150226.\n[6] I. E. Olatunji and C.-H. Cheng, ``Dynamic threshold for resource tracking\nin observed scenes,'' in Proc. 9th Int. Conf. Inf., Intell., Syst. Appl. (IISA),\n2018, pp. 1\u00156.\n[7] IDC Corporation. (2019). The Growth in Connected IoT Devices.\nAccessed: Dec. 7, 2019. [Online]. Available: https://www.idc.com/\ngetdoc.jsp?containerId=prUS45213219\n[8] T. Huang, ``Surveillance video: The biggest big data,'' Comput. Now,\nvol. 7, no. 2, pp. 82\u001591, 2014.\n[9] P. Mell and T. Grance, ``The NIST de\u001cnition of cloud computing,'' Nat.\nInst. Standards Technol., Gaithersburg, MD, USA, Tech. Rep. SP 800-\n145, 2011.\n[10] P. Zikopoulos, D. Deroos, K. Parasuraman, T. Deutsch, J. Giles, and\nD. Corrigan, Harness the Power of Big Data The IBM Big Data Platform.\nNew York, NY, USA: McGraw-Hill, 2012.\n[11] EWS Amazon. (Nov. 2012). Amazon Web Services. [Online]. Available:\nhttp://aws.amazon.com/es/ec2/\n[12] M. Copeland, J. Soh, A. Puca, M. Manning, and D. Gollob, ``Overview\nof Microsoft azure services,'' in Microsoft Azure. Berkeley, CA, USA:\nSpringer, 2015, pp. 27\u001569.\n[13] J.-P. Dijcks, ``Oracle: Big data for the enterprise,'' Oracle, Redwood\nCity, CA, USA, Oracle White Paper, 2012, p. 16. [Online]. Available:\nhttp://www.oracle.com/us/products/database/big-data-for-enterprise-\n519135.pdf\n[14] J. R. Mashey, ``Big data and the next wave of infras-tress,'' in Computer\nScience Division Seminar. Berkeley, CA, USA: Univ. of California, 1997.\n[15] C. Snijders, U. Matzat, and U.-D. Reips, ```Big data': Big gaps of knowl-\nedge in the \u001celd of Internet science,'' Int. J. Internet Sci., vol. 7, no. 1,\npp. 1\u00155, 2012.\n[16] I. A. T. Hashem, I. Yaqoob, N. B. Anuar, S. Mokhtar, A. Gani, and S.\nU. Khan, ``The rise of `big data' on cloud computing: Review and open\nresearch issues,'' Inf. Syst., vol. 47, pp. 98\u0015115, Jan. 2015.\n[17] N. Elgendy and A. Elragal, ``Big data analytics: A literature review\npaper,'' in Proc. Ind. Conf. Data Mining. Cham, Switzerland: Springer,\n2014, pp. 214\u0015227.\n[18] H. zkse, E. S. Ar\u0019, and C. Gencer, ``Yesterday, today and tomor-\nrow of big data,'' Procedia-Social Behav. Sci., vol. 195, pp. 1042\u00151050,\nJul. 2015.\n[19] S. O. Fadiya, S. Saydam, and V. V. Zira, ``Advancing big data for human-\nitarian needs,'' Procedia Eng., vol. 78, pp. 88\u001595, Jan. 2014.\n[20] B. Marr, Big Data: Using SMART Big Data, Analytics and Metrics to\nMake Better Decisions and Improve Performance. Hoboken, NJ, USA:\nWiley, 2015.\n[21] D. E. O'Leary, ``Arti\u001ccial intelligence and big data,'' IEEE Intell. Syst.,\nvol. 28, no. 2, pp. 96\u001599, Mar./Apr. 2013.\n[22] J. J. Berman, Principles of Big Data: Preparing, Sharing, and Analyzing\nComplex Information. London, U.K.: Newnes, 2013.\n[23] V. Mayer-Schnberger and K. Cukier, Big Data: A Revolution That Will\nTransform How We Live, Work, and Think. Houghton Mif\u001din Harcourt,\n2013.[24] B. Marr. (2015). Why Only One of the 5 vs of Big Data Really Matters.\n[Online]. Available: http://www.ibmbigdatahub.com/blog/why-only-one-\n5-vs-big-data-really-matters\n[25] G. F. P\u001cster, Search Clusters, vol. 2. Upper Saddle River, NJ, USA:\nPrentice-Hall, 1998.\n[26] R. Buyya, High Performance Cluster Computing: Architectures and Sys-\ntems, vol. 1. Upper Saddle River, NJ, USA: Prentice-Hall, 1999.\n[27] H. Brian, ``Cloud computing,'' Commun. ACM, vol. 51, no. 7, pp. 9\u001511,\n2008.\n[28] G. Aceto, A. Botta, W. de Donato, and A. Pescap, ``Cloud monitoring:\nA survey,'' Comput. Netw., vol. 57, no. 9, pp. 2093\u00152115, Jun. 2013.\n[29] N. Marz and J. Warren, Big Data: Principles and Best Practices of\nScalable Realtime Data Systems. New York, NY, USA: Manning, 2015.\n[30] H. Liu, S. Chen, and N. Kubota, ``Intelligent video systems and analytics:\nA survey,'' IEEE Trans. Ind. Informat., vol. 9, no. 3, pp. 1222\u00151233,\nAug. 2013.\n[31] I. E. Olatunji and C.-H. Cheng, ``Video analytics for visual surveil-\nlance and applications: An overview and survey,'' in Machine Learning\nParadigms. Cham, Switzerland: Springer, 2019, pp. 475\u0015515.\n[32] W. Hu, N. Xie, L. Li, X. Zeng, and S. Maybank, ``A survey on visual\ncontent-based video indexing and retrieval,'' IEEE Trans. Syst., Man,\nCybern. C, Appl. Rev., vol. 41, no. 6, pp. 797\u0015819, Nov. 2011.\n[33] B. V. Patel and B. B. Meshram, ``Content based video retrieval sys-\ntems,'' 2012, arXiv:1205.1641. [Online]. Available: http://arxiv.org/abs/\n1205.1641\n[34] M. Haseyama, T. Ogawa, and N. Yagi, ``[Survey paper] a review of video\nretrieval based on image and video semantic understanding,'' ITE Trans.\nMedia Technol. Appl., vol. 1, no. 1, pp. 2\u00159, 2013.\n[35] B. Tian, B. T. Morris, M. Tang, Y. Liu, Y. Yao, C. Gou, D. Shen,\nand S. Tang, ``Hierarchical and networked vehicle surveillance in ITS:\nA survey,'' IEEE Trans. Intell. Transp. Syst., vol. 16, no. 2, pp. 557\u0015580,\nApr. 2015.\n[36] A. Ben Mabrouk and E. Zagrouba, ``Abnormal behavior recognition for\nintelligent video surveillance systems: A review,'' Expert Syst. Appl.,\nvol. 91, pp. 480\u0015491, Jan. 2018.\n[37] N. Khan, I. Yaqoob, I. A. T. Hashem, Z. Inayat, M. Ali, W. Kamaleldin,\nM. Alam, M. Shiraz, and A. Gani, ``Big data: Survey, technologies,\nopportunities, and challenges,'' Sci. World J., vol. 2014, Jul. 2014,\nArt. no. 712826.\n[38] C.-W. Tsai, C.-F. Lai, H.-C. Chao, and A. V. Vasilakos, ``Big data analyt-\nics: A survey,'' J. Big Data, vol. 2, no. 1, p. 21, 2015.\n[39] D. Agrawal, S. Das, and A. El Abbadi, ``Big data and cloud computing:\nCurrent state and future opportunities,'' in Proc. 14th Int. Conf. Extending\nDatabase Technol., 2011, pp. 530\u0015533.\n[40] L. Zhou, S. Pan, J. Wang, and A. V. Vasilakos, ``Machine learning\non big data: Opportunities and challenges,'' Neurocomputing, vol. 237,\npp. 350\u0015361, May 2017.\n[41] D. Che, M. Safran, and Z. Peng, ``From big data to big data mining:\nChallenges, issues, and opportunities,'' in Proc. Int. Conf. Database Syst.\nAdv. Appl. Berlin, Germany: Springer, 2013, pp. 1\u001515.\n[42] L. Zhu, F. R. Yu, Y. Wang, B. Ning, and T. Tang, ``Big data analytics in\nintelligent transportation systems: A survey,'' IEEE Trans. Intell. Transp.\nSyst., vol. 20, no. 1, pp. 383\u0015398, Jan. 2019.\n[43] H. Zahid, T. Mahmood, A. Morshed, and T. Sellis, ``Big data analytics\nin telecommunications: Literature review and architecture recommen-\ndations,'' IEEE/CAA J. Automatica Sinica, vol. 7, no. 1, pp. 18\u001538,\nJan. 2020.\n[44] D. Elliott, ``Intelligent video solution: A de\u001cnition,'' Security, vol. 47,\nno. 6, 2010. [Online]. Available: https://www.securitymagazine.com/\narticles/80912-intelligent-video-solution-a-de\u001cnition\n[45] Y.-L. Tian, L. Brown, A. Hampapur, M. Lu, A. Senior, and C.-F. Shu,\n``IBM smart surveillance system (S3): Event based video surveillance\nsystem with an open and extensible framework,'' Mach. Vis. Appl.,\nvol. 19, nos. 5\u00156, pp. 315\u0015327, 2008.\n[46] M. A. Sujana, ``Zoneminder video surveillance by linux platform,'' Ph.D.\ndissertation, Universiti Teknologi MARA, Shah Alam, Malaysia, 2004.\n[47] A. Alam and Y.-K. Lee, ``TORNADO: Intermediate results orchestration\nbased service-oriented data curation framework for intelligent video big\ndata analytics in the cloud,'' Sensors, vol. 20, no. 12, p. 3581, Jun. 2020.\n[48] J. Kreps, N. Narkhede, and J. Rao, ``Kafka: A distributed messaging\nsystem for log processing,'' in Proc. NetDB, 2011, pp. 1\u00157.\n[49] A. Videla and J. J. Williams, RabbitMQ in Action: Distributed Messaging\nfor Everyone. Greenwich, CT, USA: Manning, 2012.\n[50] B. Snyder, D. Bosnanac, and R. Davies, ActiveMQ in Action, vol. 47.\nGreenwich, CT, USA: Manning, 2011.\n152414 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\n[51] W. Zhang, L. Xu, P. Duan, W. Gong, Q. Lu, and S. Yang, ``A video\ncloud platform combing online and of\u001dine cloud computing tech-\nnologies,'' Pers. Ubiquitous Comput., vol. 19, no. 7, pp. 1099\u00151110,\nOct. 2015.\n[52] G. Banavar, T. Chandra, R. Strom, and D. Sturman, ``A case for message\noriented middleware,'' in Distributed Computing, P. Jayanti, Ed. Berlin,\nGermany: Springer, 1999, pp. 1\u001517.\n[53] A. Ejsmont, Web Scalability for Startup Engineers. New York, NY, USA:\nMcGraw-Hill, 2015.\n[54] A. Ejsmont, Web Scalability for Startup Engineers, 1st ed. New York, NY,\nUSA: McGraw-Hill, 2015.\n[55] E. Gamma, Design Patterns: Elements of Reusable Object-Oriented Soft-\nware. New Delhi, India: Pearson, 1995.\n[56] S. A. Weil, S. A. Brandt, E. L. Miller, D. D. E. Long, and C. Maltzahn,\n``Ceph: A scalable, high-performance distributed \u001cle system,'' in Proc.\n7th Symp. Operating Syst. Design Implement. (OSDI), Berkeley, CA,\nUSA: USENIX Association, 2006, pp. 307\u0015320. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=1298455.1298485\n[57] E. B. Boyer, M. C. Broom\u001celd, and T. A. Perrotti, ``Glusterfs one storage\nserver to rule them all,'' Los Alamos Nat. Lab., Los Alamos, NM, USA,\nRep. LA-UR-12-23586, 2012.\n[58] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, ``The Hadoop dis-\ntributed \u001cle system,'' in Proc. IEEE 26th Symp. Mass Storage Syst.\nTechnol. (MSST), May 2010, pp. 1\u001510.\n[59] P. J. Braam and R. Zahir, ``Lustre: A scalable, high performance \u001cle sys-\ntem. cluster \u001cle systems,'' Technic Inc., Cranston, RI, USA, White Paper,\n2002. [Online]. Available: https://cse.buffalo.edu/faculty/tkosar/cse710/\npapers/lustre-whitepaper.pdf\n[60] D. Gaston, C. Newman, G. Hansen, and D. Lebrun-Grandi, ``MOOSE:\nA parallel computational framework for coupled systems of nonlin-\near equations,'' Nucl. Eng. Des., vol. 239, no. 10, pp. 1768\u00151778,\nOct. 2009. [Online]. Available: http://www.sciencedirect.com/science/\narticle/pii/S0029549309002635\n[61] M. Ovsiannikov, S. Rus, D. Reeves, P. Sutter, S. Rao, and J. Kelly,\n``The quantcast \u001cle system,'' Proc. VLDB Endowment, vol. 6, no. 11,\npp. 1092\u00151101, Aug. 2013, doi: 10.14778/2536222.2536234.\n[62] D. G. Chandra, ``BASE analysis of NoSQL database,'' Future Gener.\nComput. Syst., vol. 52, pp. 13\u001521, Nov. 2015. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0167739X15001788\n[63] A. B. M. Moniruzzaman and S. A. Hossain, ``NoSQL database:\nNew era of databases for big data analytics\u0015classi\u001ccation, character-\nistics and comparison,'' 2013, arXiv:1307.0191. [Online]. Available:\nhttp://arxiv.org/abs/1307.0191\n[64] R. Cattell, ``Scalable sql and nosql data stores,'' SIGMOD Rec., vol. 39,\nno. 4, pp. 12\u001527, May 2011. [Online]. Available: http://doi.acm.org/\n10.1145/1978915.1978919\n[65] E. Brewer, ``Pushing the cap: Strategies for consistency and avail-\nability,'' Computer, vol. 45, no. 2, pp. 23\u001529, Feb. 2012, doi:\n10.1109/MC.2012.37.\n[66] E. Brewer, ``CAP twelve years later: How the `rules' have changed,''\nComputer, vol. 45, no. 2, pp. 23\u001529, Feb. 2012.\n[67] J. L. Carlson, Redis in Action. Greenwich, CT, USA: Manning, 2013.\n[68] K. Banker, MongoDB in Action. Greenwich, CT, USA: Manning,\n2011.\n[69] M. N. Vora, ``Hadoop-HBase for large-scale data,'' in Proc. Int. Conf.\nComput. Sci. Netw. Technol., Dec. 2011, pp. 601\u0015605.\n[70] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows,\nT. Chandra, A. Fikes, and R. E. Gruber, ``Bigtable: A distributed storage\nsystem for structured data,'' ACM Trans. Comput. Syst., vol. 26, no. 2,\npp. 4:1\u00154:26, Jun. 2008, doi: 10.1145/1365815.1365816.\n[71] M. Stonebraker and A. Weisberg, ``The VoltDB main memory DBMS,''\nIEEE Data Eng. Bull., vol. 36, no. 2, pp. 21\u001527, Jun. 2013.\n[72] S. Ghemawat, H. Gobioff, and S.-T. Leung, ``The Google \u001cle system,''\ninProc. 19th ACM Symp. Operating Syst. Princ. (SOSP), New York, NY,\nUSA, 2003, pp. 29\u001543, doi: 10.1145/945445.945450.\n[73] L. Xie, H. Sundaram, and M. Campbell, ``Event mining in multimedia\nstreams,'' Proc. IEEE, vol. 96, no. 4, pp. 623\u0015647, Apr. 2008.\n[74] M.-L. Shyu, Z. Xie, M. Chen, and S.-C. Chen, ``Video semantic\nevent/concept detection using a subspace-based multimedia data min-\ning framework,'' IEEE Trans. Multimedia, vol. 10, no. 2, pp. 252\u0015259,\nFeb. 2008.\n[75] C. Regazzoni, A. Cavallaro, Y. Wu, J. Konrad, and A. Hampapur,\n``Video analytics for surveillance: Theory and practice [From the\nguest editors,'' IEEE Signal Process. Mag., vol. 27, no. 5, pp. 16\u001517,\nSep. 2010.[76] L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller,\nO. Grisel, V. Niculae, P. Prettenhofer, A. Gramfort, J. Grobler,\nR. Layton, J. Vanderplas, A. Joly, B. Holt, and G. Varoquaux,\n``API design for machine learning software: Experiences from the\nscikit-learn project,'' 2013, arXiv:1309.0238. [Online]. Available:\nhttp://arxiv.org/abs/1309.0238\n[77] R. R. Schultz and R. L. Stevenson, ``Extraction of high-resolution frames\nfrom video sequences,'' IEEE Trans. Image Process., vol. 5, no. 6,\npp. 996\u00151011, Jun. 1996.\n[78] C. Saravanan, ``Color image to grayscale image conversion,'' in Proc. 2nd\nInt. Conf. Comput. Eng. Appl., vol. 2, 2010, pp. 196\u0015199.\n[79] J. S. Boreczky and L. A. Rowe, ``Comparison of video shot boundary\ndetection techniques,'' J. Electron. Imag., vol. 5, no. 2, pp. 122\u0015129,\n1996.\n[80] A. Hampapur, T. Weymouth, and R. Jain, ``Digital video segmentation,''\ninProc. 2nd ACM Int. Conf. Multimedia, 1994, pp. 357\u0015364.\n[81] A. Vetro, C. Christopoulos, and H. Sun, ``Video transcoding architectures\nand techniques: An overview,'' IEEE Signal Process. Mag., vol. 20, no. 2,\npp. 18\u001529, Mar. 2003.\n[82] Y. Zhuang, Y. Rui, T. S. Huang, and S. Mehrotra, ``Adaptive key frame\nextraction using unsupervised clustering,'' in Proc. Int. Conf. Image\nProcess. (ICIP), vol. 1, Oct. 1998, pp. 866\u0015870.\n[83] Y. Bengio, A. Courville, and P. Vincent, ``Representation learning: A\nreview and new perspectives,'' IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 35, no. 8, pp. 1798\u00151828, Aug. 2013.\n[84] F. Baumann, A. Ehlers, B. Rosenhahn, and J. Liao, ``Computation strate-\ngies for volume local binary patterns applied to action recognition,'' in\nProc. 11th IEEE Int. Conf. Adv. Video Signal Based Surveill. (AVSS),\nAug. 2014, pp. 68\u001573.\n[85] F. Baumann, A. Ehlers, B. Rosenhahn, and J. Liao, ``Recognizing human\nactions using novel space-time volume binary patterns,'' Neurocomput-\ning, vol. 173, pp. 54\u001563, Jan. 2016.\n[86] A. Amir, M. Berg, S.-F. Chang, W. Hsu, G. Iyengar, C.-Y. Lin,\nM. Naphade, A. Natsev, C. Neti, H. Nock, J. R. Smith, B. Tseng, Y. Wu,\nand D. Zhang, ``IBM research TRECVID-2003 video retrieval system,''\ninProc. NIST TRECVID, 2003, vol. 7, no. 8, p. 36.\n[87] J. Adcock, A. Girgensohn, M. Cooper, T. Liu, L. Wilcox, and E. Rieffel,\n``FXPAL experiments for TRECVID 2004,'' in Proc. TREC Video Retr.\nEval. (TRECVID), 2004, pp. 70\u001581.\n[88] R. Yan and A. G. Hauptmann, ``A review of text and image retrieval\napproaches for broadcast news video,'' Inf. Retr., vol. 10, nos. 4\u00155,\npp. 445\u0015484, Sep. 2007.\n[89] J. Sivic, M. Everingham, and A. Zisserman, ``Person spotting: Video\nshot retrieval for face sets,'' in Proc. Int. Conf. Image Video Retr. Berlin,\nGermany: Springer, 2005, pp. 226\u0015236.\n[90] R. Visser, N. Sebe, and E. Bakker, ``Object recognition for video\nretrieval,'' in Proc. Int. Conf. Image Video Retr. Berlin, Germany:\nSpringer, 2002, pp. 262\u0015270.\n[91] R. Mattivi and L. Shao, ``Human action recognition using LBP-\ntop as sparse spatio-temporal feature descriptor,'' in Proc. Int. Conf.\nComput. Anal. Images Patterns. Berlin, Germany: Springer, 2009,\npp. 740\u0015747.\n[92] G. Zhao and M. Pietikainen, ``Dynamic texture recognition using\nlocal binary patterns with an application to facial expressions,'' IEEE\nTrans. Pattern Anal. Mach. Intell., vol. 29, no. 6, pp. 915\u0015928,\nJun. 2007.\n[93] L. Nanni, S. Brahnam, and A. Lumini, ``Local ternary patterns from three\northogonal planes for human action classi\u001ccation,'' Expert Syst. Appl.,\nvol. 38, no. 5, pp. 5125\u00155128, May 2011.\n[94] Y. Yi, Z. Zheng, and M. Lin, ``Realistic action recognition with\nsalient foreground trajectories,'' Expert Syst. Appl. , vol. 75, pp. 44\u001555,\nJun. 2017.\n[95] M. Uddin and Y.-K. Lee, ``Feature fusion of deep spatial features and\nhandcrafted spatiotemporal features for human action recognition,'' Sen-\nsors, vol. 19, no. 7, p. 1599, Apr. 2019.\n[96] K. Simonyan and A. Zisserman, ``Two-stream convolutional networks for\naction recognition in videos,'' in Proc. Adv. Neural Inf. Process. Syst.,\n2014, pp. 568\u0015576.\n[97] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and\nL. Fei-Fei, ``Large-scale video classi\u001ccation with convolutional neu-\nral networks,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2014, pp. 1725\u00151732.\n[98] L. Wang, Y. Qiao, and X. Tang, ``Action recognition with trajectory-\npooled deep-convolutional descriptors,'' in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., Jun. 2015, pp. 4305\u00154314.\nVOLUME 8, 2020 152415\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\n[99] X. Lu, H. Yao, S. Zhao, X. Sun, and S. Zhang, ``Action recognition with\nmulti-scale trajectory-pooled 3D convolutional descriptors,'' Multimedia\nTools Appl., vol. 78, no. 1, pp. 507\u0015523, Jan. 2019.\n[100] G. Yao, T. Lei, J. Zhong, and P. Jiang, ``Learning multi-temporal-scale\ndeep information for action recognition,'' Int. J. Speech Technol., vol. 49,\nno. 6, pp. 2017\u00152029, Jun. 2019.\n[101] L. Wang, J. Zang, Q. Zhang, Z. Niu, G. Hua, and N. Zheng,\n``Action recognition by an attention-aware temporal weighted\nconvolutional neural network,'' Sensors, vol. 18, no. 7, p. 1979,\nJun. 2018.\n[102] R. Girdhar, D. Ramanan, A. Gupta, J. Sivic, and B. Russell, ``Action-\nVLAD: Learning spatio-temporal aggregation for action classi\u001cca-\ntion,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jul. 2017,\npp. 971\u0015980.\n[103] S. Zhao, Y. Liu, Y. Han, R. Hong, Q. Hu, and Q. Tian, ``Pool-\ning the convolutional layers in deep convnets for video action recog-\nnition,'' IEEE Trans. Circuits Syst. Video Technol., vol. 28, no. 8,\npp. 1839\u00151849, Aug. 2018.\n[104] S. C. Hoi, J. Wang, P. Zhao, and R. Jin, ``Online feature selection for\nmining big data,'' in Proc. 1st Int. Workshop Big Data, Streams Hetero-\ngeneous Source Mining Algorithms, Syst., Program. Models Appl. , 2012,\npp. 93\u0015100.\n[105] H. Tan and L. Chen, ``An approach for fast and parallel video processing\non apache Hadoop clusters,'' in Proc. IEEE Int. Conf. Multimedia Expo\n(ICME), Jul. 2014, pp. 1\u00156.\n[106] Y. Cong, S. Wang, B. Fan, Y. Yang, and H. Yu, ``UDSFS: Unsupervised\ndeep sparse feature selection,'' Neurocomputing, vol. 196, pp. 150\u0015158,\nJul. 2016.\n[107] O. Dekel, ``From online to batch learning with cutoff-averaging,'' in Proc.\nAdv. Neural Inf. Process. Syst., 2009, pp. 377\u0015384.\n[108] A. E. Abdel-Hakim and A. A. Farag, ``CSIFT: A SIFT descrip-\ntor with color invariant characteristics,'' in Proc. IEEE Comput.\nSoc. Conf. Comput. Vis. Pattern Recognit. (CVPR), vol. 2, 2006,\npp. 1978\u00151983.\n[109] Y. Fang and Z. Wang, ``Improving LBP features for gender classi\u001c-\ncation,'' in Proc. Int. Conf. Wavelet Anal. Pattern Recognit., vol. 1,\nAug. 2008, pp. 373\u0015377.\n[110] O. Dniz, G. Bueno, J. Salido, and F. De la Torre, ``Face recognition using\nhistograms of oriented gradients,'' Pattern Recognit. Lett., vol. 32, no. 12,\npp. 1598\u00151603, 2011.\n[111] F. Borisyuk, A. Gordo, and V. Sivakumar, ``Rosetta: Large scale system\nfor text detection and recognition in images,'' in Proc. 24th ACM SIGKDD\nInt. Conf. Knowl. Discovery Data Mining, 2018, pp. 71\u001579.\n[112] Y. LeCun, Y. Bengio, and G. Hinton, ``Deep learning,'' Nature, vol. 521,\nno. 7553, p. 436, 2015.\n[113] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato,\nA. Senior, P. Tucker, K. Yang, Q. V. Le, and A. Y. Ng, ``Large scale\ndistributed deep networks,'' in Proc. Adv. Neural Inf. Process. Syst., 2012,\npp. 1223\u00151231.\n[114] J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad,\nM. Mostofa Ali Patwary, Y. Yang, and Y. Zhou, ``Deep learning scaling is\npredictable, empirically,'' 2017, arXiv:1712.00409. [Online]. Available:\nhttp://arxiv.org/abs/1712.00409\n[115] H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie,\nand E. P. Xing, ``Poseidon: An ef\u001ccient communication architecture for\ndistributed deep learning on GPU clusters,'' in Proc. Annu. Tech. Conf.\n(USENIX ATC), 2017, pp. 181\u0015193.\n[116] J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals,\nR. Monga, and G. Toderici, ``Beyond short snippets: Deep networks for\nvideo classi\u001ccation,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2015, pp. 4694\u00154702.\n[117] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ``ImageNet classi\u001ccation\nwith deep convolutional neural networks,'' in Proc. Adv. Neural Inf.\nProcess. Syst., 2012, pp. 1097\u00151105.\n[118] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, ``Gradient-based learn-\ning applied to document recognition,'' Proc. IEEE, vol. 86, no. 11,\npp. 2278\u00152324, 1998.\n[119] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich, ``Going deeper with convo-\nlutions,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2015,\npp. 1\u00159.\n[120] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV. Vanhoucke, and A. Rabinovich, ``Going deeper with convolutions,'' in\nProc. Comput. Vis. Pattern Recognit. (CVPR), 2015. [Online]. Available:\nhttp://arxiv.org/abs/1409.4842[121] K. Simonyan and A. Zisserman, ``Very deep convolutional networks\nfor large-scale image recognition,'' 2014, arXiv:1409.1556. [Online].\nAvailable: http://arxiv.org/abs/1409.1556\n[122] K. He, X. Zhang, S. Ren, and J. Sun, ``Deep residual learning for\nimage recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2016, pp. 770\u0015778.\n[123] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,\nS. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,\nM. Wicke, Y. Yu, and X. Zheng, ``TensorFlow: A system for large-scale\nmachine learning,'' in Proc. OSDI, vol. 16, 2016, pp. 265\u0015283.\n[124] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell, ``Caffe: Convolutional architecture for fast\nfeature embedding,'' in Proc. 22nd ACM Int. Conf. Multimedia, 2014,\npp. 675\u0015678.\n[125] N. Ketkar, ``Introduction to pytorch,'' in Deep Learning With Python.\nBerkeley, CA, USA: Springer, 2017, pp. 195\u0015208.\n[126] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,\nB. Xu, C. Zhang, and Z. Zhang, ``MXNet: A \u001dexible and ef\u001ccient\nmachine learning library for heterogeneous distributed systems,'' 2015,\narXiv:1512.01274. [Online]. Available: http://arxiv.org/abs/1512.01274\n[127] F. Seide and A. Agarwal, ``Cntk: Microsoft's open-source deep-learning\ntoolkit,'' in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data\nMining, 2016, p. 2135.\n[128] M. Mathwork, Deep Learning Toolbox. Natick, MA, USA: Mathworks.\n[129] S. Gao, M. Zhou, Y. Wang, J. Cheng, H. Yachi, and J. Wang, ``Den-\ndritic neuron model with effective learning algorithms for classi\u001ccation,\napproximation, and prediction,'' IEEE Trans. Neural Netw. Learn. Syst.,\nvol. 30, no. 2, pp. 601\u0015614, Feb. 2019.\n[130] R. Mayer and H.-A. Jacobsen, ``Scalable deep learning on dis-\ntributed infrastructures: Challenges, techniques and tools,'' 2019,\narXiv:1903.11314. [Online]. Available: http://arxiv.org/abs/1903.11314\n[131] A. Deshpande and M. Kumar, Arti\u001ccial Intelligence for Big Data: Com-\nplete Guide to Automating Big Data Solutions Using Arti\u001ccial Intelli-\ngence Techniques. Birmingham, U.K.: Packt, 2018.\n[132] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee,\nJ. Ngiam, Q. V. Le, Y. Wu, and Z. Chen, ``GPipe: Ef\u001ccient training of\ngiant neural networks using pipeline parallelism,'' in Proc. Adv. Neural\nInf. Process. Syst., 2019, pp. 103\u0015112.\n[133] J. Dean and S. Ghemawat, ``MapReduce: Simpli\u001ced data process-\ning on large clusters,'' Commun. ACM, vol. 51, no. 1, pp. 107\u0015113,\nJan. 2008.\n[134] M. Zaharia, R. S. Xin, P. Wendell, T. Das, M. Armbrust, A. Dave,\nX. Meng, J. Rosen, S. Venkataraman, M. J. Franklin, A. Ghodsi,\nJ. Gonzalez, S. Shenker, and I. Stoica, ``Apache spark: A uni\u001ced engine\nfor big data processing,'' Commun. ACM, vol. 59, no. 11, pp. 56\u001565,\n2016.\n[135] P. Carbone, A. Katsifodimos, S. Ewen, V. Markl, S. Haridi, and\nK. Tzoumas, ``Apache \u001dink: Stream and batch processing in a single\nengine,'' Bull. IEEE Comput. Soc. Tech. Committee Data Eng., vol. 36,\nno. 4, pp. 28\u001538, 2015.\n[136] A. Toshniwal, A. Shukla, K. Ramasamy, J. M. Patel, S. Kulkarni,\nJ. Jackson, K. Gade, M. Fu, J. Donham, N. Bhagat, S. Mittal, D. Ryaboy,\n``Storm Twitter,'' in Proc. ACM SIGMOD Int. Conf. Manage. Data , 2014,\npp. 147\u0015156.\n[137] G. Wang, J. Koshy, S. Subramanian, K. Paramasivam, M. Zadeh,\nN. Narkhede, J. Rao, J. Kreps, and J. Stein, ``Building a replicated logging\nsystem with apache kafka,'' Proc. VLDB Endowment, vol. 8, no. 12,\npp. 1654\u00151655, Aug. 2015.\n[138] S. Owen and S. Owen, Mahout in Action. Stamford, CT, USA: Manning\nPublications Co., 2012.\n[139] D. Team, ``Deeplearning4j: Open-source distributed deep learning for the\nJVM,'' Apache Softw. Found. License, vol. 2, 2016. [Online]. Available:\nhttps://deeplearning4j.org/\n[140] F. Chollet, ``Keras,'' 2015.\n[141] J. Dai, Y. Wang, X. Qiu, D. Ding, Y. Zhang, Y. Wang, X. Jia, C. Zhang,\nY. Wan, Z. Li, J. Wang, S. Huang, Z. Wu, Y. Wang, Y. Yang, B. She,\nD. Shi, Q. Lu, K. Huang, and G. Song, ``BigDL: A distributed deep\nlearning framework for big data,'' 2018, arXiv:1804.05839. [Online].\nAvailable: http://arxiv.org/abs/1804.05839\n[142] X. Meng, J. Bradley, B. Yavuz, E. Sparks, S. Venkataraman, D. Liu,\nJ. Freeman, D. B. Tsai, M. Amde, S. Owen, D. Xin, R. Xin, M. J. Franklin,\nR. Zadeh, M. Zaharia, and A. Talwalkar, ``MLlib: Machine learning in\napache spark,'' J. Mach. Learn. Res., vol. 17, no. 1, pp. 1235\u00151241,\n2016.\n152416 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\n[143] G. De Francisci Morales and A. Bifet, ``SAMOA: Scalable advanced mas-\nsive online analysis,'' J. Mach. Learn. Res., vol. 16, no. 1, pp. 149\u0015153,\n2015.\n[144] J. Dean and S. Ghemawat, ``MapReduce: A \u001dexible data processing tool,''\nCommun. ACM, vol. 53, no. 1, pp. 72\u001577, Jan. 2010.\n[145] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Sto-\nica, ``Spark: Cluster computing with working sets,'' HotCloud, vol. 10,\nnos. 10\u001510, p. 95, 2010.\n[146] M. Zaharia, T. Das, H. Li, T. Hunter, S. Shenker, and I. Stoica, ``Dis-\ncretized streams: Fault-tolerant streaming computation at scale,'' in Proc.\n24th ACM Symp. Operating Syst. Princ., 2013, pp. 423\u0015438.\n[147] D. Cook, Practical Machine Learning With H2O: Powerful, Scalable\nTechniques for Deep Learning and AI. Newton, MA, USA: O'Reilly\nMedia, 2016.\n[148] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ``ImageNet:\nA large-scale hierarchical image database,'' in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., Jun. 2009, pp. 248\u0015255.\n[149] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni,\nD. Poland, D. Borth, and L.-J. Li, ``YFCC100M: The new data in\nmultimedia research,'' 2015, arXiv:1503.01817. [Online]. Available:\nhttp://arxiv.org/abs/1503.01817\n[150] Trecvid. TREC Video Retrieval Evaluation: TRECVID. Accessed:\nDec. 7, 2019. [Online]. Available: https://trecvid.nist.gov/\n[151] K. Soomro, A. R. Zamir, and M. Shah, ``UCF101: A dataset of 101\nhuman actions classes from videos in the wild,'' 2012, arXiv:1212.0402.\n[Online]. Available: http://arxiv.org/abs/1212.0402\n[152] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier,\nS. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman,\nand A. Zisserman, ``The kinetics human action video dataset,'' 2017,\narXiv:1705.06950. [Online]. Available: http://arxiv.org/abs/1705.06950\n[153] CEUR. (2015). Multimedia Benchmark Workshop. Accessed:\nJul. 12, 2019. [Online]. Available: http://ceur-ws.org/Vol-1436/\n[154] A. Nambiar, M. Taiana, D. Figueira, J. C. Nascimento, and A. Bernardino,\n``A multi-camera video dataset for research on high-de\u001cnition surveil-\nlance,'' Int. J. Mach. Intell. Sensory Signal Process., vol. 1, no. 3,\npp. 267\u0015286, 2014.\n[155] F. C. Heilbron, V. Escorcia, B. Ghanem, and J. C. Niebles, ``ActivityNet:\nA large-scale video benchmark for human activity understanding,'' in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2015, pp. 961\u0015970.\n[156] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, ``HMDB:\nA large video database for human motion recognition,'' in Proc. Int. Conf.\nComput. Vis., Nov. 2011, pp. 2556\u00152563.\n[157] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li,\nS. Vijayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar, C. Schmid,\nand J. Malik, ``A V A: A video dataset of spatio-temporally localized\natomic visual actions,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recog-\nnit., Jun. 2018, pp. 6047\u00156056.\n[158] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic,\n``HowTo100M: Learning a text-video embedding by watching hundred\nmillion narrated video clips,'' 2019, arXiv:1906.03327. [Online]. Avail-\nable: http://arxiv.org/abs/1906.03327\n[159] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,\nB. Varadarajan, and S. Vijayanarasimhan, ``YouTube-8M: A large-\nscale video classi\u001ccation benchmark,'' 2016, arXiv:1609.08675.\n[Online]. Available: http://arxiv.org/abs/1609.08675\n[160] L. Ballan, M. Bertini, A. Del Bimbo, and G. Serra, ``Video annotation\nand retrieval using ontologies and rule learning,'' IEEE MultimediaMag.,\nvol. 17, no. 4, pp. 80\u001588, Oct. 2010.\n[161] A. F. Smeaton, P. Over, and A. R. Doherty, ``Video shot boundary detec-\ntion: Seven years of TRECVid activity,'' Comput. Vis. Image Understand.,\nvol. 114, no. 4, pp. 411\u0015418, Apr. 2010.\n[162] A. F. Smeaton, P. Over, and W. Kraaij, ``High-level feature detection\nfrom video in TRECVid: A 5-year retrospective of achievements,''\ninMultimedia Content Analysis. Boston, MA, USA: Springer, 2009,\npp. 1\u001524.\n[163] H. Bhaumik, S. Bhattacharyya, M. D. Nath, and S. Chakraborty,\n``Hybrid soft computing approaches to content based video retrieval:\nA brief review,'' Appl. Soft Comput., vol. 46, pp. 1008\u00151029,\nSep. 2016.\n[164] S.-M. Hu, T. Chen, K. Xu, M.-M. Cheng, and R. R. Martin, ``Internet\nvisual media processing: A survey with graphics and vision applications,''\nVis. Comput., vol. 29, no. 5, pp. 393\u0015405, May 2013.\n[165] N. Spolar, H. D. Lee, W. S. R. Takaki, L. A. Ensina, C. S. R. Coy, and\nF. C. Wu, ``A systematic review on content-based video retrieval,'' Eng.\nAppl. Artif. Intell., vol. 90, Apr. 2020, Art. no. 103557.[166] L. Shang, L. Yang, F. Wang, K.-P. Chan, and X.-S. Hua, ``Real-time large\nscale near-duplicate Web video retrieval,'' in Proc. 18th ACM Int. Conf.\nMultimedia, 2010, pp. 531\u0015540.\n[167] H. Wang, F. Zhu, B. Xiao, L. Wang, and Y.-G. Jiang, ``GPU-based\nmapreduce for large-scale near-duplicate video retrieval,'' Multimedia\nTools Appl., vol. 74, no. 23, pp. 10515\u001510534, 2015.\n[168] K. Mikolajczyk and C. Schmid, ``Scale & af\u001cne invariant interest point\ndetectors,'' Int. J. Comput. Vis., vol. 60, no. 1, pp. 63\u001586, 2004.\n[169] J. Sivic and A. Zisserman, ``Video Google: A text retrieval approach to\nobject matching in videos,'' in Proc. 9th IEEE Int. Conf. Comput. Vis.,\nOct. 2003, pp. 1470\u00151477.\n[170] S. Ding, G. Li, Y. Li, X. Li, Q. Zhai, A. C. Champion, J. Zhu, D. Xuan,\nand Y. F. Zheng, ``SurvSurf: Human retrieval on large surveillance video\ndata,'' Multimedia Tools Appl., vol. 76, no. 5, pp. 6521\u00156549, Mar. 2017.\n[171] N. Zhu, W. He, Y. Hua, and Y. Chen, ``Marlin: Taming the big streaming\ndata in large scale video similarity search,'' in Proc. IEEE Int. Conf. Big\nData (Big Data), Oct. 2015, pp. 1755\u00151764.\n[172] J. Lv, B. Wu, S. Yang, B. Jia, and P. Qiu, ``Ef\u001ccient large scale near-\nduplicate video detection base on spark,'' in Proc. IEEE Int. Conf. Big\nData (Big Data), Dec. 2016, pp. 957\u0015962.\n[173] M. N. Khan, A. Alam, and Y.-K. Lee, ``FALKON: large-scale content-\nbased video retrieval utilizing deep-features and distributed in-memory\ncomputing,'' in Proc. IEEE Int. Conf. Big Data Smart Comput. (Big-\nComp), Feb. 2020, pp. 36\u001543.\n[174] F.-C. Lin, H.-H. Ngo, and C.-R. Dow, ``A cloud-based face video\nretrieval system with deep learning,'' J. Supercomput. , vol. 76, pp. 1\u001521,\nJan. 2020.\n[175] W. Hu, D. Xie, Z. Fu, W. Zeng, and S. Maybank, ``Semantic-based\nsurveillance video retrieval,'' IEEE Trans. Image Process., vol. 16, no. 4,\npp. 1168\u00151181, Apr. 2007.\n[176] J. Sivic and A. Zisserman, ``Video Google: Ef\u001ccient visual search of\nvideos,'' in Toward Category-Level Object Recognition. Berlin, Germany:\nSpringer, 2006, pp. 127\u0015144.\n[177] Y. Aytar, M. Shah, and J. Luo, ``Utilizing semantic word similarity\nmeasures for video retrieval,'' in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2008, pp. 1\u00158.\n[178] L. S. Kennedy, A. P. Natsev, and S.-F. Chang, ``Automatic discovery\nof query-class-dependent models for multimodal search,'' in Proc. 13th\nAnnu. ACM Int. Conf. Multimedia, 2005, pp. 882\u0015891.\n[179] R. Yan, J. Yang, and A. G. Hauptmann, ``Learning query-class dependent\nweights in automatic video retrieval,'' in Proc. 12th Annu. ACM Int. Conf.\nMultimedia, 2004, pp. 548\u0015555.\n[180] R. Lienhart, ``A system for effortless content annotation to unfold the\nsemantics in videos,'' in Proc. cbaivl, 2000, p. 45.\n[181] C. G. M. Snoek, B. Huurnink, L. Hollink, M. de Rijke, G. Schreiber, and\nM. Worring, ``Adding semantics to detectors for video retrieval,'' IEEE\nTrans. Multimedia, vol. 9, no. 5, pp. 975\u0015986, Aug. 2007.\n[182] S.-Y. Neo, J. Zhao, M.-Y. Kan, and T.-S. Chua, ``Video retrieval using\nhigh level features: Exploiting query matching and con\u001cdence-based\nweighting,'' in Proc. Int. Conf. Image Video Retr. Berlin, Germany:\nSpringer, 2006, pp. 143\u0015152.\n[183] L.-H. Chen, K.-H. Chin, and H.-Y. Liao, ``An integrated approach to\nvideo retrieval,'' in Proc. 19th Conf. Australas. Database, vol. 75, 2008,\npp. 49\u001555.\n[184] F. Hopfgartner, J. Urban, R. Villa, and J. Jose, ``Simulated testing of an\nadaptive multimedia information retrieval system,'' in Proc. Int. Work-\nshop Content-Based Multimedia Indexing, Jun. 2007, pp. 328\u0015335.\n[185] R. Yan, A. G. Hauptmann, and R. Jin, ``Negative pseudo-relevance feed-\nback in content-based video retrieval,'' in Proc. 11th ACM Int. Conf.\nMultimedia, 2003, pp. 343\u0015346.\n[186] A. G. Hauptmann, M. G. Christel, and R. Yan, ``Video retrieval\nbased on semantic concepts,'' Proc. IEEE, vol. 96, no. 4, pp. 602\u0015622,\nApr. 2008.\n[187] C. Sun and R. Nevatia, ``DISCOVER: Discovering important segments\nfor classi\u001ccation of video events and recounting,'' in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit., Jun. 2014, pp. 2569\u00152576.\n[188] A. Habibian and C. G. M. Snoek, ``Recommendations for recognizing\nvideo events by concept vocabularies,'' Comput. Vis. Image Understand.,\nvol. 124, pp. 110\u0015122, Jul. 2014.\n[189] H. Song, X. Wu, W. Yu, and Y. Jia, ``Extracting key segments of videos for\nevent detection by learning from Web sources,'' IEEE Trans. Multimedia,\nvol. 20, no. 5, pp. 1088\u00151100, May 2018.\n[190] H. Wang, X. Wu, and Y. Jia, ``Video annotation via image groups\nfrom the Web,'' IEEE Trans. Multimedia, vol. 16, no. 5, pp. 1282\u00151291,\nAug. 2014.\nVOLUME 8, 2020 152417\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\n[191] X. Zhang, Y. Yang, Y. Zhang, H. Luan, J. Li, H. Zhang, and T.-S. Chua,\n``Enhancing video event recognition using automatically constructed\nsemantic-visual knowledge base,'' IEEE Trans. Multimedia, vol. 17, no. 9,\npp. 1562\u00151575, Sep. 2015.\n[192] H. Song, X. Wu, W. Liang, and Y. Jia, ``Recognizing key segments of\nvideos for video annotation by learning from Web image sets,'' Multime-\ndia Tools Appl., vol. 76, no. 5, pp. 6111\u00156126, Mar. 2017.\n[193] H. J. Escalante, I. Guyon, V. Athitsos, P. Jangyodsuk, and J. Wan, ``Prin-\ncipal motion components for one-shot gesture recognition,'' Pattern Anal.\nAppl., vol. 20, no. 1, pp. 167\u0015182, Feb. 2017.\n[194] F. Jiang, S. Zhang, S. Wu, Y. Gao, and D. Zhao, ``Multi-layered gesture\nrecognition with Kinect,'' J. Mach. Learn. Res., vol. 16, pp. 227\u0015254,\nFeb. 2015.\n[195] D. Wu, F. Zhu, and L. Shao, ``One shot learning gesture recognition from\nRGBD images,'' in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern\nRecognit. Workshops, Jun. 2012, pp. 7\u001512.\n[196] C. Lu, J. Jia, and C.-K. Tang, ``Range-sample depth feature for action\nrecognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2014, pp. 772\u0015779.\n[197] X. Yang and Y. Tian, ``Super normal vector for activity recognition using\ndepth sequences,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2014, pp. 804\u0015811.\n[198] O. Oreifej and Z. Liu, ``HON4D: Histogram of oriented 4D normals for\nactivity recognition from depth sequences,'' in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., Jun. 2013, pp. 716\u0015723.\n[199] X. Yang, C. Zhang, and Y. Tian, ``Recognizing actions using depth motion\nmaps-based histograms of oriented gradients,'' in Proc. 20th ACM Int.\nConf. Multimedia, 2012, pp. 1057\u00151060.\n[200] P. Wang, W. Li, Z. Gao, C. Tang, J. Zhang, and P. Ogunbona, ``Convnets-\nbased action recognition from depth maps through virtual cameras\nand pseudocoloring,'' in Proc. 23rd ACM Int. Conf. Multimedia, 2015,\npp. 1119\u00151122.\n[201] P. Wang, W. Li, Z. Gao, J. Zhang, C. Tang, and P. O. Ogunbona, ``Action\nrecognition from depth maps using deep convolutional neural networks,''\nIEEE Trans. Human-Mach. Syst. , vol. 46, no. 4, pp. 498\u0015509, Aug. 2016.\n[202] D. Wu, L. Pigou, P.-J. Kindermans, N. D.-H. Le, L. Shao, J. Dambre, and\nJ.-M. Odobez, ``Deep dynamic neural networks for multimodal gesture\nsegmentation and recognition,'' IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 38, no. 8, pp. 1583\u00151597, Aug. 2016.\n[203] P. Wang, W. Li, Z. Gao, C. Tang, and P. O. Ogunbona, ``Depth pool-\ning based large-scale 3-D action recognition with convolutional neural\nnetworks,'' IEEE Trans. Multimedia, vol. 20, no. 5, pp. 1051\u00151061,\nMay 2018.\n[204] V. Veeriah, N. Zhuang, and G.-J. Qi, ``Differential recurrent neural net-\nworks for action recognition,'' in Proc. IEEE Int. Conf. Comput. Vis.,\nDec. 2015, pp. 4041\u00154049.\n[205] Y. Du, W. Wang, and L. Wang, ``Hierarchical recurrent neural network\nfor skeleton based action recognition,'' in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2015, pp. 1110\u00151118.\n[206] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, ``Learning\nspatiotemporal features with 3D convolutional networks,'' in Proc. IEEE\nInt. Conf. Comput. Vis., Dec. 2015, pp. 4489\u00154497.\n[207] S. Ji, W. Xu, M. Yang, and K. Yu, ``3D convolutional neural networks\nfor human action recognition,'' IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 35, no. 1, pp. 221\u0015231, Jan. 2013.\n[208] P. Guler, D. Emeksiz, A. Temizel, M. Teke, and T. T. Temizel, ``Real-\ntime multi-camera video analytics system on GPU,'' J. Real-Time Image\nProcess., vol. 11, no. 3, pp. 457\u0015472, Mar. 2016.\n[209] V. Pham, P. Vo, V. T. Hung, and L. H. Bac, ``GPU implementation of\nextended Gaussian mixture model for background subtraction,'' in Proc.\nIEEE RIVF Int. Conf. Comput. Commun. Technol., Res., Innov., Vis.\nFuture (RIVF), Nov. 2010, pp. 1\u00154.\n[210] R. Cucchiara, C. Grana, M. Piccardi, and A. Prati, ``Detecting objects,\nshadows and ghosts in video streams by exploiting color and motion\ninformation,'' in Proc. 11th Int. Conf. Image Anal. Process., 2001,\npp. 360\u0015365.\n[211] C. Beyan and A. Temizel, ``Adaptive mean-shift for automated multi\nobject tracking,'' IET Comput. Vis., vol. 6, no. 1, pp. 1\u001512, 2012.\n[212] B. Risse, M. Mangan, L. Del Pero, and B. Webb, ``Visual tracking of small\nanimals in cluttered natural environments using a freely moving camera,''\ninProc. IEEE Int. Conf. Comput. Vis. Workshops (ICCVW), Oct. 2017,\npp. 2840\u00152849.\n[213] S. Shantaiya, K. Verma, and K. Mehta, ``A survey on approaches of object\ndetection,'' Int. J. Comput. Appl., vol. 65, no. 18, pp. 14\u001520, 2013.[214] B. Deori and D. M. Thounaojam, ``A survey on moving object tracking\nin video,'' Int. J. Inf. Theory, vol. 3, no. 3, pp. 31\u001546, Jul. 2014.\n[215] L. Leal-Taix, A. Milan, K. Schindler, D. Cremers, I. Reid, and S. Roth,\n``Tracking the trackers: An analysis of the state of the art in mul-\ntiple object tracking,'' 2017, arXiv:1704.02781. [Online]. Available:\nhttp://arxiv.org/abs/1704.02781\n[216] X. Yin, B. Wang, W. Li, Y. Liu, and M. Zhang, ``Background subtraction\nfor moving cameras based on trajectory-controlled segmentation and\nlabel inference,'' Ksii Trans. Internet Inf. Syst., vol. 9, no. 10, pp. 4092\u0015\n4107, 2015.\n[217] S. Zhang, J.-B. Huang, J. Lim, Y. Gong, J. Wang, N. Ahuja, and\nM.-H. Yang, ``Tracking persons-of-interest via unsupervised repre-\nsentation adaptation,'' 2017, arXiv:1710.02139. [Online]. Available:\nhttp://arxiv.org/abs/1710.02139\n[218] E. Hayman and J.-O. Eklundh, ``Statistical background subtraction for a\nmobile observer,'' in Proc. Int. Conf. Comput. Vis. (ICCV), 2003, p. 67.\n[219] P. Lenz, J. Ziegler, A. Geiger, and M. Roser, ``Sparse scene \u001dow segmen-\ntation for moving object detection in urban environments,'' in Proc. IEEE\nIntell. Vehicles Symp. (IV), Jun. 2011, pp. 926\u0015932.\n[220] C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland, ``P\u001cnder:\nReal-time tracking of the human body,'' IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 19, no. 7, pp. 780\u0015785, Jul. 1997.\n[221] Z. Zivkovic and F. van der Heijden, ``Ef\u001ccient adaptive density estimation\nper image pixel for the task of background subtraction,'' Pattern Recognit.\nLett., vol. 27, no. 7, pp. 773\u0015780, May 2006.\n[222] Y. Jin, L. Tao, H. Di, N. I. Rao, and G. Xu, ``Background modeling from\na free-moving camera by multi-layer homography algorithm,'' in Proc.\n15th IEEE Int. Conf. Image Process., Oct. 2008, pp. 1572\u00151575.\n[223] Y. Wu, X. He, and T. Q. Nguyen, ``Moving object detection with a\nfreely moving camera via background motion subtraction,'' IEEE Trans.\nCircuits Syst. Video Technol., vol. 27, no. 2, pp. 236\u0015248, Feb. 2017.\n[224] M. Braham and M. Van Droogenbroeck, ``Deep background subtraction\nwith scene-speci\u001cc convolutional neural networks,'' in Proc. Int. Conf.\nSyst., Signals Image Process. (IWSSIP), May 2016, pp. 1\u00154.\n[225] T. Bouwmans, A. Sobral, S. Javed, S. K. Jung, and E.-H. Zahzah,\n``Decomposition into low-rank plus additive matrices for background/\nforeground separation: A review for a comparative evaluation with a\nlarge-scale dataset,'' Comput. Sci. Rev., vol. 23, pp. 1\u001571, Feb. 2017.\n[226] M. Yazdi and T. Bouwmans, ``New trends on moving object detection\nin video images captured by a moving camera: A survey,'' Comput. Sci.\nRev., vol. 28, pp. 157\u0015177, May 2018.\n[227] I. Elhart, M. Mikusz, C. G. Mora, M. Langheinrich, and N. Davies,\n``Audience monitor: An open source tool for tracking audience mobility\nin front of pervasive displays,'' in Proc. 6th ACM Int. Symp. Pervas.\nDisplays, 2017, p. 10.\n[228] G. M. Farinella, G. Farioli, S. Battiato, S. Leonardi, and G. Gallo, ``Face\nre-identi\u001ccation for digital signage applications,'' in Proc. Int. Workshop\nVideo Anal. Audience Meas. Retail Digit. Signage . Cham, Switzerland:\nSpringer, 2014, pp. 40\u001552.\n[229] T. Li, H. Chang, M. Wang, B. Ni, R. Hong, and S. Yan, ``Crowded scene\nanalysis: A survey,'' IEEE Trans. Circuits Syst. Video Technol., vol. 25,\nno. 3, pp. 367\u0015386, Mar. 2015.\n[230] X. Wang, X. Yang, X. He, Q. Teng, and M. Gao, ``A high accuracy\n\u001dow segmentation method in crowded scenes based on streakline,'' Optik,\nvol. 125, no. 3, pp. 924\u0015929, Feb. 2014.\n[231] R. Mehran, B. E. Moore, and M. Shah, ``A streakline representation\nof \u001dow in crowded scenes,'' in Proc. Eur. Conf. Comput. Vis. Berlin,\nGermany: Springer, 2010, pp. 439\u0015452.\n[232] H. Su, H. Yang, S. Zheng, Y. Fan, and S. Wei, ``The large-scale crowd\nbehavior perception based on spatio-temporal viscous \u001duid \u001celd,'' IEEE\nTrans. Inf. Forensics Security, vol. 8, no. 10, pp. 1575\u00151589, Oct. 2013.\n[233] L. Kratz and K. Nishino, ``Tracking pedestrians using local spatio-\ntemporal motion patterns in extremely crowded scenes,'' IEEE Trans.\nPattern Anal. Mach. Intell., vol. 34, no. 5, pp. 987\u00151002, May 2012.\n[234] P.-M. Jodoin, Y. Benezeth, and Y. Wang, ``Meta-tracking for video scene\nunderstanding,'' in Proc. 10th IEEE Int. Conf. Adv. Video Signal Based\nSurveill., Aug. 2013, pp. 1\u00156.\n[235] M. Lewandowski, D. Simonnet, D. Makris, S. A. Velastin, and J. Orwell,\n``Tracklet reidenti\u001ccation in crowded scenes using bag of spatio-temporal\nhistograms of oriented gradients,'' in Proc. Mex. Conf. Pattern Recognit.\nBerlin, Germany: Springer, 2013, pp. 94\u0015103.\n[236] B. Zhou, X. Wang, and X. Tang, ``Understanding collective crowd behav-\niors: Learning a mixture model of dynamic pedestrian-agents,'' in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2012, pp. 2871\u00152878.\n152418 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\n[237] W. Chongjing, Z. Xu, Z. Yi, and L. Yuncai, ``Analyzing motion patterns\nin crowded scenes via automatic tracklets clustering,'' China Commun.,\nvol. 10, no. 4, pp. 144\u0015154, Apr. 2013.\n[238] B. T. Morris and M. M. Trivedi, ``A survey of vision-based trajectory\nlearning and analysis for surveillance,'' IEEE Trans. Circuits Syst. Video\nTechnol., vol. 18, no. 8, pp. 1114\u00151127, Aug. 2008.\n[239] C. Piciarelli, C. Micheloni, and G. L. Foresti, ``Trajectory-based anoma-\nlous event detection,'' IEEE Trans. Circuits Syst. Video Technol., vol. 18,\nno. 11, pp. 1544\u00151554, Nov. 2008.\n[240] S. Coar, G. Donatiello, V. Bogorny, C. Garate, L. O. Alvares, and\nF. Brmond, ``Toward abnormal trajectory and event detection in video\nsurveillance,'' IEEE Trans. Circuits Syst. Video Technol., vol. 27, no. 3,\npp. 683\u0015695, Mar. 2017.\n[241] X. Song, X. Shao, Q. Zhang, R. Shibasaki, H. Zhao, J. Cui, and\nH. Zha, ``A fully online and unsupervised system for large and high-\ndensity area surveillance: Tracking, semantic scene learning and abnor-\nmality detection,'' ACM Trans. Intell. Syst. Technol., vol. 4, no. 2,\np. 35, 2013.\n[242] L. Brun, A. Saggese, and M. Vento, ``Dynamic scene understanding for\nbehavior analysis based on string kernels,'' IEEE Trans. Circuits Syst.\nVideo Technol., vol. 24, no. 10, pp. 1669\u00151681, Oct. 2014.\n[243] D. Tran, J. Yuan, and D. Forsyth, ``Video event detection: From sub-\nvolume localization to spatiotemporal path search,'' IEEE Trans. Pattern\nAnal. Mach. Intell., vol. 36, no. 2, pp. 404\u0015416, Feb. 2014.\n[244] A. R. Revathi and D. Kumar, ``An ef\u001ccient system for anomaly detection\nusing deep learning classi\u001cer,'' Signal, Image Video Process., vol. 11,\nno. 2, pp. 291\u0015299, Feb. 2017, doi: 10.1007/s11760-016-0935-0.\n[245] O. P. Popoola and K. Wang, ``Video-based abnormal human behavior\nrecognition\u0016A review,'' IEEE Trans. Syst., Man, Cybern. C, Appl. Rev.,\nvol. 42, no. 6, pp. 865\u0015878, Nov. 2012.\n[246] Y. Yuan, Y. Feng, and X. Lu, ``Statistical hypothesis detector for abnormal\nevent detection in crowded scenes,'' IEEE Trans. Cybern., vol. 47, no. 11,\npp. 3597\u00153608, Nov. 2017.\n[247] G. Xiong, J. Cheng, X. Wu, Y.-L. Chen, Y. Ou, and Y. Xu, ``An\nenergy model approach to people counting for abnormal crowd\nbehavior detection,'' Neurocomputing, vol. 83, pp. 121\u0015135, Apr. 2012.\n[Online]. Available: http://www.sciencedirect.com/science/article/pii/\nS0925231211006990\n[248] Y. Zhang, L. Qin, R. Ji, H. Yao, and Q. Huang, ``Social attribute-\naware force model: Exploiting richness of interaction for abnormal crowd\ndetection,'' IEEE Trans. Circuits Syst. Video Technol., vol. 25, no. 7,\npp. 1231\u00151245, Jul. 2015.\n[249] S. Yi, X. Wang, C. Lu, and J. Jia, ``L0 regularized stationary time\nestimation for crowd group analysis,'' in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit. (CVPR), Washington, DC, USA: IEEE Computer\nSociety, 2014, pp. 2219\u00152226, doi: 10.1109/CVPR.2014.284.\n[250] K.-W. Cheng, Y.-T. Chen, and W.-H. Fang, ``Video anomaly detection\nand localization using hierarchical feature representation and Gaussian\nprocess regression,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2015, pp. 2909\u00152917.\n[251] Y.-J. Lee, Y.-R. Yeh, and Y.-C.-F. Wang, ``Anomaly detection via online\noversampling principal component analysis,'' IEEE Trans. Knowl. Data\nEng., vol. 25, no. 7, pp. 1460\u00151470, Jul. 2013.\n[252] B. Krausz and C. Bauckhage, ``Loveparade 2010: Automatic video analy-\nsis of a crowd disaster,'' Comput. Vis. Image Understand., vol. 116, no. 3,\npp. 307\u0015319, 2012. [Online]. Available: http://www.sciencedirect.com/\nscience/article/pii/S1077314211002037\n[253] D.-G. Lee, H.-I. Suk, S.-K. Park, and S.-W. Lee, ``Motion in\u001duence map\nfor unusual human activity detection and localization in crowded scenes,''\nIEEE Trans. Circuits Syst. Video Technol., vol. 25, no. 10, pp. 1612\u00151623,\nOct. 2015.\n[254] C. C. Loy, T. Xiang, and S. Gong, ``Salient motion detection in crowded\nscenes,'' in Proc. 5th Int. Symp. Commun., Control Signal Process.,\nMay 2012, pp. 1\u00154.\n[255] S. Vishwakarma and A. Agrawal, ``A survey on activity recognition and\nbehavior understanding in video surveillance,'' Vis. Comput., vol. 29,\nno. 10, pp. 983\u00151009, 2013, doi: 10.1007/s00371-012-0752-6.\n[256] K. Xu, X. Jiang, and T. Sun, ``Anomaly detection based on stacked sparse\ncoding with intraframe classi\u001ccation strategy,'' IEEE Trans. Multimedia,\nvol. 20, no. 5, pp. 1062\u00151074, May 2018.\n[257] D. Sen and B. Raman, ``Video skimming: Taxonomy and comprehensive\nsurvey,'' ACM Comput. Surv., vol. 52, no. 5, p. 106, 2019.\n[258] X.-N. Xie and F. Wu, ``Automatic video summarization by af\u001cnity prop-\nagation clustering and semantic content mining,'' in Proc. Int. Symp.\nElectron. Commerce Secur., 2008, pp. 203\u0015208.[259] J. Wu, S.-H. Zhong, J. Jiang, and Y. Yang, ``A novel clustering method\nfor static video summarization,'' Multimedia Tools Appl., vol. 76, no. 7,\npp. 9625\u00159641, Apr. 2017.\n[260] H. Bhaumik, S. Bhattacharyya, S. Dutta, and S. Chakraborty, ``Towards\nredundancy reduction in storyboard representation for static video sum-\nmarization,'' in Proc. Int. Conf. Adv. Comput., Commun. Informat.\n(ICACCI), Sep. 2014, pp. 344\u0015350.\n[261] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman, ``Summary transfer:\nExemplar-based subset selection for video summarization,'' in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2016, pp. 1059\u00151067.\n[262] M. Gygli, H. Grabner, and L. Van Gool, ``Video summarization by learn-\ning submodular mixtures of objectives,'' in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., Jun. 2015, pp. 3090\u00153098.\n[263] J. Mohan and M. S. Nair, ``Static video summarization using sparse\nautoencoders,'' in Proc. IEEE Int. Conf. Electr., Comput. Commun. Tech-\nnol. (ICECCT), Feb. 2019, pp. 1\u00158.\n[264] Z. Ji, Y. Zhao, Y. Pang, and X. Li, ``Cross-modal guidance based\nauto-encoder for multi-video summarization,'' Pattern Recognit. Lett.,\nvol. 135, pp. 131\u0015137, Jul. 2020.\n[265] K. A. Peker, I. Otsuka, and A. Divakaran, ``Broadcast video program\nsummarization using face tracks,'' in Proc. IEEE Int. Conf. Multimedia\nExpo, Jul. 2006, pp. 1053\u00151056.\n[266] C.-W. Ngo, Y.-F. Ma, and H.-J. Zhang, ``Video summarization and scene\ndetection by graph modeling,'' IEEE Trans. Circuits Syst. Video Technol.,\nvol. 15, no. 2, pp. 296\u0015305, Feb. 2005.\n[267] R.-G. Xiao, Y.-Y. Wang, H. Pan, and F. Wu, ``Automatic video sum-\nmarization by spatio-temporal analysis and non-trivial repeating pat-\ntern detection,'' in Proc. Congr. Image Signal Process., vol. 4, 2008,\npp. 555\u0015559.\n[268] Y. Gao, W.-B. Wang, and J.-H. Yong, ``A video summarization tool\nusing two-level redundancy detection for personal video recorders,'' IEEE\nTrans. Consum. Electron., vol. 54, no. 2, pp. 521\u0015526, May 2008.\n[269] B. Xu, X. Wang, and Y.-G. Jiang, ``Fast summarization of user-generated\nvideos: Exploiting semantic, emotional, and quality clues,'' IEEE Multi-\nmediaMag., vol. 23, no. 3, pp. 23\u001533, Jul. 2016.\n[270] Z. Ji, K. Xiong, Y. Pang, and X. Li, ``Video summarization with attention-\nbased encoder\u0015decoder networks,'' IEEE Trans. Circuits Syst. Video\nTechnol., vol. 30, no. 6, pp. 1709\u00151717, Jun. 2020.\n[271] J. Wu, S.-H. Zhong, and Y. Liu, ``Dynamic graph convolutional network\nfor multi-video summarization,'' Pattern Recognit., vol. 107, Nov. 2020,\nArt. no. 107382.\n[272] S.-H. Zhong, J. Wu, and J. Jiang, ``Video summarization via spatio-\ntemporal deep architecture,'' Neurocomputing, vol. 332, pp. 224\u0015235,\nMar. 2019.\n[273] A. Rav-Acha, Y. Pritch, and S. Peleg, ``Making a long video short:\nDynamic video synopsis,'' in Proc. IEEE Comput. Soc. Conf. Comput.\nVis. Pattern Recognit. (CVPR), vol. 1, Jun. 2006, pp. 435\u0015441.\n[274] Y. He, Z. Qu, C. Gao, and N. Sang, ``Fast online video synopsis based\non potential collision graph,'' IEEE Signal Process. Lett., vol. 24, no. 1,\npp. 22\u001526, Jan. 2017.\n[275] Y. He, C. Gao, N. Sang, Z. Qu, and J. Han, ``Graph coloring based surveil-\nlance video synopsis,'' Neurocomputing, vol. 225, pp. 64\u001579, Feb. 2017.\n[276] K. B. Bakurt and R. Same, ``Improved adaptive background subtrac-\ntion method using pixel-based segmenter,'' Nat. Inst. Standards Tech-\nnol., Gaithersburg, MD, USA, Tech. Rep., 2017. [Online]. Available:\nhttps://dspace5.zcu.cz/handle/11025/29610\n[277] K. B. Bakurt and R. Samet, ``Long-term multiobject tracking using alter-\nnative correlation \u001clters,'' Turkish J. Electr. Eng. Comput. Sci., vol. 26,\nno. 5, pp. 2246\u00152259, 2018.\n[278] L. Lin, W. Lin, W. Xiao, and S. Huang, ``An optimized video synopsis\nalgorithm and its distributed processing model,'' Soft Comput., vol. 21,\nno. 4, pp. 935\u0015947, Feb. 2017.\n[279] S. A. Ahmed, D. P. Dogra, S. Kar, R. Patnaik, S.-C. Lee, H. Choi,\nG. P. Nam, and I.-J. Kim, ``Query-based video synopsis for intelligent\ntraf\u001cc monitoring applications,'' IEEE Trans. Intell. Transp. Syst., vol. 21,\nno. 8, pp. 3457\u00153468, Aug. 2020.\n[280] X. Zhu, J. Liu, J. Wang, and H. Lu, ``Key observation selection-based\neffective video synopsis for camera network,'' Mach. Vis. Appl., vol. 25,\nno. 1, pp. 145\u0015157, Jan. 2014.\n[281] A. Mahapatra, P. K. Sa, B. Majhi, and S. Padhy, ``MVS: A multi-view\nvideo synopsis framework,'' Signal Process., Image Commun., vol. 42,\npp. 31\u001544, Mar. 2016.\n[282] Z. Zhang, Y. Nie, H. Sun, Q. Zhang, Q. Lai, G. Li, and M. Xiao, ``Multi-\nview video synopsis via simultaneous object-shifting and view-switching\noptimization,'' IEEE Trans. Image Process., vol. 29, pp. 971\u0015985, 2020.\nVOLUME 8, 2020 152419\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\n[283] J. Ferryman and A.-L. Ellis, ``Performance evaluation of crowd image\nanalysis using the PETS2009 dataset,'' Pattern Recognit. Lett., vol. 44,\npp. 3\u001515, Jul. 2014.\n[284] P. Foggia, G. Percannella, A. Saggese, and M. Vento, ``Real-time\ntracking of single people and groups simultaneously by contextual\ngraph-based reasoning dealing complex occlusions,'' in Proc. IEEE\nInt. Workshop Perform. Eval. Tracking Surveill. (PETS), Jan. 2013,\npp. 29\u001536.\n[285] S. Dasiopoulou, V. Mezaris, I. Kompatsiaris, V.-K. Papastathis, and\nM. G. Strintzis, ``Knowledge-assisted semantic video object detection,''\nIEEE Trans. Circuits Syst. Video Technol., vol. 15, no. 10, pp. 1210\u00151224,\nOct. 2005.\n[286] A. Garca and J. Bescs, ``Video object segmentation based on feedback\nschemes guided by a low-level scene ontology,'' in Proc. Int. Conf. Adv.\nConcepts Intell. Vis. Syst. Berlin, Germany: Springer, 2008, pp. 322\u0015333.\n[287] J. Gmez-Romero, M. A. Patricio, J. Garca, and J. M. Molina,\n``Ontology-based context representation and reasoning for object tracking\nand scene interpretation in video,'' Expert Syst. Appl., vol. 38, no. 6,\npp. 7494\u00157510, Jun. 2011.\n[288] V. Haarslev and R. Mller, ``Racer system description,'' in Proc. Int.\nJoint Conf. Automated Reasoning. Berlin, Germany: Springer, 2001,\npp. 701\u0015705.\n[289] N. D. Bird, O. Masoud, N. P. Papanikolopoulos, and A. Isaacs, ``Detection\nof loitering individuals in public transportation areas,'' IEEE Trans. Intell.\nTransp. Syst., vol. 6, no. 2, pp. 167\u0015177, Jun. 2005.\n[290] B. Laxton, J. Lim, and D. Kriegman, ``Leveraging temporal, contextual\nand ordering constraints for recognizing complex activities in video,'' in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2007, pp. 1\u00158.\n[291] J. Wang, Z. Chen, and Y. Wu, ``Action recognition with multiscale spatio-\ntemporal contexts,'' in Proc. CVPR, 2011, pp. 3185\u00153192.\n[292] A. R. J. Francois, R. Nevatia, J. Hobbs, R. C. Bolles, and J. R. Smith,\n``VERL: An ontology framework for representing and annotating\nvideo events,'' IEEE MultimediaMag., vol. 12, no. 4, pp. 76\u001586,\nOct. 2005.\n[293] L. Snidaro, M. Belluz, and G. L. Foresti, ``Representing and recognizing\ncomplex events in surveillance applications,'' in Proc. IEEE Conf. Adv.\nVideo Signal Based Surveill., Sep. 2007, pp. 493\u0015498.\n[294] J. C. SanMiguel, J. M. Martinez, and . Garcia, ``An ontology for event\ndetection and its application in surveillance video,'' in Proc. 6th IEEE Int.\nConf. Adv. Video Signal Based Surveill., Sep. 2009, pp. 220\u0015225.\n[295] L. Greco, P. Ritrovato, A. Saggese, and M. Vento, ``Abnormal event\nrecognition: A hybrid approach using SemanticWeb technologies,'' in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW),\nJun. 2016, pp. 1297\u00151304.\n[296] L. Chen and C. Nugent, ``Ontology-based activity recognition in intelli-\ngent pervasive environments,'' Int. J. Web Inf. Syst., vol. 5, pp. 410\u0015430,\nNov. 2009.\n[297] D. Cavaliere, S. Senatore, M. Vento, and V. Loia, ``Towards semantic\ncontext-aware drones for aerial scenes understanding,'' in Proc. 13th\nIEEE Int. Conf. Adv. Video Signal Based Surveill. (AVSS), Aug. 2016,\npp. 115\u0015121.\n[298] B. Z. Yao, X. Yang, L. Lin, M. Wai Lee, and S.-C. Zhu, ``I2T: Image\nparsing to text description,'' Proc. IEEE , vol. 98, no. 8, pp. 1485\u00151508,\nAug. 2010.\n[299] M. Xue, S. Zheng, and C. Zhang, ``Ontology-based surveillance video\narchive and retrieval system,'' in Proc. IEEE 5th Int. Conf. Adv. Comput.\nIntell. (ICACI), Oct. 2012, pp. 84\u001589.\n[300] Z. Xu, L. Mei, Y. Liu, and C. Hu, ``Video structural description: A seman-\ntic based model for representing and organizing video surveillance big\ndata,'' in Proc. IEEE 16th Int. Conf. Comput. Sci. Eng., Dec. 2013,\npp. 802\u0015809.\n[301] Z. Xu, Y. Liu, L. Mei, C. Hu, and L. Chen, ``Semantic based representing\nand organizing surveillance big data using video structural description\ntechnology,'' J. Syst. Softw., vol. 102, pp. 217\u0015225, Apr. 2015.\n[302] M. Sah and C. Direkoglu, ``Semantic annotation of surveillance videos\nfor abnormal crowd behaviour search and analysis,'' in Proc. 14th\nIEEE Int. Conf. Adv. Video Signal Based Surveill. (AVSS), Aug. 2017,\npp. 1\u00156.\n[303] F. Sobhani, N. F. Kahar, and Q. Zhang, ``An ontology framework for auto-\nmated visual surveillance system,'' in Proc. 13th Int. Workshop Content-\nBased Multimedia Indexing (CBMI), Jun. 2015, pp. 1\u00157.\n[304] A. Alam, M. N. Khan, J. Khan, and Y.-K. Lee, ``Intellibvr-intelligent\nlarge-scale video retrieval for objects and events utilizing distributed\ndeep-learning and semantic approaches,'' in Proc. IEEE Int. Conf. Big\nData Smart Comput. BigComp), Feb. 2020, pp. 28\u001535.[305] S. O. Ajiboye, P. Birch, C. Chatwin, and R. Young, ``Hierarchical video\nsurveillance architecture: A chassis for video big data analytics and\nexploration,'' Proc. SPIE, vol. 9407, Mar. 2015, Art. no. 94070K.\n[306] C.-F. Lin, S.-M. Yuan, M.-C. Leu, and C.-T. Tsai, ``A framework for\nscalable cloud video recorder system in surveillance environment,'' in\nProc. 9th Int. Conf. Ubiquitous Intell. Comput. 9th Int. Conf. Autonomic\nTrusted Comput., Sep. 2012, pp. 655\u0015660.\n[307] X. Liu, D. Zhao, L. Xu, W. Zhang, J. Yin, and X. Chen, ``A distributed\nvideo management cloud platform using Hadoop,'' IEEE Access, vol. 3,\npp. 2637\u00152643, 2015.\n[308] C. Ryu, D. Lee, M. Jang, C. Kim, and E. Seo, ``Extensible video process-\ning framework in apache Hadoop,'' in Proc. IEEE 5th Int. Conf. Cloud\nComput. Technol. Sci., vol. 2, Dec. 2013, pp. 305\u0015310.\n[309] G. Bradski, ``The OpenCV library,'' Dr. Dobb's J. Softw. Tools, vol. 3,\n2000, Art. no. 2236121.\n[310] M. Ali, A. Anjum, O. Rana, A. R. Zamani, D. Balouek-Thomert,\nand M. Parashar, ``RES: Real-time video stream analytics using\nedge enhanced clouds,'' IEEE Trans. Cloud Comput., early access,\nMay 6, 2020, doi: 10.1109/TCC.2020.2991748.\n[311] B. White, T. Yeh, J. Lin, and L. Davis, ``Web-scale computer vision using\nmapreduce for multimedia data mining,'' in Proc. 10th Int. Workshop\nMultimedia Data Mining, 2010, p. 9.\n[312] R. Pereira, M. Azambuja, K. Breitman, and M. Endler, ``An architecture\nfor distributed high performance video processing in the cloud,'' in Proc.\nIEEE 3rd Int. Conf. Cloud Comput., Jul. 2010, pp. 482\u0015489.\n[313] C. Liu, K. Fan, Z. Yang, and J. Xiu, ``A distributed video share system\nbased on Hadoop,'' in Proc. IEEE 3rd Int. Conf. Cloud Comput. Intell.\nSyst. (CCIS), Nov. 2014, pp. 587\u0015590.\n[314] M. A. Hossain, ``Framework for a cloud-based multimedia surveil-\nlance system,'' Int. J. Distrib. Sensor Netw., vol. 10, no. 5, May 2014,\nArt. no. 135257.\n[315] L. Valentn, S. A. Serrano, R. O. Garca, A. Andrade,\nM. A. Palacios-Alonso, and L. E. Sucar, ``A cloud-based architecture for\nsmart video surveillance,'' Int. Arch. Photogramm., Remote Sens. Spatial\nInf. Sci., vol. 42, p. 99, 2017.\n[316] W. Zhang, P. Duan, Z. Li, Q. Lu, W. Gong, and S. Yang, ``A deep\nawareness framework for pervasive video cloud,'' IEEE Access, vol. 3,\npp. 2227\u00152237, 2015.\n[317] W. Zhang, L. Xu, Z. Li, Q. Lu, and Y. Liu, ``A deep-intelligence frame-\nwork for online video processing,'' IEEE Softw., vol. 33, no. 2, pp. 44\u001551,\nMar. 2016.\n[318] M. A. Uddin, A. Alam, N. A. Tu, M. S. Islam, and Y.-K. Lee, ``SIAT:\nA distributed video analytics framework for intelligent video surveil-\nlance,'' Symmetry, vol. 11, no. 7, p. 911, Jul. 2019.\n[319] H. Zhang, J. Yan, and Y. Kou, ``Ef\u001ccient online surveillance video pro-\ncessing based on spark framework,'' in Proc. Int. Conf. Big Data Comput.\nCommun. Cham, Switzerland: Springer, 2016, pp. 309\u0015318.\n[320] M. A. Uddin, J. B. Joolee, A. Alam, and Y.-K. Lee, ``Human action recog-\nnition using adaptive local motion descriptor in spark,'' IEEE Access,\nvol. 5, pp. 21157\u001521167, 2017.\n[321] H. Wang, X. Zheng, and B. Xiao, ``Large-scale human action recogni-\ntion with spark,'' in Proc. IEEE 17th Int. Workshop Multimedia Signal\nProcess. (MMSP), Oct. 2015, pp. 1\u00156.\n[322] Q. Huang, P. Ang, P. Knowles, T. Nykiel, I. Tverdokhlib, A. Yajurvedi, P.\nDapolito, X. Yan, M. Bykov, C. Liang, M. Talwar, A. Mathur, S. Kulkarni,\nM. Burke, and W. Lloyd, ``SVE: Distributed video processing at facebook\nscale,'' in Proc. 26th Symp. Operating Syst. Princ., Oct. 2017, pp. 87\u0015103.\n[323] W. Zhang, Z. Wang, X. Liu, H. Sun, J. Zhou, Y. Liu, and W. Gong,\n``Deep learning-based real-time \u001cne-grained pedestrian recognition using\nstream processing,'' IET Intell. Transp. Syst., vol. 12, no. 7, pp. 602\u0015609,\nSep. 2018.\n[324] Citilog, Citilog, Bagneux, France, 1997.\n[325] checkvideo. 1998. Checkvideo. Accessed: Dec. 7, 2018. [Online]. Avail-\nable: https://www.checkvideo.com/\n[326] Intell Vision. (2002). Intelli Vision. [Online]. Available: https://www.\nintelli-vision.com/\n[327] Google Inc. (2017). Video Ai. [Online]. Available: https://cloud.google.\ncom/video-intelligence/\n[328] BlueChasm LLC. (2015). Usher in the New Cognitive Era. [Online].\nAvailable: https://bluechasm.com/\n[329] J. H. J. Kornich and C. Casey. (2010). Media Analytics on the Media\nServices Platform. [Online]. Available: https://docs.microsoft.com/en-\ngb/azure/media-services/previous/media-services-analytics-overview\n152420 VOLUME 8, 2020\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\n[330] Google Inc. (2017). Features. [Online]. Available: https://cloud.google.\ncom/video-intelligence/docs/features\n[331] J. G. Collier. (2017). IBM Uses Watson's Power to Unlock Insights\nFrom Video in the Cloud. [Online]. Available: https://www-03.ibm.com/\npress/us/en/pressrelease/52126.wss\n[332] IBM Cloud. (2017). IBM Cloud Object Storage. [Online]. Available:\nhttps://www.ibm.com/cloud/object-storage\n[333] (2017). Cloudant. [Online]. Available: https://www.ibm.com/cloud/\ncloudant\n[334] G. Ananthanarayanan, P. Bahl, P. Bodk, K. Chintalapudi, M. Philipose,\nL. Ravindranath, and S. Sinha, ``Real-time video analytics: The killer app\nfor edge computing,'' Computer, vol. 50, no. 10, pp. 58\u001567, 2017.\n[335] F. Loewenherz, V. Bahl, and Y. Wang, ``Video analytics towards\nvision zero,'' Inst. Transp. Engineers. ITE J., vol. 87, no. 3, p. 25,\n2017.\n[336] H. Qiu, X. Liu, S. Rallapalli, A. J. Bency, K. Chan, R. Urgaonkar,\nB. S. Manjunath, and R. Govindan, ``Kestrel: Video analytics for\naugmented multi-camera vehicle tracking,'' in Proc. IEEE/ACM 3rd\nInt. Conf. Internet-of-Things Design Implement. (IoTDI), Apr. 2018,\npp. 48\u001559.\n[337] T. Gao, Z.-G. Liu, S.-H. Yue, J.-Q. Mei, and J. Zhang, ``Traf\u001cc video-\nbased moving vehicle detection and tracking in the complex environ-\nment,'' Cybern. Syst., vol. 40, no. 7, pp. 569\u0015588, Sep. 2009.\n[338] Y. Lin, L. Li, H. Jing, B. Ran, and D. Sun, ``Automated traf\u001cc\nincident detection with a smaller dataset based on generative adver-\nsarial networks,'' Accident Anal. Prevention, vol. 144, Sep. 2020,\nArt. no. 105628.\n[339] L. Fridman, D. E. Brown, M. Glazer, W. Angell, S. Dodd, B. Jenik,\nJ. Terwilliger, A. Patsekin, J. Kindelsberger, L. Ding, S. Seaman,\nA. Mehler, A. Sipperley, A. Pettinato, B. Seppelt, L. Angell, B. Mehler,\nand B. Reimer, ``MIT advanced vehicle technology study: Large-\nscale naturalistic driving study of driver behavior and interac-\ntion with automation,'' 2017, arXiv:1711.06976. [Online]. Available:\nhttp://arxiv.org/abs/1711.06976\n[340] W. Xu, H. Zhou, N. Cheng, F. Lyu, W. Shi, J. Chen, and X. Shen, ``Internet\nof vehicles in big data era,'' IEEE/CAA J. Automatica Sinica , vol. 5, no. 1,\npp. 19\u001535, Jan. 2018.\n[341] J. Wang, G. Bebis, M. Nicolescu, M. Nicolescu, and R. Miller, ``Improv-\ning target detection by coupling it with tracking,'' Mach. Vis. Appl.,\nvol. 20, no. 4, pp. 205\u0015223, Jun. 2009.\n[342] Y. J. Tsai, Z. Hu, and C. Alberti, ``Detection of roadway sign condition\nchanges using multi-scale sign image matching (M-SIM),'' Photogramm.\nEng. Remote Sens., vol. 76, no. 4, pp. 391\u0015405, Apr. 2010.\n[343] R. Ayachi, M. A\u001cf, Y. Said, and M. Atri, ``Traf\u001cc signs detection for\nreal-world application of an advanced driving assisting system using\ndeep learning,'' Neural Process. Lett., vol. 51, no. 1, pp. 837\u0015851,\nFeb. 2020.\n[344] A. M. Callan, R. Osu, Y. Yamagishi, D. E. Callan, and N. Inoue, ``Neural\ncorrelates of resolving uncertainty in driver's decision making,'' Hum.\nBrain Mapping, vol. 30, no. 9, pp. 2804\u00152812, Sep. 2009.\n[345] C. Yan, F. Coenen, and B. Zhang, ``Driving posture recognition by convo-\nlutional neural networks,'' IET Comput. Vis., vol. 10, no. 2, pp. 103\u0015114,\nMar. 2016.\n[346] S. Gite and H. Agrawal, ``Early prediction of Driver's action using deep\nneural networks,'' Int. J. Inf. Retr. Res., vol. 9, no. 2, pp. 11\u001527, Apr. 2019.\n[347] S. Fleck and W. Straer, ``Smart camera based monitoring system\nand its application to assisted living,'' Proc. IEEE, vol. 96, no. 10,\npp. 1698\u00151714, Oct. 2008.\n[348] Z. Zhou, X. Chen, Y.-C. Chung, Z. He, T. X. Han, and J. M. Keller,\n``Activity analysis, summarization, and visualization for indoor human\nactivity monitoring,'' IEEE Trans. Circuits Syst. Video Technol., vol. 18,\nno. 11, pp. 1489\u00151498, Nov. 2008.\n[349] J. Aertssen, M. Rudinac, and P. Jonker, Fall and Action Detection in\nElderly Homes. Maastricht, The Netherlands: AAATE, 2011.\n[350] D. Brulin, Y. Benezeth, and E. Courtial, ``Posture recognition based on\nfuzzy logic for home monitoring of the elderly,'' IEEE Trans. Inf. Technol.\nBiomed., vol. 16, no. 5, pp. 974\u0015982, Sep. 2012.\n[351] G. Sacco, V. Joumier, N. Darmon, A. Dechamps, A. Derreumaux,\nJ.-H. Lee, J. Piano, N. Bordone, A. Konig, B. Teboul, R. David,\nO. Guerin, F. Bremond, and P. Robert, ``Detection of activities of daily\nliving impairment in Alzheimer's disease and mild cognitive impairment\nusing information and communication technology,'' Clin. Interventions\nAging, vol. 7, p. 539, Dec. 2012.[352] M. Yu, A. Rhuma, S. M. Naqvi, L. Wang, and J. Chambers, ``A posture\nrecognition-based fall detection system for monitoring an elderly person\nin a smart home environment,'' IEEE Trans. Inf. Technol. Biomed., vol. 16,\nno. 6, pp. 1274\u00151286, Nov. 2012.\n[353] W. Choi, C. Pantofaru, and S. Savarese, ``A general framework for track-\ning multiple people from a moving camera,'' IEEE Trans. Pattern Anal.\nMach. Intell., vol. 35, no. 7, pp. 1577\u00151591, Jul. 2013.\n[354] V. Pragada, ``Intrusion detection and video surveillance activation and\nprocessing,'' U.S. Patent 14 172 880, Aug. 6, 2015.\n[355] Z. Zhao, H. Li, R. Zhao, and X. Wang, ``Crossing-line crowd counting\nwith two-phase deep neural networks,'' in Proc. Eur. Conf. Comput. Vis.\nCham, Switzerland: Springer, 2016, pp. 712\u0015726.\n[356] G. Xu and W. K. Cobb, ``Loitering detection in a video surveillance\nsystem,'' U.S. Patent 9 208 675, Dec. 8, 2015.\n[357] P. Shivakumara, D. Tang, M. Asadzadehkaljahi, T. Lu, U. Pal, and\nM. H. Anisi, ``CNN-RNN based method for license plate recognition,''\nCAAI Trans. Intell. Technol., vol. 3, no. 3, pp. 169\u0015175, Sep. 2018.\n[358] B. Krausz and R. Herpers, ``MetroSurv: Detecting events in sub-\nway stations,'' Multimedia Tools Appl., vol. 50, no. 1, pp. 123\u0015147,\nOct. 2010.\n[359] J.-L. Shih, C.-C. Han, and K.-C. Yan, ``Illegal entrant detection at a\nrestricted area in open spaces using color features,'' in Proc. 40th Annu.\nInt. Carnahan Conf. Secur. Technol., Oct. 2006, pp. 68\u001574.\n[360] H. Singh, ``Applications of intelligent video analytics in the \u001celd of retail\nmanagement: A study,'' in Supply Chain Management Strategies and Risk\nAssessment in Retail Environments. Hershey, PA, USA: IGI Global, 2018,\npp. 42\u001559.\n[361] H. Xu, P. Lv, and L. Meng, ``A people counting system based on head-\nshoulder detection and tracking in surveillance video,'' in Proc. Int. Conf.\nComput. Design Appl., vol. 1, Jun. 2010, pp. V1\u0015394.\n[362] I. Haritaoglu and M. Flickner, ``Attentive billboards: Towards to video\nbased customer behavior understanding,'' in Proc. 6th IEEE Workshop\nAppl. Comput. Vis. (WACV), Dec. 2002, pp. 127\u0015131.\n[363] Y. Hu, L. Cao, F. Lv, S. Yan, Y. Gong, and T. S. Huang, ``Action detection\nin complex scenes with spatial and temporal ambiguities,'' in Proc. IEEE\n12th Int. Conf. Comput. Vis., Sep. 2009, pp. 128\u0015135.\n[364] R. Sicre and H. Nicolas, ``Human behaviour analysis and event recog-\nnition at a point of sale,'' in Proc. 4th Paci\u001cc-Rim Symp. Image Video\nTechnol., Nov. 2010, pp. 127\u0015132.\n[365] A. D. Popescu, A. Balmin, V. Ercegovac, and A. Ailamaki, ``PRE-\nDIcT: Towards predicting the runtime of large scale iterative ana-\nlytics,'' Proc. VLDB Endowment, vol. 6, no. 14, pp. 1678\u00151689,\nSep. 2013.\n[366] S. Amershi, M. Cakmak, W. B. Knox, and T. Kulesza, ``Power to the\npeople: The role of humans in interactive machine learning,'' AI Mag.,\nvol. 35, no. 4, pp. 105\u0015120, 2014.\n[367] Y.-K. Kwok and I. Ahmad, ``Static scheduling algorithms for allocating\ndirected task graphs to multiprocessors,'' ACM Comput. Surv., vol. 31,\nno. 4, pp. 406\u0015471, Dec. 1999.\n[368] X. Zhu, J. Wang, J. Wang, and X. Qin, ``Analysis and design of fault-\ntolerant scheduling for real-time tasks on Earth-observation satellites,''\ninProc. 43rd Int. Conf. Parallel Process., Sep. 2014, pp. 491\u0015500.\n[369] E. Bortnikov, A. Frank, E. Hillel, and S. Rao, ``Predicting execution\nbottlenecks in map-reduce clusters,'' in presented at the 2012.\n[370] Y. Zhai, Y.-S. Ong, and I. W. Tsang, ``The emerging `big dimensionality,'''\nIEEE Comput. Intell. Mag., vol. 9, no. 3, pp. 14\u001526, Aug. 2014.\n[371] L. Gao, J. Song, X. Liu, J. Shao, J. Liu, and J. Shao, ``Learning in high-\ndimensional multimedia data: The state of the art,'' Multimedia Syst.,\nvol. 23, no. 3, pp. 303\u0015313, Jun. 2017.\n[372] A. Guttman, ``R-trees: A dynamic index structure for spatial searching,''\ninProc. ACM SIGMOD Int. Conf. Manage. Data (SIGMOD), New York,\nNY, USA, 1984, pp. 47\u001557, doi: 10.1145/602259.602266.\n[373] P. Ciaccia, M. Patella, and P. Zezula, ``M-tree: An ef\u001ccient access\nmethod for similarity search in metric spaces,'' in Proc. 23rd\nInt. Conf. Very Large Data Bases (VLDB), San Francisco, CA,\nUSA: Morgan Kaufmann, 1997, pp. 426\u0015435. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=645923.671005\n[374] K.-I. Lin, H. V. Jagadish, and C. Faloutsos, ``The TV-tree: An index\nstructure for high-dimensional data,'' VLDB J., vol. 3, no. 4, pp. 517\u0015542,\nOct. 1994.\n[375] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni, ``Locality-sensitive\nhashing scheme based on p-stable distributions,'' in Proc. 20th Annu.\nSymp. Comput. Geometry (SCG), 2004, pp. 253\u0015262.\nVOLUME 8, 2020 152421\n\nA. Alam et al.: Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues\n[376] P. O'Neil, E. Cheng, D. Gawlick, and E. O'Neil, ``The log-structured\nmerge-tree (LSM-tree),'' Acta Inf., vol. 33, no. 4, pp. 351\u0015385, Jun. 1996.\n[377] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, ``The case\nfor learned index structures,'' in Proc. Int. Conf. Manage. Data, 2018,\npp. 489\u0015504.\n[378] G. De Francisci Morales, ``SAMOA: A platform for mining big data\nstreams,'' in Proc. 22nd Int. Conf. World Wide Web, 2013, pp. 777\u0015778.\n[379] J. Lu, S. C. H. Hoi, J. Wang, P. Zhao, and Z.-Y. Liu, ``Large scale online\nkernel learning,'' J. Mach. Learn. Res., vol. 17, no. 1, pp. 1613\u00151655,\nJan. 2016.\n[380] D. Nallaperuma, R. Nawaratne, T. Bandaragoda, A. Adikari, S. Nguyen,\nT. Kempitiya, D. De Silva, D. Alahakoon, and D. Pothuhera, ``Online\nincremental machine learning platform for big data-driven smart traf-\n\u001cc management,'' IEEE Trans. Intell. Transp. Syst., vol. 20, no. 12,\npp. 4679\u00154690, Dec. 2019.\n[381] J. Friedman, T. Hastie, and R. Tibshirani, The Elements of Statistical\nLearning, vol. 1, no. 10. New York, NY, USA: Springer, 2001.\n[382] S. Kandel, A. Paepcke, J. M. Hellerstein, and J. Heer, ``Enterprise data\nanalysis and visualization: An interview study,'' IEEE Trans. Vis. Comput.\nGraphics, vol. 18, no. 12, pp. 2917\u00152926, Dec. 2012.\n[383] A. Kumar, R. McCann, J. Naughton, and J. M. Patel, ``Model selection\nmanagement systems: The next frontier of advanced analytics,'' ACM\nSIGMOD Rec., vol. 44, no. 4, pp. 17\u001522, May 2016.\n[384] Z. Zhang, B. Cui, Y. Shao, L. Yu, J. Jiang, and X. Miao, ``PS2: Parameter\nserver on spark,'' in Proc. Int. Conf. Manage. Data, 2019, pp. 376\u0015388.\n[385] J. Jiang, F. Fu, T. Yang, and B. Cui, ``SketchML: Accelerating distributed\nmachine learning with data sketches,'' in Proc. Int. Conf. Manage. Data,\n2018, pp. 1269\u00151284..\n[386] Y. Huang, T. Jin, Y. Wu, Z. Cai, X. Yan, F. Yang, J. Li, Y. Guo, and\nJ. Cheng, ``FlexPS: Flexible parallelism control in parameter server archi-\ntecture,'' Proc. VLDB Endowment, vol. 11, no. 5, pp. 566\u0015579, Jan. 2018.\n[387] R. M. Haralick, ``Performance characterization in computer vision,'' in\nProc. BMVC. London, U.K.: Springer, 1992, pp. 1\u00158.\n[388] W. Frstner, ``DAGM workshop on performance characteristics and\nquality of computer vision algorithms,'' Tech. Univ. Braunschweig,\nBraunschweig, Germany, Tech. Rep., 1997.\n[389] P. J. Phillips and K. W. Bowyer, ``Empirical evaluation of computer vision\nalgorithms,'' IEEE Trans. Pattern Anal. Mach. Intell., vol. 21, no. 4,\npp. 289\u0015290, Apr. 1999.\n[390] H. Christensen and W. Foerstner, ``Special issue on performance evalua-\ntion,'' Mach. Vis. Appl., vol. 9, no. 5, 1997.\n[391] A. T. Nghiem, F. Bremond, M. Thonnat, and V. Valentin, ``ETISEO,\nperformance evaluation for video surveillance systems,'' in Proc. IEEE\nConf. Adv. Video Signal Based Surveill., Sep. 2007, pp. 476\u0015481.\n[392] A. G. Hauptmann and M. G. Christel, ``Successful approaches in the\nTREC video retrieval evaluations,'' in Proc. 12th Annu. ACM Int. Conf.\nMultimedia (MULTIMEDIA), 2004, pp. 668\u0015675.\n[393] The Home Of\u001cce Scienti\u001cc Development Branch, ``Imagery library for\nintelligent detection systems (i-LIDS),'' in Proc. IET Conf. Crime Secur.,\n2006, pp. 445\u0015448.\n[394] D. Singh and C. K. Reddy, ``A survey on platforms for big data analytics,''\nJ. Big Data, vol. 2, no. 1, p. 8, 2015.\n[395] D. Wu, X. Luo, M. Shang, Y. He, G. Wang, and M. Zhou, ``A deep\nlatent factor model for high-dimensional and sparse matrices in recom-\nmender systems,'' IEEE Trans. Syst., Man, Cybern. Syst., early access,\nAug. 15, 2019, doi: 10.1109/TSMC.2019.2931393.\n[396] A. Mazrekaj, I. Shabani, and B. Sejdiu, ``Pricing schemes in cloud\ncomputing: An overview,'' Int. J. Adv. Comput. Sci. Appl., vol. 7, no. 2,\npp. 80\u001586, 2016.\n[397] S. Khalid, S. Wu, A. Alam, and I. Ullah, ``Real-time feedback\nquery expansion technique for supporting scholarly search using\ncitation network analysis,'' J. Inf. Sci., vol. 45, no. 4, Jul. 2019,\nArt. no. 0165551519863346.\n[398] D. Kang, P. Bailis, and M. Zaharia, ``Challenges and opportunities in\nDNN-based video analytics: A demonstration of the Blazeit video query\nengine,'' in Proc. CIDR, 2019, pp. 1\u00154.\n[399] Y. Lu, A. Chowdhery, and S. Kandula, ``VisFlow: A relational platform\nfor ef\u001ccient large-scale video analytics,'' in Proc. ACM Symp. Cloud\nComput. (SoCC), 2016. [Online]. Available: https://www.microsoft.com/\nen-us/research/publication/vis\u001dow-relational-platform-ef\u001ccient-large-\nscale-video-analytics/[400] D. Kang, P. Bailis, and M. Zaharia, ``BlazeIt: Optimizing declarative\naggregation and limit queries for neural network-based video analyt-\nics,'' Proc. VLDB Endowment, vol. 13, no. 4, pp. 533\u0015546, Dec. 2019.\n[Online]. Available: https://doi.org/10.14778/3372716.3372725, doi:\n10.14778/3372716.3372725.\n[401] A. Ghosh and I. Arce, ``Guest editors' introduction: In cloud computing\nwe trust\u0015but should we?'' IEEE Secur. Privacy Mag., vol. 8, no. 6,\npp. 14\u001516, Nov. 2010.\n[402] S. Khusro, A. Alam, and S. Khalid, ``Social question and answer sites:\nThe story so far,'' Program, vol. 51, no. 2, pp. 170\u0015192, Jul. 2017.\n[403] A. Alam, S. Khusro, I. Ullah, and M. S. Karim, ``Con\u001duence of social\nnetwork, social question and answering community, and user reputation\nmodel for information seeking and experts generation,'' J. Inf. Sci.,\nvol. 43, no. 2, pp. 260\u0015274, Apr. 2017.\n[404] M. Crosby, P. Pattanayak, S. Verma, and V. Kalyanaraman, ``Blockchain\ntechnology: Beyond bitcoin,'' Appl. Innov., vol. 2, nos. 6\u001510, p. 71, 2016.\nAFTAB ALAM received the master's degree\nin computer science with a specialization in\nWeb engineering from the Department of Com-\nputer Science, University of Peshawar, Peshawar,\nPakistan. He is currently pursuing the Ph.D. degree\nwith the Department of Computer Science and\nEngineering, Kyung Hee University (Global Cam-\npus), South Korea. His research interests include\nbig data analytics, distributed computing, video\nretrieval, social computing, machine/deep learn-\ning, information service engineering, knowledge engineering, knowledge\ngraph, and graph mining.\nIRFAN ULLAH received the master's degree in\ncomputer science from the National University\nof Sciences and Technology (NUST), Islamabad,\nPakistan. He is currently serving as a Research\nAssistant at the Department of Computer Science\nand Engineering, Kyung Hee University (Global\nCampus), South Korea. His research interests\ninclude natural language processing, social com-\nputing, crisis informatics, machine/deep learning,\nOS design and optimization on memory systems,\nbig data analytics, and distributed computing.\nYOUNG-KOO LEE (Member, IEEE) received the\nB.S., M.S., and Ph.D. degrees in computer sci-\nence from the Korea Advanced Institute of Science\nand Technology, Daejeon, South Korea, in 1992,\n1994, and 2002, respectively. Since 2004, he has\nbeen with the Department of Computer Science\nand Engineering, Kyung Hee University (Global\nCampus). His research interests include data min-\ning, online analytical processing, and big data\nprocessing.\n152422 VOLUME 8, 2020",
  "textLength": 259256
}