{
  "paperId": "3f247b9b9f12ac7ea7ea2d4b35780ef77c95f951",
  "title": "Opportunistic View Materialization with Deep Reinforcement Learning",
  "pdfPath": "3f247b9b9f12ac7ea7ea2d4b35780ef77c95f951.pdf",
  "text": "Opportunistic View Materialization with Deep\nReinforcement Learning\nXi Liang\nUniversity of Chicago\nxiliang@uchicago.eduAaron J. Elmore\nUniversity of Chicago\naelmore@uchicago.eduSanjay Krishnan\nUniversity of Chicago\nskr@uchicago.edu\nABSTRACT\nCarefully selected materialized views can greatly improve\nthe performance of OLAP workloads. We study using deep\nreinforcement learning to learn adaptive view materializa-\ntion and eviction policies. Our insight is that such selec-\ntion policies can be e\u000bectively trained with an asynchronous\nRL algorithm, that runs paired counter-factual experiments\nduring system idle times to evaluate the incremental value\nof persisting certain views. Such a strategy obviates the\nneed for accurate cardinality estimation or hand-designed\nscoring heuristics. We focus on inner-join views and model-\ning e\u000bects in a main-memory, OLAP system. Our research\nprototype system, called DQM , is implemented in Spark-\nSQL and we experiment on several workloads including the\nJoin Order Benchmark and the TPC-DS workload. Results\nsuggest that: (1) DQM can outperform heuristic when their\nassumptions are not satis\fed by the workload or there are\ntemporal e\u000bects like period maintenance, (2) even with the\ncost of learning, DQM is more adaptive to changes in the\nworkload, and (3) DQM is broadly applicable to di\u000berent\nworkloads and skews.\n1. INTRODUCTION\nCarefully selected materialized views can greatly improve\nthe performance of OLAP workloads. We explore oppor-\ntunistic materialization (OM) [18], where a database pre-\nemptively caches important query (or sub-query) results for\nfuture use. In an ideal world, OLAP systems would aggres-\nsively persist any result that could possibly be useful in the\nfuture. However, practical systems have resource constraints\nand usage patterns that are constantly evolving, and results\nthat seem currently relevant can fall into disuse in the future.\nFurthermore, maintaining a very large number of views can\nplace a burden on query optimizer to select which views to\nuse for a given query. Therefore, the core technical problem\nin the design of OM systems is straight-forward: an e\u000bec-\ntive dynamic view creation and eviction policy under storage\nconstraints.\nThe database community has extensively studied view rec-\nommendation systems that take in a historical query work-\nload, a database schema, and possibly a cost model to rec-\nommend the best views to create [28, 9, 5, 27, 1, 33]. Such\ntechniques are retrospective because one implicitly assumes\nthat future queries and data are similar to what was seen in\nthe past, and choices that are good in retrospect are likely to\nbe good in the future. While retrospective approaches are\nprincipled, in the sense that they optimize a well-de\fned\ncriterion, they su\u000ber the obvious limitations [15]: they willnot perform well for ad hoc queries, evolving workloads, or\nstorage constraint changes.\nTruly dynamic strategies adapt to changing environments\nthrough eviction and re-creation actions. Existing dynamic\nview selection work is largely heuristic based. They de\fne\nscoring criteria to quantify the value of keeping a view ma-\nterialized ranging from the simplest Least-Recently-Used or\nLeast-Frequently-Used approaches [10, 14, 30] to more so-\nphisticated cost-model based approaches [28, 24]. However,\nwith LRU and the known \\sequential \rooding\" failure case,\nexisting dynamic policies are brittle by nature and can have\nsubtle blind spots.\nFigure 1 compares two state-of-the-art dynamic criteria,\nHAWC [28] and Recycler [24], on two di\u000berent workloads of\n1000 queries (derived from the Join Order Benchmark and\nTPC-DS). We implemented both heuristics in SparkSQL.\nHAWC prioritizes usage frequency and Recycler prioritizes\ncostly views (more details of the baselines and workloads we\nuse can be found in Table 2 and Table 3 in Section 6.1.1). We\nobserve drastic performance di\u000berences across the bench-\nmarks. Recycler works well when it can very signi\fcantly\nimprove a small number of expensive queries, as in the Join\nOrder Benchmark where a few queries involve >10 way\njoins. Exactly the opposite happens on TPC-DS, where it\npersists super\ruous, large views and actually hurts perfor-\nmance.\nFeedback from real execution times can ameliorate this\nbrittleness. Rather than relying on a heuristic, we can ob-\nserve whether the creation of a view has a net positive or\nnegative impact on subsequent query latencies. This is fun-\ndamentally a learning problem as bene\fcial decisions should\nbe remembered for the future and adverse decisions should\nbe avoided. This broad idea is inspired by recent works in\nquery optimization, where actual runtimes are used to in-\nform/optimize future plans [23, 6, 22, 17]. To achieve this\ngoal, we need a learning framework that can automatically\nand continuously update its model based on delayed obser-\nvations.\nReinforcement Learning (RL) studies techniques that learn\nhow to control general stateful systems (e.g., a database\nwith persisted views) [31, 32]. A working de\fnition of RL\nis \\learning by doing\"; the algorithm takes actions and ob-\nserves feedback via a performance metric (e.g., query run-\ntime). It assigns credit or blame to actions based on the\nfeedback, and can even account for delayed e\u000bects. As more\nfeedback is observed, the learned behavior is increasingly\ninformed. In principle, an RL approach can learn which\nnew views are valuable to materialize given the current sys-\n1arXiv:1903.01363v1  [cs.DB]  4 Mar 2019\n\nJOB0200040006000Cumulative time cost (s)DQM\nHAWC\nRECYCLER\nTPC-DS050010001500DQM\nHAWC\nRECYCLERFigure 1: We compare state-of-the-art dynamic view\nselection criteria, HAWC and Recycler, on Spark-\nSQL for the Join Order Benchmark and TPC-DS\nbased workloads and the same storage constraint.\nHAWC and Recycler are sensitive to their partic-\nular heuristics and behave very di\u000berently on the\ntwo workloads. Our approach, DQM , learns a pre-\ndictive model based on actual runtime feedback so\nit is more robust and outperforms both heuristics\non overall time cost of the two workloads. DQM is\ncompetitive with these baselines even when learning\ntime is included in the cumulative cost.\ntem state (what views are currently persisted). This learn-\ning process can automatically adapt to changing workloads\nbased on the observed performance. It does not require a\nhard-coded heuristic nor does it need to explicitly generate\nan anticipated workload|it's predictive model is simply a\n\\means-to-an-end\" in terms of minimizing overall query la-\ntency. The caveat is that has to be able to assign credit or\nblame to good and bad decisions it makes purely from how\nthe queries execute.\nThis is the crux of the algorithmic challenge in applying\nRL in OM systems|ascertaining the net bene\ft of a view\nis di\u000ecult. Any system either makes a choice to use a view\nor not during query optimization, and the learning agent\nonly observes the \fnal runtime of one of these choices{and\ndoes not know the marginal e\u000bect with respect to the other\nchoice. We lack the \\paired\" experiment, where we observe\nthe same query with and without the view, thereby quanti-\nfying the reward of creating a view [4]. If the same queries\n(or similar queries) do not frequently repeat, the amount of\ntime needed to learn an e\u000bective and adaptive materializa-\ntion policy will be prohibitive.\nOur insight is that OM systems need a new type of asyn-\nchronous RL algorithm that runs such paired experiments\nin the background. For every query, the system identi\fes a\nset of eligible views that can be opportunistically created.\nThe scope of the current work is to focus on inner join views,\nbut the technique is more general. The system proactively\ntakes the decision it thinks is best at the time using its query\noptimizer (possibly using no views). The counter-factual de-\ncision(s), the ones that the system did not take, are queued\ninto an experiment bu\u000ber. We simplify the experimentation\nproblem by assuming an in-memory database with no extra-\nneous unobserved state (e.g., the bu\u000ber pool state or caching\ne\u000bects). Therefore, we can independently schedule and run\nthese experiments during idle times producing retroactive\nmarginal utility metrics for each view. Our system can fur-\nther model view refresh costs, but is not optimized for OLTP\nsystems where these refresh events might be very frequent.We implement this model in a prototype OM system called\nDeep Q-Materialization ( DQM ).DQM contains three main\ncomponents: (1) An online query miner that analyzes a trace\nof SQL queries to identify candidate views for the current\nquery to opportunistically materialize, (2) a RL agent that\nselects from the set of candidates, and (3) an eviction policy\nthat selects views to delete. DQM is integrated with Spark-\nSQL. The adaptive policy interacts with the Spark environ-\nment through a RESTful API and can easily be ported to\nother SQL-based data processing systems.\nFigure 1 shows the potential advantage of DQM . Over\nworkloads of 1000 queries DQM is competitive with the best\nof the heuristics on each workload in terms of cumulative\nquery latency. This is even including the time needed to\nlearn the selection model. Further experiments \fnd that\nDQM can match or outperform standard heuristics policies\nacross 5 di\u000berent temporal query patterns on two di\u000berent\nworkloads. DQM maximizes utilization of available storage\nfor the given workload and query processing engine.\nIn summary, this paper makes the following contributions:\n\u000fWe formalize online view selection in opportunistic\nmaterialization systems as a Markov Decision Process\n(MDP).\n\u000fWe propose a new asynchronous reinforcement learn-\ning algorithm, based on the Double DQN model, to\noptimize this MDP objective online.\n\u000fWe propose a new credit-based eviction model that\ncan enforce a hard storage constraint on views created\nby the learned selection policy.\n\u000fWe compare our approach to classical and state-of-\nthe-art baselines to demonstrate DQM 's adaptivity, la-\ntency, and robustness.\nThe remainder of this paper is structured as follows. Sec-\ntion 2 gives an overview of related work. Section 3 presents\nour problem setting and system architecture. In section 4,\nwe discuss technical details of our reinforcement learning ap-\nproach. Section 5 describes the design of our eviction policy.\nSection 6 presents the experimental evaluation of our sys-\ntem. Finally, Section 7 concludes the paper with discussion.\n2. BACKGROUND\nHere we overview prior approaches to OM, discuss rele-\nvant reactive and predictive policies, and review reinforce-\nment learning.\n2.1 Motivation and Applications\nWe borrow the term \\opportunistic materialization\" [18]\nthat describes automatic persistence in large-scale data pro-\ncessing systems like Hive and Pig. While very di\u000berent from\nour work (the aforementioned persistence was for intra-task\noptimization), we use the term opportunistic to describe any\nmaterialization that is an artifact of execution and not ex-\nplicitly de\fned by a human database administrator.\nReusing previously computed intermediate results across\nqueries can signi\fcantly improve overall throughput and la-\ntency. While the general idea has been studied before, we\nbelieve there are several trends that encourage us to revisit\nthis problem. First, due to advances in natural language\nprocessing and computer vision, compute-bound UDFs for\n2\n\nmachine learning inference are increasingly common. Avoid-\ning additional re-computation of previously computed values\ncan greatly improve query processing. Second, the growth in\ncloud-based database o\u000berings provide individual users with\nlarger storage constraints and more \rexibility to materialize\na large number of views. Finally, batch data processing sys-\ntems like SparkSQL are increasingly fast enough for ad hoc\nquery processing and are used in a data exploration context\nwhere many related queries are executed in quick succession.\nDespite this new need, existing heuristic-based approaches\nare limited. As the example in Figure 1 shows, there is no\n\\one-size \fts all\" heuristic. It is challenging to decide a pri-\noriwhether a heuristic-based approach will even work for a\nworkload. More subtly, these heuristics can be at the mercy\nof the DBMS's cost estimation and query optimizer and ac-\ntually hurt performance. We believe this is an opportunity\nfor adaptive online approaches like DQM that use actual\nobserved query latencies as feedback.\n2.2 Retrospective Policies\nThe classical approaches of static view selection, which se-\nlect the best materialized views from a given input candidate\nview set under storage and/or maintenance constraints, are\none extreme of the design space for OM. These approaches\nrecommend the best views to create based on a query work-\nload and storage constraints [28, 9, 5, 27, 1, 33].\nIn these approaches, \\optimality\" is well-de\fned. They\n\fnd the best set of views that satisfy the storage constraint\nand that most improve the estimated query execution cost of\nthe entire workload. The downside is that one only searches\nover \\static\" strategies, where the views are created upfront.\nThese systems have di\u000eculty reasoning about evicting and\nre-creating views. Even if we were to periodically run such\nview recommendation tools, we would have a number of dif-\n\fcult, unresolved questions: how to window the workload,\nhow to penalize view creation costs, and how frequently to\nre-run a recommendation tool.\n2.3 Reactive and Predictive Policies\nDynamic strategies ostensibly address these issues. Dy-\nnaMat [14] and WATCHMAN [30] were seminal projects in\ndynamic materialized view management. We term these ap-\nproaches \\reactive\" because rather than purely relying on a\nhistorical workload, they react to transient usage patterns.\nSystems in this space have to solve multiple problems: what\nviews to materialize [21, 29], when to evict views [10], and\nhow to select which views to use [7]. Older systems borrowed\nstrategies from database paging (e.g., LRU), and state-of-\nthe-art systems apply more sophisticated scoring heuristics\nthat account for creation and usage costs. HAWC [28] scores\nviews based on a cost-model and maintains a table of such\nscores for persisted views. New views that have a higher\nscore than those in the table force an eviction event. The\nscores in the table are windowed to consider only the latest\nKqueries to ensure adaptivity. RECYCLER [25] prioritizes\nthe most expensive views (in terms of creation cost). The\nreasoning being that these views are harder to re-create if\nthey are evicted. An intriguing variant of these ideas is to\nform a predictive model that forecasts the type and distri-\nbution of queries one may encounter [20]. As far as we can\ntell, such a predictive approach has not yet been truly ap-\nplied to materialized view selection (Ma et al. only studyindex creation) and defer a detailed exploration of workload\nforecasting for future work.\n2.4 Reinforcement Learning\nReinforcement Learning (RL) studies algorithms that learn\nhow to control a stateful system. In RL, a hypothetical\nlearning agent takes decisions to a\u000bect the state of the sys-\ntem. After each decision the system updates its state (pos-\nsibly non-deterministically), and then, the agent observes\na \\reward\", or a score of how good that decision is. The\nobjective for the agent is to learn a policy , a function that\nautomatically takes a decision based on the current state,\nwith the maximum long term reward.\nMathematically, the interaction between the agent and the\nsystem is described by a Markov Decision Process (MDP),\nwhich is a 6-tuple hS;A;p0;p;R;\ri, whereSdenotes the\nstate space (the set of all possible states), Athe action\nspace (set of all possible decisions), p0the initial state dis-\ntribution (how the system starts out), p(st+1jst;at) the\nstate transition distribution (how the state changes given\na decision), R(st;at)2Ris the reward function, and \r2\n[0;1) the discount factor (a weight to discount future re-\nwards). The objective of an MDP is to \fnd a decision pol-\nicy, a probability distribution over actions \u0019:S7! \u0001(A).\nA policy\u0019induces the distribution over trajectories \u0018=\n[(s0;a0);(s1;a1);:::;(sN;aN)]:\nP\u0019(\u0018) =p0(x0)T\u00001Y\nt=0\u0019(atjst)p(st+1jst;at):\nThevalue of a policy is its expected total discounted reward\nover trajectories\nV\u0019=E\u0018\u0018P\u0019\"T\u00001X\nt=0\rtR(st;at)#\n:\nThe objective is to \fnd a policy in a class of allowed policies\n\u0019\u00032\u0005 to maximize the return:\n\u0019\u0003= arg max\n\u00192\u0005V\u0019 (1)\nRL algorithms are empirical solutions to MDPs when an-\nalytic models for pandRare not precisely known. In the\npurest form, the agent starts o\u000b with no prior knowledge\nabout how to control the system. The agent takes random\ndecisions (exploration, and collects a time-series of observa-\ntions of the states visited, the actions taken, and the e\u000bects\nobserved:\nxi= (s;a;r;s0)\nDi\u000berent algorithms utilize these observations in di\u000bering\nways, but the essence is to build a predictive model that\n\fnds actions that result in the longest long term bene\ft\n(even if the instantaneous reward is small).\n2.5 What is Missing?\nWe believe that RL is an important missing piece in OM\nsystems and can facilitate intelligent creation and eviction\npolicy. Materialization is not like paging: there is a complex\ninterplay between immediate e\u000bects (use) and long-term ef-\nfects (the opportunity cost of storing a view). Even dis-\ncounting other uncertainty in the DBMS, like errors in query\noptimizer cost estimation, these are e\u000bects that are funda-\nmentally hard to encode as \fxed heuristics.\n3\n\nThus, we advocate for an RL approach that is grounded\nin real run-times. We are certainly not the \frst to consider\n\\learning\" (or more broadly statistical estimation) in the\nquery optimizer [23, 6, 13, 26, 22, 17]; however, we believe\nthat the problem setting described in this paper is novel.\nThe \frst step towards a practical OM system is to develop\na framework that learns such a policy through experimen-\ntation.\nUnfortunately, a direct application of RL will not work.\nThe natural reward function would be the time-improvement\nfor a future query after creating a view. Barring the use of\na cost model (Section 6.6 explains why cost models can be\nvery inaccurate), we cannot directly observe this quantity.\nOur query optimizer will select to either use or not use a\nview, thus, there is an unknown \\control\" experiment to\naccurately evaluate the bene\ft of creating the view. Our\nkey algorithmic insight is that RL algorithms can be e\u000bec-\ntively trained with a series of paired counter-factual exper-\niments ; how more e\u000ecient is the system with a particular\nview materialized? These experiments can be queued up an\nasynchronously run. Of course, this requires an assumption\nthat the full runtime state of the database is easy to reason\nabout. A query run with a di\u000berent bu\u000ber pool state can\nhave a very di\u000berent runtime. So, we assume that no such\nissues exists (e.g., in an in-memory database). The results\nof the experiments determine a reward signal that can be\nfed into an reinforcement learning algorithm.\n3. SYSTEM ARCHITECTURE\nIn this section we overview our decision model and our\nsystem architecture that is integrated with SparkSQL.\n3.1 Decision Model\nEvery database Dis a collection of base relations (ta-\nbles) and derived relations (materialized views). Let Q=\n[q0;q1:::] be an in\fnite sequence of read-only queries. These\nqueries are issued to Din the order that they arrive. As\nviews are materialized and deleted, the database state Di\nchanges. The system automatically chooses to re-write queries\ngiven the views that are materialized. Therefore, every\nquery has a latency associated with it, given the current\nstate of the database, and the overall runtime of the work-\nload is de\fned as:\nRuntime =1X\ni=0Latency (qi)\nFor some queries, a suitable rewriting plan will not exist.\nSo, associated with each query qiis a set of new views Vithat\ncan be persisted opportunistically (as the query is executed)\nfor the future. To simplify query planning and selection, we\nselect at most a single view to be persisted at every decision\npoint . When we choose to persist a view v2Vi, the overall\nlatency of the query qilogically decomposes into two parts:\nLatency (qi) =Cost(v) +Query (qi;v);\nnamely, the cost to create the view Cost(v) and the incre-\nmental cost of answering the query with the view v. We\nassume that Query (qi;v) is stateless, i.e., running the query\nagainst di\u000berent database states does not a\u000bect the runtime\nif exactly the same views are used. This means there are no\ncaching e\u000bects or bu\u000ber pool e\u000bects.There is a storage cost to persisting such views, which is\na function of the current database state:\nStorage =1X\ni=0C(Di);\nand there is further a cap on the amount of storage used at\nany given time:\nC(Di)\u0014Capacity\nTherefore, our system possibly needs to evict a view:\nDi+16 v\nAt each time-step, the state of the database increments\nbased on the view creation and eviction actions taken by\nthe policy\u0019:\nDi+1=\u0019(Di;qi)\nProblem 1 (Opportunistic Materialization). Given\na database instance D0and a stream of queries Q, plan a\nset of view creation and eviction operations to:\nmin\n\u00192\u00051X\ni=0Latency (qi)\nsubject to:Di+1=\u0019(Di;qi)\nC(Di)\u0014Capacity\n3.2 Architecture\nWe implemented DQM in SparkSQL and overview the ar-\nchitecture in Figure 3. We found that it was convenient to\nrun the OM system in a separate process outside of Spark\nthat issues the creation and deletion actions. All model\ntraining occurs asynchronously with another process and is\nPython. The system interacts with the Spark environment\nthrough a RESTful API.\nCurrent Scope: DQM currently focuses on inner join\npredicate views, where every view in the system can be ex-\npressed in the following form:\nSELECT *\nFROM R1 ,... , RN\nWHERE C1 AND .... AND CM\nThis is not a fundamental limitation of the Deep RL based\napproach in DQM , but allows for simpler query rewriting to\nuse the views and simpler featurization for views (each view\nis simply featurized by the scope of its conditions). We plan\nto explore more complex views in future work.\nQuery Rewriter: Given a set of materialized views V\nand a query qi, the query rewriter changes the query to use\nthe view. A view v2Vis eligible if its predicate conditions\nare contained in the query. The query rewriter can re-write\na query so that any eligible view can be used. Our system\nsearches through all eligible views and selects the lowest cost\nplan (possibly not using a view at all).\nGIVEN : Set of views V, a queryq\nRETURNS : Rewritten query that uses the view qv\nView Candidate Miner: Suppose a query qdoes not \fnd\nany useful, eligible views. It can materialize one of its own\n4\n\nMV1:  \nR.a = S.b  MV2:  \nS.b = T .c \nMV3:  \nR.a = S.b = T .c Q1: \nSELECT * FROM R,\nS WHERE R.a = S.bQ2:\nSELECT * FROM S,\nT WHERE S.b = T .cFigure 2: DQM considers opportunistic materializa-\ntion view candidates whose predicates have been\nseen at least once before in the workload. We cal-\nculate the transitive closure over the equality pred-\nicates.\nintermediate results for future use. While in principle, the\nsystem could search over all possible views that could be\nopportunistically materialized while executing any plan of\nthe query, the search space would be prohibitively large. As\na heuristic, we only select candidate views whose predicates\nhave been seen before in the workload. We calculate the\ntransitive closure over the equality predicates (Figure 2).\nGIVEN : A query sequence Q\nRETURNS : A set of candidate views V\nView Creation Policy: We need the system to decide\nwhich of the candidate view(s) to materialize. The View\nCreator decides if and when to create a view from the can-\ndidate views:\nGIVEN : A set of candidate views V\nRETURNS : A creation action D v\nView Eviction Policy: Whenever there's room to mate-\nrialize another view, it makes sense to take full advantage of\nthe available storage space. But once we reach the storage\nconstraint, we need to decide which view to evict to make\nroom for the new view. The View Evictor deletes an already\ncreated view in the database:\nGIVEN : The current database D\nRETURNS : A deletion action D6 v\n4. LEARNING MATERIALIZATION\nIn this section, we discuss the core technical contribution\nof the paper: a reinforcement learning approach for adaptive\nview creation.\n4.1 The One View Problem\nReinforcement learning has received intense research in-\nterest in recent years [31, 32]. We start with a simpli\fed\nproblem and discuss the challenges in making this practical.\nConsider the decision of whether to materialize a single view\nv(ignoring opportunistic selection). Let us assume there is\nan automated black-box system process that garbage col-\nlects old views when they fall into disuse. Therefore, our\ndatabase has a one-bit state{ vis currently materialized or\nnot. Our system must simply decide when to apply the\nFigure 3: DQM runs as an independent process that\nissues view creation and deletion actions to Apache\nSparkSQL. A thin wrapper layer around SparkSQL\nmanages the created views and returns any runtime\nresults to DQM .DQM learns from these observations\nand issues creation and deletion events when appro-\npriate. It also issues potential experiments to run.\nunary action to create a view if it's not currently material-\nized.\nWhile seemingly simple, this decision must actually weigh\nthe bene\ft of materializing the view in terms of potential\nquery runtime improvement v.s. the creation cost over a\nworkloadQ:\nR(v) = (X\nq2QImprovement (q;v))\u0000Cost(v)\nViews that are not frequently used are not valuable to create.\n4.1.1 Opportunistic Setting\nOne might wonder why Cost(v) is relevant to consider in\nthe opportunistic setting, where views are created as arti-\nfacts of query execution. Recall from the previous section,\nthat the latency of a query during one of these materializa-\ntion events decomposes into two terms, the view cost and\nthe incremental query cost using that view:\nLatency (qi) =Cost(v) +Query (qi;v):\nBy explicitly decomposing the problem in this way, we can\naccount for scenarios where the creation of a view may force\nan instantaneously suboptimal query plan, but creates a\nview that bene\fts the cohort of other queries in the long\nrung.\nIn our system, \\time-steps\" are synchronized with the\nquery workload, since we only observe an e\u000bect when the\ndatabase is queried. So each query qide\fnes a discrete-time\ndecision point of whether to create the view. However, to\nbe able to apply RL, we need a per-timestep (per-query) re-\nward that quanti\fes the instantaneous bene\ft or harm of an\naction at a particular state. R(v) is not a well-posed reward\nfunction because the creation cost Cost amortizes over the\nentire workload and does not readily decompose into a per-\nquery value. We can apply the following trick to produce a\nconsistent reward function:\nR(q;v) =Improvement (q;v)\u0000Cost(v)\u0001\u000e(v;q)\nNv;\n5\n\nwhere\u000e(v;q) is an indicator function determining whether q\nuses the view or not, and Nqis the number of times the view\nwas used in the past. This means that each relevant query\nincurs a fractional creation cost. It can easily be veri\fed\nthat1:\nR(v) =X\nq2QR(q;v)\nThe per-query cost function allows us to model the deci-\nsion process as an MDP:\nState :M=f0;1gview status, Qworkload until q\nAction :f;;+gcreate the view or do nothing\nReward :R(q;v) improvement minus amortized creation\nPolicy :\u0019(Q;M )7!f;;+gdecision to create view\nOur objective is to \fnd a view creation policy: given the\ncurrent system state (i.e., whether the view is materialized\nor not and the query workload until the current point), de-\ncide the right time to create the view. RL is a framework\nthat learns this policy through trial and error (explore ran-\ndom creation strategies) to optimize the cumulative reward,\norR(v) in our case.\n4.1.2 Counterfacutal Runtime Experiments\nAll RL algorithms today assume instantaneous, oracular\naccess to a reward function. This is not true in our setting.\nEvaluating the Improvement() function requires running a\nquery that the system would not ordinarily run, namely, the\ncounterfactual query that doesn't use the view. This exper-\niment can use a non-trivial amount of system resources. A\nkey challenge will be to hide this overhead.\nA counterfactual scenario is one that is contrary to what\nactually happened. Suppose, we make the decision to ma-\nterialize a view v. Then, suppose that vis used to answer a\nfuture query q(with observed runtime Query (q;v)). There\nis a hypothetical (counterfactual) world in which vwas not\ncreated and qwas answered without using v(with a coun-\nterfactual runtime of Query (qi;;)). We are really interested\ninQuery (q;v)\u0000Query (qi;;), which is the marginal improve-\nment caused by an action we took in the system:\nImprovement(q,v) =Query (q;v)\u0000Query (qi;;):\nHowever, we cannot run both queries (with and without\nthe view) in real time as it would expose additional latency\nto the user. Our system maintains a running bu\u000ber of paired\nexperiments to run. In idle times, it executes these experi-\nments and stores the marginal improvement for each query.\n4.1.3 Asynchronous RL Algorithm\nThe typical anatomy of an RL algorithm is to start with\na randomly initialized policy and take decisions to a\u000bect the\nsystem. It observes the outcomes of its decisions. It periodi-\ncally retrains the model based on these outcomes making the\npolicy increasingly informed. We now highlight the di\u000berent\nparts of our algorithmic framework.\nRolling out (Data Collection): The core component of\nan RL algorithm is the \\rollout\" procedure. Given a policy\n\u0019(whether random or informed), DQM needs to evaluate\n1In practice, we approximately compute the amortization\nfactorN, rather than revising rewards retrospectively. Ad-\nditionally, we can scale Cost(v) by a hyperparameter to ad-\njust unit di\u000berences.\nFigure 4: We diagram the \\rollout\", or data collec-\ntion, process used in DQM . At each time-step, DQM\ndecides whether to create a view or not. A reward is\nreceived if a created view is used AND it improves\na query runtime. Every time a created view is used,\nit queues up an experiment in (B) to run when the\nsystem is idle. Once the experiment is run, that ob-\nservation of improvement is placed in (A) and can\nbe used to improve the policy.\nits e\u000bects by applying it to the system. These observations\nneed to be of the form:\n(state;action;reward;new state)\nAs mentioned in the previous section, DQM collects this data\nin an asynchronous way by maintaining two bu\u000bers: an ex-\nperience bu\u000ber and an experiment bu\u000ber. Figure 4 diagrams\nthis process. At each time-step, DQM decides whether to\ncreate a view or not. A reward is received if a created view\nis used AND it improves a query runtime. Every time a cre-\nated view is used, it queues up a counterfactual experiment\nin (B) to run when the system is idle; run the query with and\nwithout the view. Once the experiment is run, the observed\nimprovement is placed in (A) and can be used to improve\nthe policy. In short, the experience bu\u000ber maintains a set of\ncomplete observations.\nThere is some additional book-keeping that is worth men-\ntioning. Because we only gather such an experience when\na view is hit (when a reward is assigned), even though the\ngoal of the algorithm is to select the best view to create, the\nexperience we collected at the time of view hitting is not\ndirectly related to view creation. Therefore, there's a gap\nbetween a view get created and used, e.g. we create a view\nat time step T but it only get used at time step T+10. To\nmitigate the gap between the two events, we tweaked the\nexperience of ( s;a;r;ns ) to (s0;a;r;ns0) wheres0=s\u0000a\nandns0=s0+a=s. The new experience represents a situ-\nation where a view is created and immediately get hit which\nmeans the action of creating a view is assigned a reward im-\nmediately thus mitigate the gap between view creation and\nview hitting.\nPolicy Update: Our system continuously collects data\nand periodically updates the policy. Our RL algorithm is\nbased on the Deep Q Neural Networks (DQN); which as an\no\u000b-policy algorithm, is robust to asynchronous data collec-\ntion. The DQN algorithm de\fnes a Q-function (similar to\nthe cost-to-go function):\nQ(s;a) =R(s;a) + max\na0Q(S0;a0) (2)\n6\n\nGiven the current state and action, what is the value of this\naction assume future optimal behavior. Of course, this func-\ntion is hypothetical since having this function would imply\nhaving an optimal policy. DQN iteratively approximates\nthis function from data. Let Q\u0012be a parametrized function\n(e.g., represented by a neural network):\nQ\u0012(fs;fa)\u0019Q(s;a)\nwherefsis afeature vector representing the state and fa\nis a feature vector representing the creation decision. \u0012is\nthe model parameters that represent this function and is\nrandomly initialized at the start. For each training tuple\niin the experience bu\u000ber, one can calculate the following\nlabel, or the \\estimated\" Q-value:\nyi=Ri+ min\na0Q\u0012(s0;a0)\nThefyigcan then be used as labels in a regression problem.\nIfQwere the true Q-function, then the following recurrence\nwould hold:\nQ(s;a) =Ri+ min\na0Q\u0012(s0;a0)\nSo, the learning process, or Q-learning , de\fnes a loss at each\niteration:\nL(Q) =X\nikyi\u0000Q\u0012(s;a)k2\n2\nThen parameters of the Q-function can be optimized with\ngradient descent until convergence.\nThe description above outlines the main theory behind\nQ-Learning. We also applied the tricks commonly used in\npractice like Experience Replay and Double DQNs [12]. Ex-\nperience replay stabilizes DQN training by maintaining a\nbu\u000ber of past observations (rather than truly learning on-\nline). Data are sampled from the bu\u000ber for each model\nupdate. The other optimization technique that we use to\nimprove our RL algorithm is called Double DQNs. This\ntechnique is used to handle noisy estimates of Q-values. As\nshown in Equation 2, we approximate the Q-function by\ncombining the immediate reward and the discounted maxi-\nmum long term value determined by the DQN itself, which\nmeans we are constantly using the DQN to \fnd the best\naction to take while updating it. The problem is that the\nbest action given by a DQN during updating can be noisy\nthus could complicate the learning process. To address this\nproblem, the Double DQNs technique suggests using two\nparametrized neural networks: one network for evaluation\nthe other for updating. We then synchronize the two net-\nworks every time an updating process (e.g. 10 epochs) \fn-\nishes. As a consequence, the learning process become more\nstable.\nFeaturization: DQN requires that each state and action\ntuple is featurized. In the 1-view problem, featurization is\ntrivial. It is simply a 1-bit binary vector indicating whether\nthe view is created or not and another bit representing the\naction to create the view or do nothing.\n4.2 Generalizing to N Views\nFor simplicity, we introduced the algorithm with a single\nview to create. The multiple view case when there is a pool\nof possible views to create is not that much harder. In prin-\nciple, we can think of it as N-independent versions of the\nabove algorithm.However, there are a few major caveats. First, the set\nof views may not be known in advance, and could even be\ndynamic as the workload evolves. Second, views might be\nhighly correlated with each other or even mutually exclusive\n(there is no point creating two very similar views that ex-\nclude each other). So the key change in the N-view version of\nthe RL problem is to additionally record the context of the\nviews, namely, what relations and predicates they consider.\nWe take a featurization approach that is similar to [17], i.e.\nwe focus on the relations (tables) that are involved in each\nview, and encode them with one-hot encoding. Featurizing\nan action is straightforward because one action is simply one\nview. For a state contains multiple views, we perform one-\nhot encoding on the union of relations involved in all alive\nviews. Finally, we concatenate the action vector and state\nvector. Table 1 demonstrates our featurization process on a\nworkload that contains 7 relations.\n5. VIEW EVICTION\nIn a pure RL setting, it is di\u000ecult to enforce a hard con-\nstraint, such as a storage limit, with a learned model. There-\nfore, we have to decouple the creation policy from the evic-\ntion policy, which independently enforces this constraint.\n5.1 Submissive Eviction\nOur eviction policy \\submissive\" to the RL algorithm in\nthe sense that it's objective is to allow the RL algorithm\nto act as optimally as possible while enforcing the storage\nconstraint. Whenever there's room to materialize a desired\nview, it allows the RL algorithm to make the decision. But\nif a certain decision exceeds the allotted space, it attempts\nto free up space such that the constraint can be enforced.\nDue to this inherent dependence on the cost of a view\nand its observed bene\ft, we believe it cannot be addressed\nby conventional eviction policies: (1) recency metrics like\nLRU do not account for how bene\fcial a view is, and (2)\nmost other heuristics do not account for the potential cost\nof re-materializing a view. In other words, if we already paid\nthe price to materialize an expensive view, then we should\nevict a cheaper view even if both views brought the same\nbene\ft. We need an algorithm that is sort of the inverse of\nthe previous RL algorithm; that maintains an estimate of\nthe negative e\u000bects of evicting a view.\n5.2 Algorithm\nFor each created view, the observed value of keeping it\nmaterialized is:\nR\u00001(v) =X\nq2QImprovement (q;v) +Cost(v);\nor the cumulative improvement so far plus the cost of re-\nmaterializing the view. Unlike during creation, where Cost\namortizes over each query, there is no such amortization. If\nwe delete the view there is always a \fxed cost of re-creating\nit. We maintain a running estimate of its current value as a\nmember of the table (Figure 5), each time a view is used by\na query:\nCT+1(v) =CT(v) +R\u00001(v):\nAgain, as with the view creation policy, we may have to tune\nhyper-parameters that scale the sum to account for di\u000bering\nunits or di\u000bering preferences Improvement (q;v)+\r\u0003Cost(v) .\n7\n\nView Featurization State Action Featurization\nView Tables Encoding Action State Encoding\nMV1 A, B [1, 1, 0, 0, 0, 0, 0] MV1 MV2, MV3 [1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]\nMV2 B, C [0, 1, 1, 0, 0, 0, 0] MV2 MV1 [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\nMV3 A, D, E [1, 0, 0, 1, 1, 0, 0] MV3 MV2, MV4 [1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0]\nMV4 C, D, E [0, 0, 1, 1, 1, 0, 0] MV4 MV1, MV2, MV3 [0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0]\nTable 1: Featurization of a workload of 7 relations A,B,C,D,E,F,G\nFigure 5: DQM maintains a table to prioritize which\nviews to delete to enforce the storage constraint.\nThis table is continuously updated with rewards ac-\ncrued in the experience bu\u000ber.\nOne challenge is modeling dynamic workloads. If a view\nwas very valuable in the early stage of a workload but then\nfalls into disuse the credit table might have an in\rated score.\nIn practice, we decay the credits of each view by a rate of\n\u00162(0;1]2:\nCT+1(v) =\u0016\u0001CT(v) +R\u00001(v):\nGiven the credit table, our eviction policy is to simply evict\nthe view of lowest credit until su\u000ecient space is freed up for\nthe new view.\n5.3 View Maintenance Through Eviction\nWe consider an OLAP setting where the materialized views\nare maintained infrequently. In this problem setting, it is\nsu\u000ecient to treat view maintenance as an automatic evic-\ntion event. For every view currently materialized, if one\nof its base tables have been updated, we evict it from the\npool. After eviction, we additionally have to \rush the exper-\niment bu\u000ber of any queued up experiments that use the view\nsince the paired experimental results are now stale. Since we\nexplicitly model Cost(v) and how it amortizes, our reward\nfunction is consistent under maintenance events, as creating\na view that is repeatedly evicted will force the view to incur\nhigh creation costs that do not amortize well. This model\n2In practice, it is possible for a view to cause negative im-\nprovement, we do not decay a negative credit and the hyper-\nparameter we use to scale the cost will also be negative so\nthat the cost became a penalty instead of a reward to its\ncredit.is su\u000ecient to capture maintenance through re-computation\nand not incremental view maintenance. We hope to explore\nmodeling incremental view maintenance in further detail in\nfuture work.\n6. EXPERIMENTS\nWe explore the following questions: (1) how does DQM\ncompare to conventional heuristics as well as recent state-\nof-the-art approaches, (2) how quickly does DQM learn a\ncreation e\u000bective policy, and (3) how do di\u000berent hyper-\nparameters settings a\u000bect DQM .\n6.1 Setup\nWe implemented DQM in SparkSQL. Our RL algorithm is\ndecoupled from the Spark environment and is implemented\nusing the Keras framework[8]. All experiments are run on\na cluster of machines each with 2 Intel E5-2680 2.40 GHz\nCPUs and 64G memory running Scienti\fc Linux 7.2. We\nrun each experiment 5 times and present the average of the\n5 runs.\n6.1.1 Workloads\nQueries and data are derived from the Join Order Bench-\nmark (JOB) and TPC-DS. JOB is based on the IMDB dataset\nand consists of 113 aggregate queries with joins of up-to 16\ntables. TPC-DS is based on a synthetic dataset and a query\ngenerator that generates queries with aggregates, joins, and\nsubqueries. Our TPC-DS data is generated with a scale\nfactor of 1.\nWe generate di\u000berent scenarios from these two bench-\nmarks. Our default workload is called para.para is a steady\nstate workload (does not evolve with time) that does not\ncontain a skew of frequency on speci\fc queries. Queries\nfrom both benchmarks are augmented with random single-\nattribute predicates, so that the exact same query never\nappears twice. The workloads contain a sequence of 1000\nsuch queries and the queries are submitted and served in a\nsequential manner. Queries arrive at regular intervals, and\nthe asynchronous experiments can be run in a single time-\nstep (we evaluate these e\u000bects explicitly later).\nThen in subsequent experiments, we go beyond para and\napply the power laws skew the frequency of queries as pre-\nvious work has done [11, 19]. dzipf skews both query gen-\nerators to run more expensive queries with a much higher\nfrequency, and azipf skews the query generators to run the\nleast expensive queries with a much higher frequency. How-\never, for both these workloads the query frequency is \fxed\nthroughout the 1000 queries, albeit skewed. Next, we con-\nsider dynamic workloads. dablend is a 1000 query workload\nthat starts of executing the most expensive queries then\nswitches to executing the least expensive queries, and ad-\nblend does the opposite.\n8\n\nName Description\nazipf Rank the queries by latency in ascending order\nthen apply the zipf distribution.\ndzipf Rank the queries by latency in descending order\nthen apply zipf distribution.\nrzipf Shu\u000fe the queries then apply zipf distribution.\nadblend Concatenate the \frst 500 queries from the azipf\nworkload with the \frst 500 queries from the\ndzipf workload.\ndablend Concatenate the \frst 500 queries from the dzipf\nworkload with the \frst 500 queries from the\nazipf workload.\npara Parameterize the queries with unique random\nparameters.\nTable 2: Macro-workloads characteristics\nJOB5000100001500020000Cumulative time cost (s)ORIG\nDQM\nLFU\nHAWC\nFIFO\nLRU\nRECYCLER\nTPC-DS500100015002000\nORIG\nDQM\nLFU\nHAWC\nFIFO\nLRU\nRECYCLER\nFigure 6: We compare DQM to all of the baselines\non the paraworkload. Even including learning time,\nDQM is competitive with the best baselines in both\nTPC-DS and JOB.\nWe normalize the storage constraint across all experi-\nments. The available storage for opportunistic materializa-\ntion is set to 200MB (roughly 20% the size of the largest\nbase table in the experiments). This allows us to compare\nresults across workloads in an apples-to-apples way.\n6.1.2 Baselines\nWe implemented a number of baselines shown in Table 3\nthat range from conventional cache algorithm, like Least\nRecently Used (LRU), to sophisticated heuristic-based ap-\nproaches from previous work. All of the baselines bene\ft\nfrom the other components of DQM such as the candidate\nview miner. DQM proposes relevant views that can be op-\nportunistically generated by the current query and relevant\nto the past workload. The baselines have to select which of\nthese views to persist and evict existing views if necessary.\n6.2 Baseline Performance\nWe \frst evaluate DQM and the baselines on para (Figure\n6). We measure the cumulative runtime of the entire 1000\nquery workload. The neural network of DQM is initialized\nrandomly and has to learn the creation and deletion policy\nonline. This exploration time for DQM is included in the\noverall runtime.\nAs described in the introduction, Recycler works well when\nits creation cost heuristic correlates with improvements in\nruntime. Recycler speeds up query latency in JOB by over\n10x. There is a signi\fcant amount of nuance in these results.\nWe provide Recycler with an exact cardinality estimate for\nthe size of the views. While this is possible to know in hind-\nsight after the views are created in order to prioritize dele-\ntions; it is impossible to know this exactly during creationBaselines\nEviction Only\nLRU Randomly select one of the candidates, evict\nthe least recently used view in the cache.\nLFU Randomly select one of the candidates, evict\nthe least frequently used view in the cache.\nFIFO Randomly select one of the candidates, evict\nthe earliest view inserted into the cache.\nSelection and Eviction\nHAWC [28] Select the best view from the candidates\nbased on the Spark query optimizer cost\nmodel. For each materialized view, main-\ntain a \\credit table\" based on subsequent\nquery cost that uses the view (cost di\u000ber-\nence of using vs. not using the view). The\ncredit table is windowed to take the latest\nKqueries. HAWC evicts the lowest credit\nview.\nRECYCLER [25] Select the most expensive view (in terms of\ncreation cost). Materialize a new view if\nits cost is higher than existing views. Evict\nthe lowest cost view otherwise. For views in\nthe cache, the cost is scaled up when used,\nscaled down when not used. Our default im-\nplementation of Recycler makes use of the\ntrue costs of views, we further study a more\npractical alternative using cost model esti-\nmated view costs in Section 6.6.\nHypothetical\nBELADY\u0003Select the optimal view to use based on com-\nplete, accurate foresight and use Belady's al-\ngorithm[3] to evict old views.\nTable 3: Baselines used in the paper\ntime (i.e., a join cardinality estimation problem). Nonethe-\nless, we are generous to Recycler as future experiments show\nthat a faulty cardinality estimate very signi\fcantly a\u000bects\nresults.\nThe next interesting insight from this experiment is that\nall of the \\eviction-only\" strategies perform reasonably well\non both benchmarks. Randomized selection with a sensi-\nble eviction heuristic leads to up-to a 3x improvement on\nboth benchmarks. HAWC uses an informed selection pol-\nicy based on Spark's query optimizer but its drastic perfor-\nmance shift on the two workloads indicates optimizer based\nselection policy is not reliable, we further discuss how such\nan inexpensive estimation would a\u000bect DQM in Section 6.6.\nDQM is competitive with all baselines on both bench-\nmarks even when it has to learn. By leveraging real run-\ntime observations, it is robust to cost estimation issues in\nthe query optimizer. To us, this is a very surprising insight.\nThere is overhead in the exploration process as the system\nhas to learn from suboptimal actions. Even so, the system\nis competitive with the best baselines during this learning\nphase. We will also see that para is a worst-case of sorts for\nDQM .\n6.3 Skew Performance\nWe dig deeper on these baselines and consider di\u000berent\nquery skews and query distributions. We evaluate DQM ,\nLFU and the two heuristic-based approaches with all 12 dif-\nferent workloads. Results are shown in Figure 8 A and Fig-\nure 8 B for JOB-based workloads and TPCDS-based work-\nloads respectively. We \fnd that the results from the previous\n9\n\nexperiment broadly hold across all of the di\u000berent skews.\nRecycler works very well on JOB-based workloads, it out-\nperforms DQM on JOB-based workloads. On all of these\nworkloads, Recycler stops admitting new views after pro-\ncessing the \frst 100 queries. However, this heuristic has\nsigni\fcant issues on the TPC-DS workloads. When we dug\ninto the TPC-DS results, we found the view miner gener-\nated a single view candidate repeatedly that was very large,\nand confusing enough to the query optimizer that it hurt\nthe performance when used by some queries.\nWe \fnd that HAWC's performance also changes drasti-\ncally on JOB and TPCDS. This is because its view selection\nis based on the inaccurate estimation of Spark's optimizer\nwhich we also demonstrate in Section 6.6.\n6.4 Maintenance Performance\nThe heuristics break down when there are costs that they\ndo not model or anticipate. Maintenance costs in OLAP sys-\ntems are infrequent but are signi\fcant. In this experiment,\nwe study how view maintenance could a\u000bect DQM and the\nbaselines. Because Spark does not support incremental up-\ndate of views, every time the base tables are modi\fed we\nhave to re-compute and re-materialize the views that are af-\nfected. To simulate periodic maintenance, our system will\nrandomly select a base table of the workload and evict all\nviews using the table.\nFor comparison purpose, we perform a eviction at every\n100 queries, and this is controlled so all approaches have the\nsame maintenance routine. Even though the maintenance\nroutine is the same, a di\u000berent approach will introduce dif-\nferent maintenance cost because di\u000berent views are materi-\nalized.\nResults can be found in Figure 7 for the para workload\non the JOB benchmark. Maintenance certainly adds an\noverhead to all techniques, but the results demonstrate that\nDQM is more e\u000ecient and robust to maintenance. In the\nprevious experiment, we found that Recycler was very ef-\nfective on this workload. But after maintenance, we found\nDQM now outperforms Recycler non-trivially on 4 of the\n6 JOB-based workloads because DQM . Recycler's heuris-\ntic favors expensive views thus selecting views that incur a\nhigher maintenance overhead than DQM . Again, the bene\ft\nofDQM is a direct optimization of observed query latencies.\nViews that have to be constantly recreated because they\nare maintained fall out of favor of the learning algorithm\nquickly.\nazipf dzipf rzipf para adblend dablend102103104105Cumulative time cost (s)ORIG\nDQM-with-maintenance\nNo-maintenance\nLFU-with-maintenance\nHAWC-with-maintenance\nRECYCLER-with-maintenance\nFigure 7: We compare DQM to selected baselines\nwith periodic view maintenance (every 100 queries)\non the JOB workloads.\n6.5 Exploration vs. ExploitationWe evaluate DQM in a pure online setting. At the begin-\nning of each workload, DQM has to deal with the cold-start\nproblem. DQM starts with random selection to explore un-\ntil the number of experience and as observations come it, it\nperiodically re-trains its model. As we collect more obser-\nvations, we become more con\fdent about DQM , and then\nwe start to explore less with random actions. The explo-\nration parameter is \u000f;\u000frepresents the exploration rate and\n1\u0000\u000frepresents the probability of exploiting what we have\nlearned.\nOn the other hand, we must always have some degree of\nrandom creation to ensure that DQM is adaptive to changes.\nOur system starts from an \u000f= 1 (always take random ac-\ntions) and decays this value to \u000fmin. We evaluate DQM\nusing di\u000berent \u000fmin.\nWe set\u000fmin to 0.1, 0.2, 0.3, 0.4, 0.5 and results are\nshown in Figure 93. As expected, even though exploration\nis necessary for DQM to avoid settling on a local optimal,\nas we learn more we should prefer more exploitation and a\nhigh\u000fmin hurts the performance. However, even with a\nhigh\u000fmin of 0.5, DQM still performs competitively with\nLFU. Curiously, a higher exploration term bene\fts DQM in\nthe early stages of the workload (such as 0.2).\n6.6 Cost Estimation Issues\nIn our previous results, we were actually very generous\nto our baselines. We provide the baselines with exact view\ncardinality estimates ( DQM does not use this as it learns\npurely from observed runtimes).\nThe Recycler baseline is most sensitive to this. We im-\nplemented a more realistic alternative of Recycler called\nSORecycler. The only di\u000berence between the two is that\nSORecycler uses SparkSQL's query optimizer to estimate\nthe cost of a view instead of using the true cost. As we can\nsee in Figure 10, this change signi\fcantly a\u000bects Recycler's\nperformance because the costs of views play an important\nrule in Recycler's heuristic: it assumes that a more expen-\nsive view will bring more bene\fts. Therefore, when the costs\nof views are inaccurate its performance drops up to 25x on\nthedzipf workload.\nWe could do the opposite with DQM ; what is the e\u000bect\nif we use Spark's optimizer for an inexpensive cost estimate\nrather than the counter-factual experiments. The di\u000berence\nin query cost using or not using the view can be used to\ndetermine the improvement. In this experiment, we modify\nDQM to use estimated reward from SparkSQL's query opti-\nmizer to investigate how it would a\u000bect the performance of\nDQM . We call the Spark optimizer based version SODQM\nand results can be found in Figure 10. As mentioned in\nearlier sections, reward functions play an important role in\nRL systems, it is designed to guide the model towards the\ndirection of the highest long term value. Therefore, it is\nnot surprising an RL system underperforms when its reward\nfunction is inaccurate or even wrong. In practice, this gap\nis up to a factor of 2x in the adblend workload.SODQM\nis still reasonable in its performance but we believe that the\npower of RL is to feedback true execution times. Directly\noptimizing the true reward function explains much of the\npower of DQM .\n6.7 Storage Constraints\n3Other experiments use a \fxed \u000fmin of 0.1.\n10\n\nazipf dzipf rzipf para adblend dablend101102103104105Cumulative time cost (s)ORIG\nDQM\nLFU\nHAWC\nRECYCLER\nazipf dzipf rzipf para adblend dablend101102103104Cumulative time cost (s)ORIG\nDQM\nLFU\nHAWC\nRECYCLERFigure 8: We compare DQM to selected baselines on all workloads. DQM is competitive (or outperforms) the\nbest heuristic on all the test scenarios.\n0 250 500 750 1000\nQuery Sequence102103104105Cumulative time cost (s)Epsilon=0.1\nEpsilon=0.2\nEpsilon=0.3\nEpsilon=0.4\nEpsilon=0.5\n0.1 0.2 0.3 0.4 0.5\nEpsilon102103104105 ORIG\nDQM\nLFU\nBELADY*\nFigure 9: DQM using di\u000berent exploration terms on\nJOB with the rzipf workload.\nazipf\ndzipf\nrzipf\npara\nadblend\ndablend0200040006000800010000Cumulative time cost (s)DQM\nDQM\nSO_DQM\nazipf\ndzipf\nrzipf\npara\nadblend\ndablend020000400006000080000RECYCLER\nRECYCLER\nSO_RECYCLER\nFigure 10: We run DQM without true runtimes and\nan improvement metric derived from a cost model.\nWhile this version of DQM still performs reasonably\nwell, the use of true runtimes is a strength of the\nRL-based algorithm. We also evaluate a more real-\nistic implementation of Recycler by using the cost\nmodel estimated view costs instead of the true costs\nof views and this change drastically a\u000bects Recy-\ncler's performance.\nTo study how DQM reacts to changes in the storage con-\nstraint, we use the same JOB-rzipf workload from the previ-\nous experiments. The result is shown in Figure 11, where the\nstorage constraint is normalized by a fraction of candidate\nviews that could possibly be materialized.\nOM is most valuable when there is a substantial amount of\nspare storage in the system. The power of OM is trading o\u000b\nthis spare storage for future query latency. As expected, the\nperformance of DQM and baselines improve as we increase\nthe storage constraint. We note that the most signi\fcant\nincrease is between 40MB and 100MB4.\n4For reference the 200MB datapoint is the level used for\nDQM and all baselines in the previous experiments.\n40MB 100MB 200MB 300MB 400MB0500010000150002000025000Cumulative time cost (s)ORIG\nDQM\nLFU\nFIFO\nLRUFigure 11: We measure the performance of DQM\nas a function of the storage constraint on the JOB\nrzipf workload. The storage constraint is presented\nas normalized by a fraction of candidate views that\ncould possibly be materialized at any time.\n6.8 Delayed Rewards\nIn this experiment, we explore how delays in the asyn-\nchronous experimentation a\u000bect DQM .DQM relies on sys-\ntem idle time to execute paired experiments, what if this\nidle time is contended? We simulate this in the following\nway: given a delay of K, an experience that is generated\nat time step Twill only be available to DQM for learning\nat time step T+K. As an extreme example if Kequals\n1000, DQM will select views completely randomly for our\n1000 query workloads.\nThis is a worst case simulation of delayed reward. In\npractice, the system idle time will likely be more randomly\ndistributed and some queries might get earlier observations.\nHowever, by pushing all the experiences to the end of the\nprocess, we are simulating the worst case of delayed rewards.\nI.e. if the same amount of experiences were thrown away but\nsystem idle time was more randomly distributed, then more\nexperiences will be available earlier for the agent to learn\nfrom thus bene\fting the system.\nWe use the para workload from JOB in this experiment\nand test delay of 50, 100, 200, 300, 400 and 500. The results\ncan be found in Figure 12. We see that the performance of\nDQM changes drastically with a delay of 500 because half of\nthe experiences are thrown away. A delay under 200 does\nnot a\u000bect much of DQM 's performance, it still outperforms\nLFU non-trivially.\n6.9 DQM vs. Belady\u0003\nNext, we want to understand how well DQM is doing in\nabsolute terms. Belady\u0003is a hypothetical baseline whose\neviction policy is based on the Belady's algorithm[3], which\n11\n\n0 50 100 200 300 400 500100020003000400050006000Cumulative time cost (s)DQM\nLFU\nBELADY*Figure 12: We measure the performance of DQM\nusing di\u000berent delays of reward on JOB para work-\nload.\nis hypothetical because it relies on hindsight to evict a view\nthat will not be needed for the longest time in the future.\nBelady\u0003is also hypothetical because it always selects the\nbest view (from an oracle) in foresight. Therefore, Belady\u0003's\nadmission policy is optimal, but its eviction policy is not\noptimal because it does not consider the cost and bene\ft of\na view. We believe that it would be prohibitively expensive\nto optimize over all possible optimal creation AND deletion\npolicies.\nWe evaluate Belady\u0003on all 12 workloads. By learning\na predictive model that directly optimizes for runtime im-\nprovement, DQM actually performs competitively with Belady\u0003\non both Job-based workloads and TPC-DS workloads as\nshown in Figure 13. DQM learns a stateful creation pol-\nicy that considers what views are already materialized and\na cost-aware eviction policy informed by those improvement\nexperiments. In the rest of this section, we evaluate DQM\non a set of micro-benchmarks to understand how di\u000berent\nsettings a\u000bect DQM 's performance.\nazipf\ndzipf\nrzipf\npara\nadblend\ndablend10002000300040005000Cumulative time cost (s)JOB\nDQM\nBELADY*\nazipf\ndzipf\nrzipf\npara\nadblend\ndablend200400600800TPC-DS\nDQM\nBELADY*\nFigure 13: We compare DQM to a hypothetical near-\noptimal solution called Belady\u0003, which uses perfect\nforesight to select the best views and evict a view\nthat will not be needed for the longest time in the\nfuture (Belady's algorithm).\n6.10 Overhead\nIn all of our previous experiments, we included the over-\nhead of learning and exploration in our cumulative runtimes.\nNext, we try to understand the system performance after\nlearning (assuming that the future workload is stationary,\nof course). We \frst train DQM with 1000 queries from the\npara workload using a \u000fmin of 0.1 then we change \u000fmin\nto 0 (to avoid exploration), and then, use the trained model\nto select views for another 1000 queries (with no additional\nexploration).The result can be found in Figure 14. The over all per-\nformance improves by about 15%. This shows that DQM is\na very data-e\u000ecient learning approach. The paired exper-\niments give a very strong signal for learning, and DQM is\nable to quickly learn from this signal.\nIt also shows that continuously learning is not too oner-\nous for a real system. It is advantageous to always have\nsome amount of exploration (some seemingly suboptimal\ndecisions). This allows the system to \\bounce\" out of lo-\ncal minima if the workload or the execution environment\nchanges. However, that said, we do believe that over\ft-\nting can be an issue{if this was the case, the di\u000berence be-\ntween DQM and a Trained DQM would be more signi\fcant if\nTrained DQM erroneously memorized patterns of the work-\nload (if any). We leave further investigation of this to future\nwork as we build and deploy DQM in realistic scenarios.\npara10002000300040005000Cumulative time cost (s)TrainedDQM\nBELADY*\nDQM\nFigure 14: We evaluate overhead caused by learn-\ning and exploration by comparing DQM with a\nTrained DQM with no exploration.\n7. DISCUSSION AND FUTURE WORK\nWe believe one limitation of our current work is a sophis-\nticated methodology for query featurization. A better query\nfeaturization will de\fnitely help the agent learn better and\nmake better decisions, e.g. capture latent patterns in the\nworkload. This problem is related to the work studied by\nOrtiz et al. in the context of query optimization [26]. We\nalso believe that more work can be done studying relatistic\ndynamic workloads. We are actively looking for benchmark\nworkloads that simulate ad hoc querying. We believe such\nworkloads are necessary for building and evaluating more\npractical methodologies.\nThere are also numerous opportunities for reusing com-\nputation and intermediate query state in OLAP workloads,\nand we believe that machine learning will be an impor-\ntant part of future OLAP systems. Applications of machine\nlearning in database internals are still the subject of signi\f-\ncant debate, and will continue to be a contentious question\nfor years to come [2, 16, 20]. An important question is what\nproblems are amenable to machine learning solutions. We\nbelieve that materialization is one such sub-area.\nWe see DQM as a \frst step towards a View-Oriented\ndatabase, one that aggressively anticipates future queries\nand materializes anything that could be useful. Such an ar-\nchitecture shifts the query optimization burden from plan-\nning a query to e\u000eciently reusing past computation. New\nalgorithms and theory will have to developed to understand\nthe new problem setting. We believe machine learning will\nbe an important part of this discussion. In the short-term,\nextending DQM to consider OLTP settings and more com-\nplex reward functions is certainly a priority. We also want\nto explore dynamic or periodic workloads.\n12\n\n8. REFERENCES\n[1] S. Agrawal, S. Chaudhuri, and V. R. Narasayya.\nAutomated selection of materialized views and indexes\nin sql databases. In VLDB , volume 2000, pages\n496{505, 2000.\n[2] P. Bailis, K. S. Tai, P. Thaker, and M. Zaharia. Don't\nthrow out your algorithms book just yet: Classical\ndata structures that can outperform learned indexes.\nhttps://dawn.cs.stanford.edu/2018/01/11/\nindex-baselines/ , 2017.\n[3] L. A. Belady, R. A. Nelson, and G. S. Shedler. An\nanomaly in space-time characteristics of certain\nprograms running in a paging machine. Commun.\nACM , 12(6):349{353, June 1969.\n[4] G. E. Box, W. G. Hunter, J. S. Hunter, et al.\nStatistics for experimenters. 1978.\n[5] N. Bruno and S. Chaudhuri. Automatic physical\ndatabase tuning: a relaxation-based approach. In\nProceedings of the 2005 ACM SIGMOD international\nconference on Management of data , pages 227{238.\nACM, 2005.\n[6] S. Chaudhuri, V. Narasayya, and R. Ramamurthy. A\npay-as-you-go framework for query execution\nfeedback. Proceedings of the VLDB Endowment ,\n1(1):1141{1152, 2008.\n[7] C. M. Chen and N. Roussopoulos. The implementation\nand performance evaluation of the adms query\noptimizer: Integrating query result caching and\nmatching. In International Conference on Extending\nDatabase Technology , pages 323{336. Springer, 1994.\n[8] F. Chollet et al. Keras. https://keras.io , 2015.\n[9] B. Dageville, D. Das, K. Dias, K. Yagoub, M. Zait,\nand M. Ziauddin. Automatic sql tuning in oracle 10g.\nInProceedings of the Thirtieth international\nconference on Very large data bases-Volume 30 , pages\n1098{1109. VLDB Endowment, 2004.\n[10] P. M. Deshpande, K. Ramasamy, A. Shukla, and J. F.\nNaughton. Caching multidimensional queries using\nchunks. In ACM SIGMOD Record , volume 27, pages\n259{270. ACM, 1998.\n[11] A. L. Drapeau, D. A. Patterson, and R. H. Katz.\nToward workload characterization of video server and\ndigital library applications. ACM SIGMETRICS\nPerformance Evaluation Review , 22(1):274{275, 1994.\n[12] H. V. Hasselt. Double q-learning. In J. D. La\u000berty,\nC. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and\nA. Culotta, editors, Advances in Neural Information\nProcessing Systems 23 , pages 2613{2621. Curran\nAssociates, Inc., 2010.\n[13] A. Kipf, T. Kipf, B. Radke, V. Leis, P. Boncz, and\nA. Kemper. Learned cardinalities: Estimating\ncorrelated joins with deep learning. arXiv preprint\narXiv:1809.00677 , 2018.\n[14] Y. Kotidis and N. Roussopoulos. Dynamat: a dynamic\nview management system for data warehouses. In\nACM Sigmod record , volume 28, pages 371{382. ACM,\n1999.\n[15] Y. Kotidis and N. Roussopoulos. A case for dynamic\nview management. ACM Transactions on Database\nSystems (TODS) , 26(4):388{423, 2001.\n[16] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. InProceedings of the 2018 International Conference on\nManagement of Data , pages 489{504. ACM, 2018.\n[17] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and\nI. Stoica. Learning to optimize join queries with deep\nreinforcement learning. arXiv preprint\narXiv:1808.03196 , 2018.\n[18] J. LeFevre, J. Sankaranarayanan, H. Hacigumus,\nJ. Tatemura, N. Polyzotis, and M. J. Carey.\nOpportunistic physical design for big data analytics.\nInProceedings of the 2014 ACM SIGMOD\ninternational conference on Management of data ,\npages 851{862. ACM, 2014.\n[19] E. Lo, N. Cheng, and W.-K. Hon. Generating\ndatabases for query workloads. Proceedings of the\nVLDB Endowment , 3(1-2):848{859, 2010.\n[20] L. Ma, D. Van Aken, A. Hefny, G. Mezerhane,\nA. Pavlo, and G. J. Gordon. Query-based workload\nforecasting for self-driving database management\nsystems. In Proceedings of the 2018 International\nConference on Management of Data , pages 631{645.\nACM, 2018.\n[21] I. Mami and Z. Bellahsene. A survey of view selection\nmethods. ACM SIGMOD Record , 41(1):20{29, 2012.\n[22] R. Marcus and O. Papaemmanouil. Deep\nreinforcement learning for join order enumeration.\narXiv preprint arXiv:1803.00055 , 2018.\n[23] V. Markl, G. M. Lohman, and V. Raman. LEO: An\nautonomic query optimizer for DB2. IBM Systems\nJournal , 42(1):98{106, 2003.\n[24] F. Nagel. Recycling intermediate results in pipelined\nquery evaluation . PhD thesis, Citeseer.\n[25] F. Nagel, P. Boncz, and S. D. Viglas. Recycling in\npipelined query evaluation. In 2013 IEEE 29th\nInternational Conference on Data Engineering\n(ICDE) , pages 338{349, April 2013.\n[26] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi.\nLearning state representations for query optimization\nwith deep reinforcement learning. In Proceedings of\nthe Second Workshop on Data Management for\nEnd-To-End Machine Learning , DEEM'18, pages\n4:1{4:4, New York, NY, USA, 2018. ACM.\n[27] S. Papadomanolakis, D. Dash, and A. Ailamaki.\nE\u000ecient use of the query optimizer for automated\nphysical design. In Proceedings of the 33rd\ninternational conference on Very large data bases ,\npages 1093{1104. VLDB Endowment, 2007.\n[28] L. L. Perez and C. M. Jermaine. History-aware query\noptimization with materialized intermediate views. In\nData Engineering (ICDE), 2014 IEEE 30th\nInternational Conference on , pages 520{531. IEEE,\n2014.\n[29] T. Phan and W.-S. Li. Dynamic materialization of\nquery views for data warehouse workloads. In Data\nEngineering, 2008. ICDE 2008. IEEE 24th\nInternational Conference on , pages 436{445. IEEE,\n2008.\n[30] P. Scheuermann, J. Shim, and R. Vingralek.\nWatchman: A data warehouse intelligent cache\nmanager. 1996.\n[31] R. S. Sutton and A. G. Barto. Reinforcement learning:\nAn introduction . MIT press, 2018.\n13\n\n[32] R. S. Sutton, A. G. Barto, and R. J. Williams.\nReinforcement learning is direct adaptive optimal\ncontrol. IEEE Control Systems , 12(2):19{22, 1992.\n[33] D. C. Zilio, C. Zuzarte, S. Lightstone, W. Ma, G. M.\nLohman, R. J. Cochrane, H. Pirahesh, L. Colby,J. Gryz, E. Alton, et al. Recommending materialized\nviews and indexes with the ibm db2 design advisor. In\nAutonomic Computing, 2004. Proceedings.\nInternational Conference on , pages 180{187. IEEE,\n2004.\n14",
  "textLength": 68672
}