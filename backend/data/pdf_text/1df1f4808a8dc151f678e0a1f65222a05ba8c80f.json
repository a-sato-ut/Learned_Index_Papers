{
  "paperId": "1df1f4808a8dc151f678e0a1f65222a05ba8c80f",
  "title": "Non-clairvoyant Scheduling with Partial Predictions",
  "pdfPath": "1df1f4808a8dc151f678e0a1f65222a05ba8c80f.pdf",
  "text": "Non-clairvoyant Scheduling with Partial Predictions\nZiyad Benomar1 2Vianney Perchet1 3\nAbstract\nThe non-clairvoyant scheduling problem has\ngained new interest within learning-augmented\nalgorithms, where the decision-maker is equipped\nwith predictions without any quality guarantees.\nIn practical settings, access to predictions may be\nreduced to specific instances, due to cost or data\nlimitations. Our investigation focuses on scenar-\nios where predictions for only Bjob sizes out of\nnare available to the algorithm. We first estab-\nlish near-optimal lower bounds and algorithms in\nthe case of perfect predictions. Subsequently, we\npresent a learning-augmented algorithm satisfy-\ning the robustness, consistency, and smoothness\ncriteria, and revealing a novel tradeoff between\nconsistency and smoothness inherent in the sce-\nnario with a restricted number of predictions.\n1. Introduction\nOptimal job scheduling is a longstanding and actively stud-\nied class of optimization problems (Panwalkar & Iskan-\nder, 1977; Lenstra & Rinnooy Kan, 1978; Graham et al.,\n1979; Martel, 1982; Cheng & Sin, 1990; Lawler et al., 1993;\nPinedo, 2012), with applications in various domains span-\nning from supply chain management (Hall & Potts, 2003;\nIvanov et al., 2016) to operating systems (Jensen et al.,\n1985; Ramamritham & Stankovic, 1994; Steiger et al., 2004).\nA particular setting is preemptive single-machine schedul-\ning (Pinedo, 2012; Baker & Trietsch, 2013), where njobs\ni∈[n]must be executed on the same machine, with the\npossibility of interrupting a job and resuming it afterward,\nand the objective is to minimize the sum of their comple-\ntion times. An algorithm is called clairvoyant if it has\ninitial access to the job sizes, otherwise, it is called non-\nclairvoyant (Motwani et al., 1994). The design of non-\nclairvoyant scheduling algorithms is a classical problem\n1ENSAE, FAIRPLAY joint team, CREST, Palaiseau, France\n2Ecole polytechnique, Palaiseau, France3Criteo AI Lab, FAIR-\nPLAY joint team, Paris, France. Correspondence to: Ziyad Beno-\nmar<ziyad.benomar@ensae.fr >.\nProceedings of the 41stInternational Conference on Machine\nLearning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).in competitive analysis and online algorithms (Borodin &\nEl-Yaniv, 2005). In this paradigm, decisions must be made\nin an environment where the parameters governing the out-\ncome are unknown or might evolve over time.\nDue to the inherent difficulty of the problems in competi-\ntive analysis, the performance of any online algorithm stays\nbounded away from that of the optimal offline algorithm.\nHowever, the ascent of machine learning motivated the incor-\nporation of predictions in algorithm design, which started in\nworks such as (Munoz & Vassilvitskii, 2017; Kraska et al.,\n2018), then was formalized in (Lykouris & Vassilvtiskii,\n2018) and (Purohit et al., 2018). Since then, learning-\naugmented algorithms became a popular research topic and\nhad multiple applications (Mitzenmacher & Vassilvitskii,\n2022). The outcome of these algorithms depends both on the\nparameters of the problem and the quality of the predictions.\nThey are required to have a performance that is near-optimal\nwhen the predictions are accurate (consistency), near the\nworst-case performance without advice if the predictions\nare arbitrarily erroneous (robustness), and that degrades\nsmoothly as the prediction error increases (smoothness).\nIn practice, predictions often incur costs and, at times, are\ninfeasible due to the lack of data. It is, therefore, crucial to\nunderstand the limitations and the feasible improvements\nwith a restrained number of predictions in scenarios with\nmultiple unknown variables. This question was first inves-\ntigated for the caching problem (Im et al., 2022), and very\nrecently for metrical task systems (Sadek & Elias, 2024), in\nsettings where the algorithm is allowed to query a limited\nnumber of predictions. It was also explored for the schedul-\ning problem (Benomar & Perchet, 2023), assuming that the\ndecision-maker can query the true sizes of Bjobs out of n.\nThe authors present a\u0000\n2−B(B−1)\nn(n−1)\u0001\n-competitive algorithm,\nand they give a lower bound on the competitive ratio of\nany algorithm only when B=o(n). The case of imperfect\npredictions, however, is not examined.\nIn non-clairvoyant scheduling, besides the querying model\nstudied in the works mentioned above, predictions of the\nsizes of certain jobs i∈Imay be available, where I⊂[n],\nperhaps derived from previous executions of similar tasks.\nAssuming that Iis a subset of [n]of size B, taken uni-\nformly at random, we examine the limitations and possible\nimprovements of non-clairvoyant algorithms.\n1arXiv:2405.01013v2  [cs.LG]  4 Aug 2024\n\nNon-clairvoyant Scheduling with Partial Predictions\n1.1. Contributions\nWe initiate our analysis by addressing the scenario of perfect\npredictions. We establish a lower bound on the (n, B)-\ncompetitive ratio of any algorithm (for fixed n≥2and\nB≤n), which extends the lower bound of 2−4\nn+3in the\nnon-clairvoyant case (Motwani et al., 1994). Considering\nthatB=wn+o(n)for some w∈[0,1], we derive from\nthe prior bound that the competitive ratio of any algorithm is\nat least 2−w−(4\ne−1)w(1−w), and we show an improved\nbound of 2−w−(3−2√\n2)w(1−w). Demonstrating\nthese bounds is considerably more challenging than the case\nB= 0, due to the eventual dependency between the actions\nof the algorithm and the known job sizes.\nIn the case of perfect predictions, we show that knowing\nonly the relative order of the Bjob sizes, without knowledge\nof their values, enables a (2−B\nn)-competitive algorithm,\nwhich improves substantially upon the result of (Benomar &\nPerchet, 2023). We propose a second algorithm leveraging\nthe true sizes of the Bjobs, yielding an (n, B)-competitive\nratio of (2−B\nn−2(1−B/n)\nn+1), which is strictly better than\nthe former, although both are asymptotically equivalent.\nSubsequently, we adapt the latter algorithm to handle im-\nperfect predictions. While the difficulty in most works\non learning-augmented algorithms lies in ensuring robust-\nness and consistency, smoothness in the case of schedul-\ning with limited predictions is also not immediate. Along-\nside the typical consistency-robustness tradeoff, our algo-\nrithm also exhibits a consistency-smoothness tradeoff. More\nprecisely, governed by two hyperparameters λ, ρ∈[0,1],\nthe(n, B)-competitive ratio of the algorithm is at most\nmin(2\n1−λ,C\nλ+S\nλnE[η]\nOPT). Here, E[η]denotes the total ex-\npected prediction error, OPT is the objective function\nachieved by the optimal offline algorithm,2\n1−λis the al-\ngorithm’s robustness,C\nλ=1\nλ(2−B\nn+ρB\nn(1−B−1\nn−1))\nits consistency, andS\nλ=1\nλ(4\nρ(1−B\nn) +B\nn)its smooth-\nness factor, characterizing the sensitivity of the bound to\nE[η]. Notably, alterations in the parameter ρyield oppos-\ning variations on the consistency and the smoothness factor.\nNonetheless, this tradeoff vanishes for Bclose to 0orn,\nand does not appear, for instance, in (Purohit et al., 2018),\n(Bampis et al., 2022) or (Lindermayr & Megow, 2022).\nWe illustrate our results for the case of perfect predictions in\nFigure 1, comparing them with the competitive ratio proved\nin (Benomar & Perchet, 2023).\n1.2. Related work\nSince their introduction in (Purohit et al., 2018; Lykouris\n& Vassilvtiskii, 2018), learning-augmented algorithms wit-\nnessed an exponentially growing interest, as they offered a\nfresh perspective for revisiting online algorithms, and pro-\n0.0 0.2 0.4 0.6 0.8 1.0\nFraction B/nof known job sizes1.01.21.41.61.82.0Competitive ratioLower bound (Corollary 2.2)\nLower bound (Corollary 2.3)\nCR of Algorithms 1 and 2\nCR of (Benomar & Perchet, 2023)Figure 1. Lower bounds and competitive ratios for Bknown job\nsizes.\nvided new applications for machine learning in algorithm\ndesign (Mitzenmacher & Vassilvitskii, 2022) and in the im-\nplementation of data structures (Kraska et al., 2018; Lin\net al., 2022). Many fundamental problems in competitive\nanalysis were studied in this setting, such as ski rental (Gol-\nlapudi & Panigrahi, 2019; Anand et al., 2020; Bamas et al.,\n2020b; Diakonikolas et al., 2021; Antoniadis et al., 2021a;\nMaghakian et al., 2023; Shin et al., 2023), secretary (Anto-\nniadis et al., 2020; D ¨utting et al., 2021), matching (Dinitz\net al., 2021; Chen et al., 2022; Sakaue & Oki, 2022; Jin &\nMa, 2022), caching and metrical task systems (Lykouris &\nVassilvtiskii, 2018; Chlkedowski et al., 2021; Antoniadis\net al., 2023b;a; Christianson et al., 2023). In particular,\nscheduling is one of the problems that were studied most\nthoroughly. Different works cover various objective func-\ntions (Purohit et al., 2018; Lattanzi et al., 2020; Azar et al.,\n2021), prediction types (Antoniadis et al., 2021b; Merlis\net al., 2023; Lassota et al., 2023), error metrics (Im et al.,\n2021; Lindermayr & Megow, 2022), and other aspects and\napplications (Wei & Zhang, 2020; Bamas et al., 2020a;\nDinitz et al., 2022)\nThe setting of learning-augmented algorithms with limited\npredictions was initially explored by Im et al. (2022) for\ncaching. The authors presented an algorithm using parsimo-\nnious predictions, with a competitive ratio increasing with\nthe number of allowed queries. In another very recent paper\n(Sadek & Elias, 2024), a similar setting is studied for the\nmore general problem of metrical task systems, where the\nalgorithm is allowed to query a reduced number of action\npredictions (Antoniadis et al., 2023b), each giving the state\nof an optimal algorithm at the respective query step. An\nadditional related study by Drygala et al. (2023) focuses on\nthe ski-rental and Bahncard problems in a penalized adapta-\ntion of the setting with limited advice, where the cost of the\npredictions is added to the algorithm’s objective function.\nOther works have explored related settings with different\ntypes of limited advice. For instance, the setting with a\nrestricted number of perfect hints was examined in the con-\n2\n\nNon-clairvoyant Scheduling with Partial Predictions\ntext of online linear optimization by (Bhaskara et al., 2021)\nand in the multi-color secretary problem by (Benomar et al.,\n2023). Another setting, where the algorithm can query two\ntypes of hints—one that is free but possibly inaccurate, and\nanother that is expensive but accurate—has been studied\nin several problems, such as correlation clustering (Silwal\net al., 2023), computing minimum spanning trees in a metric\nspace (Bateni et al., 2023), sorting (Bai & Coester, 2024),\nand matroid optimization (Eberle et al., 2024).\nIn the context of scheduling, Benomar & Perchet (2023)\nintroduced the B-clairvoyant scheduling problem, where an\nalgorithm can query the exact sizes of Bjobs at any moment\nduring its execution. They show that the optimal strategy\ninvolves querying the sizes of Bjobs selected uniformly at\nrandom at the beginning of the process. They establish that,\nifB=o(n), then the competitive ratio of any algorithm is\nat least 2, then they provide a\u0000\n2−B(B−1)\nn(n−1)\u0001\n-competitive\nalgorithm. The same paper also addresses the secretary prob-\nlem with restricted access to binary predictions of known\naccuracy, and the ski-rental problem with access to an oracle\nwhose accuracy improves progressively over time.\nThe limit scenario B= 0corresponds to the non-clairvoyant\nscheduling problem, studied in-depth in (Motwani et al.,\n1994). In particular, the paper demonstrates that the com-\npetitive ratio of any non-clairvoyant algorithm is at least 2,\nand that it is achieved by the round-robin algorithm, exe-\ncuting all unfinished jobs concurrently at equal rates. On\nthe other hand, B=ncorresponds to the setting presented\nin (Purohit et al., 2018), where the authors introduce, for\nallλ∈(0,1), apreferential round-robin algorithm with\nrobustness2\n1−λand consistency1\nλ.\n1.3. Problem and notations\nThe decision-maker is given njobsi∈[n]with unknown\nsizes x1, . . . , x nto schedule on a single machine, and pre-\ndictions (yi)i∈Iof(xi)i∈I, with Ia uniformly random sub-\nset of [n]of size B. The objective is to leverage the available\npredictions to minimize the sum of the completion times.\nWe assume that preemption is allowed, i.e. it is possible to\ninterrupt the execution of a job and resume it later, which is\nequivalent, by neglecting the preemption cost, to assuming\nthat the jobs can be run in parallel at rates that sum to at\nmost 1.\nTo simplify the presentation, we consider that there are pre-\ndictions y1, . . . , y nofx1, . . . , x n, but the decision-maker\nhas only access to yσ(1), . . . , y σ(B), where σis a uniformly\nrandom permutation of [n]. We denote by ηi=|xi−yi|the\nerror of the prediction yi, and by ησ=PB\ni=1ησ(i)the total\nerror of the predictions accessed by the algorithm.\nConsider an algorithm Aand an instance x= (x1, . . . , x n)\nof job sizes, we denote by A(x)the sum of the completiontimes of all the jobs when executed by A. Furthermore, for\nalli̸=j∈[n]andt >0, we denote by\n•SA\ni(t)the processing time spent on job iuntil time t,\n•tA\ni= inf{t≥0 :SA\ni(t) =xi}its completion time,\n•DA\nij=SA\ni(tA\nj)the total time spent on job ibefore job\njterminates,\n•andPA\nij=DA\nij+DA\njithe mutual delay caused by i, j\nto each other.\nWhen there is no ambiguity, we omit writing the dependency\ntoA. With these notations, it holds that tA\ni=xi+P\nj̸=iDA\nji\nfor all i∈[n]. Consequently, the objective function of A\ncan be expressed as\nA(x) =nX\ni=1xi+X\n1≤i<j≤nPA\nij. (1)\nObserving that, for all i̸=j∈[n], ifiterminates be-\nforejthenPA\nij≥xi, otherwise PA\nij≥xj, we deduce that\nPA\nij≥min(xi, xj). Equality is achieved by the clairvoy-\nant algorithm that runs the jobs until completion in non-\ndecreasing size order (Motwani et al., 1994), which is the\noptimal offline algorithm, that we denote OPT , satisfying\nOPT(x) =nX\ni=1xi+X\n1≤i<j≤nmin(xi, xj). (2)\nWhen the predictions are perfect, for all n≥2andB≤n,\nwe define the (n, B)-competitive ratio of algorithm Aas the\nworst-case ratio between its objective, knowing the sizes\nofBjobs taken uniformly at random, and that of OPT , on\ninstances of njobs\nRn,B(A) = sup\nx∈(0,∞)nE[A(x)]\nOPT(x),\nwhere the expectation E[A(x)]is taken over the permutation\nσand the actions of Aif it a randomized algorithm.\nIf the number of predictions depends on the number of jobs,\ni.e.B= (Bn)n≥1defines a sequence of integers, then the\ncompetitive ratio of Ais defined by\nCRB(A) = sup\nn≥2Rn,Bn(A).\nWhen the predictions are imperfect, the competitive ratio\nbecomes also a function of E[ησ].\n3\n\nNon-clairvoyant Scheduling with Partial Predictions\n2. Lower Bounds\nIn this section, we assume that the predictions are error-free,\nand we establish lower bounds on the (n, B)-competitive\nratio of any algorithm, followed by a lower bound inde-\npendent of nwhen Bn=wn+o(n)for some w∈[0,1].\nThese lower bounds are obtained by constructing random\njob size instances xsuch that, for any deterministic algo-\nrithm A, the ratio Eσ,x[A(x)]/Ex[OPT(x)]is above them.\nThe result then extends to randomized algorithms and yields\nbounds on their (n, B)-competitive ratio by using Lemma\nA.1, which is a consequence of Yao’s principle (Yao, 1977).\nIn all this section, we consider i.i.d. job sizes. Therefore,\nwe can assume without loss of generality that the Bknown\njob sizes are x1, . . . , x B.\nIn the non-clairvoyant case B= 0, for any algorithm A,\ntaking i.i.d. exponentially distributed sizes gives, with easy\ncomputation, that E[PA\nij] = 1 (Remark A.4), which yields,\nusing Equations (1)and(2), the lower bound 2−4\nn+3on\nthe competitive ratio (Motwani et al., 1994). However, if\nB > 0, the algorithm can act according to the information\nit has on (xi)i∈I, and the dependence between its actions\nand these job sizes makes the analysis more sophisticated.\nFor any positive and continuous function φ, and positive\nnumbers T≥x >0we denote\nGφ(x, T) =ZT−x\n0dt\nφ(t)+x\nφ(T−x). (3)\nWe prove in the following theorem a generic lower bound,\nusing job sizes sampled independently from the distribution\nPr(xi≤t) = 1−φ(0)\nφ(t).\nTheorem 2.1. Letφ: [0,∞)→[0,∞)be a continuously\ndifferentiable and increasing function satisfying φ(0)>0,\nφ′/φis non-increasing andR∞\n0dt\nφ(t)2<∞, and let αφa\nnon-negative constant satisfying\nZ∞\n0\u001a\ninf\nT≥xGφ(x, T)\u001bφ′(x)\nφ(x)2dx≥αφZ∞\n0dt\nφ(t)2.(4)\nIfB=wn+o(n)for some w∈[0,1], then it holds for any\nrandomized algorithm Athat\nCRB(A)≥2−2(2−αφ)w+ (3−2αφ)w2.\nMoreover, ifR∞\n0dt\nφ(t)<∞, then for all n≥2andB≤n\nRn,B(A)≥Cφ,n,B−Cφ,n,B−1\n1 +n−1\n2R∞\n0dt\nφ(t)2R∞\n0dt\nφ(t),\nwhere Cφ,n,B = (2−B\nn)−(3−2αφ)B\nn\u0000\n1−B−1\nn−1\u0001\n.\nTo establish this theorem, we analyze i.i.d. job sizes sam-\npled from the distribution Pr(xi≤t) = 1 −φ(0)\nφ(t). Wederive in Lemma A.5 a lower bound on the mutual delays\nincurred by these jobs during the run of any algorithm A.\nThis involves solving a functional minimization problem,\nwhose solution is expressed using the function Gφdefined\nin(3). The left term in Inequality (4)is proportional to the\nobtained lower bound, while the right term is proportional\ntoE[min( xi, xj)], which is the mutual delay caused in a\nrun of OPT . Finally, using the identity (1), this inequality,\nwhich relates the mutual delays caused respectively by exe-\ncuting AandOPT on the chosen job sizes, can be extended\nto an inequality involving the objectives of both algorithms,\ngiving a lower bound on the competitive ratio.\nIfR∞\n0dt\nφ(t)=∞, the expectation of the job sizes is infinite.\nIn this case, we consider a truncated distribution with a\nmaximum a >0. After completing the analysis, we derive\na lower bound that depends on aandw, by considering\nB=wn+o(n)andn→ ∞ , then, we conclude by taking\nthe limit a→ ∞ .\nFor any value αφin Theorem 2.1, observe that Cφ,n,0= 2\nandCφ,n,n = 1, which means that the lower bound interpo-\nlates properly the non-clairvoyant and clairvoyant settings.\nThe remaining task is to choose an adequate function φ\nsatisfying the conditions of the theorem with αφas large\nas possible. We first consider exponentially distributed job\nsizes, often used to prove lower bounds in scheduling prob-\nlems. This corresponds to φ(t) =et.\nCorollary 2.2. For any algorithm A, it holds that\nRn,B(A)≥Cn,B−4(Cn,B−1)\nn+ 3,\nwithCn,B= 2−B\nn−(4\ne−1)B\nn\u0000\n1−B−1\nn−1\u0001\n. In particular,\nifB=wn+o(n)then\nCRB(A)≥(2−w)−(4\ne−1)w(1−w).\nCorollary 2.2 gives, in particular, that Rn,0(A)≥2−4\nn+3,\nwhich corresponds exactly to the tight lower bound for the\nnon-clairvoyant scheduling problem (Motwani et al., 1994).\nHowever, the bound is not tight for all values of B≤n.\nTo refine it, we consider distributions that would be more\ndifficult to process by the algorithm. One idea is to sample\na different parameter λi∼ E(1)independently for each\ni∈[n], then sample xi∼ E(λi). The distribution of xiin\nthis case is given by\nPr(xi≥t) =Z∞\n0Pr(xi≥t|λi=λ)e−λdλ\n=Z∞\n0e−(1+t)λdλ=1\n1 +t,\nwhich corresponds to φ(t) = 1 + t. More generally, we\nconsider φ(t) = (1 + t)rforr∈(1\n2,1]. Such functions\n4\n\nNon-clairvoyant Scheduling with Partial Predictions\nφcorrespond to distributions with a heavy tale and with\ninfinite expectation (i.e.R∞\n0dt\nφ(t)=∞). Using Theorem\n2.1, they only yield lower bounds on the competitive ratio\nbut not on Rn,B. Corollary 2.3 shows the bound obtained\nforr→1\n2.\nCorollary 2.3. Letw∈[0,1]. IfB=wn+o(n), then it\nholds for any algorithm Athat\nCRB(A)≥(2−w)−(3−2√\n2)w(1−w).\nRemark 2.4.Ifxiis sampled from the distribution induced\nbyφ(t) = (1+ t)r, then xi+1follows a Pareto distribution\nwith scale 1 and shape r(Arnold, 2014), which is commonly\nused to model the distribution of job sizes in the context of\nthe scheduling problem.\n3. Known Partial Order\nBefore investigating the problem within the learning-\naugmented framework, we introduce an algorithm exclu-\nsively for the scenario with perfect information. Subse-\nquently, in Section 4, we present a second algorithm, that\nwe adapt to handle possibly erroneous predictions.\nThe optimal algorithm OPT does not necessitate precise\nknowledge of job sizes. Instead, it relies solely on their\nordering. This observation suggests that it might be possible\nto improve the competitive ratio of the non-clairvoyant case\nby only knowing the relative order of a subset of the job\nsizes. Therefore, rather than having access to the values\nxσ(1), . . . , x σ(B), we assume that the decision-maker is only\ngiven a priority ordering πof them, i.e. a bijection π:\n[B]→σ([B])satisfying xπ(1)≤. . .≤xπ(B).\nAlgorithm 1 Catch-up and Resume Round-Robin ( CRRR )\nInput: Ordering πofxσ(1), . . . , x σ(1)\nSetxπ(0)= 0\nfori= 1toBdo\nRun job π(i)forxπ(i−1)units of time\nwhile jobπ(i)is not finished do\nRun round-robin on {σ(j)}j>B∪ {π(i)}\nend while\nend for\nRun round-robin on {σ(j)}n\nj=B+1until completion\nIn Algorithm 1 ( CRRR ), for all i∈[B], the execution of\njobπ(i)starts only upon the completion of job π(i−1). At\nthis moment, all jobs σ(j)forj > B are either completed or\nhave undergone execution for xπ(i−1)units of time. CRRR\nthen runs job π(i)for a period of length xπ(i−1)to catch\nup with the progress of the jobs {σ(j)}j>B. Following\nthis synchronization phase, it runs round-robin on the set of\njobs{σ(j)}j>B∪ {π(i)}untilπ(i)terminates. The same\nprocess iterates with π(i+ 1) afterward. Once all the jobsσ(i)fori∈[B]are completed, the algorithm runs round\nrobin on the unfinished jobs in {σ(j)}j>B.\nLeveraging the ordering π, the algorithm aims to minimize\nthe delays caused by longer jobs to shorter ones. In the ideal\nscenario where B=n, each job begins execution only after\nall shorter ones have been completed. When B < n , it is\nevident that the jobs {σ(i)}i∈[B]should be executed in the\norder specified by π. However, CRRR takes advantage of\nthis ordering even further, ensuring that job xπ(i)not only\navoids delaying xπ(i−1)but also does not delay any job σ(j)\nwithj > B that has a size at most xπ(i−1).\nTheorem 3.1. Algorithm CRRR satisfies\n2−B\nn−2(1−B\nn)\n(n+ 1)( B+ 1)≤Rn,B(CRRR )≤2−B\nn.\nMoreover, if B=⌊wn⌋for some w∈[0,1], then\nCR(CRRR ) = 2−w.\nTheorem 3.1 shows a substantially stronger result than\nthe one presented in (Benomar & Perchet, 2023), where\nthe algorithm leveraging the values of the job sizes\nxσ(1), . . . , x σ(B)is only\u0000\n2−B(B−1)\nn(n−1)\u0001\n-competitive.\nAction predictions The information provided to CRRR\nis the order in which the jobs {σ(i)}i∈[B]would be exe-\ncuted by OPT . This corresponds to the error-free scenario\nofaction predictions (Antoniadis et al., 2023b; Lindermayr\n& Megow, 2022; Lassota et al., 2023; Sadek & Elias, 2024),\nwhere the decision-maker receives predictions regarding the\nactions taken by the optimal offline algorithm, rather than\nnumeric predictions of unknown parameters. In the context\nof the scheduling problem, utilizing the ℓ1norm to measure\nthe error is not ideal for analyzing the action prediction\nsetting (Im et al., 2021). Alternative error metrics, which\naccount for the number of inversions in the predicted permu-\ntation in comparison to the true one (Lindermayr & Megow,\n2022), would be more suitable. Therefore, adapting CRRR\nto imperfect action predictions is left for future research\nas it requires different considerations. For now, we shift\nour focus to introducing another algorithm that utilizes not\nonly the priority order induced by the job sizes, but the size\nvalues themselves.\n4. Predictions of the Job Sizes\nWe propose in this section a generic algorithm Switch ,\nwhich we will adapt in the cases of perfect and imperfect pre-\ndictions. The algorithm takes as input njobs with unknown\nsizes and breakpoints zσ(1), . . . , z σ(B)that depend on the\npredictions of xσ(1), . . . , x σ(B), then it alternates running\nround-robin on the jobs {σ(j)}j>B andShortest Predicted\nJob First (SPJF), introduced in (Purohit et al., 2018), on the\n5\n\nNon-clairvoyant Scheduling with Partial Predictions\njobs{σ(i)}i∈[B], where the moment of switching from an\nalgorithm to another is determined by the breakpoints.\nAs in Section 3, we call ordering ofzσ(1), . . . , z σ(B)any\nbijective application π: [B]→σ([B])satisfying zπ(1)≤\n. . .≤zπ(B). Note that, if the breakpoints are not pairwise\ndistinct, then the ordering is not unique. In that case, Switch\nchooses an ordering πuniformly at random. We assume\nfurthermore that the breakpoints induce the same order as\nthe predictions, i.e. zσ(i)< zσ(j)⇐⇒ yσ(i)< yσ(j)for\nalli, j∈[B].\nAlgorithm 2 Switch algorithm Switch (zσ, x)\nInput: Breakpoints zσ= (zσ(i))i∈[B]\nπ←ordering of zσchosen uniformly at random\nfori= 1toBdo\nwhile min\nj>BSσ(j)(t)\nxσ(j)<1andmax\nj>BSσ(j)(t)< zπ(i)do\nRun round-robin on {σ(j)}n\nj=B+1\nend while\nRun job π(i)until completion\nend for\nRun round-robin on {σ(j)}n\nj=B+1until completion\nConsider a run of Switch , and let i∈[B]. The first condi-\ntion for entering the while loop is the existence of j > B\nsuch that Sσ(j)(t)< x σ(j). This signifies that the jobs\n{σ(j)}j>B are not all completed, which is a verification\nfeasible for the decision-maker without knowledge of the\nsizes{xσ(j)}j>B. The second condition means that no job\nσ(j)forj > B has been in execution for more than zπ(i)\nunits of time. Given that round-robin allocates equal impor-\ntance to all jobs {σ(j)}j>B, upon exiting the while loop,\neach job σ(j)is either completed or has been in execution\nfor precisely zπ(i)units of time. Following this step, job\nπ(i)is executed until completion, and the same process\nrecurs for i+ 1.\nThis algorithm ensures that any job xσ(i)withi≤Bdoes\nnot delay any other job jwhose size is at most xj≤zσ(i),\nand the delay it causes to jobs not satisfying this condition\nis exactly xσ(i). This allows efficient control of the mu-\ntual delays between the jobs by conveniently choosing the\nbreakpoints.\n4.1. Perfect Predictions\nAssuming that the predictions are perfect, i.e. the decision-\nmaker knows the exact sizes of jobs σ(1), . . . , σ (B), it is\npossible to set zσ(i)=xσ(i)for all i∈[B].\nTheorem 4.1. Algorithm Switch with breakpoints zσ(i)=\nxσ(i)for all i∈[B]satisfies\nRn,B(Switch ) = 2−B\nn−2(1−B\nn)\nn+ 1.In particular, if B=⌊wn⌋for some w∈[0,1]then\nCRB(Switch ) = 2−w.\nNote that the (n, B)-competitive ratio above is strictly better\nthan that of CRRR , presented in Theorem 3.1. However,\nboth algorithms have equivalent performance when nis\nlarge. In particular, their competitive ratios coincide when\nB=⌊wn⌋.\nA slight improvement on the (n, B)-competitive ratio can be\nobtained by introducing randomness into Switch . Indeed,\nconsider the Run To Completion algorithm ( RTC ) defined\nin (Motwani et al., 1994), executing all the jobs until com-\npletion in a uniformly random order. Then we have the\nfollowing result.\nProposition 4.2. The algorithm that runs RTC with prob-\nability2(n−B)\nn(n+3)−2B, and runs Switch with breakpoints\nzσ(i)=xσ(i)for all i∈[B]with the remaining proba-\nbility, has an (n, B)-competitive ratio of\n2−B\nn−2(1−B\nn)(2−B\nn)\nn+ 3−2B\nn.\nForB= 0, the ratio above becomes 2−4\nn+3, which is the\nbest possible in the non-clairvoyant setting.\n4.2. Imperfect Predictions\nWe assume in this section that no quality guarantees are\ngiven on the predictions {yσ(i)}i∈[B]. Recall that the total\nerror ησ=PB\ni=1|xσ(i)−yσ(i)|is a random variable be-\ncause σis a uniformly random permutation of [n], hence\nour results will depend on E[ησ].\nThe goal is to design an algorithm that is consistent, robust,\nand with a competitive ratio having a smooth dependency\ntoE[ησ]. We first study the consistency and smoothness of\nSwitch with well-chosen breakpoints, then we show that\ncombining it with round-robin as in (Purohit et al., 2018;\nLassota et al., 2023) gives robustness guarantees.\nUsing the trivial breakpoints zσ(i)=yσ(i)as in the previous\nsection is not enough to guarantee smoothness. Consider,\nfor example, job sizes all equal to 1, and Bpredictions\nyσ(i)= 1−ϵfor an arbitrarily small ϵ. Blindly following\nthese predictions, taking zσ(i)=yσ(i)for all i∈[B], re-\nsults in delaying all jobs with unknown sizes by Btime units\ncompared to the case of perfect predictions. This creates a\ndiscontinuity in the competitive ratio when ϵbecomes posi-\ntive, proving non-smoothness. Hence, we consider instead\nrandomized breakpoints.\nAlgorithm 3 simply runs Switch with random breakpoints\nzσ(i)=ξyσ(i). The following lemma gives an upper bound\non the algorithm’s objective function depending on the dis-\ntribution Fofξ.\n6\n\nNon-clairvoyant Scheduling with Partial Predictions\nAlgorithm 3 imperfect predictions Switch (ξyσ, x)\nInput: predictions (yσ(i))i∈[B], distribution F\nSample ξ∼F\nRunSwitch with breakpoints zσ(i)=ξyσ(i)\nLemma 4.3. LetFbe a probability distribution on (0,∞),\nand consider the mappings hF: (0,∞)2→RandgF:\n(0,∞)→Rdefined by\nhF(s, t) =t·Prξ∼F(ξ <s\nt)\ngF(s) = (1 −s)Prξ∼F(ξ < s ) +Eξ∼F[ξ 1ξ<s].\nLetβF= sups∈(0,1]gF(s)\nsandγF= sups≥1(gF(s) +s).\nIfβF, γF<∞andhFisLF-Lipschitz w.r.t. the second\nvariable t, then for any job sizes x1, . . . , x nandB≤n, the\nexpected sum of the completion times achieved by Switch\nwith breakpoints zσ(i)=ξyσ(i)is at most\nnX\ni=1xi+C1\nn,B,FX\ni<jmin(xi, xj) +C2\nn,B,FE[ησ],\nwithC1\nn,B,F = 2−B\nn−\u0000\n2−βF−γF\u0001B\nn\u0000\n1−B−1\nn−1\u0001\nand\nC2\nn,B,F = (1 + LF+E[ξ])(n−B) +B−1.\nA trivial choice of ξis the constant random variable equal\nto1a.s., but this is not enough to guarantee smoothness, as\nit corresponds to the Dirac distribution F=δ1, for which\nhFis not continuous w.r.t. to t. In the next lemma, we\nprovide a specific choice of distribution Fdepending on a\nsingle parameter ρ, and we express the upper bound from\nthe previous lemma using this parameter.\nLemma 4.4. Letρ∈(0,1]and\nF:s7→(1−e−(s−1)/ρ) 1s>1\na shifted exponential distribution with parameter 1/ρ, i.e.\nξ∼1 +E(1/ρ), then Switch with breakpoints zσ(i)=\nξyσ(i)for all i∈[B]has an (n, B)-competitive ratio of at\nmost\n\u0010\n2−B\nn+ρB\nn(1−B−1\nn−1)\u0011\n+\u0010\n4\nρ(1−B\nn) +B\nn\u0011nE[ησ]\nOPT(x).\nThe previous lemma highlights a tradeoff between the\nsmoothness and the consistency of the algorithm. Indeed,\nasρdecreases, the algorithm gains in consistency, but the\nterm\u0010\n4\nρ(1−B\nn) +B\nn\u0011\nmultiplying E[ησ]becomes larger.\nHowever, while setting ρclose to zero results in an arbitrar-\nily high sensitivity to the error, setting it close to 1gives\nconsistency of at most\u0000\n2−B(B−1)\nn(n−1)\u0001\n, which is still a de-\ncreasing function of B, interpolating the values 2and1\nin the clairvoyant and non-clairvoyant cases. This impliesthat sacrificing a small amount of consistency significantly\nimproves smoothness.\nForB=n, assuming that all the job sizes are at least 1,\nit holds that OPT(x)≥n(n+ 1)/2and the lemma gives\nRn,B(η;Switch )≤1 +2η\nn, matching the bound proved on\n(SPJF) in (Purohit et al., 2018). On the other hand, if η= 0,\nsetting ρ= 0results in Rn,B(0;Switch )≤2−B\nn. Using\ntighter inequalities in the proof, it is possible to retrieve the\nbound established in Theorem 4.1 (See Inequality (36) in\nAppendix C).\nPreferential algorithm Now we need to adapt the algo-\nrithm to guarantee robustness in the face of arbitrarily erro-\nneous predictions. We use the same approach as Lemma 3.1\nof (Purohit et al., 2018), which consists of running concur-\nrently a consistent algorithm and round-robin at respective\nrates λ,1−λfor some λ∈[0,1]. However, their result\nonly applies for deterministic algorithms Asatisfying for\nany instances x= (x1, . . . , x n)andx′= (x′\n1, . . . , x′\nn)that\n\u0000\n∀i∈[n] :xi≤x′\ni\u0001\n=⇒A(x)≤A(x′).\nSuch algorithms are called monotonic .Switch with break-\npoints zσ(i)=ξyσ(i)is not deterministic since its objective\nfunction depends both on σandξ. Nonetheless, we over-\ncome this difficulty by proving that, conditionally to σand\nξ, its outcome is deterministic and monotonic, then we es-\ntablish the following theorem.\nTheorem 4.5. Letρ∈(0,1]andF= 1+E(1/ρ). Then the\npreferential algorithm ALG λwhich runs Algorithm 3 at rate\nλand round-robin at rate 1−λhas an (n, B)-competitive\nratio of at most\nmin\u00122\n1−λ,Cρ,n,B\nλ+Sρ,n,B\nλ·nE[ησ]\nOPT(x)\u0013\n,\nwith\nCρ,n,B =\u0000\n2−B\nn\u0001\n+ρB\nn\u0000\n1−B−1\nn−1\u0001\nSρ,n,B =4\nρ\u0000\n1−B\nn\u0001\n+B\nn.\nThis upper bound generalizes that of (Purohit et al., 2018). It\npresents a consistency-robustness tradeoff that can be tuned\nby adjusting the parameter λ, and a consistency-smoothness\ntradeoff controlled by the parameter ρ, which vanishes for\nBclose to 0orn, as the terms multiplying ρand1/ρre-\nspectively in Cρ,n,B andSρ,n,B become zero.\n5. Experiments\nIn this section, we validate our theoretical findings by test-\ning the algorithms we presented on various benchmark job\nsizes. In all the figures, each point is averaged over 104\nindependent trials.\n7\n\nNon-clairvoyant Scheduling with Partial Predictions\nPerfect information We test the performance of Algo-\nrithms Switch andCRRR against the hard instances used\nto prove the lower bounds of Section 2: we consider i.i.d.\njob sizes sampled from the exponential distribution with pa-\nrameter 1, and job sizes drawn from the distribution Φ(r, a)\nwith parameters r= 0.51anda= 104, characterized by\nthe tail probability\nPr(xa\ni≥t) =(1 +t)−r−(1 +a)−r\n1−(1 +a)−r1t<a,\nThis distribution is a truncated version of the one defined\nbyPr(x∞\ni≥t) =1\n(1+t)r. The bound of Corollary 2.3 is\nobtained by using this distribution for a >0andr∈(1\n2,1),\nand taking the limits n→ ∞ ,a→ ∞ , and r→1/2.\n1.001.251.501.752.00xi∼E(1)\nSwitch\nCRRR\nCor 2.3\nn = 20xi∼Φ(0.51,104)\nSwitch\nCRRR\nCor 2.4\n0 1/2 11.001.251.501.752.00Switch\nCRRR\nCor 2.3\n0 1/2 1\nn = 1000Switch\nCRRR\nCor 2.4Empirical competitive ratio\nfraction B/nof known job sizes\nFigure 2. Lower bounds and ratios of Switch ,CRRR\nFigure 2 exhibits the empirical ratios achieved by both\nalgorithms with a number n∈ {20,1000}of jobs. For\nn= 20 ,Switch outperforms CRRR for the two distribu-\ntions, whereas their ratios are very close for n= 1000 . This\nconfirms that Switch andCRRR are asymptotically equiva-\nlent, as can be deduced from Theorems 3.1 and 4.1. For the\nexponential distribution, as expected, both algorithms have\nratios above the non-asymptotic lower bound of Corollary\n2.2. Meanwhile, considering the distribution Φ(0.51,104),\nthe empirical ratios for n= 20 are below the lower bound\nof Corollary 2.3, because it is proved by taking n→ ∞ .\nForn= 1000 , the ratios match the lower bound.\nPreferential algorithm In the remaining discussion, we\nrefer to Switch with breakpoints zσ(i)=ξyσ(i)andξ∼\n1 +E(1/ρ), asSwitch with parameter ρ.\nWe generate a synthetic instance of n= 50 job sizes, drawn\nindependently from the Pareto distribution with scale 1 and\nshape 1.1. The Pareto distribution, known for its heavytail, is particularly suitable for modeling job sizes (Harchol-\nBalter & Downey, 1997; Bansal & Harchol-Balter, 2001;\nArnold, 2014), and it is a commonly used benchmark for\nlearning-augmented scheduling algorithms (Purohit et al.,\n2018; Lindermayr & Megow, 2022). Furthermore, we con-\nsider noisy predictions yi=xi+εifor all i∈[50], where\nεiis sampled independently from a normal distribution with\nmean 0and standard deviation τ.\n0 20 40 60 80 100\nError parameter t1.41.61.82.0Emprical competitive ratioPA(r,l) with B=n/2\nRound-robin\nSwitch( r=0.5)\nPA(r=0,l=0.5)\nPA(r=0.5,l=0.5)\n0 100 200 300 400 500\nError parameter t1.01.52.02.53.03.5\nSwitch( r=0.5,B) for B∈[1,n]\nB=0\nB = 10\nB = 20B = 30\nB = 35\nB = 50\nFigure 3. Preferential Algorithm (PA) with different parameters\nFigure 3 illustrates the empirical ratio of the Preferential\nAlgorithm (PA) across various parameter configurations,\nwith varying error parameter τ.\nThe left plot displays the ratios for different λandρvalues,\nwithB= 25 = n/2. When λ= 0, PA becomes round-\nrobin. For λ= 1andρ= 0.5, PA simply runs Switch (ρ=\n0.5), which gives an improved consistency ( τ= 0), not\nequal to 1because B < n andρ > 0, and gives a ratio\nthat deteriorates arbitrarily as τincreases. In contrast, PA\nwith λ= 0.5gives a weaker consistency but maintains\nbounded ratios, even with arbitrarily erroneous predictions.\nThe choice of ρ= 0exhibits a slightly better consistency\ncompared to ρ= 0.5, in line with theoretical expectations,\nbut there is no significant difference regarding sensitivity\nto errors. This should not be surprising since setting ρ >0\nensures smoothness in the worst-case (see Figure 4), but it\nis not necessarily needed for all instances.\nThe right plot examines the influence of Bon PA with pa-\nrameters λ= 1andρ= 0.5, which corresponds to Switch\nwithρ= 0.5. Larger Bvalues improve consistency and\nalso yield a smaller sensitivity to small errors. However, for\nhighτvalues, having numerous predictions leads to faster\nperformance deterioration compared to having fewer predic-\ntions. This shows that more predictions enhance consistency,\nwhile fewer predictions enhance robustness.\nConsistency-smoothness To shed light on the tradeoff\nbetween consistency and smoothness raised in Section 4.2,\nwe consider i.i.d. job sizes x1, . . . , x 100, each taking the\nvalue 1 w.p. 1/2 and 2 w.p. 1/2, and we consider noisy\npredictions of the form yi=xi+εi, where εifollows a\nuniform distribution over [−τ, τ]. Figure 4 illustrates the\nevolution, for τvarying in [0,0.15], of the empirical com-\npetitive ratio of Switch with parameter ρ∈ {0,0.1,0.5}\n8\n\nNon-clairvoyant Scheduling with Partial Predictions\nandB∈ {50,95},\nFor both values of B, the experiment reveals that larger\nvalues of ρgive bigger ratios when τ= 0(less consistency),\nbut on the other hand they yield less sensitivity to variations\nof the expected prediction error (better smoothness), which\nconfirms our theoretical findings. In particular, for ρ= 0,\na significant discontinuity arises when τbecomes positive.\nFigure 4 also shows that this tradeoff is less significant as\nBapproaches n= 100 , with the consistency values for\nρ∈ {0,0.1,0.5}drawing closer.\n0.00 0.05 0.10 0.15\nError parameter t1.351.401.451.50Empirical competitive ratio\nr= 0,B= 50\nr= 0.1, B= 50\nr= 0.5, B= 50\n0.00 0.05 0.10 0.15\nError parameter t1.031.041.05\nr= 0,B= 95\nr= 0.1, B= 95\nr= 0.5, B= 95\nFigure 4. Tradeoff between consistency and smoothness\n6. Conclusion and Future Work\nThis paper explores the non-clairvoyant scheduling prob-\nlem with a limited number of predicted job sizes. We give\nnear optimal lower and upper bounds in the case of perfect\npredictions, and we introduce a learning-augmented algo-\nrithm raising the common consistency-robustness tradeoff\nand an additional consistency-smoothness tradeoff, the latter\nvanishing when Bapproaches 0orn.\nOur findings join previous works in demonstrating that on-\nline algorithms can indeed achieve improved performance\neven when armed with a restricted set of predictions, which\nis an assumption more aligned with practical scenarios. Fur-\nthermore, they affirm the necessity of studying and under-\nstanding these regimes, as they may unveil unique behaviors\nabsent in the zero- or full-information settings.\n6.1. Open Questions\nTight lower bounds In the case of perfect predictions,\nthere is a (small) gap between the lower bounds of Section\n2 and the competitive ratios of Switch andCRRR . An\ninteresting research avenue is to close this gap, either by\ndesigning better algorithms or improving the lower bound.\nThis could involve using Theorem 2.1 with more refined\ndistributions.\nReduced number of action predictions Algorithm\nSwitch leverages the job sizes’ predictions, not only the\norder they induce. Using the ℓ1norm to measure the error\nis thus a suitable choice. However, as discussed in Section\n3, Algorithm CRRR only uses the priority order in whichOPT runs(xσ(i))i∈[B]. An interesting question to explore\nis how to adapt it in the case of imperfect action predictions ,\nusing appropriate error measures.\nSmooth and (2−B\nn)-consistent algorithm Lemma 4.4\nand Figure 4 emphasize that, to achieve smoothness, Switch\nwith parameter ρmust exhibit a consistency exceeding 2−B\nn.\nA compelling question arises: Is it possible to devise a\nsmooth algorithm with a consistency of at most 2−B\nn?\nImpact Statement\nThis paper presents a work whose goal is to advance the\nfield of learning-augmented algorithms. There are many\npotential societal consequences of our work, none of which\nwe feel must be specifically highlighted here.\nAcknowledgements\nVianney Perchet acknowledges support from the French\nNational Research Agency (ANR) under grant number\n(ANR19-CE23-0026 as well as the support grant, as\nwell as from the grant “Investissements d’Avenir” (LabEx\nEcodec/ANR11-LABX-0047).\nReferences\nAnand, K., Ge, R., and Panigrahi, D. Customizing ml predic-\ntions for online algorithms. In International Conference\non Machine Learning , pp. 303–313. PMLR, 2020.\nAntoniadis, A., Gouleakis, T., Kleer, P., and Kolev, P. Secre-\ntary and online matching problems with machine learned\nadvice. Advances in Neural Information Processing Sys-\ntems, 33:7933–7944, 2020.\nAntoniadis, A., Coester, C., Eli ´as, M., Polak, A., and Simon,\nB. Learning-augmented dynamic power management\nwith multiple states via new ski rental bounds. Advances\nin Neural Information Processing Systems , 34:16714–\n16726, 2021a.\nAntoniadis, A., Ganje, P. J., and Shahkarami, G. A novel\nprediction setup for online speed-scaling. arXiv preprint\narXiv:2112.03082 , 2021b.\nAntoniadis, A., Boyar, J., Eli ´as, M., Favrholdt, L. M.,\nHoeksma, R., Larsen, K. S., Polak, A., and Simon, B.\nPaging with succinct predictions. In International Confer-\nence on Machine Learning , pp. 952–968. PMLR, 2023a.\nAntoniadis, A., Coester, C., Eli ´aˇs, M., Polak, A., and Simon,\nB. Online metric algorithms with untrusted predictions.\nACM Transactions on Algorithms , 19(2):1–34, 2023b.\nArnold, B. C. Pareto distribution. Wiley StatsRef: Statistics\nReference Online , pp. 1–10, 2014.\n9\n\nNon-clairvoyant Scheduling with Partial Predictions\nAzar, Y ., Leonardi, S., and Touitou, N. Flow time scheduling\nwith uncertain processing time. In Proceedings of the\n53rd Annual ACM SIGACT Symposium on Theory of\nComputing , pp. 1070–1080, 2021.\nBai, X. and Coester, C. Sorting with predictions. Advances\nin Neural Information Processing Systems , 36, 2024.\nBaker, K. R. and Trietsch, D. Principles of sequencing and\nscheduling . John Wiley & Sons, 2013.\nBamas, ´E., Maggiori, A., Rohwedder, L., and Svensson, O.\nLearning augmented energy minimization via speed scal-\ning.Advances in Neural Information Processing Systems ,\n33:15350–15359, 2020a.\nBamas, E., Maggiori, A., and Svensson, O. The primal-dual\nmethod for learning augmented algorithms. Advances\nin Neural Information Processing Systems , 33:20083–\n20094, 2020b.\nBampis, E., Dogeas, K., Kononov, A. V ., Lucarelli, G.,\nand Pascual, F. Scheduling with untrusted predictions.\nInProceedings of the Thirty-First International Joint\nConference on Artificial Intelligence, IJCAI , pp. 4581–\n4587, 2022.\nBansal, N. and Harchol-Balter, M. Analysis of srpt schedul-\ning: Investigating unfairness. In Proceedings of the 2001\nACM SIGMETRICS International conference on Measure-\nment and modeling of computer systems , pp. 279–290,\n2001.\nBateni, M., Dharangutte, P., Jayaram, R., and Wang, C.\nMetric clustering and mst with strong and weak distance\noracles. arXiv preprint arXiv:2310.15863 , 2023.\nBenomar, Z. and Perchet, V . Advice querying under bud-\nget constraint for online algorithms. In Thirty-seventh\nConference on Neural Information Processing Systems ,\n2023.\nBenomar, Z., Chzhen, E., Schreuder, N., and Perchet, V .\nAddressing bias in online selection with limited budget\nof comparisons. arXiv preprint arXiv:2303.09205 , 2023.\nBhaskara, A., Cutkosky, A., Kumar, R., and Purohit, M.\nLogarithmic regret from sublinear hints. Advances in Neu-\nral Information Processing Systems , 34:28222–28232,\n2021.\nBorodin, A. and El-Yaniv, R. Online computation and\ncompetitive analysis . cambridge university press, 2005.\nChen, J., Silwal, S., Vakilian, A., and Zhang, F. Faster\nfundamental graph algorithms via learned predictions.\nInInternational Conference on Machine Learning , pp.\n3583–3602. PMLR, 2022.Cheng, T. and Sin, C. A state-of-the-art review of parallel-\nmachine scheduling research. European Journal of Oper-\national Research , 47(3):271–292, 1990.\nChlkedowski, J., Polak, A., Szabucki, B., and .Zolna, K. T.\nRobust learning-augmented caching: An experimental\nstudy. In International Conference on Machine Learning ,\npp. 1920–1930. PMLR, 2021.\nChristianson, N., Shen, J., and Wierman, A. Optimal\nrobustness-consistency tradeoffs for learning-augmented\nmetrical task systems. In International Conference on Ar-\ntificial Intelligence and Statistics , pp. 9377–9399. PMLR,\n2023.\nDiakonikolas, I., Kontonis, V ., Tzamos, C., Vakilian, A.,\nand Zarifis, N. Learning online algorithms with distribu-\ntional advice. In International Conference on Machine\nLearning , pp. 2687–2696. PMLR, 2021.\nDinitz, M., Im, S., Lavastida, T., Moseley, B., and Vassilvit-\nskii, S. Faster matchings via learned duals. Advances in\nneural information processing systems , 34:10393–10406,\n2021.\nDinitz, M., Im, S., Lavastida, T., Moseley, B., and Vassil-\nvitskii, S. Algorithms with prediction portfolios. Ad-\nvances in neural information processing systems , 35:\n20273–20286, 2022.\nDrygala, M., Nagarajan, S. G., and Svensson, O. Online\nalgorithms with costly predictions. In International Con-\nference on Artificial Intelligence and Statistics , pp. 8078–\n8101. PMLR, 2023.\nD¨utting, P., Lattanzi, S., Paes Leme, R., and Vassilvitskii,\nS. Secretaries with advice. In Proceedings of the 22nd\nACM Conference on Economics and Computation , pp.\n409–429, 2021.\nEberle, F., Hommelsheim, F., Lindermayr, A., Liu, Z.,\nMegow, N., and Schl ¨oter, J. Accelerating matroid op-\ntimization through fast imprecise oracles. arXiv preprint\narXiv:2402.02774 , 2024.\nGollapudi, S. and Panigrahi, D. Online algorithms for rent-\nor-buy with expert advice. In International Conference\non Machine Learning , pp. 2319–2327. PMLR, 2019.\nGraham, R. L., Lawler, E. L., Lenstra, J. K., and Kan, A. R.\nOptimization and approximation in deterministic sequenc-\ning and scheduling: a survey. In Annals of discrete math-\nematics , volume 5, pp. 287–326. Elsevier, 1979.\nHall, N. G. and Potts, C. N. Supply chain scheduling: Batch-\ning and delivery. Operations Research , 51(4):566–584,\n2003.\n10\n\nNon-clairvoyant Scheduling with Partial Predictions\nHarchol-Balter, M. and Downey, A. B. Exploiting process\nlifetime distributions for dynamic load balancing. ACM\nTransactions on Computer Systems (TOCS) , 15(3):253–\n285, 1997.\nIm, S., Kumar, R., Montazer Qaem, M., and Purohit, M.\nNon-clairvoyant scheduling with predictions. In ACM\nSymposium on Parallelism in Algorithms and Architec-\ntures , 2021.\nIm, S., Kumar, R., Petety, A., and Purohit, M. Parsimonious\nlearning-augmented caching. In International Conference\non Machine Learning , pp. 9588–9601. PMLR, 2022.\nIvanov, D., Dolgui, A., Sokolov, B., Werner, F., and Ivanova,\nM. A dynamic model and an algorithm for short-term\nsupply chain scheduling in the smart factory industry\n4.0.International Journal of Production Research , 54(2):\n386–402, 2016.\nJensen, E. D., Locke, C. D., and Tokuda, H. A time-driven\nscheduling model for real-time operating systems. In\nRtss, volume 85, pp. 112–122, 1985.\nJin, B. and Ma, W. Online bipartite matching with advice:\nTight robustness-consistency tradeoffs for the two-stage\nmodel. Advances in Neural Information Processing Sys-\ntems, 35:14555–14567, 2022.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis,\nN. The case for learned index structures. In Proceedings\nof the 2018 international conference on management of\ndata, pp. 489–504, 2018.\nLassota, A. A., Lindermayr, A., Megow, N., and Schl ¨oter,\nJ. Minimalistic predictions to schedule jobs with online\nprecedence constraints. In International Conference on\nMachine Learning , pp. 18563–18583. PMLR, 2023.\nLattanzi, S., Lavastida, T., Moseley, B., and Vassilvitskii,\nS. Online scheduling via learned weights. In Proceed-\nings of the Fourteenth Annual ACM-SIAM Symposium on\nDiscrete Algorithms , pp. 1859–1877. SIAM, 2020.\nLawler, E. L., Lenstra, J. K., Kan, A. H. R., and Shmoys,\nD. B. Sequencing and scheduling: Algorithms and com-\nplexity. Handbooks in operations research and manage-\nment science , 4:445–522, 1993.\nLenstra, J. K. and Rinnooy Kan, A. Complexity of schedul-\ning under precedence constraints. Operations Research ,\n26(1):22–35, 1978.\nLin, H., Luo, T., and Woodruff, D. Learning augmented\nbinary search trees. In International Conference on Ma-\nchine Learning , pp. 13431–13440. PMLR, 2022.Lindermayr, A. and Megow, N. Permutation predictions\nfor non-clairvoyant scheduling. In Proceedings of the\n34th ACM Symposium on Parallelism in Algorithms and\nArchitectures , pp. 357–368, 2022.\nLykouris, T. and Vassilvtiskii, S. Competitive caching with\nmachine learned advice. In International Conference on\nMachine Learning , pp. 3296–3305. PMLR, 2018.\nMaghakian, J., Lee, R., Hajiesmaili, M., Li, J., Sitaraman,\nR., and Liu, Z. Applied online algorithms with hetero-\ngeneous predictors. In International Conference on Ma-\nchine Learning , pp. 23484–23497. PMLR, 2023.\nMartel, C. Preemptive scheduling with release times, dead-\nlines, and due times. Journal of the ACM (JACM) , 29(3):\n812–829, 1982.\nMerlis, N., Richard, H., Sentenac, F., Odic, C., Molina, M.,\nand Perchet, V . On preemption and learning in stochastic\nscheduling. In International Conference on Machine\nLearning , pp. 24478–24516. PMLR, 2023.\nMitzenmacher, M. and Vassilvitskii, S. Algorithms with\npredictions. Communications of the ACM , 65(7):33–35,\n2022.\nMotwani, R., Phillips, S., and Torng, E. Nonclairvoyant\nscheduling. Theoretical computer science , 130(1):17–47,\n1994.\nMunoz, A. and Vassilvitskii, S. Revenue optimization with\napproximate bid predictions. Advances in Neural Infor-\nmation Processing Systems , 30, 2017.\nPanwalkar, S. S. and Iskander, W. A survey of scheduling\nrules. Operations research , 25(1):45–61, 1977.\nPinedo, M. L. Scheduling , volume 29. Springer, 2012.\nPurohit, M., Svitkina, Z., and Kumar, R. Improving on-\nline algorithms via ml predictions. Advances in Neural\nInformation Processing Systems , 31, 2018.\nRamamritham, K. and Stankovic, J. A. Scheduling algo-\nrithms and operating systems support for real-time sys-\ntems. Proceedings of the IEEE , 82(1):55–67, 1994.\nSadek, K. A. A. and Elias, M. Algorithms for caching and\nMTS with reduced number of predictions. In The Twelfth\nInternational Conference on Learning Representations ,\n2024. URL https://openreview.net/forum?\nid=QuIiLSktO4 .\nSakaue, S. and Oki, T. Discrete-convex-analysis-based\nframework for warm-starting algorithms with predictions.\nAdvances in Neural Information Processing Systems , 35:\n20988–21000, 2022.\n11\n\nNon-clairvoyant Scheduling with Partial Predictions\nShin, Y ., Lee, C., Lee, G., and An, H.-C. Improved learning-\naugmented algorithms for the multi-option ski rental prob-\nlem via best-possible competitive analysis. arXiv preprint\narXiv:2302.06832 , 2023.\nSilwal, S., Ahmadian, S., Nystrom, A., McCallum, A.,\nRamachandran, D., and Kazemi, S. M. Kwikbucks:\nCorrelation clustering with cheap-weak and expensive-\nstrong signals. In The Eleventh International Confer-\nence on Learning Representations , 2023. URL https:\n//openreview.net/forum?id=p0JSSa1AuV .\nSteiger, C., Walder, H., and Platzner, M. Operating systems\nfor reconfigurable embedded platforms: Online schedul-\ning of real-time tasks. IEEE Transactions on computers ,\n53(11):1393–1407, 2004.\nWei, A. and Zhang, F. Optimal robustness-consistency\ntrade-offs for learning-augmented online algorithms. Ad-\nvances in Neural Information Processing Systems , 33:\n8042–8053, 2020.\nYao, A. C.-C. Probabilistic computations: Toward a unified\nmeasure of complexity. In 18th Annual Symposium on\nFoundations of Computer Science (sfcs 1977) , pp. 222–\n227. IEEE Computer Society, 1977.\n12\n\nNon-clairvoyant Scheduling with Partial Predictions\nA. Lower bounds\nA.1. Preliminary result\nLemma A.1. LetFbe a probability distribution on (0,∞)with finite expectation, xan array of ni.i.d. random variables\nsampled from F, and αn,B≥0satisfying for any deterministic algorithm Awith access the sizes of Bjobs that\nEσ,x[A(x)]\nEx[OPT(x)]≥αn,B,\nthen for any (deterministic or randomized) algorithm ALG , we have\nRn,B(ALG)≥αn,B.\nProof. LetFandαn,Bbe as stated in the lemma. Using Yao’s minimax principle (Yao, 1977) we deduce that for any\nrandomized algorithm ALG\nEσ,x[ALG(x)]≥ inf\nAdeterministicEσ,x[A(x)]\n≥αn,BEx[OPT(x)],\nwhere the infimum is taken over all deterministic algorithms. The previous inequality can be written as\nEx\u0002\nEσ[ALG(x)]−αn,BOPT(x)\u0003\n≥0,\nand this implies that, necessarily, there exists a value x∗= (x∗\nn, . . . , x∗\nn)taken by xverifying Eσ[ALG(x∗)]−\nαn,BOPT(x∗)≥0, hence\nRn,B(ALG)≥Eσ[ALG(x∗)]\nOPT(x∗)≥αn,B.\nA.2. Proof of Theorem 2.1\nLetAbe a deterministic algorithm. Using Equation 1, it suffices to bound E[PA\nij]for all i̸=jto deduce a bound on E[A(x)],\nand since PA\nijis a non-negative random variable, the focus can be narrowed down to bounding Pr(PA\nij> t)for all t >0.\nFollowing the proof scheme of (Motwani et al., 1994), we introduce the following definition.\nDefinition A.2. LetAbe a deterministic algorithm given an instance of njobs. For all i̸=j∈[n]andt≥0, we denote by\nuA\ni,j(t)the time spent on job iwhen the total time spent on both jobs iandjist, assuming neither job inorjis completed.\nMore precisely, uA\ni,j(t)is defined by\nuA\ni,j(t) =SA\ni\u0000\ninf{t′≥0 :SA\ni(t′) +SA\nj(t′) =t}\u0001\n.\nuA\ni,jis therefore a rule defined solely by the algorithm. For an instance x= (x1, . . . , x n)of job sizes, the real time spent on\niwhen a total time of thas been spent on both jobs i, jis given by min(uA\ni,j(t), xi).\nFor all t≥0, the following Lemma allows to express the event PA\nij> tusing uA\nij(t).\nLemma A.3. Letx1, . . . , x nbe instance of njob sizes, then for any algorithm A, for any i̸=j∈[n]andt≥0, the\nfollowing equivalence holds\u0000\nPA\nij> t\u0001\n⇐⇒\u0000\nxi> uA\ni,j(t)andxj> t−uA\nj,i(t)\u0001\nProof. LetAbe an algorithm and i̸=j∈[n]. We denote by Sij(t) =Si(t) +Sj(t)the total processing time spent on\nboth jobs iandjup to time t. Assuming that job ifinishes first, i.e. ti≤tj, no processing time is spent on job iafterti,\nhence Si(tj) =Si(ti) =xi, and Pij=Si(tj) +Sj(ti) =Sij(ti). By symmetry, we deduce that Pij=Sij(min( ti, tj)).\n13\n\nNon-clairvoyant Scheduling with Partial Predictions\nTherefore, using that SiandSjare non-decreasing and continuous, it holds for all t≥0\nPij> t⇐⇒ Sij(min( ti, tj))> t\n⇐⇒ min(ti, tj)>inf{t′:Sij(t′)≥t}\n⇐⇒ xi> Si(inf{t′:Sij(t′)≥t})andxj> Sj(inf{t′:Sij(t′)≥t}) (5)\n⇐⇒ xi> uij(t)andxj> t−uij(t). (6)\nEquivalence (5)holds because ti= inf{t′≥0 :Si(t′)≥xi}, thus for any s≥0we have t > s ⇐⇒ xi> Si(s). The\nsame holds for j. For Equivalence (6), we simply used Definition A.2 and the observation uij(t) +uji(t) =t.\nRemark A.4.In the case of non-clairvoyant algorithms, the rule defining uA\nij(·)is dictated by the algorithm, independent of\nthe job sizes. Thus, if the job sizes are sampled independently from the exponential distribution, Lemma A.3 gives for all\ni̸=jthatPr(PA\nij> t) = Pr( xi> uA\ni,j(t)) Pr( xj> t−uA\nj,i(t)) =e−t, and it follows immediately that E[PA\nij] = 1 for any\ndeterministic algorithm. This argument, used in (Motwani et al., 1994), is not applicable in our context since the algorithm\npossesses access to certain job sizes, enabling the formulation of a rule for uA\ni,jthat considers this information. Therefore,\nmore sophisticated techniques become necessary for our analysis since the independence of the events xi> uA\ni,j(t)and\nxj> t−uA\nj,i(t)is lost.\nLemma A.5. Leta∈(0,∞)andφ: [0,∞)→[0,∞)a continuously differentiable and increasing function satisfying that\nφ(0)>0,t7→φ′(t)\nφ(t)is non-increasing andR∞\n0dt\nφ(t)2<∞. Letxa\n1, . . . , xa\nni.i.d. random job sizes with distribution\nPr(xa\n1≤t) = 1−φ(t)−1−φ(a)−1\nφ(0)−1−φ(a)−11t<a,\nthen for any algorithm Ahaving access to the sizes of the first Bjobsxa\n1, . . . , xa\nB, it holds that\nE[PA\nij]≥Z∞\n0φ(0)2\nφ(t)2dt−o(1)\na→∞∀i̸=j≤B ,\nE[PA\nij]≥2Z∞\n0φ(0)2\nφ(t)2dt−o(1)\na→∞∀i̸=j > B ,\nE[PA\nij]≥φ(0)2Z∞\n0\u001a\ninf\nT≥xGφ(x, T)\u001bφ′(x)\nφ(x)dx−o(1)\na→∞∀i≤B, j > B ,\nwhere Gφ(x, T)is defined in Equation (3), and the o(1)\na→∞term does not depend on the algorithm A.\nAlthough Lemma A.5 is stated with a∈(0,∞), the results also hold for random variables x∞\n1, . . . , x∞\nnsampled from the\nlimit distribution Pr(x∞\n1≤t) = 1−φ(0)\nφ(t), where the o(1)term becomes zero.\nBefore proving the lemma, let us first observe that, since φis increasing andR∞\n0dt\nφ(t)2<∞, then necessarily\nlimx→∞φ(x) =∞.\nProof. Let us first observe that for all t≥0\nPr(min( x∞\n1, x∞\n2)≥t) = Pr( x∞\n1≥t) Pr(x∞\n2≥t) =φ(0)2\nφ(t)2,\nhence\nE[min( x∞\n1, x∞\n2)] =Z∞\n0Pr(min( x∞\n1, x∞\n2)≥t)dt=Z∞\n0φ(0)2\nφ(t)2dt .\nIn the following, we prove separately the three claims of the Lemma: in the cases where both jobs sizes xi, xjare known,\nboth are unknown, and where only xiis known.\n14\n\nNon-clairvoyant Scheduling with Partial Predictions\nBoth job sizes are known For any i̸=j∈[n], it holds that Pij≥min(xa\ni, xa\nj), and this true in particular for i̸=j∈[B].\nFurthermore, for any t≥0, since φ(t)≥φ(0), we have that the mapping a7→φ(t)−1−φ(a)−1\nφ(0)−1−φ(a)−1is non-increasing. It follows\nthat\nPr(xa\n1≤t)≤1−\u0012\nlim\na′→∞φ(t)−1−φ(a′)−1\nφ(0)−1−φ(a′)−1\u0013\n1t<a\n= 1−φ(0)\nφ(t)1t<a, (7)\nand therefore, for any i̸=j≤B\nE[Pij]≥E[min( xa\n1, xa\n2)]\n=Z∞\n0Pr(xa\n1≥t)2dt\n≥Z∞\n0φ(0)2\nφ(t)21t<adt=Za\n0φ(0)2\nφ(t)2dt\n=Z∞\n0φ(0)2\nφ(t)2dt−o(1)\na→∞.\nBoth job sizes are unknown Fori̸=j > B , the algorithm ignores the job sizes xa\niandxa\njTherefore, ui,jis independent\nofxa\niandxa\nj. Consequently, using Lemma A.3, the independence of xa\niandxa\nj, then Inequality (7), we obtain\nPr(Pij> t) = Pr( xi> ui,j(t)andxj> t−ui,j(t))\n= Pr( xi> ui,j(t)) Pr( xj> t−ui,j(t))\n≥\u0012φ(0)\nφ(ui,j(t))1ui,j(t)<a\u0013\u0012φ(0)\nφ(t−ui,j(t))1t−ui,j(t)<a\u0013\n=φ(0)21t−a<ui,j(t)<a\nφ(ui,j(t)φ(t−ui,j(t)).\nUsing thatφ′\nφis non-increasing, we have for any t, u≥0that\nd\ndu(φ(u)φ(t−u)) =φ′(u)φ(t−u)−φ(u)φ′(t−u)\n=φ(u)φ(t−u)\u0012φ′(u)\nφ(u)−φ′(t−u)\nφ(t−u)\u0013\nThe sign ofd\ndu(φ(u)φ(t−u))is the same as that ofφ′(u)\nφ(u)−φ′(t−u)\nφ(t−u), which is a non-increasing function. It is null for\nu=t/2, hence it is non-positive for u≤t/2and non-negative for u≥t/2. This implies that u7→φ(u)φ(t−u)is minimal\nforu=t/2, i.e.φ(u)φ(t−u)≤φ(t\n2)2for all t, u≥0, in particular\nPr(Pij> t)≥φ(0)2\nφ(t/2)21t−a<ui,j(t)<a,\nand it follows that\nE[Pij] =Z∞\n0Pr(Pij> t)dt≥Z∞\n0φ(0)2\nφ(t\n2)21t−a<ui,j(t)<adt .\nObserving that, for any t≥0, the mapping a7→φ(0)2/φ(t\n2)21t−a<ui,j(t)<ais non-decreasing, and its limit is\nφ(0)2/φ(t\n2)2, the monotone convergence theorem guarantees that\nlim\na→∞Z∞\n0φ(0)2\nφ(t\n2)21t−a<ui,j(t)<adt=Z∞\n0φ(0)2\nφ(t\n2)2dt\n= 2Z∞\n0φ(0)2\nφ(t)2dt ,\n15\n\nNon-clairvoyant Scheduling with Partial Predictions\ntherefore, for any i̸=j > B\nE[Pij]≥2Z∞\n0φ(0)2\nφ(t)2dt−o(1)\na→∞.\nOnly the size of one job is known Fori≤Bandj > B , the algorithm knows xa\ni, therefore uij(·)can be a function of\nxa\ni. By Lemma A.3 then Inequality (7), it holds for all t≥0that\nPr(Pij> t) = Pr( xa\ni> ui,j(t)andxa\nj> t−ui,j(t))\n=E[ 1xa\ni>ui,j(t) 1xa\nj>t−ui,j(t)]\n=E\u0002\n1xa\ni>ui,j(t)E[ 1xa\nj>t−ui,j(t)|xa\ni, ui,j(t)]\u0003\n=E\u0002\n1xa\ni>ui,j(t)Pr\u0000\nxa\nj> t−ui,j(t)|ui,j(t)\u0001\u0003\n≥E\u0014\n1xa\ni>ui,j(t)φ(0) 1t−ui,j(t)<a\nφ(t−ui,j(t))\u0015\n≥φ(0)E\u00141t−a<ui,j(t)<xa\ni\nφ(t−ui,j(t))\u0015\n,\nand we obtain by integrating over tand then using Tonelli’s theorem that\nE[Pij] =Z∞\n0Pr(Pij> t)dt\n≥φ(0)E\u0014Z∞\n01t−a<ui,j(t)<xa\ni\nφ(t−ui,j(t))dt\u0015\n.\nRecall that ui,j(t)is defined as the time spent on job iwhen a total of tunits of time have been spent on both jobs iandj,\nthusuij(0) = 0 ,uijis non-decreasing and it is 1-Lipschitz. Let us denote by Vthe set of functions on [0,∞)satisfying\nthese properties\nV=n\nv: [0,∞)→[0,∞) :v(0) = 0 andv(t2)−v(t1)\nt2−t1∈[0,1]∀t1< t2o\n,\nand for all x≥0andT∈[0,∞], we denote by Vx,Tthe subset of Vdefined as\nVx,T=n\nv∈ V: inf{t≥0 :v(t)≥x}=To\n.\nFor any x≥0andv∈ V, letTv= inf{t≥0 :v(t)≥x}, then v∈ Vx,Tv. Since vis1-Lipschitz and v(0) = 0 , it holds\nthatTv≥x. Ifv(t)< x for all t≥0thenTv=∞. Therefore, we have for any x≥0\nV=[\nT∈[x,∞]Vx,T.\nFurthermore, given that vis non-decreasing, v(t)< x if and only if t < T v. Consequently, E[Pij]satisfies\nE[Pij]≥φ(0)E\u0014\ninf\nv∈VZ∞\n01t−a<v(t)<xa\ni\nφ(t−v(t))dt\u0015\n=φ(0)E\"\ninf\nT∈[xa\ni,∞]inf\nv∈Vxa\ni,TZ∞\n01t−a<v(t)<xa\ni\nφ(t−v(t))dt#\n=φ(0)E\"\ninf\nT∈[xa\ni,∞]inf\nv∈Vxa\ni,TZT\n01t−a<v(t)\nφ(t−v(t))dt#\n. (8)\nLetx >0andT∈[x,∞], and define v∗\nx,T:t≥07→(t−T+x) 1t>T−x, then for all v∈ Vx,Twe have v(t)≥v∗\nx,T(t)\nfor all t∈[0, T]. Indeed, v(t)≥0 =v∗\nx,T(t)fort∈[0, T−x], and if v(t)< v∗\nx,T(t)for some t∈[T−x, T]then,\nbecause vis1-Lipschitz, we have\nv(T)≤v(T−x) +x < v∗\nx,T(T−x) +x=x ,\n16\n\nNon-clairvoyant Scheduling with Partial Predictions\nwhich contradicts v(T)≥x. Finally, as φandv7→ 1t−a<vare both non-decreasing, it holds for any v∈ Vx,Tthat\nZT\n01t−a<v(t)\nφ(t−v(t))dt≥ZT\n01t−a<v∗\nx,T(t)\nφ(t−v∗\nx,T(t))dt\n=ZT−x\n01t−a<0\nφ(t)dt+ZT\nT−x1t−a<t−T+x\nφ(T−x)dt\n=Zmin(a,T−x)\n0dt\nφ(t)+x 1T−x<a\nφ(T−x)\n=(RT−x\n0dt\nφ(t)+x\nφ(T−x)ifT−x < aRa\n0dt\nφ(t)ifT−x≥a(9)\n=\u001aGφ(x, T)ifT−x < aRa\n0dt\nφ(t)ifT−x≥a. (10)\nTaking the infimum over v∈ Vx,Tthen over T∈[x,∞]in (10) gives for any x≥0that\ninf\nT∈[x,∞]inf\nv∈Vx,TZT\n01t−a<v(t)\nφ(t−v(t))dt≥min\u0012\ninf\nT∈[x,a+x)Gφ(x, T),Za\n0dt\nφ(t)\u0013\n≥min\u0012\ninf\nT∈[x,∞]Gφ(x, T),Za\n0dt\nφ(t)\u0013\n,\nand substituting into 8 leads to\nE[Pij]≥φ(0)E\u0014\nmin\u0012\ninf\nT∈[xa\n1,∞]Gφ(xa\n1, T),Za\n0dt\nφ(t)\u0013\u0015\n.\nLet us denote by faandf∞respectively the density functions of xa\n1andx∞\n1. We have for any x >0that\nfa(x) =d\ndxPr(xa\n1≤x) =φ′(x)/φ(x)2\nφ(0)−1−φ(a)−11x<a≥φ(0)φ′(x)\nφ(x)21x<a=f∞(x) 1x<a, (11)\ntherefore\nE[Pij] =φ(0)Z∞\n0min\u0012\ninf\nT∈[x,∞]Gφ(x, T),Za\n0dt\nφ(t)\u0013\nfa(x)dx\n≥φ(0)Z∞\n0min\u0012\ninf\nT∈[x,∞]Gφ(x, T),Za\n0dt\nφ(t)\u0013\nf∞(x) 1x<adx . (12)\nObserving for all x >0that\ninf\nT∈[x,∞]Gφ(x, T)≤lim\nT→∞Gφ(x, T) =Z∞\n0dt\nφ(t)(13)\nand that the mapping a7→min\u0010\ninfT∈[x,∞]Gφ(x, T),Ra\n0dt\nφ(t)\u0011\nf∞(x) 1x<ais non-decreasing, the monotone convergence\ntheorem, the continuity of the minimum, then Inequality 13 guarantee that\nlim\na→∞Z∞\n0min\u0012\ninf\nT∈[x,∞]Gφ(x, T),Za\n0dt\nφ(t)\u0013\nf∞(x) 1x<adx\n=Z∞\n0lim\na→∞\u001a\nmin\u0012\ninf\nT∈[x,∞]Gφ(x, T),Za\n0dt\nφ(t)\u0013\nf∞(x) 1x<a\u001b\ndx\n=Z∞\n0min\u0012\ninf\nT∈[x,∞]Gφ(x, T),Z∞\n0dt\nφ(t)\u0013\nf∞(x)dx\n=Z∞\n0\u0012\ninf\nT∈[x,∞]Gφ(x, T)\u0013\nf∞(x)dx\n=φ(0)Z∞\n0\u0012\ninf\nT∈[x,∞]Gφ(x, T)\u0013φ′(x)\nφ(x)2dx .\n17\n\nNon-clairvoyant Scheduling with Partial Predictions\nFinally, the previous term is finite because\nZ∞\n0\u0012\ninf\nT∈[x,∞]Gφ(x, T)\u0013φ′(x)\nφ(x)2dx≤Z∞\n0Gφ(x,2x)φ′(x)\nφ(x)2dx\n=Z∞\n0\u0012Zx\n0dt\nφ(t)+x\nφ(x)\u0013φ′(x)\nφ(x)2dx\n≤Z∞\n0\u0012\n2Zx\n0dt\nφ(t)\u0013φ′(x)\nφ(x)2dx\n= 2Z∞\n0\u0012Z∞\ntφ′(x)\nφ(t)2\u0013dt\nφ(t)\n= 2Z∞\n0dt\nφ(t)2<∞.\nTherefore\nZ∞\n0min\u0012\ninf\nT∈[x,∞]Gφ(x, T),Za\n0dt\nφ(t)\u0013φ(0)\nφ(t)1t<adt\n=φ(0)Z∞\n0\u0012\ninf\nT∈[x,∞]Gφ(x, T)\u0013φ′(x)\nφ(x)2dx−o(1),\nand substituting into Inequality (12) gives the aimed result.\nA.2.1. P ROOF OF THE THEOREM\nUsing the previous lemmas, we can now prove the theorem.\nProof. Letx= (x1, . . . , x n)be an array of i.i.d. random job sizes with distribution Pr(xi≤t) = 1−φ(0)\nφ(t). Observe that\nE[x1] =Z∞\n0Pr(x1≥t)dt=φ(0)Z∞\n0dt\nφ(t),E[min( x1, x2)] =Z∞\n0Pr(x1≥t)2dt=φ(0)2Z∞\n0dt\nφ(t)2.\nLemma A.5 with a=∞gives for any algorithm AthatE[Pij]≥E[min( x1, x2)]fori̸=j≤B,E[Pij]≥2E[min( x1, x2)]\nfori̸=j > B , andE[Pij]≥αφE[min( x1, x2)]fori≤B, j > B . Indeed, the probability density function of x1is\nt7→φ(0)φ′(t)\nφ(t)2, and we have for all i≤Bandj > B that\nφ(0)E\u0002\ninf\nT≥x1Gφ(x1, T)\u0003\n=φ(0)2Z∞\n0\u001a\ninf\nT≥xGφ(x, T)\u001bφ′(x)\nφ(x)2dx\n≥αφφ(0)2Z∞\n0dt\nφ(t)2=αφE[min( x1, x2)].\nAssume thatR∞\n0dt\nφ(t)<∞, i.e.E[x1]<∞, then by Equation (1) it holds that\nE[A(x)]≥nE[x1] +X\n1≤i<j≤BE[min( x1, x2)] +X\nB<i<j ≤n2E[min( x1, x2)] +BX\ni=1nX\nj=B+1αφE[min( x1, x2)]\n=nE[x1] +\u0012B(B−1)\n2+ (n−B)(n−B−1) +αφB(n−B)\u0013\nE[min( x1, x2)],\n18\n\nNon-clairvoyant Scheduling with Partial Predictions\nthe terms multiplying E[min( x1, x2)]can be reordered as follows\nB(B−1)\n2+ (n−B)(n−B−1) +αφB(n−B)\n=B(B−1)\n2+ (n−B)(n−1 + (αφ−1)B)\n=B(B−1)\n2+n(n−1) + ( αφ−1)nB−nB−B((αφ−1)B−1)\n=B(B−1)\n2+n(n−1) + ( αφ−2)nB−(αφ−1)B(B−1)−(αφ−2)B\n=n(n−1)−(2−αφ)(n−1)B+ (3\n2−αφ)B(B−1) (14)\n=n(n−1)\n2\u0012\n2−(4−2αφ)B\nn+ (3−2αφ)B(B−1)\nn(n−1)\u0013\n(15)\n=n(n−1)\n2\u0012\n2−B\nn−(3−2αφ)B\nn\u0012\n1−B(B−1)\nn(n−1)\u0013\u0013\n, (16)\nhence\nE[A(x)] =nE[x1] +Cn,Bn(n−1)\n2E[min( x1, x2)].\nOn the other hand, by equation (2)\nE[OPT(x)] =E\nnX\ni=1xi+X\ni<jmin(xi, xj)\n=nE[x1] +n(n−1)\n2E[min( x1, x2)],\ntherefore\nE[A(x)]\nE[OPT(x)]≥nE[x1] +Cn,Bn(n−1)\n2E[min( x1, x2)]\nnE[x1] +n(n−1)\n2E[min( x1, x2)]\n=Cn,B−Cn,B−1\n1 +n−1\n2E[min( x1,x2)]\nE[x1],\nand using Lemma A.1, this yields\nRn,B(A)≥Cn,B−Cn,B−1\n1 +n−1\n2E[min( x1,x2)]\nE[x1].\nIfBn=wn+o(n)for some w∈[0,1], then\nCRB(A)≥lim inf\nn→∞Rn,Bn(A)\n≥2−2(2−αφ)w+ (3−2αφ)w2.\nAssume now thatR∞\n0dt\nφ(t)=∞, thusE[x1] =∞. Leta >0, and xa= (xa\n1, . . . , xa\nn)be i.i.d. job sizes with distribution\nPr(xa\n1≤t) = 1−φ(t)−1−φ(a)−1\nφ(0)−1−φ(a)−11t<a.\nTherefore xa\n1has a finite expectation E[xa\n1]≤a. Denoting by x1, x2i.i.d. random variables with distribution Pr(x1≤t) =\n1−φ(0)\nφ(t), Lemma A.5 and Equation (1) give for any algorithm Athat\nE[A(xa)]≥nE[xa\n1] +X\n1≤i<j≤B(E[min( x1, x2)]−o(1)\na→∞) +X\nB<i<j ≤n(2E[min( x1, x2)]−o(1)\na→∞)\n+BX\ni=1nX\nj=B+1(αφE[min( x1, x2)]−o(1)\na→∞)\n=nE[xa\n1] +Cn,Bn(n−1)\n2E[min( x1, x2)]−n2o(1)\na→∞, (17)\n19\n\nNon-clairvoyant Scheduling with Partial Predictions\nwhere the last equation is obtained with the same computation as in the caseR∞\n0dt\nφ(t)<∞. Moreover, if a≥1then\nE[min( xa\n1, xa\n2)] =Z∞\n0Pr(min( xa\n1, xa\n2)≥t)dt=Z∞\n0\u0012φ(t)−1−φ(a)−1\nφ(0)−1−φ(a)−1\u00132\n1t<adt ,\nand we have for any t≥0that\u0010\nφ(t)−1−φ(a)−1\nφ(0)−1−φ(a)−1\u00112\n≤φ(t)−2\n(φ(0)−1−φ(1)−1)2, which is an integrable function. Furthermore, φis\nincreasing and 1/φ2is integrable, hence lim\na→∞φ(a) =∞. Therefore, the dominated convergence theorem gives that\nlim\na→∞E[min( xa\n1, xa\n2)] =Z∞\n0lim\na→∞(\u0012φ(t)−1−φ(a)−1\nφ(0)−1−φ(a)−1\u00132\n1t<a)\ndt\n=Z∞\n0φ(0)2\nφ(t)2dt\n=E[min( x1, x2)].\nIt follows that\nOPT(xa) =E\nnX\ni=1xa\ni+X\ni<jmin(xa\ni, xa\nj)\n\n=nE[xa\n1] +n(n−1)\n2E[min( xa\n1, xa\n2)]\n=nE[xa\n1] +n(n−1)\n2E[min( x1, x2)] +n2o(1)\na→∞.\nCombining this with Inequality (17), then using Lemma A.1, we obtain that\nRn,B≥nE[xa\n1] +Cn,Bn(n−1)\n2E[min( x1, x2)]−n2o(1)\na→∞\nnE[xa\n1] +n(n−1)\n2E[min( x1, x2)] +n2o(1)\na→∞,\nand if B=wn+o(n)then\nCRB(A)≥lim inf\nn→∞Rn,B(A)≥\u0000\n2−2(2−αφ)w+ (3−2αφ)w2\u0001\nE[min( x1, x2)]−o(1)\na→∞\nE[min( x1, x2)] +o(1)\na→∞\nand finally, taking the limit a→ ∞ yields\nCRB(A)≥2−2(2−αφ)w+ (3−2αφ)w2.\nA.3. Proof of Corollary 2.2\nProof. Let us consider i.i.d. exponentially distributed job sizes. This corresponds to φ(t) =etfor all t≥0. It holds that\nφ(0) = 1 >0,φis increasing,φ′\nφ= 1is non-increasing, and\nZ∞\n0dt\nφ(t)=Z∞\n0e−tdt= 1,Z∞\n0dt\nφ(t)2=Z∞\n0e−2tdt=1\n2.\nFurthermore, it holds for all x≥0andT≥xthat\nG(x, T) =ZT−x\n0dt\nφ(t)+x\nφ(T−x)\n=ZT−x\n0e−tdt+xe−(T−x)\n= 1 + ( x−1)e−(T−x).\n20\n\nNon-clairvoyant Scheduling with Partial Predictions\nThus, for all x≥0\ninf\nT≥xG(x, T) =(\nG(x, x) =x ifx <1\nlim\nT→∞G(x, T) = 1 ifx≥1,\nand it follows that\nφ(0)2Z∞\n0\u001a\ninf\nT≥xGφ(x, T)\u001bφ′(x)\nφ(x)2dx=Z∞\n0\u001a\ninf\nT≥xGφ(x, T)\u001b\ne−xdx\n=Z1\n0xe−xdx+Z∞\n1e−xdx\n= 1−1\ne\n= 2(1 −1\ne)Z∞\n0dt\nφ(t)2.\nFinally, by Theorem 2.1, it holds for any algorithm Ahaving access to the sizes of Bjobs that\nRn,B(A)≥Cn,B−Cn,B−1\n1 +n−1\n4=Cn,B−4(Cn,B−1)\nn+ 3,\nwith\nCn,B= 2−(4/e)B\nn+ (4/e−1)B(B−1)\nn(n−1).\nIn particular, if B=wn+o(n), then again by Theorem 2.1 we have\nCR(A)≥2−4\new+ (4\ne−1)w2.\nA.4. Proof of Corollary 2.3\nProof. Letr∈(1\n2,1), and consider φ:t7→(1 + t)r. It holds that φ(0) = 1 >0,φis increasing,φ′\nφ:t7→r\n1+tis\nnon-increasing, andR∞\n0dt\nφ(t)2=R∞\n0dt\n(1+t)2r=1\n2r−1<∞because r >1\n2. Moreover, it holds for all x≥0andT≥xthat\nG(x, T) =ZT−x\n0dt\nφ(t)+x\nφ(T−x)\n=ZT−x\n0dt\n(1 +t)r+x\n(1 +T−x)r\n=−1\n1−r+(1 +T−x)1−r\n1−r+x\n(1 +T−x)r,\ntherefore, for all x≥0\ninf\nT≥xG(x, T) =−1\n1−r+ inf\nT≥x\u001a(1 +T−x)1−r\n1−r+x\n(1 +T−x)r\u001b\n=−1\n1−r+ inf\ny∈[0,1]\u001ay−(1−r)\n1−r+xyr\u001b\n,\nwhere the last inequality is obtained by considering y=1\n1+T−x. It holds that\nd\ndy\u0012y−(1−r)\n1−r+xyr\u0013\n=−y−(2−r)+rxy−(1−r)\n=rxy−(2−r)\u0012\ny−1\nrx\u0013\n,\n21\n\nNon-clairvoyant Scheduling with Partial Predictions\nThe mapping y7→y−(1−r)\n1−r+xyris thus minimized on [0,∞)fory=1\nrx, and it is minimized on [0,1]fory∗= min(1 ,1\nrx).\nTherefore, it holds for x≤1\nrthaty∗= 1andinfT≥xG(x, T) =−1\n1−r+ (1\n1−r+x) =x, and for x >1\nrwe have y∗=1\nrx\nand\ninf\nT≥xG(x, T) =−1\n1−r+(rx)1−r\n1−r+ (rx)−r\n=−1\n1−r+\u00121\n1−r+1\nr\u0013\n(rx)1−r\n=−1\n1−r+(rx)1−r\nr(1−r).\nWe deduce that\nφ(0)2Z∞\n0\u001a\ninf\nT≥xGφ(x, T)\u001bφ′(x)\nφ(x)2dx=Z1/r\n0\u0012\n−1\n1−r+x\u0013r\n(1 +x)r+1dx\n+Z∞\n1/r\u0012\n−1\n1−r+(rx)1−r\nr(1−r)\u0013r\n(1 +x)r+1dx\n=−1\n1−r+Z1/r\n0rx\n(1 +x)1+rdx+r1−r\n1−rZ∞\n1/rx1−r\n(1 +x)r+1dx\n≥ −1\n1−r+r1−r\n1−rZ∞\n1/rx1−r\n(1 +x)r+1dx . (18)\nLet us denote Wr=R∞\n1/rx1−r\n(1+x)r+1dx. It holds that\nWr=Z∞\n1/rx1−r\n(1 +x)r+1dx\n=Z∞\n1/r\u0012x\n1 +x\u00131−rdx\n(1 +x)2r\n≥Z∞\n2rx\n1 +x·dx\n(1 +x)2r(r >1\n2)\n≥Z1/3\n0√1−s\ns2−2rds (s←1\n1+x)\n=Z1/3\n0√\n1−s·d\nds\u0012s2r−1\n2r−1\u0013\nds\n=\u0014s2r−1√1−s\n2r−1\u00151/3\n0+1\n2(2r−1)Z1/3\n0s2r−1\n√1−sds\n=3−(2r−1)p\n2/3\n2r−1+1\n2(2r−1)Z1/3\n0s2r−1\n√1−sds .\nIt holds for all r∈(1\n2,1)thats2r−1\n√1−s≤1√1−s, which is an integrable function on [0,1\n3], hence the dominated convergence\ntheorem guarantees that\nlim\nr→1\n21\n2Z1/3\n0s2r−1\n√1−sds=1\n2Z1/3\n0lim\nr→1\n2s2r−1\n√1−sds\n=1\n2Z1/3\n0ds√1−s\n=\u0002\n−√\n1−s\u00031/3\n0\n= 1−p\n2/3,\n22\n\nNon-clairvoyant Scheduling with Partial Predictions\nwe deduce that\n(2r−1)Wr= 3−(2r−1)p\n2/3 +1\n2Z1/3\n0s2r−1\n√1−sds\n= \np\n2/3−o(1)\nr→1/2!\n+ \n1−p\n2/3−o(1)\nr→1/2!\n= 1 + o(1)\nr→1/2.\nSubstituting into Inequality 18, and recalling thatR∞\n0dt\nφ(t)2=1\n2r−1, we deduce that for rclose to1\n2\nφ(0)2Z∞\n0\u001a\ninf\nT≥xGφ(x, T)\u001bφ′(x)\nφ(x)2dx≥ −1\n1−r+r1−r\n1−rWr\n=\u0012\n−2r−1\n1−r+\u0012r1−r\n1−r\u0013\n(2r−1)Wr\u0013Z∞\n0dt\nφ(t)2\n=\u0010\no(1) + (√\n2 +o(1))(1 −o(1))\u0011Z∞\n0dt\nφ(t)2\n=\u0010√\n2 +o(1)\u0011Z∞\n0dt\nφ(t)2.\nConsequently, by Theorem 2.1, if B=wn+o(n), it holds for any algorithm Athat\nCR(A)≥2−2 \n2−√\n2−o(1)\nr→1/2!\nw+ \n3−2√\n2−o(1)\nr→1/2!\nw2,\nand taking the limit when r→1\n2gives\nCR(A)≥2−2(2−√\n2)w+ (3−2√\n2)w2.\nB. Known partial order\nB.1. Preliminary results\nLemma B.1. For any real numbers x1, . . . , x n, ifσis a uniformly random permutation of [n]then\nE[min( xσ(1), xσ(2))] =2\nn(n−1)X\n1≤i<j≤nmin(xi, xj).\nProof. Since σis uniformly random, we have for any i̸=jthatE[min( xσ(1), xσ(2))] =E[min( xσ(i), xσ())], thus\nE[min( xσ(1), xσ(2))] =2\nn(n−1)EhX\n1≤i<j≤nmin(xσ(i), xσ(j))i\n=2\nn(n−1)X\n1≤i<j≤nmin(xi, xj).\nLemma B.2. For any real numbers x1, . . . , x n, ifσis a uniformly random permutation of [n]then\nE[xσ(1) 1xσ(1)<xσ(2)]≤E[min( xσ(1), xσ(2))] =1\nn(n−1)X\n1≤i<j≤nmin(xi, xj),\nwith equality if x1, . . . , x nare pairwise distinct.\n23\n\nNon-clairvoyant Scheduling with Partial Predictions\nProof. σis a uniformly random permutation of [n], therefore E[xσ(i) 1xσ(i)<xσ(j)] =E[xσ(1) 1xσ(1)<xσ(2)]for all i̸=j∈[n].\nIt follows that\nE[xσ(1) 1xσ(1)<xσ(2)] =1\n2\u0000\nE[xσ(1) 1xσ(1)<xσ(2)] +E[xσ(2) 1xσ(2)<xσ(1)]\u0001\n≤1\n2E[xσ(i) 1xσ(1)<xσ(2)+xσ(2) 1xσ(2)≤xσ(1)]\n=1\n2E[min( xσ(1), xσ(2))],\nand using Lemma B.1 concludes the proof. To have equality, it must hold that 1xσ(2)<xσ(1)= 1xσ(2)≤xσ(1)for any\npermutation, and this is true if and only if the values x1, . . . , x nare pairwise distinct.\nB.2. Proof of Theorem 3.1\nProof. For any i̸=j≥B+ 1, as in round-robin, Pσ(i)σ(j)= 2 min( xσ(i), xσ(j)). For i̸=j≤B, since the jobs\n{xσ(k)}B\nk=1are run in non-decreasing order one after the other, it holds that Pσ(i)σ(j)= min( xσ(i), xσ(j)). Finally, for\ni≤Bandj≥B+ 1, the delay caused by σ(j)toπ(i)is always Dσ(j)π(i)= min( xπ(i), xσ(j)). On the other hand if\nxσ(j)≤xπ(i−1)thenxσ(j)terminates before phase ibegins, thus job π(i)does not delay job σ(j):Dπ(i)σ(j)= 0. If\nxσ(j)> xπ(i−1), then after the first step of phase i, the time spent on each of the jobs π(i)andσ(j)isxπ(i−1). Both jobs\nare then executed with identical processing powers until one of them is terminated, thus\nDπ(i)σ(j)= min( xπ(i), xσ(j)) 1xσ(j)>xπ(i−1)\n=xπ(i) 1xσ(j)>xπ(i)+xσ(j) 1xπ(i)≥xσ(j)>xπ(i−1).\nWith the convention xπ(0)=0 , taking the sum over igives\nBX\ni=1Dπ(i)σ(j)=BX\ni=1xπ(i) 1xσ(j)>xπ(i)+xσ(j)BX\ni=11xπ(i)≥xσ(j)>xπ(i−1)\n=BX\ni=1xσ(i) 1xσ(j)>xσ(i)+xσ(j) 1xσ(j)≤xπ(B)\n≤BX\ni=1xσ(i) 1xσ(j)>xσ(i)+xσ(j). (19)\nUsing Lemma B.2, and recalling that {π(i)}B\ni=1={σ(i)}B\ni=1, we have in expectation,\nEhBX\ni=1Dσ(i)σ(j)i\n=EhBX\ni=1Dπ(i)σ(j)i\n≤B\n2E[min( xσ(1), xσ(2))] +1\nnnX\nk=1xk, (20)\nand it follows that, for any j≥B+ 1\nEhBX\ni=1Pσ(i)σ(j)i\n=EhBX\ni=1Dσ(j)σ(i)i\n+EhBX\ni=1Dσ(i)σ(j)i\n≤BE[min( xσ(1), xσ(2))] +B\n2E[min( xσ(1), xσ(2))] +1\nnnX\nk=1xk\n=3B\n2E[min( xσ(1), xσ(2))] +1\nnnX\nk=1xk,\n24\n\nNon-clairvoyant Scheduling with Partial Predictions\nhence\nnX\nj=B+1EhBX\ni=1Pσ(i)σ(j)i\n≤3\n2B(n−B)E[min( xσ(1), xσ(2))] +\u0012\n1−B\nn\u0013nX\nk=1xk.\nFinally, using Equation (1), the previous upper bounds on Pσ(i)σ(j)for all i̸=j, then Equation (16) with3/2instead of απ\ngives that the objective of CRRR on the instance x={x1, . . . , x n}satisfies in expectation\nE[CRRR (x)] =nX\ni=1xi+EhX\n1≤i<j≤nPσ(i)σ(j)i\n=nX\ni=1xi+X\n1≤i<j≤BE[min( xσ(i), xσ(j))] +X\nB<i<j ≤n2E[min( xσ(i), xσ(j))]\n+nX\nj=B+1EhBX\ni=1Pσ(i)σ(j)i\n(21)\n≤\u0012\n2−B\nn\u0013nX\ni=1xi+\u0012B(B−1)\n2+ (n−B)(n−B−1) +3\n2B(n−B)\u0013\nE[min( xσ(1), xσ(2))]\n=\u0012\n2−B\nn\u0013nX\ni=1xi+\u0012\n2−B\nn\u0013n(n−1)\n2E[min( xσ(1), xσ(2))]\n=\u0012\n2−B\nn\u0013nX\ni=1xi+\u0012\n2−B\nn\u0013X\n1≤i<j≤nmin(xi, xj) (22)\n=\u0012\n2−B\nn\u0013\nOPT(x), (23)\nwhere we used B.1 for (22) then Equation (2) for (23). Therefore, the (n, B)-competitive ratio of Algorithm 1 satisfies\nRn,B(CRRR )≤2−B\nn. (24)\nFor proving the lower bound on Rn,B, letε >0and let us consider job sizes xε\ni= 1 + iεfor all i∈[n]. Observe that the\nonly inequalities we used in the analysis are Inequalities (19) (xσ(j) 1xσ(j)≤xπ(B)≤xσ(j)), and (20) given by Lemma B.2.\nThe second one becomes equality if the job sizes are pairwise distinct, which is satisfied by {xε\ni}n\ni=1. As for Inequality (19),\nsince the job sizes xεare pairwise distinct and all larger than 1, we can give instead a lower bound on E[xσ(j) 1xσ(j)≤xπ(B)]\n25\n\nNon-clairvoyant Scheduling with Partial Predictions\nforj > B as follows\nE[xσ(j) 1xσ(j)≤xπ(B)] =E[xσ(j) 1σ(j)≤π(B)]\n≥Pr(σ(j)< π(B)) = Pr( σ(n)<max\ni≤Bσ(i))\n=nX\nk=B+1k−1X\nm=1Pr(σ(n) =m,max\ni≤Bσ(i) =k)\n=nX\nk=B+1k−1X\nm=1BX\nℓ=1Pr(σ(n) =m, σ(ℓ) =k,∀i∈[B]\\ {ℓ}:σ(i)∈[k−1]\\ {m})\n=nX\nk=B+1k−1X\nm=1BX\nℓ=11\nn!\u0012k−2\nB−1\u0013\n(B−1)!(n−B−1)!\n=nX\nk=B+1k−1X\nm=1B!(n−B−1)!\nn!\u0012k−2\nB−1\u0013\n=B!(n−B−1)!\nn!nX\nk=B(k−1)\u0012k−2\nB−1\u0013\n=B·B!(n−B−1)!\nn!nX\nk=B\u0012k\nB\u0013\n=B·B!(n−B−1)!\nn!\u0012n\nB+ 1\u0013\n=B\nB+ 1.\nTherefore, instead of (19), we have the lower bound\nEhBX\ni=1Dσ(i)σ(j)i\n=EhBX\ni=1Dπ(i)σ(j)i\n=B\n2E[min( xσ(1), xσ(2))] +E[xσ(j) 1xσ(j)≤xπ(B)] (25)\n≥B\n2+B\nB+ 1, (26)\nand thus, since we proved earlier that Dσ(i)σ(j)= min( xε\nσ(i),xε\nσ(j)for all i≤Bandj > B , it holds that\nEhBX\ni=1Pσ(i)σ(j)i\n≥BX\ni=1E[min( xσ(i),xσ(j)] +B\n2+B\nB+ 1≥3B\n2+B\nB+ 1\nand using that xε\ni≥1for all i∈[n], we obtain by substituting the previous inequalities into (21) that\nE[CRRR (xε)]≥n+B(B−1)\n2+ (n−B)(n−B−1) + ( n−B)\u0012B\n2+B\nB+ 1\u0013\n=\u0012\nn+ (n−B)B\nB+ 1\u0013\n+\u0012B(B−1)\n2+ (n−B)(n−B−1) +3B\n2(n−B)\u0013\n=\u0012\n2n−B−n−B\nB+ 1\u0013\n+\u0012\nn(n−1)−(n−1)B\n2\u0013\n= (n+ 1)\u0012\nn−B\n2\u0013\n−n−B\nB+ 1.\n26\n\nNon-clairvoyant Scheduling with Partial Predictions\nOn the other hand, Equation (2), along with xε\ni≤1 +nεfor all i∈[n], gives\nOPT(xε)≤nX\ni=1(n−i+ 1)(1 + nε) =n(n+ 1)\n2(1 +nε),\nand it follows that\nRn,B(CRRR )≥E[CRRR (xε)]\nOPT(xε)\n≥(n+ 1)\u0000\nn−B\n2\u0001\n−n−B\nB+1\nn(n+1)\n2(1 +nε)\n=1\n1 +nε \n2−B\nn−2(1−B\nn)\n(n+ 1)( B+ 1)!\n.\nTaking the limit ε→0and using Inequality (24), we obtain that\n2−B\nn−2(1−B\nn)\n(n+ 1)( B+ 1)≤Rn,B(CRRR )≤2−B\nn.\nFinally, if Bn=⌊wn⌋for some w∈[0,1]thenBn≥wn, hence CRB(CRRR ) = supn≥2Rn,Bn(CRRR )≤2−w, and\nthis bound is reached by the lower bound on Rn,Bnforn→ ∞ , which gives that CRB(CRRR ) = 2−w.\nC. Predictions of the job sizes\nC.1. Proof of Theorem 4.1\nWe first compute the mutual delays of the jobs in Switch with any breakpoints.\nLemma C.1. For any job sizes x= (x1, . . . , x n), and for any permutation σof[n]and breakpoints zσ= (zσ(1), . . . , z σ(B)),\nthe Switch algorithm Switch (zσ, x)satisfies for all i̸=j∈[n]that\nPσ(i)σ(j)=xσ(i) 1zσ(i)<zσ(j)+xσ(j) 1zσ(i)>zσ(j)+ (θijxσ(i)+ (1−θij)xσ(j)) 1zσ(i)=zσ(j) ifi, j≤B ,\nPσ(i)σ(j)= 2 min( xσ(i), xσ(j)) ifi, j > B ,\nPσ(i)σ(j)=xσ(i) 1xσ(j)>zσ(i)+ min( zσ(i), xσ(j)) ifi≤B, j > B .\nwhere θijis a Bernoulli random variable with parameter 1/2independent of σfor all i̸=j∈[B].\nProof. Let us first define the random variables θij. For all i̸=j∈[B], ifzσ(i)̸=zσ(j)then let θijbe an independent\nBernoulli random variable with parameter 1/2, and if zσ(i)=zσ(j)then let θijbe the indicator that zσ(i)comes before zσ(j)\nwith the ordering π. Since πis an ordering of the breakpoints chosen uniformly at random, then θijis a Bernoulli random\nvariable with parameter 1/2independently of σ.\nFori̸=j∈[B], ifzσ(i)< zσ(j)then job σ(i)is executed until completion before job σ(j)starts being executed, thus\nDσ(i)σ(j)=xσ(i)andDσ(j)σ(i)= 0 andPσ(i)σ(j)=xσ(i). By symmetry, if zσ(i)> zσ(j)thenPσ(i)σ(j)=xσ(j).\nIn the case where zσ(i)=zσ(j), since πis chosen uniformly at random among all the permutations of [B]satisfying\nzπ(1)≤. . .≤zπ(B), each of the jobs σ(i), σ(j)is run until completion before the other one starts with equal probability\n1/2, hence Pσ(i)σ(j)=θijxσ(i)+ (1−θij)xσ(j). It follows that\nPσ(i)σ(j)=xσ(i) 1zσ(i)<zσ(j)+xσ(j) 1zσ(i)>zσ(j)+ (θijxσ(i)+ (1−θij)xσ(j)) 1zσ(i)=zσ(j).\nFori̸=j > B , jobs σ(i), σ(j)are processed symmetrically and the delays they cause to each other are the same as in\nround-robin. Therefore Dσ(i)σ(j)=Dσ(j)σ(i)= min( xσ(i), xσ(j)), and it holds that\nPσ(i)σ(j)= 2 min( xσ(i), xσ(j)).\n27\n\nNon-clairvoyant Scheduling with Partial Predictions\nFori≤Bandj > B , at the time when job σ(i)starts being executed, it holds by definition of Algorithm 2 that either job\nσ(j)is completed or Sσ(j)=zσ(i), hence the delay caused by job σ(j)to job σ(i)isDσ(j)σ(i)= min( zσ(i), xσ(j)). On\nthe other hand, job σ(i)delays job σ(j)if and only if xσ(j)> zσ(i), and in this case job σ(i)runs until completion before\njobσ(j)is completed, hence Dσ(i)σ(j)=xσ(i) 1xσ(j)>zσ(i), and it follows that\nPσ(i)σ(j)=xσ(i) 1xσ(j)>zσ(i)+ min( zσ(i), xσ(j))\nC.1.1. P ROOF OF THE THEOREM\nUsing the previous lemma, we now prove Theorem 4.1\nProof. Using Lemma C.1, it holds for all i̸=j∈[n]thatPσ(i)σ(j)= min( xσ(i), xσ(j))ifi, j≤B,Pσ(i)σ(j)=\n2 min( xσ(i), xσ(j))ifi, j > B , and if i≤Bandj > B then\nPσ(i)σ(j)= min( xσ(i), xσ(j)) +xσ(i) 1xσ(i)<xσ(j).\nTaking the expectation over σthen using Lemma B.2 gives\nE[xσ(i) 1xσ(i)<xσ(j)] =E[xσ(1) 1xσ(1)<xσ(2)]\n≤1\n2E[min( xσ(1), xσ(2))], (27)\nhence for any i≤Bandj > B\nE[Pσ(i)σ(j)]≤3\n2E[min( xσ(1), xσ(2))].\nIt follows from Equation (1)that for any instance of njob sizes x= (x1, . . . , x n), the switch algorithm 2 with breakpoints\n(zσ(i))B\ni=1= (xσ(i))B\ni=1satisfies\nE[Switch (xσ, x)]≤nX\ni=1xi+X\n1≤i<j≤BE[min( xσ(1), xσ(2))] +X\nB<i<j ≤n2E[min( xσ(i), xσ(j))]\n+BX\ni=1nX\nj=B+13\n2E[min( xσ(i), xσ(j))]\n=nX\ni=1xi+\u0012B(B−1)\n2+ (n−B)(n−B−1) +3\n2B(n−B)\u0013\nE[min( xσ(1), xσ(2))]\n=nX\ni=1xi+\u0012\n2−B\nn\u0013n(n−1)\n2E[min( xσ(1), xσ(2))]\n=nX\ni=1xi+\u0012\n2−B\nn\u0013X\n1≤i<j≤nmin(xi, xj), (28)\nwhere we used Lemma B.1 for the last equation. We can assume without loss of generality that x1≤. . .≤xn, which yieldsP\n1≤i<j≤nmin(xi, xj) =Pn\ni=1(n−i)xi. Finally, Equation (2) gives\nE[Switch (xσ, x)]\nOPT(x)≤Pn\ni=1xi+\u0000\n2−B\nn\u0001Pn\ni=1(n−i)xiPn\ni=1xi+Pn\ni=1(n−i)xi\n= 2−B\nn−(1−B\nn)Pn\ni=1xiPn\ni=1(n−i+ 1)xi\n= 2−B\nn−(1−B\nn)Pn\ni=1(n−i+ 1)qi, (29)\n28\n\nNon-clairvoyant Scheduling with Partial Predictions\nwhere qi= (Pn\nj=1xj)−1xifor all i∈[n]. The variables (qi)n\ni=1satisfyPn\ni=1qi= 1, and we can assume without\nloss of generality that q1≤. . .≤qn(i.e.x1≤. . .≤xn). Expression (29) is maximized under these constraints for\nq1=. . .=qn=1\nn. It follows for all job sizes x1, . . . , x nthat\nE[Switch (xσ, x)]\nOPT(x)≤2−B\nn−2(1−B\nn)\nn+ 1,\nhence Rn,B(Switch )≤2−B\nn−2(1−B\nn)\nn+1. Let us now prove that this is exactly the competitive ratio of ALG . Observe that\nthe only inequality we used while analyzing Switch (xσ, x)is Inequality (27), which becomes an equality if the job sizes\nare pairwise distinct. Let us, therefore, consider job sizes x(ε)\ni= 1 + iε∈[1,1 +nε]for some ε >0. All the inequalities\nbecoming inequalities, it holds in particular that\nRn,B(Switch )≥E[Switch (xσ, x(ε))]\nOPT(x(ε))= 2−B\nn−(1−B\nn)Pn\ni=1x(ε)\niPn\ni=1(n−i+ 1)x(ε)\ni\n≥2−B\nn−(1−B\nn)Pn\ni=1(1 +nε)Pn\ni=1(n−i+ 1)\n= 2−B\nn−(1 +nε)2(1−B\nn)\nn+ 1,\nand taking ε→0yields that\nRn,B(Switch ) = 2−B\nn−2(1−B\nn)\nn+ 1.\nLetw∈[0,1]. IfB=⌊wn⌋, then B/n≥wand it holds for all n≥2thatRn,B(Switch )≤2−w, and this bound is\nreached for n→ ∞ , thus CRB(Switch ) = 2−w.\nC.2. Proof of Proposition 4.2\nProof. It is shown in (Motwani et al., 1994) that for any instance x= (x1, . . . , x n), the expected sum of completion times\nresulting from a run of RTC isE[RTC(x)] =n+1\n2Pn\ni=1xi. Using Equation 28, we deduce that the algorithm ALG that\nrunsRTC with probability2(n−B)\nn(n+3)−2Band runs Switch with breakpoints zσ=xσwith the remaining probability satisfies\nE[ALG(x)] =2(n−B)\nn(n+ 3)−2BRTC(x) +\u0012\n1−2(n−B)\nn(n+ 3)−2B\u0013\nE[Switch (xσ, x)]\n=2(n−B)\nn(n+ 3)−2B·n+ 1\n2nX\ni=1xi+\u0012\n1−2(n−B)\nn(n+ 3)−2B\u0013\nnX\ni=1xi+\u0012\n2−B\nn\u0013X\n1≤i<j≤nmin(xi, xj)\n\n=\u0012(n−1)(n−B)\nn(n+ 3)−2B+ 1\u0013nX\ni=1xi+\u0012\n1−2(n−B)\nn(n+ 3)−2B\u0013\u0012\n2−B\nn\u0013X\n1≤i<j≤nmin(xi, xj)\n= \n2−B\nn−2(1−B\nn)(2−B\nn)\nn+ 3−2B\nn!\nnX\ni=1xi+X\n1≤i<j≤nmin(xi, xj)\n\n= \n2−B\nn−2(1−B\nn)(2−B\nn)\nn+ 3−2B\nn!\nOPT(x).\n29\n\nNon-clairvoyant Scheduling with Partial Predictions\nC.3. Proof of Lemma 4.3\nProof. Letx1, . . . , x nLetξbe a random variable with distribution F. Leti̸=j≤B, by symmetry, we can assume that\nxσ(i)≤xσ(j). Lemma C.1 gives that\nPσ(i)σ(j)=xσ(i) 1yσ(i)<yσ(j)+xσ(j) 1yσ(i)>yσ(j)+xσ(i)+xσ(j)\n21yσ(i)=yσ(j)\n≤xσ(i) 1yσ(i)<yσ(j)+xσ(j) 1yσ(i)≥yσ(j)\n=xσ(i)+ (xσ(j)−xσ(i)) 1yσ(i)≥yσ(j)\n≤xσ(i)+ (xσ(j)−yσ(j)+yσ(i)−xσ(i)) 1yσ(i)≥yσ(j)\n≤xσ(i)+ (ησ(i)+ησ(j))\n= min( xσ(i), xσ(j)) + (ησ(i)+ησ(j)),\nand we obtain in expectation that for all i̸=j≤B\nE[Pσ(i)σ(j)]≤E[min( xσ(i), xσ(j))] +2\nBE[ησ]. (30)\nFori̸=j > B , Lemma C.1 gives\nE[Pσ(i)σ(j)] = 2E[min( xσ(i), xσ(j))]. (31)\nFori≤Bandj > B , we have again by Lemma C.1 that\nE[Pσ(i)σ(j)] =E[min( ξyσ(i), xσ(j))] +E[xσ(i) 1ξyσ(i)<xσ(j)].\nConditional on the permutation σ, i.e. taking the expectation only over ξ, the first term can be bounded as follows\nE[min( ξyσ(i), xσ(j))|σ]≤E[min( ξxσ(i), xσ(j)) +ξησ(i)|σ]\n=E[xσ(j) 1ξxσ(i)≥xσ(j)+ξxσ(i) 1ξxσ(i)<xσ(j)|σ] +E[ξ]ησ(i)\n=xσ(j)Pr(ξ≥xσ(j)\nxσ(i)|σ) +xσ(i)Eh\nξ 1ξ<xσ(j)\nxσ(i)|σi\n+E[ξ]ησ(i), (32)\nand for the second term, the expectation conditional on σsatisfies\nE[xσ(i) 1ξyσ(i)<xσ(j)|σ]≤E[yσ(i) 1ξyσ(i)<xσ(j)+ησ(i)|σ]\n=yσ(i)Pr(ξ <xσ(j)\nyσ(i)|σ) +ησ(i)\n=hF(xσ(j), yσ(i)) +ησ(i)\n≤hF(xσ(j), xσ(i)) +LF|xσ(i)−yσ(i)|+ησ(i) (33)\n=xσ(i)Pr(ξ <xσ(j)\nxσ(i)|σ) + (1 + LF)ησ(i), (34)\nwhere we used for (33) that hFisL-Lipschitz with respect to the second variable. Combining (32) and (34) yields\nE[Pσ(i)σ(j)|σ]≤xσ(j)+ (xσ(i)−xσ(j)) Pr(ξ <xσ(j)\nxσ(i)|σ) +xσ(i)Eh\nξ 1ξ<xσ(j)\nxσ(i)|σi\n+ (1 + LF+E[ξ])ησ(i)\n=xσ(j)+xσ(i) \n\u0000\n1−xσ(j)\nxσ(i)\u0001\nPr(ξ <xσ(j)\nxσ(i)|σ) +Eh\nξ 1ξ<xσ(j)\nxσ(i)|σi!\n+ (1 + LF+E[ξ])ησ(i)\n=xσ(j)+xσ(i)gF\u0000xσ(j)\nxσ(i)\u0001\n+ (1 + LF+E[ξ])ησ(i). (35)\n30\n\nNon-clairvoyant Scheduling with Partial Predictions\nWe can assume without of generality that x1≤. . .≤xn. It holds by definition of the constants βFandγFthat\ngF\u0000xσ(j)\nxσ(i)\u0001\n≤βFxσ(j)\nxσ(i)ifσ(j)< σ(i), and gF\u0000xσ(j)\nxσ(i)\u0001\n≤γF−xσ(j)\nxσ(i)ifσ(j)> σ(i), which gives\nEh\nxσ(j)+xσ(i)gF\u0000xσ(j)\nxσ(i)\u0001i\n=1\n2Eh\nxσ(j)+xσ(i)gF\u0000xσ(j)\nxσ(i)\u0001\n|σ(j)< σ(i)i\n+1\n2Eh\nxσ(j)+xσ(i)gF\u0000xσ(j)\nxσ(i)\u0001\n|σ(j)> σ(i)i\n≤1 +βF\n2E[xσ(j)|σ(j)< σ(i)] +γF\n2E[xσ(i)|σ(j)> σ(i)]\n=1 +βF\n2E[min( xσ(i), xσ(j))|σ(j)< σ(i)] +γF\n2E[min( xσ(i), xσ(j))|σ(j)> σ(i)]\n=1 +βF+γF\n2E[min( xσ(i), xσ(j))],\nthus we obtain by taking the expectation over σin (35) that\nE[Pσ(i)σ(j)]≤\u0010\n1+βF+γF\n2\u0011\nE[min( xσ(i), xσ(j))] +\u0010\n1+LF+E[ξ]\nB\u0011\nE[ησ].\nUsing Equation 1, the inequality above, Inequalities (30),(31), then Equation 16 with1+βF+γF\n2instead of απ, we deduce\nthat Algorithm 3 satisfies for any job sizes x1, . . . , x nthat\nE[Switch (ξyσ, x)] =nX\ni=1xi+X\n1≤i<j≤nE[Pσ(i)σ(j)]\n≤nX\ni=1xi+X\n1≤i<j≤B\u0012\nE[min( xσ(i), xσ(j))] +2\nBE[ησ]\u0013\n+X\nB<i<j ≤n2E[min( xσ(i), xσ(j))]\n+BX\ni=1nX\nj=B+1\u0010\u0010\n1+βF+γF\n2\u0011\nE[min( xσ(i), xσ(j))] +\u0010\n1+LF+E[ξ]\nB\u0011\nE[ησ]\u0011\n=nX\ni=1xi+\u0012B(B−1)\n2+ (n−B)(n−B−1) +\u0010\n1+βF+γF\n2\u0011\nB(n−B)\u0013\nE[min( xσ(1), xσ(2))]\n+\u0000\nB−1 + (1 + LF+E[ξ])(n−B)\u0001\nE[ησ]\n=nX\ni=1xi+\u0010\nn(n−1) +\u0010\n1−βF+γF\n2\u0011\nB(B−1)−\u0010\n3\n2−βF+γF\n2\u0011\nB(n−1)\u0011\nE[min( xσ(1), xσ(2))]\n+ (B−1 + (1 + LF+E[ξ])(n−B))E[ησ]\n=nX\ni=1xi+\u0012\n2−\u0010\n3−βF−γF\u0011B\nn+\u0010\n2−βF−γF\u0011B(B−1)\nn(n−1)\u0013X\ni<jmin(xi, xj)\n+\u0000\nB−1 + (1 + LF+E[ξ])(n−B)\u0001\nE[ησ],\nwhere we used Lemma B.1 for the last equality.\nC.4. Proof of Lemma 4.4\nProof. Letξ∼1 +E(1/ρ). The mapping hFdefined in Lemma 4.3 becomes for all s, t > 0\nhF(s, t) =tF(s\nt) =t\u0010\n1−e−1\nρ(s\nt−1)\u0011\n1t<s.\nFor all s >0, it holds for all t < s that\n∂hF(s, t)\n∂t= 1−e1\nρ(s\nt−1)−t×s\nρt2e−1\nρ(s\nt−1)= 1−\u0012s\nρt+ 1\u0013\ne−1\nρ(s\nt−1),\n31\n\nNon-clairvoyant Scheduling with Partial Predictions\nbut the mapping u7→(u\nρ+ 1)e−u−1\nρis decreasing on [1,∞]. Indeed\n∂\n∂u\u0010\n(u\nρ+ 1)e−u−1\nρ\u0011\n=1\nρe−u−1\nρ−1\nρ(u\nρ+ 1)e−u−1\nρ=−u\nρ2e−u−1\nρ<0,\nand sinces\nt>1we deduce that\n−1\nρ= 1−lim\nu→1(u\nρ+ 1)e−u−1\nρ≤∂hF(s, t)\n∂t≤1−lim\nu→∞(u\nρ+ 1)e−u−1\nρ= 1,\nthus|∂hF(s,t)\n∂t| ≤max(1 ,1\nρ) =1\nρfor all t < s . Otherwise, if t≥sthenhF(s, t) = 0 andhF(s,·)is continuous in t=s,\ntherefore t7→hF(s, t)is1\nρ-Lipschitz.\nOn the other hand, given that ξ >1a.s., it holds for s≤1thatE[ξ 1ξ<s] = 0 , and for s >1that\nE[ξ 1ξ<s] = Pr( ξ < s ) +E[(ξ−1) 1ξ−1<s−1]\n=F(s) +Z∞\n0t 1t<s−1e−t/ρ\nρdt\n=F(s) +Zs−1\n0te−t/ρ\nρdt\n=F(s) +h\n−(t+ρ)e−t/ρis−1\n0\n=\u0010\n1−e−(s−1)/ρ\u0011\n+\u0010\n−(s−1 +ρ)e−t/ρ+ρ\u0011\n= 1 + ρ−(s+ρ)e−(s−1)/ρ.\nIf follows for all s >0that\ngF(s) = (1 −s)\u0010\n1−e−(s−1)/ρ\u0011\n1s>1+\u0010\n1 +ρ−(s+ρ)e−(s−1)/ρ\u0011\n1s>1\n=\u0010\n(2 +ρ)−s−(1 +ρ)e−(s−1)/ρ\u0011\n1s>1,\nhence βF= sup0<s≤1gF(s)\ns= 0and\nγF= sup\ns≥1(gF(s) +s) = sup\ns≥1\u0010\n(2 +ρ)−(1 +ρ)e−(s−1)/ρ\u0011\n= 2 + ρ .\nFinally, with LF=1\nρ,βF= 0andγF= 2 + ρ, and observing that E[ξ] = 1 + ρ, the constants C1\nn,B,F andC1\nn,B,F defined\nin Lemma 4.3 satisfy\nC1\nn,B,F = 2−(1−ρ)B\nn−ρB(B−1)\nn(n−1)= 2−B\nn+ρB\nn\u0012\n1−B−1\nn−1\u0013\n,\nC2\nn,B,F≤(2 +ρ+1\nρ)(n−B) +B≤4\nρ(n−B) +B ,\nand the objective function of Algorithm 3 with any job sizes x={x1, . . . , x n}can be upper-bounded as follows\nE[Switch (ξyσ, x)]≤nX\ni=1xi+C1\nn,B,FX\ni<jmin(xi, xj) +\u00124\nρ(n−B) +B\u0013\nE[ησ],\nand by Equation (2) we obtain\nE[Switch (ξyσ, x)]\nOPT(x)≤Pn\ni=1xi+C1\nn,B,FP\ni<jmin(xi, xj)Pn\ni=1xi+P\ni<jmin(xi, xj)+\u00124\nρ(n−B) +B\u0013E[ησ]\nOPT(x)\n=C1\nn,B,F−(C1\nn,B,F−1)Pn\ni=1xiPn\ni=1xi+P\ni<jmin(xi, xj)+\u00124\nρ(n−B) +B\u0013E[ησ]\nOPT(x).\n32\n\nNon-clairvoyant Scheduling with Partial Predictions\nAs we showed in the proof of Theorem 4.1, we havePn\ni=1xiPn\ni=1xi+P\ni<jmin(xi,xj)≥2\nn+1, and the minimum is reached when all\nthe job sizes are equal.\nE[Switch (ξyσ, x)]\nOPT(x)≤C1\nn,B,F−2(C1\nn,B,F−1)\nn+ 1+\u0010\n4\nρ(1−B\nn) +B\nn\u0011nE[ησ]\nOPT(x),\nwhich yields after taking the supremum over instances xofnjobs\nRn,B(η;Switch )≤C1\nn,B,F−2(C1\nn,B,F−1)\nn+ 1+\u0010\n4\nρ(1−B\nn) +B\nn\u0011nE[ησ]\nOPT(x)(36)\n≤C1\nn,B,F +\u0010\n4\nρ(1−B\nn) +B\nn\u0011nE[ησ]\nOPT(x)\n=\u0010\n2−B\nn+ρB\nn(1−B−1\nn−1)\u0011\n+\u0010\n4\nρ(1−B\nn) +B\nn\u0011nE[ησ]\nOPT(x).\nC.5. Proof of Theorem 4.5\nProof. Letρ∈(0,1], and x= (x1, . . . , x n)be an instance of job sizes all at least equal to 1. Before starting to run, the\nalgorithm has access to predictions yσ(1), . . . , y σ(B)ofxσ(1), . . . , x σ(B), it samples ξ∼1+E(1/ρ)and sets the breakpoints\nzσ(i)=ξyσ(i)for all i∈[B]. With σand the breakpoints fixed, the preferential algorithm runs Algorithm 2 at rate λand\nround-robin at rate 1−λ. Lemma C.1 and Equation 1 guarantee that, with a fixed permutation σand fixed breakpoints,\nAlgorithm 2 is monotonic, as all the mutual delays Pσ(i)σ(j)are non-decreasing functions of xσ(i)andxσ(j). Following the\nproof of Lemma 3.1 in (Purohit et al., 2018), when running the preferential algorithm, the completion times of Algorithm\n2 increase by a factor of 1/λ, while the completion times of round-robin increase by a factor 1/(1−λ), and since both\nalgorithms are monotonic, the fact that some jobs are run simultaneously by both of them can only improve the objective\nfunction. Denoting Switch (ξyσ, x)the objective function of Algorithm 2 with instance x= (xi)n\ni=1and breakpoints\nξyσ= (ξyσ(i))B\ni=1, and RR(x)the objective function of round-robin with x, it follows that\nALG λ(x)≤min\u0012Switch (ξyσ, x)\nλ,RR(x)\n1−λ\u0013\n.\nGiven that RR(x)is deterministic and 2-competitive, and using Lemma 4.4, we obtain by taking the expectation that\nE[ALG λ(x)]≤E\u0014\nmin\u0012RR(x)\n1−λ,Switch (ξyσ, x)\nλ\u0013\u0015\n≤min\u0012RR(x)\n1−λ,E[Switch (ξyσ, x)]\nλ\u0013\n≤min\n2\n1−λ,\u0010\n2−B\nn+ρB\nn(1−B−1\nn−1)\u0011\n+\u0010\n4\nρ(1−B\nn) +B\nn\u0011\nnE[ησ]\nOPT(x)\nλ\nOPT(x),\nwhich concludes the proof.\n33",
  "textLength": 87379
}