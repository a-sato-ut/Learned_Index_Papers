{
  "paperId": "721a1639ebf45dc826abb76adb72adaac5ee81de",
  "title": "Warm-starting Push-Relabel",
  "pdfPath": "721a1639ebf45dc826abb76adb72adaac5ee81de.pdf",
  "text": "Warm-starting Push-Relabel\nSami Davies *, Sergei Vassilvitskii†, Yuyan Wang†\nMay 30, 2024\nAbstract\nPush-Relabel is one of the most celebrated network flow algorithms. Maintaining a pre-flow that sat-\nurates a cut, it enjoys better theoretical and empirical running time than other flow algorithms, such as\nFord-Fulkerson. In practice, Push-Relabel is even faster than what theoretical guarantees can promise, in\npart because of the use of good heuristics for seeding and updating the iterative algorithm. However, it\nremains unclear how to run Push-Relabel on an arbitrary initialization that is not necessarily a pre-flow or\ncut-saturating. We provide the first theoretical guarantees for warm-starting Push-Relabel with a predicted\nflow, where our learning-augmented version benefits from fast running time when the predicted flow is\nclose to an optimal flow, while maintaining robust worst-case guarantees. Interestingly, our algorithm uses\nthegap relabeling heuristic , which has long been employed in practice, even though prior to our work there\nwas no rigorous theoretical justification for why it can lead to run-time improvements. We then provide\nexperiments that show our warm-started Push-Relabel also works well in practice.\n1 Introduction\nMaximum flow is a fundamental problem in combinatorial optimization. It admits many algorithms, from\nthe famous Ford-Fulkerson algorithm [13] which employs augmenting paths, to recent near-linear time\nscaling based approaches [9]. In practice, however, the push-relabel family of algorithms is the benchmark\nfor fast implementations [27, 7].\nDesigned by Goldberg and Tarjan [15], the core Push-Relabel algorithm (Algorithm 1) has running time\nO(n2m), where nandmare the number of vertices and edges in the network. There are practical vari-\nants that reduce the running time to O(n2√m), and more theoretical adaptations that lead to sub-cubic\nO(nmlog(n2/m)run-time. Given the popularity of max-flow as a subroutine in many large scale applica-\ntions [5, 19, 27], it is no surprise that improving running times has been a subject of a lot of study, with\nmultiple heuristic methods being developed [2, 10, 14].\nTo complement the heuristics, researchers recently started looking at max-flow algorithms in the algo-\nrithms with predictions framework [24], and have successfully shown that one can improve running times\nwhen problem instances are not worst case, but share some commonalities [26, 11]. Two independent\ngroups initiated the study and proved that the running time of the Edmonds-Karp selection rule for Ford\nFulkerson can be improved from O(m2n)toO(m||f∗−ˆf||1), where f∗is an optimal flow on the network\nand ˆfis a predicted flow [11, 26]. These algorithms begin by modifying a predicted flow to form a feasible\nflow [11] or assuming that the predicted flow is already feasible [26], and then start the augmenting path\nalgorithms from that point. It is then relatively straightforward to bound the number of augmentations by\ntheℓ1-distance between the predicted and maximum flows.\nWhile these works have been shown to improve upon the cold-start, non learning-augmented versions,\nit is important to note that they have been improving upon sub-optimal algorithms for max flow. In this\nwork, we show how to warm-start Push-Relabel, whose cold-start version is nearly state-of-the-art for the\n*Department of EECS and the Simons Institute for the Theory of Computing, UC Berkeley. samidavies@berkeley.edu .\n†Google Research – New York. sergeiv@google.com ,wangyy@google.com .\n1\n\nmaximum flow problem. This directly addresses the challenge specified in [11] on bringing a rigorous\nanalysis for warm-starting non-augmenting path style algorithms. In the process of doing so, we provide a\ntheoretical explanation for the success of the popular gap relabeling heuristic in improving the running time\nof Push-Relabel algorithms. Specifically, both the gap relabeling heuristic and our algorithm maintain a\ncut with monotonically decreasing t-side nodes (see Section 1.2 for more), which directly leads to improved\nrunning times for our version of Push-Relabel and the gap relabeling heuristic. Lastly, we show that our\ntheory is predictive of what happens in practice with experiments on the image segmentation problem.\n1.1 Preliminaries\nGraph, flow and cut concepts. Our input is a network G= (V, E), where each directed edge e∈Eis\nequipped with an integral capacity ce∈Z≥0. Let|V|=nand|E|=m.Gcontains nodes s, the source, and\nt, the sink. Gis connected: ∀u∈V, there are both s−uandu−tpaths in G. A flow f∈Zm\n≥0is feasible if\nit satisfies: (1) flow conservation , meaning any u∈V\\ {s, t}satisfyP\n(v,u)∈Efe=P\n(u,w)∈Efe; (2) capacity\nconstraints , meaning for all e∈E,fe≤ce. Our goal is to find the maximum flow, i.e. one with the largest\namount of flow leaving s.\nWe call fapseudo-flow if it satisfies capacity constraints only. A node u∈V\\ {s, t}is said to have\nexcess if it has more incoming flow than outgoing, i.e.,P\n(v,u)∈Efe>P\n(u,w)∈Efe; analogously it has deficit\nif its outgoing flow is more than ingoing. We denote the excess and deficit of a node uwith respect to f\nasexcf(u) = max {P\n(v,u)∈Efe−P\n(u,w)∈Efe,0}anddeff(u) = max {P\n(u,w)∈Efe−P\n(v,u)∈Efe,0}, where\nat most one can be positive. A pseudo-flow can have both excesses and deficits, whereas a pre-flow is a\npseudo-flow with excess only.\nFor a pseudo-flow f, the residual graph Gfis a network on V; for every e= (u, v)∈E,Gfhas edge e\nwith capacity c′\ne=ce−feand a backwards edge (v, u)with capacity fe. Let E(Gf)denote the edges in\nGf. The value of a pseudo-flow fisval(f) =P\ne=(s,u)fe, the total flow going out of s. Notice that this is\nnot necessarily equivalent to the total flow into tsince flow conservation is not satisfied. A cut-saturating\npseudo-flow is one that saturates some s−tcut in the network. Push-Relabel maintains a cut-saturating\npre-flow; equivalently, there is no s−tpath in the residual graph of the pre-flow. We use δ(S, T)to denote\nans−tcut between two sets SandT. Note that the cut induced by any cut-saturating pseudo-flow fcan\nbe found by taking T={u∈V:∃u−tpath in Gf}(including t) and S=V\\T.\nPrediction. The prediction that we will use to seed Push-Relabel is some ˆf∈Zm\n≥0, which is a set of\nintegral values on each edge. Observe that one can always cap the prediction by the capacity on every\nedge to maintain capacity constraints, so throughout this paper we will assume ˆfis a pseudo-flow. It\nis important to note that our predicted flow is notnecessarily feasible or cut-saturating, and part of the\ntechnical challenge is making use of a good predicted flow despite its infeasibility.\nError metric. We measure the error of a predicted pseudo-flow ˆfonG. The smaller the error is, the\nhigher quality the prediction is, and the less time Push-Relabel seeded with ˆfshould take. A pseudo-flow\nbecomes a maximum flow when it is both feasible and cut-saturating. Hence, the error measures how far\nˆfis from being cut-saturating while being feasible. We say that a pseudo-flow ˆfisσfar from being cut-\nsaturating if there exists a feasible flow f′onGˆfwhere val(f′)≤σand ˆf+f′is cut-saturating on G(though\nthe cut does not have to be a min-cut). To measure how far ˆfis from being feasible, we sum up the total\nexcesses and deficits. In total we use the following error metric:\nDefinition 1. For pseudo-flow bfon network G, the error ofbfis the smallest integer ηsuch that (1) bfisηfar from\nbeing cut-saturating and (2) the total excess and deficit in Gwith respect to bfisP\nu∈V\\{s,t}excbf(u)+defbf(u)≤η.\nIfη= 0,ˆfis the max-flow and the cut that is saturated is the min-cut. The previously studied error\nmetric for predicted flows, such as by [11] and [26], was ||f∗−ˆf||1, for any max-flow f∗.\nPAC-learnability is the standard to justify that the choice of prediction and error metric are reasonable.\nFlows are PAC-learnable with respect to the ℓ1-norm [11]. Our results hold replacing our error metric with\n2\n\nAlgorithm 1 Push-Relabel\nInput : Network G\nDefine fe=cefore= (s, u)andfe= 0for all other e\nDefine h(u) = 0 for all u∈V\\ {s}andh(s) =n\nBuild residual network Gf\nwhile ∃node uwith excf(u)>0do\nif∃admissible (u, v)∈E(Gf)with f(u,v)< c(u,v)then\nUpdate fby sending an additional flow value of min{excf(u), c′\n(u,v)}along (u, v)\nUpdate Gf\nelse\nUpdate h(u) = 1 + min v:(u,v)∈E(Gf)h(v)\nOutput :f\ntheℓ1-norm because our metric provides a more fine-grained guarantee than the ℓ1-norm (i.e., if a prediction\nˆfhas error η, then η≤ ||f∗−ˆf||1). Thus we can omit any theoretical discussion of learnability. We present\nthis work with respect to our error metric as we find the ℓ1error metric to be unintuitive, in the sense that\nit is not really descriptive of how good a predicted flow is.\nPush-Relabel. Here, we review the “vanilla” form of Push-Relabel. The Push-Relabel algorithm main-\ntains a pre-flow and set of valid heights (also called labels). Heights h:V→Z≥0arevalid for a pre-flow f\nif for every edge in the residual network (u, v)∈E(Gf),h(u)≤h(v) + 1 , and if h(s) =nandh(t) = 0 . An\nedge (u, v)∈E(Gf)is called admissible ifh(u) =h(v) + 1 andc′\n(u,v)>0, which means we can push flow\nfrom utov. The formal Push-Relabel algorithm is in Algorithm 1, seeded with finitwhere finit\ne=cefor all\ne= (s, u)and otherwise finit\ne= 0.\nIt is known from the original analysis that all heights in Push-Relabel are bounded by 2n.\nLemma 1. For a pre-flow fon network G, every node uwith excf(u)>0has a path in Gftos. Further, for d(u, v)\nthe length of the shortest path between utovinGf, any valid heights in Push-Relabel (Algorithm 1) satisfy\nh(u)≤h(v) +d(u, v)\nChoosing v=s, we have h(u)≤h(s) +n= 2n.\nAt any point of the algorithm, the s−tcut maintained by the pre-flow can be induced using the heights.\nLemma 2. Forhvalid heights for a cut-saturating pseudo-flow fon network G, letθbe the smallest positive integer\nsuch that θ /∈ {h(u)}u∈V. Then S={u∈V:h(u)> θ}andT={u∈V:h(u)< θ}form a cut saturated by f.\nWe will call this the cut induced by the heights . Indeed, such a threshold θcan be found because {h(u)}u∈V\nhas at most ndifferent values, but h(s) =nandh(t) = 0 , so among the n+ 1values {0,1, . . . , n }, there is\nat least one not in the set. It is easy to see δ(S, T)is a saturated cut. For any u∈S, v∈T, we have that\nh(u)> h(v) + 1 , so(u, v)is not admissible in the residual graph. It follows that either (u, v)∈Eand it is\nsaturated, or (v, u)∈Eandf(v,u)= 0.\nA saturated cut be can defined from the set of vertices that can reach the sink in the residual graph.\nLemma 3. For any pseudo-flow fon network G, letTbe all nodes that can reach tinGfandS=V\\T. If both\nS, T are non-empty, then δ(S, T)is a saturated cut.\nProof. Fixu∈S,v∈T. Since vcan reach tanducannot, any edge (u, v)from StoTinGmust be saturated\nbyf, and any edge (v, u)from TtoSinGmust have no flow. This is because if either of these were not\ntrue, the edge (u, v)inGfwould have positive capacity, allowing uto reach t. Hence δ(S, T)is saturated\nbyf.\n3\n\nLemmas 2 and 3 apply to all pseudo-flows, whereas vanilla Push-Relabel must take a pre-flow as input.\nBefore this work, it was unclear how to seed Push-Relabel with anything other than finit.\nPush-Relabel can be implemented with the gap relabeling heuristic , i.e., whenever there is some integer\n0< θ < n with no nodes at height θ, then nodes with height between θandnhave their height increased\nton. See Algorithm 2 for the formal statement, where the cold-start version of this algorithm is to take as\ninput the cut-saturating pre-flow finit.\n1.2 Technical contribution\nWe first review Push-Relabel with the gap relabeling heuristic when the algorithm is seeded with a pre-\ndiction that is a cut-saturating pre-flow with error η. We show that this version of Push-Relabel finds an\noptimal solution in time O(η·n2). Recall that in this setting, Definition 1 implies that ηis just the total\nexcess. This running time also holds for cold-start versions of the algorithm when the max-flow/min-cut\nvalue is known to be bounded by η. This is (1) the first theoretical analysis of the gap relabeling heuristic,\nand, (2) the first result showing a running time bounded by the volume of the cut in Push-Relabel. Unlike\nthe Ford-Fulkerson algorithm, which admits a naive run-time bound of O(η·m)when the max-flow value\nis bounded by η, an analogous claim cannot be made easily for Push-Relabel.\nIntuitively, Push-Relabel with the gap relabeling heuristic essentially maintains a cut whose t-side is\nmonotonically decreasing (i.e., it moves nodes on the t-side of the cut to the s-side, but not the other way\naround), and resolves excess on the t-side by routing excess flow to t, or updating the cut so the excess\nnode is on the s-side of the new cut. In the latter case, the excess flow will be sent back to slater. The\nsame insight will be used in our general warm-started version of Push-Relabel that can be seeded with any\npseudo-flow.\nOur main result is the following theorem, which applies in the general setting where the prediction is\nany pseudo-flow, i.e., the prediction is not necessarily a pre-flow and is not necessarily cut-saturating.\nTheorem 1. Given a predicted pseudo-flow bfwith error ηon network G, there exists a warm-start version of Push-\nRelabel that obtains the minimum cut in time O(η·n2).\nOur warm-start version of Push-Relabel has several phases. Within each phase, Push-Relabel with the\ngap relabeling heuristic is used as a subroutine on auxiliary graphs.\nFirst, we show that one can modify the prediction bfto be a cut-saturating pseudo-flow; we call this\ncut-saturating pseudo-flow f. This is accomplished by running the cold-start Push-Relabel with the gap\nrelabeling heuristic on the residual graph to find a max-flow/min-cut.\nWe begin the second phase by routing flow within the two sides of the cut induced by the pseudo-flow\nto resolve some of the excess and deficit. The maintained cut gradually changes as we send flow from node\nto node, and push certain nodes to different sides of the cut. We do this by running the standard, cold-start\nversion of Push-Relabel on auxiliary networks on each side of the cut, and then adding the resulting flow\ntof. Because these auxiliary networks have small minimum cut value, the flows can be found quickly. We\ncontinue changing the cut until all excess nodes end up on the s-side of the cut and all deficit nodes end up\non the t-side of the cut. This “swapping” procedure between excess and deficits nodes between the s- and\nt- sides of the cut is our biggest technical innovation. Either the excesses are resolved within the t-side of\nthe cut, or we find a new cut between the t-side excess nodes and the t-side deficit nodes plus t. We modify\nthe cut in Gaccordingly to separate all excess from the t-side, which also results in a cut whose t-side is\nmonotonically decreasing—an interesting point which we show also occurs in the cut maintained by the\ngap relabeling heuristic. A mirrored version of this process is performed on the s-side of the cut.\nIn the final phase, we have a new cut-saturating pseudo-flow with all excess nodes on the s-side of the\ncut and all deficit nodes on the t-side of the cut. This cut is actually a min-cut. On the s-side, the excess\nnodes send flow to the source, and on the t-side, the sink sends flow to deficit nodes (hence removing\nexisting flows). The result is a max-flow. See Figure 1 for an illustration of phases 2 and 3.\nIn Section 4, we run our warm-start Push-Relabel compared to a cold-start version. We see that the\nwarm-start improves over the cold-start by a larger percentage as the size of the image increases.\n4\n\ndef s t \nex c ……ex c def \nex c ……\ndef s t \nex c ……ex c def ……def def \ndef s t \nex c ……ex c def ……\ns t …………Phase 2(a): Resolve excess on t-side, move cut. Phase 2(b): Resolve deficit on s-side, move cut. Min-cut found at end of phase. \nPhase 3: Maintain cut, fix excess/deficit within s- and t- side. End: No excess/deficit left. Both min-cut and max-flow are now found. Figure 1: An illustration of different phases of warm-start Push-Relabel, starting with a cut-saturating\npseudo-flow. The red curve denotes the cut. The black arrows denote the existing flows, whereas the\nred arrow denotes the flows sent in each phase to resolve excesses/deficits. Notice that as flows are sent,\nnew edges become saturated and smaller cuts are found, swapping excess and deficit nodes to the opposite\nsides of the cut.\n1.3 Related work\nPush-Relabel is one of the most popular algorithms for finding a max flow or min cut. See Algorithm 1 for\nits statement. The algorithm has running time O(n2m)and was designed by Goldberg and Tarjan [15].\nSeveral computational studies focus on the performance of Push-Relabel. A very popular heuristic is\nto choose the push operation to occur from the node with the largest height, and this gives theoretical\nimprovements too, with running time O(n2√m). In a well-known empirical study performed in 1997,\nlargest-height Push-Relabel was the fastest max-flow algorithm on most classes of networks, outperforming\nDinic’s augmenting flow algorithm, Karzanov’s algorithm, and Ahuja, Orlin, and Tarjan’s excess–scaling\nalgorithm [2]. Since then, Hochbaum’s Pseudo-flow algorithm—and extensions of it—have been shown in\nexperiments to be faster than largest-height Push-Relabel [17, 7, 16]. Throughout the development of new\nmax-flow algorithms, Push-Relabel remains one of the most versatile and is often the standard benchmark\nto which new flow algorithms are compared.\nTwo notable heuristics for Push-Relabel are the global relabeling heuristic and gap relabeling heuristic\n[18, 10, 14]. The global relabeling heuristic occasionally updates the heights to be a node’s distance from t\nin the current residual graph. Interestingly, these heuristics are more effective together than separately in\npractice [10].\nThe field of learning-augmented algorithms, also known as algorithms with predictions, has gained\nnotable popularity over the past 5 years. An algorithm is given access to a prediction about a quantity\npertaining to the input, and this prediction can guide the algorithm into making better choices. Predictions\nhave primarily been used to improve competitive ratios for online algorithms for problems in many areas,\nincluding scheduling [21, 1], caching [23, 3], and data structures [20, 22]. More recently, they have also been\nused to improve the running times of algorithms [12, 8, 25]. Two independent groups initiated the study\nof learning-augmented max-flow by warm-starting Ford-Fulkerson procedures [26, 11]. These works show\nthat the running time of Edmonds-Karp can be improved from O(m2n)toO(m||f∗−bf||1), forf∗an optimal\nflow on the network and bfthe prediction; experiments on image segmentation instances exemplify that the\ntheory is predictive of practice [11].\nMore on the many max-flow algorithms,can be found in the survey by Williamson [27], while more on\nlearning-augmented algorithms can be found in the survey by Mitzenmacher and Vassilvitskii [24].\n5\n\nAlgorithm 2 Warm-start Push-Relabel with Gap Relabeling\nInput: Network G, a cut-saturating pre-flow f\nConstruct residual network Gfwith capacity c′\nRun Algorithm 3 on Gandf, obtain hand(S, T)\nInitialize θ= min {z∈Z>0:∄u∈Twith z=h(u)}\nwhile ∃u∈Twith excf(u)>0do\nif∃v∈Twith h(u) =h(v) + 1 ,(u, v)∈E(Gf), and f(u,v)< c(u,v)then\nUpdate fby sending an additional flow value of min{excf(u), c′\n(u,v)}along (u, v)\nUpdate Gf\nelse\nRelabel uwith h(u) = min v:(u,v)∈E(Gf)h(v) + 1\nUpdate θ= min {z∈Z>0:∄u∈Twith z=h(u)}\nforp∈Twith h(p)> θdo\nRemove pfrom T, add ptoS\nUpdate p’s height to h(p) =n\nTake Gfas input and run Algorithm 1 on it to fix excesses, outputs flow f∗\nReturn cut parts SandT, cutδ(S, T), and flow f+f∗\n2 Gap Relabeling Push-Relabel: Cold- and Warm-Start\nAmong the many heuristic adaptations for Push-Relabel, the gap relabeling heuristic is known to empiri-\ncally improve the performance. In this section, we analyze the performance of Push-Relabel with the gap\nrelabeling heuristic (Algorithm 2) when given a cut-saturating pre-flow f, and tie the running time to the\nerror of f.\nAlgorithm 2 begins by running Algorithm 3 as a subroutine to find the s−tcut saturated by fand\ndefine valid heights for fwhich also induce that cut. Algorithm 3 runs a BFS in the residual graph to find\nall nodes that have a path to tand names this set T. The other nodes belong to S. The cut δ(S, T)has to be\nsaturated by f(see Lemma 3).\nFrom there on, Algorithm 2 has a two-phase structure. In phase one (the main WHILE loop), the algo-\nrithm maintains a set of t-side nodes of the cut, denoted by T, and all heights in Tmust cover a series of\nconsecutive numbers starting from 0. Intuitively, for any node with excess in T, Algorithm 2 tries to resolve\nits excess by re-routing it to other nodes. If this is not possible, the algorithm forces the node (and possibly\nother nodes, too) to leave Tand changes the cut maintained by the pre-flow. The cut only changes when\na node is relabeled in a way that results in a break in the series of consecutive heights starting from 0in\nT, where the smallest missing height is denoted by θ. The algorithm then removes all nodes from Twith\nheight bigger than θ; importantly, these nodes will never enter Tagain. Phase one terminates when Thas no\nmore excess nodes. With the correct data structure, the threshold height θand the set Tcan be maintained\nat minimal cost.\nAt the end of phase one, despite potential excess nodes on the s-side, the cut obtained is a min-cut.\nLemma 4. Letfbe a pre-flow saturating cut δ(S, T)on network G. If there are no excess nodes in T, then all excess\ninScan be sent back to swithout crossing the cut, implying that the cut δ(S, T)is a min-cut.\nProof. It is known from the proof of the vanilla Push-Relabel algorithm that all excess nodes in a pre-flow\nmust have a path back to s; see Lemma 1. When fsaturates δ(S, T), such a path cannot go from StoT, so\nthe path must be within S. The last two lines of Algorithm 2 will resolve the excesses without effecting the\nsaturated cut. So we have a feasible flow saturating a cut, meaning the flow is a max-flow and the cut is a\nmin-cut.\nFor applications where max-flow is simply a subroutine for finding a min-cut rather than the goal—such\nas in image segmentation — in Algorithm 2, one can omit running Algorithm 1 after the WHILE loop ends.\n6\n\nAlgorithm 3 Define Heights\nInput : Network G, a cut-saturating pseudo-flow f\nDefine h(s) =n,h(t) = 0\nRun BFS in Gf\nLetT={u∈V:∃u−tpath in Gf}\nLetS=V\\T\nforallu∈Sdo\nLeth(u) =n\nforallu∈Tdo\nLeth(u)be the shortest path length from utotinGf\nOutput : Valid heights h, and cut parts SandT\nWe show that the running time is tied to η, which, in this case, is the total excess in f.\nTheorem 2. Given a cut-saturating pre-flow fwith error ηon network G, Algorithm 2 finds a max-flow/min-cut in\nrunning time O(η·n2).\nProof. The algorithm first works to resolve excess in T, possibly moving nodes from TtoSto do so. Once\nall excess is in S, correctness follows from Lemma 4. Note that the conditions of Lemma 4 are satisfied since\nby Lemma 3 the cut output by Algorithm 3 is saturated by f.\nTo bound the running time of Algorithm 2, we use a potential function argument that is different from\nthan in the standard Push-Relabel analysis.\nWe first bound the running time of the main WHILE loop that terminates when all excess is contained in\nSand the min-cut is found. We define the potential function Φ(T) =P\nu∈Texcf(u)·h(u). The operations\ninvolved change the value of Φ(T)in the following way.\n• Saturated/Unsaturated push: In either case, at least one unit of excess flow is pushed from a higher\nheight to a lower height, since for edge (u, v)to be admissible, h(u) =h(v) + 1 . Therefore, Φ(T)\ndecreases by at least 1.\n• Relabeling: Any relabeling operation increases Φ(T). However, the total of all of these increases is at\nmost η·n2. The ηterm upper bounds the possible excess at any node, whereas the n2term is because\na node’s (of which there are at most n) height only ever increases, and the height cannot increase\nbeyond nbefore it must leave Tpermanently.\n• Removing nodes from T: Decreases Φ(T).\nHence the total running time before finding the min-cut is bounded by O(η·n2).\nTo bound the time for finding the max-flow, notice that the total excess in Gonly decreases, so when we\nstart to route excesses in Stos, the total excess is also bounded by η. The same potential function argument\ncan be used to prove it also takes O(η·n2)time to resolve all excess in S, though using the fact that in\nPush-Relabel, heights are always bounded by 2n(see Lemma 1).\nAlthough Algorithm 2 is presented as being seeded with an existing pre-flow, the same bound applies\nto the cold-start gap relabeling Push-Relabel when the min-cut of Gis at most η. This will prove useful in\nSection 3, as we repeatedly use Algorithm 2 as a subroutine to fix excess and deficits and redefine cuts on\nnetworks with small cut value.\nCorollary 1 (cold-start run-time with ηmin-cut value) .If network Gis known to have a max-flow/min-cut value\nof at most η, one can use Algorithm 2 to obtain a max-flow and min-cut for Gin running time O(η·n2).\nProof. Create an auxiliary graph G′by taking a copy of Gand adding a super-source s∗and an edge (s∗, s)\nwith capacity η. Create a pre-flow finitonG′by saturating (s∗, s)and letting fe= 0on all other edges in G′.\n7\n\nAlgorithm 4 Find a Cut-saturating Pseudo-flow\nInput : Network G, a pseudo-flow bf\nBuild G′, a copy of the residual network Gbf\nAdd super-source s∗, edge (s∗, s)with capacity ηtoG′\nLetfinitsaturate (s∗, s)and have flow 0 on all other edges\nRun Algorithm 2 with inputs G′andfinit, call output f′\nDelete f′\n(s∗,s)from f′\nOutput :f=f′+bf\nNow run Algorithm 2 with inputs G′andfinit. The initial (and maximum) excess in G′wasη, and so the\nrun-time is bounded by O(η·n2), as in the proof of Theorem 2.\nNote that we only assume ηto be known for simplicity of argument. A slightly modified algorithm can\nachieve the same running time with unknown η. See the discussion in Appendix A.1.\n3 Warm-starting Push-Relabel with General Pseudo-flows\nWe extend the results in Section 2 to when the given prediction is a general pseudo-flow bfas opposed to\na cut-saturating pre-flow, i.e., bfmay not be cut-saturating and may have deficit nodes. We assume bfhasη\nerror, as defined in Definition 1. The first phase of our algorithm augments bfby finding an s−tflow to add\ntobfso that the resulting pseudo-flow saturates a cut. Then, in phase two, it sends flow within both sides\nof the cut to eliminate and swap excess/deficit nodes, until all excess nodes are on the s-side of the cut and\nall deficit nodes are on the t-side. The min-cut is found at this point. Finally in phase three, the algorithm\nsends the remaining excess to sand deficit to tto obtain a feasible flow, which is also a max-flow.\n3.1 Obtaining a cut-saturating pseudo-flow from ˆf\nThe first phase is to pre-process bfinto a cut-saturating pseudo-flow on G. See Algorithm 4.\nWe create the auxiliary graph G′as in Algorithm 4, and then run the gap-relabeling Push-Relabel on\nG′(together with the standard initializing pre-flow) to find a minimum cut between s∗andtand obtain a\nflow f′. Corollary 1 bounds the Push-Relabel run-time in this case. Adding f′tobfcreates a cut-saturating\npseudo-flow.\nThe next lemma proves the output of this algorithm satisfies the desired properties and that the algo-\nrithm runs in time O(η·n2).\nLemma 5. Suppose bfis a predicted pseudo-flow with error ηfor network G. Then Algorithm 4 finds a cut-saturating\npseudo-flow fforGwith error ηin time O(η·n2).\nProof. In the residual graph Gbf, the min-cut is bounded by η, since it is at most ηfar from being cut-\nsaturating. Therefore, we can apply Corollary 1 to Gbfand obtain an optimal flow f′onGbfinO(η·n2)\nrunning time.\nThe flow we desire is fe=f′\ne+bfefor all e∈E. It is cut-saturating for Gby the optimality of f′onGbf.\nFurther, it is a pseudo-flow since f′does not have any excess or deficit in Gbfand clearly f′\ne+bfe≤cefor all\ne∈E.\nNotably, one can also run Algorithm 2 and terminate it upon finding the min-cut, in which case f′\nwill be a pre-flow on Gbf, and the resulting f=f′+bfwill have total excess bounded by 2η. In fact, one\ncan do this in other steps of the algorithm as well, if the goal is only to find a min-cut, and only lose an\n8\n\nadditional constant factor in the running time; see Appendix A.3. Additionally, in practice one may wish\nto use a predicted cut instead of finding a cut-saturating pseudo-flow as in Algorithm 4; see the discussion\nin Appendix A.1.\n3.2 Saturating a cut separating excesses from deficits\nOnce we have a cut-saturating pseudo-flow f, which by Lemma 5 can be obtained from the prediction using\nAlgorithm 4, we are ready to define the accompanying heights and cut using Algorithm 3 again. Note that\nthe initial cut with two sides T0={u∈V:∃u−tpath in Gf}andS0=V\\T0is by definition the same cut\nas that induced by the heights (as in Lemma 2).\nWe update the pseudo-flow so that it always maintains a saturated cut, but eventually, the nodes with\nexcess and the nodes with deficit are separated by the saturated cut. This is a generalization of what hap-\npens with Algorithm 2, where we transfer all excess nodes to the s-side of the cut. Here, we transfer all\nexcess to the s-side, and all deficit to the t-side of the cut. Interestingly, we observe that this is the sufficient\ncondition for the pseudo-flow to saturate a min-cut.\nLemma 6. For a cut-saturating pseudo-flow ffor a network G, letδ(S, T)be a cut it saturates. If all the nodes in T\nhave no excess and all the nodes in Shave no deficit, then the cut is a minimum cut.\nLemma 6 is essentially the analog of Lemma 4 in the more general pseudo-flow setting. The proof\ntechniques are similar—we prove that a flow can be found by sending all excess flow back to s, and by\nsending flow from tto all deficits. This fixes all excess and deficit, while maintaining the same cut.\nTo prove Lemma 6, we use the following result from Davies et al. [11]:\nLemma 7 (Lemma 5, restated from [11]) .Given any pseudo-flow ffor network G, every excess node has a path in\nGftoeither a deficit node or s; every deficit node has a path in Gffrom either an excess node or t.\nProof of Lemma 6. Consider the residual network Gf. By Lemma 7, every excess node uinSmust have a\npath to either a deficit node or to s. Since the current pseudo-flow fsaturates a cut, the path cannot go\nacross this cut and reach T, where all the deficits are. Therefore, uhas a path back to s, which only uses\nnodes in S. Similarly, by Lemma 7, for every deficit node v∈Tthere is a path that starts with either an\nexcess node or tand ends with v. Again, all excesses are in Sand the cut δ(S, T)is already saturated by f,\nso there is no path from StoT. This path then is from ttovand only uses nodes in T.\nIt follows that we can send all excess to sand send flow from tto all deficit nodes until the pseudo-flow\nbecomes a feasible flow. Notice that δ(S, T)remains saturated in this process. A feasible flow saturating a\ncut is a max-flow, and δ(S, T)is a min-cut.\nBy Lemma 6, it is sufficient to find a pseudo-flow and accompanying saturated cut where the excess\nnodes are all on the s-side and the deficit nodes are all on the t-side. We begin by focusing on the nodes\non the t-side of the cut, then briefly justify that the same can be done for the s-side by considering the\nbackwards network.\nMoving excess to the s-side. To resolve all excess on the t-side, we solve an auxiliary graph problem,\nwhere the goal is to send the maximum amount of flow from excess nodes to either deficit nodes or twithin\nthet-side (currently denoted T0). If the max-flow in this problem matches the total excess in T0, all excess\ncan be resolved locally and only deficits remain; otherwise, the max-flow solution on the auxiliary graph\nalso provides us with a min-cut that “blocks” excess nodes from deficit nodes and t. This cut will become\nthe new cut maintained by the pseudo-flow after adding the auxiliary flow to it.\nFor the construction of the auxiliary graph G′, take the residual graph induced on the nodes T0,Gf[T0].\nAdd a super-source and -sink s∗andt∗to it. Add edges (s∗, u)with capacity excf(u)for every excess node\nu∈T0; add edges (v, t∗)with capacity deff(v)for every deficit node v∈T0; and finally, add an edge (t, t∗)\nwith capacity η+ 1.\n9\n\nAlgorithm 5 Moving all excess to the s-side of the cut\nInput : Network G, a cut saturating pseudo-flow f\nRun Algorithm 3, get output heights h\nLetT0={u∈V:∃u−tpath in Gf}andS0=V\\T0\nBuild the residual Gf\nBuild G′on copy of Gf[T0]plus{s∗, t∗}\nforexcess node u∈T0\\ {t}do\nAdd edge (s∗, u)with capacity excf(u)\nfordeficit node v∈T0do\nAdd edge (v, t∗)with capacity deff(u)\nAdd edge (t, t∗)with capacity η+ 1\nLetfinit\n(s∗,u)=c(s∗,u)for all (s∗, u), and all other finit\ne= 0\nRun Algorithm 2 on G′andfinit, outputs f′andT′\n0,T′′\n0\nforall copies of e= (u, v)∈E(Gf)where f′\ne>0do\nUpdate fe←fe+f′\ne\nOutput : Flow fand cut parts S0∪T′\n0andT′′\n0\nWhen we run cold-start Push-Relabel (Algorithm 2) on G′, it outputs a flow f′and the s∗−t∗cut\nδ(T′\n0, T′′\n0). Note that t∈T′′\n0, since (t, t∗)has infinite capacity and therefore cannot be in the cut. Any s∗−t∗\npath pinG′along which fsends δunits of flow exactly identifies nodes uandv(where (s∗, u)∈pand\n(v, t∗)∈p) for which δunits of flow can be sent from utovalong the interior of pinGf. Thus we can send\nflow as indicated by f′to update f. See Algorithm 5 for details. We obtain the following guarantee on the\nupdated pseudo-flow f.\nClaim 1. In Algorithm 5, the output pseudo-flow fsaturates the cut δ(S0∪T′\n0, T′′\n0), and all excess nodes are in\nS∪T′\n0. Moreover, the total excess and deficit in Ghas not increased.\nProof. Letfolddenote the input to Algorithm 5.\nThe fact that the output fsaturates the cut δ(S∪T′\n0, T′′\n0)immediately follows from the fact that f′\nsaturated the cut δ(T′\n0, T′′\n0)inG′. Indeed, all edges from T′\n0toT′′\n0are now saturated and all edges from T′′\n0\ntoT′\n0have no flow. All edges from StoT′′\n0are already saturated in the old flow foldand remain so after\nadding f′since its flows are locally within T0. For the same reason, all edges from T′′\n0back to Sstill have\nno flow.\nNow, we consider the total excess and deficit. First note that the nodes that have excess/deficit with\nrespect to the updated pseudo-flow fare a subset of the nodes that had excess/deficit with respect to fold,\nand the excess/deficit of a node is clearly never increased.\nAssume for sake of contradiction that there is an excess node u∈T′′\n0. Then uhad excess with respect\ntofoldtoo, so there is an edge (s∗, u)that had capacity excfold(u)inG′but was not saturated by f′. Further,\nsince a min-cut in G′isδ(T′\n0, T′′\n0), it must be that ucan reach tinG′. This means that in G′there is a path\nwith positive remaining capacity between s∗andt∗, contradicting the fact that f′was a max-flow in G′.\nBy Claim 1, the updated fsatisfies the conditions of Lemma 8 by taking S∗=S0∪T′\n0andT∗=T′′\n0.\nLastly, the run-time claimed in Lemma 8 follows by applying Corollary 1 on G′. Putting everything together\nwe have the following lemma.\nLemma 8. Letfbe a pseudo-flow for network Gwith error ηthat saturates cut δ(S0, T0). Algorithm 5 finds a new\ncut-saturating pseudo-flow in time O(η·n2)so that the new pseudo-flow saturates an additional s−tcutδ(S∗, T∗)\nthat has no excess nodes in T∗, and the total excess and deficit is still bounded by η.\nMoving deficits to the t-side. Next, we will do a similar procedure for the s-side of the cut, though this\ntime we wish to remove deficit nodes. We will show that this is exactly the backward process of what\n10\n\nhappens to the t-side, and can be done by reversing the graph edges and flows and running Algorithm 5\non the reversed network.\nWe will build the reverse network of G, call it B(for backwards). The network Bconsists of a copy\nofGbut all of the edges go the opposite direction. More specifically, for every node u∈V(G)there is\na mirror node u′inB, and for every edge e= (u, v)∈E(G)with capacity ce, there is a mirror edge\ne′= (v′, u′)∈E(B)with capacity ce. Note that the source sinGis mirrored to the sink s′inB, whereas the\nsinktinGis mirrored to the source t′inB.\nWe can reverse any pseudo-flow fonGto be another pseudo-flow f′onB, where for all e∈E(G),f′\ne′=\nfe. Notably, fandf′saturate the same cut, and we observe excf(u) =deff′(u′)anddeff(u) =excf′(u′).\nSuppose we have a pseudo-flow fthat saturates cut δ(S0, T0)inGwith no excess nodes in T0. Then in\nthe backwards network B,f′saturates δ(T′\n0, S′\n0), where T0(resp. S0) is all mirror nodes p′for such p∈T0\n(resp. S0). Now S′\n0becomes the sink-side of the cut. In B, we can send flow from excess nodes and s′to\ndeficit nodes within S′\n0, and this can be done by running Algorithm 5 on B.\nThe true algorithm for Gis Algorithm 6, which defer to Appendix A.2, since it is really just the mirror\nimage of Algorithm 5, though we may skip the execution of Algorithm 3, as we already know the cut.\nThis flow, when reversed back into G, is the maximum amount of flow that can go from excess nodes\nandsto deficit nodes in Gf[S0]. After adding this reversed flow to f, the result is a cut-saturating pseudo-\nflow for G, where there is no deficit on the s-side of the cut. Observe that there is no excess or deficit created\non either side of the cut in the process.\nWe obtain the following corollary of Lemma 8.\nCorollary 2. Letfbe a pseudo-flow for network Gwith error ηthat saturates cut δ(S0, T0). One can update fin\ntimeO(η·n2)so that all flow in T0remains unchanged, but now fsaturates a cut δ(S∗, T∗)and there are no deficit\nnodes in S∗.\n3.3 From min-cut to max-flow\nSummarizing this section, we prove our main theorem.\nProof of Theorem 1. Given a predicted pseudo-flow bfwith error ηon network G, Lemma 5 proved that\nAlgorithm 4 finds a cut-saturating pseudo-flow fforGwith error ηin time O(η·n2). To find a min-cut,\nLemma 6 shows that it is enough to find a pseudo-flow saturating a cut so that the t-side of the cut contains\nno excess and the s-side of the cut contains no deficit.\nWe run Algorithm 5 seeded with fonGto obtain an updated cut-saturating pseudo-flow with no excess\non the t-side of the maintained cut by Lemma 8. Then, Algorithm 5 can be run on the backwards network\nB, and from Corollary 2, the updated cut-saturating pseudo-flow now has no excess on the t-side of the cut\nand no deficit on the s-side.\nThe last phase of the algorithm can be left out if only the min-cut is desired; suppose the min-cut is\nδ(S, T). By the proof of Lemma 6, to obtain a max-flow we only need to send all excess flow back to s, and\nsend flow from tto every deficit node. Label all nodes in Swith height nand all nodes in Twith height 0.\nThen run Algorithm 2 to fix all excess in S. The algorithm will only send flow back to s, since there is no\nway to cross the cut δ(S, T). Then reverse the graph and flow, and again run Algorithm 2 to fix the excess\nnodes in the reversed graph, which exactly correspond to the deficit nodes in the original graph.\n4 Empirical Results\nIn this section, we validate the theoretical results in Sections 3. To demonstrate the effectiveness of our\nmethods, we consider image segmentation , a core problem in computer vision that aims at separating an\nobject from the background in a given image. It is common practice to re-formulate image segmentation\nas a max-flow/min-cut optimization problem (see for example [6, 5, 4]), and solve it with combinatorial\ngraph-cut algorithms.\n11\n\n(a) Birdhouse\n (b) Head\n (c) Shoe\n (d) Dog\nFigure 2: The cropped and gray-scaled images from Figure 4 (copy from Figure 2 in [11]).\nThe experiment design we adopt largely resembles that in [11], which studied warm-starting the Ford-\nFulkerson algorithm for max-flow/min-cut. As in previous work, we do not seek state-of-the-art running\ntime results for image segmentation. Our goal is to show that on real-world networks, warm-starting can\nlead to significant run-time improvements for the Push-Relabel min-cut algorithm, which claims stronger\ntheoretical worst-case guarantees and empirical performance than the Ford-Fulkerson procedures. We\nhighlight the following:\n• Our implementation of cold-start Push-Relabel is much faster than Ford-Fulkerson on these graph in-\nstances, enabling us to explore the effects of warm-starting on larger image instances. This improved\nefficiency results from implementing the gap labeling and global labeling heuristics, both known to\nboost Push-Relabel’s performance in practice.\n• As we increase the number of image pixels (i.e., the image’s resolution), the size of the constructed\ngraph increases and the savings in time becomes more significant.\n• Implementation choices (such as how to learn the seed-flow from historical graph instances and their\nsolutions) that make the predicted pseudo-flow cut-saturating and that reroute excesses and deficits\nare crucial to the efficiency of warm-starting Push-Relabel.\nDatasets and data prepossessing Our image groups are from the Pattern Recognition and Image Processing\ndataset from the University of Freiburg, and are titled B IRDHOUSE , H EAD, SHOE , and D OG. The first\nthree groups are .jpg images from the Image Sequences1dataset. The last group, D OG, was a video that we\nconverted to a sequence of .jpg images from the Stereo Ego-Motion2dataset.\nEach of the image groups consists of a sequence of photos of an object and its background. There are\nslight variations between consecutive images in a sequence, which are the result of the object and back-\nground’s relative movements or a change in the camera’s position. These changes alter the solution to the\nimage segmentation problem, but the effects should be minor when the change between consecutive im-\nages is minor. In other words, we expect an optimal flow and cut found on an image in a sequence to be a\ngood prediction for the next image in the sequence.\nFrom each group, we consider 10 images and crop them to be either 600×600or500×500pixel images,\nstill containing the object, and gray-scale all images. We rescale the cropped, gray-scaled images to be N×N\npixels to produce different sized datasets. Experiments are performed for N∈ {30,60,120,240,480}. In the\nconstructed graph, we have |V|=N2+ 2. Every graph is sparse, with |E|=O(|V|), hence both |V|and|E|\ngrow as O(N2). Detailed description of raw data and example original images can be found in Appendix B\n(Table 4, Figure 4).\n1https://lmb.informatik.uni-freiburg.de/resources/datasets/sequences.en.html\n2https://lmb.informatik.uni-freiburg.de/resources/datasets/StereoEgomotion.en.html\n12\n\nFigure 3: Cuts (red) on images chronologically evolving from the 240×240pixel images from B IRDHOUSE .\nGraph construction As in [11], we formulate image segmentation as a max-flow/min-cut problem. The\nconstruction of the network flow problem applied in both our work and theirs is derived from a long-\nestablished line of work on graph-based image segmentation; see [4]. The construction takes pixels in\nimages to be nodes; and a penalty function value which evaluates the contrast between the pigment of any\nneighboring pixels to be edge capacity. We leave details on translating the images to graphs on which we\nsolve max-flow/min-cut to Appendix B.\nImplementation details in warm-start Push-Relabel Throughout the experiments, whenever the Push-\nRelabel subroutine is called on any auxiliary graph, it is implemented with the gap relabeling heuristic,\nas shown in Algorithm 2, and the global relabeling heuristic, which occasionally updates the heights to be\na node’s distance from tin the residual graph. These heuristics are known to improve the performance\nof Push-Relabel. As a tie-breaker for choosing the next active node to push from, we choose the one with\nhighest height, which is known to improve the running time of Push-Relabel. We found the generic Push-\nRelabel algorithm without these heuristics to be slower than Ford-Fulkerson.\nAll images from the same sequence share the same seed sets. The constructed graphs are on the same\nsets of nodes and edges, but the capacities on the edges are different. The first image in the sequence is\nsolved from scratch. For the second image in the sequence, we reuse the old optimal flow and cut from\nthe first image one, then for the ithimage in the sequence, we reuse the optimal flow and cut from the\ni−1stimage. We reuse the old max-flow on the new network by rounding down the flow on edges whose\ncapacity has decreased, hence producing excesses and deficits, and pass this network and flow to the warm-\nstart Push-Relabel algorithm in Section 3.\nTo find a saturating cut, instead of sending flow from stotas suggested in Algorithm 4, we reuse the\nmin-cut on the previous image δ(S0, T0)and send flow from S0toT0that originates from either sor an\nexcess node, and ends at either tor a deficit node. We experimented with a few different ways of projecting\nthe old flow to a cut-saturating one on the new graph. The way we implemented was by far the most\neffective, although it shares the same theoretical run-time as Algorithm 4.\nThe graph-based image segmentation method finds reasonable object/background boundaries. Figure\n3 shows an example of how the target cut could evolve as the image sequence proceeds. Even with the\nsame set of seeds, the subtle difference in images could lead to different min-cuts that need to be rectified.\nHowever, the hope is that the old min-cut bears much resemblance to the new one, hence warm-starting\nPush-Relabel with it could be beneficial. See Appendix B for other examples.\nResults Table 1 shows average running times for both Ford-Fulkerson in [11] and Push-Relabel for two\nimage sizes: 120×120(the largest size tested in prior work) and 480×480. The full data on all data sizes are\nin Table 5 in Appendix B. Table 2 shows how the run-time of warm-start Ford-Fulkerson and Push-Relabel\nscales with growing image sizes on image group D OG. The “N/A” in both tables marks overly long run-\ntime ( >1 hour), at which point we stop evaluating the exact run-time. The run-time growth across data\nsizes on other image groups can be found in Appendix B.\nThese results show warm-starting Push-Relabel, while slightly losing in efficiency on small images,\ngreatly improves in it on large ones. As for the scaling of run-time with growing data sizes, both cold- and\n13\n\nTable 1: Average run-times (s) of cold-/warm-start Ford Fulkerson (FF) and Push-Relabel (PR)\nImage Group FF cold-start FF warm-start PR cold-start PR warm-start\nBIRDHOUSE 120×120 109.06 37.31 5.42 4.98\nHEAD120×120 101.79 28.43 5.90 5.92\nSHOE 120×120 98.95 30.44 6.44 3.74\nDOG120×120 190.36 38.08 6.76 6.38\nBIRDHOUSE 480×480 N/A N/A 604.54 502.58\nHEAD480×480 N/A N/A 365.25 285.75\nSHOE 480×480 N/A N/A 756.77 364.42\nDOG480×480 N/A N/A 834.63 363.41\nTable 2: Growth of average running times of warm-start Ford Fulkerson (FF) and Push-Relabel (PR) in\nseconds, on image group D OG\nAlgorithm 30×30 60 ×60 120 ×120 240 ×240 480 ×480\nFord-Fulkerson 0.41 6.89 42.04 459.48 NA\nPush-Relabel 0.11 0.95 6.38 52.42 363.41\nwarm- start’s running time increases polynomially with the image width n, but warm-start scales better,\nand as nincreases to 480, it gains a significant advantage over cold-start. Despite the different warm-start\ntheoretical bounds ( O(η|V|2)for Push-Relabel versus O(η|E|)for Ford-Fulkerson), in practice both warm-\nstart algorithms scale similarly as the dataset size grows, as shown in Table 2.\nTable 3 shows how the running time of warm-start Push-Relabel breaks down into the three phases\ndescribed in Section 3: (1) finding a cut-saturating pseudo-flow; (2) fixing excess on t-side; (3) fixing deficits\nons-side. Note phase (1) takes the most time, but results in a high-quality pseudo-flow, in that it takes little\ntime to fix the excess/deficits appearing on the “wrong” side of the cut.\nTable 3: Running time of warm-start Push-Relabel break down, on B IRDHOUSE\nSize 30×30 60 ×60 120 ×120 240 ×240 480 ×480\nTotal 0.06 0.45 4.98 55.68 502.58\nSaturating cut 0.04 0.34 4.17 46.25 431.49\nFixing texcesses 0.01 0.09 0.53 5.29 64.01\nFixing sdeficits 0.01 0.02 0.27 4.13 7.08\n5 Conclusions\nWe provide the first theoretical guarantees on warm-starting Push-Relabel with a predicted flow, improving\nthe run-time from O(m·n2)toO(η·n2). Our algorithm uses one the most well-known heuristics in practice,\nthe gap relabeling heuristic, to keep track of cuts in a way that allows for provable run-time improvements.\nOne direction of future work is extending the approaches in this work to generalizations of s-tflow\nproblems, for instance, tackling minimum cost flow or multi-commodity flow. An ambitious goal of such\nan agenda would be to develop new warm-start methods for solving arbitrary linear programs.\nA different line of work is to develop rigorous guarantees for other empirically proven heuristics by\nanalyzing them through a lens of predictions, providing new theoretical insights and developing new al-\ngorithms for fundamental problems.\n14\n\nReferences\n[1] Sara Ahmadian, Hossein Esfandiari, Vahab Mirrokni, and Binghui Peng. “Robust load balancing with\nmachine learned advice”. In: Journal of Machine Learning Research 24.44 (2023), pp. 1–46.\n[2] Ravindra K Ahuja, Murali Kodialam, Ajay K Mishra, and James B Orlin. “Computational investiga-\ntions of maximum flow algorithms”. In: European Journal of Operational Research 97.3 (1997), pp. 509–\n542.\n[3] Nikhil Bansal, Christian Coester, Ravi Kumar, Manish Purohit, and Erik Vee. “Learning-augmented\nweighted paging”. In: Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms\n(SODA) . SIAM. 2022, pp. 67–89.\n[4] Yuri Boykov and Gareth Funka-Lea. “Graph cuts and efficient ND image segmentation”. In: Interna-\ntional journal of computer vision 70.2 (2006), pp. 109–131.\n[5] Yuri Boykov and Vladimir Kolmogorov. “An experimental comparison of min-cut/max-flow algo-\nrithms for energy minimization in vision”. In: IEEE transactions on pattern analysis and machine intelli-\ngence 26.9 (2004), pp. 1124–1137.\n[6] Yuri Y Boykov and M.P . Jolly. “Interactive graph cuts for optimal boundary & region segmentation\nof objects in ND images”. In: Proceedings eighth IEEE international conference on computer vision. ICCV\n2001 . Vol. 1. IEEE. 2001, pp. 105–112.\n[7] Bala G Chandran and Dorit S Hochbaum. “A computational study of the pseudoflow and push-\nrelabel algorithms for the maximum flow problem”. In: Operations research 57.2 (2009), pp. 358–376.\n[8] Justin Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. “Faster fundamental graph algorithms\nvia learned predictions”. In: International Conference on Machine Learning . PMLR. 2022, pp. 3583–3602.\n[9] Li Chen, Rasmus Kyng, Yang P Liu, Richard Peng, Maximilian Probst Gutenberg, and Sushant Sachdeva.\n“Maximum flow and minimum-cost flow in almost-linear time”. In: 2022 IEEE 63rd Annual Sympo-\nsium on Foundations of Computer Science (FOCS) . IEEE. 2022, pp. 612–623.\n[10] Boris V Cherkassky and Andrew V Goldberg. “On implementing push-relabel method for the maxi-\nmum flow problem”. In: International conference on integer programming and combinatorial optimization .\nSpringer. 1995, pp. 157–171.\n[11] Sami Davies, Benjamin Moseley, Sergei Vassilvitskii, and Yuyan Wang. “Predictive Flows for Faster\nFord-Fulkerson”. In: Proceedings of the 40th International Conference on Machine Learning . Ed. by An-\ndreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan\nScarlett. Vol. 202. Proceedings of Machine Learning Research. PMLR, 23–29 Jul 2023, pp. 7231–7248.\nURL:https://proceedings.mlr.press/v202/davies23b.html .\n[12] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. “Faster\nmatchings via learned duals”. In: Advances in neural information processing systems 34 (2021), pp. 10393–\n10406.\n[13] Lester Randolph Ford and Delbert R Fulkerson. “Maximal flow through a network”. In: Canadian\njournal of Mathematics 8 (1956), pp. 399–404.\n[14] Andrew V Goldberg. “The partial augment–relabel algorithm for the maximum flow problem”. In:\nEuropean Symposium on Algorithms . Springer. 2008, pp. 466–477.\n[15] Andrew V Goldberg and Robert E Tarjan. “A new approach to the maximum-flow problem”. In:\nJournal of the ACM (JACM) 35.4 (1988), pp. 921–940.\n[16] Andrew V Goldberg, Sagi Hed, Haim Kaplan, Pushmeet Kohli, Robert E Tarjan, and Renato F Wer-\nneck. “Faster and more dynamic maximum flow by incremental breadth-first search”. In: Algorithms-\nESA 2015: 23rd Annual European Symposium, Patras, Greece, September 14-16, 2015, Proceedings . Springer.\n2015, pp. 619–630.\n15\n\n[17] Dorit S Hochbaum. “The pseudoflow algorithm: A new algorithm for the maximum-flow problem”.\nIn:Operations research 56.4 (2008), pp. 992–1009.\n[18] David S Johnson, Catherine C McGeoch, et al. Network flows and matching: first DIMACS implementation\nchallenge . Vol. 12. American Mathematical Soc., 1993.\n[19] Vladimir Kolmogorov, Yuri Boykov, and Carsten Rother. “Applications of parametric maxflow in\ncomputer vision”. In: 2007 IEEE 11th International Conference on Computer Vision . IEEE. 2007, pp. 1–8.\n[20] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. “The case for learned index\nstructures”. In: Proceedings of the 2018 international conference on management of data . 2018, pp. 489–504.\n[21] Alexandra Anna Lassota, Alexander Lindermayr, Nicole Megow, and Jens Schl ¨oter. “Minimalistic\npredictions to schedule jobs with online precedence constraints”. In: International Conference on Ma-\nchine Learning . PMLR. 2023, pp. 18563–18583.\n[22] Honghao Lin, Tian Luo, and David Woodruff. “Learning augmented binary search trees”. In: Interna-\ntional Conference on Machine Learning . PMLR. 2022, pp. 13431–13440.\n[23] Thodoris Lykouris and Sergei Vassilvitskii. “Competitive caching with machine learned advice”. In:\nJournal of the ACM (JACM) 68.4 (2021), pp. 1–25.\n[24] Michael Mitzenmacher and Sergei Vassilvitskii. “Algorithms with predictions”. In: Communications of\nthe ACM 65.7 (2022), pp. 33–35.\n[25] Taihei Oki and Shinsaku Sakaue. “Faster Discrete Convex Function Minimization with Predictions:\nThe M-Convex Case”. In: arXiv preprint arXiv:2306.05865 (2023).\n[26] Adam Polak and Maksym Zub. “Learning-augmented maximum flow”. In: arXiv preprint arXiv:2207.12911\n(2022).\n[27] David P Williamson. Network flow algorithms . Cambridge University Press, 2019.\n16\n\nA More Discussion on Warm-starting Push-Relabel\nWe include some more insights and details on warm-starting Push-Relabel.\nA.1 Tackling Unknown ηValue\nNotice that, Algorithm 4 directly uses the error value η, and treats it as given input. Due to the definition\nofη, it covers both the total excess/deficit and how far the current flow ˆfis from being cut-saturating.\nThe former is easy to measure; whereas the latter is not, since there are numerous cuts in Gand it is not\nobvious which one is closest to being saturated by ˆf. However, this challenge can be tackled by running the\nalgorithm in the binary-search fashion. Say there is an η∗which is the true error from being cut-saturating.\nWe cannot know this value for sure because we cannot compute the value on each possible cut. One can\nstart with some very small value of η(such as 1), put it on the edge (s∗, s), and try to use Push-Relabel to\nsend the flow from stot. If we successfully send the current ηfrom stot, there exists a s−tflow of ηin the\nresidual graph; meaning when Push-Relabel terminates the s∗−tcut we will find is just the edge η. If this is\nthe case, double η, saturate (s∗, s), and again use Push-Relabel to send the extra ηdownstream to t. Repeat\nthis until the returned cut is not (s∗, s). It will then be a s−tcut. Intuitively, the role ηplays here is just a\nsurplus of flow provided to the source s; hence it should not be bounding the flow-sending. Otherwise it\nmeans the current pseudo-flow is not yet cut-saturating. This gives the same run-time bound as each time\nwe double η, the excess to resolve only increases by η; hence the total excess we have resolved throughout\nall iterations is still O(η∗).\nIt is noteworthy that in experiments, we initialize ηto be the error computed on the old min-cut on\nthe previous image. While this cut is not necessarily the one that bounds η, we found it to be an effective\nsurrogate value for the real underlying η.\nNotably, one can also run Algorithm 2 and terminate it upon finding the min-cut, in which case f′will\nbe a pre-flow on Gˆf, and the resulting f=f′+ˆfwill have total excess bounded by 2η. In fact, one can do\nthis in other steps of the algorithm as well, if the goal is only to find a min-cut, and only lose an additional\nconstant factor in the running time; see Appendix A.3. As discussed, in practice one may wish to use a\npredicted cut instead of finding a cut-saturating pseudo-flow as in Algorithm 4.\nBy Definition 1, if a pseudo-flow ˆfisσfar from cut-saturating it means augmenting it by another flow\nfwith value at most σcan saturate some cut. Let this cut be δ(S, T). Another way to look at this is, within\nˆf, the total flow passing through the cut δ(S, T)satisfies:\nX\nu∈S,v∈Tˆf(u,v)−X\nu∈S,v∈Tˆf(v,u)≥X\nu∈S,v∈Tc(u,v)−X\nu∈S,v∈Tc(v,u)+σ.\nApart from solving max-flow in the residual graph to saturate this cut, there may be other options to\ncreate a cut-saturating pseudo-flow. For example, the ηbound on error does not directly tell us where this\ncut is. However, if a practitioner can “guess” a good enough cut δ(S, T)from past problem instances, such\na pseudo-flow can also be obtained simply by saturating all edges (u, v)∈δ(S, T)and removing the flow\non all backward edges. The downside is that such a practice will transfer the error on that particular cut\nto the total excess and deficit on nodes incident to the cut. Overall, there may be a trade-off where one can\nomit Algorithm 4 in lieu of using a predicted cut, but at the cost of having to fix more excess and deficit in\nlater steps.\nA.2 The mirror algorithm\nA.3 Early termination of auxiliary Push-Relabel upon finding min-cut\nIn Section 3, we mentioned that one can choose to quit the Push-Relabel algorithm on auxiliary graphs\nwhenever a cut is found. The resulting pseudo-flow, although violating flow conservation constraints, can\n17\n\nAlgorithm 6 Moving all deficit to the t-side of the cut\nInput : Network G, a pseudo-flow fsaturating cut δ(S0, T0)\nBuild the residual Gf.\nBuild G′on copy of Gf[S0]plus{s∗, t∗}\nforexcess node u∈S0do\nAdd edge (s∗, u)with capacity excf(u)\nfordeficit node v∈S0\\ {s}do\nAdd edge (v, t∗)with capacity deff(u)\nAdd edge (s∗, s)with capacity η+ 1(or sufficiently large capacity)\nLetfinit\n(s∗,u)=c(s∗,u)for all (s∗, u)andfinit\n(s∗,s)=c(s∗,s), and all other finit\ne= 0\nRun Algorithm 2 on G′andfinit, outputs f′andS′\n0,S′′\n0\nforall copies of e= (u, v)∈E(Gf)where f′\ne>0do\nUpdate fe←fe+f′\ne\nOutput : Flow fand cut parts S′\n0andS′′\n0∪T0\nstill be added to the initial pseudo-flow. We give a brief analysis of how this effects the execution of the\nalgorithm.\nThe pseudo-flow is constructed in three places:\n1. In Algorithm 4, where we saturate a cut;\n2. In Algorithm 5, where we push flow from t-side excess nodes to deficit nodes and t;\n3. In Algorithm 6, where we push flow from s-side excess nodes and sto deficit nodes.\nNotice this simple fact:\nClaim 2. For pseudo-flows f, f′,andf′′where f=f′+f′′(without violating capacities constraints), we have:\nX\np(excf(u) +deff(u))≤X\nu(excf′(u) +deff′(u)) +X\nu(excf′′(u) +deff′′(u))\nIn Step 1, Algorithm 2 starts with finitwith excess η, hence the resulting pre-flow also has at most η\nexcess, and adding this pre-flow without restoring it to a max-flow may increase the excess by η. In Step 2,\nthe initial flow in G′also has total excess of at mostP\nu∈T0excbf(u)≤η, so at the end of Algorithm 5 the total\nexcesses also increases by this much. In Step 3, correspondingly the maximum increase isP\nu∈S0defbf(u)≤\nη. To sum up, early termination in the auxiliary networks after finding the min-cut increases the total error\nbyO(η), and therefore has the same run-time bound up to a constant factor.\nB More on Experiments\nB.1 More on graph construction\nWe take as input an image on pixel set V, and two sets of seedsO,B ⊆ V. The seed set Ocontains pixels\nthat are known to be part of the object, while the seed set Bcontains pixels that are known to be part of the\nbackground. The intensity or gray scale of pixel vis denoted by Iv. We say that two pixels are neighbors\nif they are either in the same column and in adjacent rows or same row and adjacent columns. Intuitively,\nif neighboring pixels have very different intensities, we might expect one to be part of the object and one\nto be part of the background. For any two pixels p, q∈V, a solution that separates them, i.e., puts one\npixel in the object and the other one in the background, incurs a penalty ofβp,q. For neighbors pandq,\n18\n\nβp,q=Cexp(−(Ip−Iq)2/(2σ2)), forCa large constant, otherwise the penalty is 0. Note that the quantity\nβp,qgets bigger when neighbors pandqhave stronger contrast.\nA segmentation solution seeded with OandBlabels each pixel as either being part of the object or part\nof the background, and the labeling must be consistent with the seed sets. Let Jdenote the object pixels for\na fixed segmentation solution. Then the boundary-based objective function is the sum of all of the penalties\nmax JP\np∈J,q/∈Jβp,q,forJwithO ⊆ J,B ⊆ V\\J. As in the definition, a positive penalty cost is only\nincurred on the object’s boundary. The goal is to minimize the total penalty, which is in turn maximizing\nthe contrast between the object and background, for the given object and background seed sets.\nSolving this maximization problem is equivalent to solving the max-flow/min-cut problem on the fol-\nlowing network. There is a node for each pixel, plus the object terminal sand the background terminal t.\nAs notation suggests, sis the source of the network and tis the sink. The edge set on the nodes is as follows:\n(1) for every v∈ O add edge (s, v)with capacity M, forMa huge enough value that it is never saturated\nin any optimal cut; (2) for every u∈ B add edge (u, t), again with capacity M; (3) for every pair of nodes\np, q∈V, add edges (p, q)and(q, p)with capacity βp,q. If an image is on n×npixels, note that the graph is\nsparse with |V|=O(n2)nodes and |E|=O(n2)edges.\nIn our experiments, all βp,q’s are rounded down to the nearest integer, so that capacities are integral.\nSince βp,q≤Cby definition, it suffices for us to let M=C|V|2.\nB.2 Omitted tables and figures for experiments\nTable 4 contains a detailed description of each of the four image groups, their original size in the raw\ndataset, the cropped grey-scaled image size, the foreground/background they feature, etc.\nTable 4: Image groups’ descriptions (copy of Table 1 from [11])\nImage Group Object, background Original size Cropped size\nBIRDHOUSE wood birdhouse, backyard 1280, 720 600, 600\nHEAD a person’s head, buildings 1280, 720 600, 600\nSHOE a shoe, floor and other toys 1280, 720 600, 600\nDOG Bernese Mountain dog, lawn 1920, 1080 500, 500\nFigure 4 gives one example of raw images from each image group.\n(a) Birdhouse\n (b) Head\n(c) Shoe\n (d) Dog\nFigure 4: Instances of images from each group (copy of Figure 1 from [11]).\n19\n\nFigure 5: Cuts (red) on images chronologically evolving from the 240×240pixel images from D OG.\nFigure 6: Cuts (red) on images chronologically evolving from the 240×240pixel images from H EAD.\nIn the main body, Figure 3 shows examples of cuts found in some images from the B IRDHOUSE image\nsequence. Figure 5, 7, 6 show example cuts from the other image groups.\nB.3 Full running time results\nTable 5 compares the running time of cold-/warm-start Ford-Fulkerson as implemented in [11] against\nPush-Relabel on all data sizes and all image groups. The experiments were performed with the same\ncomputing configuration environment. One can see Push-Relabel greatly outperforms on the same image\nsize, allowing us to collect run-time statistics on images of sizes up to 480×480pixels, which we could not\ndo with implementations of Ford-Fulkerson, due to its slow run-time.\nFigure 7: Cuts (red) on images chronologically evolving from the 240×240pixel images from D OG.\n20\n\nTable 5: Average run-times (s) of cold-/warm-start Ford Fulkerson (FF) and Push-Relabel (PR)\nImage Group FF cold-start FF warm-start PR cold-start PR warm-start\nBIRDHOUSE 30×30 0.80 0.51 0.05 0.06\nHEAD30×30 0.62 0.43 0.05 0.05\nSHOE 30×30 0.65 0.39 0.07 0.06\nDOG30×30 0.69 0.32 0.10 0.11\nBIRDHOUSE 60×60 8.22 3.25 0.30 0.45\nHEAD60×60 9.36 4.10 0.50 0.50\nSHOE 60×60 8.09 3.04 0.69 0.47\nDOG60×60 21.91 6.73 0.76 0.95\nBIRDHOUSE 120×120 109.06 37.31 5.42 4.98\nHEAD120×120 101.79 28.43 5.90 5.92\nSHOE 120×120 98.95 30.44 6.44 3.74\nDOG120×120 190.36 38.08 6.76 6.38\nBIRDHOUSE 240×240 NA 400.19 60.67 55.68\nHEAD240×240 NA 374.79 32.46 31.00\nSHOE 240×240 NA 338.05 69.29 35.57\nDOG240×240 NA 459.48 73.76 52.42\nBIRDHOUSE 480×480 NA NA 604.54 502.58\nHEAD480×480 NA NA 365.25 285.75\nSHOE 480×480 NA NA 756.77 364.42\nDOG480×480 NA NA 834.63 363.41\n21",
  "textLength": 65141
}