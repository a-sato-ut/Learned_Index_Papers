{
  "paperId": "92c472e58852967ee4a2ba911fe08de4670d4775",
  "title": "Automatically Finding Optimal Index Structure",
  "pdfPath": "92c472e58852967ee4a2ba911fe08de4670d4775.pdf",
  "text": "Automatically Finding Optimal Index Structure\nExtended Abstracts\nSupawit Chockchowwat, Wenjie Liu, Yongjoo Park\n{supawit2,wenjie3,yongjoo}@illinois.edu\nCreateLab @UIUC, USA\nABSTRACT\nExisting learned indexes (e.g., RMI, ALEX, PGM) optimize the inter-\nnal regressor of each node, not the overall structure such as index\nheight, the size of each layer, etc. In this paper, we share our re-\ncent findings that we can achieve significantly faster lookup\nspeed by optimizing the structure as well as internal regres-\nsors. Specifically, our approach (called AutoIndex) expresses the\nend-to-end lookup time as a novel objective function , and searches\nfor optimal design decisions using a purpose-built optimizer. In\nour experiments with state-of-the-art methods, AutoIndex achieves\n3.3Ã—â€“7.7Ã—faster lookup for the data stored on local SSD, and 1.4 Ã—â€“\n3.0Ã—faster lookup for the data on Azure Cloud Storage.\n1 INTRODUCTION\nIndexes enable fast lookup and are employed by many data systems\nfor fast search and analytics: search engines [ 19,40], key-value\nstores [ 5,25,37], RDBMS [ 4,29], etc. Conventional indexes (e.g.,\nB-tress [ 6,39,44], red-black trees [ 9,11,20], skip lists [ 46]) offer\nğ‘‚(logğ‘›)lookup speed for ğ‘›data items. Learned indexes [ 15,17,\n18,22,24,41,43] can offer even lower latencies based on more\ncompact representations [ 16]. To achieve high performance, indexes\nare typically optimized specifically for target storage media (e.g.,\nSSD [ 27], NVMe [ 42], memory [ 12], L1 cache [ 38]). For example, a\nB-tree node fits in a disk page (e.g., 4KB). RMI [ 24] has three layers\nwith the fast random-access assumption.\nProblem. The performance of these indexes is suboptimal when they\noperate in an environment different from what they are designed\nfor. We have observed this suboptimality (thus, missed opportunity)\nas we develop a cloud-optimized document store [ 13]. Also, see Â§ 2\nfor a concrete example with B-tree indexes.\nOpportunity & Challenge. We observe that we can greatly im-\nprove lookup speed by carefully choosing structural parameters\nsuch as the number of layers, layer sizes, the types and accuracy\nof internal regressors, etc. Our intuition is as follows. As depicted\nin Fig. 1, we should prefer shallow and wide indexes if a storage\ndevice has very long latency (round-trip time), because we want to\nreduce the number of I/O operations. In contrast, if the latency is\nvery small in comparison to bandwidth, tall indexes should be pre-\nferred. However, finding an optimal design is non-trivial in practice\nbecause there are exponentially many candidates (Â§ 3.2).\nThis paper is published under the Creative Commons Attribution 4.0 International\n(CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their\npersonal and corporate Web sites with the appropriate attribution, provided that you\nattribute the original work to the authors and AIDB 2022. 4th International Workshop\non Applied AI for Database Systems and Applications (AIDB â€™22). September 5th, 2022,\nSydney, Australia.Fast SlowSmallLarge\nTall &\nNarrowShallow\n& Wide\nLocal SSD\nCloud Storage\nLatency (RTT)Bandwidth\nFigure 1: Expected optimal structures by I/O characteristics.\nOur approach, AutoIndex, can automatically find optimal in-\ndex structures as well as internal regressors.\nOur Approach. We propose a new index builder (called AutoIndex)\nthat can learn the optimal structure in a principled manner by navi-\ngating in a high-dimensional design space. Specifically, we formulate\na novel optimization problem that expresses an end-to-end lookup\ntime in terms of core design parameters (e.g., the number of layers,\nthe size of each node, regressors, etc.) as well as I/O performance. By\nsolving the optimization problem using our highly parallel search\nmethod, we can build a low-latency index for large-scale data.\nNote that our approach is orthogonal to learned indexes [15,17,\n18,22,24,41,43], which propose the use of regressors for branching\nfunctions (instead of exact pointers). In contrast, this work focuses\non optimizing the overall index structure . This work is in line with\nthe recent advances at the intersection of data systems and machine\nlearning [7, 8, 23, 26, 28, 31â€“36, 45].\n2 MOTIVATION & SCOPE\n2.1 Motivation\nOur intuition is the following: the structure of an index is crucial\nfor its performance because depending on the system environment\nit operates on, the optimal index structure can vastly differ . For\nexample, optimal index structures can be different if we store data\non network-attached devices instead of local SSD, even if we index\nthe same dataset. We illustrate this with a concrete example.\nConcrete Example. Suppose two different B-tree structures: Tall\nandWide . (Note: our model formulation in Â§ 3 supports more than\nB-trees.) Tallconsists of 4 KB nodes, where each node has 200\nfanout. Wide consists of 100 KB nodes, where each node has 5,000\nfanout. Both TallandWide index the same dataset with one million\ndistinct keys. The dataset is stored in 4 KB pages.\nTo index the dataset, Tallneeds three layers (because the third\nlayer can hold up to 2003=8ğ‘€pointers). Likewise, Wide needs\ntwo layers (because the second layer can hold up to 50002=25ğ‘€arXiv:2208.03823v1  [cs.DB]  7 Aug 2022\n\nAIDBâ€™22, September 5th, 2022, Sydney, Australia Supawit Chockchowwat, Wenjie Liu, and Yongjoo Park\nÂ·Â·Â·\nÂ·Â·Â·Â·Â·Â·\nData Layer (1M records)\n(a) Tall & Narrow (w/ 200 fanout)Â·Â·Â·\nData Layer (1M records)\n(b) Shallow & Wide (w/ 5,000 fanout)Short RTT Long RTT0.511.52Query Latency (relative)Tall & Narrow\nShallow & Wide\n(c) Query Latency Comparison\nFigure 2: A motivating example for optimizing the entire index structure. Two candidate structures in (a) and (b). (a) has a\nlower branching factor (200); thus, it is quicker to retrieve each node while the index is higher. (b) has a greater branching\nfactor (5,000); thus, its height is lower while retrieving each node takes more time. Figure (c) shows that depending on system\nenvironments (i.e., Short RTT vs.Fast RTT ), different structures can show better performance (note: the latency of â€œHigh and\nNarrowâ€ is used as a unit latency). See the text for the simulation setup.\npointers). Note that while Wide is shallower than Tall, fetching\neach node of Wide takes longer because its page size is 25 Ã—larger.\nFigs. 2a and 2b depict these structures.\nInterestingly, neither of these two indexes (i.e., TallandWide )\nis superior to the other; that is, there is no single dominant index\nstructure that offers faster lookup speed in all different system\nenvironments. To illustrate this, we suppose two storage devices,\nShortRTT andLongRTT .ShortRTT operates with 5 ms latency (RTT)\nand 100 MB/sec bandwidth. LongRTT operates with 100 ms latency\n(RTT) and 100 MB/sec bandwidth. That is, their bandwidth is the\nsame, but their latencies are different. (Note that our formulation\nin Â§ 3 handles more general cases.)\nNow, we show that (i) Talloffers higher performance than Wide\nif we store data on ShortRTT , and that (ii) Wide offers higher per-\nformance than Tallif we store data on LongRTT . For this, we adopt\na widely used formula: (data transfer time) =(latency) + (data size)\n/ (bandwidth). For instance, fetching a single page of the dataset\nstored on ShortRTT takes 5 ms + 4 KB / (100 KB/ms) = 5.04 ms.\nEnvironmentâ€” ShortRTT :Wide is 24% slower than Tall.\nâ€¢Tallneeds 3 nodes and 1 data page = 3 Ã—(5ms + 4KB / (100KB/ms))\n+ (5ms + 4KB / (100KB/ms)) = 20.16 ms\nâ€¢Wide needs 2 nodes and 1 data page = 2 Ã—(5ms + 500KB /\n(100KB/ms)) + (5ms + 4KB / (100KB/ms)) = 25.04 ms\nEnvironmentâ€” LongRTT :Tallis 29% slower than Wide .\nâ€¢Tallneeds 3 nodes and 1 data page = 3 Ã—(100ms + 4KB /\n(100KB/ms)) + (100ms + 4KB / (100KB/ms)) = 400.16 ms\nâ€¢Wide needs 2 nodes and 1 data page = 2 Ã—(100ms + 500KB /\n(100KB/ms)) + (100ms + 4KB / (100KB/ms)) = 310.04 ms\nFig. 2c summarizes this relative performance strength. For each\nenvironment, the figure reports the relative difference in end-to-end\nlookup time. It proves that different index structures offer higher\nlookup performance, depending on the storage device.\nNeed for Efficient Algorithm. While we use a relatively simple\nexample to illustrate the core intuition, finding an optimal index\nstructure is non-trivial to solve by hand, because there are exponen-\ntially many configurations we need to consider. To efficiently find\nan optimal index, we develop an intelligent learning technique that\nconsiders the entire index building as an optimization problem.2.2 Scope of This Work\nRead-only Index. We focus on building read-only indexes on a\nsorted key-value dataset. There are two reasons. First, this is a\nnovel attempt at optimizing the entire index by casting it into a\nstatistical optimization problem. Thus, we aim to ensure at least we\ncan build best-in-class read-only indexes. Second, our long term\nplan is to integrate our index with write-oriented data systems\nbased on LSM trees [ 30]. Note that segments of an LSM tree are not\nupdatable; they are only merged occasionally to compose bigger\n(next level) segments. For these compaction operations, it suffices\nto build read-only indexes as part of compaction.\nPoint Lookup. We focus our evaluation on point lookupâ€”fetching\na value associated with a search key. There are two reasons. First,\nby optimizing point lookup, we also optimize range search because\nthey involve essentially the same internal operationsâ€”traversing\nnodes from top to bottom. Second, range search can easily be sup-\nported by extending point lookup. To retrieve values for a key range\n(begin, end) , we can find the offset for begin using point lookup,\nthen continue fetching data until we see end.\nPersistent Index. We focus on the index we persist on storage (e.g.,\nSSD, NVMe, flash drive), not the index maintained in main memory.\nThis is because we aim to enable faster lookup against large-\nscale data kept in persistent databases (e.g., RocksDB, PostgreSQL,\nSQLite, etc). For this reason, our cost analysis (for fetching data)\nmust incorporate storage-specific data transfer speed.\n3 INDEX AS STATISTICAL OPTIMIZATION\nWe build an index by solving a statistical optimization problem.\nSpecifically, our algorithm builds the fastest index for the data\nstored on a certain storage system, where the storage system is\nabstracted by the data transfer function ğ‘‡(ğ‘œ).ğ‘‡(ğ‘œ)is the time it takes\nto retrieve the data ğ‘œ(serialized on a storage device; see Â§ 3.3 for\nits usage). In the rest of this section, we formulate an optimization\nproblem and discuss how to solve it.\n3.1 Index Class\nAnindex class is a class of indexes we can express. The more general\nan index class is, the larger the set of indexes that we can represent;\nhowever, finding an optimal index can be more computationally\n\nAutomatically Finding Optimal Index Structure AIDBâ€™22, September 5th, 2022, Sydney, Australia\nData Layer (= key-value pairs )Regressor Layer 1\nRegressor Layer 2\nRegressor Layer 3Â·Â·Â·Â·Â·Â·\nÂ·Â·Â·Â·Â·Â·Opt 1: Model Type Opt 2: Index Height\nFigure 3: Index structure and optimization parameters.\nexpensive. Our index class generalizes hierarchical indexes\nsuch as B-trees [ 10,14] and learned indexes [ 24], as described below.\nAnindex consists of layers (see Fig. 3). There are two types of\nlayers: regressor layers andthe data layer . The data layer contains\nrecords sorted by keys. Each regressor layer consists of regressor(s).\nA regressor outputs a range (start, end) which is the data range\n(in byte offsets) we need to fetch in the next layer. If the next layer\nis a regressor layer, we fetch regressor(s); if the next layer is a data\nlayer, we fetch key-value pairs (including the search key).\nQuerying. First, we fetch the entire root layer (i.e., Regressor Layer\n1) containing multiple regressors. We use an appropriate regressor\n(for the given search key) to obtain an offset range for the next\nlayer. This step continues until we reach the data layer. Note that\nwe fetch the entire range at a time; that is, if there are ğ¿regressor\nlayers, we fetch ğ¿+1times until we obtain a desired key-value.\n3.2 Parameter Space\nAparameter tuple Î˜captures the design considerations for an index.\nThe goal of our statistical optimization is to find the optimal Î˜\n(=Î˜âˆ—) that minimizes the end-to-end lookup time â„“.Î˜consists of\nthe following parameters:\n(1)Numberğ¿of regressor layers ( ğ‘…1,...,ğ‘… ğ¿). (Note: For conve-\nnience, we use ğ‘…ğ¿+1for the data layer.)\n(2)Regressor Type ( ğ‘ª=ğ¶1,...,ğ¶ ğ¿): A regressor layer ğ‘…ğ‘™consists\nof regressor(s) of the same type (e.g., linear regression, step\nfunctions), which we denote by ğ¶ğ‘™. We search for the optimal\nregressor type for each regressor layer.\n(3)Precision ( ğ€=ğœ†1,...,ğœ† ğ¿) of regressor layers: A regressor out-\nputs a range. While we ensure the range contains an appropri-\nate regressor (in the next index layer) or a desired key-value\npair (in the data layer), we must tune the sizes of those ranges,\nwhich we achieve via ğ€.\nIn determining ğ€, there is a natural trade-off. That is, if we build\nan accurate regressor (with a fine ğœ†ğ‘–), its range output tends to be\nsmall, but the size of the regressor itself (and the regressor layer)\nmust be big (e.g., with more coefficients for higher accuracy).\nUsing Î˜=(ğ¿,ğ‘ª,ğ€), we express the end-to-end lookup latency\nin terms of Î˜as described in the next section.\n3.3 Objective Function for Latency\nWe express a (tight) upper-bound on the end-to-end lookup time â„“\nin terms of Î˜and the data transfer function, ğ‘‡(ğ‘œ). Specifically, â„“is\nexpressed in terms of actual regressor layers ( ğ‘…1,...,ğ‘… ğ¿) createdas specified by Î˜; here, the size of ğ‘…ğ‘™depends on ğ‘…ğ‘™+1as well as Î˜\n(becauseğ‘…ğ‘™is to obtain the data fetch range for ğ‘…ğ‘™+1).\nObjective Function. Letğ‘š(Â·)be a process for creating a regressor.\nThe created regressorâ€™s output range size is expressed by ğœ€.\nğ‘…ğ‘™Bğ‘š(ğ¶ğ‘™,ğœ†ğ‘™;ğ‘…ğ‘™+1)\nwhereğ‘…ğ‘™fetchesğœ€(ğ¶ğ‘™,ğœ†ğ‘™;ğ‘…ğ‘™+1)bytes forğ‘…ğ‘™+1\nFor a lookup, we first fetch the entire root layer ( ğ‘…1); then, we sub-\nsequently fetch every range output by the latest obtained regressor\nuntil the data layer. Thus, â„“can be expressed as:\nâ„“(ğ¿,ğ‘ª,ğ€)=ğ‘‡(ğ‘š(ğ¶1,ğœ†1;ğ‘…2))+ğ‘‡(ğœ€(ğ¶1,ğœ†ğ‘™;ğ‘…2))\n+ğ¿âˆ‘ï¸\nğ‘™=2ğ‘‡(ğœ€(ğ¶ğ‘™,ğœ†ğ‘™;ğ‘…ğ‘™+1))(1)\nThe goal is to find Î˜âˆ—=(ğ¿âˆ—,ğ‘ªâˆ—,ğ€âˆ—)at which the above expression\nis minimized. We present our parameter search algorithm in Â§ 3.4.\nDiscussion. Our objective function is an upper bound because\nsome data may be cached in memory; in these cases, no data transfer\nis needed. We empirically observed that our approach also delivers\nstrong performance in partially cached scenarios.\n3.4 Index Build via Parameter Search\nWe provide a high-level sketch of our parameter search algorithm.\nWe omit theoretical justifications and parallelization techniques.\nOur optimization is in the form of branch-and-bound algorithmsâ€”\nwebranch out to construct the bestğ‘…ğ‘™given (i) already constructed\nğ‘…ğ‘™+1,...,ğ‘… ğ¿+1and (ii) optimal upper layers (i.e.,ğ‘…1,...,ğ‘… ğ‘™âˆ’1); then,\nwebound by pruning candidates. Here, the optimal upper layers\nare obtained via a recursive call of our algorithm, and the bestğ‘…ğ‘™\nis evaluated using Eq. (1). There are three features that make our\napproach logically sound and efficient:\n(1)Principled search for ğ¿âˆ—: In designing our algorithm, one of the\ngreatest technical challenges is finding the optimal number\nof layers. Our algorithm does this by asking â€œdoes stacking\nanother layer reduce the total lookup time?â€ based on Eq. (1).\nThis approach makes our algorithm logically sound.\n(2)Effective enumeration of candidate Î˜: Our algorithm is not\namenable to gradient-descent-style algorithms for two rea-\nsons. First, the parameters for Î˜include discrete choices (i.e.,\nintegerğ¿and regressor type ğ‘ª). Second, the regressor cre-\nation process ğ‘š(Â·)is not always differentiable. Nevertheless,\nour search algorithm can find almost optimal Î˜by effectively\nnavigating in the discrete search space.\n(3)Highly parallel: While our algorithm examines many different\nÎ˜, our total search & build process is highly efficient (compara-\nble to existing learned indexes building) because our algorithm\nis designed to run in an embarrassingly parallel manner.\n4 EXPERIMENT\nWe share our latest experiment results. While we continue making\nfurther enhancements in our implementation, our current results\nare strong, with the following key points:\n(1)AutoIndexâ€™s learning algorithm exhibits expected behavior\nof finding optimal index structure for given system environ-\nments (Â§ 4.2).\n\nAIDBâ€™22, September 5th, 2022, Sydney, Australia Supawit Chockchowwat, Wenjie Liu, and Yongjoo Park\n1Âµs2Âµs4Âµs8Âµs16Âµs33Âµs66Âµs\n131Âµs\n262Âµs\n524Âµs1ms 2ms 4ms 8ms0246\nLatency (RTT) of Storage DeviceIndex Height\nFigure 4: AutoIndex can adapt. For greater RTTs, AutoIndex\nproperly creates shallower & wider indexes.\n(2)AutoIndexâ€™s lookup speed outperforms state-of-the-art meth-\nods on different storage devices (Â§ 4.3).\nWe present each point after explaining our experiment setup.\n4.1 Setup\nWe conduct experiments on Microsoft Azure. Our experiments use\na large-scale benchmark dataset.\nSystem. For VMs, we use Standard_D8s_v3 instances (16 vCPUs,\n64 GB memory). For Small RTT, we use OS disks (Premium SSD\nLRS, 256 GB, P20-2300 IOPS, 150 MBps). For Large RTT, we use\nAzure Cloud Storage (StorageV2) [2] connected via NFS v3 [3].\nCompared Methods. We compare AutoIndex against several state-\nof-the-art index libraries:\n(1)RMI: RMI [ 24] is one of the most commonly studied learned\nindex implementations. RMI comes with an accuracy knobâ€”\nour results compare RMI with the highest accuracy.\n(2)PGM : PGM-Index [ 17] is one of the latest learned indexes. It\noffers provable worst-case bounds.\n(3)LMDB : Lightning Memory-Mapped Database Manager ( LMDB )\nis one of the fastest open-source B-tree libraries. A benchmark\nreports that LMDB is orders-of-magnitude faster than com-\nmonly used indexing systems such as LevelDB and SQLite3 [ 1].\nDataset & Queries. We use the Facebook dataset included in â€œSOSD:\nA Benchmark for Learned Indexesâ€ [ 21]. This dataset has 200 mil-\nlion keys. For querying, we randomly pick a key from the dataset\nand measure the end-to-end time for retrieving the value associated\nwith the key (for each method we compare). To reduce variance,\nwe report the average over twenty iterations.\n4.2 AutoIndex Adapts\nFirst, we test if AutoIndex produces different (and properly opti-\nmized) index structures according to given system environments.\nWhile there are many internal parameters defining the structure of\nan index, we specifically examine the number of layers (or equiv-\nalently, index height) because its results are easier to understand\nthan other parameters (e.g., size of regressor layers, types of regres-\nsors). Expectation: As illustrated in Â§ 2, a shallow index should be\npreferred if the latency (RTT) is large. Actual: To see if our learn-\ning algorithm also exhibits such outcomes, we run AutoIndex for\ndifferent latency values while the bandwidth is fixed at 134 MB/sec.\nFig. 4 shows the results. When the latency is extremely small (i.e., 1\nmicrosecond), the algorithm tells us that a six-layered index delivers\nthe fastest lookup speed. The number of layers gradually decreasesLMDB RMI PGM AutoIndex0ms10ms20ms30msThe lower, the betterQuery Latency\n(a) Query Latency under Small RTT (Local Disk)\nLMDB RMI PGM AutoIndex0ms100ms200ms300ms\nThe lower, the betterQuery Latency\n(b) Query Latency under Large RTT (Azure Storage)\nFigure 5: AutoIndex (ours) delivers significantly faster query\nlatency compared to other state-of-the-art methods. This is\nbecause AutoIndex optimizes the entire index according to\ntarget system environments.\n(from 6 to 2) as we increase the latency, which is an expected result.\nWe compare the actual lookup speed below.\n4.3 AutoIndex is Fastest\nSecond, we show that AutoIndex outperforms the other indexes.\nOur comparison is performed using both local SSDs (small RTT)\nand Azure Cloud Storage (large RTT). Â§ 4.1 describes the details\nabout the methods we compare and also the dataset we use. Fig. 5\nsummarizes the results. Fig. 5a compares the lookup time for Small\nRTT (local disks). AutoIndex is the fastest: it is 3.3 Ã—â€“7.7Ã—faster\nthan other methods. Fig. 5b compares the lookup time for Large\nRTT (Azure Storage). Again, AutoIndex is the fastest: it is 1.4 Ã—â€“3.0Ã—\nfaster than other methods. This is because AutoIndex can optimize\nan entire index structure for target system environments.\n5 ONGOING WORK\nWe are enhancing AutoIndex in several orthogonal directions. First,\nwe are conducting additional theoretical analyses to formally study\nthe optimality of our parameter search process. Second, we are\nextending our optimization problem to incorporate possible vari-\nance in I/O performance. Third, we are improving our builder to\nstream-process very large data.\nACKNOWLEDGMENTS\nWe thank the creators/developers of RMI [ 24], PGM-Index [ 17],\nandLMDB [1], who open-sourced their software, which helped us\ngreatly in conducting accurate experiments. This work is supported\nin part by Microsoft Azure.\n\nAutomatically Finding Optimal Index Structure AIDBâ€™22, September 5th, 2022, Sydney, Australia\nREFERENCES\n[1] 2012. Database Microbenchmarks. http://www.lmdb.tech/bench/microbench/\n[2]Accessed: 2022-05-29. Azure Blob Storage. https://azure.microsoft.com/en-\nus/services/storage/blobs.\n[3]Accessed: 2022-05-29. Mount Blob storage by using the Network File System\n(NFS). https://docs.microsoft.com/en-us/azure/storage/blobs/network-file-\nsystem-protocol-support-how-to\n[4] Accessed: 2022-05-29. MySQL database service. https://www.mysql.com/.\n[5]Accessed: 2022-05-29. RocksDB: A persistent key-value store. https://rocksdb.\norg/.\n[6]Marcos K Aguilera, Wojciech Golab, and Mehul A Shah. 2008. A practical scalable\ndistributed b-tree. Proceedings of the VLDB Endowment 1, 1 (2008), 598â€“609.\n[7]Michael R Anderson, Dolan Antenucci, Victor Bittorf, Matthew Burgess, Michael J\nCafarella, Arun Kumar, Feng Niu, Yongjoo Park, Christopher RÃ©, and Ce Zhang.\n2013. Brainwash: A Data System for Feature Engineering.. In Cidr.\n[8]Johes Bater, Yongjoo Park, Xi He, Xiao Wang, and Jennie Rogers. 2020. Saqe:\npractical privacy-preserving approximate query processing for data federations.\nProceedings of the VLDB Endowment 13, 12 (2020), 2691â€“2705.\n[9]Rudolf Bayer. 1972. Symmetric binary B-trees: Data structure and maintenance\nalgorithms. Acta informatica 1, 4 (1972), 290â€“306.\n[10] R Bayer and EM McCreight. 1972. Organization and maintenance of large ordered\nindexes. Acta Informatica 1, 3 (1972), 173â€“189.\n[11] Juan Besa and Yadran Eterovic. 2013. A concurrent redâ€“black tree. J. Parallel\nand Distrib. Comput. 73, 4 (2013), 434â€“449.\n[12] Robert Binna, Eva Zangerle, Martin Pichl, GÃ¼nther Specht, and Viktor Leis.\n2018. Hot: A height optimized trie index for main-memory database systems. In\nProceedings of the 2018 International Conference on Management of Data . 521â€“534.\n[13] Supawit Chockchowwat, Chaitanya Sood, and Yongjoo Park. 2021. Airphant:\nCloud-oriented Document Indexing. arXiv preprint arXiv:2112.13323 (2021).\n[14] Douglas Comer. 1979. Ubiquitous B-tree. ACM Computing Surveys (CSUR) 11, 2\n(1979), 121â€“137.\n[15] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\net al.2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data . 969â€“984.\n[16] Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2021. On the per-\nformance of learned data structures. Theoretical Computer Science 871 (2021),\n107â€“120. https://doi.org/10.1016/j.tcs.2021.04.015\n[17] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proceedings of the\nVLDB Endowment 13, 8 (2020), 1162â€“1175.\n[18] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. Fiting-tree: A data-aware index structure. In Proceedings of the 2019\nInternational Conference on Management of Data . 1189â€“1206.\n[19] Clinton Gormley and Zachary Tong. 2015. Elasticsearch: the definitive guide: a\ndistributed real-time search and analytics engine . \" Oâ€™Reilly Media, Inc.\".\n[20] Leo J Guibas and Robert Sedgewick. 1978. A dichromatic framework for balanced\ntrees. In 19th Annual Symposium on Foundations of Computer Science (sfcs 1978) .\nIEEE, 8â€“21.\n[21] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2019. SOSD: A Benchmark for Learned\nIndexes. NeurIPS Workshop on Machine Learning for Systems (2019).\n[22] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management . 1â€“5.\n[23] Tim Kraska, Mohammad Alizadeh, Alex Beutel, H Chi, Ani Kristo, Guillaume\nLeclerc, Samuel Madden, Hongzi Mao, and Vikram Nathan. 2019. Sagedb: A\nlearned database system. In CIDR .\n[24] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 International\nConference on Management of Data . 489â€“504.\n[25] Leveldb. Accessed: 2022-05-29. LevelDB: A fast key-value storage library. https:\n//github.com/google/leveldb.\n[26] Guoliang Li, Xuanhe Zhou, Shifu Li, and Bo Gao. 2019. Qtune: A query-aware\ndatabase tuning system with deep reinforcement learning. Proceedings of the\nVLDB Endowment 12, 12 (2019), 2118â€“2130.\n[27] Yinan Li, Bingsheng He, Robin Jun Yang, Qiong Luo, and Ke Yi. 2010. Tree\nindexing on solid state drives. Proceedings of the VLDB Endowment 3, 1-2 (2010),\n1195â€“1206.\n[28] Ryan Marcus, Olga Papaemmanouil, Sofiya Semenova, and Solomon Garber. 2018.\nNashDB: an end-to-end economic method for elastic database fragmentation,\nreplication, and provisioning. In Proceedings of the 2018 International Conference\non Management of Data . 1253â€“1267.\n[29] Bruce Momjian. 2001. PostgreSQL: introduction and concepts . Vol. 192. Addison-\nWesley New York.[30] Patrick Oâ€™Neil, Edward Cheng, Dieter Gawlick, and Elizabeth Oâ€™Neil. 1996. The\nlog-structured merge-tree (LSM-tree). Acta Informatica 33, 4 (1996), 351â€“385.\n[31] Yongjoo Park. 2017. Active Database Learning.. In CIDR .\n[32] Yongjoo Park, Michael Cafarella, and Barzan Mozafari. 2016. Visualization-aware\nsampling for very large databases. In 2016 ieee 32nd international conference on\ndata engineering (icde) . IEEE, 755â€“766.\n[33] Yongjoo Park, Barzan Mozafari, Joseph Sorenson, and Junhao Wang. 2018. Ver-\ndictdb: Universalizing approximate query processing. In Proceedings of the 2018\nInternational Conference on Management of Data . 1461â€“1476.\n[34] Yongjoo Park, Jingyi Qing, Xiaoyang Shen, and Barzan Mozafari. 2019. BlinkML:\nEfficient maximum likelihood estimation with probabilistic guarantees. In Pro-\nceedings of the 2019 International Conference on Management of Data . 1135â€“1152.\n[35] Yongjoo Park, Ahmad Shahab Tajik, Michael Cafarella, and Barzan Mozafari.\n2017. Database learning: Toward a database that becomes smarter every time.\nInProceedings of the 2017 ACM International Conference on Management of Data .\n587â€“602.\n[36] Yongjoo Park, Shucheng Zhong, and Barzan Mozafari. 2020. Quicksel: Quick\nselectivity learning with mixture models. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data . 1017â€“1033.\n[37] Pandian Raju, Rohan Kadekodi, Vijay Chidambaram, and Ittai Abraham. 2017.\nPebblesdb: Building key-value stores using fragmented log-structured merge\ntrees. In Proceedings of the 26th Symposium on Operating Systems Principles . 497â€“\n514.\n[38] Jun Rao and Kenneth A Ross. 2000. Making B+-trees cache conscious in main\nmemory. In Proceedings of the 2000 ACM SIGMOD international conference on\nManagement of data . 475â€“486.\n[39] Benjamin Sowell, Wojciech Golab, and Mehul A Shah. 2012. Minuet: A Scalable\nDistributed Multiversion B-Tree. Proceedings of the VLDB Endowment (2012).\n[40] Splunk. Accessed: 2022-05-29. Splunk: The Data-to-EverythingTM Platform.\nhttps://www.splunk.com/\n[41] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: a scalable learned index for multicore data\nstorage. In Proceedings of the 25th ACM SIGPLAN Symposium on Principles and\nPractice of Parallel Programming . 308â€“320.\n[42] Li Wang, Zining Zhang, Bingsheng He, and Zhenjie Zhang. 2020. PA-Tree:\nPolled-Mode Asynchronous B+ Tree for NVMe. In 2020 IEEE 36th International\nConference on Data Engineering (ICDE) . IEEE, 553â€“564.\n[43] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. 2020. SIndex: a\nscalable learned index for string keys. In Proceedings of the 11th ACM SIGOPS\nAsia-Pacific Workshop on Systems . 17â€“24.\n[44] Sai Wu, Dawei Jiang, Beng Chin Ooi, and Kun-Lung Wu. 2010. Efficient B-tree\nbased indexing for cloud data processing. Proceedings of the VLDB Endowment 3,\n1-2 (2010), 1207â€“1218.\n[45] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi\nChen, Pieter Abbeel, Joseph M. Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019.\nDeep Unsupervised Cardinality Estimation. Proceedings of the VLDB Endowment\n(2019).\n[46] Jingtian Zhang, Sai Wu, Zeyuan Tan, Gang Chen, Zhushi Cheng, Wei Cao, Yusong\nGao, and Xiaojie Feng. 2019. S3: a scalable in-memory skip-list index for key-value\nstore. Proceedings of the VLDB Endowment 12, 12 (2019), 2183â€“2194.",
  "textLength": 29401
}