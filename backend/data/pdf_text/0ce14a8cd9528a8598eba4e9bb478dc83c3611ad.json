{
  "paperId": "0ce14a8cd9528a8598eba4e9bb478dc83c3611ad",
  "title": "Just-in-Time Index Compilation",
  "pdfPath": "0ce14a8cd9528a8598eba4e9bb478dc83c3611ad.pdf",
  "text": "Just-in-Time Index Compilation\nDarshana Balakrishnan, Lukasz Ziarek, Oliver Kennedy\nUniversity at Bu\u000balo\nfdbalakri, lziarek, okennedy g@buffalo.edu\nAbstract\nCreating or modifying a primary index is a time-\nconsuming process, as the index typically needs to\nbe rebuilt from scratch. In this paper, we explore a\nmore graceful \\just-in-time\" approach to index reor-\nganization, where small changes are dynamically ap-\nplied in the background. To enable this type of reor-\nganization, we formalize a composable organizational\ngrammar, expressive enough to capture instances of\nnot only existing index structures, but arbitrary hy-\nbrids as well. We introduce an algebra of rewrite rules\nfor such structures, and a framework for de\fning and\noptimizing policies for just-in-time rewriting. Our\nexperimental analysis shows that the resulting index\nstructure is \rexible enough to adapt to a variety of\nperformance goals, while also remaining competitive\nwith existing structures like the C++ standard tem-\nplate library map.\n1 Introduction\nAn in-memory index is backed by a data structure\nthat stores and facilitates access to records. An al-\nphabet soup of such data structures have been devel-\noped to date ([10, 6, 7, 20, 30, 19, 22, 26, 21, 12] to list\nonly a few). Each structure targets a speci\fc trade-\no\u000b between a range of performance metrics (e.g., read\ncost, write cost), resource constraints (e.g., memory,\ncache), and supported functionality (e.g. range scans\nor out-of-core storage). As a trivial example, contrast\nlinked lists with a sorted arrays: The former provides\nfast writes and slow lookups, while the latter does ex-\nactly the opposite.\nTime Spent OrganizingAccess LatencyR/B Tree FasterR/B Tree\nJITD FasterJITDFigure 1: A classical index data structure provides no\nbene\fts until ready, while JITD s provide continuous\nincremental performance improvements.\nCreating or modifying an in-memory index is a\ntime-consuming process, since the data structure\nbacking the index typically needs to be rebuilt from\nscratch when its parameters change. During this\ntime, the index is unusable, penalizing the perfor-\nmance of any database relying on it. In this paper, we\npropose a more graceful approach to runtime index\nadaptation. Just-in-Time Indexes ( JITD s) continu-\nously make small, incremental reorganizations in the\nbackground, while client threads continue to access\nthe structure. Each reorganization brings the JITD\ncloser to a state that mimicks a speci\fc target data\nstructure. As illustrated in Figure 1, the performance\nof aJITD continuously improves as it transitions from\none state to another, while other data structures im-\nprove only after \fxed investments of organizational\ne\u000bort.\nThree core challenges must be addressed to realize\nJITD s. First, because each individual step is small,\nat any given point in time an JITD may need to be in\nsome intermediate state between two classical data\nstructures. For example, an JITD transitioning from\na linked list to a binary tree may need to occupy a\n1arXiv:1901.07627v1  [cs.DB]  22 Jan 2019\n\nstate that is neither linked list, nor binary tree, but\nsome combination of the two. Second, there may be\nmultiple pathways to transition from a given source\nstate to the desired target state. For example, to get\nfrom an unsorted array to a sorted array, we might\nsort the array (faster in the long-term) or crack [12]\nthe array (more short-term bene\fts). Finally, we\nwant to avoid blocking client access to the JITD while\nit is being reorganized. Client threads should be able\nto query the structure while the background thread\nworks.\nWe address the \frst challenge by building on prior\nwork with JITD s [17], where we de\fned them as a\nform of adaptive index that dynamically assembles\nindexes from composable, immutable building blocks.\nMimicking the behavior of a just-in-time compiler, a\njust-in-time data structure dynamically reorganizes\nbuilding blocks to improve index performance. Our\nmain contributions in this paper address the remain-\ning challenges.\nWe \frst precisely characterize the space of available\nstate transitions by formalizing the behavior of JITD s\ninto a composable organizational grammar ( cog). A\nsentence in cog corresponds directly to a speci\fc\nphysical layout. Many classical data structures like\nbinary trees, linked lists, and arrays are simply syn-\ntactic restrictions on cog. Lifting these restrictions\nallows intermediate hybrid structures that combine\nelements of each. Thus, the grammar can precisely\ncharacterize any possible state of a JITD .\nNext, we de\fne transforms , syntactic rewrite rules\novercog and show how these rewrite rules can be\ncombined into a policy that dictate how and where\ntransforms should be applied. This choice generally\nrequires runtime decisions, so we identify a speci\fc\nfamily of \\local hierarchical\" policies in which run-\ntime decisions can be implemented by an e\u000eciently\nmaintainable priority heap. As an example, we de\fne\na family of policies for transitioning between unsorted\nand sorted arrays (e.g., for interactive analysis on a\ndata \fle that has just been loaded [1]).\nTo automate policy design, we provide a simulator\nframework that predictively models the performance\nof a JITD under a given policy. The simulator can\ngenerate performance-over-time curves for a set of\npotential policies. These curves can then be queried\n⊎Concat\nBinTreeArray\nSorted\nAn Arbitrary\nSubtreeFigure 2: Node types in a JITD\nto \fnd a policy that best satis\fes user desiderata like\n\\get to 300 mslookups as soon as possible\" or \\give\nme the best scan performance possible within 5 s\".\nFinally, we address the issue of concurrency by\nproposing a new form of \\semi-functional\" data\nstructure. Like a functional (immutable) data struc-\nture, elements of a semi-functional data structure are\nstable once created. However, using handle-style [11]\npointer indirection, we draw a clear distinction be-\ntween code that expects physical stability and code\nthat merely expects logical stability. In the latter\ncase correctness is preserved even if the element is\nmodi\fed, so long as the element's logical content re-\nmains unchanged.\n1.1 System Overview\nA Self-Adjusting Index ( JITD ) is a key-value style pri-\nmary (clustered) index storing a collection of records,\neach (non-uniquely) identi\fed by a key with a well\nde\fned sort order. As illustrated in Figure 3, a JITD\nconsists of three parts: an index, an optimizer, and\na policy simulator. The JITD 's index is a tree rooted\nat a node designated root . Following just-in-time\ndata structures, JITD s use four types of nodes, sum-\nmarized in Figure 2: (1) Array : A leaf node stor-\ning an unsorted array of records, (2) Sorted : A\nleaf node storing a sorted array of records, (3) Con-\ncat: An inner node pointing to two additional nodes,\nand (4) BinTree : A binary tree node that segments\nrecords in the two nodes it points to by a separator\nvalue.\nThe second component of JITD is a just-in-time\noptimizer, an asynchronous process that incremen-\ntally reorganizes the index, progressively rewriting its\ncomponent parts to adapt it to the currently running\nworkload. These rewrites are guided by a policy , a\n2\n\nClients\nClients\nScheduler\nClient Threads\nAccesses\nIndex\nJust-in-Time Optimizer\nPolicy Optimizer\nWorkerFigure 3: A JITD\nset of rules for identifying points in the index to be\nrewritten and for determining what rewrites to ap-\nply. To help users to select an appropriate policy,\nJITD includes a policy simulator that generates pre-\ndicted performance over time curves for speci\fc poli-\ncies. This simulator can be used to quickly compare\npolicies, helping users to select the policy that best\nmeets the user's requirements for latency, preparation\ntime, or throughput.\n1.2 Access Paths\nAJITD provides lock-free access to its contents\nthrough access paths that recursively traverse the in-\ndex: (1) get(key) returns the \frst record with a tar-\nget key, (2) iterator(lower) returns an un-ordered\niterator over records with keys greater than or equal\ntolower , and (3) ordered iterator(lower) returns\nan iterator over the same records, but in key order.\nAs an example, Algorithm 1 implements the \frst of\nthese access paths by recursively descending through\nthe index. Semantic constraints on the layout pro-\nvided by Sorted andBinTree are exploited where\nthey are available.\n1.3 Updates\nOrganizational e\u000bort in a JITD is entirely o\u000foaded to\nthe just-in-time optimizer. Client threads performing\nupdates do the minimum work possible to register\ntheir changes. To insert, the updating thread instan-\ntiates a new Array nodeCand creates a subtree\nlinking it and the current index root:\nConcat (root;C)\nThis subtree becomes a new version of the root. Al-\nthough only one thread may update the index atAlgorithm 1 Get(C,k)\nRequire:C: AJITD node k: A key\nEnsure:r: A record with key korNone if none\nexist.\nifCmatches Array (~ r)then\nreturn linearScan (k;~ r)\nelse ifCmatches Sorted (~ r)then\nreturn binarySearch (k;~ r)\nelse ifCmatches Concat (C1;C2)then\nr=Get(C1, k)\nifr6=None then return r\nelse return Get(C2, k)\nelse ifCmatches BinTree (k0;C1;C2)then\nifk0\u0016kthen return Get(C2, k)\nelse return Get(C1, k)\na time, updates can proceed concurrently with the\nbackground worker thread. This is achieved through\na layer of indirection called a handle that we intro-\nduce and discuss further in Section 5.\n1.4 Organization and Policy\nThe background worker thread is responsible for iter-\natively rewriting fragments of the index into (hope-\nfully) more e\u000ecient forms. It needs (1) to identify\nfragments of the structure that need to be rewrit-\nten, (2) to decide how to rewrite those fragments,\nand (3) to decide how to prioritize these tasks. We\naddress the \frst two challenges by de\fning a \fxed\nset of transformations for JITD . Like rewrite rules in\nan optimizing compiler, transformations replace sub-\ntrees of the grammar with logically equivalent struc-\ntures. Following this line of thought, we \frst develop\na formalism that treats the state of the index at any\npoint in time as a sentence in a grammar over the\nfour node types. We show that transformations can\nbe expressed as structural rewrites over this gram-\nmar, and that for any sentence (i.e., index instance)\nwe can enumerate the sentence fragments to which a\ntransformation can be successfully applied. A policy\nthat balances the trade-o\u000bs between di\u000berent types\nof transformations is then de\fned to prioritize which\ntransforms should be applied and when.\n3\n\n1.5 Paper Outline\nThe remainder of this paper is organized as follows.\nEncoding hybrid index structures . In Sec-\ntion 2, we introduce and formalize the cog grammar\nand show how it allows us to encode a wide range of\ntree-structured physical data layouts. These include\nrestricted sub-grammars that capture, for example,\nsingly linked lists or binary trees. The grammar can\nexpress transitional physical layouts that combine el-\nements of multiple classes of data structure.\nData structure transitions as an algebra . We\nnext outline an algebra over the cog grammar in\nSection 3. Speci\fcally we introduce the concept of\ntransforms, rewrites on the structure of a sentence in\ncog that preserve logical equivalence and syntactic\nconstraints over the structure.\nCombining transforms into a policy . Next,\nin Section 4, we show how sequences of transforms,\nguided by a policy, may be used to incrementally re-\norganize an index. In order to remain competitive\nwith classical index structures, policies need to make\nsplit-second decisions on which transforms to apply.\nAccordingly, we identify a speci\fc class of local hier-\narchical policies that can be implemented via an in-\ncrementally maintained priority queue that tracks or-\nganizational goals and e\u000eciently selects transforms.\nImplementation and runtime . After providing\na theoretical basis for JITD s, we describe how we ad-\ndressed key challenges in implementing them, outline\nthe primary components of the JITD runtime, and\nprovide an illustrative example policy: Crack-or-Sort.\nPolicy optimization . Section 6 introduces a JITD\nsimulator. This simulator emulates the evolution of\naJITD , allowing us to e\u000eciently determine which\nof a range of alternative policies best meets user-\nprovided performance goals for transitioning between\nindex structures.\nAssessing JITD's generality . Section 7 uses a\ntaxonomy of index data structures proposed in [16]\nto evaluate JITD 's generality. We propose three ideas\nfor future work that could fully generalize 19 of the\n22 design dimensions identi\fed.Evaluation . Finally, in Section 8, we assess the\nperformance overheads JITD s, relative to both com-\nmonly used and state-of-the art in-memory indexes.\n2 A Grammar of Data Struc-\ntures\nEach record r2Ris accessed exclusively by a (poten-\ntially non-unique) identi\fer id(r)2I. We assume\na total order\u0016is de\fned over elements of I. We\nabuse syntax and use records and keys interchange-\nably with respect to the order, writing r\u0016kto mean\nid(r)\u0016k. We write [ \u001c],f\u001cg, andfj\u001cjgto denote the\ntype of arrays, sets, and bags (respectively) with el-\nements of type \u001c. We write [ r1;:::;rN] (resp.,f:::g,\nfj:::jg) to denote an array (or set or bag) with ele-\nmentsr1;:::;rN.\nTo support incremental index transitions, we need\na way to represent intermediate states of an index,\npart way between one physical layout and another.\nIn this section we propose a compositional organiza-\ntional grammar ( cog) that will allow us to reason\nabout the state of a JITD , and the correctness of its\nstate transitions.\n2.1 Notation and De\fnitions\nThe atoms of cog are de\fned by four symbols\nArray ,Sorted ,Concat ,BinTree . Acog instance\nis a sentence in cog, de\fned by the grammar Cas\nfollows:\nC=Array ([R])jSorted ([R])\njConcat (C;C)jBinTree (I;C;C)\nAtoms in cog map directly to physical building\nblocks of a data structure, while atom instances cor-\nrespond to instances of a data structure or one of its\nsub-structures. For example an instance of Array\nrepresents an array of records laid out contiguously\nin memory, while Concat represents a tuple of point-\ners referencing other instances. We write typeof (C)\nto denote the atom symbol at the root of an instance\nC2C.\n4\n\nExample 1 (Linked List) .A linked list may be de-\n\fned as a syntactic restriction over cog as follows\nLL =Concat (Array ([R]);LL)jArray ([R])\nA linked list is either a concatenation of an array\n(with one element by convention), and a pointer to\nthe next element, or a terminal array (with no ele-\nments by convention).\nTwo di\u000berent instances, corresponding to di\u000berent\nrepresentations may still encode the same data. We\ndescribe the logical contents of an instance Cas a\nbag, denoted by D(C), and use this term to de\fne\nlogical equivalence between two instances.\nD(C) =8\n>>><\n>>>:fjr1;:::;r NjgifC=Array ([r1;:::;r N])\nfjr1;:::;r NjgifC=Sorted ([r1;:::;r N])\nD(C1)]D(C2)ifC=Concat (C1;C2)\nD(C1)]D(C2)ifC=BinTree (;C1;C2)\nDe\fnition 1 (Logical Equivalence) .Two instances\nC1andC2arelogically equivalent if and only if\nD(C1) =D(C2). To denote logical equivalence we\nwriteC1\u0019C2.\nWe writeC\u0003to denote the bag consisting of the\ninstanceCand its descendants.\nC\u0003=8\n><\n>:C\u0003\n1]C\u0003\n2]fjCjgifC=Concat (C1;C2)\nC\u0003\n1]C\u0003\n2]fjCjgifC=BinTree (;C1;C2)\nfjCjg otherwise\nProposition 1. The setC\u0003is \fnite for any C.\n2.2 cog Semantics\nArray andConcat represent the physical layout of\nelements of a data structure. The remaining two\natoms provide provide semantic constraints (using\nthe identi\fer order \u0016) over the physical layout that\ncan be exploited to make the structure more e\u000ecient\nto query. We say that instances satisfying these con-\nstraints are structurally correct .\nDe\fnition 2 (Structural Correctness) .We de\fne\nthe structural correctness of an instance C2C (de-\nnoted by the unary relation StrCor (C)) for each\natom individually:Case 1 .Array instance is structurally correct.\nCase 2 . The instance Concat (C1;C2)is struc-\nturally correct if and only if C1andC2are\nboth structurally correct.\nCase 3 . The instance Sorted ([r1;:::;rN])is struc-\nturally correct if and only if 80\u0014i < j\u0014\nN:ri\u0016rj\nCase 4 . The instance BinTree (k;C 1;C2)is struc-\nturally correct if and only if both C1andC2\nare structurally correct, and if 8r12D(C1) :\nr1\u001ekandr22D(C2) :k\u0016r2.\nIn short, Sorted is structurally correct if it rep-\nresents a sorted array. Similarly, BinTree is struc-\nturally correct if it corresponds to a binary tree node,\nwith its children partitioned by its identi\fer. Both\nConcat andBinTree additionally require that their\nchildren be structurally correct.\nExample 2 (Binary Tree) .A binary tree may be\nde\fned as a syntactic restriction over cog as follows\nB=BinTree (I;B;B)jArray ([R])\nA binary tree is a hierarchy of BinTree inner nodes,\noverArray leaf nodes (containing one element by\nconvention).\n3 Transforms over cog\nWe next formalize state transitions in a JITD through\npattern-matching rewrite rules over cog called trans-\nforms.\nDe\fnition 3 (Transform) .We de\fne a transform\nTas any member of the family Tof endomorphisms\novercog instances. Equivalently, any transform T2\nTis a morphism T:C!C from instance to instance.\nFigure 4 illustrates a range of common trans-\nforms that correspond to common operations on in-\ndex structures. For consistency, we de\fne transforms\nover all instances and not just instances where the\noperation \\makes sense.\" On other instances, trans-\nforms behave as the identity ( id(C) =C).\n5\n\nUnSort (C) =(\nArray (~ r)ifC=Sorted (~ r)\nC otherwiseSort (C) =(\nSorted (sort(~ r))ifC=Array (~ r)\nC otherwise\nDivide (C) =(\nConcat (Array (h\nr1:::rbN\n2ci\n);Array (h\nrbN\n2c+1:::rNi\n))ifC=Array ([r1:::rN])\nC otherwise\nCrack (C) =(\nBinTree (id\u0010\nrbN\n2c\u0011\n;Array (h\nri\f\fri\u001erbN\n2ci\n);Array (h\nri\f\frbN\n2c\u0016rii\n)ifC=Array ([r1:::rN]))\nC otherwise\nMerge (C) =8\n><\n>:Array ([r1:::rN;rN+1:::rM])ifC=Concat (Array ([r1:::rN]);Array ([rN+1:::rM]))\nArray ([r1:::rN;rN+1:::rM])ifC=BinTree (;Array ([r1:::rN]);Array ([rN+1:::rM]))\nC otherwise\nPivotLeft (C) =8\n><\n>:Concat (Concat (C1;C2);C3) ifC=Concat (C1;Concat (C2;C3))\nBinTree (k2;BinTree (k1;C1;C2);C3)ifC=k1\u001ek2and BinTree (k1;C1;BinTree (k2;C2;C3))\nC otherwise\nFigure 4: Examples of correct transforms. Sort andUnSort convert between Array andSorted and visa\nversa. Crack andDivide both fragment Array s, and both are reverted by Merge .Crack in particular\nuses an arbitrary array element to partition its input value (theN\n2th element in this example), analogous\nto the RadixCrack operation of [15]. PivotLeft rotates tree structures counterclockwise and a symmetric\nPivotRight may also be de\fned. The function sort : [R]![R] returns a transposition of its input sorted\naccording to\u0016.\nClearly not all possible transforms are useful for\norganizing data. For example, the well de\fned, but\nrather unhelpful transform Empty (C) =Array ([])\ntransforms any cog instance into an empty array.\nTo capture this notion of a \\useful\" transform, we\nde\fne two correctness properties: structure preser-\nvation and equivalence preservation.\nDe\fnition 4 (Equivalence Preserving Transforms) .\nA transform Tis de\fned to be equivalence preserving\nif and only if8C:C\u0019T(C)(De\fnition 1).\nDe\fnition 5 (Structure Preserving Transforms) .A\ntransformTis de\fned to be structure preserving if\nand only if8C:StrCor (C) =)StrCor (T(C))\n(De\fnition 2).\nA transform is equivalence preserving if it preserves\nthe logical content of the instance. It is structure pre-\nserving if it preserves the structure's semantic con-\nstraints (e.g., the record ordering constraint on in-\nstances of the Sorted atom). If it is both, we say\nthat the transform is correct.De\fnition 6 (Correct Transform) .We de\fne a\ntransformTto be correct (denoted Correct (T)) if\nTis both structure and equivalence preserving.\nIn Appendix A we give proofs of correctness for\neach of the transforms in Figure 4.\n3.1 Meta Transforms\nTransforms such as those illustrated in Figure 4\nform the atomic building blocks of a policy for re-\norganizing data structures. For the purposes of this\npaper, we refer to these six transforms, together with\nPivotRight and the identity transform id, collec-\ntively as the atomic transforms , denotedA. We next\nintroduce a framework for constructing more complex\ntransforms from these building blocks.\nDe\fnition 7 (Composition) .For any two trans-\nformsT1;T22 T , we denote by T1\u000eT2the com-\nposition of T1andT2:\n(T1\u000eT2)(C)def=T2(T1(C))\n6\n\nTransform composition allows us to build more\ncomplex transforms from the set of atomic trans-\nforms. We also consider meta transforms that ma-\nnipulate transform behavior.\nDe\fnition 8 (Meta Transform) .A meta transform\nMis any correctness-preserving endofunctor over the\nset of transforms. That is, any functor M:T !\nTis a meta transform if and only if 8T2 T :\nCorrect (T) =)Correct (M[T])(De\fnition 6).\nWe are speci\fcally interested in two meta\ntransforms that will allow us to apply\ntransforms not just to the root of an in-\nstance, but to any of its descendants as well.\nLHS [T](C) =8\n><\n>:Concat (T(C1);C2) ifC=Concat (C1;C2)\nBinTree (k;T(C1);C2)ifC=BinTree (k;C 1;C2)\nC otherwise\nRHS [T](C) =8\n><\n>:Concat (C1;T(C2)) ifC=Concat (C1;C2)\nBinTree (k;C 1;T(C2))ifC=BinTree (k;C 1;C2)\nC otherwise\nTheorem 1 (LHS andRHS are meta transforms) .\nLHS andRHS are correctness-preserving endofunc-\ntors overT.\nThe proof, given in Appendix B, is a simple struc-\ntural recursion over cases.\nWe refer to the closure of LHS andRHS over the\natomic transforms as the set of hierarchical trans-\nforms , denoted \u0001.\n\u0001 =A[f LHS [T]jT2\u0001g[fRHS [T]jT2\u0001g\nCorrolary 1. Any hierarchical transform is correct.\n4 Policies for Transforms\nTransforms give us a means of manipulating in-\nstances, but to actually allow an index to transition\nfrom one form to another we need a set of rules,\ncalled a policy , to dictate which transform to apply\nand when. We begin by de\fning policies broadly, be-\nfore re\fning them into an e\u000eciently implementable\nfamily of enumerable score-based policies .\nDe\fnition 9 (Policy) .A policyPis de\fned by the 2-\ntupleP=hD;Hi, where the policy's domainD\u0012Tis a set of transforms and H:C!D is a heuristic\nfunction that selects one of these transforms to apply\nto a given instance.\nA policy guides the transition of an index from an\ninstance representing its initial state to a \fnal state\nachieved by repeatedly applying transforms selected\nby the heuristicH. We call the sequence of instances\nreached in this way a trace.\nDe\fnition 10 (Trace) .The trace of a policy P=\nhD;Hion instance C0, denoted Trace (P;C0), is de-\n\fned as the in\fnite sequence of instances [C0;C1;:::]\nstarting with C0, and with subsequent instances Ci\nobtained as:\nCidef=Ti(Ci\u00001)whereTi=H(Ci\u00001)\nAlthough traces are in\fnite, we are speci\fcally in-\nterested in policies with traces that reach a steady\n(\fxed point) state. We say that such a trace (resp.,\nany policy guaranteed to produce such a trace) is ter-\nminating .\nDe\fnition 11 (Terminating Trace, Policy) .A trace\n[C1;C2;:::] terminates afterNsteps if and only if\n8i;j > N :Ci=Cj;. A policyPisterminating\nwhen\n8C9N:Trace (P;C)terminates after Nsteps\nA policy's domain may be large, or even in\fnite as\nin the case of the hierarchical transforms. However,\nonly a much smaller fragment will typically be useful\nfor any speci\fc instance. We call this fragment the\nactive domain .\nDe\fnition 12. The active domain of a policy\nhD;Hi, relative to an instance C(denotedDC) is\nthe subset of the policy's domain that does not behave\nas the identity on C.\nDCdef=fTjT2D^T(C)6=Cg\n4.1 Bounding the Active Domain\nA policy's heuristic function will be called numer-\nous times in the course of an index transition, mak-\n7\n\ning it a prime candidate for performance optimiza-\ntion. We next explore one particular family of poli-\ncies that admit a stateful, incremental implementa-\ntion of their heuristic function. This approach treats\nthe heuristic function as a ranking query over the\nactive domain, selecting the most appropriate (high-\nest scoring) transform at any given time. However,\nrather than recomputing scores at every step, we in-\ncrementally maintain a priority queue over the active\ndomain. For this incremental approach to be feasi-\nble, we need to ensure that only a \fnite (and ideally\nsmall) number of scores change with each step.\nDe\fnition 13 (Enumerable Policy) .A policy\nhD;Hi is enumerable if and only if its active do-\nmain is \fnite for every \fnite instance C, or equiva-\nlently when8C:jDCj2N\nWe are particularly interested in policies that use\nthe hierarchical transforms as their domain. We also\nrefer to such policies as hierarchical . In order to show\nthat hierarchical policies are enumerable, we \frst de-\n\fne a utility target function that \\unrolls\" an arbi-\ntrarily deep stack of LHS andRHS meta transforms.\nThe target function returns (1) The atomic transform\nat the base of the stack of meta transforms and (2)\nthe descendant that this atomic transform would be\napplied to.\nDe\fnition 14. Given a hierarchical policy h\u0001;Hi\nand an instance C, let the target function f\u0003\nC:DC!\n(C\u0003\u0002A)of the policy on Cis de\fned as follows\nf\u0003\nC(T)def=8\n>>><\n>>>:hC;Tiif typeof (C)2fArray;Sortedg\nhC;Tielse ifT2A\nf\u0003\nC1(T0)else ifT=LHS [T0]\nf\u0003\nC2(T0)else ifT=RHS [T0]\nLemma 1 (Injectivity of f\u0003\nC).The target function f\u0003\nC\nof any hierarchical policy h\u0001;Hifor any instance C\nis injective.\nProof. By recursion over C. The base case oc-\ncurs when typeof (C)2fArray;Sortedg. In this\ncaseC\u0003=fjCjg. Furthermore, 8T:LHS [T] =\nRHS [T] =idand soDC\u0012 A. The target func-\ntion always follows its \frst case and is trivially in-\njective. The \frst recursive case occurs for C=Concat (C1;C2). By de\fnition, a hierarchical trans-\nform can be (1) An atomic transform, (2) LHS [T],\nor (3) RHS [T]. Each of the latter three cases\ncovers one of each of the three parts of the de\f-\nnition of a hierarchical transform. Assuming that\nf\u0003\nC1andf\u0003\nC2are injective, f\u0003\nCwill also be injective\nbecause each case maps to a disjoint partition of\nC\u0003=C\u0003\n1]C\u0003\n2]fjCjg. The proof for the second\nrecursive case, where typeof (C) =BinTree is iden-\ntical. Thus8C:f\u0003\nCis injective\nUsing injectivity of the target function, we can\nshow that any hierarchical policy is enumerable.\nTheorem 2 (Hierarchical policies are enumerable) .\nAny hierarchical policy h\u0001;Hiis enumerable.\nProof. Recall the de\fnition of hierarchical transforms\n\u0001 =A[f LHS [T]jT2\u0001g[fRHS [T]jT2\u0001g\nBy Lemma 1,jDCj\u0014jC\u0003\u0002Aj\u0014jC\u0003j\u0002jAj . By\nProposition 1, C\u0003is \fnite and the set of atomic trans-\nformsAis \fnite by de\fnition Thus, DCmust also be\n\fnite.\nIntuitively, there is a \fnite number of atomic trans-\nforms (jA), that can be applied at a \fnite set of po-\nsitions within C(jC\u0003). Any other hierarchical trans-\nform must be idempotent, so we can (very loosely)\nbound the active domain of a hierarchical policy on\ninstanceCbyjC\u0003j\u0002jAj\n4.2 Scoring Heuristics\nAs previously noted, we are particularly interested\nin policies that work by scoring the set of available\ntransforms with respect to their utility.\nDe\fnition 15 (Scoring Policy) .Letscore : (D\u0002\nC)!N0be a scoring function for every transform,\ninstance pair ( T;C) that satis\fes the constraint:\n(T(C) =C))(score (T;C) = 0) Ascoring pol-\nicyhD;Hscoreiis a policy with a heuristic function\nde\fned asHscore(C)def=argmaxT2D(score (T;C))\nIn short, a scoring heuristic policy one that selects\nthe next transform to apply based on a scoring func-\ntionscore , breaking ties arbitrarily. Additionally, we\n8\n\nrequire that transforms not in the active domain (i.e.,\nthat leave their inputs unchanged) must be assigned\nthe lowest score (0).\nAs we have already established, the number of\nscores that need to be computed is \fnite and enu-\nmerable. However, it is also linear in the number\nof atoms in the instance. Ideally, we would like to\navoid recomputing all of the scores at each iteration\nby precomputing the scores once and then incremen-\ntally maintaining them as the instance is updated.\nFor this to be feasible, we also need to bound the\nnumber of scores that change with each step of the\npolicy. We do this by \frst de\fning two properties of\npolicies: independence, which requires that the score\nof a (hierarchical) transform be exclusively dependent\non its target atom (De\fnition 14); and locality, which\nfurther requires that the score of a transform be in-\ndependent of the node's descendants past a bounded\ndepth. We then show that with any scoring function\nthat satis\fes these properties, only a \fnite number of\nscores change with any transform, and consequently\nthat the output of the scoring function on every el-\nement of the active domain can be e\u000eciently incre-\nmentally maintained.\nDe\fnition 16 (Independent Policy) .LetC<be the\nset of instances with Cas a left child.\nC<=fConcat (C;C0)jC02Cg\n[fBinTree (k;C;C0)jk2I^C02Cg\nand de\fne C>symmetrically as the set of instances\nwithCas a right child. We say that a hierarchical\nscoring policyh\u0001;Hscoreiisindependent if and only\nif for anyT,C\n8C02C<:score (T;C) = score (LHS [T];C0)\n8C02C>:score (T;C) = score (RHS [T];C0)\nDe\fnition 17 (Local Policy) .An independent hi-\nerarchical scoring policy h\u0001;Hscoreiis local if and\nonly if:\n8T8C18C2s.t. (C\u0003\n1\u0000fjC1jg) = (C\u0003\n2\u0000fjC2jg) :\nscore (T;C 1) =score (T;C 2)The following de\fnition uses the policy's target\nfunction (De\fnition 14) to de\fne a weighted list of\nall of the policy's targets.\nDe\fnition 18 (Weighted Targets) .Leth\u0001;Hscorei\nbe a hierarchical scoring policy. The weighted targets\nof instance C, denotedWC:fjA\u0002 N0jgis bag of\n2-tuples de\fned as\nWC=\b\f\f\nT0;score (T;C0)\u000b\f\fT2DC^(C0;T0) =f\u0003\nC(T)\f\f\t\nTheorem 3 (Bounded Target Updates) .Let\nh\u0001;Hscoreibe a local hierarchical scoring policy,\nCbe an instance, T2 DCbe a transform, and\nC0=T(C). The weighted targets of CandC0di\u000ber\nby at most 4\u0002jAj elements.\n\f\f(WC]WC0)\u0000(WC\\WC0)\f\f\u00144\u0002jAj\nThe proof, given in Appendix C, is based on the\nobservation that the independence and locality prop-\nerties restrict changes to the target function's outputs\nto exactly the set of nodes added, removed, or modi-\n\fed by the applied transform, excluding ancestors or\ndescendants. In the worst cases ( Divide ,Crack , or\nMerge ) this is 4 nodes.\nTheorem 3 shows that we can incrementally main-\ntain the weighted target list incrementally, as only\na \fnite number of its elements change at any pol-\nicy step. This allows us to materialize the weighted\ntarget list as a priority queue, who's \frst element is\nalways the policy's next transform.\n5 Implementing The JITD Run-\ntime\nSo far, we have introduced cog and shown how poli-\ncies can be used to gradually reorganize a cog in-\nstance by repeatedly applying incremental transforms\nto the structure.\nIn this section, we discuss the challenges in trans-\nlating JITD s from the theory we have de\fned so far\ninto practice. As already noted, cog instances de-\nscribe the physical layout of a JITD . We implemented\neach atom as a C++ class using the reference-counted\nshared ptrs for garbage collection. To implement\ntheArray andSorted atoms, we used the C++\nStandard Template Library vector class.\n9\n\nZXYX’Logical ContentPhysicalRepresentationRootZXYX’RootZ’(a)(b)Figure 5: Classical immutable data structures (a) vs\nwith handles (b).\n5.1 Concurrency and Handles\nBecause JITD s rely on background optimization, ef-\n\fcient concurrency is critical. This motivated our\nchoice to base the JITD index on functional data\nstructures. In a functional data structure, objects\nare immutable once instantiated. Only the root may\nbe updated to a new version, typically through an\natomic pointer swap. Explicit versioning makes it\npossible for the background worker thread to con-\nstruct a new version of the structure without taking\nout any locks in the process. Only a short lock is\nrequired to swap in the new version.\nImmutability does come with a cost: any muta-\ntions must also copy un-modi\fed data into a new ob-\nject. However, careful use of pointers can minimize\nthe impact of such copies.\nExample 3. Figure 5.a shows the e\u000bects of applying\nLHS [Sort ]to an immutable cog instance, replac-\ning an unsorted Array Xwith a Sorted equivalent\nX'. Note that in addition to replacing X, each of its\nancestors must also be replaced.\nReplacing a logarithmic number of ancestors is bet-\nter than replacing the entire structure. However,\neven a logarithmic number of new objects for ev-\nery update can be a substantial expense when in-\ndividual transforms can take on the order of micro-\nseconds. We avoid this overhead by using indirection\nto allow limited mutability under controlled circum-\nstances. Inspired by early forms of memory manage-\nment [11], we de\fne a new object called a handle .\nHandles store a pointer to a cog atom, and all\ncog atoms (i.e., BinTree andConcat ), as well asthe root, use handles as indirect references to their\nchildren. Handles provide clear semantics for a pro-\ngrammer expectations: A pointer to an atom guaran-\ntees physical immutability, while a pointer to a han-\ndle guarantees only logical immutability. Thus, any\nthread can safely replace the pointer stored in a han-\ndle with a pointer to any other logically equivalent\natom. Accordingly, we refer to such structures as\nsemi-functional data structures.\nExample 4. Continuing the example, Figure 5.b\nshows the same operation performed on a structure\nthat uses handles. Ancestors of the modi\fed node are\nunchanged: Only the handle pointer is modi\fed.\nWe observe that cog atoms can safely be imple-\nmented using handles. The only correctness property\nwe need to enforce is structural correctness, which de-\npends only on the node itself and the logical contents\n(D(\u0001)) of its descendants. Thus only logical consis-\ntency is needed and handles su\u000ece.\nSimilarly, the LHS andRHS and meta transform\ncreates an exact copy of the root, modulo the af-\nfected pointer. Furthermore the only node modi\fed\nis the one reached by unrolling the stack of meta\ntransforms, and by de\fnition correct transforms must\nproduce a new structure that is logically equivalent.\nThus, any hierarchical transform can be safely, e\u000e-\nciently applied to a JITD by a single modi\fcation to\nthe handle of the target atom (De\fnition 14).\n5.2 Concurrent Access Paths\nWe have already described the get method in\nSection 1.1. The remaining access paths instantiate\niterators that traverse the tree, lazily dereferencing\nhandles as necessary. Un-ordered iterators provide\ntwo methods:\när Get() returns the iterator's current record\näStep() advances the iterator to the next record\nAdditionally, ordered iterators provide the method:\näSeek(k) advances to the \frst record rwherer\u0017k\nFor iterators over Sorted andArray atoms, we di-\nrectly use the C++ vector class's iterator. Generat-\ning an ordered iterator over an Array atom forces\naSort \frst. Iterators for the remaining atom types\n10\n\nlazily create a replica of the root instance using only\nphysical references to ensure consistency. Unordered\niterators traverse trees left to right. Ordered iterators\noverConcat atoms are implemented using merge-\nsort. We implement a special-case iterator for Bin-\nTree s that iterates over contiguous BinTree s, lazily\nloading nodes from their handles as needed.\n5.3 Handles and Updates\nHandles also make possible concurrency between a\nJITD 's worker thread and threads updating the JITD .\nIn keeping with the convention that structures refer-\nenced by a handle pointers can only be swapped with\nlogically equivalent structures, a thread updating a\nJITD must replace the root handle with an entirely\nnew handle. Because the worker thread will only ever\nswap pointers referenced by a handle, it will never\nundo the e\u000bects of an update. Better still, if the old\nroot handle is re-used as part of the new structure\n(as discussed in Section 1.1), optimizations applied\nto the old root or any of its descendants will seam-\nlessly be applied to the new version of the index as\nwell.\n5.4 Transforms and the Policy Sched-\nuler\nOur policy scheduler is optimized for local hierarchi-\ncal policies. Policies are implemented by de\fning a\nscoring function\nN0 score(T,C)whereT2A\nBased on this function, the policy scheduler builds a\npriority queue of 3-tuples hhandle;A;N0i, includ-\ning a handle to a descendant of the root, an atomic\ntransform to apply to the descendant instance, and\nthe policy's score for the transform applied to the in-\nstance referenced by the handle. As an optimization,\nonly the highest-scoring transform for each handle is\nmaintained in the queue. The scheduler iteratively\nselects the highest scoring transform and applies it\nto the structure. Handles destroyed (resp., created)\nby applied transforms are removed from (resp., added\nto) the priority queue. The iterator continues until\nno transforms remain in the queue or all remainingtransforms have a score of zero, at which point we\nsay the policy has converged .\n5.5 Example Policy: Crack-Sort-\nMerge\nAs an example of policies being used to manage\ncost/bene\ft tradeo\u000bs in index structures, we com-\npare two approaches to data loading: database\ncracking [12] and upfront organization. In a study\ncomparing cracking to upfront indexing, Schuhknecht\net. al. [25] observe that for workloads consisting\nof more than a few scans, it is faster to build an\nindex upfront. Here, we take a more subtle approach\nto the same problem. The Crack transform has\nlower upfront cost than the Sort transform (scaling\nasO(N) vsO(NlogN)), but provides a smaller\nbene\ft. Given a \fxed time budget or \fxed latency\ngoal, is it better to repeatedly crack, sort, or mix\nthe two approaches together. We address this\nquestion with a family of scoring functions score\u0012,\nparameterized by a threshold value \u0012as follows:\nscore\u0012(T,Array ([r1:::rN]))=8\n><\n>:NifT=Sort andN <\u0012\nNifT=Crack and N <\u0012\n0otherwise\nArray s smaller than the threshold are sorted, while\nthose larger are cracked. Larger instances are pre-\nferred over smaller. All other instances are ignored.\nOnce all Array s are sorted, the resulting Sorted s\nare iteratively Merge ed, ultimately leaving behind\na single Sorted .\nThis is one example of a parameterized policy, a re-\norganizational strategy that uses thresholds to guide\nits behavior. Once such a policy is de\fned, the next\nchallenge is to select appropriate values for its pa-\nrameters.\n6 Policy Optimization\nAJITD 's performance curve depends on its policy. As\nwe may have a range of policies to choose from | for\nexample by varying policy parameters as mentioned\nabove | we want a way to evaluate the utility of a\npolicy for a given workload. Naively, we might do this\nby repeatedly evaluating the structure under each\n11\n\npolicy, but doing so can be expensive. Instead, we\nnext propose a performance model for JITD s, policies,\nand a lightweight simulator that approximates the\nperformance of a policy over time. Our approach is\nto see each transformation as an overhead performed\nin exchange for improved query performance. Hence,\nour model is based on two measured characteristics\nof the JITD : The costs of accessing an instance, and\nthe cost of applying a transform. A separate driver\nprogram measures (1) the cost of each access path\non each instance atom type, varying every parame-\nter available, and (2) the cost of each case of every\ntransform.\nExample 5. As an illustrative example, we will use\nthe Crack-or-Sort policy described above. This pol-\nicy makes use of the Array ,Sorted , and BinTree\natoms, as well as the Crack andSort transforms.\nFor this policy we need to measure 5 factors.\nOperation Symbol Scaling\nGet(Array ([r1:::rN]))\u000b(N)O(N)\nGet(Sorted ([r1:::rN])))\f(N)O(log2(N))\nGet(BinTree (k;C 1;C2)))\r O (1)\nCrack (Array ([r1:::rN]))\u000e(N)O(N)\nCrack (Array ([r1:::rN]))\u0017(N)O(nlog2(n))\nThe driver program \fts each of the \fve functions\nby conducting multiple timing experiments, varying\nthe size ofNwhere applicable.\nThe simulator mirrors the behavior of the full JITD ,\nbut uses a lighter-weight version of the cog grammar\nthat does not store actual data:\nC`=Array (N0)jSorted (N0)\njConcat (C`;C`)jBinTree (C`;C`)\nThe simulator iteratively simulates applying trans-\nforms to instances expressed in C`according to the\npolicy being simulated. After each transform, the\nsimulator uses the measured cost of the transform to\nestimate the cumulative time spent reorganizing the\nindex. The simulator captures multiple performance\nmetrics metric :C!R.\nExample 6. Continuing the example, one useful\nmetric is the read latency for a uniformly distributedread workload on a Crack-or-Sort index.\nlatency (C) =8\n>>>><\n>>>>:\u000b(N) ifC=Array (N)\n\f(N) ifC=Array (N)\n\r+jC1j\njCjlatency (C1)\n+jC2j\njCjlatency (C2)ifC=BinTree (C1;C2)\nwherejCjis the sum of sizes of Array s and Sorted s\ninC\u0003.\nThe simulator produces a sequence of status in-\ntervals: periods during which index performance is\n\fxed, prior to the pointer swap after the next trans-\nform is computed. A user-provided utility function\naggregates these intervals to provide a \fnal utility\nscore for the entire policy. Given a \fnite set of poli-\ncies, the optimizer tries each in turn and selects the\none that best optimizes the utility function. Given a\nparameterized policy, the optimizer instead uses gra-\ndient descent.\nExample 7. Examples of utility functions for Crack-\nor-Sort include: (1) Minimize time spent with more\nthan\u0012Get() latency, (2) Maximize throughput for\nNseconds, (3) Minimize runtime of Nqueries.\n7 On the Generality of JITDs\nIdeally, we would like cog to be expressive enough to\nencode the instantaneous state of any data structure.\nIn\fnite generality is obviously out of scope for this\npaper. However we now take a moment to assess\nexactly what index data structure design patterns are\nsupported in a JITD .\nAs a point of reference we use a taxonomy of data\nstructures proposed as part of the Data Calcula-\ntor [16]. The data calculator taxonomy identi\fes 22\ndesign primitives, each with a domain of between 2\nand 7 possible values. Each of the roughly 1018valid\npoints in this 22-dimensional space describes one pos-\nsible index structure. To the best of our knowledge,\nthis represents the most comprehensive a survey of\nthe space of possible index structures developed to\ndate.\nThe data calculator taxonomy views index struc-\ntures through the general abstraction of a tree with\n12\n\ninner nodes and leaf nodes. This abstraction is some-\ntimes used loosely: A hash table of size N, for exam-\nple, is realized as as a tree with precisely one inner-\nnode and N leaf nodes. Each of the taxonomy's de-\nsign primitives captures one set of mutually exclusive\ncharacteristics of the nodes of this tree and how they\nare translated to a physical layout.\nFigure 6 classi\fes each of the design primitives as\n(1) Fully supported by JITD if it generalizes the entire\ndomain, (2) Partially supported by JITD if it sup-\nports more than one element of the domain, or (3)\nNot supported otherwise. We further subdivide this\nlatter category in terms of whether support is fea-\nsible or not. In general, the only design primitives\nthat JITD can not generalize are related to muta-\nbility, since JITD 's (semi-)immutability is crucial for\nconcurrency, which is in turn required for optimiza-\ntion in the background.\nJITD completely generalizes 7 of the remaining 22\nprimitives. We \frst explain these primitives and how\nJITD s generalize them. Then, we propose three ex-\ntensions that, although beyond the scope of this pa-\nper, would fully generalize the \fnal 14 primitives. For\neach, we brie\ry discuss the extension and summarize\nthe challenges of realizing it.\nKey retention (1) . This primitive expresses\nwhether inner nodes store keys (in whole or in part),\nmirroring the choice between Concat andBinTree .\nIntra-node access (5) . This primitive expresses\nwhether nodes (inner or child) allow direct access to\nspeci\fc children or whether they require a full scan,\nmirroring the distinction between cog nodes with\nand without semantic constraints.\nKey partitioning (9) . This primitive expresses\nhow newly added values are partitioned. Examples\ninclude by key range (as in a B+Tree) or temporally\n(as in a log structured merge tree [22]). Although a\nJITD only allows one form of insertion, policies can\nconverge to the full range of states permitted for this\nprimitive.\nSub-block homogeneous (18) . This primitive\nexpresses whether all inner nodes are homogeneous\nor not.Sub-block consolidation/instantiation\n(19/20) . These primitives express how and\nwhen organization happens, as would be determined\nby a JITD 's policy.\nRecursion allowed (22) . This primitive expresses\nwhether inner nodes form a bounded depth tree, a\ngeneral tree, or a \\tree\" with a single node at the\nroot. JITD s support all three.\n7.1 Supporting New cog Atoms\nFive of the remaining primitives can be generalized\nby the addition of three new atoms to cog. First,\nwe would need a generalization of BinTree atoms\ncapable of using partial keys as in a Trie (primitive 1),\nor hash values (primitive 3) Second, a unary Filter\natom that imposes a constraint on the records below\nit could implement both boom \flters (primitives 7,9)\nand zone maps (primitives 8,9). These two atoms\nare conceptually straightforward, but introduce new\ntransforms and increase the complexity of the search\nfor e\u000bective policies.\nThe remaining challenge is support for colum-\nnar/hybrid layouts (primitive 4). Columnar layouts\nincrease the complexity of the formalism by requiring\nmultiple record types and support for joining records.\nAccordingly, we posit that a binary Join atom, rep-\nresenting the collection of records obtained by joining\nits two children could e\u000eciently capture the seman-\ntics of columnar (and hybrid) layouts.\n7.2 Atom Synthesis\nFive of the remaining primitives express various tac-\ntics for removing pointers by inlining groups of nodes\ninto contiguous regions of memory. These primitives\ncan be generalized by the addition of a form of atom\nsynthesis, where new atoms are formed by merging\nexisting atoms. Consider the Linked List of Exam-\nple 1. Despite the syntactic restriction over cog, a\nsingle linked list element must consist of two nodes\n(aConcat and a (single-record) Array ), and an un-\nnecessary pointer de-reference is incurred on every\nlookup. Assume that we could de\fne a new node\ntype: A linked list element ( Link (R;C)) consisting\n13\n\n#Data Calculator Primitive JITD Note\n1Key retention w No partial keys\n2Value retention m\n3Key order w K-ary orders unsupported\n4Key-Value layout m No columnar layouts\n5Intra-node access l\n6Utilization 7\n7Bloom \flters m\n8Zone map \flters w Implicit via BinTree\n9Filter memory layout m Requires \flters (7,8)\n10Fanout/Radix m Limited to 2-way fanout\n11Key Partitioning l\n12Sub-block capacity 7\n13Immediate node links m Simulated by iterator impl.\n14Skip-node links m\n15Area links m Simulated by iterator impl.\n16Sub-block physical location. m Only pointed supported\n17Sub-block physical layout. w/7Realized by merge rewrite\n18Sub-block homogeneous l\n19Sub-block consolidation l Depends on policy\n20Sub-block instantiation l Depends on policy\n21Sub-block link layout m Requires links (13,14,15)\n22Recursion allowed l\nl: Full Support w: Partial Support m: Support Possible\n7: Not applicable to immutable data structures\nFigure 6: JITD support for the DC Taxonomy [16]\n14\n\nof a record and a forward pointer. Because this node\ntype is de\fned in terms of existing node types, it\nwould be possible to automatically synthesize new\ntransformations for it from existing transformations,\nand existing performance models could likewise be\nadapted.\nAtom synthesis could be used to create inner nodes\nthat store values (primitive 2), increase the fanout of\nConcat andBinTree nodes (primitive 10), inline\nnodes (primitive 16), and provide \fner-grained con-\ntrol over physical layout of data (primitive 17).\n7.3 Links / DAG support\nThe \fnal four remaining properties (13, 14, 15, and\n20) express a variety of forms of link between inner\nand leaf nodes. Including such links turns the result-\ning structure into a directed acyclic graph (DAG).\nIn principle, it should be possible to generalize trans-\nforms for arbitrary DAGs rather than just trees as we\ndiscuss in this paper. Such a generalization would re-\nquire additional transforms that create/maintain the\nnon-local links and more robust garbage collection.\n8 Evaluation\nWe next evaluate the performance of JITD s in com-\nparison to other commonly used data structures. Our\nresults show that: (1) In the longer term, JITD s have\nminimal overheads relative to standard in-memory\ndata structures; (2) The JITD policy simulator reli-\nably models the behavior of a JITD ; (3) In the short\nterm, JITD s can out-perform standard in-memory\ndata structures; (4) Concurrency introduces minimal\noverheads; and (5) JITD s scale well with data, both\nin their access costs and their organizational costs.\n8.1 Experimental setup\nAll experiments were run on a 2 \u00026-core 2.5 GHz In-\ntel Xeon server with 198 GB of RAM and running\nUbuntu 16.04 LTS. Experimental code was written\nin C++ and compiled with GNU C++ 5.4.0. Each\nelement in the data set is a pair of key and value,\neach an 8-Byte integer. Unless otherwise noted, weuse a data size of up to a maximum of 109records\n(16GB) with keys generated uniformly at random.\nTo mitigate experimental noise, we use srand() with\nan arbitrary but consistent value for all data gener-\nation. To put our performance numbers into con-\ntext, we compare against (1) R/B Tree : the C++\nstandard-template library (STL) mapimplementation\n(a classical red-black tree), (2) HashTable the C++\nstandard-template library (STL) unordered-map im-\nplementation (a hash table), and (3) BTree a pub-\nlicly available implementation of b-trees1. For all\nthree, we used the find() method for point lookups\nand lower bound() /++(where available) for range-\nscans. For point lookups, we selected the target key\nuniformly at random2. For range scans, we selected\na start value uniformly at random and the end value\nto visit approximately 1000 records. Except where\nnoted, access times are the average of 1000 point\nlookups or 50 range scans.\nWe speci\fcally evaluated JITD s using the Crack-\nSort-Merge family of policies described in Section 5.5,\nvarying the crack threshold over 106, 107, 108, and\n109records. When there are exactly 109records,\nthis last policy simply sorts the entire input in one\nstep. For point lookups we use the get() access path,\nand for range scans we use the ordered iterator()\naccess path. By default, we measure JITD read\nperformance through a synchronous (i.e., with the\nworker thread paused) microbenchmark. We con-\ntrast synchronous and asynchronous performance in\nSection 8.4.\nSynchronous read performance was measured\nthrough a sequence of trials, each with a progres-\nsively larger number of transforms (i.e., a progres-\nsively larger fragment of the policy's trace) applied\nto the JITD . We measured total time to apply the\ntrace fragment (including the cost of selecting which\ntransforms to apply) before measuring access laten-\ncies. For concurrent read performance a client thread\nmeasured access latency approximately once per sec-\nond.\n1https://github.com/JGRennison/cpp-btree\n2We also tested a heavy-hitter workload that queried for\n30% of the keyspace 80% of the time, but found no signi\fcant\ndi\u000berences between the workloads.\n15\n\n0 500 1000 1500 2000 2500 3000 3500\nTransformation Time(sec)10-610-510-410-310-210-1100101Time for one lookup (sec)\nSAI-106\nSAI-107\nSAI-108\nSAI-109\nBTree\nHashTable\nR/B-Tree(a) Point Lookups\n0 500 1000 1500 2000 2500 3000 3500\nTransformation Time(sec)10-510-410-310-210-1100101102103104Time ofr one lookup(sec)\nSAI-106\nSAI-107\nSAI-108\nSAI-109\nR/B-Tree\nBTree (b) Range Scans\nFigure 7: Performance improvement over time as each JITD is organized\n8.2 Cost vs Bene\ft Over Time\nOur \frst set of experiments mirrors Figure 1, track-\ning the synchronous performance of point lookups\nand range scans over time. The results are shown\nin Figure 7a and Figure 7b The x-axis shows time\nelapsed, while the y-axis shows index access latency\nat that point in time. In both sets of experiments,\nwe include access latencies and setup time for the\nR/B-Tree (yellow star), the HashTable (black trian-\ngle), and the BTree (pink circles) We treat the cost\nof accessing an incomplete data structure as in\fnite,\nstepping down to the structure's normal access costs\nonce it is complete.\nIn general, lower crack thresholds achieve faster\nupfront performance by sacri\fcing long-term perfor-\nmance. A crack threshold of 106(approximately1\n105\ncracked partitions) takes approximately twice as long\nto reach convergence as a threshold of 109(sort ev-\nerything upfront)\nUnsurprisingly, for point lookups the Hash Table\nhas the best overall performance curve. However,\neven it needs upwards of 6 minutes worth of data\nloading before it is ready. By comparison, a JITD\nstarts o\u000b with a 10 second response time, and has\ndropped to under 3 seconds by the 3 minute mark.\nThe BTree signi\fcantly outperforms the R/B-Tree\non both loading and point lookup cost, but still takes\nnearly 25 minutes to fully load. By that point the\nThreshold108policy JITD has already been servingpoint lookups with a comparable latency (after its\nsort phase) for nearly 5 minutes. Note that lower\ncrack thresholds have a slightly slower peak perfor-\nmance than higher ones before their merge phase This\nis a consequence of deeper tree structures and the in-\ndirection resulting from handles.The performance at\nconvergence of the 108threshold point scan trial is\nsurprising, as it suggests binary search is as fast as\na hash lookup. We suspect this due to lucky cache\nhits, but have not yet been able to con\frm it.\n8.3 Simulated vs Actual Performance\nFigure 8 shows the result of using our simulator to\npredict the performance curves of Figure 7a. As can\nbe seen, performance is comparable. Policy runtimes\nare replicated reliably, features like time to conver-\ngence and crossover are replicated virtually identi-\ncally.\n8.4 Synchronous vs Concurrent\nFigures 9a, 9b, and 9c contrast the synchronous per-\nformance of the JITD with a more realistic concur-\nrent workload. Performance during the crack phase\nis comparable, though admittedly with a higher vari-\nance. As expected, during the sort phase perfor-\nmance begins to bifurcate into fast-path accesses to\nalready sorted arrays and slow-path scans over array\nnodes at the leaves.\n16\n\n0 500 1000 1500 2000 2500 3000 3500\nTransformation Time(sec)10-610-510-410-310-210-1100101Time for one lookup(sec)\nSAI-106\nSAI-107\nSAI-108\nSAI-109Figure 8: Predicted performance using the simulator.\nThe time it takes the worker to converge is largely\nuna\u000bected by the introduction of concurrency. How-\never, as the structure begins to converge, we see a\nconstant 100 \u0016soverhead compared to synchronous\naccess. We also note periodic 100 msbursts of latency\nduring the sort phases of all trials. We believe these\nare caused when the worker thread pointer-swaps in\na new array during the merge phase, as the entire\nnewly created array is cold for the client thread.\n8.5 Short-Term Bene\fts for interac-\ntive workloads\nOne of the primary bene\fts of JITD s is that they can\nprovide signi\fcantly better performance during the\ntransition period. This is particularly useful in inter-\nactive settings where users pose tasks comparatively\nslowly. We next consider such a hypothetical scenario\nwhere a data \fle is loaded and each data structure is\ngiven a short period of time (5 seconds) to prepare.\nIn these experiments, we use a cracking threshold of\n105(our worst case), and vary the size of the data set\nfrom 106records (16MB) to 109records (16GB). The\nlookup time is the time until an answer is produced:\nthe cost of a point lookup for the JITD . The baseline\ndata structures are accessible only once fully loaded,\nso we model the user waiting until the structure is\nready before doing a point lookup. Up through 107\nrecords, the unordered mapcompletes loading within\n5 seconds. In every other case, the JITD is able toproduce a response orders of magnitude faster.\n8.6 DataSize Vs TransformTime\nFigure 11 illustrates the scalability of JITD from the\nperspective of data loading. As before, we vary the\nsize of the data set and use the time taken to load a\ncomparable amount of data into the base data struc-\ntures. Note that data is accessible virtually imme-\ndiately after being loaded into a JITD . We measure\nthe cost for the JITD to reach convergence. The per-\nformance of the JITD and the other data structures\nboth scale linearly with the data size (note the log\nscale).\n8.7 CrackThreshold Vs ScanTime\nFigure 12 explores the e\u000bects of the crack threshold\non performance at convergence of the Crack-Sort pol-\nicy. The Merge Policy was excluded from testing as at\nconvergence it would lead to one huge sorted array of\nsize 109irrespective of the crack threshold. In these\nset of experiments the crack threshold for cracking\nan array in the JITD structure was varied from 106\nto 109. For each, we performed one thousand point\nscans, measuring the total time and computing the\naverage cost per scan. This \fgure shows the over-\nhead from handles | at a crack threshold of 109, the\nentire array is sorted in a single step. As the crack\nthreshold grows by a factor of 10, the depth of the\ntree increases by roughly a factor of three, necessitat-\ning approximately 3 additional random accesses via\nhandles rather than directly on a sorted array, and as\nshown in the graph, increasing access time by roughly\n1\u0016s.\n9 Related Work\nJITD s speci\fcally extend work by Kennedy and\nZiarek on Just-in-Time Data Structures [17] with a\nframework for de\fning policies, tools for optimizing\nacross families of policies, and a runtime that sup-\nports optimization in the background rather than as\npart of queries. Most notably, this enables e\u000ecient\ndynamic data reorganization as an ongoing process\n17\n\n500\n 0 500 1000 1500 2000 2500 3000 3500\nTransformation Time(sec)10-510-410-310-210-1100101102Time for one lookup (sec)\nSAI-106\nConcurrent-SAI-106(a) Crack Threshold = 106\n500\n 0 500 1000 1500 2000 2500 3000\nTransformation Time(sec)10-510-410-310-210-1100101102Time for one lookup (sec)\nSAI-107\nConcurrent-SAI-107(b) Crack Threshold = 107\n500\n 0 500 1000 1500 2000\nTransformation Time(sec)10-610-510-410-310-210-1100101102Time for one lookup (sec)\nSAI-108\nConcurrent-SAI-108\n(c) Crack Threshold = 108\nFigure 9: Synchronous vs Concurrent performance of the JITD on point lookups.\n106107108109\nData size10-710-610-510-410-310-210-1100101102103104Time for one Lookup(sec)\nSAI-105\nR/B-Tree\nHashTable\nBTree\nFigure 10: Point lookup latency relative to data size.\n106107108109\nData size10-1100101102103104Transformation Time(sec)\nSAI-105\nR/B-Tree\nHashTable\nBTreeFigure 11: The time required to load and fully orga-\nnize a data set relative to data size\n18\n\n106107108109\nCrack Threshold10-610-510-410-310-2Time for one Lookup(sec)\nSAI\nBTree\nHashTable\nR/B-TreeFigure 12: For the Uniform workload each point on\nthe graph indicates the change in scan time for dif-\nferent crack thresholds.\nrather than as an inline, blocking part of query exe-\ncution.\nOur goal is also spiritually similar to The Data Cal-\nculator [16]. Like our policy optimizer, it searches\nthrough a large space of index design choices for one\nsuitable for a target workload. However, in con-\ntrast to JITD s, this search happens once at compile\ntime and explores mostly homogeneous structures. In\nprinciple, the two approaches could be combined, us-\ning the Data Calculator to identify optimal struc-\ntures for each workload and using JITD s to migrate\nbetween structures as the workload changes.\nAlso related is a recently proposed form of \\Re-\nsumable\" Index Construction [2]. The primary chal-\nlenge addressed by this work is ensuring that updates\narriving after index construction begins are properly\nre\rected in the index. While we solve this problem\n(semi-)functional data structures, the authors pro-\npose the use of temporary bu\u000bers.\nAdaptive Indexing .JITD s are a form of adaptive\nindexing [14, 8], an approach to indexing that re-\nuses work done to answer queries to improve index\norganization. Examples of adaptive indexes include\nCracker Indexes [12, 13], Adaptive Merge Trees [9],\nSMIX [28], and assorted hybrids thereof [15, 17]. No-\ntably, a study by Schuhknecht et. al. [25] compares\n(among other things) the overheads of cracking to\nthe costs of upfront indexing. Aiming to optimizeoverall runtime, upfront indexing begins to outper-\nform cracker indexes after thousands to tens of thou-\nsands of queries. By optimizing the index in the back-\nground, JITD s avoid the overheads of data reorgani-\nzation as part of the query itself .\nOrganization in the Background . Unlike adap-\ntive indexes, which inline organizational e\u000bort into\nnormal database operations, several index structures\nare designed with background performance optimiza-\ntion in mind. These begin with work in active\ndatabases [29], where reactions to database updates\nmay be deferred until CPU cycles are available. More\nrecently, bLSM trees [26] were proposed as a form\nof log-structured merge tree that coalesces partial\nindexes together in the background. A wide range\nof systems including COLT [24], OnlinePT [3], and\nPeloton [23] use workload modeling to dynamically\nselect, create, and destroy indexes, also in the back-\nground.\nSelf-Tuning Databases . Database tuning advi-\nsors have existed for over two decades [4, 5], auto-\nmatically selecting indexes to match speci\fc work-\nloads. However, with recent advances in machine\nlearning technology, the area has seen signi\fcant re-\ncent activity, particularly in the context of index se-\nlection and design. OtterTune [27] uses \fne-grained\nworkload modeling to predict opportunities for set-\nting database tuning parameters, an approach com-\nplimentary to our own.\nGeneric Data Structure Models . More spiritu-\nally similar to our work is The Data Calculator [16],\nwhich designs custom tree structures by searching\nthrough a space of dozens of parameters describing\nboth tree and leaf nodes. A similarly related e\u000bort\nuses small neural networks [18] as a form of universal\nindex structure by \ftting a regression on the CDF of\nrecord keys in a sorted array.\n10 Conclusions and Future\nWork\nIn this paper, we introduced JITD s a type of in-\nmemory index that can incrementally morph its per-\n19\n\nformance characteristics to adapt to changing work-\nloads. To accomplish this, we formalized a compos-\nable organizational grammar ( cog) and a simple al-\ngebra over it. We introduced a range of equivalence-\nand structure-preserving rewrite rules called trans-\nforms that serve as the basis of organizational policies\nthat guide the transition from one performance en-\nvelope to another. We described a simulation frame-\nwork that enables e\u000ecient optimization of policy pa-\nrameters. Finally, we demonstrated that a JITD can\nbe implemented with minimal overhead relative to\nclassical in-memory index structures.\nOur work leaves open several challenges. We have\nalready identi\fed three speci\fc challenges in Sec-\ntion 7: New atoms, Atom synthesis, and DAG sup-\nport. Addressing each of these challenges would allow\ncog to capture a wide range of data structure seman-\ntics. There are also several key areas where perfor-\nmance tuning is possible: First, our use of reference-\ncounted pointers also presents a performance bottle-\nneck for high-contention workloads | we plan to ex-\nplore more active garbage-collection strategies. Sec-\nond, Handles are an extremely conservative realiza-\ntion of semi-functional data structures. As a result,\nJITD s are a factor of 2 slower at convergence than\nother tree-based indexes. We expect that this perfor-\nmance gap can be reduced or eliminated by identi-\nfying situations where Handles are unnecessary (e.g.,\nat convergence). A \fnal open challenge is the use of\nstatistics to guide rewrite rules, both detecting work-\nload shifts to trigger policy shifts (e.g., as in Peloton),\nas well as identifying statistics-driven policies that\nnaturally converge to optimal behaviors for dynamic\nworkloads.\nReferences\n[1] I. Alagiannis, R. Borovica, M. Branco, S. Idreos,\nand A. Ailamaki. Nodb: e\u000ecient query execu-\ntion on raw data \fles. In SIGMOD Conference ,\npages 241{252. ACM, 2012.\n[2] P. Antonopoulos, H. Kodavalla, A. Tran,\nN. preti, C. Shah, and M. Sztajno. Resumableonline index rebuild in SQL server. PVLDB ,\n10(12):1742{1753, 2017.\n[3] N. Bruno and S. Chaudhuri. An online approach\nto physical design tuning. In ICDE , pages 826{\n835. IEEE Computer Society, 2007.\n[4] S. Chaudhuri and V. R. Narasayya. Autoad-\nmin 'what-if' index analysis utility. In SIGMOD ,\n1998.\n[5] S. Chaudhuri and V. R. Narasayya. Self-tuning\ndatabase systems: A decade of progress. In\nVLDB , pages 3{14, 2007.\n[6] D. Comer. The ubiquitous b-tree. ACM Comput.\nSurv. , 11(2):121{137, 1979.\n[7] G. Graefe. B-tree indexes for high update rates.\nInData Always and Everywhere - Management\nof Mobile, Ubiquitous, Pervasive, and Sensor\nData , volume 05421 of Dagstuhl Seminar Pro-\nceedings , 2005.\n[8] G. Graefe, S. Idreos, H. A. Kuno, and S. Mane-\ngold. Benchmarking adaptive indexing. In\nTPCTC , volume 6417 of Lecture Notes in Com-\nputer Science , pages 169{184. Springer, 2010.\n[9] G. Graefe and H. A. Kuno. Self-selecting, self-\ntuning, incrementally optimized indexes. In\nEDBT , volume 426 of ACM International Con-\nference Proceeding Series , pages 371{381. ACM,\n2010.\n[10] L. J. Guibas and R. Sedgewick. A dichromatic\nframework for balanced trees. In FOCS , pages\n8{21. IEEE Computer Society, 1978.\n[11] A. Hertzfeld. Hungarian. http://www.\nfolklore.org/StoryView.py?project=\nMacintosh&story=Hungarian.txt .\n[12] S. Idreos, M. L. Kersten, and S. Manegold.\nDatabase cracking. In CIDR , pages 68{78, 2007.\n[13] S. Idreos, M. L. Kersten, and S. Manegold. Up-\ndating a cracked database. In SIGMOD Confer-\nence, pages 413{424. ACM, 2007.\n20\n\n[14] S. Idreos, S. Manegold, and G. Graefe. Adaptive\nindexing in modern database kernels. In EDBT ,\npages 566{569. ACM, 2012.\n[15] S. Idreos, S. Manegold, H. A. Kuno, and\nG. Graefe. Merging what's cracked, cracking\nwhat's merged: Adaptive indexing in main-\nmemory column-stores. PVLDB , 4(9):585{597,\n2011.\n[16] S. Idreos, K. Zoumpatianos, B. Hentschel, M. S.\nKester, and D. Guo. The data calculator: Data\nstructure design and cost synthesis from \frst\nprinciples and learned cost models. In SIGMOD\nConference , pages 535{550. ACM, 2018.\n[17] O. Kennedy and L. Ziarek. Just-in-time data\nstructures. In CIDR , 2015.\n[18] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index struc-\ntures. In SIGMOD Conference , pages 489{504.\nACM, 2018.\n[19] P. Larson. Dynamic hash tables. Commun.\nACM , 31(4):446{457, 1988.\n[20] D. Lee, S. Baek, and H. Bae. acn-rb-tree: Up-\ndate method for spatio-temporal aggregation of\nmoving object trajectory in ubiquitous environ-\nment. In ICCSA Workshops , pages 177{182.\nIEEE Computer Society, 2009.\n[21] C. Okasaki. Purely Functional data structures .\nCambridge University Press, 1999.\n[22] P. E. O'Neil, E. Cheng, D. Gawlick, and E. J.\nO'Neil. The log-structured merge-tree (lsm-\ntree). Acta Inf. , 33(4):351{385, 1996.\n[23] A. Pavlo, G. Angulo, J. Arulraj, H. Lin, J. Lin,\nL. Ma, P. Menon, T. C. Mowry, M. Perron,\nI. Quah, S. Santurkar, A. Tomasic, S. Toor,\nD. V. Aken, Z. Wang, Y. Wu, R. Xian, and\nT. Zhang. Self-driving database management\nsystems. In CIDR , 2017.\n[24] K. Schnaitter, S. Abiteboul, T. Milo, and\nN. Polyzotis. COLT: continuous on-line tuning.InSIGMOD Conference , pages 793{795. ACM,\n2006.\n[25] F. M. Schuhknecht, A. Jindal, and J. Dit-\ntrich. The uncracked pieces in database cracking.\nPVLDB , 7(2):97{108, 2013.\n[26] R. Sears and R. Ramakrishnan. blsm: a general\npurpose log structured merge tree. In SIGMOD\nConference , pages 217{228. ACM, 2012.\n[27] D. Van Aken, A. Pavlo, G. J. Gordon, and\nB. Zhang. Automatic database management\nsystem tuning through large-scale machinelearn-\ning. In SIGMOD Conference , pages 1009{1024.\nACM, 2017.\n[28] H. Voigt, T. Kissinger, and W. Lehner. SMIX:\nself-managing indexes for dynamic workloads. In\nSSDBM , pages 24:1{24:12. ACM, 2013.\n[29] J. Widom and S. Ceri. Introduction to active\ndatabase systems. In Active Database Systems:\nTriggers and Rules For Advanced Database Pro-\ncessing , pages 1{41. Morgan Kaufmann, 1996.\n[30] Z. Zhou, J. Tang, L. Zhang, K. Ning, and\nQ. Wang. Egf-tree: an energy-e\u000ecient index tree\nfor facilitating multi-region query aggregation in\nthe internet of things. Personal and Ubiquitous\nComputing , 18(4):951{966, 2014.\nA Correctness of Example\nTransforms\nAs a warm-up and an example of transform correct-\nness, we next review each of the transforms given in\nFigure 4 and prove the correctness of each.\nProposition 2 (Identity is correct) .Letiddenote\nthe identity transform id(C) =C.idis both equiva-\nlence preserving and structure preserving.\nLemma 2 (Sort is correct) .Sort is both equivalence\npreserving and structure preserving.\nProof. For any instance Cwhere typeof (C)6=\nArray , correctness follows from Proposition 2.\n21\n\nOtherwise C=Array ([r1;:::;rN]), and conse-\nquently Sort (C) =Sorted (sort ([r1;:::;rN])). To\nshow correctness we \frst need to prove that\nD(Array ([r1;:::;r N])) =D(Sorted (sort ([r1;:::;r N])))\nLet the one-to-one (hence invertable) function f:\n[1;N]![1;N] denote the transposition applied by\nsort .\nD(Sorted (sort ([r1;:::;r N])))\n=D\u0000\nSorted (\u0002\nrf\u00001(1);:::;rf\u00001(N)\u0003\n))\u0001\n=\b\f\frf\u00001(1);:::;rf\u00001(N)\f\f\t\n=fjr1;:::;r Njg\n=D(Array ([r1;:::;r N]))\ngiving us equivalence preservation. Structure\npreservation requires that\u0002\nrf\u00001(1);:::;rf\u00001(N)\u0003\nbe in\nsorted order, which it is by construction. Thus, Sort\nis a correct transform.\nLemma 3 (UnSort is correct) .UnSort is both\nequivalence preserving and structure preserving.\nProof. For any instance Cwhere typeof (C)6=\nSorted , correctness follows from Proposition 2.\nOtherwise C=Sorted ([r1:::rN]) and we\nneed to show \frst that D(Sorted ([r1:::rN])) =\nD(Array ([r1:::rN])). The logical contents of both\narefjr1:::rNjg, so we have equivalence. Structure\npreservation is a given since any Array instance is\nstructurally correct.\nLemma 4 (Divide is correct) .Divide is both equiv-\nalence preserving and structure preserving.\nProof. For any instance Cwhere typeof (C)6=\nArray , correctness follows from Proposition 2.\nOtherwiseC=Array ([r1:::rN]) and we need to\nshow \frst that\nD(Array ([r1:::rN])) =\nD\u0010\nConcat\u0010\nArray (h\nr1:::rbN\n2ci\n);Array (h\nrbN\n2c+1:::rNi\n)\u0011\u0011\nEvaluating the right hand side of the equation re-\ncursively and simplifying, we have\n=n\f\f\fr1:::rbN\n2c\f\f\fo\n]n\f\f\frbN\n2c+1:::rN\f\f\fo\n=n\f\f\fr1:::rbN\n2c;rbN\n2c+1:::rN\f\f\fo\n=fjr1:::rNjg=D(Array ([r1:::rN]))Hence we have equivalence preservation. The Array\ninstances are always structurally correct and Concat\ninstances are structurally correct if their children are,\nso we have structural preservation as well. Hence,\nDivide is correct.\nLemma 5 (Crack is correct) .Crack is both equiv-\nalence preserving and structure preserving.\nProof. For any instance Cwhere typeof (C)6=\nArray , correctness follows from Proposition 2.\nOtherwiseC=Array ([r1:::rN]) and we need to\nshow \frst that\nD(Array ([r1:::rN])) =\nD\u0000\nBinTree\u0000\nk;Array (\u0002\nri\f\fri\u001ek\u0003\n);Array (\u0002\nri\f\fk\u0016ri\u0003\n)\u0001\u0001\nHerek=id(ri) for an arbitrary i. Evaluating the\nright hand side of the equation recursively and sim-\nplifying, we have\n=\b\f\fri\f\fri\u001ek\f\f\t\n]\b\f\fri\f\fk\u0016ri\f\f\t\n=\b\f\fri\f\f(ri\u001ek)_(k\u0016ri)\f\f\t\n=fjr1:::rNjg=D(Array ([r1:::rN]))\nInstances of Array are always structurally correct.\nThe newly created BinTree instance is structurally\ncorrect by construction. Thus Crack is correct.\nLemma 6 (Merge is correct) .Merge is both equiv-\nalence preserving and structure preserving.\nProof. For any instance Cthat matches neither of\nMerge 's cases, correctness follows from Proposi-\ntion 2. Of the remaining two cases, we \frst consider\nC=Concat (Array ([r1:::rN]);Array ([rN+1:::rM]))\nThe proof of equivalence preservation is identical to\nthat of Theorem 4 applied in reverse. In the second\ncase\nC=BinTree (;Array ([r1:::rN]);Array ([rN+1:::rM]))\nNoting that BinTree (;C1;C2)\u0019Concat (C1;C2)\nby the de\fnition of logical contents, the proof of\nequivalence preservation is again identical to that of\nTheorem 4 applied in reverse. For both cases, struc-\ntural preservation is given by the fact that Array\nis always structurally correct. Thus Merge is cor-\nrect.\n22\n\nLemma 7 (PivotLeft is correct) .PivotLeft is both\nequivalence preserving and structure preserving.\nProof. For any instance Cthat matches neither of\nPivotLeft 's cases, correctness follows from Proposi-\ntion 2. Of the remaining two cases, we \frst consider\nC=Concat (C1;Concat (C2;C3))\nEquivalence follows from from associativity of bag\nunion.\nD(Concat (C1;Concat (C2;C3)))\n=D(C1)]D(C2)]D(C3)\n=D(Concat (Concat (C1;C2);C3))\nConcat instances are structurally correct if their\nchildren are, so the transformed instance is struc-\nturally correct if \u000b(C1),\u000b(C2), and\u000b(C3). Hence,\nif the input is structurally correct, then so is the out-\nput and the transform is structurally preserving in\nthis case. The proof of equivalence preservation is\nidentical for the case where\nC=BinTree (k1;C1;BinTree (k2;C2;C3))andk1\u001ek2\nFor structural preservation, we additionally need to\nshow: (1)8r2D(C1) :r\u001ek1,(2)8r2D(C2) :\nk1\u0016r,(3)8r2D(BinTree (k1;C1;C2)) :r\u001ek2,\nand(4)8r2D(C3) :k2\u0016rgiven thatCis struc-\nturally correct.\nProperties (1) and (4) follow trivially from the\nstructural correctness of C. Property (2) follows\nfrom structural correctness of Crequiring that8r2\n(D(C2)]D(C3)) :k1\u0016rTo show property (3), we\n\frst use transitivity to show that 8r2D(C1) :r\u001e\nk1\u001ek2. For the remaining records, 8r2D(C2) :r\u001e\nk2follows trivially from the structural correctness of\nC. Thus PivotLeft is correct3\nCorrolary 2. PivotRight is correct.\nB LHS / RHS are meta trans-\nforms\nProof. We show only the proof for LHS ; The proof\nforRHS is symmetric. We \frst show that LHS\n3Note the limit on k1\u001ek2, which could be violated with\nan emptyC2.is an endofunctor. The kind of LHS is appropri-\nate, so we only need to show that it satis\fes the\nproperties of a functor. First, we show that LHS\ncommutes the identity ( id). In other words, for any\ninstanceC,LHS [id](C) =C. In the case where\nC=Concat (C1;C2), then\nLHS [id](C) =Concat (id(C1);C2) =Concat (C1;C2)\nThe case where typeof (C) =BinTree is identi-\ncal, and LHS [T] is already the identity in all other\ncases. Next, we need to show that LHS distributes\nover composition. That is, for any instance Cand\ntransforms T1andT2we need that\nLHS [T1\u000eT2](C) = (LHS [T1]\u000eLHS [T2]) (C)\nIfC=Concat (C1;C2),LHS [T1\u000eT2](C) =\nConcat (C0\n1;C2), whereC0\n1=T2(T1(C1)). For the\nother side of the equation:\n(LHS [T1]\u000eLHS [T2]) (C) = LHS [T2](LHS [T1](C))\n=LHS [T2](Concat (T1(C1);C2)\n=Concat (T2(T1(C1));C2)\nThe case where typeof (C) =BinTree is similar,\nand the remaining cases follow from LHS [T] =idfor\nall other cases. Thus LHS is an functor. For LHS to\nbe a meta transform, it remains to show that for any\ncorrect transform T,LHS [T] is also correct. We \frst\nconsider the case where C=Concat (C1;C2) and\nassume that T(C1) is both equivalence and structure\npreserving, or equivalently that D(C1) =D(T(C1))\nandStrCor (C1) =)StrCor (T(C1)).\nD(LHS [T](C)) = D(Concat (T(C1);C2))\n=D(Concat (C1;C2)) =D(C)\nThus, LHS [T] is equivalence preserving for this case.\nThe proof of structure preservation follows a similar\npattern\nStrCor (LHS [T](C)) = StrCor (Concat (T(C1);C2))\n=StrCor (T(C1))^StrCor (C2)\nGiven StrCor (C) =StrCor (C1)^StrCor (C2)\nand the assumption of StrCor (C1) =)\nStrCor (T(C1)), it follows that LHS [T] is structure\npreserving for this C. The proof for the case where\nC=BinTree (k;C 1;C2) is similar, but also requires\n23\n\nshowing that8r2D(T(C1)) :r\u001ekunder the as-\nsumption that8r2D(C1) :r\u001ek. This follows\nfrom our assumption that D(T(C1)) =D(C1). The\nremaining cases of LHS are covered under Proposi-\ntion 2. Thus, LHS is a meta transform.\nC Target Updates are Bounded\nProof. By recursion over T. The atomic transforms\nare the base case. By de\fnition idis not in the\nactive domain, so we only need to consider seven\npossible atomic transforms. For Sort orUnSort to\nbe in the active domain, typeof (C) must be Array\norSorted respectively. By the de\fnition of each\ntransform, typeof (C0) will be Sorted orArray\nrespectively By Theorem 2, the active domain of any\nArray orSorted instance is bounded by jAjand by\nconstruction,jWCj=jDCj\u0014jAj . Hence, the total\nchange in the weighted targets for this case is at most\n2\u0002jAj . Following a similar line of reasoning, the\nweighted targets change by at most 4 \u0002jAj elements\nas a result of any Divide ,Crack , orMerge . Next\nconsiderC = Concat (Concat (C1;C2);C3),\nand consequently C0=PivotLeft (C) =\nConcat (C1;Concat (C2;C3)). For each trans-\nform of the form LHS [LHS [T]] in the active domain\nofC, there will be a corresponding LHS [T], asC1\nis identical in both paths. Similar reasoning holds\nforC2andC3. Because the policy is local, the\nweighted targets are independent of any LHS or\nRHS meta transforms modifying them. Thus, at\nmost, the active domain will lose TandLHS [T] for\nT2A, and gain TandRHS [T] forT2A, and\nthe weighted targets will change by no more than\n4\u0002jAj elements. Similar lines of reasoning hold for\nthe other case of PivotLeft and for both cases of\nPivotRight . The recursive cases are trivial, since\nthe weighted targets are independent of pre\fxes in a\nlocal policy.\n24",
  "textLength": 77185
}