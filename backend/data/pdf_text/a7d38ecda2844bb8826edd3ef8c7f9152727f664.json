{
  "paperId": "a7d38ecda2844bb8826edd3ef8c7f9152727f664",
  "title": "Learning to Warm-Start Fixed-Point Optimization Algorithms",
  "pdfPath": "a7d38ecda2844bb8826edd3ef8c7f9152727f664.pdf",
  "text": "Learning to Warm-Start Fixed-Point Optimization\nAlgorithms\nRajiv Sambharya1, Georgina Hall2, Brandon Amos3,\nand Bartolomeo Stellato1\n1Princeton University\n2INSEAD\n3Meta AI\nSeptember 15, 2023\nAbstract\nWe introduce a machine-learning framework to warm-start fixed-point optimization\nalgorithms. Our architecture consists of a neural network mapping problem parameters\nto warm starts, followed by a predefined number of fixed-point iterations. We propose\ntwo loss functions designed to either minimize the fixed-point residual or the distance\nto a ground truth solution. In this way, the neural network predicts warm starts with\nthe end-to-end goal of minimizing the downstream loss. An important feature of our\narchitecture is its flexibility, in that it can predict a warm start for fixed-point algo-\nrithms run for any number of steps, without being limited to the number of steps it\nhas been trained on. We provide PAC-Bayes generalization bounds on unseen data\nfor common classes of fixed-point operators: contractive, linearly convergent, and av-\neraged. Applying this framework to well-known applications in control, statistics, and\nsignal processing, we observe a significant reduction in the number of iterations and\nsolution time required to solve these problems, through learned warm starts.\n1 Introduction\nWe consider parametric fixed-point problems of the form\nfindzsuch that z=Tθ(z), (1)\nwhere z∈Rpis the decision variable and θ∈Θ⊆Rdis the problem parameter defining\neach instance of (1) via the fixed-point operator Tθ. We assume that θis drawn from an\nunknown distribution Q, accessible only via samples, and that for every θ∈Θ, problem (1)\n1arXiv:2309.07835v1  [math.OC]  14 Sep 2023\n\nis a solvable ( i.e.,Tθadmits a fixed-point) convex optimization problem. Almost all convex\noptimization problems can be cast as finding a fixed-point of an operator (Ryu and Yin,\n2022), often representing the optimality conditions (Garstka et al., 2019; O’Donoghue et al.,\n2019; Stellato et al., 2020). To solve problem (1), we repeatedly apply the operator Tθ,\nobtaining the iterations\nzi+1=Tθ(zi). (2)\nWe assume that the iterations (2) converge to a fixed-point, that is, lim i→∞∥zi−z⋆(θ)∥= 0,\nwhere z⋆(θ) is a fixed point of Tθ. In practice, it is common to return an ϵ-approximate\nsolution, corresponding to a vector zifor which the fixed-point residual ,∥Tθ(zi)−zi∥2is\nbelow ϵ.Many optimization algorithms correspond to fixed-point iterations of the form (2);\nsee Table 1 for some examples.\nTable 1: Many optimization algorithms can be written as fixed-point iterations.\nAlgorithm Problem Iterates Fixed-point operator Tθ\nGradient descent min fθ(z) zi+1=zi−α∇fθ(zi) Tθ(z) =z−α∇fθ(z)\nProximal gradient descent min fθ(z) +gθ(z) zi+1=proxαgθ(zi−α∇fθ(zi)) Tθ(z) =proxαgθ(z−α∇fθ(z))\nADMM\n(Douglas and Rachford, 1956)\n(Gabay and Mercier, 1976)min fθ(u) +gθ(u)˜ui+1=proxgθ(zi)\nui+1=proxfθ(2˜ui+1−zi)\nzi+1=zi+ui−˜uiTθ(z) =z+proxfθ(2proxgθ(z)−z)\nOSQP\n(Stellato et al., 2020)min (1 /2)xTPx+cTx\ns.t. l≤Ax≤u\nwith θ= (vec(P),vec(A), c, l, u )solve Qxi+1=σxi−c+AT(ρwi−yi)\nwi+1= Π [l,u](Axi+ρ−1yi)\nyi+1=yi+ρ(Axi+1−wi+1)\nwith Q=P+σI+ρATATθ(z) where z= (x, Ax +ρ−1y)\nSCS\n(O’Donoghue, 2021)min (1 /2)xTPx+cTx\ns.t. Ax+s=b\ns∈ K\nwith θ= (vec(P),vec(A), c, b)solve Q˜ui+1=zi\nui+1= ΠC(2˜ui+1−zi)\nzi+1=zi+ui+1−˜ui+1\nwith Q=\"\nP+I AT\n−A I#\nC=Rn× KTθ(z) where zis the\ndual variable to u= (x, y)\nWe denote prox as the proximal operator (Parikh and Boyd, 2014) and vecas the vectorization operator stacking the columns of a matrix (See the\nnotation paragraph in 1 for formal definitions). See Appendix A for more information on the algorithms in this table.\nApplications. Parametric fixed-point problems arise in several applications in machine\nlearning, operations research, and engineering, where we repeatedly solve a problem of the\nform (1) with varying parameter θ. For example, in optimal control, we update the inputs\n(e.g., propeller thrusts) as sensor signals ( e.g., system state) and goals ( e.g., desired trajec-\ntory) vary (Borrelli et al., 2017, Section 7.1). Other examples include backtesting financial\nmodels (Boyd et al., 2017), power flow optimization (Hentenryck, 2021; Zamzam and Baker,\n2020), and image restoration (Elad and Aharon, 2006). In non-convex optimization, finding\na stationary point can also be cast as a fixed-point problem (Wang et al., 2019; Hong et al.,\n2016). In game theory, finding the Nash equilibrium of a multi-player game can be formu-\nlated as a fixed-point problem under some mild assumptions on the utility functions of each\n2\n\nplayer (Brice˜ no-Arias and Combettes, 2013; Ryu and Boyd, 2015). Finding fixed-points are\nalso important in other areas, such as finding the optimal policy of Markov decision pro-\ncesses (Bellman, 1957) and solving variational inequality problems (Rockafellar and Wets,\n1998; Bauschke and Combettes, 2011).\nAcceleration. In spite of the widespread use of fixed-point iterative algorithms, they are\nknown to suffer from slow convergence to high-accuracy solutions (Zhang et al., 2020). Accel-\neration schemes (Zhang et al., 2020; Walker and Ni, 2011; d’Aspremont et al., 2021; Sopasakis\net al., 2019) are an active area of research designed to speed up the solving of fixed-point\nproblems. These methods, such as Anderson acceleration (Walker and Ni, 2011; Zhang et al.,\n2020), combine past iterates to generate the next one in order to improve the convergence\nbehavior. Although acceleration methods are known to work well in certain cases, such\nas Nesterov acceleration to solve smooth, convex optimization problems, it is still an open\nresearch question to design schemes that are robust and versatile.\nLearning for optimization. Instead of designing acceleration methods for single prob-\nlems, recent approaches take advantage of the parametric structure of fixed-point problems\nencountered in practice to learn efficient solution methods. In particular, they learn algorithm\nsteps using data from past solutions (Amos, 2023; Chen et al., 2021). Despite recent suc-\ncesses in a variety of fields, e.g., in sparse coding (Gregor and LeCun, 2010; Liu et al., 2019),\nconvex optimization (Ichnowski et al., 2021; Venkataraman and Amos, 2021), and meta-\nlearning (Li and Malik, 2016; Finn et al., 2017), most of these approaches lack convergence\nguarantees because they directly alter the algorithm iterations with learned variants (Chen\net al., 2021; Amos, 2023). Although some efforts have been made to safeguard the learned\niterations (Pr´ emont-Schwarz et al., 2022; Heaton et al., 2020; Banert et al., 2021), guaran-\nteeing convergence for general learned optimizers is still a challenge. In addition, most of\nthese approaches do not provide generalization guarantees on unseen data (Chen et al., 2021;\nAmos, 2023).\nAnother data-driven approach to reduce the number of iterations is to learn warm starts\nrather than the steps of the algorithm (Chen et al., 2022; Baker, 2019). An advantage to\nlearning warm starts as opposed to algorithm steps is that this approach can be integrated\nwith existing algorithms that provably converge from any starting point. However, exist-\ning methods to learn warm starts still lack generalization guarantees. They also decouple\nthe learning procedure from the algorithm behavior after warm-starting, which can lead to\nsuboptimality and infeasibility issues on unseen problem instances.\nOur contributions. We present a learning framework that predicts warm starts for iter-\native algorithms of the form (2), which solve parametric fixed-point problems of the type\ngiven in (1). The framework consists of two modules. The first module maps the parameter\nto a warm start via a neural network, and the second runs a predefined number of steps of\nthe fixed-point algorithm. We propose two loss functions. The first one is the fixed-point\nresidual loss which directly penalizes the fixed-point residual of the output of the architec-\n3\n\nture. The second one is the regression loss which penalizes the distance between the output\nof the architecture and a given ground truth fixed-point (among possibly many).\nCompared to existing literature on learning warm starts, we train our architecture by\ndifferentiating through the fixed-point iterations. In this way, we construct warm-start pre-\ndictions that perform well after a specific number of fixed-point iteration in an end-to-end\nfashion. Furthermore, after training, our architecture allows the flexibility of selecting an\narbitrary number of fixed-point iterations to perform and is not limited to the number it\nwas originally trained on.\nBy combining operator theory with the PAC-Bayes framework (McAllester, 1998; Shawe-\nTaylor and Williamson, 1997), we provide two types of guarantees on the performance of our\nframework. First, we give bounds on the fixed-point residual when we apply our framework\nto an arbitrary number of steps, larger than the number used during training. Second, we\nprovide generalization bounds to unseen problems for common classes of operators: contrac-\ntive, linearly convergent, and averaged.\nFinally, we apply our framework to a variety of algorithms including gradient descent,\nproximal gradient descent, and the alternating direction method of multipliers. In our\nbenchmarks, we show that our learned warm starts lead to a significant reduction in the\nrequired number of iterations used to solve the fixed-point problems. We also demonstrate\ncompatibility with state-of-the-art solvers by learning architectures specifically tailored to\nSCS (O’Donoghue et al., 2019) and OSQP (Stellato et al., 2020), and inputting warm starts\ninto the corresponding C implementations.\nNotation. We denote the set of non-negative vectors of length nasRn\n+, and the set of\nvectors with positive entries of length nasRn\n++. We let the set of n×npositive semidefinite\nand positive definite matrices be Sn\n+andSn\n++respectively. We define the set of fixed-points\nof the operator T, assumed to be non-empty, as fixT. For any closed and convex set S, we\ndenote dist S:Rn→Rto be the distance function, where dist S(x) = min s∈S∥s−x∥2. For\nany set S⊂Rn, we define the indicator function IS:Rn→R∪ {+∞}where IS(x) = 0 if\nx∈SandIS(x) = +∞otherwise. We take kapplications of any single-valued operator T\nto be Tk:Rn→Rn. For any matrix A, we denote its spectral norm and Frobenius norm\nwith∥A∥2and∥A∥Frespectively. For a matrix Z∈Rm×n,vec(Z) is the vector obtained by\nstacking the columns of Z. For a symmetric matrix Y∈Sn,vec(Y) is the vector obtained\nby taking the upper triangular entries of matrix Y. We let the all-ones vectors of length n\nbe1∈Rn. The proximal operator, proxh:Rn→Rn, ofhis defined as (Parikh and Boyd,\n2014)\nproxh(v) = argmin\nx\u0010\nh(x) + (1 /2)∥x−v∥2\n2\u0011\n.\nOutline. We structure the rest of the paper as follows. In Section 2, we review some related\nwork on learned solvers. In Section 3, we present our learning to warm-start framework. In\nSection 4, we provide generalization guarantees to unseen data for our method. In Section 5,\nwe discuss choosing the right architecture, namely the choice of loss function and the number\nsteps to train on. Section 6 presents various numerical benchmarks.\n4\n\n2 Related work\nLearning warm starts. A common approach to reduce the number of iterations of itera-\ntive algorithms is to learn a mapping from problem parameters to high-quality initializations.\nSambharya et al. (2023) learn warm starts for Douglas-Rachford splitting to solve convex\nquadratic programs (QPs). While this work conducts end-to-end learning, our work is more\ngeneral in scope since we consider fixed-point problems rather than QPs. Additionally, we\nprovide generalization guarantees for more cases of operators, and finally, we add a regression\nloss. In contrast to our approach, most of the techniques to learn warm starts don’t consider\nthe downstream algorithm in the warm start prediction. Baker (2019) and Mak et al. (2023)\nuse machine learning to warm-start the optimal power flow problem. In the model predic-\ntive control (MPC) (Borrelli et al., 2017) paradigm, Chen et al. (2022) use a neural network\nto accelerate the optimal control law computation by warm-starting an active set method.\nOther works in MPC use machine learning to predict an approximate optimal solution and,\ninstead of using it to warm-start an algorithm, directly ensure feasibility and optimality.\nChen et al. (2018) and Karg and Lucia (2020) use a constrained neural network architecture\nthat guarantees feasibility by projecting its output onto the QP feasible region. Zhang et al.\n(2019) uses a neural network to predict the solution while also certifying suboptimality of\nthe output. Our paper differs from these works in that the training of the neural network we\npropose is designed to minimize the loss after many fixed-point steps, allowing us to improve\nsolution quality. Our work is also more general in scope since we consider general parametric\nfixed-point problems. Finally, we provide generalization guarantees to unseen data which\nother works lack.\nLearning algorithm steps for convex optimization. In the area of learning to opti-\nmize (Chen et al., 2021) or amortized optimization (Amos, 2023), a parallel approach to\nlearning warm starts consists in learning the algorithm steps themselves to solve convex\noptimization problems. Ichnowski et al. (2021) and Jung et al. (2022) use reinforcement\nlearning to solve quadratic programs quickly by learning high-quality hyperparameters of\nalgorithms. Venkataraman and Amos (2021) learns to accelerate fixed-point problems that\ncorrespond to convex problems quickly. One risk of some of these approaches is that conver-\ngence may not be guaranteed (Amos, 2023). To solve this problem, some works safeguard\nlearned optimizers to guarantee convergence by reverting to a fallback update if the learned\nupdate starts to diverge (Heaton et al., 2020; Pr´ emont-Schwarz et al., 2022). Other strate-\ngies guarantee convergence by making sure that the learned algorithm does not deviate too\nmuch from a known convergent algorithm (Banert et al., 2021) or by providing convergence\nrate bounds (Tan et al., 2023). In addition to convergence challenges, approaches that learn\nalgorithm steps generally do not have generalization guarantees to unseen data (Amos, 2023;\nChen et al., 2021). Lastly, these methods generally cannot interface with existing algorithms\nthat are written in C.\n5\n\nLearning algorithm steps beyond convex optimization. Many works have learned\nalgorithm steps for problems outside of convex optimization. For example, in non-convex\noptimization, Sj¨ olund and B˚ ankestad (2022) use graph neural networks (Wu et al., 2022) to\naccelerate algorithms to solve matrix factorization problems, and Bai et al. (2022) learn the\nacceleratation scheme to solve fixed-point problems quickly. The idea of learning algorithm\nsteps has ventured beyond optimization. There has been a surge in recent years to learn\nalgorithm steps to solve inverse problems , that is, problems where one wishes to recover a\ntrue signal, rather than minimizing an objective (Chen et al., 2021). This is typically done by\nembedding algorithm steps or reasoning layers (Chen et al., 2020) into a deep neural network\nand has been applied to various fields such as sparse coding (Gregor and LeCun, 2010; Liu\net al., 2019; Wu et al., 2020), image restoration (Diamond et al., 2017; Zhang et al., 2017;\nChang et al., 2017), and wireless communication (He et al., 2020; Balatsoukas-Stimming and\nStuder, 2019). A widely used technique involves unrolling algorithmic steps (Monga et al.,\n2021), meaning differentiating through these steps to minimize a performance loss. While we\nalso unroll algorithm steps, our work is different in scope since we aim to solve optimization\nproblems rather than inverse problems, and in method since we learn warm starts rather than\nalgorithm steps. Additionally, generalization and convergence remain issues in the context\nof learning to solve inverse problems (Chen et al., 2021; Amos, 2023).\nLearning surrogate optimization problems. Instead of solving the original parametric\nproblem, several works aim to learn a surrogate model of large optimization problems. Then,\nan approximate solution can be obtained by solving the simpler or smaller optimization\nproblem. For instance, Wang et al. (2020) learn a mapping to reduce the dimensionality of\nthe decision variables in the surrogate problem. Li et al. (2023) use a neural approximator\nwith reformulation and relaxation steps to solve linearly constrained optimization problems.\nOther works predict which constraints are active (Misra et al., 2022) and the value of the\noptimal integer solutions (Bertsimas and Stellato, 2021, 2019). In contrast, our approach\nrefrains from approximating any problem; instead, we warm-start the fixed-point iterations.\nThis allows us to clearly quantify the suboptimality achieved within a set number of fixed-\npoint iterations.\nMeta-learning. Meta-learning (Hospedales et al., 2020; Vilalta and Drissi, 2001; Ruder,\n2017) or learning to learn overlaps with the learning for optimization literature when the\ntasks are general machine learning tasks (Chen et al., 2021). A wide array of works learn\nthe update function to gradient-based methods to speed up machine learning tasks with a\nvariety of techniques including reinforcement learning (Li and Malik, 2016), unrolled gradient\nsteps (Andrychowicz et al., 2016), and evolutionary strategies (Metz et al., 2022). More in\nthe spirit of our work, Finn et al. (2017) learn the initial model weights so that a new task can\nbe learned after only a few gradient updates. While the initialization of the model weights\nfor their method is shared across the tasks, in our method we, instead, predict the warm\nstart from the problem parameter. This tailors our initialization to the specific parametric\nproblem under consideration.\n6\n\nAlgorithms with predictions. Another area that uses machine learning to improve al-\ngorithm performance is algorithms with predictions (Mitzenmacher and Vassilvitskii, 2020;\nKraska et al., 2018; Khodak et al., 2022). Here, algorithms take advantage of a possibly im-\nperfect prediction of some aspect of the problem to improve upon worst-case analysis. This\nidea has been applied to many problems such as ski-rental (Purohit et al., 2018), caching (Ro-\nhatgi, 2020), and bipartite matching (Dinitz et al., 2021). Even though the prediction can be\nused to improve the warm start for algorithms (Dinitz et al., 2021; Sakaue and Oki, 2022),\nthe task we consider is fundamentally different since we aim to solve parametric problems\nas quickly as possible rather than to take advantage of a prediction.\nGeneralization guarantees. The generalization guarantees we provide use a PAC-Bayes\nframework, which has been used in prior work in the amortized optimization setting (Gupta\nand Roughgarden, 2017; Bartlett et al., 2022). Chen et al. (2020) provide generalization\nguarantees for architectures with reasoning layers, using a local Rademacher complexity\nanalysis. However, to the best of our knowledge, generalization guarantees have not been\nobtained with methods that aim to solve fixed-point problems quickly. Additionally, the\nbounds from these works mentioned above are obtained in methods where the algorithm\nsteps are learned rather than the warm start. Unlike Sambharya et al. (2023) which focused\non solving QPs, we obtain guarantees in the non-contractive case by using the PAC-Bayes\nframework rather than Rademacher complexity theory.\n3 Learning to warm-start framework\nWe now present our learning framework to learn warm starts to solve the parametric fixed-\npoint problem (1). A key feature of our framework is the inclusion of a predefined number of\nfixed-point steps within the architecture. In this way, the warm-start predictions are tailored\nfor the downstream algorithm, and we conduct end-to-end learning. The section is organized\nas follows. In Section 3.1, we provide intuition as to why learning end-to-end can be beneficial\nthrough a small illustrative example. In Section 3.2, we describe our architecture, and in\nSection 3.3 we introduce the two different loss functions we consider. A concise summary of\nthese aspects is depicted in Figure 2.\n3.1 An illustrative example\nTo build intuition, we provide a two-dimensional example that illustrates the importance of\ntailoring the warm-start prediction to the downstream algorithm. Consider the problem,\nminimize (1 /2)zTQz\nsubject to z≥0,(3)\nwhere Q=diag(10,1). We solve problem (3) using proximal gradient descent (see Table 1)\nwith the iterates\nzi+1= Π( zi−α∇f(zi))\n7\n\nwhere ∇f(z) =Qz,α∈R++is picked to get the fastest worst-case convergence rate (Ryu\nand Boyd, 2015), and Π is the projection onto the non-negative orthant. The optimal solution\nfor problem (3) is at the origin, and we consider three different warm starts shown in Figure 1.\nAll three are equidistant to the optimal solution, but lead to different convergence behavior.\nThe purple warm start has the fastest convergence since the projection step clips non-negative\nvalues to zero. The orange warm start converges more quickly than the green warm start due\nto the difference in scaling of the objective function along each axis. This results in faster\nconvergence for the orange warm start compared with the green one since the orange warm\nstart is closer to the z1axis. This example shows the necessity of considering the downstream\nalgorithm when choosing a warm start. All three warm starts in this case appear of equal\nquality as they are equidistant from z⋆, but when considering the downstream algorithm,\nthere is a clear hierarchy in terms of convergence speed: purple takes the lead, followed by\norange, then green.\nz⋆\n0 20 40\nevaluation iterations10−410−2100ﬁxed-point residual\nFigure 1: The iterates of proximal gradient descent to solve problem (3) with different warm\nstarts. For three different warm starts equidistant to the optimal solution z⋆, we plot the\nfirst 5 iterates on the left. The contour lines of the objective function are in blue and the\ninfeasible region is shaded in pink. We plot the fixed-point residuals for the different warm\nstarts on the right. Depending on the warm start, the convergence to the optimal solution,\ncan vary greatly.\n3.2 Learning to warm-start architecture\nOur learning architecture consists of two modules, a neural network with Llayers and k\niterations of operator Tθ; see Figure 2. The neural network uses ReLU activation functions\ndefined as ϕ(z) = max(0 , z) element-wise. We let w={Wi}L\ni=1be the neural network weights\nfor each layer where Wi∈Rmi×ni. Our warm-start prediction is computed as\nhw(θ) =WLϕ(WL−1ϕ(. . . ϕ(W1θ))). (4)\nWhile we do not explicitly represent bias terms, we can include them by appending a new\ncolumn to matrices Wifori= 1, . . . , L, and a 1 to the input vector. The warm-start\nprediction hw(θ)∈Rpfeeds into the fixed-point algorithm parametrized by θ. The second\n8\n\n✓<latexit sha1_base64=\"fl5E5l1QKA2L2PdkyWn9P8sloqw=\">AAACAnicbZC7SgNBFIZn4y3G26qV2AwGIWnCrgS1DNikjJAbJOsyO5lNhsxemDmrhCXY+Co2ForY+hR2vo2TZAtN/GHg4z/ncOb8Xiy4Asv6NnJr6xubW/ntws7u3v6BeXjUVlEiKWvRSESy6xHFBA9ZCzgI1o0lI4EnWMcb38zqnXsmFY/CJkxi5gRkGHKfUwLacs2T5t3YTfswYkCmpZH7UFpwueyaRatizYVXwc6giDI1XPOrP4hoErAQqCBK9WwrBiclEjgVbFroJ4rFhI7JkPU0hiRgyknnJ0zxuXYG2I+kfiHguft7IiWBUpPA050BgZFars3M/2q9BPxrJ+VhnAAL6WKRnwgMEZ7lgQdcMgpiooFQyfVfMR0RSSjo1Ao6BHv55FVoX1Tsy0r1tlqs1bM48ugUnaESstEVqqE6aqAWougRPaNX9GY8GS/Gu/GxaM0Z2cwx+iPj8weTpJbs</latexit>Tk✓(hw(✓))\n<latexit sha1_base64=\"nB4c06tN2e83H1lxyaIBX4NeevE=\">AAAB/3icZVDLSgMxFM3UV62vqks3g0VwY5kpotuCm66kgn1AO5RMetuGJpmQZIpl6MKfcKs7d+LWT3Hjt5hpZ+HYA4HDuTknJzeUjGrjed9OYWNza3unuFva2z84PCofn7R1FCsCLRKxSHVDrIFRAS1DDYOuVIB5yKATTu/SeWcGStNIPJq5hIDjsaAjSrCxUh8rMqEGiIkVDMoVr+ot4a4TPyMVlKE5KP/0hxGJOQhDGNa653vSBAlWhhIGi1I/1iAxmeIx9CwVmIMOkmXnhXthlaE7ipQ9wrhLNecYzqjUmedpZfo7X+WYkOVVzLWe89Dmc2wmOpcooqtUDJL0RS2BWKsGwzEVqZI0gM3AdsfuPcS2v12I///766Rdq/o31euHWqXeyFZTRGfoHF0iH92iOmqgJmohgiR6Qa/ozXl23p0P53N1teBknlOUg/P1C1IJmIU=</latexit>architecture<latexit sha1_base64=\"owknPdFlILBlKeAtEXvnbfQ5o5o=\">AAACAHicZVDLTsJAFL3FF+ILdemmEUxwIWmJ0S2JG1YGE3kk0JDpcIEJ02ntTFFC2PgTbnXnzrj1T9z4LU6hCysnmeTk3Dlnzlw34Ewqy/o2MmvrG5tb2e3czu7e/kH+8Kgp/Sik2KA+98O2SyRyJrChmOLYDkIknsux5Y5v4nlrgqFkvrhX0wAdjwwFGzBKlJac4qj3WOqqESpyXuzlC1bZWsBcJXZCCpCg3sv/dPs+jTwUinIiZce2AuXMSKgY5TjPdSOJAaFjMsSOpoJ4KJ3ZovTcPNNK3xz4oT5CmQs15ehPWCATz9PS9He+zFEuT6vEk3LquTrfI2okU4nCv4hFZxa/KAOk2ipReYSJWJnVkE9QdyfmLUa6v16I/f/7q6RZKdtX5cu7SqFaS1aThRM4hRLYcA1VqEEdGkDhAV7gFd6MZ+Pd+DA+l1czRuI5hhSMr19TnpfV</latexit>hw(✓)<latexit sha1_base64=\"Awd5odlEPaGexe43BgqpJsJIqQI=\">AAACAHicZVC7TsMwFHXKq5RXgZElokJioUoqBGsllk6oSPQhtVHluLfFqu2EXKeiirrwE6ywsSFW/oSFb8FpMxB6JEvnnutzfH39UHDUjvNtFdbWNza3itulnd29/YPy4VEbgzhi0GKBCKKuTxEEV9DSXAvohhFQ6Qvo+JObtN+ZQoQ8UPd6FoIn6VjxEWdUG8kTAaI9ihVLq0G54lSdBexV4makQjI0B+Wf/jBgsQSlmaCIPdcJtZfQSHMmYF7qxwghZRM6hp6hikpAL1kMPbfPjDK0R0FkjtL2Qs05hlMeYuZ5Wpr+9pc52hd5lUrEmfRNvqT6AXOJKrhIRS9JX8QQmLEiaEm5SpWkAWIKZnZq30Js5jcLcf9/f5W0a1X3qnp5V6vUG9lqiuSEnJJz4pJrUicN0iQtwsgjeSGv5M16tt6tD+tzebVgZZ5jkoP19QvlVpjT</latexit>loss function<latexit sha1_base64=\"NaeEz/vaURfEUOVkyI7VCN5qrVo=\">AAADHXicZVLLbtNAFJ2YVymvFJZsLNJKiVQiO6qAZaVusikqUtNWqkM0Ht/Eo8zDnbnOQyP/Ar/AT7CFHTvEFrHgX7CTSJXTuzpzzpz70L1xJrjFIPjb8O7df/Dw0c7j3SdPnz1/0dx7eWF1bhgMmBbaXMXUguAKBshRwFVmgMpYwGU8Pan0yxkYy7U6x2UGQ0knio85o1hSo2Y7AiFGLsIUkBbt88/T20c6mrfXuNPpjJqtoBuswr8Lwg1okU2cjfYa/6JEs1yCQiaotddhkOHQUYOcCSh2o9xCRtmUTuA6mfHMKirBDt1iNVVNdysKY1FnqbR2KePCP5AUU7utxXZZ+P5BrEVS6dtyxR36JcBU3qYwoGDOtJRUJS66gaSqoEXh9qNYlGZ7k1MD+/VGKi9qLWx9qhzHH4aOqyxHUKxuibWeIo3tocwFcqPnpWwBmc4VgnGndHFK0fDFSZXV9YJiuzVqDF1aNKUnLVzY7dVrK/226mroxlqhzYCt80vKVcW4PogZlJug/kfIYS0KUBNMXZRRw1VS7q1wAZP1vkHlkiPIojyIcHv9d8FFrxu+6x59Omod9zensUNekzekTULynhyTPjkjA8LIF/KNfCc/vK/eT++X93v91WtsPK9ILbw//wHtgwk9</latexit>`✓(Tk✓(hw(✓)))\n<latexit sha1_base64=\"pni7zQUZlPdJTQ4Yi64scDHwy3g=\">AAADAHicZVJNb9NAEN2Yr1I+2sKRi0VaiUOJ7KgCjpV6yaWoSKStVFvR7noSr7Ifzu44TbTyhT/BFW7cEFf+CQf+C3YSCTmd09N7+2beaoYVUjiMoj+d4N79Bw8f7TzeffL02fO9/YMXl86UlsOQG2nsNaMOpNAwRIESrgsLVDEJV2x61uhXc7BOGP0ZlwWkik60GAtOsabSBKQc+QRzQFqN9rtRL1pVeBfEG9Alm7oYHXT+JpnhpQKNXFLnbuKowNRTi4JLqHaT0kFB+ZRO4Cabi8JpqsClfrEK3tL9ikIm2yxVzi0Vq8IjRTF32xpzyyoMj5iRWaNvyw13HNYAc/W/hQUNt9woRXXmkxlkzQQjK3+YMFmb3aykFg7bQRovGiNd+1cljj+kXuiiRNC8bWHGTJEyd6xKicKa21p2gNyUGsH6c7o4p2jF4qzp6vtRtR2NWkuXDm3tySsf9/rt2dq8bVKlfmw0ugL4ur+iQjeMH4CcQ70JGn6EEtaiBD3B3CcFtUJn9d4qH3HVzg26VAJBNQcRb6//Lrjs9+J3vZNPJ93TweY0dsgr8pq8ITF5T07JgFyQIeFkRr6Sb+R78CX4EfwMfq2fBp2N5yVpVfD7H8kf/mY=</latexit>`✓\n<latexit sha1_base64=\"NMD6KDX4V5NHRZppgEYSM0kFLWo=\">AAAFoHiclVRbT9swGA2MbozdYHvcS0aLBBNDTUG7vKGBNCaNARM3jZTKSb+kXh072F9oi+Xn/Zq9br9l/2ZOWy5p4WF+iOxzvnN8HMtfkDKqsFr9OzF5b6p0/8H0w5lHj588fTY79/xQiUyGcBAKJuRxQBQwyuEAKTI4TiWQJGBwFLQ3cv7oHKSigu9jL4V6QmJOIxoStFBj9tVXyCRhLgfsCNl2fb9DseV2gMYtVG6lU2nMlqsr1f5wxyfecFJ2hmO3MTcV+00RZglwDBlR6sSrpljXRCINGZgZP1OQkrBNYjhpntNUcZKAqutu/zgFXvchDFgRJYlSvSQw7kJCsKVGuUD1jOsuBII1c36UzrFl106wlVxbSODQCUWSEN7U/hk08x0EM7riB8yK1VlGJFSKQXItCsFU8VQZRu/rmvI0Q+BhURII0UYSqOUkY0il6FhaAYYi4whSb5PuNkFJuxu5q65VzWg0IiXpKZRW0zLaW6kV9+biTZ6qriPBUaUQDvwTQnmO6C1g52Bvgrj26mFAMuAxtrSfEkl5096b0dUwKeYGniUUIUdvpkml6BrtI3QxiHR/NVIRoY0ZNWxNC5CMsnHOxnexIjV6/y7yi2ibE69+clHXPjB2VXaqfZnoKDVm8Vp72l4se0tLYxb0Vo8GvaG1i7vU+B8B8NJiE+yzkLA5fCMbl3b7P4x2d3Tb2M9F/jGuPf2pLnvG/xwdEpbBvi6vmqtc+cLoxXJtqfhrXtv9oZlH04Ob6b8inWO52VhxYJ3HqvvgZXkTIttgBkQkJCiMJQA3+tunj0avri273uqHZXd1bdR6UDbqXbC4LZGQhMfjmYbwUDJjW5M32ojGJ4e1Fe/tytperby+NWxS085LZ95ZdDznnbPubDm7zoETOj+dX85v509pvrRV2intDUonJ4aaF05hlL7/A8ad/6I=</latexit>Neural networkwith weightsw<latexit sha1_base64=\"yn7YVmFuYrrzMYbiudifqlMjYzY=\">AAAF/3iclVTLbtw2FFUek4fbJE667IboOIBdTAbSPGJ7Z8QB6gJ5tfAjgDUeUNLVDDMUqZBX9tgEF/2D/kV2Qbdd9EOyzrb5h1Az49ga24twQZHn3Ht4KJI3yjnT6Pufrl2/cbN26/aduws//Hjv/oPFh492tSxUDDux5FK9jagGzgTsIEMOb3MFNIs47EWjzZLfOwSlmRTbeJxDL6MDwVIWU3RQf3EvRBjjRMdEvAANGFjzAqgS5IjhkCyFgkac9o9ICJz3TYhDQGqXCA6VLAZD9wWSsjEkT3LJBBKNkGvbX6z7zW7Q9VfXiRust1c7vhusr3bXOi0SNP1Jq3uz9qb/8ObfYSLjIgOBMada7wd+jj1DFbKYg10ICw05jUd0APvJIcu1oBnonhlPzFd4M4Ew4lWUZlofZ5EljzOKQz3PRfrYEvI4kjwp+Xm6xBrEDXCYnUkoEHAUyyyjIjHhe0jKFSS3Zil0fy0e6fcFVbBUNVLmopRcV3dVYLrWM0zkBYKIqymRlCOkkW5kBUem5JGj3VnFshAIyryk45cUFRtvlqqm5dt5a1QpeqxRuZyhNUGzVV1byCelq55JpUCdQzzVzygTJWK2gB+COwlKXkEBU5KDGODQhDlVTCTu3Kzx46zqG0SRMYQSPe8mV3JszeTuRamZzOYiUnQ202/3bY4dlOzgKlbm1mxfRb6QI7sf9PZPeqZyow9MqDKT5tYun+UejJbrwcrKBQl2qUafnct1k6uy8TsM4KnEc3DPQsHz2RvZPJXbfmcNeW1G1nUnZWeJ2/2BqQc2/D3dpe5Rb5t6237zVU6sWa63Vqq/5le3PiSlNWPOVYUSK8UuBJfl4kL0BDwNTyB1ZWlKpFKBxoECENb8+dsza9qdBgna6w3S7sxLT8PmtSsSlzmSiorBRU8z+DJXZyVvYqnbbQStp41gzTlaWHA17LRQkasHu61m8LTZ+aNV39iaVbM73s/eL96yF3ir3oa35b3xdrzY+8/77P3vfan9VftQ+1j7Zxp6/dos5yev0mr/fgWTKyZt</latexit>Learn withrw`✓through the ﬁxed-point steps\n<latexit sha1_base64=\"arNnKfotM/9deEFJ5Xz0iWoyzws=\">AAADKHicZVI9bxNBEF0fX8F8OVDSnLAjUSTWnUEJZaQ0boKChJNIOcva3RvbK+/HZXfOsbW6H8Jf4E/QQkeHUtDwS7izjdA5U43emzfzZmdZJoXDKLptBPfuP3j4aOdx88nTZ89ftHZfnjuTWw4DbqSxl4w6kELDAAVKuMwsUMUkXLDZScVfzME6YfRnXGYwVHSixVhwiiU0ar1LGEyE9hw0gi2anVknHIsFpAdJkhmhMXQImWsmoNN/RaNWO+pGqwjvJvEmaZNNnI12G7+T1PBclXouqXNXcZTh0FOLgksomknuIKN8Ridwlc5F5jRV4IZ+sVqwxvsVhEzWUaqcWypWhHuK4tRtc8wtizDcY0amFb9NV9h+WCY4Vf9bWNBww41StNw9uYa0mmBk4TsJk6XYXefUQqdupNKiMdLVt8px/GHohc5yBM3rEmbMDClz+yqXKKy5KWkHyE1ePbc/pYtTilYsTqquvhcV29aotXTp0JaaaeHjbq8+W5uDytXQj41GlwFf91dU6ArxfZBzKC9Bw4+Qw5qUoCc49UlGrdBpebfCR1zVfYPOlUBQ1YeIt89/NznvdePD7vtPvfZxf/M1dshr8oa8JTE5IsekT87IgHDyhXwj38mP4GvwM/gV3K5Lg8ZG84rUIvjzF0cGDfk=</latexit>kﬁxed-point stepsFigure 2: Illustration of the learning framework. The architecture consists of two modules: a\nneural network mapping the parameter θto a warm start hw(θ), and a second module execut-\ningkfixed-point iterations starting from hw(θ) to obtain the candidate solution Tk\nθ(hw(θ)).\nThe fixed-point steps in the architecture depend on the parameter θ, and have no learnable\nweights. There are two options for the loss function ℓθ: the fixed-point residual loss ℓfp\nθ, or\nthe regression loss ℓreg\nθ. We backpropagate from the loss through the fixed-point iterates to\nlearn the neural network weights w.\npart of our architecture consists of kapplications of the operator Tθto the warm start hw(θ).\nThe final output is the candidate solution Tk\nθ(hw(θ)).\n3.3 Loss functions\nTraining for ksteps. We propose two loss functions to analyze the output of our learning\nto warm-start architecture, Tk\nθ(hw(θ)). The first one is the fixed-point residual loss\nℓfp\nθ(z) =∥Tθ(z)−z∥2, (5)\nwhich measures of the distance to convergence of the fixed-point algorithm (2) (Ryu and\nYin, 2022, Section 2.4). The second one is the regression loss\nℓreg\nθ(z) =∥z−z⋆(θ)∥2, (6)\nwhere z⋆(θ) is a known (possibly non-unique) fixed-point of Tθ. The learning problem is\nminimize Eθ∼Qℓθ(Tk\nθ(hw(θ))), (7)\nwhere ℓθis either ℓfp\nθorℓreg\nθ, and kis the number of fixed-point iterations in our architecture.\nNote that choosing k= 0 decouples the learning procedure from the downstream algorithm,\nthereby making our architecture no longer end-to-end.\nIt is generally infeasible to evaluate the objective in problem (7) because the distribu-\ntionQis unknown. Instead, we minimize its empirical estimate over training data hoping\nto attain generalization to unseen data. We leverage stochastic gradient descent (SGD)\nmethods to efficiently train the neural network weights, by constructing stochastic approx-\nimations of the gradient of the empirical risk (Sra et al., 2011). To compute such gradient\n9\n\nestimates, we use automatic differentiation (Baydin et al., 2017) techniques to differentiate\nthrough the kfixed-point iterations. We note that due to the inclusion of ReLU layers and\nprojection steps in the fixed-point algorithms ( e.g., the projection step in OSQP), there are\nnon-differentiable mappings in the architecture. At non-differentiable points, SGD uses sub-\ngradients (Rockafellar and Wets, 1998) to estimate directional derivatives of the loss. By\ntailoring the warm-start prediction to the downstream fixed-point algorithm, our framework\nconstitutes an end-to-end learning scheme.\nTesting for tsteps. We now evaluate the learned model with tfixed-point iterations ( t\npossibly different from kused during training) on an unseen parameter θ. While we consider\ntwo different loss functions for training, we always measure the test performance on unseen\nproblems by the fixed-point residual since it is a standard measure of progress (Ryu and Yin,\n2022, Section 2.4). To analyze the generalization of our architecture, we define the riskas\nthe following function of t:\nRt(w) =Eθ∼Qℓfp\nθ(Tt\nθ(hw(θ))). (8)\nSince we only access the distribution QviaNsamples θ1, . . . , θ N, we define the empirical\nriskover training data as\nˆRt(w) =1\nNNX\ni=1ℓfp\nθi(Tt\nθi(hw(θi))). (9)\n4 PAC-Bayes generalization bounds\nIn this section, we provide generalization bounds for our approach using the PAC-Bayes\nframework (Shawe-Taylor and Williamson, 1997; McAllester, 1998). More specifically, we\nprovide a generalization guarantee on the risk in Equation (8) after any number of evaluation\nsteps t(tneed not be equal to the number of fixed-point steps ktaken during training).\nFirst, we introduce preliminary results and definitions needed for our proofs in Sec-\ntion 4.1. In particular, we define the marginal fixed-point residual , a key ingredient of our\nproof technique, which measures the maximum fixed-point residual incurred by a warm start\nwhen subjected to a bounded perturbation. Then, we derive our main generalization bound\nresult, Theorem 2, in Section 4.2. Finally, in Section 4.3, we specialize Theorem 2 to three\ndifferent cases of operators: contractive, linearly convergent, and averaged.\n4.1 Preliminaries\nIn this subsection, we introduce our marginal fixed-point residual in Equation (10) and\nMcAllester’s bound in inequality (11).\nMarginal fixed-point residual. We define the marginal fixed-point residual to be the\nworst-case fixed-point residual for a warm start subjected to a bounded perturbation:\ngt\nγ,θ(z) = max\n∥∆∥2≤γℓfp\nθ(Tt\nθ(z+ ∆)) . (10)\n10\n\nSimilarly, we define the marginal risk and marginal empirical risk in the same way as for the\nnon-marginal case from Section 3 with\nRt\nγ(w) =Eθ∼Qgt\nγ,θ(hw(θ)) and ˆRt\nγ(w) =1\nNNX\ni=1gt\nγ,θi(hw(θi)).\nSetting γ= 0 corresponds to the original fixed-point residual and risk functions, i.e.,gt\n0,θ(z) =\nℓfp\nθ(Tt\nθ(z)),ˆRt\n0(w) =ˆRt(w),andRt\n0(w) =Rt(w) from Equations (8) and (9).\nMcAllester’s bound. The PAC-Bayesian framework provides generalization bounds ran-\ndomized predictors, as opposed to a learned single predictor. Randomized predictors are\nobtained by sampling in a set of basic predictors based on a specific probability distribu-\ntion (Alquier, 2023). This is especially useful in our setting because we can manipulate the\nbounds on the randomized predictors into bounds on our learned predictors.\nIn our case, hwfrom Equation (4) corresponds to the fixed warm-start prediction param-\neterized by the weights of the neural network w∈ W where Wis a set of possible weights.\nWe aim to bound Rt(w), the risk after tfixed-point steps from Equation (8), in terms of\nempirical quantities. To do so, we consider perturbations of the neural network weights given\nby the random variable uwhose distribution may also depend on the training data. Now,\nwe have a distribution of predictors hw+u, where wis fixed and uis random. Given a prior\ndistribution πover the set of predictors that is independent of the training data, the expected\nmarginal risk of the randomized predictor Eu[Rt\nγ(w+u)] can be bounded as (McAllester,\n2003)\nEu[Rt\nγ(w+u)]≤Eu[ˆRt\nγ(w+u)] + 2 Cγ(t)r\n2(KL( w+u||π) + log(2 N/δ))\nN−1, (11)\nwith probability at least 1 −δ. Here KL( p||π) is the KL-divergence between the distributions\np=w+uandπ,\nKL(p||π) =Z∞\n−∞p(x) log\u0012p(x)\nπ(x)\u0013\n.\nThe quantity Cγ(t) upper bounds the fixed-point residual after tsteps, i.e.,\ngt\nγ,θ(hw(θ))≤Cγ(t),∀θ∈Θ, w∈ W.\nNote that the marginal risk and empirical marginal risk lie in the range [0 , Cγ(t)]. In order\nto bound Cγ(t), we will consider predictors where the distance from warm start to the set of\nfixed-points is upper bounded by D:\ndist fixTθ(hw(θ))≤D∀θ∈Θ, w∈ W. (12)\nIn Section 4.3, we bound Cγ(t) in terms of t,γ,D, and properties of the operator Tθ.\n11\n\n4.2 Generalization bounds\nIn this subsection, we use the marginal fixed-point residual and the McAllester bound\nfrom Section 4.1 to bound the generalization gap. We first transform the McAllester bound in\n(11), which provides a generalization bound on the expected marginal risk of the randomized\npredictor, to a bound on the risk with the following lemma.\nLemma 1. Lethw: Θ→Rpbe any predictor to a warm start learned from the training\ndata such that gt\nγ/2,θ(hw(θ))≤Cγ/2(t),∀θ∈Θ. Let hwbe any learned predictor parametrized\nbywandπbe any distribution that is independent from the training data. Then, for any\nδ, γ > 0, with probability at least 1−δover a training set of size Nand for any random\nperturbation usuch that P(max θ∈Θ∥hw+u(θ)−hw(θ)∥2≤γ/2)≥1/2we have\nRt(w)≤ˆRt\nγ(w) + 4Cγ/2(t)r\nKL(w+u||π) + log(6 N/δ)\nN−1.\nSee Appendix B.1 for the proof. In the above expression, wis fixed and uis a random\nvariable. This lemma bears resemblance to Neyshabur et al. (2018, Lemma 1), and the proof\nis nearly identical. Next, we use Lemma 1 to obtain generalization bounds with our main\ntheorem.\nTheorem 2. Assume that ∥θ∥2≤Bfor all θ∈Θ. Let hw: Θ→Rpbe an L-layer\nneural network with ReLU activations where gt\nγ/2,θ(hw(θ))≤Cγ/2(t),∀θ∈Θ. Let c=\nB2L2¯hlog(L¯h)ΠL\nj=1∥Wj∥2\n2PL\ni=1∥Wi∥2\nF/∥Wi∥2\n2and let ¯h= max inibe the largest number of\noutput units in any layer. Then for any δ, γ > 0with probability at least 1−δover a training\nset of size N,\nRt(w)≤ˆRt\nγ(w) +\n\nO \nCγ/2(t)q\nc+log(LN\nδ)\nγ2N!\nifΠL\nj=1∥Wj∥2≥γ\n2B\nCγ/2(t)q\nlog(1 /δ)\n2Nelse.(13)\nSee Appendix B.2 for the proof. With Theorem 2, we bound the risk in terms of the\nempirical marginal risk and a penalty term. The main case is when the weights are sufficiently\nlarge: ΠL\nj=1∥Wj∥2≥γ/(2B). In this case, we use the PAC-Bayesian framework to provide\nthe generalization bound. We directly use the perturbation bound from Neyshabur et al.\n(2018, Lemma 2) which bounds the change in the warm start hw(θ) with respect to the\nchange in the neural network weights w. In the other case, if ΠL\nj=1∥Wj∥2≤γ/(2B), then\nthe warm start hw(θ) is close to the zero vector. Here, we leverage Hoeffding’s inequality to\nget the generalization bound.\nAst→ ∞ , the generalization gap in Theorem 2 approaches zero since Cγ/2(t) goes to\nzero. Intuitively, this happens because the algorithm is run until convergence. On the other\nhand, as N→ ∞ , the second term in each of the cases disappears and the generalization\ngap becomes the difference between the marginal empirical risk and the risk for a fixed γ.\nOur bounds also generalizes the setting where the warm start is not learned. Setting all of\nthe weights to zero corresponds to warm-starting every problem from the zero vector. In\nthis case, with high probability, Rt(0)≤ˆRt(0) + Cγ/2(t)p\nlog(1 /δ)/(2N).\n12\n\n4.3 Bounding the empirical marginal risk\nTheorem 2 bounds the risk Rt(w) in terms of the empirical marginal risk ˆRt\nγ(w) plus a\npenalty term. In this subsection, we use operator theory to bound two things: i) ˆRt\nγ(w),\nthus removing the dependency on the marginal component, and ii) Cγ/2(t) in terms of D\ngiven by Equation (12). We first assume that the operator Tθis non-expansive, which is a\ncommon characteristic of solving convex problems (Ryu and Boyd, 2015).\nDefinition 4.1 (Non-expansive operator) .An operator Tis non-expansive if\n∥Tx−Ty∥2≤ ∥x−y∥2,∀x, y∈dom T.\nSince non-expansiveness is not enough to guarantee convergence, we break our analysis\ninto three different cases of fixed-point operators which converge: contractive in Section 4.3.1,\nlinearly convergent in Section 4.3.2, and averaged in Section 4.3.3. By using the different\nproperties of each, we can bound the marginal fixed-point residual after tsteps, gt\nγ,θ(z)\ndefined in (10). Since the empirical marginal risk is the average of these marginal fixed-\npoint residuals, we can remove the dependence on the empirical marginal risk in Theorem 2.\nThe sets of the three different types of operators are not mutually exclusive as seen in the\nset relationships depicted in Figure 3. The contractive case provides the strongest bounds,\nfollowed by the linearly convergent case, and then the averaged case.\nLinearly  convergentAveragedContractive\nFigure 3: The set relationship between the different types of operators we consider in this\nsection.\nTo help in the subsequent analysis, we define the following functions which give the\ndistance to optimality and marginal distance to optimality:\nrθ(z) =dist fixTθ(z), ft\nγ,θ(z) = max\n∥∆∥2≤γrθ(Tt\nθ(z+ ∆)) . (14)\nWe give the following lemma to relate the fixed-point residual to the distance to optimality.\nLemma 3. For any non-expansive operator, Tθ,\nℓfp\nθ(z)≤2rθ(z).\nSee Appendix B.3 for the proof.\n13\n\n4.3.1 Contractive operators\nWe first consider contractive operators which give the strongest perturbation bounds.\nDefinition 4.2 (β-contractive operator) .An operator Tisβ-contractive for β∈(0,1) if\n∥Tx−Ty∥2≤β∥x−y∥2∀x, y∈dom T.\nIfTθisβ-contractive, then\ngt\nγ,θ(z)≤ℓfp\nθ(Tt\nθ(z)) + 2 βtγ, (15)\nwhich follows from ℓfp\nθ(Tt\nθ(·)) being 2 βt-Lipschitz (Sambharya et al., 2023, Appendix A.1). In\nthe contractive case, we remove the marginal risk dependency with the following corollary.\nCorollary 4. We define Band¯has in Theorem 2. Let Tθbeβ-contractive for any θ∈Θ.\nLethwbe an L-layer neural network with ReLU activations such that (12) holds with bound\nD. Let c=B2L2¯hlog(L¯h)ΠL\nj=1∥Wj∥2\n2PL\ni=1∥Wi∥2\nF/∥Wi∥2\n2. Then for any δ, γ > 0with\nprobability ≥1−δover a training set of size N,\nRt(w)≤ˆRt(w) + 2βtγ+\n\nO \nβt(D+γ\n2)q\nc+log(LN\nδ)\nγ2N!\nifΠL\nj=1∥Wj∥2≥γ\n2B\n2βt(D+γ\n2)q\nlog(1 /δ)\n2Nelse\nProof. We remove the marginal dependence by applying inequality (15) to get\nˆRt\nγ(w) =1\nNNX\ni=1gt\nγ,θ(hw(θi))≤2βtγ+1\nNNX\ni=1ℓfp\nθ(Tt\nθ(hw(θi))) = ˆRt(w) + 2βtγ.\nWe bound the worst-case fixed-point residual as Cγ/2(t)≤2βt(D+γ/2) which comes from\nC0(t)≤2βtD(Sambharya et al., 2023, Appendix A.1) and the inequality\ndist fixTθ(hw(θ) + ∆) ≤ ∥ΠfixTθ(hw(θ))−(hw(θ) + ∆) ∥2≤dist fixTθ(hw(θ)) +∥∆∥2.(16)\nHere, Π fixTθis the projection on the set fixTθ. The first inequality in (16) uses the definition\nof the distance function dist fixTθ, and the second uses the triangle inequality. ■\n4.3.2 Linearly convergent operators\nNow, we consider a broader category of operators, linearly convergent operators.\nDefinition 4.3 (β-linearly convergent operator) .An operator Tisβ-linearly convergent for\nβ∈[0,1) if\ndist fixT(Tx)≤βdist fixT(x)∀x∈dom T.\n14\n\nIf the operator, Tθ, is not contractive, then we get the weaker property that ℓfp\nθ(Tt\nθ(·)) is\n2-Lipschitz. To provide tighter perturbation bounds, we first establish the following lemma.\nLemma 5. For any non-expansive operator Tθand for any t≥0,\n|rθ(Tt\nθ(z))−rθ(Tt\nθ(w))| ≤2∥z−w∥2.\nSee Appendix B.4 for the proof. We now use Lemma 5 and the linear convergence\nguarantee, Definition 4.3, to bound gt\nγ,θin terms of empirical quantities.\nLemma 6. Assume that Tθisβ-linearly convergent where β∈(0,1). Then the following\nbounds hold for all t≥0:\nft\nγ,θ(z)≤rθ(Tt\nθ(z)) + 2 γ, ft+1\nγ,θ(z)≤βft\nγ,θ(z)\ngt\nγ,θ(z)≤2ft\nγ,θ(z).\nProof. The inequality ft+1\nγ,θ(z)≤βft\nγ,θ(z) comes from\nft+1\nγ,θ(z) =rθ(Tt+1\nθ(z+ ∆⋆))≤βrθ(Tt\nθ(z+ ∆⋆))≤βft\nγ,θ(z),\nwhere ∥∆⋆∥2≤γis the maximizer to ft+1\nγ,θ(z). The first inequality comes from Definition 4.3\nand the second from (14). The inequality ft+1\nγ,θ(z)≤rθ(Tt+1\nθ(z))+2∥∆∥2in Lemma 6 follows\nfrom Lemma 5. The final inequality in Lemma 6 is derived as follows:\ngt\nγ,θ(z) =ℓfp\nθ(Tt\nθ(z+ ∆⋆))≤2rθ(Tt\nθ(z+ ∆⋆))≤2ft\nγ,θ(z).\nHere,∥∆⋆∥2≤γis the maximizer for gt\nγ,θ(z), and Lemma 3 gives the first inequality. ■\nUsing Lemma 6, we can bound the marginal empirical risk for the linearly convergent\ncase. For the β-linearly convergent case, Cγ/2(t) is bounded by 2 βt(D+γ/2) which uses (16)\nandC0(t)≤2rθ(Tt\nθ(z))≤2βtD. The first inequality comes from Lemma 3 and the second\ninequality follows from Definition 4.3.\n4.3.3 Averaged operators\nLastly, we consider the averaged operator case which in general gives sublinear convergence.\nDefinition 4.4 (α-averaged operator) .An operator Tisα-averaged for α∈(0,1) if there\nexists a non-expansive operator Rsuch that T= (1−α)I+αR.\nLemma 7. LetTθbe an α-averaged. Then the following bound holds:\ngt\nγ,θ(z)≤min\nj=0,...,trα\n(1−α)(t−j+ 1)(rθ(Tj\nθ(z)) + 2 γ)fort≥0.\n15\n\nProof. Let ¯αt,j=q\nα\n(1−α)(t−j+1). There exists ∥∆⋆∥2≤γsuch that the equality below holds\nby definition of the marginal fixed-point residual.\ngt\nγ,θ(z) =ℓfp\nθ(Tt\nθ(z+ ∆⋆))≤¯αt,j(rθ(Tj\nθ(z+ ∆⋆)))≤¯αt,jfj\nγ,θ(z)≤¯αt,j(rθ(Tj\nθ(z)) + 2 γ) (17)\nThe three inequalities comes from Ryu and Yin (2022, Theorem 1), the definition of fj\nγ,θ(z)\nin (14), and Lemma 6 respectively. Equation (17) holds for all 0 ≤j≤t. ■\nUsing Lemma 7, we can bound the marginal empirical risk for the averaged case. We\nbound the worst-case marginal fixed-point residual with Cγ/2(t)≤p\nα/((1−α)(t+ 1))( D+\nγ) which follows from Lemma 7 by letting z=hw(θ) and j= 0. Then the inequality holds\nfor every θ∈Θ.\n5 Choosing the right architecture\nIn this section, we discuss how the number of fixed-point steps the model is trained on, k,\nand the loss function affect performance.\n5.1 Bounds on the fixed-point residual for tevaluation steps\nIn this subsection, we derive bounds on the fixed-point residual after tsteps, ℓfp\nθ(Tt\nθ(z)), in\nterms of the loss after ksteps, ℓθ(Tk\nθ(z)) where k < t . A summary of these results is given\nin Table 2 where we provide the bound for each of the two loss functions. The bounds in\nTable 2 using the fixed-point residual loss in the denominator are given by either applying\nthe definition of contractiveness in the contractive case or non-expansiveness in the other\ntwo cases. To get the bounds in Table 2 when using the regression loss in the denominator,\nwe first establish the inequality\nℓfp\nθ(z)≤2ℓreg\nθ(z), (18)\nfor any non-expansive operator Tθ. This result follows from Lemma 3 since rθ(z)≤ℓreg\nθ(z).\nThe results in the contractive and linearly convergent cases follow from applying the def-\ninition of each and inequality (18). In the averaged case, we directly apply Ryu and Yin\n(2022, Theorem 1). Unless the operator is contractive, the results from Table 2 indicate that\nstronger bounds can be obtained from using the regression loss.\n5.2 Training for the fixed-point residual vs regression loss\nThe fixed-point residual (5) and regression (6) losses, align with the main distinction of learn-\ning methods mentioned in Amos (2023, Section 2.2) which splits between learning strategies\nthat penalize suboptimality directly and those that penalize the distance to known ground\ntruth solutions. The primary advantages of using our fixed-point residual loss are twofold:\ni) there is no need to compute a ground truth solution z⋆(θ) for each problem instance be-\nfore training, and ii) the loss exactly corresponds to the evaluation metric, the fixed-point\n16\n\nTable 2: Bounds for the ratios of testing at tsteps and training at ksteps. Here, we bound the\nratio of the fixed-point residual after tsteps and the loss after ksteps where t > k . The value\nin the table provides the bound, e.g., for a β-contractive operator, ℓfp\nθ(Tt\nθ(z))/ℓreg\nθ(Tk\nθ(z))≤\n2βt−k.\nOperatorℓfp\nθ(Tt\nθ(z))\nℓfp\nθ(Tk\nθ(z))ℓfp\nθ(Tt\nθ(z))\nℓreg\nθ(Tk\nθ(z))\nβ-contractive βt−k2βt−k\nβ-linearly convergent 1 2 βt−k\nα-averaged 1q\nα\n(1−α)(t−k+1)\nresidual. On the other hand, there are two main advantages to using the regression loss: i)\nthe regression loss uses the global information of the ground truth solution z⋆(θ), while the\nfixed-point residual loss exploits only local information, and ii) as mentioned in Section 5.1,\nstronger bounds on future iterations can be obtained when using the regression loss.\n6 Numerical experiments\nWe now illustrate our method on examples of fixed-point algorithms from Table 1. We imple-\nmented our architecture in the JAX library (Bradbury et al., 2018) using the Adam (Kingma\nand Ba, 2015) optimizer to train. We use 10000 training problems and evaluate on 1000 test\nproblems for the examples except the first one in Section 6.1. In our examples, we conduct\na hyperparameter sweep over learning rates of either 10−3or 10−4, and architectures with\n0,1,or 2 layers with 500 neurons each. We decay the learning rate by a factor of 5 when the\ntraining loss fails to decrease over a window of 10 epochs. All computations were run on the\nPrinceton HPC Della Cluster and each example could be trained under 5 hours. The code\nto reproduce our results is available at\nhttps://github.com/stellatogrp/l2ws_fixed_point .\nBaselines. We compare our learned warm start, for both the fixed-point residual loss and\nthe regression loss functions, against the following initialization approaches:\nCold start. We initialize the fixed-point algorithm for a test problem with parameter θ\nwith the prediction hwcs(θ) where wcshas been randomly initialized.\nNearest-neighbor warm start. The nearest-neighbor warm start initializes the test prob-\nlem with an optimal solution of the nearest of the training problems measured by dis-\ntance in terms of its parameter θ∈Rd. In most of our examples, the parametrized\nproblems are sufficiently far apart so that the nearest-neighbor initializations do not\nsignificantly improve upon the cold start.\n17\n\nIn every experiment, we plot the average of the fixed-point residuals of the test problems for\nvarying tas defined in Section 3.3. Additionally, we plot the average gain of each initializa-\ntion relative to the cold start across the test problems. This gain for a given parameter θ\ncorresponds to the ratio\nℓfp\nθ(Tt\nθ(hwcs(θ)))\nℓfp\nθ(Tt\nθ(hw(θ))),\nwhere hwis the initialization technique in question and hwcsis the cold-start predictor de-\nscribed above. Importantly, we code exact replicas of the OSQP and SCS algorithms in\nJAX. This allows us to input the learned warm starts into the corresponding C implemen-\ntations; moreover, we report the solve times in milliseconds to reach various tolerances for\nthe experiments we run with OSQP in Section 6.3 and SCS in Section 6.4.\n6.1 Gradient descent\n6.1.1 Unconstrained QP\nWe first consider a stylized example to illustrate why unrolling fixed-point steps can signifi-\ncantly improve over a decoupled approach, where k= 0. Consider the problem\nminimize (1 /2)zTPz+cTz,\nwhere P∈Sn\n++, and c∈Rnare the problem data and z∈Rnis the decision variable. The\nparameter is θ=c.\nNumerical example. We consider a small example where n= 20. We have a single\nhidden layer with 10 neurons, and 100 training problems. Let P∈Sn\n++be a diagonal matrix\nwhere the first 10 diagonals take the value 100 and the last ten take the value of 1. Let\nθ=c∈Rn. Here, each θiis sampled according to the uniform distribution ψiU[−10,10],\nwhere ψi= 10000 if i≤10 else 1. The idea is that the first 10 indices of the optimal solution\nz⋆(θ) vary much more than the last 10, but the first 10 indices of zwill converge much faster.\nResults. Figure 4 and Table 3 show the convergence behavior of our method. The de-\ncoupled approaches prioritize minimizing the error to predict the first 10 indices and fail to\nimprove on the cold start. By unrolling these gradient steps, our learning framework with\nk >0 is able to adapt the warm start to take advantage of the downstream algorithm. These\ngains remain constant as the number of evaluation steps increases.\n6.2 Proximal gradient descent\n6.2.1 Lasso\nWe first consider the lasso problem\nminimize (1 /2)∥Az−b∥2\n2+λ∥z∥1,\n18\n\n10−310−1101103test ﬁxed-point residual\ntraining with ﬁxed-point residual losses\n training with regression losses\n0 100 200 300 400 500\nevaluation iterations246810 test gain to cold start\n0 100 200 300 400 500\nevaluation iterations\ncold start nearest neighbor warm start\nlearned warm-start k={ 0 5 15 60}\nFigure 4: Unconstrained QP results. All of the learned warm starts provide large improve-\nments over the cold start and nearest neighbor initializations except for the ones learned\nwith k= 0.\nTable 3: Unconstrained QP.\n(a) Mean iterations to reach a given fixed point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 57 51 76 1 1 1 1 73 1 1 1 1\n0.01 286 280 305 59 70 55 50 302 97 86 86 86\n0.001 515 510 534 289 299 285 279 531 326 315 315 315\n0.0001 744 739 763 518 529 514 509 760 555 544 544 544\n(b) Mean reduction in iterations from a cold start to a given fixed-point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 0 0.11 -0.33 0.98 0.98 0.98 0.98 -0.28 0.98 0.98 0.98 0.98\n0.01 0 0.02 -0.07 0.79 0.76 0.81 0.83 -0.06 0.66 0.7 0.7 0.7\n0.001 0 0.01 -0.04 0.44 0.42 0.45 0.46 -0.03 0.37 0.39 0.39 0.39\n0.0001 0 0.01 -0.03 0.3 0.29 0.31 0.32 -0.02 0.25 0.27 0.27 0.27\nwhere A∈Rm×n,b∈Rm, and λ∈R++are problem data and z∈Rnis the decision\nvariable. The parameter here is θ=b.\nNumerical example. We generate A∈R500×500with i.i.d standard Gaussian entries and\npickλ= 10. We sample each bvector from the uniform distribution U[0,30].\n19\n\nResults. Figure 5 and Table 4 show the convergence behavior of our method. While most\nof the learned warm starts significantly improve upon the baselines, the warm starts learned\nwith k= 5 and the regression loss perform the best.\n10−210−1100101test ﬁxed-point residual\ntraining with ﬁxed-point residual losses\n training with regression losses\n0 100 200 300 400 500\nevaluation iterations05101520 test gain to cold start\n0 100 200 300 400 500\nevaluation iterations\ncold start nearest neighbor warm start\nlearned warm-start k={ 0 5 15 60}\nFigure 5: Lasso results. All of the learned warm starts except for k= 0 with fixed-point\nresidual loss significantly improve upon the baselines for any number of steps up to 500.\nTable 4: Lasso.\n(a) Mean iterations to reach a given fixed point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 88 59 33 2 6 10 11 7 5 12 15 24\n0.01 559 492 540 159 141 138 159 150 129 131 136 142\n0.001 1821 1725 1814 1274 1239 1233 1267 1201 1181 1182 1205 1210\n0.0001 4041 3894 4034 3410 3347 3334 3352 3229 3208 3209 3243 3240\n(b) Mean reduction in iterations from a cold start to a given fixed-point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 0 0.33 0.62 0.98 0.93 0.89 0.88 0.92 0.94 0.86 0.83 0.73\n0.01 0 0.12 0.03 0.72 0.75 0.75 0.72 0.73 0.77 0.77 0.76 0.75\n0.001 0 0.05 0.0 0.3 0.32 0.32 0.3 0.34 0.35 0.35 0.34 0.34\n0.0001 0 0.04 0.0 0.16 0.17 0.17 0.17 0.2 0.21 0.21 0.2 0.2\n20\n\n6.3 OSQP\nIn this subsection, we apply our learning framework to the OSQP (Stellato et al., 2020) algo-\nrithm from Table 1 to solve convex quadratic programs (QPs).We compare solve times using\nOSQP code written in C for our learned warm starts against the baselines. Table 5 shows\nthe sizes of the problems we run: model predictive control of a quadcopter in Section 6.3.1\nand image deblurring in Section 6.3.2.\nTable 5: Sizes of conic problems from Table 1 that we use OSQP to solve. We give the\nnumber of primal constraints ( m), size of the primal variable ( n), and the parameter size, d.\nQuadcopter Image deblurring\nconstraints m 600 2102\nvariables n 550 802\nparameter size d44 784\n6.3.1 Model predictive control of a quadcopter\nIn our next example, we use model predictive control (Borrelli et al., 2017) to control a\nquadcopter to follow a reference trajectory. The idea of MPC is to optimize over a finite\nhorizon length, but then to only implement the first control before optimizing again. Since\nthe family of problems is sequential in nature, we add an additional baseline called the\nprevious-solution warm start where we shift the solution of the previous problem by one\ntime index to warm-start the current problem.\nWe model the quadcopter as a rigid body controlled by four motors as in Song and\nScaramuzza (2022). The state vector is x= (p, v, q )∈Rnxwhere the state size is nx= 10.\nThe position vector p= (px, py, pz)∈R3and the velocity vector v= (vx, vy, vz)∈R3\nindicate the coordinates and velocities of the center of the quadcopter respectively. The\nvector q= (qw, qx, qy, qz)∈R4is the quaternion vector indicating the orientation of the\nquadcopter. The inputs are u= (c, ωx, ωy, ωz)∈Rnuwhere the input size is nu= 4. The\nfirst input is the vertical thrust, and the last three are the angular velocities in the body\nframe. The dynamics are\n˙p=v, ˙v=\n2(qwqy+qxqz)c\n2(qwqy+qxqz)c\n(q2\nw−q2\nx−q2\ny+q2\nz)c−g\n, ˙q=1\n2\n−wxqx−wyqy−wzqz\nwxqw−wyqz+wzqy\nwxqz+wyqw−wzqx\nwxqy+wyqx+wzqw\n,\nwhere gis the gravitational constant. At each time step, the goal is to track a reference\ntrajectory given by xref= (xref\n1, . . . , xref\nT), while satisfying constraints on the states and the\n21\n\ncontrols. We discretize the system with ∆ tand solve the QP\nminimize ( xT−xref\nT)TQT(xT−xref\nT) +PT−1\nt=1(xt−xref\nt)TQ(xt−xref\nt) +uT\ntRut\nsubject to xt+1=Axt+Butt= 0, . . . , T −1\numin≤ut≤umin t= 0, . . . , T −1\nxmin≤xt≤xmax t= 1, . . . , T\n|ut+1−ut| ≤∆u t = 1, . . . , T −1.\nHere, the decision variables are the states ( x1, . . . , x T) where xt∈Rnxand the controls\n(u1, . . . , u T−1) where ut∈Rnu. The dynamics matrices AandBare determined by lin-\nearizing the dynamics around the current state x0, and the previous control input u0(Diehl\net al., 2009). The matrices Q, Q T∈Snx\n+penalize the distance of the states to the reference\ntrajectory, ( xref\n1, . . . , xref\nT). The matrix R∈Snu\n++regularizes the controls. The parameter is\nθ= (x0, u0, xref\n1, . . . , xref\nT)∈R(T+1)nx+nu. We generate many different trajectories where the\nsimulation length is larger than the time horizon T.\nNumerical example. We discretize our continuous time model with a value of ∆ t= 0.05\nseconds. The gravitational constant is 9 .8. Each trajectory has a length of 100, and the\nhorizon we consider at each timestep for each QP is T= 10. We use state bounds of xmax=\n−xmin= (1,1,1,10,10,10,1,1,1,1). We constrain the controls with umax= (20 ,6,6,6) and\numin= (2,−6,−6,−6), and set ∆ u= (18 ,6,6,6). For each simulation, the quadcopter is\ninitialized at p=v= 0 and q= (1,0,0,0). We sample 5 waypoints for each of the ( x, y, z )\ncoordinates for each trajectory from the uniform distribution, U[−0.5,0.5]. Then we use a B-\nspline (de Boor, 1972) to smoothly interpolate between the waypoints to generate 100 points\nfor the entire trajectory. Since each reference trajectory is made up of ( x, y, z ) coordinates\nrather than the full state vector, we shorten the parameter size to θ∈Rnx+nu+3T.\nResults. Figure 6 and Table 6 show the convergence behavior of our method. While all\nof the warm starts learned with the regression losses deliver substantial improvements over\nthe baselines, our method using k= 60 with the fixed-point residual loss stands out as the\nbest for a larger number of steps. To simulate a strict latency requirement, we also compare\nvarious initialization techniques in a closed-loop system where only 15 OSQP iterations are\nallowed per QP in Figure 7. The learned warm start can more accurately track the reference\ntrajectory compared with the other two methods.\n6.3.2 Image deblurring\nWe turn our attention to the task of image deblurring. Given a blurry image b∈Rn, the goal\nis to recover the original image x∈Rn. Both the noisy vector band the target vector xare\nformed by stacking the columns of their respective images. We formulate this well-studied\n22\n\nTable 6: Quadcopter.\n(a) Mean iterations to reach a given fixed point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborPrevious\nSolutionFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 103 26 28 86 9 10 15 34 14 14 15 26 39\n0.01 682 127 115 696 480 79 24 50 25 26 26 35 54\n0.001 2262 1210 1416 2582 4268 5728 693 277 596 673 604 616 636\n0.0001 9958 6573 6906 9980 12938 14000 7212 5681 6041 5957 6015 6027 6053\n(b) Mean reduction in iterations from a cold start to a given fixed-point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborPrevious\nSolutionFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 0 0.75 0.73 0.17 0.91 0.9 0.85 0.67 0.86 0.86 0.85 0.75 0.62\n0.01 0 0.81 0.83 -0.02 0.3 0.88 0.96 0.93 0.96 0.96 0.96 0.95 0.92\n0.001 0 0.47 0.37 -0.14 -0.89 -1.53 0.69 0.88 0.74 0.7 0.73 0.73 0.72\n0.0001 0 0.34 0.31 -0.0 -0.3 -0.41 0.28 0.43 0.39 0.4 0.4 0.39 0.39\n(c) Mean solve times (in milliseconds) in OSQP with absolute and relative tolerances set to tol.\ntol.Cold\nStartNearest\nNeighborPrevious\nSolutionFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 0.19 0.17 0.17 0.32 0.17 0.18 0.18 0.19 0.16 0.16 0.16 0.17 0.18\n0.01 2.44 0.23 0.21 2.32 0.68 0.25 0.18 0.21 0.16 0.17 0.16 0.17 0.28\n0.001 12.0 4.69 6.33 11.77 17.65 35.89 1.37 0.77 1.18 1.23 1.23 1.3 1.45\n0.0001 54.59 24.63 28.66 50.15 68.56 104.18 29.14 14.34 18.26 18.86 18.51 18.9 18.6\n1e-05 114.62 76.03 78.21 103.42 128.59 174.69 85.6 63.35 65.23 67.0 65.88 68.24 65.43\n10−2100102104test ﬁxed-point residual\ntraining with ﬁxed-point residual losses\n training with regression losses\n0 100 200 300 400 500\nevaluation iterations0200400600 test gain to cold start\n0 100 200 300 400 500\nevaluation iterations\ncold start nearest neighbor warm start previous solution warm start\nlearned warm-start k={ 0 5 15 60}\nFigure 6: Quadcopter results. Learned warm starts offer substantial improvements over the\nbaselines. In particular, warm starts learned with k= 60 and the fixed-point residual loss\nhave the largest gain for evaluation steps over about 50.\n23\n\n(a) previous solution\n (b) nearest neighbor\n (c) learned\nFigure 7: Visualizing closed-loop MPC of flying a quadcopter to track a reference trajectory.\nEach row corresponds to a different unseen reference trajectory. Each column uses a different\ninitialization scheme to track the same unseen black reference trajectory in a closed-loop.\nEach technique is given a budget of 15 OSQP iterations to solve each QP. The learned\napproach which is trained on k= 5 with the regression loss tracks the trajectory well\ncompared against the other two.\nproblem (Beck and Teboulle, 2009; Benvenuto et al., 2010) as\nminimize ∥Ax−b∥2\n2+λ∥x∥1\nsubject to 0 ≤x≤1.\nHere, the matrix A∈Rn×nis the blur operator which represents a two-dimensional convo-\nlutional operator. The regularization hyperparameter λ∈R++, weights the fidelity term\n∥Ax−b∥2\n2, relative to the ℓ1penalty. The ℓ1penalty is used as it less sensitive to outliers and\nencourages sparsity (Beck and Teboulle, 2009). The constraints ensure that the deblurred\nimage has pixel values within its domain.\nNumerical example. We consider handwritten letters from the EMNIST dataset (Cohen\net al., 2017). We apply a Gaussian blue of size 8 to each letter and then add i.i.d. Gaussian\nnoise with standard deviation 0 .001. The hyperparameter weighting term is λ= 1e−4.\nResults. Figure 8 and Table 7 show the convergence behavior of our method. Learned\nwarm starts with the regression loss tend to outperform the learned warm starts with the\nfixed-point residual loss. We show visualizations of our method in Figure 9. For images that\n24\n\nare particularly challenging, the image quality after 50 OSQP steps is significantly better for\nthe learned warm start than the baseline initializations.\n10−2100102test ﬁxed-point residual\ntraining with ﬁxed-point residual losses\n training with regression losses\n0 100 200 300 400 500\nevaluation iterations051015 test gain to cold start\n0 100 200 300 400 500\nevaluation iterations\ncold start nearest neighbor warm start\nlearned warm-start k={ 0 5 15 60}\nFigure 8: Image deblurring. Warm starts learned with the regression loss provide bigger\ngains compared with those learned with the fixed-point residual loss.\n6.4 SCS\nIn this subsection, we apply our learning framework to the SCS (O’Donoghue et al., 2019)\nalgorithm from Table 1 to solve convex conic optimization problems. We compare solve\ntimes using SCS code written in C for our learned warm starts against the baselines. We\nrun our experiments on two second-order cone programs (SOCPs) in robust Kalman filtering\nin Section 6.4.1 and robust non-negative least squares in Section 6.4.2 and two semidefinite\nprograms (SDPs) in phase retrieval in Section 6.4.3 and sparse PCA in Section 6.4.4.\n6.4.1 Robust Kalman filtering\nKalman filtering (Kalman, 1960) is a widely used technique for predicting system states\nin the presence of noise in dynamic systems. In our first SOCP example, we use robust\nKalman filtering (Xie and Soh, 1994) which mitigates the impact of outliers on the filtering\nprocess and model misspecifications to track a moving vehicle from noisy data location as\nin Venkataraman and Amos (2021). The dynamical system is modeled by\nxt+1=Axt+Bwt, y t=Cxt+vt,for t= 0,1, . . . , (19)\n25\n\nTable 7: Image deblurring.\n(a) Mean iterations to reach a given fixed point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 24 16 6 5 8 10 12 5 6 8 9 11\n0.01 194 181 107 93 17 27 36 16 15 18 27 42\n0.001 1348 1253 982 953 454 589 658 215 171 208 266 321\n0.0001 7767 7607 6613 6689 5668 6359 6812 4238 3292 3779 4399 4744\n(b) Mean reduction in iterations from a cold start to a given fixed-point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 0 0.33 0.75 0.79 0.67 0.58 0.5 0.79 0.75 0.67 0.62 0.54\n0.01 0 0.07 0.45 0.52 0.91 0.86 0.81 0.92 0.92 0.91 0.86 0.78\n0.001 0 0.07 0.27 0.29 0.66 0.56 0.51 0.84 0.87 0.85 0.8 0.76\n0.0001 0 0.02 0.15 0.14 0.27 0.18 0.12 0.45 0.58 0.51 0.43 0.39\n(c) Mean solve times (in milliseconds) in OSQP with absolute and relative tolerances set to tol.\ntol.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 6.34 6.37 6.32 6.31 6.30 6.38 6.32 6.33 6.34 6.32 6.35 6.33\n0.01 9.13 9.62 6.32 6.33 6.32 6.31 6.33 6.33 6.33 6.35 6.32 6.33\n0.001 62.37 68.25 48.0 38.51 9.9 14.78 18.38 9.51 7.55 8.21 11.13 15.54\n0.0001 464.0 472.7 398.8 363.2 230.1 290.2 333.9 115.9 80.8 106.0 140.8 168.6\n1e-05 3048 2997 2630 2778 2500 2761 2934 1957 1691 1832 1973 2090\n10thpercentile\n optimal\n blurred\n cold-start\n nearest\nneighbor\nlearned\n50th\n90th\n99th\nFigure 9: EMNIST image deblurring. Each row corresponds to an unseen sample from the\nEMNIST dataset. The last three columns depict several different initialization techniques\nafter 50 OSQP steps. In the learned column, we use the regression loss with k= 5. To adjust\nthe difficulty of the images displayed, we select images corresponding to different percentiles\nof distance from the nearest neighbor to the optimal solution.\n26\n\nTable 8: Sizes of conic problems from Table 1 that we use SCS to solve. We give the number\nof primal constraints ( m), size of the primal variable ( n), and the parameter size, d. Then,\nwe provide the sizes of the cones for each conic program. For the second-order and the\npositive semidefinite cones, we supply arrays specifying the lengths of each respective cone.\nThe notation 100 ×[3] means that there are 100 second-order cones each of size 3.\nKalman filter robust least squares phase retrieval sparse PCA\nconstraints m 600 2102 3480 4022\nvariables n 550 802 1600 2420\nparameter size d 100 500 120 55\nzero 600 0 240 1\nnon-negative 550 800 0 3201\nsecond-order 100 ×[3] [801,501] 0 0\npositive semidefinite 0 0 [80] [40]\nwhere xt∈Rnxis the state, yt∈Rnois the observation, wt∈Rnuis the input, and\nvt∈Rnois a perturbation to the observation. The matrices A∈Rnx×nx,B∈Rnx×nu,\nandC∈Rno×nxgive the dynamics of the system. Our goal is to recover the state xtfrom\nthe noisy measurements yt. To do so, we solve the problem\nminimizePT−1\nt=1∥wt∥2\n2+µψρ(vt)\nsubject to xt+1=Axt+Bwtt= 0, . . . , T −1\nyt=Cxt+vtt= 0, . . . , T −1.\nHere, the Huber penalty function (Huber, 1964) parametrized by ρ∈R++that robustifies\nagainst outliers is\nψρ(a) =(\n∥a∥2 ∥a∥2≤ρ\n2ρ∥a∥2−ρ2∥a∥2≥ρ,\nandµ∈R++weights this penalty term. The decision variables are the xt’s,wt’s, and\nvt’s. The parameters are the observed yt’s,i.e.,θ= (y0, . . . , y T−1). In this example, we\ntake advantage of rotational invariance of the problem. We rotate the noisy trajectory so\nthat yTis on the x-axis for every problem. After solving the transformed problem (for any\ninitialization) we reverse the rotation to obtain the solution of the original problem.\nNumerical example. As in Venkataraman and Amos (2021), we set nx= 4, no= 2,\nnu= 2,µ= 2,ρ= 2, and T= 50. The dynamics matrices are\nA=\n1 0 (1 −(γ/2)∆t)∆t 0\n0 1 0 (1 −(γ/2)∆t)∆t\n0 0 1 −γ∆t 0\n0 0 0 1 −γ∆t\n, B=\n1/2∆t20\n0 1 /2∆t2\n∆t 0\n0 ∆ t\n, C=\"\n1 0 0 0\n0 1 0 0#\n,\n27\n\nwhere ∆ t= 0.5 and γ= 0.05 are fixed to be respectively the sampling time and the velocity\ndampening parameter. We generate the problem instances in the following way. We generate\ntrue trajectories {x∗\n0, . . . , x∗\nT−1}of the vehicle by first letting x∗\n0= 0. Then we sample the\ninputs as wt∼ N(0,0.01) and vt∼ N(0,0.01). The trajectories are then fully defined via\nthe dynamics equations in Equation (19) with the sampled wt’s and vt’s.\nTable 9: Robust Kalman filtering.\n(a) Mean iterations to reach a given fixed point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 40 36 34 4 8 32 34 24 9 33 32 34\n0.01 90 85 87 33 25 81 79 77 26 81 79 79\n0.001 142 139 140 84 73 134 131 131 75 134 131 131\n0.0001 195 193 195 137 126 187 185 185 128 187 185 185\n(b) Mean reduction in iterations from a cold start to a given fixed-point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 0 0.1 0.15 0.9 0.8 0.2 0.15 0.4 0.78 0.18 0.2 0.15\n0.01 0 0.06 0.03 0.63 0.72 0.1 0.12 0.14 0.71 0.1 0.12 0.12\n0.001 0 0.02 0.01 0.41 0.49 0.06 0.08 0.08 0.47 0.06 0.08 0.08\n0.0001 0 0.01 0.0 0.3 0.35 0.04 0.05 0.05 0.34 0.04 0.05 0.05\n(c) Mean solve times (in milliseconds) in SCS with absolute and relative tolerances set to tol.\ntol.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 0.49 0.49 0.45 0.49 0.49 0.49 0.49 0.35 0.52 0.49 0.5 0.49\n0.01 0.67 0.49 0.56 0.49 0.48 0.49 0.5 0.49 0.52 0.49 0.5 0.51\n0.001 1.36 1.31 1.47 0.5 0.48 1.14 1.12 1.23 0.53 1.13 1.13 1.11\n0.0001 2.23 2.19 2.33 1.18 0.94 2.03 1.95 2.12 1.06 1.99 1.99 1.95\n1e-05 3.11 3.13 3.31 2.05 1.76 2.85 2.82 3.03 1.97 2.86 2.83 2.8\nResults. Since this is a control example, we use the shifted previous solution as a warm\nstart from Section 6.3.1. Figure 10 and Table 9 show the convergence behavior of our method.\nIn this example, the learned warm starts do well with the fixed-point residual loss for k= 5\nandk= 15 and the regression loss for k= 5, but hardly improve in the other cases. In all\ncases, the gains relative to the cold start remain nearly constant throughout the evaluation\niterations. Figure 11 illustrates how our learned solutions after 5 iterations outperforms the\nsolution returned after 5 iterations from the baselines.\n6.4.2 Robust non-negative least squares\nWe consider the problem of non-negative least squares with matrix uncertainty\nmin x≥0max∥∆A∥≤ρ∥(ˆA+ ∆A)x−b∥2,\n28\n\n10−610−410−2100102test ﬁxed-point residual\ntraining with ﬁxed-point residual losses\n training with regression losses\n0 50 100 150 200 250 300\nevaluation iterations020406080100 test gain to cold start\n0 50 100 150 200 250 300\nevaluation iterations\ncold start nearest neighbor warm start previous solution warm start\nlearned warm-start k={ 0 5 15 60}\nFigure 10: Robust Kalman filtering. The learned warm starts that train with k= 5 for\nboth losses and with k= 15 for the fixed-point residual loss have significant gains over the\nbaselines.\nwhere the right-hand side vector b∈Rm, nominal matrix ˆA∈Rm×n, and maximum pertur-\nbation ρ∈R++are the problem data. The decision variable of the minimizer and maximizer\narex∈Rnand ∆ A∈Rm×nrespectively. Here, ∥∆A∥denotes the largest singular value of\nthe perturbation matrix ∆ A. El Ghaoui and Lebret (1997) provide an SOCP formulation\nfor this problem\nminimize u+ρv\nsubject to ∥ˆAx−b∥2≤u\n∥x∥2≤v\nx≥0,\nwhere x∈Rn,u∈R, and v∈Rare the decision variables. The parameter is θ=b.\nNumerical example. We pick ρ= 4 and ˆA∈R500×800where the entries of ˆAare sampled\nthe uniform distribution U[−1,1]. We sample bin an i.i.d. fashion from U[1,2].\nResults. Figure 12 and table 10 show the convergence behavior of our method. The\nlearned warm starts with positive ksubstantially improve upon the baselines for both losses.\nFigure 12 show linear convergence of our method; this results in the gains from the learned\nwarm starts staying roughly constant as the number of evaluation steps increases.\n29\n\noptimal solution noisy trajectory\nnearest neighbor previous solution learned\nFigure 11: Visualizing test problems for robust Kalman filtering. Each plot is a separate test\nproblem. The noisy, observed trajectory are the red points which serve as problem data for\nthe SOCP. The robust Kalman filtering recovery, the optimal solution of the SOCP, is shown\nas green dots. After 5 iterations, SCS with our learned warm start using the regression loss\nwith k= 5 is very close to the optimal solution while SCS initialized with both the shifted\nprevious solution and the nearest neighbor still is noticeably far away from optimality.\n10−510−310−1101103test ﬁxed-point residual\ntraining with ﬁxed-point residual losses\n training with regression losses\n0 50 100 150 200 250 300\nevaluation iterations050100150 test gain to cold start\n0 50 100 150 200 250 300\nevaluation iterations\ncold start nearest neighbor warm start\nlearned warm-start k={ 0 5 15 60}\nFigure 12: Robust non-negative least squares. All of the learned warm starts apart from the\nones with k= 0 substantially improve the gain over the cold start.\n30\n\nTable 10: Robust non-negative least squares.\n(a) Mean iterations to reach a given fixed point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 85 33 20 5 12 16 20 15 8 14 15 22\n0.01 139 87 75 45 50 52 53 68 39 41 38 48\n0.001 199 146 134 103 108 109 109 127 95 97 93 100\n0.0001 258 208 196 166 171 172 171 189 156 158 153 161\n(b) Mean reduction in iterations from a cold start to a given fixed-point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 0 0.61 0.76 0.94 0.86 0.81 0.76 0.82 0.91 0.84 0.82 0.74\n0.01 0 0.37 0.46 0.68 0.64 0.63 0.62 0.51 0.72 0.71 0.73 0.65\n0.001 0 0.27 0.33 0.48 0.46 0.45 0.45 0.36 0.52 0.51 0.53 0.5\n0.0001 0 0.19 0.24 0.36 0.34 0.33 0.34 0.27 0.4 0.39 0.41 0.38\n(c) Mean solve times (in milliseconds) in SCS with absolute and relative tolerances set to tol.\ntol.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 61.79 4.16 2.23 22.13 26.95 29.2 26.73 2.32 22.44 22.39 22.48 22.14\n0.01 101.73 46.01 42.08 24.13 31.61 36.61 36.37 42.59 26.17 25.47 23.97 37.31\n0.001 141.53 101.75 89.95 61.95 79.9 82.98 76.75 95.76 63.65 64.69 62.16 67.32\n0.0001 185.46 145.54 148.4 107.75 142.34 158.18 149.7 143.77 112.38 115.18 107.6 119.79\n1e-05 241.16 199.63 187.8 169.57 197.37 210.4 198.4 195.24 168.44 167.4 161.76 164.64\n6.4.3 Phase retrieval\nOur first SDP example is the problem of phase retrieval (Fienup, 1982) where the goal is to\nrecover an unknown signal x∈Cnfrom observations. This problem has applications in X-ray\ncrystallogpraphy (Millane, 1990) and coherent diffractive imaging (Shechtman et al., 2015).\nSpecifically, for known vectors ai∈Cn, we have mscalar measurements: bi=|⟨ai, x⟩|2, i=\n1, . . . , m . Since the values are complex, we denote the conjugate transpose of a matrix A\nbyA∗. Noting that |⟨ai, x⟩|2= (a∗\nix)(x∗ai),we introduce a matrix variable X∈Sn\n+and\nmatrices Ai=aia∗\ni. The exact phase retrieval problem becomes a feasibility problem over\nthe matrix variable with a rank constraint\nfind X\nsubject to tr(AiX) =bi, i= 1, . . . , m\nrank (X) = 1 , X⪰0.\nWe arrive at the following SDP relaxation by dropping the rank constraint:\nminimize tr(X)\nsubject to tr(AiX) =bi, i= 1, . . . , m\nX⪰0.\nTo parameterize each problem, we let θ=b∈Rm.\n31\n\nNumerical example. For the signal, we sample xfrom a complex normal distribution,\ni.e., we sample the real and imaginary parts of each component independently from N(µ, σ2).\nTo construct the constraint matrices, we use the coded diffraction pattern model (Cand` es\net al., 2015). The specific modulating waveforms follow the setup from Yurtsever et al.\n(2021, Section F.1). For a signal of size n, we generate d= 3nmeasurements. Specifically,\nwe draw 3 independent modulating waveforms ψj∈Rn, j= 1, . . . , 3. Each component of\nψjis the product of two random variables, with one drawn uniformly from {1, i,−1,−i}and\nthe other drawn from {√\n2/2,√\n3}with probabilities 0 .8 and 0 .2, respectively. Then, each ai\ncorresponds to computing a single entry of the Fourier transform of xafter being modulated\nby the waveforms. Letting Wbe the n×ndiscrete Fourier transform matrix, the ai’s can\nbe written explicitly as a(j−1)n+l=WT\nl(diag(ψj))⋆where WT\nlis the l-th row of W. We take\nn= 40, µ= 5, and σ= 1.\nResults. Figure 13 and Table 11 show the convergence behavior of our method. In this\ncase, while the decoupled approach with k= 0 offers the largest gains over the first few\niterations, the gain degrades as tincreases to the point where it’s performance becomes\nworse than that of the nearest-neighbor initialization. The learned warm starts with the\nregression loss for positive ktend to sustain their gains for a larger value of tcompared with\nthe learned warm starts that use the fixed-point residual loss.\nTable 11: Phase retrieval.\n(a) Mean iterations to reach a given fixed point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n1.0 112 47 11 4 6 7 11 12 15 16 19 23\n0.1 531 254 320 280 209 96 76 322 122 115 117 129\n0.01 2558 1629 2259 2710 2890 2626 1982 2249 666 669 641 686\n0.001 6478 5547 6133 6803 7113 6861 6127 6098 3609 3611 3408 3667\n0.0001 11512 10837 11281 12021 12379 12107 11303 11223 8322 8268 7888 8217\n(b) Mean reduction in iterations from a cold start to a given fixed-point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n1.0 0 0.58 0.9 0.96 0.95 0.94 0.9 0.89 0.87 0.86 0.83 0.79\n0.1 0 0.52 0.4 0.47 0.61 0.82 0.86 0.39 0.77 0.78 0.78 0.76\n0.01 0 0.36 0.12 -0.06 -0.13 -0.03 0.23 0.12 0.74 0.74 0.75 0.73\n0.001 0 0.14 0.05 -0.05 -0.1 -0.06 0.05 0.06 0.44 0.44 0.47 0.43\n0.0001 0 0.06 0.02 -0.04 -0.08 -0.05 0.02 0.03 0.28 0.28 0.31 0.29\n(c) Mean solve times (in milliseconds) in SCS with absolute and relative tolerances set to tol.\ntol.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 72.6 48.5 35.3 34.9 35.3 35.4 35.2 35.3 35.1 35.2 35.4 35.4\n0.01 490.7 257.6 495.5 512.8 364.8 208.8 118.4 500.3 140.4 127.9 137.4 146.9\n0.001 3230 1979 3152.5 3732 3926 3575 2813 3111 1001 1044 1014 1047\n0.0001 8359 6365 7965 8830 9191 8851 7883 7924 4888 5003 4844 5071\n1e-05 12673 11113 12237 12514 12698 12712 12491 12232 10954 10845 10147 10865\n32\n\n10−210−1100101102test ﬁxed-point residual\ntraining with ﬁxed-point residual losses\n training with regression losses\n0 100 200 300 400 500\nevaluation iterations010203040 test gain to cold start\n0 100 200 300 400 500\nevaluation iterations\ncold start nearest neighbor warm start\nlearned warm-start k={ 0 5 15 60}\nFigure 13: Phase retrieval results. Other than the k= 0 case, the learned warm starts with\nregression loss improvements are maintained for many evaluation steps.\n6.4.4 Sparse PCA\nNext, we examine the problem of sparse PCA (Zou et al., 2006). Unlike standard PCA (Jol-\nliffe, 2005), which typically finds principal components that depend on all observed variables,\nsparse PCA identifies principal components that rely on only a small subset of the variables.\nThe Sparse PCA problem is\nmaximize xTAx\nsubject to ∥x∥2≤1,card (x)≤c,(20)\nwhere x∈Rnis the decision variable and card (x) is the number of nonzero terms of vector\nx. The covariance matrix A∈Sn\n+and desired cardinality c∈R+are problem data. We\nconsider an SDP relaxation of the non-convex problem (20) which takes the form\nmaximize tr(AX)\nsubject to tr(X) = 1\n1T|X|1≤c\nX⪰0,(21)\nwhere the decision variable is X∈Sn\n+. We use an r-factor model (Boyd et al., 2017) and set\nA=FΣFTwhere F∈Rn×ris the factor loading matrix and Σ ∈Sn\n+is a matrix that holds\nthe factor scores. The parameter is θ=vec(Σ).\n33\n\nNumerical example. We run our experiments with matrix size of n= 40, a factor size of\nr= 10, and a cardinality size of c= 10. To generate the covariance matrices, we first generate\na random nominal matrix A0, whose entries are sampled as an i.i.d. standard Gaussian. We\nthen take the singular value decomposition of A0=UΣ0UT, and let the shared factor loading\nmatrix F∈Rn×rbe the first rsingular vectors of U. Let B0∈Sr\n+be the diagonal matrix\nfound by taking the square root of the first rsingular values of A0. Then, for each problem,\nwe take Σ = BBTwhere B= ∆ + B0. Here, the elements of ∆ ∈Rr×rare sampled i.i.d.\nfrom the uniform distribution U[−0.1,0.1].\nTable 12: Sparse PCA.\n(a) Mean iterations to reach a given fixed point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 26 1 0 2 8 10 10 0 5 8 12 17\n0.01 122 62 262 92 70 33 38 262 33 37 44 55\n0.001 338 269 491 313 289 169 160 490 145 151 154 172\n0.0001 982 822 1000 881 935 766 738 1000 681 698 689 709\n(b) Mean reduction in iterations from a cold start to a given fixed-point residual (Fp res.)\nFp res.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 0 0.96 1.0 0.92 0.69 0.62 0.62 1.0 0.81 0.69 0.54 0.35\n0.01 0 0.49 -1.15 0.25 0.43 0.73 0.69 -1.15 0.73 0.7 0.64 0.55\n0.001 0 0.2 -0.45 0.07 0.14 0.5 0.53 -0.45 0.57 0.55 0.54 0.49\n0.0001 0 0.16 -0.02 0.1 0.05 0.22 0.25 -0.02 0.31 0.29 0.3 0.28\n(c) Mean solve times (in milliseconds) in SCS with absolute and relative tolerances set to tol.\ntol.Cold\nStartNearest\nNeighborFp\nk= 0Fp\nk= 5Fp\nk= 15Fp\nk= 30Fp\nk= 60Reg\nk= 0Reg\nk= 5Reg\nk= 15Reg\nk= 30Reg\nk= 60\n0.1 9.66 0.59 0.65 10.62 8.35 8.22 8.22 0.73 10.06 8.27 8.25 8.4\n0.01 26.34 9.63 97.97 14.31 8.81 8.25 8.22 88.35 10.09 8.25 8.3 9.94\n0.001 67.88 45.91 121.94 71.17 43.23 17.39 17.23 110.4 20.09 16.92 18.56 25.31\n0.0001 136.21 107.61 187.43 140.64 100.76 66.36 63.95 170.05 73.65 61.11 62.77 70.46\n1e-05 255.52 204.37 309.51 264.94 201.32 156.11 151.28 279.27 174.63 144.53 146.15 153.85\nResults. Figure 14 and table 12 show the convergence behavior of our method. In this\nexample, both the fixed-point residual loss and the regression loss perform with k= 0. All of\nthe other learned warm starts with the regression loss and some with the fixed-point residual\nloss show good performance.\nA Examples of fixed-point algorithms\nGradient descent. Here, z∈Rnis the decision variable and fθis a convex and L-smooth\nfunction. Recall that f:Rn→RisL-smooth if ∥∇f(x)−∇f(y)∥2≤L∥x−y∥2∀x, y∈Rn.\nIfα∈(0,2/L), then the iterates of gradient descent are guaranteed to converge to an optimal\n34\n\n10−410−310−210−1100test ﬁxed-point residual\ntraining with ﬁxed-point residual losses\n training with regression losses\n0 100 200 300 400 500\nevaluation iterations0102030 test gain to cold start\n0 100 200 300 400 500\nevaluation iterations\ncold start nearest neighbor warm start\nlearned warm-start k={ 0 5 15 60}\nFigure 14: Sparse PCA. The learned warm starts with positive kthat use the regression loss\nprovide large gains.\nsolution (Ryu and Boyd, 2015). If fθis strongly convex, then the fixed-point operator is a\ncontraction (Ryu and Boyd, 2015).\nProximal gradient descent. Here, z∈Rnis the decision variable, fθis a convex and\nL-smooth function, and gθis a convex but possibly non-smooth function. The iterations of\nproximal gradient descent converge to a solution if α∈(0,2/L) (Parikh and Boyd, 2014).\nAlternating direction method of multipliers (ADMM). Here, u∈Rnis the decision\nvariable and fθandgθare closed, convex, proper, and possibly non-smooth functions. The\niterations of ADMM generate a sequence of iterates, resulting in the convergence of both ˜ ui\nanduito each other and to a solution of the problem. The z∈Rnvariable serves as the\nassociated dual variable. We use the equivalence of ADMM to Douglas-Rachford splitting\n(Gabay, 1983) and write the Douglas-Rachford splitting iterations in Table 1. While the\nassociated fixed-point operator to ADMM is averaged (Ryu and Boyd, 2015), ADMM is\nknown to converge linearly under certain conditions (Eckstein, 1989; Giselsson and Boyd,\n2017).\nOSQP. The operator splitting quadratic program (OSQP) (Stellato et al., 2020) solver is\nbased on ADMM. Here, P∈Sn\n+,A∈Rm×n,c∈Rn,l∈Rm, and u∈Rmare problem data,\nand Π [l,u]is the projection onto the box, [ l, u]. The decision variable is x∈Rn. While the\nalgorithm uses x,w, and yvariables, the fixed-point operator is represented as an operator\non a smaller vector, as shown in Banjac et al. (2019).\n35\n\nSCS. The splitting conic solver (SCS) (O’Donoghue, 2021) is also based on ADMM. Here,\nP∈Sn\n+,A∈Rm×n,c∈Rn, and b∈Rmare problem data, and Π Cis the projection\nonto the cone C. The decision variables are x∈Rnands∈Rm. For simplicity, Table 1\nincludes the simplified version of the SCS algorithm without the homogeneous self-dual\nembedding. The SCS algorithm, the one we use in the numerical experiments in Section 6.4,\nis based on the homogeneous self-dual embedding; see O’Donoghue (2021) for the details. As\nin Venkataraman and Amos (2021), our implementation normalizes the fixed-point residual\nby the τscaling factor to ensure that the fixed-point residual is not artificially small.\nB Proofs\nB.1 Proof of Lemma 1\nLetw′=w+uand let Swbe the set of perturbations w′such that\nSw⊂ {w′|max θ∈Θ∥hw′(θ)−hw(θ)∥2≤γ/2}.\nLetqbe the probability density function over w′. We construct a new distribution ˜Qover\npredictors h˜wwhere ˜ wis restricted to Swwith the probability density function ˜ q( ˜w) =\n(1/Z)q( ˜w) if ˜w∈Swand otherwise 0, where Zis a normalizing constant. By the assumption\nof the lemma, Z=P(w′∈Sw)≥1/2. By the definition of ˜Q, we have\nmax θ∈Θ∥h˜w(θ)−hw(θ)∥2≤γ/2.\nTherefore, ℓfp\nθ(Tt\nθ(hw(θ)))≤gt\nγ/2,θ(h˜w(θ))≤gt\nγ,θ(hw(θ)) almost surely for every θ∈Θ. Hence,\nfor every ˜ wdrawn from the probability density function ˜Q, almost surely,\nRt(w)≤Rt\nγ/2( ˜w),ˆRt\nγ/2( ˜w)≤ˆRt\nγ(w). (22)\nNow using these two inequalities above and the PAC-Bayes theorem, we get\nRt(w)≤E˜w[Rt\nγ/2( ˜w)]\n≤E˜w[ˆRt\nγ/2( ˜w)] + 2 Cγ/2(t)p\n(2KL( ˜w||π) + log(2 N/δ))/(N−1)\n≤ˆRt\nγ(w) + 2Cγ/2(t)p\n(2KL( ˜w||π) + log(2 N/δ))/(N−1)\n≤ˆRt\nγ(w) + 4Cγ/2(t)p\n(2KL(w′||π) + log(6 N/δ))/(N−1).\nThe first and third inequalities come from (22), and the second inequality follows from (11).\nThe last inequality comes from the following calculation which we repeat from Neyshabur\net al. (2018, Section 4). Let Sc\nwdenote the complement of Swand ˜qcdenote the density\nfunction qrestricted to Sc\nwand normalized. Then we get\nKL(q||p) =ZKL(˜q||p) + (1 −Z)KL(˜ qc||p)−H(Z),\nwhere H(Z) =−ZlogZ−(1−Z) log(1 −Z) is the binary entropy function. Since the\nKL-divergence is always positive,\nKL(˜q||p) = [KL( q||p) +H(Z)−(1−Z)KL(˜qc||p)]/Z≤2(KL( q||p) + 1) .\nUsing the additive properties of logarithms, 1 + log(2 N/δ)≤log(6 N/δ).\n36\n\nB.2 Proof of Theorem 2\nOur proof follows a similar structure as the proof of Neyshabur et al. (2018, Theorem 1).\nLetζ= (ΠL\ni=1∥Wi∥2)1/Land consider a neural network with weights ˜Wi=ζWi/∥Wi∥2.\nDue to the homogeneity of the ReLU, we have hw(θ) =h˜w(θ) for all θ∈Θ (Neyshabur\net al., 2018). Since (ΠL\ni=1∥Wi∥2)1/L= (ΠL\ni=1∥˜Wi∥2)1/Land∥Wi∥F/∥Wi∥2=∥˜Wi∥F/∥˜Wi∥2,\ninequality (13) is the same for wand ˜w. Therefore, it is sufficient to prove the theorem\nonly for the normalized weights ˜ wand we can assume that the spectral norm of the weight\nmatrix is equal across all layers, i.e.,∥Wi∥2=ζ. Now, we break our proof into two cases\ndepending on the product of the spectral norm of the weight matrices. The main difference\nbetween our proof and the proof for Neyshabur et al. (2018, Theorem 1) is that we introduce\na secondary case. The main case analysis is similar.\nMain case. In the main case, ζL≥γ/(2B). We choose the prior distribution πto be\nN(0, σ2) and consider the perturbation u∼ N(0, σ2). As in Neyshabur et al. (2018), since\nthe prior distribution πcannot depend on ζ, we consider predetermined values of ˜ζon a\ngrid and then do a union bound. For now, we consider ˜ζfixed and consider all ζsuch that\n|ζ−˜ζ| ≤ζ/L. This ensures that each relevant value of ζis covered by some ˜ζon the grid.\nSince|ζ−˜ζ| ≤ζ/Lwe get the inequalities\nζL−1/e≤˜ζL−1≤eζL−1. (23)\nThis follows from the inequalities (1 + 1 /x)x−1≤eand 1 /e≤(1−1/x)x−1which themselves\nare consequences the inequality 1 + y≤eyfor all y. Since the entries of each Uiare drawn\nfromN(0, σ2), we have the bound on the spectral norm of each Ui(Tropp, 2011)\nPUi∼N(0,σ2)(∥Ui∥2> t)≤2¯he−t2/(2¯hσ2).\nWe can take a union bound to get\nPU1,...,U L∼N(0,σ2)(∥U1∥2≤t, . . . ,∥UL∥2≤t)≥1−2L¯he−t2/(2¯hσ2). (24)\nBy setting the right hand side of (24) to 1 /2, we establish that with probability at least 1 /2,\nthe spectral norm of every perturbation Uiis bounded by σp\n2¯hlog(4 L¯h) simultaneously.\nWe choose σ=γ/(21LB˜ζL−1p¯hlog(4 ¯hL)) and now verify that with probability at least\n1/2,∥Ui∥2≤ ∥Wi∥2/L=ζ/Lholds, a condition of Neyshabur et al. (2018, Lemma 2):\n∥Ui∥2≤σq\n2¯hlog(4 L¯h) =γ√\n2/(21LB˜ζL−1)\n≤e2√\n2γ/(42LBζL−1)≤2√\n2eζ/(21L)≤ζ/L.\nIn the first line, the inequality comes from the perturbation bound on ∥Ui∥2, and the equality\nfollows from plugging in for σ. The second line follows from (23), and the assumption from\n37\n\nthe main case that ζL> γ/ (2B). Now that the conditions are met, we apply Neyshabur\net al. (2018, Lemma 2). The following holds with probability at least 1 /2:\nmax θ∈Θ∥hw(θ)−hw+u(θ)∥2≤eBζL−1PL\ni=1∥Ui∥2\n≤e2LB˜ζL−1σp\n2¯hlog(4 L¯h)≤γ/2.\nIn the second inequality, we use (23). The last inequality follows from the choice of σ. Now\nwe calculate the KL-term with π∼ N(0, σ2) and uchosen with the above value of σ:\nKL(w+u||π)≤∥w∥2\n2\n2σ2=212L2B2˜ζ2L−2¯hlog(4 ¯hL)\n2γ2PL\ni=1∥Wi∥2\nF\n≤212ζ2L\n2γ2B2L2¯hlog(4 L¯h)PL\ni=1∥Wi∥2\nF\nζ2. (25)\nWhat remains is to take a union bound over the different choices of ˜ζ. We only need to\nconsider values of ζin the range of\n(γ/(2B))1/L≤ζ≤(γ√\nN/(2B))1/L. (26)\nSince we are in the main case, we do not have to consider ζL< γ/ (2B). Alternatively, if\nζL> γ√\nN/(2B), then the upper bound on the KL term in (25) is greater than N. To\nsee this, first note that the frobenius norm is always at least the operator norm of a given\nmatrix, so ∥Wi∥F≥ζfori= 1, . . . , L . Then, the right hand side of (25) becomes at least\n212L2¯hlog(4 L¯h)N/8 which is greater than N. Theorem 2 is obtained by using the bound in\nthe right hand side of (25) for the KL term in Lemma 1. Therefore Theorem 2 holds trivially\nsince Cγ/2(t) upper bounds Rt(w) and the entire square root term in Lemma 1 is at least\none. Hence, we only need to consider ζin the range of (26).\nThe condition L|˜ζ−ζ| ≤(γ/(2B))1/Lis sufficient to satisfy the required condition that\n|˜ζ−ζ| ≤ζ/Lsince ζL≥γ/(2B). For each ˜ζthat we pick, we consider ζwithin a distance\nof (γ/(2B))1/L/L. We need to pick enough ˜ζ’s to cover the whole region in (26). Picking a\ncover size of LN1\n2Lsatisfies this condition since\n(γ√\nN\n2B)1/L−(γ\n2B)1/L\n1\nL(γ\n2B)1/L=L(N1/(2L)−1).\nTherefore, by using Lemma 1, with probability at most ˜δand for all ˜ wsuch that |ζ−˜ζ| ≤ζ/L,\nthe following bound is violated:\nRt(w)≤ˆRt\nγ( ˜w) +O\ns\nB2L2log(L¯h)ΠL\nj=1∥˜Wj∥2\n2PL\ni=1∥˜Wi∥2\nF\n∥˜Wi∥2\n2+log(N\n˜δ)\nγ2N\n.\nBy applying the union bound over the cover size, with probability at most ˜δLN1/(2L), the\nsame bound is violated for at least one of the ˜ζ’s out of the cover. Setting δ=˜δLN1/(2L)\nand recalling that the proof generalizes from normalized weights ˜ wto weights wgives the\nfinal result.\n38\n\nSecondary case. In this case, ∥hw(θ)∥2≤B(ΠL\ni=1∥Wi∥2)≤γ/2. We get the following:\nRt(w)≤Rt\nγ/2(0)\n≤ˆRt\nγ/2(0) + Cγ/2(t)p\nlog(1 /δ)/(2N) w.p. at least 1 −δ\n≤ˆRt\nγ(w) +Cγ/2(t)p\nlog(1 /δ)/(2N) w.p. at least 1 −δ\nThe first and third lines come from ∥hw(θ)∥2≤γ/2 and the definition of the marginal\nfixed-point residual. The second lines uses Hoeffding’s inequality as in Alquier (2023, Equa-\ntion 1.3), which is permissible since the prediction is the zero vector and is therefore inde-\npendent of the data.\nB.3 Proof of Lemma 3\nFirst, let z⋆(θ) be the nearest fixed-point of the operator Tθtozso that rθ(z) =∥z−z⋆(θ)∥2.\nℓfp\nθ(z) =∥Tθ(z)−z∥2≤ ∥Tθ(z)−z⋆(θ)∥2+∥z−z⋆(θ)∥2≤2rθ(z)\nThe first inequality uses the triangle inequality, and the second inequality uses the non-\nexpansiveness of Tθ.\nB.4 Proof of Lemma 5\n|rθ(Tt\nθ(z))−rθ(Tt\nθ(w))|=|∥Tk\nθ(z)−ΠfixTθ(Tk\nθ(z))∥2− ∥Tk\nθ(w)−ΠfixTθ(Tk\nθ(w))∥2|\n≤ ∥Tk\nθ(z)−ΠfixTθ(Tk\nθ(z)) + Π fixTθ(Tk\nθ(w))−Tk\nθ(w)∥2\n≤ ∥Tk\nθ(z)−Tk\nθ(w)∥2+∥ΠfixTθ(Tk\nθ(z))−ΠfixTθ(Tk\nθ(w))∥2\n≤2∥Tk\nθ(z)−Tk\nθ(w)∥2≤2∥z−w∥2\nThe first two inequalities use the reverse triangle inequality and triangle inequality. Since\nTθis non-expansive, fixTθis a convex set (Ryu and Boyd, 2015, Section 2.4.1). The third\ninequality follows since the projection onto a convex set is non-expansive (Ryu and Boyd,\n2015, Section 3.1). In the last inequality, we use the non-expansiveness of Tθ.\nReferences\nP. Alquier. User-friendly introduction to pac-bayes bounds. arXiv e-prints , 2023.\nB. Amos. Tutorial on amortized optimization. Foundations and Trends in Machine Learning ,\n16(5):592–732, 2023. ISSN 1935-8237.\nM. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W. Hoffman, D. Pfau, T. Schaul,\nB. Shillingford, and N. de Freitas. Learning to learn by gradient descent by gradient\ndescent. In Proceedings of the 30th International Conference on Neural Information Pro-\ncessing Systems , NIPS’16, page 3988–3996, Red Hook, NY, USA, 2016. Curran Associates\nInc. ISBN 9781510838819.\n39\n\nS. Bai, V. Koltun, and J. Z. Kolter. Neural deep equilibrium solvers. In International\nConference on Learning Representations , 2022.\nK. Baker. Learning warm-start points for ac optimal power flow. In 2019 IEEE 29th Inter-\nnational Workshop on Machine Learning for Signal Processing (MLSP) , pages 1–6, 2019.\nA. Balatsoukas-Stimming and C. Studer. Deep unfolding for communications systems: A sur-\nvey and some new directions. In 2019 IEEE International Workshop on Signal Processing\nSystems (SiPS) , pages 266–271, 2019.\nS. Banert, J. Rudzusika, O. ¨Oktem, and J. Adler. Accelerated forward-backward optimiza-\ntion using deep learning. arXiv preprint arXiv:2105.05210 , 2021.\nG. Banjac, P. Goulart, B. Stellato, and S. Boyd. Infeasibility detection in the alternating\ndirection method of multipliers for convex optimization. Journal of Optimization Theory\nand Applications , 183, 2019.\nP. L. Bartlett, P. Indyk, and T. Wagner. Generalization bounds for data-driven numerical\nlinear algebra. In P. Loh and M. Raginsky, editors, Conference on Learning Theory, 2-5\nJuly 2022, London, UK , volume 178 of Proceedings of Machine Learning Research , pages\n2013–2040. PMLR, 2022.\nH. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in\nHilbert Spaces . Springer, 1st edition, 2011.\nA. G. Baydin, B. A. Pearlmutter, A. Radul, and J. M. Siskind. Automatic differentiation in\nmachine learning: a survey. J. Mach. Learn. Res. , 18:153:1–153:43, 2017.\nA. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm with application\nto wavelet-based image deblurring. In 2009 IEEE International Conference on Acoustics,\nSpeech and Signal Processing , pages 693–696, 2009.\nR. Bellman. Dynamic Programming . Dover Publications, 1957. ISBN 9780486428093.\nF. Benvenuto, R. Zanella, L. Zanni, and M. Bertero. Nonnegative least-squares image de-\nblurring: improved gradient projection approaches. Inverse Problems , 26(2):025004, 2010.\nD. Bertsimas and B. Stellato. Online mixed-integer optimization in milliseconds. arXiv\ne-prints , 2019.\nD. Bertsimas and B. Stellato. The voice of optimization. Machine Learning , 110:249–277,\n2021.\nF. Borrelli, A. Bemporad, and M. Morari. Predictive Control for Linear and Hybrid Systems .\nCambridge University Press, 2017.\nS. Boyd, E. Busseti, S. Diamond, R. N. Kahn, K. Koh, P. Nystrup, and J. Speth. Multi-\nperiod trading via convex optimization, 2017.\n40\n\nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula,\nA. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable trans-\nformations of Python+NumPy programs, 2018.\nL. M. Brice˜ no-Arias and P. L. Combettes. Monotone operator methods for nash equilibria in\nnon-potential games. In D. H. Bailey, H. H. Bauschke, P. Borwein, F. Garvan, M. Th´ era,\nJ. D. Vanderwerff, and H. Wolkowicz, editors, Computational and Analytical Mathematics ,\npages 143–159, New York, NY, 2013. Springer New York. ISBN 978-1-4614-7621-4.\nE. J. Cand` es, X. Li, and M. Soltanolkotabi. Phase retrieval from coded diffraction patterns.\nApplied and Computational Harmonic Analysis , 39(2):277–299, 2015. ISSN 1063-5203.\nJ. R. Chang, C.-L. Li, B. P´ oczos, B. Vijaya Kumar, and A. C. Sankaranarayanan. One\nnetwork to solve them all — solving linear inverse problems using deep projection models.\nIn2017 IEEE International Conference on Computer Vision (ICCV) , pages 5889–5898,\n2017.\nS. Chen, K. Saulnier, N. Atanasov, D. D. Lee, V. Kumar, G. J. Pappas, and M. Morari.\nApproximating explicit model predictive control using constrained neural networks. In\n2018 Annual American Control Conference (ACC) , pages 1520–1527, 2018.\nS. W. Chen, T. Wang, N. Atanasov, V. Kumar, and M. Morari. Large scale model predictive\ncontrol with neural networks and primal active sets. Automatica , 135:109947, 2022. ISSN\n0005-1098.\nT. Chen, X. Chen, W. Chen, H. Heaton, J. Liu, Z. Wang, and W. Yin. Learning to optimize:\nA primer and a benchmark, 2021.\nX. Chen, Y. Zhang, C. Reisinger, and L. Song. Understanding deep architectures with\nreasoning layer. In Proceedings of the 34th International Conference on Neural Information\nProcessing Systems , NIPS’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN\n9781713829546.\nG. Cohen, S. Afshar, J. Tapson, and A. van Schaik. Emnist: an extension of mnist to\nhandwritten letters, 2017.\nC. de Boor. On calculating with b-splines. Journal of Approximation Theory , 6(1):50–62,\n1972. ISSN 0021-9045.\nS. Diamond, V. Sitzmann, F. Heide, and G. Wetzstein. Unrolled optimization with deep\npriors. ArXiv , abs/1705.08041, 2017.\nM. Diehl, H. J. Ferreau, and N. Haverbeke. Efficient Numerical Methods for Nonlinear\nMPC and Moving Horizon Estimation , pages 391–417. Springer Berlin Heidelberg, Berlin,\nHeidelberg, 2009. ISBN 978-3-642-01094-1.\n41\n\nM. Dinitz, S. Im, T. Lavastida, B. Moseley, and S. Vassilvitskii. Faster matchings via learned\nduals. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances\nin Neural Information Processing Systems , 2021.\nJ. Douglas and H. H. Rachford. On the numerical solution of heat conduction problems in\ntwo and three space variables. Transactions of the American Mathematical Society , 82(2):\n421–439, 1956. ISSN 00029947.\nA. d’Aspremont, D. Scieur, and A. Taylor. Acceleration methods. Foundations and Trends\nin Optimization , 5(1-2):1–245, 2021. ISSN 2167-3888.\nJ. Eckstein. Splitting methods for monotone operators with applications to parallel optimiza-\ntion. PhD thesis, Massachusetts Institute of Technology, 1989.\nL. El Ghaoui and H. Lebret. Robust solutions to least-squares problems with uncertain data.\nSIAM Journal on Matrix Analysis and Applications , 18:1035–1064, 1997.\nM. Elad and M. Aharon. Image denoising via sparse and redundant representations over\nlearned dictionaries. IEEE Transactions on Image Processing , 15(12):3736–3745, 2006.\nJ. R. Fienup. Phase retrieval algorithms: a comparison. Appl. Opt. , 21(15):2758–2769, 1982.\nC. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International\nConference on Machine Learning , volume 70 of Proceedings of Machine Learning Research ,\npages 1126–1135. PMLR, 2017.\nD. Gabay. Applications of the method of multipliers to variational inequalities. 1983.\nD. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems\nvia finite element approximation. Computers & Mathematics With Applications , 2:17–40,\n1976.\nM. Garstka, M. Cannon, and P. Goulart. COSMO: A conic operator splitting method for\nlarge convex problems. In European Control Conference , 2019.\nP. Giselsson and S. Boyd. Linear convergence and metric selection for douglas-rachford\nsplitting and admm. IEEE Transactions on Automatic Control , 62(2):532–544, 2017.\nK. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In Proceedings\nof the 27th International Conference on International Conference on Machine Learning ,\nICML’10, page 399–406, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077.\nR. Gupta and T. Roughgarden. A pac approach to application-specific algorithm selection.\nSIAM Journal on Computing , 46(3):992–1017, 2017.\nH. He, C.-K. Wen, S. Jin, and G. Y. Li. Model-driven deep learning for mimo detection.\nIEEE Transactions on Signal Processing , 68:1702–1715, 2020.\n42\n\nH. Heaton, X. Chen, Z. Wang, and W. Yin. Safeguarded learned convex optimization. ArXiv ,\nabs/2003.01880, 2020.\nP. V. Hentenryck. Machine Learning for Optimal Power Flows , chapter 3, pages 62–82.\n2021.\nM. Hong, Z.-Q. Luo, and M. Razaviyayn. Convergence analysis of alternating direction\nmethod of multipliers for a family of nonconvex problems. SIAM Journal on Optimization ,\n26(1):337–364, 2016.\nT. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural networks:\nA survey, 2020.\nP. J. Huber. Robust estimation of a location parameter. The Annals of Mathematical\nStatistics , 35(1):73–101, 1964. ISSN 00034851.\nJ. Ichnowski, P. Jain, B. Stellato, G. Banjac, M. Luo, F. Borrelli, J. E. Gonzales, I. Stoica,\nand K. Goldberg. Accelerating quadratic optimization with reinforcement learning. In\nAdvances in Neural Information Processing Systems 35 , 2021.\nI. Jolliffe. Principal Component Analysis . John Wiley & Sons, Ltd, 2005. ISBN\n9780470013199.\nH. Jung, J. Park, and J. Park. Learning context-aware adaptive solvers to accelerate\nquadratic programming, 2022.\nR. E. Kalman. A new approach to linear filtering and prediction problems. Transactions of\nthe ASME–Journal of Basic Engineering , 82(Series D):35–45, 1960.\nB. Karg and S. Lucia. Efficient representation and approximation of model predictive control\nlaws via deep learning. IEEE Transactions on Cybernetics , PP:1–13, 2020.\nM. Khodak, N. Balcan, A. Talwalkar, and S. Vassilvitskii. Learning predictions for algorithms\nwith predictions. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances\nin Neural Information Processing Systems , 2022.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and\nY. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015.\nT. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 International Conference on Management of Data ,\nSIGMOD ’18, page 489–504, New York, NY, USA, 2018. Association for Computing Ma-\nchinery. ISBN 9781450347037.\nK. Li and J. Malik. Learning to optimize. arXiv e-prints , 2016.\n43\n\nM. Li, S. Kolouri, and J. Mohammadi. Learning to solve optimization problems with hard\nlinear constraints. IEEE Access , 11:59995–60004, 2023.\nJ. Liu, X. Chen, Z. Wang, and W. Yin. ALISTA: Analytic weights are as good as learned\nweights in LISTA. In International Conference on Learning Representations , 2019.\nT. W. K. Mak, M. Chatzos, M. Tanneau, and P. V. Hentenryck. Learning regionally decen-\ntralized ac optimal power flows with admm, 2023.\nD. A. McAllester. Some pac-bayesian theorems. In Proceedings of the Eleventh Annual\nConference on Computational Learning Theory , COLT’ 98, page 230–234, New York, NY,\nUSA, 1998. Association for Computing Machinery. ISBN 1581130570.\nD. A. McAllester. Simplified pac-bayesian margin bounds. In Annual Conference Computa-\ntional Learning Theory , 2003.\nL. Metz, J. Harrison, C. D. Freeman, A. Merchant, L. Beyer, J. Bradbury, N. Agrawal,\nB. Poole, I. Mordatch, A. Roberts, and J. Sohl-Dickstein. Velo: Training versatile learned\noptimizers by scaling up, 2022.\nR. P. Millane. Phase retrieval in crystallography and optics. J. Opt. Soc. Am. A , 7(3):\n394–411, 1990.\nS. Misra, L. Roald, and Y. Ng. Learning for constrained optimization: Identifying optimal\nactive constraint sets. INFORMS J. on Computing , 34(1):463–480, 2022. ISSN 1526-5528.\nM. Mitzenmacher and S. Vassilvitskii. Algorithms with predictions, 2020.\nV. Monga, Y. Li, and Y. C. Eldar. Algorithm unrolling: Interpretable, efficient deep learning\nfor signal and image processing. IEEE Signal Processing Magazine , 38(2):18–44, 2021.\nB. Neyshabur, S. Bhojanapalli, and N. Srebro. A PAC-bayesian approach to spectrally-\nnormalized margin bounds for neural networks. In International Conference on Learning\nRepresentations , 2018.\nB. O’Donoghue. Operator splitting for a homogeneous embedding of the linear complemen-\ntarity problem. SIAM Journal on Optimization , 31(3):1999–2023, 2021.\nB. O’Donoghue, E. Chu, N. Parikh, and S. Boyd. SCS: Splitting conic solver, version 2.1.2.\nhttps://github.com/cvxgrp/scs , 2019.\nN. Parikh and S. Boyd. Proximal algorithms. Found. Trends Optim. , 1(3):127–239, 2014.\nISSN 2167-3888.\nI. Pr´ emont-Schwarz, J. Vitku, and J. Feyereisl. A simple guard for learned optimizers.\nIn K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv´ ari, G. Niu, and S. Sabato, editors,\nInternational Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,\n44\n\nMaryland, USA , volume 162 of Proceedings of Machine Learning Research , pages 17910–\n17925. PMLR, 2022.\nM. Purohit, Z. Svitkina, and R. Kumar. Improving online algorithms via ml predictions. In\nS. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\nitors, Advances in Neural Information Processing Systems , volume 31. Curran Associates,\nInc., 2018.\nR. Rockafellar and R. J.-B. Wets. Variational Analysis . Springer Verlag, Heidelberg, Berlin,\nNew York, 1998.\nD. Rohatgi. Near-Optimal Bounds for Online Caching with Machine Learned Advice , pages\n1834–1845. 2020.\nS. Ruder. An overview of multi-task learning in deep neural networks, 2017.\nE. K. Ryu and S. P. Boyd. A primer on monotone operator methods. 2015.\nE. K. Ryu and W. Yin. Large-Scale Convex Optimization: Algorithms amp; Analyses via\nMonotone Operators . Cambridge University Press, 2022.\nS. Sakaue and T. Oki. Discrete-convex-analysis-based framework for warm-starting algo-\nrithms with predictions. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,\nand A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages\n20988–21000. Curran Associates, Inc., 2022.\nR. Sambharya, G. Hall, B. Amos, and B. Stellato. End-to-End Learning to Warm-Start for\nReal-Time Quadratic Optimization. In N. Matni, M. Morari, and G. J. Pappas, editors,\nProceedings of the 5th Annual Learning for Dynamics and Control Conference , volume 211\nofProceedings of Machine Learning Research , pages 220–234. PMLR, 2023.\nJ. Shawe-Taylor and R. C. Williamson. A pac analysis of a bayesian estimator. In Proceedings\nof the Tenth Annual Conference on Computational Learning Theory , COLT ’97, page 2–9,\nNew York, NY, USA, 1997. Association for Computing Machinery. ISBN 0897918916.\nY. Shechtman, Y. C. Eldar, O. Cohen, H. N. Chapman, J. Miao, and M. Segev. Phase\nretrieval with application to optical imaging: A contemporary overview. IEEE Signal\nProcessing Magazine , 32(3):87–109, 2015.\nJ. Sj¨ olund and M. B˚ ankestad. Graph-based neural acceleration for nonnegative matrix fac-\ntorization, 2022.\nY. Song and D. Scaramuzza. Policy search for model predictive control with application to\nagile drone flight. IEEE Transactions on Robotics , 38(4):2114–2130, 2022.\nP. Sopasakis, K. Menounou, and P. Patrinos. Superscs: fast and accurate large-scale conic\noptimization. pages 1500–1505, 2019.\n45\n\nS. Sra, S. Nowozin, and S. J. Wright. Optimization for Machine Learning . The MIT Press,\n2011. ISBN 026201646X.\nB. Stellato, G. Banjac, P. Goulart, A. Bemporad, and B. Stephen. OSQP: An Operator\nSplitting Solver for Quadratic Programs. Mathematical Programming Computation , 12\n(4):637–672, 2020.\nH. Y. Tan, S. Mukherjee, J. Tang, and C.-B. Sch¨ onlieb. Data-driven mirror descent with\ninput-convex neural networks. SIAM Journal on Mathematics of Data Science , 5(2):558–\n587, 2023.\nJ. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Com-\nputational Mathematics , 12(4):389–434, 2011.\nS. Venkataraman and B. Amos. Neural fixed-point acceleration for convex optimization.\nCoRR , abs/2107.10254, 2021.\nR. Vilalta and Y. Drissi. A perspective view and survey of meta-learning. Artificial Intelli-\ngence Review , 18, 2001.\nH. F. Walker and P. Ni. Anderson acceleration for fixed-point iterations. SIAM Journal on\nNumerical Analysis , 49(4):1715–1735, 2011.\nK. Wang, B. Wilder, A. Perrault, and M. Tambe. Automatically learning compact quality-\naware surrogates for optimization problems. In Proceedings of the 34th International Con-\nference on Neural Information Processing Systems , NIPS’20, Red Hook, NY, USA, 2020.\nCurran Associates Inc. ISBN 9781713829546.\nY. Wang, W. Yin, and J. Zeng. Global convergence of admm in nonconvex nonsmooth\noptimization. J. Sci. Comput. , 78(1):29–63, 2019. ISSN 0885-7474.\nK. Wu, Y. Guo, Z. Li, and C. Zhang. Sparse coding with gated learned ista. In International\nConference on Learning Representations , 2020.\nL. Wu, P. Cui, J. Pei, and L. Zhao. Graph Neural Networks: Foundations, Frontiers, and\nApplications . Springer Singapore, Singapore, 2022.\nL. Xie and Y. C. Soh. Robust kalman filtering for uncertain systems. Systems & Control\nLetters , 22(2):123–129, 1994. ISSN 0167-6911.\nA. Yurtsever, J. A. Tropp, O. Fercoq, M. Udell, and V. Cevher. Scalable semidefinite\nprogramming. SIAM Journal on Mathematics of Data Science , 3(1):171–200, 2021.\nA. S. Zamzam and K. Baker. Learning optimal solutions for extremely fast ac optimal\npower flow. In 2020 IEEE International Conference on Communications, Control, and\nComputing Technologies for Smart Grids (SmartGridComm) , pages 1–6, 2020.\n46\n\nJ. Zhang, B. O’Donoghue, and S. Boyd. Globally convergent type-I anderson acceleration\nfor nonsmooth fixed-point iterations. SIAM Journal on Optimization , 30(4):3170–3197,\n2020.\nK. Zhang, W. Zuo, S. Gu, and L. Zhang. Learning deep cnn denoiser prior for image restora-\ntion. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\npages 2808–2817, Los Alamitos, CA, USA, 2017. IEEE Computer Society.\nX. Zhang, M. Bujarbaruah, and F. Borrelli. Safe and near-optimal policy learning for model\npredictive control using primal-dual neural networks. In 2019 American Control Confer-\nence (ACC) , pages 354–359, 2019.\nH. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of\nComputational and Graphical Statistics , 15(2):265–286, 2006. ISSN 10618600.\n47",
  "textLength": 111986
}