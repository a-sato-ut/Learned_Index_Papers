{
  "paperId": "2d70288c89a05c59f5306b6edf4342cda2e8cc15",
  "title": "Online Page Migration with ML Advice",
  "pdfPath": "2d70288c89a05c59f5306b6edf4342cda2e8cc15.pdf",
  "text": "Online Page Migration with ML Advice\nPiotr Indyk\u0003Frederik Mallmann-TrennySlobodan Mitrovi\u0013 c\u0003\nRonitt Rubinfeldz\nAbstract\nWe consider online algorithms for the page migration problem that use predictions, potentially\nimperfect, to improve their performance. The best known online algorithms for this problem, due\nto Westbrook'94 and Bienkowski et al'17, have competitive ratios strictly bounded away from 1.\nIn contrast, we show that if the algorithm is given a prediction of the input sequence, then it can\nachieve a competitive ratio that tends to 1 as the prediction error rate tends to 0. Speci\fcally, the\ncompetitive ratio is equal to 1+ O(q), where qis the prediction error rate. We also design a \\fallback\noption\" that ensures that the competitive ratio of the algorithm for anyinput sequence is at most\nO(1=q). Our result adds to the recent body of work that uses machine learning to improve the\nperformance of \\classic\" algorithms.\n1 Introduction\nRecently, there has been a lot of interest in using machine learning to design improved algorithms\nfor various computational problems. This includes work on data structures [KBC+18, Mit18], online\nalgorithms [LV18, PSK18, GP19a, Roh20], combinatorial optimization [KDZ+17, BDSV18], similarity\nsearch [WLKC16], compressive sensing [MPB15, BJPD17] and streaming algorithms [HIKV19]. This\nbody of work is motivated by the fact that modern machine learning methods are capable of discover-\ning subtle structure in collections of input data, which can be utilized to improve the performance of\nalgorithms that operate on similar data.\nIn this paper we focus on learning-augmented online algorithms . An on-line algorithm makes non-\nrevocable decisions based only on the part of the input seen so far, without anyknowledge of the future.\nIt is thus natural to consider a relaxation of the model where the algorithm has access to (imperfect)\npredictors of the future input that could be used to improve the algorithm performance. Over the last\ncouple of years this line of research has attracted growing attention in the machine learning and algorithms\nliterature, for classical on-line problems such as caching [LV18, Roh20], ski-rental and scheduling [PSK18,\nGP19b, LLMV20] and graph matching [KPS+19]. Interestingly, most of the aforementioned works\nconclude that the \\optimistic\" strategy of simply following the predictions, i.e., executing the optimal\nsolution computed o\u000b-line for the predicted input, can lead to a highly sub-optimal performance even\nif the prediction error is small.1For instance, for caching, even a single misprediction can lead to an\nunbounded competitive ratio [LV18].\nIn this paper we show that, perhaps surprisingly, the aforementioned \\optimistic\" strategy leads\nto near-optimal performance for some well-studied on-line problems. We focus on the problem of page\nmigration [BS89] (a.k.a. \fle migration [Bie12] or 1-server with excursions [MMS90]). Here, the algorithm\nis given a sequence sof points (called requests )s1;s2;:::from a metric space ( X;d), in an online fashion.\nThe state of the algorithm is also a point from ( X;d). Given the next request si, the algorithm moves to its\nnext stateai(at the cost of D\u0001d(ai\u00001;ai), whereDis a parameter), and then \\satis\fes\" the request si(at\nthe cost ofd(ai;si)). The objective is to satisfy all requests while minimizing the total cost. The problem\nhas been a focus on a large body of research, see e.g., [ABF93, Wes94, CLRW97, BCI97, KM16, BBM17].\n\u0003CSAIL, MIT, {indyk,slobo}@mit.edu\nyKing's College London, frederik.mallmann-trenn@kcl.ac.uk\nzCSAIL, MIT, ronitt@csail.mit.edu\n1To the best of our knowledge the only problem for which this strategy is known to result in an optimal algorithm is\nthe online bipartite matching, see Section 1.1 for more details.\n1arXiv:2006.05028v1  [cs.DS]  9 Jun 2020\n\nThe best known algorithms for this problem have competitive ratios of 4 (a deterministic algorithm due\nto [BBM17]), 3 (a randomized algorithm against adaptive adversaries due to [Wes94]) and 2 :618:::\n(a randomized algorithm against oblivious adversaries due to [Wes94]). The original paper [BS89] also\nshowed that the competitive ratio of any deterministic algorithm must be at least 3, which was recently\nimproved to 3 + \u000ffor some\u000f>0 by [Mat15].\nOur results Suppose that we are given a predicted request sequence ^ sthat, in each interval of length\n\u000fD, di\u000bers from the actual sequence son at most a fraction qof positions, where \u000f;q2(0;1) are the\nparameters (note that the lower the values of \u000fandqare, the stronger our assumption is). Under this\nassumption we show that the optimal o\u000b-line solution for ^ sis a (1+\u000f)(1+O(q))-competitive solution for s\nas long as the parameter q>0 is a small enough constant. Thus, the competitive ratio of this prediction-\nbased algorithm improves over the state of the art even if the number of errors is linear in the sequence\nlength, and tends to 1 when the error rate tends to 0.2Furthermore, to make the algorithm robust,\nwe also design a \\fallback option\", which is triggered if the input sequence violates the aforementioned\nassumption (i.e., if the fraction of errors in the su\u000ex of the current input sequence exceeds q). The\nfallback option ensures that the competitive ratio of the algorithm for anyinput sequence is at most\nO(1=q). Thus, our \fnal algorithm produces a near-optimal solution if the prediction error is small, while\nguaranteeing a constant competitive ratio otherwise.\nFor the case when the underlying metric is uniform , i.e., all distances between distinct points are\nequal to 1, we further improve the competitive ratio to 1 + O(q) under the assumption that each interval\nof lengthDdi\u000bers from the actual sequence in at most qDpositions. That is, the parameter \u000fis not\nneeded in this case. Moreover, any algorithm has a competitive ratio of at least 1 + \n( q).\nIt is natural to wonder whether the same guarantees hold even when the predicted sequence di\u000bers\nfrom the actual sequence on at most a fraction of qpositions distributed arbitrarily over ^s, as opposed\nto over chunks of length \"D. We construct a simple example that shows that such a relaxed assumption\nresults in the same lower bound as for the classical problem.\n1.1 Related Work\nMultiple variations of the page migration problem have been studied over the years. For example, if the\npage can be copied as well as moved, the problem has been studied under the name of \fle allocation ,\nsee e.g., [BFR95, ABF03, LRWY98]. Other formulations add constraints on nodes capacities, allow\ndynamically changing networks etc. See the survey [Bie12] for an overview.\nThere is a large body of work concerning on-line algorithms working under stochastic or probabilistic\nassumptions about the input [Unc16]. In contrast, in this paper we do not make such assumptions,\nand allow worst case prediction errors (similarly to [LV18, KPS+19, PSK18]). Among these works, our\nprediction error model (bounding the fraction of mispredicted requests) is most similar to the \\agnostic\"\nmodel de\fned in [KPS+19]. The latter paper considers on-line matching in bipartite graphs, where a\nprediction of the graph is given in advance, but the \fnal input graph can deviate from the prediction\nondvertices. Since each vertex impacts at most one matching edge, it directly follows that derrors\nreduce the matching size by at most d. In contrast, in our case a single error can a\u000bect the cost of the\noptimum solution by an arbitrary amount. Thus, our analysis requires a more detailed understanding of\nthe properties of the optimal solution.\nMultiple papers studied on-line algorithms that are given a small number of bits of advice [BFK+17]\nand show that, in many scenarios, this can improve their competitive ratios. Those algorithms, however,\ntypically assume that the advice is error-free.\n2 Preliminaries\nPage Migration In the classical version, the algorithm is given a sequence sof points (called requests )\ns= (si)i2[n]from a metric space ( X;d), in an online fashion. The state of the algorithm (i.e., the\npage), is also a point from ( X;d). Given the next request si, the algorithm moves to its next state\n2Note that if each interval of length Dhas at most a fraction of qof errors, then it is also the case that each interval\nof lengthpqDhas at most a fraction ofpqof errors. Thus, if qtends to 0, the competitive ratio tends to 1 even if the\ninterval length remains \fxed.\n2\n\nai(at the cost of D\u0001d(ai\u00001;ai), whereD > 1 is a parameter), and then \\satis\fes\" the request si(at\nthe cost of d(ai;si)). The objective is to satisfy all requests while minimizing the total cost. We can\nconsider a version of this problem where the algorithm is given, prior to the arrival of the requests, a\npredicted sequence ^s= (s\u0003\ni)i2[n]. The (\fnal) sequence sis generated adversarially from ^ sand an arbitrary\nadversarial sequence s?= (s\u0003\ni)i2[n]. That is either si= ^siorsi=s\u0003\ni. If we do not make any assumptions\non how well sis predicted by ^ s, then the problem is no easier than the classical online version. On the\nother hand, if s= ^s, then one obtains an optimal online algorithm, by simply computing the optimal\no\u000fine algorithm. The interesting regime lies in between these two cases. We will make the following\nassumption throughout the paper, which roughly speaking demands that a 1 \u0000qfraction of the input is\ncorrectly predicted and that the qfraction of errors is somewhat spread out.\nDe\fnition 1 (Number of mismatches m(\u0001)).LetIbe an interval of indices. We de\fne m(I)def=P\nt2I1st6=^stto be the number of mismatches between sand^swithin the interval I.\nAssumption 1. Consider an interval Iofsof length\"D. For anyIit holdsm(I)\u0014q\"D.\nRemark 1. Relaxing Assumption 1 by allowing the adversary to change an arbitrary qfraction of the\ninput results in the same lower bound as for the classical problem. To see this, consider an arbitrary\ninstance on qnelements that gives a lower bound of cin the classical problem. Call this sequence of\nelements adversarial . Let ^sconsists ofnelements being equal to the starting point. That is, ^sis simply\nthe starting position replicated ntimes. Let sbe equal to the sequence ^swhose su\u000ex of length qnis\nreplaced by the adversarial sequence. Now, on sde\fned in this way no algorithm can be better than\nc-competitive. Hence, in general this relaxation of Assumption 1 gives no advantage.\nOur main results hold for general metric space , where for all p;p0;p002Xall of the following hold:\nd(p;p) = 0,d(p;p0)>0 forp6=p0,d(p;p0) =d(p0;p), andd(p;p00)\u0014d(p;p0) +d(p0;p00). We obtain better\nresults for uniform metric space , where,d(p;p0) = 1 forp6=p0.\nNotation Given a sequence s, we usesito denote the i-th element of s. For integers iandj, such\nthat 1\u0014i\u0014j, we uses[i;j]to denote the subsequence of sconsisting of the elements si;:::;s j.\nFor a \fxed algorithm, let pibe the position of the page at time i. In particular, p0denotes the start\nposition for all algorithms.\nGiven an algorithm Bthat pays cost Cfor serving nrequests, we denote by Ct1;t2the cost paid by\nBduring the interval [ t1;t2]. We sometimes abuse notation and write Ctas a shorthand for C0;t. In\nparticular, CdenotesC0;nas well asCn. This notation is the most often used in the context of our\nalgorithm ALG and the optimal solution OPT , whose total serving costs are AandO, respectively.\n3 Proof Overview\nOur two main contributions are: algorithm ALG that is (1 + O(q))-competitive provided Assumption 1;\nand, a black-box reduction from ALG to aO(1=q)-competitive algorithm ALGrobustwhen Assumption 1\ndoes not hold. In Section 3.1 we present an overview of ALG , while an overview of ALGrobustis given\nin Section 3.2.\n3.1 ALG under assumption Assumption 1\nAlgorithm ALG (given as Algorithm 1) simply computes the optimal o\u000fine solution and moves pages\naccordingly.\nAlgorithm 1 ALG (i;s;^s)\nInput The number iof the next request.\nOutputsand ^sare sequences as de\fned in Section 2.\n1:Letpibe the position of the page in the optimal algorithm at the i-th request with respect to ^ s.\n2:Move the page to piand serve the request si.\nThe main challenge in proving that ALG still performs well in the online setting lies in leveraging\nthe optimality of ALG with respect to the o\u000fine sequence. The reason for this is that, due to sand\n3\n\n^snot being identical, OPT andALG may be on di\u000berent page locations throughout all the requests.\nIn addition to that, we have no control over which qfraction of any interval of length Dis changed\nnor to what it is changed. In particular, if si6= ^si, thensiand ^sicould be very far from each other.\nTo circumvent this, we use the following way to argue about the o\u000fine optimality, that is, about the\noptimality computed with respect to ^ s.\nWe think of ALG (OPT , respectively) as a sequence of page locations that are de\fned with respect\nto ^s(s, respectively). These page locations do not change even if, for instance, the i-th online request\ntoALG deviates from ^ si. LetAt(Ot, respectively) be the cost of ALG (OPT , respectively) serving t\nrequests given by s[1;t]. Similarly, let ^At(^Ot, respectively) be the cost of ALG (OPT , respectively) for\nserving the oracle subsequence ^ s[1;t]. In particular, Anis the cost of ALG (optimal on ^ s) on the \fnal\nsequences, whereas ^Onis the cost of the optimal algorithm for son the predicted sequence ^ s. It is\nconvenient to think of ^Onas the `evil twin' of An.\nWe have, due to optimality of ALG on the o\u000fine sequence,\nAn\u0000On=An\u0000^An+^An\u0000On\u0014An\u0000^An+^On\u0000On: (1)\nThe intuition behind this is best explained pictorially, which we do in Fig. 1. Here ALG is ataand\nOPT is ato. In the depicted example a request is moved from sto ^s. This causes An\u0000^Anto increase,\nhowever, at the same time, ^On\u0000Ondecreases by almost the same amount.3In fact, one can show that\nfor such a moved page the right hand side of Eq. (1) will increase by no more than 2 d(a;o). For pages\nthat are not moved, i.e., s= ^s, the costs of ALG andOPT do not change. It remains to bound d(at;ot),\nwhich we do next. By triangle inequality, it holds that\nd(at;ot)\u0014d(at;st) +d(ot;st)\u0014At\u0000At\u00001+Ot\u0000Ot\u00001; (2)\nConsider an interval ( ti\u00001;ti]. Letc(ti\u00001;ti]\nmove be the total sum of moving costs for both OPT andALG\nfor the requests in the interval ( ti\u00001;ti]. As a reminder (see De\fnition 1), for a given interval I,m(I) is\nthe number of mismatches between sand ^swithinI. From Eq. (2), we derive\nAn\u0000On\u00142X\nim((ti\u00001;ti])\u0001Ati\u0000Ati\u00001+Oti\u0000Oti\u00001\u0000c(ti\u00001;ti]\nmove\nti\u0000ti\u00001: (3)\nWe would like the right hand side of Eq. (3) to be small, implying that An\u0000Onis small as well. To\nunderstand the nature of the right hand side of Eq. (3) and what is required for it to be small, assume\nfor a moment that m((ti\u00001;ti]) =\u000b(ti\u0000ti\u00001). Then, the rest of the summation telescopes to An\u0000On,\nand Eq. (3) reduces to An\u0000On\u00142\u000b(An\u0000On). Now, if\u000bis su\u000eciently small, e.g., \u000b\u00142q, then we are\nable to upper-bound Eq. (3) by 4 q(An+On) and derive\nAn\nOn\u00141 + 4q\n1\u00004q;\nwhich gives the desired competitive factor.\nSo, to utilize Eq. (3), in our proof we will focus on showing that m((ti\u00001;ti]) is su\u000eciently smaller\nthanti\u0000ti\u00001. However, this can be challenging as OPT is allowed to move often, potentially on every\nrequest which results in ti\u0000ti\u00001being very small. But, if ti\u0000ti\u00001is too small, then Assumption 1\ngives no information about m((ti\u00001;ti]). However, if intervals ti\u0000ti\u00001would be large enough, e.g.,\nat least\fDfor some positive constant \f, then from Assumption 1 we would be able to conclude that\n\u000b=O(q). Since in principle OPT can move in every step, we design `lazy' versions of OPT andALG\nthat only move O(1) times in any interval of length D. This will enable us to argue that ti\u0000ti\u00001is not\ntoo small. It turns out that the respective competitive factors of the lazy versions with respect to the\noriginal versions is very close, allowing us prove\nAn\nOn\u0019Alazy\nn\nOlazy\nn\u0014(1 +\")1 +O(q)\n1\u0000O(q):\n3We oversimpli\fed here, since the right hand side of (1) only holds for the sum of all points, but a similar argument\ncan be made for a single requests.\n4\n\no\na\ns\n^sFigure 1: A pictorial representation of Eq. (1).\n3.2 ALGrobust, a robust version of ALG\nWe now describe ALGrobust. This algorithm follows a \\lazy\" variant of ALG as long as Assumption 1\nholds, and otherwise switches to ALGonline. Instead of using ALG directly, we use a `lazy' version of\nALG that works as follows: Follow the optimal o\u000fine solution given by ALG with a delay of 6 qDsteps.\nLetALGlazybe the corresponding algorithm. We point out that performing some delay with respect to\nALG is crucial here. To see that, consider the following example in the case of uniform metric spaces:\ns=f0gnand ^s=f1gn, and let the starting location be 0. According to ALG , the page should be moved\nfrom 0 to 1 in the very beginning, incurring the cost of D. On the other hand, OPT never moves from 0.\nIfALGrobustwould follow ALG until it realizes that the fraction of errors is too high, it would already\npay the cost of at least D, leading to an unbounded competitive ratio. However, if ALGrobustdelays\nfollowing ALG , then it gets some \\slack\" in verifying whether the predicted sequence properly predicts\nrequests or not. As a result, when Assumption 1 holds, this delay increases the overall serving cost by\na factorO(1 +O(q)), but in turn achieves a bounded competitive ratio when this assumption does not\nhold.\nWhile serving requests, ALGrobustalso maintains the execution of ALGonline, i.e.,ALGrobustmain-\ntains where ALGonlinewould be at a given point in time, in case a fallback is needed. Now ALGrobust\nsimply executes ALGlazyunless we \fnd a violation of Assumption 1 is detected. Once such a violation\nis detected, the algorithm switches to ALGonlineby moving its location to ALGonline's current location.\nFrom there on ALGonlineis executed.\nWe now present the intuition behind the proof for the competitive factor of the algorithm.\nCase when Assumption 1 holds. In this case ALGrobustisALGlazy, and the analysis boils down\nto proving competitive ratio of ALGlazy. We show that ALGlazyis (1 +O(q))-competitive to ALG ,\nwhich is, as we argued in the previous section, 1 + O(q) competitive to OPT . To see this, we employ the\nfollowing charging argument: whenever ALG moves from ptop0it paysD\u0001d(p;p0). The lazy algorithm\neventually pays the same moving cost of less.\nHowever, in addition, the serving cost of ALGlazyfor each of the 6 qDrequests is potentially increased,\nasALGlazyis not at the same location as ALG . Nevertheless, by triangle inequality, the cost due to the\nmovement from ptop0ofALG re\rect to an increase in the serving cost of ALGlazyby at most d(p;p0).\nIn total over all the 6 qDrequests and per each move of ALG fromptop0,ALGlazypays at most\n6qDd(p;p0) extra cost compared to ALG . Considering all migrations, this gives a 1 + O(q) competitive\nfactor.\nCase when Assumption 1 is violated. The case where Assumption 1 is violated (say at time t0) is\nconsiderably more involved. We then have\nALGrobust\u0014ALGlazy(0;t0) +ALGonline(t0+ 1;n) +D\u0001d(a;a0);\nand we seek to upper-bound each of these terms by O(OPT=q). While the upper-bound holds directly\nforALGonline(t0+ 1;n), showing the upper-bound for other terms is more challenging.\nThe key insight is that, due to the optimality of ALG ,\nd(a;p0)\u0014OPT (t0)=(qD); (4)\nwhich can be proven as follows. If ALG migrates its page to a location that is far from the starting\nlocationp0, then there have to be, even when taking into account noise, at least 4 qDpage requests that\n5\n\nare far from p0.OPT also has to serve these requests (either remotely or by moving), and hence has\nto pay a cost of at least qD\u0001d(a;p0). Equipped with this idea, we can now bound D\u0001d(a;a0) in terms\nofOPT (t0)=q. To bound ALGlazy(0;t0) we need one more idea. Namely, we compare ALGlazy(0;t0) to\nthe optimal solution that has a constraint to be at the same position as ALGlazyat timet0. A formal\nanalysis is given in Section 5.\n4 The Analysis of ALG\nNow we analyze ALG (Algorithm 1). As discussed in Section 3.1, our main objective is to establish\nEq. (3), which we do in Section 4.1. That upper-bound will be directly used to obtain our result for\nuniform metric spaces, as we present in Section 4.2. To construct our algorithm for general-metric spaces,\nin Section 4.3 we build on ALG by \frst designing its \\lazy\" variant. As the \fnal result, we show the\nfollowing. Recall that qis the fraction of symbols that the adversary is allowed to change in any sequence\nof length\"Dof the predicted sequence.\nTheorem 1. If Assumption 1 holds with respect to parameter \", then we obtain the following results:\n(A) There exists a (1 +\")\u0001(1 +O(q))-competitive algorithm for the online page migration problem.\n(B) There exists a (1 +O(q))-competitive algorithm for the online page migration problem in uniform\nmetric spaces.\nNote that Theorem 1 is asymptotically optimal with respect to q. Namely, any algorithm is at least\n1 + \n(q) competitive; even in the uniform metric case. To see this consider the following binary example\nwhere the algorithm starts at position 0. The advice is s= 111\u0001\u0001\u00011111|{z}\n(1\u0000q)D000\u0001\u0001\u0001000|{z}\n2qD. The \fnal sequence is\n^s=8\n><\n>:s w.p. 1=2\n111\u0001\u0001\u00011111|{z}\n(1+q)Dotherwise:\nIn the \frst case OPT simply stays at 0 since moving costs D; in the second case, OPT goes immediately\nto 1. Note that ALG can only distinguish between the sequences after (1 \u0000q)Dsteps at which point it\nis doomed to have an additional cost of qDwith probability at least 1 =2 depending on the sequence s.\n4.1 Establishing Eq. (3)\nIn our proofs we will use the following corollary of Assumption 1.\nCorollary 1. If Assumption 1 holds, then for any interval Iof length`>\"D it holdsm(I)\u00142q`.\nProof. This statement follows from the fact that each such Ican be subdivided into k\u00151 intervals of\nlength exactly \"Dand at most one interval I0of length less than \"D. On one hand, the total number\nof mismatches for these intervals of length exactly \"Dis upper-bounded by qk\"D\u0014q`. On the other\nhand, sinceI0is a subinterval of an interval of length \"D, it holdsm(I0)\u0014q\"D < q` . The claim now\nfollows.\nMost of our analysis in this section proceeds by reasoning about intervals where neither ALG nor\nOPT moves. Let t1;t2:::be the time steps at which either OPT orALG move. The \fnal product of\nthis section will be an upper-bound on An\u0000Onas given by Eq. (3)4, i.e.,\nAn\u0000On\u00142X\nim((ti\u00001;ti])\u0001Ati\u0000Ati\u00001+Oti\u0000Oti\u00001\u0000c(ti\u00001;ti]\nmove\nti\u0000ti\u00001:\nWe begin by rewriting and upper-bounding At\u0000Otas follows\nAt\u0000Ot=At\u0000^At+^At\u0000Ot\u0014At\u0000^At+^Ot\u0000Ot; (5)\n4As a reminder, At(Ot, respectively) is the cost of ALG (OPT , respectively) at time for the sequence s[1;t].\n6\n\nwhere we used that ^At\u0014^Otas^Atis the optimum for ^ s. Consider a \fxed interval I= (ti\u00001;ti]. Then,\nby triangle inequality, it holds\nd(at;ot)\u0014d(at;st) +d(ot;st)\u0014At\u0000At\u00001+Ot\u0000Ot\u00001:5(6)\nLetc(ti\u00001;ti]\nmove be the sum of moving costs for OPT andALG in (ti\u00001;ti]. Note that\nAti\u0000Ati\u00001+Oti\u0000Oti\u00001=X\nt2(ti\u00001;ti](At\u0000At\u00001+Ot\u0000Ot\u00001)\n\u0015c(ti\u00001;ti]\nmove +d(ati;oti)jti\u0000ti\u00001j; (7)\nwhere the inequality comes from Eq. (6) applied to every time step in ( ti\u00001;ti] and the fact that ALG or\nOPT must have moved inducing a cost of at least c(ti\u00001;ti]\nmove . The following notation is used to represent\nthe di\u000berence between serving siand ^sibyALG\nA[t\u00001;t] :=At\u0000^At\u0000(At\u00001\u0000^At\u00001) =d(at;st)\u0000d(at;^st):\nNote that this holds even when ALG moves since the moving costs for the oracle sequence and on the\n\fnal sequence are the same and therefore cancel each other out. Similarly to A[t\u00001;t], let\n^O[t\u00001;t] := ^Ot\u0000Ot\u0000(^Ot\u00001\u0000Ot\u00001) =d(ot;^st)\u0000d(ot;st):\nConsider now any t2[1;n]. By triangle inequality we have\nA[t\u00001;t] +^O[t\u00001;t] =d(at;st)\u0000d(ot;st) +d(ot;^st)\u0000d(at;^st)\n\u0014(d(at;ot) +d(ot;st))\u0000d(ot;st) + (d(at;^st) +d(at;ot))\u0000d(at;^st)\n= 2d(at;ot)(7)\n\u00142Ati\u0000Ati\u00001+Oti\u0000Oti\u00001\u0000c(ti\u00001;ti]\nmove\nti\u0000ti\u00001: (8)\nLet \u0001 i=Ati\u0000^Ati+^Oti\u0000Oti, where \u0001 0= 0 by de\fnition. Note that\nAn\u0000On(5)\n\u0014An\u0000^An+^On\u0000On=X\ni(\u0001i\u0000\u0001i\u00001)\n=X\niX\nt2(ti\u00001;ti]\u0010\nA[t\u00001;t] +^O[t\u00001;t]\u0011\n:\nRecall that, for a given interval Ithe function m(I) denotes the number of mismatches between sand\n^swithinI(see De\fnition 1). Now, as for tsuch thatst= ^stwe haveA[t\u00001;t] =^O[t\u00001;t] = 0, the\nlast chain of inequalities further implies\nAn\u0000OnEq:(8)\n\u0014X\niX\nt2(ti\u00001;ti]1st6=^st\u00012Ati\u0000Ati\u00001+Oti\u0000Oti\u00001\u0000c(ti\u00001;ti]\nmove\nti\u0000ti\u00001\n\u00142X\nim((ti\u00001;ti])\u0001Ati\u0000Ati\u00001+Oti\u0000Oti\u00001\u0000c(ti\u00001;ti]\nmove\nti\u0000ti\u00001: (9)\nThis establishes the desired upper-bound on An\u0000On. As discussed in Section 3.1, this upper-bound is\nused to derive our non-robust results for uniform (Section 4.2) and general (Section 4.3) metric spaces.\nThe main task in those two sections will be to show that m((ti\u00001;ti]) is su\u000eciently smaller than ti\u0000ti\u00001.\n4.2 Uniform Metric Spaces { Theorem 1 (B)\nWe now use the upper-bound on An\u0000Ongiven by Eq. (9) to show that ALG is (1 +O(q))-competitive\nunder Assumption 1, i.e., we show Theorem 1 (A). We distinguish between two cases: ti\u0000ti\u00001\u0015D;\nandti\u0000ti\u00001<D.\n7\n\nCaseti\u0000ti\u00001\u0015D.In this case, by Corollary 1 we have m((ti\u00001;ti])\u00142qjti\u0000ti\u00001j. Plugging this\ninto Eq. (9) we derive\nAn\u0000On\u00142X\nim((ti\u00001;ti])\u0001Ati\u0000Ati\u00001+Oti\u0000Oti\u00001\nti\u0000ti\u00001\n\u00144qX\ni(Ati\u0000Ati\u00001+Oti\u0000Oti\u00001)\n= 4q(An+On):\nCaseti\u0000ti\u00001<D.We proceed by upper-bounding all the terms in Eq. (9). As the interval ( ti\u00001;ti]\nis a subinterval of ( ti\u00001;ti\u00001+D], we have\nm((ti\u00001;ti\u00001+D])\u0014m((ti\u00001;ti])\u0014qD:\nAlso, observe that trivially it holds\nAti\u0000Ati\u00001+Oti\u0000Oti\u00001\u00142jti\u0000ti\u00001j+c(ti\u00001;ti]\nmove: (10)\nCombining the derived upper-bounds, we establish\nAn\u0000OnEq:(9)\n\u00142X\nim((ti\u00001;ti])\u0001Ati\u0000Ati\u00001+Oti\u0000Oti\u00001\u0000c(ti\u00001;ti]\nmove\nti\u0000ti\u00001\nEq:(10)\n\u0014 2X\niqD2(ti\u0000ti\u00001) +c(ti\u00001;ti]\nmove\u0000c(ti\u00001;ti]\nmove\nti\u0000ti\u00001(11)\n= 4qX\niD: (12)\nTo conclude this case, note that by de\fnition either ALG orOPT moves within ( ti\u00001;ti], incurring the\ncost of at least D. Therefore, Ati\u0000Ati\u00001+Oti\u0000Oti\u00001\u0015D. This together with Eq. (12) implies\nAn\u0000On\u00144qX\ni(Ati\u0000Ati\u00001+Oti\u0000Oti\u00001) = 4q(An+On):\nCombining the two cases. We have concluded that in either case it holds An\u0000On\u00144q(An+On)\nand hence we deriveAn\nOn\u00141 + 4q\n1\u00004q:\nThis concludes the analysis for uniform metric spaces.\n4.3 General Metric Spaces { Theorem 1 (A)\nAs in the uniform case, our goal for general metric spaces is to use Eq. (3) for proving the advertised\ncompetitive ratio. However, as we discussed in Section 3.1, the main challenge in applying Eq. (3) lies in\nupper-bounding the ratio between m((ti\u00001;ti]) andti\u0000ti\u00001by a small constant, ideally much smaller\nthan 1. Unfortunately, this ratio can be as large as 1 as OPT (orALG ) could possibly move on every\nsingle request. To see that, consider the scenario in which all the requests are on the x-axis and are\nrequested in their increasing order of their location. Then, for all but potentially the last Drequests,\nOPT would move from request to request. To bypass this behavior of OPT andALG , we de\fne and\nanalyze their \\lazy\" variants, i.e., variants in which OPT andALG are allowed to move only at the i-th\nrequest when iis a multiple of \"D. We now state the algorithm.\n4.3.1 Our Algorithm ALGlazy\nWe use the following algorithm ALGlazy: Compute the optimal o\u000fine solution (on ^ s) while only moving\non multiples of \"D. LetAlazybe the cost of the solution sand let\\Alazybe the cost of the solution on\n^s. Note that there can be better o\u000fine algorithms for ^ s, however ALGlazyhas the minimal cost among\nall online algorithms that are only allowed to move every multiple of \"D.\n8\n\n4.3.2 Proof\nWe also need to consider a lazy version of OPT , which we do in the following lemma. There we show\nthat making any algorithm lazy does not increase the cost by more than a factor of (1+ \"). In particular,\nwe will show Olazy\u0014(1 +\")OPT . LetAlazy\ntandOlazy\ntdenote their costs at time t.\nLemma 1. Let\"2(0;1]. Consider an arbitrary pre\fx wof lengthtof a sequence of requests. Let Bt\nbe the cost of anyalgorithm ALG Bservingw. LetBlazy\nt0be the cost of the algorithm that hasto move\nat every time step that is a multiple of \"D(and is not allowed to move at any other time step), and to\nmove to the position where ALG Bis at that time step. Then, we have\nBlazy\nt0\u0014(1 +\")Bt:\nProof. Letxibe the distance of the i-th move and yibe the cost for serving the i-th request remotely.\nThen,\nBt=DX\nixi+X\niyi:\nNow we relate BtandBlazy\nt0.Blazy\nt0has two components: the moving cost and the cost for serving\nremotely. By triangle inequality, the moving cost is upper-bounded by DP\nixi. Consider now interval\nIj2[j\"D+ 1;(j+ 1)\"D] for some integer j. To serve point i2Ijremotely, the cost is, by triangle\ninequality, at most the cost of yiplus the cost of traversing all the points with indices in Ijwhere ALG B\nhas moved to. Thus the cost per request i2Ijis upper-bounded by yi+P\nk2Ijxk. Note that the\nsummationP\nk2Ijxkis charged to \"Drequests. Hence, summing over all the intervals gives\nBlazy\nt0\u0014DX\nixi+X\niyi+\"DX\nixi\u0014(1 +\")Bt:\nDe\fneOlazyas the cost of the optimal algorithm for sthat is allowed to move only at time steps\nwhich are multiple of \"D. Similarly as in Lemma 1, we have Olazy\nn\u0014(1 +\")On. Thus,\nAlazy\nn\nOn\u0014(1 +\")Alazy\nn\nOlazyn: (13)\nNow we need to upper-boundAlazy\nn\nOlazyn. We will do that by showing that the same statements as we developed\nin Section 4.1 hold for AlazyandOlazy. To that end, observe that to derive Eq. (5) we used the fact\nthat ^A\u0014^O. Notice that the analog inequality \\Alazy\u0014\\Olazyholds, since ALGlazyis the the optimal\no\u000fine algorithm that only moves every multiple of \"D.\nHence, we can obtain the derivation Eq. (9) for Alazy\nn\u0000Olazy\nn\nAlazy\nn\u0000Olazy\nn\u00142X\nim((ti\u00001;ti])\u0001Alazy\nti\u0000Alazy\nti\u00001+Olazy\nti\u0000Olazy\nti\u00001\nti\u0000ti\u00001: (14)\nSince for the lazy versions we have jti\u0000ti\u00001j=\"D, Assumption 1 implies m((ti\u00001;ti])\u0014\"qD. Plugging\nthis into Eq. (14) gives\nAlazy\nn\u0000Olazy\nn\u00142qX\ni\u0010\nAlazy\nti\u0000Alazy\nti\u00001+Olazy\nti\u0000Olazy\nti\u00001\u0011\n= 2q(Alazy\nn+Olazy\nn):\nFrom Eq. (13) we establish\nAlazy\nn\nOn\u0014(1 +\")Alazy\nn\nOlazyn\u0014(1 +\")1 + 2q\n1\u00002q:\nThis concludes the proof of Theorem 1 (A).\n9\n\n5 Robust Page Migration\nSo far we designed algorithms for the online page migration problem that have small competitive ratio\nwhen Assumption 1 holds. In this section we build on those algorithm and design a (robust) algorithm\nthat performs well even when Assumption 1 does not hold, while still retaining competitiveness when\nAssumption 1 is true. We refer to this algorithm by ALGrobust. For ALGrobustwe prove the following.\nTheorem 2. Let\rbe the competitive ratio of ALG for the online page migration problem, and let qbe\na positive number less than 1=24. If Assumption 1 holds, then ALGrobustis\r\u0001(1 +O(q))-competitive,\nand otherwise ALGrobustisO(1=q)-competitive.\nUsing our techniques it is straight-forward to obtain an arbitrary trade-o\u000b between the two com-\npetitive ratios. Fix an arbitrary x\u00151, then Algorithm ALGrobustis (1 +O(x\u0001q))-competitive if\nAssumption 1 holds and O(1=(x\u0001q))-competitive otherwise.\n5.1 Algorithm ALGrobust\nLetALGonlinerefer to an arbitrary online algorithm for the problem, e.g., [Wes94]. We now de\fne\nALGrobust. This algorithm switches from ALG toALGonlinewhen it detects that Assumption 1 does\nnot hold. Instead of using ALG directly, we use a \\lazy\" version of ALG that works as follows. Follow\nthe optimal o\u000fine solution given by ALG with a delay of 6 qDsteps. Let ALGlazybe the corresponding\nalgorithm. (A lazy version for di\u000berent setup of parameters was presented in Section 4.3.)\nThroughout its execution, ALGrobustmaintains/tracks in its memory the execution of ALGonlineon\nthe pre\fx of sseen so far. That is, ALGrobustmaintains where ALGonlinewould be at a given point in\ntime in case a fallback is needed. Now ALGrobustsimply executes ALGlazyunless we \fnd a violation\nof Assumption 1 is detected. Once such a violation is detected, the algorithm switches to ALGonlineby\nmoving its location to ALGonline's current location. From there on ALGonlineis executed.\nWe now analyze ALGrobustand show that in case Assumption 1 holds, then ALG andALGrobust\nare close in terms of total cost, and otherwise the cost of ALGrobustis at mostO(1=q) larger than that\nofALGonline.\nCase 1: Assumption 1 holds for the entire sequence. In this case ALGrobustexecutes ALGlazy\nthroughout. Following the same argument for \"= 6qas given for Alazy0in the proof of Lemma 1, we\nhave\nAlazy\nt\u0014(1 + 6q)At: (15)\nThus,\nArobust=Alazy\nn\u0014(1 + 6q)An\u0014\r(1 +O(q))O;\nwhere we used the assumption that ALG is\r-competitive. This completes this case.\nCase 2: Assumption 1 is violated at the t-th request. Lett0=t\u0000qD+ 1. Note that up to this\npoint in time no violation occurred. We de\fne the following: ais the position of ALGlazyat timet0;a0\nis the position of ALGonlineat timet0+ 1;ois the position of OPT at timet0; and,Op\n0;t00is the cost of\nOPT up to time t00where we demand that OPT is at position patt00.\nIn the following, we assume the following holds. We defer the proof of its correctness for later.\nd(a;p0)\u0014Ot0=(qD): (16)\nIntuitively, this means that we can bound the distance from the starting position by the cost of OPT .\nUsing Eq. (16), we get,\nArobust\u0014Alazy\nt0+Aonline\nt0+1;n+D\u0001d(a;a0): (17)\nAsOa\n0;t0andAlazy\n0;t0are at the same position at time t0, inequality Alazy\n0;t0\u0014(1 +c1q)Oa\n0;t0follows from\nEq. (15) for a suitable constant c1. Note that Ot0\u0015D\u0001d(p0;o), which holds since this cost is already\nincurred by moving to o, where we used triangle inequality.\n10\n\nNext, using triangle inequality again, we get\nAlazy\n0;t0\n\u0014(1 +c1q)Oa\n0;t0\n\u0014(1 +c1q)\u0000\nOo\n0;t0+D\u0001d(a;o)\u0001\n\u0014(1 +c1q)\u0000\nOo\n0;t0+D\u0001d(a;p0) +D\u0001d(p0;o)\u0001\n\u0014(1 +c1q)\u0000\nOo\n0;t0+Ot0=q+Ot0\u0001\n=O(Ot0=q): (18)\nFurthermore, using Eq. (16), triangle inequality and a simple lower bound on Alazy\n0;t0as well as Eq. (18),\nwe get,\nD\u0001d(a;a0)\u0014D\u0001d(a;p0) +D\u0001d(p0;a0)\n\u0014Ot0=q+Aonline\n0;t0\n\u00142Ot0=q: (19)\nThus, plugging Eq. (19) and Eq. (18) into Eq. (17) and using Aonline\u0014O(On), we get\nArobust\u0014Alazy\nt0+Aonline\nt0+1;n+D\u0001d(a;a0)\n=O(Ot0=q) +O(On) + 2Ot0=q\n=O(On=q):\nThus, it only remains to prove Eq. (16), as we do using the following lemma. That lemma shows\nthat if ALG moves its page to a location that is far from p0, then this means that there must be pages\nthat are far from p0. Later we will show that OPT pays considerable cost to serve them, even if done\nremotely. See Fig. 2 for an illustration of the lemma.\nLemma 2. LetP=p1;p2;::: be the sequence of page locations that ALG produces. Let pmax be the\nfurthest point with respect to p0a page is moved to by the ALG , i.e.,\npmaxdef= arg max\npid(pi;p0):\nIn case that there are several pages at pmax, we letpmax be the \frst among them. Let dmaxdef=d(pmax;p0).\nLetPbe the maximal consecutive sequence of Pincludingpmax consisting of pages that are each at\ndistance at least rdef=dmax=4fromp0. Then, for q <1=24, it holds that the page locations in Pserve\ntogether at least 6qDpoints at distance rfromp0in the oracle sequence.\nProof. The proof proceeds by contradiction. Suppose that Pserves fewer than 6 qDpoints in the oracle\nsequence. We will show that a better solution consists of replacing the sequence Pby simply moving to\np0and serving all points remotely from there. Since Pis a maximal sequence of Pincludingpmaxsuch\nthat each page location is at distance rfromp0,ALG moves by at least dmax\u0000rwithinP. Hence, the\ncost of ALG using the page locations Pis at least\nD(dmax\u0000r) +X\ndi; (20)\nwhere thePdirepresents the distances to pages served remotely from the page locations in P(depicted\nas solid lines connected to p;pmaxandp0in Fig. 2). Consider a request sthat is served from location\npin the original (using P) solution. In the new solution, where all points are served from p0, serving\nany request has, by triangle inequality, a cost of at most d(p0;p) +d(p;s)\u0014dmax+d(p;s). Moreover,\nobserve that the sequence Pconsists of at most 6 qDlocations. This is because otherwise there would\nbe a location that does not serve any points. Putting everything together, the cost of the new solution\nis at most\n2Dr+ 6qDd max+X\ndi; (21)\nwhere the 2 Draccounts for moving the page from the location preceding Ptop0(the cost of at most\nDr) and moving the page from p0to the location just after P(also the cost of at most Dr). Recall that\nr=dmax=4. Thus, Eq. (21) is cheaper than the solution Eq. (20) for qsmall enough (i.e., for q<1=24),\nwhich contradicts the optimality of ALG of the oracle sequence.\n11\n\nBy Lemma 2, we conclude that there are at least 6 qDpoints at distance rfromp0in the oracle\nsequence. Note that the \fnal sequence swill contain at least 6 qD\u00002qDof these points, due to our\nassumption on noise and the fact that up to the \frst violation of Assumption 1 were detected as time t.\nOPT has to serve these points as well and thus\nOt0\u0015(6qD\u00002qD)r= 4qD\u0001dmax=4\u0015qD\u0001d(a;p0);\nwhich yields Eq. (16) and therefore completes the proof.\np0\npmax\np0\np00\ndmax\nr\nFigure 2: An illustration of Lemma 2, where we argue that the reason we moved a page to a location\nfar away (at distance dmax) fromp0means that there must be many points that are at least at distance\nr=dmax=4 fromp0.OPT will have to serve most of these points as well. The squares denote location\nof pages, the small circles denote page requests, the solid lines between squares and small circles depict\na remotely served request. The dashed lines denote the movement of the page. The sequence Pconsists\nofp0;pmaxandp00.\n6 Experiments\nWe evaluate our approach on two synthetic data sets, and compare it to the state of the art algorithm\nfor page migration due to Westbrook [Wes94]. The two data sets are obtained by generating \\predicted\"\nsequences of points in the plane, and then perturbing each point by independent Gaussian noise to obtain\n\\actual\" sequences. The predicted sequence is fed to our algorithm, while the actual sequence forms an\ninput of the online algorithm. Recall that our algorithm sees the actual sequence only in the online\nfashion.\nData sets The predicted sequences of the two sets of points are generated as follows:\n1.Line process: thet-th point ( ^X1(t);^X2(t)) is equal to ( t;0).\n2.Brownian motion process: thet-th point ^X(t) is equal to ^X(t\u00001)+(\u0001 1(t);\u00012(t)), where \u0001 t(t)\nand \u0001 2(t) are i.i.d. random variables chosen from N(0;1).\nNote that the predicted line process is completely deterministic whereas the Brownian motion points\nhas, by de\fnition, Gaussian noise. In both cases, the actual sequence is generated by adding (additional)\nGaussian noise to the predicted sequence: the t-th request X(t) in the actual sequence is equal to\n^X(t) + (N1(t);N2(t)), whereN1(t);N2(t) are i.i.d. random variables chosen from N(0;\u001b2). The value of\n\u001bvaries, depending on the speci\fc experiment. An example Brownian motion sequence is depicted in\nFig. 3.\n12\n\nFigure 3: An example of Brownian motion sequence. The predicted sequence is in blue, the actual\nsequence is in red.\nSet up We use the two data sets to compare the following three algorithms:\n\u000fPredict refers to our algorithm, which computes the optimum solution for the predicted sequence\n(by using standard dynamic programming) and follows that optimum to serve actual requests.\n\u000fOpt is the optimum o\u000fine algorithm executed on the actual sequence. This optimum is computed\nby using the same dynamic programming as in the implementation of Predict .\n\u000fOnline is state-of-the-art online randomized algorithm for page migration that achieves 2 :62-\napproximation in expectation. This algorithm is described in Section 4.1 of [Wes94]. Since it is\nrandomized, on each input we perform 100 runs of Online and as the output report the average\nof all the runs. The standard deviation is smaller than 5%.\nFor both data sets, we depict the costs of the three algorithms as a function of either Dor\u001b. See\nthe text above each plots for the speci\fcation.\nResults The results for the Brownian motion data set are depicted in Fig. 4. The top two \fgures show\nthe cost incurred by each algorithm for \fxed values of \u001band di\u000berent values of D, while the bottom two\n\fgures show the costs for \fxed values of Dwhile\u001bvaries. Not surprisingly, for low values of \u001b, the costs\nPredict andOpt are almost equal, since the predicted and the actual sequences are very close to each\nother. As the value of \u001bincreases, their costs starts to diverge. Nevertheless, the bene\ft of predictions\nis clear, as the cost of Predict is signi\fcantly lower than the cost of Online . Interestingly, this holds\neven though the fraction of requests predicted exactly is very close to 0.\nThe results for the Line data set is depicted in Fig. 5. They are qualitatively similar to those for\nBrownian motion.\n13\n\n(a) Fixed sigma, varying D.\n (b) Fixed sigma, varying D.\n(c) Fixed D= 2, varying sigma.\n (d) Fixed D= 5, varying sigma.\nFigure 4: Comparison between Predict ,Opt andOnline on Brownian motion data set.\n(a) Fixed sigma, varying D.\n (b) Fixed sigma, varying D.\n(c) Fixed D= 2, varying sigma.\n (d) Fixed D= 5, varying sigma.\nFigure 5: Comparison between Predict ,Opt andOnline on Line data set.\n14\n\nReferences\n[ABF93] Baruch Awerbuch, Yair Bartal, and Amos Fiat. Competitive distributed \fle allocation. In\nSTOC , volume 93, pages 164{173, 1993.\n[ABF03] Baruch Awerbuch, Yair Bartal, and Amos Fiat. Competitive distributed \fle allocation.\nInformation and Computation , 185(1):1{40, 2003.\n[BBM17] Marcin Bienkowski, Jaroslaw Byrka, and Marcin Mucha. Dynamic beats \fxed: On phase-\nbased algorithms for \fle migration. ICALP , 2017.\n[BCI97] Yair Bartal, Moses Charikar, and Piotr Indyk. On page migration and other relaxed task\nsystems. SODA , 1997.\n[BDSV18] Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to\nbranch. In International Conference on Machine Learning , pages 353{362, 2018.\n[BFK+17] Joan Boyar, Lene M Favrholdt, Christian Kudahl, Kim S Larsen, and Jesper W Mikkelsen.\nOnline algorithms with advice: A survey. ACM Computing Surveys (CSUR) , 50(2):19, 2017.\n[BFR95] Yair Bartal, Amos Fiat, and Yuval Rabani. Competitive algorithms for distributed data\nmanagement. Journal of Computer and System Sciences , 51(3):341{358, 1995.\n[Bie12] Marcin Bienkowski. Migrating and replicating data in networks. Computer Science-Research\nand Development , 27(3):169{179, 2012.\n[BJPD17] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using\ngenerative models. In International Conference on Machine Learning , pages 537{546, 2017.\n[BS89] David L Black and Daniel D Sleator. Competitive algorithms for replication and migration\nproblems . Carnegie-Mellon University. Department of Computer Science, 1989.\n[CLRW97] Marek Chrobak, Lawrence L Larmore, Nick Reingold, and Je\u000bery Westbrook. Page migration\nalgorithms using work functions. Journal of Algorithms , 24(1):124{157, 1997.\n[GP19a] Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert\nadvice. In Proceedings of the 36th International Conference on Machine Learning , pages\n2319{2327, 2019.\n[GP19b] Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert\nadvice. In International Conference on Machine Learning , pages 2319{2327, 2019.\n[HIKV19] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency esti-\nmation algorithms. In International Conference on Learning Representations , 2019.\n[KBC+18] Tim Kraska, Alex Beutel, Ed H Chi, Je\u000brey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of\nData , pages 489{504, 2018.\n[KDZ+17] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial\noptimization algorithms over graphs. In Advances in Neural Information Processing Systems ,\npages 6348{6358, 2017.\n[KM16] Amanj Khorramian and Akira Matsubayashi. Uniform page migration problem in euclidean\nspace. Algorithms , 9(3):57, 2016.\n[KPS+19] Ravi Kumar, Manish Purohit, Aaron Schild, Zoya Svitkina, and Erik Vee. Semi-online\nbipartite matching. ITCS , 2019.\n[LLMV20] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online\nscheduling via learned weights. In Proceedings of the Fourteenth Annual ACM-SIAM Sym-\nposium on Discrete Algorithms , pages 1859{1877. SIAM, 2020.\n15\n\n[LRWY98] Carsten Lund, Nick Reingold, Je\u000bery Westbrook, and Dicky Yan. Competitive on-line al-\ngorithms for distributed data management. SIAM Journal on Computing , 28(3):1086{1111,\n1998.\n[LV18] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice.\nInInternational Conference on Machine Learning , pages 3302{3311, 2018.\n[Mat15] Akira Matsubayashi. A 3+ omega (1) lower bound for page migration. In 2015 Third\nInternational Symposium on Computing and Networking (CANDAR) , pages 314{320. IEEE,\n2015.\n[Mit18] Michael Mitzenmacher. A model for learned bloom \flters and optimizing by sandwiching.\nInAdvances in Neural Information Processing Systems , pages 464{473, 2018.\n[MMS90] Mark S Manasse, Lyle A McGeoch, and Daniel D Sleator. Competitive algorithms for server\nproblems. Journal of Algorithms , 11(2):208{230, 1990.\n[MPB15] Ali Mousavi, Ankit B Patel, and Richard G Baraniuk. A deep learning approach to structured\nsignal recovery. In Communication, Control, and Computing (Allerton), 2015 53rd Annual\nAllerton Conference on , pages 1336{1343. IEEE, 2015.\n[PSK18] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml pre-\ndictions. In Advances in Neural Information Processing Systems , pages 9661{9670, 2018.\n[Roh20] Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In\nProceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms , pages\n1834{1845. SIAM, 2020.\n[Unc16] Special Semester on Algorithms and Uncertainty , 2016. https://simons.berkeley.edu/\nprograms/uncertainty2016 .\n[Wes94] Je\u000bery Westbrook. Randomized algorithms for multiprocessor page migration. SIAM Journal\non Computing , 23(5):951{965, 1994.\n[WLKC16] Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big\ndata - a survey. Proceedings of the IEEE , 104(1):34{57, 2016.\n16",
  "textLength": 45742
}