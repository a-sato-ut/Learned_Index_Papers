{
  "paperId": "dce2ec2c9e0f54570640bda9a77aa36c509b0553",
  "title": "Learning-Aided Heuristics Design for Storage System",
  "pdfPath": "dce2ec2c9e0f54570640bda9a77aa36c509b0553.pdf",
  "text": "Learning-Aided Heuristics Design for Storage System\nYingtian Tang1,3, Han Lu1,2, Xijun Li1,4, Lei Chen1, Mingxuan Yuan1and Jia Zeng1âˆ—\n1 Huawei Noahâ€™s Ark Lab\n2 Shanghai Jiao Tong University\n3 University of Pennsylvania\n4 University of Science and Technology of China\nABSTRACT\nComputer systems such as storage systems normally require trans-\nparent white-box algorithms that are interpretable for human ex-\nperts. In this work, we propose a learning-aided heuristic design\nmethod, which automatically generates human-readable strategies\nfrom Deep Reinforcement Learning (DRL) agents. This method ben-\nefits from the power of deep learning but avoids the shortcoming\nof its black-box property. Besides the white-box advantage, experi-\nments in our storage productionâ€™s resource allocation scenario also\nshow that this solution outperforms the systemâ€™s default settings\nand the elaborately handcrafted strategy by human experts.\nCCS CONCEPTS\nâ€¢Computing methodologies â†’Planning with abstraction\nand generalization ;Rule learning ;â€¢Computer systems or-\nganizationâ†’Real-time operating systems .\nKEYWORDS\nReal-time operating systems; Rule learning; Reinforcement learning\nACM Reference Format:\nYingtian Tang1,3, Han Lu1,2, Xijun Li1,4, Lei Chen1, Mingxuan Yuan1and\nJia Zeng1. 2021. Learning-Aided Heuristics Design for Storage System. In\nProceedings of the 2021 International Conference on Management of Data\n(SIGMOD â€™21), June 20â€“25, 2021, Virtual Event, China. ACM, New York, NY,\nUSA, 5 pages. https://doi.org/10.1145/3448016.3457554\n1 INTRODUCTION\nThe tuning process of computer systems is laborious and expen-\nsive; but of great significance in terms of performance. Effective\nand explainable heuristics have been widely adopted for tuning in\nexisting computer systems. For example, conventional heuristics\nsuch as FIFO (First In First Out) and LRU (Least Recently Used) are\nwidely used in cache scenario. Nevertheless, these strategies are\ncustomized and require sophisticated handcraft design. Recently,\na promising field of using machine learning to optimize computer\nâˆ—Han Lu and Yingtian Tang contributed equally to this research. This work was done\nwhen Han and Yingtian were interns in Huawei Noahâ€™s Ark Lab. Xijun Li is the\ncorresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGMOD â€™21, June 20â€“25, 2021, Virtual Event, China\nÂ©2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8343-1/21/06. . . $15.00\nhttps://doi.org/10.1145/3448016.3457554systems is drawing increasing attentions [ 4]. For example, Mao et.\nal.[5] apply deep reinforcement learning (DRL) on resource man-\nagement problems. Kraska et. al. [3] attempt to replace traditional\nindex in computer system (such as B-Tree and BitMap) by learning\nthe mapping relation. However, the black-box property hinders the\ndeployment of them in real-world systems, where thorough sanity\nchecks and complete traceability are required. There have been\nattempts to interpret the deep learning models, for instance, Koul\net. al. [2] propose a method to extract semantic strategies from a\ntrained recurrent neural network and uses that to play Atari games.\nHowever, how these methods could be used for practical computer\nsystem tuning still remains underexplored.\nIn this paper, we consider a real-time resource allocation problem\nin the storage system Dorado V6 [ 8]. The computation resources\n(i.e., CPU cores) are expected to be appropriately allocated into\ndifferent levels when workloads dynamically change, so as to finish\na given amount of IO requests with the minimum processing time.\nConsidering limited computation resource for tuning and the sys-\ntem performance safety issue, a lightweight white-box approach\nis required. To take advantage of the power of deep reinforcement\nlearning, we propose a learning-aided heuristics design method\nwhich can extract an explainable finite state control strategy.\nOur contributions are summarized as follows:\nâ€¢We present an integrated pipeline of learning heuristics from\nDRL policies, including heuristics extraction, generalization\ncapability enhancement, and interpretation, which aims to\nfacilitate domain experts to devise more sophisticated heuris-\ntics for computer systems.\nâ€¢We apply the above methods on a real-time resource alloca-\ntion scenario in our storage product. Experimental results\nshow that both the DRL model and corresponding extracted\nheuristics outperform the default production setting and the\nelaborately handcrafted strategy by human experts.\n2 PROBLEM DESCRIPTION\nAs shown in Figure 1, Dorado V6 storage system has a multi-level\ncomputation resource architecture. There exist three levels where\nCPU cores can reside, i.e., NORMAL, KV and RV. Specifically, KV\nlevel stands for Key-Value storage level, in which CPU cores are\nutilized to calculate the key-value mapping relation. RV level stands\nfor Resource Volume level, in which CPU cores are used in virtual-\nization management of disk resources. The CPU cores in different\nlevels perform different duties. The cores in NORMAL level load\ndata from a shared cache. The cache miss occurs when the NOR-\nMAL level cache does not contain the requested data. In this case,\nthe CPU cores in KV and RV levels fetch data from the disk and load\nit into the cache of NORMAL level. The cores in NORMAL level can\nthen read the loaded data from cache to meet the IO request. TherearXiv:2106.07288v1  [cs.AI]  14 Jun 2021\n\nFigure 1: CPU core allocation problem in our storage system\nare various kinds of IO requests, different in the size and type of\nread/write. Note that read IO requests might be finished only with\nCPU cores of NORMAL level, while for write IO requests, cores of\nall three levels must be used. For any time interval, workloads that\ncomprise different kinds of IO requests are concurrently sent to\nthe storage system. Our goal is to migrate CPU cores among the\nthree levels according to the dynamic workload distributions, to\nfinish the workload with the minimum number of time intervals.\nThe formal definitions are given as follows.\nDefinition 1 (Workload). Workloadğ‘¤(ğ‘¡)within one time in-\ntervalğ‘¡can be described with two 14-dimension vectors and a scalar:\nğ‘†ğ‘¤(ğ‘¡)=[ğ‘†ğ‘–], ğ‘–=1,2,...,14 (1)\nğ¼ğ‘¤(ğ‘¡)=[ğ¼ğ‘–], ğ‘–=1,2,...,14 (2)\nğ‘„ğ‘¤(ğ‘¡) (3)\nwhereğ‘†ğ‘–andğ¼ğ‘–together describe the ğ‘–th type of IO request. ğ‘†ğ‘–denotes\nthe IO size and type (read/write). ğ¼ğ‘–denotes the ratio of ğ‘†ğ‘–inğ‘¤. There-\nfore,Ã14\nğ‘–=1ğ¼ğ‘–=1.ğ‘„ğ‘¤(ğ‘¡)represents the total number of IO requests in\nworkloadğ‘¤(ğ‘¡)sent to storage system within time interval ğ‘¡.\nDefinition 2 (Maximum Processing Capability). Each CPU\ncore has its own maximum processing capability per time interval,\ndenoted asğ‘š, which is the maximum sum of IO request sizes (not\nthe number of IO requests) that could be processed. Suppose that the\ntotal number of cores in the storage system is ğ‘, the ideal maximum\nprocessing capability within one time interval is ğ‘Ã—ğ‘š.\nDefinition 3 (Cache Miss Rate). CPU cores of NORMAL level\nmay fail to read data from their own cache, which is referred to as\ncache miss. The probability of cache miss is denoted as ğ¶.\nBesides the above definitions, the storage system also has the\nfollowing properties: 1) IO requests of workload ğ‘¤are assigned to\ncores in a polling manner; 2) Each IO request must be processed\nand cannot be discarded. If an IO request sent in a time interval ğ‘¡\ncannot be processed ( e.g.,exceeds the total processing capability\nof all cores within time interval ğ‘¡), then it will be postponed to the\nfollowing time intervals; 3) Each core is able to migrate between\nNORMAL, KV, and RV levels, as shown in Figure 1. A core must\nfinish all the IO requests assigned to it before migration. A certain\npercentage of performance loss in the next time interval would be\ncaused by the migration of a core.\nObjective: For a sequence of workloads ğ‘¤(ğ‘¡|ğ‘¡=1,2,..ğ‘‡), the\nmakespanğ¾(ğ¾â‰¥ğ‘‡) is the number of time intervals to finish\nall IO requests. Our objective is to design dynamic CPU migration\npolicies among different levels to minimize the makespan ğ¾.\nFigure 2: The detailed process of our proposed method to\nautomatically extract heuristics using RNN-based reinforce-\nment learning, where a QBN [2] technique is adopted in the\nfirst three steps.\n3 HEURISTICS LEARNED FROM DRL POLICY\nIn this section, we present an integrate pipeline that learns heuris-\ntics from DRL policies. The overall process is shown in Figure 2. We\nfirst construct a recurrent DRL model that consists of a value net-\nwork and a policy network, and train it in the environment. Then\nwe insert two quantization auto-encoders and retrain the model.\nThe auto-encoders are for the observed data and the hidden states,\nrespectively. Next, we extract a Finite State Machine (FSM) from the\nembedding of the auto-encoders. Finally, we summarize semantics\nof the FSM states via matching corresponding observations and\nanalyzing them statistically.\n3.1 RNN-based Reinforcement Learning\nWe model the problem as a Markov Decision Process (MDP), then\nutilize a Recurrent Neural Network (RNN) based DRL model to\nsolve the MDP. It is believed that a latent context relation exists in\nthe transitions of workloads. The RNN in our DRL model is devised\nto cope with this latent relationship.\nObservation: ğ‘œğ‘¡represents the observation at the time interval ğ‘¡.\nğ‘œğ‘¡=[ğ‘ğ‘(ğ‘¡),ğ‘ğ¾(ğ‘¡),ğ‘ğ‘…(ğ‘¡),ğ‘¢ğ‘(ğ‘¡),ğ‘¢ğ¾(ğ‘¡),ğ‘¢ğ‘…(ğ‘¡),ğ‘¤(ğ‘¡),ğ‘„ğ‘¤(ğ‘¡)], where\nğ‘ğ‘(ğ‘¡),ğ‘ğ¾(ğ‘¡)andğ‘ğ‘…(ğ‘¡)respectively denote the number of cores in\nNORMAL, KV, RV levels and ğ‘¢ğ‘(ğ‘¡),ğ‘¢ğ¾(ğ‘¡)andğ‘¢ğ‘…(ğ‘¡)respectively\nrepresent the average CPU utilization rate of the three levels. The\nobservation space is denoted as O={ğ‘œğ‘¡}.\nHidden State: â„ğ‘¡denotes the hidden state at time interval ğ‘¡, which\nis updated on each transition and affects the following action se-\nlection. In particular, â„ğ‘¡=ğœ™(â„ğ‘¡âˆ’1,ğ‘œğ‘¡), whereğœ™is the transition\nfunction maintained in the recurrent network. The hidden state\nspace is denoted by H. Hence the transition function ğœ™is a mapping:\nHÃ—Oâ†¦â†’H .\nAction: For each hidden state â„ğ‘¡, an actionğ‘ğ‘¡is chosen from the\npolicy network ğœ‹,i.e.,ğ‘ğ‘¡=ğœ‹(â„ğ‘¡). The action space is denoted as\nA={ğ‘ğ‘–|ğ‘–=1,...,7}, where there are seven distinct actions in total.\nNote that action ğ‘1represents no CPU core migration between\ndifferent levels. The rest of actions respectively denote migrating\n\none CPU core from one level to another level ( e.g., migrating one\ncore from NORMAL to KV).\nReward: The reward is measured as 1/ğ¾, which is the inverse of\nthe makespan.\n3.2 Extract Heuristics from Learned Policy\n3.2.1 Finite State Machine Extraction. As Step 2 and 3 illustrate\nin Figure 2, once the DRL model converges in training, we extract\na FSM from it with Quantized Bottleneck Network (QBN) tech-\nnique [ 2]. The QBNs are auto-encoders that reconstruct continuous\nembedded observations ğ‘œğ‘¡and hidden states â„ğ‘¡asğ‘œâ€²\nğ‘¡andâ„â€²\nğ‘¡. The\nentries of their latent embeddings bğ‘œğ‘¡andbâ„ğ‘¡(whose dimension is\ndenoted as ğ¿) are restricted to be ğ‘˜-bit quantized. There are ğ‘˜ğ¿\ndistinct embeddings that span the discrete embedding space bOand\nbH. A dataset of <â„ğ‘¡,â„ğ‘¡+1,ğ‘œğ‘¡,ğ‘ğ‘¡>can be collected via running the\ntrained DRL model. The QBNs are then trained over the collected\ndataset using supervised learning to minimize the reconstruction er-\nror. In this way, a discrete dataset of <bâ„ğ‘¡,bâ„ğ‘¡+1,bğ‘œğ‘¡,ğ‘ğ‘¡>is obtained,\nwhich produces a transition table, i.e.,the extracted FSM.\n3.2.2 Generalization Capability Enhancement. Unlike the classical\nvideo game scenarios [ 2] where the DRL and extracted FSM could\nsee all possible observations, in our scenario we cannot observe\nall possible kinds of workloads. We thus propose two methods to\nenhance the generalization capability of the extracted FSM.\nThe first one is curriculum learning . In practice, we cannot oper-\nate the storage system once it has been sold to our customers. Thus\nwe are not able to obtain large amounts of real workload traces\nfrom the users unless we are granted with the permissions. As the\nresult, only a few real workload traces are available to us. However,\nwe can collect summarized characteristics of real workload traces,\nsuch as periods, trends and dominant IO types, via customer inves-\ntigation (a common business mode). With these characteristics, we\nconstruct several standard workload traces using Vdbench [ 7]. We\nregard standard workload traces as easy tasks . A policyğœ‹ğœƒis first\ntrained on multiple easy tasks until it converges. The real workload\ntraces are regarded as hard tasks , the number of which are very few\ncompared to the number of easy tasks. With knowledge learned in\neasy tasks, we continue to train policy ğœ‹ğœƒon a few hard tasks to\nget the final policy. We experimentally validate that the proposed\nmethod improves the generalization capability in Section 4.\nThe second one is to classify an unseen observation as its closest\nknown observation. The intuition behind is that the state space has\na certain continuity and similar observations could trigger similar\nactions. Specifically, we define the â€œclosenessâ€ of two observations\nas the similarity between their observation vectors. The similarity\nmeasures such as Euclidean distance and cosine similarity can be\napplied. The unseen observation can therefore trigger a transition\nin the extracted FSM.\n3.3 Interpretation of Extracted States\nA FSM for real workload could be extracted from the trained DRL\nusing the method mentioned above. We interpret the extracted\nstates in two ways, so that the strategies of DRL could be unfolded\nfor inspiring further heuristics design.\nWe firstly examine the transitions into and from each state. Ev-\nery state in the extracted FSM is associated with many observations\nwhich are divided into two classes, Fan-in and Fan-out, as shown inFigure 2 (observations that correspond to transitions between the\nsame state should be ruled out). Moreover, each state corresponds\nto one unique action. It is the action emitted by the state that causes\nthe variation between Fan-in and Fan-out observations. Here the\noriginal continuous observations are used instead of the quantized\ncounterparts obtained from auto-encoders. For each state, we com-\npare the average Fan-in and Fan-out observations, and infer how\nthe state reacts to environment and the intensity of that reaction.\nSecondly, we examine the history of observations before the\ntransition into a state. For each presence of a specific state, we\ncollect a time window of observations happened before it. We then\ntake the average of these time windows, which represents the gen-\neral history information of that state. The states are extracted from\nthe RNN-based DRL, so that the history that we obtained could be\nuseful for explaining what causes the transition into a state and\nwhat information is encoded by the state.\n4 EXPERIMENT\n4.1 Settings\n12classes of standard workload traces are synthesized using the\nVdbench tool, each of which is associated with one typical business\nmodel of the users, such as database, heavy computing, etc. Recall\nthat we only have very few real workload traces from the users.\nWe simulate real workload traces by sampling snippets from the\naforementioned standard workloads. In this way, we generate 50\nworkload traces. To sample more efficiently for RL, we write a\nsimulator to simulate the CPU core migration in Dorado V6 storage\nsystem. In addition to the characteristics of storage system described\nin Section 2, we also consider the idle rate of CPU cores which\nfollows a Poisson distribution in the simulator.\n4.2 Training procedures\nWe use a Gated Recurrent Unit (GRU) with 128hidden nodes to\nincorporate the recurrent architecture. We forward its hidden state\nto two linear layers, with output sizes of 7and 1respectively, to\nproduce the logits corresponding to all possible actions and the\npredicted state value. The loss design follows the Advantage Actor-\nCritic method (A2C) [ 6]. We use Adam [ 1] optimizer with an initial\nlearning rate 0.0003 and clip the norm of gradients to be under 2.\nThe RL learning follows the Epsilon greedy exploration with 0.1as\nthe probability of random action selection. We adopt the method\nin [2] to extract a finite state machine from the trained DRL model.\nFor the parameters of QBNs, we set ğ‘˜=3andğ¿=64.\n4.3 Experimental results\n4.3.1 Convergence. The proposed curriculum learning for storage\nsystem is validated here. One RL agent is trained for 2000 epochs\nin total based on our curriculum learning strategy ( 1000 epochs for\nstandard workload traces and 1000 epochs for real workload traces).\nWe train another agent on real workload traces for 2000 epochs\nfor comparison. The result of convergence comparison is shown\nin Figure 3. The blue curve represents the convergence process of\ntraining only on real workloads whilst the yellow and brown curves\ntogether show the convergence process of curriculum learning. The\nhorizontal and vertical axes denote the epoch number and total\nmakespan respectively. It can be seen that the RL agent with cur-\nriculum learning converges faster and better than the one learned\nfrom scratch. Besides, it is worth noting that the computation power\n\n025050075010001250150017502000\nEpochs1.21.41.61.82.02.22.42.6Total Makespan1e4\nPre-trained: standard workload  \nPre-trained: real workload \nFrom scratchFigure 3: Convergence comparison\n123 8910 4 7 5 6 \nReal Workload \nInstance100150200250300350400MakespanDefault \nHandcrafted FSM \nGRU Extracted \nFSM\nFigure 4: Performance comparison\nconsumption of training on standard workload traces is relatively\nlower than that on real workload traces. Thus, with the introduction\nof curriculum learning, we can obtain a better RL policy using lower\ncomputation power and less number of real workloads, which is\nvital to algorithm deployment in the production environment.\n4.3.2 Performance. Prior to interpret the extracted FSM, we are\nsupposed to ensure that its behavior and performance are aligned\nwith the original DRL model. We compare the performance be-\ntween the original DRL model ( i.e.,GRU-based DRL) trained with\ncurriculum learning, extracted FSM, a handcrafted FSM, and the de-\nfault setting. Roughly speaking, the principle of handcrafted FSM is\nmigrating CPU cores from the level with the lowest CPU utilization\nrate to the one with the highest CPU utilization rate. It is tested in\nUser Acceptance Testing environment and show 20%reduction of\nmakespan. The default setting refers to no CPU migration during\ntesting. The comparison result over ten real workloads is illustrated\nin Figure 4. It shows that all algorithms get lower average makespan\nthan the default setting. Besides, both the original DRL model and\nextracted FSM perform better than handcrafted FSM ( 11.5%reduc-\ntion of makespan on average). The extracted FSM performs a little\nbit worse than the original DRL model ( 0.88%increase of makespan\non average) since there must be a loss of information after the\nquantizaion of DRL model.\n4.4 FSM interpretation\nWe visualize an extracted FSM over a real workload in Figure 5.\nThere are in total five states ( i.e.,the circles) in the FSM, where each\nstate is associated with an action. For examples, â€œNoopâ€ stands for\nno operation, i.e.,without CPU core migration. â€œN=>Râ€ refers to\nmigrating one core from NORMAL level to RV level. The thickness\nof circle denotes how many transitions are associated with the state\nwhen applying the extracted FSM to the real workload.\nUsing the method described in Section 3.3, we analyze the Fan-in\nFigure 5: Visualization of extracted FSM\nFigure 6: History information of the S2\nand Fan-out statistics and the history information. The Fan-in and\nFan-out statistics in Figure 5 illustrate the basic semantic meaning\nof each state. S0 (â€œNoopâ€) is the most frequent state, since the FSM\nadapts to the workload and then keeps the stabilized configuration\nin the long term. The difference between its Fan-in and Fan-out\nCPU utilization indicates the general fluctuation of workload inten-\nsity. For S1 and S4, they tend to move cores from the level with low\nutilization to the levels with high utilization. This is a basic strategy\nthat simply gives the level with high demand more computation\ncapacity, which is also the strategy used by our handcrafted FSM.\nS2 and S3 do not follow this basic strategy. The history infor-\nmation of S2 in Figure 6 explains this phenomenon. The figure\nshows information of the last 10 average observations before the\ntransition into S2. Recall that read requests only demand loading\nthe data, whereas write requests additionally require writing data\nback to disk. Here we see that the intensity of write workload keeps\nrising while the intensity of read workload stays at 0. Moreover,\nthe capacity ratio (the ratio of computation capacity of NORMAL\nto that of KV and RV) goes up. Obviously, the FSM tried to firstly\nload all relevant data by increasing the capacity of NORMAL. At\nthis moment, it readjusts to give KV and RV more capacity so that\nthe write-back phase of write requests could be satisfied quickly. S3\nalso has similar history. We do not show it due to the limited space.\n5 CONCLUSION\nIn this paper, an integrated pipeline of learning heuristics from DRL\npolicies is presented. We apply the proposed methods to a practical\nresource allocation problem in our storage product. Experimental\nresults demonstrate that both the DRL model and extracted FSM\noutperform handcrafted FSM on various workloads. Visual and sta-\ntistical analyses of the extracted FSM are given to provide insights\ninto the DRL model for domain experts.\n\nREFERENCES\n[1]Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimiza-\ntion. arXiv preprint arXiv:1412.6980 (2014).\n[2]Anurag Koul, Sam Greydanus, and Alan Fern. 2018. Learning finite state represen-\ntations of recurrent policy networks. arXiv preprint arXiv:1811.12530 (2018).\n[3]Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 International\nConference on Management of Data . 489â€“504.\n[4]Martin Maas. 2020. A Taxonomy of ML for Systems Problems. IEEE Annals of the\nHistory of Computing 40, 05 (2020), 8â€“16.[5]Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. 2016.\nResource management with deep reinforcement learning. In Proceedings of the\n15th ACM workshop on hot topics in networks . 50â€“56.\n[6]Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy\nLillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous\nmethods for deep reinforcement learning. In International conference on machine\nlearning . 1928â€“1937.\n[7]Oracle. 2020. Vdbench: A storage benchmarking tool. https://www.oracle.com/\ndownloads/server-storage/vdbench-downloads.html\n[8]Huawei Technologies. 2020. OceanStor Dorado 8000/18000 V6 All-Flash Storage\nSystems. https://e.huawei.com/en/products/cloud-computing-dc/storage/all-\nflash-storage/dorado-8000-18000-v6",
  "textLength": 23348
}