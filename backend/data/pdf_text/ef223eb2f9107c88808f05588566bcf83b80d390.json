{
  "paperId": "ef223eb2f9107c88808f05588566bcf83b80d390",
  "title": "Dynamically Improving Branch Prediction Accuracy Between Contexts",
  "pdfPath": "ef223eb2f9107c88808f05588566bcf83b80d390.pdf",
  "text": "Dynamically Improving Branch\nPrediction Accuracy Between Contexts\nAdam Auten\nDepartment of Electrical and\nComputer Engineering\nUniversity of Illinois Urbana-Champaign\nauten2@illinois.eduTanishq Dubey\nDepartment of Electrical and\nComputer Engineering\nUniversity of Illinois Urbana-Champaign\ntdubey3@illinois.eduRohan Mathur\nDepartment of Electrical and\nComputer Engineering\nUniversity of Illinois Urbana-Champaign\nrmathur2@illinois.edu\nAbstract —Branch prediction is a standard feature in most\nprocessors, signiﬁcantly improving the run time of programs\nby allowing a processor to predict the direction of a branch\nbefore it has been evaluated. Current branch prediction methods\ncan achieve excellent prediction accuracy through global tables,\nvarious hashing methods, and even machine learning techniques\nsuch as SVMs or neural networks. Such designs, however, may\nlose effectiveness when attempting to predict across context\nswitches in the operating system. Such a scenario may lead\nto destructive interference between contexts, therefore reducing\noverall predictor accuracy. To solve this problem, we propose a\nnovel scheme for deciding whether a context switch produces de-\nstructive or constructive interference. First, we present evidence\nthat shows that destructive interference can have a signiﬁcant\nnegative impact on prediction accuracy. Second, we present an\nextensible framework that keeps track of context switches and\nprediction accuracy to improve overall accuracy. Experimental\nresults show that this framework effectively reduces the effect of\ndestructive interference on branch prediction.\nI. I NTRODUCTION\nProcessors are using speculative techniques more and more\nto increase the amount of instruction level parallelism that\noccurs. This is evident in current cache designs, where data\nis prefetched from far memory before actual values are ready\nto be used. Prediction employs the same methodology in order\nto reduce wasted CPU cycles waiting for a branch to be\nevaluated. Rather than wait for the branch direction to be\nevaluated, a prediction is made on the direction of the branch.\nCorrect predictions are rewarded with continuous execution,\nhowever, incorrect predictions are given penalized by forcing\na processor to employ some sort of backtracking method, thus\nwasting cycles correcting its error. The deeper the processor\npipeline, the more serious this penalty is, hence motivating the\nneed for higher accuracy predictors, usually through radical\nnew designs. We take a different approach – one that extends\ncurrent predictor schemes – by improving the accuracy of\npredictors between contexts.\nOur work builds on the observation that as a processor\nswitches between the contexts, the state of the branch pre-\ndictor may destructively interfere with the predictions of the\nupcoming context. It then becomes natural to ask if we can\nreduce the effect of these context switches by learning when\nto clear, or remove, prior history of a branch predictor.\nWe propose a framework that uses multiple counters and\nother bookkeeping data to efﬁciently track and analyze branchpredictor behavior across context switches. Our framework not\nonly gives the branch predictor more data to operate on, but\ncan also consider the effects of a context switch through longer\nhistories.\nThis paper makes the following contributions:\n\u000f We provide insights into the behavior of branch pre-\ndictors with respect to context switches, and show that\ndestructive interference is an issue that can lead to\nperformance degradation.\n\u000f We introduce the context switch accuracy framework\n(CSAF), the ﬁrst framework to use data for contexts as\ninput to a branch predictors, and show that in general\ncases it can improve upon existing predictor accuracy.\nFor a single core ARMv7 processor running a standard\nLinux 3.13 kernel with 1ms kernel tick rate simu-\nlated in the GEM5 framework, the CSAF improves\nmisprediction on 7 out of 11 tested benchmarks in a\npredeﬁend workload.\n\u000f We explain why the CSAF introduces interesting new\nideas for future research.\nII. R ELATED WORK\nA. Dynamic Branch Prediction\nDynamic Branch Prediction has been a well researched\ntopic in recent decades, with modern advances focusing on\nthe improvement, reﬁnement, or various schemes of the two\nlevel scheme described by Yeh et al. [1] This method uses a\nhistory table full of saturating counters that is indexed using\nsome hash of the branch address. The action of the prediction\nis based on the current state of the counter, which is updated\nonce the outcome of the branch is evaluated. The problem here\nis that this scheme, and variations of this scheme, suffer from\na few basic problems.\nThe ﬁrst of these problems is the aliasing of branch history\naddresses, and while there have been advances [2], [3] to\nreduce aliasing, the prediction method remains the same, and\nthus other problems remain unaddressed.\nIn addition, history length is a limiting factor in these\npredictors, ﬁrst simply by the number of entries that can be\nstored in the history table and secondly the size of each entry\nitself. Generous improvements in hardware technology are\nallowing for larger tables simply through brute force. TablearXiv:1805.00585v1  [cs.AR]  2 May 2018\n\nentry sizes are also limited as a function of the number of\nentries, and as the information in the entry decreases, the\nless information the predictor has to act upon, including the\nexclusion of context data from history table entries.\nB. Effects of context switching\nSome work has been done on analyzing the effects of\ncontext switching and its effect on a running application or\noverall workload. This research analyzes the performance of an\napplication during a speciﬁc workload, however, these papers\nusually focus on cache performance, with a tangential focus on\nbranch prediction [4], [5]. They show that the effects of context\nswitching can be diminished in memory through clever cache\nsize manipulations or optimizations based on the expected\nworkload, demonstrating that context switches should not be\nregarded as trivial to CPU workloads.\nIndeed, work has also been done to analyze the effects of\ncontext switching on branch prediction. Such research suggests\nthat context switching does not have the expected signiﬁcant\neffect on branch prediction accuracy as one would expect, but\nrather, most accuracies reach a steady state when using realistic\ntime slices for contexts [6]. However, this is countered by other\nworks which state that in other, nontrivial, workloads, certain\nbranch predictors, such as the two level scheme previously\ndiscussed, may be susceptible to accuracy loss when switching\ncontexts [7].\nOther work has also been done to develop branch predictor\nmodels that incorporate context data in order to improve\nprediction accuracy [8]. Results from these works demonstrate\nthat using context data, appreciable results were obtained from\nthese schemes. However, the downside remains that these\nschemes present wholly new methods for branch prediction,\ninstead of building on top of existing implementations that\nhave already been shown to perform well.\nIII. M OTIVATION\nA. Lack of predictors that use contextual data\nWhen looking at the most common branch prediction\nschemes, we can see that they are mostly based off the work of\nYeh and Patt with their standard two level design. Of course,\nsystems built off this scheme generally exclude the idea of\ncontexts, with most of the data for the predictor coming from\ncurrent PC address possibly combined with previous accuracy\nor PC data. This complete oversight of context data means that\nin thee case where contexts do play a large role in workload\nperformance, the branch predictor will not know how the\ncontext data plays a role in the prediction it is about to make.\nDue to this lack of data, we feel that a gap has been created\nin the branch prediction realm that completely ignores the fact\nthat in modern computing, contexts switches occur very often\nand also occur across CPUs in the system. This then begs the\nquestion of what would the performance of a context aware\nbranch predictor look like and what sort of insight could a\ncontext provide to a prediction scheme.\nB. Impact of context switching\nIn order to further justify the work needed to implement\nthe CSAF, we needed to quantify the amount of impact acontext switch has on branch prediction accuracy. For this,\na representative baseline workload was constructed to run on\nthe GEM5 simulator with with branch prediction accuracy\nwas recorded. Speciﬁcally, a single core ARMv7 processor\nrunning a Linux kernel modiﬁed to have a context switch every\nmillisecond, ran a workload consisting of eight benchmarks.\nWith this workload, mispredict rate was recorded and graphed.\nThe results can be seen in Figure 1. As is demonstrated in\nthe graph, there is a signiﬁcant spike in the mispredict rate\nat every context switch. These spikes average around 200,000\ncycles in length before reaching a steady state, which then\nleaves room for improvement. In addition to this simulated\nworkload, a secondary, worst-case test was done in order to\nsee what the worst possible context switch might look like. In\norder to simulate this, two methods were used. First, every ten\nthousand cycles, all branch prediction history was inverting,\nmeaning every taken was set to not-taken, and every not-taken\nwas set to taken. With this scenario, which is visualized in\nFigure 2, it can be seen that the mispredict rate spikes to 60%,\nwith mispredict rate spikes decreasing with predictor size. In\nthe second scenario, the entire history was not inverted, but\nrather reset to the default predictor state. Similar results were\nseen here, with mispredict rates spiking to nearly 40% and\ndecreasing linearly with predictor size. All in all, it is clear\nthat context switching does have an effect on the accuracy of\nthe predictor, leaving space to improve the predictor.\nFig. 1. Branch misprediction rate vs. time for a multi-process benchmark\nFig. 2. Transient response for Tournament branch predictors in response to\nworst-case aliasing and reset\nIV. D ESCRIPTION\nWhen analyzing destructive behavior between contexts\nwhen using two-level adaptive branch prediction schemes, we\nidentiﬁed that the main source of initial misprediction stems\nfrom saturation counters that have been ﬂipped since the last\ntime the context ran in its time slice. The error arises when\n\nthe new context gets switched in, and begins executing, using\nthe branch history for a previous context’s execution patterns,\ninstead of its own. This insight into the poor behavior of branch\npredictors immediately after a context switch was one of the\nguiding factors when developing our novel approach to this\nproblem.\nAnother factor that inﬂuenced our design was the fact that\na reset of the entire branch predictor’s state is often more\ndetrimental than simply using a previous processes branch\npredictor state, meaning that simply resetting the entire branch\npredictor upon every context switch would not produce viable\nresults. Only in speciﬁc cases of destructive interference is\nusing a previous processes state more detrimental than a reset\nof branch predictor state, meaning that our design needed to\nhave some adaptive qualities to it.\nUsing these factors as the primary motivations for our\ndesign, we began formulating our design. First, we recognized\nthat resetting the entire predictor state was a very destructive\noperation, especially in larger predictor tables. During a time\nslice, we saw that for the benchmarks we ran, there would\noften only be 15%-25% pattern history table (PHT) entry usage\nby a speciﬁc program, with an even fewer number of PHT\nentries changing their direction (from taken to not taken, or the\nother way around). Wiping all of the PHT entries regardless of\nwhether they actually changed since the last time that speciﬁc\nprogram was given a time slice is wasteful. Instead, we chose\na more moderate approach, to only wipe the PHT entries that\nhad changed direction (meaning, the PHT entries that changed\nfrom taken to not-taken, and vice versa) since the last time the\nprogram that is about to run. This ensured that only branch\npredictor data that remains in the predictor is either reset data,\nor data that pertains to the upcoming program, both of which\nare better than the theoretical worst-case state of the predictor.\nSecondly, we wanted to control this behavior dynamically,\nchoosing whether to reset the modiﬁed PHT entries if the\nbehavior is deemed to be optimal or not. To do this, we took\ninspiration from PHT tables themselves, by utilizing saturated\ncounters. These counters are each associated with a speciﬁc\nPID to PID transition, and indicate whether the PHT entries\nthat changed since the last run should be reset or not. By only\nupdating these counters when we see behavior that is better\nor worse by a certain threshold, we remove noise and random\nblips of branch misprediction data.\nIn order to update the saturated counters, we need to\nclassify both good and bad behavior. Building off of the\ninsight into what causes mispredictions after a context switch\n(changed PHT entries), we came up with a metric to measure\nwhether resetting the modiﬁed PHT entries was a desirable\naction or not. By simply tracking the number of PHT changes\nthat take place over the course of a process’ time slice, we\ncan compare this number to the previously seen number of\nPHT entry changes. If it has changed by a certain threshold\nby getting worse, then we invert the counter.\nUpdating of the saturated counters and choosing whether to\nreset the modiﬁed PHT entries are both intrinsically linked to\na context switch. Because of this, both these actions should\ntake place during each context switch that occurs in the\noperating system. First, the previous transitions number of\nPHT changes should be updated with the new value, and thecounter should be inverted or not, depending on whether it\nwas a unfavorable or favorable outcome, respectively. After\nthe previous transition’s data is updated, we look at the new\ntransitions saturated counter, and reset the PHT entries that\nwere modiﬁed since the last time slice if the saturated counters\nindicate that they should be.\nV. M ETHODOLOGY\nTo test this framework out to see how it performs when\ncompared to baseline runs without the framework, we im-\nplemented this framework within GEM5, a commonly used\nsystem simulation tool.\nTo hold all of the various counters and keep track of the\nnumber of modiﬁed PHT entries that changed, we chose to\nimplement this list as a ﬁxed size two dimensional array. This\narray is indexed by (current pid, next pid) upon every context\nswitch, with an LRU replacement policy if the array is ﬁlled.\nEvery entry in the two dimensional array holds two items. The\nﬁrst piece of data is the number of PHT entry changes that\noccurred after the last time the corresponding transition was\nencountered. Secondly, each entry contains a saturated counter,\ninitialized to strongly not taken at the start.\nEvery context switch, two updates would occur in the two\ndimensional array. Firstly, the previously seen transition’s entry\nneeds to be updated (old pid!current pid). If the previous\ntransitions number of changes in the PHT table was smaller by\na certain threshold than the newly found number of changes\nin the PHT table, then we classify this as worse behavior than\nwhat we previously saw. In this case, we should invert the\ncounter, so that the opposite action is taken than previously\ntaken, to see if that method yields better data. If the behavior\nof the previously taken action is considered to be better than\nwhat was stored, then we do nothing. We then update the stored\nnumber of modiﬁed PHT entries for that particular PID to PID\ntransition entry in the array.\nAfter we update the previously seen transition’s entry,\nwe have to decide what to do with the current transition\n(current pid!next pid). We then look up the new transitions\nentry in the table. If the counter indicates taken, then we wipe\nthe modiﬁed PHT entries, and continue execution as normal.\nIf implemented in a real ARM system, one of the biggest\nhurdles to go from simulation to a real working product is be-\ning aware of context switches, along with getting information\nabout process IDs. Luckily, this problem is easily circumvented\nwith a feature present on newer ARM processors, known as\nARM Software Thread ID registers [9]. By monitoring writes\nthese registers, notifying the branch prediction framework of\na context switch is very feasible.\nVI. R ESULTS\nTo form a baseline of how context switches effect multiple\nbenchmarks, we simulate context switching between multiple\nthreads. Using the full-system emulation mode [10], we sim-\nulated a single core ARM system using the O3 ARM v7a 3\nCPU, running a standard Linux 3.13 kernel with 1 ms kernel\ntick rate. Both the instantaneous misprediction for the predic-\ntion and the average misprediction rate for each process was\nmeasured. The instantaneous misprediction rate was measured\n\nTABLE I. P ER-PROCESS MISPREDICTION RATES\nBenchmark Branch Mispredictions (%)\nBaseline CSAF Always Reset\nBubblesort 10.128 10.051 11.31\nFloatMM 3.095 3.136 3.572\nIntMM 3.418 3.449 4.024\nOscar 3.811 3.786 4.136\nPerm 18.339 18.001 17.32\nPuzzle 3.854 3.849 4.09\nQueens 11.853 11.904 10.88\nQuicksort 14.638 14.679 14.699\nRealMM 3.481 3.454 4.17\nTowers 8.711 8.753 8.47\nTreesort 8.505 8.477 8.86\nusing a 1000 branch sliding window. Large spikes in the\nmisprediction rate are observed on context switch boundaries.\nTo gauge the effectiveness of our algorithm of conditionally\nresetting the predictor on destructive context switches, we\ncompared the instantaneous misprediction rate of our algorithm\nto baseline. Figure 4 shows the differential misprediction rate\nin response to a context switch, with the results tabulated\nin table I. The differential rate is calculated by simulating a\nmulti-process benchmark with and without conditional context\nswitch resets. For each time, the instantaneous misprediction\nrate is subtracted, yielding an instantaneous differential mispre-\ndiction rate. The large negative spike in mispredicts show that\ninstantaneous mispredictions are reduced at a context switch\nboundary.\nThe average misprediction rate of each thread over the\ncourse of its execution is shown in ﬁgure 3. We see that\nwhile some benchmarks see a marginal improvement, others\nare not improved. The small improvement seen is likely due\nto the infrequency of context switches in our traces (1 every\nmillisecond). Due to limitations in the Linux kernel, we were\nunable to simulate smaller time slices. Interaction between\nthreads determines to a large degree the effectiveness of our\nalgorithm. Misprediction is only improved if there is signiﬁcant\naliasing within the predictor between threads. If the degree of\naliasing is small, resetting will do more harm by unnecessarily\nun-learning any constructive or neutral aliasing. Additionally,\nif the predictor footprint of a thread is small, resetting the pre-\ndictor affects the performance of non-contiguously scheduled\nprocesses.\nVII. F UTURE WORK\nIn our simulations, we used a 128-entry BiMode predictor\nto enhance the aliasing effects. Future work will involve\nsimulations on larger predictors that are more performant\nand prevalent in industry. Additionally, instead of inverting\nthe saturating counters on bad behavior, we can use the\nsaturated counters to their full potential by incrementing and\ndecrementing them instead of simply inverting them. In our\ntrials, we found this did not provide much beneﬁt, and that\nthe thresholding was enough to reset when destructive behavior\nwas identiﬁed. However, we did not simulate for very long (at a\nmaximum, 500,000,000 CPU cycles), which only encompassed\na relatively small number of context switches. Perhaps as\na program’s behavior got better or worse, using a saturated\ncounter would have allowed for even less unnecessary resets,\nimproving the misprediction rate even further.\nFig. 3. Fraction of mispredicted branches per process when all are run\ntogether on a single core and context switched with a 1ms time slice\nFig. 4. Transient response of differential misprediction rate during a context\nswitch\nWhen looking at the full branch prediction scheme, most\nmodiﬁcations that are made generally change how the table\noperates or how to fundamentally change the scheme to be\nmore robust. With our ﬁndings, we posit a new methodology\nthat could be beneﬁcial to prediction. Instead of modifying the\nbranch predictor by adding hardware components or wholly\n\nchanging it, we could modify a critical portion of the scheme.\nBased on recent work by Kraska et al. titled The Case for\nLearned Index Structures [11], we can see that there is a\nsigniﬁcant performance in using machine learning to learn the\nhash function into a map, or the access pattern to an array. In\nthe same lieu, the idea could be applied to branch predictors.\nInstead of using a static hash function for the predictor table,\nwe can learn the best access pattern to the branch history\ntable. This could potentially signiﬁcantly improve accuracy\nas various parameters could be provided to the learning hash\nfunction, such as context, to improve performance.\nREFERENCES\n[1] T. Yeh and Y . Patt, ”Two-Level Adaptive Training Branch Prediction”, in\nProc. 24th Annual International Symposium in Microarchitecture , 1991.\n[2] C. Lee, I. Chen and T. Mudge, ”The Bi-Mode Branch Predictor”, 1997.\n[3] S. McFarling, ”Combining Branch Predictors”, in digital , Palo Alto,\nCalifornia, 1993.\n[4] W. Hwu and T. Conte, ”The susceptibility of programs to context\nswitching”, in IEEE Transactions on Computers , 1994, pp. 994-1003.\n[5] J. Mogul and A. Borg, ”The Effect of Context Switches on Cache\nPerformance”, in digital , Palo Alto, California, 1990.\n[6] M. Co and K. Skadron, ”The Effects of Context Switching on Branch\nPredictor Performance”, 2001.\n[7] J. Chen, M. Smith, C. Young and N. Gloy, ”An Analysis of Dynamic\nBranch Prediction Schemes on System Workloads”, Philadelphia, 1996.\n[8] P. Chang, Y . Patt and M. Evers, ”Using Hybrid Branch Predictors\nto Improve Branch Prediction Accuracy in the Presence of Context\nSwitches”, Philadelphia, 1996.\n[9] ”ARM Information Center”, Infocenter.arm.com, 2017. [Online]. Avail-\nable: http://infocenter.arm.com/help/index.jsp. [Accessed: 15- Dec-\n2017].\n[10] ”gem5 Full System Simulation gem5 Tutorial 0.1 documentation”,\nLearning.gem5.org, 2017. [Online]. Available: http://learning.gem5.org/\nbook/part4/intro.html. [Accessed: 15- Dec- 2017].\n[11] T. Kraska, A. Beutel, E. Chi, J. Dean and N. Polyzotis, ”The Case for\nLearned Index Structures”, 2017.",
  "textLength": 22679
}