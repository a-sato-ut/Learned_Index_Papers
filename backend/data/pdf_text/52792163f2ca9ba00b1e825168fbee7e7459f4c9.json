{
  "paperId": "52792163f2ca9ba00b1e825168fbee7e7459f4c9",
  "title": "Looper: An End-to-End ML Platform for Product Decisions",
  "pdfPath": "52792163f2ca9ba00b1e825168fbee7e7459f4c9.pdf",
  "text": "Looper: an end-to-end ML platform for product decisions\nIGOR MARKOV, HANSON WANG, NITYA KASTURI, SHAUN SINGH, MIA GARRARD, YIN HUANG,\nSZE WAI YUEN, SARAH TRAN, ZEHUI WANG, IGOR GLOTOV, TANVI GUPTA, PENG CHEN, BOSHUANG\nHUANG, XIAOWEN XIE, MICHAEL BELKIN, SAL URYASEV, SAM HOWIE, EYTAN BAKSHY, and NORM\nZHOU, Meta, USA\nModern software systems and products increasingly rely on machine learn-\ning models to make data-driven decisions based on interactions with users,\ninfrastructure and other systems. For broader adoption, this practice must\n(ùëñ) accommodate product engineers without ML backgrounds, ( ùëñùëñ) support\nfinegrain product-metric evaluation and ( ùëñùëñùëñ) optimize for product goals. To\naddress shortcomings of prior platforms, we introduce general principles\nfor and the architecture of an ML platform, Looper , with simple APIs for\ndecision-making and feedback collection. Looper covers the end-to-end ML\nlifecycle from collecting training data and model training to deployment\nand inference, and extends support to personalization, causal evaluation\nwith heterogenous treatment effects, and Bayesian tuning for product goals.\nDuring the 2021 production deployment, Looper simultaneously hosted 440-\n1,000 ML models that made 4-6 million real-time decisions per second. We\nsum up experiences of platform adopters and describe their learning curve.\nCCS Concepts: ‚Ä¢Computing methodologies ‚ÜíMachine learning ;‚Ä¢\nSoftware engineering ‚ÜíSoftware development process management .\nAdditional Key Words and Phrases: Machine Learning, platform, MLOps\nACM Reference Format:\nIgor Markov, Hanson Wang, Nitya Kasturi, Shaun Singh, Mia Garrard, Yin\nHuang, Sze Wai Yuen, Sarah Tran, Zehui Wang, Igor Glotov, Tanvi Gupta,\nPeng Chen, Boshuang Huang, Xiaowen Xie, Michael Belkin, Sal Uryasev,\nSam Howie, Eytan Bakshy, and Norm Zhou. 2022. Looper: an end-to-end\nML platform for product decisions. In Proceedings of the 28th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining (KDD ‚Äô22), August\n14‚Äì18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 11 pages.\nhttps://doi .org/10 .1145/3534678 .3539059\n1 INTRODUCTION\nWith growing adoption of machine learning (ML), personalization\nis proving essential to competitive user experience [ 18]. To support\nusers with different preferences, one needs good default tactics, user\nfeedback, prioritizing delivered content and available actions [ 37].\nWhen managing limited resources, e.g., for video serving, similar\nlogic applies to network bandwidth, response latency, and video\nquality [ 23,34]. This paper explores the use of ML for personalized\ndecision-making in software products using what we call smart\nstrategies (Section 2). Making smart strategies available to product\nengineers is challenging: 1Long-term product objectives rarely\nmatch closed-form ML loss functions. 2Product-generated data\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\n¬©2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9385-0/22/08. . . $15.00\nhttps://doi .org/10 .1145/3534678 .3539059drifts away from training data. 3Capturing correlations in train-\ning data (via ML) does not imply causal improvement of product\nmetrics [ 10,53].4Clean data to evaluate the performance of ML\nsystems is often unavailable, necessitating A/B testing. 5Tradi-\ntional A/B tests neglect personalized treatments. 6Platforms to\ntrain, host and monitor hundreds of ML models are needed and\npromise economies of scale. 7Real-time feature extraction and\ninference are needed, despite more efficient async batch processing.\n8Product engineers, many new to ML, need a simple, standard,\nfuture-proof way to embed smart strategies into products.\nData-centric ML development is a recent concept of refocusing\nML development from models to data [ 35]. It supports software per-\nsonalization with off-the-shelf models, where collecting the right\ndata and selecting the appropriate class of models become primary\ndifferentiators [ 37]. Compared to developing and training ML mod-\nels, data adequacy is often overlooked [ 45], and product platforms\nmust use automation to compensate. Per Andrew Ng, ‚Äúeveryone\njokes that ML is 80% data preparation, but no one seems to care‚Äù [ 44].\nYet, directly handling data sets and ML models in product code is\ncumbersome. Instead, software-centric ML integration with data\ncollection and decision-making APIs offers a front-end to MLOps\nautomation (Sections 2.2 and 3.2). Additionally, ML development\noften neglects structure in product evaluation data (Section 3.4).\nVertical ML platforms lower barriers to entry and support the\nentire lifecycle of ML models (Figure 1) in a repeatable way. Hor-\nizontal ML platforms provide storage, support data pipelines and\noffer basic services, whereas vertical platforms foster the reuse of\nnot only ML components, but also workflows. At firms like Google,\nMeta, LinkedIn, Netflix, specialized end-to-end vertical platforms\ndrive flagship product functionalities, such as recommendations.\nThey have also been applied to software development, code qual-\nity checks, and even to optimize algorithms such as sorting and\nsearching [ 15]. Platforms are built on ML frameworks like Tensor-\nFlow [ 1] and PyTorch [ 33] that focus on modeling for generic ML\ntasks, support hardware accelerators, and act as toolboxes for appli-\ncation development [ 24,36]. Supporting smart strategies requires\ngeneral-purpose vertical platforms to offer end-to-end ML lifecycle\nmanagement. General-purpose vertical ML platforms can be inter-\nnal to a company ‚Äî Apple‚Äôs Overton [ 41] and Uber‚Äôs Michelangelo\n[27], ‚Äî or broadly available to cloud customers ‚Äî Google‚Äôs Vertex,\nMicrosoft‚Äôs Azure Personalizer [ 2] and Amazon Personalize. A com-\nmon theme is to help engineers ‚Äúbuild and deploy deep-learning\napplications without writing code‚Äù via high-level, declarative ab-\nstractions [ 37]. Improving user experience and system performance\nwith ML remains challenging [ 40] as correlations in data found by\nML models might not lead to causal improvements. Little is known\nabout optimizing for product goals [37, 52].\n1arXiv:2110.07554v8  [cs.LG]  21 Jun 2022\n\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Markov and Wang, et al.\ndomain-specific ML tasks, e.g., ranking for a product surfacedifferent ML tasks across variety of product domainsapp specific data transforms; extensive customizationmultiple transforms available for different apps; limited customization‚óèlarge custom models‚óèmanual feature selection & eng‚óècustom model optimization‚óètransfer learning possible‚óèa small set of use cases‚óèmodular architecture‚óèmany standard model types‚óèsmaller model sizes‚óèautomated feature selection‚óèautomated model optimization‚óèlarge variety of use cases‚óèapp specific monitoring and alerts‚óècustom deployment; co-optimized    with app stack‚óèflexible monitoring and alerts‚óèco-hosted deployment; modular interface with multiple appsSpecialized Vertical Platform for App Performance & Scalabilityused for images, videos, speech, custom recommendation systemsGeneral-purpose Vertical Platform for Usability & End-To-End Managementused for smart strategies, configurable recommendation systems\n‚óèend metrics rarely changed‚óèmodularized & configurable APIs ‚óèeasy to add new metrics Problem typeData preparation\nProduct impact eval. & opt.‚óèdomain- & problem-specific data sources‚óèdata collection via custom APIs‚óèextreme data capacity possible‚óèdiff. data sources for diff. apps‚óèdata collection via standard APIs‚óèdata capacity often limited by co-hostingData sources & collectionHorizontal ML Platforms for Problem Space Coverage, ML Performance and ScalabilityModel selection & optimizationDeployment & maintenance\nFig. 1. Categories of applied ML platforms: horizontal vs. vertical, specialized vs. general-purpose (back arrows show vertical optimizations\nbased on product metrics, see Section 3.3). Specialized platforms are limited in their support for diverse applications.\nWe develop support for data-driven real-time smart strategies\nvia a general-purpose vertical end-to-end ML platform called Looper ,\ninternal to Meta, for rapid, low-effort deployment of moderate-\nsized models. Looper is a declarative ML system [27,36,37,41] with\ncoding-free full-lifecycle management of smart strategies via a GUI.\nOur technical contributions include 1a full-stack real-time\nML platform (Section 3) with causal product-impact evaluation\nand optimization and handling of heterogeneous treatment effects\n(Sections 3.4, 4.2) via an experiment optimization system and meta-\nlearners, 2a generic framework for targeting long-term outcomes\nby parameterized policies using plug-in supervised learning models\nand Bayesian optimization (Sections 3.3, 3.4, 4.2), 3thestrategy\nblueprint abstraction to optimize not only models, but the entire ML\nstack (Figures 1, 3 and Section 3.3), 4capturing decision inputs and\nobservations online via the succinct Looper API for product code;\nnot only predicting what‚Äôs logged by the API, but also optimizing\nblack-box product objectives (Section 3.2), 5broad deployment\nand substantial impact on product metrics (Section 4), 6analysis\nof resource-usage bottlenecks (Appendix A), 7qualitative analysis\nof our platform via a survey of adopters (Appendix B).\nSpecialized vertical ML platforms limit application diversity, while\nLooper hosts hundreds of production use cases thanks to its general-\npurpose architecture . Many vertical platforms [ 2,15,36,41], don‚Äôt\nsolve as wide a selection of ML tasks as Looper does (classification,\nestimation, value and sequence prediction, ranking, planning) us-\ning supervised and reinforcement learning. Unlike platforms with\nasynchronous batch-mode feature extraction and inference [ 25,27],\nLooper runs in real time and optimizes resource usage accordingly\n(Appendix A). To balance model quality, size and inference time,Looper AutoML selects models and hyperparams, and performs\nvertical optimizations via strategy blueprints (Section 3.3).\nIn the remainder of the paper , Section 2 explores ML-driven\nsmart strategies and relevant platform needs. Section 3 covers de-\nsign principles for the Looper platform, introduces the architecture,\nthe API, the blueprints, and specializations. Section 4 summarizes\nproduct impact at Meta, comparisons to baselines and adoption\nstatistics. Appendices provide data to foster reproduciibility.\n2 ML FOR SMART STRATEGIES\nCompared to benchmark-driven research, ML that interacts with\nthe world runs into additional challenges. In this paper, we target\nsmart strategies at key decision points in software products, e.g.,\n‚Ä¢application settings and preferences: selecting between de-\nfaults and user-specified preferences\n‚Ä¢adaptive interfaces ‚Äî certain options are shown only to users\nwho are likely to pursue them\n‚Ä¢controlling the frequency of ads, user notifications, etc\n‚Ä¢prefetching or precomputation to reduce latency\n‚Ä¢content ranking and prioritizing available actions\nUser preferences and context complicate decision-making. Sim-\nplifying a UI menu can boost product success, but menu preferences\nvary among users. Prefetching content to a mobile device can en-\nhance user experience, but may require predicting user behavior.\nWhile human-crafted heuristic strategies often suffice as an initial\nsolution, ML-based smart strategies tend to outperform heuristics\nupon sufficient engineering investment [ 15,29]. The Looper plat-\nform aims to lower this crossover point to broaden the adoption\nof smart strategies and deliver product impact over diverse appli-\ncations. In this section, we discuss modeling approaches to enable\nsmart strategies and cover the priorities in building such a platform.\n2\n\nLooper: an end-to-end ML platform for product decisions KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\n2.1 Modeling approaches for smart strategies\nSmart strategies are backed by supervised learning, contextual ban-\ndits (CB), and/or MDP-style reinforcement learning (RL). Given a\nmodel type, product decision problems need (1) ML optimization\nobjectives to approximate the product goal(s) and (2) a decision\npolicy to convert objective predictions into a single decision.\nApproximating product goals with predictable outcomes (al-\nternatively referred to as proxy orsurrogate objectives) is a major\ndifference between industry practice and research driven by existing\nML models with abstract optimization objectives [ 48]. Good proxy\nobjectives should be readily measurable and reasonably predictable.\nIn recommendation systems, the ‚Äúsurrogate learning problem has\nan outsized importance on performance in A/B testing but is diffi-\ncult to measure with offline experiments‚Äù [ 16]. We note a delicate\ntradeoff between easy-to-measure objectives directly linked to the\ndecision vs. more complex objectives, e.g., ad clicks vs. conversions.\nFurthermore, product goals often implicitly have different weighting\nfunctions than the ML objective (e.g., the feedback provided by most\nprolific product users does not always represent other users [ 12]).\nObjectives can be modeled directly using supervised learning ; alter-\nnatively, using CBs can model uncertainty in predictions across one\nor more objectives, which may then be used for exploring the set of\noptimal actions, e.g., in Thompson sampling [ 2,3,19,32]. The use\nof RL enables the optimization of long-term, cumulative objectives,\nhelping use cases with sequential dependencies [ 7,24,32]. To evalu-\nate any one of these types of models and decision rules, true effects\nof the ML-based smart strategies can be estimated via A/B tests.\nDecision policies postprocess the raw model outputs into a final\nproduct decision or action. For single-objective tasks in supervised\nlearning this may be as simple as making a binary decision if the\nobjective prediction exceeds a threshold, e.g. turning the probability\nof a click into a binary prefetch decision (Section 4.3). For tasks with\nmultiple objectives and more complex action spaces, the template\nfor a decision policy is to assign a scalar value or score to all possible\nactions in the decision space, which can then be ranked through\nsorting. In recommendation systems, a standard approach is to use\na combination function (usually a weighted product of objective\npredictions) to generate a score for each candidate [ 55]. When using\nreinforcement learning, reward shaping [31] weighs task scores in\nthe reward function to optimize for the true long-term objective.\nOptimizing this weighting for multi-objective tasks is explored in\nSection 3.3. More sophisticated policies also use randomization to\nexplore the action space, e.g. Thompson sampling in contextual\nbandits [ 19], orùúÄ-greedy approaches for exploration in ranking [ 3].\n2.2 Extending end-to-end ML for smart strategies\nTraditional end-to-end ML systems go as far as to cover model pub-\nlishing and serving [ 27,36,37,41], but to our knowledge rarely\ntrack how the model is used in the software stack. Assessing and\noptimizing the impact of smart strategies, especially with respect\nto product goals, requires experimentation on all aspects of the\nmodeling framework ‚Äì from metric and model selection to policy\noptimization. To streamline this experimentation and reap its bene-\nfits, smart-strategies platforms must extend the common definition\nof end-to-end into the software layer.Software-centric ML integration [2,15] ‚Äì where data collection\nand decision-making are fully managed through platform APIs ‚Äì en-\nables both high-quality data collection and holistic experimentation.\nNotably, the platform can now keep track of all decision points and\nsupport A/B tests between different configurations. Well-defined\nAPIs improve adoption among product engineers with limited ML\nbackground, and ML configuration can be abstracted via declarative\nprogramming or GUI without requiring coding [37].\nEnd-to-end AutoML. Hyperparameter tuning is often automated\nper model via black-box optimization [ 11]. But optimizing the loss\nfunction of SOTA models by 1% often brings no long-term product\ngains, whereas tuning decision policy params usually helps, e.g.,\nby better reflecting penalties for false positives/negatives. In our\nfull-stack (extended end-to-end) regime, we enable AutoML for the\nentire ML pipeline via declarative strategy blueprints (Section 3.3)\nand an adaptive experiments framework tied to product metrics [ 9].\n2.3 Additional requirements for smart strategies\nMetadata features for product-specific models (e.g., account type,\ntime spent online, interactions with other accounts) introduce new\naspects to learning smart strategies in addition to traditional content\nfeatures (images, text, video) commonly handled by ML platforms.\nUnlike image pixels, metadata features are diverse, uncorrelated,\nrequire non-uniform preprocessing, and are often joined from dif-\nferent sources. Patterns in metadata change quickly, necessitating\nregular retraining of ML models on fresh data, as well as monitoring\nand alerts. Interactions between dense metadata features can often\nbe handled by GBDTs or shallow neural nets. Sparse and categorical\nfeatures need adequate representations [ 42] and special provisions\nif used by neural network architectures [38].\nNon-stationary environments are typical for deployed products\nbut not for research prototypes and SOTA results.\nLogging and performance monitoring are important capabili-\nties for a production system. Dashboards monitor system health\nand help understand model performance in terms of statistics, dis-\ntributions and trends of features and predictions, automatically\ntriggering alerts for anomalies [ 4,13]. Our platform integrates with\nMeta ‚Äôs online experimentation framework, and production models\ncan be withdrawn quickly if needed.\nMonitoring and optimizing resource usage flags inefficiences\nacross training and inference. Our monitoring tools track resource\nusage to components of the training and inference pipeline (Section\n3.2), and help trade ML performance for resources and latency.\n3 THE LOOPER PLATFORM\nTo support a smart strategy, a vertical ML platform (Figure 1) col-\nlects features and labels from a running product, trains a model, and\nproduces predictions in real time for use in the product. Such \"loops\"\nneed operational structure ‚Äî established processes and protocols\nfor model revision and deployment, evaluation and tracking of prod-\nuct impact, and overall maintenance. We now introduce insights,\ndesign principles and an architecture for a vertical smart-strategies\nplatform to address the needs outlined in Sections 1 and 2.\n3\n\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Markov and Wang, et al.\n3.1 Design principles and a concept inventory\nIn contrast to heavy-weight ML models for vision, speech and NLP\nthat favor offline inference (with batch processing) and motivate\napplications built around them, we address the demand for smart\nstrategies within software applications and products. These smart\nstrategies operate on metadata ‚Äî a mix of categorical, sparse, and\ndense features, often at different scales. Respective ML models are\nlightweight, they can be re-trained regularly and deployed quickly\non shared infrastructure in large numbers. Downside risks are re-\nduced via (ùëñ) simpler data stewardship, ( ùëñùëñ) tracking product impact,\n(ùëñùëñùëñ) failsafe mechanisms to withdraw poorly performing models.\nSmart strategies have a good operational safety record and easily\nimprove naive default behaviors.\nThe human labeling process common for CV and NLP fails for\nmetadata because relevant decisions and predictions (a) only make\nsense in an application context, (b) in cases like data prefetch (Section\n4.3) only make sense to engineers, (c) may change seasonally, and\neven daily. Instead of human labeling, our platform interprets user-\ninteraction and system-interaction metadata as either labels for\nsupervised learning or rewards for reinforcement learning. To\nimprove operatonal safety and training efficiency, we rely on batch-\nmode (offline) training, even for reinforcement learning. Given real-\ntime inference, model agility beyond daily re-training is supported\nby real-time engineered features, such as event counters.\nOur platform ensures fast onboarding, robust deployment and\nlow-effort maintenance of multiple smart strategies where posi-\ntive impacts are measured and optimized directly in application\nterms (Appendix B). To this end, we separate application code from\nplatform code, and leverage existing horizontal ML platforms with\ninterchangeable models for ML tasks (Figure 1). Intended for com-\npany engineers, our platform benefits from high-quality data and\nengineered features in the company-wide feature store [ 39]. To sim-\nplify onboarding for product teams and keep developers productive,\nwe automate and support\n‚Ä¢Workflows avoided by engineers [ 45], e.g., feature selection\nand preprocessing, and tuning ML models for metadata.\n‚Ä¢Workflows that are difficult to reason about, e.g., tuning ML\nmodels to product metrics.\nWe first introduce several concepts for platform design.\nThedecision space captures the shape of decisions within an ap-\nplication which can be made by a smart strategy. It can be just {0,1}\nto indicate whether a notification is shown. It can be a continuous-\nvalue space for time-to-live (TTL) of a cache entry. It can be a data\nstructure with configuration values for a SW system, such as a live-\nvideo stream encoder. With reinforcement learning, the decision\nspace matches well with the concept of action space.\nApplication context captures necessary key information provided\nby a software system at inference time to make a choice in the\ndecision space. The application context may be directly used as\nfeatures or it may contain ID keys to extract the remaining features\nfrom the feature store (Section 3.3).\nProduct metrics evaluate the performance of an application and\nsmart strategies. When specific decisions can be judged by product\nmetrics, one can generate labels for supervised learning, unlike for\nmetrics that track long-term objectives.A proxy ML task casts product goals in mathematical terms to\nenable (ùëñ) reusable ML models that optimize formal objectives and ( ùëñùëñ)\ndecision rules that map ML predictions into decisions (Section 2.1).\nSetting proxy tasks draws on domain expertise, but our platofrm\nsimplifies this process.\nEvaluation of effects on live data verifies that solving the proxy\ntask indeed improves product metrics. Access to Meta‚Äôs monitoring\ninfrastructure helps detect unforeseen side effects. As in medical\ntrials, (1) we need evidence of a positive effect, (2) side-effects should\nbe tolerable, and (3) we should not overlook evidence of side-effects.\nOn our platform, product developers define the decision space, al-\nlowing the platform to automatically select model type and hyper-\nparameter settings. The models are trained and evaluated on live\ndata without user impact, and improved until they can be deployed.\nNewly trained models are canaried (deployed on shadow traffic)\nbefore product use ‚Äì such models are evaluated on a sampled subset\nof logged features and observations, and offline quality metrics (e.g.,\nMSE for regression tasks) are computed. This helps avoid degrading\nmodel quality when deploying newer models.\n3.2 Platform architecture: the core\nTraditional ML pipelines build training data offline, but our platform\nuses a live feature store and differs in two ways:\n‚Ä¢Software-centric vs. data-centric interfaces. Rather than\npassed via files or databases, training data are logged from\nproduct surfaces as Looper APIs intercept decision points\nin product software. Product engineers delegate training-\ndata quality concerns (missing or delayed labels, etc) to the\nplatform. Missing data are represented by special values.\n‚Ä¢An online-first approach. Looper API logs live features\nand labels at the decision and feedback points, then joins and\nfilters them via real-time stream processing. This immediate\nmaterialization avoids data hygiene issues [ 2] and storage\noverhead: it keeps training and inference consistent and lim-\nits label leakage by separating features and labels in time.\nLooper‚Äôs complete chain of custody for data (without exposing\ndata tables or files) helps prevent engineering mistakes.\nThe Looper RPC API relies on two core methods:\nI.getDecision(decision _id,application _context)returns\na decision-space value, e.g., True/False for binary choices or a\nfloating-point score for ranking. Unlike in the 3-call APIs in [ 2,15],\ngetDecision \n‚ë† Extract features \n‚ë¢\nCanary \nSystem Joining \nSystem Predictor \nSystem \nClient \nAPI\nBlueprint \n‚ë°Publish model \n‚ëß\n‚ë©\nPromote canary \nTraining \nSystem Feature Store \nPredict \n‚ë£\nLog data \n‚ë§Training \nTables \nPublish canary \n‚ë®Monitoring \nSystem Train \n‚ë¶\nConÔ¨Åg \nSystem ETL\n‚ë•\nExperimentation \nSystem Meta infrastructure \nLooper system \nFig. 2. Data flow in the Looper platform. Figure 3 expands the left side.\n4\n\nLooper: an end-to-end ML platform for product decisions KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\nTraining TableFeaturesLabelsVersionpage_fans: 123description_emb: [1.2, 2.0, ...]click: truerating: 4.02page_fans: 123click: true1Strategy Blueprint (version 1)Strategy Blueprint (version 2)Feature ConÔ¨ÅgLabel ConÔ¨Åg‚óèclick (binary)‚óèrating (Ô¨Çoat)Model ConÔ¨ÅgModel: neural_networkLayers: [256, 128]Policy ConÔ¨Ågvalue(x) = click(x) + 2.0 * log(rating(x))page_idContextpage_featuresGroupSIF_text_embeddingTransformdescriptiondescription_embTraining SystemAPIExperimentationSystemLoggingTrained Model[blueprint v2]InferenceFilterBlueprintVersionTo Feature StorePolicyEvaluatorAdaptive ExperimentationSystemA/B testresultsGeneratedblueprints &experimentsTarget product metrics\n‚ë†‚ë°‚ë¢‚ë£‚ë§\nFig. 3. The strategy blueprint and how it controls different aspects of the end-to-end model lifecycle. Continuation of Figure 2.\nnull is returned before a model is available (to trigger default be-\nhavior). User-defined decision _idties each decision with observa-\ntion(s) logged later (II); it may be randomly generated for clients to\nuse.application _context is a dictionary representation of the ap-\nplication context (Section 3.1), e.g., with the user ID (used to retrieve\nadditional user features), current date/time, etc.\nII.logObservations(decision _id,observations)logs labels\nfor training proxy ML task(s), where decision _idmust match a\nprior getDecision call. Observations capture users‚Äô interactions,\nresponses to a decision (e.g., clicks or navigation actions), or envi-\nronmental factors such as compute costs.\nThough deceptively simple in product code, this design fully sup-\nports the MLOps needs of the platform. We separately walk through\nthe online (inference) and offline (training) steps of the pipeline in\nFigure 2. 1Product code initializes the Looper client API with\none of the known strategies registered in the UI. getDecision()is\nthen called with the decision _idandapplication _context .2\nLooper client API retrieves a versioned configuration (the ‚Äústrategy\nblueprint‚Äù, Section 3.3) for the strategy to determine the features,\nthe model instance, etc. The exact version used may be controlled\nthrough an external experimentation system. 3The client API\npasses the application context to the Meta feature store (Section 3.3),\nwhich returns a complete feature vector. 4The client API passes\nthe feature vector and production model ID to a distributed model\npredictor system (cf. [ 47]), which returns proxy task predictions to\nthe client. Then, the client API uses a decision policy (Section 2.1) to\nmake the final decision based on the proxy predictions. Decision poli-\ncies are configured in a domain-specific language (DSL) using logic\nand formulas. 5Asynchronously, the anonymized feature vector\nand predictions are logged to a distributed online joining system (c.f.[5]), keyed by the decision ID and marked with a configurable and\nrelatively short TTL (time-to-live). The logObservations API also\nsends (from multiple request contexts) logs to this system. Com-\nplete ‚Äúrows\" with matching features and observations are logged to\na training table, with retention time set per data retention policies.\nThe remaining steps are performed offline and asynchronously.\n6Delayed and long-term observations are logged in a table\nand then joined offline via Extract, Transform, and Load (ETL)\npipelines [ 6]. These pipelines perform complex data operations\nsuch as creating MDP sequences for reinforcement learning. The\nlogged features, predictions, and observations are sent for logging\nand real-time monitoring as per Section 2.2. 7An offline training\nsystem [ 22] retrains new models nightly or when sufficient data are\navailable, addressing concerns from Section 3.1. 8Trained models\nare published to the distributed predictor for online inference. 9\nModels are then registered for canarying (Section 3.1). 10A canary\nmodel that outperforms the prior model is promoted to production\nand added to the loop configuration.\n3.3 Product optimization with strategy blueprints\nThe end-to-end nature of the Looper platform brings its own set\nof challenges regarding data and configuration management in the\nsystem. Existing ML management solutions [ 49] primarily focus on\nmanaging or versioning of data and models, which is insufficient\nin covering the full lifecycle of smart strategies. In this section we\nintroduce the concept of a strategy blueprint , a version-controlled\nconfiguration that describes how to construct and evaluate a smart\nstrategy. Blueprints are immutable, and modifications (typically\nthrough a GUI) create new versions that can be compared in produc-\ntion through an online experimentation platform, allowing for easy\n5\n\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Markov and Wang, et al.\nrollback if needed. The strategy blueprint (Figure 3) controls four\naspects of the ML model lifecycle and captures their cross-product:\nFeature configuration. Modern ML models can use thousands of\nfeatures and computed variants, which motivates a unified repos-\nitory ‚Äî a feature store ‚Äî usable in model training and real-time\ninference [ 26,39]. Features within a feature group are associated\nwith the same application context (e.g., a Web page id) and are com-\nputed together. Feature variants are produced by feature transforms ,\ne.g., pre-trained or SIF [ 8] text embeddings. The Looper blueprint\nleverages feature stores for feature management and tracks ( ùëñ) fea-\nture groups via a computational graph and ( ùëñùëñ) downstream feature\ntransforms. Blueprint modifications often try to improve model\nquality by experimenting with new features.\nLabel configuration controls how ML objectives are proxied by\nclicks, ratings, etc, as per Section 2.1. Faithful proxies for product\nmetrics are hard to find [ 48], hence experimentation with label sets.\nModel configuration helps product teams explore model architec-\nture tradeoffs (DNNs, GBDTs, reinforcement learning). The blue-\nprint only specifies high-level architecture parameters, while lower-\nlevel hyperparameters (e.g., learning rates) are delegated to AutoML\ntechniques invoked by the training system (Section 2.2).\nPolicy configuration controls how raw objective predictions are\ntranslated into decisions (Section 2.1). It uses a lightweight domain\nspecific language (DSL). Figure 3 illustrates a ranking decision,\nwhere the click and rating objectives are weighted and combined to\ngenerate a single score per candidate. Smart strategies often need\nto optimize the importance weights embedded in decision policies.\nBlueprints capture compatibility between versions, e.g., the train-\ning pipeline for ver. ùê¥may use data from ver. ùêµif features and labels\ninùê¥are subsets of those in ùêµ. Tagging each training row with the\noriginating blueprint version enables data sharing between versions.\nFigure 3 illustrates the lifecycle of a blueprint. From left to right:\n1An experimentation system enables different blueprint versions\nto be served across the user population to facilitate A/B testing\n(optionally, in concert with a ‚Äúblueprint optimizer\", described later\nbelow). 2The client API uses the blueprint feature configura-\ntion to obtain a complete feature vector from the feature store. 3\nCompleted training examples are logged to training tables, tagged\nwith the originating blueprint version. 4The training system\nfilters data by compatible version and executes the pipeline per\nthe blueprint‚Äôs feature, label, and model configurations. The policy\nconfiguration may be needed as well for more sophisticated model\ntypes (reinforcement learning). 5Trained models are published in\nthe blueprint version. So, the client API uses only models explicitly\nlinked to its served blueprint version. To generate the final product-\nfacing decision, the client also uses the policy configuration.\nVertical optimizations with the blueprint abstraction capture de-\npendencies between the four configuration types. Since long-term\nproduct metrics are rarely available in closed form, such an opti-\nmization requires ( ùëñ) a sequence of A/B tests that evaluate product\nmetrics and ( ùëñùëñ) parameter adjustment between these tests. Given\nthat each A/B test (experiment) can take significant time and impact\nmany end-users, very few A/B tests can be used in practice. This\ncalls for (multi-objective) Bayesian optimization: to tune parame-\nters in a blueprint and optimize product metrics w/o closed-formrepresentation, Looper leverages an adaptive experimentation plat-\nform [ 9], see Section 4.2 for details. Product outcomes often improve\neven by just tuning weights in the policy configuration, e.g., for\nrecommendation scores and reward shaping.\nBookkeeping for product groups using Looper is performed in\nterms of use cases tied to an application context and an ML task (Sec-\ntion 3.1). Multiple candidate configurations are maintained to train\nand evaluate candidate model instances and define decision policies .\nWhile only one blueprint (and model) per use case is in production,\nmany model instances may be undergoing shadow evaluation.\n3.4 Platform specializations\nThe core platform (Sections 3.2 and 3.3) goes a long way to ad-\ndress the challenges listed in the Introduction. However, additional\nstructures are needed because ( ùëñ) product-metric evaluation and\noptimization have serious blind spots, while ( ùëñùëñ) some classes of\napplication are cumbersome to support. Platform specializations\nthat address these deficiencies add significant value to the platform.\nIntegrated experiment optimizations. Even when a product met-\nric is approximated well by an ML loss function, the correlations cap-\ntured by the model might not lead to causal product improvements.\nHence, A/B testing estimates the average treatment effect (ATE) of\nthe change across product users. Shared repositories of product\nmetrics are common [ 10,28,53], and product variants are systemat-\nically explored by running many concurrent experiments [ 9]. While\ndealing with non-stationary measurements, balancing competing\nobjectives, and supporting the design of sequential experiments [ 9],\na common challenge with A/B tests is to find subpopulations where\ntreatment effects differ from the global ATE ‚Äì heterogeneous treat-\nment effects (HTE). Common neglect for HTEs in A/B testing leaves\nroom for improvement [ 10,12] [50], likely delivering suboptimal\ntreatments. The Looper platform and its support for A/B testing\ndramatically simplify HTE modeling on the Meta online experimen-\ntation platform, and help deploying treatment assignments based\non HTE estimates.\nIn an initial training phase, Looper‚Äôs getDecision()API acts as\na drop-in replacement for the standard A/B testing API, and falls\nthrough to a standard randomized assignment while still logging\nfeatures for each experiment participant. Then, metrics from the\nstandard A/B testing repertoire help derive the treatment outcome\n(observations) for each participant, and the Looper platform trains\nspecialized HTE models (meta-learners such as T-, X-, and S- learners\n[30]). In a final step, the HTE model predictions can be used in a\ndecision policy to help getDecision()make intelligent treatment\nassignments and measurably improve outcomes compared to any\nindividual treatment alone. In this scenario, the best HTE estimate\nfor a given user selects the actual treatment group. Our integration\nlinks Looper to an established experiment optimization system [ 9]\nand creates synergies discussed in Section 4.2. A further extension\nrelaxes the standard A/B testing contract to support fully dynamic\nassignments and enables reinforcement learning [7].\nLooper for ranking. The getDecision()+logObservations()\nAPI is general enough to implement simple recommendation sys-\ntems, but advanced systems need finer support. Higher-ranked items\n6\n\nLooper: an end-to-end ML platform for product decisions KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\n0100200300400\nfrequencyML task\nBCRegressionCBMCMTRL\n0 500\nfrequencymodel typeGBDTSparseNNRLPytorch Mobile\nFig. 4. ML tasks and model types on our platform. On the outer plot ‚Äî\nML tasks: Reinforcement Learning (RL), multitask (MT), multiclass (MC)\nand binary classification (BC), contextual bandit (CB), regression. Multitask\nmodels blend classification or regression sub-tasks. On the inner plot ‚Äî\nmodel types: PyTorch Mobile, RL, NN, Gradient Boosted Decision Trees.\nare more often chosen by users, and this positional bias can be han-\ndled (in the API) by including the displayed position as a special in-\nput during training [ 17]. To derive a final priority score for each item,\nthe multiple proxy task predictions are often combined through a\nweighted combination function [ 55]. Recommender systems learn\nfrom user feedback, as long ass lesser-explored items are occassion-\nally included among top results (the explore/exploit tradeoff [ 54]). A\nspecialized Looper ranking system abstracts these considerations in\na higher-level API ( getRanking ) allowing the ordering of an entire\nlist of application contexts, and also allows recording of display-time\nobservations such as the relative screen position of each item.\n4 PRODUCTION DEPLOYMENT\nLooper supports real-time inference with moderate-sized models\nto improve various aspects of software systems. These models\nare trained and deployed quickly and maintained on the platform,\nwhereas our two-call RPC API (Section 3.2) decouples platform code\nfrom application code. Deployed in production at Meta during the\nentire year 2021, Looper improved product metrics, made use of\ncompute resources more efficient, and streamlined maintenance. To\nenhance the reproducibility of our work, we describe empirical ob-\nservations and prominent applications, outline adoption and impact,\nand summarize adopter survey results (in Appendix B). Resource\nestimates are in \"server\" units (we do not use GPUs). For smart strate-\ngies that learn from end-users, performance and impact inevitably\ndepend on the user base and product metrics infrastructure. Hence,\nwe report Looper impact as a fraction of product teams‚Äôs half-year\nresults. Resource savings are reported as percent vs. baselines.\n4.1 Statistics for ML tasks and models in Looper\nGiven nonstationary application environments, Looper retrains\nmodels on the day when 20% fresh training data become available (or\non a set schedule). Choosing appropriate ML models requires trad-\ning off performance metrics with resource usage, inference latency,\nand configuration effort. SVM packages struggle with multimodal\ndata and scale poorly to voluminous data. DNNs scale to 1B+ data\nrows, handle sparse features, but use more memory than simpler\nmodels. DNNs are sensitive to architecture configuration and arecurrently less explored for tabular data, making configuration chal-\nlenging without ML expertise. Gradient-Boosted Decision Trees\n(GBDT, XGBoost) are compact and robust, handle multimodal tabu-\nlar data naturally (including sparse features and missing values), do\nnot require architecture search or GPUs, and scale to 100M rows.\nTypical inference latency with GBDT and XGBoost is in low single\nùëöùë†for server-side models and 1-2 ms for (smaller) mobile models.\nInference with DNN-based models, especially those using latent\nembeddings, is an order of magnitude slower. That‚Äôs why the mean\ninference latency (10ms) across Looper use cases is greater than\nthe median (2ms). 90%le and 99%le latencies are 20ms. Figure 4\nsummarizes ML tasks deployed on Looper and the models selected\nfor them. Figure 6 shows that models typically use 50-200 features,\nand most features are used by many models. Feature extraction la-\ntency has median 45ms and mean 120ms. Latencies for extr ‚Ä†acting\nsynthetic/engineered features are greater (90%le and 99%le is 240\nms). For a broader picture, Figure 5 summarizes 29 utilization rates\nfor system resources. In particular, feature extraction tends to be a\nbottleneck. This data is further discussed in Appendix A and used\nto optimize resource utilization in Looper.\n4.2 Application deep dive ‚Äì personalized experiments\nSection 3.4, while focusing on our platform architecture, outlined a\nspecial application of smart strategies ‚Äî embedding them into the\nstandard experimentation framework to automate the personaliza-\ntion of A/B treatments when HTEs are detected and captured by ML\nmodels. As A/B testing APIs are common and accessible [ 10,53],\nLooper‚Äôs integration with the standard experimentation APIs makes\ntraining and deploying personalized smart strategies as easy as\nchanging one or two lines of code by product teams. In practice,\nexposing smart strategies through such APIs brings several benefits:\n‚Ä¢Client code and the learning curve are simplified by repur-\nposing the decision API as the A/B testing API.\n‚Ä¢Dataset preparation and modeling flow are automated for\nthe task of optimizing metric responses based on end-users\nexposed to each treatment. Metric responses can be auto-\nmatically sourced from the experimentation measurement\nframework without manual labeling.\n‚Ä¢Product metrics are traded off across many strategies, offline\nand online, via multi-objective optimization (MOO) [9, 20].\n‚Ä¢Smart strategies are automatically compared against all base-\nline treatments (i.e., A or B), making the tradeoff between\nmetric impact and costs explicit.\nPreviously such experiment optimization needed dedicated engi-\nneering resources. Now the tight integration of the Looper plat-\nform with the experimentation framework allows product engineers\nquickly evaluate a smart strategy and optimize its product impact\nin several weeks . With automatic MOO, engineers find tradeoffs ap-\npropriate to a given product context. For example, during a server\ncapacity crunch, one team traded a slight deterioration in a product\nmetric for a 50% resource savings. Predicating product deployment\non such experiments creates safeguards against ML models that gen-\neralize poorly to live data. This also helps tracking product impact.\nFor example, a user authentication use case [ 7] reduced SMS cost\nby 5% while remaining neutral for engagement metrics.\n7\n\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Markov and Wang, et al.\n4.3 Application deep dive ‚Äì data prefetching\nOnline applications strive to reduce response latency for user inter-\nactions. Optimized resource prefetching based on user history helps\nby proactively loading application data. Modern ML methods can\naccurately predict the likelihood of data usage, minimizing unused\nprefetches. Our Looper platform supports prefetching strategies\nfor many systems within Meta, often deeply integrated into the\nproduct infrastructure stack. For example, Meta‚Äôs GraphQL [ 14]\ndata fetching subsystem uses our platform to decide which prefetch\nrequests to service, saving both client bandwidth and server-side\nresources. It yields around 2% compute savings at peak server load.\nAs another example, Meta‚Äôs application client for lower-end devices\n(with a ‚Äúthin-client‚Äù server-side rendering architecture [ 43]) also\nuses our platform to predictively render entire application screens.\nOur automated end-to-end system helps deploying both models\nand threshold-based decision policies then tune them for individual\nGraphQL queries or application screens, with minimal engineer-\ning effort. Based on numerous deployed prefetch models, we have\nalso developed large-scale modeling of prefetching. User-history\nmodels have proven to be helpful for this task [ 51]; building up\non this idea, we created application-independent vector embed-\ndings based on users‚Äô surface-level activity. To accomplish this, we\ntrain a multi-task auto-regressive neural-network model to predict\nhow long a user will stay in each application surface (e.g., news\nfeed, search, notifications), based on a sequence of (application\nsurface, duration) events from the user‚Äôs history. As is common\nin CV and NLP, intermediate-layer outputs of this DNN predict\nprefetch accesses well and make specialized features unnecessary.\nOptimized prefetching illustrates how secondary, domain-specific\nplatforms are enabled by the core Looper platform; infrastructure\nteams only need to wire up the prediction and labeling integration\npoints while Looper provides full ML support.\n4.4 Adoption and impact\nSeveral internal vertical platforms at Meta [ 26] compete for a rich\nand diverse set of applications. Product teams sometimes relocate\ntheir ML models to a platform with greater advantages, while a few\nhigh-value applications are run by dedicated infrastructure teams.\nLooper was chosen and is currently used by 90+ product teams at\nMeta. On any day in 2021, these teams deployed 440-1K models that\nmade 4-6 million decisions per second. Application use cases fall\ninto five categories in decreasing order of usage (Figures 5 and 7):\n‚Ä¢Personalized Experience is tailored based on the user‚Äôs\nengagement history. For example, we display a new feature\nprominently only to those likely to use it.\n‚Ä¢Ranking orders items to improve user utility, e.g., to person-\nalize a feed of candidate items for the viewer.\n‚Ä¢Prefetching/precomputing data/resources based on pre-\ndicted likelihood of usage (Section 4.3).\n‚Ä¢Notifications/prompts can be gated on a per-user basis, and\nsent only to users who find them helpful.\n‚Ä¢Value estimation predicts regression tasks, e.g., latency or\nmemory usage of a data query.\nThe impact of ML performance on product metrics varies by appli-\ncation. For a binary classifier, increasing ROC AUC from 90% to 95%\nFig. 5. Resource utilization rate by resource type and application category\n(see Section 4.4). The Service API category includes API calls other than\nfeature extraction and prediction service.\nmight not yield large product gains when such decisions contribute\nlittle to product metrics if bottlenecks lie elsewhere. But increasing\nROC AUC from 55% to 60% is impactful when each percent trans-\nlates into tangible resource savings or other metrics, as it would\nbe for online payment processing. Looper use cases contributed to\ncompute savings (server utilization), user engagement (e.g., daily\nactive users) and other top-line company reporting metrics. Many\nproduct teams at Facebook and Instagram adopted Looper without\nadditional staffing, and it is common for Looper to contribute 20-40%\nof improvements to product goal metrics. In several cases, Looper\nhelped product teams outperform their goals by over 2x.\n5 CONCLUSIONS\nWe outline opportunities to embed data-driven self-optimizing smart\nstrategies for product decisions into software systems, so as to\nenhance user experience, optimize resource utilization, and support\nnew functionalities. We describe the deployment of smart strategies\n(at an unprecedented scale) through software-centric ML integration\nwhere decision points are intercepted and data is collected through\nAPIs [ 2]. This process requires infrastructure and automation to\nreduce operational mistakes and maintain ML development velocity.\nOur ML platform Looper addresses the complexities of product-\ndriven end-to-end ML systems and facilitates at-scale deployment\nof smart strategies through technical insights, platform-level ab-\nstractions, a novel architecture, the use of Bayesian optimization,\nand interfaces with an adaptive experimentation system. As an im-\nportant simplification, inference input processing matches that for\ntraining. The Looper product RPC API is simplified down to two\ncalls. The Looper platform treats end-to-end ML development more\nbroadly than prior work [ 37,52], providing extensive support for\nproduct impact evaluation of smart strategies via causal inference.\nLooper learns heterogenous treatment effects (HTE) from product\nevaluation data by repurposing the Looper RPC API as a drop-in\nreplacement for a standard A/B testing API. Vertical optimizations\nwith long-term product objectives are enabled by our strategy blue-\nprint abstraction and the use of Bayesian optimization.\nAs observed during production deployment in 2021, Looper offers\nimmediate, tangible benefits in terms of data availability, easy con-\nfiguration, judicious use of available resources, reduced engineering\neffort, and ensuring product impact. It makes smart strategies easily\naccessible to product engineers at large scale and enables product\nteams to build, deploy and improve ML-driven capabilities in a self-\nserve fashion without ML expertise. We observed product teams\n8\n\nLooper: an end-to-end ML platform for product decisions KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\nlaunch smart strategies within their products in one month (Ap-\npendix B). The lower barriers to entry and faster deployment lead\nto more pervasive use of ML to optimize user experience in new\nproducts and old products not designed with ML in mind. Support\nfor prefetching and personalized A/B testing have been in demand,\nwhereas end-to-end management enables holistic resource account-\ning and optimization [ 52]. The overall impact in product metrics\nand resource-efficiency is substantial. We also provide empirical\ninsights into usage by resource types and application category.\nLong-term benefits of our platform approach include effort and\nmodule reuse, end-to-end reproducibility, consistent reporting, reli-\nable maintenance, and being able to upgrade ML libraries and offer\nnew ML model types with consistent interface. Successful Looper\nadopters often launch additional data-driven smart strategies, and\nthis virtuous cycle encourages designing SW systems with built-in\nML to enhance user experience and adaptation to the environment.\nREFERENCES\n[1]Mart√≠n Abadi et al .2016. TensorFlow: A System for Large-Scale Machine Learning.\nInOSDI . 265‚Äì283.\n[2]Alekh Agarwal et al .2016. Making contextual decisions with low technical debt.\narXiv:1606.03966 (2016).\n[3]Deepak Agarwal, Bee-Chung Chen, and Pradheep Elango. 2009. Explore/exploit\nschemes for web content optimization. In ICDM 2009 . 1‚Äì10.\n[4]Saleema Amershi et al .2019. Software engineering for machine learning: A case\nstudy. In ICSE-SEIP . 291‚Äì300.\n[5]Rajagopal Ananthanarayanan et al .2013. Photon: Fault-Tolerant and Scalable\nJoining of Continuous Data Streams. In ACM SIGMOD Int‚Äôl Conf. on Management\nof Data (SIGMOD ‚Äô13) . ACM, 577‚Äì588. https://doi .org/10 .1145/2463676 .2465272\n[6]Anonymous. 2021. ETL vs ELT: Must Know Differences. https://\nwww .guru99 .com/etl-vs-elt .html\n[7]Pavlos Athanasios Apostolopoulos et al .2021. Personalization for Web-based\nServices using Offline Reinforcement Learning. arXiv:2102.05612 (2021).\n[8]Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A Simple but Tough-to-Beat\nBaseline for Sentence Embeddings. ICLR (2017).\n[9]Eytan Bakshy et al .2018. AE: A domain-agnostic platform for adaptive experi-\nmentation. NeurIPS 2018 Systems for ML Workshop .\n[10] Eytan Bakshy, Dean Eckles, and Michael S Bernstein. 2014. Designing and de-\nploying online field experiments. In WWW‚Äô 14 . 283‚Äì292.\n[11] Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin\nLetham, Andrew Gordon Wilson, and Eytan Bakshy. 2020. BoTorch: A Framework\nfor Efficient Monte-Carlo Bayesian Optimization. In NeurIPS 33 .\n[12] Alex Beutel, Ed H Chi, Zhiyuan Cheng, Hubert Pham, and John Anderson. 2017.\nBeyond globally optimal: Focused learning for improved recommendations. In\nWWW‚Äô 17 . 203‚Äì212.\n[13] Eric Breck, Shanqing Cai, Eric Nielsen, Michael Salib, and D. Sculley. 2017. The ML\nTest Score: A Rubric for ML Production Readiness and Technical Debt Reduction.\nInIEEE Big Data .\n[14] Lee Byron. 2015. GraphQL: A data query language. https://engineering .fb.com/\n2015/09/14/core-data/graphql-a-data-query-language\n[15] Victor Carbune, Thierry Coppey, Alexander Daryin, Thomas Deselaers, Nikhil\nSarda, and Jay Yagnik. 2018. SmartChoices: Hybridizing programming and ma-\nchine learning. arXiv:1810.00619 (2018).\n[16] P. Covington, J. Adams, and E. Sargin. 2016. Deep neural networks for Youtube\nrecommendations. In RecSys .\n[17] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. 2008. An experi-\nmental comparison of click position-bias models. In WSDM 2008 . 87‚Äì94.\n[18] Marc D‚ÄôArcy. 2021. Opinion: The 3 Post-COVID Trends Empowering People and\nShaping the Future. https://adage .com/article/opinion/opinion-3-post-covid-\ntrends-empowering-people-and-shaping-future/2342861\n[19] Samuel Daulton et al .2019. Thompson sampling for contextual bandit problems\nwith auxiliary safety constraints. arXiv:1911.00638 (2019).\n[20] Samuel Daulton, Maximilian Balandat, and Eytan Bakshy. 2021. Parallel Bayesian\nOptimization of Multiple Noisy Objectives with Expected Hypervolume Improve-\nment, In NeurIPS 34. arXiv:2006.05078 .\n[21] Ben Dickson. 2021. Why machine learning strategies fail. https://\nventurebeat .com/2021/02/25/why-machine-learning-strategies-fail/\n[22] Jeffrey Dunn. 2016. Introducing FBLearner Flow: Facebook‚Äôs AI back-\nbone. https://engineering .fb.com/2016/05/09/core-data/introducing-fblearner-\nflow-facebook-s-ai-backbone[23] Qing Feng, Benjamin Letham, Hongzi Mao, and Eytan Bakshy. 2020. High-\ndimensional contextual policy search with unknown context rewards using\nBayesian optimization. NeurIPS 33 (2020).\n[24] Jason Gauci et al .2018. Horizon: Facebook‚Äôs Open Source Applied Reinforcement\nLearning Platform. arXiv:1811.00260 (2018).\n[25] Udit Gupta et al .2020. The Architectural Implications of Facebook‚Äôs DNN-based\nPersonalized Recommendation. HPCA (2020), 488‚Äì501. arXiv:1906.03109\n[26] Kim Hazelwood et al .2018. Applied machine learning at Facebook: A datacenter\ninfrastructure perspective. In HPCA 2018 . IEEE, 620‚Äì629.\n[27] Jeremy Hermann and Mike Del Balso. 2017. Meet Michelangelo: Uber‚Äôs Ma-\nchine Learning Platform. https://eng .uber.com/michelangelo-machine-learning-\nplatform/\n[28] Ron Kohavi, Roger Longbotham, Dan Sommerfield, and Randal M Henne. 2009.\nControlled experiments on the Web: survey and practical guide. Data mining and\nknowledge discovery 18, 1 (2009), 140‚Äì181.\n[29] Tim Kraska et al .2017. The Case for Learned Index Structures. CoRR (2017).\narXiv:1712.01208\n[30] S√∂ren R. K√ºnzel et al .2019. Metalearners for estimating heterogeneous treatment\neffects using machine learning. PNAS 116, 10 (Feb 2019), 4156‚Äì4165.\n[31] Adam Daniel Laud. 2004. Theory and application of reward shaping in reinforcement\nlearning . UIUC.\n[32] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-\nbandit approach to personalized news article recommendation. In WWW . 661‚Äì\n670.\n[33] Shen Li et al .2020. PyTorch Distributed: Experiences on Accelerating Data Parallel\nTraining. In VLDB , Vol. 13(12).\n[34] Hongzi Mao et al .2020. Real-world video adaptation with reinforcement learning.\narXiv:2008.12858 (2020).\n[35] Lester James Miranda. 2021. Towards data-centric machine learning: a short re-\nview. (2021). https://ljvmiranda921 .github .io/notebook/2021/07/30/data-centric-\nml/\n[36] P. Molino, Y. Dudin, and S. S. Miryala. 2019. Ludwig: a type-based declarative\ndeep learning toolbox. arxiv:1909.07930 (2019).\n[37] P. Molino and C. R√©. 2021. Declarative Machine Learning Systems. ACM Queue\n19 (2021). Issue 3.\n[38] Maxim Naumov et al .2019. Deep Learning Recommendation Model for Personal-\nization and Recommendation Systems. CoRR abs/1906.00091 (2019).\n[39] Laurel J. Orr et al .2021. Managing ML Pipelines: Feature Stores and the Coming\nWave of Embedding Ecosystems. CoRR (2021). arXiv:2108.05053\n[40] Andrei Paleyes, Raoul-Gabriel Urma, and Neil D Lawrence. 2020. Challenges in\ndeploying machine learning: a survey of case studies. arXiv:2011.09926 (2020).\n[41] Christopher R√© et al .2019. Overton: A data system for monitoring and improving\nmachine-learned products. arXiv:1909.05372 (2019).\n[42] Pau Rodr√≠guez, Miguel A Bautista, Jordi Gonz√†lez, and Sergio Escalera. 2018.\nBeyond One-hot Encoding: lower dimensional target embedding. arXiv:1806.10805\n(2018).\n[43] Gautam Roy. 2016. How we built Facebook Lite for every Android phone and net-\nwork. https://engineering .fb.com/2016/03/09/android/how-we-built-facebook-\nlite-for-every-android-phone-and-network\n[44] Ram Sagar. 2021. Andrew Ng Urges ML Community To Be More Data-\nCentric. https://analyticsindiamag .com/big-data-to-good-data-andrew-ng-\nurges-ml-community-to-be-more-data-centric-and-less-model-centric/\n[45] Nithya Sambasivan et al .2021. \"Everyone wants to do the model work, not the\ndata work\": Data Cascades in High-Stakes AI. SIGCHI, ACM (2021).\n[46] David Sculley et al .2015. Hidden technical debt in machine learning systems.\nNIPS 28 (2015), 2503‚Äì2511.\n[47] Jonathan Soifer et al .2019. Deep Learning Inference Service at Microsoft. In\nUSENIX Conf. Operational Machine Learning (OpML) . USENIX, 15‚Äì17. https:\n//www .usenix .org/conference/opml19/presentation/soifer\n[48] Gregory J. Stein. 2019. Proxy metrics are everywhere in machine learning. http:\n//cachestocaches .com/2019/1/proxy-metrics-are-everywhere-machine-lea\n[49] M. Vartak and S. Madden. 2018. MODELDB: Opportunities and Challenges in\nManaging Machine Learning Models. IEEE Data Eng. Bull. 41, 4 (2018), 16‚Äì25.\n[50] S. Wager and S. Athey. 2018. Estimation and inference of heterogeneous treatment\neffects using random forests. J. Amer. Stat. Assoc. 113, 523 (2018), 1228‚Äì1242.\n[51] Hanson Wang, Zehui Wang, and Yuanyuan Ma. 2019. Predictive Precompute with\nRecurrent Neural Networks. arXiv:1912.06779 (2019).\n[52] Carole-Jean Wu et al .2021. Sustainable AI: Environmental Implications, Chal-\nlenges and Opportunities. arXiv:2111.00364 [cs.LG]\n[53] Ya Xu et al .2015. From infrastructure to culture: A/B testing challenges in large\nscale social networks. In KDD . 2227‚Äì2236.\n[54] Dragomir Yankov, Pavel Berkhin, and Lihong Li. 2015. Evaluation of explore-\nexploit policies in multi-result ranking systems. arXiv:1504.07662 (2015).\n[55] Zhe Zhao et al .2019. Recommending what video to watch next: a multitask\nranking system. In RecSys ‚Äò19 . 43‚Äì51.\n9\n\nKDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Markov and Wang, et al.\n0255075100125150\nnum models per feature110100100010000num features05001000050100\nFig. 6. Histograms showing how many Looper models use a given feature\n(outer) and features per model (inner).\nA RESOURCE UTILIZATION AND ITS OPTIMIZATION\nSmart strategies tend to provide significant benefits but sometimes\nneed serious computational resources, Therefore product deploy-\nment requires judicious resource management. Looper is deployed\nin numerous and diverse applications at Meta, some of which opti-\nmize performance of other systems and some enhance functionality\n(Figure 7). Such economies-of-scale infrastructure enables resource\nreuse and load-balancing. Figure 5 shows that different use cases\nexhibit different model-lifecycle bottlenecks, with feature extraction\ndrawing the largest share of resources for demanding use cases.1\nBased on this trend, we developed a feature reaping optimization\nthat removes unimportant features. This optimization estimates the\nimportance of individual features, removes features in groups, then\nchecks the results by training a reduced model and evaluating it.\nWhen we deployed feature reaping in production, it and provided\noverall 11% resource-cost savings (10-30% per use case) with no\nadverse product impacts. Figure 8 illustrates the resource savings\n(28%) for one use case via feature reaping. It distinguishes 3 stages\nthat experience constant decision traffic: ( ùëñ) offline model training\nthat runs feature reaping and trains a reduced model, ( ùëñùëñ) the online\nstage that uses the old and the revised (20% traffic only) model to\nevaluate product impact, and ( ùëñùëñùëñ) the production stage that uses only\nthe new model whose performance had been validated by online\nevaluation (which took 2 weeks to collect statistically significant\nresults). Sharing such AutoML optimizations across multiple use\ncases makes our platform competitive with specialized platforms.\nRanking (22%)Personalize Experience\n(27%)Notifs\n(8%)Precompute/ Prefetch\n(22%)Other (15%)Estim\n(6%)\nRanking (32%) Personalize Experience (31%)Notifications/\nPrompts (21%)Precom\n(10%)Other\n(6%)\nFig. 7. Product adoption of smart strategies by use case count (top) and by\nresource consumption (bottom). Resource consumption during training is\ncorrelated with data amount, but resource consumption at inference reflects\ndecision rates in applications, the number of features used by models and\nthe presence of synthetic/engineered features.\n1We note that Looper deploys moderate-complexity models with diverse metadata\nfeatures , whereas advanced deep learning models with homogeneous image pixels, word\nembeddings, etc may exhibit different trends.\nFig. 8. The three phases of resource optimization via feature reaping .\nResource consumption is decreased without adverse product impacts.\nB A SURVEY OF PLATFORM ADOPTERS\nThe article ‚ÄúWhy Machine Learning Strategies Fail‚Äù [ 21] lists com-\nmon barriers to entry:\n‚Ä¢lacking a business case,\n‚Ä¢lacking data,\n‚Ä¢lacking ML talent,\n‚Ä¢lacking sufficient in-house ML expertise for outsourcing,\n‚Ä¢failing to evaluate an ML strategy.\nWhen talking to prospective clients, we advise against using Looper\nwhen there is no need for an end-to-end platform. For example, in a\nKaggle-like environment with well-defined data, the goal is to train\na model to optimize closed-form objectives for model performance.\nA second example is tasks without a clear product metric, such as\nbuilding latent-space embeddings and other self-supervised tasks.\nYet another reason not to use Looper is the efficiency of special-case\nplatforms, e.g., for ranking and image-processing.\nTo clarify when Looper is relevant and to clarify the adoption\nprocess of smart strategies, we interviewed product teams at Meta\nthat adopted our platform and saw product impacts (Figure 7). All\nthe teams had tried heuristic approaches but with poor results,\nhence their focus on ML. Simple heuristics proved insufficient for\nuser bases spanning multiple countries with distinct demographic\nand usage patterns. The following challenges were highlighted: 1\nmanually optimizing parameters in large search spaces, 2figuring\nout the correct rules to make heuristics effective, 3trading off\nmultiple objectives, 4updating heuristic logic quickly, especially\nin on-device code.\nThe spectrum of ML expertise varied across product teams from\nbeginners to experienced ML engineers, and only 15% of teams using\nour platform include ML engineers. For teams without production\nML experience, an easy-to-use ML platform is often the deciding\nfactor for ML adoption, and ML investment continues upon evidence\nof utility. An engineer mentioned that a lower-level ML system had\na confusing development flow and unwieldy debugging. They were\nunable to set up recurring model training and publishing. Looper ab-\nstracts away concerns about SW upgrades, logging, monitoring, etc\nbehind high-level services and unlocks hefty productivity savings.\nFor experienced ML engineers, a smart-strategies platform im-\nproves productivity by automating repetitive time-consuming work:\n10\n\nLooper: an end-to-end ML platform for product decisions KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA\nwriting database queries, implementing data pipelines, setting up\nmonitoring and alerts. Compared to narrow-focus systems, it helps\nproduct developers launch more ML use cases. An engineer shared\nprior experience writing custom queries for features and labels, and\nmanually setting up pipelines for recurring training and model pub-\nlishing without an easy way to monitor model performance and\nissue emergency alerts. Some prospective clients who evaluated our\nplatform chose other ML platforms within our company or stayed\nwith their custom-designed infrastructure. They missed batched\noffline prediction with mega-sized data and needed exceptional per-\nformance possible only with custom ML models. These issues can\nbe addressed with additional platform development efforts.\nSuccessful platform adopters configured initial ML models in two\ndays and started collecting training data. Training the model using\nproduct feedback and revising it over 1-2 weeks enabled online prod-\nuct experiments that take 2-4 weeks. HTE analysis and impact opti-\nmization take 1-3 weeks. Product launch can occur 1-3 months after\ninitial data collection. Among platform adopters, experienced engi-\nneers aware of ML-related technical debt and risks [ 2,21,40,45,46]\nappreciated the built-in support for recurring training, model pub-\nlishing, data visualization, as well as monitoring label and feature\ndistributions over time with data-drift alerts. Also noted was the\ncanarying mechanism for new models (Section 3.2). Surprisingly\nimportant was helping adopters share model insights (such as fea-\nture importance) with leadership and product managers. Among\npossible improvements, adopters mentioned development velocity.\nC IMPROVING PRODUCT-DEVELOPMENT VELOCITY\nAs a platform used primarily by product engineers, Looper aims to\nreduce friction while users modify parameters and model configu-\nrations. This is accomplished in two ways:\n(1) a GUI to easily edit all Looper parameters, and\n(2) automatic optimization of configuration parameters.\nEvery aspect of configuration is represented in the UI. Each use\ncase has a landing page to manage all related models, data ver-\nsions, and experiments. All parameters including features, labels,\nand current production models are displayed in an editable form\nfor users to modify and immediately push changes to production.\nExperiments can be launched from the use case landing page and\nthe UI allows users to select baseline and experiment models to\nimmediately compare them using integrated A/B tests.\nAll parameters are stored in configuration and this enables auto-\nmatic optimization of parameters. Using the integrated experiment\noptimization system [ 9], several values for a parameter are opti-\nmally selected to immediately test in our integrated A/B experimen-\ntation platform. These values can then be tested against product\nmetrics. Since most parameters required for experimentation are\nalready stored in strategy blueprints, including eventual support\nfor product metrics, it is possible to automatically improve various\nparameter values using the experiment optimization system and\nA/B test to check if the config optimization benefits product, system,\nand resource usage metrics without affecting production models\nand without use case owner input.D DEVELOPMENT EFFORT\nThe Looper platform described in this paper was developed at Meta\nover several years. It uses several types of software infrastructure,\nsuch as databases, horizontal ML platforms, reusable ML models\nand frameworks, product metrics, and support for product A/B test-\ning. The overall design was revised to better adapt to the needs of\napplications. On the other hand, a team of ten experienced software\nengineers should be able to implement our core platform design in\nhalf a year using relevant open-source and/or company infrastruc-\nture. In such development, it is important to focus on representative\nproduct use cases and guide software development within a well-\ndefined scope. Avoiding common problems, rather than developing\ncomprehensive solutions, can reduce time to first application. In\nparticular, the complete chain of custody of data in Looper helps\navoid or reduce many common problems with data quality, such as\ndelayed and missing data, mismatches between training and testing,\netc.\n11",
  "textLength": 68878
}