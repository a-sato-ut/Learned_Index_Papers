{
  "paperId": "550e981d841f76d1d1358f8575303f79ab6c331b",
  "title": "LeaFTL: A Learning-Based Flash Translation Layer for Solid-State Drives",
  "pdfPath": "550e981d841f76d1d1358f8575303f79ab6c331b.pdf",
  "text": "LeaFTL: A Learning-based Flash Translation Layer\nfor Solid-State Drives\nJinghan Sun\nUIUC\njs39@illinois.eduShaobo Li\nUIUC\nshaobol2@illinois.eduYunxin Sunâˆ—\nETH Zurich\nyunsun@student.ethz.ch\nChao Sun\nWestern Digital Research\nchao.sun@wdc.comDejan Vucinic\nWestern Digital Research\ndejan.vucinic@wdc.comJian Huang\nUIUC\njianh@illinois.edu\nABSTRACT\nIn modern solid-state drives (SSDs), the indexing of flash pages is a\ncritical component in their storage controllers. It not only affects\nthe data access performance, but also determines the efficiency\nof the precious in-device DRAM resource. A variety of address\nmapping schemes and optimizations have been proposed. However,\nmost of them were developed with human-driven heuristics.\nIn this paper, we present a learning-based flash translation layer\n(FTL), named LeaFTL, which learns the address mapping to tolerate\ndynamic data access patterns via linear regression at runtime. By\ngrouping a large set of mapping entries into a learned segment, it\nsignificantly reduces the memory footprint of the address mapping\ntable, which further benefits the data caching in SSD controllers.\nLeaFTL also employs various optimization techniques, including\nout-of-band metadata verification to tolerate mispredictions, opti-\nmized flash allocation, and dynamic compaction of learned index\nsegments. We implement LeaFTL with both a validated SSD sim-\nulator and a real open-channel SSD board. Our evaluation with\nvarious storage workloads demonstrates that LeaFTL saves the\nmemory consumption of the mapping table by 2.9 Ã—and improves\nthe storage performance by 1.4 Ã—on average, in comparison with\nstate-of-the-art FTL schemes.\nCCS CONCEPTS\nâ€¢Hardwareâ†’External storage ;â€¢Computer systems orga-\nnizationâ†’Architectures ;â€¢Computing methodologies â†’\nLearning linear models .\nKEYWORDS\nLearning-Based Storage, Flash Translation Layer, Solid-State Drive\n1 INTRODUCTION\nFlash-based SSDs have become an indispensable part in modern\nstorage systems, as they outperform conventional hard-disk drives\n(HDDs) by orders of magnitude, and their cost is close to that of\nHDDs [ 22,30,51,62]. The SSD capacity continues to boost by\nincreasing the number of flash channels and chips with the rapidly\nshrinking process and manufacturing technology [22, 25, 41, 46].\nThe flash translation layer (FTL) is the core component of man-\naging flash memory in SSDs, including address translation, garbage\ncollection (GC), and wear leveling [ 20,66]. The FTL maintains meta-\ndata structures for different functions such as address translation\nâˆ—Work done when visiting the Systems Platform Research Group at UIUC as a research\nintern.and valid page tracking, and caches them in the in-device DRAM\n(SSD DRAM) for improved performance [7, 12, 25].\nAmong these data structures, the address mapping table has\nthe largest memory footprint. In general, the address mapping\ntable can be categorized in three types: page-level mapping, block-\nlevel mapping, and hybrid mapping. Modern SSDs usually use the\npage-level mapping, as it offers the best performance for the flash\npage lookup, and incurs minimal GC overhead, in comparison with\nthe other two mapping schemes [ 20,66]. However, the page-level\nmapping table size is large, as it stores the entry for the LPA-to-PPA\naddress translation for each flash page.\nThe address mapping table significantly affects the performance\nof SSDs, as it not only determines the efficiency of indexing flash\npages, but also affects the utilization of SSD DRAM. Moreover, due\nto the limitations of the cost and power budget in SSD controllers,\nit is challenging for SSD vendors to scale the in-device DRAM\ncapacity [ 12,41]. This challenge becomes even worse with the\nincreasing flash memory capacity in an SSD, as larger capacity\nusually requires a larger address mapping table for indexing.\nTo improve the address mapping and translation for SSDs, vari-\nous optimization schemes have been developed [ 9,25,29,38,39,66].\nHowever, most of them were developed based on human-driven\nheuristics [ 25], and cannot capture dynamic data access patterns\nat runtime. Employing more semantic knowledge into the FTL,\nsuch as GraphSSD [ 44], can improve the data indexing and address\ntranslation, however, it is application specific and complicates the\nmanagement of address mappings [ 7], which does not scale for the\ndevelopment of generic SSDs. In this work, we do not expect that\nwe can obtain application semantics from the host and the SSD con-\ntroller. Instead, we focus on utilizing simple yet effective machine\nlearning (ML) techniques to automate the address mapping table\nmanagement in the SSDs, with the capability of learning diverse\nand dynamic data access patterns.\nTo this end, we propose a learning-based FTL, named LeaFTL, by\nutilizing the piecewise linear regression technique to learn the LPA-\nPPA mappings, and automatically exploiting the data locality of\nvarious data access patterns at runtime. Unlike the state-of-the-art\npage-level mapping, the key idea of LeaFTL is that it can learn the\ncorrelation between a set of LPAs and their mapped PPAs, based\non which it can build a space-efficient index segment, as presented\ninAin Figure 1. Since the learned index segment can be simply\nrepresented with(ğ‘†,ğ¿,ğ¾,ğ¼), where[ğ‘†,ğ‘†+ğ¿]denotes the interval\nof LPAs,ğ¾is the slope of the segment, and ğ¼is the intercept of the\nsegment (see the last diagram in Figure 1), each segment will takearXiv:2301.00072v1  [cs.OS]  30 Dec 2022\n\nJinghan Sun, Shaobo Li, Yunxin Sun, Chao Sun, Dejan Vucinic, and Jian Huang\n30 LPA\nPPA31 32 33 34\n155 156 157 158 15960 62 64 66 68\n200 201 203 204 20580 82 83 84 87\n304 305 306 307 308Index Segment A Index Segment B Index Segment C\nLPAPPA\nABC\nerror bound1 1 1 1 2 2 2 2 2 1 1 3\nFigure 1: An illustrative example of learning LPA-PPA mappings using piecewise linear regression in LeaFTL. It can learn\nvarious patterns of LPA-PPA mappings with guaranteed error bound. Each learned index segment can be represented with\n(ğ‘†,ğ¿,ğ¾,ğ¼), where[ğ‘†,ğ‘†+ğ¿]denotes the interval of LPAs, ğ¾is the slope, and ğ¼is the intercept of the index segment.\nonly 8 bytes (1 byte for ğ‘†andğ¿, 2 bytes for ğ¾, and 4 bytes for ğ¼)\nwith our optimizations (see the details in Â§3). Compared to the on-\ndemand page-level mapping [ 20], the learned segment reduces the\nmapping table size by a factor of ğ‘šâˆ—ğ‘ğ‘£ğ‘”(ğ¿)/8, whereğ‘šis the size\n(8 bytes) of each entry in the on-demand page-level mapping table,\nandğ‘ğ‘£ğ‘”(ğ¿)is the average number of LPA-PPA mappings that can\nbe represented in a learned index segment, ğ‘ğ‘£ğ‘”(ğ¿)is 20.3 according\nto our study of various storage workloads.\nBeyond learning contiguous LPA-PPA mappings, LeaFTL also\nlearns different correlation patterns, such as regular and irregular\nstrided data accesses as shown in Band C, respectively. Unlike\nexisting indexing optimizations based on human-driven heuristics,\nLeaFTL can learn more irregular patterns of LPA-PPA mappings\nwith guaranteed error bound, as shown in C. This enables LeaFTL\nto further condense the address mapping table. Therefore, given a\nlimited DRAM capacity in the SSD controller, LeaFTL can maximally\nutilize the DRAM caching and improve the storage performance.\nFor the worst case like random I/O accesses, LeaFTL will transfer\nthe mapping into single-point linear segments ( ğ¿=0,ğ¾=0, and\nğ¼=ğ‘ƒğ‘ƒğ´ in Figure 1), and its memory consumption will be no more\nthan that of the page-level mapping.\nWith the learned index segments, LeaFTL may occasionally re-\nturn an inaccurate PPA (i.e., address misprediction), which incurs\nadditional flash accesses until the correct PPA is identified. To over-\ncome this challenge, we develop an error-tolerant mechanism in\nLeaFTL. For each flash page access, we use the reverse mapping\nstored in the out-of-band (OOB) metadata of each flash page to\nverify the correctness of the data access. Since the OOB usually has\n64â€“256 bytes [20, 23] , we use it to store the accurate LPAs mapped\nto the neighbor PPAs. Thus, upon an address misprediction, we use\nthe stored reverse mappings to find the correct PPA, avoiding addi-\ntional flash accesses. LeaFTL leverages the intrinsic OOB structure\nto handle address mispredictions and make SSD perfectly-suited\nfor practical learned indexing.\nDue to the intrinsic out-of-place write property of SSDs (see\nÂ§2), the learned index segments will be disrupted by writes and\nGC, and the segments need to be relearned with new LPA-PPA\nmappings. To tolerate these disruptions, the learned segments are\norganized within multiple levels to maintain the temporal order\nin a log-structured manner: the topmost level has the most recent\nsegments, and the lower level stores older segments. The segmentsat the same level are sorted without overlapping. If the new segment\nhas a conflict with an existing segment, the old segment will be\nmoved to the lower level. Therefore, LeaFTL can always identify\nthe latest version of the corresponding LPA-PPA mapping in a top\nlevel of learned index segments. LeaFTL will compact the learned\nsegments periodically to reduce its memory footprint.\nTo further maximize the efficiency of LeaFTL, we coordinate its\nlearning procedure with flash block allocation in the SSD. As flash\nblock allocation decides the distribution of mapped PPAs, LeaFTL\nwill allocate consecutive PPAs to contiguous LPAs at its best effort,\nfor increasing the possibility of learning a space-efficient index seg-\nment. Similar to existing page-level mapping [ 20,23], LeaFTL stores\nthe learned index segments in flash blocks for recovery. Overall,\nwe make the following contributions:\nâ€¢We present a learning-based FTL, it can learn various data access\npatterns and turn them into index segments for reducing the\nstorage cost of the mapping table.\nâ€¢We develop an error-tolerant address translation mechanism to\nhandle address mispredictions caused by the learned indexes,\nwith minimal extra flash accesses.\nâ€¢We preserve the core FTL functions, and enable the coordination\nbetween the learning procedure of the address mapping table\nwith the flash block allocation and GC to maximize the efficiency\nof the learned FTL.\nâ€¢We manage the learned segments in an optimized log-structured\nmanner, and enable compaction to further improve the space\nefficiency for the address mapping.\nWe implement LeaFTL with a validated SSD simulator Wisc-\nSim[27]and evaluate its efficiency with a variety of popular storage\nworkloads. We also develop a system prototype with a real 1TB\nopen-channel SSD to verify the functions of LeaFTL and validate\nits efficiency with real data-intensive applications, such as the key-\nvalue store and transactional database. Our evaluation with the\nreal SSD shows similar benefits as that of the SSD simulator imple-\nmentation. We demonstrate that LeaFTL reduces the storage cost\nof the address mapping in the FTL by 2.9 Ã—on average. The saved\nmemory space benefits the utilization of the precious SSD DRAM,\nand further improves the storage performance by 1.4 Ã—on average.\nWe also show that LeaFTL does not affect the SSD lifetime, and its\n\nLeaFTL: A Learning-based Flash-Translation Layer for Solid-State Drives\nflashflashflashflashFlashFlashFlashFlash\nDRAM\nFlashControllerSSD Controller/FirmwarePCIe InterfaceEmbedded ProcessorInternal BusDRAM ControllerBlock I/O\nFigure 2: The internal system architecture of SSDs.\nlearning procedure introduces negligible performance overhead\nto the storage processor in the SSD controllers. The codebase of\nLeaFTL is available at https://github.com/platformxlab/LeaFTL.\n2 BACKGROUND AND MOTIVATION\nFlash-Based Solid-State Drive. An SSD has three major parts\n(see Figure 2): a set of flash memory packages, an SSD controller\nwith embedded processors, and a set of flash controllers. With the\nnature of NAND Flash, when a free page is written, the page cannot\nbe written again until that page is erased. However, erase operation\nis performed only at a block granularity. As the erase operation is\nexpensive, writes are issued to free flash pages erased in advance\n(i.e., out-of-place write). GC will be performed to clean the stale\ndata. As each flash block has limited endurance, it is important for\nthem to age uniformly (i.e., wear leveling). SSDs have a logical-\nto-physical address mapping table to index flash pages. All these\nfunctions are managed by the FTL in the SSD firmware.\nModern SSD controllers have general-purpose embedded pro-\ncessors (e.g., ARM processors). The processors help with issuing\nI/O requests, translating LPAs to PPAs, and handling GC and wear-\nleveling. SSDs also have limited DRAM capacities to cache the\nmapping table and the application data.\nAddress Mapping Table in the FTL. The address mapping table\nin FTL generally has three types: page-level mapping, block-level\nmapping, and hybrid mapping. The page-level mapping enables di-\nrect LPA-PPA mapping for fast lookup. However, each entry usually\ntakes 8 bytes (4 bytes for LPA, 4 bytes for PPA), and the entire map-\nping table requires large storage space. The block-level mapping\nsignificantly reduces the mapping table size. However, it introduces\nadditional overhead for the page lookup in the flash block. The hy-\nbrid mapping takes advantages of both page-level and block-level\nmapping. It uses log blocks to store new writes, and index them\nwith the page-level mapping. The log blocks will be moved into\ndata blocks that are indexed with block-level mapping. This incurs\nsignificant GC overhead. Therefore, modern SSDs commonly use\nthe page-level mapping scheme.\nMetadata Structures for Flash Management. The FTL usually\nemploys four metadata structures (see Figure 3): (1) the address\nmapping cache ( 1AMC) for caching the address mapping table\nin the SSD DRAM; (2) the global mapping directory ( 2GMD) for\ntracking the locations of the address mapping table pages in the\nAddress Mapping  \nCache (AMC)1Global Mapping  \nDirectory (GMD)2Block V alidity  \nCounter (BVC)3Page V alidity  \nTable (PVT)4\nLPA PPA\n... ...\nLX PY\n... ...LPA PPA\n... ...\nVX PZ\n... ...PBA Counter\n... ...\n... ...\n... ...PBA Bitmap\n... ...\nPB ...\n... ...Data Structures in the FTL of Modern SSDs\nFlash Memory\nData Blocks Address Mapping Blocks Validity BlocksFigure 3: The common data structures in the FTL of SSDs.\nSSD; (3) the block validity counter ( 3BVC) for tracking the number\nof valid pages for each flash block for assisting the GC in the SSD;\nand (4) the page validity table ( 4PVT), which uses bitmaps to\ntrack the valid pages in each flash block. During the GC, the FTL\nwill check the 3BVC to select candidate flash blocks, and migrate\ntheir valid pages to free flash blocks. After that, it will erase these\nselected flash blocks, and mark them as free blocks.\nLimited DRAM Capacity in SSD Controllers. It is hard to provi-\nsion large DRAM inside SSD controllers, due to their hardware con-\nstraints and limited budgets for power and hardware cost [ 12,41,60].\nThus, SSD controllers often use on-demand caching to maintain\nthe recently accessed metadata and data in the SSD DRAM.\nAmong all the metadata structures, the address mapping table\nhas the largest memory footprint. As discussed, 1AMC caches the\nrecently accessed mapping table entries. If a mapping entry is not\ncached, the FTL will locate the corresponding address mapping ta-\nble pages stored in the flash blocks, and place the mapping entry in\nthe 1AMC. As we scale the SSD capacity, the DRAM challenge will\nbecome even worse. To overcome this challenge, various optimiza-\ntions on the mapping table have been proposed [ 9,25,29,31,38,39]\nto improve the utilization of the SSD DRAM. However, most of\nthem cannot automatically capture diverse data access patterns at\nruntime, leaving a large room for improvement.\n3 DESIGN AND IMPLEMENTATION\nTo develop LeaFTL in the SSD controller, we have to overcome the\nfollowing research challenges.\nâ€¢LeaFTL should be able to automatically capture diverse data\naccess patterns, and generate memory-efficient address mapping\n(Â§3.1,Â§3.2,Â§3.3, andÂ§3.4).\nâ€¢LeaFTL may incur address mispredictions, which could incur\nadditional flash accesses. LeaFTL should be tolerant of errors and\nhave low misprediction penalty ( Â§3.5).\nâ€¢LeaFTL should work coordinately with other core FTL functions\nthat include GC and wear leveling ( Â§3.6).\nâ€¢LeaFTL should be lightweight and not incur much extra overhead\nto storage operations ( Â§3.7,Â§3.8 andÂ§3.9).\n\nJinghan Sun, Shaobo Li, Yunxin Sun, Chao Sun, Dejan Vucinic, and Jian Huang\n(a) Precise Linear ApproximationÂ \n(b) Inaccurate Linear ApproximationÂ \nFigure 4: Visualization of learned index segments.\n1 2 4 8 16 32 64 128 256 512 1024 2048\nLength of Learned Segments020406080100Percentage of Segments (%)\n=0, #Segments=5540\n=4, #Segments=4267\n=8, #Segments=3718\nFigure 5: Aggregated distribution of learned segments.\n3.1 Key Ideas of LeaFTL\nInstead of using the space-consuming one-to-one mapping in the\npage-level mapping, the key idea of LeaFTL is to exploit learning\ntechniques to identify various LPA-PPA mapping patterns and build\nefficient learned address mapping entries. Modern SSD controllers\nusually have a data buffer for grouping writes and write the large\ndata chunk at once for exploiting the internal flash parallelisms.\nLeaFTL utilizes this data buffer to collect LPA-to-PPA mappings for\nlearning index segments for free, and does not introduce extra data\ncollection overhead (see the details in Â§3.3).\nAs shown in Figure 4 (a), the PPA of an LPA can be obtained\nwith the expression: ğ‘ƒğ‘ƒğ´=ğ‘“(ğ¿ğ‘ƒğ´)=âŒˆğ¾âˆ—ğ¿ğ‘ƒğ´+ğ¼âŒ‰,ğ¿ğ‘ƒğ´âˆˆ\n[ğ‘†ğ¿ğ‘ƒğ´,ğ‘†ğ¿ğ‘ƒğ´+ğ¿], where[ğ‘†ğ¿ğ‘ƒğ´,ğ‘†ğ¿ğ‘ƒğ´+ğ¿]denotes the interval ( ğ¿)\nof LPAs,ğ¾is the slope, and ğ¼is the intercept. As discussed in Â§1,\neach learned index segment can be represented in 8 bytes: 1 byte for\nğ‘†ğ¿ğ‘ƒğ´andğ¿, respectively; 2 bytes for ğ¾, and 4 bytes for ğ¼. The size\nofğ‘†ğ¿ğ‘ƒğ´is reduced from 4 bytes to 1 byte with our optimizations\non the segment management (see Â§3.4).\nWe can relax the linear regression to capture more flash access\npatterns, which further reduces the learned address mapping table\nsize. As shown in Figure 4 (b), the linear regression can learn a\npattern with guaranteed error bound [âˆ’ğ›¾,ğ›¾]. As we increase ğ›¾, we\ncan cover more flash access patterns. We applied the relaxed linear\nregression with different ğ›¾values to a variety of storage workloads\n(seeÂ§4.1), our experimental results demonstrate that the number\nof learned index segments is gradually decreased, as we increase ğ›¾.\nFigure 5 shows that 98.2â€“99.2% of the learned index segments cover\nSegment SL P A L K I1B 1B 2B 4B\nType LPAs PPAs Index Segment\nAccurate [0, 1, 2, 3] [32, 33, 34, 35]\nApproximate [0, 1, 4, 5] [64, 65, 66, 67]0 31.00 32\n0 50.56 64Figure 6: Types of learned segments in LeaFTL.\nup to 128 LPA-PPA mapping entries, demonstrating the potential\nadvantages of the learning-based approach.\nAs for random access patterns, LeaFTL will transfer the learned\nsegments into single-point segments. And these linear segments\ndo not require more storage space than the page-level mapping.\n3.2 Learned Index Segment\nTypes of Learned Index Segment. The mapping table of LeaFTL\nis built with learned index segments. It has two types of segments:\naccurate and approximate segments, as shown in Figure 6. Both of\nthem are learned with piecewise linear regression technique [64].\nAs for the accurate index segments, given an LPA, we can pre-\ncisely get the corresponding PPA with ğ‘“(ğ¿ğ‘ƒğ´)=âŒˆğ¾âˆ—ğ¿ğ‘ƒğ´+ğ¼âŒ‰.\nFor example, when the LPA is 2 in Figure 6, we can directly get the\nPPA value of 34 with âŒˆ1.00âˆ—2+32âŒ‰. In this example, the learned\nsegment has ğ¿=3and it indexes 4 LPA-PPA mappings. If ğ¿=0,\nthe learned segment will become a single-point segment, the slope\nğ¾=0, and we will get its PPA with ğ‘ƒğ‘ƒğ´=ğ¼.\nAs for approximate index segments, we use the same formula\nğ‘“(ğ¿ğ‘ƒğ´)=âŒˆğ¾âˆ—ğ¿ğ‘ƒğ´+ğ¼âŒ‰to calculate the PPA. However, the returned\nPPA may not be the exact corresponding PPA. It has an error bound\n[âˆ’ğ›¾,ğ›¾]guaranteed by the linear regression, and ğ›¾is configurable.\nFor example, given ğ¿ğ‘ƒğ´=4in Figure 6, the value of the PPA is\n67, according to the calculation âŒˆ4âˆ—0.56+64âŒ‰. However, the real\nPPA should be 66. We define this as address misprediction . We will\ndiscuss how we handle the address misprediction with reduced\nmiss penalty inÂ§3.5.\nSize of Learned Index Segment. As discussed inÂ§3.1, each seg-\nment can be expressed in (ğ‘†ğ¿ğ‘ƒğ´,ğ¿,ğ¾,ğ¼). The starting LPA will take\n4 bytes. We can further reduce this size by partitioning a range of\nLPAs into small groups, and each LPA group represents a certain\nnumber of contiguous LPAs. Therefore, we can index an LPA with\nits offset in a corresponding group. In LeaFTL, each group repre-\nsents 256 contiguous LPAs. Thus, ğ‘†ğ¿ğ‘ƒğ´can be indexed by the offset\n(28=256) in the group, which takes only 1 byte. We use 256 as the\ngroup size, because the length of the learned segments is usually\nless than 256 (see Figure 5).\nGiven an LPA, we can get its offset in the group with (ğ¿ğ‘ƒğ´ğ‘šğ‘œğ‘‘\n256). In LeaFTL, we set the ğ¿as 1 byte. Thus, each segment can\nindex 256 LPA-PPA mappings. We use a 16-bit floating point to\nstore the value of the slope ğ¾. And the intercept ğ¼of a segment\ncan be represented in 4 bytes. Therefore, in combination with ğ‘†ğ¿ğ‘ƒğ´,\nboth accurate and approximate segments can be encoded with 8\nbytes (see Figure 6), which are memory aligned.\n\nLeaFTL: A Learning-based Flash-Translation Layer for Solid-State Drives\n(a) Unoptimized learned segments\n(b) Optimized learned segments with sortingLearned Segments\n78 32Â  33 76FlushData Buf fer\n11534Â  38Flash Block\n78\n32\n33\n76\n115\n34\n38\n...LPA783233761153438\nLearned SegmentsFlushData Buf ferFlash Block\n32\n33\n34\n38\n76\n78\n115\n...LPA783233761153438\n115 32Â  33Â  34Â  38 76Â  78\nFigure 7: An example of reducing the number of learned seg-\nments via exploiting the flash block allocation.\nLeaFTL uses the least significant bit of the ğ¾to indicate segment\ntypes (0 for accurate segments, 1 for approximate segments). This\nhas negligible impact on the address translation accuracy, because\nğ¾âˆˆ[0,1], which will only affect the tenth digit after decimal point.\n3.3 Improve the Learning Efficiency\nTo further reduce the number of learned segments, LeaFTL performs\noptimizations to improve its learning efficiency of address mappings\nby exploiting the flash block allocation in SSD controllers, as shown\nin Figure 7. Flash pages are usually buffered in the SSD controller\nand written to flash chips at a flash block granularity, for utilizing\nthe internal bandwidth and avoiding the open-block problem [ 6,\n22,37,48]. This allows LeaFTL to learn more space-efficient index\nsegments (i.e., index segments can cover more LPA-PPA mappings)\nby reordering the flash pages with their LPAs in the data buffer.\nAs shown in Figure 7 (a), LeaFTL learns 5 index segments (78), (32,\n33), (76), (115), and (34, 38) with ğ›¾=4. After sorting the pages in\nthe data buffer shown in Figure 7 (b), LeaFTL generates 3 index\nsegments (32, 33, 34, 38), (76, 78), and (115).\nTo develop the optimized learned segments, LeaFTL sorts the\nflash pages in ascending order of their LPAs in the data buffer (8MB\nby default). When pages in the data buffer is flushed to the flash\nchips, their PPAs are in ascending order. This ensures a mono-\ntonic address mapping between LPAs and PPAs, which reduces the\nnumber of index segments.\n3.4 Manage Learned Index Segments\nUpon new data updates or GC in the SSD, the learned index seg-\nments need to be updated, due to the intrinsic property (i.e., out-of-\nplace update) of SSDs. Unfortunately, the direct updates to learned\nindex segments are expensive, since we have to relearn the in-\ndex segments with new PPAs. This relearning procedure not only\nconsumes extra compute cycles, but also involves additional flash\naccesses, since we have to access the corresponding flash pages to\nobtain accurate PPAs for some of the LPAs in the index segment\nbeing updated. For instance, for in-place update to an approximate\nLevel 0\nLevel 10     63 100     200 230    255\n16   127 206    240non-overlapping \nat each level\nsegments can overlap \nacross levelsFigure 8: The learned index segments are managed in a log-\nstructured manner in LeaFTL.\nsegment, it can incur 21 flash accesses on average when relearn-\ning. In-place update also breaks the existing LPA-to-PPA mapping\npatterns, which results in 1.2Ã—additional segments and memory\nfootprint, according to our experiments with various workloads.\nTo address this challenge, we manage the learned index segments\nin a log-structured manner, as shown in Figure 8. Therefore, the\nnewly learned index segments will be appended to the log structure\n(level 0 in Figure 8) and used to index the updated LPA-PPA map-\npings, while the existing learned segments (level 1 and lower levels\nin Figure 8) can still serve address translations for LPAs whose map-\npings have not been updated. Such a structure supports concurrent\nlookups as enabled in the traditional log-structured merge tree. As\nwe insert the newly learned index segments at the top level of the\nlog-structured tree, this minimizes the impact on other segments.\nLog-Structured Mapping Table. The log-structured mapping ta-\nble has multiple levels to maintain the temporal order of index seg-\nments. As discussed, the topmost level has the most recent learned\nindex segments, and the lower level stores the older segments. For\nthe segments on the same level, LeaFTL ensures that they are sorted\nand do not have overlapped LPAs. This is for fast location of the\ncorresponding learned index segments in each level. For the seg-\nments across the levels, they may have overlapped LPAs, due to the\nnature of the log-structured organization. And the segments with\noverlapped LPA-PPA mappings will be compacted periodically for\nspace reclamation (see its detailed procedure in Â§3.7).\nManage Two Types of Index Segments. LeaFTL manages the ac-\ncurate and approximate index segments in the same log-structured\nmapping table, as they can be encoded in the same format. For each\naccurate segment, we can directly infer its indexed LPAs with the\nğ‘†ğ¿ğ‘ƒğ´,ğ¾, andğ¿, since it has a regular pattern. However, for approx-\nimate index segments, we only have the knowledge of the starting\nLPA and the end LPA with ğ‘†ğ¿ğ‘ƒğ´+ğ¿. Its encoded LPAs cannot be\ndirectly inferred from their metadata (ğ‘†ğ¿ğ‘ƒğ´,ğ¿,ğ¾,ğ¼), since they are\nlearned from irregular access patterns and may have mispredictions.\nIf two approximate segments have overlapping LPA ranges, we\ncould obtain inaccurate PPAs from the learned index segments.\nAs shown in Figure 9 (a), given an LPA with the value 105, we\nwill check the segment at Level 0 and may get an inaccurate PPA.\nThis will also affect the efficiency of the segment compaction, with\nwhich we eliminate duplicated entries between segments.\nTo address this challenge, LeaFTL uses a Conflict Resolution\nBuffer (CRB) for each LPA group to store the LPAs indexed by each\napproximate segment. The main purpose of CRB is to help LeaFTL\ncheck whether a given LPA belongs to one approximate segment.\nThe CRB is a nearly-sorted list [ 10] by the starting LPAs of its ap-\nproximate segments. To be specific, the CRB ensures the following\n\nJinghan Sun, Shaobo Li, Yunxin Sun, Chao Sun, Dejan Vucinic, and Jian Huang\n100 6 K1 I1  [100, 101, 103, 104, 106]\n102 6 K2 I2 [102, 105, 107, 108]L0\nL1LPAsLookup (LP A = 105)\n(a) Approximate index segments that index overlapped LPAs.\nConï¬‚ict Resolution Buf fer100 101 103 104 106 null 102 105 107 108 null ...Lookup (LP A = 105)\n102 6 K2 I2\n(b) Resolve the conflict between approximate segments with CRB\nFigure 9: A case study of conflict resolution buffer for ap-\nproximate learned index segments.\nproperties: (1) the LPAs belong to the same approximate segment\nare stored contiguously; (2) different approximate segments are\nsorted by their starting LPA, and CRB uses a ğ‘›ğ‘¢ğ‘™ğ‘™ byte to separate\nthese segments; (3) it does not have redundant LPAs, which means\nan LPA will appear at most once in the CRB. This is achieved by\nremoving existing same LPAs when we insert new approximate\nsegments into the CRB.\nHowever, if the ğ‘†ğ¿ğ‘ƒğ´of a new approximate segment is the same\nas any starting LPAs that have been stored in the CRB, LeaFTL will\nupdate theğ‘†ğ¿ğ‘ƒğ´of the old segment with the adjacent LPA. Take\nFigure 9 (b) as an example, upon a new approximate segment with\nğ‘†ğ¿ğ‘ƒğ´=100, we will update the ğ‘†ğ¿ğ‘ƒğ´of the existing segment to 101,\nand then insert the new segment into the CRB. In this case, LeaFTL\nwill ensure each approximate segment will have its unique ğ‘†ğ¿ğ‘ƒğ´.\nThis will facilitate the approximate LPA-PPA address translation\nwith high accuracy confidence.\nSince CRB is nearly sorted, its insertion, deletion, and lookup\noperations are fast. The CRB is also space efficient, as each LPA\n(the offset in its corresponding LPA group) will take only one byte,\nand it guarantees that there are no redundant LPAs. Therefore, the\nCRB will maximally store 256 LPAs. Our experiments with a variety\nof storage workloads show that the CRB will take 13.9bytes on\naverage, as shown in Figure 10.\nGiven an LPA, in order to identify which approximate index\nsegment it belongs to, LeaFTL will check the CRB with binary\nsearch. Once the LPA is found, LeaFTL will search to its left until\nidentifying the ğ‘†ğ¿ğ‘ƒğ´, and thisğ‘†ğ¿ğ‘ƒğ´ will be the starting LPA of\nthe corresponding approximate segment, as shown in Figure 9 (b).\nTherefore, CRB can assist LeaFTL to resolve the LPA lookups.\n3.5 Handle Address Misprediction\nAs discussed inÂ§3.2, the mapping table entries encoded with ap-\nproximate segments may occasionally incur mispredictions and\nreturn an approximated PPA. These approximate segments have a\nguaranteed error bound [âˆ’ğ›¾,ğ›¾], whereğ›¾is a constant value that\ncan be specified in the linear regression algorithm. To verify the\ncorrectness of the address translation, a simple method is to access\nMSR-hm MSR-src2 MSR-prxy MSR-prn MSR-usr FIU-home FIU-mail0100200300CRB Size (in Bytes)\nAverage 99 PercentileFigure 10: The distribution of CRB sizes for different storage\nworkloads, when we set ğ›¾=4in LeaFTL.\nPPA1\nPPA2\nPPA3\nPPA4\nPPA5\nData BlocksData OOBFlash Page\nLPA2 LPA4 LPAReverse Mapping\nFigure 11: The out-of-band (OOB) metadata organization. It\nstores the reverse mapping for its neighbor PPAs.\nthe flash page with the predicted PPA, and use the reverse mapping\n(its corresponding LPA) stored in the OOB metadata of the flash\npage to check whether the LPA matches or not. In this case, upon\na PPA misprediction, we need log(ğ›¾)flash accesses on average to\nidentify the correct PPA.\nTo avoid extra flash accesses for address mispredictions, LeaFTL\nleverages the OOB of the flash page to store the reverse mappings\nof its neighbor PPAs. This is developed based on the insight that:\nwith ağ‘ƒğ‘ƒğ´ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘’ğ‘‘ obtained from an approximate segment, its er-\nror bound[âˆ’ğ›¾,ğ›¾]guarantees that the correct PPA is in the range\nof[ğ‘ƒğ‘ƒğ´ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘’ğ‘‘âˆ’ğ›¾,ğ‘ƒğ‘ƒğ´ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘’ğ‘‘+ğ›¾], as discussed in Figure 4 (b).\nThus, upon a misprediction, LeaFTL will read the flash page with\nğ‘ƒğ‘ƒğ´ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘’ğ‘‘ , and use its OOB to find the correct PPA. In this case,\nLeaFTL ensures that it will incur only one extra flash access for\naddress mispredictions.\nThis is a feasible approach, as the OOB size is usually 128â€“256\nbytes in modern SSDs. As each LPA takes 4 bytes, we can store\n32â€“64 reverse mapping entries in the OOB. We show the OOB\norganization of LeaFTL in Figure 11. For the flash page ğ‘ƒğ‘ƒğ´ğ‘‹, the\nfirst 2ğ›¾+1entries in its OOB correspond to the LPAs for the flash\npages[ğ‘ƒğ‘ƒğ´ğ‘‹âˆ’ğ›¾,ğ‘ƒğ‘ƒğ´ğ‘‹+ğ›¾]. For the flash pages at the beginning\nand end of a flash block, we may not be able to obtain the reverse\nmapping of their neighbor PPAs. We place the ğ‘›ğ‘¢ğ‘™ğ‘™ bytes in the\ncorresponding entry of the OOB.\n3.6 Preserve Other Core FTL Functions\nLeaFTL preserves the core functions such as GC and wear leveling\nin an FTL. It follows the same GC and wear leveling policies in\nmodern SSDs. When the number of free blocks in an SSD is below\na threshold (usually 15-40% of the total flash blocks), the SSD con-\ntroller will trigger the GC execution. LeaFTL employs the greedy\nalgorithm [ 5] to select the candidate blocks which have the minimal\n\nLeaFTL: A Learning-based Flash-Translation Layer for Solid-State Drives\nALGORITHM 1: LeaFTL operations\nInput:ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ â†ğ¿ğ‘’ğ‘ğ¹ğ‘‡ğ¿ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ \n//Insert/Update Segment in the LeaFTL\n1Functionğ‘ ğ‘’ğ‘”_ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ (ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡,ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ ):\n2ğ‘ ğ‘’ğ‘”_ğ‘ğ‘œğ‘ =ğ‘ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘¦ _ğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„(ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™,ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡.ğ‘† ğ¿ğ‘ƒğ´)\n3ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™.ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿğ‘¡(ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡,ğ‘ ğ‘’ğ‘” _ğ‘ğ‘œğ‘ )\n4 ifğ‘›ğ‘œğ‘¡ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡.ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘’ then\n5 Insert LPAs into CRB and remove redundant LPAs\n6 ifğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡.ğ‘† ğ¿ğ‘ƒğ´ exists in CRB then\n7 Update theğ‘†ğ¿ğ‘ƒğ´ of the old segment\n8ğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘š _ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡ğ‘ â†All segments that overlap the ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡\nstarting with ğ‘ ğ‘’ğ‘”_ğ‘ğ‘œğ‘ \n9 foreachğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘šâˆˆğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘š _ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡ğ‘  do\n10ğ‘ ğ‘’ğ‘”_ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’(ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡,ğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘š)\n// if marked as removable by seg_merge()\n11 ifğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘š.ğ¿ =âˆ’1then\n12 ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™.ğ‘Ÿğ‘’ğ‘šğ‘œğ‘£ğ‘’(ğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘š)\n13 ifğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡.ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘ğ‘  (ğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘š)then\n14 Popğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘š to the next level\n15 ifğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘š has overlaps in the next level then\n16 Create level for ğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘š to avoid recursion\n//Lookup LPA in the LeaFTL\n17Functionğ‘™ğ‘œğ‘œğ‘˜ğ‘¢ğ‘ (ğ‘™ğ‘ğ‘):\n18 foreachğ‘™ğ‘’ğ‘£ğ‘’ğ‘™âˆˆğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ [ğ‘™ğ‘ğ‘ğ‘šğ‘œğ‘‘ 256]do\n19ğ‘ ğ‘’ğ‘”_ğ‘ğ‘œğ‘ =ğ‘ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘¦ _ğ‘ ğ‘’ğ‘ğ‘Ÿğ‘â„(ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™,ğ‘™ğ‘ğ‘)\n20ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡ =ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™.ğ‘”ğ‘’ğ‘¡ _ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡(ğ‘ ğ‘’ğ‘”_ğ‘ğ‘œğ‘ )\n21 ifâ„ğ‘ğ‘ _ğ‘™ğ‘ğ‘(ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡, ğ‘™ğ‘ğ‘)then\n22 returnğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡.ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘™ğ‘ğ‘¡ğ‘’ğ‘ƒğ‘ƒğ´ (ğ‘™ğ‘ğ‘)\n//LeaFTL Compaction\n23Functionğ‘ ğ‘’ğ‘”_ğ‘ğ‘œğ‘šğ‘ğ‘ğ‘ğ‘¡ ():\n24 foreachğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘âˆˆğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ğ‘  do\n25 foreachğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™,ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™âˆˆğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ do\n26 foreachğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡âˆˆğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ do\n27 ğ‘ ğ‘’ğ‘”_ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’(ğ‘ ğ‘’ğ‘”ğ‘šğ‘’ğ‘›ğ‘¡,ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™)\n28 ifğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ is empty then\n29 ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘.ğ‘Ÿğ‘’ğ‘šğ‘œğ‘£ğ‘’(ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ _ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™)\nnumber of valid pages, for reducing the data movement overhead\nat GC. As the GC move the valid pages from the candidate blocks\nto the free blocks, LeaFTL places these valid pages into the DRAM\nbuffer, sort them by their LPAs, and learn a new index segment.\nThe learning procedure is the same as we build index segments for\nnew flash writes/updates. Thus, the address mapping of the valid\npages is updated after the GC.\nLeaFTL also ensures all the flash blocks age at the same rate\n(i.e., wear leveling). It uses the throttling and swapping mechanism\ndeveloped in existing GC, in which the cold data blocks (i.e., blocks\nnot frequently accessed) will be migrated to hot blocks (i.e., blocks\nthat experience more wear). LeaFTL will learn new indexes for\nthese swapped blocks and insert them into the mapping table to\nupdate their address mappings.\n3.7 LeaFTL Operations\nNow we describe the LeaFTL operations, including segment cre-\nation, insert/update, LPA lookup, and compaction. We discuss their\nprocedures, and use examples to illustrate each of them, respec-\ntively. We present their detailed procedures in Algorithm 1 and 2.ALGORITHM 2: Segment Merge\n//Check if Segment Contains LPA\n1Functionâ„ğ‘ğ‘ _ğ‘™ğ‘ğ‘(ğ‘ ğ‘’ğ‘”, ğ‘™ğ‘ğ‘ ):\n2ğ‘ğ‘ğ‘â†ğ‘ ğ‘’ğ‘”.ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘’\n3 ifğ‘™ğ‘ğ‘âˆ‰[ğ‘ ğ‘’ğ‘”.ğ‘†ğ¿ğ‘ƒğ´,ğ‘ ğ‘’ğ‘”.ğ‘†ğ¿ğ‘ƒğ´+ğ‘ ğ‘’ğ‘”.ğ¿]ğ‘œğ‘Ÿ\n(ğ‘›ğ‘œğ‘¡ğ‘ğ‘ğ‘ &ğ‘â„ğ‘’ğ‘ğ‘˜(ğ¶ğ‘…ğµ)ğ‘“ğ‘ğ‘–ğ‘™ğ‘’ğ‘‘)ğ‘œğ‘Ÿ\n(ğ‘ğ‘ğ‘&(ğ‘™ğ‘ğ‘âˆ’ğ‘ ğ‘’ğ‘”.ğ‘†ğ¿ğ‘ƒğ´)ğ‘šğ‘œğ‘‘âŒˆ1\nğ‘ ğ‘’ğ‘”.ğ¾âŒ‰â‰ 0)then\n4ğ‘Ÿğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›ğ¹ğ‘ğ‘™ğ‘ ğ‘’\n5ğ‘Ÿğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›ğ‘‡ğ‘Ÿğ‘¢ğ‘’\n//Convert Segment into a Temporary Bitmap\n6Functionğ‘”ğ‘’ğ‘¡_ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘ (ğ‘ ğ‘’ğ‘”, ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡, ğ‘’ğ‘›ğ‘‘ ):\n7ğ‘ğ‘šâ†ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘ğ‘œğ‘“ ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„ (ğ‘’ğ‘›ğ‘‘âˆ’ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡+1)\n8 foreachğ‘™ğ‘ğ‘âˆˆ[ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡,ğ‘’ğ‘›ğ‘‘]do\n9 ifâ„ğ‘ğ‘ _ğ‘™ğ‘ğ‘(ğ‘ ğ‘’ğ‘”, ğ‘™ğ‘ğ‘)then\n10 ğ‘ğ‘š[ğ‘™ğ‘ğ‘âˆ’ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡]=1\n11 else\n12 ğ‘ğ‘š[ğ‘™ğ‘ğ‘âˆ’ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡]=0\n13 returnğ‘ğ‘š\n//Merge a New Segment with an Old Segment\n14Functionğ‘ ğ‘’ğ‘”_ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ (ğ‘›ğ‘’ğ‘¤, ğ‘œğ‘™ğ‘‘ ):\n15ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡â†ğ‘šğ‘–ğ‘›(ğ‘›ğ‘’ğ‘¤.ğ‘†ğ¿ğ‘ƒğ´, ğ‘œğ‘™ğ‘‘.ğ‘†ğ¿ğ‘ƒğ´)\n16ğ‘’ğ‘›ğ‘‘â†ğ‘šğ‘ğ‘¥(ğ‘›ğ‘’ğ‘¤.ğ‘†ğ¿ğ‘ƒğ´+ğ‘›ğ‘’ğ‘¤.ğ¿, ğ‘œğ‘™ğ‘‘.ğ‘† ğ¿ğ‘ƒğ´+ğ‘œğ‘™ğ‘‘.ğ¿)\n17ğ‘ğ‘šğ‘›ğ‘’ğ‘¤â†ğ‘”ğ‘’ğ‘¡_ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘(ğ‘›ğ‘’ğ‘¤, ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡, ğ‘’ğ‘›ğ‘‘)\n18ğ‘ğ‘šğ‘œğ‘™ğ‘‘â†ğ‘”ğ‘’ğ‘¡_ğ‘ğ‘–ğ‘¡ğ‘šğ‘ğ‘(ğ‘œğ‘™ğ‘‘, ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡, ğ‘’ğ‘›ğ‘‘)\n19ğ‘ğ‘šğ‘œğ‘™ğ‘‘â†ğ‘ğ‘šğ‘œğ‘™ğ‘‘&Â¬ğ‘ğ‘šğ‘›ğ‘’ğ‘¤\n20ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡, ğ‘™ğ‘ğ‘ ğ‘¡â†the first and last valid bit of ğ‘ğ‘šğ‘œğ‘™ğ‘‘\n21ğ‘œğ‘™ğ‘‘.ğ‘†ğ¿ğ‘ƒğ´, ğ‘œğ‘™ğ‘‘.ğ¿â†ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡+ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡, ğ‘™ğ‘ğ‘ ğ‘¡âˆ’ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡\n22 ifno valid bits in ğ‘œğ‘™ğ‘‘then\n23ğ‘œğ‘™ğ‘‘.ğ¿â†âˆ’ 1 // mark it as removable\n24 ifğ‘›ğ‘œğ‘¡ğ‘œğ‘™ğ‘‘.ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘’ then\n25 Remove outdated LPAs in CRB\nCreation of Learned Segments. Once the data buffer of the SSD\ncontroller is filled, LeaFTL takes the LPAs and PPAs of the flash\npages in the buffer as the input. It sorts the LPA-PPA mappings\nby reordering the flash pages with their LPAs (see Â§3.3), and uses\ngreedy piecewise linear regression [64] to learn the index segment.\nInsert/Update of Learned Segments. When we insert or update\na new learned index segment, we will place it in the topmost level\nof the log-structured mapping table. Since each level of the map-\nping table is sorted, we can quickly identify its insert location via\na binary search (line 2 in Algorithm 1). If the new segment is ap-\nproximate, LeaFTL will update the CRB for future lookups (line\n4-7 in Algorithm 1). After that, LeaFTL will check whether the\nnew segment overlaps with existing segments. If yes, LeaFTL will\nidentify the overlapped LPAs. The overlap detection is performed\nby the comparison between the LPA range of the new segment and\n[ğ‘†ğ¿ğ‘ƒğ´,ğ‘†ğ¿ğ‘ƒğ´+ğ¿]of the adjacent segments. We group these overlap-\nping segments as a list of victim segments (line 8 in Algorithm 1).\nLeaFTL will merge segments to remove outdated LPAs (line 10 in\nAlgorithm 1 and line 14-25 in Algorithm 2).\nTo fulfill the segment merge, LeaFTL will use the ğ‘†ğ¿ğ‘ƒğ´,ğ¿, andğ¾\nto reconstruct the list of the encoded LPAs in the victim segment.\nAnd it will create a bitmap to index these encoded LPAs (line 6-13\nin Algorithm 2). Given an accurate segment with ğ‘†ğ¿ğ‘ƒğ´=100,ğ¾=\n0.5,ğ¿=6, we can infer that its encoded LPAs are [100,102,104,106].\nWe can transfer the LPA list to the bitmap [1010101]. If the victim\n\nJinghan Sun, Shaobo Li, Yunxin Sun, Chao Sun, Dejan Vucinic, and Jian Huang\nMSR-hm MSR-src2 MSR-prxy MSR-prn MSR-usr FIU-home FIU-mail05101520# of Levels \nin Each Group\nAverage 99 Percentile\nFigure 12: A study of the number of levels in the log-\nstructured mapping table for different storage workloads.\nL0 0Â  Â  Â  63T0\nInitial Snapshot\nT1\nUpdate LP As 200 - 255L0 0Â  Â  Â 63 200Â  255\nT2\nUpdate LP As 16 - 31L0 16Â Â Â  31 200Â  255\nL1 0Â  Â  Â  63\nT4\nUpdate [72, 73, 80]L0 16Â Â Â  31 200Â  255\nL1 0Â  Â  Â  63\nT6\nLookup LP A 78L0\nL1\nT8\nCompactionTimeline Segments CRB\nT7\nUpdate LP As 32 - 9075Â  Â  Â 8272Â  Â  Â 80\n16Â Â Â  31 200Â  255\n0Â  Â  Â  63 75Â  Â  Â 8272Â  Â  Â 80T5\nLookup LP A 50L0\nL116Â Â Â  31 200Â  255\n0Â  Â  Â  63 75Â  Â  Â 8272Â  Â  Â 80\nL0\nL116Â Â Â  31 200Â  255\n0Â  Â  Â  63 75Â  Â  Â 8232Â  Â  Â 90\nL0 16Â  Â 31 200Â  255 0Â  Â  15 32Â  Â 90StartÂ  Â  Â  End Accurate Segment StartÂ  Â  Â  End Approximate Segment\n727380/757882\n727380/757882727380/757882\n757882T3\nUpdate [75, 78, 82]L0 16Â Â Â  31 200Â  255\nL1 0Â  Â  Â  6375Â  Â  Â 82\n757882\nFigure 13: Examples that involve update/insert, lookup, and\ncompaction operations in LeaFTL.\nsegment is an approximate segment, LeaFTL will leverage the ğ‘†ğ¿ğ‘ƒğ´,\nğ¿, and the LPAs stored in the CRB to reconstruct the encoded LPAs.\nAfterwards, LeaFTL will conduct a comparison between the bitmaps\nto identify the overlapped LPAs (line 15-19 in Algorithm 2).\nDuring the segment merge, LeaFTL will update the ğ‘†ğ¿ğ‘ƒğ´andğ¿\nof the old segments accordingly, remove the outdated LPAs from\nCRB for approximate segments. Note that we do not update the ğ¾\nandğ¼for the victim segments during the merge.\nAfter the merge, (1) if the victim segment does not contain any\nvalid LPA (ğ¿is negative), it will be removed from the mapping\ntable (line 11-12 in Algorithm 1). (2) If the victim segment has\nvalid LPAs but their range still overlaps with the new segment,the victim segment will be moved to the next level in the log-\nstructured mapping table (line 13-16 in Algorithm 1). To avoid\nrecursive updates across the levels, we create a new level for the\nvictim segment if it also overlaps with segments in the next level.\nAccording to our study of diverse workloads, this will not create\nmany levels in the mapping table (see Figure 12). (3) If the victim\nsegment has valid LPAs and they do not overlap with the new\nsegment, we do not need to perform further operations. This is\nbecause the victim segment is updated with new ğ‘†ğ¿ğ‘ƒğ´andğ¿during\nsegment merge (line 20-25 in Algorithm 2), and the new segment\ninsertion keeps each level sorted (line 3 in Algorithm 1).\nTo facilitate our discussion, we present a few examples in Fig-\nure 13. At the initial stage, the mapping table has one segment that\nindexes the LPA range [0, 63]. At ğ‘‡1, the new segment [200, 255] is\ndirectly inserted into the topmost level, as it does not overlap with\nexisting segments. At ğ‘‡2, we insert a new segment [16, 31] that has\noverlaps with the old segment [0, 63], LeaFTL conducts the segment\nmerge procedure. After that, the old segment still has valid LPAs.\nThus, it moves to level 1. At ğ‘‡3andğ‘‡4, we insert two approximate\nsegments [75, 82] and [72, 80], LeaFTL will also insert their encoded\nLPAs into the CRB. The segment [75, 82] will be moved to the next\nlevel as it overlaps with the new segment [72, 80].\nLPA Lookup. LeaFTL conducts an LPA lookup from the top-\nmost level of the mapping table with binary searches (line 19 in\nAlgorithm 1). We will check whether the LPA is represented by the\nmatched segment (line 21 in Algorithm 1, line 1-5 in Algorithm 2). If\ntheğ¿ğ‘ƒğ´âˆˆ[ğ‘†ğ¿ğ‘ƒğ´,ğ‘†ğ¿ğ‘ƒğ´+ğ¿]of the segment, LeaFTL will check the\nleast bit of its ğ¾. If the least bit of ğ¾is 0, it is an accurate segment,\nand LeaFTL will use ğ‘“(ğ¿ğ‘ƒğ´)=âŒˆğ¾âˆ—ğ¿ğ‘ƒğ´+ğ¼âŒ‰to get the accurate\nPPA (seeÂ§3.2). Otherwise, it is an approximate segment. LeaFTL\nwill check the CRB to identify the ğ‘†ğ¿ğ‘ƒğ´of the segment, following\nthe approach described in Figure 9 and Â§3.4. LeaFTL will use the\nsameğ‘“(ğ¿ğ‘ƒğ´)formula to obtain the PPA. If the LPA is not found in\nthe top level of the mapping table, LeaFTL will search the lower\nlevels until a segment is identified.\nWe use Figure 13 to illustrate the lookup procedure. At ğ‘‡5, we\nconduct the address translation for ğ¿ğ‘ƒğ´=50. However, none of\nthe segments in the level 0 covers this LPA, LeaFTL will continue\nthe search in the level 1 and find the accurate segment [0, 63]. At\nğ‘‡6, we do the address translation for ğ¿ğ‘ƒğ´=78. LeaFTL finds that\nthe LPA 78 is in the LPA range of the segment [72, 80]. Since this\nis an approximate segment, LeaFTL checks the CRB and finds this\nLPA is actually indexed by the segment [75, 82].\nWith the PPA, LeaFTL will read the corresponding flash page and\nuse the reversed mapping (its corresponding LPA) in its OOB to ver-\nify the correctness of the address translation. Upon mispredictions,\nwe will use the approach discussed in Â§3.5 to handle it.\nSegment Compaction. The purpose of the compaction is to\nmerge segments with overlapped LPAs across different levels, which\nfurther saves memory space. LeaFTL will iteratively move the upper-\nlevel segments into the lower level, until the mapping table is fully\ncompacted (line 27 in Algorithm 1). When an approximate segment\nis removed, its corresponding CRB entries will also be deleted. As\nshown inğ‘‡7of Figure 13, we insert a new segment [32, 90] which\nfully covers the LPA range of the segment [72, 80]. After merge,\nLeaFTL removes the old segment [72, 80]. However, some segments\n\nLeaFTL: A Learning-based Flash-Translation Layer for Solid-State Drives\nConflict Resolution\nBuffer (CRB)Key Data Structures in LeaFTL  \n6Log-Structured\nMapping T able5\nL0\nL1\nL2...Group\n0\n...CRB\n...\n...0   63 ...16  31 ... ...\n64  95\nFigure 14: Key data structures used in LeaFTL.\nin the level 0 still overlap with the segments in the level 1. After ğ‘‡8,\nLeaFTL will remove outdated segments and LPAs.\nLeaFTL performs segment compaction after each 1 million writes\nby default. According to our experiments with various storage work-\nloads, the segment compaction of the entire mapping table will take\n4.1 milliseconds (the time of 20-40 flash writes) on average. Consider\nthe low frequency (i.e., once per 1 million writes), the compaction\nincurs trivial performance overhead to storage operations.\n3.8 Put It All Together\nLeaFTL is compatible with existing FTL implementations. As shown\nin Figure 14, it uses the log-structured mapping table ( 5) to replace\nthe address mapping cache ( 1in Figure 3), and employs CRB ( 6)\nfor assisting the address translation of approximate segments. The\nCRB requires trivial storage space in the SSD DRAM (see Figure 10).\nRead Operation. For a read request, LeaFTL will first check the\ndata cache. For a cache hit, LeaFTL serves the read request with\nthe cached flash page. Otherwise, LeaFTL will perform address\ntranslation with 5(seeÂ§3.7). If there is a misprediction of PPA,\nLeaFTL checks the OOB of the mispredicted flash page, read the\ncorrect page (Â§3.5), and updates the data cache with the page.\nWrite Operation. For a write request, LeaFTL buffers it in the\ndata cache. Once the buffered writes reach the size of a flash block,\nLeaFTL will allocate a free block. It will sort the writes in the buffer\nbased on their LPAs, and learn new index segments with the PPAs\nof the allocated flash block. This enables LeaFTL to group more LPA-\nPPA mappings in the same index segment. After that, LeaFTL will\ninsert the new index segment in the mapping table, and flush the\nbuffered data to the flash blocks. For those writes, LeaFTL will also\ncheck whether their LPAs exist in the mapping table. If yes, LeaFTL\nwill update their corresponding entries in 3BVC and 4PVT to\nindicate that they become invalid and can be garbage collected in\nthe future. Otherwise, the new learned segments will have their\nLPA-PPA mappings for future address translations.\nLeaFTL caches the mapping table in SSD DRAM for fast lookup.\nThe table will also be stored in the flash blocks. LeaFTL utilizes the\nexisting 2GMD to index the translation pages. If a segment is not\nfound in the cached mapping table, LeaFTL will fetch it from the\ntranslation blocks and place it in the cached mapping table.\nCrash Consistency and Recovery. Upon system crashes or power\nfailures, LeaFTL guarantees the crash consistency of learned in-\ndexes. In order to ensure the data durability of DRAM buffer in\nSSD controllers, modern SSDs today have employed battery-backed\nDRAM and power loss protection mechanisms [ 1,2]. With battery-\nbacked DRAM, LeaFTL has sufficient time to persist the up-to-date\nmapping table to the flash blocks and record their PPAs in the GMDTable 1: SSD configurations in our simulator.\nParameter Value Parameter Value\nCapacity 2TB #Channels 16\nPage size 4KB OOB size 128B\nDRAM size 1GB Pages/block 256\nRead latency 20ğœ‡s Write latency 200ğœ‡s\nErase 1.5 millisecs Overprovisioning ratio 20%\n(2in Figure 3). During the data recovery, LeaFTL reads the GMD\nto locate its mapping table and place it into the DRAM.\nWithout battery-backed DRAM, LeaFTL periodically flushes the\nlearned mapping table and the Block Validity Counter ( 3BVC in\nFigure 3) into the flash blocks. When GC is triggered, LeaFTL also\nflushes the updated mapping table and BVC into the flash blocks.\nUpon crashes, LeaFTL will scan all the flash blocks at the channel-\nlevel parallelism, and reconstruct an up-to-date BVC. LeaFTL is able\nto identify the flash blocks allocated since the last mapping table\nflush, by comparing the up-to-date BVC with the stored BVC in the\nSSD. Therefore, LeaFTL only needs to relearn the index segments\nfor these recently allocated flash blocks and add them into the\nmapping table (see Â§3.4).\n3.9 Implementation Details\nSSD Simulator. We implement LeaFTL based on a trace-driven\nsimulator WiscSim [ 27], which has provided an event simulation\nenvironment for the end-to-end performance analysis of SSDs. We\nextend WiscSim by implementing an LRU-based read-write cache.\nLeaFTL also preserves the functions of existing FTL, such as GC and\nwear-leveling. To support the learned indexing, LeaFTL employs\na simple linear regression algorithm [ 65], which incurs negligible\ncomputation overhead with modern storage processors (see Â§4.5).\nThe error bound ğ›¾for learned segments is configurable, and we set\nit to 0 by default in LeaFTL.\nSSD Prototype. We also develop a real system prototype with\nan open-channel SSD to validate the functions and efficiency of\nLeaFTL . The SSD has 1TB storage capacity with 16 KB flash page\nsize. It has 16 channels, each channel has 16K flash blocks, and each\nflash block has 256 pages. It enables developers to implement their\nown FTL in the host by providing basic I/O commands such as read,\nwrite, and erase. We implement LeaFTL with 4,016 lines of code\nusing C programming language with the SDK library of the device.\n4 EVALUATION\nOur evaluation shows that: (1) LeaFTL significantly reduces the\naddress mapping table size, and the saved memory brings perfor-\nmance benefits (Â§4.2); (2) the benefits of LeaFTL are validated on a\nreal SSD device (Â§4.3); (3) LeaFTL can achieve additional memory\nsavings and performance benefits with larger error-tolerance, and\nit demonstrate generality for different SSD configurations ( Â§4.4);\n(4) Its learning procedure does not introduce much extra overhead\nto the SSD controller ( Â§4.5); (5) It has minimal negative impact on\nthe SSD lifetime (Â§4.6).\n\nJinghan Sun, Shaobo Li, Yunxin Sun, Chao Sun, Dejan Vucinic, and Jian Huang\nTable 2: Real workloads used in our real SSD evaluation.\nWorkload Description\nOLTP [59] Transactional benchmark in the FileBench.\nCompFlow (CompF) [59] File accesses in a computation flow.\nTPCC [13] Online transaction queries in warehouses.\nAuctionMark (AMark) [13] Activity queries in an auction site.\nSEATS [13] Airline ticketing system queries.\nMSR-hm MSR-src2 MSR-prxy MSR-prn MSR-usr FIU-home FIU-mail50x20x10x5x2x1xMemory Footprint \nReductionDFTL SFTL LeaFTL\nFigure 15: The reduction on the mapping table size of\nLeaFTL, in comparison with DFTL and SFTL.\n4.1 Experiment Setup\nWe examine the efficiency of LeaFTL with both the SSD simula-\ntor and real SSD prototype. As for the evaluation with the SSD\nsimulator, we configure a 2TB SSD with 4KB flash pages and 1GB\nDRAM in the SSD controller. We list the core SSD parameters in\nTable 1. For other parameters, we use the default setting in the\nWiscSim. We use a variety of storage workloads that include the\nblock I/O traces from enterprise servers from Microsoft Research\nCambridge [ 45] and workload traces from computers at FIU [ 16].\nAs for the evaluation with the real SSD prototype (see Â§3.9), we\nvalidate the benefits of LeaFTL using a set of real-world file system\nbenchmarks and data intensive applications as shown in Table 2.\nBefore we measure the performance, we run a set of workloads\nconsisting of various real-world and synthetic storage workload\ntraces to warm up the SSD and make sure the GC will be executed\nduring the experiments.\nWe compare LeaFTL with state-of-the-art page-level mapping\nschemes described as follows1.\nâ€¢DFTL (Demand-based FTL) [20]: it uses a page-level mapping\nscheme, and caches the most recently used address translation\nentries in the SSD DRAM.\nâ€¢SFTL (Spatial-locality-aware FTL) [25]: it is a page-level map-\nping that exploits the spatial locality and strictly sequential access\npatterns of workloads to condense mapping table entries.\n4.2 Memory Saving and Performance\nWe first evaluate the benefits of LeaFTL on the memory saving\nand storage performance with the SSD simulator. As shown in\nFigure 15, LeaFTL reduces the mapping table size by 7.5â€“37.7 Ã—,\ncompared to the page-level mapping scheme DFTL. This is because\nLeaFTL can group a set of page-level mapping entries into an 8-\nbyte segment. In comparison with SFTL, LeaFTL achieves up to\n5.3Ã—(2.9Ã—on average) reduction on the address mapping table for\ndifferent storage workloads, when we set its ğ›¾=0(i.e., the learned\n1We do not compare LeaFTL with block-level and hybrid-level mappings, as they\nperform dramatically worse than the page-level mapping [20, 25].\nMSR-hm MSR-src2 MSR-prxy MSR-prn MSR-usr FIU-home FIU-mail0.00.51.0Normalized Perf.DFTL SFTL LeaFTL(a) SSD performance when using its DRAM mainly for the address\nmapping table (lower is better).\nMSR-hm MSR-src2 MSR-prxy MSR-prn MSR-usr FIU-home FIU-mail0.00.51.0Normalized Perf.DFTL SFTL LeaFTL\n(b) SSD performance when using its DRAM partially (up to 80%) for\nthe address mapping table (lower is better).\nFigure 16: Performance improvement with LeaFTL.\nSEATS AMark TPCC OLTP CompF0.00.20.40.60.81.0Normalized Perf.DFTL SFTL LeaFTL\nFigure 17: Performance on the real SSD prototype.\n99.9% 99% 90% 60% 30%0%\nPercentage of Storage Accesses100101102103Latency ( s)\nDFTL SFTL LeaFTL\nFigure 18: The latency distribution of storage accesses when\nrunning OLTP workload on the real SSD prototype.\nsegments are 100% accurate). This is because LeaFTL captures more\nLPA-PPA mapping patterns.\nWe now evaluate the performance benefit of LeaFTL from its\nsaved memory space. We evaluate LeaFTL with two experimental\nsettings: (1) the SSD DRAM is mainly used (as much as possible)\nfor the mapping table; (2) the SSD DRAM is partially used for the\nmapping table, in which we ensure at least 20% of the DRAM will\nbe used for the data caching.\nIn the first setting, DRAM is almost used for mapping table in\nDFTL. As shown in Figure 16 (a), LeaFTL reduces the storage access\nlatency by 1.6Ã—on average (up to 2.7 Ã—), compared to SFTL. This\nis because LeaFTL saves more memory from the mapping table\n\nLeaFTL: A Learning-based Flash-Translation Layer for Solid-State Drives\nMSR-hmMSR-src2 MSR-prxy MSR-prn MSR-usrFIU-home FIU-mailSEATS AMark TPCC OLTPCompF0.00.20.40.60.81.0Memory Footprint \nReduction=0\n=1\n=4\n=16\nSSD Simulator Real SSD\nFigure 19: The reduction of the mapping table size of LeaFTL\nwith different ğ›¾(lower is better).\n=0\n=1\n=4\n=16\n0%20%40%60%80%100%Percentage of\nSegmentsAccurate Approximate\nFigure 20: The distribution of learned segments.\nthan SFTL. SFTL slightly outperforms DFTL, because it reduces the\nmapping table size by compressing mapping entries with grouping\nstrictly sequential data accesses. In the second setting, as shown in\nFigure 16 (b), LeaFTL obtains 1.4 Ã—(up to 3.4Ã—) and 1.6Ã—(up to 4.9Ã—)\nperformance speedup, compared to SFTL and DFTL, respectively.\n4.3 Benefits on the Real SSD Prototype\nWe validate the benefits of LeaFTL on the real SSD prototype with\nreal workloads (see Table 2). They include filesystem benchmark\nsuite FileBench [59], and transactional database workloads from\nBenchBase [13, 61] . All these workloads run on the ext4 file system.\nWith FileBench, we run OLTP and CompFlow (CompF) workloads\nto read/write 10GB files. With BenchBase, we run TPCC, Auction-\nMark (AMark), and SEATS workloads on MySQL, and their data-\nbase sizes are 10â€“30GB. These database workloads will generate\n37â€“230GB read traffic and 26â€“59GB write traffic to the SSD. We allo-\ncate 256MB DRAM to host the mapping table (for different DRAM\nsizes, see our sensitivity analysis in Â§4.4).\nWe present the performance benefit of LeaFTL in Figure 17.\nAcross all workloads, LeaFTL obtains 1.4Ã—performance speedup\non average (up to 1.5 Ã—), compared to SFTL and DFTL. Similar to\nour evaluation with the SSD simulator implementation, the per-\nformance benefit of LeaFTL comes from the memory saving from\nthe address mapping table. And LeaFTL demonstrates comparable\nperformance improvement on real SSD devices, in comparison with\nthe SSD simulator in Â§4.2. We also show the latency distribution of\nstorage accesses in Figure 18, when running the OLTP workload on\nthe real SSD prototype. In comparison with existing FTL schemes,\nLeaFTL does not increase the tail latency of storage accesses. And\nthe higher cache hit ratio of LeaFTL brings latency reduction for\nmany storage accesses.\n4.4 Sensitivity Analysis\nVary the value of ğ›¾.As we increase the value of ğ›¾from 0 to\n16, the size of the learned mapping table is reduced, as shown in\nMSR-hmMSR-src2 MSR-prxy MSR-prn MSR-usrFIU-home FIU-mailSEATS AMark TPCC OLTPCompF0.00.20.40.60.81.0Normalized Perf.=0\n=1\n=4\n=16\nSSD Simulator Real SSDFigure 21: Performance with various ğ›¾(lower is better).\n256MB 512MB 1024MB\n(a) Various DRAM size0.00.51.0Normalized Perf.\n4KB 8KB 16KB\n(b) Various flash page size0.00.51.0Normalized Perf.DFTL SFTL LeaFTL\nFigure 22: SSD performance with different DRAM capacity\nand flash page size (lower is better).\nFigure 19. LeaFTL achieves 1.3 Ã—reduction on average (1.2 Ã—on\nthe real SSD) with ğ›¾=16, compared to that of ğ›¾=0. The saved\nmemory with a larger ğ›¾is achieved by learning a wider range\nof LPAs into approximate segments. To further understand this,\nwe profile the distribution of segments learned by LeaFTL with\ndifferent values of ğ›¾, as shown in Figure 20. When ğ›¾=0, all the\nsegments are accurate. When ğ›¾=16, 26.5% of the learned segments\nare approximate on average, and LeaFTL delivers 1.3 Ã—improvement\non storage performance (1.2 Ã—with workloads on the real SSD), in\ncomparison with the case of ğ›¾=0(see Figure 21).\nVary the SSD DRAM capacity . We now conduct the sensitivity\nanalysis of SSD DRAM by varying its capacity from 256MB to 1GB\non the real SSD prototype. As shown in Figure 22 (a), LeaFTL always\noutperforms DFTL and SFTL as we vary the SSD DRAM capacity.\nAs we increase the DRAM capacity, the storage workloads are still\nbottlenecked by the available memory space for the data caching.\nLeaFTL can learn various data access patterns and significantly\nreduce the address mapping table size, the saved memory further\nbenefits data caching.\nVary the flash page size. In this experiment, we fix the number\nof flash pages, and vary the flash page size from 4KB to 16KB in the\nSSD simulator, as SSD vendors usually use larger flash pages for\nincreased SSD capacity. We use the simulator for this study, since\nthe flash page size of the real SSD is fixed. As shown in Figure 22\n(b), LeaFTL always performs the best in comparison with DFTL and\nSFTL. As we increase the flash page size to 16KB, we can cache less\nnumber of flash pages with limited DRAM capacity. Thus, LeaFTL\nexperiences a slight performance drop. As we fix the total SSD\n\nJinghan Sun, Shaobo Li, Yunxin Sun, Chao Sun, Dejan Vucinic, and Jian Huang\n1 5 10 15 20 25 30 35\n(a) Number of Levels99.99%99.9%99%90%0%Percentage of\nLookupsMSR-prn\nMSR-usr\nMSR-src2\nMSR-hm\nMSR-prxy\nFIU-home\nFIU-mail\n0.0 0.5 1.0 1.5\n(b) LPA Lookup Overhead (%)99.99%99.9%99%90%0%Percentage of\nLookupsSEATS      \nCompF\nOLTP\nTPCC\nAMark\nFigure 23: Performance overhead of the LPA lookup.\nMSR-hmMSR-src2 MSR-prxy MSR-prn MSR-usrFIU-home FIU-mailSEATS AMark TPCC OLTPCompF05101520Misprediction (%)=0\n=1\n=4\n=16\nSSD Simulator Real SSD\nFigure 24: Misprediction ratio of flash pages access.\ncapacity and vary the page size, LeaFTL outperforms SFTL by 1.2 Ã—\nand 1.1Ã—for the page size of 8KB and 16KB, respectively.\n4.5 Overhead Source in LeaFTL\nWe evaluate the overhead sources in LeaFTL in three aspects: (1)\nthe performance overhead of the learning procedure in LeaFTL;\n(2) the LPA lookup overhead in the learned segments; and (3) the\noverhead caused by the address misprediction in LeaFTL.\nWe evaluate the performance of segment learning and address\nlookup on an ARM Cortex-A72 core. This core is similar to the\nstorage processor used in modern SSDs. The learning time for a\nbatch of 256 mapping entries is 9.8â€“10.8 ğœ‡s (see Table 3). As we\nlearn one batch of index segments for every 256 flash writes, the\nlearning overhead is only 0.02% of their flash write latency.\nIn LeaFTL, the LPA lookup is 40.2â€“67.5 ns, as the binary search of\nsegments is fast and some segments can be cached in the processor\ncache. The lookup time is slightly higher as we increase ğ›¾, due to the\nadditional CRB accesses. We also profile the cumulative distribution\nfunction (CDF) of the number of levels to lookup for each LPA\nlookup, and present the results in Figure 23 (a). For most of the\ntested workloads, 90%of the mapping table lookup can be fulfilled\nat the topmost level, and 99%of the lookups are within 10 levels.\nAlthough MSR-prn workload requires more lookups than other\nworkloads, it only checks 1.4levels on average. We also evaluate\nthe performance overhead of the LPA lookup on the real SSD, and\nshow the results in Figure 23 (b). The extra lookup overhead for each\nflash read is 0.21%on average. And for 99.99%of all the lookups,\nthe additional overhead is less than 1%of the flash access latency.Table 3: Overhead source of LeaFTL with an ARM core.\nğ›¾ 0 1 4\nLearning (256 LPAs) 9.8ğœ‡s 10.8ğœ‡s 10.8ğœ‡s\nLookup (per LPA) 40.2 ns 60.5 ns 67.5 ns\nLeaFTL also has low misprediction ratios with approximate seg-\nments. This is because LeaFTL can still learn accurate segments\neven ifğ›¾>0, and not all entries in the approximate segments\nwill result in misprediction. As shown in Figure 24, most of the\nworkloads achieve less than 10% misprediction ratio when ğ›¾=16.\nWe obtain similar misprediction ratio on the real SSD prototype.\nNote that each misprediction only incurs one flash read access with\nthe help of our proposed OOB verification.\n4.6 Impact on SSD Lifetime\nThe flash blocks of an SSD can only undergo a certain amount of\nwrites. In this experiment, we use the write amplification factor\n(WAF, the ratio between the actual and requested flash writes) to\nevaluate the SSD lifetime. The SSD will age faster if the WAF is\nlarger. As shown Figure 25, the WAF of LeaFTL is comparable to\nDFTL and SFTL. DFTL has larger WAF in most workloads. SFTL\nand LeaFTL occasionally flush translation pages to the flash blocks,\nbut the cost is negligible.\n5 DISCUSSION\nWhy Linear Regression. Unlike deep neural networks, the lin-\near regression used in LeaFTL is simple and lightweight, which\ntakes only a few microseconds to learn an index segment with\nembedded ARM processors available in modern SSD controllers.\nIn addition, the linear regression algorithm has been well studied,\nand offers guaranteed error bounds for its learned results. LeaFTL\nis the first work that uses learning techniques to solve a critical\nsystem problem (i.e., address mapping) in SSDs.\nAdaptivity of LeaFTL. LeaFTL focuses on the page-level address\ntranslation, its design and implementation will not be affected by\nthe low-level flash memory organization (i.e., TLC/QLC). As we\nuse TLC/QLC technique to further increase the SSD capacity, the\naddress mapping issue will become more critical, since the SSD\nDRAM capacity does not scale well and becomes the bottleneck for\ncaching address mappings and user data.\nRecovery of Learned Index Segments. As discussed inÂ§3.8, us-\ning a battery or large capacitor to preserve and persist the cached\nsegments upon failures or crashes will simplify the recovery pro-\ncedure significantly. In our real SSD prototype, we do not assume\nthe battery-backed DRAM is available. Thus, we follow the conven-\ntional recovery approach in modern SSDs [20, 23] , and scan flash\nblocks in parallel by utilizing the channel-level parallelism.\nWhen we run real workloads like TPCC on the SSD prototype,\nwe intentionally reboot the system after running the workload for\na period of time (0.5-3 hours). We find that the system can recover\nin 15.8 minutes on average whenever the reboot happens. This\nis similar to the time of recovering the conventional page-level\nmapping table in DFTL [20]. This is mostly caused by scanning the\nblocks in a channel (70MB/s per channel in our SSD prototype),\nand the time for reconstructing recently learned segments is rela-\ntively low (101.3 milliseconds on average). We believe the recovery\n\nLeaFTL: A Learning-based Flash-Translation Layer for Solid-State Drives\nMSR-hmMSR-src2 MSR-prxy MSR-prn MSR-usrFIU-home FIU-mailSEATS AMark TPCC OLTPCompF0.00.51.01.5Write \nAmplificationDFTL SFTL LeaFTL\nSSD Simulator Real SSD\nFigure 25: Write amplification factor of LeaFTL.\ntime is not much of a concern as the recovery does not happen\nfrequently in reality. And the recovery can be accelerated as we\nincrease the channel-level bandwidth. In addition, if an SSD can\ntolerate more data losses, we can still ensure the crash consistency\nby only loading the stored index segments from flash chips, which\nrequires minimum recovery time.\n6 RELATED WORK\nAddress Translation for SSDs. A variety of FTL optimizations\nhave been proposed [ 8,12,20,25,28,34,49,50]. These works ex-\nploited the data locality of flash accesses to improve the cache\nefficiency of the mapping table. However, most of them were devel-\noped with human-driven heuristics. An alternative approach is to\nintegrate application semantics into the FTL, such as content-aware\nFTL [ 7]. However, they were application specific and required signif-\nicant changes to the FTL. LeaFTL is a generic solution and does not\nrequire application semantics in its learning. Researchers proposed\nto integrate the FTL mapping table into the host [ 18,23,26,66]. Typi-\ncal examples include DFS [ 26], Nameless writes [ 66], FlashMap [ 23],\nand FlatFlash [ 4]. LeaFTL is orthogonal to them and can be applied\nto further reduce their memory footprint.\nMachine Learning for Storage. Recent studies have been using\nlearning techniques to build indexes such as B-trees, log-structured\nmerge tree, hashmaps, and bloom filters [ 11,14,15,32,33,42]\nfor in-memory datasets, identify optimal cache replacement and\nprefetching policies [ 40,53,56,57], facilitate efficient storage har-\nvesting [ 52], and drive the development of software-defined stor-\nage [ 24]. LeaFTL applies learning techniques to optimize the address\nmapping. However, unlike existing optimizations [ 43,63] such as\nlearned page table for virtual memory that used deep neural net-\nworks to learn the patterns, LeaFTL provides a lightweight solution.\nSSD Hardware Development. For the recent SSD innovations [ 3,\n17,19,47] like Z-SSD [ 55], KVSSD [ 35], and ZNS SSD [ 21], DRAM\ncapacity and storage processor are still the main constraints in SSD\ncontrollers. As we scale the storage capacity, the challenge with\nthe address translation becomes only worse. Researchers recently\ndeployed hardware accelerators inside SSD controllers for near-\ndata computing [ 36,41,54,58]. We wish to extend LeaFTL with\nin-storage accelerators to deploy more powerful learning models\nas the future work.\n7 CONCLUSION\nWe present a learning-based flash translation layer, named LeaFTL\nfor SSDs. LeaFTL can automatically learn different flash access\npatterns and build space-efficient indexes, which reduces the ad-\ndress mapping size and improves the caching efficiency in the SSDcontroller. Our evaluation shows that LeaFTL improves the SSD\nperformance by 1.4Ã—on average for a variety of storage workloads.\nACKNOWLEDGMENTS\nWe thank the anonymous reviewers for their helpful comments\nand feedback. This work is partially supported by the NSF CAREER\nAward 2144796, CCF-1919044, and CNS-1850317.\nREFERENCES\n[1]2019. A Closer Look At SSD Power Loss Protection. https://www.kingston.com/\nen/blog/servers-and-data-centers/ssd-power-loss-protection.\n[2]2020. Harnessing Microcontrollers to Deliver Intelligent SSD Power Management\nand PLP Capabilities. https://www.atpinc.com/de/about/stories/microcontroller-\nSSD-power-loss-protection.\n[3] 3D NAND â€“ An Overview. 2022.\nhttps://www.simms.co.uk/tech-talk/3d-nand-overview/.\n[4]Ahmed Abulila, Vikram Sharma Mailthoday, Zaid Qureshi, Jian Huang, Nam Sung\nKim, Jin jun Xiong, and Wen mei Hwu. 2019. FlatFlash: Exploiting the Byte-\nAccessibility of SSDs within A Unified Memory-Storage Hierarchy. In Proceedings\nof the 24th ACM International Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOSâ€™19) . Providence, RI.\n[5]Nitin Agrawal, Vijayan Prabhakaran, Ted Wobber, John D. Davis, Mark Manasse,\nand Rina Panigrahy. 2008. Design Tradeoffs for SSD Performance. In Proceedings\nof the USENIX 2008 Annual Technical Conference (ATCâ€™08) . Boston, Massachusetts.\n[6]Yu Cai, Saugata Ghose, Erich F Haratsch, Yixin Luo, and Onur Mutlu. 2017. Error\ncharacterization, mitigation, and recovery in flash-memory-based solid-state\ndrives. Proc. IEEE 105, 9 (2017), 1666â€“1704.\n[7]Feng Chen, Tian Luo, and Xiaodong Zhang. 2011. CAFTL: A Content-Aware\nFlash Translation Layer Enhancing the Lifespan of Flash Memory based Solid\nState Drives. In Proceedings of the 9th USENIX Conference on File and Storage\nTechnologies (FASTâ€™11) . San Jose, CA.\n[8]Renhai Chen, Zhiwei Qin, Yi Wang, Duo Liu, Zili Shao, and Yong Guan. 2014. On-\ndemand block-level address mapping in large-scale NAND flash storage systems.\nIEEE Trans. Comput. 64, 6 (2014), 1729â€“1741.\n[9]Tae-Sun Chung, Dong-Joo Park, and Jongik Kim. 2011. LSTAFF*: An Efficient\nFlash Translation Layer for Large Block Flash Memory. In Proceedings of the 2011\nACM Symposium on Applied Computing (SACâ€™11) . TaiChung Taiwan.\n[10] Curtis R Cook and Do Jin Kim. 1980. Best sorting algorithm for nearly sorted\nlists. Commun. ACM 23, 11 (1980), 620â€“624.\n[11] Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth,\nAndrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau. 2020. From WiscKey to\nBourbon: A Learned Index for Log-Structured Merge Trees. In Proceedings of\nthe 14th USENIX Symposium on Operating Systems Design and Implementation\n(OSDIâ€™20) . Virtual Event.\n[12] Niv Dayan, Philippe Bonnet, and Stratos Idreos. 2016. GeckoFTL: Scalable Flash\nTranslation Techniques For Very Large Flash Devices. In Proceedings of the Inter-\nnational Conference on Management of Data (SIGMODâ€™16) . San Francisco, CA.\n[13] Djellel Eddine Difallah, Andrew Pavlo, Carlo Curino, and Philippe CudrÃ©-\nMauroux. 2013. OLTP-Bench: An Extensible Testbed for Benchmarking Relational\nDatabases. PVLDB 7, 4 (2013).\n[14] Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. 2020. Why Are Learned\nIndexes So Effective?. In Proceedings of the 37th International Conference on\nMachine Learning (ICMLâ€™20) . Virtual Event.\n[15] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-Index: A Fully-Dynamic\nCompressed Learned Index with Provable Worst-Case Bounds. Proceedings of\nthe VLDB Endowment 13, 8 (April 2020).\n[16] FIU. 2009. FIU Server Traces.\n[17] Flash Memory. 2022. https://en.wikipedia.org/wiki/Flash_memory.\n[18] Fusion-io Directcache: Transparent Storage Accelerator. 2011.\nhttp://www.fusionio.com/systems/directcache/.\n[19] Gartner. 2017. Forecast Overview: NAND Flash, Worldwide, 2017. https:\n//www.gartner.com/doc/3745121/forecast-overview-nand-flash-worldwide\n[20] Aayush Gupta, Youngjae Kim, and Bhuvan Urgaonkar. 2009. DFTL: A Flash\nTranslation Layer Employing Demand-based Selective Caching of Page-level\nAddress Mappings. In Proceedings of the 14th International Conference on Archi-\ntectural Support for Programming Languages and Operating Systems (ASPLOSâ€™09) .\nWashington, DC.\n[21] Kyuhwa Han, Hyunho Gwak, Dongkun Shin, and Joo-Young Hwang. 2021. ZNS+:\nAdvanced Zoned Namespace Interface for Supporting In-Storage Zone Com-\npaction. In 15th{USENIX}Symposium on Operating Systems Design and Imple-\nmentation (OSDIâ€™21) . 147â€“162.\n[22] Jian Huang, Anirudh Badam, Laura Caulfield, Suman Nath, Sudipta Sengupta,\nBikash Sharma, and Moinuddin K. Qureshi. 2017. FlashBlox: Achieving Both\nPerformance Isolation and Uniform Lifetime for Virtualized SSDs. In Proceedings\n\nJinghan Sun, Shaobo Li, Yunxin Sun, Chao Sun, Dejan Vucinic, and Jian Huang\nof the 15th Usenix Conference on File and Storage Technologies (FASTâ€™17) . Santa\nclara, CA.\n[23] Jian Huang, Anirudh Badam, Moinuddin K. Qureshi, and Karsten Schwan. 2015.\nUnified Address Translation for Memory-mapped SSDs with FlashMap. In Pro-\nceedings of the 42nd Annual International Symposium on Computer Architecture\n(ISCAâ€™15) . Portland, OR.\n[24] Jian Huang, Daixuan Li, and Jinghan Sun. 2022. Learning to Drive Software-\nDefined Storage. Workshop on Machine Learning for Systems at NIPSâ€™22 (2022).\n[25] Song Jiang, Lei Zhang, XinHao Yuan, Hao Hu, and Yu Chen. 2011. S-FTL: An\nEfficient Address Translation for Flash Memory by Exploiting Spatial Locality.\nInProceedings of the 2011 IEEE 27th Symposium on Mass Storage Systems and\nTechnologies (MSSTâ€™11) . IEEE Computer Society.\n[26] William K. Josephson, Lars A. Bongo, Kai Li, and David Flynn. 2010. DFS: A\nFile System for Virtualized Flash Storage. ACM Trans. on Storage 6, 3 (2010),\n14:1â€“14:25.\n[27] Jun He, Sudarsun Kannan, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau.\n2017. The Unwritten Contract of Solid State Drives. In Proceedings of the Twelfth\nEuropean Conference on Computer Systems (EuroSysâ€™17) . Belgrade, Serbia.\n[28] Dawoon Jung, Jeong-UK Kang, Heeseung Jo, Jin-Soo Kim, and Joonwon Lee.\n2010. Superblock FTL: A superblock-based flash translation layer with a hybrid\naddress translation scheme. ACM Transactions on Embedded Computing Systems\n(TECS) 9, 4 (2010), 1â€“41.\n[29] Jeong-Uk Kang, Heeseung Jo, Jinsoo Kim, and Joonwon Lee. 2006. A Superblock-\nBased Flash Translation Layer for NAND Flash Memory. In Proceedings of the\n6th International Conference on Embedded Software (EMSOFTâ€™06) . Seoul, South\nKorea.\n[30] Luyi Kang, Yuqi Xie, Weiwei Jia, Xiaohao Wang, Jongryool Kim, Changhwan\nYoun, Myeong Joon Kang, Jin Lim, Bruce Jacob, and Jian Huang. 2021. IceClave: A\nTrusted Execution Environment for In-Storage Computing. In Proceedings of the\n54th Annual IEEE/ACM International Symposium on Microarchitecture (MICROâ€™21) .\nVirtual Event.\n[31] Jesung Kim, Jong Min Kim, S.H. Noh, Sang Lyul Min, and Yookun Cho. 2002. A\nspace-efficient flash translation layer for CompactFlash systems. IEEE Transac-\ntions on Consumer Electronics 48, 2 (2002).\n[32] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: A Single-Pass Learned\nIndex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management (aiDM â€™20) . Portland, Oregon.\n[33] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data (SIGMODâ€™18) . Houston, TX, USA.\n[34] Hunki Kwon, Eunsam Kim, Jongmoo Choi, Donghee Lee, and Sam H Noh. 2010.\nJanus-FTL: Finding the optimal point on the spectrum between page and block\nmapping schemes. In Proceedings of the tenth ACM international conference on\nEmbedded software . 169â€“178.\n[35] Samsung Memory Solutions Lab. 2017. Samsung Key Value SSD enables High Per-\nformance Scaling. https://www.samsung.com/semiconductor/global.semi.static/\nSamsung_Key_Value_SSD_enables_High_Performance_Scaling-0.pdf (2017).\n[36] Joo Hwan Lee, Hui Zhang, Veronica Lagrange, Praveen Krishnamoorthy, Xi-\naodong Zhao, and Yang Seok Ki. 2020. SmartSSD: FPGA accelerated near-storage\ndata analytics on SSD. IEEE Computer architecture letters 19, 2 (2020), 110â€“113.\n[37] Sungjin Lee, Ming Liu, Sangwoo Jun, Shuotao Xu, Jihong Kim, and Arvind. 2016.\nApplication-managed flash. In Proceedings of the 14th USENIX Conference on File\nand Storage Technologies (FASTâ€™16) . 339â€“353.\n[38] Sungjin Lee, Dongkun Shin, Young-Jin Kim, and Jihong Kim. 2008. LAST: Locality-\nAware Sector Translation for NAND Flash Memory-Based Storage Systems. In\nProceedings of the SIGOPS Operating Systems Review (2008).\n[39] Sang-Won Lee, Dong-Joo Park, Tae-Sun Chung, Dong-Ho Lee, Sangwon Park,\nand Ha-Joo Song. 2007. A Log Buffer-Based Flash Translation Layer Using\nFully-Associative Sector Translation. ACM Transactions on Embedded Computing\nSystems 6, 3 (2007), 18:1â€“18:27.\n[40] Evan Liu, Milad Hashemi, Kevin Swersky, Parthasarathy Ranganathan, and Jun-\nwhan Ahn. 2020. An imitation learning approach for cache replacement. In\nInternational Conference on Machine Learning . PMLR, 6237â€“6247.\n[41] Vikram Sharma Mailthoday, Zaid Qureshi, Weixin Liang, Ziyan Feng, Simon Gar-\ncia de Gonzalo, Youjie Li, Hubertus Franke, Jinjun Xiong, Jian Huang, and Wen\nmei Hwu. 2019. DeepStore: In-Storage Acceleration for Intelligent Queries. In\nProceedings of the 52nd IEEE/ACM International Symposium on Microarchitecture\n(MICROâ€™19) . Columbus, OH.\n[42] Ryan Marcus, Emily Zhang, and Tim Kraska. 2020. CDFShop: Exploring and\nOptimizing Learned Index Structures. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data (SIGMODâ€™20) . Portland, OR, USA.\nhttps://doi.org/10.1145/3318464.3384706\n[43] Artemiy Margaritov, Dmitri Ustiugov, Edouard Bugnion, and Boris Grot. 2018.\nVirtual Address Translation via Learned Page Table Indexes. In Proceedings of\nthe Workshop on ML for Systems at NeurIPS . Montreal, Canada.[44] Kiran Kumar Matam, Gunjae Koo, Haipeng Zha, Hung-Wei Tseng, and Murali\nAnnavaram. 2019. GraphSSD: Graph Semantics Aware SSD. In Proceedings of\nthe 46th International Symposium on Computer Architecture (ISCAâ€™19) . Phoenix,\nArizona.\n[45] Microsoft. 2007. MSR Cambridge Traces.\n[46] Jian Ouyang, Shiding Lin, Song Jiang, Yong Wang, Wei Qi, Jason Cong, and\nYuanzheng Wang. 2014. SDF: Software-Defined Flash for Web-Scale Internet\nStorage Systems. In Proceedings of 19th International Conference on Architectural\nSupport for Programming Language and Operating Systems (ASPLOSâ€™14) . Salt Lake\nCity, UT.\n[47] Over 50 years of development history of Flash Memory Technology. 2019.\nhttps://www.elinfor.com/knowledge/over-50-years-of-development-history-\nof-flash-memory-technology-p-11271.\n[48] Nikolaos Papandreou, Haralampos Pozidis, Nikolas Ioannou, Thomas Parnell,\nRoman Pletka, Milos Stanisavljevic, Radu Stoica, Sasa Tomic, Patrick Breen, Gary\nTressler, et al .2020. Open block characterization and read voltage calibration of\n3D QLC NAND flash. In 2020 IEEE International Reliability Physics Symposium\n(IRPS) . IEEE, 1â€“6.\n[49] Chanik Park, Wonmoon Cheon, Jeonguk Kang, Kangho Roh, Wonhee Cho, and\nJin-Soo Kim. 2008. A reconfigurable FTL (flash translation layer) architecture\nfor NAND flash-based applications. ACM Transactions on Embedded Computing\nSystems (TECS) 7, 4 (2008), 1â€“23.\n[50] Zhiwei Qin, Yi Wang, Duo Liu, and Zili Shao. 2010. Demand-based block-level\naddress mapping in large-scale NAND flash storage systems. In Proceedings of\nthe eighth IEEE/ACM/IFIP international conference on Hardware/software codesign\nand system synthesis .\n[51] Benjamin Reidys, Peng Liu, and Jian Huang. 2022. RSSD: Defend against Ran-\nsomware with Hardware-Isolated Network-Storage Codesign and Post-Attack\nAnalysis. In Proceedings of the 27th ACM International Conference on Architec-\ntural Support for Programming Languages and Operating Systems (ASPLOSâ€™22) .\nLausanne, Switzerland.\n[52] Benjamin Reidys, Jinghan Sun, Anirudh Badam, Shadi Noghabi, and Jian Huang.\n2022. BlockFlex: Enabling Storage Harvesting with Software-Defined Flash\nin Modern Cloud Platforms. In Proceedings of the 16th USENIX Symposium on\nOperating Systems Design and Implementation (OSDIâ€™22) . Carlsbad, CA.\n[53] Liana V Rodriguez, Farzana Yusuf, Steven Lyons, Eysler Paz, Raju Rangaswami,\nJason Liu, Ming Zhao, and Giri Narasimhan. 2021. Learning Cache Replacement\nwith CACHEUS. In 19th USENIX Conference on File and Storage Technologies\n(FASTâ€™21) . 341â€“354.\n[54] Zhenyuan Ruan, Tong He, and Jason Cong. 2019. INSIDER: Designing In-Storage\nComputing System for Emerging High-Performance Drive. In Proceedings of the\n2019 USENIX Annual Technical Conference (USENIX ATCâ€™19) . Renton, WA.\n[55] Samsung Z-NAND. 2019. https://www.samsung.com/semiconductor/ssd/z-ssd/.\n[56] Subhash Sethumurugan, Jieming Yin, and John Sartori. 2021. Designing a Cost-\nEffective Cache Replacement Policy using Machine Learning. In 2021 IEEE Inter-\nnational Symposium on High-Performance Computer Architecture (HPCA) . IEEE,\n291â€“303.\n[57] Zhan Shi, Xiangru Huang, Akanksha Jain, and Calvin Lin. 2019. Applying deep\nlearning to the cache replacement problem. In Proceedings of the 52nd Annual\nIEEE/ACM International Symposium on Microarchitecture . 413â€“425.\n[58] smartssd 2018. SmartSSD Computational Storage Drive. https://www.xilinx.com/\napplications/data-center/computational-storage/smartssd.html.\n[59] Vasily Tarasov, Erez Zadok, and Spencer Shepler. 2016. Filebench: A flexible\nframework for file system benchmarking. The USENIX Magazine 41, 1 (2016).\n[60] Usman Saleem, Advanced SSD Buying Guide - NAND Types, DRAM Cache, HMB\nExplained. 2022. https://appuals.com/ssd-buying-guide/.\n[61] Dana Van Aken, Djellel E. Difallah, Andrew Pavlo, Carlo Curino, and Philippe\nCudrÃ©-Mauroux. 2015. BenchPress: Dynamic Workload Control in the OLTP-\nBench Testbed. In Proceedings of the 2015 ACM SIGMOD International Conference\non Management of Data (SIGMODâ€™15) .\n[62] Xiaohao Wang, Yifan Yuan, You Zhou, Chance C. Coats, and Jian Huang. 2019.\nProject Almanac: A Time-Traveling Solid-State Drive. In Proceedings of the 14th\nEuropean Conference on Computer Systems (EuroSysâ€™19) . Dresden, Germany.\n[63] Nan Wu and Yuan Xie. 2021. A Survey of Machine Learning for Computer\nArchitecture and Systems. CoRR abs/2102.07952 (2021). https://arxiv.org/abs/\n2102.07952\n[64] Qing Xie, Chaoyi Pang, Xiaofang Zhou, Xiangliang Zhang, and Ke Deng. 2014.\nMaximum Error-Bounded Piecewise Linear Representation for Online Stream\nApproximation. Proceedings of the VLDB Journal 23, 6 (Dec. 2014).\n[65] Qing Xie, Chaoyi Pang, Xiaofang Zhou, Xiangliang Zhang, and Ke Deng. 2014.\nMaximum error-bounded piecewise linear representation for online stream ap-\nproximation. The VLDB journal 23, 6 (2014), 915â€“937.\n[66] Yiying Zhang, Leo Prasath Arulraj, Andrea C. Arpaci-Dusseau, and Remzi H.\nArpaci-Dusseau. 2012. De-indirection for Flash-based SSDs with Nameless Writes.\nInProceedings of the 10th USENIX Conference on File and Storage Technologies\n(FASTâ€™12) . San Jose, CA.",
  "textLength": 80485
}