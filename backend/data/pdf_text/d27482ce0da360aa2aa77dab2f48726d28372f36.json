{
  "paperId": "d27482ce0da360aa2aa77dab2f48726d28372f36",
  "title": "Index Network",
  "pdfPath": "d27482ce0da360aa2aa77dab2f48726d28372f36.pdf",
  "text": "MANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 1\nIndex Networks\nHao Lu, Yutong Dai, Chunhua Shen, and Songcen Xu\nAbstract —We show that existing upsampling operators can be uniﬁed using the notion of the index function. This notion is inspired by\nan observation in the decoding process of deep image matting where indices-guided unpooling can often recover boundary details\nconsiderably better than other upsampling operators such as bilinear interpolation. By viewing the indices as a function of the feature\nmap, we introduce the concept of ‘learning to index’, and present a novel index-guided encoder-decoder framework where indices are\nlearned adaptively from data and are used to guide downsampling and upsampling stages, without extra training supervision. At the\ncore of this framework is a new learnable module, termed Index Network (IndexNet), which dynamically generates indices conditioned\non the feature map. IndexNet can be used as a plug-in applicable to almost all convolutional networks that have coupled downsampling\nand upsampling stages, enabling the networks to dynamically capture variations of local patterns. In particular, we instantiate,\ninvestigate ﬁve families of IndexNet, highlight their superiority in delivering spatial information over other upsampling operators with\nexperiments on synthetic data, and demonstrate their effectiveness on four dense prediction tasks, including image matting, image\ndenoising, semantic segmentation, and monocular depth estimation. Code and models are available at: https://git.io/IndexNet.\nIndex Terms —Upsampling Operators, Dynamic Networks, Image Denoising, Semantic Segmentation, Image Matting, Depth\nEstimation\nF\n1 I NTRODUCTION\nUPSAMPLING is an essential stage for dense predic-\ntion tasks using deep convolutional neural networks\n(CNNs). The frequently used upsampling operators include\ntransposed convolution [1], [2], unpooling [3], periodic shuf-\nﬂing [4] (a.k.a. depth-to-space), and naive interpolation [5],\n[6] followed by convolution. These operators, however, are\nnot general-purpose designs and often exhibit different be-\nhaviors in different tasks.\nThe widely-adopted upsampling operator in semantic\nsegmentation and depth estimation is bilinear interpolation,\nwhile unpooling is less popular. A reason might be that\nthe feature map generated by max unpooling is sparse,\nwhile the bilinearly interpolated feature map has dense and\nconsistent representations for local regions (compared to\nthe feature map before interpolation). This is particularly\ntrue for semantic segmentation and depth estimation where\npixels in a region often share the same class label or have\nsimilar depth. However, we observe that bilinear interpo-\nlation can perform signiﬁcantly worse than unpooling in\nboundary-sensitive tasks such as image matting. A fact\nis that the leading deep image matting model [7] largely\nborrows the design from the SegNet method [3], where\nunpooling was ﬁrst introduced. When adapting other state-\nof-the-art segmentation models, such as DeepLabv3+ [6]\nand ReﬁneNet [5], to this task, we observe that they tend\nto fail to recover boundary details (Fig. 1). A plausible\nexplanation is that, compared to the bilinearly upsampled\nfeature map, unpooling uses max-pooling indices to guide\nupsampling. Since boundaries in the shallow layers usually\nhave the maximum responses, indices extracted from these\nresponses record the boundary locations. The feature map\n\u000fH. Lu, Y. Dai and C. Shen are with The University of Adelaide, SA 5005,\nAustralia. Corresponding author: C. Shen.\nE-mail: ﬁrstname.lastname@adelaide.edu.au\n\u000fS. Xu is with Noah’s Ark Lab, Huawei Technologies.\nFig. 1. Alpha mattes of different models for the task of image matting.\nFrom left to right, Deeplabv3+ [6], ReﬁneNet [5], Deep Matting [7] and\nIndexNet (Ours). Bilinear upsampling tends to fail to recover subtle de-\ntails, while unpooling and our learned upsampling operator can produce\nmuch clear mattes with good local contrast.\nprojected by the indices thus shows improved boundary\ndelineation.\nWe thus believe that different upsampling operators may\nexhibit different characteristics, and we expect a speciﬁc\nbehavior of the upsampling operator when dealing with\nspeciﬁc image content for a particular vision task. A ques-\ntion of interest is: Can we design a generic operator to upsample\nfeature maps that better predict boundaries and regions simulta-\nneously? A key observation of this work is that unpooling,\nbilinear interpolation or other upsampling operators are some\nforms of index functions . For example, the nearest neighbor\ninterpolation of a point is equivalent to allocating indices of\none to its neighbor and then map the value of the point. In\nthis sense, indices are models [8], therefore indices can be\nmodeled and learned.\nIn this work, we model indices as a function of the local\nfeature map and learn index functions to implement upsampling\nwithin deep CNNs. In particular, we present a novel index-\nguided encoder-decoder framework, which naturally gener-\nalizes models like SegNet. Instead of using max-pooling and\nunpooling, we introduce indexed pooling and indexed up-\nsampling where downsampling and upsampling are guidedarXiv:1908.09895v2  [cs.CV]  5 Apr 2020\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 2\nby learned indices. The indices are generated dynamically\nconditioned on the feature map and are learned using a fully\nconvolutional network, termed IndexNet, without extra su-\npervision needed. IndexNet is a highly ﬂexible module. It\ncan be applied to almost all convolutional networks that\nhave coupled downsampling and upsampling stages. Com-\npared to the ﬁxed max function or bilinear interpolation,\nlearned index functions show potentials for simultaneous\nboundary and region delineation.\nIndexNet is a high-level concept and represents a broad\nfamily of networks modeling the so-called index function.\nIn this work, we instantiate and investigate ﬁve families\nof IndexNet. Different designs correspond to different as-\nsumptions. We compare the behavior of IndexNet with\nexisting upsampling operators and demonstrate its supe-\nriority in delivering spatial information. We show that In-\ndexNet can be incorporated into many CNNs to beneﬁt\na number of visual tasks, for instance: i) image matting :\nour MobileNetv2-based [9] model with IndexNet exhibits\nat least 16:1% improvement against the VGG-16-based\nDeepMatting baseline [7] on the Composition-1k matting\ndataset; by visualizing learned indices, the indices auto-\nmatically learn to capture the boundaries and textural pat-\nterns; ii) image denoising : a modiﬁed DnCNN model with\nIndexNet can achieve performance comparable to the base-\nline DnCNN [10] that has no downsampling stage on the\nBSD68 and Set12 datasets [11], thus reducing the compu-\ntational cost and memory consumption signiﬁcantly; iii)\nsemantic segmentation : consistently improved performance is\nobserved when SegNet [3] is equipped with IndexNet on the\nSUN RGB-D dataset [12]; and iv) monocular depth estimation :\nIndexNet also improves the performance of a recent light-\nweight FastDepth model on the NYUDv2 dataset [13], with\nnegligible extra computation cost.\nWe make the following main contributions:\n\u000fWe present a uniﬁed perspective of existing upsam-\npling operators with the notion of the index function;\n\u000fWe introduce Index Networks—a novel family of net-\nworks that can be included into standard CNNs to\nprovide dynamic, adaptive downsampling and upsam-\npling capabilities; to the best of our knowledge, In-\ndexNet is one of the ﬁrst attempts towards the design\nof generic upsampling operators;\n\u000fWe instantiate, and investigate ﬁve designs of IndexNet\nand demonstrate their effectiveness on four vision\ntasks.\nA preliminary conference version of this work appeared\nin [14]. We extend [14] by i) further investigating two light-\nweight IndexNets; ii) comparing properties and computa-\ntional complexity of IndexNets; iii) presenting a taxonomy\nof upsampling operators with the notion of guided upsam-\npling and blind upsampling; and iv) designing image re-\nconstruction experiments on synthetic data to compare two\nupsampling paradigms in recovering spatial information.\nBesides image matting in [14], we now v) apply IndexNets\nto a few more vision tasks including image denoising,\nsemantic segmentation and monocular depth estimation\nand report extensive experimental results, not only between\nindex networks but also across different vision tasks.2 L ITERATURE REVIEW\nWe review upsampling operators and a closely-related\ngroup of networks: dynamic networks.\nUpsampling in Deep Networks. Compared with other\ncomponents in the design of deep networks, downsampling\nand upsampling of feature maps are relatively less studied.\nSince learning a CNN without sacriﬁcing the spatial resolu-\ntion is computationally expensive and memory intensive,\nand suffers from limited receptive ﬁelds, downsampling\noperators are common choices, such as strided convolu-\ntion and max/average pooling. To recover the resolution,\nupsampling is thus an essential stage for almost all dense\nprediction tasks. This poses a fundamental question: What\nis the principal approach to recover the resolution of a downsam-\npled feature map (decoding) . A few upsampling operators are\nproposed. The deconvolution operator, a.k.a. transposed con-\nvolution, was initially used in [1] to visualize convolutional\nactivations and introduced to semantic segmentation [2],\nbut this operator sometimes can be harmful due to its\nbehavior in producing checkerboard artifacts [15]. To avoid\nthis, a suggestion is the “resize+convolution” paradigm,\nwhich has currently become the standard conﬁguration in\nstate-of-the-art semantic segmentation models [5], [6]. Apart\nfrom these, perforate [16] and unpooling [3] generate sparse\nindices to guide upsampling. The indices are able to capture\nand keep boundary information, but one issue is that the\ntwo operators can induce much sparsity after upsampling.\nConvolutional layers with large ﬁlter sizes must follow\nfor densiﬁcation. In addition, periodic shufﬂing (PS) was\nintroduced in [4] as a fast and memory-efﬁcient upsampling\noperator for image super-resolution. PSrecovers resolution\nby rearranging the feature map of size H\u0002W\u0002Cr2\ntorH\u0002rW\u0002C. It is also used in some segmentation\nmodels [17].\nOur work is primarily inspired by the unpooling oper-\nator [3]. We remark that, it is important to extract spatial\ninformation before its loss during downsampling, and more\nimportantly, to use stored information during upsampling.\nUnpooling shows a simple and effective use case, while we\nbelieve that there is much room to improve. Here we show\nthat unpooling is a special form of index function, and we\ncan learn an index function beyond unpooling.\nWe notice that concurrent work of [18] also pursues\nthe idea of data-dependent upsampling and proposes an\nuniversal upsampling operator termed CARAFE. Although\nthe idea is similar, IndexNet is different from CARAFE\nin several aspects. First, CARAFE does not associate up-\nsampling with the notion of the index function. Second,\nthe kernels used in CARAFE are generated conditioned\non decoder features, while IndexNet builds upon encoder\nfeatures, so the generated indices can also be used to guide\ndownsampling. Third, CARAFE can be viewed as one of\nour investigated index networks—holistic index networks,\nbut with different upsampling kernels and normalization\nstrategies. We further compare IndexNet with CARAFE in\nSection 5.2 and in experiments.\nDynamic Networks. If considering the dynamic prop-\nerty of IndexNet, IndexNet shares similarity with an in-\nteresting group of networks—dynamic networks. Dynamic\nnetworks are often implemented with adaptive modules to\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 3\nextend the modeling capabilities of CNNs. These networks\nshare the following characteristics. The output is dynamic ,\nconditioned on the input feature map. Since dynamic net-\nworks are learnable modules, they are generic in the sense\nthat they can be used as building blocks in many network\narchitectures. They are also ﬂexible to allow modiﬁcations\naccording to target tasks.\nSpatial Transformer Networks (STNs) [19]. STN allows ex-\nplicit manipulation of spatial transformation within the net-\nwork. It achieves this by regressing transformation parame-\nters\u0012with a side-branch network. A spatially-transformed\noutput is then produced by a sampler parameterized by \u0012.\nThis results in a holistic transformation of the feature map.\nThe dynamic nature of STN is reﬂected by the fact that,\ngiven different inputs, the inferred \u0012is different, allowing to\nlearn some forms of invariance to translation, scale, rotation,\netc.\nDynamic Filter Networks (DFNs) [20]. DFN implements a\nﬁlter generating network to dynamically generate kernel ﬁl-\nter parameters Compared to conventional ﬁlter parameters\nthat stay ﬁxed during inference, ﬁlter parameters in DFN\nare dynamic and sample-speciﬁc.\nDeformable Convolutional Networks (DCNs) [21]. DCNs\nintroduce deformable transformation into convolution. The\nkey idea is to predict offsets for convolutional kernels. With\noffsets, convolution can be executed on irregular sampling\ngrids, enabling adaptive manipulation of the receptive ﬁeld.\nAttention Networks [22]. Attention networks are a broad\nfamily of networks that use attention mechanisms. The\nmechanisms introduce multiplicative interactions between\nthe inferred attention map and the feature map. In computer\nvision, attention mechanisms are usually referred to spatial\nattention [23], channel attention [24] or both [25]. These\nnetwork modules are widely applied in CNNs to force\nthe network focusing on speciﬁc regions and therefore to\nreﬁne feature maps. Essentially, attention is about feature\nselection. No attentional module has been designed to deal\nwith the downsampling/upsampling stage.\nIn contrast to above dynamic networks, IndexNet fo-\ncuses on upsampling, rather than manipulating ﬁlters or\nreﬁning features. Akin to dynamic networks above, the dy-\nnamics in IndexNet also has a physical deﬁnition—indices.\nSuch a deﬁnition also closely relates to attention networks.\nLater we show that the downsampling and upsampling op-\nerators used with IndexNet can, to some extent, be viewed\nas attentional operators. Indeed, max-pooling indices are a\nform of hard attention. It is worth noting that, despite that\nIndexNet in its current implementation may closely relate\nto attention, it focuses on upsampling rather than reﬁning\nfeature maps. IndexNet also shares the other characteristics\nmentioned above. It is implemented in a convolutional side-\nbranch network, is trained without extra supervision and\nis generic and ﬂexible. We demonstrate its effectiveness on\nfour dense prediction tasks and ﬁve variants of IndexNet.\n3 A NINDEXING PERSPECTIVE OF UPSAMPLING\nWith the argument that upsampling operators are index\nfunctions, here we offer a uniﬁed indexing perspective of\nupsampling operators. The unpooling operator is straight-forward. We can deﬁne its index function in a k\u0002klocal\nregion as an indicator function\nImax(x) = 1(x= max( X));x2X; (1)\nwhere X2Rk\u0002k. 1(\u0001)is the indicator function with output\nbeing a binary matrix. Similarly, if one extracts indices from\naverage pooling, its index function takes the form\nIavg(x) = 1(x2X): (2)\nIf further using Iavg(x)during upsampling, it is equivalent\nto the nearest neighbor interpolation. As for the bilinear\ninterpolation and deconvolution operators, their index func-\ntions have an identical form\nIbilinear=dconv (x) =W\n 1(x2X); (3)\nwhere Wis the weight/ﬁlter of the same size as X, and\n\ndenotes the element-wise multiplication. The difference\nis that, Wis learned in deconvolution but predeﬁned in\nbilinear interpolation. Indeed bilinear interpolation has been\nshown to be a special case of deconvolution [2]. Note that,\nin this case, the index function generates soft indices. The\nsense of index for the PSoperator [4] is also clear, because\nthe rearrangement of the feature map is an indexing process.\nConsidering PSa tensor Zof size 1\u00021\u0002r2to a matrix Z\nof sizer\u0002r, the index function can be expressed by the\none-hot encoding\nIl\nps(x) = 1(x=Zl);l= 1;:::;r2; (4)\nsuch that Zm;n=Z[Il\nps(x)], wherem= 1;:::;r ,n= 1;:::;r ,\nandl= (r\u00001)\u0001m+n.Zldenotes the l-th element of Z.\nSimilar notation applies to Zm;n.\nSince upsampling operators can be uniﬁed by the notion\nof the index function, it is plausible to ask whether one can\nlearn an index function to dynamically capture local spatial\npatterns.\n4 L EARNING TO INDEX ,TOPOOL,AND TO UPSAM -\nPLE\nBefore introducing the designs of IndexNet, we ﬁrst present\nthe general idea about how learned indices may be used in\ndownsampling and upsampling with a new index-guided\nencoder-decoder framework. Our framework is a general-\nization of SegNet, as illustrated in Fig. 2. For ease of ex-\nposition, let us assume the downsampling and upsampling\nrates to be 2, and the pooling operator to use a kernel size of\n2\u00022. The IndexNet module dynamically generates indices\ngiven the feature map. The proposed indexed pooling and\nindexed upsampling operators further receive generated\nindices to guide downsampling and upsampling, respec-\ntively. In practice, multiple such modules can be combined\nand used analogous to the max pooling layers for every\ndownsampling and upsampling stage.\nIndexNet models the index as a function of the feature\nmapX2RH\u0002W\u0002C. Given X, it generates two index\nmaps for downsampling and upsampling, respectively. An\nimportant concept for the index is that an index can either\nbe represented in a natural order, e.g., 1, 2, 3, ..., or be\nrepresented in a logical form, i.e., 0, 1, 0, ..., meaning that\nan index map can be used as a mask. This is exactly how\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 4\nIndexed\nPoolingIndexed\nUpsampling\nEncoding stage with downsampling Decoding stage with upsamplingIndex \nBlock\nSigmoid\nSoftmaxIndexNetdecoder\nencoder\nAverage \nPooling4Indexed Pooling\n2x2, stride 2upsample\nx2\nnearest neighbor \ninterpolationIndexed Upsampling\nIndexNet\nEncoder feature maps Decoder feature maps Index maps Element‐wise multiplication\nFig. 2. The index-guided encoder-decoder framework. The proposed IndexNet dynamically predicts indices for individual local regions, conditioned\non the input local feature map itself. The predicted indices are further used to guide the downsampling in the encoding stage and the upsampling in\nthe corresponding decoding stage.\nwe use the index map in downsampling/upsampling. The\npredicted index shares the same deﬁnition of the index in\ncomputer science, except that we generate softindices for\nsmooth optimization, i.e., for any index i,i2[0;1].\nIndexNet consists of a predeﬁned index block and two\nindex normalization layers. An index block can simply be a\nheuristically deﬁned function, e.g., a max function, or more\ngenerally, a parameterized function such as neural network.\nIn this work, we use a fully convolutional network to be\nthe index block. More details are presented in Sections 4.2\nand 4.3. Note that the index maps sent to the encoder\nand decoder are normalized differently. The decoder index\nmap only goes through a sigmoid function such that for\nany predicted index i2(0;1). As for the encoder index\nmap, indices of each local region Lare further normalized\nby a softmax function such thatP\ni2Li= 1 . The second\nnormalization guarantees the magnitude consistency of the\nfeature map after downsampling.\nIndexed Pooling (IP) performs downsampling using gen-\nerated indices. Given a local region E2Rk\u0002k,IPcalculates\na weighted sum of activations and corresponding indices\noverEasIP(E) =P\nx2EI(x)x, whereI(x)is the index of\nx. It is easy to see that max pooling and average pooling are\nspecial cases of IP. In practice, this operator can be easily\nimplemented with an element-wise multiplication between\nthe feature map and the index map, an average pooling\nlayer, and a multiplication of a constant used to compensate\nthe effect of averaging, as instantiated in Fig. 2. The current\nimplementation is equivalent to 2\u00022stride-2 convolution\nwith dynamic kernels, but is more efﬁcient than explicit on-\nthe-ﬂy kernel generation.\nIndexed Upsampling (IU) is the inverse operator of IP.IU\nupsamplesd2R1\u00021that spatially corresponds to Etaking\nthe same indices into account. Let I2Rk\u0002kbe the local\nindex map formed by I(x)s,IUupsamples dasIU(d) =\nI\nD, where\ndenotes the element-wise multiplication,\nandDis of the same size as Iand is upsampled from d\nwith the nearest neighbor interpolation. IUalso relates to\ndeconvolution, but an important difference between IUand\ndeconvolution is that, deconvolution applies a ﬁxed kernel\nto all local regions (even if the kernel is learned), while IU\nupsamples different regions with different kernels (indices).\nHolistic Index Depthwise Index2x2xC1 x1x4 2x2xC1x1x4C\nHxWxCH xWx1 HxWxC HxWxCFig. 3. Conceptual differences between holistic and depthwise index.\nIndex Networks\nHINs DINs\nO2O DINs M2O DINs\nModelwise\nO2O DINsStagewise\nO2O DINs\nShared \nStagewise\nO2O DINsUnshared \nStagewise\nO2O DINs\nFig. 4. A taxonomy of proposed index networks.\n4.1 Index Networks\nHere we present a taxonomy of proposed index networks.\nAccording to the shape of the output index map, index\nnetworks can be ﬁrst categorized into two branches: holistic\nindex networks (HINs) and depthwise (separable) index networks\n(DINs). Their conceptual differences are shown in Fig. 3.\nHINs learn an index function I(X) :RH\u0002W\u0002C!RH\u0002W\u00021.\nIn this case, all channels of the feature map share a holis-\ntic index map. By contrast, DINs learn an index function\nI(X) :RH\u0002W\u0002C!RH\u0002W\u0002C, where the index map is of\nthe same size as the feature map.\nSince the index map generated by DINs can correspond\nto individual slices of the feature map, we can incorporate\nfurther assumptions into DINs to simplify the designs. If\nassuming that each slice of the index map only relates to its\ncorresponding slice of the feature map, this is the One-to-\nOne (O2O) assumption and O2O DINs . If each slice of the\nindex map relates to all channels of the feature map, this\nleads to Many-to-One (M2O) assumption and M2O DINs .\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 5\nIndexNet1IndexNet\nIndexNet2\nIndexNet3\nModelwise IndexNet Stagewise IndexNet\nFig. 5. Modelwise IndexNet vs. stagewise IndexNet.\nIn O2O DINs, one can further consider sharing IndexNet. In\nthe most simpliﬁed case, the same IndexNet can be applied\nto every slice of the feature map and can be shared across\ndifferent downsampling/upsampling stages, like the max\nfunction. We name this IndexNet Modelwise O2O DINs . If\nIndexNet is stage-speciﬁc, i.e., only sharing indices in indi-\nvidual stages, we call this IndexNet Shared Stagewise O2O\nDINs . Finally, without sharing any parameter (each feature\nslice has its speciﬁc index function), we obtain the standard\ndesign, termed Unshared Stagewise O2O DINs . Fig. 4 shows\nthe tree diagram of these index networks. The difference\nbetween modelwise IndexNet and stagewise IndexNet is\nalso shown in Fig. 5. Notice that, HINs and M2O DINs are\nboth stagewise.\nWith the taxonomy, we investigate ﬁve families of In-\ndexNet. Each family can be designed to have either linear\nmappings or nonlinear mappings, as we discuss next.\n4.2 Holistic Index Networks\nRecall that HINs learn an index function I(X) :RH\u0002W\u0002C!\nRH\u0002W\u00021. A naive design choice is to assume a linear\nmapping between the feature map and the index map.\nLinear HINs. An example is shown in Fig. 6(a). The\nnetwork is implemented in a fully convolutional network.\nIt ﬁrst applies stride- 2 2\u00022convolution (assuming that\nthe downsampling rate is 2) to the feature map of size\nH\u0002W\u0002C, generating a concatenated index map of size\nH=2\u0002W=2\u00024. Each slice of the index map ( H=2\u0002W=2\u00021)\nis designed to correspond to the indices of a certain position\nof all local regions, e.g., the top-left corner of all 2\u00022regions.\nThe network ﬁnally applies a PS-like shufﬂing operator to\nrearrange the index map to the size of H\u0002W\u00021.\nIn many situations, a linear relationship is not sufﬁcient.\nFor example, a linear function even cannot approximate the\nmax function. Thus, the second design choice is to introduce\nnonlinearity into the network.\nNonlinear HINs. Fig. 6(b) illustrates a nonlinear HIN\nwhere the feature map is ﬁrst projected to a map of size\nH=2\u0002W=2\u00022C, followed by a batch normalization layer\nand a ReLU function for nonlinear mappings. We then use\npoint-wise convolution to reduce the channel dimension to\nan indices-compatible size. The remaining transformations\nfollow its linear counterpart.\n4.3 Depthwise Index Networks\nIn DINs, we seek I(X) :RH\u0002W\u0002C!RH\u0002W\u0002C, i.e.,\neach spatial index corresponds to each spatial activation.\nAs aforementioned, this type of networks further has two\ndifferent high-level design strategies that correspond to two\ndifferent assumptions.\nConv\n2x2x4\nstride 2\nHxWxC H/2xW/2x4H xWx1\nConv+BN+ReLU\n2x2x2C, stride 2Shuffling\nHxWxC H/2xW/2x2C HxWx1 H/2xW/2x4Conv\n1x1x4(a)\n(b)ShufflingFig. 6. Holistic index networks. (a) a linear index network; (b) a nonlinear\nindex network.\n4.3.1 One-to-One Depthwise Index Networks\nO2O assumption assumes that each slice of the index map\nonly relates to its corresponding slice of the feature map. It\ncan be denoted by a local index function l(X) :Rk\u0002k\u00021!\nRk\u0002k\u00021, wherekdenotes the size of the local region. Since\nthe local index function operates on individual feature\nslices, we can design whether different feature slices share\nthe same local index function. Such a weight sharing strat-\negy can be applied at a modelwise level or at a stagewise\nlevel, which leads to the following designs of O2O DINs:\n1)Modelwise O2O DINs : the model only has a unique\nindex function that is shared by all feature slices, even\nin different downsampling and upsampling stages. This\nis the most light-weight design;\n2)Shared Stagewise O2O DINs : the index function is also\nshared by feature slices, but every stage has stage-\nspeciﬁc IndexNet. This design is also light-weight;\n3)Unshared Stagewise O2O DINs : even in the same stage,\ndifferent feature slices have distinct index functions.\nSimilar to HINs, DINs can also be designed to have\nlinear/nonlinear modeling ability. Fig. 7 shows an example\nwhenk= 2. Note that, in contrast to HINs, DINs follow\na multi-column architecture. Each column is responsible for\npredicting indices speciﬁc to a certain spatial location of all\nlocal regions. We implement DINs with group convolutions.\nLinear O2O DINs. According to Fig. 7, the feature map\nﬁrst goes through four parallel convolutional layers with\nthe same kernel size. Modelwise O2O DINs and Shared\nStagewise O2O DINs only use a kernel size of 2\u00022\u00021,\na stride of 2, and 1group, while Unshared Stagewise O2O\nDINs has a kernel size of 2\u00022\u0002C, a stride of 2, and\nCgroups. One can simply reshape the feature map, i.e.,\nreshapingH\u0002W\u0002Cto beC\u0002H\u0002W\u00021, to enable\na2\u00022\u00021kernel operating on each H\u0002W\u00021feature\nslice, respectively. All O2O DINs lead to four downsampled\nfeature maps of size H=2\u0002W=2\u0002C. The ﬁnal index map\nof sizeH\u0002W\u0002Cis composed from the four feature maps\nby shufﬂing and rearrangement. Note that the parameters\nof four columns are not shared.\nNonlinear O2O DINs. Nonlinear DINs can be easily mod-\niﬁed from linear DINs by inserting four extra convolutional\nlayers. Each of them is followed by a batch normalization\n(BN) layer and a ReLU unit, as shown in Fig. 7. The rest\nremains the same as the linear DINs.\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 6\nHxWxCHxWxC\nH/2xW/2xCGroup Conv\n1x1xM\ngroup N\nH/2xW/2x2CBN+ReLU\nBN+ReLUBN+ReLUBN+ReLUBN: Batch Normalization\nFig. 7. Depthwise index networks. M= 1;N= 1 for Modelwise O2O\nDINs and Shared Stagewise O2O DINs; M=C;N =Cfor Unshared\nStagewise O2O DINs; and M=C;N = 1 for the M2O DINs. The\nmasked modules are invisible to linear networks.\n4.3.2 Many-to-One Depthwise Index Networks\nM2O assumption assumes that all feature slices have con-\ntributions to each index slice. The local index function is\ndeﬁned by l(X) :Rk\u0002k\u0002C!Rk\u0002k\u00021. Compared to O2O\nDINs, the only difference in implementation is the use of\nstandard convolution instead of group convolution, i.e.,\nM=C;N = 1 in Fig. 7.\n4.4 Property and Model Complexity\nBoth HINs and DINs have merits and drawbacks. Here we\ndiscuss some important properties of IndexNet. We also\npresent an analysis of computational complexity.\nRemark 1. Index maps generated by HINs and used by the IP\nandIUoperators are related to spatial attention.\nThe holistic index map is shared by all feature slices,\nwhich means that the index map is required to be expanded\nto the size of H\u0002W\u0002Cwhen feeding into IPandIU. This\nindex map can be thought as a collection of local attention\nmaps [22] applied to individual local spatial regions. In\nthis case, the IPandIUoperators can also be referred\nto as “attentional pooling” and “attentional upsampling”.\nHowever, it should be noted that spatial attention has no\npooling or upsampling operators like IPandIU.\nRemark 2. HINs are more ﬂexible than DINs and more friendly\nfor decoder design.\nSince the holistic index map is expandable, the decoder\nfeature map does not need to forcibly increase/reduce its\ndimensionality to ﬁt the shape of the index map during\nupsampling. This gives much ﬂexibility for decoder design,\nwhile it is not the case for DINs.\nRemark 3. The number of parameters in Modelwise O2O DINs\nand Shared Stagewise O2O DINs is independent of the dimen-\nsionality of feature maps.\nNo matter how large the model capacity is or how\nwide the feature channels are, the number of parameters in\nModelwise O2O DINs remains at a constant level, and that\nin Shared Stagewise O2O DINs is only proportional to the\nnumber of downsampling/upsampling stages. This is desir-\nable as the number of parameters introduced by IndexNet is\nnot signiﬁcant. However, these two types of IndexNet may\nbe limited to capture sophisticated local patterns.TABLE 1\nA Comparison of Model Complexity of Different Index Networks\nIndexNet Type # Param.\nHINsL K\u0002K\u0002C\u00024\nNL K\u0002K\u0002C\u00022C+ 2C\u00024\nNL+C 2K\u00022K\u0002C\u00022C+ 2C\u00024\nModelwise O2O DINsL (K\u0002K)\u00024\nNL (K\u0002K\u00022 + 2)\u00024\nNL+C (2K\u00022K\u00022 + 2)\u00024\nShared Stagewise O2O DINsL (K\u0002K)\u00024\nNL (K\u0002K\u00022 + 2)\u00024\nNL+C (2K\u00022K\u00022 + 2)\u00024\nUnshared Stagewise O2O DINsL (K\u0002K\u0002C)\u00024\nNL (K\u0002K\u00022C+ 2C\u0002C)\u00024\nNL+C (2K\u00022K\u00022C+ 2C\u0002C)\u00024\nM2O DINsL (K\u0002K\u0002C\u0002C)\u00024\nNL (K\u0002K\u0002C\u00022C+ 2C\u0002C)\u00024\nNL+C (2K\u00022K\u0002C\u00022C+ 2C\u0002C)\u00024\nL: Linear; NL: Nonlinear; C: Context.\nRemark 4. M2O DINs have the most powerful modeling capa-\nbility among IndexNet variants, but also introduce many extra\nparameters.\nM2O DINs have higher capacity than HINs and O2O\nDINs due to the use of standard convolution.\nAnother desirable property of IndexNet is that they may\nbe able to predict the indices from a large local feature\nmap, e.g.,l(X) :R2k\u00022k\u0002C!Rk\u0002k\u00021. An intuition behind\nthis idea is that, if one identiﬁes a local maximum point\nfrom ak\u0002kregion, its surrounding 2k\u00022kregion can\nfurther support whether this point is a part of a boundary\nor only an isolated noise point. This idea can be easily\nimplemented by enlarging the convolutional kernel size and\nwith appropriate padding.\nIn Table 1, we summarize the model complexity of differ-\nent index networks used at a single downsampling and up-\nsampling stage. We assume the convolution kernel has a size\nofK\u0002Kapplied on a C-channel feature map. The number\nof parameters in BN layers is excluded. When considering\nweak context, we assume the kernel size is 2K\u00022K. Since\nC\u001dK, generally we have the model complexity M2O\nDINs>HINs>Unshared Stagewise O2O DINs >Shared Stage-\nwise O2O DINs >Modelwise O2O DINs .\n5 G UIDED UPSAMPLING OR BLIND UPSAMPLING :\nA R ECONSTRUCTION -BASED JUSTIFICATION\nHere we introduce the concept of guided upsampling and\nblind upsampling to summarize existing upsampling oper-\nators. Particularly, here we present a comparison between\ntwo data-dependent upsampling operators—IndexNet and\nCARAFE [18]. In addition, we show results of an image\nreconstruction task on synthetic data, highlighting the dif-\nference between guided upsampling and blind upsampling.\n5.1 Guided Upsampling vs. Blind Upsampling\nBy blind upsampling, we mean that an upsampling operator\nthat is pre-deﬁned with ﬁxed parameters. Guided upsam-\npling, instead, is guided with the involvement of auxiliary\ninformation.\nThus, most widely-used upsampling operators pe-\nform blind upsampling. These operators include nearest-\nneighbor (NN) interpolation, bilinear interpolation, space-\nto-depth and deconvolution. It is worth noting that the re-\ncent data-dependent upsampling operator CARAFE is also a\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 7\nTABLE 2\nBlind Upsampling and Guided Upsampling Operators\nUpsampling Downsampling\nBlind\nUpsamplingNN Interpolation Average Pooling\nBilinear Interpolation Convolution\nDeconvolution Convolution\nSpace-to-Depth Depth-to-Space\nCARAFE Convolution\nGuided\nUpsamplingMax Unpooling Max Pooling\nIndexed Upsampling Indexed Pooling\nblind upsampling operator. By contrast, guided upsampling\noperators are rare in literature. Max unpooling, albeit being\nsimple, is a guided upsampling operator. The auxiliary in-\nformation used in upsampling comes from the max-pooling\nindices. Thefore, our proposed IndexNet clearly implements\nguided upsampling, with inferred dynamic indices as the\nauxiliary information.\nA taxonomy of commonly-used upsampling operators\nand their corresponding downsampling operators is sum-\nmarized in Table 2. An upsampling operator should gen-\nerally have an corresponding downsampling operator, and\nvice versa.\nThe main difference here is that guided upsampling is\nmade possible to exploit extra information to better recover\nthe spatial information during upsampling. Thus, it is im-\nportant that the spatial information is properly encoded\nduring downsampling and is transferred to unsampling.\n5.2 IndexNet vs. CARAFE\nBoth IndexNet and CARAFE are one of the few attempts\npursuing the idea of data-dependent upsampling. The sim-\nilarities include:\ni) They both are related to dynamic networks.\nii) Both are parametric upsampling operators;\niii) CARAFE and HINs both perform holistic upsampling.\niv) CARAFE also learns an index function. The index func-\ntion has an identical form to Eq. (3), but with dynamic\nand normalized W. In this sense, CARAFE may be\nconsidered as a single-input version of IU, where the\nindex map is generated internally.\nThe differences are:\ni) CARAFE is a blind upsampling operator, while In-\ndexNet implements guided upsampling;\nii) The reassembly kernels in CARAFE are generated con-\nditioned on the low-resolution decoder feature map.\nThe index maps predicted by IndexNet, however, build\nupon the high-resolution encoder feature map, before\nspatial information is lost;\niii) In IndexNet, each upsampled feature point only asso-\nciates with a single point in the low-resolution feature\nmap. From low resolution to high resolution, it is a\none-to-many mapping. In CARAFE, each upsampled\npoint is a weighted sum of a local region from the low-\nresolution feature map. This is a many-to-one mapping;\niv) Compared to CARAFE which is presented as a sin-\ngle upsampling operator, IndexNet is a more general\nframework.\nIn particular, the key difference lies in the intermediate path\nthat allows spatial information to be visible to upsampling.TABLE 3\nPerformance of Image Reconstruction on the Fashion-MNIST Dataset\nPSNR SSIM MAE MSE\nAvgPool–NN 25.88 0.9811 0.0259 0.0509\nConv /2–Bilinear 24.45 0.9726 0.0320 0.0600\nS2D–D2S 28.93 0.9901 0.0204 0.0358\nConv /2–Deconv /2 28.75 0.9903 0.0187 0.0366\nConv /2–CARAFE 25.55 0.9798 0.0277 0.0529\nMaxPool–MaxUnpool 29.33 0.9920 0.0202 0.0342\nIP–IU*37.83 0.9989 0.0089 0.0128\nIP–IUy45.93 0.9998 0.0032 0.0051\nIP–IUz48.37 0.9999 0.0026 0.0038\n\u0003denotes Modelwise O2O DIN;yindicates HIN;zrefers to M2O\nDIN. All IndexNets are with nonlinearity and weak context. The best\nperformance is boldfaced.\nTo further demonstrate the beneﬁt of this intermediate path,\nwe present an image reconstruction experiment on synthetic\ndata, namely, the Fashion-MNIST dataset [26].\n5.3 Fashion-MNIST Image Reconstruction\nThe idea is that, if an upsampling operator can recover\nspatial information well from downsampled feature maps,\nthe reconstructed output should be visually closer to the\ninput image. The quality of reconstruction results can be a\ngood indicator how well spatial information is recovered by\nan upsampling operator.\nNetwork Architecture and Baselines. We use a standard\nencoder-decoder architecture. Let C(k) denote a 2D convolu-\ntional layer with k-channel 3\u00023ﬁlters, followed by BN and\nReLU.Drepresents a downsampling operator with a down-\nsampling ratio of 2, andUan upsampling operator with\nan upsampling ratio of 2. The reconstruction network can\ntherefore be deﬁned by C(32)-D-C(64)-D-C(128)-D-C(256)-\nC(128)-U-C(64)-U-C(32)-U-C(1). Note that BN and ReLU\nare not included in the last C(1). We build the following\nbaselines:\ni) Average Pooling–NN interpolation (AvgPool–NN);\nii) stride-2 Convolution–Bilinear interpolation (Conv /2–\nBilinear);\niii) Space-to-Depth–Depth-to-Space (S2D–D2S);\niv) stride-2 Convolution–2-stride Deconvolution (Conv /2–\nDeconv /2);\nv) stride-2 Convolution–CARAFE (Conv /2–CARAFE);\nvi) Max Pooling–Max Unpooling (MaxPool–MaxUnpool);\nvii) Indexed Pooling–Indexed Upsampling ( IP–IU).\nTraining Details. The Fashion-MNIST dataset [26] in-\ncludes 60;000 training images and 10;000 testing images.\nThe input images are resized to 32\u000232.`1loss is used\nin training. The initial learning rate is set to 0:01. We train\nthe network for 100 epochs with a batch size of 100. The\nlearning rate is decreased by \u000210at the 50-th,70-th and 85-\nth epoch, respectively. We report Peak Signal-to-Noise Ratio\n(PSNR), Structural SIMilarity index (SSIM), Mean Absolute\nError (MAE) and root Mean Square Error (MSE).\nDiscussions. Quantitative and qualitative results are\nshown in Table 3 and Fig. 8, respectively. We observe in\nFig. 8 that, all baselines that exploit blind upsampling fail to\nreconstruct the input images. While in some cases they may\nproduce reasonable reconstructions, images of trousers and\nhigh-heeled shoes for instance, in most cases these baselines\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 8\ngenerate blurred results when complex textural patterns\nappear, tops and wallets for example. By contrast, other\nbaselines that leverage guided upsampling produce visually\npleasing reconstructions, in all circumstances. In particular,\nby comparing MaxPool–MaxUnpool with IP–IU, the former\ntends to yield jittering artifacts. This suggests index maps\nextract and encode richer spatial information than max-\npooling indices. In addition, by disabling the intermediate\npath,IP–Bilinear leads to signiﬁcantly poor reconstructions,\nwhich means that the intermediate path matters. The dif-\nferences between two upsampling paradigms are also sup-\nported by the numerical results in Table 3 where guided\nupsampling exhibits signiﬁcantly better PSNR and lower\nerrors than blind upsampling. Indeed, the intermediate path\ndistinguishes guided upsampling from blind upsampling,\nand also IndexNet from CARAFE.\n6 A PPLICATIONS\nIn this section, we show several applications of IndexNet\non the tasks of image matting, image denoising, semantic\nsegmentation, and monocular depth estimation.\n6.1 Image Matting\nWe ﬁrst evaluate IndexNet on the task of image matting.\nImage matting is deﬁned as a problem of estimating soft\nforeground from images. This problem is ill-posed due to\nthe fact that solving a linear system of 7unknown variables\nwith only 3known inputs: given the RGB color at pixel\ni,Ii, one needs to estimate the corresponding foreground\ncolorFi, background color Bi, and matte \u000bi, such that\nIi=\u000biFi+ (1\u0000\u000bi)Bi, for any\u000bi2[0;1]. Previous\nmethods have extensively studied this problem from a\nlow-level view [27], [28], [29], [30]; and particularly, they\nhave been designed to solve the above matting equation.\nDespite being theoretically elegant, these methods heavily\nrely on the color cues, rendering failures of matting in\ngeneral natural scenes where colors are not reliable. With\nthe tremendous success of deep CNNs in high-level vision\ntasks [2], [31], [32], deep matting methods are emerging.\nRecently deep image matting was proposed [7]. In [7] the\nauthors presented the ﬁrst deep image matting approach\n(DeepMatting) based on SegNet [3] and signiﬁcantly out-\nperformed other competitors. In this application, we use\nDeepMatting as our baseline. Image matting is particularly\nsuitable for evaluating the effectiveness of IndexNet, be-\ncause the quality of learned indices can be visually observed\nfrom inferred alpha mattes. We conduct experiments on the\nAdobe Image Matting dataset [7]. This is so far the largest\npublicly available matting dataset. The training set has 431\nforeground objects and ground-truth alpha mattes. Each\nforeground is composited with 100 background images\nrandomly chosen from MS COCO [33]. The validation set\ntermed Composition-1k includes 100unique objects. Each of\nthem is composited with 10 background images chosen from\nPascal VOC [34]. Overall, we have 43;100 training images\nand 1;000 testing images. We evaluate the results using\nwidely-used Sum of Absolute Differences (SAD), root Mean\nSquare Error (MSE), and perceptually-motivated Gradient\n(Grad) and Connectivity (Conn) errors [35]. The evaluationcode implemented by [7] is used. In what follows, we ﬁrst\ndescribe our modiﬁed MobileNetv2-based architecture and\ntraining details. We then perform extensive ablation studies\nto justify choices of model design, make comparisons of\ndifferent index networks, and visualize learned indices.\n6.1.1 Network Architecture and Implementation Details\nHere we describe the network architecture and training\ndetails.\nNetwork Architecture . We build our model based on Mo-\nbileNetv2 [9] with only slight modiﬁcations to the backbone.\nWe choose MobileNetv2 for its lightweight model and fast\ninference. The basic network conﬁguration is shown in\nFig. 9. It also follows the encoder-decoder paradigm same\nas SegNet. We simply change all 2-stride convolution to\nbe 1-stride and attach 2-stride 2\u00022max pooling after\neach encoding stage for downsampling, which allows us to\nextract indices. If applying the IndexNet idea, max pooling\nand unpooling layers can be replaced with IPandIU, re-\nspectively. We also investigate alternative ways for low-level\nfeature fusion and whether encoding context (Section 6.1.2).\nNote that, the matting reﬁnement stage [7] is not applied\nhere.\nTraining Details . To enable a direct comparison with deep\nmatting [7], we follow the same training conﬁgurations used\nin [7]. The 4-channel input concatenates the RGB image and\nits trimap. We follow exactly the same data augmentation\nstrategies, including 320\u0002320 random cropping, random\nﬂipping, random scaling, and random trimap dilation. We\nuse a combination of the alpha prediction loss and the\ncomposition loss during training as in [7]. Only losses from\nthe unknown region of the trimap are calculated. Encoder\nparameters are pretrained on ImageNet [36]. The parame-\nters of the 4-th input channel are initialized with zeros. The\nAdam optimizer [37] is used. We update parameters with\n30epochs (around 90;000 iterations). The learning rate is\ninitially set to 0:01and reduced by 10\u0002at the 20-th and 26-\nth epoch respectively. We use a batch size of 16and ﬁx the\nBN layers of the backbone.\n6.1.2 Results on the Adobe Image Matting Dataset\nAblation Study on Model Design . To establish a better baseline\ncomparable to DeepMatting, here we ﬁrst investigate strate-\ngies for fusing low-level features (no fusion, skip fusion\nas in ResNet [38] or concatenation as in UNet [39]) and\nwhether encoding context for image matting. 11baselines\nare consequently built to justify model design. Results on\nthe Composition-1k testing set are reported in Table 4. B3 is\ncited from [7]. We can make the following observations:\ni) Indices are of great importance. Matting can signiﬁ-\ncantly beneﬁt from only indices (B3 vs. B4, B5 vs. B6);\nii) State-of-the-art semantic segmentation models cannot\nbe directly applied to image matting (B1/B2 vs. B3);\niii) Fusing low-level features help, and concatenation is\nbetter than skip connection but at a cost of increased\ncomputation (B6 vs. B8 vs. B10 or B7 vs. B9 vs. B11);\niv) Modules such as ASPP may improve the results (e.g.,\nB6 vs. B7 or B8).\nv) A MobileNetv2-based matting model can work as well\nas a VGG-16-based one (B3 vs. B11).\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 9\nConv /2—Bilinear\nMaxPool —MaxUnpoolConv /2—CARAFE HINModelwise O2O DIN\nM2O DINGround Truth\nConv /2—Deconv /2\nS2D—D2S\nAvgPool—NN\nFig. 8. Image reconstruction results on the Fashion-MNIST dataset.\nTABLE 4\nAblation Study of Design Choices\nNo. Architecture Backbone Fusion Indices Context OS SAD MSE Grad Conn\nB1 DeepLabv3+ [6] MobileNetv2 Concat No ASPP 16 60.0 0.020 39.9 61.3\nB2 ReﬁneNet [5] MobileNetv2 Skip No CRP 32 60.2 0.020 41.6 61.4\nB3 SegNet [7] VGG16 No Yes No 32 54.6 0.017 36.7 55.3\nB4 SegNet VGG16 No No No 32 122.4 0.100 161.2 130.1\nB5 SegNet MobileNetv2 No Yes No 32 60.7 0.021 40.0 61.9\nB6 SegNet MobileNetv2 No No No 32 78.6 0.031 101.6 82.5\nB7 SegNet MobileNetv2 No Yes ASPP 32 58.0 0.021 39.0 59.5\nB8 SegNet MobileNetv2 Skip Yes No 32 57.1 0.019 36.7 57.0\nB9 SegNet MobileNetv2 Skip Yes ASPP 32 56.0 0.017 38.9 55.9\nB10 UNet MobileNetv2 Concat Yes No 32 54.7 0.017 34.3 54.7\nB11 UNet MobileNetv2 Concat Yes ASPP 32 54.9 0.017 33.8 55.2\nFusion: fuse encoder features; Indices: max-pooling indices (where Indices is ‘No’, bilinear interpolation is used for upsampling); CRP: chained\nresidual pooling [5]; ASPP: atrous spatial pyramid pooling [6]; OS: output stride. The lowest errors are boldfaced.\n320x320x4\n320x320x323x3x32, stride=1\n2x2 max pool\n160x160x32\n160x160x16\n2x2 max pool160x160x24\n80x80x24\n80x80x323x3x32, stride=1\n2x2 max pool\n40x40x323x3x16\n3x3x24, stride=1\n3x3x64, stride=1\n40x40x64\n2x2 max pool\n20x20x64\n3x3x96\n20x20x96\n3x3x160,  stride=1\n20x20x160\n2x2 max pool\n10x10x160\n3x3x320\n10x10x320ASPP10x10x1602x2 max unpool20x20x1605x5x9620x20x965x5x6420x20x642x2 max unpool40x40x645x5x3240x40x322x2 max unpool80x80x325x5x2480x80x242x2 max unpool160x160x245x5x16160x160x165x5x32160x160x322x2 max unpool320x320x325x5x32320x320x325x5x1320x320x1 Input layer\nConv+BN+ReLU\nDownsampling layer\nEncoder feature maps\nDecoder feature maps\nAtrous spatial pyramid pooling\n10x10x320Ouput layerUpsampling layer\nEncoding stage\nPropagate indicesFuse encoder featuresDepthwise Conv+BN+ReLU\nE1\nE2\nE3\nE4\nE5\nE6\nE7D0\nD1E0\nD2\nD3\nD4\nD5\nD6\nE8/D7\nFig. 9. Customized MobileNetv2-based encoder-decoder network archi-\ntecture. Our modiﬁcations are boldfaced.\nFor the following experiments, we now mainly use B11.\nAblation Study on Index Networks . Here we compare dif-\nferent index networks and justify their effectiveness. Theconﬁgurations of index networks used in the experiments\nfollow Figs. 6 and 7. We primarily investigate the 2\u00022kernel\nwith a stride of 2. Whenever the weak context is considered,\nwe use a 4\u00024kernel in the ﬁrst convolutional layer of\nindex networks. To highlight the effectiveness of HINs, we\nfurther build a baseline called holistic max index (HMI) where\nmax-pooling indices are extracted from a squeezed feature\nmapX02RH\u0002W\u00021.X0is generated by applying the max\nfunction along the channel dimension of X2RH\u0002W\u0002C.\nFurthermore, since IndexNet increases extra parameters, we\nintroduce another baseline B11-1.4 where the width multi-\nplier of MobilieNetV2 is adjusted to be 1:4to increase the\nmodel capacity. In addition, to compare IndexNet against\nCARAFE in this task, we build an additional baseline B11-\ncarafe where the unpooling operator in B11 is replaced with\nCARAFE. Results on the Composition-1k testing dataset are\nlisted in Table 5. We observe that, most index networks\nreduce the errors notably, except for some low-capacity\nIndexNet modules (due to limited modeling capabilities).\nIn particular, nonlinearity and the context generally have\na positive effect on deep image matting, but they do not\nwork effectively in O2O DINs. A possible reason may be\nthat the limited dimensionality of the intermediate feature\nmap is not sufﬁcient to model complex patterns in mat-\nting. Compared to holistic max index, the direct baseline\nof HINs, the best HIN (“Nonlinearity+Context”) has at\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 10\nFig. 10. Qualitative results on the Composition-1k testing set. From left to right, the original image, trimap, ground-truth alpha matte, Closed-form\nMatting [30], DeepMatting [30], and ours (M2O DIN with ‘Nonlinearity+Context’).\nTABLE 5\nResults on the Composition-1k Testing Set\nMethod #Param. GFLOPs SAD MSE Grad Conn\nB3 [7] 130.55M 32.34 54.6 0.017 36.7 55.3\nB11 3.75M 4.08 54.9 0.017 33.8 55.2\nB11-1.4 8.86M 7.61 55.6 0.016 36.4 55.7\nB11-carafe 4.06M 5.01 50.2 0.015 27.9 50.0\nHMI 3.75M 4.08 56.5 0.021 33.0 56.4\nNL C \u0001\nHINs\n+4.99K 4.09 55.1 0.018 32.1 55.2\nX +0.26M 4.22 50.6 0.015 27.9 49.4\nX X +1.04M 4.61 49.5 0.015 25.6 49.2\nModelwise O2O DINs\n+16 4.08 57.3 0.017 37.3 57.4\nX +56 4.08 52.4 0.016 30.1 52.2\nX X +152 4.08 59.1 0.018 39.0 59.7\nShared Stagewise O2O DINs\n+80 4.08 48.9 0.014 26.2 48.0\nX +280 4.08 51.1 0.016 30.2 50.7\nX X +760 4.08 56.0 0.016 37.5 55.9\nUnshared Stagewise O2O DINs\n+4.99K 4.09 50.3 0.015 33.7 50.0\nX +17.47K 4.10 50.6 0.016 26.5 50.3\nX X +47.42K 4.15 50.2 0.016 26.8 49.3\nM2O DINs\n+0.52M 4.34 51.0 0.015 33.7 50.5\nX +1.30M 4.73 48.9 0.015 32.1 47.9\nX X +4.40M 6.30 45.8 0.013 25.9 43.7\nDeepMatting w. Reﬁnement [7] 50.4 0.014 31.0 50.8\nNL: Non-Linearity; C: Context. \u0001indicates increased parameters compared to\nB11. GFLOPs are measured on a 224\u0002224\u00024input. The lowest errors are\nboldfaced.\nleast 12:3% relative improvement. Compared to B11, the\nbaseline of DINs, M2O DIN with “Nonlinearity+Context”\nexhibits at least 16:5%relative improvement. Notice that,\nour best model outperforms the DeepMatting approach [7]\nthat even has the reﬁnement stage. In addition, according to\nthe results of B11-1.4, the performance improvement does\nnot come from increased parameters. Moreover, CARAFE\nalso enhances matting performance, but it falls behind M2O\nDIN. Some qualitative results are shown in Fig. 10. Our\npredicted mattes show improved delineation for edges and\ntextures like hair and water drops.\nAblation Study on Index Normalization . Index normaliza-\ntion is important for the ﬁnal performance. Here we justify\nthis by evaluating different normalization choices. Apart\nfrom the sigmoid function used for the decoder and the\nsigmoid+softmax function for the encoder, we compare\nother three different combinations of normalization strate-\ngies listed in Table 6. The experiment is conducted basedTABLE 6\nAblation Study of Different Normalization Choices on Index Maps\nEncoder Decoder SAD MSE Grad Conn\nsigmoid sigmoid 52.7 0.016 29.3 52.4\nsoftmax softmax 51.6 0.015 29.2 51.6\nsoftmax+sigmoid softmax 57.3 0.016 43.5 57.3\nsigmoid+softmax sigmoid 45.8 0.013 25.9 43.7\nThe lowest errors are boldfaced.\nFig. 11. Visualization of the randomly initialized index map (left) and the\nlearned index map (right) of HINs (top) and DINs (bottom). Best viewed\non screen.\non M2O DIN with “Nonlinearity+Context”. It is clear that\nkeeping the magnitude consistency during downsampling\nmatters. In fact, both max pooling and average pooling\nsatisfy this property naturally, and our normalization design\nis inspired from this fact.\nIndex Map Visualization . It is interesting to see what\nindices are learned by IndexNet. For the holistic index, the\nindex map itself is a 2D matrix and can be easily visualized.\nRegarding the depthwise index, we squeeze the index map\nalong the channel dimension and calculate the average re-\nsponses. Two examples of learned index maps are visualized\nin Fig. 11. We observe that, initial random indices have poor\ndelineation for edges, while learned indices automatically\ncapture the complex structural and textual patterns, e.g., the\nfur of the dog, and even air bubbles in the water.\n6.2 Image Denoising\nThe goal of image denoising is to recover a clean image\nxfrom a corrupted observation yfollowing an image\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 11\nIndices\nIndices\nIndices1 6 46 4 6 46 4 6 4 6 46 4 6 4 6 46 4 6 46 4 6 46 4 6 41\n1 64 128128 256 256 256 512512 256 128 128 64 64 64 64 1DnCNN\nSegNet‐like DnCNN\nFig. 12. The DnCNN architecture and our modiﬁed SegNet-like DnCNN.\ndegradation model y=x+v, where vis commonly\nassumed to be additive white Gaussian noise (AWGN)\nparameterized by \u001b. While such an assumption has been\nchallenged in recent real-image denoising [40], [41], we still\nfollow the AWGN paradigm in evaluation because our focus\nis not to improve image denoising. When deep CNNs are\nwidely accepted, the data-driven paradigm now becomes\nthe ﬁrst-class choice for image denoising [10], [42]. Most\ndeep denoising models are designed with the same high-\nlevel idea—processing the feature map without decreasing\nits spatial resolution. Indeed, it has been observed that,\nwhen the feature map is downsampled, the performance\ndrops remarkably [42]. For such networks, although the\nmodel parameters largely reduce, computational complexity\nof training and inference becomes much heavier.\nWe show that, by inserting IndexNet into a denoising\nmodel, it can effectively compensate the loss of spatial\ninformation, achieving performance comparable to or even\nbetter than the network without downsampling. Thus, de-\nspite the number of parameters increases, computation us\nmuch reduced. We choose DnCNN [10] as our baseline to\ndemonstrate this on standard benchmarks. We follow the\nexperimental setting of [43] that uses a 400-image training\nset. The performance is reported on a 68-image Berkeley\nsegmentation dataset (BSD68) and the other 12-image test\nset (Set12). The networks are trained for Gaussian denoising,\nwith three noise levels, i.e., \u001b= 15;25and50. PSNR and\nSSIM are used as evaluation metrics.\n6.2.1 Network Architecture and Implementation Details\nNetwork Architecture . We use the 17-layer DnCNN\nmodel [10], implemented by PyTorch. To enable the use of\nIndexNet, we modify DnCNN to a SegNet-like architecture\nwith 3downsampling and upsampling stages (the input im-\nage size is 40\u000240). The number of layers remains the same\nto ensure a relatively fair comparison. Fig. 12 illustrates the\noriginal DnCNN and our modiﬁed architecture. The ﬁrst 9\nlayers follow VGG-16 except that the ﬁrst layer is a single-\nchannel input, and the rest are 7decoding layers formed by\nunpooling and convolution and the ﬁnal prediction layer.\nAll convolutional operations use 3\u00023kernels. To incorpo-rate IndexNet, it is straightforward to replace max pooling\nand unpooling with IPandIU.\nTraining Details . We follow the same experimental con-\nﬁgurations used in [10]. At each epoch, 40\u000240image\npatches are cropped from multiple scales ( 0:7;0:8;0:9;1)\nwith a stride of 10and are added with Gaussian noise of\na certain noise level ( \u001b= 15;25;or50); image patches\nare further augmented with random ﬂipping and random\nrotation. This results in around 240;000 training samples.\n`2loss is used. All networks are trained from scratch with\na batch size of 128. Model parameters are initialized with\nthe improved Xavier [44]. The Adam optimizer is also used.\nParameters are updated with 60epochs. The learning rate\nis initially set to 0:001and reduced by 10\u0002at the 45-th and\n55-th epoch, respectively.\n6.2.2 Results on the BSD68 and Set12 Datasets\nApart from the DnCNN baseline, we also report the per-\nformance of our modiﬁed DnCNN-SegNet with max pool-\ning and unpooling. Furthermore, to compare IndexNet\nagainst CARAFE, we build three additional baselines where\nCARAFE is combined with different downsampling strate-\ngies, including max pooling, average pooling, and stride-\n2 convolutions, denoted by DnCNN-max-carafe, DnCNN-\navg-carafe, and DnCNN-conv-carafe, respectively. Results\nare shown in Table 7. It can be observed that, simply\ndownsampling with max pooling and upsampling by un-\npooling as in DnCNN-SegNet lead to signiﬁcant drops in\nboth PSNR (generally >1dB) and SSIM ( >0:1). This\nsuggests that spatial information plays an important role\nin image denoising. Denoising is content-irrelevant (the\nmodel is unaware of regions coming from the foreground\nor the background). Downsampling without recording suf-\nﬁcient spatial information (only the boundary information\nis not sufﬁcient) impedes the model from recovering the\nappearance and the structure in the original image. This\nis particularly true for baselines adopting CARAFE. Since\nCARAFE applies blind upsampling, no spatial informa-\ntion is transferred during upsampling, which may lead\nto inferior results. Interestingly, after IndexNet is inserted\ninto downsampled DnCNN, the loss of PSNR and SSIM is\neffectively compensated. The compensation behaviors can\nbe observed from almost all types of IndexNet, except the\ntwo cases in Modelwise O2O DINs with nonlinearity. The\npoor performance of Modelwise O2O DINs may attribute to\nthe insufﬁcient modeling ability, particularly when \u001b= 50 .\nNonlinearity and weak context generally have a positive\neffect on image denoising, and the effectiveness of different\nIndexNets is similar. Hence, Shared Stagewise O2O DINs\nappear to be a preferred choice due to slightly increased\nparameters and negligible extra computation costs.\n6.3 Semantic Segmentation\nHere we further evaluate IndexNet on semantic segmenta-\ntion. Semantic segmentation aims to predict a dense label-\ning map for each image where each pixel is labeled into\none category. Since the FCNs were introduced [2], FCN-\nbased encoder-decoder architectures have been studied ex-\ntensively [3], [5], [6], [45]. Efforts have been spent on how\nto encode contextual information, We use SegNet [3] as\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 12\nTABLE 7\nAverage PSNR (dB) and SSIM Results of Various Noise Levels on the BSD68 and Set12 Image Denoising Benchmarks\nMethod #Param. GFLOPs BSD68 Set12\nNoise Level 15 25 50 15 25 50\nDnCNN [10] 0.56M 25.89 31.74/0.9410 29.22/0.9015 26.23/0.8269 32.87/0.9544 30.42/0.9296 27.17/0.8775\nDnCNN-SegNet 7.09M 18.14 30.74/0.9278 28.27/0.8752 24.88/0.7437 31.91/0.9395 28.98/0.8881 24.99/0.7485\nDnCNN-max-carafe 7.29M 19.11 25.70/0.7578 21.41/0.5670 15.40/0.2988 24.64/0.6997 20.19/0.4965 14.22/0.2489\nDnCNN-avg-carafe 7.29M 19.11 25.70/0.7578 21.43/0.5673 15.56/0.2968 24.64/0.6997 20.20/0.4968 14.26/0.2460\nDnCNN-conv-carafe 7.29M 15.23 25.70/0.7578 21.43/0.5672 15.50/0.2994 24.64/0.6997 20.20/0.4967 14.22/0.2496\nNL C \u0001\nHINs\n+7.17K 18.16 31.13/0.9357 29.02/0.8997 26.29/0.8281 32.71/0.9536 30.28/0.9285 27.20/0.8789\nX +0.69M 19.30 31.15/0.9356 29.01/0.8999 26.29/0.8301 32.77/0.9537 30.36/0.9295 27.18/0.8799\nX X +2.76M 22.75 31.20/0.9365 29.05/0.9004 26.30/0.8305 32.79/0.9541 30.37/0.9300 27.22/0.8804\nModelwise O2O DINs\n+16 18.14 31.22/0.9366 29.06/0.9002 25.84/0.8294 32.83/0.9545 30.42/0.9302 26.21/0.8782\nX +56 18.14 30.64/0.9255 27.39/0.8391 24.15/0.6776 31.92/0.9386 27.97/0.8330 24.14/0.6747\nX X +152 18.14 30.87/0.9296 27.70/0.8617 24.09/0.6939 32.23/0.9432 28.31/0.8677 23.85/0.6634\nShared Stagewise O2O DINs\n+48 18.14 31.14/0.9364 29.05/0.9002 26.32/0.8310 32.80/0.9542 30.41/0.9302 27.24/0.8807\nX +168 18.14 31.20/0.9365 28.97/0.9000 26.18/0.8272 32.83/0.9545 30.43/0.9302 27.24/0.8801\nX X +456 18.14 31.22/0.9366 29.07/0.9004 26.31/0.8311 32.82/0.9543 30.41/0.9300 27.27/0.8814\nUnshared Stagewise O2O DINs\n+7.17K 18.16 31.17/0.9366 28.25/0.8944 25.02/0.8235 32.80/0.9544 30.23/0.9286 26.41/0.8675\nX +25.1K 18.19 31.25/0.9368 29.06/0.9002 26.33/0.8306 32.77/0.9541 30.43/0.9303 27.29/0.8814\nX X +68.1K 18.32 31.21/0.9364 27.68/0.8740 26.33/0.8312 32.83/0.9544 30.32/0.9288 27.24/0.8807\nM2O DINs\n+1.38M 20.44 31.22/0.9365 29.03/0.9005 26.33/0.8316 32.82/0.9544 30.42/0.9302 27.28/0.8812\nX +3.45M 23.88 31.23/0.9368 29.07/0.9002 26.26/0.8278 32.84/0.9546 30.44/0.9304 27.28/0.8808\nX X +11.7M 37.67 31.23/0.9365 29.06/0.8996 26.34/0.8315 32.82/0.9545 30.43/0.9301 27.29/0.8803\nNL: Non-Linearity; C: Context. \u0001indicates increased parameters compared to the SegNet-DnCNN baseline. GFLOPs are measured on a 224\u0002224\u00021input.\nIndexNetIndexNetIndexNetIPIUCIPIPIUIUIUCIU\nCIUE2D2D3D4D5E3E4E5\nM2M3M4\nIPIUCIndexed PoolingIndexed UpsamplingConcatenation\nFig. 13. IndexNet-guided feature pyramid network and multi-level feature\nfusion.\nour baseline because IndexNet is primarily inspired by the\nunpooling operator in SegNet. We follow the experimental\nsetting in [3] and report performance on the SUN RGB-\nD [12] dataset. We use RGB as the input (depth is not used).\nThe standard mean Intersection-over-Union (mIoU) is used\nas the evaluation metric. In addition, we also compare\nagainst the recent UperNet [46]. We evaluate UperNet on\nthe ADE20K dataset [47].\n6.3.1 Network Architecture and Implementation Details\nNetwork Architecture. The architecture of SegNet employs the\nﬁrst13layers of the VGG-16 model pretrained on ImageNet\nas the encoder. The decoder uses unpooling for upsampling.\nEach unpooling layer is followed by the same number of\nconvolutional layers as in the corresponding encoder stage.\nOverall, SegNet has 5downsampling and 5upsampling\nstages. Convolutional layers in the decoding stage mainly\nplay a role to smooth the feature maps generated by un-\npooling. To insert IndexNet, the only modiﬁcation is to\nreplace max pooling and unpooling layers with IPandIU,\nrespectively, which is straightforward.UperNet builds upon the idea of Pyramid Pooling Mod-\nule (PPM) [48] and Feature Pyramid Network (FPN) [49].\nUperNet also implements a Multi-level Feature Fusion\n(MFF) module that fuses multi-resolution feature maps by\nconcatenation. In FPN, downsampling is implemented by\n2-stride convolution, and upsampling uses bilinear inter-\npolation. It produces four feature levels fD2;D3;D4;D5g\nwith output strides of f4;8;16;32g, conditioned on the\nencoder features fE2;E3;E4;E5g. MFF further fuses four\nlevels of features and generates the output M2with an\noutput stride of 4. To insert IndexNet, three IndexNet blocks\ncan be inserted into the encoder to generate index maps\nto guide upsampling. The same index maps can also be\nused in MFF in a sequential upsampling manner to fuse\nfeatures, as shown in Fig. 13. Note that, in theory IndexNet\ncan also be applied to PPM, because PPM itself has inter-\nnal downsampling and upsampling stages. However, we\ndiscourage the use of IndexNet in PPM, because it will\nsigniﬁcantly increase parameters (due to mixed downsam-\npling/upsampling rates). In this case blind upsampling\nsuch as NN/bilinear interpolation may be a better choice.\nTraining Details. On the SUN RGB-D dataset, the VGG-\n16 model pretrained on ImageNet with BN layers is used.\nWe employ the standard data augmentation strategies: ran-\ndom scaling, random cropping 320\u0002320 sub-images, and\nrandom horizontal ﬂipping. We learn the model with the\nstandard softmax loss. Encoder parameters are pretrained\non ImageNet. All other parameters are initialized with the\nimproved Xavier [44]. The SGD optimizer [37] is used with a\nmomentum of 0:9and a weight decay of 0:0001 . We train the\nmodel with a batch size of 16for300epochs (around 90;000\niterations). The learning rate is initially set to 0:01and\nreduced by 10\u0002at the 250-th and 280-th epoch, respectively.\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 13\nTABLE 8\nPerformance on the SUN RGB-D Dataset.\nMethod #Param. GFLOPs mIoU\nSegNet [3] 24.96M 24.76 32.47\nSegNet-carafe 25.35M 25.75 36.30\nNL C \u0001\nHINs\n+23.55K 24.79 33.25\nX +4.90M 26.40 33.11\nX X +19.55M 31.28 33.31\nModelwise O2O DINs\n+16 24.76 33.18\nX +56 24.76 33.70\nX X +152 24.77 33.26\nShared Stagewise O2O DINs\n+80 24.76 33.26\nX +280 24.76 33.97\nX X +760 24.77 33.41\nUnshared Stagewise O2O DINs\n+0.02M 24.79 33.27\nX +0.08M 24.82 33.59\nX X +0.22M 24.96 33.50\nM2O DINs\n+9.76M 28.02 33.28\nX +24.44M 32.90 33.51\nX X +83.02M 52.42 33.48\nNL: Non-Linearity; C: Context. \u0001indicates increased parameters com-\npared to the SegNet baseline. GFLOPs are measured on a 224\u0002224\u00023\ninput.\nThe BN layers of the encoder are ﬁxed.\nOn the ADE20K benchmark, we use the MobileNetV2\npretrained on ImageNet as the encoder and UperNet the\ndecoder. Due to limited computational resources, only this\nsetting enables us to train a model on 4GPUs with a batch\nsize of 16following the ofﬁcial implementation of UperNet\nand provided experimental settings.1\n6.3.2 Results on the SUN RGB-D Dataset\nWe report the results in Table 8. All index networks show\nimprovements over the baseline, among which Modelwise\nand Shared Stagewise O2O DINs improve the baseline with\nfew extra parameters and GFLOPs. Compared with other\ntypes of IndexNet, M2O DINs and HINs (particularly under\nthe setting of “Nonlinearity+Context”) increase many pa-\nrameters and GFLOPs but do not exhibit clear advantages.\nFrom the qualitative results shown in Fig. 14, we can see\nthat the improvement comes from the ability of IndexNet\nsuppressing fractured predictions that frequently appears\nin the baseline SegNet. IndexNet seemingly does better in\nproducing predictions at boundaries.\nNotice that, in contrast to the behaviour in matting and\ndenoising, CARAFE signiﬁcantly enhances the performance\nin segmentation ( 32:47!36:30), outperforming IndexNet.\nWe observe that CARAFE tends to produce consistent\nregion-wise predictions. A plausible explanation is that,\nCARAFE is designed in a way to tackle region-sensitive\ntasks such as semantic segmentation where region-wise\nmatching between predictions and ground truths matters,\nwhile IndexNet prefers detail-sensitive tasks like image\nmatting where errors come from detail-rich regions.\n1. https://github.com/CSAILVision/semantic-segmentation-pytorchTABLE 9\nPerformance on the ADE-20K Dataset\nFPN MFF mIoU Pixel Accuracy ( %)\nBilinear Bilinear 37.08 78.29\nIndexNet Bilinear 36.25 78.23\nBilinear IndexNet 36.90 78.27\nIndexNet IndexNet 37.62 78.29\nCARAFE Bilinear 37.76 78.81\nBilinear CARAFE 38.03 78.51\nCARAFE CARAFE 38.31 78.90\nMFF: Multi-level Feature Fusion. Only HINs (‘Linear’) are eval-\nuated due to varied decoder feature dimensionality. The best\nperformance is boldfaced.\n6.3.3 Results on the ADE20K Dataset\nHere we conduct ablative studies to highlight the role of up-\nsampling in UperNet. Both IndexNet and CARAFE are con-\nsidered. In addition to the full replacement of upsampling\noperators following Fig. 13, we also replace bilinear upsam-\npling either in FPN or in MFF with IndexNet/CARAFE.\nResults are shown in Table 9. It can be observed that,\nIndexNet improves UperNet ( 37:08!37:62) only when\nbilinear upsampling in FPN and MFF is simultaneously\nreplaced. However, when only one component is modiﬁed,\nIndexNet even leads to negative results. This suggests that\nthe guided information should be used consistently in the\ndecoder. In addition, CARAFE also works better than In-\ndexNet, showing that spatial information may not play a\ncritical role in semantic segmentation.\n6.4 Monocular Depth Estimation\nEstimating per-pixel depth from a single image is challeng-\ning because one needs to recover 3D information from a\n2D plane. With deep learning, signiﬁcant progress has been\nwitnessed [50], [51], [52]. We use the recent FastDepth [52] as\nour baseline. We compare the performance with/without In-\ndexNet on the NYUv2 dataset [13] with the ofﬁcial train/test\nsplit. To be in consistent with [52], the following metrics are\nused to quantify the performance:\n\u000froot mean square error (rms):q\n1\nTPT\ni=1(di\u0000gi)2;\n\u000faccuracy with threshold th: percentage (%) of d1,s:t:\nmax\u0010\nd1\ng1;g1\nd\u0011\n=\u000e1<th .\n6.4.1 Network Architecture and Implementation Details\nNetwork Architecture. FastDepth is an encoder-decoder ar-\nchitecture, with MobileNet as its backbone. Here we choose\nthe best upsampling option suggested by the authors [52]\nwhere upsampling is implemented by \u00022NN interpolation\nand5\u00025convolution. Hence, our baseline is FastDepth-\nNNConv5: downsampling with 2-stride convolution and\nupsampling via NN interpolation. We also modify this base-\nline by changing the stride-2 convolution to be stride-1 fol-\nlowed by max-pooling, named as FastDepth-P-NNConv5.\nFig. 15 shows how we insert IndexNet into FastDepth.\nSimilar to the modiﬁcations applied to the matting network,\nstride-2 convolution layers in the encoder are changed to\nbe stride-1, followed by IP, and the NN interpolation in\nthe decoder is replaced with IU. To compare IndexNet with\nCARAFE, we build two additional baselines: FastDepth-\ncarafe and FastDepth-P-carafe, where NNConv5 is modi-\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 14\nFig. 14. Semantic segmentation results on the SUNRGB-D dataset. From left to right, the original image, ground-truth, SegNet, and ours (Shared\nStagewise O2O DIN with ‘Nonlinearity’).\nMobileNet\nIndexNet\nEncoder DecoderLayers of \ninterest\nFig. 15. Our modiﬁed FastDepth [52] architecture.\nﬁed to CARAFE in FastDepth-NNConv5 and FastDepth-P-\nNNConv5.\nTraining Details. We follow similar training settings used\nby FastDepth [52]. `1loss is used. Random rotation, random\nscaling and random horizontal ﬂipping are used for data\naugmentation. The initial learning rate is set to 0:01and\nreduced by\u000210every 5epochs. The SGD optimizer is used\nwith a momentum of 0:9and a weight decay of 0:0001 .\nEncoder weights are pretrained on ImageNet [36]. A batch\nsize of 16is used to train the network for 30epochs in total.\n6.4.2 Results on the NYUDv2 Dataset\nWe report the results in Table 10. We observe that almost\nall types of IndexNet improve the performance compared\nto the baselines except for the most light-weight design—\nlinear Modelwise O2O DIN. It may be because only 16pa-\nrameters are not sufﬁcient to model local variations of high-\ndimensional feature maps. Note that, Unshared Stagewise\nO2O DINs (with only linear mappings) shows clear im-\nprovements with only slightly increased parameters. HINs\nand M2O DINs increase a large amount of parameters and\nﬂoating-point calculations because of the high dimensional-\nity of feature maps, while the improved performance is not\nproportional to such a high cost. Some qualitative resultsTABLE 10\nPerformance of FastDepth [52] on the NYUDv2 Dataset.\nMethod #Param. GFLOPs rms\u000e1<1:25\nFastDepth-NNConv5 3.96M 0.69 0.567 0.781\nFastDepth-P-NNConv5 3.96M 1.01 0.577 0.778\nFastDepth-carafe 4.31M 1.63 0.558 0.790\nFastDepth-P-carafe 4.31M 1.96 0.571 0.782\nNL C \u0001\nHINs\n+31.23K 1.03 0.566 0.784\nX +11.17M 2.65 0.565 0.786\nX X +44.62M 7.53 0.559 0.787\nModelwise O2O DINs\n+16 1.02 0.569 0.778\nX +56 1.02 0.568 0.785\nX X +152 1.02 0.564 0.786\nShared Stagewise O2O DINs\n+80 1.02 0.562 0.783\nX +280 1.02 0.565 0.786\nX X +760 1.02 0.567 0.783\nUnshared Stagewise O2O DINs\n+31.23K 1.03 0.556 0.789\nX +0.11M 1.06 0.564 0.786\nX X +0.30M 1.16 0.562 0.788\nM2O DINs\n+22.30M 4.27 0.563 0.783\nX +55.78M 9.15 0.562 0.786\nX X +189.57M 28.67 0.565 0.787\nNL: Non-Linearity; C: Context. \u0001indicates increased parameters compared to\nthe standard FastDepth baseline. GFLOPs are measured on a 224\u0002224\u00023input.\nThe best performance is boldfaced.\nare further illustrated in Fig. 16. We observe that IndexNet\nexhibits better boundary delineation than the baseline, e.g.,\nthe edge of the desk, and the contour of the woman. More-\nover, CARAFE achieves comparable performance against\nIndexNet in this task.\nIn addition, we report the results of applying IndexNet\nto a state-of-the-art model [53] in Table 11. We modify the\nusage of IndexNet here by taking the same feature fusion\nstrategies shown in Fig. 13. Other implementation details\nand evaluations are kept consistent with [14]. Compared\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 15\nTABLE 11\nPerformance of Hu et al. [53] on the NYUDv2 dataset\nrms rel log10 \u000e<1:25\u000e<1:252\u000e<1:253\nHuet al. [53] 0.558 0.129 0.055 0.837 0.968 0.992\nHuet al. + IndexNet 0.554 0.128 0.054 0.843 0.968 0.992\nOnly HIN (‘Nonlinear+Context’) is evaluated due to varied feature dimensionality of decoder and multi-level feature fusion.\nreland log10 denote the average relative error and average log10error [51], respectively. The best performance is boldfaced.\nwith the baseline, IndexNet shows improvement in the ﬁrst\nfour metrics.\n6.5 Insights Towards Good Practices\nAs a summary of our evaluations, here we provide some\nguidelines for using guided/blind upsampling:\n1) In detail-sensitive tasks, such as image matting, image\nrestoration, and edge detection, the spatial information\nis important. Thus guided upsampling may be pre-\nferred.\n2) Blind upsampling may be used in the situation when\ncomputational budget is limited because most blind\nupsampling operators are non-parametric and compu-\ntationally efﬁcient.\n3) In image matting, the best IndexNet conﬁguration is\n“M2O DINs+Nonlinearity+Context”. This conﬁgura-\ntion is also true for the image reconstruction experiment\nand image denoising, where M2O DINs exhibit the best\nperformance and the most stable behavior, respectively.\nHence, the capacity of IndexNet is closely related to\nthe complexity of local patterns. M2O DINs is preferred\nin a detail- or boundary-sensitive task, but one should\nalso be aware of the increased model parameters and\ncomputation costs, especially when the feature maps\nare high-dimensional.\n4) If one prefers a ﬂexible decoder design, e.g., squeez-\ning/enlarging the dimensionality of the decoder feature\nmap, HINs are good choices, because DINs only gener-\nate index maps whose dimensionality is identical to the\ninput feature map.\n5) For real-time applications, Shared Stagewise O2O DINs\nare the ﬁrst choices. Model parameters increased by\nShared Stagewise O2O DINs are comparable to Mod-\nelwise O2O DINs, and the extra GFLOPs are also\nnegelectable. Shared Stagewise O2O DINs, however,\nalways work better than Modelwise O2O DINs for\napplications considered in this work. It implies that\neach upsampling stage should learn a stage-speciﬁc\nindex function;\n6) It is worth noting that, the current implementation\nof IndexNet has some limitations. Currently IndexNet\nonly implements single-point upsampling—each up-\nsampled feature point is only associated with a single\npoint. In this sense, we may not simulate the behavior\nof bilinear interpolation where each upsampled point is\naffected by multiple points of a local region.\n7 C ONCLUSION\nInspired by an observation in image matting, we examine\nthe role of indices and present a uniﬁed view of upsampling\noperators using the notion of index functions. We showthat an index function can be learned within a proposed\nindex-guided encoder-decoder framework. In this frame-\nwork, indices are learned with a ﬂexible network module\ntermed IndexNet, and are used to guide downsampling and\nupsampling using IPandIU. IndexNet itself is also a sub-\nframework that can be designed depending on the task at\nhand. We investigate ﬁve index networks, and demonstrate\ntheir effectiveness on four dense prediction tasks. We be-\nlieve that IndexNet is an important step towards generic\nupsampling operators for deep networks.\nREFERENCES\n[1] M. D. Zeiler and R. Fergus, “Visualizing and understanding\nconvolutional networks,” in Proc. European Conference on Computer\nVision (ECCV) , 2014, pp. 818–833. 1, 2\n[2] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional net-\nworks for semantic segmentation,” in Proc. IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2015, pp. 3431–\n3440. 1, 2, 3, 8, 11\n[3] V . Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A deep\nconvolutional encoder-decoder architecture for image segmenta-\ntion,” IEEE Transactions on Pattern Analysis and Machine Intelligence ,\nvol. 39, no. 12, pp. 2481–2495, 2017. 1, 2, 8, 11, 12, 13\n[4] W. Shi, J. Caballero, F. Husz ´ar, J. Totz, A. P . Aitken, R. Bishop,\nD. Rueckert, and Z. Wang, “Real-time single image and video\nsuper-resolution using an efﬁcient sub-pixel convolutional neural\nnetwork,” in Proc. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2016, pp. 1874–1883. 1, 2, 3\n[5] G. Lin, A. Milan, C. Shen, and I. Reid, “ReﬁneNet: Multi-path\nreﬁnement networks for high-resolution semantic segmentation,”\ninProc. IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2017, pp. 1925–1934. 1, 2, 9, 11\n[6] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam,\n“Encoder-decoder with atrous separable convolution for semantic\nimage segmentation,” in Proc. European Conference on Computer\nVision (ECCV) , 2018. 1, 2, 9, 11\n[7] N. Xu, B. Price, S. Cohen, and T. Huang, “Deep image matting,”\ninProc. IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2017, pp. 2970–2979. 1, 2, 8, 9, 10\n[8] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The\ncase for learned index structures,” in Proc. International Conference\non Management of Data , 2018, pp. 489–504. 1\n[9] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proc.\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2018, pp. 4510–4520. 2, 8\n[10] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, “Beyond\na Gaussian denoiser: residual learning of deep CNN for image\ndenoising,” IEEE Transactions on Image Processing , vol. 26, no. 7,\npp. 3142–3155, 2017. 2, 11, 12\n[11] S. Roth and M. J. Black, “Fields of experts,” International Journal of\nComputer Vision , vol. 82, no. 2, p. 205, 2009. 2\n[12] S. Song, S. P . Lichtenberg, and J. Xiao, “SUN RGB-D: A RGB-D\nscene understanding benchmark suite,” in Proc. IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2015, pp. 567–\n576. 2, 12\n[13] N. Silberman, D. Hoiem, P . Kohli, and R. Fergus, “Indoor segmen-\ntation and support inference from rgbd images,” in Proc. European\nConference on Computer Vision (ECCV) , 2012, pp. 746–760. 2, 13\n[14] H. Lu, Y. Dai, C. Shen, and S. Xu, “Indices matter: Learning to\nindex for deep image matting,” in Proc. IEEE/CVF International\nConference on Computer Vision (ICCV) , 2019, pp. 3266–3275. 2, 14\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 16\nFig. 16. Qualitative results on the NYUDv2 dataset. From left to right, the original image, ground-truth, FastDepth-NNConv5, and ours (Unshared\nStagewise O2O DIN with ‘Linear’).\n[15] A. Odena, V . Dumoulin, and C. Olah, “Deconvolution and\ncheckerboard artifacts,” Distill , vol. 1, no. 10, p. e3, 2016. 2\n[16] C. Osendorfer, H. Soyer, and P . Van Der Smagt, “Image super-\nresolution with fast approximate convolutional sparse coding,”\ninProc. International Conference on Neural Information Processing\n(ICONIP) , 2014, pp. 250–257. 2\n[17] T.-J. Yang, M. D. Collins, Y. Zhu, J.-J. Hwang, T. Liu, X. Zhang,\nV . Sze, G. Papandreou, and L.-C. Chen, “DeeperLab: Single-shot\nimage parser,” arXiv , 2019. 2\n[18] J. Wang, K. Chen, R. Xu, Z. Liu, C. C. Loy, and D. Lin, “CARAFE:\nContext-aware reassembly of features,” in Proc. IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV) , 2019. 2, 6\n[19] M. Jaderberg, K. Simonyan, A. Zisserman et al. , “Spatial trans-\nformer networks,” in Advances in Neural Information Processing\nSystems (NIPS) , 2015, pp. 2017–2025. 3\n[20] X. Jia, B. De Brabandere, T. Tuytelaars, and L. V . Gool, “Dynamic\nﬁlter networks,” in Advances in Neural Information Processing Sys-\ntems (NIPS) , 2016, pp. 667–675. 3\n[21] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\n“Deformable convolutional networks,” in Proc. IEEE International\nConference on Computer Vision (ICCV) , 2017, pp. 764–773. 3\n[22] V . Mnih, N. Heess, A. Graves et al. , “Recurrent models of visual\nattention,” in Advances in Neural Information Processing Systems\n(NIPS) , 2014, pp. 2204–2212. 3, 6\n[23] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang, and\nX. Tang, “Residual attention network for image classiﬁcation,” in\nProc. IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2017, pp. 3156–3164. 3\n[24] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\nProc. IEEE Conference on Computer Vision and Pattern Recognition ,\n2018, pp. 7132–7141. 3\n[25] S. Woo, J. Park, J.-Y. Lee, and I. So Kweon, “CBAM: Convolutional\nblock attention module,” in Proc. European Conference on Computer\nVision (ECCV) , 2018, pp. 3–19. 3\n[26] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image\ndataset for benchmarking machine learning algorithms,” arXiv ,\n2017. 7\n[27] Q. Chen, D. Li, and C.-K. Tang, “KNN matting,” IEEE Transactions\non Pattern Analysis and Machine Intelligence , vol. 35, no. 9, pp. 2175–\n2188, 2013. 8\n[28] Y.-Y. Chuang, B. Curless, D. H. Salesin, and R. Szeliski, “A\nbayesian approach to digital matting,” in Proc. IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2001, pp. 264–271.\n8\n[29] K. He, C. Rhemann, C. Rother, X. Tang, and J. Sun, “A global\nsampling method for alpha matting,” in Proc. IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) . IEEE, 2011, pp.\n2049–2056. 8\n[30] A. Levin, D. Lischinski, and Y. Weiss, “A closed-form solution to\nnatural image matting,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence , vol. 30, no. 2, pp. 228–242, 2008. 8, 10\n[31] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hier-\narchies for accurate object detection and semantic segmentation,”inProc. IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2014, pp. 580–587. 8\n[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁ-\ncation with deep convolutional neural networks,” in Advances in\nNeural Information Processing Systems (NIPS) , 2012, pp. 1097–1105.\n8\n[33] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,\nP . Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\ncontext,” in Proc. European Conference on Computer Vision (ECCV) ,\n2014, pp. 740–755. 8\n[34] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and\nA. Zisserman, “The pascal visual object classes (voc) challenge,”\nInternational Journal of Computer Vision , vol. 88, no. 2, pp. 303–338,\n2010. 8\n[35] C. Rhemann, C. Rother, J. Wang, M. Gelautz, P . Kohli, and P . Rott,\n“A perceptually motivated online benchmark for image matting,”\ninProc. IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2009, pp. 1826–1833. 8\n[36] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Im-\nageNet: A large-scale hierarchical image database,” in Proc. IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) . Ieee,\n2009, pp. 248–255. 8, 14\n[37] D. P . Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,” in Proc. International Conference on Learning Representations\n(ICLR) , 2015. 8, 12\n[38] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proc. IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2016, pp. 770–778. 8\n[39] O. Ronneberger, P . Fischer, and T. Brox, “U-Net: Convolutional\nnetworks for biomedical image segmentation,” in Proc. Interna-\ntional Conference on Medical Image Computing and Computer-Assisted\nIntervention (MICCAI) , 2015, pp. 234–241. 8\n[40] C. Chen, Q. Chen, J. Xu, and V . Koltun, “Learning to see in the\ndark,” in Proc. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2018, pp. 3291–3300. 11\n[41] R. Jaroensri, C. Biscarrat, M. Aittala, and F. Durand, “Generating\ntraining data for denoising real rgb images via camera pipeline\nsimulation,” arXiv , 2019. 11\n[42] X. Mao, C. Shen, and Y.-B. Yang, “Image restoration using very\ndeep convolutional encoder-decoder networks with symmetric\nskip connections,” in Advances in Neural Information Processing\nSystems (NIPS) , 2016, pp. 2802–2810. 11\n[43] Y. Chen and T. Pock, “Trainable nonlinear reaction diffusion: A\nﬂexible framework for fast and effective image restoration,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence , vol. 39,\nno. 6, pp. 1256–1272, 2016. 11\n[44] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:\nSurpassing human-level performance on imagenet classiﬁcation,”\ninProc. IEEE International Conference on Computer Vision (ICCV) ,\n2015, pp. 1026–1034. 11, 12\n[45] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.\nYuille, “Deeplab: Semantic image segmentation with deep convo-\nlutional nets, atrous convolution, and fully connected crfs,” IEEE\n\nMANUSCRIPT. V1: AUGUST 2019; V2: APRIL 2020 17\nTransactions on Pattern Analysis and Machine Intelligence , vol. 40,\nno. 4, pp. 834–848, 2017. 11\n[46] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, “Uniﬁed perceptual\nparsing for scene understanding,” in Proc. European Conference on\nComputer Vision (ECCV) , 2018, pp. 418–434. 12\n[47] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba,\n“Scene parsing through ade20k dataset,” in Proc. IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2017. 12\n[48] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing\nnetwork,” in Proc. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2017, pp. 2881–2890. 12\n[49] T.-Y. Lin, P . Doll ´ar, R. Girshick, K. He, B. Hariharan, and S. Be-\nlongie, “Feature pyramid networks for object detection,” in Proc.\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2017, pp. 2117–2125. 12\n[50] F. Liu, C. Shen, G. Lin, and I. Reid, “Learning depth from single\nmonocular images using deep convolutional neural ﬁelds,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence , vol. 38,\nno. 10, pp. 2024–2039, 2015. 13\n[51] K. Xian, C. Shen, Z. Cao, H. Lu, Y. Xiao, R. Li, and Z. Luo, “Monoc-\nular relative depth perception with web stereo data supervision,”\ninProc. IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2018, pp. 311–320. 13, 15\n[52] D. Wofk, F. Ma, T.-J. Yang, S. Karaman, and V . Sze, “Fastdepth: Fast\nmonocular depth estimation on embedded systems,” in Proc. IEEE\nInternational Conference on Robotics and Automation (ICRA) , 2019. 13,\n14\n[53] J. Hu, M. Ozay, Y. Zhang, and T. Okatani, “Revisiting single image\ndepth estimation: toward higher resolution maps with accurate\nobject boundaries,” in Proc. IEEE Winter Conference on Applications\nof Computer Vision (WACV) , 2019, pp. 1043–1051. 14, 15\nHao Lu received the Ph.D. degree from\nHuazhong University of Science and Technol-\nogy, Wuhan, China, in 2018. He is currently a\nPostdoctoral Fellow at School of Computer Sci-\nence, The University of Adelaide, Australia. His\nresearch interests include computer vision and\nmachine learning. He is currently working on\ndense prediction problems.\nYutong Dai received the M.Sc. degree from\nSoutheast University, Nanjing, in 2018. She is\ncurrently pursuing the Ph.D. degree at The Uni-\nversity of Adelaide, Australia. Her research inter-\nests include computer vision and deep learning.\nShe is currently working on image matting.\nChunhua Shen is a Professor of Computer Science at The University\nof Adelaide, Australia.\nSongcen Xu received the Ph.D. degree in electronic engineering from\nthe University of Y ork, U.K. in 2015. He is with Noah’s Ark Lab, Huawei\nTechnologies.",
  "textLength": 86952
}