{
  "paperId": "5f956f4d17e9e61edd3b143e8ffa676c597c7de9",
  "title": "On Approximability of ℓ22 Min-Sum Clustering",
  "pdfPath": "5f956f4d17e9e61edd3b143e8ffa676c597c7de9.pdf",
  "text": "On Approximability of ℓ2\n2Min-Sum Clustering\nKarthik C. S.∗Euiwoong Lee†Yuval Rabani‡\nChris Schwiegelshohn§Samson Zhou▽\nAbstract\nTheℓ2\n2min-sum k-clustering problem is to partition an input set into clusters C1,. . .,Ckto\nminimize ∑k\ni=1∑p,q∈Ci∥p−q∥2\n2. The objective is a density-based clustering and can be more\neffective than the traditional centroid-based clustering like k-median and k-means in capturing\ncomplex structures in data that may not be linearly separable, such as when the clusters have\nirregular, non-convex shapes or are overlapping. Although ℓ2\n2min-sum k-clustering is NP-hard,\nit is not known whether it is NP-hard to approximate ℓ2\n2min-sum k-clustering beyond a certain\nfactor.\nIn this paper, we give the first hardness-of-approximation result for the ℓ2\n2min-sum k-\nclustering problem. We show that it is NP-hard to approximate the objective to a factor better\nthan 1.056 and moreover, assuming a balanced variant of the Johnson Coverage Hypothesis, it\nis NP-hard to approximate the objective to a factor better than 1.327.\nWe then complement our hardness result by giving a nearly linear time parameterized PTAS\nforℓ2\n2min-sum k-clustering running in time O\u0010\nn1+o(1)d·exp((k·ε−1)O(1))\u0011\n, where dis the\nunderlying dimension of the input dataset.\nFinally, we consider a learning-augmented setting, where the algorithm has access to an\noracle that outputs a label i∈[k]for input point, thereby implicitly partitioning the input dataset\ninto kclusters that induce an approximately optimal solution, up to some amount of adversarial\nerror α∈h\n0,1\n2\u0011\n. We give a polynomial-time algorithm that outputs a1+γα\n(1−α)2-approximation to\nℓ2\n2min-sum k-clustering, for a fixed constant γ>0. Therefore, our algorithm improves smoothly\nwith the performance of the oracle and can be used to achieve approximation guarantees better\nthan the NP-hard barriers for sufficiently accurate oracles.\n∗Rutgers University. E-mail: karthik.cs@rutgers.edu\n†University of Michigan. E-mail: euiwoong@umich.edu\n‡The Hebrew University of Jerusalem. E-mail: yrabani@cs.huji.ac.il\n§Aarhus University. E-mail: cschwiegelshohn@gmail.com\n▽Texas A&M University. E-mail: samsonzhou@gmail.comarXiv:2412.03332v2  [cs.DS]  10 Apr 2025\n\n1 Introduction\nClustering is a fundamental technique that partitions an input dataset into distinct groups called\nclusters, which facilitate the identification and subsequent utilization of latent structural properties\nunderlying the dataset. Consequently, various formulations of clustering are used across a wide\nrange of applications, such as computational biology, computer vision, data mining, and machine\nlearning [JMF99, XW05].\nIdeally, the elements of each cluster are more similar to each other than to elements in other\nclusters. To formally capture this notion, a dissimilarity metric is often defined on the set of input\nelements, so that more closer objects in the metric correspond to more similar objects. Perhaps the\nmost natural goal would be to minimize the intra-cluster dissimilarity in a partitioning of the input\ndataset. This objective is called the min-sum k-clustering problem and has received significant\nattention due to its intuitive clustering objective [ GH98 ,Ind99 ,Mat00 ,Sch00 ,BCR01 ,dlVKKR03 ,\nCS07, ADHP09, BFSS19, BOR21, CKL21].\nIn this paper, we largely focus on the ℓ2\n2min-sum k-clustering formulation. Formally, the input\nis a set Xofnpoints in Rdand the goal is to partition X=C1˙∪ · · · ˙∪Ckinto kclusters to minimize\nthe quantity\nmin\nC1,...,Ckk\n∑\ni=1∑\np,q∈Ci∥p−q∥2\n2,\nwhere ∥ · ∥ 2denotes the standard Euclidean ℓ2norm.\nWhereas classical centroid-based clustering problems such as k-means and k-median leverage\ndistances between data points and cluster centroids to identify convex shapes that partition the\ndataset, min-sum k-clustering is a density-based clustering that can handle complex structures in\ndata that may not be linearly separable. In particular, min-sum k-clustering can be more effective\nthan traditional centroid-based clustering in scenarios where clusters have irregular, non-convex\nshapes or overlapping clusters. A simple example of the ability of min-sum clustering to capture\nmore natural structure is an input that consists of two concentric dense rings of points in the\nplane. Whereas min-sum clustering can partition the points into the separate rings, centroid-based\nclustering will instead create a separating hyperplane between these points, thereby “incorrectly”\ngrouping together points of different rings. See Figure 1 for an example of the ability of min-sum\nclustering to capture natural structure in cases where centroid-based clustering fails.\nMoreover, min-sum clustering satisfies Kleinberg’s consistency axiom [ Kle02 ], which informally\ndemands that the optimal clustering for a particular objective should be preserved when distances\nbetween points inside a cluster are shrunk and distances between points in different clusters are\nexpanded. By contrast, many centroid-based clustering objectives, including k-means and k-median,\ndo not satisfy Kleinberg’s consistency axiom [MNV12].\nOn the other hand, theoretical understanding of density-based clustering objectives such as\nmin-sum k-clustering is far less developed than that of their centroid-based counterparts. It can\nbe shown that min-sum k-clustering with the ℓ2\n2cost function is NP-hard, using arguments from\n[ADHP09 ]. The problem is NP-hard even for k=2[dlVK01 ] in the metric case, where the only\navailable information about the points is their pairwise dissimilarity, c.f., Section 1.3 for a summary\nof additional related work. In fact, for general kin the metric case, it is NP-hard to approximate\nthe problem within a 1.415 -multiplicative factor [ GI03 ,CKL21 ]. However, no such hardness of\napproximation is known for the Euclidean case, i.e., ℓ2\n2min-sum, where the selected cost function is\n– 1 –\n\n(a) Input dataset (b) Centroid-based clustering (c) Density-based clustering\nFig. 1: Clustering of input dataset in Figure 1a with k=2. Figure 1b is an optimal centroid-based\nclustering, e.g., k-median or k-means, while the more natural clustering in Figure 1c is an optimal\ndensity-based clustering, e.g., ℓ2min-sum k-clustering.\nbased on the geometry of the underlying space; the only known lower bound is the NP-hardness of\nthe problem [ADHP09, BOR21, AKP24]. Thus a fundamental open question is:\nQuestion 1.1. Isℓ2\n2min-sum k-clustering APX-hard? That is, does there exist a natural hardness-of-\napproximation barrier for polynomial time algorithms?\nDue to existing APX-hardness results for centroid-based clustering such as k-means and k-\nmedian [ LSW17 ,CK19 ,CKL22 ], it is widely believed that ℓ2\n2min-sum clustering is indeed APX-hard.\nThus, there has been a line of work preemptively seeking to overcome such limitations. Indeed, on\nthe positive side, [ IKI94 ] first showed that min-sum k-clustering in the d-dimensional ℓ2\n2case can\nbe solved in polynomial time if both dand kare constants. For general graphs and fixed constant k,\n[GH98 ] gave a 2-approximation algorithm using runtime nO(k). The approximation guarantees were\nimproved by a line of work [ Ind99 ,Mat00 ,Sch00 ], culminating in polynomial-time approximation\nschemes by [ dlVKKR03 ] for both the ℓ2\n2case and the metric case. Without any assumptions on d\nand k, [BCR01 ] introduced a polynomial algorithm that achieves an O\u0010\n1\nεlog1+εn\u0011\n-multiplicative\napproximation. Therefore, a long-standing direction in the study of ℓ2\n2min-sum clustering is:\nQuestion 1.2. How can we algorithmically bridge the gap between the NP-hardness of solving the ℓ2\n2\nmin-sum clustering and the large multiplicative guarantees of existing approximation algorithms?\nA standard approach to circumvent poor dependencies on the size of the input dataset is to\nsparsify the problem. Informally, we would like to reduce the search space by considering fewer\ncandidate solutions and reduce the dependency on the number of input points by aggregating them.\nFor min-sum clustering this is a particular challenge, as a candidate solution is a partition and the\ncost of that partition depends on all pairwise distances between all the points. While sparsification\nalgorithms exist for graph clustering [ JLS23 ,Lee23 ] and k-means clustering [ CSS21 ,CLS+22], where\nthe output is typically called a coreset, similar constructions are not known to exist for min-sum\nclustering.\nAnother standard approach to overcome limitations inherent in worst-case impossibility bar-\nriers is to consider beyond worst case analysis. To that end, recent works have observed that in\nmany applications, auxiliary information is often available and can potentially form the foundation\nupon which machine learning models are built. For example, previous datasets with potentially\nsimilar behavior can be used as training data for models to label future datasets. However, these\n– 2 –\n\nheuristics lack provable guarantees and can produce embarrassingly inaccurate predictions when\ngeneralizing to unfamiliar inputs [ SZS+14]. Nevertheless, learning-augmented algorithms [MV20 ]\nhave been shown to achieved both good algorithmic performance when the oracle is accurate,\ni.e., consistency, and standard algorithmic performance when the oracle is inaccurate, i.e., ro-\nbustness for a wide range of settings, such as data structure design [ KBC+18,Mit18 ,LLW22 ],\nalgorithms with faster runtime [ DIL+21,CSVZ22 ,DMVW23 ], online algorithms with better com-\npetitive ratio [ PSK18 ,GP19 ,LLMV20 ,WLW20 ,WZ20 ,BMS20 ,IKQP21 ,LV21 ,ACI22 ,AGKP22 ,\nAPT22 ,GLS+22,KBTV22 ,JLL+22,ACE+23,SLLA23 ], and streaming algorithms that are more\nspace-efficient [ HIKV19 ,IVY19 ,JLL+20,CIW22 ,CEI+22,LLL+23]. In particular, [ EFS+22,NCN23 ]\nintroduce algorithms for k-means and k-median clustering that can achieve approximation guaran-\ntees beyond the known APX-hardness limits.\n1.1 Our Contributions\nIn this paper, we perform a comprehensive study on the approximability of the ℓ2\n2min-sum\nk-clustering by answering Question 1.1 and Question 1.2.\nHardness-of-approximation of min-sum k-clustering. We first answer Question 1.1 in the affir-\nmative, by not only showing that the ℓ2\n2min-sum k-clustering is APX-hard but further giving an\nexplicit constant NP-hardness of approximation result for the problem.\nTheorem 1.3 (Hardness of approximation of ℓ2\n2min-sum k-clustering) .It is NP-hard to approximate\nℓ2\n2min-sum k-clustering to a factor better than 1.056 . Moreover, assuming the Dense and Balanced\nJohnson Coverage Hypothesis ( Balanced −JCH∗), we have that the ℓ2\n2min-sum k-clustering is NP-hard to\napproximate to a factor better than 1.327 .\nWe remark that Balanced −JCH∗in the theorem statement above is simply a balanced formula-\ntion of the recently introduced Johnson Coverage Hypothesis [CKL22].\nFast polynomial-time approximation scheme. In light of Theorem 1.3, a natural question would\nbe to closely examine alternative conditions in which we can achieve a (1+ε)-approximation to\nmin-sum k-clustering, i.e., Question 1.2. To that end, there are a number of existing polynomial-time\napproximation schemes (PTAS) [Ind99, Mat00, Sch00, dlVKKR03], the best of which uses runtime\nnO(k/ε2)for the ℓ2\n2case. However, as noted by [ CS07 ], even algorithms with runtime quadratic in\nthe size nof the input dataset are generally not sufficiently scalable to handle large datasets. In this\npaper, we present an algorithm with a running time that is nearly nearly linear. Specifically, we\nshow\nTheorem 1.4. There exists an algorithm running in time\nO\u0010\nn1+o(1)d·2η·k2·ε−12log2(k/(εδ))\u0011\n,\nfor some absolute constant η, that computes a (1+ε)-approximate solution to ℓ2\n2k-MinSum Clustering\nwith probability 1−δ.\nWe again emphasize that the runtime of 1.4 is linear in the size nof the input dataset, though it\nhas exponential dependencies in both the number kof clusters and the approximation parameter\nε>0. By contrast, the best previous PTAS uses runtime nO(k/ε2)[CS07 ], which has substantially\nworse dependency on the size nof the input dataset.\n– 3 –\n\nLearning-augmented algorithms. Unfortunately, exponential dependencies on the number kof\nclusters can still be prohibitive for moderate values of k. To that end, we turn our attention to\nlearning-augmented papers. We consider the standard label oracle model for clustering, where\nthe algorithm has access to an oracle that provides a label for each input point. Formally, for each\npoint xof the ninput points, the oracle outputs a label i∈[k]forx, so that the labels implicitly\npartition the input dataset into kclusters that induce an approximately optimal solution. However,\nthe oracle also has some amount of adversarial error that respects the precision and recall of each\ncluster; we defer the formal definition to Definition 4.3.\nOne of the reasons label oracles have been used for learning-augmented algorithms for cluster-\ning is their relative ease of acquisition via machine learning models that are trained on a similar\ndistribution of data. For example, a smaller separate dataset can be observed and used as a “train-\ning” data, an input to some heuristic to cluster the initial data, which we can then use to form a\npredictor for the actual input dataset. Indeed, implementations of label oracles have been shown to\nperform well in practice [EFS+22, NCN23].\nR R∆\nFig. 2: Note that with arbitrarily small error rate, i.e.,1\nn, a single mislabeled point among the n\ninput points causes the resulting clustering to be arbitrarily bad for ∆≫n2·R.\nWe also remark that perhaps counter-intuitively, a label oracle with arbitrarily high accuracy\ndoes not trivialize the problem. In particular, the naïve algorithm of outputting the clustering\ninduced by the labels does not work. As a simple example, consider an input dataset where half\nof the npoints are at x=0and the other half of the points are at x=1. Then for k=2, the clear\noptimal clustering is to cluster the points at the origin together, and cluster the points at x=1\ntogether, which induces the optimal cost of zero. However, if even one of the npoints is incorrect,\nthen the clustering output by the labels has cost at least 1. Therefore, even with error rate as small\nas1\nn, the multiplicative approximation of the naïve algorithm can be arbitrarily bad. See Figure 2 for\nan illustration of this example. Of course, this example does not rule out more complex algorithms\nthat combines the labels with structural properties of optimal clustering and indeed, our algorithm\nutilizes such properties.\nWe give a polynomial-time algorithm for the ℓ2\n2min-sum k-clustering that can provide guaran-\ntees beyond the computational limits of Theorem 1.3, given a sufficiently accurate oracle.\nTheorem 1.5. There exists a polynomial-time algorithm that uses a label predictor with error rate α∈\u0002\n0,1\n2\u0001\nand outputs a1+γα\n(1−α)2-approximation to the ℓ2\n2min-sum k-clustering problem, where γ=7.7forα∈\u0002\n0,1\n7\u0001\norγ=5α−2α2\n(1−2α)(1−α)forα∈\u0002\n0,1\n2\u0001\n.\nWe remark that Theorem 1.5 does not require the true error rate αas an input parameter. Because\nwe are in an offline setting, where can run Theorem 1.5 multiple times with guesses for the true\nerror rate α, in decreasing powers of1\nλfor any constant λ>1. We can then compare the resulting\nclustering output by each guess for αand take the output the best clustering.\n– 4 –\n\n1.2 Technical Overview\nHardness of approximation. Recently, the authors of [ CKL22 ] put forth the Johnson Coverage\nHypothesis ( JCH) and introduced a framework to obtain (optimal) hardness of approximation\nresults for k-median and k-means in ℓp-metrics. The proof of Theorem 1.3 builds on this framework.\nJCH roughly asserts that for large enough constant z, given as input an integer kand a collection\nofz-sets (i.e., sets each of size z) over some universe, it is NP-hard to distinguish the completeness\ncase where there is a collection Cofkmany (z−1)-sets such that every input set is covered1by\nsome set in C, from the soundness case where every collection Cofkmany (z−1)-sets does not\ncover much more than 1 −1\nefraction of the input sets (see Hypothesis 2.2 for a formal statement).\nIn this paper, we consider a natural generalization of JCH, called Balanced −JCH∗, where we\nassume that the number of input sets is “dense”, i.e., ω(k), and more importantly that in the\ncompleteness case, the collection Ccovers the input z-sets in a balanced manner, i.e., we can\npartition the input to kequal parts such that each part is completely covered by a single set in C\n(see Hypothesis 2.4 for a formal statement).\nWe now sketch the proof of Theorem 1.3 assuming Balanced −JCH∗. Given a collection of m\nmany z-sets over a universe [n]as input, we create a point for each input set, which is simply\nthe characteristic vector of the set as a subset of [n], i.e., the points are all n-dimensional Boolean\nvectors of Hamming weight z.\nIn the completeness case, from the guarantees of Balanced −JCH∗, it is easy to see that the\npoints created can be divided into kequal clusters of size m/ksuch that all the z-sets of a cluster\nare completely covered by a single (z−1)-set. This implies that the squared Euclidean distance\nbetween a pair of points within a cluster is exactly 2and thus the ℓ2\n2min-sum k-clustering cost is\nk·2·(m/k)(m/k−1)≈2m2/k.\nOn the other hand, in the soundness case, we first use the density guarantees of Balanced −JCH∗\nto argue that most clusters are not small. Then suppose that we had a low cost ℓ2\n2min-sum k-\nclustering, we look at a typical cluster and observe that the squared distance of any two points\nin the cluster must be a positive even integer, and it is exactly 2 only when the two input sets\ncorresponding to the points intersect on a (z−1)-set. Thus, if the cost of the clustering is close\ntoα·2m2/k(for some α≥1), then we argue (using convexity) that for a typical cluster that there\nmust be a (z−1)-set that covers (1−α′)m/kmany z-sets in that cluster, where α′depends on α.\nThus, from this we decode k-many (z−1)-sets which cover a large fraction of the input z-sets.\nIn order to obtain the unconditional NP-hardness result, much like in [ CKL22 ], we need to\nextend the above reduction to a more general problem. This is indeed established in Theorem 2.7,\nand after this we prove a special case of a generalization of Balanced −JCH∗(when z=3) which is\ndone in Theorem 2.6 and this involved proving additional properties of the reduction of [ CKL22 ]\nfrom the multilayered PCPs of [DGKR05, Kho02] to 3-Hypergraph Vertex Coverage.\nNearly Linear Time PTAS. An important feature of ℓ2\n2Min-Sum Clustering is that we can\nuse assignments of clusters to their mean to obtain the cost of the points in the cluster, an idea\npreviously used in [ Ind99 ,Mat00 ,Sch00 ,dlVKKR03 ]. We show how to reduce the number of\ncandidate means to a constant (depending only on kandε. The idea here is to use D2sampling\nmethods akin to k-means++ [ AV07 ]. Unfortunately, by itself, it is not sufficient as there may exist\nclusters that have significant min-sum clustering cost, but are not detectable by D2sampling. To\nthis end, we augment D2sampling via a careful pruning strategy that removes high costing points,\n1A(z−1)-set covers a z-set if the former is a subset of the latter.\n– 5 –\n\nincreasing the relative cost of clusters of high density. Thereafter, we show that given sufficiently\nmany samples, we can find a small set of suitable candidate means that are induced by a nearly\noptimal clustering.\nWhat remains to be shown is how to find an assignment of points to these centers with similar\ncost. For this, we could use a flow-based approach, but this results in a n3running time. Instead,\nwe employ a discretization and bucketing strategy that allows us to sparsify the point set while\npreserving the min-sum clustering cost, akin to coresets.\nLearning-augmented algorithm. Our starting point for our learning-augmented algorithm for\nmin-sum k-clustering is the learning-augmented algorithms for k-means clustering by [ EFS+22,\nNCN23 ]. The algorithms note that the k-means clustering objective can be decomposed across the\npoints that are given each label i∈[k]. Thus we consider the subset Piof points of the input dataset\nXthat are given label iby the oracle. Since k-means clustering objective can be further decomposed\nalong the ddimensions, then the algorithms consider Pialong each dimension.\nThe cluster Pican have an αfraction of incorrect points. The main observation is that there\ncan be two cases. Either Piincludes a number of ”bad” points that are far from the true mean and\nthus easy to identify, or Piincludes a number of “bad” points that are difficult to identify but also\nare close to the true mean and thus do not largely affect the overall k-means clustering cost. Thus\nthe algorithm simply needs to prune away the points that are far away, which can be achieved by\nselecting the interval of (1− O(α))points that has the best clustering cost. It is then shown that\nthe resulting centers provide a good approximate solution to the k-means clustering cost.\nUnfortunately, we cannot immediately utilize the previous approach because min-sum k-\nclustering is a density-based clustering rather than a centroid-based clustering. However, it is\nknown [IKI94] that we can rewrite\n∑\ni∈[k]∑\np,q∈Ci∥p−q∥2\n2=∑\ni∈[k]|Ci| ·∑\np∈Ci∥p−ci∥2\n2,\nwhere ciis the centroid of the points in the cluster Ciin an approximately optimal clustering\nC={C1,. . .,Ck}. We can use the learning-augmented k-means clustering algorithm to identify\ngood proxies for each centroid ci. Moreover, by our assumptions on the precision and recall of each\ncluster, we have that |Pi|is a good estimate of |Ci|. Therefore, we have a good approximation of the\ncost of the optimal min-sum k-clustering; it remains to identify the actual clusters.\nIn standard centroid-based clustering, each point is assigned to its closest center. However, this\nis not true for min-sum k-clustering. Thus, we seek alternative approaches to identifying a set of\napproximately |Pi|to each centroid returned by the learning-augmented k-means algorithm. To\nthat end, we define a constrained min-cost flow problem as follows. We create a source node sand\na sink node t, requiring n=|X|flow from stot. We then create a directed edge from sto each\nnode uxrepresenting a separate x∈Xwith capacity 1and cost 0. These two gadgets ensure that a\nunit of flow must be pushed across each node representing a point in the input dataset.\nWe also create a directed edge to tfrom each node virepresenting a separate ciwith capacity\n1\n1−α· |Pi|and cost 0. For each x∈X,i∈[k], create a directed edge from uxtoviwith capacity 1and\ncost1\n1−α· |Pi| · ∥x−ci∥2\n2. These two gadgets ensure that when a flow is pushed across some node to\nthe corresponding node representing a center, then the cost of the flow is almost precisely the cost\nof assigning a point to the corresponding center toward the min-sum k-clustering objective. Finally,\nwe require that at least (1−α)· |Pi|flow goes through node vicorrepsonding to center ci. This\n– 6 –\n\nensures that the correct number of points is assigned to each center consistent with the precision\nand recall assumptions.\nWe note that the constrained min-cost flow problem can be written as a linear program. There-\nfore to identify the overall clusters, we run any standard polynomial-time algorithm for solving\nlinear programs [ Kar84 ,Vai89 ,Vai90 ,LS15 ,LSZ19 ,CLS21 ,JSWZ21 ]. It then follows by that well-\nknown integrality theorems for min-cost flow, the resulting solution is integral and thus provides a\nvalid clustering with approximately optimal ℓ2\n2min-sum k-clustering objective.\n1.3 Related Works\nThe min-sum k-clustering problem was first introduced for general graphs by [ SG76 ]. The problem\nis complement of the max k-cut problem, in which the goal is to partition the vertices of an input\ngraph into ksubsets as to maximize the number or weight of the edges crossing any pair of subsets,\nc.f., [ PY91 ]. [GH98 ] showed that the ℓ2min-sum k-clustering problem is also closely related to the\nbalanced k-median problem, in which the goal is to identify kcenters c1,. . .,ckand partition the\ninput dataset Xinto clusters C1,. . .,Ckto minimize ∑k\ni=1|Ci|∑x∈X∥x−ci∥2.In particular, [ GH98 ]\nshowed that an α-approximation to balanced k-median yields a 2α-approximation to min-sum\nk-clustering. [ GH98 ] then showed that balanced k-median can be solved in time nO(k)by guessing\nthe cluster centers and sizes, and then subsequently determining the assignment between the\ninput points and the centers, which also results in a 2-approximation for min-sum k-clustering in\nnO(k)time. For the structurally different ℓ2min-sum k-clustering problem, [ BFSS19 ] achieved a\npolynomial-time algorithm that achieves the best known approximation of O(logn), by consider-\ning the embedding of metric spaces into hierarchically separated trees using dynamic programming.\nHowever, these techniques do not immediately translate into a good approximation for ℓ2\n2min-sum\nk-clustering. Even more recently, [ NRS24 ] provided a QPTAS in metrics induced by graphs of\nbounded treewidth, and graphs of bounded doubling dimension.\nFor the prize-collecting version of ℓ2min-sum k-clustering, [ HO10 ] gave a 2-approximation\nalgorithm in the metric setting that uses polynomial time for fixed constant k. In a separate line of\nwork, [ BB09 ,BBG09 ] address conditions under which the clustering would be stable. Namely for\nthe metric case and small k, they compute a clustering that is to the optimal ℓ2min-sum k-clustering\nin the sense that most of the labels are correct, though the objective value may not be close to the\noptimal value.\nOn the lower bound side, [ GH98 ] showed that the general min-sum k-clustering problem is\nNP-hard, while [ ADHP09 ] showed that even the ℓ2\n2min-sum k-clustering problem is NP-hard\neven when k=2. [KKLP97 ] first showed that it is NP-hard to approximate non-metric min-sum\nk-clustering within a multiplicative O\u0000\nn2−ε\u0001\n-factor for any ε>0and k>3. Recently, [ CKL21 ]\nshowed that for metric min-sum k-clustering, it is NP-hard to approximate within a multiplicative\n1.415 -factor. However, prior to this work, no such hardness-of-approximation was known for the\nℓ2\n2min-sum k-clustering problem.\nA popular way of obtaining polynomial time approximation schemes are coresets, which\nare succinct summaries of a data set with respect to a given clustering objective. For ℓ2\n2min-\nsum clustering, the most closesly related construction is the classic k-means problem, as well as\nvariants such as non-uniform k-clustering. Following a long line of work [ BBC+19,FSS20 ,HV20 ,\nCWZ23 ,WZZ23 ], ak-means coreset in Euclidean space of size ˜O(k\nε2·min\u0010\n1\nε2,√\nk\u0011\n)is known to\nexist [CSS21, CLS+22, BCP+24], which was surprisingly shown to be optimal [HLW24, ZTHH24].\n– 7 –\n\nFor non-uniform clustering, centers are associated with weights and the clustering cost is\n∑k\ni=1∑p∈Ciwci∥p−ci∥2, where ciis the center associated with the cluster Ciand wcidenotes its\nweight. Min-sum clustering is a related problem where the weight is not arbitrary, but chosen to\nbe equal to |Ci|. Unfortunately, the only known coreset constructions for the weighted k-means\nproblem [ FS12 ] only apply to the line metric and even in this case have size at least (logn)k.\nNevertheless, coreset based approaches have been successfully used to obtain fast algorithms\nwith additive errors in general metric spaces, see [ CS07 ]. It is unclear if these ideas can improve\nalgorithms for ℓ2\n2min-sum clustering, even when using additive errors.\n1.4 Preliminaries\nWe use the notation [n]to denote the set {1, 2,. . .,n}for an integer n>0. For a set X, we use the\nnotation X=A˙∪Bto denote that Aand Bpartition X, i.e., A∪B=Xand A∩B=∅. For a matrix\nA∈Rn×d, we define its Frobenius norm as\n∥A∥F:=vuutn\n∑\ni=1d\n∑\nj=1A2\ni,j.\nWe use poly(n)to denote a fixed polynomial in nwhose degree can be determined by setting\nappropriate constants in the algorithms or proofs. We use polylog (n)to denote poly(logn). For a\nfunction f(·, . . . ,·), we use the notation ˜O(f)to denote f·polylog (f).\nk-means clustering. In the Euclidean k-means clustering problem, the input is a dataset X⊂Rd\nand the goal is to partition Xinto clusters C1,. . .,Ckby assigning a centroid cito each cluster Cias\nto minimize the objective\nmin\nc1,...,ck∑\nx∈Xmin\ni∈[k]∥x−ci∥2\n2.\n2 Hardness of Approximation of ℓ2\n2Min-Sum k-Clustering\nIn this section, we show the hardness of approximation of ℓ2\n2min-sum k-clustering, i.e., Theorem 1.3.\nWe first define the relevant formulations of Johnson Coverage Hypothesis in Section 2.1. Next,\nin Section 2.2 we provide the main reduction from the Johnson coverage problem to the ℓ2\n2min-\nsum k-clustering problem. Finally, in Section 2.3 we prove a special case of a generalization of\nBalanced −JCH∗which yields the unconditional NP-hardness factor claimed in Theorem 1.3.\n2.1 Johnson Coverage Hypothesis\nIn this section, we recall the Johnson Coverage problem, followed by the Johnson Coverage\nhypothesis [CKL22].\nLetn,z,y∈Nsuch that n≥z>y. Let E⊆([n]\nz)and S∈([n]\ny). We define the coverage of S\nw.r.t. E, denoted by cov(S,E)as follows:\ncov(S,E) ={T∈E|S⊂T}.\n– 8 –\n\nDefinition 2.1 (Johnson Coverage Problem) .In the (α,z,y)-Johnson Coverage problem with z>y≥1,\nwe are given a universe U:= [n], a collection of subsets of U, denoted by E⊆([n]\nz), and a parameter kas\ninput. We would like to distinguish between the following two cases:\n•Completeness : There exists C:={S1, . . . , Sk} ⊆([n]\ny)such that\ncov(C):=∪\ni∈[k]cov(Si,E) =E.\n•Soundness : For every C:={S1, . . . , Sk} ⊆([n]\ny)we have |cov(C)|≤α·|E|.\nWe call (α,z,z−1)-Johnson Coverage as (α,z)-Johnson Coverage.\nNotice that (α, 2)-Johnson Coverage Problem is simply the well-studied vertex coverage prob-\nlem (with gap α). Also, notice that if instead of picking the collection Cfrom ([n]\ny), we replace it\nwith picking the collection Cfrom ([n]\n1)with a similar notion of coverage, then we simply obtain\nthe Hypergraph Vertex Coverage problem (which is equivalent to the Max k-Coverage problem\nfor unbounded z). In Figure 3 we provide a few examples of instances of the Johnson coverage\nproblem.\n(a) (b) (c)\nFig. 3: Examples of input instances of the Johnson Coverage Hypothesis for k=2. Figure 3a shows\nan example of a completeness instance of (0.7, 2, 1 ), since all subsets of size 2, i.e., all edges, can\nbe covered by k=2choices of subset of size 1, i.e., two vertices. Figure 3b shows an example of\na completeness instance of (0.7, 3, 1 ), since all subsets of size 3can be covered by k=2vertices.\nFigure 3c shows an example of a soundness instance of (0.7, 3, 2 ), since at most 2≤0.7·4subsets\nof size 3 can be covered by any choice of k=2 edges.\nWe now state the following hypothesis.\nHypothesis 2.2 (Johnson Coverage Hypothesis ( JCH) [CKL22 ]).For every constant ε>0, there exists\na constant z :=z(ε)∈Nsuch that deciding the\u0000\n1−1\ne+ε,z\u0001\n-Johnson Coverage Problem is NP-Hard.\nNote that since Vertex Coverage problem is a special case of the Johnson Coverage problem,\nwe have that the NP-Hardness of (α,z)-Johnson Coverage problem is already known for α=0.944\n[AKS11] (under unique games conjecture).\nOn the other hand, if we replace picking the collection Cfrom ([n]\nz−1)by picking from ([n]\n1), then\nfor the Hypergraph Vertex Coverage problem, we do know that for every ε>0there is some\nconstant zsuch that the Hypergraph Vertex Coverage problem is NP-Hard to decide for a factor of\u0000\n1−1\ne+ε\u0001\n[Fei98].\nFor continuous clustering objectives, a dense version of JCH is sometimes needed to prove\ninapproximability results (see [CKL22] for a discussion on this). Thus, we state:\n– 9 –\n\nHypothesis 2.3 (Dense Johnson Coverage Hypothesis ( JCH∗) [CKL22 ]).JCH holds for instances\n(U,E,k)of Johnson Coverage problem where |E|=ω(k).\nMore generally, let (α,z,y)-Johnson Coverage∗problem be the special case of the (α,z,y)-\nJohnson Coverage problem where the instances satisfy |E|=ω(k· |U|z−y−1). Then JCH∗states that\nfor any ε>0, there exists z=z(ε)such that (1−1/e+ε,z,z−1)-Johnson Coverage∗isNP-Hard.\nThis additional property has always been obtained in literature by looking at the hard instances\nthat were constructed. In [ CK19 ], where the authors proved the previous best inapproximability\nresults for continuous case k-means and k-median, it was observed that hard instances of (0.94, 2, 1 )-\nJohnson Coverage constructed in [AKS11] can be made to satisfy the above property.\nNow we are ready to define the variant of JCH needed for proving inapproximability of ℓ2\n2\nmin-sum k-clustering. For any two non-empty finite sets A,B, and a constant δ∈[0, 1], we say a\nfunction f:A→Bisδ-balanced if for all b∈Bwe have:\n|{a∈A:f(a) =b}|≤(1+δ)·|A|\n|B|.\nWe then put forth the following hypothesis.\nHypothesis 2.4 (Dense and Balanced Johnson Coverage Hypothesis ( Balanced −JCH∗)).JCH holds\nfor instances (U,E,k)of Johnson Coverage problem where |E|=ω(k)and in the completeness case there\nexistsC:={S1,. . .,Sk} ⊆([n]\nz−1)and a 0-balanced function ψ:E→[k]such that for all T∈Ewe have\nSψ(T)⊂T.\nMore generally, let (α,z,y,δ)-Balanced Johnson Coverage∗problem be the special case of the\n(α,z,y)-Johnson Coverage∗problem where the instances admit a δ-balanced function ψ:E→[k]\nin the completeness case which partitions Etokparts, say E1˙∪ · · · ˙∪Eksuch that for all i∈[k]we\nhave cov(Si,Ei) =Eiand|Ei| ≤|E|\nk·(1+δ). Then Balanced −JCH∗states that for any ε>0, there\nexists z=z(ε)such that (1−1/e+ε,z,z−1, 0)-Balanced Johnson Coverage∗isNP-Hard.\nAs with the case of JCH∗, the balanced addition to JCH∗is also quite natural and candidate\nconstructions typically give this property for free. To support this point, we will prove some special\ncase of this.\nIn [CKL22] the authors had proved the following special case of JCH∗.\nTheorem 2.5 ([CKL22 ]).For any ε>0, given a simple 3-hypergraph H= (V,H)with n=|V|, it is\nNP-hard to distinguish between the following two cases:\n•Completeness: There exists S ⊆V with |S|=n/2that intersects every hyperedge.\n•Soundness: Any subset S⊆Vwith|S| ≤n/2intersects at most a (7/8+ε)fraction of hyperedges.\nFurthermore, under randomized reductions, the above hardness holds when |H|=ω(n2).\nIn Section 2.3, we further analyze the proof of the above theorem and prove the following:\nTheorem 2.6. Theorem 2.5 holds even with the following additional completeness guarantee for all δ>0:\nthere exists S:={v1,. . .,vk} ⊆ Vand a δ-balanced function ψ:H→[k]such that for all e∈Hwe have\nvψ(e)∈e.\nThis result will be used to prove the unconditional NP-hardness of approximating ℓ2\n2min-sum\nk-clustering problem.\n– 10 –\n\n2.2 Inapproximability of ℓ2\n2min-sum k-clustering\nIn the following subsection, we will prove the below theorem.\nTheorem 2.7. Assume (α,z,y,δ)-Balanced Johnson Coverage∗isNP-Hard. For every constant ε>0,\ngiven a point-set P⊂Rdof size n(and d=O(logn)) and a parameter kas input, it is NP-Hard to\ndistinguish between the following two cases:\n•Completeness : There exists partition P∗\n1˙∪ · · · ˙∪P∗\nk:=P such that\n∑\ni∈[k]∑\np,q∈P∗\ni∥p−q∥2\n2≤(1+3δ)·(z−y)·ρn2/k,\n•Soundness : For every partition P 1˙∪ · · · ˙∪Pk:=P we have\n∑\ni∈[k]∑\np,q∈Pi∥p−q∥2\n2≥(1−o(1))·\u0010\nα·p\nz−y+ (1−α)·p\nz−y+1\u00112\n·ρn2/k,\nfor some constant ρ>0.\nPutting together the above theorem with Theorem 2.6 (i.e., NP-hardness of (7/8+ε, 3, 1, δ)-\nBalanced Johnson Coverage∗problem for all ε,δ>0), we obtain the NP-hardness of approximating\nℓ2\n2min-sum k-clustering. The above theorem also immediately yields the hardness of approximating\nℓ2\n2min-sum k-clustering under Balanced −JCH∗(i.e., conditional NP-hardness of (1−1/e+ε,z,z−\n1, 0)-Balanced Johnson Coverage∗problem for all ε>0and some z=z(ε)∈N). This completes\nthe proof of Theorem 1.3.\n2.2.1 Proof of Theorem 2.7\nFixε>0as in the theorem statement. Let ε′:=ε/11. Starting from a hard instance of (α,z,y,δ)-\nBalanced Johnson Coverage∗problem (U,E,k)with|U|=nand|E|=ω(nz−y),\nConstruction. Theℓ2\n2min-sum k-clustering instance consists of the set of points to be clustered\nP⊆ {0, 1}nwhere for every T∈Ewe have the point pt∈Pdefined as follows:\npT:=∑\ni∈T⃗ei\n. From the construction, it follows that for every distinct T,T′∈E, we have:\n∥pT−pT′∥2\n2=2z−2· |T∩T′|. (1)\nCompleteness. Suppose there exist S1,. . .,Sk∈([n]\ny)and a δ-balanced function ψ:E→[k]such\nthat for all T∈Ewe have Sψ(T)⊂T. Then, we define a clustering C1˙∪ · · · ˙∪Ck=Pas follows:\nfor every pT∈P, we include it in cluster Cψ(T). We now provide an upper bound on the ℓ2\n2\nmin-sum cost of clustering C:={C1,. . .,Ck}.(1)implies that for each Ci, for any pair T,T′such\n– 11 –\n\nthat pT,pT′∈Ci, we have that Sψ(T)⊆T∩T′and thus ∥pT−pT′∥2\n2≤2z−2y. Thus, the cost of\nclustering Cis bounded as follows:\n∑\ni∈[k]∑\np,q∈Ci∥p−q∥2\n2≤∑\ni∈[k]\u0000\u0000|Ci|2− |Ci|\u0001·2·(z−y)\u0001≤2|P| ·\u0012|P|\nk−1\u0013\n·(1+δ)2·(z−y).\nThe completeness analysis is now completed by noting that (1+δ)2≤1+3δ. Thus we turn to the\nsoundness analysis.\nSoundness. Consider the optimal ℓ2\n2min-sum k-clustering C:={C1,. . .,Ck}of the instance\n(i.e., C1˙∪ · · · ˙∪Ck=P). We aim at showing that the ℓ2\n2min-sum k-clustering cost of Cis at least\n((z−y) +2(1−α)−o(1))ℓ|P|. Given a cluster Ci, let Ei:={T∈E:pT∈Ci}be the collection of\nz-sets of Ecorresponding to Ci. For each S∈([n]\ny), we define the degree ofSinCito be\ndi,S:=|{T|S⊂Tand pT∈Ci}|.\nLett1=2z−2yand t2=2z−2y+2. For each cluster Ci, let\nFi=\f\f\f\f{(p,q)∈C2\ni:∥p−q∥2\n2≥t2}\f\f\f\f\nMi=\f\f\f\f{(p,q)∈C2\ni:∥p−q∥2\n2=t1}\f\f\f\f\nNi=\f\f\f\f{(p,q)∈C2\ni:∥p−q∥2\n2<t1}\f\f\f\f.\nBy(1),Fi,Mi, and Niare the number of (ordered) pairs within Ciwhose corresponding z-sets in\nthe Balanced Johnson Coverage∗instance intersect in <y,=y, and >yelements respectively. Let\n∆i=max\nS∈([n]\ny)di,Sand observe that ∆i≤ |Ci|. We write the total cost of the clustering as follows.\n∑\ni∈[k]∑\np,q∈Ci∥p−q∥2\n2≥∑\ni∈[k]\u0012\nFit2+Mit1\u0013\n=∑\ni∈[k]\u0012\n(|Ci|2−Mi)t2+Mit1−Nit2\u0013\n(2)\nWe first upper bound ∑i∈[k](Nit2). For each z-setT, there are at most\u0010\n∑z\nℓ=y+1(z\nℓ)(n−z\nz−ℓ)\u0011\nmany\nsets in ([n]\nz)that intersect with Tin at least y+1 elements. Therefore, we have:\n∑\ni∈[k]Ni≤ \n∑\ni∈[k]|Ci| · \nz\n∑\nℓ=y+1\u0012z\nℓ\u0013\u0012n−z\nz−ℓ\u0013!!\n≤∑\ni∈[k]|Ci| ·2z·(z−y)·nz−y−1\n=O\u0010\n|P| ·nz−y−1\u0011\n.\nBy the definition of Balanced Johnson Coverage∗,|P|=|E|=ω(k·nz−y−1), so∑i∈[k]Nit2is at\nmost o(|P|2/k).\nNext, we invoke a technical claim in [CKL22] which bounds Mi/|Ci|in terms of ∆iand|Ci|.\n– 12 –\n\nClaim 2.8 (Claim 3.18 in [ CKL22 ]).For every i∈[k], either |Ci|=o(|P|/k)orMi/|Ci| ≤(1+\no(1))∆i+o(|Ci|).\nWe can thus lower bound the cost of the clustering in (2) as follows:\n∑\ni∈[k]∑\np,q∈Ci∥p−q∥2\n2≥ \n∑\ni∈[k]|Ci|2(2z−2y+2)!\n− \n∑\ni∈[k]2∆i|Ci|!\n−o \n|P2|/k+∑\ni∈[k]|Ci|2!\n(3)\nThus, we now look at upper bounding ∑i∈[k]2∆i|Ci|. From the soundness case assumption, we\nhave that s:=∑i∈[k]∆i≤α· |E|. Without loss of generality, we may assume that |C1| ≥ | C2| ≥\n· · · ≥ | Ck|. Let t∈[k]be smallest integer such that ∑i∈[t]|Ci|>s. Since ∆i≤ |Ci|, then∑i∈[k]2∆i|Ci|\nis maximized when ∆i=|Ci|for all i∈[t]. Thus, ∑i∈[k]2∆i|Ci| ≤∑i∈[t]2|Ci|2, and we can rewrite\n(3) as follows:\n∑\ni∈[k]∑\np,q∈Ci∥p−q∥2\n2≥ \n∑\ni∈[t]|Ci|2(2z−2y)!\n+ \nk\n∑\ni=t+1|Ci|2(2z−2y+2)!\n−o \n|P2|/k+∑\ni∈[k]|Ci|2!\n(4)\nThen the quantity\u0010\n∑i∈[t]|Ci|2(2z−2y)\u0011\n+\u0010\n∑k\ni=t+1|Ci|2(2z−2y+2)\u0011\nis minimized when for all\ni∈[t], we have all |Ci|’s to be equal and for all i∈ {t+1,. . .,k}, we have all |Ci|’s to be equal (by\nconvexity). Thus,\n \n∑\ni∈[t]|Ci|2(z−y)!\n+ \nk\n∑\ni=t+1|Ci|2(z−y+1)!\n≥\u0012α2|P|2\nt(z−y)\u0013\n+\u0012(1−α)2|P|2\n(k−t)(z−y+1)\u0013\n.\nWe may rewrite the left side as follows:\n|P|2\nk\u0012α2·(z−y)\nt/k+(1−α)2·(z−y+1)\n1−(t/k)\u0013\nIf we look at the first derivative of the above expression w.r.t. t/k, then we have that the minima\nof the above expression is attained when:\n(1−α)2·(z−y+1)\n(1−(t/k))2=α2·(z−y)\n(t/k)2\nSimplifying, we obtain:\nt=k· \nα·√z−y\nα·√z−y+ (1−α)·p\nz−y+1!\nReturning to the cost of clustering, we have from (4):\n∑\ni∈[k]∑\np,q∈Ci∥p−q∥2\n2≥(1−o(1))·2|P2|\nk·\u0012α2·(z−y)\nt/k+(1−α)2·(z−y+1)\n1−(t/k)\u0013\n≥(1−o(1))·2|P2|\nk·\u0012α2·(z−y)\nt/k+\u0012α2·(z−y)\nt/k·1−(t/k)\nt/k\u0013\u0013\n– 13 –\n\n= (1−o(1))·2|P2|\nk·\u0012α2·(z−y)\n(t/k)2\u0013\n≥(1−o(1))·2|P2|\nk·\u0010\nα·p\nz−y+ (1−α)·p\nz−y+1\u00112\nDimensionality reduction. The proof of the theorem with the reduced dimension (i.e., d=\nO(logn)) of the hard instances follows from the Johnson-Lindenstrauss lemma. Elaborating,\ngiven a set of npoints in Rd, we have that the ℓ2\n2min-sum k-clustering cost of a given partition\n{C1,. . .,Ck}expressed as ∑k\ni=1∑p,q∈Ci∥p−q∥2\n2. Thus, applying the Johnson-Lindenstrauss lemma\nwith target dimension O\u0000\nlogn/ε2\u0001\nfor small enough ε, yields an instance where the ℓ2\n2min-sum\nk-clustering cost of any clustering Cis within a factor (1+ε)of the ℓ2\n2min-sum k-clustering cost of\nCin the original d-dimensional instance. It follows that the gap is preserved up to a (1+ε)factor\nand the theorem follows. Note that this can be made deterministic (for example, see the result of\nEngebretsen et al. [EIO02]).\n2.3 Proof of Theorem 2.6\nTheorem 2.6 follows from observing additional properties of the reduction of [ CKL22 ] from the\nmultilayered PCPs of [ DGKR05 ,Kho02 ] to3-Hypergraph Vertex Coverage. The description of the\nreduction is taken verbatim from [CKL22]. We first describe the multilayered PCPs that we use.\nDefinition 2.9. Anℓ-layered PCP Mconsists of\n•Anℓ-partite graph G = (V,E)where V =∪ℓ\ni=1Vi. Let E i,j=E∩(Vi×Vj).\n•Sets of alphabets Σ1, . . . ,Σℓ.\n•For each edge e = (vi,vj)∈Ei,j, a surjective projection πe:Σj→Σi.\nGiven an assignment (σi:Vi→Σi)i∈[ℓ], an edge e= (vi,vj)∈Ei,jissatisfied ifπe(σj(vj)) = σi(vi).\nThere are additional properties that Mcan satisfy.\n•η-smoothness: For any i <j, vj∈V, and x ,y∈Σj,Pr(vi,vj)∈Ei,j[π(vi,vj)(x) =π(vi,vj)(y)]≤η.\n•Path-regularity: Call a sequence p= (v1,. . .,vℓ)fullpath if(vi,vi+1)∈Ei,i+1for every 1≤i< ℓ,\nand let Pbe the distribution of full paths obtained by (1) sampling a random vertex v1∈V1and (2)\nfori=2,. . .,ℓ, sampling vifrom the neighbors of vi−1inEi−1,i.Mis called path-regular if for any\ni<j, sampling p= (v1,. . .,vℓ)fromPand taking (vi,vj)is the same as sampling uniformly at\nrandom from E i,j.\nTheorem 2.10. [DGKR05 ,Kho02 ] For any τ,η>0andℓ∈N, given an ℓ-layered PCP Mwith\nη-smoothness and path-regularity, it is NP-hard to distinguish between the following cases.\n•Completeness: There exists an assignment that satisfies every edge e ∈E.\n•Soundness: For any i <j, no assignment can satisfy more than an τfraction of edges in E i,j.\n– 14 –\n\nGiven an ℓ-layered PCP Mdescribed above, in [ CKL22 ] they design the reduction to the\nJohnson Coverage problem as follows. First, the produced instance will be vertex-weighted and\nedge-weighted, so that the problem becomes “choose a set of vertices of total weight at most k\nto maximize the total weight of covered edges.” We will explain how to obtain an unweighted\ninstance at the end of this section.\n•LetCi:={±1}|Σi|and Ui:=Vi×Ci. The resulting hypergraph will be denoted by H=\n(U,H)where U=∪ℓ\ni=1(Vi×Ci). The weight of vertex (v,x)∈Vi×Ciis\nw(v,x):=1\nℓ·1\n|Vi|·1\n|Ci|.\nNote that the sum of all vertex weights is 1.\n•LetDIbe the distribution where i∈[ℓ]is sampled with probability26(ℓ−i)2/(ℓ(ℓ−1)(2ℓ−\n1)), andDbe the distribution over (i,j)∈[ℓ]2where iis sampled from DIand jis sampled\nuniformly from {i+1,. . .,ℓ}. For each i<j, we create a set of hyperedges Hi,jthat have\none vertex in Uiand two vertices in Uj. Fix each e= (vi,vj)∈Ei,jand a set of three vertices\nt⊆({vi} ×Ci)∪({vj} ×Cj). The weight w(t)is(the probability that (i,j)is sampled from\nD)·(1/|Ei,j|)·(the probability that tis sampled from the following procedure ). The reduction\nis parameterized by δ>0 determined later.\n◦For each a∈Σi, sample xa∈ {± 1}.\n◦For each b∈Σj,\n♠Sample yb∈ {± 1}.\n♠Ifxπ(b)=−1, let zb=ybwith probability 1 −δand zb=−ybotherwise.\n♠Ifxπ(b)=1, let zb=−yb.\n◦Output {(vi,x),(vj,y),(vj,z)}.\nNote that the sum of all hyperedge weights is also 1.\nSoundness. The soundness of the reduction is proved in [CKL22].\nLemma 2.11 ([CKL22 ]).Any subset of weight at most 1/2intersects hyperedges of total weight at most\n7/8+o(1).\n(Almost) regularity. We prove the (almost) regularity of the reduction; for every vertex, the ratio\nbetween the weight of the vertex and the total weight of the hyperedges containing it is (3±o(1)).\nNote that 3is natural as both total vertex weights and total edge weights are normalized to 1and\neach hyperedge contains three vertices.\nFix a vertex (v,x)where v∈Vifor some i∈[ℓ]. Its vertex weight w(v,x) =1\nℓ·1\n|Vi|·1\n|Ci|. We\nnow consider the edge weight (described as a sampling procedure) and compute the probability\nthat a random hyperedge contains (v,x). There are two possibilities.\n2[CKL22] states (ℓ−i)2/(6ℓ(ℓ−1)(2ℓ−1)), which is a typo corrected in their analysis.\n– 15 –\n\n•The hyperedge is from the jth layer and ith layer for some j<i. For fixed j<i, the probability\nof the pair (j,i)is\n6(ℓ−j)2\nℓ(ℓ−1)(2ℓ−1)·1\nℓ−j=6(ℓ−j)\nℓ(ℓ−1)(2ℓ−1),\nand given (j,i), the probability that vis contained in the sampled hyperedge is2±o(1)\n|Vi||Ci|. (Note\nthat the distribution of either (vj,y)or(vj,z)in the procedure is the uniform distribution on\nVi×Ci. The factor 2comes from the fact that the hyperedge samples two points from the ith\nlayer; the probability that the same point is sampled twice is exponentially small and can be\nabsorbed in the o(1)term.)\n•The hyperedge is from the ith layer and jth layer for some i<j. For fixed i<j, the probability\nof the pair (i,j)is\n6(ℓ−i)2\nℓ(ℓ−1)(2ℓ−1)·1\nℓ−i=6(ℓ−i)\nℓ(ℓ−1)(2ℓ−1),\nSumming the above events for all jvalues, we get\n(1±o(1))\u0012\u0000i−1\n∑\nj=16(ℓ−j)\nℓ(ℓ−1)(2ℓ−1)2\n|Vi||Ci|\u0001+\u0000ℓ\n∑\nj=i+16(ℓ−i)\nℓ(ℓ−1)(2ℓ−1)1\n|Vi||Ci|\u0001\u0013\n=6±o(1)\nℓ(ℓ−1)(2ℓ−1|Vi||Ci|\u0012\u0000i−1\n∑\nj=12(ℓ−j)\u0001+\u0000ℓ\n∑\nj=i+1(ℓ−i)\u0001\u0013\n=6±o(1)\nℓ(ℓ−1)(2ℓ−1)|Vi||Ci|\u0012\u0000i−1\n∑\nj=12(ℓ−j)\u0001+\u0000ℓ−i\u00012\u0013\n=6±o(1)\nℓ(ℓ−1)(2ℓ−1)|Vi||Ci|\u0012\n2ℓ(i−1)−i(i−1) +ℓ2−2ℓi+i2\u0013\n=6±o(1)\nℓ(ℓ−1)(2ℓ−1)|Vi||Ci|\u0012\nℓ2−2ℓ+i\u0013\n=3± O(1/ℓ)±o(1)\nℓ|Vi||Ci|.\nBy increasing ℓto be an arbitrarily large constant, we established that the total weight of the\nhyperedges containing (v,x)is(3±o(1))times its vertex weight1\nℓ|Vi||Ci|.\nCompleteness. IfMadmits an assignment (σi:Vi→Σi)i∈[ℓ]that satisfies every edge e∈E, let\nS:={(vi,x):vi∈Vi,xσi(vi)=−1}. Fix any e= (vi,vj)∈Ei,jand consider the above sampling\nprocedure to sample x∈ {± 1}Σiand y∈ {± 1}Σjwhen b=σj(vj). Since πe(σj(vj)) = σi(vi), at\nleast one of xσi(vi),yσj(vj),zσj(vj)must be −1always. So, Sintersects every hyperedge with nonzero\nweight.\nFurthermore, an inspection of the sampling procedure reveals that for a fixed vertex (vi,x)\nand j>i, a1/2± O(δ)fraction of the hyperedges containing it has all three vertices in Sand a\n1/2± O(δ)fraction of the hyperedges containing it has only (vi,x)inS. Therefore, there must\nbe an assignment from all the hyperedges to Ssuch that (1) a hyperedge is assigned to a vertex\ncontained by it, and (2) every vertex is assigned a 1/2+1/(2·3)± O(δ)=2/3± O(δ)fraction of\nthe hyperedges containing it (which is consistent with the fact that Scontains half of the vertices).\n– 16 –\n\nTherefore, each vertex has almost the same ratio (up to 1±o(1)by taking δarbitrarily small)\nbetween its weight and the total weight of the hyperedges assigned to it. In order to obtain an\nunweighted instance, for each vertex (v,x), we create a new cloud of vertices Cv,xwhose cardinality\nis proportional to w(v,x), and replace each edge ((v1,x1),(v2,x2),(v3,x3))by all possible edges\nbetween Cv1,x1,Cv2,x2,Cv3,x3(with the total weight equal to the weight of the original edge).\n3 PTAS based on D2Sampling\nFor a set A⊂Rd, letµ(A):=1\n|A|∑p∈Apdenote its mean. Let C={C1,. . .Ck}be an optimal\nk-MinSum clustering of a point set A. We use µi=µ(Ci)to denote the mean of Ciand we use\n∆i=∑p∈Ci∥p−µi∥2\n|Ci|to denote the average mean squared distance of Citoµi. We further use Cβ\nito\ndenote the subset of Ciwith∥p−µi∥2≤β·∆i. Finally, let OPT denote the cost of an optimal\nsolution. So, OPT =∑k\ni=1|Ci|2·∆i.\nDefinition 3.1. We say that mis an ε-approximate mean of Ciif∥m−µi∥2≤ε·∆i. We say that a set\nS⊂Ais an(ε,β)-mean seeding set for Ci∈ C, if there exists a subset S′∪ {s} ⊂ Swith∥s−µi∥2≤β·∆i\nand a weight assignment w :S′→R≥0such that\n\r\r\r\r\r1\n∑p∈S′w(p)∑\np∈S′w(p)·p−µi\r\r\r\r\r2\n≤ε·∆i.\nWe will use the following well-known identities for Euclidean means.\nLemma 3.2. [IKI94] Let A ⊂Rdbe a set of points. Then for any c ∈Rd:\n•∑p∈A∥p−c∥2=∑p∈A∥p−µ(A)∥2+|A| · ∥µ(A)−c∥2.\n•∑p,q∈A∥p−q∥2=2· |A| ·∑p∈A∥p−µ(A)∥2.\nWe note that as an immediate corollary, the lemma implies that the sum of squared distances of\nall points in a cluster Cito an approximate mean is at most (1+ε)|Ci|∆iand the MinSum clustering\ncost is at most (1+ε)|Ci|2∆i.\nCorollary 3.3. For any set of points A⊂Rd. Then c∈Rdis anε-approximate mean of Aif and only if\n∑p∈A∥p−c∥2≤(1+ε)· |Ci| ·∆i.\nLemma 3.4. [BBC+19] Given numbers a ,b,c, we have for all ε>0\n(a−b)2≤(1+ε)·(a−c)2+\u0012\n1+1\nε\u0013\n(b−c)2.\nWe also show that we only have to consider seeding sets with β∈Θ(ε−2).\nLemma 3.5. For any cluster Ci,ε∈(0, 1)andβ≥12ε−2, we have that µi(Cβ\ni) =1\n|Cβ\ni|∑p∈Cβ\nipis a\nε-approximate mean of C i.\n– 17 –\n\nProof. By Markov’s inequality, |Ci\\Cβ\ni| ≤β−1· |Ci| ≤ε2\n12· |Ci|. Since1\n|Cβ\ni|∑p∈Cβ\ni∥p−µi(Cβ\ni)∥2≤\n1\n|Cβ\ni|∑p∈Cβ\ni∥p−µi∥2≤∆i·|Ci|\n|Cβ\ni|≤2∆i, Lemma 3.2 implies ∥µi(Cβ\ni)−µi∥2=1\n|Cβ\ni|∑p∈Cβ\ni∥p−µi∥2−\n1\n|Cβ\ni|∑p∈Cβ\ni∥p−µi(Cβ\ni)∥2≤2∆i. We then have due to Lemma 3.4\n∑\nq∈Ci\\Cβ\ni∥q−µi(Cβ\ni)∥2− ∥q−µi∥2≤ε\n2∑\nq∈Ci\\Cβ\ni∥p−µi∥2+\u0012\n1+2\nε\u0013\n· ∥µi−µi(Cβ\ni)∥2\n≤ε\n2∑\nq∈Ci\\Cβ\ni∥p−µi∥2+ε2\n12|Ci| ·3\nε·2∆i≤ε|Ci|∆i\nThe cost of the points in Cβ\nitoµi(Cβ\ni)only gets smaller compared to the cost of these points to µi.\nHence, the increase in cost is bounded by ε|Ci|∆i, which with Corollary 3.3 yields the claim.\nFinally, we also show how to efficiently extract a mean from a mean seeding set, while being\noblivious to ∆i.\nLemma 3.6. LetSbe an (ε/4,β)-mean seeding set of a cluster Cjwith mean µj. Then we can compute\u0010\n10β·|S|\nε+1\u0011|S|\nchoices of weights in time linear in the size of choices such that at least one of the computed\nchoices satisfies\r\r\r\r\r1\n∑p∈Sw(p)∑\np∈Sw(p)·p−µj\r\r\r\r\r2\n≤ε·∆j.\nProof. We first introduce some preprocessing. By an affine transformation of the space, subtract\nq=argmin\np∈S∥p−µj∥from all points. Now all points pinSwith∥p−µj∥ ≤pβ·∆jhave norm at\nmost 2pβ·∆j.\nLetS′⊂Sbe the set with weights wsuch that\n\r\r\r\r\r1\n∑p∈S′w(p)∑\np∈S′w(p)p−µj\r\r\r\r\r2\n≤ε\n4·∆j.\nLetwmaxbe the maximum weight of the points in S′. For w(p)every p, we set w′(p)to be\nthe largest multiple ofε\n10β·|S|·wmaxthat is at most w(p)(where we extend wto all of Sby setting\nw(p) = 0for all p̸∈S′). So, w′(p) =ε\n10β·|S|·i·wmax≤w(p)<ε\n10β·|S|·(i+1)·wmaxfor some\ni∈n\n0, 1, . . .10β·|S|\nεo\n. Observe that there are at most\u0010\n10β·|S|\nε+1\u0011|S|\nchoices of weights of points in\nS. Furthermore, we have\n\f\f\f\f\f∑\np∈S\u0000\nw′(p)−w(p)\u0001\f\f\f\f\f≤ε\n10β· |S|∑\np∈S′w(p).\nWe now argue that µ′=1\n∑p∈S′w′(p)∑p∈S′w′(p)is aε-approximate mean of Cj. We have\n\r\r\r\r\r1\n∑p∈S′w(p)∑\np∈S′w(p)p−1\n∑p∈S′w′(p)∑\np∈S′w′(p)p\r\r\r\r\r\n– 18 –\n\n=1\n∑p∈S′w′(p)\r\r\r\r\r∑p∈S′w′(p)\n∑p∈S′w(p)∑\np∈S′w(p)p−∑\np∈S′w′(p)p\r\r\r\r\r\n=1\n∑p∈S′w′(p)\r\r\r\r\r∑p∈S′w′(p)−∑p∈S′w(p)\n∑p∈S′w(p)∑\np∈S′w(p)p\r\r\r\r\r+1\n∑p∈S′w(p)\r\r\r\r\r∑\np∈S′(w(p)−w′(p))p\r\r\r\r\r\n≤2ε\n10β· |S|\r\r\r\r\r1\n∑p∈S′w(p)∑\np∈′Sw(p)p\r\r\r\r\r+∑\np∈S′ε\n10β· |S|∥p∥\n≤5ε\n10β· |S|· |S′| ·q\nβ·∆j≤ε\n2q\n∆j\nBy the triangle inequality, we can therefore conclude that µ′is aε-approximate mean of µj\nComputing a Mean-Seeding Set via Uniform Sampling.\nLemma 3.7. Letε∈(0, 1)andβ>48ε2. With probability at least 1−δ, a set of 32kε−1logδ−1points S\nsampled uniformly at random with replacement from Acontains is a (ε,β)-mean seeding set of any Ciwith\n|Ci| ≥n\nk.\nProof. Due to Lemma 3.5, The mean of Cβ\niis anε\n2-approximate mean. Hence, if we obtain a (ε/2,β)-\nseeding set of Cβ\ni, the claim follows. By Markov’s inequality, Cβ\nicontains at leastn\n2kpoints. For any\np∈Cβ\ni, we have Eh\n∥p−µ(Cβ\ni)∥2i\n=∆β\ni:=1\n|Cβ\ni|∑p∈Cβ\ni∥p−µ(Cβ\ni∥2≤∆iand therefore for any set\nofmpoints Sisampled independently with replacement from Cβ\ni,Eh\n∥1\nm∑p∈Sip−µ(Cβ\ni)∥2i\n=1\nm∆β\ni.\nTherefore, if m≥4ε−1,Siis an (ε/2,β)-mean seeding set of Ciwith probability at least1\n2. Hence,\nsampling logδ−1many copies of Siimplies that at least one of them is an (ε/2,β)-mean seeding set\nofCiwith probability 1 −2−logδ−1=1−δ.\nA sample from the point set is contained in Cβ\niwith probability at least1\n2k. Hence, sampling at\nleast 16k·ε−1·logδ−1implies that with probability at least 1−δ, the number Xof points sampled\nfrom Siis at least 4ϵ−1logδ−1, as follows. By the above analysis E[X]≥8ε−1logδ−1. Therefore,\nby standard Chernoff bounds, Pr [X<4ϵ−1logδ−1]<e−1\n8·8ε−1logδ−1≤δ.\nD2Subsampling We now define an algorithm for sampling points that induce means from the\ntarget clusters. The high level idea is as follows. We construct a rooted tree in which every node is\nlabeled by a set of candidate cluster means. For a parent and child pair of nodes, the parent’s set is\na subset of the child’s set. The construction is iterative. Given an interior node, we construct its\nchildren by adding a candidate mean to the parent’s set. The candidantes are generated using points\nsampled at random from a distribution that will be defined later. The goal is to have, eventually,\nanε-approximate mean for every optimal cluster. This will be achieved with high probability at\none of the leaves of the tree. The root of the tree is labeled with the empty set, and its children are\nconstructed via uniform sampling. Subsequently, we refine the sampling distribution to account\nfor various costs and densities of the clusters.\nWe now go into more detail for the various sampling stages of the algorithm.\nPreprocessing: We ensure that all points are not too far from each other.\n– 19 –\n\nInitialization: We initialize the set of means via uniform sampling. Due to Lemma 3.7, we can\nenumerate over potential sets of ε-approximate means for all clusters of sizen\nk. Each candidate\nmean defines a child of the root.\nSampling Stage: Consider a node of the tree labeled with a non-empty set of candidate means\nM. We put Γi=2−i·∑q∈Amin m∈M∥q−m∥2fori∈ {0, 1, . . ., 13log(nk/ε)}, where ηis an\nabsolute constant to be defined later. Let Ai,M={q∈A: min m∈M∥q−m∥2≤Γi}. (Note\nthat A0,Mincludes all the points.) Let Pidenote the probability distribution on Ai,Minduced\nby setting, for each p∈Ai,M,\nPi[p] =min m∈M∥p−m∥2\n∑p∈Ai,Mmin m∈M∥p−m∥2\nWe’ll use Pto denote P0. For each i, we sample a sufficient (polynomial in kandε, but\nindependent of n) number of points independently from the distribution Pi. Let Sdenote the\nset of sampled points.\nMean Extraction Stage: We enumerate over combinations of points in M∪S, using some non-\nuniform weighing to fix a mean to add to M, see Lemma 3.6. Each choice of mean is added to\nMto create a child of the node labeled M.\nThroughout this section we will use the following definition. Given a set of centers M, we say\nthat a cluster Ciisε-covered by Mif|Ci|2·min m∈M∥µi−m∥2≤ε\n2·\u00001\nk·OPT +|Ci|2∆i\u0001\n. Our goal\nwill be to prove the following lemma.\nLemma 3.8. LetC={C1,. . .Ck}be the clusters of an optimal Min-Sum k-clustering and let ηbe an\nabsolute constant. For every δ,ϵ>0, there is a randomized algorithm that outputs a collection of at most\nno(1)·2η·k2·ε−12log2(k/(εδ))sets of at most kcenters M, such that with probability 1−δat least one of them\nthatε-covers every C i∈ C. The algorithm runs in time n1+o(1)·d·2η·k2·ε−12log2(k/(εδ)).\nNote that if all clusters of Careε-covered, then there exists an assignment of points to centers,\nsuch that Min-Sum clustering cost of the resulting clustering is at most (1+ε)·OPT . To see this,\nnotice that if we use Cas the clustering with mi=argminm∈M∥µi−m∥2, then\nk\n∑\ni=1|Ci|∑\np∈∥p−mi∥2≤OPT+k\n∑\ni=1|Ci|∑\np∈ε\n2\u00121\nk·OPT\n|Ci|2+1\n2∆i\u0013\n≤(1+ε)·OPT .\nPreprocessing The first lemma allows us to assume that all points are in some sense close to each\nother.\nLemma 3.9. Suppose n>20. Given an set of npoints A⊂Rd, we can partition a point set into subsets\nA1,. . .Ak, such that ∥p−q∥2≤n10·OPT for any two points p,q∈Aiand such that any cluster Cjis\nfully contained in one of the A i. The partitioning takes time ˜O(nd+k2).\nProof. The proof uses similar arguments found throughout k-means and k-median research, with\nonly difference being that some of the discretization arguments are slightly finer to account for the\nMinSum clustering objective.\n– 20 –\n\nConsider a candidate 20-approximate k-means clustering with cost T, which can be computed\nin time ˜O(nd+k2)[DSS24 ]. Then we have1\n20T·OPT≤20n2·T. Now, suppose that there are two\ncenters c1and c2such that ∥c1−c2∥2≤20n7·T. Then for any point p∈C1and q∈C2, we have by\nthe triangle inequality ∥p−q∥2≤20n9·T≤n10·T. Conversely, if ∥c1−c2∥2>n8·T, we know\nthat no two points in the clusters induced by C1and C2can be in the same cluster of the optimal\nMinSum clustering.\nComputing a Mean-Seeding Set via D2Sampling. We now consider a slight modification of\nLemma 3.7 to account for sampling points from a cluster non-uniformly. We introduce the notion\nof a distorted core as follows. Given a cluster Cj, a set of centers M, and parameters α,β, we say\nthat a subset of Cβ\nj∪Mis a(Cj,β,α,M)-distorted core (denoted core(Cj,β,α,M)) iff it is the image\nof a mapping πα,M:Cβ\nj→Cβ\nj∪Msuch that for any point p∈Cβ\nj, we have\nπα,M(p) =\n\np if min m∈M∥p−m∥2≥α·∆j\nargmin\nm∈M∥p−m∥2if min m∈M∥p−m∥2<α·∆j.\nWe use D(Cj,β,α,M)to denote the set of points in Cβ\njsuch that min m∈M∥p−m∥2<α·∆j.\nThe following lemmas relate the goodness of a mean computed on an α-distorted core to the\nmean on the entire set of points when sampling points proportionate to squared distances. We start\nby proving an analogue of Lemma 3.5.\nLemma 3.10. Letα≤ε\n4and let β≥144\nε2. Given a set of centers M and a cluster C j, let\nˆµj=1\n|Cβ\nj|∑\np∈Cβ\njπα,M(p).\nThen,\n∥ˆµj−µj∥2≤ε·∆j.\nProof. First, let µ′\njbe the mean of Cβ\nj. Due to Markov’s inequality |Cβ\nj| ≥|Cj|\n2. Using Lemma 3.2, we\nhave|Cj| ·∆j≥∑p∈Cβ\nj∥p−µj∥2≥ |Cβ\nj| · ∥µ′\nj−µj∥2, which implies that ∥µ′\nj−µj∥2· |Cj| ≤2|Cj| ·∆j.\nThen\n∑\np∈Cj∥p−µ′\nj∥2=∑\np∈Cβ\nj∥p−µ′\nj∥2+∑\np∈Cj\\Cβ\nj∥p−µ′\nj∥2\n≤∑\np∈Cβ\nj∥p−µj∥2+\n+∑\np∈Cj\\Cβ\nj\u0010\n1+ε\n8\u0011\n· ∥p−µj∥2+|Cj\\Cβ\nj| ·\u0012\n1+8\nε\u0013\n· ∥µ′\nj−µj∥2\n≤\u0010\n1+ε\n8\u0011\n·∑\np∈Cj∥p−µj∥2+9\nεβ· |Cj| · ∥µ′\nj−µj∥2\n– 21 –\n\n≤\u0010\n1+ε\n8\u0011\n·∑\np∈Cj∥p−µj∥2+18\nεβ· |Cj|∆j,\nwhere we used Lemma 3.4 in the second inequality. In other words, µ′\njis an\u0010\nε\n8+18\nεβ\u0011\n-approximate\nmean of Cj. We now turn our attention to ˆµj. We have\n∥ˆµj−µj∥ ≤1\n|Cβ\nj|·∑\np∈Cα\nj∥p−πα,M(p)∥ ≤q\nα·∆j\nBy the triangle inequality, we therefore have\n∥ˆµj−µj∥ ≤ ∥ ˆµj−µ′\nj∥+∥µ′\nj−µj∥ ≤q\nα·∆j+s\u0012ε\n8+18\nεβ\u0013\n·∆j.\nBy our choice of αandβ, this implies that ˆµjis an ε-approximate mean of Cj.\nWe now characterize when Meither covers a cluster Cj, or when Mis a suitable seeding set for\nCj. The following lemma says that if Mis not a seeding set of Cj, then there exist many points in\nthe core Cβ\njofCjthat are far from M.\nLemma 3.11. Given α≤ε\n16,β≥2400\nε2, and γ≤q\nε\n16(β+α), and a set of centers M, letCjbe a cluster for\nwhich|D(Cj,β,α,M)| ≥(1−γ)· |Cβ\nj|. Then M is an (ε,β)-mean seeding set of C j.\nProof. First, let ˆµj=1\n|Cβ\nj|∑p∈Cβ\njπα,M(p)and let µ′\nj=1\n|D(Cj,β,α,M)|∑p∈D(Cj,β,α,M)pbe the mean of\nD(Cj,β,α,M). Now, observe that for any pairs of points p∈Cα\njand q∈Cβ\nj, by the triangle\ninequality\n∥q−p∥ ≤ ∥ q−µj∥+∥µj−p∥ ≤q\n(β+α)·∆j.\nThen\n∥ˆµj−µ′\nj∥\n=1\n|Cβ\nj|\r\r\r\r\r\r\r∑\np∈Cβ\njπα,M(p)−|Cβ\nj|\n|D(Cj,β,α,M)|∑\np∈D(Cj,β,α,M)p\r\r\r\r\r\r\r\n=1\n|Cβ\nj|·\r\r\r\r\r\r\n∑\np∈D(Cj,β,α,M)(πα,M(p)−p)\n+\n+\n∑\np∈Cβ\nj\\D(Cj,β,α,M)πα,M(p)−|Cβ\nj\\D(Cj,β,α,M)|\n|D(Cj,β,α,M)|∑\np∈D(Cj,β,α,M)p\n\r\r\r\r\r\r\r\n≤1\n|Cβ\nj|·\r\r\r\r\r\r∑\np∈D(Cj,β,α,M)(πα,M(p)−p)\r\r\r\r\r\r+\n– 22 –\n\n+1\n|Cβ\nj|·\r\r\r\r\r\r\r∑\np∈Cβ\nj\\D(Cj,β,α,M)πα,M(p)−|Cβ\nj\\D(Cj,β,α,M)|\n|D(Cj,β,α,M)|∑\np∈D(Cj,β,α,M)p\r\r\r\r\r\r\r\n≤q\nα·∆j+γ·q\n(β+α)·∆j\nFinally, by the triangle inequality, Lemma 3.10 and our choice of α,β, and γ, we have\n∥µ′\nj−µj∥ ≤ ∥ µ′\nj−ˆµj∥+∥ˆµj−µj∥ ≤q\nα·∆j+γ·q\n(β+α)·∆j+r\nε\n4∆j≤q\nϵ∆j,\nthus completing the proof.\nAs a consequence of this lemma and the preprocessing, we show under the assumption of\nLemma 3.9, the largest value of isuch that Cβ\nj∈Ai,Mfor an uncovered cluster Cjcannot be too\nlarge.\nLemma 3.12. Given β≥2400\nε2, suppose we have a set of points Asuch that ∥p−q∥2≤n10·OPT\nas per Lemma 3.9. Let Mbe a set of points and suppose there exists a cluster Cjsuch that such Cjis\nuncovered and such that Mis not an (ε/4,β)mean seeding set of A. We then have that Cβ\nj⊂Ai,Mimplies\ni≤13 log (nk/ε).\nProof. Suppose i>13log(nk/ε). Due to Lemma 3.11, we know there exists a point p′∈Cβ\njsuch that\nmin m∈M∥p−m∥2≥ε/16·∆j. This implies via Lemma 3.9 that ∆j≤\u0010\nk·n\nε\u0011−13\n·16ε−1·n10·OPT .\nConsider the point p∈Cβ\njwith minimumal distance to µjand let mp=argminm∈M∥p−m∥2.\nThen∥p−m∥2≤n−20·OPT , which implies that\n|Cj| ·∑\nq∈Cj∥q−m∥2≤ |Cj| ·∑\nq∈Cj2· ∥q−p∥2+2· ∥p−m∥2\n≤4|Cj| ·∑\nq∈Cj∥q−µj∥2+2|Cj|2· ∥p−m∥2\n≤4|Cj|2·16ε−1·\u0012k·n\nε\u0013−13\n·n10·OPT+2|Cj|2\u0012k·n\nε\u0013−13\n·n10·OPT\n≤66· |Cj|2·\u0012k·n\nε\u0013−13\n·16ε−1·n10·OPT≤ε\n2k·OPT ,\nwhich is a contradiction to Mnot covering Cj.\nWe now show that, given that Mis not a seeding set of some cluster Cj, that the weighted\nsquared distance of µjto its closest point in Mis a reasonably accurate proxy for the squared\ndistance of the points in the core Cβ\njto their respectively closest points in M.\nLemma 3.13. LetMbe a set of centers and let Cjbe a cluster that is not ε-covered by M. Also assume that\nM is not an (ε,β)-mean seeding set of C j. Then,\n∑\np∈Cβ\njmin\nm∈M∥p−m∥2≥1\n272\u0012ε\nβ\u00133/2\n· |Cj| ·min\nm∈M∥µj−m∥2\n– 23 –\n\nProof. For all p∈Cβ\nj\\D(Cj,β,ε/16, M), we have:\nmin\nm∈M∥µj−m∥2≤\u0012\nmin\nm∈M∥p−m∥+∥µj−p∥\u00132\n≤2 min\nm∈M∥p−m∥2+2∥µj−p∥2\n≤34β\nε·min\nm∈M∥p−m∥2,\nwhere the first inequality uses the triangle inequality and that for m′=arg min m∈M∥p−m∥, we\nhave that ∥µj−m′∥2≥min m∈M∥µj−m∥2, and the last inequality uses min m∈M∥p−m∥2≥ε\n16∆j\nand∥µj−p∥2≤β∆jandβ\nε≥1.\nWe first consider the case that min m∈M∥µj−m∥ ≥ 2pβ·∆j. In this case, all points in Cβ\njare\ncloser to µjthan to any point in M. This implies\n∑\np∈Cβ\njmin\nm∈M∥p−m∥2≥1\n4|Cβ\nj|min\nm∈M∥µj−m∥2≥1\n8|Cj|min\nm∈M∥µj−m∥2.\nNow, we consider the case that min m∈M∥µj−m∥ ≤ 2pβ·∆j. As Mis not an ε-mean seeding set\nforCj, Lemma 3.11 implies that |Cβ\nj\\D(Cj,β,ε/16, M)|>q\nε\n16β+ε|Cβ\nj| ≥1\n2q\nε\n16β+ε|Cj|. Therefore,\n∑\np∈Cβ\njmin\nm∈M∥p−m∥2≥ ∑\np∈Cβ\nj\\D(Cj,β,ε/16, M)min\nm∈M∥p−m∥2\n≥ | Cβ\nj\\D(Cj,β,ε/16, M)| ·ε\n34β·min\nm∈M∥µj−m∥2\n≥1\n2rε\n16β+ε·ε\n34β· |Cj| ·min\nm∈M∥µj−m∥2\n≥1\n272\u0012ε\nβ\u00133/2\n· |Cj| ·min\nm∈M∥µj−m∥2,\nwhich completes the proof.\nNext, we show that the marginal probability of picking a point from an uncovered cluster Cj\ncannot be significantly smaller than the marginal probability of picking a point from the union of\ncovered clusters with larger cardinality than Cj.\nLemma 3.14. LetMbe a set of centers, and let Cdenote a set of clusters that are ε-covered by M. LetH\ndenote the set of points in all the clusters in C. Letβ>2400\nε2. Consider a cluster Cj̸∈ C. Let ibe the largest\nindex such that C i∈ C. Suppose that M is not an (ε,β)-mean seeding set of C j, and that i <j. Then\nP[p∈Cβ\nj|p∈ H ∪ Cj]≥ε4·β−3/2\n1088 k.\nProof. For the points in H ∪ Cj, we have\n∑\np∈H∪ Cjmin\nm∈M∥p−m∥2=∑\nCh∈C|Ch| ·\u0012\n∆h+min\nm∈M∥µh−m∥2\u0013\n+|Cj| ·(∆j+min\nm∈M∥µj−m∥2)\n– 24 –\n\n≤∑\nCh∈C(1+ε)· |Ch| ·∆h+|Cj| ·(∆j+min\nm∈M∥µj−m∥2)\n≤2· \n∑\nCh∈C|Ch| ·∆h+ε−1· |Cj|min\nm∈M∥µj−m∥2!\n,\nwhere the first inequality holds by definition of an ε-covered cluster and the second inequality\nholds as Mdoes not ε-cover Cjand thus in particular min m∈M∥µj−m∥2≥ε·∆jdue to Corollary\n3.3.\nAssume for contradiction that the lemma does not hold, so\n∑\np∈Cβ\nj∥p−m∥2<ε4·β−3/2\n1088 k·∑\np∈H∪ Cjmin\nm∈M∥p−m∥2.\nThis yields\n1\n272\u0012ε\nβ\u00133/2\n|Cj| ·min\nm∈M∥µj−m∥2≤∑\np∈Cβ\nj∥p−m∥2\n<ε4·β−3/2\n1088 k·∑\np∈H∪ Cjmin\nm∈M∥p−m∥2\n≤ε4·β−3/2\n544k· \n∑\nCh∈C|Ch| ·∆h+ε−1· |Cj|min\nm∈M∥µj−m∥2!\n,\nwhere the first inequality uses Lemma 3.13. (Note that this lemma assumes that Mis not an\n(ε,β)-seeding set for Cj.) Rearranging the terms, we get\nε3·β−3/2·\n544· |Cj| ·min\nm∈M∥µj−m∥2≤ \n(ε/β)3/2\n272−(ε/p\nβ)3\n544k!\n· |Cj| ·min\nm∈M∥µj−m∥2\n≤ε4·β−3/2\n544k·∑\nCh∈C·|Ch| ·∆h.\nTherefore, as |Cj| ≤ | Ch|for all Ch∈ C,\n|Cj|2min\nm∈M∥µj−m∥2≤ε\nk∑\nCh∈C|Ch| ·∆h· |Cj| ≤ε\nk· |Ch|2·∆h.\nThis, however, implies that Cjisε-covered by M, contradicting the lemma’s assumption.\nWe now consider a cluster Cjthat is small compared to the union of the clusters C′\njwith j′>j.\nIn this case, we show that one of the distance-proportional distributions that we use guarantees\nthat the probability of sampling points from the core of Cjis large.\nLemma 3.15. LetMbe a set of centers. Let β>2400\nε2. Let jbe the smallest index such that Cjis not\nε-covered by M. IfMis not an (ε,β)-mean seeding set for Cj, then there exists i∈ {0, 1,. . .,ηlog(nk/ε)}\nsuch that Cβ\nj∈Ai,Mand\nPi[p∈Cβ\nj]≥1\n4352·k·\u0012ε\nβ5/8\u00134\n– 25 –\n\nProof. By Markov’s inequality |Cj|/2<|Cβ\nj|. Let ibe the smallest value such that Cβ\nj⊂Ai,M.\n(Clearly, Cβ\nj⊂A0,M, soiexists.) We have due to Lemma 3.13\n∑\np∈Cβ\njmin\nm∈M∥p−m∥2≥1\n272\u0012ε\nβ\u00133/2\n· |Cj| ·min\nm∈M∥µj−m∥2,\nAlso, for all p∈Cβ\nj,\nmin\nm∈M∥p−m∥ ≤ min\nm∈M∥µj−m∥+∥p−µj∥ ≤ min\nm∈M∥µj−m∥+q\nβ·∆j<2r\nβ\nεmin\nm∈M∥µj−m∥\nwhere the last inequality follows from the fact that Mdoes not ε-cover Cj, somin m∈M∥µj−m∥2>\nε·∆jNote that this implies min m∈M∥p−m∥2≤8·β\nε·min m∈M∥µj−m∥2for all p∈Ai,M, as\nΓi<2max p∈Cjmin m∈M∥p−m∥. Since for any cluster Cj′with j′>jwe have |Cj′| ≤ | Cj|and\ntherefore\n∑\np∈Cj′∩Ai,M∥p−m∥2≤ | Cj′∩Ai,M| ·8·β\nε·min\nm∈M∥µj−m∥2≤ |Cj| ·8·β\nε·min\nm∈M∥µj−m∥2\n≤2176·\u0012β\nε\u00135/2\n·∑\np∈Cβ\njmin\nm∈M∥p−m∥2. (5)\nDefine H=∪j−1\nh=1ChandL=∪k\nh=j+1Ch. Clearly\nPi[p∈(H ∪ Cj∪ L)∩Ai,M] =1.\nBy Lemma 3.14,\nPi[p∈Cβ\nj|p∈(Cj∪ H)∩Ai,M]≥1\n1088·k·\u0012ε\nβ3/8\u00134\n.\nBy Inequality (5),\nPi[p∈Cβ\nj|p∈(Cj∪ L)∩Ai,M]≥1\n2176·k·\u0012ε\nβ\u00135/2\n.\nNow,\nmax\b\nPi[p∈(H ∪ Cj)∩Ai,M],Pi[p∈(Cj∪ L)∩Ai,M]\t≥1\n2,\nso,\nPi[p∈Cβ\nj] =Pi[p∈Cβ\nj|p∈(H ∪ Cj)∩Ai,M]·Pi[p∈(H ∪ Cj)∩Ai,M]\n=Pi[p∈Cβ\nj|p∈(Cj∪ L)∩Ai,M]·Pi[p∈(Cj∪ L)∩Ai,M]\n≥1\n2·minn\nPi[p∈Cβ\nj|p∈(H ∪ Cj)∩Ai,M],Pi[p∈Cβ\nj|p∈(Cj∪ L)∩Ai,M]o\n≥1\n2min(\n1\n2176·k·\u0012ε\nβ\u00135/2\n,1\n1088·k·\u0012ε\nβ3/8\u00134)\n≥1\n4352·k·\u0012ε\nβ5/8\u00134\n.\nWe remark that by Lemma 3.9 we may assume that all non-zero squared distances are within a\nfactor n30of each other. Thus, the desired i<30 log n.\n– 26 –\n\nFinally, we show that we can account for the bias in the sampling in order to estimate an\napproximate mean.\nLemma 3.16. LetMbe a set of centers. Let jbe the smallest index such that Cjis not ε-covered by M.\nSuppose that M is not an (ε/4,β)-mean seeding set for C j. Consider a set of points S′sampled iid from Pi,\nand let S=S′∩Cβ\nj. Ifβ≥2400ε−2andS>17825792 ·k\u0010\nβ7/12\nε\u00116\nlog(2/δ), then with probability at\nleast 1−δ, we have that S′∪M is an (ε/4,β)-mean seeding set of C j.\nProof. We first apply some preprocessing. Let qbe an arbitrary point in S. We subtract qfrom all\npoints. Therefore, we may assume that all points p∈Cβ\nj, as well as any point m∈Mthat has\ndistance at mostq\nε2∆j/2 to some point in Cβ\njhave norm at mostq\n(β+ε2/2)∆j.\nFurthermore, let µDbe the mean of D(Cj,β,ε/16,M), and let µCbe the mean of C=Cβ\nj\\\nD(Cj,β,ε/16,M). Due to Lemma 3.10, we have that ˆµj=1\n|Cβ\nj|·\u0000\nµC· |C|+µD· |D(Cj,β,ε/16, M)|\u0001\nis anε\n4-approximate mean of µj, or more specifically\n∥ˆµj−µj∥ ≤r\nε\n4·∆j.\nThus, if we can show that Sis anε\n4-mean seeding set of µC(yielding anε\n4-approximate mean cµC,\nthen\n\r\r\r\r\r\r1\n|Cβ\nj|\u0000cµC· |C|+µD· |D(Cj,β,ε/16, M)|\u0001−µj\r\r\r\r\r\r\n≤ ∥cµC−µC∥+\r\r\r\r\r\r1\n|Cβ\nj|\u0000\nµC· |C|+µD· |D(Cj,β,ε/16, M)|\u0001−µj\r\r\r\r\r\r\n≤q\nε/4∆j+q\nε/4∆j≤q\nε∆j,\nwhere we used Lemma 3.2 in the first inequality.\nLetito denote the largest index for which Ai,Mcontains Cβ\nj. Define for every point p∈Ca\nweight wp=1\n|C|·Pi[p|C]. To clarify, Pi[p|C]is the conditional probability that a single sample drawn\nfrom the probability distribution Piisp, conditioned on the sampled point being from C. We can\nthen write\nµC=∑\np∈C(wpp)·Pi[p|C].\nIn other words, µCis the expectation of the scaled vector wppunder the conditional distribution\nPi[· |C]LetSC=S∩C. Conditioning on s=|S∩C|, the sample SCcan be generated by taking\nsindependent samples from the distribution Pi[· |C]. We write SC={p1,p2,. . .,ps}, where the\npoints are random variables. Define\ncµC=1\ns·∑\np∈SCwpp.\n– 27 –\n\nTaking expectation over Pi[· |C∧s], we have\nE\u0002∥cµC−µC∥2\u0003=E\"\n1\ns2·s\n∑\ni=1s\n∑\nj=1(wpipi−µC)·(wqjqj−µC)#\n=1\ns2·s\n∑\ni=1E\u0002∥wpipi−µC∥2\u0003\n.\nThe cross terms vanish as the sampled points are independent and the expectation of wppisµC.\nTo complete the proof, notice that for p∈C,∥wpp−µC∥2≤2wp∥p∥2+2∥µC∥2. We may\nassume without loss of generality that the entire point-set is shifted so that µC=⃗0. Hence, as\nµC∈conv(Cβ\nj)and p∈Cβ\nj, we have that ∥p∥2≤4β∆j. Also,ε\n16∆j≤min m∈M∥p−m∥2≤β·∆j,\nwhere the lower bound holds by definition of D(Cj,β,ε/16,M)and the upper bound holds by\ndefinition of Cβ\nj. Thus,ε\n16·|C|≤Pi[p|p∈C]≤β\n|C|. This implies that wp≤16\nε. Therefore,\nE\u0002∥cµC−µC∥2\u0003≤1\ns·64β\nε·∆j, soPi\u0014\n∥cµC−µC∥2>1\ns·128β\nε·∆j\u0015\n<1\n2.\nIfs≥512β\nε2, we get that cµCε\n4-covers µCwith probability at least1\n2. Thus, if s≥512β\nε2·log(2/δ)we\ncan apply this log δ−1times to boost the success probability to 1 −δ\n2.\nWe now bound the number of samples that we need to obtain SC. Due to Lemma 3.15, we have\nPi[p∈Cβ\nj]≥1\n4352·k·\u0010\nε\nβ5/8\u00114\n. Therefore, Ei[|SC|] =|S| ·Pi[p∈Cβ\nj]≥ |S| ·1\n4352·k·\u0010\nε\nβ5/8\u00114\n. Setting\n|S| ≥17825792 ·k\u0010\nβ7/12\nε\u00116\nlog(2/δ)and applying the Chernoff bound, we have\nP\u0014\n|SC|<512β\nε2·log(2/δ)\u0015\n≤exp(−8·E[|SC|]])≤δ/2.\nConversely, with probability 1 −δ,S∪Mcontains a (ε/4,β)-mean seeding set of Cj.\nWe are now ready to give a proof of Lemma 3.8.\nProof of Lemma 3.8. Due to Lemma 3.9, we know that we have at most kpoint A1,. . .Aksets such\nthat any cluster of the optimum clustering is fully contained in one the Ai. We guess the correct\nnumber of centers from each Ai, which takes at most (2k−1\nk−1)guesses. For each Ai, we then find a set\nof centers Mthatεcovers all clusters of the optimum in Ai.\nWe simplify the calculation by assuming that Aicontains all kclusters. We iteratively add\ncenters to M, writing Mjafter the j-th iteration. Our goal is to ensure that Mjcovers the clusters\nC1,. . .Cj. In every iteration, we first sample to obtain a suitable mean seeding set and then apply\nLemma 3.6 to extract the mean from the set.\nWe start with C1. We know that |C1| ≥n\nk, so we can use Lemma 3.7 to sample a set S1of\n32kε−1log(k/δ)points uniformly at random and then enumerate over all candidate means induced\nby uniformly weighted subsets of S1and the to obtain an ε-approximate mean of C1. This takes\ntime 2|S1|and yields 2|S1|candidate means, of which one is an ε-covers C1with probability 1−δ/k.\nFor subsequent iterations, Lemma 3.16 guarantees us that there exists a distribution Pisuch that\nif we sample a set Sjof17825792 ·k\u0010\nβ7/12\nε\u00116\nlog(2k/δ)points, then Mj−1∪Sis an (ε/4,β)mean\n– 28 –\n\nseeding set of Cjwith probability 1−δ/k. Moreover, Lemma 3.12 guarantees us that we have to try\nat most 13log(nk/ε)distributions to do find the correct Pi. Extracting all candiate means for each\nPivia Lemma 3.6 takes time\u001010β·|Sj|\nε+1\u0011|Sj|\nand results in\u001010β·|Sj|\nε+1\u0011|Sj|\ncandidate means.\nThus, the overall number of candidate centers Mkgenerated by the procedure, as well as the\nrunning time, is\n2|S1|·k\n∏\nj=213 log (nk/ε)·\u001210β· |Sj|\nε+1\u0013|Sj|\n=logkn·2η·k2·ε−12log2(k/(εδ))\nfor some absolute constant η. Moreover by the union bound, one of the Mkmust εcover all clusters\nwith probability 1−δ. Notice that if logn<k2, then logknis absorbed by 2η·k2·ε−12log2(k/(εδ))with a\nsuitable rescaling of η. If log n>k2, then logkn<2√\nlognlog log n<no(1).\nWe account for the enumeration over the number of clusters from each Aivia another rescaling\nofη. For a given MandPi, the probabilities can be computed in time O(n·d· |M|)Thus, the\noverall running time to obtain a set of centers that εcovers all clusters of the optimum is\nn1+o(1)·d·2η·k2·ε−12log2(k/(εδ)),\nand this completes the proof.\nEnumerating over Sizes and Obtaining the Parameterized PTAS. We complete this section by\nfunneling the mean-seeding procedure into a PTAS.\nTheorem 3.17. There exists an algorithm running in time\nO\u0010\nn1+o(1)d·2η·k2·ε−12log2(k/(εδ))\u0011\n,\nfor some absolute constant η, that computes a (1+ε)-approximate solution to ℓ2\n2k-MinSum Clustering\nwith probability 1−δ.\nProof. Given a set of candidate centers obtained via Lemma 3.8 and an estimate [OPT of the optimal\nMinSum clustering cost OPT , we wish to find an assignment of points to centers such that the\nclustering that has cost (1+ε)·[OPT , or verify that no such assignment exists. Note that given a\nclustering, we can verify its cost in time O(ndk)by computing the mean of every cluster and then\nusing the first identity of Lemma 3.2.\nWe first notice that if we are given an α-approximation \\OPT kmeans to an k-means clustering\nOPT kmeans , we also know OPT∈h\n\\OPT kmeans ,n·\\OPT kmeansi\n. A constant, say 20, approximation to k-\nmeans can be found in time ˜O(nd+k2)[DSS24 ]. We thus can efficiently obtain (1+ε)approximate\nvalue of OPT using at most 2 ε−1log(20n)estimates.\nSuppose we are given [OPT , as well as a candidate set of centers C={c1,c2,. . .ck}. Now, we\ndiscretize the cost of all points to each cluster ci, starting atε\nn2·[OPT by powers of (1+ε), going all\nthe way up to[OPT . Define\nGi,j=n\np|(1+ε)j−1·ε\nn2·[OPT≤ ∥p−ci∥2≤(1+ε)j·ε\nn2·[OPTo\n– 29 –\n\nwith Gi,0=n\np| ∥p−ci∥2≤ε\nn·[OPTo\n. Notice that if ∥p−ci∥2>(1+ε)·[OPT , then pcannot\nbe served by ciwithout invalidating [OPT as an accurate estimate of OPT . Thus we have at\nmost 2ε−1logn2\nεmany sets Gi,j. Finally, consider the set B1j′,2j′′,1j′′′,...which is the intersection of\nG1,j∩G2,j′∩G2,j′′. . .. Notice that there are\u0010\n2ε−1logn2\nε\u0011k\nmany sets Band that we can compute\nthe partitioning of the point set Ainto the sets Bin time ndk·\u0010\n2ε−1logn2\nε\u0011k\n. We finally discretize\nthe size of subsets of any set Bby powers of (1+ε), for which there are 2ε−1log|B| ≤2kε−1logn\ndiscretizations.\nWe now enumerate over all possible assignments of subsets of sets Bto centers ci. Notice that\nthere are at most\u0000\n2ε−1logn\u0001kpossible sizes, which we multiply by the number\u0000\n2ε−1logn\nε\u0001kof\nsets B.\nWe claim that if Cis the center set of a (1+ε)-approximate solution, then there exists an\nassignment of the Bthat is (1+O(ε))approximate as well. Specifically, consider any assignment\nπ:A→Ccost cost π(A,C) =∑p∈A∥p−π(p)∥2. In the following, we use Bjto refer to the\nintersection of Bwith Ci, i.e. Bi,j=Ci∩B1j′,2j′′,...,Then rewriting the sum, we obtain\n∑\np∈Ci(∑\nj|Bi,j|)∑\nj>0|Bi,j| ·2j−1ε\nn2·[OPT ≤cost π(A,C)\n≤(1+ε)·∑\np∈Ci(∑\nj|Bi,j|)∑\nj>0|Bi,j| ·2jε\nn2·[OPT+n2·ε\nn2·[OPT\n= ( 1+ε)·∑\np∈Ci(∑\nj|Bi,j|)∑\nj>0|Bi,j| ·2jε\nn2·[OPT+ε·[OPT\nand moreover\n(1+ε)·∑\np∈Ci \n∑\nj|Ci,j|!\n∑\nj>0|Ci,j| ·2jε\nn2·[OPT≤(1+ε)·∑\np∈Ci \n∑\nj|Ci,j|!\n∑\nj>0|Ci,j| ·2j−1ε\nn2·[OPT .\nIn other words, using the discretizations Binstead of the correct points in the assignment of AtoC\npreserves the cost up to a multiplicative factor (1+ε)and an additive ε·[OPT .\nNext, observe that if we have an estimate |Bi,j| ≤ ˆBi,j≤(1+ε)· |Bi,j|, then ∑j|Bi,j| ≤∑jˆBi,j≤\n(1+ε)·∑j|Bi,j|. Therefore, using the discretized estimates of |Bi,j|, we also have\n∑\np∈Ci(∑\njˆBi,j)∑\nj>0ˆBi,j| ·2j−1ε\nn2·[OPT≤cost π(A,C)\n≤(1+ε)3∑\np∈Ci \n∑\njˆBi,j!\n∑\nj>0ˆBi,j·2jε\nn2·[OPT+ε·[OPT\nGiven a (discretized) assignment of the sets BtoC, we can now extract a clustering as follows.\nIn the following the value of jis not necessary so we omit the subscript jfrom Bi,j. We sort ˆBiby\nsizes, breaking ties arbitrarily. We assign ˆBimany arbitrary points of Bto cluster Ciwith center ci.\nThe final cluster Ci′in the ordering is assigned the remaining points. Notice that assigning fewer\npoints to Ci′can only decrease the cost of Ci′.\n– 30 –\n\nThe cost of this assignment can only be cheaper than the estimated upper bound\n(1+ε)3∑\np∈Ci(∑\njˆBi,j)∑\nj>0ˆBi,j·2jε\nn2·[OPT+ε·[OPT ,\nas we can only assign fewer points from every group Bto a cluster and the cost of the points can\nonly be cheaper than the estimated upper bound. As mentioned above, evaluating the cost of the\nresulting clustering takes time O(ndk).\nThus, assuming that OPT≤ ˆOPT≤(1+ε)·OPT and that we were working with a suitable ε-\napproximate candidate set of centers C, we can extract a clustering with cost at most (1+ε)5·OPT\nin time O\u0010\nnd\u0000\n2ε−1logn\u0001k·\u0000\n2ε−1logn\nε\u0001k\u0011\nmultiplying this figure by the number of candidate\nvalues of OPT and the number of candidate centers obtained via Lemma 3.8 yields a running time\nof\nO \nnd·\u0012\n2ε−1log20n\nε\u00133k\n·no(1)·2η·k2·ε−12log2(k/(εδ))!\nplus the running time for computing the candidate centers. Using (logn)k≤k3k+2√\nlognlog log n≤\nk3k+no(1), rescaling εby a factor of 10, this yields a (1+ε)approximation with probability 1−δin\ntime\nO\u0010\nn1+o(1)d·2η·k2·ε−12log2(k/(εδ))\u0011\nfor some absolute constant η.\n4 Learning-Augmented ℓ2\n2Min-Sum k-Clustering\nIn this section, we describe and analyze our learning-augmented algorithm for ℓ2\n2min-sum k-\nclustering, corresponding to Theorem 1.5.\nWe first recall the following property describing the 1-means optimizer for a set of points.\nFact 4.1. [IKI94] Given a set X ⊂Rdof points, the unique minimizer of the 1-means objective is\n1\n|X|∑\nx∈Xx=argmin\nc∈Rd∑\nx∈X∥x−c∥2\n2.\nWe next recall the following identity, which presents an equivalent formulation of the ℓ2\n2min-\nsum k-clustering objective.\nFact 4.2. [IKI94] For each cluster C iof points, let c ibe the geometric mean of the points, i.e.,\nci=1\n|Ci|∑\nx∈Cix.\nThen1\n2∑\ni∈[k]∑\nxu,xv∈Ci∥xu−xv∥2\n2=∑\ni∈[k]|Ci| ·∑\nx∈Ci∥x−ci∥2\n2.\n– 31 –\n\nGiven Fact 4.2, it is more convenient for us to rescale the ℓ2\n2min-sum k-clustering objective for\nan input set Xin this section to be defined as:\nmin\nC1,...,Ck1\n2∑\ni∈[k]∑\np,q∈Ci∩X∥p−q∥2\n2.\nWe now formally define the precision and recall guarantees of a label predictor.\nDefinition 4.3 (Label predictor) .Suppose that there is an oracle that produces a label i∈[k]for each\nx∈X, so that the labeling partitions X=P1˙∪. . .˙∪Pkinto kclusters P1,. . .,Pk, where all points in Pi\nhave the same label i∈[k]. We say the oracle is a label predictor with error rateαif there exists some fixed\noptimal min-sum clustering P∗\n1, . . . , P∗\nksuch that for all i ∈[k],\n|Pi∩P∗\ni| ≥(1−α)max(|Pi|,|P∗\ni|).\nWe say that P∗={P∗\n1, . . . , P∗\nk}is the clustering consistent with the label oracle.\nWe also recall the following guarantees of previous work on learning-augmented k-means\nclustering for a label predictor with error rate α∈\u0002\n0,1\n2\u0001\n.\nTheorem 4.4. [NCN23 ] Given a label predictor with error rate α<1\n2consistent with some clustering P∗=\n{P∗\n1,. . .,P∗\nk}with centers {c∗\n1,. . .,c∗\nk}, there exists a polynomial-time algorithm LEARNED CENTERS that\noutputs a set of centers {c1, . . . , ck}, so that for each i ∈[k],\n∑\nx∈P∗\ni∥x−ci∥2\n2≤(1+γαα)∑\nx∈P∗\ni∥x−c∗\ni∥2\n2,\nwhere γα=7.7forα∈\u0002\n0,1\n7\u0001\norγα=5α−2α2\n(1−2α)(1−α)forα∈\u0002\n0,1\n2\u0001\n.\nDescription of LEARNED CENTERS .For the sake of completeness, we briefly describe the al-\ngorithm LEARNED CENTERS underlying Theorem 4.4. The algorithm decomposes the k-means\nclustering objective by considering the subset Piof the input dataset Xthat are assigned each label\ni∈[k]by the oracle. The algorithm further decomposes the k-means clustering objective along the\nddimensions, by considering the j-th coordinate of each subset Pi, for each j∈[d]. Now, although\nanαfraction of the points in Pican be incorrectly labeled, there are two main cases: 1) Piincludes a\nnumber of mislabeled points that are far from the true mean and hence easy to prune away, or 2)\nPiincludes a number of mislabeled points that are difficult to identify due to their proximity to\nthe true mean. However, in the latter case, these mislabeled points only has a small effect on the\noverall k-means clustering objective. Hence, it suffices for the algorithm to handle the first case,\nwhich it does by selecting the interval of (1− O(α))points of Piin dimension jthat has the best\nclustering cost. The mean of the points of Piin dimension jthat lie in that interval then forms the\nj-th coordinate of the i-th centroid output by algorithm. The algorithm repeats across j∈[d]and\ni∈[k]to form kcenters that are well-defined in all ddimensions. We give the algorithm formally\nin Algorithm 1.\nBy Fact 4.1 and Fact 4.2, it follows that these centers are also good centers for the clustering\ninduced by a near-optimal ℓ2\n2min-cost k-clustering. Specifically, the optimal center of a cluster of\npoints for ℓ2\n2min-cost k-clustering is the centroid of the cluster and similarly, the optimal center of a\n– 32 –\n\nAlgorithm 1 LEARNED CENTERS : learning-augmented k-means clustering [NCN23]\nInput: Dataset Xwith partition P1, . . . , Pkinduced by label predictor with error rate α\nOutput: Centers c1, . . . , ckfor(1+O(α))-optimal k-means clustering\n1:fori∈[k]do\n2: forj∈[d]do\n3: Letωi,jbe the collection of all intervals that contain (1− O(α))|Pi|points of Pi,j\n4: Letci,jbe the center with the lowest k-means clustering cost of any interval in ωi,j\n5: ci← { ci,j}j∈[d]for all i∈[d]\n6: return {c1, . . . , ck}\ncluster of points for k-means clustering is the centroid of the cluster. See Lemma 4.6 for the formal\ndetails.\nUnfortunately, although the centers {c1,. . .,ck}returned by LEARNED CENTERS are good cen-\nters for the clustering induced by a near-optimal ℓ2\n2min-cost k-clustering, it is not clear what the\nresulting assignment should be. In fact, we emphasize that unlike k-means clustering, the optimal\nℓ2\n2min-cost k-clustering may not assign each point to its closest center.\nConstrained min-cost flow. To that end, we now create a constrained min-cost flow problem as\nfollows. We first create a source node sand a sink node tand require that n=|X|flow must be\npushed from stot. We create a node uxfor each point x∈Xand create a directed edge from sto\neach node uxwith capacity 1and cost 0. There are no more outgoing edges from sor incoming\nedges to each ux. This ensures that to achieve nflow from stot, a unit of flow must be pushed\nacross each node ux.\nFor each center cioutput by our learning-augmented algorithm, we create a node vi. For each\nx∈X,i∈[k], create a directed edge from uxtoviwith capacity 1and cost1\n1−α· |Pi| · ∥x−ci∥2\n2.\nThere are no other outgoing edges from ux, thus ensuring that a unit of flow must exit each node\nuxto the nodes virepresenting the clusters, and with approximately the corresponding cost if x\nwere assigned to center ci. We then create a directed edge from each node vitotwith capacity\n1\n1−α· |Pi|and cost 0. Finally, we require that at least (1−α)· |Pi|flow goes through node vi, so that\nthe number of points assigned to each center ciis consistent with the oracle. The construction in its\nentirety appears in Figure 4.\nAlgorithm 2 Learning-augmented min-sum k-clustering\nInput: Dataset Xwith partition P1, . . . , Pkinduced by label predictor with error rate α\nOutput: Labels for all points consistent with a (1+O(α))-optimal min-sum k-clustering\n1:Letc1, . . . , ckbe the output centers of L EARNED CENTERS onP1, . . . , Pk\n2:Create a min-cost flow problem Fwith required flow nas in Figure 4\n3:Solve the flow problem F\n4:For each x∈X, let the flow from uxbe sent to the node vℓx, so that ℓx∈[k]\n5:Label xwithℓx\nWe first show that the ℓ2\n2min-sum k-clustering cost induced by Algorithm 2 has objective value\nat most the cost of the optimal flow in the problem Fcreated by Algorithm 2.\n– 33 –\n\nLetX=P1˙∪. . .˙∪Pkand c1, . . . , ckbe inputs\n(1) Create a source node sand a sink node t, requiring n=|X|flow from stot\n(2)Create a directed edge from sto each node uxrepresenting a separate x∈Xwith capacity\n1 and cost 0\n(3)Create a directed edge to tfrom each node virepresenting a separate ciwith capacity\u00041\n1−α· |Pi|\u0005\nand cost 0\n(4) Require that at least ⌈(1−α)· |Pi|⌉flow goes through node ci\n(5)For each x∈X,i∈[k], create a directed edge from uxtoviwith capacity 1and cost\n1\n1−α· |Pi| · ∥x−ci∥2\n2\nFig. 4: Constrained min-cost flow problem\nLemma 4.5. LetFbe the cost of the flow output by Algorithm 2. Then for the corresponding clustering\nQ1, . . . , Qkoutput by Algorithm 2, we have\n1\n2∑\ni∈[k]∑\nxu,xv∈Qi∥xu−xv∥2\n2≤F.\nProof. LetSbe any flow output by Algorithm 2 and let Q1,. . .,Qkbe the corresponding clustering\nofX. Note that Q1,. . .,Qkare well-defined, since each point of xreceives exactly one label by\nAlgorithm 2. Let q1,. . .,qkbe the geometric mean of the points in Q1,. . .,Qk, respectively, so that\nqi=1\n|Qi|∑x∈Qixfor all i∈[k].\nBy Fact 4.1 and Fact 4.2, we have that\n1\n2∑\ni∈[k]∑\nxu,xv∈Qi∥xu−xv∥2\n2=∑\ni∈[k]|Qi| ·∑\nx∈Qi∥x−qi∥2\n2\n≤∑\ni∈[k]|Qi| ·∑\nx∈Qi∥x−ci∥2\n2.\nSince each node vihas capacity1\n1−α· |Pi|, then we have |Qi| ≤1\n1−α· |Pi|. Therefore,\n1\n2∑\ni∈[k]∑\nxu,xv∈Qi∥xu−xv∥2\n2≤∑\ni∈[k]1\n1−α· |Pi| ·∑\nx∈Qi∥x−ci∥2\n2.\nBecause each x∈Qiis mapped to ci, then the cost induced by the mapping in the flow Sis exactly\n1\n1−α· |Pi| · ∥x−ci∥2\n2. Therefore, the right-hand side is exactly the cost Fof the flow S. Hence, we\nhave\n1\n2∑\ni∈[k]∑\nxu,xv∈Qi∥xu−xv∥2\n2≤F,\nas desired.\n– 34 –\n\nWe next show that the cost of the optimal ℓ2\n2min-sum k-clustering has objective value at least\nthe cost of the optimal in the problem Fcreated by Algorithm 2, up to a (1+O(α))factor.\nLemma 4.6. LetFbe the cost of the optimal solution to the min-cost flow problem Fin Algorithm 2 and let\nOPT be cost of the optimal min-sum k-clustering on X. Letγαbe the fixed constant from Theorem 4.4. Then\nOPT≥(1−α)2·1\n1+γαα·F.\nProof. LetP∗\n1,. . .,P∗\nkbe an optimal clustering consistent with the label oracle. Let c∗\n1,. . .,c∗\nkbe the\noptimal centers for P∗\n1,. . .,P∗\nkrespectively and let c1,. . .,ckbe the kcenters output by Algorithm 2.\nBy the definition of the label oracle, we have\n|Pi∩P∗\ni| ≥(1−α)max(|Pi|,|P∗\ni|),\nso that\n|P∗\ni| ≥ | Pi∩P∗\ni| ≥(1−α)max(|Pi|,|P∗\ni|)≥(1−α)· |Pi|.\nThus, by Fact 4.2,\n1\n2∑\ni∈[k]∑\nxu,xv∈P∗\ni∥xu−xv∥2\n2=∑\ni∈[k]|P∗\ni| ·∑\nx∈P∗\ni∥x−c∗\ni∥2\n2\n≥∑\ni∈[k](1−α)· |Pi| ·∑\nx∈P∗\ni∥x−c∗\ni∥2\n2\n= (1−α)2∑\ni∈[k]1\n1−α· |Pi| ·∑\nx∈P∗\ni∥x−c∗\ni∥2\n2.\nLetγαbe the fixed constant from Theorem 4.4. Then by Theorem 4.4, we have that\n∑\nx∈P∗\ni∥x−c∗\ni∥2\n2≥1\n1+γαα·∑\nx∈P∗\ni∥x−ci∥2\n2.\nTherefore,\n1\n2∑\ni∈[k]∑\nxu,xv∈P∗\ni∥xu−xv∥2\n2≥(1−α)2·1\n1+γαα·∑\ni∈[k]1\n1−α· |Pi| ·∑\nx∈P∗\ni∥x−ci∥2\n2.\nNote that since |Pi| ≥ | Pi∩P∗\ni| ≥(1−α)max(|Pi|,|P∗\ni|)≥(1−α)· |P∗\ni|, then we have |P∗\ni| ≤\n1\n1−α· |P∗\ni|. Thus a valid flow for Fwould be to send |P∗\ni|units of flow across each x∈P∗\ni. In other\nwords, ∑i∈[k]1\n1−α· |Pi| ·∑x∈P∗\ni∥x−ci∥2\n2is the cost of a valid flow for F.\nTherefore, by the optimality of the optimal min-cost flow, we have\n∑\ni∈[k]1\n1−α· |Pi| ·∑\nx∈P∗\ni∥x−ci∥2\n2≥F,\nand so\n1\n2∑\ni∈[k]∑\nxu,xv∈P∗\ni∥xu−xv∥2\n2≥(1−α)2·1\n1+γαα·F,\nas desired.\n– 35 –\n\nPutting together Lemma 4.5 and Lemma 4.6, it follows that the cost of the clustering induced by\nAlgorithm 2 is a good approximation to the optimal ℓ2\n2min-sum k-clustering.\nCorollary 4.7. Letγαbe the fixed constant from Theorem 4.4. Algorithm 2 outputs a clustering Q1,. . .,Qk\nof X such that\n1\n2∑\ni∈[k]∑\nxu,xv∈Qi∥xu−xv∥2\n2≤1+γαα\n(1−α)2·OPT ,\nwhere OPT is cost of an optimal min-sum k-clustering on X.\nProof. LetSbe the flow output by Algorithm 2 and let Q1,. . .,Qkbe the corresponding clustering\nofX. We again remark that Q1,. . .,Qkis a valid clustering of X, since each point of xreceives\nexactly one label by Algorithm 2. The claim then follows from Lemma 4.5 and Lemma 4.6.\nWe recall the following folklore integrality theorem for uncapacitated min-cost flow.\nTheorem 4.8. Any minimum cost network flow problem with integral demands has an optimal solution\nwith integral flow on each edge.\nProof. Though the proof is well-known, e.g., [ Con12 ], we repeat it here for the sake of completeness.\nConsider induction on n, the number of nodes in the flow graph. The statement is vacuously true\nforn=0and n=1, which serve as our base cases. Observe that we can write the linear program\nwith n−1constraints and thus there exists an optimal solution where at most n−1edges have\npositive flow. By a simple averaging argument, there exists a vertex vthat has at most one incident\nedge ewith positive flow. Let ube the other endpoint of the the edge e= (u,v). Since it is the only\nedge incident to v, it must satisfy the entire demand of v. Because vhas integer demand, then ehas\ninteger flow. However, the remainder of the graph has n−1vertices and thus by induction, the\nremaining of the vertex demands are satisfied by a flow with integer demands.\nWe now adjust the integrality theorem to handle capacitated edges, thereby showing that the\nresulting solution for the min-cost flow problem in Figure 4 is integral.\nCorollary 4.9. Any minimum cost network flow problem with integral demands and capacities has an\noptimal solution with integral flow on each edge.\nProof. The proof follows from a simple gadget to transform a min-cost flow problem with integer-\ncapacitated edges into an uncapacitated min-cost flow problem. Suppose there exists a directed\nedge efrom utovwith capacity cand cost p. Suppose furthermore that uhas demand d1and v\nhas demand d2. Then we create an additional vertex wand we replace ewith directed edges e1\ngoing from utowand e2going from vtow. We change the demand of vtod2−c, noting this can\nbe negative. We also require vertex wto have demand c. We then have cost pon edge e2and cost 0\non edge e2. See Figure 5 for an illustration of the transformation. Since the resulting graph after the\nreduction does not have any capacities on the edges, it follows from Theorem 4.8 that there exists\nan integral solution to the original input problem.\nHence, the min-cost flow solution defines a valid clustering that approximately optimal with\nrespect to the ℓ2\n2min-sum k-clustering objective. However, we further want to show the property\nholds for the solution returned by a linear program solver. In fact, it is well-known the constraint\nmatrix is totally unimodular, i.e., all submatrices have determinant −1, 0, or 1.\n– 36 –\n\nu\nDemand: d1v\nDemand: d2Edge e: capacity cand cost p\nu\nDemand: d1Edge e1: cost p\nw\nDemand: cEdge e2: cost 0\nv\nDemand: d2−c\nFig. 5: Example of transformation of capacitated min-cost flow problem into uncapacitated min-cost\nflow problem.\nTheorem 4.10 (Theorem 19.1 in [ Sch98 ]).LetAbe a totally unimodular matrix and let bbe an integer\nvector. Then all vertices of the polyhedron P ={x|Ax≤b}are integral.\nSince the solution of a linear program must lie at a vertex of the feasible polytope, then\nTheorem 4.10 implies any solution to the linear program will also be integral. Thus a valid\nclustering can be recovered by using the output of a linear program solver. We recall the following\nvarious implementations of solvers for linear programs.\nTheorem 4.11. [Kar84 ,Vai89 ,Vai90 ,LS15 ,LSZ19 ,CLS21 ,JSWZ21 ] There exists an algorithm that solves\na linear program with n variables that can be encoded in L bits, using poly(n,L)time.\nPutting things together, we have the following guarantees for our learning-augmented algo-\nrithm.\nTheorem 4.12. There exists a polynomial-time algorithm that uses a label predictor with error rate αand\noutputs a1+γαα\n(1−α)2-approximation to min-sum k-clustering, where γαis the fixed constant from Theorem 4.4.\nProof. Correctness follows from Corollary 4.7.\nFor the runtime analysis, first observe that the centers c1,. . .,ckcan be computed in polynomial\ntime by Theorem 4.4. Subsequently, Fcan be written as a linear programming problem with\nat most poly(n)constraints and variables. Therefore, the desired claim follows by running any\npolynomial-time linear programming solver, i.e., Theorem 4.11 and observing that the output\nsolution induces a valid clustering, by Theorem 4.10.\nAcknowledgements\nThe work was conceptualized while all the authors were visiting the Institute for Emerging CORE\nMethods in Data Science (EnCORE) supported by the NSF grant 2217058. Karthik C. S. was\nsupported by the National Science Foundation under Grants CCF-2313372 and CCF-2443697,\na grant from the Simons Foundation, Grant Number 825876, Awardee Thu D. Nguyen, and\npartially funded by the Ministry of Education and Science of Bulgaria’s support for INSAIT,\nSofia University “St. Kliment Ohridski” as part of the Bulgarian National Roadmap for Research\nInfrastructure. Euiwoong Lee was supported in part by NSF grant CCF-2236669 and Google. Yuval\nRabani was supported in part by ISF grants 3565-21 and 389-22, and by BSF grant 2023607. Chris\n– 37 –\n\nSchwiegelshohn was partially supported by the Independent Research Fund Denmark (DFF) under\na Sapere Aude Research Leader grant No 1051-00106B. Samson Zhou is supported in part by\nNSF CCF-2335411. The work was conducted in part while Samson Zhou was visiting the Simons\nInstitute for the Theory of Computing as part of the Sublinear Algorithms program.\nReferences\n[ACE+23] Antonios Antoniadis, Christian Coester, Marek Eliás, Adam Polak, and Bertrand\nSimon. Online metric algorithms with untrusted predictions. ACM Trans. Algorithms ,\n19(2):19:1–19:34, 2023. 3\n[ACI22] Anders Aamand, Justin Y. Chen, and Piotr Indyk. (optimal) online bipartite matching\nwith degree information. In Advances inNeural Information Processing Systems 35:\nAnnual Conference onNeural Information Processing Systems, NeurIPS, 2022. 3\n[ADHP09] Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. Np-hardness of\neuclidean sum-of-squares clustering. Mach. Learn., 75(2):245–248, 2009. 1, 2, 7\n[AGKP22] Keerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. Online algorithms\nwith multiple predictions. In International Conference onMachine Learning, ICML ,\npages 582–598, 2022. 3\n[AKP24] Enver Aman, Karthik C. S., and Sharath Punna. On connections between k-coloring\nand Euclidean k-means. In 32nd Annual European Symposium onAlgorithms, ESA\n2024, 2024. To appear. 2\n[AKS11] Per Austrin, Subhash Khot, and Muli Safra. Inapproximability of vertex cover and\nindependent set in bounded degree graphs. Theory Comput., 7(1):27–43, 2011. 9, 10\n[APT22] Yossi Azar, Debmalya Panigrahi, and Noam Touitou. Online graph algorithms\nwith predictions. In Proceedings ofthe2022 ACM-SIAM Symposium onDiscrete\nAlgorithms, SODA, pages 35–66, 2022. 3\n[AV07] David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seed-\ning. In Nikhil Bansal, Kirk Pruhs, and Clifford Stein, editors, Proceedings ofthe\nEighteenth Annual ACM-SIAM Symposium onDiscrete Algorithms, SODA 2007,\nNew Orleans, Louisiana, USA, January 7-9,2007, pages 1027–1035. SIAM, 2007. 5\n[BB09] Maria-Florina Balcan and Mark Braverman. Finding low error clusterings. In COLT\n2009 -The 22nd Conference onLearning Theory, 2009. 7\n[BBC+19] Luca Becchetti, Marc Bury, Vincent Cohen-Addad, Fabrizio Grandoni, and Chris\nSchwiegelshohn. Oblivious dimension reduction for k-means: beyond subspaces and\nthe johnson-lindenstrauss lemma. In Proceedings ofthe51st Annual ACM SIGACT\nSymposium onTheory ofComputing, STOC, pages 1039–1050, 2019. 7, 17\n[BBG09] Maria-Florina Balcan, Avrim Blum, and Anupam Gupta. Approximate clustering\nwithout the approximation. In Proceedings oftheTwentieth Annual ACM-SIAM\nSymposium onDiscrete Algorithms, SODA, pages 1068–1077, 2009. 7\n– 38 –\n\n[BCP+24] Nikhil Bansal, Vincent Cohen-Addad, Milind Prabhu, David Saulpic, and Chris\nSchwiegelshohn. Sensitivity sampling for k-means: Worst case and stability optimal\ncoreset bounds. CoRR, abs/2405.01339, 2024. 7\n[BCR01] Yair Bartal, Moses Charikar, and Danny Raz. Approximating min-sum k-clustering\nin metric spaces. In Proceedings on33rd Annual ACM Symposium onTheory of\nComputing, pages 11–20, 2001. 1, 2\n[BFSS19] Babak Behsaz, Zachary Friggstad, Mohammad R. Salavatipour, and Rohit Sivaku-\nmar. Approximation algorithms for min-sum k-clustering and balanced k-median.\nAlgorithmica, 81(3):1006–1030, 2019. 1, 7\n[BMS20] Étienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for\nlearning augmented algorithms. In Advances inNeural Information Processing\nSystems 33: Annual Conference onNeural Information Processing Systems,\nNeurIPS, 2020. 3\n[BOR21] Sandip Banerjee, Rafail Ostrovsky, and Yuval Rabani. Min-sum clustering (with\noutliers). In Approximation, Randomization, and Combinatorial Optimization.\nAlgorithms and Techniques, APPROX/RANDOM, pages 16:1–16:16, 2021. 1, 2\n[CEI+22] Justin Y. Chen, Talya Eden, Piotr Indyk, Honghao Lin, Shyam Narayanan, Ronitt\nRubinfeld, Sandeep Silwal, Tal Wagner, David P . Woodruff, and Michael Zhang.\nTriangle and four cycle counting with predictions in graph streams. In The Tenth\nInternational Conference onLearning Representations, ICLR, 2022. 3\n[CIW22] Justin Y. Chen, Piotr Indyk, and Tal Wagner. Streaming algorithms for support-\naware histograms. In International Conference onMachine Learning, ICML , pages\n3184–3203, 2022. 3\n[CK19] Vincent Cohen-Addad and Karthik C. S. Inapproximability of clustering in lp metrics.\nIn60th IEEE Annual Symposium onFoundations ofComputer Science, FOCS , pages\n519–539, 2019. 2, 10\n[CKL21] Vincent Cohen-Addad, Karthik C. S., and Euiwoong Lee. On approximability of clus-\ntering problems without candidate centers. In Proceedings ofthe2021 ACM-SIAM\nSymposium onDiscrete Algorithms, SODA, pages 2635–2648, 2021. 1, 7\n[CKL22] Vincent Cohen-Addad, Karthik C. S., and Euiwoong Lee. Johnson coverage hypoth-\nesis: Inapproximability of k-means and k-median in ℓp-metrics. In Proceedings of\nthe2022 ACM-SIAM Symposium onDiscrete Algorithms, SODA , pages 1493–1530,\n2022. 2, 3, 5, 8, 9, 10, 12, 13, 14, 15\n[CLS21] Michael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current\nmatrix multiplication time. J.ACM, 68(1):3:1–3:39, 2021. 7, 37\n[CLS+22] Vincent Cohen-Addad, Kasper Green Larsen, David Saulpic, Chris Schwiegelshohn,\nand Omar Ali Sheikh-Omar. Improved coresets for euclidean k-means. In\nAdvances inNeural Information Processing Systems 35: Annual Conference on\nNeural Information Processing Systems, NeurIPS, 2022. 2, 7\n– 39 –\n\n[Con12] Vincent Conitzer. Computer science 590, lecture notes. https://courses.cs.duke.\nedu/fall12/compsci590.1/network_flow.pdf , 2012. 36\n[CS07] Artur Czumaj and Christian Sohler. Sublinear-time approximation algorithms for\nclustering via random sampling. Random Struct. Algorithms , 30(1-2):226–256, 2007.\n1, 3, 8\n[CSS21] Vincent Cohen-Addad, David Saulpic, and Chris Schwiegelshohn. A new coreset\nframework for clustering. In STOC ’21:53rd Annual ACM SIGACT Symposium on\nTheory ofComputing, pages 169–182, 2021. 2, 7\n[CSVZ22] Justin Y. Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental\ngraph algorithms via learned predictions. In International Conference onMachine\nLearning, ICML, pages 3583–3602, 2022. 3\n[CWZ23] Vincent Cohen-Addad, David P . Woodruff, and Samson Zhou. Streaming euclidean\nk-median and k-means with o(log n) space. In 64th IEEE Annual Symposium on\nFoundations ofComputer Science, FOCS, pages 883–908, 2023. 7\n[DGKR05] Irit Dinur, Venkatesan Guruswami, Subhash Khot, and Oded Regev. A new multilay-\nered PCP and the hardness of hypergraph vertex cover. SIAM J.Comput. , 34(5):1129–\n1146, 2005. 5, 14\n[DIL+21] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassil-\nvitskii. Faster matchings via learned duals. In Advances inNeural Information\nProcessing Systems 34: Annual Conference onNeural Information Processing\nSystems, NeurIPS, pages 10393–10406, 2021. 3\n[dlVK01] Wenceslas Fernandez de la Vega and Claire Kenyon. A randomized approximation\nscheme for metric MAX-CUT. J.Comput. Syst. Sci., 63(4):531–541, 2001. 1\n[dlVKKR03] Wenceslas Fernandez de la Vega, Marek Karpinski, Claire Kenyon, and Yuval Rabani.\nApproximation schemes for clustering problems. In Proceedings ofthe35th Annual\nACM Symposium onTheory ofComputing, pages 50–58, 2003. 1, 2, 3, 5\n[DMVW23] Sami Davies, Benjamin Moseley, Sergei Vassilvitskii, and Yuyan Wang. Predictive\nflows for faster ford-fulkerson. In International Conference onMachine Learning,\nICML, volume 202, pages 7231–7248, 2023. 3\n[DSS24] Andrew Draganov, David Saulpic, and Chris Schwiegelshohn. Settling time vs.\naccuracy tradeoffs for clustering big data. Proc. ACM Manag. Data , 2(3):173, 2024. 21,\n29\n[EFS+22] Jon C. Ergun, Zhili Feng, Sandeep Silwal, David P . Woodruff, and Samson Zhou.\nLearning-augmented k-means clustering. In The Tenth International Conference on\nLearning Representations, ICLR, 2022. 3, 4, 6\n[EIO02] Lars Engebretsen, Piotr Indyk, and Ryan O’Donnell. Derandomized dimensionality\nreduction with applications. In David Eppstein, editor, Proceedings oftheThirteenth\nAnnual ACM-SIAM Symposium onDiscrete Algorithms, pages 705–712, 2002. 14\n– 40 –\n\n[Fei98] Uriel Feige. A threshold of ln nfor approximating set cover. J.ACM , 45(4):634–652,\n1998. 9\n[FS12] Dan Feldman and Leonard J. Schulman. Data reduction for weighted and outlier-\nresistant clustering. In Yuval Rabani, editor, Proceedings oftheTwenty-Third Annual\nACM-SIAM Symposium onDiscrete Algorithms, SODA 2012, Kyoto, Japan, January\n17-19, 2012, pages 1343–1354. SIAM, 2012. 8\n[FSS20] Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data:\nConstant-size coresets for k-means, pca, and projective clustering. SIAM J.Comput. ,\n49(3):601–657, 2020. 7\n[GH98] Nili Guttmann-Beck and Refael Hassin. Approximation algorithms for min-sum\np-clustering. Discret. Appl. Math., 89(1-3):125–142, 1998. 1, 2, 7\n[GI03] Venkatesan Guruswami and Piotr Indyk. Embeddings and non-approximability of ge-\nometric problems. In Proceedings oftheFourteenth Annual ACM-SIAM Symposium\nonDiscrete Algorithms, pages 537–538, 2003. 1\n[GLS+22] Elena Grigorescu, Young-San Lin, Sandeep Silwal, Maoyuan Song, and Samson Zhou.\nLearning-augmented algorithms for online linear and semidefinite programming.\nInAdvances inNeural Information Processing Systems 35:Annual Conference on\nNeural Information Processing Systems, NeurIPS, 2022. 3\n[GP19] Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy\nwith expert advice. In Proceedings ofthe36th International Conference onMachine\nLearning, ICML, pages 2319–2327, 2019. 3\n[HIKV19] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency\nestimation algorithms. In 7thInternational Conference onLearning Representations,\nICLR, 2019. 3\n[HLW24] Lingxiao Huang, Jian Li, and Xuan Wu. On optimal coreset construction for euclidean\n(k, z)-clustering. In Proceedings ofthe56th Annual ACM Symposium onTheory of\nComputing, STOC, pages 1594–1604, 2024. 7\n[HO10] Refael Hassin and Einat Or. Min sum clustering with penalties. Eur. J.Oper. Res.,\n206(3):547–554, 2010. 7\n[HV20] Lingxiao Huang and Nisheeth K. Vishnoi. Coresets for clustering in euclidean spaces:\nimportance sampling is nearly optimal. In Proceedings ofthe52nd Annual ACM\nSIGACT Symposium onTheory ofComputing, STOC, pages 1416–1429, 2020. 7\n[IKI94] Mary Inaba, Naoki Katoh, and Hiroshi Imai. Applications of weighted voronoi\ndiagrams and randomization to variance-based k-clustering (extended abstract). In\nProceedings oftheTenth Annual Symposium onComputational Geometry , pages\n332–339, 1994. 2, 6, 17, 31\n– 41 –\n\n[IKQP21] Sungjin Im, Ravi Kumar, Mahshid Montazer Qaem, and Manish Purohit. On-\nline knapsack with frequency predictions. In Advances inNeural Information\nProcessing Systems 34: Annual Conference onNeural Information Processing\nSystems, NeurIPS, pages 2733–2743, 2021. 3\n[Ind99] Piotr Indyk. A sublinear time approximation scheme for clustering in metric spaces.\nIn40th Annual Symposium onFoundations ofComputer Science, FOCS , pages 154–\n159, 1999. 1, 2, 3, 5\n[IVY19] Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations.\nInAdvances inNeural Information Processing Systems 32:Annual Conference on\nNeural Information Processing Systems 2019, NeurIPS, pages 7400–7410, 2019. 3\n[JLL+20] Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P . Woodruff. Learning-\naugmented data stream algorithms. In 8thInternational Conference onLearning\nRepresentations, ICLR, 2020. 3\n[JLL+22] Shaofeng H.-C. Jiang, Erzhi Liu, You Lyu, Zhihao Gavin Tang, and Yubo Zhang.\nOnline facility location with predictions. In The Tenth International Conference on\nLearning Representations, ICLR, 2022. 3\n[JLS23] Arun Jambulapati, Yang P . Liu, and Aaron Sidford. Chaining, group leverage score\noverestimates, and fast spectral hypergraph sparsification. In Barna Saha and Rocco A.\nServedio, editors, Proceedings ofthe55th Annual ACM Symposium onTheory of\nComputing, STOC 2023, Orlando, FL,USA, June 20-23, 2023 , pages 196–206. ACM,\n2023. 2\n[JMF99] Anil K Jain, M Narasimha Murty, and Patrick J Flynn. Data clustering: a review. ACM\ncomputing surveys (CSUR), 31(3):264–323, 1999. 1\n[JSWZ21] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm\nfor solving general lps. In STOC ’21: 53rd Annual ACM SIGACT Symposium on\nTheory ofComputing, pages 823–832, 2021. 7, 37\n[Kar84] Narendra Karmarkar. A new polynomial-time algorithm for linear programming.\nComb., 4(4):373–396, 1984. 7, 37\n[KBC+18] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case\nfor learned index structures. In Proceedings ofthe2018 International Conference on\nManagement ofData, SIGMOD Conference, pages 489–504, 2018. 3\n[KBTV22] Misha Khodak, Maria-Florina Balcan, Ameet Talwalkar, and Sergei Vassilvitskii.\nLearning predictions for algorithms with predictions. In Advances inNeural\nInformation Processing Systems 35: Annual Conference onNeural Information\nProcessing Systems, NeurIPS, 2022. 3\n[Kho02] Subhash Khot. Hardness results for coloring 3 -colorable 3 -uniform hypergraphs. In\n43rd Symposium onFoundations ofComputer Science (FOCS), Proceedings , pages\n23–32, 2002. 5, 14\n– 42 –\n\n[KKLP97] Viggo Kann, Sanjeev Khanna, Jens Lagergren, and Alessandro Panconesi. On the\nhardness of approximating max k-cut and its dual. Chic. J.Theor. Comput. Sci., 1997.\n7\n[Kle02] Jon M. Kleinberg. An impossibility theorem for clustering. In Advances inNeural\nInformation Processing Systems 15[Neural Information Processing Systems, NIPS ,\npages 446–453, 2002. 1\n[Lee23] James R. Lee. Spectral hypergraph sparsification via chaining. In Barna Saha and\nRocco A. Servedio, editors, Proceedings ofthe55th Annual ACM Symposium on\nTheory ofComputing, STOC 2023, Orlando, FL,USA, June 20-23, 2023 , pages 207–\n218. ACM, 2023. 2\n[LLL+23] Yi Li, Honghao Lin, Simin Liu, Ali Vakilian, and David P . Woodruff. Learning\nthe positions in countsketch. In The Eleventh International Conference onLearning\nRepresentations, ICLR, 2023. 3\n[LLMV20] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online\nscheduling via learned weights. In Proceedings ofthe2020 ACM-SIAM Symposium\nonDiscrete Algorithms, SODA, pages 1859–1877, 2020. 3\n[LLW22] Honghao Lin, Tian Luo, and David P . Woodruff. Learning augmented binary search\ntrees. In International Conference onMachine Learning, ICML , pages 13431–13440,\n2022. 3\n[LS15] Yin Tat Lee and Aaron Sidford. Efficient inverse maintenance and faster algorithms for\nlinear programming. In IEEE 56th Annual Symposium onFoundations ofComputer\nScience, FOCS, pages 230–249, 2015. 7, 37\n[LSW17] Euiwoong Lee, Melanie Schmidt, and John Wright. Improved and simplified inap-\nproximability for k-means. Inf.Process. Lett., 120:40–43, 2017. 2\n[LSZ19] Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the\ncurrent matrix multiplication time. In Conference onLearning Theory, COLT , pages\n2140–2157, 2019. 7, 37\n[LV21] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned\nadvice. J.ACM, 68(4):24:1–24:25, 2021. 3\n[Mat00] Jirí Matousek. On approximate geometric k-clustering. Discret. Comput. Geom. ,\n24(1):61–84, 2000. 1, 2, 3, 5\n[Mit18] Michael Mitzenmacher. A model for learned bloom filters and optimizing by\nsandwiching. In Advances inNeural Information Processing Systems 31:Annual\nConference onNeural Information Processing Systems, NeurIPS , pages 462–471,\n2018. 3\n[MNV12] Meena Mahajan, Prajakta Nimbhorkar, and Kasturi R. Varadarajan. The planar\nk-means problem is np-hard. Theor. Comput. Sci., 442:13–21, 2012. 1\n– 43 –\n\n[MV20] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. In Tim\nRoughgarden, editor, Beyond theWorst-Case Analysis ofAlgorithms , pages 646–662.\nCambridge University Press, 2020. 3\n[NCN23] Thy Dinh Nguyen, Anamay Chaturvedi, and Huy L. Nguyen. Improved learning-\naugmented algorithms for k-means and k-medians clustering. In The Eleventh\nInternational Conference onLearning Representations, ICLR, 2023. 3, 4, 6, 32, 33\n[NRS24] Ismail Naderi, Mohsen Rezapour, and Mohammad R Salavatipour. Approximation\nschemes for min-sum k-clustering. Discrete Optimization, 54:100860, 2024. 7\n[PSK18] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via\nML predictions. In Advances inNeural Information Processing Systems 31:Annual\nConference onNeural Information Processing Systems 2018, NeurIPS , pages 9684–\n9693, 2018. 3\n[PY91] Christos H. Papadimitriou and Mihalis Yannakakis. Optimization, approximation,\nand complexity classes. J.Comput. Syst. Sci., 43(3):425–440, 1991. 7\n[Sch98] Alexander Schrijver. Theory oflinear and integer programming . John Wiley & Sons,\n1998. 37\n[Sch00] Leonard J. Schulman. Clustering for edge-cost minimization (extended abstract).\nInProceedings ofthe Thirty-Second Annual ACM Symposium onTheory of\nComputing, pages 547–555, 2000. 1, 2, 3, 5\n[SG76] Sartaj Sahni and Teofilo F. Gonzalez. P-complete approximation problems. J.ACM ,\n23(3):555–565, 1976. 7\n[SLLA23] Yongho Shin, Changyeol Lee, Gukryeol Lee, and Hyung-Chan An. Improved learning-\naugmented algorithms for the multi-option ski rental problem via best-possible com-\npetitive analysis. In International Conference onMachine Learning, ICML , pages\n31539–31561, 2023. 3\n[SZS+14] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,\nIan J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In\n2nd International Conference onLearning Representations, ICLR, Conference Track\nProceedings, 2014. 3\n[Vai89] Pravin M. Vaidya. Speeding-up linear programming using fast matrix multiplication.\nIn30th Annual Symposium onFoundations ofComputer Science , pages 332–337,\n1989. 7, 37\n[Vai90] Pravin M. Vaidya. An algorithm for linear programming which requires o(((m+n)n2\n+ (m+n)1.5n)l) arithmetic operations. Math. Program., 47:175–201, 1990. 7, 37\n[WLW20] Shufan Wang, Jian Li, and Shiqiang Wang. Online algorithms for multi-shop ski rental\nwith machine learned advice. In Advances inNeural Information Processing Systems\n33:Annual Conference onNeural Information Processing Systems, NeurIPS , 2020. 3\n– 44 –\n\n[WZ20] Alexander Wei and Fred Zhang. Optimal robustness-consistency trade-offs\nfor learning-augmented online algorithms. In Advances inNeural Information\nProcessing Systems 33: Annual Conference onNeural Information Processing\nSystems, NeurIPS, 2020. 3\n[WZZ23] David P . Woodruff, Peilin Zhong, and Samson Zhou. Near-optimal k-clustering in the\nsliding window model. In Advances inNeural Information Processing Systems 36:\nAnnual Conference onNeural Information Processing Systems, NeurIPS, 2023. 7\n[XW05] Rui Xu and Donald Wunsch. Survey of clustering algorithms. IEEE Transactions on\nneural networks, 16(3):645–678, 2005. 1\n[ZTHH24] Xiaoyi Zhu, Yuxiang Tian, Lingxiao Huang, and Zengfeng Huang. Space com-\nplexity of euclidean clustering. In 40th International Symposium onComputational\nGeometry, SoCG, pages 82:1–82:16, 2024. 7\n– 45 –",
  "textLength": 110595
}