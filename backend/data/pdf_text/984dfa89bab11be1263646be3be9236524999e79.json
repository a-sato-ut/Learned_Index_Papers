{
  "paperId": "984dfa89bab11be1263646be3be9236524999e79",
  "title": "Bounding the Last Mile: Efficient Learned String Indexing",
  "pdfPath": "984dfa89bab11be1263646be3be9236524999e79.pdf",
  "text": "Bounding the Last Mile: Efﬁcient Learned String Indexing\n(Extended Abstracts)\nBenjamin Spector\nMIT CSAIL\nspectorb@mit.eduAndreas Kipf\nMIT CSAIL\nkipf@mit.eduKapil Vaidya\nMIT CSAIL\nkapilv@mit.edu\nChi Wang\nMicrosoft Research\nwang.chi@microsoft.comUmar Farooq Minhas\nMicrosoft Research\nufminhas@microsoft.comTim Kraska\nMIT CSAIL\nkraska@mit.edu\nABSTRACT\nWe introduce the RadixStringSpline (RSS) learned index\nstructure for e\u000eciently indexing strings. RSS is a tree of\nradix splines each indexing a \fxed number of bytes. RSS\napproaches or exceeds the performance of traditional string\nindexes while using 7-70 \u0002less memory. RSS achieves this\nby using the minimal string pre\fx to su\u000eciently distinguish\nthe data unlike most learned approaches which index the en-\ntire string. Additionally, the bounded-error nature of RSS\naccelerates the last mile search and also enables a memory-\ne\u000ecient hash-table lookup accelerator. We benchmark RSS\non several real-world string datasets against ART and HOT.\nOur experiments suggest this line of research may be promis-\ning for future memory-intensive database applications.\nAIDB Workshop Reference Format:\nBenjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar\nFarooq Minhas, Tim Kraska. Bounding the Last Mile: E\u000ecient\nLearned String Indexing. AIDB 2021.\n1. INTRODUCTION\nLearned indexing as introduced by [13] has brought new\nperspective to indexing by reframing it as a cumulative dis-\ntribution function (CDF) modeling problem. The burgeon-\ning \feld, despite its nascence, has brought with it many op-\nportunities and e\u000eciencies. However, most work in this area\nhas focused on e\u000eciently indexing numerical keys. There\nare several reasons why building learned indexes for strings\nis usually harder than for numerical keys:\n\u000fStrings are variable-length, complicating both model\ninference and memory layout.\n\u000fStrings are much larger objects which are expensive to\nstore, compare, and manipulate.\nThis article is published under a Creative Commons Attribution License\n(http://creativecommons.org/licenses/by/3.0/), which permits distribution\nand reproduction in any medium as well allowing derivative works, pro-\nvided that you attribute the original work to the author(s) and AIDB 2021.\n3rd International Workshop on Applied AI for Database Systems and Appli-\ncations (AIDB’21), August 20, 2021, Copenhagen, Denmark.\u000fStrings tend to have even worse distributional proper-\nties than integer keys due to the prevalence of common\npre\fxes and substrings, making them exceedingly hard\nto model accurately and compactly.\nWe also distinguish between two indexing cases: (1) pri-\nmary index: the data is sorted according to the key and\nthe index is used to more e\u000eciently \fnd the data inside\nthe sorted data, and (2) secondary index: the data itself\nis not sorted and the index has to store pairs of keys and\ntuple identi\fers (TIDs) in the leaf nodes. Commonly-used\nstring-key secondary index structures include B-trees and\ntheir variants [7], ART [14], HOT [5], and the highly space-\ne\u000ecient FST [19]; learned indexes are absent. Recent learned\nprimary index structures include SIndex [18] (also operating\non strings), ALEX [9] and Hist-Tree [8].\nIn our view, the most signi\fcant problem with learned in-\ndexes for strings is the last-mile search which corrects the\nlearned model's prediction. Last-mile search is usually im-\nplemented as an exponential search around the prediction\nwhich expands out until a bound is found, followed by a bi-\nnary search inwards to \fnd the true index. This search is es-\npecially expensive in string scenarios for two reasons. First,\naverage model error in these scenarios is often quite high,\ndue to di\u000eculty in modeling real-world datasets. Because\nmany real-world datasets have both long shared pre\fxes and\nrelatively low discriminative content per byte, the CDF ap-\npears to be almost step-wise from afar, which is di\u000ecult for\ntraditional learned models to accurately predict or capture.\nSecond, actually conducting the last mile search is slow.\nEach comparison is expensive and requires potentially many\nsequential operations, and the strings' large size decreases\nthe number of keys which \ft in cache. Modern column\nstores typically store strings in \fxed length sections (\frst\nfew bytes) and variable length sections (remainder of the\nstring) [16]. Increasing the size of the \fxed-length section\ncan waste memory, and searching the variable length section\nwill incur expensive cache misses. One clear improvement,\nwhich forms the basis of this research, comes from having\nbounded error (i.e., the model outputs a bounded interval\nin which the sought key will lie, if present). Then, one can\nreplace the exponential search with binary search and skip\nmany string comparisons. We use this to ameliorate the cost\nof the last-mile search for lower bound lookups (i.e., \fnding\n1arXiv:2111.14905v1  [cs.DB]  29 Nov 2021\n\nthe key that is equal to or larger than the lookup key), and\nthen, for equality lookups, we bypass it (more later).\nIn this work, we emphasize increasing the lookup perfor-\nmance and decreasing the size for the primary-index sce-\nnario. For example, such an index could be used for global\ndictionary encoding [6]. We introduce the RadixStringSpline\n(RSS), a learned string index consisting of a tree of the\nlearned index structure RadixSpline (RS) [12, 11, 15]. RS\nis a learned index that consists of an error-bounded spline\nwhich is in turn indexed in a radix lookup table. Similar to\nnodes in ART, each RSS node indexes a \fxed partial key\n(e.g., 8 bytes). But unlike ART, our nodes may have ex-\ntremely high fan-out due to the incorporation of a learned-\nmodel at each node. To increase the fan-out of a given\nbyte-pre\fx, we encode the input data using HOPE [20].\nHOPE eliminates common pre\fxes and increases informa-\ntion density. Like the original string data, HOPE-encoded\ndata has variable length. For a popular URL dataset, HOPE\nencoded-data is on average 1.6 \u0002smaller than the original\nstring data. Often, the \frst 8 bytes of the HOPE encod-\ning are su\u000ecient to distinguish between most of the string\nkeys. RSS only requires a single node to index these keys.\nHowever, typically there also are many collisions among the\n8-byte pre\fxes, which requires RSS to recurse on the follow-\ning 8 bytes.\nWhile we do not discuss updates, techniques as proposed\nin ALEX [9] are generally applicable to our approach.\nCompared to ART and HOT, RSS is faster to build, simi-\nlarly fast to query, and consumes much less memory (7-70 \u0002).\nRSS particularly bene\fts from a high information density in\nthe most signi\fcant bytes. Some data distributions, like the\nTwitter Semantic140 dataset, satisfy this well, and corre-\nspondingly RSS has high performance. For other data dis-\ntributions (e.g., URLs) that require many low-information\nbytes to distinguish keys, RSS needs many levels and hence\nmay have low performance.\n2. SPLINING STRINGS\nProblem Statement. Given an immutable, lexicographi-\ncally sorted array of strings, we want to build an index on top\nwhich supports two key operations. First, the index should\nsupport equality lookups: if the string is present, return its\nindex, and if not, return NULL. Second, the index must sup-\nport lower bound queries: given a string, \fnd the index of\nthe greatest string greater than or equal to the provided one.\nThese two operations are important in column stores that\nuse dictionary encoding. For equality queries (i.e., WHERE\nstr = X) one needs to support equality lookups on the dic-\ntionary, and for pre\fx queries (i.e., WHERE str LIKE 'A%')\nlower bound lookups are required (to \fnd the \frst string in\nthe dictionary that is lexicographically greater than or equal\nto 'A').\nMethod. Our method consists of two parts. First, we de-\nscribe the RadixStringSpline (RSS), a compact and e\u000ecient\nadaptation of RadixSpline to the string domain [12]. This\napproach provides both fast queries as well as bounded error.\nSecond, we provide an optional add-on hash corrector which\nmakes use of this bounded error to, at the cost of 12 bits\nper element, further improve equality lookup performance.\nRadixStringSpline. One useful perspective of the order-\npreserving indexing problem is that of a compressive map-\nping. If a million keys use a 64 bit integer type, the overalldensity of information is quite low relative to the capacity\nof the type. Indexing transforms these million keys into a\ncontinuous range, e\u000bectively compacting the data into the\nlast 20 bits.\nFigure 1: A sample RSS tree structure, for the toy\ndata and settings indicated. The root node corre-\nsponds to the entire data (bounds [0,8]) and indexes\nthe \frst two bytes of the data (K=2). The RadixS-\npline in the root indexes the only two keys which do\nnot have collisions in the \frst two bytes (bc and ef).\nThe remaining two-byte pre\fxes (ab and cd) have\ncollisions and are redirected to child nodes.\nThe core di\u000berence between string indexing and integer in-\ndexing is that integer keys have much higher entropy in the\n\frst few bytes than strings do because integer keys must be\nentirely distinct in this range. By contrast, string keys can\n(and in most practical applications frequently) share long\npre\fxes, requiring examination of more data to fully distin-\nguish them. There are several (not necessarily exclusive)\napproaches one might take to ameliorate this. For example,\none can actually directly compact the data, as per [20], and\nthis certainly does help. In our approach, our goal is to have\nthe model quickly operate on the minimum amount of data\nto get the job done.\nAn RSS is a tree, in which each node contains:\n\u000fThe bounds of the range over which it operates.\n\u000fA redirector map. This contains the keys for which\nthe current node cannot satisfy the error bound, and\npointers to context-aware nodes for each of those keys.\n\u000fA RadixSpline model operating on K bytes, with an\nerror-bound of E.\nWe illustrate a sample RSS in Figure 1. The RSS indexes\nthe indicated data in the bottom left from 0 to 8, with each\n2\n\nnode operating on two-byte chunks and a maximum allow-\nable error of 0. (We note that practical RSS's usually have\na greater allowable error but these are harder to intuitively\nillustrate.) All searches begin at the root node (top) and si-\nmultaneously traverse the tree and the string until the par-\ntial key can no longer be found in the redirector array. At\nthat point, the local RadixSpline at that node is guaranteed\nto provide a bounded-error prediction.\nTo build an RSS, one begins by building a RadixSpline\non the \frst K bytes of every string in the dataset (this is\nthe root node), including all duplicates. Then, we iterate\nthrough the unique K-byte pre\fxes, and check if the es-\ntimated position is within the prescribed error bounds for\nboth the \frst and last appearance of the pre\fx. This will\nalways be the case for unique pre\fxes but might not be for\npre\fxes which have duplicates, and cannot be the case for\npre\fxes which have >2Eduplicates { then, even predict-\ning the median instance can't satisfy the extrema. For each\npre\fx which fails the test, we add it to the redirector table,\nand build a new RSS over just the range of the problematic\npre\fx and starting at byte K instead of byte 0. This process\ncontinues recursively until every key is satis\fed.\nWe note that the radix table in the RadixSpline should\nbe adjusted depending on where in the tree one is. Near\nthe root, the radix table should be large; near the leaves we\noften use just 6 bits to save memory.\nPractically we have found K=8 or K=16 and E=127 to\nbe good settings; in our experiments we use K=16 (via gcc's\nbuiltin uint128 t type) since it is more robust to sparser\ndata. For simplicity we keep these parameters \fxed; prag-\nmatically there are probably reasonable improvements to be\nhad in both memory and query time by switching to K=8\nfor smaller, easier-to-model datasets. Note that a fanout\nspanning 16 bytes is generally far higher than the maximum\nfanout of ART (1 byte, i.e., 256 keys, per level) or HOT (32\nkeys per level).\nQuerying an RSS is simple. One begins by extracting the\n\frst K bytes of the string, and conducting a binary search\nof the redirector to try to \fnd the pre\fx. If it is found,\nthen one follows the redirect to the new RSS node and the\nprocess begins again, only operating on the next K bytes.\nIf it is not found in the redirector, then that means the\nkey is guaranteed to be within acceptable bounds for the\nRadixSpline, so one queries the RadixSpline at the current\nnode with the appropriate substring and returns the result.\nAs an example, suppose we want to query the string \\cdeg\"\nfrom the RSS in Figure 1. We begin by extracting the \frst\ntwo bytes of the string, in this case \\cd\", which are packed\ninto a 16-bit integer. We then search the redirector of the\nroot node for this key. Since we \fnd it in the second slot,\nwe follow the pointer at that slot to the next node of the\nRSS. Now, we extract the next two bytes, \\eg\" and check\nthe node's redirector. This time, we fail to \fnd it, so we\nknow it is correctly indexed by the local RadixSpline. So,\nwe query the RadixSpline and return the result as our pre-\ndiction. Finally, we execute a local binary search in the data\nto either \fnd the string and its index or else to validate its\nnon-existence.\nAnalogously, suppose we wanted to conduct a lower bound\nquery for the string \\defg\", not found in the data. We again\nsearch the root node's redirector for \\de\", and, failing to\n\fnd it, execute the root's RadixSpline on that pre\fx. We\nagain perform a binary search, only this time after failing to\fnd it we return the left bound.\nOne additional bene\ft of our approach is that if it is ap-\npropriately constructed (requiring a synchronization of the\npredictions of di\u000berent layers of the tree) it can also be made\nperfectly monotonic, which may prove useful in future work\non accelerating the last mile search.\nWe believe that one should not undervalue the importance\nof the model being error-bounded for two reasons. First,\nin a string setting, even with relatively low errors and an\noptimized last-mile search, the last mile search still turns out\nto be the dominant cost. A bounded error means one only\nneeds to conduct a binary search rather than an exponential\nsearch. Second, it enables a memory-e\u000ecient hash corrector,\nto be described below.\nHash Corrector. Often, while index structures certainly\nneed to support lower bound queries as described above,\nthe optimization of the actual direct lookup of known string\nkeys is equally important. To this end, we provide an auxil-\niary data structure which can improve performance for this\nproblem at the cost of a small amount of additional mem-\nory. Essentially, one stores a contiguous array of signed int8\no\u000bsets, with -128 reserved as empty. To build the HC, for\neach string in the dataset one runs the RSS and computes\nthe di\u000berence between the predicted and true values, which\nis guaranteed to \ft in the range -127 to 127. Then, one\nhashes the string into these slots several times (up to a pre-\ndetermined number) to \fnd an empty slot. If one is found,\nwe insert the o\u000bset at that slot. Compared to traditional\nCuckoo hashing, this technique trades false positives (i.e.,\nthe string at the o\u000bset does not match the lookup key) for\nmemory e\u000eciency. When one queries a string, one again\nhashes the string to a few of these slots and tries each o\u000bset.\nIf it's a match, the expensive binary search is avoided. Oth-\nerwise, the bounded binary search is a reasonable fall-back\n(this is also required for negative lookups, although we deem\nthem to be less important in practice (e.g., in the dictionary\nencoding scenario). To have some bene\ft from false positive\nlookups, our implementation uses the keys it \fnds at these\no\u000bsets to at least reduce the bounds of the binary search,\nand these reduced bounds can also help us rapidly reject\nwrong o\u000bsets (which are out of the current bounds). So,\neach query to the underlying data is guaranteed to provide\nat least some bene\ft.\nIn our implementation, we use a 128-bit MurmurHash3\nhash [4], [17] giving us 4 attempts, and we set the load factor\nto be 2/3. This then provides a speed boost to >95% of\nlookup queries at the cost of 12 bits per key. Further tuning\nthis time-space trade-o\u000b might lead to additional gains.\nFor example, suppose we want to look up a string S in\na database of N strings. We \frst execute the RSS to get\nan error-bounded index prediction p. We then hash S into\n4 positions in range [0, 3N/2) { h1,h2,h3,h4. For each\nposition hi, if o\u000bsets[ hi] == -128 or exceeds the bounds, we\nimmediately know it is invalid and skip it. Otherwise, we\ncompare S to the string at p+o\u000bsets[ hi]. If they match, we\nreturn; if S is larger, we set the location as a left bound, and\nif S is smaller we set the location as a right bound. Finally,\nif we try four times and still have not found it, we execute a\nbinary search between the left and right bounds. The data\nstructure can and should be entirely ignored for lower bound\nqueries; it does not currently accelerate them.\n3\n\nBuild, ns/item Wiki Twitter Examiner URL\nART 131 147 143 197\nHOT 207 209 217 243\nRSS 42 41 37 94\nRSS+HC 171 200 184 383\nLookup, ns\n(LowerBound)Wiki Twitter Examiner URL\nART 785 530 666 1592\nHOT 494 (584)365(472) 394(514)786(920)\nRSS 629 452 554 1733\nRSS+HC 477(629) 378 (452)427 (554) 1314 (1733)\nMemory, MB Wiki Twitter Examiner URL\nART 1219.8 223.5 374.7 15,573.0\nHOT 205.9 24.7 48.3 1,372.3\nRSS 3.6 1.1 1.6 198.8\nRSS+HC 23.8 3.4 6.1 350.7\nTable 1: Results for ART, HOT, and the\nRadixStringSpline with and without its add-on Hash\nCorrector. Note that for the second sub-table, par-\nenthetical values are for lower bound queries if they\nare appreciably di\u000berent from lookup queries.\n3. EVALUATION\nWe evaluate RSS against ART and HOT as baselines on\nfour datasets:\n\u000fWiki: >13M unique Wikipedia URL tails. [1]\n\u000fTwitterSentiment: 1.6M tweets, meant to provide rep-\nresentative natural language. [10]\n\u000fExaminer: 3M headlines from the Examiner. [3]\n\u000fURL: Approximately 100M URLs from a 2007 web\ncrawl; approximately 10GB total. [2]\nWe summarize the results in Table 1. The RSS is gener-\nally faster than ART but slower than HOT, but far smaller\n(7-70\u0002) than either of them. With its hash corrector, lookup\nspeed is usually comparable to HOT and the data structure\nremains smaller, although memory usage increases some-\nwhat. The RSS is also extremely fast to construct compared\nto existing structures, with a speed boost of 2-3 \u0002, although\nthis is at the expense of not supporting inserts. However,\nthe fast construction time emphasizes that RSS is partic-\nularly useful for bulk-loading and delta-updates. We take\nnote of RSS's poor performance on the URL dataset; we\ndiscuss this below.\nThe performance character of RSS is distinguished from\ntraditional trie-like data structures in that its compound\nnodes have unlimited fanout; what decides the cost of the\nmodel is the depth of the data required, since each inner\nnode requires another local redirector query to \fnd a new\nnode with the additional context. To this end, the URL\ndataset is practically an adversarial scenario (as would be\na \flesystem) { virtually all strings share a relatively small\nnumber of long pre\fxes, which lowers the discriminatory\ncapacity of the local spline. To validate this understanding,\nwe run the same datasets, only this time encoded by HOPE's\ntwo-gram approach in order to localize more data at the\nstart of the model. The result is a considerable improvement\nin performance and usually also memory, as can be seen in\nTable 2. With more aggressive compression schemes, this\nwould likely improve further.1\n1For the sake of completeness, we would have liked to bench-\nmark ART and HOT too on these compressed datasets, butBuild, ns/item Wiki Twitter Examiner URL\nRSS 44 41 42 67\nRSS+HC 152 163 163 317\nLookup, ns\n(LowerBound)Wiki Twitter Examiner URL\nRSS 513 406 482 1428\nRSS+HC 375 (513) 292 (406) 351 (482) 1095 (1428)\nMemory, MB Wiki Twitter Examiner URL\nRSS 2.6 1.4 1.6 123.0\nRSS+HC 22.8 3.7 6.1 278.8\nTable 2: Results for the RadixStringSpline with and\nwithout its add-on Hash Corrector, operating on\nHOPE-compressed datasets.\nWe also wish to note that these comparisons, while accu-\nrate to the current state-of-the-art, also take our competitors\nsomewhat out of their element. Both ART and HOT are de-\nsigned as secondary indexes, storing considerable additional\ninformation in leaf nodes which are not used in this primary\nindex / dictionary encoding scenario. Consequently, it is\nprobably possible to streamline both of these data struc-\ntures for the reduced task at hand. We suspect that doing\nthis would yield a reasonable improvement in memory con-\nsumption, but probably not one of the same order as can be\nprovided by a learned model leveraging last-mile search on\nthe underlying data.\n4. CONCLUSIONS\nIn this work, we have introduced a novel learned string\nindex for the purposes of primary indexing or dictionary en-\ncoding which is competitive with existing string index struc-\ntures in speed and superior in size. We evaluated the learned\nindex on datasets varied in both distribution and size, and\nfound that our method works especially well in conjunction\nwith string compression schemes.\nFuture Work. There exist many interesting future direc-\ntions: First, better compression techniques could further\nimprove the performance of both RSS and other index struc-\ntures. Second, the internal redirector of RSS could be im-\nproved to be more e\u000ecient for large datasets with common\npre\fxes (like URL). Third, RSS currently only uses splines\nas the main model. However, other types of models could\nprovide signi\fcant bene\fts. In fact, one might see the tree\nand redirector structure of RSS as a new model for generat-\ning error-bounded models out of unbounded components.2\nConsequently, this general approach might also be applied to\nnumerical keys to achieve greater space-e\u000eciency. Finally,\nwe also believe there is likely considerable further tuning\n(and auto-tuning) which could be done on RSS to further\nimprove performance.\nAcknowledgments. This research is supported by Google,\nIntel, and Microsoft as part of DSAIL at MIT, and NSF IIS\n1900933. This research was also sponsored by the United\nStates Air Force Research Laboratory and the United States\nwe were unable to easily adapt the libraries for this purpose.\nWe believe our errors were due to HOPE outputting null\ncharacters which interfere with cstring functions in internal\nuse. We do not believe that this is inherently impossible to\n\fx, though.\n2RadixSpline is usually error bounded but is not in this sce-\nnario due to the possibility of many duplicate partial keys.\n4\n\nAir Force Arti\fcial Intelligence Accelerator and was accom-\nplished under Cooperative Agreement Number FA8750-19-\n2-1000. The views and conclusions contained in this docu-\nment are those of the authors and should not be interpreted\nas representing the o\u000ecial policies, either expressed or im-\nplied, of the United States Air Force or the U.S. Govern-\nment. The U.S. Government is authorized to reproduce and\ndistribute reprints for Government purposes notwithstand-\ning any copyright notation herein.\n5. REFERENCES\n[1] English Wikipedia Article Title.\nhttps://dumps.wikimedia.org/enwiki/20190701/\nenwiki-20190701-all-titles-in-ns0.gz .\n[2] 2007. URL Dataset. http://law.di.unimi.it/\nwebdata/uk-2007-05/uk2007-05.urls.gz .\n[3] 202. Examiner Dataset. https:\n//www.kaggle.com/therohk/examine-the-examiner .\n[4] A. Appleby. Murmurhash3, 2012. URL: https://github.\ncom/aappleby/smhasher/blob/master/src/Mur-\nmurHash3.cpp ,\n2012.\n[5] R. Binna, E. Zangerle, M. Pichl, G. Specht, and\nV. Leis. Hot: A height optimized trie index for\nmain-memory database systems. In Proceedings of the\n2018 International Conference on Management of\nData , pages 521{534, 2018.\n[6] C. Binnig, S. Hildenbrand, and F. F arber.\nDictionary-based order-preserving string compression\nfor main memory column stores. In Proceedings of the\n2009 ACM SIGMOD International Conference on\nManagement of data , pages 283{296, 2009.\n[7] D. Comer. Ubiquitous b-tree. ACM Computing\nSurveys (CSUR) , 11(2):121{137, 1979.\n[8] A. Crotty. Hist-tree: Those who ignore it are doomed\nto learn. 2021.\n[9] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y. Li,\nH. Zhang, B. Chandramouli, J. Gehrke, D. Kossmann,\net al. Alex: an updatable adaptive learned index. In\nProceedings of the 2020 ACM SIGMOD International\nConference on Management of Data , pages 969{984,\n2020.\n[10] A. Go, R. Bhayani, and L. Huang. Sentiment140. data\nretrieved from\nhttp://help.sentiment140.com/for-students/ .\n[11] A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann. SOSD: Abenchmark for learned indexes. NeurIPS Workshop on\nMachine Learning for Systems , 2019.\n[12] A. Kipf, R. Marcus, A. van Renen, M. Stoian,\nA. Kemper, T. Kraska, and T. Neumann.\nRadixSpline: a single-pass learned index. In\nR. Bordawekar, O. Shmueli, N. Tatbul, and T. K. Ho,\neditors, Proceedings of the Third International\nWorkshop on Exploiting Arti\fcial Intelligence\nTechniques for Data Management, aiDM@SIGMOD\n2020, Portland, Oregon, USA, June 19, 2020 , pages\n5:1{5:5. ACM, 2020.\n[13] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and\nN. Polyzotis. The case for learned index structures. In\nG. Das, C. M. Jermaine, and P. A. Bernstein, editors,\nProceedings of the 2018 International Conference on\nManagement of Data, SIGMOD Conference 2018,\nHouston, TX, USA, June 10-15, 2018 , pages 489{504.\nACM, 2018.\n[14] V. Leis, A. Kemper, and T. Neumann. The adaptive\nradix tree: Artful indexing for main-memory\ndatabases. In 2013 IEEE 29th International\nConference on Data Engineering (ICDE) , pages\n38{49. IEEE, 2013.\n[15] R. Marcus, A. Kipf, A. van Renen, M. Stoian,\nS. Misra, A. Kemper, T. Neumann, and T. Kraska.\nBenchmarking learned indexes. Proc. VLDB Endow. ,\n14(1):1{13, 2020.\n[16] T. Neumann and M. J. Freitag. Umbra: A disk-based\nsystem with in-memory performance. In 10th\nConference on Innovative Data Systems Research,\nCIDR 2020, Amsterdam, The Netherlands, January\n12-15, 2020, Online Proceedings . www.cidrdb.org,\n2020.\n[17] P. Scott. Murmurhash3.\n[18] Y. Wang, C. Tang, Z. Wang, and H. Chen. Sindex: a\nscalable learned index for string keys. In Proceedings\nof the 11th ACM SIGOPS Asia-Paci\fc Workshop on\nSystems , pages 17{24, 2020.\n[19] H. Zhang, H. Lim, V. Leis, D. G. Andersen,\nM. Kaminsky, K. Keeton, and A. Pavlo. Surf:\nPractical range query \fltering with fast succinct tries.\nInProceedings of the 2018 International Conference\non Management of Data , pages 323{336, 2018.\n[20] H. Zhang, X. Liu, D. G. Andersen, M. Kaminsky,\nK. Keeton, and A. Pavlo. Order-preserving key\ncompression for in-memory search trees. pages\n1601{1615, 2020.\n5",
  "textLength": 26768
}