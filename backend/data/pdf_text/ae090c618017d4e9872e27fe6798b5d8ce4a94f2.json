{
  "paperId": "ae090c618017d4e9872e27fe6798b5d8ce4a94f2",
  "title": "Near-Optimal Bounds for Online Caching with Machine Learned Advice",
  "pdfPath": "ae090c618017d4e9872e27fe6798b5d8ce4a94f2.pdf",
  "text": "arXiv:1910.12172v2  [cs.DS]  29 Oct 2019Near-Optimal Bounds for Online Caching with Machine Learne d\nAdvice\nDhruv Rohatgi\nMIT\ndrohatgi@mit.edu\nOctober 31, 2019\nAbstract\nIn the model of online caching with machine learned advice, introduce d by Lykouris and\nVassilvitskii, the goal is to solve the caching problem with an online algor ithm that has access\nto next-arrival predictions: when each input element arrives, the algorithm is given a prediction\nof the next time when the element will reappear. The traditional mod el for online caching\nsuﬀers from an Ω plogkqcompetitive ratio lower bound (on a cache of size k). In contrast, the\naugmented model admits algorithms which beat this lower bound when the predictions have low\nerror, and asymptotically match the lower bound when the predictio ns have high error, even if\nthe algorithms are oblivious to the prediction error. In particular, L ykouris and Vassilvitskii\nshowed that there is a prediction-augmented caching algorithm with a competitive ratio of\nOp1`minpa\nη{opt,logkqqwhen the overall ℓ1prediction error is bounded by η, andoptis the\ncost of the optimal oﬄine algorithm.\nThe dependence on kin the competitive ratio is optimal, but the dependence on η{optmay\nbe far from optimal. In this work, we make progress towards closing this gap. Our contribu-\ntions are twofold. First, we provide an improved algorithm with a comp etitive ratio of Op1`\nminppη{optq{k,1qlogkq. Second, we provide a lower bound of Ω plogmin ppη{optq{pklogkq,kqq.\n1 Introduction\nIn theonline caching problem (also known as paging), we are given a sequence of elements which\narrive one at a time, and we must maintain a cache of some ﬁxed s izek. The cost of a caching\nalgorithm on some input is the number of cache misses. The sta ndard goal is to design an online\nalgorithm with minimal competitive ratio, relative to the o ptimal oﬄine algorithm.\nAs a fundamental problem in the study of online algorithms, c aching has been extensively stud-\nied [14, 9, 3, 1]; it is well-known that the optimal competiti ve ratio of any deterministic algorithm\nisk, and the optimal competitive ratio of any randomized algori thm is 2Hpkq´1“Θplogkq, where\nHpkqis thek-th harmonic number [1].\nHowever, the traditional framework for analyzing online al gorithms—namely, worst-case com-\npetitive ratios—is overly pessimistic, by virtue of requir ingworst-case analyses. Real-world data of-\ntensatisﬁesniceproperties—itmaybepredictable, orsimp lyrandom, orevenjustnotadversarial—\nand for this reason, theoretically unsound algorithms can p erform very well in practice. Numerous\nattempts have been made to theoretically ground this observ ation; some of the more prominent are\naverage-case analyses [6, 2, 10] and smoothed analyses [15, 16], both within online algorithms and\nbeyond.\n1\n\nOne such attempt which has recently garnered signiﬁcant att ention is the framework of online\nalgorithms with machine learned advice. In this model, the o nline algorithm is augmented with\nan oracle that makes certain predictions about future data. In practice, this oracle is likely to be\na machine learned predictor. Since machine learning is impe rfect (and can sometimes be wildly\nwrong), the algorithm must incorporate the oracle’s advice judiciously, without being given any\nbound on the oracle’s error. The goal is to develop an algorit hm which is both consistent , in that it\nnearly matches the best oﬄine algorithm when the predictor i s nearly perfect, as well as robust, in\nthat its worst-case performance is good even when the oracle is arbitrarily bad. The performance\nof the algorithm should be bounded as a function of some measu re of the oracle error, even though\nthe algorithm is oblivious to this error.\nThe ML advice model has in the past been applied to the ski rent al problem [13, 4], job\nscheduling[13, 12] and onlinerevenue maximization [11]; i t has also been usedto achieve theoretical\nand practical gains in streaming frequency estimation [5] a nd data structures [7]. Most relevant to\nthis paper is prior work by [8] in which it was shown how the mod el can be applied to the online\ncaching problem. In particular, they considered augmentin g caching algorithms with an oracle\nthat predicts the next arrival of each element. The oracle’s ℓ1error is deﬁned as the sum over all\nelements of the absolute diﬀerence between the element’s tru e and predicted next arrival. In this\nmodel, they developed a “predictive marker algorithm” with the following guarantee:\nTheorem 1. [8] The predictive marker algorithm achieves acompetitive r atio ofminp2`4a\nη{opt,4Hpkqq\nwhen the oracle has ℓ1error of at most η, and the cost of the optimal oﬄine algorithm is opt.\nNote that the competitive ratio achieved is Opminpa\nη{opt,logkqq. This ratio is of course\n(asymptotically) optimal as a function of k. However, it is an open question how far the depen-\ndence on η{opt(a measure of relative error, in some sense) can beimproved. In particular, it would\nbeinterestingtounderstandhowaccurate thepredictions n eedtobeinordertoprovideanimprove-\nment in the competitive ratio. From the previous work, it is o nly shown that η{opt “oplog2kq\nsuﬃces.\nIn this paper, we work with the same model, and make progress o n this question. Building\nupon the techniques used in [8], we provide an algorithm with an improved competitive ratio:\nTheorem 2. There is an algorithm for caching with predictions that achie ves a competitive ratio\nof\nOˆ\n1`minˆ\n1,η{opt\nk˙\nlogk˙\nwhen the oracle has ℓ1error of at most η.\nOur bound matches the prior work when η{opt ěk(in that neither algorithm improves upon\nthe classical Oplogkqcompetitive ratio) and is strictly better when η{opt ăk. For example, if\nη{opt “k{logk, then the prior algorithm had a competitive ratio of Oplogkq, whereas we show\nthatOp1qis possible.\nFurthermore, we provide a lower bound, stated informally as follows:\nTheorem 3. Any randomized algorithm for caching with predictions must have a competitive ratio\nwhich is\nΩˆ\nlogminˆη{opt\nklogk,k˙˙\nas a function of η{optandk.\n2\n\nTo our knowledge, this is the only known lower bound. The uppe r bound and lower bound are\nasymptotically tight when η{opt ďk{logkorη{opt ěk1`ǫ. There is a still a signiﬁcant gap;\nthe two bounds are non-trivial on disjoint regimes, and in th e regime kďη{opt ďklogk, neither\nbound is non-trivial.\nNonetheless, these results make progress towards determin ing the largest possible error bound\nthat still admits a non-trivial competitive ratio. Where pr ior work only showed that η{opt “\noplog2kqsuﬃcesto obtain a competitive ratio of oplogkq, theabove resultsimply that η{opt “opkq\nsuﬃces and η{opt ďk1`op1qis necessary.\n1.1 Roadmap\nIn Section 2, we formally describe the online caching model w ith machine learned advice, and\ndeﬁne the predictor. We also review some facts from traditio nal caching algorithms (speciﬁcally,\nfacts about marker-based algorithms) that we will rely on la ter in the paper.\nHaving deﬁned the necessary terminology, we then outline in Section 3 the techniques used to\nachieve our results.\nIn Section 4, we provide the ﬁrst of these results, a warm-up a lgorithm for caching with predic-\ntions. This algorithm improves upon the prior work, and is si mpler than our ﬁnal algorithm (and\nthus may have some practical advantages). Furthermore, as a marker-based algorithm, this ﬁrst\nalgorithm is somewhat simpler to analyze.\nIn Section 5, we describe our second and ﬁnal algorithm, depa rting from the marker-based\nframework to achieve an improvement in the competitive rati o.\nFinally, in Section 6, we prove the lower bound.\n2 Preliminaries\n2.1 Traditional caching\nIn the traditional online caching problem, the input is a seq uenceσ“ pz1,z2,...,znqof elements\nwhich become available one by one. The cache has ﬁxed size kand is initially empty. As elements\narrive, if an element is not present in the cache, then it coun ts as a “cache miss”, and the algorithm\nmust add it to the cache, and choose which cache element to evi ct. Otherwise nothing happens.\nThe cost cost Apσqof the algorithm Aon the input σis the number of cache misses. If the algorithm\nis randomized, this cost is the expected number of cache miss es.\nWe deﬁne optpσqto be the minimum number of caches misses achievable by an “oﬄ ine”\nalgorithm—an algorithm which is given σin advance. Our online algorithm Aisα-competitive if\nthere is some constant csuch that for every input σ,\ncostApσq ďα¨optpσq `c.\nIt is known that there is a k-competitive deterministic algorithm and an Oplogkq-competitive\nrandomized algorithm. Furthermore, these ratios are optim al.\n2.2 ML advice\nIn this paper we consider not the traditional model but rathe r an extension of it, in which our\nalgorithm is also given some advice [8]. In particular, when input element ziarrives, an oracle\ngives the algorithm hipziq, which is an estimate of yi“minjąitj:zj“ziu, the next time when\nelementziwill appear (if zinever appears again, and the input sequence has length n, then we set\n3\n\nyi“n`1). These estimates may not be correct, and we want to bound th e performance of our\nalgorithm as a function of the error. We deﬁne the error as the ℓ1distance between the real and\npredicted next arrivals: for each input element ziwe deﬁne Err ipziq “ |hipziq ´yi|, and then deﬁne\nη“nÿ\ni“1Erripziq.\nFor any time iand input element w, we also deﬁne Lpw,iqto be the last time jăisuch that\nzj“w.\nWhen analyzing an algorithm Ain this model, the goal is to bound its competitive ratio as\na function of η{opt: more precisely, to show for a desired function αand a constant c, that\ncostApσq ďαpη{optq ¨optpσq `cfor every input σ. This is the approach taken in prior work (see\nTheorem 1), and we will see how η{optarises naturally in our algorithms’ analyses. It would not\nmake sense for the competitive ratio to be a function of the ab solute error η, since duplicating the\ninput sequence would double ηbut leave the ratio pcostApσq{optpσqqapproximately unchanged.\n2.3 Marker-based algorithms\nOur ﬁrst predictive caching algorithm will be a marker-based algorithm which judiciously incor-\nporates the oracle’s advice; our second algorithm will depa rt from but still rely heavily on the\nmarker-based framework. Marker-based caching algorithms have the following structure. The exe-\ncution of the algorithm comes in phases, and at the beginning of each phase, every cache element is\nsaid to be unmarked . When a cache hit occurs, the corresponding element is marked. When a cache\nmiss occurs, some unmarked element is evicted from the cache , and the new element is inserted in\nits place, and immediately marked. If all elements of the cac he are marked and another cache miss\noccurs, then the whole cache is unmarked and a new phase begin s.\nThe decision that a marker-based algorithm has to make is whi ch unmarked element to evict\nin the event of a cache miss. In the traditional online model, the randomized marker algorithm\nachieves an Oplogkq-competitive ratio by evicting a random unmarked element. A dditionally, any\nmarker-based algorithm is k-competitive.\nFor any marker-based algorithm, we can make the following de ﬁnition.\nDeﬁnition 4. An input element is called cleanfor some phase rof the algorithm execution if it\nappeared in phase rbut did not appear in phase r´1. If it appeared in both phase rand phase\nr´1, it is called stale.\nThe following lemma is known, relating the number of clean el ements to the optimal oﬄine\ncost.\nLemma 5. [3] LetLbe the number of clean elements in an execution of a marker-ba sed algorithm\non some input σ. ThenL{2ďoptpσq ďL.\nThe phases (and consequently, the clean/stale elements) ar e in fact independent of the exact\nalgorithm.\nDeﬁnition 6. Anarrivalin phase ris an element ziwhich has not previously appeared in the same\nphase.\nThen the following fact can be readily derived for any marker -based algorithm:\nClaim 7. Every phase contains exactly karrivals.\nMore speciﬁcally, each phase continues as long as possible w ithout containing k`1 distinct\nelements.\n4\n\n2.4 Eviction chains\nTo design and analyze marker-based algorithms, it is useful to decompose the set of cache misses\nintoeviction chains , a concept perhaps ﬁrst explicitly utilized in [8].\nFor any marker-based algorithm and any phase, each clean arr ival in the phase causes a cache\nmiss. This yields a chain of evictions in that phase which can be blamed on that clean arrival: the\nclean element’s arrival evicts some element, whose next app earance evicts another element, and so\nforth until an element is evicted which never reappears in th e phase. Each element in the chain\nmust be an arrival, since elements which have previously app eared were marked and are therefore\nimmune to eviction for the remainder of the phase. Thus, each element in the chain after the ﬁrst\nclean element must be stale, since to be evicted it must have b een present in the cache.\nThese clean-element chains account for all cache misses in t he phase, since every stale element\nwas in the cache at the start of the phase, so for it to cause a ca che miss it must have been evicted\nby a previous element. So the total number of cache misses in a phase is simply the total length of\nthe eviction chains.\n3 Our techniques and related work\nGiven next-arrival predictions, an algorithmically naive approach is to trust the predictions com-\npletely. The optimal oﬄine algorithm evicts, at each cache m iss, the cache element with the latest\nnext-arrival time. Thus, if the predictions are perfect the n this approach will have a competitive\nratio of 1. However, even small errors in the predictions can lead to an unbounded prediction. So\nit is necessary to balance trusting the predictions with mak ing provably competitive decisions.\nOur work builds on [8], which proposed a marker-based algori thm for caching with predictions.\nTheir algorithm utilizes eviction chains. Since eviction c hains partition the set of cache misses,\nand the number of eviction chains is equal to the number of cle an arrivals, which is asymptotically\nequal to opt, bounding the average chain length bounds the competitive r atio of the algorithm.\nThus, algorithms which work with each chain independently c an often be cleanly analyzed. In [8],\nthis approach is carried out: for each eviction chain, the pr edictions are trusted (to choose which\nelement to evict next) until the chain reaches length Ω plogkq, after which evictions are random.\nEach chain’s length can be bounded by the prediction error of elements in that chain. Since the\nchains are disjoint, this implies a bound on the total cost of the algorithm by the total prediction\nerror (see Theorem 1).\nAs a warm-up, we ﬁrst show that a small modiﬁcation to the algo rithm from [8] improves the\ncompetitive ratio from Op1`minpa\nη{opt,logkqqtoOp1`minplogpη{optq,logkqq. The modiﬁca-\ntion is simple—trust the predictions only oncein each chain—but the analysis requires more care.\nUnlike before, the length of each chain now depends on the nex t-arrival prediction errors of ele-\nments which may or may not appear in the chain. Thus, adding up the errors could double-count.\nTo avoid this issue, we do not directly bound each chain’s len gth by prediction error. Instead, we\ncharge the length of each chain against a set of inversions in the order of element arrivals relative to\nthe predictions. These sets of inversions are disjoint for d iﬀerent chains, so summing across chains\ndoes not double count. A combinatorial lemma then relates th e total number of inversions to the\ntotal error.\nTo improve the competitive ratio further and achieve the bou nd stated in Theorem 2, we depart\nfrom the marker-based framework. In our previous algorithm , trusting the prediction once meant\nevicting the unmarked element ewith latest predicted next arrival. Intuitively, if the pre diction\nerror “per chain” is Opη{optq, then if ereappears in the same phase, it should on average be at\nmostOpη{optqelements away from the end of the phase. Our ﬁrst algorithm wo uld then proceed\n5\n\nby random eviction of unmarked elements, so e’s eviction chain would have length Oplogpη{optqq\nin expectation (for the same reason that the traditional mar ker algorithm has competitive ratio\nOplogkq).\nHowever, if η{opt !k, then a uniformly random cache element (potentially marked ) will\nprobably not appear after e, so evicting it might terminate the eviction chain at length Op1q\ninstead of Oplogpη{optqq. This is the motivation for our ﬁnal algorithm. However, it r elies on\nevicting marked cache elements, which signiﬁcantly compli cates the analysis, since facts about\nmarker-based algorithms no longer directly apply.\nIn particular, the number of eviction chains may no longer be Θpoptq; evicting a marked\nelement may cause an “extra” eviction chain in the next phase . To deal with this issue, our key\ntool is a bijection from eviction chains to special elements that began the phase in the cache but\nnever appeared. With this bijection, we show that every extr a chain can be charged against either\nprediction error or a chain which does not exist but could have. An added complication is showing\nthat these prediction errors are disjoint.\nFor the lower bound, the idea is to choose an input distributi on and predictions such that the\npredictions give no information about the future input, but have reasonably small error. Gen-\neralizing the traditional lower bound against randomized c aching algorithms—where each input\nelement is uniformly distributed over k`1 pages, so each phase has one clean element—our input\ndistribution consists of phases each with tclean elements, where tis a variable parameter of the\ndistribution. Increasing the parameter tallows smaller prediction error, at the cost of a smaller\nbound on the competitive ratio.\n4 Marker-based predictive algorithm\nOur ﬁrst algorithm lmarker is a modiﬁcation of the algorithm proposed in [8], which is it self a\nbalance between two paradigms: trust the oracle, or ignore t he oracle. A marker-based algorithm\nwhich trusts the oracle would always evict the cache element with highest predicted next arrival;\nif the oracle had zero error, this algorithm would match the o ptimal oﬄine algorithm. A marker-\nbased algorithm which ignores the oracle is the random marke r algorithm. To combine the gains\nof the former algorithm when the oracle has low error with the robustness of the latter algorithm\nwhen the oracle has high error, the algorithm from [8] uses th e following strategy: for each eviction\nchain, trust the oracle until the chain becomes long, and sub sequently ignore the oracle.\nThe modiﬁcation we make is simple: trust the oracle less—onl y once at the beginning of each\neviction chain. Intuitively, a total prediction error of ηtranslates to an error of Θ pη{optqin each of\nthe Θ poptqchains. Evicting the unmarked element with highest predict ed next arrival means that\nits true next arrival should be only Opη{optqfrom the end of the phase, resulting in Oplogη{optq\nmore cache misses for that eviction chain.\nWe now describe lmarker in more detail, and formalize the analysis. The chains canno t quite\nbe analyzed independently (unlike in [8]), so care is needed .\nAlgorithm description lmarker is a marker-based algorithm with the following eviction str at-\negy upon a cache miss: if the incoming element is clean, then e vict the unmarked element with\nthe highest predicted arrival time. If the incoming element is stale, then evict a random unmarked\nelement.\nAlgorithm analysis Fix a phase. Exactly kdistinct elements arrive during the phase; let\ni1,...,ikbe the times of the ﬁrst arrivals for the phase. These are the o nly times when cache\n6\n\nmisses may occur, and the only times when an unmarked element becomes marked.\nConsider a single clean element with arrival time it. It evicts the unmarked element with the\nhighest predicted arrival time, which must be either (1) a st ale element with arrival time in the set\ntit`1,...,ikuor (2) an element which does not appear in this phase. Case (2) results in no more\ncache misses along this chain, so we analyze case (1): some st ale element is evicted. Let ieptqbe\nthe time at which it arrives.\nFor any 1 ďaďbďk, deﬁneNapbqto bethe number of stale elements which are unmarked and\nin the cache at time ia, and have arrival times after ib. LetEtbe the distribution over executions\nof the algorithm up to time t.\nClaim 8. If the clean element arriving at time itevicts the stale element arriving at time ieptq,\nthen the expected length of the eviction chain begun at itis at most OpEr1`logpNtpeptqqqsq.\nProof.Conditioning on Et, the random variable Ntpeptqqis determined. In the worst case, each\nstale element evicts another stale element until this is no l onger possible. There are at most\nNtpeptqqunmarked stale cache elements at time ieptq. Say that there are msuch elements, and\norder them 1 ,...,mby arrival time. If zieptqevicts the j-th such element then there will be at most\nm´junmarked cache elements when that element arrives. But jis uniformly distributed over\nt1,2,...,m u. Thus, the expected length of the chain, conditioned on Et, is bounded by RNtpeptqq\ndeﬁned by the recurrence\nRm“1`1\nmmÿ\nj“1Rm´j,\nwhich solves to Rm“Oplogmq. By the law of total expectation, the unconditional expecte d length\nof the chain is bounded by OpEr1`logpNtpeptqqqsq.\nNext, ﬁx any execution of the algorithm on the entire input. W e relateNtpeptqqto prediction\nerror; more speciﬁcally, we relate it to the number of invers ions in the predicted arrival order of\nstale elements in phase r. For each stale arrival time is, letjsbe the most recent appearance of zis\nin phase r´1. LetJ“ tjs|zisis stale in phase rube the set of most recent appearances.\nIfzieptqis evicted by zit, then at time it, all unmarked stale elements win the cache with\narrivals in the set tieptq`1,...,ikuhad earlier predicted next arrivals than zieptq. Thus, the number\nofuąeptqfor which ziuis stale and hjeptqpzjeptqq ěhjupzjuqis at least Ntpeptqq.\nDeﬁneN“řNtpeptqq, summing over all clean arrival times it. Then the sequence of predicted\nstale arrivals phjpzjqqjPJhas at least Ninversions with respect to the strictly increasing integer\nsequence of actual stale arrivals. It follows from Lemma 11, which we state and prove in the next\nsection, that ÿ\njsPJ|hjspzjsq ´is| ěN{2.\nLetηr´1be the sum of prediction errors over all predictions made in p haser´1. Since Jis a\nset of times in phase r´1, we get from the above inequality that ηr´1ěN{2. On the other hand,\nby Claim 8, the expected number of cache misses in phase ris at most Erř\ntOp1`logNtpeptqqqs,\nsumming over all tsuch that itis a clean arrival time.\nTo simplify notation a bit, for each clean arrival time c“it, deﬁneNc“Ntpeptqq. Then we\nhave shown that 2 ηr´1ěř\ncPrNc, where the sum is over clean arrivals cin phase r. Furthermore,\nwe have shown that\nEr# of cache misses in phase rs ďÿ\ncPrOpEr1`logpNcqsq ďÿ\ncPrOp1`logpErNcsqq\n7\n\nby Jensen’s inequality.\nNow sum over all phases. The number of cache misses is OpL`ř\nclogpErNcsqq, where the sum\nis over the Lclean arrivals in all phases. And we know that 2 ηěř\ncNcfor all executions, implying\nthat 2ηěř\ncErNcs. By another application of Jensen’s inequality, the expect ed number of cache\nmisses can be bounded by OpL`Llogp2η{Lqq.\nSince we also know that Ncďkfor any clean arrival c, we can alternately bound the number\nof cache misses by OpL`Llogkq.\nCombining the two bounds and using the fact that L“Θpoptq, we have proven the following\ntheorem.\nTheorem 9. The algorithm lmarker has competitive ratio Op1`minplogpη{optq,logkqq, where\nηis the unknown ℓ1prediction error.\nLetHpmqbe them-th harmonic number. We can keep track of the exact constants in the\nproof, rather than using asymptotic notation:\nTheorem 10. The algorithm lmarker has competitive ratio 4`2Hpminp2η{opt,kqq, whereηis\nthe unknown ℓ1prediction error.\n4.1 Proof of combinatorial lemma\nLetM“ pm1,...,m nqbe a strictly increasing integer sequence. For any integer s equence A“\npa1,...,anqlet inv pAqbe the number of pairs of indices iăjsuch that aiěaj. Let cost pAq “řn\ni“1|ai´mi|, and deﬁne ∆ pAq “2cost pAq ´invpAq.\nLemma 11. LetA“ pa1,...,anqbe an arbitrary integer sequence. Then invpAq ď2cost pAq, with\ninvpAqandcostpAqas deﬁned above.\nProof.Without loss of generality we can assume that all elements of Aare bounded between m1\nandmn, since outliers can be thresholded without decreasing inv pAqor increasing cost pAq. So the\nset of sequences is ﬁnite. Let B“ pb1,...,bnqbe a sequence which minimizes ∆, and assume that\nBhas the maximum possible sum of elements out of all sequences minimizing ∆.\nSupposethat Bweretohave astrict inversion. Thenthereissome1 ďiănsuchthat biąbi`1.\nDeﬁne two sequences BlandBhso thatbl\ni“bi`1andbh\ni`1“bi, and in all other locations Bland\nBhagree with B. Then by construction,\ninvpBq ´invpBlq “invpBhq ´invpBq.\nFurthermore,\ncostpBq ´costpBlq “ |bi´mi| ´ |bi`1´mi|\ně |bi´mi`1| ´ |bi`1´mi`1|\n“costpBhq ´costpBq\nsincemiămi`1, and the function |bi´x| ´ |bi`1´x|is non-increasing. It follows from the above\ninequalities and the optimality of Bthat\n∆pBhq ´∆pBq ď∆pBq ´∆pBlq ď0.\nTherefore Bhminimizes ∆ as well. But it has a larger sum of elements than B. Contradiction.\nSo we know that Bis non-decreasing, and thus inv pBqis exactly the number of pairs of equal\nelements. But a constant sequence of length lhas cost at least`l\n2˘\n{2, and contributes only`l\n2˘\npairs\nof equal elements. Partitioning Binto constant sequences, we get inv pBq ď2cost pBq.\n8\n\n5 Improved algorithm\nIn the previous section, we saw that using the predictions on ce at the beginning of each eviction\nchain takes the chain most of the way through the phase; rando m evictions of unmarked elements\nare then used until the chain ends. Suppose that the second el ement of an eviction chain does\nnot reappear until Opη{optqsteps away from the end of the phase. Then evicting a uniforml y\nrandom element of the cache—marked or unmarked—would termi nate the chain immediately with\nprobability 1 ´Oppη{optq{kq.\nIn this section we present an improved algorithm lnonmarker motivated by the above obser-\nvation. As the name may suggest, it is not quite a marker-base d algorithm, and we need to give\nsome new names to familiar concepts:\nDeﬁnition 12. For any input sequence zand cache size k, thephasesof the input sequence are\ndeﬁned recursively as follows: phase rbegins right after the end of phase r´1, and extends as long\nas possible without containing k`1distinct elements.\nDeﬁnition 13. Fix an algorithm. An input element is called initialfor some phase rif it appeared\nin phase r, and was present in the cache at the beginning of phase r. If it appeared in phase rbut\nwas not present in the cache at the beginning of the phase, it i s callednon-initial .\nNote that the deﬁnition of phases given here coincides with t he phases of any marker-based\nalgorithm. This deﬁnition is algorithm-independent, and t hus is also useful for non-marker-based\nalgorithms. For any marker-based algorithm, the deﬁnition s ofcleanandnon-initial coincide, as do\nthe deﬁnitions of staleandinitial. However, it can be seen that these may diverge in the executi on\nof a non-marker-based algorithm. Some facts about clean and stale elements which we used in the\nprevious section are now facts about initial and non-initia l elements:\nClaim 14. Every phase contains exactly karrivals. Every non-initial arrival causes a cache miss.\nEvery other cache miss in the phase was caused by some previou s cache miss in the same phase.\nThus, in analogy with clean arrivals, every non-initial arr ival heads an eviction chain, and the\neviction chains partition the set of cache misses. However, unlike before, not all cache misses are\nnecessarily upon arrivals: an element might arrive, be evic ted, and reappear all in one phase.\nAlgorithm description The new algorithm lnonmarker still maintains markings on cache\nelements. At the beginning of each phase, all cache elements are unmarked. Whenever a cache hit\noccurs, the element is marked. Whenever a cache miss occurs, the algorithm evicts some element\nand marks the new element. In particular, the algorithm has t he following eviction strategy upon\na cache miss: if the incoming element was non-initial, then e vict the unmarked element with the\nhighest predicted arrival time. If the incoming element was evicted by a non-initial element, then\nevict a uniformly random element of the cache (not necessari ly unmarked). In all other cases, evict\na uniformly random unmarked element.\nBefore analyzing the algorithm, we must show that it is in fac t well-deﬁned.\nClaim 15. At any cache miss, there is at least one unmarked element. Thus , the algorithm is\nwell-deﬁned.\nProof.Each marked element of the cache must have appeared in the cur rent phase. The element\nwhich caused the cache miss is, of course, distinct from all e lements of the cache. Since the phase\ncontains at most kdistinct elements, the cache contains at most k´1 marked elements.\n9\n\nAlgorithm analysis Fix a phase rin which the set of elements is A. Fix a single execution of\nthe algorithm. Let Sbe the cache at the beginning of phase r. ThenAzSis the set of non-initial\nelements, and SzAis the set of cache elements which do not arrive in phase r. Two facts relate\nthese sets:\n•For each non-initial element c, the corresponding eviction chain evicts at most one elemen t\nofSzA.\n•EveryxPSzAis evicted at most once in the phase: xdoes not appear in the phase, so after\nit is evicted it will not return to the cache.\nTogether with the observation that |A| “ |S| “k, these facts imply that there is a bijection\nf:AzSÑSzAsuch that if c’s chain evicts xPSzAthenfpcq “x. For any cPAzSsuch that c’s\nchain does not evict any element of SzA, we setfpcqarbitrarily, subject to the condition that fis\na bijection.\nBy Claim 14, the number of cache misses in phase ris the total length of the eviction chains\nheaded by the non-initial elements of phase r. Letcbe one such non-initial element, with eviction\nchain of length length pcq, which arrives at time tcand evicts some unmarked element e. Suppose\nereappears in the phase. Let Npcqbe the number of ﬁrst arrivals after ein the same phase, and\nletN˚pcqbe the number of distinct elements after e(not necessarily ﬁrst arrivals).\nThe above deﬁnitions were made for a single execution, but th e algorithm deﬁnes a distribution\nEover executions. Let Erbe the distribution of executions of the ﬁrst r´1 phases, and let Etcbe\nthe distribution of executions through time tc.\nConditioned on Er, the non-initial elements care determined, but N˚pcqandNpcqare random\nvariables (over the randomness of the algorithm). Deﬁning l ength pcq “0 andN˚pcq “0 ifcis not\na non-initial element or if edoes not reappear in the phase, we get the following result.\nClaim 16. For any element cin phase r,\nEErlength pcq|Ers ďα¨EE„\n1`N˚pcq\nklogNpcqˇˇˇˇEr\nfor an absolute constant α.\nProof.Condition on Etc. Theneis determined, and N˚pcqandNpcqare determined. If edoes\nnot reappear in the phase, then Erlength pcq|Etcs “0. Suppose otherwise. Since eevicts a random\nelementgfrom the cache, the probability that gsubsequently appears during the same phase is at\nmostp“N˚pcq{k. Ifgdoes appear, the eviction chain continues by random evictio n of unmarked\nelements, until an evicted element is not in A. The number of unmarked elements at the time of\ng’s cache miss which are present in Ais at most Npcq, since any element which arrived earlier in\nthe phase is either marked or no longer in the cache. Thus, by t he same argument as in Claim 8,\nthe expected length of the chain is OplogNpcqq, conditioned on g’s cache miss. Since the length is\nOp1qifgdoes not appear, it follows that Erlength pcq|Etcsis at most Op1`plogNpcqq. Taking the\nexpectation over Etc|Eryields the claimed result.\nNow we want to bound N˚pcqin terms of the predictor error. We condition on the entire\nexecution of the algorithm: that is, the following lemmas ho ld deterministically for all executions.\nLemma 17. For any chain pc,e,... qin which ereappears after eviction, let tbec’s arrival time.\nLetr1be the next phase in which fpcqarrives. Then\nErrLpfpcq,tqpfpcqq `ErrLpe,tqpeq ěN˚pcq `kpr1´r´1q.\n10\n\nProof.Note that fpcqis an unmarked element of the cache at time t. It does not appear in phase r,\nso if phase r`1 begins at time tr`1thenfpcqdoes not appear until at least time tr`1`kpr1´r´1q,\nbut its predicted next appearance hLpfpcq,tqpfpcqqsatisﬁeshLpfpcq,tqpfpcqq ďhLpe,tqpeq, sinceewas\nthe unmarked cache element which maximized predicted arriv al at time t. The next appearance of\neis no later than time tr`1´N˚pcq, since at least N˚pcqelements lie between e’s appearance and\nthe end of the phase. So Err Lpfpcq,tqpfpcqq `ErrLpe,tqpeq ěN˚pcq `kpr1´r´1q.\nRemark 1.There is an edge case in Lemma 17 if fpcqnever appears after phase rand the last\nphase has less than kelements. In this case the inequality in Lemma 17 can be oﬀ by a s much\nask. However, when the lemma is applied in Lemmas 19 and 20, the ap plications almost always\nimplicitly weaken the inequality by at least kanyway. Each application where the inequality is not\nweakened corresponds to a chain in one of the last two phases. There are only Opkqsuch chains,\neach causing a discrepancy of at most k. To make the lemmas hold, it therefore suﬃces to replace\nηbyη`Opk2q. However, Opk2qis bounded as the input length and optgrow, so the competitive\nratio is unaﬀected.\nNext we would like to sum the inequality given by Lemma 17 acro ss all chains in all phases,\nto boundřN˚pcqin terms of the total prediction error. Within a single phase ,fpcqandeeach\nuniquely determine c, so the error of each prediction is counted at most once. Howe ver, the error of\na single prediction may be counted in multiple chains in succ essive phases. The following lemmas\ndo a more careful summation, taking into consideration the d ouble-counting.\nLemma 18. For distinct times t1,t2supposec1“zt1andc2“zt2are non-initial elements of\nphasesr1andr2, which evict e1ande2respectively. Suppose that e1ande2reappear after eviction.\nThenLpe1,t1q ‰Lpe2,t2q.\nProof.If equality were to hold, then e1“e2“wfor some w. Note that wis evicted at c1’s\narrival, and wis evicted at c2’s arrival, but wis unmarked both times. So the evictions occurred\nin diﬀerent phases. Without loss of generality suppose that r1ăr2. Thenwappeared in phase r1\nafter eviction by c1, soLpw,t1q ăLpw,t2q.\nLemma 19. Summing over all chains pc,e,... qin which ereappears after eviction,\nÿ\ncN˚pciq ď3η\nwhereηis the total prediction error.\nProof.Fix a time tand consider the set of chains pci,ei,...qsuch that Lpfpciq,tiq “t, wheretiis\nthe arrival time of ci: that is, fpciq “ztand the most recent appearance of fpciqprior to time ti\nwas at time t. Each chain is in a diﬀerent phase, since fis injective for any one phase. If there are\nmtsuch chains in phases r1ă ¨ ¨ ¨ ărmt, thenztdoes not appear after time tuntil phase rmt`1\nor later. Applying Lemma 17 to each of the mtchains and summing, we get\nmtErrtpztq `mtÿ\ni“1ErrLpei,tiqpeiq ěkmtpmt´1q\n2`mtÿ\ni“1N˚pciq ěmtÿ\ni“1mt`1\n2N˚pciq.\nDividing both sides by pmt`1q{2, we get that\n2Errtpztq `mÿ\ni“1ErrLpei,tiqpeiq ěmtÿ\ni“1N˚pciq.\n11\n\nNow sum over all times t. By Lemma 18, all Lpei,tiqare distinct, so the corresponding errors sum\nto at most η. Thus, we have\n3ηěÿ\ncN˚pciq\nsumming over all chains pc,e,... qin which ereappears after eviction.\nFinally, we must bound the number of eviction chains Cacross all phases. In a marker-based\nalgorithm we would have C“L, with one chain per clean element. But here there is one chain per\nnon-initial element, and not all non-initial elements are c lean (and vice versa). Nonetheless, we can\nstill bound the discrepancy C´Lagainst the prediction error. The intuition for the followi ng proof\nis that any “extra” non-initial element must have been cause d by eviction of a marked element in\nthe previous phase. But for each chain which ends by evicting a marked element, one less unmarked\nelement is evicted. So whereas in a marker-based algorithm, every cache element which did not\nappear in a given phase would be evicted by the end of the phase , in this algorithm some absent\nelements might remain in the cache. These elements have the p otential to be clean in the next\nphase and yet not start eviction chains. Hence, in one case an extra chain can be charged against a\nnon-existent chain. In the other case—when the absent eleme nt does not appear in the next phase\neither—the extra chain can be charged against prediction er ror.\nLemma 20. IfCis the number of non-initial elements across all phases, Lis the number of clean\nelements, and ηis the total prediction error, then ηěkpC´Lq{2.\nProof.Suppose some non-initial element gin phase r`1 is not clean. Then it appeared in phase\nr, but was not in the cache at the end of phase r. In the last eviction of gduring phase r,gwas\nalready marked. So by the algorithm design, that eviction mu st be third in some eviction chain\npc,e,g q, wherecis some non-initial element in phase r. Thenfpcqdoes not appear in phase r.\nNonetheless, fpcqis not in c’s eviction chain, since gis the unique element in c’s eviction chain\nwhich does not reappear in phase r. Furthermore, by deﬁnition of f, no other chain in phase r\nevictsfpcq, so it is in the cache at the end of phase r. There are two cases:\n1.fpcqappears in phase r`1. Then fpcqis clean and initial in phase r`1.\n2.fpcqdoes not appear in phase r`1. Letr1be the next phase in which fpcqdoes appear.\nThen iftis the arrival time of c, by Lemma 17\nErrLpfpcq,tqpfpcqq `ErrLpe,tqpeq ěkpr1´r´1q.\nThe composed map gÞÑcÞÑfpcqis injective for ﬁxed phase r, sincegis determined by the\neviction chain of c, andfis injective for a ﬁxed phase. If case (2) never occurred, the n every non-\ninitial, non-clean element in phase r`1 would be injectively mapped to an initial, clean element\nin phase r`1, so we would have CďL. More generally, case (2) occurs for at least C´Lchains.\nEach case (2) chain provides an inequality bounding two pred iction errors by at least k, and ideally\nwe would simply add up the inequalities to bound η. However, some predictions may be counted\nin several of the inequalities.\nBy Lemma 18, all terms Err Lpe,tqpeqin theinequalities are contributed by diﬀerent predictions —\ni.e. adding up those error terms does not double-count.\nConsider a ﬁxed time t. Consider the set of case-(2) chains pci,ei,giqsuchLpfpciq,tiq “t,\nwheretiis the time of ci’s arrival. Each chain is in a diﬀerent phase, since fis injective for any one\n12\n\nphase. If there are mtsuch chains in phases r1ă ¨ ¨ ¨ ărmt, thenztdoes not appear after time t\nuntil phase rmt`2 or later. So the case-(2) inequality applied to the earlies t chain pc1,e1,g1qgives\nErrtpztq `ErrLpe1,t1qpe1q ěkprmt`1´r1q ěkmt.\nSumming the above inequality over all times t, each prediction error is counted at most twice—\nonce as the ﬁrst term and once as the second—whereasř\ntmtěC´Las previously shown. Hence,\n2ηěkpC´Lq, as desired.\nWith the above error/performance bounds in hand, we can prov e a bound on the competitive\nratio of the algorithm.\nTheorem 21. The algorithm lnonmarker achieves a competitive ratio of\nOˆ\n1`η{opt\nklogk˙\nwhen the prediction error is no more than η.\nProof.Fix any phase r. Then for any execution Erof the ﬁrst r´1 phases,\nEr# cache misses in phase r|Er“Ers “ÿ\npc,e,... qPrErlength pcq|Er“Ers,\nsumming over chains in phase r.\nBy Claim 16, the right hand side is bounded by\nαCr`αlogk\nkE»\n–ÿ\npc,e,... qPrN˚pcqˇˇˇˇˇˇEr“Erﬁ\nﬂ,\nwhereCris the number of chains in phase r, andαis a constant. Applying law of total expectation\nand then summing over all phases,\nEr# cache misses s ďαErCs `αlogk\nkE»\n–ÿ\npc,e,... qN˚pcqﬁ\nﬂ.\nFrom Lemma 20, the inequality Cď2η{k`Lholds for all executions, and therefore in expec-\ntation. From Lemma 19, the inequalityř\ncN˚pcq ď3ηholds for all executions, where cranges\nover chains pc,e,... qwhereereappears after eviction. Recall that N˚pcqwas deﬁned to be 0 for\nall other chains. So we get that\nEr# cache misses s ď2αη\nk`αL`3αη\nklogk.\nRecalling from Lemma 5 that L{2ďopt, the competitive ratio follows.\nThe above algorithm has one ﬂaw: it does not satisfy any robus tness guarantee (at least, that\nhas been proven), since as η{opt Ñ 8the bound on the number of chains disappears, and the\ncompetitive ratio becomes potentially unbounded. This can be resolved (albeit in a somewhat\nunsatisfactory manner) by appealing to a black-box simulat ion theorem. For concreteness, we\nrecall the following theorem:\n13\n\nTheorem 22. [8] LetA,Bbe algorithms for the caching problem with competitive rati os ofαand\nβrespectively. Then there is a black box algorithm ALG with a com petitive ratio of 9min pα,βq.\nThe black box algorithm in the above theorem proceeds by simu latingAandBand switching\nbetween them whenever one starts to heavily outperform the o ther. The proof generalizes without\nchange to the learned caching problem. Since there is an Oplogkq-competitive algorithm for learned\ncaching which simply ignores the predictions, Theorem 22 im plies that we can obtain an Oplogkq\nworst-case guarantee for our predictive caching algorithm with only an extra constant factor loss:\nCorollary 23. There is an algorithm for caching with predictions that achie ves a competitive ratio\nof\nOˆ\n1`minˆ\n1,η{opt\nk˙\nlogk˙\n.\nTracing through the proof, it turns out that the exact bound i s as follows:\nCorollary 24. There is an algorithm for caching with predictions that achie ves a competitive ratio\nof\n9minˆ\n4`7η{opt\nk`3η{opt\nkHpkq,2Hpkq˙\n.\nFor example, as η{opt Ñ0, the competitive ratio of lnonmarker approaches 4, and so the\ncompetitive ratio of this black box algorithm approaches 36 .\n6 Lower bound\nInthis section weprovidealower boundagainst predictive c achingalgorithms. Thebasicstrategy is\nto construct a distribution of inputs and predictions such t hat the relative prediction error η{opt\nis not too high, but any deterministic algorithm which has ac cess to those predictions suﬀers a\nlarge number of cache misses in expectation, relative to opt. Yao’s minimax principle then implies\na lower bound against the competitive ratios of randomized a lgorithms at that level of relative\nprediction error.\nMore speciﬁcally, the input distribution and predictions w ill be chosen such that any preﬁx of\nthe input completely determines the state of the algorithm. Furthermore, each preﬁx is suﬃciently\nindependent of future inputs that the algorithm can essenti ally do no better than (a) keeping\npreviously-seen elements in the cache (for the duration of t he phase), and (b) guessing arbitrarily\nfor the remaining unseen elements.\nFixkandn. Let 1 ďtďkbe picked later; it is a free parameter which will determine\nthe relative prediction error η{opt. Let Ω be the set of sequences that can be constructed in\nthe following manner. Let C1“ t1,...,t uand letS1“ tt`1,...,k u. For all 2 ďrďn, let\nCr“ rk`tszpCr´1YSr´1qand letSrbe an arbitrary subset of Cr´1YSr´1of sizek´t. Then each\nsequence of Ω is constructed as the concatenation of nphases, where phase rconsists of 3 klogk\nelements of CrYSr(possibly with some omissions and necessarily with some rep etitions), followed\nby a single copy of CrYSrin increasing order, without repetitions.\nThat is, each phase has length m“3klogk`k. Each phase rhastclean elements Cr(which\ndid not appear in the previous phase) and k´tstale elements Sr(which did appear). For any ﬁxed\nrą1, a uniformly random sequence of Ω, conditioned on Cr, has a uniformly random set of stale\nelements Sr. Furthermore, conditioned on CrandSr, each of the ﬁrst 3 klogkelements of phase r\nis independent and uniformly distributed over CrYSr.\n14\n\nWe must also deﬁne the predictions for a ﬁxed sequence. For th e ﬁrst 3klogkelements of phase\nr, each prediction is the subsequent timestep (i.e. hpziq “i`1). For the ﬁnal copy of CrYSr,\neach prediction is the end of phase r`1.\nClaim 25. The overall prediction error is Opnk2logkq, wherenis the number of phases in Ω.\nProof.Fix a phase r. Among the ﬁrst 3 klogkelements of the phase, there are at most kdistinct\nelements. For each, the prediction error telescopes to at mo st 3klogk`k. For each of the k\nelements in the ﬁnal copy of CrYSr, the true next arrival is either in phase r`1 or phase r`2,\nso the error is at most Opklogkq. Thus, the error in phase risOpk2logkq.\nNow we would like to lower bound the average cache misses of an y algorithm on Ω. We must\nﬁrst give some simplifying notation and a probabilistic lem ma:\nFor any time Tin phase r, let „Tbe the equivalence relation on Ω where sequences zandz1\nare equivalent if zj“z1\njforjďT. Observe that z„Tz1implies that hpzjq “hpz1\njqfor alljďT\nby how the predictions were constructed. Thus, for any equiv alence class rzsTof„T, the algorithm\nhas identical executions on sequences in rzsTup to time T.\nForzPΩ and time T, letCz,Tbe the cache at time Ton input rzsT.\nLemma 26. Fixk,lwith2ďlďk. LetX1,...,X 3klogkbe independent random variables\nuniformly distributed over rks. LetYibe the number of distinct elements in tX1,...,X iu X rlsand\nfor0ďjălletTjbe the number of isuch that Yi“j. ThenErTjs ěk{pl´jq ´1{k.\nProof.Extend the sequence Xto an inﬁnite sequence. Extend Yaccordingly, and deﬁne ˆTjas the\nnumber of steps in the inﬁnite sequence at which Yi“j. For 0 ďjăl, letSj“T0` ¨ ¨ ¨ `Tjand\nˆSj“ˆT0` ¨ ¨ ¨ `ˆTj. ThenSj“minpˆSj,3klogkq, so\nErTjs ěErminpˆSj,3klogkqs ´ErˆSj´1s\n“ErˆTjs ´ErpˆSj´3klogkq /BDˆSjě3klogks.\nFor the ﬁrst term, ˆTjis a geometric random variable and ErˆTjs “k{pl´jq. For the second,\nErpˆSj´3klogkq /BDˆSjě3klogks ď8ÿ\nc“3klogkPrrˆSjěcs.\nObserve that for any c, ifˆSjěcthenYcďjăl, which occurs with probability at most\nlp1´1{kqc.It follows that\nErpˆSj´3klogkq /BDˆSjě3klogks ď8ÿ\nc“3klogklˆ\n1´1\nk˙c\nď1\nk.\nWe conclude that ErTjs ěk{pl´jq ´1{k.\nTheorem 27. For any deterministic algorithm with access to the predicti onsh, the expected number\nof cache misses achieved on an input sampled uniformly at ran dom from Ωis at least\nOˆ\nntlogk\nt˙\n.\n15\n\nProof.There are nphases in Ω. Fix a phase r. We claim that the expected number of cache misses\nin phase ris at least Optlogk{tq.\nRecall that each phase has length m“3klogk`k. LetT“ pr´1qm: the ﬁnal index of phase\nr´1. FixzPΩ. Fix 1 ďiďklogkand consider any class rwsT`iĎ rzsT. Picking a sequence\nWP rwsT`iuniformly at random, the clean elements Crand the cache CW,T `iare determined, but\nSris a random variable. Let Sseenbe the set of elements which have already been seen in phase\nrby time T`i, excluding Cr. ThenSrmust contain Sseen, butSrzSseenis a uniformly random\nsubset of rk`tszpCrYSseenqof sizek´t´ |Sseen|. Hence,\nPr\nWrcache hit on WT`is “ÿ\nˆSPr\nWrSr“ˆSsPr\nWrcache hit on WT`i|Sr“ˆSs\n“ÿ\nˆSPr\nWrSr“ˆSs|CW,T `iX pCrYˆSq|\nk\n“|CW,T `iX pCrYSseenq| `EW|CW,T `iX pSrzSseenq|\nk\nIf|CW,T `iX pCrYSseenq| “a, thenCW,T `icontains k´aelements in rk`tszpCrYSseenq, each of\nwhich is contained in SrzSseenwith probability pk´t´|Sseen|q{pk´|Sseen|q. So the above expression\nis maximized when a“ |CrYSseen| “k`t. Thus,\nPr\nWrcache hit on WT`is ďt` |Sseen| ` pk´t´ |Sseen|q ¨k´t´|Sseen|\nk´|Sseen|\nk.\nSimplifying, it follows that\nPr\nWrcache miss on WT`is ět\nkk´t´ |Sseen|\nk´ |Sseen|.\nIfZP rzsTis chosen uniformly at random, then for any 0 ďNăk´t, the expected number\nof times at which |Sseen| “Nis at least k{pk´t´Nq ´1{kěk{p2pk´t´Nqqby Lemma 26.\nSumming over N, the expected number of cache misses in phase ron input Zis at least\nk´t´1ÿ\nN“0t\nkk´t´N\nk´Nk\n2pk´t´Nq“k´t´1ÿ\nN“0t\n2pk´Nqět\n2ˆ1\nt`1` ¨ ¨ ¨ `1\nk˙\n“Ωptlogk{tq.\nSince the equivalence classes of „Tpartition Ω, and the above bound holds for each class, it hold s\nfor Ω as desired.\nNow for any ﬁxed “relative prediction error” ǫwithklogkďǫďk2logk, we can pick t“\npk2logkq{ǫ. Then every sequence in Ω has ntclean elements, and thus opt “Θpntq. Furthermore,\nby Claim 25, we have ηďnk2logk. Thus,η{opt ďǫ.\nBut by Theorem 27, any deterministic algorithm requires ntlogk{tcache misses in expectation\non Ω. By Yao’s minimax principle, for any randomized algorit hm there is some input zPΩ for\nwhich the algorithm incurs ntlogk{tcache misses in expectation. Hence, we have the following\nresult:\nTheorem 28. LetAbe a randomized online algorithm for caching, which has acce ss to next-arrival\npredictions. For any ǫ, the algorithm achieves competitive ratio no better than\nΩˆ\nlogminˆǫ\nklogk,k˙˙\nwhen restricted to inputs with η{opt ďǫ.\n16\n\nAcknowledgments. I would like to thank Costis Daskalakis and Piotr Indyk for in troducing me\nto the area of learning-augmented algorithms, and for their advice and encouragement.\nReferences\n[1] Dimitris Achlioptas, Marek Chrobak, and John Noga. Comp etitive analysis of randomized\npaging algorithms. Theoretical Computer Science , 234(1):203–218, 2000.\n[2] Deepak Ajwani and Tobias Friedrich. Average-case analy sis of online topological ordering. In\nProceedings of the 18th International Conference on Algorith ms and Computation , ISAAC’07,\npages 464–475, Berlin, Heidelberg, 2007.\n[3] Amos Fiat, Richard M. Karp, Michael Luby, Lyle A. McGeoch , Daniel D. Sleator, and Neal E.\nYoung. Competitive paging algorithms. Journal of Algorithms , 12(4):685–699, 1991.\n[4] Sreenivas Gollapudi and Debmalya Panigrahi. Online alg orithms for rent-or-buy with expert\nadvice. In International Conference on Machine Learning , pages 2319–2327, 2019.\n[5] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estima-\ntion algorithms. In International Conference on Learning Representations , 2019.\n[6] Elias Koutsoupias and Christos Papadimitriou. Beyond c ompetitive analysis. SIAM Journal\non Computing , 30(1):300–317, 2000.\n[7] Tim Kraska, Alex Beutel, Ed H. Chi, Jeﬀrey Dean, and Neokli s Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Manageme nt of Data ,\nSIGMOD ’18, pages 489–504, New York, NY, USA, 2018.\n[8] Thodoris Lykouris and Sergei Vassilvitskii. Competiti ve caching with machine learned advice.\nInInternational Conference on Machine Learning , pages 3302–3311, 2018.\n[9] Lyle A. McGeoch and Daniel D. Sleator. A strongly competi tive randomized paging algorithm.\nAlgorithmica , 6(1):816–825, Jun 1991.\n[10] Andrew McGregor. Graph stream algorithms: A survey. SIGMOD Record , 43(1):9–20, May\n2014.\n[11] Andr´ es Mu˜ noz Medina and Sergei Vassilvitskii. Reven ue optimization with approximate bid\npredictions. In Proceedings of the 31st International Conference on Neural Inf ormation Pro-\ncessing Systems , NIPS’17, pages 1856–1864, USA, 2017.\n[12] Michael Mitzenmacher. Scheduling with predictions an d the price of misprediction. arXiv\npreprint arXiv:1902.00732 , 2019.\n[13] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improvi ng online algorithms via ML pre-\ndictions. In Advances in Neural Information Processing Systems 31 , pages 9661–9670. 2018.\n[14] Daniel D. Sleator and Robert E. Tarjan. Amortized eﬃcie ncy of list update and paging rules.\nCommunications of the ACM , 28(2):202–208, February 1985.\n[15] Daniel A. Spielman and Shang-Hua Teng. Smoothed analys is of algorithms: Why the simplex\nalgorithm usually takes polynomial time. Journal of the ACM , 51(3):385–463, May 2004.\n17\n\n[16] Daniel A. Spielman and Shang-Hua Teng. Smoothed analys is: an attempt to explain the\nbehavior of algorithms in practice. Communications of the ACM , 52(10):76–84, 2009.\n18",
  "textLength": 51251
}