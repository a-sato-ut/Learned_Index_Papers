{
  "paperId": "5b58801969b3442b434f572b96189a35d4a3a703",
  "title": "SOLAR: Scalable Distributed Spatial Joins through Learning-based Optimization",
  "pdfPath": "5b58801969b3442b434f572b96189a35d4a3a703.pdf",
  "text": "arXiv:2504.01292v1  [cs.DB]  2 Apr 2025SOLAR:ScalableDistributedSpatialJoins through\nLearning-based Optimization\nYongyi Liu\nUniversity of California,Riverside\nRiverside, California\nyliu786@ucr.eduAhmedR. Mahmood\nGoogleLLC.\nMountain View, California\namahmoo@google.com\nAmr Magdy\nUniversity of California,Riverside\nRiverside, California\namr@cs.ucr.eduMinyaoZhu\nGoogleLLC.\nMountain View, California\nminyaozhu@gmail.com\nABSTRACT\nTheproliferationoflocation-basedserviceshasledtomas sivespa-\ntialdatageneration.Spatialjoinisacrucialdatabaseope rationthat\nidentiﬁespairsofobjectsfromtwospatialdatasetsbasedo nspatial\nrelationships.Duetotheintensivecomputationaldemands ,spatial\njoins are often executed in a distributed manner across clus ters.\nHowever, current systems fail to recognize similarities in the par-\ntitioning of spatial data, leading to redundant computatio ns and\nincreased overhead. Recently, incorporating machine lear ning op-\ntimizations into database operations has enhanced eﬃcienc y in\ntraditional joins bypredicting optimal strategies. Howev er, apply-\ningtheseoptimizationstospatialjoinsposeschallengesd uetothe\ncomplex nature of spatial relationships and the variabilit y of spa-\ntial data. This paper introduces SOLAR, scalable distributed spa-\ntial joins through learning-based optimization. SOLARoperates\nthroughoﬄineandonlinephases.Intheoﬄinephase,itlearn sbal-\nancedspatialpartitioningbasedonthesimilaritiesbetweendat asets\nin query workloads seen so far. In the online phase, when a new\njoinqueryisreceived, SOLARevaluatesthesimilaritybetweenthe\ndatasets in the new query and the already-seen workloads usi ng\nthetrainedlearningmodel.Then,itdecidestoeitherreuse anexist-\ning partitioner, avoiding unnecessary computational over head, or\npartition from scratch. Our extensive experimental evalua tion on\nreal-world datasets demonstrates that SOLARachieves up to 3.6X\nspeedupinoveralljoinruntimeand2.71Xspeedupinpartiti oning\ntimecomparedtostate-of-the-art systems.\nPVLDBReference Format:\nYongyi Liu, Ahmed R.Mahmood, Amr Magdy,and MinyaoZhu.SOLAR:\nScalable DistributedSpatial Joins through Learning-based Optimizat ion.\nPVLDB,14(1):XXX-XXX,2020.\ndoi:XX.XX/XXX.XX\nPVLDBArtifact Availability:\nThe source code, data, and/or other artifacts have been made av ailable at\nhttps://github.com/Yongyi-Liu/SOLAR.\nThiswork islicensed under the CreativeCommons BY-NC-ND4. 0International\nLicense.Visithttps://creativecommons.org/licenses/b y-nc-nd/4.0/to viewacopy of\nthis license. Forany usebeyondthose coveredbythislicens e, obtainpermissionby\nemailing info@vldb.org.Copyrightisheld bythe owner/aut hor(s). Publication\nrights licensed to the VLDBEndowment.\nProceedings of the VLDBEndowment, Vol. 14,No. 1 ISSN2150-8 097.\ndoi:XX.XX/XXX.XX1 INTRODUCTION\nA spatial join, speciﬁcally a spatial theta join, is a databa se opera-\ntion that identiﬁes pairs of objects from two spatial datase ts that\nsatisfya spatialpredicate.This operationis extensively appliedin\nvariousﬁeldsincludingurbanplanning[44],i.e.,overlay ingdemo-\ngraphic data over political district polygons for politica l district-\ning,publicsafety[43],i.e.,identifyingvehicle-pedest rianinjuryin\neach census tract by joining the collision data with census t ract,\nandmedicalimaging[42],i.e.,detectingproximatecells. Giventhe\nvastamountofspatialdata,spatialjoinsareincreasingly processed\nindistributedcomputingenvironmentstoeﬃcientlymanage large-\nscalespatialdata.\nMotivation: Inpractice,enterprisecustomerstendtoissuesim-\nilarqueriesrepeatedly[18,34,37].Huangetal.[18]point outthat\nMicrosoft workloads are highly recurrent. Schmidt et al. [3 4] fur-\nther propose that in Amazon Redshift data warehouse, custom er\nworkloadsexhibithighlysimilarquerypatterns,i.e.,use rsandsys-\ntems frequently issue the same queries. Similarly, in spati al data\nsystems [5, 14, 15, 35, 36, 45, 49],users frequently perform spatial\njoins involving datasetswith similar spatial characteris tics.\nAcriticalstepinexecutingdistributedspatialjoinsisco nstruct-\ningabalancedpartitioner,adatastructureusedtodistrib utespatial\ndata fairly across worker nodes. However, this partitionin g pro-\ncess incurs substantial computational overhead [4]. For in stance,\npartitioning just two terabytes of spatial data can take up t o ﬁve\nhours[6].Duetotheexpensivenatureofconstructingapart itioner\nand the repeating query patterns in industry workloads, onc e an\neﬀective spatial partitioner is established, there is no ne ed to re-\ncomputeitforeachoccurrenceofthesameorsimilarquery.H ow-\never, existing systems [5, 14, 15, 35,36, 45, 49] recalculat ethepar-\ntitioner for these frequently executed joins, which result s in re-\npeatingcomputationsforsimilarqueries.Inthemeantime, spatial\njoinsoftenshareinherent similarities.Forexample,thep rocessof\njoining datasets of parks with restaurants often requires a simi-\nlarpartitioning tothoseusedwhen joining datasets of park s with\ncafes, due to spatial correlations between these entities. Addition-\nally, when working with an updated version of a dataset, such as\npoints of interest, new or deleted records may be present. Ho w-\never,theoveralldistributionislikelytoremainunchange d.Byrec-\nognizing and leveraging these patterns, it is possible to ap ply a\npreviouslydeterminedpartitionertosimilarjoins.Inthi sway,the\n\nsystemcanbypassrescanningtheinputdatatogatherdistri bution\nstatistics and skip buildinga partitioner on-the-ﬂy (repa rtition).\nChallenges: Optimizingfuturespatialjoinworkloadsbylever-\naging historical spatial join workloads introduces four ch allenges.\n(1) Despite the frequent occurrence of similar queries over iden-\ntical or spatially similar datasets, there is no standardiz ed metric\nto quantitatively evaluate the similarity between diﬀeren t spatial\njoins and their underlying spatial join inputs, i.e., datas ets. Identi-\nfying accurate similarity metrics between inherently skew ed spa-\ntial datasets and expensive spatial joins is challenging. ( 2) Being\nabletolearnthesimilaritybetweenspatialjoinsanddatas etscalls\nfor eﬃcient embeddings of join queries and their correspond ing\ndatasets. This is very challenging as these embeddings must be\ncompactforeﬃcientstorage,yetcomprehensiveenoughtoen cap-\nsulatetheessentialfeaturesofthequeriesandtheircorre sponding\ninputs.(3)Theutilizationofhistoricalspatialjoinsand theirunder-\nlying spatial properties of join inputs to enhance the proce ssing\nof new spatial joins adds another layer of complexity. The ch al-\nlenge here is that the system needs to accurately and eﬃcient ly\nmatch a new spatial join query to a pre-existing spatially si milar\njoin or dataset in its history whenever possible. (4) Making an ac-\ncuratedecisionforanincomingspatialjointhatdeviates f romhis-\ntoricalpatterns—whethertoadaptthepartitionerfromahi storical\nspatialjoin workloadorexecutethenew querywithoutusing his-\ntorical information (i.e., repartition spatial join input s)—is a very\nchallenging problem.Thereason is that making a wrong decis ion\naboutreusinghistoricalinformationcouldleadtopoorspa tialjoin\nperformance, and avoiding partition reuse when needed will also\nincur unneeded spatialpartitioning overhead.\nExistingwork: Recently,learning-basedapproacheshavebeen\nwidelyusedtooptimizevariousoperationsindatabasesyst ems.In\nnon-spatial joins, reinforcement learning has been used to deter-\nmine the optimal join orders of multiple tables [10, 22, 26, 4 6, 50].\nIn spatial joins, reinforcement learning has been used to ﬁn d an\noptimal partitioner [17], and traditional classiﬁcation a nd regres-\nsionmodelshavebeenusedtobuildspatialjoinoptimizers[ 40,41],\nwhich can determine the best spatial join algorithm or parti tion-\ningmethodforagivenquerybasedondatadistribution.Howe ver,\nnone of these works attempts toreuse existing partitioners for fu-\ntureworkloadsofspatial joinqueries.\nContribution: To address the highlighted challenges and the\nlimitations of existing work, we introduce SOLAR, Scalable Dis-\ntributed Spatial Jo ins through L earning-based Optimization, that\noperates in both oﬄine and online phases. In the oﬄine phase,\nSOLARtrains a Siamese Neural Network [9] to capture the over-\nall spatial properties of datasets using thefeatures extra ctedfrom\ndataset’smetadata.Thistrainingshifts themodelfromdep ending\non detailed data histograms to employing abstract dataset e mbed-\ndings for similarity assessment. During the online phase, w hen a\nnewjoinqueryissubmitted, SOLARencodesthedatasetsinvolved\ninthequery.Itevaluatesthesedatasetsagainstthoseinth ereposi-\ntoryusingthetrainedneuralnetworktoevaluatepairwises imilar-\nitiesusingfastvector-basedcomparisonstoretrievethem ostsimi-\nlardatasetanditsassociatedpartitioner.Thispartition erisreused\ntoeﬃciently partitionthenew join’s dataset.\nFigure1illustrateshow SOLARreusesexistingpartitioners.Con-\nsider a scenario where SOLARexecutes a join between the hotelandtheaterdatasets. First, both datasets are embedded. Next, SO-\nLAR’s learned modelperformsa vectorizedcomputationtolocat e\ntherepositorydatasetmostsimilartoeitherinput.Ifthei dentiﬁed\ndataset is suﬃciently similar, its existing partitioner is retrieved\nand reused; otherwise, a new partitioner is constructed on t he ﬂy\n(repartition) and stored in the repository. In this example ,restau-\nrantis identiﬁed as suﬃciently similar to hotel, soSOLARreuses\ntherestaurant partitionerfor the hotel–theaterjoin. After thejoin\nis completed, data statistics (i.e., data histograms) are c reated for\ntheinputdatasetstocapturetheirunderlyingdistributio ns.These\nhistogramsallowustocomputetheground-truthsimilarity forany\npair of datasets in the repository. Such ground-truth value s are\nthen used to train SOLAR’s similarity-learning model: the model\nsees only a dataset’s metadata-based embedding and is train ed to\napproximate the similarity measured by the histograms. Thi s it-\nerative process ensures that, over time, SOLARreﬁnes its ability\nto identify and reuse partitioners accurately based on meta data\nalone. In our experiments, whenever the decision maker opts to\nreuse a partitioner, SOLARconsistently outperforms all baseline\nalgorithms,achievinguptoa3.6Xspeedupinoveralljoinru ntime\nand a 2.71Xspeedupin partitioningtime.\nThemaincontributionsofthispaperaresummarizedasfollo ws:\n•Wepropose SOLAR,scalabledistributedspatialjoinsthrough\nlearning-basedoptimizationthatimprovestheperformanc e\nofdistributedspatialjoinsbyreusingpreviouslycompute d\npartitioners.\n•We propose a method to compute the similarity between\ndatasets basedondata histograms.\n•Weproposeanembeddingmethodforspatialdatasetsbased\nontheir metadata,i.e., polygoncovering.\n•We propose a learning strategy to learn dataset similari-\nties directlyfrom dataset metadata, bypassing the need to\ncomputehistograms foreach query.\n•Weperformexperimentswithvariousdatasources,demon-\nstrating that SOLARsigniﬁcantly improves the eﬃciency\nofdistributedspatialjoinscomparedtoallspatialjoinpa r-\ntitioning and joinalgorithms ofstate-of-the-art system.\nTherestofthispaperisstructuredasfollows:Section2out lines\ntherelatedwork.Section3presentsthepreliminariesandp roblem\ndeﬁnition. Section 4 details the selection of the partition er in our\nsystem. Section 5 describes the similarity evaluation metr ics be-\ntween datasets. Sections 6 and Section 7 discuss the oﬄine an d\nonline phases of SOLAR, respectively. Section 8 presents experi-\nmentalevaluation,andSection9concludesthepaperandout lines\nfuturework.\n2 RELATED WORK\nDistributed Spatial Joins: Distributed spatial join typically fol-\nlows a two-step process [1, 3, 4, 16, 17, 19, 32, 39, 47, 48]: gl obal\npartitioningandlocaljoin.Intheglobalpartitioningpha se,spatial\ndatasets are divided into multiple partition blocks based o n their\nspatial distribution, with denser regions receiving a grea ter num-\nber of partition blocks to balance the workload. During the l ocal\njoin phase, each worker is assigned one or more of these block s.\n2\n\nFigure1: Exampleof SOLARReusingExistingPartitionersinExecutinngSpatial JoinQ ueries\nTheseworkersthenexecutethejoinontheirdesignatedbloc ks.Af-\nter each worker completes their tasks, the results are aggre gated\nand reﬁned toproducetheﬁnal outcome.\nThe mainstream systems forexecuting distributedspatial j oins\nprimarilyutilizeHadoop[13]orSpark[51]frameworks.The sesys-\ntems generally follow the two-phase approach, i.e., global parti-\ntioning and local join. Hadoop-based systems convert distr ibuted\nspatial joins into a series of Map and Reduce tasks. Examples of\nHadoop-based systems include Spatial-Hadoop [15] and Hadoop-\nGIS[5]. However, a key limitation of Hadoop-based systems is\ntheir reliance on ineﬃcient storage of intermediate result s. To ad-\ndress this issue, Spark-based systems utilize Resilient Di stributed\nDatasets (RDDs) and leverage in-memory processing to enhan ce\neﬃciency.Examplesofsuchinclude ApacheSedona [1](previously\nknown as Geospark [3]),Beast[14],Simba[45],LocationSpark [35,\n36]andetc.Ingeneral,Spark-basedsystemsaremoreeﬀecti vedue\ntotheir in-memory processing capabilities.\nMachine Learningin Non-Spatial Joins: Applying machine\nlearning to non-spatial join operations lies in two primary cate-\ngories. The ﬁrst category involves the use of reinforcement learn-\ning to optimize the order of joins [10, 22, 26, 46, 50]. In this ap-\nproach, the system models the state as a speciﬁc conﬁguratio n of\njoin orders. Actions are deﬁned as the potential joins that c an be\nexecutednext,andrewardsareinverselyproportionaltoth equery\ncost.Thereinforcement learningmodelistrainedtodevelo papol-\nicy that determines the most eﬃcient join order, minimizing the\noverall query execution time. The second category involves the\nuseoflearnedindexes toreplacetraditionalcomponentsty pically\nrequiredin joinoperations[31].Forexample, RecursiveMo delIn-\ndexes (RMI) [21] is used to replace conventional indexing me cha-\nnisms in Indexed Nested Loop Joins (INLJ). Similarly, CDF-b asedpartitioning functions [23] replace hash functions in hash -based\njoins (HJ), enhancing eﬃciency and performance. However, t his\nideacannotbeappliedtospatialjoinbecausespatialdatar equires\nspecializedindexing and partitioning techniques.\nMachineLearninginSpatialJoins: Recentworkhavefocused\nonenhancing theeﬃciency ofspatialjointhroughlearning- based\nmethods. Vu et al. [40, 41] introduce a learning-based frame work\nnamedSpatial Join Machine Learning (SJML) , aimed at improving\nspatial join executions by utilizing statistics from the da tasets in-\nvolved. This framework oﬀers a range of models, from selecti v-\nity estimation to selecting the appropriate join algorithm s. How-\never, some statistics, e.g., convoluted histogram [40, 41] , require\ntraversing the datasets twice in real-time. This requireme nt de-\ngrades performance due to increased computational overhea d. In\naddition, there has been some work on the use of machine learn -\ning to identify eﬀective partitioners. Hori et al.[17] prop ose a re-\ninforcement learning modeltoachieve load-balancedparti tioning\nfor spatial joins. Their model conceptualizes the environm ent as\ntwo-dimensionalgrids,withactionsdeﬁnedassplittingag rid,and\nrewards measured by the runtime improvement from executing\nthe join queries. However, this method is constrained by its re-\nquirement to perform numerous actual joins to evaluate rewa rds,\nwhichistime-consuming.Meanwhile,Vuetal.[41]havedeve loped\nadeeplearningmodelthathelpstoselectthebestpartition ingtech-\nnique for spatial joins. By training on various data distrib utions,\nthe model learns to correlate the characteristics of the dat a with\nthe eﬀectiveness of diﬀerent partitioning techniques. How ever, it\nstillencounterstheoverheadofpartitioningdataon-the- ﬂyatrun-\ntime.\nIn contrast, our work is fundamentally diﬀerent in that it us es\nmachinelearningnottoselectapartitioningalgorithm,bu ttoreuse\n3\n\nan existing partitioner.To thebest of ourknowledge, no pre vious\nwork has utilized a machine learning approach to learn simil ari-\ntiesbetweenspatialjoinqueriesbasedontheirinherent da tachar-\nacteristicsandqueryembeddings.Ourframework leverages these\nlearnedsimilaritiestooptimizenewjoinexecutionsbyreu singpar-\ntitioners from similar pastqueries, avoiding theneed toco nstruct\na partitioner from scratch.This capabilitytogeneralize a cross dif-\nferent join conﬁgurations signiﬁcantly improves performa nce, as\ndemonstratedin ourexperiments.\n3 PRELIMINARIES AND PROBLEM\nDEFINITION\nIn this section, we formally deﬁne the terminologies used in this\npaperand present theproblemwe address.\n3.1 Preliminaries\nSpatialdata referstodataassociatedwithgeographicallocations,\ndenotedas/u1D45C(e.g.,thecoordinatesthatmarkthelocationofarestau-\nrant).Aspatialdataset /u1D446consistsofmultiplespatialdatarecords,\nsuch as a dataset that contains all the geographical locatio ns of\nrestaurants within acity, denotedas /u1D446={/u1D45C1,/u1D45C2,...,/u1D45C/u1D45B}.\nSpatial join is a database operation that identiﬁes pairs of ob-\njects from two spatial datasets that satisfy a speciﬁc spati al predi-\ncate/u1D703,e.g.,withinaspeciﬁcdistancefromeachother.Forexampl e,\naspatialjoincanbeusedtocombinehotelandrestaurantdat asets\ntoidentifypairsthatarewithin500metersofeachother.Fo rmally,\ngiventwospatialdatasets /u1D445and/u1D446,andaspatialdistance /u1D703,aspatial\ntheta joinbetween /u1D445and/u1D446based on/u1D703returns aset /u1D447,where\n/u1D447={/an}bracketle{t/u1D45C/u1D45F,/u1D45C/u1D460/an}bracketri}ht|/u1D45C/u1D45F∈/u1D445,/u1D45C/u1D460∈/u1D446,/u1D451/u1D456/u1D460/u1D461/u1D44E/u1D45B/u1D450/u1D452(/u1D45C/u1D45F,/u1D45C/u1D460)≤/u1D703}\nThere are four fundamental spatial join algorithms [39, 40] ,Block\nNestedLoopJoin(BNLJ) [7],DistributedJoinwithIndex(DJ) [3,15],\nRepartition Join (REPJ) [3, 15], and Partition based Spatial Merge\nJoin(PBSM) [28].BNLJisthemoststraightforwardapproach,where\nspatial datasets are divided into blocks. For each block fro m the\nﬁrst dataset, all blocks from the second dataset are loaded a nd\nprocessed using a nested loop method. BNLJis eﬃcient only in\nsmallerdatasetsasitdoesnotincorporatespatialpropert iesforﬁl-\ntering[7].Incontrast, DJshares afundamentalsimilarityto BNLJ\ninitsblock-basedprocessingapproachbutimprovesonitby utiliz-\ningspatialindexes.In DJ,datasetsareindexed,andobjectsclosely\nlocated are stored within the same partition block, which fa cili-\ntatesthematchingofoverlappingblocksduringpartitioni ng.REPJ\naddresses a critical limitation of DJthat occurs when the indices\nfrom both datasets diﬀer signiﬁcantly in structure due to va ria-\ntionsindatadistributionsandindex types.Thisdisparity canlead\nto ineﬃcient matching. To overcome this, REPJuses the index of\none dataset to repartition the other dataset, ensuring that the ge-\nometries overlapping from both datasets are located in the s ame\npartition block. PBSMdoes not require any indices on the joining\nattributes of either dataset. It partitions both inputs int o manage-\nablechunksandemploysaplane-sweepingtechniquetojoint hese\nchunks eﬃciently.\nDistributedspatialjoin referstotheexecutionofaspatialjoin\nacross multiplenodes withina distributedsystem As descri bed inSection2,adistributedspatialjointypicallyfollowsatw o-steppro-\ncess:globalpartitioningandlocaljoin.Figure2illustra testhispro-\ncess:twojoindatasets,representedbyblackcirclesandre drectan-\ngles,areinitiallydividedbyapartitionerintopartition blockssuch\nas B2 and B7, reﬂecting their data distribution. The data is t hen\nroutedtodiﬀerent blocksaccordingtotheirgeographicall ocation.\nTheblocksareprocessedbythecorrespondingworkernodes, such\nasW2andW7,toperformlocaljoins.Theresultsofeachnodea re\nsubsequentlyaggregated and reﬁned.\n3.2 Problem Statement\nOur objective is to optimize distributed spatial joins by re ducing\ndistributed partitioning overhead. We accomplish this by r eusing\nexisting partitioners and learning the similarities betwe en diﬀer-\nentspatialjoinconﬁgurations.Tothisend,weintroduceap roblem\nnamedSimilarity-basedDistributedSpatial Join ( SDSJ).\nTheinputofSDSJcomprises a series of historical spatial join\nqueries,denotedJ={/u1D43D1,/u1D43D2,...,/u1D43D/u1D45B}.Eachjoinquery,represented\nas/u1D43D/u1D456,involvesapairofspatialdatasets, /u1D445/u1D456and/u1D446/u1D456,linkedbyaspatial\npredicate/u1D703/u1D456. For each spatial join /u1D43D/u1D456, the system utilizes a speciﬁc\npartitioner/u1D443/u1D456. This partitioner is typically associated with one of\nthedatasetsinvolvedinthejoin—either /u1D445/u1D456or/u1D446/u1D456—andco-partitions\nthe other dataset. Each partitioner /u1D443/u1D456used is subsequently stored\nina repositoryforpotentialreuse infuturequeries.\nTheoutputisamachinelearningmodel /u1D440thatlearnsthesimi-\nlaritiesbetweendatasetsusingdatasetsembedding.Foran ewspa-\ntialjoinquery,thismodelmatchesthedatasetsinvolvedin thenew\njointoexistingonesintherepositoryandrecommendsanexi sting\npartitionertoexecutethenew joineﬃciently.Givenanew sp atial\njoinquery/u1D43D=(/u1D445,/u1D446),themodel /u1D440operatesas follows:\n•Determine Maximum Similarity : Identify the highest simi-\nlarityscorebetween /u1D445andalldatasetsinDand between /u1D446and\nalldatasets inD:\n/u1D460/u1D456/u1D45Amax=max(max\n/u1D437/u1D456∈D/u1D460/u1D456/u1D45A(/u1D445,/u1D437/u1D456),max\n/u1D437/u1D456∈D/u1D460/u1D456/u1D45A(/u1D446,/u1D437/u1D456))\nThismaximumvaluerepresentsthemostsimilarexistingdat aset\nto/u1D446and/u1D445, respectively.\n•Partitioner Reuse Decision : Evaluate whether /u1D460/u1D456/u1D45Amaxindi-\ncatessuﬃcientsimilarityforpartitionerreuse,ifso,ret rievethe\npartitioner/u1D443/u1D458associated with the dataset /u1D437/u1D458corresponding to\n/u1D460/u1D456/u1D45Amax.Otherwise,partitionthedataseton-the-ﬂy(repartition ).\n•Join Execution : Use the retrieved partitioner /u1D443/u1D458or a newly\nconstructed partitioner to partition both /u1D445and/u1D446. Perform the\nspatial join operation using these partitions, leveraging the ex-\nistingpartitioning scheme toimprove eﬃciency.\n4 SPATIAL DATA PARTITIONING\nWithout loss of generality, we realize our techniques on top of\nApache Sedona [1], formerly known as GeoSpark [49], as a ma-\njor open-source distributed system for processing big spat ial data\nextending Apache Spark.ApacheSedona oﬀers threepartitio ning\nalgorithmsforspatialjoin,namelyuniformgrid,quadtree [33],and\nKDB-tree partitioners [30]. The uniform grid partitioner d ivides\nthespatialuniverseintoaﬁxedgridofuniformcells,indep endent\nof the data distribution. While it is simple, it typically le ads to an\n4\n\nFigure2: OverallExecutionFlow of DistributedSpatial Joi n\nimbalanceinthenumberofdatapointsperpartitionduetoth etyp-\nical skew in the spatial distribution. This imbalance signi ﬁcantly\noverloads certain worker nodesand reduces overall eﬃcienc y.\nThe KDB-tree partitioner divides the spatial domain by recu r-\nsively splitting it based on the data distribution. However , its par-\ntitioning result can be sensitive to the sequence in which da ta\npoints are inserted. This dependency on the insertion seque nce\nintroduces instability, meaning that diﬀerent runs over th e same\ndataset mayproducediﬀerent partitions.\nThe quadtree partitioner recursively subdivides the spati al do-\nmain and produces consistent partitioning regardless of th e inser-\ntion sequence. This consistency in partitioning makes it id eal for\nour purposes, as we need a partitioner whose outcome can be re -\nliably reused across diﬀerent datasets with similar charac teristics.\nGivenitsadvantages ofconsistencyandbalancedpartition ing,we\nemploy the quadtree partitioner in SOLAR. In the original imple-\nmentationof ApacheSedona,thequadtreeis constructedbys can-\nning the dataset to determine the Minimum Bounding Rectangl e\n(MBR) of the data points and then sampling a subset of data to\nguidegrowingthetreetoacertaindepth.However,theorigi nalim-\nplementationinApacheSedonaconstructsthequadtreeonly over\nthe minimum bounding rectangle (MBR) of the current dataset .\nThis limited spatial coverage makes it unsuitable for reuse if new\ndatasetsextendbeyondthatMBR.Weintroducethefollowing mod-\niﬁcations:\n•Full Spatial Coverage: Instead of starting with a Minimum\nBounding Rectangle (MBR) that tightly ﬁts the data points, w e\nbeginpartitioningfromtheentireworld’sgeographicspac e.This\nensurescompletecoverage,makingtheresultingpartition sreusable\nfor variousdatasets,regardless of individual spatial ext ents.\n•AdaptiveDepthMechanism: Intheoriginal implementation,\nthe maximum depth of the quadtreeis determined by the num-\nber of Resilient Distributed Dataset (RDD) partitions used in\nApacheSedona,basedonSparkRDDs.Toimproveﬂexibility,w e\nintroduceanadaptivemechanisminwhichthedepthisdynam-\nically set as the maximum of two values: the number of RDD\npartitionsandauser-deﬁnedmaximumdepth.Thisensuresth at\nthequadtreeis deep enough tocapturethedatadistribution .\nThe objective is to make partitioning general enough to sup-\nport diverse workloads while being speciﬁc to capture the un der-\nlying spatialdistribution.Bydefault,thepartitionerin ApacheSe-\ndona[1]partitionstheleft-sidedatasetofthejoin.Forex ample,in\na join between restaurant andpark,therestaurant dataset is parti-\ntioned,anditspartitionerisalsousedtopartitionthe parkdataset.\nFigure3: DataHistogramConstruction\nAfter executing the join, we store the structure of the quadt ree\npartitioner—includingthespatialboundariesandhierarc hyofpar-\ntitionblocksondisk.Bystoringthispartitioner structur e,thesys-\ntem can eﬃciently reuse it for subsequent joins involving si mi-\nlar datasets. Reusing an existing partitioner enables the s ystem\nto route incoming data points directly to appropriate parti tions,\navoidingtheoverheadofrecomputingthepartitioningsche mefrom\nscratch.\n5 DATASET SIMILARITY EVALUATION\nIn this section, we present how to capture the distribution o f spa-\ntialdatasetsusinghistogram-basedmethodsandevalautet hesim-\nilarties between diﬀerent datasets based on their respecti ve his-\ntograms.\n5.1 DatasetStatisticsCollection\nToeﬀectivelycaptureandanalyzethedatadistributionofa dataset,\nwe compute its histograms. A histogram is a statistical tool that\ngroups data into bins; each bin represents a speciﬁc spatial range,\nandthecountwithineachbinindicates thenumberofdatapoi nts\nit contains. This method has been widely utilized in various stud-\nies [11,12,39–41]toanalyze datadistributions.Conseque ntly,we\ncompute data histograms for datasets to illustrate their da ta dis-\ntributions. Each bin in the histogram counts the number of da ta\npoints it contains, providing a detailed representation of the spa-\ntial distribution across the dataset. Figure 3 shows the spa ce di-\nvision and construction of a 2X2 data histogram for the restau-\nrantandparkdatasets. To utilize these histograms eﬀectively in\nour Siamese Neural Network, we convert them into vector repr e-\nsentations.Thisconversion involves ﬂattening thehistog ramgrid\ninto a vector. Speciﬁcally, we concatenate the counts row by row\nfromthehistogramtoformalargevector.Inthisexample,th evec-\ntorrepresentation ofthehistogramcorrespondingto restaurant is\n[12,3,4,4]. In our experiment, we always use high-resolution his-\ntogram, i.e., 8192 X 8192,as suggested in [40], due to its eﬀe ctive-\nness in capturingﬁne-grained details ofspatial distribut ion.\n5\n\n5.2 JSD-BasedSimilarityMeasure\nWe deﬁne the similarity between two datasets based on the dis -\ntancebetweenthevectorrepresentations of theirdatahist ograms\nthatarecomputedinSection5.1.Toevaluatethisdistance, weuse\ntheJensen-Shannondivergence( /u1D43D/u1D446/u1D437)[25],ameasureofsimilarity\nbetween probability distributions. In our case, /u1D43D/u1D446/u1D437is a highly ef-\nfectivemeasuretocomparehistograms.Whenitiscomputedw ith\nlogtobase2,itprovidesvaluesnormalizedintherange [0,1].This\nboundedrangemakesmodellearningmoreeﬀectivebecausean or-\nmalized range allows for more eﬃcient optimizationduringt rain-\ning. In addition, a similarity score within [0,1] is intuiti vely inter-\npretable, where 0 indicates no similarity and 1 indicates id entical\ndistributions. This makes it easier to understand and analy ze the\nmodel’s outputs. Unlike other distance metrics such as Eucl idean\ndistance,whichonlyquantiﬁespoint-wisediﬀerencesandi ssensi-\ntive to the scale of data, Jensen-Shannon divergence captur es the\nunderlyingdistributionalcharacteristics.Itreﬂectsbo ththeshape\nandoverlapofhistograms,providingamorerobustsimilari tymet-\nric thatis less inﬂuenced bymagnitude variations.\nAlower/u1D43D/u1D446/u1D437valueindicatesasmallerdistance,i.e.,ahighersim-\nilarity,betweenthedatasets.BeforecalculatingtheJens en-Shannon\ndivergence,thehistogramvectorsmustbenormalizedtofor mprob-\nability distributions. This normalization is achieved by d ividing\neach bin count by the total number of data points, transformi ng\nthe vector into a probability distribution where each bin’s value\nrepresents the probability of ﬁnding a data point in that bin . The\nJensen-Shannon divergence between the probability distri bution\nvectors˜/u1D43B1and˜/u1D43B2, corresponding to the original two histograms\n/u1D43B1and/u1D43B2,is deﬁned as follows:\n/u1D43D/u1D446/u1D437(˜/u1D43B1/bardbl˜/u1D43B2)=1\n2/u1D43E/u1D43F/u1D437(˜/u1D43B1/bardbl/u1D440)+1\n2/u1D43E/u1D43F/u1D437(˜/u1D43B2/bardbl/u1D440)\n/u1D440=1\n2(˜/u1D43B1+˜/u1D43B2)\n/u1D43E/u1D43F/u1D437(˜/u1D43B1/bardbl/u1D440)=/summationdisplay.1\n/u1D456˜/u1D43B1(/u1D456)log/parenleftbigg˜/u1D43B1(/u1D456)\n/u1D440(/u1D456)/parenrightbigg\n/u1D43E/u1D43F/u1D437(˜/u1D43B2/bardbl/u1D440)=/summationdisplay.1\n/u1D456˜/u1D43B2(/u1D456)log/parenleftbigg˜/u1D43B2(/u1D456)\n/u1D440(/u1D456)/parenrightbigg\nWhere/u1D440is the pointwise mean of the two probability distribu-\ntions. Each Kullback-Leibler divergences ( /u1D43E/u1D43F/u1D437) [24] term calcu-\nlatesthedivergencefromoneprobabilitydistributiontot hemean,\ngivingameasureofhowmuchinformationislostwhen /u1D440isused\nto approximate ˜/u1D43B1or˜/u1D43B2. In the example from Figure 3, the origi-\nnalvectorscorrespondingto restaurant datasetand librarydataset\nare/u1D43B1=[12,3,4,4]and/u1D43B2=[5,2,3,1], and their probability\ndistribution vectors are ˜/u1D43B1=[0.522,0.130,0.174,0.174]and˜/u1D43B2=\n[0.455,0.182,0.273,0.091]. Then, the Jensen-Shannon divergence\nbetween ˜/u1D43B1and˜/u1D43B2is calculated as follows. We ﬁrst computethe\npointwise mean vector /u1D440=[0.488,0.156,0.223,0.132]of˜/u1D43B1and\n˜/u1D43B2.Next,wecomputetheKullback-Leiblerdivergences,inour case\n/u1D43E/u1D43F/u1D437(˜/u1D43B1/bardbl/u1D440)=0.0152, and/u1D43E/u1D43F/u1D437(˜/u1D43B2/bardbl/u1D440)=0.0156. Finally, the\nJensen-Shannon divergence is /u1D43D/u1D446/u1D437(˜/u1D43B1/bardbl˜/u1D43B2)=1\n2×0.0152+1\n2×\n0.0156=0.0154.6SOLAROFFLINE PHASE\nSOLARoperates in two phases: oﬄine and online. In the oﬄine\nphase, it trains a model to capture similarities between dat asets.\nAlso, it evaluates some joins to trainthe modelto decide whe ther\namatchedpartitionerwithagivensimilarityissimilareno ughfor\nthe new join execution. The online phase uses this model to pr o-\ncess new join queries by reusing a suitable partitioner from the\nrepositoryorcreating a new oneas needed.\nIn this section, we detail the oﬄine phase. This phase includ es\nseveral keycomponents:(1)datasetembedding,(2)theappr oxima-\ntionofsimilarityusingaSiameseNeuralNetwork,and(3)de cision\nmodeltraining, each outlinedin Algorithm1and detailed be low.\nAlgorithm 1 SOLAROﬄine Phase\n1:Input:set of datasets{/u1D4371,/u1D4372,.../u1D437/u1D45B}, sets of training joins\n{/u1D43D1,/u1D43D2,.../u1D43D/u1D45B}\n2:Output:learned modelsfrom oﬄinephase\n3:Step1: DatasetEmbedding\n4:foreach dataset /u1D437/u1D456do\n5:/u1D452/u1D45A/u1D44F(/u1D437/u1D456)←Extractpolygonfeatures from themetadata\n6:endfor\n7:Step2: DatasetSimilarityLearning\n8:InitializeSiamese NeuralNetwork with twinbranches\n9:foreach pair ofembeddings /u1D452/u1D45A/u1D44F(/u1D446),/u1D452/u1D45A/u1D44F(/u1D445)do\n10:/u1D439(/u1D452/u1D45A/u1D44F(/u1D446)),/u1D439(/u1D452/u1D45A/u1D44F(/u1D445))←/u1D452/u1D45A/u1D44F(/u1D446),/u1D452/u1D45A/u1D44F(/u1D445)infeaturespace\n11:Computedistance /u1D451←/bardbl/u1D439(/u1D452/u1D45A/u1D44F(/u1D446))−/u1D439(/u1D452/u1D45A/u1D44F(/u1D445)/bardbl2\n12:Distance clamp ˆ/u1D451←/u1D451\n1+/u1D451\n13:ComputeMSEloss: /bardblˆ/u1D451−/u1D451/u1D43D/u1D446/u1D437/bardbl2\n2\n14:Updatenetwork weights tominimize loss\n15:endfor\n16:Step3: PartitionerReuseDecisionModel\n17:Initializeemptysets X,Y\n18:foreach join/u1D43D/u1D456inthetraining set do\n19:/u1D460/u1D456/u1D45A/u1D45A/u1D44E/u1D465←Identify themax similarityusingthenetwork\n20:/u1D4611←Reusingthebestexistingpartitionerrunthenewjoin\n21:/u1D4612←Buildinga new partitionertorunthejoin\n22:/u1D459/u1D44E/u1D44F/u1D452/u1D459←1 if/u1D4611</u1D4612 else 0\n23:Append/parenleftbig/u1D460/u1D456/u1D45A/u1D45A/u1D44E/u1D465,/u1D459/u1D44E/u1D44F/u1D452/u1D459/parenrightbigto(X,Y)\n24:endfor\n25:Traina Random Forest using X(similarity scores) as features\nandY(reuse a partitionervs. builda new one) as labels\n6.1 DatasetEmbedding\nWe have discussed utilizing Jensen-Shannon divergence to e val-\nuatedataset similarities based on their data histograms. H owever,\nourultimateobjectiveintheonlinephaseistousealearned model\nto directly evaluate the similarities between diﬀerent dat asets us-\ning only the dataset embeddings, bypassing the expensive st ep\nof histogram construction. This will be facilitated by lear ning a\nSiamese Neural Network, as Section 6.2 details. Consequent ly, it\nis important to develop a mechanism to eﬀectively embed thes e\ndatasets.\nTo achieve this, we design an encoding mechanism that lever-\nages the metadata associated with each dataset, speciﬁcall y the\n6\n\n0 50 100\nX020406080100120140YP olygon Covering\nCentroid\nBounding Bo x\nFigure4: DatasetEmbedding\nsizeofthedatasetanditspolygoncovering.Ourapproachin volves\nextracting meaningful information from the polygon coveri ng of\nthe spatial dataset, as shown in lines 3-6 in Algorithm 1. The em-\nbedding mechanism encodes each spatial dataset into a conci se\nyetexpressive numericalvector,capturingtheessential p olygonal\ncharacteristics necessary for accurate similarity learni ng. Speciﬁ-\ncally,fromthepolygonalcoveringofeachdataset,weextra ctsev-\neralmeaningfulgeometricfeatures,includingthe numberofpoints ,\npolygonalarea ,polygoncentroidcoordinates (/u1D450/u1D452/u1D45B/u1D461/u1D45F/u1D45C/u1D456/u1D451 /u1D465,/u1D450/u1D452/u1D45B/u1D461/u1D45F/u1D45C/u1D456/u1D451 /u1D466.alt),\nboundingbox (/u1D45A/u1D456/u1D45B/u1D465,/u1D45A/u1D456/u1D45B/u1D466.alt,/u1D45A/u1D44E/u1D465/u1D465,/u1D45A/u1D44E/u1D465/u1D466.alt),andthe compactness mea-\nsure,deﬁnedas(4/u1D70B×/u1D44E/u1D45F/u1D452/u1D44E)/(/u1D45D/u1D452/u1D45F/u1D456/u1D45A/u1D452/u1D461/u1D452/u1D45F2),toforma9-dimensional\nvector.\nThese geometric attributes concisely capture a dataset’s s hape,\nspatialextent,anddensity.Theboundingboxandcentroidc onvey\nlocation,thesizeandareatrackdatadensity,andthecompa ctness\ncaptures shaperegularity.Moreover,bylimitingourselve s tonine\ndimensions, we keep the embedding space small and eﬃcient fo r\nour Siamese Neural Network training. Figure 4 gives an examp le\nof a spatial dataset along with its polygon covering. In this exam-\nple, the number of points is 20, the area of the polygonis 6196 .79,\nthe centroid of thepolygons is (59.60,53.62), the bounding box is\n(11.90,1.04,98.81,99.04), and thecompactness is 0 .87.\nTo further improve the quality of the embeddings for eﬀectiv e\nlearning, weapplynormalization.Speciﬁcally,logarithm icscaling\nis employed fornumerical stabilityand normalizationof th epoly-\ngonareaandthenumberofpoints,whilespatialcoordinates under\nthe CRS projection system, i.e., a standard way of specifyin g how\nmapcoordinatesrelatetoreallocationsonEarth,arescale ddown\nbyafactorof106toensureconsistentnumericalmagnitudes.Such\nnormalization enhances the convergence stability of the Si amese\nNeural Network training process.\n6.2 DatasetSimilarityLearning\nInthis section,wepresent ourapproachtolearnthesimilar itybe-\ntween datasets using a Siamese Neural Network. This approac h\nleverages the embeddings of datasets based on dataset metad ata,\ni.e.,polygoncovering,toapproximatethesimilaritybetw eendatasets.6.2.1Overview of Siamese Neural Networks .Siamese Neu-\nral Networks [9] are designed for tasks that involve compari ng\npairsofinputstodeterminetheirsimilarity.Inourframew ork,they\nare essential for identifying a similar dataset in the repos itory, al-\nlowingustoreuseitspartitionerratherthanrecomputingo nefor\neachspatialjoinquery.Itworksbycomputingthedistanceo fpairs\nof input embeddings in the feature space, where a larger dist ance\nmeans less similarity and vice versa. These networks consis t of\ntwobranchesthatsharethesamearchitectureandweights.I nour\ncontext,theinputstotheSiamese Networkaretheembedding s of\ntwodiﬀerentdatasets,denoted /u1D452/u1D45A/u1D44F(/u1D446)and/u1D452/u1D45A/u1D44F(/u1D445).Eachbranchof\nthenetworkprocessesoneoftheseembeddings.Denotethetr ans-\nformed encoding of /u1D452/u1D45A/u1D44F(/u1D446)and/u1D452/u1D45A/u1D44F(/u1D445)in the feature space as\n/u1D439(/u1D452/u1D45A/u1D44F(/u1D446))and/u1D439(/u1D452/u1D45A/u1D44F(/u1D445)). Then we compute the Euclidean dis-\ntance/u1D451betweentheir embedding inthefeaturespace, i.e.,\n/u1D451=||/u1D439(/u1D452/u1D45A/u1D44F(/u1D446))−/u1D439(/u1D452/u1D45A/u1D44F(/u1D445))||2\nFurthermore, since the Jensen-Shannon divergence is bound ed to\nthe range[0,1), we apply a clamping function to map /u1D451into the\n[0,1)interval:\nˆ/u1D451=/u1D451\n1+/u1D451\nFinally,ˆ/u1D451willbethepredicationofthedistancebetweendataset\n/u1D446and/u1D445givenbytheSiameseNeuralNetwork,whichcorresponds\ntotheapproximationofthegroundtruthdistancebetweenth etwo\ndatasetsbased onthedatahistogram.\nOne notable property of the Siamese Neural Network is that if\nthe same dataset reappears—meaning its metadata embedding is\nunchanged—thedistanceinthefeaturespaceis zero.Thisen sures\nthemodelwillreliablyretrievethecorrectpartitionerfo ranyprevi-\nouslyencountereddataset.Asmanyenterpriseworkloadsin clude\nrepeatedqueriesonlargelyoverlappingdatasets,thispro pertycan\nsigniﬁcantly improve performanceinpracticalsettings.\n6.2.2Loss Function .To guide the network in learning a rep-\nresentation aligned with the Jensen–Shannon distance ( /u1D43D/u1D446/u1D437), we\nminimizeameansquarederrorlossbetweenthepredicteddis tance\nand theactualJensen-Shannon Divergence score, speciﬁcal ly,\nL=/bardblˆ/u1D451−/u1D451/u1D43D/u1D446/u1D437/bardbl2\n2\nwhereˆ/u1D451is the predicted distance given by the network, as il-\nlustrated above, and /u1D451/u1D43D/u1D446/u1D437is the groundtruth distance computed\naccordingtothe /u1D43D/u1D446/u1D437measure betweendatahistograms.\n6.2.3ModelArchitecture .OurSiameseNeuralNetworkisbased\nonamodulardesignmotivatedbytheintuitionthatdiﬀerent dataset\nfeatures carry distinct semantic meanings and varying degr ees of\ncomplexity.Thus,processingthesefeaturesindividually allowsthe\nnetwork to capture their unique characteristics more eﬀect ively.\nSpeciﬁcally, we partition each dataset’s 9-dimensional me tadata\ninto ﬁve groups: number of points, area, centroid, bounding box,\nandcompactness.Eachgroupisprocessedindependentlythr ough\nits dedicated branch, implemented as specialized multi-la yer per-\nceptions.Forexample,scalarfeatureslikethenumberofpo intsand\narea are processed by simpler networks due to their limited c om-\nplexity, whereas spatially informative features such as ce ntroid\nand bounding box coordinates—which inherently contain ric her\n7\n\nFigure5: SiameseNeuralNetwork\nspatial relationships—are handled by deeper subnetworks. Given\ntheirvaryingsemanticmeaningsandcomplexity,eachfeatu regroup\nis embeddedindependently throughdedicatedmulti-layer p ercep-\ntrons (MLPs), denoted as follows:\n•Numberof points (/u1D45B/u1D44B) is processedvia:\n/u1D438/u1D434(/u1D45B/u1D44B)=ReLU/parenleftBig\n/u1D44A(/u1D434)\n2ReLU/parenleftbig/u1D44A(/u1D434)\n1/u1D45B/u1D44B+/u1D44F(/u1D434)\n1/parenrightbig+/u1D44F(/u1D434)\n2/parenrightBig\n•Area of thePolygon (/u1D44E/u1D44B) is encoded similarlythrough:\n/u1D438/u1D435(/u1D44E/u1D44B)=ReLU/parenleftBig\n/u1D44A(/u1D435)\n2ReLU/parenleftbig/u1D44A(/u1D435)\n1/u1D44E/u1D44B+/u1D44F(/u1D435)\n1/parenrightbig+/u1D44F(/u1D435)\n2/parenrightBig\n•Centroid(/u1D450/u1D44B/u1D465,/u1D450/u1D44B/u1D466.alt)isprocessedthroughadeepersubnetwork:\n/u1D438/u1D436(/u1D450/u1D44B/u1D465,/u1D450/u1D44B/u1D466.alt)=ReLU/parenleftBig\n/u1D44A(/u1D436)\n2ReLU/parenleftbig/u1D44A(/u1D436)\n1[/u1D450/u1D44B/u1D465,/u1D450/u1D44B/u1D466.alt]/u1D447+/u1D44F(/u1D436)\n1/parenrightbig+/u1D44F(/u1D436)\n2/parenrightBig\n•BoundingBox(/u1D44F/u1D44B/u1D45A/u1D456/u1D45B/u1D465,/u1D44F/u1D44B/u1D45A/u1D456/u1D45B/u1D466.alt,/u1D44F/u1D44B/u1D45A/u1D44E/u1D465/u1D465,/u1D44F/u1D44B/u1D45A/u1D44E/u1D465/u1D466.alt)is processedby:\n/u1D438/u1D437(/u1D44F/u1D44B)=ReLU/parenleftBig\n/u1D44A(/u1D437)\n2ReLU/parenleftbig/u1D44A(/u1D437)\n1[/u1D44F/u1D44B/u1D45A/u1D456/u1D45B/u1D465,/u1D44F/u1D44B/u1D45A/u1D456/u1D45B/u1D466.alt,/u1D44F/u1D44B/u1D45A/u1D44E/u1D465/u1D465,/u1D44F/u1D44B/u1D45A/u1D44E/u1D465/u1D466.alt]/u1D447\n+/u1D44F(/u1D437)\n1/parenrightbig+/u1D44F(/u1D437)\n2/parenrightBig\n•Compactness (/u1D450/u1D45C/u1D45A/u1D45D/u1D44B) is embeddedvia:\n/u1D438/u1D438(/u1D450/u1D45C/u1D45A/u1D45D/u1D44B)=ReLU/parenleftBig\n/u1D44A(/u1D438)\n2ReLU/parenleftbig/u1D44A(/u1D438)\n1/u1D450/u1D45C/u1D45A/u1D45D/u1D44B+/u1D44F(/u1D438)\n1/parenrightbig+/u1D44F(/u1D438)\n2/parenrightBig\nHere,each/u1D44A(∗)\n/u1D456and/u1D44F(∗)\n/u1D456representlearnableparameters(weights\nand biases) of their respective fully connected layers. Aft er these\nsubnetworks independently process each component, the res ult-\ningembeddingsareconcatenatedintoanintegratedreprese ntation\n/u1D438/u1D450/u1D45C/u1D45A/u1D44F(/u1D452/u1D45A/u1D44F/u1D44B).\nFinally,toproduceauniﬁedandcompactembeddingcapturin g\ninteractions amongdiﬀerent features,we introducea fusion layer :\n/u1D439(/u1D452/u1D45A/u1D44F/u1D44B)=ReLU/parenleftBig\n/u1D44A(/u1D453/u1D462/u1D460/u1D456/u1D45C/u1D45B)\n2ReLU/parenleftbig/u1D44A(/u1D453/u1D462/u1D460/u1D456/u1D45C/u1D45B)\n1/u1D438/u1D450/u1D45C/u1D45A/u1D44F(/u1D452/u1D45A/u1D44F/u1D44B)/parenrightbig\n+/u1D44F(/u1D453/u1D462/u1D460/u1D456/u1D45C/u1D45B)\n1+/u1D44F(/u1D453/u1D462/u1D460/u1D456/u1D45C/u1D45B)\n2/parenrightBig\n.\nThis fusion step merges the complementary information from\neachbranch,producingacompactembeddingthatcomprehens ivelyencodes the dataset’s characteristics. Figure 5 illustrat es the net-\nworkarchitecture.\n6.2.4Summary .The primary goal of this model is to learn a\nfunction that computes the distance between datasets using em-\nbeddingsfromdatasetmetadatatoapproximatetheactualdi stances\ncomputedusing histograms.Alarger distancerepresents le ss sim-\nilarityand vice versa. Notice that a simple partitioner loo kupwill\nworkonlyforqueriesthathavebeenseenbefore.Usingthemo del\nworksfornew queriesthataresomewhat similaryet notident ical\nto the previous queries. The pseudocodefor training the Sia mese\nNeuralNetwork is given inlines 7-15in Algorithm1.\n6.3 Partitioner Reuse Decision\nSection 6.2 discussed using the Siamese Neural Network to es ti-\nmatethedistancebetweendatasets.However,selectingthe appro-\npriatesimilaritythresholdtodetermine whethertoreusea nexist-\ningpartitioner remains challenging, as theoreticallymod elingthe\ncomplexityof spatialjoins is a diﬃcultoptimizationprobl em[40].\nTo address this, in the oﬄine phase, we execute a subset of joi n\nqueries with and withoutreusing thematched partitioner,r ecord-\ningtherespective runtimes.This empirical data allowsust otrain\na lightweight classiﬁcation model to decide, at runtime, wh ether\npartitionerreuseislikelytobebeneﬁcial.Wecollectempi ricaldata\nfroma series ofjoin experiments. Speciﬁcally,we record:\n•similarity score for each dataset pair, as produced by the net-\nwork.\n•Theruntime /u1D4611if wereuse anexisting (matched) partitioner.\n•Theruntime /u1D4612if weconstructa new partitioneron-the-ﬂy.\nIn principle, reusing a partitioner is beneﬁcial if /u1D4611</u1D4612. How-\never, if the best match in our repository is only weakly simil ar to\nthecurrent dataset,thereused partitionermight besubopt imalor\neven cause the join to fail due to heavily skewed partitions. We\n8\n\ntreatthisasaclassiﬁcationtask:giventhesimilaritysco re,wepre-\ndict whether reusing the existing partitioner with the give n best\nsimilarity scorewillbefaster (i.e., /u1D4611</u1D4612) or not.\nWe choose a random forest classiﬁer [8] because it is eﬀectiv e\nincapturingcomplexdecisionsurfacesviaanensembleofde cision\ntrees, while inherently mitigating overﬁtting through bag ging. In\nourusecase,thismeanswecanaccommodatecaseswhereasmal l\nincreaseinsimilaritymightdrasticallyalterpartition- reuseperfor-\nmanceforcertaindatasetpairs.Welabeleachtraininginst anceas\n1 if/u1D4611</u1D4612, and 0 otherwise. We then train a random forest on\nthese labeled samples, using only the similarity score . The pseu-\ndocodeof training the decision model is given in line 16-25 i n Al-\ngorithm 1.Wecall this thedecisionmaker, which is used tode ter-\nmine whether thematchedpartitionershouldbeusedtoparti tion\nthenew incoming join withthegiven similarityscore.\n6.4 ModelMaintenance\nThesystem’sdatarepositoryandworkloadevolveovertime. There-\nfore,itisessentialtomaintainthelearningmodelsothatt heycon-\ntinue to capture the distribution of new datasets. To addres s this,\nweadoptastrategyinspiredby[52],whichadvocatesperiod icand\nfeedback-based retraining:\n•PeriodicRetraining: Wescheduleregularretrainingintervals\n(e.g., weekly or monthly, depending on the workload update\nrate), during which we collect all newly arrived datasets an d\nany newly logged join feedback. At the end of each interval,\nwe retrain the Siamese Neural Network with the expanded set\nof dataset embeddings, ensuring the embedding space reﬂect s\nthebroaderdatadistribution.OncetheupdatedSiamesemod el\nis validated,wealsoretraintheRandomForestdecisionmod el\nonthenew similarityscores and observed runtimelabels.\n•Feedback-basedRetraining: Beyondﬁxedtimers,thesystem\nadministratorcanmonitorrecentqueryexecutions—especi ally\ncaseswherereusingapartitioneryieldssuboptimalruntim eor\nfails tomaintain balanced partitions.If repeated anomali es are\ndetected,wetriggeranearlierretrainingcycle.Thisenab lesthe\nmodels to adapt promptly to abrupt changes in data distribu-\ntions ortheintroductionoffundamentally new datasets.\n7SOLARONLINE PHASE\nIn the online phase, SOLARemploys the trained Siamese Neural\nNetwork and the decision maker built in the oﬄine phase to pro -\ncess incoming spatial join queries eﬃciently. When a new joi n in-\nvolving datasets /u1D445and/u1D446arrives, the system aims to leverage ex-\nisting partitioners by identifying the most similar datase ts from a\nrepositoryD.TheprocedureisoutlinedinAlgorithm2asfollows:\n(1)EmbeddingGeneration :Computetheembeddingsforthenew\ndatasets,/u1D452/u1D45A/u1D44F(/u1D445)and/u1D452/u1D45A/u1D44F(/u1D446),usingthesameembeddingmethod\nappliedduringtheoﬄinephase. Thisinitialstepis capture din\nAlgorithm 2from lines 3 to5.\n(2)Similarity Computation : For each dataset /u1D437/u1D456in the repos-\nitoryD, compute the similarity between /u1D445and/u1D437/u1D456, denoted\nas sim(/u1D445,/u1D437/u1D456), and the similarity between /u1D446and/u1D437/u1D456, denoted\nas sim(/u1D446,/u1D437/u1D456), using the vectorized computation through the\nSiamese Neural Network, as shown in Algorithm 2 from lines6 to9.For each /u1D445and/u1D446,ﬁnd themaximumsimilarity values:\n/u1D460/u1D456/u1D45A/u1D45A/u1D44E/u1D465=(max\n/u1D437/u1D456∈Dsim(/u1D445,/u1D437/u1D456),max\n/u1D437/u1D456∈Dsim(/u1D446,/u1D437/u1D456))\n(3)Partitioner Reuse Decision : Use the decision maker, as de-\ntailedinSection6.3,todeterminewhethertoreusethematc hed\npartitionerwiththegivensimilarityscore.Thisdecision -making\nis reﬂected in lines 10 to12 ofAlgorithm 2.If a partitionreu se\ndecisionis notmade, anon-the-ﬂy partitioner is construct ed.\n(4)Join Execution :Applythepartitioner topartitionboth /u1D445and\n/u1D446,andexecutethespatialjoinusingthesepartitions.Theme ta-\ndata ofthedatasets is kept forsubsequenttraining.\nAlgorithm 2 SOLAROnline Phase\n1:Input:Join/u1D43D=(/u1D445,/u1D446), RepositoryD, trained Siamese Neural\nNetwork,Threshold /u1D703\n2:Output:Reused ornewly created partitionerfor /u1D445and/u1D446\n3:Step1: EmbeddingGeneration\n4:/u1D452/u1D45A/u1D44F(/u1D445)←Generate embeddingfordataset /u1D445\n5:/u1D452/u1D45A/u1D44F(/u1D446)←Generate embeddingfor dataset /u1D446\n6:Step2: SimilarityComputation\n7:/u1D460/u1D456/u1D45Amax(/u1D445)←max/u1D437/u1D456∈D/u1D460/u1D456/u1D45A(/u1D445,/u1D437/u1D456)\n8:/u1D460/u1D456/u1D45Amax(/u1D446)←max/u1D437/u1D456∈D/u1D460/u1D456/u1D45A(/u1D446,/u1D437/u1D456)\n9:/u1D460/u1D456/u1D45Amax←max(/u1D460/u1D456/u1D45Amax(/u1D445),/u1D460/u1D456/u1D45Amax(/u1D446))\n10:Step3: PartitionerReuseDecision\n11:y=DecisionMaker ( /u1D460/u1D456/u1D45A/u1D45A/u1D44E/u1D465)\n12:/u1D443/u1D458←matchedpartitioner if /u1D466.alt=1,elsecomputedon-the-ﬂy\n13:Step4: Join Execution\n14:Apply/u1D443/u1D458topartitionboth /u1D445and/u1D446\n15:Executethespatial joinusing thesepartitions\n8 EXPERIMENTAL EVALUATION\nInthissection,wepresentexperimentalevaluationstodem onstrate\ntheeﬃciencyof SOLARinexecutingdistributedspatialjoinsqueries.\nSection 8.1 introduces the experimental setup. Section 8.2 evalu-\nates the performance of SOLARagainst the baselines. Our experi-\nments aim toanswer thefollowingquestions.\n•How often does SOLARreuse an existing partitioning scheme\nfor new join queries?\n•Does the learning-based modelintroduce runtime overhead a t\nonline phase?\n•By how much can SOLARaccelerate distributed spatial joins\ncomparedtothebaselines?\n•Howdoes SOLARperformunderdiﬀerent joinpredicates?\n8.1 Experimental Setup\nInthissection,weoutlinetheexperimentalsettings,incl udingthe\nevaluation datasets, queries to evaluate, selection of the baseline\nalgorithms, performance measures, parameter settings, an d con-\nﬁgurationsfor ourcomputingclusters.\nEvaluation Datasets: We collected a total of 37 datasets from\nmultiple sources and across diverse geographic scales. i.e ., from\ncity-level to world-scale. These datasets include two traﬃ c colli-\nsion datasets with 1.7 million and 2.1 million data points, r espec-\ntively, corresponding to vehicle collisions in New York Cit y and\n9\n\nSeattle [20, 27], as well as a crime dataset containing 7.7 mi llion\ndatapointsfromChicago[29].Additionally,weobtainedad ataset\nfromtheTwitterAPI[38]comprising40milliondatapointsa nd33\ndatasetsfromOpenStreetMap[2].Speciﬁcally,datasetsfr omOpen-\nStreetMap cover various points of interest such as shops, ﬁr e sta-\ntions, and libraries. These datasets span three regions: Ch ina, the\nUnitedStates,andtheentireworld.Toeﬀectivelytestours ystem’s\nperformance on large datasets, the original datasets from O pen-\nStreetMap are enlarged to sizes ranging between 5 million an d 50\nmilliondatapoints.Thisenlargement isachieved bymodeli ngthe\nspatial distribution of the original data using a two-dimen sional\nhistogramandgeneratingadditionaldatapointsbysamplin gfrom\nthisdistribution.Thisapproachensuresthattheaugmente ddatasets\npreserve the spatial characteristics of the original data. Our selec-\ntion of datasets allows us to evaluate SOLARacross diﬀerent geo-\ngraphic scales and spatial distributions. For our experime nts, we\nsplit each dataset into disjoint training datasets andtest datasets\n(by default, 80% train and 20% test). Training datasets are used to\ntrainourspatialpartitioningmodel. Testdatasets areusedtobuild\nspatial joins that are run against SOLAR. The objective is to reuse\na similar previously seenpartitioner whenever possible.\nThedatasetsfromthe trainingdatasets arepartitioned,andtheir\ncorrespondingpartitionersarestoredinourrepositoryto trainour\nmodel.Wealsoformpairs of trainingdatasets tocomputeground-\ntruthsimilarityviahistogram,asmentionedinSection5. testdatasets\nareunseenduringanypreprocessingoroﬄinephases,thuste sting\nthe generalizability of the model. However, it is expected t hat a\nportionofthe testdatasets sharesimilarspatialpropertiesand dis-\ntributiontosomeofthe trainingdatasets .Thisportionincreasesas\nthesizeof the trainingdatasets increases.\nQueries:/u1D446/u1D442/u1D43F/u1D434/u1D445executes spatial distance joins between pairs\nof datasets. We choose the query workload by randomly pairin g\nthetrainingdatasets toformtrainingjoins suchthatevery training\ndatasetappears in at least one join, and the totalnumber of train-\ning joins equals the number of training datasets . Meanwhile, the\ntest datasets are alsorandomlypaired togenerate test joins, which\nconsist of entirely new dataset pairs that never appeared du ring\ntraining.Bydesign,the trainingjoins andtestjoinssetsaredisjoint,\ni.e., notraining dataset appears in a test join. The total number of\ntrainingjoins and thenumber of test joinsis 40joins.\nNotice that in our evaluation of SOLAR, we follow these steps:\n(1) we train SOLARusing joins from the training joins, and (2)\nwe evaluate the performance by running queries against SOLAR.\nThesenewqueriescanbeeitherrepeatedqueries,i.e.,quer iesorig-\ninating from the training joins, or new queries, i.e., queri es orig-\ninating from the test joins. When we evaluate queries from th e\ntrainingjoins,weassess SOLAR’sabilitytodetectrepeatedspatial\njoins and usea pre-existing spatial partitioner.When we ev aluate\nqueries from the test joins, we assess SOLAR’s ability to evaluate\nthe spatial similarity between new, unseen datasets that we re not\nincluded in the training of SOLAR.SOLARthen needs to decide\nwhethertoreuseasimilarpre-existing partitionerorpart itionthe\ndata ontheﬂy fortheupcomingjoin.\nBaselineAlgorithms: Weusetwovariantsof ApacheSedona [1]\n(previouslyknownasGeospark[3]),thestate-of-the-arto pen-source\ndistributedsystemtoprocessdistributedspatialjoins,a sbaselines\nagainst our SOLAR. The two variants are denoted as Sedona-K ,which employs KDB tree as the partitioner, and Sedona-Q , which\nuses Quadtree as the partitioner. Notice that there are lear ning-\nbasedapproachesfromrelatedwork[17,41],buttheseappro aches\narenotdirectlycomparableto SOLAR.First,[17]onlysupportsself-\njoins and requires running an entire join repeatedlyto comp utea\nreward function, an impractical overhead for large dataset s (e.g.,\ntens of millions of points). Second, [41] focuses only on dec iding\nwhichof Sedona’s standard partitioners to use. Our evaluation al -\nreadycompares SOLARagainstallofSedona’sbuilt-inpartitioners\n(including KDB-tree and Quadtree), so the functionality of [41] is\ninherently covered aswecompare SOLARagainst allpossiblepar-\ntitioners ofSedona.\nPerformance Measures: We execute spatial join queries (ei-\nther from training joins ortest joins) and analyze their runtime\nusing several metrics. If SOLAR’s decision maker opts to reuse a\npartitioner,theobservedruntimeisreportedunder SOLAR.Other-\nwise, if reuse is found less eﬃcient, SOLARconstructs a new par-\ntitioner, reverting to the baseline. We detail the frequenc y of this\nreusein Section8.2.1.We reportthebestruntime, 25thperc entile,\nmedian(50thpercentile),75thpercentile,andworstrunti meacross\nall join executions. Since each join query varies in computa tional\ncomplexity due to diﬀerences in input sizes and spatial dist ribu-\ntions,wefurtherassesstheperformanceof SOLARcomparedtothe\nbaselineSedonausing speed-up ratio, deﬁned as/u1D45F/u1D462/u1D45B/u1D461/u1D456/u1D45A/u1D452(/u1D446/u1D452/u1D451/u1D45C/u1D45B/u1D44E)\n/u1D45F/u1D462/u1D45B/u1D461/u1D456/u1D45A/u1D452(/u1D446/u1D442/u1D43F/u1D434/u1D445),\nwhere/u1D45F/u1D462/u1D45B/u1D461/u1D456/u1D45A/u1D452(/u1D446/u1D452/u1D451/u1D45C/u1D45B/u1D44E)is the faster of Sedona-K andSedona-Q\nand/u1D45F/u1D462/u1D45B/u1D461/u1D456/u1D45A/u1D452(/u1D446/u1D442/u1D43F/u1D434/u1D445)is the runtime of SOLARin executing the\nquery.\nParameter Setting: For the Siamese Neural Network, the fea-\ntures of each dataset are grouped into ﬁve subsets: ( A) number\nof points, ( B) area, (C) centroids, ( D) bounding box, and ( E) com-\npactness. Subsets A, B, E (single-scalar inputs) each pass t hrough\natwo-layerMLPwith8and4hiddenunitsperlayer(ReLU),whi le\nC (2D) uses a two-layer MLP with 16and 8 units,and D(4D) uses\nadeepertwo-layerMLPwith32and16units.Theoutputsofthe se\nsubnetworks are concatenated into a 36-dimensional vector and\nfedintoatwo-layerfusionMLP(16andthen8hiddenunits,Re LU)\nto produce an 8-D embedding. We train with mean squared error\n(MSE) loss against the ground-truth Jensen-Shannon diverg ence.\nThetrainingusestheAdamoptimizerwithalearningratesel ected\nfrom0.0001,0.0003,0.001,0.003,0.01andweightdecayof0and0 .0001,\nchosenvia 5-foldcross-validation. Theﬁnalmodelis train ed with\nthebesthyperparametersonthefulltrainingsetusingabat chsize\nof24,forupto50epochswithearlystopping(patience10).T heran-\ndom forest model used in the decision maker is constructed wi th\n100trees,each ofmaximum depth5.\nComputingClusterConﬁguration: Ourexperimentsarecon-\nductedon a MicrosoftAzureclusterconsisting of twohead no des\n(E8V3,8cores,64GBRAM)andeightworkernodes(E2V3,2core s,\n16 GB RAM). All data is stored in HDFS, and we use Apache Se-\ndona[1]inYARN modetoexecutedistributedspatialjoinque ries.\n8.2 Performance Evaluation\nThissectionevaluates SOLARagainst thebaseline algorithms, i.e.,\nSedona-Q andSedona-K ,under various conﬁgurations.\n8.2.1PartitionerReuseFrequencyAnalysis .Figure6illustrates\nthe frequency with which an existing partitioner is selecte d for\n10\n\n0.50.60.70.80.91.0\n 20  40  60  80Partitioner Reuse Frequency Training Percentage (%)Dataset seen\nDataset unseen\nFigure6: Matching FrequencyVsTrainingDataPercentage\nincoming joins. We examine cases where upcoming joins are re -\npeated (i.e., were seen during SOLAR’s training) or not repeated\n(i.e., were not seen during SOLAR’s training). In this experiment,\nwe increase the percentage of datasets used in SOLAR’s training\nfrom 20%to80%ofouroverall spatialdatasets.\nThe ﬁgure highlights that when SOLARis presented with in-\ncoming joins composed of datasets encountered during train ing,\nitsmodelaccuratelyidentiﬁestheseasrepeatedoperation sandef-\nﬁcientlyreemploysapre-computedpartitioner.Thisabili tyallows\nSOLARtoeﬀectivelydetectandbypassredundantpartitioningfor\nrecurringjointasks.Thereasonisthatidenticaldatasets yieldiden-\ntical embeddings, resulting in a feature-space distance of zero, in-\ndicative ofmaximum similarity,as explained inSection6.\nHowever,ifwebuildjoinsfrom testdatasets thathavenotbeen\ndirectly seen before during the training of SOLAR, as we increase\nthe number of training datasets used int the training of SOLAR,\nthe number of corresponding training joins grows proportionally.\nAs a result, the system encounters and stores a larger variet y of\ndataset pairs and their associated partitioners during the oﬄine\nphase. Consequently, the probability of successfully matc hing a\ndatasetfromanewjointoanexistingpartitionerincreases signiﬁ-\ncantly.ThisexplainstheupwardtrendobservedinFigure6, where\nthelikelihoodofpartitionerreusefornewincomingjoinsi mproves\nwith a larger set of training datasets andtraining joins . This fur-\ntherhighlights theapplicabilityandscalabilityofourar chitecture\nin industrial environments, where datasets are continuous ly inte-\ngratedintoproductionworkﬂows.Asthemodelaccumulatesm ore\ndatasets,therepositoryofpartitionersexpands,thereby enhancing\nthesystem’s capabilitytoeﬀectively reuse existing parti tioners.\n8.2.2Partitioning Phase Speed-up .Table 1 shows the speed-\nups achieved by the partitioning phase of SOLAR. It shows that\nSOLARachieves up to 2.71X faster compared to the baselines. In\ncontrast to the baseline approaches, which require two scan s of\nthe input data—ﬁrstto collectdata samples and calculateth emin-\nimum boundingrectangle (MBR)oftheinputforconstructing the\npartitioner, followed by a second scan to route the data to pa rti-\ntions—SOLARleverages an existing partitioner to immediately di-\nrect datatotheappropriatepartitions.\n8.2.3Overhead in Partitioner Matching .When executing a\nnewjoin, SOLARincurssomeoverheadduetotheuseofitslearned\nmodel.This overhead involves using theSiamese NeuralNetw ork\nto match incoming datasets with existing partitioners, and then,Table 1: Speed-upRatio for PartitioningPhase\nCase Worst 25th 50th 75th Best\ntrain 1.83 2.07 2.16 2.30 2.70\ntest 1.96 2.01 2.26 2.40 2.71\nbased on the similarity score, using the decision-maker to d eter-\nminewhether anexisting partitioner shouldbereused orif a new\npartitioner should be computed. Our experiments show that t he\nminimum, median, and maximum overhead incurred during the\npartitionermatchingstepare4.12ms,5.25ms,and14.29ms, respec-\ntively. The minimum, median, and maximum overhead incurred\nduringthedecision-makingstepare10.84ms,12.94ms,and5 1.73ms,\nrespectively. These minimal overhead times demonstrate th at the\ncomputationalcostassociatedwiththepartitionermatchi ngproce-\ndureis negligible comparedtothesubstantialruntimeredu ctions.\n8.2.4RuntimeUnderDiﬀerentTrainingDataSizes .Figure7\nand Figure 8 illustrate the runtime performance of SOLARcom-\npared to the baseline algorithms when executing distribute d spa-\ntial joins with varying percentages of training data. Speci ﬁcally,\nwe adjust the proportion of training datasets utilized during the\noﬄine training phase to be 20%, 40%, 60%, and 80% of the total\navailabledatasets.AsexplainedinSection8.1,thenumber oftrain-\ningdatasets directlyaﬀects thenumberof trainingjoins processed\nduring the oﬄine phase. Figure 7 demonstrates that SOLARcon-\nsistentlyoutperformsthebaselinealgorithmsinallcases fortrain-\ning joins, achieving a maximum speedup that is up to 3.6X. The\nspeedupachievable by SOLARstems from its Siamese Neural Net-\nwork’s ability to detect similarity in the embeddings of tra ining\nand test datasets. As we increase the size of the training dat asets,\nthe likelihood of SOLARdetecting similarity to a previously seen\npartitionerforaspatiallysimilardatasetincreases. Thi sallowsfor\nmorepartitionreuseandgreatercomputationalsavings.Th iscapa-\nbility eliminates the overhead associated with scanning da ta and\nperformingpartitioningoperationsat runtime.\nFigure8furthershowsthat SOLARachievesamaximumspeedup\nof 2.97X compared to the baselines for test joins. This improve-\nmentresultsfromtheeﬀectivenessof SOLAR’slearning-basedpar-\ntitionermatchingapproach.Althoughthematchedpartitio nerdoes\nnotperfectlyalignwiththedistributionofnewdatasets,i texhibits\nhighsimilarity.Thetimesavedbyavoidingon-the-ﬂyparti tioning\nsigniﬁcantlyoutweighstheadditionaloverheadincurredb yminor\nmisalignmentsduringlocaljoincomputations,resultingi nanover-\nallruntimereduction.Furthermore,increasingthenumber oftrain-\ningdatasets exposestheSiameseNeuralNetworktoawidervariety\nofdatasetpairs.Consequently,thenetwork becomesbetter atrec-\nognizing dataset similarities. Additionally, processing more joins\nduringtheoﬄinephaseresultsinstoringmorereusablepart ition-\ners. This increases the likelihood of successfully matchin g incom-\ning joins to previously computed partitioners during the on line\nphase.Collectively,thesuperiorperformanceacrossboth training\njoinsandtest joins highlights the eﬀectiveness and practicality of\nSOLARinreal-world settings.\n8.2.5RuntimeUnderDiﬀerentJoinPredicates .Figure9and\nFigure 10 compare the runtime performance of SOLARand the\n11\n\n 0 100 200 300 400 500\n 20 40 60 80Runtime (s)\nTraining Percentage (%)SOLAR\nSedona-Q\nSedona-K\n(a)Best Case 0 150 300 450 600\n 20 40 60 80Runtime (s)\nTraining Percentage (%)SOLAR\nSedona-Q\nSedona-K\n(b)25 Percentile 0 200 400 600 800\n 20 40 60 80Runtime (s)\nTraining Percentage (%)SOLAR\nSedona-Q\nSedona-K\n(c) 50 Percentile 0 250 500 750 1000\n 20 40 60 80Runtime (s)\nTraining Percentage (%)SOLAR\nSedona-Q\nSedona-K\n(d)75 Percentile 0 250 500 750 1000 1250\n 20 40 60 80Runtime (s)\nTraining Percentage (%)SOLAR\nSedona-Q\nSedona-K\n(e)Worst Case\nFigure7: RuntimeunderDiﬀerentPercentageof TrainingDat afortrainingjoins\n 0 100 200 300 400 500\n 20 40 60 80Runtime (s)\nTraining Percentage (%)SOLAR\nSedona-Q\nSedona-K\n(a)Best Case 0 150 300 450 600\n 20 40 60 80Runtime (s)\nTraining Percentage (%)SOLAR\nSedona-Q\nSedona-K\n(b)25 Percentile 0 200 400 600 800\n 20 40 60 80Runtime (s)\nTraining Percentage (%)SOLAR\nSedona-Q\nSedona-K\n(c) 50 Percentile 0 200 400 600 800 1000\n 20 40 60 80Runtime (s)\nTraining Percentage (%)SOLAR\nSedona-Q\nSedona-K\n(d)75 Percentile 0 400 800 1200 1600 2000\n 20 40 60 80Runtime (s)\nTraining Percentage (%)SOLAR\nSedona-Q\nSedona-K\n(e)Worst Case\nFigure8: RuntimeunderDiﬀerentPercentageof TrainingDat afortestjoins\n 0 100 200 300\n 200 400 600 800Runtime (s)\nJoin Distance (m)SOLAR\nSedona-Q\nSedona-K\n(a)Best Case 0 150 300 450 600\n 200 400 600 800Runtime (s)\nJoin Distance (m)SOLAR\nSedona-Q\nSedona-K\n(b)25 Percentile 0 200 400 600 800\n 200 400 600 800Runtime (s)\nJoin Distance (m)SOLAR\nSedona-Q\nSedona-K\n(c) 50 Percentile 0 200 400 600 800 1000\n 200 400 600 800Runtime (s)\nJoin Distance (m)SOLAR\nSedona-Q\nSedona-K\n(d)75 Percentile 0 400 800 1200 1600\n 200 400 600 800Runtime (s)\nJoin Distance (m)SOLAR\nSedona-Q\nSedona-K\n(e)Worst Case\nFigure9: RuntimeunderDiﬀerentJoin Distancesfor trainingjoins\n 0 100 200 300 400\n 200 400 600 800Runtime (s)\nJoin Distance (m)SOLAR\nSedona-Q\nSedona-K\n(a)Best Case 0 100 200 300 400 500\n 200 400 600 800Runtime (s)\nJoin Distance (m)SOLAR\nSedona-Q\nSedona-K\n(b)25 Percentile 0 150 300 450 600\n 200 400 600 800Runtime (s)\nJoin Distance (m)SOLAR\nSedona-Q\nSedona-K\n(c) 50 Percentile 0 150 300 450 600\n 200 400 600 800Runtime (s)\nJoin Distance (m)SOLAR\nSedona-Q\nSedona-K\n(d)75 Percentile 0 250 500 750 1000\n 200 400 600 800Runtime (s)\nJoin Distance (m)SOLAR\nSedona-Q\nSedona-K\n(e)Worst Case\nFigure10: RuntimeunderDiﬀerent JoinDistancesfor testjoins\nbaselines forworkloadsfrom trainingjoins ,i.e.,repeatedjoinsand\ntestjoins,i.e.,unseenjoins,underdiﬀerentjoinpredicates,i.e., join\ndistances.For trainingjoins ,SOLARachieves runtimesupto3.52X\nfaster compared to the most eﬃcient baseline. This speedup i s at-\ntributed to SOLAR’s ability to reuse previously encountered parti-\ntioners storedinthelocalrepository.Whenadataset inajo inhas\nbeen seen before, the model accurately matches the dataset t o its\ncorresponding partitioner, bypassing the need for real-ti me parti-\ntioning. For testjoins,SOLARachieves runtimes upto2.69Xfaster\ncompared to the best-performing baseline algorithm. This p erfor-\nmance improvement demonstrates the model’s strong general iza-\ntion capability. Additionally, the speedup percentage ach ieved by\nSOLARishigher whenthejoindistanceissmaller.Thisisbecause\npartitioning overhead dominates the total runtime at small er join\ndistances, making the eﬃcient reuse of existing partitione rs par-\nticularlybeneﬁcial. Conversely, as the join distance incr eases, the\nlocal join operations become more computationally intensi ve, di-\nlutingtherelativeimpactofpartitionerreuse.Neverthel ess, underpracticalspatialjoin predicates (e.g., within 1000meter s), therun-\ntimeimprovements provided by SOLARremain substantial.\n9 CONCLUSION\nThis paper proposes SOLAR, a scalable learning-driven approach\ntooptimizedistributedspatialjoins.Atitscore, SOLARleveragesa\nSiamese Neural Network to learn similarity representation s from\ndataset embeddings. These learned representations enable the ef-\nﬁcient reuse of eﬀective partitioners when faced with new jo in\nqueries.Our experimental ﬁndings consistently show that SOLAR\nachievessigniﬁcantspeedupsoverbaselinemethodsforbot hfamil-\niar and unseen join scenarios. For futurework, we plan to acc om-\nmodateother types of spatial joins including multi-way and kNN\nspatialjoins.Beyondspatialjoins, SOLAR’sapproachexempliﬁesa\ngrowingtrendtowardreuse-orientedandself-improvingda tabase\nsystems. By embedding and comparing queries or datasets, su ch\nsystems can selectively leverage prior work, including par tition-\ninglayouts,indexes, and even completequeryplans.\n12\n\nREFERENCES\n[1] [n.d.]. Apache Sedone. https://sedona.apache.org/.\n[2] [n.d.]. OpenStreetMap. https://www.openstreetmap.o rg.\n[3] 2015. Geospark: A Cluster Computing Framework for Proce ssing Large-scale\nSpatial Data, author=Yu, Jia and Wu, Jinxuan and Sarwat, Moh amed. In ACM\nSIGSPATIALInternationalConferenceonAdvancesinGeogra phicInformationSys-\ntems.1–4.\n[4] 2020. Eﬃcient Parallel and Adaptive Partitioning forLo ad-balancinginSpatial\nJoin, author=Yang, Jie and Puri, Satish. In IEEE International Parallel and Dis-\ntributed ProcessingSymposium (IPDPS) . 810–820.\n[5] Ablimit Aji, Fusheng Wang, Hoang Vo, Rubao Lee, Qiaoling Liu, Xiaodong\nZhang, and Joel Saltz. 2013. Hadoop-GIS: A High Performance Spatial Data\nWarehousingSystemoverMapReduce. 6,11 (2013).\n[6] AhmedMAly,AhmedRMahmood,MohamedSHassan,WalidGAre f,Mourad\nOuzzani,HazemElmeleegy, and ThamirQadah.2015. AQWA: Ada ptive Query\nWorkload AwarePartitioning ofBig SpatialData. 8, 13(2015 ).\n[7] Alberto Belussi, SaraMigliorini, and Ahmed Eldawy. 202 0. Cost Estimation of\nSpatial JoininSpatialhadoop. GeoInformatica 24, 4(2020), 1021–1059.\n[8] Leo Breiman.2001. Random Forests. Machinelearning 45(2001), 5–32.\n[9] JaneBromley,IsabelleGuyon,YannLeCun,EduardSäckin ger,andRoopakShah.\n1993. SignatureVeriﬁcationusinga\"Siamese\"TimeDelayNe uralNetwork. Ad-\nvances in neural information processingsystems 6 (1993).\n[10] JinChen,GuanyuYe,YanZhao,ShunchengLiu,LiweiDeng ,XuChen,RuiZhou,\nandKaiZheng. 2022. EﬃcientJoinOrderSelectionLearningw ith Graph-based\nRepresentation.In Proceedingsofthe28thACMSIGKDDconferenceonknowledge\ndiscoveryand data mining .97–107.\n[11] AbhinandanDas,JohannesGehrke,andMirekRiedewald. 2004. Approximation\nTechniques for Spatial Data. In Proceedings of the ACM SIGMOD International\nConference onManagement of Data .695–706.\n[12] Thiago Borges de Oliveira, Fábio Moreira Costa, and Vag ner José do Sacra-\nmento Rodrigues. 2016. Distributed Execution Plans for Mul tiway Spatial Join\nQueries using Multidimensional Histograms. Journal of Information and Data\nManagement 7, 3(2016), 199–199.\n[13] JeﬀreyDeanandSanjayGhemawat.2008. MapReduce:Simp liﬁedDataProcess-\ning onLargeClusters. Commun.ACM 51,1 (2008), 107–113.\n[14] Ahmed Eldawy,VagelisHristidis,SaheliGhosh,MajidS aeedan,AkilSevim,AB\nSiddique, Samriddhi Singla, Ganesh Sivaram,TinVu, and Yam ing Zhang. 2021.\nBeast: Scalable Exploratory Analytics on Spatio-temporal Data. InThe Confer-\nence on Informationand Knowledge Management (CIKM) . 3796–3807.\n[15] Ahmed Eldawy and Mohamed F Mokbel. 2015. Spatialhadoop : A Mapreduce\nFrameworkforSpatial Data.In IEEEInternational Conference onData Engineer-\ning (ICDE) .1352–1363.\n[16] Francisco García-García,Antonio Corral, Luis Iribar ne, Michael Vassilakopou-\nlos,andYannisManolopoulos.2020. EﬃcientDistanceJoinQ ueryProcessingin\nDistributedSpatialDataManagementSystems. InformationSciences 512(2020),\n985–1008.\n[17] KeizoHori,YuyaSasaki,DaichiAmagata,YukiMurosaki ,andMakotoOnizuka.\n2023. LearnedSpatialDataPartitioning.In ProceedingsoftheSixthInternational\nWorkshop on Exploiting Artiﬁcial Intelligence Techniques for Data Management .\n1–8.\n[18] Hanxian Huang, Tarique Siddiqui, Rana Alotaibi, Carlo Curino, Jyoti Leeka,\nAlekhJindal,JishenZhao,JesúsCamacho-Rodríguez,andYu anyuanTian.2024.\nSibyl:ForecastingTime-EvolvingQueryWorkloads. ProceedingsoftheACMon\nManagement of Data 2,1(2024), 1–27.\n[19] Edwin HJacoxandHananSamet.2007. SpatialJoinTechni ques.ACMTransac-\ntionson DatabaseSystems(TODS) 32, 1(2007), 7–es.\n[20] Kaggle. 2020. Seattle SDOT Collisions Data.\nhttps://www.kaggle.com/datasets/jonleon/seattle-sdo t-collisions-data.\nhttps://www.kaggle.com/datasets/jonleon/seattle-sdo t-collisions-data\n[21] Tim Kraska, Alex Beutel, Ed H Chi, Jeﬀrey Dean, and Neokl is Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the ACM SIGMOD In-\nternational Conferenceon Managementof Data . 489–504.\n[22] Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph H ellerstein, and Ion\nStoica.2018.LearningtoOptimizeJoinQuerieswithDeepRe inforcementLearn-\ning.arXiv preprintarXiv:1808.03196 (2018).\n[23] AniKristo,KapilVaidya,UgurÇetintemel,SanchitMis ra,andTimKraska.2020.\nThe Casefor a Learned Sorting Algorithm. In Proceedings of the ACM SIGMOD\nInternational Conference onManagement of Data .1001–1016.\n[24] SolomonKullbackandRichardALeibler.1951. OnInform ationandSuﬃciency.\nThe annals of mathematicalstatistics 22, 1(1951), 79–86.\n[25] Jianhua Lin. 1991. Divergence Measuresbased on the Sha nnon Entropy. IEEE\nTransactionson Informationtheory 37, 1(1991), 145–151.\n[26] Ryan Marcus and Olga Papaemmanouil. 2018. Deep Reinfor cement Learning\nfor Join Order Enumeration. In Proceedings of the First International Workshop\non ExploitingArtiﬁcialIntelligence Techniques for DataM anagement . 1–4.\n[27] NYCOpenData. 2023. New York City Motor Vehicle Collisi ons - Crashes.\nhttps://opendata.cityofnewyork.us. https://opendata. cityofnewyork.us[28] Jignesh M Patel and David J DeWitt. 1996. Partition base d Spatial-merge Join.\nACMSigmod Record 25,2(1996), 259–270.\n[29] ChicagoDataPortal.2023. ChicagoCrimeData. https:/ /data.cityofchicago.org/.\nhttps://data.cityofchicago.org/\n[30] John T Robinson. 1981. The KDB-tree: a Search Structure for Large Multidi-\nmensional Dynamic Indexes. In Proceedings of the ACM SIGMOD International\nConferenceon Managementof Data . 10–18.\n[31] Ibrahim Sabek and Tim Kraska. 2021. The Case for Learned In-Memory Joins.\narXiv preprint arXiv:2111.08824 (2021).\n[32] Ibrahim Sabek and Mohamed F Mokbel. 2017. On Spatial Joi ns in Mapreduce.\nInACM SIGSPATIAL International Conference on Advances in Geo graphic Infor-\nmationSystems .1–10.\n[33] Hanan Samet. 1984. The Quadtree and Related Hierarchic al Data Structures.\nACMComputing Surveys(CSUR) 16, 2(1984), 187–260.\n[34] TobiasSchmidt,AndreasKipf,DominikHorn,GauravSax ena,andTimKraska.\n2024. Predicate Caching: Query-driven Secondary Indexing for Cloud Data\nWarehouses.In Companionofthe2024International ConferenceonManageme nt\nofData. 347–359.\n[35] MingjieTang,YongyangYu,AhmedRMahmood,QutaibahMM alluhi,Mourad\nOuzzani,andWalidGAref.2020. Locationspark:In-memoryD istributedSpatial\nQueryProcessingand Optimization. Frontiersin bigData 3(2020), 30.\n[36] MingjieTang,YongyangYu,QutaibahMMalluhi,MouradO uzzani,andWalidG\nAref.2016. Locationspark:ADistributedIn-memoryDataMa nagementSystem\nfor Big Spatial Data. Proceedings of the VLDB Endowment 9, 13 (2016), 1565–\n1568.\n[37] JaimeTeevan,EytanAdar,RosieJones,andMichaelASPo tts.2007. Information\nRe-retrieval:Repeat Queries in Yahoo’s Logs. In Proceedings of the 30th annual\ninternationalACMSIGIR conferenceonResearchanddevelop ment ininformation\nretrieval.151–158.\n[38] TwitterInc.[n.d.]. TwitterDataviaAPI. https://dev eloper.twitter.com/en/docs.\n[39] Tin Vu, Alberto Belussi, Sara Migliorini, and Ahmed Eld awy. 2021. A Learned\nQueryOptimizerforSpatial Join. In ACMSIGSPATIAL International Conference\nonAdvances in Geographic InformationSystems .458–467.\n[40] TinVu,AlbertoBelussi,SaraMigliorini,andAhmed Eld awy.2024. ALearning-\nbasedFrameworkforSpatialJoinProcessing:Estimation,O ptimizationandTun-\ning.TheVLDB Journal (2024), 1–23.\n[41] TinVu,AlbertoBelussi,SaraMigliorini,and Ahmed Eld way.2020. UsingDeep\nLearning for Big Spatial Data Partitioning. ACM Transactions on Spatial Algo-\nrithmsand Systems(TSAS) 7,1 (2020), 1–37.\n[42] KaiboWang,YinHuai,RubaoLee,FushengWang,Xiaodong Zhang,andJoelH\nSaltz.2012.AcceleratingPathologyImageDataCross-comp arisononCPU-GPU\nHybridSystems.In ProceedingsoftheVLDBendowmentinternationalconference\nonvery large data bases .1543.\n[43] MeganWier,JuneWeintraub,ElizabethHHumphreys,Edm undSeto,andRajiv\nBhatia.2009. AnArea-levelModel of Vehicle-pedestrianIn juryCollisionswith\nImplications for Land use and Transportation Planning. Accident Analysis &\nPrevention 41,1 (2009), 137–145.\n[44] Peter Y Wu. 2023. Spatial Join in Spatial Data Analytics . InProceedings of the\nISCAPConference ISSN . 4901.\n[45] DongXie,FeifeiLi,BinYao,GefeiLi,LiangZhou, andMi nyiGuo.2016. Simba:\nEﬃcientin-memorySpatialAnalytics.In ProceedingsoftheACMSIGMODInter-\nnational Conference onManagement of Data .1071–1085.\n[46] Zhengtong Yan, Valter Uotila, and Jiaheng Lu. 2023. Joi n OrderSelection with\nDeepReinforcementLearning:Fundamentals,Techniques,a ndChallenges. Pro-\nceedings of the VLDBEndowment 16, 12(2023), 3882–3885.\n[47] JieYang,SatishPuri,andHuiZhou.2022.Fine-grained DynamicLoadBalancing\nin Spatial Join by Work Stealing on Distributed Memory. In ACM SIGSPATIAL\nInternational Conference on Advances inGeographic Inform ation Systems .1–12.\n[48] Simin You, Jianting Zhang, and Le Gruenwald. 2015. Larg e-scale Spatial Join\nQuery Processing in Cloud. In 2015 31st IEEE international conference on data\nengineering workshops .34–41.\n[49] JiaYu,ZongsiZhang,andMohamedSarwat.2019. Spatial DataManagementin\nApacheSpark:theGeosparkPerspectiveandBeyond. GeoInformatica 23(2019),\n37–78.\n[50] Xiang Yu, Guoliang Li, Chengliang Chai, and Nan Tang. 20 20. Reinforcement\nLearning with Tree-LSTM for Join Order Selection. In 2020 IEEE 36th Interna-\ntional Conferenceon Data Engineering(ICDE) .IEEE, 1297–1308.\n[51] Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin , Scott Shenker, and\nIon Stoica. 2010. Spark:Cluster Computing with Working Set s. In2nd USENIX\nWorkshopon HotTopicsin Cloud Computing(HotCloud 10) .\n[52] Rong Zhu, Lianggui Weng, Wenqing Wei, Di Wu, Jiazhen Pen g, Yifan Wang,\nBolinDing,DefuLian,BolongZheng,andJingrenZhou.2024. Pilotscope:Steer-\ningDatabaseswithMachineLearningDrivers. Proceedings oftheVLDBEndow-\nment17,5 (2024), 980–993.\n13",
  "textLength": 77403
}