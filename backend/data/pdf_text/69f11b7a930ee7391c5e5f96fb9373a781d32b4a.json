{
  "paperId": "69f11b7a930ee7391c5e5f96fb9373a781d32b4a",
  "title": "RLCache: Automated Cache Management Using Reinforcement Learning",
  "pdfPath": "69f11b7a930ee7391c5e5f96fb9373a781d32b4a.pdf",
  "text": "RLCache: Automated Cache\nManagement Using Reinforcement\nLearning.\nSami Alabed\nGirton College\nA dissertation submitted to the University of Cambridge\nin partial ful\flment of the requirements for the degree of\nMaster of Philosophy in Advanced Computer Science\nUniversity of Cambridge\nComputer Laboratory\nWilliam Gates Building\n15 JJ Thomson Avenue\nCambridge CB3 0FD\nUnited Kingdom\nEmail: sa894@cam.ac.uk\nOctober 1, 2019arXiv:1909.13839v1  [cs.LG]  30 Sep 2019\n\nDeclaration\nI Sami Alabed of Girton College, being a candidate for the M.Phil in Ad-\nvanced Computer Science, hereby declare that this report and the work de-\nscribed in it are my own work, unaided except as may be speci\fed below,\nand that the report does not contain material that has already been used to\nany substantial extent for a comparable purpose.\nTotal word count: 15,000\nSigned :\nDate :\nThis dissertation is copyright c\r2019 Sami Alabed.\nAll trademarks used in this dissertation are hereby acknowledged.\n\nAcknowledgements\nI would like to express gratitude to my supervisor Dr. Eiko Yoneki for valu-\nable and constructive suggestions during the planning and development of\nthis research work. I would also like to especially thank Michael Schaarschmidt\nfor the constant mentoring, intellectual discussions, and encouragements\nthroughout the various ups and downs of this project. Additionally, I would\nlike to thank Raad Aldakhil and Mihai Bujanca for their proof-reading of\nthis report and comments.\n\nAbstract\nThis study investigates the use of reinforcement learning to guide a general\npurpose cache manager decisions. Cache managers directly impact the overall\nperformance of computer systems. They govern decisions about which ob-\njects should be cached, the duration they should be cached for, and decides\non which objects to evict from the cache if it is full. These three decisions im-\npact both the cache hit rate and size of the storage that is needed to achieve\nthat cache hit rate. An optimal cache manager will avoid unnecessary oper-\nations, maximise the cache hit rate which results in fewer round trips to a\nslower backend storage system, and minimise the size of storage needed to\nachieve a high hit-rate.\nCurrent approaches assume characteristics of underlying data and use rule-\nbased mechanisms that are tailored for the general case. Caches are exposed\nto a changing workload regularly, especially the caches at a lower level of the\nstack. For example, a cache in front of a general purpose database that is used\nby several di\u000berent services observes a varying level of tra\u000ec patterns. Rule-\nbased methods are static and do not adapt to changes in workload, resulting\nin the cache operating in a sub-optimal state. Using reinforcement learning,\nthe system learns ideal caching policies tailored to individual systems and\nthe tra\u000ec pattern it observes without any prior assumption about the data.\nThis project investigates using reinforcement learning in cache management\nby designing three separate agents for each of the cache manager tasks. Fur-\nthermore, the project investigates two advanced reinforcement learning archi-\ntectures for multi-decision problems: a single multi-task agent and a multi-\nagent. We also introduce a framework to simplify the modelling of computer\nsystems problems as a reinforcement learning task. The framework abstracts\ndelayed experiences observations and reward assignment in computer systems\nwhile providing a \rexible way to scale to multiple agents.\nSimulation results based on an established database benchmark system show\nthat reinforcement learning agents can achieve a higher cache hit rate over\nheuristic driven algorithms while minimising the needed space. They are also\nable to adapt to a changing workload and dynamically adjust their caching\nstrategy accordingly. The proposed cache manager model is generic and\napplicable to other types of caches, such as \fle system caches. This project\nis the \frst, to our knowledge, to model cache manager decisions as a multi-\ntask control problem.\n\nContents\n1 Introduction 1\n2 Background and Related Work 5\n2.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 The Caching Problem . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 Introduction to caches . . . . . . . . . . . . . . . . . . 5\n2.2.2 Caching Strategy . . . . . . . . . . . . . . . . . . . . . 7\n2.2.3 Data Staleness . . . . . . . . . . . . . . . . . . . . . . 8\n2.2.4 Cache Capacity . . . . . . . . . . . . . . . . . . . . . . 9\n2.2.5 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Reinforcement learning . . . . . . . . . . . . . . . . . . . . . . 10\n2.3.1 Reinforcement Learning Essentials . . . . . . . . . . . 10\n2.3.2 Reinforcement Learning in Computer Systems . . . . . 14\n2.4 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3 RLCache: Intelligent Caching 19\n3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.2 RLObserver: Framework for delayed experience observations\nand modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3.2.1 RLObserver Architecture . . . . . . . . . . . . . . . . . 20\n3.2.2 Observer Delayed experience . . . . . . . . . . . . . . . 22\n3.3 RLCache: Caching Strategy . . . . . . . . . . . . . . . . . . . 23\n3.4 RLCache: Eviction Strategy . . . . . . . . . . . . . . . . . . . 28\n3.5 RLCache: TTL Estimation Strategy . . . . . . . . . . . . . . 31\n3.6 RLCache: Multi-Agents Con\fguration . . . . . . . . . . . . . 34\n3.7 RLCache: Single Agent Multi-Task Con\fguration . . . . . . . 36\n4 Evaluation 43\n4.1 Evaluation Aims . . . . . . . . . . . . . . . . . . . . . . . . . 43\n4.2 Experiments Setup . . . . . . . . . . . . . . . . . . . . . . . . 44\ni\n\n4.2.1 Simulator . . . . . . . . . . . . . . . . . . . . . . . . . 44\n4.2.2 Hyperparameters Selection . . . . . . . . . . . . . . . . 47\n4.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.3.1 Caching Strategy . . . . . . . . . . . . . . . . . . . . . 48\n4.3.2 Eviction Strategy . . . . . . . . . . . . . . . . . . . . . 51\n4.3.3 TTL Estimation . . . . . . . . . . . . . . . . . . . . . 54\n4.3.4 Multi-Task and Multi-Agent . . . . . . . . . . . . . . . 57\n4.3.5 RL Hyperparameter Initialisation . . . . . . . . . . . . 62\n4.3.6 Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n5 Summary and Conclusions 67\n5.1 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n5.2 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n5.2.1 Distributed Cache Manager . . . . . . . . . . . . . . . 68\n5.2.2 Model-based RL . . . . . . . . . . . . . . . . . . . . . 69\nii\n\nList of Figures\n2.1 Example of a simple system using caches. . . . . . . . . . . . . 6\n2.2 Simpli\fed reinforcement learning environment architecture. . . 11\n2.3 Simpli\fed actor-critic architecture. . . . . . . . . . . . . . . . 14\n3.1 Cache manager work\row. . . . . . . . . . . . . . . . . . . . . . 20\n3.2 Observer architecture. . . . . . . . . . . . . . . . . . . . . . . 21\n3.3 Illustration of RLCache environment's cached state. . . . . . . 24\n3.4 A simpli\fed view of the cache manager's multi-agent setup. . . 36\n4.1 YCSB client to cache manager communication. . . . . . . . . 46\n4.2 RLCache caching strategy learning overtime. . . . . . . . . . . 49\n4.3 RLCache caching strategy hit-rate evaluation. . . . . . . . . . 50\n4.4 RLCache caching strategy and caching rate evaluation. . . . . 51\n4.5 RLCache eviction strategy F1 evaluation. . . . . . . . . . . . . 53\n4.6 RLCache eviction strategy hit-rate evaluation. . . . . . . . . . 54\n4.7 TTL di\u000berence between optimal value and TTL estimation\nstrategy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.8 Cache hit-rate comparison when using a dynamically adjusted\nTTL strategy. . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n4.9 Cache hit-rate for multi-agent while varying workload. . . . . 59\n4.10 Cache hit-rate for multi-task agent on various workloads. . . . 60\n4.11 Caching rate for multi-task agent on various workloads. . . . . 60\n4.12 F1 score of multi-task agent on various workloads. . . . . . . . 61\n4.13 TTL di\u000berence between optimal value and RL multi-task agent. 62\n4.14 Hyperparameter initialisation impact on hitrate . . . . . . . . 63\n4.15 Hyperparameter initialisation impact on hitrate per step . . . 63\niii\n\niv\n\nChapter 1\nIntroduction\nComputer systems need to store their data in a reliable backend that ensure\ntheir durability and availability. These systems often include a cache for\ntemporarily storing information closer to where it is needed to reduce the\ncongestion on the permanent backends. Caches are designed to provide fast\nread and write access to the data locally.\nCaches have long been used to boost the performance of computer systems,\nsuch as in operating systems, \fle systems, and web servers. This work fo-\ncuses on web caches, but many ideas and design decisions are general and\ntransferable to other cache systems. Web caches are critical to the perfor-\nmance of many systems and directly linked to user satisfaction [3, 31]. An\nexperiment conducted by Google in 2009 demonstrated that increasing web\nsearch latency 100 to 400 milliseconds reduces the daily number of searches\nper user by 0 :2% to 0:6% [6]. Hence, optimising the performance of caches\nis important and the focus of our work.\nHowever, the e\u000bectiveness of the cache is limited by how many times a client\nrequests an object that is stored in the cache. Furthermore, the cache cannot\ngrow in size inde\fnitely, it needs to be managed and cleared of unwanted\nobjects. Hence the need for a Cache Manager (CM). CMs use a set of methods\nand strategies to decide on: what objects to cache, how long they should be\n1\n\ncached for, and what to remove when the cache is full. An optimal CM\nfocuses on storing hot keys for long duration without occupying more space\nthan necessary. Occupied space is an important factor for caches, especially\nwhen the cache is on constrained devices such as smartphones.\nCurrent approaches utilise heuristics to optimise performance for the general\ncase. While heuristics are designed to work for the general case, they do\nnot push the system to its optimal performance and to further optimise the\nsystem developers write a set of hard-coded rules. Additionally, even after\ntuning the system once, changes in the tra\u000ec patterns render the system in\na sub-optimal con\fguration. Tra\u000ec patterns commonly \ructuate in social\nnetworks and news sites, where a sudden news outbreak causes a signi\fcant\nshift in the tra\u000ec pattern.\nThis study investigates using Reinforcement Learning (RL) to automate the\noptimisation of the CM decisions. RL has the ability to learn optimal heuris-\ntics tailored to each unique pattern and dynamically adjust its decision-\nmaking policies by monitoring the system metrics. However, the delay in\ncomputer system poses a real challenge to RL algorithms that model their in-\nteraction as a sequential decision problem. When decisions are only observed\nin the future, there needs to be a mechanism to correlate RL agent's action\nto the update and impact to the computer system Service-Level-Objective\n(SLO). Additionally, a typical computer system has several SLOs that may\nrequire separate RL agents to learn di\u000berent policies. Therefore, there is a\nneed for an abstraction that simpli\fes the delayed experience and allow as\nmany agents to observe the impact of their actions simultaneously.\nThis work successfully implemented and evaluated several di\u000berent RL agents\nfor each one of the CM tasks. Furthermore, we investigated a multi-task\nagent that combines the three RL agents into a single agent that learns all\nthe three key actions a CM makes. We addressed the issues of applying\nRL algorithms to computer systems by designing a scalable framework that\nmonitors system metrics, relates past actions to current observations, and\nissues them to relevant RL agents. In our simulation running using YCSB\nbenchmark[12], an established benchmark in the database community, the\n2\n\nRL agents were able to learn patterns in the tra\u000ec and outperform traditional\nheuristic while requiring less storage and the ability to handle changes in the\ntra\u000ec pattern.\nThis dissertation's key contributions are:\n\u000fApplied advanced RL approaches to solve a long-researched problem\nin computer systems while tackling the issues that arise due to the gap\nbetween applied computer system research and RL typical controlled\nenvironment. Providing a detailed discussion and implemented solution\non issues such as delayed experience, modelling problem, and using\nSLOs to craft reward signals for RL agents.\n\u000fImplemented an online CM system that dynamically adapts to chang-\ning workload and learn optimal strategies for making caching decision\n(section 3.3), eviction decisions (section 3.4), and TTL estimation (sec-\ntion 3.5).\n\u000fThis work, as far as we are aware, is the \frst to successfully introduce\nthe use of a multi-task RL approach in a computer system (section 3.7).\n\u000fDesigned a framework for system problems when actions have a delayed\nimpact and demonstrated it by building three separate agents each\nlearning an independent delayed task (section 3.2).\nThis project does not address the latency concern overhead of using RL\nneither considers the distributed case of a CM, we discuss both cases in the\nfuture work subsection 5.2.1 section. We argue that our work stands by itself\nregardless, as the CM SLOs are the hit-rate, capacity, and utilisation, so\noptimising these provides a signi\fcant improvement to the status quo.\nThe dissertation structure is as follows: chapter 2 provides a brief background\nto caching, CMs, reinforcement learning, and using reinforcement learning in\na computer system context. Additionally, it surveys current literature. After\nthat, chapter 3 discusses the design behind the observer framework architec-\nture, a framework that simpli\fes scaling and using RL for computer systems.\nFurthermore, the section covers the algorithms, models, and implementation\n3\n\ndetails of the RL agents this work uses. Next, chapter 4 covers the eval-\nuation setup and evaluates the several agents' con\fgurations and compare\nthem to traditional algorithms. Finally, chapter 5 summarises the \fndings\nand discuss possible future directions.\n4\n\nChapter 2\nBackground and Related Work\n2.1 Background\nThis chapter sets the context for this work and summarises the background.\nThe \frst section introduces CMs and their common algorithms. Next, it\npresents reinforcement learning (RL) and two popular RL algorithms this\nwork uses: Deep Q-network and Soft Actor-Critic. Finally, the last section\nsurveys related work in applying optimisation techniques to CMs and com-\npares this project to the literature.\n2.2 The Caching Problem\n2.2.1 Introduction to caches\nLatency was found to be linked directly to users satisfaction and revenue\ngeneration in multiple studies [3, 31, 6]. Many applications use databases\nhosted in data centres to store persistent information. These databases write\nto single or distributed disks to ensure the persistence and durability of the\ninformation. However, the combined cost of network communication and\nthe long processing time to these databases pose a bottleneck to the system,\n5\n\nmaking it a challenge to scale the database to handle a large number of client\nrequests. The desire to improve the performance of the system motivated the\nsystem community to look into a fast storage system that has less durability\nand consistency guarantees but scales easily. These systems are called caches.\nThe cache stores a subset of the data and act as a layer that provides fast\ndata access to the client while reducing the tra\u000ec stress on the origin server\n[25, 39]. The cache can be located locally on a device (e.g. a smartphone\nmemory), on the edge of a network (e.g. a content delivery network CDN\n[11]), hosted near the database server (e.g. Redis [7]), or a mix of them all.\nFigure 2.1: A simple system that use geo-local cache, local cache, and a\npersistent database. With an example of possible sizes of those caches.\nTo demonstrate the \row of the data between the end-user, cache, and a\ndatabase, imagine a use-case of a cache in a news site where a video of the\nlatest news will be played by all new visitors of the site. The video is pushed\nto a geocache close to the users' location to improve their perceived latency\nand avoid relying a single backend system. When the video is requested,\nit is played from the cache providing a better experience to the end-users.\nThese users will also cache the content on their device for a much faster\nreplay experience, illustrating the ability of the layering stacks as an onion,\n6\n\nas shown in \fgure 2.1.\nCaches are useful if the stored contents are requested, to ensure that the rel-\nevant objects are cached and the cache size does not grow inde\fnitely, cache\nmanagers (CM) are used. The next subsections discuss the main mechanisms\nthat CMs govern.\n2.2.2 Caching Strategy\nCaching strategy decides whether an object should be cached or not. When a\nrequest asks for data that exists in the cache, that is called a cache hit , and if\nit is not in the cache, it is a cache miss . On a cache miss, the data is fetched\nfrom the slower backend and served to the client. Application developers\nhave to decide in advance on a caching strategy that chooses which objects\nto cache. These decisions are often based on the type of data and access\npattern. Here we consider two of the common caching strategies with their\nadvantages and disadvantages.\nWrite-through\nIn write-through policy, the application writes new data updates to both\nthe cache and the backend at the same time and only reports success when\nthe write succeeds in both places at the same time. This approach is useful\nfor applications that have read-after-write access pattern. For example, a\nsocial media post by a celebrity is very likely to be requested shortly after\na write, thus proactively caching their media post is the optimal strategy\nand improves the performance for early requesters. The disadvantage of this\napproach is the slower write speed as the write has to happen in two places,\nin addition to the potential of being an unnecessary operation. Unnecessary\nwrite operations pose an overhead when the job is a batch write job where\nthe inserted items will not necessarily be queried after, or when the request\nitself does not generate additional read, for example after a log entry.\n7\n\nWrite-on-read\nIn write-on-read loads data into cache only on reads. Upon a cache miss,\nwrite-on-read strategy retrieves the data from the slower backend and re-\nmembers the results for next time the item is requested. The data remains\nin cache until invalidated or exceed its expiry time. The disadvantage of this\napproach is that in applications where the client is guaranteed to issue a read\nafter a write, there is always going to be at least a single cache miss causing\nan additional network trip to the slower backend.\n2.2.3 Data Staleness\nData staleness occurs when an item is updated in the backend but not in\nthe cache. This can happen when a cache write fails but an update to the\nbackend succeeds, or if a database update bypassed the cache. To ensure\ndata freshness and to not clutter the cache with long-lasting objects, a CM\nsets a Time to live (TTL) on an object. TTL is an integer that represents\nthe seconds an object is stored in a cache until it is evicted (deleted). This\nmechanism guarantees a time-bounded weak consistency model [51] (TTL\nbounded), web-caches depend on this model to reduce manual invalidation\nrequests. An optimal TTL is large enough to maximise the hit-rate of an\nobject and expires before the cache observes an update or invalidation to this\nobject.\nA common mechanism for con\fguring a TTL is to con\fgure a \fxed (absolute)\nvalue at development time that is applicable to all objects. The value usually\nis a large number that the application SLO tolerates but not necessarily\nthe optimal. Overestimating the TTL populates the cache needlessly with\nunwanted objects leading to the cache \flling faster while underestimating\nthe TTL leads to more cache misses.\n8\n\n2.2.4 Cache Capacity\nCaches are limited in size, especially the ones deployed on a client device.\nWhen a cache exceeds its maximum capacity, a cache eviction algorithm\ndecides on which elements to evict. There are several other cache eviction\npolicies surveyed in [37]. We chose the three most widely used algorithms as\nour baselines.\nLeast recently used (LRU)\nLRU evicts the least recently used item based on the order of queries on the\ncache. LRU is the most widely used algorithm due to its simplicity and ap-\nplicability to real-world patterns. The rationale behind the algorithm is that\nif a client requests an item every K time, the item will remain in the cache of\nsize K inde\fnitely. Thus, popular items will be cached as long as the caching\ncapacity allows it. LRU only uses the access time when making a decision,\nand this can be a disadvantage when the data is sharded (partitioned) across\nmultiple databases that have varying access times to their objects some are\nslower than others. For example, assume that retrieving object A takes twice\nas long as another object B. If the access pattern favours B, LRU will evict\nthe one that was not accessed recently (A) and not the one that will improve\nthe overall system performance (B).\nFirst in \frst out (FIFO)\nFIFO is a simple algorithm that evicts the item that was \frst inserted in the\ncache regardless of the access pattern. FIFO is an useful algorithm for linear\nevents. e.g. in web browsing, a client going back to the last visited page is\na common browsing pattern, but returning to pages that they visited much\nearlier is uncommon.\n9\n\nLeast frequently used (LFU)\nLFU incorporates information about the access pattern by tracking objects\ncache hits and evicting the object with the least number of hits. It is most\nuseful for applications where a popular object is accessed throughout the\napplication life-cycle, like caching the icon of a website. However, LFU fails to\ncapture real-world tra\u000ec patterns. For example, in a social media a popular\npost might receive a large number of hits at some time frame, then lose\ninterest with time, a pure LFU cache will keep that post in cache even when\nusers are not interested in it anymore.\n2.2.5 Remarks\nThis section provided a quick overview of the caching problem, and high-\nlighted that there are always at least two ways to make decisions depending\non the use case. Providing a solid argument for using dynamically adjustable\npolicies that learn from the data and its access pattern. The next section cov-\ners the topic of reinforcement learning and the di\u000eculty of using it directly\nwith a system problem.\n2.3 Reinforcement learning\n2.3.1 Reinforcement Learning Essentials\nReinforcement learning (RL) is a sub-type of machine learning that learns\nhow to operate in any setting dynamically. It has been successfully applied in\nvarious problems such as playing Atari games [35], or control complex robots\n[28]. Formally, RL is a sequential decision process where an agent learns\noptimal policy through interaction with the environment [48], illustrated in\n\fgure 2.2. After each action, the agent observes the environment and learns\na correlation between an action and its impact on the environment. The\n10\n\nagent receives a reward from the environment that indicates how good the\naction was, and uses it to learn optimal actions.\nFigure 2.2: The interaction between an agent and an environment in a rein-\nforcement learning setting.\nThe goal of an agent is to maximise the expected sum of future rewards, that\nis done by training over many steps (state-action transitions), and choosing\nan action that returns the maximum discounted expected reward function.\nSutton et al. [48] proved that given enough training samples, maximising\nover expected reward function yields an optimal policy. A powerful prop-\nerty of RL algorithms is their ability to explore the environment. At each\nstep, the agent decides whether to exploit (reuse) the action that it knows\nhas the highest expected reward, or to explore other possible actions. The\nexploration-exploitation trade-o\u000b is what makes RL \rexible and able to han-\ndle new data that it has not seen during the training phase.\nRL algorithms fall into two categories, model-based and model-free. In a\nmodel-free, the agent has no previous knowledge of the system and performs\nactions in the real environment in order to learn their impact on the environ-\nment. Thus model-free requires no input from the developer, is very \rexible,\n11\n\nand easy to set-up. Conversely, model-based algorithms learn a model of\nthe environment, then the agent learns from the interaction with the result-\ning model. The model provides many more simulation steps for the agent,\nallowing it to converge towards the optimal policy faster. However, a model-\nbased approach is very challenging to design as the model has to re\rect the\nreal environment accurately. In a computer system where the environment is\nstochastic and subject to abnormalities all the time, a model-based approach\nremains an active research \feld. This project uses a model-free approach\nto demonstrate its feasibility in a real computer system problem. There are\nmany model-free algorithms available, in the next subsections we discuss two\nwe have used in this work that is also widely used for many applications.\nDeep Q-Network (DQN)\nQ-learning [57] is a reinforcement learning algorithm that learns optimal\ndiscrete actions for an environment. It works by learning an action-value\nfunctionQ(s;a) that takes in an action aand a state sand outputs a value\nof how good the action in that state is based on the reward from environ-\nment. The agent collects the Q-value of all possible state and action pairs\nin a dynamic programming approach, and then it navigates the environment\nby choosing the action that maximises the Q-value in a given state. This\napproach worked early-on for small games without many state and action\npairs. However, for complicated tasks such as large scale computer systems\nstoring all possible states is not feasible due to the high memory consumption\nof the dynamic programming approach.\nThe recent success in combining RL with deep learning [4] showed that it is\npossible to replace the state-action map with a neural network approxima-\ntor. As such, RL had numerous successes in Atari games [35, 46] that were\npreviously computationally prohibitive. The deep learning model approxi-\nmates the state-action results without having to store every pair in memory.\nThe model learns feature weights from agent action-reward observation and\nupdates the neural network weights through back-propagation.\n12\n\nCombining Q-learning with deep learning motivated DQN [55]. DQN is\na widely used general-purpose an o\u000b-policy algorithm that can learn from\ndata without interacting with the real environment. An o\u000b-policy algorithm\nis attractive in computer system problems as it enables easier scaling and\ndecoupling of processes. Follow-up work [36] introduced additional enhance-\nments to the standard DQN, which includes a dual-network and using a\nreplay bu\u000ber, these helped to make the algorithm robust by delaying the ef-\nfect of an update on the network causing to be less receptive to divergences.\nFurthermore, it allows the agent to warm up by \frst learning from logs and\ndemonstration such as in DQfD [22]. For these reasons, we utilise DQN in\nRLCache.\nSoft Actor Critic (SAC)\nAnother recently emerged algorithm is SAC [18, 19]. The actor-critic algo-\nrithms learn both a policy (actor) and a value (critic) function. The policy\nfunction approximates the best action in a current state, similar to DQN.\nThe value function represents how good is a given state, by estimating the\ntotal reward that is possible for the agent to get starting from this state.\nFigure 2.3 shows the actor learning the policy function based on the feed-\nback from the critic, while the critic is learning the value directly from the\nenvironment reward function[48].\nSAC is a robust o\u000b-policy algorithm that learns continuous actions in an\nenvironment. It tackles the exploration problem di\u000berently than most RL\nalgorithms. Typical RL algorithms aim to maximise the reward, while SAC\nmaximises both the entropy of policy and the reward. The entropy refers\nto the noise a random variable introduces, in this case, the action value,\nbecause SAC works for continuous actions the action value variable takes a\nreal-number value. Low entropy results in SAC assigning the same value to\nthe variable, while a high entropy encourages exploring other values. This\nway SAC is always exploring, and it is never stuck into local maxima. We\nomit the maths involved as it is not relevant and recommend the reader\n13\n\nFigure 2.3: The interaction between an actor and critic in an environment.\nto consult the original paper [18] for more details. We used an open source\nimplementation of SAC and DQN provided through the RLGraph framework\n[44].\n2.3.2 Reinforcement Learning in Computer Systems\nImagine a large scale system capable of self-optimised its resources in a way\nthat best \ft the SLOs. For example, this system can be a given CPU util-\nisation targets and set of possible actions, and it is tasked to maximise the\nCPU utilisation by assigning tasks to the system. Such a system requires less\nmaintenance, and allow non-system experts to achieve optimal performance\nwithout extensive knowledge of underlying architecture. That motivates the\nreinforcement learning in computer system research \feld.\nRL was notoriously tricky to apply to computer systems problems due to the\nlarge con\fgurations and states a system observes rendering RL algorithms\nine\u000ecient and incapable of learning in those settings. The introduction of\ndeep learning into RL renewed the interest of the systems research in em-\n14\n\nploying RL in key research areas of computer systems. For example, Mao et\nal. [34] used RL to learn optimal scheduling strategy of tasks in a large scale\ncluster, other work in the system includes optimising indexing in a database\n[45], and optimising distributed stream processor [30]. While RL has an\nenormous potential in computer systems, there are obstacles that make it\nchallenging to integrate RL into systems.\nDelayed State Transition\nTypical RL algorithms perform an action and observe a state transition to the\nnext state immediately. For example, in an Atari game, an agent choosing a\njump action, the system can immediately observe the jump and its e\u000bect on\nthe environment. Contrarily, computer systems observe the changes several\nminutes after performing an action. We tackle this issue in this work by\nintroducing a framework to abstract this (section 3.2).\nSample Ine\u000eciency\nRL algorithms are notoriously samples ine\u000ecient, i.e. they require many\ntraining samples and observations before they learn optimal strategies for\nthe system. Large quantities of training data are di\u000ecult to obtain in non-\nproduction system. The current approach is to use a simulator to generate\nnear realistic workload and observe it in a pre-production environment until\nthe model learns how to handle that type of data. The simulator generates\nworkload from a real production system trace or a collection of open-source\nbenchmarks. We describe in detail our approach to building a simulator in\nsection 4.2.\n2.4 Related work\nEviction strategies. Survey by Podlipnig et al. [37] introduces LRU, LFU,\nand FIFO argues that LRU is the most optimal strategy that works for the\n15\n\nmajority of caches. We evaluated the RL approach against the traditional\nstrategies in subsection 4.3.2. Results show that RLCache is capable of learn-\ning caching strategy in a changing in the workload better than the traditional\napproaches.\nCaching strategies. Alireza et al. [41] used RL to create an adaptive\ncaching policy using DQN for network objects caching. The policy learnt the\ndistribution of popular contents and achieved near optimal results. Their\nwork is similar to RLCache caching strategy described in section 3.3. How-\never, the strategy we describe generalises to other problem outside the net-\nwork caching domain and can learn result-sets, object keys, and other infor-\nmation. Authors in [2] explored a machine learning approach to predict the\ndemand of an object before caching it or replacing it. Their results outper-\nformed LRU in replacing objects and improved the hit-rate of the system\non a various capacity scale. Unfortunately, we were not able to obtain an\nartefact of their code to reproduce their results and compare it to ours for\nthe workload described in section 4.2. A working supervised learning ap-\nproach provides a reasonable motivation for a RL algorithm, as the RL agent\nwill learn the same policy the supervised learning did. While the supervised\nlearning approach will fail if exposed to di\u000berent tra\u000ec pattern and data\nthan what it observed during training phase. The RL approach generalises\nto new data that it did not observe during the training phase.\nTTL estimation . We drew inspiration from Schaarschmidt et al. [42]\nwork in estimating TTL. They used a variant of DQN for continuous action\nspace, namely normalised advantage function [16] to estimate CDN objects'\nTTL, they optimised for the result set of a cached key. Their algorithm's\nreward function maximised the number of cached objects if the cache is\nempty. In our work, we used a soft-actor critic algorithm (SAC), SAC o\u000bers\na better sampling e\u000eciency, robustness, and is better suited for continuous\nproblems [19], details of the algorithm is described previously in section 2.3.1.\nFurthermore, their work optimised the cache query results, while our work\nfocuses on the cache keys its unique patterns while minimising the capacity\nneeded.\n16\n\nDelayed experience . The work in [42] proposed a delayed experience injec-\ntion algorithm to calculate rewards for a CDN objects TTL estimation. The\nalgorithm stored an observed state for exactly estimated-TTL in a queue. At\neach time-step, a thread consumes an entry from the queue and observe the\ndi\u000berence between the stored state and the current environment. We extend\nthat concept further and incorporate it into a framework that generalises to\nother problems. The framework incorporates computer system termination\nsignals and SLO metrics. Additionally, it scales to a large number of agents\nlearning concurrently. We discuss the design in-details in subsection 3.2.2.\nRL in computer systems . We draw inspiration from [34] where they used\nRL for resource management, to improve the sampling e\u000eciency of their\nalgorithm they pause the simulation and output multiple actions per-step,\nwe utilise the same technique in the RLCache eviction strategy described\nin section 3.4 and output multiple keys to evict per eviction step. Other\nrelated work in computer systems [14, 53, 52] experimented with replacing\nthe handcrafted threshold policies that maintain SLOs with machine learning\nbased one. Under the same principle, we replace the cache hit rate and\nutilisation SLOs with RL enhanced one. While [45, 43] looked into optimising\nkey-value stores and databases index selection using RL, and caches are a\nhigh-performance type of key-value stores, they do not use indexes thus our\nwork is in a separate domain.\nRemarks. This work distinguishes itself from the aforementioned respected\nworks by developing a framework that incorporates various key contributions\nand generalises them, and fully automate the CM by designing agents capable\nof performing these tasks.\n17\n\n18\n\nChapter 3\nRLCache: Intelligent Caching\n3.1 Overview\nCaches are commonly used in computer systems to improve the performance\nof the overall system, they are added at various computation stacks and as\nsuch are a primary target for optimisation. A common use-case for a cache is\nto improve the performance of a large-scale database. Large-scale databases\nsupport a diverse range of workloads. Thus, caches need a cache manager\n(CM) to ensure the caching decisions provides the best performances trade-\no\u000bs. A CM decides whether an object should be cached, the duration it\nshould be cached, and if the cache is full what objects to remove, illustrated\nin \fgure 3.1.\nFor example, BigTable [9] supports the combination of both batch jobs and\nadhoc queries. Applying a caching layer on it helps with improving scalability\nand performance. However, because of the diverse nature of workloads, the\ncache needs to be \fnely tuned to reach optimal performance. That provides\nan opportunity to apply RL to the caching layer and dynamically learn ideal\nactions based on the tra\u000ec patterns. A smart CM can improve the cache\nhit-rate by not caching batch-job tasks which minimises the memory size\nrequired, thus results in reduced \fnancial costs and increases in performance.\n19\n\nAdditionally, it learns keys that have the least impact on the system if they\nwere evicted, resulting in reduced cache misses stemming from a poor eviction\ndecision.\nFigure 3.1: The cache manager work\row and various components in the\nsystem.\nThe rest of this chapter discusses applying RL to CM's components starting\nwith introducing the observer framework that is responsible for orchestrating\nthe communication between the CM and the RL algorithms.\n3.2 RLObserver: Framework for delayed ex-\nperience observations and modelling\n3.2.1 RLObserver Architecture\nComputer systems, as can be seen in the cache management problem, made\nof multiple components that each make a decision that contributes to the\noverall stability of the system. These decisions impact is only observed much\n20\n\nlater in the process. For example in the caching problem, the TTL estima-\ntion and caching strategy are two independent processes that contribute to\nthe overall hit-rate of the cache, the CM can only observe the impact of their\ndecision after the TTL is up or if the entry receives an invalidation update.\nA core contribution of this work is introducing a framework for delayed expe-\nrience observations that enables multiple components to independently make\nintelligent decisions.\nFigure 3.2: The work\row of the observer architecture and its fan-out design.\nFigure 3.2 shows the observer architecture, the observer distributes the sys-\ntem metrics observations to interested components in a fan-out fashion. Fan-\nout architecture [10] allows the system to distribute the observation to mul-\ntiple components without blocking the processor for responses, enabling it\nto scale for a large number of agents. By decoupling the observer logic from\nstrategies and providing a \rexible mechanism to register interest in various\nsystem metrics, it allows the system to scale as components can be added\nand removed without a\u000becting each other. When a CM receives a read re-\nquest, if the item in the cache it would signal a cache hit observation to the\nobserver, if not in the cache it would send a cache miss. On an invalidate\nrequest (change or addition of objects) the CM sends an invalidation signal\n21\n\nto let the components know the value they hold is no longer considered a\nfresh copy. Additionally, the framework allows the components to de\fne a\ncallback function for when an entry expires after TTL seconds. This proce-\ndure allows the component to learn from decisions that had no positive or\nnegative impact on the system.\n3.2.2 Observer Delayed experience\nWhen a system makes a decision, the environment reports the impact of the\ndecision much later. Therefore, the system needs to store incomplete trans-\nactions into an expiring queue. When a caching strategy makes a decision\nit enqueues the decision and a snapshot of the state into the queue. If the\nCM receives non-terminal updates (i.e. cache hits), it retrieves a mutable\nstate of the stored experience and updates the hit count. Otherwise, if the\nupdate invalidates what is in the cache, the stored experience is then consid-\nered completed and updated to re\rect the reason for termination, and then\nthe components invoke the termination procedure. If the CM does not ob-\nserve any more interaction with the object for the duration of the maximum\ncon\fgurable TTL seconds, the queue will invoke an eviction procedure. The\neviction procedure is a customs procedure de\fned by the subscriber com-\nponents, and it handles the work\row of when there is no more observation\nrelated to the (action, state) pair in the given observable time.\nThe expiry queue is a dictionary from key to stored object to provide a fast\nO(1) access to the object, and a min heap to keep track of objects that are\nnext going to expire. On a separate thread, the expiry heap is monitored\nevery second for keys that have expired, when a key is expired, an observation\nis enqueued in the observer with expiration as a reason and key, value, and\nthe cache metadata so the CM strategies can handle this later.\n22\n\n3.3 RLCache: Caching Strategy\nCaching strategy decides if an object after a read or write should be cached\nor not. An optimal caching strategy aims to maximise the cache hit rate\nwhile minimising the number of items occupying the cache by predicting if\nan object is going to be requested again in a short time-frame.\nSince real-life workloads vary, a binary caching strategy such as the one\ndescribed in subsection 2.2.2 will not be able to capture unique object access\npatterns in di\u000berent workloads. Take for example Twitter, where some users\nhave a large number of followers who reads the user posts as soon as post\nthem, and some users who produce a large number of tweets but have a fewer\nnumber of followers who do not read their posts. An intelligent cache should\nlearn the patterns of those users and base the caching decision accordingly.\nNeedlessly caching objects increase the network latency as these objects are\nwritten to both the cache and persistent backend, in a write-heavy workload\nthis is not desirable. Additionally, a binary decision to caching all the content\nis challenging due to capacity constraints, especially on constrained devices\nlike smartphones, and even dedicated hosted caches have a limited capacity\ncompared to the persistent backend. Therefore, a RL based caching strategy\nis desirable, and the rest of this section discusses the RL model, algorithm,\nand reward.\nAgent Design\nThe CM has no explicit end-state as the system operates inde\fnitely or\nuntil it crashes. Therefore, RLCache agents designs observe a reward after\nevery step or update without any terminal signal. Each cached object is an\nindependent state in the Markov decision process (MDP) [23]. The state\ntransition represents the transition of the cached object from the initial state\nwithout any cache hits, to the \fnal state that includes termination reason as\nillustrated by Figure 3.3. The \fnal state for each object is when the object\nis invalidated, evicted, or expired from the cache until then the cache state\n23\n\ncirculate under the hit state.\nFigure 3.3: The state representation of an object as observed by the agent.\nWe justify the continuous system design by experimenting with an episodic\ndesign, where we introduced an arbitrary terminal state (after every 10 ;000\noperations) for the system to observe an update and reset its internal states.\nHowever, the results were underwhelming in comparison to the continuous\nsystem as the agent was not able to adapt quickly when the workload changes,\nthis is visible when the workload switches to write-heavy and as such results\nin a large number of invalidation causing the termination signal to be reached\nmuch quicker. Additionally, real-world CM has no explicit termination signal\nand it operates until failure.\nState\nLearning the importance of an object and its behaviour requires knowledge\nof its unique identi\fer (the key), the access pattern, and the result set. Thus,\nthe agent state consist of a continuous space vector resulted by concatenating:\nobject key ,operation type ,results set ,object metadata , encoded termination\nreason , and the hit count .\n24\n\n\u000fObject key : For the agent to learn important (hot) keys, the state\nneeds to encode the key's unique representation. To transform the\nobject key from the String representation to a number before feeding\nit to the tensor, RLCache maintains a dictionary of observed keys to\ntheir encoding as is usually done in natural language processing tasks.\n\u000fOperation type : The operation type encodes the invocation reason of\nthe caching strategy. The operation type distinguishes between an\nobject queried from a Get request or from a Set to help the agent learn\nwhen the workload is a write-heavy or read-heavy workload.\n\u000fResults set : Results set are the values stored in the persistent database\nassociated with the object, passed into an embedding layer as com-\nmonly used in natural language translation tasks, to produce a vector\nof continuous space. The results set being part of the state helps the\nagent learn properties about the requested object. For example, the\ncache learns an association between '.jpg' in the results set and being\na hot item.\n\u000fObject metadata : The metadata contains a vector of result set size,\nTTL, and query retrieval times, guide the agent in learning to cache\nresults that tend to be slower to retrieve and maximise the utilisation\nof available cache space.\n\u000fTermination reason : The termination reason helps the agent distin-\nguish between an object that reached the end of its life cycle because\nof cache expiry, eviction, invalidation, or because of a miss. Thus the\nagent can learn the patterns of objects that are susceptible to many\ninvalidation.\n\u000fHit count : Contains the number of hits the object observed until it\nreached its termination state. Therefore, the agent able to learn hot\nobjects that received more hits are favourable than other objects.\nThis state representation provides the necessary context to learn individ-\nual object behaviour. Keys that have high hit rate tend to be heavy-read\n25\n\nkeys, keys that are invalidated frequently are heavy-write, adding result set\nmetadata allows the agent to maximise the utilisation of the cache capacity.\nAction\nBefore a CM caches an object, it queries the caching strategy agent for a\ndecision, whether to cache it or not. The caching decision is a discrete binary\nproblem, should cache (1) or should not cache (0).\nReward\nAn optimal caching strategy aims to maximise the cache hit rate while min-\nimising needless caching operations. The agent capture that objective using\nits reward function. The reward function described in 1 aims to reward the\nagent based on its caching decision.\n\u000fThe reward scales with the number of read requests to the object (hits),\nso the agent has the incentive to cache hot objects. If the agent mis-\npredicts that a cache read will follow, the agent is punished, thus dis-\ncouraging the agent from caching entries that tend to be invalidated or\nnot read until they are expired.\n\u000fThe reward rewards the cases where the agent does not cache an object\nand the object is not requested during its TTL lifespan to motivate the\nagent to avoid caching heavy-write and unpopular objects. The reward\nencodes a punishment for when the agent does mispredict.\nAdditional optional scaling functions can be used on top of the reward func-\ntion to re\rect the nature of the backend. Scaling the reward function by the\ntime it takes to retrieve the item from the backend allows the agent to favour\ncaching items that tend to be slower to retrieve. Scaling the reward function\nby the ratio of cache hits to the size of the result set discourages caching\nitems that tend to be large in size but are not requested often, minimising\nthe storage capacity needed to achieve the same hit-rate.\n26\n\nAlgorithm 1 RLCache: Caching Strategy Reward Function\nprocedure reward(action;observation )\nifaction ==shouldcache then\nreward =hitcount\nelse .Did not cache entry\nifobservation ==cachemiss then\nreward =\u00001\nelse\nreward = 1\nend if\nend if\nend procedure\nAlgorithm\nAlgorithm 2 shows the caching strategy. The agent uses an o\u000b-policy algo-\nrithm to learn from the interactions. The advantage of using an o\u000b-policy\nalgorithm is the ability to learn without interacting with the real environ-\nment. The property to learn from data directly simpli\fes the process of\nscaling the caching strategy, as discussed in subsection 5.2.1.\nThe caching strategy operates using two procedures. The \frst is the decision\nmaking one, Should Cache . This procedure responsible to decide whether\nto cache an object or not, the output is a boolean to indicate if it should\nbe cached. The procedure utilises the RL agent to make its decision, before\nfeeding the input to the agent it needs to be tensorised. The key and value\n(result set) are converted to an integer representation using the embedding\nlayer. After converting the object into its state representation, the agent\nconsumes the state vector representation and pass it to its network. The\nnetwork outputs a Q-value that predicts how good an action is given current\nstate. To encourage exploration, the agent chooses the action that maximises\nthe Q-value with probability 1 \u0000\u000f, otherwise, it uniformly samples a random\naction. Formally, the epsilon is used to indicate the probability of choosing a\nrandom action uniformly instead. The epsilon starts at 1, exploring random\nactions aggressively at the beginning, but linearly decays until it reaches a\n27\n\n0:1. This con\fguration allows the agent to choose the value the network\nis most con\fdent while leaving room for exploration that avoids the agent\nbeing stuck in a local maximum when the tra\u000ec pattern shifts. When it\nchooses an action, the observer framework creates an incomplete experience\nand stores it in an expiring queue, as discussed in subsection 3.2.2 as the envi-\nronment cannot provide the reward directly after but only in the future. The\nincomplete experience contains an immutable original state and the agent's\naction.\nThe second procedure is a non-blocking observe whose purpose is to update\nthe agent's network weights by rewarding (or punishing) the agent. It is\ninvoked by the observer (see section 3.2) to update the state associated with\nthe object key. When the CM observes an action to an object that it made a\ndecision on previously, it retrieves the stored state, creates a new transition\nto the new state while keeping the original state intact. If the new state is a\ncache hit, the cache hit count associated with that key increases. Otherwise,\nthe environment calculates the reward using the reward function described\nabove and passes it to the agent. The agent updates the network weight\nthrough back-propagation based on how good the action was. When an\nentry hits a \fnal state, it completes its MDP, and the observer removes it\nfrom the observed queue.\n3.4 RLCache: Eviction Strategy\nEviction strategy governs the removal of the objects from the cache process\nonce the cache is full. Optimal eviction strategy removes objects that are\nno longer used or has the least harmful impact on the SLO. The traditional\neviction policy will not be able to handle a case of mixed access pattern\nwithout relying on handwritten rules, here comes this work. RLCache learns\ninformation about the key, result set, and other system information and\nderives a bespoke policy for each object.\nTo motivate using RL for eviction strategy, imagine a partitioned database\n28\n\nAlgorithm 2 RLCache: Caching Strategy.\nGiven:\n\u000fan o\u000b-policy RL algorithm A. .e.g.DQN\n\u000fexploration decaying steps Es\nInitialise an empty replay memory R \u001e\nInitialise an empty expiry observer queue Q \u001e\nInitialiseAwith random weights\nInitialise\u000f 1:0 an exploration probability that linearly decays until 0 :05\nforEssteps.\nwhile CacheManager is alive do .Continuous System\nprocedure Should Cache (key;values;TTL;operation type)\ns convertToState (key;value;TTL;operation type)\nSelect action \u000b=A(s) with exploration probability \u000f\nCreate incomplete experience ie (s;\u000b;time;operation type)\nInsert incomplete experience into Q[key] =ieforTTL seconds.\nreturn the action \u000b\nend procedure\nprocedure async Observe (key;observation type)\nifobservation type ==cachehitthen\nstoredstate =Q[key]\nincrementstoredstate hit rate\nelse .Cache miss, invalidate, or Qentry expiry\nstoredexperience Q[key]\nCompute reward r=reward (storedexperience )\ninsert complete experience into R\ndelete entry associated with the key in Q\nCalculate the loss and update Anetwork's weight\nend if\nend procedure\nend while\nwhere some keys are stored on faster (or under-loaded) servers, while others\nare on slower ones. When the cache exceeded its maximum capacity, and an\neviction must happen before caching any new objects. Traditional algorithms\nmentioned in subsection 2.2.4 will evict objects without taking into account\nthe overall impact on the system as they only have simple information about\nthe cache, a problem RLCache does not su\u000ber from.\n29\n\nAgent Design\nMuch like the caching strategy agent described in section 3.3, the state de-\nsign mirrors the one used in the caching strategy. However, the algorithm,\naction and reward di\u000ber. The details of the RLCache eviction's algorithm\nare illustrated in listing 3, while we discuss the rest of the model below.\nState\nWe have experimented with two di\u000berent state design.\n1.Single-state cache: Models the whole cache as a single entity to allow\nthe agent to learn relative importance between keys. For example, the\nagent learns to compare keys based on their size and hit-rate. This\napproach does not scale when the cache size increases, as each state\nwill have a copy of the whole cache, and each transition as well. The\nstate grows exponentially, making it very di\u000ecult for the agent network\nto learn any distinctive features.\n2.Independent objects state: Treats each cached object as a separate\nstate, and feed a single object at a time into the agent, in a similar\nmanner to the caching strategy (section 3.3). This approach yielded\nthe best result, as the agent objective is to minimise cache misses and\nevict keys that are unlikely to be read again, regardless of how they\ncompare to each other.\nThe rest of this work considers only the second approach.\nAction\nWhen the cache is full, RLCache tasks the eviction strategy to nominate ob-\nject(s) to evict from the cache and make space for new entries. The eviction\ndecision is a discrete binary problem, should evict the object (1) or should\nnot (0). Initially, we would sample uniformly from the cache until we \fnd\n30\n\nan object to evict, thus mimicking the traditional eviction algorithm of one\neviction per call. However, this approach yields less training samples for the\nagent to learn from and overall slowed the system signi\fcantly. Instead, tak-\ning the idea proposed in [34], RLCache pauses the execution of the cache,\nscans every cached item and outputs an eviction decision on it, at the end\nof the procedure RLCache evicts all entries that the strategy chose. Conse-\nquently, the eviction strategy agent receives many updates to learn from.\nReward\nOptimal eviction strategy minimises the cache misses caused by premature\nevictions. The reward function described in listing 4 punishes the strategy\nfor evicting an object that caused a cache miss. Eviction strategy, mirrors\nthe caching strategy in both the objective function and the action it takes,\nthere are two actions an eviction strategy can do:\n\u000fEvicts an object: the environment rewards the agent if the evicted\nobject is later invalidated, and will punish it if it caused a cache miss.\n\u000fThe environment rewards the agent for not evicting an object that\nthe client reads later. Thus, motivating the agent to hold on useful\nobjects. However, if the object is later invalidated the environment\nissues a punishment to the agent.\n3.5 RLCache: TTL Estimation Strategy\nAdding a cache expiration time to objects avoids cluttering the cache and\nensures data is always fresh. The TTL strategy needs to estimate a TTL that\nexpires as soon as the object is no longer expected to be read again or before\nit is invalidated. Doing so maximises the cache hit rate and minimises over-\npopulating the cache with objects that are no longer useful. In a specialised\ntype of caches such as CDNs, TTL estimation is very crucial for the overall\nperformance of the system. High TTL overpopulate the cache by keeping\n31\n\nobjects longer in it, while low TTL results in fewer cache-hits. Furthermore,\nthere is a signi\fcant performance impact for sending manual invalidation of\nan object if still exists in the cache as it requires to send invalidation to\nall subscribed clients, causing a network latency overhead. The traditional\napproach of using \fxed TTL values cannot optimise for changing content and\nchanging tra\u000ec patterns. While RLCache learns optimal TTL parameter\nfrom observing the access patterns and the content of cached objects and\nestimates the optimal TTL that reduces the cache invalidation and utilises\nthe cache better. Dynamically changing TTL is useful even when the tra\u000ec\npattern is static. For example, the TTL for an object that contains a website\nlogo should be much higher than the TTL for an object that retrieves posts\nfrom a social media platform. In the former, the website logo is unlikely to\nchange, hence there is no need to invalidate as often as a social media object\nwhich may change on a much faster rate.\nAgent Design\nLike the previous two strategies, TTL estimation treats each object as an\nindependent state rather than the whole cache as one large state. However,\nTTL estimation is a continuous problem with a continuous action space,\nunlike the discrete actions that both eviction and caching strategies use. As\nsuch the agent method di\u000bers from the previous two strategies, the method\nhas to support continuous actions and needs to be an o\u000b-policy algorithm\nto facilitate scaling-out, SAC described earlier in section 2.3.1 ful\fls both\nrequirements.\nState\nTo estimate the TTL of an object, the agent needs to know information about\nboth the object and overall cache.\n\u000fObject information: Object unique key, the value set, the value\nset size, hit counts, and termination code, similar to the two other\n32\n\nstrategies discussed above.\n\u000fCache information: Information about the ratio of cached entities\nto the capacity of the cache (cache utility). Cache utility described in\n[42] allows the agent to reason with the trade-o\u000b between estimating\nhigher TTL when the cache is emptier and when to be conservative\nwith its estimation when it is close to its capacity.\nAction\nEstimating the number of seconds the entry should remain in the cache for\nis a continuous problem. The agent outputs a number between [0 ;1) to\nindicate the object's TTL in seconds. Theoretically, the time is unbounded\nand the agent should be able to predict any number up to in\fnity and given\nin\fnite number of observations the agent will be able to learn the optimal\nrange for the TTL value. In practice, an upper bound limit is set that allows\nthe agent to converge faster as it has fewer values to try.\nReward\nUsing the di\u000berence between invalidation time and estimated time is a straight-\nforward way to calculate how good an estimate is. While it works for write-\nheavy workloads, invalidation are rare in read-heavy workloads like web-\nbased workloads [5]. For a general purpose CM, this means those workloads\ndo not observe reward majority of the time. Therefore, the environment\nrewards the agent for every hit an object receives to encourage it to assign\nhigher TTL for objects that have a higher read ratio. Objects that do not\nobserve any read will have an overall less reward than objects that do, caus-\ning the agent to favour the heavy-read objects. Adding cache utility to the\nreward function as did [42] encourages higher TTL when the cache is empty\nand a conservative TTL when the cache is close to its capacity, hence the sec-\nond term of the reward function that scales the reward by the cache utility.\nCombining both reward functions provides a reward for all cached object.\n33\n\nObjects that do not receive invalidation will receive a reward based on their\ncache hit performance and the cache utility. As the environment punishes\nthe agent if an object is invalided, the agent is motivated to minimise the\nTTL value it outputs. This complex trade-o\u000b is captured in the reward func-\ntion in algorithm 5. By punishing entries that receive an invalidation for any\nreason, the agent learns how the other strategies (such as the eviction one)\noperates as well and provide support for them through an accurate TTL. We\nmaintain a decision monitoring queue, to store transitions that were caused\nby other policies. Objects in the monitoring queue live for the maximum\nallowed TTL. The monitoring queue is required to handle cases when the\nTTL estimated much lower than actual TTL.\n3.6 RLCache: Multi-Agents Con\fguration\nPrevious sections showed the design of independent agents that each spe-\ncialises in a sub-task of the CM, the next natural step is to have all agents\noperate together in the same environment. A naive implementation that uses\nall three agents is very expensive to operate, as each agent will need to learn\nall possible actions every other agent will make in addition to the task it is op-\ntimising. This section describes our alternative approach that aims to reduce\nthe number of training steps required. The goal of this agent is to discover\nwhat possible bottleneck exists when using multiple agents concurrently in\nreal systems and possibly open new ideas for future work.\nAgent Design\nTaking inspiration from game theory and recent work in cooperative games,\nLowe et al. [32] proposed a mechanism to coordinate cooperative agents\nwho share the same goal by using a meta agent that orchestrates the action\nand assign credit to agents. They used multiple actor-critic agents, where\neach agent had its own separate policy. Their experiments showed promis-\ning results in small toy experiments. We took ideas from their work and\n34\n\nadapted it into this one to apply it to a real-world application. In their\nwork, they stored the agents' learnt policies between episodes. At the end of\neach episode, every agent sampled a policy and acted using it, by swapping\nthe policy constantly, they managed to reduce the invariant between what\neach agent learn. Because the problem domain of the system cache is a con-\ntinuous non-episodic problem, sampling per episode was not an option, and\nsampling at set interval is too disruptive to the real workload to be a viable\noption. A simpler design that tolerates invariant between agents is better\nsuited for system problems.\nRLCache multi-agent architecture uses a separate agent for each task, then\na meta (controller) agent to coordinate the output of the action from each\nagent with the goal that maximises the expected reward for all of the agent.\nFigure 3.4 illustrates a simpli\fed design of multi-agent architecture. Each\nagent states' and actions' mirrors the one described above in section 3.3,\nsection 3.5, and section 3.4. The observer framework abstracts the multi-\nagent complexity by sending the state updates to all agents simultaneously.\nThe reward function is also isolated, where each of the agent state, action,\nand reward is as described in their respective subsections above. However,\nthe meta agent has a separate state, action, and reward function. The reward\nfunction for the meta agent takes into account the sum of all agents reward\nand aims to maximise that. The meta-agent state encodes a vector of states\nof the agents, the action they took, and the reward they received. A no-op\nis encoded as a zero in the vector to represent an agent that did not take\naction, such as the eviction agent only operating when the cache is full. The\nmeta agent action is a vector of the three actions that will be committed to\nthe environment.\n35\n\nFigure 3.4: A simpli\fed view of the cache manager's multi-agent setup.\n3.7 RLCache: Single Agent Multi-Task Con-\n\fguration\nThe agents described above had a comparable state representation and learnt\ncommon features (e.g. the hot-keys key value), but the previous designs did\nnot allow to easily transfer that information. They used separate networks\nto model data in a distribution plane despite using a similar reward signal\nbased on the cache-hit rate. However, it is not straightforward to merge the\nnetworks as they operate at di\u000berent times and serve di\u000berent purposes. For\nexample, the eviction strategy only called when the cache is full. Therefore,\ncombining all the networks into a single network can improve the learning\nsigni\fcantly, also triples the number of actions (i.e. training steps) of a single\nnetwork. This approach is known as multi-task learning in the \feld of deep\nlearning. Multi-task learning [8, 54] is an approach that builds one network\nthat generalises to multiple tasks.\n36\n\nAgent Design\nA single agent uses a shared network to decide on the three caching strategies:\ncaching decision, TTL estimation, and key eviction. Multi-task learning is\ndi\u000ecult compared to a single task learning, as there needs to be a balance\nbetween the multiple tasks and their weights on the network and the reward\nthe environment gives [21]. Since the three tasks have di\u000berent action spaces\n(TTL continuous, caching and evicting are discrete), modelling these action\nspaces requires a mathematical justi\fcation.\nTang et al. [50] identi\fed that discrete action is contained within the con-\ntinuous action-space. Therefore, it is possible to use continuous action space\nto model all three task, and discretise the action for the two discrete tasks\n(eviction and caching). Discretising continuous action space was looked at in\n[56, 50]. They proposed several methods including approximating to nearest\ndiscrete value, approximating and removing some values, or discretising each\ndimension of the action space into K discrete actions and using the proba-\nbility distributions of those dimensions to choose the nearest discrete value.\nWe experimented with the simple approach \frst of using a function approxi-\nmator, which rounds a probability between \u0019i= [0;1] to the nearest integer.\nThe simple approach worked well due to the small (binary) discrete action\nspace of both the eviction and caching.\nWe use SAC with continuous action space to allow the agent to learn a set\nof three actions using the same network. We further simplify the network by\ncombining the caching decision and TTL estimation into the same task, if the\nagent estimates a TTL over a con\fgurable threshold, the object is cached.\nState\nThe state encodes the union of all previously mentioned strategies' states,\nso the agent has enough information to handle any of its tasks. Over-\nprovisioning information is not an issue in deep learning, as the agent's net-\nwork will adjust the weight assigned to the feature if it is not contributing\n37\n\nto the overall bene\ft of the system.\nAction\nThe agent outputs two values, one indicates if the object should be evicted,\nand the other a duration estimate of the TTL. To simplify the agent decision,\nboth of the actions are in the continuous action space (and indeed SAC only\nworks in that space). RLCache calls the Estimate TTL procedure when an\nobject is not in the cache, based on the results of the estimation, RLCache\ndecided whether to cache it or not and uses the estimated TTL as the TTL\nfor the object. When the cache is full, the multi-agents scans the whole cache\noutputting a value between [0 ;1] of which we later rounded it to map it to the\ndiscrete set 0 ;1 that indicate an eviction or not, as described in the eviction\nstrategy previously (section 3.4).\nReward\nDesigning a complex reward function is bene\fciary when all possible cases are\ncaptured, with a hand-crafted reward function the agent is guided towards\nconvergence faster, such as the cases with the single agents described earlier.\nHowever, in a multi-task agent where some of the tasks are contradictory (i.e.\neviction and caching decisions), designing a correct reward function that\ncaptures all possible interaction is challenging and demolishes the \rexible\nbene\ft of using RL. Therefore, we opted for a simpli\fed reward function\nthat rewards each cache hit with 1, reward a successful prediction of eviction\nor not caching with 10, while each cache misses and invalidation with \u000010.\nWhile it takes the agents longer to learn the pattern of the object in this\nway by doing so the agent also learns the optimal reward function without\nany hand engineering, and allows the multi-task agent to incorporate other\nsignals easier. Algorithm 6 describes the reward function.\n38\n\nAlgorithm 3 RLCache: Eviction Strategy\nGiven:\n\u000fan o\u000b-policy RL algorithm A. .e.g.DQN\n\u000fexploration decaying steps Es\nInitialise an empty replay memory R \u001e\nInitialise an empty expiry action queue Q \u001e\nInitialise an empty dictionary to hold observed stats D \u001e\nInitialiseAwith random weights\nInitialise\u000f 1:0 an exploration probability that linearly decays until 0 :2\nforEssteps.\nwhile CacheManager is alive do .Continuous System\nprocedure Trim Cache (cache )\nInitialise empty list of keys to evict L \u001e\nforkeyincache do\nRetrieve from Qthe statesassociated with the key.\nSelect action \u000b=A(s) with exploration probability \u000f\nif\u000b== 1 then .evict\nAppend the keytoL\nend if\nCreate incomplete experience ie (s;\u000b;time;operationType )\nInsert incomplete experience into Q[key] =ieforTTL seconds.\nend for\nreturn return the list of keys L\nend procedure\nprocedure async Observe (key;observation type)\nifobservation type ==write then .New write to cache\ns convertToState (key;value;TTL;operation type)\nInsert incomplete experience into QforTTL seconds.\nelse if observation Type = cache Hit then\nstoredstate =Q[key]\nincrementstoredstate cache hit rate\nelse .Cache miss, invalidate, or Qentry expiry\nstoredexperience Q[key]\nCompute reward r=reward (storedexperience )\ninsert complete experience into R\ndelete entry associated with the key in Q\nCalculate the loss and update Anetwork's weight\nend if\nend procedure\nend while\n39\n\nAlgorithm 4 RLCache: Eviction Strategy Reward Function\nprocedure reward(storedstate;action;observation )\nifobservation ==expiry then .expiry of TTL after the agent\ndecision\nifaction == 1 then .If should evict\nreward = 1 .object was evicted and not read after.\nelse .Object was not evicted\nreward =hitcount observation time\u0000hitcount decision time\nend if\nend if\nifobservation =invalidate then\nifaction == 1 then .If should evict\nreward = 1 .eviction followed with an invalidation.\nelse\nreward =\u00001 .no eviction but invalidated after.\nend if\nend if\nifobservation ==cachemiss then\nreward =\u00001\nend if\nreturnreward\nend procedure\nAlgorithm 5 RLCache: TTL Estimation Reward Function\nprocedure reward (tinv;texp;utility;hits )\nifobservation type ==hitthen\nr= 1\nelse ifobservation type ==eviction then\nr= 0\nelse .Miss, expire, or invalidate\ntdiff=absolute ((texp+ 1)=tinv)\u0003\u00001\nend if\nreturnr\u0003utility\nend procedure\n40\n\nAlgorithm 6 RLCache: Multi-Task Reward Function\nprocedure reward (tinv;texp;utility;hits )\nifobservation ==hitthen\nr= 1\nelse ifobservation ==invalidate then\nifaction [ttl]<10oraction [eviction ] == 1 then\nr= 10\nelse\nr=\u000010\nend if\nelse ifobservation ==miss then\nr=\u000010\nelse .Expires or evicted\nr= 0\nend if\nreturnr\u0003utility\nend procedure\n41\n\n42\n\nChapter 4\nEvaluation\n4.1 Evaluation Aims\nThis chapter describes the experiments setup, the hyperparameters choice,\nand the evaluation of several agents. The goals of the experiments are the\nfollowing:\n1. Demonstrate the advantage of using RL in computer systems, namely\nCM, by optimising its SLO (cache-hit rate).\n2. Illustrate RLCache ability to dynamically adjust its caching strategies\naccording to changes in the workload, by evaluating its SLO metrics:\nhit-rate, the ratio of caching operations, the di\u000berence between esti-\nmated and optimal TTL, and the accuracy of objects evictions.\n3. Demonstrate the \rexibility of the observer framework by using it as\nthe backbone for all the methods and operations used in this chapter\n(including the baselines).\n4. Show that multi-task agent provides full automation to all components\nof a CM.\nCMs are logical components that group several methods together. System\ndevelopers choose a combination of methods that best suits their expected\n43\n\nworkload. An open-source implementation of Redis CM [38] uses by default\na write-through strategy for caching, Fixed TTL for object expiration, and\nLFU for the eviction strategy. While Ca\u000beine [33] uses LFU for eviction,\nwrite-on-read for caching, and a \fxed TTL. To generalise and provide a fair\ncomparison to standard systems, each component of RLCache evaluated in\nisolation against a rule-based method:\n\u000fEviction. LFU, LRU, and FIFO are widely used general purpose\nmethods (subsection 2.2.4).\n\u000fCaching. Write-through and Write-on-read (subsection 2.2.2).\n\u000fTTL. Fixed TTL expiration (subsection 2.2.3).\nWe could not evaluate against a statistical augmented methods, due to lack\nof reproducible open-source implementations of the work we mentioned in\nthe related work section. Furthermore, this work, as far as we are aware, is\nthe only one that evaluates and optimises for object based on their cache key,\nand as such we cannot draw a direct comparison to reported metrics from\nrelated work.\n4.2 Experiments Setup\n4.2.1 Simulator\nRL algorithms require a large number of samples to learn meaningful ac-\ntions, in production services, the agent learns from real tra\u000ec logs or metrics.\nHowever, experimental environments lack real tra\u000ec so we used a simulator\nto generate synthetic tra\u000ec to compensate for that. Yahoo Cloud Serving\nBenchmark (YCSB) [12] is a Java framework that provides a set of workloads\nfor evaluating the performance of di\u000berent key-value stores. The framework\ncomprised of two main components, a YCSB client, and a workload genera-\ntor. The client decides on which records to read or write by sampling from\na random distribution, YCSB has multiple prede\fned distributions, one of\n44\n\nwhich is the Zip\fan distribution that we used. Zip\fan distribution follows\nthe popularity distribution where a small number of items are far more pop-\nular than the rest, and web caches do follow this distribution [5].\nWe extended the YCSB framework and de\fned a set of workloads to test\nranging from read-heavy, a mixture of reads and write, and write-heavy\nworkloads, as shown in Table 4.1. While for simple web caches the work-\nload might not change often, we argue that RLCache applicable to systems\nthat are the building blocks of other systems. For example, BigTable is the\nunderlying storage for other systems such as databases like MegaStore (and\nits successors); batch jobs such as MapReduce to store data between jobs.\nThus we argue that these systems exist and are already part of many large\nscale applications. Furthermore, varying the workload demonstrates that\nRLCache does not require any manual con\fguring from the system devel-\nopers. Automating the CM reduces the barrier to developing systems and\no\u000foads the tedious task of pro\fling and experimenting with con\fgurations.\nSince the YCSB client needs to be in Java, we wrote a Java client that com-\nmunicates with YCSB workload generators and then issues REST requests\nto the CM server. The CM server is written in Python to be compatible\nwith the established RL frameworks such as Tensor\row [1] and RLGraph\n[44]. Figure 4.1 demonstrates the communication between the YCSB client\nand the CM. The CM runs on the synchronous thread-per-request model to\nsimplify the development process and avoid possible race conditions. We use\nin-memory storage for the cache in the simulation. To build the RL agents,\nwe used the Cambridge University's RLGraph [44], a modular RL frame-\nwork with several high-performance agents implementations such as DQN\nand SAC.\nAt the start of an experiment, the backend is cleared of any record, then\nYCSB client inserts 10 ;000 records to prepare the simulator for upcoming\nqueries. After the initial loading phase, the evaluation takes place. Each\nworkload generates 100 ;000 queries, sampled from a the Zip\fan distribution\nwith hot spot fraction set to 20% of the overall data. We run four work-\nloads per experiments totalling 400 ;000 queries, and each experiment ran\n45\n\nFigure 4.1: The communication diagram between YCSB and the cache man-\nager.\nWorkload Read % Write % Application Example\nRead Only 100 0 User pro\fle cache.\nRead Mostly 95 5 Photo tagging. Tag is heavily read [49, 24].\nRead Dominant 75 25 Database read and correct job.[13]\nRead/Write Mix 50 50 Session store of recent actions.\nWrite Heavy 0 100 Batch write job.[13]\nTable 4.1: Request ratios in various workloads.\nthree times, we reported the average of those runs. We chose three runs to\nget reasonable con\fdence in the reported results without being it consider-\nably expensive, as we have run a large number of experiments: \fve di\u000berent\nagents, on four di\u000berent capacity limits, each repeated three times. Each\nexperiment took 13 hours to complete on an i7\u00007700HQ@2:80GHwith a\n16GBmemory.\n46\n\n4.2.2 Hyperparameters Selection\nRL algorithms have a large number of con\fgurable parameters. This subsec-\ntion covers the process we used to choose these parameters and their role in\nthe experiments.\nCaching and eviction agents. We used the same parameters for both\nthe caching and eviction agents, as they share the state and action repre-\nsentations despite ful\flling the opposite job. For the agent's algorithm, we\nused DQN as described in section 2.3.1, DQN is an o\u000b-policy discrete action\nalgorithm that is proven to work well for general applications. We con\fgured\nthe exploration constant \u000f= 1:0 that decays to 0 :2 over 250;000 steps, the\nlarge number of steps is based on the observation of the experiment that the\nagent's learning stabilises at that point.. The lower-bound of 0 :2 allows the\nagent to perform a random action 20% of the time to ensure it is never stuck\nin local maxima when the workload changes. As an example, when the agent\nlearns best actions to perform when the workload is read-heavy, and then the\nworkload changes to a write-heavy, with a 20% probability the agent is taken\nrandom actions, with enough samples the agent will learn that the workload\nchanged to write-heavy. The impact of this constant for our experiment had\nthe biggest impact, a lower value made the agent performs better for static\nworkload, while a higher value ensured the agent is able to adapt to changes\nto the workload, choosing too high of a value and the agent will simply be a\nrandom agent and not learning, a value of \u000f2[0:05;0:25] is what we suggest\nto experiment with.\nFor the network structure, we used a two-layer network with 64 hidden units.\nWe chose unit size \u0014128 since the experiments ran on the CPU only. CPUs\nperform better tensor operations when the network size is smaller. In con-\ntrast, a GPU handles larger networks better as it can multi-thread and par-\nallelise the operations e\u000eciently. A larger network did not yield any better\nresults as the state-value representations are relatively small (the encoded\nstate vector is 8 values and the action is a binary). Therefore, a larger net-\nwork yields slower performance and result in unstable learning.\n47\n\nMulti-task and TTL estimation agent. As both agents require an\nalgorithm that supports continuous action-space and is an o\u000b-policy, we used\nSAC for the agent (section 2.3.1). The network has three hidden-layers for\nboth of SAC's value and policy networks, each with 128 units. While the\nreasoning behind the network size is similar to the one we had for caching and\neviction, SAC, in general, requires a wider and deeper network by algorithm\ndesign [19]. SAC does not have an exploration constant and instead. It\nmaximises the entropy that is used in choosing a random variable, making it\nrobust to changes in the workload.\nNetwork optimiser and activation functions. For the neural networks\nactivation function we used ReLu [40], and used Adam [27] as the learning\noptimiser, with the learning rate set to 0 :0003. These parameters were re-\nsearched and evaluated in [20] and are considered what works best for most\ngeneral cases.\n4.3 Experiments\n4.3.1 Caching Strategy\nIntro. The decision to cache an object directly impacts the cache hit rate\nas well as the slowdown caused by eviction policy once the cache is full. A\ngood caching strategy aims to maximise the cache hit rate and by caching\nonly needed objects, thus reducing the number of commit calls to the cache.\nFor that, we measure two main performance metrics: the cache hit rate, and\nthe caching rate. Reducing the number of times an object is cached while\nachieving a similar hit rate results in reduced latency as fewer network hops\nare made, as well as, fewer objects are occupying the cache.\nExperiments. First, we demonstrate the ability of RLCache's agent to\nlearn caching strategy. Figure 4.2 shows the cache hit rate performance for\nRLCache when the system is under 95% read and 5% write workload. The\nagent starts initially with no information about the environment as a result,\n48\n\nthe cache hit-rate starts as low as 20%. Overtime, as the agent is exploring\nand updating its neural network weights, it begins to learn optimal policy,\nand achieve\u001550% cache hit-rate. We con\fgured the agent to slowly decay\nits exploring parameter \u000fto 0.1 over a 50 ;000 steps. The agent chooses an\naction that maximises the Q value with the probability 1 \u0000\u000f, otherwise it\nsamples uniformly from the available action space, hence the sine-wave shape\nof the cache performance early on. The reason the \u000fnever decays to zero, is\nto allow the agent to adapt to when the workload changes, as can be seen in\nthe next evaluation.\n0 20000 40000 60000 80000 100000 120000\nObservation0.00.10.20.30.40.50.60.7Hit RatioCache Capacity 5000\nFigure 4.2: RLCache agent learning a caching strategy overtime.\nFigure 4.3 shows the percentage of the cache operations that results in cache\nhits. It demonstrates the capability of RLCache while varying both the write\nratio and the cache capacity. RLCache is always able to perform better than\nthe traditional methods because of its ability to favour objects with higher\nhits when caching. The cache hit approaching zero with higher write ratio is\nexpected, as the number of requests that attempt to retrieve cached objects\nalso approaches zero. The simulation ran for only 100 ;000 queries for each\ntype of workload. We anticipate, given more queries per evaluation, the agent\nwill learn more distinct features thus resulting in higher cache hit rate, as\nthe trend shows.\nFigure 4.4 backs this claim, while RLCache achieves higher cache hit rate, it\n49\n\n0 20 40 60 80 100\nWrite Ratio (%)0.000.010.020.030.040.050.060.070.08Hit RatioCache Capacity 100\nrlcache_caching_strategy\nalways_write_strategy\ncache_on_read_only\n0 20 40 60 80 100\nWrite Ratio (%)0.0000.0250.0500.0750.1000.1250.1500.1750.200Hit RatioCache Capacity 1000\nrlcache_caching_strategy\nalways_write_strategy\ncache_on_read_only\n0 20 40 60 80 100\nWrite Ratio (%)0.000.050.100.150.200.250.30Hit RatioCache Capacity 2500\nrlcache_caching_strategy\nalways_write_strategy\ncache_on_read_only\n0 20 40 60 80 100\nWrite Ratio (%)0.00.10.20.30.40.5Hit RatioCache Capacity 5000\nrlcache_caching_strategy\nalways_write_strategy\ncache_on_read_onlyFigure 4.3: Comparison between the hit rate of various cache capacities and\nRLCache caching strategy. Error bars with standard deviation less than 0.05\nare omitted.\nalso does that with fewer calls to cache the objects, thus skipping objects that\nwill not receive hits. RLCache also opts to skip more objects when the write\nratio increases, demonstrating its ability to change behaviour dynamically\naccording to the workload. This behaviour is strikingly obvious when the\nwrite rate is at 100% but the caching rate is bellow 10%, thus saving many\ntrips to the cache when the backend is receiving a bulk update.\nOutro. This comparison shows two things: the capability of the system in\nachieving a competitive cache hit rate, and its ability to adapt to a dynamic\nworkload.\n50\n\n0\n5\n10\n25\n50\n100\nWrite Ratio (%)020406080100Cache RatioCache Capacity 100\nrlcache_caching_strategy\nalways_write_strategy\ncache_on_read_only\n0\n5\n10\n25\n50\n100\nWrite Ratio (%)020406080100Cache RatioCache Capacity 1000\nrlcache_caching_strategy\nalways_write_strategy\ncache_on_read_only\n0\n5\n10\n25\n50\n100\nWrite Ratio (%)020406080100Cache RatioCache Capacity 2500\nrlcache_caching_strategy\nalways_write_strategy\ncache_on_read_only\n0\n5\n10\n25\n50\n100\nWrite Ratio (%)020406080100Cache RatioCache Capacity 5000\nrlcache_caching_strategy\nalways_write_strategy\ncache_on_read_onlyFigure 4.4: Comparison between the caching rate of various cache capacities\nand RLCache caching strategy. Error bars with standard deviation less than\n0.05 are omitted.\n4.3.2 Eviction Strategy\nIntro. The CM invokes the eviction strategy when the cache is full and\nneeds to replace an old cached object with a newer one. We con\fgure the\ntesting environment to track performance metrics by storing eviction deci-\nsions for the duration of the leftover TTL after evicting the object. An\noptimal eviction strategy evicts the objects that are no longer useful and\nkeeps the objects that will be used soon in the cache. These properties can\nbe measured using precision, recall, and f1 score. The precision is the num-\nber of correctly evicted or skipped objects to the total number of evictions,\ncalculated according to Equation 4.1. The recall Equation 4.2 is the ratio\nof correctly evicted objects to the total number of decisions, measured us-\ning Equation 4.1. F1Equation 4.3 scores the balance between the precision\nand recall, per Equation 4.1. The system collects the following metrics to\n51\n\ncalculate the above formulas:\n\u000fFalseEvict: If in the period until the TTL is up the evicted object is\nrequested again (thus causing a cache miss).\n\u000fTrueEvict: If the object is invalided within the TTL time or not\nrequested.\n\u000fTrueMiss: If the object is requested after the eviction policy did not\nevict it.\n\u000fFalseMiss: If the object is invalided or not requested after the eviction\npolicy did not evict it.\nprecision =TrueEvict +TrueMiss\nTrueEvict +TrueMiss +FalseEvict(4.1)\nrecall =TrueEvict +TrueMiss\nTrueEvict +TrueMiss +FalseMiss(4.2)\nf1 = 2\u0003precision\u0003recall\nprecision +recall(4.3)\nExperiments. We have modi\fed an open source implementation [15] of\nLRU and FIFO to capture the above performance metrics. To simplify com-\nparison between runs, we set the TTL to be 60 seconds for all entries in the\ncache, and attempt to cache every entry that is read or written. We varied\nthe caching capacity to show the \rexibility of RLCache in constrained space.\nRLCache's F1 score as shown in Figure 4.5 far outperform the other baseline,\nmeaning that RLCache eviction policy results in the eviction of undesirable\nobjects more often than the baselines, and that is especially true for when\nthe workload has some reads mixed in. It can be noted that when the write\nratio approaches 100%, the F1 also approaches 1 :0. Because in a write-heavy\nworkload followup reads are rare, therefore any evict is a good evict. This\nis a great property to have in an eviction policy, predicting when an object\nis no longer needed and prematurely evicting it provides the host cache with\nmore space to store other newer objects. However, Figure 4.6 shows this did\nnot result in a higher cache hit rate as we thought it would. We believe this\n52\n\n0\n5\n10\n25\n50\n100\nWrite Ratio (%)0.00.20.40.60.81.0F1 ScoreCache Capacity 100\nrlcache_eviction_strategy\nlru\nfifo\n0\n5\n10\n25\n50\n100\nWrite Ratio (%)0.00.20.40.60.81.0F1 ScoreCache Capacity 1000\nrlcache_eviction_strategy\nlru\nfifo\n0\n5\n10\n25\n50\n100\nWrite Ratio (%)0.00.20.40.60.81.0F1 ScoreCache Capacity 2500\nrlcache_eviction_strategy\nlru\nfifo\n0\n5\n10\n25\n50\n100\nWrite Ratio (%)0.00.20.40.60.81.0F1 ScoreCache Capacity 5000\nrlcache_eviction_strategy\nlru\nfifoFigure 4.5: Comparison between the F1 score of various cache capacities and\nRLCache eviction strategy. Error bars with standard deviation less than 0.05\nare omitted.\nis due to several factors:\n\u000fRLCache is able to correctly predict when an item will expire soon\nand evicts it before it naturally expires, therefore it accumulate much\nhigher F1 score.\n\u000fRLCache outputs multiple evictions decisions at a time, while it has\nhigh precision (60% at write ratio of 0% and cache capacity of 5000),\nbecause it outputs order of magnitudes more evictions it results in lower\nhit rate overall.\nOutro. The eviction strategy is a promising direction for when evicting the\n'wrong' item is costly in comparison to a cache miss. While the strategy\nitself does not improve the cache-hit rate, it has high accuracy in predicting\nwhich objects are likely to be invalidated soon. Example use case of this is\na on device cache in a smartphone, where the cached entry can be a video\n53\n\nthat will take several minutes to download, as opposite of a cache miss to a\nwebsite logo.\n0 20 40 60 80 100\nWrite Ratio (%)0.000.020.040.060.080.100.12Hit RatioCache Capacity 100\nrlcache_eviction_strategy\nlru\nfifo\nlfu\n0 20 40 60 80 100\nWrite Ratio (%)0.0000.0250.0500.0750.1000.1250.1500.1750.200Hit RatioCache Capacity 1000\nrlcache_eviction_strategy\nlru\nfifo\nlfu\n0 20 40 60 80 100\nWrite Ratio (%)0.000.050.100.150.200.250.30Hit RatioCache Capacity 2500\nrlcache_eviction_strategy\nlru\nfifo\nlfu\n0 20 40 60 80 100\nWrite Ratio (%)0.00.10.20.30.40.5Hit RatioCache Capacity 5000\nrlcache_eviction_strategy\nlru\nfifo\nlfu\nFigure 4.6: Comparison between the hit rate of various cache capacities and\nRLCache eviction strategy. Error bars with standard deviation less than 0.05\nare omitted.\n4.3.3 TTL Estimation\nIntro. Estimating the TTL has two main objectives. The \frst is to give\nobjects that often are requested a long time to stay in the cache. The second\nis to reduce the time in the cache for objects that rarely receive reads or\noften gets invalidated. From our experiments, we found maximising for these\nobjectives (and as such the agent's reward function re\rect that) yields the\nhighest overall cache hit rate. Earlier work such as the one in [42] aimed\nto learn the time an object receives an invalidation request, and assign that\nestimated time to the object. However, objects that rarely receive updates\nand do not receive any reads either will live in the cache longer occupying\n54\n\nspace without contributing to the performance of the cache.\n0 20 40 60 80 100\nWrite Ratio (%)0200400600800Difference from optimal TTLCache Capacity 100\nrlcache_ttl_estimation\nfixed_value\n0 20 40 60 80 100\nWrite Ratio (%)600\n400\n200\n02004006008001000Difference from optimal TTLCache Capacity 1000\nrlcache_ttl_estimation\nfixed_value\n0 20 40 60 80 100\nWrite Ratio (%)2000\n1500\n1000\n500\n050010001500Difference from optimal TTLCache Capacity 2500\nrlcache_ttl_estimation\nfixed_value\n0 20 40 60 80 100\nWrite Ratio (%)2000\n1500\n1000\n500\n0500Difference from optimal TTLCache Capacity 5000\nrlcache_ttl_estimation\nfixed_value\nFigure 4.7: TTL di\u000berence between optimal value and TTL estimation strat-\negy.\nExperiments. We evaluated the performance of this agent by how far o\u000b\nfrom an optimal TTL it is. The optimal TTL is the duration between multi-\nple hits and invalidation if the object does not receive any hits in 30 minutes,\nthe optimal TTL is then set to 0 to discourage caching objects for longer than\nthey are needed. Figure 4.7 demonstrates the learning of the RLCache over-\ntime, and it is approaching an optimal TTL despite changes in the workload.\nIn the plot, the value that is closer to zero yields better results, as y-axis is set\nto the di\u000berence between the optimal TTL and target algorithm. Initially,\nthe agent explores by overestimating the TTL value and explores further by\nunderestimating the value, before it settles on overestimating on average as\nthat yield better cache hit rate. Despite the agent's ability to learn over\ntime, achieving 100% estimation of optimal TTL is very di\u000ecult due to sev-\neral factors, including the cache is limited in size and some values will be\nforced to leave the cache despite what the TTL agent predicted. Further-\n55\n\n0 20 40 60 80 100\nWrite Ratio (%)0.000.010.020.030.040.050.060.07Hit RatioCache Capacity 100\nrlcache_ttl_estimation\nfixed_60_sec\n0 20 40 60 80 100\nWrite Ratio (%)0.0000.0250.0500.0750.1000.1250.1500.175Hit RatioCache Capacity 1000\nrlcache_ttl_estimation\nfixed_60_sec\n0 20 40 60 80 100\nWrite Ratio (%)0.05\n0.000.050.100.150.200.250.30Hit RatioCache Capacity 2500\nrlcache_ttl_estimation\nfixed_60_sec\n0 20 40 60 80 100\nWrite Ratio (%)0.00.10.20.30.40.5Hit RatioCache Capacity 5000\nrlcache_ttl_estimation\nfixed_60_secFigure 4.8: Cache hit-rate comparison when using a dynamically adjusted\nTTL strategy.\nmore, optimal TTL is a continuous value and the simulators samples from a\ndistribution. Therefore, the exact timing is not guaranteed to be a constant\nvalue that can be predicted. Dynamically adjusted TTL struggles to learn\ncorrect values when the cache capacity is limited (1% of the backend size)\ndue to eviction policy interfering with the TTL policy and evicting entries\ndespite the estimated TTL, causing the agent to learn undesirable values,\nit is evident when the write ratio increases and therefore the evictions also\nincreases\nFigure 4.8 shows that cache hit-rate with a dynamically adjusted TTL. The\nbene\ft of using dynamically adjusted TTL is visible in larger cache sizes.\nRLCache learns near optimal TTL estimation in a larger cache. When the\nTTL estimation is near optimal the objects are less likely to stay in the\ncache longer than needed and the cache will not be over-populated, and as a\nresult, the objects will not be evicted prematurely and they will live to near\nmaximum useful lifespan, which increases to the cache hit-rate.\n56\n\nOutro. RLCache dynamically adjusts its TTL estimation and predicts near-\noptimal TTL despite changes in the workload pattern. It achieves higher\ncache-hit rate as a result and reduces the time objects needs to occupy the\ncache for (or increase it if they are useful objects). Accurately predict a\nTTL is a signi\fcant property in push-based caches such as CDN. In CDN\nsending an invalidation request to objects is very expensive and instead,\nsystem developers relay on setting a TTL through trial-and-error.\n4.3.4 Multi-Task and Multi-Agent\nIntro. The previous evaluations looked at caching, eviction, and TTL\nestimation decisions, each of these decisions had a direct impact on the cache\nsystem and sub-sequentially the cache hit-rate. By improving the caching\ndecision, fewer objects will need to be evicted as fewer objects will be put in\nthe cache. Similarly, by improving TTL estimation, fewer objects have to be\nmanually evicted. When the eviction decision is smarter, fewer objects will\ncause cache miss resulting in fewer objects that need to be evaluated by the\nTTL estimator and cache decision maker. The complex interaction between\nthese agents motivates this experiment to combine all actions into one.\nThis evaluation is mostly experimental, as far as we are aware, the literature\nlacks applications of multi actions systems outside toy problems. The goal of\nthis experiment was to investigate the possibility of combining all agents to\nget the best of all in one setup. There are two architectures that we looked\nat that facilitate the multi-action paradigm.\n\u000fMulti-agent learning This architecture uses a separate agent for each\ntask, then a meta (controller) agent to coordinate the output of the ac-\ntion from each agent with the goal that maximises the expected reward\nfor all of the agent. Section 3.6 goes over the details of how this agent\nworks.\n\u000fMulti-task learning In this architecture, a single agent learns how to\nperform multiple separate actions using a single neural network. This\n57\n\narchitecture discussed in depth in section 3.7.\nMulti-agent\nExperiments. Figure 4.9 shows the result of evaluating the multi-agent\nby itself. As can be seen, the results are underwhelming, while the agents\nshare the same reward for the goal of maximising the cache hit rate and\nreducing space, they do so by invoking contradictory actions. For example,\na TTL agent estimates that an object should have a long lifespan, but then\nthe eviction agent evicts, leaving no space for either agents to learn from\ntheir actions. There are two possible mitigation to this problem. The \frst,\nwhen inspecting the plot at closer scale, it becomes apparent that none of\nthe agents converged, this is caused by low number of training samples, as\nnot only each of the four agents requires a large number of samples to learn\nits task, but also need to learn the other agents actions and predict them\nbefore making decisions. This can be mitigated by running an expensive\nexperiment over several weeks that produces enough training sample for all\nagents to converge. However, due to time and resources constraints, we\ndecided against doing so. The alternative approach is using a model-based\nRL to run signi\fcantly faster simulation inside each of the agents and cause\nthem to converge in fewer training samples. We discuss this approach at\nlength in the future work section subsection 5.2.2.\nMulti-task\nIntro. Multi-task agent combines the neural network between all CM tasks\nand aims to learn multiple tasks at the same time. This approach works\nwell when the tasks have similarities between them. In RLCache the tasks\nare set to maximise the same objective function and learn the same key\nfeatures in each unique cache entry, e.g. learning the hotkeys of the cache.\nTo demonstrate the practicality of multi-task agent, we look at four key\nmetrics:\n58\n\n0 20 40 60 80 100\nWrite Ratio (%)0.000.020.040.060.080.10Hit RatioCache Capacity 5000\nmulti_agentFigure 4.9: Cache hit-rate for multi-agent while varying workload.\n\u000fCache hit-rate : The percentage of the requests that are in the cache\nwhen clients query it, it is the main objective function.\n\u000fCaching rate : Indicates how many objects were cached to achieve\nthe cache hit-rate. A higher value is better when the workload is read\nheavy, and lower is better when the workload is write-heavy.\n\u000fF1 score : Visualises the accuracy of an eviction decision (higher is\nbetter).\n\u000fOptimal TTL : The di\u000berence between the TTL's optimal value and\nthe estimated value, the value that is closer to zero is better.\nExperiments. First, \fgure 4.10 visualises the cache hit-rate and compares\nit to all previously mentioned methods. The multi-task agent outperforms\nall previously mentioned methods even when varying the write ratio, show-\ning that the combination of all tasks, when optimised and learnt together,\nachieve a higher cache hit rate, and this is the main hypothesis this work was\nstudying. To ensure that is indeed the case, we look at the other metrics.\nFigure 4.11 shows the caching rate of objects. While the multi-task agent\ndid not outperform the caching decision specialist agent, it still achieved sig-\nni\fcant improvements over the baseline. In our experiment, the exploration\n59\n\n0 20 40 60 80 100\nWrite Ratio (%)0.00.10.20.30.40.5Hit RatioCache Capacity 5000\nrlcache_multi_task_agent\nlru_60sec_cacheAll\nrlcache_ttl_estimation\nrlcache_caching_strategy\nrlcache_eviction_strategyFigure 4.10: Cache hit-rate for multi-task agent on various workloads.\n0\n5\n10\n25\n50\n100\nWrite Ratio (%)020406080100Cache RatioCache Capacity 5000\nrlcache_multi_task_agent\nrlcache_caching_strategy\nlru_60sec_cacheAll\nFigure 4.11: Caching rate for multi-task agent on various workloads.\nparameter starts at 100% of all decisions are sampled uniformly, but decays\nover time until it reaches 10%, and the agent reaches that value around the\ntime the 25% workload is executed. The agent at that time learns the weights\nthat work well for when there is a signi\fcant percentage of the requests be-\ning a read request, as a result when the workload switches to a heavy-write\nworkload, the agent did not have the chance to update its policy. Therefore,\nif the CM expects a sudden shift in the workload all the time, a higher ex-\nploration lower bound (i.e. 25%) allows the agent to react faster to those\n60\n\nshifts. If the CM has a large number of observation samples or the changes\nin the workload are not as fast, a lower exploration bound (like the one we\nused) provide better consistent performance for when the agent converges.\n0\n5\n10\n25\n50\n100\nWrite Ratio (%)0.00.20.40.60.81.0F1 ScoreCache Capacity 5000\nrlcache_multi_task_agent\nrlcache_eviction_strategy\nlru\nfifo\nlfu\nFigure 4.12: F1 score of multi-task agent on various workloads.\nNext, we look at the eviction task that \fgure 4.12 shows. The F1 score shows\nthe trade-o\u000b between precision and recall for evicted objects. We compared\nRLCache multi-task agent against RLCache single-task agent and the base-\nlines. The \fgure shows that the single-task agent's F1 score outperforms\nthe multi-task agent's score. That is expected since the single-agent needs\nto learn a subset of what the multi-tasks needs to learn. Additionally, the\nsingle agent reward function is hand-crafted that captures all edge cases and\nguides the agent, while in the case of multi-task it is challenging to design\nsimilar reward function due to challenges we discussed earlier in section 3.7.\nHowever, the multi-task agent was signi\fcantly better than the baselines,\nand when the workload shifted its tra\u000ec patterns, the multi-agent was able\nto adapt just as well as the single task agent.\nFinally, we look at the TTL estimation task of this agent. Figure 4.13 com-\npares the accuracy of the TTL estimation of both the RLCache single-task\nagent and the multi-task agent. This task heavily depends on the other two\ntasks because an accurate TTL estimation needs information about what\nobjects are likely to be evicted or cached. As a result, the multi-task, in\n61\n\n0 20 40 60 80 100\nWrite Ratio (%)1000\n800\n600\n400\n200\n0200400Difference from optimal TTLCache Capacity 5000\nrl_multi_strategy\nrlcache_ttl_estimationFigure 4.13: TTL di\u000berence between optimal value and RL multi-task agent.\nfact, outperforms the single-task (unlike the other two tasks) and estimates\nalmost near optimal TTL for all the generated workload.\nOutro. The multi-task agent showed that a fully automated cache manage-\nment system outperforms all other methods signi\fcantly, due to the knowl-\nedge sharing between tasks and ability to predict the actions of the other\ntasks at any point in time. While the multi-task agent is not able to outper-\nform the single-agent in their own sub-tasks, we hypothesise, given enough\ntraining samples the multi-task agent will perform similar to the single task\nagent in their respective tasks, and as a result, produces even better over-\nall cache-hit rate, however, such an experiment is expensive to conduct and\ncon\frm.\n4.3.5 RL Hyperparameter Initialisation\nIntro. The goal of this experiment to investigate how susceptible RLCache\nis to the random initialisation of the RL hyperparameters. Henderson et al.\n[20] investigations show that RL is well known to be sensitive to the random\ninitialisation, as a stochastic approximator are used at various levels of any\nRL algorithm.\n62\n\nExperiments. We repeated the caching strategy evaluation 13 times and\nvisualised it, the interesting metric to look at is the error bar as that shows\nthe deviations between runs and as such the impact of the random seed.\n0 20 40 60 80 100\nWrite Ratio (%)0.00.10.20.30.4Hit RatioCache Capacity 5000\nrlcache_caching_strategy\n0\n5\n10\n25\n50\n100\nWrite Ratio (%)0102030405060708090Cache RatioCache Capacity 5000\nrlcache_caching_strategy\nFigure 4.14: The hyperparameter initialisation impact on cache hitrate.\n0 20000 40000 60000 80000 100000 120000\nObservation0.1\n0.00.10.20.30.40.50.60.7Hit RatioCache Capacity 5000\nFigure 4.15: The hyperparameter initialisation impact on hitrate per 1000\nobservation.\nThe \frst \fgure 4.14 shows the impact against the objective functions the\ncache-hit rate and cache rate, the largest deviation is observed when the\nread-write ratio is at 50% the hypothesis, as at that workload depending on\nthe seed the agent can fall in either of the local maxima that favours the\nreads or the one that favours the write. Otherwise, the agent is robust and\nhas a low standard deviation.\n63\n\nFigure 4.15 shows the impact when looked at as per observation, in this \fgure\nit is obvious that the agent \frst 60 ;000 observations are the most susceptible\nto random seed and the deviations can vary massively between steps, however\nthe agent soon coverages and stabilises after, showing its ability to recover\nfrom a bad seed.\nOutro. The experiment showed that while random initialisation has a big\nimpact on the initial decisions. Over time the agent converges towards the\noptimal policy and recovers from a bad seed.\n4.3.6 Outlook\nThis chapter evaluated this thesis against claims made in the introduction,\nand summarised in the following points:\n\u000fRLCache is an online control that adapts its caching strategies to\nchanges in the workload and with information about the system such\nas capacity.\n\u000fRL-driven decisions outperform heuristic-driven one:\n{ Caching decisions: Achieved similar cache hit-rate as the base-\nline doing so while using signi\fcantly less caching operations.\n{ TTL estimation: Estimated a near-optimal TTL while achiev-\ning a higher hit-rate than comparable baseline.\n{ Cache eviction: Accurately identifying objects that will not\nbe used again, a property that is useful when evicting the wrong\nobject is a costly operation.\n\u000fDemonstrated that multi-task learning, a single RL agent learnt all the\nactions needed to manage the cache e\u000bectively, achieving better results\nin all benchmarks.\n\u000fWhile not directly visible in the evaluation, but these agents were only\npossible because of the RLObserver ability to link delayed experiences\n64\n\nand distribute them to interested agents.\n\u000fShowed that the stochastic initialisation of RL algorithm only has im-\npact at very \frst few observations, after which the system able to adapt\nto the changes in the workload and recover from a bad initialisation\nseed.\nThe key takeaway is that RL is useful for computer system problems when\napplied to the right problem. It enables a new class of computer systems\nthat adapts to the changes in the workload and learn complex interactions\nbetween various components of the system.\n65\n\n66\n\nChapter 5\nSummary and Conclusions\n5.1 Conclusion\nThis work showed the result of applying reinforcement learning to a gen-\neral purpose cache manager. RLCache estimates the time-to-live for cached\nobjects, predict the advantage of caching an object, and which object to\nevict if the cache is full. RLCache captures the objective of maximising the\ncache hit-rate while minimising populating the cache with useless objects.\nWe developed three independent RL agents that each is responsible for one\nof the three tasks and a multi-task agent that learns all three tasks at the\nsame time. Furthermore, we designed a framework to tackle the problem\nof delayed experience in computer systems. The framework allows the sys-\ntem to subscribe as many agents to the delayed observation, and abstract\nthe complication associated with the delay of state transition in computer\nsystems.\nWe evaluated our results using a database workload generator, YCSB, that\nprovides an established benchmark on various workloads databases. RL-\nCache adapts to changes in workloads pattern and outperforms the hit-rate\nof traditional methods while minimises the number of objects in the cache.\nWe identi\fed three potential future works for RLCache, applying it in a dis-\n67\n\ntributed setting, improving the training time by building a RL-model of the\nenvironment, or incorporating it with an enterprise storage system.\n5.2 Future work\n5.2.1 Distributed Cache Manager\nThis work introduced a single instance manager that has a full view of all\ncache transactions, that single instance has the responsibility to learn from\nthe observations and inference to do actions. Scaling-out is a concern when\nan application grows such that there is a large number of keys that do not\n\ft in a single instance of a CM, or there are availability requirements.\nDecoupling the RL learning and inference processes is required to allow the\nCM to scale out. The observer architecture discussed in section 3.2 simpli-\n\fes the process of decomposing the system into multiple independent logical\nchunks. RLObserver framework commits transactions and observation to\nlogs, that later synchronised across all distributed listeners, the synchronisa-\ntion delay of the logs is mitigated by using o\u000b-policy agents that learn from\nthe data without interacting with the environment.\nThe work\row of RLCache in a distributed setting is the following: A worker\nis assigned the responsibility of conducting inference operations only. Every\nnew observation and system metrics are pushed to all the workers' queue\nthrough the fan-out architecture described in section 3.2. The distributed\nworkers then learn by consuming the observations from their independent\nqueues. At set intervals, all workers to sync their weights with the main CM.\nThis architecture is typical in distributed systems and allows the system to\nscale e\u000bortlessly and reliably.\nThe future work for this is to investigate distributing the cache, study the\nlatency impact of using RLCache, and bundle the CM into an established\ncache system such as Redis [7] or Memcached [26].\n68\n\nWe expect the RL induced latency to remain a bottleneck even in a dis-\ntributed setting. For example, Kraska et al. [29] proposed an approach to\nlearn an indexing data structure, while their proposal resulted in promising\nresults, they identi\fed a critical bottleneck in the inference time it takes\nfor an indexing decision to be made. RL is still an evolving \feld and its\ntoolchains are written in Python, as such they are not easily parallelisable\nand are ill-suited for mission-critical fast response tasks. Rewriting the core\nengine of the CM and the inference engine in a high-performance language\n(e.g. C++) will further improve the performance of the cache. Once the\nCM is both distributed and parallelised, it can operate in mission-critical\nsystems.\n5.2.2 Model-based RL\nThe work here uses a model-free RL approach to learn caching related deci-\nsions. While it is a \rexible and powerful approach, it requires a large number\nof observations before the agent learn meaningful actions. Lack of observa-\ntions is a big issue for actions that are infrequent, for example, evicting\ncached object.\nIn a model-based RL, the agent interacts with the environment and build\na model that generates more training samples that the agent uses to learn\nfrom actions without having to experience them in the environment, which\nresults in a faster converging. There are two main approaches to model-based\nRL: Given a model of the environment, the agent learns through self-play to\ncollect training samples as done in AlphaZero [47]. Alternatively, if the model\ndoes not exist, World Models [17] learns features of the environment in an\nunsupervised manner and creates a compressed version of the environment, it\nthen trains the agent inside the compressed representation before outputting\nactions in the real environment.\nThe di\u000eculty with this approach is constructing an accurate model as an\ninaccurate model will cause the agent to learn sub-optimal actions.\n69\n\n70\n\nBibliography\n[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,\nS. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,\nM. Wicke, Y. Yu, X. Zheng, G. Brain, I. Osdi, P. Barham, J. Chen,\nZ. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard,\nM. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner,\nP. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng.\nTensorFlow : A System for Large-Scale Machine Learning This paper is\nincluded in the Proceedings of the TensorFlow : A system for large-scale\nmachine learning. Proc 12th USENIX conference on Operating Systems\nDesign and Implementation , pages 272{283, 2016.\n[2] W. Ali, S. M. Shamsuddin, and A. S. Ismail. Intelligent web proxy\ncaching approaches based on machine learning techniques. Decision\nSupport Systems , 53(3):565{579, 2012.\n[3] I. Arapakis, X. Bai, and B. B. Cambazoglu. Impact of response latency\non user behavior in web search. In Proceedings of the 37th international\nACM SIGIR conference on Research & development in information re-\ntrieval , pages 103{112. ACM, 2014.\n[4] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath.\nDeep Reinforcement Learning: A Brief Survey. IEEE Signal Processing\nMagazine , 34(6):26{38, 11 2017.\n[5] L. Breslau, Pei Cao, Li Fan, G. Phillips, and S. Shenker. Web caching\nand zipf-like distributions: evidence and implications. In IEEE IN-\nFOCOM '99. Conference on Computer Communications. Proceedings.\nEighteenth Annual Joint Conference of the IEEE Computer and Com-\nmunications Societies. The Future is Now (Cat. No.99CH36320) , vol-\nume 1, pages 126{134 vol.1, March 1999.\n[6] J. Brutlag. Speed matters for google web search, 2009.\n71\n\n[7] J. L. Carlson. Redis in action . Manning Publications Co., 2013.\n[8] R. Caruana. Multitask learning. Machine learning , 28(1):41{75, 1997.\n[9] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Bur-\nrows, T. Chandra, A. Fikes, and R. E. Gruber. Bigtable: A distributed\nstorage system for structured data. ACM Transactions on Computer\nSystems (TOCS) , 26(2):4, 2008.\n[10] C. Chen, R. Vitenberg, and H.-A. Jacobsen. Scaling construction of\nlow fan-out overlays for topic-based publish/subscribe systems. In 2011\n31st International Conference on Distributed Computing Systems , pages\n225{236. IEEE, 2011.\n[11] J. Choi, J. Han, E. Cho, T. Kwon, and Y. Choi. A survey on content-\noriented networking for e\u000ecient content delivery. IEEE Communications\nMagazine , 49(3):121{127, 2011.\n[12] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears.\nBenchmarking cloud serving systems with ycsb. In Proceedings of the\n1st ACM symposium on Cloud computing , pages 143{154. ACM, 2010.\n[13] M. Dellkrantz, M. Kihl, and A. Robertsson. Performance modeling and\nanalysis of a database server with write-heavy workload. In European\nConference on Service-Oriented and Cloud Computing , pages 184{191.\nSpringer, 2012.\n[14] X. Dutreilh, S. Kirgizov, O. Melekhova, J. Malenfant, N. Rivierre, and\nI. Truck. Using reinforcement learning for autonomic resource allocation\nin clouds: towards a fully automated work\row. In ICAS 2011, The Sev-\nenth International Conference on Autonomic and Autonomous Systems ,\npages 67{74, 2011.\n[15] D. Gilland. cacheout. https://github.com/dgilland/cacheout , 2018.\n[16] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine. Continuous deep q-\nlearning with model-based acceleration. In International Conference on\nMachine Learning , pages 2829{2838, 2016.\n[17] D. Ha and J. Schmidhuber. World models. arXiv preprint\narXiv:1803.10122 , 2018.\n[18] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: O\u000b-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor. arXiv preprint arXiv:1801.01290 , 2018.\n72\n\n[19] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Ku-\nmar, H. Zhu, A. Gupta, P. Abbeel, et al. Soft actor-critic algorithms\nand applications. arXiv preprint arXiv:1812.05905 , 2018.\n[20] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and\nD. Meger. Deep reinforcement learning that matters. In Thirty-Second\nAAAI Conference on Arti\fcial Intelligence , 2018.\n[21] M. Hessel, H. Soyer, L. Espeholt, W. Czarnecki, S. Schmitt, and H. van\nHasselt. Multi-task deep reinforcement learning with popart. arXiv\npreprint arXiv:1809.04474 , 2018.\n[22] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot,\nD. Horgan, J. Quan, A. Sendonaris, I. Osband, et al. Deep q-learning\nfrom demonstrations. In Thirty-Second AAAI Conference on Arti\fcial\nIntelligence , 2018.\n[23] R. A. Howard. Dynamic Programming and Markov Processes . MIT\nPress, Cambridge, MA, 1960.\n[24] Q. Huang, K. Birman, R. van Renesse, W. Lloyd, S. Kumar, and H. C.\nLi. An analysis of facebook photo caching. In Proceedings of the Twenty-\nFourth ACM Symposium on Operating Systems Principles , pages 167{\n181. ACM, 2013.\n[25] R. T. Hurley and B. Y. Li. A performance investigation of web caching\narchitectures. In Proceedings of the 2008 C3S2E Conference , C3S2E '08,\npages 205{213, New York, NY, USA, 2008. ACM.\n[26] J. Jose, H. Subramoni, M. Luo, M. Zhang, J. Huang, M. Wasi-ur Rah-\nman, N. S. Islam, X. Ouyang, H. Wang, S. Sur, et al. Memcached design\non high performance rdma capable interconnects. In 2011 International\nConference on Parallel Processing , pages 743{752. IEEE, 2011.\n[27] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980 , 2014.\n[28] J. Kober, J. A. Bagnell, and J. Peters. Reinforcement learning in\nrobotics: A survey. The International Journal of Robotics Research ,\n32(11):1238{1274, 2013.\n[29] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case\nfor learned index structures. In Proceedings of the 2018 International\nConference on Management of Data , pages 489{504. ACM, 2018.\n73\n\n[30] T. Li, Z. Xu, J. Tang, and Y. Wang. Model-free control for distributed\nstream data processing using deep reinforcement learning. Proceedings\nof the VLDB Endowment , 11(6):705{718, 2018.\n[31] G. Linden. Geeking with Greg: Marissa Mayer at Web 2.0.\n[32] R. Lowe, Y. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I. Mor-\ndatch. Multi-agent actor-critic for mixed cooperative-competitive en-\nvironments. In Advances in Neural Information Processing Systems ,\npages 6379{6390, 2017.\n[33] B. Manes. Ca\u000beine a high performance caching library for java 8. https:\n//github.com/ben-manes/caffeine , 2019.\n[34] H. Mao, M. Alizadeh, I. Menache, and S. Kandula. Resource manage-\nment with deep reinforcement learning. In Proceedings of the 15th ACM\nWorkshop on Hot Topics in Networks , pages 50{56. ACM, 2016.\n[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, and M. Riedmiller. Playing atari with deep reinforcement learning.\narXiv preprint arXiv:1312.5602 , 2013.\n[36] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al. Human-level control through deep reinforcement learning. Nature ,\n518(7540):529, 2015.\n[37] S. Podlipnig and L. B osz ormenyi. A survey of web cache replacement\nstrategies. ACM Computing Surveys (CSUR) , 35(4):374{398, 2003.\n[38] S. Project. Rediscachemanager. https://github.\ncom/spring-projects/spring-data-redis/blob/master/\nsrc/main/java/org/springframework/data/redis/cache/\nRedisCacheManager.java , 2019.\n[39] M. Rabinovich and O. Spatscheck. Web caching and replication , vol-\nume 67. Addison-Wesley Boston, USA, 2002.\n[40] P. Ramachandran, B. Zoph, and Q. V. Le. Searching for activation\nfunctions. arXiv preprint arXiv:1710.05941 , 2017.\n[41] A. Sadeghi, G. Wang, and G. B. Giannakis. Adaptive caching via deep\nreinforcement learning. CoRR , abs/1902.10301, 2019.\n[42] M. Schaarschmidt, F. Gessert, V. Dalibard, and E. Yoneki. Learning\nruntime parameters in computer systems with delayed experience injec-\ntion. arXiv preprint arXiv:1610.09903 , 2016.\n74\n\n[43] M. Schaarschmidt, A. Kuhnle, B. Ellis, K. Fricke, F. Gessert, and\nE. Yoneki. LIFT: Reinforcement Learning in Computer Systems by\nLearning From Demonstrations. arXiv preprint arXiv:1808.07903 , 2018.\n[44] M. Schaarschmidt, S. Mika, K. Fricke, and E. Yoneki. RLgraph: Modu-\nlar Computation Graphs for Deep Reinforcement Learning. In Proceed-\nings of the 2nd Conference on Systems and Machine Learning (SysML) ,\nApr. 2019.\n[45] A. Sharma, F. M. Schuhknecht, and J. Dittrich. The case for auto-\nmatic database administration using deep reinforcement learning. arXiv\npreprint arXiv:1801.05643 , 2018.\n[46] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot, et al. Mastering the game of go with deep neural networks\nand tree search. nature , 529(7587):484, 2016.\n[47] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering\nthe game of go without human knowledge. Nature , 550(7676):354, 2017.\n[48] R. S. Sutton and A. G. Barto. Reinforcement learning : an introduction .\nMIT Press, Cambridge Mass., 1998.\n[49] L. Tang, Q. Huang, W. Lloyd, S. Kumar, and K. Li. RIPQ: Advanced\nphoto caching on \rash for facebook. In 13th USENIX Conference on\nFile and Storage Technologies (FAST 15) , pages 373{386, Santa Clara,\nCA, 2015. USENIX Association.\n[50] Y. Tang and S. Agrawal. Discretizing continuous action space for on-\npolicy optimization. arXiv preprint arXiv:1901.10500 , 2019.\n[51] D. Terry. Replicated data consistency explained through baseball. Com-\nmunications of the ACM , 56(12):82{89, 2013.\n[52] G. Tesauro, R. Das, W. E. Walsh, and J. O. Kephart. Utility-function-\ndriven resource allocation in autonomic systems. In Second International\nConference on Autonomic Computing (ICAC'05) , pages 342{343. IEEE,\n2005.\n[53] G. Tesauro, N. K. Jong, R. Das, and M. N. Bennani. A hybrid rein-\nforcement learning approach to autonomic resource allocation. In 2006\nIEEE International Conference on Autonomic Computing , pages 65{73.\nIEEE, 2006.\n75\n\n[54] S. Thrun and L. Pratt. Learning to learn . Springer Science & Business\nMedia, 2012.\n[55] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning\nwith double q-learning. In Thirtieth AAAI Conference on Arti\fcial\nIntelligence , 2016.\n[56] H. Van Hasselt and M. A. Wiering. Using continuous action spaces\nto solve discrete problems. In 2009 International Joint Conference on\nNeural Networks , pages 1149{1156. IEEE, 2009.\n[57] C. J. Watkins and P. Dayan. Q-learning. Machine learning , 8(3-4):279{\n292, 1992.\n76",
  "textLength": 122109
}