{
  "paperId": "34d116ceb43176f07013c8372af86159ea09ac3d",
  "title": "Frequency Estimation in Data Streams: Learning the Optimal Hashing Scheme",
  "pdfPath": "34d116ceb43176f07013c8372af86159ea09ac3d.pdf",
  "text": "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1\nFrequency Estimation in Data Streams:\nLearning the Optimal Hashing Scheme\nDimitris Bertsimas and Vassilis Digalakis Jr.\nAbstract —We present a novel approach for the problem of frequency estimation in data streams that is based on optimization and\nmachine learning. Contrary to state-of-the-art streaming frequency estimation algorithms, which heavily rely on random hashing to\nmaintain the frequency distribution of the data steam using limited storage, the proposed approach exploits an observed stream preﬁx\nto near-optimally hash elements and compress the target frequency distribution. We develop an exact mixed-integer linear optimization\nformulation, which enables us to compute optimal or near-optimal hashing schemes for elements seen in the observed stream preﬁx;\nthen, we use machine learning to hash unseen elements. Further, we develop an efﬁcient block coordinate descent algorithm, which,\nas we empirically show, produces high quality solutions, and, in a special case, we are able to solve the proposed formulation exactly in\nlinear time using dynamic programming. We empirically evaluate the proposed approach both on synthetic datasets and on real-world\nsearch query data. We show that the proposed approach outperforms existing approaches by one to two orders of magnitude in terms\nof its average (per element) estimation error and by 45-90% in terms of its expected magnitude of estimation error.\nIndex Terms —Data streams, streaming frequency estimation, learning to hash, optimal hashing scheme.\nF\n1 I NTRODUCTION\nWEconsider a streaming model of computation [1], [2],\nwhere the input is represented as a ﬁnite sequence of\nelements from some ﬁnite universe (domain) which is not\navailable for random access, but instead arrives dynamically\nand one at a time in a stream. We further assume that each\nelement is identiﬁed by a unique key and is also associated\nwith a set of features. One of the most fundamental prob-\nlems in the streaming model is frequency estimation, i.e.,\ngiven an input stream, estimate the frequency (number of\noccurrences) of each element. Notice that this can trivially\nbe computed in space equal to the minimum of the universe\nand the stream size, by simply maintaining a counter for\neach element or by storing the entire stream, respectively.\nNevertheless, data streams are typically characterized by\nlarge volume and, therefore, streaming frequency estimation\nalgorithms should require small space, sublinear in both\nthe universe and the stream size. Furthermore, streaming\nalgorithms should generally be able to operate in a single\npass (each element should be examined at most once in ﬁxed\narrival order) and in real-time (each element’s processing\ntime must be low).\nExample. Consider a stream of queries arriving on a server. The\nuniverse of all elements is the set of all possible queries (of bounded\nlength) and each element is uniquely identiﬁed by the query text.\nNote that any unique query may appear multiple times in the\nstream. The features associated with a query could include, e.g.,\nthe query length, the unigram of the query text (possibly after\n\u000fD. Bertsimas is with the Sloan School of Management and the Operations\nResearch Center, Massachusetts Institute of Technology, Cambridge, MA,\n02139.\nE-mail: dbertsim@mit.edu.\n\u000fV . Digalakis Jr. is with the Operations Research Center, Massachusetts\nInstitute of Technology, Cambridge, MA, 02139.\nE-mail: vvdig@mit.edu.\nSubmitted: 07/2020. Revised: 05/2021.some pre-processing), etc. The goal is to estimate the frequency\ndistribution of the queries, that is, the number of times each\nquery appears in the stream, in space much smaller than the total\nnumber of unique queries.\nMassive data streams appear in a variety of applications.\nFor example, in search query monitoring, Google received\nmore than 1.2 trillion queries in 2012 (which translates to 3.5\nbillion searches per day) [3]. In network trafﬁc monitoring,\nAT&T collects over one terabyte of NetFlow [4] measure-\nment data from its production network each day [2]. More-\nover, the IPV6 protocol provides nearly 2128addresses, mak-\ning the universe of possible IP addresses gigantic, especially\nconsidering that, in many applications, we are interested\nin monitoring active IP network connections between pairs\n(source/destination) of IP addresses. Thus, being able to\nprocess a data stream in sublinear space is essential.\nMaintaining the frequency distribution of a stream of el-\nements is useful, not only as a sufﬁcient statistic for various\nempirical measures and functionals (e.g., entropy [5]), but\nalso to identify interesting patterns in the data. An example\nare the so-called “heavy-hitters” [6], [7], that is, the elements\nthat appear a big number of times, which, e.g., could be\nindicative of denial of service attacks in network trafﬁc\nmonitoring (see [2] for a detailed discussion of applications).\nIn this paper, we address the problem of frequency\nestimation in data streams, under the additional assump-\ntion that a preﬁx of the input stream has been observed.\nAlong the lines of [8], who address the same problem and\nextend classical streaming frequency estimation algorithms\nwith a machine learning component, we aim to exploit the\nobserved preﬁx and the features associated with each ele-\nment, and develop data-driven streaming algorithms. The\nproposed algorithms satisfy the small-space requirement,\nas they signiﬁcantly compress the input frequency vector,\nand do operate in a single pass and in real-time, as theirarXiv:2007.09261v2  [cs.DS]  2 Jun 2021\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 2\nupdate and query times are constant (except for the training\nphase, which is more computationally demanding, since we\nperform optimization and machine learning).\n1.1 Streaming Frequency Estimation Algorithms\nA rich body of research has emerged in the streaming\nmodel of computation [1], [2]; the ﬁrst streaming algorithms\nappeared in the early 1980s, to address, in limited space,\nproblems such as ﬁnding the most frequently occurring\nelements in a stream [6]. A vast literature has since been de-\nveloped, especially since the 1990s, and numerous problems,\nincluding complex machine learning tasks, such as decision\ntree induction [9], can now be solved in streaming settings.\nSketches [10] are among the most powerful tools to\nprocess streaming data. A sketch is a data structure which\ncan be represented as a linear transform of the input. For\nexample, in the context of frequency estimation, the input\nis the vector of frequencies (or frequency distribution) of\nthe input elements and the sketch is computed by multiply-\ning the frequency distribution by a ﬁxed, “fat” matrix. Of\ncourse, for compactness, the matrix that performs the sketch\ntransform is never explicitly materialized and is implicitly\nimplemented via the use of random hash functions.\nAny given sketch transform is deﬁned for a particular\ntask. Among the most popular sketching methods for the\ntask of frequency estimation, are the Count-Min Sketch [11]\nand the Count Sketch [12], which both rely on random\nhashing and differ in their frequency estimation procedure.\nHistorically, the so-called AMS Sketch [13], which addresses\nthe task of estimating the sum of the squares of the fre-\nquencies of the input stream, was among the ﬁrst sketching\nalgorithms that have been proposed. Sketching algorithms\nhave found numerous applications, including in measuring\nnetwork trafﬁc [14], in natural language processing [15],\nin signal processing and compressed sensing [16], and in\nfeature selection [17].\n1.2 Learning-Augmented Streaming Algorithms\nThe abundance of data that is available today has moti-\nvated the development of the ﬁeld of learning-augmented\nalgorithms, whereby traditional algorithms are modiﬁed to\nleverage useful patterns in their input data. More speciﬁ-\ncally, in the context of streaming algorithms, [18] and [19]\naugment with a machine learning oracle the Bloom ﬁlter\n[20], [21], a widely used probabilistic data structure that tests\nset membership. [8] develop learning-based versions of the\nCount-Min Sketch and the Count Sketch; a similar approach\nis taken by [22], who focus on network monitoring and\ndevelop a generalized framework to augment sketches with\nmachine learning. The latter two approaches use machine\nlearning in parallel with a standard (random) sketch, that\nis, they combine a machine learning oracle with standard\n(conventional) streaming frequency estimation algorithms,\nsuch as the Count-Min Sketch.\nIn this paper, we consider the same problem as in\n[8], namely learning-based streaming frequency estimation.\nHowever, our approach fundamentally differs in that we\ncombine a (non-random) sketch (i.e., the optimal hashing\nscheme) and machine learning into a new estimator and\nhence our approach does not rely on random hashing at all.Instead, we use optimization to learn an optimal (or near-\noptimal) hashing scheme from (training) data, and machine\nlearning to hash “unseen elements,” which did not appear\nin the training data.\n1.3 Learning to Hash\nThe proposed approach has connections with the ﬁeld of\nlearning to hash, a data-dependent hashing approach which\naims to learn hash functions from a speciﬁc dataset (see [23]\nfor a comprehensive survey). Learning to hash has mostly\nbeen considered in the context of nearest neighbor search,\ni.e., learning a hashing scheme so that the nearest neighbor\nsearch result in the hash coding space is as close as possible\nto the search result in the original space. Optimization-\nbased learning to hash approaches include the works [24],\n[25], [26]. [27] develop an optimal data-dependent hashing\nscheme for the approximate nearest neighbor problem. To\nthe best of our knowledge, our approach is the ﬁrst that con-\nsiders learning hashing schemes for the streaming frequency\nestimation problem, whereby the objective is different.\n1.4 Learning-Augmented Algorithms beyond Stream-\ning and Hashing\nBeyond streaming and hashing algorithms, [28] use\nmachine-learned predictions to improve the performance\nof online algorithms. [29] use reinforcement learning and\nneural networks to learn workload-speciﬁc scheduling algo-\nrithms that, e.g., aim to minimize the average job completion\ntime. Machine learning has also been used outside the ﬁeld\nof algorithm design, e.g., in signal processing and, speciﬁ-\ncally, in the context of “structured” (instead of sparse) signal\nrecovery [30] and in optimization. [31] and [32] propose ma-\nchine learning-based approaches for variable branching in\nmixed-integer optimization, [33] use reinforcement learning\nto learn combinatorial optimization algorithms over graphs,\n[34] use interpretable machine learning methods to learn\nstrategies behind the optimal solutions in continuous and\nmixed-integer convex optimization problems as a function\nof their key parameters, and [35] focus speciﬁcally on online\nmixed-integer optimization problems. Machine learning has\nalso been popularized in the context of data management\nand, in particular, in tasks such as learning index structures\n[18] and query optimization [36], [37].\n1.5 Contributions\nOur key contributions can be summarized as follows:\n- We develop a novel approach for the problem of\nfrequency estimation in data streams that is based on\noptimization and machine learning. By exploiting an\nobserved stream preﬁx, the proposed learning-based\nstreaming frequency estimation algorithm achieves\nsuperior performance compared to conventional\nstreaming frequency estimation algorithms.\n- We present an exact mixed-integer linear optimiza-\ntion formulation, as well as an efﬁcient block coor-\ndinate descent algorithm, that enable us to compute\nnear-optimal hashing schemes and provide a smart\nalternative to oblivious random hashing schemes.\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 3\nThis part of our work could be of independent inter-\nest, beyond the problem of frequency estimation in\ndata streams. Further, in a special case, we are able\nto solve the proposed formulation exactly in linear\ntime using dynamic programming.\n- We apply the proposed approach to the problem of\nsearch query frequency estimation and evaluate it\nusing both synthetic and real-world data. Computa-\ntional results indicate that the proposed approach no-\ntably outperforms state-of-the-art non-learning and\nlearning-based approaches in terms of its estima-\ntion error. Moreover, the proposed approach is by\nconstruction interpretable and enables us to get ad-\nditional insights into the problem of search query\nfrequency estimation.\nThe rest of the paper is organized as follows. In Section 2,\nwe formalize the streaming frequency estimation problem\nand present, at a high level, the Count-Min Sketch, the most\nwidely used random hashing-based streaming frequency\nestimation algorithm, and the Learned Count-Min Sketch,\na learning-augmented version of the Count-Min Sketch.\nSection 3 gives an overview of the proposed approach.\nIn Section 4, we formulate the problem of learning the\noptimal hashing scheme using the observed stream preﬁx\nand develop efﬁcient optimization algorithms. Section 5 de-\nscribes the frequency estimation procedure we apply, after\nthe optimal hashing scheme is learned. In Section 6, we use\nsynthetic data to explore the performance and scalability\nof the proposed algorithms and investigate the impact of\nvarious design choices on the proposed approach. Section 7\nempirically evaluates the proposed approach on real-world\nsearch query data. Section 8 concludes the paper.\n2 P RELIMINARIES\nIn this section, we formally describe the problem of fre-\nquency estimation in data streams and present the state-\nof-the-art approaches to solving it. Although we explain the\nnotation that we use in the main text, for convenience, we\nalso gather the basic notations in Table 2 in Appendix C.\nFormally, we are given input data in the form of an\nordered set of elements\nS= (u1;u2;:::;ujSj);\nwhereut2 U;8t2[jSj] :=f1;:::;jSjg, andUis the\nuniverse of input elements. Each element u2U is of the\nform\nu= (k;x);\nwhere (without loss of generality) k2[jUj]is a unique ID\nandx2X is a set of features associated with u. The goal\nis, at the end ofS, given an element u2U , to output an\nestimate ~fuof the frequency\nfu=jSjX\nt=11(ut=u)\nof that element, i.e., the number of times the element ap-\npears inS; here, 1Adenotes the indicator function of event\nA. We assume that both SandUare huge, so we wish\nto produce accurate estimates in space much smaller thanminfjSj;jUjg. We work under the additional assumption\nthat a preﬁxS0= (u1;u2;:::;ujS0j)(wherejS0j \u001c jSj )\nof the input stream has already been observed.\n2.1 Conventional Approach: Random Sketches\nThe standard approach to attack this problem is the well-\nknown Count-Min Sketch (CMS) [11], a probabilistic data\nstructure based on random hashing that serves as the fre-\nquency table ofS. In short, CMS randomly hashes (via a\nrandom linear hash function hash (\u0001)) each element u2U\nto a bucket in an array \u001eof sizew\u001cminfjSj;jUjg;\nwhenever element uoccurs inS, the corresponding counter\n\u001ehash (u)is incremented. Since w\u001cjUj , multiple elements\nare mapped to the same bucket and \u001ehash (u)overestimates\nfu. In practice, multiple arrays \u001e1;:::;\u001edare maintained\n(each array is referred to as a “level”) and the ﬁnal estimate\nforfuis\n~fu= min\nl2[d]\u001el\nhashl(u);\nwhere hashl(\u0001)is the hash function that corresponds to the\nl-th level. Intuitively, by repeating the estimation procedure\nmultiple times and taking the minimum of the estimated\nfrequencies (all of which overestimate the actual frequency),\nthe resulting estimator’s accuracy will improve. CMS pro-\nvides probabilistic guarantees on the accuracy of its esti-\nmates, namely, for each u2U;with probability 1\u0000\u000e,\nj~fu\u0000fuj\u0014\u000fjjfjj1;\nwhere\u000f=e\nwand\u000e=e\u0000d:In total, CMS consists of b=\nw\u0002dbuckets.\n2.2 Learning-Based Approach: Learned Sketches\nTo leverage the observed stream preﬁx, [8] augment the clas-\nsical CMS algorithm as follows. Noticing that the elements\nthat affect the estimation error the most are the so-called\nheavy-hitters (i.e., elements that appear many times), they\npropose to train a classiﬁer\nhHH:X!f heavy;heavyg\nthat predicts whether an element u= (k;x)is going to be a\nheavy-hitter or not.1Then, they allocate bheavy unique buck-\nets to elements identiﬁed as heavy-hitters, and randomly\nallocate the remaining brandom =b\u00002bheavy buckets to the\nrest of the universe, using, e.g., the standard CMS. We call\ntheir algorithm the Learned Count-Min Sketch (LCMS).\nAn important remark is that each of the bheavy unique\nbuckets allocated to heavy-hitters should maintain both\nthe frequency and the ID of the associated element. As\nexplained, this can be achieved by using hashing with\nopen addressing, whereby it sufﬁces to store IDs hashed\nintologbheavy +tbits (instead of whole IDs which could\nbe arbitrarily large) to ensure there is no collision with\nprobability 1\u00002\u0000t. Noticing that logbheavy +tis com-\nparable to the number of bits per counter, the space for\n1. [8] identify the heavy-hitters by ﬁrst predicting the element\nfrequencies (or log-frequencies) using machine learning and then select-\ning, using validation data, the optimal cutoff threshold for an element to\nbe considered a heavy-hitter. In their experiments, they predict whether\nan item is in the top 1%of the frequencies.\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 4\na unique bucket is twice the space of a normal bucket.\nThe learning augmented algorithm is shown to outperform,\nboth theoretically and empirically, its conventional, fully-\nrandom counterpart. Additionally, they prove that under\ncertain distributional assumptions, allocating unique buck-\nets to heavy-hitters is asymptotically optimal [8], [38]. In\ngeneral, however, their approach remains heuristic, does not\nguarantee optimal performance, and possibly throws away\ninformation by taking hard, binary decisions.\n3 O VERVIEW OF THE PROPOSED APPROACH\nMotivated by the success of LCMS, we investigate an alter-\nnative, optimization-based approach in using the observed\nstream preﬁx to enhance the performance of the frequency\nestimator.\nAt a high level, the proposed two-phase approach works\nas follows. In the ﬁrst phase, the elements that appeared in\nthe stream preﬁx are optimally allocated to buckets based on\ntheir observed frequencies so that the frequency estimation\nerror is minimized and, at the same time, similar elements\nare mapped to the same bucket. Importantly, contrary to\nCMS-based approaches, in the proposed approach, the es-\ntimate for an element’s frequency is the average of the\nfrequencies of all elements that are mapped to the same\nbucket. Therefore, we aim to assign “similar” elements to\nthe same bucket. In the second phase, once we have an\noptimal allocation of the elements that appeared in the\npreﬁx to buckets, we train a classiﬁer mapping elements to\nbuckets based on their features. By doing so, we are able to\nprovide estimates for unseen elements that did not appear\nin the preﬁx and hence their frequencies are not recorded.\nThe proposed hashing scheme consists of a hash table\nmapping IDs of elements that appeared in the preﬁx to\nbuckets and the learned classiﬁer. In addition, for each\nbucket, we need to maintain the sum of frequencies of all\nelements mapped therein. During stream processing, that is,\nonce the estimator is ready, whenever an element that had\nappeared in the preﬁx re-appears, we increment the counter\n(i.e., the aggregated frequency) of the bucket to which the\nelement was mapped. Finally, to answer count-queries for\nany given element, we simply output the current average\nfrequency of the bucket where the element is mapped (either\nvia the hash table or via the classiﬁer).\nAppendix B provides ﬂowcharts for the proposed ap-\nproach, which further explain the learning phase, where the\nstream preﬁx is used to learn the optimal hashing scheme\nand the classiﬁer, illustrate how the proposed approach\nanswers count queries for any input element, and show the\nupdate mechanism of the proposed approach.\n4 L EARNING THE OPTIMAL HASHING SCHEME\nIn this section, we develop the proposed approach in learn-\ning the optimal hashing scheme.\n4.1 Exact Formulation\nLetS0= (u1;:::;ujS0j)be the observed stream preﬁx. We\ndenote byf0\nuthe empirical frequency of element uinS0,\ni.e.,\nf0\nu=jS0jX\nt=11(ut=u);and byf0(S0)the entire frequency distribution after observ-\ningS0. Moreover,U0=fu2U :f0\nu>0gis the set of all\ndistinct elements that appeared in S0and letjU0j=n. We\nintroducen\u0002bbinary variables, where bis the total number\nof available buckets, deﬁned as\nzij=\u001a1;ifith element ofU0is mapped to bucket j;\n0;otherwise.\nEach rowziofZ(where we denote [Z]ij=zij)can be\nviewed as an one-hot binary hash code mapping element i\nto one of the buckets. At the end of the stream and given a\nﬁxed assignment for the variables zij, the ﬁnal estimate of\nthe frequency of element i2[n]is\n~fi=X\nj2[b]zij P\nk2[n]zkjfkP\nk2[n]zkj!\n:\nThe resulting, e.g., absolute estimation error isP\ni2[n]j~fi\u0000\nfij; a natural objective is to pick the variables zijthat\nminimize this absolute error in the observed stream preﬁx.\nAn alternative objective we could pick is the expected\nmagnitude of the absolute error1P\nk2[n]fkP\ni2[n]fi\u0001j~fi\u0000fij,\nwhereby it is assumed that the probability piof observing\nelementiis equal to its empirical probability in the observed\nstream preﬁx, i.e., pi:=fiP\nk2[n]fk. In fact, this metric is used\nby [8] in their theoretical analysis. However, such an ap-\nproach would heavily weigh the most frequently occurring\nelements and would probably produce highly inaccurate\nestimates for less frequent elements. As we would like to\nachieve a uniformly small estimation error, we stick to the\nformer objective and select the variables zijthat solve the\noptimization formulation which we will present shortly. We\nincorporate an additional term in the objective function of\nthe proposed formulation, to take the features associated\nwith each element into account when computing the opti-\nmal mapping of elements to buckets. For \u00152[0;1], we have:\nmin\nZ2f0;1gn\u0002bX\ni2[n]X\nj2[b]zij\"\n\u0015\f\f\f\f\ff0\ni\u0000P\nk2[n]zkjf0\nkP\nk2[n]zkj\f\f\f\f\f\n+(1\u0000\u0015)X\nk2[n]zkjkxi\u0000xkk23\n5\ns.t.X\nj2[b]zij= 1;8i2[n]:\n(1)\nThe parameter \u00152[0;1]controls the trade-off between\nhashing schemes that map to the same bucket elements\nthat are similar in terms of their observed frequencies in the\npreﬁx (\u0015!1)and hashing schemes that put more weight\non the elements’ feature-wise similarity (\u0015!0). Therefore,\nwe refer to the ﬁrst term in the objective as the estimation\nerror and to the second term as the similarity error.\nProblem (1) is a nonlinear binary optimization problem,\nso it is, in principle, hard to solve. Therefore, we next\ndevelop different approaches that can be used to solve it\nto optimality or near-optimality in different regimes.\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 5\n4.2 Mixed-Integer Linear Reformulation\nAs we show next, Problem (1) can be as reformulated as a\nmixed integer linear optimization problem by introducing\nauxiliary variables and new constraints. Formally, we have\nthe following theorem:\nTheorem 1. Problem (1)is equivalent with the following mixed-\ninteger linear optimization problem:\nmin\nZ2f0;1gn\u0002b;\nE2Rn\u0002b\n\u00150;\n\u00022Rn\u0002n\u0002b\n\u00150;\n\u00012[0;1]n\u0002n\u0002bX\ni2[n]X\nj2[b]2\n4\u0015\u0012iij+ (1\u0000\u0015)X\nk2[n]\u000eikjkxi\u0000xkk23\n5\ns.t.X\nj2[b]zij= 1;\n8i2[n];X\nk2[n]\u0012ikj\u0000f0\niX\nk2[n]zkj+X\nk2[n]f0\nkzkj\u00150;\n8i2[n];8j2[b];X\nk2[n]\u0012ikj+f0\niX\nk2[n]zkj\u0000X\nk2[n]f0\nkzkj\u00150;\n8i2[n];8j2[b];\n\u0012ikj\u0015eij\u0000M(1\u0000zkj);\n8i2[n];8k2[n];8j2[b];\n\u0012ikj\u0014eij;\n8i2[n];8k2[n];8j2[b];\n\u0012ikj\u0014Mzkj;\n8i2[n];8k2[n];8j2[b];\n\u000eikj\u0015zij+zkj\u00001;\n8i2[n];8k2[n];8j2[b];\n\u000eikj\u0014zij;\n8i2[n];8k2[n];8j2[b];\n\u000eikj\u0014zkj;\n8i2[n];8k2[n];8j2[b];\n(2)\nwhereMis a constant that satisﬁes M\u0015maxi2[n]f0\ni.\nProof. The proof is presented in Appendix A.\nProblem (2) consists of O(n2b)variables and constraints.\nAs our computational study in Section 6 suggests, by solv-\ning the reformulated Problem (2), we are able to compute\noptimal hashing schemes for problems with thousands of\nelements. Nevertheless, solving a mixed integer linear op-\ntimization problem of that size can still be prohibitive in\nthe applications we consider. For example, in the real-world\ncase study in Section 7, we map up to tens of thousands of\nelements to up to thousands of buckets, so Formulation (2)\nwould consist of variables and constraints in the order of\n1011. Therefore, we next develop a tailored block coordinate\ndescent algorithm that works well in practice.\n4.3 Efﬁcient Block Coordinate Descent Algorithm\nBy exploiting the problem structure, we propose the follow-\ning efﬁcient block coordinate descent algorithm (Algorithm1) that can be used to either heuristically solve Problem (1)\nor compute high-quality warm starts for Problem (2).\nConcerning the algorithm’s initialization, we start from\na random allocation of elements to buckets. Alternatively,\nwe could sort elements in U0in terms of their observed\nfrequencies and allocate the ﬁrsth\nU0\nbi\nelements to the ﬁrst\nbucket, the nexth\nU0\nbi\nto the second bucket, and so forth,\nor we could even use the heavy-hitter heuristic (that is,\nassign heavy-hitters to their own bucket and the remaining\nelements at random).\nIn our implementation, we maintain, for each bucket,\nthe set of elements Ijmapped therein, its cardinality cj\nand mean frequency \u0016j, as well as the associated esti-\nmation error ej=P\ni2Ij\f\ff0\ni\u0000\u0016j\f\fand similarity error\nsj=P\n(i;k)2Ij\u0002Ijkxi\u0000xkk2. After any update performed\nby Algorithm 1 we only need to update the above quantities,\ninstead of having to recompute them from scratch and,\ntherefore, we can directly evaluate the objective function\nvalue\"associated with any particular mapping of elements\nto buckets.\nIn each iteration, Algorithm 1 examines sequentially and\nin random order all nblocks ofbvariableszi; i2[n].\nNotice that each block contains all possible mappings of\na particular element to any bucket. For each element i,\nwe greedily select the mapping that minimizes the overall\nestimation error. To do so, we remove element ifrom its\ncurrent bucket and compute the estimation error associated\nwith each bucket j, ﬁrst with element iallocated to bucket\njand then without element i. We allocate element ito the\nbucketj?that minimizes the sum of all error terms.\nThe algorithm terminates when the improvement in\nestimation error is negligible; in case we are willing to obtain\nan intermediate solution faster, the termination criterion can\nbe set to a user-speciﬁed maximum number of iterations.\nAs we empirically show, Algorithm 1 converges to a local\noptimum after a few tens of iterations and produces high-\nquality solutions. Given that algorithm is not guaranteed to\nconverge to a globally optimum solution, the process can be\nrepeated multiple times from different starting points.\nAlgorithm 1 can be efﬁciently implemented so that the\ncomplexity of each iteration is O(n2b). This is to be expected\nsince, for each bucket, we need to compute the similarity\nerror between all pairs of elements mapped therein, which\nrequiresO(n2b)operations.\n4.4 The\u0015= 1 Case: Efﬁcient Dynamic Programming\nAlgorithm\nIn the special case where we set \u0015= 1, that is, we do not\ntake the features into account when computing the optimal\nhashing scheme, we obtain the following formulation:\nmin\nZ2f0;1gn\u0002bX\ni2[n]X\nj2[b]zij\f\f\f\f\ff0\ni\u0000P\nk2[n]zkjf0\nkP\nk2[n]zkj\f\f\f\f\f\ns.t.X\nj2[b]zij= 1;8i2[n]:(3)\nProblem (3) is an one-dimensional k-median clustering\nproblem and has been thoroughly studied in the literature.\nIt is fairly straightforward to develop an O(n2b)dynamic\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 6\nAlgorithm 1: Block Coordinate Descent Algorithm.\nInput: Observed frequency vector f02Nn, number of buckets b2N, hyperparameter \u00152[0;1].\nOutput: Learned one-hot hashing scheme Z2f0;1gn\u0002b.\n1:InitializeZsatisfyingP\nj2[b]zij= 1;8i2[n]\n2:\"0 0.Objective function value for initial map\n3:forj2[b]do\n4:.Find set of elements, cardinality, and mean for bucket jin initial map:\n5:Ij;cj;\u0016j fi2[n] :zij= 1g;jIjj;P\ni2Ijf0\ni\ncj\n6:.Compute estimation error ejand similarity error sjfor bucket jin initial map:\n7:ej;sj P\ni2Ij\f\ff0\ni\u0000\u0016j\f\f;P\n(i;k)2Ij\u0002Ijkxi\u0000xkk2\n8:\"0 \"0+ [\u0015ej+ (1\u0000\u0015)sj]\n9:end for\n10:t 0\n11:repeat\n12: Draw a random permutation \u001bof the set [n]\n13: fori2[n]do\n14: forj2[b]do\n15:.Check if \u001biis already in bucket jand compute error with and without \u001bi:\n16: if\u001bi2Ijthen\n17: \"\u001bi;j \u0015ej+ (1\u0000\u0015)sj\n18: \"\u0000\u001bi;j \u0015\u0012P\nk2Ijnf\u001big\f\f\f\ff0\nk\u0000cj\u0016j\u0000f0\n\u001bi\ncj\u00001\f\f\f\f\u0013\n+ (1\u0000\u0015)\u0010\nsj\u00002P\nk2Ijkx\u001bi\u0000xkk2\u0011\n19: .Update bucket jstats and errors after removing \u001bi:\n20:Ij;cj;\u0016j Ijnf\u001big;cj\u00001;cj\u0016j\u0000f0\n\u001bi\ncj\u00001\n21: ej;sj P\nk2Ij\f\ff0\nk\u0000\u0016j\f\f;sj\u00002P\nk2Ijkx\u001bi\u0000xkk2\n22: else\n23: \"\u001bi;j \u0015\u0012P\nk2Ij[f\u001big\f\f\f\ff0\nk\u0000cj\u0016j+f0\n\u001bi\ncj+1\f\f\f\f\u0013\n+ (1\u0000\u0015)\u0010\nsj+ 2P\nk2Ijkx\u001bi\u0000xkk2\u0011\n24: \"\u0000\u001bi;j \u0015ej+ (1\u0000\u0015)sj\n25: end if\n26: end for\n27:.Find best bucket j?and update stats and errors after mapping \u001bito it:\n28:j? argminj2[b]\"\u001bi;j+P\n`2[b]nfjg\"\u0000\u001bi;`\n29:zi ej?.ej?denotes the j?-th standard unit vector\n30:Ij?;cj?;\u0016j? Ij?[f\u001big;cj?+ 1;cj?\u0016j?+f0\n\u001bi\ncj?+1\n31:ej?;sj? P\nk2Ij?\f\ff0\nk\u0000\u0016j?\f\f;sj?+P\nk2Ij?kx\u001bi\u0000xkk2\n32: end for\n33:t t+ 1\n34:\"t P\nj2[b][\u0015ej+ (1\u0000\u0015)sj]\n35:until\"t\u00001\u0000\"t<\u000f\n36:returnZ\nprogramming algorithm to solve Problem (3) to provable\noptimality as per [39]. An even more efﬁcient solution\nmethod for Problem (3) has been developed in the context\nof optimal quantization; using dynamic programming in\ncombination with a matrix searching technique, [40] solves\nProblem (3) to optimality in O(nb)time. We refer the inter-\nested reader to [41] for a detailed and uniﬁed presentation\nof the above methods.\nGiven that we can obtain an optimal solution to Problem\n(3) very fast, in O(nb)time, we propose to use it as a warm\nstart for the general \u00152[0;1)case. Therefore, we provide\nanother alternative for the initialization step of Algorithm 1,\nin addition to the ones discussed in Section 4.3.5 F REQUENCY ESTIMATION\nIn this section, we describe the frequency estimation compo-\nnent of the proposed estimator, which, in its simplest form,\nconsists of a multi-class classiﬁer.\n5.1 Frequency Estimation for Elements Seen in the Pre-\nﬁx\nOnce the optimal assignment Zis computed, we essentially\nhave a hash code hi=P\nj2[b]j\u0001 1(zij=1); i2[n];for each\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 7\nelementu2U0. Therefore, for element u2U0, indexed by\ni2[n], we simply estimate its frequency as\n~fi=P\nk2[n]:hk=hifkP\nk2[n]:hk=hi1=\u0016j:\nWe denote by hS:U0![b]the function that maps elements\nseen in the preﬁx to buckets according to the learned hash\ncode.\n5.2 Similarity-Based Frequency Estimation for Unseen\nElements\nTo be able to produce frequency estimates for elements that\ndid not appear in the preﬁx, i.e., u2UnU 0, we formulate\na multi-class classiﬁcation problem, mapping elements to\nbuckets based on their features. Formally, we search for a\nfunction\nhU:X! [b]:\nThe training set consists of all data points in\nf(xi;hi) :ui= (ki;xi)2U0g;\nthat is, all feature-hash code tuples for elements that ap-\npeared in the preﬁx. Such a classiﬁer will allow us to\nestimate the frequencies of unseen elements based on the\naverage of the frequencies of elements that “look” similar.\nThe estimate for element u= (k;x)2UnU 0is then\n~fu=P\nk2[n]:hk=hU(x)fk\nP\nk2[n]:hk=hU(x)1:\n5.3 Adaptive Counting Extension: Keeping Track of the\nFrequencies of Unseen Elements\nSo far, we have described a static approach; we learn the\noptimal hashing scheme for the elements that appear in the\nstream preﬁx and then keep track only of their frequencies.\nThe estimated frequencies for all elements are based only\non the frequencies of elements in U0(which appeared in\nS0). We next describe a dynamic approach, that keeps track\nof the frequencies of elements beyond the ones in U0. At a\nhigh level, the adaptive approach is based on approximately\ncounting the distinct elements in each bucket. We work as\nfollows.\n1) We learn the optimal hashing scheme based on\nthe observed stream preﬁx and train a classiﬁer\nmapping elements to buckets, as outlined above. For\neach bucket, we only record the number of elements\nthat are mapped therein (instead of storing the IDs\nof the elements that are mapped to this bucket).\nWe use the classiﬁer to determine which bucket any\nelement is mapped to.\n2) We maintain a Bloom ﬁlter [20] BF, i.e., a probabilis-\ntic data structure that, given a universe of elements\nUand a setU0\u0012 U , probabilistically tests, for\nany element u2 U , whetheru2 U0(here,U0\ncorresponds to the elements that have appeared in\nthe stream). If u2 U0, then we deterministically\nhave that BF (u) = 1 . However, if u62 U0, then\nit need not be the case that BF (u) = 0 (thereforea Bloom ﬁlter is prone to false positives - we will\nexplain the impact of those in the sequel).\n3) We initialize the Bloom ﬁlter based on the elements\nu2U0. Therefore, all elements u2U0will initially\nhave BF (u) = 1 . On the other hand, elements u62U0\nmay initially have either BF (u) = 0 or BF (u) = 1 .\n4) For every subsequent element uthat appears in\nthe stream after the stream preﬁx S0has been pro-\ncessed, we map it to a bucket j2[b]using the\ntrained classiﬁer. Then, we test whether we have\nalready seen u, using the Bloom ﬁlter. If BF (u) = 0 ,\nwe increase both the frequency \u001ejand the number\nof elements cjin the bucket j, and we set BF (u) = 1 .\nIf BF(u) = 1 , we only increase the frequency \u001ej.\n5) When queried for the frequency of any element u2\nU, regardless of whether it appeared in U0or not,\nwe estimate\n~fu=\u001ej\ncjBF(u);\nwherejis the bucket in which uis mapped using\nthe classiﬁer.\nThe impact of Bloom ﬁlters’ false positives is that the\nproposed approach will mark as seen elements that have\nnot appeared in the stream. When one such element actually\nappears in the stream, we will not increase the counter cj\nthat tracks the number of elements in the bucket jwhere\nthis element is mapped. Therefore, the estimated number of\nelementscjin bucketjwill be less than the actual number.\nAs a result, the adaptive counting extension will generally\noverestimate elements’ frequencies.\nThe ﬂowchart for the adaptive counting extension of the\nproposed approach is given in Figure 9d in Appendix B\n6 E XPERIMENTS ON SYNTHETIC DATA\nIn this section, we empirically evaluate the proposed ap-\nproach on synthetic data. We investigate the performance\nand scalability of the optimization approaches discussed\nin Section 4, and explore the possibility of using different\nclassiﬁers for unseen elements (as per Section 5).\n6.1 Data Generation Methodology\nThe data that we use in our synthetic experiments are\ngenerated according to the following methodology:\n- Elements: We parameterize the universe of elements\nUby a positive integer G2Z>0that controls the\nproblem size in the way that we explain next. We\ngenerateGgroups of elements G1;:::;GGof expo-\nnentially increasing sizes 2G0+1;:::; 2G0+G(where\nG02Z\u00150is an additional parameter that determines\nthe size of the smallest group; we use G0= 2 in our\nexperiments). We associate each group Gg;g2[G];\nwith ap-dimensional normal distribution (we use\np= 2 in our experiments to enable visualization)\nwith mean\u0016gselected uniformly at random from\n[\u000010;10]pand covariance matrix equal to the iden-\ntity. We draw the features associated with each ele-\nmentu2Ggas a realization of the p-dimensional\nnormal distribution N(\u0016g;I)that corresponds to the\nelement’s group.\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 8\n- Stream: We generate the data stream Saccording\nto the following process. We associate each group\nGg;g2[G]with an arrival probability that is pro-\nportional to1\ng. Within groupGg, we assign to each\nelementu2 Gga uniform probability of arrival\n1\njGgj. Thus, smaller groups are more likely to appear\nand elements therein have a larger probability of\nselection so that they represent the heavy hitters. We\nconstruct the stream by ﬁrst selecting the group that\neach new arrival belongs to and then selecting the\nactual element from within that group. As far as the\nstream preﬁxS0is concerned, we would want to\nmimic a real-world scenario where not all elements\nfrom within each group start appearing since the be-\nginning of the stream. Therefore, when we generate\nthe preﬁx, we only allow for a fraction g02[0;1]\nof elements to be selected from within each group\nGg;g2[G];each with probability1\ng0jGgj. Finally, we\nremark that, in our experiment, we generate a stream\npreﬁx of sizejS0j= 10\u00012G.\nFor example, by setting G= 10 andg0= 0:5, we obtain a\nproblem with 8;192 elements, out of which we only allow\nfor4;096 to appear in the preﬁx, which in turn has size\n10;240. Therefore, we aim to learn a hashing scheme that\nmaps at most 4;096 elements to 10buckets; the memory\nrequirements of such a hashing scheme would be \u001920KB.\n6.2 Algorithms and Software\nWe next summarize the algorithms and software that we use\nin our experiments. We note that all algorithms were imple-\nmented in Python 3 and all experiments were performed\non a standard Intel(R) Xeon(R) CPU E5-2690 @ 2.90GHz\nrunning CentOS release 7. We independently repeat each\nexperiment 10times and report the averaged error, as well\nas its standard deviation.\nWe implement and refer to the optimization algorithms\npresented in Section 4 as follows:\n-milp : Solves the mixed-integer linear optimization\nproblem (Problem (2)) from Section 4.2 using the\ncommercial MIO solver Gurobi [42].\n-bcd: Implements the block coordinate descent algo-\nrithm (Algorithm 1) from Section 4.3.\n-dp: Solves Problem (3) in linear time via dynamic\nprogramming (Section 4.4) using a Python wrapper\nfor the R package Ckmeans.1d.dp [39].\nThe machine learning algorithms that we examine in-\nclude a linear classiﬁer, namely, multinomial logistic regres-\nsion ( logreg ), a tree-based classiﬁer, namely, CART ( cart )\n[43], and an ensemble classiﬁer, namely, random forest ( rf)\n[44]. All methods are tuned using 10-fold cross validation;\nthe hyperparameters that we tune are the weight of a ridge\nregularization term for logreg , the minimum impurity\ndecrease and the maximum depth for cart , the maximum\nnumber of features in each split and the maximum depth\nforrf. Unless stated otherwise, we use cart as the under-\nlying classiﬁer in our experiments. We use the Scikit-learn\nmachine learning package’s implementation of all the above\nalgorithms [45].Finally, we use the following notation for the frequency\nestimation algorithms presented in this paper. We refer to\nthe proposed estimator as opt-hash . We refer to CMS\n(the standard Count-Min Sketch) as count-min and to\nLCMS (the learned Count-Min Sketch with the heavy-hitter\nheuristic) as heavy-hitter . We implement all the above\nestimators in Python.\n6.3 Visualization: Learned Hash Code for Seen and Un-\nseen Elements\nIn Figure 1, we show an instance of a synthetically generated\nproblem with G= 10 groups (Figure 1a colors elements\ndepending on their actual group). Figure 1b shows the\nlogarithm of the frequency of each element that appeared\nin a preﬁx of length jS0j= 1;000; we assume that a fraction\nofg0= 0:33elements from each group can appear in the\npreﬁx. In Figure 1c, we present the learned hash code for\nelements that actually appeared in the preﬁx (using the\nbcd algorithm), whereas Figure 1d illustrates the hash code\npredicted for unseen elements (using cart ).\n6.4 Results\nWe next present the results from our computational study\non synthetic data. Let Zdenote the learned hash code;\nfor elements i2 S 0,ziis obtained using one of the\nalgorithms presented in Section 4; for elements i62 S 0,\nziis obtained using machine learning, as per Section 5.2.\nThen, the metrics that we consider are the estimation er-\nrorP\ni2[n]P\nj2[b]zij\f\f\f\ff0\ni\u0000P\nk2[n]zkjf0\nkP\nk2[n]zkj\f\f\f\f, the similarity error\nP\ni2[n]P\nj2[b]zijP\nk2[n]zkjkxi\u0000xkk2, and the overall error,\ni.e., the convex combination of the above two error terms,\nweighted by \u0015and1\u0000\u0015, respectively, which is exactly the\nobjective function that we use in the proposed formulation.\nWe separately study the two error terms to shed light on\nthe trade-off that the proposed approach is faced with.\nMoreover, we distinguish between the error on elements\nwhich appeared in the preﬁx (and hence their estimate is\nextracted from the learned hashing scheme) and the error\non unseen elements which did not appear in the preﬁx (and\nhence their estimate is inferred using machine learning) to\nexamine the individual performance of each component of\nthe proposed approach. We also measure the running time\n(in seconds) of the algorithms (note that the running time\nincludes the time to learn both the hashing scheme and the\nclassiﬁer).\nExperiment 1: Impact of hyperparameter \u0015.In this\nexperiment, we study the impact of the hyperparameter\n\u0015on the learned hashing scheme. We set G= 6 and run\nthree different versions of opt-hash for varying \u0015: one\nthat uses milp to learn the hashing scheme, one uses bcd,\nand one uses dp. We record the estimation, similarity, and\noverall error on the preﬁx, as well as the running time of\neach algorithm. To examine the degree of sub-optimality\nofbcd, we present the actual values of the error terms\nthat constitute the objective function (estimation, similarity,\nand overall error), i.e., we do not convert them in a per\nelement/per pair of elements scale, which would be more\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 9\n10\n 5\n 0 5 1010\n5\n0510\n(a) Element groups.\n10\n 5\n 0 5 1010.0\n7.5\n5.0\n2.5\n0.02.55.07.510.0\n01234 (b) Preﬁx element frequencies.\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.010.0\n7.5\n5.0\n2.5\n0.02.55.07.510.0 (c) Hash code for elements that\nappeared in the preﬁx.\n10\n 5\n 0 5 1010\n5\n0510(d) Hash code for unseen ele-\nments.\nFig. 1. Visualization of element groups and hash codes.\ninterpretable. The results are presented in Figure 2. The key\ntakeaways from this experiment are as follows:\n-milp achieves the smallest overall error at the cost of\nincreased running times. Its edge over the heuristic\nbcd approach can be veriﬁed in terms of the esti-\nmation error, as it almost always improves over the\nsolution obtained by bcd.\n- The solutions obtained by bcd are of high quality;\nthe improvement achieved by applying the exact\nmilp approach is often negligible. For small problem\nsizes, the runtime of bcd is less than a second.\n- As expected, dpachieves the smallest estimation\nerror, since it optimizes only for the estimation error\nindependently of the value of \u0015. In terms of the\nsimilarity and the overall, the performance of dpis\nnotably worse, whereas its running time is less than\na second.\nNote that, in the \u0015= 1 case, all three methods are able to\nﬁnd comparable near-optimal solutions. The small deviation\nis due to suboptimality tolerances of the algorithms used.\nExperiment 2: Comparison between bcd and dpin\nthe\u0015= 1 case. In this experiment, we focus on the\n\u0015= 1 case and compare, for increasing values of G,bcd\nwith dp; in this case, the latter is guaranteed to ﬁnd the\noptimal hashing scheme. We again record the estimation,\nsimilarity, and overall error on the preﬁx, as well as the\nrunning time of each algorithm. In this and in subsequent\nexperiment, we convert the errors in a per element/per pair\nof elements scale. The results are presented in Figure 3. We\nobserve that, for problems with G\u001410,bcd computes near-\noptimal solutions fast; however, as Gfurther increases, the\nperformance of bcd deteriorates.\nExperiment 3: bcd from multiple starting points in the\ngeneral\u0015case. In this experiment, we set \u0015= 0:5and\nrunbcd multiple times from different starting points and\nfor increasing values of Gto examine the stability of the\nsolutions obtained. We again record the estimation, similar-\nity, and overall error on the preﬁx, as well as the running\ntime of each algorithm. The results, presented in Figure 4,\nindicate that bcd is robust to the (random) initialization of\nthe algorithm and computes stable solutions.\nExperiment 4: Impact of the fraction of elements seen\nin the preﬁx. In this experiment, we set G= 10 and vary\nthe value of g0;which controls the fraction of elements thatappear in the preﬁx. We explore two approaches for learning\nthe hashing scheme: ﬁrst, we set \u0015= 0:5and run bcd;\nthen, we run dp(which implies \u0015= 1 ). We now record\nthe estimation and similarity error both on the preﬁx S0\nand on elements that did not appear in S0but did appear\nwithinjSj= 10jS0jarrivals afterS0. Figure 5 suggests that\nobserving more elements in the preﬁx results in a decrease\nof the estimation error on both seen and unseen elements at\nthe cost of an increased similarity error.\nExperiment 5: Comparison between classiﬁcation\nmethods. In this experiment, we set g0= 0:33and\u0015= 0:5,\nvary the value of G, and explore the impact of using\ndifferent types of classiﬁers ( logreg ,cart ,rf) as part of\nopt-hash . We record the estimation, similarity, and overall\nerror on elements that did not appear in S0but did appear\nwithinjSj= 10jS0jarrivals afterS0. We also report the\ntraining time for each method. In Figure 6, we see that\nthere is indeed merit in using non-linear classiﬁers. We\nremark, however, that the results heavily depend on the data\ngenerating process.\n7 E XPERIMENTS ON REAL-WORLD DATA:\nSEARCH QUERY ESTIMATION\nIn this section, we empirically evaluate the proposed ap-\nproach on real-world search query data. The task of search\nquery frequency estimation seems particularly suited for\nthe proposed learning-based approach, given that popular\nsearch queries tend to appear consistently across multiple\ndays.\n7.1 Dataset\nIn the lines of [8], we use the AOL query log dataset, which\nconsists of 21 million search queries (with 3.8 million unique\nones) collected from 650 thousand anonymized users over\n90 days in 2006. Each query is a search phrase in free\ntext; for example, the 1stmost common query is “google”\nand appears 251,463 times over the entire 90-day period,\nthe10this “www.yahoo.com” and its frequency is 37,436,\nthe100this “mys” and its frequency is 5,237, the 1000th\nis “sharon stone” and its frequency is 926, the 10000this\n“online casino” and its frequency is 146, and so forth. As\nshown in [8], the distribution of search query frequency\nindeed follows the Zipﬁan law and hence the setting seems\nideal for their proposed algorithm (LCMS).\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 10\n0.0 0.2 0.4 0.6 0.8 1.0\nlambda100200300400500prefix_estimation_error\nExperiment 1\nbcd\ndp\nmilp\n(a) Estimation error on S0.\n0.0 0.2 0.4 0.6 0.8 1.0\nlambda10000200003000040000prefix_similarity_error\nExperiment 1\nbcd\ndp\nmilp (b) Similarity error on S0.\n0.0 0.2 0.4 0.6 0.8 1.0\nlambda010000200003000040000prefix_overall_error\nExperiment 1\nbcd\ndp\nmilp (c) Overall error (objective function\nvalue) onS0.\n0.0 0.2 0.4 0.6 0.8 1.0\nlambda0200400600800elapsed_time\nExperiment 1\nbcd\ndp\nmilp(d) Elapsed time (in sec).\nFig. 2. Impact of hyperparameter \u0015forG= 6.\n4 5 6 7 8 9 10 11 12\nnum_groups0.51.01.52.02.53.0prefix_estimation_error\nExperiment 2\n(a) Estimation error on S0.\n4 5 6 7 8 9 10 11 12\nnum_groups0.000.250.500.751.001.251.501.75prefix_similarity_error\nExperiment 2 (b) Similarity error on S0.\n4 5 6 7 8 9 10 11 12\nnum_groups0.51.01.52.02.53.0prefix_overall_error\nExperiment 2 (c) Overall error (objective function\nvalue) onS0.\n4 5 6 7 8 9 10 11 12\nnum_groups0100200300400500600700elapsed_time\nExperiment 2(d) Elapsed time (in sec).\nFig. 3. Comparison between dpandbcdfor\u0015= 1.\n4 5 6 7 8 9 10 11 12\nnum_groups2468prefix_estimation_errorExperiment 3\n(a) Estimation error on S0.\n4 5 6 7 8 9 10 11 12\nnum_groups0.050.100.150.200.250.30prefix_similarity_errorExperiment 3 (b) Similarity error on S0.\n4 5 6 7 8 9 10 11 12\nnum_groups1234prefix_overall_errorExperiment 3 (c) Overall error (objective function\nvalue) onS0.\n4 5 6 7 8 9 10 11 12\nnum_groups0100200300400500600700800elapsed_timeExperiment 3(d) Elapsed time (in sec).\nFig. 4. Comparison between bcdfrom multiple starting points for \u0015= 0:5.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nfraction_seen024681012prefix_estimation_error\nExperiment 4 - epoch 0\nbcd\ndp\n(a) Estimation error on S0.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nfraction_seen0.10.20.30.40.5prefix_similarity_error\nExperiment 4 - epoch 0\nbcd\ndp (b) Similarity error on S0.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nfraction_seen0.00.10.20.30.40.50.60.7unseen_estimation_error\nExperiment 4 - epoch 10\nbcd\ndp (c) Estimation error on elements\nu62S0afterjSj= 10jS0jarrivals.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nfraction_seen0.2\n0.00.20.40.60.81.01.21.4unseen_similarity_error\nExperiment 4 - epoch 10\nbcd\ndp(d) Similarity error on elements\nu62S0afterjSj= 10jS0jarrivals.\nFig. 5. Impact of fraction of seen elements in the preﬁx ( g0) forG= 10 .\n7.2 Baselines\nAs baselines, we use count-min andheavy-hitter . For\neach method, we maintain multiple versions correspond-\ning to different values of the method’s hyperparameters\nand report the best performing version. More speciﬁcally,\nfor ﬁxed sketch size (i.e., total number of buckets b),\nwe report the best performing for count-min ’s depth\nfrom the set d2 f1;2;4;6gand for heavy-hitter ’s\ndepthd2f1;2;4;6gand number of heavy-hitter buckets\nbheavy2f10;102;103;104g(provided that bheavy ﬁts within\nthe available memory, i.e., bheavy\u0014b=2). Additionally, weassume that heavy-hitter has access to an ideal heavy-\nhitter oracle, i.e., the IDs of the heavy-hitters in the test set\n(over the entire 90-day period) are known. Therefore, we\ncompare the proposed method with the ideal version of the\nmethod proposed in [8], which was in fact shown to signif-\nicantly outperform any realistically implementable version\nofheavy-hitter that relied upon non-ideal heavy-hitter\noracles (e.g., recurrent neural network classiﬁer).\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 11\n4 5 6 7 8 9 10\nnum_groups24681012141618unseen_estimation_errorExperiment 5 - epoch 10\ncart\nlogreg\nrf\n(a) Estimation error on elements\nu62S0afterjSj= 10jS0jarrivals.\n4 5 6 7 8 9 10\nnum_groups0.10.20.30.40.5unseen_similarity_errorExperiment 5 - epoch 10\ncart\nlogreg\nrf(b) Similarity error on elements\nu62S0afterjSj= 10jS0jarrivals.\n4 5 6 7 8 9 10\nnum_groups246810121416unseen_overall_errorExperiment 5 - epoch 10\ncart\nlogreg\nrf(c) Overall error (objective function\nvalue) on elements u62 S 0after\njSj= 10jS0jarrivals.\n4 5 6 7 8 9 10\nnum_groups010203040506070elapsed_timeExperiment 5 - epoch 10\ncart\nlogreg\nrf(d) Elapsed time (in sec).\nFig. 6. Comparison between classiﬁcation methods.\n7.3 Remarks on the Learned Hashing Scheme\nAs far as opt-hash is concerned, we make the following\nremarks:\n- We consider the ﬁrst day to be the observed stream\npreﬁxS0and use (part of) the queries u2 U0\n0\u0012\nU0therein (along with their number of occurrences\nduring the ﬁrst day) to learn the optimal hashing\nscheme via Algorithm 1 and for \u0015= 1.\n- The ﬁrst day consists of over 200,000 unique queries\nand just storing their IDs would require 200,000\nbuckets. Thus, we randomly sample a subset of the\nobserved queries, with probabilities proportional to\ntheir observed frequencies. We use the sampled sub-\nset of queries as input to Algorithm 1.\n- For ﬁxed number of total buckets btotal, we need to\ndetermine the ratio cbetween the number of buckets\nbthat the learned hashing scheme will consist of and\nthe number of queries nwhose IDs we will store.\nTherefore, for user-speciﬁed btotal andc, we pickb\nandnaccording to\nn=btotal=1+c; b =btotal\u0000n:\nIn our experiments, we examine c2f0:03;0:3g:\n- For the classiﬁer g:X ! [b], mapping unseen\nqueriesu2UnU0\n0to buckets (as per Section 5.2),\nwe found that rfachieves the best trade-off between\ntraining time and classiﬁcation accuracy and use this\nmodel in the results we report.\n- To create input features for the classiﬁer g, we follow\na simple bag-of-words approach and only keep the\n500 most common words in the training queries.\nWe also include as features the number of ASCII\ncharacters in the query text, the number of punctu-\nation marks, the number of dots, and the number\nof whitespaces. As a result, the proposed approach\nis simple and interpretable, yet strong (as we show\nnext).\n7.4 Results\nWe implement our experiments in Python 3 and use the\nScikit-learn machine learning package [45]. We indepen-\ndently repeat each experiment 5times and report the av-\neraged error, as well as its standard deviation. We remark\nthat each bucket consumes 4bytes of memory and hence\nthe total number of buckets used in each experiment can be\ncalculated as b=m\u0001103\n4;wheremis the size of the estimatorin KB. Moreover, we denote by Utthe set of queries that\nappear in day t, and byft\nUtand~ft\nUttheir aggregated\ntrue frequencies and estimated frequencies, respectively,\nbetween days 0andt.\nIn Figure 7, we show the estimation error as function of\nthe estimator’s size in KB, after the 30thand the 70thday.\nOn the the left (Figures 7a and 7c), we plot the average (per\nelement) estimation error\n1\njUtjX\nu2Utjft\nu\u0000~ft\nuj:\nOn the the right (Figures 7b and 7d), we plot the expected\nmagnitude of the absolute estimation error\n1P\nu2UtfuX\nu2Utft\nu\u0001jft\nu\u0000~ft\nuj:\nNotice that the former metric is expressed in a per element\nscale, that is, we normalize the overall error by the total\nnumber of elements jUtjand hence all elements are penal-\nized uniformly, whereas the second metric, the expected\nmagnitude of the absolute estimation error, penalizes el-\nements proportionally to their actual frequencies, as per\nSection 4.1.\nWe observe that the trend in the estimation error is\nvery similar after the 30thand the 70thday. What changes\nis the absolute value of the estimation error, which, as\nexpected, deteriorates with time, uniformly for all methods.\nThe proposed method opt-hash consistently outperforms\nits competitors, in terms of both metrics. Unsurprisingly, as\nthe size of all estimators increases, their errors drop. This is\nthe case with both the average and the expected estimation\nerror. We make the following additional remarks:\n- The superiority of opt-hash is most notable in\nterms of average (per element) error. This is partly\ndue to the fact that opt-hash does a substantially\nbetter job at estimating the frequencies of rarely\noccurring queries. In particular, queries that appear\nvery few times are placed in the same bucket and\nhence the estimation error on them is small. In con-\ntrast, heavy-hitter andcount-min often place\nsuch queries in the same bucket with queries of\nmedium or even high frequencies, which produces\nbig estimation error.\n- The expected magnitude of the estimation error\nofheavy-hitter andcount-min does seem to\nslowly converge towards that of opt-hash when the\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 12\n0 20 40 60 80 100 120\nSize (in KB)102103104Average Absolute ErrorDay 30.\ncount-min\nheavy-hitter\nopt-hash\n(a) Average per element absolute\nerror after the 30thday.\n0 20 40 60 80 100 120\nSize (in KB)103104Expected Absolute ErrorDay 30.\ncount-min\nheavy-hitter\nopt-hash(b) Expected magnitude of abso-\nlute error after the 30thday.\n0 20 40 60 80 100 120\nSize (in KB)102103104105Average Absolute ErrorDay 70.\ncount-min\nheavy-hitter\nopt-hash(c) Average per element absolute\nerror after the 70thday.\n0 20 40 60 80 100 120\nSize (in KB)103104Expected Absolute ErrorDay 70.\ncount-min\nheavy-hitter\nopt-hash(d) Expected magnitude of abso-\nlute error after the 70thday.\nFig. 7. Estimation error as function of the estimator’s size (in KB).\n0 20 40 60 80\nTime (in days)0500010000150002000025000Average Absolute ErrorSize = 4.0 KB.\ncount-min\nheavy-hitter\nopt-hash\n(a) Average per element absolute\nerror using 4 KB of memory.\n0 20 40 60 80\nTime (in days)0500010000150002000025000Expected Absolute ErrorSize = 4.0 KB.\ncount-min\nheavy-hitter\nopt-hash(b) Expected magnitude of abso-\nlute error using 4 KB of memory.\n0 20 40 60 80\nTime (in days)0200400600800Average Absolute ErrorSize = 120.0 KB.\ncount-min\nheavy-hitter\nopt-hash(c) Average per element absolute\nerror using 120 KB of memory.\n0 20 40 60 80\nTime (in days)0200400600800Expected Absolute ErrorSize = 120.0 KB.\ncount-min\nheavy-hitter\nopt-hash(d) Expected magnitude of abso-\nlute error using 120 KB of memory.\nFig. 8. Estimation error as function of time (in days).\nestimators’ size becomes sufﬁciently large. This indi-\ncates that opt-hash is particularly suited for low-\nspace regimes and can achieve much more effective\ncompression of the frequency vector.\n- As far as heavy-hitter andcount-min are con-\ncerned, the former does produce better estimates,\nwhich is in agreement with the results in [8]. The\nimprovement is much more notable in terms of\nthe expected magnitude of the estimation error.\nThis observation is to be expected as well, given\nthatheavy-hitter makes zero error on the most\nfrequently occurring elements, which are heavily\nweighed in this metric.\nFigure 8 reports the estimation error as function of time\n(in days), for two different memory conﬁgurations (4 KB\nin Figures 8a and 8b, 120 KB in Figures 8c and 8d). The\nsuperiority of opt-hash is preserved over time, in terms\nof both metrics. Moreover, we observe opt-hash achieves\nthe smallest standard deviation in its estimation error. This\ncan be attributed to the fact that the mappings of elements\nto buckets are more stable than those of heavy-hitter\nand count-min , as they are obtained via optimization\ninstead of randomization; the main source of randomness\nforopt-hash is the classiﬁer.\nWe next experiment with memory conﬁgurations that\nvary between 1.2 KB and 120 KB, and compare opt-hash\nwith count-min andheavy-hitter . The proposed ap-\nproach provides an average improvement (over the entire\n90-day period) by one to two orders of magnitude, in terms\nof its average (per element) absolute estimation error, and\nby 45-90%, in terms of its expected magnitude of estimation\nerror. For example, with 120 KB of memory, opt-hash\nmakes an average absolute estimation error of \u001829in\nestimating the frequency of each query, whereas the error of\nheavy-hitter is\u0018479(Figure 7a). With 4 KB of memory,the errors of opt-hash andheavy-hitter are\u0018167\nand\u001814;661, respectively (Figure 7c). Table 1 shows the\naverage (per element) error after the entire 90-day period\nas a percentage of each query’s frequency for the 1st, the\n10th, the100th, the1;000th, and the 10;000thmost common\nqueries.\nQuery rank (by frequency) Query frequency Average error percentage (%)\n1 251,463 0.01\n10 37,436 0.08\n100 5,237 0.55\n1,000 926 3.13\n10,000 146 19.86\nTABLE 1\nAverage (per element) error as percentage of query’s frequency.\nAn additional feature of opt-hash is that, by using\ninterpretable features in its machine learning component, it\nprovides insights into the underlying frequency estimation\nproblem. In particular, the features that were consistently\nmarked as most important are the four counts (i.e., number\nof ASCII characters in the query text, the number of punctu-\nation marks, the number of dots, and the number of whites-\npaces), as well as the words “com,” “www,” “google,” and\n“yahoo.” Intuitively, this observation makes sense. For in-\nstance, a large number of ASCII characters and whitespaces\nwould be indicative of a big query with multiple words,\nmaking it more likely to be rare. On the other hand, a query\ncontaining the word “google” would be more likely to be\ncommon, given that “google” is consistently part of the most\nfrequently occurring queries.\n8 C ONCLUSION\nIn this paper, we developed a novel approach for the prob-\nlem of frequency estimation in data streams that relies on the\nuse of optimization and machine learning on an observed\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 13\nstream preﬁx. First, we formulated and efﬁciently solved the\nproblem of optimally (or near-optimally) hashing the ele-\nments seen in the preﬁx to buckets, hence providing a smart\nalternative to oblivious random hashing schemes. To this\nend, we reformulated the problem as a mixed-integer lin-\near optimization problem, we developed an efﬁcient block\ncoordinate descent algorithm, and, in a special case, we\nused dynamic programming to solve the problem in linear\ntime. Next, we trained a classiﬁer mapping unseen elements\nto buckets. As we discussed, during stream processing,\nwe only keep track of the frequencies of those elements\nthat appeared in the preﬁx; the estimate the frequency of\nany element (either seen or unseen) is the average of the\nfrequencies of all elements that map to the same bucket.\nWe also described an adaptive approach that enables us to\nupdate the compressed frequency vector and keep track of\nthe frequencies of all elements. We used synthetic data to\ninvestigate the performance, the scalability, and the impact\nof various design choices for the proposed approach; our\nstudy suggested that the proposed algorithms can compute\noptimal hashing schemes for problems with thousands of\nelements, using the mixed-integer linear optimization re-\nformulation or the dynamic programming approach, and\nhigh quality hashing schemes for problems with tens of\nthousands of elements using the block coordinate descent\nalgorithm. Finally, we applied the proposed approach to\nthe problem of search query frequency estimation and eval-\nuated it using real-world data and empirically showed that\nthe proposed learning-based streaming frequency estima-\ntion algorithm achieves superior performance compared to\nexisting streaming frequency estimation algorithms.\nAPPENDIX A\nPROOF OF THEOREM 1\nProof. We introduce variables E2Rn\u0002b\n\u00150such thateij\ncorresponds to the absolute estimation error associated with\nmapping element ito bucketj. Since we are minimizing a\nnonnegatively weighed sum of such nonnegative terms, it\nsufﬁces to require that\neij\u0015f0\ni\u0000P\nk2[n]zkjf0\nkP\nk2[n]zkj; eij\u0015\u0000f0\ni+P\nk2[n]zkjf0\nkP\nk2[n]zkj;\n(4)\nfor alli2[n];j2[b]. To get rid of the fractional term in (4),\nwe multiply both equations withP\nk2[n]zkj; this results in\nbilinear terms of the form eijP\nk2[n]zkj. To linearize those,\nwe introduce variables \u00022Rn\u0002n\u0002b\n\u00150such that\u0012ikj=eijzkj\ncan be interpreted as the error associated with mapping ele-\nmentito bucketjwhenkis also mapped therein. Since \u0012ikj\nis the product of a binary variable and a continuous variable,\nwe can linearize the constraint \u0012ikj=eijzkjby introducing\na big-M constant such that, for all i2[n];j2[b],eij\u0014M.\nWe then require that\n\u0012ikj\u0015eij\u0000M(1\u0000zkj); \u0012ikj\u0014eij; \u0012ikj\u0014Mzkj;(5)\nfor alli2[n];k2[n];j2[b]. Thus, (4) can be rewritten as\nX\nk2[n]\u0012ikj\u0015f0\niX\nk2[n]zkj\u0000X\nk2[n]f0\nkzkj;\nX\nk2[n]\u0012ikj\u0015\u0000f0\niX\nk2[n]zkj+X\nk2[n]f0\nkzkj;(6)which is linear in all variables. To linearize the other bilinear\nterm that appears in the objective function, we introduce\nanother set of auxiliary variables \u00012[0;1]n\u0002n\u0002bsuch\nthat\u000eikj=zijzkjindicates whether elements iandkare\nmapped together to bucket j. We then have the constraints\n\u000eikj\u0015zij+zkj\u00001; \u000eikj\u0014zij; \u000eikj\u0014zkj; (7)\nfor alli2[n];k2[n];j2[b]. Using the above new\nvariables, the objective function can be written as\nX\ni2[n]X\nj2[b]2\n4\u0015\u0012iij+ (1\u0000\u0015)X\nk2[n]\u000eikjkxi\u0000xkk23\n5:(8)\nFinally, we have to properly select the constant Min (5)\nso that it is a valid upper bound for the variables E; such\na bound can be obtained by setting M\u0015maxi2[n]f0\ni, i.e.,\nthe estimation error associated with any element cannot be\ngreater than the largest frequency observed in the preﬁx.\nAPPENDIX B\nFLOWCHARTS FOR THE PROPOSED APPROACH\nFigure 9 provides the ﬂowcharts for the proposed approach.\nIn particular, Figure 9a corresponds to the learning phase,\nwhere the stream preﬁx is used to learn the optimal hashing\nscheme and the classiﬁer; Figure 9b illustrates how the\nproposed approach answers count queries for any input\nelement; Figures 9c and 9d show the update mechanism of\nthe proposed approach without and with the use of Bloom\nﬁlters, respectively.\nAPPENDIX C\nTABLE OF NOTATIONS\nTable 2 includes the basic notations that are used repeatedly\nthroughout the paper; we explain notations that are not used\nrepeatedly in the main text. We note that we generally use\nthe indexuto refer to elements, the indices iandkto refer to\nelement IDs, the index jto refer to buckets. For simplicity in\nnotation, elements are referred to using either their symbol u\nor their IDi, depending on the context; similarly, frequencies\nare indexed using either of the two approaches.\nREFERENCES\n[1] S. Muthukrishnan, Data streams: Algorithms and applications . Now\nPublishers Inc, 2005.\n[2] M. Garofalakis, J. Gehrke, and R. Rastogi, Data stream management:\nprocessing high-speed data streams . Springer, 2016.\n[3] “Internet live stats,” https://www.internetlivestats.com/\ngoogle-search-statistics, accessed: 2020-07-01.\n[4] “Netﬂow services and applications,” Cisco systems white paper\n(1999) http://www.cisco.com.\n[5] L. Bhuvanagiri and S. Ganguly, “Estimating entropy over data\nstreams,” in European Symposium on Algorithms . Springer, 2006,\npp. 148–159.\n[6] J. Misra and D. Gries, “Finding repeated elements,” Science of\ncomputer programming , vol. 2, no. 2, pp. 143–152, 1982.\n[7] G. Cormode and M. Hadjieleftheriou, “Finding the frequent items\nin streams of data,” Communications of the ACM , vol. 52, no. 10, pp.\n97–105, 2009.\n[8] C.-Y. Hsu, P . Indyk, D. Katabi, and A. Vakilian, “Learning-based\nfrequency estimation algorithms,” in International Conference\non Learning Representations , 2019. [Online]. Available: https:\n//openreview.net/forum?id=r1lohoCqY7\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 14\nTraining data:𝒙𝑢,ℎ!𝑢,∀𝑢∈𝑆\"Learn hash function:ℎ!:𝑢:𝑢∈𝑆\"→[𝑏]Learn classifier:ℎ#:𝑋→[𝑏]Input stream prefix:𝑆!=(𝑢\",…,𝑢|$!|)Element u’s features: 𝒙(𝑢)Start\nEnd\n(a) Learning the optimal hashing scheme.\nInputquery:𝑢∈𝑈𝑢∈𝑆!Find u’s bucket using learned hash function:𝑗=ℎ\"(𝑢)Output estimate:𝑓#𝑐#Find u’s bucket using learned classifier:j =ℎ$(𝒙(𝑢))YesNoStart\nEnd (b) Answering count queries for element u2U.\nInput arrival:𝑢!∈𝑈𝑢!∈𝑆\"Find 𝑢’s bucket using learned hash function:j ∶=ℎ#(𝑢)YesNo𝑓$=𝑓$+1Start\nEnd\n(c) Updating sketch at time tupon arrival of element ut2U.\nInputarrival:𝑢!∈𝑈𝑢!∈𝑆\"Find 𝑢’s bucket using learned hash function:j =ℎ#(𝑢)Find 𝑢’s bucket using learned classifier:𝑗=ℎ$(𝒙(𝑢))YesNo𝑓%=𝑓%+1Check𝐵𝐹𝑢!.Has 𝑢!appeared before?\n𝑐%=𝑐%+1NoYes\nUpdate Bloom filter so that 𝑢_𝑡is marked as seen:𝐵𝐹(𝑢!)=1Start\nEnd (d) Updating sketch with Bloom ﬁlter extension at time tupon arrival of\nelementut2U.\nFig. 9. Flowcharts for the proposed approach.\n[9] P . Domingos and G. Hulten, “Mining high-speed data streams,”\ninProceedings of the sixth ACM SIGKDD international conference on\nKnowledge discovery and data mining , 2000, pp. 71–80.\n[10] G. Cormode, M. Garofalakis, P . J. Haas, and C. Jermaine, “Syn-\nopses for massive data: Samples, histograms, wavelets, sketches,”\nFoundations and Trends in Databases , vol. 4, no. 1–3, pp. 1–294, 2012.\n[11] G. Cormode and S. Muthukrishnan, “An improved data stream\nsummary: the count-min sketch and its applications,” Journal of\nAlgorithms , vol. 55, no. 1, pp. 58–75, 2005.\n[12] M. Charikar, K. Chen, and M. Farach-Colton, “Finding frequent\nitems in data streams,” in International Colloquium on Automata,\nLanguages, and Programming . Springer, 2002, pp. 693–703.\n[13] N. Alon, Y. Matias, and M. Szegedy, “The space complexity of\napproximating the frequency moments,” Journal of Computer and\nsystem sciences , vol. 58, no. 1, pp. 137–147, 1999.\n[14] M. Yu, L. Jose, and R. Miao, “Software deﬁned trafﬁc measurement\nwith opensketch,” in Presented as part of the 10th fUSENIXgSympo-\nsium on Networked Systems Design and Implementation ( fNSDIg13),\n2013, pp. 29–42.\n[15] A. Goyal, H. Daumé III, and G. Cormode, “Sketch algorithms for\nestimating point queries in nlp,” in Proceedings of the 2012 joint\nconference on empirical methods in natural language processing and\ncomputational natural language learning , 2012, pp. 1093–1103.\n[16] A. Gilbert and P . Indyk, “Sparse recovery using sparse matrices,”\nProceedings of the IEEE , vol. 98, no. 6, pp. 937–947, 2010.[17] A. Aghazadeh, R. Spring, D. Lejeune, G. Dasarathy, A. Shrivastava\net al. , “Mission: Ultra large-scale feature selection using count-\nsketches,” in International Conference on Machine Learning , 2018, pp.\n80–88.\n[18] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data , 2018, pp. 489–504.\n[19] M. Mitzenmacher, “A model for learned bloom ﬁlters and optimiz-\ning by sandwiching,” in Advances in Neural Information Processing\nSystems , 2018, pp. 464–473.\n[20] B. H. Bloom, “Space/time trade-offs in hash coding with allowable\nerrors,” Communications of the ACM , vol. 13, no. 7, pp. 422–426,\n1970.\n[21] A. Broder and M. Mitzenmacher, “Network applications of bloom\nﬁlters: A survey,” Internet mathematics , vol. 1, no. 4, pp. 485–509,\n2004.\n[22] T. Yang, L. Wang, Y. Shen, M. Shahzad, Q. Huang, X. Jiang,\nK. Tan, and X. Li, “Empowering sketches with machine learning\nfor network measurements,” in Proceedings of the 2018 Workshop\non Network Meets AI & ML , ser. NetAI’18. New York, NY, USA:\nAssociation for Computing Machinery, 2018, p. 15–20. [Online].\nAvailable: https://doi.org/10.1145/3229543.3229545\n[23] J. Wang, T. Zhang, N. Sebe, H. T. Shen et al. , “A survey on\nlearning to hash,” IEEE transactions on pattern analysis and machine\nintelligence , vol. 40, no. 4, pp. 769–790, 2017.\n\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 15\nSymbol Explanation\nGeneral symbols:\nU Universe of elements\nU0 Set of elements that appeared in the stream\npreﬁx\nn jU0j\nu2U Element\nk2[jUj] Element’s unique ID\nX Feature space\nx2X Element’s features\nS= (u1;:::;u jSj) Data stream\nS0 Data stream preﬁx\nfu Frequency of element uinS\nf0\nu Frequency of element uinS0\n~fu Estimate of frequency of element uinS\nb Sketch’s total buckets\nSymbols related to CMS and LCMS:\nwandd Sketch width and depth\n\u001ej(or\u001el\nj) Aggregate frequency in bucket j(or bucketj\nin levell); this is used in CMS and LCMS\nhHH(\u0001) Classiﬁer that decides whether element uis a\nheavy hitter\nSymbols related to the proposed approach:\nIj Set of elements in bucket j\ncj Number of elements in bucket j\n\u0016j Mean of frequencies of elements in bucket j\nzi One-hot binary hash code for element with\nIDi\nhi Integer hash code for element with ID i\n\u0015 Hyperparameter that controls the trade-off\nbetween estimation error and similarity error\nhS(\u0001) Function that maps elements that appeared\nin the preﬁx to buckets based on the learned\nhash code\nhU(\u0001) Classiﬁer that maps elements to buckets\nTABLE 2\nNotations.\n[24] B. Kulis and T. Darrell, “Learning to hash with binary recon-\nstructive embeddings,” in Advances in neural information processing\nsystems , 2009, pp. 1042–1050.\n[25] G. Lin, C. Shen, D. Suter, and A. Van Den Hengel, “A general\ntwo-step approach to learning-based hashing,” in Proceedings of\nthe IEEE international conference on computer vision , 2013, pp. 2552–\n2559.\n[26] G. Lin, C. Shen, Q. Shi, A. Van den Hengel, and D. Suter, “Fast su-\npervised hashing with decision trees for high-dimensional data,”\ninProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , 2014, pp. 1963–1970.\n[27] A. Andoni and I. Razenshteyn, “Optimal data-dependent hashing\nfor approximate near neighbors,” in Proceedings of the forty-seventh\nannual ACM symposium on Theory of computing , 2015, pp. 793–801.\n[28] M. Purohit, Z. Svitkina, and R. Kumar, “Improving online al-\ngorithms via ml predictions,” in Advances in Neural Information\nProcessing Systems , 2018, pp. 9661–9670.\n[29] H. Mao, M. Schwarzkopf, S. B. Venkatakrishnan, Z. Meng, and\nM. Alizadeh, “Learning scheduling algorithms for data processing\nclusters,” in Proceedings of the ACM Special Interest Group on Data\nCommunication , 2019, pp. 270–288.\n[30] A. Mousavi, A. B. Patel, and R. G. Baraniuk, “A deep learning\napproach to structured signal recovery,” in 2015 53rd annual aller-\nton conference on communication, control, and computing (Allerton) .\nIEEE, 2015, pp. 1336–1343.\n[31] E. B. Khalil, P . Le Bodic, L. Song, G. Nemhauser, and B. Dilkina,\n“Learning to branch in mixed integer programming,” in Thirtieth\nAAAI Conference on Artiﬁcial Intelligence , 2016.\n[32] M.-F. Balcan, T. Dick, and T. Sandholm, “Learning to branch,” in\nInternational Conference on Machine Learning , 2018.\n[33] E. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song, “Learning\ncombinatorial optimization algorithms over graphs,” in Advances\nin Neural Information Processing Systems , 2017, pp. 6348–6358.\n[34] D. Bertsimas and B. Stellato, “The voice of optimization,” Machine\nLearning , vol. 110, no. 2, pp. 249–277, 2021.[35] ——, “Online mixed-integer optimization in milliseconds,” arXiv\npreprint arXiv:1907.02206 , 2019.\n[36] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and I. Stoica,\n“Learning to optimize join queries with deep reinforcement learn-\ning,” arXiv preprint arXiv:1808.03196 , 2018.\n[37] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi, “Learning state\nrepresentations for query optimization with deep reinforcement\nlearning,” in Proceedings of the Second Workshop on Data Management\nfor End-To-End Machine Learning , 2018, pp. 1–4.\n[38] A. Aamand, P . Indyk, and A. Vakilian, “Learned frequency es-\ntimation algorithms under zipﬁan distribution,” arXiv preprint\narXiv:1908.05198 , 2019.\n[39] H. Wang and M. Song, “Ckmeans. 1d. dp: optimal k-means\nclustering in one dimension by dynamic programming,” The R\njournal , vol. 3, no. 2, p. 29, 2011.\n[40] X. Wu, “Optimal quantization by matrix searching,” Journal of\nalgorithms , vol. 12, no. 4, pp. 663–673, 1991.\n[41] A. Grønlund, K. G. Larsen, A. Mathiasen, J. S. Nielsen, S. Schnei-\nder, and M. Song, “Fast exact k-means, k-medians and bregman\ndivergence clustering in 1d,” arXiv preprint arXiv:1701.07204 , 2017.\n[42] Gurobi Optimization Inc., “Gurobi optimizer reference manual;\n2016,” http://www. gurobi. com , 2016.\n[43] L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classiﬁcation\nand regression trees,” Wadsworth and Brooks , vol. 37, no. 15, pp.\n237–251, 1984.\n[44] L. Breiman, “Random forests,” Machine learning , vol. 45, no. 1, pp.\n5–32, 2001.\n[45] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel et al. , “Scikit-\nlearn: Machine learning in Python,” Journal of Machine Learning\nResearch , vol. 12, pp. 2825–2830, 2011.\nDimitris Bertsimas is the Associate Dean of\nBusiness Analytics, Boeing Professor of Opera-\ntions Research and faculty director of the Master\nof Business Analytics at MIT. He received his\nSM and PhD in Applied Mathematics and Oper-\nations Research from MIT in 1987 and 1988 re-\nspectively. He has been MIT faculty since 1988.\nHis research interests include optimization, ma-\nchine learning, and applied probability, and their\napplications in health care, ﬁnance, operations\nmanagement, and transportation. He has co-\nauthored more than 200 scientiﬁc papers and ﬁve graduate level text-\nbooks and has received numerous awards, with the most recent being\nthe John von Neumann Theory Prize, INFORMS, and the President’s\naward, INFORMS, both in 2019.\nVassilis Digalakis Jr. is a PhD candidate at\nMIT’s Operations Research Center, advised by\nProf. Dimitris Bertsimas. Prior to joining MIT,\nhe earned his Diploma in Electrical and Com-\nputer Engineering from the Technical University\nof Crete, Greece, in 2018. His research inter-\nests lie at the intersection of machine learning\nand optimization, with application to big-data set-\ntings.",
  "textLength": 75134
}