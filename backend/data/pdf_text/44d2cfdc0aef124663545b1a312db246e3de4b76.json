{
  "paperId": "44d2cfdc0aef124663545b1a312db246e3de4b76",
  "title": "Enhancing In-Memory Spatial Indexing with Learned Search",
  "pdfPath": "44d2cfdc0aef124663545b1a312db246e3de4b76.pdf",
  "text": "Enhancing In-Memory Spatial Indexing with Learned Search\nVARUN PANDEYâˆ—,TU Berlin, Germany\nALEXANDER VAN RENENâˆ—â€ ,Amazon Web Services, Germany\nELENI TZIRITA ZACHARATOU, IT University of Copenhagen, Denmark\nANDREAS KIPFâ€ ,Amazon Web Services, Germany\nIBRAHIM SABEK, University of Southern California, USA\nJIALIN DINGâ€ ,Amazon Web Services, Germany\nVOLKER MARKL, TU Berlin and DFKI GmbH, Germany\nALFONS KEMPER, TU Munich, Germany\nSpatial data is ubiquitous. Massive amounts of data are generated every day from a plethora of sources such as billions of GPS-enabled\ndevices (e.g., cell phones, cars, and sensors), consumer-based applications (e.g., Uber and Strava), and social media platforms (e.g.,\nlocation-tagged posts on Facebook, Twitter, and Instagram). This exponential growth in spatial data has led the research community\nto build systems and applications for efficient spatial data processing.\nIn this study, we apply a recently developed machine-learned search technique for single-dimensional sorted data to spatial indexing.\nSpecifically, we partition spatial data using six traditional spatial partitioning techniques and employ machine-learned search within\neach partition to support point, range, distance, and spatial join queries. Adhering to the latest research trends, we tune the partitioning\ntechniques to be instance-optimized. By tuning each partitioning technique for optimal performance, we demonstrate that: (i) grid-based\nindex structures outperform tree-based index structures (from 1.23 Ã—to 2.47Ã—), (ii) learning-enhanced variants of commonly used spatial\nindex structures outperform their original counterparts (from 1.44 Ã—to 53.34Ã—faster), (iii) machine-learned search within a partition\nis faster than binary search by 11.79% - 39.51% when filtering on one dimension, (iv) the benefit of machine-learned search diminishes\nin the presence of other compute-intensive operations (e.g. scan costs in higher selectivity queries, Haversine distance computation, and\npoint-in-polygon tests), and (v) index lookup is the bottleneck for tree-based structures, which could potentially be reduced by linearizing\nthe indexed partitions.\nAdditional Key Words and Phrases: spatial data, indexing, machine-learning, spatial queries, geospatial\nâˆ—These authors contributed equally to this work.\nâ€ Work done prior to joining Amazon.\nAuthorsâ€™ addresses: Varun Pandey, TU Berlin, Berlin, Germany, varun .pandey@tu-berlin .de; Alexander van Renen, Amazon Web Services, Munich,\nGermany, vanralex@amazon .com; Eleni Tzirita Zacharatou, IT University of Copenhagen, Copenhagen, Denmark, elza@itu .dk; Andreas Kipf, Amazon\nWeb Services, Munich, Germany, kipf@amazon .com; Ibrahim Sabek, University of Southern California, Los Angeles, USA, sabek@usc .edu; Jialin Ding,\nAmazon Web Services, Munich, Germany, jialind@amazon .com; Volker Markl, TU Berlin and, DFKI GmbH, Berlin, Germany, volker .markl@tu-berlin .de;\nAlfons Kemper, TU Munich, Munich, Germany, kemper@in .tum.de.\n1arXiv:2309.06354v1  [cs.DB]  12 Sep 2023\n\n2 Pandey, van Renen, Tzirita Zacharatou, et al.\n1 INTRODUCTION\nWith the increase in the amount of spatial data available today, the database community has devoted substantial atten-\ntion to spatial data management. For example, the NYC Taxi Rides open dataset [ 54] consists of pick-up and drop-off\nlocations of more than 2.7 billion rides taken in the city since 2009. This represents more than 650,000 taxi rides per\nday in one of the most densely populated cities in the world but is only a fraction of the location data that is captured\nby many applications today. Uber, a popular ride-hailing service, operates on a global scale and completed 10 billion\nrides in 2018 [ 90]. The unprecedented generation rate of location data has prompted a considerable amount of research\nefforts focused on scale-out systems [ 1,3,17â€“19,29,80â€“82,98,101,102], databases [ 24,49,50,55,60], improving spatial\nquery processing [ 22,23,36â€“38,66,73,74,76,83â€“85,88,89,93,97,104], or leveraging modern hardware and compiling\ntechniques [13â€“16, 77â€“79, 87], to handle the increasing demands of applications today.\nRecently, Kraska et al. [ 42] proposed using learned models instead of traditional database indexes to predict the location\nof a key in a sorted dataset and demonstrated that they are typically faster than binary searches. Kester et al. [ 33] showed\nthat index scans are more efficient than optimized sequential scans in main-memory analytical engines for queries that\nselect a small portion of the data. In this paper, we build on top of these recent research results and thoroughly investigate\nthe impact of applying ideas from learned index structures (e.g., Flood [52]) on classical multidimensional indexes.\nFig. 1. Machine Learning vs. Binary Search (Spatial Range Query).\nFor low selectivity (0.00001%), the index and boundary refinement\nphases dominate. For high selectivity (0.1%), the scan phase domi-\nnates. Parameters are tuned to favor Binary Search.Specifically, we focus on six core spatial index-\ning techniques, namely linearization using Hilbert\nspace-filling curve, fixed-grid [ 5], adaptive-grid [ 53],\nKd-tree [ 4], Quadtree [ 20] and STRtree [ 45]. Query\nprocessing using these indexing techniques typi-\ncally consists of three phases: index lookup, bound-\nary refinement, and scanning. The index lookup\nphase identifies the intersecting partitions, the\nboundary refinement phase locates the lower bound\nof the query on the sorted dimension within the par-\ntition, and the scan phase scans the partition to find\nthe qualifying points. Section 2.3 provides more de-\ntails on these phases. In this paper, we propose using\nlearned models, such as RadixSpline [ 39], to replace\nthe traditional search techniques (e.g., binary search)\nused in the boundary refinement phase .\nInterestingly, we discovered that using a learned\nmodel as the search technique for boundary refine-\nment can significantly improve query runtime, par-\nticularly for low-selectivity1range queries (similar to the observation from Kester et al. [ 33]). This applies to various\nqueries, but the benefit decreases when other dominant costs such as scans, Haversine distance computations, and\npoint-in-polygon tests are present. Figure 1 shows the average running time of a range query using adaptive-grid on\na Tweets dataset, which consists of 83 million records (Section 3.2 provides more details about the dataset), with and\nwithout learning. As shown in the figure, for a low-selectivity query (which selects 0.00001% of the data, i.e., 8 records), the\n1We adopt the definition of â€œselectivityâ€ used by Pat Selinger et al. [ 75]. Therefore, low selectivity indicates that the result set of a query has few qualifying\ntuples, while high selectivity indicates the opposite.\n\nEnhancing In-Memory Spatial Indexing with Learned Search 3\nindex and boundary refinement times dominate the lookup. In contrast, for a high-selectivity query (which selects 0.1% of\nthe data, i.e., 83 thousand records) the scan time dominates. Additionally, our study found that one-dimensional grid\npartitioning techniques (e.g., fixed-grid) benefit more from the use of learned models than two-dimensional techniques\n(e.g., Quadtree).\nWe have also discovered that, contrary to conventional wisdom, grid-based indexes, which filter on one dimension and\nindex on the other, are consistently faster than tree-based indexes. This is because grid-based indexes typically have very\nlarge partitions for optimal performance, allowing for fast searches based on learned models within each partition. This\nadvantage might not extend to disk-based index structures due to the confinement of partition size by page dimensions.\nWe also note that another advantage of grid-based indexes is that they are the simplest to implement.\nIn this paper, we extend the work presented in our previous publication [ 61]. In that previous work, we showed\npreliminary results only for range queries using three datasets and fivelearned indexes. In this study, we expand on our\nprevious research by adding three more query types ,one more learned index , and two competitive methods commonly used\nin various applications and systems. Specifically, we extend the previous work as follows:\nâ€¢Queries : We implement three additional query types (i.e., point, distance, and spatial join) in this work, bringing\nthe total number of evaluated queries to four.\nâ€¢Index Structures : We also add a learning-enhanced index based on linearization using the Hilbert curve to the five\npreviously evaluated learning-enhanced indexes (i.e., fixed-grid, adaptive-grid, K-d tree, Quadtree, and STRtree),\nbringing the total number of evaluated learning-enhanced indexes to six.\nâ€¢Competitors : We further extend our preliminary study by including two index structures that are widely used in\nhundreds of applications and systems [ 62,63]. Concretely, we implement all four queries using the JTS STRtree\nfrom the JTS library and the S2PointIndex from the Google S2 library.\nWe summarize the main findings of our experimental study as follows:\nâ€¢Grid-based vs Tree-based : Grid-based index structures, when tuned for optimal performance, outperform\ntree-based index structures due to (1) fewer random accesses, and (2) allowing for fast search using learned models\nover large partitions. They are up to 2.47 Ã—faster compared to the closest tree-based competitor, exhibit robust\nperformance across various queries, and are also the simplest to implement.\nâ€¢Effect of Learned Search : We show that using learned models to search within partitions is 11.79% to 39.51%\nfaster than binary search.\nâ€¢Compute-Intensive Operations : The combined effect of grid-based indexes and learned models diminishes in\nthe presence of compute-intensive operations, such as Haversine distance computations and point-in-polygon\ntests.\nâ€¢Performance Compared to Widely Used Indexes : When compared to two commonly used index structures\nin various systems and applications, machine-learned indexes demonstrate superior performance, with speedups\nranging from 1.44Ã—to 53.34Ã—.\nOur study sheds light on the effectiveness of different spatial index structures when used in conjunction with learned\nmodels. By evaluating multiple query types and index structures, our aim is to guide researchers and practitioners in\nchoosing the best approach for their specific needs. Furthermore, our findings contribute to the design of improved spatial\nindexing methods using learned models.\n\n4 Pandey, van Renen, Tzirita Zacharatou, et al.\nOutline. The remainder of this paper is structured as follows. Section 2 presents the spatial indexing techniques that\nwe implemented in our work, their learned variants, as well as the implementation of the different query types. Then,\nSection 3 presents our experimental study. Finally, we discuss the related work in Section 4 before concluding in Section 5.\n2 APPROACH\n(a)Hilbert Space Filling Curve\n (b)Fixed-grid\n (c)Adaptive-grid\n(d)K-d tree\n (e)Quadtree\n (f)STRtree\nFig. 2. An illustration of the different partitioning techniques.\nIn this section, we first describe the spatial indexing techniques that we implemented in our work (Section 2.1). We then\nproceed to explain how we built the learned indexes (Section 2.2). Finally, Sections 2.3- 2.6 explain how we implemented\neach of the queries covered in this work (i.e., range, point, distance, and spatial join).\n2.1 Indexing Techniques\nMultidimensional access methods are broadly classified into two categories: Point Access Methods (PAMs) and Spatial\nAccess Methods (SAMs) [ 21]. PAMs are designed to handle point data without spatial extent. SAMs, however, manage\nextended objects such as linestrings and polygons. In this work, we focus mainly on PAMs.\nSpatial partitioning is the process of splitting a spatial dataset into partitions, or cells, where objects within the\nsame partition are close to each other in space. Spatial partitioning techniques can be divided into two categories:\nspace partitioning ones, which partition the embedded space, and data partitioning ones, which partition the data\nspace. In this work, we employ three space partitioning techniques: linearization using the Hilbert curve, fixed-grid [ 5],\nand Quadtree [ 20]. Additionally, we employ three data partitioning techniques: adaptive-grid [ 53], K-d tree [ 4], and\nSort-Tile-Recursive (STR) [45].\nFigure 2 illustrates these techniques on a sample of the Tweets dataset that we use in our experiments (further details can\nbe found in Section 3.2). The figure shows the sample points and partition boundaries as dots and grid axes, respectively.\n2.1.1 Linearization using Hilbert Curve. In a one-dimensional data space, all data points can be sorted along a single\ndimension, making it easy to perform range queries by retrieving all data between given lower and upper bounds. However,\napplying learned models to a spatial (or multidimensional) index is challenging because multidimensional data does not\nhave an inherent sort order. One way to tackle this challenge is to linearize the multidimensional space using space-filling\ncurves. In this work, we utilize the Hilbert space-filling curve (SFC) [ 30] to map a multidimensional vector space to a\n\nEnhancing In-Memory Spatial Indexing with Learned Search 5\none-dimensional space. As shown in Figure 2(a), the process of linearization using the Hilbert SFC involves dividing the\ntwo-dimensional space with a uniform grid and then using the Hilbert curve to enumerate the cells of this grid. Once all\ncells have been enumerated, we can sort their identifiers, and thus learn an index on this sorted order. This approach is\nsimilar to the recently proposed Z-order Model (ZM) index [ 95], which uses the Z-curve to enumerate the cells. We chose\nthe Hilbert curve instead as it has been shown to perform better for multi-dimensional indexing [ 43,44,51]. The Hilbert\ncurve is also the choice of multidimensional clustering in the recent Databricks runtime engine [ 7], replacing the Z-curve.\nHowever, we show that linearization-based techniques can suffer from skewed cases, where the queries cover a large\nportion of the curve, as we will demonstrate in Section 3.4.2. For example, if a query rectangle covers all partitions at the\nbottom of Figure 2(a), the whole curve would lie within the query rectangle. This would result in scanning a large number\nof points that do not satisfy the query, leading to poor performance.\n2.1.2 Fixed and Adaptive Grid. Grid-based indexing methods are primarily designed to optimize the retrieval of records\nfrom disk. They work by dividing the d-dimensional attribute space into cells, where each cell corresponds to one data\npage (or bucket) and stores a pointer to the data page it indexes. Data points that fall within the boundaries of a cell are\nstored on the corresponding data page.\nThis allows for quick navigation to the specific data page containing the desired records, rather than having to search\nthrough the entire dataset. The fixed-grid [ 5] enforces equidistant grid lines, while the adaptive-grid (or grid file [ 53])\nrelaxes this constraint. Instead, to define the partition boundaries of the d-dimensions, the adaptive-grid introduces an\nauxiliary data structure containing a set of d-dimensional arrays called linear scales. In our implementation, we divide the\nspace along one dimension and use the other dimension as the sort dimension. We also note that the grid-based indexes\nare the simplest to implement since they only require maintaining a vector of grid lines and computing the intersection\nbetween the vector and a given query using offset computation and binary search for fixed-grid and adaptive-grid,\nrespectively.\n2.1.3 K-d tree. The K-d tree [4] is a binary search tree that recursively subdivides the space into equal subspaces using\nrectilinear (or iso-oriented) hyperplanes. The splitting hyperplanes, known as discriminators, alternate between the\nğ‘˜dimensions at each level of the tree. For example, in a 2-dimensional space, the splitting hyperplanes are alternately\nperpendicular to the x- and y-axes. The original K-d tree partitions the space into equal partitions. For example, if the\ninput space consists of a GPS coordinate system (-90.0, -180 to 90, 180), it would be divided into equal halves (-45, -90 to 45,\n90). This results in an unbalanced K-d tree if the data is skewed, i.e., most of the data lies in one partition. However, we can\nmake the K-d tree data aware by dividing the data at each level into two halves based on the median point in the data. This\nensures that both partitions in the binary tree are balanced. In our work, we implemented this data-aware version of the\nK-d tree.\n2.1.4 Quadtree. The Quadtree [ 20], along with its many variants, is a tree data structure that partitions the space similarly\nto the K-d tree. The term quadtree typically refers to the two-dimensional variant, but the concept can easily be generalized\nto multiple dimensions. Like the K-d tree, the Quadtree decomposes the space using rectilinear hyperplanes. However, it\ndiffers from the K-d tree in that it is not a binary tree. For d dimensions, interior nodes have 2ğ‘‘children. In the case of 2\ndimensions, each interior node has four children, each representing a rectangle. The search space is recursively divided\ninto four quadrants until the number of objects in each quadrant is below a predefined threshold, typically the page size.\nQuadtrees are generally not balanced, as the tree goes deeper in areas of higher density.\n\n6 Pandey, van Renen, Tzirita Zacharatou, et al.\n2.1.5 Sort-Tile-Recursive packed R-tree. An R-tree [ 27] is a hierarchical data structure that is primarily designed for the\nefficient execution of range queries. The R-tree approximates arbitrary geometric objects with their minimum bounding\nrectangle (MBR) and stores the resulting collection of rectangles. Each node in the R-tree stores a maximum of N entries,\neach containing a rectangle ğ‘…and a pointer ğ‘ƒ. At the leaf level, ğ‘ƒpoints to the actual object and ğ‘…is the MBR of the object.\nIn internal nodes, ğ‘…represents the MBR of the subtree pointed to by ğ‘ƒ.\nThe Sort-Tile-Recursive (STR) packing algorithm [ 45] is a method for filling R-trees that aims to maximize space\nutilization. The main idea behind STR packing is to tile the data space into an ğ‘†Ã—ğ‘†grid. Assuming that the number of\npoints in a data set is ğ‘ƒand the capacity of a node is ğ‘,ğ‘†=âˆšï¸\nğ‘ƒ/ğ‘. STR first sorts the data on the x-dimension (in the case\nof rectangles, the x-dimension of the centroid), and then divides it into ğ‘†vertical slices . Within each vertical slice, it sorts\nthe data on the y-dimension and packs it into nodes by grouping them into runs of length ğ‘, formingğ‘†horizontal slices.\nThis process continues recursively, resulting in completely filled nodes, except for the last node which may have fewer\nthanğ‘elements.\n2.2 Index Building\nAlgorithm 1: A generic way of building learning-enhanced in-\ndexes\nInput :ğ·: the input location dataset; ğ‘™: the partition size\nOutput :ğ·â€²: the partitioned and indexed input dataset\n1ğ·â€²â†{}\n2ğ‘ƒâ†Partition( some approach from the techniques described in Section 2.1, ğ‘™)\n3forğ‘âˆˆğ‘ƒdo\n4 Sort(ğ‘,ğ‘¦)\n5 BuildLearnedIndex( ğ‘,ğ‘¦)\n6ğ·â€²â†ğ·â€²âˆª{ğ‘}\n7end\n8returnğ·â€²In this section, we outline how we can turn\nthe above indexing techniques into learned\nindexes that index a given location dataset ğ·,\nwhich contains points in latitude/longitude\nformat (referred to as the x-dimension and\ny-dimension respectively for ease of under-\nstanding). First, we partition ğ·using one of\nthe techniques described in Section 2.1.\nEach partition has a size of ğ‘™points, also\nknown as the leaf size or partition size. Once\nğ·has been partitioned, we iterate through all\npartitions and sort all the points within each\npartition on the y-dimension. Then, we build\na learned index on the y-dimension for each partition. Algorithm 1 outlines the index building process.\n2.3 Range Query Processing\nA two-dimensional range query takes as input a query range ğ‘that has a lower and an upper bound in both dimensions,\nrepresented by(ğ‘ğ‘¥ğ‘™,ğ‘ğ‘¦ğ‘™)and(ğ‘ğ‘¥â„,ğ‘ğ‘¦â„)respectively. It also takes as input a location dataset ğ·, containing two-dimensional\npoints represented by (ğ‘ğ‘¥,ğ‘ğ‘¦). The range query returns all points in ğ·that are contained in the query range ğ‘. Formally:\nğ‘…ğ‘ğ‘›ğ‘”ğ‘’(ğ‘,ğ·)={ğ‘|ğ‘âˆˆğ·:(ğ‘ğ‘¥ğ‘™â‰¤ğ‘ğ‘¥)âˆ§(ğ‘ğ‘¦ğ‘™â‰¤ğ‘ğ‘¦)âˆ§\n(ğ‘ğ‘¥â„â‰¥ğ‘ğ‘¥)âˆ§(ğ‘ğ‘¦â„â‰¥ğ‘ğ‘¦)}.\nTo accelerate query processing, we use the partitioned and indexed input dataset ğ·â€²generated by Algorithm 1. Given\nğ·â€², range query processing works in three phases, as shown in Algorithm 2:\nPhase I: Index Lookup. The index lookup phase involves identifying the partitions that intersect with the given range\nquery using the index directory, i.e., the grid directories or trees. These partitions are represented by ğ¼ğ‘ƒ, which stands for\n\nEnhancing In-Memory Spatial Indexing with Learned Search 7\nintersected partitions and is reflected in line 2 of Algorithm 2. Note that the specific method used for this step depends on\nthe partitioning technique.\nAlgorithm 2: Range Query Algorithm\nInput :ğ·â€²: a partitioned and indexed input dataset; ğ‘: a query range\nOutput :ğ‘…ğ‘„: a set of all points in ğ·â€²withinğ‘\n1ğ‘…ğ‘„â†{}\n/* find intersected partitions (IP) */\n2ğ¼ğ‘ƒâ†IndexLookup( ğ·â€²,ğ‘)\n3forğ‘–ğ‘âˆˆğ¼ğ‘ƒdo\n/* if completely inside x-dimension range */\n4 ifğ‘ğ‘¥ğ‘™<=ğ‘–ğ‘ğ‘¥ğ‘™andğ‘–ğ‘ğ‘¥â„<=ğ‘ğ‘¥â„then\n/* if also completely inside y-dimension range, copy\nentire partition */\n5 ifğ‘ğ‘¦ğ‘™<=ğ‘–ğ‘ğ‘¦ğ‘™andğ‘–ğ‘ğ‘¦â„<=ğ‘ğ‘¦â„then\n/* copy all points in partition */\n6 ğ‘…ğ‘„â†ğ‘…ğ‘„âˆªğ‘–ğ‘\n7 else\n/* lower bound */\n8 ğ‘™ğ‘â†EstimateFrom( ğ‘–ğ‘,ğ‘ğ‘¦ğ‘™)\n/* get exact lower bound */\n9 ğ‘™ğ‘â†LocalSearch( ğ‘–ğ‘,ğ‘™ğ‘,ğ‘ğ‘¦ğ‘™)\n/* upper bound */\n10 ğ‘¢ğ‘â†EstimateTo( ğ‘–ğ‘,ğ‘ğ‘¦â„)\n/* get exact upper bound */\n11 ğ‘¢ğ‘â†LocalSearch( ğ‘–ğ‘,ğ‘¢ğ‘,ğ‘ğ‘¦â„)\n/* copy all points between lower and upper bound */\n12 ğ‘…ğ‘„â†ğ‘…ğ‘„âˆªğ‘–ğ‘.range(ğ‘™ğ‘,ğ‘¢ğ‘)\n13 end\n14 else\n/* lower bound */\n15ğ‘™ğ‘â†EstimateFrom( ğ‘–ğ‘,ğ‘ğ‘¦ğ‘™)\n16ğ‘™ğ‘â†SearchPoint( ğ‘–ğ‘,ğ‘™ğ‘,ğ‘ğ‘¦ğ‘™)\n/* upper bound */\n17ğ‘¢ğ‘â†EstimateTo( ğ‘–ğ‘,ğ‘ğ‘¦â„)\n18ğ‘¢ğ‘â†LocalSearch( ğ‘–ğ‘,ğ‘¢ğ‘,ğ‘ğ‘¦â„)\n/* scan */\n19 forğ‘–âˆˆ[ğ‘™ğ‘,ğ‘¢ğ‘]do\n/*ğ‘–ğ‘¡â„point in partition ğ‘–ğ‘*/\n20 ğ‘â†ğ‘–ğ‘ğ‘–\n21 ifğ‘withinğ‘then\n22 ğ‘…ğ‘„â†ğ‘…ğ‘„âˆª{ğ‘}\n23 end\n24 end\n25 end\n26end\n27returnğ‘…ğ‘„Phase II: Boundary Refinement. After\nidentifying the intersected partitions in the in-\ndex lookup phase, the next step is to locate the\nbounds of the query on the sorted dimension\nwithin each partition. However, when a parti-\ntion is fully contained within the query range,\nthere is no need for boundary refinement, and\nall points within that partition can be imme-\ndiately returned. This is reflected in line 6 of\nAlgorithm 2. Otherwise, when the query only\npartially intersects with a partition, there are\ntwo cases: (1) the partition is fully inside the\nx-dimension range. In this case, we employ a\nsearch technique to compute both the lower\nand upper bounds, and then copy all points\nwithin these bounds (reflected in lines 8-12\nof the algorithm), (2) the partition is not fully\ncontained within the x-dimension range. In\nthis case, we compute the lower and upper\nbounds on the sorted y-dimension and then\nswitch to the scan phase.\nTypically, binary search is used as the\nsearch technique. In this paper, we propose\nreplacing binary search with a learned model.\nSpecifically, we use the RadixSpline index [ 39,\n40] to efficiently search the sorted dimension\nof our data. RadixSpline consists of two com-\nponents: a set of spline points and a radix table.\nThe radix table is used to quickly identify the\nspline points for a given lookup key (in our\ncase, the dimension over which the data is\nsorted). At lookup time, the radix table is con-\nsulted first to determine the range of spline\npoints to examine. Next, these spline points\nare searched to locate the spline points sur-\nrounding the lookup key. Finally, linear inter-\npolation is applied to predict the position of\nthe lookup key within the sorted array.\n\n8 Pandey, van Renen, Tzirita Zacharatou, et al.\nGiven the inherent error introduced by the RadixSpline (and generally, learned indexes), a local search (referred to\nasğ¿ğ‘œğ‘ğ‘ğ‘™ğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„()in Algorithm 2) is necessary to find the exact lookup point, which, in our context, corresponds to the\nquery bound. Without loss of generality, we describe the local search procedure for the computation of the lower query\nbound. For range scans, there are two possible scenarios. In the first scenario, the estimated value from the spline is lower\nthan the true lower bound within the sorted dimension. Thus, we scan the partition upward until we reach the lower\nbound. Conversely, when the estimated value is higher than the actual lower bound, we scan the partition downward\nuntil we reach the lower bound while also materializing all encountered points. Consequently, in this scenario, the local\nsearch does not incur additional materialization costs (unless the estimated value exceeds the queryâ€™s upper bound), as\nthe points within the query bounds are materialized in any case.\nPhase III: Scan. When the partition partially intersects the x-dimension range, then after determining the bounds\nof the query on the sorted dimension in the boundary refinement phase, the final step is to scan the partition to find\nthe qualifying points on the x-dimension. During this scan phase, we iterate through the partition starting from the\ndetermined lower bound and continue until we reach either the upper bound of the query on the sorted y-dimension or\nthe end of the partition. This process is reflected in Algorithm 2 from line 14 onwards.\n2.4 Point Query Processing\nIn this study, we also implement point queries, in keeping with the trend in recent research [ 48,65,95]. A point query\ntakes as input a query point ğ‘ğ‘, and a set of geometric objects ğ·. The query returns true if ğ‘ğ‘is found within ğ·, and false\nif it is not. Formally:\nğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡(ğ‘ğ‘,ğ·)=âˆƒğ‘âˆˆğ·.ğ‘ğ‘.ğ‘¥=ğ‘.ğ‘¥âˆ§ğ‘ğ‘.ğ‘¦=ğ‘.ğ‘¦.\nAlgorithm 3: Point Query Algorithm\nInput :ğ·â€²: a partitioned and indexed input dataset; ğ‘ğ‘: a query\npoint\nOutput :ğ‘¡ğ‘Ÿğ‘¢ğ‘’ if the pointğ‘ğ‘is inğ·â€²,ğ‘“ğ‘ğ‘™ğ‘ ğ‘’ otherwise\n/* find intersected partition (IP) */\n1ğ¼ğ‘ƒâ†IndexLookup( ğ·â€²,ğ‘ğ‘)\n/* search within the partition */\n2ifğ¼ğ‘ƒâ‰ âˆ…then\n/* get estimate */\n3ğ‘’ğ‘ ğ‘¡â†EstimateFrom( ğ¼ğ‘ƒ,ğ‘ğ‘.ğ‘¦)\n4ğ‘“ğ‘œğ‘¢ğ‘›ğ‘‘â†SearchPoint( ğ‘–ğ‘,ğ‘’ğ‘ ğ‘¡,ğ‘ğ‘)\n5 returnğ‘“ğ‘œğ‘¢ğ‘›ğ‘‘\n6else\n7 returnğ‘“ğ‘ğ‘™ğ‘ ğ‘’\n8endWe use the partitioned and indexed dataset, ğ·â€², from\nAlgorithm 1, to speed up point query processing. Point\nquery processing is outlined in Algorithm 3. First, we is-\nsue anğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ğ¿ğ‘œğ‘œğ‘˜ğ‘¢ğ‘()using a degenerate rectangle from\nthe query point ğ‘ğ‘. Since the point can only exist in one\npartition, we first check if the ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ğ¿ğ‘œğ‘œğ‘˜ğ‘¢ğ‘()phase pro-\nduces any intersected partition. If no intersected partition\nis found, we immediately return false. Next, we search for\nthe point within the intersected partition using a two-\nstep process: (i) we estimate the location of the point\nin the y-dimension of the partition using the learned\nsearch technique, and (ii) we refine the result using the\nğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡()procedure, which aims to mitigate the error\nintroduced by the learned search technique, similarly to\ntheğ¿ğ‘œğ‘ğ‘ğ‘™ğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„()procedure used in range query process-\ning.\nFor the RadixSpline, there can now be three cases for the ğ‘†ğ‘’ğ‘ğ‘Ÿğ‘â„ğ‘ƒğ‘œğ‘–ğ‘›ğ‘¡()procedure. First, if the estimated value from\nthe spline is lower than the actual lower bound on the sorted dimension, we simply scan upward comparing the elements\non the sorted dimension until we reach the actual lower bound. We then continue scanning on both dimensions until\n\nEnhancing In-Memory Spatial Indexing with Learned Search 9\nwe find the query point or reach the upper bound of the partition. Note that for two points to be considered equal, their\nvalues should match on both dimensions. Second, if the estimated value is higher than the upper bound on the sorted\ndimension, we scan downward comparing the values on the sorted dimension until we reach the upper bound. We then\ncontinue scanning downward and comparing on both dimensions until we find the query point or reach the lower bound\nof the partition. Third, if the estimated value matches the query pointâ€™s value on the search dimension, we perform an\nupward scan, comparing on both dimensions, to locate the query point. If the partitionâ€™s upper bound is reached without\nfinding the point, we then continue with a downward scan, again comparing on both dimensions, until we locate the\nquery point or reach the partitionâ€™s lower bound.\nIn the case of a binary search, the process is simpler. The value of the query point on the sorted dimension is used to\nfind the lower bound. Then, we scan upward, comparing on both dimensions until the query point is found.\n2.5 Distance Query Processing\nA distance query takes a query point ğ‘ğ‘, a distanceğ‘‘, and a set of geometric objects ğ·. It returns all objects in ğ·that lie\nwithin the distance ğ‘‘of query point ğ‘ğ‘. Formally:\nğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘ğ‘,ğ‘‘,ğ·)={ğ‘|ğ‘âˆˆğ·âˆ§dist(ğ‘ğ‘,ğ‘)â‰¤ğ‘‘}.\nAlgorithm 4: Distance Query Algorithm\nInput :ğ·â€²: partitioned and indexed input dataset; ğ‘ğ‘: a query\npoint;ğ‘‘: distance\nOutput :ğ·ğ‘„: a set of all points in ğ·â€²within distance ğ‘‘ofğ‘ğ‘\n1ğ·ğ‘„â†{}\n/* Get minimum bounding rectangle (mbr) of the circle */\n2ğ‘šğ‘ğ‘Ÿâ†GetMBR(ğ‘ğ‘,ğ‘‘)\n/* Filter using ğ‘šğ‘ğ‘Ÿ */\n3ğ‘…ğ‘„â†RangeQuery( ğ·â€²,ğ‘šğ‘ğ‘Ÿ )\n/* Refine */\n4forğ‘âˆˆğ‘…ğ‘„do\n5 ifWithinDistance( ğ‘,ğ‘ğ‘,ğ‘‘)then\n6ğ·ğ‘„â†ğ·ğ‘„âˆª{ğ‘}\n7 end\n8end\n9returnğ·ğ‘„As in the case of Range query processing (Section 2.3),\nwe use the partitioned and indexed input dataset ğ·â€²from\nAlgorithm 1 for faster query processing. The implementa-\ntion of the distance query employs the filter and refine [56]\napproach, which is commonly used in popular database\nsystems such as Oracle Spatial [ 32]. Note that using the\nfilter and refine approach is also prevalent in many re-\ncent research works where various spatial queries (e.g.,\nkNN queries, distance queries, and spatial join queries) are\nfirst decomposed to range queries as a preliminary filter,\nfollowed by query-specific refinement [25, 46, 47, 65].\nAlgorithm 4 shows the algorithm for distance query\nprocessing. We first filter using a rectangle (reflected in line\n1 of Algorithm 4), whose corner vertices are at a distance\nofdfrom the query point q. We issue a range query using\nthis rectangle, and then refine the resulting candidate set\nof points using a withinDistance predicate. Note that we are using GPS coordinates (i.e., a Geographic coordinate system).\nTherefore, special attention must be given if either of the poles or the 180th meridian is within the query distance ğ‘‘. We\ncompute the coordinates of the minimum bounding rectangle by moving along the geodesic arc as described in [ 6] and\nthen handle the edge cases of the poles and the 180th meridian.\nHowever, currently, we only use one bounding box. This approach is not optimal as it can result in materializing a\nlarge number of unnecessary points when the 180th meridian falls within the query distance. One way to improve the\nefficiency is to break the bounding box into two parts, one on either side of the 180th meridian. We leave this optimization\nfor future work. After materializing all the points within the MBR of ğ‘andğ‘‘, we refine this candidate set of points. To that\n\n10 Pandey, van Renen, Tzirita Zacharatou, et al.\nend, we compute the Haversine distance between the query point ğ‘and each of the candidate points and add to the final\nresult all the points that are found to be within the specified distance ğ‘‘.\n2.6 Join Query Processing\nA spatial join combines two input spatial datasets, ğ‘…andğ‘†, using a specified join predicate ğœƒ(such as overlap, intersect,\ncontains, within, or withindistance). It returns a set of pairs (ğ‘Ÿ,ğ‘ )whereğ‘Ÿâˆˆğ‘…,ğ‘ âˆˆğ‘†that meet the join predicate ğœƒ.\nFormally:\nğ‘…âŠ² âŠ³ğœƒğ‘†={(ğ‘Ÿ,ğ‘ )|ğ‘Ÿâˆˆğ‘…,ğ‘ âˆˆğ‘†,ğœƒ(ğ‘Ÿ,ğ‘ )holds}.\nAlgorithm 5: Join Query Algorithm\nInput :ğ·â€²: partitioned and indexed input dataset; ğ‘ğ‘œğ‘™ğ‘¦ğ‘”ğ‘œğ‘›ğ‘  : a set of polygons\nOutput :ğ½ğ‘„: a set of sets, a set of points within each polygon in ğ‘ğ‘œğ‘™ğ‘¦ğ‘”ğ‘œğ‘›ğ‘ \n1ğ½ğ‘„â†{}\n2forğ‘ğ‘œğ‘™ğ‘¦ğ‘”ğ‘œğ‘›âˆˆğ‘ğ‘œğ‘™ğ‘¦ğ‘”ğ‘œğ‘›ğ‘  do\n/* Get minimum bounding rectangle (mbr) of the polygon */\n3ğ‘šğ‘ğ‘Ÿâ†GetMBR(ğ‘ğ‘œğ‘™ğ‘¦ğ‘”ğ‘œğ‘› )\n/* Filter using ğ‘€ğµğ‘… */\n4ğ‘…ğ‘„â†RangeQuery( ğ·,ğ‘šğ‘ğ‘Ÿ )\n5ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘‘â†{}\n/* Refine */\n6 forğ‘âˆˆğ‘…ğ‘„do\n7 ifContains(ğ‘ğ‘œğ‘™ğ‘¦ğ‘”ğ‘œğ‘› ,ğ‘)then\n8 ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘‘â†ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘‘âˆª{ğ‘}\n9 end\n10 end\n11ğ½ğ‘„â†ğ½ğ‘„âˆª{ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘‘}\n12end\n13returnğ½ğ‘„We implemented a join query between a set\nof polygons and the partitioned and indexed\ninput location dataset ğ·â€². The join algorithm\nis outlined in Algorithm 5 and is based on the\nfilter and refine [56] approach.\nThis involves using the minimum bound-\ning rectangle of each polygon to perform a\nrange query. We then refine the candidate set\nof points using contains as the predicate ğœƒ, thus\ncomputing all points contained in each poly-\ngon. We implemented the contains predicate\nusing the ray-casting algorithm, where a ray\nis casted from the candidate point to a point\noutside the polygon, and then the number of\nintersections with polygon edges is counted.\nSome polygons could potentially contain hun-\ndreds or thousands of edges. Therefore, to fa-\ncilitate a quick lookup of the edges intersected\nwith the ray, we index the polygon edges in an interval tree. We implemented the interval tree using a binary search tree.\n3 EVALUATION\n3.1 Experimental Setup\nHardware Configuration. All experiments were performed single-threaded on an Ubuntu 18.04 machine equipped\nwith an Intel Xeon E5-2660 v2 CPU (2.20 GHz, 10 cores, 3.00 GHz turbo)2and 256 GB DDR3 RAM. We use the numactl\ncommand to bind the thread and memory to one node to avoid NUMA effects. CPU scaling was also disabled during\nbenchmarking using the cpupower command.\nSoftware Configuration. In all our experiments, we sort on the longitude value of the location within each partition.\nThe currently available open-source implementation of RadixSpline only supports integer values. However, most spatial\ndatasets contain floating-point values. To address this issue, we adapted the RadixSpline implementation to work with\nfloating-point values. We set the spline error to 32 for all experiments in our RadixSpline implementation.\n2CPU: https://ark .intel.com/content/www/us/en/ark/products/75272/intel-xeon-processor-e5-2660-v2-25m-cache-2-20-ghz .html\n\nEnhancing In-Memory Spatial Indexing with Learned Search 11\n(a)Twitter\n (b)Taxi Trips\n (c)OSM\nFig. 3. Datasets: (a) Tweets are spread across New York, (b) NYC Taxi trips are clustered in central New York, and (c) All\nNodes dataset from OSM.\n3.2 Datasets and Queries\nFor evaluation, we used three datasets, the New York City Taxi Rides dataset [ 54] (NYC Taxi Rides), geo-tagged tweets in\nthe New York City area (NYC Tweets), and Open Streets Maps (OSM). NYC Taxi Rides contains 305 million taxi rides\nfrom 2014 and 2015. NYC Tweets data was collected using Twitterâ€™s Developer API [ 86] and contains 83 million tweets.\nThe OSM dataset is taken from [ 59] and contains 200M records from the All Nodes (Points) dataset. Figure 3 shows\nthe spatial distribution of the three datasets. We further generated two types of query workloads for each of the three\ndatasets: skewed queries, which follow the distribution of the underlying data, and uniform queries. For each type of\nquery workload, we generated six different workloads ranging from 0.00001% to 1.0% selectivity. For example, in the\ncase of the Taxi Rides dataset (305M records), these queries would materialize from 30 to 3 million records. The query\nworkloads consist of one million queries each. To generate skewed queries, we select a record from the data and expand\nits boundaries (using a random ratio in both dimensions) until the selectivity requirement of the query is met. For uniform\nqueries, we generate points uniformly in the embedding space of the dataset and expand the boundaries similarly until the\nselectivity requirement of the query is met. The query selectivity and the type of query are mostly application-dependent.\nFor example, consider a user issuing a query to find a popular pizzeria nearby on Google Maps. The expected output for\nthis query should be a handful of records, i.e., the query selectivity is low (a list of 20-30 restaurants near the user). On the\nother hand, a query on an analytical system would materialize many more records (e.g., find the average cost of all taxi\nrides originating in Manhattan).\n3.3 Baselines\nFirstly, we compare the performance of learned indexes and binary search as search techniques within a partition.\nFurthermore, we compare our implementation of the learned indexes with the two best-performing indexes from earlier\nstudies [ 62,63], which compared state-of-the-art spatial libraries. More specifically, for range and distance queries, we\ncompare our implementation with the STRtree implementation from the Java Topology Suite (JTS) and the S2PointIndex\nfrom Google S2. For join queries, we use the S2ShapeIndex provided by Google S2. The source code used in this work\nis available on GitHub3. Additionally, we will open source the implementation of the learned indexes and the query\n3https://github .com/varpande/learnedspatial\n\n12 Pandey, van Renen, Tzirita Zacharatou, et al.\n246810skewed queries\nTweets (83M)\n2.55.07.510.012.515.0\nTaxi Rides (305M)\n2.55.07.510.012.5\nOSM (200M)\n1021031041051060204060uniform queries\n102103104105106050100150200\n1021031041051060200400600\nAverage number of points per partition (log)Average query time [ Âµs]ml-ï¬xed-grid bs-ï¬xed-grid ml-adaptive-grid bs-adaptive-grid ml-quadtree bs-quadtree\nFig. 4. Range Query Configuration - ML vs. BS for low selectivity (0.00001%).\nworkloads with the camera-ready version of this work. Given the popularity of machine-learned indexes in current\nresearch, we believe that our implementations will be useful in evaluating many influential works to come in the near\nfuture.\n3.4 Range Query Performance\nIn this section, we first explore the tuning of partition sizes and why the tuning is crucial to obtain optimal performance.\nNext, we present the total query runtime when the partition size for each index is tuned for optimal performance.\n3.4.1 Tuning Indexing Techniques. Recent work in learned multidimensional and spatial indexes has focused on learning\nfrom the data and the query workload. The essential idea behind learning from both the data and the query workload is\nthat a particular use case can be instance-optimized [ 9,41]. To study this effect, we conducted multiple experiments on the\nthree datasets by varying the sizes of the partitions, tuning them on two workloads with different selectivities (to cover a\nbroad spectrum, we tune the indexes using queries with low and high selectivity) for both skewed and uniform queries.\nWe omit the results for tuning the indexing techniques for the rest of the queries (point, distance, and join queries) as they\nare similar to the ones for range queries.\nFigure 4 shows the effect of tuning when the indexes are tuned for the lowest selectivity workload for the two query\ntypes. It can be seen in the figure that it is essential to tune the grid indexing techniques for a particular workload. Firstly,\nthey are highly susceptible to the size of the partition. As the size of the partition increases, we notice an improvement in\nthe performance until a particular partition size is reached that corresponds to the optimal performance. After this point,\nincreasing the size of the partitions only degrades the performance. This shows that determining the optimal partition is\ncrucial for optimal performance. For example, fixing the partition size of fixed-grid to 100 points per partition (a fairly\ncommon default value of the leaf size for an index in many open-source spatial libraries) results in up to 300Ã—worse\nperformance compared to the optimal partition size. It can be seen that, usually, for grid (single-dimension) indexing\ntechniques, the optimal partition sizes are much larger compared to indexing techniques that filter on both dimensions\n\nEnhancing In-Memory Spatial Indexing with Learned Search 13\n103104105106\nAverage number of points per partition23456789Average query time [ Âµs]\nQuery runtime\nNumber of cells\nScanned points\n0123456\nNumber of cells\u0002105\n0500100015002000250030003500\nScanned Points\nFig. 5. Effect of the number of cells and scanned points for\nfixed-grid on Taxi Trip dataset for skewed queries (0.00001%\nselectivity).\n102103104105\nAverage number of points per partition345678Average query time [ Âµs]\n0:00:51:01:52:02:5\nNumber of cells\u0002106\n050010001500200025003000\nScanned points\n0:60:81:01:21:41:6Index Time\nQuery runtime\nNumber of cells\nScanned points\nIndex TimeFig. 6. Effect of the number of cells and scanned points for\nQuadtree on Taxi Trip dataset for skewed queries (0.00001%\nselectivity).\nTaxi Trips (Skewed Queries) Taxi Trips (Uniform Queries)\nFixed Adaptive Quadtree Fixed Adaptive Quadtree\nSelectivity (%) ML BS ML BS ML BS ML BS ML BS ML BS\n0.00001 1.78 2.35 1.86 2.40 2.77 2.51 2.02 2.58 81.4 10.54 1.48 1.31\n0.0001 4.54 5.82 4.67 6.12 6.12 5.82 5.85 6.91 228.1 27.69 3.69 3.42\n0.001 14.97 18.83 15.32 19.49 20.84 19.47 22.87 24.34 708.8 87.49 13.59 12.98\n0.01 90.13 97.04 89.48 95.96 117.01 104.37 141.24 151.47 2634.4 309.62 98.85 112.77\n0.1 678.12 698.39 675.14 696.49 922.67 793.96 988.35 922.96 9609.9 1174.79 891.24 1101.95\n1.0 8333.94 8408.15 8301.56 8399.69 10678.04 9512.29 8843.71 8753.68 8574.84 8836.28 10647.97 12377.14\nTable 1. Total query runtime (in microseconds) for both RadixSpline (ML) and binary search (BS) for Taxi Rides dataset\non skewed and uniform query workloads (parameters are tuned for selectivity 0.00001%).\n(only Quadtree is shown in the figure but the same holds for the other indexing techniques we have covered in this work;\nwe do not show the other trees because the curve is similar for them). Due to the large partition sizes in grid indexing\ntechniques, we notice a large increase in performance while using a learned index compared to binary search. This is\nespecially evident for skewed queries (which follow the underlying data distribution). We encountered a speedup from\n11.79% to 39.51% compared to binary search. Even when we tuned a learned index to a partition size that corresponds to\nthe optimal performance for binary search, we found that the learned index frequently outperformed the binary search.\nLearned indexes do not help much for indexing techniques that filter on both dimensions. In contrast, as shown in Table 1,\nthe performance of Quadtree (and STRtree) dropped in many cases. The reason is that the optimal partition size for these\ntechniques is very low (less than 1,000 points per partition for most configurations). The refinement cost for the learned\nindexes is an overhead in such cases. The k-d tree, on the other hand, contains more points per partition (from 1200 to\n7400) for the optimal configuration for Taxi Trips and OSM datasets, and thus the learned indexes perform faster by 2.43%\nto 9.17% than binary search. For the Twitter dataset, the optimal configuration contains less than 1200 points per partition,\nand we observed a similar drop in performance using learned indexes.\nFigure 5 shows the effect of the number of cells and the number of points that are scanned in each partition on the query\nruntime for fixed-grid on Taxi Trips dataset for the lowest selectivity. As the number of points per partition increases\n(i.e., there are fewer partitions), the number of cells decreases. At the same time, the number of points that need to be\nscanned for the query increases. The point where these curves meet is the optimal configuration for the workload, which\n\n14 Pandey, van Renen, Tzirita Zacharatou, et al.\n100101102103104skewed queries\nTweets (83M)\n Taxi Rides (305M)\n OSM (200M)\n1e-05 0.0001 0.001 0.01100101102103104uniform queries\n1e-05 0.0001 0.001 0.01\n 1e-05 0.0001 0.001 0.01\nQuery Selectivity (in percent)Average query time [ Âµs]ml-\fxed-grid ml-adaptive-grid ml-kdtree ml-quadtree ml-strtree ml-hilbert jts-strtree s2-pointindex\nFig. 7. Range Query Runtime - Total range query runtime on skewed and uniform queries for the three datasets.\ncorresponds to the lowest query runtime. For tree structures, the effect is different. Figure 6 shows that the structures\nthat filter on both dimensions do most of the pruning in the index lookup. The dominating cost in these structures is\nthe number of points scanned within the partition, and the query runtime is directly proportional to this number. To\nminimize the number of points scanned, they do most of the pruning during index lookup, which requires more partitions\n(i.e., fewer points per partition). Tree-based structures pay more during the index lookup phase, which requires chasing\npointers (random access) during the index lookup and leads to more cache misses. However, once the desired partition is\nreached, these index structures scan very few points, as most of the partitions qualify for the query.\nKey Takeaways. Tuning partition sizes is crucial for grid-based indexing techniques. The optimal size of grid partitions\nis typically large, which enables fast searches within the partitions using learned models. In contrast, tree-based indexes\ndo not gain as much from learned models because optimal partition sizes are typically small.\n3.4.2 Query Performance. Figure 7 shows the query runtime for all learned index structures. It can be seen that fixed-grid\nalong with adaptive-grid (1D schemes) perform the best for all cases except for uniform queries on Taxi and OSM datasets.\nFor skewed queries, fixed-grid is 1.23 Ã—to 1.83Ã—faster than the closest competitor, Quadtree (2D), across all datasets and\nselectivity. The slight difference in performance between fixed-grid and adaptive-grid comes from the index lookup. For\nadaptive-grid, we use binary search on the linear scales to find the first partition the query intersects with. For fixed-grid,\nthe index lookup is almost negligible as only an offset computation is needed to find the first intersecting partition.\nThis is also in contrast to traditional knowledge that grid-based index structures can become skewed and thus perform\nworse than tree-based index structures. Since the index structures and data reside in memory and are tuned to optimal\npartition size, the grid-based structures perform better as (1) they avoid pointer chasing as in the case of tree-based index\nstructures, thus leading to fewer random accesses, and (2) they can utilize the fast lookups using learned models within the\nlarge indexed partitions. This would not be possible for disk-based index structures. Note that partition sizes for optimal\n\nEnhancing In-Memory Spatial Indexing with Learned Search 15\nperformance of grid-based index structures are very large for every datasets. For disk-based index structures to exhibit\nsimilar performance, it would require allocating very large pages on disk.\nIt can also be seen in the figure that the Quadtree is significantly better for uniform queries in the case of the Taxi Rides\ndataset (1.37Ã—) and OSM dataset (2.68 Ã—) than the closest competitor, fixed-grid. There are two reasons for this. First, as\nTable 2 shows, the Quadtree intersects with fewer partitions than the other index structures. Second, for uniform queries,\nthe Quadtree is more likely to traverse the sparse and low-depth region of the index. This is consistent with previously\nreported findings [34], where the authors compare the Quadtree with the R*-tree and the Pyramid-Technique.\nTable 2. Average number of partitions intersected for each\nindexing method for selectivity 0.00001% on Taxi Rides and\nOSM datasets.\nTaxi Rides OSM\nIndexing Skewed Uniform Skewed Uniform\nFixed 1.97 7.98 1.72 23.73\nAdaptive 1.74 31.57 1.51 24.80\nk-d tree 1.70 21.62 1.56 30.95\nQuadtree 1.79 2.12 1.37 7.96\nSTR 2.60 47.03 1.90 11.05In Figure 7, we can also see the performance of the\nlearnedindexescomparedtoJTSSTRtreeandS2PointIndex.\nFixed-grid is from 8.67 Ã—to 43.27Ã—faster than the JTS\nSTRtree. Fixed-grid is also from 24.34 Ã—to 53.34Ã—faster\nthan S2PointIndex. Quadtree, on the other hand, is from\n6.26Ã—to 33.99Ã—faster than JTS STRtree, and from 17.53 Ã—\nto 41.91Ã—faster than S2PointIndex. Note that the index\nstructures in the libraries are not tuned and are taken as is\nout of the box with default values. The poor performance of\nS2PointIndex is because it works on top of the Hilbert curve\nvalues and is not optimized for range queries. S2PointIndex\nand the learned linearized Hilbert curve index counterpart\nare rather similar in nature. Both use linearization to one\ndimension for indexing. The learned counterpart is learned on the sorted values of the linearized values where the\nunderlying implementation is a densely packed array, while S2PointIndex stores these linearized values in a main-memory\noptimized B-tree. S2PointIndex is a B-tree on the 64-bit integers called S2CellId. T he cell ids are a result of the Hilbert curve\nenumeration of a Quadtree -like space decomposition. Hilbert curve (as previously mentioned in Section 2.1.1) suffers from\nskewed cases where the range query rectangle covers the whole curve. To avoid such a case, S2PointIndex decomposes\nthe query rectangle into four parts to reduce the overlap with the curve. However, it still scans many superfluous points.\nKey Takeaways. Fixed-grid performs the best across all queries, except for uniform queries on Taxi and OSM datasets.\nThis is because it avoids random pointer chasing and utilizes fast learning-enhanced search within its large partitions.\nQuadtree outperforms fixed-grid for uniform queries on Taxi and OSM because these queries intersect with fewer\npartitions and only traverse the sparse, low-depth region of the index. Finally, the linearized Hilbert curve does not\nperform well in most cases because the queries need to scan a large portion of the curve.\n3.5 Point Query Performance\nSection 2.4 defines how the point query has been implemented in this work. JTS STRtree does not provide a way to search\nfor a point and thus we did not implement the point query using JTS STRtree. S2PointIndex in the Google S2 library, on\nthe other hand, allows querying for a point. Moreover, we run the point queries using only the skewed queries workload,\nwhich uniformly selects a random point from the dataset itself. This ensures that the point query actually produces a result\nand does not skew the results in favor of the learned indexes. It is also consistent with real-world workloads, where it is\nmore common to search for an existing point in the dataset, such as retrieving metadata for a specific restaurant location.\n\n16 Pandey, van Renen, Tzirita Zacharatou, et al.\nTweets (83M) Taxi Rides (305M) OSM (200M)0200400600800100012001400Average query time [ns]ml-\fxed-grid\nml-adaptive-gridml-kdtree\nml-quadtreeml-strtree\nml-hilberts2-pointindex\nFig. 8. Point Query Performance - Total point query runtime\nfor skewed queries on the three datasets.Figure 8 shows the point query runtime for all index-\ning techniques. Fixed-grid is again the best-performing\nindex for point queries on skewed workloads. It is 1.94 Ã—,\n2.27Ã—, and 1.51Ã—faster that the closest tree-based com-\npetitor, kdtree, across Tweets, Rides, and OSM datasets.\nHowever, the performance difference of fixed-grid with\nadaptive-grid are marginal. It is 1.37 Ã—, 1.1Ã—, 1.04Ã—faster\nthan the adaptive-grid across the Tweets, Rides, and OSM\ndatasets. Lastly, fixed-grid is also 2.44 Ã—, 2.56Ã—, 2.79Ã—faster\nthan S2PointIndex. Another very important observation\nis that the learned index on the linearized values is very\ncompetitive in the point queries. This is counter-intuitive\nfrom the observation in range queries in Section 3.4.2. We\nnoted that range searches on sorted Hilbert curve values\nperform poorly in skewed cases where the query rectangle covers a large portion of the curve. However, for point queries,\nonly one point on the curve needs to be searched, rather than scanning multiple points. This makes the learned index on\nthe linearized values very competitive for point queries. In fact, in the case of the OSM dataset, it is the best-performing\nindex with an average query time of 385 nscompared to 390 nsfor the fixed-grid (the best-performing index in the other\ntwo datasets).\nKey Takeaways. Fixed-grid performs the best across all queries. The linearized Hilbert curve is highly competitive for\npoint queries since it only requires searching for a single point on the curve. This is in contrast to range queries where a\nlarge portion of the curve needs to be scanned.\n3.6 Distance Query Performance\nWe implemented the distance query using the filter and refine [ 56] approach (see Section 2.5), which is the norm in spatial\ndatabases such as Oracle Spatial [ 32] and PostGIS [ 64]. We index GPS coordinates and use the Harvesine distance in the\nrefinement phase.\nFigure 9 shows the distance query runtime for all indexing techniques as well as the two spatial indexes, S2PointIndex\nand JTS STRtree. We can make two important observations. First, the difference in performance between the learned\nindexes diminishes quickly as we increase the selectivity of the query. Grid-based indexes perform the best for lower\nselectivities (0.00001% and 0.0001%), except for uniform queries on Taxi Rides and OSM datasets where Quadtree is better,\nsimilar to range query. However, as more points qualify in the filter phase, Haversine distance computation becomes the\ndominant cost, causing the performance of all indexing methods to converge. Haversine distance is computationally\nexpensive and requires multiple additions, multiplications, and divisions as well as three trigonometric function calls.\nAlthough we only use Harvesine distance on a subset of points in the filter phase, it is still expensive to compute.\nThe second observation is that S2PointIndex outperforms most of the indexes for uniform queries on the OSM dataset.\nThe reason for this is that after the filter phase, many points need refinement for uniform queries for the OSM dataset. For\nexample, for the OSM dataset, the average number of points that need refinement after the filter phase for skewed queries\nis 25, 257, and 2561 for selectivities 0.00001%, 0.0001%, and 0.001%, respectively. For uniform queries, the average number\nof points that need refinement after the filter phase is 4257, 7263, 17612 (6 Ã—to 170Ã—more than skewed queries) for the\n\nEnhancing In-Memory Spatial Indexing with Learned Search 17\n101102103skewed queries\nTweets (83M)\n101102103104\nTaxi Rides (305M)\n101102103104\nOSM (200M)\n1e-05 0.0001 0.001 0.01101102103104uniform queries\n1e-05 0.0001 0.001 0.01101102103104\n1e-05 0.0001 0.001 0.01102103104\nQuery Selectivity (in percent)Average query time [ Âµs]ml-\fxed-grid ml-adaptive-grid ml-kdtree ml-quadtree ml-strtree ml-hilbert jts-strtree s2-pointindex\nFig. 9. Distance Query Performance - Total distance query runtime on skewed and uniform queries for the three datasets.\nOSM dataset. The dominant cost for most index structures is the Haversine distance computation, and thus we also do not\nobserve much difference in performance between the learned indexes. S2PointIndex on the other hand has optimizations\nfor distance queries, where it carefully increases the radius of the internal data structure called S2Cap (a circular disc with\na center and a radius) to visit the Hilbert curve. It does not explicitly rely on the Haversine distance computation but\nworks similarly to a range query with a radius. The S2PointIndex first searches for the point in the Hilbert curve using\nthe query points and then increases the radius along the Hilbert curve until the query radius is satisfied. Therefore, the\ndistance query using the S2PointIndex for uniform queries on the OSM dataset is from 1.91 Ã—to 7.75Ã—faster than the\nlearned indexes. This effect does not reflect in the other datasets since after the filter phase the number of points that\nqualify for Haversine distance computation is similar to that for the skewed queries in the OSM dataset. The comparison\nof learned indexes with JTS STRtree is fairer since both the learned indexes and JTS STRtree deploy the filter and refine\napproach to evaluate distance queries. Fixed-grid is from 1.33 Ã—to 11.92Ã—faster than JTS STRtree.\nKey Takeaways. The computation of Haversine distance dominates the cost of distance queries. While learned search\noffers significant performance gains for lower selectivity queries and for queries with skewed distributions, its advantages\ndiminish for queries with higher selectivity and uniform distributions. These queries produce a large number of points for\nrefinement (i.e., Haversine distance computations). As a result, in many cases, S2PointIndex outperforms learned indexes.\n3.7 Join Query Performance\nFor join queries, we utilized the filter and refine approach for the learned indexes and JTS STRtree. We use the bounding\nbox of the polygon objects and issue a range query on the indexed points, while in S2, we utilize the S2ShapeIndex which\nis specifically built to test for the containment of points in polygonal objects. As mentioned in Section 2.6, we index the\npolygon objects using an interval tree in case of the learned indexes. For JTS STRtree, we utilize the PreparedGeometry4\nabstraction, to index line segments of all individual polygons, which helps in accelerating the refinement check.\n4https://locationtech .github.io/jts/javadoc/org/locationtech/jts/geom/prep/PreparedGeometry .html\n\n18 Pandey, van Renen, Tzirita Zacharatou, et al.\nboroughs neighborhoods census01020304050Join Time [s]Tweets (83M)\nboroughs neighborhoods census050100150200250300Taxi Rides (305M)\ncountries010203040506070OSM (200M)ml-\fxed-grid ml-adaptive-grid ml-kdtree ml-quadtree ml-strtree ml-hilbert jts-strtree s2-pointindex\nFig. 10. Join Query Performance - Total join query runtime for the three datasets.\nWe used three different polygonal datasets for the join query with the location datasets that are in the NYC area\n(i.e., Tweets and Taxi Rides datasets). Specifically, we used the Boroughs, Neighborhoods, and Census block boundaries\nconsisting of five, 290, and approximately 40 thousand polygons, respectively. For the OSM dataset, we perform the join\nusing the Countries dataset which consists of 255 country boundaries. Similarly to range and distance queries, we first\nfind the optimal partition size for each learned index and dataset.\nFigure 10 shows the join query performance. It can be observed in the figure that most of the learned indexes are\nsimilar in join query performance. The reason behind this is that the filter phase is not expensive for the join query,\nwhile the refinement phase is the dominant cost. This result is in conformance to earlier studies [ 62,63], which compared\nstate-of-the-art spatial libraries used by hundreds of systems and other libraries. Although we use an interval tree to\nindex the edges of the polygons to quickly determine the edges intersecting the ray casted from the candidate point,\nthis phase is still expensive. For future work, we plan to investigate the performance using the main-memory index for\npolygon objects proposed in [36].\nIt can also be seen in the figure that the learned indexes are considerably faster than JTS STRtree and S2ShapeIndex\nfor the join query. Fixed-grid, for example, is 1.81 Ã—to 2.69Ã—faster than S2ShapeIndex and 2.7 Ã—to 3.44Ã—faster than JTS\nSTRtree for the Tweets dataset across all three polygonal datasets. Similarly, for the Taxi Rides dataset, fixed-grid is 2.39 Ã—\nto 4.96Ã—faster than S2ShapeIndex and 3.017 Ã—to 4.49Ã—faster than JTS STRtree. Finally, for the OSM dataset, it is 2.89 Ã—\nfaster than S2ShapeIndex and 7.311 Ã—faster than JTS STRtree.\nKey Takeaways. Thefilter phase of the join query produces a large number of points for the refinement phase, which\ninvolves computationally-expensive point-in-polygon tests. The computational overhead of the point-in-polygon tests\ndiminishes the benefits gained from the fast filter phase. As a result, the query performance is similar to that of the\ndistance query, where the dominant cost is the Haversine distance computation.\n3.8 Indexing Costs\nFigure 11 shows that fixed-grid and adaptive-grid are faster to build than tree-based learned indexes. Fixed-grid is 2.11 Ã—,\n2.05Ã—, and 1.90Ã—faster to build than the closest competitor, STRtree. Quadtree is the slowest to build because it generates\na large number of cells for optimal configuration. Not all partitions in Quadtree contain an equal number of points as\n\nEnhancing In-Memory Spatial Indexing with Learned Search 19\nTweets\n(83M)Taxi\n(305M)OSM\n(200M)101102Index Build Time [s]\nTweets\n(83M)Taxi\n(305M)OSM\n(200M)05101520Index Size [GBs]ml-ï¬xed-grid\nml-adaptive-gridml-kdtree\nml-quadtreeml-strtree\njsi-rtrees2-point-index\nFig. 11. Indexing Costs - Index build times and sizes for the three datasets.\nit divides the space rather than the data, thus leading to an imbalanced number of points per partition. Fixed-grid and\nadaptive-grid do not generate a big number of partitions, as the partitions are quite large for optimal configuration. They\nare smaller for similar reasons. The index size in Figure 11 also includes the size of the indexed data.\nIn the figure, we can also see that the learned indexes are faster to build and consume less memory than the S2PointIndex\nand JTS STRtree. Fixed-grid, for example, is from 2.34 Ã—to 15.36Ã—faster to build than S2PointIndex, and from 11.09 Ã—\nto 19.74Ã—faster to build than JTS STRtree. It also consumes less memory than S2PointIndex (from 3.04 Ã—to 3.4Ã—) and\nJTS STRtree (from 4.96 Ã—to 8.024Ã—). However, we note that the comparison of the index size with JTS STRtree is not\ncompletely fair. JTS STRtree is a SAM (spatial access method), where it stores four coordinates for each point (since the\npoints have been stored as degenerate rectangles). The learned indexes implemented in this work are PAMs (point access\nmethod), where we only store two coordinates for each data point.\nKey Takeaways. Grid-based indexes are faster to build and consume less space compared to tree-based indexes. Optimally-\ntuned grid-based indexes use larger partitions, which results in a small total number of partitions. In contrast, tree-based\nindexes produce a large number of smaller partitions. Since we embed learned models within each partition, tree-based\nindexes have a higher build time and are larger in size, as they maintain more learned models.\n4 RELATED WORK\nRecent work by Kraska et al. [ 42] proposed the idea of replacing traditional database indexes with learned models. Since\nthen, there has been a corpus of work on extending the ideas of the learned index to spatial and multidimensional data.\nLearned Multidimensional Indexing and Partitioning. Flood [ 52] is an in-memory read-optimized multidimen-\nsional index that organizes the physical layout of ğ‘‘-dimensional data by dividing each dimension into some number of\npartitions, which forms a grid over the ğ‘‘-dimensional space and adapts to the data and query workload.\nSimilarly to Flood, our implementation of the grid indexes partitions the data using a grid across ğ‘‘âˆ’1dimensions and\nuses the last dimension as the sort dimension. Tsunami[ 10,11] is an improvement over Flood, designed to efficiently handle\ncorrelated and skewed data. Machine learning techniques have also been applied to reduce the I/O cost for disk-based\n\n20 Pandey, van Renen, Tzirita Zacharatou, et al.\nmultidimensional indexes. Qd-tree [ 100] uses reinforcement learning to construct a partitioning strategy that minimizes\nthe number of disk-based blocks accessed by a query. The ZM-index [ 95] combines the standard Z-order space-filling\ncurve with the Recursive-Model Indexes (RMI) proposed by Kraska et al. [ 42] by mapping multidimensional values\ninto a single-dimensional space that is learnable using models. The ML-index [ 8] combines the ideas of iDistance [ 31]\nand RMI [ 42] to support range and KNN queries. There are also efforts to augment existing indexes with lightweight\nmodels to accelerate range and point queries [ 28]. Machine learning has also been applied to various other aspects of data\nmanagement [35, 57, 67, 71, 72, 99].\nLearned Spatial Indexes and Algorithms. LISA [ 47] is a disk-based learned spatial index that achieves low storage\nconsumption and I/O cost while supporting range and nearest neighbor queries, insertions, and deletions. In [ 58], the\nauthors propose an instance-optimized Z-curve index. They present alternate ordering and partitioning of the Z-curve\nand propose two greedy heuristics that learn the most effective ordering for a particular workload and dataset.\nIn traditional R-trees, the branches visited for a particular range query depend on the overlap of the query with multiple\nchildren nodes. In [ 2], the authors propose using the overlap ratio (the required number of leaf nodes / actually visited\nleaf nodes) to identify high-overlap queries. They also build an AI tree that uses uniquely assigned IDs to the R-tree leaf\nnodes as class labels for multi-label classification. At runtime, their technique identifies queries with high overlap and\nuses the AI tree for them; otherwise, it falls back to the regular R-tree for low-overlap queries. The AI tree helps minimize\nthe number of visited leaf nodes. In [ 94], the authors work on SAMs (i.e., spatial objects with extent such as linestrings\nand polygons). The authors compute the Z-curve extent (minimum and maximum of the Z-interval) of the MBR enclosing\nthe spatial object, sort the geometries based on the Z-address interval and build a hierarchical tree-like structure where\ninternal nodes contain the linear regression model and an array of pointers to child nodes, while the leaf node contains\nthe linear regression model and an array of actual data along with the MBR of the data. This is in contrast to the ZM-index\nwhich uses Z-curve values of the points instead of spatial object with extents, as well as the Hibert-curve index that we\nuse to store the points. SPRIG [ 105] is a spatial interpolation function-based grid index. The authors use spatial bilinear\ninterpolation function to predict the position of the query points and then use a local binary search to refine the result.\nSpatial Join Machine Learning (SJML) [ 91,92] is a machine learning-based query optimizer for distributed spatial joins. It\nconsists of three levels: (1) the first level builds the cardinality estimates model that learns the result size of the spatial\njoin, (2) the second level combines the predicted result size with other dataset characteristics to build a separate model\nwhich predicts the number of geometric comparison operations, and (3) builds a classification model which is able to\npredict the best join algorithm. In [ 25], the authors exploit the fact that the R tree can be constructed using multiple\nsplitting strategies, e.g., linear split, quadratic split, R*-tree split etc. They run a microbenchmark to find that the query\nperformance varies for various query workloads and datasets. They identify that ChooseSubtree and Split operations\nfor the tree construction can be considered as sequential decision-making problems. They model them as two Markov\nDecision Processes (MDP) and use reinforcement learning to learn the model for the two operations.\nMachine Learning techniques on spatial data have also been applied to various scenarios such as spatio-textual\nqueries [ 12], social media data [ 26], passage retrieval [ 96], and streaming [ 103], and other areas [ 35,57,71,99]. There also\nexists various surveys that cover in-depth various works that apply machine learning to spatial data [68â€“70].\n5 CONCLUSIONS AND FUTURE WORK\nIn this work, we implemented learning-enhanced variants of six classical spatial indexes. We found that, in most cases, the\nfixed-grid is the best-performing learning-enhanced index and also the simplest one to implement. Next, we summarize\nthe results based on the four queries covered in this work and discuss avenues for future work.\n\nEnhancing In-Memory Spatial Indexing with Learned Search 21\nRange Queries. Recent advancements in applying machine learning to databases have focused on creating instance-\noptimized [ 9,41] versions of various components, including index structures. Motivated by this, in Section 3.4.1 we\nshowed that the performance of various index structures fluctuates depending on the query workload and dataset. We\ndemonstrated that tuning the index structures based on both the dataset and the query workloads is essential to obtain\noptimal performance. Additionally, we showed that using learned models instead of binary search improves performance\nby 11-39%.\nWe also found that learned models do not help much in the case of tree-based index structures. The performance gains\nwere minimal, ranging from 2% to 9%, and in some instances, learned models even resulted in worse performance. We\nshowed that fixed-grid was the best-performing index for range queries, outperforming the closest tree-based competitor\nby a factor of 1.23Ã—up to 1.83Ã—. This is in contrast to traditional knowledge that grid-based structures suffer from skewed\npartitions. We argue that when all data can be indexed and stored in main memory, grid-based index structures, when\ntuned for optimal partition sizes, outperform tree-based index structures. We also showed that fixed-grid is from 8.67 Ã—\nto 43.27Ã—faster than JTS STRtree and from 4.34 Ã—to 53.34Ã—faster than S2PointIndex. Additionally, we addressed the\nreasons behind the poor performance of Hilbert curve-based approaches, i.e., S2PointIndex, and learned linearized Hilbert\ncurve-based index.\nPoint Queries. In Section 3.5, we again observe that fixed-grid is the best-performing index. It is 1.51 Ã—to 2.27Ã—faster\ncompared to the closest tree-based competitor, kdtree. We also showed that the learned index based on linearized Hilbert\ncurve values is highly competitive, as point queries require searching for a specific point on the Hilbert curve. This is in\ncontrast to range queries where it may end up scanning a lot of redundant points.\nDistance Queries. The fixed-grid index has again the best performance for distance queries. However, we note that, as\nin the case of high selectivity range queries, the performance gains diminish with increasing selectivity, and the Haversine\ndistance computation becomes the dominant cost. We also observe that S2PointIndex performs the best in the case of\nuniform queries in the OSM dataset. There are two reasons for this: (1) the filter phase produces a lot more candidate\npoints in the case of uniform queries compared to other datasets and query workloads, and (2) S2PointIndex traverses the\nHilbert curve in an efficient manner avoiding the overheads of the filter and refine approach.\nJoin Queries. Thefilter phase of the join, which internally issues a range query to the index structures using the\nbounding box of the polygons, exhibits the same performance as range queries. However, the refinement phase of the join\nis the most dominant cost. This leads to similar performance of the learned index structures and performance gains are\nlimited. Currently, we utilize interval trees to index the polygons used in the join operation. In the future, we plan to use\nlearned index structures, such as those proposed in [88] and [94], to index the polygons as well.\nFuture Work. Thus far, we have only studied the case where both the indexes and data fit into RAM. For disk-based\nuse cases, the performance will likely be dominated by I/O, and the search within partitions will be less important. We\nexpect the partition sizes to be performance-optimal when aligned with the physical page size. To reduce I/O, it will be\ncrucial to eliminate any unnecessary points from the partitions. Therefore, we anticipate that using two-dimensional\nindexing will be the best approach for disk-based storage. For further discussion on this topic, we refer to LISA [ 47]. Our\nfindings demonstrate that grid-based index structures outperform tree-based indexes by reducing random accesses and\nusing efficient fast search over large partitions. However, the gains diminish when computationally expensive operations,\nsuch as Haversine distance computation and point-in-polygon tests, are required. To minimize the overall query runtime,\nit is necessary to develop novel effective indexing methods for polygons (to minimize or eliminate point-in-polygon tests)\nand circles (to avoid Haversine distance computation). Moreover, in our work, we tuned the index structures using a\n\n22 Pandey, van Renen, Tzirita Zacharatou, et al.\nmanual process. In the future, our aim is to investigate the use of deep learning-based methods to tune the index structures\nfor various types of queries, datasets, and workloads.\nREFERENCES\n[1] Ablimit Aji, Fusheng Wang, Hoang Vo, Rubao Lee, Qiaoling Liu, Xiaodong Zhang, and Joel H. Saltz. 2013. Hadoop-GIS: A High Performance Spatial\nData Warehousing System over MapReduce. PVLDB 6, 11 (2013), 1009â€“1020.\n[2] Abdullah Al-Mamun, Ch. Md. Rakin Haider, Jianguo Wang, and Walid G. Aref. 2022. The \"AI + R\" - tree: An Instance-optimized R - tree. In 23rd IEEE\nInternational Conference on Mobile Data Management, MDM 2022, Paphos, Cyprus, June 6-9, 2022 . 9â€“18. https://doi .org/10.1109/MDM55031 .2022.00023\n[3] Koichiro Amemiya and Akihiro Nakao. 2020. Layer-Integrated Edge Distributed Data Store for Real-time and Stateful Services. In NOMS 2020 - IEEE/IFIP\nNetwork Operations and Management Symposium, Budapest, Hungary, April 20-24, 2020 . 1â€“9. https://doi .org/10.1109/NOMS47738 .2020.9110436\n[4]Jon Louis Bentley. 1975. Multidimensional Binary Search Trees Used for Associative Searching. Commun. ACM 18, 9 (1975), 509â€“517. https:\n//doi.org/10.1145/361002.361007\n[5]Jon Louis Bentley and Jerome H. Friedman. 1979. Data Structures for Range Searching. ACM Comput. Surv. 11, 4 (1979), 397â€“409. https:\n//doi.org/10.1145/356789.356797\n[6] Ilja N Bronshtein and Konstantin A Semendyayev. 2013. Handbook of mathematics . Springer Science & Business Media, Germany.\n[7] Databricks Runtime [n.d.]. Databricks Runtime 7.6 . https://docs .databricks.com/release-notes/runtime/7 .6.html.\n[8]Angjela Davitkova, Evica Milchevski, and Sebastian Michel. 2020. The ML-Index: A Multidimensional, Learned Index for Point, Range, and\nNearest-Neighbor Queries. In 2020 Conference on Extending Database Technology (EDBT) .\n[9] Jialin Ding, Umar Farooq Minhas, Badrish Chandramouli, Chi Wang, Yinan Li, Ying Li, Donald Kossmann, Johannes Gehrke, and Tim Kraska. 2021.\nInstance-Optimized Data Layouts for Cloud Analytics Workloads. In SIGMOD â€™21: International Conference on Management of Data, Virtual Event,\nChina, June 20-25, 2021 . 418â€“431. https://doi .org/10.1145/3448016 .3457270\n[10] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020. Tsunami: A Learned Multi-dimensional Index for Correlated Data and\nSkewed Workloads. Proc. VLDB Endow. 14, 2 (2020), 74â€“86. https://doi .org/10.14778/3425879 .3425880\n[11] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020. Tsunami: A Learned Multi-dimensional Index for Correlated Data and\nSkewed Workloads. Proc. VLDB Endow. 14, 2 (2020), 74â€“86. https://doi .org/10.14778/3425879 .3425880\n[12] Xiaofeng Ding, Yinting Zheng, Zuan Wang, Kim-Kwang Raymond Choo, and Hai Jin. 2022. A learned spatial textual index for efficient keyword\nqueries. Journal of Intelligent Information Systems (2022), 1â€“25.\n[13] Harish Doraiswamy and Juliana Freire. 2020. A GPU-friendly Geometric Data Model and Algebra for Spatial Queries. In Proceedings of the 2020\nInternational Conference on Management of Data, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020 . 1875â€“1885.\nhttps://doi.org/10.1145/3318464 .3389774\n[14] Harish Doraiswamy and Juliana Freire. 2020. A GPU-friendly Geometric Data Model and Algebra for Spatial Queries: Extended Version. CoRR\nabs/2004.03630 (2020). arXiv:2004.03630 https://arxiv .org/abs/2004.03630\n[15] Harish Doraiswamy and Juliana Freire. 2022. GPU-Powered Spatial Database Engine for Commodity Hardware: Extended Version. CoRR\nabs/2203.14362 (2022). https://doi .org/10.48550/arXiv.2203.14362 arXiv:2203.14362\n[16] Harish Doraiswamy and Juliana Freire. 2022. SPADE: GPU-Powered Spatial Database Engine for Commodity Hardware. In 38th IEEE International\nConference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022 . 2669â€“2681. https://doi .org/10.1109/ICDE53745 .2022.00245\n[17] Ahmed Eldawy, Vagelis Hristidis, Saheli Ghosh, Majid Saeedan, Akil Sevim, A. B. Siddique, Samriddhi Singla, Ganesh Sivaram, Tin Vu, and Yaming\nZhang. 2021. Beast: Scalable Exploratory Analytics on Spatio-temporal Data. In CIKM â€™21: The 30th ACM International Conference on Information and\nKnowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021 , Gianluca Demartini, Guido Zuccon, J. Shane Culpepper, Zi Huang,\nand Hanghang Tong (Eds.). 3796â€“3807. https://doi .org/10.1145/3459637 .3481897\n[18] Ahmed Eldawy and Mohamed F. Mokbel. 2015. SpatialHadoop: A MapReduce framework for spatial data. In 31st IEEE International Conference on\nData Engineering, ICDE 2015, Seoul, South Korea, April 13-17, 2015 . 1352â€“1363.\n[19] Ahmed Eldawy, Ibrahim Sabek, Mostafa Elganainy, Ammar Bakeer, Ahmed Abdelmotaleb, and Mohamed F. Mokbel. 2017. Sphinx: Empowering\nImpala for Efficient Execution of SQL Queries on Big Spatial Data. In Advances in Spatial and Temporal Databases - 15th International Symposium,\nSSTD 2017, Arlington, VA, USA, August 21-23, 2017, Proceedings . 65â€“83. https://doi .org/10.1007/978-3-319-64367-0_4\n[20] Raphael A. Finkel and Jon Louis Bentley. 1974. Quad Trees: A Data Structure for Retrieval on Composite Keys. Acta Inf. 4 (1974), 1â€“9. https:\n//doi.org/10.1007/BF00288933\n[21] Volker Gaede and Oliver GÃ¼nther. 1998. Multidimensional Access Methods. ACM Comput. Surv. 30, 2 (1998), 170â€“231. https://doi .org/10.1145/\n280277.280279\n[22] Francisco GarcÃ­a-GarcÃ­a, Antonio Corral, Luis Iribarne, and Michael Vassilakopoulos. 2020. Improving Distance-Join Query processing with\nVoronoi-Diagram based partitioning in SpatialHadoop. Future Gener. Comput. Syst. 111 (2020), 723â€“740. https://doi .org/10.1016/j.future.2019.10.037\n[23] Thanasis Georgiadis, Eleni Tzirita Zacharatou, and Nikos Mamoulis. 2023. APRIL: Approximating Polygons as Raster Interval Lists. CoRR\nabs/2307.01716 (2023). https://doi .org/10.48550/arXiv.2307.01716 arXiv:2307.01716\n\nEnhancing In-Memory Spatial Indexing with Learned Search 23\n[24] David Gomes. 2019. MemSQL Live: Nikita Shamgunov on the Data Engineering Podcast . https://www .memsql.com/blog/memsql-live-nikita-\nshamgunov-on-the-data-engineering-podcast/.\n[25] Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, and Sheng Wang. 2023. The RLR-Tree: A Reinforcement Learning Based R-Tree for Spatial\nData. Proc. ACM Manag. Data 1, 1 (2023), 63:1â€“63:26. https://doi .org/10.1145/3588917\n[26] Na Guo, Yaqi Wang, Haonan Jiang, Xiufeng Xia, and Yu Gu. 2022. TALI: An Update-Distribution-Aware Learned Index for Social Media Data.\nMathematics 10, 23 (2022), 4507.\n[27] Antonin Guttman. 1984. R-Trees: A Dynamic Index Structure for Spatial Searching. In SIGMODâ€™84, Proceedings of Annual Meeting, Boston, Massachusetts,\nUSA, June 18-21, 1984 . 47â€“57. https://doi .org/10.1145/602259.602266\n[28] Ali Hadian, Ankit Kumar, and Thomas Heinis. 2020. Hands-off Model Integration in Spatial Index Structures. In AIDB@VLDB 2020, 2nd International\nWorkshop on Applied AI for Database Systems and Applications, Held with VLDB 2020, Monday, August 31, 2020, Online Event / Tokyo, Japan , Bingsheng\nHe, Berthold Reinwald, and Yingjun Wu (Eds.). https://drive .google.com/file/d/1c3uPyv9apCHWz2wcr1bYNB3JVgCKteKM/view?usp =sharing\n[29] Stefan Hagedorn, Philipp GÃ¶tze, and Kai-Uwe Sattler. 2017. The STARK Framework for Spatio-Temporal Data Analytics on Spark. In Datenbanksysteme\nfÃ¼r Business, Technologie und Web (BTW 2017), 17. Fachtagung des GI-Fachbereichs â€Datenbanken und Informationssysteme\" (DBIS), 6.-10. MÃ¤rz 2017,\nStuttgart, Germany, Proceedings . 123â€“142.\n[30] David Hilbert. 1935. Ãœber die stetige Abbildung einer Linie auf ein FlÃ¤chenstÃ¼ck. In Dritter Band: Analysis Â·Grundlagen der Mathematik Â·Physik\nVerschiedenes . 1â€“2.\n[31] H. V. Jagadish, Beng Chin Ooi, Kian-Lee Tan, Cui Yu, and Rui Zhang. 2005. IDistance: An Adaptive B+-Tree Based Indexing Method for Nearest\nNeighbor Search. ACM Trans. Database Syst. 30, 2 (June 2005), 364â€“397. https://doi .org/10.1145/1071610 .1071612\n[32] Kothuri Venkata Ravi Kanth, Siva Ravada, and Daniel Abugov. 2002. Quad-tree and R-tree indexes in oracle spatial: a comparison using GIS data. In\nProceedings of the 2002 ACM SIGMOD International Conference on Management of Data, 2002 . 546â€“557. https://doi .org/10.1145/564691.564755\n[33] Michael S. Kester, Manos Athanassoulis, and Stratos Idreos. 2017. Access Path Selection in Main-Memory Optimized Data Systems: Should I Scan or\nShould I Probe?. In Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD Conference 2017, Chicago, IL, USA, May\n14-19, 2017 . 715â€“730. https://doi .org/10.1145/3035918 .3064049\n[34] You Jung Kim and Jignesh M. Patel. 2007. Rethinking Choices for Multi-dimensional Point Indexing: Making the Case for the Often Ignored Quadtree.\nInCIDR 2007, Third Biennial Conference on Innovative Data Systems Research, Asilomar, CA, USA, January 7-10, 2007, Online Proceedings . 281â€“291.\nhttp://cidrdb.org/cidr2007/papers/cidr07p32 .pdf\n[35] Andreas Kipf, Dominik Horn, Pascal Pfeil, Ryan Marcus, and Tim Kraska. 2022. LSI: a learned secondary index structure. In aiDM â€™22: Proceedings of the\nFifth International Workshop on Exploiting Artificial Intelligence Techniques for Data Management, Philadelphia, Pennsylvania, USA, 17 June 2022 , Rajesh\nBordawekar, Oded Shmueli, Yael Amsterdamer, Donatella Firmani, and Ryan Marcus (Eds.). ACM, 4:1â€“4:5. https://doi .org/10.1145/3533702 .3534912\n[36] Andreas Kipf, Harald Lang, Varun Pandey, Raul Alexandru Persa, Christoph Anneser, Eleni Tzirita Zacharatou, Harish Doraiswamy, Peter A.\nBoncz, Thomas Neumann, and Alfons Kemper. 2020. Adaptive Main-Memory Indexing for High-Performance Point-Polygon Joins. In Proceedings\nof the 23nd International Conference on Extending Database Technology, EDBT 2020, Copenhagen, Denmark, March 30 - April 02, 2020 . 347â€“358.\nhttps://doi.org/10.5441/002/edbt .2020.31\n[37] Andreas Kipf, Harald Lang, Varun Pandey, Raul Alexandru Persa, Peter A. Boncz, Thomas Neumann, and Alfons Kemper. 2018. Adaptive Geospatial\nJoins for Modern Hardware. CoRR abs/1802.09488 (2018). arXiv:1802.09488 http://arxiv .org/abs/1802.09488\n[38] Andreas Kipf, Harald Lang, Varun Pandey, Raul Alexandru Persa, Peter A. Boncz, Thomas Neumann, and Alfons Kemper. 2018. Approximate\nGeospatial Joins with Precision Guarantees. In 34th IEEE International Conference on Data Engineering, ICDE 2018, Paris, France, April 16-19, 2018 .\n1360â€“1363. https://doi .org/10.1109/ICDE.2018.00150\n[39] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and Thomas Neumann. 2020. RadixSpline: a\nsingle-pass learned index. In Proceedings of the Third International Workshop on Exploiting Artificial Intelligence Techniques for Data Management,\naiDM@SIGMOD 2020, Portland, Oregon, USA, June 19, 2020 , Rajesh Bordawekar, Oded Shmueli, Nesime Tatbul, and Tin Kam Ho (Eds.). ACM, 5:1â€“5:5.\nhttps://doi.org/10.1145/3401071 .3401659\n[40] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass\nlearned index. In Proceedings of the Third International Workshop on Exploiting Artificial Intelligence Techniques for Data Management, aiDM@SIGMOD\n2020, Portland, Oregon, USA, June 19, 2020 . 5:1â€“5:5. https://doi .org/10.1145/3401071 .3401659\n[41] Tim Kraska. 2021. Towards instance-optimized data systems. Proc. VLDB Endow. 14, 12 (2021), 3222â€“3232. https://doi .org/10.14778/3476311 .3476392\n[42] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018. The Case for Learned Index Structures. In Proceedings of the 2018\nInternational Conference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA, June 10-15, 2018 . 489â€“504. https://doi .org/10.1145/\n3183713.3196909\n[43] Jonathan K. Lawder and Peter J. H. King. 2000. Using Space-Filling Curves for Multi-dimensional Indexing. In Advances in Databases, 17th British\nNational Conferenc on Databases, BNCOD 17, Exeter, UK, July 3-5, 2000, Proceedings (Lecture Notes in Computer Science) , Brian Lings and Keith G.\nJeffery (Eds.), Vol. 1832. 20â€“35. https://doi .org/10.1007/3-540-45033-5_3\n[44] Jonathan K. Lawder and Peter J. H. King. 2001. Querying Multi-dimensional Data Indexed Using the Hilbert Space-filling Curve. SIGMOD Rec. 30, 1\n(2001), 19â€“24. https://doi .org/10.1145/373626.373678\n[45] Scott T. Leutenegger, J. M. Edgington, and Mario Alberto LÃ³pez. 1997. STR: A Simple and Efficient Algorithm for R-Tree Packing. In Proceedings of the\nThirteenth International Conference on Data Engineering, April 7-11, 1997, Birmingham, UK . 497â€“506. https://doi .org/10.1109/ICDE.1997.582015\n\n24 Pandey, van Renen, Tzirita Zacharatou, et al.\n[46] Jiangneng Li, Zheng Wang, Gao Cong, Cheng Long, Han Mao Kiah, and Bin Cui. 2023. Towards Designing and Learning Piecewise Space-Filling\nCurves. Proc. VLDB Endow. 16, 9 (2023), 2158â€“2171. https://www .vldb.org/pvldb/vol16/p2158-li .pdf\n[47] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A Learned Index Structure for Spatial Data. In Proceedings of the 2020\nInternational Conference on Management of Data (Portland, OR, USA) (SIGMOD â€™20) . https://doi.org/10.1145/3318464 .3389703\n[48] Guanli Liu, Jianzhong Qi, Christian S. Jensen, James Bailey, and Lars Kulik. 2023. Efficiently Learning Spatial Indices. In 39th IEEE International\nConference on Data Engineering, ICDE 2023, Anaheim, CA, USA, April 3-7, 2023 . IEEE, 1572â€“1584.\n[49] Antonios Makris, Konstantinos Tserpes, Giannis Spiliopoulos, and Dimosthenis Anagnostopoulos. 2019. Performance Evaluation of MongoDB and\nPostgreSQL for Spatio-temporal Data. In Proceedings of the Workshops of the EDBT/ICDT 2019 Joint Conference, EDBT/ICDT 2019, Lisbon, Portugal,\nMarch 26, 2019 (CEUR Workshop Proceedings) , Vol. 2322. http://ceur-ws .org/Vol-2322/BMDA_3 .pdf\n[50] MongoDB 2013. MongoDB Releases - New Geo Features in MongoDB 2.4 . https://www .mongodb.com/blog/post/new-geo-features-in-mongodb-24/.\n[51] Bongki Moon, H. V. Jagadish, Christos Faloutsos, and Joel H. Saltz. 2001. Analysis of the Clustering Properties of the Hilbert Space-Filling Curve.\nIEEE Trans. Knowl. Data Eng. 13, 1 (2001), 124â€“141. https://doi .org/10.1109/69.908985\n[52] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learning Multi-dimensional Indexes. In Proceedings of the 2020 International\nConference on Management of Data (Portland, OR, USA) (SIGMOD â€™20) . https://doi.org/10.1145/3318464 .3380579\n[53] JÃ¼rg Nievergelt, Hans Hinterberger, and Kenneth C. Sevcik. 1984. The Grid File: An Adaptable, Symmetric Multikey File Structure. ACM Trans.\nDatabase Syst. 9, 1 (1984), 38â€“71. https://doi .org/10.1145/348.318586\n[54] NYCTaxiData 2019. NYC Taxi and Limousine Commission (TLC) - TLC Trip Record Data . https://www1 .nyc.gov/site/tlc/about/tlc-trip-record-\ndata.page.\n[55] Oracle 2019. Oracle Spatial and Graph Spatial Features . https://www .oracle.com/technetwork/database/options/spatialandgraph/overview/\nspatialfeatures-1902020 .html/.\n[56] Jack A. Orenstein. [n.d.]. Redundancy in Spatial Databases. In Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data,\nPortland, Oregon, USA, May 31 - June 2, 1989 . https://doi.org/10.1145/67544.66954\n[57] Sachith Gopalakrishna Pai, Michael Mathioudakis, and Yanhao Wang. 2022. Towards an Instance-Optimal Z-Index. (2022).\n[58] Sachith Gopalakrishna Pai, Michael Mathioudakis, and Yanhao Wang. 2022. Towards an Instance-Optimal Z-Index. (2022).\n[59] Varun Pandey, Andreas Kipf, Thomas Neumann, and Alfons Kemper. 2018. How Good Are Modern Spatial Analytics Systems? Proc. VLDB Endow. 11,\n11 (2018), 1661â€“1673. https://doi .org/10.14778/3236187 .3236213\n[60] Varun Pandey, Andreas Kipf, Dimitri Vorona, Tobias MÃ¼hlbauer, Thomas Neumann, and Alfons Kemper. 2016. High-Performance Geospatial\nAnalytics in HyPerSpace. In Proceedings of the 2016 International Conference on Management of Data, SIGMOD Conference 2016, San Francisco, CA,\nUSA, June 26 - July 01, 2016 . 2145â€“2148.\n[61] Varun Pandey, Alexander van Renen, Andreas Kipf, Jialin Ding, Ibrahim Sabek, and Alfons Kemper. 2020. The Case for Learned Spatial Indexes. In\nAIDB@VLDB 2020, 2nd International Workshop on Applied AI for Database Systems and Applications, Held with VLDB 2020, Monday, August 31, 2020,\nOnline Event / Tokyo, Japan . https://drive .google.com/file/d/1Q_kmSPhM86FeeZb8Kz196eNIn7uWcu8L/view?usp =sharing\n[62] Varun Pandey, Alexander van Renen, Andreas Kipf, and Alfons Kemper. 2020. An Evaluation Of Modern Spatial Libraries. In Database Systems for\nAdvanced Applications - 25th International Conference, DASFAA 2020, Jeju, South Korea, September 21-24, 2020, Proceedings, Part II (Lecture Notes in\nComputer Science) , Vol. 12113. 157â€“174. https://doi .org/10.1007/978-3-030-59416-9_46\n[63] Varun Pandey, Alexander van Renen, Andreas Kipf, and Alfons Kemper. 2021. How Good Are Modern Spatial Libraries? Data Sci. Eng. 6, 2 (2021),\n192â€“208. https://doi .org/10.1007/s41019-020-00147-9\n[64] PostGIS [n.d.]. PostGIS . http://postgis .net/.\n[65] Jianzhong Qi, Guanli Liu, Christian S. Jensen, and Lars Kulik. 2020. Effectively Learning Spatial Indices. Proc. VLDB Endow. 13, 11 (2020), 2341â€“2354.\nhttp://www.vldb.org/pvldb/vol13/p2341-qi .pdf\n[66] Keven Richly. 2019. Optimized Spatio-Temporal Data Structures for Hybrid Transactional and Analytical Workloads on Columnar In-Memory\nDatabases. In Proceedings of the VLDB 2019 PhD Workshop, co-located with the 45th International Conference on Very Large Databases (VLDB 2019), Los\nAngeles, California, USA, August 26-30, 2019 (CEUR Workshop Proceedings) , Vol. 2399. http://ceur-ws .org/Vol-2399/paper10 .pdf\n[67] Ibrahim Sabek and Tim Kraska. 2023. The Case for Learned In-Memory Joins. Proc. VLDB Endow. 16, 7 (2023), 1749â€“1762. https://www .vldb.org/\npvldb/vol16/p1749-sabek .pdf\n[68] Ibrahim Sabek and Mohamed F. Mokbel. 2019. Machine Learning Meets Big Spatial Data. Proc. VLDB Endow. 12, 12 (2019), 1982â€“1985. https:\n//doi.org/10.14778/3352063 .3352115\n[69] Ibrahim Sabek and Mohamed F. Mokbel. 2020. Machine Learning Meets Big Spatial Data. In 36th IEEE International Conference on Data Engineering,\nICDE 2020, Dallas, TX, USA, April 20-24, 2020 . 1782â€“1785. https://doi .org/10.1109/ICDE48307 .2020.00169\n[70] Ibrahim Sabek and Mohamed F. Mokbel. 2021. Machine Learning Meets Big Spatial Data (Revised). In 22nd IEEE International Conference on Mobile\nData Management, MDM 2021, Toronto, ON, Canada, June 15-18, 2021 . 5â€“8. https://doi .org/10.1109/MDM52706 .2021.00014\n[71] Ibrahim Sabek, Tenzin Samten Ukyab, and Tim Kraska. 2022. LSched: A Workload-Aware Learned Query Scheduler for Analytical Database Systems.\n(2022).\n[72] Ibrahim Sabek, Kapil Vaidya, Dominik Horn, Andreas Kipf, Michael Mitzenmacher, and Tim Kraska. 2022. Can Learned Models Replace Hash\nFunctions? Proc. VLDB Endow. 16, 3 (2022), 532â€“545. https://www .vldb.org/pvldb/vol16/p532-sabek .pdf\n\nEnhancing In-Memory Spatial Indexing with Learned Search 25\n[73] Majid Saeedan and Ahmed Eldawy. 2022. Spatial parquet: a column file format for geospatial data lakes. In Proceedings of the 30th International\nConference on Advances in Geographic Information Systems, SIGSPATIAL 2022, Seattle, Washington, November 1-4, 2022 , Matthias Renz and Mohamed\nSarwat (Eds.). ACM, 102:1â€“102:4. https://doi .org/10.1145/3557915 .3561038\n[74] Majid Saeedan and Ahmed Eldawy. 2022. Spatial parquet: a column file format for geospatial data lakes. In Proceedings of the 30th International\nConference on Advances in Geographic Information Systems, SIGSPATIAL 2022, Seattle, Washington, November 1-4, 2022 . 102:1â€“102:4. https://doi .org/\n10.1145/3557915 .3561038\n[75] Patricia G. Selinger, Morton M. Astrahan, Donald D. Chamberlin, Raymond A. Lorie, and Thomas G. Price. 1979. Access Path Selection in a Relational\nDatabase Management System. In Proceedings of the 1979 ACM SIGMOD International Conference on Management of Data, Boston, Massachusetts, USA,\nMay 30 - June 1 , Philip A. Bernstein (Ed.). 23â€“34. https://doi .org/10.1145/582095.582099\n[76] Darius Sidlauskas, Sean Chester, Eleni Tzirita Zacharatou, and Anastasia Ailamaki. 2018. Improving Spatial Data Processing by Clipping Minimum\nBounding Boxes. In 34th IEEE International Conference on Data Engineering, ICDE 2018, Paris, France, April 16-19, 2018 . 425â€“436. https://doi .org/\n10.1109/ICDE.2018.00046\n[77] Ruby Y. Tahboub, GrÃ©gory M. Essertel, and Tiark Rompf. 2018. How to Architect a Query Compiler, Revisited. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018, Houston, TX, USA, June 10-15, 2018 . 307â€“322. https://doi .org/10.1145/3183713 .3196893\n[78] Ruby Y. Tahboub and Tiark Rompf. 2016. On supporting compilation in spatial query engines: (vision paper). In Proceedings of the 24th ACM\nSIGSPATIAL International Conference on Advances in Geographic Information Systems, GIS 2016, Burlingame, California, USA, October 31 - November 3,\n2016. 9:1â€“9:4. https://doi .org/10.1145/2996913 .2996945\n[79] Ruby Y. Tahboub and Tiark Rompf. 2020. Architecting a Query Compiler for Spatial Workloads. In Proceedings of the 2020 International Conference\non Management of Data, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14-19, 2020 . 2103â€“2118. https://doi .org/10.1145/\n3318464.3389701\n[80] MingJie Tang, Yongyang Yu, Qutaibah M. Malluhi, Mourad Ouzzani, and Walid G. Aref. 2016. LocationSpark: A Distributed In-Memory Data\nManagement System for Big Spatial Data. PVLDB 9, 13 (2016), 1565â€“1568.\n[81] Konstantinos Theocharidis, John Liagouris, Nikos Mamoulis, Panagiotis Bouros, and Manolis Terrovitis. 2019. SRX: efficient management of spatial\nRDF data. VLDB J. 28, 5 (2019), 703â€“733. https://doi .org/10.1007/s00778-019-00554-z\n[82] Theodoros Toliopoulos, Nikodimos Nikolaidis, Anna-Valentini Michailidou, Andreas Seitaridis, Anastasios Gounaris, Nick Bassiliades, Apostolos\nGeorgiadis, and Fotis Liotopoulos. 2020. Developing a Real-Time Traffic Reporting and Forecasting Back-End System. In Research Challenges\nin Information Science - 14th International Conference, RCIS 2020, Limassol, Cyprus, September 23-25, 2020, Proceedings (Lecture Notes in Business\nInformation Processing) , Vol. 385. 58â€“75. https://doi .org/10.1007/978-3-030-50316-1_4\n[83] Dimitrios Tsitsigkos, Panagiotis Bouros, Nikos Mamoulis, and Manolis Terrovitis. 2019. Parallel In-Memory Evaluation of Spatial Joins. In\nProceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, SIGSPATIAL 2019, Chicago, IL, USA,\nNovember 5-8, 2019 , Farnoush Banaei Kashani, Goce Trajcevski, Ralf Hartmut GÃ¼ting, Lars Kulik, and Shawn D. Newsam (Eds.). ACM, 516â€“519.\nhttps://doi.org/10.1145/3347146 .3359343\n[84] Dimitrios Tsitsigkos, Panagiotis Bouros, Nikos Mamoulis, and Manolis Terrovitis. 2019. Parallel In-Memory Evaluation of Spatial Joins. In Proceedings\nof the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, SIGSPATIAL 2019, Chicago, IL, USA, November\n5-8, 2019 . 516â€“519. https://doi .org/10.1145/3347146 .3359343\n[85] Dimitrios Tsitsigkos, Konstantinos Lampropoulos, Panagiotis Bouros, Nikos Mamoulis, and Manolis Terrovitis. 2020. A Two-level Spatial In-Memory\nIndex. CoRR abs/2005.08600 (2020). arXiv:2005.08600 https://arxiv .org/abs/2005.08600\n[86] TweetsDataset 2020. Tutorials: Filtering Tweets by location . https://developer .twitter.com/en/docs/tutorials/filtering-tweets-by-location.\n[87] Eleni Tzirita Zacharatou, Harish Doraiswamy, Anastasia Ailamaki, ClÃ¡udio T. Silva, and Juliana Freire. 2017. GPU Rasterization for Real-Time Spatial\nAggregation over Arbitrary Polygons. PVLDB 11, 3 (2017), 352â€“365. https://doi .org/10.14778/3157794 .3157803\n[88] Eleni Tzirita Zacharatou, Andreas Kipf, Ibrahim Sabek, Varun Pandey, Harish Doraiswamy, and Volker Markl. 2021. The Case for Distance-Bounded\nSpatial Approximations. In 11th Conference on Innovative Data Systems Research, CIDR 2021, Virtual Event, January 11-15, 2021, Online Proceedings .\nhttp://cidrdb.org/cidr2021/papers/cidr2021_paper19 .pdf\n[89] Eleni Tzirita Zacharatou, Darius Sidlauskas, Farhan Tauheed, Thomas Heinis, and Anastasia Ailamaki. 2019. Efficient Bundled Spatial Range Queries.\nInProceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, SIGSPATIAL 2019, Chicago, IL,\nUSA, November 5-8, 2019 . 139â€“148. https://doi .org/10.1145/3347146 .3359077\n[90] Uber. 2018. Uber Newsroom: 10 Billion. https://www .uber.com/newsroom/10-billion/.\n[91] Tin Vu, Alberto Belussi, Sara Migliorini, and Ahmed Eldawy. 2021. A Learned Query Optimizer for Spatial Join. In SIGSPATIAL â€™21: 29th International\nConference on Advances in Geographic Information Systems, Virtual Event / Beijing, China, November 2-5, 2021 , Xiaofeng Meng, Fusheng Wang,\nChang-Tien Lu, Yan Huang, Shashi Shekhar, and Xing Xie (Eds.). 458â€“467. https://doi .org/10.1145/3474717 .3484217\n[92] Tin Vu, Alberto Belussi, Sara Migliorini, and Ahmed Eldawy. 2022. Towards a Learned Cost Model for Distributed Spatial Join: Data, Code &\nModels. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, Atlanta, GA, USA, October 17-21, 2022 ,\nMohammad Al Hasan and Li Xiong (Eds.). 4550â€“4554. https://doi .org/10.1145/3511808 .3557712\n[93] Tin Vu, Ahmed Eldawy, Vagelis Hristidis, and Vassilis J. Tsotras. 2021. Incremental Partitioning for Efficient Spatial Data Analytics. Proc. VLDB\nEndow. 15, 3 (2021), 713â€“726. https://doi .org/10.14778/3494124 .3494150\n\n26 Pandey, van Renen, Tzirita Zacharatou, et al.\n[94] Congying Wang and Jia Yu. 2022. GLIN: A Lightweight Learned Indexing Mechanism for Complex Geometries. CoRR abs/2207.07745 (2022).\nhttps://doi.org/10.48550/arXiv.2207.07745 arXiv:2207.07745\n[95] H. Wang, X. Fu, J. Xu, and H. Lu. 2019. Learned Index for Spatial Queries. In 2019 20th IEEE International Conference on Mobile Data Management\n(MDM) . 569â€“574.\n[96] Yifan Wang, Haodi Ma, and Daisy Zhe Wang. 2022. LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval.\nProc. VLDB Endow. 16, 2 (2022), 154â€“166. https://www .vldb.org/pvldb/vol16/p154-wang .pdf\n[97] Christian Winter, Andreas Kipf, Christoph Anneser, Eleni Tzirita Zacharatou, Thomas Neumann, and Alfons Kemper. 2021. GeoBlocks: A Query-\nCache Accelerated Data Structure for Spatial Aggregation over Polygons. In Proceedings of the 24th International Conference on Extending Database\nTechnology, EDBT 2021, Nicosia, Cyprus, March 23 - 26, 2021 . 169â€“180. https://doi .org/10.5441/002/edbt .2021.16\n[98] Dong Xie, Feifei Li, Bin Yao, Gefei Li, Liang Zhou, and Minyi Guo. 2016. Simba: Efficient In-Memory Spatial Analytics. In Proceedings of the 2016\nInternational Conference on Management of Data, SIGMOD Conference 2016, San Francisco, CA, USA, June 26 - July 01, 2016 . 1071â€“1085.\n[99] Guang Yang, Liang Liang, Ali Hadian, and Thomas Heinis. 2023. FLIRT: A Fast Learned Index for Rolling Time frames. In Proceedings 26th International\nConference on Extending Database Technology, EDBT 2023, Ioannina, Greece, March 28-31, 2023 . 234â€“246. https://doi .org/10.48786/edbt.2023.19\n[100] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li, Umar F. Minhas, Per-Ã…ke Larson, Donald Kossmann, and Rajeev\nAcharya. 2020. Qd-tree: Learning Data Layouts for Big Data Analytics. In Proceedings of the 2020 International Conference on Management of Data\n(Portland, OR, USA) (SIGMOD â€™20) . https://doi.org/10.1145/3318464 .3389770\n[101] Simin You, Jianting Zhang, and Le Gruenwald. 2015. Large-scale spatial join query processing in Cloud. In 31st IEEE International Conference on Data\nEngineering Workshops, ICDE Workshops 2015, Seoul, South Korea, April 13-17, 2015 . 34â€“41.\n[102] Jia Yu, Jinxuan Wu, and Mohamed Sarwat. 2015. GeoSpark: a cluster computing framework for processing large-scale spatial data. In Proceedings of\nthe 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems, Bellevue, WA, USA, November 3-6, 2015 . 70:1â€“70:4.\n[103] Tong Yu, Guanfeng Liu, An Liu, Zhixu Li, and Lei Zhao. 2022. LIFOSS: a learned index scheme for streaming scenarios. World Wide Web (2022), 1â€“18.\n[104] Haitao Yuan, Guoliang Li, and Zhifeng Bao. [n.d.]. Route Travel Time Estimation on A Road Network Revisited: Heterogeneity, Proximity, Periodicity\nand Dynamicity. ([n. d.]).\n[105] Songnian Zhang, Suprio Ray, Rongxing Lu, and Yandong Zheng. 2021. SPRIG: A Learned Spatial Index for Range and kNN Queries. In Proceedings\nof the 17th International Symposium on Spatial and Temporal Databases, SSTD 2021, Virtual Event, USA, August 23-25, 2021 , Erik Hoel, Dev Oliver,\nRaymond Chi-Wing Wong, and Ahmed Eldawy (Eds.). 96â€“105. https://doi .org/10.1145/3469830 .3470892",
  "textLength": 101004
}