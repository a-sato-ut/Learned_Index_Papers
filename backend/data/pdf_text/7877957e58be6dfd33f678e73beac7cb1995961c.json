{
  "paperId": "7877957e58be6dfd33f678e73beac7cb1995961c",
  "title": "Xling: A Learned Filter Framework for Accelerating High-Dimensional Approximate Similarity Join",
  "pdfPath": "7877957e58be6dfd33f678e73beac7cb1995961c.pdf",
  "text": "Xling: A Learned Filter Framework for Accelerating\nHigh-Dimensional Approximate Similarity Join\n1stYifan Wang\nUniversity of Florida\nwangyifan@ufl.edu2ndVyom Pathak\nUniversity of Florida\nv.pathak@ufl.edu3rdDaisy Zhe Wang\nUniversity of Florida\ndaisyw@ufl.edu\nAbstract â€”Similarity join is a critical and widely used operation\nin multi-dimensional data applications, which finds all pairs of\nclose points within a given distance threshold. Being studied for\ndecades, many similarity join methods have been proposed, but\nthey are usually not efficient on high-dimensional space due to\nthe curse of dimensionality and data-unawareness. Inspired by\nthe Bloom join in RDBMS, we investigate the possibility of using\nmetric space Bloom filter (MSBF), a family of data structures\nchecking if a query point has neighbors in a multi-dimensional\nspace, to speed up similarity join. However, there are several\nchallenges when applying MSBF to similarity join, including\nexcessive information loss, data-unawareness and hard constraint\non the distance metric, because of which few works are designed\nin this way.\nIn this paper, we propose Xling, a generic framework to build\na learning-based metric space filter with any existing regression\nmodel, aiming at accurately predicting whether a query point\nhas enough number of neighbors. The framework provides a\nsuite of optimization strategies to further improve the prediction\nquality based on the learning model, which has demonstrated\nsignificantly higher prediction quality than existing MSBF. We\nalso propose XJoin, one of the first filter-based similarity join\nmethods, based on Xling. By predicting and skipping those\nqueries without enough neighbors, XJoin can effectively reduce\nunnecessary neighbor searching and therefore it achieves a\nremarkable acceleration. Benefiting from the generalization ca-\npability of deep learning models, XJoin can be easily transferred\nonto new dataset (in similar distribution) without re-training.\nFurthermore, Xling is not limited to being applied in XJoin,\ninstead, it acts as a flexible plugin that can be inserted to any\nloop-based similarity join methods for a speedup. Our evaluation\nshows that Xling not only leads to the high performance of XJoin\n(e.g., being up to 17x faster than the baselines while maintaining\na high quality), but also be able to further speed up many existing\nsimilarity join methods with quality guarantee.\nIndex Terms â€”similarity join, high-dimensional data manage-\nment, machine learning\nI. I NTRODUCTION\nIn multi-dimensional data management, metric space range\nsearch (shortly called range search ) and similarity join are\ncritical operators. Given a distance threshold Ïµ, the former\noperation finds all points in a dataset Dwhose distance to\na given query point is less than Ïµ, while the latter operation\nfinds all pairs of points between two datasets RandSwhose\ndistance is less than Ïµ. As range search can be seen as a\nspecial case of similarity join where |R|= 1 or|S|= 1,\nunless it is necessary, we will only discuss similarity join\nin this paper. With the emergence of deep learning, high-\ndimensional neural embedding has been widely adopted asthe data representation in a wide range of applications, which\nraises the demands for effective and efficient similarity join\nover high-dimensional data. Those applications include near-\nduplicate detection [1], [2], [3], [4], [5], data integration [6],\n[7], [8], data exploration [9], [10], [11], privacy [12], [13] and\nso on. Specifically, distance between embeddings reflects the\nsemantic similarity between the data objects, meaning that the\napplications have to frequently search for close points in the\nembedding space, i.e., the similarity join. In many real-world\nuse cases, the similarity join is approximate, i.e., they do not\nrequire 100% accurate results, but do demand high processing\nspeed, especially on large-scale data. A typical example is\nnear-duplicate video retrieval [3] that identifies online videos\nwith identical or almost identical content during the search\nprocess to diversify the video search results, in which case\nmissing a few pairs of similar videos is acceptable, while fast\nresponse is required.\nExisting work on efficient similarity join mainly includes\ntwo categories: space-grid [14], [15], [16], [17] and locality-\nsensitive hashing (LSH) based methods [18], [19], [4], [5],\n[12], [13]. The former splits the data space into grids and\njoins the points within the same or neighboring grids, while the\nlatter is essentially an adoption of RDBMS hash-join principle\nonto the high-dimensional join, i.e., hashing one dataset by\nLSH and then probing it for the points in another dataset.\nHowever, the grid-based methods poorly perform in very high-\ndimensional space due to the curse of dimensionality, while the\nunawareness for data distribution usually causes a non-trivial\nperformance degradation to LSH-based methods on unevenly\ndistributed data [20]. In addition, the grid-based methods are\nusually exact while LSH-based methods are approximate, i.e.,\ntheir accuracies are respectively 100% and lower than 100%.\nIn this paper we focus on approximate similarity join.\nThere is one more possible way that speeds up similarity\njoin with metric space Bloom filter , inspired by the Bloom join\nin RDBMS (which uses a Bloom filter to prune unnecessary\nprobings). Metric space Bloom filter (MSBF) is designed for\nchecking whether a query point has neighbors within the\ngiven distance threshold. Among the various MSBFs [21],\n[22], [23], [24], [25], [26], [27], Locality-Sensitive Bloom\nFilter (LSBF) [21] is the most widely used, based on which a\nsubstantial number of MSBFs have been developed. Mirroring\nto the hashing functions in Bloom filter, LSBF utilizes LSH\nfunctions to map a multi-dimensional point to several bitsarXiv:2402.13397v1  [cs.DB]  20 Feb 2024\n\nin the bit array, and determines the neighbor existence by\ncounting the non-zero bits. Given datasets RandS, an LSBF\ncan be built on Rand each point sâˆˆSwill act as a query. By\nskipping the range search for those negative queries (i.e., the\nqueries having no neighbors in R), the similarity join can be\naccelerated. Note that unlike Bloom filter, most MSBFs cannot\nguarantee a zero false negative rate since they are based on\nLSH which raises both false positives and negatives.\nAccording to our evaluation, negative queries usually take\nup a non-trivial portion (20% âˆ¼90%), meaning that the fil-\ntering should result in a significant performance improvement.\nBut few methods are designed in such a way, due to several\nproblems of MSBF: (1) Given the fact that they are built on\ntop of LSH, their effectiveness is also limited by the data-\nunawareness. (2) They lose more information compared to\nthe original LSH because of further mapping LSH values\nto one-dimension bit array, which additionally lowers their\neffectiveness. (3) They indicate the index of target bit by the\nLSH value, and this disables them to support many popular\ndistance metrics, like cosine distance where the LSH values\nare always 0 or 1.\nInspired by the â€œlearned Bloom filterâ€ [28] that enhances\nBloom filter with machine learning, we propose a new type\nof MSBF based on machine learning techniques which ad-\ndresses the problems above. Specifically, in this paper, we\npropose Xling , a generic framework for building a MSBF\nwith deep learning model. Instead of LSH, Xling relies on\nthelearned cardinality estimation techniques which utilize\nregression models to predict the number of neighbors for\nrange search before actually executing it. Xling is designed\nto be generic such that any cardinality estimator (or simply,\nregression model) can be encapsulated into an effective MSBF.\nIts core is a cardinality estimator with a Xling decision\nthreshold (XDT). For a range query, Xling predicts the result\ncardinality, then determines the query is positive (i.e., having\nenough neighbors) if the prediction exceeds XDT, otherwise\nit is negative (i.e., being without enough neighbors). Note\nthat we mention â€œenough neighborsâ€ instead of â€œany neigh-\nborsâ€, which reveals an advanced feature: Xling can determine\nwhether the query point has more than Ï„neighbors, where\nÏ„is a user-determined number. And Xling downgrades to a\ngeneral MSBF when Ï„= 0. We call such a feature â€œfiltering-\nby-countingâ€.\nBy learning the data distribution, Xling solves the data-\nunawareness problem, and essentially as a regressor, it is not\nlimited to any specific distance metric. Note that we have\nmentioned three thresholds until now, the distance threshold Ïµ,\ntheXling decision threshold XDT, and the neighbor threshold\nÏ„. To make it clearer, we will indicate them respectively\nwith â€œdistance thresholdâ€, â€œdecision thresholdâ€ and â€œneighbor\nthresholdâ€, or directly using their symbols.\nXling deploys novel optimization strategies to further im-\nprove the performance, including strategy to select Ïµvalues\nfor effective training, and strategy to select the best XDT.\nFurthermore, Xling can be applied as a plugin onto many\nexisting similarity join methods to significantly enhance theirefficiency with tiny or no quality loss. We have applied\nXling to a brute-force nested-loop similarity join, named\nXJoin , which achieves significantly higher efficiency (up to\n14.6x faster) than the existing high-performance similarity\njoin methods (some are used in industry), with a guarantee\nof high quality. In these applications, filtering-by-counting\nenables Xling to help the base similarity join method ignore\nthe queries having only a trivial number of neighbors (e.g., 3 or\n5, or even 50), which makes the acceleration more significant\nwith tiny sacrifice of recall. In addition, we also apply Xling\nonto those prior methods and show that they are substantially\naccelerated with slightly more quality loss. Finally, XJoin and\nXling are evaluated on their generalization capability, and the\nresults prove that they can be transferred to updated or fully\nnew dataset without re-training the learning model. To our\nbest knowledge, We are the first to propose such a learning-\nbased MSBF and XJoin is among the first practical filter-based\nsimilarity join methods for high-dimensional data.\nThe main contributions of this paper are as follows:\n1) We propose XJoin, the first filter-based similarity join\nmethod for high-dimensional data, which is both effi-\ncient and effective.\n2) We propose Xling, a generic framework for constructing\nlearned metric space Bloom filters with general regres-\nsion models.\n3) We design performance optimization strategies in Xling,\nincluding selection of Ïµvalues for effective training and\nadaptive computing of XDT.\n4) We conduct extensive evaluation to show the efficiency,\neffectiveness, usefulness and generalization of Xling and\nXJoin, as well as the remarkable performance improve-\nment by applying Xling to other similarity join methods.\nThe rest of this paper is organized as follows: Section II\nintroduces the prior works related to the techniques in this\npaper. Section III formally defines the key problems studied by\nthis paper and the important notations being used. Section IV\npresents the architecture of Xling and the workflow of applying\nXling to enhancing similarity join. Section V discusses details\nabout the optimization strategies integrated in Xling. And\nSection VI reports and analyzes the evaluation results.\nII. R ELATED WORK\nLearned Bloom filter: The learned Bloom filter is first\nproposed by [28] which treats the existence checking in Bloom\nfilter as a classification task, thereby replaces the traditional\nBloom filter with a deep learning based binary classifier\nfollowed by an overflow Bloom filter (to double check the\nnegative outputs of the classifier to guarantee a zero false\nnegative rate). And there have been many following works\nthat further improve the learned Bloom filter [29], [30], [31]\nby adding auxiliary components or improving the performance\nof the hashing functions being used.\nLearned cardinality estimation: Learning models have\nbeen widely used to predict the number of neighbors for a\nmetric space range search, which is called â€œlearned cardinality\nestimationâ€. The learned cardinality estimation techniques\n\ntreat the task as a regression problem and solve it using\nregression models. The state-of-the-art methods [32], [33],\n[34], [35] are usually based on deep regression models to\neffectively learn the data distribution and make more accurate\nprediction than the non-deep approaches. Recursive Model\nIndex(RMI) [36] is a hierarchical learning model that consists\nof multiple sub-models, where each sub-model is a regression\nmodel like neural network. CardNet [33] consists of a feature\nextraction model and a regression model. The raw data is\ntransformed by the feature extraction model into Hamming\nspace, which will then used by the regression model to predict\nits cardinality. SelNet [35] predicts the cardinality by a learned\nquery-dependent piece-wise linear function.\nMetric space Bloom filter: DSBF [26] and LSBF [21]\nare two of the representatives for metric space Bloom filter\n(MSBF), and following them many MSBF variants and rel-\nevant applications have been developed. [25] enables LSBF\nto handle multiple Euclidean distance granularity without re-\nbuilding the data structure. [24] extends [25] to Hamming dis-\ntance. [23] proposes a DSBF variant with zero false negative\nrate theoretically. Since most of those works are built on top\nof LSH, their effectiveness is usually limited by unknowing\nof the data distribution.\nSimilarity join: This problem can be further classified into\ntwo sub-types, exact and approximate similarity join, where\nthe former is to exactly find all the truly close points while\nthe latter allows some errors.\nOne family of the state-of-the-art efficient methods for\nexact similarity join is the epsilon-grid-order (EGO) based\nmethods, including EGO-join [15], EGO-star-join [16], Super-\nEGO [17], FGF-Hilbert join [14], etc. They work by splitting\nthe space into cells and sorting the data points along with those\ncells, which will then help reduce unnecessary computation in\nthe similarity join.\nApproximate similarity join methods are usually based on\nLSH [18], [37], [19], [4], [5], [12], [13]. The problem of those\nmethods is data unawareness, i.e., the hashing-based space\npartitioning usually does not consider the data distribution,\nwhich may leads to imbalanced partitions that significantly\nlower the overall search performance [20].\nIII. P RELIMINARIES\nNotation Description\nÏµ The distance threshold for range search and\nsimilarity join\nÏ„ The neighbor threshold, i.e., whether or not a\nquery has more than Ï„neighbors\nXDT The Xling decision threshold to classify the\nprediction as positive or negative\nN Dataset size\n|R|,|S| The sizes of set RandS\nMAE ,MSE Mean absolute error and mean squared error\nFPR ,FNR False positive rate and false negative rate\nTABLE I: List of notations used in this and following sectionsThis section presents the critical notations frequently used\nin this paper and defines the key problems being studied, i.e.,\nrange search and similarity join. The notations are listed and\nexplained in Table I, and more details of them are introduced\nwhere they are first referred in this paper.\nIn multi-dimensional space, range search is defined as the\nsearch of all data points whose distance to the query is\nsmaller than a given threshold under some distance metric.\nAnd similarity join is to find all close point pairs whose\ndistance is less than a threshold between two datasets. The\nformal definitions are as below:\nDefinition 1 (range search): Given a dataset P={pi|i=\n1,2, ..., n}, a distance metric d(Â·,Â·), a query point qand a\nradius/threshold Ïµ, range search tries to find a set of data points\nPâˆ—such that for any pâˆ—\niâˆˆPâˆ—,d(q, pâˆ—\ni)â‰¤Ïµ.\nDefinition 2 (similarity join): Given two datasets RandS, a\ndistance threshold Ïµ, and a distance metric d(Â·,Â·), the similarity\njoin between RandSis denoted by R â–· â— ÏµS, which combines\neach point râˆˆRwith each point sâˆˆSthat is close/similar\nenough to r(i.e., with distance smaller than or equal to Ïµ).\nFormally\nR â–· â— ÏµS={(r, s)|âˆ€râˆˆR,âˆ€sâˆˆS where d (r, s)â‰¤Ïµ}(1)\nThere are three critical â€œthresholdsâ€ in this paper, as listed in\nTable I, the â€œdistance thresholdâ€ Ïµ, the â€œneighbor thresholdâ€ Ï„\nand the â€œXling decision thresholdâ€ XDT. To avoid confusion,\nwe further explain them here:\n1) The distance threshold Ïµis part of the range search and\nsimilarity join operations, which indicates the search\nrange as shown in Definition 1 and 2.\n2)Ï„is a threshold for the groundtruth neighbors. If a query\ntruly has more than Ï„neighbors, we call it a groundtruth\npositive , otherwise it is a groundtruth negative .\n3) The Xling decision threshold (XDT) is the threshold to\nclassify the query based on the predicted neighbors. If a\nquery is predicted as having more than XDT neighbors,\nwe name it as a predicted positive , otherwise it is\npredicted negative.\nNote that we do not directly use Ï„to threshold the predictions.\nThis is because different cardinality estimation models have\ndifferent prediction accuracy, leading to different predicted\nvalues for the same query. So it is necessary to use an\nadpative threshold driven by both model and data to classify\nthe predictions, which is XDT, such that we can control the\nfalse positive or negative rate. More details are introduced in\nSection V-A.\nIV. A RCHITECTURE AND WORKFLOW\nA. The core and optimization strategies\nFigure 1 illustrates the overall architecture and workflow of\nXling. The core is the learned cardinality estimator and XDT\n(yellow boxes). The green boxes represent the optimization\nstrategies deployed in the training and predicting stages. The\nblue shapes stand for the training data, including the raw\ndata (which is input from the external) and the prepared data\n\nLearned\ncardinality estimator\nXDTTraining Îµ selection\nAdaptive XDT\nselection Query point + Îµ\nðœ + Îµ Positive/NegativeRaw\ntraining data\nPrepared\ntraining dataTrain\nPredictFig. 1: Architecture and workflow of Xling\n(which is generated based on the raw data inside Xling).\nThe workflow of Xling mainly relies on the prepared training\ndata instead of the raw data. In addition, the pink boxes\nindicate the query-time inputs, including the query point, Ïµ\nandÏ„, where the query point and Ïµare fed into the learned\ncardinality estimator while the ÏµandÏ„are used to compute\nXDT. The grey box stands for the output, i.e., whether the\nquery point is positive (with enough neighbors) or negative\n(having insufficient neighbors). For the workflow, the solid\narrows present the offline training workflow, while the hollow\narrows indicate the online workflow for querying and XDT\ncomputing.\nThe overall architecture includes (1) the core learned car-\ndinility estimator and XDT and (2) the surrounding optimiza-\ntion strategy modules. [28] uses a classifier and a decision\nthreshold to build a learned Bloom filter, while we use a\nregressor and the XDT to construct Xling, which is similar to\nthe learned accelerator framework (LAF) in [38]. Specifically,\nLAF proves that learned cardinality estimator with a proper\ndecision threshold can significantly accelerate range-search\nbased high-dimensional clustering. As similarity join is also\nbased on range search, such a solution works for it too. And\nunlike the case of learned Bloom filter, since filtering-by-\ncounting requires estimating the specific number (mentioned\nin Section I), regressor is the best choice rather than classifier.\nBut in LAF, the decision threshold has to be determined by\ngrid search. To overcome this shortcoming, we design an\nadaptive XDT selection strategy based on the training data\nsuch that XDT can be efficiently computed. More details are\nintroduced in Section V-B.\nTo reduce the false results, in addition to adaptively select-\ning a proper XDT, we also propose an adaptive training Ïµ\nselection strategy to sample the most representative Ïµvalues\nfor training the cardinality estimator effectively and thereby\nreducing the prediction error. The existing learned cardinality\nestimation studies (e.g., [39]) usually select the training Ïµval-\nues uniformly, which is not optimal, and we show in this paper\nthat our proposed strategy generates a more representative\ntraining set and leads to a more effective model training. Thistraining Ïµselection strategy is further discussed in Section\nV-A.\nB. Training and querying workflows\nThere are two workflows in Xling, the offline training\nworkflow and online querying workflow. In Figure 1, the\nsolid arrows illustrate the training workflow. As introduced\nin Definition 2 and Section I, the two point sets to be joined\nare denoted by RandS, and without loss of generality, we\nassume that the size of R(|R|) is larger than size of S(|S|).\nThen Ris used as the training set (i.e., the â€œraw training dataâ€\nin Figure 1) to train Xling while Sacts as the queries. The raw\ntraining data includes all points in Rwithout Ïµinformation.\n1) Training: The first step in the training workflow is\nconcatenating the selected Ïµvalues onto each training point\nriâˆˆR(where i= 1,2, ...,|R|), generating the â€œprepared\ntraining dataâ€ that is a Cartesian product between the point\nsetRandÏµsetE, i.e., the prepared training data is the set\n{(ri,Ïµj)| âˆ€(ri, Ïµj)âˆˆRÃ—E} , where riis a multi-dimensional\nvector and Ïµjis a real number. Here the set Eis generated by\nthe training Ïµselection strategy (Section V-A).\nTo make it clearer, suppose Ris a toy raw training set of 2\npoints raandrb, andEincludes two selected values Ïµ1andÏµ2.\nThen the prepared training set looks like {(ra,Ïµ1), (ra,Ïµ2), (rb,\nÏµ1), (rb,Ïµ2)}associated with their targets (i.e., the groundtruth\nnumbers of neighbors). Then the learned cardinality estimator\nis trained with the prepared training data.\n2) Querying: The querying workflow is presented with\nhollow arrows in Figure 1. First, ÏµandÏ„are input to compute\nthe XDT using the adaptive XDT selection strategy based on\nthe prepared training data. Second, the query point and the Ïµ\nare concatenated and fed to the cardinality estimator to predict\nthe number of neighbors. Finally, the prediction is compared\nwith XDT and the answer (positive or negative) is determined.\nC. Use in similarity join\nUse of Xling to speed up similarity join is straightforward:\ngiven Ïµ,Ï„,RandS, supposing |R|>|S|, Xling is first trained\nonR, then for each query point sâˆˆS, Xling predicts whether\nshas enough neighbors in Runder the Ïµ. If yes, the range\nsearch (either a brute-force search or an indexed search like\nusing LSH) will be executed in Rfor query s, otherwise s\nis predicted as negative and the search for it will be skipped.\nIn this way, unnecessary range search is reduced and the join\nefficiency is improved.\nAs a generic filter, Xling can be applied onto any nested-\nloop based similarity join algorithms to speed them up. When\nusing deep regressor as the core cardinality estimator in Xling,\nits prediction time complexity for each query is constant\n(O(1)) with the data scale, which in practice can be further\naccelerated by GPU. And the training time is not an issue\ndue to the generalization capability of the deep models, i.e., a\ntrained estimator can be used on any other dataset with similar\ndistribution, which is proved by our evaluation in Section VI-F.\nWe implement XJoin by applying Xling onto a naive nested-\nloop similarity join method which does a brute-force range\n\nsearch for each query. Our evaluation presents that XJoin\noutperforms the state-of-the-art similarity join baselines. We\nalso apply Xling to some approximate similarity join methods,\nwhich shows Xling successfully improves their speed-quality\ntrade-off capability.\nV. O PTIMIZATION STRATEGIES\nIn this section we introduce the details for the optimization\nstrategies we propose in Xling.\nA. Training Ïµselection\n[39] uniformly samples the training Ïµfrom range [0, Î¸max],\nwhere Î¸max is a user-determined upper limit for the distances.\nHowever, such a selection strategy is not optimal. We show\nin this section that the cardinality estimation models can be\ntrained more effectively by using our data-aware threshold\nselection strategy. Furthermore, our strategy can be generalized\nto better solve a family of regression problems.\nThe specific family of regression problems is named\nCondition-based Regression (shortly denoted by CR) and\ndefined as follows:\nDefinition 3 (Condition-based Regression problem): Given\na dataset D, a condition cassociated with each data point\npâˆˆ D, and a target tcorresponding to each (p, c)pairs, the\nCR problem is to learn the relationship between the presented\n(p, c)andt, then predict the corresponding tfor any given\n(p, c).\nIn the context of learned cardinality estimation, the condi-\ntion is the Ïµwhile the target is the number of neighbors.\nIn most Condition-based Regression problems, the condition\nis usually a continuous variable (like distance) whose values\ncannot be fully enumerated, therefore it is worth studying\nhow to sample the conditions for better training the regression\nmodel in order to lower prediction error, which is formally\ndefined as such:\nDefinition 4 (Training condition selection for CR): Given\na dataset Dof size n, a set of mcandidate values\n{ci1, ci2, ..., c im}of the condition cifor each data point\npiâˆˆ D,1â‰¤iâ‰¤n, the corresponding target tijfor each\n(pi, cij),1â‰¤jâ‰¤m, and a sampling number s, the training\ncondition selection task is to select spairs from the m(cij, tij)\npairs to form straining tuples for each point pi, which results\nin totally sntraining tuples for the whole dataset D, such that\nthe mean regression error is minimized.\nNote that our discussion starts from a discrete candidate\nset, instead of directly beginning from the continuous range\n(like [0, Î¸max]). This is just to unify the discussion between\ncontinuous-range uniform sampling and our sampling method\nwhich requires preprocessing to discretize the range, which\nwill not lose generality. mis usually large (such that the values\nare dense enough) to approximate the case of the continuous\ncondition values, so we do not use all the mcandidate values\nto generate mntraining tuples, as the memory space is limited.\nFor example, in our evaluation, m= 100 , in which case mn\ntuples require more memory space than that of our evaluationmachine. Therefore only the ssampled conditions are used to\nform the training tuples.\nUniformly sampling training conditions cannot reflect the\nunevenly data distribution in real-world datasets. Therefore,\nwe design a generic adaptive strategy to further fine-tune the\ninitial uniformly sampling conditions for CR problem based\non the density of the targets, such that the resulting training\ntuples are more representative for the data distribution of\nthe whole dataset. Our adaptive training condition selection\n(ATCS) strategy (Algorithm 1) follows the steps below for\neach data point pin the training set:\n1) Given the uniformly sampled candidate conditions Cp\nand the corresponding targets Tpfor point p(i.e., when\ncondition ciâˆˆ Cpis applied to point p, the target is\ncorrespondingly tiâˆˆ Tp), the minimum and maximum\ntargets are found ( tminandtmax, line 5) and the interval\n[tmin, tmax] is then partitioned into sbins evenly (line\n6).\n2) Then each candidate condition is mapped into one bin\nbased on its paired target (line 7-8).\n3) Finally the specific number of condition-target pairs are\nsampled from each bin (line 10-11) according to the\nfraction of the bin size ( |Bi|) over total number of the\nconditions ( |Cp|), where bin size is the number of pairs\nin that specific bin.\n4) Since some bins may generate zero samples (i.e., when\ns|Bi|<|Cp|), the final sampled condition-target pairs\nmay be not enough given the required s. In such a case,\nthe rest1of the pairs will be randomly chosen from the\nunselected ones that are not yet included in the samples\nabove (line 12-13).\nThe final training set for the whole Dis the collection of\nthe training tuples ( p,c,t) generated by the strategy for each\npâˆˆ D, i.e., the final training set includes s|D|tuples.\nUnlike uniformly sampling, ATCS is data-aware by binning\nthe condition-target pairs to estimate the density of the targets\nand then sampling the final conditions based on the density,\nwhich generates more representative training conditions and\ntargets. For example, let m= 100 , s= 5, i.e., the interval of\ntargets [ tmin,tmax] is split into 5 bins evenly and their corre-\nsponding condition values are placed into the bins accordingly.\nSupposing the numbers of conditions from B1toB5are 59,\n1, 19, 1, 20, the uniformly sampling will select 2, 1, 0, 1, 1\nconditions from them (i.e., selecting one per 20), while ATCS\nwill first select 2, 0, 0, 0, 1 from them (Algorithm 1 Line\n11) then probably select the rest 2 conditions from B1,B3\nandB5which are the most dense areas (Algorithm 1 Line\n13). In this example, the uniformly sampling gets 2 out of\nthe 5 sampled conditions from very sparse areas ( B2andB4)\nin the distribution of targets, which cannot well reflect the\noverall distribution, while ATCS probably selects all the 5\nfrom dense areas and results in more representative training\n1In our evaluation, the rest of the pairs (which are randomly chosen) usually\noccupy 10% âˆ¼20% in the final training set for the whole D, i.e., 0.1s|D| âˆ¼\n0.2s|D|\n\nAlgorithm 1 Adaptive training condition selection (ATCS)\nstrategy\nInput: Dataset D, map from each data point to its uniformly sampled\ncandidate condition list C, map from each point to its target list\nT, sampling number s\nOutput: set of resulting training tuples R\n1:R:=âˆ…\n2:for each point pinDdo\n3: Cp:=C(p) â–·the condition list for p\n4: Tp:=T(p)â–·the target list for p, one-to-one corresponding\ntoCp\n5: tmin,tmax := min( Tp), max( Tp)\n6: Split interval [ tmin,tmax] into sbins evenly\n7: for each cinCpand corresponding tinTpdo\n8: Place (c, t)into a bin bounded by [ta, tb)such that tâˆˆ\n[ta, tb)\n9: S:=âˆ… â–·the selected (c, t)collection for p\n10: for each binBido\n11: S:=S âˆª {j\ns|Bi|\n|Cp|k\nrandomly sampled (c, t)pairs from\nBi}\n12: if|S|< s then\n13: Fill with random samples from unselected (c, t)until\n|S|=s\n14: R:=R âˆª { (p, cs, ts)|(cs, ts)âˆˆ S} â–·combine all pairs in\nSwithp\n15:return R\nsamples, by which the model can be trained more effectively.\nOur evaluation in Section VI-B shows that ATCS helps reduce\n50%âˆ¼98% of the prediction error (MAE and MSE).\nB. Xling decision threshold (XDT) selection\nXling decision threshold (XDT) determines whether the\nprediction means positive. Its value is influenced by Ï„, the\nway it is computed, and the way to identify the groundtruth\nnegative training samples, which will be introduced in this\nsection. Note that XDT is determined purely based on the\ntraining set offline, regardless of the online queries, i.e., the\ncomputation of XDT does not raise overhead on the online\nquerying.\nBasically, XDT is computed using the groundtruth negative\ntraining samples (i.e., the training points with no more than\nÏ„neighbors). We propose two ways to compute XDT: false\npositive rate (FPR) based and mean based.\n1) FPR-based XDT selection: similarly to [36], [40], [30],\ngiven a FPR tolerance value tfpr(e.g., 5%), this method\nlets the estimator make predictions for the training points\nand sets XDT such that the resulting filter FPR on\ntraining set is lower than tfpr.\n2) mean-based XDT selection: this method sets XDT to be\nthe mean predicted value for all the groundtruth negative\ntraining samples.\nIn our evaluation (Section VI-B2), FPR-based method usually\nresults in a higher XDT, leading to more speedup and lower\nquality in end-to-end similarity join, so we design the mean-\nbased method to provide the second option which results in a\nlower XDT and leads to higher quality and less speedup. They\nare useful in different situations as shown in our evaluation. In\naddition, no matter using FPR-based or mean-based selection,\nthere is a trend that a larger Ï„will result in a higher XDT,which is straightforward to understand: a larger Ï„causes\ntraining samples with more neighbors to be negative, therefore\ncardinalities of the negative samples increase overall, making\nthe computed XDT increased.\nBut both methods have a problem: we have to first identify\nthe groundtruth negative samples in the training set, which\nis costly in the high-dimensional cases. Since the negative\nsamples depend on Ïµ, and the training set only include a\ntiny portion of all possible Ïµ, the queried Ïµwill probably\nnot exist in training set (named â€œout-of-domain Ïµâ€), in which\ncase intensive range search has to be executed to compute the\nnegative samples from scratch.\nTherefore, it is non-trivial to study how to easily get the\ntraining targets (i.e., groundtruth cardinalities) under the out-\nof-domain Ïµsuch that the groundtruth negative samples can be\nidentified without doing range search for each training point.\nWe propose an interpolation-based strategy to generate the ap-\nproximate targets. Specifically, given a point, its Ïµ-cardinality\ncurve is monotonically non-decreasing, i.e., with Ïµincreasing,\nthe cardinality will never decrease. So we can approximate\nthe curve segment between two neighboring training Ïµvalues\n(denoted by Ïµ1andÏµ2) as linear, and use linear-interpolation\nto estimate the groundtruth cardinality for any training point\nunder a out-of-domain Ïµ3between Ïµ1andÏµ2, as shown in\nEquation 2.\nt3=t1+t2âˆ’t1\nÏµ2âˆ’Ïµ1(Ïµ3âˆ’Ïµ1) (2)\nwhere tiis the target of the current training point under Ïµi\n(i= 1,2),t3is the approximate target under out-of-domain\nÏµ3, and Ïµ1< Ïµ2. The approximate targets are then used to find\nall groundtruth negative training samples under Ïµ3based on\nwhich XDT is computed.\nOur evaluation shows that in most cases, Xling deploying\nthe interpolation-based method has a competitive prediction\nquality to that using the naive solution. Furthermore, there are\ntwo advantages of the proposed method over the naive way:\n(1) it is significantly faster since no range search is executed,\nand (2) as in Section VI-B2, interpolation-based method tends\nto result in no higher false negative rate (FNR) than the naive\nmethod, which is better for the effectiveness (e.g., the recall)\nof Xling.\nVI. E XPERIMENTS\nA. Experiment settings\nEnvironment: All the experiments are executed on a Lambda\nQuad workstation with 28 3.30GHz Intel Core i9-9940X\nCPUs, 4 RTX 2080 Ti GPUs and 128 GB RAM.\nDatasets: Table II provides an overview for our evaluation\ndatasets, reporting their sizes, data dimensions and data types.\nWe introduce more details here:\n1) FastText: 1M word embeddings (300-dimensional) gen-\nerated by fastText model pre-trained on Wikipedia 2017,\nUMBC webbase corpus and statmt.org news dataset.\n\nDataset #Points #Sampled Dim Type\nFastText 1M 150k 300 Text\nGlove 1.2M 150k 200 Text\nWord2vec 3M 150k 300 Text\nGist 1M 150k 960 Image\nSift 1M 150k 128 Image\nNUS-WIDE 270k 150k 500 Image\nTABLE II: Evaluation dataset information, including the num-\nber of total points in the whole dataset ( #Points ), the number\nof sampled points for evaluation ( #Sampled ), data dimension\n(Dim), and the raw data type ( Type).\n2) Glove: 1.2M word vectors (200-dimensional) pre-trained\non tweets.\n3) Word2vec: 3M word embeddings (300-dimensional) pre-\ntrained on Google News dataset.\n4) Gist: 1M GIST image descriptors (960-dimensional).\n5) Sift: 1M SIFT image descriptors (128-dimensional).\n6) NUS-WIDE: 270k bag-of-visual-words vectors (500-\ndimensional) learned on a web image dataset created\nby NUSâ€™s Lab for Media Search.\nDue to computing resource limitation, we randomly sample\n150k vectors from each of them for the evaluation. In the\nrest of this paper, any mentioned dataset name is by default\nmeaning the 150k subset of the corresponding dataset.\nWe then normalize the sampled vectors to unit length\nbecause (1) this makes the distances bounded, i.e., both cosine\nand Euclidean distance are within [0, 2] on unit vectors,\nmaking it easier to determine Ïµ, and (2) some baseline methods\ndo not support cosine distance, in which case we have to\nconvert the cosine Ïµinto equivalent Euclidean Ïµfor them on\nunit vectors, as in our previous work [38]. We then split each\ndataset into training and testing sets by a ratio of 8:2, where\nthe training set acts as Rwhile the testing set is S. Xling is\ntrained on the training set, then all the methods are evaluated\nusing the corresponding testing set as queries.\nLearning models: we use several learning models to evaluate\nthe performance of the optimization strategies in Xling. They\nare introduced as follows. For the deep models, we use\nthe recommended configurations in prior works, while for\nnon-deep models a grid search is executed to find the best\nparameters. Specifically, (1) RMI [28]: We use the same\nconfiguration in [38], i.e., three stages, respectively including\n1, 2, 4 fully-connected neural networks. Each neural network\nhas 4 hidden layers with width 512, 512, 256, and 128. The\nRMI is trained for 200 epochs with batch size 512. (2) NN:\nWe also evaluate the neural network (NN), which is simply\na single sub-model extracted from the RMI above. So all the\nparameters (including the training configuration) are same as\nthat in RMI. (3) SelNet [41]: We use the the same model\nconfiguration as in the paper. (4) XGBoost Regressor (XGB),\nLightGBM Regressor (LGBM) and Support Vector Regressor\n(SVR): These are all non-deep regressors, we do a grid search\nto determine their best parameters.\nSimilarity join baselines: The evaluation baselines include\nboth exact and approximate methods, where the latter are themajor baselines.\n1) Naive: This is a brute-force based nested-loop similarity\njoin, i.e., for each query point in S, do a brute-force\nrange search for it in R. Its results act as the groundtruth\nfor measuring the result quality of all other methods.\n2) SuperEGO2[17]: This is an exact method based on\nEpsilon Grid Ordering (EGO) that sorts the data by a\nspecific order to facilitate the join.\n3) LSH: This is an approximate method using LSH. First\nthe points in Rare mapped into buckets by LSH, then\neach query is mapped to some buckets by the same LSH.\nThe points in those buckets and nearby buckets will then\nbe retrieved as candidates and verified. This method is\nimplemented using FALCONN [42], the state-of-the-art\nLSH library.\n4) KmeansTree: This is an approximate method using K-\nmeans tree, where the tree is built on Rand the space\nis partitioned and represented by sub-trees. Then each\nquery is passed into the tree and a specific sub-tree\n(which represents a sub-space of R) will be inspected to\nfind neighbors within the range. We use FLANN [43],\na widely used high-performance library for tree-based\nsearch, to implement this method.\n5) Naive-LSBF: This is an approximate filter-based method\nthat simply applies LSBF3onto the Naive method, in\nthe same way as the use of Xling in similarity join\n(Section IV-C). We use LSBF instead of the following\nMSBF variants because those variants raise unnecessary\noverhead to support specific extra features.\n6) IVFPQ: we also select an approximate nearest neigh-\nbor (ANN) index as part of the baselines, which is\nthe IVFPQ index [44] in FAISS [45], one of the in-\ndustrial ANN search libraries. Since IVFPQ does not\nsupport range search natively, we evaluate it by first\nsearching for a larger number of nearest candidates,\nthen verifying which candidates are the true neighbors\ngiven Ïµ. And as IVFPQ tends to achieve a high search\nspeed with relatively lower quality (as discussed in our\nprevious work [46]), this baseline does not make much\nsense in the fixed-parameter end-to-end evaluation (Sec-\ntion VI-D). Therefore, we only evaluate it in the trade-\noff (Section VI-E) and generalization (Section VI-F)\nexperiments.\nOur proposed methods: Following the way described in\nSection IV-C, we apply Xling to several base similarity\njoin methods mentioned above. The resulting methods are\nnamed as: (1) XJoin (2) LSH-Xling (3) KmeansTree-Xling\n(4) IVFPQ-Xling. The XJoin is our major proposed method\nthat is evaluated in all the similarity join experiments, while\nthe other proposed methods here are only compared with\nthe corresponding baseline methods to show the enhancement\nbrought by Xling to them. The learned cardinality estimator\nused by Xling in all these methods is an RMI. Note that\n2code available at https://www.ics.uci.edu/ âˆ¼dvk/code/SuperEGO.html\n3code available at https://github.com/csunyy/LSBF\n\nthe goal of this paper is to reveal the potential of such a\nnew framework on speeding up similarity join generically, so\nselecting the best estimation model is out of scope. Given that\nRMI has been used as a strong baseline for learned cardinality\nestimation in [39] and it is not the most state-of-the-art, it is\na fair choice for Xling in the evaluation, especially when we\ncan show that RMI is already good enough to outperform the\nother baselines. We will evaluate Xling with other estimators\nin the future work.\nIn XJoin, the XDT is computed by FPR-based selection\n(introduced in Section V-B) with 5% FPR tolerance, and\nÏ„= 50 , while in LSH-Xling, KmeansTree-Xling and IVFPQ-\nXling, XDT is computed by mean-based selection with Ï„= 0.\nAs discussed in Section V-B, â€œmean-based XDT selection +\nsmaller Ï„â€ leads to higher quality while â€œFPR-based selec-\ntion + larger Ï„â€ results in more acceleration. Since LSH,\nKmeansTree and IVFPQ are approximate methods that sac-\nrifice quality for efficiency, mean-based XDT with lowest Ï„\ncan accelerate them while minimizing the further quality loss.\nFor Naive, the bottleneck is the efficiency, so we choose the\nother configuration for Xling in order to achieve a non-trivial\nspeedup.\nMetrics: The distance metric for text data is cosine distance\nwhile that for image data is Euclidean distance. For the\nbaselines which do not support cosine distance, we follow [38]\nto equivalently convert cosine distance to Euclidean distance.\nThe evaluation metrics include (1) end-to-end join time for\nmeasuring the efficiency of similarity join methods, (2) recall\n(i.e., the ratio of the returned positive results over all the\ngroundtruth positive results) for measuring the similarity join\nresult quality, (3) mean absolute error (MAE) and mean\nsquared error (MSE) for measuring prediction quality of the\nlearned cardinality estimator, and (4) false positive rate (FPR)\nand false negative rate (FNR) for measuring prediction quality\nof Xling.\nEvaluation Ïµ:As we have normalized all the vectors, the\ndistances between them are bounded, i.e., [0, 2] for both cosine\nand Euclidean distances. Since many use cases of similarity\njoin usually choose Ïµfrom the range [0.2, 0.5] [9], [10], [3],\n[47], we do a grid search in this range and determine the\nrepresentative evaluation Ïµvalues: 0.4, 0.45 and 0.5, based\non the portion of negative queries (i.e., the queries having no\nneighbor in R) inS. We set the upper limit for the portion as\n90% as too many negative queries will make the evaluation\nunconvincing. Under the selected Ïµvalues, most of the datasets\n(except NUS-WIDE) have a proper portion that is no more\nthan 90%. Table III reports the portion of negative queries for\neach dataset under each Ïµ.\nB. Optimization strategy evaluation\nIn this section we report and analyze the evaluation results\nof the optimization strategies.\n1) Training Ïµselection: The results of the training Ïµselec-\ntion strategy is reported in Table IV. Due to the space limit,\nwe only present the results for 4 out of 6 datasets.DatasetPortion\n(Ïµ= 0.4)Portion\n(Ïµ= 0.45)Portion\n(Ïµ= 0.5)\nFastText 0.1103 0.0443 0.0116\nGlove 0.8668 0.7851 0.6637\nWord2vec 0.2875 0.1675 0.0803\nGist 0.8442 0.3939 0.1027\nSift 0.5578 0.3494 0.1531\nNUS-WIDE 0.9743 0.9653 0.9544\nTABLE III: The portion of negative queries for each dataset\nunder each Ïµ\nFollowing Definition 4, to make it simple, we have the\nset of candidate condition values shared by all the training\npoints, i.e., the set {ci1, ci2, ..., c im}is same for any training\npoint pi. We let m= 100 and construct such a set by\nevenly sampling 100 values from a range [ cmin, cmax], where\nwe set cmin = 0.4, cmax = 0.9for cosine distance while\ncmin = 0.5, cmax = 2.0for Euclidean distance. These cmin\nandcmax values are selected based on a grid search given\nthe evaluation experience from our previous work [38]. Then\nwe set the sampling number s= 6, i.e., for each training\npoint, 6 distinct condition values will be sampled from the\n100 candidate values and become the final training Ïµvalues\nto be paired with the point in the prepared training data. Two\nsampling strategies are deployed to select the 6 values: (1)\nuniform sampling, e.g., selecting the 1st, 20th, 40th, 60th,\n80th and 100th values (2) our ATCS strategy (Algorithm1).\nThe former is presented as â€œfixedâ€ while the latter is marked\nas â€œautoâ€ in the Strategy columns of Table IV.\nThe regression models (i.e., the learning models listed in\nSection VI-A) will first be trained respectively using the two\nprepared training sets from different strategies, then they will\ndo inference on some prepared testing sets and the inference\nquality is measured by MAE and MSE, which can reflect\nthe training effectiveness. For a fair testing, we generate two\nkinds of prepared testing sets: (1) the testing points with\nrandomly selected Ïµvalues from the 100 candidates mentioned\nabove, and (2) the same testing points with uniformly selected\nÏµvalues from the candidates. The inference quality on the\nformer set is reported in the Random Testing Ïµcolumns while\nthat on the latter is reported in the Uniform Testing Ïµcolumns\nof Table IV. And for each learning model, the Strategy which\nresults in better inference quality (i.e., lower MAE and MSE)\nis highlighted by bold text.\nIn most cases, the â€œautoâ€ strategy (i.e., our ATCS strategy)\nachieves a better inference quality (e.g., reducing up to 98%\nof the MSE on NUS-WIDE dataset) than uniform training Ïµ,\nmeaning that ATCS strategy is generic to facilitate various\nkinds of regression models and highly effective to raise a\nsignificant improvement on the prediction quality. Therefore,\nfor all the following experiments in this paper, the training\nsets are prepared by ATCS.\n2) XDT selection: As mentioned in Section V-B, XDT\nis influenced by three factors: (1) Ï„, (2) the XDT selection\nmethod (mean-based or FPR-based), and (3) the way to\nget training targets for out-of-domain Ïµ(interpolation-based\n\nRandom Testing Ïµ Uniform Testing ÏµModel StrategyMAE MSE MAE MSE\n4.04 6.69 2.57 2.68 XGB fixed\n2.54 1.32 2.54 1.26 XGB auto\n3.66 6.65 2.14 2.57 LGBM fixed\n1.8 0.83 1.79 0.81 LGBM auto\n23.79 97.16 27.55 127.15 SVR fixed\n22.26 81.06 25.34 106.22 SVR auto\n96.13 24711.7 96.44 28299.85 SelNet fixed\n93.8 22228.79 94.39 25737.67 SelNet auto\n2.08 1.39 1.69 1.00 NN fixed\n0.39 0.03 0.44 0.05 NN auto\n2.55 5.72 1.52 2.18 RMI fixed\n0.20 0.01 0.19 0.01 RMI auto\n(a) SiftRandom Testing Ïµ Uniform Testing ÏµModel StrategyMAE MSE MAE MSE\n5.32 22.84 7.81 48.16 XGB fixed\n5.02 24.09 7.89 50.7 XGB auto\n5.29 20.46 7.45 43.82 LGBM fixed\n4.49 19.61 7.1 43.52 LGBM auto\n9.01 46.41 12.6 84.58 SVR fixed\n8.7 50.18 12.32 90.46 SVR auto\n8.91 54.69 12.57 97.23 SelNet fixed\n8.91 54.7 12.57 97.24 SelNet auto\n4.38 18.87 6.78 41.95 NN fixed\n3.33 11.85 5.09 23.19 NN auto\n4.52 19.44 6.85 42.57 RMI fixed\n3.43 13.84 5.35 27.51 RMI auto\n(b) Word2Vec\nRandom Testing Ïµ Uniform Testing ÏµModel StrategyMAE MSE MAE MSE\n4.69 5.49 5.02 5.36 XGB fixed\n2.53 1.43 2.55 1.49 XGB auto\n4.7 5.49 4.76 4.94 LGBM fixed\n2.45 1.28 2.47 1.33 LGBM auto\n27.61 141.55 29.89 169.73 SVR fixed\n23.45 81.06 24.5 94.06 SVR auto\n36.19 357.57 38.14 387.45 SelNet fixed\n36.19 357.52 38.14 387.39 SelNet auto\n0.54 0.14 0.79 0.36 NN fixed\n0.32 0.03 0.38 0.05 NN auto\n2.09 2.23 2.80 4.49 RMI fixed\n0.67 0.18 0.77 0.24 RMI auto\n(c) FastTextRandom Testing Ïµ Uniform Testing ÏµModel StrategyMAE MSE MAE MSE\n4.74 14.99 3.4 6.35 XGB fixed\n3.36 2.62 3.37 2.69 XGB auto\n4.23 15.29 2.89 6.4 LGBM fixed\n2.4 2.6 2.36 2.46 LGBM auto\n22.81 70.71 24.6 80.33 SVR fixed\n25.22 99.4 25.06 97.04 SVR auto\n82.26 8655.88 82.62 9886.03 SelNet fixed\n75.96 6560.3 75.93 7490.16 SelNet auto\n3.39 10.56 2.51 5.01 NN fixed\n0.83 0.20 1.04 0.33 NN auto\n4.24 19.66 2.49 6.60 RMI fixed\n0.48 0.27 0.39 0.13 RMI auto\n(d) NUS-WIDE\nTABLE IV: Effectiveness of the two training Ïµselection strategies for different regressors trained on different datasets, where\nMAE columns show the original numbers multiplied by 10âˆ’3andMSE columns show the original numbers multiplied by 10âˆ’7\nDataset Model Ïµ XDT SelectionApproximate Targets Exact Targets\nFPR FNR Time(s) XDT FPR FNR Time(s) XDT\nGloveXGB0.4mean 0.4948 0.3982 1.3618 -8.45 0.4948 0.3982 1037.9202 -8.45\nFPR 0.0562 0.8578 1.3859 406.83 0.0562 0.8578 1037.9278 406.83\n0.45mean 0.5019 0.4247 2.5836 -10.66 0.4996 0.4259 1038.6554 -9.28\nFPR 0.0608 0.8737 2.5499 392.43 0.0548 0.882 1038.6161 405.70\n0.5mean 0.4989 0.4373 2.51 -12.56 0.5004 0.4358 1038.3489 -13.50\nFPR 0.0602 0.8903 2.8515 387.51 0.0566 0.8946 1038.379 396.47\nRMI0.4mean 0.4071 0.2506 10.0679 4.38 0.4071 0.2506 1046.1885 4.38\nFPR 0.0501 0.5049 9.9112 6.46 0.0501 0.5049 1046.1892 6.46\n0.45mean 0.4496 0.2308 10.9286 4.57 0.3537 0.2702 1046.9525 4.76\nFPR 0.0808 0.4902 11.1269 6.48 0.0506 0.5385 1047.0098 7.37\n0.5mean 0.3613 0.2546 10.8683 5.15 0.3043 0.284 1046.6399 5.34\nFPR 0.0709 0.5041 11.1455 7.99 0.0518 0.5424 1046.5916 8.82\nNUS-WIDEXGB0.4mean 0.5041 0.2202 1.4501 -3.31 0.5011 0.2215 2638.2846 15.41\nFPR 0.0555 0.6554 1.5899 3424.20 0.0524 0.6645 2638.2916 3499.68\n0.45mean 0.5024 0.2428 1.4116 -3.31 0.5008 0.2428 2655.1167 5.55\nFPR 0.0535 0.6727 1.4341 3424.20 0.0519 0.6756 2655.1188 3463.20\n0.5mean 0.5009 0.2719 1.8672 -3.31 0.5009 0.2719 2651.2069 -3.31\nFPR 0.0516 0.6981 2.0873 3424.20 0.0516 0.6981 2651.1754 3424.20\nRMI0.4mean 0.3672 0.057 10.6236 2.86 0.3537 0.0596 2647.5025 2.99\nFPR 0.0534 0.0894 11.0576 9.87 0.048 0.092 2647.5929 10.45\n0.45mean 0.3301 0.071 10.4377 2.43 0.3216 0.071 2664.379 2.52\nFPR 0.0504 0.1123 10.8119 9.55 0.0463 0.1161 2664.2925 9.95\n0.5mean 0.2872 0.087 11.3239 2.18 0.2872 0.087 2660.6731 2.18\nFPR 0.0467 0.1294 11.3493 9.54 0.0467 0.1294 2660.3748 9.54\nTABLE V: Prediction quality (FPR and FNR) of Xling when XDT is computed in different ways (while fixing Ï„= 0).\nThere are two dimensions for a way to compute XDT: (1) using mean-based or FPR-based XDT selection method (2) using\ninterpolation-based method to approximate the targets or naive method to compute the exact targets. The time to compute the\ntargets is also presented ( Time ). Results on other datasets are similar to Glove and NUS-WIDE. Note that we allow XDT to\nbe less than zero.\napproximate targets or naively computed exact targets). In this\nsection we evaluate the last two factors due to page limit.\nSpecifically, here we fix Ï„= 0. For each dataset, the learned\ncardinality estimator of Xling is first trained using the training\nset, then we vary the setting for the two factors, compute XDT\nunder each setting, and measure the FPR and FNR of Xling\non the testing set of the current dataset, as well as the time\nfor computing the training targets.\nDue to space limit, Table V reports the results for two\nrepresentative models (i.e., XGB for non-deep and RMI for\ndeep model) on two datasets Glove (text) and NUS-WIDE\n(image). For the factor of target computing, the results show\nthat the two target computing methods usually lead to sim-\nilar FPR and FNR, while our proposed interpolation-based\ntarget approximation is around 100x âˆ¼2000x faster than\nthe naive target computing, meaning that interpolation-basedtarget approximation is both effective and highly efficient. For\nthe factor of XDT selection, as mentioned in Section V-B,\nthe results present that FPR-based XDT selection usually\ngenerates a higher XDT than mean-based, thereby the former\ntends to determine more queries as negative and causes lower\nFPR and higher FNR than the latter. So in the end-to-end\nsimilarity join, using FPR-based XDT selection will lead to\nhigher speedup but lower quality than mean-based, which\nsupports the statements in Section V-B.\nC. Filter effectiveness evaluation\nIn this section we evaluate the prediction quality between\nXling and LSBF, reported by FPR and FNR of their predictions\nfor the testing sets. We fix Ï„= 0 as LSBF does not support\nthe cases where Ï„ > 0.\n\nThe results are included in Table VI. In addition to FPR and\nFNR, the table also includes the total number of true neighbors\nfound for all the testing queries ( #Nbrs ) by Naive-LSBF and\nXJoin, the number of predicted positive queries ( #PPQ ) which\nare the query points predicted as positive by the filter, and\nthe average number of neighbors per predicted positive query\n(#ANPQ ), i.e., #ANPQ = #Nbrs/#PPQ. We have the following\nobservations: (1) In most cases, Xling with mean-based XDT\nhas both lower FPR and FNR than LSBF, meaning it is more\neffective than LSBF. (2) In many cases, Xling with FPR-based\nXDT performs similarly to the observation above, while in\nsome other cases, it has higher FNR than LSBF. However, even\nwith a higher FNR, it still finds more true neighbors in those\ncases than LSBF, e.g., the case of Word2vec under Ïµ= 0.5.\nThe reason is that the queries predicted as positive by Xling\noverall have more neighbors than those by LSBF (reflected by\n#ANPQ), due to which Xling can find more true results with\nless range search (i.e., the #PPQ of FPR-based Xling is usually\nless than LSBF). In short, Xling learns the data distribution\nand makes predictions based on the data density, such that it\ncan maximize the recall with minimized number of search,\nwhich is a huge advantage over the data-unaware LSBF.\nD. End-to-end evaluation\nIn end-to-end evaluation, the key parameters for the methods\nare fixed as such: (1) Naive and SuperEGO have no user-\nspecific parameters. (2) In LSH, the number of hash tables\nl= 10 , number of has functions k= 18 , the number of hash\nbuckets to be inspected in each table np= 40 for text datasets\nwhile np= 2 for image datasets. (3) In KmeansTree, the\nbranching factor is fixed to be 3, the portion of leaves to be\ninspected Ï= 0.02for text datasets while Ï= 0.012for image\ndatasets. (4) For Naive-LSBF, its LSH-related parameters k\nandlare the same as the method LSH, length of the bit\narray in LSBF is fixed to be 2,160,000 (i.e., |R| Ã—k), the\nparameter Win the p-stable LSH functions is set to be 2.5\nfor text datasets while W= 2 for image datasets. (5) For\nIVFPQ, the parameters for building the index are C= 300 ,\nm= 32 orm= 25 when the data dimension is not an integer\nmultiple of 32, b= 8,p= 50 , where Cis the number of\nclusters, mis the number of segments into which each data\nvector will be split, bis the number of bits used to encode the\nclsuter centroids, and pis the number of nearest clusters to be\ninspected during search. In our implementation, this baseline\nfirst uses the IVFPQ index to select 1000 nearest neighbors\nthen verifies them given Ïµ. The number 1000 is determined\nsince for all datasets, at least 50% testing queries have less than\n1000 neighbors within Ïµ, and in most datasets 1000 candidates\nare enough to find all correct neighbors for 70% âˆ¼90%\ntesting queries. (6) The configuration of Xling is introduced\nin Section VI-A, while the base methods in SuperEGO-Xling,\nLSH-Xling and KmeansTree-Xling use the same parameters\nas above.\nThe results are illustrated in Figure 2. SuperEGO has\nunknown bugs that prevent it from running on FastText and\nNUS-WIDE datasets, so Figure 2 does not include SuperEGOfor these two datasets. SuperEGO relies on multi-threading\nfor acceleration, so on some datasets its running time is even\nlonger than the Naive method since we only use one thread\nto evaluate all the methods. The results show that our method\nXJoin has a higher speed (presented by the red bar) than all\nthe baseline methods, as well as higher quality (reported by\nthe red line) than the other approximate methods (i.e., LSH,\nKmeansTree and Naive-LSBF) in most cases. The recall of\nNaive and SuperEGO is always 1 as they are exact methods.\nSpecifically, XJoin achieves up to 17x and 14.6x speedup\nrespectively compared to the exact methods and approximate\nmethods while guaranteeing a high quality. The results prove\nthe high efficiency and effectiveness of our proposed method.\nE. Speed-quality trade-off evaluation\nIn this evaluation, We fix the dataset and Ïµ, then vary some\nkey parameters of the approximate baselines and XJoin, i.e.,\nnpin LSH, the branching factor and the ratio Ïin KmeansTree,\nWand bit array length in Naive-LSBF, Candpin IVFPQ, and\nthe XDT selection mode (mean or FPR based) and Ï„in XJoin.\nFor LSH and KmeansTree, we always set the parameters of\ntheir Xling-enhanced versions the same as the original version,\nin order to make the comparison between them fair. The varied\nparameters result in varied end-to-end similarity join time and\nquality, based on which we get the speed-quality trade-off\ncurves, as illustrated in Figure 3.\nWe select three cases to present in Figure 3: the datasets\nGlove, Word2vec and Gist with Ïµ= 0.45, and other cases are\nsimilar to them. According to the results, we conclude that (1)\nXJoin has better trade-off capability than the original version\nof the approximate baselines, i.e., LSH, IVFPQ, KmeansTree\nand Naive-LSBF, meaning that XJoin can achieve high quality\nwith minimum processing time, or sacrifice tiny quality for\nsignificant efficiency gain. (2) The Xling-enhanced versions\nof the approximate baselines (i.e., LSH-Xling, IVFPQ-Xling\nand KmeansTree-Xling) have significant better trade-off ca-\npability than the original versions, which also means Xling\nsuccessfully accelerates the original versions with a relatively\nnegligible quality loss. This demonstrates the generality and\nusefulness of Xling to further enhance the existing similarity\njoin methods and make them more practical.\nF . Generalization evaluation\nIn this section we evaluate the generalization capability of\nXling and XJoin. Another 150k dataset is sampled for each\noriginal dataset. We call the first 150k datasets used in previous\nexperiments â€œthe first 150kâ€ or add â€œ-1stâ€ after the dataset\nname, while call the new dataset â€œthe second 150kâ€ or add\nâ€œ-2ndâ€ after the name. The first and second 150k have no\noverlap, except NUS-WIDE.\nWe first evaluate the trade-off capabilities of XJoin and\nXling-enhanced methods again on the second 150k. All Xlings\nare those trained on the first 150k and used in the previous\nexperiments. We do not re-train them anymore for the second\n150k. As shown in Figure 4, all those methods present a\nsimilar performance and trends as in Figure 3, which prove\n\nDataset Ïµ Filter FPR FNR#Nbrs\n(Ã—105)#PPQ\n(Ã—104)#ANPQ\nFastText0.4LSBF 0.46 0.2908 127.73 2.05 624.58\nXling(mean) 0.2523 0.1114 142.15 2.46 578.95\nXling(FPR) 0.0529 0.207 141.90 2.13 664.94\n0.45LSBF 0.5124 0.2471 296.05 2.23 1329.56\nXling(mean) 0.3469 0.0682 325.25 2.72 1196.82\nXling(FPR) 0.0948 0.1295 325.07 2.51 1295.92\n0.5LSBF 0.5244 0.2062 708.64 2.37 2987.66\nXling(mean) 0.3582 0.0376 767.57 2.87 2678.19\nXling(FPR) 0.0802 0.099 767.23 2.68 2868.68\nWord2vec0.4LSBF 0.4952 0.4607 3.49 1.58 22.06\nXling(mean) 0.374 0.3197 4.63 1.78 26.08\nXling(FPR) 0.0502 0.7028 4.20 0.68 61.81\n0.45LSBF 0.5516 0.3977 6.77 1.78 37.98\nXling(mean) 0.4397 0.234 8.86 2.13 41.53\nXling(FPR) 0.0804 0.5996 8.19 1.04 78.70\n0.5LSBF 0.5943 0.3419 13.04 1.96 66.56\nXling(mean) 0.3019 0.2725 16.40 2.08 78.83\nXling(FPR) 0.039 0.5882 15.15 1.15 132.22\n(a) Text datasetsDataset Ïµ Filter FPR FNR#Nbrs\n(Ã—105)#PPQ\n(Ã—104)#ANPQ\nSift0.4LSBF 0.2415 0.6609 4.69 0.85 54.89\nXling(mean) 0.06 0.6119 7.18 0.62 116.65\nXling(FPR) 0.0351 0.6771 7.08 0.49 145.37\n0.45LSBF 0.2338 0.6358 13.04 0.96 136.42\nXling(mean) 0.081 0.4565 19.49 1.15 170.09\nXling(FPR) 0.0537 0.5064 19.39 1.02 190.19\n0.5LSBF 0.2053 0.6139 35.54 1.08 330.55\nXling(mean) 0.1798 0.237 52.83 2.02 261.36\nXling(FPR) 0.0581 0.3678 52.50 1.63 321.55\nNUS-WIDE0.4LSBF 0.2492 0.5687 1.39 0.76 18.19\nXling(mean) 0.3671 0.057 2.14 1.15 18.65\nXling(FPR) 0.0534 0.0894 2.14 0.23 94.32\n0.45LSBF 0.2498 0.5374 2.82 0.77 36.53\nXling(mean) 0.3302 0.071 4.06 1.05 38.56\nXling(FPR) 0.0503 0.1123 4.06 0.24 170.27\n0.5LSBF 0.2469 0.5029 5.15 0.78 66.42\nXling(mean) 0.2873 0.087 6.98 0.95 73.69\nXling(FPR) 0.0467 0.1294 6.98 0.25 276.09\n(b) Image datasets\nTABLE VI: The prediction quality of LSBF and Xling (mean-based or FPR-based) on different datasets, where #Nbrs is the\ntotal number of returned neighbors for all the queries, #PPQ stands for the number of Predicted P ositive Q ueries , i.e., the\nquery points predicted as positive by the filter, and #ANPQ presents the Average number of N eighbors per predicted P ositive\nQuery that equals #Nbrs over #PPQ. Due to space limit, the results on Glove and Gist datasets are hidden, which are similar\nto the other four.\n00.10.20.30.40.50.60.70.80.910100200300400500600\n0.4 0.45 0.5\nRecallTime (s)Query ÎµNaive SuperEGO LSH KmeansTree Naive-LSBF XJoin\nNaive(recall) SuperEGO(recall) LSH(recall) KmeansTree(recall) Naive-LSBF(recall) XJoin(recall)\n00.10.20.30.40.50.60.70.80.91\n050100150200250300350400450\n0.4 0.45 0.5\nRecallTimeÂ (s)\nQueryÂ Îµ\nFastText\n00.10.20.30.40.50.60.70.80.91\n050100150200250300350400450\n0.4 0.45 0.5\nRecallTimeÂ (s)\nQueryÂ Îµ Glove\n00.10.20.30.40.50.60.70.80.91\n0100200300400500600\n0.4 0.45 0.5\nRecallTimeÂ (s)\nQueryÂ Îµ Word2vec\n00.10.20.30.40.50.60.70.80.91\n02004006008001000120014001600\n0.4 0.45 0.5\nRecallTimeÂ (s)\nQueryÂ Îµ\nGist\n00.10.20.30.40.50.60.70.80.91\n050100150200250\n0.4 0.45 0.5\nRecallTimeÂ (s)\nQueryÂ Îµ Sift\n00.10.20.30.40.50.60.70.80.91\n0100200300400500600700800900\n0.4 0.45 0.5\nRecallTimeÂ (s)\nQueryÂ Îµ NUS-WIDE\nFig. 2: End-to-end query processing time and recall for all the similarity join methods on all datasets, where the figures of\nFastText and NUS-WIDE do not include SuperEGO, as it cannot run on these two datasets.\nour statement in the Introduction that Xling and XJoin have\ngreat generalization capability thanks to the learning model,\nand therefore it is not necessary to re-train Xling when the\ndata is updated or even replaced with a fully new dataset, as\nlong as the new data has similar distribution to the old.\nTo have a further quantitative view about the generalization,\nwe also compare the speed improvement and the recall loss\nmade by Xling when attaching it to the base similarity\njoin methods. In Figure 5, within each method (marked as\nâ€œIVFPQâ€, â€œLSHâ€, etc.), the first bar (solid and blue, â€œ1st\ntimeâ€) and second bar (solid and orange, â€œ1st Xling timeâ€)\nare the end-to-end running time of the original method and its\nXling-enhanced version on the first 150k, while the third bar\n(diagonal lines and blue, â€œ2nd timeâ€) and fourth bar (diagonallines and orange, â€œ2nd Xling timeâ€) are the two running time\non the second 150k. The green and red lines are the percentage\nrecall loss respectively on the first and second 150k, i.e., the\ndifference between recall of original method and enhanced\nversion over original recall on each dataset. The results show\nthat neither time improvement nor recall loss has a significant\ndifference between first and second 150k, which further prove\nour methods have outstanding generalization, meaning that\nthey are practical in real world.\nVII. C ONCLUSION\nIn this paper we propose Xling, a generic framework of\nlearned metric space Bloom filters for speeding up similarity\njoin with quality guarantee based on machine learning. Based\n\n050100150200250300350400\n0 0.5 1Time (s)Recall (Îµ= 0.45 )KmeansTree KmeansTree-Xling LSH LSH-Xling IVFPQ IVFPQ-Xling Naive-LSBF XJoin\n050100150200250300\n0 0.2 0.4 0.6 0.8 1\nTimeÂ (s)\nRecallÂ (Îµ=Â 0.45)Glove\n050100150200250300350400\n0 0.2 0.4 0.6 0.8 1\nTimeÂ (s)\nRecallÂ (Îµ=Â 0.45) Word2vec\n0200400600800100012001400\n0 0.2 0.4 0.6 0.8 1\nTimeÂ (s)\nRecallÂ (Îµ=Â 0.45) Gist\nFig. 3: Speed-quality trade-off curves for XJoin, the approximate methods and their Xling-enhanced versions on the selected\ndatasets and Ïµ, and other cases are also similar.\n050100150200250300350400\n0 0.5 1Time (s)Recall (Îµ= 0.45 )KmeansTree KmeansTree-Xling LSH LSH-Xling IVFPQ IVFPQ-Xling Naive-LSBF XJoin\n050100150200250300350400\n0 0.2 0.4 0.6 0.8 1Time (s)\nRecall (Îµ= 0.45 )\nGlove-2nd\n050100150200250300350400450\n0 0.2 0.4 0.6 0.8 1Time (s)\nRecall (Îµ= 0.45 ) Word2vec-2nd\n030060090012001500\n0 0.2 0.4 0.6 0.8 1Time (s)\nRecall (Îµ= 0.45 ) Gist-2nd\nFig. 4: Speed-quality trade-off curves for XJoin, the approximate methods and their Xling-enhanced versions on the second\n150k datasets, where all Xlings are pre-trained on the original 150k dataset without re-training for the second\n0246810121416180200400600800100012001400\nIVFPQ\nRecall loss (%)TIme(s)1st time 1st Xling time 2nd time 2nd Xling time 1st recall loss (%) 2nd recall loss (%)\nRecall (Îµ= 0.45)\n01234567\n050100150200250300\nIVFPQ LSH KmeansTree Naive\nRecall loss (%)Time(s)\nRecall (Îµ= 0.45)\nGlove 1st vs 2nd\n02468101214161820\n050100150200250300350400\nIVFPQ LSH KmeansTree Naive\nRecall loss (%)Time(s)\nRecall (Îµ= 0.45) Word2vec 1st vs 2nd\n024681012141618\n0200400600800100012001400\nIVFPQ LSH KmeansTree Naive\nRecall loss (%)Time(s)\nRecall (Îµ= 0.45) Gist 1st vs 2nd\nFig. 5: The differences of acceleration and recall loss resulting from Xling on the first and second 150k datasets\non Xling we develop an efficient and effective similarity join\nmethod that outperforms the state-of-the-art methods on both\nspeed and quality, as well as having a better speed-quality\ntrade-off capability and generalization capability. We also\napply Xling onto those state-of-the-art methods to significantly\nfurther enhance them. Xling has shown the great potential in\neffectively speeding up a wide range of existing similarity join\nmethods.\nREFERENCES\n[1] B. Gyawali, L. Anastasiou, and P. Knoth, â€œDeduplication of\nscholarly documents using locality sensitive hashing and word\nembeddings,â€ in Proceedings of the Twelfth Language Resources\nand Evaluation Conference . Marseille, France: European Language\nResources Association, May 2020, pp. 901â€“910. [Online]. Available:\nhttps://aclanthology.org/2020.lrec-1.113[2] C. Yang, D. H. Hoang, T. Mikolov, and J. Han, â€œPlace deduplication\nwith embeddings,â€ in The World Wide Web Conference , 2019, pp. 3420â€“\n3426.\n[3] H. B. da Silva, Z. K. do Patroc Â´Ä±nio, G. Gravier, L. Amsaleg, A. d. A.\nAraÂ´ujo, and S. J. F. Guimaraes, â€œNear-duplicate video detection based on\nan approximate similarity self-join strategy,â€ in 2016 14th International\nWorkshop on Content-Based Multimedia Indexing (CBMI) . IEEE, 2016,\npp. 1â€“6.\n[4] L. Zhou, J. Chen, A. Das, H. Min, L. Yu, M. Zhao, and J. Zou,\nâ€œServing deep learning models with deduplication from relational\ndatabases,â€ Proceedings of the VLDB Endowment , vol. 15, no. 10, p.\n2230â€“2243, Jun. 2022. [Online]. Available: http://dx.doi.org/10.14778/\n3547305.3547325\n[5] R. Sarwar, C. Yu, N. Tungare, K. Chitavisutthivong, S. Sriratanawilai,\nY . Xu, D. Chow, T. Rakthanmanon, and S. Nutanong, â€œAn effective and\nscalable framework for authorship attribution query processing,â€ IEEE\nAccess , vol. 6, pp. 50 030â€“50 048, 2018.\n[6] B. H Â¨attasch, M. Truong-Ngoc, A. Schmidt, and C. Binnig, â€œItâ€™s ai\nmatch: A two-step approach for schema matching using embeddings,â€\n\n2022. [Online]. Available: https://arxiv.org/abs/2203.04366\n[7] N. Adly, â€œEfficient record linkage using a double embedding scheme.â€\ninDMIN , 2009, pp. 274â€“281.\n[8] S. Herath, M. Roughan, and G. Glonek, â€œEm-k indexing for approximate\nquery matching in large-scale er,â€ 2021.\n[9] F. Nargesian, E. Zhu, K. Q. Pu, and R. J. Miller, â€œTable union search\non open data,â€ Proc. VLDB Endow. , vol. 11, no. 7, p. 813â€“825, mar\n2018. [Online]. Available: https://doi.org/10.14778/3192965.3192973\n[10] A. Berenguer, J.-N. Maz Â´on, and D. Tom Â´as, â€œTowards a tabular open data\nsearch engine for public sector information,â€ in 2021 IEEE International\nConference on Big Data (Big Data) , 2021, pp. 5851â€“5853.\n[11] Y . Dong, K. Takeoka, C. Xiao, and M. Oyamada, â€œEfficient joinable\ntable discovery in data lakes: A high-dimensional similarity-based\napproach,â€ in 2021 IEEE 37th International Conference on Data En-\ngineering (ICDE) . IEEE, 2021, pp. 456â€“467.\n[12] X. Yuan, X. Wang, C. Wang, C. Yu, and S. Nutanong, â€œPrivacy-\npreserving similarity joins over encrypted data,â€ IEEE Transactions on\nInformation Forensics and Security , vol. 12, no. 11, pp. 2763â€“2775,\n2017.\n[13] J. Yao, X. Meng, Y . Zheng, and C. Wang, â€œPrivacy-preserving content-\nbased similarity detection over in-the-cloud middleboxes,â€ IEEE Trans-\nactions on Cloud Computing , vol. 11, no. 2, pp. 1854â€“1870, 2023.\n[14] M. Perdacher, C. Plant, and C. B Â¨ohm, â€œCache-oblivious high-\nperformance similarity join,â€ in Proceedings of the 2019 International\nConference on Management of Data , ser. SIGMOD â€™19. New York,\nNY , USA: Association for Computing Machinery, 2019, p. 87â€“104.\n[Online]. Available: https://doi.org/10.1145/3299869.3319859\n[15] C. B Â¨ohm, B. Braunm Â¨uller, F. Krebs, and H.-P. Kriegel, â€œEpsilon grid\norder: An algorithm for the similarity join on massive high-dimensional\ndata,â€ in Proceedings of the 2001 ACM SIGMOD International\nConference on Management of Data , ser. SIGMOD â€™01. New York,\nNY , USA: Association for Computing Machinery, 2001, p. 379â€“388.\n[Online]. Available: https://doi.org/10.1145/375663.375714\n[16] D. V . Kalashnikov and S. Prabhakar, â€œFast similarity join for\nmulti-dimensional data,â€ Information Systems , vol. 32, no. 1, pp.\n160â€“177, 2007. [Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S0306437905000761\n[17] D. V . Kalashnikov, â€œSuper-ego: fast multi-dimensional similarity join,â€\nThe VLDB Journal , vol. 22, no. 4, pp. 561â€“585, 2013.\n[18] C. Yu, S. Nutanong, H. Li, C. Wang, and X. Yuan, â€œA generic method for\naccelerating lsh-based similarity join processing (extended abstract),â€ in\n2017 IEEE 33rd International Conference on Data Engineering (ICDE) ,\n2017, pp. 29â€“30.\n[19] H. Li, S. Nutanong, H. Xu, c. YU, and F. Ha, â€œC2net: A network-efficient\napproach to collision counting lsh similarity join,â€ IEEE Transactions\non Knowledge and Data Engineering , vol. 31, no. 3, pp. 423â€“436, 2019.\n[20] Z. Yang, W. T. Ooi, and Q. Sun, â€œHierarchical, non-uniform locality\nsensitive hashing and its application to video identification,â€ in 2004\nIEEE International Conference on Multimedia and Expo (ICME) (IEEE\nCat. No.04TH8763) , vol. 1, 2004, pp. 743â€“746 V ol.1.\n[21] Y . Hua, B. Xiao, B. Veeravalli, and D. Feng, â€œLocality-sensitive bloom\nfilter for approximate membership query,â€ IEEE Transactions on Com-\nputers , vol. 61, no. 6, pp. 817â€“830, 2012.\n[22] J. Qian, Q. Zhu, and H. Chen, â€œInteger-granularity locality-sensitive\nbloom filter,â€ IEEE Communications Letters , vol. 20, no. 11, pp. 2125â€“\n2128, 2016.\n[23] M. Goswami, R. Pagh, F. Silvestri, and J. Sivertsen, â€œDistance sensitive\nbloom filters without false negatives,â€ 2016.\n[24] J. Qian, Z. Huang, Q. Zhu, and H. Chen, â€œHamming metric\nmulti-granularity locality-sensitive bloom filter,â€ IEEE/ACM Trans.\nNetw. , vol. 26, no. 4, p. 1660â€“1673, aug 2018. [Online]. Available:\nhttps://doi.org/10.1109/TNET.2018.2850536\n[25] J. Qian, Q. Zhu, and H. Chen, â€œMulti-granularity locality-sensitive\nbloom filter,â€ IEEE Transactions on Computers , vol. 64, no. 12, pp.\n3500â€“3514, 2015.\n[26] A. Kirsch and M. Mitzenmacher, â€œDistance-sensitive bloom filters,â€ in\n2006 Proceedings of the Eighth Workshop on Algorithm Engineering\nand Experiments (ALENEX) . SIAM, 2006, pp. 41â€“50.\n[27] Y . Hua, X. Liu, Y . Hua, and X. Liu, â€œLocality-sensitive bloom filter for\napproximate membership query,â€ Searchable Storage in Cloud Comput-\ning, pp. 99â€“127, 2019.\n[28] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, â€œThe case\nfor learned index structures,â€ 2018.[29] S. Macke, A. Beutel, T. Kraska, M. Sathiamoorthy, D. Z. Cheng, and\nE. H. Chi, â€œLifting the curse of multidimensional data with learned\nexistence indexes,â€ in Workshop on ML for Systems at NeurIPS , 2018,\npp. 1â€“6.\n[30] M. Mitzenmacher, â€œA model for learned bloom filters and optimizing\nby sandwiching,â€ Advances in Neural Information Processing Systems ,\nvol. 31, 2018.\n[31] A. Bhattacharya, C. Gudesa, A. Bagchi, and S. Bedathur, â€œNew wine\nin an old bottle: Data-aware hash functions for bloom filters,â€ Proc.\nVLDB Endow. , vol. 15, no. 9, p. 1924â€“1936, may 2022. [Online].\nAvailable: https://doi.org/10.14778/3538598.3538613\n[32] J. Sun, G. Li, and N. Tang, â€œLearned cardinality estimation for similarity\nqueries,â€ in Proceedings of the 2021 International Conference on\nManagement of Data , 2021, pp. 1745â€“1757.\n[33] Y . Wang, C. Xiao, J. Qin, X. Cao, Y . Sun, W. Wang, and M. Onizuka,\nâ€œMonotonic cardinality estimation of similarity selection: A deep learn-\ning approach,â€ in Proceedings of the 2020 ACM SIGMOD International\nConference on Management of Data , 2020, pp. 1197â€“1212.\n[34] J. Qin, W. Wang, C. Xiao, Y . Zhang, and Y . Wang, â€œHigh-dimensional\nsimilarity query processing for data science,â€ in Proceedings of\nthe 27th ACM SIGKDD Conference on Knowledge Discovery and\nData Mining , ser. KDD â€™21. New York, NY , USA: Association\nfor Computing Machinery, 2021, p. 4062â€“4063. [Online]. Available:\nhttps://doi.org/10.1145/3447548.3470811\n[35] Y . Wang, C. Xiao, J. Qin, R. Mao, M. Onizuka, W. Wang, R. Zhang,\nand Y . Ishikawa, â€œConsistent and flexible selectivity estimation for high-\ndimensional data,â€ in Proceedings of the 2021 International Conference\non Management of Data , 2021, pp. 2319â€“2327.\n[36] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, â€œThe case\nfor learned index structures,â€ in Proceedings of the 2018 international\nconference on management of data , 2018, pp. 489â€“504.\n[37] H. Zhang and Q. Zhang, â€œEmbedjoin: Efficient edit similarity joins via\nembeddings,â€ in Proceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , ser. KDD â€™17.\nNew York, NY , USA: Association for Computing Machinery, 2017, p.\n585â€“594. [Online]. Available: https://doi.org/10.1145/3097983.3098003\n[38] Y . Wang and D. Z. Wang, â€œLearned accelerator framework for angular-\ndistance-based high-dimensional dbscan,â€ 2023.\n[39] Y . Wang, C. Xiao, J. Qin, X. Cao, Y . Sun, W. Wang, and M. Onizuka,\nâ€œMonotonic cardinality estimation of similarity selection: A deep\nlearning approach,â€ in Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data , ser. SIGMOD â€™20.\nNew York, NY , USA: Association for Computing Machinery, 2020,\np. 1197â€“1212. [Online]. Available: https://doi.org/10.1145/3318464.\n3380570\n[40] S. Macke, A. Beutel, T. Kraska, M. Sathiamoorthy, D. Z. Cheng, and\nE. H. Chi, â€œLifting the curse of multidimensional data with learned\nexistence indexes,â€ 2018.\n[41] Y . Wang, C. Xiao, J. Qin, R. Mao, M. Onizuka, W. Wang, R. Zhang,\nand Y . Ishikawa, â€œConsistent and flexible selectivity estimation for high-\ndimensional data,â€ in Proceedings of the 2021 International Conference\non Management of Data , 2021, pp. 2319â€“2327.\n[42] A. Andoni, P. Indyk, T. Laarhoven, I. Razenshteyn, and L. Schmidt,\nâ€œPractical and optimal lsh for angular distance,â€ Advances in neural\ninformation processing systems , vol. 28, 2015.\n[43] M. Muja and D. G. Lowe, â€œFast approximate nearest neighbors with\nautomatic algorithm configuration.â€ VISAPP (1) , vol. 2, no. 331-340,\np. 2, 2009.\n[44] H. Jegou, M. Douze, and C. Schmid, â€œProduct quantization for nearest\nneighbor search,â€ IEEE transactions on pattern analysis and machine\nintelligence , vol. 33, no. 1, pp. 117â€“128, 2010.\n[45] J. Johnson, M. Douze, and H. J Â´egou, â€œBillion-scale similarity search\nwith gpus,â€ arXiv preprint arXiv:1702.08734 , 2017.\n[46] Y . Wang, H. Ma, and D. Z. Wang, â€œLider: An efficient high-dimensional\nlearned index for large-scale dense passage retrieval,â€ 2022. [Online].\nAvailable: https://arxiv.org/abs/2205.00970\n[47] L. V . Nguyen, T.-H. Nguyen, and J. J. Jung, â€œContent-based\ncollaborative filtering using word embedding: A case study on movie\nrecommendation,â€ in Proceedings of the International Conference on\nResearch in Adaptive and Convergent Systems , ser. RACS â€™20. New\nYork, NY , USA: Association for Computing Machinery, 2020, p.\n96â€“100. [Online]. Available: https://doi.org/10.1145/3400286.3418253",
  "textLength": 75990
}