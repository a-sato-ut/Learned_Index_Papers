{
  "paperId": "7df990973b36028c1fd697775932e5640afc3932",
  "title": "Learnable and Instance-Robust Predictions for Online Matching, Flows and Load Balancing",
  "pdfPath": "7df990973b36028c1fd697775932e5640afc3932.pdf",
  "text": "Learnable and Instance-Robust Predictions for Online Matching,\nFlows and Load Balancing\nThomas Lavastida* Benjamin Moseley* R. Ravi†Chenyang Xu‡\nAbstract\nWe propose a new model for augmenting algorithms with predictions by requiring that they are for-\nmally learnable and instance robust. Learnability ensures that predictions can be efﬁciently constructed\nfrom a reasonable amount of past data. Instance robustness ensures that the prediction is robust to\nmodest changes in the problem input, where the measure of the change may be problem speciﬁc. In-\nstance robustness insists on a smooth degradation in performance as a function of the change. Ideally,\nthe performance is never worse than worst-case bounds. This also allows predictions to be objectively\ncompared.\nWe design online algorithms with predictions for a network ﬂow allocation problem and restricted\nassignment makespan minimization. For both problems, two key properties are established: high quality\npredictions can be learned from a small sample of prior instances and these predictions are robust to\nerrors that smoothly degrade as the underlying problem instance changes.\n*Carnegie Mellon University. 5000 Forbes Ave, Pittsburgh, PA 15213. Email: ftlavasti, moseleybg@andrew.cmu.edu. Sup-\nported in part by a Google Research Award, an Infor Research Award, a Carnegie Bosch Junior Faculty Chair and NSF grants\nCCF-1824303, CCF-1845146, CCF-1733873 and CMMI-1938909.\n†Carnegie Mellon University. 5000 Forbes Ave, Pittsburgh, PA 15213. Email: ravi@andrew.cmu.edu. Supported in part by\nthe U. S. Ofﬁce of Naval Research under award number N00014-21-1-2243 and the Air Force Ofﬁce of Scientiﬁc Research under\naward number FA9550-20-1-0080.\n‡Zhejiang University. Hangzhou, Zhejiang, China 310007. xcy1995@zju.edu.cn. Chenyang Xu is the corresponding author.\nSupported in part by Science and Technology Innovation 2030 –”The Next Generation of Artiﬁcial Intelligence” Major Project\nNo.2018AAA0100902 and China Scholarship Council No.201906320329.\n1arXiv:2011.11743v2  [cs.LG]  2 Jul 2021\n\n1 Introduction\nInspired by advances in machine learning, there is an interest in augmenting algorithms with predictions,\nespecially in online algorithm design [22, 10, 12, 16, 29, 31, 37, 30]. Algorithms augmented with predictions\nhave had empirical success in domains such as look-up tables [29], caching [31], and bloom-ﬁlters [34].\nThese successes and the availability of data to make predictions using machine learning have motivated the\ndevelopment of new analysis models for going beyond worst-case bounds where an algorithm is supplied\nwith accurate predictions. In these models, an algorithm is given access to a prediction about the problem\ninstance. The algorithm’s performance is bounded in terms of the quality of this prediction. Typically the\nalgorithm learns such predictions from a limited amount of past data leading to error-prone predictions.\nThe algorithm with accurate predictions should result in better performance than the best worst-case bound.\nIdeally, the algorithm never performs asymptotically worse than the best worst-case algorithm even if the\nprediction error is large. In-between, there is a graceful degradation in performance as the prediction error\nincreases. For example, competitive ratio or running time can be parameterized by prediction error. See [35]\nfor a survey.\nLearnable and Instance Robust Predictions: The model proposed in this paper has two pillars for\naugmenting algorithms with predictions. ( Learnability :) Predictions should be learnable from representa-\ntive data. ( Instance Robustness1:) Predictions should be robust to minor changes in the problem instance.\nAs in prior models, determining what to predict remains a key algorithmic challenge.\nSuppose there is an unknown distribution Dover instancesI. Building on data driven algorithm de-\nsign [22] and PAC-learning models, we require that predicted parameters are provably learnable using a\nsmall number of sample instances from D. The sample complexity of this task can be used to compare how\ndifﬁcult different predictions are to construct. Practically, the motivation is that parameters are learned from\nprior data (e.g. instances of the problem).\nIn practice, future problem instances may not come from the same distribution used to learn the param-\neters. Therefore, we also desire predictions that are robust to modest changes in the input. In particular,\nif the predictions perform well on some instance I, then the performance on a nearby instance I0should\nbe bounded as a function of the distance between these instances. This measure of the distance between\ninstances is necessarily problem speciﬁc.\nWe note that learnability is rarely addressed in prior work and our robustness model differs from many\nprior works by bounding the error by differences in problem instances (instance robustness), rather than by\nthe differences in the predictions themselves (parameter robustness). We present learnable and instance-\nrobust predictions for two concrete online problems.\nOnline Flow Allocation Problem: We consider a general ﬂow and matching problem. The input is\na Directed-Acyclic-Graph (DAG) G. Each node vhas an associated capacity Cv. There is a sink node t,\nsuch that all nodes in the DAG can reach the sink. Online source nodes arrive that have no incoming edges\n(and never have any incoming edges in the future) and the other nodes are ofﬂine and ﬁxed. We will refer to\nonline nodes Ias impressions. When impression iarrives, it is connected to a subset of nodes Ni. At arrival,\nthe algorithm must decide a (fractional) ﬂow from itotof value at most 1 obeying the node capacities. This\nﬂow is ﬁxed the moment the node arrives. The goal is to maximize the total ﬂow that reaches twithout\nexceeding node capacities. Instances are deﬁned by the number of each type of impression. The type of\nan impression is given by the subset of the nodes of Gto which it has outgoing arcs. We may consider\nspeciﬁc worst-case instances or a distribution over types in our analysis. This problem captures fractional\n1Note that the robustness here is different from the deﬁnition of robustness mentioned in previous work, which we refer to as\nparameter robustness. See Section 2 for a discussion.\n1\n\nversions of combinatorial problems such as online matching, unweighted Adwords, and ﬁnding a maximum\nindependent set in a laminar matroid or gammoid. We call the problem the Online Flow Allocation Problem.\nRestricted Assignment Makespan Minimization: In this problem, there are mmachines and njobs\narrive in an online order. When job jarrives it must be immediately and irrevocably assigned to a machine.\nThe job has size pjand can only be assigned to a subset of machines speciﬁc to that job. After jis assigned\nthe next job arrives. A machine’s load is the total size of all jobs assigned to it. The goal is to minimize the\nmakespan, or maximum load, of the assignment.\n1.1 Overview of Results for Flow Allocation and Restricted Assignment\nWe ﬁrst focus on the ﬂow allocation problem and then we give similar results for the makespan minimization\nproblem.\nNode Parameters: Our results on learnability and robustness are enabled by showing the existence of\nnode weights which capture interesting combinatorial properties of ﬂows. Inspired by the weights proven\nin [2] for bipartite matching, we establish that there is a single weight for each node in the DAG that\ncompletely describe near optimal ﬂows on a single problem instance. The weights determine an allocation\nof ﬂow for each impression which is independent of the other impressions. Each node in the DAG routes\nthe ﬂow leaving it proportionally to the weights of its outgoing neighbors. Moreover, the ﬂow is near\noptimal, giving a (1\u0000\u000f)-approximate solution for any constant \u000f >0(but requiring time polynomial in\n1=\u000fto compute). Given these weights, the ﬂow can be computed in one forward pass for each impression\nin isolation. Thus they can be used online if given as a prediction. These weights are also efﬁciently\ncomputable ofﬂine given the entire problem instance (see Theorem 3).\nInstance Robustness: We measure the distance of the two instances as the difference of the number\nof impressions of each type (see Theorem 5). We show that if the weights are near optimal for one instance,\nthe performance degrades gracefully according to the distance between the two instances. This distance is\ndeﬁned for any two instances irrespective of whether they are generated by speciﬁc distributions2.\nLearnability: For learnability it is assumed that impressions are drawn from an unknown distribution\nover types. We show that learning near-optimal weights for this distribution has low sample complexity\nunder two assumptions. First, we assume the unknown distribution is a product distribution. Second, we\nassume that the optimal solution of the “expected instance” (to be deﬁned later) has at least a constant\namount of ﬂow routed through each node. In the 2-layer case, this assumption can be simpliﬁed to requiring\neach node’s capacity to be at least a constant (depending on1\n\u000f).3The number of samples is polynomial in\nthe size of the DAG without the impressions. Note that in problems such as Adwords, the impressions are\nusually much larger than the ﬁxed portion of the graph.\nWe now present our main theorem on the ﬂow allocation problem.\nTheorem 1 (Flow Allocation - Informal) .There exist algorithmic parameters for the Online Flow Allocation\nproblem with the following properties:\n2We also show that our predictions for the online ﬂow allocation problem have “parameter robustness”, the kind of robustness\nthat has been considered in prior work (Theorem 6).\n3This is similar to the lower bound requirement on budgets in the online analysis of the AdWords problem [17].\n2\n\n(i) (Learnability) Learning near-optimal parameters has sample complexity polynomial in1\n\u000fand the size\nof the graph excluding the impressions. These parameters result in an online algorithm that is a (1\u0000\u000f)-\napproximate solution in expectation as compared to the expected optimal value on the distribution for\nany constant \u000f>0. (Theorem 4)\n(ii) (Instance Robustness) Using the optimal parameters for an instance on another instance gives a com-\npetitive ratio that improves as their distance decreases, where the distance is proportional to the dif-\nference of impressions (Theorem 5).\n(iii) (Worst-Case Robustness) The competitive ratio of the online algorithm using the parameters is never\nworse than1\nd+1, regardless of the distance between the two instances, where dis the diameter of G.\n(Theorem 5)\nThe theorem states that weights are learnable and only a small number of samples are required to con-\nstruct weights that are near optimal. These predictions break the worst-case 1\u00001\nebound on the competitive\nratio for any randomized algorithm for online fractional matching, a special case. Moreover, the difference\nin the types of impressions between two instances gives a metric under which we can demonstrate instance\nrobustness. Further the algorithm has worst-case guarantees, i.e. the ratio is never worse than1\nd+1, which\nis tight for deterministic integral online algorithms and d-layer graphs (see Theorem 61) even though we\noutput fractional allocations.\nWe now discuss our results for makespan minimization.\nTheorem 2 (Restricted Assignment - Informal) .There exist algorithmic parameters for the Restricted As-\nsignment Makespan Minimization problem with the following properties:\n(i) (Learnability) Learning the near optimal parameters has sample complexity polynomial in m, the num-\nber of machines, and1\n\u000f. These parameters result in an online algorithm that is a (1 +\u000f)approximate\nsolution in expectation as compared to the expected optimal value on the distribution for any constant\n\u000f>0. (Theorem 8)\n(ii) (Instance Robustness) Using the optimal parameters for any instance on a nearby instance gives a\ncompetitive ratio for fractional assignment that is proportional to their distance, where the distance is\nproportional to the relative difference of job sizes of the same type. (Theorem 7)\n(iii) (Worst-Case Robustness) The competitive ratio of the algorithm using the parameters is never worse\nthanO(logm), matching the known \n(logm)lower-bound on any integral online algorithm. (Theo-\nrem 7)\nThis theorem shows that the predictions of [30] have much stronger properties than what is known and\nare learnable and instance robust. That paper left open the question if their predictions can be formally\nlearned in any model. Moreover, it was not known if they are instance robust. We remark that this theorem\nassumes fractional assignments, whereas the original problem (and the lower bound [9]) requires integer\nassignments. Lattanzi et al. [30] shows that any fractional assignment can be rounded online while losing a\nO((log logm)3)factor in the makespan.\n1.2 Related Work\nAlgorithms with Predictions: In this paper, we consider augmenting the standard model of online\nalgorithms with erroneous predictions. Several online problems have been studied in this context, including\ncaching [31, 38, 25, 40], page migration [24], metrical task systems [7], ski rental [37, 20, 4], schedul-\ning [37], load balancing [30], online linear optimization [14], speed scaling [41], set cover [42], and bipartite\nmatching and secretary problems [8].\n3\n\nAntoniadis et al. [8] studies online weighted bipartite matching problems with predictions. The main\naspect of this work which distinguishes it from ours is that it considers the random order arrival model,\nrather than adversarial orders.\nMahdian et al. [32] focuses on the design of robust algorithms. Rather than considering online algo-\nrithms which use a prediction, they consider two black box online algorithms, one optimistic and the other\npessimistic. The goal is to give an online algorithm which never performs much worse than the better of\nthese two algorithms for any given instance. This is shown for problems such as load balancing, facility\nlocation, and ad allocation.\nThe predictions utilized in our algorithm come in the form of vertex weights that guide a proportional al-\nlocation scheme. Agrawal et al. [2] ﬁrst studied proportional allocations for maximum cardinality fractional\nmatching as well as weighted fractional matchings with high entropy. Lattanzi et al. [30] utilize similar\npredictions based on proportional weights to give algorithms with predictions for online load balancing.\nData-Driven Algorithm Design: This paper considers the learnability of the predictions through the\nmodel of data-driven algorithms. In classical algorithm design, the main desire is ﬁnding an algorithm that\nperforms well in the worst case against all inputs for some measure of performance, e.g. running time or\nspace usage. Data-driven algorithm design [22, 13, 11, 10, 12, 16], in contrast, wants to ﬁnd an algorithm that\nperforms well on the instances that the user is typically going to see in practice. This is usually formalized\nby ﬁxing a class of algorithms and an unknown distribution over instances, capturing the idea that some\n(possibly worst case) instances are unlikely to be seen in practice. The typical question asked is: how many\nsample instances are needed to guarantee you have found the best algorithm for your application domain?\nOther Related Work: Online matching and related allocation problems have been extensively studied\nin both the adversarial arrival setting [28, 26, 18, 33] and with stochastic arrivals [17, 19, 21, 36, 1]. A related\nbut different setting to ours is the online algorithms with advice setting [15]. Here the algorithm has access to\nan oracle which knows the ofﬂine input. The oracle is allowed to communicate information to the algorithm\nabout the full input, and the goal is to understand how many bits of information are necessary to achieve\na certain competitive ratio. This has also been extended to the case where the advice can be arbitrarily\nwrong [5]. This can be seen as similar to our model, however the emphasis isn’t on tying the competitive\nratio to the amount of error in the advice.\n2 Algorithms with Learnable and Instance-Robust Predictions\nLearnability via Sample Complexity: We consider the following setup inspired by PAC learning and\nrecently considered in data-driven algorithms. Assume a maximization problem and let Dbe an unknown\ndistribution over problem instances. Let ALG(I;y)be the performance4of an algorithm using parameters\nyon instanceI. The ideal prediction for this distribution is then y\u0003:= arg max yEI\u0018D[ALG(I;y)]. Since\nwe assume thatDis unknown, we wish to learn from samples. In particular, we wish to use some number s\nof independent samples to compute a parameter ^ysuch that EI\u0018D[ALG(I;^y)]\u0015(1\u0000\u000f)EI\u0018D[ALG(I;y\u0003)]\nwith probability 1\u0000\u000e, for any\u000f;\u000e2(0;1). The sample complexity sdepends on the problem size as well\nas1=\u000fand1=\u000e. As is standard in learning theory, we require the sample complexity to be polynomial in\nthese parameters5.\nInspired by competitive analysis, we also compare to the following stronger benchmark in this paper.\nFor any instanceI, let OPT (I)be the value of an optimal solution on I. We give learning algorithms\n4In general this can be any performance metric, such as running time or solution value. Here we focus on the value of some\nobjective function such as the size of a fractional ﬂow.\n5For more difﬁcult problems, we can relax the 1\u0000\u000frequirement to be a weaker factor.\n4\n\nproducing predicted parameters ^ysuch that EI\u0018D[ALG(I;^y)]\u0015(1\u0000\u000f)EI\u0018D[OPT(I)]and polynomial\nsample complexity under the assumptions on Ddescribed earlier. Note that this guarantee implies the ﬁrst\none.\nInstance Robustness: LetIandI0be two problem instances, and consider running the algorithm on\ninstanceI0with the prediction y\u0003(I). We bound the performance of the algorithm as a function of the\ndifference between these two instances. In contrast, prior work focuses on differences in the predicted\nparametersy\u0003andy0for the same instance I. Moreover, it is desirable that the algorithm never performs\nworse than the best worst-case algorithm.\nFor example, in online ﬂow allocation, we can consider an instance as a vector of types, i.e. Iiis the\nnumber of impressions of type i. Then we can take the difference between the instances as \r=kI\u0000I0k1.\nSayy\u0003(I)can be used to give a c-competitive algorithm on instance I. Let\u000bbe the best competitive ratio\nachievable in the worst-case model. We desire an algorithm that is maxff(c;\r);\u000bg-competitive where f\nis a monotonic function depending on cand\r.We remark that the online model requires I0to arrive in a\nworst-case order.\n2.1 Putting the Model in Context\nRelationship to Prior Predictions Model: The ﬁrst main difference in this model as compared to\nprior work is learnability . With the notable exception of [4], prior work has introduced predictions without\nestablishing they are learnable. Without this requirement there is no objective metric to detail if a prediction\nis reasonable or not. To see this shortcoming, imagine simply predicting the optimal solution for the problem\ninstance. This is often not reasonable because the optimal solution is too complex to learn and use as\na prediction. We introduce bounded sample complexity for learning predictions in our model to ensure\npredictions can be provably learned.\nNext difference is in how to measure error. The performance of the algorithm is bounded in terms of\nthe error in the prediction in the prior model. For example, say the algorithm is given a predicted vector\n^y(I)for problem instance Iand the true vector that should have been predicted is y\u0003(I). One can deﬁne\n\u0011^y(I)=k^y(I)\u0000y\u0003(I)kpto be the error in the parameters for some norm p\u00151. The goal is to give an\nalgorithm that is f(\u0011^y(I))-competitive for an online algorithm where fis some non-decreasing function of\n\u0011^y(I): the better the function f, the better the algorithm performance. One could also consider run time or\napproximation ratio similarly. Notice the bound is worst-case for a given error in the prediction. This we\ncallparameter robustness .\nIt is perhaps more natural to deﬁne a difference between two problem instances as in our model rather\nthan the difference between two predicted parameters. Indeed, consider predicting optimal dual linear pro-\ngram values. These values can be different for problem instances that are nearly identical. Therefore,\naccurate parameters will not be sufﬁcient to handle inconsequential changes in the input. Instance robust-\nness allows for more accurate comparison of two predictions on similar problem instances. More practically,\ninstance closeness is easier to monitor than closeness of the proposed predictions to an unknown optimal\nprediction for the whole instance.\nLearning Algorithm Parameters: Learning algorithmic parameters has distinct advantages over\nlearning an input distribution. In many cases it can be easier to learn a decision rule than it is to learn a\ndistribution. For example, consider the unweighted b-matching problem in bipartite graphs for large bin\nthe online setting. In this problem there is a bipartite graph G= (I[A;E)with capacities b2ZA\n+. The\nobjective is to ﬁnd a collection of edges such that each node in Iis matched at most once and each node\na2Ais matched at most batimes. Nodes on one side of the graph arrive online and must be matched on\narrival. Say the nodes are i.i.d. sampled from an unknown discrete distribution over types. A type is deﬁned\n5\n\nProblem Parameter Robust-\nnessLearnability Instance Robust-\nness\nCaching [31, 38, 25, 40] - -\nCompletion Time Schedul-\ning[37] - [37]\nSki Rental [37, 4, 20] [4] [37, 4, 20]\nRestricted Assignment [30] This Paper This Paper\nb-Matching This Paper This Paper This Paper\nFlow Allocation This Paper This Paper This Paper\nTable 1: Relationship to Prior Work\nby the neighbors of the node. Let sbe the number of types. Then the sample complexity of learning the\ndistribution is \n(s). Notice that scould be superlinear in the number of nodes. In our results, the sample\ncomplexity is independent of the number of types in the support of the distribution. The phenomenon that\nit is sometimes easier to learn good algorithmic parameters rather than the underlying input distribution has\nbeen observed in several prior works. See [22, 13, 11, 10, 12, 16] for examples.\nTable 1 illustrates how our paper relates to prior work, focusing on the two pillars for augmenting\nalgorithms emphasized in our model.\nPaper Organization: For online ﬂow allocation, both learnability and instance-robustness rely on\nshowing the existence of node predictions which is described in Section 3, followed by learnability and\nrobustness in Sections 4 and 5 respectively. While these sections give technical overviews, full proofs\nare in the appendix. We show the existence of predictions in 3-layer DAGs in Appendix A and give the\nresults for general DAGs in Appendix D. We ﬁrst prove the learnability of predictions in 2-layered graphs in\nAppendix B and then generalize this result to general DAGs in Appendix E. The proofs about the instance-\nand parameter-robustness are in Appendices C and F respectively. For load balancing, the results are in\nSection 6 and the corresponding proofs are in Appendix G.\n3 Matchings and Flows: Existence of Weights\nConsider a directed acyclic graph G= (fs;tg[V;E), where each vertex v2Vhas capacity Cvand is on\nsomes\u0000tpath. Our goal is to maximize the ﬂow sent from stotwithout violating vertex capacities. Before\nconsidering the general version, we examine the 3-layered version. Say a graph is d-layered if the vertices\nexcludings;tcan be partitioned into dordered sets where all arcs go from one set to the next. Then the\n3-layered case is deﬁned as follows. The vertices in Vare partitioned into 3 sets I;A, andB.sis connected\nto all ofIandtis connected from all of B, while the remaining edges are only allowed to cross from Ito\nAand fromAtoB. LetNu:=fv2Vj(u;v)2Egbeu’s out-neighbors. We have the following result\ngeneralizing the prior work of Agrawal et al [2] on 2-layered graphs.\nTheorem 3. LetG= (fs;tg[V;E)be a 3-layered DAG. For each edge (u;v)2E, letxuvbe the\nproportion of ﬂow through uwhich is routed along (u;v). For any\u000f2(0;1), there exist weights f\u000bvgv2V\nsuch that setting xuv=\u000bvP\nv02Nu\u000bv0yields a (1\u0000\u000f)-approximate ﬂow. Moreover, these weights can be\nobtained in time O(n4log(n=\u000f)=\u000f2).\nWe can generalize this theorem to d-layered graphs. In particular, our algorithm for the d-layered case\nproduces weights with additional properties, which we leverage to handle general DAGs. See Section A for\nprecise statements. Notice that the number of weights is proportional to the number of nodes in the graph and\nnot the number of edges. We believe it is an interesting combinatorial property that such succinct weights\n6\n\non the nodes can encode a good ﬂow on the asymptotically larger number of edges and is of independent\ninterest.\nTechnical Overview\nHere we give a technical overview. The full proof is in Section A.\nA Simple Algorithm for Layered Graphs: Prior work [2] showed that there exists a set of weights giving\nnearly the same guarantees we show, but only for bipartite graphs. The existence of such weights can be\ngeneralized to d-layer graphs easily as follows. First ﬁnd an (optimal) maximum ﬂow f. For each vertex v,\nletf(v)be the ﬂow going through v. Reset the vertex capacity of vto bef(v). For each pair of adjacent\nlayers ﬁnd the weights between the two layers independently using the algorithm of [2], treating nodes von\nthe left hand side as f(v)individual impressions. By the previous result, each layer only loses a negligible\nportion of the total ﬂow which can be compounded to yield a low loss for these set of weights.\nThe above reduction does not generalize to general DAGs. One can arrange a DAG into layers, but\nthere are fundamental algorithmic challenges with constructing weights that arise when edges cross layers.\nOne of this paper’s algorithmic contributions is showing how to construct such weights for general DAGs.\nMoreover, as an intermediate result, we show how to compute the weights directly extending the approach\nof [2] for multi-layer graphs without ﬁrst solving a ﬂow problem optimally as in the above reduction.\nFinding Weights for Bipartite Graphs: We begin by ﬁrst simplifying the algorithm of [2] for bipartite\ngraphs. Let G= (fs;tg[I[A;E)be such a graph. In this case, the fraction of ﬂow u2Isends to\nv2Asimpliﬁes to xuv=\u000bvP\nv02Nu\u000bv0wheref\u000bgv2I[Aare the set of weights. Initially all of the weights\nare1for a vertexa2A. Some of the nodes receive more ﬂow than their capacity in this initial proportional\nallocation according to the weights. We say a node for which the current proportional allocation of ﬂow\nexceeds its capacity by a 1 +\u000ffactor is overallocated . In an iteration, the algorithm decreases the weights\nof these nodes by a 1 +\u000ffactor.6After this process continues for a poly-logarithmic number of iterations,\nwe will be able to show the resulting weights result in a near optimal ﬂow.\nTo prove that the ﬁnal weights are near optimal, we show that the weights can be directly used to identify\na vertex cut whose value matches the proportional ﬂow given by the weights. In particular, we will partition\nthe nodes in Abased on their weight values. For a parameter \f, we say a node is ‘above the gap’ if its weight\nis larger than \fn=\u000f . A node of weight less than \fis below the gap. All others are in the gap. The parameter\n\fis chosen such that the nodes in the gap contribute little to the overall ﬂow and they can essentially be\ndiscarded via an averaging argument7. Assume this set is empty for simplicity. Let G(A)+andG(A)\u0000be\nthe sets of vertices in Aabove and below the gap, respectively.\nWe now describe a cut. Let I0\u0012Ibe the impression nodes adjacent to at least one node in G(A)+.\nThen the vertex cut is I0[G(A)\u0000. Since all paths must either cross I0orG(A)\u0000, this is a valid vertex cut.\nWe now show the cut value is close to the ﬂow achieved by the weights, completing the analysis using the\nweaker direction of the max-ﬂow min-cut theorem.\nFirst, nodes in I0are cut. Due to the way ﬂow is sent based on the weight proportions, for any vertex I0,\nat least a (1\u0000\u000f)proportion of its ﬂow will be sent to G(A)+. Since the nodes in G(A)+did not decrease the\nweights at least once, at some point they were not over-allocated. We claim that because of this, they will\nnever be over-allocated hence and, therefore, nodes in I0send nearly all of their ﬂow to the sink successfully.\nNext nodes inG(A)\u0000are cut. These nodes decreased their weights (almost) every iteration because they are\nat or above their allocation. Thus, for all these nodes we get ﬂow equal to their total capacity. The fraction\n6Prior work [2] performed this operation as well as increasing the weights of nodes whose allocation was signiﬁcantly below\nthe capacity. Our simpliﬁcation to allow only decreases helps with the generalization to more complex graphs and correcting for\nerror in the weights.\n7This averaging is what necessitates the poly-logarithmic number of weight update iterations in the algorithm\n7\n\nof this ﬂow through G(A)\u0000coming from paths using I0is negligible because of the weight proportions so\nthis ﬂow is almost disjoint from that of I0. Thus, we have found a proportional ﬂow nearly matching the\nvalue of the cut identiﬁed.\nGeneral Graphs: Now we consider the more general algorithm. To convey intuition, we will only consider\ndirectly computing weights for a 3-layered graph G= (fsg[I[A[B[ftg;E)where edges are between\nadjacent layers. This will highlight several of the new ideas. As before, weights of all nodes are initially\none. And as before, a node decreases its weight if it is over-allocated, which we will refer now to as a\nself-decrease . Now though, whenever a node in Adecreases its weight it does so by a (1 +\u000f0)factor and\nthose inBdecrease at a (1 +\u000f)factor where \u000f0\u0014\u000f.\nA new challenge is that a node bin layerBmay be over allocated and it may not be enough for Bto\nreduce its weight. Indeed, Bmay need some neighbors in Ato reduce their allocation. For instance, if\nb2Bhas neighbors in Afor which it is the only neighbor, then reducing b’s weight does not change its\nallocation and the ﬂow needs to be redistributed in the ﬁrst layer. In this case, the nodes in Bwill specify\nthat some nodes in Aneed to decrease their allocation. We call this a forced decrease . This set has to be\ncarefully chosen and intuitively only the nodes in Athat are the largest weight as compared to b2Bare\ndecreased. We run this procedure for a polylogarithmic number of steps and again we seek to ﬁnd a cut\nmatching the achieved ﬂow.\nWe discuss the need for different \u000fand\u000f0. In the bipartite case when a node decreased its weight,\nthat node is guaranteed to receive no more allocation in the next round (it could remain the same though).\nIntuitively, this is important because in the above proof for bipartite graphs we want that if a node is in\nG(A)+, above the gap, if it was ever under-allocated then it never becomes over-allocated in later iterations.\nOur update ensures this will be the case since self-decreases will continue henceforth to keep the load of\nsuch a node below its capacity. Consider setting \u000f=\u000f0for illustration. Because of the interaction between\nlayers, a node iinBcould receive more allocation even if it decreases its weight in an iteration. This is\nbecause the nodes in AandBcould change their weights. Nodes in Achanging their weight can give up\nto an extra (1 +\u000f)allocation (via predecessors of ithat are not decreased), and the same for Bfor a total\nof(1 +\u000f)2extra allocation arriving at i. The node decreasing its weight reduces its allocation by a (1 +\u000f)\nfactor for a total change of a (1 +\u000f)2\u00011\n(1+\u000f)>1factor. By choosing \u000fand\u000f0to be different, as well\nas the characterization of which nodes decrease during a forced decrease, we can show any node will not\nreceive less allocation if its weight does not decrease and will not receive more allocation if it performs a\ndecrease. We call these properties “Increasing monotonicity” (Property 3) and “Decreasing monotonicity”\n(Property 4) in Section A.\nAs in the bipartite case, we can ﬁnd a gap in layers AandB, which gives sets above the gap G(A)+and\nG(B)+inAandB, respectively. Let the sets G(A)\u0000andG(B)\u0000be the nodes below the gap. As before\nnodes inG(A)\u0000(resp.,G(B)\u0000) decreased their weight many more iterations than G(A)+(resp. ,G(B)+,).\nFor simplicity, assume this partitions the nodes of the entire graph, so no nodes are inside either gap. Let\nI0\u0012Ibe the nodes adjacent to at least one node in G(A)+. LetA0be nodes inG(A)\u0000, below the gap,\nthat have an edge to G(B)+(the analogue of I0in theA-layer) . Any ﬂow path that crosses the Alayer at\nG(A)+;A0andG(A)\u0000nA0are blocked by the sets I0;A0andG(B)\u0000respectively showing that this set\nforms a vertex cut.\nWe show the ﬂow obtained is nearly the value of the vertex cut I0[A0[G(B)\u0000. As before, nodes in\nI0(resp.A0) send almost all their ﬂow to nodes above the gap in the next layer G(A)+(resp.G(B)+). Like\nbeforeG(A)+andG(B)+do not decrease every round, so we can show they are not allocated more than\ntheir capacity (see Lemma 11 and Lemma 13). The algorithmic key is that, by choosing the forced decrease\ncarefully, we can show each node in G(A)+has a neighbor inG(B)+. This ensures almost all of G(A)+’s\nﬂow reaches the sink because all of these nodes will send essentially all their the ﬂow to G(B)+and these\nnodes are not at capacity. Thus, I0can send all its ﬂow to the sink successfully. Similarly, A0sends its ﬂow\n8\n\ntoG(B)+and then to the sink. Finally, as before, G(B)\u0000(as well asG(A)\u0000) are sets of nodes near their\nallocation because they decreased essentially every iteration (see Lemma 12 and Lemma 14). Thus, G(B)\u0000\nsends its ﬂow directly to the sink. Moreover, we ensure that only a negligible fraction of this ﬂow is double\ncounted by the deﬁnition of the large weight reduction across the gaps (see Lemma 15); therefore we have\nfound a ﬂow allocation obtained by the weights whose value nearly matches the value of an identiﬁed cut.\nThis analysis generalizes to layered DAGs where edges do not cross layers. To extend the existence of\nthese weights to general DAGs we reduce the problem to ﬁnding weights with additional structural properties\non a layered DAG. From the input DAG we make additional copies of nodes that have ancestors in earlier\nlayers and link these copies via a path to the original neighbor. We convert any edge that crosses many layers\nto one in the layered DAG from its original head node to the copy of its tail node in the next layer. Then we\nargue that a key functional relation exists between the weights of any original node and its copies in earlier\nlayers. This allows us to transfer the weights computed in the auxiliary layered DAG to the original DAG\n(see Section A.1 for more details).\n4 Matchings and Flows: Learnability of Predictions\nWe show that the weights are efﬁciently learnable. Assuming that each arriving impression is i.i.d. sampled\nfrom an unknown distribution, we want to learn a set of weights from a collection of past instances and\nexamine their expected performance on a new instance from the same distribution8. A direct approach might\nbe to learn the unknown distribution from samples and utilize known ideas from stochastic optimization\n(where knowledge of the distribution is key). A major issue though is that there can be a large number of\npossible types (potentially exponential in the number of nodes of the DAG). A distribution that is sparse\nover types is not easy to learn with a small number of samples.\nWe claim that the weights are efﬁciently learnable, even if the distribution of types of impressions is not.\nWe show that this task has low sample complexity and admits an efﬁcient learning algorithm. Consequently,\nif there is an unknown arbitrary distribution that the impressions are drawn from, then only a small number\nof instances is required to compute the weights. The number of samples is proportional to size of the DAG\nwithout the impressions. In most problems such as Adwords, the number of arriving impressions is much\nlarger than the ﬁxed (ofﬂine) portion of the graph.\nBefore stating our results, we introduce two necessary assumptions. The ﬁrst assumption is that each\nimpression is i.i.d. sampled from an unknown distribution D. Where no ambiguity will result, we also say\nan instance is sampled from Dif each impression is an i.i.d sample from D. The second assumption is\nrelated to the expected instance of the distribution D. The expected instance of a distribution is the instance\nwhere the number of each type of impressions is exactly the expected value.9We assume that in the optimal\nsolution of the expected instance, the load of each node is larger than a constant. Namely, it cannot happen\nthat in the optimal ﬂow, there exist many vertices which obtain very small amount of ﬂow.\nTheorem 4. Under the two assumptions above, for any \u000f;\u000e2(0;1), there exists a learning algorithm\nsuch that, after observing O(n2\n\u000f2ln(nlogn\n\u000e))instances, returns weights f^\u000bg, satisfying that with probability\nat least 1\u0000\u000e,EIsD[R(^\u000b;I)]\u0015(1\u0000\u000f)EIsD[R(\u000b\u0003;I)]whereR(\u000b;I)is the value of the fractional ﬂow\nobtained by applying \u000bto instanceIand\u000b\u0003= arg max\n\u000bEIsD[R(\u000b;I)].\nTechnical Overview: Here we overview the analysis. The full proof is in Section B. To show that the\nweights are learnable we utilize a model similar to that of data-driven algorithm design. To illustrate our\ntechniques we focus on the case when the instance is a bipartite graph G= (I[A;E)with capacities Ca\nfor eacha2A(also recall thatjAj=n).\n8We can also analyze the performance on similar distributions by applying techniques from our instance robustness result\n9Note this could be a fractional value.\n9\n\nIn this setting there is an unknown distribution Dover instances of Iof lengthm. Thet’th entry of\nIrepresents the t’th impression arriving online. Our goal is to ﬁnd a set of weights that performs well\nfor the distribution D. In particular let \u000b\u0003:= arg max \u000b2SEI\u0018D[R(\u000b;I)]be the best set of weights for the\ndistribution. Deﬁne R(\u000b;I)to be the value of the matching using weights \u000bon instanceI. HereSis a set of\n“admissible” weights, and in particular we only consider weights output by a proportional algorithm similar\nto the algorithm of Agrawal et al. [2]. We are allowed to sample sindependent samples I1;I2;:::;Isfrom\nDand use these samples to compute a set of weights ^\u000b. We say that a learning algorithm (\u000f;\u000e)-learns the\nweights if with probability at least 1\u0000\u000eover the samples from D, we compute a set of weights ^\u000bsatisfying\nEI\u0018D[R(^\u000b;I)]\u0015(1\u0000\u000f)EI\u0018D[R(\u000b\u0003;I)]:\nThis deﬁnition is similar to PAC learning [39]. Also note we are aiming for a relative error guarantee\nrather than an absolute error. The main quantity of interest is then the sample complexity , i.e. how large\ndoessneed to be as a function of n,m,\u000f, and\u000ein order to (\u000f;\u000e)-learn a set of weights? Ideally, sonly\ndepends polynomially on n,m,1=\u000f, and 1=\u000e, and smaller is always better.\nThe standard way to understand the sample complexity for this type of problem is via the pseudo-\ndimension . Intuitively, pseudo-dimension is the natural extension of VC-dimension to a class of real valued\nfunctions. In our case the class of functions is fR(\u000b;\u0001)j\u000b2Sg , i.e. we are interested in the class of\nfunction mapping each instance Ito the value of the fractional matching given by each ﬁxed set of weights\n\u000b. If the pseudo-dimension of this class of functions is d, thens\u0019d\n\u000f2log(1=\u000e)samples are needed to (\u000f;\u000e)\nlearn the weights, given that we are able to approximately optimize the empirical average performance [6].\nThe good news for our setting is that the pseudo-dimension of our class of functions is bounded. Each\nnode in the set Acan only have one of Tdifferent weight values for some parameter T. Then since the\nnumber of nodes in Aisn, there can only be at most Tndifferent “admissible” weights. It is well known\nthat the pseudo-dimension of a ﬁnite class of kdifferent functions is log2(k). Thus the pseudo-dimension\nof our class of functions is d=nlog2(T). As long as Tisn’t growing too fast as a function of nandm, we\nget polynomial sample complexity.\nUnfortunately, ﬁnding weights to optimize the average performance across the ssampled instances is\ncomplicated. Note that for a ﬁxed instance I, the value of the matching as a function of the weights, R(\u0001;I),\nis non-linear in the weights since we are using proportional allocation. Moreover, it is neither convex nor\nconcave in the parameters \u000bso applying a gradient descent approach will not work. Due to this, it is difﬁcult\nto analyze the learnability via known results on pseudo-dimension.\nThe main tool we have at our disposal is that for a ﬁxed instance I, we can compute weights \u000bsuch\nthatR(\u000b;I)\u0015(1\u0000\u000f)OPT(I). This motivates the following natural direct approach. Take the ssampled\ninstancesI1;I2;:::;Isand take their union to form a larger “stacked” instance ^I. We then run the afore-\nmentioned algorithm on ^Ito get weights ^\u000b. Intuitively, if sis large enough, then by standard concentration\ninequalities ^I\u0019sE[I], i.e. the stacked instance approaches scopies of the “expected” instance. Then\nto complete the analysis, we need to show that E[R(^\u000b;I)]\u0019R(^\u000b;E[I]). In general, it is not true that\nE[R(^\u000b;I)]\u0019R(^\u000b;E[I]). Using more careful analysis, we show that when the distribution Dis a product\ndistribution and our two assumptions hold, this is in fact the case.\n5 Matching and Flows: Robustness\nInstance Robustness: To show the instance robustness, we assume that we can describe the instance\ndirectly. Say we have a description of the entire instance denoted by a vector ^I= ( ^m1;:::; ^mi;:::), where\n^miis the number of impressions of type i. We show that if a set of weights performs well in instance ^I, it\ncan be transferred to a nearby instance Irobustly.\nTheorem 5. For any\u000f >0, if a set of weights ^\u000breturns a (1\u0000\u000f)-approximated solution in instance ^I, it\nyields an online ﬂow allocation on instance Iwhose value is at least maxf(1\u0000\u000f)OPT\u00002\r;OPT=(d+1)g:\n10\n\nHere OPT is the maximum ﬂow value on instance I,dis the diameter of this graph excluding vertex t, and\n\ris the difference between two instances, deﬁned by jj^I\u0000Ijj1.\nThis theorem can be interpreted as follows. If we sample the instance and compute the weights in it,\nthese weights will work well and break through worst-case bounds when the type proportions are sampled\nwell. Indeed, the weights will perform well in nearby instances with similar type proportions. Moreover,\nthe algorithm never performs worse than a1\nd+1factor of optimal. We remark that this is the best competitive\nratio a deterministic integral algorithm can achieve because we show a lower bound on such algorithms in\nSection F.2. This example builds a recursive version of the simple lower bound of1\n2on the competitive ratio\nof deterministic online algorithms for the matching problem.\nThe technical proof is deferred to Appendix C. Recall that the key to showing existence of the weights\nwas to construct a cut whose capacity is nearly the same as the value of the ﬂow given by the weights. To\nshow robustness against nearby instances, we observe how this proof can be extended to nearby cuts. This\nallows us to argue about the optimal value of the new instance. Standard calculations then let us connect\nthe value of the predicted weights to this optimal value while losing only O(\r)in the value of the ﬂow. To\nensure the algorithm is never worse than a1\nd+1factor of the optimal, we guarantee that the algorithm always\nreturns a maximal allocation.\nParameter Robustness: For the parameter robustness, we show that the performance degrades lin-\nearly in the relative error of the weight parameter. Thus, our algorithm has the same robustness guarantees\nshown in other works as well.\nTheorem 6. Consider a prediction of ^\u000bvfor each vertex v2V. Due to scale invariance, we can assume\nthat the minimum predicted vertex weight ^\u000bmin= 1. Deﬁne the prediction error \u0011:= maxv2V(^\u000bv\n\u000b\u0003v;\u000b\u0003\nv\n^\u000bv);\nwheref\u000b\u0003\nvgv2Vare vertex weights that can achieve an (1\u0000\u000f)-approximate solution and \u000b\u0003\nmin= 1 for\nany ﬁxed\u000f>0. Employing predictions ^\u000b, we can obtain a solution with competitive ratio max(1\nd+1;1\u0000\u000f\n\u00112d);\nwheredis the diameter of this graph excluding vertex t.\nWhen the prediction error \u0011approaches one, the performance smoothly approaches optimal. While the\nabove theorem involves comparing the propagation of the prediction errors in the performance analysis,\nwe also investigate how inaccurate predictions can be adaptively corrected and improved in the 2-layered\nAdwords case. For that case, we give an improved algorithm that can correct error in the weights, so that\nthe loss is only O(log\u0011). This result is in Appendix F.1. Moreover, we show that the way we adapt is tight\nand the best possible up to constant factors for any algorithm given predicted weights with error \u0011.\nThe parameter robustness follows almost directly from the deﬁnition of the proportional assignment\ngiven by the weights. In each layer, the over allocation (potentially above the capacity) can be easily bounded\nby a\u00112factor, resulting in a loss of at most \u00112don adlayer graph. We remark that directly using the weights\nensures the algorithm is never worse than a1\nd+1factor of the optimal solution. The technical proof, along\nwith the weight-adapting technique for the 2-layered case, is deferred to Appendix F.\n6 Results on Load Balancing\nNext we show results for restricted assignment load balancing problem in our model. In particular, we\nstudy the instance robustness and learnability of proportional weights for this problem. The existence of\nuseful weights and parameter robustness for predicting these weights were shown in prior work [2, 30]. As\ndiscussed before, we focus on analyzing fractional assignments.\nTo describe our results we introduce the following notation. Let [m]denote the set of machines and S\ndenote a set of jobs. Each job j2Shas a sizepjand a neighborhood N(j)of feasible machines. Given\n11\n\na set of positive weights fwigi2[m]on the machines, we deﬁne a fractional assignment for each job jby\nsettingxij(w) =wiP\ni02N(j)wi0for eachi2N(j). Let ALG(S;w)be the fractional makespan on the jobs in\nSwith weights wand similarly let OPT (S)be the optimal makespan on the jobs in S. Prior work [2, 30]\nshows that for any set of jobs Sand\u000f>0there exists weights \u000bsuch that ALG(S;w)\u0014(1 +\u000f)OPT(S).\nTo describe our instance robustness result we consider an instance of the problem as follows. Consider\ninstances with njobs. The type of a job is the subset of machines to which it can be assigned. Let Sjbe\nthe total size of jobs of type jin instanceS. We consider relative changes in the instance and deﬁne the\ndifference between instances SandS0as\u0011(S;S0) := max jmaxfSj\nS0\nj;S0\nj\nSjg. Our instance robustness result is\ngiven in the following theorem.\nTheorem 7. For any instance Sand\u000f>0, letwbe weights such that ALG(S;w)\u0014(1 +\u000f)OPT(S). Then\nfor any instance S0we have ALG(S0;w)\u0014(1 +\u000f)2\u0011(S;S0)2OPT(S0).\nLetwbe as in the statement of the theorem and w0be weights such that ALG(S0;w0)\u0014(1+\u000f)OPT(S0).\nIntuitively, we lose the ﬁrst factor of \u0011(S;S0)by bounding the performance of wonS0and the second factor\nby bounding the performance of w0onS.\nNext we study learnability. We give the ﬁrst result showing these weights are learnable in any model.\nIn order to understand the sample complexity of learning the weights we need to consider an appropriate\ndiscretization of the space of possible weights. For integer R > 0and\u000f >0, letW(R) =f\u000b2Rmj\n\u000bi= (1 +\u000f)k;i2[m];k2f0;1;:::;Rgg. Additionally, let pmaxbe an upper bound on all jobs sizes. The\nfollowing theorem characterizes the learnability of the weights for restricted assignment load balancing.\nTheorem 8. Let\u000f;\u000e2(0;1)be given and set R=O(m2\n\u000f2log(m\n\u000f))and letD=Qn\nj=1Djbe a product\ndistribution over n-job restricted assignment instances such that ES\u0018D[OPT(S)]\u0015\n(1\n\u000f2log(m\n\u000f)). There\nexists an algorithm which ﬁnds weights w2W(R)such that\nES\u0018D[ALG(S;w)]\u0014(1 +\u000f)ES\u0018D[OPT(S)]with probability at least 1\u0000\u000ewhen given access to s=\n~O(m3\n\u000f2log(1\n\u000e))independent samples S1;S2;:::;Ss\u0018D.\nOur techniques here are similar to that of the online ﬂow allocation problem in that we use the samples\nto construct a “stacked” instance then compute a set of near optimal weights on this instance. We then have\nto show that these weights work well in expectation with high probability. This step necessitates the two\nassumptions in the theorem statement. First, we need that the expected optimal makespan is reasonably large\nso that the expected makespan is close to the maximum of the expected loads of the machines. Second, we\nneed that the instance is drawn from a product distribution so that the stacked instance converges in some\nsense toscopies of the “expected” instance. See Section G for complete arguments.\n12\n\nReferences\n[1] Shipra Agrawal, Zizhuo Wang, and Yinyu Ye. A dynamic near-optimal algorithm for online linear\nprogramming. Oper. Res. , 62(4):876–890, 2014. doi: 10.1287/opre.2014.1289. URL https://\ndoi.org/10.1287/opre.2014.1289 .\n[2] Shipra Agrawal, Morteza Zadimoghaddam, and Vahab Mirrokni. Proportional allocation: Simple,\ndistributed, and diverse matching with high entropy. In Jennifer Dy and Andreas Krause, editors,\nProceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of\nMachine Learning Research , pages 99–108, Stockholmsm ¨assan, Stockholm Sweden, 10–15 Jul 2018.\nPMLR. URL http://proceedings.mlr.press/v80/agrawal18b.html .\n[3] Nir Ailon, Bernard Chazelle, Kenneth L. Clarkson, Ding Liu, Wolfgang Mulzer, and C. Seshadhri.\nSelf-improving algorithms. SIAM J. Comput. , 40(2):350–375, 2011. doi: 10.1137/090766437. URL\nhttps://doi.org/10.1137/090766437 .\n[4] Keerti Anand, Rong Ge, and Debmalya Panigrahi. Customizing ml predictions for online algorithms.\nICML 2020 , 2020.\n[5] Spyros Angelopoulos, Christoph D ¨urr, Shendan Jin, Shahin Kamali, and Marc P. Renault. Online\ncomputation with untrusted advice. In Thomas Vidick, editor, 11th Innovations in Theoretical Com-\nputer Science Conference, ITCS 2020, January 12-14, 2020, Seattle, Washington, USA , volume 151 of\nLIPIcs , pages 52:1–52:15. Schloss Dagstuhl - Leibniz-Zentrum f ¨ur Informatik, 2020. doi: 10.4230/\nLIPIcs.ITCS.2020.52. URL https://doi.org/10.4230/LIPIcs.ITCS.2020.52 .\n[6] Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations . Cambridge\nUniversity Press, USA, 1st edition, 2009. ISBN 052111862X.\n[7] Antonios Antoniadis, Christian Coester, Marek Eli ´as, Adam Polak, and Bertrand Simon. Online metric\nalgorithms with untrusted predictions. CoRR , abs/2003.02144, 2020. URL https://arxiv.org/\nabs/2003.02144 .\n[8] Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online matching\nproblems with machine learned advice. CoRR , abs/2006.01026, 2020. URL https://arxiv.\norg/abs/2006.01026 .\n[9] Yossi Azar, Joseph Sefﬁ Naor, and Raphael Rom. The competitiveness of on-line assignments. J.\nAlgorithms , 18(2):221–237, March 1995. ISSN 0196-6774. doi: 10.1006/jagm.1995.1008. URL\nhttp://dx.doi.org/10.1006/jagm.1995.1008 .\n[10] Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch.\nIn Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Confer-\nence on Machine Learning, ICML 2018, Stockholmsm ¨assan, Stockholm, Sweden, July 10-15, 2018 ,\nvolume 80 of Proceedings of Machine Learning Research , pages 353–362. PMLR, 2018. URL\nhttp://proceedings.mlr.press/v80/balcan18a.html .\n[11] Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design,\nonline learning, and private optimization. In Mikkel Thorup, editor, 59th IEEE Annual Symposium\non Foundations of Computer Science, FOCS 2018, Paris, France, October 7-9, 2018 , pages 603–614.\nIEEE Computer Society, 2018. doi: 10.1109/FOCS.2018.00064. URL https://doi.org/10.\n1109/FOCS.2018.00064 .\n13\n\n[12] Maria-Florina Balcan, Travis Dick, and Colin White. Data-driven clustering via parameterized lloyd’s\nfamilies. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol `o Cesa-\nBianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: An-\nnual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December\n2018, Montr ´eal, Canada , pages 10664–10674, 2018. URL http://papers.nips.cc/paper/\n8263-data-driven-clustering-via-parameterized-lloyds-families .\n[13] Maria-Florina Balcan, Dan F. DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm, and Ellen\nVitercik. How much data is sufﬁcient to learn high-performing algorithms? CoRR , abs/1908.02894,\n2019. URL http://arxiv.org/abs/1908.02894 .\n[14] Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, and Manish Purohit. Online learning with imperfect\nhints. CoRR , abs/2002.04726, 2020. URL https://arxiv.org/abs/2002.04726 .\n[15] Joan Boyar, Lene M. Favrholdt, Christian Kudahl, Kim S. Larsen, and Jesper W. Mikkelsen. Online\nalgorithms with advice: A survey. SIGACT News , 47(3):93–129, August 2016. ISSN 0163-5700. doi:\n10.1145/2993749.2993766. URL http://doi.acm.org/10.1145/2993749.2993766 .\n[16] Shuchi Chawla, Evangelia Gergatsouli, Yifeng Teng, Christos Tzamos, and Ruimin Zhang. Pandora’s\nbox with correlations: Learning and approximation, 2020. URL https://arxiv.org/abs/\n1911.01632 .\n[17] Nikhil R. Devanur and Thomas P. Hayes. The adwords problem: online keyword matching with\nbudgeted bidders under random permutations. In Proceedings 10th ACM Conference on Electronic\nCommerce (EC-2009), Stanford, California, USA, July 6–10, 2009 , pages 71–78, 2009.\n[18] Nikhil R. Devanur, Kamal Jain, and Robert D. Kleinberg. Randomized primal-dual analysis of\nRANKING for online bipartite matching. In Sanjeev Khanna, editor, Proceedings of the Twenty-\nFourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2013, New Orleans, Louisiana,\nUSA, January 6-8, 2013 , pages 101–107. SIAM, 2013. doi: 10.1137/1.9781611973105.7. URL\nhttps://doi.org/10.1137/1.9781611973105.7 .\n[19] Jon Feldman, Monika Henzinger, Nitish Korula, Vahab S. Mirrokni, and Clifford Stein. Online stochas-\ntic packing applied to display ad allocation. In Mark de Berg and Ulrich Meyer, editors, Algorithms -\nESA 2010, 18th Annual European Symposium, Liverpool, UK, September 6-8, 2010. Proceedings, Part\nI, volume 6346 of Lecture Notes in Computer Science , pages 182–194. Springer, 2010. doi: 10.1007/\n978-3-642-15775-2 n16. URL https://doi.org/10.1007/978-3-642-15775-2_16 .\n[20] Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert ad-\nvice. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th Interna-\ntional Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA ,\nvolume 97 of Proceedings of Machine Learning Research , pages 2319–2327. PMLR, 2019. URL\nhttp://proceedings.mlr.press/v97/gollapudi19a.html .\n[21] Anupam Gupta and Marco Molinaro. How the experts algorithm can help solve lps online. Math.\nOper. Res. , 41(4):1404–1431, 2016. doi: 10.1287/moor.2016.0782. URL https://doi.org/10.\n1287/moor.2016.0782 .\n[22] Rishi Gupta and Tim Roughgarden. A PAC approach to application-speciﬁc algorithm selection. SIAM\nJ. Comput. , 46(3):992–1017, 2017. doi: 10.1137/15M1050276. URL https://doi.org/10.\n1137/15M1050276 .\n14\n\n[23] Yanjun Han, Jiantao Jiao, and Tsachy Weissman. Minimax estimation of discrete distributions under\n`f1gloss. IEEE Transactions on Information Theory , 61:6343–6354, 2015.\n[24] Piotr Indyk, Frederik Mallmann-Trenn, Slobodan Mitrovic, and Ronitt Rubinfeld. Online page mi-\ngration with ML advice. CoRR , abs/2006.05028, 2020. URL https://arxiv.org/abs/2006.\n05028 .\n[25] Zhihao Jiang, Debmalya Panigrahi, and Kevin Sun. Online algorithms for weighted paging with pre-\ndictions. In Artur Czumaj, Anuj Dawar, and Emanuela Merelli, editors, 47th International Colloquium\non Automata, Languages, and Programming, ICALP 2020, July 8-11, 2020, Saarbr ¨ucken, Germany\n(Virtual Conference) , volume 168 of LIPIcs , pages 69:1–69:18. Schloss Dagstuhl - Leibniz-Zentrum\nf¨ur Informatik, 2020. doi: 10.4230/LIPIcs.ICALP.2020.69. URL https://doi.org/10.4230/\nLIPIcs.ICALP.2020.69 .\n[26] Bala Kalyanasundaram and Kirk Pruhs. An optimal deterministic algorithm for online b-matching.\nTheor. Comput. Sci. , 233(1-2):319–325, 2000. doi: 10.1016/S0304-3975(99)00140-1. URL https:\n//doi.org/10.1016/S0304-3975(99)00140-1 .\n[27] Sudeep Kamath, Alon Orlitsky, Dheeraj Pichapati, and Ananda Theertha Suresh. On learning distribu-\ntions from their samples. In Peter Gr ¨unwald, Elad Hazan, and Satyen Kale, editors, Proceedings of The\n28th Conference on Learning Theory , volume 40 of Proceedings of Machine Learning Research , pages\n1066–1100, Paris, France, 03–06 Jul 2015. PMLR. URL http://proceedings.mlr.press/\nv40/Kamath15.html .\n[28] Richard M. Karp, Umesh V . Vazirani, and Vijay V . Vazirani. An optimal algorithm for on-line bipartite\nmatching. In Harriet Ortiz, editor, Proceedings of the 22nd Annual ACM Symposium on Theory of\nComputing, May 13-17, 1990, Baltimore, Maryland, USA , pages 352–358. ACM, 1990. doi: 10.1145/\n100216.100262. URL https://doi.org/10.1145/100216.100262 .\n[29] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 International Conference on Management of Data , SIGMOD\n’18, pages 489–504, New York, NY , USA, 2018. ACM. ISBN 978-1-4503-4703-7.\n[30] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online scheduling\nvia learned weights. In Shuchi Chawla, editor, Proceedings of the 2020 ACM-SIAM Symposium\non Discrete Algorithms, SODA 2020, Salt Lake City, UT, USA, January 5-8, 2020 , pages 1859–\n1877. SIAM, 2020. doi: 10.1137/1.9781611975994.114. URL https://doi.org/10.1137/\n1.9781611975994.114 .\n[31] Thodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned advice. In\nJennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Ma-\nchine Learning , volume 80 of Proceedings of Machine Learning Research , pages 3302–3311, Stock-\nholmsm ¨assan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL http://proceedings.mlr.\npress/v80/lykouris18a.html .\n[32] Mohammad Mahdian, Hamid Nazerzadeh, and Amin Saberi. Online optimization with uncertain in-\nformation. ACM Trans. Algorithms , 8(1):2:1–2:29, 2012. doi: 10.1145/2071379.2071381. URL\nhttps://doi.org/10.1145/2071379.2071381 .\n[33] Aranyak Mehta, Amin Saberi, Umesh V . Vazirani, and Vijay V . Vazirani. Adwords and generalized\nonline matching. J. ACM , 54(5):22, 2007. doi: 10.1145/1284320.1284321. URL https://doi.\norg/10.1145/1284320.1284321 .\n15\n\n[34] Michael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching. In Advances\nin Neural Information Processing Systems 31: Annual Conference on Neural Information Processing\nSystems 2018, NeurIPS 2018, 3-8 December 2018, Montr ´eal, Canada. , pages 462–471, 2018.\n[35] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions, 2020.\n[36] Marco Molinaro and R. Ravi. Geometry of online packing linear programs. In Artur Czumaj, Kurt\nMehlhorn, Andrew M. Pitts, and Roger Wattenhofer, editors, Automata, Languages, and Programming\n- 39th International Colloquium, ICALP 2012, Warwick, UK, July 9-13, 2012, Proceedings, Part I ,\nvolume 7391 of Lecture Notes in Computer Science , pages 701–713. Springer, 2012. doi: 10.1007/\n978-3-642-31594-7 n59. URL https://doi.org/10.1007/978-3-642-31594-7_59 .\n[37] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ML\npredictions. In Advances in Neural Information Processing Systems 31: Annual Confer-\nence on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018,\nMontr ´eal, Canada. , pages 9684–9693, 2018. URL http://papers.nips.cc/paper/\n8174-improving-online-algorithms-via-ml-predictions .\n[38] Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In Shuchi\nChawla, editor, Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA\n2020, Salt Lake City, UT, USA, January 5-8, 2020 , pages 1834–1845. SIAM, 2020. doi: 10.1137/\n1.9781611975994.112. URL https://doi.org/10.1137/1.9781611975994.112 .\n[39] Leslie G. Valiant. A theory of the learnable. Commun. ACM , 27(11):1134–1142, 1984. doi: 10.1145/\n1968.1972. URL https://doi.org/10.1145/1968.1972 .\n[40] Alexander Wei. Better and simpler learning-augmented online caching. CoRR , abs/2005.13716, 2020.\nURLhttps://arxiv.org/abs/2005.13716 .\n[41] ´Etienne Bamas, Andreas Maggiori, Lars Rohwedder, and Ola Svensson. Learning augmented energy\nminimization via speed scaling, 2020.\n[42] ´Etienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning augmented\nalgorithms, 2020.\n16\n\nA Existence of Useful Weights for Max Flow in 3-layer DAGs\nIn this section, we show the existence of useful weights for 3-layer DAGs. We ﬁrst state the generalized\ntheorem for online ﬂow allocation on DAGs formally and give a reduction from DAGs to layered graphs\nsuch that if the weights in layered graphs satisfy some properties, the theorem can be proved. Then we focus\non the 3-layered case and give the detailed proof of Theorem 3.\nConsider a directed acyclic graph G= (fs;tg[V;E), where each vertex v2Vhas capacity Cv. Our\ngoal is to maximize the ﬂow sent from stotwithout violating any vertex capacity constraint. For any vertex\nv, letdvbe the longest distance from stov. Deﬁneduv:=dv\u0000du\u00001for each edge (u;v). We claim the\nfollowing theorem:\nTheorem 9. For any edge (u;v)2E, letxuvbe the proportion of ﬂow crossing this edge in all ﬂow received\nbyu. For any given \u000f2(0;1), there exists a weight \u000bvfor each vertex vsuch that we can obtain a (1\u0000\u000f)-\napproximate solution by setting the proportion of ﬂow out of uto neighborvto bexuv=\u000b1=(2n)duv\nv\nP\nv02Nu\u000b1=(2n)duv0\nv0;\nwherenis the number of vertices and Nuis the set of vertices pointed by u. Moreover, these weights can be\nobtained in a time of O(d2nd+1log(n=\u000f)=\u000f2)wheredis the diameter of this graph excluding vertex t.\nTo prove Theorem 9, we ﬁrst reduce our model to a maximum ﬂow problem in a d-layered graph where\nan algorithm must return the weights that obey certain structural properties. We then ﬁnd such weights in\nthed-layered graph in Appendix D.2 within the claimed time and use them to ﬁnd weights on the original\ngraph. We provide the key ingredients for proving the d-layered result by proving the 3-layered case in\nAppendix A.2 below.\nA.1 Reduction from DAGs to Layered DAGs\nFor any directed acyclic graph G= (fs;tg[V;E)with vertex capacities, we construct a d-layered DAG\n~G= (fs;tg[~V;~E)where all arcs go from one layer to the next and d:= maxv2Vdv.\nWe initialize dempty vertex layers between sandtinGr. Copy vertex tdtimes and add them to d\ndifferent layers. They all have inﬁnite capacities. Use ~tjto represent the copy in the j-th layer. For each\nvertexv2V, copydvtimes and add them to the previous dvlayers. Similarly, use ~vjto represent the copy\nin thej-th layer. We call the copy in the dv-th layer the real copy of vand other vertices the virtual copies of\nv. Set the capacity of the real copy to be Cvand the capacities of virtual copies to be inﬁnite: C~vdv:=Cv\nandC~vj=1forj <dv.\nNow we construct the edge set ~E. For eachv2V[ftg, connect its two copies in every two neighboring\nlayers. Use ~E1to represent the set of these edges. For each edge (u;v)2E, connect the real copy of uto\nthe copy ofvin the (du+ 1) -th layer. Use ~E2to represent the set of these edges. In other words, we add\n(~udu;~vdu+1)to~E2for each (u;v)2E. Let ~E:=~E1[~E2. Finally, remove all vertices that can not be\nreached bysin the new graph. According to our construction, the edges can only occur in two neighboring\nlayers, indicating that ~Gis an (s-t)d-layered graph (see Fig 1 as an illustration).\nClearly, any feasible s-tﬂow inGcorresponds to a feasible s-tﬂow in the new graph ~Gand vice-versa.\nThus, the values of the maximum ﬂow in these two graphs are the same.\nIn order to obtain the vertex weights in the original DAG, we compute the vertex weights in the reduced\nd-layered graph and transfer these weights back to the original graph. To be able to transfer the weights back\nto the original graph, we will need that the weights satisfy certain structural properties. In general, without\nthese properties the weights compute on the d-layered graph will not be useful for obtaining weights in the\noriginal graph.10\n10As mentioned in the Techniques section, the vertex weights for ~Gcan be computed by a simple algorithm. However, using that\n17\n\nFigure 1: An illustration of the reduction. On the left is the original graph Gand on the right is the new\nd-layered graph ~G. In graph ~G, blue vertices are the real copies while the white ones are virtual. The dotted\nvertices are the vertices removed in the last stage. We color the edges in ~E1orange and the edges in ~E2blue.\nNow we state two properties and show that if there exists weights for ~Gwith these two properties, then\nwe can prove Theorem 9. The key property is the virtual-weight dependence, which ensures that the weights\ngiven to copies of a node created in the reduction can be interpreted in the original graph.\nProperty 1 (Near optimality) .For thed-layered graph ~G, a set of vertex weights f\u000bvgis near optimal if we\ncan obtain a near optimal solution by setting xuv=\u000bvP\nv02Nu\u000bv0for each edge (u;v)2E.\nProperty 2 (Virtual-weight dependence) .For thed-layered graph ~G, a set of vertex weights f\u000bvghas\ndependent virtual weights if for any two neighboring copies ~vj;~vj+1of any vertex v2G, we have\u000b~vj=\n(\u000b~vj+1)\u001afor some\u001a>0.\nProof of Theorem 9 .Assume that we have a set of vertex weights f\u000bgof~Gwith the two properties. The\nvertex weightsf\fginGare constructed by letting \fv=\u000b~vdvfor eachv2V. Namely, for each vertex v, its\nweight\fvis the weight of v’s copy in ~G. Each edge (u;v)inGcorresponds to edge (~udu;~vdu+1)in~G. We\nshow the values of ﬂow crossing the edge in both graphs with their respective weights are the same. This\nwill prove that the weights f\fgreturn a near optimal solution in G.\nConsider any edge pair (u;v)and(~udu;~vdu+1). According to the allocation rules in Theorem 9, for the\noriginal graph G, we havexuv=\f\u001aduv\nvP\nw2Nu\f\u001aduw\nw;and according to the allocation rules in the near optimal\nproperty, for the reduced graph ~G, we havexu0\nduv0\ndu+1=\u000b~vdu+1 P\nw2Nu\u000b~wdu+1:For any two neighboring copies\n~vj;~vj+1of a vertexv,\u000b~vj= (\u000b~vj+1)\u001a, we have\u000b~vdu+1=\u000b\u001a(dv\u0000du\u00001)\n~vdv=\u000b\u001aduv\n~vdv=\f\u001aduv\nv:Thus, using\nweightsf\fvg, for each edge pair, we have xuv=x~udu~vdu+1, completing the proof of weight existence.\nTo ﬁnish the proof, we will have to argue the existence of weights with these above two properties in\nd-layered DAGs and that they can be computed in the time claimed in the theorem. We use \u001a= 1=(2n)to\nachieve these properties. We supply these proofs in Section D.2 in the Appendix to complete the proof.\nIn the following, we will show how to obtain the weights with the two properties for the special case of\n3-layered graphs.\nsimple algorithm, we may not be able to transfer these vertex weights back to the original graph. In particular they may not satisfy\nthe properties stated.\n18\n\nFigure 2: An (s-t)3-layered graph.\nA.2 Max Flow in 3-layered Graphs\nThis section gives an algorithm to compute the requisite weights in three-layered graphs and this uses many\nof the key ideas needed for general DAGs. If the reader is interested in an even simpler proof, we provide a\nsimpliﬁed algorithm to compute vertex weights for two-layered (bipartite) graphs in Appendix D.1.\nWe ﬁrst focus on the near optimality property (Theorem 3), showing that for any 3-layered graphs, our\nalgorithm returns near optimal weights. Then we claim that if the 3-layered graph is a reduction graph ~G, the\nvertex weights satisfy the virtual-weight dependence property. Particularly, in the proof of the virtual-weight\ndependence property, the parameter \u001ais set to be 1=2n.\nConsider an (s-t)3-layered graph G= (fs;tg[I[A[B;E)(see Fig 2). Each vertex i,a,bin theI\nlayer, theAlayer and the Blayer, respectively, have capacity 1,CaandCb.\nUseN(i;A),N(a;I),N(a;B)andN(b;A)to represent i’s neighborhood in A,a’s neighborhood in I,a’s\nneighborhood in Bandb’s neighborhood in the Arespectively. For each (i;a)2E, we usexi;ato denote\nthe proportion of the ﬂow sent from itoa. Similarly, we deﬁne ya;bfor each (a;b)2E. For each vertex v,\nuseAllocvto represent the total amount of ﬂow sent to it. The algorithm runs iteratively. We use superscript\n(t)to denote the value of variables in the end of iteration t. For example, let Alloc(t)\nvbe the value of\nAllocvin the end of iteration t. But for simplicity, we will use Allocvdirectly to represent Alloc(T)\nv. Let\n\u000fmax= max(\u000fA;\u000fB)and\u000fmin= min(\u000fA;\u000fB).\nOur framework is stated in Algo 1. We set weights for both layers and update them iteratively. Each\nvertexb2Bupdates its weight according to its allocation while each vertex a2Aupdates the weight, not\nonly because of itself (self-decrease), but also due to its neighborhood in B(forced-decrease). Cond is a\nBoolean function of N(a;B)for anya2A. Namely, only when N(a;B)satisﬁes some conditions (represented\nbyCond ), we let\u000bado forced-decrease. By deﬁning different \u000fA,\u000fBandCond , we can obtain different\nalgorithms.\nSince each weight decreases at most once per iteration, the potential minimum weight \u000bminin theA\nlayer and\fminin theBlayer are1\n(1+\u000fA)Tand1\n(1+\u000fB)Trespectively after Titerations. Then we can partition\nvertices inAandBinto several classes (we also call them levels in the following):\nB(k) :=fb2Bj\fb= (1 +\u000fB)k\fming;\nA(k) :=fa2Aj\u000ba= (1 +\u000fA)k\u000bming:\n19\n\nAlgorithm 1: The framework of the algorithm in (s\u0000t)3-layered graphs\nInput:G= (fs;tg[I[A[B;E),fCaga2A,fCbgb2B, parameter\u000fA;\u000fB\nInitialize\u000ba= 18a2Aand\fb= 18b2B.\nforiteration 1;2;:::;T do\nFor each (i;a)2Eand(a;b)2E, letxi;a=\u000baP\na02N(i;A)\u000ba0andya;b=\fbP\nb02N(a;B)\fb0.\nFor each vertex in the Alayer and the Blayer, letAlloca=P\ni2N(a;I)xi;aand\nAllocb=P\na2N(b;A)min(Alloca;Ca)ya;brespectively.\nforeachb2Bdo\nifAllocb>(1 +\u000fB)(1 +\u000fA)Cbthen\n\fb \fb=(1 +\u000fB).\nforeacha2Ado\nifAlloca>(1 +\u000fA)Cathen\n\u000ba \u000ba=(1 +\u000fA)/* Self-decrease */\nelse if Cond (N(a;B)) = True then\n\u000ba \u000ba=(1 +\u000fA)/* Forced-decrease */\nOutput:f\u000baga2Aandf\fbgb2B\nFor any vertex v, use Lev(v)to denote the level that it belongs to. Similarly, deﬁne Lev(t)(v)be the value\nofLev(v)in iterationt. Clearly, if Lev(v) =T, we know that the weight of vertex vhas not decreased so\nfar, while Lev(v) = 0 indicates that its weight decreased in every iteration.\nWe now introduce four properties such that for any algorithm under this framework, if it satisﬁes these\nfour properties, it will return a (1\u0000O(\u000f))-approximated solution when T= poly(n;\u000f)(nis the number of\nvertices).\nProperty 3 (Increasing monotonicity) .For any vertex vinA[B, in an iteration t, if its weight does not\ndecrease (i.e. \u000b(t)\na=\u000b(t\u00001)\na ), then we have Alloc(t)\nv\u0015Alloc(t\u00001)\nv .\nProperty 4 (Decreasing monotonicity) .For any vertex vinA[B, in an iteration t, if its weight decreases\n(i.e.\u000b(t)\na<\u000b(t\u00001)\na ), then we have Alloc(t)\nv\u0014Alloc(t\u00001)\nv .\nProperty 5 (Layer dominance) .For any vertex ain theAlayer, in any iteration t, there exists at least one\nvertexb2N(a;B)such that Lev(t)(b)\u0015Lev(t)(a).\nProperty 6 (Forced decrease exemption) .For any vertex ain theAlayer, in any iteration t, if there exists\none vertexb2N(a;B)withLev(t)(b) =Tor satisfying Lev(t)(b)\u0000Lev(t)(a)\u0015log(n=\u000fmax)=\u000fmin, then\nCond (N(a;B)) = False .\nThese properties will be used to show the following theorem:\nTheorem 10. For any algorithm under our framework with the four properties, if T=O(nlog(n=\u000fmax)\n\u000fmax\u000fmin), the\nalgorithm will return a (1\u0000O(\u000fmax))-approximated solution.\nTo prove Theorem 10, we consider a new graph G0, a smaller graph by removing some vertices and\nedges. We ﬁrst construct an s-tcut inG0. Then prove that the value of our solution in Gis at least\n(1\u0000O(\u000fmax))times the value of this cut, thus at least (1\u0000O(\u000fmax))times the maximum ﬂow in G0.\nFinally, we show that the optimal value in G0is close to that in G, completing the proof.\nWe now deﬁne some vertex sets in order to construct G0. Given any integer 1 +log(n=\u000fmax)\n\u000fmin\u0014`\u0014\nT\u00001\u0000log(n=\u000fmax)\n\u000fmin, we can deﬁne a gap in the Alayer:G(A) :=S`0\nk=`A(k), where`0=`+log(n=\u000fmax)\n\u000fmin.\n20\n\n(a)\n (b)\nFigure 3: Fig (a) is an illustration of the partition of the Alayer and the Blayer. RemovingG(A)andG(B)\ncan get the graph G0. Fig (b) is an illustration of the vertex cut in graph G0. According to our rules, all red\nvertices are added to C.\nAs shown in Fig 3a, all vertices in the Alayer are partitioned into three parts: G(A),G(A)\u0000andG(A)+,\nwhereG(A)\u0000:=S`\u00001\nk=0A(k)andG(A)+:=ST\nk=`0+1A(k). Similarly, we can deﬁne G(B),G(B)\u0000and\nG(B)+using the same `. The length of these two gaps are bothlog(n=\u000fmax)\n\u000fminin order to make sure that\nregardless of the layer, any vertex above the gap has weight at leastn\n\u000fmaxtimes than the weight of any vertex\nbelow the gap.\nThe graphG0is created by removing all vertices in G(A)andG(B)and all edges adjacent to them. We\ngive two rules to construct an s-tvertex cutC:\n1. For anys-tpath crossing both G(A)\u0000andG(B)\u0000, add the vertex bin theBlayer toC.\n2. LetG+=G(A)+[G(B)+. For anys-tpath crossing at least one vertex in G+, ﬁnd the ﬁrst vertex v\ninG+and add the vertex before vtoC.\nSee Fig 3b as an illustration. Observe that Cis a feasible s-tvertex cut, meaning that all s-tpaths inG0\nwill be blocked if removing C. UseG(A\u0000;B+)to represent the vertices in G(A)\u0000which are adjacent to at\nleast one vertex in G(B)+. Then according to the two rules, we can compute the value of this cut:\nC(C) =C(G(B)\u0000) +C(G(A\u0000;B+)) +jN(G(A)+;I)j:\nThe value of our solution can be computed easily using the ﬂow into t:\nVal =X\nb2G(B)\u0000min(Allocb;Cb) +X\nb2G(B)min(Allocb;Cb) +X\nb2G(B)+min(Allocb;Cb)\nIn the following, we ﬁrst show thatP\nb2G(B)\u0000min(Allocb;Cb)andC(G(B)\u0000)are close, and then estab-\nlish the relationship betweenP\nb2G(B)min(Allocb;Cb)+P\nb2G(B)+min(Allocb;Cb)andC(G(A\u0000;B+))+\njN(G(A)+;I)j:To prove the ﬁrst statement, we need the following lemma:\n21\n\nLemma 11. If the increasing monotonicity property holds, after iteration T,8b2ST\u00001\nk=0B(k), we have\nAllocb\u0015Cb.\nProof .For any vertex binST\u00001\nk=0B(k),\fbdecreased at least once. Consider the last iteration tthat it\ndecreased. In the beginning of that iteration, we have Alloc(t\u00001)\nb>(1 +\u000fB)(1 +\u000fA)Cb:Since in one\niteration, for any a2N(b;A),Allocadecreases at most (1 +\u000fA)andya;bdecreases at most (1 +\u000fB), we\nhaveAlloc(t)\nb\u0015Alloc(t\u00001)\nb\n(1+\u000fA)(1+\u000fB)> Cb:After iteration t,\fbdid not decrease. Due to Property 3, Allocbalso\ndid not decrease, thus ﬁnally, Allocb\u0015Alloc(t)\nb\u0015Cb:\nAccording to the lemma above, the ﬁrst statement can be proved easily:P\nb2G(B)\u0000min(Allocb;Cb) =P\nb2G(B)\u0000Cb=C(G(B)\u0000):To prove the second statement, we ﬁrst show that for any vertex b2G(B)[\nG(B)+,min(Allocb;Cb)is close toAllocb, and then analyze Allocbto complete this proof.\nLemma 12. If the decreasing monotonicity property holds, after iteration T,8b2ST\nk=1Bk, we have\nAllocb\u0014(1 + 5\u000fmax)Cb.\nProof .The basic idea of this proof is similar to the proof of Lemma 11. Clearly, for any vertex b2ST\nk=1B(k),\fbdid not decrease in every iteration. Consider the last iteration tthat it did not decrease. In\nthe beginning of that iteration, we have Alloc(t\u00001)\nb\u0014(1 +\u000fB)(1 +\u000fA)Cb:Since in one iteration, for any\na2N(b;A),Allocaincreases at most (1 +\u000fA)andya;bincreases at most (1 +\u000fB), we haveAlloc(t)\nb\u0014\nAlloc(t\u00001)\nb(1 +\u000fA)(1 +\u000fB)\u0014(1 + 5\u000fmax)Cb:After iteration t,\fbdecreased in every iteration. Due to\nProperty 4,Allocbdid not increase, thus ﬁnally, Allocb\u0014Alloc(t)\nb\u0014(1 + 5\u000fmax)Cb:\nAccording to the lemma above, for any vertex b2G(B)[G(B)+we obtain a relationship between\nmin(Allocb;Cb)andAllocb:\nmin(Allocb;Cb)\u0015Allocb\n1 + 5\u000fmax\u0015(1\u0000O(\u000fmax))Allocb\nNow we analyze Allocb. For any vertex binG(B)+[G(B), according to our assignment, we have:\nAllocb=X\na2Nbmin(Alloca;Ca)ya;b\nFor a vertex a2A, deﬁneN+\n(a;B)=N(a;B)\\(G(B)+[G(B)). Summing Allocbover allbin\nG(B)+[G(B), we have\nX\nb2G(B)+[G(B)Allocb=X\na2Amin(Alloca;Ca)X\nb2N+\n(a;B)ya;b\nAs mentioned above, the length of the gap islog(n=\u000fmax)\n\u000fmin, meaning that any vertex above the gap has\nweightn\n\u000fmaxtimes the weight of any vertex below the gap. If N(a;B)\\G(B)+6=;, the total proportion\nassigned toG(B)\u0000is at most\u000fmax. Namely,\nX\nb2N+\n(a;B)ya;b\u00151\u0000\u000fmax:\nThus, due to Property 5, we have\n22\n\nX\nb2G(B)+[G(B)Allocb\u0015X\na2G(A)+min(Alloca;Ca)(1\u0000\u000fmax)\n+X\na2G(A\u0000;B+)min(Alloca;Ca)(1\u0000\u000fmax)(1)\nApplying the same technique as in the proof of Lemma 11, we have the following lemma:\nLemma 13. If Property 3 holds, 8a2ST\u00001\nk=0A(k), if in the last iteration its weight did self-decrease, then\nwe haveAlloca\u0015Ca.\nAccording to Property 6, for any vertex a2G(A\u0000;B+),\u000badid self-decrease in the last iteration tthat\nit decreased, because there already existed a vertex b2NawithLev(t)(b)\u0000Lev(t)(a)large enough due to\nthe deﬁnition ofG(A\u0000;B+). Thus, we have\nX\na2G(A\u0000;B+)min(Alloca;Ca)(1\u0000\u000fmax)\u0015X\na2G(A\u0000;B+)Ca(1\u0000\u000fmax)\n= (1\u0000\u000fmax)C(G(A\u0000;B+)):\nLemma 14. If Property 4 holds, 8a2ST\nk=1A(k), we haveAlloca\u0014(1 + 3\u000fA)Ca.\nThis lemma can also be proved by the same technique as in the proof of Lemma 12. Thus, for any vertex\na2G(A)+[G(A), we have\nmin(Alloca;Ca)\u0015(1\u0000O(\u000fmax))Alloca:\nNow we start to analyze Alloca. Due to the gap in the Alayer, we know at most O(\u000fmax)proportion of\nﬂow fromN(G(A)+;I)is assigned toG(A)\u0000. Namely,\nX\na2G(A)+Alloca+X\na2G(A)Alloca\u0015(1\u0000O(\u000fmax))jN(G(A)+;I)j(2)\nCombining all related inequalities, we can obtain the relationship between the value of our solution and\nthe size of cutC:\nVal\u0015(1\u0000O(\u000fmax))C(C)\u0000(1\u0000O(\u000fmax))X\na2G(A)Alloca\n\u0015(1\u0000O(\u000fmax))OPT(G0)\u0000(1\u0000O(\u000fmax))X\na2G(A)Alloca\nIn the following, we will show that by selecting appropriate `, both OPT (G)\u0000OPT(G0)andP\na2G(A)Allocacan be made very small compared to Val. To prove this, we need to divide Ainto two\nsetsPandQﬁrst:\nP=fa2AjN(a;B)\\B(T)6=;g;\nQ=fa2AjN(a;B)\\B(T) =;g:\nLetG(P) =P\\G(A)andG(Q) =Q\\G(A).\nClearly, if we addG(A)andG(B)back to the graph, the maximum ﬂow will increase at most C(G(P))+\nC(N(G(Q);B)[G(B)). Namely,\nOPT(G)\u0000OPT(G0)\u0014C(G(P)) +C(N(G(Q);B)[G(B)):\n23\n\nCheckingP\na2G(A)Alloca, we can obtain the following inequality due to Lemma 14:\nX\na2G(A)Alloca=X\na2G(P)Alloca+X\na2G(Q)Alloca\n\u0014(1 +O(\u000fmax))C(G(P)) + (1 +O(\u000fmax))X\na2G(Q)min(Alloca;Ca)\nRecall thatP\na2G(Q)min(Alloca;Ca)is the total amount of ﬂow sent from G(Q)toN(G(Q);B). To\ncomplete the proof, we try to bound this value by C(N(G(Q);B)).\nAccording to Lemma 12, the ﬂow sent from G(Q)toN(G(Q);B)nB(0)can be bounded by C(N(G(Q);B)n\nB0), because for any vertex bin theBlayer except B(0), the total amount of ﬂow that it received is at most\n(1 +\u000fmax)Cb.\nThe range of `is[1 +log(n=\u000fmax)\n\u000fmin;T\u00001\u0000log(n=\u000fmax)\n\u000fmin]. According to Property 5, the proportion of ﬂow\nsent fromG(Q)toB(0)is very small, at most O(\u000fmax). Thus, the total amount of ﬂow sent from G(Q)to\nN(G(Q);B)can be bounded:\nX\na2G(Q)min(Alloca;Ca)\u0014(1 +O(\u000fmax))C(N(G(Q);B)):\nThen we have X\na2G(A)Alloca\u0014(1 +O(\u000fmax))(C(G(P)) +C(N(G(Q);B))):\nNow, if we prove that C(G(P)) +C(N(G(Q);B)[G(B))is at mostO(\u000fmax) Val , the whole proof is\ncompleted. We do this with a simple averaging argument.\nLemma 15. WhenT=O(nlog(n=\u000fmax)\n\u000fmax\u000fmin), there exist an appropriate `such that\nC(G(P)) +C(N(G(Q);B)[G(B))\u0014O(\u000fmax) Val\nProof .SummingC(G(P)) +C(N(G(Q);B)[G(B))over all potential `(noticeG(P)andG(Q)are deﬁned\nbyG(A)that depends on `), we have\nT\u00001\u0000log(n=\u000fmax )\n\u000fminX\n`=1+log(n=\u000fmax )\n\u000fminC(G(P)) +C(N(G(Q);B)[G(B))\n\u0014log(n=\u000fmax)\n\u000fminT\u00001X\nk=0C(P(k)) +nlog(n=\u000fmax)\n\u000fminT\u00001X\nk=0C(B(k))\nThis inequality holds because for each k2[0;T\u00001],C(P(k))occurs at mostlog(n=\u000fmax)\n\u000fmintimes and\nC(Q(k))occurs at mostnlog(n=\u000fmax)\n\u000fmintimes.\nDue to Property 6, every vertex in Ponly did self-decreases. Then according to Lemma 13 and\nLemma 11, we have\nT\u00001X\nk=0C(P(k)) +C(B(k))\u00142 Val\n24\n\nAlgorithm 2: Cond (N(a;B))\nLet\f(a)\nmax= maxb02N(a;B)\fb0,N\u0003\n(a;B):=fb2N(a;B)j\fb=\f(a)\nmaxg.\nif8b2N\u0003\n(a;B),\fbdecreases in this iteration and Lev(b)\u0000Lev(a)<log(n=\u000fmax)=\u000fminthen\nreturn True\nelse\nreturn False\nCombing the two inequalities above, we have\nT\u00001\u0000log(n=\u000fmax )\n\u000fminX\n`=1+log(n=\u000fmax )\n\u000fminC(G(P)) +C(N(G(Q);B)[G(B))\n\u00142nlog(n=\u000fmax)\n\u000fminVal\nTaking the average over all potential `,\n1\nT\u00002\u00002 log(n=\u000fmax)\n\u000fminT\u00001\u0000log(n=\u000fmax )\n\u000fminX\n`=1+log(n=\u000fmax )\n\u000fminC(G(P)) +C(N(G(Q);B)[G(B))\n\u00142nlog(n=\u000fmax)=\u000fmin\nT\u00002\u00002 log(n=\u000fmax)=\u000fminVal\n\u0014O(\u000fmax) Val;\nwhenT= \n(nlog(n=\u000fmax)\n\u000fmax\u000fmin).\nProof of Theorem 10 .Combining all related inequalities, we have the following inequality:\nVal\u0015(1\u0000O(\u000fmax))(OPT(G0)\u0000X\na2G(A)Alloca)\n\u0015(1\u0000O(\u000fmax))(OPT(G)\u0000(OPT(G)\u0000OPT(G0) +X\na2G(A)Alloca))\n\u0015(1\u0000O(\u000fmax))OPT(G)\u0000O(\u000fmax) Val\nThus, we get a (1\u0000O(\u000fmax))-approximate solution.\nWe give our Cond function in Algo 2. Our ﬁnal algorithm is designed by letting \u000fA=\u000fB=nand using\nthisCond function. In the following, we will show that our algorithm has the four properties mentioned\nabove.\nLemma 16. Increasing monotonicity (Property 3) holds if \u000fA=\u000fB=(2n)and we use Cond in Algo 2.\nProof .For any vertex a2A, if\u000badoes not decrease, all xi;awill not decrease. Since Alloca=P\ni:a2Nixi;a,Allocawill not decrease. But the situation is different in the Blayer.\nRecall the assignment rule of the Blayer. For any b2B, letAllocb=P\na2N(b;A)min(Alloca;Ca)ya;b.\nIf\fbdoesn’t decrease, all ya;bwill not decrease. However, some a2N(b;A),min(Alloca;Ca)may decrease.\nIn one iteration, min(Alloca;Ca)decreases at most (1 +\u000fA). If the claim below is proved, ya;bwill\nincrease at least (1 +\u000fB=n). Since\u000fA=\u000fB=(2n),Allocbwill not decrease, proving that Property 3 holds.\n25\n\nClaim 17. For any vertex a2N(b;A)in any iteration t, ifmin(Alloca;Ca)decreases,y(t)\na;b\u0015(1 +\n\u000fB=n)y(t\u00001)\na;b\nProof of Claim 17. For simplicity, in this proof, we use ya;bandy0\na;bto denotey(t\u00001)\na;bandy(t)\na;brespectively.\nIf\u000badoes a self-decrease, min(Alloca;Ca)does not decrease. So the only reason that min(Alloca;Ca)\ndecreases is that \u000bdoes a forced-decrease, indicating that for all b02N\u0003\n(a;B),\fb0decreases in this iteration\nusingCond in Algo 2. We show that their decrease is sufﬁcient to guarantee that the proportional allocation\nya;bincreases by enough to offset the decrease of Alloca.\ny0\na;b=\f0\nbP\nb02N(a;B)\f0\nb0\n=\f0\nbP\nb02N\u0003\n(a;B)\f0\nb0+P\nb02N(a;B)nN\u0003\n(a;B)\f0\nb0\n\u0015\fbP\nb02N\u0003\n(a;B)\fb0=(1 +\u000fB) +P\nb02N(a;B)nN\u0003\n(a;B)\fb0\n=(1 +\u000fB)\fbP\nb02N\u0003\n(a;B)\fb0+ (1 +\u000fB)P\nb02N(a;B)nN\u0003\n(a;B)\fb0\n=(1 +\u000fB)P\nb02N(a;B)\fb0\nP\nb02N\u0003\n(a;B)\fb0+ (1 +\u000fB)P\nb02N(a;B)nN\u0003\n(a;B)\fb0\u0001\fbP\nb02N(a;B)\fb0\n= (1 +\u000fBP\nb02N\u0003\n(a;B)\fb0\nP\nb02N\u0003\n(a;B)\fb0+ (1 +\u000fB)P\nb02N(a;B)nN\u0003\n(a;B)\fb0)ya;b\n\u0015(1 +\u000fB\nn)ya;b\nLemma 18. Decreasing monotonicity (Property 4) holds if \u000fA=\u000fB=(2n)and we use Cond in Algo 2.\nProof .This property can be proved similarly. For any vertex a2A, if\u000badecreases, all xi;awill not\nincrease, so Allocawill not increase.\nFor any vertex b2B, when\fbdecreases, all ya;bwill not increase, but for some a2N(b;A),\nmin(Alloca;Ca)may increase. We give a similar claim to prove this property:\nClaim 19. For any vertex a2N(b;A)in any iteration t, ifmin(Alloca;Ca)increases, then y(t)\na;b\u0014\n1\n(1+\u000fB=n)y(t\u00001)\na;b\nProof .For simplicity, in this proof, we use ya;bandy0\na;bto denotey(t\u00001)\na;bandy(t)\na;brespectively. The increase\nofmin(Alloca;Ca)indicates that \u000badoes not change in this iteration. If \u000badoes not decrease, we know\nthat either Lev(N\u0003\n(a;B))\u0000Lev(a)\u0015log(n=\u000fmax)=\u000fmin, or9b\u00032N\u0003\n(a;B), such that\fb\u0003does not decrease.\nIfLev(N\u0003\n(a;B))\u0000Lev(a)\u0015log(n=\u000fmax)=\u000fmin, the last iteration that \u000badecreased is due to it-\nself. If the weights of aandN\u0003\n(a;B)decreased together, Lev(N\u0003\n(a;B))\u0000Lev(a)would still be less than\nlog(n=\u000fmax)=\u000fmin. Thus, due to Lemma 13, Alloca\u0015Ca, indicating that min(Alloca;Ca)does not in-\ncrease even if Allocaincreases.\n26\n\nFor the second case, we employ the similar technique in the proof of Claim 17:\ny0\na;b=\f0\nbP\nb02N(a;B)\f0\nb0\n=\f0\nb\n\f0\nb\u0003+P\nb02N(a;B);b06=b\u0003\f0\nb0\n\u0014\fb=(1 +\u000fB)\n\fb\u0003+P\nb02N(a;B);b06=b\u0003\fb0=(1 +\u000fB)\n=\fb\n(1 +\u000fB)\fb\u0003+P\nb02N(a;B);b06=b\u0003\fb0\n=P\nb02N(a;B)\fb0\n(1 +\u000fB)\fb\u0003+P\nb02N(a;B);b06=b\u0003\fb0\u0001\fbP\nb02N(a;B)\fb0\n=ya;b\n1 +\u000fB\fb\u0003P\nb02N(a;B)\fb0\n\u0014ya;b\n1 +\u000fB=n\nLemma 20. Layer dominance (Property 5) holds if we use Cond in Algo 2.\nProof .Assume that8b2N\u0003\n(a;B),Lev(b)<Lev(a). Consider the ﬁrst iteration that this situation occurs.\nClearly, in the beginning of that iteration, Lev(N\u0003\n(a;B)) = Lev(a)and in that iteration, \u000badid not decrease\nwhile8b2N\u0003\n(a;B),\fbdecreased, contradicting our algorithm. Thus, there must exist at least one b2N(a;B)\nsuch that Lev(b)\u0015Lev(a).\nLemma 21. Forced decrease exemption (Property 6) holds if we use Cond in Algo 2.\nThis lemma can be proved directly from the description of our algorithm. Since our ﬁnal algorithm\nsatisﬁes the four properties, according to Theorem 10, when Tis large enough, it will return a near-optimal\nsolution. We complete the last piece of the proof for Theorem 3 by giving the following lemma for the\nrunning time:\nLemma 22. If\u000fA=\u000f=(2n)and\u000fB=\u000f, the running time of our algorithm is O(n4log(n=\u000f)=\u000f2).\nProof .According to Theorem 10, the number of iterations is O(nlog(n=\u000fmax)\n\u000fmax\u000fmin). Since\u000fA=\u000f=(2n)and\n\u000fB=\u000f, the number of iterations is O(n2log(n=\u000f)\n\u000f2). In each iteration, we need to compute xuvfor each edge\ninGand update the weight of each vertex. Thus, the running time of an iteration is O(n2), completing this\nproof.\nFinally, we prove the virtual-weight dependence of these weights.\nTheorem 23. Under the framework of Algo 1, if we let \u000fA=\u000fB=(2n)and use Cond in Algo 2, given an\n(s-t)3-layered reduction graph ~G, for any two neighboring copies ~vj;~vj+1of any vertex v, we have\n\u000b~vj+1= (\u000b~vj)2n:\n27\n\nProof .Clearly, the weights of the Ilayer can be arbitrary. So we only need to consider the case that ~vj;~vj+1\nare in theAlayer and the Blayer respectively. For simplicity, use aandbto denote these two vertices.\nSinceais a virtual copy, it has inﬁnite capacity. According to our framework, it can only do forced-\ndecrease.\nRecall the construction of ~E1and~E2in the reduction graph. Vertex bis the only neighbor of ain the\nBlayer. Thus, due to Algo 2, any time, if \fbdecreases,\u000baalso decreases. Assuming that \fbdecreasesk\ntimes, we have\nlog\fb\nlog\u000ba= log(1 +\u000fB)(\u0000k)=log(1 +\u000fA)(\u0000k)\n= log(1 +\u000fB)=log(1 +\u000fA)\n= log(1 + 2n\u000fA)=log(1 +\u000fA)\n\u0019log(1 +\u000fA)2n=log(1 +\u000fA)\n=2n;\ncompleting this proof.\nUsing Theorem 10 and Theorem 23, if the reduction graph ~Gis(s-t)3-layered DAG, our algorithm\nreturns a set of vertex weights with near optimality and virtual-weight dependence, indicating a set of good\nvertex weights for the original directed acyclic graph. Moreover, these weights can be obtained in time\nO(n4log(n=\u000f)=\u000f2)by Lemma 22 completing the analysis for the 3-layered DAGs.\nThe basic algorithmic framework, as well as the properties and the cut constructing rules can be general-\nized to more general d-layered graphs smoothly. We give the algorithm and the proof for general d-layered\ngraphs in Appendix D.2.\nB Learnability of Predictions for Online Flow Allocation in 2-layered\ngraphs\nIn this section, we consider the learnability of the vertex weights. We ﬁrst introduce our formal deﬁnition of\nlearnability. Given a DAG G, let\u0005be a class of instances of the online ﬂow allocation problem on the ﬁxed\ngraphG. That is, each I2\u0005represents a different sequence of impressions. There is also an unknown\ndistributionDover instance \u0005, and we can access independent samples from this distribution. Our goal is\nto ﬁnd the best set of weights for this distribution.\nMore precisely, we follow the deﬁnition proposed by Gupta and Roughgarden [22] for application spe-\nciﬁc algorithm selection.\nDeﬁnition 24. A learning algorithm L(\u000f;\u000e)-learns the optimal algorithm in an algorithm set Aif for any\ndistributionDover the instance set \u0005, givensinstancesI1;I2;:::;Is\u0018D , with probability at least 1\u0000\u000e,\nLoutputs an algorithm ^A2A such that\njEI\u0018D[Obj( ^A;I)]\u0000EI\u0018D[Obj(A\u0003;I)]j\u0014\u000f;\nwhere Obj(A;I)is the objective value obtained by algorithm Ain instanceIandA\u0003is the algorithm in A\nwith optimal EI\u0018D[Obj(A;I)].\nConsider the simplest strategy that uses the predicted weights directly to send ﬂow for a given instance.\nThis strategy can be viewed as an algorithm set A, where each set of predicted weights f\u000bgcorresponds to\nan algorithm A(\u000b)2A. Thus in our setting we are interested in the case when Obj(A(\u000b);I) =R(\u000b;I).\nRecall thatR(\u000b;I)is the amount of ﬂow we route to the sink using weights \u000bon impression set I. We say\n28\n\nAlgorithm 3: Learning Algorithm\nInput:\u000f2(0;1),ssampled impression set I1;I2;:::;Is\nConstruct a new impression set ^I, where the number of impressions with each type is its (rounded)\nmean value in the ssamples.\nCompute the (1\u0000\u000f)-approximate weights f^\u000bgfor this new instance.\nOutput:f^\u000bg\nthese vertex weights are PAC-learnable if there exists a learning algorithm Lthat(\u000f;\u000e)-learns the optimal\nalgorithm inA.\nThe case of general distributions is challenging for our problem. In particular, we cannot use some\nstandard approaches for general distributions since for ﬁxed I,R(\u000b;I)is neither a convex nor concave\nfunction of the weights \u000b. Thus we restrict the class of distributions considered\nOur results hold for the case when the distribution Dis a product distribution, i.e. D=D1\u0002D 2\u0002\n:::\u0002Dmand eachDiis an independent distribution over impressions. Such distributions have been studied\nin the context of self improving algorithms [3]. To simplify the presentation below, we focus on the i.i.d.\ncase. Below we let Dbe a ﬁxed unknown distributions over impressions and we are interested in instances\nIsampled according to D\u0002D\u0002:::\u0002D =Dm. We note that the proofs easily generalize to the case of\nmore general product distributions, as we mainly require independence across impressions.\nTheorem 25. Assume that for any instance I2\u0005, each impression is i.i.d. sampled from an unknown\ndistributionD. Namely, an instance Iis sampled from the distribution Dm. Under some mild assumptions,\nfor any\u000f;\u000e2(0;1), there exists a learning algorithm such that, after observing O(n2\n\u000f2ln(nlogn\n\u000e))instances,\nit will return a set of weights f^\u000bg, satisfying that with probability at least 1\u0000\u000e,\nEI\u0018Dm[R(^\u000b;I)]\u0015(1\u0000\u000f)EI\u0018Dm[R(\u000b\u0003;I)]\nwhereR(\u000b;I)is the value of the fractional ﬂow obtained by applying \u000bto instanceIand\n\u000b\u0003= arg max\n\u000bEIsDm[R(\u000b;I)].\nTo prove this theorem for DAG’s and d-layered graphs we need to make some assumptions about how\nwell the optimal solution saturates the internal vertices of the graph. Intuitively, we need to disallow vertices\nthat the optimal solution only sends a negligible amount of ﬂow through. See Appendix E for a formal\ndiscussion.\nUsedto denote the diameter of the graph G. In this section, we focus on the proof of the learnability for\nthe case that d= 2. Whend= 2, this problem can be seen as the maximum cardinality bipartite matching\nproblem. The graph excluding the sink tconsists of two layer IandA, whereAis given initially and Iis\nsampled fromDm. We claim the following theorem.\nTheorem 26. Assume that for any instance I2\u0005, each impression is i.i.d. sampled from an unknown\ndistributionD. Given any \u000f;\u000e2(0;1), if any vertex a2Ahas a capacity Ca\u0015poly(1=\u000f), there exists a\nlearning algorithm such that, after observing O(n2\n\u000f2ln(nlogn\n\u000e))instances, it will return a set of weights f^\u000bg,\nsatisfying that with probability at least 1\u0000\u000e,\nEI\u0018Dm[R(^\u000b;I)]\u0015(1\u0000\u000f)EI\u0018Dm[R(\u000b\u0003;I)]:\nWe ﬁrst introduce our learning algorithm in Algorithm 3. The algorithm is very simple: we construct a\nnew instance by averaging over all impressions from srandomly sampled instances, and compute its weights\nasf^\u000bg.\n29\n\nWe start by deﬁning some notations. For an instance Iand a set of weights f\u000bg, letAlloca(\u000b;I)be the\nnumber of the impressions assigned to aand letRa(\u000b;I) := min(Alloca(\u000b;I);Ca)to represent advertiser\na’s real contribution to the objective value.\nSincef^\u000bgis(1\u0000\u000f)-approximate for instance ^I, we have\nR(^\u000b;^I)\u0015(1\u0000\u000f)OPT(^I)\u0015(1\u0000\u000f)R(\u000b\u0003;^I);\nwhere OPT (^I)is the optimal value of instance ^I.\nThus, if we prove the following two inequalities:\nR(\u000b\u0003;^I)\u0015(1\u0000O(\u000f))EI\u0018Dm[R(\u000b\u0003;I)]; (3)\nand\nEI\u0018Dm[R(^\u000b;I)]\u0015(1\u0000O(\u000f))R(^\u000b;^I);\nTheorem 26 can be obtained directly.\nWe consider these two inequalities one by one. By the deﬁnition of R(\u000b;I), we have\nR(\u000b;I) =X\na2Amin(Alloca(\u000b;I);Ca):\nDue to the concavity of the min function and Jensen’s inequality, for any weights f\u000bg\nR(\u000b;E[I]) =X\na2Amin(E[Alloca(\u000b;I)];Ca)\u0015X\na2AE[min(Alloca(\u000b;I);Ca)] =E[R(\u000b;I)]: (4)\nNow if we can prove R(\u000b;E[I])andR(\u000b;^I)are close, Eq (3) can be proved.\nLemma 27. Given any\u000f>0,\u000e2(0;1]and vertex weights f\u000bg, if the number of instances sis no less than\nO(n\n\u000f2ln(n\n\u000e)), with probability at least 1\u0000\u000e, ,\njR(\u000b;E[I])\u0000R(\u000b;^I)j\u0014O(\u000f)R(\u000b;E[I])\nProof .Consider a vertex a2A. Due to the property of the min function, we only need to show that\nAlloca(\u000b;^I)is close to E[Alloca(\u000b;I)]. Since each impression is i.i.d. sampled, sAlloca(\u000b;^I)can be\nviewed as the sum of smi.i.d. random variables xi;a(\u000b)2[0;1]. For simplicity, let \u0016=E[Alloca(\u000b;I)].\nEmploying Chernoff’s inequality, we have\nPr[jAlloca(\u000b;^I)\u0000\u0016j\u0015\u000f\u0016+\u000f=n]\n= Pr[jsAlloca(\u000b;^I)\u0000s\u0016j\u0015(\u000f+\u000f=(n\u0016))s\u0016]\n\u00142 exp(\u0000(\u000f+\u000f=(n\u0016))2s\u0016=4)\n= 2 exp(\u0000\u000f2(1 + 2=(n\u0016) + 1=(n\u0016)2)s\u0016=4)\n= 2 exp(\u0000\u000f2(\u0016+ 1=(n2\u0016) + 2=n)=4)s\n\u00142 exp(\u0000s\u000f2=n)\nIf for eacha2A, we have\njAlloca(\u000b;^I)\u0000E[Alloca(\u000b;I)]j\u0014\u000fE[Alloca(\u000b;I)] +\u000f=n; (5)\nthen according to the property of min function, we will get\njmin(Alloca(\u000b;^I);Ca)\u0000min(E[Alloca(\u000b;I)];Ca)j\u0014\u000fmin(E[Alloca(\u000b;I)];Ca) +\u000f=n:\n30\n\nSumming the above over the nimpressions, we have\njR(\u000b;E[I])\u0000R(\u000b;^I)j\u0014X\na2Ajmin(Alloca(\u000b;^I);Ca)\u0000min(E[Alloca(\u000b;I)];Ca)j\n\u0014X\na2A\u000fmin(E[Alloca(\u000b;I)];Ca) +\u000f=n\n=\u000fR(\u000b;E[I]) +\u000f\nIt is reasonable to assume that the number of satisﬁed impressions is at least 1. So if for each a2A,\nEq (5) holds, we have\njR(\u000b;E[I])\u0000R(\u000b;^I)j\u0014\u000fR(\u000b;E[I]) +\u000f\n\u0014O(\u000f)R(\u000b;E[I])\nThus, we can bound the probability of the event that jR(\u000b;E[I])\u0000R(\u000b;^I)j\u0015O(\u000f)R(\u000b;E[I]):\nPr[jR(\u000b;E[I])\u0000R(\u000b;^I)j\u0015O(\u000f)R(\u000b;E[I])]\u0014Pr[9a2A;jAlloca(\u000b;^I)\u0000\u0016j\u0015\u000f\u0016+\u000f=n]:\nDue to the union bound,\nPr[9a2A;jAlloca(\u000b;^I)\u0000\u0016j\u0015\u000f\u0016+\u000f=n]\u00142nexp(\u0000s\u000f2=n)\nLetting\u000ebe2nexp(\u0000s\u000f2=n), we obtains=O(n\n\u000f2ln(n\n\u000e)), completing this proof.\nTo prove Theorem 26, we need to show that the above lemma holds, not just for a single ﬁxed set of\nweights, but any set of weights that could be output by the learning algorithm. This can be accomplished\nby setting\u000ein the above lemma appropriately and then applying a union bound over all sets of weights.\nAccording to the weight computing algorithm, the number of potential weight sets is O((logn)n). Thus to\nmake the union bound argument go through, we need to let the number of samples be O(n2\n\u000f2ln(nlogn\n\u000e))in\nTheorem 26.\nCombining Lemma 27 and Eq (4), the ﬁrst inequality Eq (3) can be proved.\nBy Lemma 27, the second inequality in Equation B holds if we prove the following lemma.\nLemma 28. For any\u000f2(0;1)and anya2A, we have\nE[Ra(^\u000b;I)]\u0015(1\u0000O(\u000f)) min(E[Alloca(^\u000b;I)];Ca)\nifCa= \n(1\n\u000f2(ln1\n\u000f)).\nProof .We divide the proof into three cases based on the value of E[Alloca(^\u000b;I)]:\n(1) when E[Alloca(^\u000b;I)]\u0015(1\u0000\u000f)Ca\n(2) when 1\u0014E[Alloca(^\u000b;I)]<(1\u0000\u000f)Ca\n(3) when E[Alloca(^\u000b;I)]<1\nCase 1 According to the deﬁnition of the expectation, we can do the following expansion:\nmin(E[Alloca(^\u000b;I)];Ca)\u0000E[Ra(^\u000b;I)]\n= min(E[Alloca(^\u000b;I)];Ca)\u0000X\nI2\u0005Pr[I]Ra(^\u000b;I)\n=X\nI2\u0005Pr[I](min(E[Alloca(^\u000b;I)];Ca)\u0000Ra(^\u000b;I))\n31\n\nIntuitively, we want to show that for most instance, min(E[Alloca(^\u000b;I)];Ca)\u0000Ra(^\u000b;I)is small. We\nﬁrst construct a set of good instances with small min(E[Alloca(^\u000b;I)];Ca)\u0000Ra(^\u000b;I), and then prove the\nprobability that instance Idoes not belong to this set is very small. More speciﬁcally, we deﬁne a set of\ngood instancesG=fIjAlloca(^\u000b;I)\u0015(1\u00002\u000f)Cag. Clearly, for each instance I2G,\nmin(E[Alloca(^\u000b;I)];Ca)\u0000Ra(^\u000b;I)\u00142\u000fCa\u00142\u000f\n1\u0000\u000fmin(E[Alloca(^\u000b;I)];Ca)\nThe remaining part is to show that Pr[I =2G]is very small. Employing Chernoff’s inequality, we have\nPr[I =2G]\n= Pr[Alloca(^\u000b;I)<(1\u00002\u000f)Ca]\n\u0014Pr[Alloca(^\u000b;I)<1\u00002\u000f\n1\u0000\u000fE[Alloca(^\u000b;I)]]\n\u0014exp(\u00001\n2(\u000f\n1\u0000\u000f)2E[Alloca(^\u000b;I)])\n\u0014exp(\u0000\u000f2\n2(1\u0000\u000f)Ca)\nWhenCa\u00152(1\u0000\u000f)\n\u000f2ln(1\n\u000f),Pr[I =2G]is at most\u000f.\nCombining the two inequalities above, we complete the proof of this case:\nmin(E[Alloca(^\u000b;I)];Ca)\u0000E[Ra(^\u000b;I)]\n=X\nI2\u0005Pr[I](min(E[Alloca(^\u000b;I)];Ca)\u0000Ra(^\u000b;I))\n=X\nI2GPr[I](min(E[Alloca(^\u000b;I)];Ca)\u0000Ra(^\u000b;I))\n+X\nI=2GPr[I](min(E[Alloca(^\u000b;I)];Ca)\u0000Ra(^\u000b;I))\n\u00142\u000f\n1\u0000\u000fmin(E[Alloca(^\u000b;I)];Ca) + Pr[I =2G] min(E[Alloca(^\u000b;I)];Ca)\n\u0014O(\u000f) min(E[Alloca(^\u000b;I)];Ca):\nCase 2 In this case, since E[Alloca(^\u000b;I)]is less than (1\u0000\u000f)Ca, we have\nmin(E[Alloca(^\u000b;I)];Ca) =E[Alloca(^\u000b;I)]:\nThus,\nmin(E[Alloca(^\u000b;I)];Ca)\u0000E[Ra(^\u000b;I)]\n=E[Alloca(^\u000b;I)]\u0000E[Ra(^\u000b;I)]\n=Z1\n0Pr[Alloca(^\u000b;I) =t](Alloca(^\u000b;I)\u0000Ra(^\u000b;I))dt:\nNote thatRa(^\u000b;I) = min(Alloca(^\u000b;I);Ca). Clearly, ifAlloca(^\u000b;I)\u0014Ca,Alloca(\u000b;I)\u0000Ra(^\u000b;I) =\n0. Otherwise, Alloca(^\u000b;I)\u0000Ra(^\u000b;I) =Alloca(^\u000b;I)\u0000Ca. Thus,\nmin(E[Alloca(^\u000b;I)];Ca)\u0000E[Ra(^\u000b;I)]\n=Z1\nCaPr[Alloca(^\u000b;I) =t](t\u0000Ca)dt\n=Z1\nCaPr[Alloca(^\u000b;I)\u0015t]dt:\n32\n\nThe remaining part is to show that for any t>Ca,Pr[Alloca(^\u000b;I)\u0015t]is very small. For any t>Ca,\nwe can ﬁnd a \r\u0015\u000f\n1\u0000\u000fsuch thatt= (1 +\r)E[Alloca(^\u000b;I)]. Employing Chernoff’s inequality again, we\ncan obtain\nPr[Alloca(^\u000b;I)\u0015t]\n=Pr[Alloca(^\u000b;I)\u0015(1 +\r)E[Alloca(^\u000b;I)]]\n\u0014exp(\u0000E[Alloca(^\u000b;I)]\r2=4)\n= exp(\u0000\r2\n4(1 +\r)t)\n\u0014exp(\u0000\u000f2\n4(1\u0000\u000f)t):\nFor simplicity, we let \u001a=\u000f2\n4(1\u0000\u000f), thus\nmin(E[Alloca(^\u000b;I)];Ca)\u0000E[Ra(^\u000b;I)]\n=Z1\nCaPr[Alloca(^\u000b;I)\u0015t]dt\n\u0014Z1\nCaexp(\u0000\u001at)dt\n=1\n\u001aexp(\u0000\u001aCa):\nWhenCa\u00151\n\u001aln(1\n\u001a\u000f), we can complete the proof of this case:\nmin(E[Alloca(^\u000b;I)];Ca)\u0000E[Ra(^\u000b;I)]\u0014\u000f\u0014\u000fmin(E[Alloca(^\u000b;I)];Ca):\nCase 3 In this case, E[Alloca(^\u000b;I)]is very small, less than 1, so we still have\nmin(E[Alloca(^\u000b;I)];Ca) =E[Alloca(^\u000b;I)]:\nIntuitively, when E[Alloca(^\u000b;I)]is very small, the contribution of each impression itoE[Alloca(^\u000b;I)]and\nE[Ra(^\u000b;I)]should be very close. For each impression i, use random variable xi;a(^\u000b)2[0;1]to represent\nthe space occupied by impression iin advertiser a. Clearly,Alloca(^\u000b;I) =P\ni2Ixi;a(^\u000b).\nTo analyze each impression’s contribution to Ra(^\u000b;I), we introduce a new random variable yi;a(^\u000b) =\nxi;a(^\u000b)\u00011fAlloc\u0000i\na(^\u000b;I)\u0014Ca\u00001g, whereAlloc\u0000i\na(^\u000b;I) =P\ni02I;i06=ixi0;a(^\u000b), representing the load of vertex a\nwithouti, and1fAlloc\u0000i\na(^\u000b;I)\u0014Ca\u00001gis an indicator of the event that the load of vertex awithoutiis less than\nCa\u00001. Clearly, we have\nRa(^\u000b;I) = min(Alloca(^\u000b;I);Ca)\u0015X\ni2Iyi;a(^\u000b):\nThis equation holds because if Alloc\u0000i\na(^\u000b;I)\u0014Ca\u00001, impression i’s contribution to Ra(^\u000b;I)is exactly\nxi;a(^\u000b)sinceAlloca(^\u000b;I)must be less than Ca, or otherwise, yi;a(^\u000b) = 0 . Taking the expectation of both\nsides, we have\nE[Ra(^\u000b;I)]\u0015X\ni2IE(yi;a(^\u000b)) =X\ni2IE[xi;a(^\u000b)\u00011fAlloc\u0000i\na(^\u000b;I)\u0014Ca\u00001g]:\n33\n\nSince each impression is sampled independently, random variable xi;a(^\u000b)and1fAlloc\u0000i\na(^\u000b;I)\u0014Ca\u00001gare in-\ndependent. Thus,\nE[Ra(^\u000b;I)]\u0015X\ni2IE[xi;a(^\u000b)\u00011fAlloc\u0000i\na(^\u000b;I)\u0014Ca\u00001g]\n=X\ni2IE[xi;a(^\u000b)]\u0001E[1fAlloc\u0000i\na(^\u000b;I)\u0014Ca\u00001g]\n=X\ni2IE[xi;a(^\u000b)]\u0001Pr[Alloc\u0000i\na(^\u000b;I)\u0014Ca\u00001]\nUsing Markov inequality to estimate Pr[Alloc\u0000i\na(^\u000b;I)>Ca\u00001], we can obtain\nPr[Alloc\u0000i\na(^\u000b;I)>Ca\u00001]\u0014E(Alloc\u0000i\na(^\u000b;I))\nCa\u00001\n\u00141\nCa\u00001\nWhenCa\u00151\n\u000f+ 1, this probability is at most \u000f. Combining the above two inequalities, we can complete the\nproof of this case:\nE[Ra(^\u000b;I)] =X\ni2IE[xi;a(^\u000b)]\u0001Pr[Alloc\u0000i\na(^\u000b;I)\u0014Ca\u00001]\n\u0015X\ni2IE[xi;a(^\u000b)]\u0001(1\u0000\u000f)\n= (1\u0000\u000f)E[Alloca(^\u000b;I)]:\nC Instance Robustness of Predictions for Online Flow Allocation\nIn this section we consider the online ﬂow allocation problem. Recall that this problem is deﬁned on a DAG\nG= (V[t;E). Each vertex v2Vhas a capacity Cvand the distinguished vertex tis a sink in the DAG,\ni.e.thas no outgoing arcs and every node can reach the sink.\nThe structure of the graph Gis known ofﬂine. Source vertices arrive online and reveal their outgoing\narcs connecting to the rest of G. Recall that source vertices have no incoming arcs. We refer to these online\nverticesIas impressions. When i2Iarrives, it is connected to a subset of vertices Ni\u0012V. At the time of\narrival, the algorithm must irrevocably decide a (fractional) ﬂow of value at most 1 from itotwhile obeying\nthe vertex capacities (taking into account ﬂow from other impressions). The goal is to allocate ﬂow online\nto maximize the total ﬂow that reaches t.\nWe study this online problem in the presence of predictions in two different models. In the ﬁrst model,\nwe assume that we can predict the instance directly and bound the performance by the instance predicted\nerror. In the second, we are given access to predictions of weights for a proportional allocation scheme,\nwhich were shown to exist in Section A. Moreover, we state a worst-case bound for integral version of this\nonline problem to show that our algorithm is really competitive.\nWe focus on the instance robustness here. The results for the parameter robustness and the worst-case\nbound are presented in Appendix F.\nConsider an instance of the online ﬂow allocation problem, consisting of an ofﬂine DAG G= (V[t;E)\nand a set of impressions arriving online. Let ibe an impression type and mibe the number of impressions\nin this type. An instance Ican be denoted by a vector <m 1;:::;mi;:::> .\n34\n\nHere we consider online proportional algorithms with predictions of the entire instance ^I. We consider\nthe error in these predictions to be given by the `1norm:\n\r:=jj^I\u0000Ijj;\nwhereIis the real instance. Learning such an instance from past data under this notion of error is well\nunderstood [23, 27]. In order to convert these predictions into our algorithm, we use the predicted instance\nto compute the weights for a proportional allocation scheme.\nRecall the results for the instance robustness.\nTheorem 29 (Theorem 5 Restated) .For any constant \u000f > 0, if a set of weights ^\u000breturns a (1\u0000\u000f)-\napproximated solution in instance ^I, it yields an online ﬂow allocation in instance Iwhose value is at least\nmaxf(1\u0000\u000f)OPT\u00002\r;OPT=(d+ 1)g:Here OPT is the maximum ﬂow value, dis the diameter of this\ngraph excluding vertex t, and\ris the difference between two instances, deﬁned by jj^I\u0000Ijj1.\nGiven a predicted instance ^I, we can compute a set of optimal weights f^\u000bvgfor it.\nTo prove Theorem 29, we ﬁrst show that if use these weights f^\u000bvgdirectly on instance I, our competitive\nratio can be bounded by 2\r. Then claim that we can always design a new algorithm that is never worse than\nthe performance of the previous policy and a (1=(d+ 1)) factor of the optimal.\nTheorem 30. For any constant \u000f > 0, if a set of weights ^\u000breturns a (1\u0000\u000f)-approximated solution in\ninstance ^I, routing ﬂow proportionally according to f^\u000bvgin instanceIreturns a feasible ﬂow whose value\nis at least\n(1\u0000\u000f)OPT\u00002\r;\nwhere OPT is the the maximum ﬂow value of instance I.\nProof .Add a source sinto the instance (^I;G), which is adjacent to each i2^I. The new graph is denoted\nby^G. According to the reduction in Section A, w.l.o.g., we can assume ^Gis an (s-t)d-layered graph. Use\nVal(^\u000b;^G)to represent the value of ﬂow if routed according to f^\u000bvgon graph ^G. Similarly, we can deﬁne\nGandVal(^\u000b;G). Note that for any impression set, we can shrink the impression with the same type into\none vertex. Thus, ^GandGcan be viewed as a same graph with different induced capacity functions on the\nimpression layer I.\nOur goal is to prove that\nVal(^\u000b;G)\u0015(1\u0000\u000f)OPT\u00002\r: (6)\nTheorem 9 implies that in graph ^Gthere exists an s-tvertex cutCsuch that\nVal(^\u000b;^G)\u0015(1\u0000\u000f)C(C;^G); (7)\nwhereC(C;^G)is the value of cutCbased on the capacity function of ^G.\nThe basic framework of proving Theorem 30 is ﬁrst showing that the performances of f^\u000bgare close in\n^GandG:\nVal(^\u000b;G)\u0015Val(^\u000b;^G)\u0000\r; (8)\nand then proving the values of cut Cin^GandGare also close:\nC(C;^G)\u0015C(C;G)\u0000\r: (9)\nSinceC(C;G)is the upper bound of OPT, Eq (6) can be proved directly by combining Eq (7), Eq (8)\nand Eq (9).\n35\n\nAlgorithm 4: Online algorithm with weight predictions\nInput:G= (I[V;E)whereIarrives online,fCaga2A, parameter\u000f2(0;1), predictionf^\u000bg\nwhile an impression icomes do\nUse\rito represent the proportion of the feasible ﬂow sent from itot. Initially,\ri= 0.\nwhile\ri<1andican reachtinGdo\nSend (1\u0000\ri)ﬂow fromitotaccording to the predicted weights.\nUpdate\ri.\nRemove all blocked vertices in G.\nCreate a new graph G0based onGand^G, where the only difference is the capacity function of impres-\nsion layerI. For each impression type i, its capacity in G0is the minimum value of its capacity in Gand^G,\ni.e. deﬁneC(i;G0) := min(C(i;G);C(i;^G)). Thus we have\njC(i;G0)\u0000C(i;^G)j\u0014jmi\u0000^mij;\nindicating that with the same weights, if the capacity of impression type iincreases from C(i;G0)toC(i;^G),\nthe objective value increases by at most jmi\u0000^mij. So we have\njVal(^\u000b;G0)\u0000Val(^\u000b;^G)j\u0014mX\nijmi\u0000^mij=\r\nSince the capacity of each vertex in Gis no less than that in G0, we have Val(^\u000b;G)\u0015Val(^\u000b;G0). Chaining\ninequalities, we have Val(^\u000b;G)\u0015Val(^\u000b;^G)\u0000\r.\nEq (9) can also be proved in the same way by analyzing the capacity of the cut in G0and comparing this\nto the capacities in the graphs ^GandG, respectively. Doing so yields the following chain of inequalities:\nC(C;^G)\u0015C(C;G0)\u0015C(C;G\u0003)\u0000\r:\nThis now completes the proof as argued above.\nThe proof of the other 1=(d+ 1) bound is given in Appendix F. More precisely,\nNow we give the proof of the other 1=(d+ 1) bound. We claim the following theorem:\nTheorem 31. Given any set of weights, for the proportional algorithm A, there exists an improved algorithm\nA0such that the competitive ratio of A0is at least 1=(d+ 1) and always better than the ratio of A.\nThe description of algorithm A0is given in Algo. 4. Before stating the framework of this proof, we give\nseveral deﬁnitions. We say a vertex v2Vis blocked if its capacity is full or all vertices in its neighborhood\nin the next layer are blocked. Intuitively, if a vertex vis blocked, sending any ﬂow to it cannot increase our\nobjective value. In the previous algorithm, we route each ﬂow according to f^\u000bgdirectly, indicating we still\nkeep sending arriving ﬂows to some vertices after they are blocked. These ﬂows sent to blocked vertices\nhave no contribution to our objective value.\nTo prove Theorem 31, we ﬁrst show that this algorithm always performs better than the previous al-\ngorithm, then claim this algorithm always returns a maximal ﬂow and prove that any maximal ﬂow is\n1\nd+1-approximated. The deﬁnition of a maximal ﬂow will be stated later.\nLemma 32. In any instance, Algo 4 performs better than routing the ﬂow according to the predicted weights\ndirectly.\nProof .According to the proportional allocating rules, we have a following claim easily:\n36\n\nClaim 33. Consider any two neighboring layers Aj;Aj+1. Under the proportional allocating setting, if for\neacha2Aj,min(Alloca;Ca)increases, then for each a2Aj+1,min(Alloca;Ca)also increases.\nRecall that for vertices ajandaj+12N(aj;Aj+1), the contribution of ajtoaj+1is given by\nmin(Allocaj;Caj)xaj;aj+1, wherexaj;aj+1is ﬁxed if the weights is ﬁxed. Since min(Allocaj;Caj)in-\ncreases, its contribution to aj+1also increases. Thus, for each a2Aj+1,min(Alloca;Ca)also increases.\nThe objective value of a solution is the sum of min(Alloca;Ca)overa2Ad. UseAlloc0\naandAllocato\nrepresent the value of Allocain Algo 4 and the previous simple algorithm respectively. By the claim above,\nif we prove that for each ain the ﬁrst layer A1,min(Alloc0\na;Ca)\u0015min(Alloca;Ca), this lemma can be\nproved.\nConsider each vertex ain the ﬁrst layer. If this vertex is blocked in Algo 4, clearly we have\nmin(Alloc0\na;Ca)\u0015Ca\u0015min(Alloca;Ca):\nIf this vertex is not blocked, the new algorithm will not send less ﬂow to it than the old algorithm, thus, we\nalso have\nmin(Alloc0\na;Ca)\u0015Alloc0\na\u0015Alloca\u0015min(Alloca;Ca);\ncompleting this proof.\nFor each impression i, usePito represent the set of paths from ito the sinkt, and letPi(v)\u0012Pibe\nthe set of paths crossing vertex v. We say a solution is a maximal ﬂow if for any impression ithat are not\nassigned totally, no path in Pihas a free capacity. This deﬁnition can be viewed as a generalization of the\nmaximal matching. According to the statement of Algo 4, it always returns a maximal ﬂow. Now we show\nthat any maximal ﬂow is1\nd+1-approximated.\nLemma 34. For any maximal ﬂow in G, it is1\nd+1-approximated.\nProof .Consider a linear program and dual program of this model.\nmaxX\ni2IX\np2Pixi;p\ns:t:X\np2Pixi;p\u001418i2I\nX\ni2IX\np2Pi(v)xi;p\u0014Cv8v2V\nxi;p\u001508i2I;p2Pi(10)\nFor each pair (i;p), use variable xi;pto denote the proportion of impression iassigned to path p. Using dual\nvariable\rito represent the constraint that impression ihas only one unit and dual variable yvto represent\nthe capacity constraint of vertex v, we can have the following dual program:\nminX\nv2VyvCv+X\ni2I\ri\ns:t:X\nv2pyv+\ri\u001518i2I;p2Pi\nyv;\ri\u001508i2I;v2V\nGiven a maximal solution fxi;pgof LP (10), we now construct a feasible solution of its dual program.\nLet\ribe the assigned proportion of impression i. Namely,\n\ri=X\np2Pixi;p:\n37\n\nClearly, we have\nX\ni2IX\np2Pixi;p=X\ni2I\ri: (11)\nLetyvbe the proportion of vertex v’s capacity occupied by this solution. Namely,\nyv= (X\ni2IX\np2Pi(v)xi;p)=Cv:\nNote that the diameter of Gisd. Any pathpcrosses at most dvertices inV. Thus, if summing over all\nyvCv, eachxi;pis counted at most dtimes. Thus, we have\ndX\np2Pixi;p\u0015X\nv2VyvCv: (12)\nCombining Eq (11) and Eq (12), we have\n(d+ 1)X\np2Pixi;p\u0015X\nv2VyvCv+X\ni2I\ri:\nIffyv;\rigis feasible, then the maximal ﬂow is1\nd+1-approximated. For any pair (i;p), if\ri= 1, its\nconstraint is satisﬁed. Otherwise, according to our algorithm, path phas no free capacity, indicating that\nthere exists a vertex vin this path with yv= 1. So we still haveP\nv2pyv+\ri\u00151:Thus,fyv;\rigis feasible\nand this maximal ﬂow is1\nd+1-approximated.\nCombing Lemma 32 and Lemma 34, Theorem 31 can be proved, also completing the proof of Theo-\nrem 29.\nD Existence of Useful Weights for Max Flow in general DAGs\nIn this section, we ﬁrst present our simpliﬁcation of the algorithm in [2] for the bipartite (or 2-layer DAG)\ncase using only weight decreases rather than increases and decreases in their algorithm. In the following\nsubsection D.2, we extend our proof of the three-layer graph case in Appendix A.2 to the general case of d\nlayers and use this to complete the proof of Theorem 9 in Appendix A.1.\nD.1 Maximum Cardinality Bipartite Matching\nLetG= (I;A;E )be a bipartite graph. Iis the set of “impressions” and Ais the set of “advertisers”. Each\nadvertiserahas a capacity Ca. We want to ﬁnd a maximum cardinality matching where each impression\ncan be matched at most once and each advertiser can be matched at most Catimes. We focus on fractional\nsolutions. Let NiandNarepresent the neighborhoods of ianda, respectively. We want to ﬁnd (1\u0000\u000f)-\napproximate fractional solutions to the following LP.\nmaximizeX\na2Aya\nX\na2Nixia= 18i2I\nya\u0014X\ni2Naxia8a2A\nya\u0014Ca8a2A\nxia\u001508(i;a)2E(13)\n38\n\nAlgorithm 5: Proportional Allocation Algorithm\nInput:G= (I;A;E );C;T;\u000f\n8a2A; \u000ba 1\nfort= 1;2;:::;T do\nLetxia:=\u000baP\na02Ni\u000ba0\n8a2A;computeAlloca:=P\ni2Naxia\n8a2A;ifAlloca\u0015(1 +\u000f)Cathen\u000ba \u000ba=(1 +\u000f)\nOutput: Weightsf\u000baga2A, Allocationfxiag(i;a)2E\nLetf\u000baga2Abe a set of weights for the advertisers. We consider fractional solutions parameterized by\nthese weights in the following way, for all (i;a)2Ewe setxiaas follows:\nxia:=\u000baP\na02Ni\u000ba0\nInitially, each \u000bais set to 1, and then updated over time according to Algorithm 5. Intuitively, we\ncompute the resulting allocation given by the weights, then decrease the weight for each advertiser that has\nbeen signiﬁcantly over-allocated by 1 +\u000f. This repeats for some number of rounds T. At the end we scale\ndown the allocation so it is always feasible. We want to show the following guarantee for this algorithm.\nTheorem 35. Letn=jAjbe the number of advertisers. For any \u000f >0, if Algorithm 5 is run for T=\nO(1\n\u000f2log(n\n\u000f))iterations, then the allocation it returns is a (1\u0000O(\u000f))-approximation to LP (13).\nWe start by deﬁning some notation. We use superscript (t)to denote the value of variables in the end of\niterationt. For example, x(t)\niarepresents the value of xiain the end of iteration t. UseAlloca:=P\ni2Naxia\nto denote the amount allocated to advertiser ausing the weights. Deﬁne \u000bmin=1\n(1+\u000f)T. Since any weight\ncan decrease at most (1 +\u000f)per iteration, all weights are at least \u000bmin. LetA(k) =fa2Aj\u000ba=\n(1 +\u000f)k\u000bmingfor0\u0014k\u0014Tbe the advertisers with weight at “level” k.\nThe maximum cardinality bipartite matching can be seen as a special maximum ﬂow problem. Consider\nthe natural ﬂow network where there is a sink sand sourcet.sis connected to each i2Iwith capacity\n1 and eacha2Ais connected to twith capacity Ca.Iis connected to Aaccording to Gwith inﬁnite\ncapacity on each edge (or capacity 1 if you don’t like inﬁnite capacity). Let this network be G0. Note that\nany feasible ﬂow in G0corresponds to a feasible solution to our problem and vice-versa.\nTo prove Theorem 35, we consider a subgraph G00, a smaller graph by removing some vertices and edges\nofG0. We ﬁrst construct an s-tcut inG00. Then prove that the value of our solution is at least (1\u0000O(\u000f))\ntimes the value of this cut, thus at least (1\u0000O(\u000f))times the maximum ﬂow in G00. Finally, we show that\nthe optimal value in G00is close to that in G0, completing the proof.\nWe now deﬁne some vertex sets in order to construct G00. Given any integer 1\u0014`\u0014T\u00001\u0000log(n=\u000f)\n\u000f, we\ndeﬁne a gap in the Alayer:G(A) :=S`0\nk=`A(k), where`0=`+log(n=\u000f)\n\u000f. As shown in Fig 4a, all vertices\nin theAlayer are partitioned into three parts: G(A),G(A)\u0000andG(A)+, whereG(A)\u0000:=S`\u00001\nk=0A(k)and\nG(A)+:=ST\nk=`0+1A(k).\nThe subgraph G00is created by removing G(A). Now, we show how to construct an eligible s-tcut. As\nshown in Fig 4b, we group the advertisers with higher weights G(A)+and their neighboring impressions\nN(G(A)+)witht. The remaining vertices are grouped with s. This results in cutting the (a;t)edges for the\nlower weight advertisers and the (s;i)edges for the neighbors of the higher weight advertisers.\nWe deﬁne some more notation. Let N(S) :=S\na2SNaandC(S) =P\na2SCabe the collective neigh-\nborhood and capacity of S\u0012A, respectively. Let OPT (G)be the maximum ﬂow in graph G. According to\n39\n\n(a)\n (b)\nFigure 4: Fig (a) is an illustration of the partition of the Alayer. RemovingG(A)can get the graph G00.\nFig (b) is an illustration of the s-tcut in the graph. We color the vertices staying with tblue and the edges\nin the cut red.\nthe construction of the cut above, we have\nOPT(G00)\u0014jN(G(A)+)j+C(G(A)\u0000): (14)\nOur ﬁrst lemma shows that “low” weight advertisers have at least their capacity allocated to them and\n“high” weight advertisers are not too over-allocated.\nLemma 36. After iteration T, for eacha2ST\u00001\nk=0A(k), we haveAlloca\u0015Ca. Similarly, for\na2ST\nk=1A(k)we haveAlloca\u0014(1 +\u000f)2Ca\nProof .For the ﬁrst part, let a2ST\u00001\nk=0A(k)and note that such an advertiser had its weight decreased at\nleast once. Consider the last iteration twhere this occurred. In this iteration, prior to decreasing \u000ba, we had\nAlloc(t\u00001)\na\u0015(1 +\u000f)Ca\nby deﬁnition of the algorithm. After decreasing \u000ba,Allocadecreased at most (1 +\u000f). So we had\nAlloc(t)\na\u0015Alloc(t\u00001)\na\n1 +\u000f\u0015Ca:\nAllocadid not decrease in subsequent iterations since this was the last round \u000badecreased. Thus, after the\nﬁnal iteration, Allocais still at least Ca.\nThe second part is similar. Let a2ST\nk=1A(k)and that such an advertiser had at least one iteration\nwhere it didn’t decrease its weight. Consider the last such iteration t. Clearly, we had\nAlloc(t\u00001)\na\u0014(1 +\u000f)Ca:\nin the beginning of this iteration. In this iteration, Allocaincreased at most (1 +\u000f). So we have\nAlloc(t)\na\u0014(1 +\u000f)Alloc(t\u00001)\na\u0014(1 +\u000f)2Ca:\nSince advertiser adecreased its weight in all subsequent iterations, this inequality is maintained, completing\nthe proof.\n40\n\nLetValbe the value of our solution. Now we prove that Valis at least (1\u0000O(\u000f))times the value of that\ncut.\nLemma 37. For any`, we have Val\u0015C(G(A)\u0000) + (1\u0000O(\u000f))jN(G(A)+)j.\nProof .By Lemma 36, after iteration T, we have that each a2G(A)\u0000has\nAlloca\u0015Ca:\nThus, we get exactly C(G(A)\u0000)for these advertisers. Next note that some impressions i2N(G(A)+)may\nhave an edge to both G(A)\u0000andG(A)+, we show that we can neglect such edges from ItoG(A)\u0000to avoid\ndouble counting. Suppose that ihas edges (i;a1)and(i;a2)wherea12G(A)\u0000anda22G(A)+. By\ndeﬁnition of the gap, we have\n\u000ba2\u0015(1 +\u000f)log(n=\u000f)=\u000f\u000ba1\u0015n\n\u000f\u000ba1:\nIt follows that for any impression i,\nxia1\u0014\u000f\nnxia2\nand that X\na2Ni\\G(A)\u0000xia\u0014\u000f:\nCounting the value we allocate to G(A)+, we have\nX\na2G(A)+Alloca=X\ni2N(G(A)+)X\na2Ni\\G(A)+xia\u0015(1\u0000\u000f)jN(G(A)+)j:\nBy Lemma 36, the advertisers in G(A)+are not too over-allocated. Thus ﬁnally, we have\nVal\u0015X\na2G(A)\u0000min(Alloca;Ca) +X\na2G(A)+min(Alloca;Ca)\u0015C(G(A)\u0000) + (1\u0000O(\u000f))jN(G(A)+)j:\nBy Lemma 37 and Eq (14), we can bounded our solution by the maximum ﬂow in G00:\nVal\u0015C(G(A)\u0000) + (1\u0000O(\u000f))jN(G(A)+)j\u0015(1\u0000O(\u000f))OPT(G00):\nNow we show that OPT (G00)is close to OPT (G0)by selecting an appropriate `. More speciﬁcally, since\nG00is obtained by removing G(A)fromG0, we have\nOPT(G0)\u0000OPT(G00)\u0014C(G(A)):\nNext we show by averaging that for all `, the difference is small when Tis large enough, indicating that\nthere must exist one `such thatC(G(A))is small.\nLemma 38. IfT\u00152\n\u000f2log(n=\u000f), then there exist an `such that\nC(G(A))\u0014\u000fVal:\n41\n\nProof .Summing the capacity in the gap over all values of `, we have\nT\u00001\u00001\n\u000flog(n=\u000f)X\n`=1`0X\nk=`X\na2A(k)Ca\u0014log(n=\u000f)\n\u000fT\u00001X\nk=1X\na2A(k)Ca:\nDue to Lemma 36, we have\nT\u00001X\nk=1X\na2A(k)Ca\u0014Val\nThus, by averaging, there exists an `2[1;T\u00001\u00001\n\u000flog(n=\u000f)]such that the capacity in the gap is at\nmost\nlog(n=\u000f)=\u000f\nT\u00001\u0000log(n=\u000f)=\u000fT\u00001X\nk=1X\na2LkCa\u0014\u000fVal\nprovidedT\u00152\n\u000f2log(n=\u000f).\nThe above lemma will allow us to show that the amount we lose in the gap is a small fraction of our total\nvalue. Showing this and combining the above lemmas will allow us to complete the proof of Theorem 35.\nProof of Theorem 35 .WhenT=O(1\n\u000f2log(n=\u000f)), we have\nVal\u0015(1\u0000O(\u000f))OPT(G00)\nVal\u0015(1\u0000O(\u000f))(OPT(G0)\u0000\u000fVal)\nVal\u0015(1\u0000O(\u000f))OPT(G0)\nSince OPT (G0)is equal to the optimal value of LP (13), our algorithm returns a (1\u0000O(\u000f))-approximated\nmatching.\nD.2 Max Flow in d-Layered Graphs\nIn this subsection, we generalize our method to the maximum ﬂow problem in the (s-t)d-layered graph\nG(V[fs;tg;E). For simplicity, the graph used in the following is (d+ 1) -layered as shown in Fig 5. The\nvertices inVare partitioned into d+ 1layers, denoted by I;A 1;A2;... andAd. LetA=Sd\nj=1Aj. Each\nvertexvhas capacity Cvand w.l.o.g. we can assume that the capacity of each vertex i2Iis1.\nRecall the two properties needed: near optimality (Property 1) and virtual-weight dependence (Prop-\nerty 2). We also ﬁrst focus on the near optimality property (Theorem 39). Then in the end, we give the proof\nof virtual-weight dependence (Theorem 49).\nWe deﬁne some new notation ﬁrst. For each vertex i2I, useN(i;A1)to represent its neighborhood\ninA1. For each vertex aj2Aj, useN(aj;Aj+1)andN(aj;Aj\u00001)to represent its neighborhood in its next\nlayer and previous layer respectively (Note that Iis viewed as A0). For each edge (aj\u00001;aj)2E, deﬁne\nx(aj\u00001;aj)be the proportion of ﬂow sent from aj\u00001toaj. DeﬁneAllocajandAlloc(t)\najas in the three-layered\ncase.\nOur generalized framework is stated in Algo 6. In each iteration, when we compute xandAlloc , we\nsweep from A1toAd, but when we update the weights, we reverse sweep from Adback toA1. Each vertex\nain the last layer only updates its weights due to itself while the vertices in other layers update their weights\n42\n\nFigure 5: An illustration of the (s-t)(d+ 1) -layered graph.\nnot only due to themselves, but also due to their neighborhood in the next layer. We set different values of \u000f\nfor different layers. For all vertices in Aj, we partition them into several layers according to \u000fj:\nAj(k) =faj2Ajj\u000baj= (1 +\u000fj)k\u00011\n(1 +\u000fj)Tg;\nand use Lev(aj)to denote the level of aj.\nNow we introduce the generalized version of the four properties:\nProperty 7 (Increasing monotonicity) .For any vertex a2A, in iteration t, if its weight does not decrease,\nAlloc(t)\na\u0015Alloc(t\u00001)\na .\nProperty 8 (Decreasing monotonicity) .For any vertex a2A, in iteration t, if its weight decreases,\nAlloc(t)\na\u0014Alloc(t\u00001)\na .\nProperty 9 (Layer dominance) .For any vertex aj2Ajwith1\u0014j\u0014d\u00001, in any iteration, there exists at\nleast one vertex aj+12N(aj;Aj+1)such that Lev(aj+1)\u0015Lev(aj).\nProperty 10 (Forced decrease exemption) .Let\u000fmax= max\nj\u000fj,\u000fmin= min\nj\u000fjandn= max\njjAjj. For\nany vertexaj2Ajwith1\u0014j\u0014d\u00001, in any iteration, if there exists one vertex aj+12N(aj;Aj+1)with\nLev(aj+1) =Tor satisfying Lev(aj+1)\u0000Lev(aj)\u0015log(n=\u000fmax)=\u000fmin, thenCond (N(aj;Aj+1)) = False .\nSimilar to the three-layered case, we claim the following theorem:\nTheorem 39. For any algorithm under our framework with the four properties, if T=O(nlog(n=\u000fmax)\n\u000fmax\u000fmin), it\nwill return aQd\nj=1(1\u0000O(\u000fjd))-approximated solution.\nThus, given an appropriate f\u000fg, our algorithm will return a near-optimal solution. The basic idea of\nproving Theorem 39 is similar to that of Theorem 10. We ﬁrst deﬁne a gap in each layer and remove them\nto construct a new graph G0. We prove that when Tis large enough, the optimal value of the ﬂow in G0is\n43\n\nAlgorithm 6: The framework of the algorithm in (s-t)(d+ 1) -layered graphs\nInput:G= (fs;tg[I[A1[:::[Ad;E),fCaga2A, and parameter \u000f1;:::;\u000fd\nInitialize\u000ba= 18a2A.\nforiteration 1;2;:::;T do\nFor each (aj\u00001;aj)2E, letx(aj\u00001;aj)=\u000bajP\na0\nj2N(aj\u00001;Aj)\u000ba0\nj\nFor eachi2I, letAlloci=Ci= 1. And for each vertex aj2Aj, let\nAllocaj=P\naj\u000012N(aj;Aj\u00001)min(Allocaj\u00001;Caj\u00001)x(aj\u00001;aj).\nforlayerj=d;d\u00001;:::;1do\nforeach vertexaj2Ajdo\nifAllocaj>Qj\nj0=1(1 +\u000fj0)Cajthen\n\u000baj \u000baj=(1 +\u000fj)\nelse ifj <1then\nifCond (N(aj;Aj+1)) = True then\n\u000baj \u000baj=(1 +\u000fj)\nOutput:f\u000baga2A\nclose to that of G. Then we construct an s-tvertex cut in Gand show that the value of our ﬂow in G0is\nclose to the value of this cut, completing the proof.\nGiven any integer 1 +log(n=\u000fmax)\n\u000fmin\u0014l\u0014T\u00001\u0000log(n=\u000fmax)\n\u000fmin, for each layer Aj, we can deﬁne a gap:\nG(Aj) :=Sl0\nk=lAj(k), wherel0=l+log(n=\u000fmax)\n\u000fmin. As shown in Fig 6, all vertices in Ajare partitioned into\nthree parts:G(Aj),G(Aj)\u0000andG(Aj)+, whereG(Aj)\u0000:=Sl\u00001\nk=0Aj(k)andG(Aj)+:=ST\nk=l0+1Aj(k).\nDeﬁneG:=Sd\nj=1G(Aj),G+:=Sd\nj=1G(Aj)+andG\u0000:=Sd\nj=1G(Aj)\u0000. The graph G0is created by\nremoving all vertices in Gand all edges adjacent to them. The two rules to construct an s-tvertex cutCare\nthe same:\n(1) For anys-tpath always crossing G\u0000, add the vertex ad2AdtoC.\n(2) For anys-tpath crossing at least one vertex in G+, ﬁnd the ﬁrst vertex ajinG+and add the vertex\naj\u00001before it toC. (ifj= 1, add vertex itoC.)\nObserve thatCis a feasibles-tvertex cut, meaning that all s-tpaths inG0will be blocked by removing C.\nUseG(A\u0000\nj;A+\nj+1)to represent the vertices in G(Aj)\u0000which are adjacent to at least one vertex in G(Aj+1)+.\nThen according to the two rules, we can compute the value of this cut:\nC(C) =C(G(Ad)\u0000) +C(G(A\u0000\nd\u00001;A+\nd)) +:::+C(G(A\u0000\n1;A+\n2)) +jN(G(A1)+;I)j:\nAlso, we can compute the value of our solution:\nVal =X\nad2G(Ad)\u0000min(Allocad;Cad) +X\nad2G(Ad)min(Allocad;Cad) +X\nad2G(Ad)+min(Allocad;Cad):\nUsing the same technique as in the proof of Lemma 13 and Lemma 14, we have the following two\nlemmas:\nLemma 40. If the increasing monotonicity holds, in any layer Aj,8aj2ST\u00001\nk=0Aj(k), if the last iteration\nthat its weight decreased is due to a self-decrease, we have Allocaj\u0015Caj.\nLemma 41. If the decreasing monotonicity property holds, in any layer Aj,8aj2ST\nk=1Aj(k), we have\nAllocaj\u0014Caj\u0001Qj\nj0=1(1 + 3\u000fj0).\n44\n\nFigure 6: An illustration of the partition of A1,...,Ad.\nThus, we have X\nad2G(Ad)\u0000min(Allocad;Cad)\u0015C(G(Ad)\u0000): (15)\nNow, we need to establish the relationship between\nX\nad2G(Ad)min(Allocad;Cad) +X\nad2G(Ad)+min(Allocad;Cad) (16)\nand\nC(G(A\u0000\nd\u00001;A+\nd)) +:::+C(G(A\u0000\n1;A+\n2)) +jN(G(A1)+;I)j: (17)\nTo obtain the relationship, similar to Eq (1) and Eq (2), we claim the following lemma:\nLemma 42. If the layer dominance property holds, for any layer 1<j\u0014d, we have\nX\naj2G(Aj)+Allocaj+X\naj2G(Aj)Allocaj\u0015X\naj\u000012G(Aj\u00001)+min(Allocaj\u00001;Caj\u00001)(1\u0000\u000fmax)\n+X\naj\u000012G(A\u0000\nj\u00001;A+\nj)min(Allocaj\u00001;Caj\u00001)(1\u0000\u000fmax);(18)\nand for the ﬁrst layer A1, we have\nX\na12G(A1)+Alloca1+X\na12G(A1)Alloca1\u0015(1\u0000\u000fmax)jN(G(A1)+;I)j: (19)\n45\n\nProof .Since the length of the gap is log(n=\u000fmax)=\u000fmin, for any vertex aj\u000012Aj\u00001, if it is adjacent to at\nleast one vertex inG(Aj)+, the proportion of ﬂow that it sends to G(Aj)\u0000is at most\u000fmax. Thus, the second\nstatement can be proved directly.\nDue to Property 9 and the deﬁnition of G(A\u0000\nj\u00001;A+\nj), any vertex inG(Aj\u00001)+andG(A\u0000\nj\u00001;A+\nj)is\nadjacent to at least one vertex in G(Aj)+, proving the ﬁrst statement.\nDue to Lemma 41, for any vertex aj2G(Aj)+[G(Aj), we have\nmin(Allocaj;Caj)\u0015Allocaj\u0001jY\nj0=1(1\u00003\u000fj0): (20)\nDue to Lemma 40, for any vertex aj2G(Aj)\u0000, we have\nmin(Allocaj;Caj)\u0015Caj: (21)\nFor simplicity, let 1\u0000\u000ej= (1\u0000\u000fmax)\u0001Qj\nj0=1(1\u00003\u000fj0). Combing Eq (18), Eq (19), Eq (20) and\nEq (21), we can obtain that\nX\naj2G(Aj)+min(Allocaj;Caj) +X\naj2G(Aj)min(Allocaj;Caj)\n\u0015(X\naj2G(Aj)+Allocaj+X\naj2G(Aj)Allocaj)\u0001jY\nj0=1(1\u00003\u000fj0)\n\u0015(X\naj\u000012G(Aj\u00001)+min(Allocaj\u00001;Caj\u00001)(1\u0000\u000fmax))\u0001jY\nj0=1(1\u00003\u000fj0)\n+ (X\naj\u000012G(A\u0000\nj\u00001;A+\nj)min(Allocaj\u00001;Caj\u00001)(1\u0000\u000fmax))\u0001jY\nj0=1(1\u00003\u000fj0)\n\u0015X\naj\u000012G(Aj\u00001)+min(Allocaj\u00001;Caj\u00001)(1\u0000\u000ej) +C(G(A\u0000\nj\u00001;A+\nj))(1\u0000\u000ej)\nandX\na12G(A1)+min(Alloca1;Ca1) +X\na12G(A1)min(Alloca1;Ca1)\u0015(1\u0000\u000e1)jN(G(A1)+;I)j:\n46\n\nSumming the above two inequalities over j= 1;2;:::;d , we have\ndX\nj=1X\naj2G(Aj)+min(Allocaj;Caj) +dX\nj=1X\naj2G(Aj)min(Allocaj;Caj)\n\u0015d\u00001X\nj=1X\naj2G(Aj)+min(Allocaj;Caj)(1\u0000\u000ej+1)\n+d\u00001X\nj=1C(G(A\u0000\nj;A+\nj+1))(1\u0000\u000ej+1)\n+ (1\u0000\u000e1)jN(G(A1)+;I)j\n\u0015(1\u0000\u000ed)d\u00001X\nj=1X\naj2G(Aj)+min(Allocaj;Caj)\n+ (1\u0000\u000ed)d\u00001X\nj=1C(G(A\u0000\nj;A+\nj+1))\n+ (1\u0000\u000ed)jN(G(A1)+;I)j\nNote that the last two terms in the inequality above is (1\u0000\u000ed)times the value of Term (17). Now we\ncan establish a relationship between Term (16) and Term (17):\nX\nad2G(Ad)min(Allocad;Cad) +X\nad2G(Ad)+min(Allocad;Cad)\n\u0015(1\u0000\u000ed)d\u00001X\nj=1C(G(A\u0000\nj;A+\nj+1))\n+ (1\u0000\u000ed)jN(G(A1)+;I)j\n\u0000\u000edd\u00001X\nj=1X\naj2G(Aj)+min(Allocaj;Caj)\n\u0000d\u00001X\nj=1X\naj2G(Aj)min(Allocaj;Caj)(22)\nClearly,Pd\u00001\nj=1P\naj2G(Aj)+min(Allocaj;Caj)is at mostdVal. Thus, combining Eq (15) and Eq (22),\n47\n\nwe have\nVal =X\nad2G(Ad)\u0000min(Allocad;Cad)\n+X\nad2G(Ad)min(Allocad;Cad) +X\nad2G(Ad)+min(Allocad;Cad)\n\u0015C(G(Ad)\u0000)\n+ (1\u0000\u000ed)d\u00001X\nj=1C(G(A\u0000\nj;A+\nj+1))\n+ (1\u0000\u000ed)jN(G(A1)+;I)j\n\u0000\u000edd\u00001X\nj=1X\naj2G(Aj)+min(Allocaj;Caj)\n\u0000d\u00001X\nj=1X\naj2G(Aj)min(Allocaj;Caj)\n\u0015(1\u0000\u000ed)C(C)\u0000d\u000edVal\n\u0000d\u00001X\nj=1X\naj2G(Aj)min(Allocaj;Caj)\n\u00151\u0000\u000ed\n1 +d\u000edOPT(G0)\u00001\n1 +d\u000edd\u00001X\nj=1X\naj2G(Aj)min(Allocaj;Caj)(23)\nFinally, using the same technique (dividing each layer into Ppart andQpart and summing all terms\nrelated to the gaps over all potential l), we can prove the following lemma easily:\nLemma 43. If the four properties hold, when T=O(nlog(n=\u000fmax)\n\u000fmax\u000fmin), there exists an approximate lsuch that\nOPT(G)\u0000OPT(G0) +X\naj2G(Aj)min(Allocaj;Caj)\u0014d\u000fmaxVal: (24)\nProof of Theorem 39 .Combing Eq (23) and Eq (24), we have\nVal\u0015(1\u0000O(d\u000ed))OPT(G)\n\u0000(1\u0000O(d\u000ed))(OPT(G)\u0000OPT(G0) +X\naj2G(Aj)min(Allocaj;Caj))\n\u0015(1\u0000O(d\u000ed))OPT(G)\n\u0015OPT(G)\u0001dY\nj=1(1\u0000O(d\u000fj))\nNow we introduce our ﬁnal algorithm by giving Cond and setting an \u000fjfor all 2\u0014j\u0014d. TheCond\nfunction is stated in Algo 7, nearly the same as Algo 2. Clearly, Property 9 and Property 10 can be proved\nby the same technique as the proofs of Property 5 and Property 6:\n48\n\nAlgorithm 7: Cond (N(aj;Aj+1))\nLet\u000b(aj)\nmax= max\naj+12N(aj;Aj+1)\u000baj+1andN\u0003\n(aj;Aj+1):=faj+12N(aj;Aj+1)j\u000baj+1=\u000b(aj)\nmaxg.\nif8aj+12N\u0003\n(aj;Aj+1),\u000baj+1decreases in this iteration and\nLev(aj+1)\u0000Lev(aj)<log(n=\u000fmax)=\u000fminthen\nreturn True\nelse\nreturn False\nLemma 44. The layer dominance (Property 9) and the forced decrease exemption (Property 10) hold if we\nuseCond in Algo 7.\nNow we give an approximate f\u000fgto obtain Property 7 and Property 8:\nLemma 45. If for any 1\u0014j < d , we have\u000fj=\u000fj+1\n2n, then the increasing monotonicity (Property 3) and\nthe decreasing monotonicity (Property 4) hold when using Cond in Algo 7.\nProof .Let us prove Property 7 holds ﬁrst. Clearly, for any vertex a12A1, if its weight does not decrease in\none iteration, Alloca1will not decrease. To prove this property holds for the remaining layer, we generalize\nClaim 17 to the layers in our model:\nClaim 46. For any vertex aj2Aj(2\u0014j\u0014d), consider any vertex aj\u000012N(aj;Aj\u00001), if\u000bajdoes not\ndecrease but min(Allocaj\u00001;Caj\u00001)decreases in iteration t, we have\nx(t)\n(aj\u00001;aj)\u0015(1 +\u000fj\nn)x(t\u00001)\n(aj\u00001;aj)\nThe proof of this claim is the same as that of Claim 17. Since in one iteration, for any vertex aj\u000012\nAj\u00001,min(Allocaj\u00001;Caj\u00001)increases at mostQj\u00001\nj0=1(1 +\u000fj0), if we can obtain that for any 2\u0014j\u0014d,\nj\u00001Y\nj0=1(1 +\u000fj0)\u00141 +\u000fj\nn; (25)\nthen Property 7 holds.\nThe above inequality can be proved inductively. Clearly, when j= 2, if let\u000f1=\u000f2\n2n, we have\n1 +\u000f1\u00141 +\u000f2\nn:\nAssuming that for any j\u0014k, Eq (25) holds, now we prove that when j=k+ 1, Eq (25) still holds.\nkY\nj0=1(1 +\u000fj0) = (1 +\u000fk)k\u00001Y\nj0=1(1 +\u000fj0)\nAccording to our assumption, we have\nkY\nj0=1(1 +\u000fj0)\u0014(1 +\u000fk)(1 +\u000fk\nn)\n\u00141 +\u000fk+\u000fk(1\nn+\u000fk\nn)\n\u00141 + 2\u000fk\n=1 +\u000fk+1\nn\n49\n\nThus, Eq (25) holds for any 2\u0014j\u0014d, indicating that for any vertex aj2Aj, if\u000bajdoes not decrease in\none iteration, Allocajwill also not decrease.\nProperty 8 can also be proved similarly by the following claim:\nClaim 47. For any vertex aj2Aj(2\u0014j\u0014d), consider any vertex aj\u000012N(aj;Aj\u00001), if\u000bajdecreases\nbutmin(Allocaj\u00001;Caj\u00001)increases in iteration t, we have\nx(t)\n(aj\u00001;aj)\u0014x(t\u00001)\n(aj\u00001;aj)=(1 +\u000fj\nn)\nAccording to the two lemmas above and Theorem 39, if we let \u000fj=\u000fj+1\n2nfor any 1\u0014j < d and\nuseCond in Algo 7, our algorithm will return aQd\nj=1(1\u0000O(\u000fjd))-approximated solution when T=\nO(nlog(n=\u000fmax)\n\u000fmax\u000fmin).\nLemma 48. To obtain a (1\u0000\u000f)-approximated solution for an (s-t) (d+1)-layered graph, the running time\nof our algorithm is O(d2nd+2log(n=\u000f)=\u000f2).\nProof .Due to our construction of f\u000fg, we have\ndY\nj=1(1\u0000\u000fj)\u00141\u00002\u000fd;\nindicating that we can obtain a (1\u0000O(\u000fdd))-approximated solution when the number of iterations is\nO(ndlog(n=\u000fd)\n\u000f2\nd).\nFor any\u000f2(0;1), to achieve a (1\u0000O(\u000f))-approximated solution, we need to set \u000fd=\u000f\nd, so the number\nof iterations will be O(d2ndlog(n=\u000f)\n\u000f2 ). In each iteration, we compute xu;vfor each edge in Gand update each\nvertex weight. Thus, the running time of an iteration is still O(n2), completing the proof.\nNote that the diameter of the above graph excluding tisd+ 1. When this value is d, we obtain the\nrunning time O(d2nd+1log(n=\u000f)=\u000f2), as claimed in Theorem 9.\nFinally, we consider the virtual-weight dependence of these weights.\nTheorem 49. Under the framework of Algo 6, if letting \u000fj=\u000fj+1=(2n)for each 1\u0014j\u0014d\u00001and using\nCond in Algo 7, given an reduction graph ~G, for any two neighboring copies ~vj;~vj+1of any vertex v, we\nhave\n\u000b~vj+1= (\u000b~vj)2n:\nThe proof of this theorem is similar to the proof of Theorem 23. For the copies of one vertex, they share\nthe same decreasing steps according to Cond in Algo 7. Since for each 1\u0014j\u0014d\u00001,\u000fj=\u000fj+1=(2n),\nwe can obtain \u000b~vj+1= (\u000b~vj)2nby the same algebra.\nE The Learnability of Vertex Weights for Online Flow Allocation in general\nDAGs\nIn this part, we consider the learnability of vertex weights in a DAG Gand give the whole proof of Theo-\nrem 25. According to the reduction in Section A, we can always assume that Gis ad-layered graph. We\n50\n\nuse an inductive method to prove the learnability. As we shown in Section B, the vertex weights are PAC-\nlearnable when d= 2. Now we claim that if the vertex weights are learnable in d-layered graphs, under\nsome mild assumptions, for (d+ 1) -layered graphs, the vertex weights are also learnable.\nThe basic framework is the same as the previous. We still employ Algo 3 as our learning algorithm.\nSimilarly, due to the deﬁnition of f^\u000bg, we have\nR(^\u000b;^I)\u0015(1\u0000\u000f)OPT(^I)\u0015(1\u0000\u000f)R(\u000b\u0003;^I);\nwhere OPT (^I)is the optimal value of instance ^I.\nAnd we only need to focus on prove the following two inequalities:\nR(\u000b\u0003;^I)\u0015(1\u0000O(\u000f))EI\u0018Dm[R(\u000b\u0003;I)]; (26)\nand\nEI\u0018Dm[R(^\u000b;I)]\u0015(1\u0000O(\u000f))R(^\u000b;^I);\nDeﬁneR(\u000b;E[I])be the value of ﬂow obtained by f\u000bgon the expected impression set E[I]. Again due\nto the concavity of min function and and Jensen’s inequality, Eq (26) can obtained if we prove the following\nlemma:\nLemma 50. Given any\u000f>0,\u000e2(0;1]and vertex weights f\u000bg, if the number of instances sis no less than\nO(n2\n\u000f2ln(n\n\u000e)), with probability at least 1\u0000\u000e, for eacha2A,\njR(\u000b;E[I])\u0000R(\u000b;^I)j\u0014O(\u000f)R(\u000b;E[I]):\nProof .UseA1,...Adto denote the dofﬂine layers in the (d+ 1) -layered graph (Note that the layer Iis the\nonline layer ). According to the proof of Lemma 27, we know in the ﬁrst layer A1, if the number of instances\nsis no less than O(n\n\u000f2ln(n\n\u000e)), with probability at least 1\u0000\u000e, for eacha12A1, we have\njmin(Alloca1(\u000b;^I);Ca1)\u0000min(Alloca1(\u000b;E[I]);Ca1)j\n=jmin(Alloca1(\u000b;^I);Ca1)\u0000min(E[Alloca1(\u000b;I)];Ca1)j\n\u0014\u000fmin(E[Alloca1(\u000b;I)];Ca1) +\u000f=n\n=\u000fmin(Alloca1(\u000b;E[I]);Ca1) +\u000f=n(27)\nFor eacha22A2,\nAlloca2(\u000b;E[I]) =X\na12N(a2;A1)min(Alloca1(\u000b;E[I]);Ca1)ya1;a2;\nwhereya1;a2is the proportion of ﬂow sent from a1toa2, a ﬁxed value iff\u000bgis ﬁxed. Thus, if Eq (27) holds\nfor anya12A1, for eacha22A2, we also have\njmin(Alloca2(\u000b;^I);Ca2)\u0000min(Alloca2(\u000b;E[I]);Ca2)j\u0014\u000fmin(Alloca2(\u000b;E[I]);Ca2) +O(\u000f=n):\nInductively, this inequality holds for the all layers. By the deﬁnition of R(\u000b;E[I]), we have\njR(\u000b;E[I])\u0000R(\u000b;^I)j\u0014O(\u000f)R(\u000b;E[I]):\nNow we focus on the second inequality.\n51\n\nLemma 51. For any\u000f2(0;1), if ford-layered graphs, we have\nE[R(^\u000b;I)]\u0015(1\u0000O(\u000f))R(^\u000b;^I)\nwhenCa=O(1\n\u000f2(ln1\n\u000f))for each vertex a, then for (d+ 1) -layered graphs,\nE[R(^\u000b;I)]\u0015(1\u0000O(\u000f))R(^\u000b;^I)\nwhenCa=O(1\n\u000f2(ln1\n\u000f))for each vertex aand in the optimal ﬂow of instance ^I, the load of each vertex is at\nleastO(1\n\u000f2(ln1\n\u000f)).\nProof .UseA1,...,Adto represent the dofﬂine layers. We discuss this problem in two cases:\n(1) Employingf^\u000bgto instance ^I, no vertex in Adhas a load larger than its capacity.\n(2) Employingf^\u000bgto instance ^I, at least one vertex in Adhas a load larger than its capacity.\nCase 1 :\nThe basic idea of this proof is to construct a d-layered graph G0with each vertex capacity at least\nO(1\n\u000f2(ln1\n\u000f))and a set of weights f\u000b0gfor vertices in G0, such that\n(1) For any impression set I,R(\u000b;I)\u0015R0(\u000b0;I), whereR0(\u000b0;I)is the value of ﬂow obtained by\nweightsf\u000b0gin thed-layered instance (I;G0).\n(2) For the impression set ^I,R(\u000b;^I) =R0(\u000b0;^I).\nAssume there exists such a d-layered graph, this lemma can be proved directly:\nE[R(^\u000b;I)]\u0015E[R0(\u000b0;I)]\n\u0015(1\u0000O(\u000f))R0(\u000b0;^I)\n=(1\u0000O(\u000f))R(^\u000b;^I)\nThe ﬁrst step and the third step holds due to the two properties. The second step holds due to our assumption\nford-layered graphs.\nNow we construct the d-layered graph G0and its capacity function C0. The structure of G0is obtained\nby removing all vertices in the last layer AdofG. The capacity of each vertex in A0\n1,...,A0\nd\u00002is the same as\nthat inGwhile the capacity of each vertex in A0\nd\u00001is set to be the feasible load in this vertex obtained by\n^\u000bin instance (^I;G). According to our assumption about the optimal ﬂow of instance (^I;G), the capacity\nof each vertex in A0\nd\u00001is at leastO(1\n\u000f2(ln1\n\u000f)). Clearly, for each vertex a2G0, the new capacity cannot be\nlarger than its capacity in G:C0\na\u0014Ca.\nThe weights of each vertex in G0are the same as that in G. Since for each vertex a,C0\na\u0014Caand for\nthe last layer A0\nd\u00001, its capacity function is a set of feasible loads, we have R(\u000b;I)\u0015R0(\u000b0;I)for any\nimpression set I.\nAccording to our construction, their performances are the same given the impression set ^I. Namely,\nR(\u000b;I) =R0(\u000b0;I), completing the proof of this case.\nCase 2 :\nFor eacha2Ad, letAlloca(^\u000b)be its load. By the deﬁnition of R(\u000b;I), we have\nR(\u000b;I) =X\na2Admin(Alloca(^\u000b);Ca):\nUseBto represent the set of vertices in Adwith a load larger than the capacity. Now we construct a\nnew capacity function C0for the graph Gand useR0(^\u000b;I)to denote the objective value under the capacity\nfunctionC0. For each vertex not in B, its capacity is the same. For each vertex b2B, letC0\nb=Allocb(^\u000b).\n52\n\nClearly, if we use the new capacity function, no vertex in Adhas a load larger than its capacity. Accord-\ning to the proof of Case 1, we have\nE[R0(^\u000b;I)]\u0015(1\u0000O(\u000f))R0(^\u000b;^I): (28)\nNow we analysis the changes of E[R(^\u000b;I)]andR(^\u000b;^I)if we decrease the capacity of each vertex\nb2BfromC0\nbtoCb.\nDo a expansion for E[R(^\u000b;I)]:\nE[R(^\u000b;I)] =X\na2AdE[min(Alloca(^\u000b;I);Ca)]\n=X\na2AdX\nI2\u0005Pr[I] min(Alloca(^\u000b;I);Ca)\nDo a similar expansion for R(^\u000b;^I):\nR(^\u000b;^I) =X\na2Admin(Alloca(^\u000b;^I);Ca)\n=X\na2AdX\nI2\u0005Pr[I] min(Alloca(^\u000b;^I);Ca)\nNote that in the expansion of R(^\u000b;^I),Alloca(^\u000b;^I)is a ﬁxed value. Thus, for each term in the sum\nPr[I] min(Allocb(^\u000b;^I);Cb), ifCbdecreases, its value will deﬁnitely decrease.\nHowever, in the expansion of E[R(^\u000b;I)], not all such terms decrease. If Alloca(^\u000b;I)is small enough,\nthe value of term Pr[I] min(Alloca(^\u000b;I);Ca)will not decrease.\nThus, if we decrease the capacity of each vertex b2BfromC0\nbtoCb,E[R(^\u000b;I)]decreases slower than\nR(^\u000b;^I). By Eq (28), we can claim that under the original capacity function,\nE[R(^\u000b;I)]\u0015(1\u0000O(\u000f))R(^\u000b;^I);\ncompleting the proof of this case.\nF Parameter Robustness of Predictions for Online Flow Allocation\nIn this section, we focus on the parameter robustness. As mentioned above, for any online instance there\nexists a set of vertex weights for Vwhich can return a near optimal solution. Now we assume the online al-\ngorithm is given predictions of these vertex weights in the beginning. Our goal is to give an online algorithm\nbased on these weights, which can obtain a competitive ratio related to the error of these predictions.\nWe ﬁrst deﬁne the prediction error \u0011. Consider a prediction of vertex weight ^\u000bvfor each vertex v2V.\nDue to scale invariance, we can assume that the minimum predicted vertex weight ^\u000bmin= 1. Usef\u000b\u0003\nvgv2V\nto represent the optimal vertex weights, namely, the weights that can achieve an (1\u0000\u000f)-approximate solution.\nSimilarly, let \u000b\u0003\nmin= 1. Now deﬁne the prediction error\n\u0011:= max\nv2V\u0012^\u000bv\n\u000b\u0003v;\u000b\u0003\nv\n^\u000bv\u0013\n:\nWe have the following claim:\n53\n\nTheorem 52. Given any weight predictions f^\u000bg, we can obtain a solution with competitive ratio\nmax\u00121\nd+ 1;1\u0000\u000f\n\u00112d\u0013\n;\nwheredis the diameter of the graph G.\nProof .Due to the reduction in the Appendix A, we can assume w.l.o.g. that Gisd-layered in the following.\nAccording to Theorem 31, in this proof, we only need to give an online algorithm with competitive ratio\n1\u0000\u000f\n\u00112d.\nLemma 53. If route the ﬂow according to the predicted weights f^\u000bgdirectly, we can obtain a solution with\ncompetitive ratio (1\u0000\u000f)=\u00112d.\nProof of Lemma 53. We give an inductive proof. Consider the ﬁrst layer A1. For each impression iand\na2N(i;A1), we have\n^xi;a=^\u000baP\na02N(i;A1)^\u000ba0\n\u0015\u000b\u0003\na=\u0011P\na02N(i;A1)\u000b\u0003\na0\u0011\n=1\n\u00112\u000b\u0003\naP\na02N(i;A1)\u000b\u0003\na0\n=1\n\u00112x\u0003\ni;a\nThus, for each a2A1,\nmin([Alloca;Ca)\u00151\n\u00112min(Alloc\u0003\na;Ca)\nNow we prove that if for each vertex a2Aj,\nmin([Alloca;Ca)\u00151\n\u00112jmin(Alloc\u0003\na;Ca);\nthen for each vertex a2Aj+1,\nmin([Alloca;Ca)\u00151\n\u00112(j+1)min(Alloc\u0003\na;Ca):\nFor eacha2Aj+1anda02N(a;Aj), similarly, we have ^xa0;a\u00151\n\u00112x\u0003\na0;a:The contribution of vertex\na0toais\nmin([Alloca0;Ca0)^xa0;a\u00151\n\u00112(j+1)min(Alloc\u0003\na;Ca)x\u0003\na0;a:\nThus, for each vertex a2Aj+1,\nmin([Alloca;Ca)\u00151\n\u00112(j+1)min(Alloc\u0003\na;Ca):\nThe value Valof our solution is (1\u0000\u000f)=\u00112d-approximated:\nVal =X\na2Admin([Alloca;Ca)\n\u00151\n\u00112dX\na2Admin(Alloc\u0003\na;Ca)\n\u00151\u0000\u000f\n\u00112dOPT:\n54\n\nAlgorithm 8: Online algorithm with predictions when d= 1\nInput:G= (I[A[ftg;E)whereIandEarrive online,fCaga2A, parameter\u000f2(0;1),\npredictionf^\u000bg\nCreate a same imaginary instance as the input instance, we use x0andAlloc0to denote the\nassignment in this instance.\nwhile an impression icomes do\nInitially, set xi;a=x0\ni;a= 0for alla2Ni.\nwhileP\na2Nixi;a<1do\nFor eacha2Ni, increasexi;aandx0\ni;awith rate^\u000baP\na02Ni^\u000ba0untilP\na2Nixi;a= 1or there\nexists one advertiser asuch thatAlloc0\na= (1 +\u000f)2Ca.\nwhile there exists one advertiser asuch thatAlloc0\na\u0015(1 +\u000f)2Cado\nFor eacha2A, ifAlloc0\na\u0015(1 +\u000f)2Ca,^\u000ba ^\u000ba=(1 +\u000f).\nUpdate each x0\ni;aandAlloc0\naaccording to newf^\u000bag.\nF.1 2-layered Graphs\nIfd= 1, this problem is an online bipartite matching problem. In this special case, we can get an improved\nresult with a more graceful degradation in the error \u0011.\nTheorem 54. Assume that OPT can assign all impressions. For any given \u000f2(0;1), There exists an online\nalgorithm with a competitive ratio of\n1\u00004\u000flog(1+\u000f)\u0011\u00003\u000f:\nProof .The algorithm is presented in Algo. 8. We ﬁrst prove that our algorithm can terminate. Namely,\nwhen a new impression arrives, we can always ﬁnd the weights f^\u000bgsuch that in the imaginary instance,\n8a2A,Alloc0\na<(1 +\u000f)2Ca.\nLetf\u000b\u0003gbe the optimal weights. Since we assume that all impressions can be assigned, we can assume\nthat for any advertiser a, we haveAlloc\u0003\na<(1 +\u000f)Ca. We give the following claim to show that the\nalgorithm will not fall into an endless loop:\nClaim 55. In any time during the algorithm, for any a2A,^\u000ba\u0015\u000b\u0003\na=\u0011.\nProof of Claim 55. Assuming that at some point, there existed some ^\u000ba<\u000b\u0003\na=\u0011. Usebto represent a such\nvertex. Consider the ﬁrst time tthat this event occurred. Clearly, Alloc0\nb(t\u00001)\u0015(1 +\u000f)2Cb. Sincetis the\nﬁrst time, we also have for any a,^\u000ba(t\u00001)\u0015\u000b\u0003\na=\u0011and^\u000bb(t\u00001)< \u000b\u0003\nb(1 +\u000f)=\u0011. For anyx0\ni;bin time\nt\u00001, we have that\nx0\ni;b(t\u00001) =^\u000bb(t\u00001)P\na2Ni^\u000ba(t\u00001)<(1 +\u000f)\u000b\u0003\nb=\u0011P\na2Ni\u000b\u0003a=\u0011= (1 +\u000f)x\u0003\ni;b:\nThus, we know in the time t\u00001,\nAlloc0\nb(t\u00001)<(1 +\u000f)Alloc\u0003\nb<(1 +\u000f)2Cb;\ncontradicting to the fact that Alloc0\nb(t\u00001)\u0015(1 +\u000f)2Cb.\n55\n\nAccording to Claim 55, we can make sure that our algorithm can terminate because we can not keep\ndecreasing ^\u000b. The value of ^\u000bwill stop when it is close to the nearest \u000b\u0003=\u0011.\nFor any advertiser a, usekto represent the number of times that its weight decreased and w1;w2;:::;wk\nto denotekdifferent values of its weight. Our initial prediction ^\u000bais viewed as w0. Since each time, the\nweight decreased by (1 +\u000f), we havewk=w0=(1 +\u000f)k. Due to Claim 55, wk\u0015\f\u0003\na. Thus,\nw0\n(1 +\u000f)k\u0015\u000b\u0003\na=\u0011\n(1 +\u000f)k\u0014\u0011^\u000ba\n\u000b\u0003a\nk\u00142 log (1+\u000f)\u0011\nNamely, the weight decreased at most 2 log(1+\u000f)\u0011times. UseAlloc(j)\nato represent the number of impres-\nsions that assigned to awhena’s weight iswj. In the following, we will try to bound each Alloc(j)\nato give\na upper bound of the ﬁnal Alloca.\nLemma 56. For any advertiser a,Alloc(0)\na\u0014(1 +\u000f)2Ca.\nProof of Lemma 56. This lemma can be proved very easily. For any advertiser a, if its weight has not\ndecreased so far, we have Alloc0\na<(1 +\u000f)2Ca.\nWhen one impression arrived, in the ﬁrst step, the increments of AllocaandAlloc0\naare the same. In the\nsecond step, Alloc0\namay change due to the decrease of other weights. Since ^\u000bahas not decreased, Alloc0\na\ncannot decrease in this step. Thus, we have Alloca\u0014Alloc0\na<(1 +\u000f)2Ca.\nLemma 57. For any advertiser aand anyj\u00151,Alloc(j)\na\u00142\u000fCa.\nProof of Lemma 57. As we mentioned above, if ^\u000badid not change, the increment of Allocais no more than\nthe increment of Alloc0\na. Thus, if we prove that during the period that ^\u000ba=wj,Alloc0\naincreased at most\n3\u000fCa, this lemma can be proved.\nConsider the moment that advertiser a’s weight decreased from wj\u00001towj. UseAlloc0\na(\u0000)and\nAlloc0\na(+) to denote the allocation in the imaginary instance before and after this decrease. Clearly, we\nhaveAlloc0\na(\u0000)\u0015(1 +\u000f)2Ca. When the weight decreased by (1 +\u000f), the allocation decreased by at most\n(1 +\u000f). So after this decrease,\nAlloc0\na(+)\u0015Alloc0\na(\u0000)=(1 +\u000f)\u0015(1 +\u000f)Ca:\nDuring the period that ^\u000ba=wj,Alloc0\nacannot become larger than (1 +\u000f)2Caaccording to our algo-\nrithm. Thus, the increment of Alloc0\nain this period is at most\n(1 +\u000f)2Ca\u0000(1 +\u000f)Ca\u00142\u000fCa;\ncompleting this proof.\nCombining Lemma 56 and Lemma 57, we have\nAlloca=X\njAlloc(j)\na\u0014(1 + 3\u000f+ 4\u000flog(1+\u000f)\u0011)Ca\n56\n\nThis equation indicates that for each advertiser a, its allocation Allocais at most (1 + 3\u000f+\n4\u000flog(1+\u000f)\u0011)Ca. If we increase the capacity of each advertiser afromCatoC0\na= (1+3\u000f+4\u000flog(1+\u000f)\u0011)Ca,\nallmimpressions can be assigned. In other words,\nX\namin(Alloca;C0\na) =m\u0015OPT:\nFor eacha, due to the deﬁnition of C0\na, we have\nmin(Alloca;Ca) = min(Alloca;1\n1 + 3\u000f+ 4\u000flog(1+\u000f)\u0011C0\na)\u00151\n1 + 3\u000f+ 4\u000flog(1+\u000f)\u0011min(Alloca;C0\na):\nCombining the above two inequalities, we have\nX\namin(Alloca;Ca)\u0015m\n1 + 3\u000f+ 4\u000flog(1+\u000f)\u0011\u0015(1\u00003\u000f\u00004\u000flog(1+\u000f)\u0011)OPT;\ncompleting the proof of Theorem 54.\nNote that according to Theorem 31, we can also come up with an algorithm with a competitive ratio of\nmax(1\u00004\u000flog(1+\u000f)\u0011\u00003\u000f;1=2). When the predictions are nearly correct, we can obtain a near optimal\nsolution. When \u0011is large, the ratio will not be worse than 1=2.\nAdditionally, we can show that in some sense this is the best you can do in this setting (up to constant\nfactors).\nTheorem 58. Consider the online ﬂow allocation problem. For any online algorithm with weight predic-\ntions, even if d= 1, its competitive ratio is not better than 1\u0000\n(log\u0011).\nProof .The basic idea is to construct a set of impressions and predictions such that \u0011is very small and the\nexpectation of any algorithm’s competitive ratio is at most 1\u0000log\u0011, indicating the worst ratio among these\ninstances cannot be better than this expectation value.\nMore speciﬁcally, the graph Ghasnadvertisers (a1;:::;an)andnimpressions (i1;:::;in). Sample a\nuniform random permutation \u0019of set [n]. Given any parameter 0<s<n , deﬁne the edge set to be\nE=f(a\u0019(j);ik)jj;k\u0014sg[f (a\u0019(j);ik)jj >s and1\u0014k\u0014ng:\nNote that this parameter swill be served as an bridge between the competitive ratio and the prediction error.\nThe impressions arrive in the order i1;i2;:::;in. For each advertiser a, we set its capacity Cato be 1and\nits predictive weight ^\u000bato be 1.\nWe can see Fig 7 as an illustration. Given the parameter s, we can partition all advertisers into two sets\naccording to the permutation \u0019. The ﬁrstsimpressions are adjacent to all advertisers, while the last n\u0000s\nimpressions only connect to all purple advertisers.\nWe ﬁrst give the expected competitive ratio of this instance, then analyze f\f\u0003gand give\u0011. As mentioned\nabove, both the ratio and \u0011will be related to parameter s. Thus, we draw the conclusion that if ssatisﬁes\nsome conditions, the ratio will be at most 1\u0000log\u0011.\nLemma 59. Letpben\u0000s\nn, the expected competitive ratio rof any online algorithm is at most 1\u0000p(1\u0000p).\nProof of Lemma 59. Clearly, for any permutation \u0019, there is always a perfect matching in this graph.\nNamely, the expected optimal value OPT is n. Consider the expected value of each xa\u0019(j);ik. When\n1\u0014k\u0014s, this value is at most1\nnbecause for any two advertisers a\u0019(j1)anda\u0019(j2), we have\n57\n\nFigure 7: An illustration of the constructed instance.\nE(xa\u0019(j1);ik) =E(xa\u0019(j2);ik)by symmetry, and the sum of all advertisers’ values is at most 1. Simi-\nlarly, when s < k < n ,xa\u0019(j);ikis0ifj\u0014sand at most1\nn\u0000sifj > s . Thus, each green advertiser\nmatches at most s=nimpressions while each purple advertiser matches at most 1impressions. The expected\ncompetitive ratio of any online fractional matching algorithm can be bounded:\nr=Pn\nj=1Pn\nk=1E(xa\u0019(j);ik)\nn\n\u00141\nn(s\nn\u0001s+n\u0000s)\n= 1\u0000(n\u0000s)s\nn2\n= 1\u0000p(1\u0000p)\nLemma 60. Given any permutation \u0019and any\u000f2(0;1), there exists a set of weights f\f\u0003gwith\u0011\f\u0003=p=\u000f\nsuch that it can achieve a (1\u0000\u000f)-approximated fractional matching.\nProof of Lemma 60. Since the ﬁrst sadvertisers in the permutation are equivalent, we let \f\u0003\na\u0019(1)=\f\u0003\na\u0019(2)=\n:::=\f\u0003\na\u0019(s)=w1:Similarly, we have \f\u0003\na\u0019(s+1)=\f\u0003\na\u0019(s+2)=:::=\f\u0003\na\u0019(n)=w2:Clearly, the last n\u0000s\nimpressions will ﬁll out the last n\u0000sadvertisers. For each one in the ﬁrst simpressions, the unmatched\nproportion is the proportion that assigned to the last n\u0000sadvertisers. Letting w2= 1, this proportion is\n(n\u0000s)\nsw1+(n\u0000s). Thus, we can compute the size of the fractional matching obtained by these weights:\nVal =n\u0000s(n\u0000s)\nsw1+ (n\u0000s)\n58\n\nFigure 8: An illustration of the constructed graph with d= 3. The red path is the path selected by an\nonline algorithmAwhen the ﬁrst impression arrives. Clearly, the optimal solution sends 4 units ﬂow to t\nbut algorithmAonly sends one unit.\nWe desire that these weights can obtain a (1\u0000\u000f)-approximated fractional matching. Thus, we have\nn\u0000s(n\u0000s)\nsw1+ (n\u0000s)\u0015(1\u0000\u000f)n\nSolving the inequality above, we have\nw1\u0015p\n\u000f\u0000p\n1\u0000p\nIfw1=p\n\u000f, we can achieve a (1\u0000\u000f)-approximated fractional matching. Since all predicted weights equal\none, whenp>\u000f , the error is p=\u000f, completing the proof.\nAccording to Lemma 60 and the deﬁnition of \u0011, we have\u0011\u0014p=\u000f. When 0<log(p\n\u000f)\u0014p(1\u0000p), we\ncan bounded the expected competitive ratio:\nr\u00141\u0000p(1\u0000p)\u00141\u0000log(p\n\u000f)\u00141\u0000log\u0011;\ncompleting the proof of Theorem 58.\nF.2 A Worst-case Bound\nTo show that our algorithm is competitive, we presents a worst-case bound in the subsection:\nTheorem 61. Considering the integral version of the online ﬂow allocation problem, for any deterministic\nalgorithm, its competitive ratio cannot be better than 1=(d+ 1) .\n59\n\nProof .We ﬁrst construct a directed graph G(V[ftg;E), and then show that given any online algorithm\nA, there is a set of impressions such that its competitive ratio is 1=(d+ 1) .\nAs shown in Fig. 8, the graph has d+ 1layers. There are exactly two vertices and one vertex (the sink\nt) in the ﬁrst layer and the last layer respectively. Each vertex except those in the ﬁrst layer or the last layer\nis pointed by exactly one vertex in its previous layer, while each vertex except those in the last two layers is\nadjacent to exactly two vertices in its next layer. All vertices in the d-th layer are adjacent to t. The capacity\nof each vertex other than tis1.\nClearly, the graph Gexcludingtconsists of two complete binary trees. Now we construct the impression\nset. The ﬁrst impression i0is adjacent to the two vertices in the ﬁrst layer. For any online algorithm A,\nit should select a path to tfor this impression. Otherwise, let no impressions arrive any more and the\ncompetitive ratio is 0. Use (v1;v2;:::;vd;t)to represent the selected path. Then dimpressions (i1;i2;:::;id)\narrives sequentially. Each impression ikis only adjacent to the vertex vk. Since the capacity of vkis only 1,\nAsends only one unit of ﬂow to t.\nHowever, the optimal solution satisﬁes all d+ 1impressions. When impression ikarrives, the optimal\nsolution can always pick a path that does not contain any vertex in fvk+1;:::;vdgfor it. Thus, the competitive\nratio of this instance is 1=(d+ 1) , completing this proof.\nG Learnable and Instance-Robust Predictions for Online Load Balancing\nIn this section we prove the results about online load balancing with restricted assignments stated in Sec-\ntion 6. Recall that [2, 30] show the existence of useful predictions (machine weights) for this problem. Thus\nwe focus on studying the instance robustness and learnability of these predictions.\nG.1 Instance Robustness\nRecall that the theorem we want to prove is the following:\nTheorem 62 (Theorem 7 Restated) .For any instance Sand\u000f>0, letwbe weights such that ALG(S;w)\u0014\n(1 +\u000f)OPT(S). Then for any instance S0we have ALG(S0;w)\u0014(1 +\u000f)2\u0011(S;S0)2OPT(S0).\nProof .The basic idea of this proof is analyzing how much the performance of a set of weights changes\nwhen the instance changes. For each job type jand machine i2Nj, letxij(w)be the proportion of this\ntype of job that assigned to machine iusing weights w. In instance S, for each machine i, letLi(w;S)be\nthe load on machine iusing weights w. Clearly,\nLi(w;S) =X\njSjxij(w):\nNow if we turn instance Sinto a new instance S0, the load of each machine increases at most \u0011:\nLi(w;S0) =X\njS0\njxij(w)\u0014\u0011X\njSjxij(w) =\u0011Li(w;S):\nLetw0be a(1 +\u000f)-approximated weights of the instance S0. Similarly, we have\nLi(w0;S) =X\njSjxij(w0)\u0014\u0011X\njS0\njxij(w0) =\u0011Li(w0;S0):\nAccording to the near-optimality of weights won the instance S, we have\nmax\niLi(w;S)\u0014max\niLi(w0;S):\n60\n\nThus,\nmax\niLi(w;S0)\u0014\u0011max\niLi(w;S)\n\u0014(1 +\u000f)\u0011max\niLi(w0;S)\n\u0014(1 +\u000f)\u00112max\niLi(w0;S0)\n\u0014(1 +\u000f)2\u00112T;\ncompleting this proof.\nG.2 Learnability\nWe show that machine weights for makespan minimization are learnable from data in the following formal\nsense. There is an unknown distribution Dover instances of the problem. A sample S\u0018D consists of\nnjobs, where job jhas sizepjand neighborhood N(j)\u0012[m]of machines. For simplicity, we assume\nD=Qn\nj=1Dj, i.e. each job is sampled independently from it’s own “private” distribution and that pj= 1\nfor all jobs. Later we show how to generalize to different sizes. Let ALG(w;S)be the fractional makespan\non instanceSwith weights w. We want to show that we can ﬁnd weights wgivenssamplesS1;S2;:::;Ss\nfromDsuch that ES\u0018D[ALG(w;S)]\u0014(1+O(\u000f))E[OPT(S)]with high probability (i.e. probability at least\n1\u0000\u000efor\u000e >0. Here OPT (S)is the optimal (fractional) makespan on job set S. Note that such a result\nalso implies that these weights walso satisfy ES\u0018D[ALG(w;S)]\u0014(1 +O(\u000f)) minw0ES\u0018D[ALG(w0;S)]\nwith high probability, i.e. they are comparable to the best set of weights for the distribution D. Ideally, we\nwants= poly(m;1\n\u000f;1\n\u000e)number of samples, and lower is better.\nG.2.1 Preliminary Results on Proportional Weights\nWe need the following prior results about the weights. Recall that given a set of jobs Sand weights w2Rm\n+\nwe consider the following fractional assignment rule for job jandi2N(j).\nxij(w) :=wiP\ni02N(j)wi0\nFor ease of notation we assume that xij= 0wheneveri =2N(j). We would like to ﬁnd weights wsuch\nthatxij(w)approximately solves the following LP.\nmaximizeX\niX\njxij\nX\njxij\u0014Ti8i2[m]\nX\nixij\u001418j2S\nx\u00150(29)\nHere, the right hand side values Tiare inputs and can be thought as all being set to the optimal makespan.\nGiven an assignment via the weights xij(w)via weights w, we can always convert it to a feasible solution\nto LP (29) in the following way. For all i2[m]letOi= maxfP\njxij(w)=Ti;1g. It is easy to see that x0is\nfeasible for LP (29) and that the amount lost is exactly the overallocationP\nimaxfP\njxij(w)\u0000Ti;0g. We\ncan then take x0\nij=xij(w)=Oifor alli;j. The following theorem is adapted from Agrawal et al.\n61\n\nTheorem 63 (Theorem 1 in Agrawal et al.) .For any\u000e2(0;1), there exists an algorithm which ﬁnds\nweightswsuch that a downscaling of xij(w)is a1\u0000\u000e-approximation to LP (29). The algorithm operates\ninR=O(1\n\u000e2log(m=\u000e))iterations and produces weights of the form wi= (1 +\u000f)kfork2[0;R].\nUsing this theorem, we get the following result as simple corollary. Again let Sbe set ofnjobs that we\nwant to schedule on mmachines to minimize the makespan. Let Tbe the makespan of an optimal schedule\nCorollary 64. For any\u000f>0, there exists weights w2Rm\n+such thatxij(w)yields a fractional schedule with\nmakespan at most (1 +\u000f)T. The weights are computed by running for R=O(m2log(m=\u000f)=\u000f2iterations\nand produces weights of the form wi= (1 +\u000f)kfork2[0;R].\nProof .Consider running the algorithm of Theorem 63 with \u000e=\u000f=m andTi=Tfor alli. The optimal\nvalue of (29) on this instance is exactly nsinceTis the optimal makespan and thus we are able to assign\nall the jobs. After scaling down to be feasible, the solution has value at least (1\u0000\u000f=m)n. We only scaled\ndown the assignment on machines for which its assignment was greater than T, and the amount we lost in\nthis scaling down was at most \u000fn=m . Thus in the worst case, any machines assignment using the weights is\nat mostT+\u000fn=m\u0014(1 +\u000f)T, sinceT\u0015n=m .\nG.2.2 Learning the Weights\nNow we show that computing the weights on a “stacked” instance is a reasonable thing to do. Let’s set up\nsome more notation. Let W(R)be the set of possible weights output by Riterations of the proportional\nalgorithm. Let OPT (S)be the optimal fractional makespan on job set S. We are interested in the case when\nES\u0018D[OPT(S)] = \n(logm). LetLi(w;S)be the fractional load of machine ion instanceSwith weights\nw. Thus we have ALG(w;S) = max iLi(w;S). Note that Li(w;S) =P\nj2Sxij(w). Our ﬁrst lemma\nshows that ES\u0018D[ALG(w;S)]\u0019maxiES\u0018D[Li(w;S)]. When it is clear, we will suppress S\u0018D for ease\nof notation.\nLemma 65. Let\u000f > 0be given. If ES\u0018D[OPT(S)]\u00154+2\u000f\n\u000f2log(mp\u000f), then for all Rand all weights\nw2W(R), we have ES\u0018D[ALG(w;S)]\u0014(1 + 2\u000f) maxiES\u0018D[Li(w;S)].\nProof .Fix anyRandw2W(R). We have the following simply bound on ALG(w;S). It can either be at\nmost (1 +\u000f) maxiE[Li(w;S)], or it is larger in which case it is at most n. Thus we have:\nE[ALG(w;S)]\u0014(1 +\u000f) max\niE[Li(w;S)]\n+nPr[ALG(w;S)>(1 +\u000f) max\niE[Li(w;S)]]\n\u0014(1 +\u000f) max\niE[Li(w;S)] +nX\niPr[Li(w;S)\u0015(1 +\u000f)E[Li(w;S)]]\nNow we claim that for each i,Pr[Li(w;S)\u0015(1 +\u000f)E[Li(w;S)]]\u0014\u000f=m2. Indeed, if this is the case then\nwe see that\nE[ALG(w;S)]\u0014(1 +\u000f) max\niE[Li(w;S)] +\u000fn\nm\u0014(1 + 2\u000f) max\niE[Li(w;S)]\nsince maxiE[Li(w;S)]\u0015n=m , and thus proving the lemma. Thus we just need to show the claim. Recall\nthatLi(w;S) =P\nj2Sxij(w)and that each job jis chosen to be part of Sindependently from distribution\nDj. Thusxij(w)is an independent random variable in the interval [0;1]for eachj. Applying Theorem 71\ntoLi(w;S)with\u0016= maxi0E[Li0(w;S)], we see that since \u0016\u0015E[OPT(S)]\u00154+2\u000f\n\u000f2log(mp\u000f), we have\nPr[Li(w;S)>(1 +\u000f)\u0016]\u0014exp\u0012\n\u0000\u000f2\n2 +\u000f\u0016\u0013\n\u0014exp\u0012\n\u0000\u000f2\n2 +\u000fE[OPT(S)]\u0013\n\u0014\u000f\nm2\ncompleting the proof of the claim.\n62\n\nNow that we have this lemma, we can show that computing the weights on a “stacked” instance sufﬁces\nto ﬁnd weights that generalize for the distribution. The result we want to prove is the following.\nTheorem 66. Let\u000f;\u000e2(0;1)andR=O(m2\n\u000f2log(m\n\u000f))be given and let D=Qn\nj=1Djbe a\ndistribution over n-job restricted assignment instances such that ES\u0018D[OPT(S)]\u0015\n(1\n\u000f2log(m\n\u000f)).\nThere exists an algorithm which ﬁnds weights w2 W (R)such that ES\u0018D[ALG(w;S)]\u0014(1 +\nO(\u000f)) minw02W(R)ES\u0018D[ALG(w0;S)]when given access to s= poly(m;1\n\u000f;1\n\u000e)independent samples\nS1;S2;:::;Ss\u0018D . The algorithm succeeds with probability at least 1\u0000O(\u000e)over the random choice of\nsamples.\nWe will show that uniform convergence occurs when we take s= poly(m;1\n\u000f;1\n\u000e)samples. This means\nthat for alli2[m]and allw2W(R)simultaneously, we have with probability 1\u0000\u000ethat1\nsP\n\u000bLi(w;S\u000b)\u0019\nES[Li(w;S)]. Intuitively this should happen because the class of weights W(R)is not too complex. Indeed\nwe have thatjW(R)j=Rm, and thus the pseudo-dimension is log(jW(R)j) =mlog(R) =O(mlogm)\nwhenR=O(m2logm). Once we have established uniform convergence, setting up the algorithm and\nanalyzing it will be quite simple. We start with some lemmas showing uniform convergence.\nLemma 67. Let\u000f;\u000e2(0;1)andS1;S2;:::;Ss\u0018D be independent samples. If s\u0015m2\n\u000f2log(2jW(R)jm\n\u000e),\nthen with probability at least 1\u0000\u000efor alli2[m]andw2W(R)we have\n\f\f\f\f\f1\nsX\n\u000bLi(w;S\u000b)\u0000ES[Li(w;S)]\f\f\f\f\f\u0014\u000fmax\ni0ES[Li0(w;S)]\nProof .Fix a machine i2[m]andw2 W (R). We have that1\nsLi(w;S\u000b)is an independent random\nvariable in [0;n=s]for each\u000b2[s]. Moreover we have that E[1\nsP\n\u000bLi(w;S\u000b)] =E[Li(w;S)]. Applying\nTheorem 72 to1\nsP\n\u000bLi(w;S\u000b)witht=\u000fmaxi0E[Li0(w;S), we have\nPr\"\f\f\f\f\f1\nsX\n\u000bLi(w;S\u000b)\u0000E[Li(w;S)]\f\f\f\f\f\u0015t#\n\u00142 exp\u0012\n\u0000s\u000f2(maxi0E[Li0(w;S)])2\nn2\u0013\n:\nWe claim that if s\u0015m2\n\u000f2log(2jW(R)jm\n\u000e), then this probability is at most\u000e\njW(R)jm. Indeed, this claim follows\nifm\u0015n\nmaxi0E[Li0(w;S)], which is true since maxi0E[Li0(w;S)]\u0015n=m . Finally, the lemma follows by\nunion bounding over all i2[m]andw2W(R).\nLemma 68. Let\u000f;\u000e2(0;1)andS1;S2;:::;Ss\u0018 D be independent samples. For each \u000b2[s]let\nT\u000b=OPT(S\u000b). Ifs\u0015m2\n\u000f2log(2=\u000e)then with probability at least 1\u0000\u000ewe have\n(1\u0000\u000f)E[OPT(S)]\u00141\nsX\n\u000bT\u000b\u0014(1 +\u000f)E[OPT(S)]\nProof .For each\u000b2[s]we have1\nsT\u000bis an independent random variable in [0;n=s]. Moreover, we have\nE[1\nsP\n\u000bT\u000b] =E[OPT(S)]. Applying Theorem 72 to1\nsP\n\u000bT\u000bwe have\nPr\"\nj1\nsX\n\u000bT\u000b\u0000E[OPT(S)]j\u0015\u000fE[OPT(S)]#\n\u00142 exp\u0012\n\u0000s\u000f2E[OPT(S)]2\nn2\u0013\n:\nNow since E[OPT(S)]\u0015n=m we have this probability is at most 2 exp(\u0000s\u000f2=m2). Thus whenever s\u0015\nm2\n\u000f2log(2=\u000e), this probability becomes at most \u000e, completing the proof.\n63\n\nG.2.3 The Learning Algorithm\nWe can now describe and analyze the algorithm. Set R=O(m2\n\u000f2log(m=\u000f). We sample independent in-\nstancesS1;S2;:::;Ss\u0018D fors\u0015m2\n\u000f2log(2jW(R)jm\n\u000e)Next we set up a stacked instance consisting of all\nthe jobs in these samples. Next we set T\u000b=OPT(S\u000b)andT=P\n\u000bT\u000b. We run the algorithm of Corol-\nlary 64 on the stacked instance with right hand side bounds Ti=Tfor alli. The algorithm should run for\nRrounds and produce weights w2W(R)such thatP\n\u000bLi(w;S\u000b)\u0014(1 +\u000f)Tfor alli. We can now prove\nTheorem 66.\nProof of Theorem 66 .Letw2 W (R)be the weights output by the algorithm above. Now for a new\nrandomly sampled instance S\u0018D, by Lemma 65 we have that\nE[ALG(w;S)]\u0014(1 + 2\u000f) max\niE[Li(w;S)]:\nBy Lemma 67, we have maxiE[Li(w;S)]\u0014(1 +O(\u000f)) maxi1\nsP\n\u000bLi(w;S\u000b)with probability at least\n1\u0000\u000e. By construction of our algorithm, we haveP\n\u000bLi(w;S\u000b)\u0014(1 +\u000f)T= (1 +\u000f)P\n\u000bT\u000bfor alli.\nIt thus follows that maxiE[Li(w;S)]\u0014(1 +O(\u000f))1\nsP\n\u000bT\u000bwith probability at least 1\u0000\u000e. Next we have\nthat1\nsP\n\u000bT\u000b\u0014(1 +\u000f)E[OPT(S)]with probability at least 1\u0000\u000e. Finally, with probability at least 1\u00002\u000e,\nby chaining these inequalities together we get\nE[ALG(w;S)]\u0014(1 +O(\u000f))E[OPT(S)]\u0014(1 +O(\u000f))E[ALG(w\u0003;S)]\nwherew\u0003= arg min w02W(R)E[ALG(w\u0003;S)]. SinceR=O(m2\n\u000f2log(m\n\u000f)), we have that log(jW(R)j) =\nmlog(R) =O(mlog(m=\u000f)). Thus we can take s= poly(m;1\n\u000f;1\n\u000e)to get the result. This completes the\nproof.\nG.2.4 Handling Different Sizes\nNow we give a sketch of how to handle the case when each job has an integer size pj>0. For this we need\na slightly different version of Theorem 63 and Corollary 64. Consider the following variant of LP 29:\nmaximizeX\njpjX\nixij\nX\njpjxij\u0014Ti8i2[m]\nX\nixij\u001418j2S\nx\u00150\nAgain we simplify notation and assume xij= 0 wheneveri =2N(j). The following is a corollary of\nTheorem 63. Let Tbe the optimal makespan for a set of jobs\nCorollary 69. For any\u000f>0, there exists weights w2Rm\n+such thatxij(w)yields a fractional schedule with\nmakespan at most (1 +\u000f)T. The weights are computed by running a variant of the algorithm of Theorem 63\nforR=O(m2log(m=\u000f)=\u000f2iterations and produces weights of the form wi= (1 +\u000f)kfork2[\u0000R;R].\nProof .Consider creating pjunit-sized copies of each job j. Note that this only needs to be done concep-\ntually. It is easy to see that writing down LP 29 for this conceptual instance is a relaxation of LP G.2.4.\nConsider running the Algorithm of Theorem 63 with \u000e=\u000f=m,Ti=Tfor alli2[m]and for\nR=O(1\n\u000e2log(m=\u000e)iterations. Note that since Tis the optimal makespan, there exists a solution with\nvalueP\njpj. Thus since the algorithm returns a (1\u0000\u000e)-approximation, we get a solution with value at least\n64\n\n(1\u0000\u000e)P\njpj. The amount that we lose in the objective is exactly the total amount over-allocated in the\nsolution given by the weights. Thus for all i, sinceT\u0015P\njpj=m. we have\nX\njpjxij(w)\u0014T+\u000eX\njpj=T+\u000fP\njpj\nm\u0014(1 +\u000f)T:\nOur learning algorithm will be the same as before, just the jobs will now have sizes. We go through\neach lemma above and prove an analogous version for when there are job sizes. For a job set Sand weights\nw2W(R)letLi(w;S) =P\njpjxij(w). Letpmax= maxjpjbe the maximum job size. For this case our\nassumption becomes ES\u0018D[OPT(S)]\u00154+2\u000f\n\u000f2pmaxlog(mp\u000f).\nLemma 70. Let\u000f >0be given. If ES\u0018D[OPT(S)]\u00154+2\u000f\n\u000f2pmaxlog(mp\u000f), then for all Rand all weights\nw2W(R), we have ES\u0018D[ALG(w;S)]\u0014(1 + 2\u000f) maxiES\u0018D[Li(w;S)].\nProof .Fix anyRandw2W(R). We have the following simply bound on ALG(w;S). It can either be at\nmost (1 +\u000f) maxiE[Li(w;S)], or it is larger in which case it is at mostP\njpj. Thus we have:\nE[ALG(w;S)]\u0014(1 +\u000f) max\niE[Li(w;S)]\n+X\njpjPr[ALG(w;S)>(1 +\u000f) max\niE[Li(w;S)]]\n\u0014(1 +\u000f) max\niE[Li(w;S)] +X\njpjX\niPr[Li(w;S)\u0015(1 +\u000f)E[Li(w;S)]]\nNow we claim that for each i,Pr[Li(w;S)\u0015(1 +\u000f)E[Li(w;S)]]\u0014\u000f=m2. Indeed, if this is the case then\nwe see that\nE[ALG(w;S)]\u0014(1 +\u000f) max\niE[Li(w;S)] +\u000fP\njpj\nm\u0014(1 + 2\u000f) max\niE[Li(w;S)]\nsince maxiE[Li(w;S)]\u0015P\njpj=m, and thus proving the lemma. Thus we just need to show the\nclaim. Recall that Li(w;S) =P\nj2Spjxij(w)and that each job jis chosen to be part of Sindepen-\ndently from distribution Dj. Thuspjxij(w)is an independent random variable in the interval [0;pmax]for\neachj. Applying Theorem 71 to Li(w;S)=pmaxwith\u0016= maxi0E[Li0(w;S)]=pmax, we see that since\n\u0016\u0015E[OPT(S)]=pmax\u00154+2\u000f\n\u000f2log(mp\u000f), we have\nPr\u0014Li(w;S)\npmax>(1 +\u000f)\u0016\u0015\n\u0014exp\u0012\n\u0000\u000f2\n2 +\u000f\u0016\u0013\n\u0014exp\u0012\n\u0000\u000f2\n2 +\u000fE[OPT(S)]\u0013\n\u0014\u000f\nm2\nwhich implies the claim.\nModifying the remaining lemmas is simple. We can do this by replacing most instances of nin the\nproofs withP\njpj.\nG.2.5 Inequalities\nTheorem 71 (Upper Chernoff Bound) .LetX1;X2;:::;Xnbe independent random variables with Xi2\n[0;1]for1\u0014i\u0014n. LetX=P\niXiand\u0016\u0015E[X], then for all \u000f>0we have\nPr[X\u0015(1 +\u000f)\u0016]\u0014exp\u0012\n\u0000\u000f2\n2 +\u000f\u0016\u0013\n65\n\nTheorem 72 (Two-Sided Hoeffding Bound) .LetX1;X2;:::;Xnbe independent random variables with\nai\u0014Xi\u0014bifor1\u0014i\u0014n. LetX=P\niXiand\u0016=E[X]. Then for all t>0we have\nPr[jX\u0000\u0016j\u0015t]\u00142 exp\u0012\n\u0000t2\nP\ni(bi\u0000ai)2\u0013\n66",
  "textLength": 163524
}