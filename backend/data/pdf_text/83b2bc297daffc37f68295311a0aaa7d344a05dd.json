{
  "paperId": "83b2bc297daffc37f68295311a0aaa7d344a05dd",
  "title": "DeepMapping: Learned Data Mapping for Lossless Compression and Efficient Lookup",
  "pdfPath": "83b2bc297daffc37f68295311a0aaa7d344a05dd.pdf",
  "text": "DeepMapping: Learned Data Mapping for Lossless Compression and\nEfficient Lookup\nLixi Zhou\nArizona State University\nTempe, USA\nlixi.zhou@asu.eduK. Selçuk Candan\nArizona State University\nTempe, USA\ncandan@asu.eduJia Zou\nArizona State University\nTempe, USA\njia.zou@asu.edu\nAbstract —Storing tabular data to balance storage and query\nefficiency is a long-standing research question in the database\ncommunity. In this work, we argue and show that a novel DeepMap-\nping abstraction, which relies on the impressive memorization\ncapabilities of deep neural networks, can provide better storage\ncost, better latency, and better run-time memory footprint, all at\nthe same time. Such unique properties may benefit a broad class of\nuse cases in capacity-limited devices. Our proposed DeepMapping\nabstraction transforms a dataset into multiple key-value mappings\nand constructs a multi-tasking neural network model that outputs\nthe corresponding values for a given input key. To deal with\nmemorization errors, DeepMapping couples the learned neural\nnetwork with a lightweight auxiliary data structure capable of\ncorrecting mistakes. The auxiliary structure design further enables\nDeepMapping to efficiently deal with insertions, deletions, and\nupdates even without retraining the mapping. We propose a\nmulti-task search strategy for selecting the hybrid DeepMapping\nstructures (including model architecture and auxiliary structure)\nwith a desirable trade-off among memorization capacity, size,\nand efficiency. Extensive experiments with a real-world dataset,\nsynthetic and benchmark datasets, including TPC-H and TPC-\nDS, demonstrated that the DeepMapping approach can better\nbalance the retrieving speed and compression ratio against several\ncutting-edge competitors.\nI. I NTRODUCTION\nReal-time computations are increasingly pushed to edge\nservers with limited computational and storage capabilities for\nefficiency, cost, and privacy reasons. It is nevertheless critical to\nbalance storage (e.g., on-disk, in-memory) and computational\ncosts (e.g., query execution latency) on these platforms to\nachieve real-time response. Most existing works in balancing\ncompression and retrieval tasks rely on two approaches: (1)\nusing regression to approximate segmented numerical data, such\nas ModelarDB [1], and (2) enforcing ordering over compression,\nsuch as segregated encoding [2]. Unfortunately, both approaches\nprovide sub-optimal latency for exact-match queries (lookups):\nthe former requires scanning each segment, while the latter\nrequires binary search. However, lookup is important for many\nemerging edge applications. Taking self-serve retailing [3], [4],\nas an example, to provide robust service in unreliable environ-\nments, the edge device should be able to manage transaction\nand inventory data locally for random lookups and updates.\nOther examples include large-scale manufacturing, where edge\ndevices must store product quality and defect categories for\nlooking up the quality of individual products and updating\ncategorical quality control parameters [5], and autonomous\nrobots lookup information (e.g., brands and prices) about\nOrder_IDOrder_TypeOrder_Status1ShippingIn Process2Pick-UpDoneFeatures12…Order ID532Neural NetworkOrder_TypeShippingPick-UpOrder_StatusIn ProcessDoneOrder_IDOrder_TypeOrder_Status5ReturnIn Process…………Order_StatusOrder_TypeMultiple Key-ValueMappingsFig. 1: DeepMapping relies on neural networks to memorize key-\nvalue mapping in tabular data.\nobjects nearby from a local database [6]. Existing solutions\nare not sufficiently effective in integrating compression and\nindexing techniques to achieve low storage costs and low query\nlatency simultaneously: (de)compression operations are usually\ncomputational-intensive, whereas indexing techniques impose\nadditional storage overheads.\nTo address these problems, in this work, we argue for a\nnovel data abstraction, called Deep Learned Data Mapping\n(or DeepMapping) , which leverages deep neural networks to\nintegrate the compression and indexing capabilities seamlessly.\nThe main idea is to leverage the impressive learning capabilities\nof neural networks [7] for compression and efficient query\nprocessing of key-value based maps . As illustrated in Figure 1,\na tabular Orders dataset is represented as two mappings\nfrom the key Order_ID to the attributes Order_Type and\nOrder_Status , respectively. In DeepMapping, these two\nmappings are stored as one multi-tasking neural network which\ntakes Order_ID as input feature and outputs Order_Type\nandOrder_Status as labels. DeepMapping is motivated by\nopportunities brought by deep neural network models:\n•Compressibility opportunities. Compressibility is a function\nof the statistical properties of the data, such as the underlying\nkey-value correlations. If structures or patterns exist in the\nunderlying datasets, deep learning models often have signifi-\ncantly smaller sizes than their training datasets. For example,\nthe common crawl dataset is 220Terabytes in size [8], yet the\nlanguage-agnostic BERT sentence embedding model trained\non the dataset is just 1.63Gigabytes in size [9], [10].\n•Hardware acceleration opportunities . In general, (batched)\ninference computations of a neural network can be accelerated\nusing hardware such as GPU processors. ow-end GPUs that\ncost hundreds of dollars, equipped with 4−16GB memory,\nare becoming widely available at the edge [11].arXiv:2307.05861v2  [cs.DB]  26 Sep 2024\n\nWhile there exist related works exploiting these opportunities\nfor compression and learned indexing, respectively (Section II),\nDeepMapping is facing unique challenges :\n•The accuracy challenge. Unlike numerical data for which\naccuracy loss caused by compression is acceptable, categorical\ndata usually requires lossless compression with 100% accu-\nracy. Although the universal approximation theorem [12]–[14]\nstates that given a continuous function defined on a certain\ndomain, even a neural network with a single hidden layer can\napproximate this continuous function to arbitrary precision,\nthe resulting layer’s size could be significantly larger than the\ndataset size. In addition, it is challenging for a deep neural\nnetwork to recognize non-existing keys and avoid the data\nexistence hallucination, as detailed in Sec. IV-B.\n•The modification challenge. Although simple lookup\nqueries, such as SELECT Order_Type FROM Orders\nWHERE Order_ID=19 , can be implemented as inference\noperations over neural models, it is not straightforward to\nimplement update ,insertion ,deletion . Particularly,\nrelying on the incremental model (un)learning for the above\noperations can result in \"catastrophic forgetting\" issues [15].\n•The model search challenge. The overall performance (i.e.,\nsize reduction and accuracy) of DeepMapping depends on\nthe underlying neural network model. Obviously, it would be\ndifficult and laborious for developers to search for architectures\nto replace tabular data manually.\nTo address these challenges, we introduce a novel DeepMap-\ning framework , outlined in Figure 2 (and detailed in Sec-\ntion IV), with the following key contributions :\n(1) A Novel Hybrid Data Representation (Sec. IV-B )Instead\nof trying to increase the model size to achieve the last-\nmile accuracy, we propose to couple a relatively simple and\nimperfect neural network with a lightweight auxiliary structure\nthat manages the misclassified data and an existence indexing:\n•A Compact, Multi-Task Neural Network Model is trained\nto capture the correlations between the key (i.e., input features)\nand the values (i.e., labels) of a given key-value mapping. To\nmemorize the data with multiple attributes, we propose to train\na multi-task neural network, where each output layer outputs\nthe value for its corresponding attribute. The query answering\nprocess takes a (batch of) query key(s) as the input and outputs\nthe predicted value(s).\n•An Auxiliary Accuracy Assurance Structure compresses the\nmappings that are misclassified by the model and a snapshot of\nthe keys, to ensure query accuracy. While the neural network\nmemorizes a significant portion of the data, the auxiliary data\nstructure memorizes misclassified data to achieve 100% overall\naccuracy on data query; An additional bit vector is used to\nrecord the existence of the data.\n(2) Multi-Task Hybrid Architecture Search (MHAS)\n(Sec. IV-C )The objective of the model search process is\nto minimize the overall size of the hybrid architecture: (a) The\narchitecture should maximize the sharing of model layers/pa-\nrameters across the inference tasks corresponding to differentmappings. (b) The model should specialize well for tables with\nattributes that have heterogeneous types and distributions. We\nabstract the search space as a directed acyclic graph (DAG),\ncomprising a collection of knodes, where each node represents\naconfigurable layer and each edge represents a data flow from\nthe source node to the target node. A candidate model is\nsampled as a sub-graph from the DAG. We further propose a\nmulti-task search strategy based on deep reinforcement learning\nthat adaptively tunes the number of shared and private layers\nand the sizes of the layers.\n(3) Workflows for Insertions, Deletions, and Updates\n(Sec. IV-D )To address the challenge of supporting data\nmodifications, we propose a lazy-update process that re-\npurposes the lightweight auxiliary structure outlined above\nbymaterializing the modification operations in this structure\nif the model cannot capture those modifications. The system\ntriggers retraining of the neural network model only when the\nsize of the auxiliary structure exceeds a threshold.\nWe implemented the proposed approach and conducted\nextensive experiments on TPC-H, TPC-DS, synthetic datasets,\nand a real-world crop dataset. We considered baselines,\nsuch as hash-based and array-based representations of the\ndatasets partitioned and compressed by cutting-edge compres-\nsion techniques.The evaluation results demonstrated that the\nDeepMapping approach better balances efficiency, offline and\nrun-time storage requirements against the baselines in our target\nscenarios. Especially in scenarios with limited memory capacity,\nDeepMapping achieved up to 15×speedup by alleviating the\nI/O and decompression costs. In such cases, DeepMapping runs\nin memory. However, the baselines require loading (evicted)\npartitions (back) into the memory and decompressing these to\nanswer random queries.\nII. R ELATED WORKS\nLearned Indexing. Driven by the desirable properties of the\nneural networks outlined in Section I, in recent years, learned\nindex structures [16]–[24], have been proposed to improve\nthe computational efficiency of indexing structures. These\ntechniques apply machine learning to capture the correlation\nbetween the keys and the positions of the queried values in\nunderlying storage. There also exist works [25]–[28] that use a\nmachine learning model to replace the hash function for hash\nindexing. The problem we target in this paper is different from\nlearned indexes in several critical ways:\n•Generally speaking, learned indexing predicts positions in a\n(sorted) array, which is commonly posed as a regression task.\nWe, however, are aiming to learn direct key-value mapping –\nmoreover, the values can be discrete or categorical.\n•For learned indexing, if the queried value is not found in the\npredicted position, the search can continue on the (sorted)\narray until the value is found or determined as non-existing.\nThis is not possible for learning a direct key-value mapping.\n•Learned indexing only compresses the indexing structure,\nbut will not compress the data. Our DeepMapping method\ncombines both losslesscompression and indexing and strikes\nan even better trade-off between storage and lookup latency.\n\nLSTM ControllerAssetsOrder_ID Order_T ype Order_Status\n1 Shipping In Process\n2 Pick-Up Done\n... ... ...\n10000 Pick-Up Done\nAssetsOrder_ID\n1\n2\n...\n10000AssetsOrder_T ype\nShipping\nPick-Up\n...\nPick-Up\nAssetsOrder_Status\nIn Process\nDone\n...\nDoneShared LayersOrder_ID\nPrivate Layers...\n... ...\nOrder_Status Order_T ypeUpdate model\narchitecture\nand parameters\nCompute reward\nbased on model\nsize, auxiliary size,\nand bit vector size  AssetsOrder_ID Order_T ype Order_Status\n5 Pick-Up Returned\n9 Pick-Up Done\nMisclassified Data\nAssets1 2 ... 9999 10000\n1 1 ... 0 1\nBit Vector for Existence Check\nLearned\nModel\n(Sec. IV.A)Auxiliary\nStructure\nHybrid Architecture Search (Sec. IV.C) Novel Hybrid Data Representation (Sec. IV.B)9999 Shipping In ProcessNewly inserted recordInsert, if mis-classified\nby the model\nUpdate the bit vector\nModification W orkflows (Sec. IV.D)\n... ... ...... ...\n... ...Multiple Key-V alue\nMappings (Sec. III)\ninput layer\nL0next node:\nL1\nL1 L2L2 L3\n...Fig. 2: Overview of the proposed neural network-based data compression methods.\nCompression. Semantic compression [29]–[32] leverage cor-\nrelations among columns, which could be captured by tree-\nbased models, auto-encoding methods, and so on, to store a\nminimum number of columns for restoring entire tuples with\nerror bounded. However, these approaches cannot eliminate\nthe errors. In addition, these approaches cannot avoid the\ndecompression overheads and accelerate the lookups. Abundant\nsyntactic compression works [1], [33]–[37] use machine learn-\ning for error-bounded compression of high-dimensional data by\nlevaraging a statistical model to closely capture the underlying\ndistribution of the input data. However, these works mainly\nfocus on numerical data, and their approaches are not extensible\nto lossless compression for categorical data. Several syntactic\ncompression works also aim to balance the compression\nratio and query speed. For example, ModelarDB [1] focuses\non the compression and aggregation queries over streaming\nnumerical time-series data by modeling data as multiple linear\nsegments. Their work cannot be applied to the compression and\nlookups of categorical data. Segregated encoding [2] allows\nquery processing over compressed data without decompression\noverheads. However, the lookup queries require searching in\nordered segregates, which is sub-optimal.\nIII. P ROBLEM AND DESIDERATA\nWe first formulate the problem and desiderata as follows:\n•Single-Relation, Single-Key Mapping. Let\nR(K1, . . . , K l, V1, . . . , V m)be a relation where\nK= (K1, . . . , K l)defines a key that consists of l\nattributes and V1through Vmaremvalue attributes.\nThe goal is to identify a mapping data structure, dµ()\nwhich, given a key k= (k1, . . . , k l)∈domain (K),\nand a target attribute Vi∈ {V1, . . . , V m}, it returns\ndµ(k, Vi) =πVi(σK1=k1∧...∧Kl=kl(R)). Different from the\nkey in relational models, a key in DeepMapping does not\nneed to be a unique identifier. A key can consist of any\nattribute. While we focus on this problem, our approach\nis extensible to the following two problems (See [38] for\ndetailed formalization).\n•Single-Relation, Multiple-Key Mapping. Note that in\npractice, the workload may look up values using differentkey columns from the same relation, thus requiring multiple\nmappings with different keys.\n•Multiple-Relation, Multiple-Key Mapping. Furthermore, in\nmany contexts, such as databases with star schemas, the same\nattribute from one relation (e.g., the fact table) may reference\nattributes in other relations (e.g., the dimension tables). Cross-\ntable lookups require multiple-relation multiple-key mapping.\nFor any of these three alternative problem scenarios, our key\ndesiderata from the mapping data structure, dµ(), are as follows:\n•Desideratum #1 -Accuracy: dµ()is accurate – i.e., it does\nnot miss any data and it does not return any spurious results.\n•Desideratum #2 -Compressibility: dµ()structure is com-\npact in offline disk storage and runtime memory footprint.\n•Desideratum #3 -Low latency: The data structure dµ()is\nefficient and, thus, provides low data retrieval latency.\n•Desideratum #4 -Updateability: dµ()is updateable with\ninsertions of new key-value rows and deletions of some\nexisting keys from the database. Moreover, the data structure\nalso allows changing the value of an existing key.\nIV. D EEPMAPPING ARCHITECTURE\nA. Shared Multi-Task Network\nThis paper uses DeepMapping to manage key-value map-\npings, where the key and values are discrete (e.g., integers,\nstrings, and categorical values). Without loss of generality, we\nconsider a sequence of fully connected layers as the underlying\nneural network architecture, where the strings or categorical\ndata are encoded as integers using one-hot encoding before\ntraining and inference.\nTo achieve high compression rates, some of these layers can\nbe shared across multiple inference tasks within a relation and\nacross relations that have foreign key reference relationships.\nAt the same time, other layers are private to each inference\ntask to improve the accuracy of that particular inference task.\nHow to divide shared/private layers? The first few layers of\nthe neural network, which capture the structures common to\nboth inference tasks, are shared. However, the latter layers of\nthe neural network, which capture the output attribute-specific\nstructures, are private to specialize the network for each output\nattribute. An example can be found in Figure 1, which consists\n\nof two shared fully connected layers (in orange) and two private\noutput layers, one for the Order _Type column (in green) and\nthe other for the Order _Status column (in blue). The number,\ntypes, and sizes of the shared layers and private layers are\nall determined by our multi-task hybrid architecture search\n(MHAS) algorithm, as detailed in Sec. IV-C.\nB. Ensuring 100%Accuracy (Desideratum #1)\nDeepMapping should not miss any data and should not return\nspurious results. It is a challenging task because seeking an\narbitrarily large model [12], [13] to achieve 100% accuracy\nis not appropriate, considering the desiderata include update\nability, compressibility, and low latency. In addition, when\nquerying the non-existing data, it is challenging for the neural\nnetworks to tell whether the tuple exists accurately. That is\nbecause the inference task will predict an output even when\nthe data is not seen in the training data – this makes the\nneural network-based solutions useful for generalizing. Still,\nin our context, any such output will be spurious and need to\nbe avoided, as a hallucination .\nSolution: Lightweight Auxiliary Accuracy Assurance Struc-\ntures. To address these issues, we propose to use a novel\nhybrid data representation , consisting of (1) a compact\nneural network model that memorizes the data (denoted as\nM), (2) an auxiliary accuracy assurance table (denoted as\nTaux) that compresses any misclassified data, and (3) an\nexistence bit vector (denoted as Vexist), whose range corre-\nsponds to the key range – intuitively, each bit marks the\nexistence of the corresponding key.\n1) The Auxiliary Structures ( Taux):To construct Taux, the\nsystem runs all the keys in the input key-value mapping\nthrough the trained model and checks whether the inferred\nresult matches the corresponding value. If not (i.e., the key-\nvalue mapping is misclassified), the key-value pair is stored in\nthe auxiliary table. The misclassified key-value pairs are sorted\nby the key and equally partitioned. All misclassified key-value\npairs in the same partition are sorted by key. (We NEVER rekey\nto make key ordering consistent with value orderings in either\nthe model training process or the auxiliary data management).\nTo further reduce the storage overhead, we apply Z-Standard\nor LZMA compression to each partition before they are stored.\nIn this work, we focus on keys that consist of discrete values\nand use a single dynamic bit array to serve an existence index\nfor the keys, denoted as Vexist. Additionally, the decoding map\n(denoted as fdecode ) that converts predicted labels from integer\ncodes (resulting from one-hot encoding) to their original format,\nis also part of the auxiliary structure.\n2) Lookup Process: The lookup process using the proposed\nhybrid data representation is illustrated in Algorithm 1 - the\nkey aspects of the process are outlined below:\nInference. DeepMapping leverages the ONNX runtime [39] for\noptimizing the inference performance in edge environments.\nExistence check. The algorithm checks the existence of each\nquery key in Vexist to eliminate any spurious results.\nValidation. For any key that passes the existence check, the\nalgorithm checks whether this key was misclassified by theAlgorithm 1 (Parallel) Batch Key Lookup by DeepMapping\n1:INPUT:\nQ: A vector of kencoded query keys.\nM: Pre-trained neural network.\nTaux: Auxiliary table that stores the misclassified data.\nVexist : Bit vector for existence check.\nfdecode : Decoding map.\n2:OUTPUT: R: A vector of nqueried values.\n3:R=M.infer (Q)// running in GPU as batch inference\n4:fori= 1 tondo\n5: ifVexist[Q[i]] == 1 then\n6: ifQ[i]exists in Tauxthen\n7: R[i] =Taux[Q[i]]\n8: end if\n9: else\n10: R[i] =NULL\n11: end if\n12:end for\n13:R=fdecode (R)// decoding\n14:return R\nmodel using the auxiliary table, Taux. To look up the query key\nin the auxiliary table, the algorithm first locates its partition,\nbrings it to the main memory if needed, and decompresses\nit; it then applies a binary search to look up the value within\nthe partition. If the query key is located in the auxiliary table\n(i.e., the key-value pair was misclassified by the model), we\nreturn the value from the auxiliary table. Otherwise, the model’s\noutput is returned as the result. In memory-constrained devices,\nwe free up the space of the least recently used (LRU) partition\nbefore loading the subsequent partition of the auxiliary table\nwhen the memory becomes insufficient. In this case, query keys\nin a batch are sorted before validation so that each partition is\ndecompressed only once for each query batch.\nNote that while their primary benefit is to ensure 100% ac-\ncuracy without necessitating a prohibitively large model, these\nauxiliary structures also enable insert /update /delete on\nthe data. We provide details of these operations in Sec. IV-D .\nC. Multi-Task Hybrid Architecture Search (Desiderata #2,3)\nA critical part of the DeepMapping initialization process is\nto select a neural network architecture to achieve the desiderata\nlisted in Section III. Given a dataset Rthat consists of n\ntuples, where each tuple is represented as <x,y>. Here, x\nrepresents a collection of one or more attributes that serve as\nthe key so that each key uniquely and minimally identifies a\ntuple. yrepresents a collection of mattributes that are disjoint\nwith xand serve as the values. Our goal is to identify a\nhybrid data representation, Mˆ=⟨M, T aux, Vexist, fdecode⟩,\nconsisting of a neural network model, Malong with the\nauxiliary structures, Taux,Vexist, and fdecode , satisfying our\ndesiderata. As aforementioned in Sec. IV-A , we consider a\nmulti-layer fully connected neural network to memorize a\nsignificant portion of the data in the given relation(s). As\ndepicted in Figure 1, some layers (which help abstract the\nkey) are shared across multiple data columns, while others\nare private to each output attribute. Therefore, the problem is\nposed as a multi-task model search problem.\n\nShared\nDAGPrivate\nDAG 1Task 1\nTask 3Private  \nDAG 3Private\nDAG 2Task 2(a) Multi-task tree of DAGs\nHidden\nLayer 1\nHidden\nLayer 2Input\nLayerOutput\nLayer (b) DAG with ≤2hidden layers\nFig. 3: (a) A high-level view of a candidate model and (b) a DAG\n(including all nodes and edges) represents the search space of one\ntree node in (a) – here, the subgraph connected with the red edges\nillustrates a sampled network.\nNeural architecture search is an active research area [40]–\n[46]. To develop our multi-task hybrid architecture search\n(MHAS) strategy, we build on the efficient neural architecture\nsearch (ENAS) strategy [44], which supports parameter sharing\nacross sampled model architectures. While the original moti-\nvation of this parameter sharing is to improve the efficiency\nof NAS by forcing all sampled child models to share weights\nto eschew training each model from scratch, in this paper, we\nargue that this approach can also help reduce the model search\noverhead in multi-task learning by encouraging parameter\nsharing across multiple tasks. Our MHAS search algorithm,\ndiffers from ENAS in several ways: (a) MHAS is extended\nwith the ability to search for multi-task models with shareable\nand private layers. (b) To balance the compressibility brought\nby shared layers and the accuracy obtained by the private\nlayers, MHAS searches within a search space with flexible\nnumbers and sizes of hidden layers among each task’s shared\nand private layers. (c) Since our goal is not to search for a\nneural network but a hybrid structure, an objective function\n(see Eq. 1) governs the search that captures our desiderata.\nMHAS consists of two components: a search space , which\ndefines how the components of the underlying neural network\ncan be connected to enumerate new neural architectures and\na controller algorithm which seeks a structure for the target\nmodel within this search space. We describe both as follows:\n1) MHAS Multi-Task Search Space: Each model in the\nsearch space can be represented as a tree, where each non-leaf\nnode represents a sequence of shared layers, and each leaf\nrepresents a sequence of private layers for a target column.\nFigure 3(a) illustrates a class of models for memorizing a table\nwith a key column and three non-key columns. It has four\nnodes: one for the shared layers (capturing the characteristics\nof the key) and three for the private layers, each corresponding\nto a target column.\nEach node in the tree corresponds to a direct-acyclic graph\n(DAG). A DAG contains a node representing the input layer,\na node representing the output layer, and multiple nodes\nrepresenting candidate intermediate layers. Validated through\nour experiments, tabular data can be represented well using\nfully connected layers [47]. Therefore, we consider each DAG\nnode to be a fully-connected layer (with hyper-parameters,\nsuch as the number of neurons at each hidden layer also beingAlgorithm 2 MHAS Model Search Algorithm\n1:INPUT:\nD: A dataset to be memorized\nθ: The controller model parameters\nW: The weights of all candidate layers in the search space\nNt,Nm,Nc: The number of searching, model training, and\ncontroller training iterations\nDbatch : batch size used in each model/controller training iteration\nm_epochs : number of epochs in each model training iteration\n2:OUTPUT: optimal model structure M\n3:fori= 1 toNtdo\n4: # Controller parameter θis fixed\n5: ifi mod ⌊Nt\nNm⌋== 0 then\n6: # Train a sampled model for Nmiterations\n7: Sample a model Mfrom the search space\n8: fork= 1 tom_epochs do\n9: forDbatch∈Ddo\n10: Update Wthrough M.fit (Dbatch)\n11: end for\n12: end for\n13: end if\n14: ifi mod ⌊Nt\nNc⌋== 0 then\n15: # Model weight Wis fixed\n16: forDbatch∈Ddo\n17: Sample a model Mfrom the search space\n18: Update θthrough L(M, D )\n19: end for\n20: end if\n21:end for\n22:return M\nsearched). The DAG in Figure 3(b) represents a search space\nthat contains up to two hidden layers. Edges of the DAG\nrepresent all possible data flows among these layers (with\neach edge corresponding to a specific model parameter tensor\nthat connects the two layers). Given this, the model search\nprocess will enumerate subgraphs of the DAGs by activating\nanddeactivating network edges – each activated directed edge\nrepresents a connection between two neural network layers.\nDeactivated edges represent connections that do not exist in\nthat particular network. In Figure 3(b), red color is used to\nhighlight activated edges.\nOnce an edge is activated and the destination node is a\nhidden layer, the controller will further choose the number of\nneurons for that layer (i.e., layer size) flexibly to fully explore\nthe entire search space. Suppose Nis the number of different\nlayer sizes, Mis the maximum number of shared/private layers.\nThe overall search space size is N2M[∏︁M\ni=1i∏︁M\nj=1(2∗j−1)]\n(i.e., O(N2MM!(2M−1)!!)). The first term represents all\npossible layer size selections, while the second term represents\nall possible numbers of DAGs (i.e., layer connections).\n2) Multi-Task Model Search Controller: The DeepMapping\nmulti-task model search algorithm is outlined in Algorithm\n2. Since the goal is to learn a sequence of nodes (and train\nthe model parameters at each edge), as in ENAS [44], the\nDeepMapping controller is constructed as a long short-term\nmemory (LSTM) neural network architecture. The LSTM\narchitecture samples decisions via softmax classifiers in an\nautoregressive fashion to derive a sequence of nodes in the\nDAG. The algorithm runs over Ntiterations – at each iteration,\n\nthe algorithm alternatively trains the controller parameter θin\na controller training iteration or the weights of the sampled\nneural architecture Mthrough a model training iteration :\n•During a controller training iteration , for each batch of the\ndata, the controller samples a model from the search space and\nupdates the controller parameter, θ, through the loss function,\nL(Mˆ, D)(Drepresents the dataset to be compressed):\nsize(M) +size(Taux) +size(Vexist) +size(fdecode )\nsize(D)(1)\nThe controller samples a model by taking a node in the DAG\nas input and picking and configuring the next node among the\nones that connect to it; the selected node serves as the input\nfor the next iteration of the process. This process repeats until\nthe output node is selected.\n•During a model training iteration , DeepMapping trains the\nweights of the sampled neural architecture in m_epochs by\nfixing the controller parameters ( θ). We use standard cross\nentropy [48] as the loss function to update model weights\nM. Since the sampled neural architecture is a subgraph of\nthe search space, and these layers may be sampled again in\nfuture iterations, each model training iteration may improve\nthe accuracy and convergence rate of future model training\niterations by sharing parameters across iterations. Note that\neach training iteration is run forsize (D)\nsize (Dbatch )training steps.\nSince the memorization task may, in practice, need a larger\nnumber of iterations to stabilize to a desired level, usually we\nchoose Nm> N c, where Nmis the number of model training\niterations and Ncis the number of controller training iterations.\nD. DeepMapping Modification Operations (Desideratum #4)\nNeural networks are notoriously challenging to be updated:\n(a) Incremental training tends to reduce the accuracy of existing\ndata; (b) It is difficult to tell the neural network to forget\nsomething that is already learned, and (c) Retraining incurs\nsignificant latency. Therefore, to support data modification\noperations, insert /update /delete , we piggy-back on the\nauxiliary structure that we have described in Section IV-B:\nInsertions. Given a collection of key-value pairs ( Dinsert ) to\nbe inserted into the DeepMapping’s hybrid data structure, we\nfirst set the corresponding bit as 1 in the bit vector Vexist for\nthe inserted data to indicate these new data exist at query time.\nThen we run an evaluation of the model on the newly inserted\ndata ( Dinsert ) to check whether the ( M) can generalize to\nthem by running inference over the keys for the target columns.\nOnly those key-value pairs that are incorrectly inferred need\nto be inserted into the auxiliary table ( Taux).\nDeletions. The deletion process is relatively straightforward as\nit can be implemented simply by marking the corresponding\nexistence bit as 0in the bit vector, Vexist, to indicate this data\ndoes not exist, and delete the ones in the auxiliary table, Taux,\nif it was misclassified, which as detailed in Algorithm 4.\nUpdates (Substitutions). We treat updated/replaced key-value\npairs ( Dupdate ) as mis-learned data and insert the new values\ninto the auxiliary table ( Taux) , if they do not have existing\nentries there. Otherwise, the corresponding entries will beAlgorithm 3 Insert\n1: INPUT:\nDinsert : A collection of ntuples to be inserted.\nM: The pre-trained neural network model.\nTaux: The auxiliary table that stores the mis-classified data.\nVexist : The bit vector for existence check.\n2:fori= 1 tondo\n3: // set corresponding existence bit as 1\n4: Vexist[Dinsert [i].key] = 1\n5: iffdecode (M.infer (Dinsert [i].key)) =Dinsert [i].values then\n6: continue\n7: else\n8: // misclassified data is stored in Taux\n9: Taux.add(Dinsert [i])\n10: end if\n11:end for\nAlgorithm 4 Delete\n1: INPUT:\nDdelete : A collection of nkeys to be deleted.\nVexist : A bit vector for existence check.\n2:fori= 1 tondo\n3: // set corresponding existence bit as 0\n4: Vexist[Ddelete [i]] = 0\n5: ifDdelete [i]inTaux then\n6: // removed the corresponding data from Taux, if it exists\n7: Taux.remove (Ddelete [i])\n8: end if\n9:end for\nupdated in-place. Since the keys already exist (otherwise, the\nprocess would be an insertion), we do not need to update the\nexistence index. The process is illustrated in Algorithm 5.\nAlgorithm 5 Update\n1: INPUT:\nDupdate : a vector of ntuples to be updated.\nTaux: Auxiliary table that stores the mis-classified data.\n2:fori= 1 tondo\n3: iffdecode (M.infer (Dupdate [i].key)) =Dupdate [i].values then\n4: Taux.remove (Dupdate [i])\n5: else\n6: // stored the updates as misclassified data in Taux\n7: ifDupdate [i]/∈Taux then\n8: Taux.add(Dupdate [i])\n9: else\n10: Taux.update (Dupdate [i])\n11: end if\n12: end if\n13:end for\nDeepMapping retrains the model and reconstructs the\nauxiliary structures on the underlying data to optimize the\ncompression ratio and query efficiency, only when the auxiliary\ntable becomes too large to satisfy the desiderata in Section III.\nRetraining could be performed offline, in background, and/or\nduring non-peak time. In addition, a significant portion of (an-\nalytics) database workloads are write-once and read-many [49],\n[50], and thus will not frequently trigger retraining.\nE. Extension to Range Queries\nDeepMapping can be extended to support range queries\nusing two different approaches. The first approach is based on\nbatch inferences: (1) It applies the range-based filtering over the\nexistence index to collect all keys that fall into the range; (2) It\nthen runs batch inferences on the collected keys to retrieve the\n\ncorresponding values. The second approach is view-based: (1)\nIt first materializes the results of sampled range queries into a\nview that contains multiple columns, e.g., range-lower-bound,\nrange-upper-bound, range-query-results; (2) It then learns a\nDeepMapping structure on top of the materialized view using\nthe range boundaries as the key; (3) At runtime, given the range\nboundaries, we look up the result in the learned DeepMapping\nstructure. The second approach returns approximate results and\nis more suitable for range aggregation queries. We will extend\nDeepMapping to range queries and other types of queries in\nour future works.\nV. E VALUATION\nA. Experimental Environment Setup\n1) Workloads: We considered lookup queries and modifi-\ncation queries ( insert/update/delete ) over three types\nof datasets with different scale factors (SFs).\nTPC-H [51], with SF 1and10. We removed attributes that\nhave float-point types, such as quantity and retail_price, to\nfocus on attributes that have categorical and integer types.\nTPC-DS [52], also with SF 1and10. Similar to TPC-H, we\nremoved all float-point type attributes. TPC-DS scales domains\nsub-linearly and avoids the unrealistic table ratios that exist\nunder TPC-H [53].\nSynthetic datasets with different levels of key-value corre-\nlations We synthesize 1GB and 10GB datasets by sampling\nTPC-H and TPC-DS columns from tables with low/high key-\nvalue correlations as explained below. Each dataset has a key\ncolumn and single/multiple additional columns.\nThe Single-Column w/ Low Correlation synthetic dataset\nis generated by sampling <OrderKey, OrderStatus> mappings\nfrom the Order table of TPC-H. In this case, the key-value\nmapping only exhibits a Pearson correlation of 1e−4.\nThe Multiple-Column w/ Low Correlation synthetic dataset\nis generated from the TPC-H LineItem table, where the average\nPearson correlations of key-value mappings are below 5e−4.\nThe Single/Multiple-Column w/ High Correlation datasets\nare generated from TPC-DS Customer_Demographics tables.\nAt the same time, the single-column case only selects the\nCD_Education_Status column, and the multiple-column case\nuses all columns from the table. In this table, the key-value\nmappings show a 0.12Pearson correlation on average. The\ncolumns from the table exhibit periodical patterns along the key-\ndimension, which do not exist in the low-correlation dataset.\nReal-World Dataset We sampled a region of the real-world\ncropland data from CroplandCROS [54], an image-based crop\ndistribution data, where each pixel represents one crop type. We\npreprocessed the data to a three-column tabular data consisting\nof latitude, longitude, and crop type.\n2) Hardware Environments: This experimental study has\nused three environments: (1) Small-size machine (our target\nenvironment): An AWS t2-medium instance with two CPUs\nand 4 GB memory, which are used to approximate resource-\nlimited edge environments; (2) Medium-size machine: An AWS\ng4dn.xlarge instance that has 4 CPUs, 16 GB memory, and\none T4 GPU with 16 GB memory. All systems run Ubuntu20.04; (3) Large-size machine: A server that has 24CPU\ncores, 125GB memory, and 1Nvidia-A10 GPU with 24GB\nmemory, which is used to understand DeepMapping’s benefits\nfor in-memory computing.\n3) Comparison Baselines: We compare the compression\nratio and query speed of our DeepMapping ( DM) approach\nagainst the following baselines:\n•Array-based representations without compression ( AB),\nwhere each partition is encoded as a serialized numpy array;\n•Array-based representations with compression( ABC );\n•Hash-based representations with compression ( HBC );\n•Hash-based representations without compression ( HB),\nwhere each partition is a serialized hash table;\nWe considered various compression algorithms for DM (i.e.,\nto compress the auxiliary data structure), ABC, and HBC. We\nused ABC-D to refer to ABC with Dictionary Encoding [55],\nABC-G for ABC with Gzip [56], ABC-Z for ABC with Z-\nStandard [57], and ABC-L for ABC with LZMA [58]. For\nHBC and DM, we only considered thetwo best performing\ncompression algorithms: HBC-Z andDM-Z for HBC and DM\nwith Z-Standard [57], and HBC-L andDM-L for HBC and DM\nwith LZMA [58]; In addition to the aforementioned lossless\ncompression baselines, we also considered the state-of-the-art\nsemantic compression method, DeepSqueeze [32] ( DS). Since\nit is lossy compression, we set the error bound to ϵ= 0.001.\nDeepMapping was implemented in Python with the lat-\nest numpy, bitarray, and ONNX libraries with C-backends.\nWe implemented all baselines in Python with most of the\ncomputational intensive functions backed by C/C++, such as\ndictionary [59], binary search, Numpy functions [60]. For\nDictionary Encoding, Gzip, LZMA, and Z-Standard, we used\npublic software [61]–[64], which are all backed by C/C++\nwith Python bindings. When loading a partition to memory,\nthe corresponding data structure is deserialized. We used the\nstate-of-the-art Pickle [65] library for (de)serialization. The\nsource code is publically available1.\n4) Compression Tuning: We carefully fine-tuned the com-\npression algorithms for each test case. Taking Z-Standard (ztsd)\nas an example, the default compression level of 1 (fastest) is\nthe best for small-batch queries where the data loading time\nand the decompression time dominate the end-end latency.\nHowever, for other cases, after tuning the compression level,\nzstd can reduce the decompression time up to 30% compared\nto the default compression level.\n5) Partition Size Tuning: For each test case, we use grid\nsearch to tune the partition size of DeepMapping and all\nbaselines to ensure the best performance numbers are reported.\nOur observations are as follows: (1) For queries over AB\nand ABC with small batch sizes, the data loading (including\ndeserialization) and decompression time will contribute most\nto the end-end latency. Therefore, a large partition size, such\nas8MB, will reduce the frequency of partition loading and\ndecompression and thus reduce latency. However, as the batch\nsize increases, looking up the data in the array starts dominating\n1https://github.com/asu-cactus/DeepMapping\n\nthe overheads, and a small partition size, such as 128KB, will\nbenefit large-batch queries. (2) For HB and HBC, the lookup\ncomplexity is O(1), and the latency is bottlenecked by the\ndeserialization overheads when loading a partition to memory.\nWe observed that deserializing multiple small hash partitions\nwill be faster than deserializing one large partition, and a small\npartition size, such as 128 KB, leads to optimal performance.\n(3) For DM-Z, similar to AB and ABC, with a small batch size,\nsuch as 1000 , the auxiliary data’s loading and decompression\ntime will dominate the latency, and a relatively large partition\nsize, such as 4 MB, can help reduce the latency. However, for\nlarge batch sizes, such as 100K, the latency is less sensitive\nto the partition size. That is because the auxiliary structure\ncontains only misclassified data and is significantly smaller than\nthe array-based approaches. In the context of DM-L, LZMA\nintroduces additional decompression overhead, which makes\nthe latency sensitive to the partition size. Choosing a small\npartition size, such as 128KB, can enhance query performance.\n6) Architecture Search : For MHAS search, we consider a\nsearch space of up to two shared hidden layers and two private\nhidden layers for each task, which serves well for the targeting\nworkloads in this work. For each layer, we also search the\nnumber of neurons on the layer in a range of [100, 2000].\nAs in [44], the controller is constructed as an LSTM neural\nnetwork with 64hidden units. During MHAS, the controller\nparameters, θ, are initialized uniformly in N(0,0.052)and\ntrained with Adam optimizer at a learning rate of 0.00035 .\nEach model training iteration uses a learning rate of 0.001,\ndecayed by a factor of 0.999. In each iteration, the model\nhas been trained for 5epochs with batch size of 16384 ; the\ncontroller is trained for 1epoch every 50iterations with batch\nsize2048 . Other configurations are as follows: (a) number\nof iterations ( Nt) for the MHAS model search: 2000 ; (b)\nnumber of model training iterations ( Nm):2000 ; (c) number\nof controller training iterations ( Nc):40. The model search\nprocess is followed by training to finetune the accuracy. Model\nsearch and training processes terminate once the absolute value\nof the changed loss is less than 0.0001 .\nB. Lookup Queries\nFor evaluating the query performance, each time we issue a\nbatch of queries that lookup Brandomly selected keys, where\nBvaries from 1,000to100,000. We test for 5times for each\nexperiment and report the average latency.\n1) Overview: We first visualize the trade-offs made by\nDeepMapping and baselines for TPC-H, TPC-DS, as illustrated\nin Figure 4, and Figure 5 respectively. These results demon-\nstrate that DeepMapping provides the best trade-off among all\ncompetitors for an overwhelming majority of the scenarios. In\nthe TPC-DS benchmark, more columns have large cardinalities\n(hundreds to thousands of distinct values) than in TPC-H.\nA consequence is that the memorization is somewhat more\ncomplicated, and TPC-DS is generally harder to compress than\nthe TPC-H dataset. In contrast, some TPC-DS columns strongly\ncorrelate with the key column, making these mappings relatively\neasy to compress. For example, the customer _demographics\nFig. 4: Trade-off between compression ratio and lookup performance\nin TPC-H (SF=10, B=100,000) in the small-size machine – Annota-\ntions are explained in the footnote2.\ncolumn achieved a compression ratio of 0.6%, reducing the size\nfrom 95MB to 0.5MB. The proposed DeepMapping approach\ncan provide significant data size reductions, with an average\nof70.8%in TPC-H and TPC-DS (with both SF=1 and 10).\n2) Detailed Analysis: We first analyze two scenarios: (1)\nthe size of the dataset exceeds memory (Table I) vs. (2) the\ndataset fits into available memory (Table II).\nAs illustrated in Table I, in situations where the dataset\nexceeds the available memory, the in-memory DeepMapping\nstructure (DM-Z) achieves the best retrieval latency, outper-\nforming other baselines. This is because DeepMapping avoids\nthe decompression overheads and reduces I/O overheads. For\nexample, in TPC-H with SF 10, the size of the uncompressed\nLineitem dataset is about 3.2GB in length (after removing the\nnumerical attributes), which is close to the physical memory\nsize in our small-size environment that has 4GB memory\nand exceeds the available memory pool that we set as 3\nGB. As illustrated in Table I, both the compression ratio\nand the retrieving speed of DeepMapping (DM-Z and DM-L)\nsignificantly outperform the best baselines (i.e., ABC-Z is the\nfastest and ABC-L achieves the best compression ratio among\nall baselines except DM-Z and DM-L) up to 3.4×and3.7×\nrespectively. For synthetic datasets, the compression ratio and\nlatency improvements compared to the second best (i.e., ABC-\nZ for query latency and ABC-L for compression ratio) are up\nto43×and44×, respectively.\nFor the comparisons on the real-world crop dataset, DM-Z\noutperforms the fastest compression baseline, ABC-Z, by up\nto2.08×in the query lookup. DM-Z also saves 16% more\nspace compared to the most storage-efficient baseline, ABC-\nL. In the rest of the baselines (excluding DM-Z and DM-\nL), ABC-Z achieves the best query latency, while ABC-L\nachieves the best compression ratio. However, the LZMA\ncompression algorithm used by ABC-L incurs significantly\nhigher decompression overhead, resulting in slower lookup\nperformance compared to ABC-Z. By applying LZMA on the\n2In the figures, uncompressed data is always at the point (1.0,1.0)).\nConfigurations closer to (0.0,0.0)are more desirable regarding compression\nratios and latencies. The dashed arc,⊵, indicates the points in the space with\nthe same L2distance to (0.0,0.0)as the DeepMapping algorithm – hence\nconfigurations outside of this arc have a less desirable compression/latency\ntrade-off than DeepMapping; the →indicates cases where the access latency\nis more than 3×slower than accessing the uncompressed data.\n\nFig. 5: Trade-off between compression ratio and lookup performance in TPC-DS (SF=10, B=100,000) in the small-size machine – Annotations\nare explained in the footnote2.\nauxiliary data structure, DM-L can achieve the best storage\nsize among all baselines. Compared to ABC-Z, DM-L achieves\na speedup of 1.27×in performance and reduces the space by\n3×. DeepSqueeze targets lossy compression. It has to create a\nset of quantization bins to minimize the errors, which makes it\nunable to compress the data effectively. Also, the DeepSqueeze\nrelies on a complex AutoEncoder for decompression, leading to\nhigh memory consumption and leads to out-of-memory (OOM)\nerrors, while benchmarking datasets that exceed the available\nmemory (e.g., synthetic datasets and the real-world crop dataset\non the small-size machine).\nWe further compare the performance of DeepMapping and\nother baselines over datasets that fit the available memory pool\nsize in all three machine environments. Some of the results\n(for cases with SF=10 and table size larger than 30MB) are\nillustrated in Table II highlighting the following findings:\n•Even when the dataset fits memory, DeepMapping provides\nthe highest benefits for relatively large datasets, primarily when\na strong correlation exists in the underlying data.\n•In all considered cases, DeepMapping provides a good\ncompression ratio, easily outperforming the compressed array-\nbased and hash-based representations.\n•When the dataset fits into memory, DeepMapping could be\nslower in query speed than baselines, because the bottleneck\nis in lookup rather than data-loading. However, when high\nkey-value correlations exist, DeepMapping spends less time\nin checking auxiliary table and provides up to 1.5×query\nspeedup (e.g., the customer_demographics table).\n•For compressed baselines, the need to decompress data will\nsignificantly damage the lookup performance compared to the\nuncompressed baselines in most of cases.\nFigure 6 provides a detailed breakdown of the DeepMapping\nstorage mechanism for TPC-H with different scale factors. As\nshown in Figure 6, the bulk of the storage is taken by the\nauxiliary table in most scenarios. Although the model size is\nsignificantly smaller than the auxiliary data structure, the model\nmemorized 68% and66% of the tuples on average for TPC-H\nSF=1 and SF=10 respectively. This observation justifies our\nMHAS search algorithm, which targets optimizing the entire\nsize of the hybrid architecture rather than searching for a perfect\nmodel that achieves 100% accuracy. The observed pattern also\napplies to other datasets. The models can memorize 78%,80%,\nand81% of tuples from TPC-DS, SF=10, synthetic datasets,\n020406080100\nsupplier part orders lineitem customer\nExistence Vector Size Model Size Auxiliary Table Size86.53%5.55%7.92%\n93.69%0.48%5.83%\n98.31%1.67%0.01%\n75.07%20.92%4.01%\n75.44%14.96%10.49%Percentage\nPercentage of Data Stored In ModelPercentage of Data Stored In Auxiliary Table70%\n30%55%\n45%61%\n39%68%\n32%88%\n12%(a) TPC-H (Scale Factor=1)\n020406080100\nsupplier part orders lineitem customer88.86%3.02%8.12%\n93.71%2.12%4.18%\n98.77%1.21%0.01%\n94.19%0.79%5.03%\n87.06%1.93%11.01%Percentage\nExistence Vector Size Model Size Auxiliary Table Size\nPercentage of Data Stored In ModelPercentage of Data Stored In Auxiliary Table80%\n20%56%\n44%64%\n36%60%\n40%70%\n30%\n(b) TPC-H (Scale Factor=10)\nFig. 6: DeepMapping storage breakdown in the small-size machine.\nand the real-world cropland dataset, respectively.\nFigure 7 further shows a breakdown of point query latency\nover TPC-H tables. For DeepMapping, in most cases, the neural\nnetwork inference overheads are insignificant, and most of the\ntime is spent querying the auxiliary structure. DeepMapping\nsignificantly reduced the decompression overheads and the I/O\noverheads, which were included in the purple bar in Figure 7.\nIn addition, we also observed significant runtime memory\nreduction brought by DeepMapping when the datasets fit into\nmemory, which explained the query latency benefits of DM\non the medium and large-size machines as shown in Table II.\nC. Modification Queries: Insertion/Deletion/Updates\nInsertion. We focus on the synthetic datasets where the value\npart in the key-value pairs consists of multiple columns. We\ninsert a varying number of key-value pairs following and\nNOT following the underlying distribution of the data: (1) As\nillustrated in Tab. III, we first insert low-correlation data (i.e.,\nunseen tuples sampled from the TPC-H Lineitem table) into the\nlow-correlation multi-column synthetic dataset (, which is also\nsampled from the same table). Then, we insert high-correlation\ndata into the high-correlation multi-column synthetic dataset.\n(2) As illustrated in Tab. IV, we first insert highly-correlated\ndata into the low-correlation synthetic dataset, and then insert\nlow-correlation data into the high-correlation synthetic dataset.\nIn Sec. V-C,DM-Z1 represents DM-Z (DeepMapping using\nZ-Standard to compress the auxiliary data structure) with\n\nTABLE I: Offline storage size and query latency for datasets that exceed available memory pool on small-size machine.\nWorkloads Metrics AB HB ABC-D ABC-G ABC-Z ABC-L HBC-Z HBC-L DS DM-Z DM-L\nTPC-H SF=10\nLineitemStorage size (MB) 3,203 4,698 833 479 573 359 702 491 1,928 461 293\nLatency, B=1K (sec) 1.1 2.0 1.1 1.6 1.6 2.2 1.5 1.3 32,921 0.3 0.6\nLatency, B=10K (sec) 5.7 13.6 9.5 7.7 7.6 9.6 10.3 12.9 38,431 2.0 2.6\nLatency, B=100K (sec) 101 297 133 35 30 47 165 249 42,779 10 28\nSynthetic - Single\nColumn w/ Low\nCorrelationStorage size (MB) 10,000 23,284 4,175 1,668 1,415 815 2,084 1,089 624 856 563\nLatency, B=1K (sec) 1.8 7.9 2.1 1.6 1.7 2.2 7.0 8.0 failed 1.5 1.6\nLatency, B=10K (sec) 22.4 92.6 23.9 20.7 19.6 25.3 67.6 74.5 failed 15.9 17.2\nLatency, B=100K (sec) 158 742 204 119 119 164 635 820 failed 30 54\nSynthetic - Single\nColumn w/ High\nCorrelationStorage size (MB) 10,000 31,691 4,171 1,563 1,127 560 2,342 783 648 13 13\nLatency, B=1K (sec) 2.0 16.5 4.1 1.6 1.7 2.9 15.2 20.5 failed 0.8 0.9\nLatency, B=10K (sec) 11.8 178.2 5.1 11.2 10.1 20.6 149.4 185.4 failed 1.2 3.9\nLatency, B=100K (sec) 148 1,112 204 102 105 155 912 1011 failed 2 9\nSynthetic - Multi\nColumn w/ Low\nCorrelationStorage size (MB) 10,000 14,678 2,521 1,283 1,432 875 1,712 1,180 2,372 984 678\nLatency, B=1K (sec) 1.4 6.7 4.9 1.4 1.2 2.2 4.0 4.8 failed 1.1 3.0\nLatency, B=10K (sec) 23.9 72.1 29.6 22.3 20.5 26.3 43.6 52.9 failed 17.8 18.5\nLatency, B=100K (sec) 148 443 153 125 101 141 270 363 failed 27 84\nSynthetic - Multi\nColumn w/ High\nCorrelationStorage size (MB) 10,000 15,044 4,638 900 666 355 1,213 514 2,291 26 21\nLatency, B=1K (sec) 1.8 5.6 5.2 1.3 1.4 1.8 4.4 4.5 failed 0.3 0.6\nLatency, B=10K (sec) 11.8 71.3 32.4 10.4 9.1 14.9 40.0 48.5 failed 0.9 1.8\nLatency, B=100K (sec) 145 474 117 191 95 130 329 409 failed 4 12\nReal-world Crop\nDatasetStorage size (MB) 15,414 52,247 7,707 1,672 1,014 341 2,599 834 4,914 293 99\nLatency, B=1K (sec) 15.7 214.6 5.1 7.2 3.3 11.6 205.3 240 failed 0.8 2.3\nLatency, B=10K (sec) 33.3 342.3 9.0 12.3 5.9 19.9 334.5 381 failed 1.1 4.1\nLatency, B=100K (sec) 38.8 345.8 11.4 15.0 9.4 24.8 332.2 386 failed 4.5 7.4\nTABLE II: Offline storage size and query latency for datasets that fit memory pool on small-size (latency-small), medium-size (latency-\nmedium), and large-size (latency-large) machines, B=100,000.\nWorkloads Metrics AB HB ABC-D ABC-G ABC-Z ABC-L HBC-Z HBC-L DS DM-Z DM-L\nTPC-H SF=10\nOrdersStorage size (MB) 343 716 145 41 42 27 67 45 114 34 19\nLatency-Small (sec) 8 29 10 9 8 12 31 36 872 6 7\nLatency-Medium (sec) 5 12 7 6 5 9 13 16 41 3 5\nLatency-Large (sec) 4 9 4 5 5 8 10 15 24 3 3\nTPC-H SF=10\nPartStorage size (MB) 61 110 21 13 17 8 19 12 28 13 8\nLatency-Small (sec) 8 13 9 13 8 13 13 15 160 7 8\nLatency-Medium (sec) 5 3 6 3 5 7 13 10 8 4 5\nLatency-Large (sec) 5 3 4 4 5 6 3 4 7 3 4\nTPC-DS SF=10\nCatalog_salesStorage size (MB) 327 681 137 74 69 43 106 68 303 77 37\nLatency-Small (sec) 8 17 8 10 9 12 17 29 1,461 8 8\nLatency-Medium (sec) 5 11 7 7 5 8 12 26 55 4 5\nLatency-Large (sec) 5 9 5 5 5 6 10 20 50 4 4\nTPC-DS SF=10\nCus-\ntomer_demographicsStorage size (MB) 95 142 44 9 5 3 9 5 64 0.5 0.5\nLatency-Small (sec) 10 9 13 10 10 11 9 13 517 10 10\nLatency-Medium (sec) 4 3 3 5 4 6 3 4 14 2 2\nLatency-Large (sec) 6 5 6 5 5 7 6 6 13 6 4\nTPC-DS SF=10\nCatalog_returnsStorage size (MB) 37 12 19 10 9 6 12 8 29 9 5\nLatency-Small (sec) 8 6 7 9 8 11 6 7 207 7 8\nLatency-Medium (sec) 6 5 4 6 6 8 4 5 8 4 5\nLatency-Large (sec) 4 3 4 4 4 5 3 4 6 3 3\nretraining triggered after 200MB data gets modified, e.g.,\ninserted/deleted, while DM-Z represents DM-Z w/o retraining.)\nAs illustrated in Tab. III and Tab. IV, our Deep Mapping\napproaches DM-Z and DM-Z1 outperformed other baselines\nfor the compressed storage size and the query latency over the\ncompressed data. DM-Z1 can achieve the optimal compression\nratio and query speed because DM-Z1 will trigger retraining\nonce. Compared to DM-Z, DM-Z1 leads to a more reasonable\nhybrid structure with a similar storage size. DM-Z1’s searched\nnetwork consists of a slightly bigger and more accurate model\nand a smaller auxiliary structure than DM-Z. This structure\noptimization contributes to the reduction in query latency. As\nillustrated in Tab. IV, DM-Z trained on the low-correlation\nsynthetic data is robust to the input data that follows the high-\ncorrelation distribution.However, inserting low-correlation datainto high-correlation data leads to a larger auxiliary structure\nthan the results in Tab. III. The retrained model achieved\ncompression ratios similar to the cases of inserting data of\nsimilar distributions in Tab. III, which demonstrates the model’s\ncapability of learning the data mapping.\nRetraining overheads is triggered after inserting 200MB\ndata to the original 1GB data ( 1.2GB in total). For the low-\ncorrelation case, the MHAS model search process takes 2hours\n13mins, while the training/fine-tuning after the model search\nonly takes 529seconds. The high-correlation case spent 3hours\n5mins in MHAS searching and 694seconds in the post-search\ntraining. The model search and training time is significantly\nslower when the datasets exhibit high key-value correlations.\nThat’s because the compressibility of the underlying data is\nhigher, and the search space is larger than the low-correlation\n\n0\n2000\n4000\n6000\n8000\nArray-\nbased\nHash-\nbased\nArray-\nbased\nw/zstd\nHash-\nbased\nw/ zstd\nDeepMapping\n1\n10\n100\n1000\n10000\n100000\n1000000\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n0\n2000\n4000\n6000\n8000\n10000\nArray-\nbased\nHash-\nbased\nArray-\nbased\nw/zstd\nHash-\nbased\nw/ zstd\nDeepMapping\nArray-\nbased\nHash-\nbased\nArray-\nbased\nw/zstd\nHash-\nbased\nw/ zstd\nDeepMapping\nArray-\nbased\nHash-\nbased\nArray-\nbased\nw/zstd\nHash-\nbased\nw/ zstd\nDeepMapping\nArray-\nbased\nHash-\nbased\nArray-\nbased\nw/zstd\nHash-\nbased\nw/ zstd\nDeepMappingExistence Check\nNeural Network\nLookup (Auxiliary)\nData Loading + Decompression (if apply)\nLocate Partition\nOthercustomer lineitem\norders part\nsupplierLatency (ms) Latency (ms) Latency (ms)\nLatency (ms) Latency (ms)Fig. 7: Breakdown of the end-end latency for TPC-H, SF=10,\nB=100,000 in the small-size machine.\nAverage Insertion Time\nPer Tuple(ms)\nArray-based w/zstd - ABC-ZArray-based - AB\nHash-based - HB\nHash-based w/zstd - HBC-Z\nDeepMapping w/zstd - DM-Z\nFig. 8: Comparison of insertion time with varying batch sizes under\nthe Multi-column with Low Correlation (1GB)\ncase. As a result, it takes more time to converge the loss and\nstop the MHAS and training early.The retraining could run\noffline, in backgrounds, or at non-peak time. For DM-Z, which\ndoes not retrain, its insertion speed is faster than baselines, as\nillustrated in Figure 8.\nIn addition, we observed that the query performance of the\nhashing-based baselines(HB and HBC) is significantly worse\nthan other methods. That is due to the hash table representation,\nwhich results in a larger storage size and substantially higher\ndeserialization overheads. Consequently, loading hash partitions\nfrom disk to memory is significantly more expensive than other\nbaselines. Therefore, they are significantly slower if many hash\npartitions cannot fit into and need to be loaded into the memory,\nas shown in the purple bar in Figure 7.\nDeletion and Updates. As illustrated in Table V, DM and DM1\noutperform the best of baselines by 1.5×to more than 10×\nin query speed and 1.3×to more than 10×in compression\nratio. DM1’s query speed is slower than DM in a few cases\nbecause of the randomness in decompressing Vexist (due to\nthe random distribution of the 1bits). Other observations are\nsimilar to the insertion case. The update operation is similar\nto an insertion. Therefore, we do not elaborate on evaluation\nresults for the update operation to save space.\nD. Multi-Task Hybrid Architecture Search (MHAS)\nWe have discussed the MHAS overheads (i.e., retraining) in\nSec. V-C In this section, we further evaluate the effectivenessTABLE III: Comparison of the compressed storage size and the\nquery latency (with B = 100,000) after inserting varying data (follows\nthe original distribution) to synthetic datasets using various systems,\non the small machine. (DM-Z: w/o retrain; DM-Z1: retrains when\n200MB data is inserted)\nInsertion Size (MB) 0 100 200 300 400 500 600\nMulti-column with Low Correlation (Uncompressed size: 1GB)\nDM-Z-Storage (MB) 100 110 120 129 139 149 159\nDM-Z-Query (ms) 4,483 4,614 4,605 4,610 4,621 4,618 4,642\nDM-Z1-Storage (MB) - - 120 129 139 149 159\nDM-Z1-Query (ms) - - 4,302 4,306 4,343 4,325 4,330\nAB-Storage (MB) 1,000 1,100 1,200 1,300 1,400 1,500 1,600\nAB-Query (ms) 9,413 9,215 9,307 9,858 9,933 9,515 9,343\nABC-Z-Storage (MB) 143 158 172 186 200 215 229\nABC-Z-Query (ms) 7,832 7,539 7,269 7,380 7,178 7,304 7,381\nHB-Storage (MB) 1,468 1,615 1,762 1,908 2,055 2,202 2,349\nHB-Query (ms) 59,362 59,828 58,116 57,629 59,244 58,246 60,102\nHBC-Z-Storage (MB) 172 189 206 223 240 257 274\nHBC-Z-Query (ms) 33,977 34,151 33,421 33,933 33,413 33,100 33,595\nMulti-column with High Correlation (Uncompressed size: 1GB)\nDM-Z-Storage (MB) 5 14 23 33 42 51 61\nDM-Z-Query (ms) 2,210 2,398 2,399 2,337 2,403 2,366 2,340\nDM-Z1-Storage (MB) - - 21 30 40 49 58\nDM-Z1-Query (ms) - - 2,197 2,188 2,293 2,292 2,285\nAB-Storage (MB) 1,000 1,100 1,200 1,300 1,400 1,500 1,600\nAB-Query (ms) 9,998 9,543 9,517 9,972 9,933 9,741 10,122\nABC-Z-Storage (MB) 67 73 80 87 93 100 107\nABC-Z-Query (ms) 6,823 6,867 6,875 6,759 7,077 6,999 6,814\nHB-Storage (MB) 1,504 1,655 1,805 1,956 2,106 2,256 2,407\nHB-Query (ms) 63,330 62,595 61,269 62,445 65,629 61,940 61,877\nHBC-Z-Storage (MB) 121 133 146 158 170 182 194\nHBC-Z-Query (ms) 32,416 32,118 32,142 32,293 33,085 33,668 32,220\nTABLE IV: Comparison of the compressed storage size and the\nquery latency (with B = 100K) after inserting varying data that does\nnotfollow the original distribution on the small machine. (DM-Z:\nw/o retrain; DM-Z1: retrains when 200MB data is inserted)\nInsertion Size (MB) 0 100 200 300 400 500 600\nMulti-column with Low Correlation (Uncompressed size: 1GB)\nDM-Z-Storage (MB) 100 109 119 129 139 149 158\nDM-Z-Query (ms) 4,483 4,468 4,691 4,588 4,557 4,637 4,556\nDM-Z1-Storage (MB) - - 104 105 107 109 111\nDM-Z1-Query (ms) - - 4183 4,173 4,253 4,184 4,180\nAB-Storage (MB) 1,000 1,100 1,200 1,300 1,400 1,500 1,600\nAB-Query (ms) 9,413 9,457 9,564 9,9351 9,501 9,151 9,862\nABC-Z-Storage (MB) 143 146 149 151 154 157 159\nABC-Z-Query (ms) 7,832 7,978 8,012 7,703 7,679 8,129 8,155\nHB-Storage (MB) 1,468 1,615 1,762 1,908 2,055 2,202 2,349\nHB-Query (ms) 59,362 59,519 60,795 59,339 60,337 60,712 61,015\nHBC-Z-Storage (MB) 172 177 183 189 195 200 206\nHBC-Z-Query (ms) 33,977 29,344 29,407 29,458 28,839 29,466 30,992\nMulti-column with High Correlation (Uncompressed size: 1GB)\nDM-Z-Storage (MB) 5 18 31 44 57 70 83\nDM-Z-Query (ms) 2,210 2253 2245 2231 2256 2274 2237\nDM-Z1-Storage (MB) - - 23 32 41 50 59\nDM-Z1-Query (ms) - - 2,173 2,189 2,207 2,167 2,177\nAB-Storage (MB) 1,000 1,100 1,200 1,300 1,400 1,500 1,600\nAB-Query (ms) 9,998 9,921 9,676 9,512 9,609 9,851 9,743\nABC-Z-Storage (MB) 67 83 99 116 132 148 164\nABC-Z-Query (ms) 6,823 6,905 6,922 6,878 6,613 6,840 6,826\nHB-Storage (MB) 1,504 1,655 1,805 1,956 2,106 2,256 2,407\nHB-Query (ms) 63,330 63,796 64,831 61,905 62,014 60,647 61,456\nHBC-Z-Storage (MB) 121 141 160 180 199 218 238\nHBC-Z-Query (ms) 32,416 34,160 35,959 34,647 36,736 34,618 35,713\nof our proposed MHAS algorithm, using the TPC-H, SF=1.\nIn Figure 9, we plot the sampled models’ compression ratio\nagainst the controller’s training process.\nTraining Time vs. Compression Time Taking the LineItem\ntable from TPC-H SF=1, as an example, DM takes 1 hour and\n25 mins for the model search (MHAS) to converge and takes 5\nmins for model training. DeepSqueeze (DS) requires 11 mins\nto encode the data. Z-Standard takes 80 seconds for compres-\n\nTABLE V: Comparison of the compressed storage size and the query\nlatency (with B = 100,000) after deleting varying data from synthetic\ndatasets using various systems, on the small machine. (DM-Z: w/o\nretrain; DM-Z1: retrains when 200MB data is deleted)\nDeletion Size (MB) 0 -100 -200 -300 -400 -500 -600\nMulti-column with Low Correlation (Uncompressed size: 1GB)\nDM-Z-Storage (MB) 100 92 84 75 66 56 47\nDM-Z-Query (ms) 4,483 3,885 3,824 4,000 3,779 3,834 3,771\nDM-Z1-Storage (MB) - - 84 75 66 56 47\nDM-Z1-Query (ms) - - 3,496 3,430 3,310 3,321 3,169\nAB-Storage (MB) 1000 900 800 700 600 500 400\nAB-Query (ms) 9,413 7,548 7,998 7,064 6,402 6,207 6,267\nABC-Z-Storage (MB) 143 129 115 102 88 74 60\nABC-Z-Query (ms) 7,794 6,428 6,470 6,101 5,841 5,719 5,444\nHB-Storage (MB) 1,504 1,322 1,175 1,029 882 736 589\nHB-Query (ms) 63,330 53,944 49,635 42,387 28,444 19,538 20,381\nHBC-Z-Storage (MB) 172 156 140 124 108 91 75\nHBC-Z-Query (ms) 33,977 24,426 21,672 19,498 17,730 14,715 12,910\nMulti-column with High Correlation (Uncompressed size: 1GB)\nDM-Z-Storage (MB) 5 6 6 6 5 5 4\nDM-Z-Query (ms) 2,210 2,180 2,038 1,933 1,844 1,818 1,753\nDM-Z1-Storage (MB) - - 4 4 4 4 4\nDM-Z1-Query (ms) - - 2,211 2,092 1,954 1,839 1,676\nAB-Storage (MB) 1000 900 800 700 600 500 400\nAB-Query (ms) 9,998 8,295 8,136 8,148 7,150 6,657 6,054\nABC-Z-Storage (MB) 67 64 58 53 47 41 35\nABC-Z-Query (ms) 6,823 6,660 6,172 6,551 6,018 5,982 5,931\nHB-Storage (MB) 1,504 1,354 1,204 1,054 904 754 604\nHB-Query (ms) 63,330 56,836 52,929 48,052 35,353 20,715 20,862\nHBC-Z-Storage (MB) 121 112 102 90 78 66 55\nHBC-Z-Query (ms) 32,416 30,987 23,187 20,906 18,723 15,900 13,721\norders\nFig. 9: Compression ratios during MHAS (TPC-H, scale =1; plots\nsmoothed with running average window of 500).\nsion. LZMA spends 86 seconds in compression. In addition,\nHashtable with Z-Standard (HBC-Z) and LZMA (HBC-L)\nspend 82 and 152 seconds in compression respectively.\nDespite DeepMapping’s significant improvements in bal-\nancing compression ratio and query performance, the search\nand training of the neural network model is expensive. Our\nfuture work will optimize this process for DeepMapping, e.g.,\nleveraging model reuse and transfer learning [66].\nAs we see here, at the very beginning of the search stage,\nthere is a \"flat\" region where the compression ratio is not yet\ndecreasing – this is because, at the early stages, the sampled\nmodels are not yet capable of memorizing the data. In fact,\nat this stage, the data structure size may be larger than the\noriginal data since much of the memorization work is left to the\nauxiliary table. As the controller training proceeds, however,\nthe sampled models are quickly getting better at memorizingthe data, and the compression ratio improves significantly.\nFigure 10 illustrates the trade-ff between compression ratio\nand latency during the search. In the figure, each dot corre-\nsponds to a sampled architecture, and each color corresponds to\na certain search stage. Initially, samples may cover a large range,\nindicating that the model search has not stabilized. However,\nas the search progresses, the samples start clustering in an\nincreasingly shrinking region in the search space, illustrating\nthe effectiveness of the MHAS strategy.\nFig. 10: Progression of compression ratio vs. latency trade-off\nduring MHAS search process (TPC-H part table, scale = 1; each\ndot corresponds to a sampled architecture)\nVI. C ONCLUSIONS\nIn this work, we proposed a novel DeepMapping structure\nto achieve lossless compression and desirable query speed at\nthe same time. It is achieved by automatically searching multi-\ntasking deep neural networks to model key-value mappings in\nrelational tables while using an auxiliary structure to manage\nmisclassified data and support insert/update/delete .\nThe evaluation observations include:\n•DeepMapping achieves the best compression ratio and\nretrieval speeds for large datasets, especially when the key\nstrongly correlates to the value. When the uncompressed\ndataset exceeds memory but the DeepMapping structure fits\ninto memory (ensured by the MHAS), DeepMapping helps\nsignificantly reduce the I/O and decompression overheads and\noutperforms other baselines.\n•The hashing baseline may achieve better retrieval speeds for\nsmall datasets that fit memory because the hashtable lookup\noperation is cheaper than inference computations. However,\nin these cases, DeepMapping achieves a better compression\nratio than the hashing baseline applied with state-of-the-art\ncompression techniques. For large-scale datasets that do not\nfit into memory, hashing-based approaches perform the worst\ndue to the deserialization complexity of hash structures.\n•The MHAS framework can effectively decide whether a\nproper DeepMapping structure exists for the given tabular\ndataset that can gain compression size and retrieval speeds.\nVII. A CKNOWLEDGMENT\nWe thank all anonymous reviewers for their valuable feed-\nback. Special thanks to Rajan Hari Ambrish for insightful\ndiscussion and Pratanu Mandal, for providing essential crop\ndataset information. This work is supported by the Amazon\nResearch Award, NSF grants #2144923 and #2311716, U.S.\nDepartment of Homeland Security under Grant Award Number\n17STQAC00001-07-00, and U.S. USACE GR40695.\n\nVIII. D ISCLAIMER\nThe views and conclusions contained in this document are\nthose of the authors and should not be interpreted as necessarily\nrepresenting the official policies, either expressed or implied,\nof the U.S. Department of Homeland Security.\nREFERENCES\n[1]S. K. Jensen, T. B. Pedersen, and C. Thomsen, “Modelardb: Modular\nmodel-based time series management with spark and cassandra,” Pro-\nceedings of the VLDB Endowment , vol. 11, no. 11, pp. 1688–1701,\n2018.\n[2]V . Raman and G. Swart, “How to wring a table dry: Entropy compression\nof relations and querying of compressed relations,” in Proceedings of\nthe 32nd international conference on Very large data bases . Citeseer,\n2006, pp. 858–869.\n[3]X. Wang, Y . D. Wong, and K. F. Yuen, “Does covid-19 promote self-\nservice usage among modern shoppers? an exploration of pandemic-\ndriven behavioural changes in self-collection users,” International Journal\nof Environmental Research and Public Health , vol. 18, no. 16, p. 8574,\n2021.\n[4]J.-K. Kim, J.-J. Yang, and Y .-K. Lee, “How do self-service kiosks improve\ncovid-19 pandemic resilience in the restaurant industry?” Sustainability ,\nvol. 15, no. 13, p. 10168, 2023.\n[5]L. D. Xu and L. Duan, “Big data for cyber physical systems in industry\n4.0: a survey,” Enterprise Information Systems , vol. 13, no. 2, pp. 148–\n169, 2019.\n[6]X. Ye and Y . Yang, “Hierarchical and partially observable goal-driven\npolicy learning with goals relational graph,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2021, pp. 14 101–14 110.\n[7]J. Zou, “Using deep learning models to replace large materialized\nviews in relational database,” in 11th Conference on Innovative\nData Systems Research, CIDR 2021, Virtual Event, January 11-15,\n2021, Online Proceedings . www.cidrdb.org, 2021. [Online]. Available:\nhttp://cidrdb.org/cidr2021/papers/cidr2021_abstract05.pdf\n[8]J. M. Patel and J. M. Patel, “Introduction to common crawl datasets,” Get-\nting Structured Data from the Internet: Running Web Crawlers/Scrapers\non a Big Data Production Scale , pp. 277–324, 2020.\n[9] “Tensorflow hub,” https://tfhub.dev/, Last accessed on Sept 1st, 2023.\n[10] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,\nF. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov,\n“Unsupervised cross-lingual representation learning at scale,” arXiv\npreprint arXiv:1911.02116 , 2019.\n[11] “Nvidia gpu for edge computing,” https://on-demand.gputechconf.com/\nai-conference-2019/T6-2_Steve%20Byun_Jetson%20&%20ISAAC.pdf,\nLast accessed on Sept 1st, 2023.\n[12] G. Cybenko, “Approximation by superpositions of a sigmoidal function,”\nMathematics of control, signals and systems , vol. 2, no. 4, pp. 303–314,\n1989.\n[13] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward\nnetworks are universal approximators,” Neural networks , vol. 2, no. 5,\npp. 359–366, 1989.\n[14] G.-B. Huang, “Learning capability and storage capacity of two-hidden-\nlayer feedforward networks,” IEEE transactions on neural networks ,\nvol. 14, no. 2, pp. 274–281, 2003.\n[15] M. McCloskey and N. J. Cohen, “Catastrophic interference in connec-\ntionist networks: The sequential learning problem,” in Psychology of\nlearning and motivation . Elsevier, 1989, vol. 24, pp. 109–165.\n[16] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 international\nconference on management of data , 2018, pp. 489–504.\n[17] P. Ferragina and G. Vinciguerra, “The pgm-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds,” Proceedings\nof the VLDB Endowment , vol. 13, no. 8, pp. 1162–1175, 2020.\n[18] A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and\nT. Neumann, “Radixspline: a single-pass learned index,” in Proceedings\nof the third international workshop on exploiting artificial intelligence\ntechniques for data management , 2020, pp. 1–5.\n[19] J. Ding, V . Nathan, M. Alizadeh, and T. Kraska, “Tsunami: A learned\nmulti-dimensional index for correlated data and skewed workloads,” arXiv\npreprint arXiv:2006.13282 , 2020.[20] J. Ding, U. F. Minhas, J. Yu, C. Wang, J. Do, Y . Li, H. Zhang,\nB. Chandramouli, J. Gehrke, D. Kossmann et al. , “Alex: an updatable\nadaptive learned index,” in Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data , 2020, pp. 969–984.\n[21] P. Li, H. Lu, Q. Zheng, L. Yang, and G. Pan, “Lisa: A learned index\nstructure for spatial data,” in Proceedings of the 2020 ACM SIGMOD\ninternational conference on management of data , 2020, pp. 2119–2133.\n[22] V . Nathan, J. Ding, M. Alizadeh, and T. Kraska, “Learning multi-\ndimensional indexes,” in Proceedings of the 2020 ACM SIGMOD\ninternational conference on management of data , 2020, pp. 985–1000.\n[23] V . Pandey, A. van Renen, A. Kipf, I. Sabek, J. Ding, and A. Kemper,\n“The case for learned spatial indexes,” arXiv preprint arXiv:2008.10349 ,\n2020.\n[24] J. Qi, G. Liu, C. S. Jensen, and L. Kulik, “Effectively learning spatial\nindices,” Proceedings of the VLDB Endowment , vol. 13, no. 12, pp.\n2341–2354, 2020.\n[25] I. Sabek, K. Vaidya, D. Horn, A. Kipf, M. Mitzenmacher, and T. Kraska,\n“Can learned models replace hash functions?” Proceedings of the VLDB\nEndowment , vol. 16, no. 3, pp. 532–545, 2022.\n[26] I. Sabek and T. Kraska, “The case for learned in-memory joins,” arXiv\npreprint arXiv:2111.08824 , 2021.\n[27] B. Hentschel, U. Sirin, and S. Idreos, “Entropy-learned hashing: 10x\nfaster hashing with controllable uniformity.” SIGMOD, 2022.\n[28] I. Sabek, K. Vaidya, D. Horn, A. Kipf, and T. Kraska, “When are learned\nmodels better than hash functions?” arXiv preprint arXiv:2107.01464 ,\n2021.\n[29] H. Jagadish, J. Madar, and R. T. Ng, “Semantic compression and pattern\nextraction with fascicles,” in VLDB , vol. 99, 1999, pp. 186–97.\n[30] S. Babu, M. Garofalakis, and R. Rastogi, “Spartan: A model-based\nsemantic compression system for massive data tables,” ACM SIGMOD\nRecord , vol. 30, no. 2, pp. 283–294, 2001.\n[31] H. Jagadish, R. T. Ng, B. C. Ooi, and A. K. Tung, “Itcompress:\nAn iterative semantic compression algorithm,” in Proceedings. 20th\nInternational Conference on Data Engineering . IEEE, 2004, pp. 646–\n657.\n[32] A. Ilkhechi, A. Crotty, A. Galakatos, Y . Mao, G. Fan, X. Shi, and\nU. Cetintemel, “Deepsqueeze: deep semantic compression for tabular\ndata,” in Proceedings of the 2020 ACM SIGMOD international conference\non management of data , 2020, pp. 1733–1746.\n[33] F. Kingma, P. Abbeel, and J. Ho, “Bit-swap: Recursive bits-back\ncoding for lossless compression with hierarchical latent variables,” in\nInternational Conference on Machine Learning . PMLR, 2019, pp.\n3408–3417.\n[34] Y . Yang, R. Bamler, and S. Mandt, “Improving inference for neural\nimage compression,” Advances in Neural Information Processing Systems ,\nvol. 33, pp. 573–584, 2020.\n[35] F. Mentzer, L. V . Gool, and M. Tschannen, “Learning better lossless\ncompression using lossy compression,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2020, pp.\n6638–6647.\n[36] L. Huang and T. Hoefler, “Compressing multidimensional weather\nand climate data into neural networks,” in The Eleventh International\nConference on Learning Representations , 2022.\n[37] X. Liang, K. Zhao, S. Di, S. Li, R. Underwood, A. M. Gok, J. Tian,\nJ. Deng, J. C. Calhoun, D. Tao et al. , “Sz3: A modular framework for\ncomposing prediction-based error-bounded lossy compressors,” IEEE\nTransactions on Big Data , vol. 9, no. 2, pp. 485–498, 2022.\n[38] L. Zhou, K. S. Candan, and J. Zou, “Deepmapping: The case for learned\ndata mapping for compression and efficient query processing,” arXiv\npreprint arXiv:2307.05861 , 2023.\n[39] O. R. developers, “Onnx runtime,” https://onnxruntime.ai/, 2021, version:\n1.15.1.\n[40] L. Li and A. Talwalkar, “Random search and reproducibility for neural\narchitecture search,” in Uncertainty in artificial intelligence . PMLR,\n2020, pp. 367–377.\n[41] E. Real, A. Aggarwal, Y . Huang, and Q. V . Le, “Regularized evolution\nfor image classifier architecture search,” in Proceedings of the aaai\nconference on artificial intelligence , vol. 33, no. 01, 2019, pp. 4780–\n4789.\n[42] J. Bergstra, R. Bardenet, Y . Bengio, and B. Kégl, “Algorithms for hyper-\nparameter optimization,” Advances in neural information processing\nsystems , vol. 24, 2011.\n[43] B. Zoph and Q. V . Le, “Neural architecture search with reinforcement\nlearning,” arXiv preprint arXiv:1611.01578 , 2016.\n\n[44] H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean, “Efficient neural\narchitecture search via parameters sharing,” in International conference\non machine learning . PMLR, 2018, pp. 4095–4104.\n[45] H. Liu, K. Simonyan, and Y . Yang, “Darts: Differentiable architecture\nsearch,” arXiv preprint arXiv:1806.09055 , 2018.\n[46] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y . Wei, and J. Sun, “Single\npath one-shot neural architecture search with uniform sampling,” in\nEuropean conference on computer vision . Springer, 2020, pp. 544–560.\n[47] Y . Liu, Y . Sun, B. Xue, M. Zhang, G. G. Yen, and K. C. Tan, “A survey\non evolutionary neural architecture search,” IEEE transactions on neural\nnetworks and learning systems , 2021.\n[48] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal, “The\" wake-sleep\"\nalgorithm for unsupervised neural networks,” Science , vol. 268, no. 5214,\npp. 1158–1161, 1995.\n[49] Y . Chen, S. Alspaugh, and R. Katz, “Interactive analytical processing\nin big data systems: A cross-industry study of mapreduce workloads,”\narXiv preprint arXiv:1208.4174 , 2012.\n[50] I. F. Adams, M. W. Storer, and E. L. Miller, “Analysis of workload\nbehavior in scientific and historical long-term data repositories,” ACM\nTransactions on Storage (TOS) , vol. 8, no. 2, pp. 1–27, 2012.\n[51] “Tpc-h benchmark,” https://www.tpc.org/tpch/, Last accessed on Sept\n1st, 2023.\n[52] “Tpc-ds benchmark,” https://www.tpc.org/tpcds/, Last accessed on Sept\n1st, 2023.\n[53] M. Poess, B. Smith, L. Kollar, and P. Larson, “Tpc-ds, taking decision\nsupport benchmarking to the next level,” in Proceedings of the 2002\nACM SIGMOD international conference on Management of data , 2002,\npp. 582–587.\n[54] USDA National Agricultural Statistics Service, “Published crop-specific\ndata layer,” Web App, Last accessed on Feb 28th, 2024. [Online].\nAvailable: https://nassgeodata.gmu.edu/CropScape/\n[55] “Byte-dictionary encoding,” https://docs.aws.amazon.com/redshift/latest/\ndg/c_Byte_dictionary_encoding.html, Last accessed on Sept 1st, 2023.\n[56] GNU Project, “gzip,” Software, Last accessed on Feb 28th, 2024.\n[Online]. Available: https://www.gnu.org/software/gzip/\n[57] Y . Collet and M. Kucherawy, “Zstandard Compression and the\n’application/zstd’ Media Type,” RFC 8878, Feb. 2021. [Online].\nAvailable: https://www.rfc-editor.org/info/rfc8878\n[58] Igor Pavlov, “Lzma sdk,” Software, Last accessed on Feb 28th, 2024.\n[Online]. Available: https://www.7-zip.org/sdk.html\n[59] “Source code of python dictionary implementation,” https://github.com/\npython/cpython/blob/main/Objects/dictobject.c, Last accessed on Nov\n17th, 2023.\n[60] “Numpy introduction,” https://numpy.org/devdocs/user/whatisnumpy.\nhtml#why-is-numpy-fast, Last accessed on Nov 17th, 2023.\n[61] “Python bindings for gzip compression,” https://docs.python.org/3.8/\nlibrary/gzip.html, Last accessed on Feb 26th, 2024.\n[62] “Python bindings for lzma compression,” https://docs.python.org/3.8/\nlibrary/lzma.html, Last accessed on Feb 26th, 2024.\n[63] “Python bindings for the lzo data compression library,” https://pypi.org/\nproject/python-lzo/, Last accessed on Sept 1st, 2023.\n[64] “Zstd bindings for python,” https://pypi.org/project/zstd/, Last accessed\non Sept 1st, 2023.\n[65] M. Pilgrim, “Serializing python objects,” in Dive Into Python 3 . Springer,\n2009, pp. 205–223.\n[66] L. Zhou, A. Jain, Z. Wang, A. Das, Y . Yang, and J. Zou, “Benchmark\nof dnn model search at deployment time,” in Proceedings of the\n34th International Conference on Scientific and Statistical Database\nManagement , 2022, pp. 1–12.",
  "textLength": 80388
}