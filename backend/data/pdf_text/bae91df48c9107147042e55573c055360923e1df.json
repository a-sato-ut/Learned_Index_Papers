{
  "paperId": "bae91df48c9107147042e55573c055360923e1df",
  "title": "A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge",
  "pdfPath": "bae91df48c9107147042e55573c055360923e1df.pdf",
  "text": "arXiv:2310.11703v2  [cs.DB]  16 Jun 2025JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nA Comprehensive Survey on Vector Database:\nStorage and Retrieval Technique, Challenge\nLe Ma∗, Ran Zhang∗, Yikun Han∗, Shirui Yu, Zaitian Wang, Zhiyuan Ning, Jinghan Zhang, Ping Xu,\nPengjiang Li, Wei Ju, Chong Chen, Dongjie Wang, Kunpeng Liu, Pengyang Wang, Pengfei Wang,\nYanjie Fu, Chunjiang Liu†, Yuanchun Zhou, Chang-Tien Lu\nAbstract —Vector databases (VDBs) have emerged to manage\nhigh-dimensional data that exceed the capabilities of traditional\ndatabase management systems, and are now tightly integrated\nwith large language models as well as widely applied in modern\nartificial intelligence systems. Although relatively few studies\ndescribe existing or introduce new vector database architectures,\nthe core technologies underlying VDBs, such as approximate\nnearest neighbor search, have been extensively studied and are\nwell documented in the literature. In this work, we present a\ncomprehensive review of the relevant algorithms to provide a\ngeneral understanding of this booming research area. Specifically,\nwe first provide a review of storage and retrieval techniques in\nVDBs, with detailed design principles and technological evolution.\nThen, we conduct an in-depth comparison of several advanced\nVDB solutions with their strengths, limitations, and typical appli-\ncation scenarios. Finally, we also outline emerging opportunities\nfor coupling VDBs with large language models, including open\nresearch problems and trends, such as novel indexing strategies.\nThis survey aims to serve as a practical resource, enabling\nreaders to quickly gain an overall understanding of the current\nknowledge landscape in this rapidly developing area.\nIndex Terms —Vector Database, Retrieval, Storage, Large Lan-\nguage Models.\nI. I NTRODUCTION\nVectors, particularly those in high-dimensional spaces, are\nmathematical representations of data, encoding the semantic\nand contextual information of entities such as text, images,\naudio, and video [1], [2]. These vectors are generally gen-\nerated through some related machine learning models, and\nthe generated vectors are usually high-dimensional and can\nbe used for similarity comparison. The step of converting\noriginal unstructured data into vectors is the foundation of\nmany artificial intelligence (AI) applications (including large\nlanguage models (LLMs) [3], question-answering systems\n[4], [5], image recognition [6], recommendation systems [7],\n[8], etc.). However, in terms of managing and retrieving\nhigh-dimensional vector data, traditional databases designed\nfor handling structured data are often inadequate. Vector\ndatabases, on the other hand, provide a specialized solution\nto these challenges.\nVector Databases (VDBs) are tools specifically designed to\nefficiently store and manage high-dimensional vectors. Specif-\nically, VDBs store information as high-dimensional vectors,\nwhich are mathematical representations of data features or\n∗Equal contribution.†Corresponding author.\nThis paper was produced by the IEEE Publication Technology Group. They\nare in Piscataway, NJ.\nManuscript received April 19, 2021; revised August 16, 2021.attributes [9]. Depending on the complexity and granularity of\nthe underlying data, the dimensions of these high-dimensional\nvectors usually range from dozens to thousands. Unlike tra-\nditional relational databases, VDBs provide efficient mecha-\nnisms for large-scale storage, management, and search of high-\ndimensional vectors [10]–[12]. These mechanisms bring vari-\nous efficient functions to VDBs, such as supporting semantic\nsimilarity search, efficiently managing large-scale data, and\nproviding low-latency responses. These functions make VDBs\nincreasingly integrated into AI-based applications.\nVDBs have two core functions: vector storage and vector re-\ntrieval. The vector storage function relies on techniques such as\nquantization, compression, and distributed storage mechanisms\nto improve efficiency and scalability. The retrieval function\nof VDBs relies on specialized indexing techniques, including\ntree-based methods, hashing methods [13], graph-based mod-\nels, and quantization-based techniques [14]. These indexing\ntechniques optimize high-dimensional similarity search by\nreducing computational cost and improving search perfor-\nmance. In addition, hardware acceleration and cloud-based\ntechnologies have further enhanced the capabilities of VDBs,\nmaking them suitable for large-scale and real-time applications\n[15]–[17].\nTherefore, compared with traditional databases, vector\ndatabases (VDBs) have three significant advantages: (1) Vector\ndatabases possess efficient and accurate vector retrieval\ncapabilities. The core function of VDBs is to retrieve relevant\nvectors through vector similarity (distance), and this function\nis also the core of applications such as natural language pro-\ncessing (NLP), computer vision, and recommendation systems\n[18], [19]. In contrast, traditional databases can only query\ndata based on exact matching or predefined conditions, and\nthis kind of query method is relatively slow and often does\nnot consider the semantic information of the data itself. (2)\nVector databases support the storage and query of complex\nand unstructured data. VDBs can store and search data with\nhigh complexity and granularity, such as text, images, audio,\nvideo, etc. However, traditional databases, such as relational\ndatabases, are difficult to store this kind of data informa-\ntion well [20]. (3) Vector databases have high scalability\nand real-time processing capabilities. Traditional databases\nface challenges in effectively handling unstructured data and\nlarge volumes of real-time datasets [21]. However, VDBs can\nprocess vector data at large scale and in real time, which\nis crucial for modern data science and artificial intelligence\napplications [22]. By using technologies such as sharding [23],\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\npartitioning, caching, and replication, VDBs can distribute\nworkloads and optimize resource utilization across multiple\nmachines or clusters. Traditional databases, on the other hand,\nmay face scalability bottlenecks, latency issues, or concurrency\nconflicts when handling big data [19].\nRecent surveys on VDBs primarily cover fundamental con-\ncepts and practical applications of VDBs and vector database\nmanagement systems. Some studies [12], [19], [24] focus on\nthe workflow and technical challenges of VDBs, including key\naspects such as query processing, optimization, and execution\ntechniques. And some works [25] explore the critical role\nof VDBs in modern generative AI applications and provide\nan outlook on the future of VDBs. While these studies have\ntheir respective focuses, they do not provide a comprehensive\nsurvey of the overall storage and search technologies in VDBs,\nnor deliver a thorough analysis comparing the capabilities\nof existing VDBs. Furthermore, there is limited exploration\nof how these systems can integrate with rapidly advancing\nAI technologies, such as large language models (LLMs), to\nsupport modern data-intensive applications.\nThis gap shows the need for a comprehensive survey to\nconsolidate current knowledge and uncover key research chal-\nlenges related to VDB. To address this, our survey makes the\nfollowing core contributions:\n•We systematically review storage and retrieval techniques\nin VDBs, outlining their design principles and technolog-ical evolution.\n•We provide a detailed comparative analysis of existing\nVDB solutions, highlighting their strengths, limitations,\nand typical application scenarios.\n•We synthesize the main challenges, recent advancements,\nand future directions for VDBs, including open research\nproblems and trends such as novel indexing strategies,\nadaptive query optimization, and integration with ad-\nvanced AI capabilities.\nThis paper comprehensively summarizes the technologies re-\nlated to vector databases, and systematically tests the per-\nformance of existing open-source vector databases. It also\nprovides an outlook on the challenges that vector databases\nwill face in the future. Through the summary of this paper,\nresearchers can deepen their understanding of the field of\nvector databases. Figure 1 shows the overall framework of the\npaper, and we also construct a classification system of storage\nand search technologies for VDBs, as shown in Figure 2.\nII. S TORAGE\nEfficient data management strategies are essential for the\nperformance and scalability of VDBs. This section explores\nfour key techniques: sharding and partitioning for data distri-\nbution, caching for reducing query latency, and replication for\nensuring availability and fault-tolerance.\nSharding\nPartitioning\nCaching\nFIFO\n MRUCache Eviction Algorithms\nLFUCaching Method\nPartitioned  CacheMultiple \nMachines\nRange-\nbasedSharding\nSingle \nMachines\nNearest Neighbor Search\nApproximate Nearest Neighbor Search\nBrute Force \nApproach\nKeyApproaches\nQuantization-\nBased\nProduct \nQuantization\nGraph- Based\nHierarchical \nNavigable \nSmall Worlds \nHash- Based\nTree-Based \nApproach\nLocal -Sensitive \nHashing\nLoading capacity \nSearch performanceFeatures VS\nIndex \nConstruction and \nSearching High \nDimensional \nVectors\nReplication\nLeader -Follower Multi- Leader Leaderless\nSupport for \nHeterogeneou\ns Vector Data \nTypes\nDistributed \nParallel \nProcessing \nSupport\nIntegration with \nMainstream \nMachine \nLearning \nFrameworks\nIntegration \nwith emerging \napplication \nscenarios\nData security \nand privacy \nprotection\nGeographic Sharding\nList-\nBased\nK-Mean s\nPa\nrtitioning\nHash -\nbased\nRAG Memory\nVector Databases for LLMs\n LLMs for Vector Databases\nDBManagement Tasks\nVector Data Handling\nA General LLMs and Vector Databases Synergized Framework\nFig. 1. Framework overview of this survey structure covering Storage Techniques, Search Techniques, Database Comparison, Challenges, and the Synergy of\nLarge Language Models (LLMs) with VDBs. Each section represents a fundamental facet of the operation and integration of modern VDBs within advanced\nAI technologies.\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nA. Sharding\nSharding is a technique that distributes a VDB across\nmultiple machines or clusters, called shards, based on specific\ncriteria, such as a hash function or a key range. It is a foun-\ndational strategy for achieving scalability, load balancing, and\nfault tolerance in distributed systems by dividing the dataset\ninto smaller, manageable units. Different sharding methods are\ndesigned to address specific challenges, such as balancing data\ndistribution, minimizing hotspots, reducing query latency, and\nsupporting dynamic scaling. This section covers three widely\nadopted strategies: range-based sharding, hash-based sharding,\nand geographic sharding.\nRange-Based Sharding. The most straightforward sharding\nmethod is range-based sharding, which partitions vector data\nacross shards by dividing it into non-overlapping key intervals,\nor ranges, based on sorted keys [26]. For example, users can\nshard a VDB by dividing the ID column of the vector data into\ndifferent ranges, such as 0-999, 1000-1999, 2000-2999, and so\non, with each range assigned to a specific shard. Each range\ncorresponds to a shard. This way, users can query the vector\ndata more efficiently by specifying the shard name or range.\nRange-based sharding is simple to implement and provides\nefficient query performance for range-based queries, such as\nretrieving all vectors within a specific time period or ID range.\nHowever, this method often leads to data skew and uneven load\ndistribution if key values are not uniformly distributed. Range-\nbased sharding is particularly suitable for use cases where data\naccess patterns are predictable, such as time-series analytics\nor sequential ID-based queries.\nHash-Based Sharding . Another common sharding method\nin VDBs is hash-based sharding [26]–[28], which assigns\nvector data into different shards based on the hash value\nof a key column or a set of columns. For example, users\ncan shard a VDB by applying a hash function to the ID\ncolumn of the vector data. For example, users can distribute\nthe vector data evenly across the shards and avoid hotspots.\nHowever, traditional hash-based methods, such as modulo\nhashing, introduce challenges during cluster scaling, suffering\nfrom significant data redistribution when adding or removing\nshards. To address this, consistent hashing is often employed\nin distributed VDBs. Unlike traditional hashing, consistent\nhashing maps both data and nodes to a ring-like hash space.\nThis ensures that only a small fraction of data (O(1/N)) is\nremapped when nodes are added or removed. Consistent hash-\ning minimizes reorganization overhead during cluster scaling\nwhile maintaining balanced data distribution. Additionally,\nvirtual nodes (replicas of physical nodes on the hash ring)\nfurther enhance load balancing, mitigating hotspots. Although\nconsistent hashing introduces slight computational complexity,\nits advantages in dynamic environments make it a preferred\nchoice for modern distributed systems [29]–[31]\nGeographic Sharding. Another approach is geographic\nsharding (also known as geo-sharding), which distributes vec-\ntor data across shards based on geographic attributes [32],\n[33], such as user region or location. For instance, a VDB can\nbe partitioned by a “region” field, assigning each shard to a\nspecific geographic zone (e.g., “North America,” “Europe,” or“Asia”). This method is particularly useful for latency-sensitive\nlocalized queries, such as recommendation systems or geospa-\ntial analytics. By storing data in shards physically closer to\nusers, geo-sharding reduces cross-region network latency and\nhelps comply with data sovereignty regulations. Additionally,\ncombined with dynamic load balancing, it can adapt to shifting\ngeographic access patterns for optimal performance.\nB. Partitioning\nPartitioning typically refers to dividing data within a single\ndatabase instance into multiple logical subsets based on rules\nsuch as range, list, and K-Means, where all partitioned data\nremains within the same physical system. This approach im-\nproves query efficiency, facilitates parallel processing, and op-\ntimizes resource usage by organizing data into logical groups.\nFor instance, users can partition a VDB by color values such\nas red, yellow, and blue, where each partition contains vector\ndata associated with a specific “color” column value. Queries\ncan then target specific partitions, avoiding unnecessary scans\nof unrelated data and improving performance. While sharding\nfocuses on distributing data across multiple machines in a\ndistributed environment, partitioning primarily organizes data\nwithin a single machine or node. These two techniques serve\ndifferent purposes but can be used together synergistically. In\npractical applications, sharding achieves horizontal scalability\nby distributing data across nodes, while partitioning optimizes\nlocal data organization within each node. By combining the\ntwo, systems can balance global scalability and localized\nquery performance—sharding handles load distribution for\nlarge datasets, and partitioning ensures efficient data access\nwithin individual shards. Partitioning strategies vary depending\non the characteristics of the data and application requirements.\nThis section discusses four common partitioning methods:\nrange partitioning, list partitioning, k-means partitioning, and\nhash-based Partitioning.\nRange-based Partitioning. Range-based partitioning is a\nmethod widely used in VDBs, where data is divided into non-\noverlapping key ranges to form partitions [34], [35]. Each\nrange corresponds to a specific subset of data based on a\nsorted key (e.g., timestamps, numeric IDs). Similar to the\nstrength of range-based sharding, range-based partitioning is\nparticularly efficient for range-based queries, as it allows the\nsystem to target specific partitions directly. For example, users\ncan partition a VDB by date ranges, such as monthly or\nquarterly. This way, users can query the vector data more\nefficiently by specifying the partition name or range.\nList-based Partitioning. Another way that partitioning\nworks in VDB is by using a list partitioning method, which\nassigns vector data to different partitions based on their value\nlists of a key column or a set of columns [36], [37]. For\nexample, users can partition a vector database by color values,\nsuch as red, yellow, and blue. Each partition contains vector\ndata that have a given value of the color column. This way,\nusers can query the vector data more easily by specifying the\npartition name or list.\nK-Means Partitioning. A third partitioning method for\nVDBs is k-means partitioning [20], which divides vector data\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\n[98][115][16] [ 24] [31]\n[57] [58] [80][16]\n[87] [50][51][27][28]\nPartitioning\nRange \nPartitioning\nList \nPartitioning\nK-Means\nPartitioning\nRange -\nBased \nSharding\nHash -\nBased \nSharding\nGeographic \nSharding \nSharding\n[1] [2] [45][79][41][42] [ 79]\nCaching\nFirst -In  \nFirst -Out\nLeast \nRecently \nUsed\nHash -based \nPartitioning[14] [26][37] [38][84]\nReplication\nLeader-\nFollower \nReplication \nMulti-\nLeader \nReplication\nLeaderless \nReplication\nStorage\n[62][89]\nLeast \nFrequently \nUsed\n[81] \nPartitioned \nCache\n[24]\nNearest Neighbor Search\n[8] [9] [10] [11] [19]\n[25] [53]\nLocal -\nSensitive \nHashing \n[4] [69] [74]\n[75]\nDeep \nHashing[49]\nSpherical \nHashing\nApproximate Nearest Neighbor Search[43]\nMost \nRecently \nUsed\nHash -Based \nApproach\n[109]\nSpectral \nHashing\nTree -Based \nApproach\nKD-Tree [17] [40]\n Ball-Tree [30] [71] [83]\n R-Tree [47]\n M-Tree [23]\n[32] \nApproxim\n-ate\nNearest \nNeighbor-\ns Oh Yeah\n[86]\nK-means \nTree \nTree -Based\nApproach\n[68] [94]\nBest Bin \nFirst [90] [113] [114]\nNavigable \nSmall \nWorld\nGraph-Based \nApproach\n[76]\nHierachic-\nal \nNavigable \nSmall \nWorld[72]\nInverted \nFile Index\n[3] [112]\nOnline \nProduct \nQuantizati-\non[39] [66]\nOptimized \nProduct \nQuantizati-\non\nQuantization-Based Approach\n[55] [78]\nProduct \nQuantizati-\non[46] [97]\nScalable \nNearest \nNeighbor\n[45] [103]\nInverted \nFile \nProduct \nquantizati\n-on\nFig. 2. Taxonomy of Vector Database (VDB) Storage and Search Technologies.\ninto a predetermined number (k) of clusters. Each cluster\nrepresents a partition, with vectors within the same cluster\nbeing similar to each other and vectors between clusters\ndiffering significantly. Similar vectors are placed in the same\npartition, which improves query efficiency. However, for large-\nscale datasets, the computational cost of k-means clustering\ncan be high, especially when frequent updates necessitate re-\nclustering.\nHash-based Partitioning. Alternatively, some VDBs adopt\nhash-based partitioning, such as consistent or uniform hash-\ning [38]–[40]. Specifically, the hashing partitioning strategy\nuses a hash function to map data to different partitions. The\nhash value of each data point determines which partition itbelongs to. In VDBs, the hash value is typically calculated\nbased on certain features of the vector (for example, values of\nspecific dimensions or the entire vector). The hash function\ncan evenly distribute data across partitions, preventing any\nsingle partition from storing too much data. However, when\nthe data distribution is uneven, it may lead to some partitions\nbecoming overloaded. Additionally, when the node number\nchanges, hash-based partitioning may require the redistribution\nof the large dataset, which can incur significant overhead.\nC. Caching\nCaching is a technique that stores frequently accessed or\nrecently used data in a fast and accessible memory, such as\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nRAM, to reduce latency and improve data retrieval perfor-\nmance. Caching can be used in VDBs to speed up similarity\nsearch and vector retrieval. In traditional database systems,\nutilizing in-memory key-value stores (e.g., Redis) as a caching\nlayer represents a widely adopted approach. This methodology\ngenerates cache keys by combining query parameters (such\nas specific column values for matching). When a target key\nis found in the cache, Redis serves the corresponding data;\notherwise, a cache miss occurs, necessitating database queries\nto fetch the required information. However, this key-value\nparadigm proves unsuitable for VDBs, as they store high-\ndimensional vector data rather than structured data, making it\nvirtually impossible to reuse identical vectors across queries.\nTo address these challenges, it is important to explore general\ncaching algorithms and their applicability to VDBs. This\nsection discusses four common caching methods: first-in first-\nout (FIFO), least recently used (LRU), most recently used\n(MRU), and least frequently used (LFU).\nFirst-In First-Out (FIFO) . FIFO algorithm is a funda-\nmental cache eviction strategy that operates on the principle\nof “first in, first out”: when the cache space is exhausted, it\nprioritizes removing the earliest stored vector data [41], [42].\nThis algorithm maintains a simple queue structure: new data\nis always appended to the tail, while eviction removes the\noldest data from the head [43]. FIFO is particularly suitable\nfor vector data scenarios with stable access patterns and no\ndistinct hotspots, such as time-series vector data collected at\nfixed intervals. While its implementation is simple and effi-\ncient (with O(1) time complexity), it may inadvertently evict\nfrequently used data due to disregarding access frequency. A\ntypical application includes real-time processing systems for\nindustrial sensor data, where the timeliness of historical vector\ndata often outweighs its reuse value.\nLeast Recently Used (LRU) . One commonly employed\ncaching strategy in VDBs is the least recently used (LRU)\npolicy, which evicts the least recently accessed vector data\nwhen the cache reaches its capacity. This approach ensures\nthat the cache retains the most relevant or frequently queried\nvectors, thereby improving the likelihood of cache hits [43].\nFor example, Redis, a widely used in-memory database, im-\nplements LRU caching to manage vector data and facilitate\nefficient vector similarity search. However, LRU may not\neffectively retain data with long-term or periodic popularity,\nas it only considers recent access history. LRU is particularly\nsuitable for scenarios with strong temporal locality, such as\nrecommendation systems and real-time search applications,\nwhere recently accessed vectors are highly likely to be queried\nagain.\nMost Recently Used (MRU) . The Most Recently Used\n(MRU) [44] algorithm is another caching strategy employed\nin VDBs that prioritizes evicting the most recently accessed\nvector data when cache capacity is reached. This approach\noperates under the assumption that recently queried vectors\nare less likely to be accessed again in the short term, thereby\nretaining less recently used data that may be needed in the\nfuture. MRU has been adopted in certain storage systems and\napplications with transient or one-time access patterns, such\nas data streaming or processing workloads, where immediatereuse of recently accessed vectors is unlikely. However, MRU\nis suboptimal for workloads with strong temporal locality, as\nit may prematurely evict data that will soon be accessed again.\nLeast Frequently Used (LFU) . The LFU algorithm is a\nfrequency-based cache eviction mechanism that determines\nremoval priority by continuously tracking the access count\nof each vector data item. The LFU algorithm maintains a\nfrequency table and evicts the least frequently accessed items\nwhen the cache is full. A typical implementation requires\nmaintaining an access counter for each cached item, often\nusing a min-heap data structure to efficiently identify the\nlowest-frequency items with a time complexity of O(log n)\n[45], [46]. However, the LFU algorithm can be susceptible to\ncache pollution, as items frequently accessed in the past may\nlinger in the cache even when they are no longer needed. Thus,\nLFU is suitable for applications with stable and long-lived\nspot data patterns (e.g., popular product recommendations,\nhigh-frequency user profile queries), while its effectiveness\ndiminishes in environments with rapidly evolving access dis-\ntributions.\nPartitioned Cache . Partitioned caching is a common ap-\nproach in VDBs, wherein vector data are divided into multiple\npartitions based on specific criteria [47], such as geographic\nlocation, category, or access frequency. Each partition can\nbe allocated a distinct cache size and may employ different\neviction policies according to the respective demand and usage\npatterns. This enables the cache to retain the most relevant\nvector data for each partition, thereby improving resource\nutilization and overall cache effectiveness. For example, Esri,\na leading geographic information system (GIS) company,\nleverages the partitioned cache to efficiently store vector data\nand support high-performance map rendering.\nD. Replication\nReplication is a technique that creates multiple copies of\nthe vector data and stores them on different nodes or clus-\nters. Replication can improve the availability, durability, and\nperformance of VDB. This section discusses three common\nreplication methods: leader-follower replication, multi-leader\nreplication, and leaderless replication.\nLeader-Follower Replication . Leader-Follower replication\ndesignates one node as the leader and the others as the\nfollowers, and allows only the leader to accept write requests\nand propagate them to the followers [48]. Leader-follower\nreplication can ensure strong consistency and simplify the\nconflict resolution of VDB. However, it may also introduce\navailability issues and require failover mechanisms to handle\nleader failures.\nMulti-Leader Replication . Multi-Leader replication ex-\ntends the traditional leader-follower model by designating\nmultiple nodes as leaders, each capable of independently\naccepting and processing write requests [49], [50]. In this\narchitecture, all leader nodes can concurrently handle write\noperations and asynchronously propagate changes to other\nnodes in the system.\nLeaderless Replication . Leaderless replication does not\ndistinguish between leader and follower nodes, and allows any\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nnode to accept write and read requests [51], [52]. Leaderless\nreplication can avoid single points of failure and improve the\nscalability and reliability of VDB. However, it may also intro-\nduce consistency issues and require coordination mechanisms\nto resolve conflicts.\nIII. S EARCH\nVDBs are designed to facilitate efficient similarity search\nover high-dimensional vector data, an essential operation in\nmany AI and machine learning applications. This similarity\nsearch is typically implemented through nearest neighbor\nsearch algorithms, which can be further divided into exact\nnearest neighbor search (NNS) and approximate nearest neigh-\nbor search (ANNS) methods.\nNNS is the optimization problem of finding the point in a\ngiven set that is closest (or most similar) to a given point.\nCloseness is typically expressed in terms of a dissimilarity\nfunction: the less similar the objects, the larger the function\nvalues. For example, users can use NNS to find images that\nare similar to a given image based on their visual content\nand style, or documents that are similar to a given document\nbased on their topic and sentiment. ANNS is a variation\nof NNS that allows for some error or approximation in the\nsearch results. ANNS can trade off accuracy for speed and\nspace efficiency, which can be useful for large-scale and high-\ndimensional data. For example, users can use ANNS to find\nproducts that are similar to a given product based on their\nfeatures and ratings, or users that are similar to a given user\nbased on their preferences and behaviors.\nObserving the current division for NNS and ANNS algo-\nrithms, the boundary is precisely their design principle, such\nas how they organize, index, or hash the dataset, how they\nsearch or traverse the data structure, and how they measure or\nestimate the distance between points. NNS algorithms tend to\nuse more exact or deterministic methods, such as partitioning\nthe space into regions by splitting along one dimension (k-d\ntree) or enclosing groups of points in hyperspheres (ball tree),\nand visiting only the regions that may contain the nearest\nneighbor based on some distance bounds or criteria. ANNS\nalgorithms tend to use more probabilistic or heuristic methods,\nsuch as mapping similar points to the same or nearby buckets\nwith high probability (locality-sensitive hashing), visiting the\nregions in order of their distance to the query point and\nstopping after a fixed number of regions or points (best\nbin first), or following the edges that lead to closer points\nin a graph with different levels of coarseness (hierarchical\nnavigable small world).\nIn fact, a data structure or algorithm that supports NNS\ncan also be applied to ANNS, and for ease of categorization,\nsuch methods are included under the section on NNS. And\nin recent years, several new algorithms for high-dimensional\nvector NNS have emerged [13], [53]–[56]. Although these\nalgorithms have not yet been widely adopted by VDBs, they\nhold significant potential for future applications.\nA. Nearest Neighbor Search\n1) Brute Force Approach: A brute force algorithm for the\nNNS problem scans all points in the dataset, computing theirdistances to the query point and tracking the closest one. This\nalgorithm guarantees to find the true nearest neighbor for any\nquery point, but it has a high computational cost. The time\ncomplexity of a brute force algorithm for NNS problem is\nO(n), where n is the size of the dataset. The space complexity\nisO(1), since no extra space is needed.\n2) Tree-Based Approach: Four tree-based methods will be\npresented here, namely k-dimensional tree (KD-Tree), Ball-\nTree, R-Tree, and M-Tree.\nKD-Tree [57] . It is a technique for organizing points\nin a k-dimensional space, where k is usually a very big\nnumber. It works by building a binary tree in which every\nnode is a k-dimensional point. Every non-leaf node in the\ntree acts as a splitting hyperplane that divides the space into\ntwo parts, known as half-spaces. The splitting hyperplane is\nperpendicular to the chosen axis, which is associated with one\nof the k dimensions. The splitting value is usually the median\nor the mean of the points along that dimension.\nThe algorithm maintains a priority queue of nodes to visit,\nsorted by their distance to the query point. At each step, the\nalgorithm pops the node with the smallest distance from the\nqueue, and checks if it is a leaf node or an internal node. If it is\na leaf node, the algorithm compares the distance between the\nquery point and the data point stored in the node, and updates\nthe current best distance and nearest neighbor if necessary.\nIf it is an internal node, the algorithm pushes its left and\nright children to the queue, with their distances computed as\nfollows:\ndL(q, N) =(\n0 ifqN.axis≤N.value\n(qN.axis−N.value )2ifqN.axis> N. value\n(1)\ndR(q, N) =(\n0 ifqN.axis≥N.value\n(N.value−qN.axis)2ifqN.axis< N. value\n(2)\nwhere qis the query point, Nis the internal node, N.axis\nis the splitting axis of N, and N.value is the splitting value\nofN. The algorithm repeats this process until the queue is\nempty or a termination condition is met.\nThe advantage of KD-tree is that it is conceptually simpler\nand often easier to implement than some of the other tree\nstructures. The performance of KD-tree depends on several\nfactors, such as the dimensionality of the space, the number of\npoints, and the distribution of the points. These factors affect\nthe trade-off between accuracy and efficiency, as well as the\ncomplexity and scalability of the algorithm. There are also\nsome challenges and extensions of KD-tree, such as dealing\nwith the curse of dimensionality when the dimensionality\nis high, introducing randomness in the splitting process to\nimprove robustness, or using multiple trees to increase recall.\nThis is a variation of KD-tree named randomized KD-tree,\nthat introduces some randomness in the splitting process,\nwhich can improve the performance of KD-tree by reducing\nits sensitivity to noise and outliers [58].\nBall-Tree [59], [60], [61]. It is a technique for finding the\nnearest neighbors of a given vector in a large collection of\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nvectors. It works by building a ball-tree, which is a binary\ntree that partitions the data points into balls, i.e. hyperspheres\nthat contain a subset of the points. Each node of the ball-\ntree defines the smallest ball that contains all the points in its\nsubtree. The algorithm then searches for the closest ball to the\nquery point, and then searches within the closest ball to find\nthe closest point to the query point.\nTo query for the nearest neighbor of a given point, the\nball tree algorithm uses a priority queue to store the nodes\nto be visited, sorted by their distance to the query point. The\nalgorithm starts from the root node and pushes its two children\nto the queue. Then, it pops the node with the smallest distance\nfrom the queue and checks if it is a leaf node or an internal\nnode. If it is a leaf node, it computes the distance between the\nquery point and each data point in the node, and updates the\ncurrent best distance and nearest neighbor if necessary. If it is\nan internal node, it pushes its two children to the queue, with\ntheir distances computed as follows:\ndL(q, N) = max(0 , N.value − ∥q−N.center ∥)\ndR(q, N) = max(0 ,∥q−N.center ∥ −N.value )(3)\nwhere qis the query point, Nis the internal node, N.center\nis the center of the ball associated with N, and N.value is the\nradius of the ball associated with N. The algorithm repeats this\nprocess until the queue is empty or a termination condition is\nmet.\nThe advantage of ball-tree is that it can perform well\nin high-dimensional spaces, as it can avoid the curse of\ndimensionality that affects other methods such as KD-tree. The\nperformance of ball-tree depends on several factors, such as\nthe dimensionality of the data, the number of balls per node,\nand the distance approximation method used. These factors\naffect the trade-off between accuracy and efficiency, as well as\nthe complexity and scalability of the algorithm. There are also\nsome challenges and extensions of ball-tree search, such as\ndealing with noisy and outlier data, choosing a good splitting\ndimension and value for each node, or using multiple trees to\nincrease recall.\nR-Tree [62]. It is a technique for finding the nearest\nneighbors of a given vector in a large collection of vectors. It\nworks by building an R-tree, which is a tree data structure that\npartitions the data points into rectangles, i.e. hyperrectangles\nthat contain a subset of the points. Each node of the R-\ntree defines the smallest rectangle that contains all the points\nin its subtree. The algorithm then searches for the closest\nrectangle to the query point, and then searches within the\nclosest rectangle to find the closest point to the query point.\nThe R-tree algorithm uses the concept of minimum bound-\ning rectangle (MBR) to represent the spatial objects in the\ntree. The MBR of a set of points is the smallest rectangle that\ncontains all the points. The formula for computing the MBR\nof a set of points Pis:\nMBR (P) =\u0014\nmin\np∈Ppx,max\np∈Ppx\u0015\n×\u0014\nmin\np∈Ppy,max\np∈Ppy\u0015\n(4)\nwhere pxandpyare the xandycoordinates of point p, and\n×denotes the Cartesian product.The R-tree algorithm also uses two metrics to measure the\nquality of a node split: area and overlap. The area of a node is\nthe area of its MBR, and the overlap of two nodes is the area\nof the intersection of their MBRs. The formula for computing\nthe area of a node N is:\narea( N) = (N.x max−N.x min)×(N·ymax−N·ymin)\n(5)\nwhere N.x min,N.x max,N.ymin, and N.ymaxare the coordi-\nnates of the MBR of node N.\nThe advantage of R-tree is that it can support spatial\nqueries, such as range queries or nearest neighbor queries, on\ndata points that represent geographical coordinates, rectangles,\npolygons, or other spatial objects. R-tree search performance\ndepends on roughly the same factors as B-tree, and also faces\nsimilar challenges as B-tree.\nM-Tree [63]. It is a technique for finding the nearest\nneighbors of a given vector in a large collection of vectors.\nIt works by building an M-tree, which is a tree data structure\nthat partitions the data points into balls, i.e. hyperspheres that\ncontain a subset of the points. Each node of the M-tree defines\nthe smallest ball that contains all the points in its subtree. The\nalgorithm then searches for the closest ball to the query point,\nand then searches within the closest ball to find the closest\npoint to the query point.\nThe M-tree algorithm uses the concept of covering radius to\nrepresent the spatial objects in the tree. The covering radius of\na node is the maximum distance from the node’s routing object\nto any of its children objects. The formula for computing the\ncovering radius of a node Nis:\nr(N) = max\nC∈N.childd(N.object, C.object ) (6)\nwhere N.object is the routing object of node N,N.child is\nthe set of child nodes of node N,C.object is the routing\nobject of child node C, and dis the distance function.\nThe M-tree algorithm also uses two metrics to measure the\nquality of a node split: area and overlap. The area of a node\nis the sum of the areas of its children’s covering balls, and the\noverlap of two nodes is the sum of the areas of their children’s\noverlapping balls. The formula for computing the area of a\nnode Nis:\narea( N) =X\nC∈N.childπr(C)2(7)\nwhere πis the mathematical constant, and r(C)is the covering\nradius of child node C.\nThe advantage of M-tree is that it can support dynamic\noperations, such as inserting or deleting data points, by updat-\ning the tree structure accordingly. M-tree search performance\ndepends on roughly the same factors as B-tree, and also faces\nsimilar challenges as B-tree.\nB. Approximate Nearest Neighbor Search\n1) Hash-Based Approach: The core idea of the hash-\nbased approach is to reduce search complexity by mapping\nhigh-dimensional data to lower-dimensional hash codes with\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nVector SpaceHash \nCode 1\nHash \nCode 2\nHash \nCode NHash MethodQuery Vector\nSharding\ntechnique\nPartitioning\nCaching\nFIFO\n MRUCache Eviction Algorithms\nLFU\n Partitioned  CacheMultiple Machines\nRange -based\nPrimary approachesSharding\nSingle Machines\nHash -based\nReplication\nLeader -Follower Multi- Leader\nHash Code N\nHash Code 2\nHash Code 1\nQuery \nVector\nVectorSpaceHashMethod\nFig. 3. The process of approximate nearest neighbor search based on hash\napproach\ncarefully designed hash functions, while preserving similarity\nbetween data points. As shown in Figure 3, each high-\ndimensional vector is transformed into a low-dimensional hash\ncode. Similar points are mapped to the same or neighboring\ncodes, so the search only needs to examine a small subset of\ncodes, greatly improving efficiency. Based on this idea, four\nrepresentative methods will be introduced: locality-sensitive\nhashing, spectral hashing, Spherical Hashing, and deep hash-\ning. The idea is to reduce the memory footprint and the search\ntime by comparing the binary codes instead of the original\nvectors [64].\nLocal-Sensitive Hashing [65], [66]. It is a technique for\nfinding the approximate nearest neighbors of a given vector\nin a large collection of vectors. It works by using a hash\nfunction to transform the high-dimensional vectors into com-\npact binary codes, and then using a hash table to store and\nretrieve the codes based on their similarity or distance. In\nLSH, hash functions are designed to preserve the locality of\nvectors. Unlike traditional hash functions, LSH increases the\nprobability that similar items are mapped to the same code,\nthus increasing collisions among similar vectors. A trace of\nalgorithm description and implementation for locally sensitive\nhashing can be seen on the home page [67].\nThe LSH algorithm works by using a family of hash\nfunctions that use random projections or other techniques\nwhich are locality sensitive, meaning that similar vectors are\nmore likely to have the same or similar codes than dissimilar\nvectors [68], which satisfy the following property:\nPr[h(p) =h(q)] =f(d(p, q)) (8)\nwhere his a hash function, pandqare two points, dis a\ndistance function, and fis a similarity function. The similarity\nfunction fis a monotonically decreasing function of the\ndistance, such that the closer the points are, the higher the\nprobability of collision.\nThere are different families of hash functions for different\ndistance functions and similarity functions. For example, one\nof the most common families of hash functions for Euclidean\ndistance and cosine similarity is:\nh(p) =\u0016a·p+b\nw\u0017\n(9)where ais a random vector, bis a random scalar, and wis\na parameter that controls the size of the hash bucket. The\nsimilarity function for this family of hash functions is:\nf(d(p, q)) = 1 −d(p, q)\nπw(10)\nwhere d(p, q)is the Euclidean distance between pandq.\nThe advantage of LSH is that it can reduce the memory\nfootprint and the search time by comparing the binary codes\ninstead of the original vectors, also adapt to dynamic data sets,\nby inserting or deleting codes from the hash table without\naffecting the existing codes [69].\nThe performance of LSH depends on several factors, such as\nthe dimensionality of the data, the number of hash functions,\nthe number of bits per code, and the desired accuracy and\nrecall. These factors affect the trade-off between accuracy and\nefficiency, as well as the complexity and scalability of the\nalgorithm. There are also some challenges and extensions of\nLSH, such as dealing with noisy and outlier data, choosing\na good hash function family, or using multiple hash tables to\nincrease recall. It is improved by [70], [71], [72].\nSpectral Hashing [73]. It is a technique for finding the\napproximate nearest neighbors of a given vector in a large\ncollection of vectors. It works by using spectral graph theory\nto generate hash functions that minimize the quantization\nerror and maximize the variance of the binary codes. Spectral\nhashing can perform well when the data points lie on a low-\ndimensional manifold embedded in a high-dimensional space.\nThe spectral hashing algorithm works by solving an opti-\nmization problem that balances two objectives: (1) minimizing\nthe variance of each binary function, which ensures that the\ndata points are evenly distributed among the hypercubes,\nand (2) maximizing the mutual information between differ-\nent binary functions, which ensures that the binary code is\ninformative and discriminative. The optimization problem can\nbe formulated as follows:\nmin\ny1,...,y nnX\ni=1V ar(yi)−λI(y1, . . . , y n) (11)\nwhere yiis the i-th binary function, V ar(yi)is its variance,\nI(y1, . . . , y n)is the mutual information between all the binary\nfunctions, and λis a trade-off parameter.\nThe advantage of spectral hashing is that it can perform\nwell when the data points lie on a low-dimensional mani-\nfold embedded in a high-dimensional space. Spectral hashing\nsearch performance depends on roughly the same factors as\nlocal-sensitive hashing. There are also some challenges and\nextensions of spectral hashing, such as dealing with noisy and\noutlier data, choosing a good graph Laplacian for the data\nmanifold, or using multiple hash functions to increase recall.\nSpherical Hashing . Spherical hashing is a binary encod-\ning technique based on hyperspheres, designed for efficient\nANNS. Unlike traditional hyperplane-based methods, it parti-\ntions the data space using hyperspheres, which define tighter\nand more compact regions through their centers and radii.\nEach spherical hashing function, as described by Heo [74], is\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\ncharacterized by (pk∈RD)and a distance threshold tk∈R+,\nas detailed below:\nhk(x) =(\n−1when d(pk, x)> tk\n+1 when d(pk, x)≤tk(12)\nwhere d(·,·)is the Euclidean distance between two points\ninD-dimensional real space; however, alternative distance\nmetrics, such as the Lp-norms, could also be employed in\nplace of the Euclidean distance. The output of each spherical\nhashing function hk(x)determines if the point xresides\nwithin the hypersphere that has pkas its center and tk\nas its radius. To improve similarity measurement, spherical\nhashing introduces the spherical Hamming distance, which\naccounts for the number of shared hyperspheres. The spherical\nHamming distance is formulated as follows:\ndshd(bi, bj) =|bi⊕bj|\n|bi∧bj|(13)\nwhere |bi⊕bj|represents the number of different bits (where\nthe XOR operation results in 1) between two binary codes,\n|bi∧bj|represents the number of common bits (where the\nAND operation results in 1) between the two binary codes.\nCompared to hyperplane-based hashing functions, spher-\nical hashing can map more spatially coherent data points\ninto binary codes. Moreover, in high-dimensional spaces,\nhyperspheres are more powerful than hyperplanes in defining\nclosed regions, allowing more potential nearest neighbors to\nbe captured within the binary code region of a query point.\nDeep Hashing [75], [76]. It is a technique for finding the\napproximate nearest neighbors of a given vector in a large\ncollection of vectors. It works by using a deep neural network\nto learn hash functions that transform high-dimensional vec-\ntors into compact binary codes, and then using a hash table\nto store and retrieve the codes based on their similarity or\ndistance [77]. The hash functions are designed to preserve the\nsemantic information of the vectors, which means that similar\nvectors are more likely to have the same or similar codes than\ndissimilar vectors [78].\nThe deep hashing algorithm works by optimizing an ob-\njective function that balances two terms: (1) a reconstruction\nloss that measures the fidelity of the binary codes to the\noriginal data points and (2) a quantization loss that measures\nthe discrepancy between the binary codes and their continu-\nous relaxations. The objective function can be formulated as\nfollows:\nmin\nW,BNX\ni=1∥xi−Wbi∥2\n2+λ∥bi−sgn (bi)∥2\n2(14)\nwhere xiis the i-th data point, biis its continuous relaxation,\nsgn (bi)is its binary code, Wis a weight matrix that maps the\nbinary codes to the data space, and λis a trade-off parameter.\nThe advantage of deep hashing is that it can leverage the\nrepresentation learning ability of neural networks to generate\nmore discriminative and robust codes for complex data, such\nas images, texts, or audios. The performance of deep hashing\ndepends on several factors, such as the architecture of theneural network, the loss function used to train the network, and\nthe number of bits per code. These factors affect the trade-off\nbetween accuracy and efficiency, as well as the complexity and\nscalability of the algorithm. There are also some challenges\nand extensions of deep hashing, such as dealing with noisy and\noutlier data, choosing a good initialization for the network, or\nusing multiple hash functions to increase recall.\n2) Tree-Based Approach: The main idea of the tree-based\napproach is to build hierarchical or recursively partitioned data\nstructures, such as trees, to break high-dimensional datasets\ninto smaller subsets. This method improves query efficiency\nby reducing the number of points that need to be searched.\nAlong this line, three tree-based methods will be presented:\napproximate nearest neighbors oh yeah, best bin first, and k-\nmeans tree. The idea is to reduce the search space by following\nthe branches of the tree that are most likely to contain the\nnearest neighbors of the query point.\nApproximate Nearest Neighbors Oh Yeah [79]. It is\na technique which can perform fast and accurate similarity\nsearch and retrieval of high-dimensional vectors. It works by\nbuilding a forest of binary trees, where each tree splits the\nvector space into two regions based on a random hyperplane.\nEach vector is then assigned to a leaf node in each tree based\non which side of the hyperplane it falls on. To query a vector,\nAnnoy traverses each tree from the root to the leaf node that\ncontains the vector, and collects all the vectors in the same\nleaf nodes as candidates. Then, it computes the exact distance\nor similarity between the query vector and each candidate, and\nreturns the top knearest neighbors.\nThe formula for finding the median hyperplane between two\npoints pandqis:\nw·x+b= 0 (15)\nwhere w=p−qis the normal vector of the hyperplane, xis\nany point on the hyperplane, and b=−1\n2(w·p+w·q)is the\nbias term. The formula for assigning a point x to a leaf node\nin a tree is:\nsign ( wi·x+bi) (16)\nwhere wiandbiare the normal vector and bias term of the\ni-th split in the tree, and sign is a function that returns 1if the\nargument is positive, −1if negative, and 0if zero. The point\nxfollows the left or right branch of the tree depending on the\nsign of this expression, until it reaches a leaf node.\nThe formula for searching for the nearest neighbor of a\nquery point qin the forest is:\nmin\nx∈C(q)d(q, x) (17)\nwhere C(q)is the set of candidate points obtained by travers-\ning each tree in the forest and retrieving all the points in the\nleaf node that qbelongs to, and dis a distance function, such\nas Euclidean distance or cosine distance. The algorithm uses\na priority queue to store the nodes to be visited, sorted by\ntheir distance to q. The algorithm also prunes branches that\nare unlikely to contain the nearest neighbor by using a bound\non the distance between qand any point in a node.\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nThe advantage of Annoy is that it can uses multiple random\nprojection trees to index the data points, which can increase\nthe recall and robustness of the search, also reduce the memory\nusage and improve the speed of NNS, by creating large read-\nonly file-based data structures that are mapped into memory so\nthat many processes can share the same data. The performance\nof Annoy depends on several factors, such as the dimensional-\nity of the data, the number of trees built, the number of nearest\ncandidates to search, and the distance approximation method\nused. These factors affect the trade-off between accuracy and\nefficiency, as well as the complexity and scalability of the\nalgorithm. There are also some challenges and extensions of\nAnnoy, such as dealing with noisy and outlier data, choosing a\ngood splitting plane for each node, or using multiple distance\nmetrics to increase recall.\nBest Bin First [80], [81]. It is a technique for finding\nthe approximate nearest neighbors of a given vector in a\nlarge collection of vectors. It works by building a kd-tree that\npartitions the data points into bins, and then searching for the\nclosest bin to the query point. The algorithm then searches\nwithin the closest bin to find the closest point to the query\npoint. The best bin first algorithm still follows (1) (2). The\nadvantage of best bin first is that it can reduce the search\ntime and improve the accuracy of NNS, by focusing on the\nmost promising bins and avoiding unnecessary comparisons\nwith distant points. The performance of best bin first depends\non several factors, such as the dimensionality of the data, the\nnumber of bins per node, the number of nearest candidates to\nsearch, and the distance approximation method used. These\nfactors affect the trade-off between accuracy and efficiency,\nas well as the complexity and scalability of the algorithm.\nThere are also some challenges and extensions of best bin\nfirst, such as dealing with noisy and outlier data, choosing a\ngood splitting dimension and value for each node, or using\nmultiple trees to increase recall.\nK-means Tree [82]. It is a technique for clustering high-\ndimensional data points into a hierarchical structure, where\neach node represents a cluster of points. It works by applying\na k-means clustering algorithm to the data points at each level\nof the tree, and then creating child nodes for each cluster. The\nprocess is repeated recursively until a desired depth or size of\nthe tree is reached.\nThe formula for assigning a point xto a cluster using the\nk-means algorithm is:\narg min\ni=1,...,k∥x−ci∥2\n2(18)\nwhere argmin is a function that returns the argument that\nminimizes the expression, and ∥ · ∥ 2denotes the Euclidean\nnorm. The formula for assigning a point xto a leaf node in a\nk-means tree is:\narg min\nN∈L(x)∥x−N.center ∥2\n2 (19)\nwhere L(x)is the set of leaf nodes that x belongs to, and N\n.center is the cluster center of node N. The point xbelongs to\na leaf node if it belongs to all its ancestor nodes in the tree.\nquery vector qnearest vectorstarting node\nQuery \nVectorStarting \nNodeNearest VectorFig. 4. The process of performing nearest neighbor search on a small-world\nnetwork.\nThe formula for searching for the nearest neighbor of a\nquery point qin the k-means tree is:\nmin\nx∈C(q)∥q−x∥2\n2 (20)\nwhere C(q)is the set of candidate points obtained by travers-\ning each branch of the tree and retrieving all the points in\nthe leaf nodes that qbelongs to. The algorithm uses a priority\nqueue to store the nodes to be visited, sorted by their distance\ntoq. The algorithm also prunes branches that are unlikely to\ncontain the nearest neighbor by using a bound on the distance\nbetween qand any point in a node.\nThe advantage of K-means tree is that it can perform fast\nand accurate similarity search and retrieval of data points based\non their cluster membership, by following the branches of the\ntree that are most likely to contain the nearest neighbors of\nthe query point. K-means tree can also support dynamic oper-\nations, such as inserting and deleting points, by updating the\ntree structure accordingly. The performance of K-means tree\ndepends on several factors, such as the dimensionality of the\ndata, the number of clusters per node, and the distance metric\nused. These factors affect the trade-off between accuracy and\nefficiency, as well as the complexity and scalability of the\nalgorithm. There are also some challenges and extensions of\nK-means tree, such as dealing with noisy and outlier points,\nchoosing a good initialization for the k-means algorithm, or\nusing multiple trees to increase recall.\n3) Graph-Based Approach: The core concept of the graph-\nbased approach is the small-world network. A small-world\nnetwork is a complex network where most nodes are not\ndirectly connected, but almost any node can be reached from\nanother in just a few steps. This means the average path\nlength between any two nodes is much shorter than the total\nnumber of nodes. The ”six degrees of separation” theory in\nsociology [83] is a concrete manifestation of a small-world\nnetwork.\nGiven a query vector qand the task finding the kclosest\nvectors from a set O, the graph-based approach uses a graph\nG(V, E)to represent these objects, where each object oi\ncorresponds to a node vi. The small-world network graph is\nconstructed by sequentially adding all nodes. As shown in\nFigure 4, when a new node is added, a list of neighboring\nnodes is generated using a greedy algorithm, and bidirectional\nconnections are established between the new node and all\nnodes in the list. Once the graph is constructed, searching for\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\nqis similar to the process of adding a new node. The search\nstarts from a randomly selected node (with different small-\nworld variants possibly using different selection strategies).\nFrom the list of neighbors of the current node, the node most\nsimilar to qis identified. If such a node is found, it becomes\nthe next node, and the search process is repeated. If a node\nhas a higher similarity to qthan all of its neighbors, the search\nstops, and that node is considered the one most similar to q.\nTwo types of graph-based methods are introduced: navigable\nsmall world (NSW), and hierachical navigable small world\n(HNSW).\nNavigable Small World It is a technique that uses a graph\nstructure to store and retrieve high-dimensional vectors based\non their similarity or distance [84]. The NSW algorithm builds\na graph by connecting each vector to its nearest neighbors,\nas well as some random long-range links that span different\nregions of the vector space. The idea is that these long-range\nlinks create shortcuts that allow for faster and more efficient\ntraversal of the graph, similar to how social networks have\nsmall world properties [85].\nThe NSW algorithm works by using a greedy heuristic to\nadd edges to the graph [86]. The algorithm starts with an\nempty graph and adds one point at a time. For each point,\nthe algorithm finds its nearest neighbor in the graph using\na random walk, and connects it with an edge. Then, the\nalgorithm adds more edges by connecting the point to other\npoints that are closer than its current neighbors. The algorithm\nrepeats this process until all points are added to the graph.\nThe formula for finding the nearest neighbor of a point p\nin the graph using a random walk is:\narg min\nq∈N(p)d(p, q) (21)\nwhere N(p) is the set of neighbors of p in the graph, and\nd is a distance function, such as Euclidean distance or cosine\ndistance. The algorithm starts from a random point in the graph\nand moves to its nearest neighbor until it cannot find a closer\npoint. The formula for adding more edges to the graph using\na greedy heuristic is:\n∀q∈N(p),∀r∈N(q), ifd(p, r)< d(p, q),\nthen add edge (p, r)(22)\nwhere N(p)andN(q)are the sets of neighbors of pandq\nin the graph, respectively, and dis a distance function. The\nalgorithm connects pto any point that is closer than its current\nneighbors.\nThe advantage of the NSW algorithm is that it can handle\narbitrary distance metrics, it can adapt to dynamic data sets,\nand it can achieve high accuracy and recall with low memory\nconsumption. The NSW algorithm also uses a greedy routing\nstrategy, which means that it always moves to the node that is\nclosest to the query vector, until it reaches a local minimum\nor a predefined number of hops.\nThe performance of the NSW algorithm depends on several\nfactors, such as the dimensionality of the vectors, the number\nof neighbors per node, the number of long-range links per\nnode, and the number of hops per query. These factors affectthe trade-off between accuracy and efficiency, as well as\nthe complexity and scalability of the algorithm. There are\nalso some extensions and variations of the NSW algorithm,\nsuch as hierarchical navigable small world (HNSW), which\nadds multiple layers of graphs, each with different scales and\ndensities, or navigable small world with pruning (NSWP),\nwhich removes redundant links to reduce memory usage and\nimprove search speed.\nHierachical Navigable Small World [87]. It is a state-of-\nthe-art technique for finding the approximate nearest neighbors\nof a given vector in a large collection of vectors. It works by\nbuilding a graph structure that connects the vectors based on\ntheir similarity or distance, and then using a greedy search\nstrategy to traverse the graph and find the most similar vectors.\nThe HNSW algorithm still follows (21) and (22). The HNSW\nalgorithm also builds a hierarchical structure of the graph\nby assigning each point to different layers with different\nprobabilities. The higher layers contain fewer points and edges,\nwhile the lower layers contain more points and edges. When a\nsearch query comes in, the HNSW algorithm finds the closest\nmatching data points in the highest layer. It then proceeds\nlayer by layer, moving downwards and finding the nearest data\npoints in each subsequent layer based on those from the layer\nabove. These points are considered the nearest neighbors. The\nalgorithm continues this process in the lower layers, updating\nthe list of nearest neighbors at each step. Once it reaches the\nbottom layer, the HNSW algorithm returns the data points that\nare closest to the search query. The algorithm uses a parameter\nM to control the maximum number of neighbors for each point\nin each layer.\nThe formula for assigning a point pto a layer lusing a\nrandom probability is:\nPr[p∈l] =\u001a1 ifl= 0\n1\nMifl >0(23)\nwhere Mis the parameter that controls the maximum number\nof neighbors for each point in each layer. The algorithm\nassigns pto layer lwith probability Pr[p∈l], and stops when\nit fails to assign pto any higher layer.\nThe formula for searching for the nearest neighbor of a\nquery point q in the hierarchical graph is:\nmin\np∈C(q)d(q, p) (24)\nwhere C(q)is the set of candidate points obtained by travers-\ning each layer of the graph from top to bottom and retrieving\nall the points that are closer than the current best distance.\nThe algorithm uses a priority queue to store the nodes to be\nvisited, sorted by their distance to q. The algorithm also prunes\nbranches that are unlikely to contain the nearest neighbor by\nusing a bound on the distance between qand any point in a\nnode.\nThe advantage of HNSW is that it can achieve better\nperformance than other methods of ANNS, such as tree-\nbased or hash-based techniques. For example, it can handle\narbitrary distance metrics, it can adapt to dynamic data sets,\nand it can achieve high accuracy and recall with low memory\nconsumption. HNSW also uses a hierarchical structure that\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\nallows for fast and accurate search, by starting from the highest\nlayer and moving down to the lowest layer, using the closest\nnode as the next hop at each layer. So, HNSW can effectively\n“jump over” large portions of data that don’t require searching.\nSuppose a data store has only a single layer. In that case,\nthe search algorithm is unable to bypass unrelated objects,\nmeaning it has to examine many more data points, even if\nthey are unlikely to be relevant matches.\nThe performance of HNSW depends on several factors, such\nas the dimensionality of the vectors, the number of layers, the\nnumber of neighbors per node, and the number of hops per\nquery. These factors affect the trade-off between accuracy and\nefficiency, as well as the complexity and scalability of the\nalgorithm. There are also some challenges and extensions of\nHNSW, such as finding the optimal parameters for the graph\nconstruction, dealing with noise and outliers, and scaling to\nvery large data sets. Some of the extensions include optimized\nproduct quantization (OPQ), which combines HNSW with\nproduct quantization to reduce quantization distortions, or\nproduct quantization network (PQN), which uses a neural\nnetwork to learn an end-to-end product quantizer from data.\n4) Quantization-Based Approach: The core idea of quan-\ntization is to map points in a high-dimensional space to a\nlow-precision representation in a finite set. This reduces the\nnumber of bits required for storage, lowering storage demands.\nBy using these quantized representations during queries, the\ndistance between the original points can be quickly estimated.\nSpecifically, a vector quantizer maps k-dimensional vectors\nfrom the vector space Rkto a finite set of vectors S={si:\ni= 1, . . . , n }. Each vector siis called a code vector or code-\nword or centroids. The collection of all codewords is referred\nto as a codebook. Associated with each codeword, si, is a\nnearest neighbor region called V oronoi region, and it is defined\nby:Vi=\b\nx∈Rk:∥x−yi∥ ≤ ∥ x−yj∥,forall j̸=i\t\n. The\nset of V oronoi region partitions the entire space Rksuch that:\nN[\ni=1Vi=Rk\nN\\\ni=1Vi=ϕfor all i̸=j\nFigure 5 shows the codewords in a two-dimensional space,\nwhere input vectors are marked with blue stars, codewords\nare marked with orange circles, and the V oronoi regions are\nseparated with boundary lines.\nA vector quantizer consists of two primary components:\nan encoder and a decoder, as shown in figure6. The encoder\ntakes an input vector and outputs the index of the codeword\nthat minimizes the distortion. This minimal distortion is found\nby calculating the distance between the input vector and\neach codeword in the codebook, typically using metrics like\nEuclidean or Hamming distance. Once the codeword with\nthe smallest distance is identified, its index is transmitted\nto the decoder. At the receiver, the decoder then maps this\nindex back to the corresponding codeword. Generating an\neffective codebook involves selecting codewords that best\nrepresent a given set of input vectors, along with determining\n𝑠𝑠1 𝑠𝑠2𝑠𝑠4\n𝑠𝑠5𝑠𝑠3𝑠𝑠8\n𝑠𝑠6𝑠𝑠7𝑠𝑠9𝑠𝑠10\n𝑠𝑠11𝑠𝑠12Vectors\nCodewords\nVoronoi\nRegion\n𝑠𝑠8\n𝑠𝑠1\n𝑠𝑠2𝑠𝑠3𝑠𝑠4𝑠𝑠5𝑠𝑠6\n𝑠𝑠7\n𝑠𝑠9𝑠𝑠10𝑠𝑠11\n𝑠𝑠12\n𝑠𝑠13\nVector\nCodeword\nVoronoiRegionFig. 5. Codewords and V oronoi Regions in a Two-Dimensional Space\nr\nSearch\nEngineInput Vector\nCodebook\nEncoder\nIndices\nOutput Vector\nHash Code N\nFig. 6. Vector Quantization Process with Encoder and Decoder Operations\nthe appropriate number of codewords. Designing an optimal\ncodebook is an NP-hard problem, implying that finding the\nabsolute best set of codewords through exhaustive search\nbecomes impractically complex as the number of codewords\nincreases. As a result, heuristic methods, such as the Linde-\nBuzo-Gray (LBG) algorithm, which is conceptually similar to\nthe K-means clustering algorithm, are commonly employed.\nTo create a codebook using the LBG algorithm, one first\nspecifies the number of codewords, N, which defines the\nsize of the codebook. Initially, Ncodewords are selected at\nrandom, often from the set of input vectors themselves. Each\ninput vector is then associated with the nearest codeword based\non the Euclidean distance. After all vectors have been assigned\nto their respective clusters, a new set of codewords is generated\nby computing the average of the vectors within each cluster.\nThis process involves summing the components of the vectors\nin each cluster and dividing by the total number of vectors\nin that cluster. This iterative procedure refines the codebook\nuntil a satisfactory level of representation is achieved. The\nformula for calculating the average of the components within\neach cluster is:\nyi=1\nmmX\nj=1xij\nwhere iis the component of each vector , mis the number of\nvectors in the cluster. There are also many other methods to de-\nsigning the codebook, methods such as Generative Pre-trained\nTransformer Vector quantization (GPTVQ) [88], Vector Post-\nTraining Quantization (VPTQ) [89], deep network architecture\nfor vector quantization (DeepVQ) [90], etc. Based on the\ncore idea described above, six quantization-based methods will\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\nbe presented here, namely inverted file index (IVF), product\nquantization (PQ) [91], [92], optimized product quantization\n(OPQ) [93], [94], online product quantization [95], scalable\nnearest neighbor (ScaNN) [96], and inverted file product\nquantization (IVF PQ) [40], [97]. Product quantization can\nreduce the memory footprint and search time of ANN search,\nby comparing codes instead of the original vectors [98].\nInverted File Index . Inverted File Index is a technique\ndesigned to enhance search efficiency by narrowing the search\narea through the use of neighbor partitions or clusters [99]. It\nuses clustering (e.g., K-means) to partition high-dimensional\nvectors into multiple regions (V oronoi Cells) and records the\nvectors within each region through an inverted index. During\na query, the search is restricted to a few regions closest to\nthe query vector, significantly reducing the search space and\nimproving retrieval efficiency. IVF is often combined with\nother techniques, such as Product Quantization (PQ), to further\noptimize storage and computation, making it widely used in\nimage retrieval, recommendation systems, and VDBs. Its main\nadvantages are fast search speed and high efficiency, though\nits performance in high-dimensional spaces may be limited by\nclustering quality and the complexity of dynamic updates.\nProduct Quantization . It is a technique for compress-\ning high-dimensional vectors into smaller and more efficient\nrepresentations [91], [92]. It works by dividing a vector into\nseveral sub-vectors, and then applying a clustering algorithm\n(such as k-means) to each sub-vector to assign it to one of a\nfinite number of possible values (called centroids). The result\nis a compact code that consists of the indices of the centroids\nfor each sub-vector.\nThe PQ algorithm works by using a vector quantization\ntechnique to map each subvector to its nearest centroid in a\npredefined codebook. The algorithm first splits each vector into\nmequal-sized subvectors, where m is a parameter that controls\nthe length of the code. Then, for each subvector, the algorithm\nlearns kcentroids using the k-means algorithm, where kis a\nparameter that controls the size of the codebook. Finally, the\nalgorithm assigns each subvector to its nearest centroid and\nconcatenates the centroid indices to form the code.\nThe formula for splitting a vector xintomsubvectors is:\nx= (x1, x2, . . . , x m) (25)\nwhere xiis the i-th subvector of x, and has dimension d/m ,\nwhere dis the dimension of x.\nThe formula for finding the centroids of a set of subvectors\nPusing the k-means algorithm is:\nci=1\n|Si|X\nx∈Six (26)\nwhere ciis the i-th centroid, Siis the set of subvectors\nassigned to the i -th cluster, and |·|denotes the cardinality of\na set.\nThe formula for assigning a subvector xto a centroid using\nthe k-means algorithm is:\nargmini=1,...,k∥x−ci∥2\n2(27)where argmin is a function that returns the argument that\nminimizes the expression, and ∥ · ∥ 2denotes the Euclidean\nnorm.\nThe formula for encoding a vector xusing PQ is:\nc(x) = (q1(x1), q2(x2), . . . , q m(xm)) (28)\nwhere xiis the i-th subvector of x, and qiis the quantization\nfunction for the i-th subvector, which returns the index of the\nnearest centroid in the codebook.\nThe advantage of product quantization is that it is simple\nand easy to implement, as it only requires a standard clustering\nalgorithm and a simple distance approximation method.\nThe performance of product quantization depends on several\nfactors, such as the dimensionality of the data, the number\nof sub-vectors, the number of centroids per sub-vector, and\nthe distance approximation method used. These factors affect\nthe trade-off between accuracy and efficiency, as well as the\ncomplexity and scalability of the algorithm. There are also\nsome challenges and extensions of product quantization, such\nas dealing with noisy and outlier data, optimizing the space\ndecomposition and the codebooks, or adapting to dynamic data\nsets.\nOptimized Product Quantization [93]. It is a variation of\nproduct quantization (PQ), which is a technique for compress-\ning high-dimensional vectors into smaller and more efficient\nrepresentations. OPQ works by optimizing the space decompo-\nsition and the codebooks to minimize quantization distortions.\nOPQ can improve the performance of PQ by reducing the loss\nof information and increasing the discriminability of the codes\n[94].\nThe advantage of OPQ is that it can achieve higher accuracy\nand recall than PQ, as it can better preserve the similarity or\ndistance between the original vectors.\nThe formula for applying a random rotation to the data is:\nx′=Rx (29)\nwhere xis the original vector, x′is the rotated vector, and\nRis a random orthogonal matrix. The formula for finding the\nrotation matrix for a subvector using an optimization technique\nis:\nmin\nRiX\nx∈Pi∥x−Rici(Rix)∥2\n2 (30)\nwhere Piis the set of subvectors assigned to the i-th cluster,\nRiis the rotation matrix for the i-th cluster, and ciis the\nquantization function for the i-th cluster, which returns the\nnearest centroid in the codebook.\nThe formula for encoding a vector xusing OPQ is:\nc(x) = (q1(R1x1), q2(R2x2), . . . , q m(Rmxm)) (31)\nwhere xiis the i-th subvector of x,Riis the rotation matrix for\nthei-th subvector, and qiis the quantization function for the\ni-th subvector, which returns the index of the nearest centroid\nin the codebook.\nThe performance of OPQ depends on several factors, such\nas the dimensionality of the data, the number of sub-vectors,\nthe number of centroids per sub-vector, and the distance\napproximation method used. These factors affect the trade-off\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14\nbetween accuracy and efficiency, as well as the complexity and\nscalability of the algorithm. There are also some challenges\nand extensions of OPQ, such as dealing with noisy and outlier\ndata, choosing a good optimization algorithm, or combining\nOPQ with other techniques such as hierarchical navigable\nsmall world (HNSW) or product quantization network (PQN).\nOnline Product Quantization [100]. It is a variation of\nproduct quantization (PQ), which is a technique for compress-\ning high-dimensional vectors into smaller and more efficient\nrepresentations. Online product quantization (O-PQ) works by\nadapting to dynamic data sets, by updating the quantization\ncodebook and the codes online. O-PQ can handle data streams\nand incremental data sets, without requiring offline retraining\nor reindexing.\nThe formula for splitting a vector xintomsubvectors is:\nx= (x1, x2, . . . , x m) (32)\nwhere xiis the i-th subvector of x, and has dimension d/m ,\nwhere dis the dimension of x.\nThe formula for initializing the centroids of a set of sub-\nvectors Pusing the k-means++ algorithm is:\nci=randomly choose a point from P (33)\nwhere ciis the i-th centroid, with probability proportional to\nD(x)2,D(x)is the distance between point xand its closest\ncentroid among {c1, . . . , c i−1}.\nThe formula for assigning a subvector xto a centroid using\nPQis:\nargmini=1,...,k∥x−ci∥2\n2 (34)\nwhere arg min is a function that returns the argument that\nminimizes the expression, and ∥ · ∥ 2denotes the Euclidean\nnorm.\nThe formula for encoding a vector xusing PQis:\nc(x) = (q1(x1), q2(x2), . . . , q m(xm)) (35)\nwhere xiis the i-th subvector of x, and qiis the quantization\nfunction for the i-th subvector, which returns the index of the\nnearest centroid in the codebook.\nThe O-PQ algorithm also updates the codebooks and codes\nfor each subvector using an online learning technique. The\nalgorithm uses two parameters: α, which controls the learning\nrate, and β, which controls the forgetting rate. The algorithm\nupdates the codebooks and codes as follows:\nFor each new point x, assign it to its nearest centroid in\neach subvector using PQ.\nFor each subvector xi, update its centroid cqi(xi)as:\ncqi(xi)= (1−α)cqi(xi)+αxi (36)\nFor each subvector xi, update its code qi(xi)as:\nqi(xi) = arg min\nj=1,...,k∥(1−β)xi+βxi−(1−β)cj+βcj∥2\n2\n(37)\nwhere xiandcjare the mean vectors of all points and\ncentroids in subvector i, respectively.\nThe advantage of O-PQ is that it can deal with changing\ndata distributions and new data points, as it can update the\ncodebooks and the codes in real time.O-PQ search performance depends on roughly the same\nfactors as OPQ, and also faces similar challenges as OPQ.\nScalable Nearest Neighbor . It is a technique for efficient\nvector similarity search at scale [96], [101]. ScaNN optimizes\nMaximum Inner Product Search (MIPS) through search space\npruning and quantization. Traditional MIPS schemes aim to\nminimize the average distance between each vector xand its\ncentroids ˜x, that is, to minimize quantization distortions.\nThe formula for typically measure the quantization distor-\ntion is:\nD=1\nNNX\ni=1∥xi−˜xi∥2\n2(38)\nwhere Nis the total number of vectors, xiis original vector,\n˜xis quantized centroid, and ∥ · ∥ 2denotes Euclidean norm.\nWhile the ScaNN algorithm argues that optimizing the\naverage distance is not equivalent to optimizing the accuracy\nof nearest-neighbor searches. The hypothesis it puts forward\nis that the objective of maximizing the inner product between\ntwo points is not entirely consistent with the objective of\nminimizing the average distance between two points.\nScaNN taking into account the distribution characteristics of\nthe data in different directions, ellipsoidal or other shaped re-\ngions are used instead of spherical regions around the centroids\nto better fit the local structure of the data. Building on this\nperspective, the anisotropic loss function can further enhance\nthe adaptability of vector quantization to data anisotropy.\nBy explicitly separating quantization errors into parallel and\northogonal components, the anisotropic loss function assigns\ndistinct scaling parameters h∥\niandh⊥\nito these components,\nrespectively. This allows for more fine-grained control over the\nquantization process, ensuring that the errors are distributed in\nalignment with the data’s geometric characteristics.\nThe anisotropic vector quantization algorithm shares sim-\nilarities with the Lloyd algorithm, iteratively refining the\ncodebook and data partitions. The key distinction lies in the\nupdate rule for the codebook centroids:\ncj=P\ni∈Xjh∥\ni·x∥\ni+h⊥\ni·x⊥\ni\nP\ni∈Xj\u0010\nh∥\ni+h⊥\ni\u0011 (39)\nwhere Xjis the set of data points assigned to the codeword cj.\nThis update formula takes into account the directional scaling,\nensuring that the resulting codewords are optimally positioned\nin accordance with the anisotropic properties of the data.\nBy integrating this anisotropic loss framework, the quanti-\nzation process moves beyond spherical symmetry and better\naccommodates ellipsoidal or irregularly shaped distributions\nin the data. This alignment with ScaNN’s approach to using\nellipsoidal regions around centroids enhances both the repre-\nsentation accuracy and the retrieval efficiency, especially in\ncases where the data exhibits significant directional variance.\nMoreover, extending this concept to product quantization\nallows the construction of multiple subspace-specific dictio-\nnaries, each tuned to the anisotropic characteristics of the\ncorresponding subspace. This not only retains the efficiency\nof ScaNN’s design but also adds a layer of flexibility for\nhandling complex, high-dimensional data distributions. The\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15\nperformance of ScaNN depends on several factors, such as the\nanisotropy of the data distribution, the choice of quantization\nmethods like vector or product quantization, the size and\nquality of the codebooks, and the efficiency of the partitioning\nand scoring processes. These factors impact the trade-off\nbetween retrieval accuracy and computational speed, as well as\nthe scalability of the algorithm for massive datasets. Additional\nchallenges and potential extensions include handling highly ir-\nregular data distributions, selecting optimal scaling parameters\nfor the score-aware loss function, and integrating ScaNN with\ncomplementary techniques like hierarchical search structures\nor advanced compression methods.\nInverted File Product quantization . It is a widely used\ntechnique for approximate nearest neighbor (ANN) search\nin high-dimensional vector spaces [40], [97]. This algorithm\nis a combination of the Inverted File Indexing (IVF) and\nProduct Quantization (PQ) algorithms. IVF PQ first uses the\nIVF algorithm to divide or partition the data into clusters and\nuses the parameter nprobe to control the number of clusters.\nThe higher the nprobe , the better the search results, but it\nalso increases the time required. It then identifies the top-N\nclusters closest to the query vector and performs the search\nwithin these N clusters using the Product Quantization (PQ)\nalgorithm.\nThe IVF PQ algorithm naturally results in two different\napproaches when using the PQ algorithm: the first involves\nperforming K-means clustering with the IVF algorithm, fol-\nlowed by applying a local PQ algorithm for dimensionality\nreduction within each cluster; the second also starts with the\nIVF algorithm to divide all data points into several clusters\nbut applies a globally unified PQ algorithm for dimensionality\nreduction within each cluster.In addition, there is another\nimplementation scheme for the IVFPQ algorithm in FAISS\n(Facebook AI Similarity Search)1. First, all data points are\nclustered using the IVF algorithm. Then, for all data points\nwithin each cluster, the difference between each point and its\ncluster center (referred to as the ”””residual”) is calculated.\nMathematically, the residual represents the offset of a data\npoint relative to its cluster center. This operation is equivalent\nto shifting all cluster centers to the origin, causing all points\nto focus around the origin. Afterward, the PQ algorithm\nis applied to the residuals. Compared to the previous two\napproaches, the key difference is that PQ is applied to the\nresiduals rather than the original vectors. The advantage of\nthis scheme is that, as the data points become more tightly\nclustered, the average size of each cluster region is smaller,\nleading to reduced approximation errors during distance com-\nputations.\nIV. V ECTOR DATABASE COMPARISON\nIn the realm of VDBs, a variety of storage and search\ntechnologies has given rise to a diverse range of commercial\nand open-source solutions. In this section, to help users\nbetter understand the performance of different VDBs, we\nhave conducted a comprehensive comparison of several pop-\n1https://ai.meta.com/tools/faiss/ular options, including PgVector2, QdrantCloud3, Weaviate-\nCloud4, ZillizCloud5, Milvus6, ElasticCloud7, and Pinecone8.\nThe comparison of VDBs includes both the attributes and\ncharacteristics of different VDBs, as well as a comparison of\ntheir loading capacity and search performance.\n1) The Comparison of Features and Characteristics of\nVector Databases: The characteristics of VDBs directly affect\ntheir performance in practical applications. Therefore, gaining\na deep understanding of these databases’ features is essential\nfor selecting the most suitable one. As shown in Table I, we\ncompare several popular VDBs, focusing on their differences\nin indexing methods, query types, distance functions, scala-\nbility, maximum dimension, and support for data management\nfeatures such as replication, sharding, and partitioning.\nIt can be observed from the table I that all VDBs sup-\nport NNS and ANNS. However, the implementation strate-\ngies and optimizations for these searches vary significantly\nacross databases, depending on their underlying indexing\nmethods and architectural designs. For example, the indexing\nmethods and distance functions are not exactly the same\nacross databases, but there are commonalities. For instance,\nall databases except Pinecone support graph-based methods,\nwhich indicates that graph-based methods are widely adopted\nfor their ability to handle complex relationships and data\nstructures. Additionally, the majority of databases also support\nthree distance functions: inner product, cosine similarity, and\nEuclidean distance. For details on the indexing methods and\ndistance functions supported by different databases, see the\ntable II and table III below.\nScalability is a critical factor in evaluating the performance\nand flexibility of VDBs, especially for large-scale and high-\ndemand applications. Scalability is typically categorized into\nhorizontal scaling and vertical scaling. Horizontal scaling\nrefers to a database’s ability to distribute data and computation\nacross multiple nodes, allowing it to handle large datasets\nand high query throughput. This approach is particularly\nbeneficial for cloud-native environments and distributed archi-\ntectures, where data is sharded and replicated across multiple\nmachines. In contrast, vertical scaling involves upgrading a\nsingle machine with more resources, such as additional CPU\npower or memory, to manage increased workloads. Both\nscaling methods offer distinct advantages depending on the\napplication’s requirements and the environment in which the\ndatabase operates. Specifically, PgVector, QdrantCloud, and\nPinecone support both horizontal and vertical scaling modes,\nwhile WeaviateCloud, Milvus, and ElasticCloud only support\nhorizontal scaling. ZillizCloud is the only one that supports\nonly vertical scaling. Although the level of support for scala-\nbility varies across databases, most exhibit strong capabilities\nin data storage and backup. Specifically, all databases, except\nfor ElasticCloud, for which no relevant information was found,\n2http://github.com/pgvector.\n3https://www.qdrant.tech.\n4http://weaviate.io.\n5http://zilliz.com/.\n6http://milvus.io.\n7http://elastic.co.\n8http://pinecone.io.\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16\nTABLE I\nFEATURES OF VECTOR DATABASES\nDatabase Query Types Indexing Methods NSD Scalability Replication Sharding PartitioningMaximum\nDimension\nANNS NNSBrute\nForceTree\nBasedHash\nBasedGraph\nBasedQuantization\nBasedHorizontal\nScalingVertical\nScaling\nPgVector ✓ ✓ ✓ ✓ ✓ ✓ ✓ 7✓ ✓ ✓ ✓ ✓ 16,000\nQdrantCloud ✓ ✓ ✓ × × ✓ ✓ 4✓ ✓ ✓ ✓ ✓ 65,535\nWeaviateCloud ✓ ✓ ✓ × × ✓ × 6✓ ×✓ ✓ ✓ 65,535\nZillizCloud ✓ ✓ ✓ ×✓ ✓ ✓ 4×✓ ✓ ✓ ✓ 32,768\nMilvus ✓ ✓ ✓ × × ✓ ✓ 6✓ ×✓ ✓ ✓ 32,768\nElasticCloud ✓ ✓ ✓ × × ✓ × 4✓ ×✓ ✓ ✓ N/A\nPinecone ✓ ✓ N/A N/A N/A N/A N/A 3✓ ✓ ✓ ✓ ✓ N/A\nAbbreviations: NSD Number of Supported Distance Functions, N/A Unknown, ✓Support, ×Not Support\nThe database information listed above is based on data up to December 1, 2024.\nTABLE II\nOVERVIEW OF SUPPORTED DISTANCE FUNCTIONS IN VECTOR DATABASES\nPgVector QdrantCloud WeaviateCloud ZillizCloud Milvus ElasticCloud PineconeDistance FunctionInner Product ✓ ✓ ✓ ✓ ✓ ✓ ✓\nCosine Similarity ✓ ✓ ✓ ✓ ✓ ✓ ✓\nManhattan Distance ✓ ✓ ✓ × × × ×\nHamming Distance ✓ × ✓ ✓ ✓ × ×\nJaccard Distance ✓ × × ✓ ✓ × ×\nTaxicab Distance ✓ × × × × × ×\nEuclidean Distance ✓ ✓ ✓ ×✓ ✓ ✓\nStructural Similarity × × × × ✓ × ×\nMax Inner Product × × × × × ✓ ×\nAbbreviations: ✓Support, ×Not Support\nThe database information listed above is based on data up to December 1, 2024.\nsupport Replication, Sharding, and Partitioning. These features\nensure fault tolerance, efficient data distribution, and flexible\nquery handling.\nThe last column of the table I provides statistics on the\nmaximum vector dimensions supported by each database. It\ncan be observed that, with the exception of ElasticCloud and\nPinecone, for which no relevant information was available,\nmost of the listed VDBs support a total vector dimension in\nthe range of tens of thousands, with the maximum supported\ndimensions ranging from 16,000 to 65,535. It should be noted\nthat QdrantCloud has a default support for up to 65,535\ndimensions, though this can be configured to support higher\ndimensions.\n2) The Comparison of Loading Capacity and Search Per-\nformance of Vector Database: In this subsection, we have\nopted to use the performance results obtained from the exist-\ning benchmarking tool, VectorDBBench(A Benchmark Tool\nfor VectorDB)9, rather than conducting our own tests. This\ndecision is based on the tool’s comprehensive and standardized\n9https://github.com/zilliztech/VectorDBBench?tab=readme-ov-filetesting methodology, which provides reliable, reproducible\nresults across various VDBs. By utilizing pre-existing data,\nwe ensure consistency and comparability, as these results\nhave been generated under controlled conditions, following\nestablished benchmarks.\nVectorDBBench provides a comprehensive performance\nanalysis by evaluating VDBs based on metrics such as Queries\nPer Second (QPS), recall rate, latency(the time required for\neach query from submission to system response), load du-\nration, and maximum load count(The maximum number of\nvectors a database can successfully insert or store in a single\nloading operation.). Its testing methodology employs a relative\nscoring mechanism to ensure fair comparisons. For QPS, the\nhighest observed value among all tested databases serves as\nthe reference baseline; for latency, the lowest observed value\namong all tested databases is used as the baseline, with an\nadditional 10ms adjustment to avoid distortions when latency\nis very low. For systems that fail or encounter timeouts in a\nspecific test case, their scores are penalized by assigning a\nvalue proportionally worse than the lowest-performing result,\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 17\nTABLE III\nOVERVIEW OF SUPPORTED INDEXING METHODS IN VECTOR DATABASES\nPgVector QdrantCloud WeaviateCloud ZillizCloud Milvus ElasticCloud PineconeIndexing MethodHNSW ✓ ✓ ✓ ✓ ✓ ✓ N/A\nFlat × × ✓ ×✓ × N/A\nBINFlat × × × × ✓ × N/A\nIVF Flat × × × × ✓ × N/A\nBIN IVF Flat × × × × ✓ × N/A\nIVF SQ8 × × × × ✓ × N/A\nIVF PQ × × × × ✓ × N/A\nB-tree ✓ × × × × × N/A\nLSH × × × ✓ × × N/A\nBRIN ✓ × × × × × N/A\nInverted File Index ✓ × × × ✓ ✓ N/A\nSPARSE Inverted Index × × × × × × N/A\nSPARSE WAND × × × × ✓ × N/A\nGIST ✓ × × × × × N/A\nGIN ✓ × × × × × N/A\nDiskANN × ✓ × ✓ × × N/A\nSCANN × × × ✓ × × N/A\nSparse Vector Index × ✓ × × × × N/A\nParameterized index × ✓ × × × × N/A\nAbbreviations: N/A Unknown, ✓Support, ×Not Support\nThe database information listed above is based on data up to December 1, 2024.\nTABLE IV\nVECTOR DATABASE EVALUATION TEST CASES\nCase No. Case Type Dataset Dataset Size Vector Dimensions Filtering Rate Test Metrics\n1 Capacity SIFT1500K 128 N/A NIV\n2 Capacity GIST2100K 960 N/A NIV\n3 Search Performance Google C43500K 1536 N/A IBT, R, L, MQPS\n1http://corpus-texmex.irisa.fr/\n2http://corpus-texmex.irisa.fr/\n3The processed version of Google C4 dataset(https://huggingface.co/datasets/allenai/c4)\nAbbreviations: N/A. Not Applicable, NIV . Number of inserted vector, IBT. Index building time, R. Recall, L.\nLatency, MQPS. Maxiumum QPS\nusing a factor of two. For example, in the case of QPS, the\nscore is reduced to half of the minimum observed value, while\nfor latency, it is increased to twice the maximum observed\nvalue. The formulas for calculating QPS and latency metrics\nfor VDB xare as follows:\nQPS x=origin QPS x\nbase QPS×100 (40)\nLatency x=base Latency + 10ms\norigin Latency x+ 10ms×100 (41)\nwhere origin QPS xandorigin Latency xrepresent the\noriginal QPS value and original latency value, respectively,\nmeasured for database xduring the test. base QPS and\nbase Latency is the reference baseline.\nSpecifically, as shown in table IV, the VDB evaluation con-\nsists of a series of test cases designed to assess capacity, search\nperformance, and filtering search performance. Capacity cases\n(Cases 1 and 2) measure the database’s ability to handle large\ndatasets, focusing on the number of inserted vectors usingSIFT and GIST datasets. Search performance cases (Cases 3)\nevaluate index building time, recall, latency, and maximum\nQPS using Google C4 dataset dataset.\nThe VDB versions involved in the performance tests are as\nfollows: Milvus-2c8g-hnsw-v2.2.12 (hereafter referred to as\nMilvus), Pinecone-p1.x1 (hereafter referred to as Pinecone),\nWeaviateCloud-standard (hereafter referred to as Weaviate\nCloud), ZillizCloud-2cu-cap-v2023.6 (hereafter referred to\nas ZillizCloud), QdrantCloud-2c8g-1node (hereafter referred\nto as QdrantCloud), PgVector-2c8g (hereafter referred to as\nPgVector), and ElasticCloud-upTo2.5c8g (hereafter referred to\nas ElasticCloud). To ensure minimal differences in hardware\nperformance across the tested databases, a configuration of\n2 CPUs and 8GB of memory was specifically selected. For\nVDBs that do not meet this hardware requirement, similar\nconfigurations were chosen as closely as possible. The specific\ntest results are shown in the figure7. The overall ranking\nin the figure 7 is calculated by averaging the rankings of\neach sub-test item, with the final overall ranking determined\nin ascending order of the average values. According to the\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 18\nFig. 7. Performance Test Results for Vector Databases\noverall ranking, Milvus ranks first with its comprehensive\nperformance, achieving a QPS of 380, latency of 12.4 mil-\nliseconds, and outstanding performance in load capacity. In\ncontrast, ElasticCloud ranks last, with a QPS of 11.29 and\nlatency as high as 361 milliseconds. In terms of recall rate,\nall databases perform close to 1.0, indicating little difference\nin query accuracy. ZillizCloud demonstrates the best latency\nperformance, at only 6 milliseconds, but its load capacity is\nrelatively low. It is worth noting that ZillizCloud, ElasticCloud,\nand PgVector were assigned penalty values (P) due to timeouts\nin the load capacity tests, which may have been caused by\nnetwork issues and should not be taken as a definitive measure\nof their actual performance. Overall, no single database ranked\nin the top three across all tests, indicating that different\ndatabases may have their own strengths. Additionally, the\noverall rank is simply a straightforward average of the rankings\nin each test item, without applying weighted averages based\non specific tasks, so it should be regarded as a preliminary\nreference.\nV. C HALLENGES\nA. Index Construction and Searching of High-Dimensional\nVectors\nVDBs require efficient indexing and searching of billions\nof vectors in hundred or thousand dimensions, which poses a\nhuge computational and storage challenge. Traditional index-\ning methods, such as B-trees or hash tables, are not suitable\nfor high-dimensional vectors because they suffer from dimen-\nsionality catastrophe. Therefore, VDBs need to use specialized\ntechniques such as ANN search, hashing, quantization, or\ngraph-based search to reduce complexity and improve the\naccuracy of vector similarity search.\nB. Support for Heterogeneous Vector Data Types\nVDBs need to support different types of vector data, such\nas dense vectors, sparse vectors, binary vectors, and so on.\nEach type of vector data may have different characteristicsand requirements, such as dimensionality, sparsity, distribu-\ntion, similarity metrics, and so on. Therefore, VDBs need\nto provide a flexible and adaptive indexing system to handle\nvarious vector data types and optimize their performance and\navailability.\nC. Distributed Parallel Processing Support\nVDBs need to be scalable to handle large-scale vector data\nand queries that may exceed the capacity of a single machine.\nTherefore, VDBs need to support distributed parallel process-\ning of vector data and queries across multiple computers or\nclusters. This involves challenges such as data partitioning,\nload balancing, fault tolerance, and consistency.\nD. Integration with Mainstream Machine Learning Frame-\nworks\nVDBs need to be integrated with popular machine learn-\ning frameworks such as TensorFlow, PyTorch, Scikit-learn,\netc., which are used to generate and use vector embeddings.\nAs a result, VDBs need to provide easy-to-use application\nprogramming interfaces (API) and encapsulated connectors\nto seamlessly interact with these frameworks and support a\nvariety of data formats and models.\nE. Integration with Emerging Application Scenarios\nCurrently, many emerging application scenarios remain un-\nderexplored. For instance, the incremental k-NN search [102]\nadopted by recommendation and e-commerce platforms faces\nsignificant challenges due to the vast imbalance between the\nvolume of vector data processed and the data displayed to\nusers. This method cannot be effectively supported by most\nVDBs [20]. Furthermore, with the latest advancements in\nsparse vector technology, integrating these technologies into\nVDBs to enable hybrid retrieval (combining keyword and\nvector retrieval methods) is increasingly regarded as a best\npractice. Such hybrid systems must manage large datasets\nwhile enhancing computational efficiency and maintaining\nretrieval quality.\nF . Data Security and Privacy Protection\nAs cyber threats intensify and regulatory requirements be-\ncome more stringent, data security and privacy protection for\ndatabases have become top priorities [103]. Compared to the\ncomprehensive data security and privacy protection features\nof traditional relational databases, VDBs are still in the early\nstages. Unlike traditional relational databases, VDBs typically\nhandle large amounts of high-dimensional embedding vectors,\nwhich pose higher risks of privacy breaches during storage,\nquerying, and transmission. This is especially true in cloud\nplatforms, where data is often stored in an unencrypted form\nor transmitted between different nodes, increasing the potential\nattack surface [104], [105]. In the future, VDBs will need\nto not only leverage traditional data security and privacy\nprotection methods to build a robust security framework, but\nalso integrate emerging technologies, such as blockchain tables\n[106] (to ensure data immutability) and AI-based anomaly de-\ntection (for proactive threat management), to adapt to specific\napplication scenarios.\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 19\nVI. S YNERGY OF LLM S AND VDB S\nBy virtue of its excellent capability for rapid processing of\nunstructured data, a VDB can naturally meet the requirements\nof vector-intensive applications, especially for LLMs, where its\nrole is becoming increasingly crucial. When processing natural\nlanguage, LLMs need to convert text into high-dimensional\nvectors for computation and analysis, which demands robust\nstorage and retrieval capabilities for high-dimensional vectors.\nMeanwhile, alleviating issues such as hallucinations and for-\ngetfulness in LLMs also requires supporting facilities like vast\nexternal knowledge bases, all of which necessitate the support\nof VDBs. Therefore, the integration of LLMs and VDBs is\nset to be an inevitable trend in the future. Consequently, this\nsection will delve into the integration and mutual influence\nbetween VDBs and LLMs, and provide an outlook on rele-\nvant potential applications, with the aim of offering valuable\nreferences and insights for subsequent scientific research and\nindustrial applications. In the following sections, We will delve\ninto the synergistic interaction between VDBs and LLMs,\nexploring their concrete application prospects in depth.\nGiven these challenges, researchers and developers have\nbeen exploring innovative solutions to enhance the perfor-\nmance and reliability of LLMs. One promising approach is\nthe integration of VDBs into LLM systems.\nA. VDBs for LLMs\nLLMs are characterized by large model capacity and vast\ndata corpus [107]. With hundreds of billions (or more) of\nparameters and extensive textual training, they are highly adept\nat comprehending human knowledge and instructions [108].\nHowever, LLMs do have certain shortcomings, though [3].\nOne major shortcoming is hallucinations where the model\ngenerates a response that is factually inaccurate. This short-\ncoming is mainly caused by the following issues, including\nknowledge limitations confined by the training corpa, the\ninternal knowledge in LLMs cannot be updated resulting in\noutdated knowledge, LLMs may also introduce systematic\nerrors due to the large dataset used for training. Another\nshortcoming is oblivion problem. LLMs have been found\nhaving the inclination to forget the previous input information,\nand also exhibit catastrophic forgetting behavior. In response\nto these issues, VDBs can offer robust support for LLMs in\nthe following aspects:\n1) VDBs as an External Knowledge Base: Retrieval-\nAugmented Generation (RAG). The Retrieval-Augmented\nGeneration (RAG) technique is an artificial intelligence tech-\nnology that combines information retrieval technology with\nlanguage generation models [109]. This technique enhances\nthe capability of LLMs in handling knowledge-intensive tasks,\nsuch as question answering, text summarization, and content\ngeneration, by retrieving relevant information from an external\nknowledge base and inputting it as a prompt to the LLMs.\nTo address the above limitations, recent research introduced\nthe RAG technique. Retrieval models play an important role\nin various knowledge-intensive tasks by providing timely and\naccurate external knowledge through effective data mainte-\nnance in external databases. RAG models provide a promisingdirection for enhancing the performance and adaptability of\nLLMs in a variety of tasks by integrating retrieval with\ngeneration.\nThe RAG framework has become a paradigm and has\nbrought a huge shift to NLP [110]. RAG models consist of\nseveral major processes in the era of LLMs, including retrieval,\ngeneration, and augmentation. A common workflow of RAG\nwhen meeting LLMs is illustrated in figure8. The complete\noperational workflow of the system essentially consists of\nthree core components: data storage, information retrieval, and\ncontent generation.\nThe RAG workflow begins with the data storage phase.\nDuring this phase, externally collected unstructured data (text,\nimages, audio, video) undergoes preprocessing. The processed\ndata is then divided into smaller chunks, converted into vectors\nvia an embedding model to capture semantic representations,\nand stored in a VDB for subsequent vector retrieval.\nNext is the information retrieval phase. This stage starts\nwhen a user poses a question to the model in the form of\na prompt. The embedding model (used earlier for processing\nexternal unstructured data) generates an embedding vector for\nthe query, which is then used to retrieve the most semantically\nsimilar data chunks from the VDB. These retrieved results are\nconverted back from vector format to their original format and\nreturned to the user.\nFinally, in the content generation phase, the large language\nmodel (LLM) generates the final answer. The user’s original\nquestion and the retrieved information are integrated into a\ntask-specific prompt template (the selection of the prompt\ntemplate depends on the task type). The LLM then processes\nthis prompt and produces the answer.\nVDBs as a Cost-effective Semantic Cache. The running of\nLLMs consume huge resources. Its training requires massive\ncomputing power,and frequent API calls to third-party models\nracks up significant costs. With the help of VDBs, the inter-\naction cost and computing workload of LLMs can be reduced\nsignificantly, thereby promoting cost-effective and end-to-end\napplications of LLMs.\nBy integrating VDBs with LLMs, VDBs serve as GPT\nsemantic cache, which leverages semantic caching of query\nembeddings in in-memory storage. This method can efficiently\nidentify semantically similar questions by storing embeddings\nof user queries, allowing for the retrieval of pre-generated\nresponses without redundant API calls to the LLMs. This\ntechnique is an efficient way to reduce operational costs\n,improve response times and address inefficiency [111]. This\narchitecture mainly consists of three components, including an\nembedding generation that converts user queries into semantic\nembeddings, an in-memory caching that manages storage and\nretrieval of embeddings and responses and a similarity search\nthat identifies semantically similar queries. The combination\nof VDBs with LLMs offers several advantages. This technique\nreduces API dependency while maintaining high response\naccuracy. In addition, it demonstrates substantial scalability\nand adaptability. By using vector searching algorithms and in-\nmemory caching, it allows for the handling of large volumes\nof queries without a proportional increase in computational de-\nmands, supporting stable performance even under fluctuating\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 20\nImage & Text Audio & Video\nChunking\n2025 China EV sales leaders?\nRelevant Docs\n•2025 China Current\nmar\nket leaders\n•2025 Report of the\nChina Asso\nciation of\nAutomobileManufacturersVector Search\nRetrieve\nPrompt\nPlease answer the question using the given documents\nDocuments: “[ Relevant Docs ]”\nQuestion:  ”[ User Question]”\nResultEmbedding Model Vector Database\nUnstructured Data\nVectorized\nLarge Language ModelUser Question\nRank Brand Share\n1 BYD 35%\n2 Tesla 15%Based on RAG data, the top EV brands in China by 2025 are likely to be:\nDdata Storage Content GenerationInformation Retrieval\nFig. 8. A common workflow of RAG\nworkloads. It also supports for multiple embedding models\nand configurations, making the system adaptable to various\ndeployment needs [112]. Using VDBs as GPT semantic cache\nwill be a viable solution to facilitate the large-scale application\nof LLMs.\nVDBs as A Reliable Memory of LLMs. Memory systems\ncan power the intelligence LLMs, enabling them to demon-\nstrate the capability of autonomous and thus show impressive\nperformance in a wide range of tasks. The integration of mem-\nory systems and LLMs is conductive to the coherence, con-\ntextual, and efficiency of interactions and that the system can\nlearn and adapt over time. Currently, a significant drawback of\nLLMs is lacking strong long-term memory capabilities [113].\nThis limitation will hinder LLMs’ ability to maintain context\nover long periods of time and retrieve relevant information\nfrom past interactions. Therefore, in order to improve the\ndecision quality and reasoning efficiency of LLMs in complex\ntasks, it is necessary to research and develop effective long-\nterm memory mechanisms. Through the external knowledge\nstorage and historical interactions, long-term memory is avail-\nable for LLMs to store and retrieve and use in subsequent\ninteractions, which enhances LLMs’ intelligence ability in\nmaintaining contextual coherence, improving decision-making\nquality, reducing cost of reasoning and demonstrating higher\nintelligence in long-term interactions.\nThe VDBs can be used as the underlying basic tool of\nLLMs to support the storage of historical information, so that\nLLMs can effectively store different types of historical inter-\naction information, such as knowledge information, dialogue\ninformation, and related task information. Then different types\nof information during intelligent interaction are stored in the\nVDBs as long term memory after slicing and vectorizing,which facilitates the subsequent retrieval and updating of\nrelevant memory information. Another shortcoming of LLMs\nis that they cannot update knowledge dynamically, lacking of\nthe few-shot learning ability. VDBs provide a robust memory\nlayer for LLMs to update new information continuously in the\nway of storage, thus ensuring that LLMs can make response\naccording to the most current and relevant data.\nB. LLMs for VDBs\nIn addition, LLMs in turn can empower databases.AI\ntechnology has been proved to perform well in many data\nmanagement tasks, such as data processing, database opti-\nmization, and data analysis. However, traditional machine\nlearning algorithms are unable to solve generalization and\ninference problems. For example, traditional machine learning\nalgorithms have difficulty in adapting to different databases,\ndifferent query workloads, and different hardware environ-\nments, making them unable to solve the generalizability and\ninference problems in data management tasks. In addition,\ntraditional machine learning algorithms cannot satisfy the\nneed for contextual understanding and multi-step reasoning in\noptimization scenarios such as database diagnosis, root cause\nanalysis, etc. However, LLMs bring promising solutions to the\nabove problems [114].\nLLMs assist database management tasks. LLMs revo-\nlutionize data management. LLMs show great potential in\noptimizing data management problems due to its excellent\nlanguage comprehension and generalization capabilities in\ntasks such as data processing, database optimization, and data\nanalysis. For example, LLMs can analyze anomalous database\nmetrics and report root causes and potential solutions to data\nbase administrators. LLMs can also be used as a natural\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 21\nlanguage (NL) interface for data analysis tasks, converting\nNL requests into executable queries against their databases.\nLLMs bring several advantages to database management tasks.\nThe first point is higher transfer capability. Existing instance-\noptimal works can optimize an instance but cannot be extended\nto other instances, whereas, the combination of LLMs with\ndatabases demonstrates exceptional transfer capability. With\na few fine-tuned examples, comparable performance can be\nachieved on novel database tasks, making them more adaptable\nto database schema, workload, or even data and hardware\nchanges. The second point is the ability to provide a user-\nfriendly interface. LLMs allow users to provide some prompts\nas hints to guide the model’s inference, thus offering an\nintuitive interface. The second is the ability to provide a\nuser-friendly interface. LLMs allow users to provide some\nprompts as hints to guide the model’s inference, thus offering\nan intuitive user experience without the need for large amounts\nof training data (supervised modelling) or multiple iterations\n(reinforcement learning) to capture and merge user feedback.\nThe third point is to learn from prior knowledge. LLMs can\nextract insights from existing database components, including\ndocuments and even code. By integrating the strengths of these\ncomponents, databases’ performance can be enhanced while\nmitigating the individual weaknesses of the components [115].\nLLMs smarten vector data handling . The deep integration\nof LLMs with VDBs has pioneered innovative application\nscenarios for data-driven workflows, encompassing content\ngeneration, knowledge enhancement, and system optimiza-\ntion. By combining semantic understanding with vectorized\nretrieval, LLMs can generate customized texts (e.g., thematic\narticles, stylized summaries) based on vector inputs, enrich\nambiguous texts with additional details (e.g., supplementing\nstatistical data or case studies), and facilitate cross-language,\ncross-domain text transformations (e.g., multilingual simplifi-\ncation of legal documents). Furthermore, LLMs significantly\noptimize the management tasks of VDBs: they recommend\nconfiguration parameters by analyzing historical performance\ndata to improve system stability, automatically diagnose per-\nformance bottlenecks while generating interpretable reports,\nand efficiently process heterogeneous data through semantic\nanalysis (e.g., schema matching and error correction). These\napplications not only reduce the cost of manual intervention\nbut also extend the generalization capabilities of traditional\nmethods through adaptive solutions, highlighting the core\nvalue of LLMs in enhancing the intelligence and scalability\nof VDBs [116]–[122].\nC. A General LLMs and VDBs Synergized Framework\nFor a framework that incorporates a large language model\nand a VDB, as shown in figure8 it can be understood by\nsplitting it into four levels: the user level, the model level, the\nAI database level, and the data level, respectively. For a user\nwho has never been exposed to large language modeling, it is\npossible to enter natural language to describe their problem.\nFor a user who is proficient in large language modeling, a\nwell-designed prompt can be entered. The LLM next processes\nthe problem to extract the key- words in it, or in the case of\nUser Level\nModel Level\nData Level\nAI Database Level\nApplications of quantum \ncomputing in drug discoveryUser Intent\nPrompt:\n[You are an expert in quantum computing and \nbiomedicine. Please accomplish the following \nobjectives :.]\n[Objective 1 :Analyze the efficiency improvements \nof quantum computing algorithms (e.g., VQE, QAOA) in molecular dynamics simulations .\nObjective 2 :Compare with classical methods ]\nJournal Article\n Patent Databases\n Code RepositoryKey Term Extraction\nModel -extracted Keywords:  \n[VQE, QAOA, Molecular Dynamics \nSimulation, Comparison]\nVector Embedding\n 0.4\n 0.5\n 0.7\n 0.9\n 0.1\n0.4\n 0.5\n 0.7\n 0.9\n 0.1\n0.2\n 0.7\n 0.7\n 0.8\n 0.3\nVector Database\nMulti -type\nUnstructured\nData\nOnline CourseMultidisciplinary FieldsEmbeddings\nandUnstructureddata\nSimilarity SearchDomain -specific model\nAnalysis of Quantum Computing Algorithms in Molecular Dynamics Simulations:\n[Objective 1: Variational \nQuantum Eigensolver (VQE )]\n[Objective 2: Comparison with \nClassical Methods: ]Fig. 9. A common workflow of Retrieval-Augmented Generation (RAG).\nopen source LLMs, the corre- sponding vector embeddings\ncan be obtained directly. The VDB stores unstructured data\nand their joint embeddings. The next step is to go to the\nVDB to find similar nearest neighbors. The ones obtained from\nthe sequences in the big language model are compared with\nthe vector encodings in the VDB, by means of the NNS or\nANNS algorithms. And different results are derived through a\npredefined serialization chain, which plays the role of a search\nengine. If it is not a generalized question, the results derived\nneed to be further put into the domain model, for example,\nimagine we are seeking an intelligent scientific assistant, which\ncan be put into the model of AI4S to get professional results.\nEventually it can be placed again into the LLM to get coherent\ngenerated results. For the data layer located at the bottom, one\ncan choose from a variety of file formats such as PDF, CSV ,\nMD, DOC, PNG, SQL, etc., and its sources can be journals,\nconferences, textbooks, and so on. Corresponding disciplines\ncan be art, science, engineering, business, medicine, law, and\netc.\nVII. C ONCLUSION\nIn this paper, we provide a comprehensive and up-to-date\nliterature review on VDBs, including the key algorithms,\nstorage, and retrieval methods. We also compare representative\nVDB systems, analyze their design trade-offs, and discuss\ntheir strengths, limitations, and typical use cases. Furthermore,\nwe identify key challenges and outline potential research\ndirections, including improved indexing and closer integration\nwith LLMs. We believe this survey offers a solid reference\nfor researchers and practitioners, and contributes to a clearer\nunderstanding of the current state and future direction of vector\ndatabases.\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 22\nREFERENCES\n[1] J. Cao, J. Fang, Z. Meng, and S. Liang, “Knowledge graph embed-\nding: A survey from the perspective of representation spaces,” ACM\nComputing Surveys , vol. 56, no. 6, pp. 1–42, 2024.\n[2] S. Pouyanfar, Y . Yang, S.-C. Chen, M.-L. Shyu, and S. S. Iyengar,\n“Multimedia big data analytics: A survey,” ACM Comput. Surv. , vol. 51,\nno. 1, Jan. 2018. [Online]. Available: https://doi.org/10.1145/3150226\n[3] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min,\nB. Zhang, J. Zhang, Z. Dong et al. , “A survey of large language\nmodels,” arXiv preprint arXiv:2303.18223 , 2023.\n[4] A. M. N. Allam and M. H. Haggag, “The question answering systems:\nA survey,” International Journal of Research and Reviews in Informa-\ntion Sciences (IJRRIS) , vol. 2, no. 3, 2012.\n[5] G. M. Biancofiore, Y . Deldjoo, T. D. Noia, E. Di Sciascio, and F. Nar-\nducci, “Interactive question answering systems: Literature review,”\nACM Computing Surveys , vol. 56, no. 9, pp. 1–38, 2024.\n[6] Y . Zhang, J. Wu, and J. Cai, “Compact representation of high-\ndimensional feature vectors for large-scale image recognition and\nretrieval,” IEEE Transactions on Image Processing , vol. 25, no. 5, pp.\n2407–2419, 2016.\n[7] Z. Zhao, W. Fan, J. Li, Y . Liu, X. Mei, Y . Wang, Z. Wen, F. Wang,\nX. Zhao, J. Tang et al. , “Recommender systems in the era of large\nlanguage models (llms),” IEEE Transactions on Knowledge and Data\nEngineering , 2024.\n[8] Q. Liu, J. Hu, Y . Xiao, X. Zhao, J. Gao, W. Wang, Q. Li, and J. Tang,\n“Multimodal recommender systems: A survey,” ACM Computing Sur-\nveys, vol. 57, no. 2, pp. 1–17, 2024.\n[9] G. Touya and I. Lokhat, “Deep learning for enrichment of vector\nspatial databases: Application to highway interchange,” ACM Trans.\nSpatial Algorithms Syst. , vol. 6, no. 3, Apr. 2020. [Online]. Available:\nhttps://doi.org/10.1145/3382080\n[10] M. Wang, L. Lv, X. Xu, Y . Wang, Q. Yue, and J. Ni, “An efficient\nand robust framework for approximate nearest neighbor search with\nattribute constraint,” Advances in Neural Information Processing Sys-\ntems, vol. 36, 2024.\n[11] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 international\nconference on management of data , 2018, pp. 489–504.\n[12] X. Xie, H. Liu, W. Hou, and H. Huang, “A brief survey of vector\ndatabases,” in 2023 9th International Conference on Big Data and\nInformation Analytics (BigDIA) . IEEE, 2023, pp. 364–371.\n[13] X. Zhao, Y . Tian, K. Huang, B. Zheng, and X. Zhou, “Towards efficient\nindex construction and approximate nearest neighbor search in high-\ndimensional spaces,” Proceedings of the VLDB Endowment , vol. 16,\nno. 8, pp. 1979–1991, 2023.\n[14] Z. Wang, P. Wang, T. Palpanas, and W. Wang, “Graph-and tree-\nbased indexes for high-dimensional vector similarity search: Analyses,\ncomparisons, and future directions.” IEEE Data Eng. Bull. , vol. 46,\nno. 3, pp. 3–21, 2023.\n[15] W. Li, Y . Zhang, Y . Sun, W. Wang, M. Li, W. Zhang, and\nX. Lin, “Approximate nearest neighbor search on high dimensional\ndata—experiments, analyses, and improvement,” IEEE Transactions on\nKnowledge and Data Engineering , vol. 32, no. 8, pp. 1475–1488, 2019.\n[16] V . Karthik, S. Khan, S. Singh, H. V . Simhadri, and J. Vedurada, “Bang:\nBillion-scale approximate nearest neighbor search using a single gpu,”\narXiv preprint arxiv:2401.11324 , 2024.\n[17] H. J ´egou, M. Douze, J. Johnson, L. Hosseini, and C. Deng, “Faiss:\nSimilarity search and clustering of dense vectors library,” Astrophysics\nSource Code Library , pp. ascl–2210, 2022.\n[18] J. Mohoney, A. Pacaci, S. R. Chowdhury, A. Mousavi, I. F. Ilyas,\nU. F. Minhas, J. Pound, and T. Rekatsinas, “High-throughput vector\nsimilarity search in knowledge graphs,” Proceedings of the ACM on\nManagement of Data , vol. 1, no. 2, pp. 1–25, 2023.\n[19] J. J. Pan, J. Wang, and G. Li, “Vector database management techniques\nand systems,” in Companion of the 2024 International Conference on\nManagement of Data , 2024, pp. 597–604.\n[20] ——, “Survey of vector database management systems,” The VLDB\nJournal , vol. 33, no. 5, pp. 1591–1615, 2024.\n[21] T. R. Rao, P. Mitra, R. Bhatt, and A. Goswami, “The big data\nsystem, components, tools, and technologies: a survey,” Knowledge\nand Information Systems , vol. 60, pp. 1165–1245, 2019.\n[22] M. Wang, W. Xu, X. Yi, S. Wu, Z. Peng, X. Ke, Y . Gao, X. Xu,\nR. Guo, and C. Xie, “Starling: An i/o-efficient disk-resident graph\nindex framework for high-dimensional vector similarity search on data\nsegment,” Proceedings of the ACM on Management of Data , vol. 2,\nno. 1, pp. 1–27, 2024.[23] Y . Su, Y . Sun, M. Zhang, and J. Wang, “Vexless: A serverless vector\ndata management system using cloud functions,” Proceedings of the\nACM on Management of Data , vol. 2, no. 3, pp. 1–26, 2024.\n[24] T. Taipalus, “Vector database management systems: Fundamental con-\ncepts, use-cases, and current challenges,” Cognitive Systems Research ,\nvol. 85, p. 101216, 2024.\n[25] S. Joshi, “Introduction to vector databases for generative ai: Ap-\nplications, performance, future projections, and cost considerations,”\nInternational Advanced Research Journal in Science, Engineering and\nTechnology ISSN (O) , pp. 2393–8021.\n[26] V . Beecher. (2021) Oracle database using oracle sharding. [Online].\nAvailable: https://docs.oracle.com/en/database/oracle/oracle-database/\n18/shard/book-index.html\n[27] C. H. Costa, P. Maia, F. Carlos et al. , “Sharding by hash partitioning,”\ninProceedings of the 17th International Conference on Enterprise\nInformation Systems , vol. 1, 2015, pp. 313–320.\n[28] P. Done, Practical MongoDB Aggregations . GitHub,\n2023, accessed: 2024-12-01. [Online]. Available: www.\npractical-mongodb-aggregations.com\n[29] V . Mirrokni, M. Thorup, and M. Zadimoghaddam, “Consistent hashing\nwith bounded loads,” in Proceedings of the Twenty-Ninth Annual ACM-\nSIAM Symposium on Discrete Algorithms . SIAM, 2018, pp. 587–604.\n[30] D. Karger, A. Sherman, A. Berkheimer, B. Bogstad, R. Dhanidina,\nK. Iwamoto, B. Kim, L. Matkins, and Y . Yerushalmi, “Web caching\nwith consistent hashing,” Computer Networks , vol. 31, no. 11-16, pp.\n1203–1213, 1999.\n[31] D. Karger, E. Lehman, T. Leighton, R. Panigrahy, M. Levine, and\nD. Lewin, “Consistent hashing and random trees: Distributed caching\nprotocols for relieving hot spots on the world wide web,” in Pro-\nceedings of the twenty-ninth annual ACM symposium on Theory of\ncomputing , 1997, pp. 654–663.\n[32] R. Taft, I. Sharif, A. Matei, N. VanBenschoten, J. Lewis, T. Grieger,\nK. Niemi, A. Woods, A. Birzin, R. Poss et al. , “Cockroachdb: The\nresilient geo-distributed sql database,” in Proceedings of the 2020 ACM\nSIGMOD international conference on management of data , 2020, pp.\n1493–1509.\n[33] Y . Zhang, R. Power, S. Zhou, Y . Sovran, M. K. Aguilera, and J. Li,\n“Transaction chains: achieving serializability with low latency in geo-\ndistributed storage systems,” in Proceedings of the Twenty-Fourth ACM\nSymposium on Operating Systems Principles , 2013, pp. 276–291.\n[34] D. DeWitt and J. Gray, “Parallel database systems: The future of high\nperformance database systems,” Communications of the ACM , vol. 35,\nno. 6, pp. 85–98, 1992.\n[35] D. J. DeWitt and S. Ghandeharizadeh, “Hybrid-range partitioning strat-\negy: A new declustering strategy for multiprocessor database machine,”\ninProc. 16th international Conference on VLDB , 1990, pp. 481–492.\n[36] L. Hobbs, S. Hillson, S. Lawande, and P. Smith, Oracle 10g data\nwarehousing . Elsevier, 2011.\n[37] D. Hotka, Oracle9i Development by Example . Que Publishing, 2002.\n[38] http://weaviate.io.\n[39] http://vespa.ai.\n[40] R. Guo, X. Luan, L. Xiang, X. Yan, X. Yi, J. Luo, Q. Cheng, W. Xu,\nJ. Luo, F. Liu et al. , “Manu: a cloud native vector database management\nsystem,” arXiv preprint arXiv:2206.13843 , 2022.\n[41] D. Grund and J. Reineke, “Abstract interpretation of fifo replacement,”\ninInternational Static Analysis Symposium . Springer, 2009, pp. 120–\n136.\n[42] ——, “Precise and efficient fifo-replacement analysis based on static\nphase detection,” in 2010 22nd Euromicro Conference on Real-Time\nSystems . IEEE, 2010, pp. 155–164.\n[43] R. L. Mattson, J. Gecsei, D. R. Slutz, and I. L. Traiger, “Evaluation\ntechniques for storage hierarchies,” IBM Systems journal , vol. 9, no. 2,\npp. 78–117, 1970.\n[44] X. Gu and C. Ding, “On the theory and potential of lru-mru collab-\norative cache management,” ACM SIGPLAN Notices , vol. 46, no. 11,\npp. 43–54, 2011.\n[45] D. Lee, J. Choi, J.-H. Kim, S. H. Noh, S. L. Min, Y . Cho, and C. S.\nKim, “On the existence of a spectrum of policies that subsumes the\nleast recently used (lru) and least frequently used (lfu) policies,” in\nProceedings of the 1999 ACM SIGMETRICS international conference\non Measurement and modeling of computer systems , 1999, pp. 134–\n143.\n[46] S. Podlipnig and L. B ¨osz¨ormenyi, “A survey of web cache replacement\nstrategies,” ACM Computing Surveys (CSUR) , vol. 35, no. 4, pp. 374–\n398, 2003.\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 23\n[47] S. Mittal, “A survey of techniques for cache partitioning in multicore\nprocessors,” ACM Computing Surveys (CSUR) , vol. 50, no. 2, pp. 1–39,\n2017.\n[48] D. Ongaro and J. Ousterhout, “In search of an understandable consen-\nsus algorithm,” in 2014 USENIX annual technical conference (USENIX\nATC 14) , 2014, pp. 305–319.\n[49] H. Garcia-Molina and D. Barbara, “How to assign votes in a distributed\nsystem,” Journal of the ACM (JACM) , vol. 32, no. 4, pp. 841–860,\n1985.\n[50] J. Garmany and R. G. Freeman, Oracle Replication: Snapshot, Multi-\nmaster and Materialized Views Scripts . Rampant TechPress, 2003.\n[51] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman,\nA. Pilchin, S. Sivasubramanian, P. V osshall, and W. V ogels, “Dynamo:\nAmazon’s highly available key-value store,” ACM SIGOPS operating\nsystems review , vol. 41, no. 6, pp. 205–220, 2007.\n[52] P. Bailis, S. Venkataraman, M. J. Franklin, J. M. Hellerstein, and I. Sto-\nica, “Quantifying eventual consistency with pbs,” Communications of\nthe ACM , vol. 57, no. 8, pp. 93–102, 2014.\n[53] J. Gao and C. Long, “High-dimensional approximate nearest neighbor\nsearch: with reliable and efficient distance comparison operations,”\nProceedings of the ACM on Management of Data , vol. 1, no. 2, pp.\n1–27, 2023.\n[54] M. D. Manohar, Z. Shen, G. Blelloch, L. Dhulipala, Y . Gu, H. V .\nSimhadri, and Y . Sun, “Parlayann: Scalable and deterministic parallel\ngraph-based approximate nearest neighbor search algorithms,” in Pro-\nceedings of the 29th ACM SIGPLAN Annual Symposium on Principles\nand Practice of Parallel Programming , 2024, pp. 270–285.\n[55] C. Fu, C. Wang, and D. Cai, “High dimensional similarity search\nwith satellite system graph: Efficiency, scalability, and unindexed query\ncompatibility,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence , vol. 44, no. 8, pp. 4139–4150, 2021.\n[56] Z. Wang, Q. Wang, P. Wang, T. Palpanas, and W. Wang, “Dumpy:\nA compact and adaptive index for large data series collections,”\nProceedings of the ACM on Management of Data , vol. 1, no. 1, pp.\n1–27, 2023.\n[57] J. L. Bentley, “Multidimensional binary search trees used for asso-\nciative searching,” Communications of the ACM , vol. 18, no. 9, p.\n509–517, 1975.\n[58] B. Ghojogh, S. Sharifian, and H. Mohammadzade, “Tree-based opti-\nmization: A meta-algorithm for metaheuristic optimization,” 2018.\n[59] S. M. Omohundro, Five balltree construction algorithms . Berkeley:\nInternational Computer Science Institute, 1989.\n[60] T. Liu, A. W. Moore, A. Gray, and K. Yang, “New algorithms for\nefficient high-dimensional nonparametric classification,” Journal of\nmachine learning research , vol. 7, no. 6, 2006.\n[61] M. Dolatshah, A. Hadian, and B. Minaei-Bidgoli, “Ball*-tree: Efficient\nspatial indexing for constrained nearest-neighbor search in metric\nspaces,” Nov. 2015, arXiv:1511.00628 [cs]. [Online]. Available:\nhttp://arxiv.org/abs/1511.00628\n[62] A. Guttman, “R-trees: a dynamic index structure for spatial searching,”\ninProceedings of the 1984 ACM SIGMOD international conference on\nManagement of data , 1984, p. 47–57.\n[63] P. Ciaccia, M. Patella, and P. Zezula, “M-tree: An efficient access\nmethod for similarity search in metric spaces,” in Vldb , vol. 97, 1997,\np. 426–435.\n[64] D. Cai, “A revisit of hashing algorithms for approximate nearest neigh-\nbor search,” IEEE Transactions on Knowledge and Data Engineering ,\nvol. 33, no. 6, pp. 2337–2348, 2019.\n[65] M. Datar, N. Immorlica, P. Indyk, and V . S. Mirrokni, “Locality-\nsensitive hashing scheme based on p-stable distributions,” in Proceed-\nings of the twentieth annual symposium on Computational geometry ,\n2004, p. 253–262.\n[66] O. Jafari, P. Maurya, P. Nagarkar, K. M. Islam, and C. Crushev, “A\nsurvey on locality sensitive hashing algorithms and their applications,”\n2021.\n[67] A. Andoni, P. Indyk et al. , “Locality sensitive hashing (lsh) home page,”\nˆ1ˆ, 2023, accessed: 2023-10-18.\n[68] N. Dikkala, G. Kaplun, and R. Panigrahy, “For manifold learning, deep\nneural networks can be locality sensitive hash functions,” 2021.\n[69] K. Bob, D. Teschner, T. Kemmer, D. Gomez-Zepeda, S. Tenzer,\nB. Schmidt, and A. Hildebrandt, “Locality-sensitive hashing enables\nefficient and scalable signal classification in high-throughput mass\nspectrometry raw data,” BMC Bioinformatics , vol. 23, no. 1, p. 287,\n2022. [Online]. Available: ˆ1ˆ\n[70] A. Andoni and P. Indyk, “Near-optimal hashing algorithms for ap-\nproximate nearest neighbor in high dimensions,” Communications of\nthe ACM , vol. 51, no. 1, pp. 117–122, 2008.[71] A. Andoni, P. Indyk, H. L. Nguyen, and I. Razenshteyn,\n“Beyond Locality-Sensitive Hashing,” Oct. 2013, arXiv:1306.1547\n[cs]. [Online]. Available: http://arxiv.org/abs/1306.1547\n[72] A. Andoni and I. Razenshteyn, “Optimal Data-Dependent Hashing\nfor Approximate Near Neighbors,” Jul. 2015, arXiv:1501.01062 [cs].\n[Online]. Available: http://arxiv.org/abs/1501.01062\n[73] Y . Weiss, A. Torralba, and R. Fergus, “Spectral hashing,” Advances in\nneural information processing systems , vol. 21, 2008.\n[74] J.-P. Heo, Y . Lee, J. He, S.-F. Chang, and S.-E. Yoon, “Spherical\nhashing,” in 2012 IEEE conference on computer vision and pattern\nrecognition . IEEE, 2012, pp. 2957–2964.\n[75] H. Liu, R. Wang, S. Shan, and X. Chen, “Deep supervised hashing\nfor fast image retrieval,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2016, p. 2064–2072.\n[76] X. Luo, H. Wang, D. Wu, C. Chen, M. Deng, J. Huang, and X.-S. Hua,\n“A survey on deep hashing methods,” 2022.\n[77] ——, “A survey on deep hashing methods,” ACM Transactions on\nKnowledge Discovery from Data , vol. 17, no. 1, pp. 1–50, 2023.\n[78] “Remote Sensing |Free Full-Text |Deep Hashing Using Proxy\nLoss on Remote Sensing Image Retrieval.” [Online]. Available:\nhttps://www.mdpi.com/2072-4292/13/15/2924/htm\n[79] B. E and S. AB., “Annoy (approximate nearest neighbors oh yeah),”\n[DB/OL]. (2015) [2023-07-28]. 3, 2015.\n[80] B. J. S and L. D. G., “Shape indexing using approximate nearest-\nneighbour search in high-dimensional spaces,” in Proceedings of IEEE\ncomputer society conference on computer vision and pattern recogni-\ntion, 1997, pp. 1000–1006.\n[81] H. Liu, M. Deng, and C. Xiao, “An improved best bin first algorithm\nfor fast image registration,” in Proceedings of 2011 International\nConference on Electronic & Mechanical Engineering and Information\nTechnology , vol. 1. IEEE, 2011, pp. 355–358.\n[82] T. P, T. P, and S. M., “K-means tree: an optimal clustering tree for\nunsupervised learning,” The Journal of Supercomputing , vol. 77, pp.\n5239–5266, 2021.\n[83] J. Guare, “Six degrees of separation,” in The Contemporary Mono-\nlogue: Men . Routledge, 2016, pp. 89–93.\n[84] A. Ponomarenko, Y . Malkov, A. Logvinov, and V . Krylov, “Approxi-\nmate nearest neighbor search small world approach,” in International\nConference on Information and Communication Technologies & Ap-\nplications , vol. 17, 2011.\n[85] M. Y , P. A, L. A, and K. A., “Approximate nearest neighbor algorithm\nbased on navigable small world graphs,” Information Systems , vol. 45,\npp. 61–68, 2014.\n[86] ——, “Scalable distributed algorithm for approximate nearest neighbor\nsearch problem in high dimensional general metric spaces,” in Simi-\nlarity Search and Applications: 5th International Conference, SISAP\n2012, Toronto, ON, Canada, August 9-10, 2012. Proceedings 5 , 2012,\npp. 132–147.\n[87] Y . A. Malkov and D. A. Yashunin, “Efficient and robust approxi-\nmate nearest neighbor search using hierarchical navigable small world\ngraphs,” IEEE transactions on pattern analysis and machine intelli-\ngence , vol. 42, no. 4, p. 824–836, 2018.\n[88] M. van Baalen, A. Kuzmin, M. Nagel, P. Couperus, C. Bas-\ntoul, E. Mahurin, T. Blankevoort, and P. Whatmough, “Gptvq: The\nblessing of dimensionality for llm quantization,” arXiv preprint\narXiv:2402.15319 , 2024.\n[89] Y . Liu, J. Wen, Y . Wang, S. Ye, L. L. Zhang, T. Cao, C. Li, and\nM. Yang, “Vptq: Extreme low-bit vector post-training quantization for\nlarge language models,” arXiv preprint arXiv:2409.17066 , 2024.\n[90] D.-K. Le Tan, H. Le, T. Hoang, T.-T. Do, and N.-M. Cheung, “Deepvq:\nA deep network architecture for vector quantization,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition\nWorkshops , 2018, pp. 2579–2582.\n[91] H. Jegou, M. Douze, and C. Schmid, “Product quantization for nearest\nneighbor search,” IEEE transactions on pattern analysis and machine\nintelligence , vol. 33, no. 1, p. 117–128, 2010.\n[92] Y . Matsui, Y . Uchida, H. J ´egou, and S. Satoh, “A survey of product\nquantization,” ITE Transactions on Media Technology and Applica-\ntions , vol. 6, no. 1, pp. 2–10, 2018.\n[93] T. Ge, K. He, Q. Ke, and J. Sun, “Optimized product quantiza-\ntion,” IEEE transactions on pattern analysis and machine intelligence ,\nvol. 36, no. 4, pp. 744–755, 2013.\n[94] L. Li and Q. Hu, “Optimized high order product quantization for\napproximate nearest neighbors search,” Frontiers of Computer Science ,\nvol. 14, no. 2, pp. 259–272, 2020. [Online]. Available: ˆ1ˆ\n\nJOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 24\n[95] “Online Product Quantization |IEEE Transactions on Knowledge\nand Data Engineering.” [Online]. Available: https://dl.acm.org/doi/10.\n1109/TKDE.2018.2817526\n[96] R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and\nS. Kumar, “Accelerating large-scale inference with anisotropic vec-\ntor quantization,” in International Conference on Machine Learning .\nPMLR, 2020, pp. 3887–3896.\n[97] J. Wang, X. Yi, R. Guo, H. Jin, P. Xu, S. Li, X. Wang, X. Guo,\nC. Li, X. Xu et al. , “Milvus: A purpose-built vector data management\nsystem,” in Proceedings of the 2021 International Conference on\nManagement of Data , 2021, pp. 2614–2627.\n[98] Y . Wang, Z. Pan, and R. Li, “A new cell-level search based non-\nexhaustive approximate nearest neighbor (ann) search algorithm in the\nframework of product quantization,” IEEE Access , vol. 7, pp. 37 059–\n37 070, 2019.\n[99] Y . Liu, Z. Pan, L. Wang, and Y . Wang, “A new fast inverted file-based\nalgorithm for approximate nearest neighbor search without accuracy\nreduction,” Information Sciences , vol. 608, pp. 613–629, 2022.\n[100] D. Xu, I. W. Tsang, Y . Zhang, and J. Yang, “Online product quantiza-\ntion,” IEEE Transactions on Knowledge and Data Engineering , vol. 30,\nno. 11, p. 2185–2198, 2018.\n[101] P. Sun, D. Simcha, D. Dopson, R. Guo, and S. Kumar, “Soar: improved\nindexing for approximate nearest neighbor search,” Advances in Neural\nInformation Processing Systems , vol. 36, pp. 3189–3204, 2023.\n[102] G. Zhao, K. Xuan, D. Taniar, and B. Srinivasan, “Incremental k-\nnearest-neighbor search on road networks,” Journal of Interconnection\nNetworks , vol. 9, no. 04, pp. 455–470, 2008.\n[103] O. A. Farayola, O. L. Olorunfemi, and P. O. Shoetan, “Data privacy\nand security in it: a review of techniques and challenges,” Computer\nScience & IT Research Journal , vol. 5, no. 3, pp. 606–615, 2024.\n[104] R. R. Asaad and S. R. Zeebaree, “Enhancing security and privacy\nin distributed cloud environments: A review of protocols and mech-\nanisms,” Academic Journal of Nawroz University , vol. 13, no. 1, pp.\n476–488, 2024.\n[105] A. Amaithi Rajan and V . V , “Systematic survey: secure and privacy-\npreserving big data analytics in cloud,” Journal of Computer Informa-\ntion Systems , vol. 64, no. 1, pp. 136–156, 2024.\n[106] D. C. G. Valadares, A. Perkusich, A. F. Martins, M. B. M. Kamel,\nand C. Seline, “Privacy-preserving blockchain technologies,” Sensors ,\nvol. 23, no. 16, 2023. [Online]. Available: https://www.mdpi.com/\n1424-8220/23/16/7172\n[107] J. A. OpenAI, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L.\nAleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al. ,\n“Gpt-4 technical report, 2024,” URL https://arxiv. org/abs/2303.08774 ,\nvol. 2, p. 6, 2024.\n[108] M. Shanahan, “Talking about large language models,” 2023.\n[109] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval-\naugmented generation for knowledge-intensive nlp tasks,” Advances in\nneural information processing systems , vol. 33, pp. 9459–9474, 2020.\n[110] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. K ¨uttler, M. Lewis, W. tau Yih, T. Rockt ¨aschel, S. Riedel, and\nD. Kiela, “Retrieval-augmented generation for knowledge-intensive\nnlp tasks,” 2021. [Online]. Available: https://arxiv.org/abs/2005.11401\n[111] S. Regmi and C. P. Pun, “Gpt semantic cache: Reducing llm\ncosts and latency via semantic embedding caching,” arXiv preprint\narXiv:2411.05276 , 2024.\n[112] F. Bang, “Gptcache: An open-source semantic cache for llm applica-\ntions enabling faster answers and cost savings,” in Proceedings of the\n3rd Workshop for Natural Language Processing Open Source Software\n(NLP-OSS 2023) , 2023, pp. 212–218.\n[113] K. Hatalis, D. Christou, J. Myers, S. Jones, K. Lambert, A. Amos-\nBinks, Z. Dannenhauer, and D. Dannenhauer, “Memory matters: The\nneed to improve long-term memory in llm-agents,” in Proceedings of\nthe AAAI Symposium Series , vol. 2, no. 1, 2023, pp. 277–280.\n[114] X. Zhou, Z. Sun, and G. Li, “Db-gpt: Large language model meets\ndatabase,” Data Science and Engineering , vol. 9, no. 1, pp. 102–111,\n2024.\n[115] G. Li, X. Zhou, and X. Zhao, “Llm for data management,” Proceedings\nof the VLDB Endowment , vol. 17, no. 12, pp. 4213–4216, 2024.\n[116] R. Tang, X. Han, X. Jiang, and X. Hu, “Does synthetic data generation\nof llms help clinical text mining?” 2023.\n[117] A. Albalak, Y . Elazar, S. M. Xie, S. Longpre, N. Lambert, X. Wang,\nN. Muennighoff, B. Hou, L. Pan, H. Jeong et al. , “A survey on\ndata selection for language models,” arXiv preprint arXiv:2402.16827 ,\n2024.[118] M. Fan, X. Han, J. Fan, C. Chai, N. Tang, G. Li, and X. Du, “Cost-\neffective in-context learning for entity resolution: A design space\nexploration,” in 2024 IEEE 40th International Conference on Data\nEngineering (ICDE) . IEEE, 2024, pp. 3696–3709.\n[119] X. Zhou, G. Li, Z. Sun, Z. Liu, W. Chen, J. Wu, J. Liu, R. Feng,\nand G. Zeng, “D-bot: Database diagnosis system using large language\nmodels,” arXiv preprint arXiv:2312.01454 , 2023.\n[120] X. Huang, H. Li, J. Zhang, X. Zhao, Z. Yao, Y . Li, Z. Yu, T. Zhang,\nH. Chen, and C. Li, “Llmtune: Accelerate database knob tuning with\nlarge language models,” arXiv preprint arXiv:2404.11581 , 2024.\n[121] S. Chang and E. Fosler-Lussier, “How to prompt llms for text-to-sql:\nA study in zero-shot, single-domain, and cross-domain settings,” 2023.\n[122] C. Whitehouse, M. Choudhury, and A. F. Aji, “Llm-powered data\naugmentation for enhanced crosslingual performance,” 2023.",
  "textLength": 137159
}