{
  "paperId": "71b676fdf3ee3eec9c11089a957e55749a8334d5",
  "title": "A Survey of Machine Learning for Computer Architecture and Systems",
  "pdfPath": "71b676fdf3ee3eec9c11089a957e55749a8334d5.pdf",
  "text": "1\nA Survey of Machine Learning for Computer Architecture\nand Systems\nNAN WU and YUAN XIE, University of California, Santa Barbara\nIt has been a long time that computer architecture and systems are optimized for efficient execution of machine\nlearning (ML) models. Now, it is time to reconsider the relationship between ML and systems, and let ML\ntransform the way that computer architecture and systems are designed. This embraces a twofold meaning:\nimprovement of designers‚Äô productivity, and completion of the virtuous cycle. In this paper, we present a\ncomprehensive review of the work that applies ML for computer architecture and system design. First, we\nperform a high-level taxonomy by considering the typical role that ML techniques take in architecture/system\ndesign, i.e., either for fast predictive modeling or as the design methodology. Then, we summarize the common\nproblems in computer architecture/system design that can be solved by ML techniques, and the typical ML\ntechniques employed to resolve each of them. In addition to emphasis on computer architecture in a narrow\nsense, we adopt the concept that data centers can be recognized as warehouse-scale computers; sketchy\ndiscussions are provided in adjacent computer systems, such as code generation and compiler; we also give\nattention to how ML techniques can aid and transform design automation. We further provide a future vision\nof opportunities and potential directions, and envision that applying ML for computer architecture and systems\nwould thrive in the community.\nCCS Concepts: ‚Ä¢Computing methodologies ‚ÜíMachine learning ;‚Ä¢Computer systems organization\n‚ÜíArchitectures ;‚Ä¢General and reference ‚ÜíSurveys and overviews .\nAdditional Key Words and Phrases: machine learning for computer architecture, machine learning for systems\nACM Reference Format:\nNan Wu and Yuan Xie. 2021. A Survey of Machine Learning for Computer Architecture and Systems. ACM\nComput. Surv. 1, 1, Article 1 (January 2021), 37 pages. https://doi.org/10.1145/3494523\n1 INTRODUCTION\nMachine learning (ML) has been doing wonders in many fields. As people are seeking better\nartificial intelligence (AI), there is a trend towards larger, more expressive, and more complex\nmodels. According to the data reported by OpenAI [ 21], from 1959 to 2012, the amount of compute\nused in the largest AI training runs doubles every two years; since 2012, deep learning starts taking\noff, and the required amount of compute has been increasing exponentially with a 3.4-month\ndoubling period. By comparison, Moore‚Äôs law [ 168], the principle that has powered the integrated-\ncircuit revolution since 1960s, doubles the transistor density every 18 months. While Moore‚Äôs law\nis approaching its end [ 228], more pressure is put on innovations of computer architecture and\nsystems, so as to keep up with the compute demand of AI applications.\nConventionally, computer architecture/system designs are made by human experts based on\nintuitions and heuristics, which requires expertise in both ML and architecture/system. Meanwhile,\nAuthors‚Äô address: Nan Wu, nanwu@ucsb.edu; Yuan Xie, yuanxie@ucsb.edu, University of California, Santa Barbara, Santa\nBarbara, California, 93106.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n¬©2021 Association for Computing Machinery.\n0360-0300/2021/1-ART1 $15.00\nhttps://doi.org/10.1145/3494523\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.arXiv:2102.07952v2  [cs.LG]  6 Nov 2021\n\n1:2 Wu and Xie.\nFig. 1. A comprehensive overview of applying ML for computer architecture and systems. Existing work\nroughly falls into two categories: ML for fast system modeling, and ML as design methodology.\nthese heuristic-based designs can not guarantee scalability and optimality, especially in the case of\nincreasingly complicated systems. As such, it seems natural to move towards more automated and\npowerful methodologies for computer architecture and system design, and the relationship between\nML and system design is being reconsidered. Over the past decade, architecture and systems are\noptimized to accelerate the execution and improve the performance of ML models. Recently, there\nhave been signs of emergence of applying ML for computer architecture and systems, which\nembraces a twofold meaning: 1‚óãthe reduction of burdens on human experts designing systems\nmanually, so as to improve designers‚Äô productivity, and 2‚óãthe close of the positive feedback loop,\ni.e., architecture/systems for ML and simultaneously ML for architecture/systems, formulating a\nvirtuous cycle to encourage improvements on both sides.\nExisting work related to applying ML for computer architecture and system design falls into\ntwo categories. 1‚óãML techniques are employed for fast and accurate system modeling , which\ninvolves performance metrics or some criteria of interest (e.g. power consumption, latency, through-\nput, etc.). During the process of designing systems, it is necessary to make fast and accurate\npredictions of system behaviors. Traditionally, system modeling is achieved through the forms of\ncycle-accurate or functional virtual platforms, and instruction set simulators (e.g. gem5 [ 18]). Even\nthough these methods provide accurate estimations, they bring expensive computation costs asso-\nciated with performance modeling, which limits the scalability to large-scale and complex systems;\nmeanwhile, the long simulation time often dominates design iteration, making it impossible to fully\nexplore the design space. By contrast, ML-based modeling and performance prediction are capable\nto balance simulation cost and prediction accuracy. 2‚óãML techniques are employed as a design\nmethodology to directly enhance architecture/system design . ML techniques are skilled at\nextracting features that might be implicit to human experts, making decisions without explicit\nprogramming, and improving themselves automatically with accumulated experience. Therefore,\napplying ML techniques as design tools can explore design space proactively and intelligently, and\nmanage resource through better understanding of the complicated and non-linear interactions\nbetween workloads and systems, making it possible to deliver truly optimal solutions.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:3\nTable 1. Machine learning techniques.\nRealm of ML Category Classical ML Deep Learning Counterpart [71]\nSupervised\nLearningClassification Logistic regression [80]\nCNN, RNN,\nGNN [245], etc.Working for bothSupport vector machines/regression [202]\nK-nearest neighbors [8]\nDecision tree, e.g., CART [141], MARS [62]\nANN [197]\nBayesian analysis [68]\nEnsemble learning [199], e.g., gradient boosting, random forest\nRegressionLinear regression with variants [204], e.g., lasso (L1 regularization),\nridge (L2 regularization), elastic-net (hybrid L1/L2 regularization)\nNon-linear regression [193]\nUnsupervised\nLearningClustering K-means clustering [88] Autoencoder,\nGAN, etc. Dimension reduction Principal component analysis (PCA) [237]\nReinforcement\nLearningValue-based Q-learning [214] DQN [167]\nPolicy-basedActor-critic [214] A3C [166], DDPG [132]\nPolicy gradient, e.g., REINFORCE [214] PPO [203]\nIn this paper, we present a comprehensive overview of applying ML for computer architecture and\nsystems. As depicted in Figure 1, we first perform a high-level taxonomy by considering the typical\nrole that ML techniques take in architecture/system design, i.e., either for fast predictive modeling\nor as the design methodology; then, we summarize the common problems in architecture/system\ndesign that can be solved by ML techniques, and the typical ML techniques employed to resolve\neach of them. In addition to emphasis on computer architecture in a narrow sense, we adopt the\nconcept that data centers can be recognized as warehouse-scale computers [ 15], and review studies\nassociated with data center management; we provide sketchy discussions on adjacent computer\nsystems, such as code generation and compiler; we also give attention to how ML techniques can\naid and transform design automation that involves both analog and digital circuits. At the end\nof the paper, we discuss challenges and future prospects of applying ML for architecture/system\ndesign, aiming to convey insights of design considerations.\n2 DIFFERENT ML TECHNIQUES\nThere are three general frameworks in ML: supervised learning, unsupervised learning and rein-\nforcement learning. These frameworks mainly differentiate on what data are sampled and how\nthese sample data are used to build learning models. Table 1 summarizes the commonly used ML\ntechniques for computer architecture and system designs. Sometimes, multiple learning models\nmay work well for one given problem, and the appropriate selection can be made based on available\nhardware resource and data, implementation overheads, performance targets, etc.\n2.1 Supervised Learning\nSupervised learning is the process of learning a set of rules able to map an input to an output based\non labeled datasets. These learned rules can be generalized to make predictions for unseen inputs.\nWe briefly introduce several prevalent techniques in supervised learning, as shown in Figure 2.\n‚Ä¢Regression is a process for estimating the relationships between a dependent variable and one or\nmore independent variables. The most common form is linear regression [ 204], and some other\nforms include different types of non-linear regression [ 193]. Regression techniques are primarily\nused for two purposes, prediction/forecasting, and inference of causal relationships.\n‚Ä¢Support vector machines (SVMs) [ 202] try to find the best hyperplanes to separate data classes\nby maximizing margins. One variant is support vector regression (SVR), which is able to conduct\nregression tasks. Predictions or classifications of new inputs can be decided by their relative\npositions to these hyperplanes.\n‚Ä¢Decision tree is one representative of logical learning methods, which uses tree structures to\nbuild regression or classification models. The final result is a tree with decision nodes and leaf\nnodes. Each decision node represents a feature and branches of this node represent possible\nvalues of the corresponding feature. Starting from the root node, input instances are classified\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:4 Wu and Xie.\nFig. 2. Examples of supervised learning: (a) regression, (b) SVM, (c) decision tree, (d) MLP, and (e) ensemble\nlearning.\nby sequentially passing through nodes and branches, until they reach leaf nodes that represent\neither classification results or numerical values.\n‚Ä¢Artificial neural networks (ANNs) [ 197] are capable to approximate a broad family of functions:\na single-layer perceptron is usually used for linear regression; complex DNNs [ 71] consisting of\nmultiple layers are able to approximate non-linear functions, such as the multi-layer perceptron\n(MLP); variants of DNNs that achieve excellent performance in specific fields benefit from the\nexploitation of certain computation operations, e.g., convolutional neural networks (CNNs) with\nconvolution operations leveraging spatial features, and recurrent neural networks (RNNs) with\nrecurrent connections enabling learning from sequences and histories.\n‚Ä¢Ensemble learning [ 199] employs multiple models that are strategically designed to solve a\nparticular problem, and the primary goal is to achieve better predictive performance than those\ncould be obtained from any of the constituent models alone. Several common types of ensembles\ninclude random forest and gradient boosting.\nDifferent learning models have different preference of input features: SVMs and ANNs generally\nperform much better with multi-dimension and continuous features, while logic-based systems\ntend to perform better when dealing with discrete/categorical features. In system design, supervised\nlearning is commonly used for performance modeling, configuration predictions, or predicting\nhigher-level features/behaviors from lower-level features. One thing worth noting is that supervised\nlearning techniques need well labeled training data prior to the training phase, which usually\nrequire tremendous human expertise and engineering.\n2.2 Unsupervised Learning\nUnsupervised learning is the process of finding previously unknown patterns based on unlabeled\ndatasets. Two prevailing methods are clustering analysis [ 88] and principal component analysis\n(PCA) [237], as depicted in Figure 3.\n‚Ä¢Clustering is a process of grouping data objects into disjoint clusters based on a measure of\nsimilarity, such that data objects in the same cluster are similar while data objects in different\nclusters share low similarities. The goal of clustering is to classify raw data reasonably and to\nfind possibly existing hidden structures or patterns in datasets. One of the most popular and\nsimple clustering algorithms is k-means clustering.\n‚Ä¢PCA is essentially a coordinate transformation leveraging information from data statistics. It\naims to reduce the dimensionality of the high-dimensional variable space by representing it with\na few orthogonal (linearly uncorrelated) variables that capture most of its variability.\nSince there is no label in unsupervised learning, it is difficult to simultaneously measure the\nperformance of learning models and decide when to stop the learning process. One potential\nworkaround is semi-supervised learning [273], which uses a small amount of labeled data together\nwith a large amount of unlabeled data. This approach stands between unsupervised and supervised\nlearning, requiring less human effort and producing higher accuracy. The unlabeled data are used\nto either finetune or re-prioritize hypotheses obtained from labeled data alone.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:5\nFig. 3. Examples of unsupervised learning: (a) clustering, and (b) PCA.\n Fig. 4. A typical framing of RL.\n2.3 Reinforcement Learning\nIn standard reinforcement learning (RL) [ 214], an agent interacts with an environment Eover a\nnumber of discrete time steps, as shown in Figure 4. At each time step ùë°, the agent receives a state\nùë†ùë°from the state spaceS, and selects an action ùëéùë°from the action spaceAaccording to its policy ùúã,\nwhere ùúãis a mapping from states ùë†ùë°to actions ùëéùë°. In return, the agent receives the next state ùë†ùë°+1\nand a scalar reward ùëüùë°:S√óA‚Üí R. This process continues until the agent reaches a terminal state\nafter which the process restarts. The return ùëÖùë°=‚àû√ç\nùëò=0ùõæùëòùëüùë°+ùëòis the totally accumulated rewards at\nthe time step ùë°with a discount factor ùõæ‚àà(0,1]. The goal of the agent is to maximize the expected\nreturn for each state ùë†.\nThe state-action value ùëÑùúã(ùë†, ùëé)=Eùúã[ùëÖùë°|ùë†ùë°=ùë†, ùëéùë°=ùëé]is the expected return of selecting\naction ùëéat state ùë†with policy ùúã. Similarly, the state value ùëâùúã(ùë†)=Eùúã[ùëÖùë°|ùë†ùë°=ùë†]is the expected\nreturn starting from state ùë†by following policy ùúã. There are two general types of methods in RL:\nvalue-based, and policy-based.\n‚Ä¢In value-based RL, the state-action value function ùëÑùúã(ùë†, ùëé)is approximated by either tabular\napproaches or function approximations. At each state ùë†ùë°, the agent always selects the optimal\naction ùëé‚àó\nùë°that could bring the maximal state-action value ùëÑùúã(ùë†ùë°, ùëé‚àó\nùë°). One well-known example of\nvalue-based methods is Q-learning.\n‚Ä¢In policy-based RL, it directly parameterizes the policy ùúã(ùëé|ùë†;ùúÉ)and updates the parameters ùúÉ\nby performing gradient ascent on E[ùëÖùë°]. One example is the REINFORCE algorithm.\nRL is modeled based on Markov decision process, and thus it is suitable to handle control\nproblems or sequential decision-making processes. With these characteristics, RL is able to explore\ndesign space proactively and intelligently, and learn how to achieve resource management or task\nscheduling in system designs through interactions with environments. The optimal behaviors can\nbe found by embedding optimization goals into reward functions.\n3 ML FOR FAST SYSTEM MODELING\nThis section reviews studies that employ ML techniques for fast and accurate system modeling,\nwhich involves predictions of performance metrics or some other criteria of interest. Although\ncycle-accurate simulators, which are commonly used for system performance prediction, can\nprovide accurate estimations, they usually run multiple orders of magnitude slower than native\nexecutions. By contrast, ML-based techniques can balance simulation costs and prediction accu-\nracy, showing great potentials in exploring huge configuration spaces and learning non-linear\nimpacts of configurations. Most of existing work applies supervised learning for either pure system\nmodeling or efficient design space exploration (DSE) enabled by fast predictions. Table 2 and\nTable 3 summarize the studies for predictive modeling in computer architecture/system and design\nautomation respectively, in terms of task domains, prediction targets, adopted ML techniques, and\ncorresponding inputs.\n3.1 Sub-system Modeling and Performance Prediction\n3.1.1 Memory System. In memory systems, ML-based performance models are exploited to help\nexplore trade-offs among different objectives. To explore non-volatile memory (NVM) based cache\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:6 Wu and Xie.\nTable 2. Summary of applying ML techniques for fast modeling in computer architecture and systems.\nDomain Prediction of Technique Input\nMemory\nSystem\n(¬ß3.1.1)Throughput/cache miss ANN [52] Cache configurations\nThroughput/lifetime/energy Gradient boosting/quadratic regression with lasso [48] NVM configurations\nThroughput CNN [133] Memory controller placements\nDisk block correlation DNN [42] Data blocks in the context window\nNoC\n(¬ß3.1.2)Latency SVR [185] Queue information\nHotspots ANN [212] Buffer utilization\nBuffer/link utilization Regression with ridge regularization [35, 224], decision tree [51] NoC configurations\nProbability of errors Decision tree [50] Link utilization and transistor wearing-out\nGPU\n(¬ß3.2.1)Speedup or execution time\nby cross-platform inputsNearest neighbor/SVM [14],\nensemble of regression-based learners [9],\nrandom forest [10]Static analysis and/or\ndynamic profiling of\nsource CPU code\nExecution timeStepwise regression [90],\nensemble of linear regression and random forest [175] GPU configurations\nand performance counters Throughput/power Ensemble of NNs [99]\nScaling behavior of GPGPU ANN and K-means clustering [239]\nKernel affinity and execution time Logistic regression and linear regression [181] Kernel characteristics\nTraffic patterns in GPGPU CNN [130] Grayscale heat maps\nSingle-Core\nCPU(¬ß3.2.2)Throughput Linear regression [100], non-linear regression [57, 101, 121] Micro-architectural parameters\nand performance counters Program execution time Linear regression[268, 269]\nGeneral\nModeling\n(¬ß3.2.3)Throughput/latency/powerANN [19, 83, 110, 123, 172, 177]\nMicro-architectural parameters\nand performance countersNon-linear regression [122, 124, 200, 244]\nLinear regression [12, 41, 140]\nHierarchical Bayesian model [163, 165]\nLSTM [157, 191]\nGenerative model [49]\nSlowdown caused by\napplication interferenceLinear regression with elastic-net\nregularization [164]\nSpeedup of multi-thread applications Gausian process regression [3] Profiling of single-thread execution\nData\nCenter\n(¬ß3.2.4)Job completion time SVR [253]Application characteristics\nand cluster configurations\nResource demand Statistical learning [70], linear regression/MLP [85]\nWorkload characterization Incoming workload ARMA [196], ARIMA [27]\nWorkload pattern Hidden Markov model [109]\nPower usage effectiveness MLP [65] Data center configurations\nDisk FailureBayesian methods [75], clustering [170],\nSVM/MLP [271], random forest [246]SMART (Self-Monitoring, Analysis\nand Reporting Technology) attributes\nof data centersHealth assessment of drives CART [126], gradient boosted regressions tree [127], RNN [248]\nPartial drive failureCART/random forest/SVM/ANN/logistic regression [146]\nGradient boosted regression trees [249] SMART attributes and system-level signals\nhierarchies, Dong et al. [52] develop an ANN model to predict higher-level features (e.g. miss of\ncache read/write, and instruction-per-cycle (IPC)) from lower-level features (e.g. cache associa-\ntivity, capacity and latency). To adaptively select architectural techniques in NVMs for different\napplications, Memory Cocktail Therapy [ 48] estimates lifetime, IPC, and energy consumption\nthrough lightweight online predictors by gradient boosting and quadratic regression with lasso.\nTo optimize memory controller placements in throughput processors, Lin et al. [133] build a CNN\nmodel that takes memory controller placements as inputs to predict throughput, which accelerates\nthe optimization process by two orders of magnitude.\nSome studies concentrate on learning efficient representations of memory access patterns.\nBlock2Vec [ 42] tries to mine data block correlations by training a DNN to learn the best vec-\ntor representation of each block and capturing block similarities via vector distances, which enables\nfurther optimization for caching and prefetching. Shi et al. [209] use a graph neural network (GNN)\nto learn fused representations of static code and its dynamic execution. This unified representation\nis capable to model both data flows (e.g., prefetching) and control flows (e.g., branch prediction).\n3.1.2 Network-on-Chip (NoC). In NoCs, several performance metrics of interest are latency, energy\nconsumption, and reliability. 1‚óãRegarding latency predictions, Qian et al. [185] use an SVR model\nto predict the traffic flow latency and the average channel waiting time in mesh-based NoCs,\nwhich relaxes some assumptions in the classical queuing theory. Rather than explicitly predicting\nlatency, a lightweight hardware-based ANN [ 212] predicts existence of traffic hotspots, which\nare intensive network congestions significantly degrading the effective throughput and implicitly\nindicate the average communication latency in NoCs. The input features are buffer utilization rates\nfrom neighboring NoC routers, and the trained predictor is combined with a proactive hotspot-\npreventive routing algorithm to avert hotspot formation, attaining significant improvements for\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:7\nsynthetic workloads while modest melioration for real-world benchmarks. 2‚óãRegarding estimating\nenergy consumption, the learned predictors are often leveraged for saving dynamic and/or static\nenergy in NoCs. DiTomaso et al. [51] use per-router decision trees to predict link utilization and\ntraffic direction, which are combined with sleepy link storage units to power-gate links/routers and\nto change link directions. Clark et al. [35] use ridge regression models to predict buffer utilization,\nchanges in buffer utilization, or a combined metric of energy and throughput, based on which a\nrouter can select proper voltage/frequency. In photonic NoCs, the ridge regression model is also\napplicable to predict the number of packets to be injected into each router in the following time\nwindow [ 224], based on which the number of wavelengths are scaled properly to reduce static\nenergy consumed by photonic links. 3‚óãRegarding the reliability of NoCs, a per-link decision tree\ntrained offline can predict the probability of timing faults on links during runtime [ 50], based on\nwhich a proactive fault-tolerant technique is developed to mitigate errors by using the strengthened\ncyclic redundancy check with error-correction code and relaxed transmission.\n3.2 System Modeling and Performance Prediction\nAccurate and fast performance estimation is a necessity for system optimization and design space\nexploration. With the increasing complexity of systems and variety of workloads, ML-based tech-\nniques can provide highly accurate performance estimations with reasonable simulation costs,\nsurpassing the capability of commonly-used cycle-accurate simulators that require highly compu-\ntational costs and long simulation time.\n3.2.1 Graphics Processing Unit (GPU). There are two types of predictions for GPU modeling: cross-\nplatform predictions and GPU-specific predictions. Cross-platform predictions are used to decide in\nadvance whether to offload an application from a CPU to a GPU, since not every application benefits\nfrom GPU execution and the porting process requires considerably additional efforts; GPU-specific\npredictions are used to estimate metrics of interest and to assist GPU design space exploration,\nhelpful to handle design space irregularities and complicated interactions among configurations.\nCross-platform predictions can be formulated as a binary classification problem that identifies\nwhether the potential GPU speedup of an application would be greater than a given threshold. This\ntask can be solved by the nearest neighbor and SVMs using dynamic instruction profiles [ 14], or a\nrandom forest that composes of one thousand decision trees using static analysis of source CPU\ncode (i.e., memory coalescing, branch divergence, kernel size available parallelism and instruction\nintensities) [ 10]. With both dynamic and static program properties from single-thread CPU code,\nan ensemble of one hundred regression-based learners can predict the GPU execution time [9].\nIn terms of GPU-specific predictions that take GPU configurations and performance counters as\ninput features, the execution time can be predicted by stepwise linear regression, which recognizes\nthe most important input features among many GPU parameters and thus achieves high accuracy\neven with sparse samples [ 90]; the power/throughput can be modeled by an ensemble of NN\npredictors [ 99]. Provided with profiling results from earlier-generation GPUs, an ensemble of\nlinear and non-linear regression models is capable to predict cross-generation GPU execution\ntime for later/future-generation GPUs, which achieves more than 10,000 times speedup compared\nto cycle-accurate GPU simulators [ 175]. Focusing on processing-in-memory (PIM) assisted GPU\narchitectures, Pattnaik et al. [181] classify GPU cores into two types: powerful GPU cores but\nfar away from memory, and auxiliary/simple GPU cores but close to memory. They develop a\nlogistic regression model that takes kernel characteristics as input features to predict architecture\naffinity of kernels, aiming to accurately identify which kernels would benefit from PIM and offload\nthem accordingly to auxiliary GPU cores. They also build a linear regression model to predict\nthe execution time of each kernel, so that a concurrent kernel management mechanism can be\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:8 Wu and Xie.\ndeveloped based on these two models and kernel dependency information. Focusing on general-\npurpose GPUs (GPGPUs), Wu et al. [239] model kernel scaling behaviors with respect to the number\nof compute units, engine frequency, and memory frequency. During training, kernels with similar\nperformance scaling behaviors are grouped by K-means clustering, and when encountering a new\nkernel, it is mapped to the cluster that best describes its scaling performance by an ANN-based\nclassifier. Li et al. [130] reassess prevailing assumptions of GPGPU traffic patterns, and combine a\nCNN with a t-distributed stochastic neighbor embedding to classify different traffic patterns.\n3.2.2 Single-Core Processor. In predictive performance modeling of single-core processors, early-\nstage work mostly targets superscalar processors. To predict the application-specific cycle-per-\ninstruction (CPI) of superscalar processors, Joseph et al. [100] introduce an iterative procedure to\nbuild linear regression models using 26 key micro-architectural parameters. Later they construct\npredictive models by non-linear regression techniques (i.e., radial basis function networks generated\nfrom regression trees) with 9 key micro-architectural parameters [ 101]. In parallel with Joseph‚Äôs\nwork, Lee and Brooks [ 121] use regression modeling with cubic splines to predict application-\nspecific performance (billions of instructions per second) and power.\nLater work focuses on performance modeling for existing hardware (e.g., Intel, AMD, and ARM\nprocessors) by using micro-architectural parameters and performance counters. Eyerman et al.\n[57] construct a mechanistic-empirical model for CPI predictions of three Intel processors. The\ninitially parameterized performance model is inspired by mechanistic modeling, where the unknown\nparameters inside the model are derived through regression, benefiting from both mechanistic\nmodeling (i.e., interpretability) and empirical modeling (i.e., ease of implementation). Zheng et al.\n[268,269] explore two approaches to cross-platform predictions of program execution time, where\nprofiling results on Intel Core i7 and AMD Phenom processors are used to estimate the execution\ntime on a target ARM processor. The first approach [ 269] relaxes the assumption of global linearity\nto local linearity in the feature space and applies constrained locally sparse linear regression; the\nother approach [268] applies lasso linear regression with phase-level performance features.\n3.2.3 General Modeling and Performance Prediction. Regression techniques are the mainstream to\npredict performance metrics from micro-architectural parameters or other features, which attributes\nto their capability to make high-accuracy estimations with reasonable training costs.\nFor conventional regression-based models, ANNs and non-linear regression with different designs\nare the common practice to predict throughput/latency [ 83,110,122,124,244] and power/energy\n[110,122]. Consequently, there are comparisons among different techniques. Lee et al. [123] compare\npiecewise polynomial regression with ANNs, with emphasis that piecewise polynomial regression\noffers better explainability while ANNs show better generalization ability. Ozisikyilmaz et al. [177]\ncontrast several linear regression models and different ANNs, indicating that the pruned ANNs\nachieve best accuracy while requiring longer training time. Agarwal et al. [3] estimate the parallel\nexecution speedup of multi-threaded applications on a target hardware platform, and mention that\nGaussian process regression performs the best among several explored methods in this case.\nMore recent work tends to take advantage of data-driven approaches. Ithemal [ 157] leverages a\nhierarchical multi-scale RNN with long short term memory (LSTM) to predict throughput of basic\nblocks (i.e., sequences of instructions with no branches or jumps), and evaluations demonstrate that\nIthemal is more accurate and as fast as analytical throughput estimators. By employing a variant of\nIthemal as a differentiable surrogate to approximate CPU simulators, DiffTune [ 191] is able to apply\ngradient-based optimization techniques to learn the parameters of x86 basic block CPU simulators\nsuch that simulators‚Äô error is minimized. The learned parameters finally are plugged back into\nthe original simulator. Ding et al. [49] provide some insights in learning-based modeling methods:\nthe improvement of prediction accuracy may receive diminishing returns; the consideration of\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:9\ndomain knowledge will be helpful for system optimizations, even if the overall accuracy may\nnot be improved. Thus, they propose a generative model to handle data scarcity by generating\nmore training data, and apply a multi-phase sampling to improve prediction accuracy of optimal\nconfiguration points.\nML-based predictive performance modeling enables efficient resource management and rapid de-\nsign space exploration to improve throughput. Equipped with ANNs for IPC predictions, strategies\nfor resource allocation [ 19] and task scheduling [ 172] can always select decisions that would bring\nthe best predicted IPC. ESP [ 164] constructs a regression model with elastic-net regularization to\npredict application interference (i.e., slowdown), which is integrated with schedulers to increase\nthroughput. MetaTune [198] is a meta-learning based cost model for convolution operations, and\nwhen combined with search algorithms, it enables efficient auto-tuning of parameters during com-\npilation. In consideration of rapid design space exploration of the uncore (i.e., memory hierarchies\nand NoCs), Sangaiah et al. [200] uses a regression-based model with restricted cubic splines to\nestimate CPI, reducing the exploration time by up to four orders of magnitude.\nML-based predictive performance modeling benefits adaptations between performance and power\nbudgets. Leveraging off-line multivariate linear regression to predict IPC and/or power of different\narchitecture configurations, Curtis-Maury et al. [41] maximize performance of OpenMP applications\nby dynamic concurrency throttling and dynamic voltage and frequency scaling (DVFS); Bailey et\nal.[12] apply hardware frequency-limiting techniques to select optimal hardware configurations\nunder given power constraints. To effectively apply DVFS towards various optimization goals, the\ndesigned strategy can adopt predictions for power consumption by a constrained-posynomial model\n[102] or job execution time by a linear regression model [ 140]. To conduct smart power management\nin a more general manner, LEO [ 165] employs hierarchical Bayesian models to predict performance\nand power, and when integrated for runtime energy optimization, it is capable to figure out the\nperformance-power Pareto frontier and select the configuration satisfying performance constraints\nwith minimized energy. CALOREE [ 163] further breaks up the power management task into two\nabstractions: a learner for performance modeling and an adaptive controller leveraging predictions\nfrom the learner. These abstractions enable both the learner to use multiple ML techniques and\nthe controller to maintain control-theoretic formal guarantees. Since no user-specified parameter\nexcept the goal is required, CALOREE is applicable even for non-experts.\n3.2.4 Data Center Performance Modeling and Prediction. Data centers have been in widespread\nuse for both traditional enterprise applications and cloud services. Many studies employ ML\ntechniques to predict workload/resource-related metrics, so as to enable elastic resource provision.\nCommon examples include but not limited to using SVR to predict job completion time [ 253],\nleveraging the autoregressive moving average (ARMA) model [ 196] or the autoregressive integrated\nmoving average (ARIMA) model [ 27] to forecast incoming workloads, exploiting the hidden Markov\nmodeling to characterize variations in workload patterns [109], and estimating dynamic resource\ndemand of workloads by light-weight statistical learning algorithms [ 70] or MLP [ 85]. Jim Gao [ 65]\nbuilds an MLP model to predict power usage effectiveness of data centers, which is extensively\ntested and validated at Google data centers. Cortez et al. [38] predict virtual machine (VM) behaviors\n(including VM lifetimes, maximum deployment sizes, and workload classes) for a broader set of\npurposes (e.g., health/resource management and power capping), where the evaluated ML models\nare random forests and extreme gradient boosting trees.\nIn addition to workload/resource-related metrics, the availability in data centers or cloud services\nis also a topic of concern, where one of the key tasks is to predict disk failure in advance. Leveraging\nSMART (Self-Monitoring, Analysis and Reporting Technology) attributes, the disk failure prediction\nmodel can be built via various ML techniques, such as different Bayesian methods [ 75], unsupervised\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:10 Wu and Xie.\nclustering [ 170], SVM and MLP [ 271]. The adoption of classification and regression trees (CART)\n[126], RNNs [ 248], or gradient boosted regression trees [ 127] makes it possible to assess health\nstatus of drives. While all the aforementioned methods rely on offline training, online random\nforests [ 246] can evolve with forthcoming data on-the-fly by generating new trees and forget old\ninformation by discarding outdated trees, consequently avoiding the model aging problem in disk\nfailure predictions. To predict partial drive failures (i.e., disk error or sector error), Mahdisoltani\net al. [146] explore five ML techniques (CART, random forests, SVM, NN and logistic regression),\namong which random forests consistently outperform others. Xu et al. [249] incorporate SMART\nattributes and system-level signals to train a gradient boosted regression tree, which is an online\nprediction model that ranks disks according to the degree of error-proneness in the near future.\n3.3 Performance Modeling in Chip Design and Design Automation\n3.3.1 Analog Circuit Analysis. Analog circuit design is usually a manual process that requires\nmany trial-and-error iterations between pre-layout and post-layout phases. In recent years, the\ndiscrepancy between schematic (i.e., pre-layout) performance estimations and post-layout simu-\nlation results is further enlarged. On the one hand, the analytical performance estimations from\nschematics are no longer accurate with device scaling; on the other hand, even though post-layout\nsimulations can provide high-accuracy estimations, they are extremely time-consuming and have\nbecome the major bottleneck of design iteration time. To shrink the gap in performance modeling\nof integrated circuits (ICs), ML techniques are widely applied for fast circuit evaluation.\nWe discuss the studies based on whether their input features are extracted from pre-layout\nor post-layout information. 1‚óãGiven design schematics, parasitics in layouts can be predicted\nfrom pre-layout stage, which helps bridge the gap of performance difference between pre-layout\nand post-layout simulations. ParaGraph [ 190] builds a GNN model to predict layout-dependent\nparasitics and physical device parameters. MLParest [ 210] shows that non-graph based methods\n(e.g., random forest) also work well for estimating interconnect parasitics, whereas the lack of\nplacement information may cause large variations in predictions. 2‚óãGiven circuit schematics as\nwell as device information as inputs, it is possible to directly model post-layout performance from\npre-layout designs. Alawieh et al. [5] propose a hierarchical method that combines the Bayesian\nco-learning framework and semi-supervised learning to predict power consumption. The entire\ncircuit schematic is partitioned into multiple blocks to build block-level performance models, upon\nwhich circuit-level performance models are built. By combining these two low-dimensional models\nwith a large amount of unlabeled data, pseudo samples can be labeled with almost no cost. Finally, a\nhigh-dimensional performance model mapping low-level features to circuit-level metrics is trained\nwith pseudo samples and a small amount of labeled samples, which demonstrates the feasibility\nof performance modeling with inadequate labeled samples. Several variants of Bayesian-based\nmethods also perform well for estimating post-layout performance, e.g., combining Bayesian\nregression with SVM to predict circuit performance [ 179] and using Bayesian DNNs to compare\ncircuit designs [ 74].3‚óãSince post-layout simulations with SPICE-like simulators is time-consuming,\nML techniques are applied to quickly assess layout design performance [ 128]. To make better use\nof structural information inside layouts, intermediate layout placement results are represented as\n3D images to feed a 3D CNN model [ 138], or encoded as graphs to train a customized GNN model\n[129], with the goal to predict whether a design specification is satisfied.\n3.3.2 High-Level Synthesis (HLS). HLS is an automated transformation from behavioral languages\n(e.g., C/C++/SystemC) to register-transfer level (RTL) designs, which significantly expedites the\ndevelopment of hardware designs involving with field-programmable gate arrays (FPGAs) or\napplication-specific integrated circuits (ASICs). Since HLS tools usually take considerable time\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:11\nTable 3. Summary of applying ML techniques for performance modeling and prediction in design automation.\nDomain Prediction of Technique Input\nAnalog\nCircuit\n(¬ß3.3.1)Parasitics GNN [190], random forest [210] Circuit schematics\nPower/area/bandwidthBayesian co-learning and semi-supervised learning [5],\nBayesian regression and SVM [179]Circuit schematics\nand device informationProbability of superiority between designs Bayesian DNN [74]\nGain/unity gain frequency/bandwidth/phase margin SVM/ANN/random forest [128], 3D CNN [138], GNN [129]Circuit placementElectromagnetic properties GNN [262]\nHLS\n(¬ß3.3.2)Area/latency/throughput/logic utilization Random forest [137, 159], transfer learning [119] Directives in HLS scripts\nResource utilization ANN [112]\nIR graphs from HLS front-ends Resource mapping and clustering GraphSAGE [223]\nRouting congestion Linear regression/ANN/gradient boosted regression tree [265]\nPower Linear regression/SVM/tree-based models/DNNs [136] IR graphs and HLS reports\nThroughput and throughput-to-area ratio Ensemble learning by stacked regression [148]HLS reportsResource utilization and timing Linear regression/ANN/gradient tree boosting [43]\nCross-platform latency and power Random forest [176] CPU program counters\nSpeedup over an ARM processor ANN [149]Application characteristics,\nHLS reports, FPGA configurations\nLogic and\nPhysical\nSynthesis\n(¬ß3.3.3)Area/delay CNN [256], LSTM [258] Synthesis flows\nDRVsLinear regression/ANN/decision tree [125], MARS [184] Placement and GR information\nMARS/SVM [28], MLP [216] Placement\nDRC hotspotsFCN [247] Placement and GR information\nVariant of FCN [131]\nPlacement GR congestion map FCN [29]\nRouting congestion in FPGAsLinear regression [145]\nConditional GAN [6, 257] Post-placement images\nto synthesize each design, it prevents designers from exploring design space sufficiently, which\nmotivates the application of ML models for fast and accurate performance estimation.\nIn performance estimation of HLS designs, the input features to ML models are extracted from\nthree major sources: HLS directives, IRs from HLS front-ends, and HLS reports. 1‚óãTaking the\ndirectives in an HLS script as input features, random forest is capable to forecast different design\nmetrics, such as area and effective latency [ 137], and throughput and logic utilization [ 159]. In order\nto reuse knowledge from previous experiences, a transfer learning approach [ 119] can transfer\nknowledge across different applications or synthesis options. 2‚óãTaking advantages of IR graphs\ngenerated by HLS front-ends, Koeplinger et al. [112] count resource requirements of each node in\ngraphs by using pre-characterized area models, which are then used as inputs to ANNs to predict\nthe LUT routing usage, register duplication, and unavailable LUTs. The exploitation of GNNs\nmakes it possible to automatically predict the mapping from arithmetic operations in IR graphs to\ndifferent resources on FPGAs [ 223]. To forecast post-implementation routing congestion, Zhao et al.\n[265] build a dataset that connects the routing congestion metrics after RTL implementation with\noperations in IRs, with the goal to train ML models locating highly congested regions in source code.\n3‚óãTaking the information that can be directly extracted from HLS reports, Dai et al. [43] try several\nML models (linear regression, ANN, and gradient tree boosting) to predict post-implementation\nresource utilization and timing. Pyramid [ 148] applies the ensemble learning by stacked regression\nto accurately estimate the throughput or the throughput-to-area ratio. HL-Pow [ 136] employs\nfeatures from both IR graphs and HLS reports to predict the power by a variety of ML models.\nThe surge of heterogeneous platforms with FPGA/AISC and CPU provides more possibility\nof hardware/software co-design, motivating cross-platform performance predictions. HLSPredict\n[176] uses random forest to predict FPGA cycle counts and power consumption based on program\ncounter measurements obtained from CPU execution. While HLSPredict targets the same FPGA\nplatform in training and testing, XPPE [ 149] considers different FPGA platforms, and uses ANNs to\npredict the speedup of an application on a target FPGA over an ARM processor.\n3.3.3 Logic and Physical Synthesis. In digital design, logic synthesis converts RTL designs into\noptimized gate-level representations; physical synthesis then transforms these design netlists into\nphysical layouts. Since these two stages may take hours or days to generate final bitstreams/layouts,\nmany problems benefit from the power of ML models for fast performance estimation.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:12 Wu and Xie.\nIn logic synthesis, CNN models [ 256] or LSTM-based models [ 258] can be leveraged to forecast\nthe delay and area after applying different synthesis flows on specific designs, where the inputs are\nsynthesis flows represented in either matrices or time series.\nIn physical synthesis, routing is a sophisticated problem subject to stringent constraints, and EDA\ntools typically utilize a two-step method: global routing (GR) and detailed routing (DR). GR tool\nallocates routing resource coarsely, and provides routing plans to guide DR tools to complete the\nentire routing. In general, routing congestion can be figured out during or after GR; the routability\nof a design is confirmed after DR and design rule checking (DRC). Endeavors have been made\nto predict routability from early layout stages, so as to avoid excessive iterations back and forth\nbetween placement and routing.\nIn ASICs, some investigations predict routability by estimating the number of design rule\nviolations (DRVs). Taking GR results as inputs, Li et al. [125] explore several ML models (linear\nregression, ANN, and decision tree) to predict the number of DRVs, final hold slack, power, and\narea. Qi et al. [184] rely on placement data and congestion maps from GR as input features, and\nuse a nonparametric regression technique, multivariate adaptive regression splines (MARS) [ 62],\nto predict the utilization of routing resource and the number of DRVs. By merely leveraging\nplacement information, it is possible to predict routability by MARS and SVM [ 28], or to detect\nDR short violations by an MLP [ 216]. When representing placement information as images, fully\nconvolutional networks (FCNs) are capable to predict locations of DRC hotspots by considering GR\ninformation as inputs [ 247], or to forecast GR congestion maps by formulating the prediction task\nas a pixel-wise binary classification using placement data [ 29]. J-Net [ 131] is a customized FCN\nmodel, and takes both high-resolution pin patterns and low-resolution layout information from the\nplacement stage as features to output a 2D array that indicates if the tile corresponding to each\nentry is a DRC hotspot.\nIn FPGAs, routing congestion maps can be directly estimated by linear regression [ 145] using\nfeature vectors coming from pin counts and wirelength per area of SLICEs. By constructing the\nrouting congestion prediction as an image translation problem, a conditional GAN [ 6,257] is able\nto take post-placement images as inputs to predict congestion heat maps.\n4 ML AS DESIGN METHODOLOGY\nThis section introduces studies that directly employ ML techniques as the design methodology for\ncomputer architecture/systems. Computer architecture and systems have been becoming increas-\ningly complicated, making it expensive and inefficient for human efforts to design or optimize them.\nIn response, visionaries have argued that computer architecture and systems should be imbued with\nthe capability to design and configure themselves, adjust their behaviors according to workloads‚Äô\nneeds or user-specified constraints, diagnose failures, repair themselves from the detected failures,\netc. With strong learning and generalization capabilities, ML-based techniques are naturally suitable\nto resolve these considerations, which can adjust their policies during system designs according to\nlong-term planning and dynamic workload behaviors. As many problems in architecture/system\ndesign can be formulated as combinatorial optimization or sequential decision-making problems,\nRL is broadly explored and exploited. Table 4 and Table 5 recapitulate the studies that apply ML\ntechniques as the design methodology for computer architecture/system and design automation\nrespectively, in terms of target tasks and adopted ML techniques.\n4.1 Memory System Design\nThe \"memory wall\" has been a performance bottleneck in von Neumann architectures, where\ncomputation is orders of magnitude faster than memory access. To alleviate this problem, hier-\narchical memory systems are widely used and there arise optimizations for different levels of\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:13\nTable 4. Summary of applying ML techniques as the design methodology for computer architecture/ systems.\nDomain Task Technique\nMemory\nSystem\nDesign\n(¬ß4.1)Cache replacement policy Perceptron learning [96, 219], Markov decision process [16], LSTM and SVM [207]\nCache prefetching policy Perceptron learning [17, 230], contextual bandit [183], LSTM [24, 77, 208, 261]\nMemory controller policy Q-learning [84, 153, 169]\nGarbage collection Q-learning [104, 105]\nBranch\nPrediction\n(¬ß4.2)Branch directionMLP [26], piecewise linear regression [92],\nperceptron [67, 91, 93‚Äì95, 213], CNN [218]\nNoC\n(¬ß4.3)Link management ANN [192, 201]\nDVFS for routers Q-learning [60, 266]\nRouting Q-learning [23, 54, 59, 118, 147]\nArbitration policy DQN [254, 255]\nAdjusting injection rates Q-learning [46], ANN [229]\nSelection of fault-tolerant modes Q-learning [233, 234]\nLink placement in 3D NoCs STAGE algorithm [44, 45, 97]\nLoop placement in routerless NoCs Advantage actor-critic with MCTS [134]\nPower\nManagement\n(¬ß4.4.1)DVFS and thread packing Multinomial logistic regression [36]\nDVFS and power gating MLP [187]\nDVFS, socket allocation, and use of HyperThreads Extra trees/gradient boosting/KNN/MLP/SVM [82]\nDVFS for CPU cores/uncore/through-silicon interposers Propositional rule [1], ANN [238], Q-learning [182]\nDVFS for CPU-GPU heterogeneous platforms Weighted majority algorithm [144]\nDVFS for multi-/many-core systems Q-learning [11], semi-supervised RL [103], hierarchical Q-learning [32, 33, 178]\nResource\nManagement\n& Task\nAllocation\n(¬ß4.4.2)Tuning architecture configurations Maximum likelihood [53], statistical machine learning [20, 64]\nDynamic cache partitioning Enforced subpopulations [69], Q-learning [89]\nTask allocation in many-core systems Q-learning [142], DDPG [241]\nWorkflow management SVM and random forest [56]\nHardware resource assignment REINFORCE [106], Bayesian optimization [98]\nDevice placement REINFORCE [160, 162], policy gradient [2], PPO [66, 270]\nScheduling\n(¬ß4.4.3)Scheduling jobs in single-core processors Q-learning [236]\nScheduling jobs in multi-processor systems Value-based RL [58, 226]\nData Center\nManagement\n(¬ß4.5)Assignment of servers to applications Value-based RL [220, 221]\nContent allocation in CDNs Fuzzy RL [227]\nPlacement of virtual machines onto physical machines PPO [13]\nTraffic optimization Policy gradient and DDPG [30]\nScheduling jobs with complex dependency REINFORCE [151]\nStraggler diagnosis Statistical ML [267]\nData-center-level caching policy Decision tree [232], LSTM [171], gradient boosting [211], DDPG [242]\nBitrate selection for video chunks A3C [150, 252]\nScheduling video workloads in hybrid CPU-GPU clusters DQN [263]\nCode\nGeneration\n(¬ß4.6.1)Code completion N-gram model and RNN [188]\nCode generation LSTM [40]\nProgram translation Tree-to-tree encoder-decoder [31, 63], seq2seq [111], transformer [120]\nCompiler\n(¬ß4.6.2)Instruction scheduling Temporal difference [154], projective reparameterization [87]\nImproving compiler heuristics NEAT [37], LSTM [39]\nOrdering of optimizations NEAT [114]\nAutomatic vectorization Imitation learning [158]\nProgram transformation for approximate computing MLP [55, 251]\nCompilation for DNN workloads PPO [4], policy gradient [107]\nmemory systems. As both the variety and the size of modern workloads are drastically growing,\nconventional designs that are based on heuristics or intuitions may not catch up with the demand\nof the ever-growing workloads, leading to sharply degradation in performance. As such, many\nstudies resort to ML-based techniques to design smart and intelligent memory systems.\n4.1.1 Cache. The conspicuous disparity in latency and bandwidth between CPUs and memory\nsystems motivates investigations for efficient cache management. There are two major types of\nstudies on cache optimization: improving cache replacement policies, and designing intelligent\nprefetching policies. 1‚óãTo develop cache replacement policies, perceptron learning is employed to\npredict whether to bypass or reuse a referenced block in the last-level cache (LLC) [ 96,219]. Instead\nof using perceptrons, Beckmann et al. [16] model the cache replacement problem as a Markov\ndecision process and replace lines according to the difference between their expected hits and\nthe average hits. Shi et al. [207] train an attention-based LSTM model offline to extract insights\nfrom history program counters, which are then used to build an online SVM-based hardware\npredictor to serve as the cache replacement policy. 2‚óãTo devise intelligent prefetchers, Wang et\nal.[230] propose a prefetching mechanism that uses conventionally table-based prefetchers to\nprovide prefetching suggestions and a perceptron trained by spatio-temporal locality to reject\nunnecessary prefetching decisions, ameliorating the cache pollution problem. Similarly, Bhatia et al.\n[17] integrate a perceptron-based prefetching filter with conventional prefetchers, increasing the\ncoverage of prefetches without hurting accuracy. Instead of the commonly used spatio-temporal\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:14 Wu and Xie.\nlocality, a context-based memory prefetcher [ 183] leverages the semantic locality that characterizes\naccess correlations inherent to program semantics and data structures, which is approximated by\na contextual bandit model in RL. Interpreting semantics in memory access patterns is analogous\nto sequence analysis in natural language processing (NLP), and thus several studies use LSTM-\nbased models and treat the prefetching as either a regression problem [ 261] or a classification\nproblem [ 77]. Even with better performance, especially for long access sequences and noise traces,\nLSTM-based prefetchers suffer from long warm-up and prediction latency, and considerable storage\noverheads. The discussion of how hyperparameters impact LSTM-based prefetchers‚Äô performance\n[24] highlights that the lookback size (i.e. memory access history window) and the LSTM model\nsize strongly affect prefetchers‚Äô learning ability under different noise levels or workload patterns.\nTo accommodate the large memory space, Shi et al. [208] introduce a hierarchical sequence model\nto decouple predictions of pages and offsets by using two separate attention-based LSTM layers,\nwhereas the corresponding hardware implementation is impractical for actual processors.\n4.1.2 Memory Controller. Smart memory controllers can significantly improve memory bandwidth\nutilization. Aiming at a self-optimizing memory controller adaptive to dynamically changing\nworkloads, it can be modeled as an RL agent that always selects legal DRAM commands with the\nhighest expected long-term performance benefits (i.e., Q-values) [ 84,153]. To allow optimizations\ntoward various objectives, this memory controller is then improved in two major aspects [ 169].\nFirst, the rewards of different actions (i.e., legal DRAM commands) are automatically calibrated\nby genetic algorithms to serve different objective functions (e.g., energy, throughput, etc). Second,\na multi-factor method that considers the first-order attribute interactions is employed to select\nproper attributes used for state representations. Since both of them use table-based Q-learning and\nselect limited attributes to represent states, the scalability may be a concern and their performance\ncould be improved with more informative representations.\n4.1.3 Others. A variety of work targets different parts of the memory system. Margaritov et al.\n[152] accelerate virtual address translation through learned index structures [ 113]. The results are\nencouraging in terms of the accuracy, which reaches almost 100% for all tested virtual addresses; yet\nthis method has unacceptably long inference latency, leaving practical hardware implementation\nas the future work. Wang et al. [235] reduce data movement energy in interconnects by exploiting\nasymmetric transmission costs of different bits, where data blocks to be transmitted are dynamically\ngrouped by K-majority clustering to derive energy-efficient expressions for transmission. In terms\nof garbage collection in NAND flash, Kang et al. [104] propose an RL-based method to reduce the\nlong-tail latency. The key idea is to exploit the inter-request interval (idle time) to dynamically\ndecide the number of pages to be copied or whether to perform an erase operation, where decisions\nare made by table-based Q-learning. Their following work [ 105] considers more fine-grained states,\nand introduces a Q-table cache to manage key states among enormous amount of states.\n4.2 Branch Prediction\nBranch predictor is one of the mainstays of modern processors, significantly improving the\ninstruction-level parallelism. As pipelines gradually deepen, the penalty of mis-prediction in-\ncreases. Traditional branch predictors often consider limited history length, which may hurt the\nprediction accuracy. In contrast, the perceptron/MLP-based predictors can handle long histories\nwith reasonable hardware budgets, outperforming prior state-of-the-art non-ML-based predictors.\nStarting with a static branch predictor trained with static features from program corpus and\ncontrol flow graphs, an MLP is used to predict the direction of a branch at compile time [ 26]. Later,\na dynamic branch predictor uses a perceptron-based method [ 95]. It hashes the branch address\nto select the proper perceptron and computes the dot product accordingly to decide whether to\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:15\ntake this branch, which shows great performance on linearly separable branches. Its latency and\naccuracy can be further improved by applying ahead pipelining and selecting perceptrons based on\npath history [ 91]. To attain high accuracy in non-linearly separable branches, the perceptron-based\nprediction is generalized as piecewise linear branch prediction [ 92]. In addition to the path history,\nmultiple types of features from different organizations of branch histories can be leveraged to\nenhance the overall performance [ 94]. When considering practical hardware implementation of\nbranch predictors, SNAP [ 213] leverages current-steering digital-to-analog converters to transfer\ndigital weights into analog currents and replaces the costly digital dot-product computation to the\ncurrent summation. Its optimized version [ 93] equips several new techniques, such as the use of\nglobal and per-branch history, trainable scaling coefficients, dynamic training thresholds, etc.\nRather than making binary decisions of whether to take a certain branch, it is possible to directly\npredict the target address of an indirect branch at the bit level via perceptron-based predictors [ 67].\nWhile high accuracy is achieved by current perceptron/MLP-based predictors, Tarsa et al. [218]\nnotice that a small amount of static branch instructions are systematically mispredicted, referred to\nas hard-to-predict branches (H2Ps). Consequently, they propose a CNN helper predictor for pattern\nmatching of history branches, ultimately improving accuracy for H2Ps in conditional branches.\n4.3 NoC Design\nThe aggressive transistor scaling has paved the way for integrating more cores in a single chip or\nprocessor. With the increasing number of cores per chip, NoC plays a gradually crucial role, since\nit is responsible for inter-core communication and data movement between cores and memory\nhierarchies. Several problems attracting attention are as follows. First, communication energy scales\nslower than computation energy [ 22], implying necessity to improve power efficiency of NoCs.\nSecond, the complexity of routing or traffic control grows with the number of cores per chip and\nthis problem is even exacerbated by the rising variety and irregularity of workloads. Third, with\nthe continuous scaling down of transistors, NoCs are more vulnerable to different types of errors\nand thus reliability becomes a key concern. Fourth, some non-conventional NoC architectures\nmight bring promising potentials in the future, whereas they usually come with large design spaces\nand complex design constraints, which is nearly impossible for manually optimization. Among all\naforementioned fields, ML-based design techniques display their strength and charm.\n4.3.1 Link Management and DVFS. Power consumption is one crucial concern in NoCs, in which\nlinks usually consume a considerable portion of network power. While turning on/off links according\nto a static threshold of link utilization is a trivial way to reduce power consumption, it can not\nadapt to dynamically changing workloads. Savva et al. [201] use multiple ANNs for dynamic link\nmanagement. Each ANN is responsible for one region of the NoC, and dynamically computes\na threshold for every time interval to turn on/off links given the link utilization of each region.\nDespite significant power savings with low hardware overheads, this approach causes long latency\nin routing. In order to meet certain power and thermal budgets, hierarchical ANNs [ 192] are used to\npredict optimal NoC configurations (i.e., link bandwidth, node voltage and task assignment to nodes),\nwhere the global ANN predicts globally optimal NoC configurations exploiting local optimal energy\nconsumption predicted by local ANNs. To save dynamic power, several investigations [ 60,266]\nemploy per-router based Q-learning agents, which are offline trained ANNs to select optimal\nvoltage/frequency levels for each router.\n4.3.2 Routing and Traffic Control. With the increasing variety and irregularity of workloads\nand their traffic patterns, learning-based routing algorithms and traffic control approaches show\nsuperior performance due to their excellent adaptability. 1‚óãAs routing problems can be formulated\nas sequential decision-making processes, several studies apply Q-learning based approaches, namely\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:16 Wu and Xie.\nthe Q-routing algorithm [ 23], which uses local estimation of delivery time to minimize total packets\ndelivery time, capable to handle irregular network topologies and keep a higher network load than\nthe conventional shortest path routing. Q-routing is then extended to several other scenarios, such\nas combining with dual RL to improve learning speed and routing performance [ 118], resolving\npackets routing in dynamic NoCs whose network structures/topologies are dynamically changing\nduring runtime [ 147], handling irregular faults in bufferless NoCs by the reconfigurable fault-\ntolerant Q-routing [ 59], and enhancing the capability to reroute messages around congested\nregions by the congestion-aware non-minimal Q-routing [ 54]. In addition to routing problems,\ndeep Q-network is also promising for NoC arbitration policies [ 254,255], where the agent/arbiter\ngrants a certain output port to the input buffer with the largest Q-value. Even displaying some\nimprovements in latency and throughput, the direct hardware implementation is impractical\ndue to the complexity of deep Q-networks, and thus insights are distilled to derive a relatively\nsimple circuitry implementation. 2‚óãWith the goal to control congestion in NoCs, the SCEPTER\nNoC architecture [ 46], a bufferless NoC with single-cycle multi-hop traversals and a self-learning\nthrottling mechanism, controls the injection of new flits into the network by Q-learning. Each\nnode in the network independently selects whether to increase, decrease, or retain the throttle\nrate according to their Q-values, which conspicuously improves bandwidth allocation fairness and\nnetwork throughput. Wang et al. [229] design an ANN-based admission controller to determine\nthe appropriate injection rate and the control policy of each node in a standard NoC.\n4.3.3 Reliability and Fault Tolerance. With the aggressive technology scaling down, transistors\nand links in NoCs are more prone to different types of errors, indicating that reliability is a crucial\nconcern and proactive fault-tolerant techniques are required to guarantee performance. Wang et\nal.[233] employ per-router-based Q-learning agents to independently select one of four fault-\ntolerant modes, which can minimize the end-to-end packet latency and power consumption. These\nagents are pre-trained and then fine-tuned during runtime. In their following work [ 234], these\nerror-correction modes are extended and combined with various multi-function adaptive channel\nconfigurations, retransmission settings, and power management strategies, significantly improving\nlatency, energy efficiency, and mean-time-to-failure.\n4.3.4 General Design. With the growing number of cores per chip/system, the increasing hetero-\ngeneity of cores, and various performance targets, it is complicated to simultaneously optimize\ncopious design knobs in NoCs. One attempt to automated NoC design is the MLNoC [ 186], which\nutilizes supervised learning to quickly find near-optimal NoC designs under multiple optimization\ngoals. MLNoC is trained by data from thousands of real-world and synthetic SoC (system-on-chip)\ndesigns, and evaluated with real-world SoC designs. Despite disclosure of limited details and ab-\nsence of comprehensive comparison with other design methods, it shows superior performance to\nmanually optimized NoC designs, delivering encouraging results.\nApart from conventional 2D mesh NoCs, a series of investigations focuses on 3D NoC designs,\nwhere the STAGE algorithm is applied to optimize vertical and planar placement of communication\nlinks in small-world network based 3D NoCs [ 44,45]. The STAGE algorithm repeatedly alternates\nbetween two stages, the base search that tries to find the local optima based on the learned evaluation\nfunction, and the meta-search that uses SVR to learn evaluation functions. Later, the STAGE\nalgorithm is extended for multi-objective optimization in heterogeneous 3D NoC systems [ 97],\nwhich jointly considers GPU throughput, average latency between CPUs and LLCs, temperature,\nand energy. In terms of routerless NoCs that any two nodes are connected via at least one ring/loop,\na deep RL framework that exploits Monte-Carlo tree search for efficient design space exploration is\ndeveloped to optimize loop placements [ 134], and the design constraints can be strictly enforced\nby carefully devising the reward function.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:17\n4.4 Resource Allocation or Management\nResource allocation or management is the coordination between computer architecture/systems\nand workloads. Consequently, its optimization difficulty occurs with the booming complexity from\nboth sides and their intricate interactions. ML-based approaches have blazed the trail to adjusting\npolicies wisely and promptly pursuant to dynamic workloads or specified constraints.\n4.4.1 Power Management. ML-based techniques have been applied broadly to improve power\nmanagement, due to two main reasons. First, power/energy consumption can be recognized as\none metric of runtime costs. Second, under certain circumstances there could be a hard or soft\nconstraint/budget of power/energy, making power efficiency a necessity.\nIn consideration of power management for different parts of systems, PACSL [ 1] uses the\npropositional rule to adjust dynamic voltage scaling (DVS) for CPU cores and on-chip L2 cache,\nwhich achieves an improvement in the energy-delay product by 22% on average (up to 46%) over\nindependently applying DVS for each part. Won et al. [238] coordinate an ANN controller with a\nproportional integral for uncore DVFS. The ANN controller can be either pre-trained offline by a\nprepared dataset or trained online by bootstrapped learning. Manoj et al. [182] deploy Q-learning\nto adaptively adjust the level of output-voltage swing at transmitters of 2.5D through-silicon\ninterposer I/Os, under constraints of communication power and bit error rate.\nFrom the system level, DVFS is one of the most prevalent techniques. Pack & Cap [ 36] builds\na multinomial logistic regression classifier that is trained offline and queried during runtime, to\naccurately identify the optimal operating point for both thread packing and DVFS under an arbitrary\npower cap. GreenGPU [ 144] focuses on heterogeneous systems with CPUs and GPUs, and applies\nthe weighted majority algorithm to scale frequency levels for both GPU cores and memory in a\ncoordinated manner. CHARSTAR [ 187] targets joint optimization of power gating and DVFS within\na single core, where frequencies and configurations are dynamically selected by a lightweight offline\ntrained MLP predictor. To minimize energy consumption, Imes et al. [82] use ML-based classifiers\n(e.g., extra trees, gradient boosting, KNN, MLP and SVM) to predict the most energy-efficient\nresource settings (specifically, tuning socket allocation, the use of HyperThreads, and processor\nDVFS) by using low-level hardware performance counters. Bai et al. [11] consider the loss caused\nby on-chip regulator efficiency during DVFS, and try to minimize energy consumption under a\nparameterized performance constraint. The online control policy is implemented by a table-based\nQ-learning, which is portable across platforms without accurate modeling of a specific system.\nA series of studies leverages RL for dynamic power management in multi-/many-core systems.\nAs systems scale up, these RL-based methods often suffer from state space explosion, and two types\nof methods are introduced to resolve the scalability issue. 1‚óãBy combining RL with supervised\nlearning, a semi-supervised RL-based approach [ 103] achieves linear complexity with the number of\ncores, which is able to maximize throughput ensuring power constraints and cooperatively control\ncores and uncores in synergy. 2‚óãThe exploitation of hierarchical Q-learning reduces the time\ncomplexity to ùëÇ(ùëõlgùëõ), where ùëõdenotes the number of cores. Pan et al. [178] introduce multi-level\nQ-learning to select target power modes, where Q-values are approximated by a generalized radial\nbasis function. Table-based distributed Q-learning also performs well for DVFS [ 32], and there is\none variant [33] aware of the priorities of different applications.\nSome energy management policies target specific applications or platforms. JouleGuard [ 79] is a\nruntime control system coordinating approximate computing applications with system resource\nunder energy budgets. It uses a multi-arm bandit approach to identifying the most energy effi-\ncient system configuration, upon which application configurations are determined to maximize\ncompute accuracy within energy budgets. Targeting Intel SkyLake processors, a post-silicon CPU\ncustomization applies various ML models for dynamically clock-gating unused resource [217].\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:18 Wu and Xie.\n4.4.2 Resource Management and Task Allocation. Modern architectures and systems have been\nbecoming so sophisticated and diverse that it is non-trivial to either optimize performance or fully\nutilize system resource. This rapidly evolving landscape is further complicated by various workloads\nwith specific requirements or targets. In order to keep the pace, one cure is to develop more efficient\nand automated methods for resource management and task allocation, where ML-based techniques\nare excelled to explore large design spaces and simultaneously optimize multiple objectives, and\npreserve better scalablility and portability after carefully designed.\nFor a single-core processor, a regularized maximum likelihood approach [ 53] predicts the best\nhardware micro-architectural configuration for each phase of a program, based on runtime hard-\nware counters. For multi-core processors, a statistical machine learning (SML) based method [64]\ncan quickly find configurations that simultaneously optimize running time and energy efficiency.\nSince this method is agnostic to application and micro-architecture domain knowledge, it is a\nportable alternative to human expert optimization. SML can also be applied as a holistic method to\ndesign self-evolving systems that optimize performance hierarchically across circuit, platform, and\napplication levels [ 20]. In addition to tuning architectural configurations, dynamic on-chip resource\nmanagement is crucial for multi-core processors, where one example is dynamic cache partitioning.\nIn response to changing workload demands, an RNN evolved by the enforced subpopulations algo-\nrithm [ 69] is introduced to partition L2 cache dynamically. When integrating dynamic partitioning\nof LLC with DVFS on cores and uncore, a co-optimization method using table-based Q-learning\nachieves much lower energy-delay products than any of the techniques applied individually [89].\nTo guarantee efficient and reliable execution in many-core systems, task allocation should\nconsider several aspects, such as heat and communication issues. Targeting the heat interaction of\nprocessor cores and NoC routers, Lu et al. [142] apply Q-learning to assign tasks to cores based on\ncurrent temperatures of cores and routers, such that the maximum temperature in the future is\nminimized. Targeting the non-uniform and hierarchical on/off-chip communication capability in\nmulti-chip many-core systems, core placement optimization [ 241] leverages deep deterministic\npolicy gradient (DDPG) [ 132] to map computation onto physical cores, able to work in a manner\nagnostic to domain-specific information.\nSome studies pay attention to workflow management and general hardware resource assignment.\nSmartFlux [ 56] focuses on the workflow of data-intensive and continuous processing. It intelligently\nguides asynchronous triggering of processing steps with the help of predictions made by multiple\nML models (e.g., SVM, random forest), which indicate whether to execute certain steps and to decide\ncorresponding configurations upon each wave of data. Given target DNN models, deployment\nscenarios, platform constraints, and optimization objectives (latency/energy), ConfuciuX [ 106]\napplies a hybrid two-step scheme for optimal hardware resource assignments (i.e., assigning the\nnumber of processing elements and the buffer sizes to each DNN layer), where REINFORCE [ 214]\nperforms a global coarse-grained search followed by a genetic algorithm for fine-grained tuning.\nApollo [ 98] is a general architecture exploration framework for sample-efficient accelerator designs,\nwhich leverages ML-based black-box optimization techniques (e.g., Bayesian optimization) to\noptimize accelerator configurations to satisfy use-specified design constraints.\nIn heterogeneous systems with CPUs and GPUs, device placement refers to the process of\nmapping nodes in computational graphs of neural networks onto proper hardware devices. Initially,\ncomputational operations are grouped manually, and assigned to devices by REINFORCE that\nemploys a sequence-to-sequence RNN model as the parameterized policy [ 162]. Later, a hierarchical\nend-to-end model makes this manual grouping process automatic [ 160]. The training speed is\nfurther improved by introduction of proximal policy optimization (PPO) [ 66]. Despite great advance\nbrought by the above approaches, they are not transferable and a new policy should be trained from\nscratch specifically for each new computational graph. By encode structure of computational graphs\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:19\nwith static graph embeddings [ 2] or learnable graph embeddings [ 270], the trained placement policy\nexhibits great generalizability to unseen neural networks.\n4.4.3 Scheduling. In classical real-time scheduling problems, the key task is to decide the order,\naccording to which the currently unscheduled jobs should be executed by a single processor, such\nthat the overall performance is optimized. As multi-core processors have been the mainstream,\nthe scheduling is gradually perplexing. One major reason is that multiple objectives besides the\nperformance should be carefully considered, such as balanced assignments among various cores and\nresponse time fairness. Equipped with the capability to well understand the feedback provided by\nthe environment and to dynamically adjust policies, RL is a common tool for real-time scheduling.\nTo optimize the execution order of jobs after they are routed to a single CPU core, Whiteson and\nStone [ 236] propose an adaptive scheduling policy that exploits Q-routing, where the scheduler\nutilizes the router‚Äôs Q-table to assess a job‚Äôs priority and decides jobs‚Äô ordering accordingly so as\nto maximize the overall utility. In multi-core systems, Fedorova et al. [58] present a blueprint for\na self-tuning scheduling algorithm based on the value-based temporal-difference method in RL,\naiming to maximize a cost function that is an arbitrary weighted sum of metrics of interest. This\nalgorithm is then improved to be a general method for online scheduling of parallel jobs [ 226],\nwhere the value functions are approximated by a parameterized fuzzy rulebase. This scheduling\npolicy always selects to execute jobs with the maximum value functions in the job queue, which\npossibly preempts currently running jobs and squeezes some jobs into fewer CPUs than they ideally\nrequire, with the goal of achieving optimized long-term utility.\n4.5 Data Center Management\nWith the rapid scale expansion of data centers, issues that may be trivial in a single machine become\nincreasingly challenging, let alone the inherently complicated problems.\nEarly work aims at a relatively simple scenario of resource allocation, i.e., to dynamically assign\ndifferent numbers of servers to multiple applications. This problem can be modeled as an RL problem\nwith service-level utility functions as rewards: the arbiter will select a joint action that would\nbring the maximum total return after consulting local value functions estimated via either table-\nbased methods [ 221] or function approximation [ 220]. In order to better model interactions among\nmultiple agents, a multi-agent coordination algorithm with fuzzy RL [ 227] can be used to solve the\ndynamic content allocation in content delivery networks (CDNs), in which each requested content\nis modeled as an agent, trying to move toward the area with a high demand while coordinating\nwith other agents/contents. A recent innovation [ 13] pays attention to the placement of virtual\nmachines onto physical machines, so as to minimize the peak-to-average ratio of resource usage\nacross physical machines, where PPO and hindsight imitation learning are evaluated.\nTo improve data center performance and quality of experience (QoE) for users, ML-based tech-\nniques have been explored in a few directions. 1‚óãIt is important to efficiently schedule jobs and\neffectively diagnose stragglers within jobs. Aiming at traffic optimization (e.g., flow scheduling, load\nbalancing) in data centers, Chen et al. [30] develop a two-level RL system: peripheral systems, which\nare trained by DDPG, reside on end-hosts and locally make instant traffic optimization decisions\nfor short flows; the central system, which is trained by policy gradient, aggregates global traffic\ninformation, guides behaviors of peripheral systems, and makes traffic optimization decisions for\nlong flows. Decima [151] exploits GNNs to represent cluster information and dependency among\njob stages, so that the RL-based scheduler can automatically learn workload-specific scheduling\npolicies to schedule data processing jobs with complex dependency. Hound [ 267] combines sta-\ntistical ML with meta-learning to diagnose causes of stragglers at data-center-scale jobs. 2‚óãIt\nis essential to deploy an intelligent data-center-level cache. DeepCache [ 171] employs an LSTM\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:20 Wu and Xie.\nencoder-decoder model to predict future content popularity, which can be combined with existing\ncache policies to make smarter decisions. Song et al. [211] apply gradient boosting machines to\nmimic a relaxed Belady algorithm that evicts an object whose next request is beyond a reuse\ndistance threshold but not necessarily the farthest in the future. Phoebe [ 242] is an online cache\nreplacement framework leveraging DDPG to predict priorities of objects and to conduct eviction\naccordingly. Considering non-history based features, Wang et al. [232] build a decision tree to\npredict whether the requested file will be accessed only once in the future. These one-time-access\nfiles will be directly sent to users without getting into cache, to avoid cache pollution. 3‚óãFrom the\nworkload perspective, video workloads on CDNs or clusters are prevalent but their optimization is\nquite challenging: first, network conditions fluctuate overtime and a variety of QoE goals should\nbe balanced simultaneously; second, only coarse decisions are available and current decisions will\nhave long-term effects on following decisions. This scenario naturally matches the foundation of\nRL-based techniques. To optimize users‚Äô QoE of streaming videos, adaptive bitrate algorithms have\nbeen recognized as the primary tool used by content providers, which are executed on client-side\nvideo players and dynamically choose a bitrate for each video chunk based on underlying network\nconditions. Pensieve [ 150,252] applies asynchronous advantage actor-critic [ 166] to select proper\nbitrate for future video chunks based on resulting performance from past decisions. When consider-\ning large-scale video workloads in hybrid CPU-GPU clusters, performance degradation often comes\nfrom uncertainty and variability of workloads, and unbalanced use of heterogeneous resources. To\naccommodate this, Zhang et al. [263] use two deep Q-networks to build a two-level task scheduler,\nwhere the cluster-level scheduler selects proper execution nodes for mutually independent video\ntasks and the node-level scheduler assigns interrelated video subtasks to appropriate computing\nunits. This scheme enables the scheduling model to adjust policies according to runtime status of\ncluster environments, characteristics of video tasks, and dependency among video tasks.\n4.6 Code Generation and Compiler\n4.6.1 Code Generation. Due to the similarities in syntax and semantics between programming\nlanguages and natural languages, the problem of code generation or translation is often modeled as\nan NLP problem or a neural machine translation (NMT) problem. Here, we would like to bring up a\nbrief discussion. For more reference, a comprehensive survey [ 7] detailedly contrasts programming\nlanguages against natural languages, and discusses how these similarities and differences drive the\ndesign and application of different ML models in code.\nTargeting code completion, several statistical language models (N-gram model, RNN, and a\ncombination of these two) [ 188] are explored to select sentences that have the highest probability\nand satisfy constraints to fill up partial programs with holes. As for code generation, CLgen [ 40]\ntrains LSTM models by a corpus of hand-written code to learn semantics and structures of OpenCL\nprograms, and generates human-like programs via iteratively sampling from the learned model.\nTargeting program translation, NMT-based techniques are widely applied to migrate code from\none language to another. For example, a tree-to-tree model with the encoder-decoder structure\neffectively translates programs from Java to C# [ 31]; the sequence-to-sequence (seq2seq) model can\ntranslate from CUDA to OpenCL [ 111]. Rather than translating between high-level programming\nlanguages, Coda [ 63] translates binary executables to the corresponding high-level code, which\nemploys a tree-to-tree encoder-decoder structure for code sketch generation and an ensembled\nRNN-based error predictor for iterative error correction on the generated code. Notably, these\nsupervised NMT-based techniques may confront several issues: difficulty to generalize to programs\nlonger than training ones, limited size of vocabulary sets, and scarcity of aligned input-output\ndata. Fully counting on unsupervised machine translation, TransCoder [ 120] adopts a transformer\narchitecture and uses monolingual source code to translate among C++, Java, and Python.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:21\n4.6.2 Compiler. The complexity of compilers grows with the complexity of computer architectures\nand workloads. ML-based techniques can optimize compilers from many perspectives, such as\ninstruction scheduling, compiler heuristics, the order to apply optimizations, hot path identification,\nauto-vectorization, and compilation for specific applications. 1‚óãFor instruction scheduling, the\npreference function of one scheduling over another can be computed by the temporal difference\nalgorithm in RL [ 154]. Regarding scheduling under highly-constrained code optimization, the\nprojective reparameterization [ 87] enables automatic instruction scheduling under constraints\nof data-dependent partial orders over instructions. 2‚óãFor improving compiler heuristics, Neuro-\nEvolution of Augmenting Topologies (NEAT) [ 37] improves instruction placement heuristics by\ntuning placement cost functions. To avoid manual feature engineering, LSTM-based model [ 39]\nautomatically learns compiler heuristics from raw code, which constructs proper embeddings of\nprograms and simultaneously learn the optimization process. 3‚óãFor choosing the appropriate order\nto apply different optimizations, NEAT [ 114] can automatically generate beneficial optimization\norderings for each method in a program. 4‚óãFor path profiling, CrystalBall [ 260] uses an LSTM\nmodel to statically identify hot paths, the sequences of instructions that are frequently executed.\nAs CrystalBall only relies on IRs, it avoids manual feature crafting and is independent of language\nor platform. 5‚óãFor automatic vectorization, Mendis et al. [158] leverage imitation learning to\nmimic optimal solutions provided by superword-level-parallelism based vectorization [ 156].6‚óãFor\ncompilation of specific applications, there are studies improving compilation for approximate com-\nputing or DNN applications. Considering compilation for approximate computing, Esmaeilzadeh\net al. [55] propose a program transformation method, which trains MLPs to mimic regions of\napproximable code and eventually replaces the original code with trained MLPs. The following\nwork [ 251] extends this algorithmic transformation to GPUs. Considering compilation for DNNs,\nRELEASE [ 4] utilizes PPO to search optimal compilation configurations for DNNs. EGRL [ 107]\noptimizes memory placement of DNN tensors during compilation, which combines GNNs, RL, and\nevolutionary search to figure out optimal mapping onto different on-board memory components\n(i.e., SRAM, LLC, and DRAM).\n4.7 Chip Design and Design Automation\nAs technology scales down, the increased design complexity comes with growing process variations\nand reduced design margins, making chip design an overwhelmingly complex problem for human\ndesigners. Recent advancements in ML create a chance to transform chip design workflows.\n4.7.1 Analog Design. Compared with the highly automated digital design counterpart, analog\ndesign usually demands many manual efforts and domain expertise. First, analog circuits have large\ndesign spaces to search proper topology and device sizes. Second, there is an absence of a general\nframework to optimize or evaluate analog designs, and design specifications often vary case by\ncase. Recently, ML techniques have been introduced to expedite analog design automation. We\ndiscuss these studies following the top-down flow of analog design: in the circuit level, a proper\ncircuit topology is selected to satisfy system specifications; then in the device level, device sizes\nare optimized subject to various objectives. These two steps compose of pre-layout designs. After\ncircuit schematics are carefully designed, analog layouts in the physical level are generated.\n1‚óãIn the circuit level, there is an attempt towards automatic circuit generation currently targeting\ntwo-port linear analog circuits [ 195]. The design specifications are encoded by a hypernetwork\n[72] to generate weights for an RNN model, which is trained to select circuit components and their\nconfigurations. 2‚óãIn the device level, the combination of RL and GNNs enables automatic transistor\nsizing [ 231], which is able to generalize across different circuit topologies or different technology\nnodes. AutoCkt [ 205] introduces transfer learning techniques into deep RL for automatic sizing,\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:22 Wu and Xie.\nTable 5. Summary of applying ML techniques as the design methodology for design automation.\nDomain Task Technique\nAnalog\nDesign\n(¬ß4.7.1)Circuit Level Generating circuit topology RNN and hypernetwork [195]\nDevice Level Device sizing Actor critic [205, 231], ANN [194]\nPhysical LevelRouting VAE [272]\nOptimizing layout configurations Multi-objective Bayesian optimization [139]\nDigital\nDesign\n(¬ß4.7.2)HLSOptimizing loop unrolling pragma Random forest [259]\nOptimizing placement of multiple pragmas Bayesian optimization [155]\nOptimizing resource pragma Actor-critic and GNN [243]\nLogic\nSynthesisSelecting proper optimizers MLP [173]\nLogic optimization Policy gradient [73], actor-critic [81]\nDetermining the maximum error of each node Q-learning [180]\nPhysical\nSynthesisOptimizing flip-flop placement in clock networks K-means clustering [240]\nOptimizing clock tree synthesis Conditional GAN [143]\nOptimizing memory cell placement PPO [161]\nOptimizing standard cell placement Cast as an NN training problem [135]\nFix design rule violations PPO [189]\nachieving 40√óspeedup over a traditional genetic algorithm. Rosa et al. [194] provide comprehensive\ndiscussions of how to address automatic sizing and layout of analog ICs via deep learning and ANNs.\n3‚óãIn the physical level, GeniusRoute [ 272] automates analog routing through the guidance learned\nby a generative neural network. The analog placements and routing are represented as images\nto pass through a variational autoencoder (VAE) [ 71] to learn routing likelihoods of each region.\nGeniusRoute achieves competitive performance to manual layouts and is capable to generalize to\ncircuits of different functionality. Liu et al. [139] apply multi-objective Bayesian optimization to\noptimize combinations of net weighting parameters, which could significantly change floor plans\nand placement solutions, so as to improve analog layouts of building block circuits.\n4.7.2 Digital Design. For the studies applying ML techniques to directly optimize digital designs,\nwe organize them following a top-down flow, i.e., HLS, logic synthesis, and physical synthesis.\nThe design space exploration in HLS designs usually relates to properly assigning directives\n(pragmas) in high-level source code, since directives significantly impact the quality of HLS designs\nby controlling parallelism, scheduling, and resource usage. The optimization goal is often to find\nPareto solutions between different objectives or to satisfy pre-defined constraints. With IR analysis,\nthe employment of a random forest is able to select suitable loop unrolling factors to optimize\na weighted sum of execution latency and resource usage [ 259]. Prospector [ 155] uses Bayesian\noptimization to optimize placement of directives (loop unrolling/pipelining, array partitioning,\nfunction inlining, and allocation), aiming to find Pareto solutions between execution latency and\nresource utilization in FPGAs. IronMan [ 243] targets Pareto solutions between different resources\nwhile keeping the latency unchanged. It combines GNNs with RL to conduct a finer-grained design\nexploration in the operation level, pursuing optimal resource allocation strategies by optimizing\nassignments of the resource pragma.\nIn logic synthesis, RTL-designs or logic networks are represented by directed acyclic graphs\n(DAGs). The goal is to optimize logic networks subject to certain constraints. LSOracle [ 173]\nemploys an MLP to automatically decide which one of the two optimizers should be applied on\ndifferent parts of circuits. The logic optimization can be formulated as an RL problem solved by the\npolicy gradient [ 73] or the advantage actor-critic [ 81]: the state is the current status of a design;\nthe action is a transformation between two DAGs with equivalent I/O behaviors; the optimization\nobjective is to minimize area or delay of designs. Q-ALS [ 180] aims at approximate logic synthesis\nand embeds a Q-learning agent to determine the maximum tolerable error of each node in a DAG,\nsuch that the total error rates at primary outputs are bounded by pre-specified constraints.\nIn physical synthesis, placement optimization is a popular topic. 1‚óãTo optimize flip-flop place-\nment in clock networks, Wu et al. [240] apply a modified K-means clustering to group post-\nplacement flip-flops, and relocate these clusters by reducing the distance between flip-flops and\ntheir drivers while minimizing disruption of original placement results. To optimize clock tree\nsynthesis (CTS), Lu et al. [143] train a regression model that takes pre-CTS placement images\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:23\nand CTS configurations as inputs to predict post-CTS metrics (clock power, clock wirelength, and\nmaximum skew), which is used as the supervisor to guide the training of a conditional GAN, such\nthat the well-trained generator can recommend CTS configurations leading to optimized clock trees.\n2‚óãAiming at cell placement, a deep RL approach [ 161] is introduced to place macros (memory cells),\nafter which standard cells are placed by a force-directed method. This method is able to generalize\nto unseen netlists, and outperforms RePlAce [ 34] yet several times slower. DREAMPlace [ 135]\ncasts the analytical standard cell placement optimization into a neural network training problem,\nachieving over 30√óspeedup without quality degradation compared to RePlAce. NVCell [ 189] is\nan automated layout generator for standard cells, which employs RL to fix DRVs after placement\nand routing. 3‚óãML-based techniques demonstrates their versatility in many design automation\ntasks, such as post-silicon variation extraction by sparse Bayesian learning, and post-silicon timing\ntuning to mitigate the effects caused by process variation [274].\n5 DISCUSSION AND POTENTIAL DIRECTIONS\nIn this section, we discuss limitations and potentials of ML techniques for computer architecture and\nsystems, which span the entire development and deployment stack that involves data, algorithms,\nimplementation, and targets. We also envision that the application of ML techniques could be the\npropulsive force for hardware agile development .\n5.1 Bridging Data Gaps\nData are the backbone to ML, however, perfect datasets are sometimes non-available or prohibitively\nexpensive to obtain in computer architecture and system domain. Here, we would like to scrutinize\ntwo points, the gap between small data and big data, and non-perfect data. 1‚óãIn some EDA\nproblems, such as placement and routing in physical synthesis, the simulation or evaluation is\nextremely expensive [ 250], leading to data scarcity. As ML models usually require enough data\nto learn underlying statistics and make decisions, this gap between small data and big data often\nlimits the capability of ML-based techniques. There have been different attempts to bridge this\ngap. From the algorithm side, algorithms that can work with small data await to be developed,\nwhere one current technique is Bayesian optimization that is effective in small parameter space\n[108]; active learning [ 206], which significantly improve sample efficiency, may also be a cure\nto this problem. From the data side, generative methods can be used to generate synthetic data\n[49], mitigating data scarcity. 2‚óãRegarding non-perfect data, even if some EDA tools produce a\nlot of data, they are not always properly labeled nor presented in the form suitable to ML models.\nIn the absence of perfectly labeled training data, possible alternatives are to use unsupervised\nlearning, self-supervised learning [ 78], or to combine supervised with unsupervised techniques [ 5].\nMeanwhile, RL could be a workaround where training data can be generated on-the-fly.\n5.2 Developing Algorithms\nDespite the current achieved accomplishments, we are still expecting novel ML algorithms or\nschemes to further improve system modeling and optimization, with respect scalability, domain\nknowledge interpretability, etc.\nNew ML Schemes. Classical analytic-based methods usually adopt a bottom-up or top-down pro-\ncedure, encouraging ML-based techniques to distill hierarchical structures of systems/architecture.\nOne example is hierarchical RL [ 115] that has flexible goal specifications and learns goal-directed\nbehaviors in complex environments with sparse feedback. Such kind of models enables more flexi-\nble and effective multi-level design and control. Additionally, many system optimizations involve\nparticipation of multiple agents, such as NoC routing, which are naturally suitable to the realm\nof multi-agent RL [ 264]. These agents can be fully cooperative, fully competitive, or a mix of the\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:24 Wu and Xie.\ntwo, enabling versatility of system optimization. Another promising approach is self-supervised\nlearning [ 78], beneficial in both improving model robustness and mitigating data scarcity. While\napplying a single ML method solely has led to powerful results, hybrid methods, i.e., combining\ndifferent ML techniques or combining ML techniques with heuristics, unleash more opportunities.\nFor example, RL can be combined with genetic algorithms for hardware resource assignment [ 106].\nScalability. The system scaling-up poses challenges on scalability issues. From the algorithm\nside, multi-level techniques can help reduce the computation complexity, e.g., multi-level Q-learning\nfor DVFS [ 32,33,178]. One implicit workaround is to leverage transfer learning: the pre-training\nis a one-time cost, which can be amortized in each future use; the fine-tuning provides flexibility\nbetween a quick solution from the pre-trained model and a longer yet better one for a particular\ntask. Several examples [161, 205, 231] are discussed in Section 4.7.\nDomain Knowledge and Interpretability. Making better use of domain knowledge unveils\npossibilities to choose more proper models for different system problems and provide more in-\ntuitions or explanations of why and how these models work. By making analogy of semantics\nbetween memory access patterns/program languages and natural languages, the prefetching or\ncode generation problems can be modeled as NLP problems, as discussed in Section 4.1.1 and\nSection 4.6.1. By making analogy of graphical representations in many EDA problems, where data\nare intrinsically presented as graphs (e.g., circuits, logic netlists or IRs), GNNs are expected to be\npowerful in these fields [108]. Several examples are provided in Section 3.3 and Section 4.7.\n5.3 Improving Implementations and Deployments\nTo fully benefit from ML-based methods, we need to consider practical implementations, appropriate\nselection of deployment scenarios, and post-deployment model maintenance.\nBetter Implementations. To enable practical implementations of ML-based techniques, im-\nprovement can be made from either the model side or software/hardware co-design [ 215]. From\nthe model level, network pruning and model compression reduce the number of operations and\nmodel size [ 76]; weight quantization improves computation efficiency by reducing the precision\nof operations/operands [ 86]. From the co-design level, strategies that have been used for DNN\nacceleration could also be used in applying ML for system.\nAppropriate Scenarios: online vs. offline. When deploying ML-based techniques for system\ndesigns, it is crucial to deliberate design constraints under different scenarios. Generally, existing\nwork falls into two categories. 1‚óãML-based techniques are deployed online or during runtime,\nno matter the training phase is performed online or offline. Obviously, the model complexity and\nruntime overhead are often strictly limited by specific constraints, e.g., power/energy, timing/latency,\narea, etc. To take one more step, if the online training/learning is further desired, the design\nconstraint will be more stringent. One promising approach is to employ semi-online learning\nmodels, which have been applied to solve some classical combinatorial optimization problems, such\nas bipartite matching [ 116] and caching [ 117]. These models enable smooth interpolation between\nthe best possible online and offline training algorithms. 2‚óãML-based techniques are applied offline,\nwhich often refers to architectural design space exploration. Such problems leverage ML-based\ntechniques to guide system implementation, and once the designing phase is completed, ML models\nwill not be invoked again. Thus, the offline applications can tolerate relatively higher overheads.\nModel Maintenance. In the case of offline training and online deployment, ML models employed\nfor computer architecture domain, as in other scenarios, require regular maintenance and updating\nto meet performance expectations, since workload variations over time and hardware aging often\ncause data drift or concept drift [ 222]. To proactively circumvent performance degradation of\nML models, some measures could be taken during post-deployment periods. 1‚óãML models can\nbe retrained either at a regular interval or when key performance indicators are below certain\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:25\nthresholds. Retraining models regularly, regardless of their performance, is a more direct way,\nbut it requires a clear understanding of how frequently a model should be updated under its own\nscenario. The model performance will decline if retraining intervals are too spaced out in the\ninterim. Monitoring key performance indicators relies on a comprehensive panel of measurements\nthat explicitly demonstrate model drift, whereas this may introduce additional hardware/software\noverhead and incorrect selection of measurements often defeats the intention of this method. 2‚óã\nDuring the retraining of ML models, there is often a trade-off between newly collected data and\nprevious data. Properly assigning importance of input data would improve retraining efficacy [ 25].\n5.4 Supporting Non-homogeneous Tasks\nML-based techniques are supposed to be applicable in both current architectures and emerging\nsystems, leading to long-term advancement in computer architecture and systems.\nNon-homogeneous Components. Design and development for computer architectures are\noften based upon earlier-generation architectures of similar purpose, but commonly rely on next-\ngeneration hardware components that were not present in earlier generations. Examples include\nemployment of new device nodes with technology scaling, and replacement of conventional con-\nstituents in memory systems with NVM- or PIM-based components. In addition to the heterogeneity\nof components from different generations, one architecture or system usually consists of both\nstandard parts from library and specialized/customized hardware components. This provides the\nmotivation that ML-assisted architectures/systems should have the flexibility to transfer among\ndifferent-generation components, and to support standard and specialized parts simultaneously.\nNon-homogeneous Applications. In computer architecture and system design, some issues are\nuniversal, while others may arise with the advent of new architecture/systems and new workloads.\n1‚óãFor evergreen design areas, several examples include caching in hardware/software/data centers\n(Section 4.1.1 and Section 4.5), resource management and task allocation in single/multi-/many-core\nCPUs and heterogeneous systems (Section 4.4), NoC design under various scenarios (Section\n4.3), etc. 2‚óãFor problems aroused from new systems/workloads, transfer learning and meta-\nlearning [ 174,225] could be helpful in either exploring new heuristics or directly deriving design\nmethodology. For example, combining meta-learning with RL [ 61] allows training a \"meta\" agent\nthat is designed to adapt to a specific workload with only a few observations.\n5.5 Facilitating General Tool Design and Hardware Agile Development\nEven though ML-based modeling significantly reduces the evaluation cost during design iteration,\nmaking great strides towards the landing of hardware agile development, there is still a long\nway to go in the ML-based design methodology prospective. One ultimate goal might be the fully\nautomated design, which should entangle two core capabilities: holistic optimization in system-wise,\nand easy migration across different systems, to enable rapid and agile hardware design.\nHolistic Optimization. Fueled by recent advancements, ML techniques have been increasingly\nexplored and exploited in computer system design and optimization [ 47]. The target problems that\nawait further endeavors could be multi-objective optimizations under highly constrained situations,\nor optimizing several components in a system simultaneously. We envisage an ML-based system-\nwise and holistic framework with a panoramic vision: it should be able to leverage information from\ndifferent levels of systems in synergy, so that it could thoroughly characterize system behaviors\nas well as their intrinsically hierarchical abstractions; it should also be able to make decisions in\ndifferent granularity, so that it could control and improve systems precisely and comprehensively.\nPortable, Rapid, and Agile. Striving for portable, rapid, and agile hardware design, there are\ntwo potential directions. 1‚óãThe well-designed interfaces between systems/architectures and ML-\nbased techniques would facilitate the portability across different platforms, since ML models can\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:26 Wu and Xie.\nperform well without explicit descriptions of the target domain. 2‚óãThe proliferation of ML-based\ntechniques have more or less transformed the workflow of design automation, directly driving\nrapid and agile hardware design. We expect GNNs make better use of naturally graphical data in\nEDA field; we expect deep RL be a powerful and general-purpose tool for many EDA optimization\nproblems, especially when the exact heuristic or objective is obscure; we expect these ML-based\ndesign automation tools enhance designers‚Äô productivity and thrive in the community.\n6 CONCLUSION\nThe flourishing of ML would be retarded without the great systems and powerful architectures\nsupportive to run these algorithms at scale. Now, it is the time to return the favor and let ML\ntransform the way that computer architecture and systems are designed. Existing work that\napplies ML for computer architecture/systems roughly falls into two categories: ML-based fast\nmodeling that involves performance metrics or some other criteria of interest, and ML-based design\nmethodology that directly leverages ML as the design tool. We hope to see the virtuous cycle,\nin which ML-based techniques are efficiently running on the most powerful computers with the\npursuit of designing the next generation computers. We hope ML-based techniques could be the\nimpetus to the revolution of computer architecture and systems.\nREFERENCES\n[1]Nevine AbouGhazaleh, Alexandre Ferreira, Cosmin Rusu, Ruibin Xu, Frank Liberato, Bruce Childers, Daniel Mosse,\nand Rami Melhem. 2007. Integrated CPU and L2 cache voltage scaling using machine learning. In ACM SIGPLAN\nNotices , Vol. 42. ACM, 41‚Äì50.\n[2]Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan, Shreyan Gupta, Hongzi Mao, and Mohammad Alizadeh.\n2018. Placeto: Efficient Progressive Device Placement Optimization. In NIPS Machine Learning for Systems Workshop .\n[3]Nitish Agarwal, Tulsi Jain, and Mohamed Zahran. 2019. Performance Prediction for Multi-threaded Applications. In\nInternational Workshop on AI-assisted Design for Architecture (AIDArc), held in conjunction with ISCA .\n[4]Byung Hoon Ahn, Prannoy Pilligundla, and Hadi Esmaeilzadeh. 2019. Reinforcement Learning and Adaptive Sampling\nfor Optimized DNN Compilation. arXiv preprint arXiv:1905.12799 (2019).\n[5]Mohamad Alawieh, Fa Wang, and Xin Li. 2017. Efficient hierarchical performance modeling for integrated circuits\nvia bayesian co-learning. In Proceedings of the 54th Annual Design Automation Conference 2017 . ACM, 9.\n[6]Mohamed Baker Alawieh, Wuxi Li, Yibo Lin, Love Singhal, Mahesh A Iyer, and David Z Pan. 2020. High-definition\nrouting congestion prediction for large-scale FPGAs. In 2020 25th Asia and South Pacific Design Automation Conference\n(ASP-DAC) . IEEE, 26‚Äì31.\n[7]Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for\nbig code and naturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 1‚Äì37.\n[8]Naomi S Altman. 1992. An introduction to kernel and nearest-neighbor nonparametric regression. The American\nStatistician 46, 3 (1992), 175‚Äì185.\n[9]Newsha Ardalani, Clint Lestourgeon, Karthikeyan Sankaralingam, and Xiaojin Zhu. 2015. Cross-architecture per-\nformance prediction (XAPP) using CPU code to predict GPU performance. In Proceedings of the 48th International\nSymposium on Microarchitecture . ACM, 725‚Äì737.\n[10] Newsha Ardalani, Urmish Thakker, Aws Albarghouthi, and Karu Sankaralingam. 2019. A Static Analysis-based\nCross-Architecture Performance Prediction Using Machine Learning. arXiv preprint arXiv:1906.07840 (2019).\n[11] Yuxin Bai, Victor W Lee, and Engin Ipek. 2017. Voltage regulator efficiency aware power management. ACM SIGOPS\nOperating Systems Review 51, 2 (2017), 825‚Äì838.\n[12] Peter E Bailey, David K Lowenthal, Vignesh Ravi, Barry Rountree, Martin Schulz, and Bronis R De Supinski. 2014.\nAdaptive configuration selection for power-constrained heterogeneous systems. In 2014 43rd International Conference\non Parallel Processing . IEEE, 371‚Äì380.\n[13] Bharathan Balaji, Christopher Kakovitch, and Balakrishnan Narayanaswamy. 2020. FirePlace: Placing FireCracker\nVirtual Machines with Hindsight Imitation. In ML for Systems Workshop at NeurIPS 2020 .\n[14] Ioana Baldini, Stephen J Fink, and Erik Altman. 2014. Predicting gpu performance from cpu runs using machine\nlearning. In 2014 IEEE 26th International Symposium on Computer Architecture and High Performance Computing . IEEE,\n254‚Äì261.\n[15] Luiz Andr√© Barroso, Urs H√∂lzle, and Parthasarathy Ranganathan. 2018. The datacenter as a computer: Designing\nwarehouse-scale machines. Synthesis Lectures on Computer Architecture 13, 3 (2018), i‚Äì189.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:27\n[16] Nathan Beckmann and Daniel Sanchez. 2017. Maximizing cache performance under uncertainty. In 2017 IEEE\nInternational Symposium on High Performance Computer Architecture (HPCA) . IEEE, 109‚Äì120.\n[17] Eshan Bhatia, Gino Chacon, Seth Pugsley, Elvira Teran, Paul V Gratz, and Daniel A Jim√©nez. 2019. Perceptron-based\nprefetch filtering. In Proceedings of the 46th International Symposium on Computer Architecture . ACM, 1‚Äì13.\n[18] Nathan Binkert, Bradford Beckmann, Gabriel Black, Steven K Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness,\nDerek R Hower, Tushar Krishna, Somayeh Sardashti, et al .2011. The gem5 simulator. ACM SIGARCH Computer\nArchitecture News 39, 2 (2011), 1‚Äì7.\n[19] Ramazan Bitirgen, Engin Ipek, and Jose F Martinez. 2008. Coordinated management of multiple interacting re-\nsources in chip multiprocessors: A machine learning approach. In 2008 41st IEEE/ACM International Symposium on\nMicroarchitecture . IEEE, 318‚Äì329.\n[20] Ronald D Blanton, Xin Li, Ken Mai, Diana Marculescu, Radu Marculescu, Jeyanandh Paramesh, Jeff Schneider,\nand Donald E Thomas. 2015. Statistical learning in chip (SLIC). In 2015 IEEE/ACM International Conference on\nComputer-Aided Design (ICCAD) . IEEE, 664‚Äì669.\n[21] OpenAI Blog. 2018. Al and Compute . https://openai.com/blog/ai-and-compute/\n[22] Shekhar Borkar. 2013. Exascale computing‚Äìa fact or affliction. Keynote presentation at IPDPS 10 (2013).\n[23] Justin A Boyan and Michael L Littman. 1994. Packet routing in dynamically changing networks: A reinforcement\nlearning approach. In Advances in neural information processing systems . 671‚Äì678.\n[24] Peter Braun and Heiner Litz. 2019. Understanding Memory Access Patterns for Prefetching. In International Workshop\non AI-assisted Design for Architecture (AIDArc), held in conjunction with ISCA .\n[25] Jonathon Byrd and Zachary Lipton. 2019. What is the effect of importance weighting in deep learning?. In International\nConference on Machine Learning . PMLR, 872‚Äì881.\n[26] Brad Calder, Dirk Grunwald, Michael Jones, Donald Lindsay, James Martin, Michael Mozer, and Benjamin Zorn. 1997.\nEvidence-based static branch prediction using machine learning. ACM Transactions on Programming Languages and\nSystems (TOPLAS) 19, 1 (1997), 188‚Äì222.\n[27] Rodrigo N Calheiros, Enayat Masoumi, Rajiv Ranjan, and Rajkumar Buyya. 2014. Workload prediction using ARIMA\nmodel and its impact on cloud applications‚Äô QoS. IEEE Transactions on Cloud Computing 3, 4 (2014), 449‚Äì458.\n[28] Wei-Ting J Chan, Yang Du, Andrew B Kahng, Siddhartha Nath, and Kambiz Samadi. 2016. BEOL stack-aware\nroutability prediction from placement using data mining techniques. In 2016 IEEE 34th International Conference on\nComputer Design (ICCD) . IEEE, 41‚Äì48.\n[29] Jingsong Chen, Jian Kuang, Guowei Zhao, Dennis J-H Huang, and Evangeline FY Young. 2020. PROS: A plug-in for\nroutability optimization applied in the state-of-the-art commercial eda tool using deep learning. In 2020 IEEE/ACM\nInternational Conference On Computer Aided Design (ICCAD) . IEEE, 1‚Äì8.\n[30] Li Chen, Justinas Lingys, Kai Chen, and Feng Liu. 2018. Auto: Scaling deep reinforcement learning for datacenter-\nscale automatic traffic optimization. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data\nCommunication . 191‚Äì205.\n[31] Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-to-tree neural networks for program translation. Advances in\nneural information processing systems 31 (2018), 2547‚Äì2557.\n[32] Zhuo Chen and Diana Marculescu. 2015. Distributed reinforcement learning for power limited many-core system\nperformance optimization. In Proceedings of the 2015 Design, Automation & Test in Europe Conference & Exhibition .\nEDA Consortium, 1521‚Äì1526.\n[33] Zhuo Chen, Dimitrios Stamoulis, and Diana Marculescu. 2017. Profit: priority and power/performance optimization\nfor many-core systems. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 37, 10 (2017),\n2064‚Äì2075.\n[34] Chung-Kuan Cheng, Andrew B Kahng, Ilgweon Kang, and Lutong Wang. 2018. Replace: Advancing solution quality\nand routability validation in global placement. IEEE Transactions on Computer-Aided Design of Integrated Circuits and\nSystems 38, 9 (2018), 1717‚Äì1730.\n[35] Mark Clark, Avinash Kodi, Razvan Bunescu, and Ahmed Louri. 2018. LEAD: Learning-enabled energy-aware dynamic\nvoltage/frequency scaling in NoCs. In Proceedings of the 55th Annual Design Automation Conference . ACM, 82.\n[36] Ryan Cochran, Can Hankendi, Ayse K Coskun, and Sherief Reda. 2011. Pack & Cap: adaptive DVFS and thread\npacking under power caps. In 2011 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO) .\nIEEE, 175‚Äì185.\n[37] Katherine E Coons, Behnam Robatmili, Matthew E Taylor, Bertrand A Maher, Doug Burger, and Kathryn S McKinley.\n2008. Feature selection and policy optimization for distributed instruction placement using reinforcement learning.\nInProceedings of the 17th international conference on Parallel architectures and compilation techniques . ACM, 32‚Äì42.\n[38] Eli Cortez, Anand Bonde, Alexandre Muzio, Mark Russinovich, Marcus Fontoura, and Ricardo Bianchini. 2017. Resource\ncentral: Understanding and predicting workloads for improved resource management in large cloud platforms. In\nProceedings of the 26th Symposium on Operating Systems Principles . 153‚Äì167.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:28 Wu and Xie.\n[39] Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather. 2017. End-to-end deep learning of optimization\nheuristics. In 2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT) . IEEE,\n219‚Äì232.\n[40] Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather. 2017. Synthesizing benchmarks for predictive\nmodeling. In 2017 IEEE/ACM International Symposium on Code Generation and Optimization (CGO) . IEEE, 86‚Äì99.\n[41] Matthew Curtis-Maury, Ankur Shah, Filip Blagojevic, Dimitrios S Nikolopoulos, Bronis R De Supinski, and Martin\nSchulz. 2008. Prediction models for multi-dimensional power-performance optimization on many cores. In Proceedings\nof the 17th international conference on Parallel architectures and compilation techniques . 250‚Äì259.\n[42] Dong Dai, Forrest Sheng Bao, Jiang Zhou, and Yong Chen. 2016. Block2vec: A deep learning strategy on mining block\ncorrelations in storage systems. In 2016 45th International Conference on Parallel Processing Workshops (ICPPW) . IEEE,\n230‚Äì239.\n[43] Steve Dai, Yuan Zhou, Hang Zhang, Ecenur Ustun, Evangeline FY Young, and Zhiru Zhang. 2018. Fast and accurate\nestimation of quality of results in high-level synthesis with machine learning. In 2018 IEEE 26th Annual International\nSymposium on Field-Programmable Custom Computing Machines (FCCM) . IEEE, 129‚Äì132.\n[44] Sourav Das, Janardhan Rao Doppa, Dae Hyun Kim, Partha Pratim Pande, and Krishnendu Chakrabarty. 2015. Optimiz-\ning 3D NoC design for energy efficiency: A machine learning approach. In Proceedings of the IEEE/ACM International\nConference on Computer-Aided Design . IEEE Press, 705‚Äì712.\n[45] Sourav Das, Janardhan Rao Doppa, Partha Pratim Pande, and Krishnendu Chakrabarty. 2016. Energy-efficient and\nreliable 3D Network-on-Chip (NoC): Architectures and optimization algorithms. In Proceedings of the 35th International\nConference on Computer-Aided Design . ACM, 57.\n[46] Bhavya K Daya, Li-Shiuan Peh, and Anantha P Chandrakasan. 2016. Quest for high-performance bufferless NoCs with\nsingle-cycle express paths and self-learning throttling. In 2016 53nd ACM/EDAC/IEEE Design Automation Conference\n(DAC) . IEEE, 1‚Äì6.\n[47] Jeffrey Dean. 2020. 1.1 the deep learning revolution and its implications for computer architecture and chip design. In\n2020 IEEE International Solid-State Circuits Conference-(ISSCC) . IEEE, 8‚Äì14.\n[48] Zhaoxia Deng, Lunkai Zhang, Nikita Mishra, Henry Hoffmann, and Frederic T Chong. 2017. Memory cocktail therapy:\na general learning-based framework to optimize dynamic tradeoffs in NVMs. In Proceedings of the 50th Annual\nIEEE/ACM International Symposium on Microarchitecture . ACM, 232‚Äì244.\n[49] Yi Ding, Nikita Mishra, and Henry Hoffmann. 2019. Generative and multi-phase learning for computer systems\noptimization. In Proceedings of the 46th International Symposium on Computer Architecture . ACM, 39‚Äì52.\n[50] Dominic DiTomaso, Travis Boraten, Avinash Kodi, and Ahmed Louri. 2016. Dynamic error mitigation in NoCs using\nintelligent prediction techniques. In The 49th Annual IEEE/ACM International Symposium on Microarchitecture . IEEE\nPress, 31.\n[51] Dominic DiTomaso, Ashif Sikder, Avinash Kodi, and Ahmed Louri. 2017. Machine learning enabled power-aware\nnetwork-on-chip design. In Proceedings of the Conference on Design, Automation & Test in Europe . European Design\nand Automation Association, 1354‚Äì1359.\n[52] Xiangyu Dong, Norman P Jouppi, and Yuan Xie. 2013. A circuit-architecture co-optimization framework for exploring\nnonvolatile memory hierarchies. ACM Transactions on Architecture and Code Optimization (TACO) 10, 4 (2013), 23.\n[53] Christophe Dubach, Timothy M Jones, Edwin V Bonilla, and Michael FP O‚ÄôBoyle. 2010. A predictive model for dynamic\nmicroarchitectural adaptivity control. In Proceedings of the 2010 43rd Annual IEEE/ACM International Symposium on\nMicroarchitecture . IEEE Computer Society, 485‚Äì496.\n[54] Masoumeh Ebrahimi, Masoud Daneshtalab, Fahimeh Farahnakian, Juha Plosila, Pasi Liljeberg, Maurizio Palesi, and\nHannu Tenhunen. 2012. HARAQ: Congestion-aware learning model for highly adaptive routing algorithm in on-chip\nnetworks. In 2012 IEEE/ACM Sixth International Symposium on Networks-on-Chip . IEEE, 19‚Äì26.\n[55] Hadi Esmaeilzadeh, Adrian Sampson, Luis Ceze, and Doug Burger. 2012. Neural acceleration for general-purpose\napproximate programs. In Proceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture .\nIEEE Computer Society, 449‚Äì460.\n[56] S√©rgio Esteves, Helena Galhardas, and Lu√≠s Veiga. 2018. Adaptive Execution of Continuous and Data-intensive\nWorkflows with Machine Learning. In Proceedings of the 19th International Middleware Conference . 239‚Äì252.\n[57] Stijn Eyerman, Kenneth Hoste, and Lieven Eeckhout. 2011. Mechanistic-empirical processor performance modeling\nfor constructing CPI stacks on real hardware. In (IEEE ISPASS) IEEE International Symposium on Performance Analysis\nof Systems and Software . IEEE, 216‚Äì226.\n[58] Alexandra Fedorova, David Vengerov, and Daniel Doucette. 2007. Operating system scheduling on heterogeneous\ncore systems. In Proceedings of the Workshop on Operating System Support for Heterogeneous Multicore Architectures .\n[59] Chaochao Feng, Zhonghai Lu, Axel Jantsch, Jinwen Li, and Minxuan Zhang. 2010. A reconfigurable fault-tolerant\ndeflection routing algorithm based on reinforcement learning for network-on-chip. In Proceedings of the Third\nInternational Workshop on Network on Chip Architectures . ACM, 11‚Äì16.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:29\n[60] Quintin Fettes, Mark Clark, Razvan Bunescu, Avinash Karanth, and Ahmed Louri. 2019. Dynamic Voltage and\nFrequency Scaling in NoCs with Supervised and Reinforcement Learning Techniques. IEEE Trans. Comput. 68, 3\n(2019).\n[61] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. In International Conference on Machine Learning . PMLR, 1126‚Äì1135.\n[62] Jerome H Friedman. 1991. Multivariate adaptive regression splines. The annals of statistics (1991), 1‚Äì67.\n[63] Cheng Fu, Huili Chen, Haolan Liu, Xinyun Chen, Yuandong Tian, Farinaz Koushanfar, and Jishen Zhao. 2019. Coda:\nAn End-to-End Neural Program Decompiler. In Advances in Neural Information Processing Systems . 3703‚Äì3714.\n[64] Archana Ganapathi, Kaushik Datta, Armando Fox, and David Patterson. 2009. A case for machine learning to optimize\nmulticore performance. In Proceedings of the First USENIX conference on Hot topics in parallelism . USENIX Association\nBerkeley, CA, 1‚Äì1.\n[65] Jim Gao. 2014. Machine learning applications for data center optimization. (2014).\n[66] Yuanxiang Gao, Li Chen, and Baochun Li. 2018. Spotlight: Optimizing device placement for training deep neural\nnetworks. In International Conference on Machine Learning . 1662‚Äì1670.\n[67] Elba Garza, Samira Mirbagher-Ajorpaz, Tahsin Ahmad Khan, and Daniel A Jim√©nez. 2019. Bit-level perceptron\nprediction for indirect branches. In Proceedings of the 46th International Symposium on Computer Architecture . ACM,\n27‚Äì38.\n[68] Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian data\nanalysis . CRC press.\n[69] Faustino J Gomez, Doug Burger, and Risto Miikkulainen. 2001. A neuro-evolution method for dynamic resource\nallocation on a chip multiprocessor. In IJCNN‚Äô01. International Joint Conference on Neural Networks. Proceedings (Cat.\nNo. 01CH37222) , Vol. 4. IEEE, 2355‚Äì2360.\n[70] Zhenhuan Gong, Xiaohui Gu, and John Wilkes. 2010. Press: Predictive elastic resource scaling for cloud systems. In\n2010 International Conference on Network and Service Management . Ieee, 9‚Äì16.\n[71] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning . MIT press.\n[72] David Ha, Andrew Dai, and Quoc V Le. 2016. Hypernetworks. arXiv preprint arXiv:1609.09106 (2016).\n[73] Winston Haaswijk, Edo Collins, Benoit Seguin, Mathias Soeken, Fr√©d√©ric Kaplan, Sabine S√ºsstrunk, and Giovanni\nDe Micheli. 2018. Deep learning for logic optimization algorithms. In 2018 IEEE International Symposium on Circuits\nand Systems (ISCAS) . IEEE, 1‚Äì4.\n[74] Kourosh Hakhamaneshi, Nick Werblun, Pieter Abbeel, and Vladimir Stojanoviƒá. 2019. Bagnet: Berkeley analog\ngenerator with layout optimizer boosted with deep neural networks. In 2019 IEEE/ACM International Conference on\nComputer-Aided Design (ICCAD) . IEEE, 1‚Äì8.\n[75] Greg Hamerly, Charles Elkan, et al .2001. Bayesian approaches to failure prediction for disk drives. In ICML , Vol. 1.\n202‚Äì209.\n[76] Song Han, Huizi Mao, and William J Dally. 2016. Deep compression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. In International Conference on Learning Representations .\n[77] Milad Hashemi, Kevin Swersky, Jamie Smith, Grant Ayers, Heiner Litz, Jichuan Chang, Christos Kozyrakis, and\nParthasarathy Ranganathan. 2018. Learning Memory Access Patterns. In International Conference on Machine Learning .\n1924‚Äì1933.\n[78] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. 2019. Using self-supervised learning can improve\nmodel robustness and uncertainty. arXiv preprint arXiv:1906.12340 (2019).\n[79] Henry Hoffmann. 2015. JouleGuard: energy guarantees for approximate applications. In Proceedings of the 25th\nSymposium on Operating Systems Principles . 198‚Äì214.\n[80] David W Hosmer Jr, Stanley Lemeshow, and Rodney X Sturdivant. 2013. Applied logistic regression . Vol. 398. John\nWiley & Sons.\n[81] Abdelrahman Hosny, Soheil Hashemi, Mohamed Shalan, and Sherief Reda. 2020. Drills: Deep reinforcement learning\nfor logic synthesis. In 2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC) . IEEE, 581‚Äì586.\n[82] Connor Imes, Steven Hofmeyr, and Henry Hoffmann. 2018. Energy-efficient application resource scheduling using\nmachine learning classifiers. In Proceedings of the 47th International Conference on Parallel Processing . ACM, 45.\n[83] Engin √èpek, Sally A McKee, Rich Caruana, Bronis R de Supinski, and Martin Schulz. 2006. Efficiently exploring\narchitectural design spaces via predictive modeling . Vol. 41. ACM.\n[84] E Ipek, O Mutlu, JF Martinez, and R Caruana. 2008. Self-Optimizing Memory Controllers: A Reinforcement Learning\nApproach. In 2008 International Symposium on Computer Architecture (ISCA) . ACM.\n[85] Sadeka Islam, Jacky Keung, Kevin Lee, and Anna Liu. 2012. Empirical prediction models for adaptive resource\nprovisioning in the cloud. Future Generation Computer Systems 28, 1 (2012), 155‚Äì162.\n[86] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and\nDmitry Kalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:30 Wu and Xie.\ninference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2704‚Äì2713.\n[87] Ajay Jain and Saman Amarasinghe. 2019. Learning automatic schedulers with projective reparameterization. In\nProceedings of the ML-for-Systems Workshop at the 46th International Symposium on Computer Architecture (ISCA‚Äô19) .\n[88] Anil K Jain. 2010. Data clustering: 50 years beyond K-means. Pattern recognition letters 31, 8 (2010), 651‚Äì666.\n[89] Rahul Jain, Preeti Ranjan Panda, and Sreenivas Subramoney. 2016. Machine learned machines: adaptive co-optimization\nof caches, cores, and on-chip network. In 2016 Design, Automation & Test in Europe Conference & Exhibition (DATE) .\nIEEE, 253‚Äì256.\n[90] Wenhao Jia, Kelly A Shaw, and Margaret Martonosi. 2012. Stargazer: Automated regression-based GPU design space\nexploration. In 2012 IEEE International Symposium on Performance Analysis of Systems & Software . IEEE, 2‚Äì13.\n[91] Daniel A Jim√©nez. 2003. Fast path-based neural branch prediction. In Proceedings. 36th Annual IEEE/ACM International\nSymposium on Microarchitecture, 2003. MICRO-36. IEEE, 243‚Äì252.\n[92] Daniel A Jim√©nez. 2005. Piecewise linear branch prediction. In 32nd International Symposium on Computer Architecture\n(ISCA‚Äô05) . IEEE, 382‚Äì393.\n[93] Daniel A Jim√©nez. 2011. An optimized scaled neural branch predictor. In 2011 IEEE 29th International Conference on\nComputer Design (ICCD) . IEEE, 113‚Äì118.\n[94] Daniel A Jim√©nez. 2016. Multiperspective perceptron predictor. Championship Branch Prediction (CBP-5) (2016).\n[95] Daniel A Jim√©nez and Calvin Lin. 2001. Dynamic branch prediction with perceptrons. In Proceedings HPCA Seventh\nInternational Symposium on High-Performance Computer Architecture . IEEE, 197‚Äì206.\n[96] Daniel A Jim√©nez and Elvira Teran. 2017. Multiperspective reuse prediction. In 2017 50th Annual IEEE/ACM Interna-\ntional Symposium on Microarchitecture (MICRO) . IEEE, 436‚Äì448.\n[97] Biresh Kumar Joardar, Ryan Gary Kim, Janardhan Rao Doppa, Partha Pratim Pande, Diana Marculescu, and Radu\nMarculescu. 2018. Learning-based Application-Agnostic 3D NoC Design for Heterogeneous Manycore Systems. IEEE\nTrans. Comput. 68, 6 (2018), 852‚Äì866.\n[98] Albin Jones, Amir Yazdanbakhsh, Berkin Akin, Christof Angermueller, James Pierce Laudon, Kevin Swersky, Milad\nHashemi, Ravi Narayanaswami, Sat Chatterjee, and Yanqi Zhou. 2020. Apollo: Transferable Architecture Exploration.\nInML for Systems Workshop at NeurIPS 2020 .\n[99] Ali Jooya, Nikitas Dimopoulos, and Amirali Baniasadi. 2016. MultiObjective GPU design space exploration optimiza-\ntion. In 2016 International Conference on High Performance Computing & Simulation (HPCS) . IEEE, 659‚Äì666.\n[100] PJ Joseph, Kapil Vaswani, and Matthew J Thazhuthaveetil. 2006. Construction and use of linear regression models for\nprocessor performance analysis. In The Twelfth International Symposium on High-Performance Computer Architecture,\n2006. IEEE, 99‚Äì108.\n[101] PJ Joseph, Kapil Vaswani, and Matthew J Thazhuthaveetil. 2006. A predictive performance model for superscalar\nprocessors. In 2006 39th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO‚Äô06) . IEEE, 161‚Äì170.\n[102] Da-Cheng Juan, Siddharth Garg, Jinpyo Park, and Diana Marculescu. 2013. Learning the optimal operating point\nfor many-core systems with extended range voltage/frequency scaling. In Proceedings of the Ninth IEEE/ACM/IFIP\nInternational Conference on Hardware/Software Codesign and System Synthesis . IEEE Press, 8.\n[103] Da-Cheng Juan and Diana Marculescu. 2012. Power-aware performance increase via core/uncore reinforcement\ncontrol for chip-multiprocessors. In Proceedings of the 2012 ACM/IEEE international symposium on Low power electronics\nand design . 97‚Äì102.\n[104] Wonkyung Kang, Dongkun Shin, and Sungjoo Yoo. 2017. Reinforcement learning-assisted garbage collection to\nmitigate long-tail latency in SSD. ACM Transactions on Embedded Computing Systems (TECS) 16, 5s (2017), 1‚Äì20.\n[105] Wonkyung Kang and Sungjoo Yoo. 2018. Dynamic management of key states for reinforcement learning-assisted\ngarbage collection to reduce long tail latency in SSD. In 2018 55th ACM/ESDA/IEEE Design Automation Conference\n(DAC) . IEEE, 1‚Äì6.\n[106] Sheng-Chun Kao, Geonhwa Jeong, and Tushar Krishna. 2020. ConfuciuX: Autonomous Hardware Resource Assign-\nment for DNN Accelerators using Reinforcement Learning. In 2020 53rd Annual IEEE/ACM International Symposium\non Microarchitecture (MICRO) . IEEE, 622‚Äì636.\n[107] Shauharda Khadka, Estelle Aflalo, Mattias Marder, Avrech Ben-David, Santiago Miret, Hanlin Tang, Shie Mannor,\nTamir Hazan, and Somdeb Majumdar. 2020. Optimizing Memory Placement using Evolutionary Graph Reinforcement\nLearning. arXiv preprint arXiv:2007.07298 (2020).\n[108] Brucek Khailany, Haoxing Ren, Steve Dai, Saad Godil, Ben Keller, Robert Kirby, Alicia Klinefelter, Rangharajan\nVenkatesan, Yanqing Zhang, Bryan Catanzaro, et al .2020. Accelerating chip design with machine learning. IEEE\nMicro 40, 6 (2020), 23‚Äì32.\n[109] Arijit Khan, Xifeng Yan, Shu Tao, and Nikos Anerousis. 2012. Workload characterization and prediction in the cloud:\nA multiple time series approach. In 2012 IEEE Network Operations and Management Symposium . IEEE, 1287‚Äì1294.\n[110] Salman Khan, Polychronis Xekalakis, John Cavazos, and Marcelo Cintra. 2007. Using predictivemodeling for cross-\nprogram design space exploration in multicore systems. In 16th International Conference on Parallel Architecture and\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:31\nCompilation Techniques (PACT 2007) . IEEE, 327‚Äì338.\n[111] Yonghae Kim and Hyesoon Kim. 2019. A Case Study: Exploiting Neural Machine Translation to Translate CUDA to\nOpenCL. arXiv preprint arXiv:1905.07653 (2019).\n[112] David Koeplinger, Raghu Prabhakar, Yaqi Zhang, Christina Delimitrou, Christos Kozyrakis, and Kunle Olukotun.\n2016. Automatic generation of efficient accelerators for reconfigurable hardware. In 2016 ACM/IEEE 43rd Annual\nInternational Symposium on Computer Architecture (ISCA) . Ieee, 115‚Äì127.\n[113] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018. The case for learned index structures.\nInProceedings of the 2018 International Conference on Management of Data . ACM, 489‚Äì504.\n[114] Sameer Kulkarni and John Cavazos. 2012. Mitigating the compiler optimization phase-ordering problem using machine\nlearning. In Proceedings of the ACM international conference on Object oriented programming systems languages and\napplications . 147‚Äì162.\n[115] Tejas D Kulkarni, Karthik R Narasimhan, Ardavan Saeedi, and Joshua B Tenenbaum. 2016. Hierarchical deep rein-\nforcement learning: integrating temporal abstraction and intrinsic motivation. In Proceedings of the 30th International\nConference on Neural Information Processing Systems . 3682‚Äì3690.\n[116] Ravi Kumar, Manish Purohit, Aaron Schild, Zoya Svitkina, and Erik Vee. 2019. Semi-Online Bipartite Matching. In\nProceedings of the 10th Innovations in Theoretical Computer Science Conference, ITCS .\n[117] Ravi Kumar, Manish Purohit, Zoya Svitkina, and Erik Vee. 2020. Interleaved Caching with Access Graphs. In\nProceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms . SIAM, 1846‚Äì1858.\n[118] Shailesh Kumar and Risto Miikkulainen. 1997. Dual reinforcement Q-routing: An on-line adaptive routing algorithm.\nInProceedings of the artificial neural networks in engineering Conference . 231‚Äì238.\n[119] Jihye Kwon and Luca P Carloni. 2020. Transfer Learning for Design-Space Exploration with High-Level Synthesis. In\nProceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD . 163‚Äì168.\n[120] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised Translation of\nProgramming Languages. arXiv preprint arXiv:2006.03511 (2020).\n[121] Benjamin C Lee and David M Brooks. 2006. Accurate and efficient regression modeling for microarchitectural\nperformance and power prediction. In ACM SIGOPS Operating Systems Review , Vol. 40. ACM, 185‚Äì194.\n[122] Benjamin C Lee and David M Brooks. 2007. Illustrative design space studies with microarchitectural regression\nmodels. In 2007 IEEE 13th International Symposium on High Performance Computer Architecture . IEEE, 340‚Äì351.\n[123] Benjamin C Lee, David M Brooks, Bronis R de Supinski, Martin Schulz, Karan Singh, and Sally A McKee. 2007.\nMethods of inference and learning for performance modeling of parallel applications. In Proceedings of the 12th ACM\nSIGPLAN symposium on Principles and practice of parallel programming . ACM, 249‚Äì258.\n[124] Benjamin C Lee, Jamison Collins, Hong Wang, and David Brooks. 2008. CPR: Composable performance regres-\nsion for scalable multiprocessor models. In Proceedings of the 41st annual IEEE/ACM International Symposium on\nMicroarchitecture . IEEE Computer Society, 270‚Äì281.\n[125] Bowen Li and Paul D Franzon. 2016. Machine learning in physical design. In 2016 IEEE 25th conference on electrical\nperformance of electronic packaging and systems (EPEPS) . IEEE, 147‚Äì150.\n[126] Jing Li, Xinpu Ji, Yuhan Jia, Bingpeng Zhu, Gang Wang, Zhongwei Li, and Xiaoguang Liu. 2014. Hard drive\nfailure prediction using classification and regression trees. In 2014 44th Annual IEEE/IFIP International Conference on\nDependable Systems and Networks . IEEE, 383‚Äì394.\n[127] Jing Li, Rebecca J Stones, Gang Wang, Xiaoguang Liu, Zhongwei Li, and Ming Xu. 2017. Hard drive failure prediction\nusing Decision Trees. Reliability Engineering & System Safety 164 (2017), 55‚Äì65.\n[128] Yaguang Li, Yishuang Lin, Meghna Madhusudan, Arvind Sharma, Wenbin Xu, Sachin Sapatnekar, Ramesh Harjani,\nand Jiang Hu. 2020. Exploring a machine learning approach to performance driven analog IC placement. In 2020 IEEE\nComputer Society Annual Symposium on VLSI (ISVLSI) . IEEE, 24‚Äì29.\n[129] Yaguang Li, Yishuang Lin, Meghna Madhusudan, Arvind Sharma, Wenbin Xu, Sachin S Sapatnekar, Ramesh Harjani,\nand Jiang Hu. 2020. A customized graph neural network model for guiding analog IC placement. In 2020 IEEE/ACM\nInternational Conference On Computer Aided Design (ICCAD) . IEEE, 1‚Äì9.\n[130] Yunfan Li, D Penney, Abhishek Ramamurthy, and Lizhong Chen. 2019. Characterizing On-Chip Traffic Patterns in\nGeneral-Purpose GPUs: A Deep Learning Approach. In International Conference on Computer Design (ICCD) .\n[131] Rongjian Liang, Hua Xiang, Diwesh Pandey, Lakshmi Reddy, Shyam Ramji, Gi-Joon Nam, and Jiang Hu. 2020. DRC\nhotspot prediction at sub-10nm process nodes using customized convolutional network. In Proceedings of the 2020\nInternational Symposium on Physical Design . 135‚Äì142.\n[132] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and\nDaan Wierstra. 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).\n[133] Ting-Ru Lin, Yunfan Li, Massoud Pedram, and Lizhong Chen. 2019. Design Space Exploration of Memory Controller\nPlacement in Throughput Processors with Deep Learning. IEEE Computer Architecture Letters 18, 1 (2019), 51‚Äì54.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:32 Wu and Xie.\n[134] Ting-Ru Lin, Drew Penney, Massoud Pedram, and Lizhong Chen. 2019. Optimizing Routerless Network-on-Chip\nDesigns: An Innovative Learning-Based Framework. arXiv preprint arXiv:1905.04423 (2019).\n[135] Yibo Lin, Shounak Dhar, Wuxi Li, Haoxing Ren, Brucek Khailany, and David Z Pan. 2019. DREAMPIace: Deep\nLearning Toolkit-Enabled GPU Acceleration for Modern VLSI Placement. In 2019 56th ACM/IEEE Design Automation\nConference (DAC) . IEEE, 1‚Äì6.\n[136] Zhe Lin, Jieru Zhao, Sharad Sinha, and Wei Zhang. 2020. HL-Pow: A learning-based power modeling framework for\nhigh-level synthesis. In 2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC) . IEEE, 574‚Äì580.\n[137] Hung-Yi Liu and Luca P Carloni. 2013. On learning-based methods for design-space exploration with high-level\nsynthesis. In Proceedings of the 50th annual design automation conference . 1‚Äì7.\n[138] Mingjie Liu, Keren Zhu, Jiaqi Gu, Linxiao Shen, Xiyuan Tang, Nan Sun, and David Z Pan. 2020. Towards decrypting\nthe art of analog layout: Placement quality prediction via transfer learning. In 2020 Design, Automation & Test in\nEurope Conference & Exhibition (DATE) . IEEE, 496‚Äì501.\n[139] Mingjie Liu, Keren Zhu, Xiyuan Tang, Biying Xu, Wei Shi, Nan Sun, and David Z Pan. 2020. Closing the design\nloop: Bayesian optimization assisted hierarchical analog layout synthesis. In 2020 57th ACM/IEEE Design Automation\nConference (DAC) . IEEE, 1‚Äì6.\n[140] Daniel Lo, Taejoon Song, and G Edward Suh. 2015. Prediction-guided performance-energy trade-off for interactive\napplications. In Proceedings of the 48th International Symposium on Microarchitecture . ACM, 508‚Äì520.\n[141] Wei-Yin Loh. 2011. Classification and regression trees. Wiley interdisciplinary reviews: data mining and knowledge\ndiscovery 1, 1 (2011), 14‚Äì23.\n[142] Shiting Justin Lu, Russell Tessier, and Wayne Burleson. 2015. Reinforcement learning for thermal-aware many-core\ntask allocation. In Proceedings of the 25th edition on Great Lakes Symposium on VLSI . ACM, 379‚Äì384.\n[143] Yi-Chen Lu, Jeehyun Lee, Anthony Agnesina, Kambiz Samadi, and Sung Kyu Lim. 2019. GAN-CTS: A generative\nadversarial framework for clock tree prediction and optimization. In 2019 IEEE/ACM International Conference on\nComputer-Aided Design (ICCAD) . IEEE, 1‚Äì8.\n[144] Kai Ma, Xue Li, Wei Chen, Chi Zhang, and Xiaorui Wang. 2012. Greengpu: A holistic approach to energy efficiency\nin gpu-cpu heterogeneous architectures. In 2012 41st International Conference on Parallel Processing . IEEE, 48‚Äì57.\n[145] Dani Maarouf, Abeer Alhyari, Ziad Abuowaimer, Timothy Martin, Andrew Gunter, Gary Grewal, Shawki Areibi, and\nAnthony Vannelli. 2018. Machine-learning based congestion estimation for modern FPGAs. In 2018 28th International\nConference on Field Programmable Logic and Applications (FPL) . IEEE, 427‚Äì4277.\n[146] Farzaneh Mahdisoltani, Ioan Stefanovici, and Bianca Schroeder. 2017. Proactive error prediction to improve storage\nsystem reliability. In 2017{USENIX}Annual Technical Conference ( {USENIX}{ATC}17). 391‚Äì402.\n[147] Mateusz Majer, Christophe Bobda, Ali Ahmadinia, and J√ºrgen Teich. 2005. Packet routing in dynamically changing\nnetworks on chip. In 19th IEEE International Parallel and Distributed Processing Symposium . IEEE, 8‚Äìpp.\n[148] Hosein Mohammadi Makrani, Farnoud Farahmand, Hossein Sayadi, Sara Bondi, Sai Manoj Pudukotai Dinakarrao,\nHouman Homayoun, and Setareh Rafatirad. 2019. Pyramid: Machine learning framework to estimate the optimal\ntiming and resource usage of a high-level synthesis design. In 2019 29th International Conference on Field Programmable\nLogic and Applications (FPL) . IEEE, 397‚Äì403.\n[149] Hosein Mohammadi Makrani, Hossein Sayadi, Tinoosh Mohsenin, Setareh Rafatirad, Avesta Sasan, and Houman\nHomayoun. 2019. XPPE: cross-platform performance estimation of hardware accelerators using machine learning. In\nProceedings of the 24th Asia and South Pacific Design Automation Conference . 727‚Äì732.\n[150] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. 2017. Neural adaptive video streaming with pensieve. In\nProceedings of the Conference of the ACM Special Interest Group on Data Communication . 197‚Äì210.\n[151] Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng, and Mohammad Alizadeh. 2019.\nLearning scheduling algorithms for data processing clusters. In Proceedings of the ACM Special Interest Group on Data\nCommunication . 270‚Äì288.\n[152] Artemiy Margaritov, Dmitrii Ustiugov, Edouard Bugnion, and Boris Grot. 2018. Virtual Address Translation via\nLearned Page Table Indexes. In 32nd Conference on Neural Information Processing Systems (NeurIPS) .\n[153] Jose F Martinez and Engin Ipek. 2009. Dynamic multicore resource management: A machine learning approach. IEEE\nmicro 29, 5 (2009), 8‚Äì17.\n[154] Amy McGovern, Eliot Moss, and Andrew G Barto. 2002. Building a basic block instruction scheduler with reinforcement\nlearning and rollouts. Machine learning 49, 2-3 (2002), 141‚Äì160.\n[155] Atefeh Mehrabi, Aninda Manocha, Benjamin C Lee, and Daniel J Sorin. 2020. Prospector: synthesizing efficient\naccelerators via statistical learning. In 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE) . IEEE,\n151‚Äì156.\n[156] Charith Mendis and Saman Amarasinghe. 2018. goSLP: globally optimized superword level parallelism framework.\nProceedings of the ACM on Programming Languages 2, OOPSLA (2018), 1‚Äì28.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:33\n[157] Charith Mendis, Alex Renda, Saman Amarasinghe, and Michael Carbin. 2019. Ithemal: Accurate, Portable and Fast\nBasic Block Throughput Estimation using Deep Neural Networks. In International Conference on Machine Learning .\n4505‚Äì4515.\n[158] Charith Mendis, Cambridge Yang, Yewen Pu, Saman Amarasinghe, and Michael Carbin. 2019. Compiler Auto-\nVectorization with Imitation Learning. In Advances in Neural Information Processing Systems . 14598‚Äì14609.\n[159] Pingfan Meng, Alric Althoff, Quentin Gautier, and Ryan Kastner. 2016. Adaptive threshold non-pareto elimination:\nRe-thinking machine learning for system level design space exploration on FPGAs. In 2016 Design, Automation & Test\nin Europe Conference & Exhibition (DATE) . IEEE, 918‚Äì923.\n[160] Azalia Mirhoseini, Anna Goldie, Hieu Pham, Benoit Steiner, Quoc V Le, and Jeff Dean. 2018. A hierarchical model for\ndevice placement. In Proceedings of the 35th International Conference on Machine Learning . JMLR. org.\n[161] Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric\nJohnson, Omkar Pathak, Sungmin Bae, et al .2020. Chip placement with deep reinforcement learning. arXiv preprint\narXiv:2004.10746 (2020).\n[162] Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad\nNorouzi, Samy Bengio, and Jeff Dean. 2017. Device placement optimization with reinforcement learning. In Proceedings\nof the 34th International Conference on Machine Learning-Volume 70 . JMLR. org, 2430‚Äì2439.\n[163] Nikita Mishra, Connor Imes, John D Lafferty, and Henry Hoffmann. 2018. CALOREE: Learning Control for Predictable\nLatency and Low Energy. In Proceedings of the Twenty-Third International Conference on Architectural Support for\nProgramming Languages and Operating Systems . 184‚Äì198.\n[164] Nikita Mishra, John D Lafferty, and Henry Hoffmann. 2017. Esp: A machine learning approach to predicting application\ninterference. In 2017 IEEE International Conference on Autonomic Computing (ICAC) . IEEE, 125‚Äì134.\n[165] Nikita Mishra, Huazhe Zhang, John D Lafferty, and Henry Hoffmann. 2015. A probabilistic graphical model-based\napproach for minimizing energy under performance constraints. In ACM SIGPLAN Notices , Vol. 50. ACM, 267‚Äì281.\n[166] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver,\nand Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. In International conference\non machine learning . PMLR, 1928‚Äì1937.\n[167] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,\nMartin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al .2015. Human-level control through deep reinforcement\nlearning. nature 518, 7540 (2015), 529‚Äì533.\n[168] Gordon E Moore et al. 1965. Cramming more components onto integrated circuits.\n[169] Janani Mukundan and Jose F Martinez. 2012. MORSE: Multi-objective reconfigurable self-optimizing memory\nscheduler. In IEEE International Symposium on High-Performance Comp Architecture . IEEE, 1‚Äì12.\n[170] Joseph F Murray, Gordon F Hughes, and Kenneth Kreutz-Delgado. 2005. Machine learning methods for predicting\nfailures in hard drives: A multiple-instance application. Journal of Machine Learning Research 6, May (2005), 783‚Äì816.\n[171] Arvind Narayanan, Saurabh Verma, Eman Ramadan, Pariya Babaie, and Zhi-Li Zhang. 2018. Deepcache: A deep\nlearning based framework for content caching. In Proceedings of the 2018 Workshop on Network Meets AI & ML . 48‚Äì53.\n[172] Daniel Nemirovsky, Tugberk Arkose, Nikola Markovic, Mario Nemirovsky, Osman Unsal, and Adrian Cristal. 2017.\nA machine learning approach for performance prediction and scheduling on heterogeneous CPUs. In 2017 29th\nInternational Symposium on Computer Architecture and High Performance Computing (SBAC-PAD) . IEEE, 121‚Äì128.\n[173] Walter Lau Neto, Max Austin, Scott Temple, Luca Amaru, Xifan Tang, and Pierre-Emmanuel Gaillardon. 2019.\nLSOracle: A logic synthesis framework driven by artificial intelligence. In 2019 IEEE/ACM International Conference on\nComputer-Aided Design (ICCAD) . IEEE, 1‚Äì6.\n[174] Alex Nichol, Joshua Achiam, and John Schulman. 2018. On first-order meta-learning algorithms. arXiv preprint\narXiv:1803.02999 (2018).\n[175] Kenneth O‚ÄôNeal, Philip Brisk, Emily Shriver, and Michael Kishinevsky. 2017. HALWPE: Hardware-assisted light\nweight performance estimation for GPUs. In 2017 54th ACM/EDAC/IEEE Design Automation Conference (DAC) . IEEE,\n1‚Äì6.\n[176] Kenneth O‚ÄôNeal, Mitch Liu, Hans Tang, Amin Kalantar, Kennen DeRenard, and Philip Brisk. 2018. Hlspredict:\nCross platform performance prediction for fpga high-level synthesis. In 2018 IEEE/ACM International Conference on\nComputer-Aided Design (ICCAD) . IEEE, 1‚Äì8.\n[177] Berkin Ozisikyilmaz, Gokhan Memik, and Alok Choudhary. 2008. Machine learning models to predict performance\nof computer system design alternatives. In 2008 37th International Conference on Parallel Processing . IEEE, 495‚Äì502.\n[178] Gung-Yu Pan, Jing-Yang Jou, and Bo-Cheng Lai. 2014. Scalable power management using multilevel reinforcement\nlearning for multiprocessors. ACM Transactions on Design Automation of Electronic Systems (TODAES) 19, 4 (2014), 33.\n[179] Po-Cheng Pan, Chien-Chia Huang, and Hung-Ming Chen. 2019. Late breaking results: An efficient learning-based\napproach for performance exploration on analog and RF circuit synthesis. In 2019 56th ACM/IEEE Design Automation\nConference (DAC) . IEEE, 1‚Äì2.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:34 Wu and Xie.\n[180] Ghasem Pasandi, Shahin Nazarian, and Massoud Pedram. 2019. Approximate logic synthesis: A reinforcement\nlearning-based technology mapping approach. In 20th International Symposium on Quality Electronic Design (ISQED) .\nIEEE, 26‚Äì32.\n[181] Ashutosh Pattnaik, Xulong Tang, Adwait Jog, Onur Kayiran, Asit K Mishra, Mahmut T Kandemir, Onur Mutlu,\nand Chita R Das. 2016. Scheduling techniques for GPU architectures with processing-in-memory capabilities. In\nProceedings of the 2016 International Conference on Parallel Architectures and Compilation . ACM, 31‚Äì44.\n[182] Sai Manoj PD, Hao Yu, Hantao Huang, and Dongjun Xu. 2015. A Q-learning based self-adaptive I/O communication\nfor 2.5 D integrated many-core microprocessor and memory. IEEE Trans. Comput. 65, 4 (2015), 1185‚Äì1196.\n[183] Leeor Peled, Shie Mannor, Uri Weiser, and Yoav Etsion. 2015. Semantic locality and context-based prefetching using\nreinforcement learning. In 2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA) .\nIEEE, 285‚Äì297.\n[184] Zhongdong Qi, Yici Cai, and Qiang Zhou. 2014. Accurate prediction of detailed routing congestion using supervised\ndata learning. In 2014 IEEE 32nd International Conference on Computer Design (ICCD) . IEEE, 97‚Äì103.\n[185] Zhiliang Qian, Da-Cheng Juan, Paul Bogdan, Chi-Ying Tsui, Diana Marculescu, and Radu Marculescu. 2013. Svr-noc:\nA performance analysis tool for network-on-chips using learning-based support vector regression model. In 2013\nDesign, Automation & Test in Europe Conference & Exhibition (DATE) . IEEE, 354‚Äì357.\n[186] Nishant Rao, Akshay Ramachandran, and Amish Shah. 2018. MLNoC: A Machine Learning based approach to NoC\ndesign. In 2018 30th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD) .\nIEEE, 1‚Äì8.\n[187] Gokul Subramanian Ravi and Mikko H Lipasti. 2017. CHARSTAR: Clock hierarchy aware resource scaling in tiled\narchitectures. In 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA) . IEEE, 147‚Äì160.\n[188] Veselin Raychev, Martin Vechev, and Eran Yahav. 2014. Code completion with statistical language models. In Acm\nSigplan Notices , Vol. 49. ACM, 419‚Äì428.\n[189] Haoxing Ren, Matthew Fojtik, and Brucek Khailany. 2020. NVCell: Generate Standard Cell Layout in Advanced\nTechnology Nodes with Reinforcement Learning. In ML for Systems Workshop at NeurIPS 2020 .\n[190] Haoxing Ren, George F Kokai, Walker J Turner, and Ting-Sheng Ku. 2020. ParaGraph: Layout parasitics and device\nparameter prediction using graph neural networks. In 2020 57th ACM/IEEE Design Automation Conference (DAC) .\nIEEE, 1‚Äì6.\n[191] Alex Renda, Yishen Chen, Charith Mendis, and Michael Carbin. 2020. DiffTune: Optimizing CPU Simulator Parameters\nwith Learned Differentiable Surrogates. arXiv preprint arXiv:2010.04017 (2020).\n[192] Md Farhadur Reza, Tung Thanh Le, Bappaditya De, Magdy Bayoumi, and Dan Zhao. 2018. Neuro-NoC: Energy\noptimization in heterogeneous many-core NoC using neural networks in dark silicon era. In 2018 IEEE International\nSymposium on Circuits and Systems (ISCAS) . IEEE, 1‚Äì5.\n[193] Christian Ritz and Jens Carl Streibig. 2008. Nonlinear regression with R . Springer Science & Business Media.\n[194] Jo√£o PS Rosa, Daniel JD Guerra, Nuno CG Horta, Ricardo MF Martins, and Nuno CC Louren√ßo. 2019. Using artificial\nneural networks for analog integrated circuit design automation . Springer Nature.\n[195] Michael Rotman and Lior Wolf. 2020. Electric analog circuit design with hypernetworks and a differential simulator. In\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 4157‚Äì4161.\n[196] Nilabja Roy, Abhishek Dubey, and Aniruddha Gokhale. 2011. Efficient autoscaling in the cloud using predictive\nmodels for workload forecasting. In 2011 IEEE 4th International Conference on Cloud Computing . IEEE, 500‚Äì507.\n[197] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1985. Learning internal representations by error\npropagation . Technical Report. California Univ San Diego La Jolla Inst for Cognitive Science.\n[198] Jaehun Ryu and Hyojin Sung. 2021. MetaTune: Meta-Learning Based Cost Model for Fast and Efficient Auto-tuning\nFrameworks. arXiv preprint arXiv:2102.04199 (2021).\n[199] Omer Sagi and Lior Rokach. 2018. Ensemble learning: A survey. Wiley Interdisciplinary Reviews: Data Mining and\nKnowledge Discovery 8, 4 (2018), e1249.\n[200] Karthik Sangaiah, Mark Hempstead, and Baris Taskin. 2015. Uncore rpd: Rapid design space exploration of the uncore\nvia regression modeling. In Proceedings of the IEEE/ACM International Conference on Computer-Aided Design . IEEE\nPress, 365‚Äì372.\n[201] Andreas G Savva, Theocharis Theocharides, and Vassos Soteriou. 2012. Intelligent on/off dynamic link management\nfor on-chip networks. Journal of Electrical and Computer Engineering 2012 (2012), 6.\n[202] Bernhard Sch√∂lkopf, Alexander J Smola, Francis Bach, et al .2002. Learning with kernels: support vector machines,\nregularization, optimization, and beyond . MIT press.\n[203] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347 (2017).\n[204] George AF Seber and Alan J Lee. 2012. Linear regression analysis . Vol. 329. John Wiley & Sons.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:35\n[205] Keertana Settaluri, Ameer Haj-Ali, Qijing Huang, Kourosh Hakhamaneshi, and Borivoje Nikolic. 2020. Autockt: Deep\nreinforcement learning of analog circuit designs. In 2020 Design, Automation & Test in Europe Conference & Exhibition\n(DATE) . IEEE, 490‚Äì495.\n[206] Burr Settles. 2009. Active learning literature survey. (2009).\n[207] Zhan Shi, Xiangru Huang, Akanksha Jain, and Calvin Lin. 2019. Applying Deep Learning to the Cache Replacement\nProblem. In Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture . ACM, 413‚Äì425.\n[208] Zhan Shi, Akanksha Jain, Kevin Swersky, Milad Hashemi, Parthasarathy Ranganathan, and Calvin Lin. [n.d.]. A\nNeural Hierarchical Sequence Model for Irregular Data Prefetching. ([n. d.]).\n[209] Zhan Shi, Kevin Swersky, Daniel Tarlow, Parthasarathy Ranganathan, and Milad Hashemi. 2019. Learning Execution\nthrough Neural Code Fusion. arXiv preprint arXiv:1906.07181 (2019).\n[210] Brett Shook, Prateek Bhansali, Chandramouli Kashyap, Chirayu Amin, and Siddhartha Joshi. 2020. MLParest: machine\nlearning based parasitic estimation for custom circuit design. In 2020 57th ACM/IEEE Design Automation Conference\n(DAC) . IEEE, 1‚Äì6.\n[211] Zhenyu Song, Daniel S Berger, Kai Li, Anees Shaikh, Wyatt Lloyd, Soudeh Ghorbani, Changhoon Kim, Aditya Akella,\nArvind Krishnamurthy, Emmett Witchel, et al .2020. Learning relaxed belady for content distribution network caching.\nIn17th{USENIX}Symposium on Networked Systems Design and Implementation ( {NSDI}20). 529‚Äì544.\n[212] Vassos Soteriou, Theocharis Theocharides, and Elena Kakoulli. 2015. A holistic approach towards intelligent hotspot\nprevention in network-on-chip-based multicores. IEEE Trans. Comput. 65, 3 (2015), 819‚Äì833.\n[213] Renee St Amant, Daniel A Jim√©nez, and Doug Burger. 2008. Low-power, high-performance analog neural branch\nprediction. In Proceedings of the 41st annual IEEE/ACM International Symposium on Microarchitecture . IEEE Computer\nSociety, 447‚Äì458.\n[214] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction . MIT press.\n[215] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. 2017. Efficient processing of deep neural networks: A\ntutorial and survey. Proc. IEEE 105, 12 (2017), 2295‚Äì2329.\n[216] Aysa Fakheri Tabrizi, Logan Rakai, Nima Karimpour Darav, Ismail Bustany, Laleh Behjat, Shuchang Xu, and Andrew\nKennings. 2018. A machine learning framework to identify detailed routing short violations from a placed netlist. In\n2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC) . IEEE, 1‚Äì6.\n[217] Stephen J Tarsa, Rangeen Basu Roy Chowdhury, Julien Sebot, Gautham Chinya, Jayesh Gaur, Karthik Sankara-\nnarayanan, Chit-Kwan Lin, Robert Chappell, Ronak Singhal, and Hong Wang. 2019. Post-silicon cpu adaptation made\npractical using machine learning. In Proceedings of the 46th International Symposium on Computer Architecture . ACM,\n14‚Äì26.\n[218] Stephen J Tarsa, Chit-Kwan Lin, Gokce Keskin, Gautham Chinya, and Hong Wang. 2019. Improving Branch Prediction\nBy Modeling Global History with Convolutional Neural Networks. arXiv preprint arXiv:1906.09889 (2019).\n[219] Elvira Teran, Zhe Wang, and Daniel A Jim√©nez. 2016. Perceptron learning for reuse prediction. In 2016 49th Annual\nIEEE/ACM International Symposium on Microarchitecture (MICRO) . IEEE, 1‚Äì12.\n[220] Gerald Tesauro. 2007. Reinforcement learning in autonomic computing: A manifesto and case studies. IEEE Internet\nComputing 11, 1 (2007), 22‚Äì30.\n[221] Gerald Tesauro et al .2005. Online resource allocation using decompositional reinforcement learning. In AAAI , Vol. 5.\n886‚Äì891.\n[222] Alexey Tsymbal. 2004. The problem of concept drift: definitions and related work. Computer Science Department,\nTrinity College Dublin 106, 2 (2004), 58.\n[223] Ecenur Ustun, Chenhui Deng, Debjit Pal, Zhijing Li, and Zhiru Zhang. 2020. Accurate operation delay prediction\nfor FPGA HLS using graph neural networks. In Proceedings of the 39th International Conference on Computer-Aided\nDesign . 1‚Äì9.\n[224] Scott Van Winkle, Avinash Karanth Kodi, Razvan Bunescu, and Ahmed Louri. 2018. Extending the power-efficiency\nand performance of photonic interconnects for heterogeneous multicores with machine learning. In 2018 IEEE\nInternational Symposium on High Performance Computer Architecture (HPCA) . IEEE, 480‚Äì491.\n[225] Joaquin Vanschoren. 2018. Meta-learning: A survey. arXiv preprint arXiv:1810.03548 (2018).\n[226] David Vengerov. 2009. A reinforcement learning framework for utility-based scheduling in resource-constrained\nsystems. Future Generation Computer Systems 25, 7 (2009), 728‚Äì736.\n[227] David Vengerov, Hamid R Berenji, and Alex Vengerov. 2002. Adaptive coordination among fuzzy reinforcement\nlearning agents performing distributed dynamic load balancing. In 2002 IEEE World Congress on Computational\nIntelligence. 2002 IEEE International Conference on Fuzzy Systems. FUZZ-IEEE‚Äô02. Proceedings (Cat. No. 02CH37291) ,\nVol. 1. IEEE, 179‚Äì184.\n[228] M Mitchell Waldrop. 2016. The chips are down for Moore‚Äôs law. Nature News 530, 7589 (2016), 144.\n[229] Boqian Wang, Zhonghai Lu, and Shenggang Chen. 2019. ANN Based Admission Control for On-Chip Networks. In\nProceedings of the 56th Annual Design Automation Conference 2019 . ACM, 46.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\n1:36 Wu and Xie.\n[230] Haoyuan Wang and Zhiwei Luo. 2017. Data Cache Prefetching with Perceptron Learning. arXiv preprint\narXiv:1712.00905 (2017).\n[231] Hanrui Wang, Kuan Wang, Jiacheng Yang, Linxiao Shen, Nan Sun, Hae-Seung Lee, and Song Han. 2020. GCN-RL\ncircuit designer: Transferable transistor sizing with graph neural networks and reinforcement learning. In 2020 57th\nACM/IEEE Design Automation Conference (DAC) . IEEE, 1‚Äì6.\n[232] Hua Wang, Xinbo Yi, Ping Huang, Bin Cheng, and Ke Zhou. 2018. Efficient SSD Caching by Avoiding Unnecessary\nWrites using Machine Learning. In Proceedings of the 47th International Conference on Parallel Processing . ACM, 82.\n[233] Ke Wang, Ahmed Louri, Avinash Karanth, and Razvan Bunescu. 2019. High-performance, Energy-efficient, Fault-\ntolerant Network-on-Chip Design Using Reinforcement Learnin. In 2019 Design, Automation & Test in Europe Confer-\nence & Exhibition (DATE) . IEEE, 1166‚Äì1171.\n[234] Ke Wang, Ahmed Louri, Avinash Karanth, and Razvan Bunescu. 2019. IntelliNoC: a holistic design framework for\nenergy-efficient and reliable on-chip communication for manycores. In Proceedings of the 46th International Symposium\non Computer Architecture . ACM, 589‚Äì600.\n[235] Shibo Wang and Engin Ipek. 2016. Reducing data movement energy via online data clustering and encoding. In The\n49th Annual IEEE/ACM International Symposium on Microarchitecture . IEEE Press, 32.\n[236] Shimon Whiteson and Peter Stone. 2004. Adaptive job routing and scheduling. Engineering Applications of Artificial\nIntelligence 17, 7 (2004), 855‚Äì869.\n[237] Svante Wold, Kim Esbensen, and Paul Geladi. 1987. Principal component analysis. Chemometrics and intelligent\nlaboratory systems 2, 1-3 (1987), 37‚Äì52.\n[238] Jae-Yeon Won, Xi Chen, Paul Gratz, Jiang Hu, and Vassos Soteriou. 2014. Up by their bootstraps: Online learning in\nartificial neural networks for CMP uncore power management. In 2014 IEEE 20th International Symposium on High\nPerformance Computer Architecture (HPCA) . IEEE, 308‚Äì319.\n[239] Gene Wu, Joseph L Greathouse, Alexander Lyashevsky, Nuwan Jayasena, and Derek Chiou. 2015. GPGPU performance\nand power estimation using machine learning. In 2015 IEEE 21st International Symposium on High Performance\nComputer Architecture (HPCA) . IEEE, 564‚Äì576.\n[240] Gang Wu, Yue Xu, Dean Wu, Manoj Ragupathy, Yu-yen Mo, and Chris Chu. 2016. Flip-flop clustering by weighted\nK-means algorithm. In Proceedings of the 53rd Annual Design Automation Conference . ACM, 82.\n[241] Nan Wu, Lei Deng, Guoqi Li, and Yuan Xie. 2020. Core Placement Optimization for Multi-chip Many-core Neural\nNetwork Systems with Reinforcement Learning. ACM Transactions on Design Automation of Electronic Systems\n(TODAES) 26, 2 (2020), 1‚Äì27.\n[242] Nan Wu and Pengcheng Li. 2020. Phoebe: Reuse-Aware Online Caching with Reinforcement Learning for Emerging\nStorage Models. arXiv preprint arXiv:2011.07160 (2020).\n[243] Nan Wu, Yuan Xie, and Cong Hao. 2021. IronMan: GNN-assisted Design Space Exploration in High-Level Synthesis\nvia Reinforcement Learning. In Proceedings of the 2021 on Great Lakes Symposium on VLSI . 39‚Äì44.\n[244] Weidan Wu and Benjamin C Lee. 2012. Inferred models for dynamic and sparse hardware-software spaces. In\nProceedings of the 2012 45th Annual IEEE/ACM International Symposium on Microarchitecture . IEEE Computer Society,\n413‚Äì424.\n[245] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive\nsurvey on graph neural networks. IEEE transactions on neural networks and learning systems 32, 1 (2020), 4‚Äì24.\n[246] Jiang Xiao, Zhuang Xiong, Song Wu, Yusheng Yi, Hai Jin, and Kan Hu. 2018. Disk failure prediction in data centers\nvia online learning. In Proceedings of the 47th International Conference on Parallel Processing . ACM, 35.\n[247] Zhiyao Xie, Yu-Hung Huang, Guan-Qi Fang, Haoxing Ren, Shao-Yun Fang, Yiran Chen, and Jiang Hu. 2018. RouteNet:\nRoutability prediction for mixed-size designs using convolutional neural network. In 2018 IEEE/ACM International\nConference on Computer-Aided Design (ICCAD) . IEEE, 1‚Äì8.\n[248] Chang Xu, Gang Wang, Xiaoguang Liu, Dongdong Guo, and Tie-Yan Liu. 2016. Health status assessment and failure\nprediction for hard drives with recurrent neural networks. IEEE Trans. Comput. 65, 11 (2016), 3502‚Äì3508.\n[249] Yong Xu, Kaixin Sui, Randolph Yao, Hongyu Zhang, Qingwei Lin, Yingnong Dang, Peng Li, Keceng Jiang, Wenchi\nZhang, Jian-Guang Lou, et al .2018. Improving service availability of cloud systems by predicting disk error. In 2018\n{USENIX}Annual Technical Conference ( {USENIX}{ATC}18). 481‚Äì494.\n[250] Que Yanghua, Nachiket Kapre, Harnhua Ng, and Kirvy Teo. 2016. Improving classification accuracy of a machine\nlearning approach for fpga timing closure. In 2016 IEEE 24th Annual International Symposium on Field-Programmable\nCustom Computing Machines (FCCM) . IEEE, 80‚Äì83.\n[251] Amir Yazdanbakhsh, Jongse Park, Hardik Sharma, Pejman Lotfi-Kamran, and Hadi Esmaeilzadeh. 2015. Neural\nacceleration for GPU throughput processors. In Proceedings of the 48th International Symposium on Microarchitecture .\nACM, 482‚Äì493.\n[252] Hyunho Yeo, Youngmok Jung, Jaehong Kim, Jinwoo Shin, and Dongsu Han. 2018. Neural adaptive content-aware\ninternet video delivery. In 13th{USENIX}Symposium on Operating Systems Design and Implementation ( {OSDI}18).\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n\nA Survey of Machine Learning for Computer Architecture and Systems 1:37\n645‚Äì661.\n[253] Nezih Yigitbasi, Theodore L Willke, Guangdeng Liao, and Dick Epema. 2013. Towards machine learning-based auto-\ntuning of mapreduce. In 2013 IEEE 21st International Symposium on Modelling, Analysis and Simulation of Computer\nand Telecommunication Systems . IEEE, 11‚Äì20.\n[254] Jieming Yin, Yasuko Eckert, Shuai Che, Mark Oskin, and Gabriel H Loh. 2018. Toward More Efficient NoC Arbitration:\nA Deep Reinforcement Learning Approach. (2018).\n[255] Jieming Yin, Subhash Sethumurugan, Yasuko Eckert, Chintan Patel, Alan Smith, Eric Morton, Mark Oskin, Na-\ntalie Enright Jerger, and Gabriel H Loh. 2020. Experiences with ML-Driven Design: A NoC Case Study. In 2020 IEEE\nInternational Symposium on High Performance Computer Architecture (HPCA) . IEEE, 637‚Äì648.\n[256] Cunxi Yu, Houping Xiao, and Giovanni De Micheli. 2018. Developing synthesis flows without human knowledge. In\nProceedings of the 55th Annual Design Automation Conference . 1‚Äì6.\n[257] Cunxi Yu and Zhiru Zhang. 2019. Painting on placement: Forecasting routing congestion using conditional generative\nadversarial nets. In Proceedings of the 56th Annual Design Automation Conference 2019 . 1‚Äì6.\n[258] Cunxi Yu and Wang Zhou. 2020. Decision Making in Synthesis cross Technologies using LSTMs and Transfer Learning.\nInProceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD . 55‚Äì60.\n[259] Georgios Zacharopoulos, Andrea Barbon, Giovanni Ansaloni, and Laura Pozzi. 2018. Machine learning approach\nfor loop unrolling factor prediction in high level synthesis. In 2018 International Conference on High Performance\nComputing & Simulation (HPCS) . IEEE, 91‚Äì97.\n[260] Stephen Zekany, Daniel Rings, Nathan Harada, Michael A Laurenzano, Lingjia Tang, and Jason Mars. 2016. CrystalBall:\nStatically analyzing runtime behavior via deep sequence learning. In 2016 49th Annual IEEE/ACM International\nSymposium on Microarchitecture (MICRO) . IEEE, 1‚Äì12.\n[261] Yuan Zeng and Xiaochen Guo. 2017. Long short term memory based hardware prefetcher: A case study. In Proceedings\nof the International Symposium on Memory Systems . ACM, 305‚Äì311.\n[262] Guo Zhang, Hao He, and Dina Katabi. 2019. Circuit-GNN: Graph neural networks for distributed circuit design. In\nInternational Conference on Machine Learning . PMLR, 7364‚Äì7373.\n[263] Haitao Zhang, Bingchang Tang, Xin Geng, and Huadong Ma. 2018. Learning Driven Parallelization for Large-Scale\nVideo Workload in Hybrid CPU-GPU Cluster. In Proceedings of the 47th International Conference on Parallel Processing .\nACM, 32.\n[264] Kaiqing Zhang, Zhuoran Yang, and Tamer Ba≈üar. 2019. Multi-agent reinforcement learning: A selective overview of\ntheories and algorithms. arXiv preprint arXiv:1911.10635 (2019).\n[265] Jieru Zhao, Tingyuan Liang, Sharad Sinha, and Wei Zhang. 2019. Machine learning based routing congestion prediction\nin FPGA high-level synthesis. In 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE) . IEEE,\n1130‚Äì1135.\n[266] Hao Zheng and Ahmed Louri. 2019. An energy-efficient network-on-chip design using reinforcement learning. In\nProceedings of the 56th Annual Design Automation Conference 2019 . ACM, 47.\n[267] Pengfei Zheng and Benjamin C Lee. 2018. Hound: Causal learning for datacenter-scale straggler diagnosis. Proceedings\nof the ACM on Measurement and Analysis of Computing Systems 2, 1 (2018), 1‚Äì36.\n[268] Xinnian Zheng, Lizy K John, and Andreas Gerstlauer. 2016. Accurate phase-level cross-platform power and perfor-\nmance estimation. In 2016 53nd ACM/EDAC/IEEE Design Automation Conference (DAC) . IEEE, 1‚Äì6.\n[269] Xinnian Zheng, Pradeep Ravikumar, Lizy K John, and Andreas Gerstlauer. 2015. Learning-based analytical cross-\nplatform performance prediction. In 2015 International Conference on Embedded Computer Systems: Architectures,\nModeling, and Simulation (SAMOS) . IEEE, 52‚Äì59.\n[270] Yanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter C Ma, Qiumin Xu, Ming Zhong, Hanxiao Liu,\nAnna Goldie, Azalia Mirhoseini, et al .2019. GDP: Generalized Device Placement for Dataflow Graphs. arXiv preprint\narXiv:1910.01578 (2019).\n[271] Bingpeng Zhu, Gang Wang, Xiaoguang Liu, Dianming Hu, Sheng Lin, and Jingwei Ma. 2013. Proactive drive failure\nprediction for large scale storage systems. In 2013 IEEE 29th symposium on mass storage systems and technologies\n(MSST) . IEEE, 1‚Äì5.\n[272] Keren Zhu, Mingjie Liu, Yibo Lin, Biying Xu, Shaolan Li, Xiyuan Tang, Nan Sun, and David Z Pan. 2019. Geniusroute:\nA new analog routing paradigm using generative neural network guidance. In 2019 IEEE/ACM International Conference\non Computer-Aided Design (ICCAD) . IEEE, 1‚Äì8.\n[273] Xiaojin Jerry Zhu. 2005. Semi-supervised learning literature survey . Technical Report. University of Wisconsin-Madison\nDepartment of Computer Sciences.\n[274] Cheng Zhuo, Bei Yu, and Di Gao. 2017. Accelerating chip design with machine learning: From pre-silicon to\npost-silicon. In 2017 30th IEEE International System-on-Chip Conference (SOCC) . IEEE, 227‚Äì232.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2021.",
  "textLength": 173676
}