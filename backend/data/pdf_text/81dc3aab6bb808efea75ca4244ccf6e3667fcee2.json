{
  "paperId": "81dc3aab6bb808efea75ca4244ccf6e3667fcee2",
  "title": "Approximate Query Processing for Data Exploration using Deep Generative Models",
  "pdfPath": "81dc3aab6bb808efea75ca4244ccf6e3667fcee2.pdf",
  "text": "Approximate Query Processing for Data\nExploration using Deep Generative Models\nSaravanan Thirumuruganathanzy, Shohedul Hasanz, Nick Koudaszz, Gautam Dasz\nzyQCRI, HBKU;zUniversity of Texas at Arlington;zzUniversity of Toronto\nzysthirumuruganathan@hbku.edu.qa,zfshohedul.hasan@mavs, gdas@cse g.uta.edu,zzkoudas@cs.toronto.edu\nAbstract —Data is generated at an unprecedented rate sur-\npassing our ability to analyze them. The database community\nhas pioneered many novel techniques for Approximate Query\nProcessing (AQP) that could give approximate results in a\nfraction of time needed for computing exact results. In this\nwork, we explore the usage of deep learning (DL) for answering\naggregate queries speciﬁcally for interactive applications such\nas data exploration and visualization. We use deep generative\nmodels , an unsupervised learning based approach, to learn the\ndata distribution faithfully such that aggregate queries could\nbe answered approximately by generating samples from the\nlearned model. The model is often compact – few hundred KBs\n– so that arbitrary AQP queries could be answered on the\nclient side without contacting the database server. Our other\ncontributions include identifying model bias and minimizing it\nthrough a rejection sampling based approach and an algorithm\nto build model ensembles for AQP for improved accuracy. Our\nextensive experiments show that our proposed approach can\nprovide answers with high accuracy and low latency.\nI. I NTRODUCTION\nData driven decision making has become the dominant\nparadigm for businesses seeking to gain an edge over com-\npetitors. However, the unprecedented rate at which data is\ngenerated surpasses our ability to analyze them. Approximate\nQuery Processing (AQP) is a promising technique that pro-\nvides approximate answers to queries at a fraction of the cost\nneeded to answer it exactly. AQP has numerous applications in\ndata exploration and visualization where approximate results\nare acceptable as long as they can be obtained near real-time.\nCase Study. Consider an user who performs data exploration\nand visualization on a popular dataset such as NYC Taxi\ndataset. The user issues ad-hoc aggregate queries, involving\narbitrary subsets of attributes of interest, such as what is the\naverage number of passengers on trips starting from Manhat-\ntan? orwhat is the average trip duration grouped by hour?\nand so on. Since this is for exploratory purposes, an imprecise\nanswer is often adequate. A traditional approach is to issue ag-\ngregate queries to the database server, get exact or approximate\nanswers accessing the base data or pre-computed/on-demand\nsamples and display the returned results to the user. However,\nthis could suffer from high latency that is not conducive for\ninteractive analysis. In this paper, we propose an alternate\napproach where the approximate results could be computed en-\ntirely at the client side. Speciﬁcally, we build a deep generative\nmodel that approximates the data distribution with high ﬁdelity\nand is lightweight (few hundreds of KBs). This model is sent to\nthe client and could be used to generate synthetic samples overwhich AQP could be performed locally on arbitrary subsets of\nattributes, without any communication with the server. Our\napproach is complementary to traditional AQP exploring a\nnew research direction of utilizing deep generative models for\ndata exploration. It offers a lightweight model that can answer\narbitrary queries which we experimentally demonstrate exhibit\nsuperior accuracy. For queries requiring provable guarantees\nwe default to traditional AQP or exact query evaluation.\nA. Outline of Technical Results\nDeep Learning for AQP. Deep Learning (DL) [20] has\nbecome popular due to its excellent performance in many\ncomplex applications. In this paper, we investigate the fea-\nsibility of using DL for answering aggregate queries for\ndata exploration and visualization. Structured databases seem\nintrinsically different from prior areas where DL has shined\n- such as computer vision and natural language processing.\nFurthermore, the task of generating approximate estimates for\nan aggregate query is quite different from common DL tasks.\nHowever, we show that AQP can be achieved in an effective\nand efﬁcient manner using DL models.\nDeep Generative Models for AQP. Our key insight is to train\na DL model to learn the data distribution of the underlying\ndata set effectively. Once such a model is trained, it acts as\na concise representation of the dataset. The samples gener-\nated from the model have a data distribution that is almost\nidentical to that of the underlying dataset. Hence, existing\nAQP techniques [38], [37] could be transparently applied on\nthese samples. Furthermore, the model could generate as many\nsamples as required without the need to access the underlying\ndataset. This makes it very useful for interactive applications\nas all the computations could be done locally.\nTechnical Challenges. The key challenge is to identify a\nDL based distribution estimation approach that is expressive\nenough to reﬂect statistical properties of real-world datasets\nand yet tractable and efﬁcient to train. It must be non-\nparametric and not make any prior assumption about data\ncharacteristics. A large class of DL techniques - dubbed collec-\ntively as deep generative models - could be used for this pur-\npose. Intuitively, a deep generative model is an unsupervised\napproach that learns the probability distribution of the dataset\nfrom a set of tuples. Often, learning the exact distribution is\nchallenging, thus generative models learn a model that is very\nsimilar to the true distribution of the underlying data. This isarXiv:1903.10000v3  [cs.DB]  18 Nov 2019\n\noften achieved through neural networks that learn a function\nthat maps the approximate distribution to the true distribution.\nEach of the generative models have their respective advantages\nand disadvantages. We focus on variational autoencoders [11]\nthat aim to learn a low dimensional latent representation of\nthe training data that optimizes the log-likelihood of the data\nthrough evidence lower bound.\nII. P RELIMINARIES\nConsider a relation Rwithntuples and mattributes\nA1;A2;:::;Am. Given a tuple tand an attribute Ai, we denote\nthe value of Aiintast[Ai]. LetDom (Ai)be the domain of\nattributeAi.\nQueries for AQP. In this paper, we focus on aggregate\nanalytic queries of the general format:\nSELECT g, AGG(A) FROM R\nWHERE filter GROUP BY G\nOf course, both the WHERE and GROUP BY clauses are\noptional. Each attribute Aicould be used as a ﬁlter attribute\ninvolved in a predicate or as a measure attribute involved in\nan aggregate. The ﬁlter could be a conjunctive or disjunctive\ncombination of conditions. Each of the conditions could be\nany relational expression of the format A op CONST where\nAis an attribute and opis one off=;6=;<;>;\u0014;\u0015g. AGG\ncould be one of the standard aggregates AVG, SUM, COUNT\nthat have been extensively studied in prior AQP literature. One\ncould use other aggregates such as QUANTILES as long as a\nstatistical estimator exists to generate aggregate estimates.\nPerformance Measures. Letqbe an aggregate query whose\ntrue value is \u0012. Let ~\u0012be the estimate provided by the AQP\nsystem. Then, we can measure the estimation accuracy through\nrelative error deﬁned as\nRelErr (q) =j~\u0012\u0000\u0012j\n\u0012(1)\nFor a set of queries Q=fq1;:::;qrg, the effectiveness of\nthe AQP system could be computed through average relative\nerror. Let\u0012jand~\u0012jbe the true and estimated value of the\naggregate for query qj.\nAvgRelErr (Q) =1\nrrX\nj=1j~\u0012j\u0000\u0012jj\n\u0012j(2)\nWe could also use the average relative error to measure the\naccuracy of the estimate for GROUP BY queries. Suppose we\nare given a group by query qwith groups G=fg1;:::;grg.\nIt is possible that the sample does not contain all of these\ngroups and the AQP system generates estimates for groups\nfgj1;:::;gj0rgwhere each gji2G. As before, let \u0012jiandf\u0012ji\nbe the true and estimated value of the aggregate for group\ngji. By assigning 100% relative error for missing groups, the\naverage relative error for group by queries is deﬁned as,\nAvgRelErr (q) =1\nr0\n@(r\u0000r0) +r0X\ni=1jf\u0012ji\u0000\u0012jij\n\u0012ji1\nA (3)III. B ACKGROUND\nIn this section, we provide necessary background about\ngenerative models and variational autoencoders in particular.\nGenerative Models. Suppose we are given a set of data points\nX=fx1;:::;xngthat are distributed according to some\nunknown probability distribution P(X). Generative models\nseek to learn an approximate probability distribution Qsuch\nthatQis very similar to P. Most generative models also allow\none to generate samples X0=fx0\n1;:::;gfrom the model\nQsuch that the X0has similar statistical properties to X.\nDeep generative models use powerful function approximators\n(typically, deep neural networks) for learning to approximate\nthe distribution.\nVariational Autoencoders (V AEs). V AEs are a class of gener-\native models [11], [6], [5] that can model various complicated\ndata distributions and generate samples. They are very efﬁcient\nto train, have an interpretable latent space and could be adapted\neffectively to different domains such as images, text and music.\nLatent variables are an intermediate data representation that\ncaptures data characteristics used for generative modelling.\nLetXbe the relational data that we wish to model and z\na latent variable. Let P(X)be the probability distribution\nfrom which the underlying relation consisting of attributes\nA1;:::;Amwas derived and P(z)as the probability distri-\nbution of the latent variable. Then P(Xjz)is the distribution\nof generating data given latent variable. We can model P(X)\nin relation to zasP(X) =R\nP(Xjz)P(z)dzmarginalizing\nzout of the joint probability P(X;z). The challenge is that\nwe do not know P(z)andP(Xjz). The underlying idea in\nvariational modelling is to infer P(z)usingP(zjX).\nVariational Inference. We use a method called Variational\nInference (VI) to infer P(zjX)in V AE. The main idea of\nVI is to approach inference as an optimization problem. We\nmodel the true distribution P(zjX)using a simpler distribution\n(denoted as Q) that is easy to evaluate, e.g. Gaussian, and\nminimize the difference between those two distribution using\nKL divergence metric, which tells us how different Pis\nfromQ. Typically, the simpler distribution depends on the\nattribute type. Gaussian distribution is often appropriate for\nreal numbers while Bernoulli distribution is often used for\ncategorical attributes. Assume we wish to infer P(zjX)using\nQ(zjX). The KL divergence is speciﬁed as:\nDKL[Q(zjX)jjP(zjX)] =X\nzQ(zjX) log(Q(zjX)\nP(zjX)) =\nE[log(Q(zjX)\nP(zjX))] =E[log(Q(zjX))\u0000log(P(zjX))](4)\nWe can connect [11] Q(zjX)which is a projection of the data\ninto the latent space and P(Xjz)which generates data given\na latent variable zthrough Equation 5 that is also called as\nthe variational objective.\nlogP(X)\u0000DKL[Q(zjXjjP(zjX)]\n=E[logP(Xjz)]\u0000DKL[Q(zjXjjP(z)](5)\n\nEncoders and Decoders. A different way to think of this\nequation is as Q(zjX)encoding the data using zas an\nintermediate data representation and P(Xjz)generates data\ngiven a latent variable z. TypicallyQ(zjX)is implemented\nwith a neural network mapping the underlying data space\ninto the latent space ( encoder network ). Similarly P(Xjz)\nis implemented with a neural network and is responsible to\ngenerate data following the distribution P(X)given sample\nlatent variables zfrom the latent space ( decoder network ).\nThe variational objective has a very natural interpretation.\nWe wish to model our data P(X)under some error function\nDKL[Q(zjXjjP(zjX)]. In other words, V AE tries to identify\nthe lower bound of log(P(X)), which in practice is good\nenough as trying to determine the exact distribution is often\nintractable. For this we aim to maximize over some mapping\nfrom latent variables to logP(Xjz)and minimize the differ-\nence between our simple distribution Q(zjX)and the true\nlatent distribution P(z). Since we need to sample from P(z)\nin V AE typically one chooses a simple distribution to sample\nfrom such as N(0;1). Since we wish to minimize the distance\nbetweenQ(zjX)andP(z)in V AE one typically assumes that\nQ(zjX)is also normal with mean \u0016(X)and variance \u0006(X).\nBoth the encoder and the decoder networks are trained end-to-\nend. After training, data can be generated by sampling zfrom\na normal distribution and passing it to the decoder network.\nIV. AQP U SING VARIATIONAL AUTOENCODERS\nIn this section, we provide an overview of our two phase\napproach for using V AE for AQP. This requires solving a\nnumber of theoretical and practical challenges such as input\nencodings and approximation errors due to model bias.\nOur Approach. Our proposed approach proceeds in two\nphases. In the model building phase, we train a deep generative\nmodelMRover the dataset Rsuch that it learns the underlying\ndata distribution. In this section, we assume that a single model\nis built for the entire dataset that we relax in Section V.\nOnce the DL model is trained, it can act as a succinct\nrepresentation of the dataset. In the run-time phase, the AQP\nsystem uses the DL model to generate samplesSfrom the\nunderlying distribution. The given query is rewritten to run\nonS. The existing AQP techniques could be transparently\nused to generate the aggregate estimate. Figure 1 illustrates\nour approach.\nFig. 1. Two Phase Approach for DL based AQP\nA. Using VAE for AQP\nIn this subsection, we describe how to train a V AE over\nrelational data and use it for AQP.Input Encoding. In contrast to homogeneous domains such as\nimages and text, relations often consist of mixed data types that\ncould be discrete or continuous. The ﬁrst step is to represent\neach tupletas a vector of dimension d. For ease of exposition,\nwe consider one-hot encoding and describe other effective\nencodings in Section IV-E. One-hot encoding represents each\ntuple as ad=Pm\ni=1jDom (Ai)jdimensional vector where\nthe position corresponding to a given domain value is set to\n1. Each tuple in a relation Rwith two binary attributes A1\nandA2, is represented as a 4dimensional binary vector. A\ntuple withA1= 0;A2= 1 is represented as [1;0;0;1]while\na tuple with A1= 1;A2= 1is represented as [0;1;0;1]. This\napproach is efﬁcient for small attribute domains but becomes\ncumbersome if a relation has millions of distinct values.\nModel Building and Sampling from V AE. Once all the\ntuples are encoded appropriately, we could use V AE to learn\nthe underlying distribution. We denote the size of the input\nand latent dimension by dandd0respectively. For one hot\nencoding,d=Pm\ni=1jDom (Ai)j. Asd0increases, it results\nin more accurate learning of the distribution at the cost of\na larger model. Once the model is trained, it could be used\nto generate samples X0. The randomly generated tuples often\nshare similar statistical properties to tuples sampled from the\nunderlying relation Rand hence are a viable substitute for\nR. One could apply the existing AQP mechanisms on the\ngenerated samples and use it to generate aggregate estimates\nalong with conﬁdence intervals.\nThe sample tuples are generated as follows: we generate\nsamples from the latent space zand then apply the decoder\nnetwork to convert points in latent space to tuples. Recall\nfrom Section III that the latent space is often a probability\ndistribution that is easy to sample such as Gaussian. It is\npossible to speed up the sampling from arbitrary Normal\ndistributions using the reparameterization trick. Instead of\nsampling from a distribution N(\u0016;\u001b), we could sample from\nthe standard Normal distribution N(0;1)with zero mean and\nunit variance. A sample \u000ffromN(0;1)could be converted to\na sampleN(\u0016;\u001b)asz=\u0016+\u001b\f\u000f. Intuitively, this shifts \u000f\nby the mean \u0016and scales it based on the variance \u001b.\nB. Handling Approximation Errors.\nWe consider approximation error caused due to model bias\nand propose an effective rejection sampling to mitigate it.\nSampling Error. Aggregates estimated over the sample could\ndiffer from the exact results computed over the entire dataset\nand their difference is called the sampling error. Both the tra-\nditional AQP and our proposed approach suffer from sampling\nerror. The techniques used to mitigate it - such as increasing\nsample size - can also be applied to the samples from the\ngenerative model.\nErrors due to Model Bias. Another source of error is\nsampling bias. This could occur when the samples are not rep-\nresentative of the underlying dataset and do not approximate\nits data distribution appropriately. Aggregates generated over\nthese samples are often biased and need to be corrected. This\n\nproblem is present even in traditional AQP [38] and mitigated\nthrough techniques such as importance weighting [18] and\nbootstrapping [15], [38].\nOur proposed approach also suffers from sampling bias\ndue to a subtle reason. Generative models learn the data\ndistribution which is a very challenging problem - especially\nin high dimensions. A DL model learns an approximate\ndistribution that is close enough . Uniform samples generated\nfrom the approximate distribution would be biased samples\nfrom the original distribution resulting in biased estimates.\nAs we shall show later in the experiments, it is important\nto remove or reduce the impact of model bias to get accurate\nestimates. Bootstrapping is not applicable as it often works\nbyresampling the sample data and performing inference on\nthe sampling distribution from them. Due to the biased nature\nof samples, this approach provides incorrect results [15]. It\nis challenging to estimate the importance weight of a sample\ngenerated by V AE. Popular approaches such as IWAE [7] and\nAIS [39] do not provide strong bounds for the estimates.\nRejection Sampling. We advocate for a rejection sampling\nbased approach [22], [10] that has a number of appealing\nproperties and is well suited for AQP. Intuitively, rejection\nsampling works as follows. Let xbe a sample generated from\nthe V AE model with probabilities p(x)andq(x)from the\noriginal and approximate probability distributions respectively.\nWe accept the sample xwith probabilityp(x)\nM\u0002q(x)whereM\nis a constant upper bound on the ratio p(x)=q(x)for allx.\nWe can see that the closer the ratio is to 1, the higher the\nlikelihood that the sample is accepted. On the other hand, if\nthe two distributions are far enough, then a larger fraction of\nsamples will be rejected. One can generate arbitrary number\nof samples from the V AE model, apply rejection sampling on\nthem and use the accepted samples to generate unbiased and\naccurate aggregate estimates.\nIn order to accept/reject a sample x, we need the value of\np(x). Estimating this value - such as by going to the underlying\ndataset - is very expensive and defeats the purpose of using\ngenerative models. A better approach is to approximately\nestimate it purely from the V AE model.\nVariational Rejection Sampling Primer. We leverage an\napproach for variational rejection sampling that was recently\nproposed in [22]. For the sake of completeness, we describe\nthe approach as applied to AQP. Please refer to [22] for further\ndetails. Sample generation from V AE takes place in two steps.\nFirst, we generate a sample zin the latent space using the\nvariational posterior q(zjx)and then we use the decoder to\nconvertzinto a sample xin the original space. In order to\ngenerate samples from the true posterior p(zjx), we need to\naccept/reject sample zwith acceptance probability\na(zjx;M ) =p(zjx)\nM\u0002q(zjx)(6)\nwhereMis an upper bound on the ratio p(zjx)=q(zjx).\nEstimating the true posterior p(zjx)requires access to the\ndataset and is very expensive. However, we do know thatthe value of p(x;z)from the V AE is within a constant\nnormalization factor p(x)asp(zjx) =p(x;z)\np(x). Thus, we can\nredeﬁne Equation 6 as\na(zjx;M0) =p(x;z)\nM\u0002p(x)\u0002q(zjx)=p(x;z)\nM0\u0002q(zjx)(7)\nWe can now conduct rejection sampling if we know the\nvalue ofM0. First, we generate a sample zfrom the variational\nposteriorq(zjx). Next, we draw a random number Uin the\ninterval [0;1]uniformly at random. If this number is smaller\nthan the acceptance probability a(zjx;M0), then we accept\nthe sample and reject it otherwise. That way the number of\ntimes that we have to repeat this process until we accept a\nsample is itself a random variable with geometric distribution\np =P(U\u0014a(zjx;M0));P(N=n) = (1\u0000p)n\u00001p;n\u00151.\nThus on average the number of trials required to generate a\nsample isE(N) = 1=p. By a direct calculation it is easy\nto show [10] that p= 1=M0. We set the value of M0as\nM0=e\u0000TwhereT2[\u00001;+1]is an arbitrary threshold\nfunction. This deﬁnition has a number of appealing properties.\nFirst, this function is differentiable and can be easily plugged\ninto the V AE’s objective function thereby allowing us to learn\na suitable value of Tfor the dataset during training [22].\nPlease refer to Section VI for a heuristic method for setting\nappropriate values of Tduring model building and sample\ngeneration. Second, the parameter Twhen set, establishes a\ntrade-off between computational efﬁciency and accuracy. If\nT!+1, then every sample is accepted ( i.e.,no rejection)\nresulting into fast sample generation at the expense of the\nquality of the approximation to the true underlying distri-\nbution. In contrast when T!\u00001 , we ensure that almost\nevery sample is guaranteed to be from the true posterior\ndistribution, by making the acceptance probability small and\nas a result increasing sample generation time. Since ashould\nbe a probability we change equation Equation 7 to:\na(zjx;M0) = minh\n1;p(x;z)\nM0\u0002q(zjx)i\n(8)\nC. Towards Accuracy Guarantees\nAs mentioned in Section I, our approach is complementary\nto traditional AQP system. Our objective is to design a\nlightweight deep generative model that could be used to obtain\nquick-and-dirty aggregate estimates that are often sufﬁcient\nfor preliminary data exploration. Once the user has identi-\nﬁed promising queries that requires provable guarantees, we\ncan defer traditional AQP techniques or even obtain exact\nanswers. In this subsection, we describe an initial approach\nfor obtaining the accuracy guarantees. We would like to note\nthat developing a framework to quantify approximation errors\nof AQP based on deep generative models is a challenging\nproblem and a focus of our future research.\nEliminating Model Bias. Recall from Section IV-B that\napproximation errors incurred in our approach are due to\nmodel bias and sampling error. If the model bias is eliminated,\nthen our problem boils down to the traditional AQP setting. We\n\ncould readily leverage the rich set of accuracy guarantees and\nconﬁdence intervals developed for handling sampling error.\nThis is achieved by setting T=\u00001 and applying variational\nrejection sampling (VRS). However, this comes with a large\ncomputational cost whereby the vast majority of generated\ntuples are rejected. Ideally, we would like a granular accuracy-\ncomputation tradeoff. Increasing Timproves the sampling\nefﬁciency at the cost of model bias.\nDistribution Testing. We adapt techniques designed for high-\ndimensional two-sample hypothesis testing [46], [50] for\nchoosing appropriate T. Suppose we are given two sets of\nuniform random samples SDandSMfrom the original dataset\nand the learned model respectively. Let jSDj=jSMj. Suppose\nthat these samples were drawn from probability distributions\nPDandPM. If we can ascertain that the two distributions\nare the same (i.e. PD=PM), by using the corresponding\nsamples, then we can safely ignore the issue of model bias.\nThis is achieved by testing the null hypothesis H0:PD=PM.\nThere are two factors that makes this challenging: high-\ndimensionality and test-statistics for AQP. First, we train V AE\nmodel by transforming tuples into a vector whose dimensional-\nity ranges in the thousands. Classical tests such as Kolmogrov-\nSmirnov are not suitable for testing such high dimensional\ndistributions. Second, hypothesis testing methods rely on a test\nstatistic that is a function of SDandSMthat could be used to\ndistinguishPDandPM. For example, a simple test statistic is\nto choose an aggregate query such as estimating the average\nvalue of some attribute Ai. If the average of Aicomputed\noverSDandSMdeviates beyond certain threshold we can\nreject the null hypothesis. However, this is not appropriate for\nour scenario. We wish to test the null hypothesis for arbitrary\naggregate queries. The way out of this conundrum is to use\nCross-Match Test [46], [50].\nCross-Match Test for AQP. We begin by projecting tuples in\nSDandSMinto the latent space of V AE using the encoder.\nWe abuse the notation by representing the projected tuples as\nSDandSM. LetZ=SD[SM. We associate a label of 0\nif tuplet2SDand a label of 1ift2SM. We construct\nacomplete graph where each node corresponds to a tuple in\nZwhile the edge corresponds the Euclidean distance between\nthe latent space representation of the corresponding tuples. We\nthen compute a minimum weight perfect matching using the\nBlossom algorithm [14]. The output is a collection of non-\noverlapping pairs of tuples. Consider a speciﬁc pair of tuples\n(Zi;Zj). There are three possibilities: both tuples are from\nSD, both tuples are from SMor one each from SDandSM.\nLetaD;D;aM;M;aD;M be the frequency of pairs from the\nmatching of these three categories. The cross-match test [46],\n[50] speciﬁes aD;M as the test statistic. Let \u0011=aD;D+\naM;M +aD;M. We accept or reject the null hypothesis based\non the probability computed as\n2aD;M\u0002\u0011!\u0000\u0011\njSDj\u0001\n(aD;D)!(aM;M)!(aM;D)!(9)D. Variational Autoencoder AQP Workﬂow\nAlgorithm 1 provides the pseudocode for the overall work-\nﬂow of performing AQP using V AE. In the model building\nphase, we encode the input relation Rusing an appropriate\nmechanism (see Section IV-E). The V AE model is trained on\nthe encoded input and stored along with appropriate metadata.\nDuring the runtime phase, we generate sample SMfrom V AE\nusing variational rejection sampling with T= 0. We then\napply the hypothesis testing to ensure that the two distributions\ncannot be distinguished. If the null hypothesis is rejected, we\ngenerate a new sample SDwith a lower value of T. This\nwill ensure that the model bias issue is eliminated. One can\nthen apply existing techniques for generating approximation\nguarantees and conﬁdence intervals. Note that we use the V AE\nmodel for data exploration only after it passed the hypothesis\ntesting.\nAlgorithm 1 AQP using V AE\n1:Input: V AE modelV\n2:T= 0,SD= sample from D,\n3:Sz=fg //set of samples\n4:while samples are still needed do\n5: Samplez\u0018q(zjx)\n6: Accept or reject zbased on Equation 8\n7: Ifzis accepted, Sz=Sz[fzg\n8:SM=Decoder(Sz)// Convert samples to original space\n9:Test null hypothesis H0:PS=PDusing Equation 9\n10:ifH0is rejected then\n11:T=T\u00001\n12: Goto Step 3\n13:Output: ModelVandT\nE. Making VAE practical for relational AQP\nIn this subsection, we propose two practical improvements\nfor training V AE for AQP over relational data.\nEffective Input Encoding. One-hot encoding of tuples is an\neffective approach for relatively small attribute domains. If\nthe relation has millions of distinct values, then it causes two\nmajor issues. First, the encoded vector becomes very sparse\nresulting in poor performance [30]. Second, it increases the\nnumber of parameters learned by the model thereby increasing\nthe model size and the training time.\nA promising approach to improve one-hot encoding is to\nmake the representation denser using binary encoding . Without\nloss of generality, let the domain Dom (Aj)be its zero-\nindexed position [0;1;:::;jDom (Aj)j\u00001]. We can now con-\ncisely represent these values using dlog2jDom (Aj)jedimen-\nsional vector. Once again consider the example Dom (Aj) =\nfLow;Medium;High g. Instead of representing Ajas a 3-\ndimensional vectors ( i.e.,001;010;100), we can now rep-\nresent them indlog2(3)e= 2 -dimensional vector i.e.,\n\u0011(Low) = 00;\u0011(Medium ) = 01;\u0011(High ) = 10 . This\napproach is then repeated for each attribute resulting a d=Pn\ni=1dlog2jDom (Ai)je-dimensional vector (for nattributes)\n\nthat is exponentially smaller and denser than the one-hot\nencoding that requiresPn\ni=1jDom (Ai)jdimensions.\nEffective Decoding of Samples. Typically, samples are ob-\ntained from V AE in two steps: (a) generate a sample zin the\nlatent space i.e.,z\u0018q(zjx)and (b) generate a sample x0\nin the original space by passing zto the decoder. While this\napproach is widely used in many domains such as images and\nmusic, it is not appropriate for databases. Typically, the output\nof the decoder is stochastic. In other words, for the same value\nofz, it is possible to generate multiple reconstructed tuples\nfrom the distribution p(xjz). However, blindly generating a\nrandom tuple from the decoder output could return an invalid\ntuple. For images and music, obtaining incorrect values for a\nfew pixels/notes is often imperceptible. However, getting an\nattribute wrong could result in a (slightly) incorrect estimate\nTypically, the samples generated are often more correct than\nwrong. We could minimize the likelihood of an aberration\nby generating multiple samples for the same value of z. In\nother words, for the same latent space sample z, we generate\nmultiple samples X0=fx0\n1;x0\n2;:::;gin the tuple space.\nThese samples could then be aggregated to obtain a single\nsample tuple x0. The aggregation could be based on max ( i.e.,\nfor each attribute Aj, pick the value that occurred most in X0)\nor weighted random sampling ( i.e.,for each attribute Aj, pick\nthe value based on the frequency distribution of AjinX0).\nBoth these approaches provide sample tuples that are much\nmore robust resulting in better accuracy estimates.\nV. AQP USING MULTIPLE VAE S\nSo far we have assumed that a single V AE model is used\nto learn the data distribution. As our experimental results\nshow, even a single model could generate effective samples for\nAQP. However, it is possible to improve this performance and\ngenerate better samples. One way to accomplish this is to split\nthe dataset into say Knon-overlapping partitions and learn a\nV AE model for each of the partitions. Intuitively, we would\nexpect each of the models to learn the ﬁner characteristics of\nthe data from the corresponding partition and thereby generate\nbetter samples for that partition. In this section, we investigate\nthe problem of identifying the optimal set of Kpartitions for\nbuilding V AE models.\nA. Problem Setup\nTypically, especially in OLAP settings, tuples are grouped\naccording to hierarchies on given attributes. Such hierarchies\nreﬂect meaningful groupings which are application speciﬁc\nsuch as for example location, product semantics, year, etc.\nOften, these groupings have a semantic interpretation and\nbuilding models for such groupings makes more sense than\ndoing so on an arbitrary subset of the tuples in the dataset.\nAs an example, the dataset could be partitioned based on the\nattribute Country such that all tuples belonging to a particular\ncountry is an atomic group. We wish to identify Knon-\noverlapping groups of countries such that a V AE model is\ntrained on each group.More formally, let G=fg1;g2;:::;glgbe the set of\nexisting groups with gi\u0012Rsuch that[l\ni=1gi=R. We\nwish to identify a partition S=fs1;:::;sKgofRwhere\nsi\u0012Gandsi\\sj=;wheni6=j. Our objective is to\ngroup these lsubsets into Knon-overlapping partitions such\nthat the aggregate error of the V AEs over these partitions is\nminimized.\nEfﬁciently solving this problem involves two steps: (a) given\na partition, a mechanism to estimate the error of KV AEs\ntrained over the partition without conducting the actual training\nand (b) an algorithm that uses (a) to identify the best partition\nover the space of partitions. Both of these challenges are non-\ntrivial.\nB. Bounding VAE Errors\nQuantifying V AE Approximation. The parameters of V AE\nare learned by optimizing an evidence lower bound (ELBO)\ngiven by\nE[logP(Xjz)]\u0000DKL[Q(zjX)jjP(z)]\n(from Equation 5) which is a tight bound on the marginal\nlog likelihood. ELBO provides a meaningful way to measure\nthe distribution approximation by the V AE. Recall from Sec-\ntion IV-B that we perform rejection sampling on the V AE\nthat results in a related measure we call R-ELBO (resampled\nELBO) deﬁned as\nE[logP(Xjz)]\u0000DKL[R(zjX;T)jjP(z)]\nwhereR(zjX;T)is the resampled distribution for a user-\nspeciﬁed threshold of T. Given two V AEs trained on the same\ndataset for a ﬁxed value of T, the V AE with lower R-ELBO\nprovides a better approximation.\nBounding R-ELBO for a Partition. Let us assume that we\nwill train a V AE model for each of the atomic groups gj2G.\nWe train the model using variational rejection sampling [22]\nfor a ﬁxedTand compute its R-ELBO. In order to ﬁnd the\noptimal partition, we have to compute the value of R-ELBO\nfor arbitrary subsets si=fgi1;:::;g\u0012G. The naive approach\nwould be to train a V AE on the union of the data from atomic\ngroups insiwhich is time consuming. Instead, we empirically\nshow that it is possible to bound the R-ELBO of V AE trained\nonsiif we know the value of R-ELBO of each of gi1;:::; . Let\nf(\u0001)be such a function. In this paper, we take a conservative\napproach and bound it by sum f(r1;r2;:::;rk) =Pk\ni=1ri\nwhereriis the R-ELBO for group gi. In other words, f(\u0001)\nbounds the R-ELBO of V AE trained [k\ni=1gibyPk\ni=1ri. It is\npossible to use other functions that provide tighter bounds.\nEmpirical Validation. We empirically validated the function\nf(\u0001)on a number of datasets under a variety of settings. Table I\nshow the results for Census and Flights dataset that has been\nwidely used in prior work on AQP such as [32], [17], [19].\nPlease refer to Section VI for a description of the two datasets.\nWe obtained similar results for other benchmark datasets. For\neach of the datasets, we constructed multiple atomic groups for\ndifferent categorical attributes. For example, one could group\n\nthe Census dataset using attributes such as gender, income,\nrace etc. We ensured that each of the groups are at least 5%\nof the data set size to avoid outlier groups and if necessary\nmerged smaller groups into a miscellaneous group. We trained\na V AE model on each of the groups for different values of T\nusing variational rejection sampling and computed their R-\nELBO. We then construct all pairs, triples, and other larger\nsubsets of the groups and compare the bound obtained by\nf(\u0001)with the actual R-ELBO value of the V AE trained on\nthe data of these subsets. For each dataset, we evaluated 1000\nrandomly selected subsets and report the fraction in which the\nbound was true. As is evident in table I the bound almost\nalways holds.\nTABLE I\nEMPIRICAL VALIDATION OF R-ELBO B OUNDING\nDataset T=\u000010T= 0 T= +10\nCensus 0.992 0.997 0.996\nFlights 0.961 0.972 0.977\nC. Choosing Optimal Partition\nIn this section we assume we are provided with the value\nof R-ELBO for each of the groups gi2G;1\u0014i\u0014l,\na bounding function f(\u0001)and a user speciﬁed value K. We\npropose an algorithm that optimally splits a relation DintoK\nnon overlapping partitions S=fs1;:::;sKgwheresi\u0012G\nandsi\\sj=;wheni6=j. The key objective is to choose the\nsplitSin such a way that thePK\ni=1R-ELBO (si)is minimized.\nNote that there are Klpossible partitions and exhaustively\nenumerating and choosing the best partition is often infeasible.\nR-ELBO(g) corresponds to the actual R-ELBO for atomic\ngroupsg2Gwhile forsi\u0012G, this is estimated using the\nbounding function f(si). We investigate scenarios that occur\nin practice.\nOptimal Partition using OLAP Hierarchy. In OLAP set-\ntings, tuples are grouped according to hierarchies on given\nattributes that reﬂect meaningful semantics. We assume the\navailability of an OLAP hierarchy in the form of a tree where\nthe leaf node corresponds to the atomic groups ( e.g., Nikon\nDigital Cameras) while the intermediate groups correspond\nto product semantics ( e.g., Digital Camera!Camera!\nElectronics and so on). We wish to build V AE on meaningful\ngroups of tuples by constraining sito be selected from the\nleafs or intermediate nodes, be mutually exclusive and have the\nleast aggregate R-ELBO score. We observe that the selected\nnodes forms a tree cut that partitions the OLAP hierarchy into\nKdisjoint sub-trees.\nLet us begin by considering the simple scenario where the\nOLAP hierarchy is a binary tree. Let hdenote an arbitrary\nnode in the hierarchy with left(h) and right(h) returning\nthe left and right children of hif they exist. We propose\na dynamic programming algorithm to compute the optimal\npartition. We use the table Err[h;k]to denote aggregate R-\nELBO of splitting the sub-tree rooted at node husing atmostkpartitions where k\u0014K. The base case k= 1 is\nsimply building the V AE on all the tuples falling under node\nh. Whenk>1, we evaluate the various ways to split hsuch\nthat the aggregate R-ELBO is minimized. For example, when\nk= 2, there are two possibilities. We could either not split\nhor build two V AE models over left(h) and right(h) . The\noptimal decision could be decided by choosing the option with\nleast aggregate error. In general, we consider all possible ways\nof apportioning Kbetween the left and right sub-trees of h\nand pick the allocation resulting in least error. The recurrence\nrelation is speciﬁed by,\nErr[h;k] =8\n><\n>:R-ELBO(h) if k=1\nmin 1\u0014i\u0014k(Err[left(h);i]+\nErr[right(h);k\u0000i]) otherwise(10)\nThe extension to non-binary trees is also straightforward.\nLetC=fc1;:::;cjgbe the children of node h. We system-\natically partition the space of children into various groups of\ntwo and identify the best partitioning that gives the least error\n(eq. 11). A similar dynamic programming approach was also\nused for constructing histograms over hierarchical data in [45].\nErr[h;k] =8\n><\n>:R-ELBO(h) if k=1\nmin 1\u0014i\u0014k(Err[fc1;:::;cj=2g;i]+\nErr[fcj=2+1;:::;cjg;k\u0000i]) otherwise\n(11)\nScenario 2: Partitioning with Contiguous Atomic Groups.\nGiven the atomic groups G=fg1;:::;glg, a common sce-\nnario is to partition them into Kcontiguous subsets. This could\nbe speciﬁed as K+ 1 integers 1 =b1\u0014b2:::\u0014bK+1=l\nwhere the boundary of the i-th subset is speciﬁed by [bi;bi+1]\nand consists of a set of atomic groups fgbi;:::;gbi+1]. This is\noften desirable when the underlying attribute has a natural\nordering such as year. So we would prefer to train V AE\nmodels over data from consecutive years such as f2016\u0000\n2017;2018\u00002019ginstead of arbitrary groupings such as\nff2016;2018g;f2017;2019gg. This problem could be solved\nin near linear time ( i.e.,O(l)) by using the approach ﬁrst\nproposed in [23]. The key insight is the notion of sparse\ninterval set system that could be used to express any interval\nusing a bounded number of sparse intervals. The authors then\nuse a dynamic programming approach on the set of sparse\nintervals to identify the best partitioning.\nIn practice, Kis often determined by various other factors\nsuch as space budget for persisting the generative models.\nIdentifyingKautomatically is an interesting orthogonal prob-\nlem. Our bounding function for R-ELBO has a natural mono-\ntonic property. We empirically found that common heuristics\nfor selecting number of clusters such as Elbow method [24]\nworks well for our purpose.\nVI. E XPERIMENTS\nWe conduct a comprehensive set of experiments and demon-\nstrate that V AE (and deep generative models) are a promising\nmechanism for AQP. We reiterate that our proposed approach\n\nis an alternate way for generating samples, albeit very fast.\nMost of the prior work for improving AQP estimates could be\ntransparently used on the samples from V AE.\nA. Experimental Setup\nHardware and Platform. All our experiments were per-\nformed on a server with 16 cores, 128 GB of RAM and\nNVidia Tesla K80 GPU. We used PyTorch [43] for training\nV AE and GAN, bnlearn [47] for learning Bayesian Networks\nand MSPN [36] for mixed sum-product networks (MSPN).\nDatasets. We conducted our experiments on two real-world\ndatasets: Census [2] and Flights [1], [16]. Both datasets have\ncomplex correlated attributes and conditional dependencies\nthat make AQP challenging. The Census dataset has 8 cat-\negorical attributes and 6 numerical attributes and contains\ndemographic and employment information. The Flights dataset\nhas 6 categorical and 6 numerical attributes and contains\ninformation about on-arrival statistics for the last few years.\nWe used the data generator from [16] to scale the datasets\nto arbitrary sizes while also ensuring that the relationships\nbetween attributes are maintained. By default, our experiments\nwere run on datasets with 1 million tuples.\nDeep Generative Models for AQP. In our experiments, we\nprimarily focus on V AE for AQP as it is easy and efﬁcient\nto train and generates realistic samples [11]. By default, our\nV AE model consists of a 2 layer encoder and decoder that are\nparameterized by Normal and Bernoulli distributions respec-\ntively. We used binary encoding (Section IV-E) for converting\ntuples into a representation consumed by the encoder.\nIn order to generate high quality samples, we use rejection\nsampling during both V AE training and sample generation\nalbeit at different granularities. During training, the value of\nthresholdT(x)is set for each tuple xso that the acceptance\nprobability of samples generated from q(zjx)is roughly 0:9\nfor most tuples. We use the procedure from [22] to generate\na Monte Carlo estimate for T(x)satisfying acceptance prob-\nability constraints. While the trained model already produces\nrealistic samples, we further ensure this by performing rejec-\ntion sampling with a ﬁxed threshold T(for the entire dataset)\nduring sample generation (as detailed in Section IV-B). There\nare many ways for choosing the value of T. It could be\nprovided by the user or chosen by cross validation such that it\nprovides the best performance on query workload. By default,\nwe compute the value of Tfrom the ﬁnal epoch of training as\nfollows. For each tuple x, we have the Monte-Carlo estimate\nT(x). We select the 90-th percentile of the distribution T(x).\nIntuitively, this ensures that samples generated for 90% of the\ntuples would have acceptance probability of 0.9. Of course,\nit is possible to specify different values of Tfor queries with\nstringent accuracy requirements. We used Wasserstein GAN as\nthe architecture for generative adversarial networks [20]. We\nused entropy based discretization [12] for continuous attributes\nwhen training discrete Bayesian networks. We used the default\nsettings from [36] for training MSPN.Query Workload. We used IDEBench [16] to generate aggre-\ngate queries involving ﬁlter and group-by conditions. We then\nselected a set of 1000 queries that are diverse in various facets\nsuch as number of predicates, selectivity, number of groups,\nattribute correlation etc.\nPerformance Measures. As detailed in Section IV-B, AQP\nusing V AE introduces two sources of errors: sampling error\nand errors due to model bias. The accuracy of an estimate\ncould be evaluated by relative error (see Equation 1). For\neach query in the workload, we compute the relative error\nover a ﬁxed size sample (1% by default) obtained from the\nunderlying dataset Rand the learned V AE model. For a given\nquery, the relative error difference (RED) computed as the\nabsolute difference between the two relative errors provides\na meaningful way to compare them. Intuitively, RED will be\nclose to 0 for a well trained V AE model. We repeat this process\nover 10 different samples and report the average results. Given\nthat our query workload has 1000 queries, we use box plots\nto concisely visualize the distribution of the relative error\ndifference. The middle line corresponds to the median value\nof the difference while the box boundaries correspond to the\n25th and 75th percentiles. The top and bottom whiskers are\nset to show the 95th and 5th percentiles respectively.\nB. Experimental Results\nEvaluating Model Quality. In our ﬁrst experiment, we\ndemonstrate that V AE could meaningfully learn the data\ndistribution and generate realistic samples. Figure 2 shows\nthe distribution of relative error differences for both datasets\nover the entire query workload for various sample sizes. We\ncan see that the differences are less than 1% for almost\nall the cases for the Census dataset. The ﬂights dataset has\nmany attributes with large domain cardinalities which makes\nlearning the data distribution very challenging. Nevertheless,\nour proposed approach is still within 3% of the relative error\nobtained from the samples of R.\nImpact of Selectivity. In this experiment, we group the\nqueries based on their selectivity and compute the relative error\ndifference for each group. As shown in Figure 3, the differ-\nence is vanishingly small for queries with large selectivities\nand slowly increases for decreasing selectivities. In general,\ngenerating estimates for low selectivity queries is challenging\nfor any sampling based AQP. The capacity/model size con-\nstraints imposed on the V AE model could result in generating\nbad estimates for some queries with very low selectivities.\nHowever, this issue could be readily ameliorated by building\nmultiple V AE models that learn the ﬁner characteristics of data\nminimizing such errors in these cases.\nImpact of Model Capacity and Depth. Figures 4 and 5\nshows the impact of two important hyper parameters - the\nnumber of latent dimensions and depth of the encoder and\ndecoder. We vary the latent dimension from 10% to 100%\nof the input dimension. Large latent dimension results in an\nexpressive model that can learn complex data distributions at\n\n0.5% 1% 5%\nSampleSize0.00.10.20.30.40.5Relative Error Diﬀerence\nDataset\nCensus\nFlightsFig. 2. Varying Sample Size\n0.1-1.0 0.01-0.1 <0.01\nSelectivity0.00.20.4Relative Error Diﬀerence\nDataset\nCensus\nFlights Fig. 3. Varying Query Selectivity\n25% 50% 100%\nLatent Dimension0.00.20.40.60.8Relative Error Diﬀerence\nDataset\nCensus\nFlights Fig. 4. Varying Latent Dimension\n1 2 3\nDepth0.00.20.40.60.81.0Relative Error Diﬀerence\nDataset\nCensus\nFlights\nFig. 5. Varying Model Depth\nOne-hot Binary Integer\nInput Encoding0.00.51.01.5Relative Error Diﬀerence\nDataset\nCensus\nFlights Fig. 6. Varying Input Encoding\nNaive Importance Weighted\nOutput Encoding0.000.250.500.751.001.25Relative Error Diﬀerence\nDataset\nCensus\nFlights Fig. 7. Varying Output Encoding\nthe cost of increased model size and training time. Increasing\nthe depth results in a more accurate model but with larger\nmodel size and slower training time. Empirically, we found\nthat setting latent dimension size to 50% (for binary encoding)\nand encoder/decoder network depth of 2 provides good results.\nEffectiveness of Input Encoding and Output Decoding. It\nis our observation that the traditional approach of one-hot\nencoding coupled with generating a single sample tuple for\neach sample from the latent space does not provide realistic\ntuples. It may be suitable for image data but certainly not\nsuitable for relational data. Figure 6 shows how different\nencodings affect the generated samples. For datasets such as\nCensus where almost all attributes have small domain cardinal-\nity, all the three approaches provide similar results. However,\nfor the ﬂights dataset where some attributes have domain\ncardinality in tens of thousands, naive approaches such as one-\nhot encoding provides sub-optimal results. This is due to the\nfact that there are simply too many parameters to be learnt\nand even a large dataset of 1 Million tuples is insufﬁcient.\nSimilarly, Figure 6 shows that our proposed decoding approach\ndramatically decreases the relative error difference making the\napproach suitable for relational data. This is due to the fact\nthat the naive decoding could produce unrealistic tuples that\ncould violate common integrity constraints an effect that is\nminimized when using our proposed decoding.\nImpact of Rejection Sampling. Figure 8 shows how varying\nthe value of Timpacts the sample quality. Recall from\nSection IV-B that as T!+1, almost all samples from V AE\nare accepted, while when T! \u00001 , samples are rejected\nunless they are likely to be from the true posterior distribution.\nAs expected, decreasing the value of Tresults in decreasedvalue of relative error difference. However, this results in a\nlarger number of samples being rejected. Our approach allows\nTto be varied across queries such that queries with stringent\naccuracy requirements can use small Tfor better estimates. We\ninvestigate the impact of rejection sampling on model building\nand sample generation later in the section.\nOne versus Multiple V AEs. In the next set of experiments,\nwe consider the case where one uses multiple V AEs to\nlearn the underlying data distribution. We partitioned the\nattributes based on marital-status for Census and origin-state\nfor Flights. We evaluated partitioning data over other attributes\nand observed similar results. In order to compare the models\nfairly, we ensured that the cumulative model capacity for both\nscenarios were the same. For example, if we built KV AE\nmodels with capacity Ceach, then we compared it against\na single V AE model with capacity K\u0002C. Figure 9 shows\nthe results. As expected, the sample quality improves with\nlarger number of V AE models enabling them to learn ﬁner\ndata characteristics. Interestingly, we observe that increasing\nthe model capacity for the single V AE case has diminishing\nreturns due to the ﬁxed size of the training data. In other\nwords, increasing the capacity does not improve the perfor-\nmance beyond certain model capacity. Figure 10 compares the\nperformance of partitions selected by the dynamic program-\nming algorithm for the scenario where an OLAP hierarchy\nis provided. We compare it against a greedy algorithm. As\nexpected, our proposed approach that is cognizant of the R-\nELBO metric provides better partitions - especially datasets\nsuch as Flight that have complex R-ELBO distributions.\n\nT=-inf T=-10 T=0 T=+10 T=+inf\nT0.000.250.500.751.00Relative Error Diﬀerence\nDataset\nCensus\nFlightsFig. 8. Varying T\nK=1 K=5 K=All\nK0.00.10.20.30.4Relative Error Diﬀerence\nDataset\nCensus\nFlights Fig. 9. Varying K\nGreedy Dynamic Programming\nAlgorithm0.000.250.500.751.00Relative Error Diﬀerence\nDataset\nCensus\nFlights Fig. 10. Partition Algorithms\nVAE MSPN GAN BNDBEst NC HistWavelets\nGenerative Model012Relative Error Diﬀerence\nFig. 11. Performance of DL models for AQP\n100K 1M 10M 100M 1B\nSize0100020003000Training Time (s)\nT\nT=-inf\nT=-10\nT=0\nT=+10\nT=+inf Fig. 12. Performance of Model Building\n1K 10K 100K 1M\nSize02468Sampling Time (s)\nT\nT=-inf\nT=-10\nT=0\nT=+10\nT=+inf Fig. 13. Performance of Sample Generation\nC. Comparison with DL Model for AQP .\nWhile we primarily focused on V AE, it is possible to\nleverage other deep generative models for AQP. Figure 11\ncompares the performance of three common models : V AE,\nGAN and Bayesian Networks (BN). Generative Adversarial\nNetworks (GANs) [21], [20] are a popular and powerful\nclass of generative models that learn the distribution as a\nminimax game between two components - generator (that\ngenerates data) and discriminator (that identiﬁes if the sample\nis from the true distribution or not). (Deep) Bayesian networks\n(BN) are another effective generative model that speciﬁes\nthe joint distribution as a directed graphical model where\nnodes correspond to random variable Ai(for attribute Ai) and\ndirected edges between nodes signify (direct) dependencies\nbetween the corresponding attributes. Please refer to [20] for\nmore details. In order to ensure a fair comparison, we imposed\na constraint that the model size for all three approaches are\nﬁxed. Furthermore, V AE provides the best results for a ﬁxed\nmodel size. GANs provide reasonable performance but was\nheavily reliant on tuning. Training a GAN requires identifying\nan equilibria and tuning of many parameters such as the model\narchitecture and learning rate [21]. This renders the approach\nhard to use in practise for general data sets. Identifying appro-\npriate mechanisms for training GANs over relational data for\nAQP is a promising avenue for future research. BNs provide\nthe worst result among the three models. While BNs are easy\nto train for datasets involving discrete attributes, a hybrid\ndataset with discrete and continuous attributes, and attributes\nwith large domain cardinalities are challenging. When the\nbudget on model size is strict, BNs often learn a sub-optimal\nmodel.We also evaluated V AE against the recently proposed\nMSPN [36] that has been utilized for AQP in [32]. Similar\nto Bayesian Networks, MSPNs are acyclic graphs (albeit\nrooted graphs) with sum and product nodes as internal nodes.\nIntuitively, the sum nodes split the dataset into subsets while\nproduct nodes split the attributes. The leaf nodes deﬁne the\nprobability distributions for an individual variable. MSPN\ncould be used to represent an arbitrary probability distribu-\ntion [36]. We used the random sampling procedure from [32]\nfor generating samples from a trained MSPN. We observed\nthat MSPN often struggles to model distributions involving\nlarge number of attributes and/or tuples and that using a single\nMSPN for the entire model did not provide good results. As\na comparison to train a V AE on 1M tuples of the Census\ndata set on all attributes requires a few minutes versus almost\n3.5 hours for MSPN. In addition the accuracy of queries\nwith larger number of attributes for the case of MSPN was\nvery poor and not close to any of the other models. Hence,\nwe decided to provide an advantage to MSPN, building the\nmodel over subsets of attributes. That way we let the model\nfocus only on speciﬁc queries and improve its accuracy. There\nwere around 120 distinct combination of measure and ﬁlter\nattributes in our query workload. We built MSPN models for\neach combination of attributes, generate samples from it and\nevaluate our queries over it. For example, if a query involved\nan aggregate over Axand ﬁlter condition over fAi;Aj;Akg,\nwe built an MSPN over the projected dataset containing only\nfAx;Ai;Aj;Akg. Unlike GAN and BN, we did not control the\nnumber of leaf nodes. However, the size of the MSPN models\nthat were trained over attribute subsets were in the same\nballpark as the other generative models. Figure 11 presents the\nperformance of V AE and MSPN (build on specialized subsets\n\nof attributes) to be superior over GAN and BN. However,\nin the case of V AE the model was trained over the entire\ndataset being able to answer arbitrary queries while MSPN\nwas trained over speciﬁc attribute subsets utilized by speciﬁc\nqueries. Even in this case, providing full advantage to MSPN,\nthe median relative error difference for V AE and MSPN were\n0.060835 and 0.137699 respectively, more than two times\nbetter for V AE. This clearly demonstrates that a V AE model\ncan learn a better approximation of the data, being able to\nanswer arbitrary queries while it can be trained an order of\nmagnitude faster than MSPN as detailed next.\nNext, we compare our approach with DBEst [33] and Neu-\nralCubes [49] that use ML models for answer AQP queries.\nFigure 11 compares the performance of our approach against\nthese methods. In contrast to our approach that uses synthetic\nsamples, DBEst and NeuralCubes use pre-built models to\ndirectly answer AQP queries. For simple aggregate queries,\nthe performance of both these methods are comparable to\nthat of our approach. However, our approach produces more\naccurate result for ad-hoc queries that are very common in\ndata exploration. Furthermore, the ability of our approach to\ncreate arbitrary number of samples to achieve low error that\nis not possible with DBEst and NeuralCubes.\nPerformance Experiments. Our next set of experiments\ninvestigate the scalability of V AE for different dataset sizes\nand values of threshold T. Figure 12 depicts the results for\ntraining over a single GPU. All results would be substantially\nbetter with the use of multiple GPUs. As expected, the\ntraining time increases with larger dataset size. However, due\nto batching and other memory optimizations, the increase\nis sublinear. Next, incorporating rejection sampling has an\nimpact on the training time with stringent values of Trequiring\nmore training time. The increased time is due to the larger\nnumber of training epochs needed for the model to learn\nthe distribution. The validation procedure for evaluating the\nrejection rate uses a Monte Carlo approach [22] that also\ncontributes to the increased training time. However overall it\nis evident from our results that very large data sets can be\ntrained very efﬁciently even on a single GPU. This attests\nto the practical utility of the proposed approach. Figure 13\npresents the cost of generating samples of different sizes and\nfor various values of T. Not surprisingly, lower values of T\nrequire a larger sampling time due to the higher number of\nrejected samples. As Tbecomes less stringent, sampling time\ndramatically decreases. Interestingly, the sampling time does\nnot vary a lot for different sampling sizes. This is due to the\nefﬁcient vectorized implementation of the sampling procedure\nin PyTorch and the availability of larger memory that could\neasily handle samples of large size. It is evident again that the\nproposed approach can generate large number of samples in\nfractions of a second making the approach highly suitable for\nfast query answering with increased accuracy.VII. R ELATED WORK\nDeep Learning for Databases. Recently, there has been\nincreasing interest in applying deep learning techniques for\nsolving fundamental problems in databases. SageDB [28]\nproposes a new database architecture that integrates deep\nlearning techniques to model data distribution, workload and\nhardware and use it for indexing, join processing and query\noptimization. Deep learning has also been used for learning\ndata distribution to support index structures [29], join car-\ndinality estimation [27], [40], join order enumeration [31],\n[35], physical design [44], entity matching [13], workload\nmanagement [34] and performance prediction [48].\nSampling based Approximate Query Processing. AQP\nhas been extensively studied by the database community.\nA detailed surveys is available elsewhere [18], [38]. Non\nsampling based approaches involve synopses data structures\nsuch as histograms, wavelets and sketches. They are often\ndesigned for speciﬁc types of queries and could answer them\nefﬁciently. In our paper, we restrict ourselves to sampling\nbased approaches [3], [4], [42], [26], [8]. Samples could either\nbe pre-computed or obtained during runtime. Pre-computed\nsamples often leverage prior knowledge about workloads to\nselect samples that minimize the estimation error. However,\nif workload is not available or is inaccurate, the chosen\nsamples could result in worse approximations. In this case,\nrecomputing samples is often quite expensive. Our model\nbased approach could easily avoid this issue by generating\nsamples as much as needed on-demand. Online aggregation\nbased approaches such as [25], [51] continuously reﬁne the\naggregate estimates during query execution. The execution\ncan be stopped at any time if the user is satisﬁed with\nthe estimate. Prior approaches often expect the data to be\nretrieved in a random order which could be challenging. Our\nmodel based approach could be easily retroﬁtted into online\naggregation systems as they could generate random samples\nefﬁciently. Answering ad-hoc queries and aggregates over rare\nsub-populations is especially challenging [9] . Our approach\noffers a promising approach where as many samples as needed\ncould be generated to answer such challenging queries without\nhaving to access the dataset. [32] uses mixed sum-product\nnetworks (MSPN) to generate aggregate estimates for interac-\ntive visualizations. While in the same spirit as our work, their\nproposed approach suffers from scalability issues that limits\nits widespread applicability. Even for a small dataset with 1\nmillion tuples, it requires hours for training. This renders such\nan approach hard to apply for very large data sets. In contrast\na V AE model can be trained in a matter of minutes making it\nideal for very large data sets.\nVIII. C ONCLUSION\nWe proposed a model based approach for AQP and demon-\nstrated experimentally that the generated samples are realistic\nand produce accurate aggregate estimates. We identify the\nissue of model bias and propose a rejection sampling based\napproach to mitigate it. We proposed dynamic programming\n\nbased algorithms for identifying optimal partitions to train\nmultiple generative models. Our approach could integrated\neasily into AQP systems and can satisfy arbitrary accuracy\nrequirements by generating as many samples as needed with-\nout going back to the data. There are a number of interesting\nquestions to consider in the future. Some of them include\nbetter mechanisms for generating conditional samples that\nsatisfy certain constraints. Moreover, it would be interesting\nto study the applicability of generative models in other data\nmanagement problems such as synthetic data generation for\nstructured and graph databases extending ideas in [41].\nIX. A CKNOWLEDGMENTS\nThe work of Gautam Das was supported in part by grants\n1745925 and 1937143 from the National Science Foundation,\nand a grant from AT&T.\nREFERENCES\n[1] Bureau of transportation statistics. Flights Data Set , 2019.\n[2] UCI Machine Learning Repository. Adult Data Set , 2019.\n[3] S. Acharya, P. B. Gibbons, V . Poosala, and S. Ramaswamy. The\naqua approximate query answering system. In ACM Sigmod Record ,\nvolume 28, pages 574–576. ACM, 1999.\n[4] S. Agarwal, B. Mozafari, A. Panda, H. Milner, S. Madden, and I. Stoica.\nBlinkdb: Queries with bounded errors and bounded response times on\nvery large data. In EuroSys , 2013.\n[5] J. Altosaar. Tutorial - What is a variational autoencoder? , 2018.\n[6] Y . Bengio, I. J. Goodfellow, and A. Courville. Deep learning. Nature ,\n521(7553):436–444, 2015.\n[7] Y . Burda, R. Grosse, and R. Salakhutdinov. Importance weighted\nautoencoders. arXiv preprint arXiv:1509.00519 , 2015.\n[8] S. Chaudhuri, G. Das, and V . Narasayya. Optimized stratiﬁed sampling\nfor approximate query processing. TODS , 32(2):9, 2007.\n[9] S. Chaudhuri, B. Ding, and S. Kandula. Approximate query processing:\nNo silver bullet. In SIGMOD , 2017.\n[10] W. G. Cochran. Sampling techniques . John Wiley & Sons, 2007.\n[11] C. Doersch. Tutorial on variational autoencoders. arXiv preprint\narXiv:1606.05908 , 2016.\n[12] J. Dougherty, R. Kohavi, and M. Sahami. Supervised and unsupervised\ndiscretization of continuous features. In MLR . Elsevier, 1995.\n[13] M. Ebraheem, S. Thirumuruganathan, S. Joty, M. Ouzzani, and N. Tang.\nDistributed representations of tuples for entity resolution. PVLDB ,\n11(11):1454–1467, 2018.\n[14] J. Edmonds. Paths, trees, and ﬂowers. Canadian Journal of mathematics ,\n17:449–467, 1965.\n[15] B. Efron and R. J. Tibshirani. An introduction to the bootstrap . CRC\npress, 1994.\n[16] P. Eichmann, C. Binnig, T. Kraska, and E. Zgraggen. Idebench: A\nbenchmark for interactive data exploration. arXiv:1804.02593 , 2018.\n[17] A. Galakatos, A. Crotty, E. Zgraggen, C. Binnig, and T. Kraska.\nRevisiting reuse for approximate query processing. PVLDB , 2017.\n[18] M. N. Garofalakis and P. B. Gibbons. Approximate query processing:\nTaming the terabytes. In VLDB , pages 343–352, 2001.\n[19] L. Getoor, B. Taskar, and D. Koller. Selectivity estimation using\nprobabilistic models. In ACM SIGMOD Record , volume 30, 2001.\n[20] I. Goodfellow, Y . Bengio, and A. Courville. Deep Learning . MIT Press,\n2016. http://www.deeplearningbook.org.\n[21] I. J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks.\nCoRR , abs/1701.00160, 2017.\n[22] A. Grover, R. Gummadi, M. Lazaro-Gredilla, D. Schuurmans, and\nS. Ermon. Variational rejection sampling. In AISTATS , 2018.\n[23] S. Guha, N. Koudas, and D. Srivastava. Fast algorithms for hierarchical\nrange histogram construction. In PODS , 2002.\n[24] J. Han, J. Pei, and M. Kamber. Data mining: concepts and techniques .\nElsevier, 2011.[25] J. M. Hellerstein, P. J. Haas, and H. J. Wang. Online aggregation. In\nAcm Sigmod Record , volume 26, pages 171–182. ACM, 1997.\n[26] S. Kandula, A. Shanbhag, A. Vitorovic, M. Olma, R. Grandl, S. Chaud-\nhuri, and B. Ding. Quickr: Lazily approximating complex adhoc queries\nin bigdata clusters. In SIGMOD , pages 631–646. ACM, 2016.\n[27] A. Kipf, T. Kipf, B. Radke, V . Leis, P. Boncz, and A. Kemper. Learned\ncardinalities: Estimating correlated joins with deep learning. CIDR ,\n2019.\n[28] T. Kraska, M. Alizadeh, A. Beutel, E. Chi, J. Ding, A. Kristo, G. Leclerc,\nS. Madden, H. Mao, and V . Nathan. Sagedb: A learned database system.\nCIDR, 2019.\n[29] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case\nfor learned index structures. In SIGMOD , pages 489–504. ACM, 2018.\n[30] R. G. Krishnan and M. Hoffman. Inference and introspection in deep\ngenerative models of sparse data. Advances in Approximate Bayesian\nInference Workshop at NIPS , 2016.\n[31] S. Krishnan, Z. Yang, K. Goldberg, J. Hellerstein, and I. Stoica. Learning\nto optimize join queries with deep reinforcement learning. arXiv preprint\narXiv:1808.03196 , 2018.\n[32] M. Kulessa, A. Molina, C. Binnig, B. Hilprecht, and K. Kersting. Model-\nbased approximate query processing. arXiv:1811.06224 , 2018.\n[33] Q. Ma and P. Triantaﬁllou. Dbest: Revisiting approximate query\nprocessing engines with machine learning models. In SIGMOD , 2019.\n[34] R. Marcus and O. Papaemmanouil. Releasing cloud databases for the\nchains of performance prediction models. In CIDR , 2017.\n[35] R. Marcus and O. Papaemmanouil. Deep reinforcement learning for join\norder enumeration. arXiv preprint arXiv:1803.00055 , 2018.\n[36] A. Molina, A. Vergari, N. Di Mauro, S. Natarajan, F. Esposito, and\nK. Kersting. Mixed sum-product networks: A deep architecture for\nhybrid domains. In AAAI , 2018.\n[37] B. Mozafari. Approximate query engines: Commercial challenges and\nresearch opportunities. In SIGMOD , pages 521–524, 2017.\n[38] B. Mozafari and N. Niu. A handbook for building an approximate query\nengine. IEEE Data Eng. Bull. , 38(3):3–29, 2015.\n[39] R. M. Neal. Annealed importance sampling. Statistics and computing ,\n11(2):125–139, 2001.\n[40] J. Ortiz, M. Balazinska, J. Gehrke, and S. S. Keerthi. Learning state\nrepresentations for query optimization with deep reinforcement learning.\nInDEEM Workshop , page 4. ACM, 2018.\n[41] N. Park, M. Mohammadi, K. Gorde, S. Jajodia, H. Park, and Y . Kim.\nData synthesis based on generative adversarial networks. PVLDB ,\n11(10):1071–1083, 2018.\n[42] Y . Park, B. Mozafari, J. Sorenson, and J. Wang. Verdictdb: universalizing\napproximate query processing. In SIGMOD , 2018.\n[43] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\nA. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in\npytorch. 2017.\n[44] A. Pavlo, G. Angulo, J. Arulraj, H. Lin, J. Lin, L. Ma, P. Menon, T. C.\nMowry, M. Perron, I. Quah, et al. Self-driving database management\nsystems. In CIDR , 2017.\n[45] F. Reiss, M. Garofalakis, and J. M. Hellerstein. Compact histograms for\nhierarchical identiﬁers. In Proceedings of the 32nd international con-\nference on Very large data bases , pages 870–881. VLDB Endowment,\n2006.\n[46] P. R. Rosenbaum. An exact distribution-free test comparing two multi-\nvariate distributions based on adjacency. JRSS: Series B , 67(4):515–530,\n2005.\n[47] M. Scutari and J.-B. Denis. Bayesian Networks with Examples in R .\nChapman and Hall, Boca Raton, 2014.\n[48] S. Venkataraman, Z. Yang, M. J. Franklin, B. Recht, and I. Stoica.\nErnest: Efﬁcient performance prediction for large-scale advanced an-\nalytics. In NSDI , pages 363–378, 2016.\n[49] Z. Wang, D. Cashman, M. Li, J. Li, M. Berger, J. A. Levine, R. Chang,\nand C. Scheidegger. Neuralcubes: Deep representations for visual data\nexploration. CoRR , abs/1808.08983, 2018.\n[50] L. Wasserman. Modern Two-Sample Tests , 2012.\n[51] S. Wu, B. C. Ooi, and K.-L. Tan. Continuous sampling for online\naggregation over multiple queries. In SIGMOD , 2010.",
  "textLength": 67764
}