{
  "paperId": "bfe69bb71e2b2eadca48ab0c414540e30a47224d",
  "title": "CAMAL: Optimizing LSM-trees via Active Learning",
  "pdfPath": "bfe69bb71e2b2eadca48ab0c414540e30a47224d.pdf",
  "text": "CAMAL: Optimizing LSM-trees via Active Learning\nWEIPING YU, Nanyang Technological University, Singapore\nSIQIANG LUOâ€ ,Nanyang Technological University, Singapore\nZIHAO YU, Nanyang Technological University, Singapore\nGAO CONG, Nanyang Technological University, Singapore\nWe use machine learning to optimize LSM-tree structure, aiming to reduce the cost of processing various\nread/write operations. We introduce a new approach Camal , which boasts the following features: (1) ML-\nAided :Camal is the first attempt to apply active learning to tune LSM-tree based key-value stores. The\nlearning process is coupled with traditional cost models to improve the training process; (2) Decoupled Active\nLearning : backed by rigorous analysis, Camal adopts active learning paradigm based on a decoupled tuning\nof each parameter, which further accelerates the learning process; (3) Easy Extrapolation :Camal adopts\nan effective mechanism to incrementally update the model with the growth of the data size; (4) Dynamic\nMode :Camal is able to tune LSM-tree online under dynamically changing workloads; (5) Significant System\nImprovement : By integrating Camal into a full system RocksDB, the system performance improves by 28%\non average and up to 8x compared to a state-of-the-art RocksDB design.\nCCS Concepts: â€¢Information systems â†’Data management systems ;â€¢Theory of computation â†’\nData structures design and analysis .\nAdditional Key Words and Phrases: LSM-tree, optimization, active learning\nACM Reference Format:\nWeiping Yu, Siqiang Luo, Zihao Yu, and Gao Cong. 2024. CAMAL: Optimizing LSM-trees via Active Learning.\nProc. ACM Manag. Data 2, N4 (SIGMOD), Article 202 (September 2024), 26 pages. https://doi.org/10.1145/\n3677138\n1 Introduction\nLSM-Tree based Key-Values Stores. Key-value stores, increasingly prevalent in industry, underpin\napplications in social media [ 8,12], stream and log processing [ 14,18], and file systems [ 43,73].\nNotably, platforms like RocksDB [ 30] at Facebook, LevelDB [ 32] and BigTable [ 16] at Google,\nHBase [ 1] and Cassandra [ 7] at Apache, X-Engine [ 39] at Alibaba, WiredTiger [ 2] at MongoDB,\nand Dynamo [ 27] at Amazon extensively utilize Log-Structured Merge (LSM) trees [ 62] for high-\nperformance data ingestion and fast reads.\nAn LSM-tree is a multi-level data structure that operates on key-value pairs. The top level of\nthe LSM-tree has a smaller capacity and stores the freshest data, while the lower levels can hold\nexponentially more data but with progressively older timestamps. Initially, data is inserted into\nLevel-0 (a.k.a. buffer level), until it is full and sort-merged into the next deeper level. The capacity\nâ€ Corresponding Author\nAuthorsâ€™ Contact Information: Weiping Yu, Nanyang Technological University, Singapore, WEIPING001@e.ntu.edu.sg;\nSiqiang Luo, Nanyang Technological University, Singapore, siqiang.luo@ntu.edu.sg; Zihao Yu, Nanyang Technological\nUniversity, Singapore, zihao.yu@ntu.edu.sg; Gao Cong, Nanyang Technological University, Singapore, GAOCONG@ntu.\nedu.sg.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\nÂ©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM 2836-6573/2024/9-ART202\nhttps://doi.org/10.1145/3677138\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.arXiv:2409.15130v1  [cs.DB]  23 Sep 2024\n\n202:2 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\nPure theoretical cost models\nPlain ML\nCAMALless training cost\nTraining Sample\nSystem Latencyfaster queryPlain AL+ previous samples information\n+ complexity analysis \nFig. 1. Illustrating plain ML approach (e.g., polynomial regression), plain active learning approach and our\nCamal regarding the tradeoff between the training samples and system performance.\nof Levelğ‘–+1isğ‘‡times than that of Level ğ‘–, whereğ‘‡is referred to as the size ratio. Similar merge\nbehavior happens in any two consecutive levels, leading to multiple sorted runs in each level, where\neach run has an associated Bloom filter to facilitate lookups.\nInstance-Optimized LSM-Trees. LSM-trees are commonly used in supporting diverse workloads\nwith varying percentages of operation types, such as point lookups ,range lookups anddata writes .\nThe point lookup (resp. range lookup) is a query that extracts the value (resp. values) corresponding\nto a given key (resp. key range), whereas the data writes are operations to insert, delete or update\nthe value for a key. The various possibility of the workload raises a crucial question of how to\nselect suitable parameters (e.g., size ratio, compaction policy, memory allocation between buffer\nlevel and Bloom filters) to construct an LSM-tree optimized for a given workload, leading to the\nnotion of instance-optimized LSM-trees.\nSeveral studies [ 23,26,40,41] have explored instance-optimized LSM-tree designs, with Dos-\ntoevsky [ 23] and K-LSM [ 41] focusing on compaction policy tuning, LSM-Bush [ 26] discussing\nthe choice of size ratio between adjacent levels, and Endure [ 40] investigating LSM-tree tuning\nunder workload uncertainty. All these methods can be classified as complexity-based models, which\nprimarily rely on complexity analysis to predict the I/O cost of each operation. Orthogonal to\nthese approaches, in this paper we ask whether machine learning (ML) can give an even more\nfiner-grained tuning of LSM-tree based key-value stores. The potential of machine learning based\nmethods is that the mapping between system knobs and performance is directly captured in an\nend-to-end manner, instead of being implicitly obtained via a complexity-based cost model. Such\nmachine-learning aided approach has been proven effective in other data structures such as search\ntrees [ 49] and spatial data structures [ 33], yet it has been rarely explored for disk-resident data\nstructures such as LSM-trees. It is important to note that the goal of using ML for LSM-tree tuning\nis not to replace the techniques mentioned above; instead, we aim to explore the power of ML in\ntuning LSM-trees, seeing how to harness the strengths of these two kinds of methods, and this\nhybrid technique would be one key element in our design.\nOur Idea: Active learning for instance-optimized LSM-trees. To achieve ML-based tuning, a\nplain ML approach is expected to train a model, which predicts the LSM-tree performance for a\ngiven workload, and is later used for searching a desired configuration of the LSM-tree. The model\ntraining process starts with collecting samples, followed by feeding the samples to the model for\nfitting the parameters. A sample is in the form of (ğ‘Š,ğ‘‹,ğ‘Œ), whereğ‘Šis a workload, ğ‘‹is a point in\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:3\nthe configuration space formed by all possible ranges for each tunable parameter, and ğ‘Œis the true\nrunning time for workload ğ‘Šusing the LSM-tree constructed by parameters ğ‘‹.\nUnfortunately, the cost of sample collection can be prohibitive. Evaluating the system perfor-\nmanceğ‘Œin a sample requires the ingestion of the entire dataset into the LSM-tree system, followed\nby the execution of a sufficiently large workload ğ‘Šcomprising numerous queries. Consequently,\nobtaining a single sample with 10GB data may consume several minutes [ 40], with the time frame\nescalating at an exponentially rapid pace as the dataset grows. Moreover, given the extensive\nconfiguration space, a considerable number of samples are required to ensure the modelâ€™s accuracy,\nleading to an impractical cost.\nTo address the challenge, we turn to the active learning (AL) paradigm [ 38,55,59], which has\nbeen proven capable of refining the sample quality in an interactive manner, and thus fewer samples\nare required to secure the model accuracy. For a training workload, AL involves iterative sampling\nrounds until the given sampling budget is exhausted. Initially the sample set only contains one set\nof random samples. Each round commences with training an ML model using the existing samples\nin the set. The model then identifies and selects a new sample that is predicted to have the lowest\ncost by the newly trained model. This new sample is subsequently added to the sample set, and if\nthe budget permits, the next sampling round is triggered. As the training process advances through\neach round, the ML modelâ€™s accuracy progressively improves. The quality of the selected samples\nalso benefits from this refinement over time.\nChallenges and New Designs. Although active learning effectively refines the exploration in\nconfiguration space, we foresee three challenges when applying it to optimize LSM-trees. Firstly,\nby default, the initial sample is chosen randomly, which may deviate significantly from the true\noptimum, requiring additional rounds of exploration. Secondly, the tunable parameters are not\neffectively decoupled throughout the sampling process, failing to narrow down the configuration\nspace during the process. Lastly, in the presence of dynamic workloads, re-training becomes\nnecessary and introduces extra costs to maintain model accuracy.\nThe benefits of our approach Camal stem from the following novel designs, which address the\nchallenges respectively.\nDesign 1: Complexity-analysis driven techniques to avoid random initialization. Our\ninsight is that the optimal LSM-tree parameter settings obtained through a complexity-based cost\nmodel (e.g., the one in [ 22]) still provides much better results than a random sample, although\nthey may not be the true optimum within the configuration space. As a result, this cost model can\nefficiently and effectively guide active learning to pinpoint the neighborhood containing the true\noptimum. Specifically, we can initiate the active learning process with the theoretically optimal\nsolution obtained from the complexity analysis in each training workload, to significantly prune\nthe sampling space for each parameter.\nDesign 2: Decoupling parameters for faster approaching a desired solution. To reduce\nthe vast sampling space due to the combination of different parameters, we propose a novel\nhierarchical sampling technique that decouples each parameter from the complex I/O model. We\nhave discovered that although the parameters may be correlated, their optimal settings can be\nrelatively independent, allowing tuning the parameter one round at a time. In particular, we establish\na theoretical foundation, allowing us to first assess the desired values of one parameter, and then\naddress more intricate dependencies of other parameters to guide the sampling rounds in active\nlearning.\nDesign 3: Extrapolation strategy for data growth. While decoupled active learning effectively\nnarrows the sampling space, the training cost tends to rise with larger data sizes. To accommodate\ndata growth, we have theoretically proven that it is possible to rapidly transition to new tuned\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:4 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\nTerm Definition Unit\nğ‘ total number of entries entries\nğ¿ number of levels levels\nğµ number of entries that fit in a storage block entries\nğ¸ size of a key-value entry bits\nğ‘‡ size ratio between adjacent levels\nğ‘‡ğ‘™ğ‘–ğ‘š size ratio at which ğ¿converges to 1\nğ‘€ğ‘ memory allocated for write buffer bits\nğ‘€ğ‘“ memory allocated for bloom filters bits\nğ‘€ğ‘ memory allocated for block cache bits\nğ‘€ total memory ( ğ‘€ğ‘+ğ‘€ğ‘“+ğ‘€ğ‘) bits\nğ‘£ percentage of zero-result point lookups\nğ‘Ÿ percentage of non-zero-result point lookups\nğ‘ percentage of range lookups\nğ‘¤ percentage of writes\nğ‘  selectivity of range lookups entriesOperation Leveling Tiering\nzero-result point\nlookups (ğ‘‰)ğ‘’âˆ’ğ‘€ğ‘“\nğ‘ğ‘’âˆ’ğ‘€ğ‘“\nğ‘Â·ğ‘‡\nnon-zero-result\npoint lookups ( ğ‘…)ğ‘’âˆ’ğ‘€ğ‘“\nğ‘+1ğ‘’âˆ’ğ‘€ğ‘“\nğ‘Â·ğ‘‡+1\nrange lookups ( ğ‘„)ğ¿+ğ‘ \nğµğ¿Â·ğ‘‡+ğ‘‡Â·ğ‘ \nğµ\nwrites (ğ‘Š)ğ¿Â·ğ‘‡\nğµğ¿\nğµ\nTheoretical  I/OLevelingLeveling\nTieringTiering\nOptimal Optimal50% + 50% 50% + 50%\nFig. 2. Parameters of LSM-trees and workloads used throughout the paper, complexity-based I/O cost models,\nand base sampling space of instance-optimized LSM-trees.\nparameters from existing ones without retraining. This method helps avoid the exponential increase\nin training costs, leading to greater efficiency.\nDesign 4: Dynamic mode. In practical scenarios, system faces dynamically changing workloads.\nWe then design the dynamic LSM-tree based on the extrapolation strategy to combat the challenge,\nsee Section 6.\nContributions. In summary, Figure 1 illustrates Camal â€™s position relative to other ML approaches\nbased on our experimental evaluation in Section 8. Our contributions are summarized as follows.\nâ€¢We propose a new model named Camal , the first attempt to apply active learning for LSM-tree\ninstance optimizations. It integrates the complexity-based models to align active learning to the\nLSM-tree context (Â§ 3).\nâ€¢We present a novel hierarchical sampling technique, which is particularly designed for LSM-\ntree applications, to reduce the sampling space, significantly cutting down training time and\nimproving practical usability (Â§ 4).\nâ€¢Our model Camal embraces data growth, in that it reasonably extrapolates the desired settings\nwithout retraining (Â§ 5).\nâ€¢Equipped with the extrapolation strategy, we have enhanced Camal to handle dynamically\nchanging scenarios. We further introduce a novel design named DLSM, an LSM-tree variant\nspecifically engineered to adapt to dynamic workloads (Â§ 6).\nâ€¢We examine three widely used ML models to be embedded into Camal and discuss their benefits\nand drawbacks (Â§ 7).\nâ€¢We integrated our method with the widely-used LSM key-value database, RocksDB, to demon-\nstrate its practicality. Our approach can significantly reduce latency by up to 8x, compared to a\nstate-of-the-art RocksDB design (Â§ 8).\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:5\n2 Background\nLSM-tree Structure. An LSM-tree is structured with multiple levels, where each level contains one\nor multiple sorted runs. New data is initially stored in an unsorted state in a memory buffer, whose\nfilled-up will trigger a write to the first level of the LSM-tree. As each level becomes full, its data\nis sorted and merged into the next level recursively. During point lookup, the LSM-tree searches\nthe most recent level first, followed by lower levels until it finds the matching data. Additionally,\nBloom filters optimize point lookup by efficiently determining if a key exists in a sorted run without\nperforming I/Os. The capacity of each level in an LSM-tree grows by a size ratio of ğ‘‡. Therefore,\nthe number of levels ğ¿is determined by the size ratio ğ‘‡, the memory allocated to the write buffer\nğ‘€ğ‘, and the size of the data. Level ğ‘–hasğ‘€ğ‘\nğ¸Â·(ğ‘‡âˆ’1)ğ‘‡ğ‘–âˆ’1entries [ 40], whereğ¸represents the size\nof an entry. If there are ğ‘entries in the LSM-tree, the number of levels can be calculated as:\nğ¿=\u0018\nlogğ‘‡(ğ‘Â·ğ¸\nğ‘€ğ‘+1)\u0019\n(1)\nIn line with Dostoevsky [ 23], we restrict the size ratio range to 2â‰¤ğ‘‡â‰¤ğ‘‡ğ‘™ğ‘–ğ‘š, whereğ‘‡ğ‘™ğ‘–ğ‘šis defined\nasğ‘Â·ğ¸/ğ‘€ğ‘™ğ‘–ğ‘š\nğ‘.\nWorkload. Following previous works [ 22,23,40], key-value databases commonly involve four\ntypes of operations: zero-result point lookups, non-zero-result point lookups, range lookups, and\nwrites. In the case of point lookups, a single key is searched in the database, and the Bloom filter is\noften used to determine whether the key is located in the block before proceeding with the actual\nblock reading. Point reads can be further divided based on whether the key exists in the database,\ncategorized as zero-result point lookups and non-zero-result point lookups. Range lookups aim\nto find the most recent versions of all entries within a specified key range. This process entails\nmerging the relevant key range across all runs at every level while sorting the entries. Writes,\nincluding inserts, deletes, and modifications, typically involve appending a new key in the write\nbuffer instead of locating the older version for in-place updates. In practice, workloads for key-value\ndatabases are often comprised of varying proportions of the four operations.\nComplexity-analysis based Model. The complexity-analysis based Model, or theoretical I/O\nmodel, analyzes the number of I/Os generated per query, based on a given workload and basic\nparameters of an LSM-tree. We adopt the state-of-the-art Bloom filter bits-allocation strategy\nproposed in Monkey [ 22], which designs different false positive rates (FPRs) for Bloom filters\nin different levels to achieve optimal memory allocation. We also follow the models derived in\nMonkey [ 22] for the four workload types, as shown in Figure 2. These models have been widely\nrecognized and adopted in later works, such as Dostoevsky [ 23] and Endure [ 40]. It is important to\nnote that although we use a relatively simple complexity-based model, it serves the purpose in our\nhybrid framework that combines the complexity-based model and ML model because the goal of\nusing the complexity-based model is to estimate a reasonable range. Our framework can integrate\nwith a more sophisticated model when necessary. According to the cost model of each operation\nin Figure 2, the overall average cost can be calculated based on the known proportions ( ğ‘£,ğ‘Ÿ,ğ‘,ğ‘¤ ) of\nthe four operations:\nğ‘“=ğ‘£Â·ğ‘‰+ğ‘ŸÂ·ğ‘…+ğ‘Â·ğ‘„+ğ‘¤Â·ğ‘Š (2)\nObjectives. This paper aims to minimize the end-to-end latency incurred by the input workload,\ngiven the knowledge of the system and workload. We consider both the static mode (Sections 3-5)\nwhere the workload is stable, and the dynamic mode (Section 6) where the workload changes online.\nFigure 2 provides examples of how the performance varies across different workloads based on\nspecific parameters. The system includes information such as the number of entries ğ‘, the total\nmemory budget ğ‘€, the size of an entry ğ¸, and the number of entries ğµthat fit in a storage block.\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:6 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\nTable 1. Operation percentages in 15 training workloads.\nNo. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nğ‘£: 25 97 1 1 1 49 49 49 1 1 1 33 33 33 1\nğ‘Ÿ: 25 1 97 1 1 49 1 1 49 49 1 33 33 1 33\nğ‘: 25 1 1 97 1 1 49 1 49 1 49 33 1 33 33\nğ‘¤: 25 1 1 1 97 1 1 49 1 49 49 1 33 33 33\nunimodal bimodal trimodal\nThe workload mainly includes the proportion of each operation ( ğ‘£,ğ‘Ÿ,ğ‘,ğ‘¤ ). The tunable parameters\nwe consider include the size ratio ğ‘‡, the memory budget ğ‘€ğ‘for the writer buffer (Level 0 of the\nLSM-tree), the memory budget ğ‘€ğ‘“for the Bloom filter, and the compaction policy (leveling or\ntiering).\nThe reason we select end-to-end latency as the optimization objective is that it is one of the most\nstraightforward metrics to reflect the system performance, evaluated in many studies [ 22,23,58].\nIt is equivalent to maximizing throughput since it represents the number of operations a system\ncompletes in a period, which can be derived from the reciprocal of end-to-end latency. The I/O\ncost, which counts the I/Os triggered by an operation, is sometimes not general enough to indicate\noverall system performance, as it excludes the influence of devices and background jobs. We will\ndiscuss this further in the evaluation part.\nIn selecting the tunable parameters, we mainly consider their high impact in performance [ 22,\n23,40] and their prevalence in practical applications in LSM-based storage systems [ 30,32]. Our\nprimary aim is to explore the potential of using active learning to tune LSM-trees, and thus we\nselect these relatively representative tunable parameters for a pioneering study. We acknowledge\nthat incorporating the tuning parameters within a broader scope (e.g., sub-compaction, SST file size,\nbackground threads, cache policies) could further optimize the LSM-trees. However, adding more\nparameters also increases the complexity of training and prediction, which may not necessarily\nenhance overall performance. For example, while adjusting level-based compaction policies [ 58] to\nassign varying numbers of runs ( ğ¾ğ‘–) to each level could improve system performance, the expansion\nof the parameter space could lead to prohibitive sampling costs. Moreover, many parameters are\nnot universally applicable across all LSM systems (e.g., sub-compaction is specific to RocksDB).\nTherefore, we limit our parameter tuning to showcase the idea of our proposed model. Nonetheless,\nwe still provide an evaluation on a broader range of parameters to demonstrate how to extend our\nmodel in the evaluation section.\n3Camal Overview\nThis section introduces Camal , outlined in Algorithm 1, where an ML model estimates LSM-tree\ncosts for given workloads and identifies desired parameter settings for enhanced performance. As\nshown in Figure 3, in its training phase, Camal first decouples the sampling space and identifies\nthe theoretical optimum using a complexity-based model. Following this, it integrates an ML model\nto facilitate an active learning cycle, where the model is continuously iterated to select subsequent\nsamples. To address data growth, we introduce an extrapolation strategy that extends the desired\nparameters to testing scenarios with larger data sizes without the need for retraining. Both of\nthese methods are designed to reduce training costs. Additionally, to meet practical demands, we\napply the extrapolation strategy to enhance Camal for dynamic environments, which also includes\nequipping LSM-trees with the ability to dynamically adapt to changing parameters.\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:7\nAlgorithm 1: Camal overview\nInput: Workloads to be tuned W\nOutput: Tuned configurations C\n1Collect training samples Sby decoupled active learning (see Section 4 and Algorithm 2).\n2Train ML cost models with S(see Section 7).\n3Get tuned configurations Cfrom ML models.\n4ifWis scaled-up then\n5 GetCwith extrapolation strategy (see Section 5).\n6ifWis dynamic then\n7 GetCwith dynamic mode (see Section 6).\n8returnC\nML model (Sec 7)\nDB instanceTrainingCost-based\nSamplingActive learning cycle\nDecoupling sampling space\n+ Pinpointing theoretical optimal (Sec 4)Complexity-based \nmodelTraining phase\nBuffer\nL1\nL2Training LSM-tree\nBuffer\nL1\nL2Testing LSM-treeExtrapolation phase (Sec 5)\nExtrapolating\nTesting\nData size \ngrowing\nBuffer\nL1\nL2Initial LSM-tree\nBuffer\nL1Transitioned LSM-treeDynamic mode (Sec 6)\nOptimizing by \nnew workloads\nL2\nWorkloads\ntransition\n, , , , , , \nParameters\ntransition+ Data size \ngrowing, , , \n, , , \nâ€¦\n, , , \nFor each \ntraining \nworkload\nFig. 3. The overview of Camal : in its training phase, Camal first decouples the sampling space and identifies\nthe theoretical optimum using a complexity-based model. Following this, it integrates an ML model to\nfacilitate an active learning cycle, where the model is continuously iterated to select subsequent samples. To\naddress data growth, we introduce an extrapolation strategy that extends the training optimals for larger\ndata sizes without retraining. Both of these methods are designed to reduce training costs. Additionally, to\nmeet practical demands, we apply the extrapolation strategy to enhance Camal for dynamic environments,\nwhich also includes equipping LSM-trees with the ability to dynamically adapt to changing parameters.\nCamal considers training with various workloads as shown in Table 1, in line with settings in\nEndure [ 40]. For ease of discussion, let us focus on one workload ğ‘Šwith a sampling budget â„, as\nextending to multiple workloads is straightforward â€“ simply training one workload after another.\nAs shown in Figure 3, the main workflow of Camal follows an active-learning approach, which\nconsists of multiple rounds of sample generation and model training using existing samples. At\na high level, Camal employs a novel technique called decoupled active learning , which enables\nthe tuning of individual parameters in separate rounds. The parameter is fixed to its tuned value\nfound based on an intermediate model trained during active learning. This technique addresses a\nlimitation commonly found in typical active learning, where all parameters are sampled together in\na correlated manner, resulting in an extensive sampling space. Assuming the sampling space size\nfor each parameter is ğ‘›ğ‘–, the total sampling space required to explore all parameter correlations\nisÃğ‘›ğ‘–. In contrast, decoupled sampling allows us to tune each parameter separately in a round,\nreducing the sample space toÃğ‘›ğ‘–. Additionally, by using decoupled sampling, the model gains\nearly exposure to desired settings, leading to higher-quality samples in subsequent rounds. One\nmay wonder why the desired setting for a parameter can be determined by an intermediate model\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:8 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\nin AL. Addressing this query leads us to another crucial technique in Camal , which involves\nemploying a complexity-based cost model to assess the independence between desired settings for\neach parameter, and is particularly geared to the LSM-tree applications.\nIn a nutshell, each round in Camal comprises the following steps. Initially, a parameter or a set\nof parameters to be adjusted in the current round is chosen. Next, an analysis is performed on the\nselected parameter configuration using a complexity-based cost model, and we narrow down the\nsampling range to the neighborhood of the selected value. Subsequently, the parameter is sampled\nwithin this narrowed range while keeping the previously selected parameters intact, and other\nparameters are set to their default values. Then, the selected parameters, denoted by ğ‘‹, are used\nto construct the LSM-tree instance and record the performance (denoted by ğ‘Œ) of running the\nworkloadğ‘Što form the sample (ğ‘Š,ğ‘‹,ğ‘Œ). Afterwards, a machine learning model (will be discussed\nin Section 7) is trained using all existing samples. Finally, the trained model is used to tune the\ncurrent parameter by selecting the one that leads to the lowest inference value for the model. The\nselected parameter value is then fixed for subsequent rounds to mitigate potential errors arising\nfrom complexity-based analysis.\nWhile decoupled active learning effectively reduces the sampling space for moderate data sizes,\nthe training cost can exponentially increase with larger data sizes. To address this challenge, we\ndesign an extrapolation strategy that scales the selected settings from a smaller to a larger dataset\nwithout retraining. Specifically, if ğ‘‡â€²,ğ‘€â€²\nğ‘“, andğ‘€â€²\nğ‘are the selected parameters for a database with\nğ‘â€²entries and an ğ‘€â€²memory budget, we prove that when the data size grows to ğ‘˜ğ‘â€²and the\nmemory budget to ğ‘˜ğ‘€â€², the desired parameters become ğ‘‡â€²,ğ‘˜ğ‘€â€²\nğ‘“, andğ‘˜ğ‘€â€²\nğ‘. Though the factor ğ‘˜\nmay have practical limitations, this strategy still substantially reduces training costs, cutting down\ntraining time by approximately an order of magnitude.\nIn practical applications, another challenge is managing dynamic workloads where the desired\nsettings for an initial workload may not remain effective for subsequent ones. To illustrate this\nscenario, we have a current workload represented by (ğ‘£ğ‘–,ğ‘Ÿğ‘–,ğ‘ğ‘–,ğ‘¤ğ‘–)and an expected next workload\n(ğ‘£ğ‘–+1,ğ‘Ÿğ‘–+1,ğ‘ğ‘–+1,ğ‘¤ğ‘–+1). In the interval between these two workloads, there are enough updates to\ntrigger sufficient compactions, allowing for a change in parameters. Our method involves initially\nsetting the desired parameters ğ‘‡â€²,ğ‘€â€²\nğ‘“, andğ‘€â€²\nğ‘for workload ğ‘–. We then use the extrapolation strategy\nto estimate the desired parameters ğ‘‡â€²â€²,ğ‘€â€²â€²\nğ‘“, andğ‘€â€²â€²\nğ‘for workload ğ‘–+1. In the interval between\nthese workloads, we incrementally adjust the size ratio during each compaction, and change the\nbit per key for Bloom filters when new runs are formed. This approach enables the settings to\ngradually shift, aligning with the evolving workloads and ensuring continued optimization.\n4 Decoupled Active Learning\nDecoupled active learning aims to decouple each individual parameter from the complex I/O model\nto ease the sampling of a single parameter at each round. In particular, given a configuration space\nÎ›that consists of all tunable parameters, we select a parameter ğœ†ğ‘–âˆˆÎ›to analyze first, if there\nexists an optimal solution ğœ†âˆ—\nğ‘–forğœ†ğ‘–, such that\nï£±ï£´ï£´ ï£²\nï£´ï£´ï£³ğœ•ğ‘“\nğœ•ğœ†ğ‘–\f\f\f\nğœ†ğ‘–=ğœ†âˆ—\nğ‘–=0\nğœ†âˆ—\nğ‘–âŠ¥Î›\\ğœ†ğ‘–(3)\nwhereğ‘“is the cost function, and âŠ¥denotes an orthogonal relationship. Essentially, the first\nequation implies that ğœ†âˆ—\nğ‘–is an optimal solution to ğœ†ğ‘–for the cost function ğ‘“(Â·)1, whereas the second\nexpression indicates that the optimal solution ğœ†âˆ—\nğ‘–does not depend on other tunable parameters in\n1The partial derivative at the optimal value is zero.\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:9\nAlgorithm 2: Decoupled active learning\nInput: Set of training workloads W={ğ‘¤ğ‘–}\n1Initialize the set of training samples Sâ†{}\n2foreachğ‘¤ğ‘–âˆˆW do\n3 Get theoretical optimal size ratio ğ‘‡âˆ—\n4 Get neighbour range Taroundğ‘‡âˆ—\n5 foreachğ‘‡ğ‘–âˆˆTdo\n6 Execute database instance with ğ‘‡ğ‘–\n7Sâ†Sâˆ§(ğ‘¤ğ‘–,ğ‘¥ğ‘–,ğ‘¦ğ‘–)\n8 /*ğ‘¥ğ‘–is LSM-tree parameters*/\n9 /*ğ‘¦ğ‘–is system latency */\n10 Train the ML model using S\n11 Get practical desired ğ‘‡â€²by the ML model\n12 Get theoretical optimal buffer ğ‘€âˆ—\nğ‘with fixedğ‘‡â€²\n13 Get neighbour range Maroundğ‘€âˆ—\nğ‘\n14 foreachğ‘€ğ‘–âˆˆM do\n15 Execute database instance with ğ‘€ğ‘–\n16Sâ†Sâˆ§(ğ‘¤ğ‘–,ğ‘¥ğ‘–,ğ‘¦ğ‘–)\n17returnS\nthe configuration space. As such, we can safely determine the optimal setting for parameter ğœ†âˆ—\nğ‘–first\n(inCamal , we use machine learning to help calibrate the setting, see Section 7) and then shrink\nthe configuration space to Î›âˆ—=Î›\\ğœ†âˆ—by eliminating the dimension of ğœ†ğ‘–(which has been fixed as\nğœ†âˆ—\nğ‘–). The process then repeats in the newly reduced dimension Î›âˆ—.\nThe process for setting each parameter involves several key steps. Initially, we formulate a\ncost function related to the parameter. Next, we proceed to solve the derivative of this function,\nobtaining a theoretical optimum. This theoretical optimum establishes an effective sampling range\ncentered around the optimum to encompass the practical desired solution close to this range. The\nunderlying theoretical basis for this approach is based on the following lemma:\nLemma 4.1. Given the prevalent cost model [ 22,23] for leveling, the process of configuration opti-\nmization can be decoupled into two distinct stages: firstly, determining the optimal value of ğ‘‡âˆ—, and\nsecondly, allocating memory between ğ‘€ğ‘andğ‘€ğ‘“. This decoupling ensures the attainment of a globally\noptimal configuration combination.\nProof. According the terms in Figure 2, the I/O cost for a leveling LSM-tree can be represented\nas:\nğ‘“ğ‘™(ğ‘‡)=ğ‘£ğ‘’âˆ’ğ‘€ğ‘“\nğ‘+ğ‘Ÿ(ğ‘’âˆ’ğ‘€ğ‘“\nğ‘+1)+ğ‘(ğ¿+ğ‘ \nğµ)+ğ‘¤ğ¿Â·ğ‘‡\nğµ\nIts derivative can be formulated as:\nğœ•ğ‘“ğ‘™\nğœ•ğ‘‡=ğ¿\nğµğ‘‡log(ğ‘‡)Â·(ğ‘¤ğ‘‡(logğ‘‡âˆ’1)âˆ’ğ‘ğµ) (4)\nThe theoretical optimal ğ‘‡âˆ—(the optimal value of ğ‘‡) can be obtained by setting Equation 4 to zero\naccording to the Derivative Test, giving us\nğ‘¤ğ‘‡âˆ—(logğ‘‡âˆ—âˆ’1)âˆ’ğ‘ğµ=0 (5)\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:10 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\nClearly, from Equation 5 we conclude that ğ‘‡âˆ—is independent of ğ‘€ğ‘“andğ‘€ğ‘. So, we can determine\nğ‘‡âˆ—first.\nWhen optimizing for ğ‘€ğ‘“andğ‘€ğ‘, we have:\nğœ•ğ‘“ğ‘™(ğ‘€ğ‘“)\nğœ•ğ‘€ğ‘“=âˆ’1\nğ‘(ğ‘£+ğ‘Ÿ)ğ‘’âˆ’ğ‘€ğ‘“\nğ‘+ğ‘ğµ+ğ‘¤ğ‘‡\nğµlogğ‘‡1\n(ğ‘€âˆ’ğ‘€ğ‘“)(6)\nEquation 6 implies that when ğ‘€ğ‘“increases, the absolute value of the first term will decrease, thereby\ndiminishing its effect on optimizing point lookups (whose cost is (ğ‘£+ğ‘Ÿ)ğ‘’âˆ’ğ‘€ğ‘“/ğ‘+ğ‘Ÿ). Concurrently, a\nlargerğ‘€ğ‘“will result in a reduced ğ‘€ğ‘, indicating that the second term will increase, further causing\nthe cost to escalate at an accelerating rate. To strike a balance between the two parameters, we set\nEquation 6 equal to zero, which will allow us to derive the theoretically optimal values for ğ‘€ğ‘“and\nğ‘€ğ‘. This completes the proof of the lemma. â–¡\nUpon segregating the parameters across various rounds, during the sampling process, we gather\nsamples from a contiguous vicinity centered around the theoretical optimal values, based on the\nspecified sample count. For ğ‘‡, the sampling interval is an integer. For ğ‘€ğ‘“, the interval is measured\nin bits per key (BPK), determined by the formulağ‘€âˆ—\nğ‘“\nğ‘.\nIn summary, decoupled active learning is outlined as Algorithm 2. It stands out compared to\nplain active learning primarily for two reasons: firstly, it identifies theoretical optimal parameters,\nwhich are often close to the practical desired parameters. Secondly, it reduces the dimensions of\nthe sampling space, significantly decreasing the number of active learning cycles required to find\nthe practical desired solutions.\nExtension to Tiered LSM-tree. The decoupled sampling approach remains largely pertinent\nfor the tiering compaction policy. First, we can differentiate with respect to ğ‘‡using the tiering\ncost model. This differentiation closely resembles Equation 4, augmented with a specific term\nrepresenting the point read cost:\nğœ•ğ‘“ğ‘¡\nğœ•ğ‘‡=(ğ‘£+ğ‘Ÿ)ğ‘’âˆ’ğ‘€ğ‘“\nğ‘+ğ‘ğ‘ \nğµ+ğ¿(ğ‘ğµğ‘‡(logğ‘‡âˆ’1)âˆ’ğ‘¤)\nğµğ‘‡logğ‘‡(7)\nBased on prior research [ 40], this term is quantitatively less significant compared to I/Os of range\nlookups and post-optimization writes. Therefore, in practice, the desired ğ‘‡âˆ—exhibits only mild\nfluctuations with changes in ğ‘€ğ‘“. Furthermore, employing machine learning for subsequent sampling\neffectively addresses these variations, as the sampling phase refines and corrects inaccuracies.\n5 Extrapolation without Retraining\nEmploying ML models for estimating the cost of an LSM-tree introduces a related issue concerning\ntheir extrapolation capabilities. This means that when testing configurations deviate from those\nused during training, it becomes important to determine if the stale model can still effectively\noptimize the LSM-tree. For instance, if the stale model is trained on a database with ğ‘=106, it\nis unclear whether it can be applied to a database with ğ‘=107. While retraining the model is\na possibility, we aim to identify an incrementally updated solution based on the existing model,\nwhich does not require a costly retraining.\nLemma 5.1. Given the optimal ğ‘‡â€²andğ‘€â€²\nğ‘“under the memory budget ğ‘€â€²and the number of entries\nğ‘â€², we have the new optimal ğ‘‡â€²â€²andğ‘€â€²â€²\nğ‘“under the memory budget ğ‘€â€²â€²=ğ‘˜ğ‘€â€²and the number of\nentriesğ‘â€²â€²=ğ‘˜ğ‘â€²as:\nğ‘‡â€²â€²=ğ‘‡â€², ğ‘€â€²â€²\nğ‘“=ğ‘˜ğ‘€â€²\nğ‘“ (8)\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:11\nProof. According to [ 42], in leveling policy, the write amplification of each level is ğ‘‡, signaling\nthat on average the key-value entry of the update will take part in ğ‘‡compactions in each level. So\nthe average complexity of CPU to compact an entry is expected to be ğ‘‡Â·ğ¿. Further separating the\nI/O cost model in Figure 2 by writes and reads, on average the write query incurs a total overhead of\nğ¿ğ‘‡\nğµÂ·(ğ¼ğ‘¤+ğ¼ğ‘Ÿ)+ğ¶ğ‘¤Â·ğ‘‡, whereğ¼ğ‘¤andğ¼ğ‘Ÿare the write and read I/O costs while ğ¶ğ‘¤is the CPU cost, such\nas merge sorting and space allocation. In the same way, let ğ¶ğ‘Ÿbe the cost of probing the metadata\nof a sorted run in the main memory, then the total cost of an zero-result point lookup is expected\nto beğ‘’âˆ’ğ‘€ğ‘“\nğ‘Â·ğ¼ğ‘Ÿ+ğ¶ğ‘ŸÂ·ğ¿, and the cost for a non-zero-result point lookup is (ğ‘’âˆ’ğ‘€ğ‘“\nğ‘+1)Â·ğ¼ğ‘Ÿ+ğ¶ğ‘ŸÂ·ğ¿. For\nrange lookups, the main CPU overhead is also to retrieve the information in the metadata, which\nleads to a total overhead (ğ¿+ğ‘ \nğµ)Â·ğ¼ğ‘Ÿ+ğ¶ğ‘Â·ğ¿.\nIn summary, when the number of entries is ğ‘â€²and the memory budget is ğ‘€â€², the total overhead\nper operation of leveling policy is\nğ‘”ğ‘™=ğ¼ğ‘Ÿğ‘£ğ‘’âˆ’ğ‘€ğ‘“\nğ‘â€²+ğ¼ğ‘Ÿğ‘Ÿ(ğ‘’âˆ’ğ‘€ğ‘“\nğ‘â€²+1)+2ğ¶ğ‘Ÿğ¿\n+ğ¼ğ‘Ÿğ‘(ğ¿+ğ‘ \nğµ)+ğ¶ğ‘ğ¿+(ğ¼ğ‘¤+ğ¼ğ‘Ÿ)ğ¿ğ‘‡\nğµ+ğ¶ğ‘¤ğ‘‡ğ¿(9)\nIf the overhead is minimized when ğ‘‡=ğ‘‡â€²andğ‘€ğ‘“=ğ‘€â€²\nğ‘“, then we have the equations:\nlogğ‘â€²ğ¸\nğ‘€â€²âˆ’ğ‘€â€²\nğ‘“2ğ¶ğ‘Ÿ+ğ¼ğ‘Ÿğ‘+ğ¶ğ‘+ğ‘‡â€²(ğ‘‡â€²âˆ’logğ‘‡â€²)(ğ¼ğ‘¤+ğ¼ğ‘Ÿ+ğ¶ğ‘¤)\nğ‘‡â€²log2ğ‘‡â€²=0\nâˆ’ğ¼ğ‘Ÿ\nğ‘â€²(ğ‘£+ğ‘Ÿ)ğ‘’âˆ’ğ‘€â€²\nğ‘“\nğ‘â€²+2ğ¶ğ‘Ÿ+ğ¼ğ‘Ÿğ‘+ğ¶ğ‘+(ğ¼ğ‘¤+ğ¼ğ‘Ÿ)ğ‘‡â€²\nğµ+ğ¶ğ‘¤ğ‘‡\nlogğ‘‡(ğ‘€â€²âˆ’ğ‘€â€²\nğ‘“)=0(10)\nWhenğ‘â€²â€²=ğ‘˜ğ‘â€², to ensure that the equation still holds, we can make ğ‘‡â€=ğ‘‡â€²andğ‘€â€²\nğ‘“=ğ‘˜ğ‘€â€²\nğ‘“, then\nthe new version of Equations 10 is transformed to be\nlogğ‘˜ğ‘â€²ğ¸\nğ‘˜ğ‘€â€²âˆ’ğ‘˜ğ‘€â€²\nğ‘“2ğ¶ğ‘Ÿ+ğ¼ğ‘Ÿğ‘+ğ¶ğ‘+ğ‘‡â€²(ğ‘‡â€²âˆ’logğ‘‡â€²)(ğ¼ğ‘¤+ğ¼ğ‘Ÿ+ğ¶ğ‘¤)\nğ‘‡â€²log2ğ‘‡â€²=0\nâˆ’ğ¼ğ‘Ÿ\nğ‘˜ğ‘â€²(ğ‘£+ğ‘Ÿ)ğ‘’âˆ’ğ‘˜ğ‘€â€²\nğ‘“\nğ‘˜ğ‘â€²+2ğ¶ğ‘Ÿ+ğ¼ğ‘Ÿğ‘+ğ¶ğ‘+(ğ¼ğ‘¤+ğ¼ğ‘Ÿ)ğ‘‡â€²\nğµ+ğ¶ğ‘¤ğ‘‡\nlogğ‘‡(ğ‘˜ğ‘€â€²âˆ’ğ‘˜ğ‘€â€²\nğ‘“)=0,\nwhich means ğ‘‡â€²â€²=ğ‘‡â€²andğ‘€â€²â€²\nğ‘“=ğ‘˜ğ‘€â€²\nğ‘“are the new optimum of the new configuration. This\ncompletes the proof. â–¡\nBased on Lemma 5.1, the extrapolation strategies of ğ‘can be inducted as: First, we train the\nmodel with configuration of ğ‘â€²andğ‘€â€². Then we get a stale optimal ğ‘‡â€²andğ‘€â€²\nğ‘“under given\nworkloads. Then we get ğ‘‡â€²â€²andğ‘€â€²â€²\nğ‘“from Equation 9 as the new optimal when ğ‘â€²â€²=ğ‘˜ğ‘â€²and\nğ‘€â€²â€²=ğ‘˜ğ‘€â€².\n6 Dynamic System Mode\nWorkloads can be dynamic. This section discusses how to adapt Camal to workload shifts. Assume\nthat we have a current workload represented by (ğ‘£ğ‘–,ğ‘Ÿğ‘–,ğ‘ğ‘–,ğ‘¤ğ‘–)and an expected next workload\n(ğ‘£ğ‘–+1,ğ‘Ÿğ‘–+1,ğ‘ğ‘–+1,ğ‘¤ğ‘–+1). Our method initially sets the desired parameters ğ‘‡â€²,ğ‘€â€²\nğ‘“, andğ‘€â€²\nğ‘for workload\nğ‘–, and then uses the extrapolation strategy to estimate the desired parameters ğ‘‡â€²â€²,ğ‘€â€²â€²\nğ‘“, andğ‘€â€²â€²\nğ‘for\nworkloadğ‘–+1. In the interval between these workloads, we incrementally adjust the size ratio\nat each compaction, and change the bit per key for Bloom filters when creating new runs. This\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:12 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\nBuffer\nL1\nL2Compaction\nTransition T \nfrom 2 to 3\n10MB\n20MB\n40MB10MB\n20MBâœ30MB\n40MBâœ90MB \nTransition buffer \nfrom 10MB to 8MB\nCompaction\n30MBâœ24MB8MB\n90MBâœ72MB Filter\nFilter\n24MB8MB\n72MB\nTransition filters \nfrom 8MB to 10MB\n8BPK\n6BPK\n8BPK10BPKCreate new filters          \nduring              \ncompaction\nFig. 4. Running example of the dynamic system mode. We employ a lazy transition strategy to keep the\ntransition cost lower.\napproach enables a gradual shift of the settings, aligning with the evolving workloads and ensuring\ncontinued optimization.\nTo achieve the dynamicity, we propose a dynamic LSM-tree design, capable of lazy dynamically\nadjusting the size ratio and memory allocation. Specifically, regarding the size ratio and memory\nbuffer, their impact manifests chiefly through their ability to reshape each levelâ€™s capacity. This\nchanging process is guided by a common principle:\nâ€¢Firstly, in cases where the target size for a level is less than its present size, we employ a\nstrategy of compacting data from shallower levels into deeper ones to align with the desired\nlevel size. This is done during standard compactions. As a result, this often means that when\ncompaction is triggered, some files from the current level are moved to the next level, also\nleading to more frequent compaction. For example, Figure 4 illustrates four states of the dynamic\nLSM-tree, reflecting the process of parameter changes during LSM-tree compactions in response\nto workload changes. Initially, the size ratio is 2, and the memory buffer is allocated 10MB. Level\n1 (L1) contains 20MB and has a filter of 8 BPK, while Level 2 (L2) holds 40MB, also with a filter of\n6 BPK. In the second state, we see an adjustment in the size ratio, with L1 expanded to 30MB and\nL2 to 90MB, suggesting compactions have occurred allowing for increased data volume per level.\nâ€¢Secondly, if the desired size for a level is greater than its current size, possibly due to an increase\ndesired size ratio or write buffer, the level is gradually expanded using data from shallower levels\nduring regular compaction processes. This typically results in less frequent compaction since\neach level has the capacity to hold more files. In the third scenario depicted in Figure 4, the buffer\nsize is reduced to 8MB, with L1 now at 24MB and L2 at 72MB. The remaining files are compacted\ninto deeper levels, adapting the structure to better suit the current workload.\nIn practice, the actual size of each level may deviate from the ideal configuration dictated by\nthe predefined size ratio and write buffer due to the lazy transition strategy. However, when\nworkloads evolve gradually, the expected optimal parameters transition smoothly as well. This\nincremental adaptation allows the actual parameters to closely align with the desired value over\ntime. It can be validated in our experiments that the dynamic LSM-tree is engineered to maintain\nbetter optimization performance than plain LSM-trees. For altering the memory allocated to the\nbloom filter, we allocate the new bits-per-key calculated from ğ‘€â€²\nğ‘“to each file as it is created. In the\nfinal stage of Figure 4, the adjustment lets the filter size be increased to 10MB. This happens when\neach new Bloom filter is created during compaction. For instance, the filter at L1 is enhanced to 10\nBPK. This adjustment signifies a refinement of the filters, optimizing them for more efficient key\nlookups in the context of the changing data environment.\nFor automatic optimization in dynamic scenarios, we employ threshold-based detection methods.\nSpecifically, to recognize workload changes, the system monitors the percentages of each operation\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:13\nwithin a period ofğ‘operations, where ğ‘is a hyper-parameter. Reconfiguration does not occur in\nevery period, but only when the percentage for any operation type varies by a predefined threshold\nğœcompared to its percentage at the time the last reconfiguration was triggered. The system then\nhandles the reconfiguration according to the approach elaborated above. Additionally, we evaluate\nthe sensitivity of ğ‘andğœin Section 8.\n7 Embedded Machine Learning Models\nThis section examines the three most prevalent ML models, evaluating their pros and cons for\nintegration into Camal . It is nevertheless to note that other ML models can potentially be embedded\nintoCamal , and we limit the discussion scope to simple and representative models for ease of\ndiscussions.\nPolynomial Regression (Poly) [ 35]uses basis functions to capture nonlinear relationships\nbetween variables, replacing linear terms in linear regression. We treat each term from the cost\nmodels in Figure 2 as basis functions, and include a constant term for each operation type to account\nfor CPU time consumption. The regression model for the cost function can be formulated as:\nğ‘¦ğ‘ğ‘œğ‘ ğ‘¡=âˆ‘ï¸\nğ›½ğ‘–ğ‘¥ğ‘– (11)\nHere,ğ›½ğ‘–represents the coefficients to be learned, and ğ‘¥ğ‘–denotes the basis functions derived from\ntheoretical models.\nTree Ensembles (Trees) [ 19,57] are a type of ML model that combines multiple decision trees\nto enhance accuracy and robustness. The model can mitigate the impact of individual tree biases\nand errors, resulting in more accurate and stable predictions. In this paper, we primarily focus on\ngradient-boosted trees due to its widely adoption [ 29,81].Tree ensembles offer the advantage of\nbypassing manual basis function design and directly incorporating cost function elements. They\ndetect feature relationships automatically and have strong fitting capability. However, they are\nprone to overfitting and outlier sensitivity due to uncertainty, and their extrapolation ability is\nweaker than polynomial regression.\nDuring training and inference, we input the influential factors as independent features into the\ntree ensembles, including ğ‘,ğ‘‡,ğ‘€ğ‘,ğ‘€ğ‘,ğ‘£,ğ‘Ÿ,ğ‘¤, andğ‘. The average latency is used as the label, and\nthe ensemble trees automatically capture the relationships between these features.\nNeural Networks. We also explore more complex and advanced models, such as neural networks\n(NN), which consist of interconnected nodes organized into layers for data processing and analysis.\nHowever, these complex models typically demand a larger amount of training data [ 37,44,85],\nresulting in increased sampling time. To verify the expectation, while considering previous expe-\nriences [ 57,74] and conducting experimental model selection, we evaluate a standard NN model\nwith four fully connected layers. This model requires three times the number of samples compared\nto the other two models in order to achieve similar optimization outcomes (see more details in\nFigure 5a in Section 8).\n8 Evaluation\nWe integrated Camal into RocksDB and conduct systematic experimental evaluations to demon-\nstrate its effectiveness.\n8.1 Experimental Setup\nHardware. Our experiments are done on a server with Intel(R) Core(TM) i9-13900K processors,\n128GB DDR5 main memory, 2TB NVMe SSD, a default page size of 4 KB, and running 64-bit Ubuntu\n20.04.4 LTS on an ext4 partition.\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:14 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\n0.5 1.5 7 14 210.60.81.01.21.41.61.82.02.2Normalized LatencyClassic\n \n \n \n \n \n \n \n CAMAL(Poly w/ Ext.)\nCAMAL(Trees w/ Ext.)\nCAMAL(NN w/ Ext.)\nCAMAL (Poly w/o Ext.)\nCAMAL (Trees w/o Ext.)\nCAMAL (NN w/o Ext.)\n \n \n Plain AL (Poly)\nPlain AL (Trees)\nPlain AL (NN)\nBayes (Poly)\nBayes (Trees)\nBayes (NN)\nPlain ML (Poly)\nPlain ML (Trees)\nPlain ML (NN)Stopping point\n(a) Sampling hours of different strategiesMethods Mean 90th\nCamal (Poly) 0.11 0.14\nCamal (Trees) 0.10 0.14\nCamal (NN) 0.19 0.22\nPlain AL (Poly) 0.19 0.23\nPlain AL (Trees) 0.18 0.24\nPlain AL (NN) 0.22 0.29\nBayes (Poly) 0.22 0.26\nBayes (Trees) 0.19 0.25\nBayes (NN) 0.25 0.28\nPlain ML (Poly) 0.18 0.24\nPlain ML (Trees) 0.19 0.25\nPlain ML (NN) 0.24 0.29\nClassic 0.13 0.18\nClassic (Cache) 0.12 0.17\nMonkey 0.14 0.20\n(b) Latency/Op (ms)\nMethods Mean\nCamal (Poly) 6.2\nCamal (Trees) 4.5\nCamal (NN) 32.8\nPlain AL (Poly) 15.2\nPlain Al (Trees) 15.5\nPlain AL (NN) 34.7\nBayes (Poly) 16.0\nBayes (Trees) 14.4\nBayes (NN) 34.2\nPlain ML (Poly) 21.3\nPlain ML (Trees) 18.6\nPlain ML (NN) 34.9\nClassic 16.2\nClassic (Cache) 13.4\nMonkey 24.3\n(c) I/Os per operation\n050100System Latency (s)\nClassic\nMonkeyCAMAL (Poly)\nCAMAL (Trees)\nzero-result point lookups non-zero-result point lookups range lookups writes01020I/Os per Query\n10e6 11e6 12e6 16e6Data size (d) Latency and I/O comparison on dynamic test workloads\nFig. 5. Classic methods relying solely on theoretical cost models necessitate no training samples but demon-\nstrate limited optimization. Conversely, other ML or AL methods that employ plain sampling strategies\nachieve high performance but entail relatively extensive sampling costs. Camal is the only complexity-analysis\ndriven ML-aided framework that achieves high performance while significantly reducing sampling costs.\nDatabase Setup. We employ RocksDB [ 30], a widely-used LSM tree-based storage system. Consis-\ntent with Endure [ 40], we evaluate the steady-state performance of databases by initializing each\ninstance with 10 million unique key-value pairs, where each pair is 1 KB in size, unless otherwise\nspecified. Every key-value entry includes a randomly selected 16-bit key, and the rest of the bits\nare filled with randomly generated values.\nWe allocate a varying number of bits per element for the Bloom filters at each level based on\nMonkey [ 22]. The total memory budget for this is set by default to 16MB. In our default experimental\nsetup, we focus on adjusting only the size ratio and the memory allocated to the write buffer and\nBloom filter as tunable parameters. Additionally, the leveling compaction policy is the only one\nwe consider in this default setting, unless explicitly stated otherwise. This implies that other\ncompaction policies and parameter adjustments are explored only in specific, later experiments.\nFollowing Endure [ 40], to obtain an accurate count of block accesses we enable direct I/Os for both\nqueries and compaction and disable the compression.\nWorkloads. Our evaluation is divided into static and dynamic modes. The queries are generated\nbased on both the 15 standard workloads detailed in Table 1 and the 24 shifting workloads outlined\nin Table 2. The shifting workloads follow the principle of progressively transitioning weights\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:15\nTable 2. Operation percentages in 24 test workloads.\nzero-result point\nlookupsnon-zero-result\npoint lookupsrange lookups writes\nğ‘£60 75 91 75 60 45 30 15 3 5 5 5 5 5 3 5 5 5 5 5 3 15 30 45\nğ‘Ÿ 5 5 3 15 30 45 60 75 91 75 60 45 30 15 3 5 5 5 5 5 3 5 5 5\nğ‘ 5 5 3 5 5 5 5 5 3 15 30 45 60 75 91 75 60 45 30 15 3 5 5 5\nğ‘¤30 15 3 5 5 5 5 5 3 5 5 5 5 5 3 15 30 45 60 75 91 75 60 45\namong different operations, enabling Camal to gradually adapt the parameters and include a\nbroad spectrum of application scenarios. In the static mode, we assess both standard and shifting\nworkloads, whereas in the dynamic mode, we focus solely on the shifting workloads. By default,\nour evaluations are conducted in static mode, unless otherwise specified. In the dynamic mode, to\nensure adequate compactions for dynamic LSMâ€™s gradual parameter adjustment, we continuously\ninsert additional entries during the intervals of workload queries.\nWe assess the latency and I/O times of a series of 500,000 queries, reporting the average perfor-\nmance across all workloads, unless specified differently. All range queries are set up with minimal\nselectivity, functioning as short-range queries that generally access between zero to two blocks per\nlevel on average.\nRegarding data distribution, we employ both uniform and Zipfian query distributions, utilizing\nYCSB [ 20] for implementation. We also modify the skew coefficient of the Zipfian distribution,\nadjusting it within a range from 0 to 0.99. This distribution is applied to both the data being ingested\nand the queries.\nCamal Setup. In our evaluation, the distinction between using (w/ Ext.) or not using (w/o Ext.) the\nextrapolation strategy is made only in the first experiment. In all other experiments, we consistently\napply the extrapolation strategy. We set the factor ğ‘˜in the extrapolation strategy to 10, meaning\nwe train with only 1/10th of the testing data size and memory budget. This choice is justified as this\nscaling factor strikes a balance between minimizing sampling costs and maximizing optimization\nperformance, a concept further evidenced in subsequent experiments. Thus, unless otherwise\nspecified, Camal is trained with the scaled-down setting and extrapolation strategy and tested\nunder full settings.\nAs for the ML models, we implement polynomial regression using the least square method [ 34]\n(Camal (Poly)), tree ensembles with XGBoost [ 19] (Camal (Trees)) and neural network with\nPyTorch [ 63] (Camal (NN)). Once training samples are ready, all models can be trained in 5 seconds\nand traversed in 200 milliseconds to search the desired parameters, with a space overhead of under\n200KB. This is negligible compared to the time saved through optimization and the total memory\nbudget.\nImplementation optimizations. We offer three applications with Camal to better integrate\nour system into a real key-value store for practical purposes. First, we incorporate block cache\nmemory allocation as an input to the ML models to optimize cache strategy. Second, we consider\ndata distribution beyond a default uniform setting - mainly Zipfian distribution [ 17,20,21,51].\nHere we discuss three choices for incorporating different levels of distribution knowledge: (a) When\nthe data distribution is unknown during runtime, we simply train the models using uniform data\nand test them with arbitrary data. (b) If the coefficient that represents the data distribution during\nruntime can be determined, we train the model using the same distribution. (c) When multiple\npotential coefficients exist for a data distribution during runtime, we integrate the coefficient\nas an input feature within the ML model. Third, we address workload uncertainty, which is the\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:16 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\nğ‘ 1ğ‘’6 2ğ‘’6 1ğ‘’7\nClassic 1 1 1\nPoly 0.84 0.82 0.83\nTrees 0.82 0.81 0.82\nğ‘€(ğ‘€ğµ) 16 32 64\nClassic 1 1 1\nPoly 0.84 0.82 0.86\nTrees 0.82 0.83 0.84\n(a)Normalized Latency\n0.2 0.4 0.6 0.80.70.80.91.0Normalized Latency\nClassic\nCAMAL (Poly)\nCAMAL (Trees) (b)Skew Coefficient\n0.5 1.0 2.0 4.0 10.0 50.00.70.80.91.0Normalized Latency\nClassic\nCAMAL (Poly)\nCAMAL (Trees) (c)ğ‘˜in extrapolation\n0.5 1 4 15 23 310.81.01.21.41.61.8 Normalized Latency\nClassic\nCAMAL(Trees w/o Ext.)\nCAMAL(Trees w/ Ext.)\nPlain AL (d)Large data sampling hours\n0 25 50 75 10005101520 System Latency(s)\n99%W\n50%W+50%R\n(e)% deletes in workload\n0.4 0.5 0.6 0.7 0.8 0.9Normalized Write Buffer0.70.80.91.0 Normalized Latency\nT=2\nT=5\nT=10 (f)Independence example\n0.0 0.2 0.4 0.6 0.8 1.0Normalized Latency of RocksDBCAMAL\n(Poly)\nTierCAMAL\n(Poly)\nLevelCAMAL\n(Trees)\nTierCAMAL\n(Trees)\nLevel\n+T+Mf&Mb+Mc Tuned Latency (g)Parameters Breakdown\n100k 50k 10k 5k 1kp0.70.80.91.0 Normalized Latency (vs. RocksDB)\n(Latency)\np(Latency)(I/O)\np(I/O)\n0.81.21.6\nAverage Transition I/Os\n30% 20% 10% 5% 1%\n (h)Sensitivity of ğœandğ‘\nFig. 6. The robustness of Camal .\ninconsistency between observed and expected operation proportions, a challenge initially addressed\nby Endure [ 40]. We adopt the hyperparameter ğœŒ, representing the subjective uncertainty region\nanticipated in testing workloads defined by KL-divergence distance [ 40]. Our solution is statistically\nbased â€“ while maintaining the training process, during testing, we randomly sample several\nworkloads within a region of size ğœŒand identify settings with the lowest average latency across\nthese workloads as the desired solution.\nBaselines. We compare the performance of Camal against five LSM-tree tuning techniques: (1)\nwell-tuned RocksDB with Monkey [ 22], (2) classic tuning implemented in Endure [ 40], (3) plain\nML methods that ignore sampling improvement techniques, (4) plain active learning without\nconsidering theoretical-driven decoupled sampling, and (5) Bayesian Optimization as sampling,\nwhich has been adopted in tuning relational databases [ 28,76]. The first two techniques represent\nindustry and research practices, while the latter three serve as baselines to show the improvement of\nCamal over classic ML sampling techniques. Our evaluation of well-tuned RocksDB incorporates\nthe memory allocation strategy for the bloom filter from Monkey [ 22] and minor fixes from\nSpooky [ 25]. We also include a set of reasonable parameters based on experience and experimental\nsettings, such as using leveling compaction with a size ratio of 10, ğ‘€ğ‘“set to 10ğ‘bits (i.e., 10 bits\nper key), and the remaining budget allocated to ğ‘€ğ‘. For classic tuning, we apply the nominal tuning\napproach used in Endure [ 40], which minimizes the classic I/O cost model using the Sequential Least\nSquares Programming optimizer [ 79]. For plain ML methods, we employ grid search to partition\nthe sampling space according to the sampling budget. As for plain active learning, we choose a set\nof samples per round similar to Camal . We also employ a prevalent implementation of Bayesian\nOptimization with Gaussian processes [61].\n8.2 Overall System Performance\nDecoupled active learning in Camal significantly reduces sampling costs. As illustrated in\nFigure 5a, even without the extrapolation strategy, our sampling process using Camal with polyno-\nmial regression and tree ensembles can be finished in just 7 hours. In contrast, alternative methods\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:17\ntypically require over 20 hours to reach a similar level of performance. The faster identification of\nthe desired solution by Camal is attributed to its unique combination of complexity-analysis driven\nsampling and ML-aided sampling. Plain ML methods often fail to locate a suitable solution within\na limited sampling budget, as their grid search approach divides the sampling space uniformly.\nBayesian optimization, while more efficient than grid search, still falls short in some aspects. It\nbetter approximates desired settings by using prior information within the current workload, but it\nrequires additional iterations for exploration. This is because it tends to explore each workload in an\ninstance-optimized LSM-tree independently, without utilizing information from other workloads.\nAdditionally, its random initialization for each training workload leads to more exploration itera-\ntions to find the desired solution. Plain active learning, despite its ability to leverage information\nacross different workloads, is only marginally more effective than Bayesian optimization due to its\nrandom initialization approach. This limitation prevents it from outperforming the more integrated\nand efficient approach offered by Camal with decoupled active learning.\nThe extrapolation strategy in Camal can greatly reduce sampling costs when used with\nappropriate settings. Figure 5a demonstrates that, with extrapolation, Camal can save around\n80% of sampling time (1.5 hour vs. 7 hours). The time includes generating three samples for each\ngroup of parameters in each training workload, which means the average time to generate a sample\nis about 40 seconds. Note that the model training time is negligible compared to the sampling\ntime. However, there are important considerations for achieving this efficiency: (1) Scaling Factor\nRange: As Figure 6c indicates, while optimization performance remains relatively stable up to a\nscaling factor of 10, it drops significantly when the factor exceeds about 50. This decline is due\nto two main reasons: firstly, as the training database size diminishes, the uncertainty in system\nlatency becomes more pronounced. Secondly, other hardware factors, such as garbage collection (as\ndescribed in Spooky [ 25]), also affect system latency. (2) Performance Gap with Full-Size Training:\nWhen using extrapolation, Camal incurs around 5% more system latency compared to training\nwith a full-size database instance. Adding more scaled-down training data does not bridge this gap.\nThis discrepancy is attributed to both the increased uncertainty and the aforementioned hardware\nfactors. In conclusion, while the extrapolation strategy can significantly optimize performance, its\neffectiveness is subject to other engineering factors that must be carefully managed.\nThe choice of embedded ML models significantly influences Camal â€™s performance. As\nshown in Figure 5d, when Camal incorporates polynomial regression and tree ensembles, we\nobserve comparable optimization levels after 7 hours of sampling. However, using a neural network\nas the base model leads to subpar performance at the same stopping point. Although employing\nextrapolation reduces sampling costs for neural networks, their performance still lags significantly\nbehind the other two ML models. Similar patterns are seen when neural networks are combined\nwith other sampling methods like Bayesian optimization and grid search. This trend could be\nattributed to the fact that deep neural networks typically require more extensive training data to\ngeneralize effectively and avoid overfitting [ 37,85]. This necessity for extensive data underpins\nour preference for polynomial regression and tree ensembles as primary ML models in Camal .\nAs discussed in Section 7, polynomial regression and tree ensembles have a moderate complexity\nlevel compared to neural networks, making them more suitable for scenarios with fewer training\nsamples.\nCamal consistently achieves better end-to-end performance than traditional tuning tech-\nniques. Figure 5a shows that Camal with polynomial regression and tree ensembles consistently\nachieves lower latency (per operation) compared to classic tuning methods across various work-\nloads, with an average improvement of 16% âˆ¼18% (0.10âˆ¼0.11 ms vs. 0.13 ms). This can be attributed\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:18 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\ntoCamal â€™s ability to capture the true relationship between parameters and the actual system con-\nditions. In contrast, classic methods may provide biased estimates as they are purely based on I/O\ncost and neglect CPU time. As a result, it may still lead to high I/O cost, as shown in the range\nlookups-heavy workloads in Figure 5d. Further investigation reveals that theoretical models do not\nfit well with the seek andnext operations in RocksDB, leading to unexpected performance. Monkey\noffers a universal solution for all workloads, leading to less variance but higher latency than classic\ntuning methods. Compared to Monkey, Camal significantly reduces latency by up to 8x (e.g., for\nsome write-heavy workloads in Figure 5d), thanks to its ability to adapt to the unique operation\nratios in each workload.\nAlthough Camal â€™s learning objective does not specifically target I/O cost, it generally leads to\na significant reduction in I/O cost compared to other methods in most cases. When compared to\nclassic tuning, Camal reduces I/O cost by up to 72% (4.5 vs. 16.2). This happens because classic\ntuning, despite aiming to minimize I/O cost, may not always achieve lower I/O cost due to inaccurate\nmodels. In particular, optimizing the theoretical complexity-based cost model results in sub-optimal\nsolutions for system I/Os, such as the range lookup-heavy and write-heavy workloads in Figure 5d.\nThe rationale behind this is that the real database does not perform exactly as predicted by the\ntheoretical model. Monkey provides a default setting that is relatively stable for all workloads,\nresulting in higher I/Os compared to other methods, especially in write workloads. Since Camal\nhas lower end-to-end latency, it implies that the sum of the I/O time and the CPU time is lower,\nwhich typically implies fewer I/Os. Our insight is that low latency is a sufficient condition for a low\nI/O cost, but low I/O cost does not necessarily implies a low latency due to potentially higher CPU\ncosts. This explains the rationale for using latency as a learning objective.\nCamal exhibits superior performance on the dynamic LSM-tree. The trends illustrated in\nFigure 5d are derived from the dynamic LSM-tree under progressively changing workloads, where\nCamal distinctly outperforms competing methods. To further clarify the setup in Figure 5d, each\npoint of x-axis represents a workload in Table 2, in which the ratio of operations shifts gradually.\nInternal the workloads we adopt the detection methods and reconfigure the dynamic LSM-tree\nwhich has been elaborated in Section 6. As the workloads include writes, the data size will gradually\nincrease. So we also employ the extrapolation strategy elaborated in Section 5. We report the system\nlatency and average I/Os of each workload. The trajectory of Camal demonstrates that optimization\nperformance progressively adapts as workloads evolve. This points to the dynamic LSM-treeâ€™s\nability to adjust parameters, thereby surpassing the performance of a statically configured LSM-tree.\nThe primary reason for this is that while Monkey and RocksDB can deliver a generally effective\nconfiguration for a range of workloadsâ€”maintaining stable performanceâ€”they lack the capability to\nfine-tune settings for specific workload scenarios. In contrast, the dynamic LSM-tree, guided by ML,\ncan incrementally modify parameters to align closely with the theoretically optimal configuration.\n8.3 Parameter Sensitivity Study\nThe impact of each tunable parameter. As shown in Figure 6g, we break down the impact of\neach parameter on performance compared with well-tuned RocksDB. Initially, we configure ğ‘€ğ‘“as\n10ğ‘, setğ‘€ğ‘toğ‘€âˆ’ğ‘€ğ‘“, andğ‘€ğ‘to 0, while focusing on tuning ğ‘‡. The label \" + ğ‘‡\" indicates tuning\nonlyğ‘‡; \" +ğ‘€ğ‘“&ğ‘€ğ‘\" involves balancing between ğ‘€ğ‘“andğ‘€ğ‘; and \" +ğ‘€ğ‘\" additionally includes\ntuningğ‘€ğ‘. The results reveal that adjusting ğ‘‡leads to significant performance improvements\n(reducing normalized latency from 1 to 0.88/0.86 in two models with leveling), while tuning ğ‘€ğ‘\nalso provides substantial benefits. This suggests that our feature optimization approach in Camal\nis in a reasonable order of importance.\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:19\nWhen further examining the optimization effects based on the compaction policy, as shown in\nFigure 6g, we find that both leveling and tiering compaction policies achieve comparable effective-\nness after tuning with Camal . However, combining these policies can result in better optimization.\nThis finding highlights the potential of selecting an appropriate compaction policy to maximize\noptimization outcomes. Therefore, in Section 8.4, we further discuss how to extend our model\nto incorporate the number of runs in an LSM-tree level as another tunable parameter for system\noptimization.\nWe also experimentally verify the parametersâ€™ independence. We vary ğ‘‡under specified work-\nloads and then explore the sampling space for memory allocation to the write buffer and Bloom filter.\nAs illustrated in Figure 6f, with varying size ratios, the tuned latency consistently approaches the\npractical desired when the write buffer occupies 60% to 70% of the memory budget. This supports\nthe approach of tuning ğ‘‡first, followed by memory allocation.\nThe impact of data scales. To demonstrate the scalability of Camal , we explore settings with\nlarger data volumes and increased memory budgets. Specifically, we expand the number of entries\nto 50 million and the memory budget to 80MB proportionally. As illustrated in Figure 6d, Camal\nmaintains approximately the same optimization performance as seen with the 10MB setting, both\nat 4 and 23 hours, with and without extrapolation. In contrast, plain active learning (AL) still\nunderperforms relative to Camal , achieving only a 5% reduction in latency over 31 sampling hours.\nThis change in sampling hours is primarily due to the increase in the sampling cost per sample\ncompared to the original settings.\nThe impact of delete workload. To investigate the generalizability of Camal to a broader range\nof operations, we evaluate its performance in static scenarios with not only updates but also deletes.\nWe vary the ratio of deletes within the writes across two workloads. As illustrated in Figure 6e,\nthe tuned system latency remains nearly unchanged in both scenarios. This is because, unless\nspecifically targeted by optimization strategies, deletes are handled in much the same way as inserts\nand updates in LSM-trees.\nThe sensitivity of parameters for dynamic mode. We have detailed the detection method in\nSection 6. In our default setting, we set ğœ=10%andğ‘=10ğ‘˜(ğ‘˜stands for thousand), where we\nsplit the entire dynamic workload into sub-workloads of 10ğ‘˜operations each. For example, if the\nlast reconfiguration was triggered when ğ‘¤=30%, and in the current sub-workload ğ‘¤=15%, we\nstart the reconfiguration; if ğ‘¤=25%, we keep the configuration unchanged and move on to the\nnext sub-workload.\nWe examine the sensitivity of ğœandğ‘in Figure 6h. Decreasing ğ‘below 50ğ‘˜improves the post-\ntuning system latency, achieving 72% âˆ¼75% normalized latency, but when ğ‘is smaller than 1ğ‘˜\noperations degrades performance as it cannot accurately inspect the true percentage of the randomly\ngenerated workloads. For ğœ, the post-tuning system latency becomes stable when the threshold\nis below 20%. Further, decreasing ğ‘andğœwill both increase the transition I/Os, which is the I/O\ncost incurred during reconfiguration, when reconfiguration becomes more frequent. However, the\ntransition I/Os will stabilize when ğœ<20%andğ‘<50ğ‘˜, and compared to the post-tuning system\nI/Os, they are relatively small. This is because the lazy transition strategy we adopted ensures that\nthe LSM-tree structure only changes during natural compaction, even though with more frequent\nreconfigurations. We also note that when the workload change gradually, which is often the case\nin practice, the tuned LSM-tree does not need to change dramatically.\nIn summary, moderate values of ğœ(5% to 20%) and ğ‘(5ğ‘˜to50ğ‘˜) would provide a range where\nthe system performance after tuning is less sensitive to the parameters.\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:20 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\n+3 +6 +90.60.70.8Normalized Latency(vs. RocksDB)+K(Independent)\n+K(codependent)+File Size\n(a)Samples of new parameters\n0.5 1 1.5 8 160.80.91.01.11.2Normalized Objectives\nClassic\nCAMAL(Trees)+Tail Latency\nCAMAL(Trees)+I/Os (b)Objectives sampling hours\n0.2 0.4 0.6 0.80.850.900.951.001.05Normalized Latency\nCAMAL(Trees,a)\nCAMAL(Trees,b)\nCAMAL(Trees,c) (c)Strategy vs. Skewness\n0 1 2 3 40.80.91.01.11.2Normalized Latency\nCAMAL(Poly)\nCAMAL(Poly,Uncertain)\nEndure (d)Uncertainty Region\nFig. 7. Camal demonstrates versatility with potential extensions to new parameters, alternative objectives,\nand varied data distributions.\n8.4 Model Extension and Discussion\nIn this section, we broaden the application across various scenarios to validate the generalization\ncapabilities of Camal and to explore its potential further. This involves evaluating the model\nagainst diverse data distributions, incorporating a wider array of tunable parameters, exploring the\nuse of alternative learning objectives.\nPerformance under different data distributions. As shown in Figure 6b, compared with\ntraditional tuning methods, Camal can accurately capture the actual distribution of accessed data\nblocks, which further allows more precise configurations. In addition, with an increase in the skew\ncoefficient, certain blocks experience higher access frequency, resulting in improved efficiency of\nthe block cache. Consequently, Camal facilitates more rational memory allocation, wherein the\nblock cache receives a greater allocation of memory when the data distribution exhibits higher\nskewness.\nAs previously discussed, we have proposed three training options that can be employed if\ninformation regarding the data distribution is available. As shown in Figure 7c, strategies (b) and\n(c) can each enhance the optimization of strategy (a) by approximately up to 15% when skewness\nincreases. This improvement can be attributed to more effective cache allocation based on the known\ndata distribution, as cache sensitivity to data distribution is demonstrated in many works [ 11,84].\nExtend to uncertain workloads. We follow the setting in Endure [ 40], where the expected\nuncertainty regains are the same as the observed regions. As shown in Figure 7d, the original\nCamal , which does not take uncertainty into account, can outperform Endure for reasonable\nuncertain ranges. This is because Camal â€™s tuning performance is significantly better than the\nclassic tuning adopted in Endure, so small uncertainty does not have enough impact to cause change.\nIf we take workload uncertainty into account, the effect becomes more pronounced compared to\nthe original settings. This is because we choose the settings that lead to the samples with the lowest\naverage latency in the uncertainty region based on accurate estimations.\nThe possibility of involving more parameters based on group-wise sampling. In addition to\nthe tunable parameters explored in previous experiments, numerous specific or practical parameters\nexist in various LSM-tree systems. Here we discuss one possible solution to extend Camal for\nconsidering these parameters. Through theoretical and experimental analysis, we observe that\nsome parameters are strongly dependent. Therefore, we can group strongly dependent parameters\ntogether, and then sample and fine-tune the parameters within one group before proceeding to\nanother group. For instance, let us consider two more parameters â€“ the number of runs at each\nlevel (ğ¾), and the file size of each SST. We will then select to group ğ¾andğ‘‡, as they are strongly\ndependent [ 41,50,58]. Meanwhile, the SST file size does not have an explicit correlation with\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:21\nother parameters we chose. Within each group, we would sample the parameters together. For\ninstance, our solution groups ğ‘‡andğ¾in a 2-dimensional sampling space. In the active learning\ncycle, we first calculate the theoretical optimal pair (ğ‘‡âˆ—,ğ¾âˆ—). Then, based on the given sampling\nbudget, we sample these two parameters in a 2-dimensional neighborhood. The subsequent steps\nfollow Algorithm2, except that we obtain the tuned combination (ğ‘‡â€²,ğ¾â€²)using ML models, instead\nof just tuning ğ‘‡â€². For the file size, as it has not been shown to have an explicit correlation with our\nexisting parameters ( ğ‘‡,ğ¾, and memory-related parameters), we sample it after the other parameters.\nFollowing our decoupled active learning paradigm, we initiate with a practically reasonable file\nsize (e.g., the default file size in RocksDB) as the sampling center and then obtain samples from its\nneighbors.\nAs shown in Figure 7a, we evaluate two strategies for incorporating the parameter ğ¾. When\nsamplingğ¾andğ‘‡separately, which means we first obtain a tuned ğ‘‡âˆ—and then sample ğ¾based\nonğ‘‡âˆ—, the optimization performance is noticeably inferior compared to when they are sampled\ntogether. This is because ğ¾andğ‘‡are closely correlated, and sampling them independently is likely\nto result in a local optimum for a narrow range of sampled ğ‘‡. For co-dependent sampling, we also\nneed to add extra samples to achieve better performance, as the correlation requires more data\nto be accurately learned. On the other hand, we sample the file size independently since we have\nnot identified any explicit correlation between it and other tunable parameters in this study. The\noptimization impact is not as pronounced as with other parameters, because it primarily affects\nspace amplification rather than I/Os or latency [25].\nWe acknowledge the complexity inherent in tuning LSM-tree parameters, as they can be inter-\ndependent, slightly dependent, or only show dependency under certain queries. This complexity\nguides us to thoughtfully categorize parameters within the Camal framework for practical exami-\nnation: (1) The first category consists of parameters that are crucial to the LSM-treeâ€™s structure,\nnotablyğ‘‡andğ¾. (2) The second category encompasses memory-related parameters, such as the\nwrite buffer, Bloom filter, and block cache. (3) The third category is dedicated to detailed merge poli-\ncies, like file size, along with system-specific parameters, such as sub-compaction and compaction\ngranularity. This categorization reflects observed correlations: parameters within each category\ntend to have strong or inherent correlations, whereas those across different categories generally\ndisplay relatively implicit connections. Although this classification might not align perfectly with\nevery theoretical consideration, the flexible nature of ML models to generalize could help mitigate\nthe gaps in terms of performance. Exploring the integration of additional parameters further, partic-\nularly in refining our understanding and enhancing the modelâ€™s applicability, remains a compelling\ndirection for future research endeavors.\nThe possibility of using other optimization objectives in Camal .Given that machine learn-\ning methods can model a wide range of learning objectives, we have explored two additional\nrepresentative objectives: tail latency and I/O operations. These have been employed both as\nlearning objectives and evaluation metrics within Camal . The results demonstrate their feasibility\nas optimization objectives. The tail latency we focus on is the 90th percentile, a common metric\nin SQL databases [ 74,81]. As illustrated in Figure 7b, with sampling hours increased to 1.5, we\nobserve approximately 15% lower latency compared to a well-tuned RocksDB configuration. When\noptimizing for I/O operations, the improvement is less pronounced, achieving only 8% lower la-\ntency than RocksDB at 1.5 sampling hours. This reduced effectiveness can be attributed to the\ncomplex correlations between I/O operations and configurations, primarily due to the randomness\nintroduced by compaction and block cache activities, which can adversely affect the learning\neffectiveness of Camal . Despite these challenges, there remains potential for employing other\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:22 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\nobjectives within the Camal framework in certain scenarios, such as those where outlier response\ntimes are critical or I/O operations are a bottleneck.\n9 Related Work\nMachine Learning for Database Systems. There have been many extensive studies focused on\nusing machine learning to optimize storage systems, particularly for SQL databases [ 13,56,82].\nOttertune [ 76] uses machine learning techniques to analyze and tune database configuration\nknobs and give recommendation settings based on statistics collected from the database. Pavlo et\nal.[64,65] and Aken et al. [77] introduced the concept of a self-driving database that can make\nautomated decisions with machine learning models. Abu-Libdeh et al. [4] present several design\ndecisions involved in integrating learned indexes into Bigtable [ 16] and demonstrate significant\nperformance improvements as a result. Meanwhile, Qtune [ 48], CDBTune [ 87], and CDBTune+ [ 88]\nare reinforcement learning-based tuning methods for databases. There are also studies concentrating\non cost estimation [ 57,74] and cardinality estimation [ 29,36] based on the queries to optimize the\nplan.\nThe utilization of machine learning in LSM-tree has rarely been considered, particularly in\nthe context of instance-optimization. Bourbon [ 21] incorporates machine learning into the LSM-\ntree, which uses learned index [ 47] to improve the fence pointers. Leaper [ 84] employs a learned\nprefetcher to improve the block cache. The most recent work, RusKey [ 58], is the first to use\nreinforcement learning to optimize the compaction policy of an LSM-tree. To the best of our\nknowledge, we have the first attempt at applying active learning to the instance-optimized LSM-\nbased key-value stores.\nLSM-tree Optimization. There are numerous studies on LSM-tree optimizations [ 3,5,6,9,10,\n12,15,17,22â€“26,31,39,40,42,45,46,51â€“54,60,66â€“73,75,78,80,83,86,89â€“91]. Monkey [ 22] first\nsystematically formulates the theoretical cost models referred in this paper for an LSM-tree by\nestimating the expected I/O cost. Based on the model, Monkey co-tunes the compaction policy, the\nmemory allocated to the write buffer and the Bloom filter to locate an optimal LSM-tree design with\nthe minimal I/O cost for a given workload. Dostoevsky [ 23] shows that existing compaction policies\n(tiering and leveling) cannot fully trade between read costs and write costs. Therefore, it proposes\nFluid LSM-tree to enable hybrid compaction policies. Cosine [ 17] presents a more meticulous I/O\ncost model that is aware of workload distribution for key-value stores on the cloud. Endure [ 40]\nintroduces a pipeline to jointly tune the compaction policy, size ratio, and allocated memory, and\noptimize the performance when workload uncertainty is involved. Spooky [ 25] designs a new\ncompaction granulation for LSM-tree by partitioning data more reasonably and cheapening garbage\ncollection. Compared with these works, our study is the first one that employs both theoretical\ncost analysis and ML model to jointly tune the common parameters in an LSM-tree.\n10 Conclusion\nWe present Camal , a complexity-analysis driven ML-aided tuner for LSM-tree based key-value\nstores. The core idea is to use machine learning to model the actual cost of workloads while using\nboth theoretical analysis and active learning paradigm to help prune the sampling space. Compared\nto well-tuned RocksDB, Camal achieves an average reduction in system latency by 28%. Moreover,\nit significantly reduces training costs by up to 90% compared to conventional ML approaches,\nthereby enhancing practical usability.\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:23\nAcknowledgments\nThis research is supported by NTU-NAP startup grant (022029-00001) and Singapore MOE AcRF\nTier-2 grant MOE-T2EP20223-0004. We thank the anonymous reviews for their valuable suggestions.\nReferences\n[1] 2016. HBase. http://hbase.apache.org/. [Online; accessed 12-January-2022].\n[2] 2021. WiredTiger. https://github.com/wiredtiger/wiredtiger.\n[3]Ildar Absalyamov, Michael J Carey, and Vassilis J Tsotras. 2018. Lightweight cardinality estimation in LSM-based\nsystems. In Proceedings of the 2018 International Conference on Management of Data . 841â€“855.\n[4]Hussam Abu-Libdeh, Deniz AltÄ±nbÃ¼ken, Alex Beutel, Ed H Chi, Lyric Doshi, Tim Kraska, Andy Ly, Christopher Olston,\net al. 2020. Learned indexes for a google-scale disk-based database. arXiv preprint arXiv:2012.12501 (2020).\n[5] Muhammad Yousuf Ahmad and Bettina Kemme. 2015. Compaction management in distributed key-value datastores.\nProceedings of the VLDB Endowment 8, 8 (2015), 850â€“861.\n[6]Wail Y Alkowaileet, Sattam Alsubaiee, and Michael J Carey. 2019. An LSM-based Tuple Compaction Framework for\nApache AsterixDB (Extended Version). arXiv preprint arXiv:1910.08185 (2019).\n[7] Apache. 2016. Cassandra. http://cassandra.apache.org.\n[8]Timothy G Armstrong, Vamsi Ponnekanti, Dhruba Borthakur, and Mark Callaghan. 2013. Linkbench: a database\nbenchmark based on the facebook social graph. In Proceedings of the 2013 ACM SIGMOD International Conference on\nManagement of Data . 1185â€“1196.\n[9]Oana Balmau, Diego Didona, Rachid Guerraoui, Willy Zwaenepoel, Huapeng Yuan, Aashray Arora, Karan Gupta, and\nPavan Konka. 2017. {TRIAD}: Creating Synergies Between Memory, Disk and Log in Log Structured Key-Value Stores.\nIn2017{USENIX}Annual Technical Conference ( {USENIX}{ATC}17). 363â€“375.\n[10] Oana Balmau, Florin Dinu, Willy Zwaenepoel, Karan Gupta, Ravishankar Chandhiramoorthi, and Diego Didona. 2019.\nSILK: Preventing Latency Spikes in Log-Structured Merge Key-Value Stores.. In USENIX Annual Technical Conference .\n753â€“766.\n[11] Christian Berthet. 2017. Approximation of LRU caches miss rate: Application to power-law popularities. arXiv preprint\narXiv:1705.10738 (2017).\n[12] Edward Bortnikov, Anastasia Braginsky, Eshcar Hillel, Idit Keidar, and Gali Sheffi. 2018. Accordion: Better memory\norganization for LSM key-value stores. Proceedings of the VLDB Endowment 11, 12 (2018), 1863â€“1875.\n[13] Matthew Butrovich, Wan Shen Lim, Lin Ma, John Rollinson, William Zhang, Yu Xia, and Andrew Pavlo. 2022. Tastes\nGreat! Less Filling! High Performance and Accurate Training Data Collection for Self-Driving Database Management\nSystems. In Proceedings of the 2022 International Conference on Management of Data . 617â€“630.\n[14] Zhao Cao, Shimin Chen, Feifei Li, Min Wang, and X Sean Wang. 2013. LogKV: Exploiting key-value stores for event\nlog processing. In Proc. Conf. Innovative Data Syst. Res .\n[15] Helen HW Chan, Yongkun Li, Patrick PC Lee, and Yinlong Xu. 2018. Hashkv: Enabling efficient updates in {KV}\nstorage via hashing. In 2018{USENIX}Annual Technical Conference ( {USENIX}{ATC}18). 1007â€“1019.\n[16] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C Hsieh, Deborah A Wallach, Mike Burrows, Tushar Chandra,\nAndrew Fikes, and Robert E Gruber. 2008. Bigtable: A distributed storage system for structured data. ACM Transactions\non Computer Systems (TOCS) 26, 2 (2008), 1â€“26.\n[17] Subarna Chatterjee, Meena Jagadeesan, Wilson Qin, and Stratos Idreos. 2021. Cosine: a cloud-cost optimized self-\ndesigning key-value storage engine. Proceedings of the VLDB Endowment 15, 1 (2021), 112â€“126.\n[18] Guoqiang Jerry Chen, Janet L Wiener, Shridhar Iyer, Anshul Jaiswal, Ran Lei, Nikhil Simha, Wei Wang, Kevin Wilfong,\nTim Williamson, and Serhat Yilmaz. 2016. Realtime data processing at facebook. In Proceedings of the 2016 International\nConference on Management of Data . 1087â€“1098.\n[19] Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, Rory Mitchell,\nIgnacio Cano, Tianyi Zhou, et al. 2015. Xgboost: extreme gradient boosting. R package version 0.4-2 1, 4 (2015), 1â€“4.\n[20] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell Sears. 2010. Benchmarking cloud\nserving systems with YCSB. In Proceedings of the 1st ACM symposium on Cloud computing . 143â€“154.\n[21] Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth, Andrea C Arpaci-Dusseau, and Remzi H\nArpaci-Dusseau. 2020. From wisckey to bourbon: A learned index for log-structured merge trees. In Proceedings of the\n14th USENIX Conference on Operating Systems Design and Implementation . 155â€“171.\n[22] Niv Dayan, Manos Athanassoulis, and Stratos Idreos. 2017. Monkey: Optimal navigable key-value store. In Proceedings\nof the 2017 ACM International Conference on Management of Data . 79â€“94.\n[23] Niv Dayan and Stratos Idreos. 2018. Dostoevsky: Better space-time trade-offs for LSM-tree based key-value stores via\nadaptive removal of superfluous merging. In Proceedings of the 2018 International Conference on Management of Data .\n505â€“520.\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:24 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\n[24] Niv Dayan and Moshe Twitto. 2021. Chucky: A succinct cuckoo filter for LSM-tree. In Proceedings of the 2021\nInternational Conference on Management of Data . 365â€“378.\n[25] Niv Dayan, Tamar Weiss, Shmuel Dashevsky, Michael Pan, Edward Bortnikov, and Moshe Twitto. 2022. Spooky:\ngranulating LSM-tree compactions correctly. Proceedings of the VLDB Endowment 15, 11 (2022), 3071â€“3084.\n[26] Dayan, Niv and Idreos, Stratos. 2019. The log-structured merge-bush & the wacky continuum. In Proceedings of the\n2019 International Conference on Management of Data . 449â€“466.\n[27] Giuseppe DeCandia, D Hastorun, Madan Jampani, Gunavardhan Kakulapati, Avinash Lakshman, S Sivasubramanian A\nPilchin, P Vosshall, and W Vogels. 2007. Dynamo: Amazonâ€™s Highly Available Key-Value Store. SOSP, 2007.\n[28] Songyun Duan, Vamsidhar Thummala, and Shivnath Babu. 2009. Tuning database configuration parameters with\nituned. Proceedings of the VLDB Endowment 2, 1 (2009), 1246â€“1257.\n[29] Anshuman Dutt, Chi Wang, Azade Nazi, Srikanth Kandula, Vivek Narasayya, and Surajit Chaudhuri. 2019. Selectivity\nestimation for range predicates using lightweight models. Proceedings of the VLDB Endowment 12, 9 (2019), 1044â€“1057.\n[30] Facebook. 2016. RocksDB. https://github.com/facebook/rocksdb.\n[31] Guy Golan-Gueta, Edward Bortnikov, Eshcar Hillel, and Idit Keidar. 2015. Scaling concurrent log-structured data\nstores. In Proceedings of the Tenth European Conference on Computer Systems . 1â€“14.\n[32] Google. 2016. LevelDB. https://github.com/google/leveldb/.\n[33] Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, and Sheng Wang. 2023. The RLR-Tree: A Reinforcement\nLearning Based R-Tree for Spatial Data. Proceedings of the ACM on Management of Data 1, 1 (2023), 1â€“26.\n[34] Charles R Harris, K Jarrod Millman, StÃ©fan J Van Der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric\nWieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al .2020. Array programming with NumPy. Nature 585,\n7825 (2020), 357â€“362.\n[35] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The elements of statistical learning:\ndata mining, inference, and prediction . Vol. 2. Springer.\n[36] Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian Kersting, and Carsten Binnig. 2019.\nDeepDB: Learn from Data, not from Queries! Proceedings of the VLDB Endowment 13, 7 (2019).\n[37] Xia Hu, Lingyang Chu, Jian Pei, Weiqing Liu, and Jiang Bian. 2021. Model complexity of deep learning: A survey.\nKnowledge and Information Systems 63 (2021), 2585â€“2619.\n[38] Enhui Huang, Liping Peng, Luciano Di Palma, Ahmed Abdelkafi, Anna Liu, and Yanlei Diao. 2018. Optimization\nfor active learning-based interactive database exploration . Ph. D. Dissertation. Ecole Polytechnique; University of\nMassachusetts Amherst.\n[39] Gui Huang, Xuntao Cheng, Jianying Wang, Yujie Wang, Dengcheng He, Tieying Zhang, Feifei Li, Sheng Wang, Wei\nCao, and Qiang Li. 2019. X-Engine: An optimized storage engine for large-scale E-commerce transaction processing.\nInProceedings of the 2019 International Conference on Management of Data . 651â€“665.\n[40] Andy Huynh, Harshal A Chaudhari, Evimaria Terzi, and Manos Athanassoulis. 2022. Endure: a robust tuning paradigm\nfor LSM trees under workload uncertainty. Proceedings of the VLDB Endowment 15, 8 (2022), 1605â€“1618.\n[41] Andy Huynh, Harshal A. Chaudhari, Evimaria Terzi, and Manos Athanassoulis. 2023. Towards Flexibility and\nRobustness of LSM Trees. arXiv:2311.10005 [cs.DB]\n[42] Stratos Idreos, Niv Dayan, Wilson Qin, Mali Akmanalp, Sophie Hilgard, Andrew Ross, James Lennon, Varun Jain,\nHarshita Gupta, David Li, et al .2019. Design Continuums and the Path Toward Self-Designing Key-Value Stores that\nKnow and Learn.. In CIDR .\n[43] William Jannen, Jun Yuan, Yang Zhan, Amogh Akshintala, John Esmet, Yizheng Jiao, Ankur Mittal, Prashant Pandey,\nPhaneendra Reddy, Leif Walsh, et al .2015. BetrFS: A Right-Optimized Write-Optimized File System.. In FAST , Vol. 15.\n301â€“315.\n[44] Brendan Juba and Hai S Le. 2019. Precision-recall versus accuracy and the role of large data sets. In Proceedings of the\nAAAI conference on artificial intelligence , Vol. 33. 4039â€“4048.\n[45] Taewoo Kim, Alexander Behm, Michael Blow, Vinayak Borkar, Yingyi Bu, Michael J Carey, Murtadha Hubail, Shiva\nJahangiri, Jianfeng Jia, Chen Li, et al .2020. Robust and efficient memory management in Apache AsterixDB. Software:\nPractice and Experience 50, 7 (2020), 1114â€“1151.\n[46] Eric R Knorr, Baptiste Lemaire, Andrew Lim, Siqiang Luo, Huanchen Zhang, Stratos Idreos, and Michael Mitzenmacher.\n2022. Proteus: A Self-Designing Range Filter. In Proceedings of the 2022 International Conference on Management of\nData . 1670â€“1684.\n[47] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018. The case for learned index structures. In\nProceedings of the 2018 international conference on management of data . 489â€“504.\n[48] Guoliang Li, Xuanhe Zhou, Shifu Li, and Bo Gao. 2019. Qtune: A query-aware database tuning system with deep\nreinforcement learning. Proceedings of the VLDB Endowment 12, 12 (2019), 2118â€“2130.\n[49] Honghao Lin, Tian Luo, and David Woodruff. 2022. Learning augmented binary search trees. In International Conference\non Machine Learning . PMLR, 13431â€“13440.\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\nCAMAL: Optimizing LSM-trees via Active Learning 202:25\n[50] Junfeng Liu, Fan Wang, Dingheng Mo, and Siqiang Luo. 2024. Structural Designs Meet Optimality: Exploring Optimized\nLSM-tree Structures in A Colossal Configuration Space. In Proceedings of the 2024 ACM SIGMOD International Conference\non Management of Data (SIGMOD) . ACM.\n[51] Lanyue Lu, Thanumalayan Sankaranarayana Pillai, Hariharan Gopalakrishnan, Andrea C Arpaci-Dusseau, and Remzi H\nArpaci-Dusseau. 2017. Wisckey: Separating keys from values in ssd-conscious storage. ACM Transactions on Storage\n(TOS) 13, 1 (2017), 1â€“28.\n[52] Chen Luo and Michael J Carey. 2019. On performance stability in LSM-based storage systems (extended version).\narXiv preprint arXiv:1906.09667 (2019).\n[53] Chen Luo and Michael J Carey. 2020. Breaking down memory walls: Adaptive memory management in LSM-based\nstorage systems. Proceedings of the VLDB Endowment 14, 3 (2020), 241â€“254.\n[54] Siqiang Luo, Subarna Chatterjee, Rafael Ketsetsidis, Niv Dayan, Wilson Qin, and Stratos Idreos. 2020. Rosetta: A robust\nspace-time optimized range filter for key-value stores. In Proceedings of the 2020 ACM SIGMOD International Conference\non Management of Data . 2071â€“2086.\n[55] Lin Ma, Bailu Ding, Sudipto Das, and Adith Swaminathan. 2020. Active learning for ML enhanced database systems.\nInProceedings of the 2020 ACM SIGMOD International Conference on Management of Data . 175â€“191.\n[56] Lin Ma, William Zhang, Jie Jiao, Wuwen Wang, Matthew Butrovich, Wan Shen Lim, Prashanth Menon, and Andrew\nPavlo. 2021. MB2: decomposed behavior modeling for self-driving database management systems. In Proceedings of the\n2021 International Conference on Management of Data . 1248â€“1261.\n[57] Ryan Marcus and Olga Papaemmanouil. 2019. Plan-structured deep neural network models for query performance\nprediction. Proceedings of the VLDB Endowment 12, 11 (2019), 1733â€“1746.\n[58] Dingheng Mo, Fanchao Chen, Siqiang Luo, and Caihua Shan. 2023. Learning to Optimize LSM-trees: Towards A\nReinforcement Learning based Key-Value Store for Dynamic Workloads. Proceedings of the ACM on Management of\nData 1, 3 (2023), 1â€“25.\n[59] Barzan Mozafari, Purnamrita Sarkar, Michael J Franklin, Michael I Jordan, and Samuel Madden. 2012. Active learning\nfor crowd-sourced databases. arXiv preprint arXiv:1209.3686 (2012).\n[60] Ju Hyoung Mun, Zichen Zhu, Aneesh Raman, and Manos Athanassoulis. 2022. LSM-Trees Under (Memory) Pressure.\nInProceedings of the International Workshop on Accelerating Data Management Systems Using Modern Processor and\nStorage Architectures (ADMS) .\n[61] Fernando Nogueira. 2014â€“. Bayesian Optimization: Open source constrained global optimization tool for Python.\nhttps://github.com/fmfn/BayesianOptimization\n[62] Patrick Oâ€™Neil, Edward Cheng, Dieter Gawlick, and Elizabeth Oâ€™Neil. 1996. The log-structured merge-tree (LSM-tree).\nActa Informatica 33 (1996), 351â€“385.\n[63] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,\nNatalia Gimelshein, Luca Antiga, et al .2019. PyTorch: An imperative style, high-performance deep learning library.\nAdvances in neural information processing systems 32 (2019).\n[64] Andrew Pavlo, Gustavo Angulo, Joy Arulraj, Haibin Lin, Jiexi Lin, Lin Ma, Prashanth Menon, Todd C Mowry, Matthew\nPerron, Ian Quah, et al. 2017. Self-Driving Database Management Systems.. In CIDR , Vol. 4. 1.\n[65] Andrew Pavlo, Matthew Butrovich, Lin Ma, Prashanth Menon, Wan Shen Lim, Dana Van Aken, and William Zhang.\n2021. Make your database system dream of electric sheep: towards self-driving operation. Proceedings of the VLDB\nEndowment 14, 12 (2021), 3211â€“3221.\n[66] Pandian Raju, Rohan Kadekodi, Vijay Chidambaram, and Ittai Abraham. 2017. Pebblesdb: Building key-value stores\nusing fragmented log-structured merge trees. In Proceedings of the 26th Symposium on Operating Systems Principles .\n497â€“514.\n[67] Kai Ren, Qing Zheng, Joy Arulraj, and Garth Gibson. 2017. SlimDB: A space-efficient key-value storage engine for\nsemi-sorted data. Proceedings of the VLDB Endowment 10, 13 (2017), 2037â€“2048.\n[68] Subhadeep Sarkar and Manos Athanassoulis. 2022. Dissecting, Designing, and Optimizing LSM-based Data Stores. In\nProceedings of the 2022 International Conference on Management of Data . 2489â€“2497.\n[69] Subhadeep Sarkar, Niv Dayan, and Manos Athanassoulis. 2023. The LSM Design Space and its Read Optimizations. In\nProceedings of the IEEE International Conference on Data Engineering (ICDE) .\n[70] Subhadeep Sarkar, Tarikul Islam Papon, Dimitris Staratzis, and Manos Athanassoulis. 2020. Lethe: A tunable delete-\naware LSM engine. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data . 893â€“908.\n[71] Subhadeep Sarkar, Dimitris Staratzis, Zichen Zhu, and Manos Athanassoulis. 2022. Constructing and analyzing the\nLSM compaction design space. arXiv preprint arXiv:2202.04522 (2022).\n[72] Russell Sears and Raghu Ramakrishnan. 2012. bLSM: a general purpose log structured merge tree. In Proceedings of the\n2012 ACM SIGMOD International Conference on Management of Data . 217â€“228.\n[73] Pradeep J Shetty, Richard P Spillane, Ravikant R Malpani, Binesh Andrews, Justin Seyster, and Erez Zadok. 2013.\nBuilding workload-independent storage with VT-trees. In Presented as part of the 11th {USENIX}Conference on File\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.\n\n202:26 Weiping Yu, Siqiang Luo, Zihao Yu & Gao Cong.\nand Storage Technologies ( {FAST}13). 17â€“30.\n[74] Ji Sun and Guoliang Li. 2018. An End-to-End Learning-based Cost Estimator. Proceedings of the VLDB Endowment 13,\n3 (2018).\n[75] Risi Thonangi and Jun Yang. 2017. On log-structured merge for solid-state drives. In 2017 IEEE 33rd International\nConference on Data Engineering (ICDE) . IEEE, 683â€“694.\n[76] Dana Van Aken, Andrew Pavlo, Geoffrey J Gordon, and Bohan Zhang. 2017. Automatic database management system\ntuning through large-scale machine learning. In Proceedings of the 2017 ACM international conference on management\nof data . 1009â€“1024.\n[77] Dana Van Aken, Dongsheng Yang, Sebastien Brillard, Ari Fiorino, Bohan Zhang, Christian Bilien, and Andrew Pavlo.\n2021. An inquiry into machine learning-based automatic configuration tuning services on real-world database\nmanagement systems. Proceedings of the VLDB Endowment 14, 7 (2021), 1241â€“1253.\n[78] Tobias VinÃ§on, Sergej Hardock, Christian Riegger, Julian Oppermann, Andreas Koch, and Ilia Petrov. 2018. Noftl-kv:\nTackling write-amplification on kv-stores with native storage management. In Advances in database technology-EDBT\n2018: 21st International Conference on Extending Database Technology, Vienna, Austria, March 26-29, 2018. proceedings .\nUniversity of Konstanz, University Library, 457â€“460.\n[79] Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski,\nPearu Peterson, Warren Weckesser, Jonathan Bright, et al .2020. SciPy 1.0: fundamental algorithms for scientific\ncomputing in Python. Nature methods 17, 3 (2020), 261â€“272.\n[80] Peng Wang, Guangyu Sun, Song Jiang, Jian Ouyang, Shiding Lin, Chen Zhang, and Jason Cong. 2014. An efficient\ndesign and implementation of LSM-tree based key-value store on open-channel SSD. In Proceedings of the Ninth\nEuropean Conference on Computer Systems . 1â€“14.\n[81] Xiaoying Wang, Changbo Qu, Weiyuan Wu, Jiannan Wang, and Qingqing Zhou. 2020. Are we ready for learned\ncardinality estimation? arXiv preprint arXiv:2012.06743 (2020).\n[82] Zhaoguo Wang, Zhou Zhou, Yicun Yang, Haoran Ding, Gansen Hu, Ding Ding, Chuzhe Tang, Haibo Chen, and Jinyang\nLi. 2022. WeTune: Automatic Discovery and Verification of Query Rewrite Rules. In Proceedings of the 2022 International\nConference on Management of Data . 94â€“107.\n[83] Xingbo Wu, Yuehai Xu, Zili Shao, and Song Jiang. 2015. LSM-trie: An LSM-tree-based ultra-large key-value store for\nsmall data. In Proceedings of the 2015 USENIX Conference on Usenix Annual Technical Conference . USENIX Association,\n71â€“82.\n[84] Lei Yang, Hong Wu, Tieying Zhang, Xuntao Cheng, Feifei Li, Lei Zou, Yujie Wang, Rongyao Chen, Jianying Wang, and\nGui Huang. 2020. Leaper: A learned prefetcher for cache invalidation in LSM-tree based storage engines. Proceedings\nof the VLDB Endowment 13, 12 (2020), 1976â€“1989.\n[85] Xue Ying. 2019. An overview of overfitting and its solutions. In Journal of physics: Conference series , Vol. 1168. IOP\nPublishing, 022022.\n[86] Huanchen Zhang, Hyeontaek Lim, Viktor Leis, David G Andersen, Michael Kaminsky, Kimberly Keeton, and Andrew\nPavlo. 2018. Surf: Practical range query filtering with fast succinct tries. In Proceedings of the 2018 International\nConference on Management of Data . 323â€“336.\n[87] Ji Zhang, Yu Liu, Ke Zhou, Guoliang Li, Zhili Xiao, Bin Cheng, Jiashu Xing, Yangtao Wang, Tianheng Cheng, Li Liu,\net al.2019. An end-to-end automatic cloud database tuning system using deep reinforcement learning. In Proceedings\nof the 2019 International Conference on Management of Data . 415â€“432.\n[88] Ji Zhang, Ke Zhou, Guoliang Li, Yu Liu, Ming Xie, Bin Cheng, and Jiashu Xing. 2021. CDBTune+: An efficient deep\nreinforcement learning-based automatic cloud database tuning system. The VLDB Journal 30, 6 (2021), 959â€“987.\n[89] Teng Zhang, Jianying Wang, Xuntao Cheng, Hao Xu, Nanlong Yu, Gui Huang, Tieying Zhang, Dengcheng He, Feifei\nLi, Wei Cao, et al. 2020. FPGA-Accelerated Compactions for LSM-based Key-Value Store.. In FAST . 225â€“237.\n[90] Yueming Zhang, Yongkun Li, Fan Guo, Cheng Li, and Yinlong Xu. 2018. ElasticBF: Fine-grained and Elastic Bloom\nFilter Towards Efficient Read for LSM-tree-based KV Stores.. In HotStorage .\n[91] Zichen Zhu, Ju Hyoung Mun, Aneesh Raman, and Manos Athanassoulis. 2021. Reducing bloom filter cpu overhead in\nlsm-trees on modern storage devices. In Proceedings of the 17th International Workshop on Data Management on New\nHardware (DaMoN 2021) . 1â€“10.\nReceived January 2024; revised April 2024; accepted May 2024\nProc. ACM Manag. Data, Vol. 2, No. N4 (SIGMOD), Article 202. Publication date: September 2024.",
  "textLength": 101205
}