{
  "paperId": "bf5d9e92e921a945d3e6652dc786c3c7d96988ae",
  "title": "Testing the Robustness of Learned Index Structures",
  "pdfPath": "bf5d9e92e921a945d3e6652dc786c3c7d96988ae.pdf",
  "text": "Testing the Robustness of Learned Index Structures\nMatthias Bachfischer\nThe University of Melbourne\nMelbourne, Australia\nbachfischer.matthias@googlemail.comRenata Borovica-Gajic\nThe University of Melbourne\nMelbourne, Australia\nrenata.borovica@unimelb.edu.auBenjamin I. P. Rubinstein\nThe University of Melbourne\nMelbourne, Australia\nbenjamin.rubinstein@unimelb.edu.au\nABSTRACT\nWhile early empirical evidence has supported the case for learned in-\ndex structures as having favourable average-case performance, little\nis known about their worst-case performance. By contrast, classical\nstructures are known to achieve optimal worst-case behaviour. This\nwork evaluates the robustness of learned index structures in the\npresence of adversarial workloads. To simulate adversarial work-\nloads, we carry out a data poisoning attack on linear regression\nmodels that manipulates the cumulative distribution function (CDF)\non which the learned index model is trained. The attack deteriorates\nthe fit of the underlying ML model by injecting a set of poisoning\nkeys into the training dataset, which leads to an increase in the\nprediction error of the model and thus deteriorates the overall per-\nformance of the learned index structure. We assess the performance\nof various regression methods and the learned index implementa-\ntions ALEX and PGM-Index. We show that learned index structures\ncan suffer from a significant performance deterioration of up to\n20% when evaluated on poisoned vs. non-poisoned datasets.\nKEYWORDS\nLearned index, Database indexing, Adversarial Machine Learning\n1 INTRODUCTION\nIn traditional database design, tree-based data structures such as B+\nTrees and their variants have seen wide adoption due to their rela-\ntive ease of implementation and optimal worst-case computational\ncomplexity guarantees. B+ Trees are general-purpose data struc-\ntures that make no prior assumptions about the data distribution\nand do not take advantage of any patterns that might be specific\nto the data that the data structure stores. In practice however, real-\nworld data often follows some underlying pattern that, if modeled\nappropriately, could significantly speed-up the data retrieval pro-\ncess. For instance, given a set of contiguous integer keys ( e.g., keys\nfrom 1 to 100 million), the key itself could be used as an offset for\nthe index, thus reducing the time required to look-up any key in\nthe dataset from ùëÇ(logùëõ)for a B+ Tree to ùëÇ(1). This increase in\nperformance is what Kraska et al. hoped to achieve when they first\nintroduced their work on Learned Index Structure ( LIS) models\n[21]. Even though the concept of a LISis still new, it has already\nled to a surge of inspiring results that leverage ideas from Machine\nLearning ( ML), data structures, and database systems [ 7], [6], [31],\n[15], [1], [32], [5], [30], [23],[16], [11], [19], [8], [13], [27].\nThe core idea of a LISis to model the functionality of a data\nstructure as a prediction task, i.e., given an input key, the objective\nof the LISis to predict the key‚Äôs position in a key-sorted collection\nof key-value pairs. This approach allows the use of continuous\nfunctions to encode the data and leverage learning algorithms to\napproximate key lookup. Specifically, the approach proposed by\nKraska et al. [ 21] is to approximate the Cumulative DistributionFunction ( CDF) of the keys. Given a key ùëòas an input, the CDF\nreturns the probability that the chosen key takes a value less than or\nequal toùëò(i.e.,ùëÉ(ùëã‚â§Key k)). Based on this observation, one can\nuse the CDF to compute the number of keys less than the queried\nkeyùëòand infer the key‚Äôs memory location. In the context of a LIS,\ntheCDF is used to provide a mapping from the key ùëòto its position\nin a sorted array. The underlying learning task is one of supervised\nregression, or simply function interpolation [31].\nWhile the LISapproach may be beneficial in a certain average-\ncase sense, it also carries the risk of exploitation by a malicious\nadversary. Indeed adversarial analysis is an important tool for un-\nderstanding worst-case performance and whether LISapproaches\nare worst-case competitive with classical structures.\n2 RELATED WORK\n2.1 Learned Index Structures\nSince the first published work on learned index structures by Kraska\net al. [ 21], the research community has explored a variety of ideas\non how LIScould be used as a replacement for traditional index\nstructures such as B+ Trees. Contrary to a B+ Tree, learned index\nstructures rely on an underlying MLmodel to approximate the CDF.\nThe most commonly used models are the Piecewise Linear Approx-\nimation ( PLA) and Linear Spline ( LS) models. The PLA model is\na variant of the linear regression model that tries to approximate\ntheCDF of the dataset by dividing it into variable-sized segments.\nThe first and last keys of each segment are then used to construct a\nlinear approximation of the data, resulting in a PLA model. As an\nalternative, LSmodels fit the data by approximating the CDF via\nlinear spline points.\nTable 1: Overview of LIS implementations\nIndex Model Updates Open-Source Reference\nRMI Multiple ‚úó ‚úì [21]\nRadixSpline LS ‚úó ‚úì [19]\nPGM-Index PLA ‚úó ‚úì [11]\nALEX PLA ‚úì ‚úì [7]\nLIPP PLA ‚úì ‚úì [9]\nCOLIN PLA ‚úì ‚úó [35]\nTable 1 overviews the most relevant learned index implementa-\ntions published to date. A majority of indexes rely on a single type of\nmodel to construct the learned index: LSis used by RadixSpline [ 19]\nandPLA is used by Piecewise Geometric Model ( PGM )-Index [ 11],\nAdaptive Learned indEX ( ALEX ) [7] and a variety of other com-\npetitors [ 9], [35]. The Recursive Model Index ( RMI) [21] is the only\nLISthat supports a variety of model types, which gives the RMI a\ngreater degree of flexibility, but also increases the complexity of\ntuning the RMI [24].arXiv:2207.11575v1  [cs.DB]  23 Jul 2022\n\nConference‚Äô17, July 2017, Washington, DC, USA Bachfischer, Borovica-Gajic, and Rubinstein\n2.2 Adversarial Machine Learning\nThe term Adversarial Machine Learning describes the study of\nMachine Learning ( ML) techniques against an adversarial opponent\nthat aims to fool a model by supplying deceptive input. Adversarial\nMLhas emerged in recent years as a new field, mostly driven by new\nadvancements in computing capabilities [ 17], [2]. A main domain\nof focus has been computer vision.\nData poisoning attacks are widely studied within Adversarial ML\nand have been applied in a variety of contexts such as poisoning\nof neural network or recommender systems [14], [28]. To date, re-\nsearch on data poisoning has mostly been focused on classification\nand anomaly detection [ 3], [33], [4], while adversarial regression\nhas largely remained underrepresented [22].\nA gradient-based optimization framework for linear classifiers\nlike Lasso or Ridge Regression was first introduced by Xiao et al.\n[34]. Jagielski et al. [ 18] have built upon this work and also proposed\na defense mechanism against poisoning attacks called TRIM . An-\nother novel attack algorithm on regression learning was proposed\nby M√ºller et al. [ 29]. It works by manipulating the training dataset\nin a way that causes maximum disturbance of the data points. In\ntheir experimental evaluation, the authors were able to observe\nthat the Mean Squared Error ( MSE) of the regressor increased by\n150 percent after inserting only 2%of poisoned samples.\nPoisoning attacks on LISmodels differ significantly from previ-\nous attempts of poisoning linear regression models, because they\nrequire poisoning of the CDF. This task is challenging, because ev-\nery insertion affects the values of all points in the dataset. The first\nresearchers to study this new area of research were Kornaropou-\nlos et al., who formulated two poisoning attacks on the hierar-\nchical structure of the RMI model [ 20]. By leveraging the attacks\ndescribed in [ 20], the researchers were able to increase the error of\nthe poisoned RMI model by a factor of up to 300√ócompared to a\nnon-poisoned model. We expand on this initial idea and perform a\ncomprehensive empirical evaluation across a variety of models ( e.g.,\nSLR, LogTE, DLogTE, 2P, TheilSen, and LAD discussed in [ 9], as\nwell as ALEX [ 7] and PGM [ 11]), open-sourcing our ready-to-use\npoisoning benchmark to the research community.\n3 PRELIMINARIES\n3.1 Terminology\nIn this work we focus on poisoning attacks against LISmodels based\non Piecewise Linear Approximation ( PLA). To define the model,\nwe letùê∑={ùë•ùëñ,ùë¶ùëñ}ùëõ\nùëñ=1denote the data used by a learned index\nstructure, with ùë•‚ààRrepresenting the input data vector and ùë¶‚ààR\nrepresenting the output variable. In ordinary linear regression,\npredictions are made via a linear function ùëì(ùë•,ùëé,ùëè)=ùëéùëáùë•+ùëèwith\nparameters ùëé‚ààRùëëandùëè‚ààRchosen to minimize average loss\nL(ùê∑,ùëé,ùëè)=1\nùëõ√çùëõ\nùëñ=1(ùëì(ùë•ùëñ,ùëé,ùëè)‚àíùë¶ùëñ)2, also known as the Mean\nSquared Error.\nIn the context of our data poisoning attacks, a key is denoted\nbyùëòand its key universe (range of potential keys) as K, where\n|K|=ùëö. Similar to previous work on LISmodels, it is assumed that\nkeys are given as non-negative integer and that the total order of\nthe keyset can always be derived. It is further assumed that each\nkey is associated with a record and that the records are stored in\nan in-memory array that is sorted with respect to the key values.3.2 Adversarial Model\n3.2.1 Adversary‚Äôs Goals. When executing a poisoning attack against\naLISmodel, the adversary‚Äôs goal is to corrupt the learned index\nmodel during the training phase ( i.e., index construction), so that its\nperformance deteriorates during the test phase ( i.e., prediction of\nthe position of the given key). In this work, we focus on poisoning\navailability attacks , where the adversary‚Äôs goal is to deteriorate\nthe performance of a learning-based data structure. Specifically,\nthe objective of the adversary is to generate a small number of\npoisoning keys that are used to augment the training dataset that\nconsists of so-called legitimate keys . The assumption is that training\naLISmodel with both the poisoning keys andlegitimate keys will\nresult in a model whose performance is worse compared to a LIS\nmodel that is trained on only the legitimate keys .\n3.2.2 Adversary‚Äôs Capabilities & Knowledge. Data poisoning at-\ntacks usually distinguish between two attack scenarios: white-box\nandblack-box poisoning attacks.\nIn this work, we focus on white-box poisoning attacks where\nthe attacker is assumed to have full access to the training data, i.e.,\nthe keysetùêæand the slope and intercept parameters ùëéandùëèof the\nlinear regression [ 25], [3], [20]. When performing the attack, the\nadversary is able to inject up to ùëùmaliciously-crafted poisoning\nkeys into the training set prior to training the LISmodel. The total\nnumber of data points in the training set is given by ùëÅ=ùëõ+ùëù,\nwhereùëõdenotes the number of legitimate keys and ùëùthe number\nof poisoning keys in the training data. Similar to previous work on\npoisoning attacks, it is assumed that the adversary is able to control\nonly a tiny fraction of the training set limited by the poisoning\npercentageùõº, whereùõº=ùëù/ùëõ[18], [34].\nIf we were to adapt this attack to the black-box scenario where\nthe adversary has no direct access to the training data or training\nparameters of the model, the attacker would first need to infer the\nparameters of the model and subsequently use their estimates to\nperform the poisoning attack. Though more difficult to execute,\nblack-box attacks allow better transferability of poisoning attacks\nagainst different training sets, as shown in [34] and [28].\n3.2.3 Attack Evaluation Metric. In this research, the effect of poi-\nsoning the LISis measured with respect to the mean lookup time in\nnanoseconds. To measure the performance impact of the poisoning\nattack, we calculate the ratio of the mean lookup time of a model\nthat is trained on the legitimate data and the mean lookup time of\na model trained on the poisoned data. For completeness, we also\nreport the mean lookup time (in nanoseconds) across all poisoning\nthresholds for all the models considered.\n4 TESTING THE ROBUSTNESS OF LEARNED\nINDEX STRUCTURES\nTo test the robustness of learned index structures, we execute a\npoisoning attack on linear regression models by attacking the CDF\nof the training data. The attack works by inserting a certain number\nofpoisoning keys into the training dataset with the aim of increasing\nthe approximation error of the regression and thus deteriorating\nthe overall performance of the index.\nTo formulate the poisoning attack, we consider a LISto consist\nof an index that is being constructed on a keyset ùêæof sizeùëõ, where\n\nTesting the Robustness of Learned Index Structures Conference‚Äô17, July 2017, Washington, DC, USA\neach keyùëò‚ààùêæhas a rankùëüin the interval[1,ùëõ]. Here,ùëüdenotes\nthe position of ùëòin an ordered sequence of ùêæ. The objective of the\nLISis to approximate the rank of the queried key by constructing a\nregression model on (ùëò,ùëü), where the X-value is given by the key\nùëò, and the Y-value is denoted by the rank ùëü. In other words, the\nfunction that the regression model approximates is the CDF of the\ninput dataset.\nPrior work on poisoning attacks on linear regression models was\naimed at inserting maliciously-crafted poisoning keys that cause\na ‚Äúlocal change‚Äù, i.e., inserting keys that do not affect the X- and\nY-values of the legitimate points [ 18]. In the case of LISmodels, the\ninsertion of a single, maliciously-crafted key ùëòùëùwill cause a shift in\nthe rank of all keys larger than ùëòùëù. This change will in turn trigger\na shift of the CDF, thus compounding the effect of the adversarial\ninsertion.\nTo this date, the ‚Äúcompounding effect of adversarial insertion‚Äù\nhas only been studied by the authors of [ 20], where they intro-\nduced a novel poisoning attack for linear regression on CDFs called\nGreedyPoisoningRegressionCDF . We follow their approach and de-\nscribe the poisoning strategy of an adversary targeting linear regres-\nsion models on CDFs in Definition 4.1. The parameter ùúÜdenotes\nthe upper bound that limits the size of the poisoning keyset ùëÉ\nand is chosen to be proportional to the size of the keyset. In the\nexperiments described in Section 5.1, ùúÜwas set to a range of val-\nues between ùúÜ=0.01ùëõandùúÜ=0.20ùëõ. For further details on the\npoisoning algorithm, the interested reader is referred to [20].\nDefinition 4.1 ( Poisoning Linear Regression on CDF ).Letùêæ\nbe the set of ùëõintegers that correspond to the keys and let ùëÉ\nbe the set of ùëùintegers that comprise the poisoning keys. The\naugmented set on which the linear regression model is trained\nis{(ùëò‚Ä≤\n1,ùëü‚Ä≤\n1),(ùëò‚Ä≤\n2,ùëü‚Ä≤\n2),¬∑¬∑¬∑,(ùëò‚Ä≤\nùëõ‚Ä≤,ùëü‚Ä≤\nùëõ‚Ä≤)}, whereùëò‚Ä≤\nùëñ‚ààùêæ‚à™ùëÉandùëü‚Ä≤\nùëñ‚àà\n[1,ùëõ+ùëù]. The goal of the adversary is to choose a set ùëÉof size at\nmostùúÜso as to maximize the loss function of the augmented set\nùêæ‚à™ùëÉ:\narg maxùëÉs.t.|ùëÉ|‚â§ùúÜ\u0012\nmin\nùëé,ùëèL\u0010\n{ùëò‚Ä≤\nùëñ,ùëü‚Ä≤\nùëñ}ùëõ+ùëù\nùëñ=1,ùëé,ùëè\u0011\u0013\n5 EXPERIMENTAL EVALUATION\n5.1 Experimental Setup\nTo test the robustness of learned index structures, we set-up a flexi-\nble microbenchmark that allows us to quickly test the robustness\nof learned index structures against data poisoning attacks. The\nmicrobenchmark is based on the source code that was published by\nEppert et al. [ 9]. We have extended the existing microbenchmark\nby implementing the CDF poisoning attack against different types\nof regression models and the learned index implementations ALEX\nandPGM -Index. The corresponding source code used in this work\nis available online.1\nThe system architecture that we used for our experiments is\nshown in Figure 1. To simulate database workload, we have first\ngenerated a synthetic dataset consisting of 1000 keys. We subse-\nquently executed the poisoning attack against the synthetic dataset\nwhile varying the poisoning threshold parameter from ùëù=0.01\n1https://github.com/Bachfischer/LogarithmicErrorRegression\nFigure 1: Benchmarking architecture used for experiments.\ntoùëù=0.20. For each poisoning threshold, we obtained the corre-\nsponding set of poisoning keys and used the legitimate and poisoned\nkeysets to measure the mean lookup time of the indexes on the\nlegitimate (non-poisoned) dataset and the mean lookup time on the\npoisoned dataset.\nTo test the robustness of learned index structures, we focused on\nLISmodels that approximate the CDF viaPLA and whose source\ncode is available as open-source. We have therefore decided to\ninclude ALEX ,PGM -Index, and the regression models2discussed\nin [9] into our benchmark. Additional details on the indexes are\nprovided in Table 2.\nMethod Description Parameters Source\nRegressions (SLR etc.) [9] - [10]\nALEX [7] - [26]\nPGM-Index [11] max. error ùúñ[12]\nTable 2: Overview of evaluated indexes.\n5.2 Experimental Results\nFigure 2 shows the results of the experiments. The performance\ndeterioration of the LISis calculated as the ratio between the mean\nlookup time in nanoseconds for the poisoned datasets and the mean\nlookup time for the legitimate (non-poisoned) dataset.\nFrom the graphs, we can observe that simple linear regression\n(SLR) is particularly prone to the poisoning attack, as this regression\nmodel shows a steep increase in the mean lookup time when evalu-\nated on the poisoned data. The performance of the competitors that\noptimize a different error function such as LogTE, DLogTE and 2P\nare more robust against adversarial attacks. For these regression\nmodels, the mean lookup time remains relatively stable even when\nthe poisoning threshold is increased substantially.\nBecause SLR is the de-facto standard in learned index struc-\ntures and used internally by the ALEX and the PGM -Index imple-\nmentations, we would expect that these two models also exhibit a\nrelatively high performance deterioration when evaluated on the\npoisoned dataset. Surprisingly, ALEX does not show any signifi-\ncant performance impact. This is most likely due to the usage of\ngapped arrays that allows the model to easily capture outliers in\nthe data (up to a certain point). The performance of the PGM-Index\ndeteriorates by a factor of up-to 1.3 √ó.\n2Regression models: SLR, LogTE, DLogTE, 2P, TheilSen, and LAD\n\nConference‚Äô17, July 2017, Washington, DC, USA Bachfischer, Borovica-Gajic, and Rubinstein\n(a) SLR\n (b) LogTE\n (c) DLogTE\n (d) 2P\n(e) TheilSen\n (f) LAD\n (g) ALEX\n (h) PGM\nFigure 2: Performance deterioration of LIS models under poisoning attacks.\nFigure 3: Mean lookup time avg. across all poisoning levels.\nFor further analysis, we have also calculated the overall mean\nlookup time for the evaluated learned indexes, averaged across all\nexperiments. The results are shown in Figure 3. From Figure 3, we\ncan observe that ALEX dominates all learned index structures. The\nperformance of the regression models SLR, LogTE, DLogTE, 2P,\nTheilSen and LAD is also relatively similar, in a range between 30 -\n40 nanoseconds, while the PGM-Index performs worst with a mean\nlookup time of > 50 nanoseconds.\n6 CONCLUSIONS AND FUTURE OUTLOOK\nIn this research, we have tested the robustness of a variety of re-\ngression models as well as two learned index implementations\nALEX andPGM -Index against adversarial workload. To simulate\nadversarial workload, we have executed a poisoning attack on a\nsynthetic dataset consisting of 1000 keys. To evaluate the success of\nthe poisoning attack, we measured the performance deterioration\nof the lookup time for various indexes. The results show that LIS\nmodels are prone to poisoning attacks and exhibit significantly\nworse performance after only a small subset of poisoning keys is\nintroduced into the keysets.The experiments described in this research focused exclusively\non poisoning models that try to approximate the CDF via linear\nregression. Recent publications have introduced other models for\nconstructing a LISsuch as polynomial interpolation [ 31] and loga-\nrithmic error regression [ 9]. These novel model architectures would\nalso provide an interesting target for poisoning attacks. While the\npoisoning attack used in this paper works by introducing poisoning\nkeys prior to training the index models, future research may inves-\ntigate how an adversary could leverage the update functionality of\ndynamic learned index models to insert and remove keys from a\ntrained LIS model during runtime to deteriorate the fit of the LIS.\nREFERENCES\n[1]Hussam Abu-Libdeh, Deniz Altƒ±nb√ºken, Alex Beutel, Ed H Chi, Lyric Doshi,\nTim Kraska, Andy Ly, and Christopher Olston. 2020. Learned Indexes for a\nGoogle-scale Disk-based Database. arXiv preprint arXiv:2012.12501 (2020).\n[2]Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. 2010. The\nsecurity of machine learning. Machine Learning 81, 2 (2010), 121‚Äì148.\n[3]Battista Biggio, Blaine Nelson, and Pavel Laskov. 2012. Poisoning attacks against\nsupport vector machines. arXiv preprint arXiv:1206.6389 (2012).\n[4]Battista Biggio and Fabio Roli. 2018. Wild patterns: Ten years after the rise of\nadversarial machine learning. Pattern Recognition 84 (2018), 317‚Äì331.\n[5]Antonio Boffa, Paolo Ferragina, and Giorgio Vinciguerra. 2021. A ‚ÄúLearned‚Äù\nApproach to Quicken and Compress Rank/Select Dictionaries. In 2021 Proceedings\nof the Workshop on Algorithm Engineering and Experiments (ALENEX) . SIAM,\n46‚Äì59.\n[6]Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth,\nAndrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau. 2020. From wisckey to bour-\nbon: A learned index for log-structured merge trees. In 14th USENIX Symposium\non Operating Systems Design and Implementation (OSDI 20) . 155‚Äì171.\n[7]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, and Donald Kossmann.\n2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data . 969‚Äì984.\n[8]Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A learned multi-dimensional index for correlated data and skewed\nworkloads. arXiv preprint arXiv:2006.13282 (2020).\n[9]Martin Eppert, Philipp Fent, and Thomas Neumann. 2021. A Tailored Regression\nfor Learned Indexes: Logarithmic Error Regression. (2021).\n[10] Martin Eppert, Philipp Fent, and Thomas Neumann. 2021. A Tailored Regression\nfor Learned Indexes: Logarithmic Error Regression . https://github.com/umatin/\nLogarithmicErrorRegression\n[11] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proceedings of the\nVLDB Endowment 13, 10 (2020), 1162‚Äì1175.\n\nTesting the Robustness of Learned Index Structures Conference‚Äô17, July 2017, Washington, DC, USA\n[12] Paolo Ferragina and Giorgio Vinciguerra. 2021. PGM-index: State-of-the-art\nlearned data structure . https://github.com/gvinciguerra/PGM-index\n[13] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-aware Index Structure. , 1189‚Äì1206 pages.\nhttps://doi.org/10.1145/3299869.3319860\n[14] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild,\nDawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. 2020. Data Security\nfor Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses. arXiv\npreprint arXiv:2012.10544 (2020).\n[15] Ali Hadian and Thomas Heinis. 2019. Interpolation-friendly B-trees: Bridging\nthe Gap Between Algorithmic and Learned Indexes. (2019).\n[16] Darryl Ho, Jialin Ding, Sanchit Misra, Nesime Tatbul, Vikram Nathan, Vasimuddin\nMd, and Tim Kraska. 2019. LISA: towards learned DNA sequence search. arXiv\npreprint arXiv:1910.04728 (2019).\n[17] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and\nJ Doug Tygar. 2011. Adversarial machine learning. In Proceedings of the 4th ACM\nworkshop on Security and artificial intelligence . 43‚Äì58.\n[18] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru,\nand Bo Li. 2018. Manipulating machine learning: Poisoning attacks and coun-\ntermeasures for regression learning. In 2018 IEEE Symposium on Security and\nPrivacy (SP) . IEEE, 19‚Äì35.\n[19] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management . 1‚Äì5.\n[20] Evgenios M Kornaropoulos, Silei Ren, and Roberto Tamassia. 2020. The Price of\nTailoring the Index to Your Data: Poisoning Attacks on Learned Index Structures.\narXiv preprint arXiv:2008.00297 (2020).\n[21] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 International\nConference on Management of Data . 489‚Äì504.\n[22] Chang Liu, Bo Li, Yevgeniy Vorobeychik, and Alina Oprea. 2017. Robust lin-\near regression against training data poisoning. In Proceedings of the 10th ACM\nWorkshop on Artificial Intelligence and Security . 91‚Äì102.\n[23] Stephen Macke, Alex Beutel, Tim Kraska, Maheswaran Sathiamoorthy,\nDerek Zhiyuan Cheng, and EH Chi. 2018. Lifting the curse of multidimen-\nsional data with learned existence indexes. In Workshop on ML for Systems at\nNeurIPS .[24] Ryan Marcus, Emily Zhang, and Tim Kraska. 2020. CDFShop: Exploring and\nOptimizing Learned Index Structures. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data . 2789‚Äì2792.\n[25] Shike Mei and Xiaojin Zhu. 2015. Using machine teaching to identify optimal\ntraining-set attacks on machine learners. In Twenty-Ninth AAAI Conference on\nArtificial Intelligence .\n[26] Microsoft. 2021. ALEX - A library for building an in-memory, Adaptive Learned\nindEX . https://github.com/microsoft/ALEX\n[27] Michael Mitzenmacher. 2019. A model for learned bloom filters, and optimizing\nby sandwiching. arXiv preprint arXiv:1901.00902 (2019).\n[28] Luis Mu√±oz-Gonz√°lez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin\nWongrassamee, Emil C Lupu, and Fabio Roli. 2017. Towards poisoning of deep\nlearning algorithms with back-gradient optimization. In Proceedings of the 10th\nACM Workshop on Artificial Intelligence and Security . 27‚Äì38.\n[29] Nicolas M√ºller, Daniel Kowatsch, and Konstantin B√∂ttinger. 2020. Data Poisoning\nAttacks on Regression Learning and Corresponding Defenses. In 2020 IEEE 25th\nPacific Rim International Symposium on Dependable Computing (PRDC) . IEEE,\n80‚Äì89.\n[30] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning multi-dimensional indexes. In Proceedings of the 2020 ACM SIGMOD Interna-\ntional Conference on Management of Data . 985‚Äì1000.\n[31] Naufal Fikri Setiawan, Benjamin IP Rubinstein, and Renata Borovica-Gajic. 2020.\nFunction interpolation for learned index structures. In Australasian Database\nConference . Springer, 68‚Äì80.\n[32] Chuzhe Tang, Zhiyuan Dong, Minjie Wang, Zhaoguo Wang, and Haibo Chen.\n2019. Learned indexes for dynamic workloads. arXiv preprint arXiv:1902.00655\n(2019).\n[33] Sanli Tang, Xiaolin Huang, Mingjian Chen, Chengjin Sun, and Jie Yang. 2019. Ad-\nversarial attack type i: Cheat classifiers by significant changes. IEEE transactions\non pattern analysis and machine intelligence (2019).\n[34] Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and\nFabio Roli. 2015. Is feature selection secure against training data poisoning?. In\ninternational conference on machine learning . PMLR, 1689‚Äì1698.\n[35] Zhou Zhang, Pei-Quan Jin, Xiao-Liang Wang, Yan-Qi Lv, Shou-Hong Wan, and\nXi-Ke Xie. 2021. COLIN: A Cache-Conscious Dynamic Learned Index with High\nRead/Write Performance. Journal of Computer Science and Technology 36, 4 (2021),\n721‚Äì740.",
  "textLength": 27491
}