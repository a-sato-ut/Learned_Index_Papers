{
  "paperId": "87ec24cce780c28b19047e53b47cb414c1d06202",
  "title": "On Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing",
  "pdfPath": "87ec24cce780c28b19047e53b47cb414c1d06202.pdf",
  "text": "On Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\nSepanta Zeighami1Cyrus Shahabi1\nAbstract\nA fundamental problem in data management is\nto find the elements in an array that match a\nquery. Recently, learned indexes are being ex-\ntensively used to solve this problem, where they\nlearn a model to predict the location of the items\nin the array. They are empirically shown to outper-\nform non-learned methods (e.g., B-trees or binary\nsearch that answer queries in O(logn)time) by\norders of magnitude. However, success of learned\nindexes has not been theoretically justified. Only\nexisting attempt shows the same query time of\nO(logn), but with a constant factor improvement\nin space complexity over non-learned methods,\nunder some assumptions on data distribution. In\nthis paper, we significantly strengthen this result,\nshowing that under mild assumptions on data\ndistribution, and the same space complexity as\nnon-learned methods, learned indexes can answer\nqueries in O(log log n)expected query time. We\nalso show that allowing for slightly larger but still\nnear-linear space overhead, a learned index can\nachieve O(1)expected query time. Our results\ntheoretically prove learned indexes are orders of\nmagnitude faster than non-learned methods, theo-\nretically grounding their empirical success.\n1. Introduction\nIt has been experimentally observed, but with little theoret-\nical backing, that the problem of finding an element in an\narray has very efficient learned solutions (Galakatos et al.,\n2019; Kraska et al., 2018; Ferragina & Vinciguerra, 2020;\nDing et al., 2020). In this fundamental problem in data\nmanagement, the goal is to find, given a query, the elements\nin the dataset that match the query (e.g., find the student\nwith grade= q, for a number q, where “grade= q” is the query\non a dataset of students). Assuming the query is on a single\n1Univerisity of Southern California. Correspondence to:\nSepanta Zeighami <zeighami@usc.edu >, Cyrus Shahabi <sha-\nhabi@usc >.\nProceedings of the 40thInternational Conference on Machine\nLearning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).attribute (e.g., we filter students only based on grade), and\nthat data is sorted based on this attribute, binary search finds\nthe answer in O(logn)for an ordered dataset with nrecords.\nExperimental results, however, show that learning a model\n(called a learned index (Kraska et al., 2018)) that predicts\nthe location of the query in the array can provide accurate\nestimates of the query answers orders of magnitude faster\nthan binary search (and other non-learned approaches). The\ngoal of this paper is to present a theoretical grounding for\nsuch empirical observations.\nMore specifically, we are interested in answering exact\nmatch and range queries over a sorted array A. Exact match\nqueries ask for the elements in Aexactly equal to the query\nq(e.g., grade= q), while range queries ask for elements that\nmatch a range [q, q′](e.g., grade is between qandq′). Both\nqueries can be answered by finding the index of the largest\nelement in Athat is smaller than or equal to q, which we call\nthe rank of q,rank(q). Range queries require the extra step,\nafter obtaining rank (q), of scanning the array sequentially\nfromqup to q′to obtain all results. The efficiency of meth-\nods answering range and exact match queries depends on\nthe efficiency of answering rank (q), which is the operation\nanalyzed in the rest of this paper.\nIn the worst-case, and without further assumption on the\ndata, binary search finds rank (q)optimally, and in O(logn)\noperations. Materializing the binary search tree and vari-\nations of it, e.g., B-Tree (Bayer & McCreight, 1970) and\nCSS-trees (Rao & Ross, 1999), utilize caching and hard-\nware properties to improve the performance in practice but\ntheoretical number of operations remains O(logn)(we con-\nsider data in memory and not external storage). On the other\nhand, learned indexes have been empirically shown to out-\nperform non-learned methods by orders of magnitude. Such\napproaches learn a model that predicts rank (q). At query\ntime, a model inference provides an estimate of rank (q), and\na local search is performed around the estimate to find the\nexact index. An example is shown in Fig. 1, where for the\nquery 13, the model returns index 3, while the correct index\nis 5. Then, assuming the maximum model error is ϵ, a binary\nsearch on ϵelements of Awithin the model prediction (i.e.,\nthe purple sub-array in Fig. 1) finds the correct answer. The\nsuccess of learned models is attributed to exploiting patterns\nin the observed data to learn a small model that accurately\nestimates the rank of a query in the array.\n1arXiv:2306.10651v1  [cs.DB]  19 Jun 2023\n\nOn Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\nI nde x5634012= 135101520241011121721a t mostmax err .modelestima te c orr e c t inde xr ank(   )le arne d inde x elemen tsF ilter wi th \nbinar y \nse ar ch\nFigure 1. A learned index used to solve the rank problem.\nHowever, to date, no theoretical result has justified their\nsuperior practical performance. Ferragina & Vinciguerra\n(2020) shows a worst-case bound of O(logn)on query time,\nthe same as traditional methods, but experimentally shows\norders of magnitude difference. The only existing result\nthat shows any theoretical benefit to learned indexing is Fer-\nragina et al. (2020), that shows constant factor better space\nutilization while achieving O(logn)query time under some\nassumptions on data distribution. The question remains\nwhether theoretical differences, beyond merely constant\nfactors, exist between learned and traditional approaches.\nWe answer this question affirmatively. We show that\n(i)Using the same space overhead as traditional indexes\n(e.g., a B-tree), and under mild assumptions on the\ndata distribution, a learned index can answer queries in\nO(log log n)operations on expectation, a significant\nand asymptotic improvement over the O(logn)of\ntraditional indexes;\n(ii)With the slightly higher but still near-linear space con-\nsumption O(n1+ϵ), for any ϵ >0, a learned index can\nachieve O(1)expected query time; and\n(iii) Under stronger assumptions on data distribution, we\nshow that O(log log n)expected query time is also\npossible with O(1)space overhead ( O(1)space over-\nhead is similar to performing binary search without\nbuilding any auxiliary data structure).\nWe present experiments showing these asymptotic bounds\nare achieved in practice.\nThese results show order of magnitude benefit in terms\nof expected query time, where the expectation is over the\nsampling of the data, and not worst-case query time (which,\nunsurprisingly, is O(logn)in all cases). Intuitively, this\nmeans that although there may exist data instances where\na learned index is as slow as binary search, for many data\ninstances (and on expectation), it is fast and sub-logarithmic.\nAnalyzing expected query time allows us to incorporate\nproperties of the data distribution. Our results hold assuming\ncertain distribution properties: query time in (i) and (ii)\nis achieved assuming bounded p.d.f of data distribution\n((i) also assumes non-zero p.d.f), while (iii) assumes the\nc.d.f of data distribution is efficiently computable. Overall,\ndata distribution had been previously hypothesized to be an\nimportant factor on the performance of a learned index (e.g.,\nKraska et al. (2018)). This paper shows how such properties\ncan be used to analyze the performance of a learned index.2. Preliminaries and Related Work\n2.1. Problem Definition\nSetup . We are given an array A⊆ Dn, consisting of nele-\nments, where D ⊆Ris the domain of the elements. Unless\notherwise stated, assume D= [0,1]; we discuss extensions\nto other bounded or unbounded domains in Sec. 3.4. Ais\nsorted in ascending order, where airefers to the i-th ele-\nment in this sorted order and A[i:j]denotes the sorted\nsubarray containing {ai, ...,aj}. We assume Ais created\nby sampling ni.i.d random variables and then sorting them,\nwhere the random variables follow a continuous distribution\nχ, with p.d.f fχand c.d.f Fχ. We use the notation A∼χto\ndescribe the above sampling procedure.\nRank Problem . Our goal is to answer the rank problem :\ngiven the array Aand a query q, return the index i∗=Pn\ni=1IA[i]≤q, where Iis the indicator function. i∗is the\nindex of the largest element no greater than qand is 0 if no\nsuch element exists. Furthermore, if q∈A,qwill be at\nindex i∗+ 1. We define the rank function of an array Aas\nrA(q) =Pn\ni=1IA[i]≤q. The rank function takes a query as\nan input and outputs the answer to the rank problem. We\ndrop the dependence on Aif it is clear from context and\nsimply use r(q).\nThe rank problem is therefore the problem of designing a\ncomputationally efficient method to evaluate the function\nr(q). Let ˆRA(q;θ)be a function approximator, with param-\neters θthat correctly evaluates r(q). The parameters θof\nˆRAare found at a preprocessing step and are used to per-\nform inference at query time. Let T(ˆRA, q)be the number\nof operations performed by ˆRAto answer the query qand\nletS(ˆRA)be the space overhead of ˆRA, i.e., the number\nof bits required to store θ(note that S(ˆRA)does not in-\nclude storage required for the data itself, but only considers\nthe overhead of indexing). We study the expected query\ntime of any query qasEA∼χ[T(ˆRA, q)], and the expected\nspace overhead asEA∼χ[S(ˆRA)]. In our analysis of space\noverhead, we assume integers are stored using their binary\nrepresentation so that kintegers that are at most Mare\nstored in O(klogM)bits (i.e., assuming no compression).\nLearned indexing . Alearned indexing approach solves\nthe rank problem as follows. A function approximator (e.g.,\nneural network or a piecewise linear approximator) ˆrA(q;θ)\nis first learned that approximates rup to an error ϵ, i.e.,\n∥ˆrA−r∥∞≤ϵ. Then, at the step called error correction , an-\nother function, h(i, q), takes the estimate i= ˆrA(q;θ)and\ncorrects the error, typically by performing a binary search\n(or exponential search when ϵis not known a priori (Ding\net al., 2020)) on the array, A. That is, given that the estimate\nˆrAis within ϵof the true index of qinA, a binary search\non the 2ϵelement in Athat are within ϵofˆrA(q;θ)finds\nthe correct answer. Letting ˆRA(q;θ) =h(ˆrA(q, θ), q), we\nobtain that for any function approximator, ˆrAwith non-zero\n2\n\nOn Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\nerrorϵ, we can obtain an exact function with expected query\ntime of EA∼χ[T(ˆrA, q)] +O(logϵ)and space overhead of\nEA∼χ[S(ˆrA)]since binary search requires no additional\nstorage space. In this paper, we show the existence of func-\ntion approximators, ˆRAthat can achieve sub-logarithmic\nquery time with various space overheads.\n2.2. Related Work\nLearned indexes . The only existing work theoretically\nstudying a learned index is Ferragina et al. (2020). It shows,\nunder assumptions on the gaps between the keys in the ar-\nray, as n→ ∞ and almost surely, one can achieve logarith-\nmic query time with a learned index with a constant factor\nimprovement in space consumption over non-learned in-\ndexes. We significantly strengthen this result, showing sub-\nlogarithmic expected query time under various space over-\nheads. Our assumptions are on the data distribution itself\nwhich is more natural than assumption on the gaps, and our\nresults hold for any n(and not as n→ ∞ ). Though scant\nin theory, learned indexes have been extensively utilized\nin practice, and various modeling choices have been pro-\nposed under different settings, e.g., Galakatos et al. (2019);\nKraska et al. (2018); Ferragina & Vinciguerra (2020); Ding\net al. (2020) to name a few. Our results use a hierarchical\nmodel architecture, similar to Recursive Model Index (RMI)\n(Kraska et al., 2018) and piecewise approximation similar\nto Piecewise Geometric Model index (PGM) (Ferragina &\nVinciguerra, 2020) to construct function approximators with\nsub-logarithmic query time.\nNon-Learned Methods . Binary search trees, B-Trees\n(Bayer & McCreight, 1970) and many other variants (Rao\n& Ross, 1999; Lehman & Carey, 1985; Bayer, 1972), exist\nthat solve the problem in O(logn)query time, which is the\nbest possible in the worst case in comparison based model\n(Navarro & Rojas-Ledesma, 2020). The space overhead for\nsuch indexes is O(nlogn)bits, as they have O(n)nodes\nand each node can be stored in O(logn)bits. We also note\nin passing that if we limit the domain of elements to a finite\ninteger universe and do not consider range queries, various\nother time/space trade-offs are possible (P ˘atras ¸cu & Thorup,\n2006), e.g., using hashing (Fredman et al., 1984).\n3. Asymptotic Behaviour of Learned Indexing\n3.1. Constant Time and Near-Linear Space\nWe first consider the case of constant query time.\nTheorem 3.1. Suppose the p.d.f, fχ(x), is bounded, i.e.,\nfχ(x)≤ρfor all x∈ D, where ρ <∞. There exists a\nlearned index with space overhead O(ρ1+ϵn1+ϵ), for any\nϵ >0, with expected query time of O(log1\nϵ)operations for\nany query. ρis a constant independent of n, and for any\nconstant ϵ, asymptotically in n, space overhead is O(n1+ϵ)\nand expected query time is O(1).\nThe theorem shows the surprising result that we can in\nfact achieve constant query time with a learned index of\nI nde x0635101520max err .c onstan tFigure 2. Approximation with a\npiecewise constant function\nI nde x0635101520max err .r e cursiv el y se ar chFigure 3. Approximation with\nc.d.f\nsizeO(n1+ϵ). Although the space overhead is near-linear,\nthis overhead is asymptotically larger than the overhead of\ntraditional indexes (with overhead O(nlogn)) and thus the\nquery time complexities are not directly comparable.\nInterestingly, the function approximator that achieves the\nbound in Theorem 3.1 is a simple piecewise constant func-\ntion approximator, which can be seen as a special case of\nthe PGM model that uses piece-wise linear approximation\n(Ferragina & Vinciguerra, 2020). Our function approxima-\ntor is constructed by uniformly dividing the space into k\nintervals and for each interval finding a constant that best\napproximates the rank function in that interval. Such a\nfunction approximator is shown as ˆrA(q;θ)in Fig. 2 for\nk= 5. Obtaining constant query time requires such a func-\ntion approximator to have constant error. It is, however,\nnon-obvious why and when only O(n1+ϵ)pieces will be\nsufficient on expectation to achieve constant error. In fact,\nfor the worst-case (and not the expected case), for a heavily\nskewed dataset, achieving constant error would require an\narbitrarily large k, as noted by Kraska et al. (2018).\nHowever, Theorem 3.1 shows as long as the p.d.f. of the data\ndistribution is bounded, O(n1+ϵ)pieces will be sufficient\nfor constant query time on expectation. Intuitively, the\nbound on the p.d.f. is used to argue that the number of data\npoints sampled in a small region is not too large, which is in\nturn used to bound the error of the function approximation.\nFinally, dependence on ρin Theorem 3.1 is expected, as\nperformance of learned indexes depends on the dataset char-\nacteristics. ρcaptures such data dependencies, showing that\nsuch data dependencies only affect space overhead by a con-\nstant factor. From a practical perspective, our experiments\nin Sec. 5.2 show that for many commonly used real-world\nbenchmarks for learned indexes, trends predicted by Theo-\nrem 3.1 hold with ρ= 1. However, Sec. 5.2 also shows that\nfor datasets where learned indexes are known to perform\npoorly, we observe large values of ρ. Thus, ρcan be used\nto explain why and when learned indexes perform well or\npoorly in practice.\n3.2. Log-Logarithmic Time and Constant Space\nRequiring constant query time, as in the previous theorem,\ncan be too restrictive. Allowing for slightly larger query\ntime, we have the following result.\nTheorem 3.2. Suppose c.d.f of data distribution Fχ(x)can\n3\n\nOn Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\nLev el 1M odel 1.1pie c e wise\nmodelM odel 2.1M odel 2.2M odel 3.3M odel 3.1M odel 3.2M odel 3.4M odel 3.5Lev el 2Lev el quer y\nEstima te d posi tion\nFigure 4. RMI of height log log nwith piecewise constant models\nbe evaluated exactly with O(1)operations and O(1)space\noverhead. There exists a learned index with space overhead\nO(1), where for any query q, the expected query time is\nO(log log n)operations.\nThe result shows that we can obtain O(log log n)query\ntime if the c.d.f of the data distribution is easy to compute.\nThis is the case for the uniform distribution (whose c.d.f\nis a straight line), or more generally any distribution with\npiece-wise polynomial c.d.f. In this regime, we only utilize\nconstant space, and thus our bound is comparable with per-\nforming a binary search on the array, which takes O(logn)\noperations, showing that the learned approach enjoys an\norder of magnitude theoretical benefit.\nOur model of the rank function is n×Fχ, where Fχis the\nc.d.f of the data distribution. As Fig. 3 shows, our search\nalgorithm proceeds recursively, at each iteration reducing\nthe search space by around√n. Intuitively, the√nis due\nto the Dvoretzky-Kiefer-Wolfowitz (DKW) bound (Massart,\n1990), which is used to show that with high probability the\nanswer to a query, qis within√nofnFχ(q). Reducing\nthe search space, s, by roughly√sat every level by re-\ncursively applying DKW, we obtain the total search time\nofO(log log n)(note that binary search only reduces the\nsearch space by a factor of 2at every iteration).\n3.3. Log-Logarithmic Time and Quasi-Linear Space\nFinally, we show that the requirement of Theorem 3.2 on\nthe c.d.f. is not necessary to achieve O(log log n)query\ntime, provided quasi-linear space overhead is allowed. The\nfollowing theorem shows that a learned index can achieve\nO(log log n)query time under mild assumptions on the data\ndistribution and utilizing quasi-linear space.\nTheorem 3.3. Suppose p.d.f of data distribution fχ(x)is\nbounded and more than zero, i.e., ρ1≤fχ(x)≤ρ2for all\nx∈ D, where ρ1>0andρ2<∞. There exists a learned\nindex with expected query time equal to O(log log n)oper-\nations and space overhead O(ρ2\nρ1nlogn), for any query.\nSpecifically,ρ2\nρ1is a constant independent of n, so that,\nasymptotically in n, space overhead is O(nlogn).\nThis regime takes space similar to data size, and is where\nmost traditional indexing approaches lie, e.g., binary treesand B-trees, where they need O(nlogn)storage (the logn\nis due to the number of bits needed to store each node\ncontent) and achieve O(logn)query time.\nThe learned index that achieves the bound in Theorem 3.3\nis an instance of the Recursive Model Index (RMI) (Kraska\net al., 2018). Such a learned index defines a hierarchy of\nmodels, as shown in Fig. 4. Each model is used to pick a\nmodel in the next level of the tree until a model in the leaf\nlevel is reached, whose prediction is the estimated position\nof the query in the array. Unlike RMI in (Kraska et al.,\n2018), its height or size of the model within each node is\nnot constant and set based on data size.\nIntuitively, the hierarchy of models is a materialization of\na search tree based on the recursive search used to prove\nTheorem 3.2. At any level of the tree, if the search space is\nselements (originally, s=n) a model is used to reduce the\nsearch space to roughly√s. It is however non-trivial why\nand when such a model should exist across all levels and how\nlarge the model should be. We use the relationship between\nthe rank function and the c.d.f (through DKW bound), and\nthe properties of the data distribution to show that a model\nof size around√sis sufficient with high probability. Note\nthat models at lower levels of the hierarchy approximate\nthe rank function only over subsets of the array, but with\nincreasingly higher accuracy. A challenge is to show that\nsuch an approximability result holds across all models and\nall subsets of the array, which is why a lower bound on the\np.d.f. is needed in this theorem.\nSimilar to ρin Theorem 3.1, ρ1andρ2capture data charac-\nteristics in Theorem 3.3, showing constant factor dependen-\ncies on the model size. Our experiments in Sec. 5.2 show\nthat for most commonly used real-world benchmarks for\nlearned indexes, trends predicted by Theorem 3.3 hold with\nρ2\nρ1= 1. However, Sec. 5.2 also shows that for datasets\nwhere learned indexes are known to perform poorly,ρ2\nρ1\nis large, so thatρ2\nρ1can be used to explain why and when\nlearned indexes perform well or poorly in practice.\n3.4. Distributions with Other Domains\nSo far, our results assume that the domain of data distribu-\ntion is [0, 1]. The result can be extended to distributions\nwith other bounded domains, [r, s]forr, s∈R,r < s , by\nstandardizing χasχ−r\ns−r. This transformation scales p.d.f\nofχbys−r. Note that scaling the p.d.f does not affect\nTheorem 3.3, since both ρ1andρ2will be scaled by s−r,\nyielding the same ratioρ2\nρ1. On the other hand, ρin Theo-\nrem 3.1 will be scaled by s−r. Overall, bounded domain\ncan be true in many scenarios, as the data can be from some\nphenomenon that is bounded, e.g., age, grade, data over a\nperiod of time. Next, we extend our results to distributions\nwith unbounded domains.\nLemma 3.4. Suppose a learned index, ˆR, achieves expected\n4\n\nOn Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\nquery time t(n)and space overhead s(n)on distributions\nwith domain [0,1]and bounded (and non-zero) p.d.f. There\nexists a learned index, ˆR′with expected query time t(n) + 1\nand space overhead O(s(n) logn)on any sub-exponential\ndistribution with bounded (and non-zero) p.d.f.\nCombining Lemma 3.4 with Theorems 3.1 and 3.3, our re-\nsults cover various well-known distributions, e.g., Gaussian,\nsquared of Gaussian and exponential distributions.\nProof of lemma 3.4 builds the known learned index for\nbounded domains on logndifferent bounded intervals. This\nachieves the desired outcome due to the tail behavior of\nsub-exponential distributions (i.e., for distributions with tail\nat most as heavy as exponential, see Vershynin (2018) for\ndefinition). The tail behaviour allows us to, roughly speak-\ning, assume that the domain of the function is O(logn),\nbecause observing points outside this range is unlikely. We\nnote that other distributions with unbounded domain can\nalso be similarly analyzed based on their tail behaviour, with\nheavier tails leading to higher space consumption.\n4. Proofs\nProofs of the theorems are all constructive. PCA Index\n(Sec. 4.1) proves Theorem 3.1, RDS algorithm proves The-\norem 3.2 and RDA Index proves Theorem 3.3. Without loss\nof generality, we assume the bounded domain Dis[0,1].\nThe proof for the unbounded domain case (i.e., Lemma 3.4)\nis deferred to Appendix A. Proof of technical lemmas stated\nthroughout this section can also be found in Appendix A.\n4.1. Proof of Theorem 3.1: PCA Index\nWe present and analyze Piece-wise Constant Approximator\n(PCA) Index that proves Theorem 3.1.\n4.1.1. A PPROXIMATING RANK FUNCTION\nWe show how to approximate the rank function rwith a\nfunction approximator ˆr. To achieve constant query time,\napproximation error should be a constant independent of n\nwith high probability, and we also should be able to evaluate\nˆrin constant time.\nLemma 4.1 shows these properties hold for a piece-wise\nconstant approximation to r. Such a function is presented\nin Alg. 1 (and an example was shown in Fig. 2). Alg. 1\nuniformly divides the function domain into kintervals, so\nthat the i-th constant piece is responsible for the interval\nIi= [i×1\nk,(i+ 1)×1\nk]. Since r(q)is a non-decreasing\nfunction, the constant with the lowest infinity norm error\napproximating roverIiis1\n2(r(i\nk) +r(i+1\nk))(line 6). Let\nˆrkbe the function returned by PCF( A,k,0,1).\nLemma 4.1. Under the conditions of Theorem 3.1 and for\nk≥n1+ϵρ1+ϵ\n2, the error of ˆrkis bounded as\nP(∥ˆrk−r∥∞≥2\nϵ+ 1)≤1\nn.\nProof of Lemma 4.1 . Let ei= supx∈Ii|ˆr(x;θ)−r(x)|beAlgorithm 1 PCA Index Construction\nRequire: A sorted array A, number of pieces k, approxi-\nmation domain lower and upper bounds landu\nEnsure: Piecewise constant approximation of rover[l, u]\n1:procedure PCF( A,k,l,u)\n2: P←array of length kstoring the pieces\n3: α←(u−l)\nk\n4: δ←0\n5: fori←0tokdo\n6: P[i]←1\n2(rA(l+αi) +rA(l+α(i+ 1))\n7: δcurr←\u00061\n2(rA(l+α(i+ 1))−rA(l+αi))\u0007\n8: δ←max{δ, δcurr}\nreturn P,δ\nthe maximum error in the i-th piece of ˆr.eican be bounded\nby the number of points sampled in Iias follows.\nProposition 4.2. Letsi=|{j|aj∈Ii}|be the number of\npoints in Athat are in Ii. We have ei≤si\nUsing Prop. 4.2, we have ∥ˆr−r∥∞≤max i∈{1,...,k}si.\nProp. 4.2 is a simple fact that relates approximation er-\nror to statistical properties of data distribution. Define\nsmax= max i∈{1,...,k}siand observe that smaxis a random\nvariable denoting the maximum number of points sampled\nper interval, across kequi-length intervals. The following\nlemma shows that we can bound smax with a constant and\nwith probability1\nn, as long as kis near-linear in n.\nLemma 4.3. For any cwith c≥3, and if k≥\nn1+2\nc−1ρ1+1\nc−1we have P(smax≥c)≤1\nn.\nSetting c=2\nϵ+ 1, we see k≥n1+ϵρ1+ϵ\n2holds, so that\nLemma 4.3 together with Prop. 4.2 prove Lemma 4.1.\n4.1.2. I NDEX CONSTRUCTION AND QUERYING\nLetk=⌈n1+ϵ\n2ρ1+ϵ\n4⌉. We use PCF( A,k,0,1)to obtain\nˆrkandδ, where δis the maximum observed approximation\nerror. As Alg. 1 shows, ˆrkcan be stored as an array, P, with\nkelements. To perform a query, the interval, i, a query falls\ninto is calculated as i=⌊qk⌋and the constant responsible\nfor that interval, P[i], returns the estimate. Given maximum\nerrorδ, we perform a binary search on the subarray A[l:u],\nforl=P[i]−δandu=P[i] +δto obtain the answer.\n4.1.3. C OMPLEXITY ANALYSIS\nPhasO(n1+ϵ\n2)entries, and each can be stored in O(nϵ\n2).\nThus, total space complexity is O(n1+ϵ). Regarding query\ntime, the number of operations needed to evaluate ˆrkis\nconstant. Thus, the total query time of the learned index is\nO(logδ). Lemma 4.1 bounds δ, so that the query time for\nany query is at most log(4\nϵ+1) with probability at least 1−1\nn\nand at most lognwith probability at most1\nn. Consequently,\nthe expected query time is at most O(log(4\nϵ+ 1)×(1−\n1\nn) + log n×1\nn)which is O(1)for any constant ϵ >0.\n5\n\nOn Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\n4.2. Proof of Theorem 3.2: RDS Algorithm\nWe present and analyze Recursive Distribution Search (RDS)\nAlgorithm that proves Theorem 3.2.\n4.2.1. A PPROXIMATING RANK FUNCTION\nWe approximate the rank function using the c.d.f of the data\ndistribution, which conditions of Theorem 3.2 imply is easy\nto compute. As noted by Kraska et al. (2018), rank (q) =\nnFn(q), where Fnis the empirical c.d.f. Using this together\nwith DKW bound (Massart, 1990), we can establish that\nrank(q) is within error√nofnFχwith high probability.\nHowever, error of√nis too large: error correction to find\nrank(q)would require O(log√n) =O(logn)operations.\nInstead, we recursively improve our estimate by utilizing\ninformation that becomes available from observing elements\nin the array. After observing two elements, aiandajinA\n(i < j ), we update our knowledge of the distribution of\nelements in A[i+ 1 : j−1]as follows. Define Fi,j\nχ(x) =\nFχ(x)−Fχ(ai)\nFχ(aj)−Fχ(ai). Informally, any element XinA[i+1 :j−1]\nis a random variable sampled from χand knowing the value\nofaiandajimplies that X∈[ai, aj], so that the conditional\nc.d.f of Xis\nP\nX∼χ(X≤x|ai≤X≤aj) =Fi,j\nχ(x).\nWe then use DKW bound to show Fi,j\nχis a good estimate\nof the rank function for the subarray A[i+ 1 : j−1],\ndefining the rank function for the subarray A[i+1 : j−1]as\nri,j(q) =Pj−1\nz=i+1Iaz≤q. Formally, the following lemma\nshows that given observations A[i] =aiandA[j] =ajthe\nelements of A[i+ 1 : j−1]are i.i.d random variables with\nthe conditional c.d.f Fi,j\nχ(x)and uses the DKW bound to\nbound the approximation error of using the conditional c.d.f\nto approximate the conditional rank function.\nLemma 4.4. Consider two indexes i, j, where 1≤i < j≤\nnandai< aj. Letk=j−i−1. For k≥2, we have\nP(sup\nx|ri,j(x)−kFi,j\nχ(x)| ≥p\n0.5klog log k)≤1\nlogk.\n4.2.2. Q UERYING\nWe use Lemma 4.4 to recursively search the array. At every\niteration, the search is over a subarray A[i:j](initially, i=1\nandj=n). We observe the values of aiandajand use\nLemma 4.4 to estimate which subarray is likely to contain\nthe answer to the query. This process is shown in Alg. 2.\nIn lines 5-7 the algorithm observes aiandajand attempts\nto answer the query based on those two observations. If it\ncannot, lines 8-11 use Lemma 4.4 and the observed values\nofaiandajto estimate which subarray contains the answer.\nLine 12 then checks if the estimated subarray is correct,\ni.e., if the query does fall inside the estimated subarray. If\nthe estimate is correct, the algorithm recursively searches\nthe subarray. Otherwise, the algorithm exits and performs\nbinary search on the current subarray. Finally, line 3 exits\nwhen the size of the dataset is too small. The constant 25 is\nchosen for convenience of analysis (see Sec. 4.2.3).Algorithm 2 Recursive Distribution Search Algorithm\nRequire: A sorted array Aof size nsearched from index i\ntoj, a query q\nEnsure: Rank of qinA[i:j]\n1:procedure SEARCH (A,q,i,j)\n2: k←j−i−1\n3: ifk <25then\n4: return i-1+B INARY SEARCH (A,q,i,j)\n5: ifai> qthen return 0\n6: ifai=qthen return 1\n7: ifaj≤qthen return j−i+ 1\n8: ˆi←i+ 1 + k×Fi,j\nχ(q)\n9: r←√0.5klog log k\n10: l← ⌊ˆi−r⌋\n11: u← ⌈ˆi+r⌉\n12: ifal> qorar< qthen\n13: return i−1+BINARY SEARCH (A,q,i,j)\n14: return l−1+SEARCH (A,q,l,u)\n4.2.3. C OMPLEXITY ANALYSIS\nTo prove Theorem 3.2, it is sufficient to show that expected\nquery time of Alg. 2 is O(log log n)for any query. The\nalgorithm recursively proceeds. At each recursion level, the\nalgorithm performs a constant number of operations unless\nit exits to perform a binary search. Let the depth of recursion\nbehand let kibe the size of the subarray at the i-th level of\nrecursion (so that binary search at i-th level takes O(logki)).\nLetBidenote the event that the algorithm exits to perform\nbinary search at the i-th iteration. Thus, for any query q, the\nexpected number of operations is\nEA∼χ[T(ˆr, q)] =hX\ni=1c1+c2P(Bi,¯Bi−1, ....¯B1) logki\nfor constants c1andc2. Note that P(Bi,¯Bi−1, ....¯B1)≤\nP(Bi|¯Bi−1, ....¯B1), where P(Bi|¯Bi−1, ....¯B1)is the prob-\nability that the algorithm reaches i-th level of recursion and\nexits. By Lemma 4.4, this probability bounded by1\nlogki.\nThusEA∼χ[T(ˆr, q)]isO(h).\nTo analyze the depth of recursion, recall that at the last\nlevel, the size of the array is at most 25. Furthermore, at\nevery iteration the size of the array is reduced to at most\n2√0.5nlog log n+ 2. Forn≥25,2√0.5nlog log n+ 2≤\nn3\n4, so that the size of the array at the i-th recursions is at\nmostn(3\n4)iand the depth of recursion is O(log log n). Thus,\nthe expected total time is O(log log n) .\n4.3. Proof of Theorem 3.3: RDA Index\nWe present and analyze Recursive Distribution Approxima-\ntor (RDA) Index that proves Theorem 3.3.\n4.3.1. A PPROXIMATING RANK FUNCTION\nWe use ideas from Theorems 3.1 and 3.2 to approximate\nthe rank function. We use Alg. 2 as a blueprint, but instead\n6\n\nOn Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\nof the c.d.f, we use a piecewise constant approximation\nto the rank function. If we can efficiently approximate\nthe rank function for subarray A[i−1 :j+ 1],ri,j, to\nwithin accuracy O(√klog log k)where k=j−i−1,\nwe can merely replace line 8 of Alg. 2 with our function\napproximator and still enjoy the O(log log n)query time.\nIndeed, the following lemma shows that this is possible\nusing the piecewise approximation of Alg. 1 and under mild\nassumptions on the data distribution. Let ˆri,j\ntbe the function\nreturned by PCF( A[i+ 1 : j−1],t,ai,aj) with tpieces.\nLemma 4.5. Consider two indexes i, j, where 1≤i < j≤\nnandai< aj. Let k=j−i−1. For k≥2, under the\nconditions of Theorem 3.3 and for t≥ρ2\nρ1√\nkwe have\nP(∥ri,j−ˆri,j\nt∥∞≥(p\n0.5 log log k+ 1)√\nk)≤1\nlogk.\nProof of Lemma 4.5 . Alg. 1 finds the piecewise constant\napproximator to ri,jwithtpieces with the smallest infinity\nnorm error. Thus, we only need to show the existence of an\napproximation with tpieces that satisfies conditions of the\nlemma. To do so, we use the relationship between ri,jand\nthe conditional c.d.f. Intuitively, Lemma 4.4 shows that ri,j\nand the conditional c.d.f are similar to each other and thus,\nif we can approximate conditional c.d.f well, we can also\napproximate ri,j. Formally, by triangle inequality and for\nany function approximator ˆrwe have\n∥ri,j−ˆr∥∞≤ ∥ri,j−kFi,j\nχ∥∞+∥kFi,j\nχ−ˆr∥∞.(1)\nCombining this with Lemma 4.4 we obtain\nP(∥ri,j−ˆr∥∞≥p\n0.5klog log k+∥kFi,j\nχ−ˆr∥∞)≤1\nlogk.\nFinally, Lemma 4.6 stated below shows how we can approx-\nimate the conditional c.d.f and completes the proof.\nLemma 4.6. Under the conditions of Lemma 4.5, there\nexists a piecewise constant function approximator, ˆr, with\nρ2\nρ1√\nkpieces such that ∥ˆr−kFi,j\nχ∥∞≤√\nk.\n4.3.2. I NDEX CONSTRUCTION AND QUERYING\nLemma 4.5 is an analog of Lemma 4.4, showing a function\napproximator enjoys similar properties as the c.d.f. How-\never, different function approximators are needed for every\nsubarray (for c.d.f.s we merely needed to scale and shift\nthem differently for different subarrays). Given that there\nareO(n2)different subarrays, a naive implementation that\ncreates a function approximator for each subarray takes\nspace quadratic in data size. Instead, we only approximate\nthe conditional rank function for certain sub-arrays while\nstill retaining the O(√klog log k)error bound per subarray.\nConstruction . Note that r(q) = 0 only if q < a 1, so we\ncan filter this case out and assume r(q)∈ {1, ..., n}. RDA\nis a tree, shown in Fig. 4, where each node is associated\nwith a model. When querying the index, we traverse the tree\nfrom the root, and at each node, we use the node’s model to\nchoose the next node to traverse. Traversing down the treeAlgorithm 3 RDA Index Construction\nRequire: A sorted array Aof size nsampled from a distri-\nbution χwith CDF Fχ, a query q\nEnsure: The root node of the learned index\n1:procedure BUILD TREE(A,i,j)\n2: k←j−i+ 1 ▷size of A[i:j]\n3: ifk≤61then\n4: return Leaf node with content A[i:j]\n5: ˆr, ϵ←PCF( A[i:j],⌈ρ2\nρ1√\nk⌉,ai,aj)\n6: k′← ⌈2√\nk(1 +√0.5 log log k) + 2⌉\n7: ifϵ >k′\n2then\n8: return Leaf node with content A[i:j]\n9: C←array of size ⌈k\nk′⌉containing children\n10: forz←0to⌈k\nk′⌉do\n11: C[z]←BUILD TREE(A,zk′,(z+ 2)k′)\n12: return Non-leaf node with children Cand\nmodel ˆrwithmaxerrk′\n2\nnarrows down the possible answers to r(q). We say that a\nnode Ncovers a range SN, if we have r(q)∈SNfor any\nquery, q, that traverses the tree and reaches N. We call |SN|\nnodeN’scoverage size . Coverage size is the size of search\nspace left to search after reaching a node. The root node, N,\ncovers {1, ..., n}with coverage size nand the coverage size\ndecreases as we traverse down the tree. Leaf nodes have\ncoverage size independent of nwith high probability, so that\nfinding r(q)takes constant time after reaching a leaf. Each\nleaf node stores the subarray corresponding to the range it\ncovers as its content .\nRDA is built by calling BUILD TREE(A,1,n), as presented\nin Alg. 3. BUILD TREE(A,i,j) returns the root node, N,\nof a tree, where Ncovers {i, ..., j}. If the coverage size\nofNis smaller than some prespecified constant (line 3,\nanalogous to line 3 in Alg. 2), the algorithm turns Ninto a\nleaf node. Otherwise, in line 5 it uses Lemma 4.5 to create\nthe model ˆrforN, where ˆrapproximates ri−1,j+1(recall\nthatri,jis the index function for subarray A[i+ 1 : j−1]).\nIf the error of ˆris larger than predicted by Lemma 4.5, the\nalgorithm turns Ninto a leaf node and discards the model\n(this is analogous to line 12 in Alg. 2). Finally, for k′as\nin line 6, the algorithm recursively builds ⌈k\nk′⌉children for\nN. Each child has a coverage size of 2k′and the ranges\nare spread at k′intervals (line 11). This ensures that the set\nˆR={ˆr−ϵ,ˆr−ϵ+ 1, ...,ˆr+ϵ}, (with |ˆR| ≤k′ensured\nby line 7) is a subset of the range covered by one of N’s\nchildren. Furthermore, for any query q,ϵis the maximum\nerror of ˆr, sor(q)∈ˆR. Thus, the construction ensures that\nfor any query qthat reaches N,r(q)is in the range covered\nby one of the children of N.\nPerforming Queries . As Alg. 4 shows, to traverse the tree\nfor a query qfrom a node N, we find the child of Nwhose\ncovered range contains r(q). When ˆiisN.model estimate\n7\n\nOn Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\n102\n103\n104\n105\n106\n107\nn1.01.52.0No. Operations\n(a) Time Complexity\n102\n103\n104\n105\n106\n107\nn103105107109Index Size\n(b) Space Complexity(=0.1)\n (=0.01)\n Uniform =0.1\n =0.01\nFigure 5. Constant Query and Near-Linear Space\n102\n103\n104\n105\n106\n107\nn101520No. Operations\nRDS log2(n)Figure 6. Log-Logarithmic\nQuery and Constant Space\n102\n103\n104\n105\n106\n107\nn101520No. Operations\n(a) Time Complexity\n102\n103\n104\n105\n106\n107\nn103105107Index Size\n(b) Space ComplexityRDA log2(n) nFigure 7. Log-Logarithmic Query and Quasi-Linear\nSpace\nAlgorithm 4 RDA Index Querying\nRequire: The root node, N, of a learned index, a query q\nEnsure: Rank of query q\n1:procedure QUERY (N,q)\n2: ifNis a leaf node then\n3: return BINARY SEARCH (N.content )\n4: ˆi←N.model (q)\n5: k←N.maxerr(q)\n6: z← ⌊ˆi−k\n2k⌋\n7: return QUERY (N.children [z],q)\nwith maximum error k,z=⌊ˆi−k\n2k⌋gives the index of the\nchild whose range covers {⌊ˆi−k\n2k⌋2k, ..., (⌊ˆi−k\n2k⌋+2)2 k}and\ncontains {ˆi−k, ..., ˆi+k}as a subset and therefore contains\nr(q). Thus, the child at index zis recursively searched.\n4.3.3. C OMPLEXITY ANALYSIS\nThe query time analysis is very similar to the analysis in\nSec. 4.2.3 and is thus deferred to Appendix A. Here, we\nshow the space overhead complexity.\nAll nodes at a given tree level have the same coverage size.\nIf the coverage size of nodes at level iiszi, then the number\nof pieces used for approximation per node is O(ρ2\nρ1√zi)and\nthe total number of nodes at level iis at most O(n\nzi). Thus,\ntotal number of pieces used at level iiscρ2\nρ1n√zifor some\nconstant c. Note that if the coverage size at level iisk,\nthe coverage size at level i+ 1is4√\nk(1 +√0.5 log log k)\nwhich is more than k1\n2. Thus, zi≥n(1\n2)iandcρ2\nρ1n√zi≤\ncρ2\nρ1n\nn(1\n2)i+1. The total number of pieces is therefor at most\ncnρ2\nρ1Pc′log log n\ni=0n−(1\n2)i+1≤3cnρ2\nρ1for some constant c′.\nEach piece has magnitude nand can be written in O(logn)\nbits, so total overhead is O(ρ2\nρ1nlogn)bits.\n5. Experiments\nWe empirically validate our theoretical results on synthetic\nand real datasets (specified in each experiment).\nFor each experiment, we report index size andnumber of\noperations . Index size is the number of stored integers by\neach method. Number of operations is the total number of\nmemory operations performed by the algorithm and is used\nas a proxy for the total number of instructions performed\nby CPU. The two metrics differ by a constant factors in our\nalgorithm (our methods perform a constant number of oper-ations between memory accesses), but the latter is compiler\ndependent and difficult to compute. To report the number of\noperations, we randomly sample a set of 1000 queries Qand\na set of Aof100different arrays from the distribution χ. Let\nnq,Abe the number of operations for each query in q∈Q\non an array A∈ A. We report max q∈QP\nA∈Anq,A\n|A|, which\nis the maximum (across queries) of the average (across\ndatasets) number of operations.\n5.1. Results on Synthetic Datasets\nConstant Query Time and Near-Linear Space . We show\nthat the construction presented in Sec. 4.1 achieves the\nbound of Theorem 3.1. We consider Uniform and two Gaus-\nsian (with σ= 0.1andσ= 0.01) distributions. We vary\nGaussian standard deviation to show the impact of the bound\non p.d.f (as required by Theorem 3.1). Uniform p.d.f. has\nbound 1, and bound on Gaussian p.d.f with standard devia-\ntionσis1\nσ√\n2π. We present results for ϵ= 0.1andϵ= 0.01,\nwhere ϵis the space complexity parameter in Theorem 3.1.\nFig. 5 shows the results. It corroborates Thoerem 3.1, where\nFig. 5 (a) shows constant query time achieved by near-linear\nspace shown in Fig. 5 (b). We also see for larger ϵ, query\ntime actually decreases, suggesting our bound on query time\nis less tight for larger ϵ. Furthermore, recall that PCA Index\nscales the number of pieces by ρ1+ϵ\n2to provide the same\nbound on query time for all distributions (where ρis the\nbound on p.d.f). We see an artifact of this in Fig. 5 (b),\nwhere when ρincreases index size also increases.\nLog-Logarithmic Query Time and Constant Space . We\nshow that the construction presented in Sec. 4.2 achieves\nthe bound of Theorem 3.2. The theorem applies to distri-\nbutions with efficiently computable c.d.f.s, so we consider\ndistributions over [0,1]withFχ(x) =xtfort∈ {1,4,16}.\nAtt= 1, we have the uniform distribution and for larger t\nthe distribution becomes more skewed. Fig. 6 corroborates\nthe log-logarithmic bound of Theorem 3.2. Moreover, the\nresults look identical across distributions (multiple lines are\noverlaid on top of each other in the figure), showing similar\nperformance for distributions with different skew levels.\nLog-Logarithmic Query Time and Quasi-Linear Space .\nWe show that the construction presented in Sec. 4.3.3\nachieves the bound of Theorem 3.3. We consider Uni-\nform and two Gaussian ( σ= 0.1andσ= 0.01) distri-\nbutions. The results are presented in Fig. 7. It corroborates\n8\n\nOn Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\n102\n103\n104\n105\n106\n107\nn010203040No. Operations\n(a) =0.5\n102\n103\n104\n105\n106\n107\nn0102030No. Operations\n(b) =1\n102\n103\n104\n105\n106\n107\nn246810No. Operations\n(c) =5\n102\n103\n104\n105\n106\n107\nn1.52.02.5No. Operations\n(d) =10\nWL IOT BK FB OSM WK\nFigure 8. Constant Query Time on Real Datasets\n102\n103\n104\n105\n106\n107\nn104107Index Size\n=0.5\n=1\n=5\n=10\nnFigure 9. Near-Linear\nSpace on Real Datasets\n102\n103\n104\n105\n106\n107\nn101520No. Operations\n(a) 1\n2=0.5\n102\n103\n104\n105\n106\n107\nn101520No. Operations\n(b) 1\n2=1\n102\n103\n104\n105\n106\n107\nn101520No. Operations\n(c) 1\n2=10\n102\n103\n104\n105\n106\n107\nn101520No. Operations\n(d) 1\n2=20\nWL IOT BK FB OSM WK log2(n)\nFigure 10. Log-Logarithmic Query on Real Datasets\n102\n103\n104\n105\n106\n107\nn102104106Index Size\n1\n2=0.5\n1\n2=1\n1\n2=10\n1\n2=20\nnFigure 11. Quasi-Linear\nSpace on Real Datasets\nTheorem 3.3, where Fig. 7 (a) shows constant query time\nachieved by quasi-linear space shown in Fig. 7 (b). Similar\nto the previous case results look identical across distribu-\ntions (multiple lines are overlaid on top of each other in the\nfigure). Comparing Fig. 7 (a) and Fig. 6, we observe that\nusing a piecewise function approximator achieves similar\nresults as using c.d.f for rank function approximation.\n5.2. Results on Real Datasets\nSetup . On real datasets, we do not have access to the data\ndistribution and thus we do not know the value of ρin\nTheorem 3.1 or ρ1andρ2in Theorem 3.3. Thus, for each\ndataset, we perform the experiments for multiple values of\nρandρ1orρ2to see at what values the trends predicted by\nthe theorems emerge. Since we do not have access to the\nc.d.f, Theorem 3.2 is not applicable.\nWe use 6 real datasets commonly used for benchmarking\nlearned indexes. For each real dataset, we sample ndata\npoints uniformly at random for different values of nfrom\nthe original dataset, and queries are generated uniformly at\nrandom from the data range. The datasets are WL and IOT\nfrom Ferragina & Vinciguerra (2020); Kraska et al. (2018);\nGalakatos et al. (2019) and BK, FB, OSM, WK from Mar-\ncus et al. (2020) described next. WL: Web Logs dataset\ncontaining 714M timestamps of requests to a web server.\nIOT: timestamps of 26M events recorded by IoT sensors in-\nstalled throughout an academic building. BK: popularity of\n200M books from Amazon, where each key represents the\npopularity of a particular book. FB: 200M randomly sam-\npled Facebook user IDs, where each key uniquely identifies\na user. OSM: cell IDs of 800M locations from Open Street\nMap, where each key represents an embedded location. WK:\ntimestamps of 200M edits from Wikipedia, where each key\nrepresents the time an edit was committed.\nResults . Figs. 8 and 9 show time and space complexity of\nthe PCA algorithm (Theorem 3.1) on the real datasets for\nvarious values of ρ. Note that value of ρaffects the numberof pieces used, as described by Lemma 4.1. Furthermore,\nFigs. 10 and 11 show time and space complexity of the RDA\nalgorithm (Theorem 3.3) on the real datasets for various\nratios ofρ2\nρ1. Note that value ofρ2\nρ1affects the number of\npieces used per node, as described by Lemma 4.5.\nFor all except OSM datasets, trends described by Theo-\nrems 3.1 and 3.3 hold for values of ρandρ2\nρ1as small as 1.\nThis shows our theoretical results hold on real datasets, and\nthe distribution dependent factors, ρandρ2\nρ1, are typically\nsmall in practice. However, on OSM dataset value of ρand\nρ2\nρ1may be as large as 10 and 20 respectively. In fact, Marcus\net al. (2020) shows that non-learned methods outperform\nlearned methods on this dataset. As such, our results provide\na possible explanation (large values of ρ) for learned meth-\nods not performing as well on this dataset. Indeed, OSM\nis a one-dimensional projection of a spatial dataset using\nHilbert curves (see Marcus et al. (2020)), which distorts\nthe spatial structure of the data and can thus lead to sharp\nchanges to the c.d.f (and therefore large ρ).\n6. Conclusion\nWe theoretically showed and empirically verified that a\nlearned index can achieve sub-logarithmic expected query\ntime under various storage overheads and mild assumptions\non data distribution. All our proofs are constructive, using\npiecewise and hierarchical models that are common in prac-\ntice. Our results provide evidence why learned indexes per-\nform better than traditional indexes in practice. Future work\nincludes relaxing assumptions on data distribution, finding\nnecessary conditions for sub-logarithmic query time and\nanalyzing the trade-offs of different modeling approaches.\nAcknowledgements\nThis research has been funded in part by NSF grants CNS-\n2125530 and IIS-2128661, and NIH grant 5R01LM014026.\nOpinions, findings, conclusions, or recommendations ex-\npressed in this material are those of the author(s) and do not\nnecessarily reflect the views of any sponsors, such as NSF.\n9\n\nOn Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\nReferences\nBayer, R. Symmetric binary b-trees: Data structure and\nmaintenance algorithms. Acta informatica , 1(4):290–306,\n1972.\nBayer, R. and McCreight, E. Organization and maintenance\nof large ordered indices. In Proceedings of the 1970\nACM SIGFIDET (Now SIGMOD) Workshop on Data\nDescription, Access and Control , pp. 107–141, 1970.\nDing, J., Minhas, U. F., Yu, J., Wang, C., Do, J., Li, Y .,\nZhang, H., Chandramouli, B., Gehrke, J., Kossmann, D.,\net al. Alex: an updatable adaptive learned index. In\nProceedings of the 2020 ACM SIGMOD International\nConference on Management of Data , pp. 969–984, 2020.\nFerragina, P. and Vinciguerra, G. The pgm-index: a fully-\ndynamic compressed learned index with provable worst-\ncase bounds. Proceedings of the VLDB Endowment , 13\n(8):1162–1175, 2020.\nFerragina, P., Lillo, F., and Vinciguerra, G. Why are learned\nindexes so effective? In International Conference on\nMachine Learning , pp. 3123–3132. PMLR, 2020.\nFredman, M. L., Koml ´os, J., and Szemer ´edi, E. Storing a\nsparse table with 0 (1) worst case access time. Journal of\nthe ACM (JACM) , 31(3):538–544, 1984.\nGalakatos, A., Markovitch, M., Binnig, C., Fonseca, R.,\nand Kraska, T. Fiting-tree: A data-aware index structure.\nInProceedings of the 2019 International Conference on\nManagement of Data , pp. 1189–1206, 2019.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis,\nN. The case for learned index structures. In Proceedings\nof the 2018 International Conference on Management of\nData , pp. 489–504, 2018.\nLehman, T. J. and Carey, M. J. A study of index structures\nfor main memory database management systems. Techni-\ncal report, University of Wisconsin-Madison Department\nof Computer Sciences, 1985.\nMarcus, R., Kipf, A., van Renen, A., Stoian, M., Misra, S.,\nKemper, A., Neumann, T., and Kraska, T. Benchmarking\nlearned indexes. Proceedings of the VLDB Endowment ,\n14(1):1–13, 2020.\nMassart, P. The tight constant in the dvoretzky-kiefer-\nwolfowitz inequality. The annals of Probability , pp. 1269–\n1283, 1990.\nNavarro, G. and Rojas-Ledesma, J. Predecessor search.\nACM Computing Surveys (CSUR) , 53(5):1–35, 2020.P˘atras ¸cu, M. and Thorup, M. Time-space trade-offs for\npredecessor search. In Proceedings of the thirty-eighth\nannual ACM symposium on Theory of computing , pp.\n232–240, 2006.\nRao, J. and Ross, K. A. Cache conscious indexing for\ndecision-support in main memory. Proceedings of the\n25th VLDB Conference , 1999.\nVershynin, R. High-dimensional probability: An introduc-\ntion with applications in data science , volume 47. Cam-\nbridge university press, 2018.\nA. Proofs\nProof of Lemma 3.4 . Assume w.l.o.g that the sub-\nexponential distribution is centered. Let Zlbe the event that\nany point in the array is larger than lor smaller than −lfor a\npositive number l. Since the distribution is sub-exponential\nand using union bound, P(Zl)≤2ne−lKfor some constant\nK. To have 2ne−lK≤1\nlognwe get that 2nlogn≤elK\nandl≥1\nKlog(2nlogn). So let l=⌈2\nklog 2n⌉and we\nhaveP(Zl)≤1\nlogn. Now, to construct ˆR′we first check\nif any point in Ais larger than lor smaller than −l. If so,\nwe don’t build the index and only do binary search. Oth-\nerwise, create 2linstances of ˆRindex, with the i-th index\ncovering the range [−l+i,−l+i+ 1]. Note that interval\n[−l+i,−l+i+ 1] has length 1, so that scaling and translat-\ning the distribution to interval [0,1]does not impact the p.d.f\nof the distribution. Queries use one of the learned models to\nfind the answer. Thus, the query time is O(logn)with prob-\nability1\nlogn, and it is t(n)with probability 1−1\nlogn, which\nisO(t(n)). The space overhead is now O(s(n) logn).\nProof of Lemma 4.4 . Recall that X1,...,Xnare i.i.d random\nvariables sampled from χ. Furthermore, the array Ais a\nreordering of the random variables so that A[i] =Xsifor\nsome index si. That is, each element A[i]is itself a random\nvariable and equal to one of X1, ...,Xn.A[i] =aiis a\nrandom event.\nFork=j−i−1≥2, let{Xr1, ..., X rk} ⊆ { X1, ..., X n}\nbe the elements of the sub-array A[i+ 1 : j−1], which we\ndenote by Xrz∈A[i+ 1 : j−1]for1≤z≤k. Note that\nXr1, ..., X rkis not sorted, but the subarray A[i+ 1 : j−1]\nis the random variables Xr1, ..., X rkin a sorted order. For\nany random variable Xrzfor some z∈ {1, ..., k}, we first\nobtain it’s conditional c.f.d given the observations A[i] =ai\nandA[j] =aj. The conditional c.d.f can be written as\nP(Xrz< x|A[i] =ai, A[j] =aj). (2)\nLet¯X={X1, ...X n} \\ {Xrz}. Given that Xrz∈A[i+ 1 :\nj−1], the event A[i] =ai, A[j] =aj,is equivalent to the\nconjunction of the following events: (i) at most i−1of\nr.v.s in ¯Xare less than ai, (ii) at least iof r.v.s in ¯Xare\n10\n\nOn Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\nless than or equal to ai, (iii) at most j−2of r.v.s in ¯Xare\nless than aj, (iv) at least j−1of r.v.s in ¯Xare less than\nor equal to aj, and (v) ai≤Xrz≤aj. This is because (i)\nand (ii) imply A[i] =ai, while (iii)-(v) imply A[j] =aj.\nConversely, A[i] =aiandXrz∈A[i+ 1 : j−1]imply (i)\nand (ii), A[j] =ajandXrz∈A[i+ 1 : j−1]imply (iii)\nand (iv), and A[i] =ai, A[j] =aj, Xrz∈A[i+ 1 : j−1]\nimply (v).\nNow, denote by ϕ(¯X)the event described by (i)-(iv), so that\nEq. 2 is\nP(Xrz< x|ϕ(¯X), ai≤Xrz≤aj).\nXrzis independent from all r.v.s in ¯X, so that Eq. 2 simpli-\nfies to\nP(Xrz< x|ai≤Xrz≤aj) =Fi,j\nχ(x),\nFor all Xrz∈ {Xr1, ..., X rk}. Thus, the r.v.s in\n{Xr1, ..., X rk}have the conditional c.d.f Fi,j\nχ(x).\nA similar argument to the above shows that r.v.s in any\nsubset of Xr1, ..., X rkare independent, given A[i] =\nai, A[j] =aj. Specifically, let ˜X⊆ {Xr1, ..., X rk}for\n|˜X| ≥2. Then, the joint c.d.f of r.v.s in ˜Xcan be written as\nP(∀X∈˜XX < x |A[i] =ai, A[j] =aj). (3)\nSimilar to before, define ¯X={X1, ..., X n}\\˜X. Given that\n∀X∈˜XX∈A[i+ 1 : i−1], the event A[i] =ai, A[j] =aj\nis equivalent to the conjunction of the following events: (i)\nat most i−1of r.v.s in ¯Xare less than ai, (ii) at least iof\nr.v.s in ¯Xare less than or equal to ai, (iii) at most j−1−|˜X|\nof r.v.s in ¯Xare less than aj, (iv) at least j− |˜X|of r.v.s in\n¯Xare less than or equal to aj, and (v) ∀X∈˜X, we have\nai≤X≤aj. Now let ϕ(¯X)be the event described by\n(i)-(iv), so that Eq. 3 can be written as\nP(∀X∈˜XX < x |ϕ(¯X),∀X∈˜Xai≤X≤aj).\nAll r.v.s in ˜Xare independent from all r.v.s in ¯X, so that\nEq. 3 simplifies to\nP(∀X∈˜XX < x |∀X∈˜Xai≤X≤aj).\nFinally, all r.v.s in ˜Xare also independent from each other,\nso we obtain that Eq. 3 is equal to\nΠX∈˜XP(X < x |ai≤X≤aj), (4)\nProving the independence of r.v.s in ˜Xconditioned on\nA[i] =ai, A[j] =aj.\nTo summarize, we have shown that Xr1, ..., X rkr.v.s condi-\ntioned on A[i] =ai, A[j] =ajareki.i.d random variables\nwith the c.d.f Fi,j\nχ(x). Moreover,1\nkri,j\nA(x)is the empirical\nc.d.f of the kr.v.s. By DKW bound (Massart, 1990) and for\nt≥0, we have\nP(sup\nx|1\nkri,j\nA(x)−Fi,j\nχ(x)| ≥t√\nk)≤2e−2t2.\nRearranging and substituting t=√0.5 log log kproves the\nlemma.\nProof of Lemma 4.6 .k=j−i−1. Divide the range[ai, aj]intotuniformly spaced pieces, so that the z-th piece\napproximates kFi,j\nχoverIz= [ai+zaj−ai\nt, ai+ (z+\n1)aj−ai\nt], which is an interval of lengthaj−ai\nt. LetP(x)be\nthe constant in the Taylor expansion of kFi,j\nχaround some\npoint in Iz. By Taylor’s remainder theorem,\nsup\nx∈Iz|P(x)−kFi,j\nχ(x)| ≤k×fχ(c)\nFχ(aj)−Fχ(ai)×aj−ai\nt\n(5)\nfor some c∈Iz, where we have used the fact that the\nderivative of the c.d.f is the p.d.f, and that any two point in\nIzare at mostaj−ai\ntapart.\nBy mean value theorem, there exist a c′∈Izso that\nFχ(aj)−Fχ(ai)\naj−ai=fχ(c′). This together with Eq. 5 yields\nsup\nx∈R|P(x)−kFi,j\nχ(x)| ≤k×fχ(c)\nfχ(c′)×1\nt≤ρ2\nρ1k\nt.\nSetting t≥ρ2\nρ1√\nkensuresρ2\nρkk\nt≤√klog log kso that\n∥P(x)−kFi,j\nχ(x)∥∞≤p\nklog log k\nProof of Query Time Complexity of Theorem 3.3 . The al-\ngorithm traverses the tree recursively proceeds. At each\nrecursion level, the algorithm performs a constant number\nof operations unless it perform a binary search. Let the\ndepth of recursion be hand let kibe the coverage size of\nthe node at the i-th level of recursion (so that binary search\nati-th level takes O(logki)). Let Bidenote the event that\nthe algorithm performs binary search at the i-th iteration.\nThus, for any query q, the expected number of operations is\nEA∼χ[T(ˆr, q)] =hX\ni=1c1+c2P(Bi,¯Bi−1, ....¯B1) logki\nfor constants c1andc2. Note that P(Bi,¯Bi−1, ....¯B1)≤\nP(Bi|¯Bi−1, ....¯B1), where P(Bi|¯Bi−1, ....¯B1)is the prob-\nability that the algorithm reaches i-th level of tree and\nperforms binary search. By Lemma 4.5, this probability\nbounded by1\nlogki. Thus EA∼χ[T(ˆr, q)]isO(h).\nTo analyze the depth of recursion, recall that at the last\nlevel, the size of the array is at most 61. Furthermore, at\nevery iteration the size of the array is reduced by 4(√n)(1+√0.5 log log n). Forn≥61,4(√n)(1+√0.5 log log n)≤\nncfor some constant c <1, so that the size of the array at\nthei-th recursions is at most nciand the depth of recursion is\nO(log log n). Thus, the expected total time is O(log log n)\n.\nProof of Prop. 4.2 . Note that rA(x)is a non-decreasing step\nfunction, where each step has size 1. Let sibe the number\nof steps of rA(x)in the interval Ii. Therefore,\n|rA(x)−rA(x′)| ≤si, (6)\nfor any x, x′∈Ii. Therefore, for x∈Ii, substituting\n11\n\nOn Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing\nˆr(x;θ) =rA(i×1\nk)into Eq. 6 we get\nei≤si. (7)\nFurthermore, points of discontinuity (i.e., steps) of rA(x)oc-\ncur when x=A[j]forj∈[n]. Therefore, si=|{j|A[j]∈\nIi}|. That is, siis equal to the number of points in Athat\nare sampled in the range Ii.\nProof of Lemma 4.3 . Specifically, we bound the probability\nthatsmax≥c, for some constant c. In other words, we\nbound the probability of the event, E, that any interval has\nmore than cpoints sampled in it, for any c≥3. Let δi=\nFχ(i+1\nk)−Fχ(i\nk)be the probability that a point falls inside\nIi, so that δc\niis the probability that a set of csampled points\nfall inside Ii. Taking the union bound over all possible\nsubsets of size c, we get that the probability, pi, that the i-th\ninterval has cpoints or more is at most\npi≤\u0012n\nc\u0013\nδc\ni≤(en)c\nccδc\ni,\nWhere the second inequality follows from Sterling’s approx-\nimation. By mean value theorem, there exists c∈[i\nk,i+1\nk]\nsuch that Fχ(i+1\nk)−Fχ(i\nk) =fχ(c)(1\nk). Therefore, δi≤ρ\nk.\nThus, by union bound\nP(E)≤kX\ni=1(en)c\ncc(ρ\nk)c=k(en)c\ncc(ρ\nk)c. (8)\nNow set k≥n1+2\nc−1ρ1+1\nc−1and substitute into Eq. 8, we\nobtain that P(E)≤1\nn.\n12",
  "textLength": 56579
}