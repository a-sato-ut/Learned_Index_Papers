{
  "paperId": "a44ec507d2f26b3ce54d4fd2c3b1aa479b9a9c0d",
  "title": "An Unified System for Data Analytics and In Situ Query Processing",
  "pdfPath": "a44ec507d2f26b3ce54d4fd2c3b1aa479b9a9c0d.pdf",
  "text": "A Uniﬁed System for Data Analytics and In Situ\nQuery Processing\nAlex Watson§, Suvam Kumar Das§, and Suprio Ray\nFaculty of Computer Science, University of New Brunswick, Canada. Email:fawatson, suvam.das, sray g@unb.ca\nAbstract —In today’s world data is being generated at a high\nrate due to which it has become inevitable to analyze and quickly\nget results from this data. Most of the relational databases\nprimarily support SQL querying with a limited support for\ncomplex data analysis. Due to this reason, data scientists have\nno other option, but to use a different system for complex\ndata analysis. Due to this, data science frameworks are in huge\ndemand. But to use such a framework, all the data needs to be\nloaded into it. This requires signiﬁcant data movement across\nmultiple systems, which can be expensive.\nWe believe that it has become the need of the hour to come\nup with a single system which can perform both data analysis\ntasks and SQL querying. This will save the data scientists from\nthe expensive data transfer operation across systems. In our\nwork, we present DaskDB, a system built over the Python’s\nDask framework, which is a scalable data science system having\nsupport for both data analytics and in situ SQL query processing\nover heterogeneous data sources. DaskDB supports invoking any\nPython APIs as User-Deﬁned Functions (UDF) over SQL queries.\nSo, it can be easily integrated with most existing Python data\nscience applications, without modifying the existing code. Since\njoining two relations is a very vital but expensive operation, so\na novel distributed learned index is also introduced to improve\nthe join performance. Our experimental evaluation demonstrates\nthat DaskDB signiﬁcantly outperforms existing systems.\nI. I NTRODUCTION\nDue to the increasing level of digitalization, large volumes\nof data are constantly being generated. To make sense of\nthe deluge of data, it must be cleaned, transformed and ana-\nlyzed. Data science offers tools and techniques to manipulate\ndata in order to extract actionable insights from data. These\ninclude support for data wrangling, statistical analysis and\nmachine learning model building. Traditionally, practitioners\nand researchers make a distinction between query processing\nand data analysis tasks. Whereas relational database systems\n(henceforth, databases or DBMSs) are used for SQL-style\nquery processing, a separate category of frameworks are used\nfor data analyses that include statistical and machine learning\ntasks. Currently, Python has emerged as the most popular\nlanguage-based framework, for its rich ecosystem of data\nanalysis libraries, such as Pandas, Numpy, scikit-learn. These\ntools make it possible to perform in situ analysis of data that is\nstored outside of any database. However, a signiﬁcant amount\nof data is still stored in databases. To do analysis on this\ndata, it must be moved from a database into the address space\nof the data analysis application. Similarly, to do SQL query\nprocessing on data that is stored in a raw ﬁle, it must be loaded\n§Equal contributioninto a database using a loading mechanism, which is known\nas ETL (extract, transform, load). This movement of data and\nloading of data are both time consuming operations.\nTo address the movement of data across databases and\ndata analysis frameworks, recently researchers have proposed\nseveral approaches. Among them, a few are in-database so-\nlutions, that incorporate data analysis functionalities within\nan existing database. These include PostgreSQL/Madlib [1],\nHyPer [2] and AIDA [3]. In these systems, the application\ndevelopers write SQL code and invoke data analysis func-\ntionalities through user-deﬁned functions (UDF). There are\nseveral issues with these approaches. First , the vast body of\nexisting data science applications that are written in a popular\nlanguage (Python or R), need to be converted into SQL.\nSecond , the data analysis features supported by databases are\nusually through UDF functions, which are not as rich as that\nof the language-based API ecosystem, such as in Python or\nR.Third , data stored in raw ﬁles needs to be loaded into a\ndatabase through ETL. Although, some support for executing\nSQL queries on raw ﬁles exist, such as PostgreSQL’s support\nfor foreign data wrapper, this can easily break if the ﬁle is\nnot well-formatted. In recent years several projects [4], [5],\n[6] investigated how to support in situ SQL querying on\nraw data ﬁles. However, they primarily focused on supporting\ndatabase-like query processing, operating on a single machine.\nThese systems lack sophisticated data wrangling and data\nscience features that is available in Python or R. Fourth , most\nrelational databases are not horizontally scalable. Even with\nparallel databases, the parallel execution of UDFs is either\nnot supported or not efﬁcient. Although “Big Data” systems\nsuch as Spark [7] and Hive/Hivemall [8] address some of\nthese issues, that they often involve more complex APIs and\na steeper learning curve. Also, it is not practical to rewrite the\nlarge body of existing data science code with these APIs.\nTo address the issues with the existing approaches, we\nintroduce a scalable data science system, DaskDB, which\nseamlessly supports in situ SQL query execution and data\nanalysis using Python. DaskDB extends the scalable data\nanalytics framework Dask [9] that can scale to more than\none machine. Dask’s high-level collections APIs mimic many\nof the popular Python data analytics library APIs based on\nPandas and NumPy. So, existing applications written using\nPandas collections need not be modiﬁed. On the other hand,\nDask does not support SQL query processing. In contrast,\nDaskDB can execute SQL queries in situ without requiring\nthe expensive ETL step and movement of data from rawarXiv:2102.09295v2  [cs.DB]  7 Apr 2021\n\nﬁles into a database system. Furthermore, with DaskDB, SQL\nqueries can have UDFs that directly invoke Python APIs. This\nprovides a powerful mechanism of mixing SQL with Python\nand enables data scientists to take advantage of the rich data\nscience libraries with the convenience of SQL. Thus, DaskDB\nuniﬁes query processing and analytics in a scalable manner.\nA key issue with distributed query processing and data\nanalytics is the movement of data across nodes, which can\nsigniﬁcantly impact the performance. We propose a distributed\nlearned index , to improve the performance of join that is an\nimportant data operation. In DaskDB, a relation (or dataframe)\nis split into multiple partitions, where each partition consists\nof numerous tuples of that relation. These partitions are\ndistributed across different nodes. While processing a join,\nit is possible that not all partitions of a relation contribute to\nthe ﬁnal result when two relations are joined. The distributed\nlearned index is designed to efﬁciently consider only those\npartitions that contain the required data in constant time, by\nidentifying the data pattern in each partition. This minimizes\nthe unnecessary data movement across nodes. DaskDB also in-\ncorporates selective data persistence that signiﬁcantly reduces\nserialization/de-serialization overhead and data movement.\nWe conduct extensive experimental evaluation to compare\nthe performance of DaskDB against two horizontally scalable\nsystems: PySpark and Hive/Hivemall. Our experiments involve\nworkloads from a few queries from TPC-H [10] benchmark,\nwith different data sizes (scale factors). We also created a\ncustom UDF benchmark to evaluate DaskDB and PySpark.\nOur results show that DaskDB outperforms others in both of\nthese benchmarks. The key contributions of this paper are:\n\u000fWe propose DaskDB that integrates in situ query process-\ning and data analytics in a scalable manner.\n\u000fDaskDB supports SQL queries with UDFs that can di-\nrectly invoke Python data science APIs.\n\u000fWe introduce a novel distributed learned index.\n\u000fWe present extensive experimental results involving TPC-\nH benchmark and a custom UDF benchmark.\nII. B ACKGROUND\nDaskDB was built by extending the Dask [9], which is\nan open-source library for parallel computing in Python. The\nmain advantage of Dask is that it provides Python APIs and\ndata structures that are similar to Numpy, Pandas, and Scikit-\nLearn. Hence, programs written using Python data science\nAPIs can easily be switched to Dask by changing the import\nstatement. Dask comes with an efﬁcient task scheduler, which\ncan run programs on a single node and scale to many nodes.\nHowever, Dask does not support SQL queries. We show that\nDaskDB, which is built over Dask, can outperform Spark [11].\nIII. R ELATED WORK\nIn this section, ﬁrst we discuss about systems to perform\ndata analytics and query processing. Next, we look at works\nrelated to learned index, followed by in situ query processing.A. Data Analytics and Query Processing\n1) Dedicated Data Analytics Frameworks: Many open-\nsource data analytic applications traditionally use R. More\nrecently, Python has become very popular because of the\nAnaconda distribution [12]. It contains many data science and\nanalytics packages, such as Pandas, SciPy, Matplotlib, and\nscikit-learn. Some popular commercial data analytic systems\ninclude Tableau [13] and MATLAB [14].\n2) In-Database Analytics: An increasing number of the\nmajor DBMSs now include data science and machine learning\ntools. For instance, PostgreSQL supports SQL-based algo-\nrithms for machine learning with the Apache MADlib library\n[1]. However, interacting with a DBMS to implement analytics\ncan be challenging [15]. Although SQL is a mature technology,\nit is not rich enough for extensive data analysis. DBMSs typ-\nically support analytics functionalities through User Deﬁned\nFunctions (UDF). Since, a UDF may execute any external code\nwritten in R, Python, Java or T-SQL, a DBMS treats a UDF as\na black box because no optimization can be performed on it.\nIt is also difﬁcult to debug and to incrementally develop [3].\nThe in-database analytics approaches still have the constraint\nof ETL (extract, transform, load), which is a time-consuming\nprocess and not practical in many cases.\n3) Integrating Analytics and Query Processing: There have\nbeen several attempts at creating more efﬁcient solutions and\nthey combine two or more of either dedicated data analytic\nsystems, DBMS or big data frameworks. These systems can\nbe classiﬁed into 2 categories that we describe next.\nHybrid Solutions. These solutions integrate two or more\nsystem types together into one and are primarily DBMS-\ncentric approaches. AIDA [3] integrates a Python client\ndirectly to use the DBMS memory space, eliminating the\nbottleneck of transferring data. In [16] the authors present a\nprototype system that integrates a columnar relational database\n(MonetDB) and R together using a same-process zero-copy\ndata sharing mechanism. In [17], the authors proposed an\nembeddable analytical database DuckDB. The key drawback\nof these hybrid systems is ETL, since the data needs to\nbe loaded into a database. Moreover, existing data science\napplications written in Python or R, need to be modiﬁed to\nwork in such systems, since their interface is SQL-based.\n“Big Data” Analytics Frameworks. The most popular big\ndata frameworks are Hadoop and Spark [7]. Spark supports\nmachine learning with MLlib [18] and SQL like queries.\nHive is based on Hadoop that supports SQL-like queries and\nsupports analytics with the machine learning library Hive-\nmall [8]. Some drawbacks of big data frameworks include\nmore complicated development and steeper learning curve than\nmost other analytics systems and the difﬁculty in integration\nwith DBMS applications. To run any existing Python or R\napplication within a big data system, it will require rewriting\nthese applications with new APIs. This is not a viable option\nin most cases.\n2\n\n2.QueryPlanner \nGenerates a \nlogical and \nphysical plan 1. SQLParser \nExtracts \nmetadata \ninformation from \nSQL Query \nRaco is a subset of Myria stack, Myria was developped for big data management and \nanalytics, it was developed by the database group at the University of Washington. Raco \nis from the MyriaL at the University of Washington \\cite{Wang2017Myria}. Raco is a SQL \nquery parser. It takes an SQL query and returns a a preliminary optimized physical plan. \nWe chose to use Raco at because it was natively built in in python, this allowed for easy \nintegration and no potential overheard or trying to merge separate run-time \nenvironments. We added a few features to the existing code and changed a few things \nfor the SQL Query Parser and Optimizer. Namely we added functionality to the parser to \nsupport more keywords commonly used in SQL. \nSQL Parser first takes in the original \nsql query. In this step we gather \ninformation about relevant table \nnames, column names and any user \ndefined function. This is done because \nthe format Raco needs all this \ninformation beforehand to \nAvailable at \nhttps://github.com/TwoLaid/python-sqlp\narser  a. Metadata \nand SQL \nquery \nDaskDB takes in the Physical Plan \nfrom raco then creates a Dask Plan. \nFrom the dask Plan we are able to \nconvert this plan into dask which is \nexecutable in the dask client. \nSimple but subtle improvements to \nimprove query performance. Only read \nin b. Physical \nPlan \nc. Executable \nDask Code\nWithin Dask I head to modify few \nfeatures to make it capatable . Such \nas returning nth (smallest or largest \nfunction) with a multi-index. 4.DaskDB Execution Engine \n  Client \nScheduler \nW1W2 Wn\n5. HDFS Data Data ……3. DaskPlanner \nConverts to \nDaskplan then \nconverts to Dask \ncode and sends \nto Dask Fig. 1: DaskDB System Architecture\nB. Learned Index\nData structures such as B+trees are the mainstay of indexing\ntechniques. These approaches require the storage of all keys\nfor a dataset. Recent studies have shown that learned models\ncan be used to model the cumulative distribution function\n(CDF) of the keys in a sorted array. This can be used to predict\ntheir locations for the purpose of indexing and this idea was\ntermed as learned index [19]. Several learned indexes were\ndeveloped that include FITing-Tree [20] and PGM-Index [21].\nThe learned index approaches proposed so far were meant\nonly for stand-alone systems. These ideas have not been\nincorporated as part of any database system yet, to the best\nof our knowledge. Also, no learned index model has yet been\ndeveloped for any distributed data system.\nC. In Situ Query Processing\nA vast amount of data is stored in raw ﬁle-formats that are\nnot inside traditional databases. Data scientists, who frequently\nlack expertise in data modeling, database admin and ETL\ntools, often need to run interactive analysis on this data. To\nreduce the “time to query” and avoid the overhead associated\nwith relational databases, a number of research projects inves-\ntigated in situ query processing on raw data.\nNoDB [4] was one of the earliest systems to support in\nsitu query processing on raw data ﬁles. PostgresRaw [22] is\nbased on the idea of NoDB and it supports SQL querying over\nCSV ﬁles in PostgreSQL. The SCANRAW [5] system exploits\nparallelism during in situ raw data processing. Slalom [6]\nintroduced adaptive partitioning and on-the-ﬂy per partition\nindexes to improve query processing performance on raw data.\nAll these systems were focused on database-style SQL query\nprocessing on raw data and on a single machine. Our system,\nDaskDB supports in situ querying on heterogeneous data\nsources, and it also supports doing data science. Moreover, it\nis a distributed data system that can scale over a node cluster.\nIV. O URAPPROACH : DASKDB\nIn this section, we present DaskDB. DaskDB, in addition to\nsupporting all Dask features, also enables in situ SQL querying\non raw data in a data science friendly environment. Next, we\ndescribe DaskDB system architecture and its components.A. System Architecture\nThe system architecture of DaskDB incorporates ﬁve\nmain components: the SQLParser, QueryPlanner, DaskPlanner,\nDaskDB Execution Engine, and HDFS. They are shown in\nFigure 1. First, the SQLParser gathers metadata information\npertaining to the SQL query, such as the names of the tables,\ncolumns and functions. This information is then passed along\nto the QueryPlanner. Next, in the QueryPlanner component,\nphysical plan is generated from the information sent by SQL-\nParser about the SQL query. The physical plan is an ordered\nset of steps that specify a particular execution plan for a query\nand how data would be accessed. The QueryPlanner then sends\nthe physical plan to the DaskPlanner. In the DaskPlanner,\na plan is generated, which includes operations that closely\nresemble Dask APIs, called the Daskplan . The Daskplan is\nproduced from the physical plan, and it is then converted into\nPython code and sent to DaskDB Execution Engine. DaskDB\nExecution Engine then executes the code and gathers the data\nfrom the HDFS, and thus executes the SQL query. Further\ndetails are provided in the next sections.\n1) SQLParser: The SQLParser is the ﬁrst component of\nDaskDB that is involved in query processing. The input for\nthe SQLParser is the original SQL query. It ﬁrst checks for\nsyntax errors and creates a parse tree with all the metadata\ninformation about the query. We then process the parse tree\nto gather the metadata information needed by the QueryPlan-\nner. This metadata information includes table names, column\nnames and UDFs. We then check if the table(s) exist and\nif they do, we dynamically generate a schema. The schema\ncontains information about tables and column names and data\ntypes used in the SQL query. The schema, UDFs (if any) and\nthe original SQL query are then passed to the QueryPlanner.\n2) QueryPlanner: The QueryPlanner creates logical and\npreliminary physical plans. The schema and UDFs produced\nby SQLParser, along with the SQL query, are passed into the\nQueryPlanner. The QueryPlanner uses these to ﬁrst create a\nlogical plan and then an optimized preliminary physical plan.\nThis plan is then sent to the DaskPlanner.\n3) DaskPlanner: The DaskPlanner is used to transform the\npreliminary physical query plan from the QueryPlanner into\nPython code that is ready for execution. The ﬁrst step in this\nprocess is for the DaskPlanner to go through the physical plan\nobtained from QueryPlanner and convert it into a Daskplan.\nThis maps the operators from the physical plan into operators\nthat more closely resemble the Dask API. This Daskplan also\nassociates relevant information with each operator from the\nphysical plan. This information includes columns and tables\ninvolved and speciﬁc metadata information for a particular op-\nerator. We also keep track of each operator’s data dependency.\nThis is needed to pass intermediate results from one operation\nto the next. Algorithm 1 shows how DaskDB converts the\nphysical plan into the Daskplan.\nIn the next step, the DaskPlanner converts the Daskplan\ninto the Python code, which utilizes the Dask API. All of the\ndetail about each table and their particular column names and\nindexes are tracked and maintained in a dynamic dictionary\n3\n\nAlgorithm 1: DaskPlanner: Conversion of Physical\nPlan to Daskplan\nInput: A physical plan ( P) is given as input. P\ncontains ordered groups of dependent operators\n(G). Each Gconsists of an ordered list of\ntuples ( k, o, d ), where kcontains the unique\nkey of a operation, ois the operation type and\ndcontains the operation metadata information.\nOutput: The ﬁnal result is a Daskplan DP, which\nconsists of an ordered list of operators.\n1DP list()\n2forsorted( G2P)do\n3 forsorted( k; o; d2G)do\n4 dp dict() //create Daskplan operator\n5 dp[o] convertToDaskPlanOperator(o)\n6 dp[d] getMetadataInfo(d)\n7 dp[key] k //adds key (used to get data\ndependencies)\n8 DP.add(dp) //adds dp to Daskplan\n9forsorted( dp2DP)do\n10 while dphas children (c) do\n11 dp[ti] get table information from c i\n12return DP\nthroughout the execution of a query. This is because multiple\ntables and columns may be created, removed or manipulated\nduring execution. For these reasons, the names of the tables,\ncolumns, and indexes are dynamically maintained while trans-\nforming the Daskplan into Python code to execute it.\n4) DaskDB Execution Engine: There are three main com-\nponents of the DaskDB execution engine: the client, scheduler\nand workers. The client transforms the Dask Python code into\na set of tasks. The scheduler creates a DAG (directed acyclic\ngraph) from the set of tasks, automatically partitions the data\ninto chunks, while taking into account data dependencies. The\nscheduler sends a task at a time to each of the workers ac-\ncording to several scheduling policies. The scheduling policies\nfor task and workers depend on various factors including data\nlocality. A worker stores a data chunk until it is not needed\nanymore and is instructed by the scheduler to release it.\n5) HDFS: The Hadoop Distributed File System\n(HDFS) [23] is a storage system used by Hadoop applications.\nDaskDB uses HDFS to store and share the data ﬁles among\nits nodes.\nB. Illustration of SQL query execution\nAnin situ query is executed within DaskDB by calling\nquery function with the SQL string as argument. The query\nin Figure 2 is a simpliﬁed version of a typical TPC-H query.\nThe Daskplan, shown in Figure 3, is generated from the\nphysical plan in the DaskPlanner component. The Daskplan\noperators more closely resemble the Dask API. For example,\nthese include the read_csv andfilters methods shown\nin the tree. This Daskplan is then converted into executable\nPython code, which is omitted due to space constraint.from daskdb_core import query\nsql = \"\"\"SELECT l_orderkey, sum(l_extendedprice *\n(1-l_discount)) as revenue\nFROM orders, lineitem\nWHERE l_orderkey = o_orderkey and o_orderdate >= '1995-01-01'\nGROUP BY l_orderkey\nORDER BY revenue LIMIT 5 ; \"\"\"\nquery(sql)\nFig. 2: Code showing SQL query execution\nColumnMapping  l_orderkey, l_extendedprice,l_discount \n  groupby: (l_orderkey)  \naggregate: sum (revenue) order by revenue \nread_csv  orders  filter  o_orderdate>='1995-01-01' DaskDBJoin \nread_csv  lineitem o_orderkey=l_orderkey \nNewColumn revenue = (l_extendedprice * (1-l_discount)) limit 5  l_orderkey revenue \nFig. 3: Generated Daskplan for the code in Figure 2\nC. Support for SQL query with UDFs\nDaskDB supports UDFs in SQL as part of in situ querying.\nA UDF enables a user to create a function using Python code\nand embed it into the SQL query. Since DaskDB converts the\nSQL query, including the UDF back into Python code, the\nUDFs can reference and utilize features from any of the data\nscience packages available in Anaconda Python API ecosys-\ntem. Spark introduced UDF’s since version 0.7 but it operated\none-row-at-a-time and thus suffered from high serialization\nand invocation overhead. Later they came up with Pandas\nUDF which provides low-overhead, high-performance UDFs\nentirely in Python. Although Pandas UDFs are efﬁcient, but are\nrestrictive to use in queries. Sometimes it also requires to use\nSpark’s own data types, which would be inconvenient for users\nwho are not experienced in Spark. In contrast, in DaskDB\nUDFs for SQL queries can easily be written. Any native\nPython function (either imported from an existing package or\ncustom-made), which accepts Pandas dataframes as parameters\ncan be applied as UDFs to the SQL queries in DaskDB. The\nreturn type of the UDFs is not ﬁxed like Spark’s Pandas UDF,\nand hence allows the user to design UDFs with ease. Like a\ngeneral Python function, UDFs with code involving machine\nlearning, data visualization and numerous other functionalities\ncan easily be developed and applied on queries in DaskDB.\nD. Illustration of SQL query with UDF\nWe illustrate UDF with SQL in DaskDB with K-Means\nclustering. Similar to Spark, a UDF needs to be registered to\nDaskDB system using the register_udf API. As shown in\nFigure 4, the UDF myKMeans takes as input a single Pandas\ndataframe having 2 columns; hence the UDF is registered\n4\n\nasregister_udf (myKMeans, [2]) . As part of the\nquery, the UDF is invoked as myKMeans(l_discount,\nl_tax) , which means after application of the selection con-\ndition(l_orderkey <50) and the limit (limit 50)\nto thelineitem relation, both the columns l_discount\nandl_tax together form a Pandas dataframe and is passed\ntomyKMeans . The output of the code is plotted in Figure 5,\nwhere the different clusters are shown by different colours.\nfrom daskdb_core import query, register_udf\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\ndefmyKMeans(df):\nkmeans = KMeans(n_clusters=4).fit(df)\ncol1 = list(df.columns)[0]\ncol2 = list(df.columns)[1]\nplt.scatter(df[col1], df[col2],\nc= kmeans.labels_.astype(float), s=50)\nplt.xlabel(col1)\nplt.ylabel(col2)\nplt.show()\nregister_udf(myKMeans,[2])\nsql_kmeans = \"\"\"select myKMeans(l_discount, l_tax)\nfrom lineitem where l_orderkey < 50 limit 50; \"\"\"\nquery(sql_kmeans)\nFig. 4: UDF code showing K-Means Clustering\nE. Distributed Learned Index\nJoin is considered one of the most expensive data opera-\ntions. We propose a novel distributed learned index, to accel-\nerate distributed execution of join in DaskDB. Our distributed\nlearned index relies on Heaviside step function [24]. Any step\nfunction can be represented by a combination of multiple\nHeaviside step functions, which forms the basis of our learned\nindex structure. A Heaviside step function can be deﬁned as\nH(x) =(\n0x <0\n1x\u00150(1)\nWe deﬁne a Partition Function, P as\nPa;b;c(y) =H((b\u0000y)\u0003(y\u0000a))\u0003c (2)\nwhich returns cwhenever a\u0014y\u0014b, or returns 0 otherwise.\nWhile constructing the distributed learned index it is as-\nsumed that one of the relations is sorted by the join attribute.\nHence, if an index can maintain the ﬁrst and last values of\nthe keys for each partition, then given any key, the partition\ncontaining the key can be identiﬁed by a Partition Function .\nA sparse index in this case can entail huge storage savings,\nsince all the keys are not required to be stored. We illustrate\nthis using a simpliﬁed example with the customer table from\nTPC-H, where ccustkey is the primary key. If there are 500\ntuples in this relation and each table partition can store 100\ntuples, then there will be total 5 partitions. The distribution of\nthe keys is shown in Table II, and also plotted in Figure 6.It can be seen that the plot is a step function f, where\nf(key) =8\n>>>>>><\n>>>>>>:1 1\u0014key\u0014200\n2 250\u0014key\u0014380\n3 400\u0014key\u0014560\n4 580\u0014key\u0014700\n5 701\u0014key\u0014800(3)\nwhich can equivalently be represented by summing several\nPartition Functions , which constitutes the Learned Index\nFunction Lcustomer on the customer table as\nLcustomer (key) =P1;200;1(key) +P250;380;2(key)\n+P400;560;3(key) +P580;700;4(key) +P701;800;5(key)\nwhere, a, b andcof the Partition Function P represent the\nbegin and end keys and the corresponding partition id.\nNext we explain how our learned index can effectively be\nused for join queries. Let AandBbe two relations, which\nneed to be joined on colAandcolBofAandBrespectively.\nWithout loss of generality, we assume colAis sorted and a\nLearned Index Function (LA) is generated on this column.\nSince DaskDB internally uses Dask APIs, before joining the\nrelations, they are converted to Dask dataframes. Each dask\ndataframes consists of several partitions. For each tuple tof\na partition of B,LA(t[colB])is calculated (which returns\nthe partition number of Ato which t[colB]belongs) and is\nappended to tas a new ‘Partition’ column. This process is\nparallely executed for each partition of B. Then for each\npartition number iofA, all the tuples tofBare selected\nsuch that t[0Partition0] =iand are hence joined with the\nithpartition of A. The processes of partition identiﬁcation,\npartition selection and the partition-wise join are individually\nexecuted in parallel. For joining the dataframes, we developed\na variant of the merge API of Pandas package.\nV. E VALUATION\nIn this section, we present the experimental setup, TPC-H\nbenchmark results and a custom UDF benchmark results.\nA. Experimental Setup\nDaskDB was implemented in Python by extending Dask.\nThe SQLParser of DaskDB utilizes the tool sql-metadata [25].\nThe QueryPlanner of DaskDB extends Raco [26]. We ran\nexperiments on a cluster of 8 machines each having 16 GB\nmemory and 8 Intel(R) Xeon(R) CPUs running at 3.00 GHz\nand each machine ran Ubuntu 16.04 OS.\nWe evaluated DaskDB against two systems that support\nboth SQL query execution and data analytics: PySpark and\nHive/Hivemall ( henceforth referred to as Hivemall ). HDFS\nwas used to store the datasets for each system. The software\nversions of Python, PySpark and Hive were 3.7.6, 3.0.1\nand 2.1.0 respectively. PySpark and Hivemall were allocated\nmaximum resources available (i.e. cores and memory).\n5\n\nTasks Query\nLR select myLinearFit(l discount, l tax) from lineitem where l orderkey <10\nlimit 50\nK-Means select myKMeans(l discount, l tax) from lineitem, orders where l orderkey\n= o orderkey limit 50\nQuantiles select myQuantile(l discount) from lineitem, orders where l orderkey =\noorderkey limit 50\nCGO select myCGO(l discount, l tax) from lineitem where l orderkey <10 limit\n1\nTABLE I: Queries with UDF\n0.00 0.02 0.04 0.06 0.08 0.10\nl_discount0.000.020.040.060.08l_tax Fig. 5: K-Means output\nQ1Q3Q5Q6Q10020406080100120140\n839399\n25106\n14\n911\n612\n22314Hivemall PySpark DaskDBTime (Seconds)\n(a) Scale Factor 1 (GB)\nQ1Q3Q5Q6Q100100200300400500\n334420\n310\n94333\n4870\n50\n1837\n12913\n121Hivemall PySpark DaskDBTime  (seconds) (b) Scale Factor 5 (GB)\nQ1Q3Q5Q6Q100200400600800\n534584644\n338585\n250\n264\n97\n7291\n22\n1317\n157Hivemall PySpark DaskDBTime (seconds) (c) Scale Factor 10 (GB)\nQ1Q3Q5Q6Q100200400600800100012001400\n1065\n1047\n1020\n6931017\n615\n631697\n472622\n60\n2334\n2132Hivemall PySpark DaskDBTime (seconds) (d) Scale Factor 20 (GB)\nFig. 7: Execution times - TPC-H benchmark queries\nLRK-Means Quantiles CGO020406080100120140\n7119\n15\n514 4 1PySpark DaskDBTime (Seconds)\n(a) UDF on SF 1\nLRK-Means Quantiles CGO0100200300400500600700800\n15618\n52\n15\n1 6 6 1PySpark DaskDBTime (seconds) (b) UDF on SF 5\nLRK-Means Quantiles CGO020040060080010001200\n105839\n245\n68\n2 13121PySpark DaskDBTime (Seconds) (c) UDF on SF 10\nLRK-Means Quantiles CGO0100200300400500600700800\n463624\n482\n14137\n1PySpark DaskDBTime (Seconds)\nPySpark execution too long (d) UDF on SF 20\nFig. 8: Execution times - SQL queries with UDFs\nBegin End Partition\n1 200 1\n250 380 2\n400 560 3\n580 700 4\n701 800 5\nTABLE II: Sparse index\nforcustomer relation\n010020030040050060070080012345\nKeysPartition Number Fig. 6: Keys vs Partition plot\nB. TPC-H Benchmark Evaluation Results\nWe evaluated the systems with several queries from TPC-\nH decision support benchmark [10]. We used 4 scale factors\n(SF): 1, 5, 10 and 20, where SF 1 indicates roughly 1 GB.\nWe executed 5 queries from TPC-H benchmark and the\nresults are plotted in Figure 7. As can be seen, DaskDB\noutperforms PySpark and Hivemall on all queries for all the\nscale factors. Hivemall performs worse than both DaskDB and\nPySpark in all cases. However, in general higher the SF, larger\nthe performance gap between them. For instance, with Q10,\nDaskDB is 3.5\u0002faster than PySpark at SF 1 and 4.7 \u0002faster\nthan PySpark at SF 20. DaskDB achieves a speedup of 182 \u0002\nwith Q5 at SF 20. The superior performance of DaskDB can be\ncredited to its efﬁcient data distribution, join implementationusing distributed learned index and selective data persistence.\nC. UDF Benchmark Evaluation Results\nWe developed a custom UDF benchmark, which consists\nof four machine learning tasks with UDFs: LR (Linear\nRegression), K-Means (K-Means Clustering), Quantiles\n(Quantiles Estimation) andCGO (Conjugate Gradient Op-\ntimization) . They were developed using the available machine\nlearning packages in Python. For each of the machine learning\ntasks, SQL queries in Table I were executed. The UDFs were\nwritten to perform the same task in both the systems by\nimporting the same Python packages. For this evaluation, the\nperformance of DaskDB was compared with that of PySpark,\nwhereas Hivemall results are skipped due to poor performance.\nThe results are plotted in Figure 8. Similar to the TPC-H\nbenchmark results, DaskDB outperforms PySpark here as well\nfor all the machine learning tasks for all scale factors. Among\nall the tasks K-Means performs worst in PySpark. With respect\nto K-Means, DaskDB performs 28.5 \u0002faster than PySpark at\nSF 1 and 64\u0002faster at SF 10. For SF 20, PySpark took too\nlong to perform K-Means and hence could not be measured,\nwhereas DaskDB took only 41s approximately. For Quantiles,\nDaskDB was 4\u0002and 16.6\u0002faster than PySpark for SF 1 and\n6\n\nSF 20 respectively. These results also show that larger the SF,\nthe better DaskDB performs compared to PySpark.\nVI. C ONCLUSION\nWe presented DaskDB, a scalable data science system. It\nbrings in situ SQL querying to a data science platform in a\nway that supports high usability, performance, scalability and\nbuilt-in capabilities. Moreover, DaskDB also has the ability\nto incorporate any UDF into the input SQL query, where the\nUDF could invoke any Python library call. Furthermore, we\nintroduce a novel distributed learned index that accelerates\njoin/merge operation. We evaluated DaskDB against two state-\nof-the-art systems, PySpark and Hive/Hivemall, using TPC-\nH benchmark and a custom UDF benchmark. We show that\nDaskDB signiﬁcantly outperforms these systems.\nREFERENCES\n[1] J. M. Hellerstein et al. , “The MADlib Analytics Library: Or MAD Skills,\nthe SQL,” PVLDB , vol. 5, no. 12, pp. 1700–1711, 2012.\n[2] A. Kemper and T. Neumann, “Hyper: A hybrid oltp & olap main\nmemory database system based on virtual memory snapshots,” in ICDE ,\n2011, p. 195–206.\n[3] J. V . D’silva, F. De Moor, and B. Kemme, “AIDA: Abstraction for\nAdvanced In-database Analytics,” PVLDB , vol. 11, no. 11, 2018.\n[4] I. Alagiannis, R. Borovica, M. Branco, S. Idreos, and A. Ailamaki,\n“NoDB: efﬁcient query execution on raw data ﬁles,” in SIGMOD , 2012,\npp. 241–252.\n[5] Y . Cheng and F. Rusu, “Parallel in-situ data processing with speculative\nloading,” in SIGMOD , 2014, p. 1287–1298.\n[6] M. Olma et al. , “Adaptive partitioning and indexing for in situ query\nprocessing,” VLDB J. , vol. 29, no. 1, pp. 569–591, 2020.\n[7] M. Zaharia et al. , “Spark: Cluster computing with working sets,” in\nUSENIX HotCloud , 2010, pp. 10–10.\n[8] “Hivemall,” https://hivemall.apache.org/.\n[9] M. Rocklin, “Dask: Parallel computation with blocked algorithms and\ntask scheduling,” in Python in Science Conference , 2015, pp. 130 – 136.\n[10] “TPC-H,” http://www.tpc.org/tpch/.\n[11] “Apache Spark,” https://spark.apache.org/.\n[12] “Anaconda,” https://www.continuum.io/.\n[13] “Tableau,” https://www.tableau.com/.\n[14] “MATLAB,” https://www.mathworks.com/products/matlab.html.\n[15] E. Fouch ´e, A. Eckert, and K. B ¨ohm, “In-database analytics with ib-\nmdbpy,” in SSDBM , 2018.\n[16] J. Lajus and H. M ¨uhleisen, “Efﬁcient data management and statistics\nwith zero-copy integration,” in SSDBM , 2014.\n[17] M. Raasveldt and H. M ¨uhleisen, “Data management for data science -\ntowards embedded analytics,” in CIDR , 2020.\n[18] X. Meng et al. , “MLlib: Machine Learning in Apache Spark,” Journal\nof Machine Learning Research , vol. 17, no. 34, pp. 1–7, 2016.\n[19] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in SIGMOD , 2018, p. 489–504.\n[20] A. Galakatos et al. , “FITing-Tree: A Data-Aware Index Structure,” in\nSIGMOD , 2019, p. 1189–1206.\n[21] P. Ferragina and G. Vinciguerra, “The PGM-index: a fully-dynamic\ncompressed learned indexwith provable worst-case bounds,” in PVLDB ,\nvol. 13, no. 8, 2020, pp. 1162–1175.\n[22] “PostgresRAW,” https://github.com/HBPMedical/PostgresRAW.\n[23] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The hadoop\ndistributed ﬁle system,” in MSST , 2010, pp. 1–10.\n[24] “Heaviside Function,” https://en.wikipedia.org/wiki/Heaviside step function.\n[25] “sql-metadata,” https://pypi.org/project/sql-metadata/.\n[26] J. Wang et al. , “The Myria Big Data Management and Analytics System\nand Cloud Services,” in CIDR , 2017.\n7",
  "textLength": 35946
}