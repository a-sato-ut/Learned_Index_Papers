{
  "paperId": "b2fa087a302940c92073b007c24c3b1a8e434a15",
  "title": "Approximate Proportionality in Online Fair Division",
  "pdfPath": "b2fa087a302940c92073b007c24c3b1a8e434a15.pdf",
  "text": "Approximate Proportionality in Online Fair Division\nDavin Choo∗1, Winston Fu∗2, Derek Khu∗3, Tzeh Yuan Neoh∗1, Tze-Yang Poon∗4, and\nNicholas Teh∗4\n1Harvard University, USA\n2Princeton University, USA\n3Institute for Infocomm Research (I2R), A*STAR, Singapore\n4University of Oxford, UK\nWe study the online fair division problem, where indivisible goods arrive sequentially and must be allocated\nimmediately and irrevocably to agents. Prior work has established strong impossibility results for approximat-\ning classic fairness notions, such as envy-freeness and maximin share fairness, in this setting. In contrast, we\nfocus on proportionality up to one good (PROP1), a natural relaxation of proportionality whose approxima-\nbility remains unresolved. We begin by showing that three natural greedy algorithms fail to guarantee any\npositive approximation to PROP1 in general, against an adaptive adversary. This is surprising because greedy\nalgorithms are commonly used in fair division and a natural greedy algorithm is known to be able to achieve\nPROP1 under additional information assumptions. This hardness result motivates the study of non-adaptive\nadversaries and the use of side-information, in the spirit of learning-augmented algorithms. For non-adaptive\nadversaries, we show that the simple uniformly random allocation can achieve a meaningful PROP1 approx-\nimation with high probability. Meanwhile, we present an algorithm that obtain robust approximation ratios\nagainst PROP1 when given predictions of the maximum item value (MIV). Interestingly, we also show that\nstronger fairness notions such as EF1, MMS, and PROPX remain inapproximable even with perfect MIV\npredictions.\n1 Introduction\nTheproblemoffairlyallocatingindivisibleresourcesamongmultipleinterestedpartiesisafundamentalproblem\nat the intersection of computer science and economics, with applications ranging from course assignment and\nvaccine distribution to manpower deployment and supplies distribution.\nWhile most of the classical literature assumes an offline setting—where all resources and preferences are\nknown in advance and must be allocated in a single shot—many real-world scenarios are inherently dynamic.\nIn applications such as cloud computing resource allocation or driver assignment in ride-sharing platforms,\nresources (or goods) arrive sequentially in an onlinefashion and must be allocated immediately and irrevocably\nupon arrival. This raises a natural and fundamental question: can we design (online) algorithms that fairly\nallocate goods as they arrive? The online nature of the problem introduces significant challenges: future\narrivals are unknown, and fairness must be achieved with the absence of information about future goods (and\ncorrespondingly, agents’ preferences over these goods).\nSeveral well-studied fairness benchmarks in the offline fair division setting include envy-freeness up to one\ngood (EF1) [Bud11, LMMS04], envy-freeness up to any good (EFX) [CKM+19], and maximin share fairness\n(MMS) [AMNS17, KPW18]. The first two are relaxations of the stronger envy-freeness concept (which may\nnot exist even in the trivial setting with two agents and one good). Here, an agent is said to envyanother if\nthey strictly prefer the bundle allocated to the other agent over their own bundle. Intuitively, EF1 requires\nthat any envy an agent may have towards another agent can be eliminated by removing a single good from the\nenvied bundle, while EFX strengthens this by requiring that the envy can be eliminated by removing anygood\nfrom the envied bundle. MMS, on the other hand, benchmarks an agent’s bundle against the best guarantee\nthey could secure if they were to partition the goods into as many bundles as there are agents and receive the\n∗Equal contribution; Alphabetical ordering\n1arXiv:2508.03253v1  [cs.GT]  5 Aug 2025\n\nleast-preferred bundle, ensuring each agent gets at least this value in the final allocation. While EFX implies\nEF1, there is no other implication relationship between these properties.\nIn the offline setting, while EF1 allocations are known to always exist and can be computed efficiently\n[LMMS04], the existence of EFX allocations in general remains elusive and is considered one of “fair division’s\nmost enigmatic open problems” [Pro20]. In contrast, exact MMS allocations may not exist in all instances,\nbut various multiplicative approximation guarantees are known [AMNS17], making the study of approximate\nMMS allocations an active area of research.\nIn contrast, achieving these fairness guarantees in the online model is significantly more challenging, because\ndecisions must be made irrevocably typically without knowledge of future arrivals. Indeed, [ZBW23] show\nthat no finite approximation to MMS is possible in the online setting for three or more agents, even given\nnormalization information (i.e., when the total value that each agent assigns to the entire set of arriving goods\nis known in advance). Building on this, [NPT25] establish that no finite approximation to EF1 is possible under\nthe same informational assumptions, again for three or more agents. For the case of two agents, however, a\n1/2-approximation to MMS is achievable, and EF1 can be attained; these guarantees, however, break down\nwhen the total valuation information is not available.1\nThese results highlight a stark contrast with the offline setting and underscore the intrinsic difficulty of\nachieving even approximate fairness in the online model, revealing a significant gap in the attainable fairness\nguarantees.\nIn this work, we focus on a fairness notion known as proportionality , which requires that each agent receive\na bundle worth at least a 1/nfraction of the total value, according to their own valuation. However, just as\nexact envy-freeness and MMS may be unachievable with indivisible goods, proportionality is often unattainable\nin such settings. This motivates the study of a relaxed variant called proportionality up to one good (PROP1),\nwhich requires that each agent’s bundle can be made proportional by adding a single good to the agent’s\nbundle.2While PROP1 is a relaxation of proportionality, it is also implied by both EF1 and MMS, making it\na natural candidate to study in settings where neither of the stronger notions is guaranteed to exist.3\nIn the online setting, [NPT25] showed that although PROP1 allocations may not always exist (using a result\nfrom [BKPP18]), they can be computed when normalization information (i.e., the sum of agents’ valuations is\nknown) is available. However, a gap remains: they neither establish any finite approximation to PROP1, nor\nprove that no such approximation is possible, as has been shown for the stronger notions EF1 and MMS. In\nthis work, we make progress toward closing this gap.\nTobetterreflecttheconstraintsof onlinedecision-making,weadoptaweakerinformationalassumptionthan\nthose considered in prior work. Rather than assuming access to normalization information, which effectively\nreveals an estimate of the total remaining value and can be quite powerful in online settings, we assume only\naccess to maximum item value (MIV) predictions: for each agent, we are given their maximum valuation over\nany single item. To the best of our knowledge, ours is the first work to establish fairness guarantees under this\nmore limited assumption.\nMoreover, motivated by existing hardness results, we adopt the framework of learning-augmented algo-\nrithms [LV21], which equips online algorithms with external advice of unknown quality. This model provides\na principled way to combine data-driven forecasts with worst-case guarantees, evaluating performance via ro-\nbustness (guaranteed performance regardless of advice quality) and consistency (performance when advice is\naccurate). Within this framework, we design and analyze allocation algorithms that leverage MIV predictions\nto obtain non-trivial approximation ratios with respect to PROP1, outperforming purely online approaches\nwhen predictions are accurate, while degrading gracefully when they are not.\n1.1 Our Contributions and Paper Outline\nThis work focuses on achieving approximate proportionality up to one good (PROP1) in the online fair division\nmodel.\n1. Greedy allocation against adaptive adversaries (Section 3). We begin by showing that three\ndifferent natural greedy allocation strategies fail to guarantee a non-zero approximation to PROP1 against\nan adaptive adversary. This result is surprising as greedy algorithms are widely used in the fair division\n1Under these informational assumptions, EFX remains unattainable, with no finite approximation possible.\n2A stronger variant, PROPX, guarantees proportionality after the removal of any good from another agent’s bundle.\n3In the offline setting, EF1 is always guaranteed to exist, which has made it the more commonly studied notion. Nonetheless,\nPROP1 has been examined in numerous works [ACIW19, AMS20, BK19, BS24, CFS17].\n2\n\nliterature and typically yield at least non-zero (possibly trivial) approximation guarantees. Our findings reveal\nfundamental limitations of greedy methods in adversarial online environments.\nTo make progress, we turn to two alternative settings: (1) randomization against non-adaptive adversaries,\nand (2) predictive information against adaptive adversaries.\n2. Random allocation against non-adaptive adversaries (Section 4). We study the guarantees of\nthe uniform-at-random allocation rule ( Rand) against a non-adaptive adversary which commits to the entire\ninput sequence in advance, independent of the algorithm’s realized randomness. Such a weakened adversary is\ncommonly studied in online algorithms and enables meaningful probabilistic guarantees. We show that Rand\nachieves an Ω(1/log(n/δ))-approximation to PROP1 with probability at least 1−δ, for any number of agents\nn≥2and failure probability δ >0. This is the first known result for random allocation against non-adaptive\nadversaries in this setting, requiring neither asymptotic assumptions nor structural constraints.\n3. Algorithm with MIV predictions against adaptive adversaries (Section 5). We design an algo-\nrithm that guarantees1\nn-PROP1 against adaptive adversaries, assuming access to maximum item value (MIV)\npredictions. We further show that this guarantee degrades gracefully under noisy predictions . Specifically, if\neach agent iis given a prediction pisuch that the true maximum value lies in the interval [(1−ε)pi, pi], then\nany algorithm achieving α-PROP1 under perfect predictions can be adapted to achieve α·\u0010\n1−ε\n1−εα\nn\u0011\n-PROP1\nunder one-sided error ε >0, implying\u0010\n1−ε\nn−ε\nn\u0011\n-PROP1 in the presence of prediction error. Unfortunately, unlike\ntypical guarantees of learning-augmented algorithms, we do not have robustness guarantees as the status of\nadvice-free PROP1 approximation remains unresolved.\nTo complete the picture, we prove that three natural and widely studied stronger fairness notions—EF1,\nMMS, and PROPX— cannotachieve any non-zero approximation, even with perfect MIV predictions. This\nbroadensknownimpossibilityresultsandunderscoresPROP1asameaningfulfairnessnotioninonlinesettings.\n2 Preliminaries and Related Work\nWe use the notation N+={1,2,3, . . .}to denote the set of positive integers (excluding zero), and R≥0for the\nset of non-negative real numbers. For any positive integer z, we write [z] :={1, . . . , z }. For any set A, we\ndenote its powerset, the set of all subsets of A, by2A.\nOur setting involves n≥2agentsand a sequence of m≥1indivisible goods G={g1, . . . , g m}that\narrive online, one at a time. Each good must be allocated immediately and irrevocably to one of the agents\nupon arrival. Crucially, the total number of goods mis not known in advance and is implicitly revealed only\nretrospectively once all goods have arrived. We label the goods in their arrival order, and for any time step\nt∈[m], we define G(t):={g1, . . . , g t}to be the set of the first tgoods observed. Thus, G(m)=Gdenotes the\nfull set once all goods have arrived. Each agent i∈[n]has a non-negative valuation function vi: 2G→R≥0,\nwhich assigns a value to any bundle of goods. Following standard assumptions in fair division, we assume that\nvaluations are additive: for any subset S⊆G, we have vi(S) =P\ng∈Svi({g}). For convenience, we write vi(g)\nas shorthand for vi({g}).\nAnallocation is a tuple A= (A1, . . . , A n), where each Ai⊆Gis the bundle allocated to agent i. The\nallocation must form a partition of the goods: the Aiare pairwise disjoint and their union equals G.\n2.1 Fairness Notions\nFor any time step t∈[m]and agent i∈[n], let A(t)\nidenote the bundle held by agent iafter the allocation\nof good gt, with A(0)\ni=∅as the initial allocation and A(m)\ni=Aias the final bundle. We are interested in\nonline algorithms that produce allocations satisfying approximate fairness guarantees. In particular, we focus\nonproportionality up to one good (PROP1) and its multiplicative relaxations.\nA classical proportional allocation ensures that each agent receives utility at least vi(G)/n, but such guar-\nantees are often unachievable even in the offline setting with two agents and one good. We thus focus on the\nmore permissive relaxation of PROP1 in this work, defined as follows.\nDefinition 1 (PROP1).An allocation Asatisfiesproportionality up to one good (PROP1) if for each agent\ni∈[n], either Ai=G, or there exists a good g∈G\\Aisuch that vi(Ai∪ {g})≥vi(G)\nn.\n3\n\nUnfortunately, it is known that even PROP1 is not always achievable in online settings [BKPP18, NPT25].\nThis motivates the study of multiplicative approximations to PROP1 as a more attainable fairness benchmark.\nFollowing prior work, we define the notion of α-PROP1 for α∈[0,1]: an allocation is α-PROP1 if, for each\nagent i∈[n], either Ai=G, or there exists g∈G\\Aisuch that vi(Ai∪ {g})≥α·vi(G)\nn. Note that 1-PROP1\nis equivalent to PROP1.\nWe also briefly consider three well-studied, stronger fairness notions: EF1, MMS, and PROPX; note that\nthese are not the focus of this paper, but we define them in order to (later) establish impossibility results\nand delineate the boundary of what is achievable. These notions are pairwise incomparable but each implies\nPROP1.\nDefinition 2 (EF1).An allocation A= (A1, . . . , A n)isenvy-free up to one good (EF1) if for every pair of\nagents i, j∈[n]with Aj̸=∅, there exists a good g∈Ajsuch that vi(Ai)≥vi(Aj\\ {g}).\nDefinition 3 (MMS).LetΠ(G)denote the set of all n-partitions of G. Themaximin share of agent i∈[n]\nis defined as MMS i:= max X∈Π(G)minj∈[n]vi(Xj). An allocation A= (A1, . . . , A n)is then said to be maximin\nshare(MMS) fair if vi(Ai)≥MMS ifor all i∈[n].\nDefinition 4 (PROPX) .An allocation Asatisfiesproportionality up to any item (PROPX) if for each agent\ni∈[n], we either have Ai=G, orvi(Ai∪ {g})≥vi(G)\nnfor all goods g∈G\\Ai.\nWhile these stronger fairness notions are attractive, they are known to be inapproximable in online settings\nunder minimal assumptions [NPT25, ZBW23].4We denote the approximate versions of these fairness notions\nasα-EF1, α-MMS, and α-PROPX in a similar fashion as α-PROP1.\n2.2 Maximum Item Value (MIV) predictions\nLater in Section 5, we study learning-augmented algorithms that have access to useful predictive information\nabout the instance. In particular, we focus on maximum item value (MIV) predictions. Given the set of all\ngoods G, let vmax\ni= max g∈Gvi(g)denote the maximum value agent i∈[n]assigns to any single good. We\nassume access to predictions pi≈vmax\ni, and let p= (p1, . . . , p n)denote the vector of MIV predictions for all\nagents.\nThis form of predictive input is considerably weaker than full knowledge of valuations or normalized value\nvectors, yet it can still meaningfully guide allocation decisions. We refer to the case where pi=vmax\nifor\nalli∈[n]as having perfect predictions . This model is in line with prior work that assumes oracle access\nto predicted information in online fair division [NPT25, ZBW23]. We also consider settings with one-sided\nbounded prediction error. Specifically, we assume that the predictions overestimate the true maximum value\nby at most a factor of1\n1−εfor some ε∈[0,1), and never underestimate it.\nDefinition 5 (MIV Predictions with One-Sided Errors) .The prediction vector phas one-sided error ε∈[0,1)\nif we have vmax\ni∈[(1−ε)·pi, pi]for each agent i∈[n].\nThis conservative (upper-biased) approach to prediction is motivated by both theory and practice. In\nonline learning and bandit algorithms, for example, upper confidence bounds (UCB) are a common strategy\nfor exploration under uncertainty [ACBF02]. Similarly, in inventory planning and demand forecasting, safety\nmargins are often introduced to guard against shortfalls, favoring overestimation over underestimation. In\nsuch settings, underestimating future values or demand is typically more costly than overestimating, making\none-sided prediction errors both natural and desirable.\n2.3 Types of Adversaries in Online Algorithms\nIn analyzing online algorithms, it is standard to distinguish between two types of adversaries: adaptive and\nnon-adaptive . An adaptive adversary can observe the algorithm’s internal randomness and past decisions,\nand respond dynamically by selecting future inputs to undermine performance. While adaptive adversaries\nmodel worst-case behavior, they often make meaningful fairness guarantees impossible in online allocation\nsettings. In contrast, a non-adaptive adversary commits to the full input sequence in advance, before any of\nthe algorithm’s random decisions are made. This restriction allows for meaningful probabilistic guarantees and\nis widely adopted when studying randomized algorithms.\n4PROPX was not explicitly considered in prior work, but the EFX impossibility results from [NPT25] extend naturally to\nPROPX as well.\n4\n\n2.4 Other Related Work\nWe discussed earlier two directly relevant works, [NPT25] and [ZBW23], which study online fair division with\npredictive information in the same model as ours. These works assume perfect predictive information. Here,\nwe highlight several other related lines of work that are related to the model more generally.\nLearning-Augmented Online Fair Division Since the seminal work of [LV21], there has been a surge of\ninterest in incorporating unreliable advice into algorithm design and analyzing performance as a function of\nadvice quality across various areas of computer science. This framework has been especially successful in online\noptimization, where the core challenge lies in handling uncertainty about future inputs. In this context, advice\ncan serve as a useful proxy for the unknown future. Most previous works in this setting are in the context\nof online algorithms, e.g. for the ski-rental problem [GP19, WLW20, ADJ+24], non-clairvoyant scheduling\n[PSK18], scheduling [LLMV20, BMRS20, AJS22], augmenting classical data structures with predictions (e.g.\nindexing [KBC+18] and Bloom filters [Mit18]), online selection and matching problems [AGKK23, DLPLV21,\nCGLB24], online TSP [BLMS+22, GLS23], and a more general framework of online primal-dual algorithms\n[BMS20]. However, there have been some recent applications to other areas, e.g. graph algorithms [CSVZ22,\nDIL+21], causallearning[CGB23], mechanismdesign[GKST22,ABG+22], anddistributionlearning[BCJG24].\nFor an overview of this growing area, we refer the reader to the survey by [MV22].5\nThere is a growing body of work on which leverage predictive information about future arrivals and analyze\nthe extent to which desirable fairness properties can be achieved, even when predictions may be inaccurate.\nHowever, most such works in the context of online fair division consider a model with divisible goods . It is\nimportant to note that fairness concepts and the structure of the model differ significantly between divisible\nand indivisible goods.\nIn the setting with indivisible goods (which is the focus of our work), [SE23] study the allocation of\nadvertising impressions (goods) to advertisers (agents), subject to cardinality constraints on agents’ bundle\nsizes, and with utilitarian social welfare as the objective. Other works such as [BGT23] and [CEEV24] explore\nlearning-augmented approaches focused on MMS and incentive-compatibility as desirable properties.\nOnline/Temporal Fair Division [AAGW15] study the online fair division problem modeled after a food\nbank charity problem, where agents are assumed to have binary valuations. They examine fairness properties\nsuch as envy-freeness as well as incentive compatibility. [BKPP18] focus on minimizing envy as an objective,\nwhile [ZP20] build on this by analyzing the trade-off between approximate envy-freeness and a notion of\neconomic efficiency. [HPPZ19] explore a variant of the online model whereby past allocations can be revisited\nand swapped. Several other works consider an online fair division model with divisible goods [BGGJ22,\nBGH+23, BKM22, HLSW23]. For a broader overview of earlier works in online fair division, see the survey by\n[AW20].\nAnotherrelatedmodelis temporal fairdivision. [ELL+25]considerasettingwherethevaluationsoverfuture\ngoods is known in advance, with the goal to achieve an EF1 allocation at every round prefix , a significantly\nstronger requirement. [CES25] study a similar model, but focus on stochastic dominant variants of PROP1\nand EF1.\nProportionality in Other Sequential Models Several other works study proportionality in the pub-\nlic goods model, where items are shared among agents rather than allocated privately [CFS17, ENT24]. A\nsignificantly stronger notion of proportionality, typically studied in the context of voting, has been investi-\ngated in the closely related model of temporal voting, which also features shared outcomes (items) over time\n[BHP+21, CGP24, EOPT25, PETW25].\n3 Greedy Fails in General\nIn this section, we study the natural class of greedy algorithms , which aim to allocate each arriving good in a\nway that attempts to satisfy a given fairness criterion as much as possible at the current timestep.\nIn the classic online setting without predictive information, the greedy approach is both intuitive and\narguably the only viable strategy. The key question then is, what properties or (non-predictive) information\ncan be leveraged to guide the greedy algorithm’s decisions?\n5See also https://algorithms-with-predictions.github.io/ .\n5\n\nPrior work that incorporates predictive information typically uses it to guide greedy algorithms that achieve\napproximate fairness guarantees. For instance, [ZBW23] proposed a greedy algorithm for n= 2that guarantees\n1/2-MMS under normalized valuations. Building on this, [NPT25] introduced a different greedy algorithm that\nsatisfies EF1 (and thus also 1/2-MMS) for n= 2, and achieves PROP1 for all n.\nGiven the intuitive appeal and historical success of greedy approaches in related settings, it is natural to\nconsider them as a potential means of achieving PROP1. However, we show that such strategies generally fail\nto provide any non-trivial approximation to the PROP1 ratio. Specifically, we analyze three natural greedy\nallocation strategies and demonstrate that none of them can guarantee α-PROP1, for any constant α >0.\nTo simplify the description of the greedy allocation strategies, we first introduce some useful notation. For\nany timestep t∈[m]and agent i∈[n], let A(t)\ni⊆G(t)denote the set of goods allocated to agent i, where\nG(t)={g1, . . . , g t} ⊆Gis the set of goods that have arrived up to timestep t. Let c(t)\ni= max {vi(g) :g∈\nG(t)\\A(t)\ni}(or0if the set is empty) denote the value of the most valuable unallocated good (among those\nthat have arrived so far) for agent iafterthe good gthas been allocated. We then define the (unnormalized)\nα-PROP1 value for agent iat timestep tas:\nα(t)\ni=vi(A(t)\ni) +c(t)\ni\nvi(G(t))(or∞ifvi(G(t)) = 0) .\nIn other words, the PROP1 ratio at timestep tis simply min(1 , n·mini∈[n]α(t)\ni). For the initial state when\nt= 0, we let G(0)=∅,c(0)\ni= 0, and define α(−1)\ni=∞. Then, the three greedy allocation strategies considered\nare as follows.\n•Greedy Strategy 1 : At timestep t, allocate good gtto an agent from argmaxi∈[n]vi(gt)\nvi(G(t)).\n•Greedy Strategy 2 : At timestep t, allocate good gtto an agent from argmini∈[n]vi(Ai)\nvi(G(t)).\n•Greedy Strategy 3 : At timestep t, allocate good gtto an agent from\nargmin\ni∈[n]vi(A(t−1)\ni) + max {c(t−1)\ni, vi(gt)}\nvi(G(t)).\nIntuitively, the first strategy allocates the arriving good gtto the agent which values it the most, the second\nstrategy allocates gtto the agent which is currently most unsatisfied, and the third strategy allocates gtto the\nagent who would become the most unsatisfied if notgiven gt. The following three results show that all of these\nnatural greedy strategies fail to achieve any non-zero approximation ratio to PROP1.\nProposition 6. Forn≥2and any α >0, there exists a sequence of marriving goods such that the Greedy\nStrategy 1 fails to produce an α-PROP1allocation.\nProof.Suppose vi(g1) = 1for all i∈[n]. Without loss of generality, suppose g1is assigned to agent 1. Suppose\nv1(gt) = 1andv2(gt) =1\n2for all subsequent goods, where t∈ {2, . . . , m }. For any t∈ {2, . . . , m }, observe\nthatv1(gt)\nv1(G(t))=1\nt>1/2\n1+(t−1)/2=v2(gt)\nv2(G(t)). Thus, agent 2willneverreceive any good, i.e., A2=∅. When\nm > 1 + 2(n\nα−1), we havev2(∅)+v2(g1)\nv2(G)=1\n1+m−1\n2<α\nn.\nProposition 7. Forn≥2and any α >0, there exists a sequence of marriving goods such that the Greedy\nStrategy 2 fails to produce an α-PROP1allocation.\nProof.Suppose vi(g1) = 1for all i∈[n]. Without loss of generality, suppose g1is assigned to agent 1. Suppose\nv1(gt) = 1andv2(gt) =1\nm2for all subsequent goods, where t∈ {2, . . . , m }. For any t∈ {2, . . . , m }, observe\nthatv2(A2)\nv2(G(t))≤t−1\nm2\n1+t−1\nm2=t−1\nm2+t−1<1\nt=v1(A1)\nv1(G(t)). Thus, agent 1willneverreceive any subsequent good, i.e.,\nA1={g1}. When m >n\nα, we havev1(g1)+v1(g2)\nv1(G)=2\nm<α\nn.\nProposition 8. Forn≥2and any α > 0, there exists a sequence of marriving goods such that Greedy\nStrategy 3 fails to produce an α-PROP1allocation.\n6\n\nProof sketch. The construction for Proposition 8 is rather complicated; we outline the key ideas here for the\ncase of n= 2and defer the full details to the appendix. In our construction, we guarantee that α(t)\n1̸=α(t)\n2\nfort≥3. Then, fix any timestep t≥4and suppose without loss of generality that α(t−1)\n1 < α(t−1)\n2. Let\nζ=c(t−1)\n2/(2v2(G(t−1))). We consider two cases.\n•Case 1:α(t−1)\n1 > α(t−1)\n2(1 + ζ).Then, we construct a sequence of τ≥1arriving goods such that\nα(t−1)\n1 =α(t−1+τ)\n1 ≤α(t−1+τ)\n2 (1 +ζ), where τis a function of ζ. That is, the PROP1 ratio remains the\nsame but the gap between the two agent’s αvalues decreases until they are within a 1 +ζmultiplicative\nfactor.\n•Case 2:α(t−1)\n1≤α(t−1)\n2(1 + ζ).Then, we define the arriving good gtto have v1(gt) =c(t−1)\n1and\nv2(gt) =c(t−1)\n2. Under Greedy Strategy 3, we can show that min{α(t)\n1, α(t)\n2}< α(t−1)\n1. That is, the\nPROP1 ratio is strictly decreased.\nBy alternating between these two cases, we ensure that the minimum PROP1 ratio continues to decrease over\ntime. Repeating this process yields an allocation whose PROP1 ratio becomes arbitrarily small, establishing\nthe claim.\nThese negative results highlight the fundamental limitations of greedy allocation strategies when facing\nadaptive adversaries. Thus, in the next two sections, we explore two approaches to overcome these barriers. In\nSection 4, we show that a random allocation strategy can achieve a non-trivial PROP1 approximation against\nnon-adaptive adversaries. In Section 5, we show that access to MIV predictions enables non-trivial PROP1\napproximations even against adaptive adversaries.\n4 Random Allocations\nWeinvestigatetheapproximatePROP1guaranteeachievedbythesimplealgorithm Rand, whichallocateseach\narriving good to an agent uniformly at random. [LMMS04] analyzed this algorithm in the offline fair division\nsetting, for its incentive-compatible property (i.e., no agent will have the incentive to misreport their valuations\nso as to obtain a strictly better outcome). Moreover, the uniformly random allocation rule is also a frequently\nused as a benchmark in the fair division literature, either as a point of comparison for fairness guarantees or as\na baseline for empirical evaluation [BFHN24, Bas18, HF23, Nes17, YKAI24]. Given the limited understanding\nof how to guarantee PROP1 in the online setting, this baseline provides especially valuable insights.\nAs mentioned earlier in the paper, to the best of our knowledge, this is the first multiplicative approximation\nbound for PROP1 in the online setting that holds withoutany additional assumptions (such as asymptotic\nbehavior, specific valuation classes, or constraints on the arrival order of items). Achieving such a guarantee\nis inherently challenging under adaptive adversaries (who tailor their inputs based on the algorithm’s random\ndecisions), often forcing worst-case outcomes that render fairness guarantees vacuous.\nIn contrast, studying non-adaptive adversaries offer a realistic and tractable framework for evaluating\nrandomized allocation rules in adversarial environments.\nWe analyze the performance of Randagainst a non-adaptive adversary. By linearity of expectation, Rand\nensuress that the expected value of each agent’s bundle is exactlyvi(G)\nn, implying that its expected competitive\nratio with respect to PROP1 is 1. However, this expectation-based guarantee is not particularly informative\nin isolation. A more meaningful perspective is to study the tail guarantee of Rand: fixing a failure probability\nδ >0, we ask: what is the largest αsuch that Randguarantees an α-PROP1 allocation with probability at\nleast 1−δ?\nTheorem 9. Fix any δ∈(0,1)andn≥2agents. Against a non-adaptive adversary, Randachieves\nΩ (1/log(n/δ))-PROP1with probability at least 1−δ.\nProof idea. Fix any agent i∈[n]and let α=27\n128 log( n/δ). If some single good gis already worth at least α·vi(G)\nn\nto agent i, then this is sufficient to guarantee that vi(Ai∪ {g})≥α·vi(G)\nn. Thus, the only interesting case is\nwhen every good is “small”, i.e., vi(gj)< α·vi(G)\nnfor all j∈[m]. Since under Rand, every good is allocated\ntoiwith probability1\nn, letS′\nibe the total value that other agents receive. Since the goods are small, S′\niis a\nsum of independent, bounded terms whose mean isn−1\nn·vi(G)with varianceα·vi(G)\nn2.\n7\n\nA careful application of Bernstein’s inequality then implies that the probability that S′\niexceeds its mean\nby1−α\nn·vi(G)is at most exp\u0000δ\nn\u0001\n. Applying a union bound over all nagents, the probability that anyagent\nfails this guarantee is at most δ. Thus, with probability 1−δ, the final allocation is α-PROP1.\n5 Maximum Item Value (MIV) Predictions\nIn this section, we investigate how maximum item value (MIV) predictions can aid in achieving (approxi-\nmate) PROP1 allocations in online settings. Prior work has studied the possibility of achieving PROP1 under\nstrong informational assumptions. In particular, while [NPT25] presented an online algorithm that guarantees\na PROP1 allocation, assuming access to normalization information about agents’ valuations. However, such\nas information that is often unavailable or unrealistic to obtain in many online settings. In contrast, MIV\npredictions require significantly less information . For each agent, we assume access only to a (possibly approx-\nimate) prediction of the value of their most-preferred good among the entire sequence of arriving goods. This\nrepresents a more “lightweight” and realistic informational assumption in the online setting.\nWe note that MIV predictions are less demanding than normalization information. The results of [NPT25]\nimplythatwhilePROP1isalwaysachievableundernormalizationinformation, itisnotalwaysachievableunder\nMIV predictions; the latter is a conclusion that can be derived from their analysis following their Theorem 3.3.\nSimilarly, while EF1 and 1/2-MMS are achievable for n= 2given normalization information [NPT25, ZBW23],\nthe following proposition shows that neither EF1 nor MMS can be approximated (even for n= 2) when only\nMIV predictions are available. For completeness, we also include results for PROPX, a natural strengthening\nof PROP1 and another relaxation of proportionality (see Definition 4).\nProposition 10. Forn≥2and any α > 0, there is no online algorithm can produce an allocation that is\nα-EF1,α-MMS, orα-PROPX, even with perfect MIVpredictions.\nGiven these relatively lightweight predictions, we first show that under accurate MIV predictions, it is\npossible to design an online algorithm that achieves a finite approximation ratio for PROP1, which scales\ninversely with the number of agents. Without loss of generality, we assume p= (p1, . . . , p m) = (1 , . . . , 1); for\narbitrary p, one can normalize the valuations by dividing each vi(gt)bypi, which preserves the structure of the\nproblem. We introduce the following algorithm (Algorithm 1) that is based on the following potential function,\nfor each agent i∈[n]:\nϕt\ni=a(t)\ni\n(n2+ 1)·a(t)\ni+n2·vi(Ai\\ {gri})·a(t)\ni−1.\nBefore stating the theorem statement and delving into the provided proof sketch, we would like to give\nsome intuition behind our potential function.\nFirstly, the quantity a(t)\niin our potential function roughly corresponds to the value of the “one additional\ngood\" afforded by the PROP1 definition. Crucially, even before agent i’s predicted maximum-valued item\narrives, we anticipate its arrival and set a(t)\ni=1\n1+vi(G). By accounting in advance for the arrival of gri, the\nalgorithm can avoid allocating too many of the earlier goods to agent i, especially if the goods preceding gri\nare of low value.\nNext, the potential function ϕt\nimeasures how close agent iis to violating the1\nn-PROP1 guarantee. A lower\npotential value indicates a more favorable state. We visualize how ϕt\nibehaves with respect to different values\nofa(t)\niand the product vi(Ai\\ {ri})·a(t)\ni.\nThis potential function captures two key intuitions: (1) when vi(Ai\\ {ri})·a(t)\niis large, it is better for a(t)\ni\nto be small, indicating that future goods are relatively small compared to vi(G), and thus will not significantly\nreduce agent i’s surplus; (2) conversely, when vi(Ai\\ {ri})·a(t)\niis small, it is preferable for a(t)\nito be large,\nensuring that the algorithm does not need to allocate a low-valued good to agent imerely to avoid violating\nthe1\nn-PROP1 condition.\nThen, our result is as follows.\nTheorem 11. Given the MIVpredictions p, Algorithm 1 returns a1\nn-PROP1 allocation.\nProof sketch. Let the global potential function be Φt=P\ni∈[n]ϕt\ni. The proof relies on two key properties of\nthe potential function (which we prove formally in the full version), both of which directly lead to the result:\n1. If Φm≤Φ0=1\nn+1, then the allocation Areturned by Algorithm 1 satisfies1\nn-PROP1.\n8\n\nAlgorithm 1 Returns a1\nn-PROP1 allocation given MIV predictions p= (p1, . . . , p n) = (1 , . . . , 1)\n1:Initialize the empty allocation A= (A1, . . . , A n)where Ai←∅for each i∈[n]\n2:ri← ∞for each i∈[n]\n3:whilethere exists a good gtarriving online do\n4:foreach agent i∈[n]do\n5:ifvi(gt) = 1andri> tthen\n6: ri←t\n7:end if\n8:ift < r ithen\n9: a(t)\ni←1\n1+vi(G(t)),\n10: b(t)\ni←a(t)\ni\n(n2+n+1)·a(t)\ni+n2·vi(Ai)·a(t)\ni−1\n11: c(t)\ni←a(t)\ni\n(n2+n+1)·a(t)\ni+n2·vi(Ai∪{gt})·a(t)\ni−1\n12: else\n13: a(t)\ni←1\nvi(G(t)),\n14: b(t)\ni←a(t)\ni\n(n2+n+1)·a(t)\ni+n2·vi(Ai\\{gri})·a(t)\ni−1\n15: c(t)\ni←a(t)\ni\n(n2+n+1)·a(t)\ni+n2·vi(Ai∪{gt}\\{gri})·a(t)\ni−1\n16: end if\n17:end for\n18:Leti∗∈arg min i∈[n]\u0010\nc(t)\ni+P\nj∈[n],j̸=ib(t)\nj\u0011\n19: Ai∗←Ai∗∪ {gt}\n20:end while\n21:return A= (A1, . . . , A n)\n2. For every t∈[m], there always exists an agent to whom gtcan be allocated such that Φt≤Φt−1.\nSince the algorithm selects, at each step, the agent to allocate gtto, such that Φtis minimized, the second\nproperty guarantees that the potential never increases: Φt≤Φt−1for all t, and thus Φt≤Φ0. Combined with\nthe first property, this implies the desired1\nn-PROP1 guarantee.\nTo show the second property, let ϕ+\nidenote the increase in agent i’s potential function if they do notreceive\ngt, and let ϕ−\nibe the decrease in their potential if they do receive gt. The key idea is to show that for every\nagent i,(n−1)·ϕ+\ni≤ϕ−\ni. This inequality implies that the decrease in potential for at least one agent (upon\nreceiving gt) outweighs the total increase in potential across all other agents if they were not allocated the\ngood. Hence, at each step, there always exists an agent whose allocation of gtensures the global potential does\nnot increase.\nNext, we extend our results to account for one-sided error in the MIV predictions—i.e., when each agent’s\npredicted MIV overestimates their true maximum value. The following theorem shows that any algorithm\ndesigned for perfect predictions can be made robust to such errors, while degrading gracefully in its guarantees.\nNote that the approximation ratio achieved is non-trivial as long as ε <1and smoothly recovers the original\napproximation ratio when ε= 0.\nTheorem 12. Letα∈[0,1]and suppose there exists an online algorithm that, given the perfect MIVpre-\ndictions, always outputs an α-PROP1 allocation. For any ε∈[0,1), there is an online algorithm which,\ngiven the MIVpredictions whose one–sided error is at most ε, always returns a β-PROP1allocation with\nβ=α(1−ε)/(1−αε\nn).\nProof sketch. We construct an algorithm that adapts to overestimated MIV predictions by modifying each\nagent’s valuation function in a controlled way. Specifically, for each agent, we artificially increase the value of\ntheir first good (in the arrival order) that has value at least 1−εto exactly 1. This adjustment affects at most\none good per agent and can be performed in an online manner. We then run the original α-PROP1 algorithm\nusing these augmented valuations. By assumption, this yields an allocation Athat is α-PROP1 with respect\nto the augmented valuations.\n9\n\nFigure 1: ϕt\niat different values of a(t)\niandvi(Ai\\ {ri})·a(t)\ni.\nThen, since we only increased the value of a single good per agent, both the agents’ valuation for their\nbundle and their total valuation for the set of goods may decrease, but by at most ε. Moreover, under the\naugmented valuations, each agent’s maximum value is exactly 1. A straightforward calculation (using the fact\nthat ε <1) shows that the degradation in the PROP1 guarantee is bounded, and the allocation is in fact\nβ-PROP1 under the original valuations.\nBy combining Theorem 11 and Theorem 12, we obtain the following corollary, which provides a robustness\nguarantee for our algorithm under one-sided prediction error.\nCorollary 13. For any ε∈[0,1), there is an online algorithm which, given the MIVpredictions whose\none–sided error is at most ε, always returns a β-PROP1allocation with β=(1−ε)\n(n−ε\nn).\n6 Conclusion\nIn this work, we initiate a study of approximate proportionality in the online fair division mode, a setting where\nmany classic fairness notions often admit strong impossibility results. Our results present both fundamental\nlimitations and promising algorithmic directions.\nOn the negative side, we show that several natural greedy algorithms fail to provide any PROP1 guarantee\nin adversarial environments, highlighting the challenges of online decision making under uncertainty.\nOnthepositiveside, wedemonstratethatrandomallocationachievesmeaningfulfairnessguaranteesagainst\nnon-adaptiveadversaries, andthatlightweightpredictiveinformationsuchasestimatesofmaximumitemvalues\ncan admit robust PROP1 approximations against adaptive adversaries.\nSeveral important questions remain open. Chief among them is whether any non-trivial approximation\nto PROP1 is achievable withoutpredictive or structural assumptions. Beyond known impossibility results,\nno approximation bounds are currently established in this regime, and resolving this question remains a key\nchallenge for future work.\nReferences\n[AAGW15] Martin Aleksandrov, Haris Aziz, Serge Gaspers, and Toby Walsh. Online fair division: Analysing\na food bank problem. In Proceedings of the 24th International Joint Conference on Artificial\nIntelligence (IJCAI) , pages 2540–2546, 2015.\n[ABG+22] Priyank Agrawal, Eric Balkanski, Vasilis Gkatzelis, Tingting Ou, and Xizhi Tan. Learning-\naugmented mechanism design: Leveraging predictions for facility location. In Proceedings of the\n23rd ACM Conference on Economics and Computation (EC) , pages 497–528, 2022.\n10\n\n[ACBF02] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit\nproblem. Machine learning , 47(2):235–256, 2002.\n[ACIW19] Haris Aziz, Ioannis Caragiannis, Ayumi Igarashi, and Toby Walsh. Fair allocation of indivis-\nible goods and chores. In Proceedings of the 28th International Joint Conference on Artificial\nIntelligence (IJCAI) , pages 53–59, 2019.\n[ADJ+24] Spyros Angelopoulos, Christoph Dürr, Shendan Jin, Shahin Kamali, and Marc Renault. Online\nComputation with Untrusted Advice. Journal of Computer and System Sciences , 144:103545,\n2024.\n[AGKK23] Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and Online\nMatching Problems with Machine Learned Advice. Discrete Optimization , 48(2):100778, 2023.\n[AJS22] Antonios Antoniadis, Peyman Jabbarzade, and Golnoosh Shahkarami. A Novel Prediction Setup\nfor Online Speed-Scaling. In Proceedings of the 18th Scandinavian Symposium and Workshops on\nAlgorithm Theory (SWAT) , pages 9:1–9:20, 2022.\n[AMNS17] Georgios Amanatidis, Evangelos Markakis, Afshin Nikzad, and Amin Saberi. Approximation\nalgorithms for computing maximin share allocations. ACM Transactions on Algorithms , 13(4):1–\n28, 2017.\n[AMS20] Haris Aziz, Hervé Moulin, and Fedor Sandomirskiy. A polynomial-time algorithm for computing\na pareto optimal and almost proportional allocation. Operations Research Letters , 48(5):573–578,\n2020.\n[AW20] Martin Aleksandrov and Toby Walsh. Online Fair Division: A Survey. In Proceedings of the 34th\nAAAI Conference on Artificial Intelligence (AAAI) , pages 13557–13562, 2020.\n[Bas18] Christian Basteck. Fair solutions to the random assignment problem. Journal of Mathematical\nEconomics , 79:163–172, 2018.\n[BCJG24] Arnab Bhattacharyya, Davin Choo, Philips George John, and Themis Gouleakis. Learning mul-\ntivariate gaussians with imperfect advice. arXiv preprint arXiv:2411.12700 , 2024.\n[BFHN24] Yakov Babichenko, Michal Feldman, Ron Holzman, and Vishnu V. Narayan. Fair division via\nquantile shares. In Proceedings of the 56th Annual ACM Symposium on Theory of Computing\n(STOC), pages 1235–1246, 2024.\n[BGGJ22] Siddhartha Banerjee, Vasilis Gkatzelis, Artur Gorokh, and Billy Jin. Online nash social welfare\nmaximization with predictions. In Proceedings of the 2022 Annual ACM-SIAM Symposium on\nDiscrete Algorithms (SODA) , pages 1–19, 2022.\n[BGH+23] Siddhartha Banerjee, Vasilis Gkatzelis, Safwan Hossain, Billy Jin, Evi Micha, and Nisarg Shah.\nProportionally fair online allocation of public goods with predictions. In Proceedings of the 32nd\nInternational Joint Conference on Artificial Intelligence (IJCAI) , pages 20–28, 2023.\n[BGT23] Eric Balkanski, Vasilis Gkatzelis, and Xizhi Tan. Strategyproof scheduling with predictions. In\nProceedings of the 14th Innovations in Theoretical Computer Science (ITCS) , page 11:1–11:22,\n2023.\n[BHP+21] Laurent Bulteau, Noam Hazon, Rutvik Page, Ariel Rosenfeld, and Nimrod Talmon. Justified\nrepresentation for perpetual voting. IEEE Access , 9:96598–96612, 2021.\n[BK19] Siddharth Barman and Sanath Kumar Krishnamurthy. On the proximity of markets with integral\nequilibria. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI) , pages\n1748–1755, 2019.\n[BKM22] Siddharth Barman, Arindam Khan, and Arnab Maiti. Universal and tight online algorithms for\ngeneralized-mean welfare. In Proceedings of the 36th AAAI Conference on Artificial Intelligence\n(AAAI), pages 4793–4800, 2022.\n11\n\n[BKPP18] Gerdus Benadè, Aleksandr M. Kazachkov, Ariel D. Procaccia, and Christos-Alexandros Psomas.\nHow to make envy vanish over time. In Proceedings of the 19th ACM Conference on Economics\nand Computation (EC) , pages 593–610, 2018.\n[BLMS+22] Giulia Bernardini, Alexander Lindermayr, Alberto Marchetti-Spaccamela, Nicole Megow, Leen\nStougie, and Michelle Sweering. A Universal Error Measure for Input Predictions Applied to\nOnline Graph Problems. In Advances in Neural Information Processing Systems , 2022.\n[BMRS20] Étienne Bamas, Andreas Maggiori, Lars Rohwedder, and Ola Svensson. Learning Augmented\nEnergy Minimization via Speed Scaling. Advances in Neural Information Processing Systems ,\n33:15350–15359, 2020.\n[BMS20] Etienne Bamas, Andreas Maggiori, and Ola Svensson. The Primal-Dual method for Learning\nAugmented Algorithms. Advances in Neural Information Processing Systems , 33:20083–20094,\n2020.\n[BS24] Simina Brânzei and Fedor Sandomirskiy. Algorithms for competitive division of chores. Mathe-\nmatics of Operations Research , 49(1):398–429, 2024.\n[Bud11] Eric Budish. The combinatorial assignment problem: Approximate competitive equilibrium from\nequal incomes. Journal of Political Economy , 119(6):1061–1103, 2011.\n[CEEV24] Ilan Reuven Cohen, Alon Eden, Talya Eden, and Arsen Vasilyan. Plant-and-steal: Truthful\nfair allocations via predictions. In Proceedings of the 37th International Conference in Neural\nInformation Processing Systems (NeurIPS) , pages 110057–110096, 2024.\n[CES25] Benjamin Cookson, Soroush Ebadian, and Nisarg Shah. Temporal fair division. In Proceedings of\nthe 39th AAAI Conference on Artificial Intelligence (AAAI) , pages 13727–13734, 2025.\n[CFS17] Vincent Conitzer, Rupert Freeman, and Nisarg Shah. Fair public decision making. In Proceedings\nof the 18th ACM Conference on Economics and Computation (EC) , pages 629–646, 2017.\n[CGB23] DavinChoo, ThemistoklisGouleakis, andArnabBhattacharyya. ActiveCausalStructureLearning\nwith Advice. In Proceedings of the 40th International Conference on Machine Learning (ICML) ,\npages 5838–5867, 2023.\n[CGLB24] Davin Choo, Themistoklis Gouleakis, Chun Kai Ling, and Arnab Bhattacharyya. Online Bipartite\nMatching with Imperfect Advice. In Proceedings of the 41st International Conference on Machine\nLearning (ICML) , pages 8762–8781, 2024.\n[CGP24] Nikhil Chandak, Shashwat Goel, and Dominik Peters. Proportional aggregation of preferences for\nsequential decision making. In Proceedings of the 38th AAAI Conference on Artificial Intelligence\n(AAAI), pages 9573–9581, 2024.\n[CKM+19] Ioannis Caragiannis, David Kurokawa, Hervé Moulin, Ariel D. Procaccia, Nisarg Shah, and Junx-\ning Wang. The unreasonable fairness of maximum Nash welfare. ACM Transactions on Economics\nand Computation , 7(3):12:1–12:32, 2019.\n[CSVZ22] JustinChen, SandeepSilwal, AliVakilian, andFredZhang. FasterFundamentalGraphAlgorithms\nvia Learned Predictions. In Proceedings of the 39th International Conference on Machine Learning\n(ICML), pages 3583–3602, 2022.\n[DIL+21] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster\nMatchings via Learned Duals. Advances in Neural Information Processing Systems , 34:10393–\n10406, 2021.\n[DLPLV21] Paul Dütting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. Secretaries with Advice.\nInProceedings of the 22nd ACM Conference on Economics and Computation (EC) , pages 409–429,\n2021.\n[DP09] Devdatt P Dubhashi and Alessandro Panconesi. Concentration of measure for the analysis of\nrandomized algorithms . Cambridge University Press, 2009.\n12\n\n[ELL+25] Edith Elkind, Alexander Lam, Mohamad Latifian, Tzeh Yuan Neoh, and Nicholas Teh. Tem-\nporal fair division of indivisible items. In Proceedings of the 24th International Conference on\nAutonomous Agents and Multiagent Systems (AAMAS) , pages 676–685, 2025.\n[ENT24] Edith Elkind, Tzeh Yuan Neoh, and Nicholas Teh. Temporal elections: Welfare, strategyproofness,\nand proportionality. In Proceedings of the 27th European Conference on Artificial Intelligence\n(ECAI), pages 3292–3299, 2024.\n[EOPT25] Edith Elkind, Svetlana Obraztsova, Jannik Peters, and Nicholas Teh. Verifying proportionality in\ntemporal voting. In Proceedings of the 39th AAAI Conference on Artificial Intelligence (AAAI) ,\npages 13805–13813, 2025.\n[GKST22] Vasilis Gkatzelis, Kostas Kollias, Alkmini Sgouritsa, and Xizhi Tan. Improved Price of Anarchy via\nPredictions. In Proceedings of the 23rd ACM Conference on Economics and Computation (EC) ,\npages 529–557, 2022.\n[GLS23] Themis Gouleakis, Konstantinos Lakis, and Golnoosh Shahkarami. Learning-Augmented Algo-\nrithms for Online TSP on the Line. In Proceedings of the 37th AAAI Conference on Artificial\nIntelligence (AAAI) , pages 11989–11996, 2023.\n[GP19] Sreenivas Gollapudi and Debmalya Panigrahi. Online Algorithms for Rent-or-Buy with Expert\nAdvice. In Proceedings of the 36th International Conference on Machine Learning (ICML) , pages\n2319–2327, 2019.\n[HF23] Homa Hosseinzadeh Ranjbar and Mehdi Feizi. Random assignments with uniform preferences: An\nimpossibility result. Operations Research Letters , 51(3):304–307, 2023.\n[HLSW23] Zhiyi Huang, Minming Li, Xinkai Shu, and Tianze Wei. Online nash welfare maximization without\npredictions. In Proceedings of the 19th Conference on Web and Internet Economics (WINE) , pages\n402–419, 2023.\n[HPPZ19] Jiafan He, Ariel Procaccia, Alexandros Psomas, and David Zeng. Achieving a fairer future by\nchanging the past. In Proceedings of the 28th International Joint Conference on Artificial Intelli-\ngence (IJCAI) , pages 343–349, 2019.\n[KBC+18] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The Case for Learned\nIndex Structures. In Proceedings of the 2018 International Conference on Management of Data\n(SIGMOD) , pages 489–504, 2018.\n[KPW18] David Kurokawa, Ariel D Procaccia, and Junxing Wang. Fair enough: Guaranteeing approximate\nmaximin shares. Journal of the ACM (JACM) , 65(2):1–27, 2018.\n[LLMV20] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online Scheduling\nvia Learned Weights. In Proceedings of the 14th Annual ACM-SIAM Symposium on Discrete\nAlgorithms (SODA) , pages 1859–1877. SIAM, 2020.\n[LMMS04] Richard J. Lipton, Evangelos Markakis, Elchanan Mossel, and Amin Saberi. On approximately\nfair allocations of indivisible goods. In Proceedings of the 5th ACM Conference on Electronic\nCommerce (EC) , pages 125–131, 2004.\n[LV21] Thodoris Lykouris and Sergei Vassilvitskii. Competitive Caching with Machine Learned Advice.\nJournal of the ACM (JACM) , 68(4):1–25, 2021.\n[Mit18] Michael Mitzenmacher. A Model for Learned Bloom Filters, and Optimizing by Sandwiching.\nAdvances in Neural Information Processing Systems , 31, 2018.\n[MV22] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with Predictions. Communications of\nthe ACM , 65(7):33–35, 2022.\n[Nes17] Alexander S. Nesterov. Fairness and efficiency in strategy-proof object allocation mechanisms.\nJournal of Economic Theory , 170:145–168, 2017.\n13\n\n[NPT25] TzehYuanNeoh,JannikPeters,andNicholasTeh. Onlinefairdivisionwithadditionalinformation.\narXiv preprint arXiv:2505.24503 , 2025.\n[PETW25] Bradley Phillips, Edith Elkind, Nicholas Teh, and Tomasz Wąs. Strengthening proportionality in\ntemporal voting. arXiv preprint arXiv:2505.22513 , 2025.\n[Pro20] Ariel Procaccia. Technical perspective: An answer to fair division’s most enigmatic question.\nCommunications of the ACM , 63(4):118, 2020.\n[PSK18] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving Online Algorithms via ML Predic-\ntions.Advances in Neural Information Processing Systems , 31, 2018.\n[SE23] Fabian Spaeh and Alina Ene. Online ad allocation with predictions. In Proceedings of the 37th\nInternational Conference on Neural Information Processing Systems (NeurIPS) , pages 17265–\n17295, 2023.\n[WLW20] Shufan Wang, Jian Li, and Shiqiang Wang. Online Algorithms for Multi-shop Ski Rental with\nMachine Learned Advice. Advances in Neural Information Processing Systems , 33:8150–8160,\n2020.\n[YKAI24] Hakuei Yamada, Junpei Komiyama, Kenshi Abe, and Atsushi Iwasaki. Learning fair division from\nbandit feedback. In Proceedings of the 27th International Conference on Artificial Intelligence and\nStatistics (AISTATS) , pages 3106–3114, 2024.\n[ZBW23] Shengwei Zhou, Rufan Bai, and Xiaowei Wu. Multi-agent online scheduling: MMS allocations\nfor indivisible items. In Proceedings of the 40th International Conference on Machine Learning\n(ICML), pages 42506–42516, 2023.\n[ZP20] David Zeng and Alexandros Psomas. Fairness-efficiency tradeoffs in dynamic fair division. In\nProceedings of the 21st ACM Conference on Economics and Computation (EC) , pages 911–912,\n2020.\n14\n\nA Omitted Proofs from Section 3\nA.1 Proof of Proposition 8\nLemma 14. Suppose goods have been allocated up to and including timestep t∈N+. Assume that vj(G(t))>0\nandcj:=c(t)\nj>0for all j∈[n], and that there is some i∈[n]such that α(t)\ni< α(t)\njfor all j̸=i. Then, for\nany nonempty subset S⊆[n]not containing i, there exist a nonnegative integer τand a sequence of τgoods\ngt+1, gt+2,···, gt+τsuch that Greedy Strategy 3 must give:\n•c(t+τ)\nj =c(t)\njandvj(A(t+τ)\nj) =vj(A(t)\nj)for all j∈[n],\n•vj(G(t+τ)) =vj(G(t))andα(t+τ)\nj =α(t)\njforj∈[n]\\S, and\n•vj(G(t+τ)) =vj(G(t)) +cj\n2τjandα(t)\ni< α(t+τ)\nj≤α(t)\ni(1 +δ(t+τ)\nj)forj∈S, where\nτj:=⌈2\ncjvj(G(t))(α(t)\nj/α(t)\ni−1)⌉ −1\nandδ(t′)\nj:=cj/(2vj(G(t′)))fort′≥t.\nIn fact, we may take τ= max j∈Sτj.\nProof.For all jand all t′≥t, we inductively define the good gt′+1to have valuation\nvj(gt′+1) =(\ncj/2ifj∈Sandα(t′)\nj> α(t)\ni(1 +δ(t′)\nj),\n0otherwise .\nFirst, notice that vj(gt′+1)< cj=c(t)\njfor all t′≥t, soc(t′)\nj=cjby definition. Also, because vj(gt′+1) = 0for\nallt′≥tand all j∈[n]\\S, it follows that vj(A(t′)\nj) =vj(A(t)\nj)andvi(G(t′)) =vi(G(t))andα(t′)\nj=α(t)\njfor all\nsuch t′, j. Moreover, observe that if α(t′)\nj≤α(t)\ni(1 +δ(t′)\nj)for some j∈Sandt′≥t, then α(t′′)\nj≤α(t)\ni(1 +δ(t′′)\nj)\nforallt′′≥t′by induction on t′′, using that vj(gt′′+1) = 0implies α(t′′+1)\nj =α(t′′)\njandδ(t′′+1)\nj =δ(t′′)\njby\ndefinition.\nFor this sequence of goods, we will show that whenever t′≥t, the statement P(t′)that α(t)\ni< α(t′)\njfor\nj̸=iand the statement Q(t′)that the good gt′+1is given to agent iare always true. Note that P(t)holds by\nassumption.\nThe condition for Q(t′)to hold, i.e., for gt′+1to be allocated to agent i, is:\nvi(A(t′)\ni) +ci\nvi(G(t′)) +vi(gt′+1)<vj(A(t′)\nj) +cj\nvj(G(t′)) +vj(gt′+1)(1)\nfor all j̸=i. Noting that vi(gt′+1) = 0, this is equivalent to\nvi(A(t′)\ni) +ci\nvi(G(t′))<vj(A(t′)\nj) +cj\nvj(G(t′))vj(G(t′))\nvj(G(t′)) +vj(gt′+1),\ni.e.,α(t′)\ni< α(t′)\nj/(1 +vj(gt′+1)/vj(G(t′)))for all j̸=i. Ifj̸=iis such that vj(gt′+1) = 0, then this simplifies\ntoα(t)\ni< α(t′)\nj(given the observation that α(t)\ni=α(t′)\ni), which will be true if P(t′)is. If instead j̸=iis such\nthatvj(gt′+1) =cj/2(i.e., j∈Sandα(t′)\nj> α(t)\ni(1 +δ(t′)\nj)), then the inequality instead simplifies to the true\nstatement α(t′)\nj> α(t)\ni(1 +cj/(2vj(G(t′))))(noting cj/(2vj(G(t′))) =δ(t′)\nj). Thus Q(t′)follows from P(t′).\nAssuming Q(t′)holds, it means the inequality in Eq. (1) holds for all j̸=i. As noted earlier, the LHS is\nsimply α(t)\ni, while the RHS is in fact α(t′+1)\njsince gt′+1was indeed not allocated to agent j. Hence P(t′+ 1)\nholds.\nThus, P(t′)andQ(t′)always hold for all t′≥t. In particular, the goods are always allocated to agent i\nand we have vj(A(t′)\nj) =vj(A(t)\nj)for all j∈[n](noting all goods have value 0to agent i).\nFixj∈S. As noted earlier, for any t′≥t, ifα(t′)\nj≤α(t)\ni(1 + δ(t′)\nj)holds, then the same holds when\nwe replace t′by any t′′≥t′. Let τ′\njbe the smallest nonnegative integer (or ∞if none exists) such that\n15\n\nα(t+τ′\nj)\nj ≤α(t)\ni(1 + δ(t+τ′\nj)\nj ), soα(t′)\nj> α(t)\ni(1 + δ(t′)\nj)is true precisely when t≤t′< t+τ′\nj. This means\nvj(gt′+1) = cj/2fort′∈ {t, t+ 1,···, t+τ′\nj−1}6andvj(gt′+1) = 0otherwise. Thus, for any (finite)\nnonnegative integer σ, if we define σj:= min( τ′\nj, σ), then the statement α(t+σ)\nj > α(t)\ni(1 +δ(t+σ)\nj)is equivalent\nto\nvj(A(t)\nj) +cj\nvj(G(t)) +σjcj/2> α(t)\ni\u0012\n1 +cj\n2(vj(G(t)) +σjcj/2)\u0013\n.\nMultiplying both sides by vj(G(t))+σjcj/2and using vj(A(t)\nj)+cj=α(t)\njvj(G(t))gives the equivalent statement\nα(t)\njvj(G(t))> α(t)\ni(vj(G(t)) +σjcj/2 +cj/2),\nwhich simplifies to the equivalent\nσj<2\ncjvj(G(t))(α(t)\nj/α(t)\ni−1)−1.\nBut the statement α(t+σ)\nj > α(t)\ni(1 +δ(t+σ)\nj)is also equivalent to t≤t+σ < t +τ′\nj, i.e., to σ < τ′\nj, which means\nwe have the equivalence\nσ < τ′\nj⇐⇒ min(τ′\nj, σ)<2\ncjvj(G(t))(α(t)\nj/α(t)\ni−1)−1.\nIn particular, this means τ′\nj̸=∞, because otherwise the left side of the equivalence will always hold but the\nright side of the equivalence will fail for sufficiently large σ. Moreover, taking σ=τ′\njgives\nτ′\nj≥2\ncjvj(G(t))(α(t)\nj/α(t)\ni−1)−1.\nIfτ′\nj>0, we may take σ=τ′\nj−1to get\nτ′\nj−1<2\ncjvj(G(t))(α(t)\nj/α(t)\ni−1)−1,\nfrom which we obtain\nτ′\nj=⌈2\ncjvj(G(t))(α(t)\nj/α(t)\ni−1)⌉ −1 =:τj.\nIfτ′\nj= 0, this last formula is still true, because the earlier inequality derived from taking σ= 0has already\nshown2\ncjvj(G(t))(α(t)\nj/α(t)\ni−1)≤1, but this quantity is clearly strictly positive since α(t)\nj> α(t)\ni.\nThus, for any j∈S, we have α(t′)\nj≤α(t)\ni(1 +δ(t′)\nj)andvj(G(t′)) =vj(G(t)) +cj\n2τjwhenever t′≥t+τj. As\nsuch, the lemma holds if we take τ= max j∈Sτjand goods gt+1,···, gt+τas defined at the beginning of the\nproof.\nProposition 8. Forn≥2and any α > 0, there exists a sequence of marriving goods such that Greedy\nStrategy 3 fails to produce an α-PROP1allocation.\nProof.It suffices to prove that there is an infinite sequence of goods g1, g2,···such that lim inf t→∞α(t)= 0,\nwhere α(t):= min i∈[n]α(t)\ni.\nWe begin by setting vi(g1) = 1for all i∈[n]. Without loss of generality, suppose the good g1is given to\nagent1. Next, for t∈ {2,3}, setvi(gt) = 1fori∈ {1,2}andvi(g2) = 0otherwise. Astraightforwardcalculation\nshows that the good g2must be given to agent 2, and that g3must be given to either agent 1 or 2. Without\nloss of generality, suppose g3is given to agent 1. Under this setup, 1 =α(3)\n1> α(3)\n2= 2/3, with v1(A(3)\n1) = 2,\nv2(A(3)\n2) = 1,v1(G(3)) =v2(G(3)) = 3andc(3)\n1=c(3)\n2= 1. Also, vi(A(3)\ni) = 0andα(3)\ni=c(3)\ni=vi(G(3)) = 1\nfor all i≥3.\nWe now inductively define the goods g4, g5,···and an infinite sequence of time steps 3 =t0< t1< t2<···\nsuch that for each nonnegative integer k,\n1.vr(A(tk)\nr) =vr(A(t0)\nr),c(tk)\nr=c(t0)\nr,vr(G(tk)) =vr(G(t0)), and α(tk)\nr=α(t0)\nrfor all r≥3,\n2.vr(A(tk)\nr)≤vr(A(t0)\nr) +kc(t0)\nrandc(tk)\nr=c(t0)\nrfor all r∈ {1,2}, and\n6The set {t, t+ 1,···, t+τ′\nj−1}is the empty set when τj= 0and is the infinite set {t, t+ 1, t+ 2,···}when τj=∞.\n16\n\n3. there is some i∈ {1,2}such that α(tk)\ni< α(tk)\nrfor all r̸=i. (So α(tk)=α(tk)\ni.)\nThese conditions are met for k= 0, with i= 2.\nFork≥0, assume that we have defined t0,···, tkand goods up to and including gtk. The conditions of\nLemma 14 are met at time step tk, so taking S={3−i}7in the lemma produces a sequence of\nτk:=⌈2\nc(tk)\n3−iv3−i(G(tk))(α(tk)\n3−i/α(tk)\ni−1)⌉ −1≥0\ngoods gtk+1, gtk+2,···, gtk+τksuch that:\n•c(tk+τk)\nr =c(tk)\nr=c(t0)\nrandvr(A(tk+τk)\nr ) =vr(A(tk)\nr)for all r∈[n],\n•vr(G(tk+τk)) =vr(G(tk))andα(tk+τk)\nr =α(tk)\nrforr̸= 3−i, and\n•v3−i(G(tk+τk)) =v3−i(G(tk)) +c(tk)\n3−i\n2τkandα(tk)\ni< α(tk+τk)\n3−i≤α(tk)\ni(1 + δ(tk+τk)\n3−i), where δ(tk+τk)\n3−i :=\nc(tk)\n3−i/(2v3−i(G(tk+τk))).\nObserve that\nv3−i(G(tk+τk))\n=v3−i(G(tk)) +c(tk)\n3−i\n2τk\n≥v3−i(G(tk)) +c(tk)\n3−i\n2\u0012\n2v3−i(G(tk))\nc(tk)\n3−i\u0012\nα(tk)\n3−i\nα(tk)\ni−1\u0013\n−1\u0013\n=v3−i(G(tk))α(tk)\n3−i\nα(tk)\ni−c(tk)\n3−i\n2\n=v3−i(A(tk)\n3−i) +c(t0)\n3−i\nα(tk)\ni−c(t0)\n3−i\n2,\nand thus\nvr(G(tk+τk))≥vr(A(tk)\nr) +c(t0)\nr\nα(tk)\ni−c(t0)\nr\n2(2)\nfor both r∈ {1,2}, noting that this is true for r=isince the left-hand side is equal to the first term on the\nright-hand side.\nWe can now define good gtk+τk+1to be of value c(t0)\nrto each agent r∈ {1,2}and value 0to the other agents.\nThe greedy strategy must allocate the good to either agent 1 or 2, because the value of α(tk+τk+1)\nr should agent\nrnot be allocated the good will be α(tk+τk)\nr =α(tk)\nr> α(tk)\niforr≥3but will be less than α(tk+τk)\ni =α(tk)\nifor\nr=i(because the nonzero numerator in the calculation of the ratio remains the same but the denominator\nstrictly increases).\nIf the greedy strategy allocates gtk+τk+1to agent 3−i(rather than agent i), then we have\nα(tk+τk+1)\ni < α(tk+τk)\ni =α(tk)\ni< α(tk+τk)\n3−i≤α(tk+τk+1)\n3−i ,\ngiving us α(tk+τk+1)=α(tk+τk+1)\ni < α(tk). If instead the greedy algorithm allocates gtk+τk+1to agent i(rather\n7Note that 3−iis the element in {1,2}different from i.\n17\n\nthan agent 3−i), we will have\nα(tk+τk+1)\n3−i\n=v3−i(A(tk+τk)\n3−i)+c(tk+τk+1)\n3−i\nv3−i(G(tk+τk))+c(tk+τk+1)\n3−i\n=v3−i(A(tk+τk)\n3−i)+c(t0)\n3−i\nv3−i(G(tk+τk))+c(t0)\n3−i\n=α(tk+τk)\n3−iv3−i(G(tk+τk))\nv3−i(G(tk+τk))+c(t0)\n3−i\n≤α(tk)\ni(1 +δ(tk+τk)\n3−i)v3−i(G(tk+τk))\nv3−i(G(tk+τk))+c(t0)\n3−i\n=α(tk)\ni\u0012\n1 +c(t0)\n3−i\n2v3−i(G(tk+τk))\u0013\nv3−i(G(tk+τk))\nv3−i(G(tk+τk))+c(t0)\n3−i\n< α(tk)\ni\u0012\n1 +c(t0)\n3−i\nv3−i(G(tk+τk))\u0013\nv3−i(G(tk+τk))\nv3−i(G(tk+τk))+c(t0)\n3−i\n=α(tk)\ni=α(tk+τk)\ni ≤α(tk+τ+1)\ni ,\ngiving us α(tk+τk+1)=α(tk+τk+1)\n3−i < α(tk). Note that by the inequality in (2),\nvr(G(tk+τk+1)) =vr(G(tk+τk)) +c(t0)\nr\n≥vr(A(tk)\nr)+c(t0)\nr\nα(tk)\ni+c(t0)\nr\n2(3)\nforr∈ {1,2}.\nNow take tk+1=tk+τk+ 1> tk. We need to show that the three numbered conditions imposed earlier\ncontinue to hold for kreplaced with k+ 1. The first condition regarding the values of vr(A(tk+1)\nr ),vr(G(tk+1)),\nc(tk+1)\nr, and α(tk+1)\nrwhen r≥3can easily be verified since only goods of value 0are given to these agents.\nWe just showed that the third condition holds, with the new value of ibeing the agent (among 1 and 2) that\ndid not receive good gtk+τk+1. Finally, the second condition follows by induction, noting that none of the new\ngoods have value to agent r∈ {1,2}exceeding c(t0)\nrand that the only allocated good of nonzero value is good\ngtk+τk+1of value c(t0)\nrto agent r∈ {1,2}.\nThis completes the construction of the infinite sequence of goods g1, g2, g3,···.\nWe will now show that lim inf t→∞α(t)= 0. To this end, it suffices to show that limk→∞(1/α(tk)) =∞, for\nthen limk→∞α(tk)= 0.\nFixk≥0. Suppose that i, j∈ {1,2}satisfy α(tk)=α(tk)\ni< α(tk)\n3−iandα(tk+1)=α(tk+1)\nj < α(tk+1)\n3−j. Then\n1\nα(tk+1)=vj(G(tk+1))\nvj(A(tk+1)\nj)+c(t0)\nj\n≥ \nvj(A(tk)\nj)+c(t0)\nj\nα(tk)\ni+c(t0)\nj\n2!\nvj(A(tk)\nj)+c(t0)\nj\n=1\nα(tk)+c(t0)\nj/2\nvj(A(tk)\nj)+c(t0)\nj\n≥1\nα(tk)+c(t0)\nj/2\nvj(A(t0)\nj)+kc(t0)\nj+c(t0)\nj\n=1\nα(tk)+1\n2(vj(A(t0)\nj)/c(t0)\nj+k+1)\nwhere we applied the inequality in (3) at the second step. Taking λ:= max( v1(A(t0)\n1)/c(t0)\n1, v2(A(t0)\n2)/c(t0)\n2)\n(which is 2in our case), we see that\n1\nα(tk+1)−1\nα(tk)≥1\n2(k+ 1 + λ).\n18\n\nThe above holds for any k≥0, so a telescoping sum of these inequalities gives\n1\nα(tk)≥1\nα(t0)+kX\ns=11\n2(s+λ).\nThe right-hand side tends to ∞ask→ ∞, solimk→∞(1/α(tk)) =∞, as desired.\nB Omitted Proofs from Section 4\nB.1 Proof of Theorem 9\nFix an agent i∈[n]. Let α:=27\n128 log ( n/δ). We will first show that for g:= max g′∈G\\Aivi(g),\nPr\u0014\nvi(Ai∪ {g})< α·vi(G)\nn\u0015\n≤δ\nn. (4)\nOnce the above is proved, a union bound over all nagents will imply that the probability the allocation is\nα-PROP1 is at least 1−δ.\nThen, since there exists a g∈G\\Aisuch that vi(Ai∪{g})≥max j∈[m]vi(gj), ifmax j∈[m]vi(gj)≥α·vi(G)\nn,\nwe get that\nvi(Ai∪ {g})≥max\nj∈[m]vi(gj)≥α·vi(G)\nn\nand equivalently, Prh\nvi(Ai∪ {g})< α·vi(G)\nni\n= 0, giving us (Eq. (4)) as desired. Thus, it suffices to consider\nthe case when max j∈[m]vi(gj)< α·vi(G)\nn.\nConsider any agent i∈[n]. Let Si=P\nj∈[m](vi(gj)·Xj)be a random variable representing vi(Ai)whereby\nX1, . . . , X m∼Bernoulli (1/n); and let S′\ni=P\nj∈[m]\u0000\nvi(gj)·X′\nj\u0001\nbe a random variable whereby X′\n1, . . . , X′\nm∼\nBernoulli\u0000n−1\nn\u0001\n. Since for each i∈[n],Si=vi(G)−S′\ni,\nPr\u0014\nvi(Ai∪ {g})<α·vi(G)\nn\u0015\n≤Pr\u0014\nvi(Ai)<α·vi(G)\nn\u0015\n= Pr\u0014\nSi<α·vi(G)\nn\u0015\n= Pr\u0014\nS′\ni>(n−1)·vi(G)\nn+(1−α)·vi(G)\nn\u0015\nWe now prove several properties of the random variable S′\ni, relating to its expectation, variance, and the\nindividual terms vi(gj)·X′\njin the summation.\nLemma 15. ForS′\ni=P\nj∈[m]\u0000\nvi(gj)·X′\nj\u0001\n, the following properties always hold:\n(i)E[S′\ni] =(n−1)\nn·vi(G);\n(ii)σ2(S′\ni)≤α·vi(G)2\nn2; and\n(iii) vi(gj)·X′\nj−E[vi(gj)·X′\nj]≤α·vi(G)\nnfor all j∈[m].\nProof.We first prove (i). By linearity of expectation, we get that\nE[S′\ni] =X\nj∈[m]\u0000\nvi(gj)·E[X′\nj]\u0001\n=X\nj∈[m]vi(gj)·n−1\nn\n=n−1\nn·vi(G),\n19\n\nwhere the last line follows from the fact thatP\nj∈[m]vi(gj) =vi(G). Next, we prove (ii). Since vi(gj)≤α·vi(G)\nn\nfor all j∈[m], we have that\nσ2(S′\ni) =X\nj∈[m]\u0000\nvi(gj)·vi(gj)·σ2(X′\nj)\u0001\n≤X\nj∈[m]\u0000\nvi(gj)·α·vi(G)·σ2(X′\nj)\u0001\n=X\nj∈[m]\u0012\nvi(gj)·α·vi(G)\nn·n−1\nn2\u0013\n=α·vi(G)\nn·n−1\nn2·X\nj∈[m]vi(gj)\n=α·vi(G)\nn·n−1\nn2·vi(G)\n≤α·vi(G)2\nn2.\nFinally, we prove (iii). For all j∈[m], since X′\nj≤1andvi(g)≤α·vi(G)\nn, we have that\nvi(gj)·X′\nj−E[vi(gj)·X′\nj] =vi(gj)−n−1\nn·vi(gj)\n=vi(gj)\nn\n≤α·vi(gj)\nn2\n=α·vi(G)\nn.\nWe now state Bernstein’s inequality, which we will use for the proof.\nLemma 16 ([DP09]; Theorem 1.4) .Let the random variables X1, . . . , X nbe independent with Xi−E[Xi]≤b\nfor each i∈[n]. Also let X:=P\ni∈[n]Xiandσ2:=P\ni∈[n]σ2\nibe the variance of X. Then, for any t >0,\nPr[X >E[X] +t]≤exp\u0012\n−t2\n2σ2+ 2bt/3\u0013\n.\nWe can then apply Bernstein’s inequality to S′\ni. By setting t=(1−α)·vi(G)\nnandb=α·vi(G)\nn, we get\nPr\u0014\nS′\ni>(n−1)·vi(G)\nn+(1−α)·vi(G)\nn\u0015\n= Pr [ S′\ni>E[S′] +t]\n≤exp\u0012\n−t2\n2σ2+ 2bt/3\u0013\n,\nwhere the equality follows from property (i) of Lemma 15. We also note that Bernstein’s inequality is applicable\ndue to property (iii) of Lemma 15.\nSince our goal is to find an upper bound to our expression, it suffices to use a lower bound on the numerator\nt2, and an upper bound for the denominator 2σ2+ 2bt/3.\nFor the numerator t2, since α=27\n128 log ( n/δ)≤1\n4for all n≥2, we have that\nt2=(1−α)2·vi(G)2\nn2≥9·vi(G)2\n16n2\n20\n\nFor the denominator 2σ2+ 2bt/3, by bounding our variance with property (ii) of Lemma 15, we have that\n2σ2+2bt\n3≤2α·vi(G)2\nn2+2α(1−α)·vi(G)2\n3n2\n<2α·vi(G)2\nn2+2α·vi(G)2\n3n2\n=8α·vi(G)2\n3n2,\nwhere the second inequality follows from the fact that α >0and hence 1−α <1. Thus, we get that\nexp\u0012\n−t2\n2σ2+ 2bt/3\u0013\n≤exp \n−9·vi(G)2\n16n2\n8α·vi(G)2\n3n2!\n= exp\u0012\n−27\n128·α\u0013\n= exp ( −log(n/δ)) =δ\nn,\ngiving us (Eq. (4)). Then, by summing the bound in (Eq. (4)) over the nagents: the probability that any\nagent violates the α-PROP1 condition is at most n·δ\nn=δ. Therefore, with probability at least 1−δ, for every\nagent i∈[n]andg:= max g′∈G\\Aivi(g′),\nvi(Ai∪ {g})≥α·vi(G)\nn,\nand the allocation returned is α-PROP1.\nC Omitted Proofs from Section 5\nC.1 Proof of Proposition 10\nSuppose for a contradiction that there exists an online algorithm that achieves an approximation ratio of αfor\nEF1, MMS, or PROPX. Let vi(g1) = 1for all i∈[n], and assume without loss of generality that agent 1is\nallocated g1.\nWe now define an instance with mgoods. Let vi(g1) = 1for all i∈[n], and for each t∈ {2, . . . , m }, the\nvaluations are as follows:\nv1(gt) =(\n1if|At\n1|= 1ort≤n\n0otherwise\nvi(gt) =(\nεKt−2if|At\n1|= 1ort≤n\n0otherwise\nwhere m=⌈n\nα⌉+n+ 2,K=⌈3\nα⌉, and ε=1\nKm−2. Since every good has value at most 1 for all agents, this\nconstruction is consistent with the MIV prediction with p= (1, . . . , 1).\nWe split our analysis into two cases.\nCase 1: |A1|= 1.Then, by the assumption that agent 1is allocated g1, agents’ valuations for incoming goods\nmust be as follows:\nvg1g2g3. . . g m\n11 1 1 . . . 1\n2 1 ε εK . . . εKm−2\n..................\nn 1 ε εK . . . εKm−2\n21\n\nNote that since\nm=ln\nαm\n+n+ 2≥n\nα+n+ 2, (5)\nwe have that\nα≥n\nm−n−2>n\nm−n−1. (6)\nFurthermore, as |A1|= 1, we get that v1(A1) =v1(g1) = 1. We first show a contradiction to α-EF1.\nSince |A1|= 1, by the pigeonhole principle, there must exist an agent i̸= 1that receives at least\n⌈m−1\nn−1⌉ ≥m−1\nn−1>m−1\nngoods. Thus, there exists a good g∈Aisuch that\nv1(Ai\\ {g})≥m−1\nn−1 =m−n−1\nn.\nConsequently, we get that\nv1(A1)\nv1(Ai\\ {g})≤n\nm−n−1< α,\nwhere the rightmost inequality follows from (Eq. (6)), giving us\nv1(A1)< α·v1(Ai\\ {g}),\na contradiction.\nNext, we show a contradiction to α-MMS. Note that\nMMS 1=jm\nnk\n≥m\nn−1 =m−n\nn.\nThen,\nv1(A1)\nMMS 1≤n\nm−n< α,\nwhere again, the rightmost inequality follows from (Eq. (6)), giving us\nv1(A1)< α·MMS 1,\na contradiction.\nFinally, we show a contradiction to α-PROPX. From (Eq. (5)) and the fact that α≤1, we get that\nm≥2n+ 2.\nObserve that for g∈argming′∈G\\A1v1(g′), we have that\nv1(A1∪ {g}) = 1 + 1 = 2 .\nTogether with the fact that v1(G) =m, we obtain\nn\nv1(G)·v1(A1∪ {g}) =2n\nm≤2n\n2n+ 2\n=n\nn+ 1\n≤n\nm−n−1< α,\nwhere the last inequality follows from (Eq. (6)). This gives us vi(A1∪ {g})< α·v1(G)\nn, a contradiction.\nCase 2: |A1|>0.Letgtbe the first good after g1that agent 1receives. We further split our analysis into\ntwo sub-cases: (a) t≤nand (b) t≥n+ 1.\nCase 2(a): t≤n.Then agents’ valuations for incoming goods must be as follows:\nvg1g2··· gt··· gn\n11 1···1 ··· 1\n2 1 ε··· εKt−2··· εKn−2\n.....................\nn 1 ε··· εKt−2··· εKn−2\n22\n\nand if n < m,vi(gj) = 0for all i∈[n]andj∈ {n+ 1, . . . , m }. Since agent 1receives two goods out\nof the first ngoods, by the pigeonhole principle, there necessarily exists an agent i∈[n]\\ {1}that\ndoes not receive any good out of the first ngoods, and vi(Ai) = 0. Also, as vi(A1\\ {g})≥εKt−2\n(forg:= argmaxg∈A1vi(g)), we have that\nvi(Ai)\nvi(A1\\ {g})≤0\nεKt−2< α,\ngiving us vi(Ai)< α·vi(A1\\ {g}), a contradiction to α-EF1.\nFurthermore, as MMS i=vi(g2) =ε, we have that\nvi(Ai)\nMMS i≤0\nε< α,\ngiving us vi(Ai)< α·MMS i, a contradiction to α-MMS.\nNext, we prove a contradiction to α-PROPX. Again let ibe the agent that does not receive any\ngood out of the first ngoods (as reasoned above). First observe that\nvi(G) = 1 +nX\nj=2εKj−2= 1 +ε(Kn−1−1)\nK−1.\nAlso, since K=⌈3\nα⌉, it implies α >3\nK. Since α≤1, it also means K > 3. Consequently, we have\nthat\n3Kn−3≥3·3n−3= 3n−2≥n,\ngiving us n≤3Kn−3, which is equivalent to\nn\nKn−2≤3\nK< α.\nMoreover, for g:= argming′∈G\\Aivi(g′), we have vi(Ai∪ {g}) = 0 + vi(g2) =ε.\nCombining all the facts above, we get that\nn·vi(Ai∪ {g})\nvi(G)=n·ε\n1 +ε(Kn−1−1)\nK−1\n≤n·ε\nε(Kn−1−1)\nK−1\n=n(K−1)\nKn−1−1\n≤n(K−1)\nKn−2(K−1)\n=n\nKn−2\n< α,\ngiving us vi(Ai∪ {g})< α·vi(G)\nn, a contradiction.\nCase 2(b): t > n + 1.Then agents’ valuations for incoming goods must be as follows:\nvg1g2··· gt\n11 1···1\n2 1 ε··· εKt−2\n...............\nn 1 ε··· εKt−2\n23\n\nand if t < m,vi(gj) = 0for all i∈[n]andj∈ {t+ 1, . . . , m }.\nConsider the set {gt+2−n, . . . , g t}. Since the set contains n−1goods and agent 1is allocated gt,\nby the pigeonhole principle, there exists an agent i∈[n]\\ {1}that does not receive any good in\n{gt+2−n, . . . , g t}. Hence,\nvi(Ai)≤t+1−nX\nj=2ε·Kj−2=t−1−nX\nj′=0ε·Kj′\n≤ε(Kt−n−1)\nK−1\n≤2ε(Kt−n−1)\nK\n≤2\nK\u0000\nεKt−n\u0001\n.\nMoreover, since K=⌈3\nα⌉ ≥3\nαandα≤1, we know that K≥2,1\nK−1≤2\nK, and α≥3\nK>2\nK.\nWe now show a contradiction to α-EF1. Since vi(g1)≥vi(gt), we have that\nvi(A1\\ {g1}) =vi(gt) =εKt−2≥εKt−n,\nwhere the last inequality follows from the fact that n≥2. This means there exists a g∈A1whereby\nvi(Ai)\nvi(A1\\ {g})≤2\nK(εKt−n)\nεKt−n=2\nK< α,\nand implies vi(Ai)< α·vi(Ai\\ {g}), a contradiction.\nFurthermore, consider the set of ngoods G′={gt+2−n, . . . , g t} ∪ {g1}. We have that\nMMS i≥min\ng∈G′vi(g) =vi(gt2−n) =εKt−n.\nThus, we get\nvi(Ai)\nMMS i≤2\nK(εKt−n)\nεKt−n≤2\nK< α,\ngiving us vi(Ai)< α·MMS i, a contradiction.\nFinally, for g:= argming′∈G\\Aivi(g), we have that\nvi(Ai∪ {g})≤2\nK\u0000\nεKt−n\u0001\n+εKt−n\n=εKt−n\u00122\nK+ 1\u0013\nand\nvi(G) = 1 +ε(Kn−1−1)\nK−1\nThen,\nn·vi(Ai∪ {g})\nvi(G)=nεKt−n\u00002\nK+ 1\u0001\n1 +ε(Kn−1−1)\nK−1\n≤nεKt−n\u00002\nK+ 1\u0001\nε(Kn−1−1)\nK−1\n=nKt−n\u00002\nK+ 1\u0001\n(K−1)\nKn−1−1\n≤nK⌈n/α⌉+2\u00002\nK+ 1\u0001\n(K−1)\nKn−1−1,\n24\n\nwhere the last inequality follows from the fact that n+ 1< t≤mand hence\nKt−n≤Km−n≤K⌈n/α⌉+2.\nNow,\nnK⌈n/α⌉+2\u00002\nK+ 1\u0001\n(K−1)\nKn−1−1\n<nK⌈n/α⌉+3\nKn−1−1(since\u00122\nK+ 1\u0013\n(K−1)< K)\n≤2nK⌈n/α⌉+3−(n−1)(since Kn−1−1≥1\n2Kn−1)\n= 2nK⌈n/α⌉+4−n\n≤2nKn/α+5−n(since ⌈n/α⌉ ≤n/α+ 1)\n=2n\nKn(1−1/α)−5\n<2n\u0010α\n3\u0011n(1−1/α)−5\n(since K >3\nα)\n< α.\nThis gives us vi(Ai∪ {g})< α·vi(G), which is a contradiction to α-PROPX.\nIn both cases, we get a contradiction, and hence our result follows.\nC.2 Proof of Theorem 11\nFor each agent i∈[n], note that riis the earliest iteration (of the whileloop in Line 3 of Algorithm 1) whereby\nagent ivalues a good at 1, i.e., vi(gri) = 1and for each 1≤t < r i,vi(gt)<1.\nRecall that for each i∈[n]andt∈ {1, . . . , m },At\nidenotes the bundle of agent iafter gthas been allocated.\nTo simplify the analysis, we introduce new variables in place of those used in the algorithm (from Line 8\nonwards). For each t∈ {0, . . . , m }, we denote\nxt\ni=(1\n1+vi(Gt)ift < r i\n1\nvi(Gt)otherwise\nand\nyt\ni=\n\nvi(At\ni)\n1+vi(Gt)ift < r i\nvi(At\ni\\{gri})\nvi(Gt)otherwise\nNote that x0\ni= 1andy0\ni= 0for all i∈[n]. Also, for each i∈[n]andt∈[m], we let\nϕt\ni:=xt\ni\n(n2+n+ 1)·xt\ni+n2·yt\ni−1(7)\nand\nΦt=X\ni∈[n]ϕt\ni. (8)\nThen, it is easy to observe that Algorithm 1 is essentially an online algorithm that minimizes Φtat each round.\nWe first present a lemma, which will be useful in our proof.\nLemma 17. For each t∈ {0, . . . , m }, if (i) ϕt\ni≥0for all i∈[n], and (ii) Φt≤Φ0=1\nn+1, then (1) for all\ni∈[n],xt\ni+yt\ni≥1\nn2, and (2) if t=m, the allocation Areturned by Algorithm 1 satisfies1\nn-PROP1.\nProof.IfAi=G, we are done. Thus, assume Ai⊊G. We first prove that for each agent i∈[n],xt\ni+yt\ni≥1\nn2.\nConsider any t∈ {0, . . . , m }. Suppose for a contradiction that\n(i)ϕt\ni≥0for all i∈[n], and\n25\n\n(ii)Φt≤1\nn+1,\nbut there exists some agent j∈[n]such that xt\nj+yt\nj<1\nn2. Then, we have that\nϕt\nj=xt\nj\n(n2+n+ 1)·xt\nj+n2·yt\nj−1(by (Eq. (7)))\n>xt\nj\n(n+ 1)·xt\nj(since xt\nj+yt\nj<1\nn2)\n=1\nn+ 1\n≥Φt(by (i))\n=X\ni∈[n]ϕt\ni(by Eq. (8))\n≥ϕt\nj,(since ϕt\ni≥0for all i∈[n])\nthereby giving us a contradiction. Thus, for each agent i∈[n],xt\ni+yt\ni≥1\nn2.\nNext, we show that if the two invariant holds, then when the algorithm terminates, the resulting allocation\nAsatisfies1\nn-PROP1.\nNow, at the last iteration t=m, for each agent i∈[n], it must be that t≥ri, and hence by definition we\nget\nxm\ni=1\nvi(Gm)and ym\ni=vi(Am\ni\\ {gri})\nvi(Gm).\nSince Gm=GandAm\ni=Ai, we get\nxm\ni+ym\ni=1 +vi(Ai\\ {gri})\nvi(G). (9)\nIfgri∈Ai, then there exists a good g∈G\\Ai(note this must be true because we assumed Ai⊊G) such that\nvi(Ai∪{g})≥vi(Ai) =vi(Ai\\{gri}) + 1; and if gri/∈Ai, then vi(Ai∪{gri}) =vi(Ai) + 1 = vi(Ai\\{gri}) + 1.\nIn both cases, we have that there exists a g∈G\\Aisuch that\nvi(Ai∪ {g}) =vi(Ai\\ {gri}) + 1.\nConsequently,\nvi(Ai∪ {g})≥vi(Ai\\ {gri}) + 1\n= (xm\ni+ym\ni)·vi(G)(by (Eq. (9)))\n≥1\nn2·vi(G),\nwhere the last inequality follows from the fact we proved earlier in this lemma. Observe that the expression is\nequivalent to\nvi(Ai∪ {g})≥1\nn·vi(G)\nn\nfor some g∈G\\Ai, and1\nn-PROP1 is satisfied.\nNext, we prove that the algorithm has two invariants.\nLemma 18. For each t∈ {0, . . . , m }, the following hold:\n(i)ϕt\ni≥0for all i∈[n], and\n(ii)Φt≤1\nn+1.\nProof.We prove the lemma by induction.\n26\n\nFirst, consider the base case when t= 0. For every agent i∈[n],G0=∅and thus, vi(G0) =vi(A0\ni) =\nvi(Ai) = 0. This gives us x0\ni=1\n1+0= 1andy0\ni=0\n1+0= 0. Consequently, we get that\nϕ0\ni=1\n(n2+n+ 1)−1=1\nn2+n≥0,\nsatisfying (i). Moreover, we have that\nΦ0=X\ni∈[n]ϕ0\ni=X\ni∈[n]1\nn2+n=n\nn2+n=1\nn+ 1,\nsatisfying (ii). Thus, the base case holds.\nNext, we prove the inductive step. Suppose that for every k∈ {0, . . . , T −1}, we have that\nϕk\ni≥0for all i∈[n]and Φk≤1\nn+ 1. (10)\nWe want to prove that\nϕk+1\ni≥0for all i∈[n]and Φk+1≤1\nn+ 1.\nFor each agent i∈[n], let\n•y+\ni, ϕ+\nibe the values of xk+1\ni, yk+1\ni, ϕk+1\nirespectively agent iis allocated good gk+1(i.e., gk+1∈Ak+1\ni),\nand\n•y−\ni, ϕ−\nibe the values of xk+1\ni, yk+1\ni, ϕk+1\nirespectively if agent iis not allocated good gk+1(i.e., gk+1/∈\nAk+1\ni).\nWe first prove that for each agent i∈[n],\nϕk+1\ni≥0and ϕ+\ni+ (n−1)·ϕ−\ni−n·ϕk\ni≤0. (11)\nWe split our analysis into two cases, depending on whether k+ 1 = riork+ 1̸=ri.\nCase 1: k+ 1 = ri.This means that vi(gk+1) = 1. Moreover, since k < r i, we get that\nxk\ni=1\n1 +vi(Gk)and yk\ni=vi(Ak\ni)\n1 +vi(Gk).\nThus,\nxk+1\ni=1\nvi(Gk+1)\n=1\nvi(gk+1) +vi(Gk)\n=1\n1 +vi(Gk)(since vi(gk+1) = 1)\n=xk\ni.\nSimilarly, we have that\ny+\ni=y−\ni=vi(Ak+1\ni\\ {gk+1})\nvi(Gk+1)=vi(Ak\ni)\n1 +vi(Gk)=yk\ni.\nAlso, we have that xk+1\ni=xk\niandyk+1\ni=yk\ni, giving us\nϕ+\ni=ϕ−\ni=ϕk+1\ni−1.\nConsequently, we get\nϕ+\ni+ (n−1)·ϕ−\ni−n·ϕk\ni= 0,\nwhich proves the second part of (Eq. (11)). Moreover, since ϕk\ni≥0(from (Eq. (10))), we have that\nϕ+\ni=ϕ−\ni≥0, which means ϕk+1\ni≥1>0, giving us the first part of (Eq. (11)).\n27\n\nCase 2: k+ 1̸=ri.To simplify our argument, we define an auxiliary valuation function v′\ni(which behaves\nmore like a variable rather than a valuation function) defined as follows for kandk+ 1.\nv′\ni(Gk) =(\n1 +vi(Gk)ifk+ 1< ri;\nvi(Gk)otherwise .\nv′\ni(Gk+1) =(\n1 +vi(Gk+1)ifk+ 1< ri;\nvi(Gk+1)otherwise .\nWe also note that vi(Ak\ni\\ {gri}) =vi(Ak\ni)ifk+ 1< ri. This gives us\nxk\ni=1\nv′\ni(Gk)and yk\ni=vi(Ak\ni\\ {gri})\nv′\ni(Gk).\nNow, denote ε:=vi(gk+1)\nv′\ni(Gk). Then, since vi(gk+1)≤pi=vmax\ni= 1, we have that\nε=vi(gk+1)\nv′\ni(Gk)≤1\nv′\ni(Gk)=xk\ni. (12)\nThus, we have\nxk+1\ni=1\nv′\ni(Gk+1)\n=1\nvi(gk+1) +v′\ni(Gk)\n=1\nε·v′\ni(Gk) +v′\ni(Gk)(by definition of ε)\n=1\n(1 +ε)·v′\ni(Gk)\n=xk\ni\n1 +ε(by definition of xk\ni).\nAlso, get that\ny+\ni=vi(Ak\ni\\ {gri}) +vi(gk+1)\nv′\ni(Gk+1)\n=vi(Ak\ni\\ {gri}) +vi(gk+1)\nv′\ni(Gk) +vi(gk+1)\n=vi(Ak\ni\\ {gri}) +ε·v′\ni(Gk)\nv′\ni(Gk) +ε·v′\ni(Gk)(by definition of ε)\n=vi(Ak\ni\\ {gri}) +ε·v′\ni(Gk)\n(1 +ε)·v′\ni(Gk)\n=yk\ni+ε\n1 +ε(by definition of yk\ni)\nand\ny−\ni=vi(Ak\ni\\ {gri})\nv′\ni(Gk+1)\n=vi(Ak\ni\\ {gri})\nv′\ni(Gk) +vi(gk+1)\n=vi(Ak\ni\\ {gri})\nv′\ni(Gk) +ε·v′\ni(Gk)(by definition of ε)\n=vi(Ak\ni\\ {gri})\n(1 +ε)·v′\ni(Gk)\n=yk\ni\n1 +ε(by definition of yk\ni).\n28\n\nWe also have\nϕ+\ni=xk+1\ni\n(n2+n+ 1)·xk+1\ni+n2·y+\ni−1\n=xk\ni\n1+ε\n(n2+n+ 1)·xk\ni\n1+ε+n2·yk\ni+ε\n1+ε−1\n=xk\ni\n(n2+n+ 1)·xk\ni+n2·yk\ni+ε·(n2−1)−1\nand\nϕ−\ni=xk+1\ni\n(n2+n+ 1)·xk+1\ni+n2·y−\ni−1\n=xk\ni\n1+ε\n(n2+n+ 1)·xk\ni\n1+ε+n2·yk\ni\n1+ε−1\n=xk\ni\n(n2+n+ 1)·xk\ni+n2·yk\ni−ε−1.\nNow, denote ∆ := ( n2+n+ 1)·xk\ni+n2·yk\ni−1. For any i∈[n], we can then express ϕ+\ni,ϕ−\ni, and ϕk\nias\nthe following:\nϕ+\ni=xk\ni\n∆ +ε·(n2−1), ϕ−\ni=xk\ni\n∆−ε, ϕk\ni=xk\ni\n∆.\nMoreover, by (Eq. (10)), we know that ϕk\ni≥0for all i∈[n]andΦk≤1\nn+1. Applying Lemma 17, we get\nthat\nn2·(xk\ni+yk\ni)≥1.\nThis means\n∆ = ( n2+n+ 1)·xk\ni+n2·yk\ni−1\n=n2·(xk\ni+yk\ni) + (n+ 1)·xk\ni−1\n≥1 + (n+ 1)·xk\ni−1\n= (n+ 1)·xk\ni. (13)\nThen, we know that\n•∆>0since xk\ni>0;\n•∆−ε >0since ε≤xk\ni(by (Eq. (12))); and\n•∆ +ε·(n2−1)>0since ε≥0.\nThis gives us ϕ+\ni≥0andϕ−\ni≥0. Then, by the definition of ϕ+\niandϕ−\ni, we get that ϕk+1\ni≥0, thereby\nproving the first part of (Eq. (11)).\nNow, by (Eq. (13)) and (Eq. (12)), we have that\n∆≥(n+ 1)·xk\ni≥(n+ 1)·ε,\nwhich gives us\n∆≥(n+ 1)·ε=n(n2−1)\nn(n−1)·ε.\nEquivalently, we get\n∆·(n2−n)≥n(n2−1)·ε.\n29\n\nThen, multiplying εon both sides, we obtain\n0≥n(n2−1)·ε2−∆·ε·(n2−n)\n=n(n2−1)·ε2−∆·ε·(n2−1) + ∆ ·ε·(n−1)\n= ∆·(∆−ε) + (n·ε−∆)(∆ + ε·(n2−1))\n≥∆·(∆−ε) + (n·ε−∆)(∆ + ε·(n2−1))\n(∆ + ε·(n2−1))·∆·(∆−ε)\n=1\n∆ +ε·(n2−1)+n·ε−∆\n∆·(∆−ε)\n=1\n∆ +ε·(n2−1)+n−1\n∆−ε−n\n∆. (14)\nFinally,\nϕ+\ni+ (n−1)·ϕ−\ni−n·ϕk\ni\n=xk\ni\n∆ +ε·(n2−1)+(n−1)·xk\ni\n∆−ε−n·xk\ni\n∆\n=xk\ni\u00121\n∆ +ε·(n2−1)+n−1\n∆−ε−n\n∆\u0013\n≤0,\nwhere the inequality follows from (Eq. (14)) and the fact that xk\ni≥0, thereby proving the second part\nof (Eq. (11)).\nIt remains to prove that Φk+1≤1\nn+1to complete the inductive step. Now, we have that\n0≥X\ni∈[n]\u0000\nϕ+\ni+ (n−1)·ϕ−\ni−n·ϕk\ni\u0001\n(by (Eq. (11)))\n=X\ni∈[n]ϕ+\ni+ (n−1)·X\ni∈[n]ϕ−\ni−n·X\ni∈[n]ϕk\ni\n=X\ni∈[n]ϕ+\ni+ (n−1)·X\ni∈[n]ϕ−\ni−n·Φk\n=X\ni∈[n]\n−Φk+ϕ+\ni+X\nj∈[n]\\{i}ϕ−\nj\n.\nThis necessarily means that there exists an agent i∗∈[n]such that −Φk+ϕ+\ni∗+P\nj∈[n]\\{i∗}ϕ−\nj≤0. Ifgk+1\nis allocated to agent i∗, then for all i∈[n], we have that\nϕk+1\ni=(\nϕ+\ni∗ifi=i∗;\nϕ−\niotherwise .(15)\nLetΦk+1\ni∗be the value of Φk+1ifgk+1was allocated to agent i∗. This means that\nΦk+1\ni∗=ϕ+\ni∗+X\ni∈[n]\\{i∗}ϕ−\ni.\nSince Φk+1\ni∗−Φk≤0andΦk≤1\nn+1(by (Eq. (10))), we get that\nΦk+1\ni∗≤Φk≤1\nn+ 1.\nFinally, since the algorithm allocates gk+1to the agent that minimizes Φk+1(Line 18 of Algorithm 1), we know\nthat Φk+1≤Φk+1\ni∗≤1\nn+1, as desired.\nWe have shown both the base case and inductive step, and thus, by induction, the lemma holds.\nThus, the invariants hold, and our result follows.\n30\n\nC.3 Proof of Theorem 12\nWithout loss of generality, we can assume that pi= 1for each i∈[n]. Also let ribe the earliest timestep\nwhereby agent ivalues a good at least 1−ε, i.e., vi(gri)≥1−εand for each 1≤t < r i,vi(gt)<1−ε.\nNow, for each agent i∈[n], define δi:= 1−vi(gri)(so that 0≤δi≤ε) and create the augmented valuation\nfunction v′\nidefined as follows:\nv′\ni(gt) =(\n1 ift=ri\nvi(gt)otherwise.\nThis also means\nv′\ni(gri) = 1 = vi(gri) +δi. (16)\nRun the α-PROP1 algorithm on this sequence of goods, but using the augmented valuation functions\nv′\n1, . . . , v′\nninstead. This returns an α-PROP1 allocation with respect to the augmented valuation functions.\nLetA= (A1, . . . , A n)be the returned allocation. By definition, for each agent i∈[n], there exists some good\ng∈G\\Aisuch that\nv′\ni(Ai∪ {g})≥α·v′\ni(G)\nn. (17)\nFor each agent i∈[n], we have that v′\ni(gt) =vi(gt)for all t∈[m]\\{ri}andv′\ni(gri) =vi(gri)+δi(by (Eq. (16))).\nThis gives us\nv′\ni(G) =vi(G) +δi (18)\nand for any g∈G\\Ai,\nv′\ni(Ai∪ {g})≤vi(Ai∪ {g}) +δi, (19)\nwith the above being an equality if gri/∈Ai∪ {g}. Together with the fact that δi≤ε <1, this also implies\nthere exists a g∈G\\Aisuch that\nvi(Ai∪ {g})\n1−δi≥v′\ni(Ai∪ {g})−δi\n1−δi≥1. (20)\nThe rightmost inequality follows from the fact that v′\ni(Ai∪ {g})≥1, which is true if gri∈Ai, otherwise we\ncan choose gto be gri.\nAlso, since δ≤ε, we get that1\n1−δi≤1\n1−εand\nδi\n1−δi≤ε\n1−ε. (21)\nNow, for each i∈[n], there exists a g∈G\\Aisuch that\nα·vi(G)\nn\n=α\nn·(v′\ni(G)−δi)(by (Eq. (18)))\n≤v′\ni(Ai∪ {g})−α\nn·δi(by (Eq. (17)))\n≤(vi(Ai∪ {g}) +δi)−α\nn·δi(by (Eq. (19)))\n=vi(Ai∪ {g}) +\u0010\n1−α\nn\u0011\n·δi\n≤vi(Ai∪ {g}) +\u0010\n1−α\nn\u0011\n·vi(Ai∪ {g})·δi\n1−δi(by (Eq. (20)))\n=\u0012\n1 +\u0010\n1−α\nn\u0011\n·δi\n1−δi\u0013\n·vi(Ai∪ {g})\n≤\u0012\n1 +\u0010\n1−α\nn\u0011\n·ε\n1−ε\u0013\n·vi(Ai∪ {g})(by (Eq. (21)))\n=1−αε\nn\n1−ε·vi(Ai∪ {g}) =α\nβ·vi(Ai∪ {g}).\n31\n\nThis gives us\nvi(Ai∪ {g})≥β·vi(G)\nn,\nas desired.\n32",
  "textLength": 81422
}