{
  "paperId": "dd06469bb70bd488d3318fefbb96620dfe5b3fa9",
  "title": "A Reinforcement Learning Based R-Tree for Spatial Data Indexing in Dynamic Environments",
  "pdfPath": "dd06469bb70bd488d3318fefbb96620dfe5b3fa9.pdf",
  "text": "A Reinforcement Learning Based R-Tree for Spatial Data\nIndexing in Dynamic Environments\nTu Gu1, Kaiyu Feng1, Gao Cong1, Cheng Long1, Zheng Wang1, Sheng Wang2\n1School of Computer Science and Engineering, Nanyang Technological University, Singapore\n2DAMO Academy, Alibaba Group\ngutu0001@e.ntu.edu.sg,{kyfeng,gaocong,c.long,wang_zheng}@ntu.edu.sg,sh.wang@alibaba-inc.com\nABSTRACT\nLearned indices have been proposed to replace classic index struc-\ntures like B-Tree with machine learning (ML) models. They require\nto replace both the indices and query processing algorithms cur-\nrently deployed by the databases, and such a radical departure is\nlikely to encounter challenges and obstacles. In contrast, we propose\na fundamentally different way of using ML techniques to improve\non the query performance of the classic R-Tree without the need of\nchanging its structure or query processing algorithms. Specifically,\nwe develop reinforcement learning (RL) based models to decide\nhow to choose a subtree for insertion and how to split a node when\nbuilding an R-Tree, instead of relying on hand-crafted heuristic\nrules currently used by R-Tree and its variants. Experiments on\nreal and synthetic datasets with up to more than 100 million spatial\nobjects clearly show that our RL based index outperforms R-Tree\nand its variants in terms of query processing time.\nACM Reference Format:\nTu Gu1, Kaiyu Feng1, Gao Cong1, Cheng Long1, Zheng Wang1, Sheng\nWang2. 2021. A Reinforcement Learning Based R-Tree for Spatial Data\nIndexing in Dynamic Environments. In Proceedings of ACM Conference\n(Conferenceâ€™17). ACM, New York, NY, USA, 13 pages. https://doi.org/10.\n1145/nnnnnnn.nnnnnnn\n1 INTRODUCTION\nTo support efficient processing of spatial queries, such as range\nqueries and KNN queries, spatial databases have relied on delicate\nindices. The R-Tree [ 14] is arguably the most popular spatial in-\ndex that prunes irrelevant data for queries. R-Trees have attracted\nextensive research interests [ 1â€“3,5,7,12,16,17,21,22,26,29,31â€“\n33,35,37,39,42] and are widely used in commercial databases such\nas PostgreSQL and MySQL.\nThe learned index has been proposed in [ 20], which proposes\na recursive model index (RMI) for indexing 1-dimensional data\nby learning a cumulative distribution function (CDF) to map a\nsearch key to a rank in a list of ordered data objects. To address the\nlimitations of the RMI, such as the lack of supporting updates, and\nto improve it, several learned indices have been proposed based\non the RMI. The idea of learned indices is also extended for spatial\ndata [ 22,28,31,39] and multi-dimensional data [ 5,7,26]. They\nusually map spatial data points in a dataset to a uniform rank\nspace (e.g., using a space filling curve), and then learn the CDF\nfor this dataset. Despite the success of these learned indices in\nimproving the efficiency of processing some types of queries, they\nConferenceâ€™17, July 2017, Washington, DC, USA\nÂ©2021 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnnstill have various limitations, e.g., they can only handle spatial\npoint objects and limited types of spatial queries, some only return\napproximate query results, and they either cannot handle updates\nor need a periodic rebuild to retain high query efficiency (Details\nin Section 5). These limitations, together with the requirement that\nthe learned indices need a replacement of the index structures and\nquery processing algorithms currently used by spatial database\nsystems, would make them not easy to be deployed in current\ndatabase systems.\nIn this work, rather than learning a CDF for spatial data, we\nconsider a fundamentally different approach, i.e., to use machine\nlearning techniques to construct an R-Tree in a data-driven way for\nbetter query efficiency in a dynamic environment where updates\noccur frequently and bulk loading is not viable. Specifically, we pro-\npose to build machine learning models for the two key operations\nof building an R-Tree, namely ChooseSubtree andSplit operations,\nwhich currently rely on hand-crafted heuristic rules. Note that\nwedonotmodify thebasic structure oftheR-Tree andthus all\nthecurrently deployed query processing algorithms will stillbe\napplicable toourproposed index. This would make it easier for\nthe learning based index to be deployed by current databases.\nTo motivate our idea, we next revisit ChooseSubtree andSplit.\nWhen inserting a new spatial object, the ChooseSubtree operation\nis invoked iteratively, i.e., choosing which child node to insert the\nnew data object, until a leaf node is reached. If the number of entries\nin a node exceeds the capacity, the Split operation is invoked to\ndivide the entries into two groups. Many R-Tree variants have been\nproposed, which mainly differ in their strategies for the insertion of\nnew objects (i.e., ChooseSubtree ) as well as algorithms for splitting\na node (i.e., Split). Almost all these strategies are based on hand-\ncrafted heuristics. However, there is no single heuristic strategy that\ndominates the others in terms of query performance. To illustrate\nthis, we generate a dataset with 1 million uniformly distributed\ndata points and construct four R-Tree variants using four different\nSplit strategies, namely linear [14],quadratic [14],Greeneâ€™s [13]\nandR*-Tree [2]. We run 1,000 random range queries and rank the\nfour indices based on the query processing time of each individual\nquery. We observe that no single index has the best performance\nfor all the queries. For example, Greeneâ€™s Split has the best query\nperformance among 50% of the queries while R*-Tree Split is the\nbest for 49% of the queries. The observation that no single index\ndominates the others holds on other datasets as well, and the top\nperformers may be different on different datasets.\nThe observation motivates us to develop machine learning mod-\nels to handle the ChooseSubtree andSplit operations, to replace\nthe heuristic strategies used in the R-Tree and its variants. Further-\nmore, we observe that the two operations can be considered as twoarXiv:2103.04541v2  [cs.DB]  11 Oct 2021\n\nConferenceâ€™17, July 2017, Washington, DC, USA Tu Gu1, Kaiyu Feng1, Gao Cong1, Cheng Long1, Zheng Wang1, Sheng Wang2\nsequential decision making problems. Therefore, we model them\nas two Markov decision processes (MDPs) [ 30] and propose to use\nreinforcement learning (RL) to learn models for the two operations.\nHowever, it is very challenging to make the idea workâ€”We have\ntried various ways to define the two MDPs, which significantly\naffect the performance of the learned models. The first challenge is\nhow to formulate ChooseSubtree andSplit as MDPs. How should\nwe define the states, actions, and the reward signals for each MDP?\nSpecifically, 1) Designing theaction space . A straightforward idea\ncould be to define an action as one of the existing heuristic strategies.\nHowever, the idea did not work, for which we observed from the\nexperiments that different strategies would often make the same\ninsertion/splitting decision, and thus it leaves us very little room\nfor improvement. Some other possible ideas include defining larger\naction spaces, but then it would increase the difficulty of training\nthe model. 2) Designing thestate . As the number of entries varies\nacross different R-Tree nodes, it is challenging to represent the state\nof a node for both operations. 3) Designing thereward signal . It is\nnontrivial to design a function that evaluates the reward of past\nactions during model training to encourage the RL agent to take\nâ€œgoodâ€ actions in our problem.\nThe second challenge is how to use RL to address the defined\nMDPs. For instance, in the construction of an R-Tree, node over-\nflow (and thus the Split operation) occurs less frequently than the\nChooseSubtree operation. Therefore, only a few state transitions\nfor Split operations are generated, making it difficult for the RL\nagent to learn useful information. Moreover, a previous decision\n(ChooseSubtree orSplit ) may affect the structure of the R-Tree in\nthe future. Therefore, a â€œgoodâ€ action may receive a bad reward due\nto some bad actions that were made previously. This makes it even\nmore challenging to learn a good policy to solve the two MDPs.\nOur method. To this end, we propose the RL based R-Tree, called\nRLR-Tree. In the RLR-Tree, we carefully model the two MDPs,\nChooseSubtree andSplit , and propose a novel RL-based method to\nlearn policies to solve them. The learned policies replace the hand-\ncrafted heuristic rules to build a different R-Tree, namely RLR-Tree.\nThe RLR-Tree possesses several salient features:\n(1) Its models are trained on a small training dataset and then\nused to build an RLR-Tree on datasets of much larger scales. Fur-\nthermore, models trained on one data distribution can be applied\nto data with a different distribution to build an RLR-Tree, the RLR-\nTree still significantly outperforms the R-Tree and its variants in\nanswering spatial queries.\n(2) It achieves up to 95% better query performance than the R-\nTree and its variants that are designed for a dynamic environment.\n(3) Like the R-Tree, the RLR-Tree can be built on spatial objects\nof different types, such as points or rectangles. However, to the best\nof our knowledge, existing learned indices [ 5,7,22,26,31,39] can\nonly handle point objects when used for spatial data.\n(4) The RLR-Tree simply deploys any existing query processing\nalgorithms for R-Trees to answer queries of different types, without\nthe need of designing new query processing algorithms. However,\nother learning based indices [ 5,7,22,26,31,39] often only focus\non limited types of queries, e.g., range queries, and they need to\ndesign new algorithms for each type of query.In summary, we make the following contributions:\n(1) We propose to train machine learning models to replace\nheuristic rules in the construction of an R-Tree to improve on its\nquery efficiency in a dynamic environment where updates occur\nfrequently and bulk loading is not viable. To the best of our knowl-\nedge, this is the first work that uses machine learning to improve on\nthe R-Tree without modifying its structure; Therefore, all currently\ndeployed query processing algorithms are still applicable and the\nproposed index can be easily deployed by current databases.\n(2) We model the ChooseSubtree and the Split operations as two\nMDPs, and carefully design their states, actions and reward signals.\nWe also present some of our unsuccessful trials of designing.\n(3) We design an effective and efficient learning process that\nlearns good policies to solve the MDPs. The learning process enables\nus to apply our RL models trained with a small dataset to build an\nR-Tree for up to more than 100 million spatial objects.\n(4) We conduct extensive experiments on both real and synthetic\ndatasets. The experimental results show that our proposed index\nachieves up to 95% better query performance for range queries\nand 96% for KNN queries than R-Tree and its variants, and up to\n40% better query performance for range queries and 42% for KNN\nqueries than LISA [ 22], which is the only disk based learned spatial\nindex that returns exact results for range queries and KNN queries.\n2 PRELIMINARY AND PROBLEM\n2.1 Preliminary\nR-Tree [ 14] is a balanced tree for indexing multi-dimensional ob-\njects, such as coordinates and rectangles. Each tree node can contain\nat mostğ‘€entries. Each node (except the root node) must also con-\ntain at least ğ‘šentries. Each entry in a non-leaf node consists of\na reference to a child node and the minimum bounding rectangle\n(MBR) of all entries within this child node. Each leaf node contains\nentries, each of which consists of a reference to an object and the\nMBR of that object. Therefore, a query that does not intersect with\nthe MBR cannot intersect any of the contained objects.\nThe algorithms for building an R-Tree comprise two key opera-\ntions, ChooseSubtree andSplit . To insert an object into an R-Tree,\nstarting from the root node, ChooseSubtree is iteratively invoked\nto decide in which subtree to insert the object, until a leaf node\nis reached. The object is inserted into the leaf node and its corre-\nsponding MBR is updated accordingly. If the number of entries in a\nleaf node exceeds ğ‘€, the Split operation is invoked to divide the\nobjects into two groups: one remains in the original leaf node and\nthe other will become a new leaf node. The Split operation may be\npropagated upwards as an entry referring to the new leaf node is\nadded to its parent node, which may overflow and need to be split.\nThe query performance of an R-Tree highly depends on how the\nR-Tree is built. Many R-Tree variants have been proposed with dif-\nferent ChooseSubtree andSplit strategies as discussed in Section 1.\n2.2 Problem Statement\nAs discussed in Section 1, most of the existing R-Tree variants adopt\nhand-crafted ChooseSubtree andSplit strategies, and no strategy\ncan build an R-Tree with dominant query performance in all cases.\nMotivated by this, we aim to learn to build an R-Tree, i.e., using RL\nmodels to make decisions for ChooseSubtree andSplit instead of\nrelying on heuristic rules. The new index is called RLR-Tree.\n\nA Reinforcement Learning Based R-Tree for Spatial Data Indexing in Dynamic Environments Conferenceâ€™17, July 2017, Washington, DC, USA\n3 RLR-TREE\n3.1 Overview\nThe process of inserting a new object into an R-Tree is essentially\na combination of two typical sequential decision making processes.\nIn particular, starting from the root, it needs to make a decision on\nwhich child node to insert the new object at each level in a top-down\ntraversal ( ChooseSubtree ). It also needs to make a decision on how\nto split an overflowing node and divide the entries in a bottom-up\ntraversal ( Split ). Reinforcement learning (RL) has been proven to be\neffective in solving sequential decision making problems. Therefore,\nwe propose to model the insertion of a new object as a combination\nof two Markov decision processes (MDPs) and adopt RL to learn\nthe optimal policies for ChooseSubtree andSplit operations.\nFigure 1 depicts an overview of the proposed solution to build an\nRLR-Tree, as well as using the RLR-Tree to answer queries. In offline\ntraining, we propose new solutions to train RL ChooseSubtree and\nRLSplit models using a small dataset or a subset. This is the focus\nof this work. Note that models trained on one dataset is readily\napplied on a different dataset as shown in experiments. The two\ntrained models can be integrated into the algorithms for R-Tree\nconstruction to build the RLR-Tree and R-tree maintenance with\ndynamic updatets. Finally, any existing query processing algorithm\ndesigned for the R-Tree family can be used for RLR-Tree to answer\ndifferent types of spatial queries.\nFigure 1: RLR-Tree Overview\nNext, we focus on the offline training and present our final\ndesigns for the two MDPs and the other representative designs\nthat we have explored to formulate the problem. We present RL\nChooseSubtree and its model training in Section 3.2, and RL Splitand its model training in Section 3.3. We present how to train the\ntwo models together in Section 3.4. We briefly introduce how to\nintegrate the trained models into existing algorithms to construct\nthe RLR-Tree and then to handle updates in Section 3.5.\n3.2 ChooseSubtree\nTo insert a new object into the R-Tree, we need to conduct a top-\ndown traversal starting from the root. In each node, we need to\ndecide which child node to insert the new object. To choose a\nsubtree with RL, we formulate this problem as an MDP. We proceed\nto present how to train a model to learn a policy for the MDP.\n3.2.1 MDP Formulation. An MDP consists of four components,\nnamely states ,actions ,transitions , and rewards . We proceed to ex-\nplain how states and actions are represented in our model, and then\npresent the reward signal design, which is particularly challenging.\nMDP: State Space. A state captures the environment that is taken\ninto account for decision making. For ChooseSubtree , it is a natural\nidea that a state is from the tree node whose child nodes are to be\nselected for inserting a new object. The challenging question is:\nwhat kind of information should we extract from the tree node to\nrepresent the state?\nIntuitively, as we need to decide which child node to insert the\nnew object, it is necessary to incorporate the change of the child\nnode if we add the new object into it for each child node. Possible\nfeatures that capture the change of a child node ğ‘include: (1)\nÎ”ğ´ğ‘Ÿğ‘’ğ‘(ğ‘,ğ‘œ), which is the area increase of the MBR of ğ‘if we add\nthe new object ğ‘œintoğ‘; (2)Î”ğ‘ƒğ‘’ğ‘Ÿğ‘–(ğ‘,ğ‘œ), which is the perimeter\nincrease of the MBR of ğ‘if we addğ‘œintoğ‘, and (3) Î”ğ‘‚ğ‘£ğ‘™ğ‘(ğ‘,ğ‘œ),\nwhich is the increase of the overlap between ğ‘and other child\nnodes afterğ‘œis inserted into ğ‘. Furthermore, it is helpful to know\nthe occupancy rate of the child node, denoted by ğ‘‚ğ‘…(ğ‘), which\nis the ratio of the number of entries to the capacity. A child node\nwith a high occupancy rate is more likely to overflow in the future.\nAs we have presented several features to capture the proper-\nties of a tree node, a straightforward idea is that we compute the\naforementioned features for every child node and concatenate them\nto represent the state. However, the number of child nodes varies\nacross different nodes, making it difficult to represent a state with\na vector of a fixed length. An idea to address this challenge is to do\npadding, i.e., to append zeros to the features of the child nodes to\nget a 4Â·ğ‘€dimensional vector, as there are four features and there\nare at most ğ‘€child nodes. However, the padded representations\nare likely to have many zeros which will add noises and mislead\nthe model, resulting in poor performance. This is confirmed by our\npreliminary experiments.\nTo address the challenge, we propose to only use a small part\nof child nodes to define the state. This is because most of the child\nnodes are not good candidates for hosting the new object, as insert-\ning the new object may greatly increase their MBRs. Here we aim\nto prune unpromising child nodes from the state space, and our RL\nagent will not consider them for representing a state. Our design of\nstate representation is as follows: We first retrieve the top- ğ‘˜child\nnodes in ascending order of area increase, where the choice of area\nincrease is based on empirical findings. Then for each retrieved\nchild node, we compute four features Î”ğ´ğ‘Ÿğ‘’ğ‘(ğ‘,ğ‘œ),Î”ğ‘ƒğ‘’ğ‘Ÿğ‘–(ğ‘,ğ‘œ),\nÎ”ğ‘‚ğ‘£ğ‘™ğ‘(ğ‘,ğ‘œ), andğ‘‚ğ‘…(ğ‘). We concatenate the features of the ğ‘˜\nchild nodes to get a 4Â·ğ‘˜dimensional vector to represent a state. ğ‘˜is\n\nConferenceâ€™17, July 2017, Washington, DC, USA Tu Gu1, Kaiyu Feng1, Gao Cong1, Cheng Long1, Zheng Wang1, Sheng Wang2\na parameter to be set empirically. Note that to make the representa-\ntion of different states comparable, the increases of area, perimeter\nand overlap are normalized by the maximum corresponding value\namong allğ‘˜child nodes.\nRemark. It is a natural idea that we can include more features to\nrepresent a state. For instance, we can include global information,\nsuch as the tree depth and the size of the tree, and local information,\nsuch as the depth of the tree node, the coordinates of the boundary\nof the MBR. However, our experiments show that these features\ndo not improve the performance of our model while making the\nmodel training slower. The four features that we use are sufficient\nto train our model to make good subtree choices as shown in our\nexperiments. It would be a useful future direction to design and\nevaluate other state features.\nMDP: Action Space. As many R-Tree variants have been proposed\nwith different ChooseSubtree strategies, such as minimizing the\nincrease of area, perimeter, or overlap. A straightforward idea is to\nmake the different cost functions the actions, i.e., to decide which\ncost function to use to choose the subtree. After trying different\ncombinations of these cost functions and different state space de-\nsigns, this idea is proven to be ineffective by our experimental\nresults. Table 1 depicts the average relative I/O cost for processing\n1,000 random range queries on three datasets of different distribu-\ntions, namely Skew, Gaussian and Uniform. The relative I/O cost\nwill be defined in Section 4.1. Intuitively, if the relative I/O cost is\nsmaller than 1, the index requires fewer nodes accesses than the\nR-Tree does. We observe from Table 1 that compared with the R-\nTree, an RL model with the cost functions as actions only achieves\nan improvement of less than 2% in terms of query processing time.\nWe find from our experiments that in 90% of the nodes, different\ncost functions end up with the same subtree choice which gives us\nvery little room for improvement.\nTable 1: Performance of cost function based action space.\nRelative I/O cost\nSkew Gaussian Uniform\nUse cost functions 0.98 0.98 1.00\nOur final design 0.29 0.08 0.56\nAs a result, we propose a new idea of training the RL agent to\ndecide which child node to insert the new object directly. Based\non the idea, one design is to have all child nodes to comprise the\naction space. However, this incurs two challenges: 1) the number\nof child nodes contained by different nodes is usually different,\nand 2) the action space is large. Considering all child nodes as the\nactions leads to a large action space with many â€œbad actionsâ€. The\nbad actions make the exploration during model training ineffective\nand inefficient. To address the challenges, we use the similar idea\nas we use for designing state space. Recall that in designing the\nstate space, we propose to retrieve top- ğ‘˜child nodes in terms of\nthe increase of area to represent a state. To make the action space\nand the state representation consistent, we define the action space\nA={1,...,ğ‘˜}, where action ğ‘=ğ‘–means the RL agent chooses the\nğ‘–-th retrieved child node to be inserted with the new object.\nMDP: Transition. In the process of the ChooseSubtree operation,\ngiven a state (a node in the R-Tree) and an action (inserting the new\nobject into a child node), the RL agent transits to the child node. If\nthe child node is a leaf node, the agent reaches a terminal state.MDP: Reward Signal. A reward associated with a transition corre-\nsponds to some feedback indicating the quality of the action taken\nat a given state. A larger reward indicates a better quality. Since\nour objective is to learn to build an R-Tree that processes query\nefficiently, the reward signal is expected to reflect the improvement\nof query performance.\nIn the process of ChooseSubtree , it is challenging to directly\nevaluate if an action taken at a state is good, because the new object\nhas not been fully inserted into the tree yet. A straightforward\nidea is after the new object has been inserted, we use the R-Tree to\nprocess a set of random range queries. The inverse of the cost (e.g.,\nthe number of accessed nodes) for processing the queries is set as\nthe reward shared by all of the state-action pairs encountered in\nthe insertion of the new object. The agent seems to be encouraged\nto take the actions to build a tree that can process range queries by\naccessing as few nodes as possible. However, this is not the case due\nto the following reasons: (1) A previous action may affect the tree\nstructure and hence the query performance in the future. Therefore,\na â€œgoodâ€ action may receive a poor reward due to some bad actions\nthat were made previously. (2) More importantly, as we aim to learn\nto construct an R-Tree that outperforms the competitors, we are\ninterested in knowing what kind of actions makes the resulting\ntree better than a competitor, and what kind of actions makes it\nworse. The aforementioned reward signal cannot distinguish the\ntwo types of actions, making it ineffective for the agent to learn\na good policy to outperform the competitors. (3) As more objects\nare inserted into the R-Tree, the average number of accessed nodes\nnaturally increases. Therefore, the reward signal becomes weaker\nand weaker, which makes it difficult for the model to learn useful\ninformation in the late stage of the training.\nInspired by the observations, we design a novel reward signal for\nChooseSubtree . The high level idea is that we maintain a reference\ntree with a fixed ChooseSubtree andSplit strategy. The reference\ntree serves as a competitor and can be any existing R-Tree variant.\nThe reward signal is computed based on the gap between costs for\nprocessing random queries with the reference tree and the RLR-Tree.\nSpecifically, the design of the reward signal is as follows:\n(1) We maintain an R-Tree, namely RLR-Tree, that uses RL to\ndecide which child node to insert the new object, and adopts a\npre-specified Split strategy.\n(2) We maintain a reference tree which adopts a pre-specified\nChooseSubtree strategy and the same Split strategy as RLR-Tree.\n(3) We synchronize the reference tree with the RLR-Tree, so that\nthey have the same tree structure.\n(4) Givenğ‘new objects{ğ‘œ1,...,ğ‘œğ‘}, we insert them into both\nthe reference tree and the RLR-Tree.\n(5) After the ğ‘objects are inserted, we generate ğ‘range queries\nof predifined sizes whose centers are at the ğ‘objects, respectively.\n(6) Theğ‘range queries are processed with both the reference\ntree and the RLR-Tree. We compute the normalized node access\nrate, which is defined as# acc. nodes\nTree heightand is the number of accessed\nnodes for answering a range query over the tree height. Let ğ‘…and\nğ‘…â€²be the normalized node access rate of the RLR-Tree and the\nreference tree, respectively. We compute ğ‘Ÿ=ğ‘…â€²âˆ’ğ‘…as the reward\nsignal. The higher ğ‘Ÿis, the fewer nodes RLR-Tree needs to access\nto process the range queries than the reference tree.\n\nA Reinforcement Learning Based R-Tree for Spatial Data Indexing in Dynamic Environments Conferenceâ€™17, July 2017, Washington, DC, USA\n(7) All the transitions encountered in the insertion of the ğ‘objects\nshare the same reward ğ‘Ÿ.\nWith the idea, we are able to distinguish the good actions from\nthe bad actions: A positive reward means that the RLR-Tree pro-\ncesses the recent ğ‘insertions well as it requires fewer nodes ac-\ncesses to process the queries compared with the reference tree.\nMoreover, as the reference tree is periodically synchronized with\nthe RLR-Tree, we can avoid the effect of previous actions. Therefore,\nmaximizing the accumulated reward is equivalent to encouraging\nthe agent to take the actions that can make the RLR-Tree outperform\nthe competitors.\n3.2.2 Training the Agent for ChooseSubtree .\nDeep-ğ‘„-Network (DQN) Learning. Deep Q-learning is a com-\nmonly used model-free RL method. It uses a Q-function ğ‘„âˆ—(ğ‘ ,ğ‘)\nto represent the expected accumulated reward that the agent can\nobtain if it takes action ğ‘in stateğ‘ and then follows the optimal\npolicy until it reaches a terminal state. The optimal policy takes the\naction with the maximum ğ‘„-value in any state. Deep- ğ‘„-Network\n[25] has been proposed to approximate the Q-function ğ‘„âˆ—(ğ‘ ,ğ‘)\nwith a deep neural network ğ‘„(ğ‘ ,ğ‘;Î˜)with parameters Î˜. In our\nmodel, we adopt the deep Q-learning with experience replay [ 25]\nfor learning the ğ‘„-functions.\nGiven a batch of transitions (ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²), parameters in ğ‘„(ğ‘ ,ğ‘;Î˜)\nis updated with a gradient descent step by minimizing the mean\nsquare error (MSE) loss function, as shown in Equation 1.\nğ¿(ğœƒ)=âˆ‘ï¸\nğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²[(ğ‘Ÿ+ğ›¾ğ‘šğ‘ğ‘¥ğ‘â€²Ë†ğ‘„(ğ‘ â€²,ğ‘â€²;Î˜âˆ’)âˆ’ğ‘„(ğ‘ ,ğ‘;Î˜))2],(1)\nwhereğ›¾is the discount rate, and Ë†ğ‘„(;Î˜âˆ’)is frozen target network.\nTraining the Agent. We present the RL ChooseSubtree train-\ning process in Algorithm 1. We first initialize the main network\nğ‘„(ğ‘ ,ğ‘;Î˜)and the target network Ë†ğ‘„(ğ‘ ,ğ‘;Î˜âˆ’)with the same random\nweights (line 3). In each epoch, it first resets the replay memory\n(line 5). Then it involves a sequence of insertions of the objects in\nthe training dataset (lines 6â€“20). Specifically, for every ğ‘objects\n{ğ‘œ1,...,ğ‘œğ‘}, we synchronize the structure of ğ‘‡ğ‘Ÿwithğ‘‡ğ‘Ÿğ‘™(line 7). For\neachğ‘œğ‘–of theğ‘objects, we first insert it into the reference tree (line\n9). Then a top-down traversal on the RLR-Tree is conducted (lines\n10â€“15). At each level, we compute the state representation (line 12)\nand useğœ–-greedy to choose the action based on their ğ‘„-values (line\n13), until we reach a terminal state (leaf node). The transitions are\nstored inğ‘†ğ´(line 14). At the leaf node, we insert the new object\nand use the same Split strategy as the reference tree in a bottom-up\nscan to ensure no node overflows (line 15). Meanwhile, we generate\na range query with a predefined size centered at ğ‘œğ‘–and add the\nquery toğ‘…ğ‘„(line 16). When the ğ‘objects have been inserted, we\ncompute the reward with the queries in ğ‘…ğ‘„(line 17). The reward\ncomputation process is illustrated in Figure 2. All transitions en-\ncountered in the insertions of the ğ‘objects share the same reward\nğ‘Ÿand are pushed into the replay memory (line 18). Then we draw\na batch of transitions randomly from the replay memory and use\nthe batch to update the parameters in the main network ğ‘„(;Î˜)as\nDQN does (line 19). The parameters in the target network Ë†ğ‘„are\nperiodically synchronized with ğ‘„(line 20).\nRemark . The new object to be inserted may be fully contained in\none of the child nodes. If we add the new object into such a child\nFigure 2: RL ChooseSubtree Reward Computation\nnode, the MBRs of all child nodes are not affected. Therefore, it is\nunnecessary to make the agent consider such cases. When such\ncases happen, we do not pass the state representation to the model,\nbut choose the child node that contains the new object directly.\nAlgorithm 1: DQN Learning for ChooseSubtree\n1Input: A training dataset;\n2Output: Learned action-value function ğ‘„(ğ‘ ,ğ‘;Î˜);\n3Initializeğ‘„(ğ‘ ,ğ‘;Î˜),Ë†ğ‘„(ğ‘ ,ğ‘;Î˜âˆ’);\n4forğ‘’ğ‘ğ‘œğ‘â„ =1,2,...do\n5 Replay memoryMâ†âˆ… ;\n6 foreveryğ‘objects{ğ‘œ1,...,ğ‘œ ğ‘}indataset do\n7ğ‘‡ğ‘Ÿâ†ğ‘‡ğ‘Ÿğ‘™,ğ‘†ğ´â†âˆ… ,ğ‘…ğ‘„â†âˆ… ;\n8 forğ‘œğ‘–âˆˆ{ğ‘œ1,...,ğ‘œ ğ‘}do\n9 Insertğ‘œğ‘–intoğ‘‡ğ‘Ÿ;\n10 ğ‘â†the root ofğ‘‡ğ‘Ÿğ‘™;\n11 whileğ‘isnon-leaf do\n12 ğ‘ â†state representation of ğ‘andğ‘œğ‘–;\n13 ğ‘â†an action selected by ğœ–-greedy based on\nğ‘„-values;\n14 ğ‘â†ğ‘,ğ‘†ğ´â†ğ‘†ğ´âˆª{(ğ‘ ,ğ‘)};\n15 Insertğ‘œğ‘–intoğ‘, split until no node overflows;\n16 ğ‘…ğ‘„â†ğ‘…ğ‘„âˆª{a range query centered at ğ‘œğ‘–};\n17ğ‘Ÿâ†compute reward with queries in ğ‘…ğ‘„;\n18 Add(ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²)for every(ğ‘ ,ğ‘)âˆˆğ‘†ğ´into memory;\n19 Draw samples from memory and update Î˜inğ‘„(;Î˜);\n20 Periodically synchronize Ë†ğ‘„(;Î˜âˆ’)withğ‘„(;Î˜);\n3.2.3 Time Complexity. In our analysis, the additional computation\ncost associated with the use of neural networks in an RLR-Tree is\ndeemed constant. Assume the RLR-Tree has a size of ğ‘†and a height\nofâ„. Inserting an object into the RLR-Tree encounters â„âˆ’1states.\nAt each state, it takes ğ‘‚(ğ‘˜Â·ğ‘€)time to retrieve the top- ğ‘˜child nodes\nandğ‘‚(ğ‘€)to compute the features for each child node. Therefore,\nthe overall time complexity is ğ‘‚(â„Â·ğ‘˜Â·ğ‘€). As a comparison, it takes\nğ‘‚(â„Â·ğ‘€)time for ChooseSubtree in the R-Tree.\n3.3 Split\nThe top-down traversal ends up at a leaf node. If the leaf node\noverflows, it will be split into two nodes and the Split operation\nmay be propagated upwards. Next, we present how to model Split\nas an MDP and how to train the model.\n\nConferenceâ€™17, July 2017, Washington, DC, USA Tu Gu1, Kaiyu Feng1, Gao Cong1, Cheng Long1, Zheng Wang1, Sheng Wang2\n3.3.1 MDP Formulation. We have also explored different ideas to\ndesign the state, action, transition and reward signal of the MDP\nforSplit . Due to space limitation, we only present the final design\nhere.\nMDP: State Space. ForSplit, it is natural that a state comes from\nan overflowing node. A straightforward idea is to make the repre-\nsentation of a state capture the goodness of all the possible splits,\nso that the agent can decide how to split the node. However, since\nan overflowing node contains ğ‘€+1entries, there are(2ğ‘€+1âˆ’2)\npossible splits in total. It is impractical to reflect all of these splits\nin the state representation.\nIn order to avoid considering so many possible splits, we adopt\na similar idea as Râˆ—-Tree [ 3] as follows: We first sort the entries\nwith respect to their projection to each dimension. For each sorted\nsequence, we consider the split at the ğ‘–-th element ( ğ‘šâ‰¤ğ‘–â‰¤ğ‘€+\n1âˆ’ğ‘š), where the first ğ‘–entries are assigned to the first group and\nthe remaining ğ‘€+1âˆ’ğ‘–entries are assigned to the second group.\nThis means for each sorted sequence, we only consider ğ‘€+2âˆ’2Â·ğ‘š\nsplits. Next, we further discard the splits which create two nodes\nthat overlap with each other. We pick the top- ğ‘˜splits from the\nremaining splits in ascending order of total area and construct the\nrepresentation of the state. Specifically, for each split, we consider\nfour features: the areas and the perimeters of the two nodes created\nby the split. We concatenate the features of all ğ‘˜splits and generate\na4ğ‘˜-dimensional vector to represent the state. Note that the areas\nand perimeters are normalized by the maximum area and perimeter\namong all splits, so that each dimension of the state representation\nfalls in(0,1].\nMDP: Action Space. Similar to ChooseSubtree , in order to make\nthe action space consistent with the candidate splits that are used\nto represent the state, we define the action space as A={1,...,ğ‘˜},\nwhereğ‘˜is the number of splits used to represent the state. An\nactionğ‘=ğ‘–means that the ğ‘–-th split is adopted.\nMDP: Transitions. InSplit, given a state (a node in the R-Tree)\nand an action (a possible split), the agent transits to the state that\nrepresents the parent of the node. If the node does not overflow, it\nis the terminal state.\nMDP: Reward Signal. The reward signal for Split is similar to\nthat of ChooseSubtree . We maintain a reference tree that is peri-\nodically synchronized with the RLR-Tree. We use the difference of\nthe normalized node access rate as the two trees process training\nqueries as the reward signal. Note that the RLR-Tree adopts the\nsame ChooseSubtree strategy as the reference tree and uses the RL\nagent to decide how to split an overflowing node.\n3.3.2 Training the Agent for Split. Compared with ChooseSubtree ,\ntraining the agent for Split is a more challenging task. To insert\nan object into the R-Tree, ChooseSubtree is iteratively invoked\nat each level in the top-down traversal. On the other hand, Split\noperation is invoked only when a node overflows. Therefore, only a\nfew transitions for Split are available for training, making it difficult\nfor the agent to learn a good policy in reasonable time.\nTo tackle this challenge, we design a new method for the agent to\ninteract with the environment, so that Split operation is frequently\ninvoked. Theoretically, if all the nodes are full, the insertion of a\nnew object definitely causes a tree node to split. Inspired by this,\nwe propose to first build a tree in which most of the nodes are full,\nso that node splits can be frequently encountered. We generateAlgorithm 2: DQN Learning for Split\n1Input: A training dataset;\n2Output: Learned action-value function ğ‘„(ğ‘ ,ğ‘;Î˜);\n3Initializeğ‘„(ğ‘ ,ğ‘;Î˜),Ë†ğ‘„(ğ‘ ,ğ‘;Î˜âˆ’);\n4forğ‘’ğ‘ğ‘œğ‘â„ =1,2,3,...do\n5 forğ‘—=1,2,3,...(ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘ âˆ’1)do\n6 training partâ†âˆ… ;\n7 Constructğ‘‡ğ‘ğ‘ğ‘ ğ‘’ with the firstğ‘—\nğ‘ğ‘ğ‘Ÿğ‘¡ğ‘ of the objects (initial\npart);\n8 forğ‘œintheremaining objects do\n9 ifInsertingğ‘œintoğ‘‡ğ‘ğ‘ğ‘ ğ‘’ causes asplitthen Addğ‘œto\ntraining part;\n10 else Addğ‘œto fill part and insert into ğ‘‡ğ‘ğ‘ğ‘ ğ‘’;\n11 foreveryğ‘objects{ğ‘œ1,...,ğ‘œ ğ‘}âŠ†training partdo\n12 ğ‘‡ğ‘Ÿğ‘™â†ğ‘‡ğ‘ğ‘ğ‘ ğ‘’,ğ‘‡ğ‘Ÿâ†ğ‘‡ğ‘ğ‘ğ‘ ğ‘’,ğ‘†ğ´â†âˆ… ,ğ‘…ğ‘„â†âˆ… ;\n13 forğ‘œğ‘–âˆˆ{ğ‘œ1,...,ğ‘œ ğ‘}do\n14 Insertğ‘œğ‘–intoğ‘‡ğ‘Ÿ;\n15 Top-down traversal on ğ‘‡ğ‘Ÿğ‘™to a leaf node ğ‘;\n16 ifğ‘overflows then\nğ‘…ğ‘„â†ğ‘…ğ‘„âˆª{training query};\n17 whileğ‘overflows do\n18 ğ‘ â†state representation of ğ‘;\n19 ğ‘â†an action selected by ğœ–-greedy based\nğ‘„-values;\n20 ğ‘â†ğ‘â€™s parent,ğ‘†ğ´â†ğ‘†ğ´âˆª{(ğ‘ ,ğ‘)}\n21 ğ‘Ÿâ†compute reward with queries in ğ‘…ğ‘„;\n22 Add(ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²)for all(ğ‘ ,ğ‘)âˆˆğ‘†ğ´to memory;\n23 Draw samples from memory and update ğ‘„(;Î˜);\n24 Periodically synchronize Ë†ğ‘„(;Î˜âˆ’)withğ‘„(;Î˜);\nsuch R-Trees with different sizes, and use the transitions caused by\ninserting the remaining objects in the dataset for training.\nThe procedure is presented in Algorithm 2. In each epoch, we\nrepeatğ‘ğ‘ğ‘Ÿğ‘¡ğ‘ âˆ’1iterations to train the agent (lines 5â€“24). In particu-\nlar, in theğ‘–-th iteration, the firstğ‘–\nğ‘ğ‘ğ‘Ÿğ‘¡ğ‘ of the training dataset forms\nthe initial part which is used to construct an R-Tree ğ‘‡ğ‘ğ‘ğ‘ ğ‘’ (line 7).\nThe remaining data are then divided into 2 parts, i.e., the fill part\ncontaining objects that will not cause node overflow in ğ‘‡ğ‘ğ‘ğ‘ ğ‘’ and\nthe training part containing objects not in the fill part. Objects in\nthe fill part are inserted into ğ‘‡ğ‘ğ‘ğ‘ ğ‘’ while objects in the training part\nwill be used to trigger splits for training later (line 9â€“10). In this way,\nmost of the nodes in ğ‘‡ğ‘ğ‘ğ‘ ğ‘’ are likely to be full and the objects in the\ntraining part are likely to cause splits. This pre-training preparation\nprocess is illustrated in Figure 3. After ğ‘‡ğ‘ğ‘ğ‘ ğ‘’ is constructed, we start\ntraining with objects in the training part (lines 11â€“24). For every ğ‘\nobjects, we first synchronize ğ‘‡ğ‘Ÿandğ‘‡ğ‘Ÿğ‘™withğ‘‡ğ‘ğ‘ğ‘ ğ‘’ (line 12). This\nmakesğ‘‡ğ‘Ÿandğ‘‡ğ‘Ÿğ‘™have the same structure and are almost full. Then\nfor each of the ğ‘objects, we insert it into the reference tree ğ‘‡ğ‘Ÿ\ndirectly with pre-specified ChooseSubtree andSplit strategies (line\n14). For the RLR-Tree, we use the same ChooseSubtree strategy as\nthe reference tree to reach a leaf node ğ‘(line 15). Ifğ‘overflows,\nwe generate a range query with a predefined size centered at ğ‘œğ‘–and\nadd the query to ğ‘…ğ‘„(line 16). Then we iteratively split ğ‘and move\nto its parent until ğ‘does not overflow (lines 17â€“20). For each node\nğ‘, we compute the state representation and use ğœ–-greedy to select\nan action based on their ğ‘„-values (lines 18â€“19). The transitions are\nstored inğ‘†ğ´(line 20). When the ğ‘objects have been processed,\n\nA Reinforcement Learning Based R-Tree for Spatial Data Indexing in Dynamic Environments Conferenceâ€™17, July 2017, Washington, DC, USA\nwe compute the reward with the queries in ğ‘…ğ‘„(line 21). The tran-\nsitions encountered in processing the ğ‘objects share the same\nreward (line 22). Then we draw a batch of random transitions from\nthe replay memory and use it to update the parameters in the main\nnetwork (line 23). The parameters in the target network Ë†ğ‘„(;Î˜âˆ’)are\nperiodically synchronized with the main network ğ‘„(;Î˜)(line 24).\nFigure 3: RL Split Pre-Training Preparation\nRemark. When we split a node to two, the ideal case is that the two\nnodes do not overlap with each other, so that the number of node\naccesses for processing a query can be reduced. It is more challeng-\ning when there are many candidate splits that generate two nodes\nwith zero overlap, as we need to carefully consider how to break the\ntie. As a result, we consider such a special case in the exploration\nof the agent. Specifically, if there exists at most one candidate split\nthat generates two non-overlapping nodes, we simply select the\nsplit with the minimum overlap. We only use RL to decide how to\nsplit the node when more than one split generates non-overlapping\nnodes. In this way, the agent is trained more effectively.\nThe training process for Split has two advantages. Firstly, using\ndifferent fractions of objects to build an â€œalmost-fullâ€ base tree helps\nto learn a more general model. Secondly, by building the â€œalmost-\nfullâ€ base tree and periodically resetting ğ‘‡ğ‘Ÿandğ‘‡ğ‘Ÿğ‘™to the base tree\nensures that the Split operation is consistently invoked at a high\nfrequency. This makes the training process more efficient.\n3.3.3 Time Complexity. Similar to ChooseSubtree , the additional\ncomputation cost associated with the use of neural networks is\ndeemed constant. If a node overflows, at most ğ‘‚(â„)Split opera-\ntions are invoked. In each Split operation, we first sort the entries\nalong each dimension, which takes ğ‘‚(ğ‘€logğ‘€)time. Then it takes\nğ‘‚(ğ‘˜Â·(ğ‘€âˆ’2ğ‘š)Â·ğ‘€)time to retrieve the top ğ‘˜splits with minimum\ntotal area. Finally, it takes ğ‘‚(ğ‘˜Â·ğ‘€)time to compute the four features\nfor theğ‘˜splits. Therefore, the overall time complexity for RLR-Tree\nSplit isğ‘‚(â„Â·ğ‘˜Â·(ğ‘€âˆ’2ğ‘š)Â·ğ‘€). As a comparison, Split operation takes\nğ‘‚(â„Â·ğ‘€2)time in the classic R-Tree.\n3.4 The Combined Model\nWe expect that the two agents are closely tied together and are able\nto help each other achieve better performance. A possible idea for\ncombing the two models is to make the agents share information\nsuch as the states observed and the corresponding actions selected\nwith each other. Since both RL models have the same goal, this kind\nof information sharing has the potential to enhance the agentsâ€™ pol-\nicy learning. However, it is difficult for the two models to exchange\ninformation, as they are dealing with different procedures. Thisapproach does not lead to any query performance improvement in\nthe final RLR-Tree.\nRecall that we specially design a learning process of Split, as\nnode overflow occurs infrequently in the construction of an R-Tree.\nMotivated by this, we propose an enhanced training process to train\nthe two agents alternately. Specifically, in odd epochs, we train the\nRL agent for ChooseSubtree and the agent for Split is fixed to be\ntheSplit strategy for the RLR-Tree. In even epochs, we train the\nagent for Split and the agent for ChooseSubtree is fixed to be the\nChooseSubtree strategy for RLR-Tree.\n3.5 RLR-Tree Construction & Dynamic Updates\nWith the learned models for ChooseSubtree andSplit , we incorpo-\nrate the models into the insertion algorithm of R-Tree to build the\nRLR-Tree as follows. For each object to be inserted, in the top-down\ntraversal, we iteratively compute the state representation of a node\nand use the model trained for ChooseSubtree to select the subtree\ncorresponding to the action with the maximum ğ‘„-value until the\nobject is inserted into a leaf node. When a node overflows, we com-\npute its state representation and use the model trained for Split to\nchoose the split corresponding to the action with the maximum\nğ‘„-value. For dynamic updates, new data records can be inserted\ninto an existing tree using the trained models. Our experimental re-\nsults (Section 4.2.4) show that the trained models do not experience\nobvious performance deterioration even when there is a change in\ndata distribution.\n4 EXPERIMENTS\n4.1 Experimental Setup\nDatasets. We use 3 synthetic datasets (1-3) used in previous work\non spatial indices [1, 31, 32], and 2 large real-life datasets (4-5).\n(1)Skew (SKE): It consists of small squares of a fixed size. The ğ‘¥\nandğ‘¦coordinates of the squares centers are randomly gener-\nated from a uniform distribution in the range [0,1]and then\nâ€œsqueezedâ€ in the ğ‘¦-dimension, that is, each square center (ğ‘¥,ğ‘¦)\nis replaced by(ğ‘¥,ğ‘¦ğ‘)whereğ‘is the skewness and the default\nvalue of which is set to be 9;\n(2)Gaussian (GAU): It consists of small squares of a fixed size. The\ncoordinates of the center of a square are (ğ‘¥,ğ‘¦)whereğ‘¥and\nğ‘¦are randomly generated from a Gaussian distribution with\nmeanğœ‡=0.5and standard deviation ğœ=0.2;\n(3)Uniform (UNI): It consists of small squares of a fixed size. The\nğ‘¥andğ‘¦coordinates of the centers of the squares are randomly\ngenerated from a uniform distribution in the range [0,1];\n(4)OSM China (CHI): It contains more than 98 million locations in\nChina extracted from OpenStreetMap;\n(5)OSM India (IND): It contains more than 100 million locations\nin India extracted from OpenStreetMap.\nNote that the centers of all spatial data objects in synthetic\ndatasets fall within the unit square.\nQueries. For model training, we run range queries over both the\nreference R-Tree and the RLR-Tree. Range queries of different sizes\nare generated. When a range query is generated, we first set its\ncenter the same as the center of the last inserted object. Its length\nto width ratio is then randomly selected in the range [0.1,10].\nTo compare the query performance of RLR-Tree with others, we\n\nConferenceâ€™17, July 2017, Washington, DC, USA Tu Gu1, Kaiyu Feng1, Gao Cong1, Cheng Long1, Zheng Wang1, Sheng Wang2\nrandomly generate 1,000 range queries for each query size ranging\nfrom 0.005% to 2% of the whole region. The testing queries sizes\nfollow the setting in previous work [ 32]. Note that queries used for\ntraining and testing are generated separately and hence different.\nBaselines. We compare with R-Tree and its variants that are de-\nsigned for dynamic environments where updates occur frequently.\nBaselines used in the experiments include R-Tree [ 14], which is also\nthe reference tree used for model training, R*-Tree [ 2], RR*-Tree\n[3] which is reported to have the best query performance among\nR-Tree variants built using one-by-one insertion for supporting\nqueries in dynamic environments. We also compare with LISA [ 22]\nwhich is the only disk based learned index that returns exact results\nfor range queries and KNN queries. Note that LISA only supports\npoint data, so it is tested on CHI and IND datasets only. We do\nnot compare with packing R-Trees that are designed for the static\ndatabases [ 16] or other learned indices as they are not designed for\ndynamic environments as discussed in Section 5.\nMeasurements. For measurements of query performance, we con-\nsider both running time and the I/O cost. We find that both measures\nyield qualitatively consistent results (as exemplified in Figure 5).\nWe follow [ 22] to report the average relative I/O cost mainly. For\neach query, the relative I/O cost of an index is computed by the\nratio of the I/O cost for it to answer the query to the I/O cost for an\nR-Tree to answer the same query. Smaller relative I/O costs indicate\nbetter query performance compared with the R-Tree.\nParameter settings. Table 2 shows a list of parameters and their\ncorresponding values tested in our experiments. The default settings\nare bold. For all R-Tree variants evaluated in this paper, we maintain\na maximum of 50 and a minimum of 20 child nodes per tree node.\nTable 2: Parameters and Values\nParameters Values\nData distribution SKE, GAU , UNI\nDataset size (million) 1, 5, 10, 20, 100\nTraining set size (thousand) 25, 50, 100, 200\nTraining query size (%) 0.005, 0.01, 2\nTesting query size (%) 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2\nAction space size ğ‘˜ 2, 3, 5, 10\nNumber of dimensions 2, 4, 6, 8, 10\nThe DQN models for both ChooseSubtree andSplit contain 1\nhidden layer of 64 neurons with SELU [ 19] as the activation function.\nIn the training process, the learning rate is set to be 0.003 for RL\nChooseSubtree and 0.01 for RL Split. The initial value of ğœ–is set\nto be 1 and the decay rate is set to be 0.99. The value of ğœ–is never\nallowed to be less than 0.1 in order to maintain a certain degree of\nexploration throughout model training. The replay memory can\ncontain at most 5,000 (ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²)tuples. Network update is done by\nfirst sampling a batch of 64 tuples from the replay memory. Then\nÎ˜is updated by using gradient descent of the MSE loss function to\nclose the gap between the Q-value predicted by Î˜and the optimal\nQ-value derived from Î˜âˆ’. The discount factor is set to be 0.95 for\nRLChooseSubtree and 0.8 for RL Split . Synchronization of Î˜âˆ’with\nÎ˜is done once every 30 network updates.\nDuring the model training for RL ChooseSubtree (resp. RL Split ),\nthe deterministic splitting (resp. insertion) rules are set to be the\nsame as that used by the reference tree which is minimum overlap\npartition (resp. minimum node area enlargement). We train the RLChooseSubtree andSplit models for 20 and 15 epochs, respectively,\nand setğ‘ğ‘ğ‘Ÿğ‘¡ğ‘  in Algorithm 2 to be 15, i.e., the training dataset is\ndivided into 15 equal parts. The action space size ğ‘˜for both RL\nChooseSubtree and RL Split is set to be 2 by default. Note that the\ntrivial case of ğ‘˜=1simply gives us the reference tree.\nWe train our models on NVIDIA Tesla V100 SXM2 16 GB GPU\nusing PyTorch 1.3.1. All indices are coded using C++.\n4.2 Experimental Results\nOur experiments aim to find out:\n(1)Can RL ChooseSubtree and RL Split individually build better\nR-Trees (Section 4.2.1 and Section 4.2.2)?\n(2)Can RLR-Tree outperform the baselines for range queries\nand KNN queries (Section 4.2.3)?\n(3)How well RLR-Tree handles dynamic updates considering\nchanges in data distributions (Section 4.2.4)?\n(4)The effect of different parameters on performance such as the\ntraining dataset size, the action space size of the RL models\nand the training query size (Section 4.2.5);\n(5)How well RLR-Tree scales with dimensions and how large\nRLR-Treeâ€™s construction time and size are (Section 4.2.6)?\n4.2.1 RL ChooseSubtree .Figure 4 reports the average relative\nI/O cost of the RL ChooseSubtree on the 3 synthetic datasets by\nvarying the query region size and dataset size, respectively. RL\nChooseSubtree outperforms R-Tree consistently over different query\nsizes on all datasets. The best relative I/O cost is 0.07 and observed\non GAU dataset for query size 0.005%. We also observe that the per-\nformance of RL ChooseSubtree gets better as query size decreases.\nThis is because a query with a larger size intersects with a larger\nnumber of tree nodes, and more tree nodes will then be traversed\nwhen answering such a query. In this case, all R-Tree based indices\nneed to visit a larger portion of the data and the difference between\ndifferent indexing techniques will diminish. Consider an extreme\ncase where a range query covers the entire data space. Then all tree\nnodes will be traversed when answering the query, irrespective the\nindex used, and thus relative I/O cost will be close to 1.\nThe RL ChooseSubtree model also outperforms the R-Tree con-\nsistently over different dataset sizes. It is remarkable that although\nthe training dataset size is only 100,000, the trained model can\nbe applied on large datasets successfully. Furthermore, the model\nperformance gets better on larger datasets! This could be because\nas dataset gets larger the index tree becomes larger and more RL\nbased subtree selections are done on average. Hence, the accu-\nmulated benefits from the RL based ChooseSubtree enable the RL\nChooseSubtree model to outperform the R-Tree more significantly.\nQuery Processing Time. Figure 5 reports the query processing\ntime of RL ChooseSubtree on UNI dataset, where the left y-axis is\nthe average relative query time and the right y-axis is the average\nquery time. We observe that the average relative query time (green\nline) is generally consistent with the relative I/O cost of UNI re-\nported in Figure 4. Therefore, for the remaining of the paper, we\nreport only relative I/O cost as a measure of query performance\ndue to the space limit.\n4.2.2 RL Split. To evaluate the effect of RL Split on query per-\nformance, Figure 6 report results on the 3 synthetic datasets of\ndifferent sizes by running range queries of different sizes. RL Split\n\nA Reinforcement Learning Based R-Tree for Spatial Data Indexing in Dynamic Environments Conferenceâ€™17, July 2017, Washington, DC, USA\nFigure 4: Performance of RL ChooseSubtree\nFigure 5: RL ChooseSubtree Query Processing Time\noutperforms the R-Tree consistently over different query sizes, and\nthe improvement can be up to 80%. We also observe that the RL\nSplit model has better improvement on the R-Tree as the query\nsize decreases. Possible reasons would be similar as we discussed\nin Section 4.2.1 for RL ChooseSubtree .\nThe RL Split model also outperforms the R-Tree consistently over\ndatasets of different sizes. Note that RL Split is trained on a small\ntraining dataset with only 100,000 data objects, but can be applied\non large datasets successfully. Moreover, we also observe that as\ndataset size increases from 1 to 20 million, the improvement over the\nR-Tree generally increases on all 3 distributions. However, model\nperformance slightly deteriorates as dataset size increases from 20\nmillion to 100 million. The reason might be due to the inherent\ncomplicatedness of Split. Node splitting results in the creation of\n2 new nodes. As dataset size increases, the increase in tree height\nfurther contributes to this complicatedness because more nodes\nmay be split from one Split process, and the RL Split model trained\non 100,000 data objects would have more room for improvement.\nFigure 6: Performance of RL Split\n4.2.3 RLR-Tree. This set of experiments is to evaluate the per-\nformance of RLR-Tree, which is constructed from a combined RL\nChooseSubtree and RL Split model.\nThe Enhanced Training Process. We first compare the perfor-\nmance of RL ChooseSubtree , RL Split, Naive RLR-Tree, which is\nobtained by directly applying RL ChooseSubtree and RL Split , and\nRLR-Tree, which uses the enhanced training process (Section 3.4),\non all the 5 datasets in terms of relative I/O cost. As shown in Table\n3, by applying the enhanced training process, RLR-Tree has the\nbest performance on all datasets.\nRange queries. We evaluate the query performance of the RLR-\nTree on all the 5 datasets using range queries of different sizes.\nExperimental results are shown in Figure 7. We observe that RLR-\nTree outperforms the three baselines, the R-Tree, the R*-Tree, andTable 3: RL ChooseSubtree , RL Split and RLR-Tree\nSKE GAU UNI CHI IND\nRLR-Tree 0.21 0.06 0.54 0.56 0.65\nNaive RLR-Tree 0.24 0.08 0.58 0.61 0.67\nRLChooseSubtree 0.29 0.08 0.56 0.60 0.67\nRLSplit 0.22 0.28 0.58 0.63 0.71\nthe RR*-Tree, on all the synthetic datasets. The best performance\nis observed on the GAU dataset where RLR-Tree outperforms RR*-\nTree by 78.6% and R*-Tree by 92% for query size 0.005%. On the\nother hand, on the 2 real datasets, RLR-Tree outperforms R*-Tree,\nRR*-Tree and LISA by up to 27.3%, 22.9% and 40% respectively. On\nall the 5 datasets, RLR-Tree has more significant advantages over\nbaselines for smaller query sizes. This observation is consistent\nwith RL ChooseSubtree and RL Split and possible reasons have\nbeen discussed in section 4.2.1. For query size 2% all the indices\nhave similar performance, which is similar to the results reported\nin previous work [32].\nFigure 7: RLR-Tree Performance (Range Queries)\nFigure 8: RLR-Tree Performance (KNN Queries)\nKNN queries. To evaluate the performance of the RLR-Tree for\ntypes of queries that are not used in the model training process,\nwe look into the K-Nearest-Neighbor (KNN) queries, which is a\ntype of very popular spatial queries. A KNN query returns the\nğ¾nearest objects to a given query point. In our experiments, we\nconsider different ğ¾values, i.e.ğ¾âˆˆ{1,5,25,125,625}withğ¾=1\nbeing the default. For each ğ¾value, 1,000 uniformly distributed\nquery points are randomly generated in the data space. We use the\nalgorithms proposed in [ 34] to compute KNN queries accurately.\nIn Figure 8, we observe that the RLR-Tree outperforms R*-Tree,\n\nConferenceâ€™17, July 2017, Washington, DC, USA Tu Gu1, Kaiyu Feng1, Gao Cong1, Cheng Long1, Zheng Wang1, Sheng Wang2\nRR*-Tree and LISA by up to 93.5%, 30.4% and 42% respectively.\nThe RLR-Tree outperforms all the baselines in almost all the cases\nexcept for the SKE dataset. We also observe that the relative query\nperformance of RLR-Tree to R-Tree gets better for larger ğ¾values.\nThe finding that the RLR-Tree also outperforms the baselines is\nparticularly interesting â€”â€” the RLR-Tree is designed and trained to\noptimize the performance of range queries, rather than KNN queries.\nAdditionally, we compute the reward and design the state features\nfor the RLR-Tree in a way such that the model is trained to minimize\nthe number of nodes accesses when answering range queries and\nnot KNN queries. However, despite those design features that do\nnot favour KNN queries, RLR-Tree still has the best performance\nmost of the time for KNN queries.\n4.2.4 Effect of Data Change. We run a series of experiments to\nevaluate the robustness of the RLR-Tree when changes in data\ndistribution are encountered in dynamic environments.\nApply Trained Models on Different Data Distributions. In\nthis set of experiments, we train RL ChooseSubtree and RL Split\nmodels on one dataset and apply the trained models to build RLR-\nTree on a different dataset. Figure 9 reports relative I/O cost of the\nevaluated methods. For example, in Figure 9a, we use the models\ntrained on CHI and IND, respectively, to build different RLR-Trees\non IND. On CHI and IND, we observe that though the models\nare trained on a different dataset, the constructed RLR-Trees still\noutperform all baselines for all query sizes except 2%. Note that com-\npared with the RLR-Trees that are constructed by models trained on\nthe same dataset, these RLR-Trees only experience a small degree\nof performance deterioration. This is perhaps because these real\ndatasets share common features as they mostly consist of \"clusters\"\nof high data density which represent developed regions surrounded\nby vast regions of low data density which represent rural areas.\nIn contrast, for synthetic datasets, we use the models trained on\nthe SKE dataset to build RLR-Trees for the GAU dataset and the\nUNI dataset. We observe that the performance deterioration of the\nRLR-Trees constructed with models trained on a different dataset is\nmore significant. On GAU, although the RLR-Tree with SKE train-\ning is able to outperform all baselines for all query sizes, its query\nperformance is up to 57% poorer than the RLR-Tree with GAU\ntraining. On UNI, we observe that the performance of RLR-Tree\nwith SKE training is generally close to that of the RR*-Tree.\nDynamic Updates with Gradual Data Change. This experiment\nis to investigate how well the RLR-Tree handles dynamic updates\nwith gradual change of data distribution. Specifically, we first train\nand build 2 RLR-Trees of size 1 million using the GAU dataset\nwith (ğœ‡=0.5,ğœ=0.2) and the SKE dataset with ( ğ‘=9) with\ntheir default settings respectively. For the RLR-Tree built using\nthe GAU dataset, we insert up to 100 million spatial objects from\nGAU distribution with (ğœ‡=0.5,ğœ=0.1)and(ğœ‡=0.5,ğœ=0.4),\nand report the average relative I/O cost of 1,000 random queries,\nrespectively. For the RLR-Tree built using the SKE dataset, we insert\nup to 100 million spatial objects from SKE distribution with (ğ‘=5)\nand(ğ‘=1), and report the average relative I/O cost of 1,000 random\nqueries, respectively. Relevant experimental results are shown in\nFigure 10. We observe that despite inserting up to 100 times as many\ndata objects from a different data distribution, RLR-Tree generally\ndoes not experience obvious performance deterioration. For the\nGAU dataset, RLR-Tree consistently outperforms all baselines. On\n(a) IND Dataset\n (b) CHI Dataset\n(c) GAU Dataset\n (d) UNI Dataset\nFigure 9: Apply Trained Models on Other Data Distributions\nFigure 10: Updates with Different Data Distributions\nthe other hand, for the SKE dataset, RLR-Tree outperforms R*-Tree\nsignificantly but is slightly outperformed by RR*-Tree.\n4.2.5 The Effects of Parameters.\nTraining Dataset Size. This experiment is to evaluate the effect of\ntraining dataset size on the performance of the RLR-Tree. We would\nexpect better query performance if we train the RL ChooseSubtree\nand RL Split models on the full dataset. However, the training is slow\non large datasets. Instead, we propose to use a small training dataset.\nExperimental results are shown in Figure 11. First, we observe that\nthe training time of RL ChooseSubtree model for different data\ndistributions is similar, and increases significantly with the size of\nthe training dataset. As expected, the query performance of the\ntrained models improves as training dataset size increases from\n25,000 to 100,000. However, the query performance becomes stable\nafter the dataset size reaches 100,000, and the improvement of\nusing training dataset of size 200,000 over 100,000 is not significant.\nThe results for RL Split are qualitatively similar to those for RL\nChooseSubtree . Therefore we set training dataset size to be 100,000\nby default which achieves a good tradeoff between training time and\nquery performance. Note that RLR-Tree only needs to be trained\nonce on a small training dataset and the learned models can be used\non large dataset to build index and then to handle updates.\nThe Value of ğ‘˜.As shown in Figure 12a, as top ğ‘˜candidates are\nshortlisted to form the action space, the value of ğ‘˜has a direct\n\nA Reinforcement Learning Based R-Tree for Spatial Data Indexing in Dynamic Environments Conferenceâ€™17, July 2017, Washington, DC, USA\nFigure 11: Effect of Varying Training Dataset Sizes\nimpact on the query performance of the resulted RLR-Tree. On one\nhand, when ğ‘˜is larger, more actions are available to be selected\nfor the trained model. On the other hand, model performance can\nbe adversely affected when the action space is large as the trained\nmodel may not do a good job to filter out \"bad\" candidates. To find\na goodğ‘˜value, we test different values of ğ‘˜on the 3 synthetic\ndatasets of size 500,000.\n(a) Varying ğ‘˜Values\n (b) Varying Training Query Sizes\nFigure 12: The Effects of Parameters\nWe use RL ChooseSubtree as an example to show the effect of\nğ‘˜. We observe qualitatively similar trends on all the 3 synthetic\ndatasets. We make 2 observations. Firstly, recall that ğ‘˜=1is the\ntrivial case that generates the reference tree. By including one\nadditional candidate in the action space, i.e., when ğ‘˜=2, we achieve\nsignificant query performance improvement for RL ChooseSubtree .\nThe best performance improvement is 56% on the GAU dataset.\nSecondly, we observe that RL ChooseSubtree has the best result at\nğ‘˜=2for all the 3 datasets. As ğ‘˜value increases, model performance\ndeteriorates gradually. This observation aligns with our expectation.\nWhenğ‘˜value approaches and exceeds 10, the RL ChooseSubtree\nmodel starts to fail to outperform the R-Tree. We observe similar\ntrends for RL Split on the 3 datasets, and do not report the result\nhere due to space limitation.\nTraining Query Size. This experiment is to evaluate the effect of\nthe training query size on the query performance of the resulted\nRLR-Tree. Figure 12b reports the average query performance of RL\nChooseSubtree for each training query size for each dataset. We\nobserve that the query performance of RL ChooseSubtree is rather\npoor when using the largest training query size, i.e. 2%. On the GAU\ndataset, the model performance with training query size of 2% is\nmore than 100% poorer than that with our default setting of 0.01%.\nOn the other hand, when using training query size of 0.005%, model\nperformance is on par with our default setting on GAU dataset and\nshows slightly poorer results on SKE and UNI datasets. Therefore,\nwe set the training query size to be 0.01% by default. We observe\nsimilar trends for RL Split .\n4.2.6 Other Experiments.\nNumber of Dimensions. This experiment is to show how well\nRLR-Tree scales with dimensions. We vary the number of dimen-\nsions from 2 to 10. For each case, we generate a synthetic datasetwith 20 million objects following the Uniform distribution. The\nUniform distribution is chosen because it is commonly used for\nhigh dimension synthetic datasets in previous works [ 3,26]. For\neach dataset, we report the average relative I/O cost for answering\n1,000 random queries for each index in Figure 13. The average data\nselectivity of the query workload for each dataset is kept constant\nat 0.01%. We observe that RLR-Tree consistently outperforms all\nbaselines all the time. Moreover, its advantage becomes more sig-\nnificant as the number of dimensions increases, which illustrates\nthe robustness of RLR-Tree w.r.t. the number of dimensions.\nFigure 13: Changing Dimen-\nsions\nFigure 14: Index construction\ntime\nIndex Construction Time and Size. The index construction time\nis similar for different data distributions of the same size, and we\nuse the GAU dataset as an example. As shown in Figure 14, index\nconstruction time increases almost linearly with increase in dataset\nsize. Note that the construction time of RLR-Tree is comparable to\nthat of R*-Tree and RR*-Tree. And we expect RLR-Tree construction\ntime to be shorter when using better GPU devices. For index size, the\nRLR-Tree and other baselines almost have the same size. Therefore,\nwe report the RLR-Tree sizes for datasets of different sizes. As\nshown in Table 4, RLR-Tree size increases linearly as dataset size\nincreases.\nTable 4: Index size for GAU datasets\nDataset size (million) 1 5 10 20 100\nRLR-Tree size (MB) 39 195 390 780 3900\n5 RELATED WORK\n5.1 Spatial Indices\nWe next introduce three categories of spatial indices.\nData partitioning based indices: R-Tree and its variants. The\ncategory of indices partitions the dataset into subsets and indexes\nthe subsets. Typical examples include the R-Tree and its variants,\nsuch as Râˆ—-Tree [ 2], R+-Tree [ 37], RRâˆ—-Tree [ 3]. An R-Tree is usually\nbuilt through a one-by-one insertion approach, and is maintained\nby dynamic updates. As discussed in Section 1, these R-Tree variants\nare equipped with various heuristic rules for the two key operations\nof building an R-Tree, ChooseSubtree orSplit, but none of the\nrules have dominant query performance. This partly motivates\nus to design learning-based solutions for ChooseSubtree andSplit\noperations in this work.\nThere exist R-Tree variants that explore the query workload\nproperty or the special property of data. For example, in QUASII\n[29] and CUR-Tree [ 33], the query workload is utilized to build an\nR-Tree. These extensions are orthogonal to our work.\nData partitioning based indices: Packing. An alternative way\nof building an R-Tree is to pack data points into leaf nodes and\nthen build an R-Tree bottom up. The original R-Tree and almost\n\nConferenceâ€™17, July 2017, Washington, DC, USA Tu Gu1, Kaiyu Feng1, Gao Cong1, Cheng Long1, Zheng Wang1, Sheng Wang2\nall of its variants are designed for a dynamic environment, being\nable to handle insertions and deletions, while packing methods\nare for static databases [ 16]. Packing needs to have all the data\nbefore building indices, which are not always available in many\nsituations. The existing packing methods explore different ways of\nsorting spatial objects to achieve better ordering of the objects and\neventually better packing. Some ordering methods [ 1] are based on\nthe coordinates of objects, such as STR [ 21], TGS [ 12] and the lowx\npacked R-Tree [ 35] and other ordering methods are based on space\nfilling curves such as z-ordering [ 27,32], Gray coding [ 8] and the\nHilbert curve [9].\nOther spatial indices. In space partitioning based indices, such as\nkd-Tree [ 4] and Quad-Tree [ 11], the space is recursively partitioned\nuntil the number of objects in a partition reaches a threshold.\n5.2 Learned Indices\nLearned one-dimensional indices. The idea behind learned in-\ndex is to learn a function that maps a search key to the storage\naddress of a data object. The idea of learned indices is first intro-\nduced by [ 20], which proposes the Recursive Model Index (RMI) to\nlearn the data distribution in a hierarchical manner. It essentially\nlearns a cumulative distribution function (CDF) using a neural net-\nwork to predict the rank of a search key. The idea inspires a number\nof follow-up studies [6, 10, 18, 41] on learned indices.\nLearned spatial indices. Inspired by the idea of learned one-\ndimensional indices, several learned spatial indices have been pro-\nposed. The Z-order model [ 39] extends RMI to spatial data by using\na space filling curve to order data points and then learning the\nCDF to map the key of a data point to its rank. Recursive spatial\nmodel index (RSMI) [ 31] further develops the Z-order idea [ 39]\nand RMI. RSMI maps the data points to a rank space using the\nrank space-based transformation technique [ 32]. LISA [ 22] is a\ndisk-based learned index. It partitions the data space with a grid,\nnumbers the grid cells, and learns a data distribution based on this\nnumbering. Similar to Z-order model [ 39] and RSMI [ 31], Flood\n[26] also maps a dataset to a uniform rank space before learning\na CDF. Differently, it utilizes workload to optimize the learning\nof the CDF, and it learns the CDF of each dimension separately.\nTsunami [ 7] extends Flood to better utilize workload to overcome\nthe limitation of Flood in handling skewed workload and correlated\ndata. The ML-Index [ 5] generalizes the idea of the iDistance scaling\nmethod [ 15] to map point objects to a one-dimensional space and\nthen learns the CDF.\nRemark. These learned indices all aim to learn a CDF for a partic-\nular data to replace the traditional indices. However, RLR-Tree is\nfundamentally different â€”â€” Instead of learning any CDF, we train\nRL models to handle ChooseSubtree andSplit operations. Further-\nmore, these learned indices have the following limitations compared\nwith our solution: First, they can only handle spatial point objects\nwhile our proposed method is able to handle any spatial data, such\nas rectangular objects. Second, they all need customized algorithms\nto handle each type of query. They focus on certain types of queries\nand it is not clear how they can process other types of queries. For\nexample, some of them [ 5,7,26] do not consider KNN queries, an\nimportant type of spatial queries; Some learned indices [ 22] extend\ntheir algorithm for range queries to handle KNN queries by issuinga series of range queries until ğ‘˜points are found. However, the\nquery performance largely depends on the size of the region used.\nIn contrast, the RLR-Tree simply uses existing query processing\nalgorithms for R-Tree to handle different types of queries. Third,\nsome of these learned indices [ 31,39] return approximate query\nresults while our query results are accurate. Fourth, both Flood\nand Tsunami need the query workload as the input. However, we\ndo not assume the query workload to be known. Finally, updates\nare not discussed for Flood, Tsunami or the ML-Index. Although\nRSMI [ 31] and LISA [ 22] can handle updates, their models have\nto be retrained periodically to retain good query performances. In\ncontrast, our proposed RLR-Tree readily handles updates without\nthe need to keep retraining the models.\n5.3 Applications of Reinforcement Learning\nTo the best of our knowledge, no work is done to use RL to improve\non the R-Tree index or its variants. RL has been successfully applied\nto solve other database applications, such as database tuning tasks\n[24,38,44], similarity search [ 40], join order selection [ 43], index\nselection [ 36], and QD-Tree for data partitioning [ 42]. In particular,\nto build a QD-Tree, an RL model is trained to learn a policy to\nmake partitioning decisions to maximize the data skipping ratio\nfor a given query workload. The MDP design of our RLR-Tree\nis fundamentally different from that of QD-Tree, which is based\non NeuroCuts [ 23], in at least three aspects. 1) State design: QD-\nTree follows a tree-structured MDP where a cut at a node (state)\ncreates 2 new nodes (next states). In contrast, in an RLR-Tree, an\nRL ChooseSubtree decision at a node (state) leads to the selected\nchild node (next state) and an RL Split decision at a node (state)\nleads to the parent node (next state). 2) Action space: For QD-Tree,\nthe query workload is assumed to be known and candidate cuts are\ngenerated from a standard SQL planner. For RLR-Tree, we do not\nassume any prior knowledge of the query workload and have to\ncome out with suitable actions ourselves. 3) Reward: Among others,\nQD-Tree computes rewards after the whole tree is built, while RLR-\nTree computes rewards after each splitting/insertion process. We\nwould also like to highlight that QD-Tree cannot handle updates as\nthe RLR-Tree does.\n6 CONCLUSIONS AND FUTURE WORK\nWe propose the RLR-Tree to improve on the R-Tree and its variants\ndesigned for a dynamic environment. Experimental results show\nthat RLR-Tree is able to outperform the RR*-Tree, the best R-Tree\nvariant, by up to 78.6% for range queries and up to 30.4% for KNN\nqueries. Although the models of the RLR-Tree are trained on a\nsmall training dataset of size 100,000, the trained models are readily\napplied on large datasets and are able to handle dynamic updates\neven with changes in data distribution.\nThis work take a first step to use machine learning to improve\non the R-Tree, and we believe it we would open a few promising\ndirections for future work: 1) further explore and refine the designs\nof the states, the action space and the reward signal; 2) extend the\nidea to index enriched spatial data, such as spatial-temporal data,\nmoving objects, and spatial-textual data; 3) explore other machine\nlearning models.\n\nA Reinforcement Learning Based R-Tree for Spatial Data Indexing in Dynamic Environments Conferenceâ€™17, July 2017, Washington, DC, USA\nREFERENCES\n[1]Lars Arge, Mark De Berg, Herman Haverkort, and Ke Yi. 2008. The priority\nR-tree: A practically efficient and worst-case optimal R-tree. ACM Transactions\non Algorithms (TALG) 4, 1 (2008), 1â€“30.\n[2]Norbert Beckmann, Hans-Peter Kriegel, Ralf Schneider, and Bernhard Seeger.\n1990. The R*-tree: an efficient and robust access method for points and rectangles.\nInProceedings of the 1990 ACM SIGMOD international conference on Management\nof data . 322â€“331.\n[3]Norbert Beckmann and Bernhard Seeger. 2009. A revised r*-tree in comparison\nwith related index structures. In Proceedings of the 2009 ACM SIGMOD Interna-\ntional Conference on Management of data . 799â€“812.\n[4]Jon Louis Bentley. 1975. Multidimensional binary search trees used for associative\nsearching. Commun. ACM 18, 9 (1975), 509â€“517.\n[5]Angjela Davitkova, Evica Milchevski, and Sebastian Michel. 2020. The ML-Index:\nA Multidimensional, Learned Index for Point, Range, and Nearest-Neighbor\nQueries.. In EDBT . 407â€“410.\n[6]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\net al.2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data . 969â€“984.\n[7]Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A learned multi-dimensional index for correlated data and skewed\nworkloads. arXiv preprint arXiv:2006.13282 (2020).\n[8]Christos Faloutsos. 1986. Multiattribute hashing using gray codes. In Proceedings\nof the 1986 ACM SIGMOD international conference on Management of data . 227â€“\n238.\n[9]Christos Faloutsos and Shari Roseman. 1989. Fractals for secondary key re-\ntrieval. In Proceedings of the eighth ACM SIGACT-SIGMOD-SIGART symposium\non Principles of database systems . 247â€“252.\n[10] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proceedings of the\nVLDB Endowment 13, 10 (2020), 1162â€“1175.\n[11] Raphael A. Finkel and Jon Louis Bentley. 1974. Quad trees a data structure for\nretrieval on composite keys. Acta informatica 4, 1 (1974), 1â€“9.\n[12] YvÃ¡n J GarcÃ­a R, Mario A LÃ³pez, and Scott T Leutenegger. 1998. A greedy\nalgorithm for bulk loading R-trees. In Proceedings of the 6th ACM international\nsymposium on Advances in geographic information systems . 163â€“164.\n[13] Diane Greene. 1989. An implementation and performance analysis of spatial data\naccess methods. In Proceedings. Fifth International Conference on Data Engineering .\nIEEE Computer Society, 606â€“607.\n[14] Antonin Guttman. 1984. R-trees: A dynamic index structure for spatial searching.\nInProceedings of the 1984 ACM SIGMOD international conference on Management\nof data . 47â€“57.\n[15] Hosagrahar V Jagadish, Beng Chin Ooi, Kian-Lee Tan, Cui Yu, and Rui Zhang.\n2005. iDistance: An adaptive B+-tree based indexing method for nearest neighbor\nsearch. ACM Transactions on Database Systems (TODS) 30, 2 (2005), 364â€“397.\n[16] Ibrahim Kamel and Christos Faloutsos. 1993. On packing R-trees. In Proceedings\nof the second international conference on Information and knowledge management .\n490â€“499.\n[17] Kothuri Venkata Ravi Kanth, Divyakant Agrawal, Ambuj K Singh, and Amr El Ab-\nbadi. 1997. Indexing non-uniform spatial data. In Proceedings of the 1997 Interna-\ntional Database Engineering and Applications Symposium (Cat. No. 97TB100166) .\nIEEE, 289â€“298.\n[18] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In Proceedings of the Third International Workshop on Exploiting Artificial\nIntelligence Techniques for Data Management . 1â€“5.\n[19] GÃ¼nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter.\n2017. Self-normalizing neural networks. Advances in neural information processing\nsystems 30 (2017), 971â€“980.\n[20] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 International\nConference on Management of Data . 489â€“504.\n[21] Scott T Leutenegger, Mario A Lopez, and Jeffrey Edgington. 1997. STR: A sim-\nple and efficient algorithm for R-tree packing. In Proceedings 13th International\nConference on Data Engineering . IEEE, 497â€“506.\n[22] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A\nlearned index structure for spatial data. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data . 2119â€“2133.\n[23] Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica. 2019. Neural packet classification.\nInProceedings of the ACM Special Interest Group on Data Communication . 256â€“269.\n[24] Xi Liang, Aaron J Elmore, and Sanjay Krishnan. 2019. Opportunistic view ma-\nterialization with deep reinforcement learning. arXiv preprint arXiv:1903.01363\n(2019).\n[25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, et al .2015. Human-level control through deep reinforcement learning.Nature 518, 7540 (2015), 529â€“533.\n[26] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning multi-dimensional indexes. In Proceedings of the 2020 ACM SIGMOD Interna-\ntional Conference on Management of Data . 985â€“1000.\n[27] Jack A Orenstein. 1986. Spatial query processing in an object-oriented database\nsystem. In Proceedings of the 1986 ACM SIGMOD international conference on\nManagement of data . 326â€“336.\n[28] Varun Pandey, Alexander van Renen, Andreas Kipf, Ibrahim Sabek, Jialin Ding,\nand Alfons Kemper. 2020. The case for learned spatial indexes. arXiv preprint\narXiv:2008.10349 (2020).\n[29] Mirjana Pavlovic, Darius Sidlauskas, Thomas Heinis, and Anastasia Ailamaki.\n2018. QUASII: query-aware spatial incremental index. In 21st International Con-\nference on Extending Database Technology (EDBT) .\n[30] Martin L Puterman. 2014. Markov decision processes: discrete stochastic dynamic\nprogramming . John Wiley & Sons.\n[31] Jianzhong Qi, Guanli Liu, Christian S Jensen, and Lars Kulik. 2020. Effectively\nlearning spatial indices. Proceedings of the VLDB Endowment 13, 12 (2020), 2341â€“\n2354.\n[32] Jianzhong Qi, Yufei Tao, Yanchuan Chang, and Rui Zhang. 2018. Theoretically\noptimal and empirically efficient r-trees with strong parallelizability. Proceedings\nof the VLDB Endowment 11, 5 (2018), 621â€“634.\n[33] Kenneth A Ross, Inga Sitzmann, and Peter J Stuckey. 2001. Cost-based unbal-\nanced R-trees. In Proceedings Thirteenth International Conference on Scientific and\nStatistical Database Management. SSDBM 2001 . IEEE, 203â€“212.\n[34] Nick Roussopoulos, Stephen Kelley, and FrÃ©dÃ©ric Vincent. 1995. Nearest neighbor\nqueries. In Proceedings of the 1995 ACM SIGMOD international conference on\nManagement of data . 71â€“79.\n[35] Nick Roussopoulos and Daniel Leifker. 1985. Direct spatial search on picto-\nrial databases using packed R-trees. In Proceedings of the 1985 ACM SIGMOD\ninternational conference on Management of data . 17â€“31.\n[36] Zahra Sadri, Le Gruenwald, and Eleazar Leal. 2020. Online index selection using\ndeep reinforcement learning for a cluster database. In 2020 IEEE 36th International\nConference on Data Engineering Workshops (ICDEW) . IEEE, 158â€“161.\n[37] Timos Sellis, Nick Roussopoulos, and Christos Faloutsos. 1987. The R+-Tree: A\nDynamic Index for Multi-Dimensional Objects. Technical Report.\n[38] Immanuel Trummer, Junxiong Wang, Deepak Maram, Samuel Moseley, Saehan\nJo, and Joseph Antonakakis. 2019. Skinnerdb: Regret-bounded query evaluation\nvia reinforcement learning. In Proceedings of the 2019 International Conference on\nManagement of Data . 1153â€“1170.\n[39] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned Index for Spatial\nQueries. In 2019 20th IEEE International Conference on Mobile Data Management\n(MDM) . IEEE, 569â€“574.\n[40] Zheng Wang, Cheng Long, Gao Cong, and Yiding Liu. 2020. Efficient and effective\nsimilar subtrajectory search with deep reinforcement learning. Proceedings of\nthe VLDB Endowment 13, 12 (2020), 2312â€“2325.\n[41] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao\nXing. 2021. Updatable Learned Index with Precise Positions. arXiv preprint\narXiv:2104.05520 (2021).\n[42] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li,\nUmar Farooq Minhas, Per-Ã…ke Larson, Donald Kossmann, and Rajeev Acharya.\n2020. Qd-tree: Learning data layouts for big data analytics. In Proceedings of the\n2020 ACM SIGMOD International Conference on Management of Data . 193â€“208.\n[43] Xiang Yu, Guoliang Li, Chengliang Chai, and Nan Tang. 2020. Reinforcement\nlearning with tree-lstm for join order selection. In 2020 IEEE 36th International\nConference on Data Engineering (ICDE) . IEEE, 1297â€“1308.\n[44] Ji Zhang, Yu Liu, Ke Zhou, Guoliang Li, Zhili Xiao, Bin Cheng, Jiashu Xing,\nYangtao Wang, Tianheng Cheng, Li Liu, et al .2019. An end-to-end automatic\ncloud database tuning system using deep reinforcement learning. In Proceedings\nof the 2019 International Conference on Management of Data . 415â€“432.",
  "textLength": 81179
}