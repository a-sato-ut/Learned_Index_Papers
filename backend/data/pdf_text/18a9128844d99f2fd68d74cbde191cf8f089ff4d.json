{
  "paperId": "18a9128844d99f2fd68d74cbde191cf8f089ff4d",
  "title": "Learned Accelerator Framework for Angular-Distance-Based High-Dimensional DBSCAN",
  "pdfPath": "18a9128844d99f2fd68d74cbde191cf8f089ff4d.pdf",
  "text": "Learned Accelerator Framework for Angular-Distance-Based\nHigh-Dimensional DBSCAN\nYifan Wang\nUniversity of Florida\nGainesville, FL, USA\nwangyifan@ufl.eduDaisy Zhe Wang\nUniversity of Florida\nGainesville, FL, USA\ndaisyw@ufl.edu\nABSTRACT\nDensity-based clustering is a commonly used tool in data science.\nToday many data science works are utilizing high-dimensional\nneural embeddings. However, traditional density-based clustering\ntechniques like DBSCAN have a degraded performance on high-\ndimensional data. In this paper, we propose LAF, a generic learned\naccelerator framework to speed up the original DBSCAN and the\nsampling-based variants of DBSCAN on high-dimensional data\nwith angular distance metric. This framework consists of a learned\ncardinality estimator and a post-processing module. The cardinality\nestimator can fast predict whether a data point is core or not to\nskip unnecessary range queries, while the post-processing module\ndetects the false negative predictions and merges the falsely sepa-\nrated clusters. The evaluation shows our LAF-enhanced DBSCAN\nmethod outperforms the state-of-the-art efficient DBSCAN variants\non both efficiency and quality.\n1 INTRODUCTION\nToday‚Äôs data science research benefits significantly from neural\nembeddings that are high-dimensional vectors generated by deep\nneural models. As a widely applied technique in data science, clus-\ntering has been associated with embeddings, e.g., [ 22,28] learn\neffective passage embeddings with clustering, [ 19,23] utilize clus-\ntering to accelerate the similarity search over embeddings, etc.\nAs a representative clustering algorithm, Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) [ 7] has a long\nhistory of being applied on low-dimensional spatial data (2D or\n3D). However, DBSCAN usually has a low efficiency, caused by\nthe compute-intensive range queries and becoming more signifi-\ncant on high-dimensional data due to the curse of dimensionality.\nSpecifically, DBSCAN considers clusters to be high-density areas\nseparated by low-density areas. Based on this, the algorithm re-\npeatedly expands each cluster to its neighboring high-density areas\n(where the points are called core points ) until the cluster is com-\npletely surrounded by low-density areas (where each point is either\nanon-core point or a noise). For each point, DBSCAN has to do a\nheavy range search to determine whether it is core or not, which\nrequires intensive computation and limits its application in large-\nscale high-dimensional data analysis. To improve the efficiency of\nDBSCAN, previous works propose many variants, e.g., sampling-\nbased DBSCAN variants [ 5,11,12,14,21] improve the efficiency\nby executing the heaviest computation within a small subset in-\nstead of the whole dataset, some other works reduce the latency\nby accelerating the range queries in DBSCAN [ 15,26], [1‚Äì4,8,9]\nprune unnecessary distance computation during the clustering,\netc. But many of them are designed for low to middle dimensional\ndata (mostly less than 100-dimensions). Therefore they are still notsuitable for high-dimensional neural embeddings with hundreds to\nthousands of dimensions (e.g., the 768-dim BERT [ 6] embeddings).\nIn this paper, we solve this problem by skipping the range queries\nfor non-core and noise points, given that only the number of neigh-\nbors is needed to confirm the point is non-core or noise. And this\ncan be done by cardinality estimation, i.e., the techniques to es-\ntimate the number of results before executing a query. In multi-\ndimensional data management, cardinality estimation is usually\nused to predict the number of neighbors from a distance-based\nrange query without executing it, and the estimation results can\nhelp optimize the critical operations like range search and similarity\njoin. Traditional cardinality estimation for range queries relies on\nsampling or kernel density estimation [ 17]. Recent works apply\nadvanced machine learning to it and propose the learned cardinal-\nity estimation techniques. They are normally based on regression\nmodels from non-deep regressors (e.g., XGBoost) to deep regressors\n(e.g., deep neural networks), whose input is the query point and the\ndistance threshold (i.e., the range), and output is the estimated num-\nber of neighbors in that range. By learning the data distribution,\nlearned cardinality estimation makes more accurate prediction than\nthe traditional approaches. The state-of-the-art learned cardinality\nestimation methods deploy various deep regression models, e.g.,\nConvolutional neural network [ 18], Recursive Model Index [ 13,24],\nDeep Lattice Network [ 27], CardNet [ 24], SelNet [ 25], etc., which\nperform effectively on high-dimensional data. And they can achieve\nhigh predicting efficiency both theoretically and practically. In the-\nory, when a model structure is fixed, its prediction time complexity\nis constant with the data scale, while in practice, the models can\nbe significantly accelerated by GPU. The training time is not an\nissue due to the generalization capability of the neural models, i.e.,\na trained estimator can be used on any other dataset with similar\ndistribution. With a learned cardinality estimator, whether a point\nis core or not can be predicted before executing the range query, by\nwhich the unnecessary computation on non-core and noise points\nwill be effectively reduced.\nParticularly, our approach is designed for clustering based on\nangular distance, like cosine distance, and in this paper we will\nnot investigate other distance metrics like Euclidean distance, due\nto two reasons: (1) Angular distance is worth being specifically\nstudied. In neural embedding based applications, cosine and Eu-\nclidean distances are the dominant distance metrics for measuring\nembedding similarity. So focusing on cosine distance is enough to\nbenefit a wide range of applications. (2) Our idea is theoretically\nmore suitable for this metric. Angular distance is usually bounded,\ne.g., cosine distance is within the range 0 ‚àº2, which makes training\nof the cardinality estimator more effective than using Euclidean dis-\ntance. Specifically, a regressor normally makes better predictionsarXiv:2302.03136v1  [cs.IR]  6 Feb 2023\n\nConference‚Äô17, July 2017, Washington, DC, USA Yifan Wang and Daisy Zhe Wang\nwhen the training set covers more possible input values, which\nis hard for Euclidean distance whose value range is infinite (i.e.,\n0‚àº+‚àû ), but easier for the bounded cosine distance. For example, in\nour evaluation we construct the training set using cosine distance\nthresholds from 0.1 to 0.9, which is enough to cover most cases.\nTherefore, other distances are out of scope for this paper. However,\nour method does not have a hard constraint on the distance metric,\nso we may explore Euclidean distance in future work.\nWe propose LAF, a generic Learned Accelerator Framework\nto speed up DBSCAN and its sampling-based variants based on\nangular distance. LAF enhances the algorithm efficiency by plac-\ning an extra cardinality estimation step before each range query.\nIf a point is predicted as non-core/noise, the range query for it\nwill be skipped to reduce the computation. This approach works\nnot only on DBSCAN, but also on its sampling-based variants,\nas the same kind of computation waste also exists in processing\nthe sampled subset. LAF also includes a post-processing module\nto compensate for the clustering quality loss by detecting and\nmerging the wrongly separated clusters caused by the false neg-\native predictions (i.e., predicting core points as non-core/noise).\nTo our best knowledge, we are one of the first studies that im-\nprove the efficiency of high-dimensional DBSCAN-like clustering\nby cardinality estimation. And we have open-sourced the code at\nhttps://github.com/wyfunique/LAF-DBSCAN.\nOur LAF-enhanced DBSCAN outperforms the state-of-the-art\napproximate DBSCAN variants in the evaluation. Specifically, LAF-\nenhanced DBSCAN achieves up to 2.9x speedup for DBSCAN and is\n60%‚àº140% faster than the state-of-the-art approximate DBSCAN\nvariants, with high clustering quality on high-dimensional vectors,\nand the selected sampling-based DBSCAN variant is also acceler-\nated significantly by LAF (i.e., up to 6.7x speedup) with only tiny or\nno quality loss. The main contributions of this paper are as follows:\n(1)We develop LAF, a generic learned accelerator framework to\naccelerate a wide range of DBSCAN-like clustering algorithms.\n(2)We propose a novel efficient high-dimensional DBSCAN algo-\nrithm using the framework.\n(3)We conduct experiments on popular high-dimensional datasets\nand show the high performance of our proposed algorithm\nand the usefulness of LAF.\n2 THE APPROACH\n2.1 DBSCAN, LAF and enhanced DBSCAN\nAlgorithm 1 shows the DBSCAN pseudocode in black text (the red\ntext is inserted by LAF, which is introduced below). DBSCAN classi-\nfies the data points as core, non-core and noise points, depending on\nthe number of their neighbors within a range. Given a distance func-\ntionùëë(¬∑,¬∑)and a distance threshold ùúñ, DBSCAN does a range query\nfor each data point P to find its neighbors N={ùëÑ|ùëë(ùëÉ, ùëÑ)<ùúñ}. If\nit has at least ùúèneighbors, point P is a core point and the current\ncluster will be expanded to its neighbors; otherwise P is non-core or\nnoise, and the current cluster will not grow from P to its neighbors.\nWhen the current cluster cannot grow any more, the next cluster\nwill start from some other un-clustered core points. Such a process\nis repeated until there are no more unclassified (i.e., undefined in\nAlgorithm 1) points. The difference between non-core and noise\npoints is whether it has a core point neighbor. If a point itself isnot core but has at least one neighbor that is core point, then it is\na non-core point and will be part of the cluster boundary, while a\nnoise point has no core point neighbor and will not be classified\ninto any cluster. For simplification, in the rest of this paper we\ndenote both of the non-core and noise points as stop points when it\nis unnecessary to distinguish between them.\nLAF works as a plugin to the target algorithm: (1) The cardinality\nestimator is placed right before each range query, and the range\nquery will be executed only if the current point is predicted as core.\n(2) The post-processing module is inserted at the end of the clus-\ntering to detect false negative predictions which wrongly estimate\ncore points as not, and merge the clusters separated by such false\nstop points. This is to compensate for the effectiveness loss caused\nby the prediction error. Based on LAF, we implement an efficient\nhigh-dimensional DBSCAN, called LAF-enhanced DBSCAN (a.k.a,\nLAF-DBSCAN). We show its pseudocode in Algorithm 1 and use red\ntext to highlight the inserted lines by LAF, while the other lines are\nthe same as original DBSCAN. We also develop LAF-DBSCAN++ ,\nan enhanced version of DBSCAN++ [ 11] by LAF, to present the\ncapability of LAF for accelerating variants of DBSCAN. The details\nare discussed in Section 3.\nBasically, LAF inserts three critical functions: CardEst , UpdatePar-\ntialNeighbors and PostProcessing , as well as a mapErecording all\nthe points which are predicted as stop points (called predicted stop\npoints ) and their partial neighbors . Here we use the term ‚Äúpartial\nneighbors‚Äù because for each predicted stop point, Edoes not record\nall its neighbors, but only a subset generated by UpdatePartialNeighbors .\nCardEst is simply using cardinality estimator to predict number\nof the range query results, while the other two functions are both\nfor the post-processing. Note that we do not use the exact value of\nùúèwith CardEst to predict whether a point is core or not. Instead,\nùúèis multiplied with a positive factor ùõºto threshold the predicted\ncardinality, as shown in line 6 and 22 of Algorithm 1. Here ùõºis\nused to adjust the false positive and false negative rates, such that\nusers can control the prediction error and manipulate the speed-\nquality trade-off. Specifically, when ùõºincreases, false negative rate\nincreases as more predictions become lower than the threshold,\nresulting in higher speed and lower quality. When ùõºdecreases, false\npositive rate increases, leading to lower speed and higher quality.\nIf a point is predicted to be stop point (line 6-9, 26-27), the corre-\nsponding entry will be added into E, otherwise the range query will\nbe executed and the point will be double checked with the query re-\nsults. At the same time, Ewill be updated by UpdatePartialNeighbors\nusing the query results. Finally the post-processing uses Eto update\nthe clustering results C.\n2.2 Post-processing strategy\nErecords the partial neighbors (i.e., a subset of the true neighbors)\nfor each predicted stop point. It is filled by UpdatePartialNeighbors\nin such a way (as shown in Algorithm 2): if a predicted stop point\nPùëõis found by another point P as neighbor, then P is also neighbor\nof Pùëõand will be added to E(Pùëõ). Function PostProcessing (Algo-\nrithm 3) detects the false predicted stop points and merges the\nclusters separated by those points. Specifically, a point P in Eis a\nfalse negative if it has at least ùúèpartial neighbors (line 2). In such\ncase PostProcessing will randomly select a cluster around it as the\n\nLearned Accelerator Framework for Angular-Distance-Based High-Dimensional DBSCAN Conference‚Äô17, July 2017, Washington, DC, USA\nAlgorithm 1 LAF-enhanced DBSCAN (LAF-DBSCAN)\nInput: DatasetD, distance function ùëë(¬∑,¬∑), distance threshold ùúñ, minimum\nnumber of neighbors ùúè, error factor ùõº\nOutput: the map from points to their cluster IDs C\n1:Cluster ID c := 0\n2:Map from predicted stop points to partial neighbors E:=‚àÖ\n3:for each point P inDdoC(P) := undefined\n4:for each point P inDdo\n5: ifC(P)‚â†undefined then continue\n6: ifCardEst(P) < ùõºùúèthen\n7:C(P) := noise\n8: ifP not inEthenE(P) :=‚àÖ\n9: continue\n10: NeighborsN:= RangeQuery(D,ùëë, P,ùúñ)\n11:E:= UpdatePartialNeighbors(P, N,E)\n12: if|N| <ùúèthen\n13:C(P) := noise\n14: continue\n15: c := c + 1\n16:C(P) := c\n17:S:=N- {P}\n18: for each point Q inSdo\n19: ifC(Q) = noise thenC(Q) := c\n20: ifC(Q)‚â†undefined then continue\n21:C(Q) := c\n22: ifCardEst(Q)‚â•ùõºùúèthen\n23:N:= RangeQuery(D,ùëë, Q,ùúñ)\n24:E:= UpdatePartialNeighbors(Q, N,E)\n25: if|N|‚â•ùúèthenS:=S‚à™N\n26: else\n27: ifQ not inEthenE(Q) :=‚àÖ\n28:C:= PostProcessing(C,E,ùúè)\n29:returnC\nAlgorithm 2 UpdatePartialNeighbors\nInput: Data point P, its neighbors N, the mapE\nOutput: the updatedE\n1:for each neighbor P ùëõinNdo\n2: ifPùëõis inEthenE(Pùëõ) :=E(Pùëõ)‚à™{P}\n3:returnE\nAlgorithm 3 PostProcessing\nInput: the mapCfrom point to cluster, the map E,ùúè\nOutput: the updatedC\n1:for each point P inEdo\n2: if|E(P)|‚â•ùúèthen\n3: Randomly select a non-noise neighbor P‚Ä≤in setE(P)\n4: Destination cluster ID ùëê‚Ä≤:=C(P‚Ä≤)\n5: Merge the clusters of E(P) into the destination cluster.\n6:returnC\ndestination cluster (line 3-4), and merge the rest wrongly separated\nclusters to the destination (line 5).\n3 EXPERIMENTS\n3.1 Experiment settings\nEnvironment: A Lambda Quad workstation with 28 3.30GHz Intel\nCore i9-9940X CPUs, 4 RTX 2080 Ti GPUs and 128 GB RAM.\nDatasets: Table 1 provides an overview for our evaluation datasets,\nreporting their sizes, data dimensions, error factors used in evalua-\ntion, and their vector types. We introduce more details here:\n(1)NYTimes: 300k bag-of-words vectors of NYTimes news arti-\ncles. We randomly sample 150k vectors from them, normal-\nize the samples and reduce their dimension to 256 throughGaussian random projection, which is the same way as ANN-\nbenchmark1. The resulting dataset is named NYT-150k .\n(2)Glove: 1.2M word embeddings (200-dimensional) pre-trained\non tweets. We sample 150k vectors from them and name the\nsampled dataset Glove-150k .\n(3)MS MARCO [ 16]: a benchmarking dataset for passage re-\ntrieval, including 8.8M passages. We follow a similar way to\n[23] to process this dataset, i.e., generating a 768-dimensional\nembedding for each passage using the same deep model as\n[23], sampling the embeddings into several subsets and nam-\ning them as ‚ÄúMS-‚Äù followed by the size (e.g., ‚ÄúMS-100k‚Äù in-\ncludes around 100k embeddings). In this paper we sample 3\ndatasets, MS-50k, MS-100k and MS-150k.\nIn addition, we normalize all the data vectors and split each dataset\ninto training and testing sets by a ratio of 8:2. For each dataset, we\nfirst train the learned cardinality estimator on the training set, then\nevaluate all the methods on the corresponding testing set, i.e., all\nthe reported experiment results are collected on those testing sets.\nDataset #Points Dimùõº Type\nNYT-150k 150,000 256 1.15 Bag-of-words\nGlove-150k 150,000 200 2.0 Word embedding\nMS-150k 152,185 768 7.7 Passage embedding\nMS-100k 107,400 768 2.0 Passage embedding\nMS-50k 53,700 768 1.5 Passage embedding\nTable 1: Evaluation dataset information, including the num-\nber of points ( #Points ), data dimension ( Dim), error factor ùõº\nof LAF-DBSCAN on each of them, and the vector type ( Type ).\nMetrics: As discussed in Section 1, the distance metric in the\nevaluation is cosine distance. For some baselines which support\nEuclidean distance only, since all data points are normalized, we use\nEquation 1 to convert cosine distance ( ùëëùëêùëúùë†) into Euclidean distance\n(ùëëùëíùë¢ùëê), such that the distances in our methods are equivalent to those\nin the baselines. For example, by the equation, when ùëëùëêùëúùë†=0.5, the\nequivalent ùëëùëíùë¢ùëê=1.0, so if we set the distance threshold ùúñ=0.5in\nour methods, the threshold in the baselines will be set as 1.0.\nùëëùëíùë¢ùëê(¬Æùë¢,¬Æùë£)=‚àöÔ∏Å\n2ùëëùëêùëúùë†(¬Æùë¢,¬Æùë£) (if‚à•¬Æùë¢‚à•=‚à•¬Æùë£‚à•=1) (1)\nThe evaluation metrics include efficiency and effectiveness met-\nrics. For efficiency, the metric is elapsed time of clustering (including\nthe cardinality estimator prediction time and excluding its training\ntime). For effectiveness, the metrics are (1) adjusted RAND index\n(ARI) [ 10] and (2) adjusted mutual information score (AMI) [ 20],\ncomputed against the ground truth. A higher score means a better\nclustering quality. Here we use the clustering results of original\nDBSCAN as ground truth.\nOur methods: In addition to LAF-DBSCAN, we also use LAF to\naccelerate a sampling-based DBSCAN variant, DBSCAN++ [ 11].\nThe resulting method is named LAF-DBSCAN++ , whose goal is\nto present that LAF works not only on DBSCAN but also on its\nsampling-based variants (as mentioned in Section 1). So it just acts\nas an auxiliary method and our major method is still LAF-DBSCAN\nin the evaluation. In both methods, the cardinality estimator model\nis an RMI [ 13] with three stages, respectively including 1, 2, 4 fully-\nconnected neural networks from top to bottom stage. Each neural\nnetwork has 4 hidden layers whose widths are 512, 512, 256, and\n1https://github.com/erikbern/ann-benchmarks\n\nConference‚Äô17, July 2017, Washington, DC, USA Yifan Wang and Daisy Zhe Wang\n128. Such an estimator has been used as a strong baseline in [ 24],\nfrom where we borrow the code directly. On each training set, the\ncardinality estimator is trained for 200 epochs with batch size 512.\nThough there are also other learned cardinality estimators, like\nCardNet [ 24] and SelNet [ 25], we will not explore which estimator\nis the best for our methods, as it is out of scope for this paper.\nSpecifically, the goal of this paper is to reveal the potential of such\na new idea on speeding up DBSCAN, and in our evaluation the RMI\nhas already performed well enough to demonstrate the potential.\nBaselines: The baselines are described as follows:\n(1)DBSCAN: the original DBSCAN. Its clustering results are\nused as the ground truth for other methods.\n(2)DBSCAN++2[11]: an approximate DBSCAN variant that\nspeeds up DBSCAN by sampling the dataset and limiting\nthe heaviest computation within the samples. Specifically,\nDBSCAN++ samples a subset of data points, within which\nthe core points are detected w.r.t. the entire dataset. Then\nthe clusters first grow around those core points within the\nsubset, and finally all the unclassified points outside the\nsubset are directly assigned to their closest core points. Our\nLAF-DBSCAN++ method is built on top of DBSCAN++.\n(3)KNN-BLOCK DBSCAN3[3]: an approximate DBSCAN vari-\nant which improves efficiency by pruning unnecessary dis-\ntance computation with K-nearest neighbor queries. We de-\nnote it as ‚ÄúKNN-BLOCK‚Äù in the tables and figures.\n(4)BLOCK-DBSCAN4[2]: a method similar to KNN-BLOCK\nDBSCAN, but facilitated by cover tree based range queries.\n(5)ùúå-approximate DBSCAN5[8, 9]: an approximate DBSCAN\nvariant which accelerates DBSCAN by relaxing the density\ncriteria with an approximation factor ùúå(ùúå>0).\nOur methods and the baselines are all implemented mainly in C++.\nParameters: The key parameters in all experiments (except the\ntrade-off evaluation) are set as follows, while their settings in the\ntrade-off are introduced separately in Section 3.4. (1) Distance\nthreshold ùúñand neighbor threshold ùúèare set dynamically in different\nexperiments, which will be explicitly stated. (2) For LAF-DBSCAN,\nthe error factor ùõºis set in an ad-hoc manner for different datasets,\nas reported in Table 1. For LAF-DBSCAN++, its ùõºis fixed to be 1.0.\n(3) For DBSCAN++, the sample fraction ùëùis automatically set based\non the ratio of predicted core points. Specifically, we first get the\nratio of points that are predicted as core by the cardinality estima-\ntor (denoted by ùëÖùëê), then ùëù=ùõø+ùëÖùëê, where ùõøis a user-determined\noffset ranging from 0.1 to 0.3. In our evaluation, the final ùëùnor-\nmally ranges within 0.2 ‚àº0.6. And ùëùof LAF-DBSCAN++ keeps\nidentical to DBSCAN++. (4) For KNN-BLOCK DBSCAN, we control\ntwo parameters of the k-means tree for KNN search: branching\nfactor (set as 10) and ratio of leaves to check (set as 0.6). (5) For\nBLOCK-DBSCAN, we control the basis of the cover tree (set as 2)\nand the maximum iterations when computing the minimum dis-\ntance between inner core blocks (i.e., RNT in [2], set as 10). (6) For\nùúå-approximate DBSCAN, we set ùúå=1.0.\n2code available at https://github.com/jenniferjang/dbscanpp\n3code available at https://github.com/XFastDataLab/KNN-BLOCK-DBSCAN\n4code available at https://github.com/XFastDataLab/BLOCK-DBSCAN\n5code and binary available at https://sites.google.com/view/approxdbscan, we use the\nversion 2.03.2 Representative ( ùúñ,ùúè) and proper ùõºselection\nWe select the proper ùúñandùúèaccording to the noise ratio , i.e., the\nportion of noise points in each dataset. A proper ( ùúñ,ùúè) should lead\nto (1) a low to middle noise ratio and (2) enough number of clusters,\nsince the clustering makes no sense when too many noises exist\nor most points are grouped into very few clusters. So we do a grid\nsearch to select the values of ( ùúñ,ùúè) which make (1) noise ratio smaller\nthan 0.6 and (2) the number of clusters large than 20 in most datasets.\nTable 2 shows part of the statistics, where each cell includes a pair\n(noise ratio, number of clusters) for the corresponding case. The cells\nsatisfying the conditions are highlighted, for example, (0.55, 5) and\n(0.6, 5) are both proper ( ùúñ,ùúè) since either of them makes at least 2\nout of 3 datasets satisfy the conditions; while (0.5, 5) and (0.7, 5)\nshould be avoided. Finally, we choose three ( ùúñ,ùúè) values to report\nthroughout this paper: (0.5, 3), (0.55, 5) and (0.6, 5).\n(ùùê, ùùâ ) MS-50k MS-100k MS-150k\n(0.5, 3) (0.63, 654) (0.53, 1071) (0.47, 1225)\n(0.5, 5) (0.83, 174) (0.72, 348) (0.64, 380)\n(0.55, 5) (0.65, 183) (0.48, 223) (0.39, 175)\n(0.6, 5) (0.38, 92) (0.21, 70) (0.15, 47)\n(0.7, 5) (0.005, 1) (0.0007, 1) (0.0004, 1)\nTable 2: Part of the statistics about noise ratio and number of\nclusters. They are collected by running DBSCAN with differ-\nentùúñand ùúèon each dataset. In the table each cell below the\ndataset name is a pair (noise ratio, total number of clusters) ,\nand the proper value pairs are highlighted by bold text.\nIn this section we also discuss the proper setting of error factor\nùõº. Basically, there is no quantifiable way to predict the best ùõº, as it\ndepends on the dataset. The method we use for this paper is grid\nsearch, and our observation can help guide the users: when the\n(ùùê, ùùâ ) Method NYT-150k Glove-150k MS-150k\nARI(0.5,3)KNN-BLOCK - 0.8597 0.6004\nBLOCK-DBSCAN - 0.8825 0.4953\nDBSCAN++ 0.7933 0.8129 0.4218\nLAF-DBSCAN 0.7731 0.8660 0.4134\nLAF-DBSCAN++ 0.7321 0.7746 0.4113\n(0.55,5)KNN-BLOCK - 0.6942 0.1862\nBLOCK-DBSCAN - 0.8508 0.2283\nDBSCAN++ 1.0 0.7869 0.1321\nLAF-DBSCAN 1.0 0.8520 0.2309\nLAF-DBSCAN++ 1.0 0.7444 0.1138\n(0.6,5)KNN-BLOCK - 0.2665 -0.0444\nBLOCK-DBSCAN - 0.6399 0.0046\nDBSCAN++ 1.0 0.7801 0.3687\nLAF-DBSCAN 1.0 0.8797 0.2643\nLAF-DBSCAN++ 1.0 0.7653 0.3519\nAMI(0.5,3)KNN-BLOCK - 0.4994 0.4254\nBLOCK-DBSCAN - 0.6613 0.3945\nDBSCAN++ 0.6872 0.6369 0.3965\nLAF-DBSCAN 0.7050 0.7558 0.4196\nLAF-DBSCAN++ 0.6245 0.5947 0.3879\n(0.55,5)KNN-BLOCK - 0.3391 0.1738\nBLOCK-DBSCAN - 0.6364 0.2626\nDBSCAN++ 1.0 0.6578 0.2288\nLAF-DBSCAN 1.0 0.7554 0.3017\nLAF-DBSCAN++ 1.0 0.6068 0.2210\n(0.6,5)KNN-BLOCK - 0.1427 0.0390\nBLOCK-DBSCAN - 0.4988 0.1259\nDBSCAN++ 1.0 0.7061 0.2836\nLAF-DBSCAN 1.0 0.8167 0.2763\nLAF-DBSCAN++ 1.0 0.6822 0.2750\nTable 3: Clustering quality (AMI and ARI scores) of the ap-\nproximate methods on the three largest datasets\n\nLearned Accelerator Framework for Angular-Distance-Based High-Dimensional DBSCAN Conference‚Äô17, July 2017, Washington, DC, USA\n020040060080010001200140016001800\nNYT-150k Glove-150k MS-150kTime(s)Œµ= 0.5, œÑ= 3\nDBSCAN\nKNN-BLOCK\nBLOCK-DBSCAN\nDBSCAN++\nLAF-DBSCAN\nLAF-DBSCAN++\n(a)ùúñ=0.5,ùúè=3.\n02004006008001000120014001600180020002200\nNYT-150k Glove-150k MS-150kTime(s)Œµ= 0.55, œÑ= 5\nDBSCAN\nKNN-BLOCK\nBLOCK-DBSCAN\nDBSCAN++\nLAF-DBSCAN\nLAF-DBSCAN++ (b)ùúñ=0.55,ùúè=5.\n02004006008001000120014001600180020002200240026002800\nNYT-150k Glove-150k MS-150kTime(s)Œµ= 0.6, œÑ= 5\nDBSCAN\nKNN-BLOCK\nBLOCK-DBSCAN\nDBSCAN++\nLAF-DBSCAN\nLAF-DBSCAN++ (c)ùúñ=0.6,ùúè=5.\nFigure 1: Clustering time of all the methods on the three largest datasets\n(ùùê, ùùâ ) MS-50k MS-100k MS-150k\n(0.5, 3) 864.7s/206.6s 3499.8s/882.7s 6931.1s/1669.5s\n(0.55, 5) 854.7s/180.3s 3367.0s/827.6s 6595.1s/1936.6s\n(0.6, 5) 753.3s/219.9s 2817.9s/1041.1s 5385.9s/2539.9s\nTable 4: Clustering time of ùúå-approximate DBSCAN vs. DB-\nSCAN on different dataset scales. Each cell presents a pair of\ntime, ‚Äú ùë°1/ùë°2‚Äù, where ùë°1is the time of ùúå-approximate DBSCAN\nand ùë°2is that of DBSCAN given the same ( ùúñ,ùúè) and dataset.\nvector type is fixed (e.g., dense neural embedding), ùõºshould be\nlarger for the larger dataset size or higher data dimension. This can\nbe observed in Table 1 on Glove and the three MS datasets. The\nreason is probably the bias in training set. For example, according\nto Table 2, with the increasing data scale, the noise ratio decreases,\nmeaning the fraction of core points increases. Such a bias in training\nmakes the cardinality estimator more likely to predict a larger value.\nTherefore the ùõºshould also increase accordingly.\n3.3 Efficiency and effectiveness evaluation\nWe first evaluate the efficiency and effectiveness of each method\non the three largest datasets, NYT-150k, Glove-150k and MS-150k.\nTable 3 reports the clustering quality via ARI and AMI scores (the\nhigher, the better) for all the approximate methods. As the ground\ntruth, DBSCAN is not included in the table. And Figure 1 illustrates\nthe clustering time of those methods. Due to unknown bugs in\nKNN-BLOCK DBSCAN and BLOCK-DBSCAN, they cannot run on\nNYT-150k, so their results on NYT-150k are missed in Table 3 and\nFigure 1. Note that we do not include ùúå-approximate DBSCAN in\nTable 3, Figure 1 or any following experiment, due to its significantly\nlow efficiency on high-dimensional data. Specifically, by [ 8], a larger\nùúåmakes ùúå-approximate DBSCAN more efficient, and ùúåranges from\n0.001 to 0.1 in [ 8]. However, though we have enlarged ùúåto 1.0 in\nour evaluation, the method still presents a low efficiency which is\neven slower than the naive DBSCAN, as shown in Table 4. This\nmeans it suffers much from curse of dimensionality and should\nnot be applied in high-dimensional space. And [ 2] provides further\nexplanation for this problem of ùúå-approximate DBSCAN.\nIt is observed that (1) LAF-DBSCAN and LAF-DBSCAN++ achieve\nthe highest efficiency in most cases. For example, LAF-DBSCAN\nmakes up to 2.9x acceleration to DBSCAN as well as reaches 1.6x\nspeed over DBSCAN++, 2.2x speed over KNN-BLOCK DBSCAN\nand 2.4x speed over BLOCK-DBSCAN. (2) LAF-DBSCAN achieves\nthe highest quality in most cases, and in the cases of NYT-150kwhere the three methods have same scores, LAF-DBSCAN only\ntakes 60%‚àº70% time of DBSCAN++, as shown in Figure 1. (3) LAF-\nDBSCAN++ usually has a slightly lower quality than DBSCAN++,\nbut gains much more on the efficiency (i.e., up to 6.7x acceleration\nto DBSCAN++), which makes it practical with better speed-quality\ntrade-off capability than DBSCAN++. (4) Due to curse of dimension-\nality, all methods perform worse on MS-150k than the other datasets.\nSpecifically, higher dimension usually means more complex distri-\nbution. For LAF, the distribution is harder to fit and more false neg-\native predictions (FN) are made, e.g., when ùúñ=0.5, ùúè=3, the num-\nber of FN in NYT/Glove/MS-150k are respectively 5687/2010/7425,\nwhich has a negative correlation with the results in Table 3. More\ncomplex distribution also makes sampling less representative and\nneighbor search less effective, which degrades clustering quality of\nthe baselines too.\n3.4 Speed-quality trade-off evaluation\nWe use MS-150k and Glove-150k with the setting ùúñ=0.5, ùúè=3to\npresent the speed-quality trade-off capabilities of all the approx-\nimate methods except ùúå-approximate DBSCAN as discussed in\nSection 3.3. We adjust the performance of DBSCAN++ and LAF-\nDBSCAN++ by varying the sample fraction ùëù, which is completed\nby varying the offset ùõø(mentioned in Section 3.1) within 0.1 ‚àº0.9,\nwhile for LAF-DBSCAN the error factor ùõºis varied from 1.1 to\n15.0 (which is fixed as 1.0 in LAF-DBSCAN++). For KNN-BLOCK\nDBSCAN, we vary the branching factor within 3 ‚àº20 and the leaves\nratio from 0.001 to 0.3. For BLOCK-DBSCAN, we vary the cover\ntree basis from 1.1 to 5 while fix the maximum iterations as 10.\nAs illustrated in Figure 2 and 3, LAF-DBSCAN and LAF-DBSCAN++\nhave the best speed-quality trade-off capabilities in the high-quality\nareas (e.g., where AMI > 0.4) on both Glove-150k and MS-150k.\nGiven that the real world normally demands a relatively high clus-\ntering quality, we conclude that LAF-DBSCAN and LAF-DBSCAN++\nachieve better trade-off capabilities than all the baselines in practice.\nFurthermore, on both datasets, LAF-DBSCAN++ presents a better\ntrade-off than DBSCAN++, meaning that LAF significantly reduces\nthe clustering time of DBSCAN++ with a relatively tiny quality\nloss, which further proves the strength and usefulness of LAF for a\nwide range of DBSCAN variants.\n\nConference‚Äô17, July 2017, Washington, DC, USA Yifan Wang and Daisy Zhe Wang\n0200400600800100012001400\n0 0.1 0.2 0.3 0.4 0.5 0.6Time(s)\nAMIKNN-BLOCK\nBLOCK-DBSCAN\nDBSCAN++\nLAF-DBSCAN\nLAF-DBSCAN++\nFigure 2: Speed-quality trade-off curves of the approximate\nmethods on dataset MS-150k\n0100200300400500600\n0.15 0.35 0.55 0.75 0.95Time(s)\nAMIKNN-BLOCK\nBLOCK-DBSCAN\nDBSCAN++\nLAF-DBSCAN\nLAF-DBSCAN++\nFigure 3: Speed-quality trade-off curves of the approximate\nmethods on dataset Glove-150k\nMethod MS-50k MS-100k MS-150k\nARIKNN-BLOCK 0.7577 0.3828 0.1862\nBLOCK-DBSCAN 0.7710 0.4632 0.2283\nDBSCAN++ 0.7238 0.4690 0.1321\nLAF-DBSCAN 0.7581 0.5524 0.2309\nLAF-DBSCAN++ 0.6455 0.3077 0.1138\nAMIKNN-BLOCK 0.5708 0.2736 0.1738\nBLOCK-DBSCAN 0.6134 0.3518 0.2626\nDBSCAN++ 0.6264 0.4494 0.2288\nLAF-DBSCAN 0.7043 0.5034 0.3017\nLAF-DBSCAN++ 0.5328 0.3197 0.2210\nTable 5: Clustering quality of all the approximate methods\non datasets of different scales ( ùúñ=0.55,ùúè=5)\n3.5 Scalability evaluation\nTo evaluate the scalability, we run all the methods on the three MS\ndatasets of different scales, and report the results for ùúñ=0.55and\nùúè=5, as the results of other ( ùúñ, ùúè) are similar. The quality scores\nare reported in Table 5 and the efficiency results are reported in\nFigure 4 where we annotate the points of MS-150k with the numbers\nof the clustering time for a clearer view. They prove that our LAF-\nenhanced methods are highly effective and scalable, based on these\nobservations: (1) similar to the case of efficiency and effectiveness\nevaluation, here LAF-DBSCAN still achieves the best quality in most\ncases, with the highest speed on the largest dataset (whose time\nis the shortest 547.2s). (2) LAF-DBSCAN has the slowest growth\nof clustering time when data scale increases, which presents its\nhigher scalability than the baselines. (3) In most cases the quality\nof LAF-DBSCAN++ is close to DBSCAN++, while in other cases\n1936.6\n1179.11309.5\n603.0\n547.2599.3\n0200400600800100012001400160018002000\nMS-50k MS-100k MS-150kTime (s)\nDatasetsŒµ= 0.55, œÑ= 5\nDBSCAN\nKNN-BLOCK\nBLOCK-DBSCAN\nDBSCAN++\nLAF-DBSCAN\nLAF-DBSCAN++Figure 4: Clustering time of all the methods on datasets of\ndifferent scales ( ùúñ=0.55,ùúè=5)\n(ùùê, ùùâ ) Dataset MC/TC MP/TPC ASMC\n(0.5, 3) NYT-150k 63/92 209/19358 3.32\n(0.55, 5) Glove-150k 39/81 250/7879 6.41\n(0.55, 5) MS-150k 159/176 1107/18384 6.96\nTable 6: Statistics for fully missed clusters by LAF-DBSCAN.\nMC, the number of fully M issed C lusters; TC, the T otal num-\nber of groundtruth C lusters; MP, the number of M issed data\nPoints; TPC, the T otal number of data P oints belonging to\nthe groundtruth C lusters, i.e., the non-noise points; ASMC ,\nAverage S ize of the fully M issed C lusters.\nthey get closer quickly with the increasing data scale, showing the\nhigher scalability of LAF-DBSCAN++ than DBSCAN++.\n3.6 Missed cluster analysis\nIn addition to the wrongly split cluster error discussed in Section 2, a\ncluster may be fully missed if all its core points are falsely predicted\nto be non-core or noise. Fortunately, this fully missed cluster error\nonly has a negligible impact on the quality, as it usually occurs\nin very tiny clusters. We choose the cases where LAF-DBSCAN\nachieves the lowest quality on each dataset according to Table 3\n(i.e., ( ùúñ,ùúè) = (0.5,3) on NYT-150k, (0.55,5) on Glove-150k and MS-\n150k) and report the fully missed cluster information in Table 6.\nThough in the worst cases, LAF-DBSCAN fully misses more than\n50% clusters, it still guarantee the major clusters to be found, since\nthe missed clusters in total include only 1% ‚àº6% of the non-noise\npoints. And the average size of the missed clusters (ASMC) is too\ntiny (i.e., only including 3 ‚àº7 points on average) to have a non-\ntrivial impact on the overall clustering quality. Therefore, we do\nnot further discuss such an error in this paper.\n4 CONCLUSION AND FUTURE WORK\nTo improve efficiency and scalability of high-dimensional DBSCAN-\nlike clustering for angular distance, we propose LAF, a generic\nlearned accelerator framework using learned cardinality estimation\ntechniques to reduce unnecessary range queries in the clustering,\nand compensating for the quality loss by detecting the false negative\nand merging the wrongly separated clusters. Our evaluation shows\nthat the LAF-enhanced methods do have a significantly higher\nefficiency than the state-of-the-art efficient DBSCAN approaches\nwith also high quality, as well as a better speed-quality trade-off\n\nLearned Accelerator Framework for Angular-Distance-Based High-Dimensional DBSCAN Conference‚Äô17, July 2017, Washington, DC, USA\ncapability than the baselines. The main limitation of this work is\nthe limited range of applicable distance metrics. But since there is\nno hard constraint on the distance metric, our methods are easy to\nadapt to other distances, which will be explored in the future work.\nThe future work also includes studying the impact of the cardinality\nestimator being used, extensively investigating the proper ùõº, etc.\nREFERENCES\n[1]Yewang Chen, Shengyu Tang, Nizar Bouguila, Cheng Wang, Jixiang Du, and\nHaiLin Li. 2018. A fast clustering algorithm based on pruning unnecessary\ndistance computations in DBSCAN for high-dimensional data. Pattern Recognition\n83 (2018), 375‚Äì387.\n[2]Yewang Chen, Lida Zhou, Nizar Bouguila, Cheng Wang, Yi Chen, and Jixiang Du.\n2021. BLOCK-DBSCAN: Fast clustering for large scale data. Pattern Recognition\n109 (2021), 107624. https://doi.org/10.1016/j.patcog.2020.107624\n[3]Yewang Chen, Lida Zhou, Songwen Pei, Zhiwen Yu, Yi Chen, Xin Liu, Jixiang Du,\nand Naixue Xiong. 2019. KNN-BLOCK DBSCAN: Fast clustering for large-scale\ndata. IEEE transactions on systems, man, and cybernetics: systems 51, 6 (2019),\n3939‚Äì3953.\n[4]Difei Cheng, Ruihang Xu, and Bo Zhang. 2021. Fast Density Estimation for\nDensity-based Clustering Methods. arXiv preprint arXiv:2109.11383 (2021).\n[5]Igor de Moura Ventorim, Diego Luchi, Alexandre L. Rodrigues, and Fl√°vio Miguel\nVarej√£o. 2021. BIRCHSCAN: A sampling method for applying DBSCAN to large\ndatasets. Expert Syst. Appl. 184 (2021), 115518.\n[6]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nhttps://doi.org/10.48550/ARXIV.1810.04805\n[7]Martin Ester, Hans-Peter Kriegel, J√∂rg Sander, Xiaowei Xu, et al .1996. A density-\nbased algorithm for discovering clusters in large spatial databases with noise.. In\nkdd, Vol. 96. 226‚Äì231.\n[8]Junhao Gan and Yufei Tao. 2015. DBSCAN Revisited: Mis-Claim, Un-Fixability,\nand Approximation. In Proceedings of the 2015 ACM SIGMOD International\nConference on Management of Data (Melbourne, Victoria, Australia) (SIGMOD\n‚Äô15). Association for Computing Machinery, New York, NY, USA, 519‚Äì530.\nhttps://doi.org/10.1145/2723372.2737792\n[9]Junhao Gan and Yufei Tao. 2017. On the Hardness and Approximation of Eu-\nclidean DBSCAN. ACM Trans. Database Syst. 42, 3, Article 14 (jul 2017), 45 pages.\nhttps://doi.org/10.1145/3083897\n[10] Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal of\nclassification 2, 1 (1985), 193‚Äì218.\n[11] Jennifer Jang and Heinrich Jiang. 2018. DBSCAN++: Towards fast and scalable\ndensity clustering. https://doi.org/10.48550/ARXIV.1810.13105\n[12] Heinrich Jiang, Jennifer Jang, and Jakub Lacki. 2020. Faster DBSCAN via sub-\nsampled similarity queries. Advances in Neural Information Processing Systems\n33 (2020), 22407‚Äì22419.\n[13] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 international\nconference on management of data . 489‚Äì504.\n[14] Diego Luchi, Alexandre Loureiros Rodrigues, and Fl√°vio Miguel Varej√£o. 2019.\nSampling approaches for applying DBSCAN to large datasets. Pattern Recognition\nLetters 117 (2019), 90‚Äì96.\n[15] Yinghua Lv, Tinghuai Ma, Meili Tang, Jie Cao, Yuan Tian, Abdullah Al-Dhelaan,\nand Mznah Al-Rodhaan. 2016. An efficient and scalable density-based clustering\nalgorithm for datasets with complex structures. Neurocomputing 171 (2016),\n9‚Äì22.\n[16] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, and Li Deng. 2016. MS MARCO: A human generated machine reading\ncomprehension dataset. In CoCo@ NIPS .\n[17] Jianbin Qin, Wei Wang, Chuan Xiao, Ying Zhang, and Yaoshu Wang. 2021. High-\nDimensional Similarity Query Processing for Data Science. In Proceedings of the\n27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Virtual\nEvent, Singapore) (KDD ‚Äô21) . Association for Computing Machinery, New York,\nNY, USA, 4062‚Äì4063. https://doi.org/10.1145/3447548.3470811\n[18] Ji Sun, Guoliang Li, and Nan Tang. 2021. Learned cardinality estimation for simi-\nlarity queries. In Proceedings of the 2021 International Conference on Management\nof Data . 1745‚Äì1757.\n[19] Yao Tian, Tingyun Yan, Xi Zhao, Kai Huang, and Xiaofang Zhou. 2022. A Learned\nIndex for Exact Similarity Search in Metric Spaces. https://doi.org/10.48550/\nARXIV.2204.10028\n[20] Nguyen Xuan Vinh, Julien Epps, and James Bailey. 2010. Information theoretic\nmeasures for clusterings comparison: Variants, properties, normalization and\ncorrection for chance. The Journal of Machine Learning Research 11 (2010), 2837‚Äì\n2854.\n[21] P Viswanath and V Suresh Babu. 2009. Rough-DBSCAN: A fast hybrid density\nbased clustering method for large data sets. Pattern Recognition Letters 30, 16(2009), 1477‚Äì1488.\n[22] Xiao Wang, Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. 2021. Pseudo-\nrelevance feedback for multiple representation dense retrieval. In Proceedings of\nthe 2021 ACM SIGIR International Conference on Theory of Information Retrieval .\n297‚Äì306.\n[23] Yifan Wang, Haodi Ma, and Daisy Zhe Wang. 2022. LIDER: An Efficient High-\ndimensional Learned Index for Large-scale Dense Passage Retrieval. https:\n//doi.org/10.48550/ARXIV.2205.00970\n[24] Yaoshu Wang, Chuan Xiao, Jianbin Qin, Xin Cao, Yifang Sun, Wei Wang, and\nMakoto Onizuka. 2020. Monotonic cardinality estimation of similarity selection:\nA deep learning approach. In Proceedings of the 2020 ACM SIGMOD International\nConference on Management of Data . 1197‚Äì1212.\n[25] Yaoshu Wang, Chuan Xiao, Jianbin Qin, Rui Mao, Makoto Onizuka, Wei Wang,\nRui Zhang, and Yoshiharu Ishikawa. 2021. Consistent and flexible selectivity\nestimation for high-dimensional data. In Proceedings of the 2021 International\nConference on Management of Data . 2319‚Äì2327.\n[26] Shaoyuan Weng, Jin Gou, and Zongwen Fan. 2021. ‚Ñé-DBSCAN: A simple fast\nDBSCAN algorithm for big data. In Asian Conference on Machine Learning . PMLR,\n81‚Äì96.\n[27] Seungil You, David Ding, Kevin Canini, Jan Pfeifer, and Maya Gupta. 2017. Deep\nlattice networks and partial monotonic functions. Advances in neural information\nprocessing systems 30 (2017).\n[28] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma.\n2022. Learning Discrete Representations via Constrained Clustering for Effective\nand Efficient Dense Retrieval. In Proceedings of the Fifteenth ACM International\nConference on Web Search and Data Mining (Virtual Event, AZ, USA) (WSDM\n‚Äô22). Association for Computing Machinery, New York, NY, USA, 1328‚Äì1336.\nhttps://doi.org/10.1145/3488560.3498443",
  "textLength": 41802
}