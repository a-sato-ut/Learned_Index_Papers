{
  "paperId": "a00fb89878de2677acf951b8a43c14c69030ba5e",
  "title": "OSM-tree: A Sortedness-Aware Index",
  "pdfPath": "a00fb89878de2677acf951b8a43c14c69030ba5e.pdf",
  "text": "OSM-tree: A Sortedness-Aware Index\nAneesh Raman1, Subhadeep Sarkar1, Matthaios Olma2, Manos Athanassoulis1\n1Boston University,2Microsoft Research\nABSTRACT\nIndexes facilitate efficient querying when the selection predicate is\non an indexed key. As a result, when loading data, if we anticipate\nfuture selective (point or range) queries, we typically maintain an\nindex that is gradually populated as new data is ingested. In that\nrespect, indexing can be perceived as the process of adding structure\nto an incoming, otherwise unsorted, data collection. The process of\nadding structure comes at a cost, as instead of simply appending\nincoming data, every new entry is inserted into the index. If the data\ningestion order matches the indexed attribute order, the ingestion\ncost is entirely redundant and can be avoided (e.g., via bulk loading\nin a B+-tree). However, state-of-the-art index designs do not benefit\nwhen data is ingested in an order that is close to being sorted but\nnotfully sorted.\nIn this paper, we study how indexes can benefit from partial\ndata sortedness ornear-sortedness , and we propose an ensemble\nof techniques that combine bulk loading ,index appends ,variable\nnode fill/split factor , and buffering , to optimize the ingestion cost\nof a tree index in presence of partial data sortedness. We further\naugment the proposed design with necessary metadata structures\nto ensure competitive read performance. We apply the proposed\ndesign paradigm on a state-of-the-art B+-tree, and we propose\nthe Ordered Sort-Merge tree (OSM-tree). OSM-tree outperforms\nthe state of the art by up to 8.8Ã—in ingestion performance in the\npresence of sortedness, while falling back to a B+-treeâ€™s ingestion\nperformance when data is scrambled. OSM-tree offers competitive\nquery performance, leading to performance benefits between 28%\nand 5Ã—for mixed read/write workloads.\nPVLDB Artifact Availability:\nThe artifacts have been made available at https://github.com/BU-DiSC/\nosmtree and https://github.com/BU-DiSC/sortedness-workload.\n1 INTRODUCTION\nDatabase indexing sits at the heart of almost any data system vary-\ning from full-blown relational systems [ 35] to NoSQL key-value\nstores [ 23]. Indexes help accelerate query processing both for analyt-\nical and transactional workloads by allowing efficient data accesses\nof selective (range or point) queries. Essentially, in presence of read\nqueries, database administrators decide to build and maintain in-\ndexes to improve query performance at the expense of space and\nwrite amplification [ 5], and the time needed to update the indexes.\nIndexing Adds Structure to Facilitate Queries. We pay the cost\nof index construction and maintenance because it adds structure to\nthe data, which, in turn, allows for efficient queries. As shown in\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XX\nData  sortedness\nin-order insertion\nO(log(n) )append\nO(1)scan\nO(n)\nlogarithmic \nsearch\nO(log(n)) \nor fasterInsert \noptimized\nRead\n optimized\nOSM design spaceunsorted\nsortedRead Cost\nWrite CostFigure 1: State-of-the-art indexing and data organization\ntechniques pay a higher write cost in order to store data\nas sorted (or, in general, more organized) and offer efficient\nreads. Since the goal of indexing is to store the data as sorted,\nwe ideally expect that ingesting near-sorted data would be\nmore efficient, which is not the case. We introduce the OSM\nmeta-design that offers better performance as data exhibit\nhigher degree of sortedness.\nFigure 1, with the thick black line, every index or, in general, any\ndata organization technique, exhibits a fundamental tradeoff be-\ntween its read andwrite cost. To achieve efficient logarithmic search\ntime for point queries, a classical index would insert data in their\ncorrect position ( in-order insertion ) leading to efficient logarithmic\nsearch (bottom right part of the figure). On the other extreme, if\nread queries are infrequent, then, scanning is acceptable and in-\nstead of adding new entries to an index, we can simply append them\n(appends leading to scans in the top left part of the figure). This read\nvs. write tradeoff holds for any data organization effort, including,\nclassical indexing like B+-tree [ 20], write-optimized log-structured\nmerge (LSM) trees [ 29], or simple online sorting via in-order inser-\ntion. Since all such data organization efforts are essentially adding\nstructure to an otherwise unstructured data collection , one would\nexpect they benefit when such structure exists already. Indeed, if\nthe data entries are already fully sorted, then we can benefit, for\nexample, by bulk loading a B+-tree. However, a question that re-\nmains open is what happens when the data entries arrive with some\ndegree of structure but are not entirely sorted (e.g., due to implicit\nclustering [ 4,31]). Before addressing this question, we first explain\nthe concept of data sortedness and showcase real-life examples of\nworkloads with variable degree of data sortedness.\nData Sortedness. Data Sortedness can be defined as the arrival or-\nderof the indexed key upon ingestion. This arrival order can be fully\nsorted, nearly sorted, less sorted, or scrambled. Data entries may\nbenear-sorted in several real-world cases. Consider the TPC-H [ 40]\nlineitem table that has three date-related attributes. Figure 2(a),\nwhich depicts the first 10,000values of shipdate ,commitdate ,\nandreceiptdate of the lineitem table, shows that when the data\narrives in order based on shipdate , the other two attributes are\nvery close to also being sorted. There are several scenarios that lead\nto near-sorted data collections. For example, (i) a relation that wasarXiv:2202.04185v1  [cs.DB]  8 Feb 2022\n\nData Sort edness Sorted Scrambl edIngestion LatencyB+-tree\nIdeal tree data structure(b)Figure 2: (a) TPC-H implicit clustering between shipdate ,\ncommitdate , and receiptdate leads to near-sorted columns\nwhen the data is sorted based on one of them. (b) Ideally, in-\ndex insertion performance should improve when inserting\nalready sorted or near-sorted data.\nsorted but a few new arbitrary updates took place, (ii) data that has\nbeen created based on a previous operation or query, e.g., a join,\n(iii) data that is sorted based on another attribute that is naturally\ncorrelated (like the TPC-H example above), or (iv) the timestamp\nattribute of an incoming data stream that has a few data packets\narriving out of order due to network congestion [6].\nIn order to be able to exploit this near-sortedness, we need a way\nto quantify it. In fact, there have been multiple sortedness metrics\nproposed [ 6,13,30]. Several of the sortedness metrics focus on\nquantifying the number of inversions or transpositions needed to\nachieve full sortedness [ 2,15,18,21,30]. However, a more natural\nway of thinking about the degree of sortedness in the context of\nindexing is how many elements are in the â€œwrongâ€ position and,\nmore crucially, by how much . As a result, we use the (ğ¾,ğ¿)sort-\nedness metric [ 6] that quantifies sortedness using two parameters:\nğ¾, that captures the number of elements that are out of order and\nğ¿, that captures the maximum displacement in terms of position\nof the out of order elements. Going back to the TPC-H example in\nFigure 2(a), when the data is sorted on shipdate ,commitdate has\nğ¾=99.2%andğ¿=1.6%, while receiptdate has higher sortedness\nwithğ¾=96.7%andğ¿=0.1%. For the latter, this means that 96.7%\nof the entries are not at the position they would be if they were\nsorted and the maximum displacement is 0.1% of the data size. The\n(ğ¾,ğ¿)metric helps quantify the number of entries we need to work\non to absorb sortedness when inserting new data.\nProblem: Indexes Do Not Exploit Data Near-Sortedness.\nWhile indexes can already benefit from inserting a fully sorted\ndata collection via bulk loading [ 1,16] (assuming that it is already\nknown that data is sorted), they are not designed to exploit near-\nsortedness. Further, widespread buffering techniques to optimize\nindex insertion (like in Bğœ–-trees [ 10] and LSM-trees [ 32]) are not\ndesigned to exploit sortedness and will end up sorting the data even\nwhen they are (almost) sorted. We argue that when inserting data\nto an index, the higher the data sortedness, the lower the insertion cost\nshould be , as depicted in Figure 2(b), for the ideal tree data structure .\nNote that state-of-the-art indexes, like B+-trees, do not exhibit any\nperformance improvement when inserting near-sorted data. In fact,\nif data is inserted in order and bulk loading is not employed, a B+-\ntree would have the worst-possible space amplification, since every\nnode will be exactly 50% full. In contrast, a sortedness-aware index\nshould achieve a better read vs. write tradeoff as data sortedness\nincreases , as depicted by the dashed lines in Figure 1. Following thegreen line, which constitutes a third axis â€“ the one for sortedness â€“\nwe envision a new class of data structures that is able to navigate\nthe entire shaded region in Figure 1 and perform â€œlessâ€ indexing for\nnear-sorted data, leading up to the ideal performance of append-like\ncost of insertion with efficient searching if data is sorted.\nOur Approach: OSM-tree. To realize this vision, we propose a\nnew paradigm for designing indexes that is capable of exploit-\ning any existing degree of sortednes to improve index insertion\nperformance without hurting read latency. We achieve this using\nan ensemble of techniques which, when combined appropriately,\nsolve a problem that cannot be solved by any one of them alone.\nSpecifically, we employ buffering ,partial bulk loading ,query-driven\npartial sorting , and merging to create an index ingestion mecha-\nnism that substantially accelerates the ingestion performance in\nthe presence of data sortedness. This sortedness-aware approach,\nhowever, comes at the cost of increased read latency since every\nquery may have to search a buffer. To alleviate this cost, we aug-\nment the design with a collection of Zonemaps [ 31] and Bloom\nfilters (BFs) [ 9], which help make the read cost comparable to the\nbaseline. By combining the ingestion and the read optimizations on\ntop of a state-of-the-art B+-tree, we propose our new design termed\nOrdered Sort-Merge tree ( OSM-tree ). In a nutshell, OSM-tree buffers\nincoming data to bulk load as much as possible and reverts back to\ninsertion from the root ( top-inserts ) otherwise. By adaptively sorting\nbuffered data during queries, OSM-tree avoids the burden of sorting\nlarge data collections. With respect to reads, it uses interpolation\nsearch for the sorted parts of the buffer, and pays the cost of scan-\nning a small amount of data when the Zonemaps and Bloom filters\ndirect a query to unsorted entries in the buffer. Note that the OSM\nparadigm can make any tree-based data structure (e.g., radix\ntrees, Bğœ–-trees, and LSM-trees) amenable to data sortedness . It\nis not a new index per se , rather, a new framework for creating\nsortedness-aware counterparts for any tree-based index.\nContributions. Our work offers the following contributions.\nâ€¢We identify sortedness as a resource that can be harnessed to\ningest data faster in tree indexes.\nâ€¢We propose a new index meta-design that employs buffering,\npartial bulk loading, and merging to enhance ingestion in the\npresence of any degree of data sortedness.\nâ€¢We augment this design to propose OSM-tree that encompasses\nquery-driven sorting, merging, Zonemaps, and Bloom filters to\nmaintain competitive performance for point and range queries.\nâ€¢We apply this design on a state-of-the-art B+-tree, and we show\nthat we can achieve up to 8.8Ã—faster data ingestion with com-\npetitive read query performance leading to performance benefits\nof up to 5Ã—in mixed read/write workloads.\nâ€¢The OSM meta-design provides the foundation needed to cap-\nture a varying degree of data pre-sortedness for tree indexes.\n2 PROBLEM STATEMENT\nBoth indexing and sorting pay the cost to add structure to the data\nto facilitate faster queries. Both assume that the data is not sorted\nand that the desired state if fully sorted. Do we need to pay the same\ncost (of sorting or indexing) for near-sorted data?\nChallenge: Handling Variable Degrees of Sortedness.\n2\n\nThe ingestion complexity for an entry in a B+-tree isğ‘‚(ğ‘™ğ‘œğ‘”ğ¹(ğ‘)),\nirrespective of the order of ingested data. Although this is beneficial\nin the worst-case when incoming data is completely scrambled,\nsuch performance is suboptimal even when some amount of data\nsortedness exists. Not only do B+-trees do more work, but also carry\ntheir worst space amplification for fully sorted inserts. Insertions,\nin this case, are right-deep, while both the internal and leaf nodes\nare split in half, leaving 50% of all index nodes unused. The B+-tree\nand other popular modern indexes are not designed to identify data\nsortedness to do less work and improve insert efficiency.\nGoal: We set out to design an index that offers better ingestion\nperformance for higher data sortedness, without hurting the\nperformance of read queries.\n3 DESIGN ELEMENTS\nWe now present the four fundamental design elements which, when\nappropriately combined, allow us to exploit data sortedness. The\nfirst three: (i) right-most leaf insertions, (ii) bulk loading, and (iii)\nfill/split factor adjustment, benefits as-is a fully sorted data inges-\ntion, and when combined with (iv) buffering, can lead to a design\nthat can exploit variable data sortedness. We discuss the key ideas\nbehind the four design elements and, later in Section 4, we discuss\nhow to put them together. We illustrate these ideas in Figure 3.\nRight-Most Leaf Insertion. When inserting data that follow the\norder of the index-attribute, we can avoid the logarithmic tree\ntraversal cost by always maintaining a pointer to the right-most\nleaf node, as shown in Figure 3(a). That way, for every new insert,\nwe first check that indeed it should be directed to the right-most\nleaf (that is, that the inserted key is larger than the minimum value\nof that leaf), and we can simply insert it in the leaf. Note that this\napproach can also absorb a very small degree of sortedness if the\nsize of a leaf node is large enough. In terms of insertion performance,\nin-order insertion allows us to have a ğ‘‚(1)insertion cost instead\nofğ‘‚(ğ‘™ğ‘œğ‘”ğ¹(ğ‘)). In-order insertion can fall-back to classical insert\nfrom the root, which we term top-inserts , when the leaf node is not\nenough to capture the sortedness.\nBulk Loading. If the data is fully sorted , we can perform better\nthan in-order insertion, by bulk loading the data [ 16], as shown\nin Figure 3(b). That way, we can avoid accessing a node for every\nentry. Instead, we append to an in-memory buffer as data arrive, and\nonce a page is full, we create a new right-most leaf. This amortizes\nthe insertion cost across ğ¹entries. While bulk loading gives great\nindex creation time if data is fully sorted, it cannot exploit a varying\ndegree of sortedness.\nFill Factor/Split Factor Adjustment. When employing any of\nthe above two techniques, we can further optimize the shape of\nthe tree by carefully deciding how we split internal nodes and\nleaf nodes. Specifically, if the data is fully sorted, the classical split\nalgorithm will create half-full nodes which will never receive any\nfuture inserts. Hence, the nodes always remain half-full leading the\nworst-case with respect to the index space amplification and, in\nsome cases, affecting the index height as well, since the effective\nfanout will also be half of the nominal one. Instead, if we anticipate\ndata to arrive fully sorted (or as near-sorted as we discuss in the\nnext section), we can decide to employ a different split factor where,\nm\nk1 k2knew<m knew>=mtail leaf pointer\nd11,â€¦,dn1d12,â€¦,dn2 leaf nodesk1\nd11,â€¦,dn1new leaf d12,â€¦,dn2bulk load\nsplit\nfactor = 80:20future\ninserts\nsplit\nfactor = 50:50(a)\n(c)(b)\nroot pointer\nd1: (m,v)new batch \nof entries(knew,vnew), ... \nk new(knew,vnew) Figure 3: Design elements aimed at exploiting data sorted-\nness. (a) An in-order entry can be directly inserted at the tail\nleaf ifğ‘˜ğ‘›ğ‘’ğ‘¤â‰¥ğ‘š. (b) If we have a batch of new in-order inserts,\nthey can be bulk loaded to the tree. (c) The split factor can\nbe adjusted such that a newly created node from the split\nreserves more space for following inserts.\nfor example, 80% of the entries would stay on the original node and\nthe newly created one will only hold 20% of the data in anticipation\nof the new â€“ higher in terms of value â€“ keys, as shown in Figure 3(c).\nBy changing the split factor, we also allow the nodes to have a higher\nfill factor on average. The split factor change reduces the number\nof overall splits needed, improving the insertion performance. The\nresulting higher fill factor throughout the tree reduces the overall\nnumber of nodes needed to hold the data which, in turn, reduces\nthe memory footprint of the data structure.\nBuffering. The above techniques do not offer substantial benefits\nif the data is not fully sorted. To utilize these techniques for data\nwith a varying degree of sortedness, we need to buffer incoming\ndata to propagate to the tree only those inserts that are in-order.\nThe buffer is periodically sorted as new entries arrive, to make the\nfirst part eligible for bulk loading. In the next section, we discuss\nhow we do this without employing expensive in-memory sorting,\nand ultimately, without hurting reads that may have to access the\nin-memory buffer to find the requested values.\n4 OSM-TREE DESIGN\nIn this section, we present a new meta-index design that can exploit\ndata sortedness to accelerate ingestion. Section 4.1 presents the\npreliminary design that puts together the three fundamental design\nelements from Section 3. In Section 4.2, we augment this design to\nimprove its lookup performance, and finally, in Section 4.3, we put\neverything together to present our novel OSM-tree design.\n4.1 Sortedness-Aware Ingestion\nWhile right-most leaf insertion and bulk loading can help when the\ndata is fully sorted, any degree of sortedness essentially would need\na staging area. Hence, we employ a dedicated in-memory insertion\nbuffer which intercepts all index inserts to facilitate future bulk\ningestion. Specifically, the buffer allows bulk loading of multiple\n3\n\nPages Entries to ï¬‚ush\nOSM-buï¬€er\n}\nB+-tree\nâ€¦ â€¦\nLeaf Node Leaf Node Leaf Node Leaf NodeFigure 4: The basic design of OSM-tree with a buffer on top\nof a state-of-the-art B+-tree.\ndata pages into the tree at a constant cost. We now describe the\nbuffering mechanism in detail.\nBasic Structure. We assume a state-of-the-art tree index like the\nB+-tree in Figure 4. Note that any variation of B+-tree that supports\nbulk loading can be part of this meta-design. On top of the basic\nindex, our design includes a buffer that receives incoming data.\nThe OSM-buffer. The in-memory buffer, termed OSM-buffer , main-\ntains all recently inserted data and checks whether the entries are\ninserted in order. In general, the data in the buffer is eventually\ninserted into the index either through bulk loading, when possible\n(that is, when the buffered data have higher values than the data al-\nready in the index) or through traditional inserts from the root node\n(termed top-inserts ). By having this design, we can already guar-\nantee that if data is inserted in order, they will be efficiently bulk\nloaded. We now discuss the buffer flushing strategies that optimize\ndata insertion in the presence of varying degree of sortedness.\nFlush Strategy. When the buffer becomes full, we flush the buffer\nto the tree. This flushing can happen either in the form of bulk\nloading or in the form of top-inserts . Our goal is to maximize the\namount of data inserted into the index via bulk loading. In the\nbest case â€“ i.e., when the buffer is fully sorted by virtue of the\npre-existing data sortedness â€“ we bulk load the contents of the\nbuffer with no sorting effort. In general, the buffer may not be fully\nsorted, and we would need to sort it before flushing. At this point,\nwe either (i) bulk load as many pages as possible, if the tree has\nstrictly smaller values than what the (now sorted) buffer has, and (ii)\nperform top-inserts if there is overlap. Note, that when we perform\ntop-inserts, after inserting a page worth of data, we re-check for\noverlap, and if possible, revert to bulk loading.\nAnother decision we make at every flush cycle (i.e., every time\nthe buffer is full) is what portion of the buffer to flush. The insight\nhere is that if we flush the entire buffer, we may insert to the\nindex, entries that overlap with future inserts if the data is anything\nbut fully sorted. Hence, instead of flushing the entire buffer at\nevery cycle, we flush a portion of the buffer (by default, half of it).\nPartially retaining entries in the buffer after a flush operation helps\ncapture overlaps with future insertions to some extent, which, in\nturn, increases the number of bulk loaded pages (and decreases\ntop-inserts) across flush cycles. Note that every top-insert costs\nğ‘‚(ğ‘™ğ‘œğ‘”ğ¹(ğ‘))while bulk loading costs ğ‘‚(1/ğ¹)sinceğ¹inserts are\nserviced by a single node addition.\nZonemaps to Identify Overlaps. Our design targets ingestion of\nnear-sorted data. In this case, out-of-order entries are likely to be\nlast_sorted_zone (a)\nlast_sorted_zone\nX(b)\nlast_sorted_zone (c)\nX\nlast_sorted_zone (d)Figure 5: The lifecycle of an insert in OSM-buffer. (a)\nlast_sorted_zone moves as new ordered entries are inserted.\n(b) An out-of-order entry moves last_sorted_zone to the\nleft. (c) Newer inserts are added and last_sorted_zone only\nmoves if required. (d) After a flush, the remaining entries in\nthe buffer are sorted and last_sorted_zone is reset to mark\nthe last sorted page in the buffer.\ndisplaced by a few pages from their ideal position. After a flush cy-\ncle, the buffer is half full, and its contents are fully sorted. As a result,\nno entries are out of order in the buffer, and we mark the last page\ncontaining sorted data as the last_sorted_zone (Figure 5(a)). Note\nthat every buffer page is treated as a separate zone. As new entries\narrive, because of the potential displacement, a newly appended\nentry may be either (i) overlapping with data in earlier pages, hence\nmoving the last_sorted_zone to the left (Figure 5(b)), (ii) overlap-\nping without having to move the last_sorted_zone (Figure 5(c)),\nor (iii) strictly greater, thus moving the last_sorted_zone to the\nright (Figure 5(d)). To update the last_sorted_zone , we also main-\ntain Zonemaps per page that allow for a quick overlap test after\nevery insertion. Maintaining the last_sorted_zone accurately,\nhelps to avoid unnecessary sorting at every flush cycle.\nWhen the buffer becomes full, we use the last_sorted_zone\nto decide how much to flush. If the most recent entries have moved\nthelast_sorted_zone to correspond to less than half of the buffer,\nwe flush only the pages up to the last_sorted_zone and attempt\nto bulk load, if possible. That way, we avoid the sorting cost before\nflushing. At the same time, the rest of the pages are sorted and\nmoved towards the left to make space for new inserts.\n4.2 Optimizing Read Queries\nWhile flushing the OSM-buffer helps to harness the sortedness by\nincreasing the fraction of inserts that are bulk loaded, it has an\nadverse impact on read performance. Specifically, every query goes\nthrough the following steps: (i) search the buffer that may contain\na sorted part and an unsorted part, and (ii) perform a tree search.\nIn the worst-case, a read query will need a full scan of the buffer.\nWe now discuss how to reduce the cost of a lookup aiming to make\nit as close as possible to that of the underlying tree.\nScanning the Unsorted Section First. In steady-state, the OSM-\nbuffer can be in one of two states: (i) fully sorted or (ii) it can have\na sorted portion and an unsorted portion. Note that even if the\nlast_sorted_zone is moved to the beginning of the buffer, we still\nhave half of the buffer sorted. So, for any search query, we only\nneed to scan the unsorted portion of the buffer that contains the\nmost recent data. If the lookup key is not found in this part of the\nbuffer, we continue to efficiently search the sorted section of the\nbuffer, and if the lookup has still not terminated, we search the tree.\nNote that if the key is found in the buffer, we can terminate and\navoid searching the tree, as this will be the most recent version of\nthe key. The lifecycle of a query is shown in Figure 6.\n4\n\nUnsort ed Section Sorted Section\nOSM Buf. Ran geTree Ran ge\nSorted Ran ge Unsort ed Ran ge\nState-of-the-art ind ex\n(eg., B+-tree)InterpolationGlobal BF p robe + \nZonemap scan + \nPer-p age BF p robe + OSM-buï¬€er Zonemap\nSortedness Zonemap \nfor every page\nOSM-buï¬€erIf not foundIf not foundFigure 6: The lifecycle of a point query.\nA BF to Skip the Unsorted Section. The unsorted section is up to half\nof the OSM-buffer and, thus, holds a small fraction of the overall data\n(residing in the buffer and the tree). As a result, most queries will not\nfind the desired key in it. Hence, to avoid the cost of unnecessary\nscanning the unsorted section, we employ a BF that is continuously\nupdated as new entries are inserted. This drastically reduces the\ncost of queries that do not terminate in the unsorted section.\nUsing Zonemaps to Skip Pages in the Unsorted Section. When the BF\nreturns a positive result, all pages of the unsorted section are marked\nfor scanning. However, we can skip many unnecessary page ac-\ncesses using the Zonemaps that are already part of the OSM-buffer\n(used to identify the last_sorted_zone ).\nUsing Per-Page BF. While the BF mentioned above and the\nZonemaps help avoid many unnecessary accesses, they are not\nenough if the data is scrambled. Hence, we also maintain a BF per\npage, which is updated as data is appended to the buffer. Overall,\na query starts searching in the unsorted section by first visiting\ntheglobal BF (with respect to the unsorted section). As shown in\nFigure 7 for a search query on key 1400, if the BF returns a positive\nresult, we access all Zonemaps to find which pages contain the key\nin question. Subsequently, for the Zonemaps that contain the key,\nwe probe the per-page BF, and we visit only the qualifying pages.\nInterpolation Search to Search Sorted Section. After searching\nthe unsorted part of the buffer is complete, if the query has not\nyet terminated, it will search the sorted section. Note that after\nevery flush the retained data is sorted. Irrespectively to whether\nthe newly inserted data overlap with the sorted section (and, thus,\nmove the last_sorted_zone ), the data retained after the previous\nflush remain sorted, and we maintain the position in the buffer until\nwhich the data is in sorted order, as previous_boundary . While\nthelast_sorted_zone may move to the left as new entries are\ninserted into the buffer, the previous_boundary may only move\nrightwards as long as entries are inserted in fully sorted order, and\nuntil the first out of order entry is inserted. Since the sorted section\nof the buffer is a contiguous sorted array, we employ interpolation\nsearch [ 34,41] which finishes in ğ‘‚(ğ‘™ğ‘œğ‘”(ğ‘™ğ‘œğ‘”(ğ‘)))steps, a notable\nupgrade from the binary search and is efficient unless there is very\nhigh data skew, in which case simply binary search or a variation\nof exponential sorting [7] can also be employed.\nAn Optimized Read Query. Putting it all together, we have now\nan optimized read query path that avoids the vast majority of un-\nnecessary data accesses. As Figure 6 shows, we maintain two more\nK=1400\n1010 1500 1060 1890 1834 2321 1389 2872 2100 3200 3201 3900\nBF\nBF BF BF\n1400Global Bloom ï¬lter\nZonemaps\nPer-page BFs\nUnsorted Section \nof OSM-buï¬€erFigure 7: Focusing on the unsorted part of the buffer, a query\nwill first visit the global BF. If the BF query is positive, it\nwill ask all Zonemaps, and then the qualified per-page BFs\nbefore it scans any page.\nZonemaps: one for the OSM-buffer and one for the tree. Hence,\nif the desired key is not in the range of the buffer, we can skip\nthe buffer entirely. On the other hand, in the worst case, we have\nto access the unsorted section of the buffer. However, due to the\nper-page BFs, even if the data is completely scrambled, we will only\naccess a very small number of pages from the unsorted section.\n4.3 Fine-tuning OSM-tree\nIn the previous two subsections, we put together a new meta-design\nthat absorbs inserts faster if they come as near-sorted. Our design\nfurther keeps the read query cost close to the cost of the underlying\ntree. This new meta-design, however, introduces new components\nand a few tuning knobs that can be further tuned. We begin by the\nfill and split factors and the choice of sorting algorithms that affect\nmostly the insertion latency, and we then discuss query-driven\nsorting to adaptively accelerate read queries.\nAdjusting Fill & Split Factor. The textbook bulk loading algo-\nrithms used in Section 4.1 fills every node with the bulk loaded data\nto maximize node utilization (and, thus, minimize space amplifi-\ncation). Since we anticipate several of top-inserts (the fraction of\nwhich depends on the sortedness of the workload), we also leave in\nevery bulk loaded node several empty slots to facilitate top-inserts\nwithout expensive cascading splits. Hence, we adjust the fill factor\nof every bulk loaded leaf to be 95%.\nSimilarly, in the textbook insertion and bulk loading, when an\ninternal node is full it splits right in the middle to generate two\nhalf-full internal nodes. Since during bulk loading, we anticipate\nthat most of the future inserts will be of larger values, we also\nadjust the split factor to 80%, as shown earlier in Figure 3(c). This\nallows us to maintain most of the internal nodes of the underlying\ntree nearly full, even when the data is coming fully sorted, and\nessentially avoid the worst-case space amplification of B+-trees for\nsorted data. In addition to reducing space amplification, we also\nreduce the total number of node splits, leading to lower overall\ninsertion cost.\nChoice of Sorting Algorithm. To reduce the cost of read queries,\nwe sort the buffer after every flush making sorting small data col-\nlections a very frequent operation. We consider three algorithms:\n(i)quicksort , because it is the most common algorithm and has min-\nimal space requirements, (ii) (ğ¾,ğ¿)-adaptive sorting [6], because\n5\n\nSorted Section\nprevious_boundary qxqyqz\nsb1sb2sb3\nquery sorted blocks\nunsortedFigure 8: As inserted data accumulate in the unsorted buffer\nabove a threshold, an incoming query sorts this part as a\nstandalone sorted component, to benefit future queries.\nit aggressively takes into account pre-existing data sortedness (at\nthe expense of ğ‘‚(ğ¾+ğ¿)space usage), and (iii) mergesort , because\nit maintains relative order of duplicate values (a key property to\nknow which overlapping entry is the latest one) at the expense of\nğ‘‚(ğ‘›)space usage. Because we need to maintain relative order of du-\nplicates we are constrained between mergesort and (ğ¾,ğ¿)-adaptive\nsorting. Our analysis shows that for low data-sortedness mergesort\noutperforms(ğ¾,ğ¿)-adaptive sorting (in fact, (ğ¾,ğ¿)-adaptive sort-\ning fails for very high values of ğ¾orğ¿). However, for ğ¾<10%or\nğ¿<5%, their performance is similar and we opt for (ğ¾,ğ¿)-adaptive\nsorting because it has smaller space requirements (ğ¾+ğ¿<ğ‘›).\nIn summary, when the estimated values of ğ¾andğ¿areğ¾<10%\norğ¿<5%of the buffer size we employ (ğ¾,ğ¿)-adaptive sorting,\nand otherwise mergesort, and specifically the C++ standard library\nimplementation of std::stable_sort .\nQuery-Driven Sorted Components. In our current design, only\nthe first part of the buffer that is sorted can benefit from interpola-\ntion search. An additional read optimization is to adaptively add\nstructure to the unsorted part of the buffer with incoming queries.\nWe set a threshold of how large we allow the unsorted portion\nof the buffer to be ( unsorted_threshold ). When the threshold is\nexceeded, the next read query will sort this portion and create a\nnew sorted component. Similar to progressive indexing [ 22] that al-\nlocates a small indexing budget for every query, we allocate a small\nsorting budget for every query as long as we have enough entries\nin the unsorted component. In the general case, the OSM-buffer\nmay contain the main sorted section, multiple sorted components\nof size equal to unsorted_threshold , and a small unsorted sec-\ntion. For example, if the unsorted_threshold is 10% of the buffer\nsize, the buffer will contain five sorted runs (the sorted section and\nfour sorted components) and one unsorted section. The unsorted\nsection still uses all the metadata discussed in Â§4.2 and each sorted\ncomponent employs interpolation search to accelerate queries.\nOSM-buffer Size. The final tuning knob is the size of the OSM-\nbuffer. The goal is to have a large enough buffer that can capture\nsortedness, focusing on ğ¿, i.e., the maximum displacement from the\nexpected position. On the other hand, a large buffer would negate\nany optimization for read queries since the cost of scanning the\nunsorted part will dominate. In Section 6, we vary the buffer size\nand the relative values of ğ¾andğ¿, and we show that even with a\nbuffer significantly smaller than ğ¿we can absorb sortedness to a\nlarge degree, without hurting read queries.\n4.4 Discussion\nHandling Strings. Near-sortedness manifests typically through\nintegers than any other data type, thus, our work focuses on integer\nrepresentations of keys. However, the OSM-design paradigm and\nOSM-tree can be extended to other data formats such as strings.\nSpecifically, to address storage constraints, strings with variable\n(a)\n(b)\n(c)\n(d)\n(e)\n(îƒ )Figure 9: Workloads with varying degrees of sortedness : (a)\nsorted, (b) K=5%, L=10%, (c) K=10%, L=10%, (d) K=25%, L=25%,\n(e) K=50%, L=50%, and (f) scrambled (uni-rand). x-axis: posi-\ntion of entry in data, y-axis: value of entry.\nsizes can be mapped to fixed-length representations using a dic-\ntionary. This accommodates all strings regardless of length. Sub-\nsequently, the fixed-length representations can be binary encoded\nbefore insertion into the index. We can ensure that the order is\npreserved through the mapping and the encoding steps. We aim to\nextend this work and further include a study over near-sorted strings\nincluding designing a synthetic workload generator for near-sorted\nstrings, which to our knowledge has been unexplored.\nLearned Indexes. Contrary to traditional index data structures,\nlearned indexes, use machine learning to learn a model reflecting\npatterns in the data and enable automatic synthesis of indexes at a\nlow engineering cost [ 26]. Lookups in learned indexes avoid expen-\nsive tree traversals, aiming to offer lower access latency. However,\nexisting learned index proposals require trading workload general-\nity for accuracy. Specifically, to achieve sufficient lookup accuracy,\nlearned indexes make the assumption of completely sorted keys,\nthus, depend on prior knowledge of the data. OSM-tree addresses\nthe cost of tree traversal by adapting operations to the data. By\ntaking advantage of inherent sortedness, OSM-tree significantly\nreduces the ingestion cost while offering low latency and full ac-\ncuracy. As future work, we further aim to extend existing learned\nindexes to make them sortedness-aware and relax the assumptions\nregarding input data.\n5 DATA SORTEDNESS BENCHMARK\nWe present the data sortedness benchmark for testing indexes against\nvarying sortedness. The benchmark uses the (ğ¾,ğ¿)-near sorted\nmetric (discussed in Â§2) and is used in our evaluation (Â§6).\nBenchmark Data. The benchmark creates a family of differently\nsorted collections that vary in both ğ¾andğ¿(as a fraction of the\ntotal data size). These metrics effectively capture sortedness by\nrepresenting how many entries are out-of-order and how far apart\nare the entries from their actual position, underlining the effort it\nwould take to establish order in the data collection. When ğ¾=0%\norğ¿=0%the dataset is fully sorted. A dataset with ğ¾=10%and\nğ¿=2%will have 10%of the total entries out of order, each placed\nwithin a distance of 2%of the total entries from its in-order position.\nFigure 9 shows a sample set of differently sorted collections. The\nx-axis denotes the position of the entry in a data collection and\nthe y-axis represents the value of the entry. The band across every\nillustration highlights the ğ¿-window through which elements can\n6\n\nbe shuffled. As this window grows, the band increases in width,\nmeaning elements may be further apart from their ideal positions.\nEvaluation Metrics. The sortedness benchmark evaluates the per-\nformance of a data structure in the presence of variable sortedness\nby measuring: (i) ingestion performance, (ii) overall performance\nof a mixed workload with variable read/write ratio. The default\nsettings for the benchmark include comparison with multiple ğ¿-\nwindows ( 1%,5%,10%,25%,50%), and various ğ¾values for each\nwindow ( 0%,1%,5%,10%,15%,25%,50%).\nIngestion Speedup. This metric quantifies the benefit in terms of\ningestion latency by comparing the underlying index with its OSM\ncounterpart. The benchmark reports both raw performance num-\nbers and the speedup that quantifies the ingestion benefit.\nOverall Speedup. This metric quantifies the benefit when running\na mixed workload with a varying degree of insert-to-lookup ratio\nbetween 10%:90% and 90%:10% . The benchmark reports raw perfor-\nmance and the overall speedup when comparing the underlying\nindex with its OSM counterpart.\nMicrobenchmark Varying ğ¾andğ¿.The above measurements are\ntaken for fixed value of ğ¾andğ¿. The last set of measurements\nof the sortedness benchmark vary both ğ¾andğ¿within a window\nin order to capture their impact. The reported values are raw per-\nformance and speedup as well.\n6 EXPERIMENTAL EVALUATION\nWe now present the experimental evaluation of OSM-tree.\nExperimental Setup. We run the experiments in our in-house\nserver equipped with two sockets each with an Intel Xeon Gold\n5230 2.1GHz processor with 20 cores and virtualization enabled.\nThe server has 384GB of RDIMM main memory at 2933 MHz with\n27.5MB L3 cache and a 240GB SSD. The machine runs on CentOS 8.\nIndex Design. We use a state-of-the-art B+-tree implementation\n[8] and build on top of it to add support for in-order bulk inser-\ntion. Our B+-tree implementation is equipped with a bufferpool of\n300GB, so all our experiments are purely in-memory. Note that the\nbufferpool is orthogonal to the OSM-buffer, which does not have\na disk-resident counterpart. The indexed key and payload, unless\notherwise noted, is 8B in total, with keys and values of 4B each,\nand by default, we use 4KB index pages.\nDefault Setup. The default size of the OSM-buffer is 40MB which\ncan hold up to 5M entires. The OSM-buffer is essentially a dense\narray accompanied by the Zonemaps and Bloom filters as discussed\nin Section 4. For the BFs, we use 10bits-per-entry and maintain the\nfilters at two granularities: (i) one for the entire OSM-buffer and (ii)\none for each page in the OSM-buffer. The BFs use MurmurHash [3]\nfor hashing, following the state-of-the-art [42].\nWorkload. Given the lack of sortedness benchmarks, we create\nour custom workload generator based on the (ğ¾,ğ¿)-nearly sorted\nmetric [ 6] to generate workloads with varying degrees of sortedness\nas discussed in Section 5. Unless otherwise mentioned, the ingestion\nworkload consists of 500M key-value entries with a total size of\n4GB, and the query workload has a variable number of uniform\nrandom non-empty point lookups, interleaved with inserts after\n80%of the ingestion is complete.\n10:90 25:75 40:60 50:50 60:40 75:25 90:10\nRead:Write Ratio0246810Speedup\nK=0%\nK=5%, L=5%\nK=50%, L=50%Fully sorted\nNear-sorted\nLess sorted\nScrambled\nB+-tree CostFigure 10: OSM-tree (Buffer size =5M) is efficient with reason-\nable data sortedness for any read-write ratio.\nOSM Tuning. Unless otherwise mentioned, we tune our OSM-tree\nas follows. The OSM-buffer flushes 50%of the entries when satu-\nrated. The nodes split as 80:20 between the left and right one, and\nthe in-order bulk insertion fills every leaf up to 95%. For workloads\nwith queries interleaved with inserts, a sorted block is created (trig-\ngered by a query) after at least 500K new entries ( 10%of the buffer\ncapacity) are accumulated.\n6.1 Mixed Workload\nWe first compare the performance of OSM-tree with B+-tree by exe-\ncuting a set of mixed workloads with interleaved inserts and queries.\nWe vary the read-write ratio, constructing a continuum between\na write-heavy and a read-heavy workload. For each workload, we\nalso vary the sortedness for the ingested data as: (i) fully sorted , (ii)\nnear-sorted (K=5%, L=5%), (iii) less sorted (K=50%, L=50%), and (iv)\nscrambled (uniformly random). For each experiment, we measure\nthe speedup offered by OSM-tree as ğ‘ ğ‘ğ‘’ğ‘’ğ‘‘ğ‘¢ğ‘ =ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦(B+-tree)\nğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦(OSM-tree),\nwhereğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦(B+-tree)andğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦(OSM-tree)refer to the total\nworkload execution latency of the OSM-tree and B+-tree, respec-\ntively.\nOSM-tree Outperforms B+-Tree. Figure 10 shows that OSM-\ntree significantly outperforms B+-tree if the data is fully\nsorted or near-sorted. For an ingestion-heavy workload, OSM-\ntree leads to 8.8Ã—speedup for fully sorted data and 5Ã—\nbetter for near-sorted data in ingestion-heavy workloads.\n0 1 5 10 25 50\nK(%)012345#EntriesÃ—106 top-insert bulk load\nFigure 11: For higher ğ¾%,\nOSM-tree performs more\ntop-inserts and bulk loads\nfewer entries.OSM-tree achieves this by\nbuffering entries in-memory to\nadd structure to the data and re-\nduce the number of top-inserts.\nIn the case of fully sorted data,\nall of the entries are ingested\nthrough the bulk insertion,\nwhile for the near-sorted data,\nonlyâˆ¼4%of the entries are\ntop-inserts and the remaining\nfollow bulk insertion. Figure 11\nshows that OSM-tree performs\nsignificantly fewer top-inserts for workloads with a high degree of\nsortedness. For fully sorted or nearly sorted workloads (i.e., ğ¾or\nğ¿â‰¤10%), OSM-tree ingests >90%data through bulk loading, and\nthereby, reduces the overall cost for ingestion. As data becomes\nless sorted, OSM-tree mimics the behavior of a B+-tree. This is\n7\n\n0 1 5 10 25 50\nK (%)051015Insert Latency ( \u0016s)(a)\n0 1 5 10 25 50\nK (%)051015Lookup Latency ( \u0016s) (b) B+-tree OSM-tree\n0 1 5 10 25 50\nK (%)051015Latency/Operation ( \u0016s)(c)\n0.01 0.05 0.1 0.5 1 2 510\nSelectivity (%Entries)0246Scan Latency (s)(d)Figure 12: Performance of OSM-tree (buffer size =40MB) forğ¿=5%. (a) OSM-tree offers better ingestion performance with work-\nloads with some degree of data sortedness. (b) OSM-tree incurs a small overhead for point queries. (c) For a mixed workload\nwith equal proportions of reads and writes, the ingestion-benefits outweigh the lookup-overhead and offers better overall\nperformance. (d) OSM-tree offers competitive performance for both short and long range scans.\nbecause, as the degree of data sortedness decreases, OSM-treeâ€™s\nability to capture the out-of-order elements using a relatively small\nbuffer ( 1%of data size) diminishes, and the number of top-inserts\nperformed becomes comparable to that of a B+-tree.\nRegardless of data sortedness, the benefit of OSM-tree is more\npronounced for write-intensive workloads. Conversely, in Figure 10\nwe observe that for a lookup-heavy workload ( 90%lookups), OSM-\ntree offers a speedup of 1.4Ã—and 1.3Ã—for fully sorted and nearly\nsorted data, respectively. This is because the significant perfor-\nmance benefits of OSM-tree during ingestion are countered by the\nlookup overhead incurred.\nIngesting Scrambled Data Does Not Benefit from OSM-tree.\nWhen the ingestion is scrambled, OSM-tree does not offer perfor-\nmance benefits. Specifically, when the data is generated uniformly\nrandom, using a B+-tree is about 20% faster than OSM-tree, regard-\nless of the proportion of ingestion and lookups in a workload. This\nis attributed to the fact that a finite buffer is unable to capture the\n(minimal) sortedness of the incoming data. This, in turn, forces\nOSM-tree to always perform top-inserts. For that reason, the OSM\nbuffer management cost (sorting the buffer, managing metadata,\nand probing BFs during lookups) does not pay off, however, it keeps\nthe penalty to a modest 20%. This observation is inline with our\ngoals and our expectation. While OSM-tree is very useful for a vary-\ning degree of sortedness, for fully scrambled data, the worst-case\nguarantees of a classical B+-tree are enough.\nOSM-tree Reduces Memory Footprint for High Data Sorted-\nness. The memory footprint of OSM-tree is 0.52Ã—(0.6Ã—) of the size\nof a B+-tree for fully sorted (near-sorted) data as shown in Table 1.\nThe gain in memory footprint is attributed to the 80:20 split ratio\nof OSM-tree nodes, which achieves higher fill factor for high data\nsortedness. On the other hand, for less-sorted data the 80:20 split\nratio does not offer any benefit.\n6.2 Raw Performance\nNext, we compare the OSM-tree and B+-tree in terms of ingestion\nand query performance separately. We vary the number of out-of-\norder entries ( ğ¾%) in the ingestion workload, while keeping the\nmaximum displacement of an out-of-order entry ( ğ¿%) constant. We\nkeepğ¿constant because capturing the degree of sortedness with\ndifferentğ¿values requires changing the buffer size as a function of ğ¿.\nIn Sections 6.3 and 6.4, we discuss the implications on performance\nof varying both ğ¾,ğ¿as well as the buffer size.Sortedness Degree# Nodes (#Int + #Leaf)\nB+-tree OSM-tree\nFully-Sorted 2.004M (8K, 1.996M) 0.52 Ã—(0.32Ã—, 0.52Ã—)\nNear-Sorted 1.847M (7K, 1.840M) 0.6 Ã—(0.38Ã—, 0.6Ã—)\nLess-Sorted 1.878M (4.3K, 1.873M) 1.01 Ã—(1.7Ã—, 1.01Ã—)\nTable 1: Memory footprint is reduces in OSM-tree with fully\nsorted and near-sorted data\nSetup. To measure the raw ingestion performance, we first ingest\n500M entries ( 4GB), and for the query performance, we perform\n50M (10%) lookups on the inserted keys. We report the worst-case\nlookup performance, by making sure that the buffer is full before\nexecuting any query. To analyze the range scan performance, we\nexecute 100range scans generated randomly from the key-domain\non a preloaded database for different scan-selectivities.\nOSM-tree Dominates the Ingestion Performance. Figure 12(a)\nshows that OSM-tree performs significantly better than B+-tree for\ninserts if there is any degree of data sortedness. For fully sorted\n(ğ¾=0%) and nearly sorted ( ğ¾=1%,5%, and 10%) workloads, OSM-\ntree reduces the ingestion latency by âˆ¼90%andâˆ¼82%, respectively.\nEven for data with lower degrees of sortedness ( ğ¾=25%and 50%),\nthe ingestion latency in OSM-tree is reduced by âˆ¼27%compared to\nB+-tree. Figure 13(a) shows the breakdown of the ingestion costs in\nOSM-tree for (i) fully sorted ( ğ¾=0%), (ii) nearly sorted ( ğ¾=ğ¿=5%),\nand less sorted ( ğ¾=ğ¿=50%) workloads. We observe that for fully\nsorted workloads, the OSM-tree is able to bulk load the entire data\nset without requiring any additional processing. For near-sorted\ndata, OSM-tree sorts the buffer periodically (driven by the queries\nand the buffer-saturation) which accounts for 38% of the work-\nload execution latency. However, this additional cost paid to add\nstructure to the data leads to significantly fewer top-inserts, which\nin turn, reduces the overall latency. Finally, for less sorted data,\nOSM-tree ingests fewer entries via bulk loading. Instead, a signifi-\ncant amount of data is ingested through top-inserts â€“ this behavior\nresembles the one of a B+-tree. However, as shown in Figure 10,\nOSM-tree still outperforms the state of the art by a significant mar-\ngin. Note that OSM-tree achieves this performance with a buffer\nthat is 20% in size when compared to ğ¿(1% vs. 5%). This implies\nthat even with a considerably small buffer that does not capture\nthe out-of-order elements , OSM-tree performs significantly fewer\ntop-inserts and is able to bulk load a large fraction of the data.\nFast Ingestion Comes at a Small Overhead in Queries. Fig-\nure 12(b) compares the point lookup performance of OSM-tree to\n8\n\nthat of the B+-tree, and we observe that OSM-tree incurs an over-\nhead betweenâˆ¼5%andâˆ¼26%for point lookups. This is due to the\nadditional time spent searching for the target entry in the buffer.\nFigure 13(b) shows the breakdown of the point query latency in\nOSM-tree for (i) fully sorted ( ğ¾=0%), (ii) nearly sorted ( ğ¾=ğ¿=5%),\nand less sorted ( ğ¾=ğ¿=50%) workloads. We observe that regardless\nof the degree of data sortedness, between 80%and 99%of the query\nlatency comes from the tree-search. With a full buffer, OSM-tree\nintroduces an overhead due to (i) probing the OSM-buffer using\ninterpolation search and sequential scans and (ii) performing OSM-\noperations such as sort-merging the entries in buffer and updating\nthe metadata. This overhead depends on the number of entries in\nthe buffer and degree of data sortedness. Note that for fair compar-\nison, we assumed the buffer to be completely full when the query\nworkload was executed. In practice, the buffer is expected to be\n50%saturated on average, which would reduce the buffer-related\noverheads by 2Ã—. Further, searching within the buffer and in the\ntree in parallel can reduce OSM-treeâ€™s lookup cost.\nThe Benefits Outweigh the Overhead almost Always. To\nweigh the benefits of OSM-tree against its read overhead, in Figure\n12(c), we show the mean latency per operation for a mixed workload.\nWe observe that for a workload with equal number of reads and\nwrites, OSM-tree improves the mean latency by âˆ¼70%for fully and\nnearly sorted data. Even for workloads with larger degrees of sort-\nedness, OSM-tree offers 1.25Ã—improved overall performance. To\nsummarize, for read-only workloads, the performance of OSM-tree\nis similar to that of B+-trees, as the buffer remains empty, and thus,\nadds no overhead. However, if a mixed workload is read-dominated\n(writes <1%), the incurred read overhead out-weighs the benefits\non ingestion of OSM-tree. We present a detailed account of the\napplicability of OSM-tree in Section 6.3.\nOSM-Tree Offers Competitive Scan Performance. Figure 12(d)\nshows that OSM-tree performs similarly to B+-trees for range\nqueries with different selectivity, varying from 0.01%(50K entries)\nto1%(5M entries). We observe that scan latency for OSM-tree and\nB+-tree remain largely comparable while the selectivity is varied.\nThis is because the scan latency is directly proportional to selectiv-\nity, and with a tree of the same height, the tree-traversal and leaf\nnode scan costs are similar for both systems. For OSM-tree, the tree\nsearch latency dominates the scan time of the OSM-buffer, which\ncan be further reduced by parallelizing it with the tree search and\nsort-merging the result sets.\n6.3 Workload Influence\nSetup. We measure the speedup offered by OSM-tree over state-of-\nthe-art B+-trees for mixed workloads with varying data sortedness.\nWe vary both ğ¾=(0,1,5,10,50)andğ¿=(1,5,10,50)and experiment\nwith 20different degrees of data sortedness.\nVarying the Workload Composition. We observe in Figures\n14(a)-14(c) that as the proportion of reads increases in a work-\nload (from 10%to50%to90%), the overhead incurred by reads in\nOSM-tree begins to counterbalance its ingestion-benefits. Even for\na fully sorted workload, as the read-proportion increases from 10%\nto90%, the speedup is reduced from 9.2 Ã—to 1.4Ã—. Thus, for different\ndegrees of data sortedness, Figure 14 serve as a guideline for the\napplicability of the OSM-tree design.\nSorted K=L=5% K=L=50%\nDegree ofSortedness02000400060008000Latency (s)(a)bulk load\nsort\ntop-insert\nSorted K=L=5% K=L=50%\nDegree of Sortedness0100200Latency (s)(b) bu\u000ber search\nsortOSM ops.\ntree searchFigure 13: Latency Breakdown of Operations in OSM-tree.\n(a) The time spent by OSM-tree for top-insert of entries\ninto the index escalates with decreasing data sortedness. (b)\nTree search dominates query latency, while the fraction of\ntime spent for metadata management and maintenance of\nzonemaps + BFs increases for lower data sortedness.\nVarying the Degree of Sortedness. Analyzing the variation in\nthe speedup for ğ¾=1%(second column) and ğ¿=1%(forth row) in\nFigure 14(a), we observe that the effects on the number of unordered\nentries in the workload (i.e., ğ¾) influences the performance of OSM-\ntree by a greater extent compared to the maximum displacement\nof an unordered element (i.e., ğ¿). This is because if the buffer size is\ncomparable to the ğ¿-value, the number of unordered entries drive\nthe cost of data reordering, and relative overlaps between buffer\ncycles (causing top-inserts) is minimal. However, as ğ¿gets larger\n(first row), its impact on the speedup becomes more significant\nthan that of ğ¾(fifth column). With increase in both ğ¾andğ¿, the\nOSM-tree begins to operate similarly to B+-trees, and the speedup\noffered approaches 1.\n6.4 OSM-buffer Tuning\nSetup. To analyze the implications of buffer size on OSM-treeâ€™s\nperformance, we first increase the buffer size to 5%of the data size\nand compare the results with those discussed in Section 6.2. Next,\nwe run an ingestion-only workload with 500M entries followed by\na read-only workload with 50M point lookups, and we vary the\nbuffer size between 0.5%-5%of the data size. For mixed workload,\nwe pre-load the index (to 80%) and perform interleaved inserts and\nreads ( 50%ğ‘Š-50%R) for different degrees of sortedness.\nIncreasing the Buffer Size Improves Performance. We now\nvary the size of the OSM-buffer to show its implications on per-\nformance. The OSM-buffer acts as a reservoir that absorbs the\nout-of-order ingested elements to an extent. Thus, increasing the\nbuffer size allows for bulk loading a larger fraction on the data while\nreducing the number of top-inserts. Figure 14(d) shows the speedup\noffered by OSM-tree when OSM-buffer size is increased to 5%of\nthe data size ( 200MB) for a mixed workload with equal proportions\nof reads and writes. Comparing with Figure 14(b) (which has the\nsame setup but a buffer size of 1%), we observe that a 5Ã—increase\nin the buffer size increases the speedup from 4.2Ã—to8.2Ã—(a95.2%\nincrease) for a fully sorted workload and between 27.6%and 176.9%\nfor nearly sorted data. Even for ğ¾=ğ¿=50%, increasing the buffer\nsize improves the achieves speedup from 1Ã—to1.3Ã—.\nBuffer Size Affects the Ingestion Performance Significantly.\nFigure 15 shows the implications of varying buffer size sepa-\nrately on the ingestion and lookup performance for OSM-tree.\nIn this set of experiments, we vary the OSM-buffer size for a\nfixed sortedness ( ğ¾=ğ¿=5%). We observe that even with a\nsmall buffer size of 20MB ( 0.5%of the data size), OSM-tree offers\n9\n\n0 1 5 10 50\nK(%)151050L(%)\n9.2 6.3 6.2 5.7 3.49.2 6.0 5.9 4.1 1.29.2 5.9 4.8 2.4 1.19.2 5.5 1.3 1.1 1.0(a) Buï¬€er size=40MB; 10% R, 90% W\n0 1 5 10 50\nK(%)4.2 3.5 3.4 3.3 3.04.2 3.3 3.1 2.8 1.24.2 3.2 2.9 2.0 1.14.2 2.9 1.3 1.1 1.0(b) Buï¬€er size=40MB; 50% R, 50% W\n0 1 5 10 50\nK(%)1.4 1.4 1.3 1.3 1.21.4 1.3 1.3 1.2 1.11.4 1.3 1.3 1.1 1.11.4 1.2 1.0 0.9 0.9 (c) Buï¬€er size=40MB; 90% R, 10% W\n0 1 5 10 50\nK(%)8.2 5.1 5.1 5.0 4.98.2 4.3 4.2 4.1 3.68.2 4.4 4.1 4.0 2.68.2 3.7 3.6 2.6 1.3Speedup\n2468(d) Buï¬€er size=200MB ; 50%  R, 50%  WFigure 14: Performance of OSM-tree with varying degrees of sortedness. (a) In a write-heavy workload, OSM-tree exploits data\nsortedness to offer maximum benefit in overal performance. (b) Increase in reads to the index diminishes the benefit offered\nby OSM-tree. (c) OSM-tree performs similar to B+-tree for read-heavy workloads with minimal performance benefits due to\ndata sortedness. (d) A larger buffer in OSM-tree is better at capturing even higher sortedness, to improve overall performance.\na5.7Ã—speedup during ingestion. As the buffer size increases to\n5%, the ingestion speedup further increases to 7Ã—, since a larger\nbuffer reduces the proportion of data ingested through top-inserts.\n0.5 1 2 5\nBuï¬€er Size (%Entries)0.02.55.07.5SpeedupB+-treeInserts Lookups\nFigure 15: The ingestion\nperformance of OSM-tree\nincreases proportionally with\nbuffer size.We also observe in Figure 15\nthat point lookup perfor-\nmance in OSM-tree is affected\nby the buffer size marginally.\nA10Ã—increase in the buffer\nsize (from 20MB to 200MB)\ninduces a deterioration of\n11.5%in the lookup latency.\nThis conforms with our ob-\nservations from Figures 12(b)\nand 14(c) regarding the\nsuitability of the OSM-design;\nhowever, as shown in Figure 10 the benefits on OSM-tree outweigh\nthe read-overheads even for a small fraction of writes ( â‰¥5%).\nTuning the Buffer Flush Threshold. Adjusting the flush thresh-\nold of the OSM-buffer affects the overall performance of OSM-tree.\nWe now vary the proportion of entries flushed from the buffer at a\ngiven cycle between 25%,50%, and 75%, and run mixed workloads.\nIn the interest of space, we omit the figure and focus on the key\nobservation. When the buffer flush threshold is set to 25%, OSM-\ntree offers a speedup between 1.0Ã—and 4.0Ã—. For flush threshold\n50%, the speedup of OSM-tree becomes between 1.0Ã—and 4.3Ã—, and\nfor a threshold of 75%, between 0.91Ã—and 4.2Ã—. Hence, OSM-tree\nperforms best for 50%flush threshold, which we default to.\nTuning Query-Based Sorting. Figure 16 shows the implications\nof query-based sorting on the overall performance of OSM-tree. We\n0 1 5 10 50\nK(%)0123456SpeedupQ-S = 1%\nQ-S = 5%\nQ-S = 10%Q-S = 25%\nw/o Q-S\nFigure 16: Query-based sort-\ning threshold set to 10%offers\nthe highest speedup.vary the query-based sort-\ning threshold between 1%,5%,\n10%,25%and 100% (disabling\nquery-based sorting) and run\nmixed workloads. The y-axis\nshows the speedup when com-\npared against our B+-tree base-\nline. We observe that enabling\nquery-based sorting offers a\nperformance improvement be-\ntween 7%(for 1%threshold)\n0 1 5 10 25 50\nK(%)036912Latency per Insert( \u0016s) (a)Na \u0010ve OSM\nOSM-Global BF\nFull OSM\n0 1 5 10 25 50\nK(%)012345Latency per Lookup( \u0016s)(b)Figure 17: Latency breakdown for different OSM-tree con-\nfigurations. (a) Adding BFs to the OSM-buffer slightly in-\ncreases the insert latency. (b) The use of BFs in the buffer for\nlookups is more pronounced as data sortedness decreases.\nand 25% (for 10% threshold). As expected by gradually sorting\nthe buffered data significantly accelerates query performance on\naverage since the portion the buffer that needs to be scanned is\nkept small. Moreover, we observe that query-based sorting has\ndiminishing returns if we apply it too frequently. Specifically, set-\nting the threshold to 10% offers the maximum speedup for any\ndata sortedness, while other values affect performance adversely\n(if at all). Reducing the threshold (to 1%or5%) leads to too much\nsorting, while increasing the threshold (to 25%) results in fewer\nsorted blocks within the buffer, so the cost of scanning the unsorted\nsection remains high. Hence, we emperically tune OSM-tree to\nperform query-based sorting with threshold 10%.\nBenefits from the Global and Per-page BFs. To demonstrate\nthe benefits coming from the global BF and from the per-page BFs\nin the OSM-buffer we compare OSM-tree with two simpler vari-\nations: one without any BFs ( NaÃ¯ve OSM ) and one with only the\nglobal BF ( OSM-Global BF ) by ingesting 500M near-sorted entries\nand then performing 50M non-empty point lookups. As expected,\nupdating the BFs during every insertion comes at a marginal cost\nincrease at ingestion time as shown in Figure 17(a). For less sorted\ndata the added cost at ingestion time is a smaller fraction of the total\ninsert time. However, the additional cost at ingestion time pays\noff since at query time we have significant performance benefits.\nFigure 17(b) shows that adding the global BF speeds up queries up\nto14%, while the per-page BFs further boost performance leading\nto16%aggregate improvement. The positive impact of per-page BFs\nis limited by query-based sorting that also helps avoid scanning un-\nnecessary data (limiting the portion of OSM-buffer to be scanned to\n<10%). Note that the ingestion performance of OSM-tree is always\n10\n\n10:90 25:75 40:60 50:50 60:40 75:25 90:10\nRead:Write Ratio0246810Speedup\nK=0%\nK=5%, L=5%\nK=50%, L=50%Fully sorted\nNear-sorted\nLess sorted\nScrambled\nB+-tree CostFigure 18: For disk-based execution with 1% bufferpool,\nOSM-tree always outperforms B+-tree.\nsignificantly faster than B+-tree regardless of the BF cost. Overall,\nthe benefit of BFs is pronounced for workloads with more reads.\nTuning Zonemaps. OSM-buffer uses Zonemaps during ingestion\nto approximate sortedness (Â§4.1) making them integral to the overall\ndesign. While we opt to always use them at query time since they\nare always available, we experimentally measured that skipping\nZonemaps at query time reduces performance by 35% on average.\n6.5 On-Disk Performance\nIn our next experiment, we explore an OSM-tree setup that accesses\ndisk-resident data. Our design comes with a bufferpool that, in this\nexperiment, fits all internal tree nodes ( âˆ¼1%of data size). We repeat\nthe initial experiment for variable data sortedness and variable\nread vs. write ratio, to present the speedup between OSM-tree and\nB+-tree as shown in Figure 18. Note that the OSM-buffer is set\nto1%of the total data. From the disk-based experiment we draw\nsimilar conclusions to the initial in-memory one (Fig. 10), with a\nnotable difference. OSM-tree now always outperforms B+-tree even\nfor read-intensive workloads with fully scrambled data. This is\nbecause, regardless of data sortedness, we increase locality through\nour sorting procedures in the buffer. Though this is applicable for\nboth in-memory and disk-based experiments with OSM-tree, the\noverhead of managing the buffer is negligible when compared to\naccessing tree nodes for the disk-based experiment. Overall, when\nspilling to disk, OSM-tree leads to significant performance savings\nup to 8Ã—for write-intensive workloads with high data sortedness,\nwhile always outperforming B+-tree.\n6.6 Scalability\nOSM-Tree Scales Similarly to the State of the Art. To analyze\nthe scalability of OSM-tree, we increase the number of entries\ningested from 31.25M to 1B while varying ğ¾andğ¿as proportional\n(5%) to the workload size. We also scale the actual size of OSM-buffer\nby keeping it equal to 1%of the dataset size. For each experiment,\nwe first ingest 80%of the ingestion workload and then interleave the\nremain inserts with equal number of of point lookups. As expected,\nthe degree of data sortedness does not affect the performance of\nB+-trees and the latency per operation remains nearly unaffected\nby the increase in data size. OSM-tree scales similarly to the state of\nthe art while offering a speedup between 2.32Ã—and 3.14Ã—as shown\nin Figure 19(a). Increasing the buffer size proportionally with ğ¿\nallows OSM-tree to absorb the same degree of sortedness within\nthe buffer regardless of the buffer size.\n0.5 1 2 4 8 16\nData Size (GB)0510Latency/Operation ( \u0016s)\n(a) B+-tree OSM-tree\n0.5 1 2 4 8 16\nData Size (GB)0510Latency/Op eration (Âµs)\n(b) B+-tree OSM-treeFigure 19: OSM-tree scales well with data size and always\noutperforms B+-tree.\nDatasize 0.5GB 1GB 2GB 4GB 8GB 16GB\n#Entries 31.25M 62.5M 125M 250M 500M 1B\n#Ent. in Buff. 2.5M 2.5M 2.5M 2.5M 2.5M 2.5M\n%Ent. in Buff. 8% 4% 2% 1% 0 .5% 0.25%\n#Ent. in Tree 28.75M 60M 122.5M 247.5M 497.5M 997.5M\nTable 2: For a fixed ğ¿and buffer size, the fraction of entries\nin the buffer reduce with increasing workload data size.\nOSM-Tree Scales Better for Fixed ğ¿and Buffer Size. In this\nexperiment, we treat ğ¿as a constant rather than as a fraction of the\ndata set. We set ğ¿to12.5M and the buffer size to 40MB. We then\nrun a mixed workload with equal reads and writes on a preloaded\nindex. In Figure 19(b), we observe that while B+-tree scales steadily\nwith the data size (as explained earlier), as we increase the number\nof entries, the mean latency per operation reduces for OSM-tree.\nOSM-tree offers between 43% and 65% improvement in terms of\nthe latency per operation compared to B+-tree. A 32Ã—increase in\nthe data size reduces the average query latency by âˆ¼37%. As the\ndata size increases, the fraction of the ingested data retained in the\nbuffer reduces exponentially (as shown in Table 2). This causes an\nincreasing fraction of point lookups to skip the buffer through the\nOSM-buffer Zonemap, hence, avoiding the OSM-buffer access cost.\n6.7 Experimenting with TPC-H\nFinally, we evaluate OSM-tree by comparing its performance\nagainst state-of-the-art B+-trees on TPC-H [40] data.\nSetup. For this experiment, we use the tuples in the lineitem table\n(as discussed in Section 1) as our workload. We sort the tuples based\non the shipdate attribute which, in turn, creates a nearly sorted\ndata set with respect to the receiptdate attribute. We attribute this\ndegree of sortedness on receiptdate as:ğ¾=96.67% andğ¿=0.1%.\nThe workload is comprised of 6M tuples. For both OSM-tree and\nthe B+-tree, we first preload the database and the index with 4.8M\nentries and then execute a mixed workload with varying propor-\ntions of reads and writes. We measure the latency per operation\nduring the mixed workload execution and compute the speedup\naccordingly. For each set of experiment, we also vary the buffer\nsize between 0.05%and 1%of the data size.\nOSM-Tree Offers Superior Performance. Table 3 shows that\nOSM-tree performs significantly better than B+-trees across all\nbuffer sizes and for all workload compositions. Even with a buffer\nthat is 0.05%of the data size, OSM-tree offers between 1.14Ã—and\n1.63Ã—speedup. As the buffer size increases, it is able to cache more\nentries before flushing, which reduces the number of top-inserts per-\nformed, improving ingestion performance. We also observe that the\nbenefits of OSM-tree diminish as the proportion of reads increase\nin the workload; however, even for a workload with 90% reads,\n11\n\nRead : WritesBuffer Size (%data size )\n0.05% 0.1% 0.25% 0.5% 1.0%\n10%:90% 1.63Ã— 2.60Ã— 2.88Ã— 4.46Ã— 5.28Ã—\n25%:75% 1.54Ã— 2.40Ã— 2.49Ã— 3.62Ã— 4.57Ã—\n50%:50% 1.56Ã— 2.07Ã— 2.82Ã— 3.21Ã— 3.40Ã—\n75%:25% 1.25Ã— 1.65Ã— 1.72Ã— 2.09Ã— 2.01Ã—\n90%:10% 1.14Ã— 1.24Ã— 1.28Ã— 1.42Ã— 1.41Ã—\nTable 3: When querying TPC-H data, OSM-tree always out-\nperforms B+-tree offering speedups between 1.14Ã—and 5.3Ã—.\nOSM-tree offers a speedup of 1.3Ã—on average. Interestingly, for\nlarger proportions of reads ( â‰¥75%), a larger buffer size ( â‰¥1%) causes\nreads to probe more data in the buffer for every lookup, which, in\nturn, causes a slight drop in OSM-treeâ€™s speedup. Overall, this ex-\nperiment highlights that the OSM-design is able to offer significant\nperformance benefits compared to the state of the art with a very\nsmall buffer size ( 0.05%) even for workloads with large proportions\nof reads ( 90%) and low degree of data sortedness ( ğ¾=96.67%).\n7 RELATED WORK\nWhile to the best of our knowledge, this is the first work on de-\nsigning sortedness-aware indexes, in this section, we discuss the\nliterature on ingestion-optimized index structures.\nOptimizing for Tree Ingestion. B+-trees are widely used as the\nindexing data structure in commercial database systems due to\nits balanced ingestion and query performance [ 14]. Over the past\nyears, several B+-tree-variants have been proposed that focus on\noptimizing ingestion and promote batching. Lehman and Carey\nproposed an in-memory tree index, T-tree [ 27], which improves\ninsertion and access performance by storing pointers to data in the\nnodes. CSB+-tree [ 36] and PLI-tree [ 39] attempt to maximize the\ncache line utilization by using arithmetic operations to calculate the\nchild nodes rather than pointer operations to reduce the ingestion\nlatency. YATS-tree (or Y-tree) [24] is a hierarchical indexing struc-\nture that aims to maximize bulk insertion by pushing new inserts\ninto separate blocks based on a total order. Partitioned B+-tree [ 19]\noptimizes bulk insertion by using an artificial leading column to\nalways append, which leads to creating multiple indexes on over-\nlapping data. At query time, in case the leading column (e.g., epoch)\nis known, this leads to efficient execution, otherwise, multiple par-\ntitions have to be searched. Similarly, Bğœ–-tree [ 10] reduces the cost\nof trickle insertions by buffering data in internal nodes.\nContrary to OSM-tree, none of the above B+-tree-variants aim to\nexploit implicit structure in the ingestion workload. The OSM-tree\ndesign techniques presented in this paper, offer superior perfor-\nmance by allowing efficient out-of-order insertions and taking ad-\nvantage of data sortedness when available. Furthermore, the OSM\ndesign paradigm can be adapted to reduce storage overhead and\ncache misses, as well as to reduce the balancing cost.\nLSM-trees. LSM-trees [ 28,32] optimize data ingestion by buffering\nand flushing to disk sorted pages, however, their design comes at a\nhigh write amplification cost. Data entries are repeatedly re-writtenon disk, and LSM-trees periodically sort-merge smaller sorted com-\nponents to create larger sorted collections of data through the\nprocess of compactions [37]. While LSM-trees aim to maximize in-\ngestion throughput, they are not designed to exploit sortedness. In\nfact, most LSM-tree designs are completely agnostic to data sorted-\nness and they perform the same amount of merging and (re-)writing\nof the data on disk even when the data arrive fully sorted. When\nan LSM-tree employs a leveled partial compaction strategy with\nno-overlap data movement policy [ 37] it can accelerate ingestion of\nsorted data, however, to fully exploit a varying degree of sortedness,\nmore changes are needed in various compaction decisions. LSM\ncan benefit from the OSM meta-design to better exploit variable\nsortedness. Further, the LSM design per se can be optimized to better\nhandle sorted data ingestion.\nData Series and Data Streaming. Data series store data with a\nmonotonically increasing component, typically a timestamp [ 33].\nData series indexing assumes that data ingestion follows the ex-\npected order [ 25,43â€“45]. The ingested data is converted to shapes\nusing specialized representations like iSAX [ 11], in order to allow\nsimilarity comparisons between data series.\nData streaming applications operate on windows of data (typ-\nically time-based) in order to calculate state on-the-fly, and then,\ndiscard the incoming entries [ 12,17]. Hence, streaming systems\ninspect whether data arrives out of the expected order and often\nuse a buffer to capture this arrival skew [ 38]. They do not build an\nindex for the entire dataset, and similar to data series, the default\nexpectation is that data arrives in the expected order.\nContrary to data series and data streaming, in relational systems,\nthe arrival of data is, in general, scrambled; however, indexes are\nnot designed to benefit from data arriving in-order or near-order. In\nthis work, we treat sortedness as a resource , and we build a general\npurpose index that can substantially outperform existing indexes if\ndata ingestion order follows the indexed key, while falling back to\nbaseline performance if there is no underline data sortedness.\n8 CONCLUSION\nInserting data to an index can be perceived as the process of adding\nstructure to an otherwise unsorted data collection. We identify\ninherent data sortedness as a resource that should be harnessed\nwhen ingesting data. However, state-of-the-art index designs like\nB+-trees can only enable faster ingestion through bulk loading when\ndata arrives fully sorted, and they fail to benefit from sortedness\nwhen data is near-sorted.\nTo address this, we propose an index meta-design that allows for\nprogressively faster ingestion when the incoming data has increas-\ningly higher sortedness. Our proposed design, called OSM-tree,\nuses partial bulk loading, index appends, variable split factor, and\nin-memory buffering to amortize the insertion cost. OSM-tree also\noffers competitive lookup performance by engaging Bloom filters\nand Zonemaps for data in the in-memory buffer. Our experiments\ndemonstrate that OSM-tree outperforms the baseline B+-tree by up\nto8.8Ã—in presence of data sortedness and offers up to 5Ã—perfor-\nmance improvement for mixed read/write workloads.\n12\n\nREFERENCES\n[1]D. Achakeev and B. Seeger. Efficient Bulk Updates on Multiversion B-trees.\nProceedings of the VLDB Endowment , 6(14):1834â€“1845, 2013.\n[2]M. Ajtai, T. S. Jayram, R. Kumar, and D. Sivakumar. Approximate counting of\ninversions in a data stream. In Proceedings of the Annual ACM Symposium on\nTheory of Computing (STOC) , pages 370â€“379, 2002.\n[3] A. Appleby. MurmurHash. https://sites.google.com/site/murmurhash/ , 2011.\n[4]M. Athanassoulis and A. Ailamaki. BF-Tree: Approximate Tree Indexing. Pro-\nceedings of the VLDB Endowment , 7(14):1881â€“1892, 2014.\n[5]M. Athanassoulis, M. S. Kester, L. M. Maas, R. Stoica, S. Idreos, A. Ailamaki, and\nM. Callaghan. Designing Access Methods: The RUM Conjecture. In Proceedings\nof the International Conference on Extending Database Technology (EDBT) , pages\n461â€“466, 2016.\n[6]S. Ben-Moshe, Y. Kanza, E. Fischer, A. Matsliah, M. Fischer, and C. Staelin. De-\ntecting and Exploiting Near-Sortedness for Efficient Relational Query Evaluation.\nInProceedings of the International Conference on Database Theory (ICDT) , pages\n256â€“267, 2011.\n[7]J. L. Bentley and A. C.-C. Yao. An Almost Optimal Algorithm for Unbounded\nSearching. Information Processing Letters , 5(3):82â€“87, 1976.\n[8] T. Bingmann. STX B+ Tree. https://github.com/bingmann/stx-btree , 2007.\n[9]B. H. Bloom. Space/Time Trade-offs in Hash Coding with Allowable Errors.\nCommunications of the ACM , 13(7):422â€“426, 1970.\n[10] G. S. Brodal and R. Fagerberg. Lower Bounds for External Memory Dictionaries. In\nProceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) ,\npages 546â€“554, 2003.\n[11] A. Camerra, T. Palpanas, J. Shieh, and E. J. Keogh. iSAX 2.0: Indexing and Mining\nOne Billion Time Series. In Proceedings of the IEEE International Conference on\nData Mining (ICDM) , pages 58â€“67, 2010.\n[12] P. Carbone, M. Fragkoulis, V. Kalavri, and A. Katsifodimos. Beyond Analytics:\nThe Evolution of Stream Processing Systems. In Proceedings of the ACM SIGMOD\nInternational Conference on Management of Data , pages 2651â€“2658, 2020.\n[13] S. Carlsson and J. Chen. On Partitions and Presortedness of Sequences. In Acta\nInformatica , volume 29, pages 267â€“280, 1992.\n[14] D. Comer. The Ubiquitous B-Tree. ACM Computing Surveys , 11(2):121â€“137, 1979.\n[15] G. Cormode, S. Muthukrishnan, and S. C. Sahinalp. Permutation Editing and\nMatching via Embeddings. In Proceedings of the International Colloquium on\nAutomata, Languages and Programming (ICALP) , volume 2076, pages 481â€“492,\n2001.\n[16] J. V. den Bercken and B. Seeger. An Evaluation of Generic Bulk Loading Tech-\nniques. In Proceedings of the International Conference on Very Large Data Bases\n(VLDB) , pages 461â€“470, 2001.\n[17] M. Fragkoulis, P. Carbone, V. Kalavri, and A. Katsifodimos. A Survey on the\nEvolution of Stream Processing Systems, 2020.\n[18] P. Gopalan, T. S. Jayram, R. Krauthgamer, and R. Kumar. Estimating the Sortedness\nof a Data Stream. In Proceedings of the Annual ACM-SIAM Symposium on Discrete\nAlgorithms (SODA) , pages 318â€“327, 2007.\n[19] G. Graefe. Sorting And Indexing With Partitioned B-Trees. In Proceedings of the\nBiennial Conference on Innovative Data Systems Research (CIDR) , 2003.\n[20] G. Graefe. Modern B-Tree Techniques. Foundations and Trends in Databases ,\n3(4):203â€“402, 2011.\n[21] A. Gupta and F. Zane. Counting inversions in lists. In Proceedings of the Annual\nACM-SIAM Symposium on Discrete Algorithms (SODA) , pages 253â€“254, 2003.\n[22] P. Holanda, S. Manegold, H. MÃ¼hleisen, and M. Raasveldt. Progressive Indexes:\nIndexing for Interactive Data Analysis. Proceedings of the VLDB Endowment ,\n12(13):2366â€“2378, 2019.\n[23] S. Idreos and M. Callaghan. Key-Value Storage Engines. In Proceedings of the\nACM SIGMOD International Conference on Management of Data , pages 2667â€“2672,2020.\n[24] C. Jermaine, A. Datta, and E. Omiecinski. A Novel Index Supporting High Volume\nData Warehouse Insertion. In Proceedings of the International Conference on Very\nLarge Data Bases (VLDB) , pages 235â€“246, 1999.\n[25] H. Kondylakis, N. Dayan, K. Zoumpatianos, and T. Palpanas. Coconut: A Scalable\nBottom-Up Approach for Building Data Series Indexes. Proceedings of the VLDB\nEndowment , 11(6):677â€“690, 2018.\n[26] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The Case for Learned\nIndex Structures. In Proceedings of the ACM SIGMOD International Conference on\nManagement of Data , pages 489â€“504, 2018.\n[27] T. J. Lehman and M. J. Carey. A Study of Index Structures for Main Memory\nDatabase Management Systems. In Proceedings of the International Conference on\nVery Large Data Bases (VLDB) , pages 294â€“303, 1986.\n[28] C. Luo and M. J. Carey. LSM-based Storage Techniques: A Survey. CoRR ,\nabs/1812.0, 2018.\n[29] C. Luo and M. J. Carey. LSM-based Storage Techniques: A Survey. The VLDB\nJournal , 29(1):393â€“418, 2020.\n[30] H. Mannila. Measures of Presortedness and Optimal Sorting Algorithms. IEEE\nTransactions on Computers (TC) , 34(4):318â€“325, 1985.\n[31] G. Moerkotte. Small Materialized Aggregates: A Light Weight Index Structure\nfor Data Warehousing. In Proceedings of the International Conference on Very\nLarge Data Bases (VLDB) , pages 476â€“487, 1998.\n[32] P. E. Oâ€™Neil, E. Cheng, D. Gawlick, and E. J. Oâ€™Neil. The log-structured merge-tree\n(LSM-tree). Acta Informatica , 33(4):351â€“385, 1996.\n[33] T. Palpanas. Data Series Management: The Road to Big Sequence Analytics. ACM\nSIGMOD Record , 44(2):47â€“52, 2015.\n[34] Y. Perl, A. Itai, and H. Avni. Interpolation Searchâ€”A log logN Search. Communi-\ncations of the ACM , 21(7):550â€“553, 1978.\n[35] R. Ramakrishnan and J. Gehrke. Database Management Systems . McGraw-Hill\nHigher Education, 3rd edition, 2002.\n[36] J. Rao and K. A. Ross. Making B+-trees Cache Conscious in Main Memory. In\nProceedings of the ACM SIGMOD International Conference on Management of Data ,\npages 475â€“486, 2000.\n[37] S. Sarkar, D. Staratzis, Z. Zhu, and M. Athanassoulis. Constructing and Analyz-\ning the LSM Compaction Design Space. Proceedings of the VLDB Endowment ,\n14(11):2216â€“2229, 2021.\n[38] U. Srivastava and J. Widom. Flexible Time Management in Data Stream Systems.\nInProceedings of the Twenty-third ACM SIGACT-SIGMOD-SIGART Symposium\non Principles of Database Systems, June 14-16, 2004, Paris, France , pages 263â€“274,\n2004.\n[39] K. Torp, L. Mark, and C. S. Jensen. Efficient Differential Timeslice Computation.\nIEEE Trans. Knowl. Data Eng. , 10(4):599â€“611, 1998.\n[40] TPC. Specification of TPC-H benchmark. http://www.tpc.org/tpch/ , 2021.\n[41] P. Van Sandt, Y. Chronis, and J. M. Patel. Efficiently Searching In-Memory Sorted\nArrays: Revenge of the Interpolation Search? In Proceedings of the ACM SIGMOD\nInternational Conference on Management of Data , pages 36â€“53, 2019.\n[42] Z. Zhu, J. H. Mun, A. Raman, and M. Athanassoulis. Reducing Bloom Fil-\nter CPU Overhead in LSM-Trees on Modern Storage Devices. http://disc-\nprojects.bu.edu/documents/DiSC-TR-Reducing-BF-Overhead-in-LSM.pdf , 2021.\n[43] K. Zoumpatianos, S. Idreos, and T. Palpanas. Indexing for interactive exploration\nof big data series. In Proceedings of the ACM SIGMOD International Conference on\nManagement of Data , pages 1555â€“1566, 2014.\n[44] K. Zoumpatianos, S. Idreos, and T. Palpanas. ADS: the adaptive data series index.\nThe VLDB Journal , 25(6):843â€“866, 2016.\n[45] K. Zoumpatianos and T. Palpanas. Data Series Management: Fulfilling the Need\nfor Big Sequence Analytics. In Proceedings of the IEEE International Conference\non Data Engineering (ICDE) , 2018.\n13",
  "textLength": 79685
}