{
  "paperId": "e09e4d81ba38de21cc7e7e9a3133bc38b132408c",
  "title": "Cardinality estimation with local deep learning models",
  "pdfPath": "e09e4d81ba38de21cc7e7e9a3133bc38b132408c.pdf",
  "text": " \n \n \n \n \nDieses Dokument ist eine Zweitveröffentlichung  (Postprint ) / \nThis is a self -archiving document  (accepted version ):  \n \n \n \n \n \n \n \n \n \n \nDiese Version ist verfügbar  / This version is available on :  \nhttps://nbn -resolving.org/urn:nbn:de:bsz:14 -qucosa2 -794673   \n \n \n \n \n \n  \nLucas Woltmann, Claudio Hartmann, Maik Thiele, Dirk Habich, Wolfgang  Lehner  \nCardinality Estimation with Local Deep Learning  Models  \n \nErstveröffentlichung in / First published  in: \nSIGMOD/PODS '19: International Conference on Management of Data , Amsterdam  \n05.06.2019 . ACM Digital Library , Art. Nr. 5 . ISBN 978-1-4503 -6802 -5. \nDOI: https://doi.org/ 10.1145/3329859.3329875   \n \n\nCardinality Estimation with Local Deep Learning\nModels\nLucas Woltmann, Claudio Hartmann, Maik Thiele, Dirk Habich, and Wolfgang\nLehner\n￿rstname.lastname@tu-dresden.de\nTechnische Universität Dresden\nDresden, Germany\nABSTRACT\nCardinality estimation is a fundamental task in database\nquery processing and optimization. Unfortunately, the accu-\nracy of traditional estimation techniques is poor resulting in\nnon-optimal query execution plans. With the recent expan-\nsion of machine learning into the ￿eld of data management,\nthere is the general notion that data analysis, especially neu-\nral networks, can lead to better estimation accuracy. Up to\nnow, all proposed neural network approaches for the cardi-\nnality estimation follow a global approach considering the\nwhole database schema at once. These global models are\nprone to sparse data at training leading to misestimates for\nqueries which were not represented in the sample space\nused for generating training queries. To overcome this is-sue, we introduce a novel local-oriented approach in this\npaper, therefore the local context is a speci￿c sub-part of the\nschema. As we will show, this leads to better representation\nof data correlation and thus better estimation accuracy. Com-\npared to global approaches, our novel approach achieves an\nimprovement by two orders of magnitude in accuracy and\nby a factor of four in training time performance for local\nmodels.\nCCS CONCEPTS\n•Information systems →Query optimization ;•Com-\nputing methodologies →Neural networks ;Supervised\nlearning by regression ;Ensemble methods .\n©2019 Copyright held by the owner/author(s). Publication rights licensed \nto ACM. This is the author’s version of the work. It is posted here for your \npersonal use. Not for redistribution. The definitive Version of Record was \npublished in aiDM’19, July 5, 2019, Amsterdam, Netherlands\nDOI: https://doi.org/10.1145/3329859.3329875ACM Reference Format:\nLucas Woltmann, Claudio Hartmann, Maik Thiele, Dirk Habich,\nand Wolfgang Lehner. 2019. Cardinality Estimation with LocalDeep Learning Models. In International Workshop on Exploiting\nArti￿cial Intelligence Techniques for Data Management (aiDM’19),July 5, 2019, Amsterdam, Netherlands. ACM, New York, NY, USA,\n8 pages. https://doi.org/10.1145/3329859.3329875\n1 INTRODUCTION\nQuery optimization is still an important challenge due to\never-increasing data sizes, whereby most query optimization\ntechniques are cost-based [ 2,19]. In this cost-based approach,\ncardinality estimation plays a dominant role with the task to\napproximate the number of returned tuples for every query\noperator within a query execution plan [ 2,19,21]. These es-\ntimations are used within di￿erent optimization techniques\nfor various decisions such as determining the right join or-\nder [3], choosing the optimal operator variant [ 27], or￿nding\nthe optimal placement within a heterogeneous hardware en-\nvironment [ 9,10]. For this reason, it is important to have\ncardinality estimations with high accuracy.\nUnfortunately, most traditional estimation approaches,\nwhich are based on statistical models with strong assump-\ntions, are not accurate enough [ 15]. Here, the main critical\nassumptions are uniformity and data independence [ 19]. For\nexample, the color redis usually uniformly distributed over\nall car brands but its distribution for the manufacturer Fer-\nrariis rather skewed since most of them are red. In this\ncase, the color and the manufacturer are highly correlated innon-uniform way leading to erroneous cardinality estimates\nusing traditional approaches.\nA promising way to overcome these limitations is the use\nof machine learning, including neural networks, for cardi-\nnality estimation [ 11,16,17,23]. Here, the main assumption\nis that a su￿ciently deep neural network can model the\nvery complex data dependencies and correlations. In this\nregard, available techniques are based on a global approach\nby creating a single neural network (global model ) over the\nentire database schema. This concept is detailed in Figure\n1, where Tables R,S,T,U,V, and Ware a complete schema\nFinal edited form was published in \"SIGMOD/PODS '19: International Conference on Management of Data. Amsterdam 2019\", Art. Nr. 5, ISBN 978-1-4503-6802-5 \nhttps://doi.org/10.1145/3329859.3329875\n1 \nProvided by Sächsische Landesbibliothek, Staats- und Universitätsbibliothek Dresden\n\naiDM’19, July 5, 2019, Amsterdam, Netherlands Woltmann et al.\nU\nS R TV W\nContext of global model Context of local modelRelationship\nNeural network structure Neural network structure\nFigure 1: Global vs. local approach.\nin a database. The resulting global model for cardinality es-\ntimation can become large in structure to model as many\naspects of the schema as possible and arbitrary cardinality\nestimates are possible. The global model is trained on sam-\npled queries from the entire schema. However, this leads to\nsparse query sampling because even for a limited number\nof tables and predicates, the number of possible training\nqueries becomes extremely large. For example, our six tables\nwith￿ve columns each containing 1,000 possible values and\nthree possible predicate operators ( <,=,>) would generate\na sample space of (26\u00001)·25·1,000 ·3=6,048,000queries.\nOur Contribution and Outline\nTo tackle this issue, we propose a cardinality estimation tech-\nnique with a focus on smaller neural network structures in\nthis paper. We call this a local approach because each lo-\ncal model focuses on a small part of the schema instead of\nthe whole schema. An example for a model on a sub-partof a schema (i.e. a single join) is depicted in Figure 1. Theschema sub-part for a local model can be any number andcombination of joins. That means, each local model is al-\nways specialized to a speci￿c schema sub-part. Without loss\nof generalization, we focus on equi-joins in this paper. The\nadvantage of our local approach is that the query sampling\ngets less sparse. Given our example from before concentrat-\ning on the join between RandT, we get a sample space of\n(22\u00001)·25·1000 ·3=288,000queries. Thus, the coverage of a\nsample would increase because the same amount of sampledqueries would be less sparse for the local sample space. If we\nsample 100,000 queries in both sample spaces, we cover ca.\n1.6% of the global sample space but ca. 35% of the local sam-\nple space. Sampling 100,000 queries needs the same amountof time in both scenarios but the coverage of seen queries\nwith di￿erent cardinalities is higher for our local approach.\nThis gives a learned model the possibility to generalize its\nprediction making it more accurate.\nIn detail, we make the following contributions in this\npaper: The contributions of our work are the following:\n•In Section 2, we outline the current research context\nof our approach.\n•We introduce an approach for learned cardinalities\nwith local models, i.e. neural networks in Section 3.\n•Section 3 also provides details on our vectorization pro-\ncess transforming SQL queries into numerical vectors\nto be utilizable by the neural network as input.\n•We provide a comprehensive evaluation in Section 4.\nIn particular, we show that our local approach shows\nan accuracy improvement by two orders of magnitude\nand a speed-up by three orders of magnitude for the\nforward pass compared to a global model.\nFinally, we close the paper with an outlook onto our future\nresearch in Section 5 and a brief summary in Section 6.\n2 RELATED WORK\nTraditional approaches of cardinality estimation in relational\ndata management systems (RDBMS) rely on statistics that in-\nclude one-dimensional equi-depth or equi-width histograms\non each column in a table [ 5–7,25], a list of most frequent\nvalues and their frequencies, the number of distinct values,\nand min and max values [ 24]. However, the main assump-\ntions of uniformity and data independence have shown to\nbe a major problem for these statistical approaches leading\nto non-optimal query execution plans in the end [15, 26].\nTo solve this issue in a more sophisticated way, there have\nbeen works on formulating cardinality estimation as a su-\npervised learning problem [ 11,14,16]. On the one hand, the\nauthors in [ 16] proposed learned cardinality estimators for\nsingle tables only. On the other hand, Kipf et al. introduced\na universal approach called multi-set convolutional neural\nnetwork (MSCN) which models cardinality estimation as a\nglobal model. The neural network processes three inputs\nfrom an SQL query independently. These are the used tables,\nthe join keys, and the chosen predicates on the used tables.\nAdditionally, MSCN uses samples of the ￿rst 1,000 rows of a\nquery as a bitmap given the truth values of the query’s predi-cates. MSCN is capable of modeling joins and predicates overseveral tables and hence can cover correlations in the data. A\nvery complex network structure estimates the cardinality of\nthe given query. The large sampling space to cover the whole\nschema leads to a sparse representation of di￿erent queries\nand their cardinalities. If a query without a representative is\npassed to the network, the network’s interpolation capability\nfails and the estimate is erroneous. The complex network\nFinal edited form was published in \"SIGMOD/PODS '19: International Conference on Management of Data. Amsterdam 2019\", Art. Nr. 5, ISBN 978-1-4503-6802-5 \nhttps://doi.org/10.1145/3329859.3329875\n2 \n \n \nProvided by Sächsische Landesbibliothek, Staats- und Universitätsbibliothek Dresden\n\nCardinality Estimation with Local Deep Learning Models aiDM’19, July 5, 2019, Amsterdam, Netherlands\nSELECT *\nFROM R, SWHERE R.id = S.id\nAND R.p1 = 1\n...\n5320.23010\n...Vectorization\n...... 526\nHidden\nlayersInputvectorInputlayerOutputlayerCardinalityestimateDatabase Truecardinality\nEvaluation\nFigure 2: Process for cardinality estimation with learned models.\nstructure also results in an increased learning time of the\nmodel and slower cardinality estimation itself.\nBesides cardinality estimation, machine learning tech-\nniques have been also used to optimize other RDMS internals.\nFor example, reinforcement learning has been applied for\nquery optimization as well as to solve the join ordering chal-\nlenge [ 17,23]. Moreover, Kraska et al. proposed a learned\nindex structure as novel indexing approach [ 12]. Here, a\nmodel learns the sort order or the structure of lookup keys\nand uses these signals to e￿ectively predict the position or\nexistence of tuples.\n3 LOCAL CARDINALITY ESTIMATION\nAs already demonstrated by [ 11,14,16], the application of\ndeep learning techniques to the cardinality estimation task\nenhances the accuracy compared to traditional approaches.\nNevertheless, all recently introduced approaches have short-\ncomings in terms of accuracy, network size or e￿ort for train-ing as mentioned in the previous section. To overcome theseissues, we propose a novel local model approach in this paper.\nOur local model approach is characterized by the fact that\nwe build di￿erent neural networks (models) for various sub-\nparts of the database schema instead of having one globalneural network for the whole schema at once. Speci￿cally,\nwe build our models at the granularity of n-ary joins and\ntheir corresponding ￿lter predicates that occur in a given\nworkload. Without loss of generality, we restrict ourselves\nto equi-joins at the moment with di￿erent ￿lter predicates\nopportunities.\nThe exact structure of the network, i.e. number of layers\nand the number of neurons, is subject to the adaption of our\nlocal model to the problem, also known as hyperparameter\ntuning. We explain these structural properties of the neural\nnetwork and their in￿uences in Section 4.3 as part of the\nevaluation. In machine learning, cardinality estimation canbe seen as a regression problem using SQL queries with\nrelation and attribute constraints as input and the cardinality\nas the objective. A machine learning based estimator takesany vectorized query as input and returns a cardinality for\nsaid query. This assumption holds for both global and local\nmodels. In Section 3.1, we detail how a neural network can beused for regression. The featurization of queries is described\nin Section 3.2.\n3.1 Regression with Neural Networks\nA standard way for modeling regression with neural net-\nworks are multi-layer perceptrons (MLP). This kind of neu-\nral network is de￿ned by three types of layers: an input\nlayer, hidden layers, and an output layer. Figure 2 shows an\nexample of such a neural network. Each layer contains acon￿gurable but ￿xed number of neurons. Each neuron is\nconnected to all neurons in both the previous and follow-\ning layer. Therefore, these layers are called fully-connected.\nWhen applied to regression, MLPs use an n-dimensional vec-\ntor as input and produce a ￿oating point or an integer number\nas output. The input vector dictates the numbers of neurons\nin the input layer of the neural net. The output of a regression\nneural network is a scalar. While training, example queries\nwith known cardinalities will be passed through the network.\nSuch example queries are called the ground truth. The neural\nnetwork self-optimizes its prediction of the output (i.e. car-\ndinalities) based on the ground truth. Once trained, a neural\nnetwork can be used to estimate cardinalities for both known\nqueries which were already in the workload or unknownqueries which are newly introduced to the workload. Our\napproach uses such a regression neural network. The input\nvector is the vectorized query (see Section 3.2) and the output\nis the estimated cardinality of the query.\nFinal edited form was published in \"SIGMOD/PODS '19: International Conference on Management of Data. Amsterdam 2019\", Art. Nr. 5, ISBN 978-1-4503-6802-5 \nhttps://doi.org/10.1145/3329859.3329875\n3 \n \n \nProvided by Sächsische Landesbibliothek, Staats- und Universitätsbibliothek Dresden\n\naiDM’19, July 5, 2019, Amsterdam, Netherlands Woltmann et al.\nSELECT * FROM R, S WHERE R.id = S.id AND R.p1 = 1 AND S.p2 > 100\n0 1 0 0.23 0 0 1 0.5 0 0 0 0 Input vector Æx=predicate p3not in query\n< = > ˜\u00001 < = > ˜\u00002 < = > |\u00003|\nFigure 3: Vectorization of a query\n3.2 Vectorization\nIn order to model a join and the resulting correlations in the\ndata, we need to instantiate the given join in the database.\nThis generates a join table of width nwhere n=#cols (R)+\n#cols (S). These assumptions also hold for more complex joins\nover three or more tables.\nA neural network can only take numerical values as its\ninput vector. An SQL query has to be transformed from its\nstring representation to a numerical vector. This process\nis called vectorization. We use the predicates on the two\ntables on which the join is based as a foundation for our\nvectorization. Every possible predicate pi2{p1,..., pn}on\nthose two tables generates 4 entries in the input vector Æx2\nR4n. We are only allowing selections on non-key predicates\nbecause we argue that there are no useful selections on key\npredicates in our scenario.\nWe di￿erentiate between two encodings in our vector-\nization: the operator encoding and the value encoding. The\noperator encoding vectorizes the choice of operator for the\npredicate pi. We use one-hot encoding to transform a number\nof choices to a vector. With three singular operators <,=,\nand >, we need a vector of length three to model all possible\noperators. The presence of an operator dictates a 1 at the cor-responding position in the vector. We have chosen to use the\norder (<,=,>)for our purposes. For example, <generates\nthe vector (1,0,0)and <=generates the vector (1,1,0). The\noperator encoding takes up the ￿rst three entries for each\npredicate. Next, we need to encode the chosen value for the\npredicate pi. Usually, this is the numeric or character value\n\u0000ion the right hand side of the operator. The value encoding\nis a single ￿oating point number representing this value. If\nthis value is non-numeric, we use dictionary encoding to\ntransform it to an integer. Neural networks are usually used\nwith min-max-normalized input vectors ranging from 0 to 1\nto enhance their accuracy [ 8]. We normalize our predicate\nvalue \u0000ito this range as shown in Equation (1).\n˜\u0000i=\u0000i\u0000min( pi)\nmax( pi)\u0000min( pi)(1)\nThe minimum and maximum values are the boundaries of\nthe range of the predicate piwhere \u0000iis the value of piin\nthe query. These can be obtained directly from the database.For the purpose of our model, we only take those predicates\ninto account which select directly on the join table.\nFrom Figure 2, we assume that the join of RandShas\nthree columns {p1,p2,p3}excluding the join predicate. The\nvectorization generates an input vector Æxof length 12, four\nentries for each predicate. Note that the predicates are chosen\nfrom di￿erent relations, i.e. p1comes from Rand p2and p3\ncome from S. Figure 3 shows the encoding for the example\nquery. We mark the direct translation of query predicates to\nparts of the input vector.\n4 EVALUATION\nWe conduct an experimental study to evaluate the perfor-\nmance of our approach. We begin with the experimentalsetup (Section 4.1), including the evaluation data and thecomparison techniques. This is followed by a detailed de-\nscription of the experiments and the discussion of their re-\nsults.\nThe process of generating data according to the experi-\nmental setup of Kipf et al. is described in Section 4.1. We\ncompare our results to two other estimators, one traditional\nestimator and one learned estimator, in Section 4.2. In Sec-\ntion 4.3, we evaluate di￿erent con￿gurations of our networktopology. We perform experiments on how many queries arenecessary for a stable model in Section 4.4. Last, we show theperformance of all tested models and compare their training\ntime and test time in Section 4.5.\n4.1 Experimental Setup\nThe base for our evaluation is the IMDB data set1. We fo-\ncused on two joins: (1) title ./movie_info and (2) movie_\ncompanies ./movie_info ./title . The￿rst join results in ap-\nproximately 29 million tuples and the second one in 134million tuples. Further information about the data can be\nfound in Table 1. All properties are chosen in order to ensure\ncomparability to other approaches. We selected all columns\nfrom each join which can be represented as integers. All\nexperiments are executed ￿ve times and their results are av-\neraged. For each of the ￿ve complete training runs in every\nexperiment, we randomly split our data set of 105,000 queries\nin 90,000 queries for training, 10,000 queries for validation,\n1ftp://ftp.fu-berlin.de/pub/misc/movies/database/frozendata/\nFinal edited form was published in \"SIGMOD/PODS '19: International Conference on Management of Data. Amsterdam 2019\", Art. Nr. 5, ISBN 978-1-4503-6802-5 \nhttps://doi.org/10.1145/3329859.3329875\n4 \n \n \nProvided by Sächsische Landesbibliothek, Staats- und Universitätsbibliothek Dresden\n\nCardinality Estimation with Local Deep Learning Models aiDM’19, July 5, 2019, Amsterdam, Netherlands\ntable column number of\ndistinct values\ntitle kind_id 8\ntitle production_year 145\nmovie_info info_type_id 78\nmovie_companies company_id 362,131\nmovie_companies company_type_id 2\nTable 1: Properties of IMDB tables and columns.\nand 5,000 queries for testing. All sets are disjunctive from\neach other. This gives a good assessment of the model’s gen-\neralization and prevents over￿tting. Note that in real-world\nscenarios, there can be an overlap between training and test\nsets but this would only improve the performance of our\nneural network. The neural network sees di￿erent queries\nand cardinalities each training run. Thus, we prevent it from\njust remembering speci￿c queries through repetition.\nAs the evaluation metric, we choose the q-error or multi-\nplicative error [ 20]. This measurement is the factor of mis-\njudgment between the ground truth value \u0000true and the\nestimated value \u0000est. It ranges from 1 for \u0000est=\u0000trueto1\nfor misestimates because it has no upper limit.\nq(\u0000true,\u0000est)=max( \u0000true,\u0000est)\nmin(\u0000true,\u0000est)(2)\nWe use the sampling technique for the generation of queries\nas proposed in [ 11]. We only need to sample the predicates\nbecause the number of joins is ￿xed for each of our neural\nnetwork. For this, we sample the number of predicates luni-\nformly from [1,pn]with l2N. This is done coherently to\nKipf et al. We use the same operator set ( <,=,>) as well.\nEvery generated query is executed through a database to\nobtain the correct cardinality. We generate a total of 105,000\nqueries for each join table. This is a strict subset of the sam-\npling space proposed by Kipf et al. This set of queries only\nassesses the example joins but they are still covered in the\noriginal sample space. So, the global MSCN should be able to\nmodel mean median max 95th mean\nq-error q-error q-error q-error\n2-way joinPosgres 212,311.8 62.6 23,913,682.0 638,227.4MSCN 1,353.8 3.4 194,170.7 1,973.6Local NN 4.9 1.4 1,569.7 11.7\n3-way joinMSCN 4748.5 5.0 1315521.7 2041.1Local NN 26.4 1.9 24304.8 32.4\nTable 2: Resulting q-errors of di￿erent approaches.\nFigure 4: Evaluation results of di￿erent cardinality es-\ntimators.\ngeneralize from its training data to estimate the cardinalities\nof these queries.\nAll experiments are executed on an AMD A10-7870K Ra-\ndeon R7 with 32GB RAM. All neural networks, includingnetworks from other publications, are trained with CUDA\ncapability on an NVIDIA Tesla K20c with keras2.\n4.2 Accuracy\nTo evaluate the estimation accuracy of our local model, we\ncompare our approach with two other techniques. First, we\nselect a baseline estimator which does not use machine\nlearning. Postgres uses traditional estimations based on his-\ntograms or most frequent values3. This approach is not ca-\npable to model correlations between predicates because it\nassumes data independence. Next, we choose multi-set con-\nvolutional neural network (MSCN) as a state-of-the-art ap-\nproach using deep learning for cardinality estimation. The\nMSCN model is trained on the data set from the original\npublication. These queries are sampled from the same IMDB\ntables, we use throughout our evaluation. Last, we train our\nlocal estimator on the sampled queries mentioned in Sec-tion 4.1. For this, we choose the network structure whichperforms best on our data sets. An evaluation of di￿erent\ntested structures can be found in Section 4.3.\nFigure 4 and Table 2 detail the accuracy results for all\nestimators on join (1) and (2). The q-error of each estimate\nin our test set is scaled to a symmetric log-space on they-axis. Whereas underestimates get a negative value and\n2https://keras.io\n3https://www.postgresql.org/docs/10/row-estimation-examples.html\nFinal edited form was published in \"SIGMOD/PODS '19: International Conference on Management of Data. Amsterdam 2019\", Art. Nr. 5, ISBN 978-1-4503-6802-5 \nhttps://doi.org/10.1145/3329859.3329875\n5 \n \n \nProvided by Sächsische Landesbibliothek, Staats- und Universitätsbibliothek Dresden\n\naiDM’19, July 5, 2019, Amsterdam, Netherlands Woltmann et al.\noverestimates a positive value, both between 1 and 1. The\nblue line is the tendency of an estimator to overestimate or\nunderestimate. The box for each model details the con￿dence\ninterval (CI) around the median q-error. The number of q-\nerrors which are outside the CI but inside 1.5 times the CI\nare represented with whiskers. Outliers are plotted as points\noutside the whiskers. The closer all elements are around\nthe q-error range between \u00001and1the better the accuracy\nof a model. Table 2 shows the mean q-error, the median q-\nerror, the maximum q-error, and the mean q-error of the 95th\npercentile. All q-errors refer to the accuracy of the models\non 5,000 test queries.\nIt can be observed that our model can estimate data cor-\nrelation much more precisely. We gain an improvement in\naccuracy of four orders of magnitude to traditional estima-\ntors and a factor of 275 to the global model regarding the\nmean q-errors from Table 2. With a smaller focus, our neural\nnetwork has the advantage of using more of its estimation\nquality on queries which access similar data. The neural net-\nwork does not need to generalize over a large data context\nlike a complete schema but learns local data correlations\nwhich are easier to model in general. This leads to two im-\nprovements as shown by the ￿iers and whiskers in Figure 4:\n1) Our model is not as susceptible to misestimates. 2) The\nvariance in our estimates is much smaller.\nThe assumption that MSCNs can generalize from a general\nworkload of sample queries to a speci￿c one does not hold.\nBy using our local approach di￿erent joins can be evaluated\nwith di￿erent specialized local models (see Section 5). This\nalso allows for using other approaches such as histograms orsketches where a neural network would introduce too much\noverhead.\nOur model can not only be used for joins over two tables\nbut also for an arbitrary number of tables. To show the accu-racy of a larger join, we use join (2) mentioned in Section 4.1.\nThe last two box plots in Figure 4 and the last lines in Table 3\nshow the performance of MSCN and our model on join (2).\nThe accuracy improves by a factor of 180 compared to the\nglobal approach when comparing the mean q-errors. Note\nthat this model is trained on only 50,000 sample queries as\nmotivated in Section 4.4. This shows, that our approach can\nbe generalized to n-way joins be keeping higher accuracy\ncompared to global models.\n4.3 Network Structure\nThere are two main components which have the most signif-\nicant in￿uence on the accuracy of a neural network: width\nand depth. The width of a fully-connected neural network\nis the number of neurons in the ￿rst hidden layer4. The\ndepth describes the number of consecutive hidden layers\n4All following layers have half the number of neurons as their predecessor.\nFigure 5: Neural network structure evaluation.\nin the network. To tune the network for our purposes, we\nuse an exhaustive grid search with depth and width as the\nsearch space axes. The width varies between the values\n(32,64,128,256,512 )and the depth varies between the values\n(1,2,3,4,5). For each combination of a given depth and width,\na network is instantiated with these parameters, trained on\nthe train data with 100 epochs and tested on the test data.\nWe have chosen the 2-way join (1) for this experiment with\npredicates on three attributes depicted in Table 3. Figure 5\nshows the q-errors for all network combinations. The x-axis\ndescribes the depth parameter, whereas the y-axis details the\nwidth parameter. It is clearly visible that the best combina-\ntion is a network with two hidden layers and 512 neurons in\nthe￿rst layer. Another result is that width has more impact\non the q-error than the depth. Shallow models with broad\nlayers perform better than deep models with narrow layers\nup to a factor of ten. For all other experiments, we therefore\ndecide to use a network with the following con￿guration: in-\nput layer (4 nneurons), hidden layer 1 (512 neurons), hidden\nlayer 2 (256 neurons), output layer (1 neuron).\n4.4 Number of Sampled Queries\nThe cold start problem in machine learning means that if a\nsystem is freshly installed, a lot of training data is required for\na￿rst model. Most of the time, one does not have a su￿cient\nnumber of samples for training in this scenario and compute\nexpensive data generation (see Section 4.1) is required. Indata management and especially in cardinality estimationthis usually leads to random sampling of example queries\nrepresenting an arti￿cial workload. To mitigate the cold start\nproblem, we assess how fast our network converges given a\nFinal edited form was published in \"SIGMOD/PODS '19: International Conference on Management of Data. Amsterdam 2019\", Art. Nr. 5, ISBN 978-1-4503-6802-5 \nhttps://doi.org/10.1145/3329859.3329875\n6 \n \n \nProvided by Sächsische Landesbibliothek, Staats- und Universitätsbibliothek Dresden\n\nCardinality Estimation with Local Deep Learning Models aiDM’19, July 5, 2019, Amsterdam, Netherlands\nFigure 6: Number of samples needed.\nspeci￿c number of input queries from the workload as train-\ning data. We look at the number of required queries for an\narti￿cial start-up workload to result in a stable estimation for\njoin (1). The number of queries used for training is increasedby 10,000 in each step and a sample of 5,000 queries is chosen\nfrom the remaining queries for testing. The neural network\nis trained with a batch size of 32. The results are shown in\nFigure 6.\nFirst of all, the runtime of a neural network in training\nscales linearly with the number of training queries if param-\neters like width, depth and batch size are kept constant on a\nsingle machine. It shows that we reach a mean q-error of 6.9\nwhen using 50% of queries. We argue that therefore 50,000\nqueries are enough to build a ￿rst robust model for this join.\nThis leaves us with 50% of runtime for model training (i.e.\n598s) compared to the full sample set.\n4.5 Runtime Performance\nSince performance is a key feature for using learned car-dinality estimators in database management systems, wedetail the runtime evaluation of all models. For evaluating\nthe runtimes of di￿erent models, we use join (1) and 105,000\nsampled queries accessing this join. The experiment is car-\nried out as described in Section 4.1. Additionally, our model\nis also trained with the number of samples necessary for a\nstable model (see Section 4.4). The MSCN and our model are\ntrained with GPU capability and a batch size of 32.\nTable 3 details the training and test time for all model-\nbased approach and the runtime for the Postgres query ana-\nlyzer. The training time consists of the 100 epochs needed for\neach model to be ￿tted to the 100,000 train queries. The test\ntime is the evaluation time for a single query, i.e. the time\nfor the EXPLAIN ANALYZE statement for Postgres or the time\nfor a forward pass through the neural network. One couldModel Training time Testing time\n(per sample)\nPostgres – 1.8s\nMSCN 4945s 33ms\nLocal NN (full sample set) 1159s 29￿s\nLocal NN (su￿cient samples) 598s 29￿s\nTable 3: Runtime performance of models.\nargue that histogram construction in Postgres is equivalent\nto a training process but it uses a di￿erent type of data, the\ndata distribution of each attribute. Therefore, a comparison\nwould be insu￿cient.\nFor model training, we can achieve a speed-up of factor\nfour and a factor of eight if only a reduced number of queriesis used. Our neural network estimator is faster than Postgres\nby a factor of 62,000. Compared to MSCN, the forward pass\nof the local model is three orders of magnitude faster since\nthe structure is much smaller. When we apply our modelis fast enough to have little to zero impact on the query’s\nexecution time.\n5 FUTURE WORK\nThe approach of using neural network for cardinality estima-\ntion is still in its infancy and there is a multitude of challengesahead that needs to be solved. The most important issues that\nhave to be tackled are the cold start problem, the number of\nsamples needed to train the network, the maintenance of the\nmodels, and their interplay and integration with statistical\napproaches based on histograms.\nCurriculum Learning. We want to mitigate the cold\nstart problem for cardinality estimation by applying cur-\nriculum learning. The basis notion is that in human learningthe supervision often follows a curriculum where the teacher\npresents the examples not randomly but in a speci￿c order.\nThe approach of curriculum learning [ 1] transfers this prin-\nciple to supervised learning. It is an extension of stochastic\ngradient descent where easy examples are over-sampled at\nthe beginning of training. This gives a higher probability to\nescape a low quality local minimum, since the variance ofthe gradient direction increases with the di￿culty of sam-\nples [ 28]. In [ 13,28] it was shown, that by applying curricu-\nlum learning the training converges much faster but also\nimproves the generalization performance of the learned mod-\nels. For learning cardinality estimations, the key question is\nhow to rank samples according to their di￿culty. This can be\ndone either by using the estimation error of the underlying\ndatabase, by investigating existing models or by using opera-tor embeddings [\n18]. Operator embeddings [ 18] map a query\noperator to a low-dimensional vector space that capturesmuch information about the operator. The authors of [\n18]\nshowed that operator embeddings can be used to classify\nFinal edited form was published in \"SIGMOD/PODS '19: International Conference on Management of Data. Amsterdam 2019\", Art. Nr. 5, ISBN 978-1-4503-6802-5 \nhttps://doi.org/10.1145/3329859.3329875\n7 \n \n \nProvided by Sächsische Landesbibliothek, Staats- und Universitätsbibliothek Dresden\n\naiDM’19, July 5, 2019, Amsterdam, Netherlands Woltmann et al.\noperators for which the query optimizer’s cardinality esti-\nmate is correct, too high, or too low. This could be used as\nan approximation of the di￿culty.\nModel Maintenance. Currently, we are assuming that\nour database schema as well as the underlying data is static.\nWhile the learned cardinality estimation models should be\ngeneral enough to tolerate minor shifts in data distribution\nand correlation, there will also be cases where these models\nneed to be adapted or learned from scratch. Therefore, we\nneed to monitor the cardinality estimation error and trigger\nmodel maintenance if needed. The notion of model main-\ntenance shares some aspects with the approach of transfer\nlearning. Transfer learning is the improvement of learning in\na new task through the transfer of knowledge from a related\ntask that has already been learned [ 22]. The core question\nthat has to be investigated is whether it is possible to transfer\nthe knowledge from a given model to another setup with\ndi￿erent data characteristics.\nIntegration with Statistical Approaches. Moreover, we\nwant to combine existing statistical approaches with learned\nmodels. Since the computation of histograms is pretty cheap\ncompared to learning neural networks, they provide a plain-\nvanilla approach that could be used to mitigate the cold\nstart problem. Additionally, they remain applicable for all\ncases where the independence assumption holds. To support\na decision-making process in choosing either statistical or\nneural-network approaches, we plan to discover functional\ndependencies and correlations between pairs of column [ 4]\n6 CONCLUSION\nIn this paper, we present a neural network approach for car-\ndinality estimation that focuses on local models instead of\nglobal models. The evaluation shows that our local models\noutperform global models in terms of accuracy as well astraining and testing time. Beside this signi￿cant improve-\nment, we believe that local models have the advantage to be\nmuch easier to maintain when it comes to drifts in data or\nschema and workload changes compared to global models\nREFERENCES\n[1]Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston.\n2009. Curriculum Learning. In ICML. 41–48.\n[2]Nicolas Bruno and Surajit Chaudhuri. 2002. Exploiting statistics on\nquery expressions for optimization. In SIGMOD. 263–274.\n[3]Pit Fender and Guido Moerkotte. 2011. A new, highly e￿cient, and\neasy to implement top-down join enumeration algorithm. In ICDE.\n864–875.\n[4]Ihab F Ilyas, Volker Markl, Peter Haas, Paul Brown, and Ashraf Aboul-\nnaga. 2004. CORDS: Automatic Discovery of Correlations and Soft\nFunctional Dependencies. In SIGMOD. 647–658.\n[5]Yannis E Ioannidis and Stavros Christodoulakis. 1993. Optimal His-\ntograms for Limiting Worst-case Error Propagation in the Size of Join\nResults. ACM Trans. Database Syst. 18, 4 (Dec. 1993), 709–748.[6]Yannis E Ioannidis and Viswanath Poosala. 1995. Balancing Histogram\nOptimality and Practicality for Query Result Size Estimation. In SIG-\nMOD. 233–244.\n[7]H V Jagadish, Nick Koudas, S Muthukrishnan, Viswanath Poosala,\nKen Sevcik, and Torsten Suel. 1998. Optimal Histograms with Quality\nGuarantees. In VLDB. 275–286.\n[8]T Jayalakshmi and A Santhakumaran. 2011. Statistical normalization\nand back propagation for classi￿cation. IJCTE 3, 1 (2011), 1793–8201.\n[9]Tomas Karnagel, Dirk Habich, and Wolfgang Lehner. 2015. Local vs.\nGlobal Optimization: Operator Placement Strategies in Heterogeneous\nEnvironments. In EDBT/ICDT Workshops. 48–55.\n[10] Tomas Karnagel, Dirk Habich, and Wolfgang Lehner. 2017. Adaptive\nWork Placement for Query Processing on Heterogeneous Computing\nResources. PVLDB 10, 7 (2017), 733–744.\n[11] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter A.Boncz, and Alfons Kemper. 2019. Learned Cardinalities: Estimating\nCorrelated Joins with Deep Learning. In CIDR.\n[12] Tim Kraska, Alex Beutel, Ed H Chi, Je￿rey Dean, and Neoklis Polyzotis.\n2018. The case for learned index structures. In SIGMOD. 489–504.\n[13] Kai A Krueger and Peter Dayan. 2009. Flexible shaping: How learning\nin small steps helps. Cognition 110, 3 (2009), 380 – 394.\n[14] Seetha Lakshmi and Shaoyu Zhou. 1998. Selectivity estimation in\nextensible databases-A neural network approach. In VLDB. 623–627.\n[15] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons\nKemper, and Thomas Neumann. 2015. How good are query optimizers,\nreally? PVLDB 9, 3 (2015), 204–215.\n[16] Henry Liu, Mingbin Xu, Ziting Yu, Vincent Corvinelli, and CalistoZuzarte. 2015. Cardinality Estimation Using Neural Networks. In\nCASCON. 53–59.\n[17] Ryan Marcus and Olga Papaemmanouil. 2018. Deep reinforcement\nlearning for join order enumeration. In aiDM. ACM, 3.\n[18] Ryan Marcus and Olga Papaemmanouil. 2019. Flexible Operator Em-\nbeddings via Deep Learning. arXiv:1901.09090 (2019).\n[19] Volker Markl, Vijayshankar Raman, David E. Simmen, Guy M. Lohman,and Hamid Pirahesh. 2004. Robust Query Processing through Progres-\nsive Optimization. In SIGMOD. 659–670.\n[20] Guido Moerkotte, Thomas Neumann, and Gabriele Steidl. 2009. Pre-\nventing bad plans by bounding the impact of cardinality estimation\nerrors. VLDB 2, 1 (2009), 982–993.\n[21] Ravi Mukkamala and Sushil Jajodia. 1991. A Note on Estimating the\nCardinality of the Projection of a Database Relation. ACM Trans.\nDatabase Syst. 16, 3 (1991), 564–566.\n[22] E S Olivas, J D M Guerrero, M M Sober, J R M Benedito, and A J S\nLopez. 2009. Handbook Of Research On Machine Learning Applications\nand Trends: Algorithms, Methods and Techniques.\n[23] Jennifer Ortiz, Magdalena Balazinska, Johannes Gehrke, and S Sathiya\nKeerthi. 2018. Learning state representations for query optimization\nwith deep reinforcement learning. arXiv:1803.08604 (2018).\n[24] G Piatetsky-Shapiro and C Connell. 1984. Accurate Estimation of the\nNumber of Tuples Satisfying a Condition. In SIGMOD. 256–276.\n[25] Viswanath Poosala, Peter J Haas, Yannis E Ioannidis, and Eugene JShekita. 1996. Improved Histograms for Selectivity Estimation of\nRange Predicates. In SIGMOD. 294–305.\n[26] Viswanath Poosala and Yannis E Ioannidis. 1997. Selectivity estimation\nwithout the attribute value independence assumption. In VLDB, Vol. 97.\n486–495.\n[27] Viktor Rosenfeld, Max Heimel, Christoph Viebig, and Volker Markl.2015. The Operator Variant Selection Problem on Heterogeneous\nHardware. In ADMS@VLDB. 1–12.\n[28] Daphna Weinshall, Gad Cohen, and Dan Amir. 2018. Curriculum\nLearning by Transfer Learning: Theory and Experiments with Deep\nNetworks. arXiv:1802.03796 (2018).\nFinal edited form was published in \"SIGMOD/PODS '19: International Conference on Management of Data. Amsterdam 2019\", Art. Nr. 5, ISBN 978-1-4503-6802-5 \nhttps://doi.org/10.1145/3329859.3329875\n8 \n \n \nProvided by Sächsische Landesbibliothek, Staats- und Universitätsbibliothek Dresden",
  "textLength": 40145
}