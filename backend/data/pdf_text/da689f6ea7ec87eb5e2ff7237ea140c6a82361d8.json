{
  "paperId": "da689f6ea7ec87eb5e2ff7237ea140c6a82361d8",
  "title": "Learned Indexes for a Google-scale Disk-based Database",
  "pdfPath": "da689f6ea7ec87eb5e2ff7237ea140c6a82361d8.pdf",
  "text": "Learned Indexes for a Google-scale Disk-based\nDatabase\nHussam Abu-Libdeh\nGoogleDeniz Altınbüken\nGoogleAlex Beutel\nGoogleEd H. Chi\nGoogleLyric Doshi\nGoogle\nTim Kraska\nMIT, GoogleXiaozhou (Steve) Li\nGoogleAndy Ly\nGoogleChristopher Olston\nGoogle\u0003\nAbstract\nThere is great excitement about learned index structures, but understandable skep-\nticism about the practicality of a new method uprooting decades of research on\nB-Trees. In this paper, we work to remove some of that uncertainty by demon-\nstrating how a learned index can be integrated in a distributed, disk-based database\nsystem: Google’s Bigtable. We detail several design decisions we made to inte-\ngrate learned indexes in Bigtable. Our results show that integrating learned index\nsigniﬁcantly improves the end-to-end read latency and throughput for Bigtable.\n1 Introduction\nThere has been a lot of excitement about the potential beneﬁts of using machine learning methods\nin databases, but understandable skepticism about the practicality of these new methods for real\napplications. Two years ago learned index structures [ 10] were proposed as a novel technique to\nreplace B-Trees and similar range index structures. Learned index structures use a learned CDF-\nmodel instead of the traditional tree-structures to achieve signiﬁcantly faster lookup times and index\nspace savings. While there has since been a ﬂurry of follow up research, the impact of the work on\nreal-world systems has been under question. The most common points of criticism include, that (1)\nthe original learned index didn’t support inserts, (2) wasn’t designed for disk-based systems, where\nindexes matter the most, and, more generally, (3) the index size or speed improvement do not really\nmatter in practice as it only makes up a small fraction of the entire query execution time. While\nsupport for inserts has already been addressed by follow-up work [ 4,6,9], the question of whether\nlearned indexes can have an impact on real, disk-based systems remained unaddressed.\nIn this paper, we present initial results on integrating learned index structures into Google’s Bigtable\n[1,2] and show that they can signiﬁcantly improve the read latency and throughput, especially\nat the tail. This is a surprising result as Bigtable is a distributed and disk-based system that has\nbeen optimized for high-throughput and low-latency reads and writes. Requests usually take tens\nto hundreds of milliseconds rather than nanoseconds. How can a learned index structure, which\nimproves request times by nanoseconds in [ 10], have a positive impact on the overall latency of such\na system? While the faster lookup time has little impact, we see big performance gains from the\nsecond-order effects of using a learned index, such as reducing the index size, requiring fewer index\nseeks, reading fewer index blocks, and simplifying data block prefetching.\nBigtable uses a traditional B-Tree to determine which key is stored in which block. Hash maps are\nnot an option in Bigtable as the system has to support range scans. Like many database systems,\nthe B-Tree itself is also stored in the same block structure and can consist of several blocks. As a\nresult, a request for a key might involve several block reads; usually 1-2 to read the index blocks\nand then 1 more to read the block with the data. Caching can signiﬁcantly reduce the number of\nblocks to fetch from the distributed ﬁle system, especially the index blocks. However, in practice\n*Work done while at Google, although the author currently does not work at Google.\nWorkshop on ML for Systems at NeurIPS 2020, Vancouver, Canada.arXiv:2012.12501v1  [cs.DB]  23 Dec 2020\n\n(1) Bigtable instances can become really big, so big that the index might not ﬁt into main memory,\n(2) machines might host several Bigtable instances, reducing the amount of available memory, (3)\nfailures and load-balancing might clear the cache, and (4) some Bigtable instances are both large and\nrarely used. All these factors create pressure on the cache and can cause index blocks to be fetched\nagain. Reducing the index size by orders of magnitude can have an overall positive impact, especially\non the tail latency. With a smaller index, fewer blocks have to be fetched even with a cold cache. The\nsmaller index size also reduces the overall resource consumption and improves throughput.\nThis paper explores the opportunity of learned indexes to reduce the index size for distributed database\nsystems in general, and Bigtable in particular. In order to integrate learned index structures into\nBigtable, we had to signiﬁcantly extend the original learned index design [ 10] for disk-based systems\nwith block identiﬁers. The original learned index assumed that all data is stored in a large, continuous\narray in memory, thus avoiding the non-trivial space cost of storing any pointers and allowing for\ncheap local searches following mispredictions of data locations. Unfortunately, this is not possible\nin Bigtable, which uses block-based ﬁles to store data and does not allow laying out data in one\ncontinuous array. We overcome this issue by using the model to deﬁne which records are stored in\nwhich block, as opposed to learning which records are in which block. Therefore, model predictions\nare always correct. We also create a novel compressible mapping table for the block pointers.\nIn summary, we make the following contributions:\n1. We present the ﬁrst design of learned indexes for a distributed, disk-based database.\n2. We offer a new learned index model design for string keys with monotonicity guarantees.\n3.We present the evaluation of our method with uniform value sizes to understand the trade-\noffs in read performance. We demonstrate using learned indexes improves latency 36% for\npoint lookups and 22% for scans on average. Similarly, using learned indexes increases\nthroughput by 55% for point lookups and 28% for scans on average.\n2 Related Work\nLearned index structures were ﬁrst introduced in [ 10]. Several works on learned index structures\nfollowed [ 6,7,4,9,14,13,15,12,5,11]. For example, PGM index [ 6], FITing tree [ 7], RadixS-\npline [ 9] and ALEX [ 4] all investigate how to efﬁciently support inserts, provide better worst-case\nguarantees, and/or faster index construction. Lisa [ 8] extends the idea of learned index structures\nfor DNA Sequence search, [ 13,15] for spatial applications, and Flood [ 12] and Tsunami [ 5] for\nmulti-dimensional data. More recently, the SOSD benchmark [ 11] demonstrated that learned index\nstructures outperform their traditional counterparts on a wide range of datasets in size and lookup\nlatency. However, none of these algorithms are designed for disk-based or distributed systems.\nMost closely related to this work is BOURBON [ 3], which shows how to integrate learned index\nstructure for an LSM-based Key-Value Store. A log-structure merge (LSM)-tree is a highly write\noptimized data structure for disk. However, their system is a research prototype - so it is still not\nanswering the question if learned indexes would impact a real production system, nor are their\ntechniques targeted toward distributed systems and read-heavy workloads.\n3 Background System Design\nBigtable [2] is a distributed structured storage system that serves petabytes of data. Bigtable can\nsupport jobs that need to meet high throughput requirements as well as jobs that require low latency,\nusing a server that manages the Bigtable cell, and many tablet servers that serve data.\nWrites in Bigtable are handled directly by the tablet servers, where they are ﬁrst committed to a log\non disk, then written to the memtable. When the memtable size reaches a threshold, it is compacted\nto disk as a read-only SSTable. Reads are also handled directly by the tablet servers either from the\nmemtable or SSTables. During reads, SSTables are loaded into memory from a distributed ﬁle system\nand scanned to ﬁnd the correct value.\nSSTables are structured as a map of key-value pairs, sorted by keys of the form (row, column,\ntimestamp). To support efﬁcient operations on very large data sets, SSTable implementation partitions\ndata into data blocks and keeps an index of which keys reside in which data blocks. Reads can then\nuse the SSTable index to ﬁnd the correct data blocks for a given key.\nThe SSTable index is stored in index blocks as an ordered mapping from the last key in a data block\nto the associated index entry for that data block. The index entry contains information about the\n2\n\nlocation and size of the block. Using these, the data block can be efﬁciently loaded into memory.\nThe index block is intended to stay in memory, meaning reads, especially continuous reads, can be\nfulﬁlled by querying the index block and only making directed disk reads for the requested data.\nThe SSTable index is created while SSTables are constructed. During construction, once a data block\nhas been ﬁlled up, an appropriate entry is added to the index. The data block is then written out and\nevicted from memory. To ﬁnalize the SSTable, the index is written out as well.\nWith growing SSTable sizes, the size of the index block can also grow. To improve index efﬁciency,\nSSTables use two-level indexes, comprising a level-0 index and a level-1 index. The level-0 index\nspans a single index block and always resides in memory, pointing to level-1 index blocks. The\nlevel-1 index is a sequence of level-1 index blocks, which point to data blocks. The level-1 index\nblocks are loaded into memory when a data block that they point to is accessed.\n4 Learned Indexes for Disk-based Databases\nDesign As described above, an SSTable Dconsists of key, value pairs r=hk;vi. These records\nare sorted by the key kand stored into blocks. A traditional look-up index searches through the index\ndata structure to identify which block contains the record for a given key. The learned index design in\n[10] proposed the index tries to predict the record’s location directly by a given key k. If the predicted\nlocation was incorrect, the system needed to search nearby (e.g. with exponential search) to ﬁnd the\nrequested record. Unfortunately, this doesn’t work well for Bigtable. Predicting the exact location of\na record is not useful since the system needs to read the entire block. As a result, we instead focus\non the learned index predicting blocks not records. Even then, if the predicted block is incorrect,\nsearching nearby would require reading whole new blocks, which is much more costly.\nTherefore, we take a new approach. Let’s assume we are training a learned index f(\u0001)to take a\nkeykand predict a block number. Given a learned index f, we designate block boundaries during\nSSTable creation such that each record hk;viis placed in predicted block f(k). By writing the data\nto match the index, we guarantee that the learned index always predicts the correct block for a given\nkey. Further, by using a monotonic f, we ensure that records are still sorted by key and we can still\nperform range scans.\nLearned Index Training How should we train the learned index to predict the block number before\nthe block number is set? Rather than train the learned index to predict the block number, we train it\nto predict how many bytes into the data the record falls. We can use this estimation of number of\nbytes to build approximately equal-sized blocks.\nTo make this precise, assume that s(hk;vi)is the number of bytes in record hk;viand letS(hk;vi)\ndenote the number of bytes preceding kin the SSTableD:\nS(hk;vi) =X\nhk0;v0i2Djk0<ks(hk0;v0i) (1)\nWe useS(hk;vi)as the supervision for training f, i.e. minfP\nhk;vi(f(k)\u0000S(hk;vi))2. Given a\nprediction of the number of bytes into the data a record lies, we can recover the predicted block\nnumber bybf(k)=\u001ccwhere\u001cis the desired average block size. While model inaccuracy may result\nin a less uniform distribution of block sizes, the model will never predict the wrong block.\nModel Structure The results presented in this paper are based on a linear regression model, but\nany model can be used (e.g., RMIs [ 10]). The simplicity of the model reduces both storage and\ninference costs while providing the necessary monotonicity guarantees over string keys.\nSince the keys are strings of varying lengths, we convert them into an integer for input to the linear\nregression. In this conversion, we treat the leading characters of the string as the leading bits of\nthe integer. Doing this naively would mean we could only take into account the ﬁrst few characters\nin the key. Instead, we shift and re-base on a character by character basis, based on the range of\ncharacters observed at each character position. During training, we determine the minimum and\nmaximum ASCII value of each character position over all keys. We then convert the keys into\nintegers by choosing a different numerical base for each character position according to the range of\nvalues that position can take. To give an example, consider ﬁrst how we encode numbers in base 10:\n132 = 1\u0003100 + 3\u000310 + 2 . Now, consider that the only keys are \"aab\", \"bdd\", and \"bcb\". We need 2\nvalues to encode the range of the 0thposition, 4 values to encode the range of the 1st, and 3 values to\n3\n\n(a) p99 Latency\n (b) Mean Latency\n (c) p99 Throughput\n (d) Mean Throughput\nFigure 1: Learned index demonstrates performance improvements over the two-level index for lookups and\nscans. (a), (b): Latency reduced. (c), (d) Throughput improved. Scan numbers are normalized as per 1KB row.\nencode the range of the 2nd. Therefore, \"bcb\" becomes 1\u0003(4\u00033) + 2\u0003(3) + 0 . These converted\nkeys become the input xfor an Ordinary Least Squares (OLS) model matching those xtoS(hk;vi).\nWe can compute the parameters in closed form after a single pass over the data.\nBlock Locations As the learned index predicts block numbers instead of disk locations, we need a\nway to map a predicted block number to a disk location. For this, we store the location of each block\nlocation in an array Bsuch thatB[bf(k)=\u001cc]outputs the location of the block on disk. Because the\nblocks are stored contiguously, B[bf(k)=\u001cc+ 1]\u0000B[bf(k)=\u001cc]makes the block size available. For\nbackwards compatibility with traditional indexes in Bigtable, we also store additional metadata (e.g.\nchecksums) for each block. While this design may not seem more efﬁcient than the last layer of a\ntraditional index, it saves signiﬁcant space in two regards: (1) none of the keys need to be stored (2)\nthe disk locations can be compressed as they are approximately uniformly spaced2.\n5 Evaluation and Conclusion\nWe evaluate how using a learned index affects Bigtable read performance in comparison to using\na two-level index (Bigtable’s default). In our preliminary results, we measure point lookups and\nscans from 5 tablet servers, each with 4G RAM. For point lookups, 15 clients maintain a buffer of\n100 asynchronous single row read requests each, sending new requests as they receive replies. For\nscans, every operation scans 100 consecutive rows of size 1KB. During reads, tablet servers prefetch\nindex and data blocks as needed. The SSTables are constructed with random integer keys (encoded\nas strings). Each row key is associated with data of size 1KB, and a data block size of 32KB.\nWe ran experiments with tables of different sizes (16 to 512 million rows) and the size of the table\ndid not impact the read performance. The results we are presenting are for a table size of 256 million\nrows.\nFigure 1(a) shows that the learned index reduces the read latency by 38% for point lookups while\nmaintaining it for scans in the 99th percentile. The mean latency was also reduced 36% for point\nlookups and 22% for scans as shown in Figure 1(b). Learned index eliminates the need for disk\naccesses for fetching index blocks, which improves the read latency for databases with larger indexes.\nThe learned index also signiﬁcantly increases the throughput and indirectly, reduces CPU resource\nusage. Throughput increased 54% for point lookups and 56% for scans (see Figure 1(c)) in the 99th\npercentile. Comparably, the mean throughput increased by 55% for point lookups and by 28% for\nscans (see Figure 1(d)). The reason for this is more subtle: Like many disk-based systems, Bigtable\nheavily compresses the data. Reducing the number of blocks accessed reduces the amount of work\nneeded to decompress the data in addition to other secondary effects.\nEarly results from our ongoing experiments with different key distributions, workloads, and value\nsizes show similar performance improvements from using a learned index (see also [ 11]). Furthermore,\nusing the learned index does not affect the write performance because the model training is decoupled\nfrom the write path in Bigtable. The learned index model is trained in the background during SSTable\ncreation after memtables ﬁll up.\nIn summary, our evaluation shows that learned indexes can improve real-world systems because of\nthe cascading beneﬁts of their signiﬁcantly smaller size and simple usage.\n2In the experiments below we don’t use a compressed form of this map.\n4\n\nReferences\n[1]F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, T. Chandra, A. Fikes, and\nR. E. Gruber. Bigtable: A distributed storage system for structured data. In 7th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI) , pages 205–218, 2006.\n[2]F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, T. Chandra, A. Fikes, and\nR. E. Gruber. Bigtable: A distributed storage system for structured data. ACM Transactions on Computer\nSystems (TOCS) , 26(2):4, 2008.\n[3]Y . Dai, Y . Xu, A. Ganesan, R. Alagappan, B. Kroth, A. Arpaci-Dusseau, and R. Arpaci-Dusseau. From\nwisckey to bourbon: A learned index for log-structured merge trees. In 14th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 20) , pages 155–171. USENIX Association, Nov.\n2020.\n[4]J. Ding, U. F. Minhas, H. Zhang, Y . Li, C. Wang, B. Chandramouli, J. Gehrke, D. Kossmann, and D. Lomet.\nALEX: An Updatable Adaptive Learned Index. arXiv:1905.08898 [cs] , May 2019.\n[5]J. Ding, V . Nathan, M. Alizadeh, and T. Kraska. Tsunami: A learned multi-dimensional index for correlated\ndata and skewed workloads. CoRR , abs/2006.13282, 2020.\n[6]P. Ferragina and G. Vinciguerra. The PGM-index: A fully-dynamic compressed learned index with\nprovable worst-case bounds. Proceedings of the VLDB Endowment , 13(8):1162–1175, Apr. 2020.\n[7]A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska. FITing-Tree: A Data-aware Index\nStructure. In Proceedings of the 2019 International Conference on Management of Data , SIGMOD ’19,\npages 1189–1206, New York, NY , USA, 2019. ACM.\n[8]D. Ho, J. Ding, S. Misra, N. Tatbul, V . Nathan, V . Md, and T. Kraska. LISA: towards learned DNA\nsequence search. CoRR , abs/1910.04728, 2019.\n[9]A. Kipf, R. Marcus, A. van Renen, M. Stoian, A. Kemper, T. Kraska, and T. Neumann. RadixSpline: A\nsingle-pass learned index. In Proceedings of the Third International Workshop on Exploiting Artiﬁcial\nIntelligence Techniques for Data Management , pages 1–5. Association for Computing Machinery, June\n2020.\n[10] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In\nProceedings of the 2018 International Conference on Management of Data , pages 489–504. ACM, 2018.\n[11] R. Marcus, A. Kipf, A. van Renen, M. Stoian, S. Misra, A. Kemper, T. Neumann, and T. Kraska.\nBenchmarking learned indexes. CoRR , abs/2006.12804, 2020.\n[12] V . Nathan, J. Ding, M. Alizadeh, and T. Kraska. Learning multi-dimensional indexes. In D. Maier,\nR. Pottinger, A. Doan, W. Tan, A. Alawini, and H. Q. Ngo, editors, Proceedings of the 2020 International\nConference on Management of Data, SIGMOD Conference 2020, online conference [Portland, OR, USA],\nJune 14-19, 2020 , pages 985–1000. ACM, 2020.\n[13] V . Pandey, A. van Renen, A. Kipf, I. Sabek, J. Ding, and A. Kemper. The case for learned spatial indexes.\nCoRR , abs/2008.10349, 2020.\n[14] N. Setiawan, B. Rubinstein, and R. Borovica-Gajic. Function Interpolation for Learned Index Structures.\nInDatabase Theory and Applications , DTA ’20, 2020.\n[15] H. Wang, X. Fu, J. Xu, and H. Lu. Learned index for spatial queries. In 2019 20th IEEE International\nConference on Mobile Data Management (MDM) , pages 569–574, 2019.\n5",
  "textLength": 20281
}