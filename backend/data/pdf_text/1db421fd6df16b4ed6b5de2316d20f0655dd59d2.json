{
  "paperId": "1db421fd6df16b4ed6b5de2316d20f0655dd59d2",
  "title": "Aggregate-based Training Phase for ML-based Cardinality Estimation",
  "pdfPath": "1db421fd6df16b4ed6b5de2316d20f0655dd59d2.pdf",
  "text": "SCHWERPUNKTBEITRAGhttps://doi.org/10.1007/s13222-021-00400-zDatenbank Spektrum (2022) 22:45–57\nAggregate-based Training Phase for ML-based Cardinality Estimation\nLucas Woltmann1·C l a u d i oH a r t m a n n1·D i r kH a b i c h1· Wolfgang Lehner1\nReceived: 27 October 2021 / Accepted: 13 December 2021 / Published online: 10 January 2022\n© The Author(s) 2022\nAbstract\nCardinality estimation is a fundamental task in database que ry processing and optimization. As shown in recent papers,\nmachine learning (ML)-based approaches may deliver more accurate cardinality estimations than traditional approaches.\nHowever, a lot of training queries have to be executed during the model training phase to learn a data-dependent ML model\nmaking it very time-consuming. Many of those training or e xample queries use the same base data, have the same query\nstructure, and only differ in their selective predicates. To speed up the model training phase, our core idea is to determine\napredicate-independent pre-aggregation of the base data and to execute the example queries over this pre-aggregated data.\nBased on this idea, we present a speciﬁc aggregate-based training phase for ML-based cardinality estimation approaches\nin this paper. As we are going to show with different workloads in our evaluation, we are able to achieve an averagespeedup of 90 with our aggregate-based training phase and thus outperform indexes.\nKeywords Cardinality estimation · M achine learning · Database support · Pre-aggregation\n1I n t r o d u c t i o n\nDue to skew and correlation in data managed by database\nsystems (DBMS), query optimization is still an impor-tant challenge. The main task of query optimization isto determine an efﬁcient execution plan for every SQL\nquery, whereby most of the optimization techniques are\ncost-based [ 1]. For these techniques, cardinality estimation\nhas a prominent position with the task to approximate thenumber of returned tuples for every query operator within\na query execution plan [ 1–4]. Based on these estimations,\nvarious decisions are made by different optimization tech-niques such as choosing (i) the right join order [ 5], (ii)\nthe right physical operator variant [ 6], (iii) the best-ﬁt-\nting compression scheme [ 7], or (iv) the optimal operator\n/envelopebackLucas Woltmann\nlucas.woltmann@tu-dresden.de\nClaudio Hartmann\nclaudio.hartmann@tu-dresden.de\nDirk Habich\ndirk.habich@tu-dresden.de\nWolfgang Lehner\nwolfgang.lehner@tu-dresden.de\n1Dresden Database Research Group, Technische UniversitätDresden, Dresden, Germanyplacement within heterogeneous hardware [ 8]. However, to\nmake good decisions in all cases, it is important to have\ncardinality estimations with high accuracy.\nAs shown in recent papers [ 9,10], including our own\nwork [ 11], machine learning-based cardinality estimation\napproaches are able to meet higher accuracy requirements,\nespecially for highly correlated data. While traditional\napproaches such as histogram -based and frequent values\nmethods assume data independence for their estimation [ 1],\nML-based approaches assume that a sufﬁciently deep neu-\nral network can model the very complex data dependencies\nand correlations [ 9]. For this reason, ML-based cardinality\nestimation approaches may thus give much more accurateestimations as clearly demonstrated in [ 9–11]. However,\nthe main drawback of these ML-based techniques com-\npared to traditional approaches is the high construction\ncost of the data-dependent ML-models based on the under-\nlying supervised learning approach. During the so-calledtraining phase , the task of supervised learning is to train\na model, or more speciﬁcally learn a function, that maps\ninput to an output based on example (input, output)\npairs. Thus, in the case of cardinality estimation, manypairs consisting of (query, output-cardinality)\nare required during the training phase . To determine the\ncorrect output-cardinalities , the queries have to\nbe executed [ 9,11], whereby the execution of those ex-\nample queries can be very time consuming, especially for\nK\n\n46 Datenbank Spektrum (2022) 22:45–57\ndatabases with many tables, many columns, and millions or\nbillions of tuples resulting in a heavy load on the databasesystem.\nCore Contribution. To overcome these shortcomings,\nwe propose a novel training phase based on pre-aggregated\ndata for ML-based cardinality estimation approaches. This\nis an extended version of previous work [ 12]. Here, we ex-\npand the evaluation and introduce further improvement ofthe aggregate-based training phase . Usually, as described\nin [9,11], every example query is (i) rewritten with a count\naggregate to retrieve the correct output-cardinality\nand (ii) executed individually. However, many of those ex-ample queries use the same base data, have the same querystructure, and only differ in t heir selective predicates. To\noptimize the query execution, our core idea is to provide\napredicate-independent pre-aggregation of the base data\nand to execute the example queries over this pre-aggregateddata. Consequently, the set of similar example queries has\nto read and process less data because the pre-aggregation\nis a compact representation of the base data. To realize thispre-aggregation , the most common solution in DBMS is to\ncreate a data cube for storing and computing aggregate in-\nformation [ 13]. However, this pre-aggregation is only ben-\neﬁcial if the execution of the example queries on the data\ncube plus the time for creating the data cube is faster thanthe execution of the example queries over the base data. Aswe are going to show with different workloads of example\nqueries in our evaluation, we are able to achieve an average\nspeed up of 90. We also compare our approach to standardquery optimization with index structures on the base dataand show their limited beneﬁt for this use case.\nContributions in Detail and Outline. Our aggregate-\nbased training phase consists of two phases: (i) creation\nof a set of meaningful pre-aggregated data sets using data\ncubes and (ii) rewrite and execute the example queries onthe corresponding data cubes or the base data. In detail, the\ncontributions in this paper are:\n1. We start with a general overview of ML processes in\nDBMS in Sect. 2. In particular, we detail cardinality es-\ntimation as a case study for ML in DBMS. We introduce\nglobal and local models as two representatives for ML-\nbased cardinality estimation approaches. Primarily, weshow their properties in terms of example workload com-plexity and conclude the need for optimization of suchworkloads.\n2. Based on this discussion, we introduce our general solu-\ntion approach of an aggregated-based training phase by\npre-aggregating the base data using the data cube con-\ncept and executing the example queries over this pre-\naggregated data. Moreover, we introduce a beneﬁt crite-\nrion to decide whether the pre-aggregation is beneﬁcial\nor not.3. In Sect. 4, we present our aggregate-based training phase\nfor ML-based cardinality estimation approaches in detail.Our approach consists of two components: Analyzer\nandRewrite . While the main task of the Analyzer\ncomponent is to ﬁnd and build all beneﬁcial data cubes,\ntheRewrite component is responsible for rewriting the\nexample queries to the constructed data cubes if possible.\n4. Then, we present experimental evaluation results for four\ndifferent workloads for the training phase of ML-based\ncardinality estimation in Sect. 5. The workloads are de-\nrived from different ML-based cardinality estimation ap-\nproaches [ 9,11] on the IMDb data set [ 14]. Moreover,\nwe improve our approach by using additional indexes onthe pre-aggregates. We compare our results with the op-\ntimization using just index structures on the base data.\nFinally, we conclude the paper with related work in Sect. 6\nbefore concluding in Sect. 7.\n2 Machine Learning Models for DBMS\nIn this section, we start with a brief description of the gen-\neral process of machine learning (ML) in the context of\nDBMS. Moreover, we discuss ML-based cardinality esti-mation for DBMS as an important case study and revisittwo ML-based approaches solving this speciﬁc challenge.\nFinally, we analyze the speciﬁc query workloads for the\ntraining phases and clearly state the need for optimizeddatabase support.\n2.1 Machine Learni ng Support for DBMS\nMost ML-supported techniques for DBMS are supervised\nlearning problems. In this category, there are amongst oth-ers: cardinality estimation [ 9–11], plan cost modeling [ 15,\n16], and indexing [ 17]. The proposed ML solutions for\nthose highly relevant DBMS problems have a general pro-cess in common as shown in Fig. 1. This process is usually\nsplit into two parts: forward pass and training phase .\nForward pass: This pass consists of the application of the\nmodel triggered by a request of the DBMS to the ML\nFig. 1 The general process of supervised ML in DBMS\nK\n\nDatenbank Spektrum (2022) 22:45–57 47\nmodel. Each request pulls some speciﬁc information from\nthe model such as an estimated cardinality or an index po-sition [ 9–11,17]. The execution time of each forward pass\nrequest is normally in the range of milliseconds. This isadvantageous because forwar d passes occur often and reg-\nularly during the run time of the DBMS [ 9–11,17].\nTraining phase: To enable the forward pass, a training\nphase is necessary to construct the required ML model,\nwhereby the challenge for the model lies in the gener-\nalization from example data [ 9–11,17]. Therefore, the\nmodel usually requests a lot of diverse labeled exampledata—pairs of (input, output) —from the DBMS to\nlearn the rules of the underlying problem. Even though the\ntraining is performed once, its run time may take hours.\nThis is mainly caused by the generation and execution ofa large number of queries against the DBMS to determinethe ground truth.\nAs a consequence, the training phase of ML models\nto support DBMS usually generates a spike high load onthe DBMS. Compared to the forward pass , the training is\nsigniﬁcantly more expensive from a database perspective.\nTherefore, the training phase is a good candidate for opti-mization to reduce (i) the time for the training phase and(ii) the spike load on the DBMS. Thus, database support oroptimization of the training phase is a novel and interesting\nresearch ﬁeld leading to an increased applicability of ML\nsupport for DBMS.\n2.2 Case Study: Cardi nality Estimation\nAs already mentioned in the introduction, we restrict our fo-\ncus to the ML-based cardinality estimation use case [ 9–11].\nIn this setting, each forward pass requests an estimated\ncardinality for a given query from the ML model. In the\ntraining phase , the ML cardinality estimator model requires\nFig. 2 Overview of ML-based cardinality estimation approachesexample queries as example data from the DB where the\nqueries are labeled with their true cardinality resulting inpairs of ( query ,cardinality ). These cardinalities\nare retrieved from the DB by executing the queries en-hanced with a count aggregate. Two major approaches for\nuser-workload-independent cardinality estimation with ML\nmodels have been proposed in recent years: global and\nlocal models .\n2.2.1 Global Model Approach\nAglobal model is trained on the complete database schema\nas the underlying model context [ 9]. It is effective in cover-\ning correlations in attributes for high-quality estimates [ 9].\nIn Fig. 2, this is depicted by a single model stated and\nmapped to the complete schema. Global models have down-sides like (i) the complexity of the ML model and (ii) thevery expensive training phase. Both disadvantages arise for\nthe following reason: the single ML model handles all at-\ntributes and joins in the same way leading to a huge problemspace. This huge problem space is directly translated to themodel complexity as well as to a high number of exam-ple queries to cover all predicates and joins over the whole\nschema as shown [ 9].\n2.2.2 Local Model Approach\nTo overcome the shortcomings of the global model ap-\nproach, the concept of local models has been intro-\nduced [ 11]. Local models are ML models which only\ncover a certain sub-part of th e complete database schema\nas their model context. This can be a base table or any\nn-way join. Again, Fig. 2details several local models each\ncovering a different part of the schema. As each of themfocuses on a part of the schema, there are many advantagescompared to global models. Firstly, local models produce\nestimates of the same quality as global models [ 11]. Sec-\nK\n\n48 Datenbank Spektrum (2022) 22:45–57\nondly, their model complexity is much smaller, because\nthey cover a smaller problem space in different combina-tions of predicates and joins. The lower complexity stemsfrom a more focused or locali zed problem solving. A local\nmodel has to generalize a smaller problem than the global,\ni.e. the cardinality estimate of a sub-part of a schema and\nnot the whole schema at once. Th e lower complexity leads\nto faster example query sampling and training becausethe easier problem requires fewer queries during training.\nA major disadvantage of local models is the high number\nof models needed to cover all objects touched by a querywithin the forward pass. Ther efore, a separate local ML\nmodel needs to be constructed for each part of the schema.Additional queries need to be generated because every\nlocal model requests the same amount of example queries.\nHowever, these queries are l ess complex because there\nare fewer combinations of pr edicates and tables in a local\ncontext.\n2.3 Training Phase Workload Analysis\nFundamentally, the global as well as the local ML-basedcardinality estimation approach use the same method to\nsample example queries for the training phase .T h i sp r o -\ncedure is shown on the right side of Fig. 2, where a local\nmodel is trained on an example table consisting of threeattributes Brand ,Color ,a n d Year . To train the local\ndata-dependent ML-model, a collection of count queries\nwith different predicate combinations over the table is gen-erated. In our example, the predicates are speciﬁed over thethree attributes using different operators ¤;< ;/DC4;=;> ;/NAK\nand different predicate values. Thus, all queries have the\nsame structure but differ in their predicates to cover every\naspect of the data properties in the underlying table. An ex-ample query workload to train a global model would look\nabc\nFig. 3 Analysis of ML-workload complexity for 1 global and 1 to 3 local models for the IMDb. aNumber of queries, bnumber of accessed tables,\ncnumber of predicatessimilar. However, the different contexts for global and local\nmodels have an impact on the number and the complexityof the example queries. In general, the query complexity isgiven by the combinations of joined tables and predicatesin a query. The larger the model context, the more complex\nthe example queries. Thus, global models have workloads\nwith a higher number of variations for predicates and joinsbecause they cover the whole schema. Local models aretrained with workloads with lesser variation [ 11].\nTo better understand the query workload complexity for\nthe training phase, we analyzed the workloads publishedby the authors of the global [ 18] and local approach [ 19].\nIn both cases, the authors used the IMDb database [ 14]f o r\ntheir evaluation, because this database contains many corre-\nlations and is thus very challenging for cardinality estima-\ntion. Our analysis results are summarized in Fig. 3and 4.\nFor a global model and an increasing number of local\nmodels, Fig. 3a shows the workload complexity in terms of\nnumbers of example queries used per workload. While the\nglobal model requires up to 100,000 example queries forthe IMDb database [ 9], the local model only requires 5000\nexample queries per local model [ 11] to determine a stable\ndata-dependent ML model. In general, the number of ex-\nample queries is much higher for a global model, but the\nnumber of example queries also increases with the numberof local models. Thus, we can afford more local modelsbefore their collective query count exceeds the number of\nqueries for the global model.\nFig. 3b speciﬁes the workload size in terms of data access\nthrough the number of joined tables per workload. Similarto the previous ﬁgure, the global model has the highest com-plexity because it requires example queries over more tables\nto cover the whole schema at once. Each local model cov-\ners only a limited part of the schema and therefore queries\nK\n\nDatenbank Spektrum (2022) 22:45–57 49\nFig. 4 Predicate operator occurrences for IMDb\nfewer tables per model. Thereby, the complexity of accessed\ndata for local models increases with the number of models.\nAnother important aspect to describe the workload com-\nplexity is the number of predicates per query as shown inFig. 3c. Here, the global model workload has a much larger\nspread over the number of predicates. The local models de-\ntail a more focused distribution with little variation. Again,the local model workload does not require the amount of al-ternation in predicates of a global model workload becauseit covers a smaller fragment of the schema. Additionally,\nFig. 4gives an overview of the distribution of occurrences\nof all predicate operators in the workloads as a box plotwith mean values. The global model workload only usesthe operators <;=;>, whereas the local model workloads\nuse the full set of operators ¤;< ;/DC4;=;> ;/NAK. As described\nby both authors, the predicate operators in each examplequery are sampled from a uniform distribution [ 9,11]. The\nslight variation between the operators per workload is dueto the fact that both approaches ﬁlter 0-tuple queries which\ndo not occur uniformly.\n3 Training on Pre-Aggregated Data\nAs discussed above, the global as well as the local ML-\nbased approach for cardinality estimation generates manyexample queries with a count aggregate function during thetraining phase . Depending on the model context, there is\na small variance in the number of accessed tables, but there\nis a high variance for predicates in terms of (i) number ofpredicates, (ii) used predicate operators, and (iii) predicatevalues in general. So, many queries work on the same data\nbut look at different aspects. Executing such workloads in\na naive way, i.e. executing each example query individuallyon large base data, is very expensive and generates a highspike load on the database system. The utilization of indexstructures for an optimized execution in database systems\nappears to be an ideal technique at a ﬁrst glance. However,\ntheir beneﬁt is limited as we will show in our evaluation.The same can be said about materialized views. We omittedtheir evaluation b ecause the experiment did not ﬁnish within\n30 days.\nTo tackle this problem more systematically, our core idea\nis to pre-aggregate the base data for different predicate com-\nbinations and to reuse this pre-aggregated data for several\nexample queries. In general, a ggregates compress the data\nby summarizing information and reducing redundancy. Thislessens the amount of data to be scanned by each examplequery because the aggregates can be smaller than the origi-\nnal data. The aggregate pre-calculates information with the\nresult that the workload queries need to scan less data dur-ing execution. Therefore, it is important that the construc-tion of the aggregate does not take longer than the reductionof the workload execution time.\n3.1 Grouping Sets as Pre-aggregates\nIt might sound expensive to aggregate all possible combina-\ntions of predicates. However , DBMS already offer substan-\ntial supportive data structures for this kind of aggregation.The basic idea of such grouping comes from Online An-\nalytical Processing (OLAP) workloads. These aggregate-\nheavy workloads spawned the idea of pre-aggregating in-\nformation in data cubes [13] helping to reduce the execution\ntime of OLAP queries by collecting and compressing thelarge amount of data accessed into an aggregate. The con-cept of data cubes is well-known from data warehouses for\nstoring and computing aggregate information and almost\nall database systems are offering efﬁcient support for datacube operations [ 13,20–23].\nEach attribute of a table or join generates a dimension\nin the data cube and the distinct attribute values are the\ndimension values. The cells of a data cube are called facts\nand contain the aggregate for a particular combination ofattribute values. To instantiate the concept of a data cube ina DB, there are different cube operators . Usually, these are\nCUBE ,ROLLUP ,a n d GROUPING SET .T h e CUBE opera-\ntor instantiates aggregates for the power set of combinationsof attribute values. The ROLLUP operator builds the linear\nhierarchy of attribute combinations. The GROUPING SET\noperator only constructs combinations with all attributes.\nThis characteristic of a grouping set is the major advantage\nfor our use case. With the grouping set aggregation, wecompress the original data and avoid the calculation of un-necessary attribute combinations. Fig. 5details an example\nof a grouping set for a count aggregate over discrete data.\nThe example data has a multidimensional structure after ag-gregating where each dimension is a property of a car. Thecells of the grouping set are ﬁlled with the aggregate value,i.e. the count of cars with a particular set of properties.\nGiven the grouping set data structure, we adapt the gen-\neration of example queries from Fig. 2. By introducing the\ndata cube, we add an intermediate step before executing the\nK\n\n50 Datenbank Spektrum (2022) 22:45–57\nFig. 5 Aggregating information with grouping sets\nworkload. This step constructs a data cube, i.e. grouping\nset, and rewrites the workload to ﬁt the grouping set. Fig. 6\ndetails the construction and the rewrite of queries for thesample data. On the left side, the construction builds a ta-\nble matching the multidimensional character of the group-\ning set. Due to this new table layout, the rewrite must in-clude a different aggregate function as shown in Fig. 6.\nFor a count aggregate, the corresponding function is a sum\nover the pre-aggregated count. Last, the rewritten workload\nis executed and retrieves the output-cardinalities .\nAfter the sampling of example queries, the queries and car-dinalities are fed to the ML model in the same way asin the original process. This is an advantage of our ap-\nproach because it does not interfere with other parts of the\ntraining process. Therefore, it is independent of the type ofML model and can be applied to a multitude of supervisedlearning problems.\nEven though our approach is independent of the ML\nmodel, it is not independent of the data. In general, group-ing sets are only beneﬁcial if the aggregate is smaller thanthe original data. A negative example would be an aggre-gate over several key columns. Here, the number of distinct\nvalues per column equals the number of tuples in the table.\nIf such a grouping set is instantiated, each of its dimensionshas a length equal to the number of tuples. This grouping\nFig. 6 Illustration of database-\nsupported training phase basedon pre-aggregated data/groupingsetsset is larger than the original data because it includes the\naggregate. With all that in mind, we need to quantify thebeneﬁt of a grouping set for our use cases.\n3.2 Beneﬁt Criterion\nTo ﬁnd a useful criterion when to instantiate grouping sets,we evaluate the usefulness of these sets on synthetic data.Again, we need to ﬁnd a way to express the compression of\ninformation in a grouping set. From the synthetic data, we\nwill derive a general rule for the theoretical improvementof a grouping set on a given table or join. Our experimentcomprises six steps per itera tion. The ﬁrst step generates\na synthetic table given three properties. These properties\nare: the number of tuples in a single table or in the largest\ntable of a join N, the number of columns C, and the number\nof distinct values per column D. We vary the properties in\nthe ranges:\nN2f1000I10,000 I100,000 I1,000,000 g (1)\nV\n˛0=1− eŒ−k/SOH.M S−TQ//c141(2)\nD2f5I10I100I1000I100,000 I1,000,000 g (3)\nChanging one property per iteration, this leads to jNj/SOHjCj/SOH\njDj=4/SOH5/SOH6 = 120 different tables or iterations. The values\nin a column are uniformly sampled from the range of dis-tinct values. With an increasing number of distinct values\nper column, we simulate ﬂoating point columns which have\na large number of different values. Columns with few dis-tinct values resemble categorical data. In the second step,we sample 1000 count aggregate queries as an exampleworkload over all possible combinations of columns (pred-\nicates), operators, and values in this iteration. In the third\nstep, we execute these queri es against the table and mea-\nsure their execution time. This is equivalent to the standard\nK\n\nDatenbank Spektrum (2022) 22:45–57 51\nprocedure to sample example cardinality queries for an ML\nmodel. The fourth step constructs the grouping sets over the\nwhole synthetic table and measures its construction time.\nNext, in step ﬁve, we rewrite the queries in a way that they\ncan be executed against the grouping set. We measure their\nexecution time on this grouping set. In the ﬁnal step, we\ndivide the execution time of the workload on the grouping\nset by the run time of the workload on the table. This speed\nup factor ranges from close to zero for a negative speed up\nto inﬁnity for a positive speed up. All time measurements\nare done three times and averaged. We use PostgreSQL 10\nfor the necessary data management.\na\nb\nc\nFig. 7 Execution times and speed ups on synthetic data. aThe speed up for different combinations of columns and distinct values with tuples ﬁxed\nto 1 m, bthe speed up for different combinations of tuples and distinct values with columns ﬁxed to ﬁve, cthe speed up for different combinations\nof tuples and columns with distinct values ﬁxed to ﬁveFig. 7shows the results of all 120 iterations. A darker\ncell hue means either better execution times or higher speed\nup, whereas lighter areas mean longer execution times or\nlower speed up. The ﬁrst column shows the execution time\nof the workload against the table. The next column shows\nthe execution time of the workload against the grouping\nset including the construction time of the grouping set. The\nlast column is the quotient of the second and ﬁrst column.\nThis is the achieved speed up by using a grouping set. In\neach row, only two properties are changed while the third\nproperty is kept ﬁxed. The ﬁrst row keeps the number of\ntuples, the second row the number of columns, and the last\nK\n\n52 Datenbank Spektrum (2022) 22:45–57\nrow the number of distinct values ﬁxed. From this ﬁgure,\nwe can derive three conclusions.\nFirst, we notice that few distinct values in a few columns\nare beneﬁcial for the aggregation. Next, the more tuples N\nare in a table, the more distinct values per column can be\nthere for the speed up to be sustained. As the last conclu-\nsion, this also applies to the number of columns. All inall, the larger the original table the more distinct values andcolumns still lead to a speed up. Our experiments show that\na grouping set is only beneﬁcial if its size is smaller than\nthe original table. Only then the aggregate compresses in-formation and causes less dat a to be scanned by the queries.\nGiven our evaluation, this happens if the product of the dis-tinct values of all columns is smaller than the table size.\nWe can model this as an equation to be used as a criterion\nfor instantiating beneﬁcial grouping sets.\nscaling factor =1\nNCY\nc=1jdistinct values .column c/j (4)\nIf this scaling factor is smaller than one, we call a grouping\nset beneﬁcial. The scaling f actor is also a measure of data\ncompression. Therefore, it sh ows how much faster the scan\nover the aggregated data can be.4 Implementation\nIn this section, we describe the implementation of our ag-\ngregate-based training phase for ML-based cardinality es-\ntimation in DBMS in detail. In our implementation, we\nassume that a regular DBMS with an SQL interface pro-\nvides the base data and the ML models are trained outsidethe DBMS, e.g., in Python. Based on this setting, we addeda new layer implemented in Python between these systems\nto realize our aggregate-based training phase in a very ﬂex-\nible way. Thus, the input of this layer is an ML workloadthat is necessary for training the ML model. Then, the maintasks of this layer are:\n1. discover as well as create as many beneﬁcial grouping\nsets in the DBMS as possible for the given ML workload\n2. rewrite as well as execute the workload queries according\nto the grouping sets and base data.\nThe output of this layer is an annotated ML workload with\nthe retrieved cardinalities on which the ML model is trainedafterward. To achieve that, our layer consists of two com-ponents. The ﬁrst component is the Analyzer which is\nresponsible for the construction of beneﬁcial grouping sets.\nThe second component is the Rewrite rewriting and ex-\necuting the queries of the ML workload against the con-structed grouping sets. In the following, we introduce bothcomponents in more detail.\n4.1 Analyzer Component\nAlgorithm 1 gives a more detailed overview over theAnalyzer Component . Given an ML workload and\na database instance, our Analyzer consists of three steps\nto ﬁnd and build all beneﬁcial grouping sets. In step one ,\ntheAnalyzer scans all queries in the ML workload and\ncollects all joins or tables and their respective predicates in\nuse. This generates all possible grouping sets as a mapping\nfrom tables building the grouping set to the predicates onthose tables. In Algorithm 1, this is covered in lines 1 to 5.The second step then collects the number of distinct values\nper predicate attribute (regardless of their type) and the\nmaximum number of tuples of all tables in the grouping\nset from the metadata (statistics) of the database. This iss h o w ni nl i n e s7t o1 1 .I nt h e third and ﬁnal step , our\ndeﬁned beneﬁt criterion (Eq. ( 4)) is used to calculate the\nscaling factor and therefore the beneﬁt of each grouping\nset. If the scaling factor is smaller than one, the Analyzer\nconstructs the grouping set with all collected predicates.If the scaling factor is larger than or equal to one, thegrouping set is constructed with the maximum number of\npredicates where the scaling factor still is smaller than one.\nThis may disregard certain queries that are subsequentlynot executed against the grouping set if they have more\nK\n\nDatenbank Spektrum (2022) 22:45–57 53\npredicates than the grouping set. On the other hand, queries\non the table or join with the predicates in the groupingset can still beneﬁt from it. Moreover, all queries to beexecuted against a grouping set are marked for rewriting.This ﬁnal step is detailed in lines 12 to 18.\n4.2 Rewrite Component\nWith all beneﬁcial grouping sets instantiated by theAnalyzer Component , it is necessary to modify the\nML workload queries to be able to use the pre-aggre-\ngates. For this, all queries which can be run against anygrouping set will be rewritten in the Rewrite component.\nTheRewrite component receives information about each\nquery from the Analyzer and rewrites queries in a way\nthat they can be executed against the grouping sets. Allqueries where the Analyzer does not recognize a group-\ning set are kept as they are and will be executed overthe base data. The Rewrite component is described in\nAlgorithm 2.\nWhen all queries have been processed, the optimized\nworkload is executed as a whole on the database. If a queryhas been rewritten, it will be executed against the group-\ning set, otherwise, it will be executed against the original\ndata. Finally, the retrieved results (i.e. cardinalities) are for-warded to the ML system to train the ML model.\n5E v a l u a t i o n\nTo show the beneﬁt of our novel aggregate-based training\nphase , we conducted an exhaustive experimental study with\nboth presented types of ML models for cardinality estima-\ntion (cf. Sect. 2). Thus, we start this section by explaining\nthe experimental settings followed by a description of se-lective results for the local as well as global ML model ap-proaches. Afterward, we su mmarize the main experimental\nﬁndings.5.1 Experimental Setting\nFor our experiments, we used the original workloads for\nthe local and global ML model approaches [ 18,19]o n\nt h eI M D bd a t as e t[ 14]. The IMDb contains a snowﬂake\ndatabase schema with several millions of tuples in both\nthe fact and the 20 dimension tables. As already presentedin Sect. 2.3, the global model workload contains 100,000\nqueries. For the local models, we used three workloads\nwhere each workload has 5000 queries more than the previ-\nous one. These workloads correspond to one, two, and threetrained local models. Overall, we have four workloads forour experiments: one for a global model and three work-loads for an increasing number of local models. Moreover,\nall experiments are conducted on an AMD A10-7870K sys-\ntem with 32GB main-memory with PostgreSQL 10 as theunderlying database system.\nIn our experiments, we measured the workload execu-\ntion times, whereby we distinguish four different execution\nmodes:\nBase Data w/o Indexes: ML workload is executed on the\nIMDb base data without any indexes on the base data.\nBase Data w/ Indexes: ML workload is executed on the\nIMDb base data with indexes on all (join) predicates inuse, totaling 15 indexes.\nGrouping Set (GS): ML workload is executed on pre-aggre-\ngated data as determined by our approach.\nGrouping Set (GS) w/ Indexes: ML workload is executed on\npre-aggregated data where indexes were built on all predi-\ncates in the GS, totaling 39 indexes.\nThe ﬁrst two execution modes represent our baselines\nbecause both are currently used in the presented ML model\napproaches for cardinality estimation [ 9–11].\n5.2 Experimental Results: Local Model Workloads\nFig. 8shows the results for the three local model work-\nloads. The ﬁrst workload local 1 contains the necessary\nqueries to build one local ML model to estimate thecardinalities for the join title ‰movie_keyword .\nThe second workload local 2 adds 5000 queries to the\nﬁrst workload to construct a second ML model for the\njoin title ‰movie_info . The third workload lo-\ncal 3 adds another ML model for an additional join\ntitle ‰movie_companies . Therefore, we increment\nthe number of local ML models.\nFig. 8a details the execution times for all three local\nmodel workloads local 1 ,local 2 ,a n d local 3 for all inves-\ntigated execution modes. In each of the three groups, the\nK\n\n54 Datenbank Spektrum (2022) 22:45–57\nTable 1 Execution times ML workloads (GS: grouping sets)\nModel Base Data\nw/o IndexBase Data\nw/ IndexConstruction\nGSExecution\nGSTotal\nGSTotal GS\nw/ IndexCoverage\nGS\nLocal 1 2 h30m 1h44m 6.17 s 191.34 s 197.51 s 139.47 s 100%\nLocal 2 7 h53m 5h10m 23.70 s 205.91 s 229.61 s 149.99 s 100%\nLocal 3 10 h12m 6h56m 29.10 s 430.36 s 459.46 s 295.25 s 100%\nGlobal\nfull–4 d14h 2h22m 20d20h 20d22h – 100%\nGlobal\nopt–4 d14h 34m29s 2d11h 2d12h 1d21h 55%\nleft bar shows the complete workload execution time on the\nIMDb without indexes, the second bar on the IMDb withindexes, the third bar the execution time with our grouping\nset approach, and the last bar the execution time with our\ngrouping sets including indexes. As we can see, indexes onthe base data are already re ducing the workload execution\ntimes compared to execution on the base data w/o indexes,\nbut the speedup is very marginal as shown in Fig. 8bb e -\ncause the DBMS might decide to abstain from using theindexes. In contrast to that, our grouping set approach haslower execution times in all cases and the achieved speed\nups compared to execution on the base data w/o indexes are\nin the range between 45 and 125 as depicted in Fig. 8b. Ad-\nditionally, building indexes on the grouping sets improvesthe speed up to a range of 65 to 180. Thus, we can con-clude that our aggregation-based training phase is much\nmore efﬁcient than state- of-the-art approaches.\nFor each considered join, our aggregation-based ap-\nproach creates a speciﬁc grouping set containing allcolumns from the corresponding workload queries. Ac-c o r d i n gt oE q .( 4), the scaling factors are: 0.02, 0.003, and\n0.05 for the three joins. Thus, the instantiation of grouping\nsets is beneﬁcial. So, all grouping sets achieve a very goodcompression rate and the rewritten workload queries onthe grouping sets have to read much less data compared to\nab c\nFig. 8 Local model evaluation based on our aggregate-based training phase (GS: Grouping Sets). aExecution times, bspeed ups, cspeed ups per\nMBthe execution on base data. Moreover, all workload queries\ncan be rewritten, so that the coverage is 100% and everyquery beneﬁts from this optimization. Nevertheless, the\nthree scaling factors differ explaining the different speed\nups.\nThe construction of the grouping sets can be consid-\nered a drawback. However, as illustrated in Table 1,t h e\nconstruction times for the grouping sets and indexes are\nnegligible because th e reduction in execution time is signif-\nicantly higher. From a storage perspective, index structuresand grouping sets need some extra storage space, wherethe storage overhead for grouping sets is larger than for\nindexes. But, as illustrated in Fig. 8c, the speed up per\nadditional MB for grouping sets is much larger than forindexes. We also show that the use of indexes on group-ing sets is beneﬁcial but grouping sets are more efﬁcient\nregarding memory consumption. So, the additional storage\nrequirements for grouping sets and indexes are justiﬁable.All in all, we gain a much larger speed up making groupingsets or the combination of grouping sets and indexes themore efﬁcient approach.\nK\n\nDatenbank Spektrum (2022) 22:45–57 55\n5.3 Experimental Results: Global Model Workload\nFig. 9shows the evaluation results in terms of execution\nand construction times for the global model workload. Asshown in the previous experiment, the utilization of indexes\nis always beneﬁcial. Thus, we only compare the execution\non base data with indexes, the execution on the aggregateddata, and the execution on the combined aggregated andindexed data in this evaluation.\nIn general, there are 21 grouping sets possible for\nthe global workload. However, some of these groupingsets have a scaling factor larger than one. Therefore, ourAnalyzer component disregards the attributes of some\ngrouping sets until the scaling factor is smaller than one\n(cf. Sect. 4). As a consequence, only 55% of the global\nworkload queries can be rewritten to this optimal set ofgrouping sets. This strategy called GSin Fig. 9a reduces\nthe workload execution time of the global workload. The\nspeed up compared to the ex ecution on the base data with\nindexes is almost 2 (Fig. 9b). Using indexes on the pre-\naggregated data brings the speed up to 2.5.\nTo show the beneﬁt of our grouping set selection strat-\negy, we also constructed all grouping sets with all attributes\n(GS full ). There, we are able to rewrite all global workload\nqueries to be executed on these aggregated data. As shownin Fig. 9a, the overall workload execution time is longer\nthan the execution on base dat a with indexes. Therefore,\ngrouping sets have to be selected carefully. Moreover, this\nexperiment shows that our deﬁnition of a beneﬁcial group-ing set is applicable because (i) not all grouping sets arebeneﬁcial and (ii) not all queries can or need to be opti-mized with a grouping set. The beneﬁt criterion considers\nboth aspects to reduce workload execution times.\n5.4 Main Findings\nFor both types of ML models for cardinality estimation, our\naggregation-based training phase offers a database-centric\nway to reduce execution time. Table 1summarizes our eval-\nuation results. The overhead introduced by the constructionof a grouping set is much smaller than the savings in execu-\ntion time. So, grouping sets reduce the workload execution\nab\nFig. 9 Global model evaluation results. aExecution times, bspeed upstimes and amortize their own construction time. Addition-\nally, the usage of indexes is a lways beneﬁcial including\ntheir construction on the grouping sets. The simpler struc-ture of the local model workloads is better supported bygrouping sets because they contain fewer combinations of\ncolumns and fewer distinct values. These are exactly two\nof the assets for grouping sets identiﬁed in Sect. 3.T h i s\nleads to a higher performance speed up for local modelworkloads than for global model workloads with consistent\nhigh-quality estimates. Thus, we can afford a larger amount\nof local models to reach the schema coverage of a globalmodel. Even if these models request more queries than theglobal model, their beneﬁts from the use of grouping setsoutweigh the higher number of queries.\n6 Related Work\nIn this section, we detail the importance of database supportfor machine learning in other works. We look at the mo-tivation for pre-aggregates from both the database systemand the machine learning point of view.\nWhen looking at the synergy of database and machine\nlearning systems, there are three possible interactions: (i)\nintegrate machine learning into database systems, (ii) adaptdatabase techniques for machine learning models, and (iii)combine database and mach ine learning into one life cy-\ncle system [ 24]. Based on that, we classify our work in\ncategory (iii). However, the focus in this area is more onfeature and model selection and not on sampling exampledata. We argue that the direct support of machine learningtraining phases with databases should be treated with the\nsame attention.\nTo the best of our knowledge, there is only little re-\nsearch on directly optimizing the sampling of workloadsfor machine learning problems. The authors of [ 9] detail\ntheir method of speeding up query sampling in [ 25]. They\nuse massive parallelism by distributing the workload overseveral DB instances. We see this as a promising step be-cause our approach can also proﬁt from parallel execution.Especially the instantiation and the querying of grouping\nsets can be done in parallel because grouping sets are or-\nthogonal to each other.\nAnother thing to look at is the availability of supportive\ndata structures in database systems. The cube operators are\nestablished in databases an d beneﬁt from a wide-ranged\nsupport [ 13,20–23]. The ability of a database to deliver\nnecessary meta information is also important. For example,fast querying for the distin ct values of each column has\na large impact on performance. A simple solution for this\nis a dictionary encoding of the data in the database. Some\ndatabase systems already use dictionary coding for all theirdata [ 26]. This is beneﬁcial for our approach because from\nK\n\n56 Datenbank Spektrum (2022) 22:45–57\na dictionary encoded column it is easy to yield the number\nof distinct values with a dictionary scan. Moreover, dictio-nary encoding directly supports the transformation of thedata into the grouping set dimension and the deﬁnition ofranges of these dimensions.\nAside from machine learning, database support for\ndata mining has already been an important researchtopic [ 27–31]. For example, [ 29] identiﬁes that aggre-\ngation in sub-spaces formed by co mbinations of attributes\nis a common task in many data mining algorithms. Based\non that observation, we see a large potential for tightercoupling of databases an d mining algorithms.\n7C o n c l u s i o n\nWe made the case for cardinality estimation as a candidate\nfor database support of machine learning for DBMS. We\ndetailed an approach for pre-aggregating count information\nfor cardinality estimation workloads. It uses grouping sets,a well-known database opera tor, to reduce the data to be\nscanned by example queries for cardinality estimation withmachine learning models. Thi s reduces the execution time\nof a given workload even though we spend extra time to\nconstruct the intermediate data structures. These practicalbeneﬁts are also shown in PostCENN , an extended version\nof PostgreSQL, where we evaluated our approach by in-\ntegrating local models and the Aggregate-based Training\nPhase [32]. This extended PostgreSQL system gives the\nusers the possibility to use and to compare histograms andNNs as cardinality estimators both in quality and perfor-mance. Nevertheless, for the broad applicability of NNs for\ncardinality estimation in database systems, further research\nis necessary in the following areas: (i) how to deal withdata inserts and updates or (ii) how to reduce the trainingtimes even more.\nMoreover, our presented approach in this paper has\na strong potential to be applied to the other similar ma-chine learning problems as well, like plan cost modelingor indexing. In particular, we see parallels between thepotential for machine learning workloads and any of these\nmachine learning problems where information about the\ndata in the DB is aggregated. These parallels make group-ing sets and therefore DB support beneﬁcial for ML forDBMS in general.\nFunding Open Access funding enabled and organized by Projekt\nDEAL.\nOpen Access This article is licensed under a Creative Commons At-\ntribution 4.0 International License, which permits use, sharing, adapta-tion, distribution and reproduction in any medium or format, as long asyou give appropriate credit to the original author(s) and the source, pro-vide a link to the Creative Commons licence, and indicate if changeswere made. The images or other third party material in this article areincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not includedin the article’s Creative Commons licence and your intended use is notpermitted by statutory regulation or ex ceeds the perm itted use, you will\nneed to obtain permission directly from the copyright holder. To viewa copy of this licence, visit http://creativecommons.org/licenses/by/4.\n0/.\nReferences\n1. Leis V , Gubichev A, Mirchev A et al (2015) How good are query\noptimizers, really? Proc VLDB Endow 9(3):204–215\n2. Harmouch H, Naumann F (2017) Card inality estimation: an exper-\nimental survey. Proc VLDB Endow 11(4):499–512\n3. Moerkotte G, Neumann T, Steidl G (2009) Preventing bad plans by\nbounding the impact of car dinality estimation errors. Proc VLDB\nEndow 2(1):982–993\n4. Yousseﬁ K, Wong E (1979) Query processing in a relational\ndatabase management system. In: VLDB, pp 409–417\n5. Fender P, Moerkotte G (2011) A new, highly efﬁcient, and easy\nto implement top-down join enumeration algorithm. In: ICDE, pp864–875\n6. Rosenfeld V , Heimel M, Viebig C et al (2015) The operator variant\nselection problem on heterogeneous hardware. In: ADMS@VLDB,pp 1–12\n7. Damme P, Ungethüm A, Hildebrandt J et al (2019) From a compre-\nhensive experimental survey to a cost-based selection strategy forlightweight integer compression algorithms. ACM Trans DatabaseSyst 44(3):9:1–9:46. https://doi.org/10.1145/3323991\n8. Karnagel T, Habich D, Lehner W (2017) Adaptive work placement\nfor query processing on heterogeneous computing resources. ProcVLDB Endow 10(7):733–744\n9. Kipf A, Kipf T, Radke B et al ( 2019) Learned car dinalities: Esti-\nmating correlated joins with deep learning. In: CIDR\n10. Liu H, Xu M, Yu Z et al (2015) Card inality estimation using neural\nnetworks. In: CASCON, pp 53–59\n11. Woltmann L, Hartmann C, Thiel e M et al (2019) Ca rdinality esti-\nmation with local deep learning models. In: Bordawekar R, ShmueliO (eds) Proceedings of the Second International Workshop on Ex-ploiting Artiﬁcial Intelligence Techniques for Data Management,aiDM at SIGMOD 2019 Amsterdam, The Netherlands, 5 July 2019.vol 10. ACM, pp 5:1–5:8 https://doi.org/10.1145/3329859.3329875\n12. Woltmann L, Hartmann C, Habich D et al (2021) Aggregate-\nbased training phase for ML-based cardinality estimation. In:Sattler K, Herschel M, Lehner W (eds) Datenbanksysteme für\nBusiness, Technologie und Web (BTW 2021), 19. Fachtagung des\nGI-Fachbereichs “Datenbanken und Informationssysteme” (DBIS)Dresden, Germany, 13.-17. September 2021. Proceedings, LNI, volP-311. Gesellschaft für Informatik, Bonn, pp 135–154 https://doi.\norg/10.18420/btw2021-07\n13. Gray J, Bosworth A, Lyaman A et al (1996) Data cube: a relational\naggregation operator generalizing group-by, cross-tab, and sub-to-tals. In: ICDE\n14. IMDb (2017) Imdb: Internet movie database. ftp://ftp.fu-berlin.de/\npub/misc/movies/database/frozendata/ . Accessed 1 Sept 2021\n15. Marcus R, Papaemmanouil O (2019) Plan-structured deep neural\nnetwork models for query performance prediction. Proc VLDB En-dow 12(11):1733–1746. https://doi.org/10.14778/3342263.3342646\n16. Sun J, Li G (2019) An end-to-end learning-based cost estima-\ntor. Proc VLDB Endow 13(3):307–319. https://doi.org/10.14778/\n3368289.3368296\n17. Kraska T, Beutel A, Chi EH et al (2018) The case for learned index\nstructures. In: SIGMOD, pp 489–504\n18. Kipf A (2019) Learned car dinalities in p ytorch. https://github.com/\nandreaskipf/learnedcardinalities/ . Accessed 20 Oct 2021\nK\n\nDatenbank Spektrum (2022) 22:45–57 57\n19. Woltmann L ( 2019) Cardina lity estimation with lo cal deep learning\nmodels. https://github.com/lucaswo/cardest/ . Accessed 20 Oct 2021\n20. Agarwal S, Agrawal R, Deshpande P et al (1996) On the computa-\ntion of multidimen sional aggregates. In: VLDB, pp 506–521\n21. Harinarayan V , Rajaraman A, Ullman JD (1996) Implementing data\ncubes efﬁciently. In: SIGMOD, pp 205–216\n22. Shukla A, Deshpande P, Naughton JF et al (1996) Storage estima-\ntion for multidimensional aggregates in the presence of hierarchies.In: VLDB, pp 522–531\n23. Zhao Y , Deshpande P, Naughton JF (1997) An array-based algo-\nrithm for simultaneous multidimensional aggregates. In: SIGMOD,pp 159–170\n24. Kumar A, Boehm M, Yang J (2017) Data management in machine\nlearning: challenges, techniques, and systems. In: SIGMOD, pp1717–1722\n25. Kipf A, V orona D, Müller J et al (2019) Estim ating cardinalities\nwith deep sketches. In: SIGMOD, pp 1937–1940 https://doi.org/10.\n1145/3299869.332021826. Färber F, May N, Lehner W et al (2012) The SAP HANA database\n– an architecture overview. IEEE Data Eng Bull 35:28–33\n27. Agrawal R, Shim K (1996) Developing tightly-coupled data mining\napplications on a relational database system. In: KDD, pp 287–290\n28. Cho C, Wu Y , Chen ALP (2009) Effective database transformation\nand efﬁcient support computation for mining sequential patterns.J Intell Inf Syst 32(1):23–51\n29. Hinneburg A, Lehner W, Habich D (2003) Combi-operator:\ndatabase support for data mining applications. In: VLDB, pp429–439\n30. Netz A, Chaudhuri S, Fayyad UM et al (2001) Integrating data min-\ning with SQL databases: OLE DB for data mining. In: ICDE, pp379–387\n31. Ordonez C, Cereghini P (2000) SQLEM: fast clustering in SQL\nusing the EM algorithm. In: SIGMOD, pp 559–570\n32. Woltmann L, Olwig D, Hartmann C et al (2021) PostCENN: post-\ngresql with machine learning models for cardinality estimation.Proc VLDB Endow 14(12):2715–2718\nK",
  "textLength": 50917
}