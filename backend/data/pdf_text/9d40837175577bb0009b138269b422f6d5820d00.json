{
  "paperId": "9d40837175577bb0009b138269b422f6d5820d00",
  "title": "Transformer Memory as a Differentiable Search Index",
  "pdfPath": "9d40837175577bb0009b138269b422f6d5820d00.pdf",
  "text": "Transformer Memory as a\nDifferentiable Search Index\nYi Tay\u0003, Vinh Q. Tran\u0003, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta\nZhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster\nWilliam W. Cohen, Donald Metzler\nGoogle Research\n{yitay,vqtran,metzler}@google.com\nAbstract\nIn this paper, we demonstrate that information retrieval can be accomplished with\na single Transformer, in which all information about the corpus is encoded in\nthe parameters of the model. To this end, we introduce the Differentiable Search\nIndex (DSI), a new paradigm that learns a text-to-text model that maps string\nqueries directly to relevant docids; in other words, a DSI model answers queries\ndirectly using only its parameters, dramatically simplifying the whole retrieval\nprocess. We study variations in how documents and their identiÔ¨Åers are represented,\nvariations in training procedures, and the interplay between models and corpus\nsizes. Experiments demonstrate that given appropriate design choices, DSI signiÔ¨Å-\ncantly outperforms strong baselines such as dual encoder models. Moreover, DSI\ndemonstrates strong generalization capabilities, outperforming a BM25 baseline in\na zero-shot setup.\n1 Introduction\nInformation retrieval (IR) systems map a user query q2Q to a ranked list of relevant documents\nfd1; : : : ; d ng\u0012D , typically represented by integers or short strings called document identiÔ¨Åers\n(docids). The most widely used IR approaches are based on pipelined retrieve-then-rank strategies.\nFor retrieval, approaches based on inverted indexes or nearest neighbor search are common where\ncontrastive learning based dual encoders (DEs) (Gillick et al., 2018; Karpukhin et al., 2020; Ni et al.,\n2021) are the present state-of-the-art.\nThis paper proposes an alternative architecture, wherein a sequence-to-sequence (seq2seq) learning\nsystem (Sutskever et al., 2014) is used to directly map a query qto a relevant docid j2Y. This\nproposal is shown in the bottom half of Figure 1, for a sequence-to-sequence encoder-decoder\narchitecture.\nWe call this proposed architecture a differentiable search index (DSI), and implement it with a\nlarge pre-trained Transformer (Vaswani et al., 2017) model, building on the recent success of large\ngenerative language models (LMs) (Brown et al., 2020; Raffel et al., 2019; Devlin et al., 2018;\nThoppilan et al., 2022; Du et al., 2021). In this proposed architecture, all information of the corpus is\nencoded within the parameters of the Transformer language model.\nAt inference time, the trained model takes as input a text query qand outputs a docid j. If desired,\nbeam search can be used to produce a ranked list of potentially-relevant docids. As we show, this\nprocess can work surprisingly well when trained properly. In our experiments it can consistently\noutperform DE baselines, sometimes drastically: for a base-sized T5 model, Hits@1 on the smallest\ncorpus is improved by more than 20 points, from 12.4% for a DE to 33.9% for DSI; and on a corpus\n\u0003Co-leads.\n36th Conference on Neural Information Processing Systems (NeurIPS 2022), New Orleans, LA, USA.arXiv:2202.06991v3  [cs.CL]  21 Oct 2022\n\nWho is the author of tipping the velvet?‚Ä¶ As part of her research ‚Ä¶ Sarah Waters came across the title of her first book, Tipping the Velvet. ‚Ä¶‚Ä¶.  Keeley Hawes is known for her roleasKitty Butler in Tipping the Velvet, ‚Ä¶query123doc456doc137EncoderLearning to encodeRetrieveEncoder‚Ä¶Who is the author of tipping the velvet?‚Ä¶ As part of her research ‚Ä¶ Sarah Waters came across the title of her first book, Tipping the Velvet. ‚Ä¶‚Ä¶.  Keeley Hawes is known for her roleasKitty Butler in Tipping the Velvet, ‚Ä¶query123doc456doc137EncoderLearning to encode and retrieve1.doc4562.doc283‚Ä¶Rank List\ndoc456doc456doc137Decoder1.doc4562.doc283‚Ä¶Rank ListBeam Search ùëâ!\"#ùëâ$%&ùëâ$'%ùëâ$'%ùëâ'(%ùëâ!\"#ùëâ$%&ùëâ$'%ùëâ!\"#Contrastive loss during trainingquery123Maximal Inner Product SearchFigure 1: Comparison of dual encoders (top) to differentiable search index (bottom).\nTable 1: Information retrieval requires a series of decisions, associated with the subproblems of\ndocument representation, indexing, and retrieval. Structured-document variants of DSI are also\nsensitive to a fourth decision, namely how docids are represented.\nBM25 or TFIDF Dual Encoder (DE) Differentiable Search Index (DSI)\ndoc/query rep. sparse vdjvector in RjVjdense vdjvector in RdVarious (see Section 3.1.2)\ndocid rep. \u0000 \u0000 Various (see Section 3.2)\nindexing build inverted index mapping build table mapping train model (see Section 3.1.1)\neach term t!fdj1; : : : ; d jkg each docvec vdj!j to map dj!j\nretrieval approximate sparse matmul approximate MIPS run trained model\n(top-1) to Ô¨Ånd argmaxjvT\nqvdj to Ô¨Ånd argmaxjvT\nqvdj to Ô¨Ånd argmaxjPr(jjq)\n30\u0002larger, performance is improved by nearly 7 points. These gains increase when larger models are\nused: for an 11B-parameter T5 model, Hits@1 performance improves by more than 25 points over\nDE on the small corpus, and more than 15 points on the large corpus. DSI also performs extremely\nwell in a zero-shot setting, e.g., improving Hits@1 by 14 points over BM25.\nIn addition to these quantitative gains, the DSI architecture is much simpler than a DE (see Table 1).\nA DE system Ô¨Åxes a search procedure (MIPS) and learns internal representations that optimize\nperformance for that search procedure; in contrast, a DSI system contains no special-purpose Ô¨Åxed\nsearch procedure, instead using standard model inference to map from encodings to docids.\nOf particular interest to the machine learning community, as Table 1 shows, in DSI allaspects of\nretrieval are mapped into well-understood ML tasks. This may lead to new potential approaches to\nsolving long-standing IR problems. As one example, since indexing is now a special case of model\ntraining, incrementally updating an index becomes a special case of model updating (Sun et al., 2020).\nIn this paper, DSI is applied to moderate-sized corpora (from 10k to 320k documents), all of which\nare derived from one challenging retrieval task, and we leave the important question of the scaling\nDSI to larger corpora to future work. The task considered is retrieving supporting passages given\nquestions from the Natural Questions (NQ) dataset, a challenging task for lexical models.\nWhile the idea of DSI is simple, there are a number of ways it can be realized, some of which work\nsurprisingly well, and some of which work surprisingly poorly. Below we explore a number of\nvariations of the DSI architecture.\nDocument representation. We explore several approaches to representing documents, including a\n‚Äúnaive‚Äù approach of using the document‚Äôs full text, as well as variants of the bag-of-words representa-\ntion used by traditional IR engines.\nDocid representation. We look at several ways to represent docids. In addition to naively representing\nintegers as text strings, we also consider unstructured atomic docids , where each document is assigned\na unique token, and some simple baselines for constructing structured semantic docids that describe\n2\n\nhow to navigate to a document through a hierarchical clustering of the corpus. Structured docids‚Äî\neither semantically structured via clustering, or naively structured as tokenized integers‚Äîscale better\nto large corpora, since the size of the vocabulary used in the decoder is made larger.\nIndexing. A trainable IR system traditionally has two phases: indexing a corpus (i.e., memorizing\ninformation about each document), and learning how to effectively retrieve from the index. In DSI,\nthe index is stored in the model parameters, and indexing is simply another kind of model training.\nFigure 1 suggests one approach to indexing a corpus: namely, to train on (1) examples (dj; j)that\npair document djwith its docid j, in addition to (2) examples (q; j)that pair a query qwith a relevant\ndocid j. In this setup the examples of type (1) are ‚Äúindexing‚Äù examples.\nWhile it is clear that examples of type (2) alone do not provide enough information for a system to\ngeneralize to novel retrievals, there are many alternatives to examples of type (1) that might plausibly\n‚Äúteach‚Äù a model about the associations between documents and docids. We explore a number of these\nbelow, and show that some plausible-seeming techniques perform very poorly. We also explore a\nnumber of alternative multi-task optimization and curriculum learning schemes for combining these\ntypes of examples.\nEffects of model and corpus size. Since recent results suggest that some properties of large LMs\nemerge only for very large model sizes Brown et al. (2020), we explore the performance of DSI for a\nrange of model sizes and corpus sizes of 10k, 100k, and 320k documents.\nSummary. We show that even naive representations for documents and docids, coupled with appropri-\nate training procedures to Ô¨Åne-tune modern large LMs, can perform surprisingly well; we present\ntwo improved docid representations, unstructured docids and semantically-structured docids, which\nimprove the naive representation choice. We show that there is substantial variation in performance\namong indexing/training strategies and we show that performance of DSI signiÔ¨Åcantly and consis-\ntently improves with model scale. To our knowledge this is the Ô¨Årst case of generative indexing\nimproving performance over strong baselines for a well-studied document retrieval task.\n2 Related Work\nDe Cao et al. (2020) describe a related sequence-to-sequence system called autoregressive entity\nlinking , in which documents mentioning an entity‚Äîperhaps implicitly, e.g., by posing a question\nto which that entity is an answer‚Äîare mapped to a canonical name of that entity. In the case of\nWikipedia, canonical entity names correspond to page titles, so this could be viewed as a sort of\ndocument retrieval. This approach has been adapted to other purposes, such as generating knowledge\nbase triples in canonical form (Josifoski et al., 2021). The task we consider is different from those\nconsidered in autoregressive entity linking: our goal is to retrieve a document containing the answer,\nrather than a document whose title is the answer. More importantly, in autoregressive entity linking\nthe generation target is a semantically meaningful name , whereas we allow targets to be arbitrary\ndocids . This makes our approach applicable to general retrieval tasks, but raises new questions about\ndocid representation and indexing strategies.\nIn autoregressive entity linking, generation is constrained to return an output from a Ô¨Åxed set. It\nwould be feasible to constrain DSI generation outputs to be valid docids. Although we do not use this\ntechnique, the degree to which this might improve performance is a worthwhile question.\nThere is a large body of work on retrieval augmented generation , i.e., retrieving auxiliary documents\nto enhance language models (Borgeaud et al., 2021; Guu et al., 2020). These techniques are useful for\nmany tasks including question-answering, but rely on traditional retrieval methods such as DEs. Here\nwe use generation to replace a retrieval process, rather than using retrieval to augment a generation\nprocess.\nDual encoders (Dehghani et al., 2017; Gillick et al., 2018; Gao et al., 2021; Ni et al., 2021; Karpukhin\net al., 2020) are a well-established paradigm for retrieval. The key idea is produce query and document\nembeddings independently and perform a similarity retrieval in vector space across all embedding\npairs. Query and candidate documents are produced by a sequence encoder and training is performed\nusing a form of contrastive loss.\nThe interpretation of a large Transformer model as a memory store have been investigated in prior\nwork. (Roberts et al., 2020) demonstrated success on a closed-book QA task whereby they train\n3\n\nT5 models to retrieve facts that are encoded within the parameters of the model during pretrain-\ning. However, different from CBQA, the presented problem here in this paper is to retrieve full\ndocuments based on docids instead of generating direct answers. Meanwhile, (Petroni et al., 2019)\nalso investigated language models as knowledge bases and found that pretrained LMs may already\ncontain relational knowledge. (Geva et al., 2020) analyzes the knowledge encoded within Transformer\nfeedforward layers. There have been also works that demonstrate the relation of Transformers to\nassociative memory and HopÔ¨Åeld networks (Ramsauer et al., 2020), which reinforce the notion that\nTransformers should intuitively serve as a good associative memory store or search index.\n3 Differentiable Search Index\nThe core idea behind the proposed Differentiable Search Index (DSI) is to fully parameterize tradi-\ntionally multi-stage retrieve-then-rank pipelines within a single neural model. To do so, DSI models\nmust support two basic modes of operation:\n‚Ä¢Indexing : a DSI model should learn to associate the content of each document djwith its\ncorresponding docid j. This paper utilizes a straightforward sequence-to-sequence (seq2seq)\napproach that takes document tokens as input and generates identiÔ¨Åers as output.\n‚Ä¢Retrieval : Given an input query, a DSI model should return a ranked list of candidate docids.\nHere, this is achieved with autoregressive generation.\nFollowing these two operations, a DSI model can be trained to index a corpus of documents and\noptionally Ô¨Åne-tune on an available set of labeled data (queries and labeled documents), and thereafter\nused to retrieve relevant documents‚Äîall within a single, uniÔ¨Åed model. As opposed to retrieve-then-\nrank approaches, this type of model allows for simple end-to-end training and can easily be used as a\ndifferentiable sub-component of a larger, more complex neural model.\n3.1 Indexing Strategies\nWe investigate various indexing strategies that are meant to learn associations between documents\nand their identiÔ¨Åers. We train our model to predict docids given a sequence of document tokens. This\nallows our model to learn which identiÔ¨Åer belongs to which document and can be thought of as a\ndifferentiable take on traditional search indexes. We consider various alternatives and ablate these\nsettings in subsequent sections. The Ô¨Ånal strategy employed was Inputs2Targets with direct indexing.\n3.1.1 Indexing Method\nThis section discusses the indexing task variants that we consider.\nInputs2Target We frame this as a seq2seq task of doc_tokens!docid . As its name suggests,\nthis binds the docids to the document tokens in a straightforward inputs-to-targets fashion. The\nadvantage here is that the identiÔ¨Åer is the denoising target, which puts it in closer proximity to the\nloss function. Since the retrieval task is also concerned with predicting identiÔ¨Åers, this formulation\nallows the network to follow a similar input-target balance in terms of sequence length. A potential\nweakness is that the document tokens are not denoising targets and therefore there is no opportunity\nfor general pre-training on document tokens.\nTargets2Inputs This formulation considers the opposite of the above, i.e., generating document\ntokens from identiÔ¨Åers, i.e., docid!doc_tokens . Intuitively, this is equivalent to training an\nautoregressive language model that is conditioned on the docid.\nBidirectional This formulation trains both Inputs2Targets and Targets2Inputs within the same\nco-training setup. A preÔ¨Åx token is prepended to allow the model to know which direction the task is\nbeing performed in.\nSpan Corruption We also explored a setup that performs span corruption-based denoising (Raffel\net al., 2019) with the inclusion of docid tokens. In this approach, we concatenate the identiÔ¨Åer to the\ndocument tokens as a preÔ¨Åx that can be randomly masked as spans in the span corruption objective.\n4\n\nThis method has the advantage of (1) also performing general pre-training during indexing and (2)\nachieving a good balance of docids as denoising targets and inputs.\n3.1.2 Document Representation Strategies\nIn the previous section, we explored ‚Äúhow to index‚Äù . This section investigates ‚Äúwhat to index?‚Äù ,\ni.e., how to best represent doc_tokens . We state our options here and carefully ablate them in our\nexperiments later. The best option in the end was the direct indexing method.\nDirect Indexing This strategy represents a document exactly. We take the Ô¨Årst Ltokens of a\ndocument, with sequential order preserved, and associate them with the docid.\nSet Indexing Documents may contain repeated terms and/or non-informative words (e.g., stop-\nwords). This strategy de-duplicates repeated terms using the default Python set operation and\nremoves stopwords from the document. The rest of the document after Ô¨Åltering is passed into the\nmodel in similar fashion to the direct index.\nInverted Index This strategy maps chunked documents (contiguous blocks of tokens) instead of\nentire documents directly to the docid. We randomly subsample a single contiguous chunk of k\ntokens and associate them with the docid. The key advantage of this approach is to allow looking\nbeyond the Ô¨Årst ktokens.\n3.2 Representing Docids for Retrieval\nRetrieval within seq2seq-based DSI models is accomplished by decoding docids given an input query.\nHow to do this decoding in an effective way largely depends on how docids are represented in the\nmodel. The remainder of this section explores a number of possible ways for representing docids and\nhow to handle decoding for each.\nUnstructured Atomic IdentiÔ¨Åers The most naive way to represent documents is assign each an\narbitrary (and possibly random) unique integer identiÔ¨Åer. We refer to these as unstructured atomic\nidentiÔ¨Åers .\nWith these identiÔ¨Åers, an obvious decoding formulation is to learn a probability distribution over the\nidentiÔ¨Åers. In this case, models are trained to emit one logit for each unique docid ( jNdocumentsj).\nThis is analogous to the output layer in standard language models, but extended to include docids.\nTo accommodate this, we extend the output vocabulary of a standard language model as follows:\nO=Softmax ([Wtokens ;Wdocs]Thlast)\nwhere [; ]is the row-wise concatenation operator, Wtokens2Rdmodel\u0002jNtokensjandWdocs2\nRdmodel\u0002jNdocumentsj.hlastis the last layer‚Äôs hidden state ( 2Rdmodel ) of the decoder stack. To\nretrieve the top-k documents for a given query, we simply sort the output logits and return the corre-\nsponding indices. This is also reminiscent of standard listwise learning to rank where all documents\nare considered at once.\nNaively Structured String IdentiÔ¨Åers We also consider an ostensibly absurd approach that treats\nunstructured identiÔ¨Åers, i.e., arbitrary unique integers, as tokenizable strings. We refer to these as\nnaively structured identiÔ¨Åers .\nIn this formulation, retrieval is accomplished by decoding a docid string sequentially one token at\na time. This eliminates the need for the large softmax output space that comes with unstructured\natomic identiÔ¨Åers. It also eliminates the need to learn embeddings for each individual docid.\nWhen decoding, beam search is used to obtain the predicted best docid. With this strategy, it is less\nstraightforward to obtain a top-k ranking. One could exhaustively comb through the entire docid\nspace and obtain the likelihood of each docid given the query. Instead, we use the partial beam search\ntree to construct top-k retrieval scores. We Ô¨Ånd this approximation to be quite efÔ¨Åcient and effective\nin practice.\n5\n\nSemantically Structured IdentiÔ¨Åers All of the approaches for representing docids thus far as-\nsumed that the identiÔ¨Åers are assigned in an arbitrary manner. While exploring the limits of arbitrary\nidentiÔ¨Åers is quite interesting, it is only intuitive that imbuing the docid space with semantic structure\ncan lead to better indexing and retrieval capabilities. As such, this section explores semantically\nstructured identiÔ¨Åers .\nSpeciÔ¨Åcally, we aim to automatically cre-\nate identiÔ¨Åers that satisfy the following\nproperties: (1) the docid should capture\nsome information about the semantics of its\nassociated document, (2) the docid should be\nstructured in a way that the search space is\neffectively reduced after each decoding step.\nThis results in identiÔ¨Åers where semantically\nsimilar documents share identiÔ¨Åer preÔ¨Åxes.\nIn this work, we treat this as a fully unsuper-\nvised pre-processing step. However, as part\nof future work it may be possible to integrate\nand automatically learn semantic identiÔ¨Åers\nin a fully end-to-end manner.\nTo construct identiÔ¨Åers with this property,\nwe employ a simple hierarchical clustering\nprocess over document embeddings to in-\nduce a decimal tree (or more generally, a trie).\nGiven a corpus to be indexed, all documents\nare clustered into 10 clusters. Each document\nis assigned an identiÔ¨Åer with the number of\ntheir cluster from 0-9. For every cluster con-\ntaining more than cdocuments, the algorithm\nis applied recursively, with the next level‚Äôs\nresult (the remaining sufÔ¨Åx of the identiÔ¨Åer)\nappended to the existing identiÔ¨Åer.\nFigure 2: Visual example of a hierarchical cluster-\ning process used to assign semantically structured\nidentiÔ¨Åers. During inference, beam search navi-\ngates this trie to decode the correct docid.\nAlgorithm 1 Generating semantically structured\nidentiÔ¨Åers. (Referenced in Section 3.2.)\nInput: Document embeddings X1:N, where Xi2Rd\nOutput: Corresponding docid strings J1:N\nfunction GENERATE SEMANTIC IDS(X1:N)\nC1:10 Cluster (X1:N; k= 10)\nJ empty list\nfori= 0 to9do\nJcurrent [i]\u0003jCi+1j\nifjCi+1j> c then\nJrest GENERATE SEMANTIC IDS(Ci+1)\nelse\nJrest [0; : : : ;jCi+1j\u00001]\nend if\nJcluster elementwiseStrConcat( Jcurrent ,Jrest )\nJ J:appendElements (Jcluster )\nend for\nJ reorderToOriginal (J; X 1:N; C1:10)\nreturn J\nend function\nFor clusters with cdocuments or less, each element is assigned an arbitrary number from 0 to at\nmost c\u00001and likewise its digits are appended to the existing identiÔ¨Åer. Although this speciÔ¨Åc\nprocess induces a decimal tree, it is possible to induce similar types of tries using any number of\nother reasonable strategies.In practice, we simply apply k-means over embeddings generated by a\nsmall 8-layer BERT model, with c= 100 . We include pseudo-code for this process in Algorithm 1.\n3.3 Training and Optimization\nThe DSI models that we train are optimized for seq2seq cross entropy loss and are trained with teacher\nforcing. We explored two main strategies for training DSI models. The Ô¨Årst and more straightforward\nstrategy is to Ô¨Årst train a model to perform indexing (memorization), followed by a Ô¨Åne-tuning stage\nwhere the trained model is used to map queries to docids (e.g., retrieval). The second strategy is to\ntrain them together in a multi-task setup. To this end, we frame co-training tasks in similar fashion to\nT5-style co-training (e.g., using task prompts to differentiate them). The latter performed signiÔ¨Åcantly\nbetter, especially when the proportion of indexing to retrieval task examples is high. Hence, we\nadopted multi-task learning as the default strategy.\nHere, we make the observation that our setup is unique and unlike traditional multi-task learning\nor transfer learning. In typical multi-task setups, two tasks have shared commonalities that could\nimprove the performance of both tasks if they were learned together. However, in our setup, the\nretrieval task is completely dependent on the indexing task. In particular, without the indexing task,\nthe identiÔ¨Åers leveraged by the retrieval task would be completely meaningless. Hence, in order\nto solve task B (retrieval), the model needs to learn task A (indexing) well enough. This problem\n6\n\nsetup presents unique and largely unexplored research challenges that might be of interest to the ML\ncommunity.\n4 Experiments\nIn this section, we discuss our experimental setup, datasets used and baselines compared. We also\ndiscuss experimental results, Ô¨Åndings and effect of various strategies discussed in earlier sections\nof the paper. Since this is fairly new concept, this work aims to put forth a proof-of-concept and\nseeks to answer research questions instead of making a ‚Äòsotaeesque‚Äô comparison. We leave extensive\ncomparisons on other setups and baselines to future work.\nDataset We conduct our experiments on the challenging Natural Questions (NQ) (Kwiatkowski\net al., 2019) dataset. NQ consists of 307K query-document training pairs and 8K validation pairs,\nwhere the queries are natural language questions and the documents are Wikipedia articles. Given a\nquestion, the retrieval task is to identify the Wikipedia article that answers it. For evaluating how\nDSI models perform at different scales, we construct three sets from NQ to form our testbed, namely\nNQ10K, NQ100K, and NQ320K denoting different numbers of total query-document pairs in the\ncombined train and validation splits. NQ320K is the full NQ set and uses its predetermined training\nand validation split for evaluation purposes. Unlike NQ320K, NQ10K and NQ100K constructs\nrandomly sampled validation sets. For all datasets, we use the same docid space/budget of 320K\ntokens for all unstructured atomic and naively structured identiÔ¨Åer experiments. Semantically\nstructured identiÔ¨Åers are generated separately for each dataset so as to prevent leakage of semantic\ninformation from larger splits into smaller ones. Text is lowercased. Note that there exists fewer\nunique documents than query-document pairs in these datasets. Please refer to Table 4 (Appendix)\nwhich reports the statistics of these datasets.\nMetrics We evaluate our models on Hits@N where N= f1;10g. This metric reports the proportion\nof correct documents ranked in the top Npredictions.\nImplementation Details All DSI models are initialized using standard pretrained T5 (Raffel\net al., 2019) model conÔ¨Ågurations. The conÔ¨Ågurations names and corresponding number of model\nparameters are: Base (0.2B), Large (0.8B), XL (3B) and XXL (11B). For unstructured atomic\nidentiÔ¨Åers runs, we initialize the identiÔ¨Åers randomly as new parameters and only Ô¨Ånetune the weights\nduring the indexing stage. We use the Jax/T5X2implementation for our experiments. The DSI\nmodels are trained for a maximum of 1M steps using a batch size of 128. We pick the best checkpoint\nbased on retrieval validation performance. Our training hardware consists of 128-256 TPUv4 chips\nfor models above 1B parameters and 64-128 TPUv3 or TPUv4 chips otherwise. As an estimate,\nmodels above 1B parameters typically take about at least a full day for convergence for NQ320K.\nWe tune the learning rate amongst f0:001;0:0005gand linear warmup amongst f10K, 100K, 200K,\n300Kgand/or none. Semantically structured identiÔ¨Åers are generated using an 8-layer BERT (Devlin\net al., 2018) model3, and the default k-means clustering in scikit-learn. Based on our early ablation\nexperiments of various DSI setting, the main results presented use direct indexing ( L= 32 ) and\ntheInputs2Targets indexing strategy. We present results for all the docid representation methods.\nFollowing the main results, we present our ablation studies.\n4.1 Baselines\nFor baselines, we use T5-based dual encoders implemented by (Ni et al., 2021). We use the gensim4\npackage for computing BM25 scores. For the T5-based dual encoders, we train with contrastive\nlearning on the NQ pairs until convergence ( \u001910K steps) and obtain top-k nearest neighbors with a\nsystem similar to ScaNN (Guo et al., 2020). For zero-shot retrieval, we also compare with a state-of-\nthe-art unsupervised baseline, Sentence T5 (Ni et al., 2021) which have been specially pre-trained\nwith a similarity learning task. There two reasons why we consider (Ni et al., 2021) the relevant dual\nencoder baseline for this work rather than other dense retrieval works such as DPR (Karpukhin et al.,\n2https://github.com/google-research/t5x\n3https://tfhub.dev/google/collections/bert\n4https://pypi.org/project/gensim\n7\n\nTable 2: Experimental results on NQ document retrieval. DSI outperforms BM25 and Dual Encoder\nbaselines. Among all the Docid representation methods, Semantic String Docids perform the best.\nNQ10K NQ100K NQ320K\nModel Size Params Method Hits@1 Hits@10 Hits@1 Hits@10 Hits@1 Hits@10\nBM25 - - - 12:4 33 :5 20:9 46 :4 11:6 34 :4\nT5 Base 220M Dual Encoder 16:2 48 :6 18:7 55 :2 20:5 58 :3\nT5 Large 800M Dual Encoder 18:8 55 :7 22:3 60 :5 22:4 63 :3\nT5 XL 3B Dual Encoder 20:8 59 :6 23:3 63 :2 23:9 65 :8\nT5 XXL 11B Dual Encoder 22:1 61 :6 24:1 64 :5 24:3 67 :3\nDSI Base 250M Atomic Docid 13:0 38 :4 23:8 58 :6 20:7 40 :9\nDSI Large 800M Atomic Docid 31:3 59 :4 17:1 52 :3 11:6 37 :6\nDSI XL 3B Atomic Docid 40:1 76 :9 19:0 55 :3 28:1 61 :9\nDSI XXL 11B Atomic Docid 39:4 77 :0 25:3 67:9 24:0 55 :1\nDSI Base 250M Naive String Docid 28:1 48 :0 18:7 44 :6 6:7 21 :0\nDSI Large 800M Naive String Docid 34:7 60 :5 21:2 50 :7 13:3 33 :6\nDSI XL 3B Naive String Docid 44:7 66 :4 24:0 55 :1 16:7 58 :1\nDSI XXL 11B Naive String Docid 46:7 77:9 27:5 62:4 23:8 55 :9\nDSI Base 250M Semantic String Docid 33:9 57 :3 19:0 44 :9 27:4 56 :6\nDSI Large 800M Semantic String Docid 37:5 65 :1 20:4 50 :2 35:6 62 :6\nDSI XL 3B Semantic String Docid 41:9 67 :1 22:4 52 :2 39:1 66 :8\nDSI XXL 11B Semantic String Docid 48:5 72:1 26:9 59 :5 40:4 70 :3\nTable 3: Experimental results on Zero-Shot NQ document retrieval. DSI outperforms BM25, T5\nembeddings and SentenceT5, the state-of-the-art for unsupervised similarity modeling. Among Docid\nrepresentation method, the Atomic Docid performs the best on zero-shot learning.\nNQ10K NQ100K NQ320K\nModel Size Method Hits@1 Hits@10 Hits@1 Hits@10 Hits@1 Hits@10\nBM25 - - 12:4 33 :5 20:9 46 :4 11:6 34 :4\nT5 XXL Dual Encoder 0:3 1 :3 1:9 8 :0 1:1 5 :9\nSentenceT5 Large Dual Encoder 17:6 50 :7 17:4 50 :8 16:9 51 :0\nDSI XXL Atomic Docid 25:7 60 :1 23:0 57 :3 25:1 56 :6\nDSI XXL Naive String Docid 43:4 67 :4 17:4 41 :5 9:2 22 :6\nDSI XXL Semantic String Docid 43:9 68 :8 11:4 26 :6 13:9 31 :1\n2020). Firstly, we employ the exact identical pretrained model, which allows systematic ablation of\nthe proposed approach without conÔ¨Çating other factors. ScientiÔ¨Åcally, we believe this comparison\nagainst Ô¨Åne-tuned T5 is the best apples to apples comparison that we provide. Secondly, Ô¨Åne-tuned T5\ndual encoders are considered to be architecturally and methodologically very identical to DPR (with\nsome minor differences such as parameter sharing but use the same concept of in-batch negatives).\n4.2 Experimental Results\nTable 2 reports retrieval results for NQ10K, NQ100K, and NQ320K with Ô¨Ånetuning and Table 3\nreports zero-shot retrieval results. For zero-shot retrieval, the model is only trained on the indexing\ntask and not the retrieval task, so the model sees no labeled query !docid data points. Section 7.2 of\nthe Appendix reports extended results regarding the indexing performance and training dynamics of\nDSI.\nSupervised Finetuning Results Our results show that DSI outperforms DE across all dataset sizes.\nOn the small dataset (NQ10K), the performance gap between DSI and DE is large, e.g., the best DSI\nvariant outperforms DE by 2 times. On NQ100K, the gap becomes less prominent with the best DSI\nmodel (unstructured atomic identiÔ¨Åers) outperforming DE by +5% Hits@1 and Hits@10. On the\nlarge dataset (NQ320K), the best DSI model (structured semantic identiÔ¨Åers) outperform the best DE\nmodel by +66% relative Hits@1 and +4:5%Hits@10.\nZero-Shot Results Table 3 reports results on zeros-shot retrieval. Recall that zero-shot retrieval is\nperformed by only performing indexing and not the retrieval task. In other words, the model does\nnot see any annotated query or document pairs. Generally, the best result is obtained by DSI with\n8\n\nFigure 3: Scaling plots for DSI vs. DE\nacross model sizes. Performance refers to\nthe Hits@1 metric.\nFigure 4: Effect of multi-task ratio of index-\ning to retrieval examples.\nFigure 5: Performance of different document representations. (Referenced in Section 4.2.)\nunstructured atomic identiÔ¨Åers on both NQ100K and NQ320K. The best performance on all NQ\ndatasets outperform well-established unsupervised retrieval baselines such as BM25. Moreover, DSI\noutperforms unsupervised representation learning methods such as SentenceT5 (Ni et al., 2021),\nwhich is trained to learn similarity-aware representations via contrastive learning. We also note that\nraw T5 embeddings perform extremely poorly and do not produce reasonable results on the task\nof unsupervised retrieval. Given that it is generally difÔ¨Åcult for an unsupervised neural method to\noutperform BM25, we Ô¨Ånd these early results very encouraging.\nDocument IdentiÔ¨Åers One key research question in this paper is the crucial choice of how to\nrepresent docids. Generally, we Ô¨Ånd that structured semantic identiÔ¨Åers are helpful and improve\nover unstructured identiÔ¨Åers. When comparing naive versus semantic string identiÔ¨Åers, it seems\nimperative to use semantic identiÔ¨Åers if possible. This is intuitive, since imbuing the target space\nwith semantic structure can facilitate greater ease of optimization and additional unsupervised\nrepresentation learning methods as external knowledge. The competitiveness of unstructured atomic\nidentiÔ¨Åers is somewhat mixed and we had some difÔ¨Åculty optimizing such models. We hypothesize\nthat this could possibly be because of the the newly initialized softmax layer and that training such\na system from scratch would mitigate these issues. However, we defer this line of investigation to\nfuture work. In lieu of the instability and high variance of the unstructured atomic identiÔ¨Åers, the\nperformance is not consistent across the different datasets. Moreover, these docids might also run\ninto intermittent non-convergence which we trace back to an optimization related quirk. However,\nwe also note that unstructured atomic identiÔ¨Åers perform the best, by a wide margin, on the zero-shot\nretrieval setup and achieve performance often more than double than that of beam decoding methods.\nIndexing Strategies In this section, we explore the effect of different indexing methods (Sec-\ntion 3.1.1). We run experiments on NQ100K with the different indexing strategies described earlier.\nModels are trained using the Naive Docid method. Without indexing, the model achieves 0%\nHits@1. This is intuitive, since the Docids are not meaningful without the indexing task. Secondly,\ntheInputs2Targets andBidirectional formulation performs the best, with the bidirectional\nmethod performing slightly worse (13.5 vs 13.2) compared to the former. Finally, the accuracy with\nTargets2Inputs and Span Corrpution with Docids yield no meaningful results ( 0%accuracy). This\n9\n\ngoes to show that there can be huge variance across indexing strategies whereby some strategies work\nreasonably well and some completely do not work at all.\nDocument Representations In this section, we explore the performance of the different document\nrepresentation strategies described in Section 3.1.2. Figure 5 reports the results on NQ320K. Overall,\nwe Ô¨Ånd that the direct indexing approach works the best. We also Ô¨Ånd that it is difÔ¨Åcult to train the\ninverted index method since the docid is repeatedly exposed to different tokens. We also Ô¨Ånd that\nshorter document lengths seem to work well where performance seems to substantially dip beyond\n64tokens suggesting that it might be harder to optimize or efÔ¨Åciently memorize when there are a\nlarger number of document tokens. Finally, we also Ô¨Ånd that there was no additional advantage in\napplying set processing or stopwords preprocessing to the document tokens.\nScaling Laws Another interesting insight is how the scaling law of DSI differs from Dual Encoders.\nUnderstanding the scaling behaviour of Transformers have garnered signiÔ¨Åcant interest in recent\nyears (Kaplan et al., 2020; Tay et al., 2021; Abnar et al., 2021). We Ô¨Ånd that the gain in retrieval\nperformance obtained from increasing model parameterization in DE seems to be relatively small.\nConversely, the scaling properties of DSI seems to be more optimistic.\nFigure 3 plots the scaling behaviour (log scale) of three methods (DE and DSI with naive and\nsemantic IDs). DSI (naive) strongly beneÔ¨Åts from scale going from base to XXL and seems to still\nhave headroom for improvement. Meanwhile, DSI (semantic) starts off equally competitive as DE\nbase but performs much better with scale. DE models, unfortunately are more or less plateaued at\nsmaller parameterization.\nInterplay Between Indexing and Retrieval Our early experiments showed that Ô¨Årst learning\nthe indexing task and then learning the retrieval task in a sequential manner results in mediocre\nperformance. There, we focused on exploring good ratios rfor co-training the indexing and retrieval\ntasks together using multi-task learning. Figure 4 shows the effect of modifying the ratio of indexing\nto retrieval samples. We Ô¨Ånd the optimization process is signiÔ¨Åcantly inÔ¨Çuenced by the interplay\nbetween the indexing and retrieval tasks. Setting rtoo high or low generally resulted in poor\nperformance. We Ô¨Ånd that a rate of 32 generally performed well.\n5 Conclusion\nThis paper proposed the Differentiable Search Index (DSI), a new paradigm for learning an end-to-end\nsearch system in a uniÔ¨Åed manner, paving the way for next generation search (Metzler et al., 2021).\nWe deÔ¨Åne novel indexing and retrieval tasks that encode the relationship between terms and docids\ncompletely within the parameters of a Transformer model. The paper proposed a number of different\nways to represent documents and docids, and explored different model architectures and model\ntraining strategies. Experiments conducted on the Natural Questions data set show that DSI performs\nfavorably against common baselines such as BM25 and dual encoders, both in a standard Ô¨Åne-tuning\nsetup as well as in a zero-shot setup.\nAlthough the models and results presented here are promising, there is a great deal of potential future\nresearch that can be explored based on this work to improve this approach. For example, it would\nbe interesting to explore alternative strategies for representing documents and docids, as well as\nto investigate mixture-of-expert models (Du et al., 2021; Fedus et al., 2021; Lepikhin et al., 2020)\nfor scaling the memory capacity of DSI. One important direction will also be to explore how such\nmodels can be updated for dynamic corpora, where documents may be added or removed from the\nsystem. Finally it may also be interesting to further investigate DSI as an unsupervised representation\nlearning method and/or memory store for other language models to leverage.\n6 Acknowledgements\nThe authors would like to thank you Fernando Pereira, Huaixiu Steven Zheng, Sebastian Ruder,\nAdam D. Lelkes, Ian Wetherbee and Dani Yogatama for their valuable feedback and discussions.\nWe would also like to extend a special thanks to Sanket Vaibhav Mehta for additional experimental\ncontributions.\n10\n\nReferences\nSamira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of\nlarge scale pre-training. arXiv preprint arXiv:2110.02095 , 2021.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving\nlanguage models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426 , 2021.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165 , 2020.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval.\narXiv preprint arXiv:2010.00904 , 2020.\nMostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce Croft. Neural\nranking models with weak supervision. In Proceedings of the 40th International ACM SIGIR\nConference on Research and Development in Information Retrieval , pages 65‚Äì74, 2017.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: EfÔ¨Åcient scaling of language models\nwith mixture-of-experts. arXiv preprint arXiv:2112.06905 , 2021.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efÔ¨Åcient sparsity. arXiv preprint arXiv:2101.03961 , 2021.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence\nembeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing , pages 6894‚Äì6910, Online and Punta Cana, Dominican Republic, November 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL https:\n//aclanthology.org/2021.emnlp-main.552 .\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are\nkey-value memories. arXiv preprint arXiv:2012.14913 , 2020.\nDaniel Gillick, Alessandro Presta, and Gaurav Singh Tomar. End-to-end retrieval in continuous space.\narXiv preprint arXiv:1811.08008 , 2018.\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar.\nAccelerating large-scale inference with anisotropic vector quantization. In International Conference\non Machine Learning , 2020. URL https://arxiv.org/abs/1908.10396 .\nKelvin Guu, Kenton Lee, Zora Tung, and Panupong Pasupat. REALM: Retrieval-Augmented\nLanguage Model Pre-Training. In Proceedings of ICML 2020 , 2020.\nMartin Josifoski, Nicola De Cao, Maxime Peyrard, and Robert West. Genie: Generative information\nextraction. arXiv preprint arXiv:2112.08340 , 2021.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361 , 2020.\nVladimir Karpukhin, Barlas O Àòguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv\npreprint arXiv:2004.04906 , 2020.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia RedÔ¨Åeld, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin Kenton Lee, Kristina Toutanova, Llion\nJones Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural Questions: a Benchmark for Question Answering Research. In Transactions of the\nACL, 2019.\n11\n\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. arXiv preprint arXiv:2006.16668 , 2020.\nDonald Metzler, Yi Tay, Dara Bahri, and Marc Najork. Rethinking search: making domain experts\nout of dilettantes. In ACM SIGIR Forum , volume 55, pages 1‚Äì27. ACM New York, NY , USA,\n2021.\nJianmo Ni, Gustavo Hern√°ndez √Åbrego, Noah Constant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei\nYang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint\narXiv:2108.08877 , 2021.\nFabio Petroni, Tim Rockt√§schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\nand Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 ,\n2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniÔ¨Åed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683 , 2019.\nHubert Ramsauer, Bernhard Sch√§Ô¨Ç, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler,\nLukas Gruber, Markus Holzleitner, Milena Pavlovi ¬¥c, Geir Kjetil Sandve, et al. HopÔ¨Åeld networks\nis all you need. arXiv preprint arXiv:2008.02217 , 2020.\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? arXiv preprint arXiv:2002.08910 , 2020.\nYu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training\nwith self-supervision for generalization under distribution shifts. In Hal Daum√© III and Aarti Singh,\neditors, Proceedings of the 37th International Conference on Machine Learning , volume 119 of\nProceedings of Machine Learning Research , pages 9229‚Äì9248. PMLR, 13‚Äì18 Jul 2020. URL\nhttps://proceedings.mlr.press/v119/sun20b.html .\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\narXiv preprint arXiv:1409.3215 , 2014.\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan\nNarang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efÔ¨Åciently: Insights from\npre-training and Ô¨Åne-tuning transformers. arXiv preprint arXiv:2109.10686 , 2021.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239 , 2022.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems , pages 5998‚Äì6008, 2017.\n12\n\n7 Appendix\nHere we include Ô¨Ågures containing additional details about our experiments.\n7.1 Dataset Statistics\nTable 4: Statistics of NQ datasets used in our experiments. (Referenced in Section 4.) The number in\nthe dataset name corresponds to the total number of document-query pairs in the dataset, while jDj\ncorresponds to the number of unique documents, based on the Ô¨Årst 4000 UTF-8 characters of the\ndocument.\nDatasetjDj Train Pairs Val Pairs Vdoc_out\nNQ10K 10K 8K 2K 320K\nNQ100K 86K 80K 20K 320K\nNQ320K 228K 307K 8K 320K\n7.2 Extended Results\nWe report additional results and observations here.\n7.2.1 Indexing/Memorization Performance\nTable 5: Indexing performance (memorization) on NQ documents via the Inputs2Targets indexing\nobjective. All models were indexed on all NQ documents (train and validation), with memorization\nevaluated on only the documents in the validation set.\nSize Params Method Indexing Hits@1\nBase 250M Atomic Docid 85:4\nLarge 800M Atomic Docid 84:9\nXL 3B Atomic Docid 88:4\nXXL 11B Atomic Docid 92:7\nBase 250M Naive String Docid 76:3\nLarge 800M Naive String Docid 92:1\nXL 3B Naive String Docid 92:2\nXXL 11B Naive String Docid 91:9\nBase 250M Semantic String Docid 87:6\nLarge 800M Semantic String Docid 91:5\nXL 3B Semantic String Docid 92:6\nXXL 11B Semantic String Docid 92:0\nWe can observe that indexing performance is relatively strong on NQ across methods and model sizes.\nIt is clear though that increasing model size improves indexing performance.\n7.2.2 Discussion of DSI Training Dynamics\nIn this paper, all indexing tasks are trained on the union of documents in both the train and validation\nsplits of Natural Questions (NQ). This aligns with traditional deÔ¨Ånitions of indices, where a document\nmust be in the index in order for the index to retrieve it. Retrieval then is trained on trained only on the\nNQ train split, with retrieval performance evaluated on NQ validation, based on the best checkpoint.\nAnalysis following this original work showed that, when training, a DSI model experiences forgetting\nof previously indexed batches as it indexes new batches, until it loops around again to the next epoch,\nand processes the same examples again. The indexing task we use in this paper was constructed by\nconcatenating validation documents after the train documents, then applying a buffered shufÔ¨Çe while\ntraining the model (sampling the next training batch from a buffer every step). We used a shufÔ¨Çe\nbuffer of size 5000, which is smaller than the size of the validation split for NQ100K and NQ320K.\n13\n\nAs a result, DSI experiments in this paper experienced cycles of minimum and maximum forgetting ,\ni.e. higher and lower validation scores, based on whether the model had just indexed the validation\ndocuments or indexed them one epoch ago, causing regular peaks and valleys in the validation\nperformance. When picking a checkpoint with maximum validation performance, as we do in\nthe main experiments of this paper, we are implicitly then picking the checkpoint with minimum\nforgetting.\nIn Table 6, we aim to provide more context to this phenomenon by providing retrieval validation\nscores for minimum forgetting checkpoints (highest peak), maximum forgetting checkpoints (highest\ntrough), as well as their average score representing if the validation documents were uniformly\ndistributed across the entire indexing split.\nTable 6: Additional NQ320K results at minimum forgetting and maximum forgetting checkpoints,\nwith their average (min-forget / max-forget / avg), at Hits@1,5,10,20.\nSize Params Method Hits@1 Hits@5 Hits@10 Hits@20\nBase 250M Atomic Docid 20.7 / 2.6 / 11.7 40.2 / 8.6 / 24.4 50.9 / 13.0 / 31.9 59.2 / 18.8 / 39.0\nLarge 800M Atomic Docid 11.6 / 2.5 / 7.0 30.5 / 7.2 / 18.9 37.6 / 10.9 / 24.2 46.7 / 15.9 / 31.3\nXL 3B Atomic Docid 28.1 / 2.7 / 15.4 52.7 / 7.2 / 30.0 61.9 / 10.4 / 36.1 69.2 / 14.4 / 41.8\nXXL 11B Atomic Docid 24.0 / 4.5 / 14.2 46.7 / 11.9 / 29.3 55.1 / 17.3 / 36.2 62.8 / 23.6 / 43.2\nBase 250M Naive String Docid 6.7 / 1.5 / 4.1 12.6 / 4.3 / 8.4 21.0 / 6.0 / 13.5 25.6 / 8.1 / 16.9\nLarge 800M Naive String Docid 13.3 / 2.6 / 8.0 26.0 / 7.9 / 16.9 33.6 / 11.0 / 22.3 40.4 / 14.7 / 27.5\nXL 3B Naive String Docid 16.7 / 1.2 / 8.9 32.8 / 3.1 / 17.9 58.1 / 4.1 / 31.1 62.5 / 5.6 / 34.0\nXXL 11B Naive String Docid 23.8 / 1.3 / 12.6 46.3 / 3.2 / 24.8 55.9 / 5.9 / 30.9 62.2 / 8.0 / 35.1\nBase 250M Semantic String Docid 27.4 / 12.0 / 19.7 47.8 / 25.4 / 36.6 56.6 / 30.6 / 43.6 61.3 / 34.9 / 48.1\nLarge 800M Semantic String Docid 35.6 / 10.2 / 22.9 54.3 / 21.6 / 38.0 62.6 / 24.5 / 43.5 67.3 / 27.8 / 47.5\nXL 3B Semantic String Docid 39.1 / 10.6 / 24.9 60.2 / 22.8 / 41.5 66.8 / 27.3 / 47.0 71.3 / 31.2 / 51.2\nXXL 11B Semantic String Docid 40.4 / 12.2 / 26.3 60.3 / 24.9 / 42.6 70.3 / 30.1 / 50.2 74.8 / 35.0 / 54.9\nResults. We see that for the best conÔ¨Åguration of DSI (semantic docids), even when experiencing\nmaximum forgetting DSI is still competitive with BM25, and in the average case DSI still outperforms\nthe Dual Encoder baseline.\n14",
  "textLength": 50301
}