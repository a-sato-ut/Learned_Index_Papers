{
  "paperId": "b5cfeb8d47948c355d9d13622710fb526bdbd8c6",
  "title": "Compressive Meta-Learning",
  "pdfPath": "b5cfeb8d47948c355d9d13622710fb526bdbd8c6.pdf",
  "text": "Compressive Meta-Learning\nDaniel Mas Montserrat\nStanford University\nStanford, California, USA\ndmasmont@stanford.eduDavid Bonet\nUniversity of California, Santa Cruz\nSanta Cruz, California, USA\ndbonet@ucsc.eduMaria Perera\nPolytechnic University of Catalonia\nBarcelona, Catalonia, Spain\nmaria.perera.baro@gmail.com\nXavier GirÃ³-i-Nieto\nPolytechnic University of Catalonia\nBarcelona, Catalonia, Spain\nxavigiro.upc@gmail.comAlexander G. Ioannidis\nUniversity of California, Santa Cruz\nSanta Cruz, California, USA\nioannidis@ucsc.edu\nABSTRACT\nThe rapid expansion in the size of new datasets has created a need\nfor fast and efficient parameter-learning techniques. Compressive\nlearning is a framework that enables efficient processing by us-\ning random, non-linear features to project large-scale databases\nonto compact, information-preserving representations whose di-\nmensionality is independent of the number of samples and can\nbe easily stored, transferred, and processed. These database-level\nsummaries are then used to decode parameters of interest from\nthe underlying data distribution without requiring access to the\noriginal samples, offering an efficient and privacy-friendly learning\nframework. However, both the encoding and decoding techniques\nare typically randomized and data-independent, failing to exploit\nthe underlying structure of the data. In this work, we propose\na framework that meta-learns both the encoding and decoding\nstages of compressive learning methods by using neural networks\nthat provide faster and more accurate systems than the current\nstate-of-the-art approaches. To demonstrate the potential of the\npresented Compressive Meta-Learning framework, we explore mul-\ntiple applicationsâ€”including neural network-based compressive\nPCA, compressive ridge regression, compressive k-means, and au-\ntoencoders.\nKEYWORDS\nCompressive Learning, Meta-Learning, Data Summarization, Neu-\nral Networks, Differential Privacy\n1 INTRODUCTION\nCompressive learning (CL) [ 29,30] allows for efficient learning\non large-scale datasets by compressing a complete dataset into\na single mean embedding, also referred to as the sketch, which\nacts as a vector of generalized moments. Ideally, the mean embed-\nding will contain all the necessary information in order to learn\nthe desired parameters of the underlying data distribution. The\ndecoding of the parameters from the sketch (i.e. the learning pro-\ncess) is typically framed as an inverse optimization problem. The\nnon-linear projection that generates the mean embedding takes\na set ofğ‘samples of ğ‘‘dimensions and compacts them into a\nunique vector of ğ‘šdimensions where ğ‘šâ‰ªğ‘ğ‘‘. Note that this\nPreprint. Extended version of a paper accepted at KDD â€™25. Publisher version:\ndoi:10.1145/3711896.3736889. This version is prepared for arXiv and may differ from\nthe published version.differs from traditional dimensionality reduction techniques (e.g.\nautoencoders or principal component analysis), since, while com-\nmon dimensionality reduction techniques project ğ‘Ã—ğ‘‘samples\nintoğ‘Ã—â„vectors with â„<ğ‘‘, the mean embedding provides a\ncompact representation for the totality of a data set, mapping ğ‘Ã—ğ‘‘\nsamples to a unique ğ‘š-dimensional vector. Such a framework is\nparticularly useful to learn models from data sets without the need\nfor accessing the original samples directly, but instead only using\ntheğ‘š-dimensional embedding. For example, CL techniques have\nproved to be effective at capturing parameters for Gaussian Mixture\nModels (GMMs), k-means, and PCA [ 29], from massive datasets\nwith orders of magnitude lower computational requirements. Note\nthat the term sketching is also used in other areas such as data\nstreaming applications [ 13,21] and numerical linear algebra [ 69],\nalthough related, here â€œsketchingâ€ has a different meaning and\nmethods between compressive learning and data streaming or nu-\nmerical linear algebra are not directly comparable (See Appendix\nfor further details).\nTraditional supervised learning (e.g. SGD-based techniques) com-\nmonly relies on performing multiple passes over the dataset and\ncomputing a loss for each sample. While accurate, this paradigm\nrequires to have access to the raw data, can be computationally\nintensive, and privacy-preserving mechanisms can be difficult to\nincorporate. Compressive learning provides an alternative para-\ndigm that is (1) memory efficient, (2) computationally efficient,\nand (3) privacy-friendly. Compressive learning makes use of linear\nsketching where the computation of the sketch can be parallelized\nthroughout massive databases and these mean embeddings can\nbe easily updated and support the addition and removal of sam-\nples. Namely, two sketches can be merged by a simple addition (or\naveraging), and new samples can be added and removed through\nsketch addition and subtraction. Note that linear sketching relates\nto how the sketch can be updated and does not imply that only lin-\near functions can be used to compute the sketches. Such a parallel\nand online nature allows to efficiently compress datasets into em-\nbeddings that can be easily updated without the need to re-access\nthe raw samples and can be easily stored and shared. Because the\ndimensionality of the embedding is independent of the size of the\ndataset, the learning process that maps the dataset-level embedding\ninto the predicted parameters can be done efficiently, even for large\ndatasets. Finally, differential privacy can be easily incorporated\nwithin compressive learning methods by adding the appropriate\nnoise into the dataset-level sketch. Once differential privacy has\n1arXiv:2508.11090v1  [cs.LG]  14 Aug 2025\n\nMas Montserrat et al.\nbeen added to the embedding, all further processing, including the\nprediction of parameters, will maintain the privacy guarantees.\nTwo important limitations are present in current compressive\nlearning systems. First, if the non-linear mapping function that\nprojects the dataset into the mean embedding is not properly de-\nsigned, the parameters of interest might not be learned accurately\n[30,62]. Second, the current learning techniques are designed for a\nspecific set of learning problems (e.g. k-means) and do not adapt well\nto new tasks, making it necessary to design a new learning approach\nfor each application. In this work, we introduce a new framework,\nCompressive Meta-Learning, that addresses both limitations by\nreplacing the sketching and learning (i.e. decoding) operations with\nneural networks that are meta-learned end-to-end. First, a neural\nnetwork (Sketch Network) performs a non-linear sample-wise pro-\njection (replacing the traditional randomized projections), followed\nby an average pooling operation that collapses all sample-level em-\nbeddings into a unique utility-preserving dataset-level embedding.\nThen, a second neural network (Query Network) takes as input the\ngenerated sketch and outputs the desired parameters (e.g. k-means\ncentroids). We refer to this complete system as Sketch-Query Net-\nwork (SQNet). The proposed method has several advantages: (a) the\nend-to-end training ensures that the sketching function properly\ncaptures the necessary information within the mean embedding. (b)\nBy jointly training the sketching and learning functions, the gener-\nated sketch is specifically tailored to the query network, allowing\nit to accurately predict the desired set of parameters, and (c) the\nsystem can be meta-learned to predict parameters from complex\nmodels (e.g. an autoencoder) by simply changing the loss function,\na task currently not possible with traditional compressive learning.\n2 RELATED WORK\nThe term â€œsketchingâ€ is used in multiple areas, and while all share a\ncommon theme of general purpose dimensionality reduction, they\nhave different characteristics depending on the field.\nCompressive Learning. Sketching techniques are used in com-\npressive learning to project a dataset into a single vector (sketch)\nwhich captures the necessary information to learn the parameters\nof a model. In other words, a sketching function ğ‘“, maps the dataset\ninto a sketch ğ‘“:Rğ‘Ã—ğ‘‘â†’Rğ‘š, and then a decoding function, ğ‘”,\nmaps the sketch into the parameter space ğ‘”:Rğ‘šâ†’Rğ‘, whereğ‘is\nthe dimensionality of the parameters. It is common that ğ‘š>ğ‘‘and\nğ‘šâ‰ªğ‘ğ‘‘. For example, in compressive k-means, the decoding func-\ntion maps the sketch ğ‘§âˆˆRğ‘šinto theğ‘˜cluster centroids ğœƒâˆˆRğ‘˜Ã—ğ‘‘.\nMost CL-based applications make use of Random Fourier Features\n(RFFs) [ 59] to project each sample into a higher-dimensional space,\nand a pooling average is performed to obtain a dataset-level descrip-\ntor. The mapping from the sketch and the parameters of interest\nis typically framed as an inverse optimization problem, such as\nCL-OMPR [ 40,41] and CL-AMP [ 12], where the predicted parame-\nters are iteratively updated by minimizing the error between the\nsketch computed with the original data and an empirical sketch\ncomputed from the predicted parameters. Some examples of CL\napplications include compressive k-means [ 41,61], Compressive\nGaussian Mixture Models [ 40], Compressive PCA [ 29,30], linear\nregression [ 23,29]. Compressive multi-class classification [ 60], and\ngenerative network training [ 58,62]. Differential privacy has beensuccessfully applied within CL applications [ 15]. Recent works\nhave explored using Nystrom approximations [ 14] to generate the\nsketches as an alternative to random features.\nData streaming. Data sketching has been widely applied in stream-\ning applications [ 19,20], where many sketching methods have been\ndeveloped to approximately capture the frequency or presence of\nitems, quantiles, or distinct counts of high-dimensional datastreams.\nSome methods include Count-Min [ 21], Count-Sketch [ 13], Bloom\nFilters [ 9], HyperLogLog [ 26], AMS Sketch [ 3], and Tensor Sketch\n[57], which rely on hashing and sketching via random and sparse\nprojections to map very high-dimensional vectors into compact\nrepresentations that allow decoding important count-related in-\nformation. Namely these techniques implement a mapping from\nğ‘Ã—ğ‘‘dimensional data into a compact sketch of dimension ğ‘š, with\nğ‘“:Rğ‘Ã—ğ‘‘â†’Rğ‘š,ğ‘šâ‰ªğ‘‘, and large dimensionality ( ğ‘‘) and sample\nsize (ğ‘). Such techniques include a decoding function, typically\nbased on inverse linear projections and heuristics, to map the ğ‘š-\ndimensional sketch into some ğ‘-dimensional representation. Recent\nworks have incorporated supervised learning [1, 35, 45].\nNumerical linear algebra (NLA). Sketching techniques are used in\napplications including linear regression, PCA, and matrix factoriza-\ntion, among others. Typically, a randomized projection is used to re-\nduce the dimensionality of matrices by combining rows (or columns)\nin order to obtain faster computations, namely ğ‘“:Rğ‘Ã—ğ‘‘â†’Rğ‘™Ã—ğ‘‘,\nwhere a matrix ğ´âˆˆRğ‘Ã—ğ‘‘is projected with the projection ğ‘†âˆˆ\nRğ‘™Ã—ğ‘to obtain a compact representation ğµ=ğ‘†ğ´,ğµâˆˆRğ‘™Ã—ğ‘‘, with\nğ‘™â‰ªğ‘. In many cases, an approximation of ğ´can be recovered\nfrom the compact representation ğµ. The projection ğ‘†will typically\nbe selected such as ||ğµğ‘‡ğµ||â‰ˆ||ğ´ğ‘‡ğ´||or||ğµğ‘¥||â‰ˆ||ğ´ğ‘¥||for a given\nğ‘¥, with theoretical guarantees that ensure that the sketch is a good\napproximation with probabilistic bounds on the loss of accuracy.\nSome examples include the Fast Johnson-Lindenstrauss Transform\n(FJLT) [ 2], randomized singular value decomposition (SVD) [ 24], or\nrandomized range finder for low-rank matrix approximation [ 33],\namong many others [ 69]. Recent works on sketching-based NLA\nhave explored learning the sketching projections [ 37,38,49,52].\nWhile NLA techniques are applicable to settings that can be framed\nas matrix decompositions or similar, our proposed framework is\napplicable to any learning task as long as a differentiable function\ncan be defined (e.g. predicting weights of an autoencoder).\nDeep Sets and Conditional Neural Processes. Our work adapts a\nsimilar structure to the previously proposed Deep Sets [ 72], which\nare neural networks that process each sample from a set and per-\nform a permutation-invariant pooling operation for supervised and\nunsupervised applications. A permutation equivariant version of\nDeep Sets is also introduced in [ 72]. Deep Set Prediction Networks\n(DSPN) [ 73] introduce an iterative neural network-based approach\nthat allows to auto-encode sets. Transformer Set Prediction Net-\nwork (TSPN) [ 44] extends DSPNs and produces a set-level summary\nthat is fed to a transformer to make predictions for each element\nof the set. Conditional Neural Processes (CNPs) [ 27] follow a simi-\nlar structure where a neural network is applied independently to\neach sample and all predicted embeddings are combined to obtain a\ndataset-level summary which is fed to a second-stage network that\nperforms supervised regression or classification. We adopt a similar\n2\n\nCompressive Meta-Learning\nFigure 1: (a) Compressive Learning, (b) Compressive Meta-Learning with (b.1) Meta-training of Sketch-Query Network (SQNet),\nand (b.2) Meta-testing of SQNet.\nstructure to such networks and frame it within the CL paradigm to\nlearn parameters from sketches.\nMeta-learning. The notion of learning-to-learn emerged early on\nwith seminal contributions [ 63,65], which laid the groundwork for\nmodels that adapt their own learning strategies across tasks. Build-\ning on these foundations, modern meta-learning approaches focus\non fast adaptation to novel tasks [ 34]. For every new task, a model\nğ‘ƒğœƒ(ğ‘¦|ğ‘¥,S)is learned, where ğ‘¦is the target, ğ‘¥is the test input, and\nS={ğ‘‹,ğ‘Œ}is the support set. Metric-based learning methods such\nas Matching Networks [ 66] and Prototypical Networks [ 64] map a\nlabelled support set Sinto an embedding space, where a distance\nis computed with the embedding of an unlabelled query sample to\nmap it to its label. As in kernel-based methods, the model ğ‘ƒğœƒcan be\nobtained through ğ‘ƒğœƒ(ğ‘¦|ğ‘¥,S)=Ã\nğ‘¥ğ‘–,ğ‘¦ğ‘–âˆˆSğ¾ğœƒ(ğ‘¥,ğ‘¥ğ‘–)ğ‘¦ğ‘–. Optimization-\nbased methods such as Model-agnostic meta-learning (MAML) [ 25]\nlearn an initial set of model parameters and perform a further op-\ntimization through a function ğ‘“ğœƒ(S), where model weights ğœƒare\nadjusted with one or more gradient updates given the support setof the taskS, i.e.,ğ‘ƒğœƒ(ğ‘¦|ğ‘¥,S)=ğ‘“ğœƒ(S)(ğ‘¥,S). Recent works have\nexplored the use of hypernetworks for meta-learning [10].\nDataset Distillation. Compressive learning and compressive meta-\nlearning are related to techniques for dataset distillation [ 71]. The\nobjective of dataset distillation (DD), also known as dataset con-\ndensation (DC), is to create a much smaller dataset consisting of\nsynthetic samples that enable models trained on it to perform com-\nparably to those trained on the full original dataset. Most dataset\ndistillation techniques rely on generating pseudo-samples by using\ngradient-based optimization techniques that compute gradients\nfrom pre-trained neural networks [ 68]. Techniques such as com-\npressive k-means can be seen as special cases of dataset distillation.\n3 COMPRESSIVE META-LEARNING\nSupervised learning tries to find parameters ğœƒof a model that mini-\nmizes a loss function L(Â·) given a training dataset x={ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘}:\nğœƒâˆ—=arg min\nğœƒL(ğœƒ|x)=arg min\nğœƒğ‘âˆ‘ï¸\nğ‘–=1â„“(ğœƒ|ğ‘¥ğ‘–) (1)\n3\n\nMas Montserrat et al.\nwhereL(Â·) is a loss function (e.g. negative log-likelihood) evaluated\nat each training sample. The loss function and parameters will vary\ndepending on the problem at hand. The parameters that minimize\nthe loss function can be approximated by optimization techniques\nsuch as gradient descent or EM. Compressive learning (Figure 1a)\ntakes a different approach: instead of searching for parameters\nthat minimize a given loss function with respect to the training\nsamples, a surrogate loss function C(ğœƒ|ğ‘§)is used which depends on\nthe sketchğ‘§but not on the training dataset xdirectly. First, a sketch\nğ‘§is computed by averaging per-sample non-linear projections:\nğ‘§=Î¦(x)=1\nğ‘ğ‘âˆ‘ï¸\nğ‘–=1ğœ™(ğ‘¥ğ‘–)(2)Ë†ğœƒ=arg min\nğœƒC(ğœƒ|ğ‘§)(3)\nwhere a mapping function Î¦(Â·)takes as input a set of ğ‘ ğ‘‘-\ndimensional samples x={ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘}, withğ‘¥ğ‘–âˆˆRğ‘‘, performs a\nnon-linear projection ğ‘§ğ‘–=ğœ™(ğ‘¥ğ‘–)(sketch projection) of each sample\nindividually, obtaining a sample-level representation, and combines\nall of these into a global dataset-level embedding ğ‘§, or sketch, with\nan average pooling (Eq. 2). Then, the estimated parameters Ë†ğœƒare\nobtained through an optimization process (Eq. 3) that minimizes\na surrogate cost function C(Â·), which acts as a proxy to a super-\nvised loss counterpart L(Â·), but involves only the sketch ğ‘§and\ndoesnâ€™t require access to the original dataset x, and withğ‘§,ğ‘§ğ‘–âˆˆRğ‘š,\nğœ™:Rğ‘‘â†’Rğ‘š,Î¦:Rğ‘Ã—ğ‘‘â†’Rğ‘š,Ë†ğœƒâˆˆRğ‘, whereğ‘will vary depend-\ning on the application, and C:Rğ‘Ã—Rğ‘šâ†’R. In most compressive\nlearning approaches, ğœ™consists of random feature projections, and\nthe optimization procedure that obtains the parameters from the\nsketch (Eq. 3) is performed with techniques such as CL-OMPR and\nCL-AMP. This has two main disadvantages: First, if the random\nfeature projection is not properly selected, the obtained sketch ğ‘§\nwill not capture the necessary information in order to decode the\nparameters. Second, it can be challenging to find an appropriate\ncost function C(Â·)with an adequate optimization procedure (Eq.\n3) that approximates a given supervised loss L(Â·) and accurately\nmaps the sketch to the parameters. If the sketch has a large enough\ndimensionality, the sketching method ğœ™(ğ‘¥ğ‘–)is properly designed,\nand the optimization problem in Eq. 3 is accurately solved, one can\nexpect that Ë†ğœƒğ¶ğ¿â‰ˆğœƒâˆ—. In fact, providing bounds on the difference\nbetween supervised learning and compressive learning parameter\nestimates is possible [29, 30].\nWe introduce Compressive Meta-Learning, a new framework\nwhere both the sketching and decoding functions are replaced by\nparameterized neural networks which are learned end-to-end. The\nproposed â€œSketch-Query Networkâ€ (SQNet) includes an encoding\nnetwork (Sketch Network Î¦ğœ”, Eq. 4) that generates the information-\npreserving dataset summaries, and a decoding network (Query\nNetworkğœ“ğœ”, Eq. 5) that maps sketches to parameters of interest\n(Figure 1b):\nğ‘§=Î¦ğœ”(x)=1\nğ‘ğ‘âˆ‘ï¸\nğ‘–=1ğœ™ğœ”(ğ‘¥ğ‘–) (4) Ë†ğœƒ=ğœ“ğœ”(ğ‘§) (5)\nwithğœ™ğœ”:Rğ‘‘â†’Rğ‘š,Î¦ğœ”:Rğ‘Ã—ğ‘‘â†’Rğ‘š, andğœ“ğœ”:Rğ‘šâ†’Rğ‘.\nThis approach removes the need of selecting appropriate random\nfeatures and surrogate losses C(Â·)and allows one to simultaneously\nlearn both the sketching function and decoding function in a super-\nvised end-to-end fashion, even for applications where a surrogate\nlossC(Â·)does not exist. The meta-parameters ğœ”of the Sketch-QueryNetwork are learned through a meta-training process that tries to\nminimize a given optimization problem: Ë†ğœ”=arg minğœ”Lğ‘€(ğœ”|x).\nOnce the parameters of Î¦ğœ”(Â·)andğœ“ğœ”(Â·)have been found, the\nsketch-query network pair can be used to infer (i.e. learn) the pa-\nrameters of interest Ë†ğœƒ=ğœ“ğœ”(ğ‘§)=(ğœ“â—¦Î¦)ğœ”(x). In practice,Lğ‘€(ğœ”|x)\ncan be obtained by substituting the predicted parameters by the\nSketch-Query Network Ë†ğœƒ=(ğœ“â—¦Î¦)ğœ”(x)into the supervised learn-\ning loss (Eq. 1), and use backpropagation to learn ğœ”:\nLğ‘€(ğœ”|x)=L(Ë†ğœƒ|x)=ğ‘âˆ‘ï¸\nğ‘–=1â„“((ğœ“â—¦Î¦)ğœ”(x)|ğ‘¥ğ‘–) (6)\nAlternatively, the loss in Eq. 6 can optimize a surrogate problem\nfrom which parameters of interest can later be recovered (e.g. pre-\ndicting a covariance matrix from which PCA and Ridge regression\ncan be obtained).\nTable 1: Summary of Supervised, Compressive, and Compres-\nsive Meta-Learning.\nFramework Mean Embedding Parameter Learning Meta-Learning\nSupervised\nLearningâˆ’ğœƒâˆ—=arg min\nğœƒğ‘âˆ‘ï¸\nğ‘–=1L(ğœƒ|ğ‘¥ğ‘–) âˆ’\nCompressive\nLearningğ‘§=1\nğ‘Ãğ‘\nğ‘–=1ğœ™(ğ‘¥ğ‘–)Ë†ğœƒğ¶ğ¿=arg min\nğœƒC(ğœƒ|ğ‘§) âˆ’\nCompressive\nMeta-Learningğ‘§=1\nğ‘Ãğ‘\nğ‘–=1ğœ™ğœ”(ğ‘¥ğ‘–) Ë†ğœƒğ‘†ğ‘„=ğœ“ğœ”(ğ‘§) Ë†ğœ”=arg min\nğœ”Lğ‘€(ğœ”|x)\nA key aspect of CL techniques is their applicability across differ-\nent data distributions without the need of performing any training\nof the sketching and decoding mechanisms. In this work, we explore\ntraining SQNets that can generalize to unseen datasets. Specifically,\nfor each proposed application, we (meta-)train the Sketch and Query\nnetworks with a set of datasets to obtain the meta-parameters ğœ”(Fig-\nure 1b.1) and then perform the evaluation by predicting parameters\nË†ğœƒin a new unseen set of datasets (Figure 1b.2). The Sketch-Query\nNetwork pair can be understood as a (meta-)learned learning algo-\nrithm Ë†ğœƒ=ğ´ğœ”(x)that predicts parameters given a training dataset.\nTable 1 provides a comparison between supervised, compressive,\nand compressive meta-learning. More details of the training and\nevaluation setup are provided in the Appendix.\nEfficient and Online Learning. The sketch can be easily updated\nby adding or removing the projection of new samples, making\nsketching-based learning an excellent framework for online learn-\ning applications. The computational time to obtain the parameters\nfrom a sketch is independent of the dataset size and only depends\non the complexity of the Query Network.\nPrivate Sketching. Because only access to the sketch is needed,\nand not to the original data samples, sketching-based learning\nis a good approach when data cannot be shared due to privacy\nrestrictions. Previous works [ 15] have successfully explored in-\ncorporating approximate differential privacy (DP) into the sketch\ngeneration process. Here, we explore the use of this technique\nwithin our proposed Sketch-Query Network. The ( ğœ–,ğ›¿)-DP sketch\nğ‘§ğœ–,ğ›¿can be computed as: ğ‘§ğœ–,ğ›¿=Ãğ‘\nğ‘–=1ğœ™â€²(ğ‘¥ğ‘–)+ğœ‰/ğ‘+ğœwhereğœ™â€²(ğ‘¥ğ‘–)=\n4\n\nCompressive Meta-Learning\nğœ™(ğ‘¥ğ‘–)min(1,ğ‘†\n||ğœ™(ğ‘¥ğ‘–)||)is a norm clipped version of the sketch pro-\njection,ğ‘†=maxğ‘¥(||ğœ™(ğ‘¥)||)is the maximum L2 norm of the sketch\nprojection across the meta-training samples, ğœ‰âˆˆRğ‘š,ğœ‰âˆ¼N( 0,ğœ2\n1Iğ‘š)\nis a Gaussian noise, and ğœâˆ¼Laplace(ğœ2)is a Laplacian noise, and\nğœ1andğœ2are selected as in [4, 15].\nDP ensures that any post-processing applied to ğ‘§ğœ–,ğ›¿will pre-\nserve its privacy guarantees. In practice, we perform a clamping\nof the sketch to remove potential out-of-range values. Then, we\ncan use the query network to learn differentially private param-\neters: Ë†ğœƒğœ–,ğ›¿=ğœ“(ğ‘§ğ‘\nğœ–,ğ›¿). In the experimental results, we show that\nby applying differential privacy during the meta-training process\nwe learn Sketch and Query Networks that provide robust private\nestimates. Both the dimensionality of the sketch and the number of\nsamples used to compute it will have an important impact on how\nmuch information is preserved after adding differential privacy.\nFurthermore, one can easily show that:\nlim\nğ‘â†’âˆğ‘§ğœ–,ğ›¿=ğ‘§ (7)\nTherefore, as the number of samples used to generate the sketch\nincreases, the distortion generated by the differential privacy be-\ncomes smaller. More details are provided in the Appendix.\nGeneralization Bounds. The generalization properties of the pa-\nrameters predicted by SQNet Ë†ğœƒ=(ğœ“â—¦Î¦)ğœ”(x)are characterized by\nthe complexity of the Sketch and Query networks. Specifically, the\ndifference between the empirical Land expected error Lğ‘‡of the\npredicted parameters can be bounded by the maximum norm of\nthe sketchğ›½ğœ™and loss function ğ‘€â„“, the Lipschitz constants of the\nQuery Network ğœŒğœ“and of the loss ğœŒâ„“. Given a bounded sketching\nfunction one can show that the differences of sketches generated\nwhile replacing its ğ‘–th sample is:\n||Î¦(S)âˆ’ Î¦(Sğ‘–)||=||1\nğ‘âˆ‘ï¸\nğ‘¥ğ‘—âˆˆSğœ™(ğ‘¥ğ‘—)âˆ’1\nğ‘âˆ‘ï¸\nğ‘¦ğ‘—âˆˆSğ‘–ğœ™(ğ‘¦ğ‘—)|| (8)\n=||1\nğ‘ğœ™(ğ‘¥ğ‘–)âˆ’1\nğ‘ğœ™(ğ‘¦ğ‘–)|| (9)\nâ‰¤2ğ›½ğœ™\nğ‘(10)\nBy considering an ğœŒğœ“-Lipschitz Query Network, and a ğœŒâ„“-Lipschitz\nloss function, it follows that âˆ€S,âˆ€ğ‘–:\n||â„“((ğœ“â—¦Î¦)(S),Â·)âˆ’â„“((ğœ“â—¦Î¦)(S\\ğ‘–),Â·)||âˆâ‰¤ğ›½ğœ™ğœŒğœ“ğœŒâ„“\nğ‘(11)\nThen, given uniform stability bounds [ 11] one can show that\nwith probability 1âˆ’ğ›¿:\nLğ‘‡(Ë†ğœƒ)<L(Ë†ğœƒ)+2ğ›½ğœ™ğœŒğœ“ğœŒâ„“\nğ‘+(4ğ›½ğœ™ğœŒğœ“ğœŒâ„“+ğ‘€â„“)âˆšï¸‚\nln1/ğ›¿\n2ğ‘(12)\nIntuitively, these results show that the generalization capabili-\nties of the parameters learned with compressive meta-learning are\nproportional to the complexity of the decoding function (Query\nNetwork). If the Query Network has a small norm (leading to a\nsmall Lipschitz constant), the predicted parameters are guaranteed\nto generalize. The full proof is provided in the Appendix.Neural Network Architectures. We make use of different resid-\nual networks as building blocks for Sketch Networks and Query\nNetworks. Specifically we use a (a) residual batch-norm ReLU fully-\nconnected network (â€œResNet styleâ€) and a (b) residual layer-norm\nGELU fully-connected network (â€œTransformer styleâ€). Figure 2 pro-\nvides a diagram for both types of architectures. When applied to\nthe Sketch Network, a pooling layer is included at the end, and\nwhen applied to the Query Network, a sigmoid layer is applied\nif the application requires it. The only hyperparameters that we\nexplore within the architecture is the number of residual blocks\nand the dimension of the hidden layers.\nFigure 2: Different architectures used within SQNet.\n4 APPLICATIONS\nWe explore PCA, k-means, ridge regression, and autoencoder learn-\ning. Table 2 provides an overview of the different applications and\ntheir respective loss functions. Both PCA and ridge regression learn\na linear projection that can be re-framed as learning the data covari-\nance from a sketch. k-means, which finds ğ‘˜prototypical elements,\nis re-framed as reducing the L2 distance between the sketch gener-\nated using the samples of the dataset, and a sketch generated using\nthe centroids Î¦(ğœƒ). Autoencoders, which are learned by minimizing\na reconstruction loss in the supervised learning training, can be\ndirectly learned by predicting the weights from a sketch. Note that\ntasks such as autoencoder weight prediction do not have a clear\ncompressive learning framing, further showing the benefits of the\nproposed method: by meta-training the sketch-query network pair,\na mapping from a sketch to parameters can be learned even if no\ncompressive learning criterion is available.\n4.1 Principal Component Analysis and\nRegression\nPrincipal Component Analysis (PCA) tries to find a linear projection\nğœƒthat minimizes the following mean squared reconstruction error,\nğœƒ=arg min\nğœƒğ‘âˆ‘ï¸\nğ‘—=1||ğ‘¥ğ‘—âˆ’ğœƒğœƒğ‘‡ğ‘¥ğ‘—||2(13)\nwhereğœƒis an orthonormal projection. It is well known that\nthe principal components projections can be found by a simple\neigendecomposition of the empirical covariance matrix of the data\nğ‘…=ğœƒğ·ğœƒğ‘‡, whereğ‘…=1\nğ‘Ãğ‘\nğ‘–=1ğ‘¥ğ‘–ğ‘¥ğ‘‡\nğ‘–andğ·is a diagonal matrix with\neigenvalues of ğ‘…. Ridge linear regression tries to find a regularized\nlinear mapping such that:\n5\n\nMas Montserrat et al.\nTable 2: Different applications with Supervised, Compressive, and Compressive Meta-Learning.\nApplication Parameters Supervised Learning\nL(ğœƒ|ğ‘¥ğ‘–)Compressive Learning\nC(ğœƒ|ğ‘§)Compressive\nMeta-LearningLğ‘€(ğœ”|x)\nPCAOrthonormal Basis\nğœƒ={ğœƒ1,...,ğœƒğ‘Ÿ}||ğ‘¥ğ‘—âˆ’ğœƒğœƒğ‘‡ğ‘¥ğ‘—||2||ğ´vec(ğ‘…)âˆ’ğ‘§||2||vecLT(ğ‘…)âˆ’ğœ“(Î¦(x))|| 1\nRidge\nRegressionLinear weights\nğœƒâˆˆRğ‘šÃ—ğ‘›||ğ‘¥(ğ‘¦)\nğ‘—âˆ’ğœƒğ‘¥(ğ‘¥)\nğ‘—||2+ğœ†||ğœƒ||2||ğ´vec(ğ‘…)âˆ’ğ‘§||2||vecLT(ğ‘…)âˆ’ğœ“(Î¦(x))|| 1\nğ‘˜-meansğ‘˜centroidsğœƒğ‘–âˆˆRğ‘‘\nğœƒ={ğœƒ1,...,ğœƒğ‘˜}minğ‘˜||ğ‘¥ğ‘—âˆ’ğœƒğ‘˜||2||Î¦(ğœƒ)âˆ’ğ‘§||2(a)minğ‘˜||ğ‘¥ğ‘—âˆ’ğœƒğ‘˜||2\n(b)||ğ‘¥ğ‘–âˆ’Ë†ğœƒğœ‹(ğ‘–)||2\nAutoencoderNeural network\nweightsğœƒ||ğ‘¥ğ‘—âˆ’(ğ‘”â—¦ğ‘“)ğœƒ(ğ‘¥ğ‘—)||2âˆ’ || ğ‘¥ğ‘—âˆ’(ğ‘”â—¦ğ‘“)ğœ“(Î¦(x))(ğ‘¥ğ‘—)||2\nğœƒ=arg min\nğœƒğ‘âˆ‘ï¸\nğ‘—=1||ğ‘¥(ğ‘¦)\nğ‘—âˆ’ğœƒğ‘¥(ğ‘¥)\nğ‘—||2+ğœ†||ğœƒ||2\n2(14)\nwhereğ‘¥ğ‘—=[ğ‘¥(ğ‘¦)\nğ‘—,ğ‘¥(ğ‘¥)\nğ‘—]are the regression labels ğ‘¥(ğ‘¦)\nğ‘—and input\nfeaturesğ‘¥(ğ‘¥)\nğ‘—of theğ‘—th sample concatenated, ğ‘…=\u0012ğ‘…11ğ‘…12\nğ‘…21ğ‘…22\u0013\n, and\nğœƒ=ğ‘…12(ğ‘…22+ğœ†ğ¼)âˆ’1. Thereforeğ‘…is sufficient statistic to obtain the\nPCA projection and Ridge regression parameters.\nCompressive PCA and Linear Regression. In many scenarios, ğ‘…\ncan have very high dimensionality. Compressive PCA (CPCA) and\nCompressive Ridge Regression (CRR) [ 29,30] try to provide a more\nefficient alternative by using the following sketch (Eq. 15) and\ndecoding functions (Eq. 16):\nğ‘§=1\nğ‘ğ‘âˆ‘ï¸\nğ‘–=1ğ´vec(ğ‘¥ğ‘–ğ‘¥ğ‘‡\nğ‘–)=ğ´vec(ğ‘…) (15)\nË†ğ‘…=arg min\nğ‘…||ğ´vec(ğ‘…)âˆ’ğ‘§||2(16)\nwhere vec(Â·)flattens the ğ‘‘Ã—ğ‘‘matrix into a ğ‘‘2vector, and ğ´\nis a random matrix with dimensions ğ‘šÃ—ğ‘‘2,ğ‘§is the empirical\nsketch. In practice, the size of the sketch is smaller than the size of\nthe covariance matrix ğ‘šâ‰ªğ‘‘2.Ë†ğ‘…can be found by minimizing Eq.\n16 through any desired optimization procedure, or by computing\nthe pseudo-inverse of the randomized projection: vec(Ë†ğ‘…)=ğ´+ğ‘§.\nBecauseğ‘…is a symmetric matrix, the sketching process (Eq. 15) and\noptimization objectives (Eq. 16) can be framed by using only the\nvectorized lower (or upper) triangular elements of ğ‘…, i.e. replacing\nvec(Ë†ğ‘…)by vec LT(Ë†ğ‘…)âˆˆRğ‘‘(ğ‘‘+1)\n2.\nNeural-Based CPCA and CRR. We frame Compressive PCA and\nCompressive Ridge Regression as a Sketch-Query Network where\nboth the sketch and the reconstructed covariance matrix are pre-\ndicted with parametric models: vecLT(Ë†ğ‘…)=ğœ“(Î¦(x)). The Sketch-\nQuery Network pair is trained by minimizing the L1 error between\nthe predicted and empirical covariance matrix:\nL(x,ğœ“,Î¦)=||vecLT(ğ‘…)âˆ’ğœ“(Î¦(x))||1 (17)\nWe train a sketch network consisting of a learned linear pro-\njection applied to the vectorized outer product of the input vectorğœ™(ğ‘¥ğ‘–)=ğ‘Šğœ™vec(ğ‘¥ğ‘–ğ‘¥ğ‘‡\nğ‘–)+ğ‘ğœ™followed by a query network consist-\ning of a linear projection followed by a tanh activation ğœ“(ğ‘§)=\nğœ(ğ‘Šğœ“ğ‘§+ğ‘ğœ“). We train the network on a large range of datasets\nand evaluate it on new unseen datasets, showing that a learned\nSketch-Query Network (Eq. 17) can be used to learn PCA and re-\ngression parameters more accurately than traditional randomized\ncompressive techniques (Eq. 16).\nExperimental Results. We make use of the OpenML-CC18 suite\n[8] composed of multiple datasets including tabular data and image\ndatasets. We additionally use several MNIST-like image datasets\nincluding EMNIST Digits and Letters [ 17], KMNIST [ 16], Quick-\nDraw10 [ 32], and AfroMNIST [ 70]; DNA sequence datasets from\nHumans (HapMap3) [ 18] and Dogs (Canids) [ 5] are also included.\nWe randomly select 196 features from each dataset, and apply zero-\npadding to datasets with dimensionality smaller than 196. We per-\nform a 50-50 split of datasets for meta-training and evaluation\n(detailed in the Appendix) and train the Sketch-Query Network\nwith the meta-training split using Adam and a learning rate of\n3Ã—10âˆ’5with a learning rate scheduler, and compare it to tradi-\ntional Compressive PCA [29, 30] approaches.\nTo evaluate each method, we obtain the principal component\nprojections, and the regression coefficients from the estimated co-\nvariance matrices for each dataset. Then, we compute the PCA\nreconstruction error (Eq. 13), and the regression MSE error, for all\nsamples within the dataset. We repeat the process for the principal\ncomponent dimensions (i.e. dimensionality of the projection) rang-\ning from 1to196and compute the average reconstruction error.\nWe include baselines from the CompressiveLearning.jl library [ 14],\nnamely Compressive PCA with random projections following the\nChi distribution (CHI), projections whose columns are on the unit\nsphere (UNIT), and decoding methods including Robust Factorized\nRank Minimization (ROBUST) [ 29,30], gradient-like approaches\nfor sparse recovery (AGD) [ 50], and Exponential-Type Gradient De-\nscent Algorithm (HUANG) [ 36]. Additionally, we include baselines\nfrom the numerical linear algebra literature, including sparse and\nGaussian projection, which combine rows of the dataset to reduce\nits dimensionality, and sampling, where a random subset of rows\nis selected [ 69] (see Appendix). Figure 3 (left) shows the average\nPCA reconstruction error for different sketch sizes ranging from\n0.01% to 100% of the dimensionality of the covariance matrix. We\ncan observe that the reconstruction error is consistently lower in\n6\n\nCompressive Meta-Learning\n10âˆ’210âˆ’1100101102\nSketch size ( %)0.00.10.20.30.40.50.60.7LRE (PCA)\nAll test datasets\nPCA\nTrue covariance\nSampling\nSparse Random Projection\nGaussian Random Projection\nComp. PCA (UNIT + ROBUST)\nComp. PCA (UNIT + AGD)\nComp. PCA (UNIT + HUANG)\nComp. PCA (CHI + ROBUST)\nSketch-Query Network\n10âˆ’210âˆ’1100101102\nSketch size ( %)0.000.020.040.060.080.100.120.140.16LRE (Regression)\nAll test datasets\nRegression\nTrue covariance\nSampling\nSparse Random Projection\nGaussian Random Projection\nComp. PCA (UNIT + ROBUST)\nComp. PCA (UNIT + AGD)\nComp. PCA (UNIT + HUANG)\nComp. PCA (CHI + ROBUST)\nSketch-Query Network\nFigure 3: (left) Logarithm relative scale error with respect to the true covariance error (LRE) of PCA reconstruction error, and\n(center) LRE of Regression for all datasets as a function of the sketch size (% of the dimensionality of ğ‘…LTâˆˆRğ‘‘(ğ‘‘+1)/2). (right)\nLRE of PCA for the MNIST dataset incorporating differential privacy as a function of ğœ–.\nSketch-Query Network, outperforming all competing methods, and\nthat it matches the reconstruction error of the actual PCA when\nthe sketch dimensionality matches the dimensionality of vecLT(ğ‘…).\nAs the sketch becomes smaller, the difference between errors be-\ncomes larger, showing the importance of learning a good sketching\nmechanism. Figure 3 (center) shows the regression reconstruction\nerror, showing a similar trend as in PCA. Figure 3 (right) shows\nthe PCA error on the MNIST dataset when at different levels of\ndifferential privacy, showing that sketching can provide an accurate\nway to estimate private parameters, even surpassing the naive non-\nsketching-based approach. All results are reported in a logarithmic\nrelative scale (LRE), with respect to the original error using the true\ncovariance matrix ğ‘…(see the Appendix).\n4.2 k-means\nk-means consists of finding ğ‘˜centroidsğœƒ={ğœƒ1,...,ğœƒğ¾}, withğœƒğ‘˜âˆˆ\nRğ‘‘, such as the average mean square error between each training\nsampleğ‘¥ğ‘—and its closest centroid ğœƒğ‘˜is minimized:\nğœƒ=arg min\nğœƒğ‘âˆ‘ï¸\nğ‘—=1min\nğ‘˜||ğ‘¥ğ‘—âˆ’ğœƒğ‘˜||2(18)\nThis is a widely used technique to perform unsupervised clus-\ntering and to learn cluster prototypes.\nCompressive k-means. As shown in [ 29,30,41] the centroids can\nbe approximately found by minimizing the distance between the\nsketch of the dataset and the sketch generated using the centroids:\nË†ğœƒ=arg min\nğœƒ||Î¦(ğœƒ)âˆ’Î¦(x)||2=arg min\nğœƒ||Î¦(ğœƒ)âˆ’ğ‘§||2(19)Previous works have successfully explored this Compressive\nk-means (CKM) approach by using Random Fourier Features (RFFs)\nto compute the sketch ğ‘§and using optimization techniques such\nas CL-OMPR to solve the objective in Eq. 19. Such techniques are\npublicly available (e.g., at the CompressiveLearning.jl library [ 14]).\nThe quality of the predicted k-means centroids Ë†ğœƒwill depend on\nthe projection function ğœ™used to compute the sketch, and on the\ndecoding algorithm used to map the sketch into predicted parame-\nters. If either the projection function fails to properly capture the\ninformation of the underlying distribution of the data, or the de-\ncoding method fails to predict the parameters given the sketch, the\nquality of the prediction will be poor. Here we propose an iterative\napproach to learn a sketching Î¦and decoding mechanism ğœ“.\nIterative CKM. By treating Eq. 19 as an iterative optimization\nprocess where we start with a set of random centroids Ë†ğœƒ0and pro-\ngressively update them to minimize the square distance between\nÎ¦(Ë†ğœƒi)andğ‘§, we can jointly optimize the sketching and query mech-\nanisms. Namely, by performing the optimization using SGD and\nunfolding (unrolling) [ 54] the optimization procedure, the query\nnetworkğœ“(ğ‘§)can be formulated as:\nË†ğœƒi+1=Ë†ğœƒiâˆ’ğ›¼âˆ‡L(ğœ™,Ë†ğœƒi,ğ‘§) (20)\nwhereğ›¼is the learning rate, Ë†ğœƒğ‘–are the estimated centroids in the\ncurrent step, andL(Î¦(Ë†ğœƒi),ğ‘§)=||Î¦(Ë†ğœƒi)âˆ’ğ‘§||2\n2is the mean square\nerror loss between the sketch computed with the current centroids\nÎ¦(Ë†ğœƒi)and the empirical sketch ğ‘§. Similar formulations can be pur-\nsued with other optimization algorithms, such as Adam [ 42]. In\nfact, an unrolled optimization procedure defining the query func-\ntionğœ“is equivalent to a recurrent neural network (RNN) defined\nby the gradient of the distance between sketches, and by training\n7\n\nMas Montserrat et al.\n980 1960 3920 9800\nSketch Size0.070.080.090.100.11MSE\n0 20000 40000 60000 80000 100000\nDataset Size050100150200250Time (seconds)\n105\n104\n103\n102\n101\n100101\n0.040.060.080.100.12MSE\nRegular Training\nDP-aware TrainingIterative CKM\nRFF + CLOMPRRFF (Ad.) + CLOMPR \nk-meansNystrom + CLOMPR\nRFF + CLAMP\nFigure 4: Benchmark of Compressive k-means methods with average MSE across datasets (left), computational time (center),\nand Iterative CKM results with DP-aware training for MNIST (right).\n(learning) the sketching network ( Î¦) the query network is simulta-\nneously learned. Because training an unrolled optimization process,\neither through a differentiable optimizer, or implicit differentiation,\ncan be unstable [ 53], we train this Sketch-Query Network with a\nderivative-free optimizer, the NGOpt optimizer [ 7], as it provides\nus the flexibility to optimize simultaneously the weights of the\nnetwork, the activation used, and hyperparameters of the inner\noptimization such as the inner learning rate (i.e. ğ›¼in Eq. 20), the\ninner optimizer used, which defines the iterative process in Eq. 20,\nand the variance of the initial estimates Ë†ğœƒ0. The proposed sketch\nnetwork consists of a linear projection followed by a non-linearity\nğœ™(ğ‘¥)=ğœ(ğ‘Šğ‘¥).\nExperimental Results. We used the same datasets and splits as\nin Compressive PCA, but we normalize all features to be bounded\nbetween 0 and 1. We compare the proposed Iterative CKM, the\nregular k-means, and traditional compressive k-means from Com-\npressiveLearning.jl , using Random Fourier Feature projections (RFF)\n[40,41], with and without adaptive radius (Ad.), Nystrom approxi-\nmation [ 14], and decoders such as CL-OMPR and CL-AMP. Figure 4\n(left) shows the mean square reconstruction error (MSE) (Eq. 18) av-\neraged across all testing datasets. For a fair comparison, the samples\nused in the Nystrom approximation are counted within the sketch\nsize. Iterative CKM provides lower error than competing methods,\nespecially with smaller sketch sizes, and matches compressive k-\nmeans with RFFs and CL-OMPR errors for larger sketch sizes. The\nerror decreases as the sketch size increases, with methods based\non CL-AMP providing unstable results. Figure 4 (center) shows the\ncomputational time for each technique (excluding CL-AMP due\nto unstable results). As expected, compressive learning methods\nprovide almost constant times regardless of the number of samples\nin the dataset, taking less than 20 seconds to process each dataset,\nwhile traditional k-means processing time grows super-linearlywith the dataset size. Furthermore, we explore using DP within\ncompressive k-means (Figure 4 (right)). We use an Iterative CKM\ntrained with (privacy-aware) and without (regular) DP during train-\ning and evaluated on the MNIST dataset. Specifically, we apply an\n(0.01,0.01)-DP when performing privacy-aware training. We show\nthat the sketching function learned using privacy-aware training\nprovides lower reconstruction errors, specifically with values of ğœ–\nclose to 1. Withğœ–<0.01both methods start performing poorly.\n4.3 Autoencoders\nAn autoencoder (AE) combines an encoder that maps inputs ğ‘¥into\nembeddings ğ‘¢=ğ‘“ğœƒ(ğ‘¥), and a decoder that tries to reconstruct\nthe input Ë†ğ‘¥=ğ‘”ğœƒ(ğ‘¢). The encoder-decoder pair (ğ‘”ğœƒâ—¦ğ‘“ğœƒ)(ğ‘¥ğ‘—)is\nparameterized by ğœƒand learned by minimizing some reconstruction\nerror such as:\nğœƒ=arg min\nğœƒğ‘âˆ‘ï¸\nğ‘—=1||ğ‘¥ğ‘—âˆ’(ğ‘”ğœƒâ—¦ğ‘“ğœƒ)(ğ‘¥ğ‘—)||2(21)\nCommonly, ğœƒis estimated with an SGD-based method which\ncan be slow, computationally intensive, and requires to have direct\ndata access. Here we explore the application of SQNet to predict\nthe parameters ğœƒof AEs such that they can adapt to new, unseen\ndatasets without the need for re-training the encoder-decoder pair,\nby replacing the slow training process of traditional AEs with the\nfast sketching and decoding to learn the parameters Ë†ğœƒ=(ğœ“â—¦Î¦)(x).\nSketch-Conditional Autoencoders. The proposed AE has two sets\nof parameters: fixed parameters ğ‘£, which are learned during the\nmeta-training process and kept fixed afterwards, and dynamic pa-\nrametersğœƒ, which are predicted from a sketch ğ‘§by the query net-\nwork for every new dataset. The encoder, decoder, and sketch net-\nwork consist of a residual MLP architecture, and the query network\nis a simple linear layer that transforms the sketch into the predicted\n8\n\nCompressive Meta-Learning\ndynamic weights. The dynamic weights ğœƒof the encoder and de-\ncoder consist of the bias vectors of their respective first linear layer.\nTherefore, the output of the first linear layer of the encoder (and\ndecoder), can be stated as:\nâ„1(ğ‘¥,ğ‘§)=ğ‘Šğ‘¥ğ‘¥+ğœ“(ğ‘§)=ğ‘Šğ‘¥ğ‘¥+ğ‘Šğ‘§ğ‘§+ğ‘ (22)\nwhereğ‘Šğ‘¥is the fixed (meta-learned) linear layer, and ğœ“(ğ‘§)=\nğ‘(x)=ğ‘Šğ‘§ğ‘§+ğ‘is the dynamic bias predicted from the sketch by\nthe query network. By using skip connections, the information\nof the sketch can be propagated throughout all the layers. Note\nthat this framework could be extended to predict more weights\nbesides the dynamic biases. The fixed weights of the AE ğ‘£, the\nsketching Î¦, and query network ğœ“are jointly learned during meta-\ntraining and kept fixed afterwards. The meta-learning is performed\nby predicting the sketch and dynamic bias with a batch of samples,\nand then computing the reconstruction error with a new batch\nfrom the same dataset. The error is backpropagated through the\nencoder, decoder, and Sketch-Query Networks. After training, when\nan unseen dataset xâ€²is found, the dynamic biases are predicted\nby the Sketch-Query Network and introduced in the AE ğ‘(xâ€²)=\nğœ“(Î¦(xâ€²)). Note that samples from the evaluation datasets are not\nused during the meta-training process. During testing, they are\nused both to generate the sketch and to evaluate the modelâ€™s final\nperformance.\nWe explore multiple variations of the autoencoder to properly\nassess the effect of conditioning by a learned sketch: a regular\nautoencoder without sketch conditioning (AE), trained with each\nof the evaluation datasets (i.e. dataset-specific AEs), an autoencoder\nconditioned with the sample mean (i.e. a sketch generated with the\nidentity function) (+M), an autoencoder conditioned by the mean\nand a learned sketch (+MS), and an autoencoder conditioned by a\nper-class sketch, where a unique sketch is created for each class\n(+MSK). For a more detailed discussion, see Appendix.\nExperimental Results. We make use of multiple datasets: 2 datasets\nof images including MNIST (M) [ 46], QuickDraw-10 (QD) [ 32], 2\ndatasets of genomic data, including human whole-genome (H) [ 58]\nand dogs (D) [ 5], UCI datasets including KDD Cup 1998 (K) [ 6] and\nAdult (A) [ 43], and Kaggleâ€™s Bank Marketing (B) [ 55] dataset. We\nprocess all datasets with binarization and one-hot encoding and\nrandomly keep 1000 dimensions. The sketch-conditional AEs are\nmeta-trained with a randomized binarized MNIST (RM) dataset,\nwhere the position of each pixel is shuffled and randomly negated\nat every batch. This heavily randomized data augmentation allows\nus to learn a network with generalization capabilities and forces the\nAE to extract useful information from the sketches. All AEs have a\nbottleneck dimension of 50. As shown in Table 3, regular AEs are\nable to reconstruct with high accuracy in-distribution samples but\ncompletely fail to reconstruct out-of-distribution samples, with the\nexception of AEs trained with the image datasets (MNIST, Quick-\nDraw) which can partly generalize to the other image datasets. On\nthe other hand, the AE trained with randomized MNIST has higher\naverage generalization capabilities, and it is surpassed by the mean-\nconditional, sketch-conditional, and categorical-sketch-conditional\nnetworks. In almost all datasets, the per-class sketch-conditional\nAE (AE+MSK) provides the second-best reconstruction, in some\ncases with an accuracy comparable to the dataset-specific AE.Table 3: Balanced accuracy of the AE reconstructions. TS:\nTrain set; RM: Randomized MNIST; +M: mean-conditional;\n+S: sketch-conditional; +K: mean and sketch-conditional per\nclass. Bold indicates best, Blue second-best.\nModel TS M H D K QD A B Avg\nAE M 99.7 54.1 53.1 50.8 74.7 58.6 55.8 63.8\nAE H 54.8 85.1 54.5 56.4 54.7 58.4 57.6 60.2\nAE D 55.2 53.4 80.4 52.5 55.2 56.7 58.1 58.8\nAE K 48.0 53.4 52.5 88.4 48.6 66.6 58.6 59.4\nAE QD 95.6 53.7 53.1 51.4 84.5 55.3 52.4 63.7\nAE A 55.8 53.0 51.9 51.5 55.2 99.5 62.5 61.3\nAE B 51.2 50.7 50.8 50.5 50.6 62.0 99.2 59.3\nAE RM 66.8 65.5 60.6 64.2 67.6 73.0 68.1 66.5\n+M RM 78.6 80.5 70.2 72.6 59.5 76.8 72.3 72.9\n+MS RM 79.2 81.2 70.1 73.7 61.9 80.2 81.5 75.4\n+MSK RM 85.7 82.4 72.2 77.0 68.3 86.0 87.5 79.9\n5 STRENGTHS AND LIMITATIONS\nOF COMPRESSIVE META-LEARNING\nA key aspect of compressive learning is that it only requires a single\npass through the dataset, making sketch computation linear with\nrespect to the size of the dataset ğ‘, which can be fully parallelized.\nFurthermore, the computational complexity of predicting the pa-\nrametersğœƒis independent of the dataset size. Such properties make\ncompressive (meta-)learning a very good fit for privacy-preserving,\nonline learning, or federated learning applications. However, the\ndimensionality of the sketch becomes an important aspect in order\nto properly capture enough data for the successful decoding of the\nparameters. For example, to obtain an accurate reconstruction of\nthe covariance matrix, a sketch of size ğ‘šwithğ‘‘â‰¤ğ‘šâ‰¤ğ‘‘(ğ‘‘+1)/2\nis recommended. For k-means, a sketch size proportional to the\ndimensionality and ğ‘˜is required with ğ‘šâˆğ‘˜ğ‘‘. This relationship\nbetween the input dimension ğ‘‘and the sketch dimension ğ‘šcan\nmake it difficult to apply compressive learning-based techniques to\nhigh-dimensional data such as high-resolution images, text, 3D ob-\njects, or whole genome DNA sequences. Future works should tackle\nsuch challenges in order to provide a compressive (meta-)learning\nparadigm that can scale properly with the dimensionality of the\ndata.\n6 CONCLUSIONS\nWe have introduced meta-learning into compressive learning ap-\nplications, demonstrating that neural networks can significantly\nimprove accuracy and replace ad hoc randomized sketching and de-\ncoding mechanisms, while easily incorporating differential privacy.\nFuture work is required to apply compressive learning techniques to\nhigh-dimensional data. In many real-world applications, the sketch\nsize needs to scale linearly, or even quadratically, with the input\ndimension, which poses a challenge. Although we show that our\napproach can be applied to data with dimensionality from hun-\ndreds to thousands, significantly larger than in previous works,\nnew approaches are needed to handle many natural signals.\n9\n\nMas Montserrat et al.\nREFERENCES\n[1] Anders Aamand, Piotr Indyk, and Ali Vakilian. 2019. (Learned) Frequency Esti-\nmation Algorithms under Zipfian Distribution. arXiv preprint arXiv:1908.05198\n(2019).\n[2] Nir Ailon and Bernard Chazelle. 2009. The fast Johnsonâ€“Lindenstrauss transform\nand approximate nearest neighbors. SIAM Journal on computing 39, 1 (2009),\n302â€“322.\n[3]Noga Alon, Yossi Matias, and Mario Szegedy. 1999. The space complexity of\napproximating the frequency moments. Journal of Computer and system sciences\n58, 1 (1999), 137â€“147.\n[4] Borja Balle and Yu-Xiang Wang. 2018. Improving the gaussian mechanism for\ndifferential privacy: Analytical calibration and optimal denoising. In International\nConference on Machine Learning . PMLR, 394â€“403.\n[5]Emily R Bartusiak, MÃ­riam BarrabÃ©s, Aigerim Rymbekova, Julia Gimbernat-\nMayol, Cayetana LÃ³pez, Lorenzo Barberis, Daniel Mas Montserrat, Xavier GirÃ³-\ni Nieto, and Alexander G Ioannidis. 2022. Predicting Dog Phenotypes from\nGenotypes. bioRxiv (2022).\n[6] Stephen D Bay, Dennis Kibler, Michael J Pazzani, and Padhraic Smyth. 2000. The\nUCI KDD archive of large data sets for data mining research and experimentation.\nACM SIGKDD explorations newsletter 2, 2 (2000), 81â€“85.\n[7] Pauline Bennet, Carola Doerr, Antoine Moreau, Jeremy Rapin, Fabien Teytaud,\nand Olivier Teytaud. 2021. Nevergrad: black-box optimization platform. ACM\nSIGEVOlution 14, 1 (2021), 8â€“15.\n[8] Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers, Frank Hut-\nter, Michel Lang, Rafael Gomes Mantovani, Jan N van Rijn, and Joaquin Van-\nschoren. 2021. OpenML Benchmarking Suites. In Thirty-fifth Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track (Round 2) .\n[9]Burton H Bloom. 1970. Space/time trade-offs in hash coding with allowable\nerrors. Commun. ACM 13, 7 (1970), 422â€“426.\n[10] David Bonet, Daniel Mas Montserrat, Xavier GirÃ³-i Nieto, and Alexander G\nIoannidis. 2024. HyperFast: Instant Classification for Tabular Data. Proceedings\nof the AAAI Conference on Artificial Intelligence 38, 10 (2024), 11114â€“11123.\n[11] Olivier Bousquet and AndrÃ© Elisseeff. 2002. Stability and generalization. The\nJournal of Machine Learning Research 2 (2002), 499â€“526.\n[12] Evan Byrne, Antoine Chatalic, RÃ©mi Gribonval, and Philip Schniter. 2019.\nSketched clustering via hybrid approximate message passing. IEEE Transac-\ntions on Signal Processing 67, 17 (2019), 4556â€“4569.\n[13] Moses Charikar, Kevin Chen, and Martin Farach-Colton. 2002. Finding frequent\nitems in data streams. In International Colloquium on Automata, Languages, and\nProgramming . Springer, 693â€“703.\n[14] Antoine Chatalic, Luigi Carratino, Ernesto De Vito, and Lorenzo Rosasco. 2022.\nMean nystrÃ¶m embeddings for adaptive compressive learning. In International\nConference on Artificial Intelligence and Statistics . PMLR, 9869â€“9889.\n[15] Antoine Chatalic, Vincent Schellekens, Florimond Houssiau, Yves-Alexandre\nDe Montjoye, Laurent Jacques, and RÃ©mi Gribonval. 2022. Compressive learning\nwith privacy guarantees. Information and Inference: A Journal of the IMA 11, 1\n(2022), 251â€“305.\n[16] Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki\nYamamoto, and David Ha. 2018. Deep learning for classical japanese literature.\narXiv preprint arXiv:1812.01718 (2018).\n[17] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. 2017.\nEMNIST: Extending MNIST to handwritten letters. In 2017 international joint\nconference on neural networks (IJCNN) . IEEE, 2921â€“2926.\n[18] International HapMap 3 Consortium et al .2010. Integrating common and rare\ngenetic variation in diverse human populations. Nature 467, 7311 (2010), 52.\n[19] Graham Cormode. 2013. Summary data structures for massive data. In Conference\non Computability in Europe . Springer, 78â€“86.\n[20] Graham Cormode. 2017. Data sketching. Commun. ACM 60, 9 (2017), 48â€“55.\n[21] Graham Cormode and Shan Muthukrishnan. 2005. An improved data stream\nsummary: the count-min sketch and its applications. Journal of Algorithms 55, 1\n(2005), 58â€“75.\n[22] Amit Daniely, Nevena Lazic, Yoram Singer, and Kunal Talwar. 2017. Short and\ndeep: Sketching and neural networks. In 5th International Conference on Learning\nRepresentations, ICLR 2017 .\n[23] Jyotikrishna Dass and Rabi Mahapatra. 2021. Householder Sketch for Accurate\nand Accelerated Least-Mean-Squares Solvers. In International Conference on\nMachine Learning . PMLR, 2467â€“2477.\n[24] Petros Drineas and Michael W Mahoney. 2016. RandNLA: randomized numerical\nlinear algebra. Commun. ACM 59, 6 (2016), 80â€“90.\n[25] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In International conference on\nmachine learning . PMLR, 1126â€“1135.\n[26] Philippe Flajolet, Ã‰ric Fusy, Olivier Gandouet, and FrÃ©dÃ©ric Meunier. 2007. Hy-\nperloglog: the analysis of a near-optimal cardinality estimation algorithm. In\nDiscrete Mathematics and Theoretical Computer Science . Discrete Mathematics\nand Theoretical Computer Science, 137â€“156.[27] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David\nSaxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami.\n2018. Conditional neural processes. In International Conference on Machine\nLearning . PMLR, 1704â€“1713.\n[28] Badih Ghazi, Rina Panigrahy, and Joshua Wang. 2019. Recursive sketches for\nmodular deep learning. In International Conference on Machine Learning . PMLR,\n2211â€“2220.\n[29] RÃ©mi Gribonval, Gilles Blanchard, Nicolas Keriven, and Yann Traonmilin. 2021.\nCompressive statistical learning with random feature moments. Mathematical\nStatistics and Learning 3, 2 (2021), 113â€“164.\n[30] RÃ©mi Gribonval, Antoine Chatalic, Nicolas Keriven, Vincent Schellekens, Laurent\nJacques, and Philip Schniter. 2020. Sketching datasets for large-scale learning\n(long version). arXiv preprint arXiv:2008.01839 (2020).\n[31] David Ha, Andrew M. Dai, and Quoc V. Le. 2017. HyperNetworks. In Interna-\ntional Conference on Learning Representations . https://openreview.net/forum?\nid=rkpACe1lx\n[32] David Ha and Douglas Eck. 2018. A Neural Representation of Sketch Drawings.\nInInternational Conference on Learning Representations . https://openreview.net/\nforum?id=Hy6GHpkCW\n[33] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. 2011. Finding Struc-\nture with Randomness: Probabilistic Algorithms for Constructing Approximate\nMatrix Decompositions. SIAM Rev. 53, 2 (2011), 217â€“288.\n[34] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. 2021.\nMeta-learning in neural networks: A survey. IEEE transactions on pattern analysis\nand machine intelligence 44, 9 (2021), 5149â€“5169.\n[35] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. 2019. Learning-Based\nFrequency Estimation Algorithms.. In International Conference on Learning Rep-\nresentations .\n[36] Meng Huang and Zhiqiang Xu. 2018. Solving systems of quadratic equations via\nexponential-type gradient descent algorithm. arXiv preprint arXiv:1806.00904\n(2018).\n[37] Piotr Indyk, Ali Vakilian, and Yang Yuan. 2019. Learning-based low-rank approx-\nimations. Advances in Neural Information Processing Systems 32 (2019).\n[38] Piotr Indyk, Tal Wagner, and David Woodruff. 2021. Few-shot data-driven algo-\nrithms for low rank approximation. Advances in Neural Information Processing\nSystems 34 (2021), 10678â€“10690.\n[39] Jiawei Jiang, Fangcheng Fu, Tong Yang, and Bin Cui. 2018. Sketchml: Accelerating\ndistributed machine learning with data sketches. In Proceedings of the 2018\nInternational Conference on Management of Data . 1269â€“1284.\n[40] Nicolas Keriven, Anthony Bourrier, RÃ©mi Gribonval, and Patrick PÃ©rez. 2018.\nSketching for large-scale learning of mixture models. Information and Inference:\nA Journal of the IMA 7, 3 (2018), 447â€“508.\n[41] Nicolas Keriven, Nicolas Tremblay, Yann Traonmilin, and RÃ©mi Gribonval. 2017.\nCompressive K-means. In 2017 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 6369â€“6373.\n[42] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[43] Ron Kohavi. 1996. Scaling up the accuracy of Naive-Bayes classifiers: a decision-\ntree hybrid. In Proceedings of the Second International Conference on Knowledge\nDiscovery and Data Mining . 202â€“207.\n[44] Adam R Kosiorek, Hyunjik Kim, and Danilo J Rezende. 2020. Conditional set\ngeneration with transformers. arXiv preprint arXiv:2006.16841 (2020).\n[45] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 international\nconference on management of data . 489â€“504.\n[46] Yann LeCun, Corinna Cortes, and C Burges. 1998. MNIST handwritten digit\ndatabase, 1998. URL http://yann. lecun. com/exdb/mnist (1998).\n[47] Fengfu Li, Bo Zhang, and Bin Liu. 2016. Ternary weight networks. arXiv preprint\narXiv:1605.04711 (2016).\n[48] Tian Li, Zaoxing Liu, Vyas Sekar, and Virginia Smith. 2019. Privacy for free:\nCommunication-efficient learning with differential privacy using sketches. arXiv\npreprint arXiv:1911.00972 (2019).\n[49] Cuiyu Liu, Chuanfu Xiao, Mingshuo Ding, and Chao Yang. 2022. Tensor-Based\nSketching Method for the Low-Rank Approximation of Data Streams. arXiv\npreprint arXiv:2209.14637 (2022).\n[50] Kaihui Liu, Liangtian Wan, and Feiyu Wang. 2018. Fast iteratively reweighted\nleast squares minimization for sparse recovery. In 2018 IEEE 23rd International\nConference on Digital Signal Processing (DSP) . IEEE, 1â€“4.\n[51] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng\nGao, and Jiawei Han. 2020. On the Variance of the Adaptive Learning Rate\nand Beyond. In Proceedings of the Eighth International Conference on Learning\nRepresentations (ICLR 2020) .\n[52] Simin Liu, Tianrui Liu, Ali Vakilian, Yulin Wan, and David P Woodruff. 2020.\nLearning the positions in countsketch. arXiv preprint arXiv:2007.09890 (2020).\n[53] Luke Metz, C Daniel Freeman, Samuel S Schoenholz, and Tal Kachman. 2021.\nGradients are not all you need. arXiv preprint arXiv:2111.05803 (2021).\n[54] Vishal Monga, Yuelong Li, and Yonina C Eldar. 2021. Algorithm unrolling:\nInterpretable, efficient deep learning for signal and image processing. IEEE\n10\n\nCompressive Meta-Learning\nSignal Processing Magazine 38, 2 (2021), 18â€“44.\n[55] SÃ©rgio Moro, Paulo Cortez, and Paulo Rita. 2014. A data-driven approach to\npredict the success of bank telemarketing. Decision Support Systems 62 (2014),\n22â€“31.\n[56] Vishvak Murahari, Carlos E Jimenez, Runzhe Yang, and Karthik Narasimhan.\n2022. DataMUX: Data Multiplexing for Neural Networks. arXiv preprint\narXiv:2202.09318 (2022).\n[57] Rasmus Pagh. 2013. Compressed matrix multiplication. ACM Transactions on\nComputation Theory (TOCT) 5, 3 (2013), 1â€“17.\n[58] Maria Perera, Daniel Mas Montserrat, MÃ­riam BarrabÃ©s, Margarita Geleta, Xavier\nGirÃ³-i Nieto, and Alexander G Ioannidis. 2022. Generative moment matching\nnetworks for genotype simulation. In 2022 44th Annual International Conference\nof the IEEE Engineering in Medicine & Biology Society (EMBC) . IEEE, 1379â€“1383.\n[59] Ali Rahimi and Benjamin Recht. 2007. Random features for large-scale kernel\nmachines. Advances in neural information processing systems 20 (2007).\n[60] Vincent Schellekens and Laurent Jacques. 2018. Compressive classification\n(machine learning without learning). arXiv preprint arXiv:1812.01410 (2018).\n[61] Vincent Schellekens and Laurent Jacques. 2018. Quantized compressive k-means.\nIEEE Signal Processing Letters 25, 8 (2018), 1211â€“1215.\n[62] Vincent Schellekens and Laurent Jacques. 2020. When compressive learning\nfails: blame the decoder or the sketch? arXiv preprint arXiv:2009.08273 (2020).\n[63] JÃ¼rgen Schmidhuber. 1987. Evolutionary principles in self-referential learning, or\non learning how to learn: the meta-meta-... hook . Ph. D. Dissertation. Technische\nUniversitÃ¤t MÃ¼nchen.\n[64] Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical networks for\nfew-shot learning. Advances in neural information processing systems 30 (2017).\n[65] Sebastian Thrun and Lorien Pratt. 1998. Learning to learn: Introduction and\noverview. In Learning to learn . Springer, 3â€“17.\n[66] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al .2016.\nMatching networks for one shot learning. Advances in neural information pro-\ncessing systems 29 (2016).\n[67] Di Wang, Jinshan Zeng, and Shao-Bo Lin. 2020. Random sketching for neural\nnetworks with ReLU. IEEE Transactions on Neural Networks and Learning Systems\n32, 2 (2020), 748â€“762.\n[68] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. 2018.\nDataset distillation. arXiv preprint arXiv:1811.10959 (2018).\n[69] David P Woodruff. 2014. Sketching as a tool for numerical linear algebra. arXiv\npreprint arXiv:1411.4357 (2014).\n[70] Daniel J Wu, Andrew C Yang, and Vinay U Prabhu. 2020. Afro-MNIST:\nSynthetic generation of MNIST-style datasets for low-resource languages.\narXiv:2009.13509 [cs.CV]\n[71] Ruonan Yu, Songhua Liu, and Xinchao Wang. 2023. Dataset distillation: A com-\nprehensive review. IEEE transactions on pattern analysis and machine intelligence\n46, 1 (2023), 150â€“170.\n[72] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R\nSalakhutdinov, and Alexander J Smola. 2017. Deep sets. Advances in neural\ninformation processing systems 30 (2017).\n[73] Yan Zhang, Jonathon Hare, and Adam Prugel-Bennett. 2019. Deep set prediction\nnetworks. Advances in Neural Information Processing Systems 32 (2019).\nA COMPRESSIVE META-LEARNING\nFRAMEWORK\nA.1 Sketch Update and Pooling Operations\nSketches can be easily updated: new samples can be added and\nremoved from a sketch, and sketches can be combined or split.\nIn order to merge two sketches ğ‘§ğ‘computed with ğ‘ğ‘samples\nxa={ğ‘¥ğ‘1,ğ‘¥ğ‘2,...,ğ‘¥ğ‘ğ‘}andğ‘§ğ‘computed with ğ‘ğ‘samples xb=\n{ğ‘¥ğ‘1,ğ‘¥ğ‘2,...,ğ‘¥ğ‘ğ‘}, a new combined sketch can be computed as:\nğ‘§â€²=ğ‘ğ‘ğ‘§ğ‘+ğ‘ğ‘ğ‘§ğ‘\nğ‘ğ‘+ğ‘ğ‘(23)\nNote that in order to combine two sketches ğ‘§ğ‘andğ‘§ğ‘, the original\nsamples xaandxbare not required, but only the sketches and their\nrespective size ğ‘ğ‘andğ‘ğ‘. This makes the sketching framework\nwell suited in online and federated learning applications, where new\nsamples become available sequentially and can be continuously\nincorporated within the sketch, and in distributed and federated\nlearning scenarios, where different parties compute sketches locallywith their data, then share the sketches which are aggregated later\non.\nSimilarly, sets of samples can be removed from sketches. To\nremove a set of ğ‘ğ‘samples xb={ğ‘¥ğ‘1,ğ‘¥ğ‘2,...,ğ‘¥ğ‘ğ‘}from a sketch\nğ‘§â€²computed with ğ‘=ğ‘ğ‘+ğ‘ğ‘samples, a subtraction between\nsketches is required:\nğ‘§ğ‘=ğ‘ğ‘§â€²âˆ’ğ‘ğ‘ğ‘§ğ‘\nğ‘(24)\nAs in the sketch merging operation, the original samples are not\nrequired to subtract sketches, but only the computed sketches and\ntheir size. This type of operation can be useful in settings where\nthe information of some samples needs to be forgotten. This can\nhappen in medical applications, where patients decide to remove\ntheir data from databases, or in other services where users leave and\nrequest to have their data removed from databases and algorithms.\nWhile in traditional supervised learning, a new set of models and\nparameters might have to be trained without the removed data,\nsketching-based learning just requires one subtraction between\nsketches and a forward pass through the query network to obtain\nupdated parameters.\nWe explore different pooling operations to combine the sample-\nlevel embeddings into a dataset-level sketch. Table 4 shows mul-\ntiple layers and how new samples (or sketches) can be added and\nremoved from the sketch. The mean andsummation pooling are\nequivalent, up to a scaling factor, with the mean requiring the value\nofğ‘, the total elements in the sketch. The max operation does\nnot allow for removal of samples; however, soft versions of the\nmax operation such as p-norm (with a large enough p) and the\nlog-sum-exp can be used, which allow for addition and removal\nof samples. Note that due to rounding errors, removal of samples\nfrom p-norm andlog-sum-exp layers can lead to noisy sketches. The\nminpooling layer and its soft approximations can easily be applied\nby making use of the fact that min(ğ‘,ğ‘)=âˆ’max(âˆ’ğ‘,âˆ’ğ‘). Further-\nmore, by re-defining the last layer of the Sketch Projection layer,\nand the first layer of the Query Network, all the pooling operations\n(except max andmin) can be defined as a mean pooling operation,\nassuming that ğ‘is known (e.g. by considering ğœ™â€²(ğ‘¥ğ‘–)=ğ‘’ğœ™(ğ‘¥ğ‘–)\nandğœ“â€²(ğ‘§)=ğœ“(log(ğ‘ğ‘§)), the log-sum-exp can be replaced by a\nmean pooling layer). Therefore, the theoretical framework of CL is\napplicable regardless the adopted pooling layer.\nTable 4: Multiple pooling operations.\nPooling Layer Merging / Addition Removal\nSum ğ‘§â€²=ğ‘§ğ‘+ğ‘§ğ‘ğ‘§â€²=ğ‘§ğ‘âˆ’ğ‘§ğ‘\nMean ğ‘§â€²=ğ‘ğ‘ğ‘§ğ‘+ğ‘ğ‘ğ‘§ğ‘\nğ‘ğ‘+ğ‘ğ‘ğ‘§â€²=ğ‘ğ‘ğ‘§ğ‘âˆ’ğ‘ğ‘ğ‘§ğ‘\nğ‘ğ‘âˆ’ğ‘ğ‘\nMax ğ‘§â€²=max(ğ‘§ğ‘,ğ‘§ğ‘) N/A\np-Norm ğ‘§â€²=ğ‘âˆšï¸ƒ\nğ‘§ğ‘\nğ‘+ğ‘§ğ‘\nğ‘ğ‘§â€²=ğ‘âˆšï¸ƒ\nğ‘§ğ‘\nğ‘âˆ’ğ‘§ğ‘\nğ‘\nLog-Sum-Exp ğ‘§â€²=log(ğ‘’ğ‘§ğ‘+ğ‘’ğ‘§ğ‘)ğ‘§â€²=log(ğ‘’ğ‘§ğ‘âˆ’ğ‘’ğ‘§ğ‘)\nB EXTENDED RELATED WORK\nThree of the main areas where sketching is found are data stream-\ning, linear algebra, and compressive learning. Note that other works\n11\n\nMas Montserrat et al.\nsuch as some computer vision tasks dealing with pictures of draw-\nings might use the term sketching in a completely unrelated mean-\ning and application.\nSketches for data streaming. Data sketching has been widely\napplied in multimedia summarization, web and streaming appli-\ncations, and data processing [ 19,20]. Such techniques must be\ncomputationally efficient and be able to handle constant streams\nof high-dimensional data. Many sketching methods have been de-\nveloped to approximately capture the frequency or membership of\nitems, information about quantiles, or to count distinct elements.\nSome key methods include Count-Min [ 21], Count-Sketch [ 13], and\nBloom Filters [ 9]. Count-Min performs multiple random projec-\ntions using a sparse non-negative linear mapping. These random\nprojections act as hashing functions to map the input elements into\na lower dimensional vector (sketch) that keeps a compact count of\nthe frequency of each item. A set of inverse linear projections are\nused to map the sketch into the estimated frequency per item. Each\nlinear projection reconstructs a different set of frequencies and the\nfinal estimate can be obtained by selecting the minimum between\nthem. Count-Sketch follows the same approach as Count-Min, but\nallows negative values in the random projections, and substitutes\nthe minimum operation with a mean or a median. Similarly, Bloom\nFilter follows an approach resembling Count-Min, but always works\nwith Boolean values. Some other examples of widely used sketches\ninclude HyperLogLog [ 26], AMS Sketch [ 3], and Tensor Sketch [ 57].\nRecent works have tried to incorporate supervised learning into\nfrequency estimation pipelines [ 1,35]. Moreover, the work in [ 45]\nmakes a clear link between indexing functions and learnable mod-\nels showing that several data structures can be learned, including\nBloom Filters.\nSketches for learning-based numerical linear algebra. Recent works\non sketching-based numerical linear algebra, have explored learn-\ning the sketching projections ğ‘†[37,38,49,52]. While these ap-\nproaches share some similarities to our proposed approach, they\nhave some key distinctions: only linear projections are learned, and\nthe techniques are only applicable to settings that can be framed as\nmatrix decomposition or similar, while our proposed framework is\napplicable to any learning task as long as a differentiable function\ncan be defined (e.g. predicting weights of an autoencoder).\nMultiplexing Networks. Recently, DataMUX [ 56], a multiplex-\ning approach with neural networks has been proposed, where se-\nquences of samples are compressed into a compact representation\nthat preserves its ordering. While architecturally similar to our ap-\nproach, DataMux differs conceptually with our proposed approach.\nDataMux performs an average of multiple embeddings and then dis-\nentangles (demultiplexing) each representation to predict a desired\nlabel for each element in the sequence. This compact embedding\nallows for the transmission of utility-preserving information along\na channel. On the other hand, the proposed Sketch-Query Network\nlearns a sample permutation invariant compact representation that\ncaptures the agglomerate essential information from a set to per-\nform a specific task. In other words, SQNet learns to summarize a\nset of samples to represent the set as a whole.\nNeural Networks and Sketching. Multiple recent works have at-\ntempted to combine the benefits of sketching into multiple aspectsof neural networks and machine learning. The work in [ 22] shows\nthat the first layer of a neural network can be replaced by a sketch-\ning mechanism with a bounded loss of information. SketchML [ 39]\nis a framework that makes use of sketches to compress gradient\ninformation in a distributed training setting, obtaining consider-\nable speedups on gradient descent-based methods. The work in\n[28] makes use of recursive randomized sketches applied to the\nintermediate outputs of modular neural networks creating semantic\nsummaries of the data. DiffSketch [ 48] is a framework that makes\nuse of Count-Sketch and its inherent differential privacy properties\nto perform private distributed training. The work in [ 67] applies\nsketching to reduce the computational requirements for training a\nReLU neural network.\nDiscussion. While the previously described fields try to obtain a\ncompact representation from data to extract information, there are\nsome key differences, specifically regarding the nature of the com-\nputed sketch. A main difference between the sketching operations\nused for Low-rank approximation (LRA) in numerical linear algebra\n(NLA) and the sketching approaches applied in data streaming and\ncompressive learning (CL) is that in LRA-based applications it is\ncommon to provide a representation with a size that scales with ğ‘\nand allows approximately reconstructing the original data, namely\nthe sketch of a ğ‘Ã—ğ‘‘matrix has size ğ‘™Ã—ğ‘‘, withğ‘™â‰ªğ‘. In many\ndata streaming applications and compressive learning applications,\nthe size of the sketch is constant and independent of ğ‘, where the\ncompleteğ‘Ã—ğ‘‘dataset is compacted into a representation that\ncaptures global information and statistics of interest. Furthermore,\ndata streaming sketching-based applications typically deal with\nvery large dimensionality ğ‘‘inputs (e.g. from thousands to millions),\nwhile sketching for LRA deals with medium dimensionality (e.g.\nthousands), and compressive learning is typically applied to lower\ndimensional inputs (e.g. from 2 to hundreds). Another differentiat-\ning aspect, is the dimensionality of the sketch with respect to the\ninput dimensionality, while in streaming applications it is common\nthatğ‘šâ‰ªğ‘‘, in compressive learning we usually have the oppo-\nsite relationship ğ‘š>ğ‘‘. Furthermore, in many LRA and streaming\napplications, linear projections are commonly used, while in CL,\nnon-linear mappings are applied to each sample in order to compute\nthe sketch.\nC GENERALIZATION BOUNDS PROOF\nStability-based bounds can be easily applied within the Compres-\nsive Meta-Learning framework in order to obtain generalization\nguarantees of the parameters predicted by the Sketch-Query Net-\nwork. The generalization properties of the predicted parameters\nË†ğœƒ=(ğœ“â—¦Î¦)ğœ”(x)are characterized by the complexity of the Sketch\nand Query networks. Namely, the difference between the empiri-\ncalLand expected error Lğ‘‡of the predicted parameters can be\nbounded by the maximum norm of the sketch ğ›½ğœ™and loss function\nğ‘€â„“, the Lipschitz constant of the Query Network ğœŒğœ“and of the loss\nğœŒâ„“. With probability 1âˆ’ğ›¿:\nLğ‘‡(Ë†ğœƒ)<L(Ë†ğœƒ)+2ğ›½ğœ™ğœŒğœ“ğœŒâ„“\nğ‘+(4ğ›½ğœ™ğœŒğœ“ğœŒâ„“+ğ‘€â„“)âˆšï¸‚\nln1/ğ›¿\n2ğ‘(25)\n12\n\nCompressive Meta-Learning\nTheorem 12 of [ 11] states that the generalization bound of a\nlearning algorithm ğ´(S)with uniform stability ğ›½, a loss function\nbounded by ğ‘€, and a training set S, with|S|=ğ‘, one has that\nwith probability at least 1âˆ’ğ›¿:\nLğ‘‡(Ë†ğœƒ)<L(Ë†ğœƒ)+2ğ›½+(4ğ‘ğ›½+ğ‘€)âˆšï¸‚\nln1/ğ›¿\n2ğ‘(26)\nBy noting that the uniform stability constant ğ›½is a function of\nthe maximum norm of the sketch ğ›½ğœ™and the Lipschitz constant of\nthe Query Network ğœŒğœ“and of the loss ğœŒâ„“, proving the generalization\nbound for Sketch-Query Network can be easily proved.\nAn algorithm ğ´has uniform stability [ 11], with respect to a loss\nfunctionâ„“if for all possible training datasets S:\nâˆ€S,âˆ€ğ‘–,||â„“(ğ´(S),Â·)âˆ’â„“(ğ´(S\\ğ‘–),Â·)||âˆâ‰¤ğ›½ (27)\nNote that the maximum norm of the sketch is related to how\nmuch a sketch can change if the ğ‘–th sample is changed:\n||Î¦(S)âˆ’ Î¦(Sğ‘–)||=||1\nğ‘âˆ‘ï¸\nğ‘¥ğ‘—âˆˆSğœ™(ğ‘¥ğ‘—)âˆ’1\nğ‘âˆ‘ï¸\nğ‘¦ğ‘—âˆˆSğ‘–ğœ™(ğ‘¦ğ‘—)|| (28)\n=||1\nğ‘ğœ™(ğ‘¥ğ‘–)âˆ’1\nğ‘ğœ™(ğ‘¦ğ‘–)|| (29)\nâ‰¤2\nğ‘||ğœ™(ğ‘¥ğ‘–)|| (30)\n=2ğ›½ğœ™\nğ‘(31)\nBy considering an ğœŒğœ“-Lipschitz Query Network, and a ğœŒâ„“-Lipschitz\nloss function, it follows that\n||â„“((ğœ“â—¦Î¦)(S),Â·)âˆ’â„“((ğœ“â—¦Î¦)(S\\ğ‘–),Â·)||âˆâ‰¤ğ›½=ğ›½ğœ™ğœŒğœ“ğœŒâ„“\nğ‘\nâˆ€S,âˆ€ğ‘–, (32)\nBy replacing the uniform stability constant ğ›½with the stability\nbounds, we conclude the proof of the proposed bound.\nD DIFFERENTIAL PRIVATE SKETCH-BASED\nLEARNING\nBecause only access to the sketch is needed in CL, and not to the\noriginal data samples, sketching-based learning is a good approach\nwhen data cannot be shared due to its privacy restrictions. Previous\nworks [ 15] have explored incorporating approximate differential\nprivacy (DP) into the sketch generation in order to provide statisti-\ncal guarantees about the privacy of the samples used to compute\nthe sketch. Here, we explore the use of ( ğœ–,ğ›¿)-DP within our pro-\nposed Sketch-Query Network. First, we perform norm clipping at\nthe output of the sketch projection:\nğœ™â€²(ğ‘¥ğ‘–)=ğœ™(ğ‘¥ğ‘–)min(1,ğ‘†\n||ğœ™(ğ‘¥ğ‘–)||) (33)\nwhereğ‘†=maxğ‘¥(||ğœ™(ğ‘¥)||)is the maximum L2 norm of the sketch\nprojection across the meta-training samples. ğ‘†can either be set\nmanually if an upper bound of the value is known, or its empirical\nmaximum across all the samples within the meta-training set can\nbe used. This ensures that when doing inference in new unseen\ndatasets, the L2 norm of the projection will always be boundedfor allğ‘¥:||ğœ™â€²(ğ‘¥)|| â‰¤ğ‘†.ğ‘†also represents the sensitivity of the\nunnormalized sketch ğ‘£=Ãğ‘\nğ‘–=1ğœ™â€²(ğ‘¥ğ‘–). Namely, when a new ğ‘¥ğ‘–\nis added or removed from ğ‘£, its L2 norm will change at most ğ‘†â‰¥\n||Ãğ‘\nğ‘–=1ğœ™â€²(ğ‘¥ğ‘–)âˆ’Ãğ‘âˆ’1\nğ‘–=1ğœ™â€²(ğ‘¥ğ‘–)||. Note that the value of ğ‘†is computed\nusing only the (meta-)training samples, therefore not violating the\nprivacy requirements when used during test inference. We privatize\nthe clipped sum with a Gaussian mechanism parameterized by\n(ğœ–1,ğ›¿), and (optionally) the count with a Laplace mechanism with\nparameterğœ–2, composing to a total privacy budget ğœ–=ğœ–1+ğœ–2. The\n(ğœ–,ğ›¿)-DP sketch ğ‘§ğœ–,ğ›¿can be computed as:\nğ‘§ğœ–,ğ›¿=ğ‘£+ğœ‰\nğ‘+ğœ(34)\nwhereğœ‰âˆˆRğ‘š,ğœ‰âˆ¼ N( 0,ğœ2Iğ‘š)is an additive multivariate\nGaussianğ‘š-dimensional noise with a standard deviation ğœapplied\nto the unnormalized sketch ğ‘£. The standard deviation is a function\nofğœ–,ğ›¿, and the sensitivity of the sketch ğ‘†, such thatğœ=ğœ‚(ğ‘†,ğœ–1,ğ›¿),\nas defined in [ 4,15].ğœâˆ¼Laplace(1/ğœ–2)is a Laplacian noise applied\nto the count of elements, ğ‘, within the sketch, with ğœ–=ğœ–1+ğœ–2.\nSmall values of ğœ–andğ›¿will lead to large values of variance to\nprovide strong privacy guarantees.\nDP ensures that any post-processing applied to ğ‘§ğœ–,ğ›¿will pre-\nserve its privacy guarantees. We perform a clamping of the sketch\nto remove potential out-of-range values due to the noise addition:\nğ‘§ğ‘\nğœ–,ğ›¿=min(max(ğ‘§ğœ–,ğ›¿,ğ‘§min),ğ‘§max), where min(Â·,Â·)andmax(Â·,Â·)are\napplied elementwise, and ğ‘§minandğ‘§maxare the minimum and max-\nimum values of the sketch, either specified manually or by selecting\nthe empirical value during the meta-training process. Then, we can\nuse the query network to learn differentially private parameters:\nË†ğœƒğœ–,ğ›¿=ğœ“(ğ‘§ğ‘\nğœ–,ğ›¿)\nBoth the dimensionality of the sketch and the number of samples\nused to compute it will have an important impact on how much\ninformation is preserved after adding differential privacy. For exam-\nple, for projections ğœ™(ğ‘¥ğ‘–)with an absolute value bounded by 1, their\nsensitivity is equal to the square root of its dimensionality ğ‘†=âˆšğ‘š.\nTherefore, as more dimensions the sketch has, the more amount of\nGaussian noise needs to be added. Furthermore, by looking at Eq.\n34, one can easily show that:\nlim\nğ‘â†’âˆğ‘§ğœ–,ğ›¿=ğ‘§ (35)\nE PCA AND RIDGE REGRESSION\nE.1 Compressive PCA and Linear Regression\nCompressive PCA (CPCA) and Compressive Ridge Regression (CRR)\nre-frame the task of parameter prediction (Principal Component\nprojection, and linear regression weights/coefficients respectively),\nwith the task of predicting the covariance matrix of the data:\nË†ğ‘…=arg min\nğ‘…C(ğ‘…|ğ‘§) (36)\nsuch that Ë†ğ‘…â‰ˆğ‘…, andğ‘…=1\nğ‘Ãğ‘\nğ‘–=1ğ‘¥ğ‘–ğ‘¥ğ‘‡\nğ‘–. For a given ğ‘…, the ridge\nregression and PCA parameters can be recovered. Specifically, ridge\nregression parameters can be obtained as:\nğœƒReg=ğ‘…12(ğ‘…22+ğœ†ğ¼)âˆ’1(37)\n13\n\nMas Montserrat et al.\nFigure 5: PCA results for the hapmap 43dataset with differential privacy results.\nwhereğ‘…=\u0012ğ‘…11ğ‘…12\nğ‘…21ğ‘…22\u0013\n,ğ‘¥ğ‘–=[ğ‘¥(ğ‘¦)\nğ‘–,ğ‘¥(ğ‘¥)\nğ‘–]are the regression la-\nbelsğ‘¥(ğ‘¦)\nğ‘–and input features ğ‘¥(ğ‘¥)\nğ‘–of theğ‘–th sample concatenated,\nğ‘…12=ğ‘…ğ‘‡\n21is the empirical cross-correlation between labels and\nfeaturesğ‘…12=1\nğ‘Ãğ‘\nğ‘–=1ğ‘¥(ğ‘¦)\nğ‘–ğ‘¥(ğ‘¥)ğ‘‡\nğ‘–, andğ‘…22is the correlation be-\ntween features ğ‘…22=1\nğ‘Ãğ‘\nğ‘–=1ğ‘¥(ğ‘¥)\nğ‘–ğ‘¥(ğ‘¥)ğ‘‡\nğ‘–.ğœ†is the regularization\nparameters which is typically set through cross-validation and in\nour experiments is set as ğœ†=||ğ‘…22||ğ¹for simplicity.\nThe PCA parameters can be recovered as the eigendecomposition\nof the covariance matrix:\nğ‘…=ğœƒğ·ğœƒğ‘‡(38)\nwhereğœƒare the eigenvectors and ğ·is a diagonal matrix with\neigenvalues of ğ‘…. Both Eq. 37 and 38 show that ğ‘…is a sufficient\nstatistic to properly recover the ridge regression and PCA parame-\nters.By simply setting ğœ™(ğ‘¥ğ‘–)=ğ‘¥ğ‘–ğ‘¥ğ‘‡\nğ‘–a sketch such as ğ‘§=ğ‘…provides\na perfect recovery from the parameters of interest. However, the\nsize ofğ‘…grows quadratically with the number of input dimensions,\ntherefore Compressive Learning adopts a more compact represen-\ntation with dimensionality ğ‘šâ‰ªğ‘‘2:\nğ‘§=1\nğ‘ğ‘âˆ‘ï¸\nğ‘–=1ğ´vec(ğ‘¥ğ‘–ğ‘¥ğ‘‡\nğ‘–)=ğ´vec(ğ‘…) (39)\nwhere vec(Â·)flattens the ğ‘‘Ã—ğ‘‘matrix into a ğ‘‘2vector, and ğ´\nis a random matrix with dimensions ğ‘šÃ—ğ‘‘2. Note that ğœ™(ğ‘¥ğ‘–)=\nğ´vec(ğ‘¥ğ‘–ğ‘¥ğ‘‡\nğ‘–)can be equivalently framed as random square features\nğœ™(ğ‘¥ğ‘–)=(ğµğ‘¥ğ‘–)2, whereğµcan be obtained from a given ğ´. Com-\npressive PCA and Ridge Regression [ 29,30] re-frames the learning\nproblem as minimizing the following objective:\n14\n\nCompressive Meta-Learning\nFigure 6: PCA data reconstruction error for different number of principal components considered, and different sketch sizes.\nË†ğ‘…=arg min\nğ‘…||ğ´vec(ğ‘…)âˆ’ğ‘§||2(40)\nwhereğ´is the given random matrix and ğ‘§is the empirical sketch,\ncomputed as described in Eq. 39. Ë†ğ‘…can be found by minimizing Eq.\n40 through any desired optimization procedure. In fact, the linear\nprojection that minimizes Eq. 40 can be obtained in closed-form by\ncomputing the pseudo-inverse of the randomized projection:\nvec(Ë†ğ‘…)=ğ´+ğ‘§ (41)\nBecauseğ‘…is a symmetric matrix, the sketching process (Eq. 39)\nand optimization objectives (Eq. 40) can be framed by using only the\nvectorized lower (or upper) triangular elements of ğ‘…, i.e. replacing\nvec(Ë†ğ‘…)by vec LT(Ë†ğ‘…)âˆˆRğ‘‘(ğ‘‘+1)\n2.E.2 Neural CPCA and CRR\nHere we provide extended details for the Neural-based CPCA and\nCRR applications described in Section 4.1. First, note that we mini-\nmize the L1 error in Eq. 17, but other distance metrics such as the\nL2 loss could be used instead of the L1. We select the L1 loss as it\nprovided a more stable training process.\nWe standardize all our meta-training and evaluation datasets to\nhave zero mean and unit variance, to ensure that the ground truth\ncovariance matrix has bounded values, such that |vecLT(ğ‘…)ğ‘—|â‰¤1.\nNote that in practical applications, the information of the mean\nand variance can be treated as additional sketches. By using a Tanh\nactivation function at the end of the query network, we ensure that\nthe predicted covariance matrix has the same range of values as\nthe ground truth covariance matrix. After meta-training the sketch-\nquery network, the covariance matrix vecLT(Ë†ğ‘…)can be predicted\n15\n\nMas Montserrat et al.\n(a) Differential privacy results, for different ğœ–.\n (b) Differential privacy results, for different sketch sizes.\nFigure 7: Differential privacy results on MNIST. (a) Varying ğœ–. (b) Varying sketch size.\nin new unseen dataset, and the PCA or regression coefficients can\nbe easily recovered.\nE.3 Experimental details\nIn order to evaluate the quality of the predicted PC projection matrix\nË†ğœƒ, we compute the reconstruction error on the testing datasets as:\nErrPCA(ğ‘‹test,Ë†ğœƒ)=1\nğ‘‘ğ‘‘âˆ‘ï¸\nğ‘Ÿ=1||ğ‘‹testâˆ’Ë†ğœƒğ‘ŸË†ğœƒğ‘‡\nğ‘Ÿğ‘‹test||2(42)\nwhere Ë†ğœƒğ‘Ÿis the PCA projection matrix containing only the first\nğ‘Ÿprincipal components. The error averages the reconstruction\nthrough the values of ğ‘Ÿfrom 1toğ‘‘. We report the relative errors,\nby computing the logarithmic ratio between the average recon-\nstruction error (LRE) of PCA with the predicted PC projection Ë†ğœƒ,\nErrPCA(ğ‘‹test,Ë†ğœƒ), and the average reconstruction error of PCA with\nthe ground-truth PC projection computed from the empirical co-\nvariance matrix using the complete dataset, ğœƒ, Err PCA(ğ‘‹test,ğœƒ):\nLRE PCA(ğ‘‹test,Ë†ğœƒ,ğœƒ)=logErrPCA(ğ‘‹test,Ë†ğœƒ)\nErrPCA(ğ‘‹test,ğœƒ)(43)\nSimilarly, we evaluate the ridge regression error by computing\nthe mean square error as follows:\nErrReg(ğ‘‹test,Ë†ğœƒ)=||ğ‘‹(ğ‘¦)\ntestâˆ’Ë†ğœƒğ‘‹(ğ‘¥)\ntest||2(44)\nwhere Ë†ğœƒare the regression coefficients, ğ‘‹(ğ‘¥)\ntestandğ‘‹(ğ‘¦)\ntestare the\ninput features and continuous labels for a given dataset. Similarly\nto the task of PCA, we compute the log ratio of the error when the\npredicted regression coefficients are used relative to the error when\nusing the ground truth regression coefficients obtained by using\nthe empirical covariance matrix:\nLRE Reg(ğ‘‹test,Ë†ğœƒ,ğœƒ)=logErrReg(ğ‘‹test,Ë†ğœƒ)\nErrReg(ğ‘‹test,ğœƒ)(45)\nThe Sketch-Query-Network for Compressive PCA and Compres-\nsive Ridge regression is trained by computing sketches with 4096samples randomly selected per dataset, with a total of 64 randomly\nchosen meta-training datasets per batch. A total of 196 input fea-\ntures are randomly selected at each iteration during training. The\nAdam optimizer with a learning rate of 3Ã—10âˆ’5is used.\nE.3.1 Extended CPCA Results. Figure 5 shows a visualization on\nthe first two principal components for a human genome dataset, for\ndifferent sketch sizes and differential privacy. Projections with the\nSketch-Query Network follow the ground truth better than with\nRandomized CPCA, both with added differential privacy (shown at\nğœ–=1.0) and without. In Figure 6 we show the data reconstruction\nerror for the test datasets when different number of principal com-\nponents are used, for different sketch sizes. Figure 7a and Figure 7b\nshow results on the MNIST dataset including differential privacy for\ndifferentğœ–and different sketch sizes, respectively. We provide tables\nwith information on how the datasets are organized in Table 7 and\nTable 8, and a per-dataset break-down of the reconstruction error\nin Table 9 and Table 10.\nAfter meta-training, the Sketch-Query-Network can predict the\nPCA projection and the linear regression parameters for a given\ndataset in an average time of 0.1 seconds in a V100 GPU.\nF COMPRESSIVE K-MEANS\nCompressive k-means (Figure 8) tries to â€œlearnâ€ (infer) the ğ‘˜cen-\ntroids reducing the following error function:\nËœy=arg min\ny||Î¦(y)âˆ’Î¦(x)||2=arg min\ny||Î¦(y)âˆ’ğ‘§||2(46)\nDifferent optimization approaches can be adopted to find the\noptimal centroids y. Here we use gradient-based optimization and\nunroll (unfold) the optimization procedure to be able to learn the\nprojection function, see Figure 9. We provide a detailed description\nof the unfolded process with SGD for simplicity, but we adopt the\nAdam optimizer [ 42] in our experiments, which, after unfolding,\nleads to the following iterative process:\nË†yi=ğœ“(ğ‘§,Ë†yiâˆ’1,Î¦)=Ë†yiâˆ’1âˆ’ğ»(Ë†yiâˆ’1,ğ‘§,Î¦,â„ğ‘–) (47)\n16\n\nCompressive Meta-Learning\nFigure 8: Compressive vs traditional k-means. Dashed yellow line (left) represents the gradient.\nFigure 9: Unrolled compressive k-means with Adam-based SQNet.\nHereğ»is a function of the empirical sketch ğ‘§, the current cen-\ntroid estimates Ë†yiâˆ’1, and a memory â„ğ‘–âˆ’1:\nâ„ğ‘–=(ğ‘šğ‘–,ğ‘£ğ‘–)=(ğ›½1ğ‘šğ‘–âˆ’1+(1âˆ’ğ›½1)ğ‘”ğ‘–,ğ›½2ğ‘£ğ‘–âˆ’1+(1âˆ’ğ›½2)ğ‘”2\nğ‘–)(48)\nğ‘”ğ‘–=ğ‘”(Ë†yiâˆ’1,ğ‘§)=âˆ‡L(Î¦(Ë†yiâˆ’1),ğ‘§)=âˆ‡||Î¦(Ë†yiâˆ’1)âˆ’ğ‘§||2(49)\nğ»(Ë†yiâˆ’1,ğ‘§,Î¦,â„ğ‘–âˆ’1)=ğ›¼\n1âˆ’ğ›½ğ‘–\n1ğ‘šğ‘–âˆšï¸\nğ‘£ğ‘–/1âˆ’ğ›½ğ‘–\n2(50)\nThe previous equations simply describe the Adam optimization\nsteps. Note that the centroid inference procedure depends both\non the optimization algorithm used (e.g. Adam) and the sketching\nfunction Î¦, and by unfolding the iterative inference of the centroids\nboth the sketching function and the optimization procedure can\nbe jointly learnt such that the loss function of traditional k-means\nis minimized. The training procedure is as follows: (1) compute\nthe sketch Î¦(x)with the set x, (2) randomly instantiate initialcentroid cluster estimates Ë†y0, (3) iteratively update Ë†yifollowing the\nequations 47,48,49,50, (4) after ğ‘¡steps, compute the k-mean loss\n(Eq. 46) or the Hungarian loss with xandyt, (5) finally, update the\nparameters of Î¦to reduce the loss.\nG SKETCH-CONDITIONAL AUTOENCODERS\nThe sketch-based autoencoder introduced in section 4.3 makes use\nof a sketch-conditional encoder ğ‘¢=ğ‘“ğœƒ(ğ‘¥)and a sketch-conditional\ndecoder Ë†ğ‘¥=ğ‘”ğœƒ(ğ‘¢). Figure 10 depicts the differences between reg-\nular autoencoders and sketch-conditional autoencoders. Regular\nautoencoders are trained with a dataset of interest following the\nsame underlying distribution as the samples expected during in-\nference. On the other hand, sketch-based autoencoders substitute\nthe traditional training stage with a fast sketching operation (i.e.\ninference with the sketch network) to adapt a meta-trained autoen-\ncoder to a new dataset or target distribution. An important aspect of\nthe sketch-based autoencoder is its built-in active learning nature.\nBecause dataset-specific learning is substituted by sketching, by\n17\n\nMas Montserrat et al.\nFigure 10: Comparison of traditional autoencoders and the sketch-conditional autoencoder.\nsimply updating the sketch, the network can adapt to new datasets,\nto distribution shifts, or to improve its performance by including\nmore samples in the sketch. As linear sketching is used, adding\nor removing a sample from the sketch can be easily done with an\naddition or subtraction operation.\nAnother way to frame sketch-conditional autoencoders is by\nunderstanding the sketching network as a hypernetwork [ 31] that\npredicts some weights of a primary network (encoder-decoder pair).\nWhile common hypernetworks generate a set of weights for a given\ninput sample, here the weights are generated given a complete\ndataset. Furthermore, sketches can be seen as external memories for\nmemory-augmented neural networks, which can be easily updated\n(by simply merging sketches), and can capture in a distributed\nmanner the properties and shape of the density distribution.\nG.1 Compression and information theory\nThe presented autoencoders map ğ‘‘-dimensional boolean sequences\nintoğ‘š-dimensional float embeddings. Note that each float is repre-\nsented by using 32-bits so one could expect to obtain an autoencoder\nthat has a ratio of input dimension over embedding dimension of\n32 (ğ‘‘/ğ‘š=32) with no reconstruction error. However this is not\npossible due to the nature of floats (not every possible 32-bit se-\nquence maps to a numeric float value). Furthermore, the implicit\nsmoothness of neural networks and the learning algorithms limit\nwhich compression functions can actually be learnt. In fact, in this\nwork we explore networks with an embedding dimension of 50\n(ğ‘‘/ğ‘š=1000/50=20) leading to non-zero reconstruction errors.\nThe role of the sketch within an autoencoder can be framed within\nan information theory perspective. Traditional lossless compression\ntechniques such as Shannon coding, make use of the underlying\nprobability distribution of the data to assign fewer bits to the fre-\nquent subsequences, and more bits to the more rare subsequences.\nHowever, if all elements are equally likely to appear, or the un-\nderlying distribution is not knowable or completely random, thebest that can be done is to assign one bit for each boolean value,\nobtaining no compression gain. In a similar manner, the sketch\nacts as a hidden representation of the underlying distribution of\nthe data, which is used by the encoder and decoder to provide a\nmore compact encoded representation of new samples, or similarly,\nobtaining lower reconstruction errors.\nG.2 Experimental Details\nWe train three main network configurations: (a) a regular autoen-\ncoder (AE), (b) a mean conditional autoencoder (+M) and (c) a mean\nand sketch conditional autoencoder (+MS). Each of the three net-\nworks uses the same base architecture, where the only difference is\nthe additional weights included in the first layer of both the encoder\nand the decoder for the dynamic biases. For this application, we use\nfor both encoder and decoder a residual LayerNorm GELU-based\nfully connected network (see Figure 2) with 5 hidden layers and\neach with a hidden dimension of 4096. The sketch network includes\nthree hidden layers with a hidden dimension of 4096.\nAs a baseline, we train dataset-specific autoencoders with the\nRAdam [ 51] optimizer, a learning rate of 0.0001, and a batch size of\n1024. We compute the validation loss every 1000 weight updates and\nstop the training process if the loss has not decreased after fifteen\nevaluations. We train the same architecture (regular autoencoder)\nwith the randomized binary MNIST. For this dataset a learning\nrate of 0.00001 and a batch size of 4096 is used. The randomized\nbinarized MNIST is also used to train the mean-conditional and\nsketch+mean-conditional networks. First, the mean-conditional\nnetwork is trained until the validation loss does not decrease for\nmore than 50 evaluations. A learning rate of 0.00001 is used with a\nbatch size of 1152. The (conditional) mean is computed using 128\ndifferent samples and concatenated to each element of the batch.\nAfter the mean-conditional autoencoder is trained, the weights are\nused as initialization of the sketch+mean-conditional autoencoder.\nThe sketch+mean-conditional autoencoder is trained with the same\n18\n\nCompressive Meta-Learning\nlearning rate, a batch size of 1024, and the sketch is computed using\n64 samples. The number of samples used to compute the sketch is\nincreased to 128 after 50 evaluations without improvement of the\nvalidation loss.\nNote that during evaluation, the autoencoder +MSK setting is\nobtained by simply using the trained sketch+mean-conditional\nnetwork (+MS) and computing a specific sketch for each class.\nG.3 Analysis of the sketch\nWe explore the effect of the sketch resolution on the reconstruction\naccuracy. Specifically, we run the mean and sketch+mean condi-\ntional autoencoders with sketches generated with a variable number\nof samples from the training set. Figure 11 shows the mean and\nstandard deviation of the balanced accuracy when including from\none to all samples of the training set in the sketch. As can be ob-\nserved, the improvement grows logarithmically, obtaining marginal\nimprovements after more than 1,000 samples have been included.\nFigure 11: Balanced accuracy of sketch+mean and mean con-\nditional autoencoders with a variable number of samples\nused to compute the sketch.\nH DATA STREAMING APPLICATIONS\nH.1 Zipf dataset\nZipf distributed datasets are commonly used to evaluate and char-\nacterize the performance of frequency and membership estimation\nalgorithms [ 9,13,21]. The following form is frequently adopted for\nthe marginal distribution of a feature ğ‘—:\nğ‘ğ‘—=1\nğ‘—ğ›¼(51)\nHere the features are ordered from most to least frequent with\nğ‘ğ‘—>ğ‘ğ‘—+1(i.e. feature ğ‘—is more frequent than ğ‘—+1) andğ›¼is a pa-\nrameter that characterizes the level of skewness of the distribution.\nWe propose an extended definition that adds a scaling factor, ğ›½, for\nsimulations with small ğ›¼:\nğ‘ğ‘—=ğ›½max(1âˆ’ğ›¼,0)1\nğ‘—ğ›¼(52)\nwhereğ›½provides a scaling factor for samples with ğ›¼<1.\nThe first application consists of predicting the frequency (i.e. nor-\nmalized counts) for each feature within a set of samples. We adopt\na setting commonly found in data streaming where the number\nof elements in a given vocabulary needs to be computed and eachelement can be represented as a binary vector by using a one-hot\nencoding. Specifically, the query output we are trying to predict\nisğ‘¦=1\nğ‘Ãğ‘\nğ‘–=1ğ‘¥ğ‘–, whereğ‘¥ğ‘–âˆˆ{0,1}ğ‘‘. Furthermore, the vectors are\nassumed to come from a distribution potentially sparse and skewed.\nIn particular, we make use of data where the frequency of each\nBoolean feature is distributed following a Zipf-like distribution\nwhere feature ğ‘—has a probability ğ‘ğ‘—=ğ›½max(1âˆ’ğ›¼,0)ğ‘—âˆ’ğ›¼and feature\nğ‘—is equally or more frequent than feature ğ‘—+1.ğ›¼controls the\namount of skewness with ğ›¼=0leading to a uniform distribution\nandğ›¼>1leading to skewed distributions. ğ›½is the background\nprobability when ğ›¼=0and has no effect for ğ›¼>1. Note that we\ntreat each feature as independent, and ğ‘ğ‘—represents the marginal\nprobability for each feature; therefore, the sum of all ğ‘ğ‘—do not need\nto add to 1.\nWith a sketch size equal to the input dimension ( ğ‘š=ğ‘‘) this\nbecomes a trivial task, where applying an identity mapping (i.e.\nğœ™(ğ‘¥ğ‘–)=ğ‘¥ğ‘–) followed by a mean provides the exact frequency esti-\nmate. Here we focus on the non-trivial scenario where ğ‘š<ğ‘‘and\nconsiderğ‘š=100andğ‘š=10with data of ğ‘‘=1000 dimensions. We\ntrain the SQNet to minimize the binary cross entropy between the\npredicted set average Ë†ğ‘¦and the normalized frequency ğ‘¦. The query\nnetwork only takes as input the sketch ğ‘§and does not make use\nof any auxiliary inputs. The networks are trained with randomly\ngenerated batches of binary vectors ğ‘¥ğ‘–using the Zipf distribution.\nDuring training, the number of elements ğ‘›within the input set x,\nandğ›¼andğ›½are chosen randomly at each batch, with ğ‘›ranging\nfrom 1 to 100, and ğ›¼andğ›½between 0 and 2, and 0 and 1 respectively.\nFurthermore, the features are randomly permuted at each iteration.\nThis randomized training allows us to obtain a network that can\ngeneralize well to a wide variety of datasets with different levels of\nskewness.\nWe explore using different sketching networks: non-linear MLP\nnetworks with skip connections (Non-linear), a linear layer (Linear),\nand a linear layer limited to ternary values (0,1, and -1) (Ternary)\ntrained following the quantization approach used in [ 47], and\nsmaller SQNets applied in parallel, each taking only 10% of the\ninput dimension (Parallel). The query network is always a non-\nlinear residual MLP network. For both Sketch and Query Network\nwe explore multiple architectural variations: using different activa-\ntion functions, pooling layers, and hidden layer dimensions. The\nuse of non-linear networks is only useful in settings where the\ninput consist of multi-hot encoding. When the input is one-hot\nencoding, a linear layer is sufficient and is equivalent to learnt\nembeddings commonly used to encode words from a vocabulary in\nNLP applications. Sketching performed with ternary linear layers\ncan benefit from efficient low-latency implementations in a similar\nway as Count-Sketch or Count-Min do. Details of the experimental\nsetting are reported in Appendix H.5.\nIn order to assess the importance of learning a good sketching\nfunction, we replace Î¦(x)by a set of random feature projections\nfollowed by a mean pooling operation, as a baseline from CL, where\nonly the query network is learnt. We refer to this setting as RFQNet\n(Random Features + Query Network). Both SQNet and RFQNet are\ncompared with well established frequency estimation algorithms:\nCount-Min (CM) [ 21] and Count-Sketch (CS) [ 13]. We look for the\noptimal hyperparameters of CS and CM for a given ğ›¼andğ›½.\n19\n\nMas Montserrat et al.\nFigure 12: Mean square error (MSE) for frequency estimation with SQNet (Non Linear), SQNet (Linear), SQNet (Ternary), SQNet\n(Parallel), RFQNet, CM, and CS in a Zipf-distributed dataset for different ğ›¼values and ğ›½=1.0using a sketch size of 100. Right\nplot shows a zoomedâ€“in view of the MSE range [0,0.0040].\nFigure 12 shows the mean square error of a Zipf dataset, with\nğ‘š=100, for SQNet, RFQNet, and CS and CM when using opti-\nmal hyperparameters (Optim), and when using fixed sub-optimal\nhyperparameters (W=20, D=5). Additionally, we include the perfor-\nmance when predicting always zero (All 0s). SQNet outperforms all\ncompeting methods providing a significant increase in accuracy for\ndatasets with ğ›¼â‰¤1, and matching CM performance with almost\nzero error for extremely skewed datasets ( ğ›¼â‰¥1.5). Appendix H.5\nprovides additional experiments in real world tabular datasets.\nThe second application, membership estimation, consists of de-\ntecting if a feature ğ‘—is present in at least one sample from a set\nx={ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘›}, withğ‘¥ğ‘–âˆˆ{0,1}ğ‘‘. Specifically, the query that\nwe aim to predict is:\nğ‘¦ğ‘—=\u001a1ifÃğ‘\nğ‘–=1ğ‘¥ğ‘–ğ‘—>0\n0ifÃğ‘\nğ‘–=1ğ‘¥ğ‘–ğ‘—=0(53)\nwhereğ‘¥ğ‘–ğ‘—indicates the feature ğ‘—at sampleğ‘–, andğ‘¦ğ‘—is the mem-\nbership indicator of feature ğ‘—. This application can be framed as\na binarized version of frequency estimation. In fact, we apply the\nsame training and testing procedure, and compare SQNet with\nRFQNet and the established method Bloom Filters (BF) [ 9] with\nboth the Zipf-distributed dataset and the previously described tab-\nular datasets. The results follow the same trend as in frequency\nestimation, with SQNet surpassing competing methods. A more in-\ndepth discussion of the experiments with Zipf and tabular datasets\nas well as an analysis of the hyperparameters of SQNet, RFQNet,\nand BF can be found in Appendix H.5.\nH.2 Frequency and membership estimation\nFrequency and membership estimation are explored in section H\n(Figure 13). We frame frequency estimation as a task of estimating\nthe normalized frequency for each feature given a set of samples\nx={ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘}, withğ‘¥ğ‘–âˆˆ{0,1}ğ‘‘:\nğ‘¦=1\nğ‘ğ‘âˆ‘ï¸\nğ‘–=1ğ‘¥ğ‘– (54)Hereğ‘¦is ağ‘‘-dimensional vector wherein each dimension ğ‘¦ğ‘—\nrepresents the normalized frequency for feature ğ‘—. Note that pre-\nvious works frame frequency estimation without the normalizing\nfactorğ‘[13,21]. Similarly, membership estimation can be framed\nas predicting a binary indicator per feature describing whether a\nfeatureğ‘¥ğ‘—is present or not in a set:\nğ‘¦ğ‘—=\u001a1ifÃğ‘\nğ‘–=1ğ‘¥ğ‘–ğ‘—>0\n0ifÃğ‘\nğ‘–=1ğ‘¥ğ‘–ğ‘—=0(55)\nHereğ‘¦is ağ‘‘-dimensional vector where each dimension ğ‘¦ğ‘—repre-\nsents the binary membership indicator for feature ğ‘—. Note that one\ncan binarize the frequency vector to obtain the membership indica-\ntor vector. For frequency estimation, the value of ğ‘will dictate the\nresolution of the frequency estimates. For membership estimation,\nthe value of ğ‘needs to be properly taken into account as for ğ‘ğ‘–>0\na large enough ğ‘will lead to membership indicators where all\nvalues are 1. Therefore, a proper tuning of membership estimation\nmethodâ€™s parameters need to include the number of elements in\nthe set.\nH.3 Count-Min, Count-Sketch, and Bloom Filter\nas neural networks\nCount-Min, Count-Sketch, and Bloom Filter can be framed as Sketch-\nQuery Networks with linear projections with fix weights as sketch-\ning functions and a fixed query function. For example, a Count-\nSketch with sketch size of ğ‘šandğ‘ğ‘¤=ğ‘šandğ‘ğ‘‘=1, withğ‘ğ‘¤\nrepresenting the output dimensionality of the linear projection, and\nğ‘ğ‘‘the number of linear projections, can be represented as follows:\nğ‘§ğ‘—=ğ‘Šğ‘¥ğ‘— (56)\nwithğ‘§ğ‘—representing the sketch projection of sample ğ‘¥ğ‘—, andğ‘Šis\nthe linear projection matrix with dimensionality ğ‘‘Ã—ğ‘šand ternary\nvaluesğ‘Šğ‘™,ğ‘˜âˆˆ{âˆ’ 1,0,1}. The dataset-level sketch can be obtained\nby performing a mean (or sum) pooling operation:\n20\n\nCompressive Meta-Learning\nFigure 13: Frequency estimation and membership estimation with SQNet.\nğ‘§=1\nğ‘ğ‘âˆ‘ï¸\nğ‘—=1ğ‘§ğ‘— (57)\nThe estimated frequency vector can be obtained by applying the\ntransposed linear projection:\nË†ğ‘¦=ğ‘Šğ‘‡ğ‘§ (58)\nCount-Min differs from Count-Sketch by limiting ğ‘Šğ‘™,ğ‘˜âˆˆ{0,1}\nand using the maximum as pooling layer. Bloom Filter adds the\nadditional constraint of limiting the elements of ğ‘§to be booleans\nğ‘§ğ‘—âˆˆ{0,1}ğ‘š. Ifğ‘ğ‘‘>1, then a total of ğ‘ğ‘‘projections and trans-\nposed projections are performed in parallel obtaining a total of ğ‘ğ‘‘\nestimated frequencies. The multiple sets of estimated frequencies\nare combined through a mean operation for Count-Sketch and with\na min operation for Count-Min and Bloom Filters.\nH.4 Sketching functions and scalability\nWe explore different neural network architectures for the Sketching\nNetwork. Specifically, we explore non-linear networks, particularly\nresidual MLPs, a linear mapping, and a ternary linear layer. Fur-\nthermore, we investigate the effectiveness of employing multiple\nsmaller networks in parallel.\nH.4.1 Linear and non-linear sketching networks. Depending on the\napplication, two different scenarios can be found: (a) the input\nvectorğ‘¥ğ‘—can contain a single one-hot encoded value (i.e. ||ğ‘¥ğ‘—||=\n1) or (b) it can contain multi-hot elements (i.e. ||ğ‘¥ğ‘—||â‰¥ 1). Non-\nlinear networks can provide improved accuracy in setting (b) as\ncorrelations between features can be exploited, however, in setting\n(a) both linear and non-linear projections can learn the same set\nof functions, providing no benefit using non-linear mappings over\nlinear ones.\nReal-world frequency estimation applications can require deal-\ning with extremely large vocabularies (e.g. the count of millions of\nelements need to be calculated). This will cause the first layer of thesketching network to have a gigantic size. For example, if a simple\nlinear layer is used as sketching network, and input dimension\n(vocabulary size) is ğ‘‘=106, with a sketch size of ğ‘š=103, the\nsketching layer will have 109parameters. However, note that in\nboth training and inference, there is no need to load all parame-\nters into memory as the sketching operation ğ‘Šğ‘¥ğ‘—can be simply\nseen as a look-up table, where the index ğ‘˜of the one-hot encoded\nelement (ğ‘¥ğ‘—ğ‘˜=1) indicates which row of the projection is loaded\n(ğ‘¤ğ‘˜=ğ‘Šğ‘¥ğ‘—). In the scenario of multi-hot encoded inputs, the look-\nup operation needs to be performed as many times as non-zero\nelements are present in the input, followed by a simple addition of\ntheğ‘š-dimensional vectors. A similar approach can be applied in\nthe first layer of non-linear sketching networks or other multi-layer\nsketching networks. Such mapping is similar to the common learnt\nembeddings used in natural language processing applications with\nneural networks.\nH.4.2 Parallel sketching for large vocabularies. Linear and non-\nlinear sketching networks can be trained and tested without the\nneed of loading all the networkâ€™s weights providing some scalability\nto high dimensional inputs (large vocabularies). However, a large\nnumber of parameters still needs to be trained and stored. To further\nincrease the scalability to large vocabularies, we propose training\na SQNet with a fix input and output dimensionality and apply in\nparallel multiple instances of the network when larger dimensional\ninputs are found. Specifically, input dimensions are divided into\nnon-overlapping windows, and the dimensions of each window are\nprocessed by a small SQNet.\nH.4.3 Ternary linear sketching for low-latency applications. Data\nstreaming applications using frequency estimation might require to\nhave a very low latency. While neural network GPU-based imple-\nmentations might not be able to match highly optimized low-latency\nfrequency and membership estimation techniques, as explored in\n[45], Sketch-Query Network can be used to learn a sketching func-\ntion that can be later deployed into a low-latency pipeline based\n21\n\nMas Montserrat et al.\nFigure 14: Mean square error (MSE) for membership estimation with SQNet, RFQNet, and BF, in a Zipf-distributed dataset for\ndifferentğ›¼values and ğ›½=0.5. Right plot shows a zoomedâ€“in view of the MSE range [0,0.3]\non Count-Sketch or Count-Min. In order to do so, we can limit the\nsketching function to a linear mapping with ternary values so each\nentry of the linear projection ğ‘Šğ‘™,ğ‘˜âˆˆ{âˆ’ 1,0,1}to obtain a mapping\nsimilar to a Count-Sketch, or to binary values with ğ‘Šğ‘™,ğ‘˜âˆˆ{0,1}to\nobtain a mapping similar to a Count-Min or Bloom Filter.\nIn many applications, the sketching function is performed with\nmore frequency than the query function. For example, in data\nstreaming scenario, every time a new sample is present, the sketch\nneeds to be updated. However, the query function is only computed\nevery time a user or a downstream application needs to extract\ninformation from the sketch. We can benefit of this asymmetry to\nuse a fast Count-Sketch-like Sketching Network, while keeping a\nnon-linear MLP as a query function to have both low-latency and\nan accurate reconstruction.\nH.5 Experimental Results\nH.5.1 Membership estimation Zipf results. Figure 14 shows the\nmean square error of a Zipf dataset with samples with ğ‘‘=1000 di-\nmensions and sketching methods of a sketch size ğ‘š=100, showing\nthat SQNet surpasses BF for mildly skewed datasets, and matches\nits performance with almost 0 error for highly skewed datasets.\nNote that a Bloom Filter can perform poorly if not properly tuned\n(Bloom filter with W=20,D=5), obtaining higher MSE than trivially\noutputting always 0s (All 0s). SQNet consistently surpasses RFQNet,\nshowing the benefits of learning the sketching function instead of\nusing a randomized projection.\nH.5.2 Frequency and membership results with tabular datasets. Ta-\nble 5 (top) shows the MSE in the datasets used for the autoencoder\nexperiments. The table includes SQNets and RFQNets trained with\nZipf dataset, as well as the networks after fine-tuning with the train-\ning sets of each respective database, showing that while networks\ntrained with Zipf datasets already surpass traditional techniques,\nnear zero error can be obtained if the networks are trained to match\nthe underlying distribution of the dataset. Similarly, Table 5 (bottom)\nshows the results in the tabular binary datasets for membership\nestimation, comparing it with SQNet and RFQNet fine-tuned witheach of the training sets. A similar trend as in frequency estimation\nis observed, showing that the error decreases significantly when\nthe networks are fine-tuned obtaining close to 0 error.\nH.5.3 Performance evaluation for frequency estimation. We evalu-\nate the computational time between different SQNet configurations\nand CS, CM, and RFQNet. Every method is trained and evaluated\nin a V100 GPU. Table 6 shows the runtimes for training and testing\nof each of the methods. CS and CM do not require any training\nand provide the fastest inference time, while providing a higher\nreconstruction error. RFQNet and SQNet using Linear and Ternary\nlayers as Sketching Networks, provide faster inference times than\nusing a non-linear SQNet.\nNote that real-world applications of frequency and membership\nestimation methods might require very low latencies that might\nbe challenging to obtain by using neural networks implemented in\nGPU (See discussion in [45]).\nH.5.4 Hyperparameter search for Count-Min and Count-Sketch. We\nexplore the optimal hyperparameters for Count-Min and Count-\nSketch. Both methods have two main parameters: ğ‘ğ‘¤, which is the\noutput dimension of the random linear projections, and ğ‘ğ‘‘, which\nis the number of projections. Both methods generate sketches of\nsizeğ‘š=ğ‘ğ‘¤ğ‘ğ‘‘, therefore, for a fair comparison, we fix the sketch\nsize for all methods (e.g. ğ‘š=100) and look for the combination of\nparametersğ‘ğ‘¤andğ‘ğ‘‘(e.g. such as ğ‘ğ‘¤ğ‘ğ‘‘=100) that provide the\nlowest mean square error.\nTo perform the hyperparameter search for CM and CS, we gen-\nerate Zipf datasets with multiple different combinations of ğ›¼and\nğ›½. We explore ğ›¼ranging from 0 to 2 and ğ›½ranging from 0 to 1.\nFor each value of ğ›¼andğ›½, we try all possible values of ğ‘ğ‘‘andğ‘ğ‘¤\n(making sure that ğ‘ğ‘¤ğ‘ğ‘‘=ğ‘š). We evaluate each setting 5 times and\ncompute the average mean square error and its standard deviation\nwith a dataset that for each ğ›¼andğ›½includes 500 batches with each\nincluding a set of 100 samples with 1000 dimensions each. (The\ninput of the sketching method has dimensionality 500Ã—100Ã—1000,\n22\n\nCompressive Meta-Learning\nTable 5: Mean square error for frequency estimation (top) and membership estimation (bottom) tasks. (f) indicates that the\nnetwork has been fine-tuned with each respective dataset.\nModel M FM H Dogs KDD Heart QD Ad B Avg\nCS 6eâˆ’2 1eâˆ’1 1eâˆ’1 2eâˆ’1 2eâˆ’1 1eâˆ’2 4eâˆ’2 6eâˆ’3 8eâˆ’389eâˆ’3\nCM 6eâˆ’1 6eâˆ’1 6eâˆ’1 5eâˆ’1 6eâˆ’1 5eâˆ’2 6eâˆ’1 3eâˆ’2 9eâˆ’240eâˆ’2\nRFQ 2eâˆ’2 7eâˆ’2 4eâˆ’2 8eâˆ’2 7eâˆ’2 1eâˆ’2 7eâˆ’3 4eâˆ’3 7eâˆ’336eâˆ’3\nSQ 2eâˆ’2 5eâˆ’2 4eâˆ’2 7eâˆ’2 7eâˆ’2 6eâˆ’3 7eâˆ’3 3eâˆ’3 6eâˆ’332eâˆ’3\nRFQ (f) 5eâˆ’4 9eâˆ’4 6eâˆ’3 4eâˆ’2 2eâˆ’3 8eâˆ’5 7eâˆ’4 2eâˆ’4 3eâˆ’454eâˆ’4\nSQ (f) 3eâˆ’4 4eâˆ’4 2eâˆ’3 8eâˆ’3 9eâˆ’4 3eâˆ’5 5eâˆ’4 5eâˆ’5 1eâˆ’414eâˆ’4\nBF 6eâˆ’1 4eâˆ’1 6eâˆ’1 5eâˆ’2 2eâˆ’1 4eâˆ’1 5eâˆ’1 4eâˆ’1 6eâˆ’142eâˆ’2\nRFQ 2eâˆ’1 4eâˆ’1 3eâˆ’1 4eâˆ’1 4eâˆ’1 1eâˆ’1 3eâˆ’1 1eâˆ’1 1eâˆ’126eâˆ’2\nSQ 2eâˆ’1 3eâˆ’1 2eâˆ’1 5eâˆ’2 2eâˆ’1 4eâˆ’2 2eâˆ’1 4eâˆ’2 7eâˆ’215eâˆ’2\nRFQ (f) 7eâˆ’2 6eâˆ’2 1eâˆ’1 1eâˆ’1 2eâˆ’1 2eâˆ’2 1eâˆ’1 2eâˆ’2 4eâˆ’286eâˆ’3\nSQ (f) 1eâˆ’2 2eâˆ’2 1eâˆ’1 1eâˆ’2 2eâˆ’1 5eâˆ’3 1eâˆ’1 6eâˆ’3 2eâˆ’264eâˆ’3\nTable 6: Train and test time of each method in seconds.\nModel CM CS SQ(Non Linear) SQ(Linear) SQ(Ternary) SQ(Parallel) RFQ\nTrain - - 1e3 4e2 2e3 2e3 5e2\nTest 5eâˆ’4 6eâˆ’4 1eâˆ’1 3eâˆ’3 3eâˆ’3 6eâˆ’2 3eâˆ’3\nthe input of the decoding method (i.e. sketches) has dimensional-\nity500Ã—ğ‘š, and the predicted query 500Ã—1000 .) We search the\nhyperparameters with a sketch size of ğ‘š=100andğ‘š=10.\nFigure 15 shows the mean and standard deviation of the Mean\nSquare Error (MSE) for Count-Min (CM) and Count-Sketch (CS)\nusing different values of ğ‘ğ‘‘andğ‘ğ‘¤. Each plot corresponds to\na particular ğ›¼andğ›½, and both x and y axes are in a log scale.\nNote that the optimal configurations of CS outperform the optimal\nconfigurations of CM, except in extremely skewed settings ( ğ›¼=2).\nFor low values of ğ›¼, the collision between elements within the\nsketch is large, leading to high errors in CM, regardless of the\nparameters selected. For skewed datasets ( ğ›¼>1) the selection\nof good parameters becomes more critical. For example, if poor\nparameters are selected (e.g. ğ‘ğ‘¤=1,ğ‘ğ‘‘=100) both CM and\nCS perform more than four orders of magnitude worse than if\nsuitable parameters are selected (e.g. ğ‘ğ‘¤=25,ğ‘ğ‘‘=4). Note that\nsmall values of ğ‘ğ‘¤will lead to high number of collisions (i.e. many\ninput elements getting projected into the same sketch dimension),\nand increasing ğ‘ğ‘‘wonâ€™t help much (e.g. ğ‘ğ‘¤=1,ğ‘ğ‘‘=100always\nperforms poorly). However, if ğ‘ğ‘¤has a medium or large dimension,\ncollision numbers are decreased and increasing ğ‘ğ‘‘(increasing the\nnumber of projections) provides a significant improvement.\nFigure 16 shows the optimal value of ğ‘ğ‘¤for both CS, CM and\nBloom Filter (discussed in the next section) for different values\nofğ›¼andğ›½. Becauseğ‘ğ‘¤ğ‘ğ‘‘=100, the parameter of ğ‘ğ‘‘can be\neasily inferred with ğ‘ğ‘‘=100/ğ‘ğ‘¤. Note that for small ğ›¼, CM works\noptimally with ğ‘ğ‘¤=100,ğ‘ğ‘‘=1, as low dimensional projections\nlead to a high number collisions and bad predictions. When ğ›¼>1,\nboth CM and CS perform better with a smaller size of ğ‘ğ‘¤and larger\nğ‘ğ‘‘. Figure 17 provides the MSE of the optimal configurations ofboth CM and CS for different values of ğ›¼andğ›½. Following the same\ntrend as in Figures 12, 15, 16, we can observe that CM performs\npoorly forğ›¼<1, while CS provides more robustness to non-skewed\ndistributions, with an error decreasing for more sparse data (lower\nerror for smaller values of ğ›½).\nH.5.5 Hyperparameter search for Bloom Filter. We apply a similar\nhyperparameter search for Bloom Filters (BF) as performed for CM\nand CS. Zipf-datasets with different values of ğ›¼andğ›½are used to\nevaluate bloom filters. Here we perform ten runs where for each\nğ›¼ ğ›½pair a total of 500 batches with each including a set with ten\n1000-dimensional binary inputs are used to evaluate bloom filter.\nFigure 18 shows the mean and standard deviation of the Mean\nSquare Error (MSE) of BF using different values of ğ‘ğ‘‘andğ‘ğ‘¤.\nEach plot corresponds to a particular ğ›¼andğ›½, and we can see how\nthe MSE varies between each run. A similar behavior as CM is\nobserved in BF, with the method performing poorly for small ğ›¼and\nobtaining a good performance for large ğ›¼, if hyperparameters are\nselected properly.\nFigure 16 (orange line), the optimal value of ğ‘¤for BF is shown\nfor different values of ğ›¼andğ›½showing a similar behaviour with\nCM. The similarity with CM can be further observed in Figure 17\n(right), which provides the MSE of BF for different ğ›¼andğ›½. The\noptimal values of ğ‘ğ‘‘andğ‘ğ‘¤for eachğ›¼andğ›½are used to compute\nthe errors of BF shown in Figure 14.\nH.5.6 Hyperparameter search for SQNet and RFQNet for frequency\nestimation. Both SQNet and RFQNet are trained using random\nbatches following the Zipf distribution. We train the networks\nwith a batch size of 500, where each batch includes a set of multiple\n1000-dimensional samples. The size of the set is randomly selected\n23\n\nMas Montserrat et al.\nFigure 15: MSE mean and standard deviation with ten execu-\ntions of CM and CS using different d and w, with a sketch size\nof 100. Using Zipf-distributed dataset for different ğ›¼values\nandğ›½=1.0.\nFigure 16: Mean and standard deviation of the optimal ğ‘¤\nfrom ten runs using Count-Min, Count-Sketch and Bloom\nFilters (sketch size of 100) for the Zipf-distributed dataset\nwith different ğ›¼andğ›½values.\nat every iteration with values between one and twenty elements.\nFurthermore, the ğ›¼andğ›½values are randomly chosen with ğ›¼taking\nvalues from zero to two, and ğ›½from zero to one.\nNote that in SQNet, both the Sketch and the Query network are\ntrained, while in RFQNet, only the Query Network is trained and\nthe Sketch Network is replaced by Random Features. For SQNet we\nexplore two different architectures for the Sketch Network, which\nare described in Figure 2. For frequency estimation task, the archi-\ntecture that provided better results in most settings is the â€œResNetâ€\nstyle architecture (Network (a) in Figure 2). We explore different\npooling layers, learning rates, hidden sizes, number of hidden layers\nand pooling layer parameters such as ğ‘values between one and ten\nFigure 17: MSE mean and standard deviation of 10 executions\nof Count Min Sketch, Count Sketch, and Bloom Filters with\nthe optimum d and w, with a sketch size of one hundred.\nUsing Zipf-distributed dataset for different ğ›¼andğ›½.\nFigure 18: MSE mean and standard deviation with 10 execu-\ntions of Bloom Filters using different d and w, with a sketch\nsize of 100. Using Zipf-distributed dataset for different ğ›¼val-\nues andğ›½=0.5.\nfor the p-norm pooling layers. Figure 19 shows the MSE obtained\nwith networks with different pooling layers. For each pooling layer,\nwe report the results from the best performing hyperparameters\nin a validation Zipf dataset generated in a similar way as the train-\ning set. The Mean pooling layer (blue line) outperforms all other\npooling operations, with log-sum-exp and p-norm based pooling\nlayers performing poorly. Because the pair of Sketch and Query\nNetwork is approximating an average function (i.e. averaging a set\nto obtain its normalized frequency), it is expected that the mean\npooling layer performs well.\nFor RFQNet, we fix the Mean as a pooling layer (as commonly\ndone in CL appliations) and we explore different type of random\nfeatures (with different activations) and variances. The range of\nvalues of the variances goes from 0.005 to 1 and the activations\nare the ReLU, GELU, LeakyReLU, Fourier Features (Cosine and\nSine), Cosine, Sine, Tanh, and Sigmoid. Note that each activation\nwill lead to random features that approximate a different kernel.\nA comparison between the activations using the best performing\nvariances can be seen in Figure 20. The best performing random\n24\n\nCompressive Meta-Learning\nfeatures are the GELU features, with Cosine and Sigmoid features\nperforming the worst.\nFigure 19: Mean square error (MSE) for frequency estimation\nwith SQNet in a Zipf-distributed dataset for different ğ›¼values\nand pooling layers with ğ›½=1.0. Right plot shows a zoomedâ€“in\nview of the MSE range [0,0.01]\nFigure 20: Mean square error (MSE) for frequency estimation\nwith RFQNet in a Zipf-distributed dataset for different ğ›¼\nvalues and activations layers with ğ›½=1.0. Right plot shows a\nzoomedâ€“in view of the alpha range [1.0,2.0]\nFigure 21: Mean square error (MSE) for membership estima-\ntion with SQNet in a Zipf-distributed dataset for different ğ›¼\nvalues and pooling layers with ğ›½=0.5. Right plot shows a\nzoomedâ€“in view of the MSE range [0,0.01]\nH.5.7 Hyperparameter search for SQNet and RFQNet for member-\nship estimation. For membership estimation, we adopt the same\ntraining procedure as in frequency estimation with minor changes.\nFor simplicity, we fix during training and testing both the num-\nber of samples within each set to 10, and the ğ›½value to 0.5. As in\nfrequency estimation, the batch size is 500 and the value of ğ›¼is ran-\ndomly selected between zero and two, with some batches randomly\nlimiting its range from 0.75 to 1.5 as those were the regions where\nthe networks were under-performing.\nFigure 22: Mean square error (MSE) for membership estima-\ntion with RFQNet in a Zipf-distributed dataset for different\nğ›¼values and activations layers with ğ›½=0.5. Right plot shows\na zoomedâ€“in view of the alpha range [1.0,2.0]\nAs in frequency estimation, we explore two different architec-\ntures, and different pooling layers for the Sketch Network. In this\nframework, some pooling layers work better with the architecture\n(b) described in Figure 2, in particular the Mean, the Minimum, the\nMaximum, and the Log-Sum-Exp. The other pooling layers work\nbetter with the architecture (a). In addition, we explore different\nlearning rates, number of hidden layers, and hidden sizes. In Fig-\nure 21 there is a comparison between the different pooling layers\nusing the best combination of hyperparameters with a ğ›½=0.5. The\nbest pooling layer is the Minimum with the architecture (b), obtain-\ning a performance very similar to the maximum operation. Note\nthat in this application the mean pooling layer is one of the pooling\nlayers providing the highest MSE. In the RFQnet, we explore several\nactivation layers and variances. The activations are the same as in\nthe frequency estimation. Figure 22 shows a comparison between\nactivation layers using the best hyperparameters and with a ğ›½=0.5.\nThe best activation layer is GELU with a variance of 0.6.\n25\n\nMas Montserrat et al.\nTable 7: Meta-training datasets for PCA experiments. Subscripts ğ‘–..ğ‘—and(Â·)denote the interval of indices and the total number\nof datasets of the same family used, respectively. Train and test subsets are combined into a single set.\nDataset name # Classes # Features # Samples\nFashion-MNIST 10 784 70000\nEMNIST-digits 10 784 280000\nletters 1 10 784 56000\nquickdraw 10 784 100000\nhapmap 1..30(30) 10 784 2214\ndogstrain 1..14(14) 10 784 1830\nBioresponse 2 1776 3751\nsplice 3 60 3190\nqsar-biodeg 2 41 1055\nMiceProtein 8 77 1080\nkr-vs-kp 2 36 3196\nchurn 2 20 5000\nCIFAR-10 10 3072 60000\nkc1 2 21 2109\njungle-chess 3 6 44819\nadult 2 14 48842\nnomao 2 118 33465\nfirst-order-theorem-proving 6 51 6118\nDevnagari-Script 46 1024 92000\nmfeat-morphological 10 6 2000\npc3 2 37 1563\nvehicle 4 18 846\nvowel 11 12 990\nletter 26 16 20000\nozone-level-8hr 2 72 2534\nspambase 2 57 4601\nPhishingWebsites 2 30 11055\nclimate-model-simulation-crashes 2 18 540\nbanknote-authentication 2 4 1372\ncredit-approval 2 15 690\nbank-marketing 2 16 45211\ncmc 3 9 1473\nphoneme 2 5 5404\nwdbc 2 30 569\neucalyptus 5 19 736\ndresses-sales 2 12 500\nbreast-w 2 9 699\n26\n\nCompressive Meta-Learning\nTable 8: Meta-testing datasets for PCA experiments. Subscripts ğ‘–..ğ‘—and(Â·)denote the interval of indices and the total number\nof datasets of the same family used, respectively. Train and test subsets are combined into a single set.\nDataset name # Classes # Features # Samples\nMNIST 10 784 70000\nletters 2 10 784 56000\nKMNIST 10 784 70000\nethiopic 10 784 70000\nosmanya 10 784 70000\nvai 10 784 70000\nhapmap 31..50(20) 10 784 2214\ndogstrain 15..30(16) 10 784 1830\nsick 2 29 3722\npendigits 10 16 10992\nisolet 26 617 7797\nconnect-4 3 42 67557\nanalcatdata-authorship 4 70 841\nanalcatdata-dmft 6 4 797\ncylinder-bands 2 37 540\noptdigits 10 64 5620\nwall-robot-navigation 4 24 5456\nmfeat-karhunen 10 64 2000\nmfeat-factors 10 216 2000\ntexture 11 40 5500\npc4 2 37 1458\nsatimage 6 36 6430\nInternet-Advertisements 2 1558 3279\nhar 6 561 10299\ndiabetes 2 8 768\nsegment 7 16 2310\ncar 4 6 1728\ncredit-g 2 20 1000\nwilt 2 5 4839\njm1 2 21 10885\nkc2 2 21 522\nnumerai28.6 2 21 96320\ndna 3 180 3186\nmfeat-zernike 10 47 2000\nmadelon 2 500 2600\nblood-transfusion-service-center 2 4 748\nbalance-scale 3 4 625\nelectricity 2 8 45312\npc1 2 21 1109\ntic-tac-toe 2 9 958\nsemeion 10 256 1593\nGesturePhaseSegmentationProcessed 5 32 9873\ncnae-9 9 856 1080\nilpd 2 10 583\nmfeat-fourier 10 76 2000\nsteel-plates-fault 7 27 1941\nmfeat-pixel 10 240 2000\n27\n\nMas Montserrat et al.\nTable 9: Reconstruction error for each test dataset with Sketch-Query Network for different sketch sizes (Part 1).\nDataset 2 10 20 100 200 500 1000 2000 15000 19306\nletters 2 81.82 82.001 81.909 81.961 81.795 80.15 77.018 74.933 63.56 56.613\nKMNIST 92.953 92.825 92.949 92.613 92.733 90.395 89.25 86.761 76.709 71.377\nethiopic 62.125 62.56 61.972 63.56 63.782 61.46 60.524 60.097 53.135 50.936\nosmanya 67.142 67.103 67.847 66.765 68.32 65.129 64.406 61.703 53.139 49.39\nvai 71.465 70.484 71.418 70.788 70.784 69.37 67.196 65.798 56.91 52.911\nhapmap31 84.963 83.258 85.661 84.84 82.812 79.871 77.452 74.353 69.641 58.107\nhapmap32 85.004 85.259 85.197 85.0 81.605 78.759 77.963 75.484 70.316 59.166\nhapmap33 84.74 83.424 84.928 82.306 81.915 78.653 76.967 74.871 70.09 59.459\nhapmap34 86.086 85.297 85.727 84.778 81.754 79.68 77.322 75.226 69.992 59.186\nhapmap35 86.59 84.957 85.67 85.213 82.184 79.245 77.785 75.702 69.984 59.432\nhapmap36 84.609 84.302 83.613 84.08 81.782 78.17 77.158 74.706 69.643 58.568\nhapmap37 86.018 84.975 84.942 85.341 81.693 78.686 76.796 74.838 70.378 58.681\nhapmap38 86.17 85.325 84.894 82.77 82.162 79.058 77.356 74.471 70.082 58.853\nhapmap39 85.943 84.46 85.651 84.372 82.407 79.161 76.158 75.443 70.575 58.958\nhapmap40 84.845 85.808 84.388 84.578 81.68 79.731 76.946 75.228 70.439 59.596\nhapmap41 86.194 85.408 83.298 83.226 82.424 78.151 76.693 75.694 70.214 58.638\nhapmap42 85.682 85.173 86.591 83.314 82.046 78.07 76.888 74.651 69.565 59.157\nhapmap43 86.191 85.045 85.127 84.689 82.33 79.121 77.762 74.811 70.576 58.732\nhapmap44 86.131 85.377 84.789 84.254 82.74 79.496 76.761 75.59 69.861 59.417\nhapmap45 86.38 85.804 84.862 84.708 81.305 79.433 77.495 75.16 69.843 58.77\nhapmap46 86.567 83.789 85.116 83.282 81.714 78.56 77.322 74.636 69.884 58.925\nhapmap47 85.907 84.696 84.836 84.062 82.874 78.945 77.503 74.984 70.566 58.801\nhapmap48 84.752 85.474 85.039 83.599 80.604 78.153 77.142 74.513 70.331 59.715\nhapmap49 84.694 84.939 85.018 83.755 82.746 78.882 76.61 75.421 69.215 59.373\nhapmap50 85.774 85.055 83.829 84.041 80.899 77.809 76.962 74.691 68.392 58.266\ndogstrain15 87.953 88.264 87.811 87.254 86.708 84.983 83.098 80.147 67.796 59.919\ndogstrain16 88.056 88.061 87.644 87.738 86.365 84.706 83.096 79.901 67.901 59.756\ndogstrain17 88.48 88.9 88.247 87.494 87.147 84.701 83.427 79.571 68.028 59.568\ndogstrain18 88.653 88.588 88.278 87.819 86.572 84.673 83.073 79.752 68.039 59.735\ndogstrain19 88.569 88.124 88.724 87.58 87.05 84.718 83.311 80.65 68.061 59.615\ndogstrain20 88.237 88.383 87.692 88.149 87.113 84.813 83.466 79.933 67.729 59.69\ndogstrain21 88.133 88.459 88.019 87.474 86.965 84.993 82.947 80.417 68.05 60.079\ndogstrain22 88.554 88.26 87.981 87.973 86.865 84.507 82.507 80.046 67.981 59.224\ndogstrain23 88.43 87.845 88.838 87.374 86.856 84.887 83.282 80.361 68.153 59.304\ndogstrain24 88.803 88.726 88.265 87.528 87.088 84.966 82.933 79.298 67.631 60.133\ndogstrain25 88.367 88.669 88.25 87.548 86.394 84.721 83.194 79.869 68.095 59.29\ndogstrain26 88.423 88.402 88.23 87.682 87.172 84.995 83.463 80.469 67.929 59.641\ndogstrain27 88.599 88.944 88.236 87.5 86.674 84.405 83.079 80.245 67.856 59.742\ndogstrain28 88.498 87.82 88.147 87.658 86.246 83.883 82.9 79.935 67.829 59.991\ndogstrain29 89.103 88.436 88.993 87.833 86.869 84.866 83.547 80.058 67.942 60.336\ndogstrain30 88.077 88.308 87.959 87.686 87.178 84.98 84.385 80.064 68.189 59.247\n28\n\nCompressive Meta-Learning\nTable 10: Reconstruction error for each test dataset with Sketch-Query Network for different sketch sizes (Part 2).\nDataset 2 10 20 100 200 500 1000 2000 15000 19306\nsick 1.779 1.713 1.797 1.705 1.667 1.53 1.459 1.408 1.2 1.067\npendigits 0.614 0.63 0.693 0.639 0.609 0.564 0.601 0.495 0.244 0.187\nisolet 95.361 92.762 92.047 91.137 86.432 78.733 73.121 67.621 48.206 28.783\nconnect-4 4.309 3.642 3.525 3.714 3.616 3.398 3.456 3.267 2.715 2.5\nanalcatdata-authorship9 12.215 12.173 12.109 11.084 10.992 10.204 9.788 8.919 7.011 6.009\nanalcatdata-dmft 0.024 0.022 0.027 0.018 0.017 0.017 0.018 0.018 0.016 0.016\ncylinder-bands 2.739 2.802 2.851 2.589 2.457 2.667 2.557 2.323 1.665 1.506\noptdigits 9.668 8.98 8.452 8.114 8.206 7.828 7.407 6.563 4.419 3.705\nwall-robot-navigation 1.449 1.268 1.156 1.387 1.397 1.318 1.242 1.127 0.854 0.751\nmfeat-karhunen 10.049 9.88 10.102 9.8 9.547 8.884 8.011 7.37 5.745 5.206\nmfeat-factors 94.695 93.217 89.874 84.274 76.066 63.446 55.125 43.893 24.106 6.802\ntexture 1.151 0.943 0.971 0.993 0.945 0.886 0.826 0.79 0.536 0.122\npc4 3.402 3.854 2.982 2.449 2.418 1.042 1.077 1.025 0.861 0.613\nsatimage 4.716 1.747 1.947 1.707 1.676 1.609 1.531 1.293 0.47 0.187\nInternet-Advertisements 96.708 95.859 96.139 94.367 93.909 89.923 86.81 81.314 57.009 43.163\nhar 69.804 69.891 69.316 61.779 53.622 51.775 46.926 45.962 38.472 23.083\ndiabetes 0.115 0.109 0.105 0.093 0.1 0.106 0.092 0.085 0.083 0.078\nsegment 0.619 0.625 0.587 0.496 0.481 0.517 0.466 0.403 0.199 0.124\ncar 0.064 0.064 0.064 0.064 0.064 0.064 0.064 0.064 0.064 0.064\ncredit-g 0.918 0.918 0.858 0.819 0.821 0.824 0.83 0.797 0.705 0.677\nwilt 0.044 0.032 0.054 0.033 0.039 0.028 0.025 0.024 0.023 0.021\njm1 0.465 0.48 0.437 1.437 1.26 1.229 0.217 0.208 0.181 0.143\nkc2 0.32 1.92 0.613 0.507 0.543 0.448 0.077 0.069 0.066 0.047\nnumerai28.6 1.0 0.93 0.992 0.906 0.859 0.829 0.803 0.656 0.423 0.322\ndna 82.035 76.077 75.716 75.176 75.053 73.455 72.484 70.341 58.116 51.701\nmfeat-zernike 5.02 4.61 4.549 4.135 4.082 3.972 3.729 3.201 1.487 0.928\nmadelon 97.05 96.761 96.771 96.34 96.281 94.609 93.526 91.604 84.172 78.698\nblood-transfusion-service-center 0.022 0.02 0.017 0.024 0.023 0.015 0.008 0.015 0.008 0.006\nbalance-scale 0.023 0.023 0.023 0.023 0.023 0.023 0.023 0.023 0.023 0.023\nelectricity 0.128 0.137 0.134 0.136 0.128 0.094 0.089 0.083 0.077 0.071\npc1 0.663 1.947 0.424 1.68 1.521 0.733 0.181 0.165 0.153 0.118\ntic-tac-toe 0.174 0.168 0.156 0.156 0.153 0.148 0.147 0.142 0.139 0.137\nsemeion 94.128 93.318 95.104 94.095 93.3 82.355 79.416 70.328 48.351 31.014\nGesturePhaseSegmentationProcessed 2.386 2.219 2.32 2.255 2.232 2.195 2.131 1.984 1.228 1.085\ncnae-9 96.042 96.437 96.238 95.997 95.31 91.713 89.857 85.756 69.218 57.001\nMNIST 79.429 79.083 79.278 79.073 79.714 77.116 76.625 74.633 66.679 62.783\nilpd 0.203 0.171 0.155 0.22 0.204 0.178 0.172 0.132 0.111 0.099\nmfeat-fourier 12.745 12.761 12.749 12.89 12.66 12.456 12.165 11.485 8.031 6.47\nsteel-plates-fault 1.775 1.646 1.626 1.455 1.565 1.391 1.299 1.125 0.644 0.466\nmfeat-pixel 93.229 94.787 95.442 89.544 84.487 72.357 64.771 55.729 35.274 16.373\n29",
  "textLength": 124335
}