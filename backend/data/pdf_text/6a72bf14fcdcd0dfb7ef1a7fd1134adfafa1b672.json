{
  "paperId": "6a72bf14fcdcd0dfb7ef1a7fd1134adfafa1b672",
  "title": "Predict and Write: Using K-Means Clustering to Extend the Lifetime of NVM Storage",
  "pdfPath": "6a72bf14fcdcd0dfb7ef1a7fd1134adfafa1b672.pdf",
  "text": "Predict and Write: Using K-Means Clustering to\nExtend the Lifetime of NVM Storage\nSaeed Kargar\nUC Santa Cruz\nskargar@ucsc.eduHeiner Litz\nUC Santa Cruz\nhlitz@ucsc.eduFaisal Nawab\nUC Santa Cruz\nfnawab@ucsc.edu\nAbstract — Non-volatile memory (NVM) technologies suffer\nfrom limited write endurance. To address this challenge, we pro-\npose Predict and Write (PNW), a K/V-store that uses a clustering-\nbased machine learning approach to extend the lifetime of\nNVMs. PNW decreases the number of bit ﬂips for PUT/UPDATE\noperations by determining the best memory location an updated\nvalue should be written to. PNW leverages the indirection level\nof K/V-stores to freely choose the target memory location for any\ngiven write based on its value. PNW organizes NVM addresses\nin a dynamic address pool clustered by the similarity of the\ndata values they refer to. We show that, by choosing the right\ntarget memory location for a given PUT/UPDATE operation, the\nnumber of total bit ﬂips and cache lines can be reduced by up\nto 85% and 56% over the state of the art.\nIndex Terms —hybrid DRAM-NVM, write endurance, K-means\nclustering, bit ﬂips\nI. I NTRODUCTION\nIn recent years, there has been a growing interest in Non-\nV olatile Memory (NVM)—such as Phase-Change Memory\n(PCM)—due to their unique characteristics, including non-\nvolatility, high density, high scalability, and byte addressabil-\nity. However, these emerging NVMs also pose a number of\nchallenges: They have limited write endurance and asymmet-\nric read/write access properties, requiring special treatment\nwhen deployed in large scale computing systems [1]. While\nDRAM’s write endurance (the number of writes that can\nbe applied to a block of storage media before it becomes\nunreliable) is on the order of 1015writes, NVM technologies,\nsuch as PCM, can be written only up to 108–109times [2].\nThe limited endurance of PCM means that cells can only\nbe written a limited number of times before they “wear\nout”. Some recent technologies, such as Intel’s Optane DIMM\n[3], aim to increase the endurance of NVMs signiﬁcantly.\nHowever, unlike other non-volatile technologies such as ﬂash,\nPCM cells are written on the byte of cache line granularity\ninstead of the page granularity leading to uneven wear-out\neven on the sub page-level. To ensure failure-atomicity for the\ndata structures stored in NVMs, software schemes, such as\nlogging [4] and shadowing [5], are used. This causes extra\noverheads in terms of write ampliﬁcation due to writing log\nentries or creating additional copies of the data [6]. Even in\nmodern NVM devices, such as Intel’s 3DXPoint—where it\nis claimed that performance is unaffected by the number of\nmodiﬁed words in a cache line—it is beneﬁcial to reducethe number of write operations, to improve endurance and\nretention.\nThere exists many proposals to increase the write endurance\nof NVM storage. One promising approach is the Read-Before-\nWrite (RBW) technique, in which the content of an old\nmemory block is read before it is overwritten with the new\ndata. This technique replaces each NVM write operation with\na more efﬁcient read-modify-write operation. Reading before\nwriting allows comparing the bits of the old and new data,\nupdating only the bits that differ. Other proposed methods,\nsuch as [6], [7], overcome the limitations of NVMs by\ndesigning data structures that decrease write ampliﬁcation.\nHowever, prior methods are either (1) application-agnostic\nwithout the ability to leverage the write and data patterns of\napplications, or (2) specialized solutions that are built for the\nwrite and data patterns of speciﬁc applications. Particularly,\napplication-agnostic solutions such as FNW [8] and NVM data\nstructures [6], [7], do not leverage the write and data patterns\nof the application and miss the opportunity to judiciously place\nwrites on memory locations that would minimize bit ﬂips.\nOn the other hand, specialized solutions such as [9], try to\nminimize the number of bit ﬂips via ﬁxed bit masks that target\nspeciﬁc predeﬁned workloads. This renders these solutions\nlimited to predeﬁned applications limiting performance for\napplications with dynamically changing write patterns.\nIn this paper, we propose Predict and Write (PNW), an\nNVM-based K/V store that uses a dynamic approach to\nminimize bit ﬂips adapting to new applications and dynamic\nworkload changes. PNW decreases both the number of NVM\nline writes as well as the number of NVM word writes (see\nsection VI-A).\nWe leverage machine learning (ML) to continuously learn\na model that reﬂects the existing write patterns of a given\nworkload. The model learns to cluster memory locations in\nNVM enabling the placement of future writes to locations\nthat minimize the amount of bit ﬂips. Furthermore, by pe-\nriodically retraining the ML model, it adapts dynamically to\na changing workload without the need for user intervention.\nIt is worth noting that unlike the previous methods, which\nare based on the RBW technique, PNW does not depend\non NVM hardware modiﬁcations. This is because we do not\nrequire using hardware-based read-modify-write operations\nbefore write operations as we can avoid writing similar data\nat a larger granularity ( e.g. a cache line). However, futurearXiv:2011.02556v1  [cs.DB]  4 Nov 2020\n\nTABLE I: Comparison of memory technologies [10], [11]\nCategory Read Latency Write Latency Write Endurance\nHDD 5ms 5ms \u00151015\nDRAM 50\u001860ns 50\u001860ns \u00151016\nPCM 50\u001870ns 120\u0018150ns 108\u0018109\nReRAM 10ns 50ns 1011\nSLC Flash 25\u0016s 500\u0016s 104\u0018105\nSTT-RAM 10\u001835ns 50ns \u00151015\nwork on combining PNW with custom hardware support could\nfurther reduce the number bit ﬂips at the bit or byte level.\nOur design consists of a ML model, a hash index, a\ntable for storing metadata named the dynamic address pool\nand a data zone to store the actual data or K/V pairs (see\nsection V-A). We also show in the evaluation section that\nthe performance beneﬁts obtained from the ML technique\nsigniﬁcantly outweigh its overhead in terms of space cost and\ntime. This is the case even when the ML models are running on\nCPUs without using specialized hardware. Future extensions\nof our proposal to use methods that process the ML model\non specialized hardware such as accelerators and TPUs would\nfurther improve the efﬁciency of our approach [12].\nII. B ACKGROUND\nA. Non-Volatile Memory Technologies\nEmerging Non-V olatile Memory (NVM) technologies, such\nas Phase-Change Random Access Memory (PCRAM, PRAM,\nor PCM) and Resistive Memory (ReRAM), provides fast\npersistent storage, signiﬁcantly outperforming traditional Disk\nand Flash technologies. Table I shows the performance charac-\nteristics of some prevalent memory technologies. While NVM\nprovides similar read latency to DRAM, its write latency is\nhigher than DRAM and thus, minimizing write operations be-\ncomes critical for designing software systems on top of NVMs.\nFor this work, we assume a hybrid memory architecture, where\nboth DRAM and PCM exist on the same main memory level,\nmanaged under a single physical address space [13]. Although\nPNW can support other memory architectures, it is designed\nto work on hybrid memory systems, in which case NVM acts\nas a fast persistent memory directly connected to the memory\nbus.\nA PCM write operation demands signiﬁcantly more current\nand power than a read operation. This property is of great\nimportance in systems like mobile systems, even requiring\nthem to support “iterative writing” of data units of smaller\nsizes than memory words to limit the instantaneous current.\nFor example, in [14] and [15], the write modes of ×2, ×4,\nand ×8 are supported instead of faster modes like ×16. As\nanother example, in [16], the serial writing of even one bit at\na time is supported. Integrating NVMs into existing computer\nsystems requires to develop new NVM-friendly data structures\n[17] that focus on special properties such as reducing write\nampliﬁcation [7], or being lock-free [18], [19]. For instance,\nthe techniques proposed in [7], [20], [21] target the reduction\nof write ampliﬁcation, leading to the reduction of bit ﬂips.However, these methods lead to the increase in wear-out cost\nmostly because of overlooking the reduction of bit ﬂips at the\nexpense of the reduction of write ampliﬁcation. We show in\nSection VI that this problem can also lead to the increase in\nthe number of written cache lines compared to PNW, which\nfocuses on minimizing bit ﬂips.\nIII. R ELATED WORK\nAlthough recent methods have been able to address write\nendurance by reducing the number of bit ﬂips through the\nRBW technique and specialized NVM data structures, they\nleav out signiﬁcant opportunities for reducing additional bit\nﬂips: (1) application-agnostic methods that do not leverage the\nwrite and data patterns including RBW based techniques [8],\n[22], [23] and NVM-based structures that aim to reduce\nwrite ampliﬁcation [6], [7] miss the opportunity of using\nexisting patterns in the stored data to minimize bit ﬂips.\nSpeciﬁcally, writes are generally updated in place, whereas\nour proposed technique determines the target memory location\nbased on the written data values (2) specialized methods that\nare designed for speciﬁc workloads, such as Captopril [9],\ndecrease the number of bit ﬂips via ﬁxed bit masks. These\nmethods only work on speciﬁc workloads and suffer from\nsigniﬁcant overheads. In particular, the bit-masks are storage\nspace intensive themselves and, furthermore, as the bit masks\nare determined once, the approach cannot adapt to changing\nworkloads.\nIn [23], the authors propose DCW to ﬁnd common patterns\nand then compress data to reduce the number of bit ﬂips in\nSCM. Like FNW, DCW replaces a write operation with an\nRBW process. MinShift [22] proposes a method to reduce\nthe total number of updated bits to SCMs. The main idea of\nthis method is that if the hamming distance falls between two\nspeciﬁc bounds, the new data is rotated to change the hamming\ndistance. Captopril [9] is another method that reduces the\ntotal number of bit ﬂips by masking speciﬁc hot locations\nthat are ﬂipped more than others. However, as we will see in\nsection VI, this method suffers from relatively high overhead.\nMore importantly, it is specialized and would only work on a\npredeﬁned application.\nFinally, there is a group of techniques that ﬁnd the similarity\nbetween items through Locality Sensitive Hashing (LSH)\n[24], [25]. In this technique, each item is transformed to\na hash ﬁngerprint (usually using minhash), and later, LSH\nis applied to it. Since LSH does not preserve the bit-wise\nsimilarity among items, it cannot be efﬁciently used for bit-\nwise similarity clustering, which is the main purpose of PNW.\nIV. P REDICT AND WRITE\nWhenever there is a need for updating memory in-place,\nthe number of bit ﬂips depends on the hamming distance\nbetween the old data—currently in the memory location—\nand the new data, which is going to overwrite the memory\nlocation. PNW reduces bit ﬂips by avoiding in-place updates\nand, instead, ﬁnding a new memory location for each write\nthat would minimize the hamming distance. By placing the\n\nTABLE II: An example of a PCM with 6 elements\nCluster Index Content\n10 ’0’, ’0’, ’0’, ’0’, ’0’, ’1’, ’1’, ’1’\n1 ’0’, ’0’, ’0’, ’0’, ’1’, ’0’, ’1’, ’1’\n22 ’0’, ’0’, ’1’, ’0’, ’1’, ’1’, ’0’, ’0’\n3 ’0’, ’0’, ’1’, ’1’, ’1’, ’1’, ’0’, ’0’\n34 ’1’, ’1’, ’0’, ’1’, ’0’, ’0’, ’0’, ’0’\n5 ’0’, ’1’, ’1’, ’1’, ’0’, ’0’, ’0’, ’0’\nFig. 1: An example of a memory content that is going to be\nreplaced by a new item with close hamming distance in PNW.\nwrite operation in the right memory location that minimizes\nthe hamming distance between the old and the new data,\nthe number of bit ﬂips can be signiﬁcantly reduced. While\npromising for reducing bit ﬂips, this technique introduces\nseveral challenges. First, it requires an indirection layer to\nmap a logical value to its current physical location. As the\nwrite unit size of NVMs is a byte, storing these mappings on\nthe byte level introduces a signiﬁcant overhead. Second, the\ntechnique requires computing the hamming distance between\nthe new (to-be-written) data and all the available physical data\nlocations. Computing the similarity between all locations is\nprohibitive.\nThe ﬁrst challenge is addressed by leveraging a K/V store\nthat already implements an indirection layer to map keys\nto values. To address the second challenge (ﬁnding the\nright memory location for a write operation to minimize the\nhamming distance), we introduce a machine learning approach\nbased on k-means clustering.\nThe intuition behind our clustering approach is that we\ncluster similar memory locations in terms of the bit patterns of\ntheir contents. Using this clustering, we can quickly retrieve\na new memory location for a PUT operation such that the\nhamming distance between the new to-be-written data and the\nold memory location where it will be written is minimized.\nWe do not need to perform k-means clustering for each\nPUT/DELETE operation; instead, it is sufﬁcient to perform\nclustering periodically. We evaluate the training frequency and\nits effect on reducing bit ﬂips in Section VI-F.\nTo illustrate our approach, consider a storage system that is\nusing a PCM as its persistent memory with a capacity of six\nequal sized (8 words) entries, managed by a free-list which we\nrefer to as the dynamic-address-pool (Table II). Now, suppose\nthat we have two PUT operations that write the following new\ndata items, d1: [’0’, ’0’, ’0’, ’0’, ’1’, ’1’, ’1’, ’1’] and d2: [’1’,\n’1’, ’1’, ’1’, ’0’, ’0’, ’0’, ’0’].\nIn a regular system, where updates are applied in place,\nthere exists only one option to write the data and hence\nthe reduction of bit ﬂips with techniques such as FNW is\nlimited. PNW, on the other hand, determines the best memory\nlocation to write the new data by computing the minimum\n(a) Proposed architecture for small keys\n(b) Proposed architecture for large keys\nFig. 2: An example of procedures which serve K/V PUT and\nDELETE operations for a) small and b) large keys.\nhamming distance between the new data and existing free\nmemory locations maintained in the dynamic-address-pool.\nComputing all hamming distances grows in complexity with\nthe number of entries in the dynamic-address-pool and hence\nbecomes intractable. To overcome this problem, PNW groups\nthe entries in the dynamic-address-pool into clusters according\nto their hamming distance.\nFor instance, we can group the elements from the example\nin Table II into three clusters where indexes 0 and 1 form\ncluster 1, indexes 2 and 3 form cluster 2, and indexes 4 and 5\nform cluster 3. Now, if we receive the same new items d1 and\nd2, we direct them to clusters that are closest to them, which\nare clusters 1 and 3, respectively. These items are grouped\ntogether because the K-means model assigns data points to a\ncluster such that the sum of the squared distance between the\ndata points and the cluster’s centroid (arithmetic mean of all\nthe data points that belong to that cluster) is at the minimum.\nIn this example, the centroids for the ﬁrst, second, and third\nclusters would be [0. 0. 0. 0. 0.5 0.5 1. 1. ], [0. 0. 1. 0.5\n1. 1. 0. 0. ], and [0.5 1. 0.5 1. 0. 0. 0. 0. ], respectively.\nBecause the variations within clusters are minimal, the data\npoints are homogeneous (similar) within the same cluster. In\nthis scenario, wherever we decide to write the items within\ntheir corresponding clusters, we will end up writing only 1\nbit for each item, without any extra ﬂag bits. This is a simple\nexample of how PNW works.\nIt is worth noting that PNW reduces the number of writes\nin two ways: (1) the ﬁrst way is by writing new items in-\nplace to replace a similar old value in terms of hamming\ndistance. This leads to PNW decreasing NVM word writes\n(i.e., the number of modiﬁed words in a cache line.) (2) In the\nsecond way, PNW decreases the number of NVM line writes,\nrespectively cache lines needed to be written per item. For\n\nFig. 3: PCA variance ratio according to the number of principal\ncomponents.\nexample, suppose that the page size in our system is 4KB as\nshown in Figure. 1. In this scenario, if the items are similar to\neach other in terms of the hamming distance, fewer number of\ncache lines are needed to fulﬁll the request (suppose each part\nin Figure. 1 is a cache line). This enables PNW to decrease\nNVM word writes in addition to NVM line writes.\nV. K EY-VALUE STORE DESIGN\nIn this section, we present the design of our K/V store\nutilizing the Predict-and-Write technique. We ﬁrst describe the\nML model and then discuss the capabilities supported by our\nproposed K/V store.\nA. Overview and system model\nOur design consists of a ML model, a hash index, a table for\nstoring available (free) NVM addresses dynamic address pool ,\nand the K/V data zone to store the K-V pairs. In Figure. 2,\nwe show a K/V store on a DRAM-NVM hybrid memory\nlayout using our PNW method. Our data store implementation\nsupports K/V operations including GET ,PUT , andDELETE .\n1) Machine Learning Model: Our proposed machine learn-\ning method learns the existing data distribution among real-\nworld workloads to decrease the bit ﬂips in write operations.\nWe utilize an unsupervised ML model that is able to cluster\ndata elements into a number of clusters based on their simi-\nlarity. In particular, we leverage K-means clustering to cluster\nthe available data on PCM. The size of the buckets (the unit\nof the value size) can vary ranging from a word size to the\nsize of a page or even the size of a document depending on\nthe system.\nIn our system, each memory location is encoded as a vector\nof bits, each of which is used as a feature/dimension. The\nentire data zone can be encoded as a 2D tensor (that is, an array\nof vectors) of shape (n, m), where the ﬁrst axis (n) represents\nthe samples (old data) and the second axis (m) represents the\nfeatures. Because the size of the buckets can be very large\n(thousands of bits), it can lead to a problem referred to as the\n“curse of dimensionality”, which increases the training time\nand space complexity of the model signiﬁcantly.\nFig. 4: Sum of Square Error graph to ﬁnd the optimal K.\nAddressing the Curse of Dimensionality To tackle the\ncurse of dimensionality problem, we use Principal Component\nAnalysis (PCA) on the data sets used in this paper reducing\nthe number of dimensions before training the model. Although\nPCA is applicable to all data sets, it is especially useful for\nthe ones with a very large number of features. Projecting data\nto a lower dimensional subspace is very common in different\nareas such as meteorology, image processing, and genomics\nanalysis, especially before K-means clustering is applied [26]–\n[29]. The main basis of PCA-based dimension reduction is to\nkeep only the principle components (features) which explain\nthe most variance in the original data [30]. Figure. 3 shows\nthe PCA variance ratio according to the number of principal\ncomponents for MNIST, which is one of the data sets we\nuse in our tests. In this example, we only keep the ﬁrst 1000\nprincipal components (features) because they are enough to\nrepresent more than 80% of the variance in the data.\nDetermining the Number of Clusters Another important\ndecision that needs to be made before training the model is\nto determine the number of clusters (K). There are a number\nof ways to determine the optimal value for K [31]. In this\nwork, we use one of the most common techniques called the\n“elbow method” [32]–[34]. The elbow method is expressed as\nthe following Sum of Squared Error (SSE) [33]:\nSSE (X;\u0005) =KX\ni=1X\nxj2Cikxj\u0000mik2\n2 (1)\nwherek:k2denotes the Euclidean (L2) norm,\nmi=1\njCijP\nxj2Cixjis the centroid of cluster Ci\nwhere the cardinality is jCij,\u0005=fC1;C2;:::;C Kg, and\nX=fx1;:::;x i;:::;x Ng(N is the feature vector).\nIn this method, the value for SSE is calculated as we\nincrease the number of clusters. To determine the optimal\nnumber of clusters, we identify a sharp decrease known as\nthe “elbow” or “knee”, which suggests the optimal value for K\n[33]–[35]. Figure. 4 shows an example of choosing the optimal\nK by seeing the signiﬁcant decrease in the SSE graph, which\nis in K = 5 (the data set is MNIST).\n\nFig. 5: Dynamic Address Pool.\nThe ML model is constructed on DRAM as it does not\nneed to be persistent and can be reconstructed after a crash.\nBy constructing the model on DRAM, we take advantage of\nboth DRAM’s high write endurance and DRAM’s high speed.\nAnother advantage of our proposed method is that this model\ncan be replaced by any customized learning model.\n2) Dynamic address pool: The dynamic address pool is a\ntable that contains a number of entries, equal to the number\nof clusters in the ML model (Figure. 5). Each entry in the\ndynamic address pool contains a free-list of the available\nmemory locations that belong to the same cluster, as it is\nlearned by the ML model.\nInitialization. The ﬁrst step of initialization is creating a K-\nmeans clustering model based on the number of clusters we\nwant to have, and then training the model on all the available\ndata in the NVM storage called the data zone (Algorithm 1).\nThe next step is to label data items in each memory location\n(line 3). Finally, we add the available addresses on the data\nzone to their corresponding entry in the dynamic address pool\n(lines 4 and 5). Now, when a PUT request is received by\nthe system, the ML model ﬁnds its label, and based on that\nlabel, the dynamic address pool returns one address from\ncorresponding cluster. We maintain a ﬂag for each address in\nthe dynamic address pool to indicate whether it is available.\nWe also remove memory addresses out of the dynamic address\npool when they are allocated to a K/V pair and reinsert them\nafterwards to ameliorate the cost of keeping a ﬂag per address\nin terms of lookup time.\nIt is worth noting that the storage overhead of the dynamic\naddress pool is proportional to the number of pointers that are\nstored per value. As a result, for large values, the size of the\ntable does not grow signiﬁcantly. For small values, however,\nthe number of addresses that needs to be stored per value\ncan grow substantially. To limit the table size, we set a ﬁxed\nnumber of entries in the table, so the size of the table cannot\nnot grow to more than a speciﬁc maximum threshold. In this\nway, the table is used by adding addresses in and removing\nthem from the table until the number of available addresses\ngoes under a minimum threshold, called the load factor, which\nis described in details in section. V-C.Algorithm 1: Initialization\n// n clusters: number of clusters\n// D’ and A: content and addresses of\nthe data zone\n// DAP: Dynamic Address Pool\n// N: len(D’)\n1:model = KMeans(n clusters)\n2:model.train(D’)\n3:labels = model.labels\n4:for (i:=0, i<N, i++)\n5: DAP[labels[i]].append(A(i))\n3) Hash index: Indexing is critical in designing K/V stores.\nOur hash index component maps each key to the memory\nlocation that contains its value in the NVM data zone. To build\nindexes that support K/V operations, there exists a variety of\nchoices ranging from B+-Tree to LSM trees to hashmaps.\nThe operational efﬁciency of each indexing structure varies\nfrom one implementation to another and hence the optimal\nimplementation is application speciﬁc. For the existing im-\nplementation, we choose hash indexing, however, it can be\nreplaced with any other indexing data structure. The only\nrequirement of the indexing structure is that it can map logical\nkeys to arbitrary physical memory addresses.\nWe have two choices to store the indexing structure:\n\u000fIf we place the indexing structure into PCM (Figure 2b),\nthere is no need to rebuild it during the recovery from\na crash. However, it also introduces extra writes to the\nNVM because of the write ampliﬁcation problem induced\nby indexing data structures such as B+Trees and hash\nindexing. It is a good design choice when the size of\nthe keys are large because in that case the wear-out\ncost of the hash index is negligible. However, for small\nkeys, it represents a problem which we mitigate by\nleveraging data structures such as NVM-friendly hashing\nindexes [20].\n\u000fAnother design choice is to build the indexing structure\non DRAM (Figure 2a). This architecture is particularly\nbeneﬁcial when the size of the keys are small. In this\ncase, we do not pay any cost for the extra bit ﬂipping\nthat is caused by the write ampliﬁcation of the indexing\nstructures. Nonetheless, we need to build the whole data\nstructure from scratch during recovery after a crash.\nIn the evaluation, we build and persist a write-friendly hash\nindex in PCM as introduced in [20]. We perform the tests\nbased on this design to explore the worst case scenario of\nputting the hash index on PCM in terms of extra bit ﬂips\nintroduced by write ampliﬁcation. Also, for every entry in\nthe hash index, there is a ﬂag bit that shows whether the\ncorresponding key is available or not. In particular, whenever\nwe receive a delete request, we can reset its corresponding bit\nin the hash index to reﬂect that the corresponding index does\nnot exist anymore instead of deleting it. We can do the same\nprocedure for deleting a K/V pair from the data zone.\n\nB. Supported K/V Operations\n1)PUT Operation: PUT andUPDATE operations are\nexecuted as follows. As shown in Figure 2, when our system\nreceives a write request such as PUT , the model is queried to\ndetermine the cluster that is closest to the value-to-be-written\nin terms of their hamming distance. Then, a memory address\nis returned from that cluster by using the dynamic address\npool. Then, the K/V pair is written into the returned address,\nwhich is in the K/V zone on NVM. Finally, the newly-added\nindex entry is added to the hash index (step 3).\nAlgorithm 2 illustrates the pseudo-code of the write opera-\ntion under the PNW scheme (Figure 2). The ﬁrst step of PNW\nis to ﬁnd its label, which is equal to its corresponding entry in\ndynamic address pool, using the ML model (line 1). Next, we\nselect one of the available addresses from the corresponding\nentry in the dynamic address pool, and write the data to the\naddress (lines 2 and 3). Next, we need to remove the selected\naddress (A) from the cluster’s free-list in the dynamic address\npool (line 4). Finally, only the bits (in the buffer D) that are\ndifferent than the data in PCM (D’) are actually updated (lines\n5 and 6). We also need to update the hash index at the end to\nenable ﬁnding the value for future lookups (line 7).\n2)DELETE operation: Algorithm 3 illustrates PNW’s\ndelete operation (also see Figure 2). The delete procedure is\naccomplished by the following steps. In step 1, to ﬁnd the\nitem in the K/V data zone, the delete request is directed to the\nhash index, and then the associated entry is deleted from the\nK/V data zone by resetting the associated ﬂag bit (lines 1 and\n2). In this step, the delete operation is completed; however, to\nmake the system more efﬁcient, we recycle the recently freed\naddress back to the dynamic address pool by ﬁnding the label\nof the deleted data (line 3), and then adding the freed address\nto the corresponding entry in the dynamic address pool (line\n4). In this way, the address can be used again in the future,\nand the model is re-trained less frequently.\n3)UPDATE Operations: An update operation can be im-\nplemented in two different ways:\n\u000fIf we care about the write endurance more than latency,\nthe update operation consists of the delete operation plus\nthePUT operation in order to prevent bit ﬂipping as much\nAlgorithm 2: Write operation\n// D’ and D: old and new (key,value)\n// DAP: Dynamic Address Pool\nWrite (D: (key,value)) f\n1:E = model.predict(D); //predict the\nentry\n2:A = DAP.get(E);//get the address\n3:D’= Read(A); //old (key,value)\n4:DAP.remove(A) //remove the address\nfrom DAP\n5:for each bit in fDgandfD’g\n6: if they differ, update memory bit\n7:HI.put(D, A) //update the hash index gAlgorithm 3: DELETE operation\n// D’: old key\n// DAP: Dynamic Address Pool\n// HI: Hash Index\nDelete (D’: key) f\n1:A = HI.get(D’); //get the address\n2:Reset-Flag-Bit(A);//delete\n3:E = model.predict(Read(A)); //predict\nthe entry\n4:DAP.update(A:address, E:entry);//add\nthe address back to DAP g\nas possible. It means that the item that has to be updated\nis ﬁrst deleted from NVM (delete operation), and then its\nnew place is found by in a dynamic address pool ( PUT\noperation) using the model. It is worth noting that we can\ndo the DELETE -PUT process asynchronously to mitigate\nthe latency problem. In other words, the system can retain\nsynchronous updates to K/V items and the hash index in\nNVM, and for the dynamic address pool in DRAM, it\ncan be asynchronously updated through the model in the\nbackground to hide the extra latency.\n\u000fOn the other hand, if the application cares about latency\nmore than the other factors, especially wear-leveling, the\nrequest just needs to go through the hash index to ﬁnd\nits place in the K/V data zone and then update the item\nin place without any further changes since it does not\naffect the dynamic address pool. In this way, we sacriﬁce\nwear-leveling to achieve lower latency.\nIn our system and evaluations, we follow the ﬁrst approach\nas our main goal is to increase write endurance. However,\nit turns out—as we present in experimental evaluations—that\nminimizing bit ﬂips is also good for performance alleviating\nthe trade-off between write endurance and latency.\n4)GET Operation: Read operations in our system are\nstraight-forward as they do not lead to changing any data\nstructures. Speciﬁcally, a get request goes through the hash\nindex to ﬁnd its corresponding value from the K/V data zone,\nand then the read value is returned.\nC. Additional design considerations\nIt is possible that all the available addresses of a cluster\n(called cluster C) are utilized. In this case, if the model sends\na request that requires a new address from cluster C, the\ndynamic address pool will not be able to serve this request\nbecause there are no more addresses available in that cluster.\nTo avoid this problem, we deﬁne a load factor for the K/V data\nzone on the NVM. Setting the load factor to xpercent, means\nthat when x percent of the available addresses in the K/V\ndata zone are used, the K/V data zone needs to be extended.\nTo add new memory addresses to the data zone, we need to\ntrain a new model. It is worth noting that, unlike traditional\nmethods, we do not need to move or change anything in the\nhash table on NVM because they still have valid information.\n\nThe only things that need to be changed are the model and\ndynamic address pool, which are both located on DRAM. So,\nour method to expand the size of a cluster does not impose\nany extra writes to the NVM.\nThe main reason behind deﬁning the load factor is to prevent\nlatency spikes or stalls in the system. The load factor is similar\nin principle to the load factors that are used in hashing schemes\nas a way to monitor the space utilization of the system to\nprevent hash collisions. In other words, the load factor is going\nto warn us that the system will need to be retrained in the near\nfuture. So, before this happens, we can re-train a new model,\nby adding some new memory locations to the K/V data zone,\nin the background while the system is running. Then, we can\nswitch to the new model and table before the previous model\ngets stuck. In this case, we can hide the re-training latency\nand the system works without disruptions due to retraining.\nWe have done some tests in the next section to ﬁgure out the\nbest time to start training a new model before the old one is\nfull to keep the system working smoothly. PNW supports any\nsize of key/values from 32-bit word size to the page sizes of\n4KB to the size of a document. Thereby, the way in which\ndata elements are provided to the models depends on the K/V\npair size. For instance, small (e.g. 64 bit) data elements can\nbe directly passed to the model, while for large data element\n(e.g. 4KB) we ﬁrst apply dimensionality reduction using PCA\nbefore passing the data to the model.\nVI. E VALUATION\nA. Methodology\nIn this section, we evaluate our proposed method using\ndifferent metrics focusing on the reduction in writes and bit\nﬂips. We leverage a collection of real and synthetic data sets.\nSince only insert and delete requests cause mutating the state\nof the NVM, we insert n items into the K/V store followed by\ndeleting 0.5n items (except for section VI-F). Also, we do not\nmake any assumption about the access pattern within or across\nclusters. So, we simply apply the K-means clustering (from the\nscikit-learn library) based on the available memory locations\non PCM. We compare PNW with both RBW solutions and\nK/V stores. For the former, we compare with the writes on\nthe storage component of PNW, which is the data zone.\nWe compare our results against other methods described\nin Section III, such as FNW [8], DCW [36], Captopril [9]\nand MinShift [22]. For synthetic data sets, our sample K/V\nstore system has at least 10M buckets. When there are 10M\nbuckets, for instance, we ﬁrst warm-up K/V stores with 10M\nkey/values. This means that we store some items as “old data”\nbefore starting our tests. The data type and distributes of these\nitems differ depending on the test. “old data” is used for the\ninitial training of the ML model.\nTo compare PNW’s results with other methods, we tune\ntheir parameters in such a way that they achieve their best\nperformance. For example, we allow MinShift to shift n times,\nwhere n is the size of the item instead of the size of the word,\nwhich means it always results in its best performance in terms\nof the number of bit ﬂips [22]. With respect to Captopril, wealso considered its best case, which happens when the blocks\nare partitioned into n = 16 segments [9].\nUnless we mention otherwise, we execute the K/V op-\nerations with randomly selected key/values from the same\ngenerator. As real NVM DIMMs are not available for us yet,\nwe emulate NVM using DRAM similar to prior works [37]–\n[40]. We assume an access latency of the latest 3D-XPoint of\n600ns [41], [42].\nThe experiments are executed on an Intel Core i7 processor\nrunning at 2.2 GHz with 2 cores (4 logical cores), each of\nwhich has 256KB L2 Cache and 4MB L3 Cache using 8\nGB of RAM, running macOS Catalina (version 10.15.4). The\nreason that we run the tests on a local computer without any\nGPU support is to get a sense of how our methods work\non an ordinary system without any unique capabilities. We\ntest our proposed method using various data sets, which can\nbe categorized as real-world textual and numerical data, real-\nworld multimedia data including image and video data sets,\nand ﬁnally, hard-to-cluster synthetic data sets. In the following\nsubsections, we show the results of the tests on these data sets\nand analyze them.\nB. Real-world textual and numerical data sets\nThe ﬁrst data set is called Amazon Access Samples Data\nSet [43], [44], containing 30K log entries. Although this data\nset has 20K attributes, in this test, only less than 10% of\nthem are used for each sample. For this test, we ﬁrst have\nset aside 5K buckets as the “old data” on the NVM memory\nand then warmed up the system by writing 5K items from\nthe data set into our buckets. Then, we replaced this “old\ndata” with new incoming data from the same data set (the\nremaining 25K items). Figure. 6a illustrates that when there\nare one or two clusters, the number of written bits in our\nmethod is more than FNW. Nevertheless, when the number of\nclusters is more than 2, we start to get better results until we\nreach between 15%(compared to CAP16) to 70%(compared\nto the conventional method) improvements compared to the\nother methods when the number of clusters is 30.\nThe next real-world data set, i.e., 3D Road Network Data\nSet [45], [46], contains information of road networks in North\nJutland, Denmark (covering a region of 185 x 135 km2). We\nused the same setup as above for this data set containing\n434874 entries. In this test, we chose 100K buckets as “old\nmemory” and warmed up the system by 100K entries from\nthe 3D Road Network Data Set. The results are shown in\nFigure. 6b. When the number of clusters is big enough (here k\n= 14), PNW starts to outperform all the other methods in terms\nof the number of bit ﬂips until it gets its highest performance\nwhen k=30 (between 10% to 63% improvements compared to\nthe other methods).\nFinally, the last real-world data set is one of the collections\nof a database called DocWord, which consists of ﬁve text\ncollections in the form of “bags-of-words”. This collection,\nwhich is called PubMed abstracts [44], consists of 730 million\nwords in total. For doing the tests, we ﬁrst created 100M\nbuckets as the “old data” storing data from the PubMed data\n\n(a) Amazon Access Samples\n (b) 3D Road Network\n (c) Sherbrooke\n(d) seq 2 trafﬁc surveillance\n (e) normal data distribution\n (f) uniform data distribution\nFig. 6: The average number of actual bit updates per writing 512 bits as well as the latency of prediction per item in PNW\nfor the real-world textual and numerical data sets (a-b), multimedia data sets (c-d), and synthetic data sets (e-f).\nset. Then, we wrote the new incoming data items from the\nsame data set on the previous data items stored on the buckets\nand kept track of the number of the updated bits per 512 bits.\nC. Real-world multimedia data sets\nIn the ﬁrst set of tests, we have used some video data\nsets to see what happens if a system, for instance, a CCTV\nrecorder, uses an NVM media as its persistence memory. We\nhave used two video data sets: 1) The Sherbrooke video data\nset [47], representing a two-minute-long video (with resolution\n800x600). 2) A Trafﬁc Surveillance video [48], collected from\nseven intersections in the Danish cities of Aalborg and Viborg,\ncontaining 21 ﬁve-minute sequences of two cameras including\nRGB and thermal data. The resolution of both cameras is\n640x480 pixels, and the frame rate is ﬁxed at 20 fps. In this\ntest, we just used one sequence of RGB camera called “day\nsequence 2”.\nFor the ﬁrst data set (Sherbrooke), we stored the ﬁrst 30\nseconds of this video as the old data, and for the second\none (Seq2), we stored the ﬁrst one minute of the video as\nthe old data and used the remaining of the video as the new\ndata. The results are shown in Figure. 6c and 6d, respectively.\nThese ﬁgures show that our method outperforms the other ones\nin both data sets. For the ﬁrst data set (Sherbrooke), PNW\nimproves the other methods between 14% to 60% and for the\nsecond one (Seq2), we outperforms the other ones between\n21% to 67%.\nThe next data set is one of the most widely used data sets for\nmachine learning research, and especially for computer vision\nalgorithms, i.e., CIFAR-10 data set [49]. This data set is a\nsubset of the 80 million tiny images data set and consists of\n60,000 32x32 color images, grouped into ten different classes.\nSimilar to the previous experiments, we ﬁrst set aside 10K of\nthese images as the old data to ﬁll out the 10K buckets we\ncreated as our NVM system. Then, the new incoming data\nitems are written in place of the old ones one by one.D. Hard-to-cluster synthetic data sets\nIn this section, we are going to observe the behavior of PNW\non some synthetic data sets that do not follow any speciﬁc data\ndistribution. The reason of doing these tests is to discover the\nlimitations of our ML-based method and analyze them to give\nthe readers a clearer view of the possible applications of PNW.\nTo perform these tests, we start with a synthetic data set that\nshows a clear pattern and then test two more data distributions\nthat are completely different in terms of their data pattern.\nFor the synthetic data sets, we used 32-bit keys and values.\nWe also generated two types of integer data (normal and\nuniformly distributed), ranging from 0 to 232. For random\nintegers, we generated them via a pseudo-random number\ngenerator. For the normal data set, we generated a synthetic\ndata set of 100M unique values sampled from a normal\ndistribution with \u0016=231and\u001b=228to test our method. In all\nsynthetic data set tests, the conﬁdence interval was less than\n103for 95% conﬁdence level.\nFirst, we show the results of the ﬁrst synthetic data set,\nfollowing a regular pattern. Figure. 6e shows the results for\ndifferent number of clusters ranging from k=1 to k=30 for\nnormal distribution. We have compared the performance of\nPNW to the other ones in terms of the number of bits\nupdated/written per 512 bits. In this ﬁgure, we observe that\nwhen we pick k=1, the result for PNW is not different from\nDCW since both do the same thing if there is no clustering.\nOur approach enhances the results of DCW and FNW more\nthan 40% and 25%, respectively, when the number of clusters\nis more than 10. It also outperforms MinShift and Captopril\nmore than 15% and 10%, respectively. Also, the delay is\nalmost 5\u0016s to 6\u0016s most of the time.\nIn the second experiment, we did the same, but for a\ndifferent data distribution, i.e. uniform random distribution,\nto learn more about the behavior of our method. Data sets\nlike this one are highly random, and as a result, difﬁcult to\n\nFig. 7: End-to-end write latency comparison for various data\nsets.\nlearn using an ML model. The results are depicted in Figure. 6f\nshowing that although our method has succeeded in improving\nthe results for DCW, MinShift, and the conventional method\nby almost 15%, 5%, and 60%, respectively, it lags behind\nFNW and CAP16 for this data set as expected for the random\ndata set.\nIn some of the previous results, there are anomalies where\nthe number of bit ﬂips suddenly jumps while increasing the\nnumber of clusters. Such anomalies are due to the unpre-\ndictability of ML-based methods. However, we expect that\nsuch anomalies would be normalized during extended opera-\ntion.\nE. End-to-end write latency\nIn the following, we are going to measure the write latency,\nwhich includes the time spent on 1) predicting a cluster\nnumber, 2) ﬁnding an empty bucket within the dynamic\naddress pool, and 3) writing the key/value on NVM. We do\nthis test to measure the overhead of our method.\nIn Figure. 7, we show the write latency comparisons for\nvarious data sets. In this test, we use the normal and uni-\nform data distributions, Amazon Access Samples, 3D Road\nNetwork, CIFAR, and the day sequence 2 trafﬁc surveillance\nvideo. For our method, we had to train the model based on\nthe old data, ﬁlling out the dynamic address pool, and then\nwriting the new data. The write latency is calculated based on\nthe number of cache lines that are written per item. In this\ntest, we observe that each method that updates fewer bits has\na higher chance of having a lower write latency because it has\nto update fewer cache lines than the others.\nFigure. 7 shows the normalized time of the write operation\nrequired by different methods. As illustrated in this ﬁgure,\nour proposed method, when the number of clusters is enough,\ncan outperform the others even though it has to perform two\nadditional steps. The reason is that our method performs fewer\nwrite operations than the other ones, and it makes up the time\nit spends on the extra steps. However, for the uniform data\ndistribution, we could not do the same since PNW is not able\nFig. 8: The impact of choosing the number of clusters (K) on\nthe average write latency for the PubMed abstracts data set.\nFig. 9: The average number of written cache lines for each\nrequest.\nto ﬁnd a clear pattern among the data items to make up the\nextra steps.\nFigure. 8 compares the average write latency for different\nnumber of clusters (K) on the PubMed abstract data set. In this\ntest, to see the impact of K on latency, we invoke insert and\ndelete operations on the system in a 1:1 ratio. Note that the\nvalue of K does not affect the lookup request latency because\nin the lookup, the request does not go through the model or\nthe dynamic address pool. This test shows that by increasing\nK, latency decreases because all the items within a cluster\nbecome more similar (in terms of hamming distance). So, the\nnew items can be written by replacing old ones with a fewer\nnumber of cache line writes, which leads to decreasing latency.\nIn the next test, we compare PNW with recent K/V stores\nto see its performance in terms of the number of written\ncache lines. Like the previous test, since only insert and delete\nrequests cause writes to NVMs, we ﬁrst insert n items into the\nsystem and then delete 0.5n items. FP-Tree [21] is a hybrid\nSCM-DRAM persistent B+-Tree method that we implement\nand compared PNW with. The second persistent K/V store that\nwe compare PNW with is NoveLSM [7], which is a persistent\nLSM-based K/V storage system. It is designed to exploit non-\nvolatile memories in an attempt to provide low latency and\n\nFig. 10: The performance change by converting the workload\nfrom MNIST into Fashion-MNIST over time.\nhigh throughput to applications. We also implement a hashing\nscheme that is designed for NVMs called Path hashing [20].\nIt is worth noting that for this test, we implement PNW as\nshown in Figure. 2a.\nFigure. 9 shows the average number of written cache lines\nfor each request. The number of written cache lines per request\nin FPTree and NoveLSM is higher than others because they\nmodify more items to process a request. Although the number\nof written lines in path hashing is fewer than the others,\nits written lines are higher than PNW because: 1) It incurs\nmore writes when re-hashing to handle conﬂicts, and 2) like\nother methods, it is not “memory-aware”. PNW has the fewest\nwritten cache lines mostly because it can save some cache\nlines per request because of replacing the old items by similar\nnew items. We also observe that for some data sets the average\nnumber of written cache lines is higher for all methods because\nof the larger item size.\nF . Training overhead\nTo see how rapidly can our method adapt to changing work-\nloads, we conduct the last experiment to track the behavior\nof our method while changing the workload. You can see the\nresults in Figure. 10. In this test, we use two data sets from the\nKeras library, i.e., MNIST database of handwritten digits and\nFashion-MNIST database of fashion articles, each of which\ncontains 60,000 28x28 gray scale images, along with a test\nset of 10,000 images. For this test, we did the follows steps:\n\u000fPhase 1: we stored 28K images from the MNIST data\nset as the old data. After training the model and creating\nthe dynamic address pool, we started streaming 27K\nimages from the same data set (MNIST) as the new data\ninto the system to overwrite the old data. As we can\nsee in Figure. 10, there is no noticeable change in the\nperformance of the system in the ﬁrst 27K frames. Even\nat the end of this stage, where the old data is almost\ncompletely replaced with the new one, we still do not\nsee any substantial change in the performance.\u000fPhase 2: we send a mixture of items from two different\ndata sets, i.e., Fashion-MNIST and MNIST, at the ratio\nof 2 to 1. We shufﬂed 15K of MNIST images with 30K\nof Fashion-MNIST and then sent them to the system as\nthe new incoming data. As it is obvious in the ﬁgure,\nthe performance is affected immediately (the number of\nupdated bits increases) since two-third of the incoming\ndata are entirely different from the previous ones and as\na result have a larger hamming distance.\n\u000fPhase 3: In this phase, we sent 12K images only from\nthe second data set, i.e., Fashion-MNIST. The number\nof updated bits ﬂuctuated less since the old data contains\nthe items mostly from Fashion-MNIST, and the incoming\ndata is also from the same one too.\n\u000fPhase 4: In this phase, we continued sending 28K images\nfrom the second data set (Fashion-MNIST) with one\ndifference: we re-trained our model on the old data, which\ncontains the images from the Fashion-MNIST data set\nnow. As you can see in the ﬁgure, the results got better\nand ﬂuctuated less.\nAs a result, we have seen that, depending on the application\nand the workload, we do not always have to re-train the model\nrapidly, and we can use the same model for a certain amount\nof time before it needs to be re-trained. This allows us to do\nthe retraining in the background lazily and update the model\nperiodically.\nPNW is designed to enable re-training in the background\nwhile the current model is serving requests. However, to set\nthe load factor to its correct value, PNW needs to know when\nto start re-training the model before the old one becomes\ninefﬁcient, i.e. the system’s performance decreases in terms\nof the number of bit ﬂips. This is of great importance because\nwe might not want to give all the available resources to the\nmodel since the system needs to serve the requests without\nany problem while the new model is being re-trained. We\nperformed additional experiments to evaluate the costs for re-\ntraining a new model using different number of the available\ncores (Figure. 11). These experiments are performed on the\ntrafﬁc surveillance [48] and the Sherbrooke video data sets\n[47].\nIn this test (Figure. 11), we calculate the time needed for\nre-training the model for 2, 4, 8, and 16 clusters. In each case,\nwe did the test on two different modes: 1) running the model\non a single core; and 2) running the model on all 4 cores. As\nwe can see from the results, as the value of k and the sample\nsize increases, the model needs more time to be re-trained.\nFor instance, for training a model with k=16 clusters on more\nthan 8000 samples/frames (Figure. 11d), we need almost 20\nand 13 seconds if we use one and 4 cores, respectively. This\ncan give us an idea of setting the load factor in a way that we\nhave enough time to ﬁnish re-training the new model before\nthe old model becomes inefﬁcient. So, if we have more than\none core available for us in the system to train the model,\nmulti-core processing is worth it when the sample size is big\nenough.\n\n(a) Seq 2\n (b) Seq 4\n (c) Seq 8\n (d) Seq 16\n(e) Sher 2\n (f) Sher 4\n (g) Sher 8\n (h) Sher 16\nFig. 11: PNW’s average model training time for different data sets using single core versus multi-core processing.\nG. Wear-leveling\nAside from decreasing the number of writes, wear-leveling\nis equally important to extend the lifetime of PCM. The reason\nis that some blocks of PCM may receive a much higher\nnumber of writes than the other blocks, and as a result, wear\nout sooner [2], [50]. Therefore, to observe the performance of\nPNW in terms of the distribution of the maximum number of\nbit ﬂips and the wear-leveling of PCM, we conduct two more\ntests. In these tests, we run PNW in two different modes, i.e.\nfor k =5 and k = 30 clusters, on the combination of MNIST\nand Fashion-MNIST data sets. Like the previous test, we ﬁrst\nwarm up the data zone with 28K items from the combination\nof both data sets. Then, we stream 112K writes from the same\ndata sets to the system. During the test, we also perform delete\nactions to make space for incoming writes. In other words,\neach word in the data zone is updated 4 times on average.\nFigure. 12 shows the maximum number of times the ad-\ndresses in the data zone are written as a cumulative distribution\nfunction (CDF). In other words, this ﬁgure illustrates the\nestimation of the likelihood to observe an address in the data\nzone of PCM that is written less than or equal to a speciﬁc\nnumber of times. For example, as we can see in Figure. 12a,\nthe estimated likelihood to observe an address in the PCM\ndata zone to be written less than or equal to 5 (P (X \u00145))\nis 85% (Figure. 12a) and 86% (Figure. 12b) when we have\nk = 5 and k = 30 clusters, respectively. We also observe that\nmore than 99% of the addresses in the data zone experience\nno more than 10 writes for k = 5 and 15 writes for k = 30.\nThis results show that, regardless of the number of clusters,\nPNW distributes write activities across the whole PCM chip.\nFinally, we analyze the wear-leveling of memory bits as\nCDFs. Figure. 13 illustrates the estimation of the likelihood\nto observe a memory bit in the data zone of PCM that is\nwritten less than or equal to a speciﬁc number of times. For\ninstance, we observe that while the estimated likelihood of a\nmemory bit being written less than or equal to 4 times is 74%\nfor k=5 clusters (Figure. 13a), this likelihood rises to 98%\nwhen k=30 (Figure. 13b). This important observation shows\nan interesting fact about PNW: By increasing the number of\n(a) k = 5\n (b) k = 30\nFig. 12: The maximum update addresses as CDFs by applying\nPNW with a) k=5 and b) k=30 clusters.\n(a) k = 5\n (b) k = 30\nFig. 13: Wear-leveling as CDFs by applying PNW with a) k=5\nand b) k=30 clusters.\nclusters, bit ﬂips are distributed more evenly across the whole\ndata zone of the PCM chip, and as a result, the lifetime of\nPCM is extended more. The reason behind this is that when\nthe number of clusters increases, the items within the clusters\nbecome more similar to each other. Therefore, regardless of\nthe number of clusters, PNW evenly distributes writes not only\nin the address level but also in the bit level.\nVII. C ONCLUSION\nIn this paper, we improve the write bandwidth, write energy,\nwrite latency, and write endurance of NVMs through Predict\nand Write (PNW), a K/V store that uses a clustering-based\napproach to extend the lifetime of NVMs using machine\nlearning. We examined the performance of our proposed\napproach with others in terms of different factors such as the\nnumber of writes and the latency for various workloads, on\nboth synthetic and real-world data, with different distributions\nof data. The results show that our method outperform existing\n\nsolutions and that the beneﬁt of using a ML model outweigh\nits overhead. Based on the results, by choosing the right target\nmemory location for a given PUT/UPDATE operation, PNW\nhas succeeded in reducing the number of total bit ﬂips and\ncache lines over the state of the art.\nVIII. A CKNOWLEDGMENTS\nThis research is supported in part by the NSF under grants\nCCF-1942754 and CNS-1815212.\nREFERENCES\n[1] B. C. Lee et al. , “Architecting phase change memory as a scalable dram\nalternative,” in ISCA , 2009, pp. 2–13.\n[2] S. Mittal and J. S. Vetter, “A survey of software techniques for using\nnon-volatile memories for storage and main memory systems,” TPDS\n2015 , vol. 27, no. 5, pp. 1537–1550, 2015.\n[3] “optane-persistent-memory-200-series-brief,” https://www.\nintel.com/content/www/us/en/products/docs/memory-storage/\noptane-persistent-memory/optane-persistent-memory-200-series-brief.\nhtml, Intel.\n[4] J. Coburn et al. , “Nv-heaps: making persistent objects fast and safe with\nnext-generation, non-volatile memories,” ACM SIGARCH Computer\nArchitecture News , vol. 39, no. 1, pp. 105–118, 2011.\n[5] Y . Ni, J. Zhao, H. Litz, D. Bittman, and E. L. Miller, “Ssp: Eliminating\nredundant writes in failure-atomic nvrams via shadow sub-paging,” in\nProceedings of the 52nd Annual IEEE/ACM International Symposium\non Microarchitecture , 2019, pp. 836–848.\n[6] J. Liu et al. , “Lb+ trees: optimizing persistent index performance on\n3dxpoint memory,” Proceedings of the VLDB Endowment , vol. 13, no. 7,\npp. 1078–1090, 2020.\n[7] S. Kannan et al. , “Redesigning lsms for nonvolatile memory with\nnovelsm,” in 2018 USENIX Annual Technical Conference (USENIX ATC\n18), 2018, pp. 993–1005.\n[8] S. Cho and H. Lee, “Flip-n-write: A simple deterministic technique to\nimprove pram write performance, energy and endurance,” in MICRO\n2009 , 2009, pp. 347–357.\n[9] M. Jalili and H. Sarbazi-Azad, “Captopril: Reducing the pressure of bit\nﬂips on hot locations in non-volatile main memories,” in DATE 2016 .\nIEEE, 2016, pp. 1116–1119.\n[10] A. K. Kamath et al. , “Storage class memory: Principles, problems, and\npossibilities,” arXiv preprint arXiv:1909.12221 , 2019.\n[11] A. van Renen et al. , “Persistent memory i/o primitives,” in DaMoN’19 ,\n2019, pp. 1–7.\n[12] T. Kraska et al. , “The case for learned index structures,” in SIGMOD\n2018 , 2018, pp. 489–504.\n[13] G. Dhiman et al. , “Pdram: A hybrid pram and dram main memory\nsystem,” in 2009 46th ACM/IEEE Design Automation Conference .\nIEEE, 2009, pp. 664–669.\n[14] S. Kang, “A 0.1 \u0016m 1.8 v 256mb 66mhz synchronous burst pram,” in\nISSCC 2006 , 2006.\n[15] W. Cho et al. , “A 90 nm 1.8 v 512 mb diode-switch pram with 266\nmb/s read throughput,” in ISSCC 2007 , 2007, pp. 26–1.\n[16] S. Hanzawa et al. , “A 512kb embedded phase change memory with\n416kb/s write throughput at 100 \u0016a cell write current,” in ISSCC 2007 .\nIEEE, 2007, pp. 474–616.\n[17] F. Nawab et al. , “Dal ´ı: A periodically persistent hash map,” in 31st Inter-\nnational Symposium on Distributed Computing (DISC 2017) . Schloss\nDagstuhl-Leibniz-Zentrum fuer Informatik, 2017.\n[18] F. Nawab, D. R. Chakrabarti, T. Kelly, and C. B. Morrey III, “Procrasti-\nnation beats prevention: Timely sufﬁcient persistence for efﬁcient crash\nresilience.” in EDBT , 2015, pp. 689–694.\n[19] F. Nawab, D. Chakrabarti, T. Kelly, and C. Morrey, “Zero-overhead nvm\ncrash resilience,” in Non-Volatile Memories Workshop , 2015.\n[20] P. Zuo and Y . Hua, “A write-friendly and cache-optimized hashing\nscheme for non-volatile memory systems,” TPDS , vol. 29, no. 5, pp.\n985–998, 2017.\n[21] I. Oukid et al. , “Fptree: A hybrid scm-dram persistent and concurrent\nb-tree for storage class memory,” in SIGMOD ’16 , 2016, pp. 371–386.\n[22] X. Luo et al. , “Enhancing lifetime of nvm-based main memory with bit\nshifting and ﬂipping,” in RTCSA 2014 . IEEE, 2014, pp. 1–7.[23] D. B. Dgien et al. , “Compression architecture for bit-write reduction in\nnon-volatile memory technologies,” in NANOARCH 2014 . IEEE, 2014,\npp. 51–56.\n[24] O. Chum et al. , “Near duplicate image detection: min-hash and tf-idf\nweighting.” in BMVC , vol. 810, 2008, pp. 812–815.\n[25] A. Ghasemazar, P. Nair, and M. Lis, “Thesaurus: Efﬁcient cache com-\npression via dynamic clustering,” in ASPLOS ’20 , 2020, pp. 527–540.\n[26] H. Zha et al. , “Spectral relaxation for k-means clustering,” in NIPS 2002 ,\n2002, pp. 1057–1064.\n[27] A. Y . Ng et al. , “On spectral clustering: Analysis and an algorithm,” in\nNIPS 2002 , 2002, pp. 849–856.\n[28] I. T. Jolliffe and J. Cadima, “Principal component analysis: a review\nand recent developments,” Philos. Trans. Royal Soc. A: Mathematical,\nPhysical and Engineering Sciences , vol. 374, no. 2065, p. 20150202,\n2016.\n[29] C. Ding and X. He, “K-means clustering via principal component\nanalysis,” in ICML’04 , 2004, p. 29.\n[30] R. Cangelosi and A. Goriely, “Component retention in principal compo-\nnent analysis with application to cdna microarray data,” Biology direct ,\nvol. 2, no. 1, p. 2, 2007.\n[31] M. E. Celebi et al. , “A comparative study of efﬁcient initialization\nmethods for the k-means clustering algorithm,” Expert systems with\napplications , vol. 40, no. 1, pp. 200–210, 2013.\n[32] K. D. Joshi and P. Nalwade, “Modiﬁed k-means for better initial cluster\ncentres,” IJCSMC 2013 , vol. 2, no. 7, pp. 219–223, 2013.\n[33] M. Syakur et al. , “Integration k-means clustering method and elbow\nmethod for identiﬁcation of the best customer proﬁle cluster,” in IOP\nConference Series: Materials Science and Engineering , vol. 336, no. 1.\nIOP Publishing, 2018, p. 012017.\n[34] T. S. Madhulatha, “An overview on clustering methods,” arXiv preprint\narXiv:1205.1117 , 2012.\n[35] L. Vendramin et al. , “Relative clustering validity criteria: A comparative\noverview,” Statistical analysis and data mining: the ASA data science\njournal , vol. 3, no. 4, pp. 209–235, 2010.\n[36] B.-D. Yang et al. , “A low power phase-change random access memory\nusing a data-comparison write scheme,” in ISCAS 2007 . IEEE, 2007,\npp. 3014–3017.\n[37] J. Ou et al. , “A high performance ﬁle system for non-volatile main\nmemory,” in EuroSys ’16 , 2016, pp. 1–16.\n[38] H. V olos et al. , “Mnemosyne: Lightweight persistent memory,” ACM\nSIGARCH Computer Architecture News , vol. 39, no. 1, pp. 91–104,\n2011.\n[39] J. Huang et al. , “Nvram-aware logging in transaction systems,” Proceed-\nings of the VLDB Endowment , vol. 8, no. 4, pp. 389–400, 2014.\n[40] F. Xia et al. , “Hikv: A hybrid index key-value store for dram-nvm\nmemory systems,” in USENIX ATC 17 , 2017, pp. 349–362.\n[41] J. Izraelevitz, J. Yang, L. Zhang, J. Kim, X. Liu, A. Memaripour,\nY . J. Soh, Z. Wang, Y . Xu, S. R. Dulloor et al. , “Basic performance\nmeasurements of the intel optane dc persistent memory module,” arXiv\npreprint arXiv:1903.05714 , 2019.\n[42] Y . Ni, S. Chen, Q. Lu, H. Litz, Z. Pang, E. L. Miller, and J. Wu,\n“Closing the performance gap between dram and pm for in-memory\nindex structures,” no. UCSC-SSRC-20-01, May 2020.\n[43] “Amazon access samples data set,” https://archive.ics.uci.edu/ml/\ndatasets/AmazonAccessSamples.\n[44] D. Dua and C. Graff, “UCI machine learning repository,” 2017.\n[Online]. Available: http://archive.ics.uci.edu/ml\n[45] M. Kaul et al. , “Building accurate 3d spatial networks to enable next\ngeneration intelligent transportation systems,” in MDM 2013 , vol. 1.\nIEEE, 2013, pp. 137–146.\n[46] C. Guo et al. , “Ecomark: evaluating models of vehicular environmental\nimpact,” in SIGSPATIAL ’12 , 2012, pp. 269–278.\n[47] J.-P. Jodoin et al. , “Urban tracker: Multiple object tracking in urban\nmixed trafﬁc,” in WACV 2014 . IEEE, 2014, pp. 885–892.\n[48] C. H. Bahnsen and T. B. Moeslund, “Rain removal in trafﬁc surveil-\nlance: Does it matter?” IEEE Transactions on Intelligent Transportation\nSystems , pp. 1–18, 2018.\n[49] A. Krizhevsky, G. Hinton et al. , “Learning multiple layers of features\nfrom tiny images,” 2009.\n[50] F. Xia et al. , “A survey of phase change memory systems,” Journal of\nComputer Science and Technology , vol. 30, no. 1, pp. 121–144, 2015.",
  "textLength": 61424
}