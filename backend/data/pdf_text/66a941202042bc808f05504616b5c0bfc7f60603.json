{
  "paperId": "66a941202042bc808f05504616b5c0bfc7f60603",
  "title": "A Dual-Store Structure for Knowledge Graphs",
  "pdfPath": "66a941202042bc808f05504616b5c0bfc7f60603.pdf",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 1\nA Dual-Store Structure for Knowledge Graphs\nZhixin Qi, Hongzhi Wang, Member, IEEE, and Haoran Zhang\nAbstract —To effectively manage increasing knowledge graphs in various domains, a hot research topic, knowledge graph storage\nmanagement, has emerged. Existing methods are classiﬁed to relational stores and native graph stores. Relational stores are able to\nstore large-scale knowledge graphs and convenient in updating knowledge, but the query performance weakens obviously when the\nselectivity of a knowledge graph query is large. Native graph stores are efﬁcient in processing complex knowledge graph queries due to\nits index-free adjacent property, but they are inapplicable to manage a large-scale knowledge graph due to limited storage budgets or\ninﬂexible updating process. Motivated by this, we propose a dual-store structure which leverages a graph store to accelerate the\ncomplex query process in the relational store. However, it is challenging to determine what data to transfer from relational store to\ngraph store at what time. To address this problem, we formulate it as a Markov Decision Process and derive a physical design tuner\nDOTIL based on reinforcement learning. With DOTIL, the dual-store structure is adaptive to dynamic changing workloads.\nExperimental results on real knowledge graphs demonstrate that our proposed dual-store structure improves query performance up to\naverage 43.72% compared with the most commonly used relational stores.\nIndex Terms —Knowledge Graph, Storage Management, Reinforcement Learning, Relational Store, Graph Store.\nF\n1 I NTRODUCTION\nASthe cornerstone of artiﬁcial intelligence, knowledge\ngraphs play an essential role in enhancing the cog-\nnitive ability of machines. A great number of knowledge\ngraphs have been constructed in various domains, such\nas DBpedia [1] integrated by the resources of Wikipedia,\ngeographical knowledge base LinkedGeoData [2], and\nUniProt [3] collected by the information on proteins. To take\nadvantage of knowledge graphs, effective storage manage-\nment is identiﬁed as the basic premise. In real applications,\nit is challenging to store knowledge graphs effectively due\nto the following characteristics of a knowledge graph.\n\u000fLarge scale. A knowledge graph often contains mil-\nlions of vertices and billions of edges. For instance, the\nnumber of triples in DBpedia is greater than 3 billion, there\nare about 3 billion edges in LinkedGeoData, and UniProt\nhas more than 13 billion triples. The large scale makes it\ndifﬁcult to store the entire knowledge graph in a centralized\ngraph database. For example, it is intractable to manage an\nentire knowledge graph in Neo4j database due to its cum-\nbersome importing process [4]. Even if there are few data\nchanges, users have to reload the entire graph and restart the\ndatabase. Also, since the storage constraint of a standalone\ngStore database is 1 billion triples, it is impossible to store\nall the triples of a large-scale knowledge graph [5].\n\u000fComplex Query Patterns. Since the nodes and edges\nof a knowledge graph represent entities and relationships in\nthe real world, a knowledge graph contains various seman-\ntics in different domains. To make full use of the semantic\ninformation, the patterns of knowledge graph queries are\n\u000fZ. Qi is with the School of Computer Science and Technology, Harbin\nInstitute of Technology, P .R. China.\n\u000fH. Wang is with the School of Computer Science and Technology,\nHarbin Institute of Technology, P .R. China. Email: wangzh@hit.edu.cn.\nCorresponding author.\n\u000fH. Zhang is with the School of Software, Harbin Institute of Technology,\nP .R. China.\nManuscript received April 19, 2020; revised August 26, 2020.usually complex. In this paper, complex query patterns refer\nto the query patterns containing more than one predicate.\nSuch as “students who took the same course”, “actors who\nacted in the same movie”, and “people who was born in\nthe same city as his or her advisor”. These complex queries\nbring a heavy burden on relational databases for graph\ndata. To answer them, the relational tables perform multiple\njoin operations and produce expensive time costs, while the\nindex-free adjacent property of graph databases makes the\ntime complexity of graph traversal positively related to the\ntraversal range but irrelevant to the entire graph size [6].\n\u000fChanging Data and Workloads. The frequency of up-\ndating a knowledge graph and its workload becomes higher\nas the number of users increases. This property brings two\nchallenges to knowledge graph stores. On the one hand, the\nincreasing knowledge requires high efﬁciency in inserting\nnew data into the database. On the other hand, in order\nto guarantee the efﬁciency of query process, the storage\nstructure needs to adjust itself according to the dynamic\nworkloads.\nThese properties of knowledge graph data have great\nimpacts on the storage effectiveness and bring the above\nchallenges to the knowledge graph store design. Unfortu-\nnately, existing stores are unable to tackle them. Current\nstorage structures for knowledge graphs are classiﬁed into\ntwo lines, i.e. relation-based stores [7], [8], [9], [10], [11], [12],\n[13] and native graph stores [4], [5]. Even though they are\nefﬁcient for some knowledge graphs, their effectiveness is\nnot satisfactory in many scenarios. Then, we give detailed\ndiscussions.\nRelation-based stores are able to store large-scale knowl-\nedge graphs and insert new triples efﬁciently. However,\nwhen the selectivity of a complex knowledge graph query is\nlarge, relational databases answer the query by scanning the\ntables instead of using indexes, which hurts the query per-\nformance severely. From the aspect of native graph stores,\nthey are efﬁcient to answer complex queries by index-free\nadjacent property [6]. However, they take a long time toarXiv:2012.06966v1  [cs.DB]  13 Dec 2020\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 2\ninsert new edges or vertexes and usually have a storage\nconstraint [14]. In addition, both of these two stores are\nstatic. They cannot adjust themselves automatically with the\nchanging patterns of their workloads.\nMotivated by these, we attempt to design a novel\nknowledge graph storage structure which overcomes the\ndrawbacks of existing stores. We compare the perfor-\nmances of a representative relational database MySQL and\na popular graph database Neo4j by answering a complex\nSPARQL query “SELECT ?p WHERE f?p y:wasBornIn ?city.\n?p y:hasAcademicAdvisor ?a. ?a y:wasBornIn ?city. g”. To\ntest the relationship between query time and data size, we\nvary the number of knowledge graph triples in the two\ndatabases from 500,000 to 5,000,000. As Table 1 shows, as\nthe data size rises, the query time of MySQL becomes longer\nsigniﬁcantly while the time cost of Neo4j grows slowly. We\nobserve that the larger data size of a knowledge graph has\nlittle impact on the complex query performance of Neo4j.\nSince Neo4j is efﬁcient to answer complex queries but has a\nstorage constraint [14], while MySQL is able to store large-\nscale knowledge graphs, the goal of our physical design is\nto leverage the speciﬁc beneﬁts of the two stores.\nTABLE 1\nQuery Performances of MySQL and Neo4j Varying #-triples from\n500000 to 5000000 (Unit: s)\n500000 1000000 1500000 2000000 2500000\nMySQL 11.2304 17.2368 27.6332 37.6454 47.9656\nNeo4j 0.6067 1.3270 1.5837 3.3893 2.2573\n3000000 3500000 4000000 4500000 5000000\nMySQL 62.5006 69.7482 68.8358 68.6312 99.4103\nNeo4j 3.4786 2.7923 3.4560 3.7312 3.9833\nTo achieve this goal, we explore a dual-store structure for\nknowledge graphs, i.e. using the graph database as an ac-\ncelerator for knowledge graph complex queries, while pre-\nserving the above-mentioned roles of both stores. The dual-\nstore structure combines the advantages of relation-based\nand native graph stores by enabling the relational database\nto store the entire data and utilize the graph database for\ncomplex query processing. Naturally, this structure requires\nthe graph database to store the share of data for complex\nqueries answering. However, it is challenging in that the\nknowledge graph workload is changing over time. The\ncrucial problem for it is to determine what data to transfer\nfrom the relational database to the graph database at what\ntime. We refer to this problem as the physical design tuning\nof a dual-store structure.\nWhile physical design tuning is a well-known problem,\nprior researches consider either a single store or a multi-\nstore including relational store, key-value store, document\nstore, HDFS, and NoSQL [15], [16], [17], [18], [19], [20]. The\nexisting techniques could not be applied to the physical\ndesign tuning problem of our dual-store structure. On the\none hand, frequent changes of knowledge graph and its\nworkload require the dual-store tuner to tune the physical\ndesign automatically based on historical experience. How-\never, existing heuristic-based methods lack the ability to\nmemorize effectiveness of different tuning operations, and\nhence cannot accumulate historical experiences to guide fu-\nture physical design tuning. On the other hand, the majorityof current studies treat materialized views or indexes as the\nbasic elements of physical design. However, in our prob-\nlem, the design elements are data partitions in knowledge\ngraphs. Thus, to the best our knowledge, this is the ﬁrst\nwork to tune the dual store with relational database and na-\ntive graph database to cope with large graph management.\nWe treat this problem as a variant of knapsack prob-\nlem [21]. Since the beneﬁt of each data partition is unknown\nand changing, this problem is more difﬁcult than the knap-\nsack problem. Thus, the dual-store physical design tuning\nproblem is at least NP-hard. To solve our problem, we need\nto memorize the previous beneﬁts of data partitions from\nthe historical experiences. Inspired by the memory ability of\nmachine learning, we tune the dual-store structure with the\nhelp of machine learning models.\nSince each tuning operation is affected by the results of\nlast operation, we model the solution of our problem as a\nMarkov Decision Process (MDP for brief). Given the current\nstate of the dual-store structure, the goal is to decide the\nnext action. That is, which data partitions are valuable to\ntransfer from the relational database to the graph database.\nNaturally, we adopt reinforcement learning (RL for brief)\ntechniques to optimize this MDP objective based on the\nobserved performance and previous experience [22], [23],\n[24], [25]. Assuming there are ndata partitions in a knowl-\nedge graph, the state space is 2n. The difﬁculty of RL-based\nsolution is how to train the RL model effectively with a large\nstate space and limited workloads. To tackle this difﬁculty,\nwe incorporates a state space decomposition strategy and\na counterfactual scenario into our proposed RL-based dual-\nstore tuner DOTIL . The decomposition strategy decomposes\nthe state space and increases the retraining frequency of\neach state, and the counterfactual scenario guarantees the\nutilization of each complex query in the workloads.\nIn summary, the main contributions of this paper are\nlisted as follows.\n\u000fWe provide a dual-store structure for knowledge\ngraphs, which has the unique combination of features:\ni) ability to store large-scale knowledge graphs; ii) high-\nefﬁciency in complex query processing and data insertion;\niii) adaptivity to the changing workloads.\n\u000fFor the efﬁciency and adaptivity of the dual-store\nstructure, we study the physical design tuning problem. To\nthe best of our knowledge, this is the ﬁrst work to tune the\nphysical design which consists of relation-based and native\ngraph stores.\n\u000fWe propose DOTIL , a Dual-stOre Tuner based on\nreInforcement Learning, to automatically determine which\ndata partitions to transfer from the relation-based store to\nthe graph store at what time according to the dynamic\nworkloads. DOTIL not only guarantees the adaptivity of our\ndual-store structure, but also soups up the complex query\nprocessing.\n\u000fTo demonstrate the effectiveness of our store, we con-\nduct extensive evaluations on real knowledge graphs. Eval-\nuation results show that the dual-store structure improves\nthe query performance up to average 43.72% compared with\nthe most commonly used relational store, and up to average\n63.01% compared with the relational store optimized by\nmaterialized views.\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 3\nThe rest of the paper is organized as follows. We review\nrelated work in Section 2. Section 3 introduces our proposed\ndual-store structure. We discuss the physical design tuning\nproblem and our reinforcement-learning-based solution in\nSection 4. Section 5 explains the query processor in dual-\nstore structure. Experiments are conducted in Section 6.\nSection 7 concludes this paper.\n2 R ELATED WORK\nThe related work is divided into the following main classes.\nKnowledge Graph Stores. Many storage structures have\nbeen proposed for knowledge graphs and are classiﬁed into\ntwo lines, that are relation-based stores [7], [8], [9], [10], [11],\n[12], [13] and native graph stores [4], [5].\nTriple table is the earliest knowledge graph store [7].\nEach tuple of it stores the subject, predicate, and object of\nan edge in knowledge graphs. This store is convenient for\nusers, but produces a lot of self-join operations during the\nquery process. Horizontal table stores all the predicates and\nobjects of a subject in a tuple [8]. The column number of\nit is the amount of predicates in knowledge graphs, while\nthe row number is the amount of subjects. It generates less\nself-join operators than triple table store, but it has more\ndisadvantages. First, since there are thousands of predicates\nin a large-scale knowledge graph, the amount of predicates\nmay exceed the limit of column number in a relational\ndatabase. Second, different subjects own different predi-\ncates. Thus, there are a lot of null values in the database.\nThird, it is costly to update the knowledge graph in that\nthe table structure changes when inserting, modifying, or\ndeleting a column. To reduce the great column number of\nhorizontal table, property table stores the subjects belonging\nto the same type in a table [9]. Since it produces multi-\ntable joins rather than self-table joins, the query readabil-\nity is improved. However, the table number may exceed\nthe constraint of relational database due to the thousands\nof subjects in a knowledge graph. Moreover, there are\nstill many null values since the predicate set of different\nsubjects are different. Vertical partitioning creates a two-\ncolumn table storing the related subjects and objects for each\npredicate [10]. This store eliminates the null values in the\ntables, but produces a great many of multi-table joins when\nprocessing complex queries, especially the queries without\ngiven predicates. Sextuple indexing is an extension of triple\ntable [11], [12]. It creates six tables for all the permutations\nof triples in a knowledge graph. This store ameliorates the\nbrittleness of self-joins and speeds up the query process, but\nstill has two drawbacks. One is the high costs of storage\nspaces. As the scale of a knowledge graph becomes larger,\nthe storage expenses rise quickly. The other is the large\nnumber of join operations between index tables in the case\nof complex knowledge graph queries. To trade-off the char-\nacteristics of triple table, property table, and vertical par-\ntitioning, DB2RDF creates dph (direct primary hash) table,\nds (direct secondary hash) table, rph (reverse primary hash)\ntable, and rs (reverse secondary hash) table [13]. The dph\ntable stores a subject, its predicates and the corresponding\nobjects in each tuple, the ds table records the object values of\nmulti-value predicates in the dph table, the rph table stores\nan object, its predicates, and the corresponding subjects ineach tuple, and the rs table contains the subject values of\nmulti-value predicates in the rph table. Although this store\naccelerates the knowledge graph query process by reducing\nthe number of join operations and null values, it lacks self-\nadaptivity to the changing workloads.\nThe native graph stores for knowledge graphs are less\nmature than the relation-based stores. Neo4j is the most\npopular property graph database [4]. Due to its index-free\nadjacency, the query performance on it is irrelevant to the\ngraph size, and only related to the range of graph traversal.\nIn spite of the high-efﬁciency in complex query processing,\nNeo4j is costly to store an entire knowledge graph due to its\ncumbersome importing process. When there are changes in\nthe knowledge graph, users need to reload the entire graph\nand restart Neo4j. Another representative graph database\ngStore adopts a bit-string-based storage strategy [5]. It maps\nthe attributes and attribute values of each resource in an\nRDF graph to a binary bit string, and creates a VS* tree with\nthe bit strings. During the execution of SPARQL queries,\nthe VS* tree is used as an index to ﬁnd the query variables.\nEven though the index speeds up the query process, gStore\nis unable to store a large-scale knowledge graph due to its\nstorage limit.\nCompared with existing knowledge graph stores, our\nproposed dual-store structure has two advantages: i) In-\nstead of storing knowledge graphs in a single store, we\nexplore a dual-store structure which combines the strengths\nof both relational-based and native graph stores. Our store\nis not only able to store a large-scale knowledge graph,\nbut also high-efﬁciency in complex query processing and\nknowledge graph updating. ii) Since the query workloads\nare changing over time, the adaptivity to dynamic work-\nloads is an essential property of a knowledge graph store.\nHowever, none of existing literatures considers this prop-\nerty. To ﬁll this gap, our dual-store structure adopts a\nreinforcement-learning-based tuner to adjust the physical\ndesign of our store according to the changing workloads.\nPhysical Design T uning. Majority of the early works\nfocus on physical design tuning for centralized RDBMS\nsystems [15], [16], [17]. The objective is to minimize the\nexecution cost of a dynamically changing workload. New\ndesigns are materialized in parallel with query execution or\nduring a tuning phase. All these works consider a single\ndata store and use indexes as the elements of physical\ndesign. Later, Consens et al. study the tuning problem for\nreplicated databases, where each RDBMS is identical and\nthe workload is provided ofﬂine [18]. LeFevre et al. tune\na multistore system online and utilize opportunistic mate-\nrialized views as the design elements [19]. The multistore\nsystem involves two types of data stores, that are a parallel\nrelational data warehouse and a system for massive data\nstorage and analysis (namely HDFS with Apache Hive).\nBugiotti et al. propose a self-tuning multistore architecture\nincluding NoSQL system, key-value store, document store,\nnested relations store, and relational store. The architecture\nutilizes view-based rewriting and view selection algorithms\nto correctly handle the features of diverse data models\ninvolved [20].\nComparing our tuner DOTIL with existing studies of\nphysical design tuning, we ﬁnd that: i) Since none of prior\nwork studies the physical design tuning of relational and\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 4\ngraph stores, our paper aims to address this problem. ii)\nInstead of relying on heuristics, DOTIL uses the “working\nby doing” mechanism of reinforcement learning to tune the\ndual-store structure for knowledge graphs. By updating its\nmodel continuously and automatically based on delayed\nobservations and historical experience, DOTIL makes our\nstore adaptive to the dynamic workloads.\nSummary. Our study differentiates itself from all these\nstudies in two aspects. On the one hand, none of existing\nliteratures considers a dual-store structure for knowledge\ngraphs. On the other hand, the study of physical design tun-\ning of relation-based and native graph stores is unexplored\nin previous work.\n3 O VERVIEW\nIn this section, we introduce the overview of our proposed\ndual-store structure.\nOur dual-store structure for knowledge graphs is\nsketched in Figure 1. Its design has two goals. One is to\nmake full use of the advantages of both relation-based and\ngraph stores. The other is to adjust itself automatically to the\ndynamically changing workloads.\nRelational stores are able to store large-scale knowledge\ngraphs and convenient in updating knowledge, but as\nshown in Table 1, when the selectivity of a knowledge graph\nquery is large, the query performance weakens obviously.\nNative graph stores are efﬁcient in processing complex\nknowledge graph queries due to its index-free adjacent\nproperty [6], but they are inapplicable to manage a large-\nscale knowledge graph due to their limited storage budgets\nor inﬂexible updating process [14]. Therefore, for the ﬁrst\ngoal, we use a native graph database as the accelerator\nfor complex queries, while storing and updating the entire\nknowledge graph in a relational database.\nSince heuristic tuning methods fail to memorize the\nhistorical experience and tune the physical design automat-\nically, we leverage the memory ability and automation of\nmachine learning to tune the dual-store structure. Thus, to\nachieve the second goal, we incorporate machine learning\ntechniques into dual-store physical design tuning according\nto the dynamic workloads.\nAs Figure 1 shows, in order to accelerate the large-\nselectivity complex queries with native graph stores, we\nadd a complex subquery identiﬁer to judge whether a query\ncontains a complex subquery. If there is a complex subquery\nin the query, the complex subquery identiﬁer marks it. To\nsupport query processing based on current state of dual-\nstore structure, we add a query processor to handle all the\nnew queries including the marked complex queries. In or-\nder to guarantee the self-adaptivity of dual-store structure,\nwe also add a dual-store tuner to learn the most recent\nworkloads and tune the physical design. To achieve this,\nthe complex subquery identiﬁer sends the marked complex\nqueries in the workloads to dual-store tuner. Based on the\naccumulated learning experience of historical workloads,\nthe tuner automatically transfers valuable data from the\nrelational store to the graph store. With the help of these\ncomponents, our dual-store structure not only speeds up\nthe process of complex queries with large selectivity, but\nalso adapts to the frequent changes of data and workloads.\nRelationalStoreGraphStorecomplexsubgraphstransfercomplexsubqueryidentifiernewqueriesdual-storetunerqueryprocessorphysicaldesignworkloads\nentiregraphFig. 1. Dual-Store Structure for Knowledge Graphs\nBased on above discussions, the dual-store structure has\nthree components, complex subquery identiﬁer, dual-store\ntuner, and query processor. In the remaining part of this\nsection, we discuss these three components.\n3.1 Complex Subquery Identiﬁer\nThe goal of this component is to identify the complex sub-\nquery of each query. In this paper, a complex subquery is a\nset of subqueries whose subject variable and object variable\nboth occur more than once in the query. Here, we give an\nexample to explain the complex subquery.\nExample 1. Consider a knowledge graph query q:\nSELECT ?GivenName ?FamilyName WHERE f\nq1: ?p y:hasGivenName ?GivenName.\nq2: ?p y:hasFamilyName ?FamilyName.\nq3: ?p y:wasBornIn ?city.\nq4: ?p y:hasAcademicAdvisor ?a.\nq5: ?a y:wasBornIn ?city.\nq6: ?p y:isMarriedTo ?p2.\nq7: ?p2 y:wasBornIn ?city. g\nSince each of the variables ?p, ?city, ?a, and ?p2 occurs\nmore than once in q, we treat the subquery set of q3,\nq4,q5,q6, andq7as a complex subquery. Its output is\nthe variable which joins it and the remaining part of the\nquery, that is the subquery set of q1andq2. Thus, the\ncomplex subquery qcofqis:\nSELECT ?p WHERE f\nq3: ?p y:wasBornIn ?city.\nq4: ?p y:hasAcademicAdvisor ?a.\nq5: ?a y:wasBornIn ?city.\nq6: ?p y:isMarriedTo ?p2.\nq7: ?p2 y:wasBornIn ?city. g\nIn the complex subquery identiﬁer, when a query q\ncomes, we scan qand extract the subqueries with each\nvariable occurring more than once in q. The set of these\nsubqueries is identiﬁed as the complex subquery qcofq.\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 5\nThe time complexity of complex subquery identiﬁer is O(n),\nwherenis twice the number of subqueries in q.\n3.2 Dual-Store Tuner\nThe aim of this component is to tune the physical design of\ndual-store structure according to the changing workloads.\nThe dual-store tuner is invoked periodically to decide which\ntriple partitions to transfer from the relational store to the\ngraph store. In this paper, triple partition refers to a set of\ntriples whose predicates are identical in a knowledge graph.\nTo determine which triple partitions are worthwhile\ntransferring, we adopt Q-learning as the training model,\nand the historical complex subqueires from complex subquery\nidentiﬁer as the training data, to train dual-store tuner. With\nthe help of Q-learning, our tuner selects triple partitions\nwhich maximize the potential query cost improvements\nbased on actual runtime feedbacks. When triple partitions\nare migrated during tuning, they are stored in the perma-\nnent graph storage and become a part of the physical design\nuntil the next tuning phase. The details of this component\nwill be discussed in Section 4.\n3.3 Query Processor\nGiven the query qand its complex subquery qcidentiﬁed by\ncomplex subquery identiﬁer , if any, as input, this component\naims to process qbased on current state of our store. The\nprocess ofqmay rely on a single store, or span the dual\nstores and utilize the physical design of each store. This\ndepends on the existing complex subgraphs in the native\ngraph store.\nIfqis processed in a single store, the query processor\nforwards it to the appropriate store. If the query process of\nqspans both stores, the query processor ﬁrst sends qcto the\ngraph store. Then, it migrates the intermediate results from\nthe graph store to the relational store. Finally, it resumes\nthe remaining process of qin the relational store. When\nthe intermediate results are migrated, they are stored in the\ntemporary relational table space, and discarded at the end of\nquery process. We will discuss the details of this component\nin Section 5.\n4 D UAL-STORE TUNER\nIn order to accelerate the complex query process, we tune\nthe physical design of our dual-store structure according\nto the dynamic workloads. In this section, we describe the\ndetails of dual-store tuner component shown in Figure 1.\nOur main idea is to transfer the triple partitions which max-\nimize the potential complex query cost improvements of\nfuture workloads from the relational store to the graph store.\nSince it is challenging to decide which triple partitions are\nworthwhile transferring, we model the dual-store physical\ndesign tuning problem as a Markov Decision Process and\ndevelop a reinforcement-learning-based solution to identify\nthe valuable triple partitions and generate the tuning policy.\n4.1 Problem Deﬁnition\nSince the elements of our dual-store physical design tuning\nare triple partitions, we denote the triple partition set inrelational store and graph store as TRandTG, respectively.\nDue to the limited storage capability of a graph store [26],\nits constraint is denoted as BG.\nLetD=hTR,TGibe a dual-store design. Dis a pair\nwhere the ﬁrst component denotes the triple partitions in\nthe relational store, and the second component represents\nthe triple partitions in the graph store.\nWe denote the historical knowledge graph complex\nquery workload as Qc. Since each query qcinQchas a\nlatency, we deﬁne the query costs of Qcwith the given dual-\nstore design Das:\nCost (Qc;D) =X\nqc2QcLatency (qc;D): (1)\nIntuitively, we measure the workload costs as the total\nlatency time of query processing in the workload. This is\na reasonable metric, but other metrics which measure the\ntotal expense of computational resources during the query\nprocess could also be applied such as total throughput.\nBased on the deﬁnitions discussed above, we deﬁne the\ndual-store physical design tuning problem as follows.\nDeﬁnition 1. (Dual-Store Physical Design Tuning Problem.)\nGiven a batch of historical knowledge graph complex\nsubqueries Qc, a dual-store design D=hTR,TGi, and\nthe storage constraint of graph store BG, the dual-store\ndesign problem computes a new dual-store design Dnew\n=hTnew\nR,Tnew\nGithat minimizes the query costs Cost (Qc,\nDnew) and satisﬁes the constraint BG.\nAs deﬁned above, the dual-store physical design tuning\nproblem is a variant of knapsack problem [21]. We can treat\neach triple partition in TRas an item, its transferring beneﬁt\nas the weight of each item, the graph store as the knapsack,\nand its storage constraint BGas the knapsack capacity.\nSince the beneﬁt of each triple partition is unknown and\nchanging, the dual-store physical design tuning problem is\nmore difﬁcult than the knapsack problem. Therefore, the\nphysical design tuning problem of dual-store knowledge\ngraph structure is at least NP-hard.\n4.2 Our solution\nSince we desire a practical solution to the dual-store physical\ndesign tuning problem, we consider the actual situation of\nknowledge graph management. In practice, a knowledge\ngraph manager suspends access to the graph, updates new\nknowledge, reconﬁgures related information, and resumes\nthe access service periodically. The reconﬁguration includes\nphysical design tuning of knowledge graph store. Since\nthe tuning process is invoked periodically, we adjust our\ndual-store structure according to the most recent batch of\nknowledge graph queries, which reﬂects the features of\nfuture workloads best. Moreover, the ofﬂine property makes\nthe complex query process high-efﬁciency without any in-\nﬂuence of physical design tuning.\nMotivated by these, we ﬁrst model the dual-store phys-\nical design tuning problem as a Markov Decision Process,\nand then train a reinforcement-learning-based tuner during\neach period of ofﬂine tuning with the most recent batch of\nworkload as the training data. Assume there are ntriple\npartitions, the state space is 2n. Thus, the new challenge is\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 6\nhow to train the reinforcement learning model effectively\nwith such a large state space and limited workloads. To\ntackle this challenge, we consider a state space decompo-\nsition strategy to increase the retraining frequency of each\nstate. In addition, we incorporate a counterfactual scenario\ninto our tuner to guarantee the utilization of each complex\nquery in the workloads.\n4.2.1 Modeling\nThe goal of our tuning process is to determine which triple\npartitions are worthwhile transferring from the relational\nstore to the graph store. Since each tuning operation is\naffected by the current state of dual-store structure as the\nresults of last operation, we attempt to model our physical\ndesign tuning problem as a Markov Decision Process.\nFirst, we denote the triple partition set in a knowledge\ngraph asT=fT1,T2, ...,Tng, wherenis the number of\npredicates in the knowledge graph. Initially, we store the\nentire setTin the relational store, and the graph store is\nempty. IfTi(1\u0014i\u0014n) is in the graph store, we denote its\nstatus as 1. Otherwise, its status is 0. Therefore, the initial\nstate ofTisf0, 0, ..., 0gand the state space of each Tiis\nf0, 1g. In our tuning process, we decide which Titransfers\nto the graph store at which time. When the storage size\nachieves the constraint of graph store, we determine to evict\nwhichTifrom graph store. The action of transferring or\nevictingTiis denoted as 1, while keeping Tiis represented\nas 0. Thus, the action space of each Tiisf0, 1g. Note that\nwhetherTiis stored in the graph store, it is not evicted\nfrom the relational store due to the efﬁciency of inserting,\ndeleting, or updating knowledge graph data. Therefore, in\nour dual-store design D,TRremains the same unless data\nchanges,TGis the set of Tiwhose value is 1.\nConsider the complex query qcin Example 1.\nWe judge whether the triple partitions correspond-\ning to “y:wasBornIn”, “y:hasAcademicAdvisor”, and\n“y:isMarriedTo”, denoted as T1,T2, andT3, need to transfer\ntogether since qcis able to be processed in the graph store\nonly when all of T1,T2, andT3store there. If we decide\nto transfer them, a reward of this action occurs after qcis\nprocessed. The reward is cost difference of qcbetween in\nthe last state of our dual-store design and in the current\nstate, which is deﬁned as:\nReward (qc) =Cost (qc;D)\u0000Cost (qc;Dnew): (2)\nIn Example 1, “y:wasBorn” accounts for3\n5inqc, while\n“y:hasAcademicAdvisor” and “y:isMarriedTo” each makes\nup1\n5. If we assign the whole reward of qcby the contribu-\ntion proportion of each Ti, the amortized reward of T1is\n3\n5Reward (qc), the reward of T2is1\n5Reward (qc), and that\nofT3is1\n5Reward (qc). Therefore, the reward of qccan be\ndivided into the amortized reward of each Ti.\nDecomposition Strategy. Since the size of state space T\nis 2n, wherenis the amount of predicates in knowledge\ngraph, it requires a large amount of training data. However,\nin practice, the possibility of occurring two identical states\nis very low. In order to achieve a good performance with\nlimited training data, we decompose the state space Tinto a\nset of state subspaces with each subspace in it denoted by Ti.\nThus, the combinations of all subspaces are able to representall the possible states of T. For eachTi, the state space is f0,\n1g, where 0 represents that Tiis not stored in the graph\nstore, 1 denotes Tistores there. The action space is f0, 1g,\nwhere 0 represents keeping Tiin the current state, 1 denotes\ntransferring Tifrom the relational store to the graph store\nor evicting Tifrom the graph store. R(0, 0) represents the\nreward of keeping Tiin the relational store, which is kept 0.\nR(0, 1) denotes the amortized reward of transferring Tifrom\nthe relational store to the graph store. R(1, 0) represents the\naccumulated reward of keeping Tiin the graph store from\nits recent migration. R(1, 1) denotes the reward of evicting\nTifrom the graph store, which is kept 0.\nThe reward of each action reﬂects the beneﬁt or harm\nof keeping, transferring or evicting a triple partition. Our\nobjective is to ﬁnd a policy, that is a sequence of actions,\nto optimize the cumulative reward of workload which is\ndeﬁned as:\nReward (Qc) =X\nqc2QcReward (qc): (3)\nTo summarize, we model the dual-store physical design\ntuning problem as a Markov Decision Process as follows.\nState :f0, 1gfor eachTiinT\nAction :f0, 1gkeep, transfer, or evict Ti\nReward :R(0, 0) andR(1, 0) are the rewards of keeping\nTi,R(0, 1) is the reward of transferring Ti,R(1, 1) is the\nreward of evicting Ti\nPolicy : decision to keep, transfer, or evict Ti\n4.2.2 DOTIL\nNaturally, we adopt reinforcement learning to optimize\nthe Markov Decision Process objective and propose a\nDual-stOre Tuner based on reInforcement Learning, named\nDOTIL . Since the state space in our setting is relative small,\nto make the training process lightweight, we select Q-\nlearning as the reinforcement learning algorithm.\nPolicy Update. Since our dual-store tuner works period-\nically, we train the tuning policy with the most recent batch\nof knowledge graph queries based on Q-learning. The Q-\nfunction is deﬁned as:\nQnew(st;at) (1\u0000\u000b)\u0001Q(st;at)+\u000b\u0001(rt+\r\u0001max\naQ(st+1;a))\n(4)\nwhere max\naQ(st+1;a)is the estimate of optimal future Q-\nvalue,\ris the discount factor, rtis the reward of at time t,\u000b\nis the learning rate, \u000b\u0001(rt+\r\u0001max\naQ(st+1;a))is the learned\nvalue, andQ(st;at)is the old value.\nFor eachTi, there is a 2\u00022 Q-matrix. During the training\nprocess, we update the values, i.e. Q(0, 0),Q(0, 1),Q(1, 0),\nQ(1, 1), in the matrix according to Equation 4. Based on the\ncurrent status and Q values, we decide the next action and\nupdate the tuning policy.\nCounterfactual Scenario. As shown in Equation 2, the\nrewardrtis the cost improvement of querying qcin the\ncurrent state of our dual-store design rather than the last\nstate. If the set of Ticorresponding to qcis transferred to the\ngraph store, rtis the cost improvement of querying qcin the\ngraph store rather than the relational store. However, qcis\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 7\nonly processed in the graph store in our setting. Therefore,\nwe attempt to create a counterfactual scenario which is not\nactually happened to obtain the query cost of qcin the\nrelational store. To achieve this, we add a parallel thread\nto processqcin the relational store and returns its query\ncost to the main thread. Since the query costs of complex\nquery in the relational store is greatly higher than that in\nthe graph store, a threshold \u0015is set to constraint the ratio of\ncost in the relational store to that in the graph store. In this\nway, we obtain the cost improvement of qc, i.e.rt.\nBefore introducing the algorithm of dual-store tuner, we\ndeﬁne several functions used through the paper in Table 2.\nThe pseudo code of DOTIL is shown in Algorithm 1. The\ninput is the current dual-store physical design Dconsisting\nofTRandTG, the storage constraint of graph store BG, the\nmost recent batch of complex subqueries Qc, the current Q-\nmatrix of each Ti, learning rate \u000b, discount factor \r, and\na threshold \u0015. The algorithm produces a new dual-store\nphysical design Dnew.\nWe tune the physical design Dbased on each complex\nsubqueryqcinQc(Lines 1 and 2). Initially, we obtain the\ntriple partition set Tccorresponding to the predicate set Pc\ninqc(Lines 3 and 4). If the triple partitions TGin graph\nstore contain Tc, we train the Q-matrix Qiof eachTiinTc\n(Lines 5-7). If TGdoes not contain Tc, we ﬁrst obtain the\nsetTsetof triple partitions which are in Tcbut not inTG\n(Lines 8-11). Then, we compare the Q-value Q00of keeping\nTsetin relational store and the Q-value Q01of transferring\nTsetfrom relational store to graph store (Lines 12-15). If\nQ01is not greater than Q00, we keepTsetin the relational\nstore (Lines 16 and 17). Otherwise, we judge whether the\nremaining size of graph store is enough to accommodate\nTset. If it is not enough, we sort each triple partition Ti\ninTGaccording to the difference between its Q-value of\nkeeping in graph store and that of evicting from graph store\n(Lines 18-21). We evict Tiin order until the remaining size\nis enough for Tsetand remove TifromTnew\nG (Lines 22-27).\nThen, we migrate Tsetfrom relational store to graph store\nand addTsettoTnew\nG (Lines 28 and 29). Since Tis inTsetare\ntransferred and those in Tcbut not inTsetare kept in the\ngraph store, we train the Q-matrix Qiof them respectively\n(Lines 30 and 31). Finally, we output Dnewconsisting of\nTnew\nR andTnew\nG (Lines 32 and 33).\nThe pseudo code of function LearningProc is shown in\nAlgorithm 2. Next, we explain the Q-learning process of\nDOTIL in detail.\nThe parameters of LearningProc include a query q, a\ntriple partition set T, a states, an action a, learning rate\n\u000b, discount factor \r, and a threshold \u0015. The function trains\nQ-matrix for each triple partition TiinT. First, we process\nqueryqin the graph store and obtain its query cost c1(Line\n1). Meanwhile, in a parallel thread, the query process proc\nofqin relational store is executed and monitored (Line 2).\nIf the execution time of proc achieves\u0015timesc1, we stop\nproc and obtain its current query cost c2(Lines 3-6). For\neach triple partition TiinT, we obtain the proportion of\nits corresponding predicate Piand compute its reward rt\nfor the process of q(Lines 7-10). With the given Q-matrix\nparameters sanda, Q-learning parameters \u000band\r, we\nupdate the corresponding Q-value Qi[s;a]for eachTi(Lines\n11 and 12).Algorithm 1: DOTIL\nInput: current dual-store physcial design D=<TR,\nTG>, storage constraint of graph store BG,\nmost recent batch of complex subqueries Qc,\ncurrentQ-matrix of each Ti, learning rate \u000b,\ndiscount factor \r, threshold\u0015\nOutput: new dual-store physical design\nDnew=<Tnew\nR,Tnew\nG>\n1Tnew\nR TR,Tnew\nG TG;\n2foreachqc2Qcdo\n3Pc qc:getPredicateSet() ;\n4Tc Pc:getPartition() ;\n5 ifTc\u0012TGthen\n6 LearningProc( qc,Tc, 1, 0,\u000b,\r,\u0015);\n7 continue ;\n8 else\n9Tset ;,sum 1 0,sum 2 0 ;\n10 foreachTi2Tc&Ti=2TGdo\n11 Tset:add(Ti)\n12 foreachTi2Tsetdo\n13 Qi Ti:getQmatrix() ;\n14 Q00 sum 1+Qi[0;0];\n15 Q01 sum 2+Qi[0;1];\n16 ifQ00\u0015Q01then\n17 continue ;\n18 ifTset:getSize() +TG:getSize()>BGthen\n19 foreachTi2TGdo\n20 Qi Ti:getQmatrix() ;\n21 TG TG:des sort(Qi[1;1]-Qi[1;0]);\n22 rs Tset .getSize() +TG:getSize() -BG;\n23 foreachTi2TGdo\n24 evict(Ti, graphStore) ;\n25 Tnew\nG Tnew\nGnTi;\n26 ifrs+Ti:getSize()\u00150then\n27 continue ;\n28 migrate(Tset, relStore, graphStore) ;\n29Tnew\nG Tnew\nG:add(Tset);\n30 LearningProc( qc,Tset, 0, 1,\u000b,\r,\u0015);\n31 LearningProc( qc,TcnTset, 1, 0,\u000b,\r,\u0015);\n32Dnew <Tnew\nR;Tnew\nG>;\n33returnDnew;\nThe time complexity of Algorithm 1 is determined by\nthe number of knowledge graph complex queries Qcand\nthe complexity of Algorithm 2. Since the time complexity\nofLearningProc is positively related to the average cost c1\nof processing a complex query qcand the number of triple\npartitionsjTj, the complexity of DOTIL is affected by the\nvalues ofjQcj,c1, andjTj.\nHere we explain two important issues to make our\nproposed DOTIL applicable. First, since there is a cold start\nin Q-learning process, we prefer to warm up DOTIL with\nhistorical queries. In this way, DOTIL will achieve a better\nperformance. Second, the initial Q-matrix Qicorresponding\nto each triple partition Tiis a zero matrix. When the Q-\nvalueQ00of keeping Tsetin relational store and the Q-\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 8\nTABLE 2\nFunction Deﬁnitions\nFunction Deﬁnition\ngetPredicateSet() Given a query q, a graphG, or a triple partition set T, returns the set of predicates contain in q,G, orT.\nrelationQuery( q) Given a query q, denotes processing qin a relational store.\ngraphQuery( q) Given a query q, denotes processing qin a graph store.\ngetResultSet() Given a query plan P, returns the result set of executing P.\nmigrate(S,s1,s2) Given a query result set or a triple partition set S, two data stores s1ands2, denotes migrating Sfroms1tos2.\ngetPartition() Given a predicate set P, returns the corresponding triple partition T.\ngetQmatrix() Given a triple partition Ti, returns the corresponding Q matrix of Ti.\ngetProportion( q) Given a predicate Pi, a queryq, returns the proportion of Piin the predicates contained in q.\nstop(proc) Given a running query process proc , stopsproc .\ngetCost() Given a query process proc , returns the cost of proc .\ngetExectime() Given a running query process proc , returns the current execution time of proc .\ngetSize() Given a triple partition set T, returns the size of T.\ndes sort(obj) Given a triple partition set T, the valueviof an object objrelated to each TiinT, sortsTiaccording to the descending order of vi.\nevict(T,s) Given a triple partition set Tand a data store s, evictTfroms.\nAlgorithm 2: LearningProc( q,T,s,a,\u000b,\r,\u0015)\nInput: queryq, triple partition set T, states, actiona,\nlearning rate \u000b, discount factor \r, threshold\u0015\nOutput: learned Q-matrix\n1c1 graphQuery( q).getCost() ;\n2proc relationQuery( q);\n// parallel thread\n3ifproc: getExectime() =\u0015c1then\n4 stop(proc);\n5c2 \u0015c1;\n6c2 proc: getCost() ;\n7foreachTi2Tdo\n8Pi Ti:getPredicateSet() ;\n9\u000e(Pi) Pi:getProportion( q);\n10rt (c2\u0000c1)\u0002\u000e(Pi);\n11Qi Ti:getQmatrix() ;\n12Qi[s;a] (1\u0000\u000b)\u0002Qi[s;a] +\u000b\u0002(rt+\r\u0002\nmax\naQ(st+1;a));\nvalueQ01of transferring Tsetfrom relational store to graph\nstore are both 0, we set an initial probability of transferring\nprob to decide the ﬁrst action of Ti. Since the value of prob\nis positively related to the number of Q-learning model\ntraining, we prefer to set prob not less than 50%. We will\ndiscuss the detailed tuning of all the parameters in DOTIL\nincludingprob in Section 6.\n5 Q UERY PROCESSOR\nIn this section, we focus on the query processor in the dual-\nstore structure. The challenge of the query processor is how\nto efﬁciently process the given query based on the current\ndual-store state.\nIn the light of this challenge, our main idea is that if\nthe query contains a complex subquery, it is time-saving to\nprocess the subquery in a native graph store rather than\na relational store due to the high cost of complex query\nprocess in relational databases. For example, we convert the\nqueryqin Example 1 into a query graph. As depicted in\nFigure 2, a dual-store execution plan splits a query into two\nparts. The bottom part is its complex subquery qc, which\nis to be executed in a graph store, while the remaining\n?p?city?GivenName?FamilyName?p2?ay:hasFamilyNamey:hasGivenNamey:wasBornIny:isMarriedToy:hasAcademicAdvisory:wasBornIny:wasBornInGraphStoreRelationalStoreFig. 2. An Instance of Query Processing\npart of the query is processed in a relational store. Since\nthe graph store is used as an accelerator, the intermediate\nresults produced by it will be migrated to the relational\nstore for further computation while the ﬁnal query results\nare outputted directly.\nHowever, whether a query qor its complex subquery\nqcis able to be processed in a native graph store depends\non the existing complex subgraphs. Thus, we consider the\nfollowing three conditions.\nCase 1. If the complex subgraphs in graph store cover all\nthe predicates in q, we plan to process qin the graph store.\nCase 2. If the complex subgraphs in graph store do not\ncover all the predicates in qbut cover the predicates in\nqc, we plan to process qcin the graph store, migrate the\nintermediate results from the graph store to the relational\nstore, and ﬁnish the remaining part of qin the relational\nstore.\nCase 3. If the complex subgraphs in graph store do\nnot cover the predicates in qc, we plan to process qin the\nrelational store.\nThe pseudo code of query processor is shown in Algo-\nrithm 3. The input is a query q, its complex subquery qc,\nand current complex subgraphs Gcin the graph store. The\nalgorithm processes the query q. We ﬁrst judge whether q\ncontains a complex subquery qc. If not, we plan to process q\nin the relational store (Lines 1 and 2). If qcexists, we obtain\nthe predicate sets of q,qc, andGc, respectively (Lines 3-\n6). If the predicates in Gccover the ones in q, we plan to\nprocessqin the graph store (Lines 7 and 8). Otherwise, if the\npredicates in qcare covered by the ones in Gc, we plan to\nprocessqcin the graph store, migrate its result set from the\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 9\ngraph store to the relational store, and process the remaining\npart ofqin the relational store (Lines 9-14). If the predicates\ninqcare not contained in the predicate set of Gc, we process\nqin the relational store (Lines 15 and 16).\nAlgorithm 3: Query Processor\nInput: queryq, complex subquery qcofq, current\ncomplex subgraphs Gcin graph store\n1ifqcis null then\n2 relationQuery( q);\n3else\n4Pq q:getPredicateSet() ;\n5Pqc qc:getPredicateSet() ;\n6PGc Gc:getPredicateSet() ;\n7 ifPq\u001aPGcthen\n8 graphQuery( q);\n9 else\n10 ifPqc\u001aPGcthen\n11 res graphQuery( qc).getResultSet() ;\n12 graphQuery( qc);\n13 migrate(res, graphStore, relStore);\n14 relationQuery( qnqc)i;\n15 else\n16 relationQuery( q);\nClearly, the time complexity of Algorithm 3 is O(m),\nwheremis the edge number of complex subgraphs in the\ngraph store. In practice, mis about 20% to 30% of the edge\nnumber of an entire knowledge graph. Thus, the query\nprocessor actually takes little overhead.\n6 E VALUATION\nIn this section, we discuss our experimental evaluation of\nour proposed knowledge-graph dual-store structure. First,\nwe describe our experimental methodology in Section 6.1.\nThen in Section 6.2, we compare our dual-store structure\nwith other store variants to highlight the beneﬁts of our\nstorage design in processing complex knowledge graph\nqueries. In Section 6.3, we test the performance results of our\ndual-store tuner DOTIL . Lastly, we compare the behavior of\nDOTIL with other tuning algorithms in Section 6.4.\n6.1 Methodology\nIn this section, we describe knowledge graphs, workloads,\nmetrics, and setup used in our experiments.\nKnowledge Graphs and Workloads. We use three pub-\nlic knowledge graphs, YAGO [27], WatDiv [28], and\nBio2RDF [29]. Knowledge graph information is shown in\nTable 3. YAGO contains all the triples in YagoFacts and the\ntriples whose predicate is “hasGivenName” or “hasFami-\nlyName”. We generate WatDiv by setting 14,634,621 as the\nnumber of triples. Bio2RDF consists of the knowledge from\nInteraction Reference Index, Online Mendelian Inheritance\nin Man, Pharmacogenomics Knowledge Base, and PubMed.\nWe use the query templates of YAGO and Bio2RDF pro-\nvided in [30]. The query templates of WatDiv are com-\nposed of linear queries (WatDiv-L), star queries (WatDiv-\nS), snowﬂake-shaped queries (WatDiv-F), and complexqueries (WatDiv-C) [31]. The query workloads of YAGO\n(20 queries), WatDiv-L (35 queries), WatDiv-S (25 queries),\nWatDiv-F (25 queries), WatDiv-C (15 queries), and Bio2RDF\n(25 queries) consist of the original query templates and four\nmutations for each query. Each workload has an ordered\nversion and a random version. In the ordered version,\neach query template and its mutations are clustered. In the\nrandom version, we randomize all the query templates and\ntheir mutations in the workload. In this paper, we set each\nbatch of queries as1\n5knowledge graph query workloads.\nTABLE 3\nKnowledge Graph Information\nname size triples #-S[O #-P #-queries\nYAGO 679.3M 16418085 5593541 39 20\nWatDiv 2.08G 14634621 1396039 86 100\nBio2RDF 7.64G 60241165 8914390 161 25\nMetrics. Our primary metric is time-to-insight, named\nTTI [32], which is the total elapsed time from a batch of\nworkload submission to completion. TTIevaluates the costs\nof our online query process. Also, we use the sum of Q-\nmatrix of each triple partition to measure the ofﬂine training\neffect. The effectiveness of DOTIL is positively related to the\nvalues in Q-matrix.\nSetup. All experiments are conducted on a server with\n32\u0002Intel Xeon(R) Gold 6151 CPU@3.00GHz, 128G memory,\nand a 148G hard disk, under Ubuntu 18.04.1 LTS. The\nrelational database used in this work is MySQL 5.7.30,\nand the graph database is Neo4j 3.5.14. All algorithms are\nimplemented in Python.\n6.2 Comparisons of Store Variants\nTo explore the effectiveness of our proposed dual-store\nstructure, we ﬁrst compare our store RDB-GDB with two\nstore variants, RDB-only and RDB-views .RDB-only stores\nand queries knowledge graphs in a relational database. To\nspeed up the knowledge graph query process in a rela-\ntional database, RDB-views creates the intermediate results\nof most frequent subqueries in the historical workloads as\nmaterialized views during the ofﬂine phase after processing\na batch of queries. RDB-GDB leverages a graph store to\naccelerate the complex subquery process in a relational\ndatabase and adopts DOTIL to tune the physical design of\ndual-store structure periodically. For a fair comparison, the\nstorage budgets for views in RDB-views and graph database\ninRDB-GDB are equal. To warm up the views in RDB-\nviews and the graph database in RDB-GDB , we run each\ntest 6 times and record the average TTI of last 5 times. The\ncomparison results of each batch on ordered workloads and\nrandom workloads are depicted in Figure 3 and Figure 4,\nrespectively. The total results of each workload are shown\nin Figure 5.\nAs depicted in Figure 3 and Figure 4, RDB-GDB achieves\nbetter efﬁciency than RDB-only andRDB-views in all cases.\nThe reason is that with the help of periodic physical design\ntuning, the triple partitions in the graph store of RDB-\nGDB effectively improve the complex subquery costs in\nRDB-only . Also, due to the storage constraints, the triple\npartitions selected by Q-learning model in RDB-GDB are\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 10\nmore valuable than the views simply chosen based on\nfrequency in RDB-views . Additionally, it can be observed\nthat TTI ofRDB-views is sometimes higher than that of\nRDB-only . This is due to the expensive costs of looking\nup the appropriate views and joining view-table and other\nrelational tables to generate ﬁnal results. We also observe\nthat TTI ofRDB-GDB is more stable than that of RDB-only\nand RDB-views . This is because that the dual-store tuner\nDOTIL inRDB-GDB cumulates the historical experience of\ntuning valuable triple partitions. As the number of train-\ning knowledge graph queries increases, the performance\nofDOTIL becomes better. From Figure 5, we observe that\nit makes little difference to the total TTI ofRDB-GDB on\nrandom and ordered workloads. The reason is that the\nadaptivity of DOTIL to dynamic workloads is insusceptible\nto the query order. Compared with the most commonly used\nknowledge graph store RDB-only ,RDB-GDB improves the\nquery performance up to average 43.72%. Compared with\nRDB-views ,RDB-GDB improves the query performance up\nto average 63.01%. This further demonstrates the limited\neffectiveness of relational views in accelerating complex\nquery process.\nThus, we can conclude that our proposed dual-store\nstructure achieves better performance in processing com-\nplex knowledge graph queries.\n6.3 Performance of DOTIL\nTo verify the performance of our proposed physical design\ntuner in the dual-store structure, we test DOTIL along three\ndimensions: ( i) tuning parameters; ( ii) cold start of graph\nstore; (iii) utilizing graph store with limited spare resources.\n6.3.1 Tuning Parameters\nInDOTIL , there are ﬁve parameters, i.e. rBG(the ratio of\nstorage constraint BGto an entire knowledge graph), prob\n(the initial probability of transferring), \u000b(learning rate), \r\n(discount factor), and \u0015(the cost ratio in relational store\nto graph store). To determine the optimal values for these\nparameters, we measure TTI and Q-matrix on half of the\nrandom version of YAGO workload varying each parameter.\nWhen we vary one parameter, the other parameters are set\nto their default values in Table 4. Experimental results are\nshown in Table 5.\nTABLE 4\nDefault Values\nparameter default value\nrBG25%\nprob 50%\n\u000b 0.5\n\r 0.5\n\u0015 3.5\nAs shown in Table 5, we vary rBGfrom 20% to 40%.\nIt can be observed that when rBGis 25%, TTI is the least\nand the values in Q-matrix are the highest. When rBGis\nless than 25%, it is difﬁcult for graph store to accommodate\ndesired triple partitions and leads to poor training results.\nWhenrBGis larger than 25%, it is infrequent to evict the\ntriple partitions whose Q values are low, hence the Q-matrix\nvalues decrease. Since the value of prob is positively related\n(a) ordered YAGO\n (b) ordered WatDiv-L\n(c) ordered WatDiv-S\n (d) ordered WatDiv-F\n(e) ordered WatDiv-C\n (f) ordered Bio2RDF\nFig. 3. Results of Each Batch Varying Store on Ordered Workloads\nto the number of Q-learning model training, we prefer to\nsetprob not less than 50%. Therefore, the value of prob is\nvaried from 50% to 100%. We observe that TTI ﬂuctuates\nslightly because TTI is unaffected by the value of prob .\nWhenprob is 90%, the values in Q-matrix are the highest.\nWhenprob is less than 90%, the number of model training\ndecreases, and hence the Q-matrix values become less. We\nvary\u000bfrom 0.3 to 0.7, and see that when \u000bis 0.5, TTI is\nthe least and the values in Q-matrix are the highest. When\n\u000bis less than 0.5, DOTIL reduces the learning ability of new\nqueries, and the training effects get worse. When \u000bis more\nthan 0.5, DOTIL weakens its memory ability of historical\nexperience and deteriorates the training results. The value of\n\ris varied from 0.5 to 0.9, and we observe that when \ris 0.7,\nTTI is the least and the values in Q-matrix are the highest.\nWhen\ris less than 0.7, DOTIL overlooks the importance of\nprevious experience and worsens the training results. When\n\ris larger than 0.7, DOTIL undervalues the instant reward\nwhich affects the Q values. Also, we vary \u0015from 3.0 to 5.0,\nand see that when \u0015is 4.5, TTI is the least, and the values\nin Q-matrix are the highest. When \u0015is less than 4.5, the\ndifference between the cost in the graph store and that in\nrelational store is diminished, and hence the reduced reward\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 11\n(a) random YAGO\n (b) random WatDiv-L\n(c) random WatDiv-S\n (d) random WatDiv-F\n(e) random WatDiv-C\n (f) random Bio2RDF\nFig. 4. Results of Each Batch Varying Store on Random Workloads\nvalue affects the Q-matrix values. When \u0015is more than 4.5,\nthe training time of Q-learning model is too long to tune the\nphysical design of dual-store structure efﬁciently.\nThus, to achieve high efﬁciency and well training perfor-\nmance of DOTIL , we set 25% as the value of rBG, 90% as the\nvalue ofprob , 0.5 as the value of \u000b, 0.7 as the value of \r, and\n4.5 as the value of \u0015.\n6.3.2 Cold Start of Graph Store\nIn the initial state of dual-store structure, the graph store\nis empty. As DOTIL tunes the dual-store physical design,\nthe triple partitions in the graph store gradually increase.\nThus, there is a cold start of graph store, and its inﬂuence\non the overall performance needs to be tested. To test this\ninﬂuence, we measure TTI of both the cost in the graph\nstore and the total cost from the initial state of the graph\nstore. Experimental results are depicted in Figure 6.\nFrom Figure 6, we observe that although in the ﬁrst two\nbatches, the cost proportion of the graph store is small, it\nrises rapidly from the third batch. This is due to the fact\nthat DOTIL transfers valuable triple partitions to the graph\nstore in a short time, and hence the increased new queries\nare processed in the graph store. Thus, we conclude that the\n(a) YAGO workloads\n (b) ordered WatDiv workloads\n(c) random WatDiv workloads\n (d) Bio2RDF workloads\nFig. 5. Total Results of Each Workload Varying Store\nTABLE 5\nResults of Parameter Tuning\nparameter value TTI Q-matrix\n20% 684.0760 [0, 3.8263, 13.7780, 0]\n25% 653.4101 [0, 5.2717, 13.7956, 0]\nrBG30% 674.3867 [0, 3.8146, 9.8068, 0]\n35% 700.0032 [0, 6.5456, 6.3852, 0]\n40% 674.8967 [0, 3.4930, 11.7358, 0]\n50% 575.0997 [0, 2.1057, 12.4025, 0]\n60% 598.9008 [0, 7.1411, 16.0713, 0]\nprob 70% 598.4994 [0, 9.7398, 13.9126, 0]\n80% 606.2928 [0, 5.2241, 20.9603, 0]\n90% 607.7997 [0, 12.2230, 23.6434, 0]\n100% 609.6003 [0, 7.0906, 18.3338, 0]\n0.3 701.4354 [0, 4.7951, 9.0349, 0]\n0.4 663.7754 [0, 4.2571, 16.0011, 0]\n\u000b 0.5 654.8847 [0, 5.2816, 15.1309, 0]\n0.6 748.5279 [0, 1.9856, 8.3675, 0]\n0.7 702.1908 [0, 0.0826, 1.4354, 0]\n0.5 645.9246 [0, 6.5384, 10.3453, 0]\n0.6 695.1923 [0, 1.6490, 12.5269, 0]\n\r 0.7 598.7205 [0, 7.4015, 12.9472, 0]\n0.8 612.1578 [0, 9.3671, 9.7828, 0]\n0.9 685.9546 [0, 7.5959, 12.3609, 0]\n3.0 697.7290 [0, 3.2530, 12.5419, 0]\n3.5 604.4655 [0, 4.7346, 18.8977, 0]\n\u0015 4.0 608.4625 [0, 5.0882, 22.0954, 0]\n4.5 608.4121 [0, 5.6235, 25.1015, 0]\n5.0 605.9493 [0, 6.5287, 22.0688, 0]\ncold start of graph store has little impact on the performance\nof our dual-store structure.\n6.3.3 Utilizing Graph Store with Limited Space Resources\nIn the process of DOTIL , there is a parallel thread to com-\npute the query execution time in the relational store. In the\nmeantime, we process queries in the graph store. Since the\nresources are partly used by the parallel thread, we study\nthe impacts of limited spare resources on utilizing graph\nstore. We ﬁrst test the slowdown of graph store with 40%\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 12\n(a) ordered YAGO workloads\n (b) random YAGO workloads\nFig. 6. Cost Proportion of Graph Store\nIO, 20% IO, 40% CPU, and 20% CPU, and report the results\nin Table 6. Then, with 40% IO resource, we measure the\ntime-varying percent of IO and CPU consumed in the graph\nstore. Experimental results are depicted in Figure 7.\nAs shown in Table 6, it can be observed that the slow-\ndown percent of the graph store is little in spite of few\nspare resources. From Figure 7, we can also see that the\npercent of consumed IO or CPU ﬂuctuates widely in the\nbeginning, and it then stabilizes at a small value. Therefore,\nthe inﬂuence of limited resources on graph store utilization\nis little.\nTABLE 6\nSlowdown of Graph Store with Limited Spare Resources\nspare resource slowdown percent\nIO 40% 0.45%\nIO 20% 0.30%\nCPU 40% 5%\nCPU 20% 18%\n(a) IO consumed of graph store\n (b) CPU consumed of graph store\nFig. 7. Impacts of Parallel Thread on Resources Consumed of Graph\nStore\n6.4 Comparisons of Tuners\nTo evaluate the effectiveness of our proposed physical de-\nsign tuner DOTIL , we compare DOTIL with other three\ntuning methods, one-off mode ,LRU policy , and ideal mode .\nOne-off mode foresees the whole future workload and tunes\nthe dual-store structure once at the beginning time. LRU\npolicy transfers the most frequent triple partitions in the\nhistorical workloads to the graph store after each batch.\nIdeal mode foresees the workload in next batch and tunes\nthe dual-store structure beforehand. Note that ideal modeis the ideal situation of DOTIL , we evaluate the difference\nbetween DOTIL and its ideal case. To warm up the graph\ndatabase, we test TTI 6 times and report the average of last\n5 times. The comparison results are illustrated in Figure 8.\nFrom Figure 8, we observe that TTI ofDOTIL is signif-\nicantly less than that of one-off mode andLRU policy . The\nreason is that one-off mode keeps the limited valuable triple\npartitions in the graph store due to the storage constraint.\nItsstatic property makes it lack the adaptivity to dynamic\nworkloads. The most frequent triple partitions chosen by\nLRU policy may not bring most beneﬁts, and hence leads to\nhigh time costs. Additionally, it can be seen that on ordered\nworkloads, the difference between DOTIL and ideal mode\nis less than that on random workloads. This is because\nthat mutations of a query template are clustered in ordered\nworkloads, and DOTIL is more adaptive to similar queries\nthan irrelevant ones.\nThe evaluation results conﬁrm the high effectiveness of\nDOTIL . With the dynamic physical design tuning of DOTIL ,\nthe dual-store structure achieves satisfactory performances.\n(a) YAGO workloads\n (b) ordered WatDiv workloads\n(c) random WatDiv workloads\n (d) Bio2RDF workloads\nFig. 8. Comparison Results with Other Tuners\n7 C ONCLUSION\nIn this paper, we propose a novel dual-store structure for\nknowledge graphs. We leverage a graph store to acceler-\nate complex knowledge graph query process in traditional\nrelational stores. To determine the triple partitions which\nare valuable to transfer from relational store to graph store,\nwe derive a reinforcement-learning-based tuner DOTIL .\nWith DOTIL , the dual-store structure adapts to the dy-\nnamic changing workloads. Our experimental results on real\nknowledge graphs verify the effectiveness and efﬁciency of\nour proposed dual-store structure. In the future, we plan\nto extend the storage structure so that it can be applied to\nvarious challenging scenarios.\n\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2020 13\nACKNOWLEDGMENTS\nThis paper was partially supported by NSFC grant\nU1866602, 61602129, 61772157, CCF-Huawei Database Sys-\ntem Innovation Research Plan DBIR2019005B and Microsoft\nResearch Asia.\nREFERENCES\n[1] “Dbpedia,” https://wiki.dbpedia.org/about.\n[2] “Linkedgeodata,” http://www.linkedgeodata.org/About.\n[3] “Uniprot,” https://www.uniprot.org/help/about.\n[4] “Neo4j,” https://neo4j.com/docs/developer-manual/current/.\n[5] L. Zou, M. T. ¨Ozsu, L. Chen, X. Shen, R. Huang, and D. Zhao,\n“gstore: a graph-based sparql query engine,” The VLDB journal ,\nvol. 23, no. 4, pp. 565–590, 2014.\n[6] M. A. Rodriguez and P . Neubauer, “The graph traversal pattern,”\nGraph Data Management Techniques & Applications , 2012.\n[7] S. Harris and G. Nicholas, “3store: Efﬁcient bulk rdf storage,” in\nProceedings of the 1st International Workshop on Practical and Scalable\nSemantic Systems , 2004, pp. 81–95.\n[8] Z. Pan and J. Heﬂin, “Dldb: Extending relational databases to sup-\nport semantic web queries,” in Proceedings of the 1st International\nWorkshop on Practical and Scalable Semantic Systems , 2004, pp. 109–\n113.\n[9] K. Wilkinson, “Jena property table implementation,” in Proceedings\nof the 2nd International Workshop on Scalable Semantic Web Knowledge\nBase Systems , 2006, pp. 35–46.\n[10] D. J. Abadi, A. Marcus, S. R. Madden, and K. Hollenbach, “Sw-\nstore: a vertically partitioned dbms for semantic web data man-\nagement,” The VLDB Journal , vol. 18, no. 2, pp. 385–406, 2009.\n[11] T. Neumann and G. Weikum, “Rdf-3x: a risc-style engine for rdf,”\nProceedings of the VLDB Endowment , vol. 1, no. 1, pp. 647–659, 2008.\n[12] C. Weiss, P . Karras, and A. Bernstein, “Hexastore: sextuple index-\ning for semantic web data management,” Proceedings of the VLDB\nEndowment , vol. 1, no. 1, pp. 1008–1019, 2008.\n[13] W. Sun, A. Fokoue, K. Srinivas, A. Kementsietsidis, G. Hu, and\nG. Xie, “Sqlgraph: An efﬁcient relational-based property graph\nstore,” in Proceedings of the 2015 ACM SIGMOD International Con-\nference on Management of Data , 2015, pp. 1887–1901.\n[14] J. Pokorn ´y, M. Valenta, and J. Kovacic, “Integrity constraints in\ngraph databases,” Procedia Computer Science , vol. 109, pp. 975–981,\n2017.\n[15] N. Bruno and S. Chaudhuri, “An online approach to physical\ndesign tuning,” in 2007 IEEE 23rd International Conference on Data\nEngineering . IEEE, 2007, pp. 826–835.\n[16] K. Schnaitter, S. Abiteboul, T. Milo, and N. Polyzotis, “On-line\nindex selection for shifting workloads,” in 2007 IEEE 23rd Inter-\nnational Conference on Data Engineering Workshop . IEEE, 2007, pp.\n459–468.\n[17] K. Schnaitter and N. Polyzotis, “Semi-automatic index tuning:\nKeeping dbas in the loop,” Proceedings of the VLDB Endowment ,\nvol. 5, no. 5, 2012.\n[18] M. P . Consens, K. Ioannidou, J. LeFevre, and N. Polyzotis, “Di-\nvergent physical design tuning for replicated databases,” in Pro-\nceedings of the 2012 ACM SIGMOD International Conference on\nManagement of Data , 2012, pp. 49–60.\n[19] J. LeFevre, J. Sankaranarayanan, H. Hacigumus, J. Tatemura,\nN. Polyzotis, and M. J. Carey, “Miso: souping up big data query\nprocessing with a multistore system,” in Proceedings of the 2014\nACM SIGMOD international conference on Management of data , 2014,\npp. 1591–1602.\n[20] F. Bugiotti, D. Bursztyn, A. Deutsch, I. Ileana, and I. Manolescu,\n“Invisible glue: Scalable self-tuning multi-stores,” in Conference on\nInnovative Data Systems Research (CIDR) , 2015.\n[21] E. Horowitz and S. Sahni, “Computing partitions with applica-\ntions to the knapsack problem,” Journal of the ACM , vol. 21, no. 2,\npp. 277–292, 1974.\n[22] D. Van Aken, A. Pavlo, G. J. Gordon, and B. Zhang, “Automatic\ndatabase management system tuning through large-scale machine\nlearning,” in Proceedings of the 2017 ACM International Conference\non Management of Data , 2017, pp. 1009–1024.\n[23] K. Kara, K. Eguro, C. Zhang, and G. Alonso, “Columnml: Column-\nstore machine learning with on-the-ﬂy data transformation,” Pro-\nceedings of the VLDB Endowment , vol. 12, no. 4, pp. 348–361, 2018.[24] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data , 2018, pp. 489–504.\n[25] B. Ding, S. Das, R. Marcus, W. Wu, S. Chaudhuri, and V . R.\nNarasayya, “Ai meets ai: Leveraging query executions to improve\nindex recommendations,” in Proceedings of the 2019 International\nConference on Management of Data , 2019, pp. 1241–1258.\n[26] A. Ghrab, O. Romero, S. Skhiri, A. A. Vaisman, and E. Zimanyi,\n“Grad: On graph database modeling,” arXiv: Databases , 2016.\n[27] “Yago,” https://www.mpi-inf.mpg.de/departments/\ndatabases-and-information-systems/research/yago-naga/yago/.\n[28] “Watdiv,” https://dsg.uwaterloo.ca/watdiv/.\n[29] “Bio2rdf,” https://download.bio2rdf.org/.\n[30] R. Harbi, I. Abdelaziz, P . Kalnis, N. Mamoulis, Y. Ebrahim, and\nM. Sahli, “Accelerating sparql queries by exploiting hash-based\nlocality and adaptive partitioning,” The VLDB Journal , vol. 25,\nno. 3, pp. 355–380, 2016.\n[31] “Watdiv query templates,” https://dsg.uwaterloo.ca/watdiv/\nbasic-testing.shtml.\n[32] M. S. Hopkins, “Big data, analytics and the path from insights to\nvalue,” MIT Sloan Management Review , vol. 52, no. 2, pp. 21–22,\n2011.",
  "textLength": 68041
}