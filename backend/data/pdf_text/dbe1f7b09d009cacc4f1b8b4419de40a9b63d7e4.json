{
  "paperId": "dbe1f7b09d009cacc4f1b8b4419de40a9b63d7e4",
  "title": "Associative Memories to Accelerate Approximate Nearest Neighbor Search",
  "pdfPath": "dbe1f7b09d009cacc4f1b8b4419de40a9b63d7e4.pdf",
  "text": "applied  \nsciences \nArticle\nAssociative Memories to Accelerate Approximate\nNearest Neighbor Search\nVincent Gripon1,*ID, Matthias Löwe2and Franck Vermet3\n1Electronics Department, IMT Atlantique, 29200 Brest, France\n2Fachbereich Mathematik und Informatik, University of Münster, Einsteinstraße 62, 48149 Münster, Germany;\nmatthias.loewe@uni-muenster.de\n3Laboratoire de Mathématiques, Université de Bretagne Occidentale, UMR CNRS 6205, 29200 Brest, France;\nfranck.vermet@univ-brest.fr\n*Correspondence: vincent.gripon@imt-atlantique.fr\nReceived: 22 August 2018; Accepted: 14 September 2018; Published: 16 September 2018\n/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001\n/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046\nAbstract: Nearest neighbor search is a very active ﬁeld in machine learning. It appears in\nmany application cases, including classiﬁcation and object retrieval. In its naive implementation,\nthe complexity of the search is linear in the product of the dimension and the cardinality of the\ncollection of vectors into which the search is performed. Recently, many works have focused on\nreducing the dimension of vectors using quantization techniques or hashing, while providing an\napproximate result. In this paper, we focus instead on tackling the cardinality of the collection of\nvectors. Namely, we introduce a technique that partitions the collection of vectors and stores each\npart in its own associative memory. When a query vector is given to the system, associative memories\nare polled to identify which one contains the closest match. Then, an exhaustive search is conducted\nonly on the part of vectors stored in the selected associative memory. We study the effectiveness\nof the system when messages to store are generated from i.i.d. uniform \u00061 random variables or\n0–1 sparse i.i.d. random variables. We also conduct experiments on both synthetic data and real data\nand show that it is possible to achieve interesting trade-offs between complexity and accuracy.\nKeywords: similarity search; vector search; associative memory; exponential inequalities\n1. Introduction\nNearest neighbor search is a fundamental problem in computer science that consists of ﬁnding\nthe data point in a previously given collection that is the closest to some query data point, according to\na speciﬁc metric or similarity measure. A naive implementation of nearest neighbor search is to\nperform an exhaustive computation of distances between data points and the query point, resulting in\na complexity linear in the product of the cardinality and the dimension of the data points.\nNearest neighbor search is found in a wide set of applications, including object retrieval,\nclassiﬁcation, computational statistics, pattern recognition, vision, data mining, and many more.\nIn many of these domains, collections and dimensions are large, leading to unreasonable computation\ntime when using exhaustive search. For this reason, many recent works have been focusing on\napproximate nearest neighbor search, where complexities can be greatly reduced at the cost of a nonzero\nerror probability in retrieving the closest match. Most methods act either on the cardinality [ 1–3] or\non the dimension [ 4–6] of the set of data points. Reducing cardinality often requires to be able to\npartition space in order to identify interesting regions that are likely to contain the nearest neighbor,\nthus reducing the number of distances to compute, whereas reducing dimensionality is often achieved\nthrough quantization techniques, and thus approximate distance computation.\nAppl. Sci. 2018 ,8, 1676; doi:10.3390/app8091676 www.mdpi.com/journal/applsci\n\nAppl. Sci. 2018 ,8, 1676 2 of 21\nIn this note, we will present and investigate an alternative approach that was introduced in [ 7]\nin the context of sparse data. Its key idea is to partition the patterns into equal-sized classes and to\ncompute the overlap of an entire class with the given query vector using associative memories based\non neural networks. If this overlap of a given class is above a certain threshold, we decide that the\ngiven vector is similar to one of the vectors in the considered class, and we perform exhaustive search\non this class, only. Obviously, this is a probabilistic method that comes with a certain probability\nof failure. We will impose two restrictions on the relation between dimension and the size of the\ndatabase, or equivalently, between dimension and the number of vectors in each class: on the one\nhand, we want the size of the database to be so large that the algorithm is efﬁcient compared to other\nmethods (primarily exhaustive search), on the other hand, we need it to be small enough, to let the\nerror probability converge to zero, as the dimension (and possibly the size of the database) converges\nto inﬁnity.\nMore precisely, in this note, we will consider two principally different scenarios: one where the\ninput patterns are sparse. Here, one should think of the data points as sequences of signals of a certain\nlength, where only very rarely a signal is sent. This will be modeled by a sequences of zeroes and ones\nwith a vast majority of zeros and another scenario. The other situation will be a scenario of “dense”\npatterns, where about half of the times there is a signal. We will in this case assume that the data points\nare “unbiased”. This situation is best modeled by choosing the coordinates of all the input vector\nas independent and identically distributed (i.i.d.) \u00061-random variables with equal probabilities for\n+1and for\u00001. The advantage of these models is that they are accessible to a rigorous mathematical\ntreatment and pretty close to the more realistic situations studied in Section 5.\nEven though the principal approach is similar (we have to bound the probability of an error),\nboth cases have their individual difﬁculties. As a matter of fact, similar problems occur when analyzing\nthe Hopﬁeld model [ 8] of neural networks for unbiased patterns (see, e.g., [ 9–11]) or for sparse,\nbiased patterns (see, e.g., [ 12,13]). On a technical level, the bounds on the error probabilities in\nthe theoretical part of the paper (Sections 3 and 4) are obtained by a union bound and further\non by an application of an exponential Chebyshev inequality. This is one of the most general\nways to obtain upper bounds on probabilities and therefore is widely used. However, in general,\nthe challenge is always to obtain appropriate estimates for the moment generating functions in\nquestion (examples are [ 10,14,15]). Here, we use a Lindeberg principle that is new in this context and\nallows us to replace our random variables with Gaussian ones, Taylor expansion techniques for the\nexponents, as well as Gaussian integration to obtain the desired bounds. We will analyze the situation\nof sparse patterns in Section 3, while Section 4 is devoted to the investigation of the situation where\nthe patterns are unbiased and dense. In Section 5, we will support our ﬁndings with simulations\non classical approximate nearest neighbor search benchmarks. These will also reveal the interplay\nbetween the various parameters in our model and conﬁrm the theoretical predictions of Sections 3\nand 4. The simulations on real data also give ideas for further research directions. In Section 6 we\nsummarize our results.\n2. Related Work\nPart of the literature focuses on using tree-based methods to partition space [ 2,16,17], resulting in\ndifﬁculties when facing high dimension spaces [ 1]. When facing real-valued vectors, many techniques\nproposed binary encoding of vectors [ 3,18,19], leading to very efﬁcient reductions in time complexity.\nAs a matter of fact, the search in Hamming space can be performed very efﬁciently, but it is well known\nthat premature discretization of data usually leads to signiﬁcant loss in precision.\nAnother line of work is to rely on quantization techniques based on Product Quantization (PQ)\nor Locality Sensitive Hashing (LSH). In PQ [ 4], vectors were split into subvectors. Each subvector\nspace is quantized independently of the others. Then, a vector is quantized by concatenating the\nquantized versions of its subvectors. Optimized versions of PQ have been proposed by performing\njoint optimization [ 20,21]. In LSH, multiple hash functions were deﬁned, resulting in storing each\n\nAppl. Sci. 2018 ,8, 1676 3 of 21\npoint index multiple times [ 5,22–24]. Very recently, the authors have proposed [ 25] to learn indexes\nusing machine learning methods, resulting in promising performance.\nIn [7], the authors proposed to split data points into multiple parts, each one stored in its own\nsparse associative memory. The ability of these memories to store a large number of messages\nresulted in very efﬁcient reduction in time complexity when data points were sparse binary vectors.\nIn [6], the authors proposed to use the sum of vectors instead of associative memories and discussed\noptimization strategies (including online scenarios). The methods we analyze in this note are in the\nsame vein.\n3. Search Problems with Sparse Patterns\nIn this section, we will treat the situation where we try to ﬁnd a pattern that is closest to some\ninput pattern and all the patterns are binary and sparse.\nTo be more precise, assume that we are given nsparse vectors or patterns xm,m=1,. . .,nwith\nxm2f0, 1gdfor some (large) d. Assume that the (xm)are random and i.i.d. and have i.i.d. coordinates\nsuch that:\nP(xm\ni=1) =c\nd=1\u0000P(xm\ni=0)\nfor all i=1,. . .,dand all m=1,. . .,n. To describe a sparse situation, we will assume cdepends\nond, such that c/d!0ascand dbecome large. We will even need this convergence to be fast\nenough. Moreover, we will always assume that there is an xmsuch that the query pattern x0has a\nmacroscopically large overlap with xm, hence that åd\nl=1x0\nlxm\nlis of order c.\nWe will begin with the situation that x0=xmfor an index m. The situation where x0is a perturbed\nversion of xmis then similar.\nThe algorithm proposed in [ 7] proposes to partition the patterns into equal-sized classes X1,. . .Xq,\nwithjXij=kfor all i=1, . . . q. Thus, n=kq. Afterwards, one computes the score:\ns(Xi,x0) =å\nm:xm2Xid\nå\nl,m=1x0\nlx0\nmxm\nlxm\nm\nfor each of these classes and takes the class Xiwith the highest score. Then, on this class, one performs\nexhaustive search. We will analyze under which condition on the parameters this algorithm works\nwell and when it is effective. To this end, we will show the following theorem.\nTheorem 1. Assume that the query pattern is equal to one of the patterns in the database. The algorithm\ndescribed above works asymptotically correctly, i.e., with an error probability that converges to zero and is\nefﬁcient, i.e., requires less computations than exhaustive search, if:\n1. d\u001ck\u001cd2, i.e.,k\nd2!0andk\nd!¥,\n2. c\u001cd and c\u001c\u0010\nd7\nk3\u00111\n6\n3. and qe\u00001\n33d2\nk!0.\nProof. To facilitate notation, assume that x0=x1(which is of course unknown to the algorithm),\nthat x12X1and that x1\ni=1for the ﬁrst ˜cindices iand that x1\ni=0for the others. Note that\nfor any #>0, with high probability (when cand dbecome large), we are able to assume that\n˜c\nd2\u0000c\nd(1\u0000#),c\nd(1+#)\u0001\n. Then, for any i, we have s(Xi,x0) =åm:xm2Xiå˜c\nl,m=1xm\nlxm\nmand in particular:\ns(X1,x0) = ˜c2+å\nm:xm2X1\nm6=1˜c\nå\nl,m=1xm\nlxm\nm.\n\nAppl. Sci. 2018 ,8, 1676 4 of 21\nWe now want to show that for a certain range of parameters n,kand d, this algorithm works\nreliably, i.e., we want to show that:\nP(9i\u00152 :s(Xi,x0)\u0015s(X1,x0))!0 as d!¥.\nNow, since, cand ˜care close together when divided by d, we will replace ˜cbyceverywhere\nwithout changing the proof. Trivially,\nP(9i\u00152 :s(Xi,x0)\u0015s(X1,x0))\u0014qP(s(X2,x0)\u0015s(X1,x0))\nand we just need to bound the probability on the right-hand side. Taking Xmto be i.i.d.\nB(c,c\nd)-distributed random variables, we may rewrite the probability on the right-hand side as:\nP(s(X2,x0)\u0015s(X1,x0)) =P(k\nå\nm=1X2\nm\u00002k\u00001\nå\nm=k+1X2\nm\u0015c2).\nCentering the variables, we obtain:\nP(k\nå\nm=1X2\nm\u00002k\u00001\nå\nm=k+1X2\nm\u0015c2)\n=P(k\nå\nm=1(Xm\u0000c2\nd)2\u00002k\u00001\nå\nm=k+1(Xm\u0000c2\nd)2+2c2\nd(k\nå\nm=1Xm\u00002k\u00001\nå\nm=k+1Xm)\u0000c4\nd2\u0015c2)\nNow, as (åk\nm=1Xm\u0000å2k\u00001\nm=k+1Xm)\u0000c4\nd2\u0014(åk\nm=1Xm\u0000å2k\u00001\nm=k+1Xm), we arrive at:\nP(k\nå\nm=1X2\nm\u00002k\u00001\nå\nm=k+1X2\nm\u0015c2)\n\u0014P(k\nå\nm=1(Xm\u0000c2\nd)2\u00002k\u00001\nå\nm=k+1(Xm\u0000c2\nd)2\u0015c2\n2) +P(k\nå\nm=1Xm\u00002k\u00001\nå\nm=k+1Xm\u0015d\n4)\nLet us treat the last term ﬁrst. For t>0:\nP(k\nå\nm=1Xm\u00002k\u00001\nå\nm=k+1Xm\u0015d\n4)\u0014e\u0000td\n4(EetX1)k(Ee\u0000tX1)k\u00001\n=e\u0000td\n4\u0010\n1+c\nd(et\u00001)\u0011kc\u0010\n1+c\nd(e\u0000t\u00001)\u0011(k\u00001)c\n=e\u0000td\n4\u0012\n1+c\nd(t+t2\n2+t3\n6+O(t4))\u0013kc\u0012\n1+c\nd(\u0000t+t2\n2\u0000t3\n6+O(t4))\u0013(k\u00001)c\n\u0014e\u0000td\n4exp\u0012c2\ndt+kc2\ndt2+c2\n3dt3+O(c2\ndkt4)\u0013\nNow, we take t=#d4q\nd\nkc2, which implies that:\nP(k\nå\nm=1Xm\u00002k\u00001\nå\nm=k+1Xm\u0015d\n4)\u0014exp \n\u0000#d4r\nd5\n256kc2+#d4r\nc6\nkd3+#2\nd4r\nc4k2\nd2+#3\nd4r\nc2\n81dk3+O(#4\nd)!\n\nAppl. Sci. 2018 ,8, 1676 5 of 21\nNow,c2\n81dk3!0 due to our assumptions c\u001cd\u001ck. Moreover,d5\n256kc2\u001dc6\nkd3, since c\u001cd. Hence,\nwe just need to consider the negative term in the exponent and #2\nd4q\nc4k2\nd2, which justiﬁes our hypothesis\nc\u001c\u0010\nd7\nk3\u00111\n6.\nThe other summand is slightly more complicated: note that V(Xm\u0000c2\nd)\u0019c2\nd; thus, we compute\nfort>0:\nP(k\nå\nm=1(Xm\u0000c2\nd)2\u00002k\u00001\nå\nm=k+1(Xm\u0000c2\nd)2\u0015c2\n2) =P0\nB@k\nå\nm=10\n@Xm\u0000c2\ndq\nc2\nd1\nA2\n\u00002k\u00001\nå\nm=k+10\n@Xm\u0000c2\ndq\nc2\nd1\nA2\n\u0015d\n21\nCA\n\u0014e\u0000td\n20\nBBB@Eet0\n@Xm\u0000c2\ndr\nc2\nd1\nA21\nCCCAk0\nBBB@Ee\u0000t0\n@Xm\u0000c2\ndr\nc2\nd1\nA21\nCCCAk\u00001\nwhere on the right-hand side, we can take any ﬁxed m.\nTo estimate the expectations, we will apply a version of Lindeberg’s replacement trick [ 26,27] or in\na similar spirit to the computation below [ 28]. To this end, assume that Sc:=Xm\u0000c2\ndq\nc2\ndd=s1+. . .+sc,\nfor appropriately centered i.i.d. Bernoulli random variables skwith variance one. Moreover, let\nx=1pcåc\ni=1xi, where the (xi)are i.i.d. standard Gaussian random variables. Finally, set:\nTk:=1pc(s1+\u0001\u0001\u0001+sk\u00001+xk+1+\u0001\u0001\u0001+xc)\nand f(x) =etx2. Then, we obtain by Taylor expansion:\njE(f(Sc)\u0000f(x))j\u0014c\nå\nk=1\f\f\f\fE\u0014\nf\u00001pc(Tk+sk)\u0001\u0000f\u00001pc(Tk+xk)\u0001\u0015\f\f\f\f\n\u0014c\nå\nk=1\f\f\f\fE\u0014\nf0\u0000Tkpc\u00011pc(sk\u0000xk) +1\n2f00\u0000Tkpc\u00011\nc(s2\nk\u0000x2\nk)\u0015\f\f\f\f+1\n6c3/2c\nå\nk=1\f\f\f\fE\u0014\nf(3)(z1)s3\nk\u0000Ef(3)(z2)x3\nk\u0015\f\f\f\f,\nwhere z1andz2are random variables that lie within the interval [Tk,Tk+sk]and[Tk,Tk+xk],\nrespectively (possibly with the right and left boundary interchanged, if skorxkare negative). First of\nall, observe that for each k, the random variable Tkis independent of the xkandsk, and xkandskhave\nmatching ﬁrst and second moments. Therefore:\nc\nå\nk=1\f\f\f\fE\u0014\nf0\u0000Tkpc\u00011pc(sk\u0000xk) +1\n2f00\u0000Tkpc\u00011\nc(s2\nk\u0000x2\nk)\u0015\f\f\f\f=0.\nFor the second term on the right-hand side, we compute:\nf(3)(x) = ( 12t2x+8t3x3)f(x).\nObserve that E[(x)netx2]<¥forn=0, 1, 2, 3 fortsmall enough and the expectation\nE\u0002 \nXm\u0000c2\ndq\nc2\nd!n\nf \nXm\u0000c2\ndq\nc2\nd!\n\u0003\nis uniformly bounded in the number of summands cforn=0, 1, 2, 3\nand if tis small enough. Finally, z1andz2have the form Tk+akxkpc+bkskfor some coefﬁcients akand\n\nAppl. Sci. 2018 ,8, 1676 6 of 21\nbkwith 0\u0014jakj,jbkj\u00141. Therefore,\f\f\f\fE\u0014\nf(3)(z1)s3\nk\u0000Ef(3)(z2)x3\nk\u0015\f\f\f\fis of order t2et/pc, i.e., for ﬁxed t\nof constant order, and:\n1\n6c3/2c\nå\nk=1\f\f\f\fE\u0014\nf(3)(z1)s3\nk\u0000Ef(3)(z2)x3\nk\u0015\f\f\f\f=O(t2etpc\npc) =Ot(1pc)\nwhereOtdenotes an upper bound by a constant that may depend on t. This notation (and the implied\ncrude way to deal with the t-dependence) is justiﬁed, because from the next line on, we will just\nconcentrate on tvalues that are smaller than1\n2. For those,t2et/pcpc\u00141pc.\nMoreover, we have for tsmaller than 1/2 by Gaussian integration:\nE[f(x)] =E[etx2] =1p\n1\u00002t. (1)\nTherefore, writing:\nE[f(Sc)]\u0014\f\fE[f(x)]\f\f+\f\fE[f(Sc)\u0000f(x)]\f\f,\nWe obtain:\n0\nBB@E[etX2m\u0000c2\ndr\nc2\nd]1\nCCAk\n\u0014e\u0000k\n2log(1\u00002t)\u0012\n1+e1\n2log(1\u00002t)Ot(1pc)\u0013k\n.\nIn a similar fashion, we can also bound:\n0\nBB@E[e\u0000tX2m\u0000c2\ndr\nc2\nd]1\nCCAk\u00001\n\u0014e\u0000k\u00001\n2log(1+2t)\u0012\n1+e1\n2log(1+2t)Ot(1pc)\u0013k\u00001\n,\nwhere now, we make use of E[e\u0000tx2] =1p\n1+2tinstead of (1).\nHence, by Taylor expansion of the logarithm altogether, we obtain:\nP(k\nå\nm=1(Xm\u0000c2\nd)2\u00002k\u00001\nå\nm=k+1(Xm\u0000c2\nd)2\u0015c2\n2)\u0014e\u0000td\n2e2kt2+O(t+t2kpc+kt4). (2)\nNote that we will always assume that c!¥such thatkpcis negligible with respect to k. Now, we\nchoose t=d\n8k. This, in particular, ensures that tconverges to zero for large dimensions and therefore\nespecially that (1) can be applied; we obtain that asymptotically:\nP(k\nå\nm=1(Xm\u0000c2\nd)2\u00002k\u00001\nå\nm=k+1(Xm\u0000c2\nd)2\u0015c2\n2)\u0014e\u00001\n33d2\nk. (3)\nIndeed, keeping the leading order term from the O-expression in (2)with our choice of t, we obtain\nthat asymptotically:\nP(k\nå\nm=1(Xm\u0000c2\nd)2\u00002k\u00001\nå\nm=k+1(Xm\u0000c2\nd)2\u0015c2\n2)\u0014e\u00001\n32d2\nk+O(d4\nk3)=e\u00001\n32d2\nk(1+O(d2\nk2)).\n\nAppl. Sci. 2018 ,8, 1676 7 of 21\nAsd\u001ck, the term (1+O(d2\nk2))will get arbitrarily close to one; hence, (3)follows. However,\nthe right-hand side of (3)goes to zero, whenever d2\u001dk. Hence, for d\u001ck\u001cd2, we obtain that the\nmethod works ﬁne if c\u001cd,c\u001c\u0010\nd7\nk3\u00111\n6and q\u001ce1\n33d2\nk, which are our last conditions.\nIn a very similar fashion, we can treat the case of an input pattern that is a corrupted version of\none of the vectors in the database.\nCorollary 1. Assume that the input pattern x0has a macroscopic overlap with one of the patterns in the\ndatabase, i.e., åd\nl=1x0\nlxm\nl=ac, for a a2(0, 1), and x0hascentries equal to one and d\u0000cindices equal to zero.\nThe algorithm described above works asymptotically correctly, i.e., with an error probability that converges to\nzero, and is efﬁcient if:\n1. d\u001ck\u001cd2,\n2. c\u001cd and c\u001c\u0010\nd7\nk3\u00111\n6\n3. and qe\u0000a4\n33d2\nk!0.\nProof. The proof is basically a rerun of the proof of Theorem 1. Again, without loss of generality,\nassume that x1is our target pattern in the database such that åd\nl=1x0\nlx1\nl=ac, that x12X1and that\nx0\ni=1 for the ﬁrst cindices iand that x0\ni=0 for the others. Then, for any i, we have:\ns(X1,x0) =a2c2+å\nm:xm2X1\nm6=1c\nå\nl,m=1xm\nlxm\nm\nwhile s(Xi,x0)is structurally the same as in the previous proof. Therefore, following the lines of the\nproof of Theorem 1 and using the notation introduced there, we ﬁnd that:\nP(9i\u00152 :s(Xi,x0)\u0015s(X1,x0))\u0014qP(s(X2,x0)\u0015s(X1,x0))\n=qP(k\nå\nm=1(Xm\u0000c2\nd)2\u00002k\u00001\nå\nm=k+1(Xm\u0000c2\nd)2\u0015a2c2\n2) + qP(k\nå\nm=1Xm\u00002k\u00001\nå\nm=k+1Xm\u0015a2d\n4).\nAgain, the second summand vanishes as long as the second condition of our corollary holds,\nwhile for the ﬁrst summand, we obtain exactly as in the previous proof:\nqP(k\nå\nm=1Xm\u00002k\u00001\nå\nm=k+1Xm\u0015a2d\n4)\u0014qe\u0000ta2d\n2e2kt2+O(t+kpc+kt4).\nNow, we choose t=a2d\n8kto conclude as in the proof of Theorem 1 that:\nqP(k\nå\nm=1Xm\u00002k\u00001\nå\nm=k+1Xm\u0015a2d\n4)\u0014qe\u0000a4d2\n33k\nwhich converges to zero by assumption.\nRemark 1. Erased or extremely corrupted patterns (i.e., a!0) can also be treated.\n\nAppl. Sci. 2018 ,8, 1676 8 of 21\n4. Dense, Unbiased Patterns\nContrary to the previous section, we will now treat the situation where the patterns are not sparse\nand do not have a bias. This is best modeled by choosing xm2f\u0000 1, 1gdfor some (large) d. We will\nnow assume that the xm,m=1, . . . , nare i.i.d. and have i.i.d. coordinates such that:\nP(xm\nl=1) =1\n2=P(xm\nl=\u00001)\nfor all l=1,. . .,dand all m=1,. . .,n. Again, we will suppose that there is an xmsuch that dH(xm,x0)\nis macroscopically large. To illustrate the ideas, we start with the situation that x0=x1. Again, we\nwill also partition the patterns into equal-sized classes X1,. . .Xq, withjXij=kfor all i=1,. . .,q\nand compute:\ns(Xi,x0) =å\nm:xm2Xid\nå\nl,m=1x0\nlx0\nmxm\nlxm\nm\nto compute the overlap of a class iwith the query pattern. We will prove:\nTheorem 2. Assume that the query pattern is equal to one of the patterns in the database. The algorithm\ndescribed above works asymptotically correctly and is efﬁcient if:\n1. d\u001ck\u001cd2, i.e.,k\nd2!0andk\nd!¥, and either\n2. qe\u00001\n8d2\nk!0if d4\u001ck3,\n3. or q exp\u0012\n\u0000d2\nk5\n4\u0013\n!0if k\u0014Cd4\n3, for some C >0.\nProof. Without loss of generality, we may again suppose that x0=x1and now that x0\nl=x1\nl\u00111for\nalll=1,. . .,d, since otherwise, we can ﬂip the spins of all coordinates and all images, where this is\nnot the case (recall that the xm\nlare all unbiased i.i.d., such that ﬂipping the spins does not change their\ndistribution). Then, the above expression simpliﬁes to:\ns(Xi,x0) =å\nm:xm2Xid\nå\nl,m=1xm\nlxm\nm.\nEspecially:\ns(X1,x0) =d2+å\nm:xm2X1\nm6=1d\nå\nl,m=1xm\nlxm\nm.\nAgain, we want to know for which parameters:\nP(9i\u00152 :s(Xi,x0)\u0015s(X1,x0))!0 as d!¥.\nTrivially,\nP(9i\u00152 :s(Xi,x0)\u0015s(X1,x0))\u0014qP(s(X2,x0)\u0015s(X1,x0)).\nBy the exponential Chebyshev inequality, we obtain for any t>0:\nP(s(X2,x0)\u0015s(X1,x0)) =P(å\nm:xm2X2d\nå\nl,m=1xm\nlxm\nm\u0000å\nn:xn2X1\nn6=1d\nå\nl,m=1xn\nlxn\nm\u0015d2)\n\u0014e\u0000td2(Eexp(td\nå\nl,m=1xm\nlxm\nm))k(Eexp(\u0000td\nå\nl,m=1xn\nlxn\nm))k\u00001.\n\nAppl. Sci. 2018 ,8, 1676 9 of 21\nTo calculate the expectations, we introduce a standard normal random variable ythat is\nindependent of all other random variables occurring in the computation. Then, by Gaussian expectation\nand Fubini’s theorem:\nEexp(td\nå\nl,m=1xm\nlxm\nm) =Eexp(t(d\nå\nl=1xm\nl)2) =ExEyep\n2ty(åd\nl=1xm\nl)\n=EyExep\n2ty(åd\nl=1xm\nl)=Eyd\nÕ\nl=1Exm\nlep\n2tyxm\nl\n=Eyd\nÕ\nl=1cosh(p\n2ty)\u0014Eyedty2=1p\n1\u00002td\nifdt<1\n2. Here, we used the well-known estimate cosh (x)\u0014ex2\n2for all x. Thus:\n(Eexp(td\nå\nl,m=1xm\nlxm\nm))k\u0014exp(\u0000k\n2log(1\u00002td)).\nOn the other hand, similarly to the above, again by introducing a standard Gaussian random\nvariable y, we arrive at:\nEexp(\u0000td\nå\nl,m=1xm\nlxm\nm) = ExEyeip\n2ty(åd\nl=1xm\nl)=EyExeip\n2ty(åd\nl=1xm\nl)=Ey[cos(p\n2ty)d]\nTo compute the latter, we write for some #d>0, which converges to zero if d!¥at a speed to\nbe chosen later:\nEy[cos(p\n2ty)d] =Z\ny\u0014#d\n(t2d)1/4cos(p\n2ty)ddP+Z\ny>#d\n(t2d)1/4cos(p\n2ty)ddP\n\u0014Z\ny\u0014#d\n(t2d)1/4(1\u0000ty2+O(t2y4))ddP+P(y>#d\n(t2d)1/4)\n\u0014Z\ny\u0014#d\n(t2d)1/4e\u0000tdy2+O((#d)4)dP+e\u0000#2\nd\n2tp\nd\n\u0014eO((#d)4)Z\ne\u0000tdy2dP+e\u0000#2\nd\n2tp\nd\n\u0014eO((#d)4) 1p\n1+2td+e\u0000#2\nd\n2tp\nd,\nwhere we used there the well-known estimate P[y\u0015u]\u0014e\u0000u2\n2for all u\u00150andE[e\u0000tdy2] =1p\n1+2td.\nThus:\n(Eexp(\u0000td\nå\nl,m=1xm\nlxm\nm))k\u00001\u0014(eO((#d)4) 1p\n1+2td+e\u0000#2\nd\n2tp\nd)k\u00001\n\u0014eO(k(#d)4)e\u0000k\u00001\n2log(1+2td)(1+eO(k(#d)4)e1\n2log(1+2td)e\u0000#2\nd\n2tp\nd)k\u00001\n\u0014eO(k(#d)4)e\u0000k\u00001\n2log(1+2td)exp(keO(k(#d)4)e1\n2log(1+2td)e\u0000#2\nd\n2tp\nd)\n\nAppl. Sci. 2018 ,8, 1676 10 of 21\nWe ﬁrst choose #d=k\u00001/4˜#dfor some small ˜#dconverging to zero as dgoes to inﬁnity, such that\nthe ﬁrst factor on the right-hand side converges to one. Then, we choose kand dsuch that the third\nterm on the right-hand side converges to one. This is true if:\n1\n2log(1+2td)\u0000#2\nd\n2tp\nd+log(k)!\u0000¥.\nAnticipating our choices of tand dsuch that dtgoes to zero, we get the condition \u0000k\u00001/4˜#2\nd\n2tp\nd+\nlog(k)!\u0000¥asdand kgo to zero.\nNow, we always suppose that d\u001ck. With these choices, we obtain that:\nP(s(X2,x0)\u0015s(X1,x0))\u0014e\u0000td2e\u0000k\u00001\n2log(1+2dt)e\u0000k\n2log(1\u00002dt)\u0019e\u0000td2e\u0000k\n2log(1\u00004d2t2)\nNow, we differentiate two cases. If d4\u001ck3(note that we always have d\u001ck), we expand the\nlogarithm to obtain:\nP(s(X2,x0)\u0015s(X1,x0))\u0014e\u0000td2+2kd2t2+O(kt4d4)\nChoosing t=1\n4k, we see that the term O(kt4d4)is in fact o(1), and therefore:\nP(s(X2,x0)\u0015s(X1,x0))\u0014e\u0000d2\n8k\nThus, if k\u001cd2and q\u001ced2\n8k, we obtain:\nP(9i\u00152 :s(Xi,x0)\u0015s(X1,x0))!0.\nOn the other hand, if d\u001ck\u0014O(d4/3), we choose t=k\u00005/4. Then, by the same expansion of\nthe logarithm:\nP(s(X2,x0)\u0015s(X1,x0))\u0014exp\u0012\n\u0000d2\nk5\n4+2d2\nk3\n2+O((d\nk)4)\u0013\nNow, the\u0000d2\nk5\n4will always dominate thed2\nk3\n2-term, and clearly, the O-term is again o(1). Thus:\nP(s(X2,x0)\u0015s(X1,x0))\u0014exp\u0012\n\u0000d2\nk5\n4\u0013\n(1+o(1)).\nSince k\u0014O(d4/3), the exponent diverges, and we see that as long as log q\u001cd2\nk5\n4, again:\nP(9i\u00152 :s(Xi,x0)\u0015s(X1,x0))!0.\nWe can easily check that our ﬁrst condition \u0000k\u00001/4˜#2\nd\n2tp\nd+log(k)!\u0000¥is fulﬁlled in both cases:\nt=1\n4kand t=k\u00005/4, ifd\u001ck.\nAgain, in a similar way, one can treat the case of an input pattern that is a corrupted version of\none of the vectors in the database.\nCorollary 2. Assume that the input pattern x0has a macroscopic overlap with one of the patterns, say x1,\nin the database, i.e., åd\nl=1x0\nlx1\nl=ad, for a a2(0, 1). The algorithm described above works asymptotically\ncorrectly and is efﬁcient if:\n1. d\u001ck\u001cd2, i.e.,k\nd2!0andk\nd!¥,\n2. and either qe\u00001\n8a4d2\nk!0if d4\u001ck3,\n\nAppl. Sci. 2018 ,8, 1676 11 of 21\n3. or qe\u0000a4d2\nk5\n4!0if k\u0014Cd4\n3for some C >0.\nProof. The proof follows the lines of the proof of Theorem 2 by using the condition that\nåd\nl=1x0\nlx1\nl=ad.\nRemark 2. In the spirit of [ 29], one might wonder how far taking a higher power than two of åd\nl=1x0\nlx1\nlin the\ncomputation of the score function changes the results. Therefore, let us assume we take:\ns(Xi,x0) =å\nm:xm2Xid\nå\nl1,...lnx0\nl1x0\nl1\u0001\u0001\u0001xm\nlnxm\nln. (4)\nThen, of course, in the setting of the proof of Theorem 2, we obtain:\ns(Xi,x0) =dn+å\nm:xm2X1\nm6=1d\nå\nl1,...lnx0\nl1x0\nl1\u0001\u0001\u0001xm\nlnxm\nln\nso we gain in the exponent. However, the probability that s(X1,x0)is smaller than s(X1,x0)is then much\nmore difﬁcult to estimate. As a matter of fact, none of the techniques used in Sections 2 and 3 work, because a\nhigher power cannot be linearized by Gaussian integration, on the one hand, and the exponential of powers\nlarger than two Gaussian random variables are not integrable. A possible strategy could include exponential\nbounds as in Proposition 3.2. in [ 14]. From here one, also learns that in the Hopﬁeld model with Nneurons\nandn-spin interaction ( n\u00153), the storage capacity grows like Np\u00001. A similar model was discussed in [ 29],\nwhere it is shown that in principle, the problems sketched above can be overcome. By similarity, this could lead to\nthe conjecture that replacing the score function by (4)leads to a class size of k\u001cdn. However, in this scenario,\nthe computational complexity of our algorithm would also increase.\n5. Experiments\nIn order to stress the performance of the proposed system when considering non-asymptotic\nparameters, we propose several experiments. In Section 5.1, we analyze the performance when using\nsynthetic data. In Section 5.2, we run simulations using standard off-the-shelf real data.\n5.1. Synthetic Data\nWe ﬁrst present results obtained using synthetic data. Unless speciﬁed otherwise, each drawn\npoint is obtained using Monte Carlo simulations with at least 100,000 independent tests.\n5.1.1. Sparse Patterns\nConsider data to be drawn i.i.d. with:\nP(xm\ni=1) =c\nd=1\u0000P(xm\ni=0).\nRecall that we have four free parameters: cthe number of one in patterns, dthe dimension of\npatterns, kthe number of pattern in each class and qthe number of classes.\nOur ﬁrst two experiments consist of varying kand afterwards qwhile the other parameters are\nﬁxed. We choose the parameters d=128and c=8. Figure 1 depicts the rate at which the highest\nscore is not achieved by the class containing the query vector (we call it the error rate), as a function\nofkand for q=10. This function is obviously increasing with kand presents a high slope for small\nvalues of k, emphasizing the critical choice of kfor applications.\n\nAppl. Sci. 2018 ,8, 1676 12 of 21\n 0 0.2 0.4 0.6 0.8 1\n 1000  2000  3000  4000  5000  6000  7000  8000Error rate\nk\nFigure 1. Evolution of the error rate as a function of k. The other parameters are q=10,d=128and\nc=8.\nFigure 2 depicts the probability of error as a function of q, for various choices of k. Interestingly,\nfor reasonable values of kthe slope of the curve is not as dramatic as in Figure 1. When adjusting\nparameters, it seems thus more reasonable to increase the number of classes rather than the number of\npatterns in each class. This is not a surprising ﬁnding as the complexity of the process depends on q,\nbut not on k.\n 0 0.2 0.4 0.6 0.8 1\n 0  5  10  15  20Error rate\nqk=2048\nk=512\nk=128\nFigure 2. Evolution of the error rate as a function of qand for various values of k. The other parameters\nared=128 and c=8.\nGiven data and their parameters cand d, a typical designing scenario would consist of ﬁnding\nthe best trade-off between qand kin order to split the data in the different classes. Figure 3 depicts the\nerror rate as a function of kwhen n=kqis ﬁxed. To obtain these points, we consider c=8,d=128\nand n= 16,384. There are two interesting facts that are pointed out in this ﬁgure. First, we see that the\ntrade-off is not simple as multiple local minima happen. This is not surprising as the decision becomes\nless precise and promissory, with larger values of k. As a matter of fact, the last point in the curve\nonly claims the correct answer is somewhere in a population of 8192 possible solutions, whereas the\nﬁrst point claims it is one of the 64possible ones. On the other hand, there is much more memory\nconsumption for the ﬁrst point where 256 square matrices of dimension 128are stored, whereas only\ntwo of them are used for the last point. Second, the error rate remains of the same order for all tested\ncouples of values kand q. This is an interesting ﬁnding as it emphasizes that the design of a solution is\nmore about complexity vs. precision of the answer—in the sense of obtaining a reduced number of\ncandidates—than it is about error rate.\n\nAppl. Sci. 2018 ,8, 1676 13 of 21\n 0 0.2 0.4 0.6 0.8 1\n 100  1000Error rate\nk\nFigure 3. Evolution of the error rate for a ﬁxed number of stored messages n=16,384 as a function of\nk(recall that q=n/k). The generated messages are such that d=128 and c=8.\nFinally, in order to evaluate both the tightness of the bounds obtained in Section 3 and the speed\nof convergence, we run an experiment in which c=log2(d),q=2. We then depict the error rate as a\nfunction of dand when k=d1.5,k=d2and k=d2.5. The result is depicted in Figure 4. This ﬁgure\nsupports the fact that the obtained bound is tight, as illustrated by the curve corresponding to the case\nk=d2for which the error rate appears almost constant.\n 0 0.1 0.2 0.3 0.4 0.5 0.6\n 0  20  40  60  80  100  120  140  160Error rate\ndk=d2.5/10\nk=d2/10\nk=d1.5/10\nFigure 4. Evolution of the error rate as a function of d. The other parameters are q=2,c=log2(d)and\nk=da/10 with various values of a.\nWe ran the same experiments using co-occurrence rules as initially proposed in [ 7] (instead of adding\ncontributions from distinct messages, we take the maximum). We observed small improvements in every\ncase, even though they were not significant.\n5.1.2. Dense Patterns\nLet us now consider data to be drawn i.i.d. according to the distribution:\nP(xm\ni=1) =1\n2=P(xm\ni=\u00001);\nrecall that we then have three free parameters: dthe dimension of patterns, kthe number of patterns in\neach class and qthe number of classes.\nAgain, we begin our experiments by looking at the inﬂuence of k(resp. q) on the performance.\nThe obtained results are depicted in Figure 5 (resp. Figure 6). To conduct these experiments, we have\nchosen d=64. The global slope resembles that of Figures 1 and 2.\n\nAppl. Sci. 2018 ,8, 1676 14 of 21\n 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\n 0  500  1000  1500  2000  2500  3000  3500  4000Error rate\nk\nFigure 5. Evolution of the error rate as a function of k. The other parameters are q=10 and d=64.\n 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\n 2  4  6  8  10  12  14  16  18  20Error rate\nqk = 2048\nk = 512\nk = 128\nFigure 6. Evolution of the error rate as a function of q. We ﬁx the value d=64and consider various\nvalues of k.\nThen, we consider the designing scenario where the number of samples is known. We plot the\nerror rate depending on the choice of k(and thus, the choice of q=n/k). The results are depicted in\nFigure 7.\n 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35\n 1  10  100  1000  10000  100000Error rate\nk\nFigure 7. Evolution of the error rate for a ﬁxed total number of samples as a function of k. The other\nparameters are n=16,384 and d=64.\nFinally, we estimate the speed of convergence together with the tightness of the bound obtained\nin Section 4. To do so, we choose kas a function of dwith q=2. The obtained results are depicted\nin Figure 8. Again, we observe that the case k=d2appears to be a limit case where the error rate\nremains constant.\n\nAppl. Sci. 2018 ,8, 1676 15 of 21\n 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45\n 0  10  20  30  40  50  60  70  80  90  100Error rate\ndk = d2.5\nk = d2\nk = d1.5\nFigure 8. Evolution of the error rate as a function of d. In this scenario, we choose k=dawith various\nvalues of aand q=2.\n5.2. Real Data\nIn this section, we use real data in order to stress the performance of the obtained methods on\nauthentic scenarios.\nSince we want to stress the interest of using our proposed method instead of classical exhaustive\nsearch, we are mainly interested in looking at the error as a function of the complexity. To do so,\nwe modify our method as follows: we compute the scores of each class and order them from largest\nto smallest. We then look at the rate for which the nearest neighbor is in one of the ﬁrst pclasses.\nWith p=1, this method boils down to the previous experiments. Larger values of pallow for more\nﬂexible trade-offs of errors versus performance. We call the obtained ratio the recall@1.\nConsidering nvectors with dimension d, the computational complexity of an exhaustive search is\ndn(orcnfor sparse vectors). On the other hand, the proposed method has a two-fold computational\ncost: ﬁrst, the cost of computing each score, which is d2q(orc2qfor sparse vectors), then the cost of\nexhaustively looking for the nearest neighbor in the selected pclasses, which is pkd(orpkcfor sparse\nvectors). Note that the cost of ordering the qobtained scores is negligible. In practice, distinct classes\nmay have a distinct number of elements, as explained later. In this case, complexity is estimated as an\naverage of elementary operations (addition, multiplication, accessing a memory element) performed\nfor each search.\nWhen applicable, we also compare the performance of our method with Random Sampling (RS)\nto build a search tree, the methodology used by PySparNN (https://github.com/facebookresearch/\npysparnn), or Annoy (https://github.com/spotify/annoy). In their method, random sampling of r\nelements in the collection of vectors is performed; we call them anchor points. All elements from the\ncollection are then attached to their nearest anchor point. When performing the search, the nearest\nanchor points are ﬁrst searched, then an exhaustive computation is performed with corresponding\nattached elements. Finally, we also look at the performance of a hybrid method in which associative\nmemories are ﬁrst used to identify which part of the collection should be investigated, then these parts\nare treated independently using the RS methodology.\nThere are several things that need to be changed in order to accommodate real data. First,\nfor non-sparse data, we center data and then project each obtained vector on the hypersphere with\nradius one. Then, due to the non-independence of stored patterns, we choose an allocation strategy\nthat greedily assigns each vector to a class, rather than using a completely random allocation.\nThe allocation strategy we use consists of the following: each class is initialized with a random\nvector drawn without replacement. Then, each remaining vector is assigned to the class that achieves\nthe maximum normalized score. Scores are divided by the number of items ucurrently contained in\nthe class, as a normalization criterion. Note that more standard clustering techniques could be used\ninstead. Figure 9 depicts all steps of the proposed method.\n\nAppl. Sci. 2018 ,8, 1676 16 of 21\nOfﬂine processing:\nStep 1: initialize matrices and counters\nu1=1W1=x1x1>\nu2=1W2=x2x2>\nu3=1W3=x3x3> . . .\nuq=1Wq=xqxq>\nStep 2: allocate each remaining vector xm:\nxq+1xq+2xq+3xq+4xq+5xq+6xq+7 xn\nW1W2W3 . . .Wq\nOnline processing (request yn):\nStep 1: compute scores:\nyn>W1ynyn>W2ynyn>W3yn . . . yn>Wqyn\nStep 2: for the plargest scores, exhaustively search:\nW1W2W3 . . .Wq\nyn>xm2\n1yn>xm2\n2yn>xm2\n3 yn>xm2\nk . . .yn>xmq\n1yn>xmq\n2yn>xmq\n3 yn>xmq\nk . . .\nFigure 9. Illustration of the proposed method. Consider xmdenotes a pattern of the collection to be\nsearched and yna request pattern. For any column vector x,x>denotes its transpose.\nThe interest of this allocation strategy is emphasized in Figure 10, where we use raw MNISTdata to\nillustrate our point. MNIST data consist of grey-level images with 784 pixels representing handwritten\ndigits. There are 60,000 reference images and 10,000 query ones. For various values of k, we compare\nthe performance obtained with our proposed allocation and compare it with a completely random one.\nWe also plot the results obtained using the RS methodology. We observe that in this scenario where the\ndimension of vectors is large with respect to their number, associative memories are less performant\nthan their RS counterparts.\n\nAppl. Sci. 2018 ,8, 1676 17 of 21\n 0 0.2 0.4 0.6 0.8 1\n 0  0.2  0.4  0.6  0.8  1  1.2  1.4Recall at 1\nRelative computational complexityk=1000 smart allocation\nk=2000 smart allocation\nk=4000 smart allocation\nk=1000 random allocation\nk=2000 random allocation\nk=4000 random allocation\nRS, r=1000\nFigure 10. Recall@1 on the MNISTdataset as a function of the relative complexity of the proposed\nmethod with regards to an exhaustive search, for various values of kand allocation methods. Each curve\nis obtained by varying the value of p.\nWe also run some experiments on a binary database. It consists of Santander customer satisfaction\nsheets associated with a Kaggle contest. There are 76,000 vectors with dimension 369 containing\n33 nonzero values on average. In this ﬁrst experiment, the vectors stored in the database are the ones\nused to also query it. The obtained results are depicted in Figure 11.\n 0 0.2 0.4 0.6 0.8 1\n 0  0.2  0.4  0.6  0.8  1Recall at 1\nRelative computational complexityk=8000\nk=4000\nk=2000\nk=1000\nFigure 11. Recall@1 on the Santander customer satisfaction dataset as a function of the relative\ncomplexity of the proposed method with regards to an exhaustive nearest neighbor search, for various\nvalues of k. Each curve is obtained by varying the value of p.\nThen, we run experiments on the SIFT1M dataset (http://corpus-texmex.irisa.fr/). This dataset\ncontains one million 128-dimension SIFT descriptors obtained using a Hessian-afﬁne detector,\nplus 10,000 query ones. The obtained results are depicted in Figure 12. As we can see, here again, the RS\nmethodology is more efﬁcient than the proposed one, but we managed to ﬁnd hybrid parameters\nfor which performance is improved. To stress the consistency of results, we also run experiments on\nthe GIST1Mdataset. It contains one million 960-dimensions GIST descriptors and 1000 query ones.\nThe obtained results are depicted in Figure 13.\n\nAppl. Sci. 2018 ,8, 1676 18 of 21\n 0 0.2 0.4 0.6 0.8 1\n 0  0.05  0.1  0.15  0.2  0.25  0.3Recall at 1\nRelative computational complexityHybrid method, k=100000, r=1000\nProposed method, k=1000\nProposed method, k=10000\nRS, r=1000\nRS, r=10000\nFigure 12. Recall@1 on the SIFT1Mdataset as a function of the relative complexity of the proposed\nmethod with regards to an exhaustive nearest neighbor search, for various values of k. Each curve is\nobtained by varying the value of p.\n 0 0.2 0.4 0.6 0.8 1\n 0  0.05  0.1  0.15  0.2  0.25  0.3Recall at 1\nRelative computational complexityHybrid method, k=100000, r=1000\nProposed method, k=10000\nProposed method, k=50000\nProposed method, k=100000\nRS, r=1000\nRS, r=5000\nFigure 13. Recall@1 on the GIST1Mdataset as a function of the relative complexity of the proposed\nmethod with regards to an exhaustive nearest neighbor search, for various values of k. Each curve is\nobtained by varying the value of p.\nIn Table 1, we compare the proposed method with random kd-trees, K-means trees [ 1], ANN [ 16]\nand LSH [ 22] on the SIFT1M dataset. To perform these measures, a computer using a i5-7400 CPU with\n16 GB of DDR4 RAM was used. We used off-the-shelf libraries (https://github.com/mariusmuja/ﬂann,\nhttp://www.cs.umd.edu/~mount/ANN/ and http://www.mit.edu/~andoni/LSH/). As expected,\nthis table shows that important gains are expected when high precision is needed. On the contrary,\ngains are limited when facing low precision challenges. These limited gains are mainly explained\nby the fact the matrix computations are too expensive in the proposed method for scenarios with\nlow precision.\nIn Table 2, we compare the asymptotical complexities of different methods. Note that we look at\nbest cases for each method: for quantization techniques such as LSH, we consider that we can encode\npoints using a number of bits that can be processed using only one operation on the given machine.\nFor search space partitioning method, we always consider the balanced case where all parts contain an\nequal number of elements. This table shows that in terms of asymptotical complexity, the proposed\nmethod is not better than random sampling or K-means. This is because the cost of computing a score\nwith the proposed scheme is larger than computing the distance to a centroid. Of course, this extra\ncost is balanced in practice by the fact that the proposed method provides a better proxy to ﬁnding the\npart of the space to search in, as emphasized in Figures 12 and 13. Finally, the hybrid method obtains\nthe best scores at each regime, thanks to the ﬂexibility of its design.\n\nAppl. Sci. 2018 ,8, 1676 19 of 21\nTable 1. Comparison of recall@1 and computation time for one scan (in ms) of the proposed method,\nkd-trees, K-means trees [ 1], ANN [ 16] and LSH [ 22] on the SIFT1M dataset for various targeted\nrecall performances.\nScan Time Recall@1 Scan Time Recall@1 Scan Time Recall@1\nRandom kd-trees [1] 0.04 0.6 0.22 0.8 3.1 0.95\nK-means trees [1] 0.06 0.6 0.25 0.8 2.8 0.99\nProposed method (hybrid) 0.17 0.6 0.25 0.8 1.1 0.99\nANN [16] 3.7 0.6 8.2 0.8 24 0.95\nLSH [22] 6.4 0.6 11.1 0.8 28 0.98\nTable 2. Comparison of the asymptotical complexity of the scan of one element in the search space for\nvarious methods, as a function of the cardinal nand the dimensionality dof the search space.\nTechnique Scan Complexity d=O(1) d=o(log(n)) d=Q(n1/3)d=Q(pn)\nQuantization O(n)O(n)O(n)O(n)O(n)\nRS or K-means O(dpn)O(pn) o(log(n)pn)O(n5/6)O(n)\nProposed O\u0000\nqd2+ (n/q)d\u0001O(pn) o(pnlog3/2(n))O(n)O(n)\nHybrid scheme O\u0000\nqd2+(k+n/q/k)d\u0001O(n1/3) o(log2(n)n1/3)O(n7/9)O(n)\n6. Conclusions\nWe introduced a technique to perform approximate nearest neighbor search using associative\nmemories to eliminate most of the unpromising candidates. Considering independent and identically\ndistributed binary random variables, we showed there exists some asymptotical regimes for which\nboth the error probability tends to zero and the complexity of the process becomes negligible compared\nto an exhaustive search. We also ran experiments on synthetic and real data to emphasize the interest\nof the method for realistic application scenarios.\nA particular interest of the method is its ability to cope with very high dimension vectors, and even\nto provide better performance as the dimension of data increases. When combined with reduction\ndimension methods, there arise interesting perspectives on how to perform approximate nearest\nneighbor search with limited complexity.\nThere are many open ideas on how to improve the method further, including but not limited to\nusing smart pooling to directly identify the nearest neighbor without the need to perform an exhaustive\nsearch, cascading the process using hierarchical partitioning or improving the allocation strategies.\nAuthor Contributions: Methodology and validation: V .G., Formal Analysis: M.L. and F.V .\nFunding: This research received no external funding.\nConﬂicts of Interest: The authors declare no conﬂicts of interest.\nReferences\n1. Muja, M.; Lowe, D.G. Scalable Nearest Neighbor Algorithms for High Dimensional Data. IEEE Trans. Pattern\nAnal. Mach. Intell. 2014 ,36, 2227–2240. [CrossRef] [PubMed]\n2. Muja, M.; Lowe, D.G. Fast Approximate Nearest Neighbors with Automatic Algorithm Conﬁguration.\nIn Proceedings of the Fourth International Conference on Computer Vision Theory and Applications\n(VISAPP 2009), Lisboa, Portugal, 5–8 February 2009.\n3. Gong, Y.; Lazebnik, S. Iterative quantization: A procrustean approach to learning binary codes.\nIn Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nColorado Springs, CO, USA, 20–25 June 2011; pp. 817–824.\n4. Jegou, H.; Douze, M.; Schmid, C. Product quantization for nearest neighbor search. IEEE Trans. Pattern Anal.\nMach. Intell. 2011 ,33, 117–128. [CrossRef] [PubMed]\n\nAppl. Sci. 2018 ,8, 1676 20 of 21\n5. Datar, M.; Immorlica, N.; Indyk, P .; Mirrokni, V .S. Locality-sensitive hashing scheme based on p-stable\ndistributions. In Proceedings of the Twentieth Annual Symposium on Computational Geometry, Brooklyn,\nNY, USA, 8–11 June 2004; pp. 253–262.\n6. Iscen, A.; Furon, T.; Gripon, V .; Rabbat, M.; Jégou, H. Memory vectors for similarity search in high-dimensional\nspaces. IEEE Trans. Big Data 2018 ,4, 65–77. [CrossRef]\n7. Yu, C.; Gripon, V .; Jiang, X.; Jégou, H. Neural Associative Memories as Accelerators for Binary Vector Search.\nIn Proceedings of the COGNITIVE 2015: 7th International Conference on Advanced Cognitive Technologies\nand Applications, Nice, France, 22–27 March 2015; pp. 85–89.\n8. Hopﬁeld, J.J. Neural networks and physical systems with emergent collective computational abilities.\nProc. Natl. Acad. Sci. USA 1982 ,79, 2554–2558. [CrossRef] [PubMed]\n9. McEliece, R.J.; Posner, E.C.; Rodemich, E.R.; Venkatesh, S.S. The capacity of the Hopﬁeld associative memory.\nIEEE Trans. Inform. Theory 1987 ,33, 461–482, doi:10.1109/TIT.1987.1057328. [CrossRef]\n10. Löwe, M.; Vermet, F. The storage capacity of the Hopﬁeld model and moderate deviations. Stat. Probab. Lett.\n2005 ,75, 237–248, doi:10.1016/j.spl.2005.06.001. [CrossRef]\n11. Löwe, M.; Vermet, F. The capacity of q-state Potts neural networks with parallel retrieval dynamics.\nStat. Probab. Lett. 2007 ,77, 1505–1514, doi:10.1016/j.spl.2007.03.030. [CrossRef]\n12. Gripon, V .; Heusel, J.; Löwe, M.; Vermet, F. A comparative study of sparse associative memories. J. Stat. Phys.\n2016 ,164, 105–129, doi:10.1007/s10955-016-1530-z. [CrossRef]\n13. Löwe, M. On the storage capacity of the Hopﬁeld model with biased patterns. IEEE Trans. Inform. Theory\n1999 ,45, 314–318, doi:10.1109/18.746829. [CrossRef]\n14. Newman, C. Memory capacity in neural network models: Rigorous lower bounds. Neural Netw. 1988 ,\n1, 223–238. [CrossRef]\n15. Löwe, M.; Vermet, F. The Hopﬁeld model on a sparse Erd˝ os-Renyi graph. J. Stat. Phys. 2011 ,143, 205–214,\ndoi:10.1007/s10955-011-0167-1. [CrossRef]\n16. Arya, S.; Mount, D.M.; Netanyahu, N.S.; Silverman, R.; Wu, A.Y. An optimal algorithm for approximate\nnearest neighbor searching ﬁxed dimensions. J. ACM (JACM) 1998 ,45, 891–923. [CrossRef]\n17. Tagami, Y. AnnexML: Approximate nearest neighbor search for extreme multi-label classiﬁcation.\nIn Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining (KDD ’17), Halifax, NS, Canada, 13–17 August 2017; ACM: New York, NY, USA, 2017; pp. 455–464.\n18. He, K.; Wen, F.; Sun, J. K-means hashing: An afﬁnity-preserving quantization method for learning binary\ncompact codes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\nPortland, OR, USA, 23–28 June 2013; pp. 2938–2945.\n19. Weiss, Y.; Torralba, A.; Fergus, R. Weiss, Y.; Torralba, A.; Fergus, R. Spectral Hashing. Available online:\nhttp://papers.nips.cc/paper/3383-spectral-hashing.pdf (accessed on 15 September 2018).\n20. Ge, T.; He, K.; Ke, Q.; Sun, J. Optimized product quantization for approximate nearest neighbor search.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA,\n23–28 June 2013; pp. 2946–2953.\n21. Norouzi, M.; Fleet, D.J. Cartesian k-means. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, Portland, OR, USA, 23–28 June 2013; pp. 3017–3024.\n22. Andoni, A.; Indyk, P . Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions.\nIn Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06),\nBerkeley, CA, USA, 21–24 October 2006; pp. 459–468.\n23. Norouzi, M.; Punjani, A.; Fleet, D.J. Fast search in hamming space with multi-index hashing. In Proceedings\nof the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Providence, RI, USA,\n16–21 June 2012; pp. 3108–3115.\n24. Liu, Y.; Cui, J.; Huang, Z.; Li, H.; Shen, H.T. SK-LSH: An efﬁcient index structure for approximate nearest\nneighbor search. Proc. VLDB Endow. 2014 ,7, 745–756. [CrossRef]\n25. Kraska, T.; Beutel, A.; Chi, E.H.; Dean, J.; Polyzotis, N. The case for learned index structures. In Proceedings of\nthe 2018 International Conference on Management of Data, Houston, TX, USA, 10–15 June 2018; pp. 489–504.\n26. Lindeberg, J.W. Über das Exponentialgesetz in der Wahrscheinlichkeitsrechnung. Ann. Acad. Sci. Fenn.\n1920 ,16, 1–23.\n27. Eichelsbacher, P .; Löwe, M. 90 Jahre Lindeberg-Methode. Math. Semesterber. 2014 ,61, 7–34. [CrossRef]\n\nAppl. Sci. 2018 ,8, 1676 21 of 21\n28. Eichelsbacher, P .; Löwe, M. Lindeberg’s method for moderate deviations and random summation. arXiv\n2017 , arXiv:1705.03837.\n29. Demircigil, M.; Heusel, J.; Löwe, M.; Upgang, S.; Vermet, F. On a model of associative memory with huge\nstorage capacity. J. Stat. Phys. 2017 ,168, 288–299, doi:10.1007/s10955-017-1806-y. [CrossRef]\nc\r2018 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access\narticle distributed under the terms and conditions of the Creative Commons Attribution\n(CC BY) license (http://creativecommons.org/licenses/by/4.0/).",
  "textLength": 48827
}