{
  "paperId": "9097002919e54361e4a25af90d226a80d90b994f",
  "title": "Can Q-Learning be Improved with Advice?",
  "pdfPath": "9097002919e54361e4a25af90d226a80d90b994f.pdf",
  "text": "arXiv:2110.13052v1  [cs.LG]  25 Oct 2021Can Q-Learning be Improved with Advice?\nNoah Golowich∗Ankur Moitra†\nOctober 25, 2021\nAbstract\nDespiterapidprogressintheoreticalreinforcementlearning(RL) overthelastfewyears,most\nof the known guarantees are worst-case in nature, failing to take advantage of structure that\nmay be known a priori about a given RL problem at hand. In this paper we address the question\nof whether worst-case lower bounds for regret in online learning of Markov decision processes\n(MDPs) canbe circumventedwhen informationaboutthe MDP,in the formofpredictionsabout\nitsoptimal Q-valuefunction, isgiventothealgorithm. Weshowthatwhenthe pre dictionsabout\nthe optimal Q-valuefunction satisfyareasonablyweakcondition wecall distillation , then wecan\nimprove regret bounds by replacing the set of state-action pairs w ith the set of state-action pairs\non which the predictions aregrosslyinaccurate. This improvementh olds for both uniform regret\nbounds and gap-based ones. Further, we are able to achieve this p roperty with an algorithm\nthat achieves sublinear regret when given arbitrary predictions (i.e ., even those which are not\na distillation). Our work extends a recent line of work on algorithms with predictions , which\nhas typically focused on simple online problems such as caching and sch eduling, to the more\ncomplex and general problem of reinforcement learning.\n∗MIT CSAIL. Email: nzg@mit.edu . Supported by a Fannie & John Hertz Foundation Fellowship an d an NSF\nGraduate Fellowship.\n†MIT Math, SDSC and CSAIL. Email: moitra@mit.edu . This work was supported in part by a Microsoft\nTrustworthy AI Grant, NSF CAREER Award CCF-1453261, NSF Lar ge CCF1565235, a David and Lucile Packard\nFellowship and an ONR Young Investigator Award.\n1\n\nContents\n1 Introduction 3\n1.1 Model overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Overview of results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Warm up: Stochastic multi-armed bandits . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.4 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7\n2 Preliminaries 9\n3 Learning in MDPs with predictions: main results 10\n3.1 Properties of the predictions /tildewideQ. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.2 Main theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .12\n4 Algorithm overview 14\n4.1 Algorithm description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2 How to choose /hatwide∆k. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n5 Proof overview 17\n5.1 Clipped range functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.2 Worst-case regret bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.3 Exploration-constrained exploitation cutoﬀs . . . . . . . . . . . . . . . . . . . . . . . 22\n5.4 Proofs for /tildewideQan approximate distillation . . . . . . . . . . . . . . . . . . . . . . . . . 22\n5.5 Proof of Theorem 3.1: implicit- λbound . . . . . . . . . . . . . . . . . . . . . . . . . 24\n6 Proofs for worst-case result 24\n6.1 Bounds on conﬁdence intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n6.2 Range functions and clipped range functions . . . . . . . . . . . . . . . . . . . . . . . 30\n6.3 Bounding the clipped range functions . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n6.4 Establishing the robustness regret bounds . . . . . . . . . . . . . . . . . . . . . . . . 39\n7 Proofs for approximate distillation bound 44\n7.1 Bounding the number of exploration episodes . . . . . . . . . . . . . . . . . . . . . . 45\n7.2 Bounding the value functions Rk\nh,/tildewideQk\nh,/tildewideVk\nh. . . . . . . . . . . . . . . . . . . . . . . . . 49\n7.3 Additional bounds on Q- andV-value functions . . . . . . . . . . . . . . . . . . . . . 52\n7.4 Regret bounds for approximate distillation . . . . . . . . . . . . . . . . . . . . . . . . 53\n8 Proofs of main theorems 60\nA Miscellaneous lemmas 62\nB Proof for bandit case 62\n2\n\n1 Introduction\nThe study of worst-case algorithm design has traditionally been a mainstay of much of computer\nscience, leading to provable and eﬃcient algorithms for var ious tractable problems. However, many\nproblems encountered in practice are often intractable, in the sense that eﬃcient algorithms for\nthem would violate widely held complexity theoretic hypoth eses, or there are strong unconditional\nlower bounds on the amount of data required to achieve desire d error bounds. As such, the study\nof beyond-worst case algorithm design [ Rou21], which aims to use additional information about\nthe structure of problem instances to improve algorithms’ g uarantees, has attracted signiﬁcant\nattention in recent years.\nAn exciting approach to beyond-worst case algorithm design is to assumethat the algorithm has\naccess to certain predictions regarding the nature of the problem instance at hand. For exa mple,\nwhile millions of samples may be required in order to teach a h umanoid robot to walk starting from\nscratch, physical approximations of the robot’s dynamics c an be used to furnish an approximately\noptimal policy, viewed as a prediction about the optimal pol icy. Then, starting from this predicted\npolicy, we can algorithmically ﬁne-tune it using relativel y few samples. This general approach,\nknown as algorithm design with predictions (oradvice), aims to improve an algorithm’s guarantees,\nsuch as by reducing sample complexity, when it is given acces s to accurate predictions. It has\nbeen studied from a theoretical perspective in several rece nt works for online problems such as the\nski rental problem, scheduling, caching, and many others [ MV20]. In this paper we address the\ndesign of algorithms with predictions for the much broader p roblem of reinforcement learning, in\nparticular the setting of no-regret online learning in tabu lar Markov decision processes (MDPs).\nIn turn, MDPs can be used to model learning problems in a pleth ora of settings including, for\ninstance, personalized medicine, optimal control, and mar ket design [ SB18].\n1.1 Model overview\nWe consider the setting of a tabular ﬁnite-horizon episodic MDP with a ﬁnite state space Sconsist-\ning ofSstates, a ﬁnite action space Aconsisting of Aactions, and a horizon of length H[AJK21].\nIn this setting, the rewards and transitions of the MDP are un known, but the learning algorithm\nhas the ability to simulate trajectories in the MDP correspo nding to policies of its choice. In total\nthe learner simulates K∈Ntrajectories, each of which consists of Hsteps; the total number of\nsamples is then T:=KH. The learner aims to minimize the regret, namely the diﬀerenc e between\nthe reward it would have received had it always followed the o ptimal policy and its actual aggregate\nreward; we refer the reader to Section 2for additional preliminaries. In Deﬁnition 1.1below, we\nintroduce the setting of RL with Q-value predictions , namely where the algorithm is given access\nto predictions /tildewideQof the optimal Q-value function Q⋆\nh(x,a). Recall that for a state x∈S, action\na∈A, and step h∈[H],Q⋆\nh(x,a) denotes the cumulative expected reward when action ais taken\nat statexand step h, and thereafter the optimal policy is followed.\nDeﬁnition 1.1 (RL with Q-value predictions) .We assume the learning algorithm is given, at the\nonset of its interaction with the MDP, access to a collection ofpredictions/tildewideQ= (/tildewideQ1,...,/tildewideQH), where\neach/tildewideQh:S×A→ Rrepresents a prediction for the optimal Q-value function Q⋆\nh.\nFor many important applications of reinforcement learning (RL), predictions /tildewideQas in Deﬁnition\n1.1may be readily available to a learning algorithm; for instan ce:\n3\n\n•Inrobotics, powerfulphysics-basedsimulationenginesal loweasycollection oflargeamountsof\ndatafromsimulatedenvironments, butthelearnedpolicies fromthesesimulatedenvironments\noften do not transfer directly to real-world environments d ue to factors such as measurement\nerror and mis-speciﬁcation of the simulation parameters. T o bridge this gap, information\ngleaned from learning in the simulated environment can be us ed as a prediction to be ﬁne-\ntuned when interacting with the real-world environment. Th is general approach (sometimes\ncalledsim-to-real ) has attracted much attention recently, such as in [ RVR+18,CHM+19].\n•In applications of RL to healthcare, Q-learning based methods (such as deep Q-learning) are\ncommonly employed [ YLN20]. It is therefore reasonable to expect that predictions /tildewideQfor a\ngiven task can be computed based on data collected for simila r tasks, such as from a previous\niteration of a clinical trial to treat a particular disease. Such ideas were used in [ LGKM20 ],\nwhere various parameters to an algorithm in a mobile health t rial,HeartSteps V2 , were set\nbased on data collected in an earlier iteration, HeartSteps V1 .1\n•More broadly, our framework provides an approach for the rel ated problems of multi-task\nlearning andtransfer learning in RL [TS09,ZLZ21]. These problems consider an RL agent\nwhichwishestoperformwell onmultiplerelated tasks (e.g. , arobotmovinginanenvironment\nwith gradually changing obstacles) throughout its lifetim e. An estimate of Q⋆for earlier tasks\n(e.g., as obtained by Q-learning) may be used as the input predictions /tildewideQfor later tasks. As\ndiscussed in Section 1.4, there has been a large body of empirical work devoted to impr oving\ntransfer learning in RL. Some of this work considers the reus e of certain representations of\nan MDP such as the Q-value function; our results can thus be interpreted as a the oretical\njustiﬁcation for such techniques.\nIn this paper we address the following question(s): Is it possible to leverage prior knowledge,\nin the form of access to predictions /tildewideQas in Deﬁnition 1.1, to show a regret bound that beats the\nworst-case? Moreover, can we achieve such a result with an al gorithm that still obtains sublinear\nregret when the predictions /tildewideQare arbitrary?\n1.2 Overview of results\nWe begin by reviewing known results regarding worst-case re gret in online learning for tabular\nMDPs. In this paper we prove both uniform regret bounds, whic h depend only on the parameters\nS,A,Hof the MDP as well as the number of samples T, as well as instance-dependent gap-based\nbounds, which we proceed to explain. The gapfor action aat state xand step his deﬁned to\nbe ∆h(x,a) =V⋆\nh(x)−Q⋆\nh(x,a), where V⋆\nhandQ⋆\nhare the optimal value function and Q-value\nfunction, respectively.2∆h(x,a) denotes the marginal loss incurred, relative to the optima l policy,\nwhen action ais taken at state xand step h. In this section (including in Theorems 1.1and1.2)\nwe make the simplifying assumption that for each ( x,h) there is a unique optimal action a(this\nassumption is relaxed in the full statements of our results i n Section 3). In this case [ XMD21]\n1The algorithm used was an ad hoc approach tuned to the particu lar task, rather than Q-learning.\n2Q⋆\nhwas deﬁned previously, and V⋆\nh(x) = max a∈A{Q⋆\nh(x,a)}; see Section 2for further details.\n4\n\nexhibit an algorithm, AMB, which satisﬁes the following regret guarantee:\nRegretT≤/tildewideO\nmin\n\n√\nH5SAT,/summationdisplay\n(x,a,h)∈S×A× [H]:\n∆h(x,a)>0H5\n∆h(x,a)\n\n\n, (1)\nThe regret bound ( 1) is optimal up to lower-order terms in the following sense: t he uni-\nform regret bound of /tildewideO(√\nH5SAT) matches the minimax regret in tabular MDPs, /tildewideO(√\nH2SAT)\n[JAZBJ18 ,ZZJ20], up to a factor of√\nH3.3Moreover, [ SJ19, Proposition 2.2] shows that a term\nof the form/summationtext\n(x,a,h):S×A×[H]:\n∆h(x,a)>0logK\n∆h(x,a)must appear in the gap-based regret bound in ( 1).\nOurﬁrstmain resultaddresses thefollowing question: Suppose the learning algorithm has access\nto predictions /tildewideQwhich are accurate on an unknown set of many state-action pai rs. Then can we\nimprove upon the worst-case regret bound ( 1), as if we had fewer state-action pairs to begin with?\nWe show an aﬃrmative answer to this question, replacing the s et of all state-action pairs with the\nset of those for which /tildewideQis inaccurate. In order for our improved bounds to “kick in”, though, it\nis necessary that the predictions /tildewideQsatisfy an additional property, which we formalize as being a\n(approximate) distillation ofQ⋆(Deﬁnition 3.1). We focus on the case of exact distillation here,\nwhich corresponds to ǫ= 0 in Deﬁnition 3.1: we say that /tildewideQis a distillation of Q⋆if for each\nstatex∈Sand step h∈[H], letting π⋆\nh(x) denote the optimal action at ( x,h), it holds that\n/tildewideQh(x,π⋆\nh(x))≥Q⋆\nh(x,π⋆\nh(x)). Intuitively, /tildewideQ“picks out” the good action π⋆\nh(x) at (x,h).4Of\ncourse, there could be additional actions a′for which/tildewideQh(x,a′) is equally large, but for which a′is\nvery much suboptimal at ( x,h). Theorem 1.1shows that when the predictions are a distillation of\nQ⋆, the regret bound ( 1) can be improved by replacing the set S×A× [H] with a much smaller\nset consisting of tuples ( x,a,h) for which/tildewideQh(x,a) is grossly inaccurate:\nTheorem 1.1 (Simpliﬁed/informal version of Theorem 3.1, item2).Suppose that /tildewideQis guaranteed\nto be a distillation of Q⋆. Then there is an algorithm ( QLearningPreds , Algorithm 15) which\nachieves regret\n/tildewideO\nmin\n\n/radicalbig\nH5T·|F|,/summationdisplay\n(x,a,h)∈FH4\n∆h(x,a)\n\n\n, (2)\nwhere\nF:=/braceleftig\n(x,a,h)∈S×A× [H] :/tildewideQh(x,a)> V⋆\nh(x)or/parenleftig\na/\\⌉}atio\\slash=π⋆\nh(x)and/tildewideQh(x,a)≥V⋆\nh(x)/parenrightig/bracerightig\n.(3)\nThe regret bound ( 2) of Theorem 1.1depends on the set Fof action-state pairs ( x,a,h) for\nwhich/tildewideQh(x,a) is larger than the optimal value at ( x,h), namely V⋆\nh(x) = max a∈AQ⋆\nh(x,a). Recall\nthat since the distillation property requires that /tildewideQh(x,π⋆\nh(x))≥Q⋆\nh(x,π⋆\nh(x)) for all ( x,h), if\n/tildewideQh(x,π⋆\nh(x))/\\⌉}atio\\slash=Q⋆\nh(x,π⋆\nh(x)), then ( x,π⋆\nh(x),h)∈F. In the full version of Theorem 3.1we relax\n3In this paper we generally disregard factors polynomial in Hand log(SAT), and do not attempt to optimize the\ndependence of our own bounds on Hand log(SAT).\n4We give an example in Section 3.1showing that this assumption is necessary to beat minimax lo wer bounds.\n5The parameter λis set to 0 for the purposes of Theorem 1.1.\n5\n\nthe condition of exact equality by allowing for an approxima te version of distillation (Deﬁnition\n3.1) and an approximate version of the set Fwhich set call the fooling set (Deﬁnition 3.2).\nTo complement Theorem 1.1, it is desirable to have a single algorithm which obtains non trivial\nregret bounds (i.e., sublinear regret) for arbitrary predi ctions/tildewideQ, i.e., even those which are not an\n(approximate) distillation of Q⋆, whichalsoobtains improved regret bounds (such as ( 2)) when/tildewideQ\nis an approximate distillation. The former guarantee is oft en known as robustness in the literature\non algorithms with predictions [ LV20,MV20]. Robustness in this context is well-motivated since\nthe predictions are often generated by an ad hoc procedure wi th few provable guarantees (such\nas the use of deep RL techniques on a simulated environment to estimate/tildewideQfor use in the real-\nworld environment), making them liable to be grossly inaccu rate. Theorem 1.2below gives such\na guarantee for the case of uniform regret bounds; Theorems 3.1and3.2provide more general\nrobustness bounds that cover the gap-based case as well.\nTheorem 1.2 (Simpliﬁed/informalversionofTheorem 3.1, uniformversion) .There is an algorithm\n(QLearningPreds , Algorithm 1) which satisﬁes the following two guarantees, when given as input\na parameter λ∈/parenleftig\nSAH4\nT,1/parenrightig\nand predictions /tildewideQ:\n1. For an arbitrary choice of /tildewideQ, the regret is /tildewideO/parenleftbigg/radicalig\nTSAH10\nλ/parenrightbigg\n.\n2. If the predictions /tildewideQare a distillation of Q⋆, then the regret is\n/tildewideO/parenleftig√\nλ·SATH10+/radicalbig\n|F|·TH5/parenrightig\n, (4)\nwhereFwas deﬁned in ( 3).\nAs was the case for Theorem 1.1, the notion of distillation and the set Fare relaxed to their\napproximate analogues in Theorems 3.1and3.2. Notice that there is a tradeoﬀ (mediated by\nthe parameter λ) in Theorem 1.2between the regret for arbitrary /tildewideQ(i.e., the robustness) and\nthe improved regret bound for /tildewideQthat is an ǫ-approximate distillation of Q⋆. This is a common\noccurrence in the study of algorithms with predictions, occ urring, for instance, in the ski rental\nproblem [ PSK18,WZ20] and the problem of non-clairvoyent scheduling [ PSK18,WZ20].\n1.3 Warm up: Stochastic multi-armed bandits\nIn this section we give a brief overview of the techniques use d to prove our regret upper bounds;\na detailed description of the algorithm is given in Section 4, and a more in-depth overview of the\nproof is given in Section 5. As a warm-up, we begin by addressing the easier case of multi -armed\nbandits: in this case there is a single state, and for each act ion (also known as an arm)a∈A,\nwe denote the expected reward of taking aasQ⋆(a).6Again we assume that there is a unique\noptimal action, denoted by a⋆. The algorithm receives at onset a function /tildewideQ:A→[0,1] denoting\npredictions for the mean reward of each arm. Moreover, /tildewideQis a distillation of Q⋆if/tildewideQ(a⋆)≥Q⋆(a⋆).\nThe below proposition specializes Theorem 1.2to the multi-armed bandit setting.\n6In the bandit setting, the reward received upon taking actio nais a random variable; this is in contrast to the\nfull RL setting where we assume the immediate rewards r(x,a) are deterministic. This discrepancy does not lead to\nany signiﬁcant diﬀerences in the algorithm or analysis, tho ugh.\n6\n\nProposition 1.3 (Bandit case) .There is an algorithm ( BanditPreds , Algorithm 5) which satisﬁes\nthe following two guarantees, when given as input a paramete rλ∈/parenleftbigA\nT,1/parenrightbig\nand predictions /tildewideQ:\n1. If the predictions /tildewideQare a distillation of Q⋆, then the regret is /tildewideO(/radicalbig\n|G|·T+√\nλ·AT), where\nG:=/braceleftig\na∈A\\{a⋆}:/tildewideQ(a)≥Q⋆(a⋆)/bracerightig\n.\n2. For an arbitrary choice of /tildewideQ, the regret is /tildewideO/parenleftbigg/radicalig\nTA\nλ/parenrightbigg\n.\nFor simplicity we have only stated uniform regret bounds in P roposition 1.3, but gap-based\nregret bounds specializing Theorems 3.1and3.2to the bandit case may readily be derived using\nsimilar techniques. Algorithm 5, which establishes Proposition 1.3, generally speaking aims to\nchoose the action awhich maximizes /tildewideQ(a). Of course, when /tildewideQis not accurate (e.g., because it is\nnot a distillation or Gis nonempty) Algorithm 5must make the following modiﬁcations:\n•To handle non-optimal actions awhich are in the set G(i.e., actions for which /tildewideQpredicts\nthem as having higher reward than a⋆), we maintain both upper and lower conﬁdence bounds\nfor the mean reward of each action a. For each twe then project /tildewideQ(a) onto the interval\n[Qt(a),Qt(a)] and use the resulting projected value instead of /tildewideQ(a).\n•Even with the use of upper and lower conﬁdence bounds, we coul d run into the following\ndiﬃculty: if/tildewideQ(a) =Q⋆(a) for all a/\\⌉}atio\\slash=a⋆, but/tildewideQ(a⋆)≪Q⋆(a⋆) (which can happen when /tildewideQ\nis not a distillation), then choosing the action ato maximize /tildewideQ(a) would simply choose the\nsecond-best action at all time steps, thus incurring linear regret. To deal with this situation,\nwe insert an initial exploration phase consisting of λTtime steps, in which we choose an\naction with the highest upper conﬁdence bound (as per the UCB algorithm [ LS20, Chapter\n7]). If/tildewideQ(a⋆) is signiﬁcantly sub-optimal, this initial exploration ph ase will discover that and\nsubsequently learn to ignore the prediction /tildewideQ(a⋆) (i.e., round it up to Qt(a⋆), which, over\ntime, will approach Q⋆(a⋆)).\nThe case of full RL (i.e., learning in MDPs) requires signiﬁc ant innovation beyond the above\ntechniques for the multi-armed bandit case. At a high level, this occurs because errors in /tildewideQcan\ncompound over multiple steps hin the standard Q-learning updates. To handle such challenges,\nwe have to use a more sophistocated rule to modify /tildewideQ(a) over time than simply projecting it onto\n[Qt(a),Qt(a)]. Additionally, the initial exploration phase described above must be made state-\nspeciﬁc, meaning that diﬀerent states may leave the exploration phas e at diﬀerent times, according\nto the current value estimates at each respective state. We r efer the reader to Sections 4and5for\nfurther details.\n1.4 Related work\nAlgorithms with predictions Many recent works studying algorithms with predictions hav e\nprimarily focused on relatively speciﬁc online problems in cluding the ski rental problem [ PSK18,\nWZ20], scheduling [ PSK18,WZ20,Mit19b,LLMV19 ], caching [ LV20,Roh19], design of bloom\nﬁlters [KBC+18,Mit19a], and revenue optimization [ MV17]; see also [ MV20] for an overview of the\nabove papers. In many of these problems, the performance par ameter optimized by the algorithm\n(and improved with access to predictions) is the competitive ratio , namely the ratio between a cost\n7\n\nmeasure speciﬁc to the problem and the optimal cost in hindsi ght, rather than the regret. There\nhas also been a fruitful line of work showing that by using pre dictions it is possible, using variants\nofoptimistic mirror descent , to signiﬁcantly decrease the regretin settings including online linear\noptimization [ HK10,RS12,RS13,SL14,MY15,BCKP20 ] and contextual bandits [ WLA20]. As\nsome of these works include the case of bandit feedback, they might seem to generalize Proposition\n1.3; however, this is not the case, since they face the signiﬁcan t limitation that a prediction is\nrequired by the algorithm at each time step, and the regret bo und depends on the aggregate\ndistance between the predictions and the realized values of the cost vectors or rewards over all time\nsteps. In the setting of stochastic multi-armed bandits, th is would require the predictions to track\nthe noise of the realized reward over all Ttime steps; in contrast, Proposition 1.3only requires a\nsingle set of predictions which must be close to the mean rewa rd vector.\nTransfer learning in RL More closely related to our results is a collection of work wh ich\nproposes to solve the problem of transfer learning in RL [TS09,ZLZ21] by reusing information\n(such as Q-values) from certain RL tasks in order to solve related RL ta sks. In the particular case\nofQ-value reuse, the Q-values for each successive task may be initialized as some f unction (e.g., the\nmean) of the Q-values from the previous tasks; these Q-values are then updated over the course of\nthe learning procedure for the current task [ Sin92,ANTH94 ,TY03,TWSM05 ,TSL09]. Many of\nthese papers show that doing so outperforms an initializati on ofQwhich is agnostic to previous\ntasks. These works, however, are purely empirical in nature , with no supporting theory.\nVeryrecentlytherehasbeensomeeﬀorttoperformtheoretica l analysesforsuchtransferlearning\ntechniques; [ TST21] shows that if the algorithm is given at onset predictions /tildewideQwhich are known\nto be equal to Q⋆at all state-action pairs except a single known state-action pair at step h= 1,\nthen it is possible to achieve regret /tildewideO(√\nH2T) usingQ-learning (thus eliminating the dependence\nonSA). Additionally, the recent work [ ZW21] shows that if Magents are interacting with separate\nMDPs whose optimal Q-value functions are known to be ǫ-close in ℓ∞distance, then by sharing\ninformation about their respective MDPs, they can decrease their aggregate regret by a factor of√\nM(in the uniform case) or M(in the gap-based case). Unlike our work, [ ZW21] does not show\nthat minimax regret boundscan bebeaten for a single MDP; mor eover, both[ TST21,ZW21] donot\nconsider any notion of robustness nor do they allow relaxati ons to the ℓ∞-closeness of the Q-value\nfunctions.\nTheoretical RL background The minimax optimal regret for online learning in tabular MD Ps\nis (up to polylogarithmic factors) Θ(√\nSATH2); the lower bound is shown in [ JAZBJ18 ], and the\nupper bound is known for both model-based algorithms such as UCBVI[AOM17], as well as the\nmodel-free algorithm UCB-Advantage [ZZJ20] (which is a variant of the Q-learning algorithm).\nNon-asymptotic gap-based upper bounds for tabular MDPs wer e shown in [ SJ19] using the model-\nbasedStrongEuler algorithm, a variant of EULER[ZB19]; additional algorithms achieving gap-\nbased bounds were shown in [ LSSS20,YYD21,XMD21]. Our gap-based bounds are based on the\ntechniques in [ XMD21]. Very recently some works [ DMMZ21 ,TPL21,WSJ21] have derived new\ninstance-dependent bounds in RL, such as by making alternat ive deﬁnitions of gaps; using insights\nfrom these works to improve our gap-based bounds is an intere sting direction left for future work.\nThe books [ AJK21,LS20] contain a more comprehensive overview of the ﬂurry of recen t work in\ntheoretical RL.\n8\n\n2 Preliminaries\nWe consider the setting of a tabular ﬁnite-horizon episodic Markov decision process (MDP) M=\n(S,A,H,P,r), whereSdenotes the (ﬁnite) state space, Adenotes the (ﬁnite) action space, H∈N\ndenotes the horizon, P= (P1,...,PH) denotes the transitions, and r= (r1,...,rH) denotes the\nreward functions. In particular, for each h∈[H],Ph(x′|x,a) (forx,x′∈S,a∈A) denotes the\nprobability of transitioning to x′fromxat stephwhen action ais taken; and rh(x,a) denotes the\nreward received when at state xand step hwhen action ais taken. We assume each reward lies\nin [0,1], i.e.,rh:S×A→ [0,1]. We write S:=|S|andA:=|A|. Apolicyπis a collection of\nmappings πh:S→A, for each h∈[H].7\nIn each episode, a state x1is picked by an adversary. For each h∈[H], the agent observes\nthe state xh, picks an action ah∈A(usually given according to some policy π, i.e.,ah=πh(xh)),\nreceives reward rh(xh,ah), and transitions to a new state xh+1, drawn according to Ph(·|xh,ah).\nUpon receiving the reward rH(xH,aH) at the ﬁnal step H, the episode ends. For a policy π, we let\nVπ\nh:S→Rdenote the V-value function at steph; in particular, Vπ\nh(x) gives the expected total\nreward received by the agent when it starts in state xat stephand thereafter follows policy π. In\na similar manner, we let Qπ\nh:S×A→ Rdenote the Q-value function at steph;Qπ\nh(x,a) gives the\nexpected total reward received by the agent when it starts in statexat steph, takes action a, and\nthereafter follows policy π. Formally, Vπ\nhandQπ\nhare deﬁned as follows:\nVπ\nh(x) :=Eπ/bracketleftiggH/summationdisplay\nh′=hrh(xh′,ah′)|xh=x/bracketrightigg\n, Qπ\nh(x,a) =Eπ/bracketleftiggH/summationdisplay\nh′=hrh′(xh′,ah′)|xh=x,ah=a/bracketrightigg\n,\nwhereEπ[·] denotes that πis used to choose the action at each state.\nWe letπ⋆denote the optimal policy , namely the policy which maximizes Vπ⋆\nh(x) for all ( x,h)∈\nS×[H]. We write V⋆\nh(x) :=Vπ⋆\nh(x) for all x,h. With slight abuse of notation, we let Phde-\nnote the Markov operator Ph:RS→RS×A, deﬁned by, for any value function Vh+1:S →R,\n(PhVh+1)(x,a) :=Ex′∼Ph(·|x,a)[Vh+1(x′)]. The following relations ( Bellman equation andBellman\noptimality equation )arestandardandfolloweasilyfromthedeﬁnitions: forall (x,a,h)∈S×A× [H],\n\n\nVπ\nh(x) =Qπ\nh(x,πh(x))\nQπ\nh(x,a) = (rh+PhVπ\nh+1)(x,a)\nVπ\nH+1(x) = 0and\n\nV⋆\nh(x) = max a∈AQ⋆\nh(x,a)\nQ⋆\nh(x,a) = (rh+PhV⋆\nh+1(x,a)\nV⋆\nH+1(x) = 0.\nFor some K∈N, over a series of Kepisodes, the RL agent interacts with the MDP Mas follows:\nfor each k∈K, the agent chooses a policy πk, and applies the policy πkin the MDP to obtain a\ntrajectory (xk\n1,ak\n1,rk\n1),...,(xk\nH,ak\nH,rk\nH), as explained above; here rk\nh:=rh(xk\nh,ak\nh)∈[0,1] denotes\nthe reward received at step h. We measure the agent’s performance with the regret:\nRegretK:=K/summationdisplay\nk=1E/bracketleftig\nV⋆\n1(xk\n1)−Vπk\n1(xk\n1)/bracketrightig\n,\nwhere the expectation is taken over the randomness of the env ironment (in particular, the policies\nπkare random variables since they depend on trajectories from previous episodes).\n7Note that our setting of ﬁnite-horizon MDPs is equivalent to the setting of layeredMDPs in the literature (e.g.,\n[XMD21]), where a diﬀerent copy of the state space Sis created for each step h, and transitions from states in layer\nhalways to go states in layer h+1.\n9\n\nNotation for gap-based bounds In this paper we will derive gap-dependent regret bounds; fo r\n(x,a,h)∈S×A× [H], thegap at(x,a,h) is deﬁned as:\n∆h(x,a) :=V⋆\nh(x)−Q⋆\nh(x,a).\nThe gap denotes the marginal sub-optimality in reward the ag ent suﬀers as a result of taking action\naat statexand step h. Forǫ >0, we write, for ( x,h)∈S×[H],\nAopt\nh,ǫ(x) :={a∈A: ∆h(x)≤ǫ}\nto denote the set of actions with gap at most ǫat (x,h). Forx∈Sandh∈[H], deﬁne ∆ min,h(x) :=\nmina/\\⌉gatio\\slash∈Aopt\nh,0(x){∆h(x,a)}to be the minimum positive gap at ( x,h). Also deﬁnethe minimum positive\ngap in the entire MDP to be ∆ min:= min x,a,h:∆h(x,a)>0{∆h(x,a)}. Following [ XMD21], our gap-\nbased bounds will have a term depending on the number of state -action pairs which are optimal\nand for which there is nota unique optimal action at that state, i.e., the size of the se t:\nAmul:={(x,a,h)∈S×A× [H] : ∆h(x,a) = 0 and|Aopt\nh,0(x)|>1}.\nPrior worst-case regret bound In the special case that each state has a unique optimal actio n\nwe discussed the worst-case regret bound ( 1) and prior work showing its optimality. In the general\ncase, [XMD21, Theorem B.1 & Corollary B.10] showed the following regret b ound:\nRegretT≤O\nH2SA+log(SAT)·min\n\nH5|Amul|\n∆min+/summationdisplay\n(x,a,h)∈S×A× [H]:\n∆h(x,a)>0H5\n∆h(x,a),√\nH5SAT\n\n\n.\n(5)\n[XMD21, Theorem 5.1] shows that a term of the form log K·|Amul|\n∆minis necessary, even in the presence\nof the term/summationtext\n(x,a,h)∈S×A× [H]:\n∆h(x,a)>01\n∆h(x,a)of the regret bound. Thus, in general, the bound ( 5) cannot\nbe improved by more than poly( H,log(SAT)) factors.\nAdditional notation Given a real number x, let [x]+denotexifx >0, and 0 otherwise. We\nwill write T=HKto denote the total number of samples over Kepisodes; note that RegretK≤T\nalways holds. We also set ι:= log(SAT). For (x,a,h,k)∈S×A× [H]×[K],Nk\nh(x,a) denotes\nthe number of episodes before episode kin which ( x,a,h) is visited, i.e., action awas taken at\nstatexand step h(Nk\nh(x,a) is also deﬁned in step 2(b)iof Algorithm 1). For integers i≥1 and\n(x,a,h)∈S×A× [H], we let ki\nh(x,a) denote the episode kwhich is the ith episode that ( x,a,h)\nwas visited. If no such episode exists, we set ki\nh(x,a) =K+1 as a matter of convention.\n3 Learning in MDPs with predictions: main results\n3.1 Properties of the predictions /tildewideQ\nOur main result shows that in the presence of arbitrary predi ctions/tildewideQ, we are able to obtain a\nsublinearregretboundforouralgorithm QLearningPreds ,andmoreover, if /tildewideQsatisﬁesanadditional\n10\n\nproperty, then we can obtain an improved regret bound that ca nbeatthe minimax regret bounds\nfor learning in MDPs (i.e., ( 5)), replacing the space S×A× [H] with a smaller space representing\nthe set of state-action pairs where /tildewideQis inaccurate (consistency). Deﬁnition 3.1below captures the\nadditional property (referred to as being an approximate distillation ofQ⋆) that/tildewideQneeds to satisfy\nin order to obtain improved regret bounds.\nTo motivate the deﬁnition, consider the setting where there is a single state x0andH= 1\n(which is equivalent to the stochastic multi-armed bandit p roblem). Moreover suppose there is a\nunique optimal action a⋆with reward 1 and all other A−1 actions have reward 1 −∆ for some\npositive ∆ <1/A. If we are given the predictions /tildewideQF\n1, where/tildewideQF\n1(x0,a) := 1−∆ for all a, then/tildewideQF\n1\nis only incorrect at a single action (namely, a⋆), but it provides no information about what a⋆is,\nand it is straightforward to show that, even given /tildewideQF\n1, the regret of any algorithm must be Ω( A/∆),\ngiving no improvement over the setting without predictions [LS20, Chapter 16]. On the other hand,\nconsider the predictions /tildewideQT\n1deﬁned as equal to Q⋆\n1except at a single (unknown) non-optimal action\na′.8Though both /tildewideQF\n1,/tildewideQT\n1both diﬀer from /tildewideQ⋆\n1at a single action, it will follow from Theorem 3.1\n(withλ=A/T) that given/tildewideQT\n1,QLearningPreds obtains themuch smaller regret boundof /tildewideO(1/∆).\nAs this example shows, a set of accurate predictions /tildewideQcannot entirely mitigate the exploration\nproblem: even if the predictions are accurate at nearly all s tates and actions, if they do not provide\nany information as to the identity of the optimal action at a g iven state (e.g., as for /tildewideQF\n1), then we\ncannot hope to beat existing regret bounds. The notion of approximate distillation , deﬁned below,\nformalizes the notion that /tildewideQmust provide information about the optimal action at each st ate:\nDeﬁnition 3.1 (Approximate distillation) .Consider a predicted Q-value function /tildewideQ∈R[H]×S×A.\nForǫ >0, we say that /tildewideQis anǫ-approximate distillation of the optimal value function Q⋆if the\nfollowing holds: for each ( x,h)∈S×[H], there is some a∈Aso that\n∆h(x,a)+[Q⋆\nh(x,a)−/tildewideQh(x,a)]+≤ǫ.\nIn words, Deﬁnition 3.1requires that for each ( x,h), there is some action awhich is nearly\noptimal and for which /tildewideQh(x,a) does not greatly underestimate the value of Q⋆\nh(x,a).\nNext we deﬁne the fooling set for any/tildewideQ, which is the set of tuples ( x,a,h) for which/tildewideQh(x,a)\ndiﬀers from Q⋆\nh(x,a) in a particular way:\nDeﬁnition 3.2 (Fooling set) .Given a set of predictions /tildewideQfor anyǫ1,ǫ2>0, we deﬁne the set of\n(ǫ1,ǫ2)-fooling tuples (x,a,h), denotedF(ǫ1,ǫ2)⊂S×A× [H], to be the set of tuples ( x,a,h) so\nthat\n/tildewideQh(x,a)−Q⋆\nh(x,a)≥∆h(x,a)−ǫ1≥ǫ2−ǫ1or/tildewideQh(x,a)> V⋆\nh(x)+ǫ2.\nIn this context, we will always have ǫ2> ǫ1>0.\nNotice that we could alternatively deﬁne the fooling set as t hose (x,a,h) for which|/tildewideQh(x,a)−\nQ⋆\nh(x,a)|> ǫ2−ǫ1; this set, however, is in general larger than F(ǫ1,ǫ2), and so usingF(ǫ1,ǫ2)\nallows us to obtain tighter regret bounds.\nOne of our results will also make use of the following assumpt ion on/tildewideQ:\nDeﬁnition 3.3 (Optimal fooling actions) .Forǫ′>0, we say that predictions /tildewideQlackǫ′-fooling\noptimal actions if there is no ( x,h) with multiple optimal actions (i.e., for which |Aopt\nh,0(x)|>1) so\nthat for some a∈Aopt\nh,0(x),/tildewideQh(x,a)> V⋆\nh(x)+ǫ′.\n8/tildewideQT\n1(x,a′) can be set to any real number.\n11\n\nNote that in the context of Deﬁnition 3.3,/tildewideQh(x,a)> V⋆\nh(x)+ǫ′implies that ( x,a,h)∈F(ǫ,ǫ′)\nfor anyǫ, explaining the terminology of the deﬁnition.\n3.2 Main theorems\nA common thread in the literature on algorithms with predict ions is an inherent tradeoﬀ between\nan algorithm’s robustness and its accuracy when it receives correct predictions (sometimes called\nconsistency ) [PSK18,WZ20]. Such a tradeoﬀ occurs in our setting too. To describe this t radeoﬀ,\nwe introduce a parameter λ∈(0,1): asλdecreases to 0, the regret in the presence of predictions\nwhich are an (approximate) distillation improves but the ro bustness (i.e., regret in the presence of\narbitrary predictions) worsens.\nλ-CostWe will be able to obtain both gap-based regret bounds and (in stance-independent) uni-\nform ones for both robustness and consistency in the presenc e of predictions. To simplify the\ndependence of these bounds on the parameter λintroduced above, we deﬁne the λ-costfor an\nMDPMas follows: given an MDP M, a value T∈Nand a value λ∈(0,1), theλ-cost ofM,\ndenoted CM,T,λ, is the following quantity:\nCM,T,λ:= min\n\n√\nλ·TSAH8ι, H7ι·\n/summationdisplay\n(x,a,h)∈S×A× [H]:a/\\⌉gatio\\slash∈Aopt\nh,0(x)1\n∆h(x,a)+|Amul|\n∆min\n\n\n(6)\nRecall that ι= log(SAT). Note that, ignoring poly( H,ι) factors, CM,T,λis in general no greater\nthan the worst-case regret bound ( 5): moreover, if the ﬁrst term in the minimum in ( 6) (i.e.,√\nλ·TSAH8ι) is much smaller than the second, then due to the factor of√\nλin this term, CM,T,λ\nwill be much smaller than the right-hand side of ( 5) (again, ignoring poly( H,ι) factors).\nExplicit- λguarantee Our ﬁrst main result is stated below; for simplicity, we pres ent here\nthe result under the additional assumption that each ( x,h) has a unique optimal action (i.e.,\n|Aopt\nh,0(x)|= 1); this assumption has been made previously in [ XMD21]. As we show in an extended\nversion of the theorem (see the version in Section 8), this assumption may be removed if we assume\nthat ∆ minis known to the algorithm; further, our second main result (T heorem3.2) avoids making\neither assumption altogether. Theorem 3.1states that the regret of QLearningPreds (Algorithm\n1) under arbitrary predictions /tildewideQis/tildewideO(H\nλ·CM,T,λ), whereas the regret under accurate predictions\n(i.e., predictions which are an approximate distillation) is the sum of /tildewideO(H·CM,T,λ) plus a quantity\nthat grows as the degree of accuracy of the predictions degra des.\nTheorem 3.1. Suppose that for each (x,h)there is a unique optimal action (i.e., |Aopt\nh,0(x)|= 1).\nThe algorithm QLearningPreds (Algorithm 1) with the DeltaIncr subroutine (Algorithm 4) with\nparameter/tildewide∆min= 0satisﬁes the following two guarantees, when given as input a parameter λ∈\n[0,1]and predictions /tildewideQ:\n1. Suppose λ≥SAH4\nT. Then for an arbitrary choice of input predictions /tildewideQ, the regret of\nQLearningPreds isO(Hι\nλ·CM,T,λ).\n12\n\n2. Fix any ǫ >0, and set ǫ′= 4ǫ·(H+1). When the input predictions /tildewideQare anǫ-approximate\ndistillation of Q⋆(Deﬁnition 3.1), the regret of QLearningPreds is\nO\nH2ι·CM,T,λ+ǫ′TH+min\n\n/radicalbig\nH5Tι·|F(ǫ′/2,ǫ′)|,/summationdisplay\n(x,a,h)∈F(ǫ′/2,ǫ′)H4ι\n[∆h(x,a)−ǫ′/2]+\n\n\n.(7)\nTo exhibit the full power aﬀorded by bounds in terms of CM,T,λ, we consider the following\nexample. Suppose the algorithm is promised that at least one of the following holds: either /tildewideQ=Q⋆\n(in particular, /tildewideQis a 0-approximate distillation),9or it holds that all non-zero gaps are at least\na constant, which implies that CM,T,λ≤poly(H,ι)·O(SA); however, which of these possibilities\nholds is unknown. Then by choosing λ=/radicalig\nSA\nTin Theorem 3.1(and using ǫ=ǫ′= 0), we obtain\na regret bound of poly( H,ι)·O(SA) (which is independent of K) in the case that /tildewideQ=Q⋆, and\na regret bound of poly( H,ι)·O(√\nSAT) in the other case. Thus we always manage to achieve\nregret at least as small as the minimax bound of /tildewideO(√\nH2SAT) (up to poly( H,ι) factors), and in\nthe former case can get a much-improved regret bound that is p olylogarithmic in K.\nImplicit- λguarantee Unlike in much of the literature on algorithms with predicti ons [MV20],\nthe quantity CM,T,λwhich appears in our regret bounds is in general unknown to the algorithm, as\nthe quantities ∆ h(x,a),|Amul|,∆minare all unknown. Therefore, the standard paradigm in which\na user chooses a parameter λand then runs an algorithm depending on λis somewhat less well-\nmotivated because the user does not have an explicit formula for how the choice of λinﬂuences\nthe regret bounds in the case when either the predictions are accurate or inaccurate. Therefore, in\nour next main result, Theorem 3.2, we adopt the alternative procedure in which the user instea d\ninputs a parameter R< T. Given R, the algorithm’s robustness (i.e., performance under arbi trary\npredictions) is guaranteed to be O(R), while the performance under accurate predictions grows\nwithCM,T,/hatwideλfor/hatwideλimplicitly chosen optimally so as to still guarantee regret O(R) in the worst case.\nTheorem 3.2. The algorithm QLearningPreds with theDeltaConst subroutine satisﬁes the fol-\nlowing two guarantees, when given as input a parameter R∈[SAH3,T\nSA]and predictions /tildewideQ:\n1. IfR≥CM,T,1, then for an arbitrary choice of input predictions /tildewideQ, the regret of QLearningPreds\nisO(R).\n2. Fix any ǫ >0, and set ǫ′= 4ǫ·(H+1). When the input predictions /tildewideQare anǫ-approximate\ndistillation of Q⋆(Deﬁnition 3.1) and lack ǫ′-fooling optimal actions (Deﬁnition 3.3), the\nregret of QLearningPreds is\nO\nH·CM,T,/hatwideλ+ǫ′TH+min\n\n/radicalbig\nH5Tι·|F(ǫ′/2,ǫ′)|,/summationdisplay\n(x,a,h)∈F(ǫ′/2,ǫ′)H4ι\n[∆h(x,a)−ǫ′/2]+\n\n\n,\n(8)\nwhere/hatwideλ∈(0,1)is chosen so that1\n/hatwideλ·CM,T,/hatwideλ=R.\n9More generally, we could assume that /tildewideQis anǫ-approximate distillation and that F(ǫ′/2,ǫ′) is small.\n13\n\nWeremarkthat initem 2oftheabovetheorem, theadditionalassumptionthat /tildewideQlacksǫ′-fooling\noptimal actions is without loss of generality if we are purel y interested in obtaining gap-based\nbounds (namely, in getting the bound of the second term of the minimum in ( 8)). In particular, if\nfor some x,a,h, ∆h(x,a) = 0 and/tildewideQh(x,a)> V⋆\nh(x)+ǫ, then (x,a,h)∈F(ǫ′,ǫ′), meaning that one\nof the terms in the sum in ( 8) will beH4ι\n[0−ǫ′]+, which is inﬁnite.\n4 Algorithm overview\n4.1 Algorithm description\nOur algorithm, QLearningPreds (Algorithm 1) used in Theorems 3.1and3.2, is based loosely oﬀ of\ntheQ-learning algorithm [ JAZBJ18 ], and incorporates numerous additional aspects (includin g sev-\neral ideas from [ XMD21]) to eﬀectively use the predictions /tildewideQh(x,a). In this section we describe the\nmain ideas of the algorithm. At each episode k, thealgorithm maintains upperand lower boundson\ntheQ-value and V-value functions, denoted Qk\nh(x,a),Vk\nh(x,a) andQk\nh(x,a),Vk\nh(x,a), respectively.\nUnlike previous versions of Q-learning, our algorithm makes use of additional functions , denoted\nRk\nh(x,a),/tildewideQk\nh(x,a),/tildewideVk\nh(x), which may be interpreted as follows:\n•/tildewideQk\nh(x,a) is a reﬁnement of the predictions /tildewideQh(x,a) given to the algorithm as input; /tildewideQ1\nhis set\nto equal/tildewideQh(step1), and/tildewideQk\nhis reﬁned over time as the algorithm collects trajectories.\n•The values Rk\nh(x,a) are used in the process of reﬁning /tildewideQk\nh(x,a);Rk\nh(x,a) represents an ap-\nproximate upper bound on Q⋆\nh(x,a), assuming that the prediction /tildewideQis anǫ-approximate\ndistillation (Deﬁnition 3.1).\n•/tildewideVk\nh(x) is an upper estimate for the V-value function at a state xthat makes use of the reﬁned\npredictions/tildewideQk\nh(x,a).\nQLearningPreds additionally employs the technique of action elimination , maintaining sets Ak\nh(x)\n(deﬁned in step 2c) which for each x,h,kcontain the actions awhich could plausibly be optimal\nat the beginning of episode k(A1\nh(x) is initialized to all of Ain step1). Action elimination has\npreviously been used in bandit learning and reinforcement l earning when one must be robust to\nadversarial corruptions [ EDMM06 ,LML18,LSSS20], as well as to obtain gap-based regret bounds\n[XMD21]. Inouralgorithm, thesets Ak\nh(x) areusedfor bothof thesepurposes(wheretherobustness\nis with respect to the possible inaccuracy of the prediction s/tildewideQh). For convenience, we set Gk\nhto\ndenotethe set of states xfor which|Ak\nh(x)|= 1(meaning all butoneaction at xhas beeneliminated\nat the beginning of episode k; see step 2f).\nAfter being initialized in step 1ofQLearningPreds , the values Qk\nh,Qk\nh,Vk\nh,Vk\nh,/tildewideQk\nh,/tildewideVk\nh,Rk\nhare\nupdated in QLearningPreds in steps 2band2daccording to established updating procedures,\nnamely using exploration bonuses of bn=C0·/radicalbig\nH3ι/n(for some constant C0) and a learning rate\nofαn=H+1\nH+n, forn∈N[JAZBJ18 ,XMD21]. In particular, Vk\nh,Vk\nh,Qk\nh,Qk\nhare updated in step\n2baccording to the adaptive multi-step bootstrap technique o f [XMD21], which uses sequences of\nmultiple rewards (namely, at contiguous sequences of state s in which the optimal action has been\ndetermined) to perform the Bellman update. Our updates diﬀer slightly from those in previous\nworks in that we also maintain supplementary estimates qk\nh,qk\nh(steps2(b)ivand2(b)vi) to ensure\n14\n\nthatQk\nh,Vk\nhare non-increasing with respect to k, andQk\nh,Vk\nhare non-decreasing with respect to k\n(Lemma 6.3).\nThe purpose of maintaining Vk\nh,Vk\nh,Qk\nh,Qk\nhis primarily to obtain the robustness regret bounds\n(i.e., of1\nλ·CM,T,λ) in Theorems 3.1and3.2. On the other hand, the values /tildewideQk\nh,/tildewideVk\nh,Rk\nh, which are\nupdated in step 2dofQLearningPreds , are used to obtain improved regret bounds in the presence\nof accurate predictions. The updates here only use a single s tep to perform the Bellman update,\nas in the standard Q-learning algorithm [ JAZBJ18 ].\nFor future reference we deﬁne the following learning rate pa rameters used in the algorithm’s\nanalysis: for n≥i≥1, set\nα0\n0:= 1, α0\nn:= 0, αi\nn:=αin/productdisplay\nj=i+1(1−αj). (9)\nIntuitively, αi\nndenotes the impact of an update made the ith time a state-action pair ( x,a,h) is\nvisited on the value of any value function (e.g., Qk\nh,Qk\nh, etc.) when ( x,a,h) is visited for the nth\ntime. In the remainder of the section, we describe how QLearningPreds chooses its policies (step\n2a); the challenge of doing so leads to most of the algorithmic n ovelties in QLearningPreds .\nState-speciﬁcexploration & exploitation phases Ateachepisode k,QLearningPreds chooses\na policy πkby using the functions Qk\nh,Qk\nh,/tildewideQk\nhin thePolicySelection subroutine (Algorithm 2).\nA key challenge addressed in this step is that of obtaining a “ best of both worlds” guarantee\nwhich improves upon the minimax regret guarantee of /tildewideO(√\nSATHO(1)) (or, in the gap-based case,\npoly(H)·/tildewideO/parenleftig/summationtext\n(x,a,h)1\n∆h(x,a)+|Amul|\n∆min/parenrightig\n) in the case that the predictions /tildewideQare accurate, but still\nmanages to obtain sublinear regret when /tildewideQis arbitrarily inaccurate. QLearningPreds overcomes\nthis challenge by dividing the set of episodes in which we vis it each state xat each step hinto two\nphases:\n•In the ﬁrst phase, we employ exploration : whenever ( x,h) is visited during an episode kin\nthis phase, thepolicy πktakes an action a∈Ak\nh(x) which maximizes thegap between Qk\nh(x,a)\nandQk\nh(x,a) (this approach is slightly diﬀerent from the more standard U CB approach which\nchoosesato maximize Qk\nh(x,a) [JAZBJ18 ], but was used in [ XMD21] to obtain gap-based\nbounds; it is used in QLearningPreds for the same reason).\n•After a certain number of episodes, QLearningPreds will decide it has suﬃciently explored\nat the state ( x,h), and thus, when visiting ( x,h), it will choose an action ˆ a∈Ak\nh(x) which\nmaximizes the reﬁned predictions /tildewideQk\nh(x,ˆa)10. This second phase may be seen as a constrained\nexploitation phase: it attempts to exploit the predictions /tildewideQh, but the action ˆ ais constrained\nto lie in the action set Ak\nh(x). As explained below, any action a′atxwhich is very suboptimal\nwill be removed from Ak\nh(x) after a bounded number of episodes, which limits the impact of\ninaccurate predictions.\nWe emphasize that the partition into the two phases is state-speciﬁc ; namely, at any given episode,\nsome states may be in their exploration phase whereas others may be in their exploitation phase.\n10For technical reasons, ˆ ais actually chosen to maximize max {/tildewideQk\nh(x,ˆa),Qk\nh(x,ˆa)}\n15\n\nNotice that there is a tradeoﬀ between the lengths of the two p hases: if the ﬁrst phase, which\ndoes not make use of the predictions /tildewideQhand thus cannot outperform the minimax bounds, is too\nlong, then if the predictions /tildewideQare accurate we will not improve suﬃciently upon the minimax\nregret guarantee. On the other hand, if the ﬁrst phase is too s hort (or nonexistent), the following\nmay occur: suppose that the predictions /tildewideQare inaccurate in that for some state x, steph, and\nsub-optimal action a,/tildewideQh(x,a) is large, but /tildewideQh(x,a⋆) is small, where a⋆/\\⌉}atio\\slash=ais the unique optimal\naction at ( x,h) and satisﬁes V⋆\nh(x) =Q⋆\nh(x,a⋆)≫Q⋆\nh(x,a). Suppose for simplicity that A=\n{a,a⋆}and that/tildewideQh≡/tildewideQk\nh(which can approximately hold). Ideally the ﬁrst phase shou ld be long\nenough to eliminate afromAk\nh(x); this will happen when Qk\nh(x,a⋆) grows suﬃciently to be greater\nthanQk\nh(x,a). However, if the ﬁrst phase ends before this happens, then a t the beginning of the\nsecond phase, Ak\nh(x) =A, and so πk\nh(x) will be set to ain step2ofPolicySelection . Thus\nQLearningPreds would suﬀer linear regret.\nAn adaptive exploration-exploitation cutoﬀ QLearningPreds trades oﬀ the lengths of the\nexploration and exploitation phases described above accor ding to the input parameter λ(or, in the\ncase of Theorem 3.2,/hatwideλas determined by R). To describehow QLearningPreds makes this tradeoﬀ,\nwe begin by deﬁning the Q- andV-range functions (following the presentation of [ XMD21]). First,\nwe make a few additional deﬁnitions: for ( k,h)∈[K]×[H], for which xk\nh/\\⌉}atio\\slash∈Gk\nh, seth′(k,h)∈[H+1]\nto be the ﬁrst step h′after step hfor which xk\nh′/\\⌉}atio\\slash∈Gk\nh(if suchh′≤Hdoes not exist, then set\nh′(k,h) =H+ 1). Next, for n∈N, deﬁne the following parameters βn, which may be viewed as\naggregated versions of the exploration bonuses bi=C0/radicalbig\nH3ι/i(recall the deﬁnition of αi\nnin (9)):\nβ0:= 0, β n= 2n/summationdisplay\ni=1αi\nn·bi. (10)\nDeﬁnition 4.1 (Range function) .For (x,a,h,k)∈S×A× [H]×[K] for which x/\\⌉}atio\\slash∈Gk\nhand\na∈Ak\nh(x), deﬁne the rangeQ-function as follows: set ∆ Q0\nh(x,a) =H, and\n∆Qk\nh(x,a) :=min/braceleftigg\n∆Qk−1\nh(x,a), α0\nnH+βn+n/summationdisplay\ni=1αi\nn·∆Vki\nh\nh′(ki\nh,h)(xki\nh\nh′(ki\nh,h))/bracerightigg\nwheren=Nk\nh(x,a), ki\nh=ki\nh(x,a)∀i∈[n].\nMoreover, for ( x,h,k)∈S×[H]×[K] for which x/\\⌉}atio\\slash∈Gk\nh, deﬁne the rangeV-function as follows:\nset ∆V0\nh(x) =H, and\n∆Vk\nh(x) := min{∆Vk−1\nh(x),∆Qk\nh(x,a⋆)}fora⋆= argmax\na′∈Ak\nh(x)Qk\nh(x,a′)−Qk\nh(x,a′).\nFinally, deﬁne ∆ Qk\nH+1(x,a) = ∆Vk\nH+1(x) = 0 for all x,a,k.\nThe functions ∆ Qk\nh,∆Vk\nhshould be interpreted as upper bounds on the gap between the u pper\nand lower Q,Vvalues; note that they satisfy a similar recursion to Qk\nh−Qk\nhandVk\nh−Vk\nh(see\n(23)). Indeed, in Lemma 6.4below we show that ∆ Qk\nh,∆Vk\nhare upperboundson Qk\nh−Qk\nh,Vk\nh−Vk\nh,\nrespectively.\n16\n\nNow that we have deﬁned the range functions, the choice of pol icy at each ( x,h) (equivalently,\nthe choice of “exploration” and “constrained exploitation ” phases described above) is simple to\nstate:QLearningPreds maintains a parameter /hatwide∆kat each episode k, which represents a “target\nerror bound” that QLearningPreds hopes to obtain. The parameter /hatwide∆kadapts to the input\nparameter R(orλ) as well as the gap-based complexity of the given MDP. Given /hatwide∆kat episode k,\nthe policy πk\nhat each step his speciﬁed in ( 11) in the algorithm PolicySelection : following our\nterminology above, a state ( x,h) is declared to bein the “exploration” phase if ∆ ˘Vk\nh(x)> ϕh(/hatwide∆k)11\n(meaning there is still much uncertainy about the optimal va lue at (x,h) relative to/hatwide∆k), and is\ndeﬁned to be in the “constrained exploitation” phase otherw ise (i.e., ∆ ˘Vk\nh(x)≤ϕh(/hatwide∆k)). We\nwill show (in Lemmas 6.7and6.8) that ∆ ˘Vk\nh(x) is nonincreasing with respect to kandϕh(/hatwide∆k)\nis nondecreasing with respect to k; thus, each state can only move from the “exploration” to\n“constrained exploitation” phase.\n4.2 How to choose /hatwide∆k\nAs we discussed in the previous section, /hatwide∆kis chosen to adapt to the input parameter Rorλ. In\nthe setting of Theorem 3.2, where the user inputs a parameter Rrepresenting the target worst-case\nregret bound, the choice of /hatwide∆kis extremely simple (Algorithm 3,DeltaConst ): for all k, we set\n/hatwide∆k:=R/(KH). In the setting of Theorem 3.1, where the user inputs a parameter λspecifying\na trade-oﬀ between the worst-case and ideal-case settings, /hatwide∆kis set (in Algorithm 4,DeltaIncr )\nto a more complex expression which is a surrogate for the wors t-case regret bound1\nλCM,T,λ(thus\novercoming the challenge that the algorithm does not know CM,T,λ). This surrogate uses the frozen\nrange function (deﬁned in Deﬁnition 5.2), denoted ∆ ˚Qk\nh(x,a), as a proxy for the action-value gaps\n∆h(x,a), for all ( x,a,h)∈S×A× [H]. We refer the reader to Section 5.5for further details.\n5 Proof overview\nIn this section we overview the proofs of Theorems 3.1and3.2; we focus mainly on Theorem 3.2\nsince its proof is slightly simpler. At a high level, the key t ools needed in the proof of Theorem 3.2\nare as follows:\n1. First, we need to deﬁne the the clipped range functions (Deﬁnition 5.1), as in [XMD21], which\naid in proving gap-based bounds.\n2. To prove the O(R) regret bound for worst-case predictions (i.e., robustnes s, ﬁrst item of\nTheorem 3.2), we ﬁrst prove a regret decomposition (Lemma 6.15) showing that regret can\nbe bounded in terms of the clipped V-range functions.\nIn Lemma 6.13, our main technical lemma for the worst-case regret bound, w e then show\nhow to bound the clipped V-range functions in the presence of arbitrary predictions /tildewideQusing\ncertainmonotonicity properties of the value functions.\n3. To establish the improved regret bounds for the case that /tildewideQis an approximate distillation\n(second item of Theorem 3.2), we ﬁrst need to bound the number of episodes during which\nthe predictions /tildewideQarenotused to choose the policy.\n11Recall that for a >0 and some constant C1, we have deﬁned ϕh(a) =C1·/parenleftbig\n1+1\nH/parenrightbig4(H+1−h)·ainQLearningPreds .\nThusϕh(/hatwide∆k) = Θ(/hatwide∆k); the function ϕh(·) is introduced for technical considerations in the proof.\n17\n\nAlgorithm 1: QLearningPreds\nInput:State spaceS, action spaceA, horizon H, number of episodes K, predictions\n/tildewideQh:S×A→ [0,H] for allh∈[H], parameter λ∈[0,1]. For some constant\nC1>0 and 1≤h≤H+1, set, for a >0,ϕh(a) =C1·/parenleftbig\n1+1\nH/parenrightbig4(H+1−h)·a.\n1. Initialize N1\nh(x,a) = 0,R1\nh(x,a) =Q1\nh(x,a) =V1\nh(x,a) =q1\nh(x,a) =H,\nQ1\nh(x,a) =V1\nh(x,a) =q1\nh(x,a) = 0 for all ( x,a,h)∈S×A× [H]. Also set/tildewideQ1\nh=/tildewideQhand\n/tildewideV1\nh(x) = max a′∈A/tildewideQ1\nh(x,a′) for all ( x,h)∈S×[H]. SetA1\nh(x) =Afor all (x,h)∈S×[H],\nandG1\nh=∅for allh∈[H]. Set/hatwide∆1←0.\n2. For episode 1≤k≤K:\n(a) Receive πkand the policy rollout ( xk\n1,ak\n1),...,(xk\nH,ak\nH) from the PolicySelection\nalgorithm.\n(b) For each h= 1,2,...,Hsuch that xk\nh/\\⌉}atio\\slash∈Gk\nh:\ni. SetNk+1\nh←Nk\nh(xk\nh,ak\nh)+1,n=Nk+1\nh(xk\nh,ak\nh), and write bn=C0/radicalbig\nH3ι/n.\nii. Letxk\nh′be the ﬁrst state in the episode after xk\nhso thatxk\nh′/\\⌉}atio\\slash∈Gk\nh′(if suchh′does\nnot exist, set h′=H+1).\niii. Let/hatwiderk\nh=/summationtexth′−1\nh′′=hrh′′(xk\nh′′,ak\nh′′).\niv. Setqk+1\nh(xk\nh,ak\nh)←min(1−αn)·qk\nh(xk\nh,ak\nh)+αn·(/hatwiderk\nh+Vk\nh′(xk\nh′)+bn).\nv. SetQk+1\nh(xk\nh,ak\nh)←mink′≤k+1/braceleftig\nqk′\nh(xk\nh,ak\nh)/bracerightig\n.\nvi. Setqk+1\nh(xk\nh,ak\nh)←(1−αn)·qk\nh(xk\nh,ak\nh)+αn·(/hatwiderk\nh+Vk\nh′(xk\nh′)−bn).\nvii. SetQk+1\nh(xk\nh,ak\nh)←maxk′≤k+1/braceleftig\nqk′\nh(xk\nh,ak\nh)/bracerightig\n.\nviii. Set Vk+1\nh(xk\nh)←maxa′∈Ak\nh(xk\nh){Qk+1\nh(xk\nh,a′)}.\nix. SetVk+1\nh(xh)←maxa′∈Ak\nh(xh){Qk+1\nh(xh,a′)}.\n(c) For all ( x,h)∈S×[H], setAk+1\nh(x)←{a′∈Ak\nh(x) :Qk+1\nh(x,a′)≥Vk+1\nh(x)}.\n(d) For each h= 1,2,...,H:\ni. SetRk+1\nh(xk\nh,ak\nh)←(1−αn)·Rk\nh(xk\nh,ak\nh)+αn·(rh(xk\nh,ak\nh)+/tildewideVk\nh+1(xh+1)+bn).\nii. Set/tildewideQk+1\nh(xk\nh,ak\nh)←min{Rk+1\nh(xk\nh,ak\nh),/tildewideQk\nh(xk\nh,ak\nh),Qk+1\nh(xk\nh,ak\nh)}.\niii. Set/tildewideVk+1\nh(xk\nh)←maxa′∈Ak+1\nh(xk\nh)max{/tildewideQk+1\nh(xk\nh,a′),Qk+1\nh(xk\nh,a′)}.\n(e) For all hand all (x,a)/\\⌉}atio\\slash= (xk\nh,ak\nh) setNk+1\nh(x,a),Qk+1\nh(x,a),Qk+1\nh(x,a),qk+1\nh(x,a),\nqk+1\nh(x,a),Vk+1\nh(x),Vk+1\nh(x),Rk+1\nh(x,a),/tildewideQk+1\nh(x,a),/tildewideVk+1\nh(x) equal to their values at\nepisodek.\n(f) For all h∈[H], setGk+1\nh←{x∈S:|Ak+1\nh(x)|= 1}.\n(g) Choose/hatwide∆k+1according to either DeltaConst orDeltaIncr .\n18\n\nAlgorithm 2: PolicySelection\nInput:Internal state of the algorithm QLearningPreds at the beginning of episode k\n(including, in particular, the previous pollicy rollouts, and the functions\n/tildewideQk\nh,Qk\nh,Qk\nh, as well as/hatwide∆k,Gk\nh,Ak\nh).\n1. Forh∈[H], construct ∆ Vk\nh(·) per Deﬁnition 4.1.\n2. Deﬁne the policy πkby, for (x,h)∈S×[H]:\nπk\nh(x) :=\n\nThe action in Ak\nh(x) if |Ak\nh(x)|= 1\nargmaxa∈Ak\nh(x){max{/tildewideQk\nh(x,a),Qk\nh(x,a)}}if ∆Vk\nh(x)≤ϕh(/hatwide∆k)\nargmaxa∈Ak\nh(x){Qk\nh(x,a)−Qk\nh(x,a)} if ∆Vk\nh(x)> ϕh(/hatwide∆k)(11)\n3. Let (xk\n1,ak\n1),...,(xk\nH,ak\nH) be a policy rollout obtained by following πk.\n4. Return the policy πkand the policy rollout ( xk\n1,ak\n1),...,(xk\nH,ak\nH).\nAlgorithm 3: DeltaConst\nInput:Episode number k, input regret bound RofQLearningPreds , and total number\nKof episodes.\n1. Return\n/hatwide∆k+1:=R/(KH). (12)\nAlgorithm 4: DeltaIncr\nInput:Internal state of the algorithm QLearningPreds at the beginning of episode k+1\n(in particular, the necessary information to compute the fr ozenQ-range function),\nand parameters λ∈[0,1] and/tildewide∆min≥0 (which is guaranteed to satisfy\n/tildewide∆min≤∆min).\n1. Forh∈[H], construct/tildewide∆˚Qk+1\nh(·) which is deﬁned identically to ∆ ˚Qk+1\nh(·) per Deﬁnition\n5.2, except with the parameter /tildewide∆minreplacing ∆ minin the clipped value functions\n∆˘Vk+1\nh,∆˘Qk+1\nh.\n2. Return\n/hatwide∆k+1:= min\n\nH5ι2\nλ·K·/summationdisplay\n(x,a,h)1\nmax/braceleftbigg\n/tildewide∆˚Qk+1\nh(x,a)\n2H,/tildewide∆min\n4H2/bracerightbigg,/radicalbigg\nSAH8ι2\nλ·K\n\n.(13)\n19\n\nThenweupperboundthevaluefunctions Rk\nh,/tildewideVk\nhat theset ofepisodes kwherethepredictions\nareused and show that doing so suﬃces to bound regret.\nThe proof of Theorem 3.1is similar to that of Theorem 3.2. One additional tool needed (which\nshows up in the algorithm DeltaIncr ) is a variation of the clipped range functions that we call th e\nfrozen range functions (Deﬁnition 5.2).\nIn Sections 5.1through 5.4we expand upon the above items to overview the proof of Theore m\n3.2. In Section 5.5we overview the changes that must be made to QLearningPreds and the proof\nto establish Theorem 3.1.\n5.1 Clipped range functions\nWe begin by deﬁning the clippedQ-value and V-value functions , which were originally introduced\nin [XMD21] to obtain gap-based bounds on the regret (they play a simila r role in this paper). For\nreal numbers x,y, deﬁne the clip function as follows: clip[ x|y] := /BD[x≥y]·x.\nDeﬁnition 5.1 (Clipped range function, [ XMD21]).For all (x,a,h,k)∈S×A× [H]×[K] for\nwhichx/\\⌉}atio\\slash∈Gk\nhanda∈Ak\nh(x), deﬁne the clipped range Q-function as follows: set ∆ ˘Q0\nh(x,a) =H,\nand\n∆˘Qk\nh(x,a) :=min/braceleftigg\n∆˘Qk−1\nh(x,a), α0\nnH+clip/bracketleftbigg\nβn|∆min\n4H2/bracketrightbigg\n+n/summationdisplay\ni=1αi\nn·∆˘Vki\nh\nh′(ki\nh,h)(xki\nh\nh′(ki\nh,h))/bracerightigg\n(14)\nwheren=Nk\nh(x,a), ki\nh=ki\nh(x,a)∀i∈[n].\nMoreover, for ( x,h,k)∈S×[H]×[K] for which x/\\⌉}atio\\slash∈Gk\nh, deﬁne the clipped range V-function as\nfollows: set ∆ ˘V0\nh(x) =H, and\n∆˘Vk\nh(x) := min{∆˘Vk−1\nh(x),∆˘Qk\nh(x,a⋆)}fora⋆= argmax\na′∈Ak\nh(x)Qk\nh(x,a′)−Qk\nh(x,a′).\nFinally, deﬁne ∆ ˘Vk\nH+1(x) = ∆˘Qk\nH+1(x,a) = 0 for all x,a,k.\nThe clipped range functions ∆ ˘Qk\nh(x,a),∆˘Vk\nh(x) are deﬁned to satisfy a similar recursion as\nthe quantities Qk\nh(x,a)−Qk\nh(x,a) andVk\nh(x)−Vk\nh(x) (see (23)). Unlike in ( 23), in the deﬁnition\nof ∆˘Qk\nh,∆˘Vk\nh, the bonuses βnare clipped, leading ∆ ˘Vk\nh,∆˘Qk\nhto be smaller than their unclipped\ncounterparts (Lemma 6.6), which aids in obtaining gap-based regret bounds. Despite this clipping,\nthe combination of Lemmas 6.4and6.5shows that, with high probability, for all x,a,h,k, the\nclipped range functions are still approximately lower boun ded by the gap between the upper and\nlowerQ,V-values, as follows:\n∆˘Qk\nh(x,a)≥Qk\nh(x,a)−Qk\nh(x,a)−∆min\n4H,∆˘Vk\nh(x)≥Vk\nh(x)−Vk\nh(x)−∆min\n4H.(15)\n5.2 Worst-case regret bound\nIn this section we overview the proof that QLearningPreds achieves regret O(R) for arbitrary\npredictions/tildewideQin the setting of Theorem 3.2. For all ( h,k) for which xk\nh/\\⌉}atio\\slash∈Gk\nh, deﬁne˘δk\nh= ∆˘Vk\nh(xk\nh).\nUsing (15), the following regret decomposition is straightforward t o prove (it is similar to that in\nLemma B.6 of [ XMD21]).\n20\n\nLemma 6.15(Regret decomposition; abbreviated) .There is an event Ewcthat occurs with prob-\nability at least 1−1/(H2K)so that the regret of QLearningPreds may be bounded as follows:\nK/summationdisplay\nk=1E/bracketleftig\nV⋆\n1(xk\n1)−Vπk\n1(xk\n1)/bracketrightig\n≤1+4·E\n/summationdisplay\n(k,h):ak\nh/\\⌉gatio\\slash∈Aopt\nh,0(xk\nh)˘δk\nh/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleEwc\n.\n(The right-hand side of the above expression makes sense sinc e under the event Ewc, it turns out\nthat for all (k,h)so thatak\nh/\\⌉}atio\\slash∈Aopt\nh,0(xk\nh),xk\nh/\\⌉}atio\\slash∈Gk\nh, i.e.,˘δk\nhis well-deﬁned.)\nLemma6.15reduces theproblem of boundingthe regret to boundingthe cl ipped valuefunctions\n˘δk\nhforh,ksuch that xk\nh/\\⌉}atio\\slash∈Gk\nh. In turn, we bound ˘δk\nhin Lemma 6.13, of which a simpliﬁed version\ncombining it with Lemma 6.16is presented below:\nLemma 6.13(Abbreviated & combined with Lemma 6.16).Fix anyh∈[H], any setW⊂[K]so\nthat for all k∈W,xk\nh/\\⌉}atio\\slash∈Gk\nh, and any k⋆≥maxk∈W{k}. Then\n/summationdisplay\nk∈W˘δk\nh≤|W|·ϕh(/hatwide∆k⋆)+O\nmin\n\n/radicalbig\nH5SA|W|ι, H5ι·\n/summationdisplay\n(x,a,h′)1\n∆h′(x,a)+|Amul|\n∆min\n\n\n\n.(16)\nThe starting point for the proof of Lemma 6.13is to use the deﬁnition of the clipped value\nfunctions ∆ ˘Vk\nhtogether with reverse induction on h(i.e., bounding the values ˘δk\nhin terms of ˘δk\nh′for\nh′> h) in a similar manner as was done in Lemma B.8 of [ XMD21]. However, the proof of Lemma\n6.13must depart from that of [ XMD21, Lemma B.8] since in the PolicySelection subroutine of\nQLearningPreds ,wedonot always choose theaction a∈Ak\nh(x) maximizing the conﬁdence interval ,\ni.e., maximizing Qk\nh(x,a)−Qk\nh(x,a). Typically such an action choice maximizing the conﬁdence\ninterval is necessary to upper bound the values ˘δk\nh. We are able to nevertheless bound ˘δk\nhusing the\nfact that for steps ( h,k) for which we do notchoose the action amaximizing the conﬁdence interval\n(i.e., we are in the constrained exploitation phase), it mus t hold that ˘δk\nh= ∆˘Vk\nh(xk\nh)≤∆Vk\nh(xk\nh)≤\nϕh(/hatwide∆k). This observation leads to the quantity |W|·ϕh(/hatwide∆k⋆) on the right-hand side of ( 16).\nThe proof of Lemma 6.13is made somewhat more complex by the fact that the choice of ac tion\nat stephaﬀects˘δk\nh′forh′< hand various k(via the reverse induction argument), and without\ncare one will end up with a multiplier of ϕh(/hatwide∆k⋆) in (16) that is much larger than |W|. To avoid\nthis complication, we must carefully account for the eﬀect th e values ˘δk\nhhave on the bounds we\ncan prove on ˘δk\nh′forh′< h. To do so we make use of a monotonicity propery of the clipped v alue\nfunctions (Lemma 6.7, showing that ∆ ˘Vk\nh(x) is non-increasing with k) and introduce the notion of\nlevel-hsets (Deﬁnition 6.1) which are intermediate sets W′of tuples ( k′,h′) for which we need to\nbound/summationtext\n(k′,h′)∈W′˘δk′\nh′in the course of the induction.\nCompleting the worst-case regret bound The proof of item 1of Theorem 3.2is fairly\nstraightforward given the above components; the details ar e worked out in Lemma 6.17. The domi-\nnant term in the bound ( 16) turns out to be |W|·ϕh(/hatwide∆k⋆), which due to the choice /hatwide∆k=R/(HK)\nand the bound|W|≤K, leads to the bound O(R) on regret.\n21\n\n5.3 Exploration-constrained exploitation cutoﬀs\nBefore discussing the proof of the improved regret bound for the case of/tildewideQbeing an approxi-\nmate distillation, we introduce the following notation rel ating to the exploration and constrained\nexploitation phases in QLearningPreds that we discussed above. For ( k,h)∈[K]×[H], deﬁne\nτk\nh∈{0,1}as follows:\nτk\nh=/braceleftigg\n0 if xk\nh∈Gk\nhor ∆Vk\nh(xk\nh)≤ϕh(/hatwide∆k)\n1 otherwise .(17)\nThe parameter τk\nhis the indicator of whether QLearningPreds is in the exploration or constrained\nexploitation step at step hof episode k: ifτk\nh= 0, then we have either determined the optimal\naction at xk\nh(i.e.,xk\nh∈Gk\nh), or else the range function ∆ Vk\nh(xk\nh) is suﬃciently small, so we engage\nin constrained exploitation (see the choice of πk\nhin (11), which chooses a′∈Ak\nh(xk\nh) maximizing\nmax{/tildewideQk\nh(xk\nh,a′),Qk\nh(xk\nh,a′)}), and otherwise, if τk\nh= 1, we use optimistic exploration, choosing\na′∈Ak\nh(xk\nh) to maximize the conﬁdence interval.\nNote that theparameters τk\nhdependon the unclipped rangefunctions∆ Vk\nh; as wehave discussed\nabove, in order obtain our gap-based bounds, it is necessary to bound the clippedrange functions\n∆˘Vk\nh. Therefore, when reasoning about the exploration and const rained exploitation phases, we\nwill additionally introduce the parameters σk\nh∈{0,1}(for (k,h)∈[K]×[H]), which are deﬁned\nsimilarly to τk\nhexcept with respect to ∆ ˘Vk\nh:\nσk\nh=/braceleftigg\n0 if xk\nh∈Gk\nhor ∆˘Vk\nh(xk\nh)≤1\n1+1\nH·ϕh(/hatwide∆k)\n1 otherwise .(18)\nThe parameters σk\nhcan be thought of as a proxy for the true exploration paramete rsτk\nh. As\ndiscussed in the following section, in order to establish im proved regret bounds for the case that /tildewideQ\nis an approximate distillation, we need to, loosely speakin g, upper bound the number of episodes\nin which we engage in exploration (i.e., in which the predict ions/tildewideQarenotused). For technical\nreasons, it turns out to be more convenient to bound the numbe r of (k,h) so that σk\nh= 1 (as\nopposed to bounding the number of ( k,h) so that τk\nh= 1).\n5.4 Proofs for /tildewideQan approximate distillation\nNow we discuss the proof of item 2of Theorem 3.2; the proof of item 2of Theorem 3.1is very\nsimilar (see Section 7for the full proof). As discussed in the previous section, th e ﬁrst step is to\nboundthe number of episodes for which we do notengage in constrained exploitation; in particular,\nfor each h, we bound the number of kfor which σk\nh= 1:\nLemma 7.1.SupposeQLearningPreds is run with DeltaConst to choose the values /hatwide∆k. Then\nfor allh∈[H], the number of episodes k∈[K]for which σk\nh= 1is at most max{SAH3,/hatwideλ·K}.\n(Recall that/hatwideλis chosen so that R=1\n/hatwideλ·CM,T,/hatwideλ.)\nWe write/hatwide∆ =/hatwide∆k(as all/hatwide∆kare equal). Also, for any h∈[H], writeYh:={k:σk\nh= 1}. The\nmain tool in the proof of Lemma 7.1is Lemma 6.13, which upper bounds/summationtext\nk∈Yh˘δk\nhby the sum of\n|Yh|·(1 + 1/H)2·ϕh+1(/hatwide∆) and some additional terms. On the other hand, that σk\nh= 1 implies\n22\n\nthat˘δk\nh≥1\n1+1/H·ϕh(/hatwide∆). These facts (together with the fact that ϕh(/hatwide∆) is greater than ϕh+1(/hatwide∆)\nby a factor of (1+1 /H)4) allow us to upper bound |Yh|in terms of an expression which ultimately\nsimpliﬁes to max{SAH3,/hatwideλ·K}.\nRegret decomposition and induction Given Lemma 7.1, we proceed to complete the proof\nof item2of Theorem 3.2. The ﬁrst step is the following regret decomposition (state d in (88)),\nwhich follows from the fact that /tildewideQis anǫ-approximate distillation as well as the deﬁnition of /tildewideVk\nh\ninQLearningPreds : for any ǫ′>0, we have\nK/summationdisplay\nk=1E/bracketleftig\nV⋆\n1(xk\n1)−Vπk\n1(xk\n1)/bracketrightig\n≤O(KH(ǫH+ǫ′))+E/bracketleftiggK/summationdisplay\nk=1H/summationdisplay\nh=1(1−τk\nh)· /BD[ak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh)]·(Rk\nh(xk\nh,ak\nh)−Q⋆\nh(xk\nh,ak\nh))+K/summationdisplay\nk=1H/summationdisplay\nh=14σk\nh˘δk\nh/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleEwc/bracketrightigg\n.\n(19)\nThe above regret decomposition reduces bounding the regret to bounding the following two types\nof quantities (under the event Ewc):\n1. The quantity Rk\nh(xk\nh,ak\nh)−Q⋆\nh(xk\nh,ak\nh), for (k,h) satisfying τk\nh= 0 and ak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh);\n2. The quantity ˘δk\nhfor (k,h) satisfying σk\nh= 1.12\nThe latter of these quantities (i.e., item 2) is straightforward to control: for each h∈[H], we\nuse Lemma 6.13with the setWequal to the set of kso thatσk\nh= 1 and k⋆=K. Crucially,\nthe conclusion of Lemma 7.1above gives that|W|≤max{SAH3,/hatwideλ·K}, which, together with the\ninequality ( 16) of Lemma 6.13, gives us that/summationtextK\nk=1/summationtextH\nh=1σk\nh˘δk\nhmay be bounded by O(CM,T,/hatwideλ). This\nargument is carried out formally in Lemma 7.12.\nBounding Rk\nh,/tildewideVk\nhon non-exploratory episodes We next describe how the quantity in item\n1above is bounded. Our general strategy is to use the deﬁnitio n ofRk\nhin terms of/tildewideVk\nh+1(step\n2(d)iofQLearningPreds ) to bound the gaps Rk\nh(xk\nh,ak\nh)−Q⋆\nh(xk\nh,ak\nh) at step hin terms of the gaps\n/tildewideVk′\nh+1(xk′\nh+1)−V⋆\nh+1(xk′\nh+1) at step h+1, for appropriate choices of k′. In turn, we will boundthe gaps\n/tildewideVk′\nh+1(xk′\nh+1)−V⋆\nh+1(xk′\nh+1) in terms of the gaps Rk′\nh+1(xk′\nh+1,ak′\nh+1)−Q⋆\nh+1(xk′\nh+1,ak′\nh+1), completing\nthe inductive step. When proving these bounds, we must take c are to meet our goal of obtaining a\nregret bound (see ( 8)) that only has terms corresponding to tuples ( x,a,h) belonging to the fooling\nsetF(ǫ(H+1),ǫ′). To do so, we use the following claim:\nClaim7.11.For any (k,h)satisfying τk\nh= 0, if either\n1.ak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh); or\n2.(/tildewideVk\nh−V⋆\nh)(xk\nh)> ǫ′,\nthen under the event Ewcit holds that (xk\nh,ak\nh,h)∈F(ǫ(H+1),ǫ′).\n12Note that σk\nh= 1 implies that xk\nh/ne}ationslash∈ Gk\nh, which implies that ˘δk\nhis indeed well-deﬁned.\n23\n\nClaim7.11allowsustoupperboundtheterm/summationtextK\nk=1/summationtextH\nh=1(1−τk\nh)· /BD[ak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh)]·(Rk\nh(xk\nh,ak\nh)−\nQ⋆\nh(xk\nh,ak\nh)) in the regret decomposition ( 19) with a sum of Rk\nh(x,a)−Q⋆\nh(x,a) over only those\n(x,a,h)∈F(ǫ(H+1),ǫ′). In turn, for such tuples ( x,a,h), it is possible to upper bound Rk\nh(x,a)−\nQ⋆\nh(x,a) in terms of thesum of βn(forn=Nk\nh(x,a)) and a weighted sum of ( /tildewideVk′\nh+1−V⋆\nh+1)(xk′\nh+1) for\ncertain values of k′(seeLemma 7.5). Theterms βninthis sumform themain contribution to there-\ngret bound( 8); crucially we usethefact that weonly have such terms for ( x,a,h)∈F(ǫ(H+1),ǫ′).\nFinally, when completing the inductive step by bounding the gaps/tildewideVk′\nh+1(xk′\nh+1)−V⋆\nh+1(xk′\nh+1),\nwe again have to ensure that we only use terms of the form ( Rk′\nh+1(x,a)−Q⋆\nh+1(x,a)) in our upper\nbound for which ( x,a,h+1)∈F(ǫ(H+1),ǫ′). For this we again use Claim 7.11(with the second\noption). We refer the reader to Section 7.4for further details.\n5.5 Proof of Theorem 3.1: implicit- λbound\nThe proof of Theorem 3.1is similar to that of Theorem 3.2. The main diﬀerence is that, because\nthe algorithm is not given as input the target worst-case reg ret bound R(which in turn is used to\nchoose/hatwide∆kinDeltaConst for the proof of Theorem 3.2), it must construct a proxy value to assign\nto/hatwide∆k. This proxy is constructed in DeltaIncr (Algorithm 4): for each episode k,/hatwide∆kis set in\n(13) to equal an expression which resembles the deﬁnition of1\nλ·CM,T,λin (6), except that (a) the\nminimum gap ∆ minis replaced with the provided lower bound /tildewide∆min, and (b) the gaps ∆ h(x,a) are\nreplaced the the frozen range function ∆˚Qk\nh(x,a), deﬁned below:\nDeﬁnition 5.2 (Frozen range function) .For all (x,a,h,k)∈S×A× [H]×[K], deﬁne the frozenQ-\nfunction, ∆˚Qk\nh(x,a), as follows: given ( x,a,h), choose k′≤kas large as possible so that ( xk′\nh,ak′\nh) =\n(x,a) andτk′\nh= 1 (if no such k′exists, set k′= 1). Then set ∆ ˚Qk\nh(x,a) = ∆˘Qk′\nh(x,a).\nIn Lemma 6.16we show, roughly speaking, that the frozen range function at the ﬁnal episode,\nnamely ∆ ˚QK\nh(x,a), is still lower bounded by the gap ∆ h(x,a), justifying its use a surrogate for the\ngaps. The main challenge in the proof of Theorem 3.1, beyond those from Theorem 3.2, is the fact\nthat/hatwide∆kchanges as kincreases (in fact, as shown in Lemma 6.8,/hatwide∆kis non-decreasing with k). Most\nnotably, this aﬀects the proof of our bound on the number of epi sodeskfor which σk\nh= 1 (Lemma\n7.2; the analogous lemma for DeltaConst is Lemma 7.1). To prove Lemma 7.2, we partition [ K]\nintoO(ι·H) contiguous intervals so that inside each interval, /hatwide∆kincreases by a factor of at most\n1+1/H. For each such interval I⊂[K], we bound the number of k∈Iso thatσk\nh= 1; this leads\nto an increase in our regret bounds by a factor of O(ιH).\n6 Proofs for worst-case result\nIn this section we establish the robustness upper bounds of T heorems 3.1and3.2, giving a regret\nbound for QLearningPreds when the user provides arbitrary predictions/tildewideQh.\n6.1 Bounds on conﬁdence intervals\nWe begin by establishing various basic guarantees on the bou ndsQk\nh,Qk\nh,Vk\nh,Vk\nhmaintained by\nQLearningPreds . The ﬁrst such result is Lemma 6.1, which establishes that, with high probability,\nQk\nhis an upper bound on Q⋆\nh,Qk\nhis a lower bound on Q⋆\nh, and similarly for Vk\nh,Vk\nh(with respect\n24\n\ntoV⋆\nh). Before stating it, we introduce the following notation: f or eachk∈[K], letHkdenote the\nσ-algebra generated by all random variables up to step Hof episode k. For each k∈[K] as well as\n(x,a,h)∈S×A× [H], deﬁne the quantities Q⋆,k,b\nh(x,a) andQ⋆,k,r\nh(x,a) (as in [XMD21]) as follows:\nsuppose we start in state xat levelh, and follow the optimal policy π⋆, generating the (random)\ntrajectory xh=x,xh+1,...,x H. Choose h′≥h+1 as small as possible so that xh′/\\⌉}atio\\slash∈Gk\nh′, and write\nQ⋆,k,b\nh(x,a) :=E/bracketleftiggh′−1/summationdisplay\nℓ=hrℓ(xℓ,π⋆\nℓ(xℓ))|Hk−1/bracketrightigg\n, Q⋆,k,r\nh(x,a) :=E[V⋆\nh′(xh′)|Hk−1].(20)\n(Note thatGk\nh′isHk−1-measurable for all h′andk.) It is immediate that\nQ⋆\nh(x,a) =Q⋆,k,b\nh(x,a)+Q⋆,k,r\nh(x,a).\nAs in [XMD21], we use the quantities /hatwiderk\nhas an unbiased estimate of Q⋆,k,b\nh(xk\nh,ak\nh). Recall that for\nsome constant C0>1, we use exploration bonuses bn=C0/radicalbig\nH3ι/n, and recall the deﬁnition of\nthe aggregated bonuses βnin (10). Notice that item 1of Lemma A.1gives that\n2C0/radicalbig\nH3ι/n≤βn≤4C0/radicalbig\nH3ι/n. (21)\nFor future reference, we will also deﬁne the constants\nC2= 8C0, C 1= 32e2C2\n2. (22)\nLemma 6.1. Setp= 1/(H2K). For a suﬃciently large choice of the constant C0, there is an\neventEwcoccurring with probability 1−pso that the following holds under the event Ewc, for all\nepisodes k∈[K]:\n1. For any (x,a,h)∈S×A× [H]so thatx/\\⌉}atio\\slash∈Gk\nhanda∈Ak\nh(x), suppose the episodes k′in\nwhich(x,a)as previously taken at step hare denoted k1,...,kn≤k. Then the following\ninequalities hold:\nQk+1\nh(x,a)−Qk+1\nh(x,a)≤α0\nn·H+n/summationdisplay\ni=1αi\nn·/parenleftig\n(Vki\nh′(ki,h)−Vki\nh′(ki,h))(xki\nh′(ki,h))/parenrightig\n+βn(23)\nQk+1\nh(x,a)≥Q⋆\nh(x,a)≥Qk+1\nh(x,a) (24)\nVk+1\nh(x)≥V⋆\nh(x)≥Vk+1\nh(x). (25)\n2. Second, for all (x,h)∈S×[H]all optimal actions a(i.e., those asatisfying ∆h(x,a) = 0)\nare inAk+1\nh(x). In particular, for all x∈Gk+1\nh,Ak+1\nh(x)contains the unique optimal action\natx.\nProof.Fork∈[K], we letEwc\nkdenote the event that items 1and2of the lemma statement hold\nfor all episodes j≤k. We wish to show that Pr[ Ewc\nK]≥1−p.\nWe use induction on kto show that for all k, Pr[Ewc\nk]≥1−pk/K. The base case k= 0 (i.e.,\nk+1 = 1) follows from the fact that α0\n0= 1,Q1\nh(x,a) =H,Q1\nh(x,a) = 0, and that for any choice of\n(x,a,h) we necessarily have n= 0 (in particular, Pr[ Ewc\n0] = 1). So choose any k≥1, and assume\nthat Pr[Ewc\nk−1]≥1−p(k−1)/K.\n25\n\nBy the algorithm’s update rule in steps 2(b)ivand2(b)vi, it holds that, for all ( x,a,h,k)∈\nS×A× [H]×[K] so that x/\\⌉}atio\\slash∈Gk\nh, letting n=Nk+1\nh(x,a),\nqk+1\nh(x,a) =/braceleftigg\n(1−αn)·qk\nh(x,a)+αn·/parenleftig\n/hatwiderk\nh+Vk\nh′(k,h)(xk\nh′(k,h))+bn/parenrightig\n: (x,a) = (xk\nh,ak\nh)\nqk\nh(x,a) : otherwise\nqk+1\nh(x,a) =/braceleftigg\n(1−αn)·qk\nh(x,a)+αn·/parenleftig\n/hatwiderk\nh+Vk\nh′(k,h)(xk\nh′(k,h))−bn/parenrightig\n: (x,a) = (xk\nh,ak\nh)\nqk\nh(x,a) : otherwise .\nBy iterating the above, we obtain that\nqk+1\nh(x,a) =α0\nn·H+n/summationdisplay\ni=1αi\nn·/parenleftig\n/hatwiderki\nh+Vki\nh′(ki,h)(xki\nh′(ki,h))+bn/parenrightig\n(26)\nqk+1\nh(x,a) =n/summationdisplay\ni=1αi\nn·/parenleftig\n/hatwiderki\nh+Vki\nh′(ki,h)(xki\nh′(ki,h))−bn/parenrightig\n, (27)\nwherek1,...,kn≤kdenote all previous episodes during which ( x,a,h) has been visited.\nTo see that ( 23) holds, we ﬁrst take the diﬀerence of ( 26) and (27) and use the deﬁnition of βn\nin (10) to get that\nqk+1\nh(x,a)−qk+1\nh(x,a) =α0\nn·H+βn+n/summationdisplay\ni=1αi\nn·/parenleftig\n(Vki\nh′(ki,h)−Vki\nh′(ki,h))(xki\nh′(ki,h))/parenrightig\n.\nNow (23) follows by noting that Qk+1\nh(x,a)≤qk+1\nh(x,a) andQk+1\nh(x,a)≥qk+1\nh(x,a) (note in\nparticular that ( 23) holds with probability 1).\nWe proceed to analyze the event under which ( 24) and (25) hold. We may compute\nqk+1\nh(x,a)−Q⋆\nh(x,a)\n=α0\nn·H+n/summationdisplay\ni=1αi\nn·/parenleftig\n/hatwiderki\nh+Vki\nh′(ki,h)(xki\nh′(ki,h))+bn/parenrightig\n−Q⋆\nh(x,a)\n=α0\nn·(H−Q⋆\nh(x,a))+n/summationdisplay\ni=1αi\nn·/parenleftig\n/hatwiderki\nh+Vki\nh′(ki,h)(xki\nh′(ki,h))−Q⋆\nh(x,a)/parenrightig\n+n/summationdisplay\ni=1αi\nn·bn\n(Using item 4of Lemma A.1andb0= 0)\n=α0\nn·(H−Q⋆\nh(x,a))+βn/2+n/summationdisplay\ni=1αi\nn·/parenleftig\n/hatwiderki\nh−Q⋆,ki,b\nh(x,a)/parenrightig\n+n/summationdisplay\ni=1αi\nn·/parenleftig\nVki\nh′(ki,h)(xki\nh′(ki,h))−V⋆\nh′(ki,h)(xki\nh′(ki,h))/parenrightig\n+n/summationdisplay\ni=1αi\nn·/parenleftig\nV⋆\nh′(ki,h)(xki\nh′(ki,h))−Q⋆,ki,r\nh(x,a)/parenrightig\n.\n(28)\n26\n\nIn a similar manner, we have\nqk+1\nh(x,a)−Q⋆\nh(x,a)\n=−α0\nn·Q⋆\nh(x,a)−βn/2+n/summationdisplay\ni=1αi\nn·/parenleftig\n/hatwiderki\nh−Q⋆,ki,b\nh(x,a)/parenrightig\n+n/summationdisplay\ni=1αi\nn·/parenleftig\nVki\nh′(ki,h)(xki\nh′(ki,h))−V⋆\nh′(ki,h)(xki\nh′(ki,h))/parenrightig\n+n/summationdisplay\ni=1αi\nn·/parenleftig\nV⋆\nh′(ki,h)(xki\nh′(ki,h))−Q⋆,ki,r\nh(x,a)/parenrightig\n.\n(29)\nClaim 6.2. There is an event Ek⊂Ewc\nk−1so thatPr[Ek]≥1−pk/Kand the following holds under\nEk: for all h∈[H], allx∈S\\Gk\nh, and all a∈Ak\nh(x), letting n=Nk+1\nh(x,a)andk1,...,kn≤k\ndenote all the previous episodes in which (x,a,h)was previously visited,\n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay\ni=1αi\nn·/parenleftig\n/hatwiderki\nh−Q⋆,ki,b\nh(x,a)/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/radicaligg\nH3\nn·log/parenleftbigg4SAHK\np/parenrightbigg\n(30)\n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay\ni=1αi\nn·/parenleftig\nV⋆\nh′(ki,h)(xki\nh′(ki,h))−Q⋆,ki,r\nh(x,a)/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/radicaligg\nH3\nn·log/parenleftbigg4SAHK\np/parenrightbigg\n. (31)\nProof of Claim 6.2.For 1≤k′≤k, recall thatHk′denotes the σ-algebra generated by all random\nvariables up to (step Hof) episode k′. Note that k1,...,knare all stopping times with respect\nto the ﬁltrationHk′. Moreover, it is evident that Ewc\nk′isFk′-measurable for all k′≤k. Next, for\n1≤i≤n, deﬁne the ﬁltration FibyFi:=Hki−1. Moreover, for i∈[n], deﬁne\nMi:=αi\nn·/parenleftig\n/hatwiderki\nh−Q⋆,ki,b\nh(x,a)/parenrightig\n· /BD[Ewc\nki−1].\nSinceki+1> ki,MiisFi+1-measurable for each i(as a matter of convention we set kn+1=k+1,\nsoMiisFi+1-measurable even for i=n). Moreover, we claim that for each i,\nE[Mi|Fi] =αi\nn·E/bracketleftig/parenleftig\n/hatwiderki\nh−Q⋆,ki,b\nh(x,a)/parenrightig\n· /BD[Ewc\nki−1]|Fi/bracketrightig\n= 0. (32)\nTo see that ( 32) holds, ﬁrst note that conditioned on Fi, /BD[Ewc\nki−1]·/hatwiderki\nhis distributed identically\nto /BD[Ewc\nki−1]·/summationtexth′−1\nℓ=hrℓ(xℓ,π⋆\nℓ(xℓ)) where xℓis the sequence of states visited starting at xh=xand\nfollowing the optimal policy π⋆andh′is as small as possible so that xh′/\\⌉}atio\\slash∈Gki\nh′(this holds since\nitem2at episode ki−1 gives that the unique action in Aki\nℓ(xki\nℓ), forh≤ℓ < h′(ki,h) is the optimal\naction, namely π⋆\nℓ(xki\nℓ)). Recall from ( 20) thatE/bracketleftig/summationtexth′−1\nℓ=hrℓ(xℓ,π⋆\nℓ(xℓ))−Q⋆,ki,b\nh(x,a)|Fi/bracketrightig\n= 0; then\nthe fact thatEwc\nki−1isFi-measurable gives ( 32).\nNext, for i∈[n], deﬁne\nNi:=αi\nn·/parenleftig\nV⋆\nh′(ki,h)(xki\nh′(ki,h))−Q⋆,ki,r\nh(x,a)/parenrightig\n· /BD[Ewc\nki−1].\nSinceki+1> ki,NiisFi+1-measurable for each i. Moreover, we claim that for each i,\nE[Ni|Fi] =αi\nn·E/bracketleftig/parenleftig\nV⋆\nh′(ki,h)(xki\nh′(ki,h))−Q⋆,ki,r\nh(x,a)/parenrightig\n· /BD[Ewc\nki−1]|Fi/bracketrightig\n= 0. (33)\n27\n\nThe validity of ( 33) is veriﬁed in the same way as that of ( 32): conditioned on Fi, /BD[Ewc\nki−1]·\nV⋆\nh′(ki,h)(xki\nh′(ki,h)) is distributed identically to /BD[Ewc\nki−1]·V⋆\nh′(xh′), where xh,...,x h′is deﬁned as\nabove, namely it is the sequence of states visited starting a txh=xand following the optimal\npolicyπ⋆, andh′is as small as possible so that xh′/\\⌉}atio\\slash∈Gki\nh′(again we use that item 2holds at episode\nki−1). Now ( 20) gives that E[V⋆\nh′(xh′)−Q⋆,ki,r\nh(x,a)|Fi] = 0 and using this together with the fact\nthatEwc\nki−1isFi-measurable gives ( 33).\nEquations ( 32) and (33) give that MiandNiare martingales with respect to the ﬁltration\nFi+1. The fact that/summationtextn\ni=1(αi\nn)2≤2H\nn(item2of Lemma A.1) together with the Azuma-Hoeﬀding\ninequality then gives that, for ﬁxed x,a,h, with probability 1 −p/(SAHK), both of the below\ninequalities hold:\n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay\ni=1αi\nn·/parenleftig\n/hatwiderki\nh−Q⋆,ki,b\nh(x,a)/parenrightig\n· /BD[Ewc\nki−1]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/radicaltp/radicalvertex/radicalvertex/radicalbtH3·log/parenleftig\n4SAHK\np/parenrightig\nn(34)\n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay\ni=1αi\nn·/parenleftig\nV⋆\nh′(ki,h)(xki\nh′(ki,h))−Q⋆,ki,r\nh(x,a)/parenrightig\n· /BD[Ewc\nki−1]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/radicaltp/radicalvertex/radicalvertex/radicalbtH3·log/parenleftig\n4SAHK\np/parenrightig\nn.(35)\nLetEkdenote the intersection of the probability 1 −p/K-event that both ( 34) and (35) hold for all\nx,a,hand the eventEwc\nk−1. Then using the inductive hypothesis that Pr[ Ewc\nk−1]≥1−p(k−1)/K,\nwe get that Pr[Ek]≥1−pk/K. Thus, under the event Ek, we have that\n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay\ni=1αi\nn·/parenleftig\n/hatwiderki\nh−Q⋆,ki,b\nh(x,a)/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/radicaligg\nH3\nn·log/parenleftbigg4SAHK\np/parenrightbigg\n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay\ni=1αi\nn·/parenleftig\nV⋆\nh′(ki,h)(xki\nh′(ki,h))−Q⋆,ki,r\nh(x,a)/parenrightig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤/radicaligg\nH3\nn·log/parenleftbigg4SAHK\np/parenrightbigg\n,\ncompleting the proof of the claim.\nNext we show that, on the event Ek, both (24) and (25) hold at episode k, for allx,a,h. Note\nthat, by ( 21), for all n,\nβn/4≥C0\n2·/radicalbigg\nH3ι\nn≥/radicaligg\nH3\nn·log/parenleftbigg4SAHK\np/parenrightbigg\nas long as the constant C0is chosen to be large enough. Thus, by ( 28) and Claim 6.2, under the\neventEk, we have that\nqk+1\nh(x,a)−Q⋆\nh(x,a)\n≥βn/2−βn/4+n/summationdisplay\ni=1αi\nn·/parenleftig\nVki\nh′(ki,h)(xki\nh′(ki,h))−V⋆\nh′(ki,h)(xki\nh′(ki,h))/parenrightig\n−βn/4\n=n/summationdisplay\ni=1αi\nn·/parenleftig\nVki\nh′(ki,h)(xki\nh′(ki,h))−V⋆\nh′(ki,h)(xki\nh′(ki,h))/parenrightig\n≥0, (36)\n28\n\nwhere the ﬁnal inequality follows from the fact that Ek⊂Ewc\nk−1and underEwc\nk−1, (25) holds (in par-\nticular, at step h′(ki,h) for state xki\nh′(ki,h)). Using the fact that Qk+1\nh(x,a) = min k′≤k+1/braceleftig\nqk′\nh(x,a)/bracerightig\n(step2(b)vofQLearningPreds ) together with the fact that Qk\nh(x,a)≥Q⋆\nh(x,a) underEwc\nk−1, we\nsee from ( 36) thatQk+1\nh(x,a)≥Q⋆\nh(x,a) under the event Ek(for alla∈Ak\nh(x)). Since Vk+1\nh(x) =\nmaxa∈Ak\nh(x)Qk+1\nh(x,a) (step2(b)ixofQLearningPreds ), it follows that Vk+1\nh(x)≥V⋆\nh(x) under\nthe eventEk.\nThus we have veriﬁed the ﬁrst inequality in each of ( 24) and (25) at episode k. The proof of the\nsecond inequality ineach follows identically: ( 29) together with Claim 6.2gives that undertheevent\nEk, we have that qk+1\nh(x,a)−Q⋆\nh(x,a)≤0 for allx,a,h. Then it follows that Qk+1\nh(x,a)≤Q⋆\nh(x,a),\nand using the fact that Vk+1\nh(x) = maxa∈Ak\nh(x)Qk+1\nh(x,a) (step2(b)viiiofQLearningPreds ), it\nfollows that Vk+1\nh(x)≤V⋆\nh(x) under the event Ek. Thus we have veriﬁed that ( 23), (24), and (25)\nhold (for any choice of x,a,hwithx/\\⌉}atio\\slash∈Gk\nhanda∈Ak\nh(x)) at episode k, under the event Ek.\nFinally we verify that item 2holds at episode kunder the eventEk. Suppose, to the contrary,\nthat there were some optimal action a⋆for some state ( x,h) so that a⋆/\\⌉}atio\\slash∈Ak+1\nh(x). SinceEk⊂Ewc\nk−1,\nwe have that a⋆∈Ak\nh(x), meaning that by the deﬁnition of Ak+1\nh(x) in step 2cofQLearningPreds ,\nwe must have that Qk+1\nh(x,a⋆)< Vk+1\nh(x). But we have just shown that under the event Ek, weh\nhave that Vk+1\nh(x)≤V⋆\nh(x) andQk+1\nh(x,a⋆)≥Q⋆\nh(x,a⋆), which implies that Q⋆\nh(x,a⋆)< V⋆\nh(x),\ncontradicting the fact that a⋆is an optimal action at ( x,h).\nThus we have shown that all statements in items 1and2for episode khold under the event Ek,\nandEk⊂Ewc\nk−1as well as Pr[Ek]≥1−pk/K. ThusEwc\nk⊃Ek, meaning that Pr[ Ewc\nk]≥1−pk/K,\nwhich completes the proof of the inductive step.\nThe following lemma shows that the upper and lower conﬁdence bounds satisfy a monotonicity\nproperty with respect to the number of episodes kthat have elapsed: in particular, the upper\nconﬁdence bounds on V⋆\nh,Q⋆\nhmaintained by QLearningPreds are non-increasing, and the lower\nconﬁdence bounds on V⋆\nh,Q⋆\nhare non-decreasing. Note that in many previous works studyi ngQ-\nlearning algorithms (such as [ JAZBJ18 ,XMD21]), these monotonicity properties do not necessarily\nhold – it is necessary to modify the Q- andV-value updates in QLearningPreds appropriately to\nensure that Lemma 6.3holds.\nLemma 6.3. For allk∈[K], the following inequalities hold for all (x,a,h)∈S×A× [H]:\nQk+1\nh(x,a)≤Qk\nh(x,a) (37)\nQk+1\nh(x,a)≥Qk\nh(x,a) (38)\nVk+1\nh(x)≤Vk\nh(x) (39)\nVk+1\nh(x)≥Vk\nh(x) (40)\n/tildewideQk+1\nh(x,a)≤/tildewideQk\nh(x,a) (41)\n/tildewideVk+1\nh(x)≤/tildewideVk\nh(x). (42)\nProof.Fix any k∈[K] andh∈[H]. First note that step 2eofQLearningPreds veriﬁes ( 37)\nthrough ( 42) for all ( x,a)/\\⌉}atio\\slash= (xk\nh,ak\nh). So it remains to to consider the case that x=xk\nhanda=ak\nh.\n29\n\nFirst note that ( 37) and (38) are directly veriﬁed by steps 2(b)vand2(b)vii, respectively, of\nQLearningPreds . To verify ( 39), note that Ak\nh(xk\nh)⊂Ak−1\nh(xk\nh), meaning that\nVk+1\nh(xk\nh) = max\na′∈Ak\nh(xk\nh){Qk+1\nh(xk\nh,a′)}≤max\na′∈Ak−1\nh(xk\nh){Qk+1\nh(xk\nh,a′)}≤max\na′∈Ak−1\nh(xk\nh){Qk\nh(xk\nh,a′)}=Vk\nh(xk\nh),\nwherethesecondinequalityuses( 37)andthelastequalityusessteps 2(b)viiiand2eofQLearningPreds\n(in particular, note that Vk\nh(xk\nh) andQk\nh(xk\nh,·) remain unchanged from the previous episode before\nkat which xk\nhwas visited).\nNext we verify ( 40); choose a∈Ak\nh(xk\nh) so that Vk+1\nh(xk\nh) =Qk+1\nh(xk\nh,a), anda′∈Ak−1\nh(xk\nh)\nso thatVk\nh=Qk\nh(xk\nh,a′). If (40) did not hold, we would have that Qk+1\nh(xk\nh,a′)≥Qk\nh(xk\nh,a′)>\nQk+1\nh(xk\nh,a), which must mean that a′/\\⌉}atio\\slash∈Ak\nh(xk\nh). But this is impossible since Qk+1\nh(xk\nh,a′)≥\nQk+1\nh(xk\nh,a′) =Vk+1\nh(xk\nh), so by step 2cofQLearningPreds a′must belong to Ak\nh(xk\nh).\nFinally, ( 41) is veriﬁed by step 2(d)iiofQLearningPreds , and to see that ( 42) holds, we again\nuse that Ak\nh(xk\nh)⊂Ak−1\nh(xk\nh) to get that\n/tildewideVk+1\nh(xk\nh) = max\na′∈Ak\nh(xk\nh)max/braceleftig\n/tildewideQk+1\nh(xk\nh,a′),Qk+1\nh(xk\nh,a′)/bracerightig\n≤max\na′∈Ak−1\nh(xk\nh)max/braceleftig\n/tildewideQk+1\nh(xk\nh,a′),Qk+1\nh(xk\nh,a′)/bracerightig\n≤max\na′∈Ak−1\nh(xk\nh)max/braceleftig\n/tildewideQk\nh(xk\nh,a′),Qk\nh(xk\nh,a′)/bracerightig\n(43)\n=/tildewideVk\nh(xk\nh),\nwhere (43) uses both ( 38) and (41).\n6.2 Range functions and clipped range functions\nRecall the deﬁnition of the range functions ∆ Vk\nh,∆Qk\nhin Deﬁnition 4.1, as well as the clipped range\nfunctions ∆ ˘Vk\nh,∆˘Qk\nhin Deﬁnition 5.1. In this section we establish some basic guarantees of these\nfunctions. The ﬁrst such result, Lemma 6.4, shows that the range functions are upper bounds on\nthe gap between the upper and lower estimates for the Q- andV-value functions.\nLemma 6.4. For all(x,h,k,a)∈S×A× [H]×[K]for which x/\\⌉}atio\\slash∈Gk\nhanda∈Ak\nh(x), the range\nfunctions satisfy the following under the event Ewc:\n∆Qk\nh(x,a)≥Qk\nh(x,a)−Qk\nh(x,a)\n∆Vk\nh(x)≥Vk\nh(x)−Vk\nh(x).\nProof.The proof closely follows that of [ XMD21, Lemma B.3]. We use reverse induction on h. The\nbase case h=H+1 is immediate since Qk\nH+1,Qk\nH+1,Vk\nH+1,Vk\nH+1are deﬁned to be identically 0 for\nallk∈[K]. Next ﬁx h≤Hand suppose that the statement of the lemma holds for all ( x,h′,k,a)\nfor which h′> h. For any ( x,k,a) for which x/\\⌉}atio\\slash∈Gk\nhanda∈Ak\nh(x), note that, for n=Nk\nh(x,a)\n30\n\nandki\nh=ki\nh(x,a),\nQk\nh(x,a)−Qk\nh(x,a)≤α0\nn·H+n/summationdisplay\ni=1αi\nn·(Vki\nh\nh′(ki\nh,h)−Vki\nh\nh′(ki\nh,h))(xki\nh\nh′(ki\nh,h))+βn (44)\n≤α0\nn·H+n/summationdisplay\ni=1αi\nn·∆Vki\nh\nh′(ki\nh,h)(xki\nh\nh′(ki\nh,h))+βn (45)\n=∆Qk\nh(x,a). (46)\nwhere (44) follows from ( 23) of Lemma 6.1(in particular, we use the validity of ( 23) for episode\nk−1) and (45) uses the inductive hypothesis (since h′(ki\nh,h)> h). Thus the inductive step for\n∆Qk\nhis veriﬁed. To lower bound ∆ Vk\nh, we ﬁrst note that by Deﬁnition 4.1for anyx/\\⌉}atio\\slash∈Gk\nh, there is\nsomek′≤kso that ∆ Vk\nh(x) = ∆Qk′\nh(x,a⋆) fora⋆= argmaxa′∈Ak′\nh(x){Qk′\nh(x,a′)−Qk′\nh(x,a′)}. Then\nVk\nh(x)−Vk\nh(x)≤Vk′\nh(x)−Vk′\nh(x) (Using ( 39) of Lemma 6.3)\n=/parenleftigg\nmax\na′∈Ak′\nh(x)Qk′\nh(x,a′)/parenrightigg\n−/parenleftigg\nmax\na′∈Ak′\nh(x)Qk′\nh(x,a′)/parenrightigg\n(47)\n≤max\na′∈Ak′\nh(x)/braceleftig\nQk′\nh(x,a′)−Qk′\nh(x,a′)/bracerightig\n≤Qk′\nh(x,a⋆)−Qk′\nh(x,a⋆) (For a⋆= argmaxa′∈Ak′\nh(x){Qk′\nh(x,a′)−Qk′\nh(x,a′)})\n≤∆Qk′\nh(x,a⋆) (Using (46))\n=∆Vk\nh(x), (48)\nwhere (47) follows from steps 2(b)viiiand2(b)ixofQLearningPreds and the ﬁnal equality ( 48)\nfollows from the deﬁnition of k′.\nLemma6.5below shows that despite the clipping of the bonus βnin the deﬁnition of the clipped\nrange function (see ( 14)), the clippedrange functions ∆ ˘Qk\nh,∆˘Vk\nhremain approximate upperbounds\non the range functions ∆ Qk\nh,∆Vk\nh.\nLemma 6.5. For all(x,h,k,a)∈ S×A× [H]×[K]for which x/\\⌉}atio\\slash∈ Gk\nhanda∈Ak\nh(x), the\npartially-clipped range functions satisfy the following:\n∆˘Qk\nh(x,a)≥∆Qk\nh(x,a)−∆min\n4H\n∆˘Vk\nh(x)≥∆Vk\nh(x)−∆min\n4H.\nProof.The lemma follows in a similar manner to [ XMD21, Proposition B.5]. We prove by reverse\ninduction on hand forward induction on kthat\n∆˘Qk\nh(x,a)≥∆Qk\nh(x,a)−(H+1−h)\nH·∆min\n4H(49)\nand\n∆˘Vk\nh(x)≥∆Vk\nh(x)−(H+1−h)\nH·∆min\n4H. (50)\n31\n\nThe base case h=H+1 is immediate since ∆ ˘Qk\nH+1,∆Qk\nH+1,∆˘Vk\nH+1,∆Vk\nH+1are identically 0.\nThe base case k= 0 is also immediate since ∆ ˘Q0\nh(x,a) = ∆˘V0\nh(x) = ∆Q0\nh(x,a) = ∆V0\nh(x) =Hfor\nallx,a,h∈S×A× [H]. To establish the inductive step, note that, for any ( x,a,h,k) for which\nx/\\⌉}atio\\slash∈Gk\nhanda∈Ak\nh(x), letting n=Nk\nh(x,a) andki\nh=ki\nh(x,a) fori∈[n], we have\n∆˘Qk\nh(x,a) =min/braceleftigg\n∆˘Qk−1\nh(x,a), α0\nnH+clip/bracketleftbigg\nβn|∆min\n4H2/bracketrightbigg\n+n/summationdisplay\ni=1αi\nn·∆˘Vki\nh\nh′(ki\nh,h)(xki\nh\nh′(ki\nh,h))/bracerightigg\n≥min/braceleftbigg\n∆Qk−1\nh(x,a)−(H+1−h)∆min\n4H2,\nα0\nnH+βn+n/summationdisplay\ni=1αi\nn·∆˘Vki\nh\nh′(ki\nh,h)(xki\nh\nh′(ki\nh,h))−∆min\n4H2/bracerightigg\n≥min/braceleftbigg\n∆Qk−1\nh(x,a)−(H+1−h)∆min\n4H2,\nα0\nnH+βn+n/summationdisplay\ni=1αi\nn·∆Vki\nh\nh′(ki\nh,h)(xki\nh\nh′(ki\nh,h))−(H−h)∆min\n4H2−∆min\n4H2/bracerightigg\n(51)\n=∆Qk\nh(x,a)−(H+1−h)∆min\n4H2,\nwhere (51) used the inductive hypothesis (in particular, ( 50) at steps h′> h). This establishes the\ninductive step for ( 49).\nWe proceed to lower bound ∆ ˘Vk\nh(x,a). For ﬁxed x,a, seta⋆= argmaxa′∈Ak\nh(x){Qk\nh(x,a′)−\nQk\nh(x,a′)}. Using Deﬁnition 5.1, the inductive hypothesis, and the validity of ( 49) for step hat\nepisodek, we have\n∆˘Vk\nh(x) =min{∆˘Vk−1\nh(x),∆˘Qk\nh(x,a⋆)}\n≥min{∆Vk−1\nh(x),∆Qk\nh(x,a⋆)}−(H+1−h)∆min\n4H2,\nwhich completes the inductive step for ( 50).\nThe following straightforward lemma shows that the clipped range functions are smaller than\nthe range functions.\nLemma 6.6. For all(x,h,k,a)∈S×A× [H]×[K]for which x/\\⌉}atio\\slash∈Gk\nhanda∈Ak\nh(x), it holds that\n∆˘Qk\nh(x,a)≤∆Qk\nh(x,a)and ∆˘Vk\nh(x)≤∆Vk\nh(x).\nProof.The lemma is a straightforward consequence of Deﬁnitions 4.1and5.1and induction on\nh,k(in particular, forward induction on kand reverse induction on h): in particular, for any\nh,k, having established the statement for all ( h′,k′) with either h′> hork′< k, we have that\n∆˘Qk\nh(x,a)≤∆Qk\nh(x,a) since\nα0\nnH+clip/bracketleftbigg\nβn|∆min\n4H2/bracketrightbigg\n+n/summationdisplay\ni=1αi\nn·∆˘Vki\nh(x,a)\nh′(ki\nh(x,a),h)(xki\nh(x,a)\nh′(ki\nh(x,a),h))\n≤α0\nnH+βn+n/summationdisplay\ni=1αi\nn·∆˘Vki\nh(x,a)\nh′(ki\nh(x,a),h)(xki\nh(x,a)\nh′(ki\nh(x,a),h)).\n32\n\nIt then follows immediately that ∆ ˘Vk\nh(x)≤∆Vk\nh(x).\nLemma6.7establishes some monotonicity (with respect to k) properties of the range functions,\nanalogously to Lemma 6.3.\nLemma 6.7. For all(x,a,h,k)so thatx/\\⌉}atio\\slash∈Gk+1\nhanda∈Ak+1\nh(x), the following inequalities hold\ntrue:\n∆˘Qk+1\nh(x,a)≤∆˘Qk\nh(x,a) (52)\n∆˘Vk+1\nh(x)≤∆˘Vk\nh(x)\n∆Qk+1\nh(x,a)≤∆Qk\nh(x,a)\n∆Vk+1\nh(x)≤∆Vk\nh(x). (53)\nMoreover, for all (x,a,h,k)∈S×A× [H]×[K], it holds that\n∆˚Qk+1\nh(x,a)≤∆˚Qk\nh(x,a). (54)\nProof.The ﬁrst four inequalities are immediate from Deﬁnitions 4.1and5.1. The ﬁnal inequality\nfollows from Deﬁnition 5.2and (52).\nLemma6.8establishes some further monotonicity properties for QLearningPreds .\nLemma 6.8. The following statements hold true:\n1. When QLearningPreds is run with either DeltaConst orDeltaIncr , for allk∈[K], it holds\nthat/hatwide∆k+1≥/hatwide∆k.\n2. For any h∈[H]andk < k′for which xk\nh=xk′\nh, we have τk′\nh≤τk\nh.\nProof.Webeginwiththeﬁrststatement; itisimmediatefor DeltaConst . Inthecaseof DeltaIncr ,\nwe note that by ( 54) of Lemma 6.7, ∆˚Qk\nh(x,a) is non-increasing as a function of kfor allx,a,h. It\nis clear that the same is true of /tildewide∆˚Qk\nh(x,a) (deﬁned in step 1of Algorithm 4). Thus the expression\nin (13) is non-decreasing as a function of k.\nTo see the second statement, note that if τk\nh= 0, then either |Ak\nh(xk\nh)|= 1, in which case it\nwill hold that|Ak′\nh(xk\nh)|= 1 (and so τk′\nh= 0), or ∆ Vk\nh(xk\nh)≤ϕh(/hatwide∆k), in which case it holds that\n∆Vk′\nn(xk\nh)≤ϕh(/hatwide∆k)≤ϕh(/hatwide∆k′) (and so τk′\nh= 0), by ( 53) and the ﬁrst item of this lemma.\nRecall that the clip function is deﬁned as follows: for real n umbersx,y, we have clip[ x|y] =\nx· /BD[x≥y]. We next state some lemmas establishing useful properties of the clip function in\nLemmas 6.9,6.10, and6.11below.\nLemma 6.9 (Claim A.8, [ XMD21]).For any positive integers a,b,cso thata+b≥cand any\nx∈(0,1), it holds that\na+b≤clip/bracketleftig\na|xc\n2/bracketrightig\n+(1+x)b.\nLemma 6.10 (Claim A.13, [ XMD21]).For any c,ǫ >0, it holds that\n∞/summationdisplay\nn=1clip/bracketleftbiggc√n/vextendsingle/vextendsingle/vextendsingle/vextendsingleǫ/bracketrightbigg\n≤4c2\nǫ.\n33\n\nLemma 6.11. Fix some c >0andh∈[H]. Forn∈N, writeγn=c/√n. Then for any function\nθ:S×A→ R≥0, and any subsetW⊂[K]of sizeM:=|W|, it holds that\n/summationdisplay\nk∈Wclip/bracketleftig\nγnk\nh/vextendsingle/vextendsingle/vextendsingleθ(xk\nh,ak\nh)/bracketrightig\n≤min\n\n2c√\nSAM,/summationdisplay\n(x,a)∈S×A4c2\nθ(x,a)\n\n.\nProof.For (x,a)∈S×A, letWx,a:={k∈W: (xk\nh,ak\nh) = (x,a)}. Then, on the one hand, we have\n/summationdisplay\nk∈Wclip/bracketleftig\nγnk\nh/vextendsingle/vextendsingle/vextendsingleθ(xk\nh,ak\nh)/bracketrightig\n=/summationdisplay\n(x,a)∈S×A/summationdisplay\nk∈Wx,aclip\nc/radicalig\nnk\nh/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ(x,a)\n\n≤/summationdisplay\n(x,a)∈S×A4c2\nθ(x,a),\nwhere the inequality uses Lemma 6.10. On the other hand, we have\n/summationdisplay\nk∈Wclip/bracketleftig\nγnk\nh/vextendsingle/vextendsingle/vextendsingleθ(xk\nh,ak\nh)/bracketrightig\n=/summationdisplay\n(x,a)∈S×A/summationdisplay\nk∈Wx,aclip\nc/radicalig\nnk\nh/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleθ(x,a)\n\n≤/summationdisplay\n(x,a)∈S×A|Wx,a|/summationdisplay\ni=1c√\ni\n≤/summationdisplay\n(x,a)∈S×A2c/radicalig\n|Wx,a|\n≤2c√\nSAM,\nwhere the ﬁnal inequality follows since/summationtext\n(x,a)∈S×A|Wx,a|=M.\n6.3 Bounding the clipped range functions\nFor all (h,k)∈[H]×[K] so that xk\nh/\\⌉}atio\\slash∈Gk\nh(so that ∆ Vk\nh(xk\nh) is deﬁned), write\n˘δk\nh:= ∆˘Vk\nh(xk\nh).\nSinceak\nh∈Ak\nh(xk\nh), ∆˘Qk\nh(xk\nh,ak\nh) is deﬁned, and we may thus further write\n˘θk\nh:= ∆˘Qk\nh(xk\nh,ak\nh).\nPertheregret decompositioninLemma 6.15, wewillboundtheregret E/bracketleftig/summationtextK\nk=1(V⋆\n1(x1)−Vπk\n1(x1))/bracketrightig\nby the quantity/summationtext\n(k,h):ak\nh/\\⌉gatio\\slash∈Aopt\nh,0(xk\nh)˘δk\nh(conditioned on thehigh-probability event Ewc). In this section\nwe prove an upper bound on this latter quantity. In fact, we pr ove a more general result which\nupper bounds/summationtext\n(k,h)∈W˘δk\nhfor various setsW⊂[K]×[H]; we will need this more general result in\norder to establish improved regret bounds in the case when th e predictions /tildewideQhare accurate.\nWe begin by deﬁning the type of set W⊂[K]×[H] for which we obtain such an upper bound,\nnamelylevel-hsets.\n34\n\nDeﬁnition 6.1. Fixh∈[H]. We say that a subset W⊂[K]×[H] is alevel-hset if the following\nconditions hold:\n1. For each k∈[K], there is at most one element ( /tildewidek,/tildewideh)∈Wso that/tildewidek=k.\n2. For all (/tildewidek,/tildewideh)∈W, it holds that both τ/tildewidek\n/tildewideh= 1 and/tildewideh≥h.\n3. For each (/tildewidek,/tildewideh)∈Wfor which ˜h > h, forh≤h′<˜h, it holds that x˜k\nh′∈G˜k\nh′.\nFor a level- hset,W, we next deﬁne its reduction , which replaces each element ( ˜k,˜h)∈Wwith\n˜h=hwith another element ( k′,h), where k′≤˜kis as small as possible subject to ( x˜k\nh,a˜k\nh) =\n(xk′\nh,ak′\nh), and to the constraint that all elements in Ware distinct. The reason that we will want\nto perform this operation is that our bounds on conﬁdence int ervals (in particular, ( 23), which\nmanifests in Deﬁnitions 4.1and5.1) are given in terms of the ﬁrstntimes a particular state-action\npair (x,a,h) is visited. The reduction Rh(W) has the property that for any ( x,a), if there are m\nelements ( ˜k,h)∈Rh(W) with (x˜k\nh,a˜k\nh) = (x,a), then those values ˜krepresent the ﬁrstmepisodes\nat which ( x,a,h) is visited.\nDeﬁnition 6.2. Fixh∈[H], and consider a level- hsetW⊂[K]×[H]. Thelevel-hreduction of\nW, denotedRh(W), is deﬁned as follows: starting with W, perform the following procedure:\n•For each ( x,a)∈S×A, letS(x,a) denote the set of elements ( ˜k,h)∈Wfor which ( x˜k\nh,a˜k\nh) =\n(x,a). Remove the elements of S(x,a) fromW, and insert the elements\n(k1\nh(x,a),h),(k2\nh(x,a),h),...,(k|S(x,a)|\nh(x,a),h) (55)\nintoW. (Recall that, for any s >0,k1\nh(x,a),...,ks\nh(x,a) are the smallest spositive integers\n˜kso that ( x˜k\nh,a˜k\nh) = (x,a).)\nNote that the level- hreduction satisﬁes the following inequality:\nmax\n(˜k,˜h)∈W{˜k}≥max\n(˜k,˜h)∈Rh(W){˜k}. (56)\nThe following lemma shows that the level- hreduction of a level- hset is a level- hset.\nLemma 6.12. Suppose thatW⊂[K]×[H]is a level- hset for some h∈[H]. Then the level- h\nreductionRh(W)is also a level- hset.\nProof.We ﬁrst verify that Rh(W) satisﬁes property 1of Deﬁnition 6.1. Using the notation of\nDeﬁnition 6.2, we must check that, for each ( x,a)∈S×A, for 1≤i≤|S(x,a)|, there is no ˜h > h\nso that ( ki\nh(x,a),˜h)∈W. However, if this were the case for some iand˜h, sinceWis a level- hset,\nitem3of Deﬁnition 6.1gives us that x=xki\nh(x,a)\nh∈Gki\nh(x,a)\nh. But then we must have τki\nh(x,a)\nh= 0,\nwhich contradicts the fact that for some ˜k≥ki\nh(x,a) so that x˜k\nh=x,τ˜k\nh= 1 and Lemma 6.8.\nThat the conditions of item 2of Deﬁnition 6.1hold forRh(W) follows directly from Lemma\n6.8, andRh(W) satisﬁes the conditions of item 3sinceWdoes.\n35\n\nWe are now ready to state and prove Lemma 6.13, which is the main technical component of\nthe worst-case (i.e., robustness) regret bounds in item 1of Theorem 3.1and item 1of Theorem 3.2.\nThe ﬁrst part (item 1) of Lemma 6.13bounds/summationtext\n(k,˜h)˘δk\n˜hfor any level- hsetW(for any h∈[H]),\nvia a quantity (denoted by fbelow) that depends on |W|, the step index h, and the largest episode\nnumberinW. Thesecond part (item 2) of the lemma then extends this upperboundto a somewhat\nmore general family of subsets W⊂[K]×[H].\nLemma 6.13. For allh∈[H], the following statements hold:\n1. For any level- hsetW⊂[K]×[H]and any k⋆satisfying k⋆≥kfor all(k,˜h)∈W, it holds\nthat/summationtext\n(k,˜h)∈W˘δk\n˜h≤f(|W|,h,k⋆), where for M∈N, h∈[H],\nf(M,h,k⋆) :=M·/parenleftbigg\n1+1\nH/parenrightbigg2\n·ϕh+1(/hatwide∆k⋆)\n+H/summationdisplay\nh′=h/parenleftbigg\n1+1\nH/parenrightbigg2(H−h)\nSAH+min\n\nC2√\nH3SAMι,/summationdisplay\n(x,a)∈S×AC2\n2H3ι\nmax/braceleftbigg\n∆˚Qk⋆\nh′(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n\n\n≤M·ϕh(/hatwide∆k⋆)+e2SAH2+min\n\ne2C2√\nH5SAMι,/summationdisplay\n(x,a,h′)∈S×A× [H]e2C2\n2H3ι\nmax/braceleftbigg\n∆˚Qk⋆\nh′(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n.\n(57)\nfor the constant C2= 8C0.\n2. Fix any setW⊂[K]×[H](not necessarily a level- hset), so that for all (k,˜h)∈W,˜h=h\nandxk\nh/\\⌉}atio\\slash∈Gk\nh. For any k⋆so thatk⋆≥kfor all(k,h)∈W, it holds that/summationtext\n(k,˜h)∈W˘δk\n˜h≤\nf(|W|,˜h,k⋆).\nProof.In the proof of the lemma we will often use the following fact: for all (k,h)∈[K]×[H] for\nwhichτk\nh= 1, by Deﬁnition 5.1and the choice of ak\nh, it holds that\n˘δk\nh= ∆˘Vk\nh(xk\nh)≤∆˘Qk\nh(xk\nh,ak\nh) =˘θk\nh. (58)\nWe willusereverseinductionon htoprovethestatement of thelemma. Thebasecase h=H+1\nis immediate from the convention that ˘δk\nH+1= 0 for all k∈[K].\nNow we treat the inductive case. Fix h≤H, and suppose that the lemma statement holds\nfor allh′> h. For (x,a)∈S×A , letZh(x,a) denote the set of all episodes k∈[K] for which\n(xk\nh,ak\nh) = (x,a) andτk\nh= 1. For a positive integer m, letZm\nh(x,a) denote the set consisting of the\nmsmallest elements of Zh(x,a) (or all ofZh(x,a), ifm >|Zh(x,a)|).\nFix anyk,hso thatτk\nh= 1, and write ki\nh:=ki\nh(xk\nh,ak\nh). Then, by Deﬁnition 5.1and (58),\n˘δk\nh≤˘θk\nh=α0\nnk\nh·H+nk\nh/summationdisplay\ni=1αi\nnk\nh·/parenleftig\n∆Vki\nh\nh′(ki\nh,h)(xki\nh\nh′(ki\nh,h))/parenrightig\n+clip/bracketleftbigg\nβn|∆min\n4H2/bracketrightbigg\n(59)\n=α0\nnk\nh·H+nk\nh/summationdisplay\ni=1αi\nnk\nh·/parenleftig\n˘δki\nh\nh′(ki\nh,h)/parenrightig\n+clip/bracketleftbigg\nβn|∆min\n4H2/bracketrightbigg\n. (60)\n36\n\nFix anym∈N, as well as any ( x,a)∈S×A. As before we abbreviate ki\nh=ki\nh(x,a). We next\nwork towards an upper bound on/summationtext\nk∈Zm\nh(x,a)˘δk\nh, using (60) for each k∈Zm\nh(x,a). We ﬁrst sum\nthe ﬁrst term of ( 60) over all k∈Zm\nh(x,a):\n/summationdisplay\nk∈Zm\nh(x,a)H·α0\nnk\nh≤H/summationdisplay\nk∈Zh(x,a)\n/BD[nk\nh= 0]≤H, (61)\nwhere the ﬁrst inequality follows since α0\n0= 1 and α0\nt= 0 fort >0, and the second inequality\nfollows since for all k∈Zh(x,a), we have ( xk\nh,ak\nh) = (x,a) and there can only be a single episode\ninZh(x,a) during which we ﬁrst visit ( x,a).\nThe sum of the second and third terms of ( 60) may be bounded as follows: if nk\nh>0,\nnk\nh/summationdisplay\ni=1αi\nnk\nh·˘δki\nh\nh′(ki\nh,h)+clip/bracketleftbigg\nβnk\nh/vextendsingle/vextendsingle/vextendsingle∆min\n4H2/bracketrightbigg\n≤clip/bracketleftigg\nclip/bracketleftbigg\nβnk\nh/vextendsingle/vextendsingle/vextendsingle∆min\n4H2/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle˘θk\nh\n2H/bracketrightigg\n+/parenleftbigg\n1+1\nH/parenrightbigg\n·nk\nh/summationdisplay\ni=1αi\nnk\nh·˘δki\nh\nh′(ki\nh,h)(62)\n≤clip/bracketleftigg\nβnk\nh/vextendsingle/vextendsingle/vextendsinglemax/braceleftigg\n∆min\n4H2,˘θk\nh\n2H/bracerightigg/bracketrightigg\n+/parenleftbigg\n1+1\nH/parenrightbigg\n·nk\nh/summationdisplay\ni=1αi\nnk\nh·˘δki\nh\nh′(ki\nh,h), (63)\nwhere (62) follows from Lemma 6.9and (60) as well as the fact that α0\nnk\nh= 0 asnk\nh>0. In the\ncase that nk\nh= 0, we have that/summationtextnk\nh\ni=1αi\nnk\nh·˘δki\nh\nh′(ki\nh,h)+clip/bracketleftig\nβnk\nh/vextendsingle/vextendsingle/vextendsingle∆min\n4H/bracketrightig\n= 0 since β0= 0 by deﬁnition\n(see (10)).\nNext, summing ( 60) over all k∈Zm\nh(x,a), and using ( 61) and (63), we see that\n/summationdisplay\nk∈Zm\nh(x,a)˘δk\nh≤H+/summationdisplay\nk∈Zm\nh(x,a)\nclip/bracketleftigg\nβnk\nh/vextendsingle/vextendsingle/vextendsinglemax/braceleftigg\n∆min\n4H2,˘θk\nh\n2H/bracerightigg/bracketrightigg\n+/parenleftbigg\n1+1\nH/parenrightbigg\n·nk\nh/summationdisplay\ni=1αi\nnk\nh·˘δki\nh\nh′(ki\nh,h)\n\n≤H+/summationdisplay\nk∈Zm\nh(x,a)clip/bracketleftigg\nβnk\nh/vextendsingle/vextendsingle/vextendsinglemax/braceleftigg\n∆min\n4H2,˘θk\nh\n2H/bracerightigg/bracketrightigg\n+/parenleftbigg\n1+1\nH/parenrightbigg\n·/summationdisplay\nk′∈Zm\nh(x,a)˘δk′\nh′(k′,h)∞/summationdisplay\nt=nk′\nhαnk′\nh\nt\n(64)\n=H+/summationdisplay\nk∈Zm\nh(x,a)clip/bracketleftigg\nβnk\nh/vextendsingle/vextendsingle/vextendsinglemax/braceleftigg\n∆min\n4H2,˘θk\nh\n2H/bracerightigg/bracketrightigg\n+/parenleftbigg\n1+1\nH/parenrightbigg2\n·/summationdisplay\nk′∈Zm\nh(x,a)˘δk′\nh′(k′,h)(65)\n≤H+/summationdisplay\nk∈Zm\nh(x,a)clip/bracketleftigg\nβnk\nh/vextendsingle/vextendsingle/vextendsinglemax/braceleftigg\n∆min\n4H2,˘θk\nh\n2H/bracerightigg/bracketrightigg\n+/parenleftbigg\n1+1\nH/parenrightbigg2\n·\n/summationdisplay\nk′∈Zm\nh(x,a):τk′\nh′(k′,h)=0ϕh′(k′,h)(/hatwide∆k′)+/summationdisplay\nk′∈Zm\nh(x,a):τk′\nh′(k′,h)=1˘δk′\nh′(k′,h)\n,\n(66)\n37\n\nwhere (64) follows from exchanging the order of summation, ( 65) uses item 3of Lemma A.1,\nand (66) uses the fact that for all k′∈Zm\nh(x,a) for which τk′\nh′(k′,h)= 0, we have that ˘δk′\nh′(k′,h)≤\n∆Vk′\nh′(k′,h)(xk′\nh′(k′,h))≤ϕh′(k′,h)(/hatwide∆k′)(byLemma 6.6), sincebydeﬁnitionof h′(k′,h), eitherh′(k′,h) =\nH+1 or else xk′\nh′(k′,h)/\\⌉}atio\\slash∈Gk′\nh′(k′,h).\nConsider any level- hsetW ⊂[K]×[H], and consider any k⋆≥max(˜k,˜h)∈W{˜k}. For each\n(x,a)∈S×A, letm(x,a) denote the number of elements ( /tildewidek,h)∈Wfor which ( x/tildewidek\nh,a/tildewidek\nh) = (x,a).\nLetM1be the number of ( /tildewidek,/tildewideh)∈Wso that either /tildewideh > hor/tildewideh=handτ/tildewidek\nh′(/tildewidek,h)= 1,M0be the\nnumber of (/tildewidek,/tildewideh)∈Wso that/tildewideh=handτ/tildewidek\nh′(/tildewidek,h)= 0, and M:=M0+M1=|W|. Then\n/summationdisplay\n(˜k,˜h)∈W˘δ˜k\n˜h≤/summationdisplay\n(/tildewidek,/tildewideh)∈Rh(W)˘δ/tildewidek\n/tildewideh(By Lemma 6.7)\n=/summationdisplay\n(x,a)∈S×Am(x,a)/summationdisplay\ni=1˘δki\nh(x,a)\nh+/summationdisplay\n(˜k,˜h)∈Rh(W):˜h>h˘δ˜k\n˜h\n≤SAH+/summationdisplay\n(x,a)∈S×A/summationdisplay\nk∈Zm(x,a)\nh(x,a)clip/bracketleftigg\nβnk\nh/vextendsingle/vextendsingle/vextendsinglemax/braceleftigg\n∆min\n4H2,˘θk\nh\n2H/bracerightigg/bracketrightigg\n+/summationdisplay\n(˜k,˜h)∈Rh(W):˜h>h˘δ˜k\n˜h\n+/parenleftbigg\n1+1\nH/parenrightbigg2\n·\nM0·ϕh+1(/hatwide∆k⋆)+/summationdisplay\n(x,a)∈S×A/summationdisplay\nk′∈Zm(x,a)\nh(x,a):τk′\nh′(k′,h)=1˘δk′\nh′(k′,h)\n\n(By (66))\n≤SAH+/summationdisplay\n(x,a)∈S×A/summationdisplay\nk∈Zm(x,a)\nh(x,a)clip/bracketleftigg\nβnk\nh/vextendsingle/vextendsingle/vextendsinglemax/braceleftigg\n∆min\n4H2,∆˚Qk⋆\nh(x,a)\n2H/bracerightigg/bracketrightigg\n+/parenleftbigg\n1+1\nH/parenrightbigg2\n·M0·ϕh+1(/hatwide∆k⋆)+/parenleftbigg\n1+1\nH/parenrightbigg2\n·/summationdisplay\n(˜k,˜h)∈W′˘δ˜k\n˜h, (67)\nwhere\nW′:=/braceleftig\n(˜k,˜h)∈Rh(W) :˜h > h/bracerightig\n∪/braceleftig\n(˜k,h′(˜k,h)) : (˜k,h)∈Rh(W), τ˜k\nh′(˜k,h)= 1/bracerightig\n,\nso that|W′|=M1and max(˜k,˜h)∈W′{˜k}≤k⋆. Moreover, in ( 67), we have used that by Lemma\n6.7,˘θk\nh= ∆˘Qk\nh(xk\nh,ak\nh)≥∆˚Qk⋆\nh(xk\nh,ak\nh) for allk∈Zm(x,a)\nh(sinceτk\nh= 1 for all k∈Zm(x,a)\nh(x,a)).\nWe claim thatW′is a level-( h+1) set. For any k∈[K], if (k,˜h)∈Rh(W) for some ˜h > h, then\nsinceRh(W) is a level- hset (Lemma 6.12), it must hold that xk\nh∈Gk\nh, meaning that τk\nh= 0, and\nthus (k,h)/\\⌉}atio\\slash∈Rh(W). This veriﬁes that W′satisﬁes condition 1of Deﬁnition 6.1. It is immediate\nthat for all ( ˜k,˜h)∈W′, we have τ˜k\n˜h= 1 and ˜h≥h+1 (condition 2), and condition 3follows from\nthe corresponding condition for Rh(W) as well as the fact that for all ( ˜k,h)∈Rh(W), for all h′\nsatisfying h+1≤h′< h′(˜k,h), we have x˜k\nh′∈G˜k\nh′.\n38\n\nThus, we may apply the inductive hypothesis for the set W′, which gives, together with ( 67)\nand Lemma 6.11, withθ(x,a) = max/braceleftbigg\n∆˚Qk⋆\nh(x,a)\n2H,∆min\n4H2/bracerightbigg\nand the setWin Lemma 6.11set to\n{k:∃(x,a)∈S×A s.t.k∈Zm(x,a)\nh(x,a)},\n/summationdisplay\n(˜k,˜h)∈W˘δ˜k\n˜h≤SAH+min\n\n8C0√\nH3SAMι,/summationdisplay\n(x,a)∈S×A64C2\n0H3ι\nmax/braceleftbigg\n∆˚Qk⋆\nh(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n\n+M0·/parenleftbigg\n1+1\nH/parenrightbigg2\n·ϕh+1(/hatwide∆k⋆)+/parenleftbigg\n1+1\nH/parenrightbigg2\n·f(M1,h+1,k⋆)\n≤M·/parenleftbigg\n1+1\nH/parenrightbigg2\n·ϕh+1(/hatwide∆k⋆)\n+H/summationdisplay\nh′=h/parenleftbigg\n1+1\nH/parenrightbigg2(H−h)\n·\nSAH+min\n\nC2√\nH3SAMι,/summationdisplay\n(x,a)∈S×AC2\n2H3ι\n∆˚Qk⋆\nh′(x,a)\n\n\n\n=f(M,h,k⋆),\nthus establishing item 1of the lemma.\nNext we establish item 2of the lemma. Fix any set W⊂[K]×[H] so that for all ( k,˜h)∈W,\n˜h=handxk\nh/\\⌉}atio\\slash∈Gk\nh. Suppose further that k⋆satisﬁes k⋆≥kfor all (k,h)∈W. Thus, for all\n(k,h)∈W, eitherτk\nh= 1 or˘δk\nh= ∆˘Vk\nh(xk\nh)≤ϕh(/hatwide∆k). Note also that W′:={(k,h)∈W:τk\nh= 1}\nis a level- hset. Then, using item 1on the setW′,\n/summationdisplay\n(k,h)∈W˘δk\nh≤/summationdisplay\n(k,h)∈W:τk\nh=0ϕh(/hatwide∆k)+/summationdisplay\n(k,h)∈W′˘δk\nh\n≤|W\\W′|·ϕh(/hatwide∆k⋆)+f(|W′|,h,k⋆)\n≤f(|W|,h,k⋆),\nas desired.\n6.4 Establishing the robustness regret bounds\nIn this section we prove a regret decomposition in Lemma 6.15and combine it with Lemma 6.13,\nwhich will suﬃce for proving the robustness regret bounds in Theorems 3.1and3.2. Lemma 6.14\nbelow is needed to prove the regret decomposition bound. It s tates that the loss incurred by\nchoosing any non-optimal action ak\nhat a state xk\nhmay be bounded by the clipped value function\n˘δk\nh; the statement (and proof) is similar to that of in Lemma 4.4 o f [XMD21].\nLemma 6.14. For all(h,k)∈[H]×[K]for which xk\nh/\\⌉}atio\\slash∈Gk\nhandak\nh/\\⌉}atio\\slash∈Aopt\nh,0(xk\nh), it holds, under the\neventEwc, that\nV⋆\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh)≤4·˘δk\nh.\n39\n\nProof.We assume throughout the proof that the event Ewcholds (in particular, this allows us to\napply Lemma 6.4). Sinceak\nh∈Ak\nh(xk\nh), we have Qk\nh(xk\nh,ak\nh)≥Vk\nh(xk\nh), and so\nV⋆\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh)≤Vk\nh(xk\nh)−Vk\nh(xk\nh)+Qk\nh(xk\nh,ak\nh)−Qk\nh(xk\nh,ak\nh).\nWe may bound Vk\nh(xk\nh)−Vk\nh(xk\nh) as follows:\nVk\nh(xk\nh)−Vk\nh(xk\nh)\n≤∆Vk\nh(xk\nh) (By Lemma 6.4and since xk\nh/\\⌉}atio\\slash∈Gk\nh)\n≤∆˘Vk\nh(xk\nh)+∆min\n4=˘δk\nh+∆min\n4. (By Lemma 6.5)\nByDeﬁnition 5.1,thereissome k′≤ksothat˘δk\nh= ∆˘Vk\nh(xk\nh) = ∆˘Qk′\nh(xk′\nh,a⋆)fora⋆= argmaxa′∈Ak′\nh(xk\nh)Qk′\nh(x,a′)−\nQk′\nh(x,a′). Now we have\nQk\nh(xk\nh,ak\nh)−Qk\nh(xk\nh,ak\nh)\n≤Qk′\nh(xk\nh,ak\nh)−Qk′\nh(xk\nh,ak\nh) (By Lemma 6.3)\n≤Qk′\nh(xk\nh,a⋆)−Qk′\nh(xk\nh,a⋆) (Since ak\nh∈Ak\nh(xk\nh)⊆Ak′\nh(xk\nh))\n≤∆Qk′\nh(xk\nh,a⋆) (By Lemma 6.4and since xk\nh/\\⌉}atio\\slash∈Gk′\nh)\n≤∆˘Qk′\nh(xk\nh,a⋆)+∆min\n4(By Lemma 6.5)\n=˘δk\nh+∆min\n4.\nSinceak\nh/\\⌉}atio\\slash∈ Aopt\nh,0(xk\nh), we have that V⋆\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh)≥∆min. Thus ∆ min/2≤(V⋆\nh(xk\nh)−\nQ⋆\nh(xk\nh,ak\nh))/2, meaning that V⋆\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh)≤4·˘δk\nh, as desired.\nNext we state and prove the regret decomposition bound which is used to bound the worst-case\nregret.\nLemma 6.15 (Regret decomposition for worst-case bound) .For the choice p= 1/(H2K), the\nregret of QLearningPreds may be bounded as follows:\nK/summationdisplay\nk=1E/bracketleftig\nV⋆\n1(xk\n1)−Vπk\n1(xk\n1)/bracketrightig\n≤1+4·E\n/summationdisplay\n(k,h):ak\nh/\\⌉gatio\\slash∈Aopt\nh,0(xk\nh)˘δk\nh/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleEwc\n.\n40\n\nProof.Note that\nK/summationdisplay\nk=1E/bracketleftig\n(V⋆\n1−Vπk\n1)(xk\n1)/bracketrightig\n=K/summationdisplay\nk=1Eπk/bracketleftiggH/summationdisplay\nh=1V⋆\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh)/bracketrightigg\n≤K/summationdisplay\nk=1Eπk/bracketleftiggH/summationdisplay\nh=1\n/BD[ak\nh/\\⌉}atio\\slash∈Aopt\nh,0(xk\nh)]·(V⋆\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh))/bracketrightigg\n≤K/summationdisplay\nk=1Eπk/bracketleftiggH/summationdisplay\nh=1\n/BD[ak\nh/\\⌉}atio\\slash∈Aopt\nh,0(xk\nh)]·(V⋆\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh))|Ewc/bracketrightigg\n+KH2·Pr[Ewc].\nNote that Pr[Ewc]≤p, which may be bounded above by 1 /(H2K) if we choose p= 1/(H2K).\nNow let us condition on the event Ewc. Sinceak\nh∈Ak\nh(xk\nh) for all h,k, and in the event that\n|Ak\nh(xk\nh)|= 1 it must be the case that Ak\nh(xk\nh) contains the optimal action at xk\nh(Lemma 6.1, item\n2) under the event Ewc,ak\nh/\\⌉}atio\\slash∈Aopt\nh,0(xk\nh) implies that xk\nh/\\⌉}atio\\slash∈Gk\nhunderEwc. Thus, conditioned on Ewc,\nusing Lemma 6.14, we have that\nK/summationdisplay\nk=1H/summationdisplay\nh=1\n/BD[ak\nh/\\⌉}atio\\slash∈Aopt\nh,0(xk\nh)]·(V⋆\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh))≤4·/summationdisplay\n(k,h):ak\nh/\\⌉gatio\\slash∈Aopt\nh,0(xk\nh)˘δk\nh.\nThis completes the proof of the lemma.\nTo combine Lemma 6.13with the regret decomposition result of Lemma 6.15, we need a way of\nupper bounding the left-hand side of ( 57) from Lemma 6.13, which looks much like the gap-based\nbound quantity in ( 6) used in Theorems 3.1and3.2, but with the actual gaps ∆ h(x,a) replaced\nby the proxies ∆ ˚Qk⋆\nh(x,a)≥∆˚QK\nh(x,a). Lemma 6.16below shows that the proxies ∆ ˚QK\nh(x,a) are\nindeed upper bounds on the true gaps ∆ h(x,a).\nLemma 6.16. Consider any (x,a,h)∈S×A× [H], and suppose the event Ewcholds. Then\nmax/braceleftigg\n∆˚QK\nh(x,a)\n2H,∆min\n4H2/bracerightigg\n≥max/braceleftbigg∆h(x,a)\n8H, /BD[Aopt\nh,0(x) ={a}]·∆min,h(x)\n8H,∆min\n4H2/bracerightbigg\n.(68)\nFurther, for any /tildewide∆min≤∆min, recalling the deﬁnition of /tildewide∆˚Qk\nhin step1of Algorithm 4, it holds\nthat\nmax/braceleftigg/tildewide∆˚QK\nh(x,a)\n2H,/tildewide∆min\n4H2/bracerightigg\n≥max/braceleftigg\n∆h(x,a)\n8H, /BD[Aopt\nh,0(x) ={a}]·∆min,h(x)\n8H,/tildewide∆min\n4H2/bracerightigg\n.(69)\nProof.Suppose the event Ewcholds (this allows us to apply Lemmas 6.4and6.14). By deﬁnition,\nthere is some k∈[K] so that ∆ ˚QK\nh(x,a) = ∆˘Qk\nh(x,a) and either ( xk\nh,ak\nh) = (x,a) andτk\nh= 1 or\nelsek= 1. In the case k= 1, we have ∆ ˚QK\nh(x,a) =H≥∆h(x,a). Otherwise, we consider two\ncases:\n41\n\n•Suppose a/\\⌉}atio\\slash∈Aopt\nh,0(x). Then\n∆˚QK\nh(x,a) = ∆˘Qk\nh(xk\nh,ak\nh)≥˘δk\nh≥1\n4·∆h(xk\nh,ak\nh) =1\n4·∆h(x,a),\nwhere the ﬁrst inequality follows from ( 58) and the second inequality follows from Lemma\n6.14.\n•Suppose that ais the unique action in Aopt\nh,0(x), i.e., thatAopt\nh,0(x) ={a}. Sinceτk\nh= 1, we\nhave that x/\\⌉}atio\\slash∈Gk\nh, meaning that there is some sub-optimal action remaining in Ak\nh(x), which\nwe denote by a′. Then\nQk\nh(xk\nh,a′)−Qk\nh(xk\nh,a′)≤Qk\nh(xk\nh,ak\nh)−Qk\nh(xk\nh,ak\nh)\n(Sinceak\nhmaximizes the conﬁdence interval)\n≤∆Qk\nh(xk\nh,ak\nh) (By Lemma 6.4)\n≤∆˘Qk\nh(xk\nh,ak\nh)+∆min\n4. (By Lemma 6.5)\nMoreover, as in the proof of Lemma 6.14, we have, by Lemmas 6.4and6.5as well as ( 58),\nVk\nh(xk\nh)−Vk\nh(xk\nh)≤∆Vk\nh(xk\nh)≤∆˘Vk\nh(xk\nh)+∆min\n4≤∆˘Qk\nh(xk\nh,ak\nh)+∆min\n4.\nCombining the above displays, we obtain\n∆min,h(x)≤∆h(x,a′)\n≤(Vk\nh(xk\nh)−Vk\nh(xk\nh))+(Qk\nh(xk\nh,a′)−Qk\nh(xk\nh,a′))\n≤2·∆˘Qk\nh(xk\nh,ak\nh)+∆min\n2,\nwhich implies that ∆ ˚QK\nh(x,a) = ∆˘Qk\nh(xk\nh,ak\nh)≥∆min,h(x)\n4.\nThe above two cases imply that∆˚QK\nh(x,a)\n2H≥max/braceleftig\n∆h(x,a)\n8H, /BD[Aopt\nh,0(x) ={a}]·∆min,h(x)\n8H/bracerightig\n. The ﬁrst\ninequality, ( 68), follows immediately.\nTo establish the second inequality, ( 69), of the lemma, we simply note that all arguments of\nthis lemma (including Lemmas 6.5and6.14) go through without modiﬁcation if ∆ minis replaced\nwith any lower bound /tildewide∆minof ∆minin the deﬁnitions of ∆ ˘Vk\nh,∆˘Qk\nh,∆˚Qk\nh.\nThe following lemma presents the worst-case regret bound fo rQLearningPreds with the sub-\nprocedure DeltaConst used to choose /hatwide∆k.\nLemma 6.17. Suppose T≥SAH3. When given as input any prediction function /tildewideQ, the regret of\nQLearningPreds (withDeltaConst and input parameter R≥max{SAH3,CM,T,1}) satisﬁes:\nE/bracketleftiggK/summationdisplay\nk=1(V⋆\n1−Vπk\n1)(xk\n1)/bracketrightigg\n≤O(R).\n42\n\nProof.We ﬁrst note that the regret decomposition of Lemma 6.15gives\nK/summationdisplay\nk=1E/bracketleftig\nV⋆\n1(xk\n1)−Vπk\n1(xk\n1)/bracketrightig\n≤1+4·E\n/summationdisplay\n(k,h):ak\nh/\\⌉gatio\\slash∈Aopt\nh,0(xk\nh)˘δk\nh/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleEwc\n.\nRecall that under the event Ewc,ak\nh/\\⌉}atio\\slash∈Aopt\nh,0(xk\nh) implies that xk\nh/\\⌉}atio\\slash∈Gk\nh. Thus, conditioned on Ewc,\nwe may bound 4/summationtext\n(k,h):ak\nh/\\⌉gatio\\slash∈Aopt\nh,0(xk\nh)˘δk\nhas follows:\n4·/summationdisplay\n(k,h):ak\nh/\\⌉gatio\\slash∈Aopt\nh,0(xk\nh)˘δk\nh\n≤4H·\nK·ϕ1(/hatwide∆K)+e2SAH2+min\n\ne2C2√\nH5SAKι,/summationdisplay\n(x,a,h)∈S×A× [H]e2C2\n2H3ι\nmax/braceleftbigg\n∆˚QK\nh(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n\n\n(Using item 2of Lemma 6.13)\n≤O(R)+O(SAH3)+O\nmin\n\n√\nH7SAKι,H6ι·\n/summationdisplay\n(x,a,h):a/\\⌉gatio\\slash∈Aopt\nh,0(x)1\n∆h(x,a)+|Amul|\n∆min\n\n\n\n\n(By the deﬁnition of /hatwide∆KinDeltaConst , Algorithm 3, and Lemma 6.16)\n≤O(R)+O(CM,T,1)≤O(R),\nwhere the second-to-last inequality follows from the fact t hatR≥SAH3.\nThe following lemma presents the worst-case regret bound fo rQLearningPreds with the sub-\nprocedure DeltaIncr used to choose /hatwide∆k.\nLemma 6.18. Suppose T≥SAH3. When given as input any prediction function /tildewideQ, the regret of\ntheQLearningPreds with input parameter λ(used with DeltaIncr and input parameter /tildewide∆min≤\n∆min) satisﬁes:\nE/bracketleftiggK/summationdisplay\nk=1(V⋆\n1−Vπk\n1)(xk\n1)/bracketrightigg\n≤O\nmin\n\n/radicalbigg\nSAH9Tι2\nλ,H8ι2\nλ·\n/summationdisplay\n(x,a,h):a/\\⌉gatio\\slash∈Aopt\nh,0(x)1\n∆h(x,a)+|Amul|\n/tildewide∆min\n\n\n\n.\nProof.We ﬁrst note that the regret decomposition of Lemma 6.15gives\nK/summationdisplay\nk=1E/bracketleftig\nV⋆\n1(xk\n1)−Vπk\n1(xk\n1)/bracketrightig\n≤1+4·E\n/summationdisplay\n(k,h):ak\nh/\\⌉gatio\\slash∈Aopt\nh,0(xk\nh)˘δk\nh/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleEwc\n.\nThe guarantee that /tildewide∆min≤∆mingives that/tildewide∆˚QK\nh(x,a)≤∆˚QK\nh(x,a) for all x,a,h. Recall that\nunder the eventEwc,ak\nh/\\⌉}atio\\slash∈Aopt\nh,0(xk\nh) implies that xk\nh/\\⌉}atio\\slash∈Gk\nh. Thus, conditioned on Ewc, we may bound\n43\n\n4/summationtext\n(k,h):ak\nh/\\⌉gatio\\slash∈Aopt\nh,0(xk\nh)˘δk\nhas follows:\n4·/summationdisplay\n(k,h):ak\nh/\\⌉gatio\\slash∈Aopt\nh,0(xk\nh)˘δk\nh\n≤4H·\nK·ϕ1(/hatwide∆K)+e2SAH2+min\n\ne2C2√\nH5SAKι,/summationdisplay\n(x,a,h)∈S×A× [H]e2C2\n2H3ι\nmax/braceleftbigg\n∆˚QK\nh(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n\n\n(Using item 2of Lemma 6.13)\n≤O\nSAH3+KH·min\n\nH5ι2\nλK·/summationdisplay\n(x,a,h)1\nmax/braceleftbigg\n/tildewide∆˚QK\nh(x,a)\n2H,/tildewide∆min\n4H2/bracerightbigg,/radicalbigg\nSAH8ι2\nλK\n\n\n.\n(By the deﬁnition of /hatwide∆KinDeltaIncr , Algorithm 4and/tildewide∆˚QK\nh≤∆˚Qk\nh)\nBy Lemma 6.16(in particular, ( 69)), we conclude that\nE/bracketleftiggK/summationdisplay\nk=1(V⋆\n1−Vπk\n1)(xk\n1)/bracketrightigg\n≤O\nSAH3+min\n\n/radicalbigg\nSAH10Kι2\nλ,H8ι2\nλ·\n/summationdisplay\n(x,a,h):a/\\⌉gatio\\slash∈Aopt\nh,0(x)1\n∆h(x,a)\n+/summationdisplay\n(x,h):|Aopt\nh,0(x)|=11\n∆min,h(x)+/summationdisplay\n(x,a,h)∈Amul1\n/tildewide∆min\n\n\n\n (70)\n≤O\nSAH3+min\n\n/radicalbigg\nSAH9Tι2\nλ,H8ι2\nλ·\n/summationdisplay\n(x,a,h):a/\\⌉gatio\\slash∈Aopt\nh,0(x)1\n∆h(x,a)+|Amul|\n/tildewide∆min\n\n\n\n.\nFinally, the fact that T≥SAH3implies that the term SAH3in the above expression is dominated\nby the second term (see also Lemma A.2).\nWe remark that in the proof of Lemma 6.18, if ∆min=|Amul|= 0, then the term|Amul|\n∆mincan\nbe interepreted as 0. This follows from the fact that in the in equality ( 70), the summation in the\nthird term/summationtext\n(x,a,h)∈Amul1\n∆minis over an empty set.\n7 Proofs for approximate distillation bound\nIn this section we establish the upper bounds in item 2of Theorem 3.1and item 2of Theorem\n3.2, which give a regret bound for QLearningPreds when the predictions /tildewideQhare anǫ-approximate\ndistillation of Q⋆\nh.\n44\n\n7.1 Bounding the number of exploration episodes\nA key challenge in establishing these bounds is to show that QLearningPreds does not spend too\nmany episodes ignoring the predictions /tildewideQhas part of the exploration phase. To this end, we bound\nthe number of episodes kfor which σk\nh= 1 (for each h∈[H]). Note that this is not exactly the\nsame as the number of episodes kfor which τk\nh= 1, and that it is the parameters τk\nh(notσk\nh)\nwhich correspond to whether the policy πk\nh(deﬁned in ( 11)) engages in exploration or constrained\nexploitation. We will show, however (in Claim 7.10), that those episodes kfor which σk\nh= 0 but\nτk\nh= 1 only contribute a small amount to the overall regret; this is in turn a consequence of Lemma\n7.6, which shows that if there is a non-optimal action in Ak\nh(x), then ∆ Vk\nh(x) (which is used to\ndeﬁneτk\nh) and ∆˘Qk\nh(x) (which is used to deﬁne σk\nh) must be close.\nRecall the deﬁnition of /hatwideλin Theorem 3.2. Lemma 7.1treats the case where DeltaConst is\nused to choose /hatwide∆k; it bounds, for each h∈[H], the number of episodes kfor which σk\nh= 1, as a\nfunction of/hatwideλ. The main tool in the proof is Lemma 6.13, which is used to show that the parameters\n˘δk\nh= ∆˘Vk\nh(xk\nh) decrease suﬃciently fast to ∆ ˘Vk\nh(xk\nh)≤1\n1+1\nH·ϕh(/hatwide∆k), i.e.,σk\nh= 0, for most episodes\nk.\nLemma 7.1. SupposeQLearningPreds is run with DeltaConst to choose the values /hatwide∆k. Then for\nallh∈[H], the number of episodes k∈[K]for which σk\nh= 1is at most max{SAH3,/hatwideλ·K}.\nProof.PerDeltaConst , we have that /hatwide∆k=R/(KH) for all k∈[K]. Therefore, throughout the\nproof of this lemma we will drop the superscript kand write/hatwide∆ :=/hatwide∆k(which holds for all k∈[K]).\nFor any ( h,k)∈[H]×[K], note that σk\nh= 1 implies that ϕh(/hatwide∆)<(1 + 1/H)·∆˘Vk\nh(xk\nh) =\n(1+1/H)·˘δk\nh. WriteYh:={k:σk\nh= 1}. Then for each h, we have that\n/summationdisplay\nk∈Yh˘δk\nh≥1\n1+1/H·ϕh(/hatwide∆)·|Yh|.\nUsing the above inequality and item 2of Lemma 6.13with the setW={(k,h) :σk\nh= 1}(which\nsatisﬁes the requirement that each ( k,h)∈Wsatisﬁesxk\nh/\\⌉}atio\\slash∈Gk\nh), we get that\nϕh(/hatwide∆)·|Yh|\n1+1/H\n≤/summationdisplay\nk∈Yh˘δk\nh\n≤|Yh|·(1+1/H)2·ϕh+1(/hatwide∆)+e2SAH2+min\n\ne2C2/radicalbig\nH5SA|Yh|ι,/summationdisplay\n(x,a,h′)∈S×A× [H]e2C2\n2H3ι\nmax/braceleftbigg\n∆˚QK\nh′(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n.\nRearranging and using the fact that ϕh(/hatwide∆\n1+1/H)−(1+1/H)2·ϕh+1(/hatwide∆)≥ϕh+1(/hatwide∆)\nH, we obtain that\n|Yh|·ϕh+1(/hatwide∆)\nH≤e2SAH2+min\n\ne2C2/radicalbig\nH5SA|Yh|ι,/summationdisplay\n(x,a,h′)∈S×A× [H]e2C2\n2H3ι\nmax/braceleftbigg\n∆˚QK\nh′(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n.\n45\n\nBy Lemma 6.16, it follows that\n|Yh|·ϕh+1(/hatwide∆)\nH≤e2SAH2+min\n\ne2C2/radicalbig\nH5SA|Yh|ι,16e2C2\n2H5ι·\n/summationdisplay\n(x,a,h′)∈S×A× [H]:a/\\⌉gatio\\slash∈Aopt\nh′,0(x)1\n∆h(x,a)+|Amul|\n∆min\n\n\n\n≤min\n\n2e2C2/radicalbig\nH5SA|Yh|ι,32e2C2\n2H5ι·\n/summationdisplay\n(x,a,h′)∈S×A× [H]:a/\\⌉gatio\\slash∈Aopt\nh′,0(x)1\n∆h(x,a)+|Amul|\n∆min\n\n\n,\n(71)\nwhere the second inequality above follows from the fact that , assuming|Yh|≥SAH3, both terms\nin the minimum are bounded below by e2SAH2. Recall that /hatwideλ≥SAH3/Kis deﬁned to be as\nsmall as possible so that\nR≥min\n\n/radicaligg\nH9SAKι\n/hatwideλ,1\n/hatwideλ·H7ι·\n/summationdisplay\n(x,a,h′)∈S×A× [H]:a/\\⌉gatio\\slash∈Aopt\nh′,0(x)1\n∆h(x,a)+|Amul|\n∆min\n\n\n,\nand that/hatwide∆ =R/(KH) (perDeltaConst ). We next consider two cases:\n1.R≥/radicalig\nH9SAKι\n/hatwideλ. Then from ( 71),\n|Yh|≤2e2C2/radicalbig\nH7SA|Yh|ι\nϕh+1(/hatwide∆)≤2e2C2\nC1·/radicalbig\nH7SA|Yh|ι/radicalig\nH7SAι/(/hatwideλ·K),\nwhich implies that\n/radicalbig\n|Yh|≤2e2C2\nC1·/radicalbig\n/hatwideλ·K,\nand in turn we get that |Yh|≤/hatwideλ·KsinceC1is chosen so that 2 e2C2≤C1.\n2.R≥1\n/hatwideλ·H7ι·/parenleftbigg/summationtext\n(x,a,h′)∈S×A× [H]:a/\\⌉gatio\\slash∈Aopt\nh′,0(x)1\n∆h(x,a)+|Amul|\n∆min/parenrightbigg\n. Then from ( 71),\n|Yh|≤32e2C2\n2H6ι\nϕh+1(/hatwide∆)·\n/summationdisplay\n(x,a,h′)∈S×A× [H]:a/\\⌉gatio\\slash∈Aopt\nh′,0(x)1\n∆h(x,a)+|Amul|\n∆min\n\n≤32e2C2\n2\nC1·K·/hatwideλ≤K·/hatwideλ,\nwhere the ﬁnal inequality follows since C1is chosen so that 32 e2C2\n2≤C1.\n46\n\nLemma7.2establishes the same result as Lemma 7.1, except for the choice of DeltaIncr in\nQLearningPreds . The proof is more subtle, though, because of the more comple x nature of the\nparameters/hatwide∆kinDeltaIncr . In particular, to establish Lemma 7.2, we need to divide the set of\nepisodes into diﬀerent phases, so that within each phase the v alue of/hatwide∆konly changes by a small\nmultiplicative factor.\nLemma 7.2. SupposeQLearningPreds is run with DeltaIncr (Algorithm 4) to choose the values\n/hatwide∆k. Then for all h∈[H], the number of episodes k∈[K]for which σk\nh= 1is at most max{SAH3,λ·\nK}.\nProof.Since ∆ ˘Q1\nh(x,a) =Hfor all (x,a,h), it holds that /hatwide∆1≥SAH\nλK. Also note that by deﬁ-\nnition we have /hatwide∆k≤/radicalig\nSAH8ι2\nλKfor allk. Note that/radicalig\nSAH8ι2\nλK·λK\nSAH≤√\nλKH6ι2. For 0≤i≤/ceilingleftig\nlog1+1\nH(√\nλKH6ι2)/ceilingrightig\n, setωi:= 2i·SAH\nλK.\nFor (h,k)∈[H]×[K], note that σk\nh= 1 implies that ϕh(/hatwide∆k)<(1 + 1/H)·∆˘Vk\nh(xk\nh) =\n(1+ 1/H)·˘δk\nh. For each 1≤i≤⌈log1+1/H(√\nλKH6ι2)⌉andh∈[H] setYi\nh:={k∈[K] :σk\nh=\n1, ωi−1≤/hatwide∆k≤ωi}.\nThen for each h∈[H] and 0≤i≤⌈log1+1\nH(√\nλKH6ι2)⌉,\n/summationdisplay\nk∈Yi\nh˘δk\nh≥|Yi\nh|·ϕh(ωi−1)\n1+1\nH. (72)\nSetYh:=/uniontext\niYi\nh. Fix any h∈[H] andisatisfying 1≤i≤⌈log1+1\nH(√\nλKH6ι2)⌉. Using ( 72) and\nthe statement of item 2of Lemma 6.13forW={(k,h) :k∈Yi\nh}, noting that for k⋆= maxk∈Yi\nh{k},\nwe have/hatwide∆k⋆≤ωi, we see that\n|Yi\nh|·ϕh(ωi−1)\n1+1/H\n≤/summationdisplay\nk∈Yi\nh˘δk\nh\n≤|Yi\nh|·/parenleftbigg\n1+1\nH/parenrightbigg2\n·ϕh+1(ωi)+e2SAH2+min\n\ne2C2/radicalig\nH5SA|Yi\nh|ι,/summationdisplay\n(x,a,h′)∈S×A× [H]e2C2\n2H3ι\nmax/braceleftbigg\n∆˚Qk⋆\nh′(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n.\nRearranging and using the fact thatϕh(ωi−1)\n1+1/H−/parenleftbig\n1+1\nH/parenrightbig2·ϕh+1(ωi−1)≥ϕh+1(ωi−1)/H, we obtain\nthat\n|Yi\nh|\nH·ϕh+1(ωi−1)≤e2SAH2+min\n\ne2C2/radicalig\nH5SA|Yi\nh|ι,/summationdisplay\n(x,a,h′)∈S×A× [H]e2C2\n2H3ι\nmax/braceleftbigg\n∆˚Qk⋆\nh′(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n\n≤e2SAH2+min\n\ne2C2/radicalig\nH5SA|Yi\nh|ι,/summationdisplay\n(x,a,h′)∈S×A× [H]e2C2\n2H3ι\nmax/braceleftbigg/tildewide∆˚Qk⋆\nh′(x,a)\n2H,/tildewide∆min\n4H2/bracerightbigg\n\n,\n(73)\n47\n\nwherethesecondinequalityabovefollowsfrom /tildewide∆min≤∆minandtherefore /tildewide∆˚Qk⋆\nh′(x,a)≤∆˚Qk⋆\nh′(x,a)\nfor allx,a,h′. We now consider two cases, based on the value of /hatwide∆k⋆(depending on which of the\ntwo terms in the minimum in ( 13) in the algorithm DeltaIncr is smaller):\n1. Suppose/hatwide∆k⋆=H5ι2\nλK·/summationtext\n(x,a,h)1\nmax/braceleftbigg/tildewide∆˚Qk⋆\nh(x,a)\n2H,/tildewide∆min\n4H2/bracerightbigg. Note that\nϕh+1(ωi−1)≥C1·ωi−1≥C1/(1+1/H)·ωi≥C1/(1+1/H)·/hatwide∆k⋆≥C1/2·/hatwide∆k⋆,(74)\nas well as⌈log1+1/H(√\nλKH6ι2)⌉≤8Hι. Then using ( 73), we get that\n|Yi\nh|≤1\nϕh+1(ωi−1)·\ne2SAH3+/summationdisplay\n(x,a,h′)e2C2\n2H4ι\nmax/braceleftbigg/tildewide∆˚Qk⋆\nh′(x,a)\n2H,/tildewide∆min\n4H2/bracerightbigg\n\n≤1\nϕh+1(ωi−1)·/summationdisplay\n(x,a,h′)2e2C2\n2H4ι\nmax/braceleftbigg/tildewide∆˚Qk⋆\nh′(x,a)\n2H,/tildewide∆min\n4H2/bracerightbigg (75)\n≤2·7Hι\nC1·/hatwide∆k⋆·⌈log1+1/H(√\nλKH6ι2)⌉·/summationdisplay\n(x,a,h′)2e2C2\n2H4ι\nmax/braceleftbigg/tildewide∆˚Qk⋆\nh′(x,a)\n2H,/tildewide∆min\n4H2/bracerightbigg\n=32e2C2\n2·λK\nC1·⌈log1+1/H(√\nλKH6ι2)⌉≤λK\n⌈log1+1/H(√\nλKH6ι2)⌉, (76)\nwhere(75) follows sincemax/braceleftbigg/tildewide∆˚Qk⋆\nh′(x,a)\n2H,/tildewide∆min\n4H2/bracerightbigg\n≤Hfor all (x,a,h′), and(76) follows since C1\nis chosen so that C1≥32e2C2\n2(see (22)). Therefore,|Yh|≤/summationtext⌈log1+1/H(√\nλKH6ι2)⌉\ni=1 |Yi\nh|≤λK.\n2. Otherwise, by the deﬁnition of /hatwide∆kin (13), we have/hatwide∆k⋆=/radicalig\nSAH8ι2\nλK=/hatwide∆K. Note that ( 74)\nstill holds, and so, using ( 73), we get that, for each i,\n|Yi\nh|≤1\nϕh+1(ωi−1)·/parenleftbigg\ne2SAH3+e2C2/radicalig\nH7SA|Yi\nh|ι/parenrightbigg\n≤1\nϕh+1(ωi−1)·2e2C2/radicalig\nH7SA|Yi\nh|ι (77)\n≤√\n8Hι\nC1·/hatwide∆k⋆·/radicalig\n⌈log1+1/H(√\nλKH7ι2)⌉·2e2C2/radicalig\nH7SA|Yi\nh|ι,\nwhich implies that\n/radicalig\n|Yi\nh|≤6e2C2\nC1·1/radicalig\n⌈log1+1/H(√\nλKH7ι2)⌉·√\nλK,\nand since C1is chosen so that C1≥6e2C2, we get that|Yh|≤λK, as desired.\nThus, in both cases, we obtain that |Zh|≤max{SAH3,λK}, completing the proof of the lemma.\n48\n\n7.2 Bounding the value functions Rk\nh,/tildewideQk\nh,/tildewideVk\nh\nIn this section we establish some basic bounds on the value fu nctionsRk\nh,/tildewideQk\nh,/tildewideVk\nhmaintained by\nQLearningPreds to reﬁne the predictions /tildewideQh. Many of the results are analogous to the bounds on\nQk\nh,Qk\nh,Vk\nh,Vk\nhproven in Section 6.1. However, since the updating procedures are distinct from\nthose used to update the upper and lower Q- andV-value functions (in particular, we do not use\nthe multi-step bootstrap of [ XMD21] to update/tildewideQk\nh,/tildewideVk\nh), we cannot derive the results in this section\ndirectly from those in Section 6.1.\nTheﬁrstresult, Lemma 7.3, isastraightforwardconsequenceoftheupdatesto Rk\nhinQLearningPreds .\nLemma 7.3. For any(x,a,h,k)∈S×A× [H]×[K], suppose the episodes in which (x,a)was pre-\nviously taken at step hare denoted k1,...,kn< k(in particular, ki=ki\nh(x,a)andn=Nk\nh(x,a)).\nThen the following identity holds:\n(Rk\nh−Q⋆\nh)(x,a) =α0\nn(H−Q⋆\nh(x,a))+n/summationdisplay\ni=1αi\nn·/parenleftig\n(/tildewideVki\nh+1−V⋆\nh+1)(xki\nh+1)+/parenleftig\n(ˆPki\nh−Ph)V⋆\nh+1/parenrightig\n(x,a)+bi/parenrightig\n.\nProof.Note that Rk\nh(x,a) is updated as follows:\nRk+1\nh(x,a) =/braceleftigg\n(1−αn)·Rk\nh(x,a)+αn·[rh(x,a)+/tildewideVk\nh+1(xk\nh+1)+bn] : (x,a) = (xk\nh,ak\nh),\nRk\nh(x,a) : else,\nwheren=Nk+1\nh(x,a) in the ﬁrstcase above. Iterating the above, we obtain that f or any (x,a,h,k),\nlettingn=Nk\nh(x,a),\nRk\nh(x,a) =α0\nn·H+n/summationdisplay\ni=1αi\nn·/parenleftig\nrh(x,a)+/tildewideVki\nh+1(xki\nh+1)+bi/parenrightig\n. (78)\nUsing the Bellman optimality equation Q⋆\nh(x,a) =rh(x,a) +PhV⋆\nh+1(x,a) together with the fact\nthat/summationtextn\ni=0αi\nn= 1 and the notation ( ˆPki\nhVh+1)(x,a) =Vh+1(xki\nh+1) for (x,a) = (xki\nh,aki\nh), we see\nthat, for n=Nk\nh(x,a),\nQ⋆\nh(x,a) =α0\nn·Q⋆\nh(x,a)+n/summationdisplay\ni=1αi\nn·/parenleftig\nrh(x,a)+(Ph−ˆPki\nh)V⋆\nh+1(x,a)+V⋆\nh+1(xki\nh+1)/parenrightig\n.(79)\nSubtracting ( 79) from (78) gives the desired result.\nThe following straightforward lemma, which generalizes it em2of Lemma 6.1, shows that any\napproximately optimal action aat any state ( x,h) either remains in Ak\nh(x) at each episode kor else\nthere is some other action in Ak\nh(x) with smaller sub-optimality than a.\nLemma 7.4. Under the eventEwc, for any ǫ >0and every (x,a,h)∈S×A× [H], if it holds that\n∆h(x,a)≤ǫ, then for each k∈[K], at least one of the following must hold true:\n•a∈Ak\nh(x); or\n•For some a⋆∈Ak\nh(x)(in particular, we may choose a⋆∈Ak\nh(x)maximizing Qk\nh(x,a⋆)),\n∆h(x,a⋆)≤V⋆\nh(x)−Qk\nh(x,a⋆)< ǫ.\n49\n\nProof.Ifa/\\⌉}atio\\slash∈Ak\nh(x), then it must be the case that for some k′≤k,Qk′\nh(x,a)< Vk′\nh(x); by Lemma\n6.3, we have Vk\nh(x)≥Vk′\nh(x), and so some action a⋆∈Ak\nh(x) must satisfy Q⋆\nh(x,a⋆)≥Qk\nh(x,a⋆) =\nVk\nh(x)>Qk′\nh(x,a)≥Q⋆\nh(x,a). Hence ∆ h(x,a⋆) =V⋆\nh(x)−Q⋆\nh(x,a⋆)≤V⋆\nh(x)−Qk\nh(x,a⋆)<\nV⋆\nh(x)−Q⋆\nh(x,a) = ∆h(x,a)≤ǫ.\nThe next lemma, Lemma 7.5, uses Lemmas 7.3and7.4above together with a martingale\nconcentration inequality to show bounds on /tildewideQk\nh,/tildewideVk\nhthat hold with high probability. We note\nthat an additional necessary ingredient is the assumption t hat the input predictions /tildewideQhare anǫ-\napproximate distillation of Q⋆\nh; this is used to show that for all k∈[K],/tildewideQk\nhis also an approximate\ndistillation with high probability (item 3), which in turn is used to show that /tildewideVk\nhis approximately\nlower bounded by V⋆\nh(item4).\nLemma 7.5. Setp= 1/(H2K). Suppose that /tildewideQis anǫ-approximate distillation on the optimal\nvalue function Q⋆\nh. Then, there is an event EpredwithPr[Epred]≥1−pso that the following hold\nunderEpred∩Ewc:\n1. Forn∈N, recall that βn= 2/summationtextn\ni=1αi\nnbi. Then for any (x,a,h,k)∈S×A× [H]×[K], it\nholds that, for n=Nk\nh(x,a),\n(Rk\nh−Q⋆\nh)(x,a)≤α0\nnH+n/summationdisplay\ni=1αi\nn·(/tildewideVki\nh(x,a)\nh+1−V⋆\nh+1)(xki\nh(x,a)\nh+1)+βn.\n2. For all (x,a,h,k)∈S×A× [H]×[K], it holds that Rk\nh(x,a)≥Q⋆\nh(x,a)−ǫ·(H+1−h).\n3. For all (x,h,k)∈S×[H]×[K], there is some ¯a∈Aso that∆h(x,¯a)≤ǫand/tildewideQk\nh(x,¯a)≥\nQ⋆\nh(x,¯a)−ǫ·(H+1−h). In particular, /tildewideQkis anǫ·(H+2−h)-approximate distillation on\nQ⋆.\n4. For all (x,h,k)∈S×[H]×[K], it holds that /tildewideVk\nh(x)≥V⋆\nh(x)−ǫ·(H+2−h).\n5. For any (h,k)∈S×A× [H]×[K], it holds that, for n=Nk\nh(xk\nh,ak\nh),\n(Rk\nh−Q⋆\nh)(xk\nh,ak\nh)≤α0\nnH+clip/bracketleftbigg\nβn|[∆h(xk\nh,ak\nh)−2ǫ·(H+1)]+\n2H/bracketrightbigg\n+/parenleftbigg\n1+1\nH/parenrightbigg\n·n/summationdisplay\ni=1αi\nn·(/tildewideVki\nh(xk\nh,ak\nh)\nh+1−V⋆\nh+1)(xki\nh(xk\nh,ak\nh)\nh+1).\nProof.Fix any ( x,a,h)∈S×A× [H]. Setk0= 0 and for i≥1,\nki:= min/parenleftig/braceleftig\nk∈[K] :k > ki−1and (xk\nh,ak\nh) = (x,a)/bracerightig\n∪{K+1}/parenrightig\n.\nLetHkdenote the σ-ﬁeld generated by all random variables up to and including e pisodek, step\nH; the random variable kiis a stopping time of the ﬁltration ( Hk)k≥0. LetFi,i≥0 be the\nﬁltrationgiven by Fi=Hki. Then/parenleftig/BD[ki≤K]·[(ˆPki\nh−Ph)V⋆\nh+1](x,a)/parenrightigK\ni=1isamartingalediﬀerence\n50\n\nsequence adapted to the ﬁltration Fi. By the Azuma-Hoeﬀding inequality and a union bound over\nallm∈[K], it holds that, for some constant C0>0, with probability at least 1 −p/(SAH),\n∀m∈[K] :/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay\ni=1αi\nm· /BD[ki≤K]·[(ˆPki\nh−Ph)V⋆\nh+1](x,a)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤C0H\n4/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay\ni=1(αim)2·ι≤C0\n2/radicalbigg\nH3ι\nm,\n(80)\nwhere the ﬁnal inequality follows from item 2of Lemma A.1. Taking a union bound over all\n(x,a,h)∈S×A× [H], we get that with probability 1 −p, for all ( x,a,h,k)∈S×A× [H]×[K],\n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglen/summationdisplay\ni=1αi\nn/bracketleftig\n(ˆPki\nh(x,a)\nh−Ph)V⋆\nh+1/bracketrightig\n(x,a)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤C0\n2/radicalbigg\nH3ι\nnwheren=Nk\nh(x,a). (81)\n(Here we have applied ( 80) withm=Nk\nh(x,a)≤K, and used the fact that /BD[ki\nh(x,a)≤K] = 1\nfori≤Nk\nh(x,a).) LetEpreddenote the probability 1 −pevent under which ( 81) holds. From ( 21)\nwe have that βn/2≥C0/radicalbig\nH3ι/n. Then item 1of the lemma follows from Lemma 7.3and (81).\nTo establish the remaining items of the lemma statement, we u se reverse induction on h. The\nbasecase h=H+1isimmediatesinceasamatterofconvention, allof Rk\nH+1,Q⋆\nH+1,/tildewideQk\nH+1/tildewideVk\nH+1,V⋆\nH+1\nare identically 0. Assuming that items 2,3, and4hold for step h+ 1, (81) and Lemma 7.3give\nthat, under the event Epred, for each ( x,a,h,k)∈S×A× [H]×[K], forn=/tildewideNk\nh(x,a),\n(Rk\nh−Q⋆\nh)(x,a)≥n/summationdisplay\ni=1αi\nn·/parenleftbigg\n(/tildewideV/tildewideki\nh(x,a)\nh+1−V⋆\nh+1)(x/tildewideki\nh(x,a)\nh+1)+/parenleftbigg\n(ˆP/tildewideki\nh(x,a)\nh−Ph)V⋆\nh+1/parenrightbigg\n(x,a)+bi/parenrightbigg\n≥−ǫ·(H+1−h)+βn/2−C0\n2/radicalbig\nH3ι/n\n≥−ǫ·(H+1−h),\nthus establishing item 2of the lemma at step h.\nTo establish item 3at steph, we use increasing induction on k. The base case k= 1 follows\nfrom the fact that, by assumption, /tildewideQ1is anǫ-approximate distillation on Q⋆. To establish the\ninductive step, we note that by construction, /tildewideQk\nh(x,a) = min{Rk\nh(x,a),/tildewideQk−1\nh(x,a),Qk\nh(x,a)}for all\n(x,a)∈S×A . By the inductive hypothesis (on k), for each x∈S, there is some ¯ a∈Aso that\n∆h(x,¯a)≤ǫand/tildewideQk−1\nh(x,¯a)≥Q⋆\nh(x,¯a)−ǫ·(H+1−h). Under the event Ewchave that Qk\nh(x,¯a)≥\nQ⋆\nh(x,¯a), and we have already established (item 2) thatRk\nh(x,¯a)≥Q⋆\nh(x,¯a)−ǫ·(H+1−h), which\nimplies that/tildewideQk\nh(x,¯a)≥Q⋆\nh(x,¯a)−ǫ·(H+1−h).\nFinally we establish item 4at steph. Again we use increasing induction on k, noting that the\nbase case k= 1 follows from the fact that, for all x∈S,/tildewideV1\nh(x) = max a∈A/tildewideQ1\nh(x,a)≥V⋆\nh(x)−ǫ,\nusingthat/tildewideQkis anǫ-approximate distillation on Q⋆. To establish theinductive step (i.e., at episode\nk, assuming that item 4holds at episode k−1 and step h), note that for any x∈S,\n/tildewideVk\nh(x) = max\na′∈Ak\nh(x)/braceleftig\nmax{/tildewideQk\nh(x,a′),Qk\nh(x,a′)}/bracerightig\n(82)\nMoreover, since /tildewideQk\nhis anǫ·(H+2−h)-approximate distillation on Q⋆\nh(item3at steph), for any\nx, there is some ¯ a∈Aso that ∆ h(x,¯a)+[Q⋆\nh(x,¯a)−/tildewideQk\nh(x,¯a)]+≤ǫ·(H+2−h). By Lemma 7.4\n51\n\nwith (x,a,h) = (x,¯a,h), since ∆ h(x,¯a)≤ǫ·(H+ 2−h), it holds that either ¯ a∈Ak\nh(x) or else\nthere is some a⋆∈Ak\nh(x) so that V⋆\nh(x)−Qk\nh(x,a⋆)< ǫ. If ¯a∈Ak\nh(x), then\nmax\na′∈Ak\nh(x)/braceleftig\nmax{/tildewideQk\nh(x,a′),Qk\nh(x,a′)}/bracerightig\n≥/tildewideQk\nh(x,¯a)≥V⋆\nh(x)−ǫ·(H+2−h).\nOtherwise, we have\nmax\na′∈Ak\nh(x)/braceleftig\nmax{/tildewideQk\nh(x,ak\nh),Qk\nh(x,ak\nh)}/bracerightig\n≥Qk\nh(x,a⋆)> V⋆\nh(x)−ǫ·(H+2−h).\nThus, by ( 82) and the inductive hypothesis (on k), it holds that /tildewideVk\nh(x)≥V⋆\nh(x)−ǫ·(H+2−h),\nas desired.\nNext we establish item 5of the lemma. By item 2of the lemma, under the event Ewc, we have,\nfor all (h,k)∈[H]×[K],\nRk\nh(xk\nh,ak\nh)≥Q⋆\nh(xk\nh,ak\nh)−ǫ·H≥Qk\nh(xk\nh,ak\nh)−ǫ·H.\nThen by ( 82) and the deﬁnition of ak\nh(using that τk\nh= 0), we have that\nRk\nh(xk\nh,ak\nh)≥max{Qk\nh(xk\nh,ak\nh),/tildewideQk\nh(xk\nh,ak\nh)}−ǫ·H\n= max\na′∈Ak\nh(xk\nh)/braceleftig\nmax{Qk\nh(xk\nh,a′),/tildewideQk\nh(xk\nh,a′)}/bracerightig\n−ǫ·H\n=/tildewideVk\nh(xk\nh)−ǫ·H.\nThus, by item 4of the lemma, we have that Rk\nh(xk\nh,ak\nh)≥V⋆\nh(xk\nh)−2ǫ·(H+1). Hence\nRk\nh(xk\nh,ak\nh)−Q⋆\nh(xk\nh,ak\nh)≥V⋆\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh)−2ǫ·(H+1)≥∆h(xk\nh,ak\nh)−2ǫ·(H+1).\nThe statement of item 5then follows from item 1and Lemma 6.9.\n7.3 Additional bounds on Q- andV-value functions\nIn this section we prove some additional bounds on Vk\nh,Vk\nh,∆Vk\nh,∆˘Vk\nh.\nRecall from Lemma 6.6that for any ( x,h,k,a) for which x/\\⌉}atio\\slash∈Gk\nh, we have ∆ ˘Vk\nh(x)≤∆Vk\nh(x).\nThenext lemma (in particular, ( 84)) shows that a reverse inequality holds up to a factor of 1+1 /H,\nif there is a non-optimal action in Ak\nh(x). This fact formalizes the intuition that the purpose of\ndeﬁning the clipped value functions ∆ ˘Vk\nh,∆˘Qk\nhis to avoid paying for the case when only optimal\nactions remain in Ak\nh(x) (in which case one should not suﬀer any regret no matter which action is\ntaken at x).\nLemma 7.6. For any (x,h,k)∈S×[H]×[K], if there is a non-optimal action in Ak\nh(x), then,\nunder the eventEwc, it holds that\n∆˘Vk\nh(x)≥∆min,h(x)\n4(83)\nVk\nh(x)−Vk\nh(x)≤∆Vk\nh(x)≤/parenleftbigg\n1+1\nH/parenrightbigg\n·∆˘Vk\nh(x). (84)\n52\n\nProof.Fixx,h,kand seta⋆= argmaxa′∈Ak\nh(x){Qk\nh(x,a′)−Qk\nh(x,a′}. Then by Deﬁnition 5.1, we\nhave that ∆ ˘Vk\nh(x) = min/braceleftig\n∆˘Vk−1\nh(x),∆˘Qk\nh(x,a⋆)/bracerightig\n. If ∆˘Vk\nh(x) = ∆˘Vk−1\nh(x), then we may replace\nkwithk−1, noting the existence of a non-optimal action in Ak\nh(x) implies the existence of a non-\noptimal action in Ak−1\nh(x) (continuing this process may eventually lead to the case k= 0, for which\n(83) and (84) hold by the deﬁnition ∆ ˘V0\nh(x) =H). So we may assume that ∆ ˘Vk\nh(x) = ∆˘Qk\nh(x,a′).\nLeta′denote some sub-optimal action in Ak\nh(x). Then under the event Ewc, we must have that\nx/\\⌉}atio\\slash∈Gk\nh, meaning that\nQk\nh(x,a′)−Qk\nh(x,a′)≤Qk\nh(x,a′)−Qk\nh(x,a′) (Since a′maximizes the conﬁdence interval)\n≤∆Qk\nh(x,a′) (By Lemma 6.4)\n≤∆˘Qk\nh(x,a′)+∆min\n4. (By Lemma 6.5)\nMoreover, as in the proof of Lemma 6.14, we have, by Lemmas 6.4and6.5,\nVk\nh(x)−Vk\nh(x)≤∆Vk\nh(x)≤∆˘Vk\nh(x)+∆min\n4H= ∆˘Qk\nh(x,a′)+∆min\n4H. (85)\nCombining the above displays, we obtain that under the event Ewc,\n∆min,h(x)≤∆h(x,a′)\n≤(Vk\nh(x)−Vk\nh(x))+(Qk\nh(x,a′)−Qk\nh(x,a′))\n≤2·∆˘Qk\nh(x,a′)+∆min\n2,\nwhich implies that ∆ ˘Vk\nh(x) = ∆˘Qk\nh(x,a′)≥∆min,h(x)\n4. This veriﬁes ( 83). To verify ( 84), we use ( 85)\nto get\nVk\nh(x)−Vk\nh(x)≤∆Vk\nh(x)≤∆˘Vk\nh(x)+∆min\n4H≤∆˘Vk\nh(x)+∆min,h(x)\n4H≤/parenleftbigg\n1+1\nH/parenrightbigg\n·∆˘Vk\nh(x),\nwhere the ﬁnal inequality follows from ( 83).\nThe following simple lemma shows that /tildewideVk\nhis bounded above by Vk\nh, which is an immediate\nconsequence of the deﬁnition of /tildewideVk\nhinQLearningPreds .\nLemma 7.7. For allx,a,h,k∈S×A× [H]×[K], it holds that /tildewideVk\nh(x)≤Vk\nh(x).\nProof.The deﬁnition of /tildewideQk\nhat step2(d)iiensures that for all x,a,j,k, we have that /tildewideQk\nh(x,a)≤\nQk\nh(x,a). The conclusion of the lemma follows from the fact that Vk\nh(x) = maxa′∈Ak\nh(x){Qk\nh(x,a′)}\nand/tildewideVk\nh(x)≤maxa′∈Ak\nh(x){/tildewideQk\nh(x,a′)}.\n7.4 Regret bounds for approximate distillation\nThe below lemma, the main result of this section, shows that i n the case that the provided pre-\ndictions/tildewideQare anǫ-approximate distillation of the true value function Q⋆, then we may bound the\nregret of QLearningPreds by a quantity that in general will be smaller than generic wor st-case\nregret bounds. In particular, the set of states and actions S×A× [H] is replaced with the fooling\nsetF(ǫ′/2,ǫ′), which will be signiﬁcantly smaller if the predictions /tildewideQhare very accurate.\n53\n\nLemma 7.8. Suppose the event Ewc∩Epredholds, and set ǫ′:= 4ǫ·(H+1). If/tildewideQis anǫ-approximate\ndistillation on Q⋆and either/tildewideQlacksǫ′-fooling optimal actions (Deﬁnition 3.3). Then the following\nregret bounds hold:\nK/summationdisplay\nk=1(V⋆\n1−Vπk\n1)(xk\n1)\n≤O((ǫH+ǫ′)·TH)+O/parenleftigg\nH·H/summationdisplay\nh=1K/summationdisplay\nk=1σk\nh˘δk\nh/parenrightigg\n+O\nmin\n\n/radicalbig\nH6Kι·|F(ǫ(H+1),ǫ′)|,/summationdisplay\n(x,a,h)∈F(ǫ(H+1),ǫ′)H4ι\n[∆h(x,a)−2ǫ·(H+1)]+\n\n\n.(86)\nProof.Recall from QLearningPreds that the values τk\nh∈{0,1}are deﬁned as follows: τk\nh= 0 if\nxk\nh∈Gk\nhor ∆Vk\nh(xk\nh)≤ϕh(/hatwide∆k), andτk\nh= 1otherwise. Also recall that we deﬁnedvalues σk\nh∈{0,1}\nfor all (h,k)∈[H]×[K] as follows: σk\nh= 0 ifxk\nh∈Gk\nhor ∆˘Vk\nh(xk\nh)≤1\n1+1\nH·ϕh(/hatwide∆k), andσk\nh= 1\notherwise.\nFor any h∈[H], letWσ\nh⊂[K] denote the set of episodes kfor which σk\nh= 1. Similarly, let\nWτ\nh⊂[K] denote the set of episodes kfor which τk\nh= 1.\nSetǫ′= 4ǫ·(H+1), and for each ( x,h)∈S×[H], letAopt\nh,ǫ′(x) denote the set of actions a′∈A\nso thatV⋆\nh(x)−Q⋆\nh(x,a′)≤ǫ′. Next, for any k∈[K], we have that, under the event Ewc,\nH/summationdisplay\nh=1V⋆\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh)\n≤ǫ′H+H/summationdisplay\nh=1\n/BD[ak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh)]·(V⋆\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh))\n≤(ǫ·(H+1)+ǫ′)H+H/summationdisplay\nh=1(1−σk\nh)· /BD[ak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh)]·(/tildewideVk\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh))+4σk\nh·˘δk\nh(87)\n≤(2ǫ·(H+1)+2ǫ′)H+H/summationdisplay\nh=1(1−τk\nh)· /BD[ak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh)]·(Rk\nh(xk\nh,ak\nh)−Q⋆\nh(xk\nh,ak\nh))+4σk\nh·˘δk\nh,\n(88)\nwhere (87) follows from item 4of Lemma 7.5and Lemma 6.14, and (88) follows from Claims 7.9\nand7.10below.\nClaim 7.9. For any (h,k)∈[H]×[K]so thatτk\nh= 0, it holds that /tildewideVk\nh(xk\nh)≤Rk\nh(xk\nh,ak\nh)+ǫ·H.\nProof.Sinceτk\nh= 0, we have that ak\nh= argmaxa′∈Ak\nh(xk\nh){max{/tildewideQk\nh(xk\nh,a′),Qk\nh(xk\nh,a′)}}. Then by\ndeﬁnition of/tildewideVk\nh,\n/tildewideVk\nh(xk\nh) = max{/tildewideQk\nh(xk\nh,ak\nh),Qk\nh(xk\nh,ak\nh}≤Rk\nh(xk\nh,ak\nh)+ǫ·H (89)\n54\n\nwhere the inequality above uses the fact that Qk\nh(xk\nh,ak\nh)≤Q⋆\nh(xk\nh,ak\nh)≤Rk\nh(xk\nh,ak\nh)+ǫ·(H+1−h)\n(item2of Lemma 7.5) and that/tildewideQk\nh(xk\nh,ak\nh)≤Rk\nh(xk\nh,ak\nh) by the deﬁnition of /tildewideQk\nh(step2(d)iiof the\nalgorithm).\nClaim 7.10. For any (h,k)∈[H]×[K]so thatτk\nh= 1, at least one of the following statements\nholds true under the event Ewc:\n•/tildewideVk\nh(xk\nh)≤Q⋆\nh(xk\nh,ak\nh)+ǫ′.\n•∆Vk\nh(xk\nh)≤/parenleftbig\n1+1\nH/parenrightbig\n·∆˘Vk\nh(xk\nh)andσk\nh= 1.\nProof.Suppose that the second statement does not hold true. Then ei ther ∆Vk\nh(xk\nh)>(1+1/H)·\n∆˘Vk\nh(xk\nh) orσk\nh= 0. First suppose that σk\nh= 0. Since τk\nh= 1, we have xk\nh/\\⌉}atio\\slash∈Gk\nh, meaning that\n∆˘Vk\nh(xk\nh)≤1\n1+1/H·ϕh(/hatwide∆k). Butτk\nh= 1 also implies that ∆ Vk\nh(xk\nh)> ϕh(/hatwide∆k), which implies that\n∆Vk\nh(xk\nh)>(1+1/H)·∆˘Vk\nh(xk\nh).\nThus we may assume from here on that ∆ Vk\nh(xk\nh)>(1+1/H)·∆˘Vk\nh(xk\nh). By Lemma 7.6, under\nthe eventEwc,Ak\nh(xk\nh) must consist of only optimal actions. By deﬁnition of /tildewideVk\nh, there is some\na∈Ak\nh(xk\nh) so that/tildewideVk\nh(xk\nh) = max{/tildewideQk\nh(xk\nh,a),Qk\nh(xk\nh,a)}. We know that amust be an optimal\naction, i.e., ∆ h(xk\nh,a) = 0. Since the input predictions /tildewideQlackǫ′-fooling optimal actions (Deﬁnition\n3.3),13it holds that /tildewideQh(xk\nh,a)≤V⋆\nh(xk\nh)+ǫ′. Therefore,\n/tildewideVk\nh(xk\nh) =/tildewideQk\nh(xk\nh,a)≤/tildewideQh(xk\nh,a)≤V⋆\nh(xk\nh)+ǫ′.\nMoreover, since ak\nh∈Ak\nh(xk\nh)(andthereforeisanoptimalaction), wehavethat V⋆\nh(xk\nh) =Q⋆\nh(xk\nh,ak\nh),\nmeaning that /tildewideVk\nh(xk\nh)−Q⋆\nh(xk\nh,ak\nh)≤ǫ′, as desired.\nWe next need the following claim:\nClaim 7.11. For any (k,h)satisfying τk\nh= 0, if either\n1.ak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh); or\n2.(/tildewideVk\nh−V⋆\nh)(xk\nh)> ǫ′,\nthen we have that, under the event Ewc,(xk\nh,ak\nh,h)∈F(ǫ·(H+1),ǫ′).\nProof of Claim 7.11.For the entirety of the proof of the claim we assume that Ewcholds. We ﬁrst\nsuppose that ak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh). Notice that ∆ h(xk\nh,ak\nh)> ǫ′sinceak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh). By item 4of Lemma\n7.5and the choice of ak\nhwhenτk\nh= 0,\nmax{/tildewideQk\nh(xk\nh,ak\nh),Qk\nh(xk\nh,ak\nh)}= max\na′∈Ak\nh(xk\nh)/braceleftig\nmax{/tildewideQk\nh(xk\nh,a′),Qk\nh(xk\nh,a′)}/bracerightig\n=/tildewideVk\nh(xk\nh)≥V⋆\nh(xk\nh)−ǫ·(H+1)\n(90)\nIfQk\nh(xk\nh,ak\nh)≥V⋆\nh(xk\nh)−ǫ·(H+1), then it holds that Q⋆\nh(xk\nh,ak\nh)≥V⋆\nh(xk\nh)−ǫ·(H+1)> V⋆\nh(xk\nh)−ǫ′\n(sinceǫ′> ǫ·(H+1)), which contradicts ak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh). Hence/tildewideQk\nh(xk\nh,ak\nh)≥V⋆\nh(xk\nh)−ǫ·(H+1),\n13We remark that this is the only place in the proof where we use t hat the predictions /tildewideQlackǫ′-fooling optimal\nactions.\n55\n\nmeaning that /tildewideQk\nh(xk\nh,ak\nh)−Q⋆\nh(xk\nh,ak\nh)≥∆h(xk\nh,ak\nh)−ǫ·(H+1). Since/tildewideQh(xk\nh,ak\nh) =/tildewideQ1\nh(xk\nh,ak\nh)≥\n/tildewideQk\nh(xk\nh,ak\nh), we get that ( xk\nh,ak\nh,h)∈F(ǫ·(H+1),ǫ′).\nNext suppose that ( /tildewideVk\nh−V⋆\nh)(xk\nh)> ǫ′. Then, again using the choice of ak\nhwhenτk\nh= 0,\nmax{/tildewideQk\nh(xk\nh,ak\nh),Qk\nh(xk\nh,ak\nh)}≥/tildewideVk\nh(xk\nh)> V⋆\nh(xk\nh)+ǫ′.\nSinceQk\nh(xk\nh,ak\nh)≤Q⋆\nh(xk\nh,ak\nh)≤V⋆\nh(xk\nh) under the event Ewc, we must have /tildewideQk\nh(xk\nh,ak\nh)> V⋆\nh(xk\nh)+\nǫ′, which implies that /tildewideQ1\nh(xk\nh,ak\nh)> V⋆\nh(xk\nh)+ǫ′, meaning that ( xk\nh,ak\nh,h)∈F(ǫ·(H+1),ǫ′).\nNext for any h∈[H], we compute\n/summationdisplay\nk/\\⌉gatio\\slash∈Wτ\nh\n/BD[ak\nh/\\⌉}atio\\slash∈Aopt\nh,ǫ′(xk\nh)]·(Rk\nh(xk\nh,ak\nh)−Q⋆\nh(xk\nh,ak\nh))\n≤/summationdisplay\n(x,a,h)∈Fh(ǫ·(H+1),ǫ′)NK+1\nh(x,a)/summationdisplay\ni=1(Rki\nh(x,a)\nh(x,a)−Q⋆\nh(x,a)) (91)\n≤H·|Fh(ǫ(H+1),ǫ′)|+/summationdisplay\n(x,a,h)∈Fh(ǫ(H+1),ǫ)\nNK+1\nh(x,a)/summationdisplay\ni=1/parenleftbigg\n1+1\nH/parenrightbiggi/summationdisplay\nt=1αt\ni·(/tildewideVkt\nh(x,a)\nh+1−V⋆\nh+1)(xkt\nh(x,a)\nh+1)\n+clip/bracketleftbigg\nβi|[∆h(x,a)−2ǫ·(H+1)]+\n4H/bracketrightbigg/parenrightbigg\n(92)\n≤H·|Fh(ǫ(H+1),ǫ′)|+/summationdisplay\n(x,a,h)∈Fh(ǫ(H+1),ǫ′)\n/parenleftbigg\n1+1\nH/parenrightbiggNK+1\nh(x,a)/summationdisplay\nt=1(/tildewideVkt\nh(x,a)\nh+1−V⋆\nh+1)(xkt\nh(x,a)\nh+1)·∞/summationdisplay\ni=tαt\ni\n+NK+1\nh(x,a)/summationdisplay\ni=1clip/bracketleftbigg\nβi|[∆h(x,a)−2ǫ·(H+1)]+\n4H/bracketrightbigg\n\n≤H·|Fh(ǫ(H+1),ǫ′)|+/summationdisplay\n(x,a,h)∈Fh(ǫ(H+1),ǫ′)min/braceleftbigg\n8C0/radicalig\nH3ι·NK+1\nh(x,a),64C2\n0H4ι\n[∆h(x,a)−2ǫ·(H+1)]+/bracerightbigg\n(1+1/H)2·/summationdisplay\nk∈[K](/tildewideVk\nh+1−V⋆\nh+1)(xk\nh+1), (93)\nwhere (91) uses Claim 7.11, (92) uses item 5of Lemma 7.5and the fact that ( xki\nh(x,a)\nh,aki\nh(x,a)\nh) =\n(x,a), and the ﬁnal inequality ( 93) uses item 3of Lemma A.1and Lemma 6.10. Moreover, we have\n56\n\nthat\n/summationdisplay\nk∈[K](/tildewideVk\nh+1−V⋆\nh+1)(xk\nh+1)\n≤K·ǫ′+/summationdisplay\nk∈[K]\n/BD[σk\nh+1= 1 and ∆ Vk\nh+1(xk\nh+1)≤(1+1/H)˘δk\nh+1]·(Vk\nh+1−Vk\nh+1)(xk\nh+1)\n+/summationdisplay\nk/\\⌉gatio\\slash∈Wτ\nh+1(/tildewideVk\nh+1−V⋆\nh+1)(xk\nh+1) (Using Claim 7.10and Lemma 7.7)\n≤(2ǫ′+ǫ·H)·K+K/summationdisplay\nk=12σk\nh+1·˘δk\nh+1+/summationdisplay\nk/\\⌉gatio\\slash∈Wτ\nh+1\n/BD[(/tildewideVk\nh+1−V⋆\nh+1)(xk\nh+1)> ǫ′]·/parenleftig\n(Rk\nh+1−Q⋆\nh+1)(xk\nh+1,ak\nh+1)/parenrightig\n(Using Claim 7.9and Lemma 6.4)\n≤(2ǫ′+ǫH)·K+2K/summationdisplay\nk=1σk\nh+1˘δk\nh+1+/summationdisplay\n(x,a,h+1)∈Fh+1(ǫ(H+1),ǫ′)NK+1\nh+1(x,a)/summationdisplay\ni=1(Rki\nh+1(x,a)\nh+1(x,a)−Q⋆\nh+1(x,a)),\n(94)\nwhere (94) follows from Claim 7.11(in particular, if τk\nh+1= 0 and (/tildewideVk\nh+1−V⋆\nh+1)(xk\nh+1)> ǫ′, then\n(xk\nh+1,ak\nh+1,h+1)∈Fh+1(ǫ(H+1),ǫ′)).\nCombining ( 93) and (94), and iterating for h,h+1,...,H, we see that\n/summationdisplay\n(x,a,h)∈Fh(ǫ(H+1),ǫ′)NK+1\nh(x,a)/summationdisplay\ni=1(Rki\nh(x,a)\nh−Q⋆\nh(xk\nh,ak\nh))\n≤e2KH·(2ǫ′+ǫH)+e2H·|F(ǫ(H+1),ǫ′)|+2e2H/summationdisplay\nh′=h+1K/summationdisplay\nk=1σk\nh′˘δk\nh′\n+e2H/summationdisplay\nh′=h/summationdisplay\n(x,a,h′)∈Fh′(ǫ(H+1),ǫ′)min/braceleftbigg\n8C0/radicalig\nH3ι·NK+1\nh′(x,a),64C2\n0H4ι\n[∆h′(x,a)−2ǫ·(H+1)]+/bracerightbigg\n≤e2KH·(2ǫ′+ǫH)+e2H·|F(ǫ(H+1),ǫ′)|+2e2H/summationdisplay\nh′=h+1K/summationdisplay\nk=1σk\nh′˘δk\nh′\n+e2H/summationdisplay\nh′=hmin\n\n8C0/radicalbig\nH3Kι·|Fh′(ǫ(H+1),ǫ′)|,/summationdisplay\n(x,a,h′)∈Fh′(ǫ(H+1),ǫ′)64C2\n0H4ι\n[∆h′(x,a)−2ǫ·(H+1)]+\n\n\n≤e2KH·(2ǫ′+ǫH)+e2H·|F(ǫ(H+1),ǫ′)|+2e2H/summationdisplay\nh′=h+1K/summationdisplay\nk=1σk\nh′˘δk\nh′\n+min\n\n8C0e2/radicalbig\nH4Kι·|F(ǫ(H+1),ǫ′)|,/summationdisplay\n(x,a,h)∈F(ǫ(H+1),ǫ′)64e2C2\n0H4ι\n[∆h(x,a)−2ǫ·(H+1)]+\n\n.\n57\n\nCombining this with ( 88) and (91) gives that\n/summationdisplay\nk∈[K](V⋆\n1−Vπk\n1)(x1)\n≤HK(2ǫ(H+1)+2ǫ′)+HK·Pr[Ewc∩Epred]+e2KH2·(2ǫ′+ǫH)+e2H2·|F(ǫ(H+1),ǫ′)|\n+min\n\n8C0e2/radicalbig\nH6Kι·|F(ǫ(H+1),ǫ′)|,/summationdisplay\n(x,a,h)∈F(ǫ(H+1),ǫ′)64e2C2\n0H5ι\n[∆h(x,a)−2ǫ·(H+1)]+\n\n\n+2e2HH/summationdisplay\nh=1K/summationdisplay\nk=1σk\nh˘δk\nh\n≤O((ǫH+ǫ′)·TH)+O\nmin\n\n/radicalbig\nH6Kι·|F(ǫ(H+1),ǫ′)|,/summationdisplay\n(x,a,h)∈F(ǫ(H+1),ǫ′)H4ι\n[∆h(x,a)−2ǫ·(H+1)]+\n\n\n\n+2e2HH/summationdisplay\nh=1K/summationdisplay\nk=1σk\nh˘δk\nh,\nwhere in the ﬁnal inequality we use that H2·|F(ǫ(H+1),ǫ′)|≤/radicalbig\nH6Kι·|F(ǫ(H+1),ǫ′)|as\nlong asK≥ |F(ǫ(H+1),ǫ′)|. (ForK <|F(ǫ(H+1),ǫ′)|the trivial regret bound of KHis\nbounded above by/radicalbig\nH6Kι·|F(ǫ(H+1),ǫ′)|.) Moreover, we also use that H2·|F(ǫ(H+1),ǫ′)|≤/summationtext\n(x,a,h)∈F(ǫ(H+1),ǫ′)H4ι\n[∆h(x,a)−2ǫ·(H+1)]+in the ﬁnal line. This veriﬁes the statement ( 86) of the\ntheorem.\nNote that Lemma 7.8does not quite establish the guarantee of the improved regre t bounds of\nTheorems 3.1or3.2when the predictions /tildewideQare an approximate distillation of Q⋆. In particular,\nwe have not yet shown how to bound the term/summationtextH\nh=1/summationtextK\nk=1σk\nh˘δk\nh. We do so in Lemmas 7.12and\n7.13below; the ﬁrst treats the case where QLearningPreds usesDeltaConst , and the second treats\nthe case where QLearningPreds usesDeltaIncr .\nLemma 7.12. For any prediction function /tildewideQ, the algorithm QLearningPreds (withDeltaConst )\nhas the following guarantee under the event Ewc:\nH/summationdisplay\nh=1K/summationdisplay\nk=1σk\nh·˘δk\nh≤O\nmin\n\n/radicalbig\n/hatwideλ·H8SATι,H7ι·\n/summationdisplay\n(x,a,h)∈S×A× [H]:a/\\⌉gatio\\slash∈Aopt\nh,0(x)1\n∆h(x,a)+|Amul|\n∆min\n\n\n\n.\nProof.Suppose thatEwcholds. Recall that ˘δk\nhis only deﬁned for h,kso thatxk\nh/\\⌉}atio\\slash∈Gk\nh; but if\nxk\nh∈Gk\nh, thenσk\nh= 0, meaning that the sum/summationtextH\nh=1/summationtextK\nk=1σk\nh·˘δk\nhis well-deﬁned.\nFurther, recall that /hatwideλis chosen so that1\n/hatwideλ·CM,T,/hatwideλ=R. We claim that /hatwideλ≥SAH3/K; to see this,\nnotethat1\nλ·CM,T,λisa decreasingfunctionof λ, andthat forthechoice λ0=SAH3/K=SAH4/T,\n1\nλ0CM,T,λ 0≥min/braceleftbigg\nTH2,H7T\nSAH4·SAH\n2H/bracerightbigg\n≥T/2≥R.\n58\n\nFinally note that /hatwide∆K=R\nKH=CM,T,/hatwideλ\n/hatwideλ·KH.\nFor each h∈H, setYh:={k:σk\nh= 1}. Lemma 7.1gives that|Yh|≤max{SAH3,/hatwideλ·K}≤/hatwideλ·K,\nwhere we use that /hatwideλis chosen so that /hatwideλK≥SAH3. By item 2of Lemma 6.13,\nH/summationdisplay\nh=1K/summationdisplay\nk=1σk\nh˘δk\nh\n=H/summationdisplay\nh=1/summationdisplay\nk∈Yh˘δk\nh\n≤H/summationdisplay\nh=1\n|Yh|·ϕh(/hatwide∆)+e2SAH2+min\n\ne2C2/radicalbig\nH5SA|Yh|ι,/summationdisplay\n(x,a,h′)∈S×A× [H]e2C2\n2H3ι\nmax/braceleftbigg\n∆˚QK\nh′(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n\n\n≤e2SAH3+min\n\ne2C2/radicalbig\nH7SA/hatwideλKι,/summationdisplay\n(x,a,h)∈S×A× [H]e2C2\n2H4ι\nmax/braceleftbigg\n∆˚QK\nh(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n\n+O(/hatwideλ·K)·min\n\n/radicaligg\nH7SAι\n/hatwideλ·K,1\n/hatwideλ·K·H6ι·\n/summationdisplay\n(x,a,h′)∈S×A× [H]:a/\\⌉gatio\\slash∈Aopt\nh′,0(x)1\n∆h(x,a)+|Amul|\n∆min\n\n\n.\nBy Lemma 6.16(which we can use since Ewcholds) and Lemma A.2(together with /hatwideλ≥\nSAH3/K), it therefore follows that\nH/summationdisplay\nh=1K/summationdisplay\nk=1σk\nh˘δk\nh\n≤O\nmin\n\n/radicalbig\n/hatwideλ·H8SATι,H7ι·\n/summationdisplay\n(x,a,h)∈S×A× [H]:a/\\⌉gatio\\slash∈Aopt\nh,0(x)1\n∆h(x,a)+|Amul|\n∆min\n\n\n\n.\nLemma 7.13. For any prediction function /tildewideQ, the algorithm QLearningPreds given some param-\neterλ≥SAH3/K(withDeltaIncr and some input parameter /tildewide∆min≤∆min) has the following\nguarantee under the event Ewc:\nH/summationdisplay\nh=1K/summationdisplay\nk=1σk\nh˘δk\nh≤O\nmin\n\n√\nλ·SAH9Tι2,H8ι2·\n/summationdisplay\n(x,a,h):a/\\⌉gatio\\slash∈Aopt\nh,0(x)1\n∆h(x,a)+|Amul|\n/tildewide∆min\n\n\n\n.\nProof.For each h∈H, setYh:={k:σk\nh= 1}. Lemma 7.2gives that|Yh|≤max{SAH3,λ·K}.\nRecall that the input parameter λwas assumed to satisfy λ≥SAH3/K, meaning that|Yh|≤λK.\n59\n\nBy item 2of Lemma 6.13,\nH/summationdisplay\nh=1K/summationdisplay\nk=1σk\nh˘δk\nh\n=H/summationdisplay\nh=1/summationdisplay\nk∈Yh˘δk\nh\n≤H/summationdisplay\nh=1\n|Yh|·ϕh(/hatwide∆K)+e2SAH2+min\n\ne2C2/radicalbig\nH5SA|Yh|ι,/summationdisplay\n(x,a,h′)∈S×A× [H]e2C2\n2H3ι\nmax/braceleftbigg\n∆˚QK\nh′(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n\n\n≤e2SAH3+min\n\ne2C2√\nH7SAλKι,/summationdisplay\n(x,a,h)∈S×A× [H]e2C2\n2H4ι\nmax/braceleftbigg\n∆˚QK\nh(x,a)\n2H,∆min\n4H2/bracerightbigg\n\n\n+O(1)·H·λK·min\n\nH5ι2\nλ·K·/summationdisplay\n(x,a,h)1\nmax/braceleftbigg\n/tildewide∆˚QK\nh(x,a)\n2H,/tildewide∆min\n4H2/bracerightbigg,/radicalbigg\nSAH8ι2\nλ·K\n\n.\nBy Lemma 6.16(which we can apply since Ewcholds) and Lemma A.2(using that λ≥SAH3/K),\nit follows that\nH/summationdisplay\nh=1K/summationdisplay\nk=1σk\nh˘δk\nh≤O\nmin\n\n√\nλ·SAH9Tι2,H8ι2·\n/summationdisplay\n(x,a,h):a/\\⌉gatio\\slash∈Aopt\nh,0(x)1\n∆h(x,a)+|Amul|\n/tildewide∆min\n\n\n\n.\n8 Proofs of main theorems\nWe begin by proving Theorem 3.2, restated below for convenience.\nTheorem 3.2(Restated) .The algorithm QLearningPreds with theDeltaConst subroutine satis-\nﬁes the following two guarantees, when given as input a param eterR∈[SAH3,T\nSA]and predictions\n/tildewideQ:\n1. IfR≥CM,T,1, then for an arbitrary choice of input predictions /tildewideQ, the regret of QLearningPreds\nisO(R).\n2. Fix any ǫ >0, and set ǫ′= 4ǫ·(H+1). When the input predictions /tildewideQare anǫ-approximate\ndistillation of Q⋆(Deﬁnition 3.1) and lack ǫ′-fooling optimal actions (Deﬁnition 3.3), the\nregret of QLearningPreds is\nO\nH·CM,T,/hatwideλ+ǫ′TH+min\n\n/radicalbig\nH5Tι·|F(ǫ′/2,ǫ′)|,/summationdisplay\n(x,a,h)∈F(ǫ′/2,ǫ′)H4ι\n[∆h(x,a)−ǫ′/2]+\n\n\n,\n(95)\n60\n\nwhere/hatwideλ∈(0,1)is chosen so that1\n/hatwideλ·CM,T,/hatwideλ=R.\nProof.We begin with the proof of item 1. It is without loss to assume T≥SAH3; otherwise, by\nsimilar reasoning to that in Lemma A.2, the trivial regret bound of Tsuﬃces. Now, item 1is an\nimmediate consequence of Lemma 6.17.\nWe next prove item 2. The eventEwc∩Epreddoes not hold with probability at most 2 p=\n2/(H2K), which adds at most T·2p=O(1) to the regret bound. Thus it suﬃces to bound the\nregret conditioned on Ewc∩Epred. Then item 2is an immediate consequence of Lemmas 7.8and\n7.12.\nWe next prove Theorem 3.1; below we present the version of the theorem which does not re quire\nthat each state has a uniqueoptimal action. In this more gene ral setting, the subroutine DeltaIncr\nofQLearningPreds requires as input a parameter /tildewide∆minwhich is guaranteed to be a lower bound\non ∆min. The resulting regret bounds will depend on a modiﬁed versio n of theλ-complexity CM,T,λ\n(see (6)) with the parameter /tildewide∆minreplacing ∆ min; more precisely, we deﬁne\nCM,T,λ,/tildewide∆min:= min\n\n√\nλ·TSAH8ι, H7ι·\n/summationdisplay\n(x,a,h)∈S×A× [H]:a/\\⌉gatio\\slash∈Aopt\nh,0(x)1\n∆h(x,a)+|Amul|\n/tildewide∆min\n\n\n(96)\nIn the special case that |Amul|= 0 (i.e., each state has a unique optimal action) and /tildewide∆min= 0, the\nquantity0\n0in (96) is to be interpreted as 0.\nTheorem 3.1(Full version) .Suppose we run algorithm QLearningPreds (Algorithm 1) with input\nparameter λ∈[0,1], together with the DeltaIncr subroutine (Algorithm 4) with parameter /tildewide∆min\nwhich is guaranteed to satisfy /tildewide∆min≤∆min. Then, when given predictions /tildewideQ, the algorithms satisfy\nthe following guarantees:\n1. Suppose λ≥SAH4\nT. Then for an arbitrary choice of input predictions /tildewideQ, the regret of\nQLearningPreds is\nO/parenleftbiggHι\nλ·CM,T,λ,/tildewide∆min/parenrightbigg\n.\n2. Fix any ǫ >0, and set ǫ′= 4ǫ·(H+1). When the input predictions /tildewideQare anǫ-approximate\ndistillation of Q⋆(Deﬁnition 3.1) and lack ǫ′-fooling actions (Deﬁnition 3.3), the regret of\nQLearningPreds is\nO\nH2ι·CM,T,λ,/tildewide∆min+ǫ′TH+min\n\n/radicalbig\nH5Tι·|F(ǫ′/2,ǫ′)|,/summationdisplay\n(x,a,h)∈F(ǫ′/2,ǫ′)H4ι\n[∆h(x,a)−ǫ′/2]+\n\n\n.\n(97)\nProof.We begin with the proof of item 1. As in the proof of Theorem 3.2, it is without loss to\nassume that T≥SAH3, as otherwise the trivial regret bound of Tsuﬃces. Now, item 1is an\nimmediate consequence of Lemma 6.18.\nWe next prove item 2. The eventEwc∩Epreddoes not hold with probability at most 2 p=\n2/(H2K), which adds at most T·2p=O(1) to the regret bound. Thus it suﬃces to bound the\nregret conditioned on Ewc∩Epred. Then item 2is an immediate consequence of Lemmas 7.8and\n7.13.\n61\n\nA Miscellaneous lemmas\nThe following simple lemma establishes some properties of t he parameters αi\nn(deﬁned in ( 9)).\nLemma A.1 (Lemma 4.1, [ JAZBJ18 ]).The real numbers αi\nnsatisfy the following properties:\n1. For every n≥1,1√n≤/summationtextn\ni=1αi\nn√\ni≤2√n.\n2. For every n≥1,maxi∈[n]αi\nn≤2H\nnand/summationtextn\ni=1(αi\nn)2≤2H\nn.\n3. For every i≥1,/summationtext∞\nn=iαi\nn= 1+1\nH.\n4. For every n≥1, it holds that/summationtextn\ni=1αi\nn= 1.\nRecall the deﬁnition of CM,T,λin (6)\nLemma A.2. For any λ≥SAH3\nK, it holds that\nCM,T,λ≥SAH6/2.\nProof.Sinceλ≥SAH3/K=SAH4/T, it holds that√\nλ·TSAH8ι≥SAH6.\nNext, it is evident that\n/summationdisplay\n(x,a,h):a/\\⌉gatio\\slash∈Aopt\nh,0(x)1\n∆h(x,a)+|Amul|\n∆min≥SAH\n2H≥SA/2,\nsincefor each ( x,h), all butoneof theactions ainAareeither counted intheformof1\n∆h(x,a)≥1/H\nor 1/∆min≥1/H.\nPutting the above statements together gives the desired bou nd.\nB Proof for bandit case\nIn this section we prove Proposition 1.3, which specializes our main results to the case of multi-\narmed bandits. Though our bounds for multi-armed bandits ar e superseded by our regret bounds\nfor online learning in MDPs, we present a separate proof for t he bandit case to provide intuition\nabout our techniques.\nProposition 1.3.There is an algorithm ( BanditPreds , Algorithm 5) which satisﬁes the following\ntwo guarantees, when given as input a parameter λ∈/parenleftbigA\nT,1/parenrightbig\nand predictions /tildewideQ:\n1. Fix any ǫ >0. If the predictions /tildewideQare anǫ-approximate distillation of Q⋆, then the regret is\n/tildewideO(ǫT+/radicalbig\n|G|·T+√\nλ·AT), where\nG:=/braceleftig\na∈A\\{a⋆}:/tildewideQ(a)≥Q⋆(a⋆)−ǫ/bracerightig\n.\n2. For an arbitrary choice of /tildewideQ, the regret is /tildewideO/parenleftbigg/radicalig\nTA\nλ/parenrightbigg\n.\n62\n\nAlgorithm 5: BanditPreds\nInput:Action spaceA, number of time steps T, predictions/tildewideQ:A→[0,1], parameter\nλ∈[0,1] andδ >0.\n1. For each a∈A, initialize Q1(a) =∞, Q1(a) =−∞,N1(a) = 0, and/tildewideQ1(a) =/tildewideQ(a).\n2. For 1≤t≤T:\n(a) Ift≤λ·T:\ni. Select action at:= argmaxa∈A{Qt(a)}.\n(b) Else (i.e., if t > λ·T):\ni. Select action at:= argmaxa∈A/braceleftig\n/tildewideQt(a)/bracerightig\n.\n(c) For each action a∈A, letNt+1(a) denote the number of times awas taken up to\n(and including) step t.\n(d) For each action a∈A, let ˆµt+1(a) denote the mean of all rewards received when\ntakingaup to step t(ifNt+1(a) = 0, set ˆ µt+1(a) = 0).\n(e) Update the Q-value functions as follows: for each a∈A, set\nQt+1(a) :=ˆµt+1(a)+/radicaligg\n2log1/δ\nNt+1(a)\nQt+1(a) :=ˆµt+1(a)−/radicaligg\n2log1/δ\nNt+1(a)\n/tildewideQt+1(a) :=max/braceleftig\nQt+1(a),min/braceleftig\nQt+1(a),/tildewideQ(a)/bracerightig/bracerightig\n.\n63\n\nFor convenience, for each t≤T, we deﬁne\n/tildewideVt:= max\na∈A/braceleftig\n/tildewideQt(a)/bracerightig\n(98)\nBy construction of the algorithm BanditPreds , note that/tildewideQt(at) =/tildewideVt. We deﬁne the following\n“good event”E0:\nE0=/braceleftigg\n∀t≤T,∀a∈A,/vextendsingle/vextendsingleQ⋆(a)−ˆµt(a)/vextendsingle/vextendsingle≤/radicaligg\n2log1/δ\nNt(a)/bracerightigg\n.\nNote that under the event E0,Q⋆(a)∈[Qt(a),Qt(a)] for all a∈A.\nLemma B.1. Suppose the event E0holds. Then for any sub-optimal action a/\\⌉}atio\\slash=a⋆:\n1. Ifais taken at step t≤λT, thenNt(a)≤8log1/δ\n∆(a)2.\n2. Ifais taken at step t > λTand/tildewideQis anǫ-approximate distillation of Q⋆, and if∆(a)> ǫ,\nthen/tildewideQ(a)≥Q⋆(a⋆)−ǫ. In such a case, we have Nt(a)≤8log1/δ\n(∆(a)−ǫ)2.\nProof.Ifais taken when t≤λT, we must have that Qt(a)≥Qt(a⋆)≥Q⋆(a⋆), meaning that\nQ⋆(a)≥Qt(a)−/radicaligg\n8log1/δ\nNt(a)≥Q⋆(a⋆)−/radicaligg\n8log1/δ\nNt(a),\nfrom which the ﬁrst point follows.\nIfais taken when t > λT, then we must have that /tildewideQt(a)≥/tildewideQt(a⋆)≥Q⋆(a⋆)−ǫ. Since\n∆(a)> ǫ, we have that Qt(a)≤Q⋆(a)< Q⋆(a⋆)−ǫ, meaning that /tildewideQt(a)≤/tildewideQ(a), and hence\n/tildewideQ(a)≥Q⋆(a⋆)−ǫ. In such a case, we have Qt(a)≥/tildewideQt(a)≥Q⋆(a⋆)−ǫ, meaning that\nQ⋆(a)≥Qt(a)−/radicaligg\n8log1/δ\nNt(a)≥Q⋆(a⋆)−/radicaligg\n8log1/δ\nNt(a)−ǫ,\nfrom which the desired inequality follows.\nLemma B.2. Suppose the event E0holds. Then if ∆>0satisﬁes16Alog1/δ\n∆2≤λT, in the ﬁrst\n16Alog1/δ\n∆2steps, some action awith∆(a)≤∆has been taken at least8log1/δ\n∆2times.\nProof.By item 1of Lemma B.1, under the event E0, each action a∈Ais taken at most8log1/δ\n∆(a)2\ntime steps in time steps t≤λT−1. LetA∆denote the number of actions awith ∆(a)≤∆. Then\nby the pigeonhole principle, in the ﬁrst\n/summationdisplay\na∈A:∆(a)>∆8log1/δ\n∆(a)2+8A∆log1/δ\n∆2≤8Alog1/δ\n∆2≤λT−1\nsteps, some action awith ∆(a)≤∆ has been taken at least8log1/δ\n∆2times.\n64\n\nProof of Proposition 1.3.We setδ= 1/(AT2) inBanditPreds (Algorithm 5). It is straightforward\nfrom a Chernoﬀ bound and union bound that Pr( E0)≥1−δ·TA. Therefore, the expected regret\nis bounded above by\nE/bracketleftigg/summationdisplay\na∈A∆(a)·NT+1(a)/bracketrightigg\n≤δTA·T+E/bracketleftigg/BD[E0]·/summationdisplay\na∈A∆(a)·NT+1(a)/bracketrightigg\n≤1+E/bracketleftigg/summationdisplay\na∈A∆(a)·NT+1(a)|E0/bracketrightigg\n,\nwhere the second inequality uses δ≤1/(AT2).\nWe begin by bounding the regret in the event that /tildewideQis anǫ-approximate distillation of Q⋆(i.e.,\nitem1of Proposition 1.3). By item 2of Lemma B.1, any arm awith ∆(a)≥2ǫthat is pulled at\nsome step t > λTmust satisfy/tildewideQ(a)≥Q⋆(a⋆)−ǫ, i.e.,a∈G. Moreover, for such arms a, we have\nthatNT+1(a)≤32log1/δ\n∆(a)2. Therefore, under the event E0, we have\n/summationdisplay\na∈A∆(a)·NT+1(a)≤2ǫT+/summationdisplay\na∈G∆(a)·NT+1(a)+/summationdisplay\na∈A\\G∆(a)·NλT+1(a)\n≤2ǫT+/radicalbig\nλTAlog1/δ+/summationdisplay\na∈A\\G:∆(a)>√\nAlog1/δ/(λT)8log1/δ\n∆(a)\n+ /BD[|G|>0]·\n/radicalbig\nT|G|log1/δ+/summationdisplay\na∈G:∆(a)>√\n|G|log1/δ/T32log1/δ\n∆(a)\n\n≤O/parenleftig\nǫT+/radicalbig\nλTAlog1/δ+/radicalbig\nT|G|log1/δ/parenrightig\n.\nNext we prove item 2(the robustness claim) of the proposition. Set ∆ λ:=/radicalig\n16Alog1/δ\nTλ. Then\nby Lemma B.2, in the ﬁrst λTsteps ofBanditPreds , some action ¯ awith ∆(¯a)≤∆λhas been\ntaken at least8log1/δ\n∆2\nλ=Tλ\n2Atimes. Hence\nQλT+1(¯a)≥Q⋆(a⋆)−∆(¯a)−/radicalbigg\n16Alog1/δ\nTλ≥Q⋆(a⋆)−2∆λ.\nTherefore, for all t > λT, maxa∈A{/tildewideQt(a)}≥maxa∈A{Qt(a)}≥Q⋆(a⋆)−2∆λ.Hence, for any action\na∈Athat is taken at step t > λTand satisﬁes ∆( a)>2∆λ, we have that Qt(a)≥Q⋆(a⋆)−2∆λ,\nmeaning that\nQ⋆(a)≥Qt(a)−/radicaligg\n8log1/δ\nNt(a)≥Q⋆(a⋆)−/radicaligg\n8log1/δ\nNt(a)−2∆λ,\nmeaning that Nt(a)≤8log1/δ\n(∆(a)−2∆λ)2.Hence, under the event E0, we have\n/summationdisplay\na∈A∆(a)·NT+1(a)≤4∆λ·T+/summationdisplay\na∈A:∆(a)>4∆λ32log1/δ\n∆(a)≤O/parenleftigg/radicalbigg\nTAlog1/δ\nλ/parenrightigg\n.\n65\n\nReferences\n[AJK21] Alekh Agarwal, Nan Jiang, and Sham M Kakade. Reinforcement Learning: Theory\nand Algorithms . 2021.\n[ANTH94] M. Asada, S. Noda, S. Tawaratsumida, and K. Hosoda. Vision-based behavior ac-\nquisition for a shooting robot by using a reinforcement lear ning. In Proceedings of\nthe Workshop on Visual Behaviors , pages 112–118, Seattle, WA, USA, 1994. IEEE\nComput. Soc. Press.\n[AOM17] Mohammad Gheshlaghi Azar, Ian Osband, and R´ emi Mun os. Minimax Regret Bounds\nfor Reinforcement Learning. arXiv:1703.05449 [cs, stat] , July 2017. arXiv: 1703.05449.\n[BCKP20] Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, and M anish Purohit. Online Learn-\ning with Imperfect Hints. arXiv:2002.04726 [cs, math, stat] , October 2020. arXiv:\n2002.04726.\n[CHM+19] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Mile s Macklin, Jan Issac,\nNathan Ratliﬀ, and Dieter Fox. Closing the Sim-to-Real Loop: Adapting Simula-\ntion Randomization with Real World Experience. arXiv:1810.05687 [cs] , March 2019.\narXiv: 1810.05687.\n[DMMZ21] Christoph Dann, Teodor V. Marinov, Mehryar Mohri, and Julian Zimmert. Beyond\nValue-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Re-\ninforcement Learning. arXiv:2107.01264 [cs] , July 2021. arXiv: 2107.01264.\n[EDMM06] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Ac tion Elimination and Stopping\nConditionsfortheMulti-ArmedBanditandReinforcementLe arningProblems. Journal\nof Machine Learning Research , 7(39):1079–1105, 2006.\n[HK10] Elad Hazan and Satyen Kale. Extracting certainty fro m uncertainty: regret bounded\nby variation in costs. Machine Learning , 80(2-3):165–188, September 2010.\n[JAZBJ18] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is Q-learning\nProvably Eﬃcient? arXiv:1807.03765 [cs, math, stat] , July 2018. arXiv: 1807.03765.\n[KBC+18] Tim Kraska, Alex Beutel, Ed H. Chi, Jeﬀrey Dean, and Neokli s Polyzotis. The Case\nfor Learned Index Structures. arXiv:1712.01208 [cs] , April 2018. arXiv: 1712.01208.\n[LGKM20] Peng Liao, Kristjan Greenewald, Predrag Klasnja, and Susan Murphy. Personalized\nHeartSteps: A Reinforcement Learning Algorithm for Optimi zing Physical Activity.\nProceedings of the ACM on Interactive, Mobile, Wearable and Ub iquitous Technologies ,\n4(1):1–22, March 2020.\n[LLMV19] Silvio Lattanzi, Thomas Lavastida, Benjamin Mose ley, and Sergei Vassilvitskii. Online\nScheduling via Learned Weights. In Proceedings of the 2020 ACM-SIAM Symposium\non Discrete Algorithms (SODA) , Proceedings, pages 1859–1877. Society for Industrial\nand Applied Mathematics, December 2019.\n66\n\n[LML18] Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust\ntoadversarial corruptions. arXiv:1803.09353 [cs, stat] , March 2018. arXiv: 1803.09353.\n[LS20] Szepesvari Lattimore and Csaba Szepesvari. Bandit Algorithms . 2020.\n[LSSS20] Thodoris Lykouris, Max Simchowitz, Aleksandrs Sl ivkins, and Wen Sun. Corruption\nrobustexploration inepisodicreinforcement learning. arXiv:1911.08689 [cs, stat] , April\n2020. arXiv: 1911.08689.\n[LV20] Thodoris Lykouris and Sergei Vassilvitskii. Compet itive caching with machine learned\nadvice.arXiv:1802.05399 [cs] , August 2020. arXiv: 1802.05399.\n[Mit19a] Michael Mitzenmacher. A Model for Learned Bloom Fi lters, and Optimizing by Sand-\nwiching. arXiv:1901.00902 [cs, stat] , January 2019. arXiv: 1901.00902.\n[Mit19b] Michael Mitzenmacher. Scheduling with Predictio ns and the Price of Misprediction.\narXiv:1902.00732 [cs] , May 2019. arXiv: 1902.00732.\n[MV17] Andr´ es Mu˜ noz Medina and Sergei Vassilvitskii. Rev enue Optimization with Approxi-\nmate Bid Predictions. arXiv:1706.04732 [cs] , November 2017. arXiv: 1706.04732.\n[MV20] Michael Mitzenmacher andSergeiVassilvitskii. Alg orithmswithPredictions. In Beyond\nthe Worst-Case Analysis of Algorithms , pages 646–662. Cambridge University Press, 1\nedition, December 2020.\n[MY15] Mehryar Mohri and Scott Yang. Accelerating Optimiza tion via Adaptive Prediction.\narXiv:1509.05760 [cs, stat] , October 2015. arXiv: 1509.05760.\n[PSK18] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Impr oving Online Algorithms via\nML Predictions. In Advances in Neural Information Processing Systems , volume 31.\nCurran Associates, Inc., 2018.\n[Roh19] Dhruv Rohatgi. Near-Optimal Bounds for Online Cach ing with Machine Learned Ad-\nvice.arXiv:1910.12172 [cs] , October 2019. arXiv: 1910.12172.\n[Rou21] Tim Roughgarden, editor. Beyond the Worst-Case Analysis of Algorithms . Cambridge\nUniversity Press, Cambridge, 2021.\n[RS12] Alexander Rakhlin and Karthik Sridharan. Online Lea rning with Predictable Se-\nquences. arXiv:1208.3728 [cs, stat] , August 2012. arXiv: 1208.3728.\n[RS13] Alexander Rakhlin and Karthik Sridharan. Optimizat ion, Learning, and Games with\nPredictable Sequences. arXiv:1311.1869 [cs] , November 2013. arXiv: 1311.1869.\n[RVR+18] Andrei A. Rusu, Mel Vecerik, Thomas Roth¨ orl, Nicolas He ess, Razvan Pascanu,\nand Raia Hadsell. Sim-to-Real Robot Learning from Pixels wi th Progressive Nets.\narXiv:1610.04286 [cs] , May 2018. arXiv: 1610.04286.\n[SB18] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction .\nThe MIT Press, second edition, 2018.\n67\n\n[Sin92] Satinder Pal Singh. Transfer of learning by composi ng solutions of elemental sequential\ntasks.Machine Learning , 8(3):323–339, May 1992.\n[SJ19] Max Simchowitz and Kevin Jamieson. Non-Asymptotic G ap-Dependent Regret\nBounds for Tabular MDPs. arXiv:1905.03814 [cs, math, stat] , October 2019. arXiv:\n1905.03814.\n[SL14] Jacob Steinhardt and Percy Liang. Adaptivity and Opt imism: An Improved Expo-\nnentiated Gradient Algorithm. In Proceedings of the 31st International Conference on\nMachine Learning , pages 1593–1601. PMLR, June 2014. ISSN: 1938-7228.\n[TPL21] Andrea Tirinzoni, Matteo Pirotta, and Alessandro L azaric. A Fully Problem-\nDependent Regret Lower Boundfor Finite-Horizon MDPs. arXiv:2106.13013 [cs] , June\n2021. arXiv: 2106.13013.\n[TS09] Matthew E. Taylor and Peter Stone. Transfer Learning for Reinforcement Learning\nDomains: A Survey. Journal of Machine Learning Research , 10(56):1633–1685, 2009.\n[TSL09] Matthew E Taylor, Peter Stone, and Yaxin Liu. Transf er Learning via Inter-Task\nMappings for Temporal Diﬀerence Learning. Journal of Machine Learning Research ,\n8:2125–2167, 2009.\n[TST21] Volodymyr Tkachuk, Sriram Ganapathi Subramanian, and Matthew E. Taylor. The\nEﬀect of Q-function Reuse on the Total Regret of Tabular, Mode l-Free, Reinforcement\nLearning. arXiv:2103.04416 [cs] , March 2021. arXiv: 2103.04416.\n[TWSM05] Lisa Torrey, Trevor Walker, Jude Shavlik, and Rich ard Maclin. Using Advice to Trans-\nfer Knowledge Acquired in One Reinforcement Learning Task t o Another. In Ma-\nchine Learning: ECML 2005 , volume 3720, pages 412–424. Springer Berlin Heidelberg,\nBerlin, Heidelberg, 2005. Series Title: Lecture Notes in Co mputer Science.\n[TY03] F. Tanaka and M. Yamamura. Multitask reinforcement l earning on the distribution\nof MDPs. In Proceedings 2003 IEEE International Symposium on Computation al In-\ntelligence in Robotics and Automation. Computational Intell igence in Robotics and\nAutomation for the New Millennium (Cat. No.03EX694) , volume 3, pages 1108–1113\nvol.3, July 2003.\n[WLA20] Chen-Yu Wei, Haipeng Luo, and Alekh Agarwal. Taking a hint: How to leverage loss\npredictors in contextual bandits? In Thirty-Third Annual Conference On Learning\nTheory, July 2020.\n[WSJ21] Andrew Wagenmaker, Max Simchowitz, and Kevin Jamie son. Beyond No Regret:\nInstance-Dependent PAC Reinforcement Learning. arXiv:2108.02717 [cs, stat] , August\n2021. arXiv: 2108.02717.\n[WZ20] Alexander Wei and Fred Zhang. Optimal Robustness-Co nsistency Trade-oﬀs for\nLearning-Augmented Online Algorithms. arXiv:2010.11443 [cs] , October 2020. arXiv:\n2010.11443.\n68\n\n[XMD21] Haike Xu, Tengyu Ma, and Simon S. Du. Fine-Grained Ga p-Dependent Bounds for\nTabular MDPs via Adaptive Multi-Step Bootstrap. arXiv:2102.04692 [cs] , February\n2021. arXiv: 2102.04692.\n[YLN20] Chao Yu, Jiming Liu, and Shamim Nemati. Reinforceme nt Learning in Healthcare: A\nSurvey.arXiv:1908.08796 [cs] , April 2020. arXiv: 1908.08796.\n[YYD21] Kunhe Yang, Lin F. Yang, and Simon S. Du. $Q$-learning with Logarithmic Regret.\narXiv:2006.09118 [cs, math, stat] , February 2021. arXiv: 2006.09118.\n[ZB19] Andrea Zanette and Emma Brunskill. Tighter Problem- Dependent Regret Bounds in\nReinforcement Learning without Domain Knowledge using Val ue Function Bounds.\narXiv:1901.00210 [cs, stat] , November 2019. arXiv: 1901.00210.\n[ZLZ21] ZhuangdiZhu,KaixiangLin, andJiayuZhou. Transfe rLearninginDeepReinforcement\nLearning: A Survey. arXiv:2009.07888 [cs, stat] , March 2021. arXiv: 2009.07888.\n[ZW21] Chicheng Zhang and Zhi Wang. Provably Eﬃcient Multi- Task Reinforcement Learning\nwith Model Transfer. arXiv:2107.08622 [cs] , July 2021. arXiv: 2107.08622.\n[ZZJ20] Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost Opt imal Model-Free Reinforce-\nment Learning via Reference-Advantage Decomposition. arXiv:2004.10019 [cs, stat] ,\nJune 2020. arXiv: 2004.10019.\n69",
  "textLength": 172625
}