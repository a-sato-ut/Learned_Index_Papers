{
  "paperId": "51287ae2d4c79d57b1ead1f99afcc8e6a54fb787",
  "title": "Learning-Augmented Maximum Flow",
  "pdfPath": "51287ae2d4c79d57b1ead1f99afcc8e6a54fb787.pdf",
  "text": "arXiv:2207.12911v1  [cs.DS]  26 Jul 2022Learning-Augmented Maximum Flow\nAdam Polak∗\nEPFLMaksym Zub\nJagiellonian University\nAbstract\nWe propose a framework for speeding up maximum ﬂow computati on by\nusing predictions. A prediction is a ﬂow, i.e., an assignmen t of non-negative\nﬂow values to edges, which satisﬁes the ﬂow conservation pro perty, but does\nnot necessarily respect the edge capacities of the actual in stance (since these\nwere unknown at the time of learning). We present an algorith m that, given\nanm-edge ﬂow network and a predicted ﬂow, computes a maximum ﬂow\ninO(mη)time, where ηis theℓ1error of the prediction, i.e., the sum over\nthe edges of the absolute diﬀerence between the predicted an d optimal ﬂow\nvalues. Moreover, we prove that, given an oracle access to a d istribution over\nﬂow networks, it is possible to eﬃciently PAC-learn a predic tion minimizing\nthe expected ℓ1error over that distribution. Our results ﬁt into the recent line\nof research on learning-augmented algorithms, which aims t o improve over\nworst-case bounds of classical algorithms by using predict ions, e.g., machine-\nlearned from previous similar instances. So far, the main fo cus in this area\nwas on improving competitive ratios for online problems. Fo llowing Dinitz\net al. (NeurIPS 2021), our results are one of the ﬁrsts to impr ove the running\ntime of an oﬄine problem.\n1 Introduction\nComputing a maximum s-tﬂow in a ﬂow network (i.e., in a directed graph with non-\nnegative edge capacities and designated source and sink nod es) is a basic problem\nin combinatorial optimization. It is a building block of a nu mber of more advanced\nalgorithms, with relevance both in theory (e.g., in graph al gorithms and scheduling)\nand in practice (e.g., in computer vision and transport).\nImagine we are to solve multiple similar instances of the max imum ﬂow prob-\nlem, e.g., the instances are drawn at random from a distribut ion, or they are snap-\nshots of a single underlying instance changing over time. Ca n we learn an approxi-\nmate shape of optimal solutions, and then use it to speed up fu rther computations?\n∗Supported by the Swiss National Science Foundation project Lattice Algorithms and Integer Pro-\ngramming (185030).\n1\n\nOr, to put it diﬀerently, assume we have a solution – e.g., obt ained from past data\nor computed by a very fast heuristic – that is not necessarily optimal, maybe not\neven feasible, but close to an optimal solution. How can we us e such an imperfect\nsolution to warm-start a maximum ﬂow algorithm and get a bett er running time?\nWarm-starting maximum ﬂow algorithms have been studied in t he past heuris-\ntically (e.g., in computer vision, where maximum ﬂow is ofte n used to compute\nminimum cuts in subsequent frames of a video [17]). In contra st, we propose an\napproach with theoretical guarantees.\nLearning-augmented algorithms (also called algorithms with predictions ) are\nthe subject of a recent line of research that aims to improve o ver worst-case bounds\nof classical algorithms by using possibly imperfect predic tions. So far, the main fo-\ncus in this area was on improving competitive ratios for onli ne problems. Dinitz et\nal. [7] took a ﬁrst step to explore improving the running time s of oﬄine problems.\nThey gave an algorithm for the weighted bipartite matching p roblem that uses a\nlearned dual solution to improve over the running time of the classic Hungarian al-\ngorithm. Our approach draws inspiration from their work, bu t it diﬀers signiﬁcantly\nin two aspects. First, we learn a primal, not a dual solution. Second, we impose an\nadditional restriction on the learned solution, namely the ﬂow conservation property.\nThis restriction makes our learning problem harder and the s ubsequent algorithmic\nproblem easier. In Section 1.2 we discuss these diﬀerences i n greater depth.\n1.1 Our results\nWe propose a framework for speeding up maximum ﬂow computati on by using\npredicted ﬂow values. Here, by prediction we mean a ﬂow, which satisﬁes the ﬂow\nconservation property, but does not necessarily respect th e edge capacities of the\nactual instance (since these were unknown at the time of lear ning). We present an\nalgorithm that, given an m-edge ﬂow network with edge capacities c∈Zm\n/greaterorequalslant0, and\na predicted ﬂow f∈Zm\n/greaterorequalslant0, computes a maximum ﬂow f∗(c)∈Zm\n/greaterorequalslant0inO(mη)\ntime, where η=||f−f∗(c)||1=/summationtext\ne∈E|f(e)−f∗(c)(e)|is theℓ1error of the\nprediction. Moreover, we prove that, given an oracle access to a (joint) distribution\nover edge capacities, it is possible to eﬃciently PAC-learn a prediction minimizing\nthe expected ℓ1error over that distribution.\nTo formally state our results, let us ﬁrst deﬁne the maximum ﬂ ow problem and\nrelated concepts.\nDeﬁnition 1. Given a directed graph G= (V,E),source andsinknodess,t∈V,\nand nonnegative integral edge capacities c:E→Z/greaterorequalslant0, themaximum ﬂow problem\nasks to ﬁnd a function f:E→Z/greaterorequalslant0, assigning nonnegative integral ﬂowto the\nedges, that satisﬁes\n2\n\n•capacity constraints :∀e∈Ef(e)/lessorequalslantc(e), and\n•ﬂow conservation :∀v∈V\\{s,t}/summationtext\n(u,v)∈Ef(u,v) =/summationtext\n(v,u)∈Ef(v,u),\nand maximizes the ﬂow value deﬁned as val(f) =/summationtext\n(s,u)∈Ef(s,u).\nWe denote by f∗(c)a maximum ﬂow for given capacities c.\nLet us note that we have made the decision to focus on the integ ral version of\nthe problem for two reasons. First, in many applications edg e capacities are integral\nanyway, and hence there always exists an integral solution a s well, see, e.g., [1].\nSecond, the error measure we work with, namely the ℓ1distance, is meaningless if\none allows arbitrary scaling without changing the problem, as it would the case for\nrational edge capacities.\nIn Section 2, we prove the following theorem giving an algori thm that can be\nseen as the Ford-Fulkerson method with a warm start.\nTheorem 2. Given a directed graph G= (V,E), source and sink s,t∈V, edge\ncapacities c:E→Z/greaterorequalslant0, and a predicted ﬂow function f:E→Z/greaterorequalslant0satisfying\nﬂow conservation, one can compute a maximum (s,t)-ﬂow inG, in time\nO/parenleftbig\n|E|·||f−f∗(c)||1/parenrightbig\n.\nNote that the above bound holds simultaneously for every max imum ﬂow f∗(c),\nwhich might not be unique. In other words, the prediction is g ood if it is close to at\nleast one optimal solution.\nOne of the sought-after properties of learning-augmented a lgorithms is robust-\nness, i.e., retaining worst-case guarantees of classic algorit hms even for arbitrarily\nbad predictions. However, in the case of running time bounds , robustness comes\nessentially for free (up to a multiplicative factor of 2, vanishing in the asymptotic\nnotation). Indeed, one can always run step-by-step an algor ithm with predictions\nalongside the fastest known classic algorithm, stopping wh en either of them stops.\nTherefore, Theorem 2 paired with the recent O(|E|1+o(1))time algorithm for the\nmaximum ﬂow problem [6] actually leads to a robust learning- augmented algorithm\nwith running time\nO/parenleftbig\n|E|·min{||f−f∗(c)||1,|E|o(1)}/parenrightbig\n.\nNow we want to argue that predictions required by the above al gorithm can be\neﬃciently learned, in a PAC-learning sense. We assume that t he underlying graph,\nas well as the choice of the source and sink nodes, are ﬁxed. (T his assumption is\nalmost without loss of generality, because one can take the u nderlying graph to\nbe a clique, with capacities zero for nonexistent edges; tha t, however, may cause a\nrunning time overhead, because of the increased number of ed ges.) Our goal is to\n3\n\nprove that, given a joint distribution over edge capacities , we can eﬃciently learn\na ﬂow approximately minimizing the expected ℓ1error over that distribution. We\ndo it in two steps. First, in Section 3, we prove Theorem 3, giv ing an algorithm\nthat ﬁnds an optimal ﬂow prediction for a given set of samples . Next, in Section 4,\nwe prove Theorem 4, arguing that, assuming a suﬃcient number of samples, such\noptimal ﬂow for samples is approximately optimal for the who le distribution.\nTheorem 3. Given a directed graph G= (V,E), with source and sink s,t∈V,\nand a collection of klists of edge capacities c1,c2,...,ck∈ZE\n/greaterorequalslant0, one can ﬁnd an\noptimal integral ﬂow prediction for this collection, i.e.,\nˆf= argmin/braceleftbigg1\nk/summationdisplay\ni∈[k]||f−f∗(ci)||1/vextendsingle/vextendsinglef:E→Z/greaterorequalslant0satisfying ﬂow conservation/bracerightbigg\n,\nin timeO((k·|E|)1+o(1)).\nTheorem 4. LetG= (V,E)be a directed graph, with source and sink s,t∈V,\nand letc1,c2,...,ck∈ZE\n/greaterorequalslant0, fork= Θ(c2\nmax|E|3log(cmax|E|)), be independent\nsamples from a distribution D, wherecmax= max c∈supp(D),e∈Ec(e). Letˆf∈ZE\n/greaterorequalslant0\nbe an optimal ﬂow prediction for this collection of samples, as in Theorem 3. Then,\nwith high probability over the choice of the samples, the exp ectedℓ1error ofˆfover\nDis approximately minimum possible, i.e.,\nEc∼D||ˆf−f∗(c)||1/lessorequalslantmin\nfEc∼D||f−f∗(c)||1+O(1),\nwhere the minimum is taken over functions f∈ZE\n/greaterorequalslant0satisfying the ﬂow conservation\nproperty.\n1.2 Related work\nMaximum ﬂow algorithms. There are numerous algorithms for the maximum\nﬂow problem. The Ford-Fulkerson method [12] is a starting po int for many of them,\nand its vanilla version runs in weakly polynomial O(|E|·val(f∗(c)))time for inte-\ngral edge capacities. The strongly polynomial time algorit hms, which also work for\nrational edge capacities, can be roughly split into three gr oups: augmenting paths\nalgorithms (e.g., [10, 8]), push-relabel algorithms (e.g. , [13]), and pseudoﬂow al-\ngorithms (e.g., [14]). Each of these groups contains algori thms with running time\n/tildewideO(|V|·|E|)that are widely used in practice, see, e.g., [4, 11]. A long li ne of research\non Laplacian solvers and interior-point methods, initiate d by [27], culminated re-\ncently with a (weakly polynomial) near-linear O(|E|1+o(1))time algorithm [6].\nIn the light of this new development, it may seem that our lear ning-augmented\nalgorithm is only relevant for very small prediction errors , namely||f−f∗(ci)||1/lessorequalslant\n4\n\n|E|1+o(1). However, at this point it is not yet clear if the new near-lin ear time algo-\nrithm will lead to practical developments.1\nLearning-augmented algorithms. The idea of using predictions to improve per-\nformance of algorithms is not a new one, see, e.g., [21]. Howe ver, the recent sys-\ntematic study of such methods – under the umbrella term on learning-augmented\nalgorithms , or simply algorithms with predictions – seems to have started with the\nworks of Lykouris and Vassilvitskii [20], and Purohit, Svit kina, and Kumar [25].\nSince then, the ﬁeld developed rapidly, see [22] for a survey . So far, the majority\nof the works focus on online algorithms, where predictions h elp reduce uncertainty\nabout the yet unseen part of the instance. There are, however , also works on, e.g.,\ndata structures [19], streaming algorithms [16], and subli near algorithms [9]. Apart\nfrom a simple example of binary search [20], until recently t here were no works on\nimproving algorithms running times using predictions. Thi s has changed with the\nwork of Dinitz et al. [7], and the recent followup work of Chen et al. [5].\nLearning-augmented weighted bipartite matching. A direct inspiration for our\napproach is the work of Dinitz et al. [7]. They study the maxim um weighted bi-\npartite matching problem, and propose to predict the dual2solution. They give a\nlearning-augmented algorithm that solves the matching pro blem inO(|E|/radicalbig\n|V| ·\nmin{η,/radicalbig\n|V|})time, where ηis theℓ1error of the predicted dual solution – our\nTheorem 2 is an analogue of that result. They also show that, g iven an oracle ac-\ncess to a joint distribution over edge weights, one can eﬃcie ntly learn a prediction\nminimizing the expected ℓ1error over the distribution – our Theorems 3 and 4 are\ntogether an analogue of that result.\nThe most apparent diﬀerence between their approach and ours is that they use\na predicted dual solution and we use a predicted primal solut ion. The reason they\nstate for focusing on the dual solution is that the primal sol ution is very volatile to\nsmall changes in the input. Let us note that this argument cle arly applies to weighted\nproblems (in particular, e.g., to the minimum cost ﬂow probl em) but it is not clear\nif it also applies to the maximum ﬂow problem. Moreover, it is also not clear if the\ndual solution is indeed less volatile, even for weighted pro blems.\nThe second important diﬀerence is that they do not impose any constraints on\npredictions, while we require that the predicted solution s atisﬁes the ﬂow conserva-\ntion property. This diﬀerence has the following consequenc es. First, their learning\n1Seehttps://codeforces.com/blog/entry/100510 for a relevant discussion with an author\nof [6].\n2Recall that the matching problem can be formulated as a linea r program, and every linear program\nhas a corresponding dual program.\n5\n\nalgorithm can be very simple – the optimal prediction is just a coordinate-wise me-\ndian over the solutions for the samples – while we need to solv e the minimum cost\nﬂow problem instead. Second, turning a prediction into a fea sible solution is also\nharder for us, as we want to maintain the ﬂow conservation pro perty. On the other\nhand, once we have a feasible solution, the remaining part of our maximum ﬂow\nalgorithm is simple and easy to analyse, in contrast with the ir tailored primal-dual\nanalysis for the analogous part of their algorithm.\nIn a recent independent work Chen et al. [5] improve the runni ng time of Dinitz\net al. for the matching problem, and extend their framework t o a couple of other\nproblems: the negative weights single-source shortest pat hs problem, the degree-\nconstrained subgraph problem, and the minimum cost 0-1 ﬂow p roblem. For all\nthese problems they use predicted dual solutions. They also propose general learn-\nability theorems, which imply what we prove in Appendix A (se e also a discussion\nbelow the proof of Lemma 7 for a comparison of these results wi th Theorem 4).\n2 Warm-starting Ford-Fulkerson\nTheorem 2. Given a directed graph G= (V,E), source and sink s,t∈V, edge\ncapacities c:E→Z/greaterorequalslant0, and a predicted ﬂow function f:E→Z/greaterorequalslant0satisfying\nﬂow conservation, one can compute a maximum (s,t)-ﬂow inG, in time\nO/parenleftbig\n|E|·||f−f∗(c)||1/parenrightbig\n.\nProof. At ﬁrst, the predicted ﬂow fdoes not necessarily satisfy the capacity con-\nstraints imposed by c, i.e., for some edges e∈Eit might happen that f(e)> c(e).\nThe algorithm consists of two steps. In the ﬁrst step, it turn sfinto¯fthat satisﬁes\nthe capacity constraints, while maintaining the ﬂow conser vation property. In other\nwords,¯fis a feasible ﬂow. Then, in the second step, the algorithm aug ments¯fto\nan optimal ﬂow.\nFirst step: feasibility. Recall that every integral ﬂow decomposes into cycles and\ns-tpaths3(see, e.g., [1, Theorem 3.5]). The algorithm initializes ¯f=f. While there\nis an edge e∈Ewith¯f(e)> c(e), the algorithm uses, e.g., depth-ﬁrst search to ﬁnd\na cycle or an s-tpath containing e(at least one of them is guaranteed to exist because\nof the integral ﬂow decomposition), and decreases the ﬂow ¯falong this cycle/path\nby one unit. This keeps the invariant that ¯fsatisﬁes the ﬂow conservation property.\nWhen the process is done, ¯fsatisﬁes also all the capacity constraints.\n3I.e., there exists a collection p1,...,p ksuch that each piis either a (simple) cycle or a (simple)\ns-tpath inG, andf(e) = #{i∈[k]|e∈pi}for every edge e∈E.\n6\n\nSecond step: optimization. Now, the algorithm constructs the residual network\nwith respect to ¯f, i.e., the ﬂow network G¯f= (V,E¯f)with edge set E¯f={(u,v)|\n(u,v)∈Eor(v,u)∈E}and residual capacities c¯f(u,v) = (c(u,v)−f(u,v))+\nf(v,u). Here, for notational simplicity, we assume that c(u,v) =f(u,v) = 0 if\n(u,v)/ne}ationslash∈E. Then, the algorithm runs the Ford-Fulkerson method [12] on G¯fto\nﬁnd a maximum ﬂow f∗(c¯f)in timeO(|E|·val(f∗(c¯f))). Finally, ¯f+f∗(c¯f)is a\nmaximum ﬂow for the original edge capacities c, see, e.g., [1, Property 2.6].\nRunning time analysis. Letδ=/summationtext\ne∈Emax{f(e)−c(e),0}be the total amount\nby which the ﬂow prediction violates the capacity constrain ts. The algorithm makes\nat mostδiterations in the ﬁrst step, and each iteration decreases th e ﬂow value\nval(¯f)by at most one. We conclude that the ﬁrst step runs in O(|E| ·δ)time, and\nthatval(f)−val(¯f)/lessorequalslantδ.\nThe second step of the algorithm runs in time\nO(|E|·val(f∗(c¯f))) =O(|E|·(val(f∗(c))−val(¯f)))\n=O(|E|·((val(f∗(c))−val(f))+(val( f)−val(¯f)))).\nLetη=||f−f∗(c)||1denote the prediction error. It is easy to see that |val(f∗(c))−\nval(f)|/lessorequalslantη, and that δ/lessorequalslantη, so, in particular, val(f)−val(¯f)/lessorequalslantδ/lessorequalslantη. Therefore,\nthe running time of both steps of the algorithm can be bounded byO(|E|·η).\n2.1 Alternative variant of the ﬁrst step\nIn this section we give an alternative variant of the ﬁrst ste p of the above algorithm.\nThe asymptotic running time remains the same, but, as we expl ain towards the end\nof this section, the alternative algorithm might be more eﬃc ient in practice.\nConsider graph /tildewideG= (V,/tildewideE)with/tildewideE={(v,u)∈V×V|(u,v)∈E}, i.e.,\na copy of Gwith reversed edges. Set capacities to ˜c(v,u) =f(u,v). Note that the\nﬁrst step of the original algorithm essentially ﬁnds an inte gralt-sﬂow˜fin/tildewideGsuch\nthat\n(i) iff(u,v)> c(u,v), then˜f(v,u)/greaterorequalslantf(u,v)−c(u,v), for every (u,v)∈E;\n(ii)val(˜f)/lessorequalslantδ.\nAt the end of the ﬁrst step ¯f(u,v) =f(u,v)−˜f(v,u). In this section we give an\nalternative way to compute such ˜f.\nAdd to/tildewideGedge(s,t), and set ˜c(s,t) =δ. Now, the problem of ﬁnding a t-s\nﬂow satisfying (i) and (ii) becomes the problem of ﬁnding a circulation4satisfying\n(i). This problem – of ﬁnding a circulation with lower bounds – can be reduced to\n4A circulation is deﬁned similarly to a ﬂow. The only exceptio n is that there are no designated\nsource and sink nodes, and hence the ﬂow conservation proper ty has to be satisﬁed for all the nodes\nof the graph (see, e.g., [1, Section 1.2]).\n7\n\na problem of ﬁnding a maximum ﬂow (without lower bounds) in a g raph with the\nmaximum ﬂow value equal to the sum of all lower bounds, see, e. g., [1, Section 6.7].\nThe reduction works as follows.\nFirst, add to /tildewideGtwo new nodes ˜sand˜t. Next, for each edge e= (u,v)∈Ethat\nviolates the capacity constraint let δe=f(u,v)−c(u,v)>0be the excess ﬂow\nfor this edge; add to /tildewideGtwo edges, (˜s,u)and(v,˜t), set their capacities to ˜c(˜s,u) =\n˜c(v,˜t) =δe, and decrease the capacity of edge (v,u)byδe, so that˜c(v,u) =c(u,v).\nThis ends the description of the graph constructed in the red uction.\nNote that the total capacity of edges leaving ˜sequals the total capacity of edges\nentering˜tequalsδ=/summationtext\ne∈Emax{f(e)−c(e),0}. As we will see in a moment,\nthe existence of a ﬂow saturating these edges is equivalent t o the existence of a\ncirculation satisfying the lower bounds – which is guarante ed to exist because the\noriginal ﬁrst step of the algorithm ﬁnds such a circulation.\nAfter constructing /tildewideGas above, the alternative ﬁrst step proceeds as follows. The\nalgorithm computes a maximum ˜s-˜tﬂow˜fin/tildewideG, using Ford-Fulkerson method.\nThen, for each edge e= (u,v)∈Ethat violates the capacity constraint (in the\noriginal graph G), the algorithm ﬁrst removes the saturated edges (˜s,u)and(v,˜t)\nfrom/tildewideG. Note that now nodes uandvdo not satisfy the ﬂow conservation property,\nnamely node vhas an excess of δeunits of incoming ﬂow and node uhas a deﬁcit\nofδeunits of incoming ﬂow. The algorithm restores the ﬂow conser vation property\nby increasing ﬂow ˜f(v,u)byδeunits, and therefore it ensures that the lower bound\nfor this edge is satisﬁed. This procedure essentially prove s the equivalence of the\nexistence of a ﬂow saturating sink and source edges and the ex istence of a suitable\ncirculation.\nThis ends the description of the alternative algorithm. Let us analyze its run-\nning time. Graph /tildewideGhasO(|E|)edges and can be constructed in O(|E|)time. Since\nval(˜f) =δ, the Ford-Fulkerson method runs in O(|E|·δ)time. Finally, transform-\ning˜fto¯ftakesO(|E|)time. Therefore, the total running time of O(|E|·δ)remains\nunchanged compared to the original ﬁrst step of the algorith m. However, the alter-\nnative algorithm diﬀers from the original one in that all the computations that take\nmore than O(|E|)time can be delegated to one of many available highly optimiz ed\nimplementations of maximum ﬂow algorithms.\nFinally, we remark that a similar trick – for handling edges i nitialized with a ﬂow\nexceeding their capacities – was already proposed, albeit w ithout provable running\ntime guarantees, in the context of repeatedly solving simil ar minimum cut instances\nin a computer vision application [18]. That trick however on ly allows computing the\nmaximum ﬂow value and a corresponding minimum cut, but not th e ﬂow itself.\n8\n\n3 Learning an optimal prediction\nTheorem 3. Given a directed graph G= (V,E), with source and sink s,t∈V,\nand a collection of klists of edge capacities c1,c2,...,ck∈ZE\n/greaterorequalslant0, one can ﬁnd an\noptimal integral ﬂow prediction for this collection, i.e.,\nˆf= argmin/braceleftbigg1\nk/summationdisplay\ni∈[k]||f−f∗(ci)||1/vextendsingle/vextendsinglef:E→Z/greaterorequalslant0satisfying ﬂow conservation/bracerightbigg\n,\nin timeO((k·|E|)1+o(1)).\nProof. The ﬁrst step of the learning algorithm is to compute a maximu m ﬂowf∗(ci)\nfor eachi∈[k]. Using the recent near-linear time algorithm [6], this step can be\ncompleted in O(k·|E|1+o(1))time in total.\nNow, the goal is to ﬁnd an integral ﬂow f(satisfying the ﬂow conservation\nproperty) that minimizes/summationtext\ni∈[k]/summationtext\ne∈E|f(e)−f∗(ci)(e)|=/summationtext\ne∈E/summationtext\ni∈[k]|f(e)−\nf∗(ci)(e)|. For an edge e∈E, letcoste(x) =/summationtext\ni∈[k]|x−f∗(ci)(e)|denote the\ncontribution of f(e)to the minimization objective, which now can be written simp ly\nas/summationtext\ne∈Ecoste(f(e)).\nLet us analyse how the function coste(x)behaves. Let x1/lessorequalslantx2/lessorequalslant···/lessorequalslantxkde-\nnote the sorted elements of the (multi-)set {f∗(c1)(e),f∗(c2)(e),...,f∗(ck)(e)}.\nClearly,coste(0) =/summationtext\ni∈[k]xi. Forx∈[0,x1], the contribution coste(x)is a de-\ncreasing linear function with slope −k. Forx∈[x1,x2], the slope is −k+2. More\ngenerally, for x∈[xi,xi+1]the slope is 2i−k, because increasing the ﬂow by δ\nincreases also by δeach of the ﬁrst isummands, and decreases by the same amount\neach of the remaining (k−i)summands in the sum/summationtext\ni∈[k]|x−xi|. Hence,coste(x)\nis piecewise linear and convex, and the overall goal is to ﬁnd a ﬂow minimizing a\nseparable piecewise linear convex cost function.\nThe above problem can be reduced to the standard minimum cost ﬂow prob-\nlem [1, Chapter 14]. The reduction works as follows. For nota tional simplicity, let\nx0= 0andxk+1= +∞. Replace each edge e∈Ewithk+ 1parallel edges\ne0,...,ek, and let edge eihave capacity xi+1−xiand cost (of sending one unit of\nﬂow) equal to 2i−k. It is easy to observe that any optimal solution to the minimu m\ncost ﬂow problem in the constructed multigraph uses some pre ﬁx of the cheapest\nparallel edges for each e∈E, and the total cost of such preﬁx behaves exactly like\ncoste. Since all the introduced capacities are integral, it is gua ranteed that there ex-\nists an optimal integral solution. The multigraph has (k+1)·|E|edges, hence the\nminimum cost ﬂow can be found in O((k·|E|)1+o(1))time [6].\n9\n\n4 Sample complexity\nTheorem 4. LetG= (V,E)be a directed graph, with source and sink s,t∈V,\nand letc1,c2,...,ck∈ZE\n/greaterorequalslant0, fork= Θ(c2\nmax|E|3log(cmax|E|)), be independent\nsamples from a distribution D, wherecmax= max c∈supp(D),e∈Ec(e). Letˆf∈ZE\n/greaterorequalslant0\nbe an optimal ﬂow prediction for this collection of samples, as in Theorem 3. Then,\nwith high probability over the choice of the samples, the exp ectedℓ1error ofˆfover\nDis approximately minimum possible, i.e.,\nEc∼D||ˆf−f∗(c)||1/lessorequalslantmin\nfEc∼D||f−f∗(c)||1+O(1),\nwhere the minimum is taken over functions f∈ZE\n/greaterorequalslant0satisfying the ﬂow conservation\nproperty.\nFor a ﬂow prediction f∈ZE\n/greaterorequalslant0, let us use\ncostc1,...,ck(f) =1\nk/summationdisplay\ni∈[k]||f−f∗(ci)||1,\ncostD(f) =Ec∼D||f−f∗(c)||1\nto denote the ℓ1error offon the samples and on the distribution, respectively. We\nwill use Hoeﬀding’s inequality to prove that the number of sa mples in Theorem 4 is\nlarge enough for costc1,...,ck(f)to be a good approximation of costD(f), with high\nprobability for all f’s simultaneously.\nTheorem 5 (Hoeﬀding’s inequality [15]) .LetX1,...,X kbe independent random\nvariables with values from 0toU, and letS=X1+···+Xkdenote their sum.\nThen, for all t >0,\nP/parenleftbig\n|S−ES|/greaterorequalslantt/parenrightbig\n/lessorequalslant2·exp(−2t2/kU2).\nTo use the inequality, ﬁrst we need a bound on the values of the considered\nfunctions.\nLemma 6. Any optimal ﬂow prediction fsatisﬁes||f||1/lessorequalslant2cmax|E|.\nLet us note that Lemma 6 is actually nontrivial. Even though ||f∗(ci)||∞/lessorequalslantcmax\nfor everyi∈[k], it may happen that ||f||∞> cmaxbecause of the ﬂow conservation\nconstraint, e.g., when multiple disjoint paths end at a sing le node and force a single\nedge going out of that node to have a ﬂow larger than cmax.\nProof of Lemma 6. For every c∈supp(D), we have ||c||1/lessorequalslantcmax|E|, and, since\n0/lessorequalslantf∗(c)/lessorequalslantc, then also ||f∗(c)||1/lessorequalslantcmax|E|. Moreover, by the triangle inequality,\n10\n\n||f−f∗(c)||1+||f∗(c)||1/greaterorequalslant||f||1, and thus ||f−f∗(c)||1/greaterorequalslant||f||1−cmax|E|. If\n||f||1>2cmax|E|, then||f−f∗(c)||1> cmax|E|for every c∈supp(D), and thus\nalsocostD(f) =Ec∼D||f−f∗(c)||1> cmax|E|.\nAt the same time, if we consider the all-zero vector as a ﬂow pr ediction, we\nhave||0−f∗(c)||1=||f∗(c)||1/lessorequalslantcmax|E|, for every c∈supp(D), and thus also\ncostD(0) =Ec∼D||0−f∗(c)||1/lessorequalslantcmax|E|. It follows that fcould not be optimal\nif||f||1>2cmax|E|.\nNow we are ready to apply Hoeﬀding’s inequality in order to pr ove the following\nlemma.\nLemma 7. With high probability over the choice of the samples, for all f∈ZE\n/greaterorequalslant0\nsatisfying the ﬂow conservation property and such that ||f||1/lessorequalslant2cmax|E|it holds\nthat\n|costc1,...,ck(f)−costD(f)|/lessorequalslant1.\nProof. For a ﬁxed f, satisfying the conditions of the lemma, let Xi=1\nk||f−\nf∗(ci)||1. We have that ||f−f∗(ci)||1/lessorequalslant||f||1+||f∗(ci)||1/lessorequalslant(2+1)·cmax|E|, so\nthe random variable Xihas values from 0to3cmax|E|/k]. Clearly,costc1,...,ck(f) =\nX1+···+Xk, andEcostc1,...,ck(f) = cost D(f). Applying Hoeﬀding’s inequality,\nwitht= 1, we get that\nP/parenleftbig\n|costc1,...,ck(f)−costD(f)|/greaterorequalslant1/parenrightbig\n/lessorequalslant2·exp/parenleftbigg\n−2\nk·(3cmax|E|/k)2/parenrightbigg\n= exp(−Θ(k/(cmax|E|)2))\n= exp(−Θ(|E|log(cmax|E|))).\nLetFdenote the set of all f’s satisfying the conditions of the lemma. We can\nupper-bound the number of such f’s by|F|/lessorequalslant(2cmax|E|+1)|E|= exp(Θ( |E|log(cmax|E|))).\nTo ﬁnish the proof, note that\n|F|·poly(cmax|E|)/lessorequalslantexp(Θ(|E|log(cmax|E|)))·exp(Θ(log( cmax|E|)))\n= exp(Θ( |E|log(cmax|E|))),\nand hence we can take the union bound to conclude that with hig h probability it\nholds that |costc1,...,ck(f)−costD(f)|/lessorequalslant1for allf∈ Fsimultaneously.\nLet us remark that the above proof of Lemma 7 crucially relies on the fact that\nthe set of possible optimal ﬂow predictions is ﬁnite – becaus e they are integral and\nbounded – and therefore we can use the union bound. Dinitz et a l. [7, Section 3.3 in\ntheir supplemental material] give a proof of an analogous re sult regarding learning\noptimal dual solution for the weighted bipartite matching p roblem. Their proof is\n11\n\nmore complex than ours, it uses the notion of pseudo-dimensi on, but thanks to that it\nworks also for fractional predictions. In Appendix A we give a proof of alternative\nversion of Lemma 7, modelled after the proof of Dinitz et al., that generalizes to\nfractional ﬂows but looses a small factor log|E|in the sample complexity.\nWith Lemma 7 at hand, it takes a standard argument to prove The orem 4.\nProof of Theorem 4. Letˆfand˜fbe optimal ﬂow predictions for the samples and\nfor the whole distribution, respectively. By Lemma 6, ||ˆf||1,||˜f||1/lessorequalslant2cmax|E|,\nand hence Lemma 7 applies. Note that it is crucial that Lemma 7 holds with high\nprobability for all f’s, because ˆfis chosen after the samples are drawn from D. We\nﬁnish the proof with the following chain of inequalities.\ncostD(ˆf)/lessorequalslant\n↑\nLemma 7costc1,...,ck(ˆf)+1/lessorequalslant\n↑\nbecauseˆfis optimal for c1,...,c kcostc1,...,ck(¯f)+1/lessorequalslant\n↑\nLemma 7costD(¯f)+2.\n5 Limitations and open problems\nRepresentation error. We do prove that a prediction with a small ℓ1error can\nbe used to speed up maximum ﬂow computation, and that given a d istribution over\nﬂow networks one can learn a prediction minimizing the ℓ1error. However, we do\nnot answer the question of what makes a distribution have suc h a minimum that\nis actually small. There seems to be no standard approach to a ddress this type of\nquestion, and the related works [7, 5] do not address it eithe r.\nDropping ﬂow conservation constraints. One could consider a similar frame-\nwork to ours but without the requirement that the predicted s olution has to satisfy\nthe ﬂow conservation property. That would make a) the learni ng algorithm sim-\npler (it would be suﬃcient to output the coordinate-wise med ian), b) the minimum\nℓ1error for a distribution smaller, and c) the capacity constr aint ﬁxing step of the\nlearning-augmented maximum-ﬂow algorithm simpler (it wou ld be suﬃcient to clip\nthe predicted ﬂows to the actual capacities). However, havi ng no ﬂow conservation\nguarantee at the warm start, the second step of the algorithm would have to deal with\nboth nodes with excess and deﬁcit ﬂow. The pseudoﬂow algorit hm [14] does work\nin such a setting – so it seems a promising starting point for a learning-augmented\nalgorithm in this modiﬁed framework – but we were not able to a nalyse its perfor-\nmance in terms of the prediction error.\n12\n\nAcknowledgments\nWe would like to thank Alexandra Lassota, Sai Ganesh Nagaraj an, and Moritz Ven-\nzin for helpful discussion.\nReferences\n[1] Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin .Network ﬂows\n– theory, algorithms and applications . Prentice Hall, 1993.\n[2] Martin Anthony and Peter L. Bartlett. Neural Network Learning –\nTheoretical Foundations . Cambridge University Press, 2002. URL:\nhttp://www.cambridge.org/gb/knowledge/isbn/item1154 061/?site_locale=en_GB .\n[3] Peter L. Bartlett, Vitaly Maiorov, and Ron Meir. Almost l inear\nVC dimension bounds for piecewise polynomial networks. In Ad-\nvances in Neural Information Processing Systems 11, [NIPS C on-\nference, 1998] , pages 190–196. The MIT Press, 1998. URL:\nhttp://papers.nips.cc/paper/1515-almost-linear-vc-d imension-bounds-for-piecewise-polynomial-networks .\n[4] Yuri Boykov and Vladimir Kolmogorov. An experimental co mpar-\nison of min-cut/max-ﬂow algorithms for energy minimizatio n in vi-\nsion. IEEE Trans. Pattern Anal. Mach. Intell. , 26(9):1124–1137, 2004.\ndoi:10.1109/TPAMI.2004.60 .\n[5] Justin Y. Chen, Sandeep Silwal, Ali Vakilian, and Fred Zh ang. Faster fun-\ndamental graph algorithms via learned predictions. CoRR , abs/2204.12055,\n2022.arXiv:2204.12055 ,doi:10.48550/arXiv.2204.12055 .\n[6] Li Chen, Rasmus Kyng, Yang P. Liu, Richard Peng, Maximili an Probst Guten-\nberg, and Sushant Sachdeva. Maximum ﬂow and minimum-cost ﬂo w in\nalmost-linear time, 2022. doi:10.48550/ARXIV.2203.00671 .\n[7] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Mose-\nley, and Sergei Vassilvitskii. Faster matchings via learne d du-\nals. In Advances in Neural Information Processing Systems , vol-\nume 34, pages 10393–10406. Curran Associates, Inc., 2021. U RL:\nhttps://papers.nips.cc/paper/2021/hash/5616060fb8ae 85d93f334e7267307664-Abstract.html .\n[8] Yeﬁm Dinitz. Algorithm for solution of a problem of maxim um ﬂow in net-\nworks with power estimation. Soviet Math. Dokl. , 11:1277–1280, 1970.\n13\n\n[9] Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinf eld,\nSandeep Silwal, and Tal Wagner. Learning-based support est ima-\ntion in sublinear time. In 9th International Conference on Learn-\ning Representations, ICLR 2021 . OpenReview.net, 2021. URL:\nhttps://openreview.net/forum?id=tilovEHA3YS .\n[10] Jack R. Edmonds and Richard M. Karp. Theoretical improv ements in algo-\nrithmic eﬃciency for network ﬂow problems. J. ACM , 19(2):248–264, 1972.\ndoi:10.1145/321694.321699 .\n[11] Barak Fishbain, Dorit S. Hochbaum, and Stefan Müller. A competitive\nstudy of the pseudoﬂow algorithm for the minimum s-t cut prob lem in\nvision applications. J. Real Time Image Process. , 11(3):589–609, 2016.\ndoi:10.1007/s11554-013-0344-3 .\n[12] L. R. Ford and D. R. Fulkerson. Maximal ﬂow through a net-\nwork. Canadian Journal of Mathematics , 8:399–404, 1956.\ndoi:10.4153/CJM-1956-045-5 .\n[13] Andrew V. Goldberg and Robert Endre Tarjan. A new approa ch to\nthe maximum ﬂow problem. In Proceedings of the 18th Annual ACM\nSymposium on Theory of Computing, 1986 , pages 136–146. ACM, 1986.\ndoi:10.1145/12130.12144 .\n[14] Dorit S. Hochbaum. The pseudoﬂow algorithm: A new algor ithm\nfor the maximum-ﬂow problem. Oper. Res. , 56(4):992–1009, 2008.\ndoi:10.1287/opre.1080.0524 .\n[15] Wassily Hoeﬀding. Probability inequalities for sums o f bounded random vari-\nables. Journal of the American Statistical Association , 58(301):13–30, 1963.\ndoi:10.1080/01621459.1963.10500830 .\n[16] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian . Learning-\nbased frequency estimation algorithms. In 7th International Conference\non Learning Representations, ICLR 2019 . OpenReview.net, 2019. URL:\nhttps://openreview.net/forum?id=r1lohoCqY7 .\n[17] Olivier Juan and Yuri Boykov. Active graph cuts. In 2006 IEEE\nComputer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR 2006) , pages 1023–1029. IEEE Computer Society, 2006.\ndoi:10.1109/CVPR.2006.47 .\n[18] Pushmeet Kohli and Philip H. S. Torr. Eﬃciently solving dynamic markov\nrandom ﬁelds using graph cuts. In 10th IEEE International Conference on\n14\n\nComputer Vision (ICCV 2005) , pages 922–929. IEEE Computer Society, 2005.\ndoi:10.1109/ICCV.2005.81 .\n[19] Tim Kraska, Alex Beutel, Ed H. Chi, Jeﬀrey Dean, and Neok lis Polyzotis. The\ncase for learned index structures. In Proceedings of the 2018 International\nConference on Management of Data, SIGMOD Conference 2018 , pages 489–\n504. ACM, 2018. doi:10.1145/3183713.3196909 .\n[20] Thodoris Lykouris and Sergei Vassilvitskii. Competit ive caching\nwith machine learned advice. J. ACM , 68(4):24:1–24:25, 2021.\ndoi:10.1145/3447579 .\n[21] Mohammad Mahdian, Hamid Nazerzadeh, and Amin Saberi. A llocating on-\nline advertisement space with unreliable estimates. In Proceedings 8th ACM\nConference on Electronic Commerce (EC-2007) , pages 288–294. ACM, 2007.\ndoi:10.1145/1250910.1250952 .\n[22] Michael Mitzenmacher and Sergei Vassilvitskii. Algor ithms with pre-\ndictions. In Tim Roughgarden, editor, Beyond the Worst-Case Anal-\nysis of Algorithms , pages 646–662. Cambridge University Press, 2020.\ndoi:10.1017/9781108637435.037 .\n[23] Jamie Morgenstern and Tim Roughgarden. On the pseudo-d imension\nof nearly optimal auctions. In Advances in Neural Informa-\ntion Processing Systems 28: Annual Conference on Neural Inf or-\nmation Processing Systems 2015 , pages 136–144, 2015. URL:\nhttps://proceedings.neurips.cc/paper/2015/hash/fbd7 939d674997cdb4692d34de8633c4-Abstract.html .\n[24] David Pollard. Convergence of Stochastic Processes . Springer, 1984.\ndoi:https://doi.org/10.1007/978-1-4612-5254-2 .\n[25] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improvi ng online\nalgorithms via ML predictions. In Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neural Informa tion Pro-\ncessing Systems 2018, NeurIPS 2018 , pages 9684–9693, 2018. URL:\nhttps://proceedings.neurips.cc/paper/2018/hash/73a4 27badebe0e32caa2e1fc7530b7f3-Abstract.html .\n[26] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning\n– From Theory to Algorithms . Cambridge University Press, 2014.\n[27] Daniel A. Spielman and Shang-Hua Teng. Nearly-linear t ime algorithms for\ngraph partitioning, graph sparsiﬁcation, and solving line ar systems. In Pro-\nceedings of the 36th Annual ACM Symposium on Theory of Comput ing, 2004 ,\npages 81–90. ACM, 2004. doi:10.1145/1007352.1007372 .\n15\n\nA Sample complexity via pseudo-dimension\nIn this section we give an alternative proof of a variant of Le mma 7, modelled after a\ncorresponding proof by Dinitz et al. [7, Section 3.3 in their supplemental material].\nLet us recall Lemma 7 ﬁrst.\nLemma 7. With high probability over the choice of the samples, for all f∈ZE\n/greaterorequalslant0\nsatisfying the ﬂow conservation property and such that ||f||1/lessorequalslant2cmax|E|it holds\nthat\n|costc1,...,ck(f)−costD(f)|/lessorequalslant1.\nNote that kin Lemma 7 refers to the number of samples in Theorem 7, i.e.,\nk= Θ(c2\nmax|E|3log(cmax|E|)). As mentioned in Section 4, the technique of Dinitz\net al. lets us prove a result that also applies to fractional ﬂ ows but gives a slightly\nworse (by logEfactor) sample complexity. Speciﬁcally, we will prove the f ollowing\nlemma.\nLemma 8. Fork= Θ(c2\nmax|E|3log(cmax|E|)logE)samples, with high probabil-\nity over the choice of the samples, for all f∈RE\n/greaterorequalslant0satisfying the ﬂow conservation\nproperty and such that ||f||1/lessorequalslant2cmax|E|it holds that\n|costc1,...,ck(f)−costD(f)|/lessorequalslant1.\nTo prove the lemma, ﬁrst let us recall a standard tool from the statistical learning\ntheory, the pseudo-dimension, which is a generalization of the VC-dimension to\nreal-valued functions.\nDeﬁnition 9 (cf. [24]) .LetF ⊆RXbe a set of real-valued functions from a domain\nX. We say that a set S={x1,x2,...,x s} ⊆Xisshattered byFif there exist\nthresholds t1,t2,...,ts∈Rsuch that for each I∈2[s]there exists a function\nfI∈ F such that I={i∈[s]|fI(xi)/lessorequalslantti}. The pseudo-dimension ofF,\ndenoted by Pdim(F), is the size |S|of a largest set S⊆Xthat is shattered by F.\nTheorem 10 (cf. [2], [23, Theorem 2.1]) .LetF ⊆[0,U]Xbe a set of bounded\nreal-valued functions from a domain X, and letDbe a distribution over X. Let\nx1,x2,...,x k∈Xbe a set of k= Θ/parenleftbig\n(U/ǫ)2(Pdim(F)log(U/ǫ) + log( 1/p))/parenrightbig\nindependent samples from D. Then, with probability at least 1−p, for every f∈ F,\nthe average of fover the samples approximates the expectation of foverDwithin\nan additive term at most ǫ, i.e.,\n/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext\ni∈[k]f(xi)\nk−Ex∼Df(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/lessorequalslantǫ.\n16\n\nConsider the following set of functions.\nH=/braceleftbig\nhf:RE\n/greaterorequalslant0∋c/mapsto→ ||f−f∗(c)||1∈R/vextendsingle/vextendsinglef∈RE\n/greaterorequalslant0such that\nfsatisﬁes ﬂow preservation and ||f||1/lessorequalslant2cmax|E|/bracerightbig\nNote that, for a ﬁxed ﬂow prediction f, function hfmaps each possible input\nof the maximum ﬂow problem (i.e., a list of edge capacities) t o theℓ1error of\nthe prediction on this input. In other words, costc1,...,ck(f) =1\nk/summationtext\ni∈[k]hf(ci)and\ncostD(f) =Ec∼Dhf(c).\nIn order to apply Theorem 10, we ﬁrst need to upper-bound Pdim(H). It fol-\nlows immediately from the deﬁnition that the pseudo-dimens ion is monotone, i.e.,\nifF ⊆ G , thenPdim(F)/lessorequalslantPdim(G), see, e.g., [26, Section 6.8, Exercise 1].\nHence, we upper-bound Pdim(H)by the pseudo-dimension of a superset of H,\nwhich is obtained by dropping the ﬂow preservation and ℓ1-norm requirements on\nthe prediction vector f, i.e.,\nH′=/braceleftbig\nhf:RE\n/greaterorequalslant0∋c/mapsto→ ||f−f∗(c)||1∈R/vextendsingle/vextendsinglef∈RE\n/greaterorequalslant0/bracerightbig\n.\nThe pseudo-dimension of H′will be in turn upper-bounded by the pseudo-\ndimension of the following class of functions analyzed by Di nitz et al.5\nTheorem 11 (cf. [7, Theorem 7]) .LetHn={fy|y∈Rn}, wherefy:Rn→Ris\ndeﬁned by fy(x) =||y−x||1. The pseudo-dimension of Hnis at most O(nlogn).\nIndeed, to see that Pdim(H′)/lessorequalslantPdim(H|E|), observe that if {c1,c2,...,cs}\nis shattered by H′, then{f∗(c1),f∗(c2),...,f∗(cs)}is shattered by H|E|, and the\nbound follows.6\nSecond, to use Theorem 10, we need a bound on the maximum value of the\nconsidered functions. For every ﬂow prediction fsatisfying ||f||1/lessorequalslant2cmax|E|and\nfor every capacities c∈supp(D)we have\nhf(c) =||f−f∗(c)||1/lessorequalslant||f||1+||f∗(c)||1/lessorequalslant2cmax|E|+cmax|E|= 3cmax|E|.\nNow, having upper bounds on Pdim(H)and on values of hf’s, we are ready to\nprove the lemma.\n5We note that the bound of Theorem 11 follows also from [3, Theo rem 2.1], because, for a ﬁxed\ny∈Rn, the function fy(x) =||y−x||1can be computed by a neural network with one hidden layer,\na piecewise linear activation function, and O(n)parameters.\n6Like monotonicity, this is a general property of the pseudo- dimension, it follows immediately\nfrom the deﬁnition, and does not use any speciﬁc property of f unctionf∗, but we were not able to\nﬁnd a suitable reference.\n17\n\nProof of Lemma 8. LetU= 3cmax|E|. Consider the following class of functions.\n¯H=/braceleftbig¯hf:RE\n/greaterorequalslant0∋c/mapsto→min{hf(c),U} ∈[0,U]/vextendsingle/vextendsinglehf∈ H/bracerightbig\nClearly, any set that is shattered by ¯His also shattered by H, and hence\nPdim(¯H)/lessorequalslantPdim(H)/lessorequalslantPdim(H′)/lessorequalslantPdim(H|E|)/lessorequalslantO(|E|log|E|).\nWe apply Theorem 10 to ¯H, withǫ= 1andp=1/poly(cmax|E|), and we conclude\nthatk= Θ(c2\nmax|E|3log(cmax|E|)logE)samples suﬃce to guarantee that with\nhigh probability\n|costc1,...,ck(f)−costD(f)|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationtext\ni∈[k]hf(ci)\nk−Ec∼Dhf(c)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/lessorequalslant1.\nfor allfwith||f||1/lessorequalslant2cmax|E|.\n18",
  "textLength": 42381
}