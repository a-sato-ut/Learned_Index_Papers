{
  "paperId": "dbc4b7508068f77b1ad9c24d963b446def8eed2c",
  "title": "The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design",
  "pdfPath": "dbc4b7508068f77b1ad9c24d963b446def8eed2c.pdf",
  "text": "The Deep Learning Revolution and Its Implications\n \nfor Computer Architecture and Chip Design\n \n \nJeffrey Dean\n \nGoogle Research\n \njeff@google.com\n \nAbstract\n \n \nThe past decade has seen a remarkable series of advances in machine learning, and in particular deep\n \nlearning approaches based on artificial neural networks, to improve our abilities to build more accurate\n \nsystems across a broad range of areas, including computer vision, speech recognition, language\n \ntranslation, and natural language understanding tasks.  This paper is a companion paper to a keynote talk\n \nat the 2020 International Solid-State Circuits Conference (ISSCC) discussing some of the advances in\n \nmachine learning, and their implications on the kinds of computational devices we need to build,\n \nespecially in the post-Moore’s Law-era.  It also discusses some of the ways that machine learning may\n \nalso be able to help with some aspects of the circuit design process.  Finally, it provides a sketch of at\n \nleast one interesting direction towards much larger-scale multi-task models that are sparsely activated\n \nand employ much more dynamic, example- and task-based routing than the machine learning models of\n \ntoday.\n \nIntroduction\n \n \nThe past decade has seen a remarkable series of advances in machine learning (ML), and in particular\n \ndeep learning approaches based on artificial neural networks, to improve our abilities to build more\n \naccurate systems across a broad range of areas [LeCun \n​\net al.\n​\n 2015].  Major areas of significant advances\n \ninclude computer vision [Krizhevsky \n​\net al.\n​\n 2012, Szegedy \n​\net al.\n​\n 2015, He et al. 2016, Real \n​\net al.\n​\n 2017, Tan\n \nand Le 2019], speech recognition [Hinton \n​\net al.\n​\n 2012, Chan \n​\net al.\n​\n 2016], language translation [Wu \n​\net al.\n \n2016] and other natural language tasks [Collobert \n​\net al.\n​\n 2011, Mikolov et al. 2013, Sutskever \n​\net al.\n​\n 2014,\n \nShazeer \n​\net al.\n​\n 2017, Vaswani et al. 2017, Devlin \n​\net al.\n​\n 2018].  The machine learning research community\n \nhas also been able to train systems to accomplish some challenging tasks by learning from interacting\n \nwith environments, often using reinforcement learning, showing success and promising advances in areas\n \nsuch as playing the game of Go [Silver \n​\net al.\n​\n 2017], playing video games such as Atari games [Mnih \n​\net al.\n \n2013, Mnih \n​\net al.\n​\n 2015] and Starcraft [Vinyals \n​\net al.\n​\n 2019], accomplishing robotics tasks such as\n \nsubstantially improved grasping for unseen objects [Levine \n​\net al.\n​\n 2016, Kalashnikov \n​\net al.\n​\n 2018],\n \nemulating observed human behavior [Sermanet \n​\net al.\n​\n 2018], and navigating complex urban environments\n \nusing autonomous vehicles [Angelova \n​\net al.\n​\n 2015, Bansal \n​\net al.\n​\n 2018].\n \n \nAs an illustration of the dramatic progress in the field of computer vision, Figure 1 shows a graph of the\n \nimprovement over time for the Imagenet challenge, an annual contest run by Stanford University [Deng \n​\net\n \nal.\n​\n 2009] where contestants are given a training set of one million color images across 1000 categories,\n \nand then use this data to train a model to generalize to an evaluation set of images across the same\n \ncategories.  In 2010 and 2011, prior to the use of deep learning approaches in this contest, the winning\n \nentrants used hand-engineered computer vision features and the top-5 error rate was above 25%.  In\n \n\n2012, Alex Krishevsky, Ilya Sutskever, and Geoffrey Hinton used a deep neural network, commonly\n \nreferred to as “AlexNet”, to take first place in the contest with a major reduction in the top-5 error rate to\n \n16% [Krishevsky \n​\net al. \n​\n2012].  Their team was the only team that used a neural network in 2012.  The\n \nnext year, the deep learning computer vision revolution was in full force with the vast majority of entries\n \nfrom teams using deep neural networks, and the winning error rate again dropped substantially to 11.7%.\n \nWe know from a careful study that Andrej Karpathy performed that human error on this task is just above\n \n5% if the human practices for ~20 hours, or 12% if a different person practices for just a few hours\n \n[Karpathy 2014].   Over the course of the years 2011 to 2017, the winning Imagenet error rate dropped\n \nsharply from 26% in 2011 to 2.3% in 2017.\n \n \n \nFigure 1: ImageNet classification contest winner accuracy over time\n \n \n \nThese advances in fundamental areas like computer vision, speech recognition, language understanding,\n \nand large-scale reinforcement learning have dramatic implications for many fields.  We have seen a\n \nsteady series of results in many different fields of science and medicine by applying the basic research\n \nresults that have been generated over the past decade to these problem areas.  Examples include\n \npromising areas of medical imaging diagnostic tasks including for diabetic retinopathy [Gulshan \n​\net al.\n \n2016, Krause \n​\net al.\n​\n 2018], breast cancer pathology [Liu \n​\net al.\n​\n 2017], lung cancer CT scan interpretation\n \n[Ardila \n​\net al.\n​\n 2019], and dermatology [Esteva \n​\net al.\n​\n 2017].  Sequential prediction methods that are useful\n \nfor language translation also turn out to be useful for making accurate predictions for a variety of different\n \nmedically-relevant tasks from electronic medical records [Rajkomar \n​\net al.\n​\n 2018].  These early signs point\n \nthe way for machine learning to have a significant impact across many areas of health and medical care\n \n[Rajkomar \n​\net al.\n​\n 2019, Esteva \n​\net al. \n​\n2019].\n \n \nOther fields that have been improved by the use of deep learning-based approaches include quantum\n \nchemistry [Gilmer \n​\net al.\n​\n 2017], earthquake prediction [DeVries \n​\net al. \n​\n2018], flood forecasting [Nevo 2019],\n \ngenomics [Poplin \n​\net al.\n​\n 2018], protein folding [Evans \n​\net al.\n​\n 2018], high energy physics [Baldi \n​\net al.\n​\n 2014],\n \nand agriculture [Ramcharan \n​\net al.\n​\n 2017].\n \n\n \nWith these significant advances, it is clear that the potential for ML to change many different fields of\n \nendeavor is substantial.\n \n \nMoore’s Law, Post Moore’s Law, and the Computational\n \nDemands of Machine Learning\n \n \nMany of the key ideas and algorithms underlying deep learning and artificial neural networks have been\n \naround since the 1960s, 1970s, 1980s, and 1990s [Minsky and Papert 1969, Rumelhart \n​\net al.\n​\n 1988,\n \nTesauro 1994].   In the late 1980s and early 1990s there was a surge of excitement in the ML and AI\n \ncommunity as people realized that neural networks could solve some problems in interesting ways, with\n \nsubstantial advantages stemming from their ability to accept very raw forms of (sometimes\n \nheterogeneous) input data and to have the model automatically build up hierarchical representations in\n \nthe course of training the model to perform some predictive task.  At that time, though, computers were\n \nnot powerful enough to allow this approach to work on anything but small, almost toy-sized problems.\n \nSome work at the time attempted to extend the amount of computation available for training neural\n \nnetworks by using parallel algorithms [Shaw 1981, Dean 1990], but for the most part, the focus of most\n \npeople in the AI and ML community shifted away from neural network-based approaches.  It was not until\n \nthe later parts of the decade of the 2000s, after two more decades of computational performance\n \nimprovements driven by Moore’s Law that computers finally started to become powerful enough to train\n \nlarge neural networks on realistic, real-world problems like Imagenet [\n​\nDeng et al. 2009\n​\n], rather than\n \nsmaller-scale, toy problems like MNIST [LeCun \n​\net al.\n​\n 2000] and CIFAR [Krizhevsky \n​\net al.\n​\n 2009].  In\n \nparticular, the paradigm of general-purpose computing on GPU cards (GPGPU) [Luebke \n​\net al.\n​\n 2006],\n \nbecause of GPU cards’ high floating point performance relative to CPUs, started to allow neural networks\n \nto show interesting results on difficult problems of real consequence.\n \n \nIt is perhaps unfortunate that just as we started to have enough computational performance to start to\n \ntackle interesting real-world problems and the increased scale and applicability of machine learning has\n \nled to a dramatic thirst for additional computational resources to tackle larger problems, the computing\n \nindustry as a whole has experienced a dramatic slowdown in the year-over-year improvement of general\n \npurpose CPU performance.  Figure 2 shows this dramatic slowdown, where we have gone from doubling\n \ngeneral-purpose CPU performance every 1.5 years (1985 through 2003) or 2 years (2003 to 2010) to now\n \nbeing in an era where general purpose CPU performance is expected to double only every 20 years\n \n[Hennessy and Patterson 2017].  Figure 3 shows the dramatic surge in computational demands for some\n \nimportant recent machine learning advances (note the logarithmic Y-axis, with the best-fit line showing a\n \ndoubling time in computational demand of 3.43 months for this select set of important ML research\n \nresults) [OpenAI 2018].  Figure 4 shows the dramatic surge in research output in the field of machine\n \nlearning and its applications, measured via the number of papers posted to the machine-learning-related\n \ncategories of Arxiv, a popular paper preprint hosting service, with more than 32 times as many papers\n \nposted in 2018 as in 2009 (a growth rate of more than doubling every 2 years).  There are now more than\n \n100 research papers per day posted to Arxiv in the machine-learning-related subtopic areas, and this\n \ngrowth shows no signs of slowing down.\n \n \n\n \nFigure 2: Computing Performance in the Moore’s Law and the Post-Moore’s Law Periods\n \n \n \nFigure 3: Some important AI Advances and their Computational Requirements\n \n(Source: \n​\nopenai.com/blog/ai-and-compute/\n​\n)\n \n \n\n \nFigure 4: Machine learning-related Arxiv papers since 2009\n \n \nMachine-Learning-Specialized Hardware\n \n \nIn 2011 and 2012, a small team of researchers and system engineers at Google built an early distributed\n \nsystem called DistBelief to enable parallel, distributed training of very large scale neural networks, using a\n \ncombination of model and data parallel training and asynchronous updates to the parameters of the\n \nmodel by many different computational replicas [Dean \n​\net al.\n​\n 2012].  This enabled us to train much larger\n \nneural networks on substantially larger data sets and, by mid-2012, using DistBelief as an underlying\n \nframework, we were seeing dramatically better accuracy for speech recognition [Hinton \n​\net al.\n​\n 2012] and\n \nimage classification models [Le \n​\net al.\n​\n 2012].  The serving of these models in demanding settings of\n \nsystems with hundreds of millions of users, though, was another matter, as the computational demands\n \nwere very large.  One back of the envelope calculation showed that in order to deploy the deep neural\n \nnetwork system that was showing significant word error rate improvements for our main speech\n \nrecognition system using CPU-based computational devices would require doubling the number of\n \ncomputers in Google datacenters (with some bold-but-still-plausible assumptions about significantly\n \nincreased usage due to more accuracy).  Even if this was economically reasonable, it would still take\n \nsignificant time, as it would involve pouring concrete, striking arrangements for windmill farm contracts,\n \nordering and installing lots of computers, etc., and the speech system was just the tip of the iceberg in\n \nterms of what we saw as the potential set of the application of neural networks to many of our core\n \nproblems and products.  This thought exercise started to get us thinking about building specialized\n \nhardware for neural networks, first for inference, and then later systems for both training and inference.\n \n \nWhy Does Specialized Hardware Make Sense for Deep Learning Models?\n \n \nDeep learning models have three properties that make them different than many other kinds of more\n \ngeneral purpose computations.  First, they are very tolerant of reduced-precision computations.  Second,\n \n\nthe computations performed by most models are simply different compositions of a relatively small\n \nhandful of operations like matrix multiplies, vector operations, application of convolutional kernels, and\n \nother dense linear algebra calculations [Vanhoucke \n​\net al.\n​\n 2011].  Third, many of the mechanisms\n \ndeveloped over the past 40 years to enable general-purpose programs to run with high performance on\n \nmodern CPUs, such as branch predictors, speculative execution, hyperthreaded-execution processing\n \ncores, and deep cache memory hierarchies and TLB subsystems are unnecessary for machine learning\n \ncomputations.  So, the opportunity exists to build computational hardware that is specialized for dense,\n \nlow-precision linear algebra, and not much else, but is still programmable at the level of specifying\n \nprograms as different compositions of mostly linear algebra-style operations.  This confluence of\n \ncharacteristics is not dissimilar from the observations that led to the development of specialized digital\n \nsignal processors (DSPs) for telecom applications starting in the 1980s\n \n[\n​\nen.wikipedia.org/wiki/Digital_signal_processor\n​\n].  A key difference though, is because of the broad\n \napplicability of deep learning to huge swaths of computational problems across many domains and fields\n \nof endeavor, this hardware, despite its narrow set of supported operations, can be used for a wide variety\n \nof important computations, rather than the more narrowly tailored uses of DSPs.  Based on our thought\n \nexperiment about the dramatically increased computational demands of deep neural networks for some of\n \nour high volume inference applications like speech recognition and image classification, we decided to\n \nstart an effort to design a series of accelerators called Tensor Processing Units for accelerating deep\n \nlearning inference and training.  The first such system, called TPUv1, was a single chip design designed\n \nto target inference acceleration [Jouppi \n​\net al.\n​\n 2017].\n \n \nFor inference (after a model has been trained, and we want to apply the already-trained model to new\n \ninputs in order to make predictions), 8-bit integer-only calculations have been shown to be sufficient for\n \nmany important models [Jouppi \n​\net al. \n​\n2017], with further widespread work going on in the research\n \ncommunity to push this boundary further using things like even lower precision weights, and techniques to\n \nencourage sparsity of weights and/or activations.\n \n \nThe heart of the TPUv1 is a 65,536 8-bit multiply-accumulate matrix multiply unit that offers a peak\n \nthroughput of 92 TeraOps/second (TOPS).  TPUv1 is on average about 15X -- 30X faster than its\n \ncontemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher, and was able to run production\n \nneural net applications representing about 95% of Google datacenters' neural network inference demand\n \nat the time with significant cost and power advantages [Jouppi \n​\net al.\n​\n 2017].\n \n \nInference on low-power mobile devices is also incredibly important for many uses of machine learning.\n \nBeing able to run machine learning models on-device, where the devices themselves are often the source\n \nof the raw data inputs used for models in areas like speech or vision, can have substantial latency as well\n \nas privacy benefits.  It is possible to take the same design principles used for TPUv1 (a simple design\n \ntargeting low precision linear algebra computations at high performance/Watt) and apply these principles\n \nto much lower power environments, such as mobile phones.  Google’s Edge TPU is one example of such\n \na system, offering 4 TOps in a 2W power envelope [\n​\ncloud.google.com/edge-tpu/\n​\n,\n \ncoral.withgoogle.com/products/\n​\n].  On-device computation is already critical to many interesting use cases\n \nof deep learning, where we want computer vision, speech and other kinds of models that can run directly\n \non sensory inputs without requiring connectivity.  One such example is on-device agriculture applications,\n \nlike identification of diseases in plants such as cassava, in the middle of cassava fields which may not\n \nhave reliable network connectivity [Ramcharan \n​\net al. \n​\n2017].\n \n \nWith the widespread adoption of machine learning and its growing importance as a key type of\n \ncomputation in the world, a Cambrian-style explosion of new and interesting accelerators for machine\n \n\nlearning computations is underway.  There are more than XX venture-backed startup companies, as well\n \nas a variety of large, established companies, that are each producing various new chips and systems for\n \nmachine learning.  Some, such as Cerebras [\n​\nwww.cerebras.net/\n​\n], Graphcore [\n​\nwww.graphcore.ai/\n​\n], and\n \nNervana (acquired by Intel) [\n​\nwww.intel.ai/nervana-nnp/\n​\n] are focused on a variety of designs for ML\n \ntraining.  Others, such as Alibaba\n \n[\n​\nwww.alibabacloud.com/blog/alibaba-unveils-ai-chip-to-enhance-cloud-computing-power_595409\n​\n] are\n \ndesigning chips focused on inference..  Some of the designs eschew larger memory-capacity DRAM or\n \nHBM to focus on very high performance designs for models that are small enough that their entire set of\n \nparameters and intermediate values fit in SRAM.  Others focus on designs that include DRAM or HBM\n \nthat make them suitable for larger-scale models.  Some, like Cerebras, are exploring full wafer-scale\n \nintegration.  Others, such as Google’s Edge TPUs [\n​\ncloud.google.com/edge-tpu/\n​\n] are building very low\n \npower chips for inference in environments such as mobile phones and distributed sensing devices.\n \n \nDesigning customized machine learning hardware for training (rather than just inference) is a more\n \ncomplex endeavor than single chip inference accelerators.  The reason is that single-chip systems for\n \ntraining are unable to solve many problems that we want to solve in reasonable periods of time (e.g.\n \nhours or days, rather than weeks or months), because a single-chip system cannot deliver sufficient\n \ncomputational power.  Furthermore, the desire to train larger models on larger data sets is such that, even\n \nif a single chip could deliver enough computation to solve a given problem in a reasonable amount of\n \ntime, that would just mean that we would often want to solve even larger problems (necessitating the use\n \nof multiple chips in a parallel or distributed system anyway).   Therefore, designing training systems is\n \nreally about designing larger-scale, holistic computer systems, and requires thinking about individual\n \naccelerator chip design, as well as high performance interconnects to form tightly coupled machine\n \nlearning supercomputers.  Google’s second- and third-generation TPUs, TPUv2 and TPUv3\n \n[\n​\ncloud.google.com/tpu/\n​\n], are designed to support both training and inference, and the basic individual\n \ndevices, each consisting of four chips, were designed to be connected together into larger configurations\n \ncalled pods.   Figure 5 shows the block diagram of a single Google TPUv2 chip, with two cores, with the\n \nmain computational capacity in each core provided by a large matrix multiply unit that can yield the results\n \nof multiplying a pair of 128x128 matrices each cycle.  Each chip has 16 GB (TPUv2) or 32 GB (TPUv3) of\n \nattached high-bandwidth memory (HBM).   Figure 6 shows the deployment form of a Google’s TPUv3 Pod\n \nof 1024 accelerator chips, consisting of eight racks of chips and accompanying servers, with the chips\n \nconnected together in a 32x32 toroidal mesh, providing a peak system performance of more than 100\n \npetaflop/s.\n \n \n \nFigure 5: A block diagram of Google’s Tensor Processing Unit v2 (TPUv2)\n \n\n \n \n \nFigure 6: Google’s TPUv3 Pod, consisting of 1024 TPUv3 chips w/peak performance of >100 petaflop/s\n \nLow Precision Numeric Formats for Machine Learning\n \nTPUv2 and TPUv3 use a custom-designed floating point format called bfloat16 [Wang and Kanwar 2019],\n \nwhich departs from the IEEE half-precision 16-bit format to provide a format that is more useful for\n \nmachine learning and also enables much cheaper multiplier circuits.  bfloat16 was originally developed as\n \na lossy compression technique to help reduce bandwidth requirements during network communications of\n \nmachine learning weights and activations in the DistBelief system, and was described briefly in section\n \n5.5 of the TensorFlow white paper [Abadi \n​\net al.\n​\n 2016, sec. 5.5].  It has been the workhorse floating format\n \nin TPUv2 and TPUv3 since 2015.  As of December, 2018, Intel announced plans to add bfloat16 support\n \nto future generations of Intel processors [Morgan 2018].\n \n \nFigure 7 below shows the split between sign, exponent, and mantissa bits for the IEEE fp32\n \nsingle-precision floating point format, the IEEE fp16 half-precision floating point format, and the bfloat16\n \nformat.\n \n \nFigure 7: Differences between single-precision IEEE/half-precision IEEE/brain16 Floating Point Formats\n \n \nAs it turns out, machine learning computations used in deep learning models care more about dynamic\n \nrange than they do about precision.  Furthermore, one major area & power cost of multiplier circuits for a\n \nfloating point format with \n​\nM\n​\n mantissa bits is the (\n​\nM\n​\n+1) \n✕\n (\n​\nM\n​\n+1) array of full adders (that are needed for\n \n\nmultiplying together the mantissa portions of the two input numbers.  The IEEE fp32, IEEE fp16 and\n \nbfloat16 formats need 576 full adders, 121 full adders, and 64 full adders, respectively.  Because\n \nmultipliers for the bfloat16 format require so much less circuitry, it is possible to put more multipliers in the\n \nsame chip area and power budget, thereby meaning that ML accelerators employing this format can have\n \nhigher flops/sec and flops/Watt, all other things being equal.  Reduced precision representations also\n \nreduce the bandwidth and energy required to move data to and from memory or to send it across\n \ninterconnect fabrics, giving further efficiency gains.\n \n \nThe Challenge of Uncertainty in a Fast Moving Field\n \n \nOne challenge for building machine learning accelerator hardware is that the ML research field is moving\n \nextremely fast (as witnessed by the growth and absolute number of research papers published per year\n \nshown in Figure 4).  Chip design projects that are started today often take 18 months to 24 months to\n \nfinish the design, fabricate the semiconductor parts and get them back and install them into a production\n \ndatacenter environment.  For these parts to be economically viable, they typically must have lifetimes of\n \nat least three years.  So, the challenge for computer architects building ML hardware is to predict where\n \nthe fast moving field of machine learning will be in the 2 to 5 year time frame.  Our experience is that\n \nbringing together computer architects, higher-level software system builders and machine learning\n \nresearchers to discuss co-design-related topics like “what might be possible in the hardware in that time\n \nframe?” and “what interesting research trends are starting to appear and what would be their implications\n \nfor ML hardware?” is a useful way to try to ensure that we design and build useful hardware to accelerate\n \nML research and production uses of ML.\n \nMachine Learning for Chip Design\n \n \nOne area that has significant potential is the use of machine learning to learn to automatically generate\n \nhigh quality solutions for a number of different NP-hard optimization problems that exist in the overall\n \nworkflow for designing custom ASICs.  For example, currently placement and routing for complex ASIC\n \ndesigns takes large teams of human placement experts to iteratively refine from high-level placement to\n \ndetailed placement as the overall design of an ASIC is fleshed out.  Because there is considerable human\n \ninvolvement in the placement process, it is inconceivable to consider radically different layouts without\n \ndramatically affecting the schedule of a chip project once the initial high level design is done.  However,\n \nplacement and routing is a problem that is amenable to the sorts of reinforcement learning approaches\n \nthat were successful in solving games, like AlphaGo.  In placement and routing, a sequence of placement\n \nand routing decisions all combine to affect a set of overall metrics like chip area, timing, and wire length.\n \nBy having a reinforcement learning algorithm learn to “play” the game of placement and routing, either in\n \ngeneral across many different ASIC designs, or for a particular ASIC design, with a reward function that\n \ncombines the various attributes into a single numerical reward function, and by applying significant\n \namounts of machine-learning computation (in the form of ML accelerators), it may be possible to have a\n \nsystem that can do placement and routing more rapidly and more effectively than a team of human\n \nexperts working with existing electronic design tools for placement and routing.   We have been exploring\n \nthese approaches internally at Google and have early preliminary-but-promising looking results.  The\n \nautomated ML based system also enables rapid design space exploration, as the reward function can be\n \neasily adjusted to optimize for different trade-offs in target optimization metrics.\n \n \n\nFurthermore, it may even be possible to train a machine learning system to make a whole series of\n \ndecisions from high-level synthesis down to actual low-level logic representations and then perform\n \nplacement and routing of these low-level circuits into a physical realization of the actual high level design\n \nin a much more automated and end-to-end fashion.  If this could happen, then it’s possible that the time\n \nfor a complex ASIC design could be reduced substantially, from many months down to weeks.  This\n \nwould significantly alter the tradeoffs involved in deciding when it made sense to design custom chips,\n \nbecause the current high level of non-recurring engineering expenses often mean that custom chips or\n \ncircuits are designed only for the highest volume and highest value applications.\n \n \nMachine Learning for Semiconductor Manufacturing Problems\n \n \nWith the dramatic improvements in computer vision over the past decade, there are a number of\n \nproblems in the domain of visual inspection of wafers during the semiconductor manufacturing process\n \nthat may be amenable to more automation, or to improved accuracy over the existing approaches in this\n \narea.  By detecting defects earlier or more accurately, we may be able to achieve higher yields or reduced\n \ncosts.  A survey of these approaches provides a general sense of the area [Huang and Pan 2015]\n \n \nMachine Learning for Learned Heuristics in Computer Systems\n \n \nAnother opportunity for machine learning is in the use of learned heuristics in computer systems such as\n \ncompilers, operating systems, file systems, networking stacks, etc.  Computer systems are filled with\n \nhand-written heuristics that have to work in the general case.  For example, compilers must make\n \ndecisions about which routines to inline, which instruction sequences to choose which of many possible\n \nloop nesting structures to use, and how to lay out data structures in memory [Aho \n​\net al. \n​\n1986].  Low-level\n \nnetworking software stacks must make decisions about when to increase or decrease the TCP window\n \nsize, when to retransmit packets that might have been dropped, and whether and how to compress data\n \nacross network links with different characteristics.  Operating systems must choose which blocks to evict\n \nfrom their buffer cache, which processes and threads to schedule next, and which data to prefetch from\n \ndisk [Tanenbaum and Woodhull 1997].  Database systems choose execution plans for high-level queries,\n \nmake decisions about how to lay out high level data on disks, and which compression methods to use for\n \nwhich pieces of data [Silberschatz \n​\net al.\n​\n 1997].\n \n \nThe potential exists to use machine-learned heuristics to replace hand-coded heuristics, with the ability\n \nfor these ML heuristics to take into account much more contextual information than is possible in\n \nhand-written heuristics, allowing them to adapt more readily to the actual usage patterns of a system,\n \nrather than being constructed for the average case.  Other uses of ML can replace traditional data\n \nstructures like B-trees, hash tables, and Bloom filters with learned index structures, that can take\n \nadvantage of the actual distribution of data being processed by a system to produce indices that are\n \nhigher performance while being 20X to 100X smaller [Kraska \n​\net al.\n​\n 2018].\n \nFuture Machine Learning Directions\n \n \nA few interesting threads of research are occuring in the ML research community at the moment that will\n \nlikely be even more interesting if combined together.\n \n\n \nFirst, work on sparsely-activated models, such as the sparsely-gated mixture of experts model [Shazeer\n \net al.\n​\n 2017], shows how to build very large capacity models where just a portion of the model is “activated”\n \nfor any given example (say, just 2 or 3 experts out of 2048 experts).  The routing function in such models\n \nis trained simultaneously and jointly with the different experts, so that the routing function learns which\n \nexperts are good at which sorts of examples, and the experts simultaneously learn to specialize for the\n \ncharacteristics of the stream of examples to which they are given.  This is in contrast with most ML\n \nmodels today where the whole model is activated for every example.   Table 4 in Shazeer \n​\net al. \n​\n2017\n \nshowed that such an approach be simultaneously ~9X more efficient for training, ~2.5X more efficient for\n \ninference, and higher accuracy (+1 BLEU point for a language translation task).\n \n \nSecond, work on automated machine learning (AutoML), where techniques such as neural architecture\n \nsearch [Zoph and Le 2016, Pham \n​\net al.\n​\n 2018] or evolutionary architectural search [Real \n​\net al.\n​\n 2017, Gaier\n \nand Ha 2019] can automatically learn effective structures and other aspects of machine learning models\n \nor components in order to optimize accuracy for a given task.  These approaches often involve running\n \nmany automated experiments, each of which may involve significant amounts of computation.\n \n \nThird, multi-task training at modest scales of a few to a few dozen related tasks, or transfer learning from\n \na model trained on a large amount of data for a related task and then fine-tuned on a small amount of\n \ndata for a new task, has been shown to be very effective in a wide variety of problems [Devlin \n​\net al. \n​\n2018].\n \nSo far, most use of multi-task machine learning is usually in the context of a single modality (e.g. all visual\n \ntasks, or all textual tasks) [Doersch and Zisserman 2017], although a few authors have considered\n \nmulti-modality settings as well [Ruder 2017].\n \n \n \nA particularly interesting research direction puts these three trends together, with a system running on\n \nlarge-scale ML accelerator hardware, with a goal of being able to train a model that can perform\n \nthousands or millions of tasks in a single model.  Such a model might be made up of many different\n \ncomponents of different structures, with the flow of data between examples being relatively dynamic on\n \nan example-by-example basis.  The model might use techniques like the sparsely-gated mixture of\n \nexperts and learned routing in order to have a very large capacity model [Shazeer \n​\net al.\n​\n 2017], but where\n \na given task or example only sparsely activates a small fraction of the total components in the system\n \n(and therefore keeps computational cost and power usage per training example or inference much lower).\n \nAn interesting direction to explore would be to use dynamic and adaptive amounts of computation for\n \ndifferent examples, so that “easy” examples use much less computation than “hard” examples (a\n \nrelatively unusual property in the machine learning models of today).  Figure 8 depicts such a system.\n \n \n\n \nFigure 8: A diagram depicting a design for a large, sparsely activated, multi-task model.  Each\n \nbox in the model represents a component.  Models for tasks develop by stitching together\n \ncomponents, either using human-specified connection patterns, or automatically learned\n \nconnectivity.  Each component might be running a small architectural search to adapt to the kinds\n \nof data which is being routed to it, and routing decisions making components decide which\n \ndownstream components are best suited for a particular task or example, based on observed\n \nbehavior.\n \n \nEach component might itself be running some AutoML-like architecture search [Pham \n​\net al.\n​\n 2017], in\n \norder to adapt the structure of the component to the kinds of data that it is being routed to that\n \ncomponent.  New tasks can leverage components trained on other tasks when that is useful.  The hope is\n \nthat through very large scale multi-task learning, shared components, and learned routing, the model can\n \nvery quickly learn to accomplish new tasks to a high level of accuracy, with relatively few examples for\n \neach new task (because the model is able to leverage the expertise and internal representations it has\n \nalready developed in accomplishing other, related tasks).\n \n \nBuilding a single machine learning system that can handle millions of tasks, and that can learn to\n \nsuccessfully accomplish new tasks automatically, is a true grand challenge in the field of artificial\n \nintelligence and computer systems engineering: it will require expertise and advances in many areas,\n \nspanning solid-state circuit design, computer networking, ML-focused compilers, distributed systems, and\n \nmachine learning algorithms in order to push the field of artificial intelligence forward by building a system\n \nthat can generalize to solve new tasks independently across the full range of application areas of machine\n \nlearning.\n \nConclusion\n \nThe advances in machine learning over the past decade are already affecting a huge number of fields of\n \nscience, engineering, and other forms of human endeavor, and this influence is only going to increase.\n \nThe specialized computational needs of machine learning combined with the slowdown of\n \ngeneral-purpose CPU performance improvements in the post-Moore’s Law-era represent an exciting time\n \nfor the computing hardware industry [Hennessy and Patterson 2019\n​\n] ​\n: we now have a set of techniques\n \nthat seem to be applicable to a vast array of problems across a huge number of domains, where we want\n \nto dramatically increase the scale of the models and datasets on which we can train these models, and\n \n\nwhere the impact of this work will touch a vast fraction of humanity.  As we push the boundaries of what is\n \npossible with large-scale, massively multi-task learning systems that can generalize to new tasks, we will\n \ncreate tools to enable us to collectively accomplish more as societies and to advance humanity.  We truly\n \nlive in exciting times.\n \nAcknowledgements\n \nAnand Babu, Alison Carroll, Satrajit Chatterjee, Jason Freidenfelds, Anna Goldie, Norm Jouppi, Azalia\n \nMirhoseini, David Patterson, and Cliff Young, as well as this year’s ISSCC chairs and anonymous ISSCC\n \nrepresentatives all provided helpful feedback on the content of this article that was much appreciated.\n \nReferences\n \n[Abadi \n​\net al.\n​\n 2016] Abadi, Martín, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\n \nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,\n \nGeoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh\n \nLevenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon\n \nShlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan,\n \nFernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang\n \nZheng. \"Tensorflow: Large-scale machine learning on heterogeneous distributed systems.\"\n \narxiv.org/abs/1603.04467\n​\n (2016).\n \n[Aho \n​\net al. \n​\n1986] Aho, Alfred V., Ravi Sethi, and Jeffrey D. Ullman. \"Compilers, principles, techniques.\" \n​\nAddison\n \nWesley\n​\n (1986).\n \n[Angelova \n​\net al.\n​\n 2015] Angelova, Anelia, Alex Krizhevsky, Vincent Vanhoucke, Abhijit Ogale, and Dave Ferguson.\n \n\"Real-time pedestrian detection with deep network cascades.\" In \n​\nProceedings of BMVC\n​\n 2015,\n \nai.google/research/pubs/pub43850\n​\n (2015).\n \n[Ardila \n​\net al.\n​\n 2019] Ardila, Diego, Atilla P. Kiraly, Sujeeth Bharadwaj, Bokyung Choi, Joshua J. Reicher, Lily Peng,\n \nDaniel Tse, Mozziyar Etemadi, Wenxing Ye, Greg Corrado, David P. Naidich and Shravya Shetty.\n \n\"End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed\n \ntomography.\" \n​\nNature Medicine\n​\n 25, no. 6 (2019): 954.\n \n[Baldi \n​\net al.\n​\n 2014] Baldi, Pierre, Peter Sadowski, and Daniel Whiteson. \"Searching for exotic particles in high-energy\n \nphysics with deep learning.\" \n​\nNature Communications\n​\n 5 (2014): 4308.\n \nwww.nature.com/articles/ncomms5308\n \n[Bansal \n​\net al.\n​\n 2018] Bansal, Mayank, Alex Krizhevsky, and Abhijit Ogale. \"ChauffeurNet: Learning to drive by imitating\n \nthe best and synthesizing the worst.\" \n​\narxiv.org/abs/1812.03079\n​\n (2018).\n \n[Chan \n​\net al.\n​\n 2016] Chan, William, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. \"Listen, attend and spell: A neural\n \nnetwork for large vocabulary conversational speech recognition.\" In \n​\n2016 IEEE International Conference on\n \nAcoustics, Speech and Signal Processing (ICASSP)\n​\n, pp. 4960-4964. IEEE, 2016.  \n​\narxiv.org/abs/1508.01211\n \n[Collobert \n​\net al.\n​\n 2011] Collobert, Ronan, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel\n \nKuksa. \"Natural language processing (almost) from scratch.\" \n​\nJournal of Machine Learning Research \n​\n12, no.\n \nAug (2011): 2493-2537.  \n​\narxiv.org/abs/1103.0398\n \n[Dean 1990] Dean, Jeffrey. \"Parallel Implementations of neural network training: two back-propagation approaches”.\n \nUndergraduate honors thesis, University of Minnesota, 1990.\n \ndrive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view\n \n[Dean \n​\net al.\n​\n 2012] Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc'aurelio\n \nRanzato et al. \"Large scale distributed deep networks.\" In \n​\nAdvances in Neural Information Processing\n \nSystems\n​\n, pp. 1223-1231. 2012.  \n​\npapers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf\n \n[Dean \n​\net al.\n​\n 2018] Dean, Jeff, David Patterson, and Cliff Young. \"A new golden age in computer architecture:\n \nEmpowering the machine-learning revolution.\" IEEE Micro 38, no. 2 (2018): 21-29.\n \nieeexplore.ieee.org/document/8259424\n \n\n[Deng \n​\net al.\n​\n 2009] Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. \"Imagenet: A large-scale\n \nhierarchical image database.\" In \n​\n2009 IEEE Conference on Computer Vision and Pattern Recognition\n \n(CVPR)\n​\n, pp. 248-255. IEEE, 2009.  \n​\nhttp://www.image-net.org/papers/imagenet_cvpr09.pdf\n \n[Devlin \n​\net al.\n​\n 2018] Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. \"Bert: Pre-training of deep\n \nbidirectional transformers for language understanding.\" \n​\narxiv.org/abs/1810.04805\n​\n (2018).\n \n[DeVries \n​\net al. \n​\n2018] DeVries, Phoebe MR, Fernanda Viégas, Martin Wattenberg, and Brendan J. Meade. \"Deep\n \nlearning of aftershock patterns following large earthquakes.\" \n​\nNature\n​\n 560, no. 7720 (2018): 632.\n \nwww.nature.com/articles/s41586-018-0438-y\n \n[Doersch and Zisserman 2017] Doersch, Carl, and Andrew Zisserman. \"Multi-task self-supervised visual learning.\" In\n \nProceedings of the IEEE International Conference on Computer Vision\n​\n, pp. 2051-2060. 2017.\n \narxiv.org/abs/1708.07860\n \n[Esteva \n​\net al.\n​\n 2017] Esteva, Andre, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and\n \nSebastian Thrun. \"Dermatologist-level classification of skin cancer with deep neural networks.\" \n​\nNature\n​\n 542,\n \nno. 7639 (2017): 115.  \n​\nwww.nature.com/articles/nature21056\n \n \n[Esteva \n​\net al. \n​\n2019] Esteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo,\n \nKatherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. \"A guide to deep learning in\n \nhealthcare.\" \n​\nNature Medicine\n​\n 25, no. 1 (2019): 24.  \n​\nwww.nature.com/articles/s41591-018-0316-z\n \n[Evans \n​\net al.\n​\n 2018] Evans, R., J. Jumper, J. Kirkpatrick, L. Sifre, T. F. G. Green, C. Qin, A. Zidek et al. \"De novo\n \nstructure prediction with deep-learning based scoring.\" Annual Review of Biochemistry 77 (2018): 363-382.\n \n[Gaier and Ha 2019] Gaier, Adam, and David Ha. \"Weight Agnostic Neural Networks.\" \n​\narxiv.org/abs/1906.04358\n \n(2019).\n \n \n[Gilmer \n​\net al.\n​\n 2017] Gilmer, Justin, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.\n \n\"Neural message passing for quantum chemistry.\" In \n​\nProceedings of the 34th International Conference on\n \nMachine Learning (ICML)\n​\n-Volume 70, pp. 1263-1272. JMLR. org, 2017.  \n​\narxiv.org/abs/1704.01212\n \n[Gulshan \n​\net al.\n​\n 2016] Gulshan, Varun, Lily Peng, Marc Coram, Martin C. Stumpe, Derek Wu, Arunachalam\n \nNarayanaswamy, Subhashini Venugopalan et al. \"Development and validation of a deep learning algorithm\n \nfor detection of diabetic retinopathy in retinal fundus photographs.\" \n​\nJournal of the American Medical\n \nAssociation\n​\n (JAMA), vol. 316, no. 22 (2016): 2402-2410.  \n​\njamanetwork.com/journals/jama/fullarticle/2588763\n \n[He \n​\net al.\n​\n 2016] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Deep residual learning for image\n \nrecognition.\" In \n​\nProceedings of the IEEE conference on computer vision and pattern recognition (CVPR)\n​\n, pp.\n \n770-778. 2016.  \n​\narxiv.org/abs/1512.03385\n \n[Hennessy and Patterson 2017] Hennessy, John L., and David A. Patterson. Computer architecture: a quantitative\n \napproach, sixth edition. Morgan Kaufmann, 2017.\n \n[Hennessy and Patterson 2019] Hennessy, John L., and David A. Patterson. \"A new golden age for computer\n \narchitecture.\" Commun. ACM 62, no. 2 (2019): 48-60.\n \ncacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext\n \n[Hinton \n​\net al.\n​\n 2012] Hinton, Geoffrey, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,\n \nAndrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. \"Deep neural\n \nnetworks for acoustic modeling in speech recognition.\" \n​\nIEEE Signal Processing Magazine\n​\n 29 (2012).\n \nwww.cs.toronto.edu/~hinton/absps/DNN-2012-proof.pdf\n \n[Huang and Pan 2015] Huang, Szu-Hao, and Ying-Cheng Pan. \"Automated visual inspection in the semiconductor\n \nindustry: A survey.\" Computers in industry 66 (2015): 1-10.\n \nwww.sciencedirect.com/science/article/abs/pii/S0166361514001845\n \n[Jouppi \n​\net al.\n​\n 2017] Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder\n \nBajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao,\n \nChris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami,\n \nRajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert\n \nHundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch,\n \nNaveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle\n \nLucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan,\n \n\nRavi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy\n \nPhelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew\n \nSnelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma,\n \nErick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon.\n \n\"In-datacenter performance analysis of a tensor processing unit.\" In \n​\n2017 ACM/IEEE 44th Annual\n \nInternational Symposium on Computer Architecture (ISCA)\n​\n, pp. 1-12. IEEE, 2017.  \n​\narxiv.org/abs/1704.04760\n \n[Kalashnikov \n​\net al.\n​\n 2018] Kalashnikov, Dmitry,  Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang,\n \nDeirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. \"Qt-opt: Scalable\n \ndeep reinforcement learning for vision-based robotic manipulation.\" \n​\narxiv.org/abs/1806.10293\n​\n (2018).\n \n[Karpathy 2014] Karpathy, Andrej.  “What I learned from competing against a ConvNet on ImageNet”,\n \nkarpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/\n​\n, 2014.\n \n \n[Kraska \n​\net al.\n​\n 2018] Kraska, Tim, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. \"The case for learned\n \nindex structures.\" In \n​\nProceedings of the 2018 International Conference on Management of Data (SIGMOD)\n​\n,\n \npp. 489-504. ACM, 2018.  \n​\narxiv.org/abs/1712.01208\n \n[Krause \n​\net al. \n​\n2018] Krause, Jonathan, Varun Gulshan, Ehsan Rahimy, Peter Karth, Kasumi Widner, Greg S.\n \nCorrado, Lily Peng, and Dale R. Webster. \"Grader variability and the importance of reference standards for\n \nevaluating machine learning models for diabetic retinopathy.\" \n​\nOphthalmology\n​\n 125, no. 8 (2018): 1264-1272.\n \narxiv.org/abs/1710.01711\n \n[Krizhevsky \n​\net al.\n​\n 2009] Krizhevsky, Alex, Vinod Nair, and Geoffrey Hinton. \"The CIFAR-10 dataset.\"\n \nwww.cs.toronto.edu/~kriz/cifar.html\n​\n (2009).\n \n[Krizhevsky \n​\net al.\n​\n 2012] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep\n \nconvolutional neural networks.\" In \n​\nAdvances in Neural Information Processing Systems (NIPS)\n​\n, pp.\n \n1097-1105. 2012.\n \npapers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n \n[LeCun \n​\net al.\n​\n 2000] LeCun, Y., C. Cortes, and C. J. Burges. \"MNIST handwritten digits dataset.\" (2000).\n \nhttp://yann.lecun.com/exdb/mnist/\n \n \n[LeCun \n​\net al.\n​\n 2015] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. \"Deep learning.\" \n​\nNature\n​\n 521, no. 7553\n \n(2015): 436.  \n​\nwww.nature.com/articles/nature14539\n \n[Le \n​\net al.\n​\n 2012] Le, Quoc V., Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff\n \nDean, and Andrew Y. Ng. \"Building high-level features using large scale unsupervised learning.\" In\n \nProceedings of the 29th International Coference on International Conference on Machine Learning\n​\n, pp.\n \n507-514, 2012.  \n​\narxiv.org/abs/1112.6209\n \n[Levine \n​\net al.\n​\n 2016] Levine, Sergey, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. \"Learning hand-eye\n \ncoordination for robotic grasping with large-scale data collection.\" In \n​\nInternational Symposium on\n \nExperimental Robotics\n​\n, pp. 173-184. Springer, Cham, 2016.  \n​\narxiv.org/abs/1603.02199\n \n[Liu \n​\net al.\n​\n 2017] Liu, Yun, Krishna Gadepalli, Mohammad Norouzi, George E. Dahl, Timo Kohlberger, Aleksey Boyko,\n \nSubhashini Venugopalan, Aleksei Timofeev, Philip Q. Nelson, Greg S. Corrado, Jason D. Hipp, Lily Peng,\n \nand Martin C. Stumpe. \"Detecting cancer metastases on gigapixel pathology images.\"\n \narxiv.org/abs/1703.02442\n​\n (2017).\n \n[Luebke \n​\net al.\n​\n 2006] Luebke, David, Mark Harris, Naga Govindaraju, Aaron Lefohn, Mike Houston, John Owens, Mark\n \nSegal, Matthew Papakipos, and Ian Buck. \"GPGPU: general-purpose computation on graphics hardware.\" In\n \nProceedings of the 2006 ACM/IEEE conference on Supercomputing, p. 208. ACM, 2006.\n \ndl.acm.org/citation.cfm?id=1103933\n \n[Lu \n​\net al.\n​\n 2019] Lu, Jiasen, Dhruv Batra, Devi Parikh, and Stefan Lee. \"Vilbert: Pretraining task-agnostic visiolinguistic\n \nrepresentations for vision-and-language tasks.\" \n​\narxiv.org/abs/1908.02265\n​\n (2019).\n \n[Mikolov \n​\net al.\n​\n 2013] Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. \"Distributed\n \nrepresentations of words and phrases and their compositionality.\" In \n​\nAdvances in Neural Information\n \nProcessing Systems\n​\n, pp. 3111-3119. 2013.  \n​\narxiv.org/abs/1310.4546\n \n\n[Minsky and Papert 1969] Minsky, Marvin and Seymour Papert.  Perceptrons. \n​\nMIT Press\n​\n, 1969, Cambridge.\n \n[Mnih \n​\net al.\n​\n 2013] Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\n \nWierstra, and Martin Riedmiller. \"Playing atari with deep reinforcement learning.\" \n​\narxiv.org/abs/1312.5602\n \n(2013).\n \n[Mnih \n​\net al.\n​\n 2015] Mnih, Volodymyr,  Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\n \nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles\n \nBeattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg and\n \nDemis Hassabis. \"Human-level control through deep reinforcement learning.\" \n​\nNature\n​\n 518, no. 7540 (2015):\n \n529.  \n​\nwww.nature.com/articles/nature14236\n \n[Morgan 2018] Morgan, Timothy Prickett.  “Intel Unfolds Roadmaps for Future CPUs and GPUs”.  NextPlatform, Dec.\n \n16, 2018.  \n​\nwww.nextplatform.com/2018/12/16/intel-unfolds-roadmaps-for-future-cpus-and-gpus/\n \n[Nevo 2019] Sella Nevo, “An Inside Look at Flood Forecasting”, Google AI Blog, Sep 2019,\n \nai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\n​\n.\n \n[OpenAI 2018] OpenAI, “AI and Compute”, OpenAI Blog, May 2018, \n​\nopenai.com/blog/ai-and-compute/\n \n[Pham \n​\net al.\n​\n 2018] Pham, Hieu, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. \"Efficient neural\n \narchitecture search via parameter sharing.\" \n​\narxiv.org/abs/1802.03268\n​\n (2018).\n \n[Poplin \n​\net al.\n​\n 2018] Poplin, Ryan,  Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander\n \nKu, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T Afshar, Sam S Gross, Lizzie Dorfman, Cory Y\n \nMcLean & Mark A DePristo. \"A universal SNP and small-indel variant caller using deep neural networks.\"\n \nNature Biotechnology\n​\n 36, no. 10 (2018): 983.  \n​\nwww.nature.com/articles/nbt.4235\n \n[Rajkomar \n​\net al.\n​\n 2018] Rajkomar, Alvin,  Eyal Oren, Kai Chen, Andrew M. Dai, Nissan Hajaj, Michaela Hardt, Peter J.\n \nLiu, Xiaobing Liu, Jake Marcus, Mimi Sun, Patrik Sundberg, Hector Yee, Kun Zhang, Yi Zhang, Gerardo\n \nFlores, Gavin E. Duggan, Jamie Irvine, Quoc Le, Kurt Litsch, Alexander Mossin, Justin Tansuwan, De\n \nWang, James Wexler, Jimbo Wilson, Dana Ludwig, Samuel L. Volchenboum, Katherine Chou, Michael\n \nPearson, Srinivasan Madabushi, Nigam H. Shah, Atul J. Butte, Michael D. Howell, Claire Cui, Greg S.\n \nCorrado & Jeffrey Dean.  \"Scalable and accurate deep learning with electronic health records.\" \n​\nNature\n \nDigital Medicine\n​\n 1, no. 1 (2018): 18.  \n​\nwww.nature.com/articles/s41746-018-0029-1\n \n[Rajkomar \n​\net al.\n​\n 2019] Rajkomar, Alvin, Jeffrey Dean, and Isaac Kohane. \"Machine learning in medicine.\" \n​\nNew\n \nEngland Journal of Medicine\n​\n 380, no. 14 (2019): 1347-1358.\n \nwww.nejm.org/doi/full/10.1056/NEJMra1814259\n \n[Ramcharan \n​\net al.\n​\n 2017] Ramcharan, Amanda, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed, James Legg,\n \nand David P. Hughes. \"Deep learning for image-based cassava disease detection.\" Frontiers in plant\n \nscience 8 (2017): 1852.  \n​\nwww.frontiersin.org/articles/10.3389/fpls.2017.01852/full\n \n[Real \n​\net al.\n​\n 2017] Real, Esteban, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan,\n \nQuoc V. Le, and Alexey Kurakin. \"Large-scale evolution of image classifiers.\" In \n​\nProceedings of the 34th\n \nInternational Conference on Machine Learning (ICML)\n​\n -Volume 70, pp. 2902-2911. JMLR. org, 2017.\n \narxiv.org/abs/1703.01041\n \n[Ruder 2017] Ruder, Sebastian. \"An overview of multi-task learning in deep neural networks.\"\n \narxiv.org/abs/1706.05098\n​\n (2017).\n \n[Rumelhart \n​\net al.\n​\n 1988] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. \"Learning representations\n \nby back-propagating errors.\" Cognitive modeling 5, no. 3 (1988): 1.\n \n \n[Sermanet \n​\net al.\n​\n 2018] Sermanet, Pierre, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal,\n \nSergey Levine, and Google Brain. \"Time-contrastive networks: Self-supervised learning from video.\" In \n​\n2018\n \nIEEE International Conference on Robotics and Automation (ICRA)\n​\n, pp. 1134-1141. IEEE, 2018.\n \narxiv.org/abs/1704.06888\n \n[Shaw 1981] Shaw, David Elliot. \"NON-VON: A parallel machine architecture for knowledge-based information\n \nprocessing.\" (1981).  \n​\npdfs.semanticscholar.org/19d8/bed84c8827025c5adad43f6ec0d4eb9ed114.pdf\n \n[Shazeer \n​\net al. \n​\n2017] Shazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n \nand Jeff Dean. \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\".  In\n \nInternational Conference on Learning Representations (ICLR)\n​\n, 2017.   \n​\narxiv.org/abs/1701.06538\n​\n (2017).\n \n\n[Silberschatz \n​\net al.\n​\n 1997] Silberschatz, Abraham, Henry F. Korth, and Shashank Sudarshan. Database system\n \nconcepts. Vol. 4. New York: McGraw-Hill, 1997.\n \n[Silver \n​\net al.\n​\n 2017] Silver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\n \nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent\n \nSifre, George van den Driessche, Thore Graepel and Demis Hassabis. \"Mastering the game of go without\n \nhuman knowledge.\" \n​\nNature\n​\n 550, no. 7676 (2017): 354.  \n​\nwww.nature.com/articles/nature24270\n \n[Sutskever \n​\net al.\n​\n 2014] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural\n \nnetworks.\" In \n​\nAdvances in Neural Information Processing Systems\n​\n, pp. 3104-3112. 2014.\n \narxiv.org/abs/1409.3215\n \n[Szegedy \n​\net al.\n​\n 2015] Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,\n \nDumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. \"Going deeper with convolutions.\" In\n \nProceedings of the IEEE conference on computer vision and pattern recognition (CVPR)\n​\n, pp. 1-9. 2015.\n \narxiv.org/abs/1409.4842\n \n[Tan and Le 2019] Tan, Mingxing, and Quoc V. Le. \"EfficientNet: Rethinking Model Scaling for Convolutional Neural\n \nNetworks.\" \n​\narxiv.org/abs/1905.11946\n​\n (2019).\n \n[Tanenbaum and Woodhull 1997] Tanenbaum, Andrew S., and Albert S. Woodhull. Operating systems: design and\n \nimplementation. Vol. 68. \n​\nEnglewood Cliffs: Prentice Hall\n​\n, 1997.\n \n[Tesauro 1994] Tesauro, Gerald. \"TD-Gammon, a self-teaching backgammon program, achieves master-level play.\"\n \nNeural Computation\n​\n 6, no. 2 (1994): 215-219.\n \nwww.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf\n \n[Vanhoucke \n​\net al.\n​\n 2011] Vanhoucke, Vincent, Andrew Senior, and Mark Z. Mao. \"Improving the speed of neural\n \nnetworks on CPUs.\" In \n​\nDeep Learning and Unsupervised Feature Learning Workshop, NIPS 2011\n​\n.\n \nai.google/research/pubs/pub37631\n \n[Vaswani \n​\net al.\n​\n 2017] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n \nŁukasz Kaiser, and Illia Polosukhin. \"Attention is all you need.\" In \n​\nAdvances in Neural Information\n \nProcessing Systems\n​\n, pp. 5998-6008. 2017.  \n​\narxiv.org/abs/1706.03762\n \n[Vinyals \n​\net al.\n​\n 2019] Vinyals, Oriol, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech M.\n \nCzarnecki, Andrew Dudzik et al. \"AlphaStar: Mastering the real-time strategy game StarCraft II.\" \n​\nDeepMind\n \nBlog\n​\n (2019).  \n​\ndeepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii\n \n[Wang & Kanwar 2019] Wang, Shibo and Pankaj Kanwar.  “BFloat16: The secret to high performance on Cloud\n \nTPUs”, Google Cloud Blog, August 2019,\n \ncloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus\n \n[Wu et al. 2016] Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,\n \nMaxim Krikun et al. \"Google's neural machine translation system: Bridging the gap between human and\n \nmachine translation.\" \n​\narxiv.org/abs/1609.08144\n​\n (2016).\n \n[Zoph and Le 2016] Zoph, Barret, and Quoc V. Le. \"Neural architecture search with reinforcement learning.\"\n \narxiv.org/abs/1611.01578\n​\n (2016).\n ",
  "textLength": 56391
}