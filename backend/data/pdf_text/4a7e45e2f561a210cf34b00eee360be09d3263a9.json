{
  "paperId": "4a7e45e2f561a210cf34b00eee360be09d3263a9",
  "title": "BLI: A High-performance Bucket-based Learned Index with Concurrency Support",
  "pdfPath": "4a7e45e2f561a210cf34b00eee360be09d3263a9.pdf",
  "text": "BLI: A High-performance Bucket-based Learned Index with\nConcurrency Support\nHuibing Dong\nUniversity of Minnesota\ndong0198@umn.eduWenlong Wang\nUniversity of Minnesota\nwang9467@umn.edu\nChun Liu\nByteDance\nchun.liu@bytedance.comDavid Du\nUniversity of Minnesota\ndu@umn.edu\nABSTRACT\nLearned indexes are promising to replace traditional tree-based in-\ndexes. They typically employ machine learning models to efficiently\npredict target positions in strictly sorted linear arrays. However,\nthe strict sorted order 1) significantly increases insertion overhead,\n2) makes it challenging to support lock-free concurrency, and 3)\nharms in-node lookup/insertion efficiency due to model inaccuracy.\nIn this paper, we introduce a Bucket-based Learned Index (BLI) ,\nwhich is an updatable in-memory learned index that adopts a \"glob-\nally sorted, locally unsorted\" approach by replacing linear sorted\narrays with Buckets . BLI optimizes the insertion throughput by only\nsorting Buckets, not the key-value pairs within a Bucket. BLI strate-\ngically balances three critical performance metrics: tree fanouts,\nlookup/insert latency for inner nodes, lookup/insert latency for leaf\nnodes, and memory consumption. To minimize maintenance costs,\nBLI performs lightweight bulk loading, insert, node scaling, node\nsplit, model retraining, and node merging adaptively. BLI supports\nlock-free concurrency thanks to the unsorted design with Buckets.\nOur results show that BLI achieves up to 2.21x better throughput\nthan state-of-the-art learned indexes, with up to 3.91x gains under\nmulti-threaded conditions.\nPVLDB Reference Format:\nHuibing Dong, Wenlong Wang, Chun Liu, and David Du. BLI: A\nHigh-performance Bucket-based Learned Index with Concurrency Support.\nPVLDB, 14(1): XXX-XXX, 2025.\ndoi:XX.XX/XXX.XX\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available\nat https://github.com/gdymind/Buck-Learned-Indexandhttps://github.com/\nwwl2755/GRE_buck_index.\n1 INTRODUCTION\nIn-memory key-value stores are widely used across various data-\ncentric applications, as evidenced by the key-value store deploy-\nment at Meta[ 11], Twitter[ 17], and other companies. While tree-\nbased indexes, such as B+-trees, have traditionally dominated the\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XXindexing of key-value stores which support range queries, recent ad-\nvancements have introduced learned indexes (e.g., RMI[ 8], ALEX[ 3]\nand LIPP[ 15]) as superior alternatives due to their enhanced or\ncomparable lookup performance[ 8,13,15]. Learned indexes use\nmachine learning models to predict the approximate position of a\nlookup key in a strictly sorted array of key-value pairs. Trained on\nthe mapping from keys to their positions in the array, these mod-\nels aim for fast and accurate prediction, outperforming traditional\ntree-based index searches. For simplicity, we refer to the distribu-\ntion of all (key, position) pairs as the key distribution hereafter. If\nthe exact key-value pair is not found at the predicted position, a\nlast-mile search is performed in nearby positions of the linear array.\nThe key-value pairs are strictly sorted in the array , and the learned\nmodels act as monotonic functions of the key-position mapping.\nThe first proposed learned index, the Recursive Model Index\n(RMI)[ 8], employs a hierarchy of models for enhanced accuracy.\nThe root model spans the entire key range, branching into child\nmodels, each dedicated to a sub-key range. This branching process\nconcludes after multiple iterations, forming multiple levels of mod-\nels. After that, the last-level models (or the leaf models) predict the\nlocation of a specific key-value pair within its key range.\nHowever, key-value pairs in RMI are tightly packed into a dense\narray, which precludes accommodating new insertions within the\narray. Therefore, various updatable learned indexes have been de-\nveloped to support insertions of key-value pairs, and the number\nof models can be dynamically increased accordingly. Thanks to the\n\"sufficiently accurate\" model predictions, the throughput of existing\nupdatable learned indexes can outperform traditional indexes by\nover 80%, according to a benchmarking paper called GRE[ 14]. These\nupdatable learned indexes typically utilize one of three structures\nto facilitate insertions: merge trees, delta buffers, or gapped arrays.\nDuring insertion, the merge tree method merges multiple read-only\nsub-indexes with newly inserted key-value pairs. In the delta-buffer\napproach, new insertions are temporarily held in a buffer area,\nwhich will be periodically merged into the main index. The gapped-\narray strategy employs pre-allocated spaces (i.e., empty positions in\nthe linear array) to accommodate new key-value pairs in place. We\nrefer to gapped-array-based learned indexes as in-place updatable\nlearned indexes . Notably, according to GRE, two state-of-the-art\n(SOTA) in-place updatable learned indexes, ALEX[ 3] and LIPP[ 15],\ngenerally outperform other learned indexes. ALEX supports inser-\ntions by shifting neighboring key-value pairs within a leaf modelâ€™s\nkey range. GREâ€™s analysis reveals that, during insertion, ALEX\nspends an average of 58.60% of its time on shifting operations afterarXiv:2502.10597v1  [cs.DB]  14 Feb 2025\n\nthe lookup process, while LIPP spends 32.24% on chaining opera-\ntions. These findings motivate us to further improve performance\nby eliminating strict-order-preserving operations such as shifting\nand chaining. We introduce a new in-place updatable learned index,\ntheBucket-Based Learned Index (BLI) , which operates without strict\nordering. Before diving into the specifics of BLI, we will introduce\nsome key performance metrics to quantify potential optimizations.\n1.1 Performance metrics\nAs aforementioned, learned indexes generally use a hierarchy of\nmachine learning models. A non-leaf model holds an array of child\nmodels, while a leaf model holds an array of key-value pairs within\nits key range. The overall lookup and insertion performance is influ-\nenced by both the efficiency of individual models and the number\nof models that need to be accessed during these operations. We\nhave defined four key metrics that impact the overall performance:\n1) Model Fanout ( ğ‘). The Model Fanout refers to the number\nof child models in a non-leaf model or the number of key-value\npairs in a leaf model. Given the same dataset, the lookup traversal\npath (i.e., the sequence of all models from the root to the leaf) can\nbe shortened if larger model fanouts are accommodated.\n2) Lookup Efficiency ( ğ¸lookup ).For an individual model, given\nğ‘¡lookupğ‘–representing the lookup latency when searching for the\nğ‘–th child model or key-value pair in the modelâ€™s array, lookup ef-\nficiencyğ¸lookup can be defined as the reciprocal of the average\nlookup latency of the entries (i.e., key-value pairs or child models)\ncorresponding to the model, expressed as ğ¸lookup =ğ‘Ãğ‘\nğ‘–=1(ğ‘¡lookup ğ‘–).\nğ¸lookup is determined by all lookup latencies ğ‘¡lookupğ‘–withğ‘–from\n1toğ‘, and these latencies are mainly determined by the model\nprediction error. Balancing the tradeoff between minimizing predic-\ntion error in ğ¸lookup and maintaining an optimal ğ‘is crucial. While\na low prediction error improves ğ¸lookup , excessively reducing the\nerror tolerance can decrease ğ‘, which is detrimental.\n3) Insert Efficiency ( ğ¸insert ).Before performing insertions, a\nlookup is required to determine the appropriate insertion position,\nafter which insertions can be executed correctly. To exclude the\neffect of lookup efficiency, we define ğ¸insert asğ‘Ãğ‘\nğ‘–=1(ğ‘¡insertğ‘–âˆ’ğ‘¡lookup ğ‘–),\nwhereğ‘¡insert ğ‘–represents the total latency during insertion.\n4) Memory Overhead ( ğ‘‚mem).Memory overhead is defined as\nthe ratio of the memory usage of the entire system (including both\nkey-value pairs and the index) to the size of all key-value pairs. For\nexample, if all data is 10GB and the memory usage of the system is\n15GB, then the memory overhead is ğ‘‚mem=15\n10=150% .\nExisting in-place updatable learned indexes often suffer from\ndegradedğ¸insert due to strict-order-preserving operations such as\nshifting in ALEX and chaining in LIPP. However, we find that by\nrelaxing the strict order constraint, the four metrics (i.e., ğ‘,ğ¸lookup ,\nğ¸insert , andğ‘‚mem) can be further balanced and optimized, leading\nto improved lookup and insertion performance.\n1.2 Our proposed Bucket design\nWe present BLI, which employs a \"globally sorted, locally unsorted\"\ndesign to address several key issues in existing learned indexes.\nThis approach utilizes a hierarchical structure of machine learning\nmodels, which is a common architecture in most existing learnedindexes. Unlike these indexes, BLI does not require the sorting of\nkey-value pairs within the key range of a leaf model. Instead, each\nleaf model in BLI groups unsorted key-value pairs into a Bucket .\nHowever, Buckets are sorted based on their non-overlapping key\nranges, thus, embodying the \"globally sorted, locally unsorted\"\nconcept. We summarize our contributions as follows:\nâ€¢Improved insertion efficiency with \"locally unsorted\"\ndesign. Key-value pairs are unsorted within a Bucket. A\nnew key-value pair will be inserted into its Bucket with-\nout strict-order-preserving operations such as shifting and\nchaining, thus significantly enhancing insertion efficiency.\nIn addition, BLI provides a suggested location within a\nBucket for each key-value pair insertion to maintain a com-\nparable lookup efficiency with state-of-the-ar learned in-\ndexes.\nâ€¢Simplified key distribution with larger prediction\nunits. Unlike existing indexes, each Bucket in BLI has a rep-\nresentative key chosen from its corresponding key range,\nand prediction models to indicate the target Bucket based\non its representative key rather than a slot holding an indi-\nvidual key-value pair. As a result, the key distribution can\nbe more linear when compared to predicting the individual\nkeys, as will be proved in subsubsection 3.1.1, which can\nlead to reduced prediction errors.\nâ€¢Consistent model accuracy over its lifetime. Existing\nlearned indexes typically update their models periodically\nto avoid high retraining overhead. However, as more in-\nsertions occur over time, the modelâ€™s accuracy gradually\ndecreases, which can negatively impact ğ¸lookup . BLIâ€™s mod-\nels predict the target Bucket rather than individual keys,\nreducing the frequency of model updates when inserting\nkey-value pairs into Buckets and thus maintaining a better\nconsistent model accuracy.\nâ€¢Tunable perfomance with N, ğ¸lookup ,ğ¸insert , andğ‘‚mem.\nWe identified four major metrics, model fanout ğ‘, lookup ef-\nficiencyğ¸lookup , insert efficiency ğ¸insert , and memory over-\nheadğ‘‚mem, that affect the performance of learned indexes.\nMoreover, we investigated BLIâ€™s ability to make tradeoffs\namong these metrics, making BLI adaptive to different work-\nloads.\nâ€¢Linear-disbtribution-aware neighbor merging: With\nnew key-value pairs inserted, the key distribution of adja-\ncent models may become \"linear enough\" at some point,\nand BLI is able to merge those adjacent models into a single\nlinear model.\nâ€¢Lock-free concurrency support: Supporting lock-free\nconcurrency in in-place updatable learned indexes is chal-\nlenging since operations to enforce strictly sorted order\n(such as shifting and chaining) require locking to guarantee\ndata consistency. BLI supports lock-free concurrency with\nthe single-producer-multi-consumer threading assumption.\nWe employ a bitmap to indicate valid key-value pairs and\nread-copy-update to ensure correctness.\nAs a result, our proposed BLI surpasses the throughput of state-\nof-the-art learned indexes by 1.94 times. In multi-threaded scenarios,\nBLI outperforms other systems by up to 3.91 times.\n2\n\nThe structure of this paper is as follows: Section 2 reviews re-\nlated work and provides a brief comparison with BLI. Section 3\nidentifies key research challenges posed by the strictly sorted order\nin existing learned indexes, supported by motivational experiments\nand analysis. Section 4 introduces the proposed BLI architecture\nbased on the \"globally sorted, locally unsorted\" design. Section 5\ndetails the main operations of BLI, including lookup, insertion, bulk\nloading, and concurrency support. Section 6 presents a comparative\nevaluation of BLI against existing learned indexes and offers an in-\ndepth analysis of BLIâ€™s performance. Finally, Section 7 summarizes\nthe overall contributions of this work.\n2 RELATED WORK\nThis section will first outline the evolution from read-only to updat-\nable learned indexes, then discuss the main operations in updatable\nlearned indexes, and briefly compare existing methods with BLI.\n2.1 Evolution from read-only to updatable\nlearned indexes\nThe first learned index, Recursive Model Index (RMI)[ 8], utilizes a\nhierarchy of models to improve prediction accuracy. RMI starts with\nthe root learned model that covers the entire key range and then\nbranches the root into child models that each handle a subset of\nthe key range. This hierarchical division continues through several\niterations, ultimately creating multiple levels of models with good\nprediction accuracy. At the last level, a leaf model makes predictions\nonly on the keys within its corresponding sub-key range.\nHowever, RMI was read-only since it was built atop a static set\nof data and a densely packed array. Later, updatable learned indexes\nwere introduced to accommodate new key-value pair insertions.\nIn updatable learned indexes, the global sorted array can further\nsplit into multiple sub-arrays corresponding to the key ranges of\ntheir leaf models. These sub-arrays of key-value pairs are stored\nalong with their leaf models for ease of maintenance. Existing ap-\nproaches typically utilize the following three techniques [ 14] to\naccommodate insertions. 1) Merge Trees : PGM-Index[ 4] employs\na collection of static sub-learned indexes. During insertions, the\nnewly inserted key-value pair, along with a subset of sub-learned\nindexes, are merged in a write-optimized LSM-tree style, which\ncompromises the lookup performance since all sub-learned indexes\nneed to be visited. 2) Delta Buffers : XIndex[ 12] and FINEdex[ 9]\nplace new insertions temporarily to some separate area called delta\nbuffers. However, these designs also degrade lookup speeds as it\nis necessary to check across both the major learned index and the\ndelta buffer. 3) Gapped Arrays : ALEX[ 3] and LIPP[ 15] incorporate\npre-allocated gaps (i.e., empty slots) to integrate incoming key-\nvalue pairs. Typically, these in-place approaches perform better\nsince the merge-tree and delta-buffer-based methods involve ac-\ncessing multiple structures (sub-indexes or delta buffers) during a\nlookup. According to GRE, two state-of-the-art gapped-array-based\nlearned indexes, ALEX [ 3] and LIPP [ 15], generally outperform\nother existing updatable learned indexes, and we mainly focus our\ncomparisons with these two systems. Subsequent sub-sections will\ndetail the main operations of existing updatable learned indexes,\nespecially ALEX and LIPP, and discuss how our BLI design can\noffer improvements.2.2 Lookup: predicting an individual key vs. a\nkey range\nThe lookup process can typically be split into a traverse-to-leaf\nlookup , which selects the appropriate sub-key range and traverses\nto the corresponding leaf model, and a leaf lookup , which uses\nthe leaf model to predict the position of key-value pairs. Existing\nlearned indexes [ 3â€“5,9,12,14] predict the offset of a target key\nwithin the corresponding array of the model.\nUnlike most existing updatable learned indexes, our proposed\nBLI focuses on a different dimension: prediction granularity. BLI\npredicts a key range, which is represented by a Bucket rather than\nan individual key. The Bucket layout allows for efficient last-mile\nsearch by bypassing all entries from non-target Buckets. Further-\nmore, the prediction model is more linear since Buckets simplifies\nthe key distribution, which will be proved in Section 3.1.\n2.3 Insertion: strictly sorted order vs. \"globally\nsorted, locally unsorted\" order\nWe focus on in-place updatable learned indexes, especially ALEX\nand LIPP, since they generally outperform other updatable learned\nindexes. Specifically, ALEX reserves empty slots in arrays corre-\nsponding to its leaf models to facilitate insertions. When a new\nkey-value pair is inserted using a leaf model, if the model-predicted\nslot is occupied, ALEX shifts neighboring pairs to nearby gaps to\nmaintain a strict sorted order. Conversely, LIPP does not distinguish\nbetween non-leaf and leaf models, allowing the placement of key-\nvalue pairs, child pointers, and empty slots throughout any model\nin its hierarchy. During insertion, if a predicted slot is occupied,\nLIPP resorts to chaining, by creating a new child model containing\nboth the existing and new pairs, subsequently replacing the existing\npair with a pointer to this new child model.\nTo ensure high lookup and insertion performance, our proposed\nlearned index, BLI, also supports in-place insertions through pre-\nallocated gaps in arrays corresponding to their leaf models. Unlike\nALEX, which requires data insertion into strictly sorted linear ar-\nrays, BLI incorporates empty slots within each Bucket. Since keys\nwithin a Bucket are unsorted, BLI avoids the overhead associated\nwith shifting operations. Moreover, while LIPP requires chaining\nfor every occupied predicted position, BLI simply searches forward\nto find the next available empty slot, benefiting from the unsorted\nnature of Buckets.\nNotably, insertions in all updatable learned indexes necessitate\nStructure-Modification Operations (SMOs), such as model split-\nting, scaling, or retraining, triggered by capacity limits or per-\nformance degradation. Efficient execution and the trigger condi-\ntions for these SMOs are crucial for maintaining system perfor-\nmance. Moreover, the way SMOs are implemented also affects the\nthree aforementioned metrics affecting performance (i.e., the model\nfanoutğ‘, in-model lookup efficiency ğ¸lookup , and in-model insert\nefficiency(ğ¸insert ).\n2.4 Bulk loading: top-down vs. bottom-up\nBulk loading is a process that builds the whole learned index given\na sorted array of key-value pairs. Bulk loading can be implemented\nusing a top-down or bottom-up method. Given a sorted linear array\nof key-value pairs, top-down methods[ 3,15] start by generating\n3\n\nthe root model and then recursively branching into child models. In\ncontrast, bottom-up bulk loading methods [ 4,9,12] first partition\nall key-value pairs into disjoint key ranges and train leaf models for\neach key range. After that, each key range is typically represented\nby its minimum key and a pointer to this key range. The ( min key ,\npointer ) pairs propagate upwards in the same way until a single\nmodel (i.e., the root) is generated.\nBoth ALEX and LIPP adopt a top-down method. To train the root\nmodel, they transverse every key-value pair in the sorted array. The\ntraversing of the training process happens in each newly-generate\nsub-key range. As a result, they traverse the data multiple passes,\nwhich can typically be avoided in a bottom-up approach.\nBLI employs a bottom-up approach to construct the tree. In a\nsingle pass, BLI segments key-value pairs into Buckets and then\noperates on the ( min key ,pointer ) iteratively. Similar to the exist-\ning bottom-up approaches, although the upper-level models also\nneed to transverse the ( min key ,pointer ) pairs for training, the\nnumber of such pairs decreases exponentially after each iteration.\nAs a result, only a single pass of data traversal is required in BLI.\nAfter bulk loading, the BLI structure has no long or skewed paths.\n2.5 Concurrency: use locking to ensure\nconrrectness\nPrevious work has relied on locking mechanisms to support con-\ncurrency. XIndex [ 12] and FINEdex [ 9] use temporary buffers to ac-\ncommodate new key-value pairs, periodically merging the buffered\ndata into the main index structure. XIndex applies locks to indi-\nvidual records within the data array and uses a per-group lock\nfor sequential insertions. FINEdex avoids locks during reads by\nemploying version control, but it still requires locks during inser-\ntions. SALI [ 6] employs per-node write locks on LIPP. The locking\nmechanism blocks user requests in the event of read or write con-\nflicts. In contrast, thanks to the unsortedness of Buckets, BLI is\nable to support lock-free concurrency by utilizing valid bits and\nRead-Copy-Updates.\n3 MOTIVATIONAL EXPERIMENTS AND\nANALYSES\nThis section highlights some research issues associated with the\nstrictly sorted order in existing learned indexes through motiva-\ntional experiments and analyses. We examine their major opera-\ntions (including lookup and insertion) and concurrency support.\n3.1 Lookup: prediction and local search\nefficiency\nAs previously mentioned, during a lookup operation, a model\nprediction in learned indexes is typically followed by a last-mile\nsearch to correct any prediction errors. Therefore, the aforemen-\ntioned in-model lookup efficiency( ğ¸lookup ) can be expanded as\nğ¸lookup =ğ‘Ãğ‘\nğ‘–=1(ğ‘¡lookup ğ‘–)=ğ‘Ãğ‘\nğ‘–=1(ğ‘¡predict ğ‘–+ğ‘¡local search ğ‘–). It is important\nto optimize and balance both ğ‘¡predict andğ‘¡local search in order to\nmaximizeğ¸lookup .\n3.1.1 Model prediction . Existing learned indexes utilize models\nto predict the position of an individual key , whereas our proposedBLI predicts a Bucket corresponding to a key-range. In other words,\nexisting learned models typically employ key prediction models ,\nwhile our method utilizes range prediction models . We argue that\nrange prediction models result in equal or smaller errors compared\nto key prediction models. To elaborate, consider a sorted array of\nğ‘key-value pairs, where a key prediction model ğ‘š(ğ‘˜)=ğ‘ğ‘˜+ğ‘is\ntrained, with ğ‘˜as the input key, ğ‘as the slope, and ğ‘as the inter-\ncept. The parameters ğ‘andğ‘are learned from the ( key,position )\ndistribution. For a ( key,position ) pair(ğ‘˜,ğ‘), the prediction error\nfor the key prediction model can be denoted as ğ‘’=|ğ‘š(ğ‘˜)âˆ’ğ‘|.\nFollowing the \"globally sorted, locally unsorted\" design, we can par-\ntition theğ‘key-value pairs into several groups, where each group\ncontainsğ‘›adjacent key-value pairs, and the key ranges of these\ngroups are not overlapped. We then define the range prediction\nmodel asğ‘€(ğ‘˜)=ğ‘š(ğ‘˜)\nğ‘›. That is, the training process for the range\nprediction model ğ‘€(ğ‘˜)is the same as that for the key prediction\nmodelğ‘š(ğ‘˜), while the output of ğ‘€(ğ‘˜)is divided by the group size.\nConsequently, the prediction error for ğ‘€(ğ‘˜)becomesğ¸=|ğ‘š(ğ‘˜)âˆ’ğ‘|\nğ‘›.\nTherefore,ğ¸=ğ‘’\nğ‘›â‰¤ğ‘’, indicating that predicting a key range results\nin equal or smaller errors compared to predicting the position of\nan individual key. The gap between key range prediction error\nand individual prediction error is influenced by the group size or\nBucket size in our context. A larger group or Bucket size can lead to\nreduced prediction errors. However, the Bucket size also affects the\nefficiency of the subsequent local search step, presenting a tradeoff\nbetween model prediction efficiency and local search efficiency.\nTo illustrate the impact of group or Bucket size on prediction\nerrors, we conducted an experiment to show how prediction errors\ncan be reduced. We trained a linear regression model on the first\n10000 keys of the datasets books ,fb, and osmwith various group\nsizes. As shown in Figure 1, we observed a significant reduction in\nthe average prediction error as the group size increased.\nFigure 1: The average prediction error with different group\nsizes on the first 10000 keys of books ,fb, and osm.\n3.1.2 Last-mile search. Existing learned indexes typically conduct\nlocal searches with the following three methods: 1) binary search\nwith error bound[ 4,8,9], 2) exponential search with model-based\ninsertion[ 3], and 3) enforced precise predictions with chaining [ 15].\nSpecifically, some learned indexes using the first approach enforce\nthe maximum prediction error for each learned model and will\nemploy more learned models if the current ones cannot ensure the\nbounded error. The error bound is a predefined parameter of the\nsystem. After a model prediction, these learned index systems will\nperform a binary search on the neighboring key-value pairs within\n4\n\nthe error bound. ALEX, using the second method, introduces model-\nbased insertion. It tries to insert a key-value pair to the model-\npredicted position if it has not been occupied. During lookup, if the\npredicted position contains a non-target key-value pair, it conducts\nan exponential search starting from the predicted location without\nbounded error, which could terminate at the end of the sorted array.\nLIPP, using the third method, also tries to insert the key-value\npairs into the model-predicted positions, and it enforces precise\nprediction by conducting chaining operations of prediction models\nwhenever there are prediction conflicts. As a result, LIPP eliminates\nthe need to search its neighboring positions. On the other hand, it\ncould result in a long chain of prediction models. Unlike existing\napproaches, our proposed BLI bounds its local searches within each\nBucket.\n3.2 Insertion: overhead due to\nstrict-order-preserving operations\nExisting updatable learned indexes typically utilize strict-order-\npreserving operations to keep the key-value pair array sortedWe\nvalidate the negative effect of the extra operations by analyzing the\ntime breakdown of ALEX and LIPP under an insert-only workload\non three popular benchmarking datasets: books ,fb, and osm. As\nobserved, after the lookup process, ALEX spends 58.60% of its time\non shifting neighbors on average, while LIPP spends 32.24% on\naverage for chaining operations. These operations could be elim-\ninated in our proposed \"glocally sorted, locally unsorted\" bucket\ndesign. On the other hand, the bucket design brings new tradeoffs\nthat must be carefully handled.\n3.3 Concurrency\nAmong learned indexes, FINEdex, XIndex, and PGM-Index support\nconcurrency but require the use of delta buffers or merge trees. This\nreliance on multiple structures, such as delta buffers or sub-learned\nindexes, reduces lookup performance and introduces challenges\nin read-write workloads. Other state-of-the-art learned indexes,\nincluding ALEX and LIPP, need costly locking mechanisms to en-\nsure data consistency in their strict-order layouts when enabling\nconcurrency. For example, GRE implemented multi-threaded ver-\nsions of ALEX and LIPP, known as ALEX+ and LIPP+, respectively,\nusing locks. To evaluate the lock overhead, we compared the per-\nformance of ALEX with ALEX+ and LIPP with LIPP+. We began\nby bulk-loading 100 million key-value pairs and then executed 50\nmillion random reads and 50 million random writes to measure\nthroughput. Under a single thread environment with the dataset\nbooks , ALEX+ exhibited a 20.10% performance degradation, while\nLIPP+ showed a 26.75% degradation compared to their original\nversions.\n4 BLI ARCHITECTURE\nThis section presents the proposed BLI architecture derived from\nthe \"globally sorted, locally unsorted\" design. BLI consists of sorted\nlinear arrays of Buckets that contain unsorted data. These Buckets\nenhance insert efficiency while introducing new tradeoffs.4.1 Architecture overview\nExisting learned indexes typically use a hierarchy of machine-\nlearning models to make \"good enough\" predictions. The lookup\nprocess starts at the root model, which covers the entire key range,\nand each model predicts the position of the next child model, nar-\nrowing the key range progressively.\nâ€¦â€¦â€¦SegmentD-BucketKVKVKVDKVKVDKVKVKVDKVKVDKVKVKVKVSegmentSegmentSegmentSegmentâ€¦â€¦keyrange=[500, 11000)\nDKVKVKVDKVKV2000Empty slotEmpty slotEmpty slot2187Empty slotD-Bucket\nD-Bucketkeyrange=[30, 54000)Model: mkey=pointer\tpositionSegment ğ’Šâ€¦Model: mkey=pointer\tpositionSegment ğ’‹â€¦â€¦â€¦â€¦â€¦keyrange=[2000, 6000)\n2586Empty slotâ€¦\nD-Bucket5700â€¦D-Bucketk\nKV for key=2187Empty slotEmpty slotKV for key=2333searchorder\t=slot#(hint)â†’#(hint+1) â†’â€¦ =slot#1â†’#2 â†’#3 â†’#0hint2333=2333%4=slot#1Hint: â„key=slot\tpositionslot#0\tslot#1\tslot#2slot#3[2187, 2586)â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦\nsorted pointersâ€¦â€¦â€¦â€¦\nFigure 2: An example of the BLI architecture, illustrating a\nthree-level Segment and the bottom-level D-Bucket structure,\nemploying the MOD hash as a hint function.\nAdopting this hierarchical design, our proposed BLI features a\nsorted linear array of Buckets containing unsorted data. Figure 2\nillustrates an example of the BLI architecture with three levels of\nSegments , with the potential for more levels if additional data are\npresent. A Segment spans a specific key range and includes a learned\nprediction model along with a sorted linear array of pointers to its\nchild Segments, each responsible for a non-overlapping sub-key\nrange. The learned model predicts the position of the child Segment\nbased on a target key, and BLI searches nearby if the prediction has\nan error. At the last level, BLI organizes all key-value pairs within\neach sub-key range into a group called a Data Bucket orD-Bucket ,\nwhere the key-value pairs are unsorted, but their positions will be\nsuggested by a hint function. Empty slots can exist in segments and\nD-Buckets to facilitate future insertions.\nInternal and last-level Segments share similar structures. The key\ndifference is that internal Segments, such as Segment ğ‘–in Figure 2,\npoint to child Segments, whereas last-level Segments, like Segment\nğ‘—, point to D-Buckets. For instance, Segment ğ‘—, responsible for\nthe key range[2000,6000), comprises a model ğ‘š(key)and sorted\npointers covering its sub-key ranges: [2000,2187),[2187,2586),Â·Â·Â·,\n[5700,6000). Specifically, D-Bucket ğ‘˜, handling the sub-key range\n[2187,2586), contains two key-value pairs and two empty slots.\n5\n\nThe key-value pairs with keys 2187 and2333 are placed in their\nrespective hinted slots 1and3using a MOD hash â„(key)=key%4.\nWhen looking up the key 2333 , BLI starts at the root Segment,\nwhich covers the entire key range [30,54000). The root Segment\nuses its prediction model to identify the target child Segment that\nspans the key range [500,11000), and then[2000,6000). This pro-\ncess is repeated in Segment ğ‘—, and BLI ultimately reaches D-Bucket\nğ‘˜. Once there, BLI applies the hint function hint(2333)=slot1as\nthe starting search position. It continues searching through slots\n2,3, and 0until the target key-value pair is found if slot1 does not\nhold the key or all slots have been checked. In this example, slot 1\nis the correct position, so no further searching is required.\n4.2 Analysis of D-Buckets\nThere are three design factors for D-Buckets: the size of D-Buckets,\ntheir fill ratio, and the hint design. These factors will influence the\nkey aforementioned metrics: model fanouts ( ğ‘), in-model lookup\nefficiency (ğ¸lookup ), in-model insert efficiency ( ğ¸insert ), and memory\noverhead (ğ‘‚mem). To fully optimize BLI, these factors should be\nbalanced according to workload patterns (e.g., read-write ratio) and\nthe systemâ€™s memory capacity.\n4.2.1 D-Bucket size and the initial fill ratio. D-Bucket size refers to\nthe number of slots within a D-Bucket, while the fill ratio represents\nthe ratio between the number of existing key-value pairs and the\nD-Bucket size. For example, D-Bucket ğ‘˜in Figure 2 has four slots\nholding two key-value pairs, giving it a D-Bucket size of 4 and a fill\nratio of 50%. We pre-define the initial fill ratio as the fill ratio when\ncreating a new D-Bucket. The number of key-value pairs BLI will\nput in a newly created D-Bucket is equal to the Bucket size times the\npre-defined initial fill ratio. For simplicity, the current BLI design\nfixes the D-Bucket size across the entire system. However, using\nvariable D-Bucket sizes may lead to a more linear key distribution.\nThe D-Bucket size influences the system in the following ways:\nâ€¢With a larger D-Bucket, more key-value pairs can be placed\ninto a single D-Bucket, which with the same model fanouts\n(ğ‘) will reduce the number of levels.\nâ€¢With a larger D-Bucket, the last mile search tail latency can\nbe negatively affected.\nâ€¢A larger D-Bucket may also increase the search time of a\nrange query since the key-value pairs are not sorted in a\nbucket.\nâ€¢However, as shown in Section 3.1.1, a larger D-Bucket size\ncan simplify data distribution, reduce prediction errors, and\npotentially improve lookup efficiency ( ğ¸lookup ).\nA higher initial fill ratio can reduce memory overhead, but it\nmay only be suitable for read-most workloads. For insert-intensive\nworkloads, maintaining the same D-Bucket size with a higher initial\nfill ratio results in fewer empty slots, potentially leading to longer\nsearch distances and more frequent D-Bucket splits.\n4.2.2 The hint design. To accelerate the in-Bucket lookup process,\nBLI introduces Hint-assisted in-Bucket inserts (H-inserts) andHint-\nassisted in-Bucket lookups (H-lookups) using hint functions . A hint\nfunction suggests a preferred slot for the key to be looked up or\ninserted. One possible hit is to use a simple hash function. During\nan H-insert, if the hint-suggested slot is occupied, BLI sequentiallychecks from that slot until an empty one is found. Similarly, an\nH-lookup follows this sequential search if the suggested slot is\noccupied. Figure 2 shows an example hint function â„(ğ‘˜)=ğ‘˜%ğ¶,\nwhereğ‘˜is the input key and ğ¶is the number of slots in a D-Bucket.\nBoth the \"model\" concept in learned indexes and our proposed\nhint functions map keys to their potential positions. The primary\ndifferences between them are as follows:\nâ€¢Hint functions are not required to be monotonic since no\nenforced sorted order is needed.\nâ€¢Hint functions may not be derived from data distribution\nlearning; instead, pre-defined hint functions like MOD hash,\nmurmur hash, or CL hash can be used.\nâ€¢If the hinted slot is occupied by a non-target key-value pair,\nBLI continues searching forward, and any available empty\nslot can be used for insertion.\nCurrently, BLI employs a simple hash function as the hint be-\ncause they result in short \"searching forward\" distances in our\nevaluation. However, a hint design with a bounded search distance\nmay further improve BLIâ€™s efficiency.\n4.3 S-Buckets: adding unsortedness in Segments\nâ€¦DDD-BucketD-BucketSegment ğ’‹\nD-Bucket\nKV for key=2187Empty slotEmpty slotKV for key=2333Hint: â„key=slot\tpositionslot#0\tslot#1\tslot#2slot#3[2187, 2586)\nunsortedpointersS-Bucket\n25862000Empty slotEmptyslot2187S-Bucketâ€¦Model: ğ‘€key=Bucket\tpositionDDS-Bucket[2000, 3000)â€¦â€¦â€¦keyrange=[2000, 6000)\nFigure 3: Structure of an S-Bucket\nOrganizing a sorted linear array of key-value pairs into D-Buckets\nintroduces new opportunities and trade-offs. In BLI, each Segment\nalso uses a sorted linear array to place child pointers. This raises\nthe question of whether replacing these sorted child pointers with\nBuckets would also be advantageous. If so, what new design con-\nsiderations arise?\nWe propose replacing the child pointer array with a list of Seg-\nment Buckets orS-Buckets , as shown in Figure 3. S-Buckets within a\nSegment are sorted by key range, while the child pointers within an\nS-Bucket remain unsorted. Empty slots within an S-Bucket allow\nfor future key range updates. Similar to D-Buckets, we define the\ninitial fill ratio as the fill ratio when generating a new S-Bucket.\nThe number of S-Buckets for a Segment depends on the number of\nchild pointers, the initial fill ratio, and the number of slots within\nan S-Bucket. Consequently, the learned model should predict the\nposition of an S-Bucket rather than a single child pointer. BLI can\nthen identify the target S-Bucket near the predicted location and\nsearch for the expected child pointer within that S-Bucket.\n6\n\n4.3.1 The hint design. Similar to D-Buckets, introducing unsort-\nedness in S-Buckets could improve insertion efficiency, while the\nlookup efficiency must be maintained in some way. However, a\nfundamental difference exists: a child pointer corresponds to a key\nrange, whereas a key-value pair represents a single data point. If\nwe want to support hints for S-Buckets, we should ensure that all\nkeys within the key range of an S-Bucket are suggested to posi-\ntions close to each other or \"clustered.\" In other words, the hint\nfunction should be somewhat order-preserving. As a result, the\naforementioned hash functions are no longer feasible.\nWe propose three possible designs for S-Buckets to improve\nlookup efficiency:\n(1)Linear regression hint. We can utilize a linear regression\nformula as a hint for each S-Bucket. Unlike learned regres-\nsion models, when the expected slot is occupied during\ninsertion, linear regression hints do not enforce order and\ninstead place the child pointer in the next available slot.\n(2)HOPE hint. HOPE [ 18] is a High-speed Order-Preserving\nEncoder, a fast dictionary-based compressor that encodes\narbitrary keys while preserving their order.\n(3)S-Bucket size cap. Rather than using hints, we can place the\nentry in the first available slot during insertion and cap the\nS-Bucket size to ensure fast lookup.\nLinear regression hints can become less \"order-preserving\" with\nmore insertions, while HOPE introduces additional retraining over-\nhead for new insertions. Designing hints for key ranges is more\nchallenging than for single data points. In BLI, we adopt the third\ndesign because it is easy to implement and maintain.\nUnder the \"S-Bucket size cap\" choice, there are two possible\nimplementations to represent a key range: 1) Record both the mini-\nmum and maximum key along with each child pointer; 2) Record the\nminimum key as the representative along with each child pointer.\nWhen looking up a key, the latter implementation requires visiting\nall representatives to determine the target (i.e., the largest repre-\nsentative smaller than or equal to the lookup key). In contrast, the\nformer can stop early at the pointer whose minimum and maximum\nkeys cover the lookup key. However, updating the maximum key\nof an S-Bucket could propagate to the Segment it belongs to and\nall parent Segments upwards. For ease of maintenance, BLI records\nonly the minimum key.\n4.3.2 S-Bucket size and the initial fill ratio. Currently, BLI caps the\nS-Bucket size to one or two cache lines. For instance, if each entryâ€™s\nminimum key is 8 bytes, each pointer is 8 bytes, and one cache line\nis 64 bytes, then the S-Bucket size would be capped at 8 or 16 entries.\nSimilar to D-Buckets, larger S-Bucket sizes can result in greater\nmodel fanouts ( ğ‘) and reduced prediction errors. However, larger\nS-Buckets could also degrade lookup performance. Regarding the\nfill ratio, its influence is similar to that on D-Buckets and should be\ndetermined by workload patterns.\n5 BLI OPERATIONS\nIn this section, we will discuss the major operations in BLI, including\nlookup, insert, bulk loading, and concurrency support.5.1 Lookup and range query\nA series of root-to-leaf segment lookups are used to look up a key\nin BLI, followed by a D-Bucket exact-match lookup.\n5.1.1 Segment lookup: Bucket-based prediction. The lookup oper-\nation in BLI starts at the root Segment. Within the Segment, the\nlinear regression model is employed to predict the target S-Bucket\nrather than an individual slot. This linear model has errors. Thanks\nto segmentation-aware splitting, as we will discuss in the SMO\nsection 5.3, the prediction errors are typically small. Moreover, as\nwe have demonstrated in Section 3.1, because models predict on\nBucket ID rather than the individual slot, the model simplicity is\nable to be tuned by S-Bucket size. With a simplified model, node\nfanouts can be further enlarged.\nStarting from the model-predicted S-Bucket, the process then\nsearches among the neighboring S-Buckets to identify the one with\nthe largest S-Bucket pivot that is smaller than or equal to the given\nlookup key. Upon identifying the target S-Bucket, an in-Bucket\nlookup is performed to find the entry with the largest pivot that\nremains smaller than or equal to the lookup key (i.e., the lower-\nbound key). Following the pointer of the target entry, the process\nproceeds to the next level. This procedure is iteratively performed\nat each level until the leaf D-Bucket is reached.\n5.1.2 D-Bucket lookup: H-Lookup . D-Bucket lookups diverge from\nS-Bucket lookups, as they do not necessitate the ordering of entries.\nBy capitalizing on this, a hint function can be utilized in D-Buckets,\nwhich serves as the starting search position. The choice of hint\nfunctions determines the entry distribution within a D-Bucket and\nthus affects the hint search distances (i.e., the number of slots visited\nstarting from the hint slot). We evaluated different hint choices,\nincluding linear model, MOD hash, and CL hash hints.\nEndpoint Linear Models To start with, the hint function can\nbe defined as a linear model:\nâ„(ğ‘˜)=ğ‘ğ‘˜+ğ‘\nIn this formula, ğ‘is the slope, and ğ‘is the intercept. As we use\nhints to accelerate in-Bucket search, the overhead of computing\nthe hint must be small. The endpoint linear model is computed in\nğ‘‚(1)time, using the pivot of both the target D-Bucket and its right\nneighboring D-Bucket.\nHash hints Using endpoint linear models almost introduces no\nadditional computing overhead, but its efficiency highly depends\non the key distribution within the D-Bucket. The linear model can\nhave more conflicts when the local key distribution is less linear\n(e.g., it has a smaller Pearson correlation coefficient).\nTherefore, we use hash functions to randomize the original key\ndistribution and reduce the number of conflicts. An ideal hash\nfunction has the following properties: low latency, even distribution,\nand low collision rate [ 16]. This paper evaluated three different hash\nfunctions, including MOD and CL hash. Any other hash functions\nor hint algorithms can be smoothly applied in the D-Bucket without\nmodifying other components in BLI.\nThe simplest hash function we evaluated is MOD hash, â„(ğ‘˜)=\nğ‘˜%ğ¶, whereğ¶is the D-Bucket size. It has low computing overhead.\nOn the other hand, if the keys have similar reminders after being\n7\n\ndivided byğ¶, there will be many conflicts within the D-Bucket. If the\nkey distribution is linear enough (i.e., has a small linear regression\nerror), it can have a high chance of having this \"similar reminder\"\ndata distribution. In fact, we observed many conflicts in one of the\nsimple datasets called covid . Besides, CL hash is designed to run\nextremely fast on modern processors equipped with the Carry-less\nMultiplication (CLMUL) instruction set.\nWe evaluated two hashing functions and found that overall, CL\nhash was able to randomize the data distributions, as shown in\nFigure 4. Note that MOD hash has almost no improvement when\ncompared with the no hint case on covid , which is an easy and\nlinear dataset.\nFigure 4: Throughput under different hint choices with the\nread-only workload.\n5.1.3 Range query. For range queries, BLI starts by identifying\nthe target D-Bucket of the start key, and continues visiting the\nsubsequent D-Buckets untils the number of visited key-value pairs\nreaches the required amount. After that, BLI use multiple thread\nworkers to copy the key-value pairs in each D-Bucket to a temporate\nbuffer, and sort these key-value pairs in parallel.\nGiven a query of ğ‘›key-value pairs, the number of D-Buckets\ninvolvedğ‘isğ‘‚(ğ‘›\nğ‘¤ğ¶)=ğ‘‚(ğ‘›), whereğ‘¤represent the number of\nthread workers, and ğ¶represents the D-Bucket size. For each D-\nBucket, the sorting takes ğ‘‚(ğ¶logğ¶)=ğ‘‚(1)time. Therefore, the\namortized in-Bucket query cost is ğ‘‚(ğ‘Â·1+ğ‘›\nğ‘›)=ğ‘‚(1). On the other\nhand, D-Bucket size can affect the tail latency as it takes a longer\ntime to create or sort a larger D-Bucket.\n5.2 Bottom-up insert and simple SMOs\nWithin BLI, insertions occur in a bottom-up fashion, and they may\ntrigger SMOs, including D-Bucket split, S-Bucket split, Segment\nsplit, Segment merging, Segment scaling, and model retraining.\nWe will first discuss the insertion into a non-full D-Bucket, which\ndoesnâ€™t necessitate any SMOs. Following that, we will introduce the\nD-Bucket split first and then the rest of the SMOs. It is worth noting\nthat the rest of the SMOs in BLI are combined in one operation and\nexecuted in linear time with respect to the number of entries in the\nS-Bucket, which is highly efficient.5.2.1 Insert in non-full D-Bucket: H-insert. In order to insert a key, a\nlookup is initially performed to identify the target D-Bucket whose\nkey range contains this key. If the target D-Bucket is not filled to\nits capacity, the new key-value pair is inserted into this D-Bucket.\nSimilar to a lookup, BLI first checks whether the hinted slot is\nalready occupied. If not, it continues forward to locate the first\nempty slot.\nThe unsorted nature of D-Buckets is highly efficient because\nit eliminates the need for strict-order-preserving operations (e.g.,\nshifting in ALEX and chaining in LIPP). First, no shifting is required\nsince any empty slots can be used for insertion. Second, BLI does\nnot need chaining because the Buckets can act as a buffer to delay\nconflicts. During insertion in LIPP, the incoming key-value pair is\ninserted into the model-predicted slot if it is empty. However, if the\nsingle predicted slot is already occupied, LIPP performs chaining\nto accommodate the new key-value pair in a new node. In contrast,\nBLI does not stick to a specific slot for insertion. Only after all empty\nslots within a D-Bucket are occupied does BLI begin to perform a\n\"conflict-resolving\" operation, which we call the D-Bucket split.\n5.2.2 D-Bucket split . When the D-Bucket targeted for insertion is\nfull, it is evenly split into two distinct D-Buckets, with each new\nD-Bucket accommodating half of the key-value pairs. This process\nis executed in linear time with respect to the number of entries in\nthe D-Bucket. We employ Hoareâ€™s quick selection algorithm[ 7] to\nidentify the median key. Subsequently, the key-value pairs with\nkeys less than or equal to the median key are placed in the first\nnewly created D-Bucket, while the rest of the key-value pairs are\nallocated to the second D-Bucket. In addition, the new key-value\npair will be inserted into one of the two new D-Buckets.\n5.3 Combined SMOs in linear Time\nAfter the D-Bucket split, the ( key,pointer ) pair representing the\nnewly generated D-Buckets needs to be inserted into their parent\nSegment. This insertion can trigger additional SMOs, including Seg-\nment scaling, Segment split, Segment merging, and model retrain-\ning.Segment scaling involves enlarging the Segment capacity while\nmaintaining the current model. Segment split divides a Segment\ninto multiple ones when the key distribution within the Segment is\nno longer linear enough, necessitating re-segmentation. Segment\nmerging combines neighboring segments that fit well into a single\nlinear model. Model retraining relearns the distribution of entries\nin the current Segment.\nInitially, the \"current Segment\" refers to the parent Segment of\nthe old D-Buckets being split. Additional SMOs may also propagate\nupwards, affecting the parent of the current Segment. In the rest\npart of this section, we will focus on a single SMO occurring within\nthe current Segment.\nThe trigger conditions for these SMOs must be carefully designed.\nWe classify them into passive SMOs and active SMOs. Passive SMOs\nare triggered due to capacity limits when a D-Bucket, S-Bucket, or\nSegment is full. Conversely, active SMOs may be triggered due to\nperformance degradation. For example, if the current Segment is\nnot linear enough and has large prediction errors, model retraining,\nSegment split, or Segment scaling can be performed to address\nthis issue. If the overall distribution of neighboring D-Buckets is\nlinear, Segment merging may be initiated. The trigger conditions\n8\n\ndetermine the SMO triggering frequency. If SMOs are triggered\ntoo frequently, it interferes with the normal lookup and insert\noperations; if SMOs are triggered too rarely, the Segment layout\nmay be suboptimal. BLI triggers one big SMO when an S-Bucket\nis full, which makes the SMO trigger less frequently than when a\nD-Bucket is full and more frequently than when a Segment is full.\nNotably, the one big SMO efficiently combines Segment scaling,\nSegment split, model retraining, and neighbor Segment merging\ntogether.\nAlgorithm 1: Combined SMO for node scaling, model re-\ntraining, node split, and node merge\nInput : Current segment ğµ, new entries ğ¸in\nOutput: Entries of newly generated segments ğ¸out\n1ifaverage number of SMOs for current bucket among\nneighboring segments <ğœƒthen\n2 ReSegment( ğµ,ğ¸in);\n3else\n4 MergeNeighbors( ğµ,ğ¸in);\nAlgorithm 2: ReSegment Function\n1ReSegment( ğµ,ğ¸in)keysâ†MergeEntries( ğµ.entries,ğ¸in);\n2(ğ¶1,ğ¶2,...,ğ¶ğ‘›)â† Segmentation(keys);\n3forğ‘–â†1toğ‘›do\n4 segmentğ‘–â†ğ¶ğ‘–.model + list ofl|ğ¶ğ‘–.keys|\nfillRatiom\nempty\nS-Buckets;\n5 foreach entryğ‘¥âˆˆğ¶ğ‘–do\n6 segmentğ‘–.ModelInsert( ğ‘¥);\nAlgorithm 3: MergeNeighbors Function\n1MergeNeighbors( ğµ,ğ¸in)leftBoundaryâ†ğµ;\n2while leftBoundary is bounded by GreedyCorridor do\n3 leftBoundaryâ†left neighboring segment of\nleftBoundary;\n4rightBoundaryâ†ğµ;\n5while rightBoundary is bounded by GreedyCorridor do\n6 rightBoundaryâ†right neighboring segment of\nrightBoundary;\n7oldLCAâ†least common ancestor of leftBoundary and\nrightBoundary;\n8pivotsâ†all D-Bucket pivots of the LCA sub-tree âˆªğ¸in;\n9newLCAâ†bottomUpPropagate(pivots);\n10oldLCAâ†newLCA;Function MergeNeighbors( ğµ):\nğ‘™ğ‘’ğ‘“ğ‘¡ğµğ‘œğ‘¢ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘¦â†ğµ;\nwhileğ‘™ğ‘’ğ‘“ğ‘¡ğµğ‘œğ‘¢ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘¦ is bounded by ğºğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ğ¶ğ‘œğ‘Ÿğ‘Ÿğ‘–ğ‘‘ğ‘œğ‘Ÿ do\nğ‘™ğ‘’ğ‘“ğ‘¡ğµğ‘œğ‘¢ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘¦â†the left neighboring Segment of ğ‘™ğ‘’ğ‘“ğ‘¡ğµğ‘œğ‘¢ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘¦ ;\nğ‘Ÿğ‘–ğ‘”â„ğ‘¡ğµğ‘œğ‘¢ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘¦â†ğµ;\nwhileğ‘Ÿğ‘–ğ‘”â„ğ‘¡ğµğ‘œğ‘¢ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘¦ is bounded by ğºğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ğ¶ğ‘œğ‘Ÿğ‘Ÿğ‘–ğ‘‘ğ‘œğ‘Ÿ do\nğ‘Ÿğ‘–ğ‘”â„ğ‘¡ğµğ‘œğ‘¢ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘¦â†the right neighboring Segment of\nğ‘Ÿğ‘–ğ‘”â„ğ‘¡ğµğ‘œğ‘¢ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘¦ ;\nğ‘œğ‘™ğ‘‘ğ¿ğ¶ğ´â†the least common ancestor of ğ‘™ğ‘’ğ‘ğ‘¡ğµğ‘œğ‘¢ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘¦ and\nğ‘Ÿğ‘–ğ‘”â„ğ‘¡ğµğ‘œğ‘¢ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘¦ ;\nğ‘ğ‘–ğ‘£ğ‘œğ‘¡ğ‘ â†all D-Bucket pivots of the LCA sub-tree and also ğ¸in;\nğ‘›ğ‘’ğ‘¤ğ¿ğ¶ğ´â†bottomUpPropagate(keys);\nğ‘œğ‘™ğ‘‘ğ¿ğ¶ğ´â†ğ‘›ğ‘’ğ‘¤ğ¿ğ¶ğ´ ;\nWe categorize the aforementioned SMOs into two groups: Seg-\nment scaling, model retraining, and Segment split are bundled in\nthe function ReSegment() , as they may be triggered when one\nSegment is not linear enough; MergeNeighbors() constitutes the\nsecond category, focusing on simplifying the node layout. As shown\nin Line 1 of Algorithm 1, we use an indicator to decide whether\nto trigger ReSegment() orMergeNeighbors() . This indicator is\nbased on the average number of SMOs for the current Segment\nto be processed among its neighboring Segments. If the average\nnumber of SMOs is higher than a threshold, we proactively trigger\nnode merging. This indicator can be easily replaced by other factors,\nand we can set different indicators to control the frequency of node\nmerging.\n5.3.1 Node scaling, model retraining, and node split. ReSegment\ntakes the current segment ğµand new entries ğ¸inas input. The new\nentries are key-pointer pairs for the two new D-Buckets, and the\ncurrent segment is the parent segment of these D-Buckets. First, it\nmerges the pivots of ğ¸inand the entries from all S-Buckets into a\nsingle sorted array keys using merge-sort in linear time. Then, a\nsingle-pass greedy segmentation algorithm GreedyCorridor [10]\nis performed on keys . Based on the segmentation result, the current\nsegment can either split into multiple segments or remain as a\nsingle segment. Finally, it generates new segments according to the\npredefined fill ratio (e.g., 60% full), during which node scaling and\nmodel retraining occur.\nThe outputs of a ReSegment() are the new entries, each repre-\nsenting a newly generated Segment. The number of new entries or\nSegments is greater than or equal to one. These new entries will be\npropagated to the upper level to be inserted. The insertion process\ncan also trigger SMOs if there are not enough empty slots at the\nparent Segment level. The propagation continues until it can be\ninserted at a level or until it reaches the root. If there is still more\nthan one Segment at the root level, a new level will be generated\nand set as the new root.\n5.3.2 Node merging. InMergeNeighbors() , BLI needs to identify\nwhich D-Buckets are to be merged in a lightweight manner. Our\nproposed identification algorithm grows the Segment incremen-\ntally, and it works as follows: at the leaf-node level, we start at the\nnewly split D-Bucket and expand in both left and right directions\nto check whether the neighbor D-Buckets can be merging candi-\ndates. We terminate if we reach a D-Bucket that is not supposed to\nbe the same Segment. We reuse the Greedycorridor segmentation\n9\n\nalgorithm, with the same predefined error bound as in bulk load-\ning and node split phase, to determine if the neighbor D-Bucket\ncan fit into the same Segment. There are three major reasons to\nchoose Greedycorridor as the node merging algorithm: 1) the time\ncomplexity of Greedycorridor is linear regarding the number of\nneighbor D-Bucket visited, which is lightweight; 2) Greedycorri-\ndor works by sequentially transverse each key and stops when\nthe slope change is too much. During the linear transversal, we\ncan incrementally update the linear model; 3) It makes the node\nsplitting and merging algorithms consistent with each other, with\nthe same hyper-parameter required (i.e., the error bound), which\nmakes it easier to tune BLI performance. After identifying the left\nand right boundary D-Buckets, BLI finds their least common ances-\ntorğ‘œğ‘™ğ‘‘ğ¿ğ¶ğ´ . After re-building the sub-tree whose root is ğ‘œğ‘™ğ‘‘ğ¿ğ¶ğ´\nin a bottom-up manner, ğ‘œğ‘™ğ‘‘ğ¿ğ¶ğ´ is finally replaced by ğ‘›ğ‘’ğ‘¤ğ¿ğ¶ğ´ .\nSimilar to bulk loading, this bottom-up propagation traverses the\nD-Buckets in a single pass, which is highly efficient.\n5.4 Bottom-up single-pass bulk loading\nBLI also supports an efficient bottom-up bulk-loading algorithm.\nGiven a sorted array of key-value pairs, BLI partitions all key-value\npairs evenly into Buckets in a single pass. Based on the initial fill\nratio and Bucket size, we compute the number of key-value pairs in\neach Bucket as Bucket size times the initial fill ratio. After that, BLI\nput adjacent key-value pairs into Buckets accordingly. However, if\nwe vary Bucket size or the initial fill ratio among different Buckets,\nit is possible to achieve a more linear key distribution. Similar to the\nexisting bottom-up approaches, the subset size at each upper level\ndiminishes exponentially. After bulk loading, the BLI structure is a\nbalanced tree without long and skewed paths.\n5.5 Lock-free concurrency support\nWe support lock-free single-producer-multi-consumer (SPMC) con-\ncurrency, where one dedicated thread can insert key-value pairs\ninto the index while not blocking lookup from other threads. Meta\nclaimed that the read-to-write ratio in their KV cache is 30:1[ 1],\nwhich matches well with our design, which has one writer and\nseveral readers. Moreover, the SPMC scenarios are especially useful\nfor sharded key-value stores[ 2]. Sharding is the mechanism that\nsplits the key range into multiple partitions (or shards). In each\nshard, to employ BLI under the SPMC scenario, we can have one\ndedicated writer to handle the insert process while all readers from\nother sessions can run in parallel without blocking.\nBLI supports SPMC in a lock-free manner. This can be achieved\nby 1) using a valid bit for each entry with a specifically designed\nupdate order and 2) using a Read-Copy-Update (RCU) for all SMOs.\nMore specifically, to ensure data consistency, updating the entryâ€™s\nvalid bit happens after the data insertions. When inserting an entry\ninto a non-full node, BLI first inserts it at an empty slot, then sets\nits valid bit to 1. If the inserted entry is the smallest one of the\nBucket, BLI will update the Bucketâ€™s pivot as the last step. For\nSMOs, including the D-Bucket split and the combined SMO, we use\nRCU to avoid locking.\nFigure 5 illustrates the RCU process for D-Bucket splits. In step\n1, BLI reads all key-value pairs from the old D-Bucket (D-Bucket1)\nto be split, which includes the key-value pairs with keys 5, 3, 2, and8, along with the new key-value pair to be inserted (key = 7). In\nstep 2, BLI calculates the median key from this set of key-value\npairs, which is 5. All key-value pairs (including the new key-value\npair to be inserted) with keys less than or equal to 5 are copied into\nthe first new D-Bucket (D-Bucket2), while the remaining key-value\npairs are copied into the second new D-Bucket (D-Bucket3). In step\n3, the key-pointer pairs for the two new D-Buckets (i.e., pair p2\nfor key = 2, address = address1, and pair p3 for key = 5, address\n= address3) are inserted into the parent Segment. In this example,\nsince there are empty slots available to store the new key-pointer\npairs, no further SMOs are triggered. However, if the new key-\npointer pairs cause an SMO due to capacity limits in the Segment,\nBLI will invalidate the key-pointer pair to the old Bucket only after\nall SMOs and propagation (e.g., ReSegment() or MergeNeighbors()\nfor the affected Segments at different levels) are complete. Other\nSMOs follow similar RCU procedures with the process for D-Bucket\nsplits.\nD-Bucket2\nkey=5\nkey=3\nkey=20\n1\n1\n0p2(key=2, address2)\nD-Bucket3\nkey=7\nkey=81\n1\n0\n1p3(key=7, address3)â‘  READ  oldKVs and the new KV â‘¡ COPY  allKVs intonew D -Buckets\nâ‘¢ UPDATE pointers intheparent Segment\nModel\nâ€¦ â€¦ â€¦ â€¦ (2,p1) 1 0 0\nModel\nâ€¦ â€¦ â€¦ â€¦ (2,p1) 0 (2,p2) 1 (7,p3) 1D-Bucket1\nkey=5\nkey=3\nkey=2\nkey=81\n1\n1\n1p1(key=2, address1)\nkey=7\nFigure 5: The RCU process for D-Bucket splits in BLI.\n6 EVALUATION\nIn this section, we first compared the performance of BLI with\nseveral state-of-art in-memory learned indexes, including ALEX[ 3],\nLIPP[ 15], PGM[ 4], and FINDex[ 9]. ALEX and LIPP are designed for\na single-core machine and did not initially work in a multi-thread\nenvironment. Thus, we used the implementation from [ 14]. The\nconcurrent versions are denoted as ALEX+ and LIPP+. After that,\nwe adopted drill-down analyses on different factors to validate the\neffectiveness and efficiency of our design components.\n6.1 Evaluation set up\nWe implemented BLI in C++ and performed evaluations on an\nUbuntu Linux machine equipped with an Intel(R) Xeon(R) CPU\nE5-2670 v3 @ 2.30GHz and 128GB RAM. We used 8B keys and 8B\npayloads in all our experiments. To have a comprehensive evalua-\ntion, we compare the following metrics: 1) The overall lookup/insert\nperformance, 2) The time breakdown of lookup/insert operations,\nand 3) The memory consumption.\n10\n\nThe dataset we used followed the GRE benchmarking paper[ 14].\nWe chose books ,fb, and osmdatasets as representatives because\nthey represent three different \"hardness\" (a measurement of how\nlinear they are) patterns defined in GRE. They are identified as easy,\nmedium, and hard datasets, respectively. Regarding the workload, to\ninvestigate different scenarios comprehensively, we studied various\nread-write ratios from 0:1, 1:9, 2:8, ... to 1:0, which covers read-only,\ninsert-only, read-write ( read :write =1 : 1), and read-write cases.\nGiven that each dataset we used consists of 200M keys in total, we\nrandomly bulk-loaded 100M key-value pairs and performed the rest\nof the 100M read or insert operations according to the read-insert\nratio.\nFigure 6: Throughput under different read-write ratios.\n6.2 Overall throughput comparison\nFigure 6 compares the throughputs of SOTA learned indexes and BLI\nacross various read-write ratios. BLI outperformed other learned\nindexes in most workloads and datasets. As shown in the figure,\nPGM achieves the highest throughput in insert-only workloads be-\ncause it accommodates insertions similarly to LSM-tree compaction,\nand the primary advantage does not stem from the learned index\ndesign itself [ 15]. Notably, the figure illustrates a sharp decline\nin PGMâ€™s performance with even a small read ratio (10% or 20%).\nConsequently, PGM is excluded from further analysis.\nIn read-only workloads, BLI showed improvements of -30.9%,\n+37.86%, and +15.35% compared to the best-performing SOTA in-\ndex on each respective dataset. BLI consistently outperformed all\nSOTA indexes on medium and hard datasets. Conversely, BLI un-\nderperforms relative to ALEX in read-only/read-heavy workloads\non the books dataset due to its extremely linear key distribution.\nFor such \"easy\" datasets, ALEXâ€™s range partitioning, guided by its\ncost estimation model, can terminate early in linear distributions,\nresulting in fewer levels.\nIn insert-only workloads, BLI demonstrated improvements of\n10.27%, 47.79%, and 40.35% compared to the bestSOTA index on eachdataset (i.e., ALEX, LIPP, and LIPP, respectively). In a more realistic\nworkload scenario with a read-write ratio of 1 : 1, BLI surpassed the\nbest SOTA learned indexes by 29.43%, 121.60%, and 84.43% on the\nrespective datasets (i.e., ALEX, LIPP, and LIPP, respectively). These\nresults affirm BLIâ€™s consistent performance advantage over other\nSOTA learned indexes as an updatable learned index, especially\nunder realistic workloads.\n6.3 S-Bucket and D-Bucket sizes\nWe evaluated how S-Bucket and D-Bucket sizes affect the through-\nput under different workloads. Figure 7 illustrates that different\nworkloads prefer different S-Bucket and D-Bucket sizes on books ,\nwith similar trends observed across the other two datasets.\nFor insert-only workloads, smaller D-Buckets are preferred be-\ncause 1) insertions can trigger SMOs, including D-Bucket splits,\nand smaller D-Buckets reduce the time needed to generate a new\nD-Bucket, and 2) the maximum hint search distance is bounded\nby D-Bucket size. As a result, smaller D-Buckets can lead to better\nğ¸ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿğ‘¡ andğ¸ğ‘™ğ‘œğ‘œğ‘˜ğ‘¢ğ‘ . Regarding S-Bucket sizes under insert-only\nworkloads, BLIâ€™s throughput achieves its peak value when the S-\nBucket size is set to 4; either increasing or decreasing the S-Bucket\nsize reduces BLIâ€™s throughput. This is because when the S-Bucket is\ntoo small, SMOs will be triggered more frequently, as SMOs occur\nwhen an S-Bucket becomes full, which can impact Model Fanout\nğ‘and Insert Efficiency ğ¸insert . Conversely, when the S-Bucket is\ntoo large, finding the target entry within the S-Bucket takes longer\nsince entries within an S-Bucket are unsorted, which negatively\naffectsğ¸lookup within the S-Bucket.\nFor read-only workloads, BLI prefers larger D-Bucket sizes and\nsmaller S-Bucket sizes. With no new key-value pairs being inserted,\nthere is no overhead associated with generating new D-Buckets.\nAdditionally, given the same number of key-value pairs, larger D-\nBucket sizes lead to fewer D-Buckets, simplifying the tree structure\nand increasing Model Fanout ğ‘. The sudden performance boost\nfrom a D-Bucket size of 16M to a D-Bucket size of 32M is due to a\nreduction in the number of levels by one. For S-Buckets, the absence\nof SMO overhead allows smaller S-Buckets to reduce ğ¸lookup within\neach S-Bucket while not affecting ğ¸insert , as previously discussed.\n6.4 Initial fill ratio\nFrom our experimental observations, D-Buckets predominantly de-\ntermine the memory size. In other words, the memory overhead of\nSegments is neglectable (i.e., less than 1%). The memory consump-\ntion of all D-Buckets is determined by the initial fill ratio, which is\nthe fill ratio when allocating new Buckets. With a smaller initial fill\nratio, the memory consumption for Buckets is larger, which is bad\nfor the Memory Overhead ğ‘‚mem . In terms of performance, BLI\nperforms best when the initial fill ratio is 60%. With a smaller fill\nratio, more D-Bucekts are needed to place the same set of key-value\npairs, which can harm Model Fanout ğ‘; with a larger fill ratio, the\nhint error distance can be bad, which can harm ğ¸lookup .\nOur experimental observations reveal that D-Buckets predom-\ninantly determine memory size, while the memory overhead of\nSegments is negligible (i.e., less than 1%). The memory consump-\ntion of all D-Buckets is determined by the initial fill ratio, which\nrepresents the fill ratio at the time of allocating new Buckets. Figure\n11\n\nFigure 7: Throughput with different S-Bucket and D-Bucket size on books\nFigure 8: Throughput with different initial fill ratios on books\n8 demonstrates BLIâ€™s performance under various initial fill ratios.\nA lower initial fill ratio increases memory consumption, adversely\nimpacting Memory Overhead ğ‘‚mem. In terms of performance, BLI\nachieves optimal results with an initial fill ratio of 60%. A lower fill\nratio requires more D-Buckets to store the same set of key-value\npairs, which can negatively impact Model Fanout ğ‘. Conversely, a\nhigher fill ratio can lead to greater hint error distances, detracting\nfrom Lookup Efficiency ğ¸lookup .\n6.5 Range query\nTo evaluate the performance of range queries, we bulk-loaded each\nindex using a dataset of 200M keys and ran a read-only scan work-\nload. A random start key was chosen, and the number of keys\nscanned was varied from 10 to 106. The results show that BLIâ€™s best\nthroughput reached 11.92 million keys/s, while ALEX achieved a\nsignificantly higher throughput of 33.22 million keys/s. BLIâ€™s lower\nperformance in range queries is due to the need to sort key-value\npairs within each D-Bucket before scanning. With a smaller D-\nBucket size, the sorting overhead decreases. However, BLI sacrifices\nrange query performance to 1) improve point query performance\nand 2) facilitate lock-free concurrency.\nIt is important to note that these evaluations are conducted in\nmemory. If the leaf data were stored on disk, the sorting overhead\nwould become negligible, as I/O would then be the primary bot-\ntleneck. Future work includes optimizing BLI for scenarios where\ndata exceeds main memory capacity.\n6.6 Time breakdown\nWe present the time breakdown of BLI execution in Table 1, based on\nexperiments conducted on the fbdataset with a read-write ratio of\n1 : 1. In this workload, reads consume 40.19% of the execution time,\nwhile insertions account for 59.90%. Within the read operations,Table 1: BLI time breakdown on fb\nget() 40.91%Segment lookup 13.49%\nD-Bucket lookup 86.51%\nput() 59.90%Insert 74.73%\nMemory management 25.27%\nonly 13.49% of the time is spent traversing to the target D-Bucket.\nFor insertions, the overhead of allocating new data comprises just\n25.27% of the insertion time.\n6.7 Concurrency\nTo compare the performance of BLI and other indexes under read-\nintensive MRSW workloads, we set the read-write ratio to 7:3. We\nvary the number of threads (or cores) from 2 to 24. For ğ‘›cores,\nwe evenly distribute all read operations to ğ‘›âˆ’1reader threads.\nFigure 9 illustrates the results. Thanks to the lock-free concurrency,\nBLI outperforms all other SOTA learned indexes by up to 3.91x in\nFigure 9.\nFigure 9: Throughput with different number of cores\n6.8 Bulk load\nTable 2 compares BLIâ€™s bulk loading throughput with ALEX and\nLIPP on the books dataset, with similar results observed on the\nother two datasets. Due to its bottom-up mechanism, BLI achieves\n12\n\nTable 2: Bulk load throughput on books\ndataset index throughput (M op/s)\nbooks BLI 16.18\nbooks ALEX 2.53\nbooks LIPP 6.77\nsignificantly higher throughput, outperforming ALEX and LIPP by\n557.19% and 149.34%, respectively.\n7 CONCLUSION\nWe presented BLI, an in-memory index that employs a \"globally\nsorted, locally hinted\" architecture. This design incorporates hint-\nassisted Buckets and supports lock-free concurrency. BLI adopts\nincremental learning, allowing for efficient updates to underlying\nlinear models. Performance evaluations confirm that BLI outper-\nforms state-of-the-art learned indexes with up to a 2.21x throughput\nspeedup. These results open avenues for future research in optimiz-\ning learned index structures.\nREFERENCES\n[1] Berk Atikoglu, Yuehai Xu, Eitan Frachtenberg, Song Jiang, and Mike Paleczny.\n2012. Workload analysis of a large-scale key-value store. In Proceedings of the 12th\nACM SIGMETRICS/PERFORMANCE joint international conference on Measurement\nand Modeling of Computer Systems . 53â€“64.\n[2] Diego Didona and Willy Zwaenepoel. 2019. Size-aware sharding for improving\ntail latencies in in-memory key-value stores. In 16th USENIX Symposium on\nNetworked Systems Design and Implementation (NSDI 19) . 79â€“94.\n[3]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\net al.2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data . 969â€“984.\n[4] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic\ncompressed learned index with provable worst-case bounds. Proceedings of the\nVLDB Endowment 13, 8 (2020), 1162â€“1175.\n[5] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. Fiting-tree: A data-aware index structure. In Proceedings of the\n2019 International Conference on Management of Data . 1189â€“1206.\n[6] Jiake Ge, Huanchen Zhang, Boyu Shi, Yuanhui Luo, Yunda Guo, Yunpeng Chai,\nYuxing Chen, and Anqun Pan. 2023. SALI: A Scalable Adaptive Learned Index\nFramework based on Probability Models. Proceedings of the ACM on Management\nof Data 1, 4 (2023), 1â€“25.\n[7] Charles AR Hoare. 1961. Algorithm 65: find. Commun. ACM 4, 7 (1961), 321â€“322.\n[8]Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 international\nconference on management of data . 489â€“504.\n[9] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: a fine-grained\nlearned index scheme for scalable and concurrent memory systems. Proceedings\nof the VLDB Endowment 15, 2 (2021), 321â€“334.\n[10] Thomas Neumann and Sebastian Michel. 2008. Smooth interpolating histograms\nwith error guarantees. In Sharing Data, Information and Knowledge: 25th British\nNational Conference on Databases, BNCOD 25, Cardiff, UK, July 7-10, 2008. Pro-\nceedings 25 . Springer, 126â€“138.\n[11] Rajesh Nishtala, Hans Fugal, Steven Grimm, Marc Kwiatkowski, Herman Lee,\nHarry C Li, Ryan McElroy, Mike Paleczny, Daniel Peek, Paul Saab, et al .2013.\nScaling memcache at facebook. In 10th USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI 13) . 385â€“398.\n[12] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie\nWang, and Haibo Chen. 2020. XIndex: a scalable learned index for multicore\ndata storage. In Proceedings of the 25th ACM SIGPLAN symposium on principles\nand practice of parallel programming . 308â€“320.\n[13] Wenlong Wang and David Hung-Chang Du. 2024. LearnedKV: Integrating\nLSM and Learned Index for Superior Performance on SSD. arXiv preprint\narXiv:2406.18892 (2024).\n[14] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and\nTianzheng Wang. 2022. Are updatable learned indexes ready? arXiv preprint\narXiv:2207.02900 (2022).\n[15] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao\nXing. 2021. Updatable learned index with precise positions. arXiv preprintarXiv:2104.05520 (2021).\n[16] Fumito Yamaguchi and Hiroaki Nishi. 2013. Hardware-based hash functions for\nnetwork applications. In 2013 19th IEEE International Conference on Networks\n(ICON) . IEEE, 1â€“6.\n[17] Juncheng Yang, Yao Yue, and KV Rashmi. 2021. A large-scale analysis of hundreds\nof in-memory key-value cache clusters at twitter. ACM Transactions on Storage\n(TOS) 17, 3 (2021), 1â€“35.\n[18] Huanchen Zhang, Xiaoxuan Liu, David G Andersen, Michael Kaminsky, Kimberly\nKeeton, and Andrew Pavlo. 2020. Order-preserving key compression for in-\nmemory search trees. In Proceedings of the 2020 ACM SIGMOD International\nConference on Management of Data . 1601â€“1615.\n13",
  "textLength": 71575
}