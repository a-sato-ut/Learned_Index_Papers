{
  "paperId": "32f33694bced4b0fb5190d1f7c70bbc6aa6b257a",
  "title": "FB+-tree: A Memory-Optimized B+-tree with Latch-Free Update",
  "pdfPath": "32f33694bced4b0fb5190d1f7c70bbc6aa6b257a.pdf",
  "text": "FB+-tree: A Memory-Optimized B+-tree with Latch-Free Update\nYuan Chen\nSchool of Computer Science, Wuhan University\nyuan.chen@whu.edu.cnAo Li\nSchool of Computer Science, Wuhan University\nleo210@whu.edu.cn\nWenhai Li\nSchool of Computer Science, Wuhan University\nlwh@whu.edu.cnLingfeng Deng\nSchool of Computer Science, Wuhan University\nlingfengdeng@whu.edu.cn\nABSTRACT\nB+-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically\nutilized for branch operations, it may lead to inefficient cache uti-\nlization in main-memory scenarios. In contrast, trie-based index\nstructures drive branch operations through prefix matching. While\nthese structures generally produce fewer cache misses and are thus\nincreasingly popular, they may underperform in range scans be-\ncause of frequent pointer chasing.\nThis paper proposes a new high-performance B+-tree variant\ncalled Feature B+-tree (FB+-tree) . Similar to employing bit or\nbyte for branch operation in tries, FB+-tree progressively considers\nseveral bytes following the common prefix on each level of its inner\nnodes‚Äîreferred to as features, which allows FB+-tree to benefit\nfrom prefix skewness. FB+-tree blurs the lines between B+-trees and\ntries, while still retaining balance. In the best case, FB+-tree almost\nbecomes a trie, whereas in the worst case, it continues to function as\na B+-tree. Meanwhile, a crafted synchronization protocol that com-\nbines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB+-tree leverages\nsubtle atomic operations seamlessly coordinated with optimistic\nlock to facilitate latch-free updates, which can be easily extended\nto other structures. Intensive experiments on multiple workload-\ndataset combinations demonstrate that FB+-tree shows comparable\nlookup performance to state-of-the-art trie-based indexes and out-\nperforms popular B+-trees by 2.3x‚àº3.7x under 96 threads. FB+-tree\nalso exhibits significant potential on other workloads, especially\nupdate workloads under contention and scan workloads.\nPVLDB Reference Format:\nYuan Chen, Ao Li, Wenhai Li, and Lingfeng Deng. FB+-tree: A\nMemory-Optimized B+-tree with Latch-Free Update. PVLDB, 18(6):\nXXX-XXX, 2025.\ndoi:XX.XX/XXX.XX\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/Spear-Neil/IndexResearch.git.\n‚àóWenhai Li is the corresponding author.\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 18, No. 6 ISSN 2150-8097.\ndoi:XX.XX/XXX.XX\nbranches branch misses LLC loads LLC misses0.00.20.40.60.81.0Normalized Events Count\n(a) Hardware events countUniform Zipfian0.00.51.01.52.02.53.03.54.0Million Operations per Second\n(b) Single-threaded throughputFB+-tree FAST STX B+-treeFigure 1: (a) Hardware statistics under uniform distribution,\n(b) Single-threaded throughput under different distributions.\n1 INTRODUCTION\nThe storage engine significantly influences the overall performance\nof a database system, especially the index data structure. Even in\nmain-memory systems, index query operations account for 14%‚àº\n94%of the overhead [ 35]. B-trees (B+-tree, B‚àó-tree, etc.) are ubiq-\nuitous in disk-based database systems because of their prominent\nIO efficiency [ 19], yet scarcely exist in main-memory database sys-\ntems due to their poor utilization of hierarchical caches. ART [ 43],\nMasstree [ 49], and other trie-based structures are generally more\nefficient than main-memory B+-tree [ 1,10,43,49,72], which makes\nthem more prevailing in modern main-memory systems (e.g., Silo\n[65] and HyPer [ 31]). What makes main-memory B+-tree untenable\nis the orders of magnitude difference in access latency between disk\nand memory. Generally, main memory provides an access latency of\n80‚àº100ns, whereas the disk and flash access latency are about 10\nms and 50 us, respectively [ 57]. When IO dominates the overhead of\na B+-tree, the impact of CPU caches is nearly negligible. However,\neffective cache utilization becomes prominent in enhancing index\nperformance when IO is eliminated [27, 43, 49, 56, 60].\nCache and Branch Optimization. A B+-tree consists of inner\nnodes and leaf nodes. Inner nodes serve as index nodes, containing\nanchor keys1and pointers to child nodes. Key-value pairs are stored\nin leaf nodes sequentially or semi-ordered [ 47,53]. All leaf nodes\nare linked together in a linked list, enabling efficient range queries.\nA query starts from the root node and navigates to a leaf node using\nbinary search for branch operations, i.e., index traversal. Thanks\nto such design, B+-tree shows efficient disk access. But when it\ncomes to main-memory scenarios, B+-tree suffers from several\ninterconnected and complex problems.\nFirst, B+-tree‚Äôs structure and binary search render it inefficient on\ncache utilization, as detailed in Section 3. Second, each comparison\n1Also known as separator keys, we follow the term anchor keys used in previous work.arXiv:2503.23397v1  [cs.DB]  30 Mar 2025\n\n24816 24 32 40 48 56 64 72 80 88 96\nThreads020406080100120140160Million Operations per Second\n(a) Scalability (skew=0.99) Blink-tree\nB+-treeOLC\nFB+-treeYCSB-C\nYCSB-A\nL1 DCache L2 Cache L3 Cache Memory050100150200250300350Clock Cycles\n(b) Access latencyFigure 2: (a) Multi-core scalability of Bùëôùëñùëõùëò-tree, B+-treeOLC,\nand FB+-tree, (b) Access latency of memory hierarchies.\nin binary search depends on the outcome of the previous compari-\nson, and the result of each comparison is hard to predict [ 43]. This\nintrinsic characteristic of binary search hinders the full exploitation\nof modern CPUs‚Äô computational and memory-level parallelism po-\ntential. Hard-to-predict comparisons pose challenges for dynamic\nbranch prediction, causing modern CPUs‚Äô long pipelines to stall.\nDependences between instructions render superscalar, speculation,\nand out-of-order execution powerless [21, 22].\nIn this paper, we propose a new optimistic main-memory B+-tree\nvariant named Feature B+-tree (FB+-tree) to tackle these issues.\nFB+-tree effectively mitigates the mismatch between B+-tree‚Äôs mem-\nory access patterns and the intrinsic characteristics of hardware\narchitecture through progressively byte-wise parallel processing.\nIn addition, FB+-tree employs a simple for loop for branch opera-\ntions in most cases. This enables FB+-tree to fully leverage modern\nCPUs‚Äô parallelism potential and to facilitate sequential memory ac-\ncess. Figure 1 illustrates a comparison of throughput and hardware\nevent statistics of FB+-tree, FAST [ 32] and STX B+-tree [ 9] dur-\ning 100 million random 64-bit integer lookups following uniform\nand zipfian distributions on a dual-socket, 48-core (96-hyperthread)\nserver. Hardware events during lookups under uniform distribu-\ntion are measured with perf. Compared to STX B+-tree, FB+-tree\nshows fewer branch instructions, branch misses, LLC loads and\nLLC misses. The FAST tree is a binary tree that collapses multiple\nnodes into one large node to facilitate SIMD instructions and cache\nconsciousness. Although FAST has fewer branch instructions and\nbranch misses, FB+-tree has better cache utilization due to its dis-\ncriminative byte processing. This makes FB+-tree outperform STX\nB+-tree and FAST in single-threaded execution.\nSynchronization Protocol. Another crucial factor that impacts\nthe overall performance of a database system is the synchronization\nprotocol of its index. A fine-grained locking protocol, such as lock\ncoupling (hand-over-hand lock) [ 3] and Bùëôùëñùëõùëò-tree [ 38,40], has\npoor scalability in main-memory database systems. Optimistic lock\nutilizes a version combined with a lock to detect changes within\na node [ 14,28,42,49]. These protocols enable latch-free2index\ntraversal except during node modification, thereby ensuring high\nscalability. FB+-tree employs a similar yet highly optimized protocol\nfor index traversal and incorporates the link technique [ 40] for\nconcurrent structure modification (i.e., node split and merge).\nThe salient innovation of FB+-tree‚Äôs synchronization protocol\nis latch-free update. Previous work acquires a lock on a node or\n2We follow the term \"latch-free\" used in OLFIT [ 14] which means without acquiring\nlocks to distinguish it from the term \"lock-free\" which means non-blocking. We follow\nthe synchronization literature to use \"lock\" except \"latch-free\".locks on more nodes when performing an update. They protect\novermuch auxiliary computations in critical sections and frequently\nretry if they cannot hold the lock, leading to coherence cache inval-\nidations. Such locks thus prevent other threads from performing\neven unrelated operations and make updates poorly scalable under\nhigh-contention workloads. As shown in Figure 2(a), B+-treeOLC\nsynchronized by optimistic lock coupling [ 42] is more scalable than\nBùëôùëñùëõùëò-tree on YCSB-C workload (read-only). However, it suffers\nfrom performance collapse on YCSB-A workload (50%-read, 50%-\nupdate). FB+-tree mitigates such collapse via latch-free update.\nIn short, FB+-tree employs a more fine-grained synchroniza-\ntion protocol which enables an update to be performed without\nacquiring any locks. FB+-tree executes an update using the compare-\nand-swap (CAS) instruction. This way, a lookup can be executed\nsimultaneously with updates, and updates only contend on the\nsame key-value pairs. Our protocol offers a general technique for\nperforming updates without acquiring any locks. Index structures\nsynchronized by optimistic lock can implement latch-free updates\nwith a few changes to existing code. Benchmarks on YCSB workload\nA (update heavy) demonstrate that FB+-tree outperforms existing\nindexes with optimistic lock under contentions.\nContributions and Paper Organization. This paper makes\nthe following contributions:\n‚Ä¢A technique supporting arbitrary key types named feature com-\nparison is introduced, which allows FB+-tree to work like a trie\nand leverage modern CPUs‚Äô computational and memory-level\nparallelism potential. This technique combined with hashtags\nin leaf nodes enables FB+-tree to outperform typical B+-trees in\nboth performance and scalability on index traversal.\n‚Ä¢Thanks to feature comparison, FB+-tree does not need to fre-\nquently access anchor keys like typical B+-trees. FB+-tree thus\nonly stores pointers to anchor keys for space efficiency.\n‚Ä¢A highly optimized optimistic synchronization protocol is de-\nvised to facilitate multi-core scalable concurrent index accesses.\nIn particular, the protocol introduces a general latch-free up-\ndate technique that utilizes CAS primitive for update operations\nwithout holding any locks.\n‚Ä¢Comprehensive experiments are conducted on multiple workload-\ndataset combinations for existing index structures and FB+-tree.\nThe results demonstrate that FB+-tree outperforms popular B+-\ntrees by 2.3x‚àº3.7x on read-only workloads under 96 threads.\nOn update-heavy workloads, FB+-tree shows the best scalability\nthanks to latch-free update technique.\nThe rest of this paper is organized as follows. Section 2 introduces\nbackground and related work. Section 3 presents the motivation,\ndata structure, and algorithm of FB+-tree. Section 4 describes the\nsynchronization protocol enabling FB+-tree multi-core scalable.\nSection 5 shows the experiment setup, workloads, and evaluation\nresults. The conclusion is given in Section 6.\n2 RELATED WORK\nQuerying an index for a key (i.e., index traversal) involves the pro-\ncess of narrowing the search key space until confirming whether the\nkey exists. In B-trees, the process relies on comparisons between the\ntarget key and the anchors. Meanwhile, in tries, it depends on prefix\nmatching. This section provides some background and related work\n\non ordered index. In addition, previous work on synchronization\nprotocols is presented at the end.\nMemory Access. From a more detailed perspective, index tra-\nversal can be divided into memory access and actual computation.\nCompared to memory access, the computational overhead is almost\nnegligible. On modern machines, several to hundreds of instructions\ncan be evaluated and retired in parallel, and most SIMD instructions\nhave 1-cycle throughput [ 21,22,25]. However, memory access typ-\nically takes several hundred cycles to complete, as shown in Figure\n2(b). Various main-memory ordered indexes have been proposed\nto optimize cache utilization. We categorize existing ordered in-\ndexes into three groups: B-trees, tries, and learned indexes. Next,\nwe introduce their related work respectively.\nB-trees. B-trees are a class of balanced, comparison-based in-\ndexes3, encompassing many variants such as B-tree, B‚àó-tree, B+-\ntree, binary B-tree, AVL tree, and others [ 19]. As memory capacity\nincreased, the T-tree placed multiple records in one binary tree‚Äôs\nnode for main-memory database systems [41]. The CSS-trees [55]\nand CSB+-tree [ 56] store each node‚Äôs children in contiguous mem-\nory to mitigate pointer chasing. Bender et al. conducted an intensive\ntheoretical analysis and experimental evaluation on cache-oblivious\nB-trees [ 5‚Äì8]. Chen et al. proposed to optimize B+-trees‚Äô perfor-\nmance through prefetching for both cache and disk [17, 18].\nCurrently, software prefetching is extensively used in main-\nmemory indexes [ 10,43,49], as it targets short array streams and\nirregular memory address patterns [ 24,39,66]. The prefix B-trees\nexplored not directly storing keys but constructing prefixes in inner\nnodes [ 4,26]. The pkT-trees and pkB-trees store fixed-size parts\nof keys directly in the tree nodes [ 12,52]. The B2-tree organizes\ninner nodes as embedded trie-like structures for indexing string\nkeys [ 59]. The DB+-tree [ 37] incorporates the discriminative bits\nfrom HOT [ 10] into B-trees. From another perspective, k-Ary, FAST\n, P-ary, etc. exploit binary search with SIMD instructions to lever-\nage parallelism of both CPUs and GPUs [ 30,32,58,61,75]. PALM\nperforms multiple concurrent queries in batches [60].\nTries. Tries, also known as radix trees, prefix trees, or digital\nsearch trees, drive branch operations through prefix matching, as\nillustrated in Figure 3. These data structures directly use the digital\nrepresentation of keys and may have excessive worst-case space\nconsumption [ 43]. The Patricia trie introduces path compression\nand only stores prefixes of keys in trie nodes [ 51]. The burst trie\nsubstitutes leaf nodes with containers maintaining a small set of\nkeys [ 29]. The HAT-trie maintains hash tables as containers for\nbetter performance [ 1]. The generalized prefix tree is a trie with\nvariable prefix length for indexing arbitrary data types [13].\nThe Adaptive Radix Tree (ART) adaptively uses four different\nnode layouts depending on the number of child nodes [ 43]. Path\ncompression and lazy expansion allow ART to efficiently index\nstring keys by collapsing nodes. The Masstree is a trie-like con-\ncatenation of B+-trees, where each trie node is a B+-tree indexing\ndifferent 8-byte slices of keys [ 49]. The Height Optimized Trie\n(HOT) dynamically varies the number of discriminative bits consid-\nered at each node [ 10,11]. Similar to B+-trees, another interesting\ndesign Wormhole [ 68] maintains a double-linked list of leaf nodes.\n3Skip list [54] and its variants are also comparison-based.It then constructs a trie with all prefixes of the lower bound of\nleaf nodes and represents the trie with a hash table. The Cuckoo\nTrie shares a similar idea and exploits memory-level parallelism to\nalleviate pointer chasing [71].\nLearned Indexes. Indexes can be viewed as models to map a key\nto the position of a record. Kraska et al. proposed learned indexes\n(machine learning models) as replacements for index structures.\nThey have demonstrated that learned indexes have the potential to\noffer benefits over state-of-the-art indexes [ 36]. The ALEX exploits\nlinear regression combined with exponential search to guarantee\naccuracy and enable adaptive update [ 23,48]. Some previous work\nhas explored indexing string keys using learned indexes [ 33,34,64,\n70]. Learned indexes and database systems with AI have become a\nprominent research topic, and extensive work has been dedicated\nto making them practical [16, 33, 46, 73, 74, 76].\nIn summary, previous research has demonstrated that tries out-\nperform B-trees in terms of point lookup. For range iteration, B-\ntrees, especially B+-trees, demonstrate better performance because\nof their balanced structure. The Wormhole combines the strengths\nof B+-tree and trie. Unfortunately, its hashed representation of meta-\ntrie and indirect ordered leaf node may incur significant overhead\nfor insert and range scan operations, respectively. Learned indexes\nmay offer advantages; however, challenges still exist in achieving\naccuracy, adaptive update capabilities, and indexing string keys.\nSynchronization on Indexes. Designing an efficient synchro-\nnization protocol is critical and challenging. Lock coupling [ 3] and\nBùëôùëñùëõùëò-tree [ 40] lock only one node simultaneously, thus demon-\nstrating good scalability in traditional database systems. Optimistic\nlock coupling, OLFIT, Masstree, and many other indexes combine\nversion with lock to avoid coherence cache miss [ 10,14,28,42,\n43,49,71]. In these protocols, readers verify the version to detect\nchanges within a node without acquiring the lock. Meanwhile, writ-\ners acquire a write lock and update the version when modifying\na node. The Read-Optimized Write Exclusion (ROWEX) protocol\nused by ART and HOT goes one step further by only providing\nexclusion relative between writers while allowing readers to always\nsucceed without block nor restart [10, 42, 44].\nOLFIT and several indexes employ a bottom-up strategy for\nstructure modification, e.g. split and merge, and adopt the link\ntechnique and high key from Bùëôùëñùëõùëò-tree for concurrency support\n[14,49]. Optimistic lock coupling provides a more general approach\nto synchronize index structures [ 42,44], utilizing a top-down strat-\negy for concurrent structure modification. If a change occurs during\nindex traversal, it restarts from the root. ROWEX utilizes subtle\natomic operations to ensure that reads are always consistent and\nnever restart [ 44]. OptiQL incorporates queue-based locking and\nopportunistic read techniques into optimistic lock coupling to miti-\ngate performance collapse under high contention [ 63]. The Bw-tree\nimplements lock-free operations through delta records and map-\nping table [ 45,67], but it may incur heavy overhead. Some work\nhas proposed universal construction strategies to transform trees\nfrom sequential code to linearizable concurrent versions [62].\nWe have learned lessons from previous work. FB+-tree considers\na B+-tree from trie‚Äôs prefix matching perspective. Common pre-\nfixes and parts of anchors are directly stored in inner nodes but\nwith a quite different arrangement. Unlike k-ary, FAST, etc., which\n\ncache linecommon prefix common prefix\ncache line o o u\nh s lJ\no o u\nh s lJ\no o u\nh s lJ\nAndrew\nAlexAlice\nAlexAliceAndrew\nAlexAliceJames\nJacob\nAustinJacob\nAustinJames\nJacob\nAustinJulia\nJoseph\nJohnJoseph\nJohnJulia\nJoseph\nJohnJustinKevinKristina\nKevinKristina\nJustinKevinKristinan rA J J\naK\nu n rA J J\naK\nucache linecommon prefix common prefix\ncache line o o u\nh s lJ\nAndrew\nAlexAliceJames\nJacob\nAustinJulia\nJoseph\nJohnJustinKevinKristinan rA J J\naK\nuA J\nl\nun\neia\ncmoK\nu\nh l s ser\nAlexAliceAndrewAustinJacobJamesJohnJosephJuliaJustinKevinKristinaC\no\nsInner Node\nLeaf NodeImplicit Node\nInner Node\nLeaf NodeImplicit NodeKseniaA J\nl\nun\neia\ncmoK\nu\nh l s ser\nAlexAliceAndrewAustinJacobJamesJohnJosephJuliaJustinKevinKristinaC\no\nsInner Node\nLeaf NodeImplicit NodeKsenia\ncache linecommon prefix common prefix\ncache line o o u\nh s lJ\nAndrew\nAlexAliceJames\nJacob\nAustinJulia\nJoseph\nJohnJustinKevinKristinan rA J J\naK\nuA J\nl\nun\neia\ncmoK\nu\nh l s ser\nAlexAliceAndrewAustinJacobJamesJohnJosephJuliaJustinKevinKristinaC\no\nsInner Node\nLeaf NodeImplicit NodeKseniaFigure 3: Illustration of a trie and the main idea of FB+-tree\nonly consider data types supported by SIMD instructions, FB+-tree\nfacilitates byte-wise parallel processing for branch operations, al-\nlowing arbitrary data types. For concurrency support, FB+-tree\nuses a highly optimized optimistic synchronization protocol and\nemploys the link technique for concurrent structure modification.\nFurthermore, FB+-tree allows multiple writers to perform updates\non one leaf node without blocking other updates nor reads.\n3 THE FB+-TREE DATA STRUCTURE\nIn this section, we start by providing a comprehensive analysis of\nthe overhead associated with comparison operations in B+-trees.\nNext, we introduce our feature comparison technique, FB+-tree‚Äôs\ndata structure, and the branch algorithm. The synchronization\nprotocol is discussed in Section 4.\n3.1 Dissonance between B+-tree and Hardware\nAs previously mentioned, binary search is employed to retrieve\nchild nodes in index traversal of B+-trees. In binary search, the\nanchor key for comparison is the median of the current search\nkey space. This leads to a complex memory access pattern that un-\nderutilizes effective hardware and software prefetching. Moreover,\nhard-to-predict comparisons and dependences between compar-\nisons make memory-level parallelism unattainable [71].\nConsider a B+-tree indexing string keys, where only pointers to\nanchor keys are stored in inner nodes and the contents of anchors\nare stored in memory allocated via malloc. Assuming a fanout factor\n256, binary search in an inner node would typically involve eight\ncomparisons. On modern 64-bit machines, storing 256 pointers to\nanchors would require 256 * 8 bytes of space, equivalent to 32 cache\nlines. Consequently, each binary search would access six cache lines\nfor pointers alone, whereas only eight pointers are effectively used.\nFor B+-trees indexing integer keys, similar issues arise. Several\nindexes therefore configure their nodes as 256 bytes with a proper\nfanout to alleviate this cache inefficiency. [10, 43, 49, 67].\nAfter obtaining the pointer to an anchor, the content of the an-\nchor is compared with that of the target key following dereference.\nNo matter how many bytes the anchor has and how many bytesare used for comparison, at least one complete cache line is loaded\nfrom memory. However, in many cases, only a few bytes are neces-\nsary to establish the relative order between string keys (similarly\nfor integer keys) [ 4,12,52]. Consequently, this results in wasted\nmemory bandwidth and the eviction of several hot cache lines.\nThis problem is exacerbated on modern CPUs, because continu-\nous several cache lines will be automatically prefetched but remain\nunused, leading to bandwidth waste [ 21,24]. In concurrent environ-\nments, these random small memory accesses further impede full\nutilization of memory bandwidth and Ultra Path interconnect (UPI)\nbandwidth [ 21,71]. These problems result in B+-trees‚Äô sub-optimal\nscalability even on read-only workloads.\n3.2 Feature Comparison\n3.2.1 Motivation. Tries have advantages over B+-trees in cache\nutilization, as they drive branch operations through prefix match-\ning. The key idea of FB+-tree is to integrate such mechanism into\nB+-trees to benefit from architecture while preserving B+-trees‚Äô\nproperties, particularly balance. We start with the intrinsic similar-\nities between B+-trees and tries, as illustrated in Figure 3.\nIn B+-trees, the entire key space is divided into intervals defined\nby anchors in inner nodes, which serve as the upper and lower\nbounds of these intervals. In contrast, tries partition the entire key\nspace into sub-trees using prefixes. B+-trees and tries are function-\nally equivalent in this partition sense. In other words, trie nodes\ninherently imply the relative order between keys through their\ndigital representation. Tries accomplish key space partitioning with\nbyte-wise prefix matching (or using smaller radix), whereas com-\nparisons between strings are also performed in a byte-wise manner.\nComparison between binary keys can be conducted similarly after\ncode transition, as detailed in Subsection 3.6. These similarities\nsuggest that branch operation in B+-trees could potentially be im-\nplemented similarly to prefix matching in tries.\nAs shown in Figure 3, we build a trie and a B+-tree for a collection\nof keys, where the anchor keys of the B+-tree are highlighted in\nboth structures. Consider a use case of querying the trie for the key\n\"John\". The most significant byte \"J\" is used to retrieve the dotted\nnode following the root node. The search key space is then reduced\nto the sub-tree with the dotted node as root. Next, the latticed\nnode is retrieved via the second byte \"o\", followed by reaching the\nleaf node \"John\". Revisiting this byte-wise prefix matching from\na B+-tree‚Äôs comparison perspective, it implies the first byte \"J\" is\ncompared with the first byte of the four anchor keys \"Andrew\",\n\"James\", \"Julia\", and \"Kristina\" simultaneously. After this byte-wise\nparallel comparison, the search key space is narrowed down to the\nkeys greater than \"Andrew\" and less than \"Kristina\".\n3.2.2 Algorithm. Obviously, this byte-wise parallel comparison can\nbe implemented with SIMD instructions. B+-tree‚Äôs branch operation\ncan be implemented with progressively byte-wise processing, as\nshown in Figure 3. As anchor keys may share a common prefix,\nbyte-wise processing on the common prefix is unproductive and\nspace-inefficient. FB+-tree constructs the common prefix in inner\nnodes and performs byte-wise parallel processing on the following\nbytes. An interesting property similar to prefixes in tries is that the\ncommon prefix length of a child node is no smaller than that of its\nparent node. Branch operation could skip the common prefix and\n\n(a) Binary search  across anchors1 2\n3 4\nAndrew\n James\n Julia\n Kristina\n Andrew\n James\n Julia\n Kristina\nJohn\n Joseph\n Julia\n John\n Joseph\n Julia\nAndrew\n James\n Julia\n Kristina\nJohn\n Joseph\n Julia\n1 2\n3 4\nAndrew\n James\n Julia\n Kristina\nJohn\n Joseph\n Julia\n(b) Totally b yte-wise processing\nn\na\nu\nr\nJ\nJ\nK\n.\n.\n.\n.\nd\nm\nl\nI\nA\no\no\nu\nJ\nJ\nn\n.\n.\nh\ns\nl\nJ\n1\n2\n3\n4\n5\n6\nn\na\nu\nr\nJ\nJ\nK\n.\n.\n.\n.\nd\nm\nl\nI\nA\no\no\nu\nJ\nJ\nn\n.\n.\nh\ns\nl\nJ\n1\n2\n3\n4\n5\n6\n(c) Hybrid byte -wise processing\nJohnSuffix5Suffix5\nn\na\nu\nr\nA\nJ\nJ\nK 1\n2\nn\na\nu\nr\nA\nJ\nJ\nK 1\n2\nh\ns\nl\no\no\nu\nK 3\n4\nh\ns\nl\no\no\nu\nK 3\n4\nn\na\nu\nr\nA\nJ\nJ\nK 1\n2\nh\ns\nl\no\no\nu\nK 3\n4\nSuccess\nFailed\nJohnSuffix5\nn\na\nu\nr\nA\nJ\nJ\nK 1\n2\nh\ns\nl\no\no\nu\nK 3\n4\nSuccess\nFailed\n(a) Binary search  across anchors1 2\n3 4\nAndrew\n James\n Julia\n Kristina\nJohn\n Joseph\n Julia\n(b) Totally b yte-wise processing\nn\na\nu\nr\nJ\nJ\nK\n.\n.\n.\n.\nd\nm\nl\nI\nA\no\no\nu\nJ\nJ\nn\n.\n.\nh\ns\nl\nJ\n1\n2\n3\n4\n5\n6\n(c) Hybrid byte -wise processing\nJohnSuffix5\nn\na\nu\nr\nA\nJ\nJ\nK 1\n2\nh\ns\nl\no\no\nu\nK 3\n4\nSuccess\nFailedFigure 4: Binary search vs. FB+-tree‚Äôs branch algorithm.\nindex traversal could gradually handle different slices of a target\nkey on each level of inner node. FB+-tree thus almost becomes a\ntrie in terms of computational complexity.\nOne reason why tries are generally more cache-conscious is their\ncapability to fit complex data distribution or prefix skewness. Trie\nnodes shared by more keys would reside in a cache level closer to\nCPUs. By contrast, B+-trees mechanically select the middle key as\nthe anchor key during node split, which is unconscious of prefix\nskewness. In other words, several anchors may share a much longer\ncommon prefix (sparse keys). Performing byte-wise parallel pro-\ncessing over all the remaining bytes following the common prefix\nthus may be inefficient. For space efficiency, FB+-tree only stores a\nfew fixed-size discriminative bytes of anchors directly in inner node.\nA binary search over suffixes will be performed when byte-wise\nparallel comparison cannot achieve branch operation.\nFor example, consider the B+-tree‚Äôs third leaf node shown in\nFigure 3 as an inner node. We show the process of querying the\nkey \"John\" with binary search, totally byte-wise parallel processing,\nand FB+-tee‚Äôs hybrid branch algorithm respectively in Figure 4,\nwhere the numbers highlight the order of memory access along\nhierarchical inner nodes. Binary search generates irregular and\nsmall memory access, as shown in Figure 4(a). Totally byte-wise\nparallel processing does not take into account common prefixes\nand sparse keys, as shown in Figure 4(b). Supposing only two bytes\nfollowing common prefix are stored directly in inner nodes in Figure\n4(c). In the root node, branch operation succeeds after two byte-\nwise comparisons. In the child node, the common prefix \"J\" can be\nskipped. In this special case, byte-wise parallel processing fails in\nbranch and a binary search over suffixes has to be performed.\n3.2.3 Discussion. With this byte-wise parallel processing on dis-\ncriminative bytes, FB+-tree works like a trie and alleviates direct\naccess to anchors. As index traversal descends to a leaf node, com-\nmon prefix length grows and branch operation gradually processes\ndifferent slices of a target key. Therefore, binary search over suffixes\nwould occur with very low probability. FB+-tree thus evolves into\na trie in the best case. In the worst case, branch operations on all\ninner nodes depend on binary search over suffixes, which is almost\nimpossible. FB+-tree thus degenerates into a B+-tree.\nIn most cases, FB+-tree forms a hybrid structure that combines\nthe characteristics of both B+-trees and tries. Prefix matching is\nsignificantly efficient for dense keys but sub-optimal for sparse\nkeys4[10,43,49]. Binary search generally incurs expensive cache\n4For tries, node collapse or path compression can mitigate this problem but not com-\npletely. HOT dynamically changes the number of discriminative bits used in a node,\nand meanwhile almost stays a constant fanout. HOT thus achieves almost extreme1 // for binary keys // for string keys\n2 struct InnerNode : struct InnerNode :\n3 Control control; Control control;\n4 int knum; int knum;\n5 int plen; int plen;\n6 char prefix[8]; String* huge; // huge prefix\n7 node* next; node* next;\n8 char features[fs][ns]; char features[fs][ns];\n9 node* children[ns]; char tiny[224]; // embedded prefix\n10 String* anchors[ns];\n11 node* children[ns];\n13 struct LeafNode : struct String :\n14 Control control; int len;\n15 uint64_t bitmap; char str[];\n16 KeyType high_key;\n17 LeafNode* sibling; struct KVPair :\n18 char tags[ns]; KeyType key;\n19 KVPair* kvs[ns]; ValueType value;\nFigure 5: Node structures of FB+-tree.\nmisses, while not affected by the sparseness of keys. FB+-tree em-\nploys byte-wise parallel processing for dense keys to enhance cache\nconsciousness. For sparse keys, this byte-wise processing on dis-\ncriminative bytes acts as a filter to narrow the search space for\nbinary search. This hybrid structure allows FB+-tree to efficiently\nindex both sparse and dense keys while maintaining a balanced\nstructure that facilitates efficient range iteration.\nTo some extent, these discriminative bytes imply some data dis-\ntribution characteristics of the keys in node intervals (sparse or\ndense). Therefore, we refer to these bytes as features and name this\nbyte-wise parallel processing as feature comparison. Even though\nFB+-tree stores feature bytes, FB+-tree only stores pointers to an-\nchor keys in inner nodes, making it even more space-efficient than\ntypical B+-trees. Next, we introduce the node implementation, al-\ngorithm, and some optimizations.\n3.3 Node Implementation\nFigure 5 illustrates the node structures of FB+-tree. The control is an\n8-byte atomic variable that governs the synchronization behaviors.\nThe knum andplen indicate the number of anchor keys and the\nlength of the common prefix in an inner node. The next specifies an\ninner node‚Äôs sibling or last child. The fsandnswhich can be man-\nually configured represent the feature and node size, respectively.\nIn leaf nodes, the bitmap indicates whether the corresponding slot\ninkvsis occupied and the tags field contains the corresponding\nhashtags. The high_key is the upper bound of a leaf node.\nEssentially, both structures of FB+-tree‚Äôs inner nodes and leaf\nnodes are identical to that of a typical B+-tree, except for the fea-\ntures in inner nodes and the hashtags in leaf nodes. To facilitate\nconcurrent structure modification, all nodes on the same level are\nlinked in a single-linked list. Each node starts with an 8-byte control\nfield, indicating the node type. Binary keys and string keys share\nsimilar inner node and leaf node structures.\nLeaf Node. Typically, key-value pairs are ordered in leaf nodes,\nwhich could lead to time-consuming rearrangement and binary\nsearch during insertion. These complex operations within a critical\nsection may limit multi-core scalability. To address these issues,\nwe store the unsorted key-value pairs in a pointer array kvsand\ncache utilization for lookup. Unfortunately, the common problems of trie structures\nstill exist, which is inefficient and complicated concurrent range iteration.\n\n1 void* branch(String& key) { // inner node\n2 void* node = nullptr;\n3 int pcmp = prefix_compare(key);\n4 if (pcmp == 0) { // prefix matches\n5 int idx, fid, cmps = min(fs, key.len - plen);\n6 uint64_t mask, eqmask = (0x01ul << knum) - 1;\n7 for (fid = 0; fid < cmps; fid++) {\n8 char byte = key.str[plen + fid] + 128;\n9 mask = compare_equal(features[fid], byte);\n10 mask = mask & eqmask;\n11 if (mask == 0) break;\n12 eqmask = mask;\n13 } // feature comparison\n14 if (fid < cmps) {\n15 char byte = key.str[plen + fid] + 128;\n16 mask = compare_less(features[fid], byte);\n17 mask = mask & eqmask;\n18 if (mask == 0) idx = index_least1(eqmask);\n19 else idx = 64 - countl_zero(mask);\n20 } else { // binary search on suffixes\n21 int hid = 64 - countl_zero(eqmask);\n22 int lid = index_least1(eqmask);\n23 idx = suffix_bs(key, plen + cmps, lid, hid);\n24 }\n25 node = children[idx];\n26 } else { node = children[0]; }\n27 return node;\n28 }\n30 KVPair* lookup(String& key) { // leaf node\n31 char tag = hash(key.str, key.len);\n32 uint64_t mask = compare_equal(tags, tag);\n33 mask = mask & bitmap; // candidates\n34 while (mask) {\n35 int idx = index_least1(mask);\n36 KVPair* kv = kvs[idx].load();\n37 if (kv != nullptr && key == kv->key)\n38 return kv; // key found\n39 mask &= ~(0x01ul << idx);\n40 }\n41 return nullptr; // key not found\n42 }\n44 // compare 64 bytes to a char 'c', AVX512\n45 uint64_t compare_equal(void* p, char c) {\n46 __m512i v1 = _mm512_loadu_si512(p);\n47 __m512i v2 = _mm512_set1_epi8(c);\n48 return _mm512_cmpeq_epi8_mask(v1, v2);\n49 }\nFigure 6: Lookup Related Algorithms\nutilize hashtags for efficient lookup, as in previous work [ 47,53,\n68]. Additionally, each leaf node includes a high_key to support\nconcurrent structure modification, which indicates the upper bound.\nFor binary keys, the high_key is directly stored in the node; for\nstring keys, each leaf node maintains a pointer to high_key .\nInner Node. The anchor keys in an inner node are ordered. For\nbinary keys, all bytes of an anchor are directly stored in features .\nThe common prefix is stored in prefix . For better performance, the\nprefixes of anchors are truncated and the remaining bytes in features\nare shifted adjacent to the next field.\nFor string keys, anchor keys are stored in a pointer array‚Äî anchors .\nThanks to feature comparison, branch operation would rarely ac-\ncess anchors. Therefore, unlike typical B+-trees that copy anchor\nkeys into inner nodes, FB+-tree stores the actual contents of anchor\nkeys in leaf nodes (i.e., high_key ), while inner nodes only maintain\npointers to high_key , which makes FB+-tree more space-efficient.\nWhenever possible, the entire common prefix is embedded in the\ntiny field5. Slab memory allocators, such as jemalloc and tcmalloc,\n5For the sake of concurrency support, the entire common prefix is embedded in an\ninner node, even if the parent and child nodes have exactly the same common prefix.always allocate a memory block not smaller than the required size.\nTherefore, we configure the size of tiny to fully utilize any excess\nmemory available. The huge field points to the first anchor as the\ncommon prefix in case it is too long to reside in tiny.\n3.4 Lookup and Update\nFigure 6 presents the slightly simplified code for branch in inner\nnodes and lookup in leaf nodes, without considering concurrency.\nThe lookup process in an FB+-tree is identical to that in a typical\nB+-tree. Index traversal utilizes the branch algorithm (lines 1‚àº28)\nto retrieve child nodes until a leaf node. Subsequently, the hash-\nbased lookup algorithm (lines 30 ‚àº42) is executed to retrieve the\nkey-value pair in the leaf node. Similar processes are used for binary\nkeys. The update process of FB+-tree has a minor difference from\nthe lookup process (lines 36 ‚àº38). An atomic update operation\nbased on CAS is employed without holding any locks.\nThe branch algorithm begins by comparing the target key with\nthe common prefix (line 3). The most significant difference between\nFB+-tree‚Äôs branch algorithm and typical binary search lies in the fea-\nture comparison (lines 7 ‚àº13). Each byte of the target key following\nthe prefix is compared with the corresponding byte in features (line\n9), until either the last feature is reached or there is no more match-\ning byte. No matching bytes in features means that the child node\ncould be determined immediately using the compare_less function\n(line 16). In cases where a binary search on suffixes is necessary, the\nbytes including both the common prefix and features are truncated\nto improve performance (line 23). The reason why 128 is added to\neach byte (lines 8 and 15) will be presented in Subsection 3.6.\nThe lookup algorithm employs a hash fingerprint in leaf nodes.\nCandidates are first filtered with hashtags (line 32), followed by\na verification comparison using the real content to prevent false\npositives (line 37). Lines 45 ‚àº49 show the implementation of com-\npare_equal using AVX512. BMI and SIMD instructions, such as\nLZCNT, are utilized for efficient bit manipulation, for instance,\nindex_least1 (line 18) and countl_zero (line 19).\n3.5 Insert and Remove\nA key-value insertion into a leaf node simply requires locating an\nempty slot using hashtags and then installing the key-value into kvs.\nFB+-tree adopts a bottom-up strategy for insertion that involves\nstructure modification. The algorithm for finding the position to\ninsert an anchor key into an inner node also relies on feature com-\nparison. The primary disparity lies in the recomputation of the\ncommon prefix and features. The common prefix of an inner node\nis recomputed only when the new anchor is less than the minimum\nanchor key. In most cases, an anchor key insertion could be easily\nconducted by inserting the pointer and features. Meanwhile, inner\nnodes are modified only during structure modifications. Remove\nfollows a similar process.\n3.6 Optimization and Tricks\nWe summarize some optimizations and technique tricks in FB+-tree:\nOptimization. As discussed in previous work, tree nodes of four\ncache lines exhibit the highest overall performance [ 49,67]. Modern\nmachines have nearly identical latency for 64-byte and 256-byte\nmemory accesses. For better single-threaded performance and space\n\n0123 8 54 64 24 0123 8 54 64 24\ndeleted sibling ordered reserved locked leaf deleted sibling ordered reserved locked leafsplitting version splitting version0123 8 54 64 24\ndeleted sibling ordered reserved locked leafsplitting versionFigure 7: The layout of optimistic lock variable ( control ).\nefficiency, the nsand fsare configured to 64 and 4, respectively.\nA smaller fanout increases the tree depth, while a larger feature\nsize entails more bandwidth requirements and incurs higher access\nlatency. A feature size of four ensures that feature comparison\nwould not lead to excessive overhead when it fails to determine the\nbranch. To minimize space consumption, the inner nodes only store\npointers to anchor keys (i.e., high_key in leaf nodes). The high_key\nserves as the upper bound of a leaf node and can be constructed\nusing discriminative prefixes to improve performance and space\nconsumption. Additionally, anchor keys in an inner node can also\nbe copied to contiguous memory to enhance locality.\nTricks. The relative magnitude between two unsigned integers\ncan be determined by byte-wise comparison. For signed integers,\nhowever, such a pattern doesn‚Äôt work because of its complement\nrepresentation. In essence, complement representation is designed\nto utilize the overflow bit to operate positive and negative integers\nuniformly. The relative magnitude among positive or negative inte-\ngers depends on the remaining bits except for the signed bit. For\nexample, in 8-bit complement representation, -2, -1, 0, 1, and 2 are\nrepresented as 0xFE, 0xFF, 0x00, 0x01, and 0x02, respectively. An un-\nsigned comparison on the integers whose sign bit is flipped can be\ntreated equivalently to a signed comparison on the original integers,\nand vice versa. Except for AVX512, the unsigned byte comparison\ninstructions are not supported in AVX2 and SSE2. Therefore, we\nadd a magic number 128 before feature comparison in Figure 6,\nmaking FB+-tree widely applicable.\n4 SYNCHRONIZATION PROTOCOL\nBesides cache utilization, the performance of a main-memory index\nsignificantly depends on its synchronization protocol. In this sec-\ntion, we begin by introducing lock-based synchronization protocols\nand their optimistic variants. Next, we present how FB+-tree is\nsynchronized using an optimistic protocol and an optimization that\nmitigates the overhead caused by synchronization protocol dur-\ning index traversal. In particular, we propose a general latch-free\nupdate technique that allows updates without holding any locks.\n4.1 Preliminaries to Index Synchronization\nIndex synchronization consists of two parts: node protection and\nconcurrent structure modification. Traditional database systems\ntypically use pessimistic lock for node protection, along with lock\ncoupling [ 3] or the link technique from Bùëôùëñùëõùëò-tree [ 38,40] for con-\ncurrent structure modification. In main-memory scenarios, opti-\nmistic lock combines version with lock to replace pessimistic lock,\nallowing index traversal without holding locks [ 14,42,44,49]. For\nstructure modification, optimistic lock coupling (OLC) keeps track\nof versions across multiple nodes and restarts if a change occurs\n[42,44]. OLFIT proceeds to sibling node when detecting a structure\nmodification through comparison with high key [14, 40].Optimistic lock offers a general approach to accessing a node\nwithout holding the lock, making it particularly beneficial for read-\nheavy workloads. However, update-heavy workloads under con-\ntention suffer from performance collapse in two aspects. First, ex-\nisting optimistic locks are typically implemented as spin locks with\noptimistic reads [ 28,42,44,63]. Writers must acquire an exclusive\nwrite lock via CAS instruction before modifying a node. This often\nleads to a scenario where many writers frequently retry CAS in\na single node until they hold the lock. A backoff algorithm may\nsomewhat mitigate this collapse, but not completely. Second, read-\ners usually have to wait for the writer‚Äôs modification and may have\nto restart if the version validation fails.\nOptiQL extended the classic MCS lock with optimistic reads\nto alleviate the former problem [ 50,63]. ROWEX mitigates the\nlatter problem by only providing exclusion relative among writ-\ners while allowing readers to always succeed without block nor\nrestart [ 42,44]. The Bw-tree, incorporating lock-free semantics\nthrough chaining delta records and mapping table, seems to be\nan ideal solution. Unfortunately, delta records require expensive\nmerge operations and the mapping table incurs additional over-\nhead for other operations. FB+-tree alleviates the former problem\nby minimizing the critical section to allow latch-free update opera-\ntion. Read operations thus can always succeed when concurrent\nwith updates, which eliminates the latter problem. We show their\ncomparisons when reads are concurrent with updates in Table 1.\nNext, we introduce FB+-tree‚Äôs synchronization protocol.\nTable 1: Comparisons of Synchronization Protocols\nOLC ROWEX OptiQL Bw-tree FB+-tree\nlatch-free update ‚úì ‚úì\nno merge overhead ‚úì ‚úì ‚úì ‚úì\nnon-blocking read ‚úì ‚úì ‚úì\nno auxiliary struct ‚úì ‚úì ‚úì ‚úì\n4.2 FB+-tree‚Äôs Synchronization Protocol\nFB+-tree utilizes a similar optimistic lock for node protection and\nlatch-free index traversal. Figure 7 illustrates the layout of per-node\noptimistic lock variable, denoted as control in Figure 5. Insert and\nremove operations increment the version , while update operations\ndo not, which differs from previous work. The splitting is only used\nin leaf nodes, indicating whether the node is undergoing a split\nand if the new node has not been inserted into its parent node. The\nordered field indicates whether the key-value pairs in leaf nodes are\nordered, which is lazily set only when either split and merge of leaf\nnodes or range iteration. The leaf specifies the type of node. The\nsibling indicates whether the node has a sibling node. The locked\nis set when acquiring an exclusive write lock. The deleted is set\nif the node‚Äôs contents have been merged into its left-sibling node,\nindicating that the node can be safely reclaimed later.\nNode Protection. Index traversal loads the version before access-\ning a node and validates that it has not changed after the access.\nIf validation fails, node access needs to restart. Both insert and\nremove operations acquire an exclusive write lock to prevent con-\ncurrent modification within the same node. Since read-only node\n\n1 KVPair* lookup(String& key) {\n2 void* node = root_;\n3 while (!is_leaf(node)) {\n4 node = InnerNode(node)->branch(key);\n5 }\n6 uint64_t version;\n7 do { // descending to leaf node\n8 version = Control(node)->begin_read();\n9 while (LeafNode(node)->to_sibling(key, node)) {\n10 version = Control(node)->begin_read();\n11 } // check whether to proceed to sibling node\n12 KVPair* kv = LeafNode(node)->lookup(key);\n13 if (kv != nullptr) return kv; // key found\n14 } while (!Control(node)->end_read(version));\n15 return nullptr; // key not found\n16}\nFigure 8: Concurrent Lookup Algorithm\naccess may occur concurrently with node modification, atomic op-\nerations are employed for insert and remove operations. Lookup\noperation atomically loads the pointer, thereby preventing readers\nfrom accessing a partially modified key-value.\nConcurrent Structure Modification. FB+-tree adopts the link tech-\nnique from Bùëôùëñùëõùëò-tree.6The top half of Figure 10 illustrates the\nprocess of concurrent node split.7A leaf node split involves two\nsteps: (1) transfer the greater half key-value pairs into the newly\ncreated node, denoted as n‚Ä≤; (2) insert the pointer to node n‚Ä≤into its\nparent node p. A leaf node nis said to be undergoing a split until\nthe pointer to the new node n‚Ä≤is inserted into its parent node.\nAfter step (2), an index traversal could correctly descend to node\nn‚Ä≤. An incorrect leaf node occurs only when an index traversal\ndescends to a leaf node that is undergoing a split. Given that the\nkey-values in node n‚Ä≤are greater than those in node n, it can be\naddressed through an alternative bypass by linking node nton‚Ä≤.\nTherefore, upon descending to a leaf node, a comparison is per-\nformed with the upper bound to determine whether it is necessary\nto proceed to the right sibling node. Split operations that propagate\nto higher-level nodes can be performed iteratively.\n4.3 Concurrent Lookup\nFigure 8 presents the concurrent lookup algorithm. The branch\nalgorithm (line 4) has been slightly modified to start by loading\ntheversion using begin_read and to restart if validation fails using\nend_read . Unlike its sequential version in Figure 6, the branch may\nreturn a sibling node if a node split occurs. Similarly, the lookup\nin leaf node (line 12) is protected by begin_read andend_read . To\navoid retrieving an incorrect leaf node, the to_sibling function (line\n9) performs a comparison with high_key upon descending to a leaf\nnode. If the key-value pair is found, the result is returned directly\nto eliminate unnecessary restarts (line 13).\nCross-Node Tracking. The overhead of comparison with high_key\nis negligible for traditional systems, but it may be expensive in\nmain-memory scenarios. Since structure modifications occur infre-\nquently, this comparison is unnecessary in most cases. We eliminate\nthis comparison overhead through a technique named cross-node\ntracking. Index traversal descends to an incorrect leaf node only\n6FB+-tree can also utilize lock coupling for concurrent structure modification. We\nemploy the link technique because it facilitates optimizations to latch-free update.\n7Concurrent node merge can be implemented similarly to node split [38].1node = index_traversal()\n2node->lock_exclusive()\n3... querying / validation\n4... install kv into kvs\n5node->unlock_exclusive()1node = index_traversal()\n2ver = node->begin_read()\n3... querying / validation\n4... install kv into kvs\n5node->end_read(ver)\nFigure 9: Lock-based (left) and latch-free update (right).\nwhen the node is undergoing a split. Additionally, a leaf node split\nends up after inserting the new anchor key into its parent node.\nWe thereby embed a splitting field in control indicating the node is\nundergoing a split, which is set when step (1) begins. Once the new\nanchor key is inserted into the parent node in step (2), the splitting\nfield is unset and the version of the parent node is incremented.\nThrough validating the splitting and the version of the parent node,\nit can be shown that index traversal descends to a correct leaf node.\nConsequently, the comparison is performed only when the splitting\nis set or the version of the parent node has changed.\n4.4 Latch-free Update\nWe propose a general latch-free update technique to achieve good\nscalability in heavy-contention scenarios. Figure 9 with the critical\nsections highlighted illustrates the key difference between the tra-\nditional lock-based approach and latch-free update. One primary\nreason for their sub-optimal scalability is that too many ancillary\noperations are protected inside the critical section. In previous op-\ntimistic protocols, a write lock is held to prevent other concurrent\nmodifications when reaching the leaf node. Unfortunately, querying\nthe node for the existing key-value pairs or validation process is\nalso protected in the critical section. Our latch-free update allows\nthis process to be executed in parallel. Only the process of installing\nkv into kvsis protected in the critical section.\nWhen only considering update and lookup, tree structure and\nkey-value residences in leaf nodes never change. Update operation\nthus can be easily implemented using CAS primitive. The challeng-\ning problem arises when another thread concurrently performs a\nsplit operation on the leaf node. As a result, the key-value to be\nreplaced might have been moved to another node. For example,\nconsider an update operation that tries to replace the key-value\n5-5 with 5-6 as shown in the bottom half of Figure 10. In this case,\nthe update descends to the leaf node and then stalls, while another\nthread performs a node split on this node. When the update re-\nsumes, it definitely fails because the key-value with key 5 cannot be\nfound. Similar issues exist in node merge and node rearrangement.\nOur latch-free update solves this false negative problem by check-\ning the version. If the version has not changed, the update fails\nbecause the key-value to be updated has not been inserted yet. Oth-\nerwise, if the version changes, it implies that the key-value may\nhave been moved to another node. In such cases, a comparison with\nhigh_key is then performed to determine whether the key-value\nis beyond the current node. If so, it proceeds to the sibling node\nto perform the update again, as illustrated in the bottom half of\nFigure 10. Otherwise, it indicates that either the node may have\nbeen rearranged or the key-value may have been removed. Restart\nin the leaf node could deal with these situations.\n\n1010\n55 ‚àû‚àû\n101010\n5 ‚àû\n1010\n5 ‚àû\n10n\nn'p\nl10\n5 ‚àû\n10n\nn'p\nl1010\n55 ‚àû‚àû\n101010\n5 ‚àû\n1010\n5 ‚àû\n10n\nn'p\nl10\n5 ‚àû\n10n\nn'p\nl\n55 1010105105\n‚àû‚àû 5 10105\n‚àûp\nn n' l\n5 10105\n‚àûp\nn n' l\n5 5 5 5 1 1 1 1 0 0 0 0 9 9 9 9 5 5 1 1 0 0 9 9version 4\n5 5 1 1 0 0 9 9version 4\n5 5 5 5 1 1 1 1 0 0 0 0 9 9 9 9 8 8 8 8 5 5 1 1 0 0 9 9 8 8version 4\n5 5 1 1 0 0 9 9 8 8version 4\n5 5 5 5 1 1 1 1 0 0 0 0 9 9 9 9 8 8 8 8 5 6 5 6 5 5 1 1 0 0 9 9 8 8 5 6version 5\n5 5 1 1 0 0 9 9 8 8 5 6version 5\nT2:  descend to leaf node; acquire a write \nlock and try to insert 8 -8T2: split and insert 8 -8 to \nthe sibling nodeT2: release the write lock and \nincrement the versionT1: descend to leaf node and lookup the \nkey-value pair of key 5T1: not found the key -value \npair of key 5T1: detect version change; proceed to the \nsibling node and update 5 -5 to 5 -6\nT2:  descend to leaf node; acquire a write \nlock and try to insert 8 -8T2: split and insert 8 -8 to \nthe sibling nodeT2: release the write lock and \nincrement the versionT1: descend to leaf node and lookup the \nkey-value pair of key 5T1: not found the key -value \npair of key 5T1: detect version change; proceed to the \nsibling node and update 5 -5 to 5 -6\nTime Seriesupdate -insert \ncoordination\n5 5 1 1 0 0 9 9version 4\n5 5 1 1 0 0 9 9 8 8version 4\n5 5 1 1 0 0 9 9 8 8 5 6version 5\nT2:  descend to leaf node; acquire a write \nlock and try to insert 8 -8T2: split and insert 8 -8 to \nthe sibling nodeT2: release the write lock and \nincrement the versionT1: descend to leaf node and lookup the \nkey-value pair of key 5T1: not found the key -value \npair of key 5T1: detect version change; proceed to the \nsibling node and update 5 -5 to 5 -6\nTime Seriesupdate -insert \ncoordination1010\n1010 ‚àû‚àû10\n10 ‚àûnp\nl10\n10 ‚àûnp\nlnode split10\n5 ‚àû\n10n\nn'p\nl10\n5 ‚àû\n10n\nn'p\nl\n5 10105\n‚àûp\nn n' l\n5 5 1 1 0 0 9 9version 4\n5 5 1 1 0 0 9 9 8 8version 4\n5 5 1 1 0 0 9 9 8 8 5 6version 5\nT2:  descend to leaf node; acquire a write \nlock and try to insert 8 -8T2: split and insert 8 -8 to \nthe sibling nodeT2: release the write lock and \nincrement the versionT1: descend to leaf node and lookup the \nkey-value pair of key 5T1: not found the key -value \npair of key 5T1: detect version change; proceed to the \nsibling node and update 5 -5 to 5 -6\nTime Seriesupdate -insert \ncoordination10\n10 ‚àûnp\nlnode splitFigure 10: Illustration of structure modification (top half) and update-insert coordination (bottom half, T1-update, T2-insert).\nTo coordinate with latch-free update when moving key-value\npairs to another node, we utilize atomic exchange instruction to\nreplace the pointer with nullptr and obtain the latest pointer to a\nkey-value. Concurrent updates would thus fail because they load a\nnullptr or fail to perform CAS, and then proceed to the sibling\nnode accordingly. Node merge and rearrangement coordinate with\nupdates similarly. Consequently, updates are performed in an al-\nmost non-blocking way, and lookups can be executed concurrently.\n4.5 Range Iteration\nIn FB+-tree, all the leaf nodes are linked in a totally ordered list. A\nconcurrent scan operation can be performed in two steps: (1) find\nthe starting point on the leaf node list; (2) sequentially iterate on the\nlist. The former is achieved using the upper_bound andlower_bound\nfunctions, and the latter is implemented with a concurrent iterator.\nLazy Rearrangement. Step (1) is akin to a lookup operation ex-\ncept lazy rearrangement. Since maintaining key-values in order\nis expensive for both lookup and insert, FB+-tree stores unsorted\nkey-values in leaf nodes. The ordered bit is embedded in control to\nindicate whether the key-values are in order. When descending to a\nleaf node, it checks the ordered bit. If unordered, it acquires a write\nlock and then rearranges the key-value pointers, allowing range\niteration to benefit from a sequential memory access pattern. Oth-\nerwise, it finds the start point without holding the lock. It should\nbe noted that lazy rearrangement incurs a small overhead, because\nover half of key-values are sorted during node split or merge.\nFB+-tree‚Äôs concurrent iterator in step (2) is almost identical to an\nSTL iterator. In concurrent environments, however, an iterator has\nto coordinate with insert and remove operations. FB+-tree‚Äôs iterator\nthus contains a version to detect whether any modifications occur\nin a leaf node. If the version changes during iteration, the ordered\nwould be checked and the node could be rearranged when necessary.\nThe iterator can thus access the newly inserted key-values.\nCross-Node Tracking. If a node split occurs when the iteration\ncrosses leaf nodes, the successor key-value is determined using\nbinary search. The cross-node tracking technique is employed to\ndetect if any structure modifications occur when crossing nodes\nduring range iteration. If no changes have occurred after proceeding\nto the sibling node, the iteration could optimistically access the\nminimum key-value for better performance.5 EXPERIMENT EVALUATION\nIn this section, we experimentally evaluate FB+-tree and compare\nit with other state-of-the-art main-memory index structures.\n5.1 Experiment Setup\nPlatform .We use a Dell PowerEdge R740 Server with two\nNUMA nodes. Each node contains an Intel Xeon(R) Gold 6248R\nprocessor with 24 3.0 GHz cores (with up to 48 hyperthreads). Each\nprocessor has 35.75 MB L3 cache and is equipped with 64 GB DDR4-\n2133 memory. We run Ubuntu 20.04 with kernel version 5.4.0. All\nour code is implemented with C++ 17 and compiled using GCC/G++\n11.4.0 with O3 optimization level. We use jemalloc to reduce dy-\nnamic memory allocation overhead at runtime. Threads are pinned\nto hardware threads to avoid migrations by the OS scheduler.\nIndexes .We compare FB+-tree with seven popular main-memory\nindex structures, including the variants of both B+-tree and trie:\n‚Ä¢STX B+-tree8: A highly optimized B+-tree container with im-\nproved memory fragmentation and cache efficiency [9].\n‚Ä¢FAST9: A read-only binary search tree that collapses multiple\nnodes into one large node to facilitate SIMD instructions [32].\n‚Ä¢B+-treeOLC10: A thread-safe B+-tree implementation synchro-\nnized by optimistic lock coupling [67].\n‚Ä¢ART: The default index of HyPer [ 31]. We use two thread-safe\nimplementations ARTOLC10and ARTOptiQL11, synchronized\nby optimistic lock coupling and OptiQL, respectively [63, 67].\n‚Ä¢HOT12: The Height Optimized Trie dynamically varies the num-\nber of discriminative bits considered in each node[ 10]. HOT\nutilizes the ROWEX protocol for synchronization [42, 44].\n‚Ä¢Masstree13: The Masstree is a trie-like concatenation of B+-tree\nused by silo [ 49,65]. It employs a customized optimistic lock\nprotocol along with the link technique for synchronization.\n‚Ä¢Wormhole14: The Wormhole substitutes inner nodes of B+-tree\nwith a trie structure and represents it as a hash table. It takes\nùëÇ(ùëôùëúùëî ùêø)worst-case time for querying a key of length ùêø[68].\n8https://github.com/tlx/tlx.git\n9https://github.com/RyanMarcus/fast64.git\n10https://github.com/wangziqi2016/index-microbench.git\n11https://github.com/sfu-dis/optiql\n12https://github.com/speedskater/hot.git\n13https://github.com/kohler/masstree-beta.git\n14https://github.com/wuxb45/wormhole.git\n\nIn STX B+-tree, B+-treeOLC, and FB+-tree, integer keys are stored\ndirectly in inner nodes, whereas string keys are stored via pointers.\nIn our experiments, all indexes maintain a pointer to each key-\nvalue. To avoid excessive compiler optimization, such as unused\nresult optimization, we compile the code of index structures into\na shared library. For FB+-tree, the nsandfsare configured to 64\nand 4 respectively, and AVX2 instructions are configured as default.\nOther indexes are configured to their default configurations. We do\nnot compare against learned indexes, as these hardly support insert\noperation. We also do not compare against hash tables, because\nthese do not support range iteration.\nWorkloads .Our experiments are based on the standard work-\nloads from the Yahoo! Cloud Serving Benchmark15(YCSB) [ 20].\nWe evaluate four core workloads with the default YCSB parame-\nters (requests follow the Zipfian distribution, skew=0.99): LOAD\n(100% insert), A (50% read, 50% update), C (100% read), and E (95%\nrange-scan, 5% insert). Our main concern is the index structure\nitself, so range scan only reads pointers without actually accessing\nthe records. Each workload consists of two phases: the load phase\ninserts 100 million keys in random order into indexes (one percent\nof keys are inserted for warmup); the run phase executes 100 million\noperations specified by the workload multiple times. We evaluate\neach workload on five datasets with different key lengths (Table\n2). The rand-int consists of random 64-bit integers. The 3-gram16\n[15] contains unique sequences of triple words often used in lan-\nguage models. We also use the default keys generated by YCSB. The\ntwitter17(cluster-27) is anonymized data by collecting real-world\nproduction traces from in-memory cache clusters at Twitter [ 69].\nThe urlconsists of distinct URLs in DBPedia dataset [2].\nTable 2: Datasets used in the experiments.\nName Description Avg. key size (bytes)\nrand-int 64-bit random integers 8.0\n3-gram unique sequences of triple words 15.8\nycsb keys generated by YCSB 22.9\ntwitter anonymized keys in cluster-27 52.7\nurl urls in DBPedia dataset 76.8\n5.2 Comparison against B+-tree Variants\nWe first measure the single-threaded performance of B+-tree vari-\nants to evaluate the structural optimization of FB+-tree. Figure 11\nshows the throughput of FB+-tree and two competitors, STX B+-tree\nand B+-treeOLC. On all workload-dataset combinations, FB+-tree\noutperforms the other two competitors‚Äîby up to 2.5x (LOAD), 2.9x\n(YCSB-A), 2.9x (YCSB-C), and 2.2x (YCSB-E). As presented in Sec-\ntion 3.2, the feature comparison technique significantly reduces\ncache misses and enables memory-level parallelism, leading to supe-\nrior single-threaded lookup, update, and scan performance. Besides\nfeature comparison, the unordered arrangement of key-values in\nFB+-tree contributes to efficient insert performance. Since FAST is\na read-only structure and only supports integer keys, we compare\nit to FB+-tree in a separate evaluation, as shown in Figure 1.\n15https://github.com/brianfrankcooper/YCSB/\n16https://www.statmt.org/lm-benchmark/\n17https://github.com/twitter/cache-trace\nrand-int 3-gram ycsb twitter url0.000.250.500.751.001.251.501.75LOAD\nrand-int 3-gram ycsb twitter url0.000.250.500.751.001.251.501.752.00YCSB-A\nrand-int 3-gram ycsb twitter url0.000.250.500.751.001.251.501.752.00YCSB-C\nrand-int 3-gram ycsb twitter url0.00.20.40.60.81.0YCSB-E (pointers)Million Operations per SecondFB+-treeB+-treeOLCSTX B+-treeFigure 11: Single-threaded throughput of B+-tree variants.\n3-gram ycsb twitter url010203040506070Million Operations per Second\n(a) Performance BreakdownB+-tree\n+prefix+hashtag\n+feature2+feature4\n+cross-track\n3-gram ycsb twitter url0246810GigaBytes\n(b) Memory ConsumptionART\nHOTB+-treeOLC\nFB+-treeMasstree\nWormhole\nFigure 12: (a) FB+-tree‚Äôs multi-threaded (48 cores) throughput\non workload YCSB-C by gradually enabling the optimiza-\ntions, and (b) Memory consumption of different indexes.\n5.3 Evaluation on Our Optimizations\nFactor Analysis on Structural Optimizations. We gradually\nenable the optimizations to evaluate the multi-threaded throughput\nof FB+-tree, as shown in Figure 12(a). We initially consider the\nB+-tree without any optimization, which employs binary search in\ninner and leaf nodes. Next, we enable prefix (denoted as +prefix) and\nhashtag (in leaf nodes, called +hashtag) in sequence to evaluate the\nstructural optimizations described in Section 3.3. The +prefix only\nstores the common prefixes of anchors directly in inner nodes, in\nwhich branch operation compares the target key with the common\nprefix and then performs a binary search on the suffix.\nIt even decreases the performance, since more cache lines need to\nbe loaded, as discussed in Section 3.1. We then evaluate the effects\nof the feature comparison technique (i.e., +feature2 and +feature4,\nby configuring two and four features, respectively), which improves\ncache utilization and exploits memory-level parallelism. Lastly, we\nenable the cross-node tracking optimization (denoted as +cross-\ntrack), which eliminates the overhead of accessing the high_key in\nleaf nodes, as detailed in Section 4.3. Since some optimizations in\nFigure 12(a) are not employed for binary keys, we do not evaluate\nthese on rand-int dataset. In conclusion, the multi-threaded (48\ncores) throughput of FB+-tree (+cross-track) on YCSB-C is higher\nthan that of typical B+-tree‚Äîby 2.3x ( 3-gram ), 2.9x ( ycsb), 2.6x\n(twitter ), 2.0x ( url), and 2.1x ( rand-int , as shown in Figure 17).\nTo deeply evaluate the impact of feature size, we also evaluate the\nmulti-threaded (48 cores) performance, average suffix comparison\n\n3-gram ycsb twitter url0204060Million Operations per Second\n(a) Performance3-gram ycsb twitter url051015Count per Operation\n(b) Suffix comparisons3-gram ycsb twitter url0246810Count per Operation\n(c) LLC misses1 2 4 6 8 12Figure 13: Evaluation with different feature size (YCSB-C).\n24816243240485664728088960255075100125150\n(a) Low contention (skew=0.90)2481624324048566472808896020406080100120\n(b) Medium contention (skew=0.99)2481624324048566472808896020406080\n(c) High contention (skew=1.10)Million Operations per SecondARTOLC HOTB+-treeOLCFB+-tree Masstree Wormhole ARTOptiQL\nFigure 14: Scalability under different skews (YCSB-A).\ncount per operation, and average LLC-misses per operation with\ndifferent feature sizes, as shown in Figure 13. Since the feature size\nof binary keys is fixed, we do not evaluate these on rand-int dataset.\nAlthough the average suffix comparison count per operation grad-\nually declines as feature size increases, the average LLC-misses per\noperation first decreases and then gradually increases, as shown\nin Figure 13(b)(c). This occurs because the mechanical selection\nof anchor keys is unconscious of prefix skewness, as discussed in\nSection 3.2. As a result, the performance of FB+-tree first increases\nwith feature size, then gradually decreases, as shown in Figure 13(a).\nIn addition, Figure 13 also illustrates the reason why FB+-tree‚Äôs\nperformance varies across different datasets. The twitter and url\ndatasets have more complicated prefix patterns leading to more\nsuffix comparisons and LLC-misses.\nScalability of Latch-free Update. First, we compare FB+-tree\nwith other state-of-the-art index structures to evaluate their scala-\nbility on rand-int dataset using YCSB-A (update-heavy) workload\nwith different skews. The results are shown in Figure 14. All index\nstructures are multi-core scalable under low contention. Thanks\nto its latch-free update and non-blocking read described in Sec-\ntion 4.1 and Section 4.4, FB+-tree is almost linearly scalable under\nmedian contention, while other index structures suffer from per-\nformance collapse. Due to retrying CAS on the pointers, FB+-tree\nalso experiences performance collapse under high contention. How-\never, it still maintains the best performance thanks to its minimal\nhardware-level critical section, as illustrated in Figure 14(c).\nNext, we horizontally compare the latch-free update with two\nother competitors, optimistic lock using CAS primitive (denoted as\nOptLock) and optimistic lock with backoff (called OptLock-Backoff).\nThe scalability of the three techniques on FB+-tree is illustrated in\nFigure 15. Due to space limitation, we only evaluate the rand-int and\nurldatasets (the highest and lowest performance on YCSB-C) with\nthe standard workload YCSB-A (skew=0.99). The OptLock suffers\nfrom scalability collapse over 48 threads on rand-int dataset and\nover 64 threads on urldataset, respectively. The OptLock with back-\noff algorithm could mitigate such performance collapse, however,\nleading to performance degradation with fewer threads. Our latch-\nfree update exhibits the best scalability and outperforms OptLock\nby 6.6x ( rand-int ) and 2.8x ( url) under 96 threads.5.4 Comparison against State-of-the-art\nWe compare FB+-tree with five state-of-the-art main-memory in-\ndexes to evaluate their scalability and performance in a concurrent\nenvironment. Meanwhile, we use the statistic interface of jemalloc\nto evaluate their space efficiency.\nMemory Consumption. Since these indexes utilize different\nkey-value storage formats, we only report the index memory con-\nsumption (the memory required by the index, including the pointers\nto key-values but excluding the key-values). The Masstree employs\na complicated design, in which key-values are not stored together\nand key slices are stored in its inner nodes. We count the whole\nmemory footprint and then subtract the memory footprint for stor-\ning key-values as Masstree‚Äôs memory consumption. The results on\n3-gram ,ycsb,twitter , and urldatasets are presented in Figure 12(b).\nThe results on rand-int dataset is consistent with results on 3-gram\ndataset. HOT only considers discriminative bits in inner nodes and\nthus is highly space-efficient. Except for HOT, FB+-tree is more\nspace-efficient than other indexes on almost all datasets.\nPerformance and Scalability. The throughput and scalability of\nthese indexes on all workload-dataset combinations are illustrated\nin Figure 17. On workload LOAD, FB+-tree outperforms all other\nindexes on all datasets. ARTOptiQL and HOT exhibit comparable\nscalability and performance on ycsb,twitter , and urldataets. Thanks\nto the latch-free update technique, FB+-tree shows the best scalabil-\nity on workload YCSB-A. All other indexes suffer from performance\ncollapse as threads increase. On the read-only workload YCSB-C,\nHOT gives the best performance and scalability on all datasets ex-\ncept rand-int. FB+-tree performs almost as fast as other trie-based\nstructures. As shown in Figure 16, the average LLC-misses and\nbranch misses per operation of all index structures are counted un-\nder a multi-threaded (48-core) environment as a shred of evidence.\nExcept for these two metrics, parallelism between instructions,\nmemory access patterns, and memory bandwidth utilization are\nalso significant, which leads to FB+-tree‚Äôs better performance than\n248 16 24 32 40 48 56 64 72 80 88 96020406080100120\n OptLock\nOptLock-Backoff\nLatch-Free\n248 16 24 32 40 48 56 64 72 80 88 960102030405060\nOptLock\nOptLock-Backoff\nLatch-FreeMillion Operations per Second\nThreadsrand-int url\nFigure 15: Scalability on rand-int and url datasets (YCSB-A).\n3-gram ycsb twitter url0510152025\n(a) LLC misses3-gram ycsb twitter url0510152025\n(b) Branch missesCount per OperationART HOTB+-treeOLCFB+-tree Masstree Wormhole\nFigure 16: Hardware events count on workload YCSB-C.\n\n2481624324048566472808896020406080\n2481624324048566472808896020406080100120\n24816243240485664728088960255075100125150175\n24816243240485664728088960102030405060\n248162432404856647280889605101520\n24816243240485664728088960102030405060\n2481624324048566472808896020406080\n2481624324048566472808896020406080100120\n2481624324048566472808896010203040\n24816243240485664728088960.02.55.07.510.012.515.0\n24816243240485664728088960102030405060\n2481624324048566472808896020406080100\n2481624324048566472808896020406080100120140\n248162432404856647280889601020304050\n24816243240485664728088960.02.55.07.510.012.515.017.5\n248162432404856647280889601020304050\n2481624324048566472808896020406080\n2481624324048566472808896020406080100\n2481624324048566472808896010203040\n24816243240485664728088960.02.55.07.510.012.515.0\n2481624324048566472808896010203040\n24816243240485664728088960102030405060\n2481624324048566472808896020406080\n2481624324048566472808896010203040\n2481624324048566472808896024681012\nMillion Operations per Second\nThreadsrand-int 3-gram ycsb twitter urlLOAD YCSB-A YCSB-C YCSB-E (pointers) YCSB-E (with kvs)ARTOLC HOT B+-treeOLCFB+-tree Masstree Wormhole ARTOptiQLFigure 17: Index throughput and scalability on different workload-dataset combinations.\nART and Mastree. It should be mentioned that all these structures\nperform lookup without holding any locks.\nThe performance of range scan may be vitally important in many\ndatabase applications, especially when secondary indexes are ex-\ntensively utilized to retrieve valuable information and translate\ndatabase operators into computations based on primary keys. In\nthese scenarios, a balanced structure may be extremely effective.\nFB+-tree exhibits superior performance than other structures on\nworkload YCSB-E thanks to its balanced structure and sequential\npointer arrangement as described in Section 4.5. Although Worm-\nhole has a similar leaf node structure, its indirect ordered key-value\narrangement hinders efficient range scan. Frequent pointer chasing\nin trie-based index structures leads to inferior range scan perfor-\nmance. For completeness, we also illustrate the range scan per-\nformance with the actual key-value records access. In summary,\nthe experiments demonstrate that FB+-tree dominates existing B+-\ntree variants on all the workload-dataset combinations. Compared\nwith trie-based structures, FB+-tree may have lower performance\non point lookup, while it has a big advantage on range scan. On\nupdate-heavy workloads, FB+-tree also demonstrates significant\npotential and better scalability over other structures.6 CONCLUSION\nIn this paper, we present the FB+-tree, a fast, cache-efficient, and\nbalanced B+-tree variant taking memory-level and computational\nparallelism into consideration. We show how to reduce cache misses\nin binary search and exploit prefetching to leverage memory-level\nparallelism. We highlight our feature comparison technique en-\nabling vectorization of binary search from a different perspective\nthan previous work. The evaluation results demonstrate FB+-tree\nexhibits comparable point lookup performance to state-of-the-art\nindexes, while exhibiting superior range scan performance. We\nsincerely believe that mitigating dependences and exploring the\npossibility of parallelization and vectorization would be increas-\ningly important to improve the performance of existing algorithms.\nACKNOWLEDGMENTS\nThis research was sponsored by the National Key Research and\nDevelopment Program of China (No. 2023YFC3321304), the National\nScience Foundation of China (No. 61572373), and CCF-Huawei\nPopulus Grove Fund (No. CCF-HuaweiDB202410). We thank the\nanonymous reviewers for their valuable comments.\n\nREFERENCES\n[1]Nikolas Askitis and Ranjan Sinha. 2007. HAT-Trie: A Cache-Conscious Trie-\nBased Data Structure For Strings. In ACSC (CRPIT) , Vol. 62. Australian Computer\nSociety, 97‚Äì105.\n[2] S√∂ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,\nand Zachary G. Ives. 2007. DBpedia: A Nucleus for a Web of Open Data. In\nISWC/ASWC (Lecture Notes in Computer Science) , Vol. 4825. Springer, 722‚Äì735.\n[3] Rudolf Bayer and Mario Schkolnick. 1977. Concurrency of Operations on B-Trees.\nActa Informatica 9 (1977), 1‚Äì21.\n[4]Rudolf Bayer and Karl Unterauer. 1977. Prefix B-Trees. ACM Trans. Database\nSyst. 2, 1 (1977), 11‚Äì26.\n[5] Michael A. Bender, Roozbeh Ebrahimi, Haodong Hu, and Bradley C. Kuszmaul.\n2016. B-Trees and Cache-Oblivious B-Trees with Different-Sized Atomic Keys.\nACM Trans. Database Syst. 41, 3 (2016), 19:1‚Äì19:33.\n[6] Michael A. Bender, Martin Farach-Colton, and Bradley C. Kuszmaul. 2006. Cache-\noblivious string B-trees. In PODS . ACM, 233‚Äì242.\n[7] Michael A. Bender, Jeremy T. Fineman, Seth Gilbert, and Bradley C. Kuszmaul.\n2005. Concurrent cache-oblivious b-trees. In SPAA . ACM, 228‚Äì237.\n[8] Michael A. Bender, Haodong Hu, and Bradley C. Kuszmaul. 2010. Performance\nguarantees for B-trees with different-sized atomic keys. In PODS . ACM, 305‚Äì316.\n[9]Timo Bingmann. 2018. TLX: Collection of Sophisticated C++ Data Structures,\nAlgorithms, and Miscellaneous Helpers. https://panthema.net/tlx, retrieved Oct.\n7, 2020.\n[10] Robert Binna, Eva Zangerle, Martin Pichl, G√ºnther Specht, and Viktor Leis. 2018.\nHOT: A Height Optimized Trie Index for Main-Memory Database Systems. In\nSIGMOD Conference . ACM, 521‚Äì534.\n[11] Robert Binna, Eva Zangerle, Martin Pichl, G√ºnther Specht, and Viktor Leis. 2022.\nHeight Optimized Tries. ACM Trans. Database Syst. 47, 1 (2022), 3:1‚Äì3:46.\n[12] Philip Bohannon, Peter McIlroy, and Rajeev Rastogi. 2001. Main-Memory Index\nStructures with Fixed-Size Partial Keys. In SIGMOD Conference . ACM, 163‚Äì174.\n[13] Matthias B√∂hm, Benjamin Schlegel, Peter Benjamin Volk, Ulrike Fischer, Dirk\nHabich, and Wolfgang Lehner. 2011. Efficient In-Memory Indexing with Gener-\nalized Prefix Trees. In BTW (LNI) , Vol. P-180. GI, 227‚Äì246.\n[14] Sang Kyun Cha, Sangyong Hwang, Kihong Kim, and Keunjoo Kwon. 2001. Cache-\nConscious Concurrency Control of Main-Memory Indexes on Shared-Memory\nMultiprocessor Systems. In VLDB . Morgan Kaufmann, 181‚Äì190.\n[15] Ciprian Chelba, Tom√°s Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp\nKoehn, and Tony Robinson. 2014. One billion word benchmark for measuring\nprogress in statistical language modeling. In INTERSPEECH . ISCA, 2635‚Äì2639.\n[16] Guanduo Chen, Meng Li, Siqiang Luo, and Zhenying He. 2024. Oasis: An Optimal\nDisjoint Segmented Learned Range Filter. Proc. VLDB Endow. 17, 8 (2024), 1911‚Äì\n1924.\n[17] Shimin Chen, Phillip B. Gibbons, and Todd C. Mowry. 2001. Improving Index\nPerformance through Prefetching. In SIGMOD Conference . ACM, 235‚Äì246.\n[18] Shimin Chen, Phillip B. Gibbons, Todd C. Mowry, and Gary Valentin. 2002. Fractal\nprefetching B¬±Trees: optimizing both cache and disk performance. In SIGMOD\nConference . ACM, 157‚Äì168.\n[19] Douglas Comer. 1979. The Ubiquitous B-Tree. ACM Comput. Surv. 11, 2 (1979),\n121‚Äì137.\n[20] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell\nSears. 2010. Benchmarking cloud serving systems with YCSB. In SoCC . ACM,\n143‚Äì154.\n[21] Intel Corporation. 2023. Intel¬Æ64 and IA-32 Architectures Optimization Reference\nManual . https://cdrdv2.intel.com/v1/dl/getContent/779559?fileName=355308-\nSoftware-Optimization-Manual-048-Changes-Doc-2.pdf\n[22] Intel Corporation. 2023. Intel¬Æ64 and IA-32 Architectures Software Developer‚Äôs\nManual . https://cdrdv2.intel.com/v1/dl/getContent/789583?fileName=325462-\nsdm-vol-1-2abcd-3abcd-4.pdf\n[23] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In SIGMOD Conference . ACM, 969‚Äì984.\n[24] Ronald G. Dreslinski, Ali G. Saidi, Trevor N. Mudge, and Steven K. Reinhardt.\n2007. Analysis of hardware prefetching across virtual page boundaries. In Conf.\nComputing Frontiers . ACM, 13‚Äì22.\n[25] Babak Falsafi and Thomas F. Wenisch. 2014. A Primer on Hardware Prefetching .\nMorgan & Claypool Publishers.\n[26] Goetz Graefe, Hideaki Kimura, and Harumi A. Kuno. 2012. Foster b-trees. ACM\nTrans. Database Syst. 37, 3 (2012), 17:1‚Äì17:29.\n[27] Goetz Graefe and Per-√Öke Larson. 2001. B-Tree Indexes and CPU Caches. In\nICDE . IEEE Computer Society, 349‚Äì358.\n[28] Rachid Guerraoui and Vasileios Trigonakis. 2016. Optimistic concurrency with\nOPTIK. In PPoPP . ACM, 18:1‚Äì18:12.\n[29] Steffen Heinz, Justin Zobel, and Hugh E. Williams. 2002. Burst tries: a fast,\nefficient data structure for string keys. ACM Trans. Inf. Syst. 20, 2 (2002), 192‚Äì\n223.\n[30] Tim Kaldewey, Jeff Hagen, Andrea Di Blas, and Eric Sedlar. 2009. Parallel search\non video cards. In Proceedings of the First USENIX Conference on Hot Topics inParallelism (HotPar‚Äô09) . USENIX Association, 9.\n[31] Alfons Kemper and Thomas Neumann. 2011. HyPer: A hybrid OLTP&OLAP\nmain memory database system based on virtual memory snapshots. In ICDE .\nIEEE Computer Society, 195‚Äì206.\n[32] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D.\nNguyen, Tim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey.\n2010. FAST: fast architecture sensitive tree search on modern CPUs and GPUs.\nInSIGMOD Conference . ACM, 339‚Äì350.\n[33] Minsu Kim, Jinwoo Hwang, Guseul Heo, Seiyeon Cho, Divya Mahajan, and\nJongse Park. 2024. Accelerating String-key Learned Index Structures via\nMemoization-based Incremental Training. Proc. VLDB Endow. 17, 8 (2024), 1802‚Äì\n1815.\n[34] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: a single-pass learned\nindex. In aiDM@SIGMOD . ACM, 5:1‚Äì5:5.\n[35] Yusuf Onur Ko√ßberber, Boris Grot, Javier Picorel, Babak Falsafi, Kevin T. Lim,\nand Parthasarathy Ranganathan. 2013. Meet the walkers: accelerating index\ntraversals for in-memory databases. In MICRO . ACM, 468‚Äì479.\n[36] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD Conference . ACM, 489‚Äì504.\n[37] Yongsik Kwon, Seonho Lee, Yehyun Nam, Joong Chae Na, Kunsoo Park, Sang K.\nCha, and Bongki Moon. 2023. DB+-tree: A new variant of B+-tree for main-\nmemory database systems. Inf. Syst. 119 (2023), 102287.\n[38] Vladimir Lanin and Dennis E. Shasha. 1986. A Symmetric Concurrent B-Tree\nAlgorithm. In FJCC . IEEE Computer Society, 380‚Äì389.\n[39] Jaekyu Lee, Hyesoon Kim, and Richard W. Vuduc. 2012. When Prefetching\nWorks, When It Doesn‚Äôt, and Why. ACM Trans. Archit. Code Optim. 9, 1 (2012),\n2:1‚Äì2:29.\n[40] Philip L. Lehman and S. Bing Yao. 1981. Efficient Locking for Concurrent Opera-\ntions on B-Trees. ACM Trans. Database Syst. 6, 4 (1981), 650‚Äì670.\n[41] Tobin J. Lehman and Michael J. Carey. 1986. A Study of Index Structures for\nMain Memory Database Management Systems. In VLDB . Morgan Kaufmann,\n294‚Äì303.\n[42] Viktor Leis, Michael Haubenschild, and Thomas Neumann. 2019. Optimistic Lock\nCoupling: A Scalable and Efficient General-Purpose Synchronization Method.\nIEEE Data Eng. Bull. 42, 1 (2019), 73‚Äì84.\n[43] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive radix\ntree: ARTful indexing for main-memory databases. In ICDE . IEEE Computer\nSociety, 38‚Äì49.\n[44] Viktor Leis, Florian Scheibner, Alfons Kemper, and Thomas Neumann. 2016. The\nART of practical synchronization. In DaMoN . ACM, 3:1‚Äì3:8.\n[45] Justin J. Levandoski, David B. Lomet, and Sudipta Sengupta. 2013. The Bw-Tree:\nA B-tree for new hardware platforms. In ICDE . IEEE Computer Society, 302‚Äì313.\n[46] Guoliang Li, Xuanhe Zhou, and Lei Cao. 2021. AI Meets Database: AI4DB and\nDB4AI. In SIGMOD Conference . ACM, 2859‚Äì2866.\n[47] Jihang Liu, Shimin Chen, and Lujun Wang. 2020. LB+-Trees: Optimizing Persis-\ntent Index Performance on 3DXPoint Memory. Proc. VLDB Endow. 13, 7 (2020),\n1078‚Äì1090.\n[48] Baotong Lu, Jialin Ding, Eric Lo, Umar Farooq Minhas, and Tianzheng Wang.\n2021. APEX: A High-Performance Learned Index on Persistent Memory. Proc.\nVLDB Endow. 15, 3 (2021), 597‚Äì610.\n[49] Yandong Mao, Eddie Kohler, and Robert Tappan Morris. 2012. Cache craftiness\nfor fast multicore key-value storage. In EuroSys . ACM, 183‚Äì196.\n[50] John M. Mellor-Crummey and Michael L. Scott. 1991. Algorithms for Scalable\nSynchronization on Shared-Memory Multiprocessors. ACM Trans. Comput. Syst.\n9, 1 (1991), 21‚Äì65.\n[51] Donald R. Morrison. 1968. PATRICIA - Practical Algorithm To Retrieve Informa-\ntion Coded in Alphanumeric. J. ACM 15, 4 (1968), 514‚Äì534.\n[52] Chris Nyberg, Tom Barclay, Zarka Cvetanovic, Jim Gray, and David B. Lomet.\n1995. AlphaSort: A Cache-Sensitive Parallel External Sort. VLDB J. 4, 4 (1995),\n603‚Äì627.\n[53] Ismail Oukid, Johan Lasperas, Anisoara Nica, Thomas Willhalm, and Wolfgang\nLehner. 2016. FPTree: A Hybrid SCM-DRAM Persistent and Concurrent B-Tree\nfor Storage Class Memory. In SIGMOD Conference . ACM, 371‚Äì386.\n[54] William W. Pugh. 1990. Skip Lists: A Probabilistic Alternative to Balanced Trees.\nCommun. ACM 33, 6 (1990), 668‚Äì676.\n[55] Jun Rao and Kenneth A. Ross. 1999. Cache Conscious Indexing for Decision-\nSupport in Main Memory. In VLDB . Morgan Kaufmann, 78‚Äì89.\n[56] Jun Rao and Kenneth A. Ross. 2000. Making B+-Trees Cache Conscious in Main\nMemory. In SIGMOD Conference . ACM, 475‚Äì486.\n[57] Steve Scargall. 2020. Programming Persistent Memory: A Comprehensive Guide\nfor Developers . Springer Nature.\n[58] Benjamin Schlegel, Rainer Gemulla, and Wolfgang Lehner. 2009. k-ary search\non modern processors. In DaMoN . ACM, 52‚Äì60.\n[59] Josef Schmei√üer, Maximilian E. Sch√ºle, Viktor Leis, Thomas Neumann, and\nAlfons Kemper. 2021. B2-Tree: Cache-Friendly String Indexing within B-Trees.\nInBTW (LNI) , Vol. P-311. Gesellschaft f√ºr Informatik, Bonn, 39‚Äì58.\n\n[60] Jason Sewall, Jatin Chhugani, Changkyu Kim, Nadathur Satish, and Pradeep\nDubey. 2011. PALM: Parallel Architecture-Friendly Latch-Free Modifications to\nB+ Trees on Many-Core Processors. Proc. VLDB Endow. 4, 11 (2011), 795‚Äì806.\n[61] Amirhesam Shahvarani and Hans-Arno Jacobsen. 2016. A Hybrid B+-tree as Solu-\ntion for In-Memory Indexing on CPU-GPU Heterogeneous Computing Platforms.\nInSIGMOD Conference . ACM, 1523‚Äì1538.\n[62] Tomer Shanny and Adam Morrison. 2022. Occualizer: Optimistic Concurrent\nSearch Trees From Sequential Code. In OSDI . USENIX Association, 321‚Äì337.\n[63] Ge Shi, Ziyi Yan, and Tianzheng Wang. 2023. OptiQL: Robust Optimistic Locking\nfor Memory-Optimized Indexes. Proc. ACM Manag. Data 1, 3 (2023), 216:1‚Äì216:26.\n[64] Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas,\nand Tim Kraska. 2021. Bounding the Last Mile: Efficient Learned String Indexing.\nCoRR abs/2111.14905 (2021).\n[65] Stephen Tu, Wenting Zheng, Eddie Kohler, Barbara Liskov, and Samuel Madden.\n2013. Speedy transactions in multicore in-memory databases. In SOSP . ACM,\n18‚Äì32.\n[66] Zhenlin Wang, Doug Burger, Steven K. Reinhardt, Kathryn S. McKinley, and\nCharles C. Weems. 2003. Guided Region Prefetching: A Cooperative Hardware/-\nSoftware Approach. In ISCA . IEEE Computer Society, 388‚Äì398.\n[67] Ziqi Wang, Andrew Pavlo, Hyeontaek Lim, Viktor Leis, Huanchen Zhang,\nMichael Kaminsky, and David G. Andersen. 2018. Building a Bw-Tree Takes\nMore Than Just Buzz Words. In SIGMOD Conference . ACM, 473‚Äì488.\n[68] Xingbo Wu, Fan Ni, and Song Jiang. 2019. Wormhole: A Fast Ordered Index for\nIn-memory Data Management. In EuroSys . ACM, 18:1‚Äì18:16.[69] Juncheng Yang, Yao Yue, and K. V. Rashmi. 2020. A large scale analysis of\nhundreds of in-memory cache clusters at Twitter. In OSDI . USENIX Association,\n191‚Äì208.\n[70] Yifan Yang and Shimin Chen. 2024. LITS: An Optimized Learned Index for\nStrings. Proc. VLDB Endow. 17, 11 (2024), 3415‚Äì3427.\n[71] Adar Zeitak and Adam Morrison. 2021. Cuckoo Trie: Exploiting Memory-Level\nParallelism for Efficient DRAM Indexing. In SOSP . ACM, 147‚Äì162.\n[72] Huanchen Zhang, David G. Andersen, Andrew Pavlo, Michael Kaminsky, Lin\nMa, and Rui Shen. 2016. Reducing the Storage Overhead of Main-Memory OLTP\nDatabases with Hybrid Indexes. In SIGMOD Conference . ACM, 1567‚Äì1581.\n[73] Jiaoyi Zhang, Kai Su, and Huanchen Zhang. 2024. Making In-Memory Learned\nIndexes Efficient on Disk. Proc. ACM Manag. Data 2, 3 (2024), 151.\n[74] Shunkang Zhang, Ji Qi, Xin Yao, and Andr√© Brinkmann. 2024. Hyper: A High-\nPerformance and Memory-Efficient Learned Index via Hybrid Construction. Proc.\nACM Manag. Data 2, 3 (2024), 145.\n[75] Weihua Zhang, Chuanlei Zhao, Lu Peng, Yuzhe Lin, Fengzhe Zhang, and Yun-\nping Lu. 2023. Boosting Performance and QoS for Concurrent GPU B+trees by\nCombining-Based Synchronization. In PPoPP . ACM, 1‚Äì13.\n[76] Rong Zhu, Lianggui Weng, Wenqing Wei, Di Wu, Jiazhen Peng, Yifan Wang,\nBolin Ding, Defu Lian, Bolong Zheng, and Jingren Zhou. 2024. PilotScope:\nSteering Databases with Machine Learning Drivers. Proc. VLDB Endow. 17, 5\n(2024), 980‚Äì993.",
  "textLength": 87751
}