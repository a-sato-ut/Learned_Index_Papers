{
  "paperId": "ffbef652d06df59e7534e83feafc206024af08f3",
  "title": "On the Power of Learning-Augmented Search Trees",
  "pdfPath": "ffbef652d06df59e7534e83feafc206024af08f3.pdf",
  "text": "On the Power of Learning-Augmented Search Trees\nJingbang Chen* 1Xinyuan Cao* 2Alicia Stepin3Li Chen4\nAbstract\nWe study learning-augmented binary search trees\n(BSTs) via Treaps with carefully designed priori-\nties. The result is a simple search tree in which the\ndepth of each item xis determined by its predicted\nweight wx. Specifically, each item xis assigned a\ncomposite priority of −⌊log log(1 /wx)⌋+U(0,1)\nwhere U(0,1)is the uniform random variable.\nBy choosing wxas the relative frequency of x,\nthe resulting search trees achieve static optimal-\nity. This approach generalizes the recent learning-\naugmented BSTs [Lin-Luo-Woodruff ICML ’22],\nwhich only work for Zipfian distributions, by ex-\ntending them to arbitrary input distributions. Fur-\nthermore, we demonstrate that our method can\nbe generalized to a B-Tree data structure using\nthe B-Treap approach [Golovin ICALP ’09]. Our\nsearch trees are also capable of leveraging local-\nities in the access sequence through online self-\nreorganization, thereby achieving the working-set\nproperty. Additionally, they are robust to predic-\ntion errors and support dynamic operations, such\nas insertions, deletions, and prediction updates.\nWe complement our analysis with an empirical\nstudy, demonstrating that our method outperforms\nprior work and classic data structures.\n1. Introduction\nThe development of machine learning has sparked signifi-\ncant interest in its potential to enhance traditional data struc-\ntures. First proposed by Kraska et al. (2018), the notion of\nlearned index has gained much attention since then (Kraska\net al., 2018; Ding et al., 2020; Ferragina & Vinciguerra,\n2020). Algorithms with predictions have also been devel-\noped for an increasingly wide range of problems, includ-\ning shortest path (Chen et al., 2022a), network flow (Po-\n*Equal contribution1University of Waterloo2Georgia Institute\nof Technology3Carnegie Mellon University4Independent; Part of\nthe work by Jingbang Chen and Li Chen was done while at Georgia\nTech. Correspondence to: Li Chen <lichenntu@gmail.com >.\nProceedings of the 42ndInternational Conference on Machine\nLearning , Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).lak & Zub, 2024; Lavastida et al., 2021), matching (Chen\net al., 2022a; Dinitz et al., 2021; Aamand et al., 2022),\nspanning tree (Erlebach et al., 2022), and triangles/cycles\ncounting (Chen et al., 2022b), with the goal of obtaining\nalgorithms that get near-optimal performances when the\npredictions are good, but also recover prediction-less worst-\ncase behavior when predictions have large errors (Mitzen-\nmacher & Vassilvitskii, 2022).\nThe problem of using learning to accelerate search trees,\nas in the original learned index question, has been widely\nstudied in the field of data structures, focusing on develop-\ning data structures optimal to the input sequence. Mehlhorn\n(1975a) showed that a nearly optimal static tree can be\nconstructed in linear time when exact key frequencies are\nprovided. Extensive work on this topic culminated in the\nstudy of dynamic optimality. Tango trees (Demaine et al.,\n2007) achieve a competitive ratio of O(log log n)while\nsplay trees (Sleator & Tarjan, 1985) and Greedy BSTs (Lu-\ncas, 1988; Munro, 2000; Demaine et al., 2009) are conjec-\ntured to be within constant factors of optimal.\nTreaps, introduced by Aragon & Seidel (1989), is a class of\nbalanced BSTs distinguished by its use of randomization to\nmaintain a low tree height. Each node in a Treap is assigned\nnot only a key but also a randomly generated priority value.\nThis design enables Treaps to satisfy the Heap property,\nensuring that every node has a lower priority than its parent.\nIn general, Treaps use randomness to ensure a low height\ninstead of balancing the tree preemptively. More recently,\nLin et al. (2022) introduced a learning-augmented Treap,\ndemonstrating stronger guarantees compared to traditional\nTreaps. However, it relies on the strong assumption of the\nZipfian distribution.\nInspired by this line of work, our research is driven by a\nseries of critical questions.\n•Whether a more general learning-augmented BST\nexists and achieves static optimality?\n•Can such BST also obtain good guarantees under the\ndynamic settings?\n•Are they robust to the errors caused by the prediction\noracles?\n1arXiv:2211.09251v3  [cs.DS]  15 May 2025\n\nOn the Power of Learning-Augmented Search Trees\nThis paper addresses the questions affirmatively by develop-\ning new learning-augmented Treaps with carefully designed\npriority scores, which are applicable to arbitrary input dis-\ntributions in both static and dynamic settings. In the static\nsetting, we show that our learning-augmented Treaps are\nwithin a constant factor of the static optimal cost when incor-\nporating a prediction oracle for the frequency of each item.\nThe proposed Treaps are robust to predicted errors, where\nthe additional cost induced by the inaccurate prediction\ngrows linearly with the KL divergence between the rela-\ntive frequency and its estimation. For the dynamic setting,\nwhere the trees can undergo changes after each access, we\nshow that given a prediction oracle for the time interval until\nthe next access, our data structure can achieve the working-\nset bound. This bound can be viewed as a strengthening\nof the static optimality bound that takes temporal locality\nof keys into account. Such dynamic BSTs are robust to\nthe prediction oracle as well, where the performance de-\ngrades smoothly with the mean absolute error between the\nlogarithm of the generated priorities and the ground truth\npriorities. Additionally, under the external memory model,\nour learning-augmented BST can be naturally extended to a\nB-Tree version via B-Treaps. Experimental results demon-\nstrate that the proposed Treap outperforms both traditional\ndata structures and other learning-augmented data structures,\neven when the predictions are inaccurate.\n1.1. Overview\nLearning-Augmented Treaps via Composite Priority\nFunctions. The Treap is a tree-balancing mechanism ini-\ntially designed around randomized priorities (Aragon &\nSeidel, 1989). When the priorities are assigned randomly,\nthe resulting tree is balanced with high probability. Intu-\nitively, this is because the root is likely to be picked among\nthe middle elements. However, if some node is accessed\nvery frequently (e.g. 10% of the time), it’s natural to assign\nit a larger priority. Therefore, setting the priority to be a\nfunction of access frequencies, as in Lin et al. (2022), is a\nnatural way to obtain an algorithm more efficient on more\nskewed access patterns. However, when the priority is set\nexactly as the access frequency, some nodes would have\nsuper-logarithmic depth: if each element iis accessed i\ntimes, setting priority exactly as the frequencies results in\na path of size n.The total time for processing this access\nsequence of size O(n2)degrades to Ω(n3).Partly as a result\nof this, the analysis in Lin et al. (2022) was limited only to\nwhen frequencies are under the Zipfian distribution.\nBuilding upon these ideas, we introduce a composite prior-\nity function, a mixture of the randomized priority function\nfrom Aragon & Seidel (1989) and the frequency-based pri-\nority function from Lin et al. (2022). This takes advantage\nof the balance coming from the randomness and manages to\nwork without the strong assumption from Lin et al. (2022).Specifically, we show in Theorem 2.4 that by setting the\ncomposite priority function to be\n−\u0016\nlog log1\nwx\u0017\n+U(0,1), (1)\nthe expected depth of node xisO(log(1 /wx)). The pre-\ndicted score wx∈(0,1), for instance, can be set as the\nrelative frequency or probability of each item.\nOur Treap-based scheme generalizes to B-Trees, where each\nnode has Binstead of 2children. These trees are highly\nimportant in external memory systems due to the behavior\nof cache performances: accessing a block of Bentries has a\ncost comparable to the cost of accessing O(1)entries. By\ncombining the B-Treaps (Golovin, 2009) with the composite\npriorities, we introduce a new learning-augmented B-Tree\nthat achieves similar bounds under the External Memory\nModel . We show in Theorem 3.1 that for any weights over\nelements w, by setting the priority to\n−⌊log2logB1\nwx⌋+U(0,1), (2)\nthe expected depth of node xisO(logB(1/wx)). It is natu-\nral to see that our proposed data structures unify BSTs and\nB-Trees. For simplicity, we provide the results of B-Trees\nin the remaining content.\nStatic Optimality of Learning-Augmented Search Trees.\nWe can construct static optimal B-Trees if we set wto be\nthe marginal distribution of elements in the access sequence.\nThat is, if we know the frequencies f1, f2, . . . , f nof each\nelement that appears in the access sequence, and let m=P\nifito be the length of the access sequence, then we set the\nscore wx=fx/min Equation (2) and the corresponding B-\nTree has a total access cost that achieves the static optimality\nX\ni∈[n]filogBm\nfi.\nDynamic Learning-Augmented Search Trees. We also\nconsider the dynamic setting in which we continually update\nthe priorities of a subset of items along with the sequence\naccess. Rather than a fixed priority for each item, we allow\nthe priorities to change as keys get accessed. The setting has\na wide range of applications in the real world. For instance,\nconsider accessing data in a relational database. A sequence\nof access will likely access related items one after another.\nSo even if the entries themselves get accessed with fixed\nfrequencies, the distribution of the next item to be accessed\ncan be highly dependent on the set of recently accessed\nitems. Consider the access sequence\n4,2,3,4,5,2,3,4,5,2,3,4,5,1,4\nversus the access sequence\n5,2,4,2,1,4,4,5,3,3,3,4,5,4,2\n2\n\nOn the Power of Learning-Augmented Search Trees\nLearning \nOracle!∈ℝ\"!!\n!\"\n!#Search Tree  priority *= −log log (1/2 #) + 5(0,1)depth 3 =\n<(log(1/2 $))Sequence  ' & % & $ % % '(\n('%&(%%\n&'$(Dynamic\nSearch Tree at Time \"−$Search Tree at Time \"\npriority *= −log log (1/2 %,#)depth 3 =\n<(log(1/2 %,$))Time \"\nSequence'&%&$%% '((%'%&%\n&'$(Learning \nOracle!$,!\n!$,\"\n!$,#%\n('&$!(=) ∈ ℝ\"%4UBUJD(\n+ 5(0,1)' &\n' &\nFigure 1. Sketch for static and dynamic learning augmented search\ntrees. Since item 3 has a higher frequency around time i, dynamic\nsearch trees adjust the priority accordingly.\nIn both sequences, the item 4is accessed the most frequently.\nSo input-dependent search trees should place 4near the root.\nHowever, in the second sequence, the item 3is accessed\nthree consecutive times around the middle. An algorithm\nthat’s allowed to modify the tree dynamically can then mod-\nify the tree to place 3closer to the root during those calls.\nAn illustration of this is in Figure 1. Note that we pay\ncosts both when accessing the items and updating the trees.\nHence, there is a trade-off between the costs of updating\nitems’ scores and the benefits of time-varying scores.\nWe study ways of designing composite priorities that cause\nthis access cost to match known sequence-dependent access\ncosts of binary trees (and their natural generalizations to\nB-Trees). Here, we focus on the working-set bound , which\nsays that the cost of accessing an item should be, at most,\nthe logarithm of the number of distinct items until they get\naccessed again. To obtain this bound, we propose a new\ncomposite priority named working-set priority , based on the\nnumber of distinct elements between two occurrences of the\nsame item accessed at step i. We give the guarantees for\nthe dynamic Treaps with the working-set priority in Theo-\nrem 4.4. The dynamic search Treaps further demonstrate the\npower of learning scores from data. While we have more\ndata, we can quantify the dynamic environment in a more\naccurate way and thus improve the efficiency of the data\nstructure.Robustness to Prediction Inaccuracy. Finally, we show\nthe robustness of our data structures with inaccuracies in\nprediction oracles. In the static case, we can directly relate\nthe overhead of having inaccurate frequency predictions to\nthe KL divergences between the true relative frequencies\npxand their estimates qx. This is because our composite\npriority can take any estimate. So plugging in the estimates\nqxgives the overall access cost\nm·X\nxpxlog2\u00121\nqx\u0013\n,\nwhich is exactly the cross entropy between pandq. On the\nother hand, the KL divergence between pandqis exactly\nthe cross entropy minus the entropy of p. So we get that\nthe overhead of building the tree using noisy estimators q\ninstead of the true frequencies pis exactly mtimes the KL\ndivergence between pandq. We formalize the argument\nabove in Section 2.3. We also achieve robustness results in\nthe dynamic setting in Appendix D.2.\nIn all, our contributions can be summarized as follows:\n•We introduce composite priorities that integrate learned\nadvice into Treaps. The BSTs and B-Trees constructed\nvia these priorities are within constants of the static\noptimal ones for arbitrary distributions (Section 2, Sec-\ntion 3).\n•When allowing updating trees along with accessing\nitems, we design a working-set priority function, and\nthe corresponding Treaps with composite priorities can\nachieve the working-set bound (Section 4).\n•Both static and dynamic learning-augmented search\ntrees are robust to predictions (Section 2.3, Ap-\npendix D.2).\n•Our experiments show favorable performance com-\npared to prior work (Section 5).\n1.2. Related Work\nIn recent years, there has been a surge of interest in inte-\ngrating machine learning models into algorithm designs. A\nnew field called Algorithms with Predictions (Mitzenmacher\n& Vassilvitskii, 2022) has garnered considerable attention,\nparticularly in the use of machine learning models to pre-\ndict input patterns to enhance performance. Examples of\nthis approach include online graph algorithms with predic-\ntions (Azar et al., 2022), improved hashing-based methods\nsuch as Count-Min (Cormode & Muthukrishnan, 2005),\nand learning-augmented k-means clustering (Ergun et al.,\n2022). Practical oracles for predicting desired properties,\nsuch as predicting item frequencies in a data stream, have\nbeen demonstrated empirically (Hsu et al., 2019; Jiang et al.,\n2020).\n3\n\nOn the Power of Learning-Augmented Search Trees\nCapitalizing on the existence of oracles that predict the prop-\nerties of upcoming accesses, researchers are now developing\nmore efficient learning-augmented data structures. Index\nstructures in database management systems are one signifi-\ncant application of learning-augmented data structures. One\nkey challenge in this domain is to create search algorithms\nand data structures that are efficient and adaptive to data\nwhose nature changes over time. This has spurred interest\nin incorporating machine learning techniques to improve\ntraditional search tree performance.\nThe first study on learned index structures (Kraska et al.,\n2018) used deep-learning models to predict the position\nor existence of records as an alternative to the traditional\nB-Tree or hash index. However, this study focused only\non the static case. Subsequent research (Ferragina & Vin-\nciguerra, 2020; Ding et al., 2020; Wu et al., 2021) intro-\nduced dynamic learned index structures with provably effi-\ncient time and space upper bounds for updates in the worst\ncase. These structures outperformed traditional B-Trees in\npractice, but their theoretical guarantees were often trivial,\nwith no clear connection between prediction quality and\nperformance. More recently, Lin et al. (2022) proposed a\nlearning-augmented BST via Treaps that works under the\nZipfian distribution.\nOther related work on BSTs analyses and B-Trees under the\nexternal memory model is included in Appendix A.\n2. Learning-Augmented Binary Search Trees\nIn this section, we show that the widely taught Treap data\nstructure can, with small modifications, achieve the static\noptimality conditions sought after in previous studies of\nlearned index structures (Lin et al., 2022; Hsu et al., 2019).\nWe start with definitions and basic properties of Treaps.\nDefinition 2.1 (Treap (Aragon & Seidel, 1989)) .LetTbe a\nBST over [n]andpriority ∈Rnbe a priority assignment on\n[n].We say (T,priority )is aTreap ifpriorityx≤priorityy\nwhenever xis a descendent of yinT.\nGiven a priority assignment priority , one can construct a\nBSTTsuch that (T,priority )is a Treap as follows. Take\nanyx∗∈arg maxxpriorityxand build Treaps on [1, x∗−1]\nand[x∗+1, n]recursively using priority .Then, we just make\nx∗the parent of both Treaps. Notice that if priorityx’s are\ndistinct, the resulting Treap is unique.\nObservation 1.Letpriority ∈Rn, which assigns each item\nxto a unique priority. There is a unique BST Tsuch that\n(T,priority )is a Treap.\nFrom now on, we always assume that priority has distinct\nvalues. Therefore, when priority is defined from the context,\nthe term Treap refers to the unique BST T. For each node\nx∈[n], we use depth (x)to denote its depth in T, i.e., the\nnumber of vertices on the path from the root to x.\nGiven any two items x, y∈[n], one can determine whetherxis an ancestor of yin a Treap without traversing the tree.\nObservation 2.Given any x, y∈[n],xis an ancestor of y\nif and only if priorityx= max z∈[x,y]priorityz.\nClassical results from Aragon & Seidel (1989) state that if\nthe priorities are randomly assigned, the depth of the Treap\ncannot be too large. Also, Treaps can be made dynamic and\nsupport operations such as insertions and deletions.\nLemma 2.2 ((Aragon & Seidel, 1989)) .LetU(0,1)be\nthe uniform distribution over the real interval [0,1].If\npriority ∼U(0,1)n, each Treap node xhas depth Θ(log2n)\nwith high probability.\nLemma 2.3 ((Aragon & Seidel, 1989)) .Given a Treap T\nand some item x∈[n],xcan be inserted to or deleted from\nTinO(depth (x))-time.\n2.1. Learning-Augmented Treaps\nIn this section, we present the construction of composite\npriorities and prove the following theorem.\nTheorem 2.4 (Learning-Augmented Treap via Composite\nPriorities) .Denote w= (w1,···, wn)∈(0,1)nas a score\nassociated with each item in [n]such that ∥w∥1=O(1).\nConsider the following priority assignment of each item:\npriorityxdef=−\u0016\nlog2log21\nwx\u0017\n+δx, (3)\nwhere δxis drawn independently uniformly from (0,1). The\nexpected depth of any item x∈[n]isO(log2(1/wx)).\nProof Plan. Note that the priority in Equation (3) consists\nof two terms. We define x’s tier as τx:=j\nlog2log21\nwxk\n=\n−⌊priorityx⌋.LetSt={x∈[n]|τx=t}be the number of\nitems whose tiers are equal t. We assume wlog that τx≥0\nfor any x. Otherwise, τx<0implies wx= Ω(1) , which\ncan hold for only a constant number of items. So, we can\nalways put them at the top of the Treap, which increases the\ndepths of other items by a constant.\nThe expected depth of xis the number of its ancestors. We\nshow in Lemma 2.5 that the number of items at tier tis\nbounded by |St|= 2O(2t). Furthermore, for each tier, the\nties are broken randomly due to the random offset δx∼\nU(0,1). Then, as we show in Lemma 2.6, any item has\nO(log2|St|) =O(2t)ancestors with tier tin expectation.\nTherefore, the expected depth E[depth (x)]can be bound by\nO(20+ 21+. . .+ 2τx) =O(2τx) =O(log2(1/wx)).\nLemma 2.5. For any integer t≥0,|St|= 2O(2t).\nProof. Observe that x∈Stif and only if\nt≤log2log2(1/wx)< t+ 1,and22t≤1\nwx<22t+1.\nSince the total score ∥w∥1=O(1), there are only\npoly(22t+1) = 2O(2t)such items.\n4\n\nOn the Power of Learning-Augmented Search Trees\nNext, we bound the expected number of ancestors of item x\nin every Stsuch that t≤τx.\nLemma 2.6. Letx∈[n]be any item and t≤τxbe a\nnon-negative integer. The expected number of ancestors of\nxinStis at most O(log2|St|).\nProof. First, we show that any y∈Stis an ancestor of x\nwith probability no more than 1/|St∩[x, y]|.Observation 2\nsays that ymust have the largest priority among items [x, y].\nThus, a necessary condition for ybeing x’s ancestor is that y\nhas the largest priority among items in St∩[x, y].However,\npriorities of items in St∩[x, y]are i.i.d. random variables of\nthe form −t+U(0,1).Thus, the probability that priorityy\nis the largest among them is 1/|St∩[x, y]|.\nTo bound the expected number of ancestors of xinSt,\nE[number of ancestors of xinSt]\n=X\ny∈StPr (yis an ancestor of x)\n≤X\ny∈St1\n|St∩[x, y]|≤2·|St|X\nu=11\nu=O(log2|St|),\nwhere the second inequality comes from the fact that for a\nfixed value of u, there are at most two items y∈Stwith\n|St∩[x, y]|=u(one with y≤x, the other with y > x ).\nNow we are ready to prove Theorem 2.4.\nProof of Theorem 2.4. By Lemma 2.6 and Lemma 2.5, the\nexpected depth of xcan be bounded by\nE[depth (x)] =τxX\nt=0E[number of ancestors of xinSt]\n≤O τxX\nt=0log2|St|!\n≤O τxX\nt=02t!\n≤O(2τx).\nWe conclude the proof by observing that\nτx≤log2log21\nwx≤τx+ 1and2τx≤log21\nwx.\nMoreover, our proposed learning-augmented treap supports\nefficient updates, where we can use rotations to do insertions,\ndeletions, and weight changes. The following corollary\nfollows naturally by Theorem 2.4 and Lemma 2.3.\nCorollary 2.7. The data structure supports insertions and\ndeletions naturally. Suppose the score of some node x\nchanges from wtow′and a pointer to the node is given, the\nTreap can be maintained with O(|log2(w′/w)|)rotations\nin expectation.\n2.2. Static Optimality\nWe present a priority assignment for constructing stati-\ncally optimal Treaps given item frequencies. Given anyaccess sequence X= (x(1), . . . , x (m)), we define fx\nfor any item x, to be its frequency inX, i.e. fx:=\n|{i∈[m]|x(i) =x}|, x∈[n].For simplicity, we assume\nthat every item is accessed at least once, i.e., fx≥1, x∈\n[n].We prove the following result as a simple application\nof Theorem 2.4 by setting wx:=fx\nm, x∈[n].\nTheorem 2.8. For any item x∈[n], we set its priority as\npriorityx:=−\u0016\nlog2log2m\nfx\u0017\n+δx, δx∼U(0,1).\nIn the corresponding Treap, each node xhas expected depth\nO(log2(m/f x)).Therefore, the total time for processing the\naccess sequence is O(P\nxfxlog2(m/f x)), which matches\nthe performance of the optimal static BSTs up to a constant\nfactor.\n2.3. Robustness Guarantees\nIn practice, one could only estimate qx≈px=fx/m, x ∈\n[n].A natural question arises: how does the estimation error\naffect the performance? In this section, we analyze the\ndrawbacks in performance given the estimation errors. As a\nresult, we will show that our Learning-Augmented Treaps\nare robust against noise and errors.\nFor each item x∈[n], define px=fx/mto be the relative\nfrequency of item x.One can view pas a probability distri-\nbution over [n]such that p(x) =px. Then we can restate\nthe expected depth of each item in Theorem 2.8 using the\nnotion of entropy. We define the entropy as follows and\nstate the corollary in Corollary 2.10.\nDefinition 2.9 (Entropy) .Given a probability distri-\nbution pover [n], define its Entropy asEnt(p):=P\nxpxlog2(1/px) =Ex∼p[log2(1/px)].\nCorollary 2.10. In Theorem 2.8, the expected depth of each\nitemxisO(log2(1/px))and the expected total cost is O(m·\nEnt(p)), where Ent(p) =P\nxpxlog2(1/px)measures the\nentropy of the distribution p.\nNow we consider the case when we cannot access the rel-\native frequency px. Instead, we are given px’s estimator,\nqx, and construct the data-augmented BST with qx. Sim-\nilarly, we view qas a data distribution over [n]such that\nq(x) =qx. Then we show that the total access of the treap\nbuilt with qxequals the total access number mtimes the\ncross entropy of pandqin Theorem 2.13. We start with\nsome definitions.\nDefinition 2.11 (Cross Entropy) .Given two distributions\np,qover [n], define its Cross Entropy asEnt(p,q):=P\nxpxlog2(1/qx) =Ex∼p[log2(1/qx)].\nDefinition 2.12 (KL Divergence) .Given two distributions\np,qover[n], define its KL Divergence asDKL(p,q) =\nEnt(p,q)−Ent(p) =P\nxpxlog2(px/qx).\nWe analyze the run time given frequency estimations q.\n5\n\nOn the Power of Learning-Augmented Search Trees\nTheorem 2.13. Given a distribution q, an estimate of the\ntrue relative frequencies distribution p. For any item x∈\n[n], we draw a random number δx∼U(0,1)and set its\npriority as\npriorityx:=−\u0016\nlog2log21\nqx\u0017\n+δx.\nIn the corresponding Treap, each node xhas expected depth\nO(log2(1/qx)).Therefore, the total time for processing the\naccess sequence is O(m·Ent(p,q)).\nProof. Define the weights wx=qxfor each item x∈\n[n].Clearly, ∥w∥1= 1and we can apply Theorem 2.4 to\nprove the bound on the expected depths. The total time for\nprocessing the access sequence is, by definition,\nO\nX\nx∈[n]fxlog21\nqx\n=O\nm·X\nx∈[n]pxlog21\nqx\n\n=O(m·Ent(p,q)).\n2.4. Analysis of Other Priority Assignments\nIn this section, we discuss two different priority assignments.\nFor each assignment, we design an input distribution that\nresults in a greater expected depth than the expected depth\nwith our priority assignment stated in Theorem 2.8. We\ndefine the distribution pasp(x) =px=fx/m, x ∈[n].\nWe use f≳gto indicate that fis greater or equal to gup\nto a constant factor.\nThe first priority assignment is used in Lin et al. (2022).\nThey assign priorities according to pxentirely, i.e.,\npriorityx=px, x∈[n].Assuming that items are ordered\nrandomly, and pis a Zipfian distribution, they show Static\nOptimality . However, it does not generally hold–there ex-\nists a distribution pwhere the expected access cost for (Lin\net al., 2022) is Ω(n), while our data structure (Theorem 2.4)\nachieves only a O(log2n)cost.\nTheorem 2.14. Consider the priority assignment that as-\nsigns the priority of each item to be priorityx:=px, x∈[n].\nThere is a distribution pover[n]such that the expected ac-\ncess time, Ex∼p[depth (x)] = Ω( n).\nProof. We define for each item x,px:=2(n−x+1)\nn(n+1).One\ncould easily verify that pis a distribution over [n].In addi-\ntion, the smaller the item x, the larger the priority priorityx.\nThus, by the definition of Treaps, item xhas depth x.The\nexpected access time of xsampled from pcan be lower\nbounded as follows:\nEx∼p[depth (x)] =X\nx∈[n]px·depth (x)\n=X\nx∈[n]2(n−x+ 1)\nn(n+ 1)·x=2\nn(n+ 1)X\nx∈[n]x(n−x+ 1)\n≳2\nn(n+ 1)·n3≳n.Next, we consider the priority assignment priorityx:=\n−⌊log21/px⌋+δx, δx∼U(0,1).\nTheorem 2.15. Consider the following priority assign-\nment that sets the priority of each node xaspriorityx:=\n−⌊log21/px⌋+δx, δx∼U(0,1). There is a distri-\nbution pover [n]such that the expected access time,\nEx∼p[depth (x)] = Ω(log2\n2n).\nProof. We assume WLOG that nis an even power of 2.\nDefine K=1\n2log2n.We partition [n]intoK+ 1segments\nS1, . . . , S K, SK+1⊆[n]. For i= 1,2, . . . , K , we add\n21−i·n/K elements to Si. Thus, S1hasn/K elements,\nS2hasn/2K, and SKhas√n/K elements. The rest are\nmoved to SK+1.\nNow, we can define the distribution p. Elements in SK+1\nhave zero-mass. For i= 1,2, . . . , K , elements in Sihas\nprobability mass 2i−1/n.One can directly verify that pis\nindeed a probability distribution over [n].\nIn the Treap with the given priority assignment, Siforms\na subtree of expected height Ω(log2n)since|Si| ≥n1/3\nfor any i= 1,2, . . . , K (Lemma 2.2). In addition, ev-\nery element of Sipasses through Si+1, Si+2, . . . , S Kon\nits way to the root since they have strictly larger pri-\norities. Therefore, the expected depth of element x∈\nSiisΩ((K−i) log2n).One can lower bound the ex-\npected access time (which is the expected depth) as:\nEx∼p[depth (x)]≳KX\ni=1X\nx∈Sipx·(K−i)·log2n\n=KX\ni=1p(Si)·(K−i)·log2n=KX\ni=11\nK·(K−i)·log2n\n≳Klog2n≳log2\n2n,\nwhere we use p(Si) =|Si| ·2i−1/n= 1/KandK=\nΘ(log2n).That is, the expected access time is at least\nΩ(log2\n2n).\n3. Learning-Augmented B-Trees\nWe now extend the ideas above, specifically the compos-\nite priority notions, to B-Trees in the External Memory\nModel . The main results are shown as follows. Full details\nare included in Appendix C. We show that the learning-\naugmented B-Treaps (Appendix C.1) obtain static optimality\n(Appendix C.2) and is robust to the noisy predicted scores\n(Appendix C.3).\nTheorem 3.1 (Learning-Augmented B-Treap via Composite\nPriorities) .Denote w= (w1,···, wn)∈(0,1)nas a score\nassociated with each element of [n]such that ∥w∥1=O(1)\nand a branching factor B= Ω(ln1/(1−α)n), consider the\nfollowing priority assignment scheme:\npriorityx:=−⌊log2logB1\nwx⌋+δx, δx∼U(0,1).\n6\n\nOn the Power of Learning-Augmented Search Trees\nThere is a randomized data structure that maintains a B-Tree\nTBoverUsuch that\n1. Each item xhas expected depth O(1\nαlogB(1/wx)).\n2.Insertion or deletion of item xinto/from Ttouches\nO(1\nαlogB(1/wx))nodes in TBin expectation.\n3.Updating the weight of item xfrom wtow′touches\nO(1\nα|logB(w′/w)|)nodes in TBin expectation.\nIn addition, if B=O(n1/2−δ)for some δ >0, all above\nperformance guarantees hold with high probability 1−δ. If\nwe are given the frequency fxfor each xand set the priority\nas\npriorityx:=−\u0016\nlog2logBm\nfx\u0017\n+δx, δx∼U(0,1).\nThen the total access cost O(P\nxfxlogB(m/f x))achieves\nthe static optimality.\n4.Dynamic Learning-Augmented Search Trees\nIn this section, we investigate the properties of dynamic\nsearch trees that permit modifications concurrent with se-\nquence access. Prioritizing items that are anticipated to be\naccessed in the near future to reside at lower depths within\nthe tree can significantly reduce access times. Nonethe-\nless, updating the B-trees introduces additional costs. The\noverarching goal is to minimize the composite cost, which\nincludes both the access operations across the entire se-\nquence and the modifications to the B-trees. In the main\ncontent, we specifically concentrate on the study of locally\ndynamic B-trees, which are characterized by the restriction\nthat tree modifications are limited solely to the adjustment\nof priorities for the items being accessed.\nHere, we construct a dynamic learning-augmented B-trees\nthat achieves the working-set property . In data structures,\nthe working set is the collection of data that a program uses\nfrequently over a given period. This concept is important\nbecause it helps us understand how a program interacts with\nmemory and thus enables us to design more efficient data\nstructures and algorithms. For example, if a program is\nsorting a list, the working set might be the elements of the\nlist it is comparing and swapping right now. The size of the\nworking set can affect how fast the program runs. A smaller\nworking set can make the program run faster because it\nmeans the program doesn’t need to reach out to slower parts\nof memory as often. In other words, if we know which parts\nof a data structure are used most, we can organize the data\nor even the memory in a way that makes accessing these\nparts faster, which can speed up the entire program.\nWe define the working-set size as the number of distinct\nitems accessed between two consecutive accesses. Corre-\nspondingly, we design a time-varying score, working-setscore , as the reciprocal of the square of one plus working-\nset size. We will show that the working-set score is locally\nchanged and there exists a data structure that achieves the\nworking-set property , which states that the time to access\nan element is a logarithm of its working-set size.\nThe formal definitions and the main theorems in this section\nare presented as follows. We include more general results\nfor dynamic B-trees and omitted proofs in Appendix D.\nDefinition 4.1 (Previous and Next Access prev(i, x)\nand next(i, x)).Let prev(i, x)be the previous access\nof item xat or before time i, i.e, prev(i, x):=\nmax{i′≤i|x(i′) =x}.Let next(i, x)to be the next\naccess of item xafter time i, i.e, next(i, x):=\nmin{i′> i|x(i′) =x}.\nDefinition 4.2 (Working-set Size work(i, x)).Define the\nworking-set size work(i, x)as the number of distinct items\naccessed between the previous access of item xat or before\ntimeiand the next access of item xafter time i. That is,\nwork(i, x)def=|{x(prev(i, x) + 1) ,···, x(next(i, x))}|.\nIfxdoes not appear after time i, we define work(i, x):=n.\nNote that this definition captures the temporal locality and\ndiversity of user behavior around item. Although it requires\nknowledge of future access items, it aligns conceptually with\npractices in recommendation systems, where temporal con-\ntext is used to model user intent and predict relevance. For\nexample, session-based recommendation often considers the\ndiversity of items within a user’s recent session (Wang et al.,\n2021). In practice, we can approximate this score using\ncausal proxies (e.g., past-only working-set size) or apply\nmachine learning models to predict it based on observable\naccess patterns.\nDefinition 4.3 (Working-set Score ω(i, x)).Define the time-\nvarying score as the reciprocal of the square of one plus\nworking-set size. That is,\nω(i, x) =1\n(1 + work(i, x))2\nTheorem 4.4 (Dynamic Search Tree with Working-set Pri-\nority) .With the working-set size work(i, x)known and the\nbranching factor B= Ω(ln1.1n), there is a randomized\ndata structure that maintains a B-tree TBover[n]with the\npriorities assigned as\npriority (i, x) =−⌊log2logB(1 + work(i, x))2⌋+U(0,1).\nUpon accessing the item xat time i, the expected depth of\nitemxisO(log2(1+ work(i, x)).The expected total cost for\nprocessing the whole access sequence Xis the following.\nThe data structure satisfies the working-set property.\ncost(X, ω) =O \nnlogBn+mX\ni=1logB(1 + work(i, x))!\n.\n7\n\nOn the Power of Learning-Augmented Search Trees\nIn particular, if B=O\u0000\nn1/2−δ\u0001\nfor some δ > 0, the\nguarantees hold with probability 1−δ.\nRemark. Consider two sequences with length m,\nX1= (1 ,2,···, n,1,2,···, n,···,1,2,···, n),X2=\n(1,1,···,1,2,2,···,2,···, n, n,···, n). Two sequences\nhave the same total cost if we have a fixed score. However,\nX2should have less cost because of its repeated pattern.\nGiven the frequency freq as a time-invariant priority, by\nTheorem 3.1, the optimal static costs are\ncost(X1,freq) = cost(X2,freq) =O(mlog2n).\nBut for the dynamic BSTs, with the working-set score, we\ncalculate both costs from Theorem 4.4 as\ncost(X1, ω) =O(mlog2(n+ 1)) ,\ncost(X2, ω) =O(nlog2n+mlog23).\nThis means that our proposed priority can better capture the\ntiming pattern of the sequence and thus perform better than\nthe optimal static setting.\nFinally, we use the following theorem to show the robustness\nof the results when the scores are inaccurate.\nTheorem 4.5 (Dynamic Search Tree with Working-set Pri-\nority) .Given the predicted locally changed working-set\nscoreeω(i)∈(0,1)nsatisfying ∥eω(i)∥1=O(1),eωi,j≥\n1/poly( n)and the branching factor B= Ω(ln1.1n), there\nis a randomized data structure that maintains a B-Tree over\nthenkeys such that the expected total cost for processing\nthe whole access sequence X,cost(X,eω), is\ncost(X, ω) +O mX\ni=1\f\flogBωi,x(i)−logBeωi,x(i)\f\f!\n.\nIn particular, if B=O(n1/2−δ)for some δ >0, the guar-\nantees hold with probability 1−δ.\n5. Experiments\nIn this section, we give experimental results that compare\nour learning-augmented Treap ( learn-bst ) with learning-\naugmented BST (Lin et al., 2022) ( learn-llw ), learning-\naugmented skip-list (Fu et al., 2025) ( learn-skiplist ) and\nclassical search tree data structures including Red-Black\nTrees ( red-black ), A VL Trees ( avl), Splay Trees ( splay ), B-\nTrees of order 3 ( b-tree ), and randomized Treaps ( random-\ntreap ). Experiments are conducted in a similar manner\nin Lin et al. (2022): (1)All keys are inserted in a random\norder to avoid insertion order sensitivity. (2)The total access\ncost is measured by the total number of comparisons needed\nwhile accessing keys.\nWe consider a synthetic data setting, with nunique items ap-\npearing in a sequence of length m. We define the frequency\nFigure 2. Zipfian distribution, α= 1.\nFigure 3. Adversarial distribution.\nFigure 4. Uniform distribution.\nof each item iasfiand its relative frequency as pi=fi/m.\nAll results are based on ten independent trials.\n5.1. Perfect Prediction Oracle on Frequency\nWe first assume that we are given a perfect prediction or-\nacle on the item frequency. The data follows one of three\ndistributions: the Zipfian distribution, the distribution de-\nscribed in Theorem 2.14 (adversarial distribution), and the\nuniform distribution. We set n= 1000 and vary mover\n[2000 ,6000,10000 ,16000 ,20000] . The x-axis represents\nthe number of unique items, and the y-axis denotes the\nnumber of comparisons made, which measures access cost.\n8\n\nOn the Power of Learning-Augmented Search Trees\nFigure 5. Inaccurate Prediction Oracle.\nZipfian Distribution. The Zipfian distribution with pa-\nrameter αhas relative frequencies pi=1\niαHn,α, where\nHn,α=Pn\ni=11\niαis the nthgeneralized harmonic number\nof order α. In our experiment, we set α= 1. As shown\nin Figure 2, our Treaps outperform all other data structures\nexcept (Lin et al., 2022), which explicitly assumes a Zipfian\ndistribution.\nAdversarial Distribution. In the proof of Theorem 2.14,\nwe construct a distribution with relative frequency given by\npi=2(n−i+1)\nn(n+1). We prove that using the priority assignment\nas in (Lin et al., 2022), the expected depth is Ω(n). As\nshown in Figure 3, the data structure in Lin et al. (2022) per-\nforms significantly worse under this adversarial distribution\ncompared to the Zipfian case, while our Treaps maintain the\nbest performance among all data structures.\nUniform Distribution. We also consider uniform distri-\nbution, where each item has a relative frequency of pi=1\nn.\nAs shown in Figure 4, our Treaps outperform all other data\nstructures as well.\n5.2. Inaccurate Prediction Oracle on Frequency\nWe consider the scenario where our learning-augmented\nTreaps are constructed based on an inaccurate prediction\nof item frequencies. The inaccuracy of the prediction is\nquantified using the KL divergence between the true relative\nfrequency and the predicted relative frequency. Specifically,\nwe initialize a uniform distribution and optimize it toward\na target distribution with a specified KL divergence level\nusing the Sequential Least Squares Programming (SLSQP)\noptimizer in SciPy (Virtanen et al., 2020). The x-axis repre-\nsents the KL divergence, while the y-axis denotes the total\naccess cost. We set n= 5000 andm= 10000 .\nAs shown in Figure 5, our Treaps not only outperform the\ninvestigated alternatives but also exhibit a graceful degra-\ndation as the difference between the predicted and actual\ndistribution increases. Additionally, we conducted other\nrobustness experiments on mixtures of distributions, as de-\ntailed in Appendix E.\nFigure 6. Relationship between total cost and KL divergence be-\ntween true and predicted frequency using our learning-augmented\ntreaps.\n5.3. Total Cost and KL Divergence\nIn Theorem 2.13, we show that in our learning-augmented\ntreaps, when the frequency predictor is inaccurate, the ad-\nditional cost increases linearly with the KL divergence\nbetween the true and predicted frequencies. We comple-\nment this theoretical result with experiments using the\nsame setup as in Section 5.2. Specifically, we set n=\n5000, m= 10000 and set the KL divergence to vary over\n[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7]. As shown in Figure 6,\nthe experimental results confirm that the additional cost\ngrows linearly with the KL divergence.\nAcknowledgments\nWe thank Chris Lambert, Richard Peng, Mars Xiang, and\nDaniel Sleator for their helpful discussions and insights, and\nTian Luo, Samson Zhou, and Chunkai Fu for their insights\non the experimental code.\nImpact Statement\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none of which we feel must be\nspecifically highlighted here.\nReferences\nAamand, A., Chen, J. Y ., and Indyk, P. (optimal)\nonline bipartite matching with degree information.\n2022. URL https://openreview.net/forum?\nid=NgwrhCBPTVk .\nAdelson-Velskii, M. and Landis, E. M. An algorithm for\nthe organization of information. Technical report, Joint\nPublications research service Washington DC, 1963.\nAllen, B. and Munro, J. I. Self-organizing binary search\n9\n\nOn the Power of Learning-Augmented Search Trees\ntrees. J. ACM , 25(4):526–535, 1978.\nAragon, C. R. and Seidel, R. Randomized search trees. In\nFOCS , volume 30, pp. 540–545, 1989.\nAzar, Y ., Panigrahi, D., and Touitou, N. Online graph\nalgorithms with predictions. In Proceedings of the 2022\nAnnual ACM-SIAM Symposium on Discrete Algorithms\n(SODA) , pp. 35–66. SIAM, 2022.\nB˘adoiu, M., Cole, R., Demaine, E. D., and Iacono, J. A\nunified access bound on comparison-based dynamic dic-\ntionaries. Theoretical Computer Science , 382(2):86–96,\n2007.\nBender, M. A., Ebrahimi, R., Hu, H., and Kuszmaul, B. C.\nB-trees and cache-oblivious b-trees with different-sized\natomic keys. ACM Transactions on Database Systems\n(TODS) , 41(3):1–33, 2016.\nBose, P., Douieb, K., and Langerman, S. Dynamic opti-\nmality for skip lists and b-trees. In Proceedings of the\nnineteenth annual ACM-SIAM symposium on Discrete\nalgorithms , pp. 1106–1114. Citeseer, 2008.\nBose, P., Cardinal, J., Iacono, J., Koumoutsos, G., and\nLangerman, S. Competitive online search trees on trees.\nInProceedings of the 14th Annual ACM-SIAM Sympo-\nsium on Discrete Algorithms (SODA) , pp. 1878–1891.\nSIAM, 2020.\nBrodal, G. S. and Fagerberg, R. Lower bounds for external\nmemory dictionaries. In SODA , volume 3, pp. 546–554,\n2003.\nBrown, T. B-slack trees: Space efficient b-trees. In Ravi,\nR. and Gørtz, I. L. (eds.), Algorithm Theory – SWAT\n2014 (Lecture Notes in Computer Science, vol. 8503) ,\nvolume 8503 of Lecture Notes in Computer Science ,\npp. 122–133. Springer, Cham, 2014. doi: 10.1007/\n978-3-319-08404-6 11. URL https://doi.org/\n10.1007/978-3-319-08404-6_11 .\nBuchsbaum, A. L., Goldwasser, M. H., Venkatasubrama-\nnian, S., and Westbrook, J. R. On external memory graph\ntraversal. In SODA , pp. 859–860, 2000.\nCanonne, C. L. A short note on learning discrete distribu-\ntions. arXiv preprint arXiv:2002.11457 , 2020.\nChalermsook, P., Chuzhoy, J., and Saranurak, T. Pinning\ndown the Strong Wilber 1 Bound for Binary Search\nTrees. In Byrka, J. and Meka, R. (eds.), Approximation,\nRandomization, and Combinatorial Optimization.\nAlgorithms and Techniques (APPROX/RANDOM 2020) ,\nvolume 176 of Leibniz International Proceedings\nin Informatics (LIPIcs) , pp. 33:1–33:21, Dagstuhl,\nGermany, 2020. Schloss Dagstuhl – Leibniz-Zentrumf¨ur Informatik. ISBN 978-3-95977-164-1. doi:\n10.4230/LIPIcs.APPROX/RANDOM.2020.33. URL\nhttps://drops.dagstuhl.de/entities/\ndocument/10.4230/LIPIcs.APPROX/RANDOM.\n2020.33 .\nChen, J., Silwal, S., Vakilian, A., and Zhang, F. Faster\nfundamental graph algorithms via learned predictions.\nInInternational Conference on Machine Learning , pp.\n3583–3602. PMLR, 2022a.\nChen, J. Y ., Eden, T., Indyk, P., Lin, H., Narayanan, S.,\nRubinfeld, R., Silwal, S., Wagner, T., Woodruff, D.,\nand Zhang, M. Triangle and four cycle counting with\npredictions in graph streams. In International Confer-\nence on Learning Representations , 2022b. URL https:\n//openreview.net/forum?id=8in_5gN9I0 .\nCole, R. On the dynamic finger conjecture for splay trees.\npart ii: The proof. SIAM Journal on Computing , 30(1):\n44–85, 2000.\nCole, R., Mishra, B., Schmidt, J., and Siegel, A. On the\ndynamic finger conjecture for splay trees. part i: Splay\nsorting log n-block sequences. SIAM Journal on Comput-\ning, 30(1):1–43, 2000.\nCormen, T. H., Leiserson, C. E., Rivest, R. L., and\nStein, C. Introduction to Algorithms, 3rd Edi-\ntion. MIT Press, 2009. ISBN 978-0-262-03384-\n8. URL http://mitpress.mit.edu/books/\nintroduction-algorithms .\nCormode, G. and Muthukrishnan, S. An improved data\nstream summary: the count-min sketch and its applica-\ntions. Journal of Algorithms , 55(1):58–75, 2005.\nDemaine, E. D., Harmon, D., Iacono, J., and Patra s ¸cu, M.\nDynamic optimality—almost. SIAM Journal on Comput-\ning, 37(1):240–251, 2007.\nDemaine, E. D., Harmon, D., Iacono, J., Kane, D., and\nPatra s ¸cu, M. The geometry of binary search trees. In\nProceedings of the 20th annual ACM-SIAM symposium\non Discrete algorithms (SODA) , pp. 496–505. SIAM,\n2009.\nDerryberry, J. C. and Sleator, D. D. Skip-splay: Toward\nachieving the unified bound in the bst model. In Work-\nshop on Algorithms and Data Structures , pp. 194–205.\nSpringer, 2009.\nDing, J., Minhas, U. F., Yu, J., Wang, C., Do, J., Li, Y .,\nZhang, H., Chandramouli, B., Gehrke, J., Kossmann, D.,\net al. Alex: an updatable adaptive learned index. In\nProceedings of the 2020 ACM SIGMOD International\nConference on Management of Data , pp. 969–984, 2020.\n10\n\nOn the Power of Learning-Augmented Search Trees\nDinitz, M., Im, S., Lavastida, T., Moseley, B., and Vassil-\nvitskii, S. Faster matchings via learned duals. Advances\nin Neural Information Processing Systems , 34:10393–\n10406, 2021.\nErgun, J. C., Feng, Z., Silwal, S., Woodruff, D., and Zhou,\nS. Learning-augmented $k$-means clustering. In In-\nternational Conference on Learning Representations ,\n2022. URL https://openreview.net/forum?\nid=X8cLTHexYyY .\nErlebach, T., de Lima, M. S., Megow, N., and Schl ¨oter,\nJ. Learning-augmented query policies for mini-\nmum spanning tree with uncertainty. arXiv preprint\narXiv:2206.15201 , 2022.\nFagerberg, R., Hammer, D., and Meyer, U. On optimal\nbalance in b-trees: What does it cost to stay in per-\nfect shape? In 30th International Symposium on Algo-\nrithms and Computation (ISAAC 2019) . Schloss Dagstuhl-\nLeibniz-Zentrum fuer Informatik, 2019.\nFerragina, P. and Vinciguerra, G. The pgm-index: a fully-\ndynamic compressed learned index with provable worst-\ncase bounds. Proceedings of the VLDB Endowment , 13\n(8):1162–1175, 2020.\nFerragina, P., Lillo, F., and Vinciguerra, G. Why are learned\nindexes so effective? In International Conference on\nMachine Learning , pp. 3123–3132. PMLR, 2020.\nFu, C., Nguyen, B. G., Seo, J. H., Zesch, R. S., and Zhou, S.\nLearning-augmented search data structures. In The Thir-\nteenth International Conference on Learning Represen-\ntations , 2025. URL https://openreview.net/\nforum?id=N4rYbQowE3 .\nGolovin, D. Uniquely represented data structures with\napplications to privacy . PhD thesis, Carnegie Mellon\nUniversity, 2008.\nGolovin, D. B-treaps: A uniquely represented alternative to\nb-trees. In Automata, Languages and Programming: 36th\nInternational Colloquium, ICALP 2009, Rhodes, Greece,\nJuly 5-12, 2009, Proceedings, Part I 36 , pp. 487–499.\nSpringer, 2009.\nGuibas, L. J. and Sedgewick, R. A dichromatic framework\nfor balanced trees. In 19th Annual Symposium on Foun-\ndations of Computer Science (sfcs 1978) , pp. 8–21. IEEE,\n1978.\nHsu, C.-Y ., Indyk, P., Katabi, D., and Vakilian, A. Learning-\nbased frequency estimation algorithms. In International\nConference on Learning Representations , 2019.\nHu, T. and Tucker, A. Optimum binary search trees. Tech-\nnical report, WISCONSIN UNIV MADISON MATHE-\nMATICS RESEARCH CENTER, 1970.Iacono, J. Alternatives to splay trees with O(logn)worst-\ncase access times. In Proceedings of the Twelfth Annual\nACM-SIAM Symposium on Discrete Algorithms , pp. 516–\n522, 2001.\nIacono, J. Key-independent optimality. Algorithmica , 42\n(1):3–10, 2005.\nIacono, J. In pursuit of the dynamic optimality conjecture. In\nSpace-Efficient Data Structures, Streams, and Algorithms ,\npp. 236–250. Springer, 2013.\nJagadish, H., Narayan, P., Seshadri, S., Sudarshan, S., and\nKanneganti, R. Incremental organization for data record-\ning and warehousing. In VLDB , pp. 16–25, 1997.\nJermaine, C., Datta, A., and Omiecinski, E. A novel index\nsupporting high volume data warehouse insertion. In\nVLDB , volume 99, pp. 235–246, 1999.\nJiang, T., Li, Y ., Lin, H., Ruan, Y ., and Woodruff, D. P.\nLearning-augmented data stream algorithms. ICLR , 2020.\nKarpinski, M., Larmore, L. L., and Rytter, W. Sequential\nand parallel subquadratic work algorithms for construct-\ning approximately optimal binary search trees. In SODA ,\npp. 36–41. Citeseer, 1996.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis,\nN. The case for learned index structures. In Proceedings\nof the 2018 international conference on management of\ndata, pp. 489–504, 2018.\nLavastida, T., Moseley, B., Ravi, R., and Xu, C. Learnable\nand Instance-Robust Predictions for Online Matching,\nFlows and Load Balancing. In Mutzel, P., Pagh, R., and\nHerman, G. (eds.), 29th Annual European Symposium\non Algorithms (ESA 2021) , volume 204 of Leibniz\nInternational Proceedings in Informatics (LIPIcs) ,\npp. 59:1–59:17, Dagstuhl, Germany, 2021. Schloss\nDagstuhl – Leibniz-Zentrum f ¨ur Informatik. ISBN 978-\n3-95977-204-4. doi: 10.4230/LIPIcs.ESA.2021.59. URL\nhttps://drops.dagstuhl.de/entities/\ndocument/10.4230/LIPIcs.ESA.2021.59 .\nLin, H., Luo, T., and Woodruff, D. Learning augmented\nbinary search trees. In International Conference on Ma-\nchine Learning , pp. 13431–13440. PMLR, 2022.\nLucas, J. M. Canonical forms for competitive binary search\ntree algorithms . Rutgers University, Department of Com-\nputer Science, Laboratory for Computer . . . , 1988.\nMargaritis, G. and Anastasiadis, S. V . Efficient range-based\nstorage management for scalable datastores. IEEE Trans-\nactions on Parallel and Distributed Systems , 25(11):2851–\n2866, 2013.\n11\n\nOn the Power of Learning-Augmented Search Trees\nMehlhorn, K. Best possible bounds for the weighted path\nlength of optimum binary search trees. In Barkhage,\nH. (ed.), Automata Theory and Formal Languages, 2nd\nGI Conference, Kaiserslautern, May 20-23, 1975 , vol-\nume 33 of Lecture Notes in Computer Science , pp. 31–41.\nSpringer, 1975a.\nMehlhorn, K. Nearly optimal binary search trees. Acta\nInformatica , 5(4):287–295, 1975b.\nMitzenmacher, M. and Vassilvitskii, S. Algorithms with\npredictions. Commun. ACM , 65(7):33–35, June 2022.\nISSN 0001-0782. doi: 10.1145/3528087. URL https:\n//doi.org/10.1145/3528087 .\nMunro, J. I. On the competitiveness of linear search. In Eu-\nropean symposium on algorithms , pp. 338–345. Springer,\n2000.\nO’Neil, P., Cheng, E., Gawlick, D., and O’Neil, E. The\nlog-structured merge-tree (lsm-tree). Acta Informatica ,\n33:351–385, 1996.\nPolak, A. and Zub, M. Learning-augmented maximum flow.\nInf. Process. Lett. , 186(C), August 2024. ISSN 0020-\n0190. doi: 10.1016/j.ipl.2024.106487. URL https:\n//doi.org/10.1016/j.ipl.2024.106487 .\nRosenberg, A. L. and Snyder, L. Time-and space-optimality\nin b-trees. ACM Transactions on Database Systems\n(TODS) , 6(1):174–193, 1981.\nSafavi, R. and Seybold, M. P. B-treaps revised: Write\nefficient randomized block search trees with high load.\n19th Algorithms and Data Structures Symposium (WADS\n2025) , 2023. URL arXivpreprintarXiv:2303.\n04722 .\nSleator, D. D. and Tarjan, R. E. Self-adjusting binary search\ntrees. Journal of the ACM (JACM) , 32(3):652–686, 1985.\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I.,\nFeng, Y ., Moore, E. W., VanderPlas, J., Laxalde, D.,\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\nF., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\n1.0: Fundamental Algorithms for Scientific Computing\nin Python. Nature Methods , 17:261–272, 2020. doi:\n10.1038/s41592-019-0686-2.\nVitter, J. S. External memory algorithms and data structures:\nDealing with massive data. ACM Computing surveys\n(CsUR) , 33(2):209–271, 2001.Wang, S., Cao, L., Wang, Y ., Sheng, Q. Z., Orgun, M. A.,\nand Lian, D. A survey on session-based recommender\nsystems. ACM Comput. Surv. , 54(7), July 2021. ISSN\n0360-0300. doi: 10.1145/3465401. URL https://\ndoi.org/10.1145/3465401 .\nWu, J., Zhang, Y ., Chen, S., Wang, J., Chen, Y ., and Xing, C.\nUpdatable learned index with precise positions. Proceed-\nings of the VLDB Endowment , 14(8):1276–1288, 2021.\nYao, F. F. Speed-up in dynamic programming. SIAM Jour-\nnal on Algebraic Discrete Methods , 3(4):532–540, 1982.\nYi, K. Dynamic indexability and the optimality of b-trees.\nJournal of the ACM (JACM) , 59(4):1–19, 2012.\n12\n\nOn the Power of Learning-Augmented Search Trees\nA. Other Related Works\nBeyond Worst-Case Analyses of Binary Trees. Binary trees are among the most ubiquitous pointer-based data structures.\nWhile schemes without re-balancing do obtain O(log2n)time bounds in the average case, their behavior degenerates to\nΩ(n)on natural access sequences such as 1,2,3, . . . , n . To remedy this, many tree balancing schemes with O(log2n)time\nworst-case guarantees have been proposed (Adelson-Velskii & Landis, 1963; Guibas & Sedgewick, 1978; Cormen et al.,\n2009).\nCreating binary trees optimal for their inputs has been studied since the 1970s. Given access frequencies, the static tree\nof optimal cost can be computed using dynamic programs or clever greedies (Hu & Tucker, 1970; Mehlhorn, 1975b; Yao,\n1982; Karpinski et al., 1996). However, the cost of such computations often exceeds the cost of invoking the tree. Therefore,\na common goal is to obtain a tree whose cost is within a constant factor of the entropy of the data, multiple schemes do\nachieve this either on worst-case data (Mehlhorn, 1975b), or when the input follows certain distributions (Allen & Munro,\n1978).\nA major disadvantage of static trees is that their cost on any permutation needs to be Ω(nlog2n). On the other hand, for\nthe access sequence 1,2,3, . . . , n , repeatedly bringing the next accessed element to the root gives a lower cost O(n). This\nprompted Allen and Munro to propose the notion of self-organizing binary search trees. This scheme was extended to splay\ntrees by (Sleator & Tarjan, 1985). Splay trees have been shown to obtain many improved cost bounds based on temporal and\nspatial locality (Sleator & Tarjan, 1985; Cole et al., 2000; Cole, 2000; Iacono, 2005). In fact, they have been conjectured to\nhave access costs with a constant factor of optimal on any access sequence (Iacono, 2013). Much progress has been made\ntowards showing this over the past two decades (Demaine et al., 2009; Derryberry & Sleator, 2009; Chalermsook et al.,\n2020; Bose et al., 2020).\nFrom the perspective of designing learning-augmented data structures, the dynamic optimality conjecture almost goes\ncontrary to the idea of incorporating predictors. It can be viewed as saying that learned advice do not offer gains beyond\nconstant factors, at least in the binary search tree setting. Nonetheless, the notion of access sequence, as well as access-\nsequence-dependent bounds, provides useful starting points for developing prediction-dependent search trees in online\nsettings. In this paper, we choose to focus on bounds based on temporal locality, specifically, the working-set bound. This is\nfor two reasons: the spatial locality of an element’s next access is significantly harder to describe compared to the time until\nthe next access; and the current literature on spatial locality-based bounds, such as dynamic finger tends to be much more\ninvolved (Cole et al., 2000; Cole, 2000). We believe an interesting direction for extending our composite scores is to obtain\nanalogs of the unified bound (Iacono, 2001; B ˘adoiu et al., 2007) for B-Trees.\nB-Trees and External Memory Model. Parameterized B-Trees (Brodal & Fagerberg, 2003) have been studied to balance\nthe runtime of read versus write operations, and several bounds have been shown with regard to the blocks of memory\nneeded to be used during an operation. The optimality is discussed in both static and dynamic settings. Rosenberg &\nSnyder (1981) compared the B-Tree with the minimum number of nodes (denoted as compact ) with non-compact B-Trees\nand with time-optimal B-Trees. Bender et al. (2016) considers keys have different sizes and gives a cache-oblivious static\natomic-key B-Tree achieving the same asymptotic performance as the static B-Tree. When it comes to the dynamic setting,\nthe trade-off between the cost of updates and accesses is widely studied (O’Neil et al., 1996; Jagadish et al., 1997; Jermaine\net al., 1999; Buchsbaum et al., 2000; Yi, 2012). Bose et al. (2008) studied the dynamic optimality of B-Trees and presented\na self-adjusting B-Tree data structure that is optimal up to a constant factor when Bis constant.\nB-Treap were introduced by Golovin (2008; 2009) as a way to give an efficient history-independent search tree in the\nexternal memory model. These studies revolved around obtaining O(logBn)worst-case costs that naturally generalize\nTreaps. Specifically, for sufficiently small B(as compared to n), Golovin showed a worst-case depth of O(1\nαlogBn)with\nhigh probability, where B= Ω(ln1/(1−α)n). The running time of this structure has recently been improved by Safavi &\nSeybold (2023) via a two-layer design.\nThe large node sizes of B-Trees interact naturally with the external memory model, where memory is accessed in blocks\nof size B(Brodal & Fagerberg, 2003; Vitter, 2001). The external memory model itself is widely used in data storage and\nretrieval (Margaritis & Anastasiadis, 2013), and has also been studied in conjunction with learned indices (Ferragina et al.,\n2020). Several previous results discuss the trade-off between update time and storage utilization (Brown, 2014; Fagerberg\net al., 2019).\n13\n\nOn the Power of Learning-Augmented Search Trees\nB. Proofs of Treaps\nIn this section, we formally prove the properties of traditional treaps in Section 2 and the static optimality of our learning-\naugmented BST in Section 2.2.\nLemma 2.2 ((Aragon & Seidel, 1989)) .LetU(0,1)be the uniform distribution over the real interval [0,1].Ifpriority ∼\nU(0,1)n, each Treap node xhas depth Θ(log2n)with high probability.\nProof. Notice that depth (x), the depth of item xin the Treap, is the number of ancestors of xin the Treap. Linearity of\nexpectation yields\nE[depth (x)] =X\ny∈[n]E[1yis an ancestor of x]\n=X\ny∈[n]Pr (yis an ancestor of x)\n=X\ny∈[n]Pr\u0012\npriorityy= max\nz∈[x,y]priorityz\u0013\n=X\ny∈[n]1\n|x−y+ 1|= Θ(log2n).\nTheorem 2.8. For any item x∈[n], we set its priority as\npriorityx:=−\u0016\nlog2log2m\nfx\u0017\n+δx, δx∼U(0,1).\nIn the corresponding Treap, each node xhas expected depth O(log2(m/f x)).Therefore, the total time for processing the\naccess sequence is O(P\nxfxlog2(m/f x)), which matches the performance of the optimal static BSTs up to a constant\nfactor.\nProof. Given item frequencies , we define the following wassignment:\nwx:=fx\nm, x∈[n]. (4)\nOne can verify that ∥w∥1=O(1). By Theorem 2.4, the expected depth of each item xisO(log2(m/f x)).\nC. Learning-Augmented B-Trees\nWe now extend the ideas above, specifically the composite priority notions, to B-Trees in the External Memory Model . We\nshow that the learning-augmented B-Treaps (Appendix C.1) obtain static optimality (Appendix C.2) and is robust to the\nnoisy predicted scores (Appendix C.3). This model is also the basis of our analyses in online settings in Appendix D.\nC.1. Learning-Augmented B-Treaps\nWe first formalize this extension by incorporating our composite priorities with the B-Treap data structure from (Golovin,\n2009) and introducing offsets in priorities.\nLemma C.1 (B-Treap, (Golovin, 2009)) .Given the unique binary Treap (T,priorityx)over the set of items [n]with their\nassociated priorities, and a target branching factor B= Ω(ln1/(1−α)n)for some α >0. Assuming priorityxare drawn\nuniformly from (0,1), we can maintain a B-Tree TB, called the B-Treap , uniquely defined by T. This allows operations\nsuch as Lookup ,Insert , and Delete of an item to touch O(1\nαlogBn)nodes in TBin expectation.\nIn particular, if B=O(n1/2−δ)for some δ >0, all above performance guarantees hold with high probability.\nThe main technical theorem is the following:\n14\n\nOn the Power of Learning-Augmented Search Trees\nTheorem C.2 (Learning-Augmented B-Treap via Composite Priorities) .Denote w= (w1,···, wn)∈(0,1)nas a score\nassociated with each element of [n]such that ∥w∥1=O(1)and a branching factor B= Ω(ln1/(1−α)n), there is a\nrandomized data structure that maintains a B-Tree TBoverUsuch that\n1. Each item xhas expected depth O(1\nαlogB(1/wx)).\n2. Insertion or deletion of item xinto/from Ttouches O(1\nαlogB(1/wx))nodes in TBin expectation.\n3. Updating the weight of item xfromwtow′touches O(1\nα|logB(w′/w)|)nodes in TBin expectation.\nWe consider the following priority assignment scheme: For any xand its corresponding score wx, we always maintain:\npriorityx:=−⌊log2logB1\nwx⌋+δ, δ∼U(0,1).\nIn addition, if B=O(n1/2−δ)for some δ >0, all above performance guarantees hold with high probability 1−δ.\nThe learning-augmented B-Treap is created by applying Lemma C.1 to a partition of the binary Treap T. Each item xhas a\npriority in the binary Treap T, defined as:\npriorityx=−\u0016\nlog2logB1\nwx\u0017\n+δx, δx∼U(0,1),for all x∈U. (5)\nWe then partition the binary Treap Tbased on each item’s tier. The tier of an item is defined as the absolute value of the\nintegral part of its priority, i.e., τxdef=⌊log2logB(1/wx)⌋.\nProof of Theorem C.2. To formally construct and maintain TB, we follow these steps:\n1. Start with a binary Treap (T,priorityx)with priorities defined using equation (5).\n2.Decompose Tinto sub-trees based on each item’s tier, resulting in a set of maximal sub-trees with items sharing the\nsame tier.\n3. For each Ti, apply Lemma C.1 to maintain a B-Treap TB\ni.\n4.Combine all the B-Treaps into a single B-Tree, such that the parent of root(TB\ni)is the B-Tree node containing the\nparent of root(Ti).\nNow, let’s analyze the depth of each item x. Keep in mind that any item yin the same B-Tree node shares the same tier.\nTherefore, we can define the tier of each B-Tree node as the tier of its items.\nSuppose x1, x2, . . .are the B-Tree nodes we encounter until we reach x. The tiers of these nodes are in non-increasing\norder, that is, τxi≥τxi+1for any i. We’ll define Ctas the number of items of tier tfor any t. As per the definition (refer to\nequation (5)), we have:\nCt=O(B2t),for all t\nUsing Lemma C.1 and the fact that B=O(C1/2\nt), t≥1, we find that the number of nodes among xiof tier tis\nO(1\nαlogBCt) =O(2t/α)with high probability. As a result, the number of nodes touched until reaching xis, with high\nprobability:\nτxX\nt=0O(2t/α) =O(2τx/α) =O\u00121\nαlogB1\nwx\u0013\nThis analysis is also applicable when performing Lookup, Insert, and Delete operations on item x.\nThe number of nodes touched when updating an item’s weight can be derived from first deleting and then inserting the item.\n15\n\nOn the Power of Learning-Augmented Search Trees\nC.2. Static Optimality\nIn this section, we show that with our priority assignment, the learning-augmented B-Treaps are statically optimal. Let\nx(1), x(2), . . . , x (m)represent an access sequence of length m. We define the relative frequency of each item xas\npxdef=|{i∈[m]|x(i)=x}|\nm. A static B-Tree is statically optimal if the depth of item xis:\ndepth (x) =O\u0012\nlogB1\npx\u0013\n,for all x∈[n]\nAs a corollary of Theorem C.2, we show that if we are given the relative frequency px, the learning-augmented B-Treaps\nwith our priority assignment achieves Static Optimality with weights wxdef=px.\nCorollary C.3 (Static Optimality for B-Treaps) .Given the relative frequency pxof each item x∈[n], and a branching\nfactor B= Ω(ln1.1n), there exists a randomized data structure that maintains a B-Tree TBover[n]such that each item x\nhas an expected depth of O(logB1/px). That is, TBachieves Static Optimality , meaning the total number of nodes touched\nisO(OPTstatic\nB)in expectation, where:\nOPTstatic\nBdef=m·X\nx∈[n]pxlogB1\npx(6)\nFurthermore, if B=O(n1/2−δ)for some δ >0, all above performance guarantees hold with high probability.\nC.3. Robustness Guarantees\nIn practice, we would not have access to the relative frequency px. Instead, we will have an inaccurate prediction qx. Letp\nandqbe the probability distribution over [n]such that p(x) =px,q(x) =qx. In this section, we will show that B-Treap\nperformance is robust to the error. Specifically, we analyze the performance under various notions of error in the prediction.\nThe notions listed here are the ones used for learning discrete distributions (refer to (Canonne, 2020) for a comprehensive\ndiscussion).\nCorollary C.4 (Kullback—Leibler (KL) Divergence) .If we are given a density prediction qsuch that dKL(p;q) =P\nxpxln(px/qx)≤ϵ, the total number of touched nodes is\nO\u0010\nOPTstatic\nB+ϵm\nlnB\u0011\nProof. Given the inaccurate prediction q, the total number of touched nodes in TBis\nO X\nxm·pxlogB1\nqx!\n=O X\nxm·pxlogB1\npx+m·X\nxpxlogBpx\nqx!\n=O(OPTstatic\nB+mdKL(p;q)\nlnB)\nCorollary C.5 (χ2).If we are given a density prediction qsuch that χ2(p;q) =P\nx(px−qx)2/qx≤ϵ, the total number\nof touched nodes is\nO\u0010\nOPTstatic\nB+ϵm\nlnB\u0011\nProof. The corollary follows from Corollary C.5 and the fact dKL(p;q)≤χ2(p;q)≤ϵ.\nCorollary C.6 (L∞Distance) .If we are given a density prediction qsuch that ∥p−q∥∞≤ϵ, the total number of touched\nnodes is\nO\u0000\nOPTstatic\nB+mlogB(1 +ϵn)\u0001\n16\n\nOn the Power of Learning-Augmented Search Trees\nProof. For item xwith its marginal probability smaller than 1/1000n, its expected depth in the B-Treap is O(logBn)using\neither pxorqxas its score. If item x’s marginal probability is at least 1/1000n, theL∞distance implies that\npx\nepx= 1 +px−epx\nepx≤1 +ϵ\n1/(1000 n)= 1 + 1000(1 + ϵn)\nTherefore, item x’s expected depth in the B-Treap with score qis roughly\nO\u0012\nlogB1\npx+ logBpx\nqx\u0013\n≤O\u0012\nlogB1\npx+ logB(1 +ϵn)\u0013\nThe corollary follows.\nCorollary C.7 (L2Distance) .If we are given a density prediction qsuch that ∥p−q∥ ≤ϵ, the total number of touched\nnodes is\nO\u0000\nOPTstatic\nB+mlogB(1 +ϵn)\u0001\nProof. This claim follows from Corollary C.6 and the fact ∥p−q∥∞≤ ∥p−q∥2≤ϵ.\nCorollary C.8 (Total Variation) .If we are given a density prediction qsuch that dTV(p,q) = 0 .5∥p−q∥1≤ϵ, the total\nnumber of touched nodes is\nO\u0000\nOPTstatic\nB+mlogB(1 +ϵn)\u0001\nProof. This claim follows from Corollary C.6 and the fact ∥p−q∥∞≤ ∥p−q∥1≤2ϵ.\nCorollary C.9 (Hellinger Distance) .If we are given a density prediction qsuch that dH(p,q) = 0 .5∥√p−√q∥2≤ϵ, the\ntotal number of touched nodes is\nO\u0000\nOPTstatic\nB+mlogB(1 +ϵn)\u0001\nProof. This claim follows from Corollary C.6 and the fact ∥p−q∥∞≤2√\n2dH(p,q)≤2√\n2ϵ.\nD. Dynamic Learning-Augmented Search Trees\nIn this section, we investigate the properties of dynamic B-trees that permit modifications concurrent with sequence\naccess. Prioritizing items that are anticipated to be accessed in the near future to reside at lower depths within the tree can\nsignificantly reduce access times. Nonetheless, updating the B-trees introduces additional costs. The overarching goal is to\nminimize the composite cost, which includes both the access operations across the entire sequence and the modifications to\nthe B-trees. We specifically concentrate on the study of locally dynamic B-trees, which are characterized by the restriction\nthat tree modifications are limited solely to the adjustment of priorities for the items being accessed.\nIn Appendix D.1, we give the total cost guarantees for any locally dynamic B-trees. In Appendix D.2, we establish\nthe robustness guarantees in the context of imprecise priority scores, which may be given from a learning oracle. In\nAppendix D.3, we demonstrate that the dynamic learning-augmented B-trees with a specific priority based on the working\nset size — the number of distinct items requested between two consecutive accesses — achieves the working set property.\nFull details are included in the appendix. Finally, in Appendix D.4, we analyze the general dynamic B-trees with general\ntime-varying priorities.\nD.1. Locally Dynamic B-trees\nOur objective is to maintain a data structure that minimizes the total cost of accessing the sequence Sgiven the time-varying\nscore w(i)∈(0,1)n, i∈[m]associated to each item. Here, we focus on the dynamic B-trees that update the priorities of\nonly the items being accessed at any given moment, leaving the priorities of all other items unchanged. We refer to these as\nlocally dynamic B-trees .\n17\n\nOn the Power of Learning-Augmented Search Trees\nGiven nitems, denoted as [n] ={1,···, n}, and a sequence of access sequence X= (x(1), . . . , x (m)), where x(i)∈\n[n]. At time i∈[m], there exists some time-dependent score wijassociated with each item j∈[n]. Let w(i) =\n(wi,1,···, wi,n)∈(0,1]nbe the time-varying score vector. The score w(i)is defined to be locally changed if it only differs\nfrom the previous score vector at the index of the item being accessed. In other words, at each time i∈ {1,2,···, n}, we\nhavewi,j=wi−1,jfor any j̸=x(i). The locally dynamic B-Treap is then defined as a B-Treap whose priorities are updated\naccording to the locally changed score. For any vector w, we write logwas the vector taking the element-wise logonw.\nWe give the guarantees in Theorem D.1.\nTheorem D.1 (Locally-Dynamic B-Treap with Given Priorities) .Given the locally changed scores w(i)∈(0,1)n, i∈[m]\nsatisfying ∥w(i)∥1=O(1)and a branching factor B= Ω(ln1.1n), there is a randomized data structure that maintains a\nB-Tree TBover[n]such that when accessing the item x(i)at time i, the expected depth of item x(i)isO(logB1\nwi,x(i)).The\nexpected total cost for processing the whole access sequence Xis\ncost(X,w) =O \nnlogBn+mX\ni=1logB1\nwi,x(i)!\n.\nMoreover, if B=O(n1/2−δ)for some δ >0, the guarantees hold with probability 1−δ.\nThe proof is an application of Theorem C.2, where the priority function dynamically changes as time goes on, rather than\ntheStatic Optimality case where the priority is fixed beforehand.\nProof of Theorem D.1. Initially, we set the priority for all items to be 1, and insert all items into the Treap. For any time\ni∈[n], forj∈[n]such that wi−1,j̸=wi,j, we set\npriority(i)\nj:=−⌊log4logB1\nwi,j⌋+δij, δij∼U(0,1).\nSince∥w(i)∥1=O(1), i∈[m], by Theorem C.2, the expected depth of item s(i)isO(logB1\nwi,x(i)). The total cost for\nprocessing the sequence consists of both accessing x(i)and updating the priorities. The expected total cost for all the\naccesses is\nO mX\ni=1logB1\nwi,x(i)!\n.\nThen we will calculate the cost to update the Treap. Since the priority of an item only changes when it is accessed. Updating\nthe priority of x(i)from wi−1,x(i)towi,x(i)has cost\nO\u0012\f\f\f\flogBwi−1,x(i)\nwi,x(i))\f\f\f\f\u0013\n.\nHence we can bound the expected total cost for maintaining the Treap by\nO \nnlogBn+mX\ni=2\f\f\f\flogBwi−1,x(i)\nwi,x(i)\f\f\f\f!\n=O \nnlogBn+ 2mX\ni=1logB1\nwi,x(i)!\n.\nTogether the expected total cost is\nO \nnlogBn+mX\ni=1logB1\nwi,x(i)!\n.\nThe high probability bound follows similarly as Theorem C.2.\n18\n\nOn the Power of Learning-Augmented Search Trees\nD.2. Robustness Guarantees\nWe have shown that given time-varying scores associated with each item w(i), there exists a B-Tree that gives us the total\ncost in terms of the scores. In this section, we address scenarios in which precise score w(i)are not accessible. Utilizing a\nlearning oracle that predicts the logarithm of the score, we demonstrate that the total cost incorporates an additive term\ncorresponding to the mean absolute error (MAE) of the logarithm of the scoresPm\ni=1|logBwi,x(i)−logBewi,x(i)|. We\npredict the logarithm of the score instead of itself to better capture the scale of it.\nTheorem D.2 (Locally Dynamic B-Treap with Predicted Scores) .Given the predicted locally changed scores ew(i)∈(0,1)n\nsatisfying ∥ew(i)∥1=O(1),ewi,j≥1/poly( n)and a branching factor B= Ω(ln1.1n), there is a randomized data structure\nthat maintains a B-Tree over the nkeys such that the expected total cost for processing the whole access sequence Xis\ncost(X,ew) = cost(X,w) +O mX\ni=1\f\flogBwi,x(i)−logBewi,x(i)\f\f!\n.\nMoreover, if B=O(n1/2−δ)for some δ >0, the guarantees hold with probability 1−δ.\nProof. We apply Theorem D.1 with the predicted score ew(i), and get the expected total loss is\ncost(X,ew) =O \nnlogBn+mX\ni=1logB1\newi,x(i)!\n≤cost(X,w) +O mX\ni=1\f\f\f\flogB1\newi,x(i)−logB1\nwi,x(i)\f\f\f\f!\n=cost(X,w) +O mX\ni=1\f\flogBwi,x(i)−logBewi,x(i)\f\f!\n.\nD.3. Working Set Property\nIn data structures, the working set is the collection of data that a program uses frequently over a given period. This concept is\nimportant because it helps us understand how a program interacts with memory and thus enables us to design more efficient\ndata structures and algorithms. For example, if a program is sorting a list, the working set might be the elements of the list it\nis comparing and swapping right now. The size of the working set can affect how fast the program runs. A smaller working\nset can make the program run faster because it means the program doesn’t need to reach out to slower parts of memory as\noften. In other word, if we know which parts of a data structure are used most, we can organize the data or even the memory\nin a way that makes accessing these parts faster, which can speed up the entire program.\nIn this section, we construct dynamic learning-augmented B-treaps that achieve the working set property. We define the\nworking-set size as the number of distinct items accessed between two consecutive accesses. Correspondingly, we design a\ntime-varying score, working-set score , as the reciprocal of the square of one plus working-set size. We will show that the\nworking-set score is locally changed and there exists a data structure that achieves the working-set property , which states\nthat the time to access an element is a logarithm of its working-set size. The formal definition of the working-set size and\nthe main theorems in this section are presented as follows.\nDefinition D.3 (Previous and Next Access prev(i, x)and next(i, x)).Letprev(i, x)be the previous access of item xat\nor before time i, i.e, prev(i, x):= max {i′≤i|x(i′) =x}.Letnext(i, x)to be the next access of item xafter time i, i.e,\nnext(i, x):= min {i′> i|x(i′) =x}.\nDefinition D.4 (Working-set Size work(i, x)).Define the working-set Size work(i, x)to be the number of distinct items\naccessed between the previous access of item xat or before time iand the next access of item xafter time i. That is,\nwork(i, x)def=|{x(prev(i, x) + 1) ,···, x(next(i, x))}|.\nIfxdoes not appear after time i, we define work(i, x):=n.\n19\n\nOn the Power of Learning-Augmented Search Trees\nTheorem D.5 (Dynamic B-Treaps with Working-set Priority) .With the working-set size work(i, x)known and the branching\nfactor B= Ω(ln1.1n), there is a randomized data structure that maintains a B-Tree TBover[n]with the priorities assigned\nas\npriority (i, x) =−⌊log2logB(1 + work(i, x))2⌋+U(0,1).\nUpon accessing the item xat time i, the expected depth of item xisO(logB(1 + work(i, x)).The expected total cost for\nprocessing the whole access sequence Xis\ncost(X,priority ) =O \nnlogBn+mX\ni=1logB(1 + work(i, x))!\nIn particular, if B=O(n1/2−δ)for some δ >0, the guarantees hold with probability 1−δ.\nRemark. Consider two sequences with length m,X1= (1 ,2,···, n,1,2,···, n,···,1,2,···, n),X2=\n(1,1,···,1,2,2,···,2,···, n, n,···, n). Two sequences have the same total cost if we have a fixed score. However, X2\nshould have less cost because of its repeated pattern. Given the frequency freqas a time-invariant priority, by Corollary C.3,\nthe optimal static costs are\ncost(X1,freq) = cost(X2,freq) =O(mlogBn).\nBut for the dynamic B-Trees, with the working-set score, we calculate both costs from Theorem D.5 as\ncost(X1, ω) =O(mlogB(n+ 1)) ,\ncost(X2, ω) =O(nlogBn+mlogB3).\nThis means that our proposed priority can better capture the timing pattern of the sequence and thus can even do better than\nthe optimal static setting.\nThe main idea to prove Theorem D.5 is to show that (1) the working-set size is locally changed and (2) the corresponding\npriority satisfies the regularity conditions in Theorem D.1. To complete the proof, we introduce the interval-set size\ninterval (i, x). See Figure 7 as an illustration.\nDefinition D.6 (Interval-set Size interval (i, x)).Define the Interval-set Size interval (i, x)to be the number of distinct items\naccessed between time iand the next access of item xafter time i. That is,\ninterval (i, x):=|{x(i+ 1),···, x(next(i, x))}|.\nIfxdoes not appear after time i, we define interval (i, x):=n.\nFurthermore, we define the working-set score as follows.\nDefinition D.7 (Working-set Score ω(i, x)).Define the time-varying priority as the reciprocal of the square of one plus\nworking-set size. That is,\nω(i, x) =1\n(1 + work(i, x))2\nNext, we will show that the interval set priority is O(1)for any time i∈[m]in Lemma D.8. The proof has three steps.\nFirstly, the interval-set size at time iis always a permutation of [n]. Secondly, for any i∈[m], x∈[n], the working-set\nsize is always no less than the interval-set size. Therefore, for any i∈[m],thel1norm of working-set score vector\nω(i)def= (ω(i,1),···, ω(i, n))can be upper bounded byPn\nj=11/(1 +j)2=O(1).\nLemma D.8 (Norm Bound for Working-set Score) .Fix any timestamp i∈[m],\nnX\nj=1ω(i, j) =O(1).\n20\n\nOn the Power of Learning-Augmented Search Trees\n123123123!(#)%142538697123123interval%,+=+(%+/),⋯,+(next(%,+))313332112211322312213313333333232333333233work%,+=+prev%,++/,,⋯,+next%,+!!!next(!,()prev(!,()interval!,(:#ofdistinctelementswork!,(:#ofdistinctelements\nFigure 7. An example of n= 3,X= (1,2,3,1,1,2,3,···). For any i, x,work(i)is a permutation of [n];interval (i, x)≥work(i, x);\ninterval (i, x)changes only when x(i) =x(highlighted in orange).\nProof. We first show that at any time i∈[m], the interval-set size is a permutation of [n]. By definition, interval (i, x)is\nthe number of items ysuch that next(i, y)≤next(i, x). Let π1,···, πnbe a permutation of all items [n]in the order of\nincreasing next(i, x). Then for any item x,interval (i, x)is the index of xinπ. So the sum of the reciprocal of squared\ninterval (i, x)is upper bounded by\nnX\nj=11\n(1 + interval (i, j))2=nX\nj=11\n(1 +j)2= Θ(1) .\nSecondly, recall that prev(i, x)≤i, and hence for any i∈[m], x∈[n],work(i, x)≥interval (i, x).So we have the upper\nbound for working-set score as follows.\nnX\nj=1ω(i, j) =nX\nj=11\n(1 + work(i, x))2\n≤nX\nj=11\n(1 + interval (i, j))2\n=O(1).\nSince we have shown the working-set score has constant l1norm and in each timestamp, we only update one item’s priority.\nWe are ready to prove and show the efficiency of the corresponding B-Treap.\nProof of Theorem D.5. By Lemma D.8, we know ∥ω(i)∥=O(1),for all i∈[m]. Also by definition of work(i, x), for any\nx∈[n],work(i−1, x)̸=work(i, x)only when x(i) =x. So for each time i, at most one item (i.e., x(i)) changes its\npriority. So we apply Theorem D.1 with wi,j=ω(i, x), i∈[m], x∈[n], and get the total cost\ncost(X, ω) =O \nnlogBn+mX\ni=1logB(1 + work(i, x(i)))!\nFurthermore, we use the following theorem to show the robustness of the results when the scores are inaccurate. This is a\ndirect corollary of Theorem D.2.\nTheorem D.9 (Locally-Dynamic B-Treaps with Predictions) .Given the predicted locally changed working-set score\neω(i)∈(0,1)nsatisfying ∥eω(i)∥1=O(1),eωi,j≥1/poly( n)and the branching factor B= Ω(ln1.1n), there is a\n21\n\nOn the Power of Learning-Augmented Search Trees\nrandomized data structure that maintains a B-Tree over the nkeys such that the expected total cost for processing the whole\naccess sequence Xis\ncost(X,eω) = cost(X, ω) +O mX\ni=1\f\flogBωi,x(i)−logBeωi,x(i)\f\f!\n.\nIn particular, if B=O(n1/2−δ)for some δ >0, the guarantees hold with probability 1−δ.\nD.4. General Results for Dynamic B-Trees\nIn this section, we give the results for general dynamic B-trees. We first construct the dynamic B-Treaps and give the\nguarantees when we have access to the real-time priorities for each item in Appendix D.4.1. Then we analyze the dynamic\nB-trees given the estimation the time-varying priorities in Appendix D.4.2. We use the same notation in Appendix D.\nD.4.1. D YNAMIC B-T REAP WITH GIVEN PRIORITIES\nTheorem D.10 (Dynamic B-Treap with Given Priorities) .Given the time-varying scores w(i)∈(0,1)n, i∈[m]satisfying\n∥w(i)∥1=O(1)and a branching factor B= Ω(ln1.1n), there is a randomized data structure that maintains a B-Tree TB\nover[n]such that when accessing the item x(i)at time i, the expected depth of item x(i)isO(logB1\nwi,x(i)).The expected\ntotal cost for processing the whole access sequence Xis\ncost(X,w) =O\nnlogBn+mX\ni=1logB1\nwi,x(i)+mX\ni=2nX\nj=1\f\f\f\flogB1\nwi,j−logB1\nwi−1,j\f\f\f\f\n.\nIn particular, if B=O(n1/2−δ)for some δ >0, the guarantees hold with probability 1−δ.\nProof. Initially, we set the priority for all items to be 1, and insert all items into the Treap. For any time i∈[n], forj∈[n]\nsuch that wi−1,j̸=wi,j, we set\npriority(i)\nj:=−⌊log4logB1\nwi,j⌋+δij, δij∼U(0,1).\nSince∥w(i)∥1=O(1), i∈[m], by Theorem C.2, the expected depth of item s(i)isO(logB1\nwi,x(i)). The total cost for\nprocessing the sequence consists of both accessing x(i)and updating the priorities. The expected total cost for all the\naccesses is\nO mX\ni=1logB1\nwi,x(i)!\n.\nThen we will calculate the cost to update the Treap. Updating the priority of jfrom wi−1,jtowi,jhas cost\nO(|logB(wi−1,j/wi,j)|). Hence we can bound the expected total cost for maintaining the Treap by\nO\nnlogBn+mX\ni=2nX\nj=1\f\f\f\flogBwi−1,j\nwi,j\f\f\f\f\n.\nTogether the expected total cost is\nO\nnlogBn+mX\ni=1logB1\nwi,x(i)+mX\ni=2nX\nj=1\f\f\f\flogB1\nwi,j−logB1\nwi−1,j\f\f\f\f\n.\nThe high probability bound follows similarly as Theorem C.2.\nRemark. The total cost for processing the access sequence has three terms. The first two terms are the same as in the\nstatic optimality bound, while the third term is incurred from updating the scores. Hence, here is a trade-off between the\ncosts of updating items and the benefits from the time-varying scores. Moreover, the locally-dynamic B-trees can avoid the\nhigh cost of keeping updating the scores because only one score is changed per time.\n22\n\nOn the Power of Learning-Augmented Search Trees\nD.4.2. D YNAMIC B-T REAP WITH PREDICTED PRIORITIES\nIn this section, we give the guarantees for the dynamic B-Treaps with predicted priorities learned by a machine learning\noracle. Similar as in Appendix D.4.2, we here predict logB1\nwi,jto better capture the scale of the scores. And we will find\nthat the total cost using the B-Trees using the predicted scores is equal to the cost using the accurate priorities plus an\nadditive error that is linear in the mean absolute error of our prediction scores:\nmX\ni=1nX\nj=1\f\f\f\flogB1\nwi,j−logB1\newi,j\f\f\f\f.\nTheorem D.11 (Dynamic B-Treap with Predicted Scores) .Given the predicted time-varying scores ew(i)∈(0,1)nsatisfying\n∥ew(i)∥1=O(1),ewi,j≥1/poly( n)and a branching factor B= Ω(ln1.1n), there is a randomized data structure that\nmaintains a B-Tree over the nkeys such that the expected total cost for processing the whole access sequence Xis\ncost(X,ew) = cost(X,w) +O\nmX\ni=1nX\nj=1\f\f\f\flogB1\nwi,j−logB1\newi,j\f\f\f\f\n\nIn particular, if B=O(n1/2−δ)for some δ >0, the guarantees hold with probability 1−δ.\nProof. We apply Theorem D.10 with score ew, and get the expected depth of x(i)is\nO\u0012\nlogB1\newi,j\u0013\n.\nThe expected total cost is\ncost(X,ew) =O\nnlogBn+mX\ni=1logB1\newi,x(i)+mX\ni=2nX\nj=1\f\f\f\flogB1\nwi,j−logB1\newi−1,j)\f\f\f\f\n\n=cost(X,w) +O mX\ni=1\f\f\f\flogB1\nwi,x(i)−logB1\newi,x(i)\f\f\f\f!\n+O\nmX\ni=2nX\nj=1\f\f\f\flogB1\nwi,j−logB1\newi,j\f\f\f\f+m−1X\ni=1nX\nj=1\f\f\f\flogB1\nwi,j−logB1\newi,j\f\f\f\f\n\n=cost(X,w) +O\nmX\ni=1nX\nj=1\f\f\f\flogB1\nwi,j−logB1\newi,j\f\f\f\f\n\nE. Additional Experiments on Inaccurate Prediction Oracle\nE.1. Mixture of Distributions\nWe consider a setting where the actual data follows a mixture of two distributions while the frequency predictor provides\nonly a single distribution. Specifically, we assume that the predicted frequency follows the adversarial distribution, whereas\nthe actual access sequence is generated by a mixture of two distributions: with probability w, the item follows the adversarial\ndistribution and with probability 1−w, it follows the Zipfian distribution (Figure 8a, Figure 8c, Figure 8e) or uniform\ndistribution (Figure 8b, Figure 8d, Figure 8f). We set n= 1000 and vary mover[2000 ,6000,10000 ,16000 ,20000] . The x-\naxis represents the number of unique items, and the y-axis denotes the number of comparisons made, which measures access\ncost. We compare our Treaps against Splay trees and randomized Treaps. Our experiments show that our Treaps outperform\nboth alternatives, even when 75% of the data comes from an unknown distribution (either Zipfian or uniform). Furthermore,\nas the prediction quality decreases, the performance of our Treaps remains stable, demonstrating their robustness.\n23\n\nOn the Power of Learning-Augmented Search Trees\n(a) Zipfian mix, 25%\n (b) Uniform mix, 25%\n(c) Zipfian mix, 50%\n (d) Uniform mix, 50%\n(e) Zipfian mix, 75%\n (f) Uniform mix, 75%\nFigure 8. Error rates under mixture distributions. Left column: Zipfian mixing; Right column: uniform mixing.\n24",
  "textLength": 83930
}