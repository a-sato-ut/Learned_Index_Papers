{
  "paperId": "45850640a59627263b5a9b995be5f248aefc20bc",
  "title": "Learning-Augmented Algorithms for Online TSP on the Line",
  "pdfPath": "45850640a59627263b5a9b995be5f248aefc20bc.pdf",
  "text": "Learning-Augmented Algorithms for Online TSP on the Line\nThemis Gouleakis1, Konstantinos Lakis2, and Golnoosh Shahkarami3\n1National University of Singapore, tgoule@nus.edu.sg\n2National and Kapodistrian University of Athens, konstlakis@gmail.com\n3Max Planck Institut für Informatik, Universität des Saarlandes,\ngshahkar@mpi-inf.mpg.de\nAbstract\nWe study the online Traveling Salesman Problem (TSP) on the line augmented with\nmachine-learned predictions. In the classical problem, there is a stream of requests released\nover time along the real line. The goal is to minimize the makespan of the algorithm. We\ndistinguish between the openvariant and the closedone, in which we additionally require\nthe algorithm to return to the origin after serving all requests. The state of the art is\na1:64-competitive algorithm and a 2:04-competitive algorithm for the closed and open\nvariants, respectively [10]. In both cases, a tight lower bound is known [6,10].\nIn both variants, our primary prediction model involves predicted positions of the requests.\nWe introduce algorithms that (i) obtain a tight 1.5 competitive ratio for the closed variant\nand a 1.66 competitive ratio for the open variant in the case of perfect predictions, (ii) are\nrobust against unbounded prediction error, and (iii) are smooth, i.e., their performance\ndegrades gracefully as the prediction error increases.\nMoreover, we further investigate the learning-augmented setting in the openvariant by\nadditionally considering a prediction for the last request served by the optimal oﬄine\nalgorithm. Our algorithm for this enhanced setting obtains a 1.33 competitive ratio with\nperfect predictions while also being smooth and robust, beating the lower bound of 1.44\nwe show for our original prediction setting for the open variant. Also, we provide a lower\nbound of 1.25 for this enhanced setting.\n1 Introduction\nThe Traveling Salesman Problem (TSP) is one of the most fundamental and widely studied\nproblems in computer science, both in its oﬄine version [16], where the input is known in\nadvance, and the online version [6] where it arrives sequentially. In this paper, we consider the\nonline Traveling Salesman Problem (TSP) on the real line. This version of the problem arises\nin real-world scenarios such as one dimensional delivery/collection tasks. Such tasks include\nthe operation of elevator systems, robotic screwing/welding, parcel collection from massive\nstorage facilities and cargo collection along shorelines [5,22]. Furthermore, one can think\nof other relevant practical settings such as the movement of emergency evacuation vehicles\nalong a perilous highway, where there cannot be knowledge in advance regarding the time and\nlocation of persons requiring assistance. However, the great availability of data as well as the\nimproved computer processing power and machine learning algorithms can make it possible for\npredictions to be made on these locations (e.g combining information from historical data, the\nweather forecast, etc). In a line of work that started a few years ago [17] and sparked a huge\n1arXiv:2206.00655v1  [cs.DS]  1 Jun 2022\n\ninterest [1,2,13,23 –26], it has been demonstrated that such prior knowledge about the input\nof an online algorithm has the potential to achieve improved performance (i.e competitive\nratio) compared to known algorithms (or even lower bounds) that do not use (resp. assume\nthe absence of) any kind of prediction. Therefore, it is natural to consider ways to utilize this\ninformation in this problem using a so-called learning-augmented approach.\nThe input to our online algorithm consists of a set of requests, each associated with a position\non the real line as well as a release time. An algorithm for this problem faces the task of\ncontrolling an agent that starts at the origin and can move with at most unit speed. The agent\nmay serve a request at any time after it is released. The algorithm’s objective is to minimize\nthe makespan, which is the total time spent by the agent before serving all requests. We have\ntwo diﬀerent variants of the problem, depending on whether the agent is required to return to\nthe origin after serving all the requests or not. This requirement exists in the closedvariant,\nwhile it does not in the openvariant. The makespan in the closed variant is the time it takes\nthe agent to serve the requests andreturn to the origin.\nWe quantify the performance of an online algorithm by its competitive ratio , i.e., the maximum\nratio of the algorithm’s costto that of an optimal oﬄinealgorithmOPT, over all possible\ninputs. We say that an algorithm with a competitive ratio of cisc-competitive. Under\nthis scope, the online TSP on the line has been extensively studied and there have been\ndecisive results regarding lower and upper bounds on the competitive ratio for both variants\nof the problem. Namely, a tight bound of \u00191:64was given for the closed variant, while the\ncorresponding value for the open variant was proven to be \u00192:04[6,10].\n1.1 Our setup\nFirst of all, to deﬁne our prediction model and algorithms, it is necessary to know the number of\nrequestsn1. This setting shows up in various real world scenarios. For example, in the case of\nitem collection from a horizontal/vertical storage facility, the capacity of the receiving vehicle,\nwhich awaits the successful collection of all items in order to deliver them to customers, dictates\nthe number of items to be collected. We note that since nis known, we can assume that each\nprediction corresponds to a speciﬁc request determined by a given labeling, which is shared by\nboth sets (requests and predictions). Under this assumption, we deﬁne the LOCATIONS\nprediction model. In this model, the predictions are estimates for the positions of the requests.\nThe error\u0011increases along with the maximum distance of a predicted location to the actual\nlocation of the identically labeled request and is normalized by the length of the smallest\ninterval containing the entire movement of the optimal algorithm. We also deﬁne an enhanced\nprediction model for the open variant named LOCATIONS +FINAL (LFin short) that\nadditionally speciﬁes a request which is predicted to be served last by OPT. In this model, we\nadditionally consider the error metric \u000e, which increases with the distance of the predicted\nrequest to the request actually served last by OPT. We also normalize \u000ein the same way as \u0011.\nThese models and their respective errors are deﬁned formally in Section 2.\n1Since this (slightly) modiﬁes the original problem deﬁnition, the previous competitive ratio results for the\nclassical problem do not necessarily hold for our setup even without predictions. We show in the Appendix\nthat the bound of 1:64still holds for the closed variant and that a tight bound of 2holds for the open variant.\n2\n\nProperties of learning-augmented algorithms. In the following we formalize the con-\nsistency, robustness and smoothness properties. We say that an algorithm is:\n1)\u000b-consistent , if it is\u000b-competitive when there is no error.\n2)\f-robust, if it is\f-competitive regardless of prediction error.\n3)\r-smoothfor a continuous function \r(err), if it is\r(err)-competitive , whereerris the\nprediction error. Note that errcould potentially be a tuple of error types.\nIn general, if cis the best competitive ratio achievable without predictions, it is desirable to\nhave\u000b<c,\f\u0014k\u0001cfor some constant kand also the function \rshould increase from \u000bto\f\nalong with the error err. We note that c;\u000b;\fand the outputs of \rmay be functions of the\ninput and not constant.\n1.2 Our contributions\nThroughout this paper, we give upper and lower bounds for our three diﬀerent settings\n(closed variant- LOCATIONS , open variant- LOCATIONS , and open variant- LF). The\nlower bounds refer to the case of perfect predictions and are established via diﬀerent attack\nstrategies. That is, we describe the actions of an adversary ADV, who can control only the\nrelease times of the requests and has the goal of maximizing the competitive ratio of any\nalgorithmALG. We emphasize that ADVis given the power to observe ALG’s actions and\nact accordingly. In more detail, ADVdoes not need to specify the release times in advance,\nbut can release a request at time t, taking the actions of ALGuntil timetinto account. This\nis, in fact, the most powerful kind of adversary. The upper bounds are established via our\nalgorithms and are deﬁned for every value of the error(s). Recall that \u0011and\u000erefer to the two\ntypes of error we consider. Our algorithms and attack strategies are intuitively described in\ntheir respective sections. We now present the main ideas and our results.\nClosed variant under LOCATIONS .We will start by intuitively describing our algorithm\nfor this setting and then continue with our lower bound. We design the algorithm FARFIRST .\nThe main idea is that we ﬁrst focus entirely on serving the requests on the side with the\nfurthest extreme, switching to the other side when all such requests are served. When serving\nthe requests on one side, we prioritize them by order of decreasing amplitude. The intuition\nis that we have the least possible amount of leftover work for our second departure from the\norigin, which limits the ways in which an adversary may attack us. We obtain the theorem\nbelow. More details are given in Section 3.\nTheorem 1. The algorithm FARFIRST isminn\n3(1+\u0011)\n2;3o\n-competitive.\nWe emphasize that for \u0011= 0, this competitive ratio remarkably matches our lower bound of\n1:5, makingFARFIRST optimal.\nOur lower bound for this setting is accomplished via an attack strategy that is analogous to a\ncunning magician’s trick. Suppose that the magician keeps a coin inside one of their hands.\nThey then ask a pedestrian to make a guess for which hand contains the coin. If the pedestrian\nsucceeds, they get to keep the coin. However, the magician can always make it so that the\npedestrian fails, for example by having a coin up each of their sleeves and producing the one\n3\n\nnot chosen by the pedestrian. One can draw an analogy from this trick to our attack strategy,\nwhich is described in Section 3 in more detail. In this way, we obtain the theorem below.\nTheorem 2. For any\u000f>0, no algorithm can be (1:5\u0000\u000f)-competitive for closed online TSP\non the line under the LOCATIONS prediction model.\nOpen variant under LOCATIONS .The algorithm we present for this setting is named\nNEARFIRST . This algorithm ﬁrst serves the requests on the side opposite to the one\nFARFIRST would choose. Another divergence from FARFIRST that should be noted is\nthat for the side focused on second, NEARFIRST prioritizes requests that are predicted to\nbecloserto the origin, since there is no requirement to return to it, thus avoiding unnecessary\nbacktracking. More details about the algorithm and the proof of the following theorem are\ngiven in Section 4.1.\nTheorem 3. The algorithm NEARFIRST isminff(\u0011);3g-competitive, where\nf(\u0011) =(\n1 +2(1+\u0011)\n3\u00002\u0011;for\u0011<2\n3\n3; for\u0011\u00152\n3:\nAs in the previous setting, we utilize the \"magician’s trick\" in order to design a similar attack\nstrategy. We describe exactly how this is done in Section 4.1. This leads to the establishment\nof a lower bound, as stated below.\nTheorem 4. For any\u000f>0, no algorithm can be\u0000\n1:44\u0000\u000f\u0001\n-competitive for open online TSP\non the line under the LOCATIONS prediction model.\nOpen variant under LOCATIONS+FINAL .Our algorithmic approach to this setting\nis again similar to the one implemented in NEARFIRST . The diﬀerence is that instead of\nchoosing the side with the near extreme ﬁrst, we choose the side whose extreme is further away\nfrom the predicted endpoint of OPT. We name this algorithm PIVOT, to emphasize that\nthe prediction for the last request acts as a pivot for the algorithm to decide the ﬁrst side it\nwill serve. A theorem about PIVOT is presented below, the proof of which has been given in\nSection 4.2.\nTheorem 5. The algorithm PIVOT isminff(\u0011;\u000e);3g-competitive, where\nf(\u0011;\u000e) =(\n1 +1+2(\u000e+3\u0011)\n3\u00002(\u000e+2\u0011);3\u00002(\u000e+ 2\u0011)>0\n3; 3\u00002(\u000e+ 2\u0011)\u00140:\nFor this setting, we reuse the attack strategy initially designed for the closed variant. The only\ndiﬀerence is that we add another request at the origin with a release time of 4. We explain\nhow we derive the following theorem in Section 4.2.\nTheorem 6. For any\u000f>0, no algorithm can be (1:25\u0000\u000f)-competitive for open online TSP\non the line under the LFprediction model.\nWe brieﬂy summarize our results in Table 1. Note that the lower and upper bound entries\ncorrespond to the no error case. We emphasize that these results are for the case where the\nnumber of requests nis known.\n4\n\nTable 1: Summary of results.\nSetting Lower bound Upper bound Best competitive ratio\nClosed variant without predictions 1.64 1.64 1.64\nClosed variant under LOCATIONS 1.5 1.5 minn\n3(1+ \u0011)\n2;3o\nOpen variant without predictions 2 2 2\nOpen variant under LOCATIONS 1.44 1.66 minn\n1 +2(1+ \u0011)\n3\u00002\u0011;3o\nOpen variant under LF 1.25 1.33 minn\n1 +1+2( \u000e+3\u0011)\n3\u00002(\u000e+2\u0011);3o\n1.3 Related work\nOnline TSP. The online TSP for a general class of metric spaces has been studied by\nAusiello et al. in [6], where the authors show lower bounds of 2for the open variant and\n1:64for the closed variant. These bounds are actually shown on the real line. Additionally, a\n2.5-competitive algorithm and a 2-competitive algorithm are given for the general open and\nclosed variants respectively. A stronger lower bound of 2:04was shown for the open variant\nin [10] by Bjelde et al., where both bounds are also matched in the real line. For the restriction\nof the closed online TSP to the non-negative part of the real line, Blom et al. [11] give a\ntight 1:5-competitive algorithm. By imposing a fairness restriction on the adversary, they also\nobtain a 1:28-competitive algorithm. Jaillet and Wagner [14] introduce the \"online TSP with\ndisclosure dates\", where each request may also be communicated to the algorithm before it is\nreleased. The authors show improvements to the competitive ratios of previous algorithms as\na function of the diﬀerence between disclosure and release dates.\nLearning-augmented algorithms. Learning-Augmented algorithms have received signiﬁ-\ncant attention since the seminal work of Lykouris and Vassilvitskii [17], where they introduced\nthe online caching problem. Based on that model, Purohit et al. [23] proposed algorithms\nfor the ski-rental problem as well as non-clairvoyant scheduling. Subsequently, Gollapudi and\nPanigrahi [13], Wang et al. [25], and Angelopoulos et al. [1] improved the initial ski-rental\nproblem. The latter also proposed algorithms with predictions for the list update and bin\npacking problem and demonstrated how to show lower bounds for algorithms with predictions.\nSeveral works, including Rohatgi [24], Antoniadis et al. [2], and Wei [26], improved the initial\nresults regarding the caching problem.\nThe scheduling problems with machine-learned advice have been extensively studied in the\nliterature. Lattanzi et al. [21] considered the makespan minimization problem with restricted\nassignments, while Mitzenmacher [19] using predicted job processing times in diﬀerent schedul-\ning scenarios. Bamas et al. [7], and Antoniadis et al. [3] focused on the online speed scaling\nproblem using predictions for workloads and release times/deadlines, respectively.\nThere is literature on classical data structures. Examples include the indexing problem, Kraska\net al. [15], bloom ﬁlters, Mitzenmacher [18]. Further learning-augmented approaches on online\nselection and matching problems [4,12] and a more general framework of online primal-dual\nalgorithms [8] also emerged, and there is a survey by Mitzenmacher et al. [20].\n5\n\nIndependent work. Compared to the problem considered in this paper, a more general one,\nthe online metric TSP, as well as a more restricted version in the half-line, have been studied\nin [9] under a diﬀerent setting, concurrently to our work. We note that only the closed variant\nis considered in [9]. Since the prediction model is diﬀerent (predictions for the positions as\nwell as release times of the requests are given) and also a diﬀerent error deﬁnition is used, the\nresults are incomparable.\n2 Preliminaries\nThe problem deﬁnition. In the online TSP on the line, an algorithm controls an agent\nthat can move on the real line with at most unit speed. We have a set Q=fq1;:::;qngof\nnrequests. The algorithm receives the value nas input. Each request qhas an associated\nposition and release time. To simplify notation, whenever a numerical value is expected from a\nrequestq(for a calculation, ﬁnding the minimum of a set, etc.) the term qwill refer to the\nposition of the request. Whenever we need the release time of a request, we will use rel(q).\nAdditionally, the algorithm receives as input a set P=fp1;:::;pngof predictions regarding\nthe positions of the requests. That is, each piattempts to approximate qi. We assume without\nloss of generality that Qalways contains a request q0at the origin with release time 0andP\ncontains a perfect prediction p0= 0for this request2.\nWe usetto quantify time. To describe the position of the agent of an algorithm ALGat time\nt\u00150, we useposALG(t). We may omit this subscript when ALGis clear from context. We can\nassume without loss of generality that pos(0)= 0. The speed limitation of the agent is given\nformally viajpos(t0)\u0000pos(t)j\u0014jt0\u0000tj;8t;t0\u00150. A request qis considered served at time tif\n9t0:pos(t0)=q; rel (q)\u0014t0\u0014t, i.e., the agent has moved to the request no earlier than it is\nreleased. We will say that a request qisoutstanding at timet, ifALGhas not served it by\ntimet, even ifrel(q)>t, i.e.qhas not been released yet. Let tservedenote the ﬁrst point in\ntime when all requests have been served by the agent. Also, let jALGjdenote the makespan of\nan algorithm ALG, for either of the two variants. Then, for the open variant jALGj=tserve\nwhile for the closed one jALGj=minft:pos(t)= 0; t\u0015tserveg. For any sensible algorithm,\nthis is equivalent to tserve+jpos(tserve)j, since the algorithm knows the number of requests and\nwill immediately return to the origin after serving the last one. The objective is to minimize\nthe valuejALGj, utilizing the predictions.\nNotation. We deﬁneL=min(Q)andR=max(Q). Recall that Qcontains a request at\nthe origin and thus L\u00140andR\u00150. We refer to each of these requests as an extremerequest.\nIfjLj>jRj, we deﬁne Far=L;Near =R. Otherwise, Far=R;Near =L. That is,Faris\nthe request with the largest distance from the origin out of all requests. Then, Nearis simply\nthe other extreme. We will also refer to the value jqjas theamplitude of requestq.\nWedenotewith O(t)thesetofoutstandingrequestsattime t. Then,LO(t)=min(O(t)[fpos(t)g)\nandRO(t)=max(O(t)[fpos(t)g). We additionally deﬁne L(t)=min(O(t)[fRg)and\nR(t)=max(O(t)[fLg). The diﬀerence between LO(t);RO(t)andL(t);R(t)is that the\n2This can be seen to be without loss of generality by considering a \"handler\" algorithm ALG 0which adds\nthis request/prediction pair to anyinput and copies the actions of any of our algorithms ALGfor the modiﬁed\ninput. We observe that jOPT jis unchanged and jALG 0j= jALG j.\n6\n\nformer also consider the position of ALGto determine the interval that must be traveled to\nserve all the requests while the latter assume that ALGis already somewhere inside the interval\nof outstanding requests (which may not be true due to ALGmoving to a bad prediction).\nFor technical reasons, we have two diﬀerent notions (and thus terms) for unreleased requests.\nWe will use the same notation for convenience but the terms we introduce will slightly diﬀer\nfor the closed and open variant. For the closed variant, we let Llim= 0;Rlim= 0while\nLlim=R;Rlim=Lfor the open variant. Thus, we deﬁne\nLU[t] =min(fq2Q:rel(q)\u0015tg[fLlimg);\nRU[t] =max(fq2Q:rel(q)\u0015tg[fRlimg);\nwhile also letting\nLU(t) =min(fq2Q:rel(q)>tg[fLlimg);\nRU(t) =max(fq2Q:rel(q)>tg[fRlimg):\nThe former will be used to prove the upper bounds while the latter will be used to prove the\nlower bounds.\nRecall that we assume without loss of generality that 02P. We deﬁne LP=min(P)and\nRP=max(P). We will say that a prediction pis (un)released/outstanding/served if the\nassociated request qis (un)released/outstanding/served. For a request q2Qmatched with a\npredictionp2P, we deﬁne\u0019(q)=pand\u0019\u00001(p)=q. That is, the function \u0019takes us from the\nrequests to the associated predictions and \u0019\u00001takes us from the predictions to the requests.\nTheLOCATIONS prediction model. We now introduce the LOCATIONS prediction\nmodel. Let q1;:::;qnbe a labeling of the requests in Q. The predictions consist of the values\np1;:::;pn, where each piattempts to predict the position of qi.\nError deﬁnition for the LOCATIONS prediction model. To give an intuition for the\nmetric we will introduce, let us ﬁrst describe what it means for a prediction to be bad. In any\nwell-posed deﬁnition, the further piis fromqi, the worse it should be graded. However, we\nmust also take into account the \"scale\" of the problem, meaning the length of the interval\n[L;R]that must be traveled by any algorithm, including OPT. The larger this interval, the\nmore lenient our penalty for pishould be. Therefore, we deﬁne the error as\n\u0011[Q;P] =maxifjqi\u0000pijg\njLj+jRj:\nAdditionally, we deﬁne M=\u0011\u0001(jLj+jRj).\nAn important lemma for the LOCATIONS prediction model. We now present a\nlemma that gives us some intuition about this prediction model.\nLemma 1. LetLP=min(P);RP=max(P). Then,jLPj\u0015jRPjimpliesjLj\u0015jRj\u00002M,\nandjRPj\u0015jLPjimpliesjRj\u0015jLj\u00002M.\nProof.The following claim constitutes the main part of our proof.\n7\n\nClaim 2.1.jLP\u0000Lj\u0014MandjRP\u0000Rj\u0014M.\nProof.IfLP=\u0019(L)=) jLP\u0000Lj=j\u0019(L)\u0000LjorL=\u0019\u00001(LP)=) jLP\u0000Lj=\njLP\u0000\u0019\u00001(LP)jthen we see thatjLP\u0000Lj\u0014M. Thus, we assume the contrary for the rest of\nthe proof.\nSinceLPis by deﬁnition the leftmost prediction, we know that LP<\u0019(L). Additionally, since\nLis the leftmost request, we know that L<\u0019\u00001(LP).\nLetX\u0014Y\u0014Z\u0014Wrepresent the values of the set fL;\u0019\u00001(LP);LP;\u0019(L)gin ascending\norder. It should be easy to see that Xmust be equal to either LorLP. Otherwise, one of\nLP<\u0019(L)orL<\u0019\u00001(LP)is violated, leading to a contradiction. We distinguish two cases.\nCase 1.X=L. In this case, LPcomes after Lbut before \u0019(L)in theX;Y;Z;W ordering.\nTherefore,jL\u0000LPj\u0014jL\u0000\u0019(L)j\u0014M.\nCase 2.X=LP. Similarly, Lcomes after LPbut before\u0019\u00001(LP)in theX;Y;Z;W ordering.\nThus,jLP\u0000Lj\u0014jLP\u0000\u0019\u00001(LP)j\u0014M.\nThe inequalityjRP\u0000Rj\u0014Mcan be seen in a symmetric way.\nUsing this claim, we can now conclude the proof of Lemma 1. We focus on the case jLPj\u0015jRPj;\nthe other case is symmetrical. By Claim 2.1, we have\njRP\u0000Rj\u0014M=) jRPj\u0000jRj\u0014M=) jRj\u0014jRPj+M:\nAdditionally, we have\njLP\u0000Lj\u0014M=) jLPj\u0000jLj\u0014M=) jLj\u0015jLPj\u0000M:\nCombining these inequalities with jLPj\u0015jRPjproves the lemma.\nEnhanced prediction model for the open variant. Motivated by the performance of\nour algorithm under the LOCATIONS prediction model, we enhance it with a prediction f0\nwhich attempts to guess the label fof a request on which OPTmay ﬁnish. We name this new\nmodelLF(short forLOCATIONS +FINAL). The error \u0011is unchanged. We also introduce\na new error metric \u000e. Letqf0be the request associated with the prediction pf0. We then choose\nqfto be a request on which OPTmay ﬁnish that minimizes the distance to qf0. We then\ndeﬁne the new error as\n\u000e[Q;qf;qf0] =jqf0\u0000qfj\njLj+jRj:\nSimilarly to before, we deﬁne \u0001 =\u000e\u0001(jLj+jRj).\n3 Closed Variant\nIn this section, we consider the closed variant under the LOCATIONS prediction model. We\nprovide the FARFIRST algorithm, which obtains a competitive ratio of 1:5with perfect\npredictions and is also smooth and robust. Additionally, we give an attack strategy that\nimplies a lower bound of 1:5for the competitive ratio of any algorithm in this setting, making\nFARFIRST optimal.\n8\n\nTheFARFIRST algorithm. Before giving the algorithm, we deﬁne the FARFIRST\nordering on the predictions of an input. For simplicity, we assume that the furthest prediction\nfrom the origin is positive. Let r1;:::;rabe the positive predictions in descending order of\namplitude and l1;:::;lbbe the negative predictions ordered in the same way. The FARFIRST\nordering is r1;:::;ra;l1;:::;lb. Any predictions on the origin are placed in the end. Ties are\nbroken via an arbitrary label ordering.\nWe present the algorithm through an update function used whenever a request is released. This\nupdate function returns the plan of moves to be executed until the next release of a request.\nNote thatext(side;set )returns the extreme element of the input set in the side speciﬁed,\nwhereside=truemeans the right side. Also, the \bsymbol is used to join moves one after\nanother. When all the moves are executed, the agent waits for the next release. This only\nhappens when waiting on a prediction.\nAlgorithm 1: FARFIRST update function.\nInput : Current position pos, setOof unserved released requests, ﬁrst unreleased\npredictionpinFARFIRST ordering or 0 if none exist, the side farSide with\nthe furthest prediction from the origin.\nOutput: A series of (unit speed) moves to carry out until the next request is released.\nposSide (pos> 0);\npSide (p>0);\nifpos= 0thenposSide farSide ;\nifp= 0thenpSide posSide ;\nreturnmove (ext(posSide;O[fposg))\bmove (ext(pSide;O[fpg))\bmove (p);\nIn order to give some further intuition on FARFIRST , we ﬁrst give the deﬁnition of a phase.\nDeﬁnition 1. A phase of an algorithm ALGis a time interval [ts;te]such thatposALG(ts)= 0,\nposALG(te)= 0andposALG(t0)6= 0;8t02(ts;te). That is,ALGstarts and ends a phase at\nthe origin and does not cross the origin at any other time during the phase.\nIn the following, when we refer to the farside, we mean the side with the furthest prediction\nfrom the origin. The nearside is the one opposite to that. We see that FARFIRST works\nin at most three phases. The ﬁrst phase ends when all predictions on the far side have been\nreleased and the agent has managed to return to the origin with no released and outstanding\nrequest on the far side. During this phase, any request on the far side is served as long\nasFARFIRST does not move closer to the origin than the far side’s extreme unreleased\nprediction. Note that some surprise requests may appear, i.e., far side requests that were\npredicted to lie on the near side. These requests are also served in this phase. The second\nphase lasts while at least one prediction is unreleased. During this phase, the agent serves any\nrequest released on the near side, using the predictions as guidance, similarly to the ﬁrst phase.\nRequests released on the far side are ignored during this phase. Note that no surprises can\noccur here, since all far side predictions were released during the ﬁrst phase. A third phase may\nexist if some requests were released on the far side during the second phase. These requests’\namplitudes are bounded by M, since they were predicted to be positioned on the near side.\nThis simple algorithm is consistent, smooth and robust, as implied by the following theorem.\nTheorem 1. The algorithm FARFIRST isminn\n3(1+\u0011)\n2;3o\n-competitive.\n9\n\nLet us begin with the intuition behind the proof. The 3-robustness is seen using an absolute\nworst case scenario in which FARFIRST isjOPTjunits away from the origin at time jOPTj\n(due to the unit speed limitation), and all the requests to serve are on the opposite side. For\nthe consistency and smoothness, we note that jOPTj\u00152(jNearj+jFarj). It is therefore\nsuﬃcient to prove that\njFARFIRSTj\u0000jOPTj\u0014jNearj+jFarj+ 3\u0011\u0001(jNearj+jFarj) =jNearj+jFarj+ 3M:\nWe refer to the left hand side as the delayofFARFIRST . We now see why this bound holds\nintuitively. We ﬁrst describe a worst case scenario. In this scenario, OPTﬁrst serves the near\nside completely, and then does the same for the far side, without stopping. Let tedenote the\nend time of the ﬁrst phase. We see that te\u0014jOPTj+M, becauseFARFIRST follows the\nfastest possible route serving the requests on the far side, except for a possible delay of M\nattributable to a misleading prediction. Note that in this worst case, all requests on the near\nside must have been released by te. Therefore, FARFIRST accumulates an extra delay of at\nmost 2 times the maximum amplitude of these requests. By Lemma 1, this value is at most\njNearj+jFarj+ 2M. There are also other possibilities than this worst case, but they also\ncan incur a delay of at most jNearj+jFarj+ 3M, becausejOPTjandjFARFIRSTjboth\nincrease when such cases occur.\nWe now give the formal proof of Theorem 1. We will ﬁrst prove the robustness part of this\ntheorem.\nLemma 2. The algorithm FARFIRST is 3-robust.\nProof.Lettfdenote the latest release time for a ﬁxed instance of the problem. We assume\nw.l.o.g. that posFARFIRST (tf)\u00140. Note that after tf,FARFIRST will move to L(tf), then\ntoR(tf)and then back to the origin. Thus, we observe that\njFARFIRSTj=tf+jpos(tf)\u0000L(tf)j+jL(tf)\u0000R(tf)j+jR(tf)j: (1)\nWe distinguish two cases based on the position of FARFIRST at timetf.Case 1.pos(tf)\u0015\nL(tf). In this case, we see that\n(1)=) jFARFIRSTj=tf+pos(tf)\u0000L(tf) +R(tf)\u0000L(tf) +jR(tf)j\u0014\ntf+ 2(jL(t)j+jR(tf)j)\u0014tf+ 2(jLj+jRj)\u00142jOPTj\u00143jOPTj:\nCase 2.pos(tf)<L(tf). Similarly, we have\n(1)=) jFARFIRSTj=tf+L(tf)\u0000pos(tf) +R(tf)\u0000L(tf) +jR(tf)j\u0014\n2tf+ 2jR(t)j\u00142tf+ 2jRj\u00143jOPTj:\nNow, to prove Theorem 1, it remains to show the consistency/smoothness part, which is given\nby the following lemma.\nLemma 3. The algorithm FARFIRST isf(\u0011)-smooth, where f(\u0011) =3(1+\u0011)\n2.\n10\n\nTo prove this lemma, we will bound FARFIRST ’sdelay, i.e. the valuejFARFIRSTj\u0000jOPTj,\nas shown below.\njFARFIRSTj\u0000jOPTj\u0014jNearj+jFarj+ 3M (2)\nThisissuﬃcientbecauseEquation (2)alongwiththeelementaryboundof jOPTj\u00152(jNearj+jFarj)\nprove Lemma 3. Thus, we now state and prove the following claim.\nClaim 3.1. For any input, we have jFARFIRSTj\u0000jOPTj\u0014jNearj+jFarj+ 3M.\nWe assume w.l.o.g. that jLPj\u0014jRPj. Thus, by Lemma 1 we see that\njLj\u0014jRj+ 2M=)2jLj\u0014jRj+jLj+ 2M=\njNearj+jFarj+ 2M=)2jLj+M\u0014jNearj+jFarj+ 3M:\nTherefore, it also suﬃces to show that\njFARFIRSTj\u0000jOPTj\u00142jLj+M (3)\nWe now describe the way in which we will prove Equation (3)or Equation (2). Recall the\ndeﬁnition of a phasegiven in Deﬁnition 1. We note here that we will also use the term delayto\nrefer to how much later a phase ends compared to jOPTj. We will use another claim stating\nthat for a single phase, FARFIRST will serve the requests on the side of the phase as fast\nas possible or OPTis seen to ﬁnish at most Mtime units before FARFIRST ﬁnishes the\nphase, thus \"resetting\" the delay counter. Using this claim for the (at most) three phases of\nFARFIRST , we can indeed show Claim 3.1. In the following, we will consider a phase in the\nright side of the origin. We now deﬁne a term that is similar to RU[t].\nLetRU0[t] =max(fq:q2Q; rel (q)\u0015t; \u0019(q)>0g[f 0g). It should be obvious that RU0[t]\u0014\nRU[t]. Note that when RU0[t] = 0, this means that all requests associated with positive\npredictions have been released by time t, thus prompting FARFIRST to conclude the phase.\nWe should explain here that RU[t]works as a blockforOPT(since it has to wait for a request\nto be released in order to serve it). Similarly RU0[t]works in the same way for FARFIRST ,\nwhich must serve all requests associated with a positive prediction before ending the phase.\nWe observe a useful relationship between these two blocks, which implies that if FARFIRST\nis blocked on a request to the right of M, then so is OPT. This relationship is encapsulated in\nthe following claim.\nClaim 3.2. IfRU[t]>M, thenRU0[t] =RU[t].\nProof.We see that \u0019(RU[t])\u0015RU[t]\u0000M > 0. Therefore, \u0019(RU[t])is apositiveprediction\nand thusRU0[t] =RU[t].\nThe next deﬁnition is about the time it would take (after t) forFARFIRST to serve all\nrequests (to the right of RU[t]) and then reach RU[t]. If this is not more than M, we can\nsee thatFARFIRST is not too far behind OPT. If it is more than M, we shall see that\nFARFIRST has enough information to progress through the phase as fast as possible.\n11\n\nD(t)denotes the least amount of time necessary to serve all requests to the right of RU[t]\n(assumingtheyhavebeenreleased)andthenmoveto RU[t], startingatposition posFARFIRST (t).\nThis amounts to\nD(t) =jposFARFIRST (t)\u0000RO(t)j+jRO(t)\u0000RU[t]j:\nThis function exhibits a useful bound property. If it drops to Mor below at some time t, it\ncan only increase above Magain due to a request release. This property is described more\nformally in the following claim. But ﬁrst, another useful deﬁnition is given.\nWe deﬁneRP[t]as the rightmost positive prediction released at time tor later. If no such\nprediction exists, then RP[t] = 0. Note that FARFIRST never moves to the left of this\nprediction. We now give a relevant claim.\nClaim 3.3.jRP[t]\u0000RU[t]j\u0014M.\nProof.We can see thatjRP[t]\u0000RU0[t]j\u0014Mby the deﬁnition of these terms. If RU[t]>M,\nthe claim immediately follows by Claim 3.2.\nOtherwise, RU[t]\u0014M. We have RP[t]\u00150 =)RU[t]\u0000RP[t]\u0014M. Additionally, we\nknow thatRU0[t]\u0014RU[t]andRP[t]\u0014RU0[t] +M=)RP[t]\u0000RU[t]\u0014M, concluding the\nproof.\nClaim 3.4. Lettdropbe a time point such that D(tdrop)\u0014M. Iftnextis the earliest release\ntime of a request after tdrop, then\nD\u0000\nt0\u0001\n\u0014M;8t02[tdrop;tnext]:\nProof.Observe that RU[t]is constant throughout the interval [tdrop;tnext]. LetRUdenote this\nconstant value. The same is true for RP[t], which is always equal to a speciﬁc value p. We\nsplit the interval [tdrop;tnext]into three parts.\nPart 1. This part lasts while FARFIRST is moving towards a released request to the right\nofmaxfp;pos (t)g. This decreases the value jpos(t)\u0000RO(t)jwhilejRO(t)\u0000RUjis constant\nand thusD(t)cannot increase.\nPart 2. This part lasts while FARFIRST is moving towards p. No released requests exist\nto the right of pos(t)during this time, since that is taken care of in Part 1. Thus, we have\nRO(t)=max(pos(t);RU). Either way, we see that D(t)=jRU\u0000pos(t)jduring this part. At\nthe start of this part, we have D(t)\u0014M. Whenpis reached, we still have D(t)\u0014M, because\nphas a distance of at most MtoRUby Claim 3.3. Thus, we have D(t)\u0014Mthroughout this\npart also.\nPart 3. This part lasts while pos(t)=p, i.e.FARFIRST is waiting on top of p. It can be\nseen thatD(t)is constant throughout this part and also not larger than M.\nWe are now ready to present and prove the main claim we discussed.\nClaim 3.5. Assume without loss of generality that FARFIRST is focusing on the right side\nduring a phase. FARFIRST ﬁnishes this phase as fast as possible or does so at most Mtime\n12\n\nunits after OPT ﬁnishes. More precisely, if the phase spans the time interval [ts;te]and the\nrightmost request served during this phase is Rphase, then\n(te\u0000ts= 2jRphasej)_(te\u0000jOPTj\u0014M):\nFirst of all, note that if at least one request is unreleased at time te, then obviouslyjOPTj\u0015\nte=)te\u0000jOPTj\u0014M. Thus, we can assume in the following that allrequests will have\nbeen released before the end of the phase.\nWe now draw our attention to a point in time that is very central to our proof.\nLettreleasebe the latest release time of a positive request associated with a positive prediction.\nNote thatRU0[t] = 0;8t>trelease. Then, we deﬁne\ntchase =minft:ts\u0014t\u0014trelease;\u0000\nD\u0000\nt0\u0001\n>M;8t<t0\u0014trelease\u0001\ng:\nIntuitively, tchasesigniﬁes the start of a series of unit speed moves executed by FARFIRST\nthat lead to a ﬁnal state in whichFARFIRST has made suﬃcient progress through the phase\nand is also not too far behind OPT. After it reaches this state, it is easier to prove Claim 3.5.\nWe now describe what exactly we mean by this state.\nDeﬁnition 2 (Final state) .We say that FARFIRST has reached a ﬁnal state in a phase at\ntimetstateifpos(tstate)\u0014Mand there are no outstanding requests or unreleased predictions to\nthe right of position M.\nWe see why this ﬁnal state is important in the following claim.\nClaim 3.6. IfFARFIRST is in a ﬁnal state at time tstatewithpos(tstate)=xstate\u0014M,\nthen\n(te\u0000tstate=jxstatej)_(te\u0000jOPTj\u0014M):\nProof.IfFARFIRST moves straight to the origin after tstate, the ﬁrst part is true. On the\nother hand, there are only two possible ways for FARFIRST notto return straight to the\norigin, both of which provide new lower bounds for jOPTj, thus \"resetting\" the delay. One of\nthem is for FARFIRST to wait for a prediction p\u0014Mwith\u0019\u00001(p)\u00140. BecauseOPTalso\nhas to wait for this request and since FARFIRST will ignore it for this phase, the delay is\nseen to be at most Mafter such a case. The other case is for a request qon the right side to\nbe released that was predicted to be on the left side, implying that q\u0014M. Again, it can take\nup to 2jqjtime units for FARFIRST to serve this request and return but also OPTneeds to\nspend at leastjqjtime units to terminate after it is released. Again, the delay is seen to be at\nmostM.\nNow that our goal has been somewhat clariﬁed, we proceed with the main part of the proof.\nWe now show that after tchase,FARFIRST moves to a ﬁnal state as soon as possible.\nClaim 3.7. Letxstate =minfM;RO(tchase)g. Then,FARFIRST reaches a ﬁnal state at\ntimetstateandpos(tstate) =xstate, where\ntstate=tchase +jpos(tchase)\u0000RO(tchase)j+jRO(tchase)\u0000xstatej:\n13\n\nProof.This can be seen by considering the moves followed by FARFIRST aftertchase.\nFirst of all, we show that FARFIRST moves straight to RO(tchase), starting at tchase. If\npos(tchase)=RO(tchase), the claim is obvious. Thus, by the deﬁnition of RO(t), we can assume\nthatpos(tchase)<RO(tchase). It suﬃces to show that FARFIRST moves to the right until it\nreachesRO(tchase). We split this move into two possible parts.\nPart 1. This part only applies if pos(tchase)<RO(tchase)\u0000M. In this part, we show that\nFARFIRST moves straight to the point RO(tchase)\u0000M. Indeed, if RO(tchase)is released\nat some point during this part, then FARFIRST will surely move to RO(tchase)(let alone\nRO(tchase)\u0000M) in order to serve it. If RO(tchase)is not released during this part, then\nRU[t] =RO(tchase)throughout this part. But because RU[t] =RO(tchase)> M, Claim\n3.2 implies that RU[t] =RU0[t] =)RP[t]\u0015RU0[t]\u0000M=RO(tchase)\u0000M. Therefore,\nFARFIRST will move to RO(tchase)\u0000Mbecause of the predictions in this case.\nPart 2. This part refers to the move from the point x=maxfRO(tchase)\u0000M;pos (tchase)gto\nRO(tchase). In any case (whether Part 1 applies or not), when pos(tx)=x,RO(tchase)must\nhave been released. Indeed, assume for the sake of contradiction that rel(RO(tchase))> tx.\nNote then that RU[t] =RO(tchase)untilrel(RO(tchase)). We can see that\njpos(t)\u0000RO(tchase)j=jpos(t)\u0000RU[t]j\u0014M8t2[tx;rel(RO(tchase))]:\nIfRU[t]\u0014Mfor sucht, the claim can be seen by noting that RU[t]\u0000M\u00140and that\nRU[t] +M\u0015RU0[t] +Mand because FARFIRST won’t exit the interval [0;RU0[t] +M]due\ntoRP[t].\nOtherwise, by Claim 3.2, we have that RU[t] =RU0[t]for sucht. This means that FARFIRST\nwill not move to the left of RO(tchase)\u0000Mdue toRP[t]and also the rightmost point that may\nbe travelled to is RO(tchase)+M, again because of RP[t]. But that would mean that there\nexistst0:tchase<t0\u0014treleasewithD(t0)\u0014M, a contradiction. Therefore since RO(tchase)is\nreleased at tx,FARFIRST will move towards it immediately.\nIt now remains to show that FARFIRST will move to xstateimmediately after reaching\nRO(tchase). Ifxstate =RO(tchase), the claim is obvious. Therefore, we can assume that\nxstate=Mandxstate<RO(tchase). We again split this move into two parts.\nPart 1. This part lasts while t\u0014treleaseandxstate =Mhas not yet been reached. This\nmeans that throughout this part, we have\nD(t)>M =)RU[t]<pos (t)\u0000M=)RU0[t] +M <pos (t) =)RP[t]<pos (t):\nThus, since RP[t]andRU[t]are always to the left of pos(t),FARFIRST neither stops to wait\nfor a prediction nor backtracks to serve a request during this part.\nPart2. Thispart starts afterPart1 andlastsuntil xstate=Misreached. Again, FARFIRST\ntrivially does not stop to wait for a prediction, since all the positive ones are released by now.\nAdditionally, we can see that RU[t]\u0014Mfor this part, since all unreleased predictions are not\npositive. Thus, RU[t]\u0014pos(t)also holds for this part, prohibiting backtracking.\nWe can see that in both parts FARFIRST moves to the left with unit speed.\nTherefore, two unit speed moves are followed after tchase, one toRO(tchase)and one to\nxstate=minfM;RO(tchase)g. Also, after these moves, FARFIRST has reached a ﬁnal state,\n14\n\nbecause no outstanding request or unreleased prediction exists to the right of xstate\u0014M. End\nof proof.\nWe will now use claims 3.6 and 3.7 along with the deﬁnition of tchaseto prove Claim 3.5.\nProof of Claim 3.5. We distinguish two cases.\nCase 1.tchase =ts. In this case, Claim 3.7 implies that FARFIRST reaches a ﬁnal state by\ntimetstate=ts+jRphasej+jRphase\u0000xstatej, wherexstate=minfRphase;Mg. Then, by Claim\n3.6, we have\n(te\u0000tstate=jxstatej)_(te\u0000jOPTj\u0014M) =)(te\u0000ts= 2jRphasej)_(te\u0000jOPTj\u0014M):\nThus, Claim 3.5 holds in this case.\nCase 2.tchase>ts. In this case, we ﬁrst show that tstate\u0014jOPTj+maxfM\u0000RO(tchase);0g,\nwheretstateis as described in Claim 3.7. To achieve this, we note that D(tchase)=M.\nIndeed, let tprevbe the latest release time before tchase, ortsif none exist. If D(t0)> M\nfor allt0:tprev< t0\u0014tchase, then the deﬁnition of tchaseis violated. Thus, there exists a\nt0:tprev< t0\u0014tchasesuch thatD(t0)\u0014Mand we have D(tchase)\u0014Mby Claim 3.4. We\nnow distinguish two subcases.\nCase 2.1. M\u0014RO(tchase) =)xstate=M. In this case, we see that\nD(tchase)\u0014M=) jpos(tchase)\u0000RO(tchase)j+jRO(tchase)\u0000RU[tchase]j\u0014M=)\nRU[tchase]\u0015jpos(tchase)\u0000RO(tchase)j+jRO(tchase)\u0000Mj=)\ntchase +RU[tchase]\u0015tchase +jpos(tchase)\u0000RO(tchase)j+jRO(tchase)\u0000Mj=)\njOPTj\u0015tstate:\nCase 2.2. M >RO(tchase) =)xstate=RO(tchase). BecauseD(tchase)\u0014M, we must have\njpos(tchase)\u0000RO(tchase)j\u0014M\u0000RO(tchase) +RU[tchase] =)\ntstate\u0014jOPTj+M\u0000RO(tchase):\nIn both of these subcases, by Claim 3.6 we have\n(te\u0000tstate=jxstatej)_(te\u0000jOPTj\u0014M) =)(te=jxstatej+tstate)_(te\u0000jOPTj\u0014M) =)\n(te\u0014jOPTj+M)_(te\u0000jOPTj\u0014M):\nClaim 3.5 is now proved in all cases.\nWe can ﬁnally use Claim 3.5 to prove Claim 3.1 by showing that Equation (3)or Equation (2)\nholds.\n15\n\nProof of Claim 3.1. Letts(1);te(1)bethestartandendtimesoftheﬁrstphaseof FARFIRST\nandts(2);te(2)are similarly deﬁned for the second phase. We can see that ts(1)= 0and\nts(2) =te(1). We distinguish two cases based on the possible existence of a third phase.\nCase1. Norequestsarereleasedontherightsideafter te(1). Thus,weseethat jFARFIRSTj=\nte(2). By Claim 3.5, we see that te(1)\u0014jOPTj+M. Using Claim 3.5 for the second phase\nalso, we see that\n(te(2)\u0000ts(2) = 2jLj)_(te(2)\u0000jOPTj\u0014M) =)\n(te(2)\u0000te(1) = 2jLj)_(te(2)\u0000jOPTj\u0014M) =)(3)=)(2):\nCase 2. At least one request is released on the right side after te(1). LetqMbe the\nrightmost such request. We can see that qM\u0014M, since it is necessarily associated with a\nnon-positive prediction. We can see that jFARFIRSTj=te(2)+ 2jqMj. We also know that\njOPTj\u0015rel(qM) +jqMj\u0015te(1) +jqMj. By Claim 3.5 for the second phase, we have\n(te(2)\u0000ts(2) = 2jLj)_(te(2)\u0000jOPTj\u0014M) =)\n(te(2)\u0000te(1) = 2jLj)_(te(2)\u0000jOPTj\u0014M) =)\n(te(2) + 2jqMj= 2jLj+te(1) + 2jqMj)_(te(2) + 2jqMj\u0014jOPTj+M+ 2jqMj) =)\n(jFARFIRSTj\u0014jOPTj+ 2jLj+jqMj)_(jFARFIRSTj\u0014jOPTj+M+ 2jqMj) =)\n(jFARFIRSTj\u0014jOPTj+ 2jLj+M)_(jFARFIRSTj\u0014jOPTj+ 3M) =)\n(3)_(2)=)(2):\nWe can now use Claim 3.1 to prove Lemma 3.\nProof of Lemma 3. We know thatjOPTj\u00152(jLj+jRj), since it must at least travel to both L\nandRand back. Also, by Claim 3.1, we have jFARFIRSTj\u0000jOPTj\u0014jNearj+jFarj+3M=\njLj+jRj+ 3M. These inequalities imply\njFARFIRSTj\njOPTj= 1 +jFARFIRSTj\u0000jOPTj\njOPTj\u00141 +jLj+jRj+ 3M\n2(jLj+jRj)=3(1 +\u0011)\n2:\nWe can ﬁnally prove Theorem 1.\nProof of Theorem 1. By Lemma 2,FARFIRST is 3-robust. Additionally, by Lemma 3,\nFARFIRST is\u0010\n3(1+\u0011)\n2\u0011\n-smooth. Thus, the theorem holds.\n16\n\nA1:5-attack. Based on the magician analogy presented in Section 1.2, we design an attack\nstrategy that yields the following theorem.\nTheorem 2. For any\u000f>0, no algorithm can be (1:5\u0000\u000f)-competitive for closed online TSP\non the line under the LOCATIONS prediction model.\nWe ﬁrst describe in high level the main ideas in the proof of this theorem. In our attack\nstrategy, we have arbitrarily many requests evenly placed in the interval [\u00001;1]. The more of\nthese requests we have, the closer the competitive ratio we achieve will be to 1:5.\nFor a given set QXof request positions , we now describe how the release times of the requests\nat these positions are decided. The strategy is that we release requests on both sides as long\nasALGhas not yet approached a released request. This is the ﬁrst phase of releases and\nit is structured in such a way that OPTcould begin serving either of the two sides as fast\nas possible. In the magician analogy we described, this corresponds to the time before the\npedestrian chooses a hand.\nWhenALGapproaches a released request, we \"freeze\" the requests on ALG’s side. That is, if\nALGmoves close to a released request on the left side, say one placed at \u00001\n2, all requests in\nthe interval [\u00001\n2;0]have their release time delayed such that OPTcan still serve the entire\nright side and then come back to serve the left side by t= 4. This corresponds to the magician\nproducing the coin on the right hand while the pedestrian has chosen the left hand. However,\nALGis now faced with a dilemma. Should it wait for these \"frozen\" requests or should it\ntravel all the way to 1in order to serve the right side ﬁrst? We will see that both options are\nbad, in the sense that jALGjcan be seen to be arbitrarily close to 6. We now proceed with\nthe formal proof.\nWe describe a family FCof inputs that is structured as follows. For a given rank n\u00152, we\nplace exactly nrequests evenly spaced across the interval [\u00001;1]. For an instance fof the\nfamilyFC,\u000b(f)is deﬁned as the distance between any consecutive pair of requests in f.\nClaim 3.8. If an instance fof the family FChas rankn, then\u000b(f) =2\nn\u00001.\nProof.There arenrequests that delimit an interval of length 2. Thus, there are n\u00001equal\nsubintervals, whose lengths’ sum is equal to 2. Therefore, each subinterval has length2\nn\u00001.\nAll that is left to determine is the release times of the requests. We split the release times into\ntwo \"phases\". The ﬁrst phase takes place for as long as LU(t)<posALG(t)<RU(t). During\nthis phase,ADVreleases any request with distance dfrom the origin at time 2\u0000d. Note that\nthis release method allows OPTto eagerly start serving any side of the origin ﬁrst without\nwaiting for requests to release.\nNow for the second phase’s releases, assuming that ALGexits the interval to its left side (i.e.\ncommits to the left extreme), the requests to the right side are released as during the ﬁrst\nphase. However, for any unreleased request to the left side with distance dfrom the origin, its\nrelease time is delayed to 4\u0000d. IfALGexits from the right instead, ADVreleases the left\nrequests as in the ﬁrst phase and delays the right requests. The input (positions and release\ntimes of requests) is now fully speciﬁed. For the following, we will deﬁne tcommitas the start\ntime of the second phase. That is,\ntcommit =min(ft::(LU(t)<posALG(t)<RU(t))g):\n17\n\nWe immediately observe the following inequality, which guarantees that the second phase of\nthe requests starts in a timely manner.\nClaim 3.9. 1\u0014tcommit\u00142.\nProof.For the sake of contradiction, assume that tcommit<1. Then,jpos(tcommit )j\u00151since\n[LU(t);RU(t)] = [\u00001;1]fort<1. However, because any algorithm is limited to unit speed,\ntcommit<1 =) jpos(tcommit )j<1, a contradiction.\nOn the other hand, assume that tcommit>2. This means that LU(2)<pos (2)<RU(2). But,\nsince the ﬁrst phase has not stopped until t= 2, we haveLU(2)\u00150;RU(2)\u00140, which clearly\nleads to a contradiction.\nWe now state a lemma ensuring that OPTﬁnishes in the absolute least time possible for any\nsuch input. This allows us to maximize the competitive ratio we achieve against ALG.\nLemma 4. For any instance fin the family FC,jOPTj= 4.\nProof.We observe that the requests of one side (the one ALGdid not exit from) are released\nsuch thatOPTcan serve them all and return to the origin by t= 2. Additionally, the other\nside’s requests are released such that OPTnever has to stop for them either, i.e. it can serve\nthem all and return to the origin by t= 4. Thus,jOPTj= 4.\nHowever,ALGhas commited to one side (by exiting the interval) and we will prove that it\nrequires at least 6\u00002\u000b(f)time units to terminate. This will be our main lemma. We state it\nhere for reference but will prove it later.\nLemma 5. For any instance fin the family FCand for any ALG, we have thatjALGj\u0015\n6\u00002\u000b(f).\nBefore proving this lemma, we give some more claims. For the following, we assume without\nloss of generality that ALGexits the interval [LU(t);RU(t)]from the left, i.e. it commits to\nthe left side.\nClaim 3.10. For any instance fin the family FC,\nLU(tcommit )\u0000\u000b(f)\u0014posALG(tcommit )\u0014LU(tcommit ):\nProof.The claim can be seen by examining two cases. If ALGexited the unreleased requests\ninterval itself by moving out of it, then posALG(tcommit )=LU(tcommit ). In the other case,\nALGwas forced out of the interval by a request release. Thus, right before this release\n(which occurs at precisely tcommit),ALGwas inside the interval. The previous interval was\n[LU(tcommit )\u0000\u000b(f);RU(tcommit ) +\u000b(f)]. Thus, the inequality holds.\nWe now draw attention to one particular value, which constitutes the backbone of our attack.\nWe deﬁnedcommit =jLU(tcommit )j, wheretcommitis the start of the second phase of releases.\nWe now show some claims that allow us to use this value to get a lower bound for jALGj.\nClaim 3.11. For any instance fin the family FC,tcommit\u00152\u0000dcommit\u0000\u000b(f).\n18\n\nProof.IfLU(tcommit )=\u00001 =)dcommit = 1, then by Claim 3.9, we have tcommit\u00151\u0015\n2\u0000dcommit\u0000\u000b(f). Therefore, we can assume that a request Lprevexists with Lprev=\nLU(tcommit )\u0000\u000b(f)<LU(tcommit ). We see that if t<2\u0000dcommit\u0000\u000b(f), thenLU(t)\u0014Lprev,\nbecauseLprevis unreleased until 2\u0000dcommit\u0000\u000b(f). Therefore, tcommit\u00152\u0000dcommit\u0000\u000b(f),\nsince otherwise we would have LU(tcommit )\u0014Lprev, a contradiction.\nThe following claim states that ALGhas essentially made no progress until tcommit. Iftcommit\nis close to 2, we can easily see why this is bad for ALG. On the other hand, an early commit\nmeans that dcommitwill be large (due to Claim 3.11), posing problems again for ALG.\nClaim 3.12. ALGhas not served any request during the ﬁrst phase, i.e. up to time tcommit.\nProof.This is due to the fact that ALGhas not exited the interval of unreleased requests until\ntcommit. Therefore, it cannot have moved to a released request. Since ALGhas to move to a\nrequest to serve it, the claim holds.\nNow we are ready to prove Lemma 5.\nProof of Lemma 5. Letusexaminetheoptionsthat ALGhasinordertoterminateafter tcommit.\nBy Claim 3.12, we know that ALGhas not yet served the requests at \u00001;LU(tcommit );1. We\nexamine cases based on the order in which it chooses to do so from tcommiton.\nCase 1.ALGserves 1before\u00001. Then,ALGat the very least needs to travel from\nposALG(tcommit )to1, then to\u00001and then back to the origin. Using Claims 3.11 and 3.10,\nthis takes at least\njALGj\u0015tcommit +jposALG(tcommit )\u00001j+j1\u0000(\u00001)j+j\u00001\u00000j\u0015\n(2\u0000dcommit\u0000\u000b(f)) + (dcommit + 1) + 2 + 1 = 6\u0000\u000b(f):\nCase 2.ALGserves\u00001, then 1and thenLU(tcommit ). Again using Claims 3.11 and 3.10,\nthis takes\njALGj\u0015tcommit +jposALG(tcommit )\u0000(\u00001)j+j(\u00001)\u00001j+j1\u0000LU(tcommit )j+jLU(tcommit )\u00000j\u0015\n(2\u0000dcommit\u0000\u000b(f)) + (1\u0000dcommit\u0000\u000b(f)) + 2 + (1 + dcommit ) +dcommit = 6\u00002\u000b(f):\nCase 3.ALGservesLU(tcommit )before 1. In this case, ALGhas to ﬁrst wait for LU(tcommit )\nto be released and then go to serve 1. BecauseLU(tcommit )is a request to the left of the origin\nreleased during the second phase, we have\njALGj\u0015rel(LU(tcommit ))+jLU(tcommit )\u00001j+j1\u00000j\u0015(4\u0000dcommit )+(1 +dcommit )+1 = 6:\nThese cases are exhaustive and thus Lemma 5 is proved.\nWith all the above, we can ﬁnally prove Theorem 2.\nProof of Theorem 2. By Lemma 5 and Lemma 4, we have a competitive ratio of at least\n6\u00002\u000b(f)\n4for any algorithm ALG. By Claim 3.8, we can see that \u000b(f)can be arbitrarily small\nand thus this competitive ratio can be arbitrarily close to 1:5, proving our claim.\n19\n\n4 Open Variant\nIn this section, we consider the open variant. We have two prediction models for this vari-\nant. The ﬁrst one is the LOCATIONS prediction model and the second is the enhanced\nLOCATIONS +FINAL model (LFin short). For both settings, we give algorithms and\nlower bounds.\n4.1 The LOCATIONS prediction model\nUnder theLOCATIONS prediction model, we design the NEARFIRST algorithm, which\nachieves a competitive ratio of 1:66with perfect predictions and is also smooth and robust.\nWe complement this result with a lower bound of 1:44using a similar attack strategy to the\none used for the closed variant.\nTheNEARFIRST algorithm. As we mentioned in the introduction, NEARFIRST is\nsimilar toFARFIRST and actually slightly simpler. In essence, NEARFIRST simply picks\na direction in which it will serve the requests. Then, it just serves the requests either from\nleft to right or from right to left, using the predictions as guidance. The pseudocode for\nNEARFIRST is given below. Recall that move (x)\bmove (y)is used to indicate a move to\nxfollowed by a move to y.\nAlgorithm 2: NEARFIRST update function.\nInput : Current position pos, setOof unserved released requests, set Pof predictions.\nOutput: A series of (unit speed) moves to carry out until the next request is released.\nP0 the unreleased predictions in P;\nifP0is emptythen\nifpos<max (O)+min (O)\n2then return move (min(O))\bmove (max(O));\nelse return move (max(O))\bmove (min(O));\nend\nifjmin(P)j<jmax(P)jthen return move (min(P0[O))\bmove (min(P0));\nelse return move (max(P0[O))\bmove (max(P0));\nWe present the following theorem regarding the competitive ratio of NEARFIRST .\nTheorem 3. The algorithm NEARFIRST isminff(\u0011);3g-competitive, where\nf(\u0011) =(\n1 +2(1+\u0011)\n3\u00002\u0011;for\u0011<2\n3\n3; for\u0011\u00152\n3:\nWeﬁrstdescribethemainideasusedintheproofofthistheorem. Asinthecaseof FARFIRST ,\nthe 3-robustness holds because at time jOPTj,NEARFIRST has \"leftover work\" of at\nmost 2jOPTjtime units (to return to the origin and then copy OPT). For the consis-\ntency/smoothness, we draw our attention to the request qfserved last by OPT. For the\nfollowing, we assume that NEARFIRST serves the requests left to right. Let d=jqf\u0000Rj.\n20\n\nWe will show that the delay of NEARFIRST is bounded by M+d. Lettqfbe the time\nwhenNEARFIRST has served all requests to the left of qf, including qf. It turns out that\ntqf\u0014jOPTj+M, becauseNEARFIRST serves this subset of requests as fast as possible,\nexcept for a possible delay of Mdue to a misleading prediction. Then, in this worst case,\nNEARFIRST accumulates an extra delay of at most d, proving our claim.\nFinally, we bound OPTfrom below as a function of d. We see that OPTcan either serve the\nrequestsL;R;qfin the order L;R;qfor in the order R;L;qf. The worst case is the latter,\nwhere we see that jOPTj\u00152jRj+jLj+(jLj+jRj\u0000d)= 3jRj+ 2jLj\u0000d. Sinced\u0014jLj+jRj,\nwe obtain\njNEARFIRSTj\njOPTj= 1 +jNEARFIRSTj\u0000jOPTj\njOPTj\u00141 +M+jLj+jRj\n2jRj+jLj:\nBecauseNEARFIRST considersLthe near extreme due to the predictions, by Lemma 1 we\nﬁnd thatjRj\u00151\u00002\u0011\n2(jLj+jRj), which in turn proves our bound.\nWe now give the formal proof of Theorem 3. First of all, we present two lemmas that are very\nimportant. Their proofs are deferred to the Appendix, since they also refer to the PIVOT\nalgorithm, which is introduced later.\nLemma 6. LetALGbe eitherNEARFIRST orPIVOT. Then,ALGis 3-robust.\nLemma 7. LetALGbe eitherNEARFIRST orPIVOT. Also, letqfbe the request served\nlast byOPT. Assume without loss of generality that ALGserves requests from left to right.\nLetd=jqf\u0000Rj. Then, we havejALGj\u0000jOPTj\u0014M+d.\nThe robustness part of Theorem 3 is implied by Lemma 6.\nNow, to prove Theorem 3, it remains to show the consistency/smoothness part, which is given\nby the following lemma.\nLemma 8. The algorithm NEARFIRST isf(\u0011)-smooth for \u0011<2\n3, wheref(\u0011)= 1 +2(1+\u0011)\n3\u00002\u0011.\nProof.Assume w.l.o.g. that NEARFIRST serves the left extreme ﬁrst. By Lemma 7, we see\nthatjNEARFIRSTj\u0000jOPTj\u0014M+d, wheredis the distance of Rto the request qfserved\nlast byOPT. We distinguish two cases based on the order in which OPTserves the requests\ninfL;R;qfg.Case 1.OPTservesL;Rand thenqf. It can be seen then that\njNEARFIRSTj\njOPTj\u00141 +M+d\n2jLj+jRj+d:\nThe derivative of the right hand side with respect to dis\njRj+ 2jLj\u0000M\n(d+jRj+ 2jLj)2:\nBecause\u0011 <2\n3, it must hold that M <2\n3(jLj+jRj)=)jRj+ 2jLj\u0000M\n(d+jRj+ 2jLj)2>0. Thus, we\nmay maximize dto get an upper bound that is valid for any value of d. Becaused\u0014jLj+jRj\nwe see that\njNEARFIRSTj\njOPTj\u00141 +M+jLj+jRj\n3jLj+ 2jRj:\n21\n\nCase 2.OPTservesR;Land thenqf. It can be seen then that\njNEARFIRSTj\njOPTj\u00141 +M+d\n2jRj+jLj+ (jLj+jRj\u0000d)= 1 +M+d\n3jRj+ 2jLj\u0000d:\nWe can see that the right hand side is an increasing function of d. Thus, we may again\nmaximizedto get an upper bound.\njNEARFIRSTj\njOPTj\u00141 +M+jLj+jRj\n2jRj+jLj: (4)\nIn both cases, the bound of Equation (4)is valid. Noting that jLPj \u0014 jRPj(because\nNEARFIRST chose to go to the left ﬁrst), we now use Lemma 1 to ﬁnalize our proof.\njLPj\u0014jRPj=) jRj\u0015jLj\u00002M=)2jRj\u0015jLj+jRj\u00002M=\n(jLj+jRj)(1\u00002\u0011) =) jRj\u00151\u00002\u0011\n2(jLj+jRj) =)\n1 +M+jLj+jRj\n2jRj+jLj\u00141 +(1 +\u0011)(jLj+jRj)\u0010\n1 +1\u00002\u0011\n2\u0011\n(jLj+jRj)= 1 +2(1 +\u0011)\n3\u00002\u0011=)\njNEARFIRSTj\njOPTj\u00141 +2(1 +\u0011)\n3\u00002\u0011:\nWe now give the proof of Theorem 3.\nProof of Theorem 3. By Lemma 6,NEARFIRST is 3-robust. Additionally, by Lemma 8,\nNEARFIRST is\u0010\n1 +2(1+\u0011)\n3\u00002\u0011\u0011\n-smooth. Thus, Theorem 3 holds.\nA1:44-attack. For this setting, we use an attack strategy that is very similar to the one\nintroduced in Section 3. This allows us to obtain the following theorem.\nTheorem 4. For any\u000f>0, no algorithm can be\u0000\n1:44\u0000\u000f\u0001\n-competitive for open online TSP\non the line under the LOCATIONS prediction model.\nThe logic behind the attack we give here is exactly the same as the one used to prove Theorem\n2. There are two main diﬀerences demanded by the nature of the open variant.\nOne is that the ﬁrst phase is shorter in this attack. Instead of stopping when ALGexits\n[LU(t);RU(t)], the phase now stops when ALGexits the interval [3LU(t)+ 2;3RU(t)\u00002]. This\nis so that both options of ALG(switch to the other side or wait for the \"frozen\" requests) are\nequally hurtful.\nThe other diﬀerence lies in the release times of the second phase. Each request on the side\nchosen byALGnow has its release time delayed to 2 +d(instead of 4\u0000d), wheredis the\nrequest’s distance from the origin. This is so OPTcan ﬁnish by t= 3, which is the fastest\npossible even if all requests are released immediately.\n22\n\nIn the following, we assume without loss of generality that ALGexits the interval [3LU(t)+\n2;3RU(t)\u00002]from the left side. We deﬁne the family FOof inputs just like we deﬁned FCin\nthe proof of Theorem 2. Of course, the release times are diﬀerent as explained in the previous\nparagraphs.\nAn immediate observation that we have already mentioned is the following lemma.\nLemma 9. For any instance fin the family FO,jOPTj= 3.\nProof.SinceALGexits the interval from the left side, each request qron the right side is\nreleased at time 2\u0000jqrj. Thus, by moving to 1and back,OPTserves all the requests on\nthe right side. Additionally, each request qlon the left side is released no later than 2 +jqlj,\nallowingOPTto serve these requests by just moving to \u00001after reaching the origin at t= 2.\nTherefore,OPTcan serve all the requests by time 3, i.e.jOPTj= 3.\nThe next piece of the puzzle is a lower bound on ALG. This is given by the following lemma.\nLemma10. For any instance fin the family FOand any algorithm ALG,jALGj\u001513\n3\u00003\u000b(f),\nwhere\u000b(f)is the distance between consecutive requests in f.\nTo prove this lemma, we will use some claims, many of which are very similar to claims used\nfor Lemma 5. To present these claims, we introduce two important terms.\nWe denote with tcommitthe start time of the second phase, i.e.\ntcommit =min(ft::(3LU(t) + 2<posALG(t)<3RU(t)\u00002)g):\nAdditionally, we draw attention to the value dcommit =jLU(tcommit )j, which is very important\nfor the attack.\nWe now present the claims which are very similar to those used for the closed variant. Claims\n4.1, 4.2 and 4.3 can be seen in the same way as Claims 3.9, 3.10 and 3.11, respectively.\nClaim 4.1. 1\u0014tcommit\u00141 +1\n3.\nClaim 4.2. pos(tcommit )\u00143LU(tcommit ) + 2.\nClaim 4.3. For any instance fin the family FO,tcommit\u00152\u0000dcommit\u0000\u000b(f).\nMoreover, we also have the following claim.\nClaim 4.4. For any instance fof the family FO,dcommit\u00152\n3\u0000\u000b(f).\nProof.By Claim 4.3, we have dcommit\u00152\u0000tcommit\u0000\u000b(f). Additionally, Claim 4.1 implies\nthattcommit\u00144\n3. The claim follows.\nWe are now ready to prove Lemma 10.\n23\n\nProof of Lemma 10. We distinguish cases based on the order in which ALGchooses to serve\n\u00001;1;LU(tcommit )aftertcommit.Case 1.ALGserves 1before\u00001. By Claims 4.3, 4.2 and\n4.4, this takes at least\njALGj\u0015tcommit +jpos(tcommit )j+ 2 + 1\u0015\n2\u0000dcommit\u0000\u000b(f) +j3LU(tcommit ) + 2j+ 2 + 1 =\n2\u0000dcommit\u0000\u000b(f) + 3dcommit\u00002 + 2 + 1 = 3 + 2 dcommit\u0000\u000b(f)\u0015\n3 +4\n3\u00003\u000b(f)\u001513\n3\u00003\u000b(f):\nCase 2.ALGservesLU(tcommit )before 1. By the deﬁnition of the second phase’s release\ntimes and Claim 4.4, we have\njALGj\u0015rel(LU(tcommit )) +jLU(tcommit )j+ 1 = 2 +dcommit +dcommit + 1\n3 + 2dcommit\u001513\n3\u00002\u000b(f):\nCase 3.ALGserves in the order \u00001,1,LU(tcommit ). By Claim 4.4, we easily obtain\njALGj\u00152 + 2 +2\n3\u0000\u000b(f)\u001513\n3\u0000\u000b(f):\nThese cases are exhaustive and thus Lemma 10 follows.\nWe can now use Lemmas 9 and 10 to prove Theorem 4.\nProof of Theorem 4. By Lemma 9, we have jOPTj= 3. On the other hand, by Lemma 10, we\nsee thatjALGj\u001513\n3\u00003\u000b(f). Thus, we obtain a competitive ratio of at least13\n3\u00003\u000b(f)\n3. For\narbitrarily small \u000b(f), this value can be arbitrarily close to 1:44, proving the Theorem.\n4.2 The LOCATIONS+FINAL prediction model\nIn our ﬁnal setting we consider the open variant under the LFprediction model. We give the\nPIVOT algorithm, which is 1:33-competitive with perfect predictions and is also smooth and\nrobust. We also reuse the attack strategy described for the closed variant to achieve a lower\nbound of 1:25.\nThePIVOTalgorithm. Theﬁnalalgorithmwepresentworksinthesamewayas NEARFIRST ,\nexcept for the order in which it focuses on the two sides of the origin. Instead of heading to\nthe near extreme ﬁrst, PIVOT prioritizes the side whose extreme is further away from the\npredicted endpoint of OPT, which is provided by the LFprediction model. The pseudocode\nforPIVOT is given below. Note that Pf0refers to the element in Pwith labelf0.\nAs for the previous algorithms, we show a theorem that pertains to PIVOT’s competitive\nratio for diﬀerent values of the \u0011and\u000eerrors.\n24\n\nAlgorithm 3: PIVOT update function.\nInput : Current position pos, setOof unserved released requests, set Pof predictions,\nlabelf0ofOPT’s predicted endpoint.\nOutput: A series of (unit speed) moves to carry out until the next request is released.\nP0 the unreleased predictions in P;\nifP0is emptythen\nifpos<max (O)+min (O)\n2then return move (min(O))\bmove (max(O));\nelse return move (max(O))\bmove (min(O));\nend\nifPf0>max (P)+min (P)\n2then return move (min(P0[O))\bmove (min(P0));\nelse return move (max(P0[O))\bmove (max(P0));\nTheorem 5. The algorithm PIVOT isminff(\u0011;\u000e);3g-competitive, where\nf(\u0011;\u000e) =(\n1 +1+2(\u000e+3\u0011)\n3\u00002(\u000e+2\u0011);3\u00002(\u000e+ 2\u0011)>0\n3; 3\u00002(\u000e+ 2\u0011)\u00140:\nThe proof is very similar to the one used for NEARFIRST ’s competitive ratio. In fact, the\nrobustness is shown in exactly the same way. For the consistency/smoothness, the delay is\nbounded by M+din the same way, where dis the distance of the last request qfserved by\nOPTto the extreme served second by PIVOT. The same lower bounds for jOPTjhold as\nwell. We additionally bound das a function of the error-dependent values \u0001andM. When\nthere is no error, we can bound dto be at mostjLj+jRj\n2instead ofjLj+jRj, which gives a\nbetter competitive ratio than that of NEARFIRST . An important distinction is that we do\nnot make use of Lemma 1, since the algorithm does not consider the amplitudes of LandR.\nThe formal proof of Theorem 5 is given here. The robustness part of Theorem 5 is implied by\nLemma 6. To prove Theorem 5, it remains to show the consistency/smoothness part, which is\ngiven by the following lemma.\nLemma 11. The algorithm PIVOT isf(\u0011;\u000e)-smooth for 3\u00002(\u000e+ 2\u0011)>0, where\nf(\u0011;\u000e) = 1 +1 + 2(\u000e+ 3\u0011)\n3\u00002(\u000e+ 2\u0011):\nWe assume without loss of generality that PIVOT ﬁrst serves the left extreme. By Lemma 7,\nwe see thatjPIVOTj\u0000jOPTj\u0014M+d, wheredis the distance of Rto the request qfserved\nlast byOPT. We now show a bound on the value of dthat depends on \u0011and\u000e.\nClaim 4.5. d=jR\u0000qfj\u0014(jLj+jRj)\u00001\n2+\u000e+ 2\u0011\u0001\n.\nProof.We ﬁrst show two inequalities that will be used later to prove the claim. Note that\njR\u0000RPj\u0014MandjL\u0000LPj\u0014Mby Claim 2.1. The ﬁrst inequality is\njR\u0000RPj\u0014M=)R\u0000Rp\u0014M=)RP\u0000qf\u0015R\u0000qf\u0000M=)\n25\n\nRP\u0000qf\u0015jR\u0000qfj\u0000M: (5)\nSimilarly, we see that\njL\u0000LPj\u0014M=)LP\u0000L\u0015\u0000M=)qf\u0000L+M\u0015qf\u0000LP\njqf\u0000Lj+M\u0015qf\u0000LP: (6)\nBecause of PIVOT’s choice to go left ﬁrst, we see that\njRP\u0000pf0j\u0014jLP\u0000pf0j=)RP\u0000pf0\u0014pf0\u0000LP=)pf0\u0015RP+LP\n2=)\nqf0\u0015RP+LP\n2\u0000M=)qf\u0015RP+LP\n2\u0000M\u0000\u0001 =)\nqf\u0000LP+M+ \u0001\u0015RP\u0000qf\u0000M\u0000\u0001(5);(6)=)\njqf\u0000Lj+ 2M+ \u0001\u0015jR\u0000qfj\u00002M\u0000\u0001 =)\njqf\u0000Lj+jR\u0000qfj+ 2M+ \u0001\u00152jR\u0000qfj\u00002M\u0000\u0001 =)\nd=jR\u0000qfj\u0014(jLj+jRj)\u00121\n2+\u000e+ 2\u0011\u0013\n:\nUsing Claim 4.5, we can prove Lemma 11.\nProof of Lemma 11. We distinguish two cases based on the order in which OPTserves the\nrequests of the set fL;R;qfg.Case 1.OPTserves in the order L;R;qf. Thus, we know that\njPIVOTj\njOPTj\u00141 +M+d\n2jLj+jRj+d:\nThe derivative of the right part with respect to dis\njRj+ 2jLj\u0000M\n(d+jRj+ 2jLj)2:\nSince we have assumed 3\u00002(\u000e+ 2\u0011)>0 =)\u0011<1, this value is always positive. Therefore,\nwe can setdto the maximum value described in Claim 4.5 to obtain the following bound.\njPIVOTj\njOPTj\u00141 +(jLj+jRj)\u00001\n2+\u000e+ 2\u0011\u0001\n+\u0011(jLj+jRj)\n2jLj+jRj+ (jLj+jRj)\u00001\n2+\u000e+ 2\u0011\u0001\u0014\n1 +1 + 2\u000e+ 6\u0011\n3 + 2\u000e+ 4\u0011\u00141 +1 + 2(\u000e+ 3\u0011)\n3\u00002(\u000e+ 2\u0011):\nCase 2.OPTserves in the order R;L;qf. In that case, we have\njPIVOTj\njOPTj\u00141 +d+M\n2jRj+jLj+ (jLj+jRj\u0000d)4:5\n\u0014\n26\n\n1 +(jLj+jRj)\u00001\n2+\u000e+ 2\u0011\u0001\n+\u0011(jLj+jRj)\n2jRj+jLj+ (jLj+jRj)\u00001\n2\u0000\u000e\u00002\u0011\u0001\u00141 +1 + 2\u000e+ 6\u0011\n3\u00002\u000e\u00004\u0011=\n1 +1 + 2(\u000e+ 3\u0011)\n3\u00002(\u000e+ 2\u0011):\nIn both cases, we have shown the smoothness bound. Therefore, the proof is complete.\nWe now give the proof of Theorem 5.\nProof of Theorem 5. ByLemma6, PIVOT is3-robust. Also,byLemma11,itis\u0010\n1 +1+2(\u000e+3\u0011)\n3\u00002(\u000e+2\u0011)\u0011\n-\nsmooth. Thus, Theorem 5 follows.\nA1:25-attack. We make use of the original attack strategy of Section 3 yet again to obtain\na lower bound for this setting. Our ﬁnal theorem is presented here.\nTheorem 6. For any\u000f>0, no algorithm can be (1:25\u0000\u000f)-competitive for open online TSP\non the line under the LFprediction model.\nTo prove this theorem, we will again utilize the attack strategy given in the proof of Theorem\n2. The inputs generated are the same, except for a new request q0placed at the origin and\nreleased att= 4. LetFC0denote this new family of inputs. We observe the following lemmas.\nLemma 12. For any instance fin the family FC0,jOPTj= 4.\nProof.We see that the requests of the side which ALGdid not exit from are released such\nthatOPTcan serve them all and return to the origin by t= 2. Additionally, the other side’s\nrequests are released such that OPTnever has to stop for them either, i.e. it can serve them all\nand return to the origin by t= 4. The request on the origin is released at exactly t= 4, so this\nis also served right as OPTreturns to the origin from the second trip. Thus, jOPTj= 4.\nIn the following, \u000b(f)will refer to the distance of consecutive requests in f, disregarding q0.\nLemma 13. For any instance fin the family FC0,jALGj\u00155\u00002\u000b(f).\nProof.Suppose for the sake of contradiction that jALGj<5\u00002\u000b(f). We can see that\njposALG(jALGj)j\u00141. Thus, an algorithm ALG0could copy ALGuntil it serves all requests\nand then return to the origin. That would mean that ALG0solves the closed variant of fsuch\nthatjALG0j<6\u00002\u000b(f). Observe that there exists an instance f02FCthat is identical to f\nexcept forq0. We can see that ALG0also solves f0in less than 6\u00002\u000b(f)= 6\u00002\u000b(f0)time\nunits, since f0only contains a subset of the requests in f. Therefore, we have a contradiction\nto Lemma 5.\nWe can now prove Theorem 6.\nProof of Theorem 6. ByLemma12, weseethat jOPTj= 4. Wealsoseethat jALGj\u00155\u00002\u000b(f)\nby Lemma 13. Thus, we get a competitive ratio of at least5\u00002\u000b(f)\n4, which can be arbitrarily\nclose to 1:25, concluding the proof.\n27\n\n5 Experimental Evaluation\nWe have generated synthetic instances and corresponding predictions and tested our algorithms\non them. In this section, we explain how this data was generated and present the results we\nacquired.\nNote that mirroring of the positions of the requests and/or uniform scaling of the positions\nand release times does not aﬀect the competitive ratio of any algorithm. Therefore, we choose\nto generate the inputs as explained below.\nGenerating inputs. In the following, any reference of randomness will correspond to a\nuniformdistribution. Wehaveamaximumnumberofrequests nmax\u00152andamaximumrelease\ntimermax. Our generator ﬁrst randomly chooses an integer number of requests n2[2;nmax].\nThen it randomly chooses a value c02[1;c]. We then generate the positions of the requests\nas follows. We always have a request at \u00001and one at c0. The other n\u00002requests are\nrandomly placed in the interval [\u00001;c0]. The release time of each request is randomly chosen\nfrom [0;rmax].\nGenerating predictions. We now brieﬂy explain how the predictions of the LOCATIONS\nmodel are generated. Each input generated also comes with a prediction \"mould\". This mould\ncontainsnscalarsmi2[\u00001;1], one for each request. At least one of these scalars has an\nabsolute value of 1. For a given error \u0011, we calculate Mand then add an oﬀset of mi\u0001M\nto the position of request qito get the prediction pi. In this way, at least one prediction is\nguaranteed to have a distance of Mto its associated request.\nFor theLFprediction model, we simply try each label of the generated input as a diﬀerent\nprediction. Each label choice corresponds to a diﬀerent error \u000e, which is calculated after\nchoosing the label.\nResults. We generated 7500random input-predictions pairs with at most 20requests. A\nvalue ofc= 2was chosen, since higher values of cin general only beneﬁt our algorithms. The\nmaximum release time was set to 6. Again, higher release times in general lead to better\ncompetitive ratios for our algorithms, because they increase jOPTj.\nThe error\u0011of these predictions varied from 0to1. We ranFARFIRST andNEARFIRST\non each of these instances. Additionally, for each of these instances, we ran PIVOT with\neach of the instance’s request labels as the prediction of the LFprediction model. Thus, the\nPIVOT algorithm was ran approximately 75000times.\nWe did not compare the results of our algorithms to the classical algorithms because that\nwould be unfair. That is because our algorithms have the beneﬁt of knowing the number of\nrequestsnwhich helps in practice, even if the theoretical lower bounds are almost identical.\nIn contrast, the theoretically optimal classical online algorithms resort to waiting techniques,\nwhich in turn almost always maximizes their competitive ratio to the theoretical bound.\nThe experiments were executed on a typical modern laptop computer (CPU: AMD Ryzen\n7 4700U 2.0 Ghz 8 cores, RAM: 16GB). The execution time did not exceed 2minutes. We\npresent our results via various graphs in the following subsections.\n28\n\n5.1FARFIRST\nFigure 1 shows the maximum competitive ratio observed for the FARFIRST algorithm in all\ninstances with error \u0011up to the value of the xaxis. In ﬁgure 2, we have also provided a plot\nthat depicts the maximum competitive ratio observed for x% of the best instances with error\n\u0011up to the value of the yaxis. We note that the grid turns red near the very edge, which\nmeans that high competitive ratios are rare.\n0.0 0.2 0.4 0.6 0.8 1.0\nError eta1.51.61.71.81.92.02.1Max competitive ratio\nFigure 1: FARFIRST ’s competitive ratio for\nincreasing error. As can be seen in the ﬁgure,\nthe competitive ratio never surpasses \u00192:15\nfor\u0011\u00141. Additionally, we ﬁnd that the com-\npetitive ratio even with zero error is close to\nthe theoretical upper bound of 1:5. It should\nalso be noted that the theoretical lower bound\nof1:64(without predictions) is broken for \u0011\nroughly up to 0:2.\n20 40 60 80 100\nPercentage of inputs considered0.00.20.40.60.81.0Error eta\n1.0001.1591.3181.4771.6361.7951.9542.113Figure 2: FARFIRST ’s competitive ratio for\nincreasing error and percentage of inputs con-\nsidered, sorted by the competitive ratio that\nFARFIRST obtains on them. The narrow-\nness of the red portion of the grid suggests\nthat high competitive ratios are rare. We\nnote that the colors of the grid are generally\nblue, i.e.FARFIRST exhibits a relatively\nlow competitive ratio in most cases.\n5.2NEARFIRST\nThe ﬁgures presented here are analogous to those of Section 5.1. Figure 3 shows the maximum\ncompetitive ratio observed for the NEARFIRST algorithm in all instances with error \u0011up\nto the value of the xaxis. In ﬁgure 4, a plot analogous to that seen in ﬁgure 2 is shown for the\nNEARFIRST algorithm. The red portion of the grid is again quite limited as in the case for\nFARFIRST .\n5.3PIVOT\nIn this ﬁnal subsection we condsider the PIVOT algorithm. In ﬁgure 5, the color of the pixel\nin coordinates (x;y)corresponds to the maximum competitive ratio observed for all instances\nwith errors \u000e\u0014xand\u0011\u0014y. We should explain here that the colors change abruptly in this\nﬁgure since the \u000eerror does not vary smoothly in the generated predictions. This is because we\n29\n\n0.0 0.2 0.4 0.6 0.8 1.0\nError eta1.61.71.81.92.0Max competitive ratioFigure 3: NEARFIRST ’s competitive ratio\nfor increasing error. As can be seen in the\nﬁgure, the competitive ratio never surpasses\n\u00192:05for\u0011\u00141. Additionally, we ﬁnd that\nthe competitive ratio even with zero error is\nclosetothetheoreticalupperboundof 1:66. It\nshould also be noted that the theoretical lower\nbound of 2(without predictions) is broken\neven for\u0011very close to 1.\n20 40 60 80 100\nPercentage of inputs considered0.00.20.40.60.81.0Error eta\n1.0001.1501.2991.4491.5981.7481.8972.047Figure 4: NEARFIRST ’s competitive ratio\nfor increasing error and percentage of inputs\nconsidered, sorted by the competitive ratio\nthat NEARFIRST obtains on them. The nar-\nrowness of the red portion of the grid suggests\nthat high competitive ratios are rare. We note\nthat the colors of the grid are generally blue,\ni.e.NEARFIRST exhibits a relatively low\ncompetitive ratio in most cases.\nonly have a discrete set of choices for the label f0of theLFprediction model through which \u000e\nis calculated.\n6 Conclusion\nWe have examined the online TSP on the line and provided lower bounds as well as algorithms\nfor three diﬀerent learning-augmented settings. An immediate extension of our results would be\nto bridge the gap between the lower and upper bounds we have shown for the open variant. Also,\nit would be interesting to establish error-dependent lower bounds and/or optimal consistency-\nrobustness tradeoﬀs. Moreover, an improvement would be to remove the assumption of knowing\nthe number of requests n. A technique that could perhaps allow an algorithm to achieve that\nis to periodically make sure that the algorithm terminates in case no new requests appear.\nFinally, more general versions of online TSP could be investigated like the case of trees.\n30\n\nFigure 5: PIVOT’s competitive ratio for increasing errors \u000eand\u0011. The color of the pixel in\ncoordinates (x;y)corresponds to the maximum competitive ratio observed for all instances\nwith errors \u000e\u0014xand\u0011\u0014y. We observe that the competitive ratio is more sensitive to \u0011than\nto\u000e, as was to be expected by the corresponding theoretical bound. With perfect predictions,\nthe maximum competitive ratio is not greater than \u00191:11, which is considerably lower than\nthe theoretical upper bound of 1:33. In general, the competitive ratio increases smoothly along\nthe main diagonal of the grid. Finally, PIVOT’s competitive ratio surpasses the lower bound\nof2(without predictions) only for large values of \u000e,\u0011.\n31\n\nReferences\n[1]Spyros Angelopoulos, Christoph Dürr, Shendan Jin, Shahin Kamali, and Marc P. Renault.\nOnline computation with untrusted advice. In ITCS, 2020.\n[2]Antonios Antoniadis, Christian Coester, Marek Eliás, Adam Polak, and Bertrand Simon.\nOnline metric algorithms with untrusted predictions. In ICML, 2020.\n[3]Antonios Antoniadis, Peyman Jabbarzade Ganje, and Golnoosh Shahkarami. A novel\nprediction setup for online speed-scaling. CoRR, 2021.\n[4]Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and\nonline matching problems with machine learned advice. In NeurIPS , 2020.\n[5]N. Ascheuer, M. Grötschel, S. O. Krumke, and J. Rambau. Combinatorial online opti-\nmization. In Operations Research Proceedings , 1999.\n[6]Giorgio Ausiello, Esteban Feuerstein, S. Leonardi, L. Stougie, and Maurizio Talamo.\nAlgorithms for the on-line travelling salesman. Algorithmica , 2001.\n[7]Étienne Bamas, Andreas Maggiori, Lars Rohwedder, and Ola Svensson. Learning aug-\nmented energy minimization via speed scaling. In NeurIPS , 2020.\n[8]Étienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning\naugmented algorithms. In NeurIPS , 2020.\n[9]Giulia Bernardini, Alexander Lindermayr, Alberto Marchetti-Spaccamela, Nicole Megow,\nLeen Stougie, and Michelle Sweering. A universal error measure for input predictions\napplied to online graph problems. CoRR, 2022.\n[10]Antje Bjelde, Jan Hackfeld, Yann Disser, Christoph Hansknecht, Maarten Lipmann, Julie\nMeißner, Miriam SchlÖter, Kevin Schewior, and Leen Stougie. Tight bounds for online\ntsp on the line. ACM Trans. Algorithms , 2021.\n[11]Michiel Blom, Sven O. Krumke, Willem E. De Paepe, and Leen Stougie. The online tsp\nagainst fair adversaries. INFORMS journal on computing , 2001.\n[12]Paul Dütting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. Secretaries\nwith advice. In EC, 2021.\n[13]Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with\nexpert advice. In ICML, 2019.\n[14]Patrick Jaillet and Michael R. Wagner. Online routing problems: Value of advanced\ninformation as improved competitive ratios. Transportation Science , 2006.\n[15]Tim Kraska, Alex Beutel, Ed H. Chi, Jeﬀrey Dean, and Neoklis Polyzotis. The case for\nlearned index structures. In SIGMOD , 2018.\n[16]E.L. Lawler. The Travelling Salesman Problem: A Guided Tour of Combinatorial Opti-\nmization. Wiley-Interscience series in discrete mathematics and optimization. John Wiley\n& Sons, 1985.\n32\n\n[17]Thodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned\nadvice. In ICML, 2018.\n[18]Michael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sandwiching.\nInNeurIPS , 2018.\n[19]Michael Mitzenmacher. Scheduling with predictions and the price of misprediction. In\nITCS, 2020.\n[20]Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. In Beyond\nthe Worst-Case Analysis of Algorithms . Cambridge University Press, 2020.\n[21]Benjamin Moseley, Sergei Vassilvitskii, Silvio Lattanzi, and Thomas Lavastida. Online\nscheduling via learned weights. In SODA, 2020.\n[22]Harilaos N. Psaraftis, Marius M. Solomon, Thomas L. Magnanti, and Tai-Up Kim. Routing\nand scheduling on a shoreline with release times. Management Science , 1990.\n[23]Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml\npredictions. In NeurIPS , 2018.\n[24]Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In\nSODA, 2020.\n[25]Shufan Wang and Jian Li. Online algorithms for multi-shop ski rental with machine\nlearned predictions. In AAMAS, 2020.\n[26]Alexander Wei. Better and simpler learning-augmented online caching. In AP-\nPROX/RANDOM , 2020.\n33\n\nA Lemmas for the case of known n\nLemma 14. For any\u000f>0, no algorithm can be (2\u0000\u000f)-competitive for open online TSP on\nthe line without predictions when the number of requests nis known. Also, there exists an\nalgorithm that matches this lower bound.\nProof.A very simple attack can be used to show the lower bound of 2. IfposALG(1)\u00140, we\npresent a request at 1with a release time of 1. In the other case, the request’s position is \u00001.\nIt is easy to see that jOPTj= 1, whilejALGj\u00152, proving the bound. There also exists a\nvery simple algorithm that matches this bound. Such an algorithm need only wait until all\nrequests have been released and then copy OPT’s actions, which are at this point computable.\nThe waiting part does not take more than jOPTjand neither does the moving part, which\nimplies a competitive ratio of 2for such an algorithm.\nLemma 15. For any\u000f>0, no algorithm can be (1:64\u0000\u000f)-competitive for closed online TSP\non the line without predictions when the number of requests nis known. Also, there exists an\nalgorithm that matches this lower bound.\nProof.By taking a close look at the attack strategy described in Section 3.3 of [6], we observe\nthat the number of requests is never higher than a speciﬁc value nmax. In fact, it turns out\nthatnmax= 3, i.e. the attack never uses more than 3 requests. We modify this attack so that\nit can also be used when nis known. Any instance of the modiﬁed attack will have exactly\nnmaxrequests. Thus, the algorithm will be informed that there will indeed be nmaxrequests.\nWe ﬁrst give a brief description of the original attack strategy for context. Let \u001a=9+p\n17\n8\u00191:64,\nI= [\u0000(2\u001a\u00003);(2\u001a\u00003)],I0= [\u0000(7\u00004\u001a);(7\u00004\u001a)]. Note that Iis contained in I0and both\nof them are contained in [\u00001;1]. IfposALG(1)=2I, then a single request at \u00001or1(released\natt= 1) suﬃces to achieve the competitive ratio. Assuming that posALG(1)2I, we\nsimultaneously present two requests at \u00001and1att= 1. Att= 3,ALGcannot have possibly\nserved both of these requests. If posALG(3)2I0, then another request at \u00001or1(released\natt= 3) is suﬃcient. Therefore, we continue assuming that posALG(3)=2I0. This means\nthatALGis close to one extreme and still has not served the other. When ALGcrosses the\norigin to serve the other extreme at time 3 +x, a request is placed at either 1 +xor\u0000(1 +x)\n(depending on which extreme ALGhas not served). The competitive ratio turns out to be at\nleast\u001ain this (ﬁnal) case also.\nWe now describe our modiﬁcation of this strategy. Initially, the original attack strategy is\nfollowed. Let qwinbe the last request released by the original attack strategy, after the release\nof which the competitive ratio is guaranteed to be at least 1:64in case no new requests\nappear. Let noriginalbe the number of requests released via the original attack strategy. If\nnoriginal< nmax, thennmax\u0000noriginalextra requests are released at time rel(qwin), placed\narbitrarily between the origin and qwin. These extra requests are served by OPTon the way\nback fromqwin, without incurring extra cost. In other words, jOPTjdoes not increase with\nthe addition of these requests. Also, jALGjcertainly cannot decrease since we only added\nrequests. Therefore, the same lower bound holds even for known n.\nThe algorithm is exactly the same as the one for unknown number of requests, since it can just\nignore the number nand still achieve the same competitive ratio.\n34\n\nB Omitted proofs from section 4\nIn this subsection, we give the formal proofs of two lemmas which we used to prove Theorems\n3 and 5.\nLemma 6. LetALGbe eitherNEARFIRST orPIVOT. Then,ALGis 3-robust.\nProof.Lettfdenote the latest release time for a ﬁxed instance of the problem. We assume\nw.l.o.g. that posALG(tf)\u0014L(tf)+R(tf)\n2. Note that after tf,ALGwill move to L(tf)and then\ntoR(tf). Thus, we observe that\njALGj=tf+jpos(tf)\u0000L(tf)j+jL(tf)\u0000R(tf)j: (7)\nWe distinguish two cases based on the position of ALGat timetf.Case 1.pos(tf)\u0015L(tf).\nIn this case, we see that\n(7)=) jALGj=tf+pos(tf)\u0000L(tf) +R(tf)\u0000L(tf)\u0014\ntf+L(tf) +R(tf)\n2\u0000L(tf) +R(tf)\u0000L(tf) =\ntf+3(jL(tf)j+jR(tf)j)\n2\u00142:5jOPTj\u00143jOPTj:\nCase 2.pos(tf)<L(tf). Similarly, we have\n(7)=) jALGj=tf+L(tf)\u0000pos(tf) +R(tf)\u0000L(tf)\u0014\n2tf+jR(tf)j\u00143jOPTj:\nLemma 7. LetALGbe eitherNEARFIRST orPIVOT. Also, letqfbe the request served\nlast byOPT. Assume without loss of generality that ALGserves requests from left to right.\nLetd=jqf\u0000Rj. Then, we havejALGj\u0000jOPTj\u0014M+d.\nTo prove this lemma, we ﬁrst give some deﬁnitions. Note ﬁrst that LU[t]is sort of a \"checkpoint\"\nforOPT, meaning that OPTmust be located at LU[t]for some point in time on or after t\nin order to serve that request. Then, it must move from LU[t]toqf. This idea helps us keep\ntrack ofjOPTjso we can compare it with jALGj.\nD(t)denotes the least amount of time necessary to serve all requests to the left of LU[t]\n(assuming they have been released) and then move to LU[t], starting at position posALG(t).\nThis amounts to\nD(t) =jposALG(t)\u0000LO(t)j+jLO(t)\u0000LU[t]j:\nThis function exhibits a useful bound property. If it drops to Mor below at some time t, it\ncan only increase above Magain due to a request release. This property is described more\nformally in the following claim. But ﬁrst, another useful deﬁnition is given.\nWe deﬁneLP[t]as the leftmost prediction that is released on or after t. That is,LP[t] =\nmin(fp2P:rel(\u0019(q))\u0015tg). If this set is empty, then LP[t] =R.\nUsing this deﬁnition, the following claim can be seen in the same way as Claim 3.4.\n35\n\nClaim B.1. Lettdropbe a time point such that D(tdrop)\u0014M. Iftnextis the earliest release\ntime of a request after tdrop, then\nD\u0000\nt0\u0001\n\u0014M;8t02[tdrop;tnext]:\nWe now draw our attention to a point in time that is very central to our proof.\nLettreleasebe the latest release time of a request. Note that LU[t] =R;8t>trelease. Then,\nwe deﬁne\ntchase =minft:ts\u0014t\u0014trelease;\u0000\nD\u0000\nt0\u0001\n>M;8t<t0\u0014trelease\u0001\ng:\nIn essence, similarly to the deﬁnition in the previous section, tchasedenotes the time after\nwhichALGgets to ﬁnish as soon as possible without waiting for predictions or backtracking for\nrequests. In the following, we assume for simplicity and without loss of generality that ALG\nalwaysserves the requests left to right, even if at time treleaseit is clear that going to the right\nﬁrst is faster. It is true that our algorithm may indeed make such a decision at time trelease,\nbut that is a trivial optimization that does not invalidate our proof, since it can only decrease\njALGjand by extension, the value jALGj\u0000jOPTj. Under this assumption, we proceed by\nshowing that after tchase,ALGmoves toLO(tchase)and then straight to LU[tchase], serving all\nintermediate requests on the way. In fact, it also keeps moving to the right until it reaches R\nand ﬁnishes. The following claim can be seen in the same way as Claim 3.7.\nClaim B.2. Lett0=tchase +D(tchase). Then,pos(t0) =LU[tchase].\nAlso,jALGj=t0+jpos(t0)\u0000Rj.\nWe now give the proof of Lemma 7.\nProof of Lemma 7. We distinguish two cases.\nCase 1.tchase =ts. This easily implies that jALGj= 2jLj+jRjby Claim B.2. It remains to\nshow thatjOPTj\u00152jLj+jRj\u0000M\u0000d.\nIfOPTfollows the order L\u0000 !R\u0000 !qf, then\njOPTj\u00152jLj+jRj+d\u00152jLj+jRj\u0000M\u0000d:\nOn the other hand, if OPTfollows the order R\u0000 !L\u0000 !qf, then\njOPTj\u00152jRj+jLj+ (jLj+jRj\u0000d)\u00153jRj+ 2jLj\u0000d\u00152jLj+jRj\u0000M\u0000d:\nCase 2.tchase>ts. It can be seen then by Claim B.1 that D(tchase)\u0014M. It is easy to see\nthatjOPTj\u0015tchase +jLU[tchase]\u0000qfj. At the same time, by Claim B.2 we see that\njALGj=tchase +D(tchase) +jLU[tchase]\u0000Rj\u0014\ntchase +M+jLU[tchase]\u0000qfj+jqf\u0000Rj\u0014jOPTj+M+d:\n36",
  "textLength": 86436
}