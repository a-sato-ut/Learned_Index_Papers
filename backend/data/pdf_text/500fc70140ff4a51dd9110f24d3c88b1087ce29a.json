{
  "paperId": "500fc70140ff4a51dd9110f24d3c88b1087ce29a",
  "title": "Randomized Strategic Facility Location with Predictions",
  "pdfPath": "500fc70140ff4a51dd9110f24d3c88b1087ce29a.pdf",
  "text": "Randomized Strategic Facility Location with Predictions∗\nEric Balkanski†1, Vasilis Gkatzelis‡2, and Golnoosh Shahkarami§3\n1Columbia University, IEOR\n2Drexel University, Computer Science\n3Max Planck Institut für Informatik, Universität des Saarlandes\nAbstract\nIn the strategic facility location problem, a set of agents report their locations in a\nmetric space and the goal is to use these reports to open a new facility, minimizing an\naggregate distance measure from the agents to the facility. However, agents are strategic\nand may misreport their locations to influence the facility’s placement in their favor. The\naim is to design truthful mechanisms, ensuring agents cannot gain by misreporting. This\nproblem was recently revisited through the learning-augmented framework, aiming to\nmove beyond worst-case analysis and design truthful mechanisms that are augmented with\n(machine-learned) predictions. The focus of this prior work was on mechanisms that are\ndeterministic and augmented with a prediction regarding the optimal facility location. In\nthis paper, we provide a deeper understanding of this problem by exploring the power of\nrandomization as well as the impact of different types of predictions on the performance of\ntruthful learning-augmented mechanisms. We study both the single-dimensional and the\nEuclidean case and provide upper and lower bounds regarding the achievable approximation\nof the optimal egalitarian social cost.\n∗Eric Balkanski was supported by NSF grants CCF-2210501 and IIS-2147361. Vasilis Gkatzelis was\nsupported by NSF grant CCF-2210501 and NSF CAREER award CCF-2047907.\n†eb3224@columbia.edu\n‡gkatz@drexel.edu\n§gshahkar@mpi-inf.mpg.de\n1arXiv:2409.07142v3  [cs.GT]  4 Nov 2024\n\n1 Introduction\nIn the classic facility location problem the goal is to determine the ideal location for a new\nfacility, taking as input the preferences of a group of nagents. Due to the wide variety\nof applications that this problem captures, it has received a lot of attention from different\nperspectives. A notable example is the strategic version of this problem which is motivated by\nthe fact that the participating agents may be able to strategically misreport their preferences,\nleading to a facility location choice that they prefer over the one that would be chosen if\nthey were truthful. In the strategic facility location problem, the goal is to design truthful\nmechanisms, i.e., mechanisms that elicit the agents’ true preferences by carefully removing any\nincentive for the agents to lie [Moulin, 1980, Procaccia and Tennenholtz, 2013]. This problem\nhas played a central role in the broader literature on mechanism design without money and a\nlot of prior work has focused on optimizing the quality of the returned location subject to the\ntruthfulness constraint (see Section 1.2 for a brief overview). However, this work has mostly\nfocused on worst-case analysis, often leading to unnecessarily pessimistic impossibility results.\nAiming to overcome the limitations of worst-case analysis, a surge of work during the last\nfew years has focused on the design of algorithms in the learning-augmented framework [Mitzen-\nmacher and Vassilvitskii, 2022]. According to this framework, the designer is provided with\nsome machine-learned prediction regarding the instance at hand and the goal is to leverage\nthis additional information in the design process. However, crucially, this information is not\nguaranteed to be accurate and can, in fact, be arbitrarily inaccurate. The goal is to achieve\nstronger performance guarantees whenever the prediction happens to be accurate (known as\ntheconsistency guarantee), while at the same time maintaining some worst-case guarantee\neven if the prediction is arbitrarily inaccurate (known as the robustness guarantee).\nAgrawal et al. [2022] and Xu and Lu [2022] recently introduced the learning-augmented\nframework to mechanism design problems involving self-interested strategic agents with private\ninformation, and both of these works studied the strategic facility location problem. Agrawal\net al. [2022] considered both the egalitarian and the utilitarian social cost objectives and\nprovided tight bounds on the best-feasible robustness and consistency trade-off for truthful\nmechanisms augmented with a prediction regarding what the optimal facility location would be.\nHowever, the achievable trade-offs between robustness and consistency may heavily depend on\nthe specific type of prediction that the mechanism is augmented with: note that the consistency\nguarantee only binds if the prediction is accurate, so more refined predictions bind on fewer\ninstances (the subset of instances where even the refined prediction is accurate) and could,\ntherefore, enable the design of learning-augmented mechanisms with improved guarantees. This\nleaves open the question of whether mechanisms equipped with more refined predictions can,\nindeed, achieve better trade-offs between robustness and consistency. Also, Agrawal et al. [2022]\nrestricted their attention to deterministic mechanisms, leaving open the possibility for truthful\nrandomized mechanisms to achieve even stronger guarantees. In this work we significantly\nexpand our understanding of this problem by studying both the power of randomization and\nthe power of alternative predictions.\n1.1 Our Results\nWe revisit the problem of designing truthful learning-augmented mechanisms for the strategic\nfacility location problem. These mechanisms ask each agent to report their preferred location\nin some metric space (which is private information) and they are also augmented with some\n2\n\n(unreliable) prediction related to this private information. Using the agents’ reported locations,\nalong with the prediction, the mechanism chooses a facility location in this metric space,\naiming to (approximately) minimize some social cost measure. Our focus in this paper is on\nthe egalitarian social cost, i.e., the maximum distance between an agent’s preferred location\nand the location chosen for the facility. Our goal is to evaluate the performance of mechanisms\nthat can leverage randomization, as well as the power of different types of predictions.\nWe first consider the well-studied single-dimensional version of the problem, where all\nagents lie on a line and the mechanism needs to choose a facility location on that line. Prior\nwork on the strategic facility location problem without predictions, showed that for this class of\ninstances no deterministic truthful mechanism can achieve an approximation better than 2and\nno randomized truthful1mechanism can achieve an approximation better than 1.5; both of these\nresults are shown to be tight [Procaccia and Tennenholtz, 2013]. In the learning-augmented\nframework, Agrawal et al. [2022] showed there exists a truthful deterministic mechanism\nprovided with a prediction regarding the optimal facility location that achieves the best of\nboth worlds: a perfect consistency of 1and an optimal robustness of 2.\nOur main result for the single-dimensional version shows that even if the mechanism is\nprovided with the strongest possible prediction (i.e., a prediction regarding every agent’s\npreferred location), any randomized truthful mechanism that is (1 +δ)-consistent for some\nδ∈[0,0.5]can be no better than (2−δ)-robust. This implies that the previously proposed\ndeterministic learning-augmented mechanism that is 1-consistent and 2-robust and the ran-\ndomized non-learning-augmented mechanism that is 1.5-robust (and hence also 1.5-consistent)\nare both Pareto optimal among all randomized mechanisms, even if they are augmented with\nthe strongest possible predictions. Beyond these two extreme points, this result proves a lower\nbound for the achievable trade-off between robustness and consistency for any randomized mech-\nanism, even with the strongest predictions. We complement this bound by observing that this\ntrade-off can, in fact, be achieved by a truthful randomized mechanism that chooses between\nthe optimal learning-augmented deterministic mechanism and the optimal non-augmented\nrandomized mechanism, thus verifying that our bound fully characterizes this Pareto frontier.\nWe then consider the two-dimensional case, for which much less is known about randomized\nmechanisms. The best-known truthful randomized mechanism is the Centroid, introduced\nby Tang et al. [2020], which guarantees a 2−1/napproximation. We provide a truthful\nrandomized mechanism that is equipped with a prediction regarding the identities of the most\nextreme agents (those who would incur the maximum cost in the optimal solution) and, using\nthis prediction, our mechanism achieves 1.67consistency and 2robustness. The idea is to use\nthese predictions to select at most three extreme agents, apply the Centroid mechanism to\nthem, and guarantee a 1.67approximation if the predictions are accurate not only for these\nthree agents but for all other agents as well, utilizing the properties of the Euler line.\nAn interesting observation is that the lower bound of 1.5from the single-dimensional case\ndoes not extend to two dimensions, so prior work does not imply any lower bound for two\ndimensions! We prove a lower bound of 1.118for all truthful randomized mechanisms and then\nthat no deterministic mechanism can simultaneously guarantee better than 2consistency and\nbetter than 1 +√\n2robustness, even if it is augmented with the strongest predictions. Similarly,\nno truthful randomized mechanism can simultaneously guarantee 1consistency and better than\n2robustness. The former result proves the optimality of a previously introduced 1-consistent\nand1 +√\n2-robust truthful deterministic mechanism provided only with a prediction regarding\n1A truthful randomized mechanism guarantees truthfulness in expectation. See Section 2 for more details.\n3\n\nthe optimal facility location [Agrawal et al., 2022]. We also show that the latter is tight.\n1.2 Related Work\nStrategic facility location. Moulin [1980] provided a characterization of deterministic\ntruthful facility location mechanisms on the line and introduced the median mechanism, which\nreturns the median of the agent location profile x=⟨x1,···, xn⟩. The median mechanism\nis known to be truthful, providing an optimal solution for the Utilitarian Social Cost and\nachieving a 2-approximation for the Egalitarian Social Cost, as demonstrated by Procaccia\nand Tennenholtz [2013]. Notably, Procaccia and Tennenholtz [2013] showed that this 2-\napproximation factor represents the best performance achievable by any deterministic truthful\nmechanism. Building on this, Border and Jordan [1983] extended these results to the Euclidean\nspace by employing median schemes independently in each dimension. Subsequently, Barberà\net al. [1993] further generalized this outcome to any L1-norms. Other settings explored include\ngeneral metric spaces [Alon et al., 2010] and d-dimensional Euclidean spaces [El-Mhamdi et al.,\n2023, Goel and Hann-Caruthers, 2020, Meir, 2019, Walsh, 2020], as well as circles [Alon et al.,\n2010, Meir, 2019] and trees [Alon et al., 2010, Feldman and Wilf, 2013]. Fundamental results\nin truthful facility location often focus on characterizing the space of truthful mechanisms. For\nthe one-dimensional case, Moulin’s characterization [Moulin, 1980] reveals that all deterministic\ntruthful mechanisms belong to the “general median mechanisms (GCM)” family. For the\ntwo-dimensional case, a similar characterization was provided by Peters et al. [1993]. For a\ncomprehensive review of previous work on this problem, see the survey by Chan et al. [2021].\nLearning-augmented algorithms. Worst-case analysis on its own is often not informative\nenough, and several alternative measures have been proposed to overcome its limitations [Rough-\ngarden, 2021]. Learning-augmented algorithms, or algorithms with predictions [Mitzenmacher\nand Vassilvitskii, 2022], aim to address these limitations by incorporating predictions into\nthe algorithm’s design. Lykouris and Vassilvitskii [2021] studied the online caching problem\nand introduced two main metrics, consistency and robustness, to evaluate the performance of\nthese algorithms. These initial results were improved by subsequent works [Antoniadis et al.,\n2020a, Chledowski et al., 2021, Rohatgi, 2020, Wei, 2020]. Various online problems have been\nexplored using learning-augmented algorithms, including scheduling problems [Antoniadis et al.,\n2022, Bamas et al., 2020a, Lattanzi et al., 2020, Mitzenmacher, 2020, Purohit et al., 2018],\nonline selection and matching problems [Antoniadis et al., 2020b, Dütting et al., 2021], the\nonline knapsack problem [Im et al., 2021], online Nash social welfare maximization [Banerjee\net al., 2022], and several online graph algorithms [Azar et al., 2022]. The online version of\nthe facility location problem, introduced by Meyerson [2001], involves points arriving online,\nrequiring us to assign them irrevocably to either an existing facility or open a new facility.\nThe objective is to minimize the distance of each agent to the assigned facility along with\nthe cost of opening the facilities. Almanza et al. [2021], Jiang et al. [2022], and Fotakis et al.\n[2021] studied this problem augmented with predictions regarding the location of the optimal\nfacility for each incoming point. The literature also covers learning-augmented algorithms for\nclassic data structures [Kraska et al., 2018], Bloom filters [Mitzenmacher, 2018], and a broader\nframework of online primal-dual algorithms [Bamas et al., 2020b]. Berger et al. [2024] explore\nthe metric distortion problem, which is closely related to the facility location problem for\ngeneral metric spaces, and introduce a learning-augmented algorithm that achieves the optimal\nrobustness-consistency trade-off. We direct interested readers to a curated and frequently\n4\n\nupdated list of papers in this area [Lindermayr and Megow].\nLearning-augmented mechanism design. The framework of learning-augmented mecha-\nnism design was first introduced by Agrawal et al. [2022] and Xu and Lu [2022]. Agrawal et al.\n[2022] focused on the strategic facility location problem, proposing mechanisms that leverage\npredictions to improve performance while maintaining truthfulness. Their work achieves the\nbest possible consistency and robustness trade-off given a prediction of the optimal facility\nlocation. They also evaluated the performance of their mechanisms as a function of the\nprediction error. More recently, Christodoulou et al. [2024] provided performance bounds\nbased on an alternative measure of prediction error. Barak et al. [2024] studied this problem\nassuming that the predictions are regarding each agent’s location, but a small fraction of\nthese predictions may be arbitrarily inaccurate. Meanwhile, Chen et al. [2024] evaluated\nnon-truthful mechanisms with respect to their price of anarchy. Istrate and Bonchis [2022]\nand Fang et al. [2024] studied the obnoxious facility location version of this problem, aiming\nto design truthful mechanisms. Apart from the facility location problem, other works in\nlearning-augmented mechanism design have been conducted in various contexts, including\nstrategic scheduling [Balkanski et al., 2023a, Xu and Lu, 2022], auctions [Balkanski et al.,\n2023a,b, Caragiannis and Kalantzis, 2024, Gkatzelis et al., 2024, Lu et al., 2023, Xu and\nLu, 2022], bicriteria mechanism design [Balcan et al., 2024], graph problems with private\ninput [Colini-Baldeschi et al., 2024], and equilibrium analysis [Gkatzelis et al., 2022, Istrate\net al., 2024]. Balkanski et al. [2023b] brought together the line of work on learning-augmented\nmechanism design with the literature on online algorithms with predictions by studying online\nmechanism design with predictions.\n2 Preliminaries\nIn the single facility location problem, there are nstrategic agents with a location profile\ndenoted by x=⟨x1,···, xn⟩, where xicorresponds to the location of agent i. A mechanism\nf(x)outputs a, potentially randomized, location for the facility. The cost incurred by agent i\nis the expected Euclidean distance E[d(xi, f(x))]between the facility location f(x)and their\nlocation xi.\nTwo renowned cost functions considered in this scenario are Egalitarian Social Cost and\nUtilitarian Social Cost. In this work, the goal is to optimize the Egalitarian Social Cost\nC(f,x)=E[max xi∈xd(xi, f(x))], representing the expected maximum cost experienced by a\nsingle agent for each possible outcome of f(x). The optimal location for the facility in the\ntwo-dimensional Euclidean space corresponds to the center of the smallest circle that encloses\nall the points, denoted by o(x).\nTo minimize the social cost, a mechanism needs to ask the agents to report their preferred\nlocations, x∈R2n, and then use this information to determine the facility location f(x)∈R2.\nHowever, the preferred location xiof each agent iis private information, and they can choose to\nmisreport their preferred location if that can reduce their own cost. A mechanism f:R2n→R2\nis considered truthful when no individual agent can benefit by misreporting their location,\ni.e., for all instances x∈R2n, every agent i∈[n], and every deviation x′\ni∈R2, we have that\nd(xi, f(x))≤d(xi, f(x−i, x′\ni)), where x−i=⟨x1,···, xi−1, xi+1,···, xn⟩is the vector of the\nlocations of all agents except agent i.\nIn the context of randomized mechanisms, we can distinguish between two forms of\n5\n\ntruthfulness: universally truthful and truthful in expectation. A universally truthful mechanism\ninvolves a randomization over deterministic truthful mechanisms, with weights that may depend\non the input. On the other hand, a mechanism is considered truthful in expectation if truth-\ntelling yields the agent the maximum expected value. Specifically, a mechanism f:R2n→R2\nis truthful in expectation if for all instances x∈R2n, every agent i∈[n], and every deviation\nx′\ni∈R2, we have that E[d(xi, f(x))]≤E[d(xi, f(x−i, x′\ni))].\nWe focus on mechanisms that are both unanimous and anonymous. A mechanism is\nunanimous if, when all points xare located at the same position ( xi=xjfor all i, j∈[n]),\nit places the facility at that specific location, i.e., f(x)=xi. To ensure bounded robustness,\na mechanism must be unanimous, as the optimal cost is zero when the facility is at the\nsame location as all points, whereas placing it elsewhere incurs a positive cost. A mechanism\nis anonymous if its outcome is independent of the agents’ identities, meaning it remains\nunchanged under any permutation of the agents. Finally, we also assume the mechanism is\nscale-independent, i.e., if we multiply every coordinate of every agent by the same factor, the\ncoordinate of the chosen facility location are scaled in the same way; this captures the fact\nthat it is only the relative distances that really matter in this problem.\nLearning-augmented algorithms encompass a class of algorithms that enhance their decision-\nmaking process by integrating pre-computed predictions or forecasts. These predictions,\nobtained from various sources, like statistical models, serve as inputs without requiring real-\ntime learning from new data. For instance, in the single facility location problem, one might\nconsider predictions ˆxregarding all of the agent’s preferred locations, a prediction F∗regarding\nthe optimal facility location, or a prediction regarding the identities of the most extreme agents\nˆe(the ones that suffer the maximum cost in the optimal solution), which, as demonstrated in\nthis paper, proves to be quite useful. We denote mechanisms enhanced with predictions in\ngeneral as f(x,∗). Specifically, for each prediction setting like ˆxorF∗, we denote a mechanism\nenhanced with ˆxandF∗byf(x,ˆx)andf(x, F∗), respectively. We define mechanisms enhanced\nwith other kinds of predictions in a similar way.\nThe effectiveness of learning-augmented mechanisms is evaluated using their consistency\nand robustness. If x◁∗denotes all instances xfor which prediction ∗is accurate, a mechanism\nis\n•α-consistent if it achieves an α-approximation when the prediction is correct, i.e.,\nmax\nx,∗:x◁∗\u001aC(f(x,∗),x)\nC(o(x),x)\u001b\n≤α.\n•β-robustif it maintains a β-approximation regardless of the quality of the prediction, i.e.,\nmax\nx,∗\u001aC(f(x,∗),x)\nC(o(x),x)\u001b\n≤β.\n3 Results for the Line\nIn this section, we provide a lower bound regarding the performance of any randomized\nmechanism for the line, even if it is equipped with the strongest type of prediction, i.e., a\nprediction ˆxregarding the preferred location of every agent.\n6\n\nTheorem 1. No mechanism for the line that is truthful in expectation and guarantees 1 +δ\nconsistency for some δ∈[0,0.5]can also guarantee robustness better than 2−δ, even if it is\nprovided with full predictions ˆxcontaining each of the agents’ locations.\nThe proof consists of two main parts. In Subsection 3.1, we show that, for any instance\ninvolving two agents, we can without loss of generality restrict our attention to a class of\nmechanisms that we call OnlyM mechanisms. In Subsection 3.2, we then show the desired\nlower bound for OnlyM mechanisms.\n3.1 The reduction to OnlyM mechanisms\nWe introduce the following notations specific to this section. Let xLbe the leftmost reported\nlocationand xRbetherightmostreportedlocationontheline. Therefore, wehave xL≤xR. Let\nMdenote the midpoint of these two extreme points, i.e., M=(xL+xR)/2, which would also\ncorrespond to the optimal facility location. For simplicity, we sometimes write f(x1, . . . , x n),\ndropping the angle brackets ⟨⟩.OnlyM is the class of mechanisms that, whenever they choose\na location within the interval (xL, xR), then this location is always M.\nDefinition 1 (OnlyM mechanisms) .A mechanism ffor the line is an OnlyM mechanism\nifP[f(x)∈(xL, xR)\\ {M}] = 0.\nThe main lemma for the proof of Theorem 1 is the following reduction that allows to\nrestrict our attention to OnlyM mechanisms.\nLemma 1. For any problem instance involving two agents with reported locations x=⟨xL, xR⟩\non the line, and any randomized truthful in expectation mechanism achieving αconsistency\nandβrobustness over this class of instances, there exists a randomized OnlyM mechanism\nthat is truthful in expectation and achieves the same consistency and robustness guarantees.\nThe remainder of Section 3.1 is devoted to the proof of Lemma 1. Consider any randomized\nmechanism f(xL, xR)and let pℓ=P[f(xL, xR)∈(xL, M)]andpr=P[f(xL, xR)∈(M, x R)]\nrepresent the probabilities that this mechanism chooses a facility location in (xL, M)and\n(M, x R), respectively. If pℓ=pr= 0, the mechanism already satisfies the desired property, and\nthe proof is complete. Otherwise, if pℓ>0, define πℓ=E[f(xL, xR)|f(xL, xR)∈(xL, M)]\nand, if pr>0, define πr=E[f(xL, xR)|f(xL, xR)∈(M, x R)]as the expected locations\nreturned by the mechanism when restricted to (xL, M)and(M, x R), respectively.\nSince πℓlies in (xL, M)andπrlies in (M, x R), we can express these two points as convex\ncombinations of xL,M, and xR:πℓ=qℓxL+(1−qℓ)Mandπr=qrxR+(1−qr)Mfor some\nqℓ, qr∈(0,1). We then modify the original mechanism fto a new OnlyM mechanism f′\ndefined as\nP[f′(xL, xR) =x] =\n\nP[f(xL, xR) =x] ifx < x Lorx > x R\n0 ifx∈(xL, M)∪(M, x R)\nP[f(xL, xR) =x] +qℓpℓ ifx=xL\nP[f(xL, xR) =x] + (1−qℓ)pℓ+ (1−qr)prifx=M\nP[f(xL, xR) =x] +qrpr ifx=xR\nThe construction of f′is illustrated in Figure 1. To show that mechanism f′achieves the\nsame consistency and robustness guarantees as mechanism f, we show that their expected\ncosts are equal on all instances with two agents.\n7\n\nAgents’ reported locations\nMid point\nNon -zero probability outcomesArbitrary Mechanism\nMechanismFigure 1: The original mechanism f(.)and the new OnlyM mechanism f′(.)in the proof\nof Lemma 1, where pℓ=P[f(xL, xR)∈(xL, M)]andpr=P[f(xL, xR)∈(M, x R)],πℓ=\nE[f(xL, xR)|f(xL, xR)∈(xL, M)]andπr=E[f(xL, xR)|f(xL, xR)∈(M, x R)], and qℓand\nqrare such that πℓ=qℓxL+ (1−qℓ)Mandπr=qrxR+ (1−qr)M.\nLemma 2. For all mechanisms ffor the line and instances x=⟨xL, xR⟩with two agents, the\nexpected costs of fandf′over xare equal, i.e., C(f,x) =C(f′,x).\nProof.First, note that\nE\u0014\nmax\nxi∈{xL,xR}d(xi, f(x))|f(x)∈(xL, M)\u0015\n·P[f(x)∈(xL, M)]\n=E[d(xR, f(x))|f(x)∈(xL, M)]·pℓ\n=d(xR, πℓ)·pℓ.\nSimilarly, we have\nE\u0014\nmax\nxi∈{x}d(xi, f(x))|f(x)∈(M, x R)\u0015\n·P[f(x)∈(M, x R)] =d(πr, xL)·pr,\nwhich implies that\nC\u0000\nf′,x\u0001\n−C(f,x) =E\u0014\nmax\nxi∈{xL,xR}d\u0000\nxi, f′(x)\u0001\u0015\n−E\u0014\nmax\nxi∈{xL,xR}d(xi, f(x))\u0015\n=d(xR, xL)·\u0000\nP[f′(x)∈ {xL, xR}]−P[f(x)∈ {xL, xR}]\u0001\n+d(xR, M)·\u0000\nP[f′(x) =M]−P[f(x) =M]\u0001\n−d(xR, πℓ)·pℓ−d(xL, πr)·pr\n=d(xR, xL)·(qℓpℓ+qrpr) +d(xR, M)·((1−qℓ)pℓ+ (1−qr)pr)\n−d(xR, πℓ)·pℓ−d(xL, πr)·pr\n=d(xR, xL)·(qℓpℓ+qrpr) +d(xR, M)·((1−qℓ)pℓ+ (1−qr)pr)\n−d(xR, xL)·qℓpℓ−d(xR, M)·(1−qℓ)pℓ\n−d(xL, xR)·qrpr−d(xL, M)·(1−qr)pr\n= 0.\n8\n\nNext, to show that mechanism f′is also truthful in expectation, we first show that the\ncosts of the agents do not change between fandf′.\nLemma 3. For all mechanisms ffor the line and instances x=⟨xL, xR⟩with two agents,\nthe cost of the agent at location xLis identical under both fandf′, i.e., E[d(xL, f(x))]=\nE[d(xL, f′(x))].Similarly, we have E[d(xR, f(x))] =E[d(xR, f′(x))].\nProof.By definition of f′, we have\nE[d(xL, f(x))]−E\u0002\nd\u0000\nxL, f′(x)\u0001\u0003\n=d(xL,E[f(x)|f(x)∈(xL, M)] =πℓ)·P[f(x)∈(xL, M)]\n+d(xL,E[f(x)|f(x)∈(M, x R)] =πr)·P[f(x)∈(M, x R)]\n+d(xL, M)·\u0000\nP[f(x) =M]−P\u0002\nf′(x) =M\u0003\u0001\n+d(xL, xR)·\u0000\nP[f(x) =xR]−P\u0002\nf′(x) =xR\u0003\u0001\n=d(xL, qℓxL+ (1−qℓ)M)·pℓ+d(xL, qrxR+ (1−qr)M)·pr\n−d(xL, M)((1−qℓ)pℓ+ (1−qr)pr)−d(xL, xR)(qrpr)\n=d(xL, M)(1−qℓ)pℓ+d(xL, xR)qrpr+d(xL, M)(1−qr)pr\n−d(xL, M)(1−qℓ)pℓ−d(xL, M)(1−qr)pr−d(xL, xR)qrpr\n= 0.\nNext, we use the previous lemma to show that mechanism f′preserves the truthful in\nexpectation guarantee of f.\nLemma 4. If a mechanism ffor the line is truthful in expectation over instances with two\nagents, then f′is also truthful in expectation over instances with two agents.\nProof.Assume that fis truthful in expectation over instances with two agents. Assume that\none of the agents deviates and reports a false location in mechanism f′. Due to symmetry, and\nwithout loss of generality, assume that the agent located at xLdeviates and reports a false\nlocation x′\nL. We need to show\nE\u0002\nd\u0000\nxL, f′\u0000\nx′\nL, xR\u0001\u0001\u0003\n≥E\u0002\nd\u0000\nxL, f′(xL, xR)\u0001\u0003\n.\nSince the original mechanism f(xL, xR)is truthful in expectation, we have\nE\u0002\nd\u0000\nxL, f\u0000\nx′\nL, xR\u0001\u0001\u0003\n≥E[d(xL, f(xL, xR))]. (1)\nCombining these two inequalities with Lemma 3, it suffices to show\nE\u0002\nd\u0000\nxL, f′\u0000\nx′\nL, xR\u0001\u0001\u0003\n≥E\u0002\nd\u0000\nxL, f\u0000\nx′\nL, xR\u0001\u0001\u0003\n.\nTo prove the above inequality, we consider two main cases. First, we focus on the scenario\nwhere the agent located at xLdeviates to the right, i.e., xL< x′\nL. Let M′=x′\nL+xR\n2. Assume\nx′\nL≤xR, define p′\nℓ=P[f(x′\nL, xR)∈(x′\nL, M′)]andp′\nr=P[f(x′\nL, xR)∈(M′, xR)]. Ifp′\nℓ=\np′\nr= 0, then f′(x′\nL, xR)=f(x′\nL, xR), and hence E[d(xL, f′(x′\nL, xR))]=E[d(xL, f(x′\nL, xR))].\nTherefore, the proof is complete in this case.\n9\n\nNext, consider the distribution where the mechanism f(x′\nL, xR)returns a random facility\nlocation within the intervals (x′\nL, M′)ifp′\nℓ>0, or within (M′, xR)ifp′\nr>0. The expected\nlocations within these intervals are π′\nℓ∈(x′\nL, M′)andπ′\nr∈(M′, xR), which can be expressed\nasπ′\nℓ=q′\nℓx′\nL+(1−q′\nℓ)M′andπ′\nr=q′\nrxR+(1−q′\nr)M′, where q′\nℓandq′\nrare the respective\nconvex coefficients.\nIn the first case, where xL< x′\nL, we show that the cost to the left agent is the same across\nthe two mechanisms. The key idea is that the difference between the two mechanisms, in\nterms of their returned locations, lies within the intervals (x′\nL, M′)and(M′, xR), both on the\nright side of xL. Thus, the agent’s cost is fully determined by the expected locations in these\nintervals. Specifically we have\nE\u0002\nd\u0000\nxL, f\u0000\nx′\nL, xR\u0001\u0001\u0003\n−E\u0002\nd\u0000\nxL, f′\u0000\nx′\nL, xR\u0001\u0001\u0003\n=d\u0000\nxL,E\u0002\nf\u0000\nx′\nL, xR\u0001\n|f\u0000\nx′\nL, xR\u0001\n∈\u0000\nx′\nL, M′\u0001\u0003\n=π′\nℓ\u0001\n·P\u0002\nf\u0000\nx′\nL, xR\u0001\n∈\u0000\nx′\nL, M′\u0001\u0003\n+d\u0000\nxL,E\u0002\nf\u0000\nx′\nL, xR\u0001\n|f\u0000\nx′\nL, xR\u0001\n∈\u0000\nM′, xR\u0001\u0003\n=π′\nr\u0001\n·P\u0002\nf\u0000\nx′\nL, xR\u0001\n∈\u0000\nM′, xR\u0001\u0003\n+d\u0000\nxL, x′\nL\u0001\n·\u0000\nP\u0002\nf\u0000\nx′\nL, xR\u0001\n=x′\nL\u0003\n−P\u0002\nf′\u0000\nx′\nL, xR\u0001\n=x′\nL\u0003\u0001\n+d\u0000\nxL, M′\u0001\n·\u0000\nP\u0002\nf\u0000\nx′\nL, xR\u0001\n=M′\u0003\n−P\u0002\nf′\u0000\nx′\nL, xR\u0001\n=M′\u0003\u0001\n+d(xL, xR)·\u0000\nP\u0002\nf\u0000\nx′\nL, xR\u0001\n=xR\u0003\n−P\u0002\nf′\u0000\nx′\nL, xR\u0001\n=xR\u0003\u0001\n=d\u0000\nxL, q′\nℓx′\nL+\u0000\n1−q′\nℓ\u0001\nM′\u0001\n·p′\nℓ+d\u0000\nxL, q′\nrxR+\u0000\n1−q′\nr\u0001\nM′\u0001\n·p′\nr\n−d\u0000\nxL, x′\nL\u0001\nq′\nℓp′\nℓ−d\u0000\nxL, M′\u0001\u0000\u0000\n1−q′\nℓ\u0001\np′\nℓ+\u0000\n1−q′\nr\u0001\np′\nr\u0001\n−d(xL, xR)q′\nrp′\nr\n=d\u0000\nxL, x′\nL\u0001\nq′\nℓp′\nℓ+d\u0000\nxL, M′\u0001\u0000\n1−q′\nℓ\u0001\np′\nℓ+d(xL, xR)q′\nrp′\nr+d\u0000\nxL, M′\u0001\u0000\n1−q′\nr\u0001\np′\nr\n−d\u0000\nxL, x′\nL\u0001\nq′\nℓp′\nℓ−d\u0000\nxL, M′\u0001\u0000\u0000\n1−q′\nℓ\u0001\np′\nℓ+\u0000\n1−q′\nr\u0001\np′\nr\u0001\n−d(xL, xR)q′\nrp′\nr\n= 0.\nNote that in the case of xR< x′\nL, the same arguments hold with a minor change in notation.\nDefine p′\nℓ=P[f(x′\nL, xR)∈(M′, x′\nL)]andp′\nr=P[f(x′\nL, xR)∈(xR, M′)]. Consider the expected\nlocations π′\nℓandπ′\nrwithin the intervals (M′, x′\nL)and(xR, M′), respectively.\nFocusing our attention on the more interesting case where x′\nL< xL, we aim to show that\nE[d(xL, f′(x′\nL, xR))]≥E[d(xL, f(x′\nL, xR))]. If we focus on the interval (x′\nL, M′)and assume\nxL≤M′, the expected location of the facility returned by the two mechanisms, f(x′\nL, xR)\nandf′(x′\nL, xR), conditioned on the facility being in the interval (x′\nL, M′), is the same; this is\ntrue by construction (our reduction maintains the expected location π′\nℓbetween the reported\nlocation and the midpoint). The crucial difference between the two mechanisms is that although\nf(x′\nL, xR)may return any facility location in the (x′\nL, M′)interval, the mechanism f′(x′\nL, xR)\nreturns the same expected location using only x′\nLandM′in its support. We show that the cost\nto an agent located at any point xLin the (x′\nL, M′)interval is weakly higher in f′(x′\nL, xR)than\nit is in f(x′\nL, xR), thus maintaining the truthfulness guarantee. Intuitively, this is true because\nthe two mechanisms return the same expected location π′\nℓin that interval, but f′(x′\nL, xR)only\nreturns the extreme points of the interval, which hurt the agent at xLthe most.\nMore formally, we replace any probability assigned to a point in (x′\nL, M′)with a convex\ncombination between x′\nLandM′, and each time we do this, we weakly increase the cost to an\nagent who is located at any point in (x′\nL, M′). Specifically, for any outcome y∈(x′\nL, M′)of\nthe mechanism f(x′\nL, xR)and any xL∈(x′\nL, M′), if we have y=w·x′\nL+(1−w)·M′then\nd(xL, y)< w·d(xL, x′\nL) + (1 −w)·d(xL, M′).\nSince π′\nℓ=q′\nℓx′\nL+(1−q′\nℓ)M′, at the end of this process, we will end up with q′\nℓp′\nℓprobability\n10\n\nincrease on x′\nL, and (1−q′\nℓ)p′\nℓprobability increase on M′. Therefore, we have\nE\u0002\nd\u0000\nxL, f\u0000\nx′\nL, xR\u0001\u0001\u0003\n< d\u0000\nxL, x′\nL\u0001\nq′\nℓp′\nℓ+d\u0000\nxL, M′\u0001\u0000\u0000\n1−q′\nℓ\u0001\np′\nℓ+\u0000\n1−q′\nr\u0001\np′\nr\u0001\n+d(xL, xR)q′\nrp′\nr\n=E\u0002\nd\u0000\nxL, f′\u0000\nx′\nL, xR\u0001\u0001\u0003\n.\nIn the case where xL> M′, similar arguments apply in the interval (M′, xR).\nThe proof of Lemma 1 then immediately follows from Lemma 2 and Lemma 4.\n3.2 The lower bound for OnlyM mechanisms\nEquipped with Lemma 1, we can now prove Theorem 1.\nProof of Theorem 1. Westartbyprovingtheresultfortwo-agentinstancesonthelineusingany\nOnlyM mechanism. Then, using Lemma 1, the result follows for any randomized mechanism.\nFirst, note that if the chosen facility location yis at distance d(M, y)from the point\nM=(xL+xR)/2, then its egalitarian social cost is equal to C(o(x),x)+d(M, y). To verify\nthis fact, assume without loss of generality that this location is on the left of Mand note that\nits distance from the agent located at xRisd(xR, M)+d(M, y). As a result, the expected\nsocial cost of a mechanism fwith agent locations xis\nC(f,x) =C(o(x),x) +E[d(M, f(x))]. (2)\nNow, assume that there exists a mechanism that is (1 +δ)-consistent and better than\n(2−δ)-robust. For robustness, this would imply that for every instance x, irrespective of the\nprediction, the mechanism must guarantee that\nC(f,x)\nC(o(x),x)<2−δ⇒E[d(M, f(x))]\nC(o(x),x)<1−δ. (3)\nFor this mechanism to be truthful in expectation, it must ensure that no agent has an\nincentive to misreport their location. Consider the instance x=⟨xL, xR⟩, and note that the\nagent at xLhas the option to misreport their location as x′\nL=xL−d(xL, xR), which would\nshift the new midpoint M′toxLin the new instance x′=⟨x′\nL, xR⟩. This deviation would\ndouble the optimal cost, i.e., C(o(x′),x′)= 2·C(o(x),x). Inequality (3), then guarantees that\nthe expected cost for this agent after the deviation would be at most 2(1−δ)·C(o(x),x)since\nwe have\nE[d(M′, f(x′))]\nC(o(x′),x′)=E[d(xL, f(x′))]\n2·C(o(x),x)<1−δ. (4)\nAs a result, to ensure that this agent will not misreport, the mechanism needs to ensure\nthat the expected cost of the agent if they report the truth is strictly less than 2(1−δ)·\nC(o(x),x). If we let P(≤L)=P[f(x)≤xL]denote the probability that the chosen location\nis weakly on the left of xL, and P(M)=P[f(x)=M]denote the probability that the chosen\nlocation is M, then the expected cost of the agent located at xLis at least C(o(x),x)P(M)+\n2C(o(x),x)(1−P(M)−P(≤L))because the mechanism is an OnlyM mechanism. This is\neven if we assume that the only chosen facility locations weakly on the left of xL(and weakly\n11\n\non the right of xR, respectively) are exactly on xL(and exactly on xR, respectively). Therefore,\nthe mechanism needs to always satisfy\nC(o(x),x)(P(M) + 2(1 −P(M)−P(≤L)))<2(1−δ)C(o(x),x)\n⇒P(M) + 2(1 −P(M)−P(≤L))<2(1−δ)\n⇒P(≤L)> δ−P(M)\n2.(5)\nIf we consider the same instance x=⟨xL, xR⟩, and assume that the mechanism is also\nprovided with accurate predictions ˆxL=xLandˆxR=xRregarding the agent locations, to\nguarantee 1 +δconsistency, Inequality (2) implies that\nC(f(x,ˆ x=x),x)\nC(o(x),x)≤1 +δ⇒E[d(M, f(x,ˆ x))]\nC(o(x),x)≤δ. (6)\nFinally, if we once again consider the same instance with inaccurate predictions ˆxL=xL\nandˆxR=xR+d(xL, xR), we observe that in this case the agent located at xRwould have the\noption to instead report x′′\nR=xR+d(xL, xR)=ˆxR, and the predictions would then appear to\nbe accurate, forcing the mechanism to satisfy Inequality (6)in order to maintain the required\nconsistency bound. Since the true location xRwould now coincide with the middle point\nM′′of the misreported instance x′′=⟨xL, x′′\nR⟩, this would yield the agent located at xRwho\nmisreported an expected cost of 2δ·C(o(x),x)since we have\nE[d(M′′, f(x′′,ˆ x))]\nC(o(x′′),x′′)=E[d(xR, f(x,ˆ x))]\n2·C(o(x),x)≤δ. (7)\nAs a result, to ensure that this agent will not misreport, the mechanism needs to ensure\nthat the expected cost of the agent if they report the truth is at most 2δ·C(o(x),x). If we\nonce again let P(≤L)denote the probability that the chosen location is weakly on the left of\nxL, and P(M)denote the probability that the chosen location is M, then the expected cost\nof the agent located at xRis at least C(o(x),x)P(M)+ 2C(o(x),x)P(≤L). This is even if\nwe once again assume that the only chosen facility locations weakly on the left of xL(and\nweakly on the right of xR, respectively) are exactly on xL(and exactly on xR, respectively).\nTherefore, the mechanism needs to always satisfy\nP(M) + 2P(≤L)≤2δ⇒P(≤L)≤δ−P(M)\n2.\nHowever, this contradicts Inequality (5), so we conclude that no mechanism can simultaneously\nguarantee (1 +δ)consistency and a robustness better than (2−δ).\n3.3 The hardness result for the line is tight\nWe observe that the lower bound regarding the robustness and consistency trade-off shown in\nTheorem 1 is actually tight. Specifically, it can be achieved by an appropriate randomization\nbetweentheoptimaldeterministiclearning-augmentedmechanismandtheoptimalnon-learning-\naugmented randomized mechanism.\nProposition 1. For any δ∈[0,0.5], there exists a randomized mechanism on the line that is\ntruthful in expectation, 1 +δ-consistent, and 2−δ-robust.\n12\n\nProof.We show that the bound of the robustness consistency trade-off in Theorem 1 is tight.\nTo verify the tightness for δ= 0.5, note that it is possible to achieve a robustness of 1.5(and\ntherefore also a consistency of 1.5) using the following randomized truthful in expectation\nmechanism of Procaccia and Tennenholtz [2013]:\nMechanism 1 (LRM).Given input x, return xLwith probability 1/4,xRwith probability 1/4,\nandMwith probability 1/2.\nOn the other extreme, to verify the tightness of the theorem for δ= 0, note that it is\npossible to achieve a robustness of 2with a consistency of 1using the following truthful\ndeterministic learning-augmented mechanism of Agrawal et al. [2022]:\nMechanism2 (MinMaxP) .Given x, and a prediction F∗regarding the optimal facility location,\nreturn F∗ifF∗∈[xL, xR], otherwise return xLifF∗< xL, and return xRifF∗> xR.\nIn fact, for any δ∈[0,0.5], we can achieve a robustness of 2−δand a consistency of\n1 +δby randomly choosing which one of these two mechanisms to run. Specifically, we can\nachieve these bounds by running the LRM mechanism with probability 2δand the MinMaxP\nmechanism with probability 1−2δ. Note that since the decision regarding which one of the\ntwo truthful mechanisms to run is independent of the agents’ reports, the resulting randomized\nmechanism is truthful as well.\nAs a result, the bound of Theorem 1 precisely captures the optimal robustness consistency\ntrade-off over all truthful in expectation mechanisms for instances on the line.\n3.4 Other prediction settings\nTo gain a more complete picture of different prediction settings, we study an alternative\nstrong prediction that is not strictly stronger than the predicted optimal facility location F∗.\nSpecifically, we consider a setting where predictions are available for all pieces of information\nexcept for one of the extreme locations. We show that these predictions are not helpful, even\nwithout any robustness constraints, to improve the consistency guarantee alone.\nTheorem 2. Given a prediction set that provides the identities of all nagents, there exist\nn−1agents such that, even if we have their exact locations, there is no deterministic truthful\nmechanism on the line that is better than 2-consistent, and there is no randomized mechanism\non the line that is truthful in expectation and better than 1.5-consistent.\nProof.We first address the deterministic case with two agents whose locations are given by\nx=⟨xL, xR⟩. Assume we have a prediction ˆxL, which predicts the location of the leftmost\nagent. We will show that even if ˆxLis accurate, there is no truthful mechanism with a\nconsistency better than 2.\nWe use the same argument as in Theorem 3.2of Procaccia and Tennenholtz [2013]. Assume,\nfor contradiction, that f:R2→Ris a truthful mechanism with a consistency less than 2.\nConsider the location profile x=⟨xL= 0, xR= 1⟩and the prediction ˆxL= 0. To achieve\nconsistency better than 2, the mechanism needs to choose a facility location y=f(x)such\nthaty∈(0,1).\nNow consider an alternative location profile x′=⟨x′\nL= 0, x′\nR=y⟩with the prediction\nˆxL= 0. The optimal facility location for this profile is at y/2, yielding a maximum cost of\ny/2. To maintain consistency better than 2, the mechanism should place the facility within\n13\n\nthe interval (0, y). However, if this were to occur, the rightmost agent would have an incentive\nto report a location of 1instead of y, as this lie would place the facility exactly at y, rather\nthan within (0, y). This contradicts the truthfulness of the mechanism.\nThis argument extends to cases with arbitrary nby situating all other agents at 0in both\nprofiles xandx′, with accurate predictions for them at 0. Similar reasoning holds true.\nFor the randomized setting, we use the idea from the proof of Theorem 3.4in Procaccia and\nTennenholtz [2013]. We first focus on the case with two agents and then extend the result to\nany number of agents. Let fbe a randomized truthful in expectation mechanism. Consider the\nlocation profile x=⟨xL= 0, xR= 1⟩. We have f(x)=P, where Pis a probability distribution\noverR. There exists xi∈xsuch that E[d(xi,P)]≥1/2. If this agent’s location prediction is\nunavailable, we can prove that consistency better than 1.5cannot be achieved.\nAssume E[d(xR,P)]≥1/2and that we have an accurate prediction ˆxL= 0for the leftmost\nagent xL. Consider the profile x′=⟨x′\nL= 0, x′\nR= 2⟩, with an accurate prediction ˆxL= 0.\nFor truthfulness, the expected distance from the location 1should still be 1/2, otherwise the\nrightmost agent would lie in profile x. We know that if E[d(M,P)] = ∆, then the expected\nmaximum cost is ∆ + 1. With ∆≥1/2, the expected cost is at least 1.5, whereas the optimal\ncost is 1, resulting in a consistency of at least 1.5.\nThis argument extends to arbitrary nby situating all other agents at 1/2in both profiles x\nandx′, with accurate predictions for them at 1/2. Similar reasoning holds true.\n4 Results for the Plane\nWe now consider instances in the Euclidean plane, with a location profile x=⟨x1, . . . , x n⟩,\nwhere xi∈R2for each agent i. Missing proofs of this section can be found in Appendix A.\n4.1 Impossibility Results\nBefore considering the robustness and consistency guarantees achievable by randomized mech-\nanisms augmented with different types of predictions, we start off by reducing the gap on what\nis known for mechanisms without predictions.\nSince the problem of designing good facility location mechanisms in two dimensions is\n“harder” than the one-dimensional case, it may seem counterintuitive at first, but the lower\nbound that prior work proved for all one-dimensional instances does not extend to two\ndimensions. The reason is that the design space for two-dimensional mechanisms is much\nricher, and the known lower bounds apply only to the more restricted class of one-dimensional\nmechanisms. For example, consider a simple instance with just two agents in two dimensions.\nFor this two-dimensional instance, the mechanism can return a location for the facility that\nis not on the line containing the two agents’ locations, which it, of course, cannot do in one\ndimension.\nNote that, in terms of social cost alone, returning such a location is always Pareto-dominated\nby returning an appropriate location on the line (e.g., its projection onto the line). However,\nreturning a location that is not on this line also allows the designer to affect the incentives of\nthe participating agents in new and non-trivial ways that are impossible in the one-dimensional\ncase. Specifically, returning points that are not on the line allows us to induce previously\nimpossible cost vectors. For a simple example, in a one-dimensional instance involving two\nagents at distance 1from each other, the only facility location that yields the same cost to\nboth agents is the midpoint between them, leading to a cost vector of (0.5,0.5). However, in\n14\n\nFigure 2: Instances x=⟨x1=(0,0), x2=···=xn−1=(1/2,0), xn=(1,0)⟩, and x′=⟨x′\n1=\n(0,0), x′\n2=···=x′\nn−1=(1/2,0), x′\nn=(2,0)⟩in the proof of Theorem 2. It is assumed that\nd(xn, f(x))≥1/2andx⋆∈arg min y:d(xn,y)≥1/2C(y,x)\n.\ntwo dimensions, we can induce a cost vector of (c, c)for any c≥0.5by simply returning the\nappropriate point on the interval’s perpendicular bisector.\nThis additional flexibility could potentially provide the designer with novel ways to ensure\nthat the agents do not misreport their locations; for example, the classic technique of “money\nburning,” used to achieve incentive compatibility without monetary payments, heavily depends\non the designer’s ability to penalize and reward agents based on their reports. Although\nwe conjecture that truthful mechanisms are always better off returning a facility on the line\ncontaining the agents’ locations in this instance, proving such a result appears non-trivial.\nDue to this additional flexibility in two dimensions, proving inapproximability results for this\nbroader class of mechanisms is a more challenging problem and seems to require new techniques\nand insights.\nTheorem 3. Any randomized mechanism that is truthful in expectation in the Euclidean metric\nspace has an approximation ratio of at least 1.118.\nProof.Letf(x)=Pbe a randomized truthful in expectation mechanism, where Pis a\nprobability distribution over R2. Consider the location profile x=⟨x1=(0,0), x2=···=\nxn−1=(1/2,0), xn=(1,0)⟩. There exists an xi(either x1orxn) such that d(xi, f(x))=\nEy∼Pd(xi, y)≥1/2. Without loss of generality, assume that d(xn, f(x))≥1/2. Now consider\nthe location profile x′=⟨x′\n1=(0,0), x′\n2=···=x′\nn−1=(1/2,0), x′\nn=(2,0)⟩. To maintain\ntruthfulness, we must have d(xn= (1,0), f(x′))≥1/2, preventing the agent at location\nxn=(1,0)in the profile xfrom having an incentive to lie about being at location x′\nn=(2,0).\nExtending the result of Theorem 3.4 in Procaccia and Tennenholtz [2013] from the line to\nthe Euclidean metric, if we have d(o(x), f(x))≥∆, then the expected maximum cost is at\nleast q\n∆2+C(o(x),x)2.\nTherefore, the expected cost of x′is at leastq\n1\n4+\u00002\n2\u00012, resulting in a√\n1.25≈1.118\napproximation, as the optimum cost isd(x′\n1,x′\nn)\n2= 1.\nOurfollowingtworesultsprovideimpossibilityresultsforbothdeterministicandrandomized\nmechanisms augmented with perfect predictions. For deterministic mechanisms, Agrawal et al.\n15\n\n[2022] showed that there is a mechanism that, given a prediction about the optimal facility\nlocation, is 1-consistent and\u0000\n1 +√\n2\u0001\n-robust. Our impossibility result for deterministic\nmechanisms shows that stronger predictions do not help: even with predictions about the\nlocation of each agent, there is no deterministic mechanism that achieves a robustness better\nthan 1 +√\n2and a consistency that improves over the best approximation achievable without\npredictions.\nTheorem 4. For any δ >0, there is no deterministic truthful mechanism that is (2−δ)-\nconsistent and\u0000\n1 +√\n2−δ\u0001\n-robust, even if it is provided with full predictions ˆx(the location\nof each agent).\nThe next result shows that there is no hope of achieving the best of both worlds in the\nrandomized setting; to obtain 1consistency, we would need to sacrifice robustness.\nTheorem 5. For any δ >0, there is no randomized mechanism that is truthful in expectation,\n1-consistent, and (2−δ)-robust, even for two-agent instances and even if it is provided with\nfull predictions ˆx(the location of each agent). This is tight, i.e., there exists a randomized\nmechanism that is truthful in expectation, 1-consistent, and 2-robust for two-agent instances.\n4.2 Positive Results Using Extreme ID Prediction\nWe now turn to positive results, and provide a learning-augmented randomized mechanism that\nis provided with a new type of prediction: the prediction does not provide us with any actual\nlocation, but instead only provides us with the identities ˆe=⟨e1,···, ek⟩of the kagents who\nwould suffer the maximum cost in the optimal solution (we refer to them as “extreme agents”),\ni.e.,{e1, . . . , e k}=arg max ei∈[n]d(xei, o(x)). Note that the smallest circle that encloses all the\npoints is the circumcircle of the locations of the extreme agents. Therefore, the center of this\ncircle is o(x), the optimum solution.\nWe propose a mechanism leveraging predictions derived from IDs of extreme agents. The\nmain idea is to return the centroid of the extreme agents, denoted by Gwith a probability\nof half, and each of the extreme points with a probability of 1/2kto prevent misreporting\nincentives.\nAlgorithm 1: Centroid Mechanism on Extreme Agents\nInput : Location profile x=⟨x1,···, xn⟩, Predictions ˆe=⟨e1,···, ek⟩\nOutput: Probability distribution Pon location of the facility\nWith probability 1/2:\nreturn the centroid G=xe1+···+xek\nk\nWith probability 1/2k:\nreturn each point xe1,···, xek\nTang et al. [2020] run this mechanism over all agents (not only the extreme ones) and\nachieve a 2−1/napproximation. We improve the approximation factor (in case of having\ngood predictions) to 2−1/3≈1.67by running this mechanism only on extreme agents. We\nuse their ideas to maintain truthfulness and we use new techniques to show the approximation\nguarantees for an arbitrarily number of agents. Moreover, as long as any returned location falls\nwithin the minimum enclosing circle of the agents predicted to be extreme (which is the case\nfor Mechanism 1), this ensures an approximation factor of 2in case of having bad predictions.\n16\n\nFirst, we argue that k≥2, meaning that there are at least two extreme agents on the\nminimum enclosing circle. Otherwise, one can find a smaller circle containing all the points.\nAs a warm-up, we first consider the k= 2case and propose a randomized mechanism that is\ntruthful in expectation and achieves 1.5consistency and 2robustness for any number of agents.\nTheorem 6. Assume there are only two extreme agents, i.e., k= 2.Then, given predictions\nˆe=⟨e1, e2⟩that provides the IDs of the only two extreme agents, there exists a randomized\nmechanism that is truthful in expectation and achieves 1.5consistency and 2robustness for\nany number of agents.\nProof.If we only have two agents xe1andxe2located on the minimum enclosing circle, xe1xe2\nrepresent a diameter of this circle; otherwise, we can find a smaller circle containing all the\npoints. Therefore, the middle point of these two locations,xe1+xe2\n2, is the optimum location of\nthe facility. Mechanism 1 runs LRM mechanism on the diameter of the minimum enclosing\ncircle, meaning that it returns the optimum location with a probability of 1/2and each of the\nreported locations of the extreme agents with a probability of 1/4, resulting in a 1.5consistency.\nIn terms of truthfulness, since no other agent besides xe1andxe2impacts the mechanism,\nit suffices to show that they do not have an incentive to lie. Agents xe1andxe2lack such\nincentive for reasons similar to those in Mechanism 1. Additionally, as any returned location\nfalls within the minimum enclosing circle, it ensures a robustness factor of 2.\nWe then show that it is sufficient to only consider the case where we have exactly three\nextreme agents on the minimum enclosing circle, meaning k= 3. If we have more than three\nagents located on the minimum enclosing circle, a continuous perturbation of the points makes\nthe probability of having at least four points lying on a circle infinitesimally small [Berg et al.,\n2008]. For k= 3, we use the properties of the Euler line to prove the 1.67consistency.\nTheorem 7. Given a prediction set that provides the IDs of the extreme agents, there exists\na randomized mechanism that is truthful in expectation and achieves 1.67consistency and 2\nrobustness for any number of agents.\nProof.Let us first consider instances with only three extreme agents. Mechanism 1 is truthful\nsince other agents apart from xe1, xe2, xekcannot influence the result and Mechanism 1 is\nequivalenttothemechanismofTangetal.[2020]overagents e1, . . . , e kandsincethismechanism\nis truthful in expectation, agents e1, . . . , e kcannot benefit from misreporting their locations.\nSince the mechanism returns the reported locations, rather than their predictions, and the\ncentroid is guaranteed to be inside the circumcircle, we can conclude that the mechanism has a\nrobustness factor of 2. The technical aspect of this proof involves establishing the consistency\nguarantee by considering the Euler line.\nEuler line. Given three arbitrary points x1, . . . , x 3, their circumcircle is the smallest circle\nthat encloses the three points, their centroid is (x1+x2+x3)/3, and their orthocenter is the\npoint where the three altitudes (the perpendicular line segments from a vertex to the line that\ncontains the opposite side) intersect. In any triangle, the center of the circumcircle ( O), the\ncentroid ( G), and the orthocenter ( H) are collinear, forming the Euler line. One property of the\nEuler line is that Gis positioned midway between OandH. Additionally, d(O,G)=d(G, H)/2,\nimplying d(O,G) =d(O, H)/3.\n17\n\nAltitudes\nMedians\nPerpendicular BisectorsEuler lineFigure 3: G=(xe1+xe2+xe3)/3is the centroid of xe1, . . . , x e3, which is the intersection of\nthe three medians. His the orthocenter, which is the intersection of the three altitudes, and O\nis the center of the circumcircle, which is the intersection of the three perpendicular bisectors.\nIn any triangle, the circumcenter ( O), the centroid ( G), and the orthocenter ( H) are collinear,\nforming the Euler line. Moreover, d(O,G) =1\n2d(G, H).\nLetRdenote the radius of the circumcircle of xe1, xe2, xe3and assume without loss of\ngenerality that the optimum cost is R= 1. For any j∈[n], we have\nd(xj,G)≤d(xj, O) +d(O,G)≤R+d(O,G)\nwhere the first inequality is by triangle inequality and the second is because all the points\nare within the circumcircle of the locations of the extreme agents when the predictions are\ncorrect. Since Hlies within the circumcircle, we infer d(O, H)≤R, conclusively showing that\nd(O,G)≤R/3. Since Ris the optimum cost and any agent can reach Oby paying that cost,\nthe cost of returning Gfor any agent is upper bounded by R+d(O,G), which is at most 1.34R.\nConsequently, the approximation of Mechanism 1 in case of having accurate predictions is less\nor equal than\n1\n2k1\nRkX\ni=1max\nj∈[n]d(xei, xj) +1\n21\nRmax\nj∈[n]d(xj,G)≤1 +1\n2R+d(O,G)\nR≤1 +1\n2\u0012\n1 +1\n3\u0013\n≈1.67.\nwherethefirstinequalityissince d(xei, xj)≤d(xei, O)+d(O, x j)≤2Rwhenthepredictionsare\ncorrect. Next, we show how we can modify the mechanism to achieve the same approximation\nguarantees as having three extreme agents while maintaining truthfulness.\nAs mentioned previously, the key concept involves perturbing the instance before requesting\npredictions to have at most three extreme agents. Given any instance x=⟨x1,···, xn⟩, define\na perturbed instance ˜ x=⟨˜x1,···,˜xn⟩, which is the result of a continuous perturbation of\nx. Consider the set of predictions for the IDs of extreme agents in ˜ x. Although the extreme\n18\n\nagents of the perturbed instance ˜ xmight differ from those of the original instance x, their\ncosts remain very close to the maximum cost in case of having good predictions. Therefore,\nthe circumcircle of them is a good representation of the minimum enclosing circle and results\nin the same approximation guarantees.\nSince we run the mechanism on the main instance x, truthfulness holds as before. Note\nthat since we ask for the ID of extreme agents of the perturbed instance ˜ x, we have consistency\nguarantees if predictions are accurate based on the perturbed instance ˜ x, and in any case we\ncan ensure the robustness factor of 2.\n5 Future Directions\nThe problem of designing randomized facility location mechanisms in the Euclidean space is\nvery natural and several open questions remain, even without predictions. While numerous\nstudies have addressed this problem in restricted spaces, there remains a gap regarding between\nthe best possible approximation guarantee in (1.118,2−1/n). The possibility of a truthful in\nexpectation mechanism achieving better than a 2−1/napproximation is intriguing. Obtaining\na stronger lower bound than 1.118would also be very interesting.\nAugmenting these mechanisms with predictions introduces a new dimension to the problem.\nWhile we have explored various prediction settings and provided both positive and negative\nresults, many of the current findings in Euclidean space lack tightness. Finding tight results\nfor different types of predictions would enable meaningful comparisons and help identify which\nprediction strategy is most effective in different scenarios.\n19\n\nReferences\nPriyank Agrawal, Eric Balkanski, Vasilis Gkatzelis, Tingting Ou, and Xizhi Tan. Learning-\naugmented mechanism design: Leveraging predictions for facility location. In EC ’22: The\n23rd ACM Conference on Economics and Computation . ACM, 2022. doi: 10.1145/3490486.\n3538306.\nMatteo Almanza, Flavio Chierichetti, Silvio Lattanzi, Alessandro Panconesi, and Giuseppe Re.\nOnline facility location with multiple advice. In Advances in Neural Information Processing\nSystems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS\n2021, pages 4661–4673, 2021.\nNoga Alon, Michal Feldman, Ariel D Procaccia, and Moshe Tennenholtz. Strategyproof\napproximation of the minimax on networks. Mathematics of Operations Research , 35(3):\n513–526, 2010.\nAntonios Antoniadis, Christian Coester, Marek Elias, Adam Polak, and Bertrand Simon. Online\nmetric algorithms with untrusted predictions. In International Conference on Machine\nLearning , pages 345–355. PMLR, 2020a.\nAntonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online\nmatching problems with machine learned advice. NeurIPS , 2020b.\nAntonios Antoniadis, Peyman Jabbarzade, and Golnoosh Shahkarami. A Novel Prediction\nSetup for Online Speed-Scaling. In SWAT, 2022.\nYossi Azar, Debmalya Panigrahi, and Noam Touitou. Online graph algorithms with predictions.\nProceedings of the Thirty-Third Annual ACM-SIAM Symposium on Discrete Algorithms ,\n2022.\nMaria-Florina Balcan, Siddharth Prasad, and Tuomas Sandholm. Bicriteria multidimensional\nmechanism design with side information. In Proceedings of the 37th International Conference\non Neural Information Processing Systems , NIPS ’23, Red Hook, NY, USA, 2024. Curran\nAssociates Inc.\nEric Balkanski, Vasilis Gkatzelis, and Xizhi Tan. Strategyproof Scheduling with Predictions.\nIn14th Innovations in Theoretical Computer Science Conference (ITCS 2023) , 2023a. doi:\n10.4230/LIPIcs.ITCS.2023.11.\nEric Balkanski, Vasilis Gkatzelis, Xizhi Tan, and Cherlin Zhu. Online mechanism design\nwith predictions. arXiv preprint:2310.02879 (to appear in the 25th ACM Conference on\nEconomics and Computation (EC ’24)) , 2023b.\nEtienne Bamas, Andreas Maggiori, Lars Rohwedder, and Ola Svensson. Learning augmented\nenergy minimization via speed scaling. arXiv preprint arXiv:2010.11629 , 2020a.\nÉtienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning\naugmented algorithms. In Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual , 2020b.\n20\n\nSiddhartha Banerjee, Vasilis Gkatzelis, Artur Gorokh, and Billy Jin. Online nash social welfare\nmaximization with predictions. In Proceedings of the 2022 ACM-SIAM Symposium on\nDiscrete Algorithms, SODA 2022 . SIAM, 2022.\nZohar Barak, Anupam Gupta, and Inbal Talgam-Cohen. Mac advice for facility location\nmechanism design. arXiv preprint:2403.12181 (to appear in 38th Conference on Neural\nInformation Processing Systems (NeurIPS)) , 2024.\nSalvador Barberà, Faruk Gul, and Stacchetti Ennio. Generalized median voter schemes and\ncommittees. Journal of Economic Theory , 61(2):262–289, 1993.\nMark Berg, Otfried Cheong, Marc van Kreveld, and Mark Overmars. Computational Geometry:\nAlgorithms and Applications . Springer, Berlin, Germany, 3rd edition, 2008. ISBN 978-3-540-\n77973-5.\nBen Berger, Michal Feldman, Vasilis Gkatzelis, and Xizhi Tan. Learning-augmented metric\ndistortion via (p, q)-veto core, 2024.\nKim C. Border and James Jordan. Straightforward elections, unanimity and phantom voters.\nReview of Economic Studies , 50(1):153–170, 1983.\nIoannis Caragiannis and Georgios Kalantzis. Randomized learning-augmented auctions with\nrevenue guarantees. In Proceedings of the Thirty-Third International Joint Conference on\nArtificial Intelligence, IJCAI-24 , 2024. doi: 10.24963/ijcai.2024/297.\nHau Chan, Aris Filos-Ratsikas, Bo Li, Minming Li, and Chenhao Wang. Mechanism design\nfor facility location problems: A survey. In Zhi-Hua Zhou, editor, Proceedings of the\nThirtieth International Joint Conference on Artificial Intelligence, IJCAI-21 , pages 4356–\n4365. International Joint Conferences on Artificial Intelligence Organization, 2021.\nQingyun Chen, Nick Gravin, and Sungjin Im. Strategic facility location via predictions. arXiv\npreprint:2410.07497 (to appear in the 20th International Conference of Web and Internet\nEconomics, WINE) , 2024.\nJakub Chledowski, Adam Polak, Bartosz Szabucki, and Konrad Tomasz Zolna. Robust\nlearning-augmented caching: An experimental study. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139\nofProceedings of Machine Learning Research , pages 1920–1930. PMLR, 2021.\nGeorge Christodoulou, Alkmini Sgouritsa, and Ioannis Vlachos. Mechanism design augmented\nwith output advice. arXiv preprint:2406.14165 (to appear in 38th Conference on Neural\nInformation Processing Systems (NeurIPS)) , 2024.\nRiccardo Colini-Baldeschi, Sophie Klumper, Guido Schäfer, and Artem Tsikiridis. To trust\nor not to trust: Assignment mechanisms with predictions in the private graph model.\narXiv preprint:2403.03725 (to appear in the 25th ACM Conference on Economics and\nComputation(EC ’24)) , 2024.\nPaul Dütting, Silvio Lattanzi, Renato Paes Leme, and Sergei Vassilvitskii. Secretaries with\nadvice. In Proceedings of the 22nd ACM Conference on Economics and Computation , pages\n409–429, 2021.\n21\n\nEl-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, and Lê-Nguyên Hoang. On\nthe strategyproofness of the geometric median. In Proceedings of The 26th International\nConference on Artificial Intelligence and Statistics , pages 2603–2640, 2023.\nJiazhu Fang, Qizhi Fang, Wenjing Liu, and Qingqin Nong. Mechanism design with predictions\nfor facility location games with candidate locations. In Annual Conference on Theory and\nApplications of Models of Computation , pages 38–49. Springer, 2024.\nMichal Feldman and Yoav Wilf. Strategyproof facility location and the least squares objective.\nInProceedings of the fourteenth ACM conference on Electronic commerce , pages 873–890,\n2013.\nDimitris Fotakis, Evangelia Gergatsouli, Themis Gouleakis, and Nikolas Patris. Learning\naugmented online facility location. CoRR, abs/2107.08277, 2021.\nVasilis Gkatzelis, Kostas Kollias, Alkmini Sgouritsa, and Xizhi Tan. Improved price of anarchy\nvia predictions. In Proceedings of the 23rd ACM Conference on Economics and Computation ,\nEC ’22, page 529–557, New York, NY, USA, 2022. Association for Computing Machinery.\nISBN 9781450391504. doi: 10.1145/3490486.3538296.\nVasilis Gkatzelis, Daniel Schoepflin, and Xizhi Tan. Clock auctions augmented with unreliable\nadvice.arXiv preprint:2408.06483 (to appear in the 2025 ACM-SIAM Symposium on Discrete\nAlgorithms, SODA) , 2024.\nSumit Goel and Wade Hann-Caruthers. Optimality of the coordinate-wise median mechanism\nfor strategyproof facility location in two dimensions. Social Choice and Welfare , 2020.\nSungjin Im, Ravi Kumar, Mahshid Montazer Qaem, and Manish Purohit. Online knapsack\nwith frequency predictions. Advances in Neural Information Processing Systems , 34, 2021.\nGabriel Istrate and Cosmin Bonchis. Mechanism design with predictions for obnoxious facility\nlocation. arXiv preprint arXiv:2212.09521 , 2022.\nGabriel Istrate, Cosmin Bonchiş, and Victor Bogdan. Equilibria in multiagent online problems\nwith predictions, 2024.\nShaofeng H.-C. Jiang, Erzhi Liu, You Lyu, Zhihao Gavin Tang, and Yubo Zhang. Online\nfacility location with predictions. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022 . OpenReview.net, 2022.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of\nData, pages 489–504, 2018.\nSilvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online schedul-\ning via learned weights. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on\nDiscrete Algorithms , pages 1859–1877. SIAM, 2020.\nAlexander Lindermayr and Nicole Megow. Alps. https://algorithms-with-predictions.\ngithub.io/ .\n22\n\nPinyan Lu, Zongqi Wan, and Jialin Zhang. Competitive auctions with imperfect predictions.\narXiv preprint:2309.15414 (to appear in the 25th ACM Conference on Economics and\nComputation(EC ’24)) , 2023.\nThodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice.\nJ. ACM, 68(4):24:1–24:25, 2021.\nReshef Meir. Strategyproof facility location for three agents on a circle. In International\nSymposium on Algorithmic Game Theory , pages 18–33. Springer, 2019.\nAdam Meyerson. Online facility location. In 42nd Annual Symposium on Foundations\nof Computer Science, FOCS 2001 , pages 426–431. IEEE Computer Society, 2001. doi:\n10.1109/SFCS.2001.959917.\nMichael Mitzenmacher. A model for learned bloom filters, and optimizing by sandwiching. In\nProceedings of the 32nd International Conference on Neural Information Processing Systems ,\npages 462–471, 2018.\nMichael Mitzenmacher. Scheduling with predictions and the price of misprediction. In 11th\nInnovations in Theoretical Computer Science Conference (ITCS 2020) . Schloss Dagstuhl-\nLeibniz-Zentrum für Informatik, 2020.\nMichael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. Commun. ACM ,\n65(7):33–35, jun 2022. ISSN 0001-0782. doi: 10.1145/3528087.\nHervé Moulin. On strategy-proofness and single peakedness. Public Choice , 35(4):437–455,\n1980.\nHans Peters, Hans van der Stel, and Ton Storcken. Range convexity, continuity, and strategy-\nproofness of voting schemes. Zeitschrift für Operations Research , 38:213–229, 1993.\nAriel D. Procaccia and Moshe Tennenholtz. Approximate mechanism design without money.\nACM Trans. Economics and Comput. , 1(4):18:1–18:26, 2013. doi: 10.1145/2542174.2542175.\nManish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml pre-\ndictions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems . Curran Associates,\nInc., 2018.\nDhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In\nProceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms , pages\n1834–1845. SIAM, 2020.\nTim Roughgarden. Beyond the Worst-Case Analysis of Algorithms . Cambridge University\nPress, 2021.\nPingzhong Tang, Dingli Yu, and Shengyu Zhao. Characterization of group-strategyproof\nmechanismsforfacilitylocationinstrictlyconvexspace. In EC ’20: The 21st ACM Conference\non Economics and Computation , pages 133–157. ACM, 2020. doi: 10.1145/3391403.3399471.\nToby Walsh. Strategy proof mechanisms for facility location in Euclidean and Manhattan\nspace.arXiv preprint arXiv:2009.07983 , 2020.\n23\n\nAlexander Wei. Better and simpler learning-augmented online caching. In APPROX/RANDOM ,\n2020.\nChenyang Xu and Pinyan Lu. Mechanism design with predictions. In Lud De Raedt, editor,\nIJCAI ’22: The 31st International Joint Conference on Artificial Intelligence, , pages 571–577,\n2022.\n24\n\nA Missing Proofs of Section 4\nTheorem 4. For any δ >0, there is no deterministic truthful mechanism that is (2−δ)-\nconsistent and\u0000\n1 +√\n2−δ\u0001\n-robust, even if it is provided with full predictions ˆx(the location\nof each agent).\nProof.Using the characterization provided by Peters et al. [1993], we know that any determin-\nistic, strategyproof, anonymous, and unanimous mechanism can be expressed as a Generalized\nCoordinatewise Median (GCM) mechanism with n−1constant points in P. The GCM\nmechanism takes as input the reported locations xof the nagents and a multiset Pof fixed\npoints (known as phantom points), which are constant and independent of the agents’ reported\nlocations. It then outputs the coordinatewise median of the combined set x∪P.\nConsider a five-agent instance with xcontaining four agents at (0,20)and one agent at\n(0,10). Our proof first shows that to achieve a consistency better than 2, this mechanism\nneeds to introduce four constant points all with y-coordinates greater than 10. Otherwise, even\nif the predictions are correct, the y-coordinate of GCM (x, P)will be at most 10, leading to\nconsistency greater than or equal to 2, since the optimal point would be (0,15).\nThen, we show that if all these four constant points have y-coordinates greater than 10, the\nrobustness of the GCM mechanism is at least 1 +√\n2. Let ˜xbe the median of the x-coordinates\nof the four constant points, and now consider the following instance, where the optimal cost is\none.\nConsider a five-agent instance with xcontaining three agents at (˜x,0), one agent at\n(˜x−1,1), and one agent at\u0010\n˜x−1−1√\n2,−1√\n2\u0011\n. Since ˜xis the median of the x-coordinates\nof both true agent locations xand constant points P,˜xwill be the x-coordinate of the\nGCM (x, P). Since all the constant points have y-coordinates greater than 10, the y-coordinate\nof the GCM (x, P)will be 1.\nSince (˜x,1)has a distance of 1 +√\n2from\u0010\n˜x−1−1√\n2,−1√\n2\u0011\nwhile (˜x−1,0), the optimal\npoint, has a distance of 1from all the points, we can conclude that the robustness factor is at\nleast 1 +√\n2.\nTheorem 5. For any δ >0, there is no randomized mechanism that is truthful in expectation,\n1-consistent, and (2−δ)-robust, even for two-agent instances and even if it is provided with\nfull predictions ˆx(the location of each agent). This is tight, i.e., there exists a randomized\nmechanism that is truthful in expectation, 1-consistent, and 2-robust for two-agent instances.\nProof.Consider the instance x=⟨xL= 0, xR= 2⟩, and assume we have predictions ˆ x=⟨ˆxL=\n0,ˆxR= 2⟩. To achieve 1-consistency, a mechanism should return M= 1with probability 1.\nNow consider another instance x′=⟨x′\nL= 0, x′\nR= 1⟩with incorrect predictions ˆ x=⟨ˆxL=\n0,ˆxR= 2⟩. To ensure that the rightmost agent will not misreport to xR= 2and have a cost\nof zero, the mechanism needs to ensure that the expected cost of the agent if they report\nthe truth is at most zero, which means returning x′\nRwith probability 1. This results in a\nrobustness factor of 2.\nAgrawal et al. [2022] proposed a deterministic Minimum Bounding Box mechanism that is\n1-consistent and 1 +√\n2-robust. We argue that this mechanism is 1-consistent and 2-robust for\ntwo-agent instances.\nMechanism 3 (Minimum Bounding Box) .Given npoints x=⟨(x1, y1),···,(xn, yn)⟩, and a\nprediction F∗=(xF, yF), return (MinMaxP (⟨x1,···, xn⟩, xF), MinMaxP (⟨y1,···, yn⟩, yF)).\n25\n\nObservation 1. Mechanism 3 is 1-consistent and 2-robust for instances with only two agents.\nProof.One of the two diagonals of the minimum bounding box has two agents on its vertices.\nTherefore, any point inside or on this box has the approximation factor of 2.\nTherefore, the upper and lower bounds match for instances with two agents, but it remains\nunclear where the exact value is even for three agents.\n26",
  "textLength": 69765
}