{
  "paperId": "79e0637f45ad85eb97fde6d10dd26f2154812cd7",
  "title": "Learning Predictions for Algorithms with Predictions",
  "pdfPath": "79e0637f45ad85eb97fde6d10dd26f2154812cd7.pdf",
  "text": "Learning Predictions for Algorithms with Predictions\nMikhail Khodak\nCarnegie Mellon University\nkhodak@cmu.eduMaria-Florina Balcan\nCarnegie Mellon University\nninamf@cs.cmu.edu\nAmeet Talwalkar\nCarnegie Mellon University\ntalwalkar@cmu.eduSergei Vassilvitskii\nGoogle Research - New York\nsergeiv@google.com\nAbstract\nA burgeoning paradigm in algorithm design is the ﬁeld of algorithms with predic-\ntions , in which algorithms can take advantage of a possibly-imperfect prediction\nof some aspect of the problem. While much work has focused on using predictions\nto improve competitive ratios, running times, or other performance measures, less\neffort has been devoted to the question of how to obtain the predictions themselves,\nespecially in the critical online setting. We introduce a general design approach\nfor algorithms that learn predictors: (1) identify a functional dependence of the per-\nformance measure on the prediction quality and (2) apply techniques from online\nlearning to learn predictors, tune robustness-consistency trade-offs, and bound the\nsample complexity. We demonstrate the effectiveness of our approach by applying\nit to bipartite matching, ski-rental, page migration, and job scheduling. In several\nsettings we improve upon multiple existing results while utilizing a much simpler\nanalysis, while in the others we provide the ﬁrst learning-theoretic guarantees.\n1 Introduction\nAlgorithms with predictions, a subﬁeld of beyond-worst-case analysis of algorithms [ 36], aims to de-\nsign methods that make use of machine-learned predictions in order to reduce runtime, error, or some\nother performance cost. Mathematically, for some prediction x, algorithms in this ﬁeld are designed\nsuch that their cost Ct(x)on an instance tis upper-bounded by some measure Ut(x)of the quality of\nthe prediction on that instance. The canonical example here is that the cost of binary search on a sorted\narray of size ncan be improved from O(logn)to\u0014Ut(x) = 2 log\u0011t(x), where\u0011t(x)is the distance\nbetween the true location of a query tin the array and the location predicted by the predictor x[36].\nIn recent years, algorithms whose cost depends on the quality of possibly imperfect predictions have\nbeen developed for numerous important problems, including caching [ 41,24,34], scheduling [ 30,43],\nski-rental [29, 1, 15], bipartite matching [16], page migration [22], and many more [10, 17, 36].\nWhile there has been a signiﬁcant effort to develop algorithms that use earned predictions, until very\nrecently [ 13,33] there has been less focus on actually learning to predict. For example, of the works\nlisted only two on ski-rental [ 1,15] and one other [ 16] show sample complexity guarantees, and none\nconsider the important online learning setting, in which problem instances may not come from a ﬁxed\ndistribution. This is in contrast to the related area of data-driven algorithm design [ 19,3], which has\nestablished techniques such as dispersion [ 6] and others [ 8,11] for deriving learning-theoretic guaran-\ntees, leading to end-to-end results encompassing both learning and computation. It is also despite the\nfact that, as we see in this work, learning even simple predictors is in many cases a non-trivial problem.\nWe bridge this gap and provide a framework for obtaining learning-theoretic guarantees for algorithms\nwith predictions. In addition to improving sample complexity bounds, we show how to learn\nthe parameters of interest in an setting with low overall regret. We accomplish this using a two-\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2202.09312v2  [cs.LG]  17 Oct 2022\n\nTable 1: Settings we apply our framework to, new learning algorithms we derive, and their regret.\nProblem Algorithm with prediction Feedback Upper bound (losses) Learning algo. Regret\nMin. weight bipartite Hungarian method Opt. dualO\u0000\nk^ x\u0000x\u0003(c)k1\u0001 Proj. onlineO\u0010\nnpp\nT\u0011\nmatching (3)\u0003;yinitialized by dual ^ x2Rnx\u0003(c) gradient\nOnline page Lazy ofﬂine optimal for Requests~O \nmax\ni2[n]Epi+\rDP\nj=i1^s[j]6=s[j]!ExponentiatedO\u0010\nnpp\nT\u0011\nmigration (4)\u0003predictionsf^s[j]\u0018p[j]gn\nj=1fs[j]gn\nj=1gradient\u0002n\nOnline job Corrected ofﬂine optimal Opt. weightsO(k^ x\u0000logwk1)EuclideanO\u0010pp\nmTlog(mT)\u0011\nscheduling (5)\u0003for predicted logits ^ x2Rmw24mKT-OCO\nNon-clairvoyant Preferential round-robin Predictionminn1+2\u0011=n\n1\u0000\u0015;2\n\u0015oExponentialO\u0000ppTlogT\u0001\njob scheduling (6)zwith trade-off parameter \u0015 quality\u0011 forecaster\nSki-rental w. integer Buy if price b\u0014x,\u0015trade- Number of minf\u0015(b1x>b+n1x\u0014b);b;ng\n1\u0000(1+1=b)\u0000b\u0015ExponentiatedO\u0010\nNpp\nTlog(NT)\u0011\ndaysn2[N](6) off with worst-case approx. ski-days n gradient\nSki-rental with Buy after xdays,\u0015trade- Number of min\beminfn;bg\n(e\u00001)\u0015; ExponentialO\u0000pp\nTlog(NT)\n\f-dispersedn(6) off with worst-case approx. ski-days nn1n\u0014x+(b+x)1n>x\n1\u0000\u0015\tforecaster +N2T1\u0000\f\u0001\n\u0003For these problems we also provide new guarantees in the statistical (i.i.d.) setting and for learning linear predictors that take instance features as their inputs.\nyWe also obtain results for its extensions to minimum-weight b-matching and other graph algorithms with predictions in Appendices B and D.\nzWe also provide new guarantees for the problem of learning job permutations in the non-clairvoyant setting in Appendix E.\nstep approach inspired by recent work on theoretical meta-learning [ 25], which has been used to\nderive numerous multi-task learning results by optimizing regret-upper-bounds that encode the task-\nsimilarity [ 31,32,9,26]. As evidenced by our results in Table 1, we believe the following two-step\nframework below holds similar potential for obtaining guarantees for algorithms with predictions:\n1.For a given algorithm, derive a convenient-to-optimize upper bound Ut(x)on the costCt(x)\nthat depends on both the prediction xand information speciﬁc to instance treturned once the\nalgorithm terminates, e.g. the optimum in combinatorial optimization. We ﬁnd that in many cases\nsuch bounds already exist, and the quality of the prediction can be measured by a distance from\nsome ground truth obtained from the output, a quantity that is usually convex and thus learnable.\n2.Apply online learning to obtain both regret guarantees against adversarial sequences and sample\ncomplexity bounds for i.i.d. instances. We provide pseudo-code for a generic setup in Algorithm 1.\nTable 1 summarizes instantiations of our framework on multiple problems. Our approach is designed\nto be simple-to-execute, leaving much of the difﬁculty to what the ﬁeld is already good at: designing\nalgorithms and proving prediction-quality-dependent upper bounds on their costs. Once the latter is\naccomplished, our framework leverages problem-speciﬁc structure to design a customized learning\nalgorithm for each problem, leading to strong regret and sample complexity guarantees. In particular,\nin multiple settings we improve upon existing results in either sample complexity or generality, and\nin all cases we are the ﬁrst to show regret guarantees in the online setting. This demonstrates the\nusefulness of and need for such a theoretical framework for studying these problems.\nWe summarize the diverse set of contributions enabled by our theoretical framework below:\n1.Bipartite matching: Our starting example builds upon the work on minimum-weight bipartite\nmatching using the Hungarian algorithm by Dinitz et al. [16]. We show how our framework\nleads directly to both the ﬁrst regret guarantees in the online setting and new sample complexity\nbounds that improve over the previous approach by a factor linear in the number of nodes. In the\nAppendix we show similar strong improvements for b-matching and other graph algorithms.\n2.Page migration: We next study a more challenging application, online page migration , and\nshow how we can adapt the algorithmic guarantee of Indyk et al. [22] into a learnable upper bound\nfor which we can again provide both adversarial and statistical guarantees.\n3.Learning linear maps with instance-feature inputs: Rather than assume the existence of a\nstrong ﬁxed prediction, it is often more natural to assume each instance comes with features that\ncan be input into a predictor such as a linear map. Our approach yields the ﬁrst guarantees for\nlearning linear predictors for algorithms with predictions, which we obtain for the two problem\nsettings above and also for online job scheduling using makespan minimization [30].\n4.Tuning robustness-consistency trade-offs: Many bounds for online algorithms with predictions\nincorporate parameterized trade-offs between trusting the prediction or falling back on a worst-case\napproximation. This suggests the usefulness of tuning the trade-off parameter, which we instantiate\non a simple job scheduling problem with a ﬁxed predictor. Then we turn to the more challenging\nproblem of simultaneously tuning the trade-off and learning predictions, which we achieve on two\nvariants of the ski-rental problem. For the discrete case we give the only learning-theoretic guaran-\ntee, while for the continuous case our bound uses a dispersion assumption [ 6] that, in the i.i.d. set-\nting, is a strictly weaker assumption than the log-concave requirement of Diakonikolas et al. [15].\n2\n\n2 Related work\nAlgorithms with predictions is a type of beyond-worst-case analysis of algorithms [ 42]; along with\nareas like smooth analysis [ 45] and data-driven algorithm design [ 3], it takes advantage of the fact\nthat real-world instances are not worst-case. Inspired by success in applications such as learned\nindices [ 27], there has been a great deal of theoretical study focusing on algorithms whose guarantees\ndepend on the quality of a given predictor (c.f. the Mitzenmacher and Vassilvitskii [36] survey). The\nactual learning of this predictor has been studied less [ 1,15,16] and rarely in the online setting; we aim\nto change this with our study. Some papers improve online learning itself using predictions [ 39,23,\n14], but they also assume known predictors or only learn over a small set of policies, and their goal is\nminimizing regret not computation. In-general, we focus on showing how algorithms with predictions\ncan make use of online learning rather than on new methods for the latter. Several works [ 4,40,2] use\nlearning while advising an algorithm, in-effect taking a learning-inspired approach to better make use\nof a prediction within an algorithm, whereas we focus on learning the prediction outside of the target\nalgorithm. Our paper presents the ﬁrst general framework for efﬁciently learning useful predictors.\nData-driven algorithm design is a related area that has seen more learning-theoretic effort [ 19,6,3].\nAt a high-level, it often studies tuning parameters such as the gradient descent step-size [ 19] or\nsettings of branch and bound [ 5], whereas the predictors in algorithms with predictions guess the\nsequence in an online algorithm [ 22] or the actual outcome of the computation [ 16]. The distinction\ncan be viewed as terminological, since a prediction can be viewed as a parameter, but it can mean that\nin our settings we have full information about the loss function since it is typically some discrepancy\nbetween the full sequence or computational outcome and the prediction. In contrast, in data-driven\nalgorithm design getting the cost of each parameter often requires additional computation, leading to\n(semi-)bandit settings [7]. A more salient difference is that data-driven algorithm design guarantees\ncompete with the parameter that minimizes average cost but do not always quantify the improvement\nattainable via learning; in algorithms with predictions we do generally quantify this improvement\nwith an upper bound on the cost that depends on the prediction quality, but we usually only compete\nwith the parameter that is optimal for prediction quality, which is not always cost-optimal. We do\nadapt data-driven algorithm design tools like dispersion [6] for algorithms with predictions.\nOur two-step approach to providing guarantees for algorithms with predictions is inspired by the\nAverage Regret-Upper-Bound Analysis (ARUBA) framework [ 25] for studying meta-learning [ 18].\nInstead of instances they have tasks with different data-points, and the upper bounds are on learning-\ntheoretic quantities such as regret rather than computational costs such as runtime. Mathematically,\nARUBA takes advantage of similar structure in the regret-upper-bounds that we ﬁnd in algorithms\nwith predictions, namely that the upper bounds encode some measure of the quality of the prediction\n(in their case an initialization for gradient descent) via a comparison to the ground-truth (in their\ncase the optimal parameter). However, whereas in ARUBA the need to know this optimal parameter\nafter seeing a task is a weakness that does not hold in practice, in algorithms with predictions the\ncorresponding quantity—the feedback listed in Table 1—is generally known after seeing the instance.\n3 Framework overview and application to bipartite matching\nIn this section we outline the theoretical framework for designing algorithms and proving guarantees\nfor learned predictors. As an illustrative example we will use the Hungarian algorithm for bipartite\nmatching, for which Dinitz et al. [16] demonstrated an instance-dependent upper bound on the\nrunning time using a learned dual vector. Along the way, we will show an improvement to their\nsample complexity bound together with the ﬁrst online results for this setting.\nBipartite matching: For a bipartite graph on nnodes andmedges, min-weight perfect matching\n(MWPM) asks for the perfect matching with the least weight according to edge-costs c2Zm\n\u00150. A\ncommon approach here is the Hungarian algorithm, a convex optimization-based approach for which\nDinitz et al. [16] showed a runtime bound of ~O(mpnminfkx\u0000x\u0003(c)k1;png), where x2Zn\ninitializes the duals in a primal-dual algorithm and x\u0003(c)2Znis dual of the optimal solution; note\nthat the latter is obtained for free after running the Hungarian method.\nStep 1 - Upper bound: The ﬁrst step of our approach is to ﬁnd a suitable function Ut(x)of the\nprediction xthat (a) upper bounds the target algorithm’s cost Ct(x), (b) can be constructed completely\nonce the algorithm terminates, and (c) can be efﬁciently optimized. These qualities allow learning the\npredictor in the second step. The requirements are similar to those of ARUBA for showing results for\nmeta-learning [ 25], although there the quantity being upper-bounded was regret, not algorithmic cost.\n3\n\nAlgorithm 1: Generic application of an online learn-\ning algorithm over Xto learn a predictor for a\nmethod AlgorithmWithPrediction that takes ad-\nvice fromXand returns upper bounds Uton its\ncost. The goal of OnlineAlgorithm is low regret\nover the sequence Ut, so that on-average Ctis upper\nbounded by the smallest possible average of Ut, up\nto some error decreasing in T. For speciﬁc instanti-\nations of algorithms and feedback see Table 1.\ninitialize x12X forOnlineAlgorithm\nforinstancet= 1;:::;T do\nobtain instance It\nrunAlgorithmWithPrediction( It;xt)\nsuffer costCt(xt)\u0014Ut(xt)\nget feedback to construct upper bound Ut\nxt+1 OnlineAlgorithm( fUtgt\ns=1;x1)\nFigure 1: Bounds f(c.f. Lemma 4.1) for\nthree different nand\rDon the expected\nlargest number of mistakes in any \rD-\ninterval as a function of the maximum\nexpected number Us(p)in any interval.\nMany guarantees for algorithms with predictions are already amenable to being optimized, although\nwe will see that they can require some massaging in order to be useful. In many cases the guarantee is\na distance metric between the prediction xand some instance-dependent perfect prediction x\u0003, which\nis convex and thus straightforward to learn. This is roughly true of our bipartite matching example,\nalthough taking the minimum of a constant and the distance kx\u0000x\u0003(c)k1between the predicted and\nactual duals makes the problem non-convex. However, we can further upper bound their result by\n~O(mpnkx\u0000x\u0003(c)k1); note that Dinitz et al. [16] also optimize this quantity, not the tighter upper\nbound with the minimum. While this might seem to be enough for step one, Dinitz et al. [16] also\nrequire the prediction xto be integral, which is difﬁcult to combine with standard online procedures.\nIn order to get around this issue, we show that rounding any nonnegative real vector to the closest\ninteger vector incurs only a constant multiplicative loss in terms of the `1-distance.\nClaim 3.1. Given any vectors x2Znandy2Rn, let~ y2Znbe the vector whose elements are\nthose of yrounded to the nearest integer. Then kx\u0000~ yk1\u00142kx\u0000yk1.\nProof. LetS\u001a[n]be the set of indices i2[n]for which x[i]\u0015y[i]() ~ y[i]=dy[i]e. For\ni2[n]nSwe havejx[i]\u0000y[i]j\u00151=2\u0015j~ y[i]\u0000y[i]jso it follows by the triangle inequality that\nkx\u0000~ yk1=X\ni2Sjx[i]\u0000~ y[i]j+X\ni2[n]nSjx[i]\u0000~ y[i]j\u0014X\ni2Sjx[i]\u0000y[i]j+X\ni2[n]nSjx[i]\u0000y[i]j+jy[i]\u0000~ y[i]j\n\u0014X\ni2Sjx[i]\u0000y[i]j+ 2X\ni2[n]nSjx[i]\u0000y[i]j\u00142kx\u0000yk1\nCombining this projection with the convex relaxation above and the result of Dinitz et al. [16] shows\nthat for any predictor x2Rnwe have (up to afﬁne transformation) a convex upper bound Ut(x) =\nkx\u0000x\u0003(ct)k1on the runtime of the Hungarian method, as desired. We now move to step two.\nStep 2 - Online learning: Once one has an upper bound Uton the cost, the second component of our\napproach is to apply standard online learning algorithms and results to these upper bounds to obtain\nguarantees for learning predictions. In online learning, on each of a sequence of rounds t= 1;:::;T\nwe predict xt2X and sufferUt(xt)for some adversarially chosen loss function Ut:X7!Rthat we\nthen observe; the goal is to use this information to minimize regretPT\nt=1Ut(xt)\u0000minx2XUt(x),\nwith the usual requirement being that it is sublinear inTand thus decreasing on average over time.\nFor bipartite matching, we can just apply regular projected online (sub)gradient descent (OGD ) to\nlossesUt(x) =kx\u0000x\u0003(ct)k1, i.e. the update rule xt+1 arg minx2X\u000bhrUt(xt);xi+1\n2kxk2\n2for\nappropriate step-size \u000b>0; as shown in Theorem 3.1, this yields sublinear regret via a textbook result.\nThe simplicity here is the point: by relegating as much of the difﬁculty as we can to obtaining an easy-\nto-optimize upper bound in step one, we make the actual learning-theoretic component easy. However,\nas we show in the following sections, it is not always easy to obtain a suitable upper bound, nor is\nit always obvious what online learning algorithm to apply, e.g. if the upper bounds are non-convex.\n4\n\nOur use of online learning is motivated by three factors: (1) doing well on non-i.i.d. instances is impor-\ntant in practical applications, e.g. in job scheduling where resource demand changes over time; (2) its\nextensive suite of algorithms lets us use different methods to tailor the approach to speciﬁc settings and\nobtain better bounds, as we exemplify via our use of exponentiated gradient over the simplex geometry\nin Section 4 and KT-OCO over unbounded Euclidean space in Section 5; (3) the existence of classic\nonline-to-batch procedures for converting regret into sample complexity guarantees [ 12], i.e. bounds\non the number of samples needed to obtain an \"-suboptimal predictor w.p. \u00151\u0000\u000e. While online-to-\nbatch conversion can be suboptimal [ 20], as we show in Theorems 3.1, B.2, and D.1 its application\nto various graph algorithms with predictions problems improves upon existing sample complexity\nresults. For completeness, we formalize online-to-batch conversion as Lemma A.1 in the Appendix.\nWe now show how to apply the second online learning step to bipartite matching by improving upon\nthe result of Dinitz et al. [16] in Theorem 3.1; the improvement is the entirely new regret bound against\nadversarial cost vectors and a ~O(n)lower sample complexity. Note how the proof needs only their\nexisting algorithmic contribution, Claim 3.1, and some standard tools in online convex optimization.\nTheorem 3.1. Suppose we have a ﬁxed bipartite graph with n\u00153vertices and m\u00151edges.\n1.For any cost vector c2Zm\n\u00150and any dual vector x2Rnthere exists an algorithm for MWPM\nthat runs in time\n~O\u0000\nmpnmin\b\nU(x);pn\t\u0001\n\u0014~O\u0000\nmpnU(x)\u0001\nforU(x) =kx\u0000x\u0003(c)k1, where x\u0003(c)the optimal dual vector associated with c.\n2.There exists a poly-time algorithm s.t. for any \u000e;\"> 0and distributionDover integer m-vectors\nwith`1-norm\u0014Cit takesO\u0010\u0000Cn\n\"\u00012log1\n\u000e\u0011\nsamples fromDand returns ^ xs.t. w.p.\u00151\u0000\u000e:\nEc\u0018Dk^ x\u0000x\u0003(c)k1\u0014min\nkxk1\u0014CEc\u0018Dkx\u0000x\u0003(c)k1+\"\n3.Letc1;:::;cT2Zm\n\u00150be an adversarial sequence of m-vectors with `1-norm\u0014C. Then OGD\nwith appropriate step-size has regret\nmax\nkxk1\u0014CTX\nt=1kxt\u0000x\u0003(ct)k1\u0000kx\u0000x\u0003(ct)k1\u0014Cnpp\n2T\nProof. The ﬁrst result follows by combining Dinitz et al. [16, Theorem 13 ]with Claim 3.1. For the\nthird result, let xtbe the sequence generated by running OGD [ 46] with step size C=p\n2Ton the losses\nUt(x) =kx\u0000x\u0003(ct)k1over domain [\u0000C;C]n. Since these losses arepn-Lipschitz and the duals\nareCpn-bounded in Euclidean norm the regret guarantee follows from Shalev-Shwartz [44, Corol-\nlary 2.7 ]. For the second result, we apply standard online-to-batch conversion to the third result, i.e.\nwe drawT= \n\u0010\u0000Cn\n\"\u00012log1\n\u000e\u0011\nsamples ct, run OGD as above on the resulting losses Ut, and set ^ x=\n1\nTPT\nt=1xtto be the average of the resulting predictions xt. The result follows by Lemma A.1.\nThis concludes an overview of our two-step approach for obtaining learning guarantees for algorithms\nwith predictions. To summarize, we propose to (1) obtain simple-to-optimize upper bounds Ut(x)\non the cost of the target algorithm on instance tas a function of prediction xand (2) optimize\nUt(x)using online learning. While conceptually simple, even in this illustrative example it already\nimproves upon past work; in the sequel we demonstrate further results that this approach makes\npossible. Note that, like Dinitz et al. [16], we are also able to generalize Theorem 3.1 to b-matchings,\nwhich we do in Appendix B; another advantage of our approach is that it lets us prove online\nand statistical learning in the case where the demand vector bvaries across instances rather than\nstaying ﬁxed as in Dinitz et al. [16]. Finally, in Appendix D we also improve upon the more recent\nlearning-theoretic results of Chen et al. [13] for related graph algorithms with predictions problems.\n4 Predicting requests for page migration\nEquipped with our two-step approach for deriving guarantees for learning predictors, we investigate\nseveral more important problems in combinatorial optimization, starting with the page migration\nproblem. Our results demonstrate that even for learning such simple predictors there are interesting\ntechnical challenges in deriving a learnable upper bound. Nevertheless, once this is accomplished\nthe second step of our approach is again straightforward.\n5\n\nPage migration: Consider a server that sees a sequence of requests s[1];:::;s [n]from metric space\n(K;d)and at each timestep decides whether to change its state a[i]2K at costDd(a[i\u00001];a[i])for\nsomeD> 1; it then suffers a further cost d(a[i];s[i]). The online page migration (OPM) problem is\nthen to minimize the cost to the server. Recently, Indyk et al. [22] studied a setting where we are given\na sequence of predicted points ^s[1];:::; ^s[n]2K to aid the page migration algorithm. They show that\nif there exists \r;q2(0;1)s.t.\rD2[n]and for any i2[n]we havePi+\rD\u00001\nj=i1s[j]6=^s[j]\u0014q\rD\nthen there exists an algorithm with competitive ratio (1 +\r)(1 +O(q))w.r.t. to the ofﬂine optimal.\nThis algorithm depends on \rbut notq, so we study the setting where \ris ﬁxed.\nDeriving an upper bound: As in the previous section, the predictions are discrete, so to use our\napproach we must convert it into a continuous problem. As we have ﬁxed \r, the competitive ratio is\nan afﬁne function of the following upper bound on q:\nQ(^s;s) =1\n\rDmax\ni2[n\u0000\rD+1]i+\rD\u00001X\nj=i1^s[j]6=s[j]\nWe assume that the set of points Kis ﬁnite with indexing k= 1;:::;jKjand use this to introduce our\ncontinuous relaxation, a natural randomized approach converting the problem of learning a prediction\nintonexperts problems on jKjexperts. For each j2[n]we deﬁne a probability vector p[j]24jKj\ngoverning the categorical r.v. ^s[j], i.e.Prf^s[j]=kg=p[j;k]8k2K. Under these distributions the\nexpected competitive ratio will be (1 +\r)(1 +O(E^s\u0018pQ(^s;s))), forpthe product distribution of\nthe vectors pj. Note that forcing each pjto be a one-hot vector recovers the original approach with\nno loss, so optimizing E^s\u0018pQ(^s;s)overp24n\njKjwould ﬁnd a predictor that ﬁts the original result.\nHowever, E^s\u0018pQ(^s;s)is not convex in p. The simplest relaxation is to replace the maximum by\nsummation, but this leads to a worst-case bound of O\u0010\nn\n\rD\u0011\n. We instead bound E^s\u0018pQ(^s;s)—and\nthus also the expected competitive ratio—by a function of the following maximum over expectations:\nUs(p) = max\ni2[n\u0000\rD+1]E^s\u0018pi+\rD\u00001X\nj=i1^s[j]6=s[j]= max\ni2[n\u0000\rD+1]i+\rD\u00001X\nj=i1\u0000hs[j];p[j]i\nwhere s[j;k]= 1s[j]=k8k2K, i.e.s[j]encodes the location in Kof thejth request. As a maximum\novern\u0000\rD+ 1convex functions this objective is also convex. Note also that if Us(p)is zero—i.e.\nthe probability vectors are one-hot and perfect—then E^s\u0018pQ(^s;s)\u0015qwill also be zero. In fact, qis\nupper-bounded by a monotonically increasing function of Us(p)that is zero at the origin, but as this\nfunction is concave and non-Lipschitz (c.f. Figure 1) we incur an additive O\u0010\nlog(n\u0000\rD+1)\n\rD\u0011\nloss to\nobtain an online-learnable upper bound. This is formalized in the following result (proof in A.1).\nLemma 4.1. There exist constants a < e;b < 2=eand a monotonically increasing function f:\n[0;1)7![0;1)s.t.f(0) = 0 and\nE^s\u0018pQ(^s;s)\u0014f(Us(p))\n\rD\u0014aUs(p) +blog(n\u0000\rD+ 1)\n\rD\nWe now have an convex bound on the competitive ratio for the OPM algorithm of Indyk et al. [22].\nFor both this and bipartite matching we resorted to a relaxation of a discrete problem. However,\nwhereas before we only incurred a multiplicative loss (c.f. Claim 3.1), here we have an additive\nloss that makes the bound meaningful only for \rD\u001dlogn. However, as the method we propose\noptimizesUs(p), which bounds qwith no additive error via the function fin Lemma 4.1, in-practice\nwe may expect it to help minimize qin all regimes. Note that the non-Lipschitzness near zero that\nprevents using ffor formal regret guarantees comes from the poor tail behavior of Poisson-like\nrandom variables with small means, which we do not expect can be signiﬁcantly improved.\nLearning guarantees: Having established an upper bound, in Theorem 4.2 we again show how\na learning-theoretic result follows from standard online learning. This time, instead of OGD\nwe run exponentiated (sub)gradient (EG) [44], a classic method for learning from experts, on\neach ofnsimplices to learn the probabilities p[j]8j2[n]. The multiplicative update xt+1/\nxt\fexp(\u0000\u000brUt(xt))of EG is notable for yielding regret logarithmic in the size jKjof the simplices,\nwhich is important for large metric spaces. Note that as the relaxation is randomized, our algorithms\noutput a dense probability vector; to obtain a prediction for OPM we sample ^st[j]\u0018p[j]8j2[n].\n6\n\nTheorem 4.2. Let(K;d)be a ﬁnite metric space.\n1.For any request sequence sand any set of probability vectors p24n\njKjthere exists an algorithm\nfor OPM with expected competitive ratio\n(1 +\r)\u0012\n1 +O\u0012Us(p) + log(n\u0000\rD+ 1)\n\rD\u0013\u0013\n2.There exits a poly-time algorithm s.t. for any \u000e;\"> 0and distributionDover request sequences\ns2Knit takesO\u0012\u0010\n\rD\n\"\u00112\u0000\nn2logjKj+ log1\n\u000e\u0001\u0013\nsamples fromDand returns ^ ps.t. w.p.\u00151\u0000\u000e:\nEs\u0018DUs(^ p)\u0014min\np24n\njKjEs\u0018DUs(p) +\"\n3.Lets1;:::;sTbe an adversarial sequence of request sequences. Then updating the distribution\npt[j]over4jKjat each timestep j2[n]using EG with appropriate step-size has regret\nmax\np24n\njKjTX\nt=1Ust(pt)\u0000Ust(p)\u0014\rDnp\n2TlogjKj\nProof. The ﬁrst result follows by combining Indyk et al. [22, Theorem 1 ]with Lemma 4.1. For the\nthird let ptbe generated by running nexponentiated gradient algorithms with step-sizeq\nlogjKj\n2\r2D2Ton\nlossesUst(p)over4n\njKj. Since these are \rD-Lipschitz and the maximum entropy is logjKj, the regret\nfollows by [ 44, Theorem 2.15]. For the second result, apply standard online-to-batch conversion to the\nthird, i.e. draw T= \n\u0012\u0010\n\rD\n\"\u00112\u0000\nn2logjKj+ log1\n\u000e\u0001\u0013\nsamples st, run EG onUst(p)as above, and\nset^ p=1\nTPT\nt=1ptto be the average of the resulting actions. The result follows by Lemma A.1.\nAs before, this result ﬁrst shows how the quantity of interest—here the competitive ratio—is upper-\nbounded by an afﬁne function of some quality measure Us(p), for which we then provide regret and\nstatistical guarantees using online learning. The difﬁculty deriving a suitable bound exempliﬁes the\ntechnical challenges that arise in learning predictors, and may also be encountered in other sequence\nprediction problems such as TCP [ 10]. Nevertheless, our approach does yield an online procedure that\nincurs onlyO(logn\n\rD)additive error over Indyk et al. [22] in the case of a perfect predictor and, unlike\ntheir work, we provide an algorithm for learning the predictor itself. In Appendix C.2 we also show\nan auto-regressive extension which does notrequire learning a distribution for each timestep j2[n].\n5 Learning linear predictors with instance-feature inputs\nSo far we have considered only ﬁxed predictors, either optima-in-hindsight in the online setting or\na population risk minimizers for i.i.d. data. Actual instances can vary signiﬁcantly and so a ﬁxed pre-\ndictor may not be very good, e.g. in the example of querying a sorted array it means always returning\nthe same index. In the online setting one can consider methods that adapt to dynamic comparators [ 46,\n23,37], which are also applicable to our upper bounds; however, these still need measures such as\nthe comparator path-length to be small, which may be more reasonable in some cases but not all.\nWe instead study the setting where all instances come with instance-speciﬁc features, a natural and\npractical assumption [ 27,30] that encompasses numerical representations of the instance itself—e.g.\nbits representing a query or a graph—or other information such as weather or day of the week. These\nare passed to functions—e.g. linear predictors, neural nets, or trees—whose parameters can be learned\nfrom data. We study linear predictors, which are often amenable to similar analyses as above since the\ncomposition of a convex and afﬁne function is convex. For example, it is straightforward to extend the\nmatching results to learning linear predictors of duals. OPM is more challenging because the outputs\nmust lie in the simplex, which can be solved by learning rectangular stochastic matrices. Both sets of\nresults are shown in Appendix C. Notably, for page migration our guarantees cover the auto-regressive\nsetting where the server probabilities are determined by a ﬁxed linear transform of past states.\nOur main example will be online job scheduling via minimizing the fractional makespan [ 30], where\nwe must assign each in a sequence of variable-sized jobs to one of mmachines. Lattanzi et al. [30] pro-\nvide an algorithm that uses predictions ^ w2Rm\n>0of “good” machine weights w2Rm\n>0to assign jobs\n7\n\nbased on how well ^ wcorresponds to machine demand; the method has a performance guarantee of\nO(log minfmaxi^ w[i]\nw[i];mg). They also discuss learning linear and other predictors, but without guar-\nantees. We study linear prediction of the logarithm of the machine weights, which makes the problem\nconvex, and assume features lie in the f-dimensional simplex. For simplicity we only consider learn-\ning the linear transform from features to predictors and not the intercept, as the former subsumes the\nlatter. For the online result, we use KT-OCO [38, Algorithm 1], a parameter-free subgradient method\nwith update xt+1 1+Pt\ns=1hgs;xsi\nt+1Pt\ns=1gsforgs=rUs(xs); it allows us to not assume any\nbound on the machine weights and thus to compete with the optimal linear predictor in all of Rm\u0002f.\nTheorem 5.1. Consider online restricted assignment with m\u00151machines [30, Section 2.1].\n1.For predicted logits x2Rmthere is an algorithm whose fractional makespan has competitive ratio\nO(minfkx\u0000logwk1;logmg)\u0014O(U(x))\nforU(x) =kx\u0000logwk1, where w2Rm\n>0are good machine weights [30, Section 3].\n2.There exists a poly-time algorithm s.t. for any \u000e;\" > 0and distributionDover machine\n(weight, feature) pairs (w;f)2Rm\n>0\u00024fs.t.klogwk1\u0014Bthe algorithm takes\nO\u0010\u0000B\n\"\u00012\u0000\nmf+ log1\n\u000e\u0001\u0011\nsamples fromDand returns ^A2Rm\u0002fs.t. w.p.\u00151\u0000\u000e\nE(w;f)\u0018Dk^Af\u0000logwk1\u0014 min\nkAkmax\u0014BE(w;f)\u0018DkAf\u0000logwk1+\"\n3.Let(w1;f1);:::; (wT;fT)2Rm\n>0\u00024fbe an adversarial sequence of (weights, feature) pairs.\nThen for any A2Rm\u0002fKT-OCO has regret\nTX\nt=1kAtft\u0000logwtk1\u0000kAft\u0000logwtk1\u0014kAkFq\nTlog(1 + 24T2kAk2\nF) + 1\nIf we restrict to matrices with entries bounded by Bthen OGD with appropriate step-size has regret\nmax\nkAkmax\u0014BTX\nt=1kAtft\u0000logwtk1\u0000kAft\u0000logwtk1\u0014Bp\n2mfT\nProof. The ﬁrst result follows by substituting maxiexp(x[i])\nw[i]for\u0011in Lattanzi et al. [30, Theorem 3.1 ]\nand upper bounding the maximum by the `1-norm. For the third, since Utis 1-Lipschitz w.r.t. the Eu-\nclidean norm we apply the guarantee for KT-OCO [ 38, Algorithm 1] using \"= 1and the subgradients\nofkAtft\u0000logwtk1as rewards [ 38, Corollary 5]. The result for B-bounded Afollows by applying\nOGD with step-size Bq\nmf\n2ToverkAkmax\u0014B[44, Corollary 2.7]. Finally, the second result follows\nby applying online-to-batch conversion to the latter result, i.e. draw T= \n\u0010\u0000B\n\"\u00012\u0000\nmf+ log1\n\u000e\u0001\u0011\nsamples (wt;ft), run OGD on the resulting losses kAft\u0000logwtk1as above, and set ^A=\n1\nTPT\nt=1Atto be the average of the resulting actions At. The result follows by Lemma A.1.\nThis guarantee is the ﬁrst we are aware of for learning non-static predictors in the algorithms with\npredictions literature. It demonstrates both how to extend ﬁxed predictor results to learning linear\npredictors—note that the former is recovered by having ft=118t—and how to handle unbounded\npredictor domains. The ability to provide such guarantees is another advantage of our approach.\n6 Tuning robustness-consistency trade-offs for scheduling and ski-rental\nWe turn to tuning robustness-consistency trade-offs, introduced in Lykouris and Vassilvitskii [34].\nThis trade-off captures the tension between following the predictions when they are good (consistency)\nand doing not much worse than the worst-case guarantee in either case (robustness). In many cases,\nthis trade-off can be made explicit, by a parameter \u00152[0;1]. The setting of \u0015is crucial, yet previous\nwork left the decision to the end-user. Here we show that it is often eminently learnable in an online\nsetting. We then demonstrate how to accomplish a much harder task—tuning \u0015at the same time as\nlearning to predict—on two related but technically very different variants of the ski-rental problem.\nThis meta-application highlights the applicability of our approach to non-convex upper bounds.\n8\n\nRobustness-consistency trade-offs: Most problems studied in online algorithms with predictions\nusually have existing worst-case guarantees on the competitive ratio, i.e. a constant \r\u00151on how\nmuch worse (multiplicatively) a learning-free algorithm does relative to the ofﬂine optimal cost OPT t\non instance t. While the goal of algorithms with predictions is to use data to do better than this\nworst-case bound, an imperfect prediction may lead to much worse performance. As a result, most\nguarantees strive to upper bound the cost of an algorithm with prediction xon an instance tas follows:\nCt(x;\u0015)\u0014minff(\u0015)ut(x);gt(\u0015)g\nHereutis some measure of the quality of xon instance t,\u00152[0;1]is a parameter, fis a mono-\ntonically increasing function that ideally satisﬁes f(0) = 1 , andgtis a monotonically decreasing\nfunction that ideally satisﬁes gt(1) =\rOPTt. A very common structure is f(\u0015) = 1=(1\u0000\u0015)and\ngt(\u0015) =\r\n\u0015OPTt. For example, consider job scheduling with predictions, a setting where we are\ngivennjobs and their predicted runtimes with total absolute error \u0011and must minimize the sum\nof their completion times when running on a single server with pre-emption. Here Kumar et al.\n[29, Theorem 3.3 ]showed that a preferential round-robin algorithm has competitive ratio at most\nminn\n1+2\u0011=n\n1\u0000\u0015;2\n\u0015o\n. Thus if we know the prediction is perfect we can set \u0015= 0 and obtain the\noptimal cost (consistency); on the other hand, if we know the prediction is poor we can set \u0015= 1\nand get the (tight) worst-case guarantee of two (robustness).\nOf course in-practice we often do not know how good a prediction is on a speciﬁc instance t; we thus\nwould like to learn to set \u0015, i.e. to learn how trustworthy our prediction is. As a ﬁrst step, we can\nconsider doing so when we are given a prediction for each instance and thus only need to optimize over\n\u0015. For example, the just-discussed problem of job scheduling has competitive ratio upper-bounded by\nUt(\u0015) = minn\n1+2\u0011t=nt\n1\u0000\u0015;2\n\u0015o\nforntand\u0011tthe number of jobs and the prediction quality, respectively,\non instancet. Assuming a bound Bon the average error makes UtLipschitz, so we can apply the\nexponentially weighted average forecaster [28, Algorithm 1], also known as the exponential\nforecaster . This algorithm, whose action at each time t+ 1is to sample from the distribution with\ndensity\u001at+1(\u0001)/\u001a1(\u0001) exp(\u0000\u000bPt\ns=1Us(\u0001)), has the following regret guarantee (proof in A.2):\nCorollary 6.1. For the competitive ratio upper bounds Utof the job scheduling problem with average\nprediction error \u0011=ntat mostBthe exponential forecaster with appropriate step-size has expected\nregret\nmax\n\u00152[0;1]ETX\nt=1Ut(\u0015t)\u0000Ut(\u0015)\u00149B \n1 +r\nT\n2logT!\nThus a standard learning method produces a sequence \u0015tthat performs as well as the best \u0015asymp-\ntotically. We next consider the more difﬁcult problem of simultaneously tuning \u0015learning to predict.\nSki-rental: We instantiate this challenge on ski-rental, in which each task tis a ski season with an\nunknown number of days nt2Z\u00152; to ski each day, we must either buy skis at price btor rent each\nday for the price of one. The optimal ofﬂine behavior is to buy iff bt<nt, and the best algorithm has\nworst-case competitive ratio e=(e\u00001). Kumar et al. [29] and Bamas et al. [10, Theorem 2 ]further\nderive an algorithm with the following robustness-consistency trade-off between blindly following a\npredictionxand incurring cost ut(x) =bt1x>bt+nt1x\u0014btor going with the worst-case guarantee:\nUt(x;\u0015) =minf\u0015ut(x);bt;ntg\n1\u0000et(\u0000\u0015);et(z) = (1 + 1=bt)btz\nAssuming a bound of N\u00152on the number of days and B > 0on the buy price implies that Utis\nbounded and Lipschitz w.r.t. \u0015. We can thus run exponentiated gradient on the functions Utto learn\na categorical distribution over the product set [N]\u0002f\u000e=2;:::; 1\u0000\u000e=2gfor some\u000es.t.1=\u000e2Z\u00152.\nThis yields the following bound on the expected regret (proof in A.3).\nCorollary 6.2. For the competitive ratio upper bounds Utof the discrete ski-rental problem the\nrandomized exponentiated gradient algorithm with an appropriate step-size has expected regret\nmax\nx2[N];\u00152(0;1]ETX\nt=1Ut(xt;\u0015t)\u0000Ut(x;\u0015)\u00146Npp\nTlog(BNT )\nThus via an appropriate discretization the sequence of predictions (xt;\u0015t)does as well as the joint\noptimum on this problem. However, we can also look at a case where we are not able to just discretize\n9\n\nto get low regret. In particular, we consider the continuous ski-rental problem, where each day\nnt>1is a real number, and study how to pick thresholds xafter which to buy skis, which has cost\nut(x) =nt1nt\u0014x+ (bt+x)1nt>x. Note thatx= 0andx=Nrecovers the previous setting where\nour decision was to buy or not at the beginning. For this setting, Diakonikolas et al. [15] adapt an\nalgorithm of Mahdian et al. [35] to bound the cost as follows:\nCt(x;\u0015)\u0014Ut(x;\u0015) = min\u001aut(x)\n1\u0000\u0015;eminfnt;btg\n(e\u00001)\u0015\u001b\nWhile the bound is simpler as a function of \u0015, it is discontinuous in xbecauseutis piecewise-\nLipschitz. Since one cannot even attain sublinear regret on adversarially chosen threshold functions,\nwe must make an assumption on the data. In particular, we will assume the days are dispersed :\nDeﬁnition 6.3. A set of (possibly random) points n1;:::;nT2Rare\f-dispersed if8\"\u0015T\u0000\f\nthe expected number in any \"-ball is ~O(\"T), i.e.Emaxx2[0;N]j[x\u0006\"]\\fn1;:::;nTgj=~O(\"T).\nDispersion encodes the stipulation that the days, and thus the discontinuities of ut(x;\u0015), are not\ntoo concentrated. In the i.i.d. setting, a simple condition that leads to dispersion with \f= 1=2\nis the assumption that the points are drawn from a \u0014-bounded distribution [ 6, Lemma 1]. Notably\nthis is a strictly weaker assumption than the log-concave requirement of Diakonikolas et al. [15]\nthat they used to show statistical learning results for ski-rental. Having stipulated that the ski-days\nare\f-dispersed, we can show that it implies dispersion of the loss functions [ 6] and thus obtain the\nfollowing guarantee for the exponential forecaster applied to Ut(x;\u0015)(proof in A.4):\nCorollary 6.4. For cost upper bounds Utof the continuous ski-rental problem the exponential\nforecaster with an appropriate step-size has expected regret\nmax\nx2[0;N];\u00152(0;1]ETX\nt=1Ut(xt;\u0015t)\u0000Ut(x;\u0015)\u0014~O\u0010p\nTlog(NT) + (N+B)2T1\u0000\f\u0011\nThus in two mathematically quite different settings of ski-rental we can directly apply online learning\nto existing bounds to not only learn online the best action for ski-rental, but to at the same time learn\nhow trustworthy the best action is via tuning the robustness-consistency trade-off.\n7 Conclusion and future work\nThe ﬁeld of algorithms with predictions has been successful in circumventing worst case lower\nbounds and showing how simple predictions can improve algorithm performance. However, except\nfor a few problem-speciﬁc approaches, the question of how to predict has largely been missing\nfrom the discussion. In this work we presented the ﬁrst general framework for efﬁciently learning\nuseful predictions and applied it to a diverse set of previously studied problems, giving the ﬁrst low\nregret learning algorithms, reducing sample complexity bounds, and showing how to learn the best\nconsistency-robustness trade-off. One current limitation is the lack of more general-case guarantees\nforsimultaneously tuning robustness-consistency and learning the predictor, which we only show for\nski-rental. There are also several other avenues for future work. The ﬁrst is to build on our results\nand provide learning guarantees for other problems where the algorithmic question of how to use\npredictions is already addressed. Another is to try to improve known bounds by solving the problems\nholistically: developing easy-to-learn parameters in concert with developing algorithms that can use\nthem. Finally, there is the direction of identifying hard problems: what are the instances where no\nreasonable prediction can help improve an algorithm’s performance?\nAcknowledgments\nWe thank Yilin Yan, Alexander Smola, Shinsaku Sakaue, and Taihei Oki for helpful discussion.\nThis material is based on work supported in part by the National Science Foundation under grants\nCCF-1535967, CCF-1910321, IIS-1618714, IIS-1705121, IIS-1838017, IIS-1901403, IIS-2046613,\nand SES-1919453; the Defense Advanced Research Projects Agency under cooperative agreements\nHR00112020003 and FA875017C0141; a Simons Investigator Award; an AWS Machine Learning\nResearch Award; an Amazon Research Award; a Bloomberg Research Grant; a Microsoft Research\nFaculty Fellowship; an Amazon Web Services Award; a Facebook Faculty Research Award; funding\nfrom Booz Allen Hamilton Inc.; a Block Center Grant; and a Facebook PhD Fellowship. Any\nopinions, ﬁndings and conclusions or recommendations expressed in this material are those of the\nauthor(s) and do not necessarily reﬂect the views of any of these funding agencies.\n10\n\nReferences\n[1]Keerti Anand, Rong Ge, and Debmalya Panigrahi. Customizing ML predictions for online\nalgorithms. In Proceedings of the 37th International Conference on Machine Learning , 2020.\n[2]Keerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. A regression approach to\nlearning-augmented online algorithms. In Advances in Neural Information Processing Systems ,\n2021.\n[3]Maria-Florina Balcan. Data-driven algorithm design. In Tim Roughgarden, editor, Beyond the\nWorst-Case Analysis of Algorithms . Cambridge University Press, Cambridge, UK, 2021.\n[4]Maria-Florina Balcan and Avrim Blum. Approximation algorithms and online mechanisms for\nitem pricing. Theory of Computing , 3:179–195, 2007.\n[5]Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch.\nInProceedings of the 35th International Conference on Machine Learning , 2018.\n[6]Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm\ndesign, online learning, and private optimization. In 2018 IEEE 59th Annual Symposium on\nFoundations of Computer Science (FOCS) , pages 603–614, 2018.\n[7]Maria-Florina Balcan, Travis Dick, and Wesley Pegden. Semi-bandit optimization in the\ndispersed setting. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence ,\n2020.\n[8]Maria-Florina Balcan, Dan DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm, and\nEllen Vitercik. How much data is sufﬁcient to learn high-performing algorithms? Generalization\nguarantees for data-driven algorithm design. In Proceedings of the 53rd Annual ACM SIGACT\nSymposium on Theory of Computingg , 2021.\n[9]Maria-Florina Balcan, Mikhail Khodak, Dravyansh Sharma, and Ameet Talwalkar. Learning-to-\nlearn non-convex piecewise-Lipschitz functions. In Advances in Neural Information Processing\nSystems , 2021.\n[10] Etienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning\naugmented algorithms. In Advances in Neural Information Processing Systems , 2020.\n[11] Peter Bartlett, Piotr Indyk, and Tal Wagner. Generalization bounds for data-driven numerical\nlinear algebra. In Proceedings of the 35th Annual Conference on Learning Theory , 2022.\n[12] Nicolò Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of\non-line learning algorithms. IEEE Transactions on Information Theory , 50(9):2050–2057, 2004.\n[13] Justin Y . Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental graph\nalgorithms via learned predictions. In Proceedings of the 40th International Conference on\nMachine Learning , 2022.\n[14] Ofer Dekel, Arthur Flajolet, Nika Haghtalab, and Patrick Jaillet. Online learning with a hint. In\nAdvances in Neural Information Processing Systems , 2017.\n[15] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Ali Vakilian, and Nikos Zariﬁs. Learning\nonline algorithms with distributional advice. In Proceedings of the 38th International Conference\non Machine Learning , 2021.\n[16] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii.\nFaster matchings via learned duals. In Advances in Neural Information Processing Systems ,\n2021.\n[17] Elbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the “learning” into learning-\naugmented algorithms for frequency estimation. In Proceedings of the 38th International\nConference on Machine Learning , 2021.\n11\n\n[18] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-\ntation of deep networks. In Proceedings of the 34th International Conference on Machine\nLearning , 2017.\n[19] Rishi Gupta and Timothy Roughgarden. A PAC approach to application-speciﬁc algorithm\nselection. SIAM Journal on Computing , 46(3):992–1017, 2017.\n[20] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: Optimal algorithms for\nstochastic strongly-convex optimization. Journal of Machine Learning Research , 15:2489–2512,\n2014.\n[21] David P. Helmbold and Manfred K. Warmuth. Learning permutations with exponential weights.\nJournal of Machine Learning Research , 10:1705–1736, 2009.\n[22] Piotr Indyk, Frederik Mallmann-Trenn, Slobodan Mitrovi ´c, and Ronitt Rubinfeld. Online page\nmigration with ML advice. In Proceedings of the 25th International Conference on Artiﬁcial\nIntelligence and Statistics , 2022.\n[23] Ali Jadbabaie, Alexander Rakhlin, and Shahin Shahrampour. Online optimization: Competing\nwith dynamic comparators. In Proceedings of the 18th International Conference on Artiﬁcial\nIntelligence and Statistics , 2015.\n[24] Zhihao Jiang, Debmalya Panigrahi, and Kevin Sun. Online algorithms for weighted paging with\npredictions. In Proceedings of the 47th International Colloquium on Automata, Languages, and\nProgramming , 2020.\n[25] Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-\nlearning methods. In Advances in Neural Information Processing Systems , 2019.\n[26] Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina Balcan, Virginia Smith, and\nAmeet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to\nweight-sharing. In Advances in Neural Information Processing Systems , 2021.\n[27] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data ,\n2018.\n[28] Walid Krichene, Maximilian Balandat, Claire Tomlin, and Alexandre Bayen. The hedge\nalgorithm on a continuum. In Proceedings of the 32nd International Conference on Machine\nLearning , 2015.\n[29] Ravi Kumar, Manish Purohit, and Zoya Svitkina. Improving online algorithms via ML predic-\ntions. In Advances in Neural Information Processing Systems , 2018.\n[30] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online schedul-\ning via learned weights. In Proceedings of the 2020 ACM-SIAM Symposium on Discrete\nAlgorithms , 2020.\n[31] Jeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially private\nmeta-learning. In Proceedings of the 8th International Conference on Learning Representations ,\n2020.\n[32] Sen Lin, Mehmet Dedeoglu, and Junshan Zhang. Accelerating distributed online meta-learning\nvia multi-agent collaboration under limited communication. In Proceedings of the Twenty-\nsecond International Symposium on Theory, Algorithmic Foundations, and Protocol Design for\nMobile Networks and Mobile Computing , 2021.\n[33] Alexander Lindermayr and Nicole Megow. Permutation predictions for non-clairvoyant schedul-\ning. In Proceedings of the 34th ACM Symposium on Parallelism in Algorithms and Architectures ,\n2022.\n[34] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice.\nJournal of the ACM , 68(4), 2021.\n12\n\n[35] Mohammad Mahdian, Hamid Nazerzadeh, and Amin Saberi. Online optimization with uncertain\ninformation. ACM Transactions on Algorithms , 8:1–29, 2012.\n[36] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. In Tim Rough-\ngarden, editor, Beyond the Worst-Case Analysis of Algorithms . Cambridge University Press,\nCambridge, UK, 2021.\n[37] Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. Online opti-\nmization in dynamic environments: Improved regret rates for strongly convex problems. In\nProceedings of the 55th IEEE Conference on Decision and Control , 2016.\n[38] Francesco Orabona and David Pal. Coin betting and parameter-free online learning. In Advances\nin Neural Information Processing Systems , 2016.\n[39] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In\nProceedings of the 26th Annual Conference on Learning Theory , 2013.\n[40] Alexander Rakhlin and Karthik Sridharan. Efﬁcient online multiclass prediction on graphs via\nsurrogate losses. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence\nand Statistics , 2017.\n[41] Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In\nProceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms , 2020.\n[42] Timothy Roughgarden. Beyond Worst-Case Analysis of Algorithms . Cambridge University\nPress, 2020.\n[43] Ziv Scully, Isaac Grosof, and Michael Mitzenmacher. Uniform bounds for scheduling with\njob size estimates. In Proceedings of the 13th Innovations in Theoretical Computer Science\nConference , 2022.\n[44] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends\nin Machine Learning , 4(2):107–194, 2011.\n[45] Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex\nalgorithm usually takes polynomial time. Journal of the ACM , 51(3):385–463, 2004.\n[46] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.\nInProceedings of the 20th International Conference on Machine Learning , 2003.\n13\n\nChecklist\n1. For all authors...\n(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes]\n(c) Did you discuss any potential negative societal impacts of your work? [N/A]\n(d)Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a)Did you state the full set of assumptions of all theoretical results? [Yes] Assumptions\nstated in Sections 3, 4, 6, and the Appendix.\n(b)Did you include complete proofs of all theoretical results? [Yes] Proofs given in\nSections 3, 4, and the Appendix.\n3. If you ran experiments...\n(a)Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [N/A]\n(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [N/A]\n(c)Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [N/A]\n(d)Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [N/A]\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [N/A]\n(b) Did you mention the license of the assets? [N/A]\n(c)Did you include any new assets either in the supplemental material or as a URL? [N/A]\n(d)Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? [N/A]\n(e)Did you discuss whether the data you are using/curating contains personally identiﬁable\ninformation or offensive content? [N/A]\n5. If you used crowdsourcing or conducted research with human subjects...\n(a)Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A]\n(b)Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c)Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\n14\n\nA Proofs of main results\nA.1 Proof of Lemma 4.1\nProof. For eachj2[n]deﬁnepj= 1\u0000hs[j];p[j]i, i.e. the probability that ^sj6=sj, and the r.v.\nXj\u0018Ber(pj). Deﬁne also the r.v. Si=Pi+\rD\u00001\nj=iXj, s.t. we have\n\rDEpq=Epmax\ni2[n\u0000\rD+1]Si=Epmax\ni2[n\u0000\rD+1]i+\rD\u00001X\nj=iXj\nNote thatSiis a Poisson binomial and so has moment-generating function Epexp(tSi) =Qi+\rD\u00001\nj=i(1\u0000pj+pjet). Therefore applying Jensen’s inequality and the union bound yields\nexp\u0012\ntEpmax\ni2[n\u0000\rD+1]Si\u0013\n\u0014Epexp\u0012\ntmax\ni2[n\u0000\rD+1]Si\u0013\n=Epmax\ni2[n\u0000\rD+1]exp (tSi)\n\u0014n\u0000\rD+1X\ni=1Epexp (tSi)\n\u0014(n\u0000\rD+ 1)i\u0003+\rD\u00001Y\nj=i\u0003(1\u0000pj+pjet)\nfor allt>0andi\u00032arg maxi2[n\u0000\rD+1]EpSi. We then have\ntEpmax\ni2[n\u0000\rD+1]Si\u0014log(n\u0000\rD+ 1) +i\u0003+\rD\u00001X\nj=i\u0003log(1\u0000pj+pjet)\n\u0014log(n\u0000\rD+ 1) +i\u0003+\rD\u00001X\nj=i\u0003log exp(pj(et\u00001))\n\u0014log(n\u0000\rD+ 1) +i\u0003+\rD\u00001X\nj=i\u0003pj(et\u00001)\n\u0014log(n\u0000\rD+ 1) + EpSi\u0003(et\u00001)\nDividing by t=W\u0010\nlog(n\u0000\rD+1)\nx\u0011\n+ 1shows thatf(x) =x(exp(t)\u00001)+log(n\u0000\rD+1)\nt\rD, whereW:\n[0;1)7![0;1)is the Lambert W-function. Deﬁne L= log(n\u0000\rD+ 1)=e, so we are interested\nin bounding f(x) =x(exp(W(L=x)+1)\u00001)+eL\nW(L=x)+1. We compute its derivative:\nf0(x) =x(exp(W(L=x) + 1)\u00001)W(L=x)2\u00003x(W(L=x) + 1=3) +xexp(W(L=x) + 1) + 2eL\nx(W(L=x) + 1)3\nand second derivative:\nf00(x) =\u0000W(L=x)\u0000\n(x+eL)W(L=x)2+ 2(2x+eL)W(L=x) +eL\u0001\nx2(W(L=x) + 1)5\nSince the second derivative is always negative, fis a concave function on x\u00150. Thus for!=W(1)\nwe have\nf(x)\u0014min\ny>0f(y) +f0(y)(x\u0000y)\n\u0014L(e=!\u00001 +e)\n!+ 1+L(e=!\u00001)!2\u00003L(!+ 1=3) +Le=! + 2eL\nL(!+ 1)3(x\u0000L)\n=\u0000\ne=!+ 1=(!+ 1)3\u0000(e+ 1)=(!+ 1)\u00001=(!+ 1)2\u0001\nx\n+\u0000\n1=(!+ 1)2+e=(!+ 1)\u00001=(!+ 1)3\u0001\nL\n<ex +2\nelog(n\u0000\rD+ 1)\n15\n\nA.2 Proof of Corollary 6.1\nProof. We have that Ut(\u0015)is bounded above by 3(1 + 2B), its largest gradient is attained at\n2=(3 + 2\u0011t=n)where it is bounded by (3 + 2B)=2. Applying Krichene et al. [28, Corollary 2 ]and\nsimplifying yields the result.\nA.3 Proof of Corollary 6.2\nProof.Ut(x;\u0015)is bounded above by 2Nand its largest gradient is attained at \u0015=minfbt;ntg\nut(x)\u00151\nN\nwith norm bounded byBexp(1=N)\n(exp(1=N)\u00001)2. Let \u0003 =fk\u000egb1=\u000ec\nk=1for some\u000e2(0;1]. Then we run EG on\nthe simplex over [N]\u0002\u0003and with step-size1\n2Nq\nlogN\n\u000e\n2Tto obtain regret compared to the best element\nof[N]\u0002\u0003of2Nq\n2TlogN\n\u000e[44, Theorem 2.15]. Setting \u000e= minn\nN(exp(1=N)\u00001)2\nBexp(1=N)q\n2\nT;1o\nyields\nETX\nt=1Ut(xt;\u0015t)\u00142Np\n2Tlog(Nb1=\u000ec) + min\nx2[N];\u00152\u0003TX\nt=1Ut(x;\u0015)\n\u00142Nr\n2TlogN\n\u000e+Bexp(1=N)\u000eT\n(exp(1=N)\u00001)2+ min\nx2[N];\u00152(0;1]TX\nt=1Ut(x;\u0015)\n\u00142Ns\n2T\u0012\nlog(BT) + max\u001a\nlogN;1\nN\u00002 log\u0012\nexp\u00121\nN\u0013\n\u00001\u0013\u001b\u0013\n+Np\n2T+ min\nx2[N];\u00152(0;1]TX\nt=1Ut(x;\u0015)\n\u00146Np\nTlog(BNT ) + min\nx2[N];\u00152(0;1]TX\nt=1Ut(x;\u0015)\nA.4 Proof of Corollary 6.4\nProof.Ut(x;\u0015)is bounded above by e(N+B), its largest gradient w.r.t. \u0015is attained at \u0015=\neminfnt;btg\n(e\u00001)ut(x)+eminfnt;btg, where it is bounded by\u0010\n2e(N+B)\ne\u00001\u00112\n, and its largest gradient w.r.t. xise(N+\nB). Thus the function is 5e(N+B)2-Lipschitz w.r.t. the Euclidean norm, apart from discontinuities at\nx=nt. Now, note that our assumption that the points n1;:::;nTare\f-dispersed implies exactly that\nthe functions Utare\f-dispersed (c.f. Balcan et al. [9, Deﬁnition 2.1 ]), so the exponentially-weighted\nforecaster attains expected regret ~O\u0010p\nTlog(NT) + (N+B)2T1\u0000\f\u0011\n.\nA.5 Online-to-batch conversion\nLemma A.1. Suppose an online learner has regret bound RTon a sequence of convex losses\n`y1;:::;` yT:X7! [0;B]whose data ytare drawn i.i.d. from some distribution D. Ifx1;:::;xT\nare the actions of the online learner, ^ x=1\nTPT\nt=1xtis their average, and T= \n\u0010\nT\"+B2\n\"2log1\n\u000e\u0011\nforT\"= min 2RT0\u0014\"T0T0, then w.p.\u00151\u0000\u000ewe have Ey\u0018D`y(^ x)\u0014minx2XEy\u0018D`y(x) +\".\nProof. Apply Jensen’s inequality, [12, Proposition 1], the regret bound, and Hoeffding’s bound:\nEy`y(^ x)\u00141\nTTX\nt=1Ey`y(xt)\u00141\nTTX\nt=1`yt(xt)+Brr\n2\nTlog2\n\u000e\u0014min\nx2X1\nTTX\nt=1`yt(x)+RT\nT+Brr\n2\nTlog2\n\u000e\n\u0014min\nx2XEy`y(x)+RT\nT+2Brr\n2\nTlog2\n\u000e(1)\n16\n\nBb-matching\nDeﬁnition B.1. Forb2Rn\n\u00150theb-seminormk\u0001kb;1:Rn7!R\u00150iskxkb;1=Pn\ni=1b[i]jx[i]j.\nClaim B.1. Given any vectors x2Znandy2Rn, let~ y2Znbe the vector whose elements are\nthose of yrounded to the nearest integer. Then for all b2Znwe havekx\u0000~ ykb;1\u00142kx\u0000ykb;1.\nProof. LetS\u001a[n]be the set of indices i2[n]for which x[i]\u0015y[i]() ~ y[i]=dy[i]e. For\ni2[n]nSwe havejx[i]\u0000y[i]j\u00151=2\u0015j~ y[i]\u0000y[i]jso it follows by the triangle inequality that\nkx\u0000~ ykb;1=X\ni2Sb[i]jx[i]\u0000~ y[i]j+X\ni2[n]nSb[i]jx[i]\u0000~ y[i]j\n\u0014X\ni2Sb[i]jx[i]\u0000y[i]j+X\ni2[n]nSb[i](jx[i]\u0000y[i]j+jy[i]\u0000~ y[i]j)\n\u0014X\ni2Sb[i]jx[i]\u0000y[i]j+ 2X\ni2[n]nSb[i]jx[i]\u0000y[i]j\u00142kx\u0000ykb;1\nTheorem B.2. Suppose we have a ﬁxed graph with n\u00153vertices and m\u00151edges.\n1.For any cost vector c2Zm\n\u00150, any demand vector b2Zn\n\u00150, and any dual vector x2Rnthere\nexists an algorithm for minimum weight perfect b-matching that runs in time ~O(mnU (x)), where\nU(x) =kx\u0000x\u0003(c;b)kb;1forx\u0003(c;b)the optimal dual vector associated with candb.\n2.There exists a poly-time algorithm s.t. for any \u000e;\"> 0and any distribution Dover (cost, demand)\nvector pairs in Zm\n\u00150\u0002Zn\n\u00150with respective `1-norms bounded by CandBthe algorithm takes\nO\u0010\u0000CBn\n\"\u00012log1\n\u000e\u0011\nsamples fromDand returns ^ xs.t. w.p.\u00151\u0000\u000e:\nE(c;b)\u0018Dk^ x\u0000x\u0003(c;b)kb;1\u0014min\nkxk1\u0014CE(c;b)\u0018Dkx\u0000x\u0003(c;b)kb;1+\"\n3.Let(c1;b1);:::; (cT;bT)2Zm\n\u00150\u0002Zn\n\u00150be an adversarial sequence of (cost, demand) vector\npairs with`1-norms bounded by CandB, respectively. Then OGD with appropriate step-size\nhas regret\nmax\nkxk1\u0014CTX\nt=1kxt\u0000x\u0003(ct;bt)kbt;1\u0000kx\u0000x\u0003(ct;bt)kbt;1\u0014CBnpp\n2T\nProof. The ﬁrst result follows by Dinitz et al. [16, Theorem 31 ]and Claim B.1. For the third,\nletxtbe the sequence generated by running OGD [ 46] with step sizeC\nBp\n2Ton the losses\nUt(x) =kx\u0000x\u0003(ct;bt)kbt;1over domain [\u0000C;C]n. Since these losses are Bpn-Lipschitz\nand the duals are Cpn-bounded in Euclidean norm the regret guarantee follows from Shalev-Shwartz\n[44, Corollary 2.7 ]. For the second result, apply online-to-batch conversion to the third result, i.e.\ndrawT= \n\u0010\u0000CBn\n\"\u00012log1\n\u000e\u0011\nsamples (ct;bt), run OGD as above on the resulting losses Ut, and\nset^ x=1\nTPT\nt=1xtto be the average of the resulting predictions xt. Applying Lemma A.1 yields\nthe result.\n17\n\nC Learning linear predictors with instance-feature inputs\nComputational instances on which we want to run algorithms with predictions often come with\ninstance-speciﬁc features, e.g. ones derived from text descriptions of the instance or summary statis-\ntics about related graphs or environments [ 27,30]. It is thus natural to learn parameterized functions,\ne.g. linear mappings or neural networks, from these features to predictions. However, there has been\nvery little work, in either the statistical or online setting, showing that such predictions are learnable.\nIn this section we show how our framework naturally handles this setting by exploiting the convexity\nof compositions of convex and afﬁne functions, resulting in the ﬁrst formal guarantees for linear pre-\ndictors for algorithms with predictions. While the ﬁrst application to the matching problem of Dinitz\net al. [16] is a straightforward extension, we also show how to handle more complicated cases, such as\nwhen the output space is constrained to probability simplices as in the page migration problem. Note\nwe assume all feature vectors lie in the f-dimensional simplex; this is generally easy to accomplish\nby normalization. For simplicity we also only consider learning the linear transform from features to\npredictors and not the intercept, as the latter follows from the former by appending an extra dimension\nwith value 1=2to the feature vector and doubling the bound on the norm of the linear transform.\nC.1 b-matching\nOur ﬁrst application for learning mappings from instance features is to the b-matching setting. Note\nthat the learning-theoretic results for the regular bipartite matching setting in Section 3 follow directly\nby setting b=1nfor all instances, and that the learning-theoretic results of Theorems 3.1 and B.2 are\nalso special cases of the following when f=11for all instances. Note that we optimize only over A2\n[\u0000C;C]n\u0002f, but unlike in the f= 1case the optimal Amay be unbounded; to handle that setting,\none could again use an algorithm such as KT-OCO that does not depend on knowing the set size [ 38].\nTheorem C.1. Consider the setting of Theorem B.2.\n1.There exists a poly-time algorithm s.t. for any \u000e;\"> 0and any distribution Dover (cost, demand,\nfeature) vector triples in Zm\n\u00150\u0002Zn\n\u00150\u00024fs.t. the respective `1-norms of the ﬁrst two are\nbounded byCandB, respectively, the algorithm takes O\u0010\u0000CBn\n\"\u00012\u0000\nf2+ log1\n\u000e\u0001\u0011\nsamples from\nDand returns ^A2Rn\u0002fs.t. w.p.\u00151\u0000\u000e:\nE(c;b;f)\u0018Dk^Af\u0000x\u0003(c;b)kb;1\u0014 min\nkAkmax\u0014CE(c;b;f)\u0018DkAx\u0000x\u0003(c;b)kb;1+\"\n2.Let(c1;b1;f1);:::; (cT;bT;fT)2Zm\n\u00150\u0002Zn\n\u00150\u00024fbe an adversarial sequence of (cost, demand,\nfeature) vector triples s.t. the `1-norms of the ﬁrst two are bounded by CandB, respectively.\nThen OGD with appropriate step-size has regret\nmax\nkAkmax\u0014CTX\nt=1kAtft\u0000x\u0003(ct;bt)kbt;1\u0000kAft\u0000x\u0003(ct;bt)kbt;1\u0014CBnfpp\n2T\nProof. For the second result let Atbe generated by running OGD with step-sizeC\nBp\n2Ton the\nlossesUt(Af) =kAf\u0000x\u0003(ct;bt)kbt;1over[\u0000C;C]n\u0002f. Since these are Bpnf-Lipschitz and\nthe duals are Cpnf-bounded in the Euclidean norm, the regret follows from Shalev-Shwartz [44,\nCorollary 2.7 ]. For the ﬁrst result, apply online-to-batch conversion to the second result, i.e. draw\nT= \n\u0010\u0000CBn\n\"\u00012\u0000\nf2+ log1\n\u000e\u0001\u0011\nsamples (ct;bt;ft), run OGD as above on the resulting losses Ut,\nand set ^A=1\nTPT\nt=1Atto be the average of the resulting predictions At. Applying Lemma A.1\nyields the result.\nC.2 Online page migration\nUsing instance features for online page migration is more involved because the output space must be\nconstrained to the product of njKj-dimensional simplices. However, we can solve this by restricting\nto tensors consisting of matrices whose columns sum to one, also known as rectangular stochastic\nmatrices. Note that the learning-theoretic results of Theorem 4.2 are special cases of the following\nwhenf=11for all instances.\n18\n\nTheorem C.2. In the setting of Theorem 4.2 let Sn\u0002jKj\u0002fbe the set of stacks of jKj\u0002fnonnegative\nmatrices whose columns have unit `1-norm.\n1.There exists a poly-time algorithm s.t. for any \u000e;\" > 0and distributionDover re-\nquest sequences sof lengthninKand associated feature vectors f2 4fit takes\nO\u0012\u0010\n\rD\n\"\u00112\u0000\nn2f2logjKj+ log1\n\u000e\u0001\u0013\nsamples fromDand returns ^As.t. w.p.\u00151\u0000\u000e:\nE(s;f)\u0018DUs(^Af)\u0014 min\nA2Sn\u0002jKj\u0002fE(s;f)\u0018DUs(Af) +\"\n2.Let(s1;f1);:::; (sT;fT)be an adversarial sequence of (request sequence, feature) pairs. Then\nupdating the distribution At[j;;k]over4jKjat each (timestep,column) pair (j;k)2[n]\u0002[f]\nusing EG with appropriate step-size has regret\nmax\nA2Sn\u0002jKj\u0002fTX\nt=1Ust(Atft)\u0000Ust(Aft)\u0014\rDnfp\n2TlogjKj\nProof. For the second result let Atbe the sequence generated by running nfEG algorithms with step\nsizeq\nlogjKj\n2\r2D2Ton the losses Ust(Af)overSn\u0002jKj\u0002f. Since these losses are \rD-Lipschitz and the\nmaximum entropy over the simplex is logjKj, the regret guarantee follows from [ 44, Theorem 2.15].\nFor the ﬁrst result, apply standard online-to-batch conversion to the second result, i.e. draw T=\n\n\u0012\u0010\n\rD\n\"\u00112\u0000\nn2f2logjKj+ log1\n\u000e\u0001\u0013\nsamples (st;ft), run EG on the resulting losses Ust(Aft)as\nabove, and set ^A=1\nTPT\nt=1Atto be the average of the resulting actions At. Applying Lemma A.1\nyields the result.\nWe can further also show a result in the perhaps more-natural setting where the linear predictor A\nis the same for each element in the sequence, and maps directly from features to the jKj-simplex.\nNotably, the linear auto-regressive setting, in which we want a linear map from the past ksequence\nelements to a probabilistic prediction of the next one, is covered by this result if we allow the features\nto bekjKj-dimensional concatenations of kone-hotjKj-length vectors.\nTheorem C.3. In the setting of Theorem 4.2 let Sa\u0002bbe the set of a\u0002bnonnegative matrices whose\ncolumns have unit `1-norm.\n1.There exists a poly-time algorithm s.t. for any \u000e;\" > 0and distributionDover re-\nquest sequences sof lengthninKand associated feature sequence FT2Sf\u0002nit takes\nO\u0012\u0010\n\rD\n\"\u00112\u0000\nn2f2logjKj+ log1\n\u000e\u0001\u0013\nsamples fromDand returns ^As.t. w.p.\u00151\u0000\u000e:\nE(s;F)\u0018DUs(F^AT)\u0014min\nA2SjKj\u0002fE(s;F)\u0018DUs(FAT) +\"\n2.Let(s1;F1);:::; (sT;FT)be an adversarial sequence of (request sequence, feature sequence)\npairs. Then updating the distribution At[;k]over4jKjat each column k2[f]has regret\nmax\nA2SjKj\u0002fTX\nt=1Ust(FtAT\nt)\u0000Ust(FtAT)\u0014\rDfp\n2TlogjKj\nProof. For the second result let Atbe the sequence generated by running fEG algorithms with step\nsizeq\nlogjKj\n2\r2D2Ton the losses Ust(FAT)overSjKj\u0002f. Since these losses are \rD-Lipschitz and the\nmaximum entropy over the simplex is logjKj, the regret guarantee follows from [ 44, Theorem 2.15].\nFor the ﬁrst result, apply standard online-to-batch conversion to the second result, i.e. draw T=\n\n\u0012\u0010\n\rD\n\"\u00112\u0000\nf2logjKj+ log1\n\u000e\u0001\u0013\nsamples (st;ft), run EG on the resulting losses Ust(FtAT)as\nabove, and set ^A=1\nTPT\nt=1Atto be the average of the resulting actions At. Applying Lemma A.1\nyields the result.\n19\n\nD Faster graph algorithms with predictions\nIn this section we compare to the results of Chen et al. [13], who analyze several prediction-based\ngraph algorithms, including one with an improved prediction-dependent runtime for the matching\napproach of Dinitz et al. [16] and a prediction-dependent bound for single-source shortest path.\nFrom the learnability perspective, they observe two important error metrics in the analysis of graph\nalgorithms with predictions: the `1-metric of Dinitz et al. [16] measuring the `1-norm between the\nprediction and a ground truth vector such as the dual and the `1-metric measuring the `1-norm\nbetween the same quantities. In the ﬁrst case their setting and results are equivalent to those of Dinitz\net al. [16], so we improve upon this by a factor of O(d), wheredis the dimension of the hint.\nTo analyze the `1case, we start by showing that—as in the `1case—we can round integer vectors\nwith only a multiplicative factor loss:\nClaim D.1. Given any vectors x2Znandy2Rn, let~ y2Znbe the vector whose elements are\nthose of yrounded to the nearest integer. Then we have kx\u0000~ yk1\u00142kx\u0000yk1.\nProof. LetS\u001a[n]be the set of indices i2[n]for which x[i]\u0015y[i]() ~ y[i]=dy[i]e. For\ni2[n]nSwe havejx[i]\u0000y[i]j\u00151=2\u0015j~ y[i]\u0000y[i]jso it follows by the triangle inequality that\nkx\u0000~ yk1= max\u001a\nmax\ni2Sjx[i]\u0000~ y[i]j;max\ni2[n]nSjx[i]\u0000~ y[i]j\u001b\n\u0014max\u001a\nmax\ni2Sjx[i]\u0000y[i]j;max\ni2[n]nSjx[i]\u0000y[i]j+jy[i]\u0000~ y[i]j\u001b\n\u0014max\u001a\nmax\ni2Sjx[i]\u0000y[i]j;2 max\ni2[n]nSjx[i]\u0000y[i]j\u001b\n\u00142kx\u0000yk1\nWe are thus able to also use online convex optimization in this setting and apply the rounded outputs\nto graph algorithms. In particular, we can use regular OGD to improve upon the `1-learnability\nresult of Chen et al. [13] by a factor of O(d2), wheredis the dimension of the prediction:\nTheorem D.1. Consider any graph algorithm with optimal d-dimensional M-bounded predictions\nh(c)associated with every instance c.\n1.There exists a poly-time algorithm s.t. for any \u000e;\"> 0and distributionDover instances it takes\nO\u0010\u0000M\n\"\u00012\u0000\nd+ log1\n\u000e\u0001\u0011\nsamples fromDand returns ^h2Rds.t. w.p.\u00151\u0000\u000e\nEc\u0018Dk^h\u0000h(c)k1\u0014min\nkhk1\u0014MEc\u0018Dkh\u0000h(c)k1+\"\n2.Letc1;:::;cTbe an adversarial sequence of instances. Then OGD with appropriate step-size\nachieves regret\nmax\nkhk1\u0014MTX\nt=1kht\u0000h(ct)k1\u0000kh\u0000h(ct)k1\u0014Mp\n2dT\nProof. The proof is the same as for the last two results of Theorem 5.1 in the special case f= 1.\n20\n\nE Permutation predictions for non-clairvoyant scheduling\nFinally, we discuss the the applicability of our framework to the results in Lindermayr and Megow\n[33], who study how to prioritize among njobs by predicting the best permutation of them under\nweights w2Rn\n\u00150and processing requirements p2Rn\n\u00150that are only known after completion.\nIgnoring robustness-consistency tradeoffs and terms that do not depend on the prediction, they show\nthat in several settings the competitive ratio depends linearly on the following error of an n\u0002n\npermutation matrix X:\nUw;p(X) = Tr( Xw((U\fX)p)T) = Tr(( U\fX)TXwpT)\nwhere U2f0;1gn\u0002nis upper triangular. The above expression is derived from the third equation in\nthe proof of Theorem 4.1 of Lindermayr and Megow [33] for the case of z= 1sample; we construct\nthe matrix form to reason about its online learnability.\nNaively, a sequence of bounded functions of permutations is computationally inefﬁciently learnable\nby using randomized EG over the n!experts corresponding to each permutation:\nTheorem E.1. Consider the setting of Lindermayr and Megow [33] withnjobs withW-bounded\nweights and P-bounded processing times. Let Pn\u0002nbe the set of n\u0002npermutation matrices.\n1.There exists an algorithm that s.t. for any \u000e;\"> 0and distributionDover weights and processing\nrequirements it takes O\u0010\u0000WPn\n\"\u00012\u0000\nnlogn+ log1\n\u000e\u0001\u0011\nsamples fromDand returns a discrete\ndistribution ^ x24n!overPn\u0002nsuch that\nEX\u0018^ xE(w;p)\u0018DUw;p(X)\u0014min\nX2Pn\u0002nE(w;p)\u0018DUw;p(X) +\"\n2.Let(w1;p1);:::; (wT;pT)be an adversarial sequence of job (weight, processing requirement)\npairs. Then running EG with appropriate step-size over 4jPn\u0002njhas regret\nEmax\nX2Pn\u0002nTX\nt=1Uwt;pt(Xt)\u0000Uwt;pt(X)\u0014WPnp\n2nTlogn\nwhere the expectation is over the randomness of the algorithm.\nProof. For the second result let xt24n!be the sequence generated by running EG with step-sizeq\nlog(n!)\n2Tover then!experts corresponding to each element of Pn\u0002n. Then the sequence of permuta-\ntion matrices Xt\u0018xtsampled from this distribution satisﬁes the guarantee on the expected regret [ 44,\nCorollary 2.14] since log(n!)\u0014nlognandUw;pisWPn -bounded. The ﬁrst result follows by apply-\ning online-to-batch conversion to this sequence, i.e. we draw T= \n\u0010\u0000WPn\n\"\u00012\u0000\nnlogn+ log1\n\u000e\u0001\u0011\nsamples (wt;pt), run randomized EG as above, and set ^ x=1\nTPT\nt=1xtto be the average of the\nresulting distributions xt. Applying Lemma A.1 yields the result.\nThe sample complexity guarantee resulting from online-to-batch conversion matches that of Linder-\nmayr and Megow [33], except that the output is a distribution over permutation matrices so the error is\nin expectation over that distribution. However, randomized EG is incredibly inefﬁcient due to the need\nto store and sample from a distribution over n!variables. Another way of learning over permutation\nmatrices is to run an online learning algorithm over the set of doubly stochastic matrices [ 21]. When\nthe losses are linear functions of the permutation matrices this is yields efﬁcient low-regret algorithms\nbecause each doubly stochastic matrix corresponds to a small convex combination of permutation\nmatrices, i.e. a distribution from which one can sample an action. However, the losses Uw;pare\nnonlinear and so a different approach is needed.\n21",
  "textLength": 73008
}