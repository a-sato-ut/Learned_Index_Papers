{
  "paperId": "96cdac8dd54d3632aa538f1a595670b7b9300869",
  "title": "A Fair and Memory/Time-efficient Hashmap",
  "pdfPath": "96cdac8dd54d3632aa538f1a595670b7b9300869.pdf",
  "text": "FairHash : A Fair and Memory/Time-efficient Hashmap\nNima Shahbazi\nnshahb3@uic.edu\nUniversity of Illinois Chicago\nUSAStavros Sintos\nstavros@uic.edu\nUniversity of Illinois Chicago\nUSAAbolfazl Asudeh\nasudeh@uic.edu\nUniversity of Illinois Chicago\nUSA\nABSTRACT\nHashmap is a fundamental data structure in computer science.\nThere has been extensive research on constructing hashmaps that\nminimize the number of collisions leading to efficient lookup query\ntime. Recently, the data-dependant approaches, construct hashmaps\ntailored for a target data distribution that guarantee to uniformly\ndistribute data across different buckets and hence minimize the\ncollisions. Still, to the best of our knowledge, none of the existing\ntechnique guarantees group fairness among different groups of\nitems stored in the hashmap.\nTherefore, in this paper, we introduce FairHash , a data-dependant\nhashmap that guarantees uniform distribution at the group-level\nacross hash buckets, and hence, satisfies the statistical parity notion\nof group fairness. We formally define, three notions of fairness and,\nunlike existing work, FairHash satisfies all three of them simul-\ntaneously. We propose three families of algorithms to design fair\nhashmaps, suitable for different settings. Our ranking-based algo-\nrithms reduce the unfairness of data-dependant hashmaps without\nany memory-overhead. The cut-based algorithms guarantee zero-\nunfairness in all cases, irrespective of how the data is distributed, but\nthose introduce an extra memory-overhead. Last but not least, the\ndiscrepancy-based algorithms enable trading off between various\nfairness notions. In addition to the theoretical analysis, we perform\nextensive experiments to evaluate the efficiency and efficacy of our\nalgorithms on real datasets. Our results verify the superiority of\nFairHash compared to the other baselines on fairness at almost\nno performance cost.\nCCS CONCEPTS\nâ€¢Information systems â†’Data structures .\nKEYWORDS\nFair Data Structures, Algorithmic Fairness, Responsible Data Man-\nagement\nACM Reference Format:\nNima Shahbazi, Stavros Sintos, and Abolfazl Asudeh. 2024. FairHash : A\nFair and Memory/Time-efficient Hashmap. In Proceedings of (SIGMODâ€™24).\nACM, New York, NY, USA, 22 pages. https://doi.org/XXXXXXX.XXXXXXX\n1 INTRODUCTION\n1.1 Motivation\nAs data-driven technologies become ingrained in our lives, their\ndrawbacks and potential harms become increasingly evident [ 12,\n83, 100]. Subsequently, algorithmic fairness has become central in\nSIGMODâ€™24, ,\nÂ©2024 Association for Computing Machinery.\nhttps://doi.org/XXXXXXX.XXXXXXXcomputer science research to minimize machine bias [ 22,24,76,\n80]. Unfortunately, despite substantial focus on data preparation,\nmachine learning, and algorithm design, data structures and their\npotential to induce unfairness in downstream tasks have received\nlimited attention [108].\nTowards filling the research gap to understand potential harms\nand designing fair data structures, this paper revisits the hashmap\ndata structure through the lens of fairness. To the best of our knowl-\nedge, this is the first paper to study group fairness in a data structure\ndesign . Hashmaps are a founding block in many applications such\nas bloom filters for set membership [ 31,34,86], hash sketches for\ncardinality estimation [ 52,56], count sketches for frequency esti-\nmation [ 45], min-hashes in similarity estimation [ 33,44], hashing\ntechniques for security applications [6, 97], and many more.\nCollision in a hashmap happens when the hash of two different\nentities is the same. Collisions are harmful as those cause false\npositives . For example, in the case of bloom filters when the hash\nof a query point collides with a point in a queried set, the query\npoint is falsely classified as a set member. In such cases, in the\nleast further computations are needed to resolve the false positives.\nHowever, this would require to explicitly storing all set members in\nthe memory, which may not be possible in all cases. Note that false\nnegative is impossible in case of hashmaps. That simply is because\nwhenğ‘¥=ğ‘¥â€², the hash of ğ‘¥andğ‘¥â€²is always the same. To further\nmotivate the problem, let us consider Example 1.\nExample 1: Consider an airline security application, which aims\nto identify passengers who may pose a threat, and subject them for\nfurther screening and potential prevention from boarding flights. A\nset of criminal records is used to create a no-fly list. Due to privacy\nreasons, the criminalsâ€™ identities are hashed and the list is a pair\nof {hash, gender} of individuals. The passenger hashes are matched\nagainst the no-fly list for this purpose. False positives in airline\nsecurity can lead to significantly inconveniencing passengers. â–¡\nTraditional ğ‘˜-wise independent hashing [ 85,96] aims to ran-\ndomly map a key (an entity) to a random value (bucket) in a specific\noutput range. However, given a set of points, it is unlikely that\nindependent random value assignment to the points uniformly dis-\ntribute the points to the buckets. For example, Fig. 1 shows the\ndistribution of 100 independent and identically distributed (iid)\nrandom numbers, we generated in range [0,9]. While in a uniform\ndistribution of the points, each bucket would have exactly 10 points,\nthe random assignment did not satisfy it. On the other hand, the\nnumber of collision is minimized, when the uniform distribution is\nsatisfied. In order to resolve this issue, data-informed approaches are\ndesigned, where given a set of data, the goal is to â€œlearnâ€ a proper\nhash function that uniformly distributes the data across different\nbuckets [ 70,78,89]. Particularly, given a data set of ğ‘›entities, the\ncumulative density function (CDF) of (the distribution representedarXiv:2307.11355v2  [cs.DS]  13 Apr 2024\n\nSIGMODâ€™24, , Shahbazi, et al.\nby) the data set is constructed. Then the hashmap is created by par-\ntitioning the range of values into ğ‘šbuckets such that each buckets\ncontainsğ‘›\nğ‘šentities. We refer to this approach [ 70] asCDF-based\nhashmap. It has been shown that such index structures [ 70,89] can\noutperform traditional hashmaps on practical workloads.\nTo the best of our knowledge, none of the existing hashmap\nschemes consider fairness in terms of equal performance for differ-\nent demographic groups. Given the wide range of hashmap appli-\ncations, this can cause discrimination against minority groups, at\nleast for social applications. Therefore, in this paper, we study group\nfairness defined as equal collision probability (false positive rate) for\ndifferent demographic groups , in hashmaps. Specifically, targeting\nto prevent disparate treatment [ 16,58], we propose FairHash , a\nhashmap that satisfies group fairness. We consider the CDF-based\nhashmap for designing our fair data structure.\nWhile there are many definitions of fairness, at a high level\ngroup fairness notions fall under three categories of independence,\nseperation, and sufficiency [ 24]. Our proposed notion of group\nfairness falls under the independence category, which is satisfied\nwhen the output of an algorithm is independent of the demographic\ngroups (protected attributes). Specifically, we adopt statistical parity ,\nA well-known definition under the independence category.\n1.2 Applications\nFairHash is adata-informed hashmap, extended upon CDF-based\nhashmap. Data-informed hashmaps are proper for applications\nwhere a large-enough workload is available for learning to uni-\nformly distribute the data across various buckets. Particularly, data-\ninformed hashmaps are preferred when the underlying data distri-\nbution is not uniform. Besides, the choice between data-informed\nhashmaps and traditional hashmaps hinges on other factors such\nas conflict resolution policy and memory constraints.\nNevertheless, data-informed hashmaps effectively address a broad\nrange of practical challenges where traditional approaches fall short.\nFor instance, when dealing with larger payloads or distributed\nhash maps, it is advantageous to minimize conflicts, making data-\ninformed hashmaps more beneficial. On the other hand, in scenar-\nios involving small keys, small values, or data following a uniform\ndistribution, traditional hash functions are likely to perform effec-\ntively [ 70]. In addition to Example 1, in the following we outline\na few examples of the applications of data-informed hashmaps,\nwhich demonstrate the need for FairHash in real-world scenarios.\nTable Joins in Data Lakes: Consider an enterprise seeking to\nshare its data with third-party companies. The data is stored in a\ndata lake and includes sensitive information such as email, phone\nnumber, or even social security number of the clients serving as the\nprimary keys for select tables. Disclosing such sensitive information\nconstitutes a breach of client privacy, hence it needs to be masked\nthrough hashing. In such cases, where the join operation is on the\nhashed columns, hash-value collisions would add invalid rows to\nthe result table. As a result, higher occurrence of false positives\ncorrelated with particular demographic groups can make the result\ntable biased against that group. This underscores the need to employ\na fair hashing scheme when anonymizing sensitive columns.Machine Learning Datasets: Consider a scenario for construct-\ning an ensemble model, where a large dataset is partitioned into\nmultiple smaller random subsets, each used for training for a base\nmodel. In this setting, ensuring that the sampling process does not\nintroduce any bias is crucial; hence, each subset of the data should\nmaintain the same ratio of each subpopulation as present in the orig-\ninal distribution. While the conventional random sampling method\nfails to ensure such distribution alignment, employing FairHash\nto bucketize the data, guarantees this goal.\nDistributed Hashmaps and Load Balancing: Collisions incur a\nsubstantial cost in distributed hashmaps, as each collision neces-\nsitates an extra lookup request on the remote machine through\nRDMA, taking on the order of microseconds. Therefore, higher\ncollision rates linked to keys from a specific demographic group\nmay cause a notable performance disadvantage against that group.\nUsing FairHash , for example, web servers can distribute incom-\ning requests across multiple server instances. This guarantees an\nequitable distribution of the load, mitigating potential harm to a\nspecific client if a server becomes unavailable.\n1.3 Technical Contributions\n(I) Proposing FairHash .We begin our technical contributions by\nproposing two notions of group fairness (single and pairwise) based\non collision probability disparity between demographic groups. In-\ntuitively, single fairness is satisfied when the data is uniformly\ndistributed across different buckets, i.e., the buckets are equi-size.\nConsequently, as reflected in Figure 1 traditional hashmaps do not\nsatisfy single fairness. On the other hand, CDF-based hashmap\nsatisfies single fairness as all of its buckets have the same size. Pair-\nwise fairness is a stronger notion of fairness that, not only requires\nequi-size buckets but it also demands equal ratio of demographic\ngroups across all buckets. To better clarify this, let us consider the\ndistribution of 100 random (synthetic) points from two groups red\nand blue into 10 buckets, using CDF-based hashmap and FairHash\nin Figure 1a and Figure 1b. Although assigning equal number of\npoints to each bucket, CDF-based hashmap fails to satisfy equal\nratio of groups across all buckets, and hence fails on the pairwise\nfairness. On the other hand, using FairHash equal group ratios,\nhence pairwise fairness, is satisfied. Also, from the figures it is evi-\ndent that pairwise fairness is a stronger notion, not only requiring\nequi-size buckets but also equal group ratios. We shall prove of this\nin Â§ 2 after providing the formal terms and definitions.\n(II) Ranking-based Algorithms. Next, making the observation\nthat only the ranking between the points (not their value distri-\nbution) impacts the fairness of the CDF-based hashmap, we use\ngeometric techniques to find alternative ranking of points for fair\nhashing. We propose multiple algorithmic results with various ben-\nefits. At a high level, our ranking-based approach, maintains the\nsame time and memory efficiency as of CDF-based hashing, while\nminimizing the unfairness (but not guaranteeing zero unfairness).\n(III) Cut-based Algorithms. Our next contribution is based on\nthe idea of adding more bins than the number of hash buckets.\nWe propose Sweep&Cut to prove that independent of the initial\ndistribution of points, there always exists a fair hashing based on the\ncutting approach. While guaranteeing fair hashing, Sweep&Cut is\nnot memory efficient. Therefore, we make an interesting connection\n\nFairHash : A Fair and Memory/Time-efficient Hashmap SIGMODâ€™24, ,\n012345678905101520\nFigure 1: Distribution of 100 random integers\nin [0,9].\n01234567890246810(a)CDF-based hashmap\n01234567890246810 (b)FairHash\nFigure 2: Distribution of 100 points belonging to two groups blue and redin 10\nbuckets.\nto the necklace splitting problem [8, 10, 77], and using some of the\nrecent advancements [ 9] on it, provide multiple algorithms for\nFairHash . While guaranteeing the fair hashing, our algorithms\nachieve the same time efficiency as of the CDF-based hashing with\na small increase in the memory requirement.\n(IV) Discrepancy-based Algorithms. We also propose discrepancy-\nbased algorithms that trade-off single fairness to achieve improve\npairwise fairness and memory efficiency. In addition to the theoret-\nical analysis, we conduct experiments to verify the efficiency and\neffectiveness of our algorithms.\n2 PRELIMINARIES\nLetğ‘ƒbe a set ofğ‘›points1inRğ‘‘(each point represents a tuple\nwithğ‘‘attributes), where ğ‘‘â‰¥1. LetG={g1,..., gğ‘˜}be a set ofğ‘˜\ndemographic groups2(e.g., male,female ). Each point ğ‘âˆˆğ‘ƒbelongs\nto group g(ğ‘)âˆˆG . By slightly abusing the notation we use gğ‘–to\ndenote the set of points in group gğ‘–.\nLetHbe a hashmap with ğ‘šbuckets,ğ‘1,...,ğ‘ğ‘š, and a hash\nfunctionâ„:Rğ‘‘â†’[1,ğ‘š]that maps each input point ğ‘âˆˆğ‘ƒto one\nof theğ‘šbuckets. Given the pair (ğ‘ƒ,H), we define three quantitative\nrequirements that are used to define fairness in hash functions.\n(1)Collision Probability (Individual fairness) : For any pair of\npointsğ‘,ğ‘taken uniformly at random from ğ‘ƒ, it should hold\nthatğ‘ƒğ‘Ÿ[â„(ğ‘)=â„(ğ‘)]=1\nğ‘š.\n(2)Single fairness : For eachğ‘–â‰¤ğ‘˜, for any point ğ‘ğ‘–taken uni-\nformly at random from gğ‘–and any point ğ‘¥taken uniformly\nat random from ğ‘ƒ, it should hold that ğ‘ƒğ‘Ÿ[â„(ğ‘ğ‘–)=â„(ğ‘¥)]=\n...=ğ‘ƒğ‘Ÿ[â„(ğ‘ğ‘˜)=â„(ğ‘¥)]=1\nğ‘š.\n(3)Pairwise fairness : For eachğ‘–â‰¤ğ‘˜, for any pair of points\nğ‘ğ‘–,ğ‘ğ‘–taken uniformly at random from gğ‘–, it should hold that\nğ‘ƒğ‘Ÿ[â„(ğ‘ğ‘–)=â„(ğ‘ğ‘–)]=...=ğ‘ƒğ‘Ÿ[â„(ğ‘ğ‘˜)=â„(ğ‘ğ‘˜)]=1\nğ‘š.\nEnsuring Pareto-optimality: A major challenge when formulat-\ning fair algorithms is that those may generate Pareto-dominated\nsolutions [ 82]. That is, those may produce a fair solution that are\n1Throughout this paper, we assume access to ğ‘ƒis the complete set of points. For cases\nwhere instead an unbiased set of samples from ğ‘ƒis available, our results in Tables 1\nand 2 will remain in an expected manner.\n2Demographic groups can be defined as the intersection of multiple sensitive attributes,\nsuch as (race, gender) as{black-female ,Â·Â·Â·} .worse for all groups, including minorities, compared to the unfair\nones. In particular, in a utility assignment setting, let the utility as-\nsigned to each group gğ‘–by the fair algorithm be ğ‘¢ğ‘–. There may exist\nanother (unfair) assignment that assigns ğ‘¢â€²\nğ‘–>ğ‘¢ğ‘–,âˆ€gğ‘–âˆˆG. This\nusually can happen when fairness is defined as the parity between\ndifferent groups, without further specifications.\nIn this paper, the requirements 2 and 3 (single and pairwise\nfairness) have been specifically defined in a way to prevent gen-\nerating Pareto-dominated fair solutions. To better explain the ra-\ntional behind our definitions, let us consider pairwise fairness (the\nthird requirement). Only requiring equal collision probability be-\ntween various groups, the fairness constraint would translate to\nğ‘ƒğ‘Ÿ[â„(ğ‘ğ‘–)=â„(ğ‘ğ‘–)]=ğ‘ƒğ‘Ÿ[â„(ğ‘ğ‘˜)=â„(ğ‘ğ‘˜)], whereğ‘ğ‘–andğ‘ğ‘–be-\nlong to group gğ‘–. Now let us consider a toy example with two\ngroups{g1,g2}, two buckets{ğ‘1,ğ‘2}, andğ‘›points where half be-\nlong to g1. Let the hashmap Hmap each point ğ‘âˆˆg1toğ‘1and\neach pointğ‘âˆˆg2toğ‘2. In this example, the collision probabil-\nity between a random pair of points is 0.5, simply because each\nbucket contains half of the points. It also satisfies the collision\nprobability equality between pairs of points from the same groups:\nğ‘ƒğ‘Ÿ[â„(ğ‘ğ‘–)=â„(ğ‘ğ‘–)]=ğ‘ƒğ‘Ÿ[â„(ğ‘ğ‘˜)=â„(ğ‘ğ‘˜)]=1.\nThis, however, is the worst assignment for both groups as their\npairs always collide . In other words, it is fair, in a sense that it is\nequally bad for both groups. Any other hashmap would have a\nsmaller collision probability for both groups and, even if not fair,\nwould be more beneficial for both groups. In other words, this is a\nfair but Pareto-dominated solution.\nIn order to ensure Pareto-optimality while developing fair hashmaps,\nin requirements 2 and 3 (single and pairwise fairness), not only we\nrequire the probabilities to be equal, but also we require them to\nbe equal to the best case , where the collision probability is1\nğ‘š. As a\nresult, we guarantee that (1) our hashmap is fair and (2) no other\nhashmap can do better for any of the groups.\nCalculation of probabilities: Next, we give the exact close forms\nfor computing the collision probability, the single fairness, and\nthe pairwise fairness. Let ğ›¼ğ‘–,ğ‘—be the number of items from group\nğ‘–at bucketğ‘—and letğ‘›ğ‘—=âˆ‘ï¸ğ‘˜\nğ‘–=1ğ›¼ğ‘–,ğ‘—. The probabilities are calcu-\nlated as follows, collision probability:âˆ‘ï¸ğ‘š\nğ‘—=1(ï¸‚ğ‘›ğ‘—\nğ‘›)ï¸‚2\n, single fairness:\n\nSIGMODâ€™24, , Shahbazi, et al.\nâˆ‘ï¸ğ‘š\nğ‘—=1ğ›¼ğ‘–,ğ‘—\n|gğ‘–|Â·ğ‘›ğ‘—\nğ‘›, pairwise fairness:âˆ‘ï¸ğ‘š\nğ‘—=1(ï¸‚ğ›¼ğ‘–,ğ‘—\n|gğ‘–|)ï¸‚2\n. We observe that, if\nand only if|gğ‘–|/ğ‘štuples from each group gğ‘–are placed in every\nbucket then all quantities above are equal to the optimum value\n1\nğ‘š.\nThe relationship between the three requirements: In this paper\nwe aim to construct a hashmap Hthat satisfies (approximately) all\nthe three requirements.\nProposition 1. Collision probability is satisfied if and only if all\nbuckets contain exactly the same number of points i.e., for each bucket\nğ‘ğ‘—,|ğ‘ğ‘—|=ğ‘›\nğ‘š.\nProposition 2. If collision probability is satisfied then single\nfairness is also satisfied.\nProposition 3. Pairwise fairness is satisfied if and only if for\nevery group gğ‘–âˆˆG, every bucket ğ‘ğ‘—contains the same number of\npoints from group gğ‘–, i.e.,ğ‘ğ‘—contains|gğ‘–|\nğ‘šitems from group gğ‘–. If\npairwise fairness is satisfied then both single fairness and the collision\nprobability are satisfied but the reverse may not necessarily hold.\nProof: We give the proofs to all propositions above.\nFor Proposition 1, if each bucket contains ğ‘›/ğ‘šitems then the\ncollision probability isâˆ‘ï¸ğ‘š\nğ‘—=1(ï¸‚ğ‘›/ğ‘š\nğ‘›)ï¸‚2\n=1\nğ‘š, so it is satisfied. For the\nother direction, we assume that the collision probability holds. No-\ntice thatâˆ‘ï¸ğ‘š\nğ‘—=1(ï¸‚ğ‘›ğ‘—\nğ‘›)ï¸‚2\n=1\nğ‘šâ‡”âˆ‘ï¸ğ‘š\nğ‘—=1ğ‘›2\nğ‘—=ğ‘›2\nğ‘š. For any integer values\nğ‘›ğ‘—â‰¥0withâˆ‘ï¸ğ‘š\nğ‘—=1ğ‘›ğ‘—=ğ‘›, it holds thatâˆ‘ï¸ğ‘š\nğ‘—=1ğ‘›2\nğ‘—â‰¥âˆ‘ï¸ğ‘š\nğ‘—=1(ï¸ğ‘›\nğ‘š)ï¸2=ğ‘›2\nğ‘š\nand the minimum value is achieved only for ğ‘›ğ‘—=ğ‘›\nğ‘šfor each\nğ‘—âˆˆ[1,ğ‘š]. The result follows.\nFor Proposition 2, we have that if the collision probability is\nsatisfied then from Proposition 1, it holds ğ‘›ğ‘—=ğ‘›\nğ‘š. Then the\nsingle fairness for any group gğ‘–is computed asâˆ‘ï¸ğ‘š\nğ‘—=1ğ›¼ğ‘–,ğ‘—\n|gğ‘–|ğ‘›ğ‘—\nğ‘›=\nâˆ‘ï¸ğ‘š\nğ‘—=1ğ›¼ğ‘–,ğ‘—\n|gğ‘–|ğ‘›/ğ‘š\nğ‘›=1\nğ‘šÂ·|gğ‘–|âˆ‘ï¸ğ‘š\nğ‘—=1ğ›¼ğ‘–,ğ‘—=1\nğ‘š, so it is satisfied.\nThe first part of Proposition 3 follows directly from Proposi-\ntion 1, because the pairwise fairness of group gğ‘–is equivalent to\nthe collision probability assuming that ğ‘ƒ=gğ‘–. For the second part,\nif pairwise fairness is satisfied, then from the first part of Proposi-\ntion 3, we know that ğ‘›ğ‘—=ğ‘›\nğ‘š. Then from Proposition 1 the collision\nprobability is satisfied, so from Proposition 2, the single fairness\nis also satisfied. Finally, the construction in the proof of Lemma 1\nshows that the reverse may not necessarily hold. â–¡\nFrom Propositions 1 to 3, in order to satisfy the three require-\nments of collision probability, single fairness, and pairwise fairness,\nit is enough to develop a hashmap that satisfies pairwise fairness\n(which will generate equal-size buckets). In other words, pairwise\nfairness is the strongest property , compared to the other two.\nOur goal is to design hashmaps that, while satisfying collision\nprobability and single fairness requirements, satisfies pairwise fair-\nnessas the stronger notion of fairness3. Specifically, we want to find\nthe hashmapHwithğ‘šbuckets to optimize the pairwise fairness.\nMeasuring unfairness: For a group gâˆˆG, letğ‘ƒğ‘Ÿgbe the pairwise\ncollision probability between its members. That is, ğ‘ƒğ‘Ÿg=ğ‘ƒğ‘Ÿ[â„(ğ‘)=\nâ„(ğ‘)], ifğ‘andğ‘are selected uniformly at random from the same\n3To simplify the terms, in the rest of the paper, we use â€œfairnessâ€ to refer to pairwise\nfairness. We shall explicitly use â€œsingle fairnessâ€ when we refer to it.group g(ğ‘)=g(ğ‘)=g. We measure unfairness as the max-to-\nmin ratio in pairwise collision probabilities between the groups.\nParticularly, using1\nğ‘šas the min on the collision probability, we\nsay a hashmap is ğœ€-unfair , if and only if\nmax gâˆˆG(ğ‘ƒğ‘Ÿg)\n1/ğ‘šâ‰¤(1+ğœ€) â‡’ max\ngâˆˆG(ğ‘ƒğ‘Ÿg)â‰¤1\nğ‘š(1+ğœ€) (1)\nIt is evident that for a hashmap that satisfies pairwise fairness,\nğœ€=0. We use1\nğ‘šas the min on the collision probability to ensure\npareto-optimality.\nMemory efficiency: A hashmap with ğ‘šbuckets needs to store\nat leastğ‘šâˆ’1boundary points to separate the ğ‘šbuckets. While\nour main objective is to satisfy fairness, we also would like to\nminimally increase the required memory to separate the buckets.\nWe say a hashmap with ğ‘šbuckets satisfies ğ›¼-memory , if and only\nif it stores at most ğ›¼(ğ‘šâˆ’1)boundary points. Evidently, the most\nmemory-efficient hashmap satisfies ğ›¼=1.\nDefinition 1 ((ğœ€,ğ›¼)-hashmap). A hashmapHis an(ğœ€,ğ›¼)-\nhashmap if and only if it is ğœ€-unfair andğ›¼-memory.\nProblem formulation: Givenğ‘ƒ,G, and parameters ğ‘š,ğœ€,ğ›¼ , the\ngoal is to design an (ğœ€,ğ›¼)-hashmap.\nIn this work we mostly focus on (ğœ€,1)-hashmaps and(0,ğ›¼)-\nhashmaps, minimizing ğœ€andğ›¼, respectively4. Note that, in the best\ncase, one would like to achieve (0,1)-hashmap. That is a hash-map\nthat is 0-unfair and does not require additional memory, i.e., satisfies\n1-memory.\n(Review) CDF-based hashmap [ 70]:is a data-informed hashmap\nthat â€œlearnsâ€ the cumulative density function of values over a spe-\ncific attribute, and use it to place the boundaries of the ğ‘šbuckets\nsuch that an equal number of points (ğ‘›\nğ‘š) fall in each bucket. Tra-\nditional hash functions and learned hash functions (CDF) usually\nsatisfy the collision probability and the single fairness probability,\nhowever they violate the pairwise fairness property.\nLemma 1. While CDF-based hashmap satisfies collision probability,\nhence single fairness, it may not satisfy pairwise fairness.\nProof: From [ 70], it is always the case that each bucket contains\nthe same number of tuples, i.e.,ğ‘›ğ‘—\nğ‘›=1\nğ‘šfor everyğ‘—=1,...,ğ‘š .\nHence, from Proposition 1, the collision probability is satisfied.\nThen, from Proposition 2, the single fairness is also satisfied.\nNext, we show that CDF-based hashmap does not always sat-\nisfy pairwise fairness using a counter-example. Let ğ‘˜=2,ğ‘š=2,\n|g1|=3,|g2|=3|, andğ‘›=6. Assume the 1-dimensional tuples\n{1,2,3,4,5,6}where the first 3of them belong to g1and the last\nthree belong to g2. The CDF-based hashmap will place the first\nthree tuples into the first bucket and the last three tuples into the\nsecond bucket. By definition, the pairwise fairness is 1(instead of\n1/2) for both groups. â–¡\nData-informed hashmaps vs. traditional hashmaps: A sum-\nmary of the comparison between CDF-based and traditional hashmaps\n4For simplicity, throughout the paper, we consider that the cardinality of each group\ngğ‘–is divisible by ğ‘š.\n\nFairHash : A Fair and Memory/Time-efficient Hashmap SIGMODâ€™24, ,\nis presented in Table 3. The most major difference between CDF-\nbased and hashmaps is that the former is data-informed. That is,\ntheCDF-based hashmap is tailored for a specific data workload,\nwhile traditional hashmaps are data-independent, i.e., their behav-\nior does not depend on the data those are applied on. As a result,\nas mentioned in Â§ 1.2, the main assumption and the requirement\nof the CDF-based hashmap is access to a large-enough workload\nğ‘ƒfor learning the CDF function. On the other hand, traditional\nhashmaps do not require access to any data workload. While the\ntraditional hashmaps compute the hash value of a query point in\nconstant time, CDF-based hashmap requires to run a binary search\non the bucket boundaries, and hence has a query time logarithmic\ntoğ‘š. Having learned the data distribution, the CDF-based hashmap\nguarantees a uniform distribution of data across different buckets,\nwhile as shown in Figure 1 traditional hashmaps cannot guarantee\nthat. As a result, CDF-based hashmap guarantees equal collision\nprobability and single fairness, while traditional hashmaps do not.\nFinally, both traditional and CDF-based hashmaps fail to guarantee\npairwise fairness, a requirement that FairHash satisfies.\n2.1 Overview of the algorithmic results\nWe propose two main approaches for defining the hashmaps, called\nranking-based approach, and cut-based approach. A summary of\nour algorithmic results with perfect collision probability and single\nfairness for all groups, is shown in Table 1.\nLetğ‘Šbe the set of all possible unit vectors in Rğ‘‘. Given a vector\nğ‘¤âˆˆğ‘Š, letğ‘ƒğ‘¤be the ordering defined by the projection of ğ‘ƒonto\nğ‘¤. Based on this ordering, we construct ğ‘šequi-size buckets. In the\nranking-based approaches, we focus on finding the best vector ğ‘¤to\ntake the projection on that minimizes the unfairness. Let ğ‘‚ğ‘ƒğ‘‡ğ‘…(ğ‘ƒğ‘¤)\nbe the smallest parameter such that an (ğ‘‚ğ‘ƒğ‘‡ğ‘…(ğ‘ƒğ‘¤),1)-hashmap\nexists inğ‘ƒğ‘¤with collision probability and single fairness equal to\n1/ğ‘š. We defineğœ€ğ‘…=minğ‘¤âˆˆğ‘Šğ‘‚ğ‘ƒğ‘‡ğ‘…(ğ‘ƒğ‘¤).\nIn the cut-based approaches, we define ğ›½(ğ‘šâˆ’1)+1intervals\nI={ğ¼1,...,ğ¼ğ›½(ğ‘šâˆ’1)+1}defined by the ğ›½(ğ‘šâˆ’1)boundary points,\nsuch that for each point ğ‘âˆˆğ‘ƒğ‘¤(the projection of ğ‘ƒon a vectorğ‘¤)\nthere exists an interval ğ¼âˆˆIwhereğ‘âˆˆğ¼. Each interval ğ¼âˆˆIis\nassigned to one of the ğ‘šbuckets. Our focus on cut-based approaches\nis on finding the best way to place the boundary points on a given\norderingğ‘ƒğ‘¤. In all cases the hashmap Hstores the vector ğ‘¤along\nwith theğ›½(ğ‘šâˆ’1)+1intervalsIand their assigned buckets. During\nthe query phase, given a point ğ‘âˆˆRğ‘‘, we first apply the projection\nâŸ¨ğ‘¤,ğ‘âŸ©and we get the value ğ‘ğ‘¤âˆˆR. Then we run a binary search\non the boundary points to find the interval ğ¼âˆˆIsuch thatğ‘ğ‘¤âˆˆğ¼.\nWe returnğ‘(ğ¼)as the bucket ğ‘belongs to.\nSo far, we consider that the collision probability and the single\nfairness should be (optimum)1\nğ‘š. However, this restricts the op-\ntions when finding fair hashmaps. What if, there exists a hashmap\nwith better pairwise fairness having slightly more or less than\nğ‘›/ğ‘šitems in some buckets? Hence, we introduce the notion of\nğ›¾âˆ’discrepancy [9]. The goal is to find a hashmap with ğ‘šbuckets\nsuch that each bucket contains at most (1+ğ›¾)|gğ‘–|\nğ‘šand at least\n(1âˆ’ğ›¾)|gğ‘–|\nğ‘špoints from each group ğ‘–â‰¤ğ‘˜. A summary of our algo-\nrithmic results with approximate collision probability and single\nfairness for all groups, is shown in Table 2. Let ğ‘Šbe the set of allpossible unit vectors in Rğ‘‘. Letğ‘‚ğ‘ƒğ‘‡ğ·(ğ‘ƒğ‘¤)be the smallest parame-\nter such that a hashmap with ğ‘‚ğ‘ƒğ‘‡ğ·(ğ‘ƒğ‘¤)-discrepancy exists in ğ‘ƒğ‘¤.\nWe defineğœ€ğ·=minğ‘¤âˆˆğ‘Šğ‘‚ğ‘ƒğ‘‡ğ·(ğ‘ƒğ‘¤).\nFairness and memory efficiency trade-off. Ideally, one would\nlike to develop a hashmap that is 0-unfair and 1-memory. But in\npractice, depending on the distribution of the data, achieving both\nat the same time may not be possible. For cases where fairness is a\nhard constraint, cut-based algorithms are preferred, as those guar-\nantee 0-unfairness, irrespective of the data distribution. But that is\nachieved at a cost of increasing memory usage. On the other hand,\nranking-based algorithms minimize unfairness without requiring\nany extra memory but do not 0-unfairness; hence those are fit when\nmemory is a hard constraint. Last but not least, the discrepancy-\nbased algorithms provide a trade-off between pairwise and single\nfairness. Specifically, these algorithms do not guarantee to contain\nexactlyğ‘›\nğ‘špoints in each bucket, and hence, may not satisfy the\nfirst two requirements (individual and single fairness). However,\nfor cases where adding a small amount of single unfairness is tol-\nerable, the discrepancy-based algorithms may further reduce the\npairwise unfairness of the ranking-based and the memory bound\nof the cut-based algorithms.\nRemark. In all cases, for simplicity, we assume that our algo-\nrithms have access to the entire input set in order to compute a\nnear-optimal hashmap. However, our algorithms can work in ex-\npectation, if an unbiased sample set from the input set is provided.\nWe verify this, experimentally in Section 6.7.\n3 RANKING-BASED ALGORITHMS\nWe start our contribution by defining a space of (ğœ€,1)-hashmaps,\nand designing algorithms to find the (near) optimum hashmap with\nthesmallestğœ€. By definition, recall that ğœ€ğ‘…is the smallest unfairness\nwe can find with this technique (assuming 1-memory).\nOur key observation is that only the ordering between the tuples\nmatters when it comes to pairwise fairness, not the attribute values.\nHence, assuming that ğ‘‘>1, our idea is to combine the attribute\nvalues of a point ğ‘âˆˆğ‘ƒinto a single score ğ‘“(ğ‘), using a function\nğ‘“:Rğ‘‘â†’Rcalled the ranking function, and consider the ordering\nof the points based on their scores for creating the hashmap. Then,\nin a class of ranking functions, the objective is to find the one that\nreturns the best(ğœ€,1)-hashmap with the smallest value of ğœ€. Of\ncourse,ğ‘“(ğ‘)needs to be computed efficiently, ideally in constant\ntime. Therefore, we select linear ranking functions, where the points\nare linearly projected on a vector ğ‘¤âˆˆRğ‘‘. That is,ğ‘“ğ‘¤(ğ‘)=âŸ¨ğ‘,ğ‘¤âŸ©=\nğ‘âŠ¤ğ‘¤. Notice that the value ğ‘“ğ‘¤(ğ‘)can be computed in ğ‘‚(ğ‘‘)=ğ‘‚(1)\ntime and it defines an ordering between the different tuples ğ‘âˆˆğ‘ƒ.\nFor a vector ğ‘¤, letğ‘ƒğ‘¤be the ordering defined by ğ‘“ğ‘¤and letğ‘ƒğ‘¤[ğ‘—]\nbe theğ‘—-th largest tuple in the ordering ğ‘ƒğ‘¤. Given the ordering\ndefined by a ranking function ğ‘“ğ‘¤, we construct(ğ‘šâˆ’1)boundaries\nto partition the data into ğ‘šequi-size buckets (each containingğ‘›\nğ‘š\npoints). Then a natural algorithm to construct an (ğœ€,1)-hashmap is\nto run the subroutine for each possible ranking function and in the\nend return the best ranking function we found.\nFor simplicity, we describe our first method in R2. All our algo-\nrithms are extended to any constant dimension ğ‘‘.\nIt is known that for ğ‘ƒâŠ‚R2there existğ‘‚(ğ‘›2)combinatorially\ndifferent ranking functions [ 54]. We can easily compute them if we\n\nSIGMODâ€™24, , Shahbazi, et al.\nTable 1: Summary of the algorithmic results with exact ( 1/ğ‘š) collision and single fairness probability.\nAssumptions Performanceâˆ—\nAlgorithm No. No.(ğœ€,ğ›¼)-hashmap Query Pre-processing\nAttributes Groups time time\nRanking ğ‘‘â‰¥2ğ‘˜â‰¥2(ğœ€ğ‘…,1) ğ‘‚(logğ‘š) ğ‘‚(ğ‘›ğ‘‘logğ‘›)\nSweep&Cut ğ‘‘â‰¥1ğ‘˜â‰¥2(0,ğ‘›\nğ‘š) ğ‘‚(logğ‘›) ğ‘‚(ğ‘›logğ‘›)\nNecklace 2ğ‘”ğ‘‘â‰¥1 2(0,2) ğ‘‚(logğ‘š) ğ‘‚(ğ‘›logğ‘›)\nNecklaceğ‘˜ğ‘”ğ‘‘â‰¥1ğ‘˜>2(0,ğ‘˜(4+logğ‘›))ğ‘‚(log(ğ‘˜ğ‘šlogğ‘›))ğ‘‚(ğ‘šğ‘˜3logğ‘›+ğ‘˜ğ‘›ğ‘š(ğ‘›+ğ‘š))\nâˆ—:ğ‘›is the dataset size, ğ‘šis the number of buckets, and ğ‘˜is the number of groups.\nTable 2: Summary of the algorithmic results with approximate collision and single fairness probability. The output of Ranking+\nholds with probability at least 1âˆ’1/ğ‘›.\nAssumptions Performanceâˆ—\nAlgorithm No. No.(ğœ€,ğ›¼)-hashmap Query Pre-processing\nAttr. Groups time time\nRanking ğ‘‘â‰¥2ğ‘˜â‰¥2(ğœ€ğ·,1) ğ‘‚(logğ‘š) ğ‘‚(ğ‘›ğ‘‘+2ğ‘šlogğ‘˜)\nRanking+ğ‘‘â‰¥2ğ‘˜â‰¥2((1+ğ›¿)ğœ€ğ·+ğ›¾,1)ğ‘‚(logğ‘š)ğ‘‚(ğ‘›+ğ‘˜ğ‘‘+2ğ‘š2ğ‘‘+5\nğ›¾2ğ‘‘+4polylog(ğ‘›,1\nğ›¿))\nNecklaceğ‘˜ğ‘”ğ‘‘â‰¥1ğ‘˜>2(ğœ€,ğ‘˜(4+log1\nğœ€))ğ‘‚(log(ğ‘˜ğ‘šlog1\nğœ€))ğ‘‚(ğ‘šğ‘˜3log1\nğœ€+ğ‘˜ğ‘›ğ‘š(ğ‘›+ğ‘š))\nTable 3: Comparison between CDF-based and traditional hashmaps.\nQuery Collision Single Pairwise\nHashmap Architecture time probability fairness fairness\ntraditional data-independent ğ‘‚(1) âœ— âœ— âœ—\nCDF-based data-dependent ğ‘‚(logğ‘š) âœ“ âœ“ âœ—\nFairHash data-dependent ğ‘‚(logğ‘š) âœ“ âœ“ âœ“\nwork in the dual space [ 54]. For a point ğ‘=(ğ‘¥ğ‘,ğ‘¦ğ‘)âˆˆğ‘ƒwe define\nthe dual line ğœ†(ğ‘):ğ‘¥ğ‘ğ‘¥1+ğ‘¦ğ‘ğ‘¥2=1. LetÎ›={ğœ†(ğ‘)|ğ‘âˆˆğ‘ƒ}be\nthe set ofğ‘›lines. LetA(Î›)be the arrangement [ 54] ofÎ›, which\nis defined as the decomposition of R2into connected (open) cells\nof dimensions 0,1,2(i.e., point, line segment, and convex polygon)\ninduced byğ‘ƒ. It is known thatA(Î›)hasğ‘‚(ğ‘›2)cells and it can be\ncomputed in ğ‘‚(ğ‘›2logğ‘›)time [54].\nGiven a vector ğ‘¤, the ordering ğ‘ƒğ‘¤is the same as the ordering\ndefined by the intersections of Î›with the line supporting ğ‘¤. Hence,\nsomeone can identify all possible ranking functions ğ‘“ğ‘¤by travers-\ning all intersection points in A(Î›). Each intersection in A(Î›)is\nrepresented by a triplet (ğ‘,ğ‘,ğ‘¤)denoting that the lines ğœ†(ğ‘),ğœ†(ğ‘)\nare intersecting and the intersection point lies on the line support-\ning the vector ğ‘¤. LetWbe the array of size ğ‘‚(ğ‘›2)containing all\nintersection triplets sorted in ascending order of the vectorsâ€™ angles\nwith theğ‘¥-axis. LetW[ğ‘–]denote theğ‘–-th triplet(ğ‘ğ‘–,ğ‘ğ‘–,ğ‘¤ğ‘–). It is\nknown that the orderings ğ‘ƒğ‘¤ğ‘–,ğ‘ƒğ‘¤ğ‘–+1differ by swapping the ranking\nof two consecutive items. Without loss of generality, we assume\nthat for(ğ‘ğ‘–,ğ‘ğ‘–,ğ‘¤ğ‘–)âˆˆW , the ranking of ğ‘ğ‘–is higher than the rank-\ning ofğ‘ğ‘–for vectors with angle greater than ğ‘¤ğ‘–. The arrayWcan\nbe constructed in ğ‘‚(ğ‘›2logğ‘›)time.\n3.1 Algorithm\nUsingW, the goal is to find the best (ğœ€,1)-hashmap over all vectors\nsatisfying the collision probability and the single fairness. The\nhigh level idea is to consider each different vector ğ‘¤âˆˆW , and\nfor each ordering ğ‘ƒğ‘¤, find the hashmap that satisfies the collisionprobability and the single fairness measuring the unfairness. In\nthe end, return the vector ğ‘¤along with the boundaries of the\nhashmap with the smallest unfairness we found. If we execute it\nin a straightforward manner, we would have ğ‘‚(|W|Â·ğ‘›logğ‘›)=\nğ‘‚(ğ‘›ğ‘‘+1logğ‘›)time algorithm. Next, we present a more efficient\nimplementation applying fast update operations each time that we\nvisit a new vector.\nThe pseudo-code of the algorithm in R2is provided in Algo-\nrithm 1. The algorithm starts with the initialization of some useful\nvariables and data structures. Let ğ‘¤0be the unit vector with angle to\ntheğ‘¥-axis slightly smaller than ğ‘¤1â€™s angle. We visit each point in ğ‘ƒ\nand we find the (current) ordering, denoted by ğ‘ƒğ‘¤, sorting the pro-\njections ofğ‘ƒontoğ‘¤0. Next, we compute the best (ğœ€,1)-hashmap in\nğ‘ƒğ‘¤. The only way to achieve optimum collision probability and sin-\ngle fairness in ğ‘ƒğ‘¤is to construct exactly ğ‘šbuckets containing the\nsame total number of tuples in each of them. Identifying the buck-\nets (and constructing the hashmap) in ğ‘ƒğ‘¤is trivial because every\nbucket should contain exactly ğ‘›/ğ‘šitems. Hence, the boundaries of\ntheğ‘—-th hashmap bucket are defined as ğ‘ƒğ‘¤[(ğ‘—âˆ’1)ğ‘›\nğ‘š+1],ğ‘ƒğ‘¤[ğ‘—ğ‘›\nğ‘š],\nforğ‘—âˆˆ[1,ğ‘š]. Next, we compute the unfairness with respect to\nğ‘¤0. For each group gğ‘–that contains at least one item in ğ‘—-th bucket\nwe setğ›¼ğ‘–,ğ‘—=0. We use the notation ğ›¼ğ‘–,ğ‘—to denote the number of\ntuples from group ğ‘–inğ‘—-th bucket. Let ğ‘ƒğ‘¤[â„“]be the next item in\ntheğ‘—-th bucket, and let ğ‘ƒğ‘¤[â„“]âˆˆgğ‘–. We update ğ›¼ğ‘–,ğ‘—â†ğ›¼ğ‘–,ğ‘—+1. Af-\nter traversing all items in ğ‘ƒğ‘¤, we compute the pairwise fairness of\ngroup gğ‘–asğ‘ƒğ‘Ÿğ‘–=âˆ‘ï¸ğ‘š\nğ‘—=1(ï¸‚ğ›¼ğ‘–,ğ‘—\n|gğ‘–|)ï¸‚2\n. The unfairness with respect to ğ‘¤0is\n\nFairHash : A Fair and Memory/Time-efficient Hashmap SIGMODâ€™24, ,\nAlgorithm 1 Ranking 2ğ·\nInput: Set of points ğ‘ƒâˆˆR2\nOutput: vectorğ‘¤âˆ—and corresponding boundaries ğµ\n1:Construct and sort the vectors in Wwith respect to their angles;\n2:ğ‘¤0â†vector with angle to the ğ‘¥-axis slightly smaller than ğ‘¤1â€™s angle;\n3:ğ‘ƒğ‘¤â†sorted projection of ğ‘ƒontoğ‘¤0;\n4:ğ‘ğ‘—â†(ï¸ğ‘ƒğ‘¤[(ğ‘—âˆ’1)ğ‘›\nğ‘š+1],ğ‘ƒğ‘¤[ğ‘—ğ‘›\nğ‘š])ï¸âˆ€ğ‘—=1,...,ğ‘š ;\n5:ğ›¼ğ‘–,ğ‘—â†|gğ‘–âˆ©ğ‘ğ‘—|,âˆ€ğ‘–=1,...,ğ‘˜ ,ğ‘—=1,...,ğ‘š ;\n6:ğ‘ƒğ‘Ÿğ‘–â†âˆ‘ï¸ğ‘š\nğ‘—=1(ï¸‚ğ‘ğ‘–,ğ‘—\n|gğ‘–|)ï¸‚2\n,âˆ€ğ‘–=1,...,ğ‘˜ ;\n7:ğ‘€â†Max-Heap on ğ‘ƒğ‘Ÿğ‘–,âˆ€ğ‘–=1,...,ğ‘š ;\n8:ğœ€â†ğ‘šÂ·maxğ‘–â‰¤ğ‘˜ğ‘ƒğ‘Ÿğ‘–âˆ’1;ğ‘¤âˆ—=ğ‘¤0;\n9:for(ğ‘ğ‘ ,ğ‘ğ‘ ,ğ‘¤ğ‘ )âˆˆW do\n10: ifğ‘ğ‘ ,ğ‘ğ‘ belong in the same bucket OR g(ğ‘ğ‘ )==g(ğ‘ğ‘ )then\n11: Swapğ‘ğ‘ ,ğ‘ğ‘ and updateğ‘ƒğ‘¤;\n12: else\n13: Letğ‘ğ‘ âˆˆğ‘ğ‘—,g(ğ‘ğ‘ )=gğ‘–,ğ‘ğ‘ âˆˆğ‘ğ‘—+1,g(ğ‘ğ‘ )=gâ„“;\n14:ğ‘ƒğ‘Ÿğ‘–â†ğ‘ƒğ‘Ÿğ‘–âˆ’(ï¸‚ğ›¼ğ‘–,ğ‘—\n|gğ‘–|)ï¸‚2\nâˆ’(ï¸‚ğ›¼ğ‘–,ğ‘—+1\n|gğ‘–|)ï¸‚2\n+(ï¸‚ğ›¼ğ‘–,ğ‘—âˆ’1\n|gğ‘–|)ï¸‚2\n+(ï¸‚ğ›¼ğ‘–,ğ‘—+1+1\n|gğ‘–|)ï¸‚2\n;\n15:ğ‘ƒğ‘Ÿâ„“â†ğ‘ƒğ‘Ÿâ„“âˆ’(ï¸‚ğ›¼â„“,ğ‘—+1\n|gâ„“|)ï¸‚2\nâˆ’(ï¸‚ğ›¼â„“,ğ‘—\n|gâ„“|)ï¸‚2\n+(ï¸‚ğ›¼â„“,ğ‘—+1âˆ’1\n|gâ„“|)ï¸‚2\n+(ï¸‚ğ›¼â„“,ğ‘—+1\n|gâ„“|)ï¸‚2\n;\n16:ğ›¼ğ‘–,ğ‘—=ğ›¼ğ‘–,ğ‘—âˆ’1,ğ›¼ğ‘–,ğ‘—+1=ğ›¼ğ‘–,ğ‘—+1+1,ğ›¼â„“,ğ‘—+1=ğ›¼â„“,ğ‘—+1âˆ’1,ğ›¼â„“,ğ‘—=ğ›¼â„“,ğ‘—+1;\n17: Updateğ‘ƒğ‘Ÿğ‘–,ğ‘ƒğ‘Ÿâ„“inğ‘€;\n18: ifğ‘šÂ·ğ‘€.ğ‘¡ğ‘œğ‘()âˆ’1<ğœ€then{ğœ€=ğ‘šÂ·ğ‘€.ğ‘¡ğ‘œğ‘()âˆ’1;ğ‘¤âˆ—=ğ‘¤ğ‘ ;}\n19:forğ‘—=1toğ‘šdoğµğ‘—â†ğ‘ƒğ‘¤âˆ—[ğ‘—ğ‘›\nğ‘š]+ğ‘ƒğ‘¤âˆ—[ğ‘—ğ‘›\nğ‘š+1]\n2;// right boundary of ğ‘ğ‘—\n20:return(ğ‘¤âˆ—,ğµ);\nğœ€=ğ‘šÂ·maxğ‘–â‰¤ğ‘˜ğ‘ƒğ‘Ÿğ‘–âˆ’1.After computing all values ğ‘ƒğ‘Ÿğ‘–, we construct\na max heap ğ‘€over{ğ‘ƒğ‘Ÿ1,...,ğ‘ƒğ‘Ÿğ‘˜}. Letğ‘¤âˆ—=ğ‘¤0. We run the algo-\nrithm visiting each vector in Wmaintaining ğœ€,ğ‘€,ğ‘¤âˆ—,ğ‘ƒğ‘¤,ğ‘ƒğ‘Ÿğ‘–,ğ›¼ğ‘–,ğ‘—\nfor eachğ‘–andğ‘—.\nAs the algorithm proceeds, assume that we visit a triplet (ğ‘ğ‘ ,ğ‘ğ‘ ,ğ‘¤ğ‘ )\ninW. Ifğ‘ğ‘ andğ‘ğ‘ belong in the same bucket, we only update the\npositions of ğ‘ğ‘ ,ğ‘ğ‘ inğ‘ƒğ‘¤and we continue with the next vector. Sim-\nilarly, if both ğ‘ğ‘ ,ğ‘ğ‘ belong in the same group gğ‘–we only update the\nposition of them in ğ‘ƒğ‘¤and we continue with the next vector. Next,\nwe consider the more interesting case where ğ‘ğ‘ âˆˆgğ‘–belongs in the\nğ‘—-th bucket and ğ‘ğ‘ âˆˆgâ„“belongs in the(ğ‘—+1)-th bucket of ğ‘ƒğ‘¤, with\nğ‘–â‰ â„“, just before we visit (ğ‘ğ‘ ,ğ‘ğ‘ ,ğ‘¤ğ‘ ). We update ğ‘ƒğ‘¤as in the other\ncases. However, now we need to update the pairwise fairness. In\nparticular, we update,\nğ‘ƒğ‘Ÿğ‘–=ğ‘ƒğ‘Ÿğ‘–âˆ’(ï¸ƒğ›¼ğ‘–,ğ‘—\n|gğ‘–|)ï¸ƒ2\nâˆ’(ï¸ƒğ›¼ğ‘–,ğ‘—+1\n|gğ‘–|)ï¸ƒ2\n+(ï¸ƒğ›¼ğ‘–,ğ‘—âˆ’1\n|gğ‘–|)ï¸ƒ2\n+(ï¸ƒğ›¼ğ‘–,ğ‘—+1+1\n|gğ‘–|)ï¸ƒ2\n,\nand similarly\nğ‘ƒğ‘Ÿâ„“=ğ‘ƒğ‘Ÿâ„“âˆ’(ï¸ƒğ›¼â„“,ğ‘—+1\n|gâ„“|)ï¸ƒ2\nâˆ’(ï¸ƒğ›¼â„“,ğ‘—\n|gâ„“|)ï¸ƒ2\n+(ï¸ƒğ›¼â„“,ğ‘—+1âˆ’1\n|gâ„“|)ï¸ƒ2\n+(ï¸ƒğ›¼â„“,ğ‘—+1\n|gâ„“|)ï¸ƒ2\n.\nBased on the new values of ğ‘ƒğ‘Ÿğ‘–,ğ‘ƒğ‘Ÿâ„“we update the max heap ğ‘€. We\nalso update ğ›¼ğ‘–,ğ‘—=ğ›¼ğ‘–,ğ‘—âˆ’1,ğ›¼ğ‘–,ğ‘—+1=ğ›¼ğ‘–,ğ‘—+1+1,ğ›¼â„“,ğ‘—+1=ğ›¼â„“,ğ‘—+1âˆ’1,\nğ›¼â„“,ğ‘—=ğ›¼â„“,ğ‘—+1. If the top value of ğ‘€is smaller than ğœ€, then we\nupdateğœ€with the top value of ğ‘€and we update ğ‘¤âˆ—=ğ‘¤ğ‘ . After\ntraversing all vectors in Wwe return the best vector ğ‘¤âˆ—we found.\nThe boundaries can easily be constructed by finding the ordering\nğ‘ƒğ‘¤âˆ—satisfying that each bucket contains exactly ğ‘›/ğ‘šitems.\nAnalysis: The correctness of the algorithm follows from the defini-\ntions. Next, we focus on the running time. We need ğ‘‚(ğ‘›2logğ‘›)to\nconstructA(Î›)andğ‘‚(ğ‘›logğ‘›)additional time to initialize ğœ€,ğ‘€,ğ‘¤âˆ—,ğ‘ƒğ‘¤,ğ‘ƒğ‘Ÿğ‘–,ğ›¼ğ‘–,ğ‘—. For each new vector ğ‘¤ğ‘ we visit, we update ğ‘ƒğ‘¤inğ‘‚(1)\ntime by storing the position of each item ğ‘âˆˆğ‘ƒinğ‘ƒğ‘¤using an auxil-\niary array. All variables ğ‘ƒğ‘Ÿğ‘–,ğ‘ƒğ‘Ÿâ„“,ğ›¼ğ‘–,ğ‘—,ğ›¼ğ‘–,ğ‘—+1,ğ›¼â„“,ğ‘—,ğ›¼â„“,ğ‘—+1are updated\nexecuting simple arithmetic operations so the update requires ğ‘‚(1)\ntime. The max heap ğ‘€is updated in ğ‘‚(logğ‘š)time. Hence, for each\nvectorğ‘¤ğ‘ âˆˆW we spendğ‘‚(logğ‘š)time. There are ğ‘‚(ğ‘›2)vectors\ninWso the overall time of our algorithm is ğ‘‚(ğ‘›2logğ‘›).\nExtension to ğ‘‘â‰¥2:The algorithm can straightforwardly be ex-\ntended to any constant dimension ğ‘‘. Using known results [ 54], we\ncan construct the arrangement of ğ‘‚(ğ‘›)hyperplanes in ğ‘‚(ğ‘›ğ‘‘logğ‘›)\ntime. Then, in ğ‘‚(ğ‘›ğ‘‘logğ‘›)time in total, we can traverse all combina-\ntorially different vectors such that the orderings ğ‘ƒğ‘¤ğ‘–,ğ‘ƒğ‘¤ğ‘–+1between\ntwo consecutive vectors ğ‘¤ğ‘–,ğ‘¤ğ‘–+1differ by swapping the ranking\nof two consecutive items. Our algorithm is applied to all ğ‘‚(ğ‘›ğ‘‘)\nvectors with the same way as described above. Hence, we conclude\nto the next theorem.\nTheorem 1. Letğ‘ƒbe a set ofğ‘›tuples in Rğ‘‘. There exists an\nalgorithm that computes an (ğœ€ğ‘…,1)-hashmap satisfying the collision\nprobability and the single fairness in ğ‘‚(ğ‘›ğ‘‘logğ‘›)time.\nSampled vectors: So far, we consider all possible vectors ğ‘¤âˆˆW\nto return the one with the optimum pairwise fairness. In practice,\ninstead of visiting ğ‘‚(ğ‘›ğ‘‘)vectors, we sample a large enough set\nof vectors Ë†ï¸‚WfromRğ‘‘. We run our algorithm using the set of\nvectors Ë†ï¸‚Winstead ofWand we return the vector that leads to the\nminimum unfairness ğœ€. This algorithm runs in ğ‘‚(|Ë†ï¸‚W|ğ‘›logğ‘›).\n4 CUT-BASED ALGORITHMS\nThe ranking-based algorithms proposed in Section 3, cannot guar-\nantee 0-unfairness. In other words, by re-ranking the ğ‘›points using\nlinear projections in Rğ‘‘, those can only achieve an (ğœ€ğ‘…,1)-hashmap.\nIn this section, we introduce a new technique with the aim to\nguarantee 0-unfairness . So far, our approach has been to partition\nthe values (after projection) into ğ‘šequi-size buckets. In other words,\neach bucket ğ‘ğ‘–is a continuous range of values specified by two\nboundary points. The observation we make in this section is that\nthebuckets do not necessarily need to be continuous . Specifically, we\ncan partition the values into more than ğ‘šbins while in a many-to-\none matching, several bins are assigned to each bucket. Using this\nidea, in the following we propose two approaches for developing\nfair hashmaps with 0-unfairness.\n4.1 Sweep&Cut\nAn interesting question we explore in this section is whether a\ncut-based algorithm exists that always guarantee 0-unfairness.\nTheorem 2. Consider a set of ğ‘›points in R, where each point\nğ‘belongs to a group ğ‘”(ğ‘)âˆˆ{ g1,Â·Â·Â·,gğ‘˜}. Independent of how the\npoints are distributed and their orders, there always exist a cut-based\nhashmap that is 0-unfair .\nWe prove the theorem by providing the Sweep&Cut algorithm\n(Algorithm 2) that always finds a 0-unfair hashmap. Without loss\nof generality, let ğ¿=âŸ¨ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›âŸ©be the sorted list of points\ninğ‘ƒbased on their values on an attribute ğ‘¥.Sweep&Cut sweeps\nthroughğ¿fromğ‘1toğ‘ğ‘›twice. During the first sweep (Lines 3\n\nSIGMODâ€™24, , Shahbazi, et al.\nAlgorithm 2 Sweep&Cut\nInput: The set of points P\nOutput: Bin boundaries ğµand corresponding buckets ğ»\n1:âŸ¨ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›âŸ©â†sortğ‘ƒbased on an attribute ğ‘¥\n2:forğ‘—=1toğ‘˜doğ‘ğ‘—â†0;// # of instances observed from gğ‘–\n3:forğ‘–=1toğ‘›do\n4: letgğ‘—=g(ğ‘ğ‘–);ğ‘ğ‘—â†ğ‘ğ‘—+1\n5:ğ»ğ‘¡ğ‘šğ‘\nğ‘–â†âŒŠï¸‚ğ‘ğ‘—Ã—ğ‘š\n|gğ‘—|âŒ‹ï¸‚\n+1\n6:ğ‘–â†0;ğ‘—â†0\n7:while True do\n8: while(ğ‘–<ğ‘›andğ»ğ‘¡ğ‘šğ‘\nğ‘–==ğ»ğ‘¡ğ‘šğ‘\nğ‘–+1)doğ‘–â†ğ‘–+1\n9:ğ»ğ‘—â†ğ»ğ‘¡ğ‘šğ‘\nğ‘–;// the bucket assigned to the ğ‘—-th bin\n10: ifğ‘–==ğ‘›then break\n11:ğµğ‘—=ğ‘ğ‘–[ğ‘¥]+ğ‘ğ‘–+1[ğ‘¥]\n2;// the right boundary of the ğ‘—-th bin\n12:ğ‘—â†ğ‘—+1\n13:return(ğµ,ğ»)\nto 5), the algorithm keeps track of the number of instances it has\nobserved from each group gğ‘–. The algorithm uses ğ»ğ‘¡ğ‘šğ‘to mark\nwhich bucket each point should fall into, such that each bucket\ncontains|gğ‘–|\nğ‘šinstances from each group gğ‘–. During the second pass\n(Lines 7 to 12), the algorithm compares the neighboring points and\nas long as those should belong to the same bucket (Line 8), there\nis no need to introduce a new boundary. Otherwise, the algorithm\nadds a new boundary (in array ğµ) to introduce a new bin, while\nassigning the bucket numbers in ğ». Finally, the algorithm returns\nthe bin boundaries and the corresponding buckets.\n4.1.1 Analysis. Sweep&Cut makes two linear-time passes over ğ‘ƒ.\nTherefore, considering the time to sort ğ‘ƒbased onğ‘¥, its time com-\nplexity isğ‘‚(ğ‘›logğ‘›). The algorithm assignsğ‘›\nğ‘špoint to each bucket;\nhence, following Propositions 1 and 2 is satisfies collision proba-\nbility and single fairness. More importantly, Sweep&Cut assigns\n|gğ‘–|\nğ‘špoint from each group gğ‘–to each bucket. As a result, for any\npairğ‘ğ‘–,ğ‘ğ‘–ingğ‘–,ğ‘ƒğ‘Ÿ[â„(ğ‘ğ‘–)=â„(ğ‘ğ‘–)]=1\nğ‘š. Therefore, the hashmap\ngenerated by Sweep&Cut is0-unfair , proving Theorem 2.\nDespite guaranteeing 0-unfairness, Sweep&Cut is not efficient\nin terms of memory. Particularly, in the worst case, it can introduce\nas much asğ‘‚(ğ‘›)boundaries.\nIn a best case, where the points are already ordered in a way that\ndividing them into ğ‘šequi-size buckets is already fair, Sweep&Cut\nwill addğ‘šbins (ğ‘šâˆ’1boundaries). On the other hand, in adversarial\nsetting, a large potion of the neighboring pairs within the ordering\nbelong to different groups with different hash buckets assigned to\nthem. Therefore, in the worst-case Sweep&Cut may add up to ğ‘‚(ğ‘›)\nboundaries, making it satisfyğ‘›\nğ‘š-memory . Applying the binary\nsearch on the ğ‘‚(ğ‘›)bin boundaries, the query time of Sweep&Cut\nhashmap is in the worst-case ğ‘‚(logğ‘›).\nLemma 2. In the binary demographic groups cases, where G=\n{g1,g2}andğ‘Ÿ=|g1|, the expected number of bins added by Sweep&Cut\nis bounded by 2(ï¸ğ‘Ÿ(ğ‘›âˆ’ğ‘Ÿ)\nğ‘›+ğ‘š)ï¸.\nProof: Sweep&Cut adds at most ğ‘šâˆ’1boundaries between the\nneighboring pairs that both belong to the same group, simply be-\ncause a boundary between neighboring pair can only happen when\nmoving from one bucket to the next while there are ğ‘šbuckets.In order to find the upper-bound on the number of bins added,\nin the following we compute the expected number of neighboring\npairs from different groups. Consider the in the sorted list of points\nâŸ¨ğ‘1,Â·Â·Â·,ğ‘ğ‘›âŸ©based on the attribute ğ‘¥. The probability that a point\nğ‘ğ‘–belongs to group ğ‘”1isğ‘ƒğ‘Ÿ1=ğ‘ƒğ‘Ÿ(g(ğ‘ğ‘–)=g1)=ğ‘Ÿ\nğ‘›. Now consider\ntwo consecutive points ğ‘ğ‘–,ğ‘ğ‘–+1, in the list. The probability that these\ntwo belong to different groups is 2ğ‘ƒğ‘Ÿ1(1âˆ’ğ‘ƒğ‘Ÿ1). LetBbe the random\nvariable representing the number of pairs from different groups. We\nhave,ğ¸[B]=âˆ‘ï¸ğ‘›âˆ’1\nğ‘–=12ğ‘ƒğ‘Ÿ1(1âˆ’ğ‘ƒğ‘Ÿ1)=2(ğ‘›âˆ’1)ğ‘Ÿ\nğ‘›(1âˆ’ğ‘Ÿ\nğ‘›)<2ğ‘Ÿ(ğ‘›âˆ’ğ‘Ÿ)\nğ‘›.\nTherefore,ğ¸[No. bins]â‰¤ğ¸[B]+ 2ğ‘š<2(ï¸‚ğ‘Ÿ(ğ‘›âˆ’ğ‘Ÿ)\nğ‘›+ğ‘š)ï¸‚\n. â–¡\n4.2 Transforming to Necklace Splitting\nAlthough guaranteeing 0-unfairness, Sweep&Cut is not memory\nefficient. The question we seek to answer in this section is whether it\nis possible to guarantee 0-unfairness while introducing significantly\nless number of bins, close to ğ‘š. In particular, we make an interesting\nconnection to the so-called necklace splitting problem [ 8,10,77],\nand use the recent advancements [ 9] on this problem by the math\nand theory community to solve the fair hashmap problem.\n(Review) Necklace Splitting: Consider a necklace of ğ‘‡beads of\nğ‘›â€²types. For each type ğ‘–â‰¤ğ‘›â€², letğ‘šâ€²\nğ‘–be the number of beads with\ntypeğ‘–, and letğ‘šâ€²=maxğ‘–ğ‘šâ€²\nğ‘–. The objective is to divide the beads\nbetweenğ‘˜â€²agents, such that (a) all agents receive exactly the same\namount of beads from each type and (b) the number of splits to the\nnecklace is minimized.\nReduction: Given an instance of the fair hashmap problem, let\nğ¿=âŸ¨ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›âŸ©be the ordering of ğ‘ƒbased on an attribute ğ‘¥.\nThe problem gets reduced to necklace splitting as following: The\npoints inğ‘ƒget mapped into the ğ‘‡=ğ‘›bead, distributed with the\norderingâŸ¨ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›âŸ©in the necklace. The ğ‘˜groups inGget\ntranslated to the ğ‘›â€²=ğ‘˜bead types{g1,Â·Â·Â·,gğ‘˜}. Theğ‘šbuckets\ntranslate to the ğ‘˜â€²=ğ‘šagents.\nGiven the necklace splitting output, each split of the necklace\ntranslates into a bin. The bin is assigned to the corresponding bucket\nof the agent who received the necklace split. Using this reduction,\nan optimal solution to the necklace splitting is the fair hashmap\nwith minimum number of cuts: first, since all party in necklace split-\nting receive equal number of each bead type, the corresponding\nhashmap satisfies 0-unfairness , as well as the collision probabil-\nity and single fairness requirements; second, since the necklace\nsplitting minimizes the number of splits to the necklace, it adds\nminimum number of bins to the fair hashmap problem, i.e., it finds\nthe optimal fair hashmap on ğ¿, with minimum number of cuts.\nUsing this mapping, in the rest of the section we adapt the recent\nresults for solving necklace splitting for fair hashmap. In particular,\nAlon and Graur [ 9] propose polynomial time algorithms (with re-\nspect to number of beads) for the Necklace Splitting problem and\ntheğœ€-approximate version of the problem.\n4.2.1 Binary groups. The fair hashmap problem when there are\ntwo groups{g1,g2}, maps to the necklace splitting instance with\ntwo bead types. While a straightforward implementation of the\nalgorithm in [ 9]-(Proposition 2) leads to an ğ‘‚(ğ‘›(logğ‘›+ğ‘š))algo-\nrithm, in Necklace 2ğ‘”Algorithm 3, we propose an optimal time\n\nFairHash : A Fair and Memory/Time-efficient Hashmap SIGMODâ€™24, ,\nalgorithm that guarantees splitting a necklace with at most 2(ğ‘šâˆ’1)\ncuts in only ğ‘‚(ğ‘›logğ‘›)time.\nAlgorithm: Without loss of generality, let ğ¶=âŸ¨ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›âŸ©be\nthe sorted list of points in ğ‘ƒbased on their values on an attribute\nğ‘¥.Necklace 2ğ‘”viewsğ¶as a circle by considering ğ‘ğ‘›beforeğ‘1. It\nuses modulo to size of the list ( %|ğ¶|) to move along the circle. The\nkey idea is that the circle ğ¶always has at least one consecutive\nwindow of sizeğ‘›\nğ‘šthat contains|g1|\nğ‘špoints from g1(and hence\n|g2|\nğ‘špoints from g2), see [ 9]. Hence, we design an algorithm to\nfind such windows efficiently. We initialize a list ğ‘‡such thatğ‘‡[ğ‘—]\ncontains the number of items from group g1betweenğ¶[ğ‘—]and\nğ¶[(ğ‘—+ğ‘›/ğ‘šâˆ’1)%|ğ¶|]. Furthermore, we initialize the list ğ‘‹such that\nğ‘‹[ğ‘—]is true if and only if ğ‘‡[ğ‘—]=|g1|/ğ‘š, i.e., the window from ğ¶[ğ‘—]\ntoğ¶[(ğ‘—+ğ‘›/ğ‘šâˆ’1)%|ğ¶|]is a good candidate for a cut. All indexes\nare initialized in lines 4â€“9 of Algorithm 3. In order to bound the\nrunning time of the new algorithm, we assume that ğ‘‹[ğ‘—]also stores\na pointer to the ğ‘—-th elements in lists ğ¶andğ‘‡. Furthermore, we\nassume that all Boolean variables in ğ‘‹are stored in a max heap\nğ‘€ğ‘‹(ifğ‘‹[ğ‘—]=true andğ‘‹[ğ‘–]=false thenğ‘‹[ğ‘—]>ğ‘‹[ğ‘–]). We use\nğ‘€ğ‘‹to callğ‘€ğ‘‹.ğ‘¡ğ‘œğ‘()that returns the top item in max heap, i.e., it\nreturns ağ‘—such thatğ‘‹[ğ‘—]=true, inğ‘‚(1)time.\nThe algorithm is executed in iterations until the list ğ¶is non-\nempty. In each iteration, we find a window of size ğ‘›/ğ‘šcontaining\nexactly|g1|/ğ‘šitems from group g1(so it also contains exactly\n|g2|/ğ‘šitems from group g2). More specifically, in line 11 we find ğ‘—\nsuch thatğ‘‡[ğ‘—]=|g1|/ğ‘š. In line 20 we define the new cut we find\nand in lines 21â€“23 we remove the cut from our lists to continue\nwith the next iteration. The points within the cut are marked for the\nbucketğ‘ğ‘˜ğ‘¡. In lines 13â€“19 we update the values of ğ‘‡(and hence the\nvalues ofğ‘‹) so thatğ‘‡andğ‘‹have the correct values after removing\nthe window from ğ¶[ğ‘—]toğ¶[(ğ‘—+ğ‘›/ğ‘šâˆ’1)%|ğ¶|]. Hence, we can con-\ntinue searching for the next window containing |g1|/ğ‘štuples from\ngroup g1in the next iteration. Finally, it sorts the discovered cuts\nand assigns the bin boundaries ğµand the corresponding buckets ğ».\nTheorem 3. In the binary demographic group cases, there exists\nan algorithm that finds a (0,2)-hashmap satisfying the collision\nprobability and the single fairness in ğ‘‚(ğ‘›logğ‘›)time.\nProof: In each iteration of the algorithm, we remove a window\ncontaining|g1|/ğ‘šitems from group g1and|g2|/ğ‘šitems from group\ng2. Hence, the correctness of our algorithm follows by the discrete\nintermediate value theorem and [ 9]. Next, we show that the running\ntime isğ‘‚(ğ‘›logğ‘›). It takesğ‘‚(ğ‘›logğ‘›)to sort based on attribute ğ‘¥.\nThen it takes ğ‘‚(ğ‘›/ğ‘š+ğ‘›)=ğ‘‚(ğ‘›)to initialize ğ‘‡andğ‘‹. In each\niteration of the while loop at line 10 we remove ğ‘›/ğ‘šitems, so in\ntotal it runs for ğ‘šiterations. In each iteration, we get ğ‘—at line 11 in\nğ‘‚(1)time using the max heap. The for loop in line 13 is executed\nforğ‘‚(ğ‘›/ğ‘š)rounds. In each round, we need ğ‘‚(1)time to update ğ‘‡.\nIt also takes ğ‘‚(1)time to update a value in ğ‘‹, andğ‘‚(logğ‘›)time\nto update the max heap. Finally, the for loop in line 21 runs for\nğ‘‚(ğ‘›/ğ‘š)rounds. In each round it takes ğ‘‚(1)time to remove an item\nfrom listsğ¶,ğ‘‡,ğ‘‹ andğ‘‚(logğ‘›)time to update the max heap. Overall\nAlgorithm 3 runs in ğ‘‚(ğ‘›logğ‘›+ğ‘šğ‘›\nğ‘šlogğ‘›)=ğ‘‚(ğ‘›logğ‘›)time. â–¡Algorithm 3 Necklace 2ğ‘”\nInput: The set of points ğ‘ƒ(with two groups{g1,g2})\nOutput: Bin boundaries ğµand corresponding buckets ğ»\n1:ğ¶=âŸ¨ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›âŸ©â†sortğ‘ƒbased on an attribute ğ‘¥;\n2:forğ‘–=0toğ‘›âˆ’1do{ğ‘‡[ğ‘–]â† 0;ğ‘‹[ğ‘–]â† false}\n3:ğ‘€ğ‘‹â†max heap storing ğ‘‹;ğœ1â†0;ğ‘ğ‘˜ğ‘¡â†0;\n//Initialize ğ‘‡\n4:forğ‘–=0toğ‘›/ğ‘šdo{ifg(ğ¶[ğ‘–])==g1thenğœ1â†ğœ1+1}\n5:forğ‘–=0toğ‘›âˆ’1do\n6:ğ‘‡[ğ‘–]â†ğœ1;\n7: ifğ‘‡[ğ‘–]==|g1|/ğ‘šthenğ‘‹[ğ‘–]â† true;ğ‘€ğ‘‹.update(ğ‘‹[ğ‘–]);\n8: ifg(ğ¶[ğ‘–])==g1thenğœ1â†ğœ1âˆ’1;\n9: ifg(ğ¶[(ğ‘–+ğ‘›/ğ‘š)%|ğ¶|])==g1thenğœ1â†ğœ1+1;\n10:while|ğ¶|>0do\n11:ğ‘—â†ğ‘€ğ‘¥.ğ‘¡ğ‘œğ‘();//Find a window[ğ‘—,(ğ‘—+ğ‘›/ğ‘šâˆ’1)%|ğ¶|]withğ‘‹[ğ‘—]=true\n//Updateğ‘‡andğ‘‹removing the window [ğ‘—,(ğ‘—+ğ‘›/ğ‘šâˆ’1])%|ğ¶|]\n12:ğœâ†ğ‘‡[(ğ‘—+ğ‘›/ğ‘š)%|ğ¶|];\n13: forğ‘–=ğ‘—âˆ’1toğ‘—âˆ’ğ‘›/ğ‘š+1with stepâˆ’1do\n14: ifğ‘–<0thenğ‘–â†|ğ¶|+ğ‘–;\n15: ifg(ğ¶[(ğ‘–+2Â·ğ‘›/ğ‘š)%|ğ¶|])==g1thenğœâ†ğœâˆ’1;\n16: ifg(ğ¶[ğ‘–])==g1thenğœâ†ğœ+1;\n17:ğ‘‡[ğ‘–]â†ğœ;\n18:ğ‘‹[ğ‘–]â† trueif(ğ‘‡[ğ‘–]==|g1|/ğ‘š)else false\n19:ğ‘€ğ‘‹.ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’(ğ‘‹[ğ‘–]);\n20: cutsâ†cutsâˆª{ï¸\nğ‘—,(ğ‘—+ğ‘›/ğ‘šâˆ’1)%|ğ¶|}ï¸\n//Remove window[ğ‘—,(ğ‘—+ğ‘›/ğ‘šâˆ’1)%|ğ¶|]\n21: forğ‘–âˆˆ[0,ğ‘›/ğ‘š)do\n22:ğ»ğ‘¡ğ‘šğ‘\nğ¶[(ğ‘–+ğ‘—)%|ğ¶|]â†ğ‘ğ‘˜ğ‘¡;ğ‘…ğ‘’ğ‘šğ‘œğ‘£ğ‘’(ğ¶[(ğ‘–+ğ‘—)%|ğ¶|]);\n23:ğ‘…ğ‘’ğ‘šğ‘œğ‘£ğ‘’(ğ‘‡[(ğ‘–+ğ‘—)%|ğ¶|]);ğ‘…ğ‘’ğ‘šğ‘œğ‘£ğ‘’(ğ‘‹[(ğ‘–+ğ‘—)%|ğ¶|]);\n24:ğ‘ğ‘˜ğ‘¡â†ğ‘ğ‘˜ğ‘¡+1;\n25:sort(cuts)\n26:forğ‘—=0to |cuts| do\n27: Letğ‘ğ‘–be the rightmost tuple in the ğ‘—-th bin;\n28:ğµğ‘—â†ğ‘ğ‘–[ğ‘¥]+ğ‘ğ‘–+1[ğ‘¥]\n2;// the right boundary of the ğ‘—-th bin\n29:ğ»ğ‘—â†ğ»ğ‘¡ğ‘šğ‘\nğ‘–;// the bucket assigned to the ğ‘—-th bin\n30:return(ğµ,ğ»)\n5 DISCREPANCY-BASED HASHMAPS\nA hashmap satisfies ğ›¾-discrepancy if and only if each bucket con-\ntains at least(1âˆ’ğ›¾)|gğ‘–|\nğ‘šand at most(1+ğ›¾)|gğ‘–|\nğ‘špoints from each\ngroup gğ‘–. In this section, we first show that a hashmap that satisfies\nğ›¾-discrepancy has bounded collision probability, single fairness,\nand pairwise fairness. Then, we propose efficient algorithms that\nconstructğ›¾-discrepancy hashmaps, where ğ›¾is close to the optimum.\nLetğ‘ƒğ‘¤be the ordering of points in ğ‘ƒbased on a vector ğ‘¤and let\nHbe a hashmap constructed on ğ‘ƒğ‘¤. Recall that ğ‘ƒğ‘Ÿğ‘–is defined as the\npairwise fairness value of Hfor group gğ‘–. Letğ¶ğ‘be the collision\nprobability and ğ‘†ğ‘ğ‘–the single fairness of group gğ‘–.\nLemma 3. LetHbe a hashmap satisfying ğ›¾-discrepancy. Then\nğ¶ğ‘â‰¤1+ğ›¾\nğ‘š,1âˆ’ğ›¾\nğ‘šâ‰¤ğ‘†ğ‘ğ‘–â‰¤1+ğ›¾\nğ‘šandğ‘ƒğ‘Ÿğ‘–â‰¤1+ğ›¾\nğ‘šfor each group gğ‘–.\nProof: Recall that ğ‘›ğ‘—is the number of items in bucket ğ‘—and\nğ›¼ğ‘–,ğ‘—is the number of items from group ğ‘–in bucketğ‘—. Notice thatâˆ‘ï¸ğ‘š\nğ‘—=1ğ›¼ğ‘–,ğ‘—=|gğ‘–|andâˆ‘ï¸ğ‘š\nğ‘—=1ğ‘›ğ‘—=ğ‘›. We have,\n\nSIGMODâ€™24, , Shahbazi, et al.\nğ‘ƒğ‘Ÿğ‘–=ğ‘šâˆ‘ï¸‚\nğ‘—=1(ï¸ƒğ›¼ğ‘–,ğ‘—\n|gğ‘–|)ï¸ƒ2\nâ‰¤ğ‘šâˆ‘ï¸‚\nğ‘—=1(1+ğ›¾)gğ‘–\nğ‘š\n|gğ‘–|Â·ğ›¼ğ‘–,ğ‘—\n|gğ‘–|=1+ğ›¾\nğ‘šğ‘šâˆ‘ï¸‚\nğ‘—=1ğ›¼ğ‘–,ğ‘—\n|gğ‘–|=1+ğ›¾\nğ‘š.\nSimilarly we show that\nğ¶ğ‘=ğ‘šâˆ‘ï¸‚\nğ‘—=1(ï¸‚ğ‘›ğ‘—\nğ‘›)ï¸‚2\nâ‰¤1+ğ›¾\nğ‘š.\nUsing the same arguments it also holds that\nğ‘†ğ‘ğ‘–=ğ‘šâˆ‘ï¸‚\nğ‘—=1ğ›¼ğ‘–,ğ‘—\n|gğ‘–|ğ‘›ğ‘—\nğ‘›â‰¤1+ğ›¾\nğ‘šandğ‘†ğ‘ğ‘–â‰¥1âˆ’ğ›¾\nğ‘š.\nâ–¡\nNext, we describe a dynamic programming algorithm to find\na hashmap with the smallest discrepancy. In the appendix, we\nshow a faster randomized algorithm approximating the smallest\ndiscrepancy. Finally, we describe a simple heuristic that works as a\npost-processing method to further improve the pairwise fairness.\n5.1 Dynamic Programming algorithm\nLetğ‘ƒğ‘¤be the ordering of the items for a vector ğ‘¤âˆˆ W . Let\nDisc[ğ‘–,ğ‘—]be the discrepancy of the optimum partition among the\nfirstğ‘–items inğ‘ƒğ‘¤usingğ‘—buckets. Let alsoD(ğ‘,ğ‘)be the discrep-\nancy of the bucket including all items in the window [ğ‘,ğ‘]inğ‘ƒğ‘¤,\ni.e.,{ğ‘ƒğ‘¤[ğ‘],ğ‘ƒğ‘¤[ğ‘+1],...,ğ‘ƒğ‘¤[ğ‘]}. We define the recursive relation\nDisc[ğ‘–,ğ‘—]=min\n1â‰¤ğ‘¥<ğ‘–max{Disc[ğ‘¥âˆ’1,ğ‘—âˆ’1],D(ğ‘¥,ğ‘–)}\nGivenğ‘–,ğ‘—, our algorithm computes the ğ‘—-th bucket with right bound-\naryğ‘–, trying all left boundaries ğ‘¥<ğ‘–, that leads to a partition with\nminimum discrepancy over the first ğ‘–items. In order to efficiently\nimplement the algorithm, each time we try a new left boundary\nğ‘¥, we do not compute D(ğ‘¥,ğ‘–)from scratch. Instead, we maintain\nand update a max-heap of size ğ‘˜storing the discrepancy of every\ngroup gğ‘–in the window[ğ‘¥,ğ‘–]. When we compute [ğ‘¥âˆ’1,ğ‘–]we\nupdate one element in the max-heap and compute D(ğ‘¥âˆ’1,ğ‘–)in\nğ‘‚(logğ‘˜)time. The table Disc hasğ‘‚(ğ‘›ğ‘š)cells and for each cell\nwe spendğ‘‚(ğ‘›logğ‘˜)time. By definition, Disc[ğ‘›,ğ‘š]computesğœ€ğ·.\nDoing standard modifications, it is straightforward to return the\npartition, instead of the discrepancy ğœ€ğ·. By repeating the algorithm\nabove for every ğ‘¤âˆˆW , we conclude to the next theorem.\nTheorem 4. Letğ‘ƒbe a set ofğ‘›tuples in Rğ‘‘. There exists an\nalgorithm that computes an (ğœ€ğ·,1)-hashmap in ğ‘‚(ğ‘›ğ‘‘+2ğ‘šlogğ‘›logğ‘˜)\ntime, satisfying ğœ€ğ·-approximation in collision probability and single\nfairness.\nFor parameters ğ›¾,ğ›¿, in the appendix, we show a randomized\nalgorithm, to compute a ((1+ğ›¿)ğœ€ğ·+ğ›¾,1)-hashmap with collision\nprobability at most1+(1+ğ›¿)ğœ€ğ·+ğ›¾\nğ‘šand single fairness in the range\n[1âˆ’(1+ğ›¿)ğœ€ğ·âˆ’ğ›¾\nğ‘š,1+(1+ğ›¿)ğœ€ğ·+ğ›¾\nğ‘š], in timeğ‘‚(ğ‘›+poly(ğ‘š,ğ‘˜,ğ›¿,ğ›¾).\n5.2 Local-search based heuristic\nSo far, we consider algorithms that return a hashmap satisfying\n(approximately) ğœ€ğ·-discrepancy. From Lemma 3 we know that a\nhashmap satisfying ğœ€ğ·-discrepancy is a(ğœ€ğ·,1)-hashmap. However,\nthere is no guarantee that ğœ€ğ·â‰¤ğœ€ğ‘….In this section, we design a practical algorithm that returns\nan(ğœ€,1)-hashmap with ğœ€â‰¤ğœ€ğ‘…allowing a slight increase in single\nfairness (and collision probability). In practice, as we see in Section 6,\nit holds that ğœ€â‰ªğœ€ğ‘…. The new algorithm is a local-search based\nalgorithm and works as a post-processing procedure to any ranking-\nbased algorithm (for example Algorithm 1). The intuition is the same\nto other discrepancy-based algorithms: The fact that we use the\nsame number of items per bucket, restricts our options to compute\na fair hashmap. Given the buckets computed by a ranking-based\nalgorithm, we try to (slightly) modify the boundaries of the buckets\nto compute a new hasmap with smaller unfairness.\nThe high level idea is that in each iteration of the algorithm we\nslightly move one of the boundaries that improves the unfairness\nthe most, maintaining a sufficient single fairness and collision prob-\nability. Letğ‘‡be the maximum number of iterations we execute our\nalgorithm, and let ğ‘“âˆ’,ğ‘“+,ğ‘+be the minimum single fairness, the\nmaximum single fairness, and the maximum collision probability,\nrespectively, that the returned hashmap should satisfy. Let ğµbe\nthe boundaries returned by Algorithm 1. For an iteration ğ‘–â‰¤ğ‘‡,\nfor every boundary ğµğ‘—âˆˆğµwe moveğµğ‘—one position to the left or\nto the right. For each movement of the boundary ğµğ‘—, we compute\nthe unfairness ğœ€ğ‘—, single fairness ğ‘“ğ‘—, and collision probability ğ‘ğ‘—\nof the new partition. If ğ‘“âˆ’â‰¤ğ‘“ğ‘—â‰¤ğ‘“+andğ‘ğ‘—â‰¤ğ‘+then this is\na valid partition/hashmap satisfying the requested single fairness\nand collision probability. In the end of each iteration, we modify\nthe boundary ğµğ‘—âˆ—that leads to a valid hashmap with the smallest\nunfairness, i.e., ğ‘—âˆ—=arg minğ‘—:ğ‘“âˆ’â‰¤ğ‘“ğ‘—â‰¤ğ‘“+,ğ‘ğ‘—â‰¤ğ‘+ğœ€ğ‘—.\nBy definition, in each iteration, we find a partition having at\nmost the same unfairness as before. In practice, we expect to find\na hashmap with much smaller unfairness. This is justified in our\nexperiments, Figure 32. For the running time, the algorithm runs in\nğ‘‡iterations. In each iteration, we go through all the ğ‘‚(ğ‘š)bound-\naries, and we compute ğœ€ğ‘—,ğ‘“ğ‘—,ğ‘ğ‘—. Using a max heap to maintain the\nunfairness for every group gğ‘–(similar to the max-heap we used in\nthe previous dynamic programming algorithm) we can compute the\nunfairnessğœ€ğ‘—inğ‘‚(logğ‘˜)time. We need the same time to compute\nğ‘“ğ‘—andğ‘ğ‘—. Our algorithm runs in ğ‘‚(ğ‘›+ğ‘‡ğ‘šlogğ‘˜).\nDue to space limitations, we show the cut-based algorithm that\nfinds a(ï¸ğœ€,ğ‘˜(4+log1\nğœ€))ï¸-hashmap in the appendix.\n6 EXPERIMENTS\nIn addition to the theoretical analysis, we conduct extensive exper-\niments on a variety of settings to confirm the fairness and mem-\nory/time efficiency of our proposed algorithms. In short, aligned\nwith the theoretical guarantees shown in the previous sections,\nthe results of our experiments demonstrate the effectiveness and\nefficiency of our algorithms in real-world settings.\n6.1 Experiments Setup\nThe experiments were conducted on a 3.5 GHz Intel Core i9 pro-\ncessor, 128 GB memory, running Ubuntu. The algorithms were\nimplemented in Python 3.\nFor evaluation purposes, we used three real-world and one semi-\nsynthetic datasets to evaluate our algorithms. With the importance\nof the scalability of our proposed methods to large settings in mind,\nwe chose datasets that are large enough to represent real-world\n\nFairHash : A Fair and Memory/Time-efficient Hashmap SIGMODâ€™24, ,\nTable 4: Overview of datasets\nDataset Size Sensitive Attributes No. of Attributes\nAdult [25]âˆ¼49K sex 15\nCompas [1]âˆ¼61K sex,race 29\nDiabetes [101]âˆ¼102K sex 49\nChicagoPop [81] 1M race 5\napplications. For each dataset, we selected the two columns that\nwere most uncorrelated to construct the hashmaps. The values in\neither column are normalized to be in the [0,1]range. As the sensi-\ntive attribute, we follow the existing literature on group fairness\nand study fairness over demographic information such as sexand\nrace. A summary of the datasets is presented in Table 4. For a more\ndetailed description of the datasets, refer to the appendix.\n6.2 Evaluation Plan\nWe evaluate our proposed algorithms based on three metrics: 1)\nunfairness, 2) space, and 3) efficiency (preprocessing time and query\ntime). For each of the above metrics, we study the effect of vary-\ning three variables: dataset size ğ‘›, minority-to-majority ratio, and\nnumber of buckets ğ‘š. In our experiments, we vary ğ‘›from 0.2 to 1.0\nfraction of the original dataset with an increasing step of 0.2. We\nvaryğ‘šfrom 100 to 1000 increasing by 100 at each step and finally,\nthe minority-to-majority ratio from 0.25 to 1.0 increasing by 0.25 at\neach step. Throughout our experiments, while varying a variable,\nwe fix the others as follows: dataset size ğ‘›=0.2Ã—|original dataset|;\nnumber of buckets ğ‘š=100;minority-to-majority ratio =0.25.\nDue to the space limitations, we present the results for two datasets\nfor each setting and present the extended results in the appendix.\nParticularly, for fairness and space evaluation, we report the re-\nsults for Compas andAdult , the fairness benchmark datasets. For\nrun-time evaluation though, we report on the larger-scale datasets,\nDiabetes andChicagoPop . We confirm that we obtained similar\nresults for all datasets.\n6.2.1 Evaluated Algorithms. In our experiments, we evaluate Rank-\ning,Necklace 2ğ‘”,Sweep&Cut algorithms, and CDF-based hashmap\n[70] (referred as Fairness-agnostic ) as the baseline, for all of the\ndatasets using the binary sensitive attributes ( sexor binary race ).\nWe also evaluate the Ranking andSweep&Cut algorithms us-\ningCompas dataset with race attribute to demonstrate that our\nalgorithms extend to non-binary sensitive attributes. Given the\npotentially large number of rankings, we use the sampled vectors\napproach for Ranking . Particularly, we report the results based on\ntwo samples of vectors of size 100 and 1000. Finally, we evaluate\nthe effectiveness of our local-search based heuristic on the output\ntheRanking algorithm.\n6.3 Unfairness Evaluations\nWe start our experiments by evaluating our algorithms for unfair-\nness. Recall that Ranking returns anğœ€ğ‘…-unfair hashmap while\nSweep&Cut andNecklace 2ğ‘”output 0-unfair hashmaps, i.e., ğœ€=0.\nIn the first experiment, we study the effect of varying dataset size ğ‘›\non the unfairness. As shown in Figures 3 and 4, irrespective of the\ndataset size, Sweep&Cut andNecklace 2ğ‘”always exhibit zero un-\nfairness. On the other hand, Ranking , while improving comparedtoFairness-agnostic , still shows a small degree of unfairness that,\nsimilar to the baseline, decreases as the size of the dataset grows.\nWe also studied the impact of increasing the number of sampled\nvectors in Figures 29 and 30 and noticed a consistent decrease in the\nunfairness with the increasing number of sampled vectors. Next,\nwe study the effect of the minority-to-majority ratio on unfairness.\nThe results are illustrated in Figures 5 and 6 with Sweep&Cut and\nNecklace 2ğ‘”showing no unfairness, while Ranking reducing the\nunfairness compared to Fairness-agnostic . It is worth mention-\ning that all unfairness values approach to zero when the dataset\nincludes an equal number of records from each group. This further\naccentuates the role of unequal base rate [ 68] in unfairness. Last but\nnot least, we evaluate the effect of increasing the number of buckets\nğ‘šon unfairness (Figures 7 and 8). Sweep&Cut andNecklace 2ğ‘”are\nindependent of the number of buckets and show zero unfairness as\nğ‘šincreases. Fairness-agnostic andRanking unfairness values\nhowever increase in a linear fashion as ğ‘šgrows. Consistent with\nthe two previous experiments, we observe that Ranking methods\nmoderately improve the unfairness. Overall, confirming our theo-\nretical analysis, Sweep&Cut andNecklace 2ğ‘”are preferred from\nthe fairness perspective.\n6.4 Space Evaluations\nNext, we evaluate our algorithms for memory demands a.k.a. space.\nRecall that Ranking is a1-memory hashmaps, meaning that no\nadditional memory is required and the number of boundary points\nis exactlyğ‘šâˆ’1.Necklace 2ğ‘”guarantees the number of cuts to be\nat most 2(ğ‘šâˆ’1)while Sweep&Cut can create up to ğ‘‚(ğ‘›)cuts in\nthe worst-case. We investigated the effect of varying dataset size ğ‘›\n(Figures 9 and 10), minority-to-majority ratio (Figures 11 and 12),\nand number of buckets (Figures 13 and 14) on the required space\nfor each algorithm. As expected the results are consistent with the\ntheoretical bounds. The number of boundaries created by Ranking\nis independent of the dataset size ğ‘›and minority-to-majority ratio\nand only depends on the number of buckets ğ‘šand therefore it is\nalways a constant ( ğ‘šâˆ’1). Our experiments also verify similar results\nforNecklace 2ğ‘”being independent of ğ‘›and minority-to-majority\nratio. Interestingly, in almost all settings, the actual number of cuts\ncreated by Necklace 2ğ‘”is close to the upper-bound 2(ğ‘šâˆ’1). The\nresults for Sweep&Cut however, verify that the major drawback of\nthis algorithm is the memory demands with close to the worst case\nofğ‘‚(ğ‘›)cuts. Lemma 2 provides an upper-bound on the expected\nnumber of cuts as a function of the minority ratio in the data set.\nTo evaluate how tight this upper-bound is in practice, in Figures 11\nand 12, we also present the actual number of cuts while varying the\nminority-to-majority ratios. At least in this experiment, the upper-\nbound was tight as it was always less than 30% larger than the actual\nnumber. In general, if an application requires maintaining the space\nat a minimum while satisfying fairness constraints, as empirically\nobserved, Ranking is the leading alternative. Necklace 2ğ‘”is also a\nfavorable choice as it provides a practical trade-off between fairness\n(0-unfair) and space ( â‰¤2(ğ‘šâˆ’1)cuts).\n6.5 Efficiency Evaluations\nIn this set of experiments, we evaluate our proposed algorithms\nfor efficiency. More specifically, we measure efficiency from two\n\nSIGMODâ€™24, , Shahbazi, et al.\n1 2 3 4\nDataset size (n) 10400.0050.010.0150.02\nFigure 3: Effect of varying\ndataset size ğ‘›on unfairness,\nAdult ,sex\n2 4 6\nDataset size (n) 10400.0050.010.0150.020.025Figure 4: Effect of varying\ndataset size ğ‘›on unfairness,\nCompas ,sex\n0.20.40.60.8\nRatio00.0050.010.0150.02Figure 5: Effect of varying\nminority-to-majority ratio on\nunfairness, Adult ,sex\n0.20.40.60.8\nRatio00.0050.010.0150.020.0250.03Figure 6: Effect of varying\nminority-to-majority ratio on\nunfairness, Compas ,sex\n0 500 1000\nNumber of buckets (m)00.050.10.150.2\nFigure 7: Effect of varying\nnumber of buckets ğ‘šon un-\nfairness, Adult ,sex\n0 500 1000\nNumber of buckets (m)00.050.10.150.20.25Figure 8: Effect of varying\nnumber of buckets ğ‘šon un-\nfairness, Compas ,sex\n1 2 3 4\nDataset size (n) 104102103104Number of cuts200Figure 9: Effect of varying\ndataset size ğ‘›on space, Adult\n23456\nDataset size (n) 104102103104Number of cutsFigure 10: Effect of varying\ndataset size ğ‘›on space, Com-\npas\n0.2 0.4 0.6 0.8 1\nRatio102103104Number of cutsTheoretical Upper-Bound for Sweep & Cut\nFigure 11: Effect of varying\nminority-to-majority ratio on\nspace, Adult\n0.2 0.4 0.6 0.8 1\nRatio102103104Number of cutsTheoretical Upper-Bound for Sweep & CutFigure 12: Effect of varying\nminority-to-majority ratio on\nspace, Compas\n0 500 1000\nNumber of buckets (m)102103104Number of cutsFigure 13: Effect of varying\nnumber of buckets ğ‘šon space,\nAdult\n0 500 1000\nNumber of buckets (m)102103104Number of cutsFigure 14: Effect of varying\nnumber of buckets ğ‘šon space,\nCompas\nperspectives: 1) the preprocessing time that is required to construct\nthe fair hashmap, and 2) the query time needed to return a hash\n(bucket) for new records when the hashmap is constructed.\n6.5.1 Preprocessing Time. We start our efficiency experiments by\nrevisiting the preprocessing time complexity of the proposed algo-\nrithms. Ranking has a time complexity of ğ‘‚(ğ‘›2logğ‘›)in 2D while\nSweep&Cut andNecklace 2ğ‘”both run in ğ‘‚(ğ‘›logğ‘›). In our first\nexperiment, we study the impact of varying the dataset size ğ‘›. First,\nin Figures 15 and 16, one can confirm that, overall, the run-time\nincreases with the dataset size. For Ranking , the exact time de-\npends on the number of rankings generated. This is also evident in\nFigures 31, where we study the impact of increasing the number of\nsampled vectors on the preprocessing time. Next, as demonstrated\nin Figures 17 and 18, we confirm that the preprocessing time is\nindependent of the minority-to-majority ratio. As with the preced-\ning experiment, we expect that varying ğ‘šshould not impact therun-time of any of the algorithms, which is consistent with our\nexperiment results in Figures 19 and 20. Overall, in time-sensitive\napplications, both Sweep&Cut andNecklace 2ğ‘”offer the fastest\nresults, all the while ensuring 0-unfairness.\n6.5.2 Query Time. Recall that the output of our algorithms consists\nof a sequence of boundaries along with a corresponding set of\nhash buckets. After constructing the hashmap, obtaining the hash\nbucket for a new query is a simple process: just execute a binary\nsearch on the boundaries and retrieve the bucket linked to the\nboundaries within which the query point resides. Therefore, query\ntime is inğ‘‚(log|ğµ|)and only depends on the number of boundaries.\nOur empirical results are consistent with the preceding analysis,\nconfirming that query time remains independent of both dataset size\n(see Figures 21 and 22) and the minority-to-majority ratio (refer to\nFigures 23 and 24). The increase in the number of buckets ( ğ‘š) leads\nto a logarithmic growth in query time across all our algorithms, as\n\nFairHash : A Fair and Memory/Time-efficient Hashmap SIGMODâ€™24, ,\n0 5 10\nDataset size (n) 104100102Time (sec)\nFigure 15: Effect of varying\ndataset size ğ‘›on preprocessing\ntime, Diabetes\n0 2 4 6 8\nDataset size (n) 105100102104Time (sec)Figure 16: Effect of varying\ndataset size ğ‘›on preprocessing\ntime, ChicagoPop\n0.2 0.4 0.6 0.8 1\nRatio10-1100101102103Time (sec)Figure 17: Effect of varying\nminority-to-majority ratio on\npreprocessing time, Diabetes\n0.2 0.4 0.6 0.8 1\nRatio100102104Time (sec)Figure 18: Effect of minority-\nto-majority ratio on prepro-\ncessing time, ChicagoPop\n0 500 1000\nNumber of buckets (m)10-1100101102103Time (sec)\nFigure 19: Effect of varying\nnumber of buckets ğ‘šon pre-\nprocessing time, Diabetes\n0 500 1000\nNumber of buckets (m)100101102103Time (sec)Figure 20: Effect of varying\nnumber of buckets ğ‘šon pre-\nprocessing time, ChicagoPop\n0 5 10\nDataset size (n) 10400.511.522.5Time (sec)10-6Figure 21: Effect of varying\ndataset size ğ‘›on query time,\nDiabetes\n0 2 4 6 8\nDataset size (n) 10500.511.522.5Time (sec)10-6Figure 22: Effect of varying\ndataset size ğ‘›on query time,\nChicagoPop\n0.2 0.4 0.6 0.8 1\nRatio00.511.522.5Time (sec)10-6\nFigure 23: Effect of varying\nminority-to-majority on query\ntime, Diabetes\n0.2 0.4 0.6 0.8 1\nRatio00.511.522.5Time (sec)10-6Figure 24: Effect of varying\nminority-to-majority on query\ntime, ChicagoPop\n0 500 1000\nNumber of buckets (m)00.511.522.5Time (sec)10-6Figure 25: Effect of varying\nnumber of buckets ğ‘šon query\ntime, Diabetes\n0 500 1000\nNumber of buckets (m)00.511.522.5Time (sec)10-6Figure 26: Effect of varying\nnumber of buckets ğ‘šon query\ntime, ChicagoPop\nFairness-AgnosticRanking Necklace\nSweep & Cut00.050.10.15\nFigure 27: Learning setting:\nUnfairness evaluation over\nheld out data, Adult\nFairness-AgnosticRanking Necklace\nSweep & Cut00.20.40.60.81Figure 28: Learning setting:\nUnfairness evaluation over\nheld out data, ChicagoPop\n101102103\nNumber of rank vectors1.21.41.61.822.22.410-3\nRankingFigure 29: Effect of varying\nnumber of sampled vectors on\nunfairness, Adult\n101102103\nNumber of rank vectors345610-3\nRankingFigure 30: Effect of varying\nnumber of sampled vectors on\nunfairness, Compas ,sex\n\nSIGMODâ€™24, , Shahbazi, et al.\n101102103\nNumber of rank vectors020406080100120Time (sec)Ranking\nFigure 31: Effect of varying\nnumber of sampled vectors\non preprocessing time, Adult\n0 50 100\nNumber of iterations00.0050.010.015UnfairnessPairwise Fairness\nSingle FairnessFigure 32: Pairwise unfair-\nness reduction using the local-\nsearch heuristic, Adult\ndepicted in Figures 25 and 26. Thanks to the logarithmic efficiency\nof binary search, all our algorithms offer remarkably fast query\ntimes, with with practically negligible variations.\n6.6 Local-search-based Heuristic Evaluation\nIn this experiment, we apply the local-search-based heuristic to the\nboundaries generated by the Ranking algorithm on an instance of\nAdult dataset. We run the heuristic in 1000 iterations, with single\nfairness lower and upper bounds as ğ‘“âˆ’=0,ğ‘“+=0.05respectively\nand the collision probability upper bound as ğ‘+=0.05. The results\nare shown in Figure 32. The local-search-based heuristic effectively\nenhances pairwise fairness by making minimal adjustments to the\nbin boundaries, incurring a slight cost in single fairness within 100\niterations before it concludes.\n6.7 Learning Settings Evaluation\nSo far in our experiments, we assumed that the algorithms have\naccess to the entire input set. In this experiment, we demonstrate\nthat our methods can work in expectation if an unbiased sample\nset from the input set is provided. To do so, we partition the input\ndatasets into training and test sets with a ratio of 0.8 to 0.2. Next,\nwe utilize our algorithms to create a hashmap on the training set.\nWe then use the test set for evaluation: each test entry is queried on\nthe constructed hashmap to identify their buckets. Having created\na hashmap that exclusively contains the test entries, we proceed to\nmeasure the pairwise unfairness. The results are illustrated in Fig-\nures 27 and 28. Although all methods demonstrate an enhancement\nin unfairness compared to the Fairness-agnostic baseline, the\nmost notable improvement is observed with Necklace 2ğ‘”, where\nthe unfairness decreases from 0.81 to 0.03 for the ChicagoPop\nand from 0.15 to 0.007 for Adult . Aligned with our previous re-\nsults, Ranking only moderately improves the unfairness. Although\nSweep&Cut consistently improves the unfairness in the learned\nsettings, depending on the number of cuts it generates, it may ex-\nhibit signs of overfitting based on the number of cuts it generates.\nThis tendency becomes more apparent, especially when dealing\nwith large training data, as illustrated in Figure 28. However, this\noverfitting phenomenon is mitigated when the size of training\ndata is smaller and the number of cuts is reduced, resulting in a\nsubstantial decrease in unfairness, as depicted in Figure 27.7 RELATED WORK\nHashing: Hashing has a long history in computer science [ 41,104].\nHashing-based algorithms and data structures find many applica-\ntions in various areas such as theory, machine learning, computer\ngraphics, computational geometry and databases [ 4,37,40,46,51,\n69,75]. Due to its numerous applications, the design of efficient\nhash functions with theoretical guarantees are of significant impor-\ntance [ 2,3,13,27,29,47,48,66,71,105]. In traditional hashmaps,\nthe goal is to design a hash function that maps a key to a random\nvalue in a specified output range. The goal is to minimize the num-\nber of collisions, where a collision occurs when multiple keys get\nmapped to the same output value. There are several well-known\nschemes such as chaining, probing, and cuckoo hashing to handle\ncollisions. Recently, machine learning is used to learn a proper\nhash function [ 70,78,89]. In a typical scenario, a set of samples is\nreceived and they learn the CDF of the underlying data distribu-\ntion. Then the hashmap is created by partitioning the range into\nequal-sized buckets. It has been shown that such learned index\nstructures [ 70,89], can outperform traditional hashmaps on prac-\ntical workloads. However, to the best of our knowledge, none of\nthese hashmap schemes can handle fair hashing with theoretical\nguarantees. Finally, learning has been used to obtain other data\nstructures as well, such as ğµ-trees [ 70] or bloom filters [ 70,78,103].\nAlgorithmic Fairness: Fairness in data-driven systems has been\nstudied by various research communities but mostly in machine\nlearning (ML) [ 23,76,87]. Most of the existing work is on training a\nML model that satisfies some fairness constraints. Some pioneering\nfair-ML efforts include [ 35,36,53,55,61,67,106]. Biases in data has\nalso been studied extensively [ 17,18,84,90,94,100] to ensure data\nhas been prepared responsibly [ 79,91,92,95]. Recent studies of fair\nalgorithm design include fair clustering [ 5,28,32,42,43,64,74,93],\nfairness in resource allocation and facility location problem [ 30,\n49,57,62,65,110], min cut [ 72], max cover [ 14], game theoretic\napproaches [ 7,50,109], hiring [ 11,88], ranking [ 15,16,59,98,99,\n107], recommendation [ 39,73,102], representation learning [ 63],\netc.\nFairness in data structures is significantly under studied, with\nthe existing work being limited to [ 19â€“21], which study individual\nfairness in near-neighbor search. Particularly, a rejection sampling\ntechnique has been added on top of local sensitive hashing (LSH)\nthat equalizes the retrieval chance for all points in the ğœŒ-vicinity\nof a query point, independent of how close those are to the query\npoint. To the best of our knowledge, we are the first to study group\nfairness in hashing and more generally in data structure design.\n8 FINAL REMARKS AND FUTURE WORK\nIn this paper we studied hashmaps through the lens of fairness and\nproposed several fair and memory/time efficient algorithms. Some\nthe interesting directions for future work are as following.\nMemory-efficient 0-unfair hashmaps for more than two groups :\nOur ranking-based algorithms do not depend on the number of\ngroups, however, those are not 0-unfair. The Sweep&Cut algorithm\nalso does not depend on the number of groups and it is even 0-\nunfair. Its memory requirement, however, can be as high as ğ‘‚(ğ‘›).\nThe performance of the necklace splitting algorithm, on the other\n\nFairHash : A Fair and Memory/Time-efficient Hashmap SIGMODâ€™24, ,\nhand, depends on the number of groups. While for two groups, the\nnumber of boundaries is independent from ğ‘›and at most 2(ğ‘šâˆ’1),\nthe state-of-the-art algorithm for more than two groups suddenly\nincreases this requirement by a factor of ğ‘‚(log(ğ‘›)). Developing a\nfair(0,ğ‘)-hashmap for this case, for a constant value of ğ‘, remains\nan interesting open problem for future work.\nBeyond the class of Linear Ranking functions : We developed our\nranking-based algorithms using the class of linear ranking functions.\nIt would be interesting to expand the scope to more general non-\nlinear classes (such as monotonic functions). Indeed one can add\nnon-linear attributes before running our ranking-based algorithms.\nBut this will increase the number of dimensions, exponentially\nreducing its run time.\nLower-bounds and trade-offs on (ğœ€,ğ›¼): Our ranking-based algo-\nrithms satisfy 1-memory requirement but cannot achieve 0-unfairness.\nThe cut-based algorithms, on the other hand, are 0-unfair but re-\nquire additional memory. This suggests a trade-off between fairness\nand memory requirements. Last but not least, formally studying\nthis trade-off and identifying the lower-bound Pareto frontier for\nfairness and memory is an interesting future work.\nACKNOWLEDGEMENTS\nThis work was supported in part by the National Science Founda-\ntion, Grant No. 2107290, and the CAHSI-Google IRP Award. The\nauthors would like to thank the anonymous reviewers and the\nmeta-reviewer for their invaluable feedback.\nREFERENCES\n[1]2015. COMPAS Recidivism Risk Score Data and Analysis. www.propublica.org/\ndatastore/dataset/compas-recidivism-risk-score-data-and-analysis.\n[2]Anders Aamand, Jakob BÃ¦k Tejs Knudsen, Mathias BÃ¦k Tejs Knudsen, Peter\nMichael Reichstein Rasmussen, and Mikkel Thorup. 2020. Fast hashing with\nstrong concentration bounds. In Proceedings of the 52nd Annual ACM SIGACT\nSymposium on Theory of Computing . 1265â€“1278.\n[3]Anders Aamand and Mikkel Thorup. 2019. Non-empty bins with simple tabula-\ntion hashing. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on\nDiscrete Algorithms . SIAM, 2498â€“2512.\n[4]Thomas D Ahle and Jakob BT Knudsen. 2020. Subsets and supermajorities:\nOptimal hashing-based set similarity search. In 2020 IEEE 61st Annual Symposium\non Foundations of Computer Science (FOCS) . IEEE, 728â€“739.\n[5]Sara Ahmadian, Alessandro Epasto, Marina Knittel, Ravi Kumar, Mohammad\nMahdian, Benjamin Moseley, Philip Pham, Sergei Vassilvitskii, and Yuyan Wang.\n2020. Fair hierarchical clustering. Advances in Neural Information Processing\nSystems 33 (2020), 21050â€“21060.\n[6]Saif Al-Kuwari, James H Davenport, and Russell J Bradford. 2011. Cryptographic\nhash functions: Recent design trends and security notions. Cryptology ePrint\nArchive (2011).\n[7]EncarnaciÃ³n Algaba, Vito Fragnelli, and JoaquÃ­n SÃ¡nchez-Soriano. 2019. The\nShapley value, a paradigm of fairness. Handbook of the Shapley value (2019),\n17â€“29.\n[8]Noga Alon. 1987. Splitting necklaces. Advances in Mathematics 63, 3 (1987),\n247â€“253.\n[9]Noga Alon and Andrei Graur. 2021. Efficient splitting of necklaces. In 48th\nInternational Colloquium on Automata, Languages, and Programming (ICALP\n2021) . Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r Informatik.\n[10] Noga Alon and Douglas B West. 1986. The Borsuk-Ulam theorem and bisection\nof necklaces. Proc. Amer. Math. Soc. 98, 4 (1986), 623â€“628.\n[11] Mohammad Reza Aminian, Vahideh Manshadi, and Rad Niazadeh. 2023. Fair\nmarkovian search. Available at SSRN 4347447 (2023).\n[12] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2022. Machine\nbias. In Ethics of data and analytics . Auerbach Publications, 254â€“264.\n[13] Sepehr Assadi, Martin Farach-Colton, and William Kuszmaul. 2023. Tight\nbounds for monotone minimal perfect hashing. In Proceedings of the 2023 Annual\nACM-SIAM Symposium on Discrete Algorithms (SODA) . SIAM, 456â€“476.\n[14] Abolfazl Asudeh, Tanya Berger-Wolf, Bhaskar DasGupta, and Anastasios\nSidiropoulos. 2023. Maximizing coverage while ensuring fairness: A tale ofconflicting objectives. Algorithmica 85, 5 (2023), 1287â€“1331.\n[15] Abolfazl Asudeh, HV Jagadish, Gerome Miklau, and Julia Stoyanovich. 2018.\nOn obtaining stable rankings. Proceedings of the VLDB Endowment 12, 3 (2018).\n[16] Abolfazl Asudeh, HV Jagadish, Julia Stoyanovich, and Gautam Das. 2019. De-\nsigning fair ranking schemes. In Proceedings of the 2019 international conference\non management of data . 1259â€“1276.\n[17] Abolfazl Asudeh, Zhongjun Jin, and HV Jagadish. 2019. Assessing and remedying\ncoverage for a given dataset. In 2019 IEEE 35th International Conference on Data\nEngineering (ICDE) . IEEE, 554â€“565.\n[18] Abolfazl Asudeh, Nima Shahbazi, Zhongjun Jin, and HV Jagadish. 2021. Iden-\ntifying insufficient data coverage for ordinal continuous-valued attributes. In\nProceedings of the 2021 international conference on management of data . 129â€“141.\n[19] Martin Aumuller, Sariel Har-Peled, Sepideh Mahabadi, Rasmus Pagh, and\nFrancesco Silvestri. 2021. Fair near neighbor search via sampling. ACM SIGMOD\nRecord 50, 1 (2021), 42â€“49.\n[20] Martin AumÃ¼ller, Sariel Har-Peled, Sepideh Mahabadi, Rasmus Pagh, and\nFrancesco Silvestri. 2022. Sampling a Near Neighbor in High Dimensionsâ€”Who\nis the Fairest of Them All? ACM Transactions on Database Systems (TODS) 47, 1\n(2022), 1â€“40.\n[21] Martin AumÃ¼ller, Rasmus Pagh, and Francesco Silvestri. 2020. Fair near neighbor\nsearch: Independent range sampling in high dimensions. In Proceedings of the\n39th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems\n(PODS) . 191â€“204.\n[22] Agathe Balayn, Christoph Lofi, and Geert-Jan Houben. 2021. Managing bias and\nunfairness in data for decision support: a survey of machine learning and data\nengineering approaches to identify and mitigate bias and unfairness within data\nmanagement and analytics systems. The VLDB Journal 30, 5 (2021), 739â€“768.\n[23] Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2017. Fairness in machine\nlearning. Nips tutorial 1 (2017), 2017.\n[24] Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and machine\nlearning: Limitations and opportunities . MIT Press.\n[25] Barry Becker and Ronny Kohavi. 1996. Adult. UCI Machine Learning Repository.\nDOI: https://doi.org/10.24432/C5XW20.\n[26] Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie\nHoude, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta,\nAleksandra Mojsilovic, et al .2018. AI Fairness 360: An extensible toolkit for\ndetecting, understanding, and mitigating unwanted algorithmic bias. arXiv\npreprint arXiv:1810.01943 (2018).\n[27] Michael A Bender, MartÃ­n Farach-Colton, John Kuszmaul, William Kuszmaul,\nand Mingmou Liu. 2022. On the optimal time/space tradeoff for hash tables. In\nProceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing .\n1284â€“1297.\n[28] Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani.\n2019. Fair algorithms for clustering. Advances in Neural Information Processing\nSystems 32 (2019).\n[29] Ioana Oriana Bercea and Guy Even. 2022. An extendable data structure for\nincremental stable perfect hashing. In Proceedings of the 54th Annual ACM\nSIGACT Symposium on Theory of Computing . 1298â€“1310.\n[30] VÃ­ctor Blanco and Ricardo GÃ¡zquez. 2023. Fairness in maximal covering location\nproblems. Computers & Operations Research 157 (2023), 106287.\n[31] Burton H Bloom. 1970. Space/time trade-offs in hash coding with allowable\nerrors. Commun. ACM 13, 7 (1970), 422â€“426.\n[32] Matteo BÃ¶hm, Adriano Fazzone, Stefano Leonardi, and Chris Schwiegelshohn.\n2020. Fair clustering with multiple colors. arXiv preprint arXiv:2002.07892\n(2020).\n[33] Andrei Z Broder. 1997. On the resemblance and containment of documents.\nInProceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.\n97TB100171) . IEEE, 21â€“29.\n[34] Jehoshua Bruck, Jie Gao, and Anxiao Jiang. 2006. Weighted bloom filter. In 2006\nIEEE International Symposium on Information Theory . IEEE, 2304â€“2308.\n[35] Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ra-\nmamurthy, and Kush R Varshney. 2017. Optimized pre-processing for discrimi-\nnation prevention. Advances in neural information processing systems 30 (2017).\n[36] L Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K Vishnoi. 2019.\nClassification with fairness constraints: A meta-algorithm with provable guaran-\ntees. In Proceedings of the conference on fairness, accountability, and transparency .\n319â€“328.\n[37] Moses Charikar and Paris Siminelakis. 2019. Multi-resolution hashing for fast\npairwise summations. In 2019 IEEE 60th Annual Symposium on Foundations of\nComputer Science (FOCS) . IEEE, 769â€“792.\n[38] Bernard Chazelle. 2000. The discrepancy method: randomness and complexity .\nCambridge University Press.\n[39] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiang-\nnan He. 2023. Bias and debias in recommender system: A survey and future\ndirections. ACM Transactions on Information Systems 41, 3 (2023), 1â€“39.\n[40] Lijie Chen, Ce Jin, R Ryan Williams, and Hongxun Wu. 2022. Truly Low-Space\nElement Distinctness and Subset Sum via Pseudorandom Hash Functions. In\nProceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms\n\nSIGMODâ€™24, , Shahbazi, et al.\n(SODA) . SIAM, 1661â€“1678.\n[41] Lianhua Chi and Xingquan Zhu. 2017. Hashing techniques: A survey and\ntaxonomy. ACM Computing Surveys (Csur) 50, 1 (2017), 1â€“36.\n[42] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. 2017.\nFair clustering through fairlets. Advances in neural information processing\nsystems 30 (2017).\n[43] Eden ChlamtÃ¡Ä, Yury Makarychev, and Ali Vakilian. 2022. Approximating fair\nclustering with cascaded norm objectives. In Proceedings of the 2022 annual\nACM-SIAM symposium on discrete algorithms (SODA) . SIAM, 2664â€“2683.\n[44] Ondrej Chum, Michal Perdâ€™och, and Jiri Matas. 2009. Geometric min-hashing:\nFinding a (thick) needle in a haystack. In 2009 IEEE Conference on Computer\nVision and Pattern Recognition . IEEE, 17â€“24.\n[45] Graham Cormode and Shan Muthukrishnan. 2005. An improved data stream\nsummary: the count-min sketch and its applications. Journal of Algorithms 55,\n1 (2005), 58â€“75.\n[46] Artur Czumaj, Shaofeng H-C Jiang, Robert Krauthgamer, Pavel Vesel `y, and\nMingwei Yang. 2022. Streaming facility location in high dimension via geometric\nhashing. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science\n(FOCS) . IEEE, 450â€“461.\n[47] Ivan Bjerre DamgÃ¥rd. 1989. A design principle for hash functions. In Conference\non the Theory and Application of Cryptology . Springer, 416â€“427.\n[48] Manik Dhar and Zeev Dvir. 2022. Linear Hashing with â„“âˆguarantees and\ntwo-sided Kakeya bounds. In 2022 IEEE 63rd Annual Symposium on Foundations\nof Computer Science (FOCS) . IEEE Computer Society, 419â€“428.\n[49] Kate Donahue and Jon Kleinberg. 2020. Fairness and utilization in allocating re-\nsources with uncertain demand. In Proceedings of the 2020 conference on fairness,\naccountability, and transparency . 658â€“668.\n[50] Kate Donahue and Jon Kleinberg. 2023. Fairness in model-sharing games. In\nProceedings of the ACM Web Conference 2023 . 3775â€“3783.\n[51] Ran Duan, Hongxun Wu, and Renfei Zhou. 2022. Faster matrix multiplication\nvia asymmetric hashing. arXiv preprint arXiv:2210.10173 (2022).\n[52] Marianne Durand and Philippe Flajolet. 2003. Loglog counting of large cardi-\nnalities. In Algorithms-ESA 2003: 11th Annual European Symposium, Budapest,\nHungary, September 16-19, 2003. Proceedings 11 . Springer, 605â€“617.\n[53] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard\nZemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations\nin theoretical computer science conference . ACM, 214â€“226.\n[54] Herbert Edelsbrunner. 1987. Algorithms in combinatorial geometry . Vol. 10.\nSpringer Science & Business Media.\n[55] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and\nSuresh Venkatasubramanian. 2015. Certifying and removing disparate impact.\nInProceedings of the 21th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining . ACM, 259â€“268.\n[56] Philippe Flajolet, Ã‰ric Fusy, Olivier Gandouet, and FrÃ©dÃ©ric Meunier. 2007. Hyper-\nloglog: the analysis of a near-optimal cardinality estimation algorithm. Discrete\nmathematics & theoretical computer science Proceedings (2007).\n[57] Pranay Gorantla, Kunal Marwaha, and Santhoshini Velusamy. 2023. Fair al-\nlocation of a multiset of indivisible items. In Proceedings of the 2023 Annual\nACM-SIAM Symposium on Discrete Algorithms (SODA) . SIAM, 304â€“331.\n[58] Tristin K Green. 2011. The future of systemic disparate treatment law. Berkeley\nJ. Emp. & Lab. L. 32 (2011), 395.\n[59] Yifan Guan, Abolfazl Asudeh, Pranav Mayuram, HV Jagadish, Julia Stoyanovich,\nGerome Miklau, and Gautam Das. 2019. Mithraranking: A system for respon-\nsible ranking design. In Proceedings of the 2019 International Conference on\nManagement of Data . 1913â€“1916.\n[60] Sariel Har-Peled. 2011. Geometric approximation algorithms . Number 173.\nAmerican Mathematical Soc.\n[61] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in\nsupervised learning. In Advances in neural information processing systems . 3315â€“\n3323.\n[62] Yuzi He, Keith Burghardt, Siyi Guo, and Kristina Lerman. 2020. Inherent trade-\noffs in the fair allocation of treatments. arXiv preprint arXiv:2010.16409 (2020).\n[63] Yuzi He, Keith Burghardt, and Kristina Lerman. 2020. A geometric solution to\nfair representations. In Proceedings of the AAAI/ACM Conference on AI, Ethics,\nand Society . 279â€“285.\n[64] Sedjro Salomon Hotegni, Sepideh Mahabadi, and Ali Vakilian. 2023. Approx-\nimation Algorithms for Fair Range Clustering. In International Conference on\nMachine Learning . PMLR, 13270â€“13284.\n[65] Yanmin Jiang, Xiaole Wu, Bo Chen, and Qiying Hu. 2021. Rawlsian fairness in\npush and pull supply chains. European Journal of Operational Research 291, 1\n(2021), 194â€“205.\n[66] Praneeth Kacham, Rasmus Pagh, Mikkel Thorup, and David P Woodruff. 2023.\nPseudorandom Hashing for Space-bounded Computation with Applications in\nStreaming. arXiv preprint arXiv:2304.06853 (2023).\n[67] Faisal Kamiran and Toon Calders. 2012. Data preprocessing techniques for\nclassification without discrimination. Knowledge and Information Systems 33, 1\n(2012), 1â€“33.[68] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2017. Inherent\ntrade-offs in the fair determination of risk scores. Proceedings of Innovations in\nTheoretical Computer Science (ITCS) (2017).\n[69] Donald Ervin Knuth. 1997. The art of computer programming . Vol. 3. Pearson\nEducation.\n[70] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 international\nconference on management of data . 489â€“504.\n[71] William Kuszmaul. 2022. A hash table without hash functions, and how to\nget the most out of your random bits. In 2022 IEEE 63rd Annual Symposium on\nFoundations of Computer Science (FOCS) . IEEE, 991â€“1001.\n[72] Jason Li, Danupon Nanongkai, Debmalya Panigrahi, and Thatchaphol Saranurak.\n2023. Near-Linear Time Approximations for Cut Problems via Fair Cuts. In\nProceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms\n(SODA) . SIAM, 240â€“275.\n[73] Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, Juntao Tan, Shuchang\nLiu, and Yongfeng Zhang. 2022. Fairness in recommendation: A survey. arXiv\npreprint arXiv:2205.13619 (2022).\n[74] Yury Makarychev and Ali Vakilian. 2021. Approximation algorithms for socially\nfair clustering. In Conference on Learning Theory . PMLR, 3246â€“3264.\n[75] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking\nlearned indexes. Proceedings of the VLDB Endowment 14, 1 (2020), 1â€“13.\n[76] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram\nGalstyan. 2021. A survey on bias and fairness in machine learning. ACM\ncomputing surveys (CSUR) 54, 6 (2021), 1â€“35.\n[77] FrÃ©dÃ©ric Meunier. 2014. Simplotopal maps and necklace splitting. Discrete\nMathematics 323 (2014), 14â€“26.\n[78] Michael Mitzenmacher. 2018. A model for learned bloom filters and optimizing\nby sandwiching. Advances in Neural Information Processing Systems 31 (2018).\n[79] Fatemeh Nargesian, Abolfazl Asudeh, and HV Jagadish. 2021. Tailoring data\nsource distributions for fairness-aware data integration. Proceedings of the VLDB\nEndowment 14, 11 (2021), 2519â€“2532.\n[80] Fatemeh Nargesian, Abolfazl Asudeh, and HV Jagadish. 2022. Responsible Data\nIntegration: Next-generation Challenges. In Proceedings of the 2022 International\nConference on Management of Data . 2458â€“2464.\n[81] Khanh Duy Nguyen, Nima Shahbazi, and Abolfazl Asudeh. 2023. PopSim: An\nIndividual-level Population Simulator for Equitable Allocation of City Resources.\nSDM Workshop on Algorithmic Fairness in Artificial intelligence, Machine learning,\nand Decision making (2023).\n[82] Hamed Nilforoshan, Johann D Gaebler, Ravi Shroff, and Sharad Goel. 2022.\nCausal conceptions of fairness and their consequences. In International Confer-\nence on Machine Learning . PMLR, 16848â€“16887.\n[83] Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju, Vasileios Iosifidis, Wolfgang Ne-\njdl, Maria-Esther Vidal, Salvatore Ruggieri, Franco Turini, Symeon Papadopou-\nlos, Emmanouil Krasanakis, et al .2020. Bias in data-driven artificial intelligence\nsystemsâ€”An introductory survey. Wiley Interdisciplinary Reviews: Data Mining\nand Knowledge Discovery 10, 3 (2020), e1356.\n[84] Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre KÄ±cÄ±man. 2019.\nSocial data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in\nbig data 2 (2019), 13.\n[85] Anna Ostlin and Rasmus Pagh. 2003. Uniform hashing in constant time and\nlinear space. In Proceedings of the thirty-fifth annual ACM symposium on Theory\nof computing . 622â€“628.\n[86] Anna Pagh, Rasmus Pagh, and S Srinivasa Rao. 2005. An optimal Bloom filter\nreplacement.. In Soda , Vol. 5. Citeseer, 823â€“829.\n[87] Dana Pessach and Erez Shmueli. 2022. A review on fairness in machine learning.\nACM Computing Surveys (CSUR) 55, 3 (2022), 1â€“44.\n[88] Manish Raghavan, Solon Barocas, Jon Kleinberg, and Karen Levy. 2020. Mitigat-\ning bias in algorithmic hiring: Evaluating claims and practices. In Proceedings of\nthe 2020 conference on fairness, accountability, and transparency . 469â€“481.\n[89] Ibrahim Sabek, Kapil Vaidya, Dominik Horn, Andreas Kipf, Michael Mitzen-\nmacher, and Tim Kraska. 2022. Can Learned Models Replace Hash Functions?\nProceedings of the VLDB Endowment 16, 3 (2022), 532â€“545.\n[90] Babak Salimi, Bill Howe, and Dan Suciu. 2019. Data management for causal\nalgorithmic fairness. arXiv preprint arXiv:1908.07924 (2019).\n[91] Babak Salimi, Bill Howe, and Dan Suciu. 2020. Database repair meets algorithmic\nfairness. ACM SIGMOD Record 49, 1 (2020), 34â€“41.\n[92] Babak Salimi, Luke Rodriguez, Bill Howe, and Dan Suciu. 2019. Interventional\nfairness: Causal database repair for algorithmic fairness. In Proceedings of the\n2019 International Conference on Management of Data . 793â€“810.\n[93] Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler. 2020. Fair core-\nsets and streaming algorithms for fair k-means. In Approximation and Online\nAlgorithms: 17th International Workshop, WAOA 2019, Munich, Germany, Sep-\ntember 12â€“13, 2019, Revised Selected Papers 17 . Springer, 232â€“251.\n[94] Nima Shahbazi, Yin Lin, Abolfazl Asudeh, and HV Jagadish. 2023. Representation\nBias in Data: A Survey on Identification and Resolution Techniques. Comput.\nSurveys (2023).\n\nFairHash : A Fair and Memory/Time-efficient Hashmap SIGMODâ€™24, ,\n[95] Suraj Shetiya, Ian P Swift, Abolfazl Asudeh, and Gautam Das. 2022. Fairness-\naware range queries for selecting unbiased data. In ICDE . IEEE, 1423â€“1436.\n[96] Alan Siegel. 1989. On universal classes of fast high performance hash functions,\ntheir time-space tradeoff, and their applications. In 30th Annual Symposium on\nFoundations of Computer Science . IEEE Computer Society, 20â€“25.\n[97] John Edward Silva. 2003. An overview of cryptographic hash functions and\ntheir uses. GIAC 6 (2003).\n[98] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of exposure in rankings.\nInProceedings of the 24th ACM SIGKDD international conference on knowledge\ndiscovery & data mining . 2219â€“2228.\n[99] Ashudeep Singh and Thorsten Joachims. 2019. Policy learning for fairness in\nranking. Advances in neural information processing systems 32 (2019).\n[100] Julia Stoyanovich, Serge Abiteboul, Bill Howe, HV Jagadish, and Sebastian\nSchelter. 2022. Responsible data management. Commun. ACM 65, 6 (2022),\n64â€“74.\n[101] Beata Strack, Jonathan P DeShazo, Chris Gennings, Juan L Olmo, Sebastian\nVentura, Krzysztof J Cios, John N Clore, et al .2014. Impact of HbA1c measure-\nment on hospital readmission rates: analysis of 70,000 clinical database patient\nrecords. BioMed research international 2014 (2014).\n[102] Ian P Swift, Sana Ebrahimi, Azade Nova, and Abolfazl Asudeh. 2022. Maximizing\nfair content spread via edge suggestion in social networks. Proceedings of the\nVLDB Endowment 15, 11 (2022), 2692â€“2705.\n[103] Kapil Vaidya, Eric Knorr, Michael Mitzenmacher, and Tim Kraska. 2020. Parti-\ntioned Learned Bloom Filters. In International Conference on Learning Represen-\ntations .\n[104] Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. 2014. Hashing\nfor similarity search: A survey. arXiv preprint arXiv:1408.2927 (2014).\n[105] Chaoping Xing and Chen Yuan. 2023. Beating the Probabilistic Lower Bound\non q-Perfect Hashing. Combinatorica (2023), 1â€“20.\n[106] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P\nGummadi. 2017. Fairness beyond disparate treatment & disparate impact: Learn-\ning classification without disparate mistreatment. In Proceedings of the 26th\nInternational Conference on World Wide Web . International World Wide Web\nConferences Steering Committee, 1171â€“1180.\n[107] Meike Zehlike, Ke Yang, and Julia Stoyanovich. 2022. Fairness in ranking, part i:\nScore-based ranking. Comput. Surveys 55, 6 (2022), 1â€“36.\n[108] Qin Zhang. 2022. Technical perspective: Can data structures treat us fairly?\nCommun. ACM 65, 8 (2022), 82â€“82.\n[109] Yan Zhao, Kai Zheng, Jiannan Guo, Bin Yang, Torben Bach Pedersen, and Chris-\ntian S Jensen. 2021. Fairness-aware task assignment in spatial crowdsourcing:\nGame-theoretic approaches. In 2021 IEEE 37th International Conference on Data\nEngineering (ICDE) . IEEE, 265â€“276.\n[110] Liping Zhou, Na Geng, Zhibin Jiang, and Xiuxian Wang. 2019. Public hospital\ninpatient room allocation and patient scheduling considering equity. IEEE\nTransactions on Automation Science and Engineering 17, 3 (2019), 1124â€“1139.\nAPPENDIX\nA EXTENDED DETAILS OF\nDISCREPANCY-BASED HASHMAPS\nA.1 Faster randomized algorithm for small ğ‘š\nThe dynamic programming algorithm we designed depends on\nğ‘›ğ‘‘+2, which is too large. Here we propose a faster randomized algo-\nrithm to return an ((1+ğ›¿)ğœ€ğ·+ğ›¾,1)-hashmap, with running time\nwhich is strictly linear on ğ‘›. We use the notion of ğ›¾-approximation.\nLetğ‘¤be a vector in Rğ‘‘, and letğ‘ƒğ‘¤be the projections of all points\ninğ‘ƒontoğ‘¤. We also use ğ‘ƒğ‘¤to denote the ordering of the points on\nğ‘¤. LetIğ‘¤be the set of all possible intervals on the line supporting\nğ‘¤. Let(ğ‘ƒğ‘¤,Iğ‘¤)be the range space with elements ğ‘ƒğ‘¤and ranges\nIğ‘¤. Letğ´ğ‘–be a random (multi-)subset of gğ‘–of sizeğ‘‚(ğ›¾âˆ’2logğœ™âˆ’1)\nwhere at each step a point from gğ‘–is chosen uniformly at ran-\ndom (with replacement). It is well known [ 38,60] thatğ´ğ‘–is ağ›¾-\napproximation of(gğ‘–,Iğ‘¤)with probability at least 1âˆ’ğœ™, satisfying\n||gğ‘–âˆ©ğ¼|\n|gğ‘–|âˆ’|ğ´ğ‘–âˆ©ğ¼|\n|ğ´ğ‘–||â‰¤ğ›¾, for everyğ¼âˆˆIğ‘¤.\nWe discretize the range [0,1]creating the discrete values of\ndiscrepancy ğ¸={0,1\nğ‘›,(1+ğ›¿)1\nğ‘›,..., 1}and run a binary search on\nğ¸. Letğ›¼âˆˆğ¸be the discrepancy we consider in the current iteration\nof the binary search. We design a randomized algorithm such that,ifğ›¼>ğœ€ğ·it returns a valid hashmap. Otherwise, it does not return\na valid hashmap. The algorithm returns the correct answer with\nprobability at least 1âˆ’1/ğ‘›ğ‘‚(1).\nAlgorithm. For eachğ‘–, we get a sample set ğ´ğ‘–âŠ†gğ‘–of size\nğ‘‚(ğ‘š2\nğ›¾2logğ‘›ğ‘‘+2). Letğ´=âˆªğ‘–ğ´ğ‘–. Then we run the binary search\nwe described in the previous paragraph. Let ğ›¼be the discrepancy\nwe currently check in the binary search. Let Wğ´be the set of all\ncombinatorially different vectors with respect to ğ´as defined in\nSection 3. For each ğ‘¤âˆˆWğ´we define the ordering ğ´ğ‘¤. Using a\nstraightforward dynamic programming algorithm we search for\na partition of ğ‘šbuckets on ğ´ğ‘¤such that every bucket contains\nbetween[(1âˆ’ğ›¼âˆ’ğ›¾/2)|ğ´ğ‘–|\nğ‘š,(1+ğ›¼+ğ›¾/2)|ğ´ğ‘–|\nğ‘š]points for each gğ‘–.\nIf it returns one such partition, then we continue the binary search\nfor smaller values of ğ›¼. Otherwise, we continue the binary search\nfor larger values of ğ›¼. In the end, we return the partition/hashmap\nof the last vector ğ‘¤that the algorithm returned a valid hashmap.\nAnalysis. Letğ‘¤be a vector in Rğ‘‘and the ordering ğ‘ƒğ‘¤. LetIğ‘¤\nbe the intervals defined on the line supporting the vector ğ‘¤. By\ndefinition,ğ´ğ‘–is ağ›¾/(2ğ‘š)-approximation of (ğ‘ƒğ‘¤,Iğ‘¤)with proba-\nbility at least 1âˆ’1/ğ‘›ğ‘‘+2. Since there are ğ‘˜<ğ‘›groups and ğ‘‚(ğ‘›ğ‘‘)\ncombinatorially different vectors (with respect to ğ‘ƒ) using the union\nbound we get the next lemma.\nLemma 4.ğ´ğ‘–is ağ›¾/(2ğ‘š)-approximation in (ğ‘ƒğ‘¤,Iğ‘¤)for every\nğ‘¤âˆˆRğ‘‘and everyğ‘–âˆˆ[1,ğ‘˜]with probability at least 1âˆ’1/(2ğ‘›).\nLetğ‘¤âˆ—be a vector in Rğ‘‘such that there exists an ğœ€ğ·-discrepancy\npartition of ğ‘ƒğ‘¤âˆ—. Letğ‘¤â€²be the vector inWğ´that belongs in the\nsame cell of the arrangement of A(Î›ğ´)withğ‘¤âˆ—, where Î›ğ´is the\nset of dual hyperplanes of ğ´. We notice that the ordering ğ´ğ‘¤â€²is\nexactly the same with the ordering ğ´ğ‘¤âˆ—.\nBy definition, there exists a partition of ğ‘šbuckets/intervals on\nğ‘ƒğ‘¤âˆ—such that each interval contains [(1âˆ’ğœ€ğ·)|gğ‘–|\nğ‘š,(1+ğœ€ğ·)|gğ‘–|\nğ‘š]\nitems from group gğ‘–, for every group gğ‘–. Letğ¼1,ğ¼2,...ğ¼ğ‘šbe theğ‘š\nintervals defining the ğœ€ğ·-discrepancy partition. By the definition\nofğ´, it holds that\n||gğ‘–âˆ©ğ¼ğ‘—|\n|gğ‘–|âˆ’|ğ´ğ‘–âˆ©ğ¼ğ‘—|\n|ğ´ğ‘–||â‰¤ğ›¾/(2ğ‘š)â‡”\n1âˆ’ğœ€ğ·âˆ’ğ›¾/2\nğ‘šâ‰¤|ğ´ğ‘–âˆ©ğ¼ğ‘—|\n|ğ´ğ‘–||â‰¤1+ğœ€ğ·+ğ›¾/2\nğ‘š\nfor everyğ‘–âˆˆ[1,ğ‘˜]andğ‘—âˆˆ[1,ğ‘š]with probability at least 1âˆ’1/ğ‘›ğ‘‘.\nSince, the ordering of ğ´ğ‘¤âˆ—is the same with the ordering of ğ´ğ‘¤â€²it\nalso holds that there are ğ¼â€²\n1,...,ğ¼â€²ğ‘šon the line supporting ğ‘¤â€²such\nthat\n1âˆ’ğœ€ğ·âˆ’ğ›¾/2\nğ‘šâ‰¤|ğ´ğ‘–âˆ©ğ¼â€²\nğ‘—|\n|ğ´ğ‘–||â‰¤1+ğœ€ğ·+ğ›¾/2\nğ‘š\nwith probability at least 1âˆ’1/ğ‘›ğ‘‘. Letğ›¼âˆˆğ¸be the discrepancy\nwe currently check in the binary search such that ğ›¼>ğœ€ğ·. Since\nthere exists a(ğœ€ğ·+ğ›¾/2)-discrepancy partition in ğ´ğ‘¤â€²it also holds\nthat the straightforward dynamic programming algorithm executed\nwith respect to vector ğ‘¤â€²will return a partition satisfying [(1âˆ’\nğ›¼âˆ’ğ›¾/2)|ğ´ğ‘–|\nğ‘š,(1+ğ›¼+ğ›¾/2)|ğ´ğ‘–|\nğ‘š]for each gğ‘–with probability at least\n1âˆ’1/ğ‘›ğ‘‘.\n\nSIGMODâ€™24, , Shahbazi, et al.\nNext, we show that any hashmap returned by our algorithm\nsatisfies(1+ğ›¿)ğœ€ğ·+ğ›¾discrepancy, with high probability. Let ğ›¼âˆˆğ¸\nbe the discrepancy in the current iteration of the binary search such\nthatğ›¼âˆˆ[ğœ€ğ·,(1+ğ›¿)ğœ€ğ·](there is always such ğ›¼inğ¸). Letğ‘¤Â¯âˆˆWğ´\nbe the vector such that our algorithm returns a valid partition with\nrespect toğ´ğ‘¤Â¯(as shown previously). Let ğ¼Â¯1,...,ğ¼ Â¯ğ‘šbe the intervals\non the returned valid partition defined on the line supporting ğ‘¤Â¯\nsuch that\n(1âˆ’ğ›¼âˆ’ğ›¾/2)|ğ´ğ‘–|\nğ‘šâ‰¤|ğ¼Â¯ğ‘—âˆ©ğ´ğ‘–|â‰¤( 1+ğ›¼+ğ›¾/2)|ğ´ğ‘–|\nğ‘š, (2)\nfor each gğ‘–andğ‘—âˆˆ[1,ğ‘š]. Using Lemma 4, we get that\n||gğ‘–âˆ©ğ¼Â¯ğ‘—|\n|gğ‘–|âˆ’|ğ´ğ‘–âˆ©ğ¼Â¯ğ‘—|\n|ğ´ğ‘–||â‰¤ğ›¾/(2ğ‘š) (3)\nfor everyğ‘–âˆˆ[1,ğ‘˜]and everyğ‘—âˆˆ[1,ğ‘š]with probability at least\n1âˆ’1/(2ğ‘›). From Equation (2) and Equation (3) it follows that\n1âˆ’(1+ğ›¿)ğœ€ğ·âˆ’ğ›¾\nğ‘šâ‰¤|gğ‘–âˆ©ğ¼Â¯ğ‘—|\n|gğ‘–|â‰¤1+(1+ğ›¿)ğœ€ğ·+ğ›¾\nğ‘š.\nfor everyğ‘–âˆˆ[1,ğ‘˜]andğ‘—âˆˆ[1,ğ‘š]with probability at least 1âˆ’1/ğ‘›.\nThe correctness of the algorithm follows.\nWe construct ğ´inğ‘‚(ğ‘˜ğ‘š2\nğ›¾2logğ‘›)time. Then the binary search\nruns forğ‘‚(loglogğ‘›\nğ›¿)rounds. In each round of the binary search we\nspendğ‘‚(ğ‘˜ğ‘‘ğ‘š2ğ‘‘\nğ›¾2ğ‘‘logğ‘‘+1ğ‘›)time to constructA(Î›ğ´). The straightfor-\nward dynamic programming algorithm takes ğ‘‚(ğ‘˜2ğ‘š5\nğ›¾4log2ğ‘›)time.\nOverall, the algorithm runs in ğ‘‚(ğ‘›+log(logğ‘›\nğ›¿)ğ‘˜ğ‘‘+2ğ‘š2ğ‘‘+5\nğ›¾2ğ‘‘+4logğ‘‘+2ğ‘›)\ntime.\nTheorem 5. Letğ‘ƒbe a set ofğ‘›tuples in Rğ‘‘and parameters ğ›¿,ğ›¾.\nThere exists an ğ‘‚(ğ‘›+log(logğ‘›\nğ›¿)ğ‘˜ğ‘‘+2ğ‘š2ğ‘‘+5\nğ›¾2ğ‘‘+4logğ‘‘+2ğ‘›)time algorithm\nsuch that, with probability at least 1âˆ’1\nğ‘›it returns an((1+ğ›¿)ğœ€ğ·+ğ›¾,1)-\nhashmapHwith collision probability at most1+(1+ğ›¿)ğœ€ğ·+ğ›¾\nğ‘šand single\nfairness in the range [1âˆ’(1+ğ›¿)ğœ€ğ·âˆ’ğ›¾\nğ‘š,1+(1+ğ›¿)ğœ€ğ·+ğ›¾\nğ‘š].\nA.2 Cut-based\nWe use a direct implementation of the necklace splitting algorithm\nproposed in [ 9] forğ‘˜>2. Even though the authors argue that their\nalgorithm runs in polynomial time, they do not provide an exact\nrunning time analysis. We slightly modify their technique to work\nin our setting providing theoretical guarantees on the collision\nprobability, single fairness, pairwise fairness, and pre-processing\nconstruction time. Skipping the details we give the next result.\nTheorem 6. In the non-binary demographic group cases, there ex-\nists an algorithm that finds a(ï¸ğœ€,ğ‘˜(4+log1\nğœ€))ï¸-hashmap with collision\nprobability within[1\nğ‘š,1+ğœ€\nğ‘š]and single fairness within [1âˆ’ğœ€\nğ‘š,1+ğœ€\nğ‘š]in\nğ‘‚(ğ‘šğ‘˜3log1\nğœ€+ğ‘˜ğ‘›ğ‘š(ğ‘›+ğ‘š))time. Ifğœ€=1\n3ğ‘›ğ‘š, the algorithm finds a(ï¸0,ğ‘˜(4+logğ‘›))ï¸-hashmap satisfying the collision probability and the\nsingle fairness in ğ‘‚(ï¸ğ‘šğ‘˜3logğ‘›+ğ‘˜ğ‘›ğ‘š(ğ‘›+ğ‘š))ï¸time.B EXTENDED EXPERIMENT RESULTS\nB.1 Datasets\nAdult [25] is a commonly used dataset in the fairness literature\n[26], with column sex={male, female} as the sensitive attribute\nandfnlwgt andeducation-num columns with continuous real and\ndiscrete ordinal values for ordering the tuples. Adult dataset in-\ncludes 15 census data related attributes with âˆ¼49000 records describ-\ning people and is primarily used to predict whether an individualâ€™s\nincome exceeds $50K per year.\nCompas [1] is another benchmark dataset widely used in the fair-\nness literature. This dataset includes âˆ¼61000 records of defendants\nfrom Broward County from 2013 and 2014 with 29 attributes in-\ncluding demographics and criminal history. Compas was origi-\nnally used to predict the likelihood of re-offense by the criminal\ndefendants. In our experiments, we picked sex={male, female}\nandrace={White, Black, Hispanic, other} as the sensitive\nattributes and Person_ID and Raw_Score with discrete ordinal\nand continuous real values as the columns used for building the\nhashmap.\nDiabetes [101] is a dataset ofâˆ¼102K records of patients with dia-\nbetes collected over 10 years from 130 US hospitals and integrated\ndelivery networks. Diabetes has 49 attributes from which we chose\nsex={male, female} as the sensitive attribute and encounter_id\nandpatient_nbr columns with discrete ordinal values for con-\nstructing the hashmap.\nChicagoPop [81] is a semi-synthetic dataset used to simulate\nindividual-level data with demographic information for the city\nof Chicago. This dataset is available in various sizes and in our\nexperiments we use the variant with 1M entries. It has 5 attributes\namong which we use race={White, Black} as the sensitive\nattribute and latitude and longitude columns with contin-\nuous real values for building the hashmap.\nB.2 Non-binary Sensitive Attribute Evaluation\nWe repeated our experiments for Ranking andSweep&Cut algo-\nrithms using the Compas dataset, with the non-binary sensitive\nattribute chosen as race . The results, as illustrated in Figures 45-\n51, align with the findings from our earlier experiments. Consis-\ntent with our expectations, our algorithms exhibit independence\nfrom the number of demographic groups and seamlessly extend to\nnon-binary sensitive attributes without compromising efficiency\nor memory requirements.\n\nFairHash : A Fair and Memory/Time-efficient Hashmap SIGMODâ€™24, ,\n1 2 3 4\nDataset size (n) 10400.0050.010.0150.02\n(a)Adult\n2 4 6\nDataset size (n) 10400.0050.010.0150.020.025 (b)Compas\n0 5 10\nDataset size (n) 104012310-3 (c)Diabetes\n0 2 4 6 8\nDataset size (n) 105012310-3 (d)ChicagoPop\nFigure 33: Effect of varying dataset size ğ‘›on unfairness\n0.20.40.60.8\nRatio00.0050.010.0150.02\n(a)Adult\n0.20.40.60.8\nRatio00.0050.010.0150.020.0250.03 (b)Compas\n0.2 0.4 0.6 0.8 1\nRatio0246810-3 (c)Diabetes\n0.20.40.60.8\nRatio00.0050.010.0150.02 (d)ChicagoPop\nFigure 34: Effect of varying majority to minority ratio on unfairness\n0 500 1000\nNumber of buckets (m)00.050.10.150.2\n(a)Adult\n0 500 1000\nNumber of buckets (m)00.050.10.150.20.25 (b)Compas\n0 500 1000\nNumber of buckets (m)00.020.040.060.08 (c)Diabetes\n0 500 1000\nNumber of buckets (m)00.010.020.030.04 (d)ChicagoPop\nFigure 35: Effect of varying number of buckets ğ‘šon unfairness\n1 2 3 4\nDataset size (n) 104102103104Number of cuts200\n(a)Adult\n23456\nDataset size (n) 104102103104Number of cuts (b)Compas\n0 5 10\nDataset size (n) 104102103104Number of cuts\n200 (c)Diabetes\n0 2 4 6 8\nDataset size (n) 105102104Number of cuts\n200 (d)ChicagoPop\nFigure 36: Effect of varying dataset size ğ‘›on space\n\nSIGMODâ€™24, , Shahbazi, et al.\n0.2 0.4 0.6 0.8 1\nRatio102103104Number of cutsTheoretical Upper-Bound for Sweep & Cut\n(a)Adult\n0.2 0.4 0.6 0.8 1\nRatio102103104Number of cutsTheoretical Upper-Bound for Sweep & Cut (b)Compas\n0.2 0.4 0.6 0.8 1\nRatio102103104Number of cuts (c)Diabetes\n0.2 0.4 0.6 0.8 1\nRatio102103104105Number of cuts (d)ChicagoPop\nFigure 37: Effect of varying majority to minority ratio on space\n0 500 1000\nNumber of buckets (m)102103104Number of cuts\n(a)Adult\n0 500 1000\nNumber of buckets (m)102103104Number of cuts (b)Compas\n0 500 1000\nNumber of buckets (m)102103104Number of cuts (c)Diabetes\n0 500 1000\nNumber of buckets (m)102103104Number of cuts (d)ChicagoPop\nFigure 38: Effect of varying number of buckets ğ‘šon space\n1 2 3 4\nDataset size (n) 10410-2100102Time (sec)\n(a)Adult\n23456\nDataset size (n) 104100102Time (sec) (b)Compas\n0 5 10\nDataset size (n) 104100102Time (sec) (c)Diabetes\n0 2 4 6 8\nDataset size (n) 105100102104Time (sec) (d)ChicagoPop\nFigure 39: Effect of varying dataset size ğ‘›on preprocessing time\n0.2 0.4 0.6 0.8 1\nRatio10-1100101102Time (sec)\n(a)Adult\n0.2 0.4 0.6 0.8 1\nRatio10-210-1100101102Time (sec) (b)Compas\n0.2 0.4 0.6 0.8 1\nRatio10-1100101102103Time (sec) (c)Diabetes\n0.2 0.4 0.6 0.8 1\nRatio100102104Time (sec) (d)ChicagoPop\nFigure 40: Effect of varying majority to minority ratio on preprocessing time\n\nFairHash : A Fair and Memory/Time-efficient Hashmap SIGMODâ€™24, ,\n0 500 1000\nNumber of buckets (m)10-1100101102Time (sec)\n(a)Adult\n0 500 1000\nNumber of buckets (m)10-210-1100101102Time (sec) (b)Compas\n0 500 1000\nNumber of buckets (m)10-1100101102103Time (sec) (c)Diabetes\n0 500 1000\nNumber of buckets (m)100101102103Time (sec) (d)ChicagoPop\nFigure 41: Effect of varying number of buckets ğ‘šon preprocessing time\n1 2 3 4\nDataset size (n) 10400.511.522.5Time (sec)10-6\n(a)Adult\n23456\nDataset size (n) 10400.511.522.5Time (sec)10-6 (b)Compas\n0 5 10\nDataset size (n) 10400.511.522.5Time (sec)10-6 (c)Diabetes\n0 2 4 6 8\nDataset size (n) 10500.511.522.5Time (sec)10-6 (d)ChicagoPop\nFigure 42: Effect of varying dataset size ğ‘›on query time\n0.2 0.4 0.6 0.8 1\nDataset size (n)00.511.522.5Time (sec)10-6\n(a)Adult\n0.2 0.4 0.6 0.8 1\nDataset size (n)00.511.522.5Time (sec)10-6 (b)Compas\n0.2 0.4 0.6 0.8 1\nRatio00.511.522.5Time (sec)10-6 (c)Diabetes\n0.2 0.4 0.6 0.8 1\nRatio00.511.522.5Time (sec)10-6 (d)ChicagoPop\nFigure 43: Effect of varying majority to minority ratio on query time\n0 500 1000\nNumber of buckets (m)00.511.522.5Time (sec)10-6\n(a)Adult\n0 500 1000\nNumber of buckets (m)00.511.522.5Time (sec)10-6 (b)Compas\n0 500 1000\nNumber of buckets (m)00.511.522.5Time (sec)10-6 (c)Diabetes\n0 500 1000\nNumber of buckets (m)00.511.522.5Time (sec)10-6 (d)ChicagoPop\nFigure 44: Effect of varying number of buckets ğ‘šon query time\n\nSIGMODâ€™24, , Shahbazi, et al.\n23456\nDataset size (n) 10400.050.10.150.2\nFigure 45: Effect of varying\ndataset size ğ‘›on unfairness,\nCompas ,race\n0 500 1000\nNumber of buckets (m)00.511.5Figure 46: Effect of varying\nnumber of buckets ğ‘šon un-\nfairness, Compas ,race\n23456\nDataset size (n) 104102103104Number of cutsFigure 47: Effect of varying\ndataset size ğ‘›on space, Com-\npas,race\n0 500 1000\nNumber of buckets (m)102103104Number of cutsFigure 48: Effect of varying\nnumber of buckets ğ‘šon space,\nCompas ,race\n23456\nDataset size (n) 104100102Time (sec)\nFigure 49: Effect of varying\ndataset size ğ‘›on preprocessing\ntime, Compas ,race\n0 500 1000\nNumber of buckets (m)10-1100101102Time (sec)Figure 50: Effect of varying\nnumber of buckets ğ‘šon pre-\nprocessing time, Compas ,race\n23456\nDataset size (n) 10400.511.522.5Time (sec)10-6Figure 51: Effect of varying\ndataset size ğ‘›on query time,\nCompas ,race\n0 500 1000\nNumber of buckets (m)00.511.522.5Time (sec)10-6Figure 52: Effect of varying\nnumber of buckets ğ‘šon query\ntime, Compas ,race\nFairness-AgnosticRanking Necklace\nSweep & Cut00.050.10.15\nFigure 53: Learning setting:\nUnfairness evaluation over\nheld out data, Adult\nFairness-AgnosticRanking Necklace\nSweep & Cut00.020.040.060.080.10.120.14Figure 54: Learning setting:\nUnfairness evaluation over\nheld out data, Compas\nFairness-AgnosticRanking Necklace\nSweep & Cut00.0050.010.0150.020.0250.03Figure 55: Learning setting:\nUnfairness evaluation over\nheld out data, Diabetes\nFairness-AgnosticRanking Necklace\nSweep & Cut00.20.40.60.81Figure 56: Learning setting:\nUnfairness evaluation over\nheld out data, ChicagoPop\n0 50 100\nNumber of iterations00.0050.010.015UnfairnessPairwise Fairness\nSingle Fairness\nFigure 57: Effect of local search\nbased algorithm on unfairness,\nAdult\n0 50 100\nNumber of iterations00.0020.0040.0060.0080.01UnfairnessPairwise Fairness\nSingle FairnessFigure 58: Effect of local search\nbased algorithm on unfairness,\nCompas\n0 20 40 60 80\nNumber of iterations012345Unfairness10-4\nPairwise Fairness\nSingle FairnessFigure 59: Effect of local search\nbased algorithm on unfairness,\nDiabetes\n101102103\nNumber of rank vectors0100200300Time (sec)Figure 60: Effect of varying\nnumber of sampled vectors on\npreprocessing time, Diabetes\n101102103\nNumber of rank vectors0.511.522.510-4\nFigure 61: Effect of varying\nnumber of sampled vectors on\nunfairness, Diabetes\n101102103\nNumber of rank vectors1.21.41.61.822.22.410-3\nRankingFigure 62: Effect of varying\nnumber of sampled vectors on\nunfairness, Adult\n101102103\nNumber of rank vectors345610-3\nRankingFigure 63: Effect of varying\nnumber of sampled vectors on\nunfairness, Compas ,sex\n101102103\nNumber of rank vectors050100150Time (sec)RankingFigure 64: Effect of varying\nnumber of sampled vectors on\npreprocessing time, Compas",
  "textLength": 114382
}