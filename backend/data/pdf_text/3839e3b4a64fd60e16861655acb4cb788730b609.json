{
  "paperId": "3839e3b4a64fd60e16861655acb4cb788730b609",
  "title": "Meta-Learning Deep Energy-Based Memory Models",
  "pdfPath": "3839e3b4a64fd60e16861655acb4cb788730b609.pdf",
  "text": "Published as a conference paper at ICLR 2020\nMETA-LEARNING DEEP\nENERGY -BASED MEMORY MODELS\nSergey Bartunov\nDeepMind\nLondon, United Kingdom\nbartunov@google.comJack W Rae\nDeepMind\nLondon, United Kingdom\njwrae@google.comSimon Osindero\nDeepMind\nLondon, United Kingdom\nosindero@google.com\nTimothy P Lillicrap\nDeepMind\nLondon, United Kingdom\ncountzero@google.com\nABSTRACT\nWe study the problem of learning an associative memory model – a system which is\nable to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are\nstored as attractors of the network dynamics and associative retrieval is performed\nby running the dynamics starting from a query pattern until it converges to an\nattractor. In such models the dynamics are often implemented as an optimization\nprocedure that minimizes an energy function, such as in the classical Hopﬁeld\nnetwork. In general it is difﬁcult to derive a writing rule for a given dynamics\nand energy that is both compressive and fast. Thus, most research in energy-\nbased memory has been limited either to tractable energy models not expressive\nenough to handle complex high-dimensional objects such as natural images, or to\nmodels that do not offer fast writing. We present a novel meta-learning approach\nto energy-based memory models (EBMM) that allows one to use an arbitrary\nneural architecture as an energy model and quickly store patterns in its weights.\nWe demonstrate experimentally that our EBMM approach can build compressed\nmemories for synthetic and natural data, and is capable of associative retrieval\nthat outperforms existing memory systems in terms of the reconstruction error and\ncompression rate.\n1 I NTRODUCTION\nAssociative memory has long been of interest to neuroscience and machine learning communities\n(Willshaw et al., 1969; Hopﬁeld, 1982; Kanerva, 1988). This interest has generated many proposals\nfor associative memory models, both biological and synthetic. These models address the problem\nof storing a set of patterns in such a way that a stored pattern can be retrieved based on a partially\nknown or distorted version. This kind of retrieval from memory is known as auto-association.\nDue to the generality of associative retrieval, successful implementations of associative memory\nmodels have the potential to impact many applications. Attractor networks provide one well-grounded\nfoundation for associative memory models (Amit & Amit, 1992). Patterns are stored in such a way\nthat they become attractors of the update dynamics deﬁned by the network. Then, if a query pattern\nthat preserves sufﬁcient information for association lies in the basin of attraction for the original\nstored pattern, a trajectory initialized by the query will converge to the stored pattern.\nA variety of implementations of the general attractor principle have been proposed. The classical\nHopﬁeld network (Hopﬁeld, 1982), for example, deﬁnes a simple quadratic energy function whose\nparameters serve as a memory. The update dynamics in Hopﬁeld networks iteratively minimize the\nenergy by changing elements of the pattern until it converges to a minimum, typically corresponding\nto one of the stored patterns. The goal of the writing process is to ﬁnd parameter values such that\n1arXiv:1910.02720v2  [stat.ML]  20 Apr 2021\n\nPublished as a conference paper at ICLR 2020\nE(x;¯✓)\nInput xEnergy Ex\nInput xEnergy E✓=w r i t e ({x1,x2,x3};¯✓)E(x;✓)x1x2x3˜xQuery read(˜x;✓)\nFigure 1: A schematic illustration of EBMM. The energy function is modelled by a neural network.\nThe writing rule is then implemented as a weight update, producing parameters \u0012from the initializa-\ntion\u0016\u0012, such that the stored patterns x1,x2,x3become local minima of the energy (see Section 3).\nLocal minima are attractors for gradient descent which implements associative retrieval starting from\na query ~x, in this case a distorted version of x3.\nthe stored patterns become attractors for the optimization process and such that, ideally, no spurious\nattractors are created.\nMany different learning rules have been proposed for Hopﬁeld energy models, and the simplicity\nof the model affords compelling closed-form analysis (Storkey & Valabregue, 1999). At the same\ntime, Hopﬁeld memory models have fundamental limitations: (1) It is not possible to add capacity\nfor more stored patterns by increasing the number of parameters since the number of parameters in a\nHopﬁeld network is quadratic in the dimensionality of the patterns. (2) The model lacks a means of\nmodelling the higher-order dependencies that exist in real-world data.\nIn domains such as natural images, the potentially large dimensionality of an input makes it both\nineffective and often unnecessary to model global dependencies among raw input measurements. In\nfact, many auto-correlations that exist in real-world perceptual data can be efﬁciently compressed\nwithout signiﬁcant sacriﬁce of ﬁdelity using either algorithmic (Wallace, 1992; Candes & Tao,\n2004) or machine learning tools (Gregor et al., 2016; Toderici et al., 2017). The success of existing\ndeep learning techniques suggests a more efﬁcient recipe for processing high-dimensional inputs by\nmodelling a hierarchy of signals with restricted or local dependencies (LeCun et al., 1995). In this\npaper we use a similar idea for building an associative memory: use a deep network’s weights to store\nand retrieve data .\nFast writing rules A variety of energy-based memory models have been proposed since the\noriginal Hopﬁeld network to mitigate its limitations (Hinton et al., 2006b; Du & Mordatch, 2019).\nRestricted Boltzmann Machines (RBMs) (Hinton, 2012) add capacity to the model by introducing\nlatent variables, and deep variants of RBMs (Hinton et al., 2006b; Salakhutdinov & Larochelle,\n2010) afford more expressive energy functions. Unfortunately, training Boltzmann machines remains\nchallenging, and while recent probabilistic models such as variational auto-encoders (Kingma &\nWelling, 2013; Rezende et al., 2014) are easier to train, they nevertheless pay the price for expressivity\nin the form of slow writing. While Hopﬁeld networks memorize patterns quickly using a simple\nHebbian rule, deep probabilistic models are slow in that they rely on gradient training that requires\nmany updates (typically thousands or more) to settle new inputs into the weights of a network. Hence,\nwriting memories via parametric gradient based optimization is not straightforwardly applicable to\nmemory problems where fast adaptation is a crucial requirement. In contrast, and by explicit design,\nour proposed method enjoys fast writing , requiring few parameter updates (we employ just 5 steps)\nto write new inputs into the weights of the net once meta-learning is complete. It also enjoys fast\nreading , requiring few gradient descent steps (again just 5 in our experiments) to retrieve a pattern.\nFurthermore, our writing rules are also fast in the sense that they use O(N)operations to store N\npatterns in the memory – this scaling is the best one can hope for without additional assumptions.\nWe propose a novel approach that leverages meta-learning to enable fast storage of patterns into the\nweights of arbitrarily structured neural networks, as well as fast associative retrieval. Our networks\noutput a single scalar value which we treat as an energy function whose parameters implement a\ndistributed storage scheme. We use gradient-based reading dynamics and meta-learn a writing rule in\nthe form of truncated gradient descent over the parameters deﬁning the energy function. We show\nthat the proposed approach enables compression via efﬁcient utilization of network weights, as well\nas fast-converging attractor dynamics.\n2\n\nPublished as a conference paper at ICLR 2020\n2 R ETRIEVAL IN ENERGY -BASED MODELS\nWe focus on attractor networks as a basis for associative memory. Attractor networks deﬁne update\ndynamics for iterative evolution of the input pattern: x(k+1)=f(x(k)).\nFor simplicity, we will assume that this process is discrete in time and deterministic, however there are\nexamples of both continuous-time (Yoon et al., 2013) and stochastic dynamics (Aarts & Korst, 1988).\nA ﬁxed-point attractor of deterministic dynamics can be deﬁned as a point xfor which it converges,\ni.e.x=f(x). Learning the associative memory in the attractor network is then equivalent to learning\nthe dynamics fsuch that its ﬁxed-point attractors are the stored patterns and the corresponding basins\nof attraction are sufﬁciently wide for retrieval.\nAn energy-based attractor network is deﬁned by the energy function E(x)mapping an input object\nx2X to a real scalar value. A particular model may then impose additional requirements on the\nenergy function. For example if the model has a probabilistic interpretation, the energy function\nis usually a negative unnormalized logarithm of the object probability logp(x) =\u0000E(x) +const ,\nimplying that the energy has to be well-behaved for the normalizing constant to exist. In our case no\nsuch constraints are put on the energy.\nThe attractor dynamics in energy-based models is often implemented either by iterative energy\noptimization (Hopﬁeld, 1982) or sampling (Aarts & Korst, 1988). In the optimization case considered\nfurther in the paper, attractors are conveniently deﬁned as local minimizers of the energy function.\nWhile a particular energy function may suggest a number of different optimization schemes for\nretrieval, convergence to a local minimum of an arbitrary function is NP-hard. Thus, we consider\na class of energy functions that are differentiable on X\u0012Rd, bounded from below and deﬁne the\nupdate dynamics over k= 1;:::;K steps via gradient descent:\nread(~x;\u0012) =x(K);x(k+1)=x(k)\u0000\r(k)rxE(x(k));x(0)=~x: (1)\nWith appropriately set step sizes f\r(k)gK\nk=0this procedure asymptotically converges to a local\nminimum of energy E(x)(Nesterov, 2013). Since asymptotic convergence may be not enough for\npractical applications, we truncate the optimization procedure (1)atKsteps and treat x(K)as a\nresult of the retrieval. While vanilla gradient descent (1)is sufﬁcient to implement retrieval, in our\nexperiments we employ a number of extensions, such as the use of Nesterov momentum and projected\ngradients, which are thoroughly described in Appendix B.\nRelying on the generic optimization procedure allows us to translate the problem of designing update\ndynamics with desirable properties to constructing an appropriate energy function, which in general\nis equally difﬁcult. In the next section we discuss how to tackle this difﬁculty.\n3 M ETA-LEARNING GRADIENT -BASED WRITING RULES\nAs discussed in previous sections, our ambition is to be able to use any scalar-output neural network\nas an energy function for associate retrieval. We assume a parametric model E(x;\u0012)differentiable in\nbothxand\u0012, and bounded from below as a function of x. These are mild assumptions that are often\nmet in the existing neural architectures with an appropriate choice of activation functions, e.g. tanh.\nThe writing rule then compresses input patterns X=fx1;x2;:::; xNginto parameters \u0012such that\neach of the stored patterns becomes a local minimum of E(x;\u0012)or, equivalently, creates a basin of\nattraction for gradient descent in the pattern space.\nThis property can be practically quantiﬁed by the reconstruction error, e.g. mean squared error,\nbetween the stored pattern xand the pattern read(~x;\u0012)retrieved from a distorted version of x:\nL(X;\u0012) =1\nNNX\ni=1Ep(~xijxi)\u0002\njjxi\u0000read(~xi;\u0012)jj2\n2\u0003\n: (2)\nHere we assume a known, potentially stochastic distortion model p(~xjx)such as randomly erasing\ncertain number of dimensions, or salt-and-pepper noise. While one can consider loss (2)as a function\nof network parameters \u0012and call minimization of this loss with a conventional optimization method\n3\n\nPublished as a conference paper at ICLR 2020\nQuery\n Iteration 1\n Iteration 2\n Iteration 3\n Iteration 4\n Iteration 5\n Original\nQuery Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Original130\n120\n110\n100\n90\n80\n70\n60\n50\nEnergy\nFigure 2: Visualization of gradient descent iterations during retrieval of Omniglot characters (largest\nmodel). Four random images are shown from the batch of 64.\na writing rule — it will require many optimization steps to obtain a satisfactory solution and thus\ndoes not fall into our deﬁnition of fast writing rules (Santoro et al., 2016).\nHence, we explore a different approach to designing a fast writing rule inspired by recently proposed\ngradient-based meta-learning techniques (Finn et al., 2017) which we call meta-learning energy-based\nmemory models (EBMM). Namely we perform many write and read optimization procedures with a\nsmall number of iterations for several sets of write and read observations, and backpropagate into the\ninitial parameters of \u0012— to learn a good starting location for fast optimization. As usual, we assume\nthat we have access to the underlying data distribution pd(X)over data batches of interest Xfrom\nwhich we can sample sufﬁciently many training datasets, even if the actual batch our memory model\nwill be used to store (at test time) is not available at the training time (Santoro et al., 2016).\nThe straightforward application of gradient-based meta-learning to the loss (2)is problematic,\nbecause we generally cannot evaluate or differentiate through the expectation over stochasticity of\nthe distortion model in a way that is reliable enough for adaptation, because as the dimensionality of\nthe pattern space grows the number of possible (and representative) distortions grows exponentially.\nInstead, we deﬁne a different writing lossW, minimizing which serves as a proxy for ensuring\nthat input patterns are local minima for the energy E(x;\u0012), but does not require costly retrieval of\nexponential number of distorted queries.\nW(x;\u0012) =E(x;\u0012) +\u000bjjrxE(x;\u0012)jj2\n2+\fjj\u0012\u0000\u0016\u0012jj2\n2: (3)\nAs one can see, the writing loss (3)consists of three terms. The ﬁrst term is simply the energy value\nwhich we would like to be small for stored patterns relative to non-stored patterns. The condition for\nxto be a local minimum of E(x;\u0012)is two-fold: ﬁrst, the gradient at xis zero, which is captured by\nthe second term of the writing loss, and, second, the hessian is positive-deﬁnite. The latter condition\nis difﬁcult to express in a form that admits efﬁcient optimization and we found that meta-learning\nusing just ﬁrst two terms in the writing loss is sufﬁcient. Finally, the third term limits deviation from\ninitial or prior parameters \u0016\u0012which we found helpful from optimization perspective (see Appendix D\nfor more details).\nWe use truncated gradient descent on the writing loss (3) to implement the writing rule:\nwrite (X) =\u0012(T);\u0012(t+1)=\u0012(t)\u0000\u0011(t)1\nNNX\ni=1r\u0012W(xi;\u0012(t));\u0012(0)=\u0016\u0012 (4)\nTo ensure that gradient updates (4)are useful for minimization of the reconstruction error (2)we\ntrain the combination of retrieval and writing rules end-to-end, meta-learning initial parameters \u0016\u0012,\n4\n\nPublished as a conference paper at ICLR 2020\n2K 4K 6K 8K 10K 12K 14K 16K\nMemory size0510152025Hamming error\nMANN\nMemory network\nEBMM conv\nEBMM FC\n(a) Omniglot.\n2K 4K 6K 8K 10K 12K 14K 16K\nMemory size51015202530Square error\n (b) CIFAR.\nFigure 3: Distortion (reconstruction error) vs rate (memory size) analysis on batches of 64 images.\nlearning rate schedules r= (f\r(k)gK\nk=1;f\u0011(t)gT\nt=1)and meta-parameters \u001c= (\u000b;\f)to perform well\non random sets of patterns from the batch distribution pd(X):\nminimize EX\u0018pd(X)[L(X;write (X))]for\u0016\u0012;r;\u001c: (5)\nIn our experiments, p(X)is simply a distribution over batches of certain size Nsampled uniformly\nfrom the training (or testing - during evaluation) set. Parameters \u0012=write (X)are produced by\nstoringXin the memory using the writing procedure (4). Once stored, distorted versions of Xcan\nbe retrieved and we can evaluate and optimize the reconstruction error (2).\nCrucially, the proposed EBMM implements both read(x;\u0012)andwrite (X)operations via truncated\ngradient descent which can be itself differentiated through in order to set up a tractable meta-learning\nproblem. While truncated gradient descent is not guaranteed to converge, reading and writing\nrules are trained jointly to minimize the reconstruction error (2)and thus ensure that they converge\nsufﬁciently fast. This property turns this potential drawback of the method to its advantage over\nprovably convergent, but slow models. It also relaxes the necessity of stored patterns to create too\nwell-behaved basins of attraction because if, for example, a stored pattern creates a nuisance attractor\nin the dangerous proximity of the main one, the gradient descent (1) might successfully pass it with\nappropriately learned step sizes \r.\n4 E XPERIMENTS\nIn this section we experimentally evaluate EBMM on a number of real-world image datasets.\nThe performance of EBMM is compared to a set of relevant baselines: Long-Short Term Mem-\nory (LSTM) (Hochreiter & Schmidhuber, 1997), the classical Hopﬁeld network (Hopﬁeld, 1982),\nMemory-Augmented Neural Networks (MANN) (Santoro et al., 2016) (which are a variant of the\nDifferentiable Neural Computer (Graves et al., 2016)), Memory Networks (Weston et al., 2014),\nDifferentiable Plasticity model of Miconi et al. (2018) (a generalization of the Fast-weights RNN (Ba\net al., 2016)) and Dynamic Kanerva Machine (Wu et al., 2018). Some of these baselines failed to\nlearn at all for real-world images. In the Appendix A.2 we provide additional experiments with\nrandom binary strings with a larger set of representative models.\nThe experimental procedure is the following: we write a ﬁxed-sized batch of images into a memory\nmodel, then corrupt a random block of the written image to form a query and let the model retrieve\nthe originally stored image. By varying the memory size and repeating this procedure, we perform\ndistortion/rate analysis, i.e. we measure how well a memory model can retrieve a remembered pattern\nfor a given memory size. Meta-learning have been performed on the canonical train splits of each\ndataset and testing on the test splits. Batches were sampled uniformly, see Appendix A.7 for the\nperformance study on correlated batches.\nWe deﬁne memory size as a number of float32 numbers used to represent a modiﬁable part of\nthe model. In the case of EBMM it is the subset of all network weights that are modiﬁed by the\ngradient descent (4), for other models it is size of the state, e.g. the number of slots \u0002the slot size for\na Memory Network. To ensure fair comparison, all models use the same encoder (and decoder, when\napplicable) networks, which architectures are described in Appendix C. In all experiments EBMM\nusedK= 5read iterations and T= 5write iterations.\n5\n\nPublished as a conference paper at ICLR 2020\nQuery\n Iteration 1\n Iteration 2\n Iteration 3\n Iteration 4\n Iteration 5\n Original\nQuery Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Original280\n260\n240\n220\n200\n180\n160\nEnergy\nMemNet\nFigure 4: Visualization of gradient descent iterations during retrieval of CIFAR images. The last\ncolumn contains reconstructions from Memory networks (both models use 10k memory).\n4.1 O MNIGLOT CHARACTERS\nWe begin with experiments on the Omniglot dataset (Lake et al., 2015) which is now a standard\nevaluation of fast adaptation models. For simplicity of comparison with other models, we downscaled\nthe images to 32\u000232size and binarized them using a 0:5threshold. We use Hamming distance as\nthe evaluation metric. For training and evaluation we apply a 16\u000216randomly positioned binary\ndistortions (see Figure 2 for example).\nWe explored two versions of EBMM for this experiment that use parts of fully-connected (FC, see\nAppendix C.2) and convolutional (conv, Appendix C.3) layers in a 3-block ResNet (He et al., 2016)\nas writable memory.\nFigure 3a contains the distortion-rate analysis of different models which in this case is the Hamming\ndistance as a function of memory size. We can see that there are two modes in the model behaviour.\nFor small memory sizes, learning a lossless storage becomes a hard problem and all models have\nto ﬁnd an efﬁcient compression strategy, where most of the difference between models can be\nobserved. However, after a certain critical memory size it becomes possible to rely just on the\nautoencoding which in the case of a relatively simple dataset such as Omniglot can be efﬁciently\ntackled by the ResNet architecture we are using. Hence, even Memory Networks that do not employ\nany compression mechanisms beyond using distributed representations can retrieve original images\nalmost perfectly. In this experiment MANN has been able to learn the most efﬁcient compression\nstrategy, but could not make use of larger memory. EBMM performed well both in the high and\nlow compression regimes with convolutional memory being more efﬁcient over the fully-connected\nmemory. Further, in CIFAR and ImageNet experiments we only use the convolutional version of\nEBMM.\nWe visualize the process of associative retrieval in Figure 2. The model successfully detected distorted\nparts of images and clearly managed to retrieve the original pixel intensities. We also show energy\nlevels of the distorted query image, the recalled images through 5 read iterations, and the original\nimage. In most cases we found the energy of the retrieved images matching energy of the originals,\nhowever, an error would occur when they sometimes do not match (see the green example).\n4.2 R EAL IMAGES FROM CIFAR-10\nWe conducted a similiar study on the CIFAR dataset. Here we used the same network architecture as\nin the Omniglot experiment. The only difference in the experimental setup is that we used squared\nerror as an evaluation metric since the data is continuous RGB images.\n6\n\nPublished as a conference paper at ICLR 2020\n4K 7K 9K 12K 14K 16K 18K\nMemory size30405060708090Square error\nMANN\nMemory network\nEnergy memory (conv)\n(a) Distortion-rate analysis on ImageNet.\nQuery\n Original\n EBMM\n MemNet\n DNC\n (b) Retrieval of 64x64 ImageNet images (all models have\n\u001918K memory).\nFigure 5: ImageNet results.\nFigure 3b contains the corresponding distortion-rate analysis. EBMM clearly dominates in the\ncomparison. One important reason for that is the ability of the model to detect the distorted part of\nthe image so it can avoid paying the reconstruction loss for the rest of the image. Moreover, unlike\nOmniglot where images can be almost perfectly reconstructed by an autoencoder with a large enough\ncode, CIFAR images have much more variety and larger channel depth. This makes an efﬁcient joint\nstorage of a batch as important as an ability to provide a good decoding of the stored original.\nGradient descent iterations shown in Figure 4 demonstrate the successful application of the model\nto natural images. Due to the higher complexity of the dataset, the reconstructions are imperfect,\nhowever the original patterns are clearly recognizable. Interestingly, the learned optimization schedule\nstarts with one big gradient step providing a coarse guess that is then gradually reﬁned.\n4.3 I MAGE NET64X64\nWe further investigate the ability of EBMM to handle complex visual datasets by applying the model\nto64\u000264ImageNet. Similarly to the CIFAR experiment, we construct queries by corrupting a\nquarter of the image with 32\u000232random masks. The model is based on a 4-block version of the\nCIFAR network. While the network itself is rather modest compared to existing ImageNet classiﬁers,\nthe sequential training regime resembling large-state recurrent networks prevents us from using\nanything signiﬁcantly bigger than a CIFAR model. Due to prohibitively expensive computations\nrequired by experimenting at this scale, we also had to decrease the batch size to 32.\nThe distortion-rate analysis (Figure 5a) shows the behaviour similar to the CIFAR experiment. EBMM\npays less reconstruction error than other models and MANN demonstrates better performance than\nMemory Networks for smaller memory sizes; however, the asymptotic behaviour of these two models\nwill likely match.\nThe qualitative results are shown in Figure 5b. Despite the arguably more difﬁcult images, EBMM is\nable to capture shape and color information, although not in high detail. We believe this could likely be\nmitigated by using larger models. Additionally, using techniques such as perceptual losses (Johnson\net al., 2016) instead of naive pixel-wise reconstruction errors can improve visual quality with the\nexisting architectures, but we leave these ideas for future work.\n4.4 A NALYSIS OF ENERGY LEVELS\nWe were also interested in whether energy values provided by EBMM are interpretable and whether\nthey can be used for associative retrieval. We took an Omniglot model and inspected energy levels of\ndifferent types of patterns. It appears that, despite not being explicitly trained to, EBMM in many\ncases could discriminate between in-memory and out-of-memory patterns, see Figure 6. Moreover,\ndistorted patterns had even higher energy than simply unknown patterns. Out-of-distribution patterns,\nhere modelled as binarized CIFAR images, can be seen as clear outliers.\n7\n\nPublished as a conference paper at ICLR 2020\n150\n 100\n 50\n 0 50050100150200250\nMemories\nNon-memories\nDistorted memories\nCIFAR images\nFigure 6: Energy distributions of different classes of patterns under an Omniglot model. Memories\nare the patterns written into memory, non-memories are other randomly sampled images and distorted\nmemories are the written patterns distorted as during the retrieval. CIFAR images were produced by\nbinarizing the original RGB images and serve as out-of-distribution samples.\n5 R ELATED WORK\nDeep neural networks are capable of both compression (Parkhi et al., 2015; Kraska et al., 2018),\nand memorizing training patterns (Zhang et al., 2016). Taken together, these properties make\ndeep networks an attractive candidate for memory models, with both exact recall and compressive\ncapabilities. However, there exists a natural trade-off between the speed of writing and the realizable\ncapacity of a model (Ba et al., 2016). Approaches similar to ours in their use of gradient descent\ndynamics, but lacking fast writing, have been proposed by Hinton et al. (2006a) and recently revisited\nby Xie et al. (2016); Nijkamp et al. (2019); Du & Mordatch (2019). Krotov & Hopﬁeld (2016) also\nextended the classical Hopﬁeld network to a larger family of non-quadratic energy functions with\nmore capacity. In general it is difﬁcult to derive a writing rule for a given dynamics equation or an\nenergy model which we attempt to address in this work.\nThe idea of meta-learning (Thrun & Pratt, 2012; Hochreiter et al., 2001) has found many successful\napplications in few-shot supervised (Santoro et al., 2016; Vinyals et al., 2016) and unsupervised\nlearning (Bartunov & Vetrov, 2016; Reed et al., 2017). Our model is particularly inﬂuenced by works\nof Andrychowicz et al. (2016) and Finn et al. (2017), which experiment with meta-learning efﬁcient\noptimization schedules and, perhaps, can be seen as an ultimate instance of this principle since we\nimplement both learning and inference procedures as optimization. Perhaps the most prominent\nexisting application of meta-learning for associative retrieval is found in the Kanerva Machine (Wu\net al., 2018), which combines a variational auto-encoder with a latent linear model to serve as an\naddressable memory. The Kanerva machine beneﬁts from a high-level representation extracted by the\nauto-encoder. However, its linear model can only represent convex combinations of memory slots\nand is thus less expressive than distributed storage realizable in weights of a deep network.\nWe described literature on associative and energy-based memory in Section 1, but other types of\nmemory should be mentioned in connection with our work. Many recurrent architectures aim at\nmaintaining efﬁcient compressive memory (Graves et al., 2016; Rae et al., 2018). Models developed\nby Ba et al. (2016) and Miconi et al. (2018) enable associative recall by combining standard RNNs\nwith structures similar to Hopﬁeld network. And, recently Munkhdalai et al. (2019) explored the idea\nof using arbitrary feed-forward networks as a key-value storage.\nFinally, the idea of learning a surrogate model to deﬁne a gradient ﬁeld useful for a problem of\ninterest has a number of incarnations. Putzky & Welling (2017) jointly learn an energy model\nand an optimizer to perform denoising or impainting of images. Marino et al. (2018) use gradient\ndescent on an energy deﬁned by variational lower bound for improving variational approximations.\nAnd, Belanger et al. (2017) formulate a generic framework for energy-based prediction driven\nby gradient descent dynamics. A detailed explanation of the learning through optimization with\napplications in control can be found in (Amos, 2019).\nModern deep learning made a departure from the earlier works on energy-based models such as\nBoltzmann machines and approach image manipulation tasks using techniques such as aforementioned\nvariational autoencoders or generative adversarial networks (GANs) (Goodfellow et al., 2014). While\n8\n\nPublished as a conference paper at ICLR 2020\nthese models indeed constitute state of the art for learning powerful prior models of data that can\nperform some kind of associative retrieval, they naturally lack fast memory capabilities. In contrast,\nthe approach proposed in this work addresses the problem of jointly learning a strong prior model\nand an efﬁcient memory which can be used in combination with these techniques. For example, one\ncan replace the plain reconstruction error with a perceptual loss incurred by a GAN discriminator or\nuse EBMM to store representations extracted by a V AE.\nWhile both V AEs and GANs can be also be equipped with memory (as done e.g. by Wu et al. (2018)),\nenergy-based formulation allows us to employ arbitrary neural network parameters as an associative\nstorage and make use of generality of gradient-based meta-learning. As we show in the additional\nexperiments on binary strings in Appendix A.2, EBMM is applicable not only to high-dimensional\nnatural data, but also to uniformly generated binary strings where no prior can be useful. At the same\ntime, evaluation of non-memory baselines in Appendix A.3 demonstrates that the combination of a\ngood prior and memory as in EBMM achieves signiﬁcantly better performance than just a prior, even\nsuited with a much larger network.\n6 C ONCLUSION AND FUTURE WORK\nWe introduced a novel learning method for deep associative memory systems. Our method beneﬁts\nfrom the recent progress in deep learning so that we can use a very large class of neural networks both\nfor learning representations and for storing patterns in network weights. At the same time, we are not\nbound by slow gradient learning thanks to meta-learning of fast writing rules. We showed that our\nmethod is applicable in a variety of domains from non-compressible (binary strings; see Appendix)\nto highly compressible (natural images) and that the resulting memory system uses available capacity\nefﬁciently. We believe that more elaborate architecture search could lead to stronger results on par\nwith state-of-the-art generative models.\nThe existing limitation of EBMM is the batch writing assumption, which is in principle possible to\nrelax. This would enable embedding of the model in reinforcement learning agents or into other tasks\nrequiring online-updating memory. Employing more signiﬁcantly more optimization steps does not\nseem to be necessary at the moment, however, scaling up to larger batches or sequences of patterns\nwill face the bottleneck of recurrent training. Implicit differentiation techniques (Liao et al., 2018),\nreversible learning (Maclaurin et al., 2015) or synthetic gradients (Jaderberg et al., 2017) may be\npromising directions towards overcoming this limitation. It would be also interesting to explore a\nstochastic variant of EBMM that could return different associations in the presence of uncertainty\ncaused by compression. Finally, many general principles of learning attractor models with desired\nproperties are yet to be discovered and we believe that our results provide a good motivation for this\nline of research.\nREFERENCES\nEmile Aarts and Jan Korst. Simulated annealing and boltzmann machines. 1988.\nDaniel J Amit and Daniel J Amit. Modeling brain function: The world of attractor neural networks .\nCambridge university press, 1992.\nBrandon Amos. Differentiable optimization-based modeling for machine learning . PhD thesis, PhD\nthesis. Carnegie Mellon University, 2019.\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\nBrendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient\ndescent. In Advances in Neural Information Processing Systems , pp. 3981–3989, 2016.\nAntreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your maml. arXiv preprint\narXiv:1810.09502 , 2018.\nJimmy Ba, Geoffrey E Hinton, V olodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast\nweights to attend to the recent past. In Advances in Neural Information Processing Systems , pp.\n4331–4339, 2016.\n9\n\nPublished as a conference paper at ICLR 2020\nSergey Bartunov and Dmitry P Vetrov. Fast adaptation in generative models with generative matching\nnetworks. arXiv preprint arXiv:1612.02192 , 2016.\nDavid Belanger, Bishan Yang, and Andrew McCallum. End-to-end learning for structured prediction\nenergy networks. In Proceedings of the 34th International Conference on Machine Learning-\nVolume 70 , pp. 429–439. JMLR. org, 2017.\nEmmanuel Candes and Terence Tao. Near optimal signal recovery from random projections: Universal\nencoding strategies? arXiv preprint math/0410542 , 2004.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of\ngated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 , 2014.\nYilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv\npreprint arXiv:1903.08689 , 2019.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. arXiv preprint arXiv:1703.03400 , 2017.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-\ntion processing systems , pp. 2672–2680, 2014.\nAlex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwi ´nska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al.\nHybrid computing using a neural network with dynamic external memory. Nature , 538(7626):471,\n2016.\nKarol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards\nconceptual compression. In Advances In Neural Information Processing Systems , pp. 3549–3557,\n2016.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international\nconference on computer vision , pp. 1026–1034, 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npp. 770–778, 2016.\nGeoffrey Hinton, Simon Osindero, Max Welling, and Yee-Whye Teh. Unsupervised discovery of\nnonlinear structure using contrastive backpropagation. Cognitive science , 30(4):725–731, 2006a.\nGeoffrey E Hinton. A practical guide to training restricted boltzmann machines. In Neural networks:\nTricks of the trade , pp. 599–619. Springer, 2012.\nGeoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief\nnets. Neural computation , 18(7):1527–1554, 2006b.\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):\n1735–1780, 1997.\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.\nInInternational Conference on Artiﬁcial Neural Networks , pp. 87–94. Springer, 2001.\nJohn J Hopﬁeld. Neural networks and physical systems with emergent collective computational\nabilities. Proceedings of the national academy of sciences , 79(8):2554–2558, 1982.\nMax Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David\nSilver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. In\nProceedings of the 34th International Conference on Machine Learning-Volume 70 , pp. 1627–1635.\nJMLR. org, 2017.\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and\nsuper-resolution. In European conference on computer vision , pp. 694–711. Springer, 2016.\n10\n\nPublished as a conference paper at ICLR 2020\nPentti Kanerva. Sparse distributed memory . MIT press, 1988.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114 , 2013.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 International Conference on Management of Data , pp.\n489–504. ACM, 2018.\nDmitry Krotov and John J Hopﬁeld. Dense associative memory for pattern recognition. In Advances\nin Neural Information Processing Systems , pp. 1172–1180, 2016.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science , 350(6266):1332–1338, 2015.\nYann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The\nhandbook of brain theory and neural networks , 3361(10):1995, 1995.\nRenjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow, Raquel Urta-\nsun, and Richard Zemel. Reviving and improving recurrent back-propagation. arXiv preprint\narXiv:1803.06396 , 2018.\nIlya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. arXiv preprint\narXiv:1711.05101 , 2017.\nDougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization\nthrough reversible learning. In International Conference on Machine Learning , pp. 2113–2122,\n2015.\nJoseph Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. arXiv preprint\narXiv:1807.09356 , 2018.\nThomas Miconi, Jeff Clune, and Kenneth O Stanley. Differentiable plasticity: training plastic neural\nnetworks with backpropagation. arXiv preprint arXiv:1804.02464 , 2018.\nTsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned neural\nmemory. ArXiv , abs/1907.09720, 2019.\nYurii Nesterov. Introductory lectures on convex optimization: A basic course , volume 87. Springer\nScience & Business Media, 2013.\nYurii E Nesterov. A method for solving the convex programming problem with convergence rate o\n(1/kˆ 2). In Dokl. akad. nauk Sssr , volume 269, pp. 543–547, 1983.\nErik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. On learning non-convergent short-run mcmc\ntoward energy-based model. arXiv preprint arXiv:1904.09770 , 2019.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al. Deep face recognition. In BMVC ,\nvolume 1, pp. 6, 2015.\nPatrick Putzky and Max Welling. Recurrent inference machines for solving inverse problems. arXiv\npreprint arXiv:1706.04008 , 2017.\nJack W Rae, Sergey Bartunov, and Timothy P Lillicrap. Meta-learning neural bloom ﬁlters. 2018.\nScott Reed, Yutian Chen, Thomas Paine, Aäron van den Oord, SM Eslami, Danilo Rezende, Oriol\nVinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to\nlearn distributions. arXiv preprint arXiv:1710.10304 , 2017.\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and\napproximate inference in deep generative models. arXiv preprint arXiv:1401.4082 , 2014.\nRuslan Salakhutdinov and Hugo Larochelle. Efﬁcient learning of deep boltzmann machines. In\nProceedings of the thirteenth international conference on artiﬁcial intelligence and statistics , pp.\n693–700, 2010.\n11\n\nPublished as a conference paper at ICLR 2020\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International conference on machine\nlearning , pp. 1842–1850, 2016.\nAmos J Storkey and Romain Valabregue. The basins of attraction of a new hopﬁeld learning rule.\nNeural Networks , 12(6):869–876, 1999.\nSebastian Thrun and Lorien Pratt. Learning to learn . Springer Science & Business Media, 2012.\nGeorge Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, and\nMichele Covell. Full resolution image compression with recurrent neural networks. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition , pp. 5306–5314, 2017.\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition , pp. 9446–9454, 2018.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. In Advances in neural information processing systems , pp. 3630–3638, 2016.\nGregory K Wallace. The jpeg still picture compression standard. IEEE transactions on consumer\nelectronics , 38(1):xviii–xxxiv, 1992.\nJason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint\narXiv:1410.3916 , 2014.\nDavid J Willshaw, O Peter Buneman, and Hugh Christopher Longuet-Higgins. Non-holographic\nassociative memory. Nature , 222(5197):960, 1969.\nYan Wu, Gregory Wayne, Karol Gregor, and Timothy Lillicrap. Learning attractor dynamics for\ngenerative memory. In Advances in Neural Information Processing Systems , pp. 9401–9410, 2018.\nJianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In\nInternational Conference on Machine Learning , pp. 2635–2644. PMLR, 2016.\nKiJung Yoon, Michael A Buice, Caswell Barry, Robin Hayman, Neil Burgess, and Ila R Fiete. Speciﬁc\nevidence of low-dimensional continuous attractor dynamics in grid cells. Nature neuroscience , 16\n(8):1077, 2013.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\ndeep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 , 2016.\n12\n\nPublished as a conference paper at ICLR 2020\nTable 1: Number of error bits in retrieved binary patterns.\nMETHOD#PATTERNS16 32 48 64 96\nHOPFIELD NETWORK , HEBB RULE 0.4 5.0 9.8 13.0 16.5\nHOPFIELD NETWORK , STORKEY RULE 0.0 0.9 6.3 11.3 17.1\nHOPFIELD NETWORK ,PSEUDO -INVERSE RULE 0.0 0.0 0.3 4.3 22.5\nDIFFERENTIABLE PLASTICITY (MICONI ET AL ., 2018) 3.0 13.2 20.8 26.3 34.9\nMANN (S ANTORO ET AL ., 2016) 0.1 0.2 1.8 4.25 9.6\nLSTM (H OCHREITER & S CHMIDHUBER , 1997) 30 58 63 64 64\nMEMORY NETWORKS (WESTON ET AL ., 2014) 0.0 0.0 0.0 0.0 10.5\nEBMM RNN 0.0 0.0 0.1 0.5 4.2\nA A DDITIONAL EXPERIMENTAL DETAILS\nWe train all models using AdamW optimizer (Loshchilov & Hutter, 2017) with learning rate 5\u000210\u00005\nand weight decay 10\u00006, all other parameters set to AdamW defaults. We also apply gradient clipping\nby global norm at 0:05. All models were allowed to train for 2\u0002106gradient updates or 1 week\nwhichever ended ﬁrst. All baseline models always made more updates than EBMM.\nOne instance of each model has been trained. Error bars showed on the ﬁgures correspond to 5- and\n95-percentiles computed on a 1000 of random batches.\nIn all experiments we used initialization scheme proposed by He et al. (2015).\nA.1 F AILURE MODES OF BASELINE MODELS\nImage retrieval appeared to be difﬁcult for a number of baselines.\nLSTM failed to train due to quadratic growth of the hidden-to-hidden weight matrix with increase of\nthe hidden state size. Even moderately large hidden states were prohibitive for training on a modern\nGPU.\nDifferential plasticity additionally struggled to train when using a deep representation instead of\nthe raw image data. We hypothesize that it was challenging for the encoder-decoder pair to train\nsimultaneously with the recurrent memory, because in the binary experiment, while not performing\nthe best, the model managed to learn a memorization strategy.\nFinally, the Kanerva machine could not handle the relatively strong noise we used in this task. By\ndesign, Kanerva machine is agnostic to the noise model and is trained simply to maximize the data\nlikelihood, without meta-learning a particular de-noising scheme. In the presence of the strong noise\nit failed to train on sequences longer than 4 images.\nA.2 E XPERIMENTS WITH RANDOM BINARY PATTERNS\nBesides highly-structured patterns such as Omniglot or ImageNet images we also conducted experi-\nments on random binary patterns – the classical setting in which associative memory models have\nbeen evaluated. While such random patterns are not compressible in expectation due to lack of any\ninternal structure, by this experiment we examine the efﬁciency of a learned coding scheme, i.e. how\nwell can each of the models store binary information in the ﬂoating point format.\nWe generate random 128-dimensional patterns, each dimension of which takes values of \u00001or+1\nwith equal probability, corrupt half of the bits and use this as a query for associative retrieval. We\ncompare EBMM employing a simple fully recurrent network (an RNN using the same input at each\niteration, see Appendix C.1) as an energy model, against a classical Hopﬁeld network (Hopﬁeld,\n1982) using different writing rules (Storkey & Valabregue, 1999) and a recently proposed differential\nplasticity model (Miconi et al., 2018). It is worth noting the differentiable plasticity model is a\ngeneralized variant of Fast Weights (Ba et al., 2016), where the plasticity of each activation is\n13\n\nPublished as a conference paper at ICLR 2020\nmodulated separately. We also consider an LSTM (Hochreiter & Schmidhuber, 1997), Memory\nnetwork (Weston et al., 2014) and a Memory-Augmented Neural Network (MANN) used by Santoro\net al. (2016) which is a variant of the DNC (Graves et al., 2016).\nSince the Hopﬁeld network has limited capacity that is strongly tied to input dimensionality and\nthat cannot be increased without adding more inputs, we use its memory size as a reference and\nconstrain all other baseline models to use the same amount of memory. For this task it equals to\n128\u0003(128\u00001)=2 + 128 to parametrize a symmetric matrix and a frequency vector. We measure\nHamming distance between the original and the retrieved pattern for each system, varying the number\nof stored patterns. We found it difﬁcult to train the recurrent baselines on this task, so we let all\nmodels clamp non-distorted bits to their true values at retrieval which signiﬁcantly stabilized training.\nAs we can see from the results shown in Table 1, EBMM learned a highly efﬁcient associative\nmemory. Only the EBMM and the memory network could achieve near-zero error when storing 64\nvectors and even though EBMM could not handle 96 vectors with this number of parameters, it was\nthe most accurate memory model.\nA.3 E VALUATION OF NON -MEMORY BASELINES\nFor a more complete experimental study, we additionally evaluate a number of baselines which have\nno capacity to adapt to patterns that otherwise would be written into the memory but still learn a prior\nover a domain and hence can serve as a form of associative memory.\nOne should note, however, that even though such models can, in principle, achieve relatively good\nperformance by pretraining on large datasets and adopting well-designed architectures, they ultimately\nfail in situations where a strong memory is more important a good prior. One very distinctive example\nof such setting is the binary string experiment presented in the previous section, where no prior can\nbe useful at all. EBMM, in contrast, naturally learn both the prior in the form of shared features and\nmemory in the form of writable weights.\nOur ﬁrst non-memory baseline is a Variational Auto-encoder (V AE) model with 256latent Gaussian\nvariables and Bernoulli likelihood. V AE deﬁnes an energy equal to the negative joint log-likelihood:\nE(x;z) =\u0000logN(zj0;I)\u0000logp(xjz):\nWe consider attractor dynamics similar to the one used by Wu et al. (2018). We start from a\nconﬁguration x(0)=~x;z(0)=\u0016(x), where\u0016(x)is the output of a Gaussian encoder q(zjx) =\nN(zj\u0016(x);diag(\u001b(x))). Then we alternate between updating these two parts of the conﬁguration as\nfollows:\nz(t+1)=z(t)\u0000\rrzE(x;z(t));\nx(t+1)= arg max\nxE(x;z(t+1)):\nSince we use a simple factorized Bernoulli likelihood model, the exact minimization with respect to\nxcan be performed analytically.\nOne can hope that under certain circumstances a well-trained V AE would assign lower energy levels\nto less distorted versions of ~xand an image We used 50, 100 and 200 iterations for Omniglot, CIFAR\nand ImageNet experiments respectively, in each case also performing a grid search for the learning\nrate\r.\nAnother baseline is a Denoising Auto-encoder (DAE) which is trained to reconstruct the original\npattern xfrom its distorted version ~xusing a bottleneck of 256hidden units. This model is trained\nexactly as our model using (2)as an objective and hence, in contrast to V AE, can adapt to a particular\nnoise pattern, which makes it a stronger baseline.\nFinally, we consider the Deep Image Prior (Ulyanov et al., 2018), a method that can be seen as a very\nspecial case of the energy-based model which also iteratively adapts parameters of a convolutional\nnetwork to generate the most plausible reconstruction. We used the network provided by the authors\nand performed 2000 Adam updates for each of the images. Similarly to how it was implemented in\nthe paper, the network was instructed about location of the occluded block which arguably is a strong\nadvantage over all other models.\n14\n\nPublished as a conference paper at ICLR 2020\nBASELINE OMNIGLOT CIFAR IMAGE NET\n25% BLOCK NOISE\nVARIATIONAL AUTO-ENCODER 109.08 544.72 1889.79\nDENOISING AUTO-ENCODER 36.08 24.66 141.07\nDEEPIMAGE PRIOR – – 170.36*\n15% SALT AND PEPPER NOISE\nVARIATIONAL AUTO-ENCODER 84.08 430.33 1385.29\nDENOISING AUTO-ENCODER 9.57 8.82 69.20\nDEEPIMAGE PRIOR – – 467.92\nTable 2: Reconstruction errors (Hamming for Omniglot, squared for CIFAR and ImageNet) for\nnon-memory baselines. *Location of the distorted block was used.\n2K 4K 6K 8K10K 12K 14K 16K\nMemory size01020304050Hamming error\nEBMM conv\nDKM\nFigure 7: Reconstruction error on Omniglot. Dynamic Kanerva Machine is compared to EBMM with\nconvolutional memory. 15% salt and pepper noise is used.\nThe quantitative results of the comparison are provided in the Table 2. Clearly, the most efﬁcient of\nthe baselines is DAE, largely because it was explicitly trained against a known distortion model. In\ncontrast, V AE failed to recover from noise producing a signiﬁcantly shifted observation distribution.\nDeep Image Prior performed signiﬁcantly better, however, since it only adapts to a single distorted\nimage instead of a whole distribution, it could not over-perform DAE with a much simpler architecture.\nAs expected, none of the non-memory baselines performed even comparably to models with memory\nwe consider in the main experiments section.\nA.4 C OMPARISON WITH DYNAMIC KANERVA MACHINE\nAs we reported earlier, Dynamic Kanerva Machine failed to perform better than a random guess\nunder the strong block noise we used in the main paper. In this appendix we evaluate DKM in simpler\nconditions where it can be reasonably compared to EBMM. Thus, we trained DKM on batches of 16\nimages and used a relatively simple 15% salt and pepper noise.\nFigure 7 contains the reconstruction error in the same format as Figure 3a. One can see that DKM\ngenerally performs poorly and does not show a consistent improvement with increasing memory size.\nAs can be seen on Figure 8, DKM is able to retrieve patterns that are visually similar to the originally\nstored ones, but also introduces a fair amount of undesirable variability and occasionally converges to\na spurious pattern. The errors increase with stronger block noise and larger batch sizes.\nA.5 G ENERALIZATION TO DIFFERENT DISTORTION MODELS\nIn this experiment we assess how EBMM trained with one of block noise of one size performs with\ndifferent levels of noise. Figure 9 contains the generalization analysis with different kinds of noise on\nOmniglot.\n15\n\nPublished as a conference paper at ICLR 2020\nFigure 8: An example of a retrieval of a single batch by Dynamic Kanerva Machine. A model with\n12K memory was used. Top: salt and pepper noise, bottom: 16\u000216block noise. First line: original\npatterns, middle line: distorted queries, bottom line: retrieved patterns.\n4×48×812×1216×1620×20\nBlock size01020304050Hamming error\n16x16\n8x8\n(a) Block noise. Batches of 64 images.\n10% 20% 40% 50% 60%\nNoise rate012345Hamming error\n0.4\n0.2 (b) Salt and pepper noise. Batches of 16 images.\nFigure 9: Generalization to different noise intensity on Omniglot. Each line represents a model\ntrained with a certain noise level. All models use 16K memory.\nOne can see that regardless of noise type, EBMM successfully generalizes to weaker noise and\ngeneralization to slightly stronger noise is also observed. One should note though that 20\u000220block\nnoise already covers almost 40% of the image and hence may completely occlude certain characters.\nGenerally we found salt and pepper noise much simpler which originally motivated us to focus on\nblock occlusions. However, as we indicate on the ﬁgure, the model used in the experiment with this\nkind of noise was only trained on batches of 16 images and hence the comparison of absolute values\nbetween Figures 9a and 9b is not possible. The relative performance degradation with increasing\nnoise rate is still representative though.\nWe did not observe generalization from type of noise to another. Perhaps, the very different kinds\nof distortions were not compatible with learned basins of attraction. In general, this is not very\nsurprising as, arguably, no model can be expected to adapt to all possible distortion models without a\nrelevant supervision of some sort.\nFor this experiment we did not anyhow ﬁne-tune model parameters, including the meta-parameters of\nthe reading gradient descent. Hence, it is possible that some amount of ﬁne-tuning and, perhaps, a\ncustom optimization strategy for the modelled energy may lead to an improvement.\n16\n\nPublished as a conference paper at ICLR 2020\nQuery\n Iteration 1\n Iteration 2\n Iteration 3\n Iteration 4\n Iteration 5\n Original\nQuery Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Original120\n110\n100\n90\n80\n70\n60\n50\nEnergy\nFigure 10: Iterative reading on MNIST. The model is learned on Omniglot.\nBATCH SAMPLING 4K MEMORY 5K MEMORY\nUNIFORM (UNCORRELATED ) 7.18 5.37\n2CLASSES (CORRELATED ) 6.22 2.57\nTable 3: Hamming error of models trained on differently constructed Omniglot batches of 32 images.\nA.6 G ENERALIZATION TO MNIST\nA natural question is whether a model trained on one task or dataset strictly overﬁts to its features or\nwhether it can generalize to similar, but previously unseen tasks. One of the standard experiments\nto test this ability is transfer from Omniglot to MNIST, since both datasets consist of handwritten\ncharacters, which, however, differ enough to present a distributional shift.\nDeveloping such transfer capabilities is out of scope for this paper, but a simple check conﬁrmed\nthat EBMM can successfully retrieve upscaled (to match the dimensionality) and binarized MNIST\ncharacters as one can see on Figure 10. One can see that although some amount of artifacts is\nintroduced the retrieved images are clearly recognizable and the energy levels are as adequate as in\nthe Omniglot experiment. As in the previous experiment, we did not ﬁne-tune the model.\nA.7 E XPERIMENTS WITH CORRELATED BATCHES\nOne of the desirable properties of a memory model would be an efﬁcient consolidation of similar\npatterns, e.g. corresponding to the same class of images, in the interest of better compression.\nWe do not observe this property in the models trained on uniformly sampled, uncorrelated batches,\nperhaps because the model was never incentivized to do so. However, we performed a simple\nexperiment where we trained EBMM on the Omniglot images, making sure that each batch of 32\nimages has characters of exactly two classes. For this purpose we employed a modiﬁed convolutional\nmodel (see Appendix C.5) in which the memory weights are located in the second residual layer\ninstead of the third as in the main architecture. As we found experimentally, this way the model could\ncompress more visual correlations in the batch.\nThe results can be found in Table 3. It is evident that training on correlated batches enabled better\ncompression and EBMM is able to learn an efﬁcient consolidation strategy if the meta-learning is set\nup appropriately.\n17\n\nPublished as a conference paper at ICLR 2020\nB R EADING IN EBMM\nB.1 P ROJECTED GRADIENT DESCENT\nWe described the basic reading procedure in section 2, however, there is a number of extensions we\nfound useful in practice.\nSince in all experiments we work with data constrained to the [0;1]interval, one has to ensure that the\nread data also satisﬁes this constraint. One strategy that is often used in the literature is to model the\noutput as an argument to a sigmoid function (logits). This may not work well for values close to the\ninterval boundaries due to vanishing gradient, so instead we adopted a projected gradient descent, i.e.\nx(k+1)=proj(x(k)\u0000\r(k)rxE(x(k)));\nwhere the proj function clips data to the [0;1]interval.\nQuite interestingly, this formulation allows more ﬂexible behavior of the energy function. If a stored\npattern xhas one of the dimensions exactly on the feasible interval boundary, e.g. xj= 0, then\nrxjE(x)does not necessarily have to be zero, since xjwill not be able to go beyond zero. We\nprovide more information on the properties of storied patterns in further appendices.\nB.2 N ESTEROV MOMENTUM\nAnother extension we found useful is to employ Nesterov momentum (Nesterov, 1983) into the\noptimization scheme and we use it in all our experiments.\n^x(k)=project (x(k\u00001)+ (k)v(k\u00001)); v(k)= (k)v(k\u00001)\u0000\r(k)rE(^x(k))\nx(k)=project (v(k)):\nB.3 S TEP SIZES\nTo encourage learning converging attractor dynamics we constrained step sizes \rto be a non-\nincreasing sequence:\n\r(k)=\r(k\u00001)\u001b(\u0011(k)); k> 1\nThen the actual parameters to meta-learn is the initial step size \r(1)and the logits \u0011. We apply a\nsimilar parametrization to the momentum learning rates  .\nB.4 S TEP-WISE RECONSTRUCTION LOSS\nAs has often been found helpful in the literature (Belanger et al., 2017; Antoniou et al., 2018) we\napply the reconstruction loss (2)not just to the ﬁnal iterate of the gradient descent, but to all iterates\nsimultaneously:\nLK(X;\u0012) =KX\nk=11\nNNX\ni=1Eh\njjxi\u0000x(k)\nijj2\n2i\n:\nC A RCHITECTURE DETAILS\nBelow we provide pseudocode for computational graphs of models used in the experiments. All\nmodules containing memory parameters are speciﬁcally named as memory .\nC.1 G ATED RNN\nWe used a fairly standard recurrent architecture only equipped with an update gate as in (Chung et al.,\n2014). We unroll the RNN for 5 steps and compute the energy value from the last hidden state.\n18\n\nPublished as a conference paper at ICLR 2020\nhidden_size = 1024\ninput_size = 128\n# 128 *(128 - 1) / 2 + 128 parameters in total\ndynamic_size = (input_size - 1) // 2\nstate = repeat_batch(zeros(hidden_size))\nmemory = Linear(input_size, dynamic_size)\ngate = Sequential([\nLinear(input_size + hidden_size, hidden_size),\nsigmoid\n])\nstatic = Linear(input_size + hidden_size, hidden_size - dynamic_size)\nfor hop in xrange(5):\nz = concat(x, state)\ndynamic_part = memory(x)\nstatic_part = static(z)\nc = tanh(concat(dynamic_part, static_part))\nu = gate(z)\nstate = u *c + (1 - u) *state\nenergy = Linear(1)(state)\nC.2 R ESNET,FULLY -CONNECTED MEMORY\nchannels = 32\nhidden_size = 512\nrepresentation_size = 512\nstatic_size = representation_size - dynamic_size\nstate = repeat_batch(zeros(hidden_size))\nencoder = Sequential([\nResBlock(channels *1, kernel=[3, 3], stride=2, downscale=False),\nResBlock(channels *2, kernel=[3, 3], stride=2, downscale=False),\nResBlock(channels *3, kernel=[3, 3], stride=2, downscale=False),\nflatten,\nLinear(256),\nLayerNorm()\n])\ngate = Sequential([\nLinear(hidden_size),\nsigmoid\n])\nhidden = Sequential([\nLinear(hidden_size),\ntanh\n])\nx = encoder(x)\nmemory = Linear(input_size, dynamic_size)\ndynamic_part = memory(x)\n19\n\nPublished as a conference paper at ICLR 2020\nstatic_part = Linear(static_size)(x)\nx = tanh(concat(dynamic_part, static_part))\nfor hop in xrange(3):\nz = concat(x, state)\nc = hidden(z)\nc = LayerNorm()(c)\nu = gate(z)\nstate = u *c + (1 - u) *c\nh = tanh(Linear(1024)()(state))\nenergy = Linear(1)(h)\nTheencoder module is also shared with all baseline models together with its transposed version as\na decoder.\nC.3 R ESNET,CONVOLUTIONAL MEMORY\nchannels = 32\nx = ResBlock(channels *1, kernel=[3, 3], stride=2, downscale=True)(x)\nx = ResBlock(channels *2, kernel=[3, 3], stride=2, downscale=True)(x)\ndef resblock_bottleneck(x, channels, bottleneck_channels, downscale=False):\nstatic_size = channels - dynamic_size\nz = x\nx = Conv2D(bottleneck_channels, [1, 1])(x)\nx = LayerNorm()(x)\nx = tanh(x)\nif downscale:\nmemory_part = Conv2D(dynamic_size, kernel=[3, 3], stride=2, downscale=True)(x)\nstatic_part = Conv2D(static_size, kernel=[3, 3], stride=2, downscale=True)(x)\nelse:\nmemory_part = Conv2D(dynamic_size, kernel=[3, 3], stride=1, downscale=False)(x)\nstatic_part = Conv2D(static_size, kernel=[3, 3], stride=1, downscale=False)(x)\nx = concat([static_part, memory_part], -1)\nx = LayerNorm)(x)\nx = tanh(x)\nz = Conv2D(channels, kernel=[1, 1])(z)\nif downscale:\nz = avg_pool(z, [3, 3] + [1], stride=2)\nx += z\nreturn x\nx = resblock_bottleneck(x, channels *4, channels *2, False)\nx = resblock_bottleneck(x, channels *4, channels *2, True)\nrecurrent = Sequential([\nConv2D(hidden_size, kernel=[3, 3], stride=1),\nLayerNorm(),\ntanh\n])\nupdate_gate = Sequential([\nConv2D(hidden_size, kernel=[1, 1], stride=1),\nLayerNorm(),\n20\n\nPublished as a conference paper at ICLR 2020\nsigmoid\n])\nhidden_size = 128\nhidden_state = repeat_batch(zeros(4, 4, hidden_size))\nfor hop in xrange(3):\nz = concat([x, hidden_state], -1)\ncandidate = recurrent(z)\nu = update_gate(z)\nhidden_state = u *candidate + (1. - u) *hidden_state\nx = Linear(1024)(x)\nx = tanh(x)\nenergy = Linear(1)\nC.4 R ESNET, IMAGE NET\nThis network is effectively a slightly larger version of the ResNet with convolutional memory\ndescribed above.\nchannels = 64\ndynamic_size = 8\nx = ResBlock(channels *1, kernel=[3, 3], stride=2, downscale=True)(x)\nx = ResBlock(channels *2, kernel=[3, 3], stride=2, downscale=True)(x)\nx = resblock_bottleneck(x, channels *4, channels *2, True)\nx = resblock_bottleneck(x, channels *4, channels *2, True)\nrecurrent = Sequential([\nConv2D(hidden_size, kernel=[3, 3], stride=1),\nLayerNorm(),\ntanh\n])\nupdate_gate = Sequential([\nConv2D(hidden_size, kernel=[1, 1], stride=1),\nLayerNorm(),\nsigmoid\n])\nhidden_size = 256\nhidden_state = repeat_batch(zeros(4, 4, hidden_size))\nfor hop in xrange(3):\nz = concat([x, hidden_state], -1)\ncandidate = recurrent(z)\nu = update_gate(z)\nhidden_state = u *candidate + (1. - u) *hidden_state\nx = Linear(1024)(x)\nx = tanh(x)\nenergy = Linear(1)\nC.5 R ESNET, CONVOLUTIONAL LOWER -LEVEL MEMORY\nThis architecture is similar to other convolutional architectures with only difference is that dynamic\nweights are in the second residual block.\n21\n\nPublished as a conference paper at ICLR 2020\nchannels = 32\ndynamic_size = 8\ndef resblock(x, channels):\nstatic_size = channels - dynamic_size\nz = x\nmemory_part = Conv2D(dynamic_size, kernel=[3, 3], stride=2, downscale=True)(x)\nstatic_part = Conv2D(static_size, kernel=[3, 3], stride=2, downscale=True)(x)\nx = concat([static_part, memory_part], -1)\nx = LayerNorm)(x)\nx = tanh(x)\nmemory_part = Conv2D(dynamic_size, kernel=[3, 3], stride=2, downscale=True)(x)\nstatic_part = Conv2D(static_size, kernel=[3, 3], stride=2, downscale=True)(x)\ny = concat([static_part, memory_part], -1)\ny += x\ny = nonlinear(LayerNorm)(y))\nx += y\nz = Conv2D(channels, [1, 1])(z)\nz = avg_pool(z, [3, 3], 2)\nx += z\nreturn x\nx = ResBlock(channels *1, kernel=[3, 3], stride=2, downscale=True)(x)\nx = resblock(x, channels *2)\nx = ResBlock(channels *4, kernel=[3, 3], stride=2, downscale=True)(x)\nrecurrent = Sequential([\nConv2D(hidden_size, kernel=[3, 3], stride=1),\nLayerNorm(),\ntanh\n])\nupdate_gate = Sequential([\nConv2D(hidden_size, kernel=[1, 1], stride=1),\nLayerNorm(),\nsigmoid\n])\nhidden_size = 256\nhidden_state = repeat_batch(zeros(4, 4, hidden_size))\nfor hop in xrange(3):\nz = concat([x, hidden_state], -1)\ncandidate = recurrent(z)\nu = update_gate(z)\nhidden_state = u *candidate + (1. - u) *hidden_state\nx = Linear(1024)(x)\nx = tanh(x)\nenergy = Linear(1)\n22\n\nPublished as a conference paper at ICLR 2020\n0 1 2 3 4 5\nIterations (105)0.0050.0100.0150.0200.0250.0300.035LossWith grad loss\nWithout grad loss\nFigure 11: Effect of including the jjrxE(x)jj2term in the writing loss (3) on Omniglot.\nC.6 T HE ROLE OF SKIP -CONNECTIONS IN ENERGY MODELS\nGradient-based meta-learning and EBMM in particular rely on the expressiveness of not just the\nforward pass of a network, but also the backward pass that is used to compute a gradient. This may\nrequire special considerations about the network architecture.\nOne may notice that all energy models considered above have an element of recurrency of some\nsort. While the recurrency itself is not crucial for good performance, skip-connections, of which\nrecurrency is a special case, are.\nWe can illustrate this by considering an energy function of the following form:\nE(x) =o(h(x)); h(x) =f(x) +g(f(x)):\nHere we can think of has a representation from which the energy is computed. We allow the\nrepresentation to be ﬁrst computed as f(x)and then to be reﬁned by adding g(f(x)).\nDuring retrieval, we use gradient of the energy with respect to xwhich can be computed as\nd\ndxE(x) =do\ndhdh\ndx=do\ndh(df\ndx+dg\ndfdf\ndx):\nOne can see, that with a skip-connection the model is able to reﬁne the gradient together with the\nenergy value.\nA simple way of incorporating such skip-connections is via recurrent computation. We allow the\nmodel to use a gating mechanism that can modulate the reﬁnement and prevent from unneces-\nsary updates. We found that usually a small number of recurrent steps (3-5) is enough for good\nperformance.\nD E XPLANATIONS ON THE WRITING LOSS\nOur setting deviates from the standard gradient-based meta-learning as described in (Finn et al., 2017).\nIn particular, we are not using the same loss function (naturally deﬁned by the energy function) in\nadaptation and inference phases. As we explain in section 3, writing loss (3) besides just the energy\nterm also contains the gradient term and the prior term.\nEven though we found it sufﬁcient to use just the energy value as the writing loss, perhaps not\nsurprisingly, minimizing the gradient norm appeared to help optimization especially in the early\ntraining (see Figure 11) and lead to better ﬁnal results.\nWe use an individual learning rate per each writable layer and each of the three loss terms, initialized\nat10\u00004and learned together with other parameters. We used softplus function to ensure that all\nlearning rates remain non-negative.\n23",
  "textLength": 67301
}