{
  "paperId": "c48393627923dbf8f80548b1e9ef3dbdd903070f",
  "title": "Plant-and-Steal: Truthful Fair Allocations via Predictions",
  "pdfPath": "c48393627923dbf8f80548b1e9ef3dbdd903070f.pdf",
  "text": "Plant-and-Steal: Truthful Fair Allocations via Predictions∗\nIlan Reuven Cohen†Alon Eden‡Talya Eden§Arsen Vasilyan¶\nJune 12, 2024\nAbstract\nWe study truthful mechanisms for approximating the Maximin-Share (MMS) allocation of\nagents with additive valuations for indivisible goods. Algorithmically, constant factor approxi-\nmations exist for the problem for any number of agents. When adding incentives to the mix, a\njarring result by Amanatidis, Birmpas, Christodoulou, and Markakis [EC 2017] shows that the\nbest possible approximation for two agents and mitems is ⌊m\n2⌋. We adopt a learning-augmented\nframework to investigate what is possible when some prediction on the input is given. For two\nagents, we give a truthful mechanism that takes agents’ ordering over items as prediction. When\nthe prediction is accurate, we give a 2-approximation to the MMS (consistency), and when the\nprediction is off, we still get an ⌈m\n2⌉-approximation to the MMS (robustness). We further show\nthat the mechanism’s performance degrades gracefully in the number of “mistakes” in the pre-\ndiction; i.e., we interpolate (up to constant factors) between the two extremes: when there are\nno mistakes, and when there is a maximum number of mistakes. We also show an impossibility\nresult on the obtainable consistency for mechanisms with finite robustness. For the general\ncase of n≥2 agents, we give a 2-approximation mechanism for accurate predictions, with re-\nlaxed fallback guarantees. Finally, we give experimental results which illustrate when different\ncomponents of our framework, made to insure consistency and robustness, come into play.\n∗The work of I.R. Cohen was supported in part by ISF grant 1737/21. The work of A. Eden was supported by the\nIsrael Science Foundation (grant No. 533/23). The work of A. Vasilyan was done while visiting Bar-Ilan university\nas a part of the MISTI-Israel program, supported by the Zuckerman Institute.\n†Bar-Ilan University; ilan-reuven.cohen@biu.ac.il\n‡The Hebrew University; alon.eden@mail.huji.ac.il\n§Bar-Ilan University; talyaa01@gmail.com\n¶MIT; vasilyan@mit.edu\n1arXiv:2406.07024v1  [cs.GT]  11 Jun 2024\n\n1 Introduction\nAllocating items among self interested agents in a “fair” way is an age-old problem, with many\napplications such as splitting inheritance and allocating courses to students. As a starting point,\nconsider the case of two agents. When the items are divisible, the famous cut-and-choose procedure\nachieves fairness in two senses. Firstly, no agent wants to switch their allocation with the other;\ni.e., there is no envy among the agents. Secondly, each agent gets a bundle of items which they\nvalue at least as much as their value for all the items divided by 2; that is, each one gets their\n“fair share”. When moving to the case of indivisible goods, which is relevant to scenarios such\nas splitting inheritance and allocating courses, things get trickier. For instance, if there’s a single\nitem, the agent that does not receive that item does not get an envy-free allocation, nor do they\nget their “fair share” according to the previous definitions. Therefore, it is clear that some fairness\nneeds to be sacrificed in this case.\nThe study of fair allocations with indivisible goods has been a fruitful research direction, with\nmany meaningful notions of fairness studied (see survey by Amanatidis et al. [10]). In this paper, we\nfocus on the notion of the Maximin Share, or MMS, introduced by Budish [18]. For two agents, this\nnotion captures the value an agent will ensure if we implement the cut-and-choose procedure. That\nis, assume Alice splits the items into two bundles, and then Bob takes one of them (adversarially),\nand Alice gets the second one. The MMS captures exactly how much value Alice can guarantee for\nherself. Generalizing the notion for nagents is pretty straightforward — the MMS is the minimum\nvalue Alice can guarantee for herself when she partitions the items into nbundles, assuming n−1\nbundles are taken adversarially.\nWe study the case where agents have additive valuations over goods.1For the case of two agents,\nthe allocation produced by the cut-and-choose procedure guarantees each of the agents their MMS\nvalue. For more than two agents, the existence of such an allocation is not longer guaranteed.\nIndeed, Kurokawa et al. [30] show an instance of three agents, where in every allocation, at least\none of the agents does not get their MMS value. Since allocating all the agents their MMS value is\nnot always feasible, various papers studied the existence of approximately optimal allocation. An\nallocation is an α-approximate MMS allocation for α >1 if every agents gets at least an 1 /αfraction\nof their MMS value. Feige et al. [22] introduce an instance where one cannot find an α-approximate\nallocation for α <40\n39. On the other hand, [30] show there always exists3\n2-approximation. The3\n2\nfactor was gradually improved [16, 24, 23, 8, 4, 3, 5], where the state-of-the-art algorithm achieves\nan approximation of 959 /720>4/3 [3]. It is worth noting that simple variants of Round-Robin\nand water-filling algorithms already achieve 2-approximation. When adding incentives to the mix,\nmatters become even more complicated.\nAmanatidis et al. [7] study the case of two additive agents and mitems, where the algorithm\n(or mechanism) does not know the values of the agents. Thus, the algorithm’s designer is faced\nwith the task of devising an allocation rule such that (i)agents will maximize their allocated value\nby bidding truthfully, and (ii)the resulting allocation is an α-approximate MMS allocation for an\nαas close to 1 as possible. [7] show that the cost of dealing with self-interested agents might be\ndire. Namely, they show that no incentive-compatible algorithm can approximate the MMS to a\nfactor better than ⌊m\n2⌋, and this is matched by the following trivial mechanism — first agent picks\ntheir favorite item, and the second agent gets the rest. We note that although allocating each\n1Agent iwith an additive valuation has a value vij=vi(j) for every item, and their value for bundle Sis\nvi(S) =P\nj∈Svij.\n2\n\nagent all items with probability 1 /2 gives each agent an expected value which is at least as large as\ntheir MMS, this solution is not deemed fair, as one agent might end up with no items at all, while\ntheir counterpart will receive all items. Thus, the fair division literature mainly considers ex-post\nguarantees.\nFor 2 < n < m ,2a trivial truthful algorithm that lets the first n−1 agents pick a single item\nin some order and gives the last agent the rest achieves an ⌊m−n+2\n2⌋-approximation, and no better\nmechanism is known. It is conjectured that one cannot drop the dependence in mforn >2. We\nare left with a stark disparity. On the one hand, assuming agents values are public information,\ngood approximate solutions are known. On the other hand, when considering private values, it\nseems that only trivial approximations are possible. The goal of this paper is to bridge these two\nregimes using predictions.\nWe study the problem of truthful allocations that approximate the MMS, taking a learning-\naugmented point of view. In the learning-augmented framework, the algorithm designer aims to\ntackle some intrinsic hardness of the problem at hand, which might arise due to computational con-\nstraints, space constraints, input arriving piecemeal online, or incentive constraints, among others.\nTo help the designer overcome these constraints, the algorithm is given some side information which\nis a function of the input, or a prediction , in order to improve the algorithm’s performance. The hope\nis that if the prediction is accurate, then the performance is greatly improved over the performance\nwithout the prediction (termed consistency ). On the other end, if the prediction is inaccurate then\nthe performance of the algorithm is comparable to the performance of the best algorithm that is not\ngiven access to predictions (termed robustness ). The learning-augmented framework has proven\nuseful in bypassing impossibilities that arise due to incentive issues [14, 1, 25, 15, 39, 32, 13].\nWhen designing a learning-augmented mechanism, one should think of realistic predictions. For\ninstance, predicting the entire valuation profile of all agents seems to be a strong assumption. A\nmore plausible assumption is to have some ordinal ranking over the items of the agents. Indeed, it\nseems unlikely that the algorithm can accurately predict Alice’s value for a car, but it is plausible\nthat the algorithm can guess that Alice values the car more than she values the table. Ideally, the\nalgorithm’s performance should remain robust if the predicted ordering is almost perfect, with only\na few pairs of items whose real ordering is swapped in the prediction. Another desired property is\nto make the prediction as space-efficient as possible, following the intuition that smaller predictions\nare easier to observe. In this paper we devise learning-augmented truthful mechanisms for the\nproblem of approximate-MMS allocations, while taking into considerations the issues mentioned\nabove.\n1.1 Our Results and Techniques\nWe start by studying the two agent case. Recall that in the two agent case, [7] show that no truthful\nmechanism gets a better approximation than ⌈m\n2⌉to the MMS. We aim at getting:\n1.Constant consistency: when the predictions are accurate, we want to get a constant approx-\nimation to the MMS.\n2.Near-optimal robustness: when the predictions are off, we want to get as close as possible to\nthe optimal ⌈m\n2⌉-approximation we can obtain by truthful mechanisms.\n2Forn > m , the MMS of each agent is trivially 0. The problem becomes more interesting for m≫n.\n3\n\nPlant-and-Steal Framework. In Section 3 we present a framework for devising learning-augmented\nmechanisms for approximating the MMS with two agents. The intuition behind the framework is\nas follows — in order to get better approximation guarantees, one must use the predictions in order\nto get a good allocation. But in case the predictions are off, only using the predictions cannot\nguarantee any finite approximation to the MMS. Therefore, in case the predictions are off, we must\nuse the reports to ensure each agent gets at least one valuable item. In doing so, the mechanism\nshould still maintains a nearly optimal allocation according to the predictions.\nOur framework, which we term Plant-and-Steal is given the set of goods, an allocation pro-\ncedure A, the prediction pand reports v. The framework operates as follows:\n1. It first applies Aon the predictions pto divide the set of goods into two bundles A1, A2. The\nprocedure Ashould be an allocation procedure with good MMS guarantees. We use different\nallocation procedures depending on the type of prediction given and on the consistency-\nrobustness tradeoffs we are aiming for.\n2.Planting phase: For each agent i, it picks i’s favorite item in set Aiaccording to prediction ,\nand “plants” this item in the bundle Ajof the other agent j̸=i. Let T1, T2denote the sets\nthat result in this planting phase.\n3.Stealing phase: To obtain the final allocation, each agent inow “steals” back their favorite\nitem from set Tjof agent j̸=iaccording to reports . Notice this is the first and only place\nwhere we use agents’ reports.\nThis procedure is trivially truthful because the only step where we use agents’ reports is the one\nwhere they pick exactly one item to steal back from Tj, and this Tjonly depends on predictions,\nand not reports (Lemma 3.1). To obtain robustness, we notice that each agent gets one of their two\nfavorite items according to their true valuations (Lemma 3.2). This implies a robustness of m−1.\nWe show that if the allocations produced by Aare more balanced, we get improved robustness\nguarantees (Lemma 3.4).\nOrdering Predictions. In Section 4, we study learning-augmented mechanisms when the pre-\ndictions given are the preference orders over items of the agents (rather than the values). In the\ncase where the predictions are preference orders, we instantiate the Plant-and-Steal framework\nwith a Round-Robin-based allocation procedure. [8] show that preceding the Round-Robin proce-\ndure with an initial allocation of large items (of worth greater than µi/2) gives a 2-approximation\nto the MMS. We observe that in the case of two agents, one can run the Round-Robin procedure\nas is, without the initial allocation phase, and still obtain the 2-approximation. The gain in using\nthe standard procedure is that the allocation is as balanced as possible. To show consistency, we\nnotice that by the properties of the Round-Robin procedure, each agent ivalues her favorite item\nmore then any item in the other agent’s set Aj, except for the other agent’s favorite item. Since\nTjis obtained by adding i’s favorite item to Tjand removing j’s favorite item from it, in case the\npredictions are accurate, itakes back the item the mechanism planted in Tj, and vice-versa. Thus,\nwe end up with the original allocation ( A1, A2), obtaining a consistency of 2. Since each agent gets\nat least ⌊m\n2⌋items, including one of their top two items, we can show that we obtain a robustness\nguarantee of ⌈m\n2⌉. This almost completely matches the ⌊m\n2⌋lower bound from [7].\nAmanatidis et al. [6] study truthful mechanisms when the agents’ rankings are global. For\ntwo agents, they were able to show that slightly modifying the Round-Robin procedure, to let\n4\n\nthe second agent choose two items each time, obtains an improved approximation ratio of3\n2to\nthe MMS. When using the modified Round-Robin as the allocation procedure Ain the Plant-\nand-Steal framework, we get an improved consistency of3\n2, but since the final allocation is less\nbalanced, our robustness guarantee becomes ⌊2m\n3⌋.\nWe then study the performance of the Plant-and-Steal framework when using the Round-\nRobin procedure, when the prediction given is not fully accurate, but accurate to some degree. To\nquantify the prediction’s accuracy, we adopt the Kendall tau distance measure (or the bubble-sort\ndistance). The Kendall tau distance counts the number of pairs of elements swapped in the two\norderings. For our purpose, we consider the Kendall tau distance between the predicted preference\norder and the order induced by the true valuations. In order to simplify the analysis, we apply the\nzero-one principle. By the zero-one principle, it is enough to show that our mechanism achieves the\ndesired approximation guarantee in instances where the values for the items are either 1’s or 0’s.\nWe first show that for such instances, the initial allocation of the Round-Robin procedure, ( A1, A2),\nachieves an additive approximation to the MMS (this is also true for the mechanisms with global\nrankings from [6]). This does not guarantee, however, any multiplicative approximation. Thus,\nwe must leverage the fact that the agents get to “steal back” an item according to their true\nvaluations. We therefore are able to show that combining the Plant-and-Steal framework with\na Round-Robin allocation procedure obtains O(√\nd)-approximation to the MMS when the Kendall\ntau distance between the predictions and the valuations is d. Since dgoes from 0 to\u0000m\n2\u0001\n= Θ( m2),\nwe recover the constant consistency when there are no errors, and the O(m) robustness when the\nnumber of errors is maximal.\nGeneral Predictions. In Section 5, we study the two-agent case where the mechanism is given\naccess to predictions which are not necessarily the preference order of the agents. We first show that\nfor any prediction given to the learning-augmented mechanism, no mechanism can simultaneously\nbeα-consistent while maintaining finite robustness for α < 6/5. For the proof, we leverage the\ncharacterization of two-agent truthful mechanisms by [7].\nWe then study small-space predictions. The Round-Robin-based mechanisms described above\nrequire an Ω( m)-bit prediction (to describe an arbitrary allocation of items). We first notice that\nwe can implement a water-filling type allocation procedure using O(logm)-bit predictions. This\nalready achieves a constant consistency along with O(m) robustness. We then devise a more refined\nallocation procedure, which requires O(logm/ϵ)-bit predictions, and achieves 2+ ϵconsistency along\nwith⌈m\n2⌉robustness.\nNote that the work of [20] showed how to learn an (1+ ϵ/2)-approximate MMS allocation in the\ncontext of the model of [31] in which the valuations vi(j) are sampled i.i.d. from a distribution Di,j\nunder a small item assumption. We remark that combining this learned allocation with our Plant-\nand-Steal framework immediately gives a truthful, (2 + ϵ)-consistent and m-robust mechanism.\nGeneral number of agents n.Finally, in Section 6, we devise a learning-augmented truthful\nmechanism for n≥2 additive agents. We obtain a 2-consistent mechanism, while relaxing the\nrobustness guarantees of the mechanism. We take a similar approach to the works of [18, 27, 28, 2, 5],\nwho compete against a relaxed benchmark of the MMS value for ˆ n > n agents, and try to minimize\nˆn. We obtain a ( m− ⌈3n/2⌉ −1)-approximation to the MMS for ˆ n=⌈3n\n2⌉agents when the\npredictions are off. Our mechanism uses the modified Round-Robin procedure from [8] to determine\nthe initial allocation using the predictions. It then applies a recursive plant-and-steal procedure\n5\n\nwhere in each stage of the recursion, agents are partitioned into two sets. For each set of agents,\nthe mechanism “plants” their current favorite item according to prediction in the combined bundle\nof items of the other set, and “steals” back an item according to her reports . In order to ensure\nconsistency, the internal order in which each set of agents steal should be the same as their order\nin the corresponding Round-Robin round. In order to get our robustness guarantee, we carefully\nchoose the order at each Round-Robin round. We then show each agent gets at least their ⌈3n\n2⌉th\nmost preferred item according to their true valuation.\nExperiments. Finally, In Section 7, we demonstrate how several components in our design come\ninto play when experimenting with synthetic data. We run different variants of mechanism on\ntwo player instances, and show that when predictions are accurate, then only using predictions is\nnearly optimal, if predictions are noisy, then the stealing component ensures robustness, and our\nPlant-and-Steal framework achieves best-of-both-worlds guarantees.\nWe summarize the known bounds for learning-augmented truthful mechanisms for MMS ap-\nproximation in Table 1.\nSetting Consistency Robustness Reference\nOrdering\npredictions,\nn= 22 ⌈m/2⌉ Section 4\n3/2 ⌊2m/3⌋ Section 4\nAny ≥ ⌊m/2⌋ [7]\n≥5/4 Any [6]\nArbitrary predic-\ntions, n= 2≥6/5 Bounded Section 5.1\nlogn+ 1 space 4 m−1 Section 5.2\nO(log(n/ϵ)) space 2 +ϵ ⌈m/2⌉ Section 5.3\nn >2 2 m− ⌈3n/2⌉ −1 for\nˆn=⌈3n/2⌉Section 6\nTable 1: Known bounds for truthful learning-augmented MMS mechanisms.\n1.2 Related Works\nThe notion of the maximin share allocation was introduced by Budish [18] as an ordinal notion,\nand extended to the notion we adopt by Bouveret and Lemaˆ ıtre [17]. Using machine learning\nadvice in algorithm design was used in theory [21, 37] and practice [29]. The learning-augmented\nframework of studying consistency-robustness tradeoffs was introduced by Lykouris and Vassilvitskii\n[33]. [34, 38] studied the performance of algorithms using imprecise predictions.\nFair division with incentives. The two closest papers to ours are Amanatidis et al. [6, 7].\nIn [6], they initiate the study of truthful mechanisms for approximating the MMS value for agents\nwith additive valuations. They show that no truthful mechanism can get an approximation better\nthan 1 /2 for the MMS in the case of 2 agents and 4 items. They give the best known approximation\nguarantee for nagents and mitems of ⌊m−n+2\n2⌋. Finally they consider the public ranking model,\nwhere the ranking over items is public information. Using this, they are able to obtain an+1\n2-\napproximation algorithm. One can view this as an algorithm that is given a prediction over the\n6\n\ninput, but does not provide robustness guarantees. [7] Fully characterize truthful mechanism for 2\nagents with additive valuations. They use this characterization to provide a strong lower bound of\n⌊m\n2⌋for any truthful mechanism.\n[12] design truthful mechanisms for dichotomous submodular valuations that maximize welfare,\nalong with desirable fairness properties such as EFX and NSW. For additive binary valuations,\nthey also maximize the MMS in a truthful manner. [26] bypass the impossibilities imposed by\n[7, 36] for truthful fair allocations with indivisible and divisible goods by considering Bayesian\nIncentive Compatible mechanisms with symmetric priors. They are able to obtain EF-1 allocations\nfor indivisible goods and proportional allocations for indivisible goods.\nFinally, [9] study the Nash equilibrium for simple mechanisms for agents with additive valua-\ntions. They show that for every number of agents, the Pure Nash equilibrium of the Round-Robin\nprocedure produces an EF-1 allocation. For two agents, they show that the Pure Nash equilibrium\nof Plaut and Roughgarden [35] cut-and-choose procedure produces an EFX and MMS allocation.\n1-out-of- k.As stated above, the MMS value of an agent is defined by the highest value an agent\ncan guarantee for themselves when partitioning the items into ndifferent bundles, where nis the\nnumber of agents, and then getting the lowest valued bundle. Thus, an agent get a value larger\nthan the worst one-out-of- nbundles that define the MMS.\nNoticing that finding an allocation that satisfies the MMS value of each agent is a demanding\ntask (which was shown to be infeasible in some cases by Kurokawa et al. [30]), Budish [18] relaxed\nthe notion and defined the 1-out-of- n+ 1 MMS to be the worst bundle out of the bundles that\ndefine the MMS when partitioning the items using an additional bundle. [18] showed it is possible\nto achieve this benchmark when adding a small number of access goods. There has been an effort\nto find the smallest kfor which an allocation that guarantees a 1-out-of- kMMS for each agent\nexists. [2] were able to show the existence for k= 2n−2, [27, 28] achieved k=⌈3n\n2⌉, and recently,\n[5] showed the smallest up-to-date k=⌈4n\n3⌉. In our n-agent mechanism, our robustness guarantee\napproximates this relaxed benchmark for k=⌈3n\n2⌉.\nLearning Augmented Mechanisms. Agrawal et al. [1] and Xu and Lu [39] first explored\nthe learning augmented framework in a mechanism design setting, where [1] studied the facility\nlocation problem while [39] applied the framework to several settings such as revenue-maximization,\npath auctions, scheduling and two-facility games. [14] give nearly optimal consistency-robustness\ntradeoffs to the strategyproof scheduling with unrelated machines. [25] use predictions to design\nmechanisms with improved Price of Anarchy bounds. [32, 19] study revenue maximization auctions\nwith predictions, and [13] devise bicriteria mechanisms.\n2 Preliminaries\nIn the setting we study, there is a set Nofnagents and a set Mofmindivisible items. Each\nagent has a private additive valuation over the items, unknown to the mechanism designer, where\nthe value of agent ifor item jisvij(also denoted as vi(j)). For a bundle S⊆Mof items,\nvi(S) =P\nj∈Svij.\nThe fairness notion we focus on is the following.\n7\n\nDefinition 2.1 (Maximin Share) .The Maximin Share (MMS) of agent iwith valuation viandn\nagents is\nµn\ni= max\nS1S·...S·Sn=Mmin\nj∈[n]vi(Sj);\nthat is, if iwere to partition the items into nbundles, and then n−1of those bundles are taken\nadversarially, what is the value ican guarantee for themselves. When clear from the context, we\nomit nand use µito denote the MMS of iwithnagents.\nWe are interested in mechanisms that produce approximately optimal allocations, as defined\nnext.\nDefinition 2.2 ((γ, k)-approximate MMS Allocation) .An allocation X= (X1, . . . , X n)is(γ, k)-\napproximate MMS allocation for γ >1and a natural number kif for every agent i,\nvi(Xi)≥µk\ni/γ.\nWhen k=n, we say the allocation is a γ-approximate MMS allocation.\nWe study mechanism that get some prediction on the input.\nDefinition 2.3 (Learning Augmented Mechanism) .A learning-augmented mechanism takes agents’\nreports r= (r1, . . . , r n)and predictions pin some prediction space P, and outputs a partition of\nthe items\nX(r,p) = (X1(r,p), X2(r,p), . . . , X n(r,p)), X 1(r,p)[\n·X2(r,p)[\n·. . .[\n·Xn(r,p) =M,\nwhere agent igetsXi(r,p).\nFor learning-augmented mechanisms, truthfulness should hold for any possible prediction p.\nDefinition 2.4. A learning-augmented mechanism is truthful if for every agent iand every possible\nreport of other agents r−iand every possible prediction p,\nvi(Xi(vi,r−i,p))≥vi(Xi(ri,r−i,p))\nfor every ri.\nWe next define the consistency and robustness measures according to which we measure the\nperformance of our mechanisms.\nDefinition 2.5 (α-consistency) .Consider a prediction function fPwhich takes a valuation profile\nand outputs a prediction in prediction space P. A learning-augmented mechanism is α-consistent\nforα >1and prediction function fPif for every valuation profile vand every prediction p=fP(v),\nX(v,p)is an α-approximate MMS allocation.\nDefinition 2.6 ((β, k)-robust) .A learning-augmented mechanism is (β, k)-robust for β > 1and\nnatural number kif for every valuation profile vand every prediction p,X(v,p)is an (β, k)-\napproximate MMS allocation. If k=n, we say the mechanism is β-robust.\nFor ease of presentation, for valuation vi, report riand prediction pi, we use vℓ\ni, rℓ\ni, pℓ\nito denote\nboth theℓth highest good according to the valuation/report/prediction andits value. Note that,\nwe may use vℓ\niforℓ > m , in this case, vℓ\ni= 0. For ℓ= 1, i.e., the highest good we use v∗\ni, r∗\ni, p∗\ni.\n8\n\n2.1 Ordering Predictions and Kendall tau Distance\nMost of our mechanisms use predictions which take the form of an ordering over agents items. That\nis,fP(v) outputs a vector of orderings p= (p1, . . . , pn), where pℓ\niis the ℓth highest valued item of\niinMaccording to p. Accordingly, for agent i, letvℓ\nibe the ℓth highest valued item according to\nv. For two items j̸=j′, We use j≻pij′to denote that jis higher ranked than j′according to p.\nWhen studying imprecise predictions, we want to quantify the degree to which the prediction\nis inaccurate. For this, we use the following measure. For an agent i, we define our noise level with\nrespect to the Kendall tau distance (also known as bubble-sort distance) between vandp.\nDefinition 2.7 (Kendall tau distance) .The Kendall tau distance counts the number of pairwise\ndisagreements between two orders. For i’s valuation viand predicted preference order pi, we define\nKd(vi, pi) =|{j≻pij′:vi(j)< vi(j′)}.\nThat is, the number of pairs of items where the prediction got their relative ordering wrong. We\nalso denote Kd(v,p) = max {Kd(v1, p1), Kd(v2, p2)}.\nWe note that the Kendall tau distance between viandpi,Kd(vi, pi), can go from 0 to\u0000m\n2\u0001\n.\n3Plant-and-Steal Framework\nIn this section, we present the framework which is used to devise learning-augmented mechanisms\nfor two agents. The ideas presented here also inspire the highly complex learning-augmented\nmechanism for n >2 agents. As described in Section 1.1, the Plant-and-Steal framework takes\nan allocation procedure A, as well agents’ predictions and reports. It first uses Aon the predictions\nto derive an initial allocation ( A1, A2). Then, it “plants” agent i’s favorite item of set Aiaccording\npredictions in set Aj,j̸=i. Let ( T1, T2) be the sets resulting from the planting phase. Finally,\neach agent i“steals” back their favorite item in Tj,j̸=i,according to reports .\nForS⊆M, and agent i, letv∗\ni(S) (p∗\ni(S),r∗\ni(S)) be the max valued item in Saccording to vi\n(pi, ri). for g∈MandS⊆M, denote S+g:=S∪{g}andS−g=S\\{g}. The Plant-and-Steal\nframework is presented in Mechanism 1.\nWe now show that for any allocation function Aand predictions pgiven to the framework, the\nresulting mechanism is truthful.\nLemma 3.1 (Truthfulness Lemma) .For any allocation procedure A,Plant-and-Steal mechanism\nusingAis truthful.\nProof. We show that agent 1 is better off reporting their true valuation, a symmetric argument\nholds for agent 2. First, notice that sets T1andT2are determined using predictions, ignoring the\nreports. Next, notice that the item ˜j2is chosen only using agent 2’s report. Therefore, the only way\nagent 1 can affect their allocation is by choosing which item in T2is allocated to them. agent 1 gets\ntheir favorite item in T2according to their report. Therefore, it is clear that the agent maximize\ntheir utility by reporting their true value.\nSince the framework is truthful, from now on, we assume that r=v. Next, we show that the\nPlant-and-Steal mechanism ensures that for each agent, an item is allocated with a value that is\nat least as good as their second-best option according to their value .\n9\n\nMECHANISM 1: Two agent Plant-and-Steal Framework\nInput : Allocation Procedure A, set of items M, predictions pand reports r\nOutput: Allocations X1S·X2=M\n/* Find an initial allocation by applying Aon the predictions */\n(A1, A2):=A(M, N, p)\n/* Plant favorite items according to predictions */\nˆj1←p∗\n1(A1)\nˆj2←p∗\n2(A2)\nT1←A1+ˆj2−ˆj1\nT2←A2+ˆj1−ˆj2\n/* Steal according to report */\n˜j1←r∗\n1(T2)\n˜j2←r∗\n2(T1)\nX1←T1+˜j1−˜j2\nX2←T2+˜j2−˜j1\nLemma 3.2. Consider the allocation (X1, X2)returned by Plant-and-Steal with some allocation\nprocedure A. For any agent i, then v1\ni∈Xiorv2\ni∈Xi.\nProof. Consider some agent i. We claim for every partition of the items into two non-empty sets,\nT1, T2,iis always guaranteed to have one of their two favorite items according to their true valuation\nviinXi. This is because either (1)ihas one of their two favorite items in Tℓ,ℓ̸=i, and igets\ntheir favorite item from Tℓ; or (2) i’s two favorite items are in Ti, and in this case, igets all items\nfrom Tibut one, so iis guaranteed one of them.\nWe next claim that if igets one of their two favorite items and any k−1 additional items, i’s\nvalue is an m−k-approximation to µi.\nLemma 3.3. For any agent i, letS⊆Mbe a subset of the items of size |S|=kandv1\ni∈Sor\nv2\ni∈Sthen\nvi(S)≥µi/(m−k).\nProof. Letg∈S∩ {v1\ni, v2\ni}, by the definition of Ssuch gexists. Let S′=S\\ {g}, by the definition\nofS, we have |S′|=k−1 and vi(S)≥v2\ni+vi(S′). Consider a partition\n(S1, S2)∈arg max(T1,T2) :T1S·T2=Mmin\nj∈{1,2}vi(Tj).\nBy definition, µi= min j∈{1,2}vi(Sj). We have,\nµi\nvi(S)≤µi\nv2\ni+vi(S′)\n=minj∈{1,2}vi(Sj)\nv2\ni+vi(S′)\n≤minj∈{1,2}vi(Sj)−vi(S′)\nv2\ni\n10\n\n≤minj∈{1,2}vi(Sj\\S′)\nv2\ni\n≤minj∈{1,2}{|Sj\\S′| ·max{vi(ℓ) :ℓ∈Sj\\S′}}\nv2\ni.\n≤(m−k)·minj∈{1,2}max{vi(ℓ) :ℓ∈Sj}\nv2\ni\n≤m−k.\nwhere the before last inequality is since if Sj⊆S′for some j, then v2\ni+vi(S′)≥µi; therefore\nS1\\S′andS2\\S′are two disjoint non empty subsets and |S1\\S′|+|S2\\S′|=m−k+ 1, hence\nthe maximum number of elements in one of these subsets is m−k.\nWe immediately get the following.\nLemma 3.4 (Robustness Lemma) .LetAbe an allocation rule guaranteeing min{|A1|,|A2|} ≥k,\nthen when Plant-and-Steal usesA, the resulting mechanism is (m−k)-robust.\nProof. By Lemma 3.2, we are guaranteed that each agent gets one of their two favorite items\naccording to their report. Combining with the condition on Aand Lemma 3.3, the proof is finished.\n4 Ordering Predictions\nIn this section, we consider the case of two agents, where the predictions (and in fact, also the re-\nports) given to the mechanism are preference orders of agents over items. Our mechanisms makes\nuse of the Plant-and-Steal framework instantiated by Round-Robin based allocation procedures.\nIn Section 4.1 we present our two round-robin allocation procedures, and give their approximation\nguarantees when the input is accurate. In Section 4.2 we prove the robustness and consistency\nguarantees. In Section 4.3 we quantify the accuracy of the predictions using the Kendall tau dis-\ntance, and obtain fine-grained approximation results, where the approximation smoothly degrades\nin the accuracy.\nAmanatidis et al. [6] studied mechanisms where the preference orders of the agents over items\nare public (while valuations are private). They showed that no truthful mechanism can achieve a\nbetter approximation than 5 /4 in this setting. This implies that when the predictions are preference\norders, no learning-augmented mechanism can obtain consistency better than 5 /4, no matter if the\nrobustness is bounded or not.\nProposition 4.1 (Corollary of Amanatidis et al. [6]) .No mechanism that is given preference orders\nas predictions can obtain consistency 5/4−ϵfor any ϵ >0.\n4.1 Round-Robin Allocation Procedures\nThe two allocation procedures we use to instantiate the Plant-and-Steal framework take as input\npreference orders of agents over items:\n•Balanced-Round-Robin : the agents take turns, and at each turn, an agent takes their highest\nranked remaining item. This results in a balanced allocation.\n11\n\n•1-2-Round-Robin : the agents take turns, where we compensate the second agent, who might\nnot get their favorite item, to take two items each turn.\nConsider the allocation procedure depicted in Algorithm 2.\nALGORITHM 2: Balanced-Round-Robin\nInput : Preference orders of agents over items v= (v1, v2).\nOutput: An allocation A1S·A2=M.\nAi← ∅for every agent i∈ {1,2}\nforr= 1, . . . ,⌈|M|/2⌉do\nA1←A1+v∗\n1(M\\A1\\A2)\nA2←A2+v∗\n2(M\\A1\\A2)\nNotice that to implement the allocation procedure of Balanced-Round-Robin , it only needs\nto receive preference orders over items. Let Ai= (a1\ni, . . . , a|Ai|\ni) be agent i’s allocation by the\nalgorithm, where ak\niis the k’th choice of agent i. We observe the following.\nObservation 4.1. The output (A1, A2)of the Balanced-Round-Robin procedure, satisfies:\n1.|A1|=⌈m\n2⌉,|A2|=⌊m\n2⌋.\n2. For each agent iand round k,ak\ni∈ {vℓ\ni}ℓ∈[2k]; that is, in round kan agent gets one of their\ntop2kitems.\nAmanatidis et al. [8] show that first allocating large items to agents, and then using a Round-\nRobin to allocate the remaining items to the remaining agents, gives a 2-approximation to the\nMMS. We observe that for two agents, Round-Robin as is, without the initial step, achieves this\napproximation guarantee. The proof of the following Lemma is deferred to Appendix B.\nLemma 4.1. Let(A1, A2)be the allocation of Balanced-Round-Robin . For every agent i,vi(Ai)≥\nµi/2.\nOne can show that the agent that picks first actually gets a value at least as large as their\nMMS, while for the second agent this analysis is indeed tight.3In order to compensate agent 2,\n1-2-Round-Robin lets this agent pick two items each round. See Algorithm 3 for details.\nALGORITHM 3: 1-2-Round-Robin\nInput : Preference orders of agents over items v= (v1, v2).\nOutput: An allocation A1S·A2=M.\nAi← ∅, for every agent i∈N\nforr= 1, . . . ,⌈|M|/3⌉:do\nA1←A1+v∗\n1(M\\A1\\A2)\nA2←A2+v∗\n2(M\\A1\\A2)\nA2←A2+v∗\n2(M\\A1\\A2)\nLetak\nibe agent i’skth choice in 1-2-Round-Robin , we observe the following.\n3Consider the case where the agents’ valuations are ( m−1,1, . . . , 1). According to Round-Robing allocation, the\nfirst item will be assigned to agent 1, and agent 2 will have m/2 items of value 1, while µ2=m−1.\n12\n\nObservation 4.2. The output (A1, A2)of the 1-2-Round-Robin procedure, satisfies:\n1.|A1|=⌈m\n3⌉and|A2|=⌊2m\n3⌋.\n2.ak\n1∈ {vℓ\n1}ℓ∈[3k−2],a2k−1\n2∈ {vℓ\n2}ℓ∈[3k−1]anda2k\n2∈ {vℓ\n2}ℓ∈[3k].\nAmanatidis et al. [6] show that 1-2-Round-Robin guarantees each agent 2 /3 of their MMS.\nLemma 4.2 (Amanatidis et al. [6]) .Let(A1, A2)be the allocation of 1-2-Round-Robin . For every\nagent i,vi(Ai)≥2µi/3.\nFor completeness, We provide the proof of the approximation in Appendix B.\nWe next use the two allocation procedures to instantiate the Plant-and-Steal framework.\n4.2 Round-Robin-Based Mechanisms\nWe analyze the two mechanisms:\n•B-RR-Plant-and-Steal : The mechanism which results from instantiating Plant-and-Steal\nwith Balanced-Round-Robin asA.\n•1-2-RR-Plant-and-Steal : The mechanism which results from instantiating Plant-and-\nSteal with 1-2-Round-Robin asA.\nWe first show that if the predictions correspond to the preference orders of the real valuations,\nthen both B-RR-Plant-and-Steal and 1-2-RR-Plant-and-Steal output the same allocation as\nBalanced-Round-Robin and1-2-Round-Robin .\nLemma 4.3. When predictions correspond to actual values, B-RR-Plant-and-Steal (1-2-RR-\nPlant-and-Steal ) outputs the same allocation as Balanced-Round-Robin (1-2-Round-Robin ).\nProof. We prove the claim for B-RR-Plant-and-Steal . The proof for 1-2-RR-Plant-and-Steal\nis identical.\nLetj1be the first item assigned in Balanced-Round-Robin to agent 1. By definition, j1is agent\n1’s favorite item in Maccording to p1. Clearly, in Plant-and-Steal ,j1is also agent 1’s favorite\nitem in A1⊆Maccording to p1. Hence, ˆj1=j1. By the definition of Plant-and-Steal ,j1∈T2.\nSince we assume the prediction corresponds to agent 1’s actual value, j1is also agent 1’s favorite\nitem in T2⊆M, which implies ˜j1=j1.\nSimilarly Let j2be the first item assigned in Balanced-Round-Robin to agent 2. By definition,\nj2is agent 2’s favorite item in M\\ {j1}according to p2. Since j1∈A1,A2⊆M\\ {j1}. Therefore,\nj2is also agent 2’s favorite item in A2according to p2. Hence, ˆj2=j2. Since we established that\nˆj1=j1, we have that T1⊆M\\ {j1}andj2∈T1. Since we assume the prediction corresponds\nto agent 1’s actual value, j2is also agent 1’s favorite item in T1, implying ˜j2=j2. We get that\nX1=A1andX2=A2as required.\nWe are now ready to prove the performance guarantees of our mechanisms.\nTheorem 4.1. Mechanism B-RR-Plant-and-Steal is truthful, 2-consistent and ⌈m\n2⌉-robust.\n13\n\nProof. By Lemma 3.1, the mechanism is truthful. By Observation 4.1, each agent receives at\nleast⌊m/2⌋items; combining with Lemma 3.4, we get that the mechanism is ⌈m\n2⌉-robust. Finally,\nif predictions correspond to valuations, by Lemma 4.1 and Lemma 4.3, the allocation is a 2-\napproximation to the MMS. Thus, the mechanism is 2-consistent.\nWe note that by Amanatidis et al. [7], our robustness guarantee matches the optimal obtainable\napproximation by any truthful mechanism (up to the rounding).\nWe next show that in 1-2-RR-Plant-and-Steal we are able to achieve a better consistency,\nwhile slightly weakening the robustness guarantee. Due to similarity to the proof of Theorem 4.1,\nwe defer the proof of the following Theorem to Appendix B.\nTheorem 4.2. Mechanism 1-2-RR-Plant-and-Steal is truthful, 3/2-consistent and ⌊2m\n3⌋-robust.\n4.3 Noisy Predictions\nWe now analyze Mechanism B-RR-Plant-and-Steal ’s performance under varying levels of noise.\nConsider the case where the Kendall tau distance between vandpis at most d. Our goal is to\nrelate the value agent igets from the allocation, vi(Xi) to their maximin share µi. To simplify the\nanalysis, we compare what igets to the worst possible set of items imight get when running the\nRound-Robin procedure using the agents’ real preferences Ri={v2j\ni}j∈{1,...,⌊m/2⌋}. In Eq. (10) of\nLemma 4.1, we show that\nvi(Ri)≥µi/2. (1)\nWe further simplify the analysis by applying the zero-one principle4. The zero-one principle ba-\nsically let’s us reduce to instances where the values are either 0’s or 1’s. For threshold τ≥0,\nlet\nhτ(q) =(\n1 q≥τ\n0 otherwise.\nAccordingly, let vτ\ni(S) =P\nj∈Shτ(vi(j)).\nBy the zero-one principle, for two sets S, T⊆M, in order to show that vi(S) approximates\nvi(T), it is enough to show that vτ\ni(S) approximates vτ\ni(T) for every threshold τ≥0.\nLemma 4.4. Forc >1and for any two sets S, T⊆M, if for every threshold τ≥0,vτ\ni(S)≥\nvτ\ni(T)/c, then vi(S)≥vi(T)/c.\nProof. LetS={s1, . . . , s k}(|S|=k) and T={t1, . . . , t ℓ}(|T|=ℓ). We have the following.\nvi(S) =kX\nj=1vi(sj) =kX\nj=1Z∞\n0hτ(vi(sj))dτ=Z∞\n0kX\nj=1hτ(vi(sj))dτ=Z∞\n0vτ\ni(S)dτ\n≥Z∞\n0vτ\ni(T)/c dτ =1\ncZ∞\n0ℓX\nj=1hτ(tj)dτ=1\ncℓX\nj=1Z∞\n0hτ(tj)dτ=1\ncℓX\nj=1tj=vi(T)/c,\nwhere we use the identityR∞\n0hτ(q)dτ=q.\n4Applied in [11], for instance, in the context of packet routing.\n14\n\nThus, we will show that when the Kendall tau distance is d, for every threshold τ≥0,vτ\ni(Xi)≥\nvτ\ni(Ri)/cfor some c=O(√\nd). Recall that Aiis the set of items assigned to iafter running the\nRound-Robin procedure on the predictions p. We first show that for Kendall tau distance d, the\nadditive approximation vτ\ni(Ai) gives to vτ\ni(Ri) is√\nd.\nLemma 4.5. If the Kendall tau distance between pandvis at most d, then for any threshold\nτ≥0, we have that vτ\ni(Ai)≥vτ\ni(Ri)−√\nd.\nProof. Let⌊m\n2⌋ ≤mi≤ ⌈m\n2⌉be the number of items agent igets by Mechanism B-RR-Plant-and-\nSteal . Let Ai={a1\ni, a2\ni, . . . , ami\ni}be the items assigned to agent iin the Round-Robin according\nto the predicted orderings p, where aℓ\niis the item allocated to iin the ℓth round of Round-Robin.\nFirst, by Observation 4.1, we have:\naℓ\ni∈ {pj\ni}j∈{1,...,2ℓ}. (2)\nFor a fixed τ≥0, let Lτ=vτ\ni(Ri) be the number of values larger than threshold τinRi. We\nshow that if the Kendall tau distance is at most d, then it must be the case that\nvτ\ni(Ai)≥Lτ−√\nd. (3)\nNote that hτ(vk\ni) = 1 for k≤2·Lτsince Rigets every second item by the sorted values of\nagent i. This implies that if hτ(v1(aℓ\ni)) = 0 then aℓ\ni=vk\nifork >2·Lτ. Moreover, if ℓ≤Lτthen\naℓ\ni=pk\nifork≤2·Lτby Eq. (2). Thus, ifPLτ\nk=1hτ(v1(ak\ni))< L τ−√\ndthere are strictly more than\n⌈√\nd⌉items whose rank according to the true valuation is at most 2 ·Lτ, and their rank according\nto the prediction is at least 2 ·Lτ+ 1. We show that this implies that the Kendall tau distance is\nlarger than d, yielding a contradiction. Formally, let\nG1={vk\n1}k∈{1,...,2·Lτ}\\ {pk\n1}k∈{1,...,2·Lτ}\nbe the set of items whose rank is at most 2 ·Lτaccording to the real values but not according to\nthe predictions, and let\nG2={vk\n1}k∈{2·Lτ+1,...,m}\\ {pk\n1}k∈{2·Lτ+1,...,m}\nbe the set of items whose rank is strictly larger than 2 ·Lτaccording to the real values but not\naccording to the predictions. By the above, |G1|=|G2|>⌈√\nd⌉, and for each pair j∈G1, j′∈G2,\n1.jrank according to viis at most 2 ·Lτandj′rank according to viis at least 2 ·Lτ+ 1;\n2.j′rank according to piis at most 2 ·Lτandjrank according to piis at least 2 ·Lτ+ 1.\nThat is, jandj′are ordered oppositely in the ordering according to piandvi. Since there are\n|G1| · |G2|> d such pairs, we get that the Kendall tau distance is strictly greater than d, a\ncontradiction.\nWe note that although vτ\ni(Ai) gives an additive approximation to vτ\ni(Ri), it can still be the case\nthat the Kendall tau distance is constant, yet vi(Ai) does not give any multiplicative approximation\ntoµi.5Therefore, we must use the fact that agent igets to “steal” an item according to their true\nvaluation in the Plant-and-Steal procedure in order to get our approximation guarantee. We\nnow prove our approximation guarantees.\n5Indeed, consider the case where there are four goods which both agents value at (1 ,1,0,0). If agent i’s prediction\norders the last two items higher then the first two items, we will get that vi(Ai) = 0, while µi= 1.\n15\n\nTheorem 4.3. Consider a prediction pand valuations vsuch that Kd(v,p) =d, then Mechanism\nB-RR-Plant-and-Steal gives a (2√\nd+ 6)-approximation to the MMS.\nProof. We use the zero-one principle to show that Lemma 4.4 holds for sets XiandRiwith\nc=√\nd+ 3. The proof then follows by Eq. (1).\nNotice that |Ai\\Xi| ≤2, because in the “stealing” phase, agent imight not take the “planted”\nitem from Aiback, and the other agent might take one item from Ai.6Moreover, by Lemma 3.2,\neither v1\niorv2\niare in Xi. Therefore, for every threshold τ≥0,\nvτ\ni(Xi)≥max{hτ(v2\ni), vτ\ni(Ai)−2}\n≥max{hτ(v2\ni), vτ\ni(Ri)−√\nd−2}, (4)\nwhere the inequality follows Lemma 4.5.\nIfhτ(v2\ni) = 0, then vτ\ni(Ri)≤ |Ri| ·hτ(v2\ni) = 0 ,and Lemma 4.4 holds with c= 0. Therefore,\nthe interesting case is when hτ(v2\ni) = 1. Consider the ratiovτ\ni(Ri)\nvτ\ni(Xi)which we want to bound. Since\nvτ\ni(Xi)≥hτ(v2\ni) = 1 , vτ\ni(Ri)∈[1,√\nd+ 3] implies that\nvτ\ni(Ri)\nvτ\ni(Xi)≤vτ\ni(Ri)≤√\nd+ 3.\nOn the other hand, by Eq. (4), setting vτ\ni(Ri) =√\nd+ 3 + δforδ > 0 implies that vτ\ni(Xi)≥\nvτ\ni(Ri)−√\nd−2≥1 +δ, which yields\nvτ\ni(Ri)\nvτ\ni(Xi)≤√\nd+ 3 + δ\n1 +δ≤√\nd+ 3.\nWe get that Lemma 4.4 holds for XiandRiwith c=√\nd+ 3. Thus,\nvi(Xi)≥vi(Ri)/(√\nd+ 3)≥µi/(2√\nd+ 6),\nwhere the last inequality follows Eq. (1).\nWe note that a similar analysis for Mechanism 1-2-RR-Plant-and-Steal will show a similar\ndependence in√\nd(up to constant factors).\n5 Non-ordering Predictions\nIn this Section, we consider the case where predictions are not necessarily preference orders over\nitems. In Section 5.1, we show that for any prediction the mechanism might get, consistency is\nbounded away from 1. Sections 5.2, 5.3, we study succinct predictions, i.e. predictions about\ngeneral structure of the preferences of two agents. Section 5.2 presents a 4-consistent and ⌈m/2⌉-\nrobust mechanism, whose consistency relies on the correctness of only a log m-bit prediction about\nthe preferences of the two agents. In Section 5.3, we show that a 2+ ϵ-consistent and ⌈m/2⌉-robust\nmechanism exists, whose consistency relies on correctly predicting only O(logm/ϵ) bit about the\npreferences of the two agents.\n6In fact, this holds for any noise in the valuations of the other agent.\n16\n\n5.1 No Mechanism with <6/5Consistency and Bounded Robustness\nIn Appendix C.1, we show that no mechanism can simultaneously achieve a consistency guar-\nantee strictly lower than 6 /5 and any bounded robustness guarantee no matter which prediction\nis given . We use the elegant characterization of [7] for 2-agent mechanisms and show that in\nany truthful mechanism with finite approximation ratio, if the valuations are identical, then each\nagent gets at least one of the two largest items. Thus in the instance where the prediction is\np1=p2= (1/2,1/2,1/3,1/3,1/3), each agent gets one item of value 1 /2. This implies that there\nis an agent with an allocation of value 1 /2 + 1 /3 = 5 /6 (and an agent with value 7 /6), while\nµ1=µ2= 1.\nTheorem 5.1. For any ϵ >0, there is no truthful a mechanism with consistency 6/5−ϵand\nbounded robustness.\n5.2 4-Consistent, (m−1)-Robust Mechanism Using a logm+ 1-Space Prediction\nLet us formally define a mechanism that uses a space- sprediction\nDefinition 5.1. A learning-augmented mechanism is a space- smechanism if the prediction space\nPcan be represented by the elements of {0,1}s.\nWe first give a simple mechanism that only requires log m+ 1 bits of information about the\nvaluations v1andv2. It will only need to know an index j0in [m] together with a bit b. The\nmechanism will utilize the Plant-and-Steal framework in conjunction with the well-known water-\nfilling allocation procedure:\nMECHANISM 4: Water-Filling\nInput : Preference orders of agents over items v= (v1, v2) on a set of items M= [m]\nOutput: Allocations A1S·A2=M\nj←1\nforj= 1, . . . , m :do\nifv1([j])\nv1([m])≥1\n2then Output ( A1, A2)←([j],[m]\\[j]) and terminate\nifv2([j])\nv2([m])≥1\n2then Output ( A1, A2)←([m]\\[j],[j]) and terminate\nWe see that, in order to predict the behaviour of the mechanism above, one only needs to\npredict accurately the index j0on which the mechanism terminates, as well as a bit b∈ {1,2}that\nencodes whether the algorithm terminates due to the conditionv1([j])\nv1([m])≥1\n2being satisfied or due\nto the conditionv2([j])\nv2([m])≥1\n2being satisfied. This can be encoded using log m+ 1 bits.\nWe also see that the The Plant-and-Steal framework when used with the Water-Filling\nallocation procedure gives a truthful 4-consistent and a m−1-robust7allocation mechanism. The\ntruthfulness and robustness follow immediately from Lemmas 3.1 and 3.4 respectively.\nThe 4-consistency holds for the following reason. It is a well-known fact (see i.e. [10]) that the\npartition ( A1, A2) given by the water-filling algorithm satisfies v1(A1)≥µ1/2 and v2(A2)≥µ2/2.\nBy inspecting the Plant-and-Steal framework (Algorithm 1), we see that both agent 1 and agent\n7Note that min( |A1|,|A2|)≤m−1 which implies that the algorithm is ( m−1)-robust.\n17\n\n2 will either (i) retain their most preferred item in A1andA2respectively or (ii) Lose this item,\nbut obtain an item that they prefer even more. Overall, this implies that in the worst case the\ndifference v1(A1)−v1(X1) will equal to the value of the second-most favorite item of Agent 1 in\nA1. This implies that v1(X1)≥1\n2v1(A1)≥µ1\n4. Analogously, we see that v1(X2)≥1\n2v1(A2)≥µ2\n4.\n5.3 2+ϵ-Consistent, ⌈m\n2⌉-Robust Mechanism Using a O(logm/ϵ)-Space Prediction\nWe now show that a better consistency of 2 + ϵcan be achieved at the cost predicting O(logm/ϵ)\nbits of information about the valuations v1andv2. We will also obtain a better robustness of ⌈m\n2⌉.\nTo do this, we will use the Plant-and-Steal framework in conjunction with the Cut-and-Balance\nallocation procedure. We first explain how the mechanisms above can be implemented by only\nALGORITHM 5: Cut-and-Balance\nOutput: Allocations A1S·A2=M\nConsider a partition S1S·S2=Msatisfying |S1| ≥ |S2|and\nmin\nj∈{1,2}v1(Sj)≥(1−ϵ)max T1S·T2=Mmin\nj∈{1,2}v1(Tj) = (1 −ϵ)µ1\nLetS′⊂S1be a set of ⌊m/2⌋ − |S2|items satisfying\n•v1(S′)≤v1(S1)/2\n•if|S2|>1 additionally satisfying v1(S′)≤v1(S1\\ {ˆj,ˆj′})/2, for some\nˆj∈arg max j∈S1v1(j) and ˆj′∈arg maxℓ∈S1\\ˆjv1(ˆj)\nSet˜S1←S1\\S′and˜S2←S2∪S′\nLeti2←arg max i∈{1,2}p2(˜Si) and let i1be the index of the other bundle\nSetA1←Si1andA2←Si2, and output the allocation ( A1, A2)\nobtaining O(logm/ϵ) bits of information about the valuations v1andv2. This follows from the\nfollowing proposition, the proof of which is given in Appendix C.2.\nProposition 5.1. Suppose M= [m]. There is a partition M=L1S·L2S·Sand indices α1, β1, α2\nandβ2with|L1|+|L2| ≤O\u00001\nϵ\u0001\n, such that the partition M=S1S·S2defined as S1=L1S(ST[α1, β1])\nandS2=L2S(ST[α2, β2])satisfies |S1| ≥ |S2|andmin(v1(S1), v1(S2))≥(1−ϵ/4)µ1.\nAdditionally, there exist integers α3, β3, α4andβ4such that the set S′=ST([α3, β3]S[α4, β4])\nsatisfies |S′|=⌊m/2⌋ − |S2|,S′⊂S1,v1(S′)≤v1(S1)/2and if |S2|>1then S′also satisfies\nv1(S′)≤v1(S1\\ {ˆj,ˆj′})/2, where ˆj∈arg max j∈S1v1(j)andˆj′∈arg maxj∈S1\\ˆjv1(j).\nThe main ideas for proving Proposition 5.1 are: (i) using the sets L1andL2to handle elements\nxwhose value v(x) is large, and separate the remaining items into the set S(ii) Showing that the\nremaining items can be separated into well-behaved subsets of the form ST[αi, βi].\nThe proposition above implies that the sets S1, S2andS′can be represented exactly via sets\nL1andL2, together with the indices {α1,···, α4, β1,···β4}. We will also need to know the index\ni2∈ {1,2}. Since the sets L1andL2have a size of O(1/ϵ), all this information amounts to\nO(logm/ϵ) bits as claimed.\nThe following proposition implies the truthfulness, the robustness and the consistency of the\nmechanism that combines the Cut-and-Balance allocation procedure with the Plant-and-Steal\nframework.\n18\n\nTheorem 5.2. The Plant-and-Steal framework, when used with Cut-and-Balance allocation\nprocedure, gives a truthful, 2 +ϵ-consistent and a ⌈m/2⌉-robust allocation mechanism.\nProof. Truthfulness follows from Lemma 3.1. Since the sets A1andA2both have size at most\n⌈m/2⌉, the robustness follows via Lemma 3.4.\nThe proof of (2 + ϵ)-consistency is deferred to Appendix C.3. The main challenge for showing\nthe bound on consistency is the fact that both the Cut-and-Balance allocation procedure and the\nPlant-and-Steal framework can reduce the consistency by a factor of 2. Naively, one would expect\nthe overall consistency to be close to 4, given that each stage can lose a factor of 2 in consistency.\nHowever, our insight is that for the instances, on which the Cut-and-Balance allocation procedure\nloses a factor of 2 in consistency, the Plant-and-Steal framework will have consistency close to 1,\nand vice versa. This allows us to prove a tighter bound of 2 + ϵon the consistency of our overall\nalgorithm.\n6 Mechanisms for nagents\nIn this section we provide a learning-augmented mechanism for n >2 agents, Learning-Augmented-\nMMS-for- n-Agents . The mechanism we devise ensures that if the predictions are accurate, then\neach agent gets an allocation with value at least µn\ni/2 (2 consistency). On the other hand, we show\nthat for any prediction, every agent gets at least µ⌈3n/2⌉\ni /αforα=m− ⌈3n/2⌉ −1 (robustness).\nTheorem 6.1. TheLearning-Augmented-MMS-for- n-Agents Mechanism (Mechanism 6) is truth-\nful, 2-consistent and µ⌈3n/2⌉\ni /α-robust for α=m− ⌈3n/2⌉ −1.\n6.1 An Overview\nThe Mechanism. The mechanism works in three phases. In the first phase, it uses the predic-\ntions in order to obtain a partial allocation to agents with high predicted items (which are then\nremoved from the set of active agents, so that we can now that for all agents, all predicted values\nare small). Then, in the second stage, the mechanism uses the predictions in order to obtain a\ntentative allocation , by running a Round-Robin procedure, where items are tentatively allocated\nto agents according to their predictions. In the third and final phase, the tentative allocation is\nused to implement a recursive plant and steal procedure, where the “planting” is done from the\ntentative allocations according to predictions, but the “stealing” is done according to the agents’\nreports and results in a final allocation .\nMECHANISM 6: Learning-Augmented-MMS-for- n-Agents\nInput : Set of agents N, set of items M, reports rN, predictions pN\nOutput: A partition of the itemsS·i∈NXi\nInvoke Algorithm 7, X←Allocate-Large (N, M, rN,pN)\nInvoke Algorithm 9, A←Tentative-Allocation-Round-Robin (N, M, pN)\nInvoke Algorithm 10, X←Split-Plant-Steal-Recurse (N, A, first-level-flag =True , X,rN,pN)\nConsistency. In the case the predictions are accurate, the initial allocation phase will take care\nof agents with high valued items (of value larger than µn\ni/2). Then, in the second phase, the\n19\n\nFigure 1: Illustration of a single round of the recursive planting and stealing phase (Algorithm 10),\nfor the case where predictions are accurate (so that each agent steals back their planted item).\nNote that the stealing is done from the union of items of agents in the opposite set (and not just\nfrom the corresponding agent).\ntentative allocation will be exactly identical to a Round-Robin allocation (made according to true\nvaluations). Finally, in the third phase, since agents steal in the same order they were allocated\nthe items in the Round-Robin allocation, and since the predictions are accurate, the agents “steal”\nback the same item the mechanism plants. Since a Round-Robin allocation achieves µn\ni/2 when\nthere are no agents with high valued items [8], correctness follows.\nRobustness. In the case the predictions are inaccurate, we show that every agent still gets\nat least µ⌈3n/2⌉\ni /α. Here we rely on the plant-and-steal phase to ensure that each agent gets at\nleast their ⌈3n/2⌉highest-valued item according to their true valuation. This property provides\nour robustness guarantee. We notice that reversing the order between the first and subsequent\nrounds of the Round-Robin procedure (and thus, the stealing phases) gives an enhanced robustness\nguarantee.\n20\n\nPrediction. In the description of the mechanism, we assume the mechanism is given a prediction\nof agents valuations. We note that in order to implement the mechanism it is enough to be given\naccess to agents’ preference order over items, and an additional information indicating which items\nare worth more than µn\ni/2 for each agent i.\nBelow, we first give a detailed description of the mechanism, and then we conclude by proving\nTheorem 6.1.\n6.2 Implementation Details\nAs discussed, in order to utilize the Round-Robin mechanism, we first allocate a single item to each\nagent with a high predicted value.\nALGORITHM 7: Allocate-Large\nInput : Set of agents N, set of items M, reports rN, predictions pN\nOutput: A partial allocationS·i∈BXi, updated sets of agents and items N, M , respectively\nforeach i∈Ndo\nCompute µn\nibased on pi\nwhile exists i∈Nsuch that p∗\ni(M)≥µn\ni/2do\nXi← {r1\ni(M)}\nM←M\\Xi\nN←N\\ {i}\nBefore describing the tentative allocation mechanism, we first give a procedure, Allocate-Best ,\nwhich performs a single round of Round-Robin according to a specific order, and preferences (either\npredictions or reports), denote o.\nPROCEDURE 8: Allocate-Best (One-Round-RR)\nInput : Ordered set of agents N, set of items M, valuation vN\nOutput: |N|singletons Xi∈M\nforeach i∈Ndo\nXi←v∗\ni(M)\nM←M\\Xi\nThe tentative allocation mechanism repeatedly invokes Allocate-Best according to given pre-\ndictions, until all items are tentatively allocated. As previously mentioned, the first round of the\ntentative allocation is performed according to the given order, and in all subsequent rounds, the\norder is reversed (recall that reversing the order enhances the robustness guarantees).\nThe final phase in the mechanism is a recursive plant and steal algorithm. The input to this\nalgorithm is an ordered set of agents N, along with their predictions, reports, and a tentative\nallocation for each agent. At each recursive invocation, the algorithm splits the set of agents into\ntwo (almost) equal-size ordered sets N0andN1. Then the mechanism “plants” for the ithagent\nin each set Nbtheir highest (according to predictions) valued item in their tentative allocation in\nthe tentative set of the ithagent in N¬b.Then we perform one round of Round-Robin, where the\nitems available to the agents of set Nbare those tentatively allocated to the agents of N¬b(after the\nplanting phase), and the allocations are determined according to agents reports. See Figure 1 for\nan illustration of a single round of plant and steal. The algorithm then recurses on each of the sets\n21\n\nALGORITHM 9: Tentative-Allocation-Round-Robin\nInput : Ordered set of agents N= (i1, . . . , i |N|), set of items M, predictions pN\nOutput: A tentative allocationS·i∈NAi=M\nA←Allocate-Best (N, M, pN)\nM←M\\ ∪i∈NAi\n/* Reverse the order for the allocation of the rest of the items */\nNr= (i|N|, . . . , i 1)\nfork= 2, . . . ,⌈m/n⌉do\n˜A=Allocate-Best (Nr, M,pNr)\nAi←Ai∪˜Aifori∈N\nM←M\\ ∪i∈N˜Ai\nN0andN1, until all sets are of size 1. At this point, the single agent in the set is further allocated\nits remaining tentatively allocated items, and the process terminates.\nGiven the above implementation details, it remains to prove Theorem 6.1 regarding truthfulness,\nconsistency and robustness of the mechanism. The proof is given below.\n6.3 Proof of Theorem 6.1\nIn this section we prove Theorem 6.1, which we now recall.\nTheorem 6.1. TheLearning-Augmented-MMS-for- n-Agents Mechanism (Mechanism 6) is truth-\nful, 2-consistent and µ⌈3n/2⌉\ni /α-robust for α=m− ⌈3n/2⌉ −1.\nFirst, we give a simple observation regarding Algorithm 7.\nObservation 6.1. The followings hold for Algorithm Allocate-Large .\n1. If the reports equal the true valuations, and agent iis allocated an item j, then vi(j)≥vn\ni/2.\n2. After the algorithm completes its run, there are no remaining agents in Nwith large predicted\nvalues for the remaining items in M.\nWe continue to prove each of the properties specified in Theorem 6.1 separately, starting with\ntruthfulness.\nLemma 6.1 (Truthfulness) .Mechanism Learning-Augmented-MMS-for- n-agents (Mechanism\n6) is truthful.\nProof. Algorithm Tentative-Allocation-Round-Robin (Algorithm 9) only depends on agents pre-\ndictions and not their reports. Hence, we only need to consider the use of the reports in Algorithms 9\nand 10.\nFor every agent i, either they are allocated a single item in Algorithms 9, or iparticipates in\nthe recursive plant ant steal, and this is determined according to the predictions, so in particular\nrihas no affect on this. Thus, we can consider the two independent events separately. In the first\ncase, where iis allocated a single item, it is the item that maximizes their report over remaining\nitems at that point, so that ihas no incentive to lie.\nIn the second case, iparticipates in the plant and steal phase. Observe that in this case,\nwhenever ichooses an item from some set A′, it will have no future interaction with this set. That\n22\n\nALGORITHM 10: Split-Plant-Steal-Recurse\nInput : Ordered set of agents N= (i1, . . . , i |N|), tentative allocations A, partial allocations XN,\nfirst-level-flag indicating if this is the first level of the recursion, reports rN, predictions pN\n/* Halting condition - Allocate all remaining items */\nifN={i}then setXi=Xi∪Aiandhalt\n/* Split the agents into two almost-equal parts */\npar=Nmod 2\nN0←(i1, i3, . . . , i N−1+par)\nN1←(i2, i4, . . . , i N−par)\n/* Plant according to predictions */\nfori = 1,. . . , ⌊|N|/2⌋do\nLeti0, i1denote the ithagent in N0, N1respectively.\nj∗\n0=p∗\ni0(Ai0)\nj∗\n1=p∗\ni1(Ai1)\nAi0=Ai0+j∗\n1−j∗\n0\nAi1=Ai1+j∗\n0−j∗\n1\n/* Plant in’s favorite item in a tentative set */\nifpar= 1then\ni0=in,i1=i2\nj∗\n0=p∗\ni0(Ai0)\nAi1=Ai1+j∗\n0,Ai0=Ai0−j∗\n0\n/* Steal from the opposite set according to reports */\nforeach b∈ {0,1}do\nˆX=Allocate-Best (Nb, AN¬b,r)\nforeach i∈Ndo\nXi←Xi∪ˆXi\n/* Reverse the order after the first level of recursion */\niffirst-level-flag then\nN0←(iN−1+par, . . . , i 3, i1)\nN1←(iN−par, . . . i 4, i2)\n/* Recursively invoke Split-Plant-Steal-Recurse on each set */\nforeach b∈ {0,1}do\nSplit-Plant-Steal-Recurse (Nb, ANb, XNb,first-level-flag =False )\nis, fix a recursive call and assume without loss of generality that i∈N0. Then after the planting\nstep, iis allocated the item in AN1that maximizes their reports. Then, in following recursive steps,\nionly continues to interact with items in AN0, soi’s choice does not affect the identity of the items\nfrom which iwill be able to choose from in future rounds. Hence, i’s only incentive is to maximize\nthe value of its allocated value in each round, implying truthfulness.\nDue to the above lemma, from now on we assume agents report truthfully, i.e., that for every\nagent i,ri=vi. We turn to show the mechanism is consistent, we rely on the following theorem.\nTheorem 6.2 (Lemma 2 in [10] (based on Theorem 3.5 in [8])) .If for every i∈Nandj∈M,\nvi(j)≤1\n2µn\ni, then the Round-Robin algorithm returns an allocation that is MMS/ 2.\n23\n\nFurthermore, their analysis holds when changing the order of allocation between the different\nrounds of the Round-Robin.\nWe are now ready to prove the mechanism is consistent.\nLemma 6.2 (Consistency) .If the set of predictions is accurate, then for every i,vi(Xi)≥µn\ni/2.\nProof. First consider agents that were allocated an item in Algorithm Allocate-Large (Algo-\nrithm 7). If the predictions are accurate, then each such agent iis allocated an item jsuch that\nvi(j)≥µn\ni/2 and so the statement holds. Moreover, at the end of this step, there are no remaining\nagents with large predicted values, hence, no agents with large values remain.\nIf the set of predictions is accurate, then the tentative allocation determined according to\nagents’ predictions in Algorithm Tentative-Allocation-Round-Robin ( Algorithm 9) is identical\nto a Round-Robin mechanism according to valuations, with reversing the order between the first\nand all subsequent rounds. Furthermore, by the above, there are no agents with large values when\nthe Round-Robin is invoked. Therefore, by Theorem 6.2, it holds that for every i,vi(Ai)≥µn\ni/2.\nWe shall prove that for every agent i, its final allocation equals its tentative allocation, Xi=Ai,\nconcluding the proof.\nWe prove that in depth kof the recursion, every agent iis allocated the kthitem in Ai. We\nprove the claim by induction on the depth kof the recursion, and the ℓthagent in that round that\nis allocated some value.\nWe first prove for k= 1, ℓ= 1. In the plant phase, ℓ0(= 1) plants j=p∗\nℓ0(Aℓ0) inAℓ1. Then,\nin the stealing phase, during the invocation of Algorithm 8, agent ℓ0is the first to choose an item\nfrom AN1, which in particular contains j. Hence, the first item in A1is allocated into X1. We now\nassume the claim holds for k= 1 and ℓ−1 and prove it for ℓ. Assume without loss of generality\nthatℓis odd so that iℓ∈N0.\nIn step ℓof the planting phase, the mechanism plants ℓ0’s (the proof for ℓ1is identical) first\n(according to value pℓ0) item in Aℓ0. Then, during the tentative allocation phase, agent ℓ0is the\nℓthto choose among the items in AN1minus the items that were allocated to the ℓ−1 agents that\nwere before her in the tentative Round-Robin. By the induction hypothesis, every agent preceding\nher chose the item the mechanism planted for them previously in that round. Therefore, the item\njthat the mechanism planted for agent ℓ0is still available. Moreover, let Mℓ−1denote the set of\nitems after ℓ−1 rounds of the tentative Round-Robin in Algorithm 9. Further let Aℓ−1\nN1denote\nthe set of items after ℓ−1 rounds of the Allocate-Best algorithm invoked in the stealing phase\nwith the set N0, i.e., Aℓ−1\nN1=AN1\\S\nj∈N0,j<ℓ{Xj}. Since the order in which the agents plant and\nsteal in each round of the recursion is equivalent to the order in which the corresponding tentative\nallocation round was performed, it holds that Aℓ−1\nN1⊂Mℓ−1. Since j=p∗\nℓ0(Mℓ−1), and pℓ0=rℓ0,\nit holds that r∗\nℓ0(Aℓ−1\nN1) equals j. Therefore ℓ0will choose jtoXℓ0as claimed.\nProving the claim for a general kis almost identical. At the planting phase of the kthround,\nthe mechanism plants for every agent ℓ0∈Nk\n0their kthitem of AiinANk\n1and vice versa. A similar\nargument to the one above, shows that this item will remain available until its their turn to choose\nan item for allocation, as by the recursion hypothesis, all agents preceding iin the Round-Robin\nwill select the items the mechanism planted for them. Hence, the kthitem in Aℓ0will be allocated\ntoXℓ0.\nFinally, once the set agent ibelongs to becomes a singleton, by our halting condition, Xi←\nXi∪Ai, so together with the previous argument, we get that for every ℓ,Xi=Aias needed.\n24\n\nWe continue to prove that the mechanism is robust. We first prove in Lemma 6.3 that for each\nagent i,vi(Xi)≥v⌈3n/2⌉\ni , and then prove in Lemma 6.5 that the value of this item is not too small\ncompared to µ⌈3n/2⌉\ni .\nLemma 6.3. For every agent i,vi(Xi)≥v⌈3n/2⌉\ni .\nProof. We first prove the claim for agents that were allocated a value during the invocation of\nAlgorithm 7. By the definition of the algorithm and its truthfulness when agent iis allocated an\nitem, at most n−1 items were previously allocated to other agents. Hence, she can always choose\nhernthhighest valued item. Therefore, we have vi(Xi)≥vn\ni≥v⌈3n/2⌉\ni , as claimed.\nWe continue to prove the claim for the set of agents with no large predicted values. Consider\ntheℓthagent in N,iℓ, and consider the following coloring process. Initially, color all items in M\nblack. We will then color all items iℓwas able to choose from green , and items allocated before she\nhad the chance to choose from gray (note that these colors are unrelated to the ones in the figure).\nNote that an item turns green when it belongs to the tentative allocation of opposite set to iℓ’s and\nhas not been taken by agents preceding her in the allocation order. We claim that by the time no\nblack items remain, at most ⌈3n/2⌉ −1 have turned gray, implying that at some point during the\nrecursion, iℓcould have chosen their ⌈3n\n2⌉thhighest valued item (according to riℓ).\nWe let Nkdenote the set of agents to which iℓbelongs to at depth kof the recursion, starting\nwith N1=N. At each recursive call, Nkis partitioned into Nk\n0, Nk\n1. We further let bk∈ {0,1}\ndenote the index of the set to which iℓbelongs to: iℓ∈Nk\nbk. We will separately bound the number\nof items turned gray due to agents in Nk\nbkandNk\n¬bk.\nIn the first iteration, for k= 1, let A1\nN1\nb0, A1\nN1\n¬b0denote the tentative sets allocated to the agents\nofN1\n0andN1\n1after the planting phase (i.e., at the beginning of the stealing phase).\nThe number of items that turn gray due to agents in N1\nb1isG1\nb1=⌈ℓ/2⌉ −1, since iℓhas access\nto all items in A1\nN¬b1excluding the ⌈ℓ/2⌉ −1 items that were allocated to the agents in her set\npreceding her in the ordering. (The rest of the items in A1\nN¬b1turn green.)\nTurning to G1\n¬b1, each agent in the opposite set to hers, N1\n¬b1, is allocated a single item (from\nA1\nN1\nb1) before continuing to the next round of the recursion. Therefore, G1\n¬b1=|N1\n¬b1|(and no item\nturns green).\nThe recursion then continues with N2=N1\nb1and in reversed order (due to the order being\nreversed). Therefore, at the beginning of the second iteration, iℓis in location |N1\nb1| − ⌈ℓ/2⌉inN2.\nAfter the partition phase, iℓis in set N2\nb2and in location ⌈|N1\nb1|−⌈ℓ\n2⌉\n2⌉. Hence, G1\nb1=⌈|N1\nb1|−⌈ℓ\n2⌉\n2⌉ −1\ndue to agents in her set preceding here in the ordering. Also, G1\n¬b1=|N1\n¬b1|due to allocations to\nagents in the opposite set to hers.\nFrom now on, the order is preserved, so for every k≥3,Gk\nbk=|Nk\nbk|andGk\n¬bk=⌈|N1\nb1|−⌈ℓ/2⌉\n2k−1⌉−1.\nWe continue by boundingP⌈logn⌉\nk=1Gk\n¬bk=P⌈logn⌉\nk=1|Nk\n¬bk|. Observe that if Nkis even then\nNk\nbk=Nk\n¬bk=Nk/2, and if Nkis odd, then either Nk\nbkis odd and Nk\n¬bkis even or vice versa. In\nthe first case, Gk\n¬bk=⌈Nk/2⌉and we recurse with Nk\nbkwhich is of size ⌊NK/2⌋. In the second case,\nGk\n¬bk=⌊NK/2⌋and we recurse with Nk\nbkof size ⌈Nk/2⌉. Hence, we have the following recursion\nformula. For even ℓ,T(ℓ) =ℓ/2 +T(ℓ/2), and for odd ℓ,either (a) T(ℓ) =⌈ℓ/2⌉+T(⌊ℓ/2⌋) or (b)\nT(ℓ) =⌊ℓ/2⌋+T(⌈ℓ/2⌉). In Claim 6.4 below, we prove that for such a function, if it also holds\nthatT(1) = 0 and T(2) = 1, then T(ℓ)≤ℓ−1. Therefore, we get thatP⌈logn⌉\nk=1Gk\n¬bk≤n−1.\n25\n\nWe continue to boundP⌈logn⌉\nk=2Gk\n¬bk=P⌈logn⌉\nk=2⌈|N1\nb1|−⌈ℓ/2⌉\n2k−1⌉ −1. The sumP⌈logX⌉\nk=1⌈X\n2k⌉can be\nbounded by\u0010P⌈logX⌉\nk=1X\n2k\u0011\n+L, where Lis the number of indices kfor which the fraction X/2kis\nrounded up. Observe that for every X,Lcan be bounded above by ⌈logX⌉asLexactly equals\nthe number of 1 bits in the binary representation of X. Hence, the overall number of items that\nturn gray can be bounded as follows:\nG⌈logn⌉=⌈logn⌉X\nk=1\u0010\nGk\n¬bk+Gk\nbk\u0011\n(5)\n≤n−1 +⌈ℓ/2⌉ −1 +⌈logn⌉X\nk=2\u0012\u0018⌈n/2⌉ − ⌈ℓ/2⌉\n2k−1\u0019\n−1\u0013\n(6)\n≤n−1 +⌈ℓ/2⌉ −1 +⌈n/2⌉ − ⌈ℓ/2⌉+⌈logn⌉ − ⌈logn⌉+ 1 (7)\n≤ ⌈3n/2⌉ −1 (8)\nTherefore, the number of items that turn gray by the end of the recursion is at most ⌈3n/2⌉ −1,\nand so iℓget their ⌈3n/2⌉highest valued item v⌈3n/2⌉\niℓ.\nWe now prove the claim regarding the cost of the recursion that was used in the previous lemma.\nLemma 6.4. LetT(n)be such that T(n) =n/2 +T(n/2)ifnis even and either (a) T(n) =\n⌈n/2⌉+T(⌊n/2⌋)or (b) T(n) =⌊n/2⌋+T(⌈n/2⌉)for odd n. Also assume T(1) = 0 , T(2) = 1 .\nThen T(n)≤n−1.\nProof. We prove the claim by induction on n. By T(1) = 0 and T(2) = 1 so the induction basis\nholds. We now assume correctness for all values smaller than nand prove for n.\nIfnis even then T(n) =n/2 +T(n/2)≤n/2 +n/2−1 =n−1, so the claim holds.\nIfnis odd, then in case (a), T(n) =⌈n/2⌉+T(⌊n/2⌋)≤ ⌈n/2⌉+⌊n/2⌋−1 =n−1, and in case\n(b),T(n) =⌊n/2⌋+T(⌈n/2⌉)−1≤ ⌊n/2⌋+⌈n/2⌉ −1 =n−1.\nFinally, we prove that the highest valued item allocated to each agent iis not too small compared\nto their MMS.\nLemma 6.5. Consider an MMS for agent i, and let j∗be the highest valued item of iin her\nallocation. Then\nvi(j∗)≥µ⌈3n/2⌉\ni /α for α=m− ⌈3n/2⌉ −1.\nProof. Consider an MMS allocation of Mfork=⌈3n/2⌉, and let Aibe the set such that vi(Ai) =\nµk\ni. By the assumption on j∗, its value is higher then the highest valued item in Ai,vi(j∗)≥v1\ni(Ai).\nTherefore, vi(Ai)≤ |Ai| ·vi(j∗), implying vi(j∗)≥vi(Ai)/|Ai|=µk\ni/|Ai|.Since |Ai| ≤m−k−1\n(as at least k−1 items must be allocated to the k−1 additional agents, it holds that vi(j∗)≥\nµ3n/2\ni/(m− ⌈3n/2⌉ −1).\nProof of Theorem 6.1. The theorem follows by Lemmas 6.1, 6.2, 6.3, and 6.5.\n26\n\n7 Experimental Results\nIn this section, we give experiments which illustrate the role of different components of our frame-\nwork for two players under various noise levels of the predictions.8The predictions we use for our\nexperiments are the predicted values of the items. The noise we introduce permutes the vectors\nof values to match the instance’s Kendall tau distance, and uses the permuted vector as predic-\ntion. We show that our framework is almost optimal for small amounts of noise while still showing\nresilience for higher noise levels. Moreover, we study the performance of variants which only use\nspecific components of our framework.\nWhen using predictions, our initial allocation procedure is a cut-and-choose procedure, imple-\nmented as follows:\n•We use the first player’s prediction to implement a water filling algorithm which sorts the\nitems by values, and then partitions the items into two sets using a greedy procedure that\nassigns each item to the set with current lowest value.\n•We use the second player’s prediction to allocated the agent the set with the higher predicted\nvalue of the two.\nThis allocation ensures that the second agent obtains their MMS value according to the prediction.\nIn the data we generates, we observe that in a sampled valuation, the two sets chosen by the water\nfilling algorithm gives the two sets the same value, up to 0.5%, which ensures that the lowest valued\nset obtains a 1 .026-approximation to the MMS.\nWe inspect the following mechanisms:\n1.Random : a mechanism that ignores reports and predictions and randomly partitions the\nitems into two sets of size m/2.\n2.Random-Steal : a mechanism that ignores predictions, randomly partitions the items into two\nsets of size m/2, and then implements the stealing phase where each player takes their favorite\nitem from the other player’s set according to reports.\n3.Partition : a mechanism that ignores reports, and partitions the items according to predic-\ntions, using the cut-and-choose procedure described above.\n4.Partition-Steal : a mechanism that partitions the items according to predictions, using the\ncut-and-choose procedure described above, and then implements the stealing phase where\neach player takes their favorite item from the other player’s set according to reports.\n5.Partition-Plant-Steal : a mechanism that implements the Plant-and-Steal framework. parti-\ntions the items according to predictions, using the cut-and-choose procedure described above,\n“plants” each player’s favorite item according to predictions, and then “steals” each player’s\nfavorite item from the other player’s set according to reports.\n8The experiments, reproducible via Matlab (2022b) at https://tinyurl.com/PlantStealExperiments, were per-\nformed on a standard PC (Intel i9, 32GB RAM) in about 30 minutes.\n27\n\nExperiments. We consider two-player scenarios with m= 100 items. For each distance measure,\nwe generate 1000 valuation profiles. For each pair of valuation profiles and corresponding Kendall\ntau distance, we generate 100 predictions based on the distance. We then assess the performance\nof the mechanisms described earlier on these instances. We examine two distinct cases regarding\nthe relationship between the players’ preference orders: the Correlated case, where both players\nhave identical preference orders, although their valuation magnitudes differ, and the Uncorrelated\ncase, where the preference orders of the players are generated independently and chosen uniformly\nat random. Further details on the procedures used to generate the valuations and predictions are\nprovided in Appendix A.\nBenchmark. We plot the percentage of these instances where both players get at least (1 −ϵ) of\ntheir MMS value for ϵ= 0.1,0.05,0.02.\nResults. The results are shown in Figure 2. We first examine the performance of the two mech-\nanisms that do not use predictions, Random andRandom-Steal . Scenarios with correlated values\nperform significantly worse, as there is a non-negligible probability of an unbalanced partition of the\nrelatively few high and medium valued items in a random partition. For ϵvalues of 0 .02,0.05,0.1,\ntheRandom strategy success rate is 11% ,25%,and 43%, respectively, under correlated preferences,\ncompared to 33% ,43%,and 60% under uncorrelated preferences. Moreover, adding the stealing\ncomponent significantly improves the success rate only in the uncorrelated case, as Random-Steal\nachieves success rates of 66% ,75%,and 87%. In the correlated case, as each player has a highly\nvaluable item stolen, their obtained value is not expected to increase.\nIn the mechanisms that use predictions, Partition ,Partition-Steal andPartition-Plant-Steal ,\nthe performance degrades as a function of noise, as expected. When comparing the performance\nofPartition , which only relies on the prediction component of our framework, and Random-Steal ,\nwhich only relies on the stealing component of our framework, we notice that in the uncorrelated\ncase, for small amount of noise guarantee a higher success rate, while as the noise increases, the\nstealing component becomes more instrumental to the performance. This is in tact with the the-\noretical results, where using the prediction is crucial to achieve the consistency guarantees, which\ntake place when the prediction is accurate, while stealing is important to achieve robustness guar-\nantees in case the prediction is inaccurate. As described above, in the case where the valuations are\ncorrelated, stealing is not expected to help. Interestingly, on fully noisy input, even Random out-\nperforms Partition asPartition might partition the items into unequally-sized sets, which performs\nworse than the equally-sized sets Random outputs.\nOur experiments show that Partition-Plant-Steal performs as well as the Partition strategy for\nsmall amounts of noise and outperforms it on uncorrelated instances for large amounts of noise.\nMoreover, for any amount of noise, it outperforms Random-Steal and converges to it for a fully\nnoisy input. This illustrates the “best of both worlds” tradeoff obtained by our framework.\nFinally, when comparing the Partition-Plant-Steal strategy to the Partition-Steal strategy, we\nobserve that Partition-Plant-Steal outperforms Partition-Steal in the correlated case with a small\namount of noise (worst-case scenario) for ϵ= 0.02, as planting guarantees your favorite items\nwould not be taken. In other scenarios, Partition-Steal outperforms Partition-Plant-Steal because\n“planting” removes a valuable item from the player’s set that might be taken otherwise, especially\nin the uncorrelated case.\n28\n\n1 510 40 160 640 2,56000.20.40.60.81ϵ=0.02Correlated\n1 510 40 160 640 2,56000.20.40.60.81Uncorrelated\n1 510 40 160 640 2,56000.20.40.60.81ϵ=0.05\n1 510 40 160 640 2,56000.20.40.60.81\n1 510 40 160 640 2,56000.20.40.60.81\nKT distϵ=0.1\n1 510 40 160 640 2,56000.20.40.60.81\nKT dist\nFigure 2: Mechanism: Random (yellow), Random-Steal (cyan), Partition (red), Partition-\nSteal (green), Partition-Plant-Steal (blue), for the correlated case (first column) and the uncorrelated\ncase (second column) for epsilons: 0 .98 (first row), 0 .95( second row) and 0 .9 (third row).\n29\n\nReferences\n[1] Priyank Agrawal, Eric Balkanski, Vasilis Gkatzelis, Tingting Ou, and Xizhi Tan. Learning-\naugmented mechanism design: Leveraging predictions for facility location. In David M. Pen-\nnock, Ilya Segal, and Sven Seuken, editors, EC ’22: The 23rd ACM Conference on Economics\nand Computation, Boulder, CO, USA, July 11 - 15, 2022 , pages 497–528. ACM, 2022. doi:\n10.1145/3490486.3538306. URL https://doi.org/10.1145/3490486.3538306 .\n[2] Elad Aigner-Horev and Erel Segal-Halevi. Envy-free matchings in bipartite graphs and their\napplications to fair division. Inf. Sci. , 587:164–187, 2022. doi: 10.1016/J.INS.2021.11.059.\nURL https://doi.org/10.1016/j.ins.2021.11.059 .\n[3] Hannaneh Akrami and Jugal Garg. Breaking the 3/4 barrier for approximate maximin share.\nCoRR , abs/2307.07304, 2023. doi: 10.48550/ARXIV.2307.07304. URL https://doi.org/10.\n48550/arXiv.2307.07304 .\n[4] Hannaneh Akrami, Jugal Garg, Eklavya Sharma, and Setareh Taki. Simplification and im-\nprovement of MMS approximation. In Proceedings of the Thirty-Second International Joint\nConference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR,\nChina , pages 2485–2493. ijcai.org, 2023. doi: 10.24963/IJCAI.2023/276. URL https:\n//doi.org/10.24963/ijcai.2023/276 .\n[5] Hannaneh Akrami, Jugal Garg, and Setareh Taki. Improving approximation guarantees for\nmaximin share. CoRR , abs/2307.12916, 2023. doi: 10.48550/ARXIV.2307.12916. URL https:\n//doi.org/10.48550/arXiv.2307.12916 .\n[6] Georgios Amanatidis, Georgios Birmpas, and Evangelos Markakis. On truthful mechanisms\nfor maximin share allocations. In Subbarao Kambhampati, editor, Proceedings of the Twenty-\nFifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY,\nUSA, 9-15 July 2016 , pages 31–37. IJCAI/AAAI Press, 2016. URL http://www.ijcai.org/\nAbstract/16/012 .\n[7] Georgios Amanatidis, Georgios Birmpas, George Christodoulou, and Evangelos Markakis.\nTruthful allocation mechanisms without payments: Characterization and implications on fair-\nness. In Constantinos Daskalakis, Moshe Babaioff, and Herv´ e Moulin, editors, Proceedings\nof the 2017 ACM Conference on Economics and Computation, EC ’17, Cambridge, MA,\nUSA, June 26-30, 2017 , pages 545–562. ACM, 2017. doi: 10.1145/3033274.3085147. URL\nhttps://doi.org/10.1145/3033274.3085147 .\n[8] Georgios Amanatidis, Evangelos Markakis, Afshin Nikzad, and Amin Saberi. Approximation\nalgorithms for computing maximin share allocations. ACM Trans. Algorithms , 13(4):52:1–\n52:28, 2017. doi: 10.1145/3147173. URL https://doi.org/10.1145/3147173 .\n[9] Georgios Amanatidis, Georgios Birmpas, Federico Fusco, Philip Lazos, Stefano Leonardi, and\nRebecca Reiffenh¨ auser. Allocating indivisible goods to strategic agents: Pure nash equilibria\nand fairness. In Michal Feldman, Hu Fu, and Inbal Talgam-Cohen, editors, Web and Inter-\nnet Economics - 17th International Conference, WINE 2021, Potsdam, Germany, December\n30\n\n14-17, 2021, Proceedings , volume 13112 of Lecture Notes in Computer Science , pages 149–\n166. Springer, 2021. doi: 10.1007/978-3-030-94676-0 \\9. URL https://doi.org/10.1007/\n978-3-030-94676-0_9 .\n[10] Georgios Amanatidis, Haris Aziz, Georgios Birmpas, Aris Filos-Ratsikas, Bo Li, Herv´ e Moulin,\nAlexandros A. Voudouris, and Xiaowei Wu. Fair division of indivisible goods: Recent progress\nand open questions. Artif. Intell. , 322:103965, 2023. doi: 10.1016/J.ARTINT.2023.103965.\nURL https://doi.org/10.1016/j.artint.2023.103965 .\n[11] Yossi Azar and Yossi Richter. The zero-one principle for switching networks. In L´ aszl´ o Babai,\neditor, Proceedings of the 36th Annual ACM Symposium on Theory of Computing, Chicago,\nIL, USA, June 13-16, 2004 , pages 64–71. ACM, 2004. doi: 10.1145/1007352.1007369. URL\nhttps://doi.org/10.1145/1007352.1007369 .\n[12] Moshe Babaioff, Tomer Ezra, and Uriel Feige. Fair and truthful mechanisms for dichoto-\nmous valuations. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,\nThirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The\nEleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual\nEvent, February 2-9, 2021 , pages 5119–5126. AAAI Press, 2021. doi: 10.1609/AAAI.V35I6.\n16647. URL https://doi.org/10.1609/aaai.v35i6.16647 .\n[13] Maria-Florina Balcan, Siddharth Prasad, and Tuomas Sandholm. Bicriteria multidimensional\nmechanism design with side information. CoRR , abs/2302.14234, 2023. doi: 10.48550/ARXIV.\n2302.14234. URL https://doi.org/10.48550/arXiv.2302.14234 .\n[14] Eric Balkanski, Vasilis Gkatzelis, and Xizhi Tan. Strategyproof scheduling with predictions.\nIn Yael Tauman Kalai, editor, 14th Innovations in Theoretical Computer Science Confer-\nence, ITCS 2023, January 10-13, 2023, MIT, Cambridge, Massachusetts, USA , volume 251\nofLIPIcs , pages 11:1–11:22. Schloss Dagstuhl - Leibniz-Zentrum f¨ ur Informatik, 2023. doi:\n10.4230/LIPICS.ITCS.2023.11. URL https://doi.org/10.4230/LIPIcs.ITCS.2023.11 .\n[15] Eric Balkanski, Vasilis Gkatzelis, Xizhi Tan, and Cherlin Zhu. Online mechanism design\nwith predictions. CoRR , abs/2310.02879, 2023. doi: 10.48550/ARXIV.2310.02879. URL\nhttps://doi.org/10.48550/arXiv.2310.02879 .\n[16] Siddharth Barman and Sanath Kumar Krishnamurthy. Approximation algorithms for maximin\nfair division. ACM Trans. Economics and Comput. , 8(1):5:1–5:28, 2020. doi: 10.1145/3381525.\nURL https://doi.org/10.1145/3381525 .\n[17] Sylvain Bouveret and Michel Lemaˆ ıtre. Characterizing conflicts in fair division of indivisible\ngoods using a scale of criteria. Auton. Agents Multi Agent Syst. , 30(2):259–290, 2016. doi:\n10.1007/S10458-015-9287-3. URL https://doi.org/10.1007/s10458-015-9287-3 .\n[18] Eric Budish. The combinatorial assignment problem: Approximate competitive equilibrium\nfrom equal incomes. Journal of Political Economy , 119(6):1061–1103, 2011.\n[19] Ioannis Caragiannis and Georgios Kalantzis. Randomized learning-augmented auctions with\nrevenue guarantees. CoRR , abs/2401.13384, 2024. doi: 10.48550/ARXIV.2401.13384. URL\nhttps://doi.org/10.48550/arXiv.2401.13384 .\n31\n\n[20] Ilan Reuven Cohen and Debmalya Panigrahi. A General Framework for Learning-Augmented\nOnline Allocation. In 50th International Colloquium on Automata, Languages, and Program-\nming (ICALP 2023) , volume 261 of Leibniz International Proceedings in Informatics (LIPIcs) ,\npages 43:1–43:21, 2023. ISBN 978-3-95977-278-5. doi: 10.4230/LIPIcs.ICALP.2023.43.\n[21] Nikhil R. Devanur and Thomas P. Hayes. The adwords problem: online keyword matching with\nbudgeted bidders under random permutations. In John Chuang, Lance Fortnow, and Pearl\nPu, editors, Proceedings 10th ACM Conference on Electronic Commerce (EC-2009), Stanford,\nCalifornia, USA, July 6–10, 2009 , pages 71–78. ACM, 2009. doi: 10.1145/1566374.1566384.\nURL https://doi.org/10.1145/1566374.1566384 .\n[22] Uriel Feige, Ariel Sapir, and Laliv Tauber. A tight negative example for MMS fair allocations.\nIn Michal Feldman, Hu Fu, and Inbal Talgam-Cohen, editors, Web and Internet Economics -\n17th International Conference, WINE 2021, Potsdam, Germany, December 14-17, 2021, Pro-\nceedings , volume 13112 of Lecture Notes in Computer Science , pages 355–372. Springer, 2021.\ndoi: 10.1007/978-3-030-94676-0 \\20. URL https://doi.org/10.1007/978-3-030-94676-0_\n20.\n[23] Jugal Garg, Peter McGlaughlin, and Setareh Taki. Approximating maximin share allocations.\nIn Jeremy T. Fineman and Michael Mitzenmacher, editors, 2nd Symposium on Simplicity in\nAlgorithms, SOSA 2019, January 8-9, 2019, San Diego, CA, USA , volume 69 of OASIcs , pages\n20:1–20:11. Schloss Dagstuhl - Leibniz-Zentrum f¨ ur Informatik, 2019. doi: 10.4230/OASICS.\nSOSA.2019.20. URL https://doi.org/10.4230/OASIcs.SOSA.2019.20 .\n[24] Mohammad Ghodsi, Mohammad Taghi Hajiaghayi, Masoud Seddighin, Saeed Seddighin, and\nHadi Yami. Fair allocation of indivisible goods: Improvements and generalizations. In ´Eva\nTardos, Edith Elkind, and Rakesh Vohra, editors, Proceedings of the 2018 ACM Conference\non Economics and Computation, Ithaca, NY, USA, June 18-22, 2018 , pages 539–556. ACM,\n2018. doi: 10.1145/3219166.3219238. URL https://doi.org/10.1145/3219166.3219238 .\n[25] Vasilis Gkatzelis, Kostas Kollias, Alkmini Sgouritsa, and Xizhi Tan. Improved price of anarchy\nvia predictions. In David M. Pennock, Ilya Segal, and Sven Seuken, editors, EC ’22: The 23rd\nACM Conference on Economics and Computation, Boulder, CO, USA, July 11 - 15, 2022 ,\npages 529–557. ACM, 2022. doi: 10.1145/3490486.3538296. URL https://doi.org/10.1145/\n3490486.3538296 .\n[26] Vasilis Gkatzelis, Alexandros Psomas, Xizhi Tan, and Paritosh Verma. Getting more by know-\ning less: Bayesian incentive compatible mechanisms for fair division. CoRR , abs/2306.02040,\n2023. doi: 10.48550/ARXIV.2306.02040. URL https://doi.org/10.48550/arXiv.2306.\n02040 .\n[27] Hadi Hosseini and Andrew Searns. Guaranteeing maximin shares: Some agents left behind. In\nZhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial\nIntelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021 , pages 238–\n244. ijcai.org, 2021. doi: 10.24963/IJCAI.2021/34. URL https://doi.org/10.24963/ijcai.\n2021/34 .\n[28] Hadi Hosseini, Andrew Searns, and Erel Segal-Halevi. Ordinal maximin share approxi-\nmation for goods (extended abstract). In Proceedings of the Thirty-Second International\n32\n\nJoint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao,\nSAR, China , pages 6894–6899. ijcai.org, 2023. doi: 10.24963/IJCAI.2023/778. URL https:\n//doi.org/10.24963/ijcai.2023/778 .\n[29] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for\nlearned index structures. In Gautam Das, Christopher M. Jermaine, and Philip A. Bernstein,\neditors, Proceedings of the 2018 International Conference on Management of Data, SIGMOD\nConference 2018, Houston, TX, USA, June 10-15, 2018 , pages 489–504. ACM, 2018. doi:\n10.1145/3183713.3196909. URL https://doi.org/10.1145/3183713.3196909 .\n[30] David Kurokawa, Ariel D. Procaccia, and Junxing Wang. Fair enough: Guaranteeing ap-\nproximate maximin shares. J. ACM , 65(2):8:1–8:27, 2018. doi: 10.1145/3140756. URL\nhttps://doi.org/10.1145/3140756 .\n[31] T Lavastida, B Moseley, R Ravi, and C Xu. Learnable and instance-robust predictions for\nonline matching, flows and load balancing. In European Symposium on Algorithms , 2021.\n[32] Pinyan Lu, Zongqi Wan, and Jialin Zhang. Competitive auctions with imperfect predictions.\nCoRR , abs/2309.15414, 2023. doi: 10.48550/ARXIV.2309.15414. URL https://doi.org/10.\n48550/arXiv.2309.15414 .\n[33] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice.\nJ. ACM , 68(4):24:1–24:25, 2021. doi: 10.1145/3447579. URL https://doi.org/10.1145/\n3447579 .\n[34] Michael Mitzenmacher. How useful is old information? IEEE Trans. Parallel Distributed Syst. ,\n11(1):6–20, 2000. doi: 10.1109/71.824633. URL https://doi.org/10.1109/71.824633 .\n[35] Benjamin Plaut and Tim Roughgarden. Almost envy-freeness with general valuations. SIAM\nJ. Discret. Math. , 34(2):1039–1068, 2020. doi: 10.1137/19M124397X. URL https://doi.\norg/10.1137/19M124397X .\n[36] Biaoshuai Tao. On existence of truthful fair cake cutting mechanisms. In David M. Pennock,\nIlya Segal, and Sven Seuken, editors, EC ’22: The 23rd ACM Conference on Economics\nand Computation, Boulder, CO, USA, July 11 - 15, 2022 , pages 404–434. ACM, 2022. doi:\n10.1145/3490486.3538321. URL https://doi.org/10.1145/3490486.3538321 .\n[37] Erik Vee, Sergei Vassilvitskii, and Jayavel Shanmugasundaram. Optimal online assignment\nwith forecasts. In David C. Parkes, Chrysanthos Dellarocas, and Moshe Tennenholtz, edi-\ntors, Proceedings 11th ACM Conference on Electronic Commerce (EC-2010), Cambridge, Mas-\nsachusetts, USA, June 7-11, 2010 , pages 109–118. ACM, 2010. doi: 10.1145/1807342.1807360.\nURL https://doi.org/10.1145/1807342.1807360 .\n[38] Adam Wierman and Misja Nuyens. Scheduling despite inexact job-size information. In Zhen\nLiu, Vishal Misra, and Prashant J. Shenoy, editors, Proceedings of the 2008 ACM SIG-\nMETRICS International Conference on Measurement and Modeling of Computer Systems,\nSIGMETRICS 2008, Annapolis, MD, USA, June 2-6, 2008 , pages 25–36. ACM, 2008. doi:\n10.1145/1375457.1375461. URL https://doi.org/10.1145/1375457.1375461 .\n33\n\n[39] Chenyang Xu and Pinyan Lu. Mechanism design with predictions. In Luc De Raedt, editor,\nProceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI\n2022, Vienna, Austria, 23-29 July 2022 , pages 571–577. ijcai.org, 2022. doi: 10.24963/IJCAI.\n2022/81. URL https://doi.org/10.24963/ijcai.2022/81 .\nA Experimental Supplement\nGenerating valuations. To generate interesting valuations for the players, we use a multi-step\nfunction to generate item values, since if the values are close together, any balanced partition obtains\ngood MMS guarantees, without considering reports and predictions. Specifically, we consider a\nthree-step (High-Med-Low) random valuation function, where each player has a high valuation\nwith a probability of 8 /m, a medium valuation with a probability of 1 /4, and a low valuation with\na probability of 1 /2. The high valuation is U[1000 ,2000], the medium valuation are U[400,800]\nand the low valuations are U[100,200] the rest of the values are U[1,2]. Figure 3 shows the value\ndistribution generated by our process for two players. we generate values over m= 100 items.\nWe generate valuations satisfying one of the two types of relations between players’ preferences:\n•Correlated : the two preference orders are identical (but not the values).\n•Uncorrelated : Both preference orders are chosen independently and uniformly at random.\nGenerating predictions. To generate predictions, we take valuations and permute elements\nrandomly to create noise. We generate predictions under varying noise levels according to the\nKendall tau distance between the valuations and the predictions. We very the Kendall tau distance\nbetween 1 to 2560, where 2560 corresponds to the expected noise level of a random permutation of\n100 items. To randomly choose a permutation of a certain noise level, we start with the ordered\npermutation and then choose two indices j < k u.a.r. and swap items randr+1 for r∈ {j, . . . , k −1}\nif it increases the Kendall Tau distance by one. We repeat this process until the distance of the\nresulting permutation equals the desired value.\nB Deferred proofs from Section 4\nProof of Lemma 4.1. By Observation 4.1, we have vi(ak\ni)≥v2k\ni, therefore\nvi(Ai) =|Ai|X\nk=1ak\ni≥⌊m/2⌋X\nk=1v2k\ni. (9)\nSince i’s favorite item must be absent from some set of the sets defining the MMS value,\nmX\nk=2vk\ni≥µi.\nSince the vk\niare ordered, v2k\ni≥v2k+1\ni, henceP⌊m/2⌋\nk=1v2k\ni≥P⌊m/2⌋\nk=1v2k+1\ni. Therefore,\n⌊m/2⌋X\nk=1v2k\ni≥µi/2 (10)\n34\n\n0 20 40 60 80 10005001,0001,5002,000\nindexvalues\nFigure 3: Plotting randomly sampled valuations for two players, where the values are sorted such\nthat lower indexed items have higher values.\nBy Equations (9),(10), we have:\nvi(Ai)≥⌊m/2⌋X\nk=1v2k\ni≥µi/2.\nProof of Lemma 4.2. We first prove the approximation for player 1 (the first player to be allocated).\nFirst, observe that v1(M)≥2µ1. Let I1={v3k−2\n1 :k= 1, . . . ,⌈m/3⌉}be the worst possible\nallocation agent 1 might get in the 1-2-Round-Robin allocation. Notice that v1(I1)≥v1(M)/3≥\n2µ1/3. By Observation 4.2, v1(ak\n1)≥v3k−2\n1. Therefore, v1(A1)≥v1(I1)≥2µ1/3.\nNow consider player 2. As stated in the proof of Lemma 4.1, v2(M\\v1\n2)≥µ2. Let\nIa\n2={v3k−1\n2 :k∈N>0∧3k−1≤m}andIb\n2={v3k\n2:k∈N>0∧3k≤m}.\nFirst, notice that\nv2(Ia\n2∪Ib\n2)≥2v2(M\\v1\n2)/3≥2µ2/3.\nMoreover, by Observation 4.2, we have, v2(a2k−1\n2)≥v3k−1\n2, and v2(a2k\n2)≥v3k\n2.Therefore, v2(A2)≥\nv2(Ia\n2∪Ib\n2)≥2µ2/3.\nProof of Theorem 4.2. By Lemma 3.1, the mechanism is truthful. By Observation 4.1, each agent\nreceives at least ⌈m/3⌉items; combining with Lemma 3.4, we get that the mechanism is ⌊2m\n3⌋-\nrobust. Finally, if predictions correspond to valuations, by Lemma 4.2 and Lemma 4.3, the alloca-\ntion is 3 /2-approximation to the MMS. Thus, the mechanism is 2 /3-consistent.\n35\n\nC Deferred proofs from Section 5\nC.1 No Mechanism with Bounded Robustness and Consistency <6/5\nIn [7] they define the following family of mechanisms.\nDefinition C.1 (Singleton Picking-Exchange Mechanisms [7]) .A mechanism Xis a singleton\npicking-exchange mechanism if for each i∈ {1,2}, there is exactly one of two sets: either Ni⊆M,\norEi={ℓi}for a single item ℓi∈M. IfNiis non-empty, then the mechanism lets player j̸=i\npick item ℓ∈Nithat maximizes vj(ℓ), and igetsNi\\ {ℓ}. If both E1, E2are non-empty, then the\nagents exchange the two items ℓ1∈E1andℓ2∈E2ifv1(ℓ2)> v1(ℓ1)andv2(ℓ1)> v1(ℓ2).Notice\nthat if m > 2, either E1orE2is empty and there will be no exchange.\n[7] showed the following.\nLemma C.1. In order for a mechanism to be truthful and have a bounded approximation, it has\nto be a singleton picking-exchange mechanism\nWe make use of this characterization in our impossibility.\nTheorem 5.1. For any ϵ >0, there is no truthful a mechanism with consistency 6/5−ϵand\nbounded robustness.\nProof. Consider the case where p1=p2= (1/2,1/2,1/3,1/3,1/3). Notice that for the predictions,\nµ1=µ2= 1. We show that for any singleton-picking-exchange mechanism, no agent obtains both\nlarge items (of value 1 /2). Consider agent 1 (the argument is symmetric for agent 2). If N1is\nnon-empty, then if both large items are in N1, surely 1 will only get one of them. If both large\nitems are in N2, then agent 2 will surely pick one of them, and agent 1 will only get one of them.\nIf one large item is in N1and the other is in N2, each agent iwill pick the large item in Ni. If\nagent 2 has a large item in E2, then since N1is non-empty, E1is empty and agent 2 will keep\nthe large item. Now consider the case where E1is non-empty. In this case, E1contains one item,\nandN1is empty. Since E2can contain at most one item, and there are more than 2 items, in this\ncase, E2=∅, and |N2|= 4. Therefore, N2contains at least one large item. Since agent 2 will\nalways pick the large item, agent one only gets one large item. We conclude that for any singleton\npicking-exchange mechanism, the large items are split among the agents. Since there are 3 small\nitems, there must be an agent that gets at most one small item, and this agent has an overall value\nof at most 1 /2 + 1 /3 = 5 /6, while the MMS is 1. Thus the claim follows.\nC.2 Proof of Proposition 5.1\nWe first show the following, which implies the first half of Proposition 5.1.\nProposition C.1. There exists a partition M=L1S·L2S·Sand and indices α1, α2, β1, β2in[m]\nsuch that M= [α1, β1]S·[α2, β2], for the sets S1=L1∪(S∩[α1, β1])andS2=L2∪(S∩[α2, β2])\nwe have\n•min{v1(S1), v1(S2)} ≥(1−ϵ/8)µ1\n•|L1|+|L2| ≤ ⌈8\nϵ⌉+ 2\n36\n\n•|S1| ≥ |S2|.\n•For every xinL1andyinS1we have v1(x)> v1(y). Analogously, for every xinL2andy\ninS2we have v1(x)> v1(y)\n•There are ˆj,ˆj′∈L1satisfying ˆj∈arg max ℓ∈S1v1(ℓ)andˆj′∈arg maxℓ∈S1\\ˆjv1(ℓ),\nWe do this by inspecting two types of items, large items, with value greater than ϵµ1/4, and\nsmall items items with value at most ϵµ1/4. We first show that there are O(1/ϵ) large items,\ntherefore, separating these items into two bundles require at most O(1/ϵ) intervals. Moreover, we\ncan find a separation of the larges items into two sets, L1, L2, and a single index j∈[m] such that\nall small items to the left of j(including) together with L1form S1, and all items to the right of j\n(excluding) together with L2form S2, such that S1, S2satisfy the approximation requirement. It\nis easy to see that this increases the number of intervals by at most 1.\nWe start by showing there are not too many large items.\nLemma C.2. There are at most ⌈8\nϵ⌉items with value strictly greater than ϵµ1for agent 1.\nProof. Let items with value greater than ϵµ1/4 be the large items. Suppose there are at least ⌈8\nϵ⌉+1\nlarge items. If ⌈8\nϵ⌉is even, consider a partition ( S1, S2) such that each Sigets at least ⌈8\nϵ⌉/2 large\nitems and the rest are allocated arbitrarily. If ⌈8\nϵ⌉is odd, consider the allocation in which each Si\ngets (⌈8\nϵ⌉+ 1)/2 large items and the rest are allocated arbitrarily. In either case, each Sigets at\nleast⌈8\nϵ⌉/2≥4\nϵlarge items. Thus, min {v1(S1), v1(S2)}> ϵµ 1/4·4\nϵ=µ1,a contradiction.\nWe are now ready to prove Proposition C.1.\nProof of Proposition C.1. Consider the set of large items, L={j∈[m] :v1(j)> ϵµ 1/4}, and let\nS=M\\Lbe the set of small items.\nWe give a constructive proof which finds both sets L1, L2and an index jsatisfying the condition\nstated in the lemma. Let\n(L1, L2)∈arg max(T1,T2) :T1S·T2=Lmin\nj∈{1,2}v1(Sj).\nWe use the following procedure to find j.\n1. Let jℓ= 0 and jr=m.\n2. While jℓ̸=jr:\n(a) Let Sℓ=L1∪ {j′∈S:j′≤jℓ}andSr=L2∪ {j′∈S:j′> jr}.\n(b) If v1(Sℓ)< v1(Sr) :\n•jℓ:=jℓ+ 1.\n(c) Else:\n•jr:=jr−1.\n3. Set j:=jℓ=jr.\n37\n\nWe consider two cases:\nCase 1: j= 0 (or symmetrically, j=m). Without loss of generality, suppose that j=m.We\nfirst show that if v1(S1)< v1(S2) then min {v1(S1), v1(S2)}=µ1. Notice that since S1gets all the\nsmall items, it must be the case that v1(L1)< v1(L2).Suppose there’s a different partition T1S·T2\nsuch that min {v1(T1), v1(T2)}>min{v1(S1), v1(S2)}.Without loss of generality, let v1(T1∩L)≤\nv1(T2∩L) (otherwise, we can rename both bundles). By the definition of L1, L2,it must be the\ncase that v1(L1)≥v1(T1∩L). Thus, Since T1\\(T1∩L)⊆S, it must be that\nv1(S1) = v1(L1) +v1(S)≥v1(T1∩L) +v1(T1\\(T1∩L)) = v1(T1)≥min{v1(T1), v1(T2)},\na contradiction.\nOn the other hand, if v1(S1)≥v1(S2) =v1(L2), by condition 2b of the above procedure, it\nmust be the case that when jℓwas equal m−1,\nv1(Sℓ)< v 1(Sr) = v1(L2) = v1(S2).\nThus,\nv1(S1) = v1(Sℓ) +v1(m)< v 1(S2) +ϵµ1/4.\nWe get that\nv1(S2)≥v1(S1)−ϵµ1/4≥2µ1−v1(S2)−ϵµ1/4⇒\nmin{v1(S1), v1(S2)}=v1(S2)≥(1−ϵ/8)µ1, (11)\nwhere the second inequality follows since 2 µ1≤v1(S1) +v1(S2).\nCase 2: 0 < j < m . In this case, since both jℓandjrwere moved, there were some values of jℓ\nandjrsuch that v1(Sℓ)≤v1(Sr) and some values such that v1(Sℓ)> v1(Sr). Assume initially that\nv1(Sℓ)≤v1(Sr). Since at each step of the procedure, the lower-valued bundle can increase by at\nmost ϵµ1/4, when the first item is added to Sℓsuch that v1(Sℓ)> v1(Sr), it must be the case that\nv1(Sℓ)≤v(Sr) +ϵµ1/4.It is easy to see that the invariant where |v1(Sℓ)−v1(Sr)| ≤ϵµ1/4 is kept\nthroughout the run of the procedure. Therefore, this also holds for the final S1andS2. Thus, we\ncan use the same reasoning of Eq. (11) to conclude that min {v1(S1), v1(S2)} ≥(1−ϵ/8)µ1.\nThus, the sets S1andS2have a form S1=L1∪(S∩[1, j]) and S2=L2∪(S∩[j+ 1, m]) and\nhave the form required. If |S1|<|S2|we can swap our definitions for the sets S1andS2, thus\nensuring that |S1|>|S2|. Due to our definitions of L1andL2we have for every xinL1andyin\nS1we have v1(x)> v1(y). Analogously, for every xinL2andyinS2we have v1(x)> v1(y).\nWe can ensure that There are ˆj,ˆj′∈L1satisfying\nˆj∈arg max\nℓ∈S1v1(ℓ) and ˆj′∈arg max\nℓ∈S1\\ˆjv1(ℓ),\nby adding such values from S∩[α1, β1] toL1(we see that after this all other properties still hold).\nOverall, we see that |L1|+|L2| ≤ ⌈8\nϵ⌉+ 2, as required.\nNow, we proceed to proving the second half of Proposition 5.1. We will need the following\nlemma.\n38\n\nLemma C.3. Letk1andk2be positive integers satisfying k1> k2, and let fbe a function mapping\n[k1]to non-negative real numbers. Then, there exist a pair of integers α, β, α′andβ′in[k1]such\nthat|[α, β]∪[α′, β′]|=k2and\nP\ni∈[α,β]∪[α′,β′]f(i)\nk2≤P\ni∈[k1]f(i)\nk1\nProof. We prove the lemma using the probabilistic method. Let jbe a uniformly random integer\nin [k1], and choose α, β, α′andβ′such that\n[α, β]∪[α′, β′] ={j, j+ 1 mod k1,···, j+k2−1 mod k1}.\nWe see that indeed a set chosen as above can be represented as a union of two intervals. Now, since\njis chosen uniformly at random form [ k1], we see that for every element iin [k1] we have\nPr\nj∼[k1][i∈ {j, j+ 1 mod k1,···, j+k2−1 mod k1}] =k2\nk1.\nThus via linearity of expectation we have:\nEj∼[k1]\n1\nk2X\ni∈{j,j+1 mod k1,···,j+k2−1 mod k1}}f(i)}\n=1\nk1X\ni∈[k1]f(i).\nThus, since f(i) is non-negative for all values of i, we see that for some specific choice of jit has\nto be the case that\n1\nk2X\ni∈{j,j+1 mod k1,···,j+k2−1 mod k1}}f(i)} ≤1\nk1X\ni∈[k1]f(i),\nwhich finishes the proof.\nNow, we apply the lemma above. If m < 4⌈t\nϵ⌉+ 2 we can satisfy Proposition C.1 by:\n1. First choosing a partition M=S1S·S2such that min( v1(S1), v1(S2))≥µ1and|R1| ≥ |R2|.\n2. Define L2:=S2, put the smallest ⌊m/2⌋−|S2elements of S1intoS, and define L1to contain\nthe rest of elements in S1.\n3. Define α1=α3= 1,β1=β3=m,α2=β2=α4=β4=m+ 1.\nOverall, this allocates S′to be the bottom ⌊m/2⌋elements of S1. We see that this suffices to\nguarantee the properties that S′needs to satisfy in Proposition 5.1.\nTherefore, henceforth we can assume that m > 4⌈t\nϵ⌉+ 2. Since |S1| ≥m/2 and |L1| ≤8\nϵ+ 2,\nandS1=L1∪(S∩[α1, β1]) this implies that |S∩[α1, β1])|>3\u0006t\nϵ\u0007\n> m/ 2 Thus, we can ensure\nthat|S′|=⌊m/2⌋ − |S2|using a subset S′⊂S∩[α1, β1]).\nIf|S2|= 1 we only need choose S′to satisfy |S′|=⌊m/2⌋−|S2|andv1(S′)≤v1(S1\\{ˆj,ˆj′})/2.\nFirst of all, since every element in L1is larger than any element in S∩[α1, β1]), we see that this is\nalso true in averageP\nℓ∈S1v1(ℓ)\n|S1|≤P\nℓ∈S∩[α1,β1])v1(ℓ)\n|S∩[α1, β1])|(12)\n39\n\nThen, applying Lemma C.3 to the set S∩[α1, β1]) we see that there exist disjoint subsets [ α3, β3]\nand [ α4, β4] of [α1, β1] such that |S∩([α3, β3]S[α4, β4]))|=⌊m/2⌋ − |S2|\nP\nℓ∈S∩[α1,β1])v1(ℓ)\n|S∩[α1, β1])|≤P\nℓ∈S∩([α3,β3]S[α4,β4]))v1(ℓ)\n|S∩([α3, β3]S[α4, β4]))|(13)\nCombing Equations 12 and 13, we see that taking S′=S∩([α3, β3]S[α4, β4]) satisfies v1(S′)≤\nv1(S1)/2 and the other requirements of Proposition 5.1.\nNow, suppose |S2|>1. Since by Proposition C.1, the set Sdoes not contain the two largest\nelements ˆjandˆj′ofS1, as well as the fact that every element in L1is at least as large as any\nelement in S∩[α1, β1]), we see that every every element in S1\\ {ˆj,ˆj′}is either in S∩[α1, β1]) or\ngreater than every element in S∩[α1, β1]). this implies that:\nP\nℓ∈S1\\{ˆj,ˆj′}v1(ℓ)\n|S1| −2≤P\nℓ∈S∩[α1,β1])v1(ℓ)\n|S∩[α1, β1])|(14)\nThen, we can again apply applying Lemma C.3 to the set S∩[α1, β1]) we see that there exist\ndisjoint subsets [ α3, β3] and [ α4, β4] of [α1, β1] such that |S∩([α3, β3]S[α4, β4]))|=⌊m/2⌋ − |S2|\nP\nℓ∈S∩[α1,β1])v1(ℓ)\n|S∩[α1, β1])|≤P\nℓ∈S∩([α3,β3]S[α4,β4]))v1(ℓ)\n|S∩([α3, β3]S[α4, β4]))|(15)\nCombing Equations 14 and 15, we see that taking S′=S∩([α3, β3]S[α4, β4]) satisfies v1(S′)≤\nv1(S1\\{ˆj,ˆj′})/2 as required in Proposition 5.1. Note that this also implies that v1(S′)≤v1(S1})/2\nsince ˆj,ˆj′have the top two largest values of v1inS1. Overall, this finishes the proof of Proposition\n5.1.\nC.3 Proof of (2 +ϵ)-consistency.\nIt remains to show that the algorithm is 2 + ϵ-consistent. We will be referencing the variables\nˆj1,ˆj2,˜j1,˜j2, T1andT2within the Plant-And-Steal framework (Algorithm 1).\nWe first reason about agent 2. First, notice that since agent 2 has a higher value for ˜Si2,\nv2(˜Si2)≥µ2.\nSince the mechanism had a chance to pick item ˆj2from T1as˜j2, it must be the case that\nv2(˜j2)≥v2(ˆj2) (and possibly ˜j2=ˆj2). If˜j1=ˆj1, then T2\\˜j1=˜Si2\\ˆj2, and\nµ2≤v2(˜Si2) = v2(˜Si2\\ˆj2) +v2(ˆj2)≤v2(T2\\˜j1) +v2(˜j2) = v2(X2).\nOtherwise, ˜j1∈˜Si2, and\n˜Si2\\ˆj2\\˜j1⊂T2\\˜j1⇒v2(˜Si2\\ˆj2\\˜j1)≤v2(T2\\˜j1). (16)\nSince ˆj2is the item with the highest value for agent 2 in ˜Si2, v2(˜j2)≥v2(ˆj2)≥v2(k1).\nCombining with Eq. (16), we get that\nv2(T2\\˜j1∪ {˜j2})≥v2(˜Si2\\ˆj2).\n40\n\nMoreover,\nv2(T2\\˜j1∪ {˜j2})≥v2(˜j2)≥v2(ˆj2).\nThus,\nv2(X2) = v2(T2\\˜j1∪ {˜j2})≥v2(˜Si2)/2 = µ2/2,\nas desired.\nIt is left to show that v1(X1)≥µ1/(2 +ϵ).Ifi1= 2, then\nv1(˜Si1) = v1(˜S2)≥v1(S2)≥(1−ϵ/4)µ1.\nIn this case, the same exact arguments used for agent 2 can be harnessed to show that v1(X1)≥\n(1−ϵ/4)µ1/2≥µ1/(2 +ϵ).Thus, it is left to consider the case where i1= 1.\nConsider the ( S1, S2) partition that is set in the first step of Cut-and-Balance-and-Choose .\nSince v1(S′)≤v1(S1)/2, we have\nv1(˜S1)≥v1(S1)/2≥(1−ϵ/4)µ1/2≥µ1\n2 +ϵ.\nIf˜j2=ˆj2, we have that\nv1(X1) =v1(˜S1∪ {˜j1} \\ {ˆj1})≥v1(˜S1)≥µ1\n2 +ϵ,\nwhere the first inequality follows since v1(˜j1)≥v1(ˆj1).\nNote also that if |S2|= 1 i.e., S2={a}, ifˆj2̸=athen v1(X1)≥v1(S2) since a∈T2, similarly\nif˜j2̸=athen v1(X1)≥v1(S2), finally we have ˆj2=k2=aandv1(X1)≥µ1/(2 +ϵ) an in the first\ncase.\nTherefore, we assume ˜j2̸=ˆj2and|S2|>1, and let ˆj′\n1∈arg maxj∈˜S1\\{ˆj1}v1(j)\nv1(X1) = v1(T1∪ {˜j1} \\ {˜j2})\n=v1(T1) +v1(˜j1)−v1(k2)\n≥v1(˜S1∪ {ˆj2} \\ {ˆj1}) +v1(ˆj1)−v1(˜j2)\n≥v1(˜S1\\ {ˆj1}) +v1(ˆj1)−v1(˜j2)\n=v1(S1\\S′\\ {ˆj1}) +v1(ˆj1)−v1(˜j2)\n≥v1(S1\\S′\\ {ˆj1}) +v1(ˆj1)−v1(ˆj′\n1)\n=v1(S1\\S′\\ {ˆj1,ˆj′\n1}) +v1(ˆj1),\nwhere the first inequality is since, v1˜j1= max j∈T2v1j≥v1ˆj1. The second inequality is since\nv1(ˆj2)≥0, the third inequality is by ˆj′\n1definition since ˜j2∈˜S1\\ˆj1by our assumption that k2̸=ˆj2.\nFinally, we have have |S1\\S′\\ {ˆj1,ˆj′\n1}| ≥ | S′|since\n|S1| −2− |S′|=|S1| −2−(m/2− |S2|) =|S1| −2−(m/2−(m− |S1|)) =m/2−2≥ |S′|,\nwhere the last inequality is since |S2|>1. Since we handles the case |S2|= 1 earlier, we can here\nassume |S2|>1 in which case the set S′is required to satisfy v1(S′)≤v1(S1\\{ˆj,ˆj′})/2. Therefore,\nwe have v1(S1\\S′\\ {ˆj1,ˆj′\n1} ≥v1(S′).\n41\n\n(1−ϵ/4)µ1≤v1(S1) = v1(S1\\S′\\ {ˆj1,ˆj′\n1}) +v1(ˆj1) +v1(ˆj′\n1) +v1(S′)\n≤v1(S1\\S′\\ {ˆj1,ˆj′\n1}) + 2·v1(ˆj1) +v1(S′)\n≤2·v1(S1\\S′\\ {ˆj1,ˆj′\n1}) + 2·v1(ˆj1)\n≤2·v1(X1),\nwhich implies that v1(X1)≥µ1\n2+ϵ, finishing the proof.\n42",
  "textLength": 107649
}