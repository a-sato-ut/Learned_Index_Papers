{
  "paperId": "50b5bf21ac66b81cdc01f685e70f602e71983c72",
  "title": "Updatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to Design Choices",
  "pdfPath": "50b5bf21ac66b81cdc01f685e70f602e71983c72.pdf",
  "text": "139Updatable Learned Indexes Meet Disk-Resident DBMS - From\nEvaluations to Design Choices\nHAI LAN, RMIT University, Australia\nZHIFENG BAOâˆ—,RMIT University, Australia\nJ. SHANE CULPEPPER, RMIT University, Australia\nRENATA BOROVICA-GAJIC, The University of Melbourne, Australia\nAlthough many updatable learned indexes have been proposed in recent years, whether they can outperform\ntraditional approaches on disk remains unknown. In this study, we revisit and implement four state-of-the-art\nupdatable learned indexes on disk, and compare them against the B+-tree under a wide range of settings.\nThrough our evaluation, we make some key observations: 1) Overall, the B+-tree performs well across a range\nof workload types and datasets. 2) A learned index could outperform B+-tree or other learned indexes on\ndisk for a specific workload. For example, PGM achieves the best performance in write-only workloads while\nLIPP significantly outperforms others in lookup-only workloads. We further conduct a detailed performance\nanalysis to reveal the strengths and weaknesses of these learned indexes on disk. Moreover, we summarize the\nobserved common shortcomings in five categories and propose four design principles to guide future design\nof on-disk, updatable learned indexes: (1) reducing the indexâ€™s tree height, (2) better data structures to lower\noperation overheads, (3) improving the efficiency of scan operations, and (4) more efficient storage layout.\nCCS Concepts: â€¢Information systems â†’Data access methods ;Data layout .\nAdditional Key Words and Phrases: Learned Indexes\nACM Reference Format:\nHai Lan, Zhifeng Bao, J. Shane Culpepper, and Renata Borovica-Gajic. 2023. Updatable Learned Indexes\nMeet Disk-Resident DBMS - From Evaluations to Design Choices. Proc. ACM Manag. Data 1, 2, Article 139\n(June 2023), 22 pages. https://doi.org/10.1145/3589284\n1 INTRODUCTION\nDriven by the promising in-memory performance profiles demonstrated in a pioneer work on\nlearned index [ 12], several in-memory, updatable learned indexes [ 7,9,10,30] have been proposed\nsubsequently. Their in-memory superiority has also been verified in a recent comprehensive\nevaluation [29].\nMeanwhile, many widely used Database Management Systems (DBMSs) still rely on disk-based\noperations for two main reasons: (1) the size of main memory is limited and the total size of the\nindexes can exceed the total RAM available on commodity hardware [ 4]; (2) in a DBMS, main\nmemory is also used to perform expensive data processing operations, such as joins or sorting,\nor to perform data modeling and analysis. Thus, if all of the index data structures are loaded into\nâˆ—corresponding author\nAuthorsâ€™ addresses: Hai Lan, RMIT University, Melbourne, Victoria, Australia; Zhifeng Bao, RMIT University, Melbourne,\nVictoria, Australia; J. Shane Culpepper, RMIT University, Melbourne, Victoria, Australia; Renata Borovica-Gajic, The\nUniversity of Melbourne, Melbourne, Victoria, Australia.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\nÂ©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n2836-6573/2023/6-ART139 $15.00\nhttps://doi.org/10.1145/3589284\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\n139:2 Hai Lan et al.\nmain memory as well, these operations can perform poorly, or in the worst case, fail to run at\nall. Moreover, the updatable, on-disk indexes are critical to support two common workload types\nâ€“ online transaction processing (OLTP) workloads and hybrid transaction/analytical processing\n(HTAP) workloads.\nTo this end, an important question still remains â€“ Can updatable learned indexes fully replace\ntraditional on-disk indexes, such as the B+-tree? However, there has not been any study trying to\nimplement those updatable learned indexes on disk, not to mention conducting a comprehensive\nevaluation of their on-disk performance [13].\nTo answer this question, we for the first time study, extend, implement, and evaluate updatable\nlearned indexes using one-dimensional data, on-disk, with two key goals: (1) To better understand\nhow in-memory learned indexes perform in an on-disk setting; (2) To provide practitioners and\nresearchers design decisions we have discovered when adapting learned indexes to disk-resident\nsettings. In summary, this work makes the following contributions:\n1â—‹We compare and contrast four state-of-the-art, in-memory, updatable learned indexes,\nFITing-tree [ 10], ALEX [ 7], PGM [ 9], and LIPP [ 30]), and show how to extend and imple-\nment each of them as on-disk indexes. Specifically, we first discuss how each index supports\nin-memory operations in Section 2. Then, in Section 3, we show how design decisions made in\nthese indexes affect different types of workloads, such as data partitioning or searching in leaf\nnodes. Next, in Section 4, we show how to extend and implement each of them on disk, and discuss\nthe External Memory (EM) model performance bounds for each learned index. Some of the learned\nindexes are difficult to re-implement as an on-disk data structure, e.g., ALEX being the most difficult.\n2â—‹We examine how on-disk operations affect learned indexes and compare each of them\nto the B+-Tree â€“ one of the most efficient and commonly used on-disk data structures\nin the database community. We perform a comprehensive evaluation in Sections 5-6, and test\nthe indexes using eleven different datasets and six workload types â€“ lookups, scan, inserts, heavy\nreads, heavy writes, and balanced reads and writes on two disk types, an HDD disk and an SDD\ndisk. Those datasets exhibit a variety of different properties in terms of difficulty of being modeled\nwith a linear function, which is the most common approach in most learned indexing models. We\nalso study the impact of caching inner nodes of these indexes in main memory. Finally, we study\nthe storage usage of each index, the impact of block size, and the robustness of each index by\nreporting the tail latencies. Based on our evaluation results, we create a set of observations ( O),\nand provide key take-aways, which can be summarized as follows (detailed observations can be\nfound in Section 6):\nK1. Performance on Disk (O1-O13) : Each learned index has both strengths and weaknesses â€“\nyet none of them are competitive with the B+-tree across all tested workloads, in an on-disk setting.\nK2. Main Memory Impact (O14-O15): When caching inner nodes in main memory, B+-tree\ncan outperform learned indexes in all tested workloads, on all datasets, since the main overhead\nof learned indexes for search and insert is in â€œthe last mileâ€ â€“ the leaf node traversal, which is\nconsistently larger than a B+-tree leaf node.\nK3. Storage Usage (O16): When including the leaf node size in a comparison, with the exception\nof PGM, existing learned indexes require more space than an on-disk B+-tree, and can even be 20x\nlarger. The space used on disk cannot be reclaimed easily, and learned indexes require significant\nmodifications to the index structure which further increases the storage size.\nK4. Impact of Block Size (O17) : A larger block size can help reduce the total number of fetched\nblocks in the FITing-tree, ALEX, and PGM, but LIPP does not show any benefits from such a change.\nK5. Robustness (O18): The B+-tree has more stable and a much smaller p 99latency in most\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to Design Choices 139:3\nevaluation settings, which translates to far fewer tail queries that are significantly slower than the\naverage.\n3â—‹We discuss important design decisions that must be considered when creating an\non-disk, updatable learned index. Based on our evaluation observations, we summarize the\ncommon shortcomings of on-disk, updatable learned indexes in Section 7.1. Finally, we propose\nfour important design principles to guide the future design of on-disk, updatable learned indexes\nin Section 7.2:\n(1)reducing the indexâ€™s height from root node to leaf node ;\n(2)better data structures to lower operation overheads ;\n(3)improving the efficiency of scan operations ;\n(4)more efficient storage layout .\nWe believe these principles and our implementations [ 1] will help both researchers and prac-\ntitioners to develop new disk-resident learned indexes which could be used in many practical\napplications.\n2 UPDATABLE LEARNED INDEXES REVISITED\nWe now revisit four updatable learned indexes in detail and discuss other indexes related to our\nwork. We broadly classify the them into two categories according to how they are constructed:\nbottom-up , which includes FITing-tree and PGM; and top-down , which includes ALEX and LIPP.\nTable 1 provides a taxonomy for them. For each index, we describe the index structure, followed by\nhow lookup and insert operations are performed.\nTable 1. A Taxonomy of Studied Indexes\nIndex YearInner Node Leaf NodeInsert2Data Partition Node SizeStructure Modification\nSearch Algo.1Error Search Algo. Error Strategy Memory Reuse\nB+-tree - B.S Tunable B.S Tunable Empty Slot Evenly Tunable Greedy âœ“\nFITing-tree [10] 2019 B.S Tunable Model + B.S Tunable Buffer Greedy Node-related Greedy Ã—\nPGM [9] 2020 Model + B.S Tunable Model + B.S Tunable Append/Rebuild Streaming Algo. N.A. Greedy Ã—\nALEX [7] 2020 Model Exact Model + E.S Unfixed Gapped Array - Tunable Cost-based Ã—\nLIPP [30] 2021 Model3Exact Model Exact Gapped Array - Tunable4GreedyÃ—\n1B.S, E.S, and Model denote binary search, exponential search, and predicting the position with a model, respectively.\n2Here, we refer to how these indexes store the new insertion key-value pairs.\n3LIPP does not distinguish the inner node and leaf node. We set the Inner Node andLeaf Node with same values.\n4Although the authors state that a maximum node size is set, they could create a larger node size than the parameter (line 2 in Algorithm 5 [30]).\nsegmentbuffersegmentbufferkey, slopepointerâ€¦key(key, payload)B+ Treekey, slopepointerkey, slopepointerâ€¦\n(a) FITing-tree Struc-\nture\n1 item2 items4 items1 item2 items4 items(key, payload)â€¦â€¦Merge the keys and buildsegmentsegmentsegmentk, sl, ick, sl, ick, sl, ick, sl, icsegmentk, sl, ick, sl, ick, sl, ickeyâ€¦â€¦(b) PGM Structure\nMInner Node AMMMData Nodeğ’Œğ’†ğ’šâˆˆ[ğŸ,ğŸ)\n01/41/23/410.51.0CDF at inner Node A (c) ALEX Structure [7]\nDATANODENULLMMMMMâ€¦key(key, payload)\nMMâ€¦ (d) LIPP Structure\nFig. 1. Structures of the Learned Indexes Studied in This Work.\n2.1 Learned Indexes Built Bottom-up\nFITing-tree [10]. Figure 1(a) shows a FITing-treeâ€™s layout. Given an error bound, which indicates\nthe maximum distance between the predicted position of one key and its true position, a FITing-tree\nfirst applies a greedy method to split the sorted array into segments, where each segment containing\na linear model (the first key of the segment and the slope) to predict the position of a key. Then, a\nB+-tree is built over the keys covered by each linear model.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\n139:4 Hai Lan et al.\nTo support lookup operations, a FITing-tree first traverses from the root to a leaf node using a\nlinear scan over inner nodes. After locating the segment that contains the search key, the learned\nlinear model is used to predict the position pred_pos , and then a binary search is applied to the\nrange [ pred_pos-error, pred_pos+error ].\nTo support insert operations, a FITing-tree has two strategies, an Inplace Insert Strategy and a\nDelta Insert Strategy . For the Inplace Insert Strategy , there areğœ€empty slots added to the beginning\nand end of each segment. The index has a predefined error bound equal to ğ‘’+ğœ€, whereğ‘’is derived\nfrom the error from the linear model. If a segment is full, a greedy method is called on the key-\npayload pairs of the targeted segment to create a new segment using a resegment operation. The\nDelta Insert Strategy adds a buffer to every segment. New key-payload pairs are inserted into the\nbuffer first. When the buffer is full, a resegment operation is triggered.\nPGM [9]. Given a predefined error bound, PGM uses a streaming algorithm [ 23] rather than a greedy\nalgorithm to create segments and the associated linear models (start key, slope, and intercept).\nThen, a streaming algorithm is applied recursively to construct the parent nodes for the keys in the\nmodels.\nTo support lookup operations, PGM first uses the model to predict a position pos, and then a\nbinary search is applied on the range [ pred_pos-error, pred_pos+error ].\nTo support insert operations, a different insert mechanism is proposed for Append-only Insert\nandArbitrary Insert . For an Append-only Insert , PGM first tries to add the new key into an end\nsegment and checks if it is outside the targeted error bound. If not, the insert is complete; otherwise,\na new segment is created with the new key and the parent nodes are updated accordingly. As\nshown in Figure 1(b), for an Arbitrary Insert , PGM maintains multiple indexes of different sizes\nsimultaneously and adopts ideas from the LSM-tree to merge many small indexes into a larger one.\n2.2 Learned Indexes Built Top-down\nALEX [7]. Figure 1(c) is an instance of an ALEX index as described in the original paper [ 7]. Alex\nhas two node types â€“ inner nodes and data nodes; both contain an array and a linear model to\npredict the positions in the associated array. The array in the inner node is a pointer array that\nstores the child pointers. The array in the data node is a gapped array that stores key-payload\npairs. Empty slots and key-payload pairs are interleaved, which reduces insertion cost overheads\nby requiring fewer shift operations to find the first available slot. A bitmap is used to identify an\nempty slot more easily.\nTo support lookup operations, ALEX traverses from the root to a data node using model-based\nsearch. When using model-based insertions, the predicted position of the inner node does not\nrequire any additional search process. When arriving at a leaf node, ALEX first calls the model to\npredict the location and then performs an exponential search to find the final position if needed.\nTo support insert operations, ALEX first uses a search process to find the slot where the key\nwould be located. If the slot is already occupied, ALEX shifts items to obtain an empty slot for the\nnew key. An SMO (Structural Modification Operation), which determines how to update the index\nstructure, is triggered when a node is full. ALEX uses four mechanisms for an SMO, and provides a\ncost model for updating the tree structure.\nLIPP [30]. LIPP has a single node type as shown in Figure 1(d). Each node in LIPP has a data array,\na bit array, and a linear model. Each element in the data array can be one of the three types, DATA ,\nNULL , and NODE . The bit array identifies the element type. A linear model predicts which slot\nto be accessed during a lookup. LIPP first adopts the Fastest Minimum Conflict Degree (FMCD)\nalgorithm to obtain a linear model of a node with the smallest â€œconflict degreeâ€ (the maximum\nnumber of keys being inserted into the same slot). Then, LIPP inserts the key set using the resulting\nmodel in a single node. If only one key is inserted into a slot, this slot is labeled as DATA and stores\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to Design Choices 139:5\na key-payload pair. If multiple keys are located in one slot, the slot is marked as NODE which stores\na pointer to a child node. LIPP builds a new child node for any conflicting keys using the same\nprocess.\nTo support lookup operations, LIPP uses a linear model in each node to predict positions. If the\nslot is NODE , it accesses its children. If the slot is DATA , it checks if the key in this slot is the same\nas the lookup key â€“ if true, it returns the payload; otherwise, it returns null. If the slot is NULL , it\nreturns null.\nTo support insert operations, LIPP first performs a search to find which slot should hold the\nnew key. If the slot is NULL , LIPP inserts the new key into that slot. If the slot is DATA , LIPP will\ncreate a new node for the inserted key and the key in that slot, mark the slot as a NODE , and store\na pointer of the new node.\n2.3 Other Updatable Learned Indexes\nModel B+-tree [ 17] and RUSLI [ 20] are two recently proposed updatable learned indexes. A Model\nB+-Tree builds a model for each B+-tree node to predict which child node to access, and uses an\nupdate process similar to a B+-tree. If the predicted leaf node is not the target node based on a\nprediction error, a Model B+-tree will fetch more nodes than a B+-tree to locate the target leaf node,\nand which can be a significant overhead in certain cases. Therefore, we omit this index from our\nstudy. RUSLI extends RadixSpline [ 11] to support insertion by adding an overflow array. However,\nit has a very restrictive assumption â€“ insertion keys must be drawn from a uniform distribution.\nSince such an assumption is rarely true in practice, we have not included this approach in our\nstudy.\nStudies [ 4,6] combine learned indexes with a log-structured merge (LSM) tree data structure [ 22].\nA learned model is constructed for each SSTable (Sorted Strings Table), which is immutable after\nbeing created. Modifications (insert, update, delete) are supported in an LSM framework, and\nmodels are rebuilt during periodic compaction processes. Since they are implemented in a real\nproduction system, there are optimizations that we cannot reliably reproduce. PGM uses a similar\nidea to support insert (compaction), which we can test extensively. Thus, we exclude them from\nour study.\nXIndex [ 27] and FINEdex [ 15] add concurrency support to the learned indexes. Wu et al . [31]\npropose a method based on the idea of Normalizing Flows [ 25] to transform the data distribution\ninto an easier one to learn, and the proposed index structure is an extension of LIPP. CARMI [ 32] is\na cache-aware learned index for main memory. Several recent studies have also adopted the idea of\nlearned indexing on string data, spatial data, and multiple dimensional data [8, 16, 21, 24, 26, 28].\n3 COMPARISON\nAs summarized in Table 1, different indexes introduce different design ideas based on one or more\nof the following aspects:\nâ€¢Searching an Inner Node. ALEX and LIPP only use a model to predict the child and can be accessed\nin constant time. In contrast, B+-tree, PGM, and FITing-tree require a search of at least ğ‘‚(logğ‘š)\ntime, where ğ‘šrefers to the number of items in the node (B+-tree and FITing-tree) or an error\nbound (PGM).\nâ€¢Search on Leaf Node. Except for LIPP, all of the indexing methods presented in Table 1 require\na search stage (binary or exponential search) to find the exact position of a target key. The\ncomplexity is ğ‘‚(logğ‘š), whereğ‘šis the item count (in ALEX and B+-tree) or the error bound (in\nPGM and FITing-tree). Therefore, LIPP has the lowest cost to find a leaf node ( ğ‘‚(1)vs.ğ‘‚(logğ‘š)).\nâ€¢Data Partitioning. A data partitioning algorithm determines how many items are indexed in the\ninner nodes of the indexes. A smaller indexed item count produces lower tree heights and, in\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\n139:6 Hai Lan et al.\nturn, faster search time. ALEX and LIPP partition the data into nodes using a learned model,\nnamely Model-based Insert , which finds the exact position of a target key with no additional\nsearch.\nâ€¢Insertion. (1) All indexes first use a search to find the position to insert a new key. Thus, insertions\nbenefit from efficient search. (2) A FITing-tree ( Delta-Insert Strategy ) adds a â€œbufferâ€ to hold\nnew keys. B+-tree, ALEX, and LIPP nodes include extra space when creating nodes to store new\nitems. In contrast, a B+-tree is a dense array, while ALEX and LIPP use gapped arrays. (3) If the\ntarget position of the new key is empty in a gapped array, ALEX and LIPP insert the new key in\nthat position and finish the insertion process. However, a shift operation is always required in a\nB+-tree. If the predicted position is occupied by another key, ALEX shifts items to find an empty\nslot, and the gapped array can reduce the number of required shift operations. In contrast, LIPP\ncreates a new node to hold the new key and any keys already occupying the predicted position\nto reduce future conflicts.\nâ€¢Structural Modification Operation (SMO). When a â€œbufferâ€ or node is full, all of the indexing\nmethods must update the inner nodes in the tree structure. When updating leaf nodes, ALEX,\nLIPP, and a FITing-tree first fetch all the items and reinsert them into new nodes. When the node\nsize is larger than that of a B+-tree, SMO can incur a much higher latency than a B+-tree.\n4 LEARNED INDEXES ON DISK\nWe now show how to extend learned indexes to an on-disk scenario. Specifically, we use ALEX as\na concrete example presented, followed by other indexes. We use ALEX as our example for the\nfollowing reasons: (1) ALEX is the most difficult index to implement when all operations must be\non-disk, due to the SMO requirement; (2) ALEX is a representative example which can be used to\ndemonstrate common drawbacks in all existing updatable learned indexes when being ported to\nsupport on-disk operations.\n4.1 Extending ALEX On Disk\nWe now discuss major extensions needed to implement ALEX to an on-disk configuration.\nLayout on Disk. Figure 2 shows how to store the indexing data structure on-disk. All of the nodes\nin ALEX are stored contiguously. For each node, a model is stored, as well as utility structures\n(bitmaps, etc). In the original paper [ 7], the pointer array of one inner node stores the child pointers\n(each is an address to a memory position). When on-disk, we still need eight bytes to store child\nnode addresses on-disk â€“ 4bytes for the block number and 4bytes for the offset in the block. Since\nnode sizes are variable, a node in ALEX may cross multiple blocks , especially data nodes (see\nN5). Multiple nodes can also be stored in one block (see N6andN3).\nThere are two different layout choices for ALEX: Layout#1 in Figure 2(a) stores inner nodes and\ndata nodes in the same file; Layout#2 in Figure 2(b) creates one file for each type. Due to the small\nsize of inner nodes in ALEX, one block in an Inner Node File can hold more than one node. Thus,\nwe can traverse multiple levels using one block. For example, if we need to access N5, we need to\nfetch one block for each inner node ( N1andN3) inLayout#1 , but we only need to fetch one block\ninLayout#2 . We implement both layouts for ALEX and test them using a lookup-only workload in\nSection 5. Layout#2 has a 0.5%-30% performance improvement compared to Layout#1 . Thus, we\nprefer Layout#2 in our implementation.\nSince the root node in ALEX can be changed as a consequence of an insert operation, the first\nblock of the index is set as the meta block , which records the address of the root node. Additionally, a\nconstraint is enforced such that the data in one node must be stored in an adjacent space. Otherwise,\nwe need a mechanism to record the mapping between the blocks and nodes.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to Design Choices 139:7\nInner NodeData NodeN2N5N6N4N1N3\nBlock 1Block 2Block 3Block 4Meta BlockBlock 0Layout#1Data Node fileInner Node fileBlock 0Block 1Block 2Block 3N5â€™s ModelPred. PosEmpty SlotLayout#2Meta BlockBlock 0Block 1(b)\n(c)(a)\nFig. 2. The On-Disk ALEX Layout\nQuery Processing on Disk. When handling a search or an insert operation, first the block\ndetermined by the node model is fetched, and then the position in a node array is computed. Next,\nthe block is accessed to obtain the on-disk child addresses. When the insert operation requires an\nSMO, new space is allocated for the new node and old nodes are marked as invalid.\nWhen performing a scan operation (a range query), ALEX will first locate the smallest key in the\nsearch range and then scan forwards. A bitmap is used to skip empty slots. Since the size of the\ndata node can be as large as 16MB, the bitmap for one data node can cover at most 32blocks if one\nblock is 4KB, and incurs additional I/O costs. Instead of loading all blocks for a bitmap into main\nmemory, in our implementation, one block is loaded into main memory and scanned first. If the\nend of the scan range is found, we do not need to fetch any more blocks related to the bitmap.\nALEX also records several statistics, such as the number of shift and lookup-only queries. Hence,\na write cost is incurred even for read-only queries. In our implementation, these records are not\nmaintained for read-only queries.\nTable 2. I/O Costs Analysis of the Studied Indexes on Disk.\nB+-Tree ALEX FITing-tree LIPP PGM\nLookup logğµğ‘ logğ‘+log(ğ‘€/ğµ)+1 logğµğ‘ƒ+2ğœ–/ğµ 2 logğ‘ log(ğ‘/ğµ)\nScan logğµğ‘+ğ‘§/ğµ logğ‘+log(ğ‘€/ğµ)+ğ‘§/ğµ+3 logğµğ‘ƒ+2ğœ–/ğµ+ğ‘§/ğµ 2 logğ‘+ğ‘§ log(ğ‘/ğµ)+ğ‘§/ğµ\nInsert 2 logğµğ‘(1+2ğ‘€/ğµ)logğ‘+1+log(ğ‘€/ğµ)2 logğµğ‘ƒ+1+2ğ‘€/ğµ(2+2ğ‘/ğµ)logğ‘ log(ğ‘/ğµ)\n1ğ‘is the total item count, ğµis the maximum item count in one block, ğ‘€is the maximum item count in one data node (segment) of ALEX\n(FITing-tree), ğ‘ƒis the segment number in FITing-tree and PGM, ğ‘§is the item count in a scan (range query), ğœ–is the predefined error\nbound in FITing-tree and PGM.\n4.2 Extending Other Learned Indexes On Disk\nFor a FITing-tree, the Delta Insert Strategy is implemented and the following optimizations are\nincluded:\nâ€¢A greedy segmentation algorithm is replaced by a more efficient streaming algorithm which was\noriginally used in PGM [23].\nâ€¢The original FITing-tree algorithm does not allow a key insertion if the key is smaller than the\ncurrent smallest key. An extra buffer (one block) is introduced to hold keys such as this, and the\nphysical address is recorded in the meta node. When this extra buffer is full, a segmentation\nalgorithm is executed to partition the data and insert the new segments generated into the index.\nâ€¢To support scan operations, additional metadata is added to the beginning of each segment to\nrecord the position of the left and right siblings, as well as how many items each of them contains.\nThis is similar to the links between leaf nodes in a B+-tree.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\n139:8 Hai Lan et al.\nWe have also implemented an updatable, on-disk version of PGM. LIPP is similar to ALEX in\nthat it is also an unbalanced tree structure with variable node sizes, and the on-disk layout is the\nsame as ALEX â€“ with the exception of the node bitmaps, which are replaced with a slot flag to\nidentify the type. This removes the overhead of fetching the bitmap from the disk.\n4.3 On-Disk I/O Cost Analysis\nTable 2 shows the worst-case I/O cost of each index studied in an on-disk configuration.\nALEX . The tree height of ALEX is logğ‘â€“ the maximum fetched block count is logğ‘when\ntraversing to one data node. Because of the larger node sizes, there is a chance that the model and\nthe slot required to access the node are not in the same block. So, there is an extra block retrieval\ncostwhen a data node is searched. The complexity of exponential search is log(ğ‘€/ğµ). Thus, the\ntotal cost of lookup is logğ‘+log(ğ‘€/ğµ)+1. ALEX uses the same process as a B+-tree to perform\nscan operations, but there are two extra blocks which store the bitmap for the node in ALEX. Thus,\nthe complexity is logğ‘+log(ğ‘€/ğµ)+ğ‘§/ğµ+3. When performing an insert operation, ALEX first\ndoes a lookup to find a slot to insert a new key ( logğ‘+log(ğ‘€/ğµ)+1). If the node is full, an SMO\nis required. ALEX reads all items in a node and constructs new nodes, which can propagate all the\nway to the root node ( (2ğ‘€/ğµ)logğ‘). Thus, the total cost is (1+2ğ‘€/ğµ)logğ‘+1+log(ğ‘€/ğµ).\nFITing-tree . A FITing-tree requires logğµğ‘ƒfetches from the root node to find the target segment,\nsince the inner nodes of the FITing-tree are the same as a B+-tree. A binary search is invoked\non the range of 2ğœ–items, which accesses 2ğœ–/ğµblocks in the worst case. The analysis of on-disk\nscans is the same as a B+-tree. To perform insertions, the Delta Insert Strategy is implemented,\nwhich introduces a sorted buffer to hold new keys. So, the cost is logğµğ‘ƒ+1. If the buffer is full, the\nFITing-tree reads all items in the segment and buffer, and resegments them with a cost of 2ğ‘€/ğµ.\nOnce the new segments are added, the B+-tree component is updated. In the worst case, the cost is\nlogğµğ‘ƒas all levels in the B+-tree may need to be updated.\nLIPP . The height of a LIPP tree is logğ‘. Similar to ALEX, the larger node sizes of the upper\nnodes can result in a higher chance that the model and the slot are stored in different blocks. Thus,\nthe lookup cost is 2logğ‘. There is only one node type in LIPP, but different slot types. In the worst\ncase, the item fetched is in multiple nodes and blocks. Thus, the total scan cost is 2logğ‘+ğ‘§/2.\nSimilar to ALEX and FITing-tree, an insertion may lead to rebuilding a subtree with at most a\nheight of logğ‘. At each level, at most ğ‘items must be read and written. Thus, the total cost is\n(2+2ğ‘/ğµ)logğ‘.\nPGM . The height of a PGM tree is logğ‘and the error bound in PGM can be smaller than half\nof the maximum item count in a block. Thus, the complexity for search and scan is logğ‘and\nlogğ‘+ğ‘§/ğµ, respectively. For insert, we can only provide amortised time. In the worst case, PGM\nwould merge all existing indexes into a new one as shown in Figure 1(b).\nTable 3. Dataset Profiling under Error Bound and Conflict Degree (block size = 4KB)\nError YCSB [7] FB [18] OSM [18] Covid [29] History [29] Genome [29] Libio [29] Planet [29] Stack [29] Wise [29] OSM(800M) [18]\n16 70,135 2,120,485 1,351,170 231,852 303,737 3,153,966 291,257 1,416,012 54,073 246,463 6,175,387\n64 6,952 523,006 326,932 42,695 40,817 295,604 77,401 268,247 6,956 27,553 1,375,143\n256 23 119,891 81,392 8,630 8,464 23,228 19,333 55,061 950 4,713 328,623\n1024 1 18,495 20,925 1,890 2,029 4,975 3,616 12,001 196 1,184 81,577\nB+-tree 980,393 980,393 980,393 980,393 980,393 980,393 980,393 980,393 980,393 980,393 3,921,569\nConflict Degree 4 114 4,106 27 9 585 2 22 1 10 10,107\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to Design Choices 139:9\n5 EXPERIMENTAL SETUP\n5.1 Datasets & Profiling\nDatasets. We use eleven datasets, which are widely used in existing studies [ 7,18,29,30]. The first\nten have 200M keys and each key is an uint_64 integer. We use the payload as the key plus 1. The\ndataset size is 2.98GB in each of the first ten datasets. The last one has 800M keys with 11.92GB\nsize used for scalability experiments.\nProfiling. As shown in Section 2, all learned indexes use a linear function as the model. If a dataset\nis difficult to model with a linear function, the performance of learned indexes can be degraded.\nâ€¢For FITing-tree, PGM, and ALEX, we use the segmentation algorithm from PGM and several\nerror bound settings to show how hard it can be to model certain datasets using a linear function.\nA dataset with more segments is harder to model under the same error bound. The results are shown\nin Table 3.\nâ€¢The performance of LIPP is related to the conflict degree of a dataset [ 30], which is shown in the\nlast row of Table 3. A dataset with a larger conflict degree lowers performance for LIPP.\nWe also report the leaf node number in a B+-tree when the block size is 4KB. For ALEX, PGM, and\na Fitting-tree, we set the default error bound to 64â€“ with the worst performing dataset being FB\nfor this error bound. Under this metric, OSM is the most difficult one. While on both settings, YCSB\nis the easiest one.\nDue to space limit, we report the performance using three representative datasets â€“ FB,YCSB ,\nandOSM . For the remaining datasets, readers can refer to a technical report [3].\n5.2 Workloads\nWe test all the indexes on six different workload types: (1) Lookup-Only workload, which performs\nlookups on the indexes built by bulkloading all keys in each dataset. We randomly sample 200,000\nlookup keys from the existing keys. (2) Scan-Only workload, which performs scan operations on\nthe same indexes as the Lookup-Only workload. A scan operation is implemented with a lookup\noperation on the start key and a scan of the next 99elements. The start keys are generated in\nthe same way as the Lookup-Only workload does. (3) Write-Only workload, which inserts 10M\nkey-payload pairs in the indexes after bulkloading 10M random keys. (4) Read-Heavy workload,\nperforms 90% lookups and 10% inserts on indexes after bulkloading 10M random keys, i.e., we\nperform 2inserts and 18lookups, then repeat the process. (5) Write-Heavy workload, which\nperforms 90%inserts and 10%lookups on indexes after bulkloading 10M random keys. We perform\n18 inserts and 2 lookups, then repeat the process. (6) Balanced workload, which performs 50%\ninserts and 50%lookups on the indexes after bulkloading 10M random keys. We perform 10inserts\nand10lookups, then repeat the process.\nIn each of the Mixed workloads (Read-Heavy, Write-Heavy, Balanced), the total number of\noperations is 10M and the search keys for the lookup in the Mixed workloads are evenly distributed.\n5.3 Other Implementation Details\nCode & Environment. We implement all the indexes in C++ and make them available at [ 1]. We\nconduct the experiments on a HDD using Red Hat Enterprise Server 7.9 on an Intel Xeon CPU\nE5-2690 v3 @ 2.60GHz with 256GB memory and a 1TB HDD, and the experiments on SSD using\nUbuntu 20.04 on AMD EPYC 7662 with 500GB memory and four 8TB SSD.\nMetrics. (1) The storage size of the whole index used on disk; (2) average throughput for each\nworkload type and tail latency for the lookup-only workload and write-only workload; (3) average\nblock count per search query (lookup-only and scan-only workloads).\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\n139:10 Hai Lan et al.\nParameters. For a FITing-tree, we set the buffer size of each segment to 256and the error bound\nto64by default. Since the optimal error bound setting for a FITing-tree can vary across datasets\nand workloads, we test several different error bounds and find that the FITing-tree achieves good\nperformance in the majority of test cases when the error bound is set to 64. PGM, ALEX and\nLIPP use the default parameter settings from the original papers. In Section 6.4, we perform an\nexperiment that explores the impact of block size. For other experiments, we fix this value to 4KB.\n6 EXPERIMENTAL EVALUATION\nOur comprehensive evaluations in Section 6.1-6.6 aim to answer:\nQ1:How good are learned indexes when compared to a B+-tree on an HDD and an SSD, if the\nentire index structure is disk-resident?\nQ2:Is there any benefit to storing inner nodes of the learned indexes in main memory?\nQ3:How much storage do learned indexes require?\nQ4:What impact do different block sizes have on performance?\nQ5:Do learned indexes have robust performance when disk-resident?\nQ6:What impact does the buffer have?\n6.1 Evaluation When the Entire Index is Disk-Resident\nSetting. In this set of experiments, we assume that the meta block, which records the root node\naddress and other utility information, is stored in main memory when in use, while the remaining\nindex structure remains disk-resident, and stored within blocks of size of 4KB. There is no buffer\nmanagement, i.e., for each request, we must read/write the required blocks from disk.\nFig. 3. Search Performance Comparison on HDD and SSD: the entire index is disk-resident using blocks of\nsize4KB.\nTable 4. An Analysis of Fetched Block Counts for Lookup-Only and Scan-Only Workloads. For LIPP, its inner\nnode count for the Scan-Only workload is provided in the brackets.\nFB OSM YCSB\nFITing-tree PGM ALEX LIPP FITing-tree PGM ALEX LIPP FITing-tree PGM ALEX LIPP\nInner Node Count 3 5 6.7 1.8 (18.8) 3 5 2.7 2.3 (23.1) 2 3 3 1.3 (16.7)\nInner Block Count 3 3.9 6.5 - 3 3.7 2.6 - 2 2 2.2 -\nLeaf Block Count (Lookup) 1.2 1.3 2.6 3.0 1.2 1.2 2.2 3.8 1.2 1.3 2 2.3\nLeaf Block Count (Scan) 2 1.7 4.1 24.0 1.8 1.5 3.8 30.0 1.6 1.7 3.6 19.7\n6.1.1 Lookup-Only Workload. Figure 3(a) and Figure 3(b) present the throughput of an HDD and\nan SSD, respectively. Figure 4(a) reports the average number of fetched blocks per query. We first\npresent the observations ( O) and then provide a detailed analysis.\nO1: When the entire index is disk-resident, the throughput of the Lookup-Only workload\nis determined by the number of fetched blocks from disk. The increase in the number of\nfetched blocks usually degrades performance, since fetching data from disk tends to dominate the\nexecution time. For example, the B+-tree and FITing-tree show similar performance on the FBand\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to Design Choices 139:11\nFig. 4. Average Fetched Block Count for Search on HDD.\nOSM datasets while the FITing-tree has a much higher throughput on YCSB (26.2%improvement\nfor an HDD).\nO2: Most existing learned indexes are competitive or outperforming B+-tree on the Lookup-\nOnly workload. LIPP outperforms all other indexes on our test datasets. When compared to the\nalternatives, the node fanout of LIPP is much larger, which contributes to its lower tree height and\nleads to fewer fetched blocks. However, a large fanout leads to a larger index size, which implies\nlonger index construction time, as shown in Figure 7(a) and 7(b). ALEX performs the worst on FB,\nwhich is attributed to the highest number of fetched blocks.\nO3: On the Lookup-Only workload, B+-tree exhibits stable performance, while the perfor-\nmance of the learned indexes fluctuates. This is because learned index throughput depends on\nthe difficulty of modeling the data distribution. For LIPP, OSM is the most difficult dataset, while\nFBmakes the most difficult dataset for ALEX, FITing-tree, and PGM. Since the B+-Tree does not\nrequire any predictions from a linear model, its performance remains consistent across all the\ndatasets.\nAnalysis of Fetched Blocks for the Lookup-Only Workload. We breakdown the fetched block\ncounts for each index into two components: (a) the inner nodes, and (b) the leaf nodes. The results\nare presented in Table 4. Since there is only one node type in LIPP, we report the average total\nnode count for LIPP. On the three datasets tested, the B+-tree has 4levels in all cases, with 3inner\nnode blocks and 1block for the leaf node.\nBased on Table 4, we observe that: (1) For the FITing-tree, accessing one inner node will fetch one\nblock; in contrast, in the case of PGM and ALEX, more than one inner node can be stored within a\nblock due to the small node sizes at the upper tree levels (hence inner block count is sometimes\nsmaller than the node count for those). (2) Compared to the B+-tree, the FITing-tree and PGM have\na smaller search range, 256vs.128. However, the average fetched block counts in the FITing-tree\nand PGM are slightly larger than the B+-tree (by 1block). This is because the FITing-tree and\nPGM cannot guarantee the entire search range to be stored in one block. (3) Compared to the other\nindexes, ALEX accesses more leaf node blocks. Due to the large node size of the leaf nodes, there\nis a greater chance that the model stored in the node header resides in a different block than the\npredicted target position. Thus, ALEX reads at least 2blocks. Moreover, on FBandOSM , the fetched\nblock count is larger than 2. Similar to the binary search in the FITing-tree and PGM, exponential\nsearch in ALEX can occur across multiple blocks. (4) Although LIPP accesses fewer nodes than the\nother indexes, on average, it requires more than 1.65fetched blocks per level. The large node size\nin LIPP can cause the model and the predicted position to reside in different blocks.\n6.1.2 Scan-Only Workload. From Figure 3(c)-(d), we observe that:\nO4: For the Scan-Only workload, regardless of the dataset hardness, B+-tree outperforms\nothers across all datasets. Just as observed in O1, the throughput of the learned indexes is highly\ndependent on the number of fetched blocks.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\n139:12 Hai Lan et al.\nFig. 5. Write Performance Comparison on HDD and SSD: the entire index is disk-resident using blocks of\nsize4KB.\nTable 5. An Average Fetched Block Count for Search on HDD under the Hybrid Design. The first (second)\nnumber is the block count for the Lookup-Only (Scan-Only) workload.\nFITing-Tree PGM ALEX LIPP B+-tree\nFB 3.25/3.74 3.25/3.74 4.02/4.51 3.15/3.64 4.0/4.49\nOSM 4.25/4.74 4.17/4.66 4.77/5.26 4.5/5.0 4.0/4.49\nYCSB 3.25/3.74 3.25/3.74 4.0/4.49 3.01/3.5 4.0/4.49\nO5: ALEX and LIPP exhibit the worst performance in the Scan-Only workload. This is\nattributed to the fact that ALEX and LIPP fetch many more blocks compared to the others.\nAnalysis of Fetched Blocks for the Scan-Only Workload. To support scan queries, all the in-\ndexes first locate the position of the start key and then scan forward until they reach the final key.\nWe set the start keys for the Scan-Only workload with the keys from the Lookup-Only workload.\nThus, the inner node counts and inner block counts are identical to the Lookup-Only workload for\nFITing tree, PGM, and ALEX as presented in Table 4. In the last row, we report the fetched block\ncounts at the leaf node for scan queries. For LIPP, we also report the total fetched node counts and\nblock counts as presented in the brackets.\nFrom Table 4, we observe that: (1) LIPP fetches the highest number of nodes. The three entry\ntypes (i.e., NODE ,DATA ,NULL ) are interleaved in one LIPP node. If we locate a NODE entry, we\nmust fetch a new node from disk and scan it. If we visit all elements for a node and its children,\nwe traverse back to the parent node and continue the scan. There is a high chance that nodes are\nstored across multiple blocks, which leads to the highest fetched block count. Furthermore, the\nelements in a node can be stored across multiple blocks, which also results in an increased fetched\nblock count. (2) To skip the empty slots in the data nodes, ALEX introduces a bit array to indicate if\na slot is empty. The bit array may require additional blocks to be fetched. In contrast, B+-tree, PGM,\nand FITing-tree, store the key-payload pairs contiguously, and the sibling segments (leaf nodes) are\nlinked. Therefore, there is negligible or no overhead in fetching the utility data or traversing nodes.\nHow Good a Hybrid Index Structure Design Is. Storing key-payload pairs continuously bene-\nfits scan in B+-tree, FITing-tree, and PGM. Thus, an emerging idea is to maintain the leaf nodes\nin a continuous manner as in a B+-tree, and adopt learned indexes as the inner part to index the\nmaximum keys in leaf nodes. Following this idea, we present the average fetched block count for\nthe Lookup-Only and Scan-Only workloads in Table 5 and make several observations: (1) On FB\nandYCSB , all hybrid designs achieve similar or better performance than a B+-tree. When we only\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to Design Choices 139:13\nFig. 6. Write Performance Breakdown.\nindex the maximum keys in the leaf nodes, OSM is a harder dataset than FBandYCSB . The segment\ncounts under an error bound of 64 are 107,1,1457 inFB,YCSB , and OSM , respectively. (2) With a\nB+-tree-styled leaf node, ALEX and LIPP perform better on scan compared to the original design.\n(3) LIPP performs the best on YSCB andFB. However, compared to the original LIPP (in Table 4),\nthe fetched block count in the Lookup-Only workload is a little larger. With the maximum keys in\neach leaf node in the LIPP part. We have to scan forward to find the next DATA slot if meeting a\nNULL slot.\n6.1.3 Write-Only Workload. The results presented in Figure 5(a) and Figure 5(e) lead to the following\nobservations:\nO6: For the Write-Only workload, the relative ranking of all indexes is consistent across\nall datasets, with PGM significantly outperforming other methods. PGM supports arbitrary\ninsertions using an LSM style tree, which uses a small sorted array (of fixed size) to cache new\ninsertion requests. When the sorted array is full, PGM merges this data into the static PGM index\n(increasing its size), as shown in Figure 1(b). Therefore, most insertion requests need to read and\nwrite only a smaller number of blocks.\nO7: Other than PGM, B+-tree significantly outperforms other learned indexes on the\nWrite-Only workload. All of the indexes first find the slot to store a new key-payload pair.\nAlthough learned indexes have shown similar or better performance than B+-tree on the Lookup-\nOnly workload, the overhead for insertion outweighs the benefit acquired in the search process.\nThe performance of ALEX and LIPP is severely impacted in this case. In order to support SMO,\nLIPP maintains statistics for each node. Thus, for each insert, LIPP will update all of the nodes in\nthe path to the inserted node. ALEX needs to maintain such statistics for leaf nodes as well.\nO8: B+-tree and PGM exhibit consistently good performance across all tested datasets,\nwhile the performance of other learned indexes fluctuates significantly. Overall, LIPP\nperforms the worst on the harder datasets. This is similar to ALEX that exhibits lower throughput\nin harder datasets. Interestingly, FITing-tree performs the worst in the easier datasets.\nPerformance Analysis for the Write-Only Workload. We break the insert process into four\nsteps: (a) initial search step, to find the insert position, (b) insertion step, to do the insertion, (c)\nSMO step, to do structural modification, and (d) maintenance step, to update statistics related to the\nSMO. We report the average latency of each step in Figure 6. Unlike other indexes, PGM initiates\nthe search process to find the location of a new key-payload pair in a sorted array of size 585(3\nblocks). PGM only needs to fetch one or two blocks to find the position. FITing-tree has similar\nsearch time to B+-tree, but exhibits larger insertion time, since FITing-tree needs to write an extra\nblock to update the current item count for a segment.\nALEX has the highest latency for the insertion step due to several reasons: (1) It reads from\nthe extra blocks for the bitmap to check if the predicted position is empty, and then updates the\nbitmap after the new insertion; (2) If a predicted position is not empty, a shift operation is triggered\nto create an empty slot for the key-payload pair, which may move the items across blocks; (3)\nAlthough the bitmap is used to determine if a slot is empty, ALEX will still overwrite empty slots\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\n139:14 Hai Lan et al.\nFig. 7. Bulkload Performance on HDD.\nuntil reaching the previous element to avoid accessing the bitmap during the lookup query. In (2)\nand (3), ALEX also needs to read the bitmap to find any related empty slot(s).\nThe FITing-tree in YCSB has a larger SMO overhead compared to FBandOSM .YCSB can be\nmodeled using fewer models and has more items in each segment. When triggering an SMO\noperation, the FITing-tree will write more blocks for YCSB . ALEX and LIPP have a moderate SMO\noverhead for FBandOSM . For ALEX, a harder dataset induces more SMO operations. For example,\nALEX requires 57,330,236, and 212,917SMO operations for YCSB ,FB, and OSM , respectively.\nLIPP has two types of SMO, creating a new node to eliminate the conflict, and adjusting the tree\nstructure. In our testing, millions of SMOs are the second cases, i.e., an SMO for every three\ninsertions. Interestingly, LIPP has a larger SMO overhead on YCSB , an easy dataset. This is because\nmany keys are conflicted locally.\nLIPP also has a larger maintenance overhead compared to ALEX. This is because LIPP will update\nall nodes in the access path, while ALEX only needs to update one data node for the insertion.\n6.1.4 Read-Write Workload. From Figure 5(b) - (d) and Figure 5(f) - (h), we derive the following\nobservations:\nO9: In most cases (6 out of 9), the B+-tree outperforms learned indexes. Since learned\nindexes incur a higher number of block writes, when disk-resident, learned indexes\nhave subpar performance compared to the B+-tree (which ranks first or second in all\nthe experiments with this workload). The second case rarely happened in our testing\nwhile there are 4.28ğ‘€,3.36ğ‘€, and 2.81Excluding PGM, the benefit of the lookup gained by the\nlearned indexes is eclipsed by the overhead created by the insertion operations. For example, LIPP\noutperforms others in Lookup-Only workloads, yet it exhibits worse performance than the B+-tree\nforFBandOSM , even in the Read-Heavy workloads.\nO10: As the read ratio increases, the throughput of PGM degrades severely, unlike the\nalternatives where it increases. As discussed in O6, PGM performs well for insertion opera-\ntions. However, due to the LSM tree layout, PGM must maintain several static PGM indexes (see\nFigure 1(b)). To support a read query, PGM must access all static PGM indexes sequentially until\nfinding the search key, or determining that the key does not exist. Each static index is stored as a\nseparate file â€“ leading to a higher I/O cost.\n6.1.5 Bulkload. From Figure 7(a) and Figure 7(b), we observe that:\nO11: Learned indexes usually require more storage space than B+-tree, and all of them\nrequire more time to build an index on disk. On all datasets, PGM has the smallest index size,\nwhile LIPP has the largest index size. PGM supports insertions using LSM. Each static PGM index\ndoes not need to allocate extra space to hold the new insertions. On the contrary, the other learned\nindexes need to allocate extra space before insertions occur. FITing-tree allocates a fixed size buffer\nfor each segment. ALEX and LIPP use a gapped array. In LIPP, if the count of items ( item_count )\ninserted into a single node is in the range [100,000,1,000,000), LIPP allocates 2âˆ—item_count slots.\nIf the count <100,000,5âˆ—item_count slots are allocated. LIPP has the largest empty slot ratio\ncompared to others.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to Design Choices 139:15\nO12: The index sizes of FITing-tree and LIPP highly depend on the distribution of the\ndataset indexed. For harder datasets, more segments are generated by the FITing-tree, and more\nbuffers are preallocated. A potential way to optimize the storage size of the FITing-tree is to allow\nlazy buffer allocation, where a buffer is only allocated for a segment when a new key-payload pair\nmust be inserted into it. If more than one item is inserted into a slot in LIPP, a new node is created.\nHarder datasets usually create more nodes. A high empty slot ratio (as discussed in O11) in new\nnodes increases the storage size.\n6.2 Evaluation When the Inner Nodes are Memory-Resident\nSetting. In this section, we investigate the impact of indexes when inner nodes are memory-resident.\nFor the B+-tree and FITing-tree, we use an STX B+-tree [ 2] for the inner nodes, which is also\nadopted by ALEX and LIPP as their B+-tree baseline. For PGM and ALEX, we use the original\nimplementation for the inner nodes and store the leaf nodes on disk. We exclude LIPP from this\nexperiment for two reasons: (1) There is only one node type in LIPP; and (2) The largest node in\nLIPP is the root node. Even if only the root node is stored in main memory, it requires more than 3\nGB of space for the FBdataset, while other indexes require no more than 40MB space. We refer to\nthis setting as the hybrid case in the rest of the paper.\nFig. 8. Search Performance Comparison on HDD and SSD: the inner nodes are memory-resident, leaves are\non disk.\nFig. 9. Write Performance Comparison on HDD and SSD: the inner nodes are memory-resident, leaves are\non disk.\n6.2.1 Lookup-Only Workload & Scan-Only Workload. When inner nodes are memory-resident,\nthe number of fetched blocks for each index is the same, as shown in the last two rows in Table 4.\nGiven Figure 8 and Table 4, we observe that:\nO13: FITing-tree and PGM are competitive with B+-tree. However, ALEX is not. Just as\nwhat we saw in the on-disk case, performance is determined by the number of blocks fetched.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\n139:16 Hai Lan et al.\nFig. 10. Storage Usage on Disk: the entire index is disk-resident using blocks of size 4KB.\nAny performance gains from faster lookups in main memory are eclipsed by on-disk reads, which\nremains the key bottleneck.\n6.2.2 Workloads With Write Queries. For workloads requiring write operations presented in Fig-\nure 9, we observe that:\nO14: Unlike the other indexes, utilizing main memory to store inner nodes does not help\nPGM. Other indexes first search for a slot to hold a new key-payload pair. Thus, the performance\ngains of lookup-only queries in the hybrid case can improve write performance. However, for PGM,\nthe write process is the same as that on disk, so only marginal performance gains can be observed.\nOne idea to alleviate the problem is to keep the sorted array of PGM in main memory, as is done\nfor the skip lists of LSM based systems.\nO15: B+-tree outperforms the others across all workloads in the Hybrid case. From Figure 6\nwe observe that the main overhead for ALEX and LIPP when they are disk-resident is for insertions,\nstructural modifications, and statistic updates. Therefore, putting the inner nodes in main memory\ndoes not help, since they still need to pay this cost. B+-trees have smaller insertion cost than PGM.\nAfter bringing inner nodes into main memory, B+-tree has better performance in the initial search\nstep, which helps it outperform PGM. Similarly, the decrease of search overhead in the FITing-tree\nmeans it has better performance than PGM for FBandOSM . However, due to large SMO overheads\nforYCSB , the FITing-tree is worse than PGM for YCSB .\n6.3 Storage Size Study\nUnlike the main memory setting, the memory allocated on disk cannot be reclaimed directly and\nwe do not consider how to reuse the invalid space here1. The result is reported in Figure 10. Other\nworkload types show a similar pattern to the Write-Only workload.\nO16: Other than FITing-tree for YCSB , the storage size ranking of the studied indexes is\nthe same as presented in the bulkload experiment. Although we use the same dataset sizes for\nFBandYCSB , FITing-tree has a larger storage size for YCSB , since YCSB can be modeled using fewer\nlinear models. That is, each segment covers more keys than in FB(more than 5,000key-payload\npairs, or a total of 19blocks). SMO operations allocate more blocks in the FITing-tree than in any\nother index. Interestingly, the FITing-tree has to write more blocks for YCSB , but still requires\nfewer than ALEX, and is competitive with LIPP for YCSB (see Figure 5). Given the performance\nbreakdown shown in Figure 6, the overhead required to update statistics and do insertion eclipses\nthe performance gains achieved by fewer SMO operations.\nPGM and B+-tree have much smaller storage sizes. When splitting a node of a B+-tree, a small\npart of the original node is retained, and the rest goes to a new node. In contrast, learned indexes\nuse learned models to predict positions, therefore we must store this as part of a node contiguously\non-disk . This makes it difficult to reuse reclaimed space, and hence higher fragmentation can be\n1One way to alleviate the issue is to reuse the invalid space by bookkeeping the address and size of invalid space. However,\nthis process incurs extra overhead to maintain the information for invalid space\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to Design Choices 139:17\nFig. 11. Fetched Block Count under Different Block Sizes.\nFig. 12. Tail Latency of the Lookup-Only Workload and Write-Only Workload on HDD.\nobserved. For PGM, when a smaller sized index has been merged, the corresponding index file can\nbe deleted from disk.\n6.4 Impact of Block Size\nIn Figure 11, we show the performance of the Lookup-Only workload when the block size is varied\non a HDD. We observe that:\nO17: The block size variation has different impacts on different indexes. The number of\nblocks fetched by LIPP does not change when the block size is varied. The position prediction in\nLIPP is accurate and requires no additional search operations. The physical address of a target\nelement can be derived from the start address in a node, and the relative position from the model\nprediction. For the other indexes, larger block sizes usually lead to fewer blocks fetched. When the\nblock size is 16KB, the height of B+-tree decreases for a larger fanout. This is also why FITing-treeâ€™s\nperformance changes on FB. For FITing-tree and PGM, the error bound =64, and the length of\nsearch range =128. There is a higher chance that elements in the search range will be in the same\nblock when the block size is large, which also reduces theblocks fetched in leaf nodes for ALEX\nusing an exponential search. In ALEX and PGM, larger block sizes increase the storage available to\nnodes in the upper levels, so we can traverse more nodes using one block.\n6.5 Tail Latency\nIn Figure 12, the tail latency ( 99-th percentile latency and standard deviation) for the Lookup-Only\nworkload and Write-Only workload are reported. We make the following observation:\nO18: Learned indexes usually have greater p 99latency than the B+-tree for Lookup-\nOnly and Write-Only workloads, and exhibit less stable performance. The 99-th latency\nperformance ranking is the same as observed in Figure 3(a) and Figure 5(a). Interestingly, for the\nLookup-Only Workload, PGM and ALEX have a larger standard deviation on FBandOSM . PGM\nuses a balanced tree structure, so it should have a similar query latency. We record the number of\nfetched blocks for each test query. Given the constraint that a binary search is conducted only on\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\n139:18 Hai Lan et al.\nFig. 13. An Average Fetched Block Count from Disk under Different Buffer Sizes on the Lookup-Only Workload.\nBuffer size indicates how many blocks can be cached.\nFig. 14. Comparison of ALL Workloads on YCSB andFB. The entire index is disk-resident and each indexâ€™s\nthroughput is normalized by the largest one in the corresponding workload (the higher is the better).\none block at a time, in the worst case, PGM may have to access two blocks alternately. As discussed\nin Section 6, we do not include buffer management by default; instead, we check whether the\nlast block fetched can be reused. ALEX is on the other hand an unbalanced tree structure. Thus,\ndifferent data regions may have different query latencies. If the insertion requires an SMO, it will\nhave a larger latency, leading to a larger standard deviation. That is why ALEX exhibits a larger\nstandard deviation for OSM , and FITing-tree has a greater variance for YCSB . LIPP frequently issues\nan SMO to create a new node (in every three insertions), and shows a low variance for the writes.\n6.6 Buffer Size Study\nWe vary the number of blocks that can be cached in the main memory and test on the Lookup-Only\nworkload. We adopt the LRU strategy. From Figure 13, we make several observations: (1) With zero\nor only a small buffer size, LIPP has the smallest fetched block count due to its lowest average tree\nheight. (2) With a larger buffer size, the other indexes will fetch fewer blocks from the disk. When\nthe buffer size is larger than 8, LIPP is outperformed by the other indexes. One reason is that LIPP\nhas a larger node size in the upper level, leading to a high probability of fetching blocks that are\nnot in the buffer. (3) PGM achieves the best performance with a large buffer size due to its small\nnon-leaf nodesâ€™ size.\n7 INSIGHTS AND DESIGN CHOICES ON DISK\n7.1 Shortcomings of Learned Indexes on Disk\nFigure 14 presents the overall performance of all the studied indexes on YCSB andFB.Except for\nLookup-Only Workloads, the B+-tree is either competitive or outperforms learned indexes on disk. We\nnow summarize commonly observed shortcomings ( S1 - S5 ).\nS1 â€“ Model-Slot Overhead. Before accessing a slot in a node, ALEX and LIPP need to fetch the\nlearned model in a node. The model is stored in the node header. The model and the searched slot\nmay be located in different blocks. Although learned indexes can have lower tree heights, the extra\nblocks needed to fetch the model can lead to more blocks being accessed overall compared to the\nB+-tree. This problem does not exist neither in the FITing-tree nor PGM, since they store the model\nin the parent node.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to Design Choices 139:19\nS2 â€“ Search Overhead. Even for very small search ranges where FITing-tree, PGM and ALEX\nroutinely outperform B+-tree in main memory, it is not the same case on disk, because no learning\napproach can guarantee that (even very few) searched items will be stored in the same block.\nWithout judicious buffer management developed for learned indexes, this problem may be severe in\nextreme cases. Moreover, PGM supports arbitrary insertions but the LSM structure must maintain\nmultiple files. When handling a query in mixed workloads, PGM may access multiple files, and\nhence exhibits much worse performance than others. LIPP does not have this problem since it tends\nto have accurate predictions in each step. However, since LIPP does not distinguish between the\ndata and inner nodes, traversal of additional nodes may still be required.\nS3 â€“ Utility Structure Overhead. This overhead comes from two parts. (1) For searches â€“ when\nperforming a scan operation (a range query), ALEX will first locate the smallest key of the search\nrange and then scan forwards. The bitmap used by ALEX can skip empty slots. However, due to\nthe size of the data nodes (up to 16MB), the bitmap for one data node can cover at most 32blocks,\nand incur additional I/O costs. (2) For writes â€“ both ALEX and LIPP maintain statistics based on\nhistorical workloads which are leveraged in SMO operations. These statistics are stored in the\nnode header. Thus, ALEX and LIPP must update the header of the node after each insertion. From\nFigure 6, we can see that the overhead for this operation is far from negligible when the index is\ndisk-resident.\nS4 â€“ SMO Overhead. As shown in Figure 6, ALEX, LIPP, and FITing-tree have large SMO overheads\nin certain cases. The SMO operation is required in order to boost performance of the later queries.\nFor example, this operation reduces the tree height in LIPP. On the other hand, this maintenance\noverhead can hurt the overall performance, especially for on-disk operations (see Figure 5 and\nFigure 9). The B+-tree thus outperforms all of them on nearly every workload involving a write\noperation.\nS5 â€“ Other Index-Specific Overheads. (1) Although the gapped array in ALEX helps reduce shift\noperations, supporting lookup queries without accessing a bitmap forces ALEX to overwrite the\npreceding empty slots until it reaches the previous element. With a large leaf node size, ALEX\nmay have to update more blocks than the B+-tree. (2) If the predicted slot is occupied by another\nkey-payload pair, ALEX will shift items to obtain an empty slot. This may move items across blocks,\nleading to extra block writes.\n7.2 Design Choices\nBased on the above observations, we propose the following best practices when designing disk-\nresident learned indexes:\nP1 â€“ Reduce the tree height. The tree height directly influences the number of fetched blocks.\nDesigning smaller-sized nodes may be an alternative choice. This implies that multiple nodes can\nbe stored in the same block. The latter will benefit search-only workloads. However, after updating\nthe nodes, some of them may need to be moved to another block. LIPP and the interpolation search\ntree [ 5,19] can reduce the tree height while they only benefit lookup-only queries and can increase\nthe storage cost. Moreover, as shown in Section 6.6, LIPP cannot utilize the buffer well due to its\nlarge node size at upper level. A potential solution worth investigating is to combine it with another\nindex structure.\nP2 â€“ Use a light-weight structural modification operation (SMO). Although existing learned\nindexes have the advantage of faster search, higher overheads stemming from SMO operations\nreduce the performance for the Write-Only and Mixed workloads. There are three aspects we\nshould consider: (1) reducing the frequency of index tree modifications; (2) reducing the size of the\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\n139:20 Hai Lan et al.\npartial trees that must be updated during each structure modification operation; (3) reducing the\nreliance on historical statistics in the SMO process or storing this information so that the process\nrequires far fewer block fetches or updates.\nP3 â€“ Lower overhead when fetching the next item. Although the design of gapped array\nboosts the performance of ALEX and LIPP for insert, it has an additional overheadâ€“more blocks\nare fetched when skipping empty slots on disk. Without the ability to differentiate between data\nnodes and inner nodes, LIPP cannot quickly locate the next item needed, and traverses additional\nnodes. When using learned indexes on disk, key-payload pairs should be stored contiguously, or\nalternatively, the nodes that store the key-payload pairs should have higher densities. As shown in\nSection 6.1.2, using a B+-tree styled leaf node may mitigate this issue.\nP4 â€“ Storage layout design. Larger node sizes in learned indexes reduce the height of the tree\nstructure in the main memory configuration. However, when we store the larger nodes on disk,\nthere is a much greater chance that two blocks must be fetched to locate the searched slot â€“ one\nfor fetching the model and one for fetching the address of the child node (or the data). If we cannot\nreduce the tree height by 2levels for each large node that crosses multiple blocks, the learned\nindexing on-disk becomes less attractive. A possible optimization is to store the model in the parent\nnode (similar to the idea adopted by PGM and FITing-tree), which will result in fetching at most\none block for each level traversed.\nP5 â€“ Co-design learned index when using buffer. Co-optimizing the size of the in-memory\nstructure (inner nodes) combined with an additional search cost over on-disk leaf nodes is a plausible\nstrategy. A B+-tree styled leaf node may be used, which usually results in fewer fetched blocks\ncompared to the original learned index, and is also efficient for scan-based queries, as shown in\nSection 6.1.1. When designing a learned index on disk in this manner, the performance difference\ndepends on which learned index is used in main memory. Based on a recent study [ 29], ALEX,\nLIPP, and ART [ 14] would be suitable choices. Another plausible strategy is to cache frequently\naccessed blocks. As shown in Figure 13, indexes with a smaller size at the upper level, e.g., PGM,\nALEX, B+-tree, and FITing-tree, are more suitable for larger buffer sizes.\n8 CONCLUSIONS\nWe raise a simple but important question: Can we build an updatable learned index to fully replace\nthe traditional disk-resident B+-tree? To answer that, we explore state-of-the-art updatable learned\nindexes (expanding their in-memory implementations), and provide a comprehensive experimental\nevaluation and analysis. While each of them has their own strengths, we find none of them are\ncompetitive with the disk-resident B+-tree. Upon our evaluation, we present four crucial design\nchoices. We believe our implementation and proposed design choices will be useful to researchers\nand practitioners in designing new and efficient disk-resident learned indexes.\nACKNOWLEDGMENTS\nThis research is supported in part by ARC DP220101434 and DE230100366.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\nUpdatable Learned Indexes Meet Disk-Resident DBMS - From Evaluations to Design Choices 139:21\nREFERENCES\n[1] [n. d.]. Source Code. https://github.com/rmitbggroup/LearnedIndexDiskExp.\n[2] [n. d.]. STX B+. https://panthema.net/2007/stx-btree/.\n[3] [n. d.]. Technical Report. https://github.com/rmitbggroup/LearnedIndexDiskExp/blob/main/TR.pdf.\n[4]Hussam Abu-Libdeh, Deniz AltinbÃ¼ken, Alex Beutel, Ed H. Chi, Lyric Doshi, Tim Kraska, Xiaozhou Li, Andy Ly, and\nChristopher Olston. 2020. Learned Indexes for a Google-scale Disk-based Database. CoRR abs/2012.12501 (2020).\n[5]Trevor Brown, Aleksandar Prokopec, and Dan Alistarh. 2020. Non-blocking interpolation search trees with doubly-\nlogarithmic running time. PPoPP (2020), 276â€“291.\n[6]Yifan Dai, Yien Xu, Aishwarya Ganesan, Ramnatthan Alagappan, Brian Kroth, Andrea C. Arpaci-Dusseau, and Remzi H.\nArpaci-Dusseau. 2020. From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees. In OSDI . 155â€“171.\n[7]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li, Hantian Zhang, Badrish Chandramouli,\nJohannes Gehrke, Donald Kossmann, David B. Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned\nIndex. In SIGMOD . 969â€“984.\n[8]Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020. Tsunami: A Learned Multi-dimensional\nIndex for Correlated Data and Skewed Workloads. Proc. VLDB Endow. 14, 2 (2020), 74â€“86.\n[9]Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-index: a fully-dynamic compressed learned index with\nprovable worst-case bounds. Proc. VLDB Endow. 13, 8 (2020), 1162â€“1175.\n[10] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim Kraska. 2019. FITing-Tree: A Data-\naware Index Structure. In SIGMOD . 1189â€“1206.\n[11] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, and Thomas Neumann.\n2020. RadixSpline: a single-pass learned index. In aiDM@SIGMOD . ACM, 5:1â€“5:5.\n[12] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018. The Case for Learned Index Structures.\nInSIGMOD . 489â€“504.\n[13] Tim Kraska, Umar Farooq Minhas, Thomas Neumann, Olga Papaemmanouil, Jignesh M. Patel, Christopher RÃ©, and\nMichael Stonebraker. 2021. ML-In-Databases: Assessment and Prognosis. IEEE Data Eng. Bull. 44, 1 (2021), 3â€“10.\n[14] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The adaptive radix tree: ARTful indexing for main-memory\ndatabases. In ICDE . IEEE Computer Society, 38â€“49.\n[15] Pengfei Li, Yu Hua, Jingnan Jia, and Pengfei Zuo. 2021. FINEdex: A Fine-grained Learned Index Scheme for Scalable\nand Concurrent Memory Systems. Proc. VLDB Endow. 15, 2 (2021), 321â€“334.\n[16] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A Learned Index Structure for Spatial Data. In\nSIMGOD . ACM, 2119â€“2133.\n[17] Anisa Llaveshi, Utku Sirin, Anastasia Ailamaki, and Robert West. 2019. Accelerating B+ tree search by using simple\nmachine learning techniques. In Proceedings of the 1st International Workshop on Applied AI for Database Systems and\nApplications .\n[18] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra, Alfons Kemper, Thomas Neumann,\nand Tim Kraska. 2020. Benchmarking Learned Indexes. Proc. VLDB Endow. 14, 1 (2020), 1â€“13.\n[19] Kurt Mehlhorn and Athanasios K. Tsakalidis. 1993. Dynamic Interpolation Search. J. ACM 40, 3 (1993), 621â€“634.\n[20] Mayank Mishra and Rekha Singhal. 2021. RUSLI: Real-time Updatable Spline Learned Index. In aiDM@SIGMOD . ACM,\n1â€“8.\n[21] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learning Multi-Dimensional Indexes. In\nSIGMOD . ACM, 985â€“1000.\n[22] Patrick E. Oâ€™Neil, Edward Cheng, Dieter Gawlick, and Elizabeth J. Oâ€™Neil. 1996. The Log-Structured Merge-Tree\n(LSM-Tree). Acta Informatica 33, 4 (1996), 351â€“385.\n[23] Joseph Oâ€™Rourke. 1981. An On-Line Algorithm for Fitting Straight Lines Between Data Ranges. Commun. ACM 24, 9\n(1981), 574â€“578.\n[24] Jianzhong Qi, Guanli Liu, Christian S. Jensen, and Lars Kulik. 2020. Effectively Learning Spatial Indices. Proc. VLDB\nEndow. 13, 11 (2020), 2341â€“2354.\n[25] Danilo Jimenez Rezende and Shakir Mohamed. 2015. Variational Inference with Normalizing Flows. In ICML , Vol. 37.\nJMLR.org, 1530â€“1538.\n[26] Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas, and Tim Kraska. 2021. Bounding the\nLast Mile: Efficient Learned String Indexing. CoRR abs/2111.14905 (2021).\n[27] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Minjie Wang, and Haibo Chen. 2020. XIndex:\na scalable learned index for multicore data storage. In PPoPP . ACM, 308â€“320.\n[28] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. 2020. SIndex: a scalable learned index for string keys.\nInAPSys . ACM, 17â€“24.\n[29] Chaichon Wongkham, Baotong Lu, Chris Liu, Zhicong Zhong, Eric Lo, and Tianzheng Wang. 2022. Are Updatable\nLearned Indexes Ready? Proc. VLDB Endow. 15, 11 (2022), 3004â€“3017.\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.\n\n139:22 Hai Lan et al.\n[30] Jiacheng Wu, Yong Zhang, Shimin Chen, Yu Chen, Jin Wang, and Chunxiao Xing. 2021. Updatable Learned Index with\nPrecise Positions. Proc. VLDB Endow. 14, 8 (2021), 1276â€“1288.\n[31] Shangyu Wu, Yufei Cui, Jinghuan Yu, Xuan Sun, Tei-Wei Kuo, and Chun Jason Xue. 2022. NFL: Robust Learned Index\nvia Distribution Transformation. Proc. VLDB Endow. 15, 10 (2022), 2188â€“2200.\n[32] Jiaoyi Zhang and Yihan Gao. 2022. CARMI: A Cache-Aware Learned Index with a Cost-based Construction Algorithm.\nProc. VLDB Endow. 15, 11 (2022), 2679â€“2691.\nReceived October 2022; revised January 2023; accepted February 2023\nProc. ACM Manag. Data, Vol. 1, No. 2, Article 139. Publication date: June 2023.",
  "textLength": 74646
}