{
  "paperId": "1715ffe2135fbf1ec8dbcb4a21dfb930253f24ca",
  "title": "Towards Better Interpretability in Deep Q-Networks",
  "pdfPath": "1715ffe2135fbf1ec8dbcb4a21dfb930253f24ca.pdf",
  "text": "The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)\nTowards Better Interpretability in Deep Q-Networks\nRaghuram Mandyam Annasamy\nCarnegie Mellon University\nrannasam@cs.cmu.eduKatia Sycara\nCarnegie Mellon University\nkatia@cs.cmu.edu\nAbstract\nDeep reinforcement learning techniques have demonstrated\nsuperior performance in a wide variety of environments. As im-\nprovements in training algorithms continue at a brisk pace, the-\noretical or empirical studies on understanding what these net-\nworks seem to learn, are far behind. In this paper we propose\nan interpretable neural network architecture for Q-learning\nwhich provides a global explanation of the model’s behavior\nusing key-value memories, attention and reconstructible em-\nbeddings. With a directed exploration strategy, our model can\nreach training rewards comparable to the state-of-the-art deep\nQ-learning models. However, results suggest that the features\nextracted by the neural network are extremely shallow and\nsubsequent testing using out-of-sample examples shows that\nthe agent can easily overﬁt to trajectories seen during training.\nIntroduction\nThe last few years have witnessed a rapid growth of research\nand interest in the domain of deep Reinforcement Learn-\ning (RL) due to the signiﬁcant progress in solving RL prob-\nlems (Arulkumaran et al .2017). Deep RL has been applied\nto a wide variety of disciplines ranging from game play-\ning, robotics, systems to natural language processing and\neven biological data (Silver et al .2017; Mnih et al .2015;\nLevine et al. 2016; Kraska et al. 2018; Williams, Asadi, and\nZweig 2017; Choi et al .2017). However, most applications\ntreat neural networks as a black-box and the problem of un-\nderstanding and interpreting deep learning models remains a\nhard problem. This is even more understudied in the context\nof deep reinforcement learning and only recently has started\nto receive attention. Commonly used visualization methods\nfor deep learning such as saliency maps and t-SNE plots of\nembeddings have been applied to deep RL models (Grey-\ndanus et al .2017; Zahavy, Ben-Zrihem, and Mannor 2016;\nMnih et al .2015). However, there are a few questions over the\nreliability of saliency methods including, as an example, sen-\nsitivity to simple transformations of the input (Kindermans et\nal.2017). The problem of generalization and memorization\nwith deep RL models is also important. Recent ﬁndings sug-\ngest that deep RL agents can easily memorize large amounts\nof training data with drastically varying test performance\nCopyright c\r2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.and are vulnerable to adversarial attacks (Zhang et al .2018;\nZhang, Ballas, and Pineau 2018; Huang et al. 2017).\nIn this paper, we propose a neural network architecture for\nQ-learning using key-value stores, attention and constrained\nembeddings, that is easier to study than the traditional deep\nQ-network architectures. This is inspired by some of the\nrecent work on Neural Episodic Control (NEC) (Pritzel et\nal.2017) and distributional perspectives on RL (Bellemare,\nDabney, and Munos 2017). We call this model i-DQN for\nInterpretable DQN and study latent representations learned\nby the model on standard Atari environments from Open AI\ngym (Brockman et al .2016). Most current work around in-\nterpretability in deep learning is based on local explanations\ni.e. explaining network predictions for speciﬁc input exam-\nples (Lipton 2016). For example, saliency maps can highlight\nimportant regions of the input that inﬂuence the output of\nthe neural network. In contrast, global explanations attempt\nto understand the mapping learned by a neural network re-\ngardless of the input. We achieve this by constraining the\nlatent space to be reconstructible and inverting embeddings\nof representative elements in the latent space (keys). This\nhelps us understand aspects of the input space (images) that\nare captured in the latent space across inputs. Our visualiza-\ntions suggest that the features extracted by the convolutional\nlayers are extremely shallow and can easily overﬁt to trajec-\ntories seen during training. This is in line with the results of\n(Zhang et al .2018) and (Zhang, Ballas, and Pineau 2018).\nAlthough our main focus is to understand learned models,\nit is important that the models we analyze perform well on\nthe task at hand. To this end, we show our model achieves\ntraining rewards comparable to Q-learning models like Dis-\ntributional DQN (Bellemare, Dabney, and Munos 2017). Our\ncontribution in this work is threefold:\n\u000fWe explore a different neural network architecture with\nkey-value stores, constrained embeddings and an explicit\nsoft-assignment step that separates representation learning\nand Q-value learning (state aggregation).\n\u000fWe show that such a model can improve interpretability\nin terms of visualizations of the learned keys (cluster),\nattention maps and saliency maps. Our method attempts\nto provide a global explanation of the model’s behavior\ninstead of explaining speciﬁc input examples (local ex-\nplanations). We also develop a few examples to test the\n4561\n\ngeneralization behavior.\n\u000fWe show that the model’s uncertainty can be used to drive\nexploration that reaches reasonably high rewards with re-\nduced sample complexity (training examples) on some of\nthe Atari environments.\nRelated Work\nMany attempts have been made to tackle the problem of in-\nterpretability with deep learning, largely in the supervised\nlearning case. (Zhang and Zhu 2018) carry out an in-depth\nsurvey on interpretability with Convolutional Neural Net-\nworks (CNNs). Our approach to visualizing embeddings is\nin principle similar to the work of (Dosovitskiy and Brox\n2016) on inverting visual representations. They train a neu-\nral network with deconvolution layers using HOG, SIFT\nand AlexNet embeddings as input and their corresponding\nreal images as ground truth (the sole purpose of this net-\nwork being visualization). Saliency maps are another popular\ntype of method that generate local explanations which gener-\nally use gradient-like information to identify salient parts of\nthe image. The different ways of computing saliency maps\nare covered exhaustively in (Zhang and Zhu 2018). Few of\nthese have been applied in the context of deep reinforce-\nment learning. (Zahavy, Ben-Zrihem, and Mannor 2016) use\nthe Jacobian of the network to compute saliency maps on\na Q-value network. Perturbation based saliency maps using\na continuous mask across the image and also using object\nsegmentation based masks have been studied in the con-\ntext of deep-RL (Greydanus et al .2017; Iyer et al .2018;\nLi, Sycara, and Iyer 2017). In contrast to these approaches,\nour method is based on a global view of the network. Given\na particular action and expected returns, we invert the cor-\nresponding key to try and understand visual aspects being\ncaptured by the embedding regardless of the input state. More\nrecently, (Verma et al .2018) introduce a new method that\nﬁnds interpretable programs that can best explain the policy\nlearned by a neural network- these programs can also be\ntreated as global explanations for the policy networks.\nArchitecturally, our network is similar to the network ﬁrst\nproposed by (Pritzel et al .2017). The authors describe their\nmotivation as speeding up the learning process using a semi-\ntabular representation with Q-value calculations similar to\nthe tabular Q-learning case. This is to avoid the inherent slow-\nness of gradient descent and reward propagation. Their model\nlearns to attend over a subset of states that are similar to the\ncurrent state by tracking all the states recently seen (up-to\nhalf-million states) using a k-d tree. However, their method\ndoes not have any notion of clustering or ﬁxed Q-values. Our\nproposed method is also similar to Bellemare, Dabney, and\nMunos’s work on categorical/distributional DQN. The differ-\nence is that in our model the cluster embeddings (keys) for dif-\nferent Q-values are accessible freely (for analysis and visual-\nization) because of the explicit soft-assignment step, whereas\nit is almost impossible to ﬁnd such representations while\nhaving fully-connected layers like in (Bellemare, Dabney,\nand Munos 2017). Although we do not employ any iterative\nprocedure (like reﬁning keys; we train fully using backpropa-\ngation), works on combining deep embeddings with unsuper-vised clustering methods (Xie, Girshick, and Farhadi 2016;\nChang et al .2017) (joint optimization/iterative reﬁnement)\nhave started to pick up pace and show better performance\ncompared to traditional clustering methods.\nAnother important direction that is relevant to our work\nis that of generalizing behavior of neural networks in the\nreinforcement learning setting. (Henderson et al .2017) dis-\ncuss in detail about general problems of deep RL research\nand evaluation metrics used for reporting. (Zhang et al .2018;\nZhang, Ballas, and Pineau 2018) perform systematic exper-\nimental studies on various factors affecting generalization\nbehavior such as diversity in training seeds and randomness\nin environment rewards. They conclude that deep RL mod-\nels can easily overﬁt to random reward structures or when\nthere is insufﬁcient training diversity and careful evaluation\ntechniques (such as isolated training and testing seeds) are\nneeded.\nProposed Method\nWe follow the usual RL setting and assume the environment\ncan be modelled as a Markov Decision Process (MDP) rep-\nresented by the 5-tuple (S;A;T;R;\r ), whereSis the state\nspace,Ais the action space, T(s0js;a)is the state transition\nprobability function, R(s;a)is the reward function and \r2\n[0;1)is the discount factor. A policy \u0019:S!Amaps every\nstate to a distribution over actions. The value function V\u0019(st)\nis the expected discounted sum of rewards by following pol-\nicy\u0019from statestat timet,V\u0019(st) =E[PT\ni=0\rirt+i]. Sim-\nilarly, the Q-value (action-value) Q\u0019(st;a)is the expected\nreturn starting from state st, taking action aand then follow-\ning\u0019. Q-value function can be recursively estimated using the\nBellman equation Q\u0019(st;a) =E[rt+\rmaxa0Q(st+1;a0)]\nand\u0019\u0003is the optimal policy which achieves the highest\nQ\u0019(st;a)over all policies \u0019.\nSimilar to the traditional DQN architecture (Mnih et al .\n2015), any state st(a concatenated set of input frames) is\nencoded using a series of convolutional layers each followed\nby a non-linearity and ﬁnally a fully-connected layer at the\nendh(st) =Conv (st). This would usually be followed by\nsome non-linearity and a fully-connected layer that outputs\nQ-values. Instead, we introduce a restricted key-value store\nover which the network learns to attend as shown in Figure 1.\nIntuitively, the model is trying to learn two things. First, it\nlearns a latent representation h(st)that captures important vi-\nsual aspects of the input images. At the same time, the model\nalso attempts to learn an association between embeddings of\nstatesh(st)and embeddings of keys in the key-value store.\nThis would help in clustering the state (around the keys)\nbased on the scale of expected returns (Q-values) from that\nstate. We can think of the Ndifferent keys ha(for a given\nactiona) weakly as the cluster centers for the corresponding\nQ-values, attention weights wa(st)as a soft assignment be-\ntween embeddings for current state h(st)and embeddings for\ndifferent Q-valuesfha\n1;ha\n2;\u0001\u0001\u0001ha\nNg. This explicit association\nstep helps us in understanding the model in terms of attention\nmaps and visualizations of the cluster centers (keys).\nThe key-value store is restricted in terms of size and values\nof the store. Each action a2Ahas a ﬁxed number of key-\n4562\n\nX\nConvolution\n(32 @ 20x20)Convolution\n(64 @ 9x9)Convolution\n(64 @ 7x7)Linear - 256\nDe-Convolution\n(64 @ 7x7)De-Convolution\n(64 @ 9x9)De-Convolution\n(32 @ 20x20)Attention W eights W = F linear_256 * Keys  \nLinear - 256Values\n V 1\nV 2\nV 3\nV N\nKeys\n(256 dimensional)  Actions\nQ(s, a) = W * V aluesFigure 1: Model Architecture- Interpretable DQN (i-DQN)\nvalue pairs (say N) and the value associated with every key\nis also held constant. Nvaluesfv1;v2;\u0001\u0001\u0001;vNgare sampled\nuniformly at random from (Vmin;Vmax)(usually (\u000025;25))\nonce and the same set of values are used for all the actions.\nAll of the keys ( N\u0002A) are also initialized randomly. To\ncompute attention, the embeddings h(st)for a statestare\ncompared to all the keys fha\n1;ha\n2;\u0001\u0001\u0001;ha\nNgin the store for a\nparticular action (a)using a softmax over their dot products.\nw(st)a\ni=exp (h(st)\u0001ha\ni)P\njexp (h(st)\u0001ha\nj)(1)\nThese attention weights over keys and their corresponding\nvalue terms are then used to calculate the Q-values.\nQ(st;a) =X\niwa\ni(st)vi\nNow that we have Q-values, we can deﬁne the different losses\nthat can be used to train the network,\n\u000fBellman Error (Lbellman ): The usual value function esti-\nmation error.\nLbellman (\u0012) = (Q(st;a;\u0012)\u0000Yt)2\nwhereYt=R(st;a;st+1) +\rmax0\naQ(st+1;a0;\u0012)\n\u000fDistributive Bellman Error ( Ldistrib ): We force the dis-\ntributive constraint on attention weights between current\nand next states similar to (Bellemare, Dabney, and Munos\n2017) using values fv1;v2;\u0001\u0001\u0001;vNgas supports of the\ndistribution. The distributive loss is deﬁned as the KL\ndivergence between \u001eTw(st+1)a\u0003andw(st)awhereT\nis the distributional Bellman operator and \u001eis the pro-\njection operator and a\u0003is best action at state st+1i.e.a\u0003= arg maxaQ(st+1;a).\nLdistrib (\u0012) =DKL(\u001eTw(st+1)a\u0003;w(st)a) (2)\n=\u0000X\ni\u001eTw(st+1)a\u0003\ni\u0001w(st)a\ni (3)\nEquation (3) is simply the cross entropy loss (assuming\nw(st+1)a\u0003to be constant with respect to \u0012, similar to the\nassumption for Ytin Bellman error).\n\u000fReconstruction Error ( Lreconstruct ): We also constrain the\nembeddings h(st)for any state to be reconstructible. This\nis done by transforming h(st)using a fully-connected\nlayer and then followed by a series of non-linearity and\ndeconvolution layers.\nhdec(st) =Wdech(st) (4)\n^st=Deconv (hdec(st))\nThe mean squarred error between reconstructed image ^st\nand original image stis used,\nLreconstruct (\u0012) =1\n2jj^st\u0000stjj2\n2\n\u000fDiversity Error (Ldiversity ): The diversity error forces at-\ntention over different keys in a batch. This is important\nbecause training can collapse early with the network learn-\ning to focus on very few speciﬁc keys (because both the\nkeys and attention weights are being learned together). We\ncould use KL-divergence between the attention weights\nbut (Lin et al .2017) develop an elegant solution to this in\ntheir work.\nLdiversity (\u0012) =jj(AAT\u0000I)jj2\nwhereAis a 2D matrix of size (batch size, N) and each\nrow ofAis the attention weight vector w(st)a. It drives\n4563\n\n(a) SpaceInvaders\n (b) Qbert\n(c) MsPacman\n (d) MsPacman, DDQN\nFigure 2: Visualizing keys, state embeddings using t-SNE: i-DQN, Q-value 25 (a)-(c); Double DQN(d)\nAATto be diagonal (no overlap between keys attended\nto within a batch) and l-2 norm ofwto be 1. Because of\nsoftmax, the l-1 norm is also 1and so ideally the attention\nmust peak at exactly one key however in practice it spreads\nover as few keys as possible. Finally, the model is trained\nto minimize a weighted linear combination of all the four\nlosses.\nLfinal(\u0012) =\u00151Lbellman (\u0012) +\u00152Ldistrib (\u0012)\n+\u00153Lreconstruct (\u0012) +\u00154Ldiversity (\u0012)\nExperiments and Discussions\nWe report the performance of our model on eight Atari envi-\nronments (Brockman et al .2016)- Alien, Freeway, Frostbite,\nGravitar, MsPacman, Qbert, SpaceInvaders, and Venture, in\nTable 1.1Using the taxonomoy of Atari games from (Belle-\nmare et al .2016) and (Ostrovski et al .2017), seven of the\neight environments tested (all except SpaceInvaders) are con-\nsidered hard exploration problems. Additionally, three of\nthem (Freeway, Gravitar and Venture) are hard to explore be-\ncause of sparse rewards. Since our focus is on interpretability,\nwe do not carry out an exhaustive performance comparison.\nWe simply show that training rewards achieved by i-DQN\nmodel are comparable to some of the state-of-the-art models.\nThis is important because we would like our deep-learning\nmodels to be interpretable but also remain competitive at\nthe same time. We look at scores against other exploration\nbaselines for Q-learning that do not involve explicit reward\nshaping/exploration bonuses- Bootstrap DQN (Osband et al.\n2016), Noisy DQN (Fortunato et al .2017) and Q-ensembles\n(Chen et al. 2018).\n1Code available at https://github.com/maraghuram/I-DQNDirected exploration\nWe use the uncertainty in attention weights to drive explo-\nration during training. U(st;a)is an approximate upper conﬁ-\ndence on the Q-values. Similar to (Chen et al .2018) we select\nthe action maximizing a UCB style conﬁdence interval,\nQ(st;a) =X\niwa\ni(st)vi\nU(st;a) =s\nQ(st;a)2\u0000X\niwa\ni(st)v2\ni\nat= arg max\na2AQ(st;a) +\u0015expU(st;a)\nTable 1 compares i-DQN’s performance (with directed ex-\nploration) against a baseline Double DQN implementation\n(which uses epsilon-greedy exploration) at 10M frames. Dou-\nble DQN (DDQN), Distributional DQN (Distrib. DQN),\nBootstrap DQN and Noisy DQN agents are trained for up\nto 50M steps which translates to 200M frames (Hessel et\nal.2017; Fortunato et al .2017; Osband et al .2016). The\nQ-ensemble agent using UCB-style exploration is trained for\nup to 40M frames (Chen et al .2018). We see that on some of\nthe games, our model reaches higher training rewards within\n10M frames compared to Double DQN, Distributional DQN\nmodels. Also, our model is competitive with the ﬁnal scores\nreported by other exploration baselines like Bootstrap DQN,\nNoisy DQN and Q-ensembles, and and even performs better\non some environments (5 out of 8 games). The training time\nfor i-DQN is roughly 2x slower because of the multiple loss\nfunctions compared to our implementation of Double DQN.\n4564\n\nEnvironment10M frames Reported Scores (ﬁnal)\nDDQN i-DQN DDQN Distrib. DQN Q-ensemble Bootstrap DQN Noisy DQN\nAlien 1,533.45 2,380.72 3,747.7 4,055.8 2,817.6 2,436.6 2,394.90\nFreeway 22.5 28.79 33.3 33.6 33.96 33.9 32\nFrostbite 754.48 3,968.45 1,683.3 3,938.2 1,903.0 2,181.4 583.6\nGravitar 279.89 517.33 412 681 318 286.1 443.5\nMsPacman 2,251.43 6,132.21 2,711.4 3,769.2 3,425.4 2,983.3 2,501.60\nQbert 10,226.93 19,137.6 15,088.5 16,956.0 14,198.25 15,092.7 15,276.30\nSpace Invaders 563.2 979.45 2,525.5 6,869.1 2,626.55 2,893 2,145.5\nVenture 70.87 985.11 98 1,107.0 67 212.5 0\nTable 1: Training scores (averaged over 100 episodes, 3 seeds). Scores for Double DQN , Distributional DQN and Noisy DQN\nare from (Hessel et al .2017); Scores for Bootstrap-DQN are as reported in the original paper (Osband et al .2016); Scores for\nUCB style exploration with Q-ensembles are from (Chen et al. 2018)\n(a) Down\n (b) Downleft\n (c) Upleft\n (d) Right\nFigure 3: MsPacman, Inverting keys for Q-value 25\n(a) Right\n (b) R-Fire\n (c) Left\n (d) L-Fire\n (e) Fire\nFigure 4: SpaceInvaders, Inverting keys for Q-value 25\nWhat do the keys represent?\nThe keys are latent embeddings (randomly initialized) that\nbehave like cluster centers for the particular action-return\npairs (latent space being R256). Instead of training using un-\nsupervised methods like K-means or mixture models, we use\nthe neural network itself to ﬁnd these points using gradient\ndescent. For example, the key for action right; Q-value 25\n(Figure 2c) is a cluster center that represents the latent em-\nbeddings for all states where the agent expects a return of 25\nby selecting action right. These keys partition the latent space\ninto well formed clusters as shown in Figure 2, suggesting\nthat embeddings also contain action-speciﬁc information cru-\ncial for an RL agent. On the other hand, Figure 2d shows em-\nbeddings for DDQN which are not easily separable (similar\nto the visualizations in (Mnih et al .2015)). Since we use sim-\nple dot-product based distance for attention, keys and state\nembeddings must lie in a similar space and this can be seen in\nthe t-SNE visualization i.e. keys (square boxes) lie within the\nstate embeddings (Figure 2). The fact that the keys lie close to\ntheir state embeddings is essential to interpretability because\nstate embeddings satisfy reconstructability constraints.Inversion of keys\nAlthough keys act like cluster centers for action-return pairs,\nit is difﬁcult to interpret them in the latent space. By inverting\nkeys, we attempt to ﬁnd important aspects of input space\n(images) that inﬂuence the agent to choose particular action-\nreturn pair ( Deconv (ha\ni)). These are ‘global explanations’\nbecause inverting keys is independent of the input. For exam-\nple, in MsPacman, reconstructing keys for different actions\n(ﬁxing return of 25) indicates yellow blobs at many different\nplaces for each action (Figure 3). We hypothesize that these\ncorrespond to the Pacman object itself and that the model\nmemorizes its different positions to make its decision i.e. the\nyellow blobs in Figure 3d correspond to different locations\nof Pacman and for any input state where Pacman is in one\nof those positions, the agent selects action right expecting\na return of 25. Figure 5 shows such examples where the\nagent’s action-return selection agrees with reconstructed key\n(red boxes indicate Pacman’s location). Similarly, in SpaceIn-\nvaders, the agent seems to be looking at speciﬁc combinations\nof shooter and alien ship positions that were observed during\ntraining (Figure 4).\nThe keys have never been observed by the deconvolution\nnetwork during training and so the reconstructions depend\nupon its generalizability. Interestingly, reconstructions for\naction-return pairs that are seen more often tend to be less\nnoisy with less artifacts. This can be observed in Figure 3\nfor Q-value 25 where actions Right, Downleft and Upleft\nnearly 65% of all actions taken by the agent. We also look at\nthe effect of different reconstruction techniques keeping the\naction-return pair ﬁxed (Figure 6). Variational autoencoder\nwith\fset to 0 yields sharper looking images but increasing\n\fwhich is supposed to bring out disentanglement in the\nembeddings yields reconstructions with almost no objects.\nDense V AE with \f= 0is a slightly deeper network similar\nto (Oh et al .2015) and seems to reconstruct slightly clearer\nshapes of ghosts and pacman.\nEvaluating the reconstructions\nTo understand the effectiveness of these visualizations, we\ndesign a quantitative metric that measures the agreement\nbetween actions taken by the agent and the actions sug-\ngested using the reconstructed images. With a fully trained\n4565\n\nInput State\n Input State\n Input State\n Input State\n Input State\n(a) (Downleft, 25)\n (b) (Right, 25)\n (c) (Downleft, 25)\n (d) (Upleft, 25)\n (e) (Up, 25)\nFigure 5: MsPacman, examples where agent’s decision agrees with the reconstructed image\nAgreement AE V AE\n(\f= 0)V AE\n(\f= 0:01)Dense V AE\n(\f= 0)\nMsPacman\n(Color)30.76 29.78 16.8 23.73\nMsPacman\n(Gray,\nRescaled)19.87 18.14 10.97 14.56\nTable 2: Evaluating visualizations: Agreement scores\nFigure 6: Reconstruction: AE, V AE \f= 0, V AE\f= 0:01,\nDense V AE\nmodel, we reconstruct images fsa\n1;sa\n2;\u0001\u0001\u0001sa\nNgfrom the keys\nfha\n1;ha\n2;\u0001\u0001\u0001ha\nNgfor all actions a2A. In every state st, we\ninduce another distribution on the Q-values using the cosine\nsimilarity in the image space,\nw0(st)a\ni=Softmax (st\u0001sa\ni\njjstjj2\u0001jjsa\nijj2)\nsimilar tow(st)a\ni(which is also a distribution over Q-values\nbut in the latent space). Using w0(st)a\ni, we can compute\nQ0(st;a)andU0(st;a)as before and select an action a0\nt=\narg maxa2AQ0(st;a) +\u0015expU0(st;a). Usingatanda0\nt, wedeﬁne our metric of agreeability as\nAgreement =1at=a0\nt\n1at=a0\nt+ 1at6=a0\nt\nwhere 1is the indicator function. We measure this across\nmultiple rollouts (5) using atand average them . In Table 2,\nwe report Agreement as a percentage for different encoder-\ndecoder models. Unfortunately, the best agreement between\nthe actions selected using the distributions in the image space\nand latent space is around 31% for the unscaled color ver-\nsion of MsPacman. In MsPacman, the agent has 9 different\nactions and a random strategy would expect to have an Agree-\nment of\u001811%. However, if the agreement scores were high\n(80-90%), that would suggest that the Q-network is indeed\nlearning to memorize conﬁgurations of objects seen during\ntraining. One explanation for the gap is that reconstructions\nrely heavily on generalizability to unseen keys.\nAdversarial examples that show memorization\nLooking at the visualizations and rollouts of a fully trained\nagent, we hand-craft a few out-of-sample environment states\nto examine the agent’s generalization behavior. For example,\nin MsPacman, since visualizations suggest that the agent\nmay be memorizing pacman’s positions (also maybe ghosts\nand other objects), we simply add an extra pellet adjacent\nto a trajectory seen during training (Figure 7a). The agent\ndoes not clear the additional pellet and simply continues\nto execute actions performed during training (Figure 7b).\nMost importantly, the agent is extremely conﬁdent in taking\nactions initially (seen in attention maps Figure 7a) which\nsuggest that the extra pellet was probably not even captured\nby the embeddings. Similarly, in case of SpaceInvaders, the\nagent has a strong bias towards shooting from the leftmost-\nend (seen in Figure 4). This helps in clearing the triangle\n4566\n\n(a) Adversarial example\n(b) Trajectory during training\n(c) Adversarial example\n(d) Trajectory during training\nFigure 7: Adversarial examples for MsPacman (a)-(b) and SpaceInvaders (c)-(d)\nlike shape and moving to the next level (Figure 7d). However,\nwhen triangular positions of spaceships are inverted, the agent\nrepeats the same strategy of trying to shoot from left and\nfails to clear ships (Figure 7c). These examples indicate that\nthe features extracted by the convolutional channels seem\nto be shallow. The agent does not really model interactions\nbetween objects. For example in MsPacman, after observing\n10M frames, it does not know general relationships between\npacman and pellet or ghosts. Even if optimal Q-values were\nknown, there is no incentive for the network to model these\nhigher order dependencies when it can work with situational\nfeatures extracted from ﬁnite training examples. (Zhang et\nal.2018) also report similar results on simple mazes where\nan agent trained on insufﬁcient environments tends to repeat\ntraining trajectories on unseen mazes.Sensitivity to hyperparameters\nI-DQN’s objective function introduces four hyperparameters\nfor weighting the different loss components ( \u00151: for bell-\nman error,\u00152: distributional error, \u00153: reconstruction error\nand\u00154: diversity error). The diversity error forces attention\nover multiple keys (examples for \u00154= 0 and\u00154= 0:01\nare shown in the supplementary material). In general, we\nfound the values \u00151= 1:0,\u00152= 1:0,\u00153= 0:05,\u00154= 0:01\nto work well across games (detailed list of hyperparame-\nters and their values is reported in the supplementary mate-\nrial). We ran experiments for different settings of \u00151,\u00152and\n\u00153keeping\u00154= 0:01constant on Ms Pacman (averaged\nover 3 trials). In general, increasing \u00153(coefﬁcient on recon-\nstruction error) to 0:5and5:0yields visually better quality\nreconstructions but poorer scores- 3;245:3and3;013:1re-\nspectively (drop by \u001845%). Increasing \u00151=\u00152= 10:0\n4567\n\nalso drops the score and yields poor reconstructions (some-\ntimes without the objects of the game which loses inter-\npretability) but converges quite quickly ( 5;267:15, drop by\n\u001812%). Out of \u00151(bellman loss) and \u00152(distributional\nloss),\u00152seems to play a more important in reaching higher\nscores compared to \u00151[\u00151= 1:0,\u00152= 10:0score: 4;916:0\n;\u00151= 10:0,\u00152= 1:0, score: 4;053:6]. So, the setting\n\u00151= 1:0,\u00152= 1:0,\u00153= 0:05,\u00154= 0:01seems to ﬁnd\nthe right balance between the q-value learning losses and\nregularizing losses. For the exploration factor, we tried a few\ndifferent values \u0015exp=f0:1;0:01;0:001gand it did not have\na signiﬁcant effect on the scores.\nConclusion\nIn this paper, we propose an interpretable deep Q-network\nmodel (i-DQN) that can be studied using a variety of tools\nincluding the usual saliency maps, attention maps and recon-\nstructions of key embeddings that attempt to provide global\nexplanations of the model’s behavior. We also show that the\nuncertainty in soft cluster assignment can be used to drive ex-\nploration effectively and achieve high training rewards com-\nparable to other models. Although the reconstructions do not\nexplain the agent’s decisions perfectly, they provide a better\ninsight into the kind of features extracted by convolutional\nlayers. This can be used to design interesting adversarial\nexamples with slight modiﬁcations to the state of the envi-\nronment where the agent fails to adapt and instead repeats\naction sequences that were performed during training. This\nis the general problem of overﬁtting in machine learning but\nis more acute in the case of reinforcement learning because\nthe process of collecting training examples depends largely\non the agent’s biases (exploration). There are many interest-\ning directions for future work. For example, we know that\nthe reconstruction method largely affects the visualizations\nand other methods such as generative adversarial networks\n(GANs) (Goodfellow et al .2014) can model latent spaces\nmore smoothly and could generalize better to unseen embed-\ndings. Another direction is to see if we can automatically\ndetect the biases learned by the agent and design meaningful\nadversarial examples instead of manually crafting test cases.\nAcknowledgments\nThis work was funded by awards AFOSR FA9550-15-1-0442\nand NSF IIS- 1724222.\nReferences\nArulkumaran, K.; Deisenroth, M. P.; Brundage, M.; and\nBharath, A. A. 2017. A brief survey of deep reinforcement\nlearning. arXiv preprint arXiv:1708.05866 .\nBellemare, M.; Srinivasan, S.; Ostrovski, G.; Schaul, T.; Sax-\nton, D.; and Munos, R. 2016. Unifying count-based ex-\nploration and intrinsic motivation. In Advances in Neural\nInformation Processing Systems , 1471–1479.\nBellemare, M. G.; Dabney, W.; and Munos, R. 2017. A\ndistributional perspective on reinforcement learning. arXiv\npreprint arXiv:1707.06887 .Brockman, G.; Cheung, V .; Pettersson, L.; Schneider, J.;\nSchulman, J.; Tang, J.; and Zaremba, W. 2016. Openai\ngym.\nChang, J.; Wang, L.; Meng, G.; Xiang, S.; and Pan, C. 2017.\nDeep adaptive image clustering. In 2017 IEEE International\nConference on Computer Vision (ICCV) , 5880–5888. IEEE.\nChen, R. Y .; Sidor, S.; Abbeel, P.; and Schulman, J. 2018.\nUcb exploration via q-ensembles.\nChoi, E.; Hewlett, D.; Uszkoreit, J.; Polosukhin, I.; Lacoste,\nA.; and Berant, J. 2017. Coarse-to-ﬁne question answering\nfor long documents. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) , volume 1, 209–220.\nDosovitskiy, A., and Brox, T. 2016. Inverting visual rep-\nresentations with convolutional networks. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition , 4829–4837.\nFortunato, M.; Azar, M. G.; Piot, B.; Menick, J.; Osband, I.;\nGraves, A.; Mnih, V .; Munos, R.; Hassabis, D.; Pietquin, O.;\net al. 2017. Noisy networks for exploration. arXiv preprint\narXiv:1706.10295 .\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-\nFarley, D.; Ozair, S.; Courville, A.; and Bengio, Y . 2014.\nGenerative adversarial nets. In Advances in neural informa-\ntion processing systems , 2672–2680.\nGreydanus, S.; Koul, A.; Dodge, J.; and Fern, A. 2017.\nVisualizing and understanding atari agents. arXiv preprint\narXiv:1711.00138 .\nHenderson, P.; Islam, R.; Bachman, P.; Pineau, J.; Precup,\nD.; and Meger, D. 2017. Deep reinforcement learning that\nmatters. arXiv preprint arXiv:1709.06560 .\nHessel, M.; Modayil, J.; Van Hasselt, H.; Schaul, T.; Ostro-\nvski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and\nSilver, D. 2017. Rainbow: Combining improvements in deep\nreinforcement learning. arXiv preprint arXiv:1710.02298 .\nHuang, S.; Papernot, N.; Goodfellow, I.; Duan, Y .; and\nAbbeel, P. 2017. Adversarial attacks on neural network\npolicies. arXiv preprint arXiv:1702.02284 .\nIyer, R.; Li, Y .; Li, H.; Lewis, M.; Sundar, R.; and Sycara, K.\n2018. Transparency and explanation in deep reinforcement\nlearning neural networks.\nKindermans, P.-J.; Hooker, S.; Adebayo, J.; Alber, M.; Sch ¨utt,\nK. T.; D ¨ahne, S.; Erhan, D.; and Kim, B. 2017. The (un) relia-\nbility of saliency methods. arXiv preprint arXiv:1711.00867 .\nKraska, T.; Beutel, A.; Chi, E. H.; Dean, J.; and Polyzotis, N.\n2018. The case for learned index structures. In Proceedings\nof the 2018 International Conference on Management of\nData , 489–504. ACM.\nLevine, S.; Finn, C.; Darrell, T.; and Abbeel, P. 2016. End-\nto-end training of deep visuomotor policies. The Journal of\nMachine Learning Research 17(1):1334–1373.\nLi, Y .; Sycara, K. P.; and Iyer, R. 2017. Object-sensitive\ndeep reinforcement learning. In GCAI 2017, 3rd Global\nConference on Artiﬁcial Intelligence, Miami, FL, USA, 18-22\nOctober 2017. , 20–35.\n4568\n\nLin, Z.; Feng, M.; Santos, C. N. d.; Yu, M.; Xiang, B.; Zhou,\nB.; and Bengio, Y . 2017. A structured self-attentive sentence\nembedding. arXiv preprint arXiv:1703.03130 .\nLipton, Z. C. 2016. The mythos of model interpretability.\narXiv preprint arXiv:1606.03490 .\nMnih, V .; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,\nJ.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,\nA. K.; Ostrovski, G.; et al. 2015. Human-level control\nthrough deep reinforcement learning. Nature 518(7540):529.\nOh, J.; Guo, X.; Lee, H.; Lewis, R. L.; and Singh, S. 2015.\nAction-conditional video prediction using deep networks in\natari games. In Advances in Neural Information Processing\nSystems , 2863–2871.\nOsband, I.; Blundell, C.; Pritzel, A.; and Van Roy, B. 2016.\nDeep exploration via bootstrapped dqn. In Advances in neural\ninformation processing systems , 4026–4034.\nOstrovski, G.; Bellemare, M. G.; Oord, A. v. d.; and Munos,\nR. 2017. Count-based exploration with neural density models.\narXiv preprint arXiv:1703.01310 .\nPritzel, A.; Uria, B.; Srinivasan, S.; Puigdomenech, A.;\nVinyals, O.; Hassabis, D.; Wierstra, D.; and Blundell, C. 2017.\nNeural episodic control. arXiv preprint arXiv:1703.01988 .\nSilver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai,\nM.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel,\nT.; et al. 2017. Mastering chess and shogi by self-play with\na general reinforcement learning algorithm. arXiv preprint\narXiv:1712.01815 .\nVerma, A.; Murali, V .; Singh, R.; Kohli, P.; and Chaudhuri, S.\n2018. Programmatically interpretable reinforcement learning.\narXiv preprint arXiv:1804.02477 .\nWilliams, J. D.; Asadi, K.; and Zweig, G. 2017. Hybrid code\nnetworks: practical and efﬁcient end-to-end dialog control\nwith supervised and reinforcement learning. arXiv preprint\narXiv:1702.03274 .\nXie, J.; Girshick, R.; and Farhadi, A. 2016. Unsupervised\ndeep embedding for clustering analysis. In International\nconference on machine learning , 478–487.\nZahavy, T.; Ben-Zrihem, N.; and Mannor, S. 2016. Gray-\ning the black box: Understanding dqns. In International\nConference on Machine Learning , 1899–1908.\nZhang, Q.-s., and Zhu, S.-C. 2018. Visual interpretability for\ndeep learning: a survey. Frontiers of Information Technology\n& Electronic Engineering 19(1):27–39.\nZhang, A.; Ballas, N.; and Pineau, J. 2018. A dissection of\noverﬁtting and generalization in continuous reinforcement\nlearning. arXiv preprint arXiv:1806.07937 .\nZhang, C.; Vinyals, O.; Munos, R.; and Bengio, S. 2018. A\nstudy on overﬁtting in deep reinforcement learning. arXiv\npreprint arXiv:1804.06893 .\n4569",
  "textLength": 35407
}