{
  "paperId": "9a734beaca83309ad2ec70bdc9d308897d62d688",
  "title": "Deep Learning Service for Efficient Data Distribution Aware Sorting",
  "pdfPath": "9a734beaca83309ad2ec70bdc9d308897d62d688.pdf",
  "text": "Deep Learning Service for Efficient Data\nDistribution Aware Sorting\nXiaoke Zhu†\nBeihang University\nChina\nzhuxk@buaa.edu.cnQi Zhang\nMeta Platforms\nUSA\nqizhang@meta.comWei Zhou*\nYunnan University\nChina\nzwei@ynu.edu.cnLing Liu\nGeorgia Institute of Technology\nUSA\nling.liu@cc.gatech.edu\nAbstract —In this paper, we present a neural network-enabled\ndata distribution aware sorting method, coined as NN-sort. Our\napproach explores the potential of developing deep learning\ntechniques to speed up large-scale sort operations, enabling data\ndistribution aware sorting as a deep learning service. Compared\nto traditional pairwise comparison-based sorting algorithms,\nwhich sort data elements by performing pairwise operations, NN-\nsort leverages the neural network model to learn the data distri-\nbution and uses it to map large-scale data elements into ordered\nones. Our experiments demonstrate the significant advantage of\nusing NN-sort. Measurements on both synthetic and real-world\ndatasets show that NN-sort yields 2.18 ×to 10×performance\nimprovement over traditional sorting algorithms.\nI. I NTRODUCTION\nSorting is a fundamental operation in the realm of big data,\npowering everything from database systems [1] to bigdata\nanalysis [2]. As the scale of data continues to grow, traditional\nsorting algorithms face increasing limitations in performance.\nWhile methods such as Quick Sort family [3], Merge Sort\nfamily [4], and Radix Sort family [5], [6] have long been re-\nlied upon, their comparison-based and non-comparison-based\noptimizations appear to be reaching its bottleneck.\nRecent research [7]–[10] has extensively explored how deep\nlearning models can enhance the performance of traditional\nbig data systems and algorithms. For instance, Tim Kraska et\nal. introduced a learned index [9] that leverages an empirical\ncumulative distribution function (CDF) to improve the per-\nformance of traditional data structures. They also proposed a\nlearned approach [10], [11] to improve sorting performance.\nTheir method, known as SageDB Sort , utilizes a learned\nmodel to generate a roughly ordered state of elements by\npredicting (mapping) the positions of elements, and then\nrefined it by a traditional sorting algorithm like Quick Sort.\nHowever, this approach has limitations. Conflicts often arise\nwhen converting the learned model’s output, such as multiple\nelements being mapped to the same position, leading to\nperformance bottlenecks (see Section IV). Resolving these\nconflicts (especially when numerous) can be time-consuming,\nmaking it less efficient than traditional algorithms.\nThe question of how to design an effective deep learning-\nbased sorting algorithm remains unanswered. Specifically, key\nissues include determining which type of neural network per-\n†work done while author was with Yunnan University\n*corresponding authorforms best for sorting, understanding the complexity of neural\nnetwork-based sorting, dealing with conflicts, and minimizing\noperations such as data comparison and movement during the\nsorting process.\nTo address these issues, this work presents NN-sort, a neural\nnetwork-based sorting algorithm designed to move beyond\ntraditional sorting paradigms. Instead of relying solely on com-\nparisons or inherent data characteristics, NN-sort harnesses\nthe power of neural networks to create a data distribution-\naware sorting method. By training a model on historical data\nto predict the sorted positions of new data, NN-sort offers\na novel approach that achieves efficient and scalable sorting\nwhile incorporating an effective conflict-handling mechanism.\nThe architecture of NN-sortconsists of three distinct phases:\nthe input phase, where data is transformed into neural network-\ncompatible vectors; the sorting phase, where a learned neural\nnetwork iteratively refines the data order; and the polish phase,\nwhere traditional methods finalize the sorting ( i.e.,ensuring the\noutput is correct). This layered approach enables NN-sort to\nhandle large datasets efficiently, minimizing the computational\noverhead from conflicts—a primary performance bottleneck\ninSageDB Sort . Moreover, we systematically explored the\npotential of NN-sort, discussed its complexity, performance,\nand advantages over traditional algorithms. Through rigorous\nexperiments on both synthetic and real-world datasets, we\njustify the effective of NN-sort.\nThe contributions of this paper are summarized as follows:\n(a) we investigate the opportunities and challenges of enhanc-\ning traditional sorting processes by leveraging neural network-\nbased learning approaches; (b) we develop NN-sort, a novel\nneural network-based sorting method that utilizes historical\ndata to train a data distribution-aware model. This trained\nmodel performs high-performance sorting on incoming data\niteratively, with an additional touch-up process to ensure the\ncorrectness of the final result. In contrast to state-of-the-art\nlearned sorting methods, e.g.,SageDB Sort ,NN-sort scales\neffectively by reducing conflicts during CDF mapping; (c)\nwe provide a formal analysis of NN-sort’s complexity using\na cost model that clarifies the intrinsic relationship between\nmodel accuracy and sorting performance. (d) we evaluate NN-\nsort’s performance on both synthetic and real-world datasets.\nExperimental results demonstrate that NN-sort achieves up to\nan order of magnitude speed-up in sorting time compared toarXiv:1907.08817v4  [cs.DS]  13 Dec 2024\n\n9.4…0.5……\n…………\n10\n15.2……\n…\n…\n 0.5\n……\n2.3…1\n…………\n8\n13.3……\n…\n…\n 16.8\n……\nInput Phase\nf\nw\nf\n1O\n…\nf\ntO\n…\n…\nquick \nsort\nSorting  Phase Polish Phase\ntO1ORoughly\nordered\n(ordered) (unordered)\n…\n…\n Records Round(row_pos)t 2\nww\nwRoughly \nordered2ORoughly\nordered1O\n2O\ntOOutput: A’ \n(ordered)\n2O\nPolish\nInput  data  elements  \n（A[0],  A[1], …, A[n‐ 1]）\nValue\nOut put data  elements  \n（A’[0], A’[1], …, A’[n‐1]）\nValue1\nFig. 1: NN-sort architecture\nstate-of-the-art sorting algorithms.\nThe rest of the paper is organized as follows: the design of\nNN-sortis presented in Section II. The complexity of NN-sort\nis discussed in Section III. The experimental results are pre-\nsented and analyzed in Section IV. Related work is reviewed\nin Section V, and the paper is concluded in Section VI.\nII. N EURAL NETWORK BASED SORT\nIn this section, we discuss the design of NN-sort, including\nhow to use a neural network model for effective sorting, as\nwell as how such a neural network model can be trained.\nChallenges. Sorting involves mapping elements from an un-\nsorted state to a sorted state. Rather than relying on traditional\ncomparison-based methods ( e.g., Quick Sort), this mapping\ncan be achieved through a data distribution-aware model that\ntakes each element as input and returns its expected position\nwithin the sorted array. Given an ideal function fand distinct\nelements x, y∈Asuch that x < y , the function would ideally\nsatisfy f(x)< f(y). While such a ”magic” function may\nnot always hold perfectly, it can still accelerate sorting, with\ngreater accuracy leading to more effective acceleration. Before\ndiscussing the methods, several challenges must be addressed\nto ensure both effectiveness and accuracy. (1) first, for\ncorrectness, the model must precisely reflect the order among\ninput data elements, producing results consistent with those\nof traditional sorting algorithms. However, it is impossible to\nhave that function f, especially if we train the model just based\non samples from the input data; (2) second, for effectiveness,\nthe model ideally should sort large volumes of data in a single\npass. Achieving this requires a model to be both complex and\naccurate enough to capture the exact order of all elements,\nwhich can result in significant training cost and inference\ncost. Thus, a balance between model accuracy and sorting\nperformance is crucial. (3) third, conflicts arise when multiple\ninput elements are mapped to the same output position, i.e.,\nf(a) =f(b)where a, b∈Aanda̸=b. Effectively managing\nthese conflicts is crucial for both the correctness and efficiency\nof the learned sorting approach. SageDB Sort addresses these\ncollisions using traditional sorting algorithms, such as Quick\nSort, which incurs computational overhead when collisions isProcedure NN-sort\nInput: A: the array of data elements\nf: the learned model\nm: the relaxation factor\nϵthe predefined iteration limit\nτthe predefined threshold\nOutput: A′: the array of data elements after sorted\n1.w←translate( A)\n2.init O← ∅,i←0\n3.while 0< i < ϵ and count( w) > τdo\n4. init oi← ∅,c← ∅\n5. w_pos←w.map (f)\n6. foridxin count( w)do\n7. pos←round( w_pos[idx]∗m)\n8. ifoi[pos] is empty do\n9. oi[pos]←w[idx]\n10. else\n11. c.append( w[idx])\n12. O.append( oi)\n13. w←c\n14. ++i\n15.QuickSort( w)\n16.A′←polish( O, w)\n17.Return A′\nFig. 2: Algorithm NN-sort\ntoo large. That is to said, there is still room for improvement.\nNN-sort Design. In response to these challenges, we designed\nan iterative neural network-based sorting method. Unlike\nSageDB Sort , which employs a complex model to sort data in\na single round, our approach utilizes a simpler model to sort\nover multiple rounds. Each round generates a roughly sorted\narray, with conflicts carried forward to the next iteration. This\nprocess continues until the conflicts in that iteration fall below\na predefined threshold or a number of iterations is reached.\nThe final small conflict array is then sorted using a traditional\nmethod like Quick Sort and merged with the roughly sorted\narrays. After a final traversal to ensure correctness, a fully\n2\n\nProcedure polish( O)\nInput: O={o1, o2, ...}: array of roughly sorted arrays.\nw: strictly sorted array.\nOutput: A′: the array of data elements after sorted\n1.A′←w\n2.foroi∈Odo\n3. A′←InsertSort( A′,oi)\n4.Return A′\nFig. 3: Algorithm polish\nsorted result is obtained. The benefits are two folds: (1) using\na simpler model reduces both inference and training costs; (2)\nthe learned model can be applied repeatedly, avoiding direct\nsorting of conflicting elements with traditional methods.\nFigure 1 illustrates this approach, where the input array A\nis sorted into A′. The process is divided into three phases:\nInput ,Sorting , and Polish . Figure 2 details the workflow of\nNN-sort, with Line 1 addressing the input phase, Lines 2-15\ncovering the sorting phase, and Line 16 corresponding to the\npolish phase.\nInput Phase. The input phase prepares the data for the neural\nnetwork by encoding it appropriately, ensuring compatibility\nfor processing. For example, string-type data is converted into\nASCII values. This encoding step is crucial, as it standardizes\nthe data format and enables the neural network to interpret\nand process a wide variety of input types, such as integers,\nfloating-point numbers, or categorical data, in a structured and\nefficient manner. For simplicity, we denoted such operations\nasw←translate (A)(line 1, Figure 2).\nSorting Phase. In the sorting phase, a learned model fit-\neratively organizes unordered data into approximately sorted\narrays. First, the learned model fmaps each element to its\nexpected position (line 5, Figure 2). Then, oistores elements of\nwbased on their value in wpos, where idenotes the iteration\nnumber. If a collision occurs in oi—where multiple elements\nmap to the same position—only the first element is stored\ninoi, while subsequent elements are placed in a temporary\nconflict array c(line 8-11, Figure 2). In the following iteration,\nelements in care reprocessed by learned model f(line 13,\nFigure 3). This process continues until either a predefined\nmaximum number of iterations ϵis reached or the size of\ncdrops below a threshold τ, at which point it is sorted using\na traditional algorithm.\nIt is worth mentioning that, each element in wpos rep-\nresents the expected position of corresponding elements of\nwwithin the final sorted array. Instead of using wpos[idx],\nwhich is the direct output of f, we use round (wpos[idx]∗m),\na rounded value, to represent the position of w[idx]. The\nreasons are two folds: (1) the outputs of fare decimals\nwhile the positions need to be integers. (2) with relaxation\nfactor mthe input data elements can be mapped into a larger\nspace, thereby reducing the number of conflicts. In addition,\nall conflicting data elements are stored in a conflict array cand\n1 6   31   38 60 81 88 37 92 3 91 32 337 9132 59 1 3 6 31 32 37 38 59 60 8188919232 31 6 60 38 37 3 59 88 92911 81 <latexit sha1_base64=\"1b5EDj64PlsE9fy0UUpd52hcSO0=\">AAAB6nicbZC5TgMxEIbHnCFc4ehoLBIkqmiXItARiQLKIMghJavI63gTK17vyvYihVUegYYChGipqXgSOkreBOcoIOGXLH36/xl5ZvxYcG0c5wstLC4tr6xm1rLrG5tb27md3ZqOEkVZlUYiUg2faCa4ZFXDjWCNWDES+oLV/f7FKK/fMaV5JG/NIGZeSLqSB5wSY62bQlBo5/JO0RkLz4M7hfz5x/335ft+WmnnPludiCYhk4YKonXTdWLjpUQZTgUbZluJZjGhfdJlTYuShEx76XjUIT6yTgcHkbJPGjx2f3ekJNR6EPq2MiSmp2ezkflf1kxMcOalXMaJYZJOPgoSgU2ER3vjDleMGjGwQKjidlZMe0QRaux1svYI7uzK81A7KbqlYunazZcdmCgDB3AIx+DCKZThCipQBQpdeIAneEYCPaIX9DopXUDTnj34I/T2A6TxkQ4=</latexit>f<latexit sha1_base64=\"1b5EDj64PlsE9fy0UUpd52hcSO0=\">AAAB6nicbZC5TgMxEIbHnCFc4ehoLBIkqmiXItARiQLKIMghJavI63gTK17vyvYihVUegYYChGipqXgSOkreBOcoIOGXLH36/xl5ZvxYcG0c5wstLC4tr6xm1rLrG5tb27md3ZqOEkVZlUYiUg2faCa4ZFXDjWCNWDES+oLV/f7FKK/fMaV5JG/NIGZeSLqSB5wSY62bQlBo5/JO0RkLz4M7hfz5x/335ft+WmnnPludiCYhk4YKonXTdWLjpUQZTgUbZluJZjGhfdJlTYuShEx76XjUIT6yTgcHkbJPGjx2f3ekJNR6EPq2MiSmp2ezkflf1kxMcOalXMaJYZJOPgoSgU2ER3vjDleMGjGwQKjidlZMe0QRaux1svYI7uzK81A7KbqlYunazZcdmCgDB3AIx+DCKZThCipQBQpdeIAneEYCPaIX9DopXUDTnj34I/T2A6TxkQ4=</latexit>fStep 1. Mapping elements bylearned modelStep 2. Re‐mapping elements by learned mode <latexit sha1_base64=\"1b5EDj64PlsE9fy0UUpd52hcSO0=\">AAAB6nicbZC5TgMxEIbHnCFc4ehoLBIkqmiXItARiQLKIMghJavI63gTK17vyvYihVUegYYChGipqXgSOkreBOcoIOGXLH36/xl5ZvxYcG0c5wstLC4tr6xm1rLrG5tb27md3ZqOEkVZlUYiUg2faCa4ZFXDjWCNWDES+oLV/f7FKK/fMaV5JG/NIGZeSLqSB5wSY62bQlBo5/JO0RkLz4M7hfz5x/335ft+WmnnPludiCYhk4YKonXTdWLjpUQZTgUbZluJZjGhfdJlTYuShEx76XjUIT6yTgcHkbJPGjx2f3ekJNR6EPq2MiSmp2ezkflf1kxMcOalXMaJYZJOPgoSgU2ER3vjDleMGjGwQKjidlZMe0QRaux1svYI7uzK81A7KbqlYunazZcdmCgDB3AIx+DCKZThCipQBQpdeIAneEYCPaIX9DopXUDTnj34I/T2A6TxkQ4=</latexit>f<latexit sha1_base64=\"1b5EDj64PlsE9fy0UUpd52hcSO0=\">AAAB6nicbZC5TgMxEIbHnCFc4ehoLBIkqmiXItARiQLKIMghJavI63gTK17vyvYihVUegYYChGipqXgSOkreBOcoIOGXLH36/xl5ZvxYcG0c5wstLC4tr6xm1rLrG5tb27md3ZqOEkVZlUYiUg2faCa4ZFXDjWCNWDES+oLV/f7FKK/fMaV5JG/NIGZeSLqSB5wSY62bQlBo5/JO0RkLz4M7hfz5x/335ft+WmnnPludiCYhk4YKonXTdWLjpUQZTgUbZluJZjGhfdJlTYuShEx76XjUIT6yTgcHkbJPGjx2f3ekJNR6EPq2MiSmp2ezkflf1kxMcOalXMaJYZJOPgoSgU2ER3vjDleMGjGwQKjidlZMe0QRaux1svYI7uzK81A7KbqlYunazZcdmCgDB3AIx+DCKZThCipQBQpdeIAneEYCPaIX9DopXUDTnj34I/T2A6TxkQ4=</latexit>fStep 3. QuickSortStep 4. Merge & Polishing59 92 Fig. 4: Example\nused as input to ffor the next iteration. If the model fdoes not\nperform effectively, i.e.,the conflicting array may never shrink\nbelow τor may decrease too slowly, potentially resulting in\nhigher overhead than traditional sorting algorithms. To prevent\nthis, a threshold ϵlimits the maximum number of iterations.\nAs we will show in the experimental section, ϵ= 2 orϵ= 3\nare good enough for accelerating sorting. There is a clearly\ndecreased edge effect on the number of iterations.\nPolish Phase. The polish phase refines the roughly sorted\narrays O={o1, o2, ...}to ensure the correctness of the output.\nFigure 3 outlines this process, where the arrays in Oare\npolished and merged with the strictly ordered array A′. The\nalgorithm iterates over each array in O, merging them with\nA′one by one. Elements in oiare either appended or inserted\ninto the result, depending on their order relative to A′(i.e.,\nInsert Sort).\nRemark. Since oiis only roughly ordered, out-of-order ele-\nments are inserted into their correct positions in A′, ensuring\nNN-sort’s reliability despite potential errors from the learned\nmodel. Though insertion is costlier than appending, it is lim-\nited to out-of-order elements. As model accuracy improves, the\npolish phase incurs acceptable overhead. Section III discusses\nNN-sort’s complexity, with experimental results showing few\nout-of-order elements, yielding nearly linear performance.\nExample 1: Figure 4 illustrates how NN-sort sorting.\nGiven thresholds τ= 2,ϵ= 2 and an unordered array\nA={32,60,31,1,81,6,88,38,3,59,37,92,91},NN-sort\nfirst checks if A’s size is below τ; if so, a traditional sorting\nmethod is applied. Otherwise, learned sorting begins.\nHere, NN-sort processes Ain two rounds with a learned\nmodel. A conflict arises between elements 37 and 38, as f\nmaps them to the same position, placing 37 in a conflict array\nc. At the end of the first iteration, elements 92, 3, 91, 32, and\n59 are also in the conflict array.\nAfter the first iteration, since w’s size exceed τand the itera-\ntion count is below ϵ, all elements in c={37,92,3,91,32,59}\nare reprocessed by fin a second iteration, yielding a new\nsorted array o2={3,37,91,92}and a smaller conflict array\nc={32,59}. Then a traditional sorting algorithm ( e.g., Quick\nSort) is applied to c, and finally, o1, o2, and sorted care merged\nin the polish phase, producing a fully ordered result.\n2\n3\n\nTABLE I: Notations\nsymbols notations\nn the amount of data elements to be sorted\nσi collision rate per iteration\neithe number of data elements that were\nout-of-order in the i-th iteration\nϵ the predefined limit of iterations\nt the number of completed iterations\nθ The operations required for data to pass through f\nTraining. While training time is not the focus, all our mod-\nels—whether shallow neural networks or simple linear/mul-\ntivariate regression models—train quickly and perform well,\nas perfect position mapping (i.e., no conflicts or out-of-order\nelements) is unnecessary. The training and test data can differ;\nany learned order relationships help the model understand the\nsorting task.\nIII. M ODEL ANALYSIS\nThis section establishes the time complexity of NN-sort by\nanalyzing key operations—moving, mapping, and comparing\ndata elements. A cost model is introduced to highlight relation-\nships among factors like conflict rate, model scale, iteration\ncount, out-of-order rate, data volume, and required operations.\nThe total operations of NN-sort is expressed as a\nT(n, e, σ, t, θ ), where: nis the number of data elements to\nbe sorted, e={e1, ..., e t}is the set of probabilities, with ei\ndenoting the proportion of out-of-order elements in the i-th\niteration, σ={σ1, ..., σ t}is the set of conflict rates, where σi\nrepresents the conflict rate in the i-th iteration, tis the number\nof iterations completed, θdenotes the number of operations\nrequired for each data element to pass through the neural\nnetwork. These basic notations are summarized in Table I.\nAs shown in Eq 1, the number of operations for NN-sortto\nsortn(n >1) data elements is C1n2+C2nlogn +C3n.\nT(n, e, σ, t, θ ) =\u001a\n1, if n = 1\nC1n2+C2nlogn +C3n, if n > 1\n(1)\nC1= [1\n2tX\ni=1ei(1−σi)(i−1Y\nj=1σj)2]\nC2=tY\nj=1σj\nC3=tX\ni=1[θiX\nj=1σj+ (1−ei)(1−αi)i−1Y\nj=1σj+iY\nj=1σj]\n+ (tY\nj=1σj)log(tY\nj=1σj)\nInNN-sort, the majority of the cost is spent in the Sorting\nand Polish phases. Let s(n)represent the time spent in the\nSorting phase and p(n)represent the time spent in the Polishphase, we now formally analyze the complexity of NN-sort.\ns(n)consists of two kinds of operations: iteratively feed-\ning the data elements into a learned model fand sorting\nthe array wat the last iterations using traditional sorting\nalgorithms ( e.g., QuickSort), the time complexity of which\nisnlogn . Ifn >1, then θPi\nj=1σjnoperations are required\nto feed data into model fin the i-th iteration. An additional\n(Qt\nj=1σj)nlog(Qt\nj=1σj)noperations are required to keep w\norder, since the size of conflicting array wupdated in the last\niteration is (Qt\nj=1σj)n. Therefore, at the end of the algorithm,\nthe total operations of s(n)is(Qt\nj=1σj)nlog(Qt\nj=1σj)n+\nθPt\ni=1Pi\nj=1σjn.\nT(n) =s(n) +p(n),(n >1)\n=(tY\nj=1σj)nlog(tY\nj=1σj)n+θtX\ni=1iX\nj=1σjn+p(n)\n=(tY\nj=1σj)nlog(tY\nj=1σj)n+θtX\ni=1iX\nj=1σjn\n+tX\ni=1[ei(1−σi)i−1Y\nj=1σjn×Qi−1\nj=1σjn\n2\n+ (1−ei)(1−σi)i−1Y\nj=1σjn+iY\nj=1σjn]\n=[1\n2tX\ni=1ei(1−σi)(i−1Y\nj=1σj)2]n2+tY\nj=1σjnlogn\n+{tX\ni=1[θiX\nn=1σj+ (1−ei)(1−αi)i−1Y\nj=1σj\n+iY\nj=1σj] + (tY\nj=1σj)log(tY\nj=1σj)}n\np(n)involves two tasks: correcting any out-of-order ele-\nments and merging the intermediate arrays ( i.e.,o1, ..., o tand\nw). If no elements are out of order in oi,NN-sort only needs\nto traverse the data once to merge them. However, in practice,\nout-of-order elements are almost inevitable, as the model fis\nunlikely to be 100% accurate.\nFor the ordered elements in oi,NN-sort only requires\nappending it, with a time complexity of time complexity of\nO(1). Therefore, in the i-th iteration, at mostQi−1\nj=1σjnoper-\nations are required to complete the insertion, and at least one\noperation is needed to insert out-of-order elements. While, for\nan out-of-order element in the i-th merge iteration,Qi−1\nj=1σjn\n2\noperations are required to insert it into the final ordered result.\nTheoretically, assume that there are ei(1−σi)Qi−1\nj=1σjnout-\nof-order elements in the i-th iteration. It takes a total of\nei(1−σi)Qi−1\nj=1σjn×Qi−1\nj=1σjn\n2operations to process these\nelements. Correspondingly, (1−ei)(1−σi)Qi−1\nj=1σjnelements\ninoiandQi\nj=1σjnelements in wremain ordered. Thus in\nthei-th merge iteration, a total ofQi\nj=1σjn+ (1−ei)(1−\n4\n\nσi)Qi−1\nj=1σjnoperations are required to append the ordered\nelements to the final result. Overall, NN-sort requires a total\nofPt\ni=1[ei(1−σi)Qi−1\nj=1σjn×Qi−1\nj=1σjn\n2+ (1−ei)(1−\nσi)Qi−1\nj=1σjn+Qi\nj=1σjn]to sort ndata elements (We show\nthe detail in Equation 1).\nIV. E XPERIMENTAL STUDY\nUsing real and synthetic data, we conducted five experi-\nments to evaluate (1) overall sorting performance, (2) iteration\nimpact, and (3) effects of changing data distribution.\nA. Experimental setup\nDatasets. The datasets used in this section are generated from\nthe most commonly observed distributions in the real world,\nsuch as uniform distribution, normal distribution, and log-\nnormal distribution. The models used in the experiments are\ntrained over a subset of the testing data. The sizes of the testing\ndataset vary from 200MB to 500MB and each data element\nis 64 bits wide floating number. To verify the performance\nof the NN-sort under the real-world dataset. We use the\nQuickDraw game dataset from Google Creative Lab [12],\nwhich consists of 50,426,265 records of schema {’key-id’,\n’word’, ’country code’, ’timestamp’, ’recognized’, ’drawing’ }.\nSorting is perform on ’key-id’.\nBaselines. We compared with five baselines (1) Quick\nSort [13]: This algorithm divides the input dataset into two\nindependent partitions, such that all the data elements in the\nfirst partition are smaller than those in the second partition.\nThen, the dataset in each partition is sorted recursively. The\ntime complexity of Quick Sort can achieve O(nlogn )in the\nbest case while O(n2)in the worst case. (2) std::sort [14]:\nstd::sort is one of the most widely used sorting algorithms from\nc++ standard library, and its time complexity is O(nlogn )(3)\nstd::heap sort [14]: std::heap sort is another sorting algo-\nrithm from c++ standard library, and it guarantees to perform\natO(nlogn )time complexity. (4) Redis Sort [15]: Redis Sort\nis a sorting method based on a data structure named sortSet .\nTo sort Mdata elements in a sortSet of size N, the efficiency\nofRedis Sort isO(N+Mlog (M)). In addition, we also\ncompared NN-sort with (5) SageDB Sort [10], [11], leading\nperformance DNN-based sorting method.The relaxation factor\nmis set to 1.25 for learned sorting methods to reduce conflicts.\nMeasurements. We used sorting time and sorting rate of\nEquation 2 to evaluate the overall performance.\nsorting rate =elements\ntime to finish sorting(2)\nWe also used traditional sorting rate to evaluate learned-based\nsorting methods which is described in Equation 3. This rate\nindicates how many data elements still require traditional sort-\ning due to model inaccuracy in the learning-based approach.\nIdeally, a lower traditional sorting rate signifies the better\nperformance of learning-based sorting.TABLE II: Evaluation under real-world data\nAlgorithm\nnameTime (sec.)Sorting Rate\n(elements/sec.)The traditional\nsorting rate\n(%)\nQuick Sort 10.86 4666 .14 -\nstd::heap sort 13.46 3746 .44 -\nstd::sort 23.71 2127 .19 -\nRedis Sort 63.14 798 .6320 -\nSageDB Sort 10.53 4790 .125 9 .16\nNN-sort 8.47 5950 .186 0 .4\nTraditional sorting rate =size(last conflicting array w)\nsize of the original array A(3)\nEnvironment. Experiments were conducted on a machine with\n64GB RAM, a 2.6GHz Intel i7 processor, and a GTX1080Ti\nGPU with 16GB memory, running RedHat Enterprise Server\n6.3. Each result reported is the median of ten runs.\nTraining details. We employed a regression model with three\nhidden layers, containing 2, 6, and 1 neurons, respectively. A\nrounding function is used to determine each element’s final\nposition. Adam [16] was the chosen optimizer. The training\nwas conducted using a GPU and is performed offline, so\ntraining time is excluded from the runtime.\nlossδ=\u001a1\n2(f(xi)−label i)2, if |f(xi)−label i| ≤δ,\nδ|f(xi)−label i| −1\n2δ2, otherwise\n(4)\nTo avoid the impact of outliers during training, the model\nused in experiments is trained according to the Huber loss [17]\nas shown in Equation 4. The batch size for training is set to\n128. For all environments, we use the Adam optimizer with a\nlearning rate of 0.001.\nB. Experimental results.\nExp-1: Overall Sorting Performance. Figure 5 presents\na performance comparison of NN-sort against traditional\nsorting algorithms across various datasets with increasing\nsizes. Figures 5(a)–(c) show the total sorting time, while\nFigures 5(d)–(f) illustrate the sorting rates. Figures 5(g)–(i)\nhighlight the traditional sorting rate comparison between NN-\nsort and SageDB sort, as defined in Equation 3. we observe\nthe following:\nNN-sortexhibits notable advantages over traditional sorting\nalgorithms. As shown in Figure 5 (d), its sort rate for a\nlognormal distribution dataset reaches nearly 8,300 elements\nper second, outperforming std::heap sort by 2.8 ×, Redis Sort\nby 10.9 ×, std::sort by 4.78 ×, and Quick Sort by 218%.\nIt also exceeds SageDB Sort by 15%. The dataset’s value\nrange—defined by its maximum and minimum values—affects\nNN-sort ’s performance. As shown in Figure 5 (h), a slight\ndecline in sorting rate occurs with highly concentrated values,\nwhich create more conflicts and reduce efficiency. In contrast,\nfewer records within the same range enhance sorting perfor-\n5\n\n/uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000017/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni00000048/uni00000046/uni00000011/uni0000000c\n/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000057/uni00000052/uni00000003/uni00000049/uni0000004c/uni00000051/uni0000004c/uni00000056/uni0000004b/uni00000003/uni00000056/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000051/uni0000004a/uni0000001d/uni00000003/uni0000004f/uni00000052/uni0000004a/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f\n/uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000017/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni00000048/uni00000046/uni00000011/uni0000000c\n/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000057/uni00000052/uni00000003/uni00000049/uni0000004c/uni00000051/uni0000004c/uni00000056/uni0000004b/uni00000003/uni00000056/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000051/uni0000004a/uni0000001d/uni00000003/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f\n/uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000017/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni00000048/uni00000046/uni00000011/uni0000000c\n/uni0000000b/uni00000046/uni0000000c/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000057/uni00000052/uni00000003/uni00000049/uni0000004c/uni00000051/uni0000004c/uni00000056/uni0000004b/uni00000003/uni00000056/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000051/uni0000004a/uni0000001d/uni00000003/uni00000058/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050\n/uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000017/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni00000046/uni00000052/uni00000055/uni00000047/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000050/uni00000056/uni00000011\n/uni0000000b/uni00000047/uni0000000c/uni00000003/uni00000036/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni0000001d/uni00000003/uni0000004f/uni00000052/uni0000004a/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f\n/uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000017/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni00000046/uni00000052/uni00000055/uni00000047/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000050/uni00000056/uni00000011\n/uni0000000b/uni00000048/uni0000000c/uni00000003/uni00000036/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni0000001d/uni00000003/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f\n/uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000017/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni00000046/uni00000052/uni00000055/uni00000047/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000050/uni00000056/uni00000011\n/uni0000000b/uni00000049/uni0000000c/uni00000003/uni00000036/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni0000001d/uni00000003/uni00000058/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050\n/uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000017/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000035/uni00000044/uni00000057/uni00000048\n/uni0000000b/uni0000004a/uni0000000c/uni00000003/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000004c/uni00000046/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni0000001d/uni00000003/uni0000004f/uni00000052/uni0000004a/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f\n/uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000017/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000035/uni00000044/uni00000057/uni00000048\n/uni0000000b/uni0000004b/uni0000000c/uni00000003/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000004c/uni00000046/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni0000001d/uni00000003/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f\n/uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000017/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000013\n/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000030/uni00000025/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000016/uni00000013/uni00000035/uni00000044/uni00000057/uni00000048\n/uni0000000b/uni0000004c/uni0000000c/uni00000003/uni00000026/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000004c/uni00000046/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni0000001d/uni00000003/uni00000058/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000031/uni00000031/uni00000010/uni00000056/uni00000052/uni00000055/uni00000057 /uni00000054/uni00000058/uni0000004c/uni00000046/uni0000004e/uni00000003/uni00000056/uni00000052/uni00000055/uni00000057 /uni00000056/uni00000057/uni00000047/uni0000001d/uni0000001d/uni0000004b/uni00000048/uni00000044/uni00000053/uni00000003/uni00000056/uni00000052/uni00000055/uni00000057 /uni00000036/uni00000044/uni0000004a/uni00000048/uni00000027/uni00000025/uni00000003/uni00000056/uni00000052/uni00000055/uni00000057 /uni00000056/uni00000057/uni00000047/uni0000001d/uni0000001d/uni00000056/uni00000052/uni00000055/uni00000057 /uni00000035/uni00000048/uni00000047/uni0000004c/uni00000056/uni00000003/uni00000056/uni00000052/uni00000055/uni00000057Fig. 5: Overall performance evaluation\nmance.\nNN-sort achieves optimal performance with uniformly dis-\ntributed data, reaching a sorting rate of approximately 8,000\nrecords per second—about 1.3 ×higher than with a normal\ndistribution—due to fewer conflicts in uniformly distributed\nrecords.\nCompared to SageDB Sort, NN-sort consistently reduces\nreliance on traditional sorting. A larger proportion of elements\nare accurately sorted by NN-sort’s neural model, minimizing\nthe need for the more time-consuming traditional sorting and\ncontributing to NN-sort’s superior performance over SageDB\nSort.\nExp-2: Evaluation on real-word Dataset. Using the model\ntrained in previous sections on uniformly distributed data, we\nevaluated NN-sort ’s performance on the real-world dataset\nQuickDraw. As shown in Table II, NN-sortdelivers significant\nperformance gains over traditional sorting algorithms on real-\nworld data. With a sorting rate of 5,950 elements per second,\nNN-sort outperforms std::sort by 2.72 ×and Redis Sort by\n/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013\n/uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni0000004c/uni00000046/uni00000048/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000015/uni00000013/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000016/uni00000018/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000017/uni00000018/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000050/uni00000056/uni00000011/uni0000000c\n/uni00000031/uni00000031/uni00000010/uni00000056/uni00000052/uni00000055/uni00000057 /uni00000028/uni0000004f/uni00000044/uni00000053/uni00000056/uni00000048/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000057/uni00000047/uni0000001d/uni0000001d/uni00000056/uni00000052/uni00000055/uni00000057Fig. 6: The impact of data distribution on NN-sort\nperformance\n7.34×, and is also 58% faster than std::heap sort. Additionally,\nNN-sortsurpasses SageDB Sort in both traditional sorting rate\nand overall sorting rate.\nExp-3: Impact of the Changing Data Distribution. As\n6\n\n/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018\n/uni00000037/uni0000004b/uni00000048/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni00000014/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000013/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000050/uni00000056/uni00000011/uni0000000c\n/uni0000000b/uni00000044/uni0000000c/uni00000003/uni0000004f/uni00000052/uni0000004a/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f\n/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018\n/uni00000037/uni0000004b/uni00000048/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni00000014/uni00000015/uni00000013/uni00000013/uni00000013/uni00000014/uni00000016/uni00000013/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013/uni00000013/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000050/uni00000056/uni00000011/uni0000000c\n/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f\n/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018\n/uni00000037/uni0000004b/uni00000048/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000001b/uni00000013/uni00000013/uni00000013/uni0000001c/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000014/uni00000013/uni00000013/uni00000013/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000050/uni00000056/uni00000011/uni0000000c\n/uni0000000b/uni00000046/uni0000000c/uni00000003/uni00000058/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050\n/uni00000013/uni00000014/uni00000015\n/uni00000044/uni00000055/uni00000055/uni00000044/uni0000005c/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000014/uni00000048/uni0000001a\n/uni00000013/uni00000014/uni00000015/uni00000016\n/uni00000044/uni00000055/uni00000055/uni00000044/uni0000005c/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000014/uni00000048/uni0000001a\n/uni00000013/uni00000014/uni00000015\n/uni00000044/uni00000055/uni00000055/uni00000044/uni0000005c/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000014/uni00000048/uni0000001a\n/uni00000036/uni00000052/uni00000055/uni00000057/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048 /uni00000037/uni0000004b/uni00000048/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni0000004f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000052/uni00000059/uni00000048/uni00000055/uni00000049/uni0000004f/uni00000052/uni0000005a/uni00000003/uni00000044/uni00000055/uni00000055/uni00000044/uni0000005cFig. 7: Impact of Iterations\nshown in previous experiments, NN-sort performs optimally\nwhen the distribution of the sorting data resembles that of\nthe training data. But how does it perform when faced with a\ndifferent data distribution? To explore this, we trained a model\nusing a 100MB uniformly distributed dataset, then applied\nit to sort datasets with varying distributions. Specifically,\nthe test dataset combined uniformly and normally distributed\ndata, with the latter considered ”noisy.” Sorting time was\nmeasured to assess NN-sort ’s effectiveness, as shown in\nFigure 6. Results indicate that as noise in the dataset increases,\nNN-sort’s effectiveness declines due to a growing number\nof conflicts elements generated in each iteration when the\ntest data distribution diverges from the training data. These\nelements must then be handled by traditional algorithms during\nthe polish phase. Nonetheless, NN-sort shows resilience to\ndistributional changes, outperforming the widely-used std::sort\nalgorithm even with up to 45\nExp-4: Impact of Iterations. NN-sort’s sorting performance\nis influenced by both the size of the final conflicting array and\nthe number of iterations. Increasing the number of iterations\nreduces the size of the remaining conflicting array that requires\ntraditional sorting, yet also extends model processing time.\nConversely, fewer iterations leave a larger conflicting array,\nincreasing the time required for traditional sorting. In this set\nof experiments, we quantify these factors to guide users in\noptimizing NN-sort for improved performance.\nIn Figure 7, the rhombus-dotted line represents the size of\nthe final conflicting array, while the round-dotted line indicates\ntotal sorting time. Results show that while additional iterations\nreduce the size of the final conflicting array, they don’t\nnecessarily improve performance, as each iteration requires\nthe model to process all input data elements. Our experiments\nsuggest that 2-3 iterations provide an optimal balance between\nconflicting array size and sorting time.\nExp-5: Evaluation of sorting accuracy. A more complex\nneural network generally enhances model expressibility, result-\ning in lower conflict rates, and fewer out-of-order elements,\nbut higher inference times. Thus, understanding the impact\nof these factors on NN-sort’s overall time complexity is\nessential. In this section, we use a cost curve to illustrate how\nmodel quality—represented by the conflict rate σiand out-\n0.0x107\n0.2x107\n0.5x107\n0.8x107\n1.0x107\n1.2x107\n1.5x107\n1.8x107\n2.0x107\nThe number of data elements to be sorted (n)0.0x1081.0x1082.0x1083.0x1084.0x1085.0x108The number of operations to sort n elementsQuick Sort\nNN-sort in which 10% of the elements collide or are misordered\nNN-sort in which 5% of the elements collide or are misorderedFig. 8: Comparison of operations between traditional sorting\nalgorithm and NN-sort with different model qualities.\nof-order rate eiin each sorting iteration—affects NN-sort ’s\nperformance.\nFigure 8 compares the number of operations required by\nNN-sort to sort nelements against Quick Sort’s baseline\ncomplexity ( O(nlogn)). To illustrate performance variations,\nwe adjust NN-sort ’s model quality. This analysis assumes\nNN-sort performs up to five iterations ( t= 5), with a model\nscale θof 32 neurons, and equal conflict and out-of-order\nrates ( σi=ei) that remain constant across iterations. Results\nshow that NN-sortsubstantially outperforms Quick Sort when\nconflict and out-of-order rates are at 10%, with even greater\nperformance gains as these rates drop to 5\nIn summary, fewer conflicts and misordered elements result\nin more efficient sorting with NN-sort. A well-trained model\nwith a misorder rate of 10% or lower can outperform tradi-\ntional sorting algorithms in terms of computational efficiency.\nV. R ELATED WORK\nSorting is one of the most fundamental algorithms in\ncomputing. We identify two key research areas: methods to\nreduce sorting time complexity and neural network-based data\nstructures.\nMethods for reducing the sorting time complexity. Many\nresearchers have focused on accelerating sorting by reduc-\ning its time complexity. Traditional comparison-based sorting\nalgorithms like Quick Sort, Merge Sort, and Heap Sort re-\nquire at least logn!≈nlogn −1.44noperations to sort n\n7\n\nelements [18]. Among these, Quick Sort achieves O(nlogn )\naverage complexity but degrades to O(n2)in the worst case.\nMerge Sort, while guaranteeing a worst-case of nlogn−0.91n,\nrequires additional linear space relative to the number of\nelements [18]. To mitigate the drawbacks of these algorithms\nand further reduce sorting time, researchers have combined\ndifferent sorting techniques to leverage their strengths. Tim\nSort [19], the default sorting algorithm in Java and Python,\ncombines Merge Sort and Insertion Sort [13] to achieve fewer\nthan nlogn comparisons on partially sorted arrays. Stefan\nEdelkamp et al. proposed Quickx Sort [20], which uses at\nmost nlogn −0.8358n+O(logn)operations for in-place\nsorting. They also introduced a median-of-medians variant of\nQuick Merge Sort [18], which employs the median-of-medians\nalgorithm for pivot selection, reducing the operation count to\nnlogn + 1.59n+O(n0.8).\nRedis Sort [15] is a build-in sorting method of the Re-\ndis database based on the sortSet data structure. It sorts\nM elements in a sortSet of size N with an efficiency of\nO(N+Mlog (M)).\nUnlike previous work, this approach uses a learned model\ncomplexity to map an unordered array to a roughly ordered\nstate, reducing overall operations. In the worst case, NN-sort\nhas complexity O(n2)if all elements map to the same posi-\ntion, though practical operations remain lower than traditional\nsorting, This is validated by our experiment in Figure 8.\nLearned data structures and algorithms. This thread of\nresearch is to explore the potential of using the neural network-\nbased learned data structures to improve the performance\nof systems. Tim Kraska [9], [10] discussed the benefits of\nlearned data structures and suggested that R-tree can be\noptimized by learned data structures. Xiang et al. [8] proposed\nan LSTM-based inverted index structure. By learning the\nempirical distribution function, their learned inverted index\nstructure led to fewer average look-ups when compared with\ntraditional inverted index structures. Alex Galakatos et al.\n[21] presented a data-aware index structure called FITing-\nTree, which can approximate an index using piece-wise linear\nfunctions with a bounded error specified at construction time.\nMichael Mitzenmacher [22] proposed a learned sandwiching\nbloom filter structure, while the learned model is sensitive to\ndata distributions.\nUnlike the research mentioned above, our approach inte-\ngrates sorting with learning by training a model to enhance\nsorting performance. Additionally, we employ an iteration-\nbased mechanism to further optimize performance by mini-\nmizing conflicts. We also provide a formal analysis of the\ntime complexity of our approach and present a cost model to\nbalance model accuracy with sorting performance. A closely\nrelated work is SageDB Sort [11], [21], which leverages deep\nneural networks for sorting. Our approach improves upon\nSageDB Sort by offering a more efficient solution for handling\nposition conflicts generated by the learned model.VI. C ONCLUSION\nSorting is fundamental in big data processing. We introduce\nNN-sort, a neural network-based sorting method that uses\nhistorical data to sort new data, iteratively reducing sorting\nconflicts—a key bottleneck in learned sorting. Our analysis\nincludes complexity, a cost model, and the balance between\nmodel accuracy and performance. Experiments show NN-sort\noutperforms traditional algorithms. Future work includes en-\nhancing NN-sort’s adaptability to changing data distributions.\nREFERENCES\n[1] G. Graefe, “Implementing sorting in database systems,” ACM Comput.\nSurv. , vol. 38, no. 3, p. 10, 2006.\n[2] R. Hilker, C. Sickinger, C. N. Pedersen, and J. Stoye, “UniMoG—a\nunifying framework for genomic distance calculation and sorting based\non DCJ,” Bioinformatics , vol. 28, no. 19, pp. 2509–2511, 2012.\n[3] D. Cederman and P. Tsigas, “A practical quicksort algorithm for graphics\nprocessors,” in Algorithms - ESA 2008 , D. Halperin and K. Mehlhorn,\nEds., 2008, pp. 246–258.\n[4] A. Andersson, T. Hagerup, S. Nilsson, and R. Raman, “Sorting in linear\ntime?” Journal of Computer and System Sciences , vol. 57, no. 1, pp.\n74–93, 1998.\n[5] S. Bandyopadhyay and S. Sahni, “GRS - GPU radix sort for multifield\nrecords,” in HiPC , 2010, pp. 1–10.\n[6] J. Tang and X. Zhou, “Cardinality sorting and its bit-based operation-\nbased optimization (in chinese),” JOURNAL OF NANJINGUNIVERSITY\nOF TECHNOLOGY , vol. 20, 2006.\n[7] X. Zhu, Q. Zhang, T. Cheng, L. Liu, W. Zhou, and J. He, “Dlb: deep\nlearning based load balancing,” in CLOUD , 2021.\n[8] W. Xiang, H. Zhang, R. Cui, X. Chu, K. Li, and W. Zhou, “Pavo: A\nrnn-based learned inverted index, supervised or unsupervised?” IEEE\nAccess , vol. 7, pp. 293–303, 2019.\n[9] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in SIGMOD , 2018, pp. 489–504.\n[10] T. Kraska, M. Alizadeh, A. Beutel, E. H. Chi, A. Kristo, G. Leclerc,\nS. Madden, H. Mao, and V . Nathan, “Sagedb: A learned database\nsystem,” in CIDR , 2019.\n[11] J. Ding, R. Marcus, A. Kipf, V . Nathan, A. Nrusimha, K. Vaidya, A. van\nRenen, and T. Kraska, “Sagedb: An instance-optimized data analytics\nsystem,” PVLDB , vol. 15, no. 13, 2022.\n[12] Google, “Google creative lab,” Available: https://github.com/\ngooglecreativelab, google Creative Lab [Online].\n[13] T. H. Cormen, Introduction to Algorithms, 3rd Edition . Press.\n[14] “C++ resources network,” http://www.cplusplus.com/, general informa-\ntion about the C++ programming language, including non-technical\ndocuments and descriptions.\n[15] “Redis,” https://redis.io/, redis is an open source (BSD licensed), in-\nmemory data structure store, used as a database, cache and message\nbroker.\n[16] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\ninICLR , 2015.\n[17] P. J. Huber, “Robust estimation of a location parameter,” Annals of\nMathematical Statistics , vol. 35, no. 1, pp. 73–101, 1964.\n[18] S. Edelkamp and A. Weiß, “Worst-case efficient sorting with quick-\nmergesort,” in ALENEX , 2019, pp. 1–14.\n[19] “Python resources network,” https://www.python.org/, general informa-\ntion about the Python programming language, including non-technical\ndocuments and descriptions.\n[20] S. Edelkamp and A. Weiß, “Quickxsort: Efficient sorting with n logn\n- 1.399n + o(n) comparisons on average,” in International Computer\nScience Symposium in Russia , 2014, pp. 139–152.\n[21] A. Galakatos, M. Markovitch, C. Binnig, R. Fonseca, and T. Kraska,\n“Fiting-tree: A data-aware index structure,” in SIGMOD , 2019.\n[22] M. Mitzenmacher, “A model for learned bloom filters, and optimizing\nby sandwiching,” CoRR , vol. abs/1901.00902, 2019. [Online]. Available:\nhttp://arxiv.org/abs/1901.00902\n8",
  "textLength": 54850
}