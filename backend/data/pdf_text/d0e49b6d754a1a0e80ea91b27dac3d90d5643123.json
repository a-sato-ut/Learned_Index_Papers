{
  "paperId": "d0e49b6d754a1a0e80ea91b27dac3d90d5643123",
  "title": "Hypersparse Network Flow Analysis of Packets with GraphBLAS",
  "pdfPath": "d0e49b6d754a1a0e80ea91b27dac3d90d5643123.pdf",
  "text": "Hypersparse Network Flow Analysis of Packets\nwith GraphBLAS\nTyler Trigg1, Chad Meiners1, Sandeep Pisharody1, Hayden Jananthan1, Michael Jones1, Adam Michaleas1,\nTimothy Davis2, Erik Welch3, William Arcand1, David Bestor1, William Bergeron1,\nChansup Byun1, Vijay Gadepally1, Micheal Houle1, Matthew Hubbell1, Anna Klein1,\nPeter Michaleas1, Lauren Milechin1, Julie Mullen1, Andrew Prout1, Albert Reuther1,\nAntonio Rosa1, Siddharth Samsi1, Doug Stetson1, Charles Yee1, Jeremy Kepner1\n1MIT,2Texas A&M,3Nvidia\nAbstract —Internet analysis is a major challenge due to the\nvolume and rate of network trafﬁc. In lieu of analyzing trafﬁc\nas raw packets, network analysts often rely on compressed\nnetwork ﬂows (netﬂows) that contain the start time, stop time,\nsource, destination, and number of packets in each direc-\ntion. However, many trafﬁc analyses beneﬁt from temporal\naggregation of multiple simultaneous netﬂows, which can be\ncomputationally challenging. To alleviate this concern, a novel\nnetﬂow compression and resampling method has been developed\nleveraging GraphBLAS hyperspace trafﬁc matrices that preserve\nanonymization while enabling subrange analysis. Standard multi-\ntemporal spatial analyses are then performed on each subrange\nto generate detailed statistical aggregates of the source packets,\nsource fan-out, unique links, destination fan-in, and destination\npackets of each subrange which can then be used for background\nmodeling and anomaly detection. A simple ﬁle format based\non GraphBLAS sparse matrices is developed for storing these\nstatistical aggregates. This method is scale tested on the MIT\nSuperCloud using a 50 trillion packet netﬂow corpus from\nseveral hundred sites collected over several months. The resulting\ncompression achieved is signiﬁcant ( <0.1 bit per packet) enabling\nextremely large netﬂow analyses to be stored and transported.\nThe single node parallel performance is analyzed in terms of both\nprocessors and threads showing that a single node can perform\nhundreds of simultaneous analyses at over a million packets/sec\n(roughly equivalent to a 10 Gigabit link).\nIndex Terms —network analyses, compression, streaming\ngraphs, hypersparse matrices\nI. I NTRODUCTION\nInternet trafﬁc analysis is crucial for billing, provisioning,\nforecasting, and security reasons. While analyses of raw pack-\nets was attempted [1], [2], it was broadly accepted that such\nanalyses have inherent scalability problems that restrict their\ndeployment to lower speed links. Network Flow (netﬂow) [3],\n[4] is a compressed data format that strikes a balance between\nhigh-ﬁdelity data and scalability. By sampling network trafﬁc\nThis material is based upon work supported by the Assistant Secretary\nof Defense for Research and Engineering under Air Force Contract No.\nFA8702-15-D-0001, National Science Foundation CCF-1533644, and United\nStates Air Force Research Laboratory and Artiﬁcial Intelligence Accelerator\nCooperative Agreement Number FA8750-19-2-1000. Any opinions, ﬁndings,\nconclusions or recommendations expressed in this material are those of the\nauthor(s) and do not necessarily reﬂect the views of the Assistant Secretary of\nDefense for Research and Engineering, the National Science Foundation, or\nthe United States Air Force. The U.S. Government is authorized to reproduce\nand distribute reprints for Government purposes notwithstanding any copyright\nnotation herein.for a period of time, and aggregating measurements, netﬂows\nprovide a balance between scalability and ﬁdelity. Such net-\nﬂows usually contain: Input interface port, IP source address,\nIP destination address, Source port number, Destination port\nnumber, Layer 3 protocol ﬁeld, and Type of service along\nwith the start and end times of the sampling window. While\nnetﬂow-based trafﬁc analysis captures the needs of billing,\nprovisioning, and forecasting communities, it falls short of\nsatisfying all the needs the Internet security community.\nNetwork analysis has emerged as important application area\nfor the protection and improvement of the Internet. These\nanalyses require a signiﬁcant amount of network trafﬁc from a\nvariety of observatories and outposts [5], [6]. Historically, the\ndata volumes, processing requirements, and privacy concerns\nof analyzing a signiﬁcant fraction of the Internet have been\nprohibitive. The North American Internet generates billions of\nnon-video Internet packets each second [7], [8]. Novel com-\npression, anonymization, and analysis technique are required\nto meet these challenges.\nThe GraphBLAS standard provides signiﬁcant performance\nand compression capabilities which improve the feasibility\nof analyzing these volumes of data [9]–[23]. Speciﬁcally,\nthe GraphBLAS is ideally suited for both constructing and\nanalyzing anonymized hypersparse trafﬁc matrices. Prior work\nwith the GraphBLAS has demonstrated rates of 75 billion\npackets per second (pps) [24], while achieving compressions\nof 1 bit per packet [25], and enabling the analysis of the\nlargest publicly available historical archives with over 40\ntrillion packets [26]. Analysis of anonymized hypersparse\ntrafﬁc matrices from a variety of sources has revealed power-\nlaw distributions [27], [28], novel scaling relations [25], [26],\nand inspired new models of network trafﬁc [29].\nWhile some raw packet corpora and datasets exist, many\ndata taps prefer netﬂow, owing to the compression and scal-\nability it offers. However, many trafﬁc analyses for security\napplications beneﬁt from temporal aggregation of multiple\nsimultaneous netﬂows into hypersparse trafﬁc matrices, which\ncan be computationally challenging. To solve this problem a\nnovel netﬂow compression and resampling method has been\ndeveloped leveraging GraphBLAS hyperspace trafﬁc matrices\nthat preserves anonymization. Furthermore, standard multi-\ntemporal spatial analyses are then performed on each subrangearXiv:2209.05725v1  [cs.NI]  13 Sep 2022\n\nvalidsource packetwindowNV=217,218,…,227(time window)validdestination packetwindowNV=217,218,…,227(time window)source packets(packets from a source)\ndestination packets(packets to a destination)uniquedestinationsuniquesourcesuniquelinkssourcefan-out\ndestinationfan-inlinkpacketsFig. 1. Streaming network trafﬁc quantities. Internet trafﬁc streams of\nNVvalid packets are divided into a variety of quantities for analysis: source\npackets, source fan-out, unique source-destination pair packets (or links),\ndestination fan-in, and destination packets.\nto generate detailed statistical aggregates of the source packets,\nsource fan-out, unique links, destination fan-in, and destination\npackets of each subrange which can be used for background\nmodeling and anomaly detection. A simple ﬁle format based\non GraphBLAS sparse matrices is developed for storing these\nstatistical aggregates.\nThe outline of the rest of the paper is as follows. Section II\ndescribes the standard anonymized hierarchical hypersparse\nanalytics for computing source packets, source fan-out, unique\nlinks, destination fan-in, and destination packets. Next, the\ngeneration of netﬂow trafﬁc matrices and the novel netﬂow\ntime matrix list (TML) format is described. Subsequently, in\nSection IV the netﬂow test data set is summarized along with\nthe corresponding MIT SuperCloud test hardware. Finally,\nSection V captures the compression and performance results;\nand Section VI captures conclusions, and directions for further\nwork.\nII. A NONYMIZED HIERARCHICAL HYPERSPARSE\nANALYTICS\nNetwork data must be handled with great care and privacy\nis a paramount concern. A primary beneﬁt of constructing\nanonymized hypersparse trafﬁc matrices with the GraphBLAS\nis the efﬁcient computation of a wide range of network\nquantities via matrix mathematics [30]. Figure 1 illustrates\nessential quantities found in all streaming dynamic networks.\nThese quantities are all computable from anonymized trafﬁc\nmatrices created from the source and destination addresses\nfound in Internet packet headers.\nThe network quantities depicted in Figure 1 are computable\nfrom anonymized origin-destination trafﬁc matrices that are\nwidely used to represent network trafﬁc [32]–[35]. To reduce\nstatistical ﬂuctuations, the streaming data should be partitioned\nsuch that any chosen time window all data sets have the\nsame number of valid packets [36]. At a given time t,NV\nconsecutive valid packets are aggregated from the trafﬁc into\na hypersparse matrix At, whereAt(i; j)is the number of valid\npackets between the source iand destination j. The sum ofTABLE I\nNETWORK QUANTITIES FROM TRAFFIC MATRICES\nFormulas for computing network quantities from trafﬁc matrix Atat time tin\nboth summation and matrix notation. 1is a column vector of all 1’s,Tis the\ntranspose operation, and j j0is the zero-norm that sets each nonzero value of\nits argument to 1 [31]. These formulas are unaffected by matrix permutations\nand will work on anonymized data.\nAggregate Summation Matrix\nProperty Notation Notation\nValid packets NVP\niP\njAt(i; j)1TAt1\nUnique linksP\niP\njjAt(i; j)j01TjAtj01\nLink packets from itoj At(i; j) At\nMax link packets ( dmax) max ijAt(i; j) max( At)\nUnique sourcesP\nijP\njAt(i; j)j01TjAt1j0\nPackets from source iP\njAt(i; j) At1\nMax source packets ( dmax) max iP\njAt(i; j) max( At1)\nSource fan-out from iP\njjAt(i; j)j0 jAtj01\nMax source fan-out ( dmax) max iP\njjAt(i; j)j0max( jAtj01)\nUnique destinationsP\njjP\niAt(i; j)j0 j1TAtj01\nDestination packets to jP\niAt(i; j)1TjAtj0\nMax destination packets ( dmax)max jP\niAt(i; j) max( 1TjAtj0)\nDestination fan-in to jP\nijAt(i; j)j01TAt\nMax destination fan-in ( dmax) max jP\nijAt(i; j)j0max( 1TAt)\nall the entries in Atis equal to NV\nX\ni;jAt(i; j) =NV\nConstant packet, variable time samples simplify the statistical\nanalysis of the heavy-tail distributions commonly found in\nnetwork trafﬁc quantities [27], [28], [37]. All the network\nquantities depicted in Figure 1 can be readily computed from\nAtusing the formulas listed in Table I. Because matrix\noperations are generally invariant to permutation (reordering\nof the rows and columns), these quantities can readily be\ncomputed from anonymized data.\nThe contiguous nature of these data allows the exploration\nof a wide range of packet windows from NV= 217(sub-\nsecond) to NV= 227(minutes), providing a unique view into\nhow network quantities depend upon time. These observations\nprovide new insights into normal network background trafﬁc\nthat could be used for anomaly detection, AI feature engineer-\ning, polystore index learning, and testing theoretical models of\nstreaming networks [38]–[40].\nNetwork trafﬁc is dynamic and exhibits varying behavior\non a wide range of time scales. A given packet window\nsizeNVwill be sensitive to phenomena on its corresponding\ntimescale. Determining how network quantities scale with NV\nprovides insight into the temporal behavior of network trafﬁc.\nEfﬁcient computation of network quantities on multiple time\nscales can be achieved by hierarchically aggregating data in\ndifferent time windows [36]. Figure 2 illustrates a binary\naggregation of different streaming trafﬁc matrices. Computing\neach quantity at each hierarchy level eliminates redundant\ncomputations that would be performed if each packet window\nwas computed separately. Hierarchy also ensures that most\ncomputations are performed on smaller matrices residing in\nfaster memory. Correlations among the matrices mean that\n\nstream of trafficmatricesNV=217sourcessparsetrafficmatrixAt:t+TNV=218sourcessparsetrafficmatrixAt:t+2T++++\nNV=219sourcessparsetrafficmatrixAt:t+4T++sourcesdestinations\nsourcesdestinations\nsourcesdestinationsFig. 2. Multi-temporal streaming trafﬁc matrices. Efﬁcient computation of\nnetwork quantities on multiple time scales can be achieved by hierarchically\naggregating data in different time windows.\nadding two matrices each with NVentries results in a matrix\nwith fewer than 2NVentries, reducing the relative number of\noperations as the matrices grow.\nOne of the important capabilities of the SuiteSparse Graph-\nBLAS library is direct support of hypersparse matrices where\nthe number of nonzero entries is signiﬁcantly less than either\ndimensions of the matrix. If the packet source and destination\nidentiﬁers are drawn from a large numeric range, such as those\nused in the Internet protocol, then a hypersparse representation\nofAteliminates the need to keep track of additional indices\nand can signiﬁcantly accelerate the computations [24].\nIt is common to ﬁlter the packets down to a valid set\nfor any particular analysis. Such ﬁlters may limit particular\nsource ranges, destinations ranges, protocols, and time win-\ndows. Anonymized data can be analyzed by subranges of\nIPs using simple matrix multiplication. For a given subrange\nrepresented by an anonymized hypersparse diagonal matrix\nAr, where Ar(i; i) = 1 implies source/destination iis in the\nrange, the trafﬁc within the subrange can be computed via:\nArAtAr. Likewise, for additional privacy guarantees that can\nbe implemented at the edge, the same method can be used to\nexclude a range of data from the trafﬁc matrix\nAt\u0000ArAtAr\nIn this work, three source and destination subranges are\nused corresponding to non-routable trafﬁc, bogon trafﬁc, and\nall other trafﬁc (see Figure 3). All network quantities are\ncomputed on the entire trafﬁc matrix and each of the 9 smaller\ntrafﬁc matrices.\nIII. N ETFLOW TRAFFIC MATRICES\nUse of compressed netﬂow data to generate matrices of\nexactly NVpackets needs time normalization to align with\nthe constant packet, variable time matrix sizing approach\ndescribed above. With time normalization, the aggregated\nnetﬂow records are converted into a series of timestamped\nﬂows that enable the grouping of netﬂows to build network\nmatrices with of ﬁxed size. The time intervals associated\nwith ﬂow entries are normalized into Unix epoch time with\na resolution of seconds. If a netﬂow record lasts for longer\nthan one second, an even data transfer rate is assumed, and\nthe packet counts ( pkts ) are divided evenly to each second\nwithin the interval.\ndestinationssourcesdestinationssparse traffic matrix A\n1à31à13à33à1range 1          range 2          range 3range 1          range 2          range 32à32à11à23à22à2Fig. 3. Trafﬁc matrix ranges. Trafﬁc matrices can be divided into subranges\nof speciﬁc sources and destinations for analysis, which can be essential if\ndifferent subranges have different statistical distributions.\nFor ﬂow records whose number of packets ( pkts ) do not\ndivide evenly by the number of 1-second time bins ( tbins ),\nthe number of bins that will contain an extra packet extra is\ngiven by\nextra =pkts\u0000\u0012\nbpkts\ntbinsc\u0002tbins\u0013\nIfextra\u00142, the extra bins are allocated to the ﬁrst and\nlast bin in the interval. Otherwise, the remaining extra bins\nare shufﬂed into the bins between the ﬁrst and last bin. This\ncreates an even distribution of ﬂows across the time interval.\nAll packets at a given time bin are then aggregate into trafﬁc\nmatrix for that time bin.\nThe representation of network ﬂow in a matrix coordinate\nspace opens up the possibility of further compressing netﬂow.\nThe Time Compressed Matrices List (TML) format leverages\nthe matrix coordinate space, and only stores the delta of coor-\ndinates between time-adjacent matrices, as show in Figure 4.\nUsing a canonical ordering of the current coordinate space\n(source-destination IP pairings), the deltas can then be used to\nreconstruct the original matrix in a lossless fashion.\nTML representation of a netﬂow is a stream of 5-vectors\nh(t0; s0; D0; I0; V0); :::;(tn; sn; Dn; In; Vn)iwhere t0totn\nare signed 64-bits Unix epoch timestamps; s0tosnare an\nencoded pair of 32-bit signed delete-insert count; D0toDn\nare the list of deleted coordinates, I0toInare the list of\ninserted coordinates, and V0toVnare the list of canonically\nordered coordinates associated with current coordinate set.\nIV. T ESTDATA AND HARDWARE\nThe test data set comprises of nearly 100TB of netﬂow data\nwith over 50 trillion packets from many different locations for\na period of several months. The trafﬁc patterns are representa-\ntive of a large enterprise environments with bidirectional trafﬁc\nat the collection points. The collection points also covered a\nrange of trafﬁc rates.\n\ntimesources\ndestinationsnetflownetflownetflownetflowDtDtDtDtNVpacketsFig. 4. Netﬂow trafﬁc matrices. The Internet can be viewed as a hypersparse trafﬁc matrix where each point represents communication between a source\nand a destination. Network ﬂows (netﬂows) are vertical lines corresponding to a start time, stop time, source, destination, and number of packets in each\ndirection. Time sliced trafﬁc matrices can be constructed by spreading the packets uniformly across time windows of width \u0001t. These trafﬁc matrices can\nthen be summed to into aggregated trafﬁc matrices with exactly NVvalid packets for subsequent analyses.\nThe analysis code was implemented using Python Graph-\nBLAS bindings with the pPython parallel library [41]. A\ntypical run could be launched in a few seconds using the\nMIT SuperCloud triples-mode hierarchical launching system\n[42]. The launch parameters were [Nnodes Nprocess Nthread],\ncorresponding to Nnodes nodes, Nprocess Python processes\nper node, and Nthread OpenMP threads per process. On each\nnode, each of the Nprocess processes and their corresponding\nNthread threads were pinned to adjacent cores to minimize\ninterprocess contention and maximize cache locality for the\nGraphBLAS OpenMP threads [43]. Within each Python pro-\ncess, the underlying GraphBLAS OpenMP parallelism is used.\nAt the end of the processing the results were aggregated\nusing asynchronous ﬁle-based messaging [44]. Triples mode\nmakes it easy to explore horizontal scaling across nodes,\nvertical scaling by examining combinations of processes and\nthreads on a node, and temporal scaling by running on diverse\nhardware from different eras.\nThe computing hardware consists of four different types of\nnodes acquired over a decade (see Table II). The nodes are all\nmulticore x86 compatible with comparable total memory. The\nMIT SuperCloud maintains the same modern software across\nall nodes, which allows for direct comparison of hardware\nperformance differences.\nV. R ESULTS\nThe use of GraphBLAS to represent network trafﬁc matrices\nhas provided tremendous compression results [5]. Netﬂow\nrepresentation of raw data provides variable compression re-\nsults, depending on the time intervals used for aggregation.\nAggregating one hour of raw trafﬁc into a single netﬂow\nwill provide much greater compression than aggregating oneTABLE II\nCOMPUTER HARDWARE SPECIFICATIONS\nMIT SuperCloud maintains a diverse set of hardware running an identical\nmodern software stack providing an unique platform for comparing perfor-\nmance over different eras.\nServerLabelProcessorMemoryEraPartClockCoresPartClockSizexeon-p82019 Q2Dual Xeon Platinum 82602.4 GHz48DDR42.93 GHz192 GBxeon-g62019 Q2Dual Xeon Gold 62482.5 GHz40DDR42.93 GHz384 GBxeon64c2016 Q2Xeon Phi 72101.3 GHz64DDR41.20 GHz192 GBxeon-e52014 Q3Dual Xeon E5-2683 v32.0 GHz28DDR42.13 GHz256 GB\nminute of raw trafﬁc - with the gain in compression being\noffset by the loss in data ﬁdelity. The use of TML format\nas described in Section III to represent netﬂow data provides\nadditional compression without any further loss in data ﬁdelity.\nFigure 5 plots the number of packets per megabyte for different\nﬁle formats in the analysis pipeline, with the TML format and\nresulting network analyses providing 105x to 107x compres-\nsion over the raw packets. For the dataset that was analyzed,\nthe TML representation of netﬂow achieved signiﬁcant ( 93%)\ncompression on the 100TB dataset that was used for analysis.\nThe benchmarking explores how different numbers of\nGraphBLAS processes and threads perform on different mul-\nticore compute nodes. This type of benchmarking is generally\nuseful in most projects as it allows the determination of the\nbest combination processing and threads prior to signiﬁcant\ncomputation. The number of processes and threads used for a\ngiven benchmark is denoted by Nprocess\u0002Nthreads whose\nproduct is equal to total number threads used in the computa-\ntion. The GraphBLAS computation described in the previous\nsection was repeated for two sets of parameters: single-process\n\nRawPacketsPacketHeadersSource &DestinationIPsGraphBLASTraffic MatrixCompressedNetflowTime MatrixListNetworkAnalyticPackets per Megabyte10111010109108107106105104103102Format1 bit/packetFig. 5. Estimated ﬁle format sizes. Number of packets per megabyte for\ndifferent ﬁle formats in the analysis pipeline. The time matrix list (TML) and\nresulting network analyses provide 105x to107x compression over the raw\npackets.\nand multi-process. In the single-process case the parameters\ntested are\n1\u00021;1\u00022;1\u00024;1\u00028; :::\nIn the multi-process case the parameters tested are\n1\u00021;2\u00021;4\u00021;8\u00021; :::\nFigure 6 shows the single node performance using different\nnumbers of processes and threads for the different servers\nlisted in Table II. In all cases, the multi-process scaling\nprovided greater aggregate performance and the single-process\nscaling provided a maximum of a 4x speedup over 1\u00021case.\nVI. C ONCLUSIONS AND FUTURE WORK\nAnalysis of trafﬁc on the Internet has long been a major\nchallenge due to the volume and rate of network trafﬁc. Recent\nadvances, combining the use of hypersparse matrix represen-\ntation of network trafﬁc coupled with GraphBLAS is proving\nto be a key enabler of privacy-preserving network analytics.\nThe effort documented in this paper further advances the ﬁeld\nby demonstrating the ability to use compressed network ﬂows\n(netﬂows) in lieu of raw packet streams to conduct similar\nanalytics. Further, it introduces a novel netﬂow compression\nand resampling method that preserves anonymization while\nenabling subrange analysis. This method is scale tested on\nthe MIT SuperCloud using a 50 trillion packet netﬂow corpus\nfrom several hundred sites collected over several months.\nThe resulting compression achieved is signiﬁcant ( <0.1 bit\nper packet) enabling extremely large netﬂow analyses to be\nstored and transported. The single node parallel performance\nis analyzed in terms of both processors and threads showing\nthat a single node can perform hundreds of simultaneous\nanalyses at over a million packets/sec (roughly equivalent to\na 10 Gigabit link).\nThe success in the use of netﬂow in place of raw packets to\nconduct analyses opens up a number of potential pathways forfuture work; amongst which are cross-correlation of netﬂow\ndata from different observatories and outposts, comparing\ngain/loss for using different netﬂow aggregation windows,\nand exploring the gain/loss using expected utility theory and\nprospect theory. Further analysis needs to be done to calibrate\nAI algorithms for classiﬁcation of background trafﬁc to use\nnetﬂow.\nACKNOWLEDGMENTS\nThe authors wish to acknowledge the following individu-\nals for their contributions and support: Bob Bond, Stephen\nBuckley, Ronisha Carter, Cary Conrad, Alan Edelman, Tucker\nHamilton, Jeff Gottschalk, Chris Hill, Mike Kanaan, Tim\nKraska, Charles Leiserson, Mimi McClure, Kyle McAlpin,\nJoseph McDonald, Sandy Pentland, Heidi Perry, Christian\nProthmann, John Radovan, Steve Rejto, Daniela Rus, Matthew\nWeiss, Marc Zissman.\nREFERENCES\n[1] V . Paxson, “Bro: a system for detecting network intruders in real-time,”\nComputer networks , vol. 31, no. 23-24, pp. 2435–2463, 1999.\n[2] M. Roesch et al. , “Snort: Lightweight intrusion detection for networks.,”\ninLisa, vol. 99, pp. 229–238, 1999.\n[3] B. Claise, “Cisco systems netﬂow services export version 9,” tech. rep.,\n2004.\n[4] C. Estan, K. Keys, D. Moore, and G. Varghese, “Building a better\nnetﬂow,” ACM SIGCOMM Computer Communication Review , vol. 34,\nno. 4, pp. 245–256, 2004.\n[5] J. Kepner, J. Bernays, S. Buckley, K. Cho, C. Conrad, L. Daigle,\nK. Erhardt, V . Gadepally, B. Greene, M. Jones, R. Knake, B. Maggs,\nP. Michaleas, C. Meiners, A. Morris, A. Pentland, S. Pisharody,\nS. Powazek, A. Prout, P. Reiner, K. Suzuki, K. Takhashi, T. Tauber,\nL. Walker, and D. Stetson, “Zero botnets: An observe-pursue-counter\napproach.” Belfer Center Reports, 6 2021.\n[6] S. Weed, “Beyond zero trust: Reclaiming blue cyberspace,” Master’s\nthesis, United States Army War College, 2022.\n[7] “ Cisco Visual Networking Index: Forecast and Trends .”\nhttps://newsroom.cisco.com/press-release-content?articleId=1955935.\n[8] “ Cisco Visual Networking Index: Forecast and Trends, 2018–2023 .”\nhttps://www.cisco.com/c/en/us/solutions/collateral/executive-\nperspectives/annual-internet-report/white-paper-c11-741490.html.\n[9] J. Kepner and J. Gilbert, Graph algorithms in the language of linear\nalgebra . SIAM, 2011.\n[10] J. Kepner, D. Bader, A. Buluc ¸, J. Gilbert, T. Mattson, and H. Mey-\nerhenke, “Graphs, matrices, and the graphblas: Seven good reasons,”\nProcedia Computer Science , vol. 51, pp. 2453–2462, 2015.\n[11] J. Kepner, P. Aaltonen, D. Bader, A. Buluc ¸, F. Franchetti, J. Gilbert,\nD. Hutchison, M. Kumar, A. Lumsdaine, H. Meyerhenke, S. McMillan,\nC. Yang, J. D. Owens, M. Zalewski, T. Mattson, and J. Moreira, “Math-\nematical foundations of the graphblas,” in 2016 IEEE High Performance\nExtreme Computing Conference (HPEC) , pp. 1–9, 2016.\n[12] A. Buluc ¸, T. Mattson, S. McMillan, J. Moreira, and C. Yang, “Design\nof the graphblas api for c,” in 2017 IEEE International Parallel and\nDistributed Processing Symposium Workshops (IPDPSW) , pp. 643–652,\n2017.\n[13] J. Kepner, M. Kumar, J. Moreira, P. Pattnaik, M. Serrano, and H. Tufo,\n“Enabling massive deep neural networks with the graphblas,” in 2017\nIEEE High Performance Extreme Computing Conference (HPEC) ,\npp. 1–10, IEEE, 2017.\n[14] C. Yang, A. Buluc ¸, and J. D. Owens, “Implementing push-pull efﬁciently\nin graphblas,” in Proceedings of the 47th International Conference on\nParallel Processing , pp. 1–11, 2018.\n[15] T. A. Davis, “Algorithm 1000: Suitesparse:graphblas: Graph algorithms\nin the language of sparse linear algebra,” ACM Trans. Math. Softw. ,\nvol. 45, Dec. 2019.\n[16] J. Kepner and H. Jananthan, Mathematics of big data: Spreadsheets,\ndatabases, matrices, and graphs . MIT Press, 2018.\n\n10000100000100000010000000\n1101001000Packets per SecondNumber of Threads1x11071061051041x64single-process1x21x41x161x322x14x18x116x1multi-process32x148x148x21x848x4xeon-p8\n10000100000100000010000000\n110100single-process1x11x21x41x81x161x321x402x14x18x116x1multi-process32x140x140x21071061051041x80Packets per SecondNumber of Threadsxeon-g6\n1000100001000001000000\n1101001000single-process1x11x21x41x81x161x321x642x14x18x116x1multi-process32x132x232x41061051041031x12832x5Packets per SecondNumber of Threadsxeon-64c\n100001000001000000\n110100single-process1x11x21x41x81x161x281x562x14x18x116x1multi-process28x110610510428x2Packets per SecondNumber of Threadsxeon-e5Fig. 6. Single node performance with different conﬁgurations of processes and threads on a xeon-p8 (upper left), xeon-g6 (upper right), xeon64c (bottom\nleft), and xeon-e5 (bottom right). Each data point is labeled: (#processes)x(#threads/process). The maximum performance for each case is denoted by a circle.\n[17] T. A. Davis, “Algorithm 1000: Suitesparse: Graphblas: Graph algorithms\nin the language of sparse linear algebra,” ACM Transactions on Mathe-\nmatical Software (TOMS) , vol. 45, no. 4, pp. 1–25, 2019.\n[18] T. Mattson, T. A. Davis, M. Kumar, A. Buluc, S. McMillan, J. Moreira,\nand C. Yang, “Lagraph: A community effort to collect graph algorithms\nbuilt on top of the graphblas,” in 2019 IEEE International Parallel and\nDistributed Processing Symposium Workshops (IPDPSW) , pp. 276–284,\nIEEE, 2019.\n[19] P. Cailliau, T. Davis, V . Gadepally, J. Kepner, R. Lipman, J. Lovitz, and\nK. Ouaknine, “Redisgraph graphblas enabled graph database,” in 2019\nIEEE International Parallel and Distributed Processing Symposium\nWorkshops (IPDPSW) , pp. 285–286, IEEE, 2019.\n[20] T. A. Davis, M. Aznaveh, and S. Kolodziej, “Write quick, run fast:\nSparse deep neural network in 20 minutes of development time via\nsuitesparse: Graphblas,” in 2019 IEEE High Performance Extreme\nComputing Conference (HPEC) , pp. 1–6, IEEE, 2019.\n[21] M. Aznaveh, J. Chen, T. A. Davis, B. Hegyi, S. P. Kolodziej, T. G.\nMattson, and G. Sz ´arnyas, “Parallel graphblas with openmp,” in 2020\nProceedings of the SIAM Workshop on Combinatorial Scientiﬁc Com-\nputing , pp. 138–148, SIAM, 2020.\n[22] B. Brock, A. Buluc ¸, T. G. Mattson, S. McMillan, and J. E. Moreira,\n“Introduction to graphblas 2.0,” in 2021 IEEE International Parallel and\nDistributed Processing Symposium Workshops (IPDPSW) , pp. 253–262,\nIEEE, 2021.\n[23] M. Pelletier, W. Kimmerer, T. A. Davis, and T. G. Mattson, “The\ngraphblas in julia and python: the pagerank and triangle centralities,” in\n2021 IEEE High Performance Extreme Computing Conference (HPEC) ,pp. 1–7, 2021.\n[24] J. Kepner, T. Davis, C. Byun, W. Arcand, D. Bestor, W. Bergeron,\nV . Gadepally, M. Hubbell, M. Houle, M. Jones, A. Klein, P. Michaleas,\nL. Milechin, J. Mullen, A. Prout, A. Rosa, S. Samsi, C. Yee, and\nA. Reuther, “75,000,000,000 streaming inserts/second using hierarchical\nhypersparse graphblas matrices,” in 2020 IEEE International Parallel\nand Distributed Processing Symposium Workshops (IPDPSW) , pp. 207–\n210, 2020.\n[25] J. Kepner, C. Meiners, C. Byun, S. McGuire, T. Davis, W. Arcand,\nJ. Bernays, D. Bestor, W. Bergeron, V . Gadepally, R. Harnasch,\nM. Hubbell, M. Houle, M. Jones, A. Kirby, A. Klein, L. Milechin,\nJ. Mullen, A. Prout, A. Reuther, A. Rosa, S. Samsi, D. Stetson, A. Tse,\nC. Yee, and P. Michaleas, “Multi-temporal analysis and scaling relations\nof 100,000,000,000 network packets,” in 2020 IEEE High Performance\nExtreme Computing Conference (HPEC) , pp. 1–6, 2020.\n[26] J. Kepner, M. Jones, D. Andersen, A. Buluc ¸, C. Byun, K. Claffy,\nT. Davis, W. Arcand, J. Bernays, D. Bestor, W. Bergeron, V . Gadepally,\nM. Houle, M. Hubbell, A. Klein, C. Meiners, L. Milechin, J. Mullen,\nS. Pisharody, A. Prout, A. Reuther, A. Rosa, S. Samsi, D. Stetson,\nA. Tse, C. Yee, and P. Michaleas, “Spatial temporal analysis of\n40,000,000,000,000 internet darkspace packets,” in 2021 IEEE High\nPerformance Extreme Computing Conference (HPEC) , pp. 1–8, 2021.\n[27] J. Kepner, K. Cho, K. Claffy, V . Gadepally, P. Michaleas, and\nL. Milechin, “Hypersparse neural network analysis of large-scale internet\ntrafﬁc,” in 2019 IEEE High Performance Extreme Computing Confer-\nence (HPEC) , pp. 1–11, 2019.\n[28] J. Kepner, K. Cho, K. Claffy, V . Gadepally, S. McGuire, L. Milechin,\n\nW. Arcand, D. Bestor, W. Bergeron, C. Byun, M. Hubbell, M. Houle,\nM. Jones, A. Prout, A. Reuther, A. Rosa, S. Samsi, C. Yee, and\nP. Michaleas, “New phenomena in large-scale internet trafﬁc,” in Mas-\nsive Graph Analytics (D. Bader, ed.), pp. 1–53, Chapman and Hall/CRC,\n2022.\n[29] P. Devlin, J. Kepner, A. Luo, and E. Meger, “Hybrid power-law models\nof network trafﬁc,” arXiv preprint arXiv:2103.15928 , 2021.\n[30] S. Pisharody, J. Bernays, V . Gadepally, M. Jones, J. Kepner, C. Meiners,\nP. Michaleas, A. Tse, and D. Stetson, “Realizing forward defense in the\ncyber domain,” in 2021 IEEE High Performance Extreme Computing\nConference (HPEC) , pp. 1–7, IEEE, 2021.\n[31] J. Karvanen and A. Cichocki, “Measuring sparseness of noisy signals,”\nin4th International Symposium on Independent Component Analysis\nand Blind Signal Separation , pp. 125–130, 2003.\n[32] A. Soule, A. Nucci, R. Cruz, E. Leonardi, and N. Taft, “How to\nidentify and estimate the largest trafﬁc matrix elements in a dynamic\nenvironment,” in ACM SIGMETRICS Performance Evaluation Review ,\nvol. 32, pp. 73–84, ACM, 2004.\n[33] Y . Zhang, M. Roughan, C. Lund, and D. L. Donoho, “Estimating point-\nto-point and point-to-multipoint trafﬁc matrices: an information-theoretic\napproach,” IEEE/ACM Transactions on Networking (TON) , vol. 13,\nno. 5, pp. 947–960, 2005.\n[34] P. J. Mucha, T. Richardson, K. Macon, M. A. Porter, and J.-P. Onnela,\n“Community structure in time-dependent, multiscale, and multiplex\nnetworks,” science , vol. 328, no. 5980, pp. 876–878, 2010.\n[35] P. Tune, M. Roughan, H. Haddadi, and O. Bonaventure, “Internet trafﬁc\nmatrices: A primer,” Recent Advances in Networking , vol. 1, pp. 1–56,\n2013.\n[36] J. Kepner, V . Gadepally, L. Milechin, S. Samsi, W. Arcand, D. Bestor,\nW. Bergeron, C. Byun, M. Hubbell, M. Houle, M. Jones, A. Klein,\nP. Michaleas, J. Mullen, A. Prout, A. Rosa, C. Yee, and A. Reuther,\n“Streaming 1.9 billion hypersparse network updates per second with\nd4m,” in 2019 IEEE High Performance Extreme Computing Conference\n(HPEC) , pp. 1–6, 2019.\n[37] J. Nair, A. Wierman, and B. Zwart, “The fundamentals of heavy tails:\nProperties, emergence, and estimation,” Preprint, California Institute of\nTechnology , 2020.\n[38] A. J. Elmore, J. Duggan, M. Stonebraker, M. Balazinska, U. Cetintemel,\nV . Gadepally, J. Heer, B. Howe, J. Kepner, T. Kraska, et al. , “A\ndemonstration of the bigdawg polystore system,” Proceedings of the\nVLDB Endowment , vol. 8, no. 12, p. 1908, 2015.\n[39] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis, “The case\nfor learned index structures,” in Proceedings of the 2018 International\nConference on Management of Data , SIGMOD 18, (New York, NY ,\nUSA), pp. 489–504, Association for Computing Machinery, 2018.\n[40] E. H. Do and V . N. Gadepally, “Classifying anomalies for network\nsecurity,” in ICASSP 2020 - 2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pp. 2907–2911,\n2020.\n[41] B. et al, “ppython for parallel python programming,” in 2022 IEEE\nHigh Performance Extreme Computing Conference (HPEC) - submitted ,\nIEEE, 2022.\n[42] A. Reuther, J. Kepner, C. Byun, S. Samsi, W. Arcand, D. Bestor,\nB. Bergeron, V . Gadepally, M. Houle, M. Hubbell, M. Jones, A. Klein,\nL. Milechin, J. Mullen, A. Prout, A. Rosa, C. Yee, and P. Michaleas,\n“Interactive supercomputing on 40,000 cores for machine learning and\ndata analysis,” in 2018 IEEE High Performance extreme Computing\nConference (HPEC) , pp. 1–6, 2018.\n[43] C. Byun, J. Kepner, W. Arcand, D. Bestor, W. Bergeron, M. Hubbell,\nV . Gadepally, M. Houle, M. Jones, A. Klein, L. Milechin, P. Michaleas,\nJ. Mullen, A. Prout, A. Rosa, S. Samsi, C. Yee, and A. Reuther,\n“Optimizing xeon phi for interactive data analysis,” in 2019 IEEE High\nPerformance Extreme Computing Conference (HPEC) , pp. 1–6, 2019.\n[44] C. Byun, J. Kepner, W. Arcand, D. Bestor, B. Bergeron, V . Gadepally,\nM. Houle, M. Hubbell, M. Jones, A. Klein, P. Michaleas, J. Mullen,\nA. Prout, A. Rosa, S. Samsi, C. Yee, and A. Reuther, “Large scale\nparallelization using ﬁle-based communications,” in 2019 IEEE High\nPerformance Extreme Computing Conference (HPEC) , pp. 1–7, 2019.",
  "textLength": 34589
}