{
  "paperId": "7f67dcd457420d48d448c243222158cce288b677",
  "title": "End-to-End Learning to Index and Search in Large Output Spaces",
  "pdfPath": "7f67dcd457420d48d448c243222158cce288b677.pdf",
  "text": "ELIAS: End-to-End Learning to Index and Search in\nLarge Output Spaces\nNilesh Gupta\nUT Austin\nnilesh@cs.utexas.eduPatrick H. Chen\nUCLA\npatrickchen@g.ucla.eduHsiang-Fu, Yu\u0003\nAmazon\nrofu.yu@gmail.com\nCho-Jui, Hsieh\nUCLA\nchohsieh@cs.ucla.eduInderjit S. Dhillon\nUT Austin & Google\ninderjit@cs.utexas.edu\nAbstract\nExtreme multi-label classiﬁcation (XMC) is a popular framework for solving many\nreal-world problems that require accurate prediction from a very large number\nof potential output choices. A popular approach for dealing with the large label\nspace is to arrange the labels into a shallow tree-based index and then learn an\nML model to efﬁciently search this index via beam search. Existing methods\ninitialize the tree index by clustering the label space into a few mutually exclusive\nclusters based on pre-deﬁned features and keep it ﬁxed throughout the training\nprocedure. This approach results in a sub-optimal indexing structure over the label\nspace and limits the search performance to the quality of choices made during the\ninitialization of the index. In this paper, we propose a novel method ELIAS which\nrelaxes the tree-based index to a specialized weighted graph-based index which is\nlearned end-to-end with the ﬁnal task objective. More speciﬁcally, ELIAS models\nthe discrete cluster-to-label assignments in the existing tree-based index as soft\nlearnable parameters that are learned jointly with the rest of the ML model. ELIAS\nachieves state-of-the-art performance on several large-scale extreme classiﬁcation\nbenchmarks with millions of labels. In particular, ELIAS can be up to 2.5% better\nat precision@ 1and up to 4% better at recall@ 100than existing XMC methods.\nA PyTorch implementation of ELIAS along with other resources is available at\nhttps://github.com/nilesh2797/ELIAS.\n1 Introduction\nMany real-world problems require making accurate predictions from a large number of potential\noutput choices. For example, search advertising aims to ﬁnd the most relevant ads to a given search\nquery from a large corpus of ads [ 26,14], open-domain question answering requires ﬁnding the\nright answers to a given question from a large collection of text documents [ 8,29], and product\nrecommendation requires recommending similar or related products from a large product catalog,\nbased on past searches and interactions by users. eXtreme Multi-label Classiﬁcation (XMC) is a\npopular framework for solving such problems [ 4], which formulates these problems as a multi-label\nclassiﬁcation task with very large number of labels; here each output choice is treated as a separate\nlabel. A label `is often parameterized by its one-versus-all classiﬁer vector w`and the relevance\nbetween label `and input xis formulated as wT\n`\u001e(x), where\u001eis an encoding function which maps\nan input xto its vector representation.\n\u0003This work does not relate to Hsiang-Fu’s position at Amazon\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.08410v2  [cs.LG]  9 Jan 2023\n\n[ROOT]\nClustersPartition Based Index\nLabels\nFixed Label\nAssignments\nELIAS Index\nLabelsClusters[ROOT]Figure 1: Traditional partition-based index vs ELIAS index; here an arrow from a cluster to a label denotes the\nassignment of the label to the cluster, arrow width indicates the weight of the assignment. ( left) Existing partition\nbased XMC methods use a shallow balanced tree as the index structure with a label uniquely assigned to exactly\none cluster; moreover, they initialize the clusters over pre-deﬁned features and keep them ﬁxed throughout the\ntraining procedure. ( right ) ELIAS generalizes the tree based index to a sparsely connected graph-based index\nand learns the cluster-to-label assignments end-to-end with the task objective during training.\nEvaluating wT\n`\u001e(x)for every label `in an XMC task can get computationally expensive since the\nnumber of labels could easily be upwards of millions. To reduce the complexity, most existing\nmethods employ a search index that efﬁciently shortlists a small number of labels for an input query\nand the relevance scores are only evaluated on these shortlisted labels. The quality of the search\nindex plays a pivotal role in the accuracy of these methods since a label `outside the shortlist will\nbe directly discarded, even if it can be correctly captured by its classiﬁer vector w`. Moreover, the\nlabel classiﬁer w`is a function of the quality of the index as during training, the label classiﬁers are\nlearned with negative sampling based on the search index. Therefore, how to improve the quality of\nthe search index becomes a key challenge in the XMC problem.\nThere are two main formulations of the search index: 1) partition-based approach [ 25,31,7,18,32]\nand 2) approximate nearest neighbor search (ANNS) based approach [ 16,9,13,10]. In partition-\nbased approach, labels are ﬁrst arranged into a tree-based index by partitioning the label space into\nmutually exclusive clusters and then a ML model is learned to route a given instance to a few relevant\nclusters. In an ANNS-based approach, a ﬁxed, black-box ANNS index is learned on pre-deﬁned label\nembeddings. Given an input embedding, this index is then used to efﬁciently query a small set of\nnearest labels based on some distance/similarity between the input and label embeddings. Both of\nthese approaches suffer from a critical limitation that the index structure is ﬁxed after it’s initialized.\nThis decoupling of the search index from the rest of the ML model training prevents the search index\nfrom adapting with the rest of the model during training, which leads to sub-optimal performance.\nTo overcome this challenge, we propose a novel method called ELIAS: End-to-end Learning to Index\nandSearch, which jointly learns the search index along with the rest of the ML model for multi-label\nclassiﬁcation in large output spaces. In particular, as illustrated in Fig. 1, ELIAS generalizes\nthe widely used partition tree-based index to a sparsely connected weighted graph-based index.\nELIAS models the discrete cluster-to-label assignments in the existing partition based approaches\nas soft learnable parameters that are learned end-to-end with the encoder and classiﬁcation module\nto optimize the ﬁnal task objective. Moreover, because ELIAS uses a graph-based arrangement of\nlabels instead of a tree-based arrangement, a label can potentially be assigned to multiple relevant\nclusters. This helps to better serve labels with a multi-modal input distribution [22].\nThrough extensive experiments we demonstrate that ELIAS achieves state-of-the-art results on\nmultiple large-scale XMC benchmarks. Notably, ELIAS can be up to 2.5% better at precision@ 1and\nup to 4% better at recall@100 than existing XMC methods. ELIAS’s search index can be efﬁciently\nimplemented on modern GPUs to offer fast inference times on million scale datasets. In particular,\nELIAS offers sub-millisecond prediction latency on a dataset with 3 million labels on a single GPU.\n2 Related Work\nOne-vs-all (OV A) methods : OV A methods consider classiﬁcation for each label as an independent\nbinary classiﬁcation problem. In particular, an OV A method learns L(number of classes) independent\nlabel classiﬁers [w`]L\n`=1, where the job of each classiﬁer w`is to distinguish training points of label `\nfrom the rest of the training points. At prediction time, each label classiﬁer is evaluated and the labels\nare ranked according to classiﬁer scores. Traditional OV A methods like DiSMEC [ 2], ProXML [ 3],\nand PPDSparse [ 30] represent each input instance by their sparse bag-of-word features and learn\n2\n\nsparse linear classiﬁers by massively parallelizing over multiple machines. OV A methods achieve\npromising results on XMC benchmarks but suffer from huge computational complexity because of\ntheir linear scaling with number of labels. Subsequent XMC methods borrow the same building\nblocks of an OV A approach but overcome the computational overhead by employing some form of\nsearch index to efﬁciently shortlist only a few labels during training and prediction.\nPartition based methods : Many XMC methods such as Parabel [ 25], Bonsai [ 19], XR-Linear [ 32],\nAttentionXML [ 31], X-Transformer [ 28], XR-Transformer [ 33], LightXML [ 18] follow this approach\nwhere the label space is partitioned into a small number of mutually exclusive clusters, and then an\nML model is learned to route a given instance to a few relevant clusters. A popular way to construct\nclusters is to perform balanced k-means clustering recursively using some pre-deﬁned input features.\nTraditional methods like Parabel, Bonsai, and XR-Linear represent their input by sparse bag-of-word\nfeatures and learn sparse linear classiﬁers with negative sampling performed based on the search\nindex. With the advancement of deep learning in NLP, recent deep learning based XMC methods\nreplace sparse bag-of-word input features with dense embeddings obtained from a deep encoder.\nIn particular, AttentionXML uses a BiLSTM encoder while X-Transformer, XR-Transformer, and\nLightXML use deep transformer models such as BERT [ 11] to encode the raw input text. In addition\nto dense embeddings, the state-of-the-art XR-Transformer method uses a concatenation of dense\nembedding and sparse bag-of-word features to get a more elaborate representation of the input, thus\nmitigating the information loss in text truncation in transformers.\nANNS based methods : Methods like SLICE [ 16], DeepXML [ 9], and GLaS [ 13] utilize approximate\nnearest neighbor search (ANNS) structure over pre-deﬁned label representations to efﬁciently shortlist\nlabels. In particular, SLICE represents each input instance by its FastText [ 24] embedding and uses\nthe mean of a label’s training points as a surrogate embedding for that label. It further constructs an\nHNSW [ 23] graph (a popular ANNS method) over these surrogate label embeddings. For a given\ninput, the HNSW graph is queried to efﬁciently retrieve nearest indexed labels based on the cosine\nsimilarity between the input and label embedding. DeepXML extends SLICE by learning an MLP\ntext encoder on a surrogate classiﬁcation task instead of using a ﬁxed FastText model to obtain input\nembeddings. GLaS takes a different approach and learns label classiﬁers with random negative\nsampling. After the model is trained, it constructs an ANNS index to perform fast maximum inner\nproduct search (MIPS) directly on the learned label classiﬁers.\nLearning search index : There has been prior works [ 21,1] that model different types of standard\ndata structures with neural networks. A recent paper [ 27] models the search index in information\nretrieval systems as a sequence to sequence model where all the parameters of the search index is\nencoded in the parameters of a big transformer model. In a more similar spirit to our work, another\nrecent paper [ 22] attempts to learn overlapping cluster partitions for XMC tasks by assigning each\nlabel to multiple clusters. Even though it serves as a generic plug-in method to improve over any\nexisting partition based XMC method, it still suffers from the following shortcomings: 1) label\nassignments are not learned end-to-end with the task objective; instead, it alternates between ﬁnding\nthe right model given the ﬁxed label assignments and then ﬁnding the right label assignments given\nthe ﬁxed model, 2) all labels are assigned to a pre-deﬁned number of clusters with equal probability\nand get duplicated in each assigned cluster, which results in increased computational complexity of\nthe method.\n3 ELIAS: E nd-to-end L earning to I ndex a nd S earch\nThe multi-label classiﬁcation problem can be formulated as following: given an input x2X, predict\ny2f0;1gLwhere yis a sparseLdimensional vector with y`= 1if and only if label `is relevant\nto input x. Here,Ldenotes the number of distinct labels - note that ycan have multiple non-zero\nentries resulting in multiple label assignments to input x. The training dataset is given in the form\noff(xi;yi) :i= 1;:::;Ng. XMC methods address the case where the label space ( L) is extremely\nlarge (in the order of few hundred thousands to millions). All deep learning based XMC methods\nhave the following three key components:\nDeep encoder \u001e:X!RDwhich maps the input xto aD-dimensional dense embedding through a\ndifferentiable function. For text input, a popular choice of \u001eis the BERT [ 11] encoder where each\ninputxis represented as a sequence of tokens.\n3\n\nrejected path\nshortlisted path[ROOT]\nClusters\nLabelsBERT\nencoderFigure 2: Illustration of ELIAS’s search procedure: an input xis ﬁrst embedded by the text encoder \u001eto\nget its embedding \u001e(x). Only a few (beam-size) clusters are shortlisted based on cluster relevance scores\n^sc\u0018^wT\nc\u001e(x). All potential edges of shortlisted clusters are explored and assigned a score based on the\nproduct ^sc\u0003^sc;`(^sc;`is normalized form of learnable edge weight parameter ac;`between cluster cand label\nl). Top-Kpaths are shortlisted based on their assigned scores and the ﬁnal label relevance is computed as\n\u001b(wT\n`\u001e(x))\u0003^sc;`\u0003^sc, here\u001bis the sigmoid function. If a label `can be reached from multiple paths then the\npath with maximal score is kept and rest are discarded.\nSearch IndexI:X!RLshortlistsKlabels along with a score assigned to each shortlisted label\nfor a given input x. More speciﬁcally, ^y=I(x)is a sparse real valued vector with only K(\u001cL)\nnon-zero entries and ^y`6= 0 implies that label `is shortlisted for input xwith shortlist relevance\nscore ^y`. As illustrated in Figure 1, many partition based methods [ 18,33] formulate their index\nas a label tree derived by hierarchically partitioning the label space into Cclusters and then learn\nclassiﬁer vectors ^WC= [^wc]C\nc=1(^wc2RD) for each cluster which is used to select only a few\nclusters for a given input. More speciﬁcally, given the input x, the relevance of cluster cto input\nxis quantiﬁed by cluster relevance scores ^sc=^wT\nc\u001e(x). The top-bclusters based on these scores\nare selected and all labels inside the shortlisted clusters are returned as the shortlisted labels, where\nb(\u001cC)is a hyperparameter denoting the beam-size.\nLabel classiﬁers WL= [w`]L\n`=1where w`2RDrepresents the classiﬁer vector for label `and\nwT\n`\u001e(x)represents the label relevance score of label`for input x. As explained above, wT\n`\u001e(x)is\nonly computed for a few shortlisted labels obtained from the search index I.\n3.1 ELIAS Index\nELIAS formulates its label index as a specialized weighted graph between a root node ;,Ccluster\nnodesC=fcgC\nc=1andLlabel nodesY=f`gL\n`=1. As illustrated in Figure 2, all cluster nodes are\nconnected to the root node and all label nodes are sparsely connected to few cluster nodes. ELIAS\nparameterizes the cluster-to-label edge assignments by a learnable adjacency matrix A= [ac;`]C\u0002L,\nwhere the scalar parameter ac;`denotes the edge importance between cluster cand label`.\nNote that Acan be very large for XMC datasets and using a dense Awill incurO(CL)cost in\neach forward pass which can be computationally prohibitive for large-scale datasets. To mitigate\nthis we restrict Ato be a row-wise sparse matrix i.e. kaik0\u0014\u0014wherek:k0represents the `0norm,\nairepresents the ithrow of Aand\u0014is a hyper-parameter which controls the sparsity of A. During\ntraining, only the non-zero entries of Ais learned and the zero entries do not participate in any\ncalculation. We defer the details of how we initialize the sparsity structure of Ato Section 3.4.\nExisting partition based XMC methods can be thought of as a special case of this formulation by\nadding additional restrictions that 1) each label is connected to exactly one cluster node, and 2)\nall cluster-to-label connections have equal importance. Moreover, existing methods initialize the\ncluster-to-label adjacency matrix Abeforehand based on clustering over pre-deﬁned features and\nkeep it ﬁxed throughout the training procedure. ELIAS overcomes these shortcomings by enabling\nthe model to learn the cluster-to-label edge importance.\n3.2 Forward Pass\nELIAS trains the entire model, including the deep encoder \u001e, the search index parameters ^WC,A\nand the label classiﬁers WLin an end-to-end manner. We now describe the details of the forward\npass of ELIAS.\n4\n\nText representation : An input xis embedded by the encoder \u001einto a dense vector representation\n\u001e(x). In particular, we use BERT-base [ 11] as the encoder and represent \u001e(x)by the ﬁnal layer’s\nCLStoken vector.\nQuery search index : Recall that the goal of the search index Iis to efﬁciently compute a shortlist\nof labels ^y2RL, where ^yis a sparse real valued vector with K(\u001cL) non-zero entries and ^y`6= 0\nimplies that label `is shortlisted for input xwith shortlist score ^y`. Similar to existing methods,\nELIAS achieves this by ﬁrst shortlisting a small subset of clusters ^C\u001aC based on cluster relevance\nscores deﬁned by ^wT\nc\u001e(x). But unlike existing methods which simply return the union of the ﬁxed\nlabel set assigned to each shortlisted cluster, ELIAS shortlists the top- Klabels based on the soft\ncluster-to-label assignments and backpropagates the loss feedback to each of the shortlisted paths.\nMore speciﬁcally, ELIAS deﬁnes the cluster relevance scores ^sC2RCas:\n^sC= [^sc]C\nc=1= min(1;\u000b\u0003softmax( ^WT\nC\u001e(x))): (1)\nHere hyperparameter \u000bis multiplied by the softmax scores to allow multiple clusters to get high\nrelevance scores. Intuitively, \u000bcontrols how many effective clusters can simultaneously activate for a\ngiven input (in practice, we keep \u000b\u001910).\nGiven cluster relevance scores ^sC, we deﬁne setCtopbas the topbclusters with the highest cluster\nrelevance scores, where b(\u001cC) is the beam size hyperparameter. In the training phase, we further\ndeﬁne a parent set Cparent to guarantee that the correct labels of xare present in the shortlist. More\nspeciﬁcally, for each positive label of x, we include the cluster with the strongest edge connection to\nlinCparent . The shortlisted set ^Cis deﬁned as the union of these two sets and the selection process\ncan be summarized as follows:\nCtopb= arg top-b(^sC);whereb(\u001cC)is the beam size, (2)\nCparent =(\nfg during prediction,S\n`:y`=1farg maxc(ac;`)gduring training(3)\n^C=Ctopb[Cparent: (4)\nAfter shortlisting a small subset of clusters ^C, all potential edges of shortlisted clusters are explored\nand a set ^Pof explored paths is constructed, where ^P=f;!c!`:c2^Candac;`>0g.\nFurthermore, each path ;!c!`2^Pis assigned a path score ^s;;c;`, where the path score ^s;;c;`\nis expressed as the product of cluster relevance score ^sc(deﬁned by Eqn. 1) and edge score ^sc;`\nwhich quantiﬁes the probability of label `getting assigned to cluster cand is deﬁned in terms of the\nlearnable edge weight parameter ac;`as follows:\n^sc;`= min(1;\f\u0003anorm\nc;`);whereanorm\nc;` =exp(ac;`)PL\n`0=1exp(ac;`0);and^s;;c;`= ^sc\u0003^sc;`: (5)\n0 250 500 750 1000 1250 1500\nSorted label edge index 0.00.20.40.60.81.0Label edge score\nFigure 3:anorm\nc;` (edge weight) distribu-\ntion averaged over all clusters of trained\nELIAS model on Amazon-670K datasetDeﬁning edge scores ^sc;`in such a manner allows mod-\nelling the desired probability distribution of label assign-\nment to a cluster, where a few relevant labels are assigned\nto a particular cluster with probability 1, and all other la-\nbels have probability 0. Hyperparameter \fcontrols how\nmany effective labels can get assigned to a cluster, we\nchoose\f\u0019L=C . Figure 3 empirically conﬁrms that\nthe trained model indeed learns the desired edge score\ndistribution with most of the probability concentrated on\na few labels and the rest of the labels getting assigned\nlow probability. Moreover, this formulation also prevents\nlabels with high softmax scores from overpowering edge\nassignments because as per 5, a relevant label `for cluster\ncgets positive feedback for ac;`only ifanorm\nc;`<1=\f,\notherwiseac;`does not participate in the calculation of ^sc;`. This allows clusters to learn balanced\nlabel assignments. Note that, because of the assumption that Ais a row-wise sparse matrix, Eqn. 5\ncan be computed efﬁciently in O(\u0014)instead ofO(L)time.\n5\n\nSince there can be multiple paths in ^Pwhich reach a particular label `, ELIAS deﬁnes shortlist score\n^y`for label`by the maximum scoring path in ^Pthat reaches `, i.e.\n^y`= max\nc0f^s;;c0;`:;!c0!`2^Pg: (6)\nFinally, only the top- Kentries in ^yare retained and the resulting vector is returned as the shortlist\nfor input x.\nEvaluating label classiﬁers : label classiﬁers [w`]L\n`=1are evaluated for the Knon-zero labels in ^y\nand the ﬁnal relevance score between label `and input xis returned as p`=\u001b(wT\n`\u001e(x))\u0003^y`, here\u001b\nis the sigmoid function.\n3.3 Loss\nELIAS is trained on a combination of classiﬁcation and shortlist loss where the shortlist loss encour-\nages correct labels to have high shortlist scores ( ^y`) and classiﬁcation loss encourages positive labels\nin the shortlist to have high ﬁnal score ( p`) and negative labels in the shortlist to have low ﬁnal score.\nMore speciﬁcally, the ﬁnal loss Lis deﬁned asL=Lc+\u0015Ls, where\u0015is a hyperparameter and\nclassiﬁcation lossLcis deﬁned as binary cross entropy loss over shortlisted labels\nLc=\u0000X\n`:^y`6=0(y`log(p`) + (1\u0000y`)(1\u0000log(p`))); (7)\nshortlist lossLsis deﬁned as negative log likelihood loss over the positive labels\nLs=\u0000X\n`:y`=1log(^y`): (8)\n3.4 Staged Training\nPrevious sub-sections described the ELIAS framework for learning the index graph along with the\nML model in an end-to-end manner. Although, in principle one can optimize the network with the\ngiven loss function from a random initialization but we highlight a few key challenges in doing so: 1)\nOptimization challenge : because of the ﬂexibility in the network to assign a label node to various\nclusters, it becomes hard for a label to get conﬁdently assigned to only a few relevant clusters. As\na result, the model is always chasing a moving target and for a given input it is not able to be sure\nabout any single path; 2) Computational challenge : the full cluster-label adjacency matrix Acan be\nvery large for large datasets and will incur O(CL)cost in each forward pass if implemented in dense\nform. To address these challenges we train the ELIAS model in two stages. In the ﬁrst stage, we only\ntrain the encoder \u001e, cluster classiﬁers ^WC, and label classiﬁers WLkeepingAﬁxed and assigned\nbased on traditional balanced partitions. We then utilize the stage-1 trained model to initialize the\nsparse adjacency matrix A. In the second stage, we take the initialized Aand rest of the stage 1\nmodel, and jointly train the full model \u001e;^WC;WL;A.\nStage 1 : In stage 1 training, similar to existing partition-based XMC methods, we partition the label\nspace intoCmutually exclusive clusters by performing balanced k-means clustering over pre-deﬁned\nlabel features. The adjacency matrix induced by these clusters is then used as ﬁxed assignment\nforA. Keeping Aﬁxed, we train the rest of the model (i.e. \u001e;^WC;WL) on the loss described in\nSection 3.3. More details on clustering are provided in Section C.1 in the Appendix.\nInitializing A: As highlighted before, to overcome the O(CL)cost associated with a full adjacency\nmatrix A, we want to restrict Ato be a row-wise sparse matrix. In other words, we want to restrict\neach cluster to choose from a candidate subset of \u0014labels instead of the whole label set. Intuitively,\nin order for the model to learn anything meaningful, the candidate subset for each cluster should\ncontain approximately similar labels. To achieve this, we utilize the stage 1 model to ﬁrst generate an\napproximate adjacency matrix A0and then select the top- \u0014entries in each row of A0as non-zero\nentries for A. More speciﬁcally, we ﬁrst identify top- bmatched clusters for each training point xiby\ncomputing the cluster matching matrix M= [mi;c]N\u0002Cas:\nmi;c=\u001a^si\ncifc2Ci\ntopb,\n0 otherwise(9)\n6\n\nwhere ^si\ncrepresents the cluster relevance score and Ci\ntopbrepresents the set of top- bclusters for\nithtraining point xi. After computing M, we deﬁne the approximate adjacency matrix A0=\n[a0\nc;`]C\u0002L=MTY, where Y= [y1;:::;yi;:::;yN]T. The element a0\nc;`essentially denotes the\nweighted count of how many times the cluster cgot placed in top- bpositions for positive training\npoints of label `. Finally, the top \u0014elements in each row of A0are selected as the non-zero parameters\nofA, i.e.\nac;`=\u001arandom(0, 1) if`2arg top-\u0014(a0\nc)\n0 otherwise(10)\nWe choose a large enough \u0014to provide the model enough degree of freedom to learn cluster-to-\nlabel assignments. In particular, \u0014\u001810\u0002L=C works well across datasets without adding any\ncomputational burden. For efﬁcient implementation on GPUs, we store matrix Ain the form of two\ntensors, one storing the non-zero indices and the other storing the values corresponding to those\nnon-zero indices.\nStage 2 : In stage 2 training, we initialize Aas described above, and \u001e,^WCfrom stage 1 model.\nWe then train the full ELIAS model (i.e. \u001e;^WC;WL;A) end-to-end to optimize the loss deﬁned in\nSection 3.3.\n3.5 Sparse Ranker\nState-of-the-art XMC methods like XR-Transformer [ 33] and X-Transformer [ 7] utilize high capacity\nsparse classiﬁers learned on the concatenated sparse bag-of-word features and dense embedding\nobtained from the deep encoder for ranking their top predictions. Because of the high capacity, sparse\nclassiﬁers are able to represent head labels more elaborately than dense classiﬁers. Moreover, bag-of-\nwords representation is able to capture the full input document instead of the truncated document that\nthe deep encoder receives.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nDeciles (decreasing frequency)0.00.10.20.30.40.50.6ScoreSparse ranker\nELIAS(d)\nCalibrated score\nFigure 4: True label’s score distribu-\ntion of sparse ranker and ELIAS(d)\nover different label deciles on Amazon-\n670K dataset. 1stdecile represents la-\nbels with most training points while\n10thdecile represents labels with least\ntraining pointsTo compare fairly with such methods, we explore an enhanced\nvariant of ELIAS represented by ELIAS ++, which additionally\nlearns a sparse ranker that re-ranks the top 100 predictions of\nELIAS. In particular, the sparse ranker takes the concatenated\nsparse bag-of-word and dense embedding input features and\nlearns sparse linear classiﬁers on the top 100 label predictions\nmade from the trained ELIAS model for each training point.\nBecause these sparse classiﬁers are only trained on 100 labels\nper training point, they can be quickly trained by parallel lin-\near solvers like LIBLINEAR [ 12]. We use the open-source\nPECOS2[32] library to train and make predictions with the\nsparse ranker.\nDuring prediction, the top 100 predictions are ﬁrst made by\nELIAS and then the learned sparse ranker is evaluated on these\ntop 100 predictions. We empirically observe that the scores\nreturned by ELIAS and sparse ranker are not well calibrated\nacross different label regimes. As shown in Figure 4, the sparse\nranker underestimates scores on tail labels while ELIAS scores are more balanced across all label\nregimes. To correct this score mis-calibration, we learn a simple score calibration module which\nconsists of a standard decision tree classiﬁer3that takes both of these scores and the training frequency\nof the label as input and predicts a single score denoting the label relevance. The score calibration\nmodule is learned on a small validation set of 5,000 points. More details on the sparse ranker are in\nAppendix Section C.2.\n3.6 Time Complexity Analysis\nThe time complexity for processing a batch of ndata-points isO(n(Tbert+Cd+b\u0014+Kd))where\nTbertrepresents the time complexity of the bert encoder, Crepresents the number of clusters in index,\n2https://github.com/amzn/pecos\n3https://scikit-learn.org/stable/modules/generated/sklearn.tree.\nDecisionTreeClassifier.html\n7\n\nTable 1: Performance comparison on extreme classiﬁcation benchmark datasets. Bold numbers represent\noverall best numbers for that dataset while underlined numbers represent best numbers for dense embedding\nbased methods. Methods which only use sparse bag-of-word features are distinguished by(s)superscript, dense\nembedding based methods are distinguished by(d)superscript and methods that use both sparse + dense features\nare distinguished by(s+d)superscript\nMethod P@1 P@3 P@5 PSP@1 PSP@3 PSP@5 P@1 P@3 P@5 PSP@1 PSP@3 PSP@5\nAmazon-670K LF-AmazonTitles-131K\nDiSMEC(s)44.70 39.70 36.10 27.80 30.60 34.20 35.14 23.88 17.24 25.86 32.11 36.97\nParabel(s)44.89 39.80 36.00 25.43 29.43 32.85 32.60 21.80 15.61 23.27 28.21 32.14\nXR-Linear(s)45.36 40.35 36.71 - - - - - - - - -\nBonsai(s)45.58 40.39 36.60 27.08 30.79 34.11 34.11 23.06 16.63 24.75 30.35 34.86\nSlice(d)33.15 29.76 26.93 20.20 22.69 24.70 30.43 20.50 14.84 23.08 27.74 31.89\nAstec(d)47.77 42.79 39.10 32.13 35.14 37.82 37.12 25.20 18.24 29.22 34.64 39.49\nGLaS(d)46.38 42.09 38.56 38.94 39.72 41.24 - - - - - -\nAttentionXML(d)47.58 42.61 38.92 30.29 33.85 37.13 32.55 21.70 15.64 23.97 28.60 32.57\nLightXML(d)49.10 43.83 39.85 - - - 38.49 26.02 18.77 28.09 34.65 39.82\nXR-Transformer(s+d)50.11 44.56 40.64 36.16 38.39 40.99 38.42 25.66 18.34 29.14 34.98 39.66\nOverlap-XMC(s+d)50.70 45.40 41.55 36.39 39.15 41.96 - - - - - -\nELIAS(d)50.63 45.49 41.60 32.59 36.44 39.97 39.14 26.40 19.08 30.01 36.09 41.07\nELIAS ++(s+d)53.02 47.18 42.97 34.32 38.12 41.93 40.13 27.11 19.54 31.05 37.57 42.88\nWikipedia-500K Amazon-3M\nDiSMEC(s)70.21 50.57 39.68 31.20 33.40 37.00 47.34 44.96 42.80 - - -\nParabel(s)68.70 49.57 38.64 26.88 31.96 35.26 47.48 44.65 42.53 12.82 15.61 17.73\nXR-Linear(s)68.12 49.07 38.39 - - - 47.96 45.09 42.96 - - -\nBonsai(s)69.20 49.80 38.80 - - - 48.45 45.65 43.49 13.79 16.71 18.87\nSlice(d)62.62 41.79 31.57 24.48 27.01 29.07 - - - - - -\nAstec(d)73.02 52.02 40.53 30.69 36.48 40.38 - - - - - -\nGLaS(d)69.91 49.08 38.35 - - - - - - - - -\nAttentionXML(d)76.95 58.42 46.14 30.85 39.23 44.34 50.86 48.04 45.83 15.52 18.45 20.60\nLightXML(d)77.78 58.85 45.57 - - - - - - - - -\nXR-Transformer(s+d)79.40 59.02 46.25 35.76 42.22 46.36 54.20 50.81 48.26 20.52 23.64 25.79\nOverlap-XMC(s+d)- - - - - - 52.70 49.92 47.71 18.79 21.90 24.10\nELIAS(d)79.00 60.37 46.87 33.86 42.99 47.29 51.72 48.99 46.89 16.05 19.39 21.81\nELIAS ++(s+d)81.26 62.51 48.82 35.02 45.94 51.13 54.28 51.40 49.09 15.85 19.07 21.52\ndis the embedding dimension, bis the beam size, \u0014is the row-wise sparsity of cluster-to-label\nadjacency matrix A, andKis the number of labels shortlisted for classiﬁer evaluation. Assuming\nC=O(p\nL),\u0014=O(L=C) =O(p\nL)andK=O(p\nL), the ﬁnal time complexity comes out to\nbeO(n(Tbert+p\nL(2d+b))). Empirical prediction and training times on benchmark datasets are\nreported in Table 6 of the Appendix.\n4 Experimental Results\nExperimental Setup We conduct experiments on three standard full-text extreme classiﬁcation\ndatasets: Wikipedia-500K, Amazon-670K, Amazon-3M and one short-text dataset: LF-AmazonTitles-\n131K which only contains titles of Amazon products as input text. For Wikipedia-500K, Amazon-\n670K, and Amazon-3M, we use the same experimental setup (i.e. raw input text, sparse features\nand train-test split) as existing deep XMC methods [ 31,33,18,7]. For LF-AmazonTitles-131K,\nwe use the experimental setup provided in the extreme classiﬁcation repository [ 5]. Comparison to\nexisting XMC methods is done by standard evaluation metrics of precision@ K(P@K= 1;3;5)\nand its propensity weighted variant (PSP@ K= 1;3;5) [15]. We also compare competing methods\nand baselines with ELIAS at recall@ K(R@K= 10;20;100) evaluation to illustrate the superior\nshortlisting performance of ELIAS’s search index. More details on the experimental setup and dataset\nstatistics are presented in Appendix Section B.\nImplementation details Similar to existing XMC methods, we take an ensemble of 3 models with\ndifferent initial clustering of label space to report ﬁnal numbers. For efﬁcient implementation on GPU,\nthe raw input sequence is concatenated to 128 tokens for full-text datasets and 32 tokens for short-text\ndataset. Number of clusters Cfor each dataset is chosen to be the same as LightXML which selects\nC\u0018L=100. We keep the shortlist size hyperparameter Kﬁxed to 2000 which is approximately\nsame as the number of labels existing partition based methods shortlist assuming beam-size b= 20\n8\n\nTable 2: Precision and recall comparison of single model dense embedding-based methods. ELIAS matches or\neven outperforms the brute-force Bert-OvA baseline while existing partition based methods fail to compare well,\nespecially at recall values.\nMethod P@1 P@3 P@5 R@10 R@20 R@100 P@1 P@3 P@5 R@10 R@20 R@100\nAmazon-670K LF-AmazonTitles-131K\nBERT-OvA-1(d)48.50 43.41 39.67 49.53 56.60 67.90 38.17 25.66 18.44 50.29 54.71 62.80\nAttentionXML-1(d)45.84 40.92 37.24 45.59 51.25 60.77 30.26 20.03 14.31 38.16 41.47 47.73\nLightXML-1(d)47.29 42.24 38.48 47.34 53.26 62.03 37.01 24.88 17.90 48.07 52.10 59.42\nXR-Transformer-1(d)45.25 40.3 36.45 45.19 51.61 61.11 34.58 23.31 16.79 45.72 49.65 56.00\nELIAS-1(d)48.68 43.78 40.04 50.33 57.67 68.95 37.90 25.61 18.45 50.12 54.62 62.88\nTable 3: Performance analysis of different components of ELIAS. Allowing the model to learn cluster-to-label\nassignments signiﬁcantly improves both precision and recall performance (see row 2 vs row 1). Sparse ranker\nfurther improves performance on top predictions (see row 4 vs row 2).\nMethod P@1 P@3 P@5 R@10 R@20 R@100 P@1 P@3 P@5 R@10 R@20 R@100\nAmazon-670K LF-AmazonTitles-131K\nStage 1 46.63 41.65 37.58 46.08 52.29 61.72 36.96 24.67 17.69 47.69 51.74 58.81\n+ Stage 2 48.68 43.78 40.04 50.33 57.67 68.95 37.90 25.61 18.45 50.12 54.62 62.88\n+ Sparse ranker w/o calibration 50.72 45.25 41.27 51.51 58.43 68.95 39.25 26.47 19.02 51.4 55.39 62.88\n+ Score calibration 51.41 45.69 41.62 51.97 58.81 68.95 39.26 26.47 19.02 51.4 55.35 62.88\n+ 3\u0002ensemble 53.02 47.18 42.97 53.99 61.33 72.07 40.13 27.11 19.54 53.31 57.78 65.15\nand the number of labels per cluster = 100 . AdamW [ 20] optimizer is used to train the whole model\nwith weight decay applied only to non-gain and non-bias parameters. Optimization update for label\nclassiﬁers WLis performed with high accumulation steps (i.e. optimization update is performed at\neveryktraining steps, where k= 10 ) since updating WLevery step is a computational bottleneck\nand only few parameters inside WLgets updated in each optimization step anyway. More details\nand hyperparameters for each dataset are presented in Appendix Section B.\nComparison on XMC benchmarks Table 1 compares our method with leading XMC methods\nsuch as DiSMEC [ 2], Parabel [ 25], XR-Linear [ 32], Bonsai [ 19], Slice [ 16], Astec [ 9], GlaS [ 13],\nAttentionXML [ 31], LightXML [ 18], XR-Transformer [ 33], and Overlap-XMC [ 22]. Most baseline\nresults are obtained from their respective papers when available and otherwise taken from results\nreported in [ 31,33] and extreme classiﬁcation repository [ 5]. To allow fair comparison among\nmethods that use the same form of input representation, we distinguish methods that use only sparse\nbag-of-word input features by(s)superscript, methods that use only dense embedding based input\nfeatures by(d)superscript, and methods that use both sparse + dense features by(s+d)superscript.\nELIAS ++ which uses sparse + dense features achieves state-of-the-art performance on all datasets at\nprecision values while being either the best or second best method at propensity scored precision\non most datasets. The dense embedding based ELIAS(d)consistently outperforms existing dense\nembedding based XMC methods by signiﬁcant margin and on many occasions achieves gains over\nprevious state-of-the-art methods which use both sparse + dense features.\nComparison with brute-force OvA baseline To establish the classiﬁcation performance that could\nhave been achieved if there was no sampling performed by the shortlisting procedure, we implement\nthe brute-force one-versus-all baseline BERT-OvA which consists of BERT encoder followed by a\nfully connected linear classiﬁcation layer and is trained and inferred in one-versus-all fashion without\nany sampling. We follow the same training procedures as ELIAS for this baseline. Table 2 compares\nthe OvA baseline with ELIAS and leading deep XMC methods such as AttentionXML, LightXML\nand a dense version of XR-Transformer which uses only dense embeddings, under single model\n(i.e. no ensemble) setup for direct comparison. Existing deep XMC methods do not compare well\nagainst the OvA model especially at recall@100 but ELIAS matches and sometimes even marginally\noutperforms, the brute-force OvA baseline while enjoying faster training and inference speed due to\nthe search index.\nComponent wise ablation of ELIAS Table 3 presents a build-up ablation of performance gains\nmade by different components of ELIAS. The stage 1 model which ﬁxes its adjacency matrix by\nclustering labels into mutually exclusive clusters performs similarly to existing single model XMC\nmethods. Allowing the model to learn the adjacency matrix Ain stage 2 improves recall by up to 7%\nand precision by up to 2.5% over the stage 1 model. Adding the sparse ranker and score-calibration\n9\n\nmodule further improves model performance on top predictions but the gains diminish as we increase\nprediction set size. Finally, the ensemble of 3 models improves performance at all evaluation metrics\nwhich is a well observed behaviour with all XMC methods.\n20 40 60 80 100\nK3540455055606570RecallELIaS-1++(s+d)\nELIaS-1(d)\nXR-Transformer-1(s+d)\nLightXML-1(d)\nXR-Transformer-1(d)\nAttentionXML-1(d)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nDeciles (decreasing frequency)0246810Contribution to R@100ELIAS-1(d)\nXR-Transformer-1(s+d)\nLightXML-1(d)\nAttentionXML-1(d)\nFigure 5: (left) Comparison of recall at different prediction set size on Amazon-670K ( right ) Decile-wise\nanalysis of recall@100 on Amazon-670K, 1stdecile represents labels with most training points i.e. head labels\nwhile 10thdecile represents labels with least training points i.e. tail labels\nRecall comparison Next, we compare the recall capabilities of existing methods with ELIAS. The\nleft plot in Figure 5 plots the recall at different prediction set size for all competing methods and\nELIAS. ELIAS strictly outperforms existing methods at all prediction set sizes and in particular, can\nbe up to 4% better at recall@ 100than the next best method. To further investigate which label regimes\nbeneﬁt most from ELIAS’s search index we plot the decile wise contribution to recall@100 for each\nmethod. As we can see, ELIAS improves recall performance over existing methods in each label\ndecile but the most improvement come from the top 2 deciles representing the most popular labels.\nWe hypothesize that because the popular labels are likely to have multi-modal input distribution,\nexisting partition based methods which assign a label to only one cluster fail to perform well on these\nmulti-modal labels. Section C.4 contains additional discussion and results to support this claim.\n5 Conclusion and Discussions\nIn this paper, we propose ELIAS, which extends the widely used partition tree based search index to a\nlearnable graph based search index for extreme multi-label classiﬁcation task. Instead of using a ﬁxed\nsearch index, ELIAS relaxes the discrete cluster-to-label assignments in the existing partition based\napproaches as soft learnable parameters. This enables the model to learn a ﬂexible index structure,\nand it allows the search index to be learned end-to-end with the encoder and classiﬁcation module\nto optimize the ﬁnal task objective. Empirically, ELIAS achieves state-of-the-art performance on\nseveral large-scale extreme classiﬁcation benchmarks with millions of labels. ELIAS can be up to\n2.5% better at precision@ 1and up to 4% better at recall@ 100than existing XMC methods.\nThis work primarily explores many-shot and few-shot scenarios where some training supervision\nis available for each label (output). It would be interesting to see how we can adapt the proposed\nsolution to zero-shot scenarios where there is no training supervision available for the labels. One\npotential approach could be to parameterize the cluster-to-label adjacency matrix as a function of\ncluster and label features instead of free learnable scalars. Furthermore, one limitation of the proposed\nsolution is that it learns a shallow graph structure over label space; this may not be ideal for scaling\nthe method to billion-scale datasets. It would be exciting to explore how one can extend ELIAS to\nlearn deep graph structures.\n10\n\nReferences\n[1]H. Abu-Libdeh, D. Altinbüken, A. Beutel, E. Chi, L. Doshi, T. Kraska, X. Li, A. Ly, and\nC. Olston. Learned indexes for a google-scale disk-based database. CoRR , abs/2012.12501,\n2020. URL https://arxiv.org/abs/2012.12501 .\n[2]R. Babbar and B. Schölkopf. DiSMEC: Distributed Sparse Machines for Extreme Multi-label\nClassiﬁcation. In WSDM , 2017.\n[3]R. Babbar and B. Schölkopf. Data scarcity, robustness and extreme multi-label classiﬁcation.\nML, 2019.\n[4]S. Bengio, K. Dembczynski, T. Joachims, M. Kloft, and M. Varma. Extreme Classiﬁcation\n(Dagstuhl Seminar 18291). Dagstuhl Reports , 8(7):62–80, 2019. ISSN 2192-5283. doi: 10.\n4230/DagRep.8.7.62. URL http://drops.dagstuhl.de/opus/volltexte/2019/10173 .\n[5]K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y . Prabhu, and M. Varma. The extreme\nclassiﬁcation repository: Multi-label datasets and code, 2016. URL http://manikvarma.\norg/downloads/XC/XMLRepository.html .\n[6]C. W. Chang, H. F. Yu, K. Zhong, Y . Yang, and I. S. Dhillon. A Modular Deep Learning\nApproach for Extreme Multi-label Text Classiﬁcation. CoRR , 2019.\n[7]W.-C. Chang, Y . H.-F., K. Zhong, Y . Yang, and I.-S. Dhillon. Taming Pretrained Transformers\nfor Extreme Multi-label Text Classiﬁcation. In KDD , 2020.\n[8]W.-C. Chang, F.-X. Yu, Y .-W. Chang, Y . Yang, and S. Kumar. Pre-training Tasks for Embedding-\nbased Large-scale Retrieval. In ICLR , 2020.\n[9]K. Dahiya, D. Saini, A. Mittal, A. Shaw, K. Dave, A. Soni, H. Jain, S. Agarwal, and M. Varma.\nDeepXML: A Deep Extreme Multi-Label Learning Framework Applied to Short Text Docu-\nments. In WSDM , 2021.\n[10] K. Dahiya, N. Gupta, D. Saini, A. Soni, Y . Wang, K. Dave, J. Jiao, G. K, P. Dey, A. Singh,\nD. Hada, V . Jain, B. Paliwal, A. Mittal, S. Mehta, R. Ramjee, S. Agarwal, P. Kar, and M. Varma.\nNgame: Negative mining-aware mini-batching for extreme classiﬁcation. arXiv , 2022. doi:\n10.48550/ARXIV .2207.04452. URL https://arxiv.org/abs/2207.04452 .\n[11] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. NAACL , 2019.\n[12] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Lib- linear: a library for large\nlinear classiﬁcation. In Journal of machine learning research , 2008.\n[13] C. Guo, A. Mousavi, X. Wu, D.-N. Holtmann-Rice, S. Kale, S. Reddi, and S. Kumar. Breaking\nthe Glass Ceiling for Embedding-Based Classiﬁers for Large Output Spaces. In NeurIPS , 2019.\n[14] N. Gupta, S. Bohra, Y . Prabhu, S. Purohit, and M. Varma. Generalized zero-shot extreme\nmulti-label learning. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining , August 2021.\n[15] H. Jain, Y . Prabhu, and M. Varma. Extreme Multi-label Loss Functions for Recommendation,\nTagging, Ranking and Other Missing Label Applications. In KDD , August 2016.\n[16] H. Jain, V . Balasubramanian, B. Chunduri, and M. Varma. Slice: Scalable Linear Extreme\nClassiﬁers trained on 100 Million Labels for Related Searches. In WSDM , 2019.\n[17] V . Jain, N. Modhe, and P. Rai. Scalable Generative Models for Multi-label Learning with\nMissing Labels. In ICML , 2017.\n[18] T. Jiang, D. Wang, L. Sun, H. Yang, Z. Zhao, and F. Zhuang. LightXML: Transformer with\nDynamic Negative Sampling for High-Performance Extreme Multi-label Text Classiﬁcation. In\nAAAI , 2021.\n11\n\n[19] S. Khandagale, H. Xiao, and R. Babbar. Bonsai: Diverse and shallow trees for extreme multi-\nlabel classiﬁcation. Mach. Learn. , 109(11):2099–2119, nov 2020. ISSN 0885-6125. doi:\n10.1007/s10994-020-05888-2. URL https://doi.org/10.1007/s10994-020-05888-2 .\n[20] P. D. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. 2014.\n[21] T. Kraska, A. Beutel, E. Chi, J. Dean, and N. Polyzotis. The case for learned index structures.\nSIGMOD , 2018.\n[22] X. Liu, W.-C. Chang, H.-F. Yu, C.-J. Hsieh, and I. S. Dhillon. Label Disentanglement in\nPartition-based Extreme Multilabel Classiﬁcation. In NeurIPS , 2021.\n[23] A. Y . Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search\nusing Hierarchical Navigable Small World graphs. TPAMI , 2016.\n[24] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of\nWords and Phrases and Their Compositionality. In NIPS , 2013.\n[25] Y . Prabhu, A. Kag, S. Harsola, R. Agrawal, and M. Varma. Parabel: Partitioned label trees for\nextreme classiﬁcation with application to dynamic search advertising. In WWW , 2018.\n[26] Y . Prabhu, A. Kusupati, N. Gupta, and M. Varma. Extreme Regression for Dynamic Search\nAdvertising. In WSDM , 2020.\n[27] Y . Tay, V . Q. Tran, M. Dehghani, J. Ni, D. Bahri, H. Mehta, Z. Qin, K. Hui, Z. Zhao, J. Gupta,\nT. Schuster, W. W. Cohen, and D. Metzler. Transformer memory as a differentiable search index.\narXiv , 2022. URL https://arxiv.org/abs/2202.06991 .\n[28] T. Wei, W. W. Tu, and Y . F. Li. Learning for Tail Label Data: A Label-Speciﬁc Feature Approach.\nInIJCAI , 2019.\n[29] L. Xiong, C. Xiong, Y . Li, K.-F. Tang, J. Liu, P. Bennett, J. Ahmed, and A. Overwijk. Approx-\nimate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint\narXiv:2007.00808 , 2020.\n[30] E. I. Yen, X. Huang, W. Dai, P. Ravikumar, I. Dhillon, and E. Xing. PPDSparse: A Parallel\nPrimal-Dual Sparse Method for Extreme Classiﬁcation. In KDD , 2017.\n[31] R. You, S. Dai, Z. Zhang, H. Mamitsuka, and S. Zhu. AttentionXML: Extreme Multi-Label\nText Classiﬁcation with Multi-Label Attention Based Recurrent Neural Networks. In NeurIPS ,\n2019.\n[32] H.-F. Yu, K. Zhong, J. Zhang, W.-C. Chang, and I. S. Dhillon. Pecos: Prediction for enormous\nand correlated output spaces. Journal of Machine Learning Research , 2022.\n[33] J. Zhang, W.-C. Chang, H.-F. Yu, and I. S. Dhillon. Fast multi-resolution transformer ﬁne-tuning\nfor extreme multi-label text classiﬁcation. In NeurIPS , 2021.\n12\n\nChecklist\n1. For all authors...\n(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] See Section 5\n(c)Did you discuss any potential negative societal impacts of your work? [Yes] See\nSection A in Appendix\n(d)Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n(b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments...\n(a)Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes] Available at\nhttps://github.com/nilesh2797/ELIAS\n(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes] See Section B in Appendix\n(c)Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [No]\n(d)Did you include the total amount of compute and the type of resources used (e.g.,\ntype of GPUs, internal cluster, or cloud provider)? [Yes] See section Section C.4 in\nAppendix\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes]\n(b) Did you mention the license of the assets? [N/A]\n(c)Did you include any new assets either in the supplemental material or as a URL? [No]\n(d)Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? [N/A]\n(e)Did you discuss whether the data you are using/curating contains personally identiﬁable\ninformation or offensive content? [N/A]\n5. If you used crowdsourcing or conducted research with human subjects...\n(a)Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A]\n(b)Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c)Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\n13\n\nA Potential Negative Societal Impact\nOur method proposes to learn efﬁcient data structure for accurate prediction in large-output space. It\nhelps existing large-scale retrieval systems used in various online applications to efﬁciently produce\nmore accurate results. To the best of our knowledge, this poses no negative impacts on society.\nB Experimental Details\nB.1 ELIAS Hyperparameters\nELIAS’s hyperparameters include,\n•max-len : denotes the maximum sequence length of input for the BERT encoder. As per standard\nXMC practices, for full-text dataset we choose 128 and for short-text we choose 32\n•C: denotes number of clusters in the index graph, we use same values as LightXML [ 18] and\nX-Transformer [6] for fair comparison\n•\u000b: multiplicative hyperparameter used in Equation 1, controls effective number of clusters that can\nget activated for a given input\n•\f: multiplicative hyperparameter used in Equation 5, controls effective number of labels that can\nget assigned to a particular cluster\n•\u0014: controls the row-wise sparsity of adjacency matrix A, we choose \u0014\u001910\u0002L=C\n•\u0015: controls importance of classiﬁcation loss Lcand shortlist lossLsin the ﬁnal lossL, we choose\n\u0015by doing grid search over the smallest dataset LF-AmazonTitles-131K\n•K: denotes the shortlist size, label classiﬁers are only evaluated on top- Kshortlisted labels. We\nchooseK= 2000 which is approximately same as the number of labels existing partition based\nmethods shortlist assuming beam-size b= 20 and number of labels per cluster = 100\n•b: denotes the beam size, similar to existing partition based methods we use b= 20\n•num-epochs : denotes the total number of epochs (i.e. including stage 1 and stage 2 training)\n•LRW,LR\u001e: We empirically observe that the network trains faster when we decouple the initial\nlearning rates of the transformer encoder ( LR\u001e) with rest of the model ( LRW). We choose a much\nsmaller values for LR\u001eand a relatively larger value for LRW\n•bsz: denotes the batch-size of the mini-batches used during training\nTable 4: ELIAS hyperparameters\nDataset max-len C \u000b \f \u0014 \u0015 K b num-epochs LRW LR\u001ebsz\nLF-AmazonTitles-131K 32 2048 10 150 1000 0 .05 2000 20 60 0 :02 1e\u00004512\nAmazon-670K 128 8192 10 150 1000 0 :05 2000 20 60 0 :01 1e\u00004256\nWikipedia-500K 128 8192 10 150 1000 0 :05 2000 20 45 0 :005 5e\u00005256\nAmazon-3M 128 32768 20 150 1000 0 :05 2000 20 45 0 :002 2e\u0000564\nB.2 Datasets\nLF-AmazonTitles-131K : A product recommendation dataset where input is the title of the product\nand labels are other related products to the given input. “LF-*” datasets additionally contain label\nfeatures i.e. a label is not just an atomic id, label features which describe a label are also given. For\nthis paper, we don’t utilize these additional label features and compare ELIAS to only methods which\ndon’t utilize label features either. Notably, even though ELIAS doesn’t use label features it achieves\nvery competitive performance with methods which use the label features in their model.\nAmazon-670K : A product recommendation dataset where input is a textual description of a query\nproduct and labels are other related products for the query.\nWikipedia-500K : A document tagging dataset where input consists of full text of a wikipedia page\nand labels are wikipedia tags relevant to that page.\n14\n\n[ROOT]\nClusters\nLabelsFixed Label Assignments\nStage  \n[ROOT]\nClusters\nInitialize   \n[ROOT]\nClusters\nStage  Labels\nLabelsFigure 6: Illustration of ELIAS’s search index graph in different training stages.\nAmazon-3M : A product recommendation dataset where input is a textual description of a query\nproduct and labels are other co-purchased products for the query.\nB.3 Evaluation Metrics\nWe use standard Precision@ K(P@K), propensity weighted variant of Precision (PSP@ K), and\nRecall@K(R@K) evaluation metrics for comparing ELIAS to baseline methods. For a single\n15\n\nTable 5: Dataset statistics, here Dbowdenotes the dimensionality of sparse bag-of-word features\nDataset Num Train Points Num Test Points Num Labels Avg. Labels per Point Avg. Points per Label Dbow\nLF-AmazonTitles-131K 294,805 134,835 131,073 2.29 5.15 40,000\nAmazon-670K 490,449 153,025 670,091 3.99 5.45 135,909\nWikipedia-500K 1,779,881 769,421 501,070 4.75 16.86 2,381,304\nAmazon-3M 1,717,899 742,507 2,812,281 22.02 36.06 337,067\ndata-pointi, these evaluation metrics can be formally deﬁned as:\nP@K=1\nKKX\nj=1yi\nrank (j) (11)\nPSP@K=KX\nj=1yi\nrank (j)\nprank (j)(12)\nR@K=1\nkyik0KX\nj=1yi\nrank (j) (13)\nWhere, yi= [yi\nl]L\nl=1;yl2f0;1grepresents the ground truth label vector, prepresents the propensity\nscore vector [ 17],k:k0represents the `0norm, andrank (j)denotes the index of jthhighest ranked\nlabel in prediction vector of input i.\nC More on ELIAS\nC.1 Additional Training Details\nFigure 6 illustrates the evolution of ELIAS’s search index graph over different stages of training.\nIn stage 1, label to cluster assignments are pre-determined and ﬁxed by clustering all labels into C\nclusters. Then, rest of the ML model i.e. \u001e;WC;WLis trained. The model obtained after stage 1\ntraining is used to initialize the row-wise sparse adjacency matrix Aas described in Section 3.4. In\nstage 2, the non-zero entries in the sparse adjacency matrix Aalong with the rest of the ML model\nis trained jointly to optimize the task objective. The clustering procedure used in stage 1 can be\ndescribed as follows:\nWe ﬁrst obtain a static representation  (xi)for each training point xias:\n (xi) = [bow(xi)\nkbow(xi)k2;\u001e(xi)\nk\u001e(xi)k2] (14)\nHere, [ ]represents the concatenation operator, bow(xi)represents the sparse bag-of-words repre-\nsentation of xiand\u001e(xi)represents the deep encoder representation of xi. Next we deﬁne label\ncentroids\u0016lfor each label as:\n\u0016l=P\ni:yi\nl=1 (xi)\nkP\ni:yi\nl=1 (xi)k2(15)\nWe then cluster all labels into Cclusters by recursively performing balanced 2-means [ 25] over label\ncentroidsf\u0016lgL\nl=1. This gives us a clustering matrix C2RC\u0002L, where Cc;l= 1 iff labellgot\nassigned to cluster c. Note that, a label is assigned to only one cluster and each cluster gets assigned\nequal number of labels. We assign this clustering matrix Cto the label-cluster adjacency matrix A\nand keep it frozen during the stage 1 training i.e. only parameters \u001e;WC;WLare trained on the loss\ndeﬁned in Section 3.3.\nC.2 Additional Sparse Ranker Details\nIn this subsection we describe the training and prediction procedure of sparse ranker in more detail.\nTraining Sparse Ranker : Let \u0016Yi=f\u0016yi\njg100\nj=1denote the set of top 100 predictions made by trained\nELIAS model for training point xi. Similar to the representation used for clustering label space in\n16\n\nTable 6: Empirical prediction time, training time, and model sizes on benchmark datasets\nDataset Prediction (1 GPU) Training (1 GPU) Training (8 GPU) Model Size\nLF-AmazonTitles-131K 0.08 ms/pt 1.66 hrs 0.33 hrs 0.65 GB\nWikipedia-500K 0.55 ms/pt 33.3 hrs 6.6 hrs 2.0 GB\nAmazon-670K 0.57 ms/pt 10.1 hrs 2.1 hrs 2.4 GB\nAmazon-3M 0.67 ms/pt 37.6 hrs 7.5 hrs 5.9 GB\nstage 1 training, sparse ranker represents the input xiwith the static representation  (xi)as:\n (xi) = [bow(xi)\nkbow(xi)k2;\u001e(xi)\nk\u001e(xi)k2] (16)\nIt learns sparse linear classiﬁers \u0016W=f\u0016wlgL\nl=1, where \u0016wl2RD0andD0is the dimensionality of  ,\non loss \u0016Ldeﬁned as following:\n\u0016L=\u0000NX\ni=1X\nl2\u0016Yi(yi\nllog(\u001b(\u0016wT\nl (xi))) + (1\u0000yi\nl)(1\u0000\u001b(\u0016wT\nl (xi)))) (17)\nBecause these classiﬁers are only trained on O(100) labels per point, the complexity of \u0016Lis only\nO(100\u0002N). Such sparse linear classiﬁers can be efﬁciently trained with second order parallel\nlinear solvers like LIBLINEAR [ 12] on CPU. In particular, even on the largest Amazon-3M dataset\nwith 3 million labels, training sparse ranker only takes about an hour on a standard CPU machine\nwith 48 cores.\nPredicting with Sparse Ranker : Similar to training, we ﬁrst get top 100 predictions \u0016Yifrom ELIAS\nmodel for each data point xi. Sparse classiﬁers are evaluated on each (xi;l)pair wherel2\u0016Yi. Let\nthe score of ELIAS for the pair (xi;l)bepi\nland score of sparse ranker be qi\nl=\u001b(\u0016wT\nl (xi)). Ideally\nwe would like the ﬁnal score to be some combination of pi\nlandqi\nlbut as observed in Section 3.5,\nthese two scores are not very well calibrated across different label regimes. To correct this issue, we\nlearn a score calibration module Twhich consists of a standard decision tree classiﬁer4trained on a\nsmall validation set of 5000 data points. In particular, let the validation set be f(xi;yi)g5000\ni=1and\u0016Yi\ndenote the set of top 100 predictions made by ELIAS on validation point xi. Training data points for\nthe score calibration module consists of all pairsS5000\ni=1S\nl2\u0016Yi(xi;l), where the input vector of a data\npoint is a 4 dimensional vector (pi\nl;qi\nl;pi\nl\u0003qi\nl;fl)and the target output is yi\nl. Here,fldenotes the\ntraining frequency (i.e. number of training points) of label l. During prediction, the ﬁnal score for a\npair(xi;l)is returned asT(pi\nl;qi\nl;pi\nl\u0003qi\nl;fl) +pi\nl\u0003qi\nl.\nC.3 Practical Implementation and Resources Used\nMany of the design choices for ELIAS’s formulation is made to enable efﬁcient implementation of the\nsearch index on GPU. For example, the row-wise sparsity constraint allows storing and operating the\nsparse adjacency matrix as two 2D tensors, which is much more efﬁcient to work with on a GPU than\na general sparse matrix. We implement the full ELIAS model excluding the sparse ranker component\nin PyTorch. Sparse ranker is implemented using LIBLINEAR utilities provided in PECOS5library.\nAll experiments are run on a single A6000 GPU. Even on the largest dataset Amazon-3M with 3\nmillion labels, prediction latency of single ELIAS model is about 1 ms per data point and training\ntime is 50 hours.\nC.4 Additional Results\nTable 7a reports the ﬁnal accuracy numbers with different \u0015on Amazon-670K dataset. With a very\nsmall\u0015the loss only focuses on the classiﬁcation objective which leads to signiﬁcantly worse R@100\nperformance. Increasing \u0015improves the overall performance up to a certain point, after that the\nperformance saturates and starts degrading slowly. Table 7b reports the effect of choosing different \u0014\n4https://scikit-learn.org/stable/modules/generated/sklearn.tree.\nDecisionTreeClassifier.html\n5https://github.com/amzn/pecos\n17\n\n(row-wise sparsity parameter) to the ﬁnal model performance on Amazon-670K dataset. We notice\nthat the model performance increases up to a certain value of \u0014, after that the model performance\n(specially P@1) saturates and starts degrading slowly.\nTable 7: ELIAS-1(d)results on Amazon-670K with (a) varying \u0015, (b) varying \u0014\n(a)\n\u0015 P@1 P@5 R@10 R@100\n0 47.80 39.45 49.17 66.05\n0.01 48.30 39.86 49.73 67.78\n0.02 48.48 39.94 49.96 68.27\n0.05 48.68 40.05 50.33 68.95\n0.1 48.72 40.05 50.19 68.91\n0.2 48.62 39.96 50.06 68.82\n0.5 48.48 39.76 49.80 68.55(b)\n\u0014 P@1 P@5 R@10 R@100\n100 46.79 36.60 42.90 56.38\n200 47.88 38.67 46.96 63.30\n500 48.68 40.04 49.99 68.48\n1000 48.68 40.05 50.33 68.95\n2000 48.58 40.07 50.27 68.91\n5000 48.57 39.93 50.15 68.91\n10000 48.32 39.73 49.97 68.84\nDue to lack of space in the main paper, the full component ablation table is reported here in Table 8\nMethod P@1 P@3 P@5 nDCG@3 nDCG@5 PSP@1 PSP@3 PSP@5 R@10 R@20 R@100\nLF-AmazonTitles-131K\nStage 1 36.96 24.67 17.69 37.47 39.21 28.29 33.16 37.44 47.69 51.74 58.81\n+ Stage 2 37.90 25.61 18.45 38.83 40.76 29.73 35.16 39.88 50.12 54.62 62.88\n+ Sparse ranker w/o calibration 39.25 26.46 19.02 40.22 42.19 30.54 36.71 41.72 51.40 55.39 62.88\n+ Score correction 39.26 26.47 19.02 40.27 42.23 31.30 37.05 41.89 51.40 55.35 62.88\n+ 3\u0002ensemble 40.13 27.11 19.54 41.26 43.35 31.05 37.57 42.88 53.31 57.79 65.15\nAmazon-670K\nStage 1 46.63 41.65 37.58 44.02 42.11 29.89 33.20 35.66 46.08 52.29 61.72\n+ Stage 2 48.68 43.78 40.04 46.24 44.68 31.22 34.94 38.31 50.33 57.67 68.95\n+ Sparse ranker w/o calibration 50.72 45.25 41.27 47.91 46.22 30.93 35.45 39.57 51.51 58.43 68.95\n+ Score correction 51.41 45.69 41.62 48.49 46.77 33.14 36.77 40.41 51.97 58.81 68.97\n+ 3\u0002ensemble 53.02 47.18 42.97 50.11 48.37 34.32 38.12 41.93 53.99 61.33 72.07\nWiki-500K\nStage 1 76.54 57.65 44.33 69.54 67.01 32.61 40.04 43.48 65.78 72.06 80.60\n+ Stage 2 77.81 59.14 45.85 71.22 68.97 33.38 41.88 45.98 68.33 74.97 84.70\n+ Sparse ranker w/o calibration 79.47 61.08 47.77 73.35 71.41 32.10 42.72 48.25 71.20 77.24 84.70\n+ Score correction 80.46 61.60 48.03 74.09 72.01 34.76 44.97 49.82 71.36 77.50 84.70\n+ 3\u0002ensemble 81.26 62.51 48.82 75.12 73.10 35.02 45.94 51.13 72.74 79.17 87.22\nAmazon-3M\nStage 1 49.12 46.31 44.10 47.46 46.26 16.32 19.44 21.57 19.12 27.90 49.15\n+ Stage 2 49.93 47.07 44.85 48.20 46.97 14.97 17.46 19.34 18.94 28.28 52.93\n+ Sparse ranker w/o calibration 52.63 49.87 47.58 51.04 49.81 15.79 19.00 21.35 20.39 29.97 53.50\n+ Score correction 52.63 49.87 47.58 51.04 49.81 15.79 19.00 21.35 20.39 29.97 53.50\n+ 3\u0002ensemble 54.28 51.40 49.09 52.65 51.46 15.85 19.07 21.52 21.59 31.76 57.09\nTable 8: Full component ablation of ELIAS on all datasets\nD Analysis of learned index\nTable 9a reports the ﬁnal accuracy numbers of ELIAS-1(d)model on Amazon-670K after threshold\nbased pruning of the learned cluster-to-label assignments (i.e. for a particular threshold we remove all\nedges in the learned Awhich has smaller weight than the threshold). These results indicate that about\n\u001884% edges can be pruned without hurting the model performance. Similarly, table 9b reports the\nﬁnal accuracy numbers of ELIAS-1(d)model on Amazon-670K after top- Kbased pruning of the\nlearned cluster-to-label assignments (i.e. we retain only top- Klabel assignments per cluster).\nFigure 7a plots the fraction of edges of the stage 1 tree that still remain in the learned adjacency\nmatrix Aafter thresholding at various cutoff thresholds (i.e. for a threshold we only retain entries\nin which are greater than and evaluate how many edges of stage 1 tree remains). on Amazon-670K\ndataset. The plot reveals that almost \u001860% stage 1 cluster assignments remain in the learned Awith\ngood conﬁdence. Figure 7b plots the distribution of the average number of clusters assigned to a\nlabel for each label decile (decile 1 represents the head most decile and decile 10 represents the tail\nmost decile). We say that a label lis assigned to a cluster ciff the weight ac;lin the learned adjacency\n18\n\nTable 9: ELIAS-1(d)results on Amazon-670K after pruning of learned cluster-to-label adjacency matrix A(a)\nafter threshold based pruning (b) after top-k based pruning\n(a)\nThreshold % pruned P@1 P@5 R@10 R@100\n0 0 48.68 40.04 50.33 68.95\n0.01 20.89 48.68 40.05 50.33 68.96\n0.05 64.42 48.68 40.04 50.33 68.96\n0.1 73.63 48.68 40.04 50.33 68.95\n0.25 84.52 48.65 40.02 50.26 68.82\n0.5 89.11 48.40 39.48 48.98 66.75\n0.75 91.95 47.70 38.19 46.38 62.17\n0.9 93.13 47.26 37.42 44.91 59.53(b)\nTop-K P@1 P@5 R@10 R@100\n1000 48.68 40.04 50.33 68.95\n750 48.70 40.05 50.34 68.95\n500 48.72 40.05 50.34 68.95\n300 48.72 40.05 50.34 68.95\n200 48.71 40.05 50.32 68.87\n100 48.22 39.04 47.98 64.80\n50 46.17 33.85 38.35 49.48\nmatrix Ais greater than 0:25. This demonstrates a clear trend that head labels get assigned to more\nnumber of clusters than tail labels.\n0.0 0.2 0.4 0.6 0.8 1.0\nLearned edge weight threshold405060708090Percentage of stage-1 edges leftOverlap analysis of stage-1 tree with stage-2 graph\n(a)\n1 2 3 4 5 6 7 8 9 10\nDeciles (decreasing frequency)01234567Average number of clustersAverage clusters assigned in each decile (b)\nFigure 7: (a) percentage of stage 1 edges remaining in the learned adjacency matrix Aat various\ncutoff thresholds on Amazon-670K dataset (b) decilewise distribution of the average number of\nassigned cluster in Amazon-670K dataset\nIn Figure 8 and 9, we qualitatively compare the training point distributions of labels which get\nassigned to multiple clusters and labels which get assigned to only one cluster by plotting TSNE\nplots of the training points of such labels and their assigned clusters. We say that a label lis assigned\nto a clusterciff the weight ac;lin the learned adjacency matrix Ais greater than 0:25. These plots\nindicate that labels assigned to multiple clusters often have training points with a more multi-modal\ndistribution than the labels which get assigned to only one cluster.\n19\n\nFigure 8: TSNE plot of training points of labels which get assigned to multiple cluster in the learned\nindex structure on Amazon-670K dataset. We randomly sample 6 labels which have more than 1\nbut less than 6 edges with more than 0.25 learned weight ( ac;l). The red dots represent the training\npoint of the sampled label and the dots in other colors indicate the training points of the respective\nassigned clusters (we say a training point xibelongs to a cluster ciffsi\nc>0:25). As we can see\ntraining points of labels which gets assigned to multiple cluster often exhibit multi-modal distribution\n20\n\nFigure 9: TSNE plot of training points of labels which get assigned to only one cluster in the learned\nindex structure on Amazon-670K dataset. We randomly sample 6 labels which only have one edge\nwith more than 0.25 learned weight ( ac;l). The red dots represent the training point of the sampled\nlabel and the blue dots indicate the training points of the assigned cluster (we say a training point xi\nbelongs to a cluster ciffsi\nc>0:25). As we can see training points of labels which gets assigned to\nonly one cluster exhibit uni-modal distribution\n21",
  "textLength": 65754
}