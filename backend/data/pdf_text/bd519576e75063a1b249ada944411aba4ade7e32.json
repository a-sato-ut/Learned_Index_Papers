{
  "paperId": "bd519576e75063a1b249ada944411aba4ade7e32",
  "title": "PerOS: Personalized Self-Adapting Operating Systems in the Cloud",
  "pdfPath": "bd519576e75063a1b249ada944411aba4ade7e32.pdf",
  "text": "Research Vision\nPEROS: Personalized Self-Adapting\nOperating Systems in the Cloud\nHongyu H `e\nETH Z ¨urich\nhongyu.he@inf.ethz.ch\nNovember 2023arXiv:2404.00057v1  [cs.HC]  26 Mar 2024\n\nAbstract\nOperating systems (OSes) are foundational to computer systems, managing hardware re-\nsources and ensuring secure environments for diverse applications. However, despite their\nenduring importance, the fundamental design objectives of OSes have seen minimal evolu-\ntion over decades. Traditionally prioritizing aspects like speed, memory efficiency, security,\nand scalability, these objectives often overlook the crucial aspect of intelligence as well as\npersonalized user experience. The lack of intelligence becomes increasingly critical amid\ntechnological revolutions, such as the remarkable advancements in machine learning (ML).\nToday’s personal devices, evolving into intimate companions for users, pose unique chal-\nlenges for traditional OSes like Linux and iOS, especially with the emergence of specialized\nhardware featuring heterogeneous components. Furthermore, the rise of large language mod-\nels (LLMs) in ML has introduced transformative capabilities, reshaping user interactions and\nsoftware development paradigms.\nWhile existing literature predominantly focuses on leveraging ML methods for system\noptimization or accelerating ML workloads, there is a significant gap in addressing personal-\nized user experiences at the OS level. To tackle this challenge, this work proposes P EROS,\na personalized OS ingrained with LLM capabilities. P EROS aims to provide tailored user\nexperiences while safeguarding privacy and personal data through declarative interfaces, self-\nadaptive kernels, and secure data management in a scalable cloud-centric architecture; therein\nlies the main research question of this work: How can we develop intelligent, secure, and\nscalable OSes that deliver personalized experiences to thousands of users?\nPage 2 of 29\n\nCONTENTS CONTENTS\nContents\n1 Introduction 4\n2 Declarative User Interface 5\n2.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Research Question and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3 Research Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3 Adaptive Kernel and its Subsystems 9\n3.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.2 Research Question and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.3 Research Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4 Secure and Scalable Architecture in the Cloud 14\n4.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4.2 Research Question and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . 15\n4.3 Research Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n4.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n5 Related Work 18\nReferences 21\nPage 3 of 29\n\n1 INTRODUCTION\n1 Introduction\nThe complexity and significance of operating systems (OSes) cannot be overstated. They serve\nas the backbone in nearly all computer systems, effectively managing hardware resources and\nensuring a secure environment for running various applications. Despite their pivotal role, the\nfundamental design objectives of OSes have seen minimal evolution over almost half a century.\nThese objectives traditionally prioritize aspects like speed, memory efficiency, security, scalabil-\nity, consistency, concurrency control, and fault tolerance in distributed settings. However, this ap-\nproach predominantly views OSes as general-purpose computing infrastructure, often overlooking\nthe individual user experience. This oversight poses a risk, particularly in the face of the ongoing\ntechnological revolution spurred by remarkable advancements in machine learning (ML).\nFirstly, personal devices like mobile phones and laptops are forging increasingly intimate con-\nnections with their users. These gadgets transcend mere computing tools; they are companions,\nor even perceived as friends or even family members, safeguarding precious memories and en-\ngaging in daily interactions. In today’s landscape of diverse applications, individual users harbor\nunique needs, each representing specific workloads. For instance, some devote substantial time to\nimmersive video gaming or AR/VR applications, demanding substantial computational resources,\nwhile others predominantly engage in web browsing and text editing. Moreover, to cater the\never-growing resource demands in the post-Moore’s Law era, hardware is becoming increasingly\nspecialized, having heterogeneous components integrated onto a single chip (e.g., AMD Versal [8]\nand Apple M3 [9]). Yet, adapting mature OSes such as Linux and IOS to these fast evolving de-\nvices poses challenges, partly due to their monolithic design, wherein subsystems closely coupled\nfor better performance. These converging trends have been expedited by the rise of ML, notably,\nlarge language models (LLMs).\nWhile training and tuning several LLMs at Apple each on hundreds of TPUs, I noticed their\nfundamental differences from the traditional ML models — they absorb web-scale data and can\npick up a data point after only a few exposures; they feature in-context learning , customizing\ntheir responses based on prior interactions; they are capable of few-shot learning , carrying out\nsimilar tasks given just a few examples; they can follow instructions , tailoring their output given\nusers’ prompts. Furthermore, LLMs have been extended to multimodal input , such as images\nand video, performing multiple desperate tasks with end-to-end trained architectures. With their\nexceptional language understanding and data manipulation capabilities, LLMs effortlessly process\nfiles of various formats and comprehend how to manipulate the data that documents contain. These\ncapabilities have drastically changed how users interact with their applications, as much as the\nways in which software is built. I believe that if the iPhone was the great hardware filter, then the\nLLM is going to be a great filter for software, including OSes.\nUnfortunately, the literature has been primarily focused on either leveraging ML methods for\nautomated system optimization (e.g., [64, 3, 68, 14, 39, 101, 102, 103, 96, 47]) or specializing\nsystems for accelerating ML workloads (e.g, [61, 88, 30, 42, 43, 92]). This inclination neglects\nthe importance of individual users embodying distinct requirements and usage patterns that require\npersonalized adaptations from OSes. Similarly, while most technology companies I have worked\nwith (e.g., Huawei, Microsoft, and Oracle) have been employing ML models to ease the burden of\nmonitoring and/or orchestrating large-scale resources, ‘personalized intelligence’ on the OS level\nhas not yet gained prominence (apart from a few general solutions like Cortana and Siri). These\naforementioned challenges give rise to the main research question ( MRQ ) of this proposal:\nPage 4 of 29\n\n2 DECLARATIVE USER INTERFACE\nMRQ How to build intelligent, secure, and scalable OSes that provide thousands of users with\nsmooth and personalized experiences?\nAs a first step towards attacking the MRQ , I propose P EROS,1a personalized LLM-ingrained\nOS capable of accommodating the needs and usage patterns of individual users while protect-\ning their privacy and personal data. P EROS aims to offer a declarative user interface (§2), self-\nadaptive kernel (§3), and secure management of personal data with a scalable cloud-centric archi-\ntecture (§4).\n2 Declarative User Interface\nThroughout history, human-computer interaction has evolved in distinct phases. Initially, users\ngrappled with systems like punch cards and command-line interfaces (CLI), forced to adapt to\ncomputers’ rigid expectations. The emergence of Graphical User Interfaces (GUIs) marked a\npivotal shift, making computers more accessible to non-technical users. However, GUIs, while\ntransformative, only partially bridged the human-computer gap. Users still had to search among\nhundreds of applications and navigate preset layouts that mirror developers’ logic, leading to frus-\ntrating experiences of hunting for specific functionalities. This reliance on thinking like developers\ncurtailed the user experience, imposing rigid behavior patterns. That been said, such unpleasant\nexperiences also apply to developers themselves. For example, CLI and writing scripts remain the\nprimary means for compiling and testing applications, which are typically error-prone and hard to\nmaintain (as seen in composing Shell scripts and chaining commands). Fortunately, the advent of\nLLMs signifies a groundbreaking leap — they enabled conversational interfaces that require no\nbehavioral shifts from users , enabling intuitive and smooth user experience.\nIntegrating LLMs into existing systems automates functionalities, enriching capabilities and\nsimplifying interactions. An exemplar is Microsoft’s Windows Copilot [58], a centralized AI\nassistant empowering users within the Windows environment. Seamlessly accessible from the\ntaskbar, Copilot acts as a personal assistant, enabling content rewriting and summarization along-\nside traditional Windows features. Similarly, the recently unveiled GPT4-turbo [63] showcases\nexpanded functionalities like sending Slack messages, setting calendar events, and invoking var-\nious software APIs, which highlights the immense potential of LLMs as a unified user interface.\nSuch advancements can radically blur the line between applications and the OS.\n2.1 Objectives\nIn P EROS, I want to build a declarative, unified interface powered by LLMs to facilitate the inter-\naction between the user and the OS. Specifically, users express the high-level tasks that they want\nto perform in natural languages like English. Then, P EROS performs the tasks and returns the\nresults to the users. Listing 1 shows an example user request. In this example, P EROS needs to\nextract the tasks from the user request in the correct order, navigate to the intended project direc-\ntory, carry out the tasks with the corresponding API calls, double-check the result with the user,\nand ask clarification questions in case of any ambiguity. Ideally, this interface could be extended\nto multi-modalities, such as speech and images. Such an extension would foster improved acces-\nsibility by allowing users to interact with the system through their preferred modalities, catering\nto various communication preferences. Although this naive example can be achieved via a chain\n1PEROS is just a working code name.\nPage 5 of 29\n\n2 DECLARATIVE USER INTERFACE 2.2 Research Question and Challenges\nUser : now, undo the most resent commit for my HappyDog project, remove all the CSV files\nlarger than 10 MB from the git cache, move those files to a new directory called data at the project\nroot, ignore this folder in git, add a suffix large to all their names, augment the previous commit\nwithout a new message, and force push to my remote repo\nPEROS: The following is the git diff after the changes:\ni n d e x 6 b 0 c 6 c f . . b37e70a 100644\n−−− a / dogs . csv\n+++ b / d a t a / d o g s l a r g e . csv\n. . .\nPEROS: Is the result correct? Otherwise, I can revert the changes.\nUser : y\nPEROS: To which branch do you want to push the changes?\nUser : i forgot the remote branches\nPEROS: You have main ,dev, and feat/chihuahua in your GitHub repo and only master on\nBitbucket.\nUser : dev github\nPEROS: I’ve pushed the changes to https://github.com/username/happydog/tree/dev\nListing 1: Example user interactions with P EROS via the declarative interface.\nof commands in a rule-based system, hand-crafted systems typically require users to structure\ntheir queries in rigid formats in order to correctly extract information such as target objects and\ndestination. Moreover, developers face challenges in anticipating all request patterns, leading to\nan ever-expanding set of rules and templates in the knowledge base to accommodate new types\nof user requests. Likewise, the evolution of system interfaces and application APIs brings about\nchanges over time. Introducing or modifying utilities might necessitate updating all associated\nfunctionalities that rely on them. This constant evolution demands ongoing adjustments to main-\ntain synchronization across the system. The benefit of employing LLMs, therefore, lies in their\nability to ensure a seamless user experience and enable continuous learning. They excel in adapt-\ning to new system APIs and unforeseen user requests by generalizing from just a few examples.\nThis capability streamlines the adaptation process and promotes agility in accommodating changes\nwithin the system landscape.\n2.2 Research Question and Challenges\nThe objectives described above raise the first RQ of this proposal:\nRQ1 How can we enable users to interact with their OSes using natural language in a declarative\nfashion, fostering personalized experiences and seamless interaction?\nChallenges. I anticipate the following challenges in answering RQ1. Firstly, collecting high-\nPage 6 of 29\n\n2 DECLARATIVE USER INTERFACE 2.3 Research Methods\nquality training data is essential to ensuring model quality. Specifically, training requires thou-\nsands of request-solution pairs. Next, based on my experience in working with LLMs, achieving\nconcise answers is a non-trivial task, as existing LLMs are not guaranteed to be self-consistent [93].\nIn essence, LLMs do not consistently produce the end-of-sequence token and often rely on pre-\ndefined stopping probability thresholds. Consequently, fine-tuning is necessary to obtain brief an-\nswers, as illustrated in Listing 1. Similarly, maintaining a continuous and coherent dialogue over\ntime poses a challenge, particularly during multi-stage tasks (akin to the scenario in Listing 1).\nAdditionally, integrating the developed LLM with a kernel (e.g., Linux) requires substantial engi-\nneering effort. For instance, efficient interactions with the LLM may necessitate a kernel module,\nsince the model is expected to run in user mode. Moreover, registering event triggers, monitoring\nkernel updates and communicating the relevant updates to the LLM in a format consistent with the\ntraining stage are crucial. Also, determining the optimal timing and strategy for model retraining\nposes significant challenges. The last challenge lies in defining representative metrics to evalu-\nate the LLM-integrated system. Evaluating LLMs remains an ongoing research area, making the\nselection of suitable metrics tricky. Common automatic metrics like BLEU, ROUGE, and ME-\nTEOR might not capture this system’s performance well. Here, not only linguistic quality but also\nthe relevance of the solution and the accuracy of its execution are paramount. These factors pose\ncritical evaluation criteria that automatic metrics might not wholly encompass.\n2.3 Research Methods\nScope. A full-fledged system as described in §2.1 is appealing but overly extensive in scope.\nTo make addressing RQ1 feasible within a year, I narrow it down in the following ways. Firstly,\nrestricting the user interface solely to text format is essential, considering that multimodality is\ncurrently the research frontier of ML modeling — a domain not central to this systems research.\nThe proof-of-concept for RQ1 would already serve as a natural passage to exploring other modal-\nities, such as images and audio. Secondly, I propose a controlled expansion of the number of\napplications and APIs. Starting with traditional POSIX syscalls and Shell scripting could estab-\nlish as a solid foundation, allowing for the gradual addition of a few application APIs at a later\nstage.\nApproach. To address RQ1, I will conduct development and application studies to create a\nprototype system. This prototype will serve as the initial incremental step toward building P EROS.\nFigure 1 provides an overview of the proposed system prototype. The process begins with the\ncollection and potential synthesis of sufficient training data. This dataset should include example\nuser requests, corresponding APIs to be invoked, and the expected system updates returned from\nthe kernel. Upon acquiring the datasets, the next phase involves training and fine-tuning the LLM.\nI plan to utilize a two-tower LLM, where the encoder functions as the Interpreter ( 1) processing\nuser requests, and the decoder acts as the Director ( 2) generating a sequence of operations in\nresponse. These components should undergo separate evaluations (§2.4) before integration into\nthe broader system.\nIn parallel, the Actuator ( 3) can be constructed to execute the sequence of operations generated\nby the Director. It will invoke the APIs and execute commands by engaging the kernel, specifi-\ncally employing an off-the-shelf monolithic kernel like Linux. For RQ1, I plan to use the kernel\nonly as a static toolbox and do not modify its functionality other than registering event triggers\nand implementing the kernel module. The Watchdog ( 4) assumes the responsibility for monitor-\ning registered kernel events and relaying updates to the Actuator, which, in turn, organizes and\nPage 7 of 29\n\n2 DECLARATIVE USER INTERFACE 2.4 Evaluation\n Deployment\n (e.g., Kubernetes, Service Weaver) Monolithic Kernel  \n (e.g., Linux)\nPEROS EventslatentsInterpr eter\nPEROS Kernel\nModuleDirector\ncommand\nsequences\n(RPC)Actuator  \nupdates  (RPC)\nWatchdogLM Manager\nUsernatural \nlanguage Declarative\nInterface\noperations\nmonitor\n(RPC)requests\n(HTTPS)\nresponsesretraining\nLLM\nEncoderLLM\nDecoder\nFigure 1: Prototype of P EROS with the declarative interface powered by LLMs.\nforwards results back to the user terminal. I plan to orchestrate these components as microser-\nvices using frameworks familiar to me (e.g., Kubernetes [48] or Service Weaver [31]), employing\nremote procedure calls (RPC) for efficient inter-service communication. Furthermore, low-end\nnetworking-enabled devices should adequately support the declarative interface for users to send\nrequests to P EROS, given that the bulk of computation occurs remotely. Finally, the LM Manager\n(5) will evaluate and retrain the LLM according to predefined retraining policies.\n2.4 Evaluation\nTo comprehensively evaluate the prototype during development, I intend to conduct both quan-\ntitative evaluations and a hybrid user study . This study is, to my knowledge, the first aiming to\nleverage LLMs for an OS user interface. Hence, potential baselines could include a separately con-\nstructed rule-based system (e.g., [26, 25]) or integrating the prototype with traditional sequence\nmodels (e.g., Bidirectional LSTMs).\nThe evaluation should be structured into steps to facilitate component integration and ablation\nstudies. Initially, for the isolated evaluation of the LLM, I will employ traditional linguistic met-\nrics (e.g., BLEU, ROUGE, and METEOR) to assess both the Interpreter and Director’s language\nunderstanding capabilities. This step ensures correct information extraction and smooth response\ngeneration. Subsequently, assessing the Director via accuracy (i.e., correctness of generated com-\nmands) and recall (i.e., retrieval rate of necessary commands) is critical.\nBefore integrating the Director with the Actuator, the rest of the system should undergo similar\ntesting, including unit testing for command execution, event triggering, and reporting. After the\nintegration, a comprehensive end-to-end evaluation is necessary, repeating linguistic and execution\nevaluations, in which the Actuator takes real Director outputs, and the Director responds based on\nthe actual results from the Watchdog.\nThe user study aims to gauge efficiency (e.g., speed) and satisfaction levels. Student volunteers\nor commercial platforms like Amazon Mechanical Turk [7] may be utilized for this purpose, where\nevaluators are tasked with performing predefined user requests through three methods: (i)using\nPage 8 of 29\n\n3 ADAPTIVE KERNEL AND ITS SUBSYSTEMS\nthe P EROS prototype, (ii)a baseline system, and (ii)manual execution (with internet access).\nTime taken and qualitative feedback for each category will be recorded and compared.\nLastly, I plan to evaluate the retraining policies used by the LM Manager, by gradually extending\nthe functionality of the prototype. Specifically, the linguistic quality and the command execution\nscores are continuously measured as new system APIs and event triggers are added over time.\nSystem performance initially may deteriorate upon the addition of new utilities and gradually\nrecovers by triggering retraining and learning the added new APIs.\n3 Adaptive Kernel and its Subsystems\nThe surge in resource demands from emerging software applications, coupled with the increas-\ning heterogeneity of hardware, renders the design and development of OSes without integrating\n‘intelligence’ unsustainable. Consider Linux, already comprising over 28 million lines of code,\nwhich grows bulkier as additional features are incorporated to meet evolving requirements within\nits general-purpose design. Tuning and debugging such a colossal system have become akin to\nan arcane art, given the intricate interconnections among system components. For instance, Linux\nboasts more than 17K kernel configurations, complicating its adaptation to new applications, such\nas AI workloads, and evolving user demands. Consequently, developers have resorted to rewriting,\npatching, or even bypassing this complexity to directly access the required functionalities.\nTraditionally, after building, installing, and configuring an OS, it remains largely static at run-\ntime, limiting its adaptability to the dynamic operating environment. To match the swift evolution\nof modern hardware and applications, I advocate for a different approach in future OS develop-\nment. Rather than relying solely on human expertise, heuristics, and huge amounts of engineering\neffort, leveraging ML techniques to automatically “learn” OS configuration and tuning becomes\npromising. Given the super-human performance we have witnessed in the recent ML develop-\nment, such learned solutions could potentially yield superior results. My proposal in this section\nentails employing ML models as a shim layer that helps tune and configure the underlying kernel\nto personalize it toward specific user workloads.\n3.1 Objectives\nUnlike §2 where a general-purpose kernel was used as a static toolbox, now I want to make the\nkernel and its subsystems in P EROS “dynamic,” i.e., they configure and tune themselves to adapt\nto the users’ usage patterns. I identify three primary areas where ML solutions hold significant\npromise, each posing an increased level of difficulty.\nThe first area involves adaptive configuration and tuning . Present-day kernels like Linux offer\nnumerous tunable knobs. For instance, the memory subsystem, filesystem, and networking sub-\nsystem individually possess 89, 351, and 729 configurations, respectively. These configurations\nmostly fall into two categories: timing parameters and sizes. Timing parameters involve elements\nlike the time slice of kernel tasks, memory swapping frequency in paging, and CPU frequency\nsampling rates (e.g., for DVFS). Likewise, various sizes, such as buffer cache size, disk/swap\nprefetching data amounts, and memory page sizes with hugetlbfs , can be configured in Linux.\nMany of these configurations significantly impact the performance and energy costs of user appli-\ncations. For example, increased CPU interruption offers potential for improved CPU utilization\nthrough more aggressive thread scheduling but may lead to performance overhead by preempting\nPage 9 of 29\n\n3 ADAPTIVE KERNEL AND ITS SUBSYSTEMS 3.2 Research Question and Challenges\nand context switching threads. Similarly, a larger buffer cache enhances storage system perfor-\nmance but reduces the amount of available memory for user applications. Rather than relying\nsolely on labor-intensive engineering efforts, ML models trained on historical user traces could\ngenerate configurations adaptive to user usage patterns.\nThe second area revolves around learning system policies , to name a few, memory allocation\npolicies determine which free space is allocated to applications; CPU scheduling algorithms dic-\ntate which kernel tasks to prioritize; cache replacement policies decide which data to evict. While\nthese policies have a more direct impact on application performance compared to configurations,\nthey still primarily rely on simple algorithms and heuristics. For instance, the ext family of\nfilesystems in Linux allocates adjacent space for files in the same directory, which is only effec-\ntive for accessing nearby files. The most prevalent cache replacement policy is quasi-LRU and\naims to swap out the least recently used data. Apparently, its efficiency heavily relies on data\nlocality. Similarly, the Linux CFS (Completely Fair Scheduling) algorithm, utilizing a red-black\ntree to manage timeshare of tasks, not only incurs a O(logN)time complexity but also falters un-\nder short-task threshing scenarios, where approximations of shortest-remaining-processing-time\n(SRPT) [12] could improve latency [36]. My intent is to leverage ML methods to dynamically\nadjust these policies or even learn new policies tailored to user workloads.\nWhile the first two categories are performance-influential, they typically do not jeopardize ker-\nnel execution correctness. The third area, however, is the most perilous: learning system function-\nalities . Inspired by the work on learning database index [45], I speculate that similar strategies\ncould be applied to many similar kernel functionalities. For example, memory translation, involv-\ning mapping virtual memory addresses to physical addresses by traversing multi-level page tables,\nconstitutes a performance bottleneck in virtualization techniques [81], where extended page tables,\nup to four levels deep, result in severe TLB miss penalties. Similarly, indexing into a file’s data\nblock requires traversing a multi-level index structure (e.g., the ext4 filesystem). These func-\ntionalities epitomize areas where ML methods could yield improved time and space solutions.\nHowever, employing ML in this category is exceedingly risky, as erroneous kernel functionalities\ncould crash the system. Therefore, instead of replacing existing kernel functionalities, ML models\ncould serve as a first-order approximation, reducing the search space and transforming a global\nindexing problem into a localized one.\n3.2 Research Question and Challenges\nThe aforementioned objectives give rise to the second RQ of this proposal:\nRQ2 How to make the OS kernel and its subsystems self-adaptive to users’ usage patterns by\nautomatically learning from user activities over time?\nChallenges. As hinted in §3.1, addressing RQ2 may encounter several roadblocks. Primarily,\nunlike RQ1, incorporating ML methods closer to the kernel necessitates crucial considerations of\nperformance and space overhead. This requirement makes leveraging LLMs unlikely for RQ2 due\nto their slow inference time (a few milliseconds per token) and substantial memory requirements.\nEven traditional ML methods face challenges due to the stringent latency demands for kernel-\nlevel decisions (single-digit microseconds). For instance, invoking a model on the present-day\nPCIe setups usually takes tens of microseconds, making this a potential bottleneck. Therefore,\nthe ML models employed nearby kernel components has to be lean and amenable to hardware\nacceleration .\nPage 10 of 29\n\n3 ADAPTIVE KERNEL AND ITS SUBSYSTEMS 3.3 Research Methods\nPEROS: You have recently created many files in the /Documents/crucial/ folder. Should I\nadd them to the backup workflow?\nUser : sg! they’re important personal data\nPEROS: Great, I’ve added them to the backup list and labeled them as important . They’re\nscheduled for weekly backups, aligning with your workflow’s highest frequency. Also, for privacy\nreason, they’ll be stored on your local SSD instead of the cloud. Any other changes you’d like?\n. . .\nListing 2: Example of personalized recommendation through the declarative interface (§2) based\non recent activities in the filesystem.\nMoreover, even if smaller ML models can fit into the kernel, their execution in kernel mode is\nimprobable due to the lack of ML runtime support and the complexity of kernel integration. Addi-\ntionally, akin to multimodality, multitasking poses an equal challenge in my experience, therefore,\nemploying a single global model for various configurations and policies may be infeasible. Hence,\ndetermining the appropriate model type for diverse tasks is pivotal. Furthermore, regardless of the\ntype of ML methods employed, their probabilistic nature conflicts with the high precision de-\nmanded by OSes.\nLastly, evaluating the learning process is the hardest for three primary reasons: (i)Workload-\ndependent ground truth complicates many learning tasks, such as determining optimal cache sizes\nand swapping frequencies. For RQ2 specifically, another challenge is obtaining several workload\ntraces of consistent usage patterns, mimicking different users. (ii)Correlated factors add complex-\nity; for example, buffer cache size, flushing frequency, and cache policy collectively impact buffer\ncache performance, subsequently affecting both the storage and memory subsystems. (iii)System\noptimization requires a holistic approach; addressing individual components in silos is similar to\ntrying to cure a cold by only treating the symptom of coughing.\n3.3 Research Methods\nScope. As outlined in §3.1, the scope of exploration for RQ2 is extensive. Diverging from prior\nwork (§5), my focus lies in integrating intelligence into the system as a whole, concentrating on\nadaptiveness to users’ usage patterns, rather than honing in solely on enhancing the performance\nof individual components such as prefetching [74] and branch prediction [16, 38, 6]. Therefore,\nthe learning tasks should collectively serve as a proof-of-concept, showcasing P EROS’s capacity\nto dynamically adapt to user workloads and fine-tune the kernel setup, improving the utilization of\nsystem resources. Ideally, these tasks should expand across the OS stack, encompassing all three\ncategories described above (§3.1). Since the utilization of ML methods in power management,\nscheduling, and the memory subsystem constitutes a relatively crowded research area (§5), my\nintention is to concentrate on adding adaptability into the storage and filesystem through learning\nmethods. This choice of focus also reflects the fact that the filesystem is one of the most intimate\ncomponents of the OS, with which users interact frequently. For example, it should seamlessly\nintegrate with the declarative interface introduced in §2, helping users maintain and manage their\nfiles (Listing 2).\nPage 11 of 29\n\n3 ADAPTIVE KERNEL AND ITS SUBSYSTEMS 3.3 Research Methods\n Micr okernel  \n (e.g., MINIX, DBOS, or seL4)syscalls Scheduler IPC File\n Server\n PEROS Kernel\n Module  Disk\n DriverMemory Server\nlearnconfigur e\nFigure 2: Adaptive kernel and subsystems of P EROS.\nApproach. Similar to addressing § RQ1, I will be conducting development and application stud-\nies, extending the initial P EROS prototype (§2.3). However, delving into RQ2 necessitates a closer\nexamination of the kernel (Figure 2), which was initially utilized as an off-the-shelf, monolithic\ntoolbox. Here, I intend to adopt a microkernel such as MINIX [84], DBOS [78], or seL4 [33].\nThese microkernels effectively decouple and deprivilege the filesystem from the kernel. This step\nis necessary as it mitigates inherent risks associated with learning system functionalities (§3.1)\nand circumvents the need for hosting any ML models in kernel space.\nFor learning configurations and automatic tuning, I want to use ML methods to dynamically set\nthe disk prefetching amount, a.k.a., the read-ahead buffer size on Linux via blockdev for example.\nLarge read-ahead buffer sizes can enhance read performance for sequential disk accesses, reducing\nI/O wait times by prefetching data into memory before it is requested. User applications perform-\ning sequential reads, such as video streaming or large file transfers, benefit the most from increased\nread-ahead buffer sizes. However, for workloads involving random or non-sequential data access,\nlarge read-ahead buffer sizes might not improve performance — prefetching large amounts of data\nahead of requests is less effective when the access pattern is irregular. Another trade-off is memory\n— while beneficial for sequential reads, allocating excessive memory to prefetching might lead to\nunnecessary memory overhead, affecting overall system performance.\nFor investigating learning system policies, I aim to explore a correlated task: learning filesystem\nspace allocation policies. Ideally, this learning task should synergize with the learned configura-\ntion of the read-ahead buffer size. Filesystem space allocation policies, such as block allocation\nstrategies (e.g., extent-based allocation or block group allocation), primarily dictate how data is\norganized and stored on disk. ML models can contribute to more intelligent and efficient data\nplacement. For example, by learning user access patterns, the model can allocate free space for\nfaster file access, such as grouping related files together or reorganizing directories. Another exam-\nple is dynamic file tiering, where files are classified based on usage patterns and are automatically\nmoved to different storage tiers for better performance and storage utilization.\nCertain filesystems provide mount options or features that exert influence over space allocation\nstrategies or behavior. For instance, ext4 offers options like mballoc orinode readahead blks\nthat can impact allocation policies or the read-ahead behavior of inodes. This relationship is\nreciprocal — setting the read-ahead buffer size can also be base on filesystem space allocation\nPage 12 of 29\n\n3 ADAPTIVE KERNEL AND ITS SUBSYSTEMS 3.4 Evaluation\npolicies. Considering how different buffer sizes align with the filesystem’s characteristics and the\nuser’s usage patterns becomes essential.\nFor investigating system functionality, I plan to study the feasibility of learning multi-level in-\ndexing (e.g., inode structures) for mapping file names to disk block addresses. In Linux ext4 ,\neach file is linked to an inode that contains metadata (e.g., permissions, timestamps, pointers to\ndata blocks). The filesystem manages these inodes in an inode table, a data structure housing these\nfile metadata entries. Each inode retains crucial information about the location and structure of\nfile data blocks. Within the inode, the filesystem utilizes a hierarchy of block pointers to facili-\ntate the mapping from file names/offsets to disk block addresses. These pointers can be direct or\nindirect (double or triple) pointers pointing to data blocks. When seeking a specific offset within\na file, the filesystem computes the corresponding logical block address by leveraging the inode’s\nblock pointers alongside the file’s offset. To navigate through this multi-level index structure, the\nfilesystem relies on those direct and indirect block pointers, aiming to index the relevant data block\nholding the requested file content. Learning this intricate mapping may be challenging due to its\ninherent complexity and the absence of easily identifiable patterns. Hence, my initial approach\nwould involve generating a dataset that encompasses file names, offsets, and their corresponding\ndisk block addresses. This dataset could be created by systematically traversing filesystem struc-\ntures. Subsequently, I plan to extract meaningful features from file names, offsets, and potentially\nfile metadata (e.g., file sizes, access patterns, inode details). These extracted features will form the\nbase for representing the input data in this complex mapping modeling.\n3.4 Evaluation\nThe evaluation methodology for the proposed approach (§3.3) predominantly comprises quanti-\ntative studies involving multiple micro-benchmarks. To my knowledge, this study would mark\nthe first exploration into intelligent OS storage and filesystems across all three categories (§3.1).\nTherefore, a reasonable (and perhaps a strong enough) baseline could be a configuration without\nML-based adjustments. To conduct this evaluation, I intend to first collect a selection of pertinent\nbenchmarks (e.g., Filebench [86]) and datasets pertinent to storage and filesystems (e.g., UMass\nI/O Trace [95]), including tools like FIO [11]. The evaluation process encompasses an array of\nperformance metrics such as throughput, latency, and I/O efficiency, covering diverse workloads\ncomprising read-heavy, write-heavy, sequential, and random access patterns. For instance, one\naspect I aim to evaluate involves measuring the influence of read-ahead buffer size adjustments on\nsystem memory usage, ensuring that memory consumption remains within acceptable thresholds.\nSimilarly, I plan to assess the impact of changes in filesystem space allocation policies on disk\nspace usage, fragmentation, and overall storage efficiency. In addition to performance analysis,\nevaluating the system’s stability under varied workloads and scenarios is critical to ensuring that\nML-driven adjustments uphold system stability and do not precipitate unexpected failures. Stress-\ntesting forms an integral part of this evaluation as well, in which I will examine P EROS’s capacity\nto manage increased workloads, ensuring that ML-based adjustments can seamlessly adapt to\nshifting demands. Lastly, similar to §2.3, an important aspect of the evaluation process involves\nassessing P EROS’s responsiveness to alterations in user workload patterns, and its efficiency in\ndynamically adapting ML-based adjustments accordingly.\nPage 13 of 29\n\n4 SECURE AND SCALABLE ARCHITECTURE IN THE CLOUD\n4 Secure and Scalable Architecture in the Cloud\nThe potential of P EROS’s adoption can be significantly improved with a secure and scalable de-\nsign. In this post-Moore’s Law era, the performance of individual chips is quickly leveling off,\nchallenging the scalability of client-side devices for LLMs and other ML models. Thus, tapping\ninto the mature cloud ecosystem emerges as a promising solution. My envisaged approach in-\nvolves leveraging three key technologies: thin-client computing (TCC), serverless computing, and\nprivacy-preserving ML (PPML).\nTCC, proposed over two decades ago, aimed to streamline hardware maintenance and cut costs\nfor deploying and updating applications across diverse users. It relies on stateless (thin) clients in-\nteracting with centralized computing resources. While commercial products like Citrix Metaframe\nand Microsoft Terminal Services emerged, TCC struggled beyond individual organizations due to\nstrict network requirements and privacy worries from its centralized setup. Yet, TCC’s relevance is\nresurging due to several key factors. Network connectivity is rapidly improving, with speeds dou-\nbling roughly every two years since 1983. Global 4G coverage is projected to hit nearly 100% in\nthe next five years, already supporting graphics-heavy mobile apps like 4K@30fps. These trends\ncreate an ideal environment for TCC’s development. Moreover, the mounting concern over elec-\ntronic waste highlights the need for sustainable computing. TCC stands out by reducing the need\nfor frequent client device upgrades , lessening environmental impact from manufacturing these\ndevices and their accessories. Furthermore, TCC has the potential to extend the operational dura-\ntion of client devices . As a counterexample, despite battery capacity improvements, the batteries\ndrain much faster in newer generations of iPhones. This phenomenon can be attributed partly to\nthe increasingly compute-intensive software and the higher power consumption of on-device ML\nmodels. Additionally, as Moore’s Law nears its limits and conventional methods struggle to meet\nescalating computational demands — especially for emerging AI/ML applications — the cloud’s\nmaturation offers a solution. Offloading resource-intensive computations to the cloud tackles de-\nvice limitations, ensuring a sustainable path to handle evolving technology demands.\nServerless computing represents the ongoing evolution in computing, mirroring the trajectory\nof cost-effective solutions like TCC. Typical aerverless services include Function-as-a-Service\n(FaaS), Database-as-a-Service (DaaS), ML-as-a-Service (MLaaS), etc. They are built upon the\nconcept of efficient resource utilization and centralized processing, akin to TCC’s approach of\nleveraging centralized computing resources for user interactions. In its essence, serverless epito-\nmizes a paradigm shift towards on-demand computing, offering a scalable and resource-efficient\nmodel for executing code without the need to manage underlying infrastructure. Just as TCC\nsought to simplify hardware maintenance and reduce deployment costs, serverless continues this\ntrajectory by enabling users to execute individual functions without concerns about the backend\ninfrastructure — a concept also aligning with the principles of TCC. While TCC emphasizes thin\nclient interactions with centralized resources, serverless computing prioritizes code execution in a\nscalable, pay-as-you-go cloud environment. This evolution from hardware-centric to code-centric\napproaches underscores the ongoing quest for efficiency, cost reduction, and scalability in com-\nputing paradigms.\nLast but not least, the awareness and plea for data privacy and security are growing. At first\nglance, this trend is seemingly incongruent with TCC and serverless, wherein computation is\ncentralized and predominantly occurs off-device. Nevertheless, recent developments in PPML\npresent a feasible solution for auditing and managing personal data for ML-integrated systems\nlike P EROS. PPML stands at the forefront of ML serving, addressing the critical concern of data\nPage 14 of 29\n\n4 SECURE AND SCALABLE ARCHITECTURE IN THE CLOUD 4.1 Objectives\nprivacy and security. In contexts like serverless and TCC, where data is centralized or processed in\nshared environments, preserving the confidentiality of sensitive information becomes paramount.\nPPML employs techniques like cryptography and data auditing to enable collaborative learning\nwithout exposing raw data, ensuring that models can be trained and refined without compromising\nusers’ privacy. Therefore, in serverless and TCC frameworks, PPML could emerge as a crucial\nsafeguard, allowing the development of intelligent systems like P EROS while safeguarding the\nsensitive information integral to these paradigms.\n4.1 Objectives\nThe objectives here are twofold: (i)protecting security and privacy of personal data; (ii)ensuring\nsystem scalability and facilitating resource sharing. To enhance the security and privacy of users’\npersonal data, I propose to physically store this information off-cloud in close proximity to the\nusers, a concept inspired by Databox [98, 65, 19]. This approach helps streamline data logging\nand auditing, pivotal for leveraging PPML techniques.\nAs most of the computation happens remotely (§2.3), the client-side devices should embody a\n“thin” design, relegating most computational tasks to the cloud. These devices could range from\nsimple displays to projected screens or envisioned VR/AR headsets in the future. Such a TCC-like\narchitecture liberates P EROS from concerns about limited compute and storage when utilizing\nLLMs. In contrast, on-device deployment necessitates heavy quantization of ML models (e.g.,\n[22]), leading to considerable performance degradation.\nImportantly, personal devices like mobile phones and laptops often remain idle in our pockets\nor backpacks. Hence, a pay-as-you-go billing model, enabled by serverless computing, is cru-\ncial for cost-effectiveness . Additionally, serverless architecture enables users to share common\nresources. For instance, multiple family members can use the same LLM, the Interpreter and Di-\nrector, as a shared interface service to their P EROSes, leveraging the LLM’s ability to differentiate\nusers via provided usernames in its prior context. Similarly, organizations could share the same\nLM Manager, aligning retraining policies. Adopting a serverless approach for hosting P EROSes\nalso empowers cloud providers to consolidate server resources and independently scale individual\ncomponents based on demand fluctuation and for load balancing. This strategy can also improve\nhardware utilization and, in turn, energy efficiency in the cloud.\n4.2 Research Question and Challenges\nThe two objectives elaborated above entail the last RQ of this proposal:\nRQ3 How to enable personalized, ML-powered OSes in the cloud, ensuring scalability to accom-\nmodate thousands of users while protecting their privacy and personal data?\nChallenges. The challenges induced by RQ3 encompass three main aspects: (i)the robustness of\nthe data privacy and security model, (ii)concerns regarding performance and latency, and (iii)the\nengineering complexity and deployment costs.\nFirstly, while PPML tries to uphold data privacy, centralizing data storage and processing in\nthe cloud under TCC raises apprehensions about potential data breaches or unauthorized access.\nSafeguarding sensitive user information is imperative, needing robust encryption and stringent\naccess control measures. These policies aim to mitigate risks such as impersonation and man-\nin-the-middle attacks. Additionally, compliance with data protection regulations like GDPR and\nPage 15 of 29\n\n4 SECURE AND SCALABLE ARCHITECTURE IN THE CLOUD 4.3 Research Methods\nDeployment\nGroup 1Subsystems\n(User Z)Subsystems\n(User Y)\n Actuator  \n Watchdogfn Actuator  \nDeployment\nGroup 2fnfn Actuator  \n WatchdogDirector\nLLM\nDecoder\nMicr okernel\nfn LM\n Manager\nDirector\nLLM\nDecoder\nInterpr eter\nLLM\nEncoder\n LM\n ManagerInterpr eter\nLLM\nEncoder\nDatabox XDatabox YDatabox Z\nDatabox\nPersonal Data    \nLogs and Audits\n Z\nY\nX\nDatabox ADatabox BDatabox C C\nB\nAThin\nClients\nData\nAccess\nRequestsSubsystems\n(User X)\nSubsystems\n(User C)Subsystems\n(User B)Micr okernel\nSubsystems\n(User A)\nfnfn\nfn\nFigure 3: P EROS architecture overview.\nHIPAA is important when managing personal data within a cloud-based AI operating system.\nAdhering to these regulations, especially across different jurisdictions, can pose intricate legal and\ncompliance challenges.\nSecondly, concerning performance and latency, TCC heavily relies on uninterrupted network\nconnectivity for user interactions with centralized resources. Despite substantial improvements\nin connectivity — improved, for instance, by offerings like Starlink [80] — network disruptions\nor latency issues can still profoundly impact user experience and degrade the responsiveness of\nthe OS. Furthermore, offloading computational tasks to the cloud can introduce large latency,\nespecially affecting real-time applications like VR/AR, leading to delays due to the round-trip\ncommunication between thin clients and cloud servers.\nThirdly, integrating the triad of technologies — TCC, PPML, and serverless architecture —\ndemands overall compatibility and interoperability. Ensuring these systems work harmoniously\nwithout conflicts presents a significant engineering challenge. Moreover, scaling P EROS in the\ncloud via serverless architectures necessitates careful resource orchestration, ensuring adequacy\nto cater to demand fluctuation while optimizing costs. Unoptimized resource allocation might\nresult in unforeseen expenses, as the net cost of serverless models tends to be much higher than\nalternatives such as renting virtual machines (VMs). Therefore, cost optimization and resource\norchestration remain a critical concern in such deployments.\nPage 16 of 29\n\n4 SECURE AND SCALABLE ARCHITECTURE IN THE CLOUD 4.3 Research Methods\n4.3 Research Methods\nScope. To ensure the feasibility of addressing RQ3 within a year, I restrict the research scope\nas follows. Firstly, similar to addressing RQ1, the focus should initially rest on text-only thin-\nclient devices and target applications. Secondly, although some kernel subsystems involve little\nstate management and can theoretically be shared among trusted tenants, this work should mainly\nfocus on sharing the microkernel (or not sharing any components at the kernel level). Thirdly,\nthe targeted attack model for this study should remain minimal, explicitly excluding sophisticated\nattacks such as meltdown and side-channel attacks. Additionally, this work should operate under\nthe presumption of ample network bandwidth. It anticipates a future with significantly improved\nnetwork connectivity, ensuring suitable conditions both between the client and Databoxes, and\nbetween the client and the cloud. Lastly, specific performance criteria outlined in §2.3 and §3.3\ncould be relaxed, if necessary. Factors such as adaptiveness and latency, which were initially\nstringent, might require adjustment due to the increased complexity of the system. This adjustment\naims to maintain a reasonable balance given the expanded system design.\nApproach. To address security and privacy concerns, I plan to first formally define the threat\nmodel for the proposed system (Figure 3). Specifically, I want to formulate the threat model from\nthe following three angles. (i)Considering the attacker’s viewpoint, threats may originate inter-\nnally or externally inside or outside the Deployment Groups (e.g., families, organizations). (ii)\nFrom an architectural angle, potential vulnerabilities may reside in each stateful system compo-\nnent and the interactions among them. Assessing these components involves ranking them by\ntheir work factor, i.e., the effort an attacker needs to compromise a component. (iii)Measuring\nthreats from an asset perspective involves considering attack motivations. Here, emphasis should\nbe placed on areas housing the most valuable data. With the threat model defined, the subsequent\nstep involves identifying corresponding indicators of attack (IOAs) and indicators of compromise\n(IOCs) in the later stages of the research. IOAs aid proactive vulnerability prevention, while IOCs\nshield the system from known security breaches. Leveraging the computing models embedded in\nthe system design also offers mitigation strategies against potential threats. For instance, access\nto personal data, as per [98], necessitates explicit examination, authorization, and logging. These\naccess traces ( 1) facilitate data auditing and can be used to automatically sign client-provider\ncontracts in PPML. Similarly, deployed as stateless FaaS functions ( 2,6,4), these system com-\nponents are less susceptible compared to stateful services.\nSubsequently, I intend to combine and extend the prototypes from §2.3 and §3.3. To align the\nsystem with the serverless model, the Actuator, Watchdog ( 3), and LM Manager ( 4) should\nbe transformed into standalone functional components to be hosted as FaaS functions ( fn). The\nInterpreter and Director ( 5), as shared components, should ideally follow the MLaaS model,\nalthough its feasibility necessitates further investigation. Regarding the microkernel ( 6), I plan\nto host it as a normal VM instance due to security and availability concerns. Ensuring scalability\nto meet fluctuating demands (§4.1), FaaS components should ideally be independently scalable in\nthis design. Subsystems of P EROS that heavily interact with protected user data, like file scanning\nand storage, should not be shared to uphold data protection in the initial development.\n4.4 Evaluation\nEvaluating the proposed system (Figure 3) includes various factors, including security, perfor-\nmance, scalability, and functionality. As the system design is unique, the baseline could mirror\nPage 17 of 29\n\n5 RELATED WORK\nthe proposed design but without any ML models and data protection via Databox, functioning as\nregular VM clusters. However, this baseline system will not be able to support declarative requests\nfrom users, limiting the comparisons to normal requests like individual system commands and file\noperations.\nMetrics outlined in previous sections (§2.4 and §3.4) for assessing ML models in the system\n(such as latency, response time, correctness, and adaptiveness) remain applicable to experiments\nfor addressing RQ3. Yet, these learning metrics might experience degradation due to increased\ncomplexity and orchestration overheads (§4.1).\nAdjusting earlier performance experiments to a serverless or cloud context is essential. I have\nextensive experience in working with benchmarks such as DeathStarBench [29], ServerlessBench [100],\nand FunctionBench [44]. I intend to tailor them for evaluating of P EROS for RQ3. These bench-\nmarks will help load-test the system, assessing scalability limits, system responses under stress,\nand degradation thresholds. I want to observe system behavior and auto-scaling mechanisms,\nevaluating resource utilization levels, scaling times, and resource allocation efficiency.\nTo assess the threat model, I plan on conducting threat simulations using open-source vulnera-\nbility scanning tools like OpenV AS [1] and Nessus [87], as well as penetration testing tools such\nas Metasploit [60] and Nmap [52]. In these experiments, I will record successful penetration at-\ntempts, access control violations, and identified threats, enabling an extensive comparison with\nthe baseline system.\nFinally, comparing the cost of hosting the proposed system over a defined period against the\nbaseline is also informative. Utilizing a consistent cost model of offerings from platforms like\nAWS or Azure will facilitate this assessment. Ideally, these experiments should provide compre-\nhensive insights into the system’s security, performance, compliance with benchmarks, vulnera-\nbility to threats, and cost-effectiveness compared to the baseline setup.\n5 Related Work\nDeclarative system interfaces. Etzioni et al. introduced Agent OS [26], presenting a goal-\noriented approach to OS command interface. Unlike traditional step-by-step commands, this\nmodel allows users to specify goals, empowering the OS agent to determine command sequences\nbased on system state and its knowledge base. It executes user requests through planning tech-\nniques. Implemented in a distributed UNIX environment, this approach showcases the practical\nintegration of automatic planning and learning into OSes with negligible performance impact. An-\nother important work is the Internet Softbot [25], an AI agent operating with a UNIX shell and\nthe web. It interacts with diverse internet resources using commands such as ftp,telnet ,mail ,\nand sensor utilities like archie ,gopher , and netfind . The Softbot dynamically selects and se-\nquences facilities, adapting its behavior based on collected runtime information. This adaptation\nprovides an integrated, flexible interface to the internet, capable of responding to user requests\nlike “Send the budget memos to Mitchell at CMU.” This paper emphasizes the conceptual foun-\ndation of declarative interface. In contrast, ibot [5] takes a different approach, focusing on agents\nthat control interactive applications directly through GUIs, rather than application APIs. It utilizes\na substrate equipped with sensors processing visual data from the display to identify interface\nelements. The interface then generates mouse/keyboard actions to manipulate these elements.\nMoving forward, Jennings and Terry introduced Fish [37], a command shell fostering intelligent\nand interactive text-based interfaces. It maintains a persistent knowledge repository across concur-\nPage 18 of 29\n\n5 RELATED WORK\nrent sessions, allowing users to define functions and access previous results. Similarly, Copas and\nEdmonds described interactive interfaces [18] in planning systems, such as geographic informa-\ntion systems, known for their high functionality but poor usability. Furthermore, Petrick explored\nusing DCOP (Desktop COmmunication Protocol) to link desktop application services, employing\nknowledge-level conditional planners to control KDE applications [66]. This research demon-\nstrates executable plans for application manipulation and information retrieval, underscoring the\npotential of desktop interfaces for automated planning in OSes.\nUnfortunately, these declarative interfaces and softbots from previous decades heavily rely on\npreset rules, primarily subsets of first-order logic stored in a knowledge base [46] containing opera-\ntion descriptions and related semantics. The “AI” or “learning” described in these papers revolves\naround planning based on reasoning and derivation of logical expressions. Consequently, these\ninterfaces lack adaptivity to shifts in user usage patterns, extensibility to new APIs and function-\nalities, as well as the smooth multi-turn conversational capability of LLMs (§2.1).\nML for system design and modeling. There is a large body of work employing ML models for\nsystem design and modeling, ranging from low-level hardware design (e.g., circuit analysis [64, 3],\nnetwork-on-chip [68, 14, 39], and logic synthesis [51, 20, 90]) to high-level resource allocation\nand management (e.g., kernel scheduling [97, 27, 91], power management [68, 28, 23, 40, 35],\nand cloud orchestration [101, 102, 103, 96]). Here, I elaborate more on existing work related to\nmemory and storage systems for RQ2\nIn the realm of memory systems, exploiting ML-based performance models proves instrumental\nin examining the tradeoffs across various objectives. Dong et al. [24] delve into NVM-based cache\nhierarchies, using an ML model to predict higher-level features like cache read/write misses and\ninstruction-per-cycle rates from lower-level attributes such as cache associativity, capacity, and la-\ntency. Block2Vec [20] trains an ML model to derive optimal vector representations for individual\ndata blocks, thereby uncovering block correlations and enabling enhanced caching and prefetch-\ning optimizations through vector distances. In a similar vein, Shi et al. [74] harness a graph neural\nnetwork to mix static code and dynamic execution into a unified representation. This approach\nmodels both data flows, such as prefetching, and control flows, including branch prediction, offer-\ning a comprehensive understanding of program behavior. A recent work that caught my eye was\nLeaFLT [82], a learning-based flash translation layer for SSDs. LeaFTL employs runtime linear\nregression to dynamically learn address mappings, reducing memory usage for addressing tables,\nenhancing data caching in SSD controllers and demonstrating speedup in storage performance\ncompared to existing flash translation schemes, while consuming less memory for mapping tables.\nAlthough the abundant literature on this topic offers valuable insights into applying ML methods\nto systems research, their primary focus remains on enhancing the performance of individual sys-\ntem components rather than facilitating personalized user experiences at the OS level. In contrast,\nRQ2 emphasizes the collaborative synergy among kernel components, spanning from high-level\nfilesystem configurations and allocation policies down to learning to index data blocks of a file.\nThe proposed approach aims to adapt proactively to individual usage patterns, thereby tailoring the\nOS for personalized experience. Furthermore, there exists a notable gap in the exploration of ML\nmethodologies to enhance filesystem operations holistically. The earliest reference I found, dat-\ning back decades, is SUMPY [79], a software agent for UNIX filesystem maintenance. SUMPY\nfocuses on optimizing disk space through file compression and backup. Its layered structure en-\nables concurrent goal pursuit, resilience against failures, and the addition of new tasks. However,\nit relies on basic fuzzy logic, which restricts its extensibility and limits its scope to automating\nrudimentary background tasks.\nPage 19 of 29\n\n5 RELATED WORK\nTCC, Serverless, and PPML. In recent years, research in TCC has been relatively scarce, pri-\nmarily focusing on performance evaluations of existing solutions (e.g., [71, 17, 62, 53, 99, 72])\nand specialized systems tailored for specific use cases (e.g., [54, 21, 83, 4, 77, 49]).\nIn contrast, serverless computing has gained significant attention, exploring various aspects such\nas minimizing cold start overhead [76, 73, 75, 89, 70, 2], function scheduling [57, 41, 85, 10, 13,\n32], and workflow orchestration [55, 67, 56, 15, 69]. However, there is an absence of attention\ntowards providing personalized, ML-enhenced OSes in the cloud as stateful services. Moreover,\nno prior work has attempted to scale similar services by sharing stateless components of the hosted\nstateful applications.\nConcerning data protection for internet-accessible applications, practical systems like the Databox [98,\n65, 19] and theoretical models like the Databank [34] offer promising solutions. Databox includes\nphysical and cloud-based software components designed to empower individuals in managing,\nlogging, and monitoring access to their personal data by third parties. Similarly, the Databank\nmodel showcases formal guarantees on information flow propagation and policy enforcement, of-\nfering insights into monitoring algorithms and prototype infrastructure.\nWithin the sphere of ML, PPML has emerged as a rapidly evolving field, encompassing tech-\nniques like cryptographic hashing, secure computation for inference, and non-colluding ML servers.\nFor instance, platforms like Piranha [94] accelerate secret sharing-based multi-party computation\nprotocols using GPUs. Meanwhile, MUSE [50] introduces a resilient two-party secure infer-\nence protocol employing novel cryptographic approaches. SecureML [59] innovates protocols for\nPPML in various domains like linear and logistic regression, as well as neural network training,\nensuring privacy while jointly training models across non-colluding servers.\nThe system model of Databox offers valuable insights into data access logging and auditing,\nwhich could be leveraged in theoretical models such as the Databank. I believe that a low-hanging\nfruit in the PPML domain lies in efficiently signing contracts between data holders (e.g., users)\nand model holders (e.g., cloud providers) based on detailed logs and audits for the requests and\naccesses of users’ personal data.\nPage 20 of 29\n\nREFERENCES REFERENCES\nReferences\n[1] G. AG. Openvas - open vulnerability assessment scanner. https://www.openvas.org/ ,\n2023.\n[2] A. Agache, M. Brooker, A. Iordache, A. Liguori, R. Neugebauer, P. Piwonka, and D.-\nM. Popa. Firecracker: Lightweight virtualization for serverless applications. In Sym-\nposium on Networked Systems Design and Implementation , 2020. URL https://api.\nsemanticscholar.org/CorpusID:211567076 .\n[3] M. B. Alawieh, W. Li, Y . Lin, L. Singhal, M. A. Iyer, and D. Z. Pan. High-definition\nrouting congestion prediction for large-scale fpgas. 2020 25th Asia and South Pacific\nDesign Automation Conference (ASP-DAC) , pages 26–31, 2020. URL https://api.\nsemanticscholar.org/CorpusID:214692445 .\n[4] F. A. Ali, P. Simoens, B. Vankeirsbilck, L. Deboosere, B. Dhoedt, P. Demeester, R. Torrea-\nDuran, and C. Desset. Reducing power consumption of mobile thin client devices. 2009.\nURL https://api.semanticscholar.org/CorpusID:37738031 .\n[5] R. S. Amant and L. Zettlemoyer. User interface softbots. In AAAI/IAAI , 2000. URL\nhttps://api.semanticscholar.org/CorpusID:5413725 .\n[6] R. S. Amant, D. A. Jim ´enez, and D. Burger. Low-power, high-performance analog neu-\nral branch prediction. 2008 41st IEEE/ACM International Symposium on Microarchitec-\nture, pages 447–458, 2008. URL https://api.semanticscholar.org/CorpusID:\n14553490 .\n[7] Amazon. Amazon mechanical turk. https://www.mturk.com , 2023.\n[8] AMD. Adaptive soc for any application from cloud to edge. https://www.xilinx.com/\nproducts/silicon-devices/acap/versal.html , 2023.\n[9] Apple. Apple unveils m3, m3 pro, and m3 max, the most advanced chips\nfor a personal computer. https://www.apple.com/newsroom/2023/10/\napple-unveils-m3-m3-pro-and-m3-max-the-most-advanced-chips-for-a-personal-computer/ ,\n2023.\n[10] M. S. Aslanpour, A. N. Toosi, M. A. Cheema, and R. K. Gaire. Energy-aware resource\nscheduling for serverless edge computing. 2022 22nd IEEE International Symposium on\nCluster, Cloud and Internet Computing (CCGrid) , pages 190–199, 2022. URL https:\n//api.semanticscholar.org/CorpusID:250715298 .\n[11] J. Axboe. Flexible i/o tester. https://github.com/axboe/fio , 2023.\n[12] N. Bansal and M. Harchol-Balter. Analysis of srpt scheduling: investigating unfair-\nness. In Measurement and Modeling of Computer Systems , 2001. URL https://api.\nsemanticscholar.org/CorpusID:6418437 .\n[13] V . M. Bhasi, J. R. Gunasekaran, A. Sharma, M. T. Kandemir, and C. R. Das. Cypress:\ninput size-sensitive container provisioning and request scheduling for serverless platforms.\nPage 21 of 29\n\nREFERENCES REFERENCES\nProceedings of the 13th Symposium on Cloud Computing , 2022. URL https://api.\nsemanticscholar.org/CorpusID:253385775 .\n[14] J. A. Boyan and M. L. Littman. Packet routing in dynamically changing networks: A\nreinforcement learning approach. In Neural Information Processing Systems , 1993. URL\nhttps://api.semanticscholar.org/CorpusID:364332 .\n[15] S. Burckhardt, B. Chandramouli, C. L. Gillum, D. Justo, K. Kallas, C. McMahon,\nC. S. Meiklejohn, and X. Zhu. Netherite: Efficient execution of serverless workflows.\nProc. VLDB Endow. , 15:1591–1604, 2022. URL https://api.semanticscholar.org/\nCorpusID:249916915 .\n[16] B. Calder, D. Grunwald, M. P. Jones, D. C. Lindsay, J. H. Martin, M. C. Mozer, and\nB. G. Zorn. Evidence-based static branch prediction using machine learning. ACM Trans.\nProgram. Lang. Syst. , 19:188–222, 1997. URL https://api.semanticscholar.org/\nCorpusID:8865665 .\n[17] P. Casas, M. Seufert, S. Egger, and R. Schatz. Quality of experience in remote virtual\ndesktop services. 2013 IFIP/IEEE International Symposium on Integrated Network Man-\nagement (IM 2013) , pages 1352–1357, 2013. URL https://api.semanticscholar.\norg/CorpusID:10213123 .\n[18] C. V . Copas and E. A. Edmonds. Intelligent interfaces through interactive planners. Interact.\nComput. , 12:545–564, 2000. URL https://api.semanticscholar.org/CorpusID:\n5527993 .\n[19] A. Crabtree, T. Lodge, J. Colley, C. Greenhalgh, K. Glover, H. Haddadi, Y . Amar,\nR. Mortier, Q. Li, J. Moore, et al. Building accountability into the internet of things: the iot\ndatabox model. Journal of Reliable Intelligent Environments , 4:39–55, 2018.\n[20] S. Dai, Y . Zhou, H. Zhang, E. Ustun, E. F. Y . Young, and Z. Zhang. Fast and accu-\nrate estimation of quality of results in high-level synthesis with machine learning. 2018\nIEEE 26th Annual International Symposium on Field-Programmable Custom Computing\nMachines (FCCM) , pages 129–132, 2018. URL https://api.semanticscholar.org/\nCorpusID:5078393 .\n[21] L. Deboosere, J. De Wachter, P. Simoens, F. De Turck, B. Dhoedt, and P. Demeester. Thin\nclient computing solutions in low-and high-motion scenarios. In International Conference\non Networking and Services (ICNS’07) , pages 38–38. IEEE, 2007.\n[22] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of\nquantized llms. ArXiv , abs/2305.14314, 2023. URL https://api.semanticscholar.\norg/CorpusID:258841328 .\n[23] S. M. P. Dinakarrao, H. Yu, H. Huang, and D. Xu. A q-learning based self-adaptive i/o\ncommunication for 2.5d integrated many-core microprocessor and memory. IEEE Transac-\ntions on Computers , 65:1185–1196, 2016. URL https://api.semanticscholar.org/\nCorpusID:30396137 .\nPage 22 of 29\n\nREFERENCES REFERENCES\n[24] X. Dong, N. P. Jouppi, and Y . Xie. A circuit-architecture co-optimization framework for\nexploring nonvolatile memory hierarchies. ACM Transactions on Architecture and Code\nOptimization (TACO) , 10:1 – 22, 2013. URL https://api.semanticscholar.org/\nCorpusID:624945 .\n[25] O. Etzioni and D. S. Weld. A softbot-based interface to the internet. Commun. ACM , 37:\n72–76, 1994. URL https://api.semanticscholar.org/CorpusID:2447472 .\n[26] O. Etzioni, H. M. Levy, R. B. Segal, and C. A. Thekkath. Os agents: Using ai techniques in\nthe operating system environment. 1993. URL https://api.semanticscholar.org/\nCorpusID:11991416 .\n[27] A. Fedorova, D. Vengerov, D. Vengerov, and D. Doucette. Operating system scheduling\non heterogeneous core systems. 2007. URL https://api.semanticscholar.org/\nCorpusID:14823905 .\n[28] Q. Fettes, M. Clark, R. C. Bunescu, A. Karanth, and A. Louri. Dynamic voltage and\nfrequency scaling in nocs with supervised and reinforcement learning techniques. IEEE\nTransactions on Computers , 68:375–389, 2019. URL https://api.semanticscholar.\norg/CorpusID:52840323 .\n[29] Y . Gan, Y . Zhang, D. Cheng, A. Shetty, P. Rathi, N. Katarki, A. Bruno, J. Hu, B. Ritchken,\nB. Jackson, et al. An open-source benchmark suite for microservices and their hardware-\nsoftware implications for cloud & edge systems. In Proceedings of the Twenty-Fourth Inter-\nnational Conference on Architectural Support for Programming Languages and Operating\nSystems , pages 3–18, 2019.\n[30] J. Gauci, E. Conti, Y . Liang, K. Virochsiri, Y . He, Z. Kaden, V . Narayanan, X. Ye, and\nS. Fujimoto. Horizon: Facebook’s open source applied reinforcement learning platform.\nArXiv , abs/1811.00260, 2018. URL https://api.semanticscholar.org/CorpusID:\n53204036 .\n[31] Google. Write your application as a modular binary. deploy it as a set of microservices.\nhttps://serviceweaver.dev , 2023.\n[32] H. H `e. In-vitro serverless clusters.\n[33] G. Heiser. The sel4 microkernel–an introduction. The seL4 Foundation , 1, 2020.\n[34] F. Hublet. The databank model. Master’s thesis, ETH Zurich, 2021.\n[35] C. Imes, S. A. Hofmeyr, and H. Hoffmann. Energy-efficient application resource schedul-\ning using machine learning classifiers. Proceedings of the 47th International Conference\non Parallel Processing , 2018. URL https://api.semanticscholar.org/CorpusID:\n195348382 .\n[36] A. A. T. Isstaif and R. Mortier. Towards latency-aware linux scheduling for serverless work-\nloads. Proceedings of the 1st Workshop on SErverless Systems, Applications and MEthod-\nologies , 2023. URL https://api.semanticscholar.org/CorpusID:258486714 .\nPage 23 of 29\n\nREFERENCES REFERENCES\n[37] J. S. Jennings and N. D. Terry. Towards more intelligent and interactive interfaces. In\nThe Florida AI Research Society , 1999. URL https://api.semanticscholar.org/\nCorpusID:13371719 .\n[38] D. A. Jim ´enez and C. Lin. Dynamic branch prediction with perceptrons. Proceedings HPCA\nSeventh International Symposium on High-Performance Computer Architecture , pages\n197–206, 2001. URL https://api.semanticscholar.org/CorpusID:3184222 .\n[39] B. K. Joardar, R. G. Kim, J. R. Doppa, P. P. Pande, D. Marculescu, and R. Mar-\nculescu. Learning-based application-agnostic 3d noc design for heterogeneous many-\ncore systems. IEEE Transactions on Computers , 68:852–866, 2018. URL https:\n//api.semanticscholar.org/CorpusID:53046469 .\n[40] D.-C. Juan and D. Marculescu. Power-aware performance increase via core/uncore rein-\nforcement control for chip-multiprocessors. In International Symposium on Low Power\nElectronics and Design , 2012. URL https://api.semanticscholar.org/CorpusID:\n15136374 .\n[41] K. Kaffes, N. J. Yadwadkar, and C. E. Kozyrakis. Hermod: principled and practical schedul-\ning for serverless functions. Proceedings of the 13th Symposium on Cloud Computing ,\n2022. URL https://api.semanticscholar.org/CorpusID:253385680 .\n[42] T. Kaler, A.-S. Iliopoulos, P. Murzynowski, T. B. Schardl, C. E. Leiserson, and J. Chen.\nCommunication-efficient graph neural networks with probabilistic neighborhood expan-\nsion analysis and caching. ArXiv , abs/2305.03152, 2023. URL https://api.\nsemanticscholar.org/CorpusID:258546707 .\n[43] S. Kharbanda, A. Banerjee, E. Schultheis, and R. Babbar. Cascadexml: Rethinking\ntransformers for end-to-end multi-resolution training in extreme multi-label classification.\nArXiv , abs/2211.00640, 2022. URL https://api.semanticscholar.org/CorpusID:\n253255408 .\n[44] J. Kim and K. Lee. Practical cloud workloads for serverless faas. In Proceedings of the\nACM Symposium on Cloud Computing , pages 477–477, 2019.\n[45] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index\nstructures. Proceedings of the 2018 International Conference on Management of Data ,\n2017. URL https://api.semanticscholar.org/CorpusID:6038777 .\n[46] B. Kruit, H. He, and J. Urbani. Tab2know: Building a knowledge base from tables in\nscientific papers. In International Workshop on the Semantic Web , 2020. URL https:\n//api.semanticscholar.org/CorpusID:226229411 .\n[47] B. Kruit, H. He, and J. Urbani. Tab2know: Building a knowledge base from tables in\nscientific papers. In The Semantic Web–ISWC 2020: 19th International Semantic Web\nConference, Athens, Greece, November 2–6, 2020, Proceedings, Part I 19 , pages 349–365.\nSpringer, 2020.\n[48] Kubernetes. Production-grade container orchestration. https://kubernetes.io , 2023.\nPage 24 of 29\n\nREFERENCES REFERENCES\n[49] J. Lee, D. Seo, Y . Kim, C. Y . Choi, H. K. Choi, and I. Jung. Thin-client computing for\nsupporting the qos of streaming media in mobile devices. In Communication Systems and\nApplications , 2006. URL https://api.semanticscholar.org/CorpusID:394966 .\n[50] R. T. Lehmkuhl, P. Mishra, A. Srinivasan, and R. A. Popa. Muse: Secure inference resilient\nto malicious clients. In IACR Cryptology ePrint Archive , 2021. URL https://api.\nsemanticscholar.org/CorpusID:232216360 .\n[51] H.-Y . Liu and L. P. Carloni. On learning-based methods for design-space exploration with\nhigh-level synthesis. 2013 50th ACM/EDAC/IEEE Design Automation Conference (DAC) ,\npages 1–7, 2013. URL https://api.semanticscholar.org/CorpusID:12059048 .\n[52] G. Lyon. Nmap: the network mapper - free security scanner. https://nmap.org/ , 2023.\n[53] D. Maga, M. Hiebel, and C. Knermann. Comparison of two ict solutions: desktop pc versus\nthin client computing. The International Journal of Life Cycle Assessment , 18:861–871,\n2013.\n[54] E. Maga ˜na, I. Sesma, D. Morat ´o, and M. Izal. Remote access protocols for desktop-as-a-\nservice solutions. PLoS ONE , 14, 2019. URL https://api.semanticscholar.org/\nCorpusID:58022821 .\n[55] A. Y . Mahgoub, E. B. Yi, K. Shankar, S. Elnikety, S. Chaterji, and S. Bagchi. Orion and\nthe three rights: Sizing, bundling, and prewarming for serverless dags. In USENIX Sym-\nposium on Operating Systems Design and Implementation , 2022. URL https://api.\nsemanticscholar.org/CorpusID:252819711 .\n[56] A. Y . Mahgoub, E. B. Yi, K. Shankar, E. Minocha, S. Elnikety, S. Bagchi, and S. Chaterji.\nWisefuse: Workload characterization and dag transformation for serverless workflows. Ab-\nstract Proceedings of the 2022 ACM SIGMETRICS/IFIP PERFORMANCE Joint Inter-\nnational Conference on Measurement and Modeling of Computer Systems , 2022. URL\nhttps://api.semanticscholar.org/CorpusID:249282028 .\n[57] N. Mahmoudi and H. Khazaei. Performance modeling of serverless computing plat-\nforms. IEEE Transactions on Cloud Computing , 10:2834–2847, 2022. URL https:\n//api.semanticscholar.org/CorpusID:226587161 .\n[58] Microsoft. Discover the power of ai with copilot in windows. https://www.microsoft.\ncom/en-us/windows/copilot-ai-features , 2023.\n[59] P. Mohassel and Y . Zhang. Secureml: A system for scalable privacy-preserving machine\nlearning. 2017 IEEE Symposium on Security and Privacy (SP) , pages 19–38, 2017. URL\nhttps://api.semanticscholar.org/CorpusID:11605311 .\n[60] H. Moore. Metasploit. https://www.metasploit.com/ , 2023.\n[61] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang, M. Elibol, Z. Yang,\nW. Paul, M. I. Jordan, et al. Ray: A distributed framework for emerging {AI}applications.\nIn13th USENIX symposium on operating systems design and implementation (OSDI 18) ,\npages 561–577, 2018.\nPage 25 of 29\n\nREFERENCES REFERENCES\n[62] J. Nieh, S. J. Yang, and N. Novik. A comparison of thin-client computing architectures.\n2000.\n[63] OpenAI. Gpt-4 turbo. https://help.openai.com/en/articles/\n8555510-gpt-4-turbo , 2023.\n[64] P.-C. Pan, C.-C. Huang, and H.-M. Chen. Late breaking results: An efficient learning-\nbased approach for performance exploration on analog and rf circuit synthesis. 2019 56th\nACM/IEEE Design Automation Conference (DAC) , pages 1–2, 2019. URL https://api.\nsemanticscholar.org/CorpusID:163164981 .\n[65] C. Perera, S. Y . Wakenshaw, T. Baarslag, H. Haddadi, A. K. Bandara, R. Mortier, A. Crab-\ntree, I. C. Ng, D. McAuley, and J. Crowcroft. Valorising the iot databox: creating value\nfor everyone. Transactions on Emerging Telecommunications Technologies , 28(1):e3125,\n2017.\n[66] R. P. A. Petrick. Planning for desktop services. 2007. URL https://api.\nsemanticscholar.org/CorpusID:55963917 .\n[67] D. B. Pons, P. Sutra, M. S. Artigas, G. Par ´ıs, and P. G. L ´opez. Stateful serverless computing\nwith crucial. ACM Transactions on Software Engineering and Methodology (TOSEM) , 31:\n1 – 38, 2022. URL https://api.semanticscholar.org/CorpusID:247300542 .\n[68] M. F. Reza, T. T. Le, B. Dey, M. A. Bayoumi, and D. Zhao. Neuro-noc: Energy optimization\nin heterogeneous many-core noc using neural networks in dark silicon era. 2018 IEEE\nInternational Symposium on Circuits and Systems (ISCAS) , pages 1–5, 2018. URL https:\n//api.semanticscholar.org/CorpusID:53084745 .\n[69] S. Risco, G. Molt ´o, D. M. Naranjo, and I. Blanquer. Serverless workflows for containerised\napplications in the cloud continuum. Journal of Grid Computing , 19, 2021. URL https:\n//api.semanticscholar.org/CorpusID:235901547 .\n[70] R. B. Roy, T. Patel, and D. Tiwari. Icebreaker: warming serverless functions better with\nheterogeneity. Proceedings of the 27th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems , 2022. URL https://api.\nsemanticscholar.org/CorpusID:247026551 .\n[71] D. Schlosser, B. Staehle, A. Binzenh ¨ofer, and B. Boder. Improving the qoe of citrix thin\nclient users. 2010 IEEE International Conference on Communications , pages 1–6, 2010.\nURL https://api.semanticscholar.org/CorpusID:1769656 .\n[72] B. K. Schmidt, M. S. Lam, and J. D. Northcutt. The interactive performance of slim: a\nstateless, thin-client architecture. ACM SIGOPS Operating Systems Review , 33(5):32–47,\n1999.\n[73] M. Shahrad, R. Fonseca, ´I. Goiri, G. I. Chaudhry, P. Batum, J. Cooke, E. Laureano, C. Tres-\nness, M. Russinovich, and R. Bianchini. Serverless in the wild: Characterizing and op-\ntimizing the serverless workload at a large cloud provider. ArXiv , abs/2003.03423, 2020.\nURL https://api.semanticscholar.org/CorpusID:212633767 .\nPage 26 of 29\n\nREFERENCES REFERENCES\n[74] Z. Shi, K. Swersky, D. Tarlow, P. Ranganathan, and M. Hashemi. Learning execu-\ntion through neural code fusion. ArXiv , abs/1906.07181, 2019. URL https://api.\nsemanticscholar.org/CorpusID:189999405 .\n[75] W. Shin, W.-H. Kim, and C. Min. Fireworks: a fast, efficient, and safe serverless framework\nusing vm-level post-jit snapshot. Proceedings of the Seventeenth European Conference\non Computer Systems , 2022. URL https://api.semanticscholar.org/CorpusID:\n247765574 .\n[76] P. Silva, D. Fireman, and T. E. Pereira. Prebaking functions to warm the serverless cold\nstart. Proceedings of the 21st International Middleware Conference , 2020. URL https:\n//api.semanticscholar.org/CorpusID:228085887 .\n[77] P. Simoens, B. Joveski, L. Gardenghi, I.-J. Marshall, B. Vankeirsbilck, M. P. Mitrea, F. J.\nPrˆeteux, F. D. Turck, and B. Dhoedt. Optimized mobile thin clients through a mpeg-4\nbifs semantic remote display framework. Multimedia Tools and Applications , 61:447–470,\n2011. URL https://api.semanticscholar.org/CorpusID:2181477 .\n[78] A. Skiadopoulos, Q. Li, P. Kraft, K. Kaffes, D. W.-K. Hong, S. Mathew, D. Bestor, M. J.\nCafarella, V . Gadepally, G. Graefe, J. Kepner, C. E. Kozyrakis, T. Kraska, M. Stone-\nbraker, L. Suresh, and M. A. Zaharia. Dbos: A dbms-oriented operating system.\nProc. VLDB Endow. , 15:21–30, 2021. URL https://api.semanticscholar.org/\nCorpusID:245827586 .\n[79] H. Song. Sumpy: A fuzzy software agent. 2007. URL https://api.semanticscholar.\norg/CorpusID:1371386 .\n[80] SpaceX. Starlink. https://www.starlink.com/ , 2023.\n[81] J. Stojkovic, D. Skarlatos, A. Kokolis, T. Xu, and J. Torrellas. Parallel virtualized memory\ntranslation with nested elastic cuckoo page tables. Proceedings of the 27th ACM Interna-\ntional Conference on Architectural Support for Programming Languages and Operating\nSystems , 2022. URL https://api.semanticscholar.org/CorpusID:246472365 .\n[82] J. Sun, S. Li, Y . Sun, C. Sun, D. Vucinic, and J. Huang. Leaftl: A learning-based flash trans-\nlation layer for solid-state drives. In Proceedings of the 28th ACM International Conference\non Architectural Support for Programming Languages and Operating Systems, Volume 2 ,\npages 442–456, 2023.\n[83] M. Suznjevic, L. Skorin-Kapov, and I. Humar. Statistical user behavior detection and qoe\nevaluation for thin client services. Computer Science and Information Systems , 12(2):587–\n605, 2015.\n[84] A. Tanenbaum, R. Appuswamy, H. Bos, L. Cavallaro, C. Giuffrida, T. Hrub `y, J. Herder, and\nE. V AN DER. Minix 3: status report and current research. login: The USENIX Magazine ,\n2010.\n[85] Q. Tang, R. Xie, F. Yu, T. Chen, R. Zhang, T. Huang, and Y . jie Liu. Distributed task\nscheduling in serverless edge computing networks for the internet of things: A learning\napproach. IEEE Internet of Things Journal , 9:19634–19648, 2022. URL https://api.\nsemanticscholar.org/CorpusID:248188338 .\nPage 27 of 29\n\nREFERENCES REFERENCES\n[86] V . Tarasov. Filebench – a model based file system workload generator, 2018.\n[87] I. Tenable. Nessus vulnerability scanner: Network security solution. https://www.\ntenable.com/products/nessus , 2023.\n[88] Y . Tian, Q. Gong, W. Shang, Y . Wu, and C. L. Zitnick. Elf: An extensive, lightweight and\nflexible research platform for real-time strategy games. ArXiv , abs/1707.01067, 2017. URL\nhttps://api.semanticscholar.org/CorpusID:1160900 .\n[89] D. Ustiugov, P. Petrov, M. Kogias, E. Bugnion, and B. Grot. Benchmarking, analysis, and\noptimization of serverless function snapshots. Proceedings of the 26th ACM International\nConference on Architectural Support for Programming Languages and Operating Systems ,\n2021. URL https://api.semanticscholar.org/CorpusID:231699170 .\n[90] E. Ustun, C. Deng, D. Pal, Z. Li, and Z. Zhang. Accurate operation delay prediction for fpga\nhls using graph neural networks. 2020 IEEE/ACM International Conference On Computer\nAided Design (ICCAD) , pages 1–9, 2020. URL https://api.semanticscholar.org/\nCorpusID:221727103 .\n[91] D. Vengerov. A reinforcement learning framework for utility-based scheduling in resource-\nconstrained systems. Future Gener. Comput. Syst. , 25:728–736, 2009. URL https://\napi.semanticscholar.org/CorpusID:12769472 .\n[92] E. N. Wang, A. Kannan, Y . Liang, B. Chen, and M. Chowdhury. Flint: A platform\nfor federated learning integration. ArXiv , abs/2302.12862, 2023. URL https://api.\nsemanticscholar.org/CorpusID:257220077 .\n[93] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. H. hsin Chi, and D. Zhou. Self-consistency\nimproves chain of thought reasoning in language models. ArXiv , abs/2203.11171, 2022.\nURL https://api.semanticscholar.org/CorpusID:247595263 .\n[94] J.-L. Watson, S. Wagh, and R. A. Popa. Piranha: A gpu platform for secure computation.\nInIACR Cryptology ePrint Archive , 2022. URL https://api.semanticscholar.org/\nCorpusID:250361679 .\n[95] T. Weibel. Smart-umass trace repository. https://traces.cs.umass.edu/index.php/\nStorage/Storage , 2013.\n[96] Y . wen Wang, D. Crankshaw, N. J. Yadwadkar, D. S. Berger, C. E. Kozyrakis, and R. Bian-\nchini. Sol: safe on-node learning in cloud platforms. Proceedings of the 27th ACM Inter-\nnational Conference on Architectural Support for Programming Languages and Operating\nSystems , 2022. URL https://api.semanticscholar.org/CorpusID:246275770 .\n[97] S. Whiteson and P. Stone. Corrected version : See errata section adaptive job routing and\nscheduling. 2005. URL https://api.semanticscholar.org/CorpusID:9636537 .\n[98] P. Yadav, J. Moore, Q. Li, R. Mortier, A. Brown, A. Crabtree, C. Greenhalgh, D. McAuley,\nY . Amar, A. S. Shamsabadi, et al. Providing occupancy as a service with databox. In\nProceedings of the 1st ACM International Workshop on Smart Cities and Fog Computing ,\npages 29–34, 2018.\nPage 28 of 29\n\nREFERENCES REFERENCES\n[99] S. J. Yang, J. Nieh, M. Selsky, and N. Tiwari. The performance of remote display mecha-\nnisms for thin-client computing. In USENIX Annual Technical Conference, General Track ,\npages 131–146, 2002.\n[100] T. Yu, Q. Liu, D. Du, Y . Xia, B. Zang, Z. Lu, P. Yang, C. Qin, and H. Chen. Charac-\nterizing serverless platforms with serverlessbench. In Proceedings of the ACM Sympo-\nsium on Cloud Computing , SoCC ’20. Association for Computing Machinery, 2020. doi:\n10.1145/3419111.3421280. URL https://doi.org/10.1145/3419111.3421280 .\n[101] H. Zhang, B. Tang, X. Geng, and H. Ma. Learning driven parallelization for large-scale\nvideo workload in hybrid cpu-gpu cluster. Proceedings of the 47th International Conference\non Parallel Processing , 2018. URL https://api.semanticscholar.org/CorpusID:\n195348830 .\n[102] Y . Zhang, W. Hua, Z. Zhou, E. Suh, C. Delimitrou, and WeizheHua. Sinan: Ml-based and\nqos-aware resource management for cloud microservices. Proceedings of the 26th ACM In-\nternational Conference on Architectural Support for Programming Languages and Operat-\ning Systems , 2021. URL https://api.semanticscholar.org/CorpusID:232118940 .\n[103] Z. Zhou, Y . Zhang, and C. Delimitrou. Aquatope: Qos-and-uncertainty-aware resource\nmanagement for multi-stage serverless workflows. Proceedings of the 28th ACM Inter-\nnational Conference on Architectural Support for Programming Languages and Operat-\ning Systems, Volume 1 , 2022. URL https://api.semanticscholar.org/CorpusID:\n254927697 .\nPage 29 of 29",
  "textLength": 86287
}