{
  "paperId": "c315333cda29b79101309002dc5172cf4f5085bf",
  "title": "A Critical Analysis of Classifier Selection in Learned Bloom Filters",
  "pdfPath": "c315333cda29b79101309002dc5172cf4f5085bf.pdf",
  "text": "A Critical Analysis of Classiﬁer Selection in Learned Bloom Filters\nDario Malchiodi1,3, Davide Raimondi1, Giacomo Fumagalli1, Raﬀaele Giancarlo2, Marco Frasca1\n1Dept. of Computer Science, University of Milan, Milano, Italy\n2Dept. of Mathematics and CS, University of Palermo, Palermo, Italy\n3DSRC, University of Milan, Milano, Italy\n{dario.malchiodi, marco.frasca }@unimi.it,\n{davide.raimondi2, giacomo.fumagalli1 }@studenti.unimi.it,\nraﬀaele.giancarlo@unipa.it\nAbstract\nLearned Bloom Filters, i.e., models induced from data\nvia machine learning techniques and solving the ap-\nproximate set membership problem, have recently\nbeen introduced with the aim of enhancing the perfor-\nmance of standard Bloom Filters, with special focus\non space occupancy. Unlike in the classical case, the\n“complexity” of the data used to build the ﬁlter might\nheavily impact on its performance. Therefore, here\nwe propose the ﬁrst in-depth analysis, to the best of\nour knowledge, for the performance assessment of a\ngiven Learned Bloom Filter, in conjunction with a\ngiven classiﬁer, on a dataset of a given classiﬁcation\ncomplexity. Indeed, we propose a novel methodol-\nogy, supported by software, for designing, analyzing\nand implementing Learned Bloom Filters in function\nof speciﬁc constraints on their multi-criteria nature\n(that is, constraints involving space eﬃciency, false\npositive rate, and reject time). Our experiments show\nthat the proposed methodology and the supporting\nsoftware are valid and useful: we ﬁnd out that only\ntwo classiﬁers have desirable properties in relation\nto problems with diﬀerent data complexity, and, in-\nterestingly, none of them has been considered so far\nin the literature. We also experimentally show that\nthe Sandwiched variant of Learned Bloom ﬁlters is\nthe most robust to data complexity and classiﬁer per-\nformance variability, as well as those usually having\nsmaller reject times. The software can be readily usedto test new Learned Bloom Filter proposals, which\ncan be compared with the best ones identiﬁed here.\n1 Introduction\nRecent studies have highlighted how the impact of\nmachine learning has the potential to change the way\nwe design and analyze algorithms and data structures.\nIndeed, the resulting research area of Learned Data\nStructures has had a well documented impact on a\nbroadandstrategicdomainsuchasthatofDataBases.\nAn analogous impact can be expected for Network\nManagement [ 50] and Computational Biology [ 28].\nMore in general, as well argued in [ 30], this novel\nway to use machine learning has the potential to\nchange how Data Systems are designed. This new\ntrend was initiated by Kraska et. al. [ 31], as far as\nData Structures are concerned, and then extended\nto Algorithms by Mitzenmacher and S. Vassilvitskii\n[40]. Concerning the former, the common theme to\nthis new approach is that of training a Classiﬁer [ 17]\nor a Regression Model [ 22] on the input data. Then\nsuch a learned model is used as an “oracle” that a\ngiven “classical” data structure can use in order to\nanswer queries with improved performance (usually\ntime). To date, Learned Indexes have been the most\nstudied, e.g., [ 2,19,20,27,29,31,35,36,37], although\nRank/select data structures have also received some\nattention [ 4]. In this work, we focus on Bloom Filters\n(BFs) [3] which, as detailed in what follows, have\n1arXiv:2211.15565v1  [cs.LG]  28 Nov 2022\n\nalso received attention in the realm of Learned Data\nStructures. Such an attention is quite natural, due to\nthe fundamental nature and pervasive use of Bloom\nFilters. Indeed, many variants and alternatives for\nthese ﬁlters have been already proposed, prior to the\nLearned versions [7].\nProblem Statement, Performance of a\nBloom Filter and a Learned Version. Bloom\nFilters (BF) solve the Approximate Set Membership\nproblem, deﬁned as follows: having ﬁxed a universeU\nand a set of keysS⊂U, for any given x∈U, ﬁnd out\nwhetheror not x∈S.False negatives , thatis negative\nanswers when x∈S, are not allowed. On the other\nhand,false positives (i.e., elements in U\\Swrongly\ndecreed as keys) are allowed, albeit their fraction\n(termed henceforth false positive rate , FPR for short)\nshould be bounded by a given /epsilon1. The key parame-\nters of any data structure solving the approximate\nset membership problem are: (i) the FPR /epsilon1; (ii) the\ntotal space needed by the data structure; and (iii) the\nreject time , deﬁned as the expected time needed to\nreject a non-member of S.\nKraska et al. [ 31] have proposed a Learned version\nof Bloom Filters (LBF) in which a suitably trained\nbinary classiﬁer is introduced with the aim of reducing\nspace occupancy w.r.t. a classical BF, having ﬁxed the\nFPR. Such classiﬁer is initially queried to predict the\nset membership, with a fallback to a standard BF in\norder to avoid false negatives. Mitzenmacher [ 39] has\nprovided a model for those ﬁlters, together with a very\ninformative mathematical analysis of their pros/cons,\nresulting in new models for LBFs, and additional mod-\nels have been introduced recently [ 15,47]. It is worth\npointing out that all the mentioned Learned Bloom\nFilters are static, i.e., no updates are allowed. This is\nthe realm we are studying here, aiming at a suitable\njoint optimization of the aforementioned key resources.\nAs for the dynamic case, some progress is reported in\n[32] in the Data Stream model of computation.\nThe Central Role of Classiﬁer Selection. It\nis worth pointing out that, although they diﬀer in\narchitecture, each of these proposals has a binary\nclassiﬁer at its core. Somehow, not much attention\nhas been devoted to the choice of the classiﬁer to\nbe used in practical settings, despite its centrality in\nthis new family of ﬁlters and its role in the relatedtheoretical analysis [ 39]. Kraska et al. use a Recurrent\nNeural Network, while Dai and Shrivastava [ 15] and\nVaidya et al. [ 47] use Random Forests. These choices\nare only informally motivated, giving no evidence\nof superiority with respect to other possible ones,\ne.g., via a comparative analysis. Therefore, apart\nfrom an initial study presented in [ 23], the important\nproblem of suitably choosing the classiﬁer to be used\nto build a speciﬁc LBF has not been fully addressed\nso far. In addition to that, although the entire area\nof Learned Data Structures and Algorithms ﬁnds its\nmethodological motivation as a conceptual tool to\nreconsider classic approaches in a data-driven way, the\nrole that the complexity of a dataset plays in guiding\nthe practical choice of a Learned Data Structure for\nthat dataset has been considered to some extent for\nLearned Indexes only [ 35]. This aspect is even more\nrelevant for LBFs. Indeed, as well discussed in [ 39],\nwhile the performance of classic BFs is “agnostic”\nw.t.r. the statistical properties of the input data, LBFs\nare quite dependent on them. In addition, it is well\nknown that the performance of a learnt classiﬁer (a\ncentral component in this context) is very sensitive\nto the “classiﬁcation complexity” of a dataset [ 1,9,\n21,34]. Such a State of the Art is problematic, both\nmethodologically and practically, for LBFs to be a\nreliable competitor of their classic counterparts.\nGoals:Our aim is to provide a methodology and the\nassociated software to support the design, analysis\nand deployment of Learned Boom Filters with respect\nto speciﬁc constraints regarding their multi-criteria\nnature. Namely, space eﬃciency, false positive rate,\nand reject time.\nContributions: In order to achieve these goals, our\ncontributions are the following.\n(1)We revisit Bloom Filters , both in their origi-\nnal and learned versions (Section 2), detailing the\nhyperparameters to be tuned within the related\ntraining procedures.\n(2)We propose a methodology , which can guide\nboth designers and users of LBFs in their design\nchoices (Section 3), to study the interplay among:\n(a)theparametersindicatinghowaﬁlter, learned\nor classic, performs on an input dataset;\n2\n\n(b) the classiﬁer used to build the LBF;\n(c) the classiﬁcation complexity of the dataset.\n(3)Software platform and ﬁndings : we provide\na software platform implementing the above-\nmentioned methodology and we show the validity\nand usefulness of our approach, as detailed next.\n(a)We address the problem of choosing the most\nappropriate classiﬁer in order to design a\nLBF having as only prior knowledge the to-\ntal space budget, the data complexity and\nthe list of available classiﬁers, including as\nselection criterion also the reject time. A\nrelated problem has been considered in [ 39]\nwith two important diﬀerences: the ﬁlter is\nﬁxed, and the obtained results supply only\npartial answers, leading to the suggestion of\nan experimental methodology, which has not\nbeen validated and it is not supported by\nsoftware. Our analysis shows the following.\n•Among the many classiﬁers used in this\nresearch, onlytwoclassiﬁersareworthof\nattention. Remarkably, none of the two\nhas been considered before for Learned\nBloom Filters (Section 5).\n(b)As a further contribution, we assess how\nthe performance of State-of-the-Art BFs is\naﬀected by datasets of increasing complexity\n(Section 5).\n•We identify a version of Learned Bloom\nFilter more robust than existing propos-\nals to variations of data complexity and\nclassiﬁer performance.\nIn conclusion, our experiments show that the pro-\nposed methodology is valid for the design of Learned\nBloom Filters and that the associated software is quite\nuseful. Indeed, in addition to the novel ﬁndings men-\ntioned above, we can also provide recommendations\non how to use State-of-the-Art solutions (Section 6).2 Bloom Filters and Learned\nBloom ﬁlters\n2.1 Bloom Filters\nA Bloom Filter [ 3] is a data structure solving the\nApproximate Set Membership problem deﬁned in the\nIntroduction, based on a boolean array vofmen-\ntries and on khash functions h1,...,hkmappingU\nto{1,...,m}. These functions are usually assumed\nto bek-wise independent [10,49], although much less\ndemanding schemes work well in practice [ 3]. A BF\nis built by initializing all the entries of vto zero,\nsubsequently considering all keys x∈Sand setting\nvhj(x)←1for eachj∈{1,...k}; a location can be\nset to 1several times, but only the ﬁrst change has\nan eﬀect. Once the ﬁlter has been built, any x∈U\nis tested against membership in Sby evaluating the\nentryvhj(x), for each hash function hj:xis classi-\nﬁed as a key if all tested entries are equal to 1, and\nrejected (a shorthand for saying that it is classiﬁed\nas a non-key) otherwise. False positives might arise\nbecause of hash collisions, and the corresponding rate\n/epsilon1is inversely bound to the array size m. More pre-\ncisely, equation (21) in [ 3] connects reject time, space\noccupancy and FPR, so that one can choose the con-\nﬁguration of the ﬁlter: for instance, given the available\nspace, one can derive the reject time that minimizes\nthe FPR. Analogous trade-oﬀs [ 7,39] can be used to\ntune the hyperparameters of a BF (namely, mand\nk) in order to drive the inference process towards the\nmost space-conscious solution. In particular, ﬁxed an\nFPR/epsilon1and a number n=|S|of keys, a BF ensuring\noptimal reject time requires an array of\nm= 1.44nlog(1//epsilon1)bits. (1)\n2.2 Learned Bloom Filters\nA Learned Bloom Filter [ 31] is a data structure simu-\nlating a BF to reduce its resource demand or its FPR\nby leveraging a classiﬁer. The main components of\na LBF are a classiﬁer C:U→[0,1]and a BFF,\ndeﬁned as follows.\n1.Using supervised machine learning techniques, C\nisinducedfromalabeleddataset Dwhosegeneric\n3\n\nitem is (x,yx), whereyxequals 1ifx∈Sand0\notherwise. In other words, Cis trained to classify\nkeys inSso that the higher is C(x), the more\nlikelyx∈S. A binary prediction is ensured\nby thresholding using τ∈[0,1], i.e. classifying\nx∈Uas a key if and only if C(x)>τ.\n2.Of course, nothing prevents us from having a set\nof false negatives {x∈S|C(x)≤τ}/negationslash=∅, thus\nabackup(classical) Bloom Filter Ffor this set\nis built. Summing up, x∈Uis predicted to be\na key ifC(x)>τ, orC(x)≤τandFdoes not\nrejectx. In all other cases, xis rejected.\nIt is important to underline that the FPR of a clas-\nsical BF is essentially independent of the distribution\nof data used to query it. This is no more true for a\nLBF [39], in which such rate should be estimated from\na query set S⊂U\\S. To remark such diﬀerence, one\ncommonly refers to the empirical FPR of a learned\nﬁlter, which is computed as /epsilon1=/epsilon1τ+(1−/epsilon1τ)/epsilon1F, where:\n1./epsilon1τ=|{x∈S|C(x)> τ}|/|S|is the analogous\nempirical FPR of the classiﬁer ConS, and\n2./epsilon1Fis the false positive rate of the backup BF.\nHence, having ﬁxed a target value for /epsilon1, the backup\nﬁlter can be built setting /epsilon1F= (/epsilon1−/epsilon1τ)/(1−/epsilon1τ)(under\nthe obvious constraint /epsilon1τ< /epsilon1). Within the learned\nsetting, the three key factors of the ﬁlter are con-\nnected (and inﬂuenced) by the choice of τ. Moreover,\nwhile data independence allows us to reliably esti-\nmate the FPR of a BF, this is no longer immediate\nfor LBFs, as pointed out in [ 39], along with an exper-\nimental methodology to assess it, which is part of the\nevaluation setting proposed in this paper.\nHere below we outline the main features of the\nLBF variants which we have considered. With the\nexception of the one in [ 47], for which the software\nis neither public nor available from the authors, our\nselection is State of the Art.\nSandwiched LBFs [39]. The Sandwiched variant\nof LBFs (SLBF for brevity) is based on the idea that\nspace eﬃciency can be optimized by ﬁltering out non-\nkeysbeforequerying the classiﬁer C, requiring asconsequence a smaller backup ﬁlter F. More in detail,\na BFIforSis initially built and used as a ﬁrst\nprocessing step. All the elements of Sthat are not\nrejectedbyIarethenusedtobuildaLBFasdescribed\nearlier. The SLBF immediately rejects an element\nx∈UifIrejects it, otherwise the answer for xof the\nsubsequent LBF is returned. The empirical FPR of\nthe SLBF is /epsilon1=/epsilon1I/parenleftbig\n/epsilon1τ+ (1−/epsilon1τ)/epsilon1F/parenrightbig\n, where/epsilon1Iis the\nFPR ofI. Here, ﬁxed the desired /epsilon1, the corresponding\nFPR to consctruct Iis/epsilon1I= (/epsilon1//epsilon1τ)(1−FN/n), where\nFNis the number of false negatives of C. Also in this\ncase, the classiﬁer accuracy aﬀects the FPR, space\nand reject time, with the constraint /epsilon1(1−FN/n)≤\n/epsilon1τ≤1−FN/n.\nAdaptive LBFs [15]. Adaptive LBFs (ADA-BF)\nrepresent an extension of LBF, partioning the training\ninstancesxinto intoggroups, according to their\nclassiﬁcation score C(x). Then, the same number of\nhash functions the backup ﬁlter of an LBF would use\nare partitioned across groups, and the membership for\nthe instances belonging to a given group is tested only\nusing the hash functions assigned to it. As for the\nLBFs mentioned earlier, the expected FPR can only\nbe estimated empirically. However, in this case the\nformula is rather complicated: the interested reader\ncan refer to [ 15]. The best performing variant of\nADA-BF has been retained in this study.\nHyperparameters. The Learned Bloom Filters de-\nscribed above have some parameters to be tuned.\nNamely, the threshold τfor LBF and SLBF, and two\nparameters gand ¯cfor ADA-BF, representing the\nnumber of groups in which the classiﬁer score inter-\nval is divided into, and the proportion of non-keys\nscores falling in two consecutive groups. The details\non the tuning of these hyperparameters are discussed\nin Section 4.3.\n3 Experimental Methodology\nIn this section we present the methodology which\nwe adopt in order to design and analyse Learned\nBloom Filters with regard to the inherent complexity\nof input data to be classiﬁed, subsumed as follows.\n4\n\nThe starting point is a dataset, either real-world or\ngenerated through a procedure suitable for synthesize\ndata in function of some classiﬁcation complexity\nmetrics.\nOverall, the pipeline adopted is as it follows: col-\nlect/generatedata; induceaclassiﬁerfromdataandes-\ntimate its empirical FPR; construct a Learned Bloom\nFilter exploiting the learnt classiﬁer, and in turn esti-\nmate its empirical FPR. The following sections review\nthe considered classiﬁer families and describe in depth\nthe adopted data generation procedure.\n3.1 A Representative Set of Binary\nClassiﬁers\nStarting from an initial list of classiﬁers—without\npresuming to be exhaustive—we performed a set of\npreliminary experiments, from which we received in-\ndications about the classiﬁers’ families to be further\nanalyzed, based on their trade-oﬀ between perfor-\nmance and space requirements1. Namely, from the\ninitial list we have removed the following classiﬁers:\nLogistic Regression [ 13], Naive Bayes [ 16] and Recur-\nrent Neural Networks [ 11], due to their poor trade-oﬀ\nperformance, conﬁrming the results of a preliminary\nstudy [23]. The remaining ones are brieﬂy described\nin the following paragraphs. Since our evaluation\nconsiders both balanced and unbalanced classiﬁcation\nproblems, we also detail how their inference is man-\naged in an unbalanced context. The hyperparameters\nof the corresponding learning algorithms are distin-\nguished between regularandkeyhyperparameters,\nthe latter aﬀecting the space occupancy of the classi-\nﬁer. The model selection phase only involves non-key\nhyperparameters, while diﬀerent conﬁgurations for\nkey hyperparameters are analysed in dedicated exper-\niments aiming at studying the interplay among FPR,\nspace occupancy and reject time of Learned Bloom\nFilters. Hereafter, when not diﬀerently speciﬁed, the\nspace occupancy serves as a proxy for the complexity\nof a classiﬁer.\nIn order to ﬁx notation, we assume U=Rqas\nuniverse, and we refer to D⊂Uas a set ofdlabeled\n1The experiments and data about this preliminary part are\navailable upon request.instances, denoting by x∈Da training instance\nand byyx∈{0,1}its label (with a slightly diﬀerent\nnotation for SVMs, as explained here below).\nLinear SVM Classiﬁcation in Linear Support Vec-\ntor Machines (SVMs) for a given instance xis usually\nbased on the optimal hyperplane ( w,b) and the sign\noff(x) =w·x+b. To fall in the setting deﬁned for a\nLBF, we need to transform the SVM prediction into\na score in [0,1]. To this end, we use f(x)as argument\nto a sigmoid function. The optimal hyperplane ( w,b)\nis learned via maximization of the margin between\nthe two classes, by solving\nminw,b1\n2/bardblw/bardbl2+c/summationtext\nx∈Dξx,\nsuch that yxf(x)≥1−ξx∀x∈D ,\nξx≥0∀x∈D ,\nwheremisclassiﬁcationisallowedbytheslackvariables\nξxand penalized using the hyperparameter c > 0\n(note that in this case yx∈{− 1,1}). Nonlinear SVMs\nmight have been used here, but the need of storing\nthe kernel matrix, e.g., a Gaussian kernel, alongside\nthe hyperplane parameters results in an unacceptable\nsize for the learned ﬁlters. For the linear case we have\nchosen, we have only one non-key hyperparameter,\nnamelyc. When dealing with unbalanced labels, we\nconsider the cost-senstive SVM version described in\n[41].\nFeed-Forward NNs We also consider Feed-\nForward neural networks [ 24,51] accepting instances\nas inputs and equipped with lhidden layers, respec-\ntively having h1,...,hlhidden units (NN- h1,...,hl\nfor short). One output unit completes the network\ntopology. As usual, training involves the estimation\nof unit connections and biases from data. In this case,\nthehis are key hyperparameters, while the learning\nratelracts as a regular hyperparameter to be tuned.\nIt is worth noting that we ﬁx the activation functions\nfor all network units (although the former are tunable\nin principle), to limit the size of the already massive\nset of experiments. Precisely, as typically done, we\nuse ReLU and Sigmoid activations for hidden and\noutput units, respectively. Where appropriate, la-\nbel imbalance is dealt using a cost-sensitive model\nvariant [8].\n5\n\nRandom Forests Finally, we consider also Ran-\ndom Forests [ 5], shortened as RF- tto make explicit\nthat this model is an ensemble of tclassiﬁcation\ntrees [6]. Each such tree is trained on a diﬀerent\nbootstrap subset of Drandomly extracted with re-\nplacement. Analogously, the splitting functions at\nthe tree nodes are chosen from a random subset of\nthe available attributes. The RF aggregates classi-\nﬁcations uniformly across trees, computing for each\ninstance the fraction of trees that output a positive\nclassiﬁcation. To address the case where labels are\nunbalanced, we adopt an imbalance-aware variant of\nRFs [26,48] in which, during the growth of each tree,\nthe bootstrap sample is not drawn uniformly over D,\nbut by selecting an instance xwith probability\npx=/braceleftBigg1\n2|D+|ifyx= 1,\n1\n2|D−|ifyx= 0,\nwhereD+={x∈D|yx= 1}, andD−=D\\D+. In\nthis way, the probabilities of extracting a positive or\na negative example are both 1/2, and the trees are\ntrained on balanced subsets. The key hyperparameter\nof a RF ist, directly impacting on the classiﬁer size.\nThe non-key hyperparameter which we consider here\nis the minimum number δof samples in a leaf, which\nallows to control the depth of the individual trees.\nIt is worth observing that this hyperparameter can\nalso slightly aﬀect the tree size, but in our setting\nthe adopted grid of values only yields a negligible\ndiﬀerence in size (see Section 4.3).\n3.2 Measures of Classiﬁcation Com-\nplexity and a Related Data Gen-\neration Procedure\nIn order to measure the complexity of a dataset, sev-\neral measures are available, e.g. see [ 33] for a survey.\nWe speciﬁcally focus on measures suitable for binary\nclassiﬁcation tasks, and hereafter we use the nota-\ntion “class i”,i= 1,2, to refer to one of the two\nclasses. A preliminary analysis highlighted that some\nof the measures in [ 33] were insensitive across a vari-\nety of synthetic data, as happened, e.g., with the F1,\nT2, orT3measures, or needed an excessive amount\nof RAM (such as network- or neighborhood-basedmeasures, like LSCandN1). As a consequence, we\nselected the feature-based measure F1vand theclass-\nimbalance measure C2. The former quantity, also\ncalled the Directional-vector Maximum Fisher’s Dis-\ncriminant Ratio, is deﬁned as follows: denote, respec-\ntively, bypi,µi, and Σithe proportion of examples\nbelonging to class iand the corresponding centroid\nand scatter matrix, so that W=p1Σ1+p2Σ2and\nB= (µ1−µ2)(µ1−µ2)/latticetopare the between- and within-\nclass scatter matrices. In this case, d=W−1(µ1−µ2)\ncorresponds to the direction onto which there is max-\nimal separation of the two classes (being W−1the\npseudo-inverse of W), and we can deﬁne the F1v\nmeasure as\nF1v=/parenleftBigg\n1 +d/latticetopWd\nd/latticetopBd/parenrightBigg−1\n. (2)\nThe second measure accounts for label balance in the\ndataset: letting nibe the number of examples of class\ni, we haveC2 = (n1−n2)2/(n2\n1+n2\n2). Both measures\nvary in [0,1]: the higher the value, the more complex\nthe data.\nData generation procedure. We generate syn-\nthetic data considering three parameters, a,randρ,\nwhich allow to tune the complexity of generated data\naccording to the aforementioned measures. Intuitively,\nacontrols the linearity of the separation boundary,\nrthe label noise, and ρthe label imbalance. More\nprecisely, in order to generate a binary classiﬁcation\ndataset with a given level of complexity, n1positive\nandn2=⌈ρn1⌉negativeinstances(with N=n1+n2),\nwe proceed as follows. Let {x1,...xN}⊂Rqbe the\nset of samples, with each sample xihavingqfeatures\nxi1,...,xiq, and a binary label yi∈{0,1}. TheN\nsamples are drawn from a multivariate normal distri-\nbutionN(0,Σ), with Σ=γIq(withγ >0andIq\ndenotingthe q×qidentitymatrix). Inourexperiments\nwe setγ= 5so as to have enough data spread, remind-\ning that this value however does not aﬀect the data\ncomplexity. Without loss of generality, we consider\nthe caseq= 2. To determine the classes of positive\nand negative samples, the parabola x2−ax2\n1= 0is\nconsidered, with a > 0: a pointxi= (xi1,xi2)is\npositive (yi= 1) ifxi2−ax2\ni1>0, negative otherwise\n6\n\n(yi= 0). This choice allows us to control the linear\nseparability of positive and negative classes by varying\nthe parameter a: the closer ato0, the more linear the\nseparation boundary. As consequence, acontrols the\nproblem complexity for a linear classiﬁer. An example\nof generated data by varying ais given in Figure 1\n(a–c). Further, to vary the data complexity even for\nnon linear classiﬁers, labels are perturbed with dif-\nferent levels of noise: we ﬂip the label of a fraction\nrof positive samples, selected uniformly at random,\nwith an equal number on randomly selected negatives.\nThe eﬀect of three diﬀerent levels of noise is depicted\nin Figure 1 (d–f), where the higher the noise, the less\nsharp the separation boundary. The third parameter\nρis the ratio between the number of negative and\npositive samples in the dataset, thus it controls the\nC2 complexity measure. Higher values of ρmake the\nnegative boundary more clear (Figure 1 (g–i)), while\nmaking harder training an eﬀective classiﬁer [25].\n4 Experiments\n4.1 Data\nDomain-Speciﬁc Data. We use a URL dataset\nand a DNA dictionary. The ﬁrst has been used by [ 15],\nwho also kindly provided us with the dataset, as part\nof their experimentation on Learned Bloom Filters.\nThe second dataset comes from experiments in Bioin-\nformatics regarding the storage and retrieval of k-mers\n(i.e., strings of length kappearing in a given genome,\nwhose spectrum is the dictionary of k-mers) [43] and\nwas directly generated by us. We point out that no\nsensible information is contained in these datasets.\nWith reference to Table 1, they represent two extreme\ncases of classiﬁcation complexity: the URL dataset\niseasy, as it is simple in terms of linear separabil-\nity (F1v), albeit exhibiting a relevant C2 complexity\ndue to the label imbalance; the DNA data is hard, in\nthat it has almost the maximum F1v possible value,\nmeaning that positive and negative classes are indis-\ntinguishable by a linear classiﬁer.\nThe URL dataset contains 485730 unique URLs,\n80002malicious and the remaining benign. Seventeen\nlexical features are associated with each URL, whichare used as the classiﬁcation features. It is worth\npointing out that all of the previous works on Learned\nBloom Filters have used URL data. In this context,\na Bloom ﬁlter can be used as a time- and space-\neﬃcient data structure to quickly reject benign URLs,\nnever erroneously trusting a malicious URL although\noccasionally misclassifying a benign one. We adhere\nto this standard here.\nThe DNA dataset refers to the human chromosome\n14, containing n= 49906253 14-mers [ 43] constituting\nthe set of our keys. As non-keys, we uniformly gener-\nate othern14-mers from the 414possible strings on\nthe alphabet{A,T,C,G}. Each 14-mer is associated\nwith a 14-dimensional feature vector, whose compo-\nnents are the integers 0,1,2,3, each associated with\none of the four DNA nucleobases A, T, C, G, respec-\ntively (for instance a 14-mer TAATTACGAATGGT\nis coded as (1,0,0,1,1,0,2,3,0,0,1,3,3,1)). A funda-\nmental problem in Bionformatics, both technological\n[46] and in terms of evolutionary studies [ 12], is to\nquickly establish whether a given k-mer belongs to\nthe spectrum of a genome. In this case, the Bloom\nFilter stores the dictionary. It is worth mentioning\nthat the use of Bloom Filters in Bioinformatics is one\nof their latest ﬁelds of application, with expected high\nimpact [18]. Such a domain has not been considered\nfor the evaluation of Learned Bloom Filters, as we do\nhere.\nSynthetic Data. We generate two categories of\nsynthetic data, each attempting to reproduce the com-\nplexity of one of the domain-speciﬁc data. The ﬁrst\ncategory has nearly the same C2 complexity of the\nURL dataset, i.e., it is unbalanced , withn1= 105\nandρ= 5. The second one has the same C2 com-\nplexity of the DNA dataset, i.e., it is balanced, with\nn1= 105andρ= 1. The choice of n1allows to have\na number of keys similar to that in the URL data,\nand at the same time to reduce the computational\nburden of the massive set of experiments planned.\nIndeed, both balanced and unbalanced categories con-\ntain nine datasets, exhibiting increasing levels of F1v\ncomplexity. Speciﬁcally, all possible combinations of\nparameters a∈{0.01,0.1,1}andr∈{0,0.1,0.25}\nare used. The corresponding complexity estimation\n7\n\n(a)\n (b)\n (c)\n(d)\n (e)\n (f)\n(g)\n (h)\n (i)\nFigure 1: Graphical representation of synthetic data: ﬁrst row, parameter conﬁguration is np= 500,r= 0,\nρ= 1anda= 0.01(a),a= 0.1(b), anda= 1(c); second row np= 500,a= 0.1,ρ= 1andr= 0(d),\nr= 0.1(e), andr= 0.25(f); third row, np= 100,a= 0.1,r= 0,ρ= 1(g),ρ= 3(h), andρ= 5(i). \"pos\"\nand \"neg\" entries in the legend stand for positive and negative class, respectively.\n8\n\nTable 1: Complexity of the real data.\nData F1v C2\nURL 0.08172 0.62040\nDNA 0.99972 0\nTable 2: Complexity of the synthetic data.\nBalanced Unbalanced\na r F1v C2 F1v C2\n0.01 0 0.127 0.0 0.129 0.615\n0.1 0 0.181 0.0 0.202 0.615\n1 0 0.306 0.0 0.360 0.615\n0.01 0.10.268 0.0 0.187 0.615\n0.1 0.10.327 0.0 0.269 0.615\n1 0.10.459 0.0 0.433 0.615\n0.01 0.250.571 0.0 0.308 0.615\n0.1 0.250.619 0.0 0.399 0.615\n1 0.250.718 0.0 0.563 0.615\nis shown in Table 2. Consistently, F1v complexity\nincreases with aandrvalues, in both balanced and\nunbalanced settings. Noteworthy, the label imbalance\nslightly aﬀects also the measure F1v: in absence of\nlabel noise ( r= 0), F1v augments, likely due to the\nfact that F1v is an imbalance-unaware measure; on\nthe contrary, in presence of noise F1v complexity is\nbarely reduced w.r.t. the balanced case. Although\nnot immediate, the sense of this behavior might reside\nin what we also observe in Figure 1(g–i). That is,\nthe boundary of negative class tends to be more crisp\nwhenρincreases, thus mitigating the opposite eﬀect\nthe noise has on the boundary (Figure 1 (d–f)).\n4.2 Hardware and Software\nWeusetwoUbuntumachines: anIntelCorei7-10510U\nCPU at 1.80GHz ×8 with 16GB RAM, and an Intel\nXeon Bronze 3106 CPU at 1.70GHz ×16 with 192GB\nRAM. This latter is used for experiments that require\na large amount of RAM, i.e., on the DNA dataset.\nThe supporting software [ 44] is written in Python\n3.8, leveraging the ADA-BF public implementation\nprovided in [ 14], which we extend as follows: 1. the\nconstruction of Learned Bloom Filters can be donein terms of the classiﬁers listed in Section 3.1 and\nof the datasets illustrated in Section 4.1; 2. SLBF\nis added to the already included BF models; 3. the\nchoice of the classiﬁer threshold τis performed consid-\nering any number of evenly spaced percentiles of the\nobtained classiﬁcation scores, instead than checking\nﬁxed values; 4. ranges for the hyperparameters of the\nlearned versions of BFs can be speciﬁed by the user;\n5. a main script allows to perform all experiments,\nrather than invoking several scripts, each dedicated\nto a LBF variant.\nThe provided implementation is built on top of\nstandard libraries, listed in a dedicated environment\nﬁle in order to foster replicability. In particular, the\nspace required by a given classiﬁer is computed, as\ntypically done in these cases, using the Picklemodule\nand accounting for both structure information and\nparameters [ 42], in order to obtain a fair comparison\namong all tested conﬁgurations. Moreover, the soft-\nware is opened to extensions concerning the inclusion\nof new datasets and/or new LBF models, thus it can\nbe used as a starting point for further independent\nresearches.\n4.3 Model Selection\nClassiﬁers. The classiﬁer generalization ability is\nﬁrst assessed independently of the ﬁlter employing it,\nvia a 3-fold cross validation (CV) procedure (outer).\nClassiﬁer performance is measured in terms of (i) the\narea under the ROC curve (AUC), and of (ii) the\narea uder the precision-recall curve (AUPRC), aver-\naged across folds. We tune non-key hyperparameters\nof each model via a nested 3-fold (CV), where in\neach round of the outer CV we select the non-key hy-\nperparameters through a grid search using the inner\nCV on the current training set, and the best con-\nﬁguration is retained. We use the following grids:\nc∈{10−1,1,10,102,103}(SVM);δ∈{1,3,5}(RF);\nlr∈{10−4,10−3}(NN). The diﬀerent size of the grid\nacross classiﬁers is due to the diﬀerent training time\nof the classiﬁer (SVM is the fastest one). The conﬁgu-\nration of a classiﬁer is strictly dependent on the space\nbudget assigned to the LBF leveraging that classiﬁer\n(see Table 5 discussed in next section); consequently,\nthe key hyperparameters for a given classiﬁer, i.e.,\n9\n\nhyperparameters inﬂuencing the space occupancy, are\nset based on the following strategy: to detect the\nsubspace of “valid” conﬁgurations, we carried out sev-\neral preliminary experiments (not shown here) for\neach classiﬁer, subject to the available space budget,\nand also evaluating their performance/space trade-oﬀ.\nThe results described here below leverage this prelim-\ninary ﬁlter. Recalling that no key hyperparameters\nexist for SVMs, we consider RFs related to two values\noft, leading to a simpler and a more complex model.\nThe simpler choice is t= 10, as a reference already\nused in the literature [ 15], whereas we consider t= 20\nandt= 100as more complex variants, used respec-\ntivelyforthesynthetic/URLandfortheDNAdatasets\n(cfr. Section 4.1). This distinction is due to diﬀerent\nkey set size, in turn inﬂuencing the available budget,\nand the choice t= 100needs more space than the\nbudget available for URL and synthetic data. For a\nfair comparison, the key hyperparameters for NNs are\nselected so as to yield three models nearly having the\nsame occupancy of the SVM and of the two RFs mod-\nels. The above-mentioned preliminary experiments\nhave suggested, where enough space budget was avail-\nable, that a two-layered topology is to be preferred\nto the one-layered one. Precisely, we consider the\nfollowing models: NN- 25, NN- 150,50and NN- 200,75\n(synthetic dataset); NN- 7, NN- 150,35and NN- 175,70\n(URL dataset); NN- 7, NN- 125,50, NN- 500,150(DNA\ndataset). The ﬁnal classiﬁer conﬁguration for all ex-\nperiments and their space requirements are detailed\nin Table 3. For the ﬁnal discussion, in Table 4 we\nalso include the average prediction time of the tested\nclassiﬁers.\nLearned Bloom Filters. The Bloom Filter vari-\nants under study are evaluated under the setting pro-\nposed by [ 15], that is: 1. train the classiﬁers on all\nkeys and 30%of non-keys, and query the ﬁlter using\nremaining 70%of non-keys to compute the empirical\nFPR; 2. ﬁx an overall memory budget of mbits for\neach ﬁlter, and compare them in terms of their empir-\nical FPR/epsilon1. In addition, in this work we also measure\ntheaverage reject time of ﬁlters, since it is can unveil\ninteresting trends about the synergy of the ﬁlter vari-\nants and the classiﬁer they employ. Each ﬁlter variantis trained leveraging in turn each of the considered\nclassiﬁers. The budget mis related to the desired\n(expected) /epsilon1of a classical Bloom Filter, according\nto (1). Being the space budget directly inﬂuenced by\nthe key set size n, we adopt a setting tailored on each\ndataset. Concerning synthetic data, as we generate\nnumerous datasets, for each of them we only test two\ndiﬀerent choices for the space budget m. Namely,\nthose yielding /epsilon1∈{0.05,0.01}for the classical Bloom\nFilter using a bit vector of mbits according to (1).\nOn real datasets we test ﬁve budgets corresponding to\n/epsilon1∈{0.01,0.005,0.001,0.0005,0.0001}. The diﬀerence\nbetween this setting and that of synthetic data is due\nto the following considerations. First, the dimension-\nality of synthetic data is 2, whereas that of real data\nit is17and14, respectively, for URL and DNA. This\nmakes the classiﬁers using real data larger than their\ncounterparts on synthetic data. For this reason, on\nreal data we omit the case /epsilon1= 0.05, which yielded\na too small budget. Indeed, some classiﬁers alone\nexceed the budget in this case (cfr. Table 3 for details\nabout the space occupancy of classiﬁers). Moreover,\nhaving only two datasets, we can test more choices\nof/epsilon1, and accordingly better evaluate the behavior\nof learned Bloom Filters when a smaller (expected)\nfalse positive rate is required. Table 5 contains the\nresulting budget conﬁgurations for all the considered\ndatasets. To build the learned Bloom Filters vari-\nants, the hyperparameters have been selected via grid\nsearch on the training data, optimizing with respect to\nthe FPR, according to the following setting: (a) 15dif-\nferent values for threshold τ, and (b) the ranges [3,15],\nand[1,5]for hyperparameters gand¯c, respectively\n(cfr. Section 2.2). Importantly, the latter choice in-\ncludes and extends the conﬁgurations adopted in [ 14]\n(namely, [8,12]forgand[1.6,2.5]for¯c).\n5 Results and Discussion\nIn this section we present the results obtained from\nthe classiﬁer screening on both synthetic and real data,\nthe experimental evaluation of all variants of LBFs\nbased on those classiﬁers, and the relative discussion.\n10\n\nTable 3: Space occupancy in Kbits of selected classiﬁers on the considered datasets.\nSynthetic Data\nSVM RF-10 RF-20 NN-25 NN-150,50 NN-200,75\n5 259.3 508.6 5.1 260.2 506.6\nURL Data\nSVM RF-10 RF-20 NN-7 NN-150,35 NN-175,70\n5.9 259.3 508.7 6.2 259.2 499.9\nDNA Data\nSVM RF-10 RF-100 NN-7 NN-125,50 NN-500,150\n5.8 259.5 2504 5.6 265.8 2652.3\nTable 4: Average classiﬁer inference time (across samples) in seconds.\nSynthetic Data\nSVM RF-10 RF-20 NN-25 NN-150,50 NN-200,75\n1.278·10−84.425·10−78.968·10−78.494·10−69.257·10−61.008·10−5\nURL Data\nSVM RF-10 RF-20 NN-7 NN-150,35 NN-175,70\n3.730·10−85.815·10−79.930·10−76.825·10−67.018·10−67.198·10−6\nDNA Data\nSVM RF-10 RF-100 NN-7 NN-125,50 NN-500,150\n2.87·10−85.572·10−75.364·10−66.572·10−68.138·10−61.044·10−5\nTable 5: Space budget in bits adopted on the various datasets. /epsilon1is the false positive rate, nis the number of\nkeys in the dataset.\nData/epsilon1 Budget (Kbits) n\nSynthetic 0.05,0.01 622, 956 105\nURL 0.01,0.005,0.001,0.0005,0.0001 765 ,880,1148,1263,1530 8 ·104\nDNA 0.01,0.005,0.001,0.0005,0.0001 477460 ,549325,716191,788056,954921 4.99·107\n11\n\nFigure 2: Performance averaged across folds of compared classiﬁers on synthetic data. First row for balanced\ndata, second row for unbalanced data. Bars are grouped by dataset, in turn denoted by a couple ( a,r).\n12\n\n(a)\n (b)\nFigure 3: Performance averaged across folds of compared classiﬁers on real data: (a) URL; (b) DNA.\n5.1 Classiﬁers\nAs evident from Section 2.2, the classiﬁer can be in-\nterpreted as an oracle for a learned BF, where the\nbetter the oracle, the better the related ﬁlter, i.e., its\nFPR once ﬁxed the space budget. Accordingly, it is of\ninterest to evaluate the performance of classiﬁers. All\nclassiﬁers described in Section 3.1 have been tested on\nthe datasets described in Section 4.1, with the conﬁg-\nuration described in Section 4.3. Figure 2 depicts the\nperformance of classiﬁers on balanced andunbalanced\nsynthetic data, whereas results obtained on real data\nare shown in Figure 3. However, it is central here\nto emphasize that the interpretation of such results\nis somewhat diﬀerent than what one would do in a\nstandard machine learning setting. Indeed, we have\na space budget for the entire ﬁlter, and the classiﬁer\nmust discriminate well keys and non-keys, while being\nsubstantially succinct with regard to the space budget\nof the data structure. Such a scenario implicitly im-\nposes a performance/space trade-oﬀ: hypothetically,\nwe might have a perfect classiﬁer using less space than\nthe budget, and on the other extreme, a poor classiﬁer\nexceeding the space budget.5.1.1 Overall Results Analysis\nFirst, the behaviour of classiﬁers in terms of AUC\nand AUPRC is coherent with what expected accord-\ning to our methodology to generate synthetic data.\nIndeed, the SVM performance decays when parameter\naincreases, being in line with the fact that it means\nincreasing the non-linearity of the class separation\nboundary. Analogously, all classiﬁers worsen as noise\nrincreases, which is clearly what to expect in this case.\nMoreover and most importantly, two main cases arise\nwith respect to classiﬁcation complexity: roughly F1v\n≤0.35and F1v>0.35. Being this threshold experi-\nmentally derived, the division between the two cases\nis not crisp. We refer to the ﬁrst case as datasets\n‘easier and easier’ to classify, for brevity ‘easy’, and to\nthe second as datasets ‘harder and harder’ to classify,\nfor brevity ‘hard’.\nEasy datasets. All classiﬁers perform very well on\nsynthetic datasets with the stated complexity (except\nfor SVMs when a>0.01). Clearly, with such excel-\nlent oracles, the remaining part of a learned Bloom\nFilter (e.g., with reference to the description of LBF,\nthe backup ﬁlter) is intuitively expected to be very\nsuccinct.\n13\n\nHard datasets. In this case, both AUC and\nAUPRC sensibly drop, being in some cases (SVM)\nnot so far from the behaviour of a random classiﬁer.\nWhile in the previous case the performance of clas-\nsiﬁers clearly yields the choice of the most succinct\nand faster model, here there is a trade-oﬀ to consider.\nIndeed, within the given space budget, at one end of\nthe spectrum, we have the choice of a small-space and\ninaccurate classiﬁer, at the other end of the spectrum\nwe have larger and more accurate ones. As an exam-\nple, for the LBF in the ﬁrst case a large backup ﬁlter\nis required, whereas in the second one the classiﬁer\nwould use most of the space budget.\n5.1.2 Preliminary observations on the classi-\nﬁers to be retained.\nHere we address the question of how to choose a\nclassiﬁer to build the ﬁlter upon, based only on the\nknowledge of space budget and data classiﬁcation\ncomplexity/classiﬁer performance. On synthetic and\nURL data (Figures 2 and 3 (a)), more complex clas-\nsiﬁers perform just slightly better than the simpler\nones, likely due to the low data complexity in these\ncases. At the same time, they require a sensibly\nhigher fraction of the space budget (Table 5), and\nit is thereby natural to retain in this cases only the\nsmallest/simplest variants, namely: RF- 10and NN- 25\n(synthetic)andNN- 7(URL),inadditiontoSVM.Con-\nversely, in DNA experiments more complex classiﬁers\nsubstantially outperform the simpler counterparts, co-\nherently with the fact that this classiﬁcation problem\nis much harder (Tables 1 and 2). Since the available\nspace budget is higher in this case, all classiﬁers have\nbeen retained in the subsequent ﬁlter evaluation.\n5.2 Learned Bloom Filters Evaluation\nThe aim of this section is: 1. to explore how the var-\nious learned ﬁlters behave with respect to the data\nclassiﬁcation complexity, an aspect so far ignored in\nthe literature (see Introduction); 2. to include the\nreject time in the overall ﬁlter assessment; 3. to gain\nfurther insights about the interplay between the diﬀer-\nent classiﬁers and the learned Bloom Filter variants.5.2.1 Learned Filters Performance and Their\nRelationship with Data Classiﬁcation\nComplexity.\nEasy datasets. Figures 4 and 5 report the FPR\nresultsoflearnedBloomFiltersonbalancedandunbal-\nanced synthetic data, respectively, whereas Figures 6\nand 7 depict the results on URL and DNA data. In\nall ﬁgures, also the baseline Bloom Filter is present.\nAccording to the deﬁnition provided in Section 5.1.1\n(F1v around 0.35or smaller), easy data can be associ-\nated to the three/four leftmost conﬁgurations on the\nx-axis in Figures 4 and 5 of synthetic and URL data.\nIn these cases, we observe results coherent with those\nobtained in the literature, where ADA-BF slightly\noutperforms the other competitors [ 15], and RF- 10\ninducing however lower FPR values with regard to\nthe classical BF. Notwithstanding, it clearly emerges\nthat such a classiﬁer is not the best choice, underlin-\ning all the doubts about a selection not motivated\nin the original studies. For instance, on URL data\nthere are at least two classiﬁers yielding a lower FPR\nin most cases and for all ﬁlter variants (SVM and\nNN-7). In additon, SVMs are much faster (Table 4).\nNN-7(or NN- 25for synthetic data) remains the best\nchoice even when the separation boundary becomes\nless linear ( a >0.01), and ﬁlters induced by SVMs\nbecome less eﬀective or even worse than the baseline\nBF.\nHard datasets. Our experiments show a novel sce-\nnario with the increase of data complexity, i.e., when\nmoving towards right on the horizontal axis in Fig-\nures 4 and 5, or when considering DNA data. We\nobserve that the performance of the ﬁlters drops more\nand more, in line with the performance decay of the\ncorresponding classiﬁers (Section 5.1), and unexpect-\nedly the drop is faster in ADA-BF (and LBF) w.r.t.\nSLBF. This happens for instance on all synthetic data\nhavingr>0(noise injection). We say unexpectedly\nsince we have an inversion of the trend also reported\nin the literature, where usually ADA-BF outperforms\nSLBF (which in turn improves LBF). Indeed SLBFs\nhere exhibit behaviours more robust to noise, which\nare likely due to a reduced dependency on the clas-\nsiﬁer for SLBF, yielded by the usage of the initial\n14\n\nFigure 4: False positive rates of learned ﬁlters attained on balanced synthetic datasets. On the horizontal axis,\nlabelsX_Ydenote the dataset obtained when using a=Xandr=Y. The blue dotted line corresponds to\nthe empirical false positive rate of the classical BF in that setting. Two space budgets mare tested, ensuring\nthat/epsilon1= 0.05(left) and/epsilon1= 0.01(right) for the classical BF .\nFigure 5: False positive rates of learned ﬁlters attained on unbalanced synthetic datasets. On the horizontal\naxis, the labels X_Ydenote the dataset obtained when using a=Xandr=Y. The blue dotted line\ncorresponds to the measured false positive rate of the classical Bloom ﬁlter in that setting. Two space budgets\nare tested: that ensuring /epsilon1= 0.05for a classical Bloom Filter (left), and that ensuring /epsilon1= 0.01(right).\n15\n\nBloom Filter. Such a ﬁlter indeed allows the classiﬁer\nto be queried only on a subset of data. Noteworthy is\nthe behavior of ﬁlters when using RFs in this setting:\ntheir FPR strongly increases, and potential explana-\ntions are the excessive score discretization (having 10\ntrees we have only 11distinct scores for all queries),\nand the space occupancy is larger (limiting the space\nto be assigned to initial/backup ﬁlters). These results\nﬁnd a particularly relevant conﬁrmation on the very\nhard, real-world, large, and novel DNA dataset (Fig-\nure 7). Here, surprisingly, the LBF cannot attain any\nimprovement with regard to the baseline BF, diﬀer-\nently from SLBF and ADA-BF. A potential cause can\nreside in the worse performance achieved by classiﬁers\non this hard dataset, compared to those obtained on\nsynthetic and URL data, and in a too marked depen-\ndency of LBF on the classiﬁer performance, mitigated\ninstead in the other two ﬁlter variants by the usage\nof the initial BF (SLBF) and by the ﬁne-grained clas-\nsiﬁer score partition (ADA-BF). SLBF outperforms\nboth LBF and baseline of one order of magnitude\nin FPR with the same space amount, and ADA-BF\nwhen using weaker classiﬁers and when a higher bud-\nget is available. This is likely due to overﬁtting of\nADA-BF in partitioning the classiﬁer codomain when\nthe classiﬁer performance is not excellent (or simi-\nlarly when the data complexity is high), as it happens\nfor DNA data. Diﬀerently from hard synthetic data,\nwhere the key set was smaller (and consequently also\nspace budget was smaller), here the classiﬁers lead-\ning to the best FPR values are the most complex,\nin particular NN-500,150 and NN-125,50 (which are\nalso the top performing ones, and might be further\ncompressed [ 38]). In other words, it means that on\nhard datasets, simple classiﬁers are useless or even\ndeleterious (SVM-induced ﬁlters never improve the\nbaseline, and in some cases they are even worse).\n5.2.2 Reject time.\nTable 6 provides the average per-element reject time\nof all learned ﬁlters, taken across all the query se-\nquences and space budgets that we have used in our\nexperiments. They are expressed as percentage in-\ncrement (or decrement) of the time required by the\nbaseline. A ﬁrst novel and interesting feature whichemerges is that learned BF are sometimes faster than\nthe baseline, which in principle is not expected, since\nlearned variants have to query a classiﬁer in addition\nto a classical BF. Our interpretation is that it can\nhappen for two main reasons: 1) the adopted classiﬁer\nis very fast and also eﬀective, hence allowing in most\ncases to skip querying the backup ﬁlter; 2) the key\nset is very large, thus requiring a large baseline BF,\nwhereas a good classiﬁer can allow to sensibly drop\nthe dimension of backup ﬁlters, thus making their\ninvocation much faster. See for instance the case of\nDNA data, where most learned ﬁlters are faster that\nthe baseline, with most classiﬁers.\nAnother intriguing behaviour concerns the reject\ntime of ADA-BF, often the worst architecture in terms\nof this metric. We believe it depends on the more\ncomplex procedure used in order to establish whether\nor not to access the backup ﬁlter. Indeed, such a\nprocedure is subject to tuning, which in turn can\nyield less or more complex instances of the ﬁlter. As\nevidentfromourexperiments, suchastrategydoesnot\nalways payoﬀ. We also observe sometimes a reversed\norder of the inference time of classiﬁers (Table 4)\nand the reject time of the corresponding ﬁlters. For\ninstance, SVM is always the fastest classiﬁer, but\nin some cases RF-based ﬁlters are faster (e.g., LBF\nand SLBF on Synthetic data). Even this behaviour\nis not so immediate to explain, and for sure it is\nrelated to the characteristics 1) and 2) mentioned\nabove. Indeed, RF is the second fastest classiﬁer, and\nit has an inference time just one order of magnitude\nslower that SVM, in average. On the other side,\nRF outperforms SVMs (even slightly), and this can\nreduce the number of queries to the backup ﬁlters\nand also their execution time (when the former is\nsmaller). Finally, it is worth pointing out that the\nclassiﬁers emerged as most eﬀective in both classiﬁer\nscreening and learned BF analysis, namely NNs, are\nat least two orders of magnitude slower than the other\nclassiﬁers, which must to be taken into account when\nconﬁguring a learned BF, as we emphasize in the\nfollowing discussion.\n16\n\n6 Guidelines\nWe summarize here our ﬁndings about the conﬁgura-\ntion of learned variants of Bloom Filters exploiting\nthe prior knowledge of data complexity and available\nspace budget.\nDataset Complexity and Classiﬁer Choice.\nWe have roughly distinguished two main categories\nof data based on their complexity and the related\nbehaviour of ﬁlters: easy dataset, having F1v≤0.35,\nand hard dataset, having F1v>0.35. The dataset\ncomplexity emerged as a central discriminant for the\nﬁlter setup. Indeed, when dealing with easy datasets,\nthe choice of the classiﬁer becomes quite easy, and,\nindependently of the space budget, it is always more\nconvenient to select simple classiﬁers. Speciﬁcally, our\nexperiments designate linear SVMs as best choice for\nthe easiest datasets (those having almost linear sepa-\nration boundary), and the smallest/simplest NNs for\nthe more complex data in this category. In addition,\nthe classiﬁer inference time plays an important role\nfor this data category: although a fastest classiﬁer\ndoes not necessarily implies a lower reject time of\nthe corresponding ﬁlter (see Section 5.2.2), when the\naverage performance of two classiﬁers is close, then\nthe inference time can be a discriminant feature for\nthe classiﬁer selection. But only in this case: see for\ninstance the URL results, where the RF- 10performed\njust slightly better than SVMs, but although having\nan inference time one order of magnitude higher, the\ninduced LBF has a lower reject time (see Section 5.2.2\nfor the relative discussion). Surprisingly, this analysis\nhas been overlooked in the literature. For instance,\nbenchmark URL data falls in this category, but all\nprevious experimental studies regarding learned BF\non this data do not consider neither SVMs, nor NNs.\nFor hard datasets instead, the space budget is cen-\ntral for the classiﬁer choice. Indeed, within the budget\ngiven by (1), on synthetic datasets, having a relative\nsmall key set and accordingly a lower budget, the\nchoice is almost forced towards small although inaccu-\nrate classiﬁers, being the larger ones too demanding\nfor the available budget. In particular, SVM is to be\nexcluded due to the increased diﬃculty w.r.t. URL\ndata, and for the remaining classiﬁers, we note thatthey behave very similarly (Figure 2). Thus the most\nsuccinct ones are to be preferred in this case, namely\nthe smallest NN. As opposite, when the space budget\nincreases, as it happens for DNA data, our ﬁndings\nsuggest to learn more accurate classiﬁers, even if this\nrequires the usage of a considerable budget fraction.\nIndeed, the gain induced by higher classiﬁcation abili-\nties in turn allows to save space when constructing the\nbackup ﬁlter, to have consequently a smaller reject\ntime, as well as an overall more eﬃcient structure (cfr.\nSection 5.2.1). This is also motivated by the fact that\nto accurately train complex classiﬁers the sample size\nmust be large enough [ 45]. Therefore, on DNA data\n(Figures 3(b) and 7), the most complex NNs resulted\nas the best choice.\nLearned Bloom Filters Choice Regarding the\nchoice of which learned BF is more suitable to use,\nourexperimentsrevealthreemaintrends: 1)onbench-\nmark data, that is those used also in the literature\nso far (URL data), ADA-BF is conﬁrmed as more\neﬀective variant in terms of FPR, when the budget is\nﬁxed; 2) however, its reject time is always and largely\nthe highest one, thus suggesting to evaluate its us-\nage in applications where fast responses are necessary\n(e.g., online applications). This subject includes also\nthe classiﬁer choice, since the most eﬀective ﬁlters\nare in most cases induced by NNs, but they are also\nslower in terms of reject time of the counterparts in-\nduced by faster classiﬁers, and accordingly a trade-oﬀ\nFPR/reject time must be carefully investigated also\nwhen choosing the classiﬁer; 3) SLBF is the ﬁlter most\nrobust to data noise, and the only one able to beneﬁt\nmore even from classiﬁer with poor performance (cfr.\nsynthetic noisy and DNA data). As a consequence,\nSBLF is clearly the ﬁlter to choose in presence of very\ncomplex data. In particular, the point 3) is a new\nand quite unforeseen behaviour, emerged only thanks\nto the study of data complexity and relative noise\ninjection procedure designed in this study, and which\nto some extent changes the ranking of most eﬀective\nlearned BF in practice, since most real datasets are\ntypically characterized by noise.\n17\n\nFigure 6: Empirical false positive rate of LBF (left), SLFB (central), and ADA-BF (right) ﬁlters on URL\ndata. On the horizontal axis the diﬀerent budgets conﬁgurations. Dotted blue line represents the baseline\nclassical Bloom Filter.\nFigure 7: Empirical false positive rate of LBF (left), SLFB (central), and ADA-BF (right) ﬁlters on DNA\ndata. On the horizontal axis the diﬀerent budgets conﬁgurations. Dotted blue line represents the baseline\nclassical Bloom Filter.\n18\n\nTable 6: Learned Bloom Filters average reject time,\nexpressed as percentage of the baseline BF reject\ntime. Positive (resp. negative) values indicate that\nthe learned ﬁlter is slower (faster) than the baseline.\nResults are averaged across test queries and the ﬁlter\nspace budgets considered. We remark that for DNA\nexperiments another machine has been used w.r.t.\nsynthetic and URL data (see Section 4.2).\nSynthetic Data (1.364·10−5)\nClassiﬁer LBF SLBF ADA-BF\nSVM 18.4 6 .1 151 .2\nRF −11.1 −17.5 112 .8\nNN 106 .9 54 .1 159 .3\nURL Data (3.259·10−5)\nSVM 22.6 3 .7 3 .9\nRF 6.6 7 .1 9 .7\nNN 43.9 49 .6 35 .3\nDNA Data ( 4.817·10−5)\nSVM −12.5 −11.7 35 .9\nRF-10 1.4 −20.6 32 .0\nRF-100 19.8 −7.4 40 .6\nNN-7 −5.0 −12.0 25 .8\nNN-125,50 −3.6 −7.5 25 .2\nNN-500,150 −1.9 −11.6 39 .27 Conclusions and Future De-\nvelopments\nWe have proposed an experimental methodology that\ncan guide in the design and validation of learned\nBloom Filters. The key point is to base the choice of\nthe classiﬁer to be used in a learned Bloom Filter on\nthe space budget of the entire data structure as well\nas the classiﬁcation complexity of the input dataset.\nWe have experimentally shown that our methodology\nyields useful indications, e.g., how robust are the\ncurrent learned Boom Filters for the processing of\ndatasets of increasing complexity. So far, only the\n“easy to classify” scenario has been considered in the\nLiterature. A potential limitation of such results is\nthat they might be dependent on the considered data;\nnonetheless, this is somehow inevitable due to the\nnature of Learned Data Structures. Finally, we point\nout that the societal impacts of our contributions\nare in line with general-purpose Machine Learning\ntechnology. Natural extensions of this research are as\nfollows. As already remarked, we have complied with\nan experimental setting coherent with the State of the\nArt. Wecanalsoconsiderthecaseinwhichthedesired\nFPR is ﬁxed and one asks for the most succinct pair\nclassiﬁer-ﬁlter. Moreover, in his seminal paper [ 39],\nMitzenmacher has shown that Learned Bloom Filters\ncan be quite sensitive to the input query distribution.\nYet, no study is available to quantify this aspect. Our\nmethodology can be extended also to those types of\nanalysis and work in this direction is in progress.\nAcknowledgements\nThis work has been supported by the Italian MUR\nPRIN project “Multicriteria data structures and al-\ngorithms: from compressed to learned indexes, and\nbeyond” (Prot. 2017WR7SHH). Additional support\nto R.G. has been granted by Project INdAM - GNCS\n“Analysis and Processing of Big Data based on Graph\nModels”.\n19\n\nReferences\n[1]Shawkat Ali and Kate A Smith. On learning\nalgorithm selection for classiﬁcation. Applied\nSoft Computing , 6(2):119–138, 2006.\n[2]D. Amato, G. Lo Bosco, and R. Giancarlo.\nLearned sorted table search and static indexes\nin small model space (Extended Abstract). In\nProc. of the 20-th Italian Conference in Artiﬁcial\nIntelligence (AIxIA), to appear in Lecture Notes\nin Computer Science , 2021.\n[3]Burton H. Bloom. Space/time trade-oﬀs in hash\ncoding with allowable errors. Commun. ACM ,\n13(7):422–426, July 1970.\n[4]Antonio Boﬀa, Paolo Ferragina, and Giorgio Vin-\nciguerra. A “learned” approach to quicken and\ncompress rank/select dictionaries. In Proceedings\nof the SIAM Symposium on Algorithm Engineer-\ning and Experiments (ALENEX) , 2021.\n[5]L. Breiman. Random Forests. Machine Learning ,\n45(1):5–32, 2001.\n[6]Leo Breiman, Jerome H. Friedman, Richard A.\nOlshen, and Charles J. Stone. Classiﬁca-\ntion and regression trees. The Wadsworth\nStatistics/Probability Series. Belmont, Califor-\nnia: Wadsworth International Group, a Division\nof Wadsworth, Inc. X, 358 p. $ 29.25; $ 18.95\n(1984)., 1984.\n[7]Andrei Broder and Michael Mitzenmacher. Net-\nwork Applications of Bloom Filters: A Survey. In\nInternet Mathematics , volume 1, pages 636–646,\n2002.\n[8]L. Bruzzone and S.B. Serpico. Classiﬁcation of\nimbalanced remote-sensing data by neural net-\nworks.Pattern Recognition Letters , 18(11):1323–\n1328, 1997.\n[9]José-Ramón Cano. Analysis of data complexity\nmeasures for classiﬁcation. Expert Systems with\nApplications , 40(12):4820–4831, 2013.[10]J.LawrenceCarterandMarkN.Wegman. Univer-\nsalclassesofhashfunctions. Journal of Computer\nand System Sciences , 18(2):143–154, 1979.\n[11]Kyunghyun Cho, Bart van Merriënboer, Dzmitry\nBahdanau, andYoshuaBengio. Ontheproperties\nof neural machine translation: Encoder–decoder\napproaches. In Proceedings of SSST-8, Eighth\nWorkshop on Syntax, Semantics and Structure\nin Statistical Translation , pages 103–111, Doha,\nQatar, October 2014. Association for Computa-\ntional Linguistics.\n[12]Benny Chor, David Horn, Nick Goldman, Yaron\nLevy, Tim Massingham, et al. Genomic DNA\nk-mer spectra: models and modalities. Genome\nBiol, 10(10):R108, 2009.\n[13]David R Cox. The regression analysis of binary\nsequences. Journal of the Royal Statistical So-\nciety: Series B (Methodological) , 20(2):215–232,\n1958.\n[14]Zhenwei Dai. Adaptive learned bloom ﬁlter (ada-\nbf): Eﬃcient utilization of the classiﬁer. https:\n//github.com/DAIZHENWEI/Ada-BF , 2022. Last\nchecked on Nov. 8, 2022.\n[15]Zhenwei Dai and Anshumali Shrivastava. Adap-\ntive Learned Bloom Filter (Ada-BF): Eﬃcient\nutilization of the classiﬁer with application to\nreal-time information ﬁltering on the web. In\nAdvances in Neural Information Processing Sys-\ntems, volume 33, pages 11700–11710. Curran As-\nsociates, Inc., 2020.\n[16]R.O.DudaandP.E.Hart. Pattern Classiﬁcation\nand Scene Analysis . John Willey & Sons, New\nYork, 1973.\n[17]Richard O. Duda, Peter E. Hart, and David G.\nStork.Pattern Classiﬁcation, 2nd Edition . Wiley,\n2000.\n[18]R.A.LeoElworth, QiWang, PavanK.Kota, C.J.\nBarberan, Benjamin Coleman, Advait Balaji,\nGaurav Gupta, Richard G. Baraniuk, Anshumali\nShrivastava, and Todd J. Treangen. To petabytes\nand beyond: recent advances in probabilistic and\n20\n\nsignal processing algorithms and their applica-\ntion to metagenomics. Nucleic acids research ,\n48(10):5217–5234, Jun 2020. 32338745[pmid],\nPMC7261164[pmcid], 5825624[PII].\n[19]Paolo Ferragina, Fabrizio Lillo, and Giorgio\nVinciguerra. On the performance of learned\ndata structures. Theoretical Computer Science ,\n871:107–120, 2021.\n[20]Paolo Ferragina and Giorgio Vinciguerra. The\nPGM-index: a fully-dynamic compressed learned\nindex with provable worst-case bounds. PVLDB,\n13(8):1162–1175, 2020.\n[21]M Julia Flores, José A Gámez, and Ana M\nMartínez. Domains of competence of the semi-\nnaive bayesian network classiﬁers. Information\nSciences, 260:120–148, 2014.\n[22]David Freedman. Statistical Models : Theory\nand Practice . Cambridge University Press, 2005.\n[23]G.Fumagalli, D.Raimondi, R.Giancarlo, D.Mal-\nchiodi, and M. Frasca. On the choice of gen-\neral purpose classiﬁers in learned bloom ﬁlters:\nAn initial analysis within basic ﬁlters. In Pro-\nceedings of the 11th International Conference on\nPattern Recognition Applications and Methods\n(ICPRAM) , pages 675–682, 2022.\n[24]SimonHaykin. Neural networks: a comprehensive\nfoundation . Prentice Hall PTR, 1994.\n[25]Haibo He and E.A. Garcia. Learning from imbal-\nanced data. Knowledge and Data Engineering,\nIEEE Transactions on , 21(9):1263–1284, Sept\n2009.\n[26]Mohammed Khalilia, Sounak Chakraborty, and\nMihail Popescu. Predicting disease risks from\nhighly imbalanced data using random forest.\nBMC Medical Informatics and Decision Making ,\n11(1):51, Jul 2011.\n[27]Andreas Kipf, Ryan Marcus, Alexander van Re-\nnen, Mihail Stoian, Alfons Kemper, Tim Kraska,and Thomas Neumann. Radixspline: A single-\npass learned index. In Proc. of the Third Inter-\nnational Workshop on Exploiting Artiﬁcial Intel-\nligence Techniques for Data Management , aiDM\n’20, pages 1–5. Association for Computing Ma-\nchinery, 2020.\n[28]Melanie Kirsche, Arun Das, and Michael C\nSchatz. Sapling: accelerating suﬃx array queries\nwith learned data models. Bioinformatics ,\n37(6):744–749, 10 2020.\n[29]Melanie Kirsche, Arun Das, and Michael C\nSchatz. Sapling: accelerating suﬃx array queries\nwith learned data models. Bioinformatics ,\n37(6):744–749, 10 2020.\n[30]Tim Kraska. Towards instance-optimized data\nsystems. Proc. VLDB Endow. , 14(12):3222–3232,\noct 2021.\n[31]Tim Kraska, Alex Beutel, Ed H. Chi, Jeﬀrey\nDean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proc. of the 2018 Int. Conf.\non Management of Data , SIGMOD ’18, pages\n489–504, New York, NY, USA, 2018. Association\nfor Computing Machinery.\n[32]Qiyu Liu, Libin Zheng, Yanyan Shen, and Lei\nChen. Stable learned bloom ﬁlters for data\nstreams. Proc. VLDB Endow. , 13(12):2355–2367,\nsep 2020.\n[33]Ana C. Lorena, Luís P. F. Garcia, Jens Lehmann,\nMarcilio C. P. Souto, and Tin Kam Ho. How\ncomplex is your classiﬁcation problem? a survey\non measuring classiﬁcation complexity. ACM\nComput. Surv. , 52(5), sep 2019.\n[34]Julián Luengo and Francisco Herrera. An au-\ntomatic extraction method of the domains of\ncompetence for learning classiﬁers using data\ncomplexity measures. Knowledge and Informa-\ntion Systems , 42(1):147–180, 2015.\n[35]Marcel Maltry and Jens Dittrich. A critical\nanalysis of recursive model indexes. CoRR,\nabs/2106.16166, 2021.\n21\n\n[36]Ryan Marcus, Andreas Kipf, Alexander van\nRenen, Mihail Stoian, Sanchit Misra, Alfons\nKemper, Thomas Neumann, and Tim Kraska.\nBenchmarking learned indexes. arXiv preprint\narXiv:2006.12804 , 14:1–13, 2020.\n[37]Ryan Marcus, Emily Zhang, and Tim Kraska.\nCDFShop: Exploring and optimizing learned\nindex structures. In Proc. of the 2020 ACM\nSIGMOD Int. Conf. on Management of Data ,\nSIGMOD ’20, pages 2789–2792, 2020.\n[38]Giosuè Cataldo Marinò, Alessandro Petrini,\nDario Malchiodi, and Marco Frasca. Deep neural\nnetworks compression: a comparative survey and\nchoice recommendations. Neurocomputing , 2022.\n[39]Michael Mitzenmacher. A model for learned\nbloom ﬁlters and optimizing by sandwiching. In\nAdvances in Neural Information Processing Sys-\ntems, volume 31. Curran Associates, Inc., 2018.\n[40]Michael Mitzenmacher and Sergei Vassilvitskii.\nAlgorithms with predictions. In Tim Roughgar-\nden, editor, Beyond the Worst-Case Analysis of\nAlgorithms , page 646–662. Cambridge University\nPress, 2021.\n[41]Katharina Morik, Peter Brockhausen, and\nThorsten Joachims. Combining statistical learn-\ning with a knowledge-based approach: a case\nstudy in intensive care monitoring. Technical\nreport, Technical Report, 1999.\n[42]Python Software Foundation. pickle – python ob-\nject serialization. https://docs.python.org/\n3/library/pickle.html , 2021. Last checked\non May. 17, 2022.\n[43]Amatur Rahman and Paul Medevedev. Represen-\ntation of k-mer sets using spectrum-preserving\nstring sets. Journal of Computational Biology ,\n28(4):381–394, 2021. PMID: 33290137.\n[44]Davide Raimondi and Giacomo Fumagalli.\nA critical analysis of classiﬁer selection\nin learned bloom ﬁlters – supporting soft-\nware. https://github.com/RaimondiD/LBF_ADABF_experiment , 2022. Last checked on\nNov. 8, 2022.\n[45]S Raudys. On the problems of sample size in\npattern recognition. In Detection, pattern recog-\nnition and experiment design: Vol. 2. Proceedings\nof the 2nd all-union conference statistical methods\nin control theory . Publ. House\" Nauka\", 1970.\n[46]Brad Solomon and Carl Kingsford. Fast search of\nthousands of short-read sequencing experiments.\nNature Biotechnology , 34(3):300–302, Mar 2016.\n[47]Kapil Vaidya, Eric Knorr, Tim Kraska, and\nMichael Mitzenmacher. Partitioned learned\nbloom ﬁlters. In International Conference on\nLearning Representations , 2021.\n[48]Jason Van Hulse, Taghi M. Khoshgoftaar, and\nAmri Napolitano. Experimental perspectives on\nlearning from imbalanced data. In Proceedings\nof the 24th International Conference on Machine\nLearning , ICML ’07, pages 935–942, New York,\nNY, USA, 2007. ACM.\n[49]Mark N. Wegman and J.Lawrence Carter. New\nhash functions and their use in authentication\nand set equality. Journal of Computer and Sys-\ntem Sciences , 22(3):265–279, 1981.\n[50]Qingtao Wu, Qianyu Wang, Mingchuan Zhang,\nRuijuan Zheng, Junlong Zhu, and Jiankun Hu.\nLearnedbloom-ﬁlterfortheeﬃcientnamelookup\nin information-centric networking. Journal of\nNetwork and Computer Applications , 186:103077,\n04 2021.\n[51]Andreas Zell. Simulation neuronaler Netze . ha-\nbilitation, Uni Stuttgart, 1994.\n22",
  "textLength": 67102
}