{
  "paperId": "ea4ba92d596b3ad1c4d68e627f94efe895d9d2bd",
  "title": "Efficient Cost Modeling of Space-filling Curves",
  "pdfPath": "ea4ba92d596b3ad1c4d68e627f94efe895d9d2bd.pdf",
  "text": "Efficient Cost Modeling of Space-filling Curves\nGuanli Liu\nThe University of Melbourne\nAustralia\nguanli@student.unimelb.edu.auLars Kulik\nThe University of Melbourne\nAustralia\nlkulik@unimelb.edu.auChristian S. Jensen\nAalborg University\nDenmark\ncsj@cs.aau.dk\nTianyi Li\nAalborg University\nDenmark\ntianyi@cs.aau.dkJianzhong Qi\nThe University of Melbourne\nAustralia\njianzhong.qi@unimelb.edu.au\nABSTRACT\nAspace-filling curve (SFC) maps points in a multi-dimensional space\nto one-dimensional points by discretizing the multi-dimensional\nspace into cells and imposing a linear order on the cells. This way,\nan SFC enables the indexing of multi-dimensional data using a one-\ndimensional index such as a B+-tree. Choosing an appropriate SFC\nis crucial, as different SFCs have different effects on query perfor-\nmance. Currently, there are two primary strategies: 1) deterministic\nschemes, which are computationally efficient but often yield subop-\ntimal query performance, and 2) dynamic schemes, which consider\na broad range of candidate SFCs based on cost functions but incur\nsignificant computational overhead. Despite these strategies, exist-\ning methods cannot efficiently measure the effectiveness of SFCs\nunder heavy query workloads and numerous SFC options.\nTo address this problem, we propose means of constant-time cost\nestimations that can enhance existing SFC selection algorithms, en-\nabling them to learn more effective SFCs. Additionally, we propose\nan SFC learning method that leverages reinforcement learning and\nour cost estimation to choose an SFC pattern efficiently. Experi-\nmental studies offer evidence of the effectiveness and efficiency of\nthe proposed means of cost estimation and SFC learning.\nKEYWORDS\nSpace-filling Curves, Cost Model, Reinforcement Learning\nACM Reference Format:\nGuanli Liu, Lars Kulik, Christian S. Jensen, Tianyi Li, and Jianzhong Qi.\n2024. Efficient Cost Modeling of Space-filling Curves. In Proceedings of\nACM Conference (Conferenceâ€™17). ACM, New York, NY, USA, 13 pages. https:\n//doi.org/XXXXXXX.XXXXXXX\n1 INTRODUCTION\nIndexing is essential to enable efficient query processing on increas-\ningly massive data, including spatial and other low-dimensional\ndata. In this setting, indices based on space-filling curves (SFC)\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nConferenceâ€™17, July 2017, Washington, DC, USA\nÂ©2024 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\nhttps://doi.org/XXXXXXX.XXXXXXXare used widely. For example, Z-order curves (ZC, see Figures 1a\nand 1b) [ 22] are used in Hudi [ 2], RedShift [ 1], and SparkSQL [ 4];\nlexicographic-order curves (LC, see Figure 1c) are used in Post-\ngreSQL [ 24] and SQL Server [ 15]; and Hilbert curves (HC) [ 5] are\nused in Google S2 [ 27]. Next, the arguably most important type\nof query in this setting is the range query that also serves as a\nfoundation for other queries, including ğ‘˜NN queries.\nThe most efficient query processing occurs when the data needed\nfor a query result is stored consecutively, or when the data is stored\nin a few data blocks. Thus, the storage organizationâ€”the order\nin which the data is storedâ€”affects the cost of processing a query\nprofoundly. When indexing data using SFC-based indices, the choice\nof which SFC to use for ordering the data is important.\nxy\nq1q3q2\n(a) Curve 1 (ZC)\nxy\nq1q3q2 (b) Curve 2 (ZC)\nxy\nq1q3q2 (c) Curve 3 (LC)\nFigure 1: Examples of SFCs (in grey) and queries (in red).\nDifferent range queries benefit differently from different SFCs.\nIn Figure 1, three SFCs on the same data space are shown along\nwith three queries. The fewer disconnected segments of an SFC\nthat need to be accessed to compute a query, the better. To compute\nğ‘1, the SFC in Figure 1a is preferable because only a single segment\nneeds to be accessed. Put differently, the data needed may be in a\nsingle or in consecutive blocks. In contrast, the SFCs in Figures 1b\nand 1c map the needed data to two and four segments, respectively.\nNext, we observe that no single SFC is optimal for all queries.\nWhile the SFC in Figure 1a is good for ğ‘1, it is suboptimal for ğ‘2\nandğ‘3. It is thus critical to select the right SFC for a given query (or\nquery workload). This in turn calls for efficient means of estimating\nthe cost of computing a query using a particular SFC (without query\nexecution) to guide SFC selection.\nExisting studies [ 17,32] provide cost estimations based on count-\ning the number of clusters (continuous curve segments) covered\nby a query. However, their calculations rely on curve segment\nscans that require ğ‘‚(ğ‘‰)time, where ğ‘‰is proportional to the size\nof a query. Given a workload of ğ‘›queries and ğ‘šcandidate SFCs,\nğ‘‚(ğ‘›Â·ğ‘šÂ·ğ‘‰)time is needed to choose an SFC. This is expensive\ngiven large ğ‘›andğ‘š(e.g., ağ‘˜Ã—ğ‘˜grid can form ğ‘š=ğ‘˜2!candidate\nSFCs), thus jeopardizing the applicability of the cost model.\n1arXiv:2312.16355v1  [cs.DB]  26 Dec 2023\n\nConferenceâ€™17, July 2017, Washington, DC, USA et al.\nIn this paper, we provide efficient means of SFC cost estimation\nsuch that a query-optimal SFC can be found efficiently. Specifically,\nwe present algorithms that compute the cost of a query in ğ‘‚(1)time.\nAfter anğ‘‚(ğ‘›)-time initialization, the algorithms compute the cost\nofğ‘›queries inğ‘‚(1)time for each new SFC to be considered. This\nmeans that given ğ‘šcandidate SFCs, our algorithms can find the\noptimal SFC in ğ‘‚(ğ‘š)time, which is much smaller than ğ‘‚(ğ‘›Â·ğ‘šÂ·ğ‘‰)\nand thus renders SFC cost estimation practical.\nOur algorithms are based on a well-chosen family of SFCs, the\nbit-merging curves (BMC) [ 7,19]. A BMC maps multi-dimensional\npoints by merging the bit sequences of the point coordinates (i.e.,\ncolumn indices) from all ğ‘‘dimensions (detailed in Section 3.1). We\nconsider BMCs for two reasons: (1) BMCs generalize ZC and LC\nused in real systems [ 2,4,15,24]. Algorithms to find optimal BMCs\ncan be integrated seamlessly into real systems. (2) The space of\nBMCs is large. For example, in a 2-dimensional space ( ğ‘‘=2), where\neach dimension uses 16 bits ( â„“=16) for a column index, there are\nğ‘˜=2â„“columns in each dimension of the grid. This yields about\n6Ã—108(i.e.,(ğ‘‘Â·â„“)!\n(â„“!)ğ‘‘) candidate BMCs. An efficient cost model enables\nfinding a query-efficient SFC in this large space.\nOur algorithms model the cost of a range query based on the\nnumber and lengths of curve segments covered by the query, which\nin turn relate to the difference between the curve values of the\nend points of each curve segment. We exploit the property that the\ncurve values of a BMC come from merging the bits of the column\nindices. This property enables deriving a closed-form equation to\ncompute the length of a curve segment in ğ‘‚(ğ‘‘Â·â„“)=ğ‘‚(1)time\n(given thatğ‘‘andâ„“are constants) for ğ‘›queries. The property also\nenables pre-computing ğ‘‘look-up tables that allow computing the\nnumber of curve segments in ğ‘‚(ğ‘‘Â·â„“)=ğ‘‚(1)time. Thus, we achieve\nconstant-time SFC cost estimation.\nWe show the applicability of the cost estimation algorithms by\nincorporating them into the state-of-the-art learned BMC-based\nstructure, the BMTree [13]. The BMTree computes empirical query\ncosts by executing a query workload on the dataset to be indexed.\nEven with its dataset sampling strategy to reduce the computational\ncosts for query cost estimation, the original SFC learning algorithm\nof the BMTree takes seven hours (cf. BMTree-SP in Figure 11a) to\nindex a dataset of 100 million points (with only 100,000 sampled\npoints for query cost estimation). Our cost estimation algorithms\nbring this time down to 57 seconds (cf. BMTree-GC in Figure 11a)\nwith little impact on query efficiency.\nFurthermore, we develop an SFC learning algorithm named\nLBMC that uses Reinforcement Learning (RL) techniques to find\nthe optimal BMC. Importantly, the reward calculation in RL lever-\nages our closed-form cost estimation equation and pre-computed\nlook-up tables, thus making the entire learning process extremely\nefficient. This enables the RL agent to converge rapidly to near-\noptimal solutions while navigating the state space.\nIn summary, the paper makes the following contributions:\n(1) We propose algorithms for efficient range query cost estima-\ntion when using BMC-based indices on multi-dimensional datasets.\nThe algorithms can compute the cost of a range query in ğ‘‚(1)time\nas well as the cost of a workload of ğ‘›queries inğ‘‚(1)time, after\na simple scan over the queries. (2) We generalize the applicabil-\nity of the cost estimation to existing state-of-the-art SFC learningmethods based on BMCs, enhancing the learning efficiency of such\nmethods. (3) We propose LBMC, an efficient BMC learning algo-\nrithm that leverages the proposed cost estimation. (4) We evaluate\nthe cost estimation and LBMC algorithms on both real and synthetic\ndatasets, finding that (i) our cost estimation outperforms baselines\nconsistently by up to 105times in efficiency, (ii) our cost estima-\ntion accelerates the reward calculation of the BMTree by 400x with\nlittle impact on query efficiency, and (iii) the LBMC algorithm has\nlower learning and query costs than the competing SFC learning\nalgorithms, including the BMTree.\nThe rest of the paper is organized as follows. Section 2 covers\nrelated work. Section 3 presents preliminaries, and Section 4 details\nour cost estimations. Section 5 presents LBMC, and Section 6 reports\nthe experimental results. Section 7 concludes the paper.\n2 RELATED WORK\nSpace-filling curves. SFCs find use in many fields, including in in-\ndexing [ 10,12,21,31], data mining [ 3], and machine learning [ 8,29].\nAn SFC maps multi-dimensional data values to one-dimensional\nvalues, which are then indexed using a one-dimensional index, e.g.,\nthe B+-tree.\nTwo popular SFCs, ZC[22] and HC[5], are being deployed in\npractical data systems [ 1,2,4]. Bit-merging curves (BMCs, detailed\nin Section 3.1) are a family of SFCs, where the curve value of a grid\ncell is formed by merging the bits of the cellâ€™s column indices from\nallğ‘‘dimensions. To better order the data points for specific query\nworkloads, QUILTS [19] provides a heuristic method to design a\nseries of BMCs and selects the optimal one. A recent technique, the\nBit Merging Tree (BMTree) [ 13], learns piece-wise SFCs (i.e., BMCs)\nby using a quadtree [ 6]-like strategy to partition the data space and\nselecting different BMCs for different space partitions.\nCost estimation for space-filling curves. To learn an optimal\nSFC, cost estimation is employed to approximate the query costs\nwithout actually computing the queries. Two studies [ 17,32] offer\ntheoretical means of estimating the number of curve segments\ncovered by a query range. They do not offer empirical results or\nguidance on how to construct a query-efficient SFC index.\nQUILTS formulates the query cost Cğ‘¡for a BMC index over a\nset of queries asCğ‘¡=Cğ‘”Â·Cğ‘™, whereCğ‘”is aglobal cost andCğ‘™\nis alocal cost . The global cost is the length of a continuous BMC\nsegment that is able to cover a query range ğ‘fully minus the length\nof the BMC segments in ğ‘, for each query. The idea is to count\nthe number of segments outside ğ‘that may need to be visited to\ncompute the queries. The local cost is the entropy of the relative\nlength of each segment of the BMC curve outside ğ‘counted in the\nglobal cost, which reflects how uniformly distributed the lengths\nof such segments are. However, computing these two costs relies\non the accumulated length of the curve segments outside ğ‘, which\nis expensive to compute. Given ğ‘›range queries, it takes ğ‘‚(ğ‘›Â·ğ‘ğ‘¡)\ntime to computeCğ‘¡, whereğ‘‚(ğ‘ğ‘¡)is the average estimation cost per\nquery. Further, they can only be used to estimate the query costs\nof a given BMC index and do not enable an efficient search for a\nquery-efficient BMC index.\nThe BMTree estimates query costs using data points sampled\nfrom the target dataset. Such cost estimations are expensive for large\ndatasets and many queries. For example, BMTree curve learning\n2\n\nEfficient Cost Modeling of Space-filling Curves Conferenceâ€™17, July 2017, Washington, DC, USA\nover a dataset of 100 million points (with 100,000 sampled points)\nand 1,000 queries can take more than seven hours (cf. BMTree-SP\nin Figure 11a). While using a smaller sampled dataset and fewer\nqueries may reduce the learning time, the resulting curve may cause\nsuboptimal query performance (cf. BMTree-SP-6/8/10 in Figure 13).\nLMSFC [ 7], another recent proposal, learns a parameterized SFC\n(which is effectively a BMC) using Bayesian optimization [ 9]. Like\nthe BMTree, LMSFC uses a sampled dataset and a query work-\nload for query cost estimation and thus has the same issues as the\nBMTree. Our study aims to address these issues by providing a\nhighly effective and efficient cost estimation.\nSpace-filling curve-based indices. The Hilbert R-tree [ 10] is\na classic index structure based on SFC. It uses an HC to map and\norder multi-dimensional data, based on which an R-tree is built on\nthe data. This simple structure has been shown to be competitive in\nmany follow-up studies. A recent study further achieves worst-case\noptimal range query processing by adding an extra rank space -\nbased mapping step over the input data before the Hilbert R-tree is\nbuilt [ 26]. Another index, the Instance-Optimal Z-Index [23], uses\na quadtree-like strategy to recursively partition the data space. It\ncreates four sub-spaces of a (sub-)space, which may be of different\nsizes. The four sub-spaces are each ordered by ZCs of different\nsizes and follow a â€˜ zâ€™ or an â€˜Nâ€™ shape. At the bottom level of the\nspace partitioning hierarchy, the ZCs of sub-spaces that come from\ndifferent parent sub-spaces are connected following the order of the\nZC that traverses the parent sub-spaces. This way, a curve is formed\nthat traverses all bottom-level sub-spaces, and the data points are\nindexed in that order.\nIn the recent wave of machine learning-based optimization for\nindices [ 11,18], SFCs have been used to order and map multi-\ndimensional data points to one-dimensional values, such that one-\ndimensional learned indices (e.g., RMI [ 11]) can be applied. ZM [ 30]\nand RSMI [ 25] are representative proposals. As the BMTree [ 13]\nwork shows, different learned SFCs can be plugged into these index\nstructures to (possibly) improve their query performance. Our cost\nestimations can be applied to further enhance the SFC learning\nprocess as discussed above, which are orthogonal to these studies.\n3 PRELIMINARIES\nWe start with core concepts underlying BMCs and list frequently\nused symbols in Table 1.\n3.1 BMC Definition\nA BMC maps multi-dimensional points by merging the bit sequences\nof the coordinates (i.e., column indices) from all ğ‘‘dimensions into\na single bit sequence that becomes a one-dimension value [19].\n  00   01   10   11x00011011y\n00000\n0001100102\n00113\n01004\n0101501106\n0111710008\n10019101010\n101111\n110012\n110113111014\n111115BMC YXYX\n  00   01   10   11x00011011y\n0000000011\n0010200113\n0100401015\n01106011171000810019\n101010101111\n110012110113\n111014111115BMC YXXY\n  00   01   10   11x00011011y\n00000\n00011\n00102\n0011301004\n01015\n01106\n0111710008\n10019\n101010\n101111110012\n110113\n111014\n111115BMC YYXX\nFigure 2: BMC examples ( ğ‘‘=2andâ„“=2).\nFigure 2 plots three BMC schemes, which are represented by\nYXYX, YXXY, and YYXX. Here, the ordering of the Xâ€™s and Yâ€™sTable 1: Frequently used symbols.\nSymbol Description\nğ‘‘ The data space dimensionality\nâ„“ The number of bits for grid cell numbering in each dimension\nğ· A multi-dimensional dataset\nğ‘ A data point\nğ‘ A range query\nğ‘„ A set of range queries\nğµ The block size\nğ‘ğ‘ ,ğ‘ğ‘’The start and end points on an SFC of a range query\nğ‘› The number of range queries\nğœ A bit-merging curve (BMC)\nFğœ The curve value calculation function over BMC ğœ\nğ›¼ğ‘—\nğ‘–Theğ‘—th bit value in dimension ğ‘–\nğ›¾ğ‘—\nğ‘–The position (0-indexed) of ğ›¼ğ‘—\nğ‘–in a BMCğœ\nğ‘¥ğ‘– A value in dimension ğ‘–\n[ğ‘¥ğ‘ ,ğ‘–,ğ‘¥ğ‘’,ğ‘–]A value range in dimension ğ‘–\nspecify how the bits from dimensions ğ‘¥andğ‘¦are combined to\nobtain a BMC ğœ. The coordinates from each dimension have two\nbits, i.e., the bit lengthâ„“of each dimension is 2. The merged bit\nsequence (i.e., the curve value in binary form) has ğ‘‘Â·â„“=4bits.\nThe bit length â„“is determined by the grid resolution, which\nis a system parameter. We use the same â„“for each dimension to\nsimplify the discussion, and we use the little endian bit order, i.e.,\nthe rightmost bit has the lowest rank (cf. Figure 3). For simplicity,\nwe call the column indices of a point ğ‘in a cell (or the cell itself)\nthecoordinates ofğ‘(or the cell).\nBMC value calculation. Given a BMC ğœ, we compute the curve\nvalue of a point ğ‘=(ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘‘)using functionFğœ(ğ‘):\nFğœ(ğ‘)=ğ‘‘âˆ‘ï¸\nğ‘–=1â„“âˆ‘ï¸\nğ‘—=1ğ›¼ğ‘—\nğ‘–Â·2ğ›¾ğ‘—\nğ‘– (1)\nLetğ‘¥ğ‘–be the dimension- ğ‘–coordinate of ğ‘. In the equation, ğ›¼ğ‘—\nğ‘–âˆˆ{0,1}\nis theğ‘—th (ğ‘—âˆˆ[1,â„“]) bit ofğ‘¥ğ‘–, andğ›¾ğ‘—\nğ‘–is the rank of ğ›¼ğ‘—\nğ‘–in the BMC.\nâ„“âˆ‘ï¸\nğ‘—=1ğ›¼ğ‘—\nğ‘–Â·2ğ‘—âˆ’1=ğ‘¥ğ‘– (2)\nNote that the order among the bits from the same dimension does\nnot change when the bits are merged with those from the other\ndimensions to calculate Fğœ(ğ‘), i.e., for bits ğ›¼ğ‘—\nğ‘–andğ›¼ğ‘—+1\nğ‘–,ğ›¾ğ‘—\nğ‘–<ğ›¾ğ‘—+1\nğ‘–.\nFor ease of discussion, we use examples with up to three dimen-\nsionsğ‘¥,ğ‘¦, andğ‘§. Figure 3 calculates Fğœ(ğ‘)forğ‘=(2,1,7)given\nğœ=XYZXYZXYZ. Here, ğ›¼1\n3=1is the first bit value in dimension ğ‘§,\nand the rank of the first (i.e., rightmost) Z bit in ğœis zero, which\nmeansğ›¾1\n3=0. To calculate the curve value of a point for a given ğœ,\nwe derive each ğ›¼ğ‘—\nğ‘–andğ›¾ğ‘—\nğ‘–based onğ‘¥ğ‘–andğœ, respectively.\n8 7 6 5 4 3 2 1 0\n001101011XYZXYZXYZ\n0 1 0 (2) 001 (1) 1 1 1 (7)2 1 0 \nY Y Y X X X Z Z Z2 1 0 2 1 0 Little Endian\nHigh Low\nPosition:\nBit sequence:\nFigure 3: BMC curve value calculation ( ğ‘‘=3andâ„“=3).\n3\n\nConferenceâ€™17, July 2017, Washington, DC, USA et al.\nBMC monotonicity. The BMC value calculation process implies\nthat any BMC is monotonic.\nTheorem 1 (Monotonicity). Givenğ‘1=(ğ‘¥1,1,...,ğ‘¥ 1,ğ‘‘)and\nğ‘2=(ğ‘¥2,1,...,ğ‘¥ 2,ğ‘‘)thenâˆ€ğ‘–âˆˆ[1,ğ‘‘](ğ‘¥1,ğ‘–â‰¤ğ‘¥2,ğ‘–)â†’Fğœ(ğ‘1)â‰¤Fğœ(ğ‘2).\nProof. Givenğ‘¥1,ğ‘–â‰¤ğ‘¥2,ğ‘–, we haveÃâ„“\nğ‘—=1ğ›¼ğ‘—\n1,ğ‘–Â·2ğ‘—âˆ’1â‰¤Ãâ„“\nğ‘—=1ğ›¼ğ‘—\n2,ğ‘–Â·2ğ‘—âˆ’1\nbased on Equation 2. The order among the bits from ğ‘¥1,ğ‘–andğ‘¥2,ğ‘–\ndo not change when they are used to calculate Fğœ(ğ‘1)andFğœ(ğ‘2),\nrespectively. Thus,Ãâ„“\nğ‘—=1ğ›¼ğ‘—\n1,ğ‘–Â·2ğ›¾ğ‘—\n1,ğ‘–â‰¤Ãâ„“\nğ‘—=1ğ›¼ğ‘—\n2,ğ‘–Â·2ğ›¾ğ‘—\n2,ğ‘–. Since this holds\nfor anyğ‘–âˆˆ[1,ğ‘‘], we haveÃğ‘‘\nğ‘–=1Ãâ„“\nğ‘—=1ğ›¼ğ‘—\n1,ğ‘–Â·2ğ›¾ğ‘—\n1,ğ‘–â‰¤Ãğ‘‘\nğ‘–=1Ãâ„“\nğ‘—=1ğ›¼ğ‘—\n2,ğ‘–Â·\n2ğ›¾ğ‘—\n2,ğ‘–, i.e.,Fğœ(ğ‘1)â‰¤Fğœ(ğ‘2). â–¡\n3.2 Range Querying Using a BMC\nNext, we present concepts on range query processing with BMCs\nthat will be used later to formulate query cost estimation.\nDefinition 1 (Range Query). Given ağ‘‘-dimensional dataset ğ·\nand a range query ğ‘=[ğ‘¥ğ‘ ,1,ğ‘¥ğ‘’,1]Ã—[ğ‘¥ğ‘ ,2,ğ‘¥ğ‘’,2]Ã—...Ã—[ğ‘¥ğ‘ ,ğ‘‘,ğ‘¥ğ‘’,ğ‘‘], where\n[ğ‘¥ğ‘ ,ğ‘–,ğ‘¥ğ‘’,ğ‘–]denotes the query range in dimension ğ‘–, queryğ‘returns all\npointsğ‘=(ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘‘)âˆˆğ·that satisfy:âˆ€ğ‘–âˆˆ[1,ğ‘‘](ğ‘¥ğ‘ ,ğ‘–â‰¤ğ‘¥ğ‘–â‰¤ğ‘¥ğ‘’,ğ‘–).\nAs mentioned earlier, computing a query ğ‘using different BMCs\ncan lead to different costs. To simplify the discussion for determin-\ning the cost of a query, we use the following corollary.\nCorollary 1. Givenğ‘ğ‘ =(ğ‘¥ğ‘ 1,...,ğ‘¥ğ‘ ğ‘‘)andğ‘ğ‘’=(ğ‘¥ğ‘’1,...,ğ‘¥ğ‘’ğ‘‘),\nany queryğ‘is bounded by the curve value range [Fğœ(ğ‘ğ‘ ),Fğœ(ğ‘ğ‘’)].\nCorollary 1 follows directly from the monotonicity of BMCs\n(Theorem 1). To simplify the discussion, we use a point ğ‘and the\ncell that encloses ğ‘interchangeably and rely on the context for\ndisambiguation.\nQuery section [ 19].A continuous curve segment in a query\nğ‘is called a query section . We denote a query section ğ‘ with end\npointsğ‘ğ‘–andğ‘ğ‘—by[Fğœ(ğ‘ğ‘–),Fğœ(ğ‘ğ‘—)]. Intuitively, each query section\ntranslates to a one-dimensional range query [Fğœ(ğ‘ğ‘–),Fğœ(ğ‘ğ‘—)]on a\nB+-tree index on dataset ğ·. Thus, the number of query sections in\n[Fğœ(ğ‘ğ‘ ),Fğœ(ğ‘ğ‘’)]determines the cost of ğ‘.\n   0   1   2   3   4   5   6   7x01234567y\nq\npiâˆ’1\npj+14039\n3536\ns1\ns2s3\ne1e2e3e4\ne5\npipj\n(a) BMC XYXYXY\n   0   1   2   3   4   5   6   7x01234567y\nq\ns1s2\ns3s4e3\ne1e2e4 (b) BMC YXYXYX\nFigure 4: Query sections and directed edges in BMCs.\nExample 1. In Figure 4a, there are three query sections ğ‘ 1,ğ‘ 2,\nandğ‘ 3, withğ‘ 2=[Fğœ(ğ‘ğ‘–),Fğœ(ğ‘ğ‘—)]=[36,39]. By definition, a point\n(cell) immediately preceding ğ‘ğ‘–or succeeding ğ‘ğ‘—must be outside ğ‘;\notherwise, it is part of the query section. For example, ğ‘ğ‘–âˆ’1(Fğœ(ğ‘ğ‘–âˆ’1)=\n35) andğ‘ğ‘—+1(Fğœ(ğ‘ğ‘—+1)=40) in Figure 4a are outside ğ‘. The number\nof query sections in ğ‘varies across different BMCs, e.g., the same ğ‘as\nin Figure 4a has four query sections in Figure 4b.Directed edge [ 32].Query sections are composed by connecting\na series of points (cells). The pair of two consecutive points ğ‘ğ‘–and\nğ‘ğ‘—forms a directed edge (denoted by ğ‘’) if the curve values of ğ‘ğ‘–and\nğ‘ğ‘—differ by one under a given ğœ, i.e.,Fğœ(ğ‘ğ‘—)âˆ’Fğœ(ğ‘ğ‘–)=1. As each\npoint is represented through a binary value, the difference occurs\nbecauseFğœ(ğ‘ğ‘–)=...|{z}\nprefix01...1|{z}\nğ¾1sandFğœ(ğ‘ğ‘—)=...|{z}\nprefix10...0|{z}\nğ¾0s, where the\nlastğ¾(ğ¾â‰¥0) bits are changed from 1 to 0 and the ( ğ¾+1)st bit is\nchanged from 0 to 1.\nExample 2. We use the binary form of two pairs of integers that\nform directed edges to illustrate this concept, one for ğ¾>0and the\nother forğ¾=0. First, suppose that the binary representations of\nFğœ(ğ‘ğ‘–)=15andFğœ(ğ‘ğ‘—)=16are001111 and 010000, respectively. In\nthis case, four bits starting from the right (i.e., ğ¾=4) are changed\nfrom 1 to 0, and the fifth bit is changed from 0 to 1. The last bit 0\nis the shared prefix . Second, if the binary forms of Fğœ(ğ‘ğ‘–)=16and\nFğœ(ğ‘ğ‘—)=17are01000 0and01000 1, respectively, only the first bit\n(from the right) is changed from 0 to 1, i.e., no bits ( ğ¾=0) are changed\nfrom 1 to 0, and the shared prefix is 01000 .\nWe explain now why the number of directed edges (denoted by\nEğœ(ğ‘)) plus the number of query sections (denoted by Sğœ(ğ‘)) in\na given query ğ‘yields the number of distinct points (denoted by\nV(ğ‘)) inğ‘. The intuition is that if ğ‘consists of a single section\nğ‘ , i.e., the curve stays completely inside ğ‘ andSğœ(ğ‘)=1then\nthere areV(ğ‘)âˆ’1directed edges connecting a given start point ğ‘ğ‘ \nand end point ğ‘ğ‘’ofğ‘ . In other words, we obtain Eğœ(ğ‘)+Sğœ(ğ‘)=\nV(ğ‘)âˆ’1+1=V(ğ‘). This is because each time a curve exits a\nquery section ğ‘ ğ‘–and enters the next section ğ‘ ğ‘–+1, the last point in\nğ‘ ğ‘–becomes disconnected (minus one directed edge) but one new\nquery section is added (plus 1 for the query section) when the curve\nreentersğ‘ ğ‘–+1. This leads to the following equation:\nEğœ(ğ‘)+Sğœ(ğ‘)=V(ğ‘) (3)\nWhileV(ğ‘)is independent of ğœ, the values forEğœ(ğ‘)andSğœ(ğ‘)\ndepend onğœ. For example, in Figure 4a ( ğœ=XYXYXY), there are\n3 query sections (Sğœ(ğ‘)) and 5 directed edges ( Eğœ(ğ‘)) inğ‘; in Fig-\nure 4b (ğœ=YXYXYX), there are 4 query sections and 4 directed\nedges inğ‘. Both figures have V(ğ‘)=8points inğ‘. Equation 3 is\nkey in computing the local cost (Section 4.2) of a query.\n4 EFFICIENT BMC COST ESTIMATION\nConsider a range query ğ‘with start point ğ‘ğ‘ and end point ğ‘ğ‘’and\nassume that dataset ğ·has been indexed with a B+-tree using BMC\nğœ. A simple query algorithm accesses the range [Fğœ(ğ‘ğ‘ ),Fğœ(ğ‘ğ‘’)]\nusing the B+-tree, and filters any false positives not included in ğ‘.\nThe query cost of ğ‘then relates to the length of [Fğœ(ğ‘ğ‘ ),Fğœ(ğ‘ğ‘’)]\nand the number of false positives in the range. The number of false\npositives in turn relates to the number of query sections in ğ‘. Thus,\nwe define the cost of ğ‘(when using BMC ğœ), denoted byCğœ(ğ‘), as\na combination of the length of [Fğœ(ğ‘ğ‘ ),Fğœ(ğ‘ğ‘’)](called the global\ncost,Cğ‘”\nğœ(ğ‘)) and the number of query sections (called the local cost ,\nCğ‘™ğœ(ğ‘)) inğ‘. Empirically, we find that the product of the global and\nthe local costs best differentiates the query performance of different\nBMC indices, which helps identify query-optimal BMC indices (i.e.,\nthe goal of our study). Hence, we define Cğœ(ğ‘)as:\nCğœ(ğ‘)=Cğ‘”\nğœ(ğ‘)Â·Cğ‘™\nğœ(ğ‘) (4)\n4\n\nEfficient Cost Modeling of Space-filling Curves Conferenceâ€™17, July 2017, Washington, DC, USA\nNote that a commonly used alternative query algorithm is to break\nğ‘into query sections and perform a range query on the B+-tree\nfor each such section. In this case, the local cost applies directly.\nThe global cost, on the other hand, applies implicitly, because a\nlarger range of[Fğœ(ğ‘ğ‘ ),Fğœ(ğ‘ğ‘’)]implies a higher cost to examine\nand uncover the query sections in the range.\nNote also that the cost model of QUILTS [ 19] uses the product\nof a global and a local cost. However, its definitions of global and\nlocal costs, described in Section 2 are different from ours.\nNext, we present efficient algorithms for computing the global\nand local costs in Sections 4.1 and 4.2, respectively.\n4.1 Global Cost Estimation for BMC\nAs mentioned above, we define the global cost of query ğ‘as the\nlength of[Fğœ(ğ‘ğ‘ ),Fğœ(ğ‘ğ‘’)].\nDefinition 2 (Global Cost). The global costCğ‘”\nğœ(ğ‘)of queryğ‘\nunder BMCğœis the length of the curve segment from ğ‘ğ‘ toğ‘ğ‘’:\nCğ‘”\nğœ(ğ‘)=Fğœ(ğ‘ğ‘’)âˆ’Fğœ(ğ‘ğ‘ )+1=ğ‘‘âˆ‘ï¸\nğ‘—=1â„“âˆ‘ï¸\nğ‘˜=1(ğ›¼ğ‘˜\nğ‘’,ğ‘—âˆ’ğ›¼ğ‘˜\nğ‘ ,ğ‘—)Â·2ğ›¾ğ‘˜\nğ‘—+1(5)\nEfficient computation. Following the definition, given a set\nğ‘„ofğ‘›queries, their total global cost can be calculated by visiting\nevery query ğ‘âˆˆğ‘„and adding upCğ‘”\nğœ(ğ‘). This naive approach takes\ntime proportional to the number of queries to compute. To reduce\nthe time cost without loss of accuracy, we rewrite the global cost\nas a closed-form function for efficient computation.\nCğ‘”\nğœ(ğ‘„)=ğ‘›âˆ‘ï¸\nğ‘–=1Cğ‘”\nğœ(ğ‘ğ‘–)=ğ‘›âˆ‘ï¸\nğ‘–=1ğ‘‘âˆ‘ï¸\nğ‘—=1â„“âˆ‘ï¸\nğ‘˜=1(ğ›¼ğ‘˜\nğ‘–,ğ‘’,ğ‘—âˆ’ğ›¼ğ‘˜\nğ‘–,ğ‘ ,ğ‘—)\n|             {z             }\nBMC independentÂ· 2ğ›¾ğ‘˜\nğ‘—\n|{z}\nBMC dependent+ğ‘›\n=ğ‘‘âˆ‘ï¸\nğ‘—=1â„“âˆ‘ï¸\nğ‘˜=1ğ‘›âˆ‘ï¸\nğ‘–=1(ğ›¼ğ‘˜\nğ‘–,ğ‘’,ğ‘—âˆ’ğ›¼ğ‘˜\nğ‘–,ğ‘ ,ğ‘—)\n|                  {z                  }\nBMC independentÂ·2ğ›¾ğ‘˜\nğ‘—+ğ‘›=ğ‘‘âˆ‘ï¸\nğ‘—=1â„“âˆ‘ï¸\nğ‘˜=1ğ´ğ‘˜\nğ‘—Â·2ğ›¾ğ‘˜\nğ‘—+ğ‘›(6)\nHere,ğ‘ğ‘–âˆˆğ‘„;ğ›¼ğ‘˜\nğ‘–,ğ‘ ,ğ‘—andğ›¼ğ‘˜\nğ‘–,ğ‘’,ğ‘—denote theğ‘˜th bits of the coordi-\nnates of the lower and the upper end points of ğ‘ğ‘–in dimension ğ‘—,\nrespectively; ğ´ğ‘˜\nğ‘—=Ãğ‘›\nğ‘–=1(ğ›¼ğ‘˜\nğ‘–,ğ‘’,ğ‘—âˆ’ğ›¼ğ‘˜\nğ‘–,ğ‘ ,ğ‘—), which is BMC independent\nand can be calculated once by scanning the ğ‘›range queries in ğ‘„\nto compute the gap between ğ‘ğ‘’andğ‘ğ‘ on theğ‘˜th bit of the ğ‘—th\ndimension, for any BMC. Only the term 2ğ›¾ğ‘˜\nğ‘—is BMC dependent and\nmust be calculated for each curve because ğ›¾ğ‘—\nğ‘–represents the rank\nof theğ‘—th bit from dimension ğ‘–of a BMC (cf. Section 3.1). If the\nBMCğœis changed, e.g., from XYXY XYto XYXY YX, thenğ›¾1\n1=1\nandğ›¾1\n2=0are changed to ğ›¾1\n1=0andğ›¾1\n2=1, respectively.\nAlgorithm costs. The above property helps reduce the cost of\ncomputing the global cost when given multiple candidate BMCs.\nFor example, when learning the best BMC from a large volume of\ncandidate BMCs (see Section 5), each BMC is evaluated individually\nin each iteration (Algorithm 3). Without an efficient cost modeling,\nthe global cost is ğ‘‚(ğ‘šÂ·ğ‘›Â·ğ‘‘Â·â„“)forğ‘šcandidate BMCs over ğ‘›\nqueries (based on Equation 5). Based on our proposed closed form\nmethod (Equation 6), after an initial ğ‘‚(ğ‘›)-time scan over the ğ‘›\nqueries (to compute ğ´ğ‘˜\nğ‘—), the holistic global cost over ğ‘›queries can\nbe calculated in ğ‘‚(ğ‘šÂ·ğ‘‘Â·â„“)time, i.e.,ğ‘‚(ğ‘š)time given constant\nnumber of dimensions ğ‘‘and number of bits â„“in each dimension.4.2 Local Cost Estimation for BMC\nThe local cost measures the degree of segmentation of the curve\nin[Fğœ(ğ‘ğ‘ ),Fğœ(ğ‘ğ‘’)], which indicates the number of false positive\ndata blocks that are retrieved unnecessarily and need to be filtered.\nWe define the local cost as the number of query sections, following\nexisting studies [ 17,32] that use the term â€œ number of clusters â€ for\nthe same concept.\nDefinition 3 (Local Cost). The local costCğ‘™ğœ(ğ‘)of queryğ‘\nunder BMCğœis the number query sections in ğ‘, i.e.,Sğœ(ğ‘).\nIntuition. Recall thatV(ğ‘)is the number of distinct points in ğ‘.\nWe assume one data point per cell and that every ğµdata points are\nstored in a block. A point is a true positive if it (and its cell) is in\nqueryğ‘and a false positive if it is outside ğ‘but is retrieved by the\nquery. Ifğ‘has only one query section, the largest number of block\naccesses isâŒŠ(V(ğ‘)âˆ’2)/ğµâŒ‹+2, i.e., only the first and last blocks can\ncontain false positives (at least one true positive point in each block).\nIn this case, the precision of the query process is at leastV(ğ‘)\nV(ğ‘)+2Â·(ğµâˆ’1).\nFollowing the same logic, if there are ğ‘›ğ‘ query sections, in the worst\ncase, each query section incurs two excess block accesses, each for\na block containing only one true positive point. The largest number\nof block accesses is âŒŠ(V(ğ‘)âˆ’2Â·ğ‘›ğ‘ )/ğµâŒ‹+2Â·ğ‘›ğ‘ , and the precision is\nV(ğ‘)\nV(ğ‘)+2Â·ğ‘›ğ‘ Â·(ğµâˆ’1). The excess block accesses grows linearly with ğ‘›ğ‘ .\nThus, we use ğ‘›ğ‘ to define the local cost.\nquery start query endTrue positive False positiveOne query  \nsection under\nThree query  \nsections under\nFigure 5: Query sections vs. block accesses\nExample 3. In Figure 5, we order points based on BMCs ğœ1andğœ2\nand place the points in blocks where ğµ=4. There are 14 true positives\n(i.e.,V(ğ‘)=14). There is only one query section under ğœ1, which leads\nto a precision of14\n5Ã—4=70% for 5 block accesses, whereas ğœ2has three\nquery sections (due to a different curve). The number of block accesses\nis 7, and the precision drops to14\n7Ã—4=50%.\nEfficient computation. A simple way to compute the local\ncost of an arbitrary range query is to count the number of query\nsections by traversing the curve segment from ğ‘ğ‘ toğ‘ğ‘’, but this is\nalso time-consuming. To reduce the cost, we rewrite Equation 3 as:\nSğœ(ğ‘)=V(ğ‘)âˆ’Eğœ(ğ‘) (7)\nGiven a query ğ‘and the grid resolution of the data space, it is\nstraightforward (i.e., taking ğ‘‚(ğ‘‘)=ğ‘‚(1)time) to compute the\nnumber of cells in ğ‘(i.e.,V(ğ‘)). Then, our key insight is thatSğœ(ğ‘)\ncan be computed by counting the number of directed edges, i.e.,\nEğœ(ğ‘), which can be done efficiently in ğ‘‚(1)time as detailed below.\nThus,Sğœ(ğ‘)can be computed in ğ‘‚(1)time.\n4.2.1 Rise and Drop Patterns. To computeEğœ(ğ‘)efficiently, we\nanalyse how the bit sequence of a BMC changes from one point\nto another following a directed edge. A directed edge is formed by\ntwo consecutive points with (binary) curve values that share the\nsame prefix , while the remaining bits are changed. We observe that\ndifferent directed edges have the same shape when they share the\nsame pattern in their changed bits, even if their prefixes are different.\n5\n\nConferenceâ€™17, July 2017, Washington, DC, USA et al.\nIn Figure 6a, consider edges ğ‘’1=(5,6)=[0001 01,0001 10]and\nğ‘’2=(13,14)=[0011 01,0011 10]. Both edges are in query ğ‘as\nindicated by the red rectangle, and they share the same â€˜\\â€™ shape\nbecause the two rightmost bits in both cases change from â€œ01â€ to\nâ€œ10â€. However, in Figure 6a, edge (1,2)=[0000 01,0000 10]is not\ninğ‘, and the prefix (â€œ0000â€) differs from that of ğ‘’1andğ‘’2above.\nA queryğ‘can only contain directed edges of a few different\nshapes. In Figure 6a, edge (31,32)=[011111,100000]is not inğ‘,\nand the pattern of the changed bits differs from that of ğ‘’1andğ‘’2.\nNote that the bits of the curve values come from the coordinates\n(i.e., column indices) of the two end points of a directed edge. By\nanalyzing the bit patterns of the column indices spanned by a query\nğ‘in each dimension, we can count the number of directed edges\nthat can appear in ğ‘.\nTo generalize, recall that given a directed edge from ğ‘ğ‘–toğ‘ğ‘—,\nFğœ(ğ‘ğ‘–)=...|{z}\nprefix01...1|{z}\nğ¾1sandFğœ(ğ‘ğ‘—)=...|{z}\nprefix10...0|{z}\nğ¾0s(ğ¾â‰¥0) must exist\nwhere theğ¾rightmost bits are changed from 1 to 0, while the\n(ğ¾+1)st rightmost bit is changed from 0 to 1. The bits of Fğœ(ğ‘ğ‘–)\nandFğœ(ğ‘ğ‘—)come from those of the column indices of ğ‘ğ‘–andğ‘ğ‘—.\nThus, theğ¾+1rightmost bits changed from Fğœ(ğ‘ğ‘–)toFğœ(ğ‘ğ‘—)must\nalso come from those of the column indices. In particular, there\nmust be one dimension, where the column index has contributed ğ‘˜\n(1â‰¤ğ‘˜â‰¤ğ¾) changed bits and one of the bits has changed from 0 to\n1, while the rest dimensions contribute bits changing from 1 to 0.\nOur key observation is that the bit-changing patterns across the\ncolumn indices in a dimension only depend on the column indices\nthemselves, making them BMC independent . By pre-computing the\nnumber of bit-changing patterns that can form the (ğ¾+1)-bit\nchange of a directed edge, we can derive efficiently the number of\ndirected edges given a query ğ‘and a BMC.\nWe summarize the bit-changing patterns to form a directed edge\nwith two basic patterns: a rise pattern and a drop pattern .\nDefinition 4 (Rise Pattern). A rise patternRğ‘˜\nğ‘of a directed edge\nfromğ‘ğ‘–toğ‘ğ‘—represents a ğ‘˜-bit (ğ‘˜â‰¥1) change in the dimension- ğ‘\ncoordinate of ğ‘ğ‘–(i.e.,ğ‘¥ğ‘–,ğ‘) to that ofğ‘ğ‘—(i.e.,ğ‘¥ğ‘—,ğ‘), where the rightmost\nğ‘˜âˆ’1bits are changed from 1 to 0 and the ğ‘˜th bit (from the right) is\nchanged from 0 to 1, i.e., ğ‘¥ğ‘–,ğ‘=...|{z}\nprefix01...1|{z}\n(ğ‘˜âˆ’1)1sandğ‘¥ğ‘—,ğ‘=...|{z}\nprefix10...0|{z}\n(ğ‘˜âˆ’1)0s.\nDefinition 5 (Drop Pattern). A drop patternDğ‘˜\nğ‘of a directed\nedge fromğ‘ğ‘–toğ‘ğ‘—represents a rightmost ğ‘˜-bit (ğ‘˜â‰¥0) 1-to-0 change\nin the dimension- ğ‘coordinate of ğ‘ğ‘–(i.e.,ğ‘¥ğ‘–,ğ‘) to that ofğ‘ğ‘—(i.e.,ğ‘¥ğ‘—,ğ‘),\ni.e.,ğ‘¥ğ‘–,ğ‘=...|{z}\nprefix1...1|{z}\nğ‘˜1sandğ‘¥ğ‘—,ğ‘=...|{z}\nprefix0...0|{z}\nğ‘˜0s.\nGiven a dimension where the coordinates use â„“bits, there can\nbeâ„“different rise patterns, i.e., ğ‘˜âˆˆ[1,â„“], and there can be â„“+1\ndifferent drop patterns, i.e., ğ‘˜âˆˆ[0,â„“]. Note the special case where\nğ‘˜=0, i.e.,D0\nğ‘, indicating no bit value drop in dimension ğ‘.\nExample 4. In Figure 6a, consider the directed edge from ğ‘ğ‘–to\nğ‘ğ‘—, whereFğœ(ğ‘ğ‘–)=1(000001) andFğœ(ğ‘ğ‘—)=2(000010), i.e., the â€˜ \\â€™\nsegment at the bottom left. The ğ‘¥-coordinate of ğ‘ğ‘–changes from 000to\n001to that ofğ‘ğ‘—(i.e., rise patternR1ğ‘¥). Theğ‘¦-coordinate of ğ‘ğ‘–changes\nfrom 001 to 000 to that of ğ‘ğ‘—(i.e., drop pattern D1ğ‘¦). Thus, this directed\nedge can be represented by a combination of R1ğ‘¥andD1ğ‘¦, denoted as\nR1ğ‘¥âŠ•D1ğ‘¦. This same combination also applies in other directed edges,\n(a) Rise pattern in dimension ğ‘¥and\ndrop pattern in dimension ğ‘¦.\n(b) Rise pattern in dimension ğ‘¦and\ndrop pattern in dimension ğ‘¥.\nFigure 6: Example of forming a directed edge with rise and\ndrop patterns: for BMC XYXYXY ( ğ‘‘=2andâ„“=3), each\ndirected edge is formulated by a rise and a drop pattern.\nsuch as that from Fğœ(ğ‘ğ‘–)=13toFğœ(ğ‘ğ‘—)=14, which is another â€˜\\â€™-\nshaped segment. Other directed edges may use a different combination,\ne.g.,R3ğ‘¥âŠ•D3ğ‘¦for the one fromFğœ(ğ‘ğ‘–)=31toFğœ(ğ‘ğ‘—)=32, andR2ğ‘¥âŠ•D2ğ‘¦\nfor the one fromFğœ(ğ‘ğ‘–)=39toFğœ(ğ‘ğ‘—)=40.\nFigure 6a has shown the rise patterns Rğ‘˜ğ‘¥in dimension- ğ‘¥and the\ndrop patternsDğ‘˜ğ‘¦in dimension- ğ‘¦. Combining a rise and a drop pattern\nfrom these patterns forms a directed edge in red in the figure.\nSimilarly, we show in Figure 6b the rise patterns Rğ‘˜ğ‘¦in dimension- ğ‘¦\nand the drop patterns Dğ‘˜ğ‘¥in dimension- ğ‘¥. Combining a rise and a\ndrop pattern from these patterns forms a black directed edge.\nThe pattern combination operator â€˜âŠ•â€™ applied on two (rise or\ndrop) patterns means that the (ğ¾+1)-bit change of a directed edge\nis formed by the two patterns.\nNote also that while the rise and the drop patterns on a dimension\nare BMC independent, which ones that can be combined to form a\ndirected edge is BMC dependent because different BMCs order the\nbits from different dimensions differently. For example, consider\nğœ=X3Y3X2Y2X1Y1(i.e., XYXYXY). From the right to the left of\nğœ, the first rise pattern is R1ğ‘¥. It can only be combined with drop\npatternD1ğ‘¦, as there is just one bit ğ‘Œ1from dimension- ğ‘¦to the right\nofğ‘‹1. Similarly,R2ğ‘¥andR3ğ‘¥can each be combined with D2ğ‘¦andD3ğ‘¦,\nrespectively, i.e., all 1-bits to the right of ğ‘‹2andğ‘‹3must be changed\nto 0, according to the bit-changing pattern of a directed edge. In\ngeneral, for each dimension, there are only â„“valid combinations of\na rise and a drop pattern, and this number generalizes to ğ‘‘Â·â„“in a\nğ‘‘-dimensional space given a BMC.\nNext,Eğœ(ğ‘)can be calculated by counting the number of valid\nrise and drop patterns in ğ‘. For example, when ğ‘‘=2:\nEğœ(ğ‘)=â„“âˆ‘ï¸\nğ‘–=1\u0010\nN(Rğ‘–\nğ‘¥)Â·N(Dğ‘Ÿğ‘¦\nğ‘¦)+N(Rğ‘–\nğ‘¦)Â·N(Dğ‘Ÿğ‘¥ğ‘¥)\u0011\n(8)\nHere,N(Â·) counts the number of times that a pattern occurs in ğ‘,\nandğ‘Ÿğ‘¥(ğ‘Ÿğ‘¦) is a parameter depending on the drop patterns that can\nbe combined with Rğ‘–ğ‘¥(Rğ‘–ğ‘¦). In Figure 6, for ğ‘=([0,4]Ã—[ 2,3]),\nthere are twoR1ğ‘¥, oneR2ğ‘¥, and oneR3ğ‘¥, i.e.,N(R1ğ‘¥)=2,N(R2ğ‘¥)=1,\nandN(R3ğ‘¥)=1. Next, there is one D1ğ‘¦, zeroD2ğ‘¦, and zeroD3ğ‘¦\nthat are valid to match with these rise patterns, i.e., N(D1ğ‘¦)=1,\nN(D2ğ‘¦)=0, andN(D3ğ‘¦)=0. Similarly,N(R1ğ‘¦)=1, andR1ğ‘¦can\nbe matched withD0ğ‘¥, whereN(D0ğ‘¥)=5. Recall thatD0ğ‘¥is the\n6\n\nEfficient Cost Modeling of Space-filling Curves Conferenceâ€™17, July 2017, Washington, DC, USA\nspecial case with no bit value drop. It is counted as the length of the\nquery range in dimension ğ‘¥. Overall,Eğœ(ğ‘)=2Ã—1+1Ã—5. Thus,\nthere are 10âˆ’7=3query sections in ğ‘according to Equation 7,\nwhich is consistent with the figure.\nEfficient counting of rise and drop patterns. A rise pattern\nRğ‘˜\nğ‘represents a change in the dimension- ğ‘coordinate from ğ‘¥ğ‘–,ğ‘=\nğ‘Â·2ğ‘˜+(2ğ‘˜âˆ’1âˆ’1)toğ‘¥ğ‘—,ğ‘=ğ‘Â·2ğ‘˜+2ğ‘˜âˆ’1(ğ‘â‰¥0âˆ§ğ‘âˆˆN). Here,ğ‘Â·2ğ‘˜\nis the prefix, while 2ğ‘˜âˆ’1âˆ’1(i.e., 01...1|{z}\n(ğ‘˜âˆ’1)1s) and 2ğ‘˜âˆ’1(i.e., 10...0|{z}\n(ğ‘˜âˆ’1)0s)\nrepresent the changed bits. Then, given the data domain [ğ‘¥ğ‘ ,ğ‘,ğ‘¥ğ‘’,ğ‘]\nof dimension ğ‘, each pattern can be counted by calculating âŒŠ(ğ‘¥ğ‘’,ğ‘âˆ’\n2ğ‘˜âˆ’1)/2ğ‘˜âŒ‹âˆ’âŒˆ(ğ‘¥ğ‘ ,ğ‘âˆ’(2ğ‘˜âˆ’1âˆ’1))/2ğ‘˜âŒ‰+1, i.e., a bound on the different\nvalues ofğ‘, which takes ğ‘‚(1)time.\nSimilarly, a drop pattern Dğ‘˜\nğ‘represents a change from ğ‘¥ğ‘–,ğ‘=\nğ‘Â·2ğ‘˜+2ğ‘˜âˆ’1toğ‘¥ğ‘—,ğ‘=ğ‘Â·2ğ‘˜+0(ğ‘â‰¥0âˆ§ğ‘âˆˆN). Here,ğ‘Â·2ğ‘˜is the prefix,\nwhile 2ğ‘˜âˆ’1(i.e., 1...1|{z}\nğ‘˜1s) and 0(i.e., 0...0|{z}\nğ‘˜0s) represent the changed bits.\nWe can count each pattern by calculating âŒŠ(ğ‘¥ğ‘’,ğ‘+1)/2ğ‘˜âŒ‹âˆ’âŒˆğ‘¥ğ‘ ,ğ‘/2ğ‘˜âŒ‰,\nagain inğ‘‚(1)time.\nGeneralizing to ğ‘‘dimensions. As mentioned at the beginning\nof the subsection, a directed edge can be decomposed into a rise\npattern in one dimension and drop patterns in the remaining ğ‘‘âˆ’\n1dimensions. We call the set of all drop patterns in the ğ‘‘âˆ’1\ndimensions a drop pattern collection .\nDefinition 6 (Drop Pattern Collection). For a directed edge\ninğ‘‘-dimensional space, a drop pattern collection Dğ‘˜â€²represents the\nbit combination over ğ‘‘âˆ’1drop patterns:DÃğ‘‘âˆ’1\nğ‘–=1,ğ‘–â‰ ğ‘ğ‘˜ğ‘–=Ã’ğ‘‘\nğ‘–=1,ğ‘–â‰ ğ‘Dğ‘˜ğ‘–\nğ‘–\n(ğ‘˜â€²=Ãğ‘‘\nğ‘–=1,ğ‘–â‰ ğ‘ğ‘˜ğ‘–=ğ¾âˆ’ğ‘˜), whereğ‘is the dimension with a rise\npattern. Here, â€˜Ã’â€™ is a pattern combination operator (like âŠ•above).\nWe note thatDğ‘˜â€²andDğ‘˜\nğ‘are interchangeable if ğ‘‘=2. For simplicity,\nwe callDğ‘˜â€²a drop pattern when the context eliminates any ambiguity.\nNow, in ağ‘‘-dimensional data space, a directed edge can be\nformed by combining one rise pattern and ğ‘‘âˆ’1drop patterns,\ni.e.,Rğ‘˜\nğ‘âŠ•DÃğ‘‘\nğ‘–=1,ğ‘–â‰ ğ‘ğ‘˜ğ‘–=Rğ‘˜\nğ‘âŠ•(Ã’ğ‘‘\nğ‘–=1,ğ‘–â‰ ğ‘Dğ‘˜ğ‘–\nğ‘–)whereğ‘˜â€²=Ãğ‘‘\nğ‘–=1,ğ‘–â‰ ğ‘ğ‘˜ğ‘–.\nEquation 8 is then rewritten as:\nEğœ(ğ‘)=ğ‘‘âˆ‘ï¸\nğ‘—=1â„“âˆ‘ï¸\nğ‘–=1N(Rğ‘–\nğ‘—)Â·N(Dğ‘Ÿ) (9)\nHere, the value of parameter ğ‘Ÿdepends on the number of drop\npatterns that can be combined with Rğ‘–\nğ‘—.\n4.2.2 Pattern Tables. We have shown how to compute the local\ncost of a query efficiently. Given a set ğ‘„ofğ‘›range queries ( ğ‘ğ‘–âˆˆğ‘„),\ntheir total local cost based on Definition 3 is:\nCğ‘™\nğœ(ğ‘„)=ğ‘›âˆ‘ï¸\nğ‘–=1Cğ‘™\nğœ(ğ‘ğ‘–)=ğ‘›âˆ‘ï¸\nğ‘–=1V(ğ‘ğ‘–)âˆ’ğ‘›âˆ‘ï¸\nğ‘–=1Eğœ(ğ‘ğ‘–) (10)\nThis cost takes ğ‘‚(ğ‘›)time to compute. Given ğ‘šBMCs, comput-\ning their respective total local costs Cğ‘™ğœ(ğ‘„)takesğ‘‚(ğ‘šÂ·ğ‘›)time.\nAsÃğ‘›\nğ‘–=1V(ğ‘ğ‘–)is independent of the BMCs, it can be computed\nonce by performing an ğ‘‚(ğ‘›)-time scan over ğ‘„. The computational\nbottleneck for ğ‘šBMCs is then the computation ofÃğ‘›\nğ‘–=1Eğœ(ğ‘ğ‘–).\nWe eliminate this bottleneck by introducing a look-up table\ncalled a pattern table that stores pre-computed numbers of rise-and-\ndrop pattern combinations to form the directed edges at differentlocations, which are BMC-independent. Since each directed edge is\na combination of a rise pattern in some dimension ğ‘andğ‘‘âˆ’1drop\npatterns, we proceed to show how to pre-compute ğ‘‘pattern tables,\neach recording the rise patterns of a dimension.\nTable 2: Pattern table Tableğ‘for dimension ğ‘usingâ„“bits on\neach dimension.\nD0D1Â·Â·Â·Dâ„“Â·(ğ‘‘âˆ’1)\nR1\nğ‘N(R1\nğ‘)Â·N(D0)N(R1\nğ‘)Â·N(D1)Â·Â·Â·N(R1\nğ‘)Â·N(Dâ„“Â·(ğ‘‘âˆ’1))\nR2\nğ‘N(R2\nğ‘)Â·N(D1)N(R2\nğ‘)Â·N(D2)Â·Â·Â·N(R2\nğ‘)Â·N(Dâ„“Â·(ğ‘‘âˆ’1))\nÂ·Â·Â·Â·Â·Â· Â·Â·Â·Â·Â·Â·Â·Â·Â·\nRâ„“\nğ‘N(Râ„“\nğ‘)Â·N(D0)N(Râ„“\nğ‘)Â·N(D1)Â·Â·Â·N(Râ„“\nğ‘)Â·N(Dâ„“Â·(ğ‘‘âˆ’1))\nDefinition 7 (Pattern Table). The pattern table for dimension\nğ‘, denoted by Tableğ‘, containsâ„“rows, each corresponding to a rise\npattern in the dimension, and â„“Â·(ğ‘‘âˆ’1)+1columns, each corresponding\nto a drop pattern in the other ğ‘‘âˆ’1dimensions. As shown in Table 2,\nthe value in row ğ‘–and column ğ‘—is the product of the numbers of rise\npatternRğ‘–\nğ‘and drop patternDğ‘—.\nThere is a total of â„“Â·(ğ‘‘âˆ’1)+1drop patterns in the ğ‘‘âˆ’1\ndimensions because there are â„“Â·(ğ‘‘âˆ’1)bits in those dimensions, i.e.,\nğ‘˜â€²âˆˆ[0,â„“Â·(ğ‘‘âˆ’1)]forDğ‘˜â€². Further, since the rise and drop patterns\ncorrespond to only the bit sequences in each dimension and not\nthe curve values, the values in the pattern tables can be computed\nonce given a set of queries ğ‘„and can then be reused across local\ncost estimation for different BMCs. Algorithm 1 summarizes the\nsteps to compute pattern table Tableğ‘based on its definition.\nAlgorithm 1: Generate pattern table (GPT)\nInput: Query setğ‘„, target dimension ğ‘, data dimensionality ğ‘‘,\nnumber of bits per dimension â„“\nOutput: Pattern table Tableğ‘\n1Initialize anâ„“Ã—(â„“Â·(ğ‘‘âˆ’1)+1)table Tableğ‘;\n2forğ‘âˆˆğ‘„do\n3 forğ‘–âˆˆ[1,â„“]do\n4 forğ‘—âˆˆ[0,â„“Â·(ğ‘‘âˆ’1)]do\n5N(Rğ‘–\nğ‘)â† count the number of Rğ‘–\nğ‘inğ‘;\n6N(Dğ‘—)â† count the number of Dğ‘—inğ‘;\n7 Tableğ‘[ğ‘–][ğ‘—]â† Tableğ‘[ğ‘–][ğ‘—]+N(Rğ‘–\nğ‘)Â·N(Dğ‘—);\n8return Tableğ‘;\nExample 5. In Figure 7a, we show two queries ğ‘1andğ‘2, and the\npattern tables Tableğ‘¥andTableğ‘¦are shown in Tables 3 and 4, respec-\ntively. In the tables, we use â€˜ +â€™ to denote summing up the pattern table\ncell values (i.e.,N(Rğ‘–\nğ‘)Â·N(Dğ‘—), andN(Dğ‘—)isN(Dğ‘—\nğ‘¥)orN(Dğ‘—\nğ‘¦))\ncomputed for ğ‘1andğ‘2. For example, in ğ‘1,N(R1ğ‘¥)=2(the twoR1ğ‘¥\nare labeled for ğ‘1in Figure 7a) and N(D0ğ‘¦)=2(the value range of ğ‘1\nin dimension ğ‘¦is 2). Meanwhile, in ğ‘2,N(R1ğ‘¥)=1(oneR1ğ‘¥is labeled\nforğ‘2in Figure 7a) and N(D0ğ‘¦)=3(the value range of ğ‘2in dimen-\nsionğ‘¦is 3). Thus, in Tableğ‘¥, the cell Tableğ‘¥[1][0](corresponding to\nR1ğ‘¥âŠ•D0ğ‘¦) is the sum ofN(R1ğ‘¥)Â·N(D0ğ‘¦)inğ‘1andğ‘2, i.e., 4+3.\nTable 3: Tableğ‘¥\nD0ğ‘¦D1ğ‘¦D2ğ‘¦D3ğ‘¦\nR1ğ‘¥4 + 3:0:+:10 + 0 0 + 0\nR2ğ‘¥2 + 0 0 + 0:0:+:00 + 0\nR3ğ‘¥2 + 3 0 + 1 0 + 0:0:+:0Table 4: Tableğ‘¦\nD0ğ‘¥D1ğ‘¥D2ğ‘¥D3ğ‘¥\nR1ğ‘¦:0:+:30 + 1 0 + 0 0 + 0\nR2ğ‘¦5 + 0:2:+:01 + 0 0 + 0\nR3ğ‘¦0 + 3 0 + 1:0:+:00 + 0\n7\n\nConferenceâ€™17, July 2017, Washington, DC, USA et al.\nxy\nR3\nxR1\nxR3\nyR1\ny\nNq2(D0\ny)=3\nNq2(D1\ny)=1Nq2(D0\nx)=3Nq2(D1\nx)=1\nR1\nxR2\nxR1\nxR3\nxR2\ny\nNq1(D0\ny)=2Nq1(D0\nx)=5Nq1(D1\nx)=2Nq1(D2\nx)=1\nq1q2\n(a) Six directed edges ( ğœ=XYXYXY)\nxy\nq1q2 (b) Nine directed edges ( ğœ=YXYXYX)\nFigure 7: Rise and drop pattern counting example ( ğ‘‘=2,â„“=3).\nThe results are shown in pattern tables in Tables 3 and 4.\n4.2.3 Local Cost Estimation with Pattern Tables. Next, we describe\nhow to derive the number of directed edges (and hence compute\nthe total local cost) given the ğ‘‘pattern tables for ğ‘›queries.\nAlgorithm 2 shows how to compute the local cost using the\npattern tables. Each dimension ğ‘—is considered for the rise patterns\n(Line 2). Then, we consider each rise pattern in the dimension, i.e.,\neach rowğ‘–inTableğ‘—(Line 3). We locate the corresponding drop\npattern (i.e., the table column index) based on ğ‘–and a given BMC ğœ,\nwhich is done by the get_col function (Line 4). Then, we add the\ncell value to the number of directed edges Eğœ(Line 5). Note that\nallâ„“rise patterns in each dimension are considered because a BMC\nhasâ„“bits on each dimension, which can all be the bit that changes\nfrom 0 to 1. We return the total local cost by subtracting the total\nnumber of directed edges from the total number of cells in ğ‘„.\nAlgorithm 2: Compute local cost with pattern tables\nInput: BMCğœ, data dimensionality ğ‘‘, number of bits per\ndimensionâ„“, all pattern tables Tableğ‘—, total number of cells\nin the queriesV\nOutput: Total local cost of ğ‘›queries\n1Eğœâ†0;\n2forğ‘—âˆˆ[1,ğ‘‘]do\n3 forğ‘–âˆˆ[1,â„“]do\n4ğ‘ğ‘œğ‘™â†get_col(ğœ,ğ‘–,ğ‘—);\n5Eğœâ†Eğœ+Tableğ‘—[ğ‘–][ğ‘ğ‘œğ‘™];\n6returnVâˆ’Eğœ;\nExample 6. Based on Example 5, given BMC XYXYXY, from Tableğ‘¥,\nwe read cells (R1\n1,D1\n2), (R2\n1,D2\n2), and (R3\n1,D3\n2), i.e., the cells with â€œwavyâ€\nlines. Similarly, we read the cells with â€œwavyâ€ lines from Tableğ‘¦. These\ncells sum up to 6, which is the number of directed edges (segments with\narrows) in Figure 7a. Similarly, the cells relevant to BMC YXYXYX are\nunderlined, which yields a total of nine directed edges in Figure 7b.\nAlgorithm costs. In general, for each rise pattern, the total\nnumber of possible drop pattern combinations is (â„“+1)ğ‘‘âˆ’1based on\nDefinition 6. The time complexity of generating the ğ‘‘pattern tables\nisğ‘‚(ğ‘‘Â·â„“Â·(â„“+1)ğ‘‘âˆ’1), whereğ‘‘denotes the number of dimensions, â„“\ndenotes the number of rows, and (â„“+1)ğ‘‘âˆ’1denotes the accumulated\nnumber of drop patterns (equal to (â„“+1)whenğ‘‘=2). After\ninitialization, the retrieval time complexity of pattern tables is ğ‘‚(ğ‘‘Â·\nâ„“)=ğ‘‚(1), i.e., we retrieve â„“cells from each table.\nWe generate ğ‘‘pattern tables, each with â„“Â·(â„“+1)ğ‘‘âˆ’1keys. Thus,\nthe space complexity for the pattern tables is ğ‘‚(ğ‘‘Â·â„“Â·(â„“+1)ğ‘‘âˆ’1).\nFor example, when ğ‘‘=3andâ„“=32, all the tables take 1.6 MB (1.2\nMB for keys and 0.4 MB for values).5COST ESTIMATION-BASED BMC LEARNING\nNext, powered by our efficient cost estimations, we aim to find the\noptimal BMC ğœğ‘œğ‘ğ‘¡that minimizes the costs of a set of queries ğ‘„\non a dataset ğ·. While using BMCs reduces the number of curve\ncandidates from (2â„“)ğ‘‘!to(ğ‘‘Â·â„“)!\n(â„“!)ğ‘‘(Section 1), it is still non-trivial to\nfind the optimal BMC from the(ğ‘‘Â·â„“)!\n(â„“!)ğ‘‘candidates. We present an\nefficient learning-based algorithm named LBMC for this search.\nProblem transformation . Starting from any random BMC ğœ,\nthe process to search for ğœğ‘œğ‘ğ‘¡can be seen as a bit-swapping process,\nuntil every bit falls into its optimal position, assuming an oracle to\nguide the bit-swapping process.\nTo reduce the search space, we impose two constraints on the bit\nswaps: (a) we only swap two adjacent bits each time, and (b) two bits\nfrom the same dimension cannot be swapped (which guarantees\nvalid BMCs after swaps, cf. Section 3.1). Any bit then takes at most\n(ğ‘‘âˆ’1)Â·â„“swaps to reach its optimal position, when such a position\nis known. Given ğ‘‘Â·â„“bits, at most ğ‘‘Â·(ğ‘‘âˆ’1)Â·â„“2swaps are needed\nto achieve the optimal BMC guided by an oracle.\nIn practice, an ideal oracle is unavailable. Now the problem\nbecomes how to run the bit swaps without an ideal oracle. There\nare two approaches: (a) run a random swap (i.e., exploration ) each\ntime and keep the result if it reduces the query cost, and (b) select\na position that leads to the largest query cost reduction each time\n(i.e., exploitation ). Using either approach yields local optima. We\nintegrate both approaches by leveraging deep reinforcement learning\n(DRL) to approach a global optimum, since DRL aims to maximize a\nlong-term objective [ 14] and balance exploration and exploitation.\nBMC learning formulation. We formulate BMC learning as a\nDRL problem: (1) State space S, where a state (i.e., a BMC) ğœğ‘¡âˆˆS\nat time step ğ‘¡is a vectorâŸ¨ğœğ‘¡[ğ‘‘Â·â„“],ğœğ‘¡[ğ‘‘Â·â„“âˆ’1],...,ğœğ‘¡[1]âŸ©, and\nğœğ‘¡[ğ‘–]is theğ‘–th bit. For example, if ğœğ‘¡=XYZ,ğœğ‘¡[3]=X,ğœğ‘¡[2]=Y, and\nğœğ‘¡[1]=Z. (2) Encoding function ğœ™(Â·), which encodes a BMC to fit\nthe model input. We use one-hot encoding. For example, X, Y, and\nZ can be encoded into [0,0,1],[0,1,0], and[1,0,0], respectively,\nand XYZ by[0,0,1,0,1,0,1,0,0]. (3) Action space A, where an\nactionğ‘âˆˆA is the position of a bit to swap. When the ğ‘th bit is\nchosen, we swap it with the (ğ‘+1)st bit (ifğ‘+1â‰¤ğ‘‘Â·â„“). Thus,\nA={ğ‘âˆˆZ: 1â‰¤ğ‘â‰¤ğ‘‘Â·â„“âˆ’1}. (4) Reward ğ‘Ÿ:SÃ—AÃ—Sâ†’ ğ‘Ÿ, which\nis the query cost reduction when reaching a new BMC ğœğ‘¡+1fromğœğ‘¡.\nSince an oracle is unavailable, we use our cost model to estimate\nthe query cost of a BMC. The reward ğ‘Ÿğ‘¡at stepğ‘¡is calculated as\nğ‘Ÿğ‘¡=(Cğœğ‘¡âˆ’Cğœğ‘¡+1)/Cğœ1, whereCğœğ‘¡=Cğ‘”\nğœğ‘¡(ğ‘„)Â·Cğ‘™ğœğ‘¡(ğ‘„)is the cost\nofğœğ‘¡estimated by Equation 6 and Algorithm 2. (5) Parameter ğœ–,\nwhich balances exploration and exploitation to avoid local optima.\nBased on this formulation, we use deep Q-learning [16] in our\nLBMC algorithm to learn a query-efficient BMC index.\nThe LBMC algorithm. We summarize LBMC in Algorithm 3\nwhere the input ğœ1can be any initial BMC, e.g., a ZC. The key\nidea of LBMC is to learn a policy ğœ‹:S â†’A that guides the\nposition selection for a bit swap given a status, to maximize a value\nfunction Qâˆ—(ğœ™(ğœğ‘¡),ğ‘)(i.e., the reward) at each step ğ‘¡. Such a policy\nğœ‹can be learned by training a model (a deep Q-network , DQN) with\nparametersğœƒover existing â€œ experience â€ (previously observed state\ntransitions and their rewards), which is used to predict the position\nğ‘to maximize the value function (i.e., maxğ‘Qâˆ—(ğœ™(ğœğ‘¡),ğ‘;ğœƒ)). After a\n8\n\nEfficient Cost Modeling of Space-filling Curves Conferenceâ€™17, July 2017, Washington, DC, USA\nnumber of iterations, the learned BMC ğœâˆ—\nğ‘œğ‘ğ‘¡is expected to approach\nğœğ‘œğ‘ğ‘¡, which is returned as the algorithm output.\nWe initialize a storage ğ‘€ğ‘„to store the latest ğ‘ğ‘€ğ‘„bit-swapping\nrecords (i.e., the experience, Line 1). We learn to approach ğœğ‘œğ‘ğ‘¡\nwithğ‘€episodes and ğ‘‡steps per episode (Lines 2 and 3). In each\nepisode, we start with ğœ1encoded by ğœ™(Â·). To select a swap po-\nsitionğ‘ğ‘¡at stepğ‘¡, we generate a random number in [0,1], if the\nnumber is greater than ğœ–, we randomly select a position ğ‘ğ‘¡, oth-\nerwise, we set ğ‘ğ‘¡as the position with the highest probability to\nobtain a maximal reward, i.e., maxğ‘Qâˆ—(ğœ™(ğœğ‘¡),ğ‘;ğœƒ)(Line 4). The pre-\ndiction is based on the current state ğœğ‘¡and model weights ğœƒ. We\nexecuteğ‘ğ‘¡(E(ğœğ‘¡,ğ‘ğ‘¡)at Line 5) and compute reward ğ‘Ÿğ‘¡using our\ncost model (Line 6). We record the new transition in ğ‘€ğ‘„and train\nthe DQN (i.e., update ğœƒ) over sampled data in ğ‘€ğ‘„ (Lines 7 and\n8). The training uses gradient descent to minimize a loss function\nğ¿ğ‘¡(ğœƒğ‘¡)=Eğœ™(ğœ),ğ‘âˆ¼ğœŒ(Â·)\u0002\n(ğ‘¦ğ‘¡âˆ’ğ‘„(ğœ™(ğœ),ğ‘;ğœƒğ‘¡))2\u0003whereğ‘¦ğ‘¡is the target\nfrom iteration ğ‘¡andğœŒ(Â·)is the action distribution [ 16]. We useğœâˆ—\nğ‘œğ‘ğ‘¡\nto record the new BMC from each swap (Line 9), which is returned\nin the end (Line 10).\nAlgorithm 3: Learn BMC (LBMC)\nInput: Initial BMCğœ1\nOutput: A query-efficient BMC ğœâˆ—\nğ‘œğ‘ğ‘¡\n1Initialize replay memory ğ‘€ğ‘„ with capacity ğ‘ğ‘€ğ‘„;\n2forğ‘’ğ‘ğ‘–ğ‘ ğ‘œğ‘‘ğ‘’âˆˆ[1,ğ‘€]do\n3 forğ‘¡âˆˆ[1,ğ‘‡]do\n4 With probability ğœ–select a random position ğ‘ğ‘¡, or\nğ‘ğ‘¡â†maxğ‘Qâˆ—(ğœ™(ğœğ‘¡),ğ‘;ğœƒ);\n5ğœğ‘¡+1â†E(ğœğ‘¡,ğ‘ğ‘¡);\n6 Compute reward ğ‘Ÿğ‘¡;\n7 Store transition(ğœ™(ğœğ‘¡),ğ‘ğ‘¡,ğ‘Ÿğ‘¡,ğœ™(ğœğ‘¡+1))inğ‘€ğ‘„;\n8 Train model ğœƒwith sampled transitions from ğ‘€ğ‘„;\n9ğœâˆ—\nğ‘œğ‘ğ‘¡â†ğœğ‘¡+1;\n10returnğœâˆ—\nğ‘œğ‘ğ‘¡;\nxy\nCg(q2)=7\nCl(q2)=2Cg(q1)=8\nCl(q1)=1Cg(q3)=3\nCl(q3)=2\nq1\nq2q3\n(a) YXXYYX,C1=175\nxy\nCg(q2)=7\nCl(q2)=2Cg(q1)=8\nCl(q1)=1Cg(q3)=3\nCl(q3)=2\nq1\nq2q3 (b) YXYXYX,C2=90\nxy\nCg(q2)=6\nCl(q2)=1Cg(q1)=8\nCl(q1)=1Cg(q3)=2\nCl(q3)=1\nq1\nq2q3 (c) YXYXXY,C3=48\nYX YXYX YX YX XY 10.24 ...YXXYYX YX YXYX 30.49Learn DQN model\nYX YX XY next:position:1\nexchange\nAdd to memoryreward:0.24\n...YX YXYX current:\nnew old \n(d) Learning through LBMC\n0 500 1000 1500\nt0.20.40.60.81.0Cost ratioOptimal\nLBMC (e) Cost ratio vs. number of steps\nFigure 8: A BMC learning example.\nExample 7. Figure 8 illustrates LBMC with â„“=3and three queries\nğ‘1,ğ‘2, andğ‘3. The initial BMC ğœ1=YX:XYYX has an (estimated)\nquery cost ofC1=175(Figure 8a). We select position ğ‘1=3and\nswap the 3rd and the 4th bits to get ğœ2=YXY:XYXsuch that the cost is\ndecreased toC2=90(Figure 8b). Next, we select position ğ‘2=1and\nswap the 1st and the 2nd bits to get ğœ3=YXYX XY with costC3=48(Figure 8c). We store all the intermediate results into memory ğ‘€ğ‘„\nfor learning the DQN model in Figure 8d, where we show the BMCs\nwithout encoding. Figure 8e shows the cost ratios, i.e., Cğ‘¡/C1, which\ndecrease asğ‘¡increases (Figures 8a to 8c are three of the steps). The\nlearned BMC approaches the optimum in this process.\nAlgorithm cost. LBMC involves ğ‘‡Â·ğ‘€iterations that each in-\nvolves three key operations: bit-swap position prediction, reward\ncalculation (cost estimation), and model training. Their costs are\nğ‘‚(1),ğ‘‚(Cğ‘¡), andğ‘‚(Tğœƒ), respectively. The total time cost is then\nğ‘‚(ğ‘‡Â·ğ‘€Â·(1+Cğ‘¡+Tğœƒ)). Here,ğ‘‡Â·ğ‘€is a constant, while ğ‘‚(Tğœƒ)is\ndetermined by the model structure. Our cost estimation results in\nğ‘‚(Cğ‘¡)=ğ‘‚(1), thus enabling an efficient BMC search.\n6 EXPERIMENTS\nWe aim to evaluate the (1) efficiency and (2) effectiveness of the\nproposed cost estimation algorithms, as well as (3) LBMC vs. other\nSFCs, including the learning-based ones.\n6.1 Experimental Settings\nOur cost estimation algorithms (i.e., GC and LC) and BMC learning\nalgorithm (i.e., LBMC) are implemented in Python (available at\nhttps://anonymous.4open.science/r/LearnSFC-B6D8). The learning\nof BMC is supported by TensorFlow. We run experiments on a\ndesktop computer running 64-bit Ubuntu 20.04 with a 3.60 GHz\nIntel i9 CPU, 64 GB RAM, and a 500 GB SSD.\nDatasets. We use two real datasets: OSM [20] and NYC [28].\nOSM contains 100 million 2-dimensional location points (2.2 GB).\nNYC contains some 150 million yellow taxi transactions (8.4 GB).\nAfter cleansing incomplete records, we retain the pick-up locations\n(2-dimensional points) of 100 million records. Additionally, we\nfollow the study of the state-of-the-art competitor, the BMTree [ 13],\nand use two synthetic datasets, each with 100 million points: UNI\nandSKEW , which follow uniform and skewed distributions.\nQueries. We again follow the BMTree study and generate syn-\nthetic query workloads. Specifically, 1,000 synthetic queries are\nused for SFC learning, while 2,000 queries are generated separately\nfor testing. The queries are of uniform size and follow the distri-\nbutions of their respective datasets. To assess our cost estimation\nalgorithms (Sections 6.2 and 6.3), we employ square queries, since\nthe query shape does not impact the cost estimation time.\nEvaluation metrics. The core evaluation metrics used are (1) the\ncost estimation time , (2) the average number of block accesses\nper query when using different SFC ordering for query processing\n(in PostgreSQL), and (3) the SFC learning time .\nParameter settings. Table 5 summarises the parameter values\nused, with default values in bold . In the table, ğ‘›denotes the number\nof queries;ğ›¿denotes the edge length of a query; ğ‘‘denotes the data\ndimensionality; and ğ‘denotes the dataset cardinality. We randomly\nsample from the datasets described above to obtain datasets of\ndifferent cardinalities.\nFor SFCs, a key parameter is the number of bits â„“, which impacts\nthe curve value mapping efficiency substantially. To evaluate the\ncost estimation efficiency, we restrict â„“to 18, beyond which a naive\nlocal cost baseline becomes computationally infeasible. In later\nexperiments, we set â„“=20following the BMTree to balance the\ncomputational costs of curve value mapping and cost estimation.\n9\n\nConferenceâ€™17, July 2017, Washington, DC, USA et al.\nThe BMTree has two additional parameters: the dataset sampling\nrateğœŒto form a subset for query cost estimation, and the depth â„\nof space partitioning.\nTable 5: Parameter settings.\nExperiments Parameter Values\nCost ğ‘› 20,21,22,23,242424,25,26,27,28,29,210\nestimation ğ›¿(Ã—24)1, 2, 4, 8, 16\nefficiency â„“ 10, 12, 14, 16, 18\nğ‘‘ 2, 3, 4\nCost ğ‘ 104,105,106,107107107,108\nestimation ğœŒ(Ã—10âˆ’3)0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 7.5, 10\neffectiveness â„ 5, 6, 7, 8, 9, 10\nğ‘› 100, 500, 1000, 1500, 2000\ndatasets OSM , SKEW\nQuery ğ‘ 104,105,106,107107107,108\nefficiency aspect ratio 16 : 1 ,4 : 1,1 : 1,1 : 4,1 : 161 : 161 : 16\nğ›¿(Ã—26) 1, 2,4, 8, 16\ndatasets OSM , NYC, UNI, SKEW,\n6.2 Cost Estimation Efficiency\nWe first evaluate the efficiency of our algorithms (excluding ini-\ntialization) to compute the global cost GCand the local cost LC\n(Algorithm 2), which are based on Equations 6 and 9. We use IGC\nandILC to denote the initialization steps of the two costs, respec-\ntively. As there are no existing efficient algorithms to compute these\ncosts, we compare with baseline algorithms based on Equations 5\nand 10, denoted by NGC andNLC .\nWe vary the number of queries ğ‘›, the query size (via ğ›¿), and\nthe number of bits â„“. We run experiments for 2- to 4-dimensional\nspaces. Due to page limits, we focus on the 2-dimensional space (the\nalgorithmsâ€™ comparative results are similar for ğ‘‘âˆˆ{3,4}). As the\ncost estimation is data independent, a dataset is not needed to study\ntheir efficiency. The queries are generated at random locations.\n20212223242526272829210\nn10âˆ’310âˆ’1101102Time (ms)\nNGC\nGC\n(a) Varyingğ‘›\n1 2 4 8 16\nÎ´ (Ã—24)10-210-1Time (ms)\nGC\nNGC (b) Varyingğ›¿\n10 12 14 16 18\n/lscript10-210-1Time (ms)\nGC\nNGC\n(c) Varyingâ„“\n2 3 4\nd02040Gain (NGC/GC)24.7627.07 27.54 (d) Varyingğ‘‘\nFigure 9: Running times of global cost estimation.\n6.2.1 Efficiency of GC. Figures 9a and 9b show the impact of ğ‘›\nandğ›¿, respectively. Since GC takes ğ‘‚(ğ‘‘Â·â„“)time to compute (after\nthe initialization step), its running time is unaffected by ğ‘›andğ›¿.\nNGC takesğ‘‚(ğ‘›Â·ğ‘‘Â·â„“)time. Its running time grows linearly with\nğ‘›and is unaffected by ğ›¿as shown in the figures. Figure 9c shows\nthat the running times of GC and NGC both increase with â„“, which\nis consistent with their time complexities. Since the relative perfor-\nmance of our algorithm and the baseline is stable when â„“is varied,\nwe use a default value of 10 instead of the maximum value 18 as\nmentioned earlier, to streamline this set of experiments. Figure 9dshows the impact of ğ‘‘. Here, we show the performance gain (i.e.,\nthe running time of NGC over that of GC) instead of the absolute\nrunning times, which are of different scales when ğ‘‘is varied such\nthat it is difficult to observe the relative performance. We see that\nGC is faster than NGC by 24x. Overall, GC is consistently faster than\nNGC, with up to more than an order of magnitude performance\ngain, which confirms the high efficiency of GC.\n20212223242526272829210\nn10âˆ’510âˆ’2101102Time (s)\nLC\nNLC\n(a) Varyingğ‘›\n1 2 4 8 16\nÎ´ (Ã—24)10âˆ’510âˆ’2101Time (s)\nLC\nNLC (b) Varyingğ›¿\n10 12 14 16 18\n/lscript10âˆ’5100104Time (s)\nLC NLC\n(c) Varyingâ„“\n2 3 4\nd104105106Gain (NLC/LC)23105.4886604.88297337.33 (d) Varyingğ‘‘\nFigure 10: Running times of local cost estimation.\n6.2.2 Efficiency of LC. Figures 10a to 10d show the running times\nof computing local costs. The performance patterns of LC and NLC\nare similar to those observed above for GC and NGC, and they are\nconsistent with the cost analysis in Section 4.2. The performance\ngains of LC are even larger, as its pre-computed pattern table en-\nables extremely fast local-cost estimation. As Figure 10d shows, LC\noutperforms NLC by five orders of magnitude when ğ‘‘=4.\n6.2.3 Initialization Costs of GC and LC. Table 6 shows the running\ntimes of IGC and ILC, which increase with ğ‘›, because the initial-\nization steps need to visit all range queries to compute a partial\nglobal cost and prepare the pattern tables, respectively. These run-\nning times are smaller than those of NGC and NLC, confirming\nthe efficiency of the proposed cost estimation algorithms. Similar\npatterns are observed when varying ğ›¿,â„“, andğ‘‘, which are omitted\nfor brevity. We do not report the result when ğ‘›=20(i.e.,ğ‘›=1) as\nno initialization is needed for a single query.\nTable 6: Initialization costs of GC and LC (Varying ğ‘›).\nğ‘› 212223242526272829210\nIGC (ms) 0.03 0.05 0.08 0.15 0.27 0.52 1.06 1.93 4.07 7.79\nNGC (ms) 0.03 0.05 0.10 0.18 0.36 0.70 1.50 2.96 5.37 10.86\nILC (s) 0.01 0.01 0.02 0.06 0.12 0.23 0.48 0.95 1.83 3.63\nNLC (s) 0.01 0.06 0.18 0.93 1.93 3.03 6.31 9.21 20.98 48.22\n6.3 Effectiveness of Cost Estimation\nWe next explore the applicability and effectiveness of our GC and\nLC cost estimations by using them to replace the built-in cost esti-\nmations of the state-of-the-art SFC learning algorithm, the BMTree.\nWe denote the resulting variants by BMTree-GC andBMTree-LC .\nThe original BMTree uses a data sampling-based empirical cost\nestimation method. We denote it as BMTree-SP .\nWe report the time cost of reward calculation for the three vari-\nants, as the other steps of the variants are the same. After the SFCs\nare learned by the three variants, we build a B+-tree with each SFC\nin PostgreSQL to index the input dataset. We measure the average\n10\n\nEfficient Cost Modeling of Space-filling Curves Conferenceâ€™17, July 2017, Washington, DC, USA\nnumber of block accesses as reported by PostgreSQL to process\neach of the queries as described earlier.\n6.3.1 Varying the Dataset Cardinality. We start by varying the\ndataset cardinality ğ‘from 104to108. Figure 11 shows the results\non the OSM dataset (the results on the other datasets show similar\npatterns and are omitted for brevity; same below). BMTree-GC\nand BMTree-LC have constant reward calculation times, since GC\nand LC are computed in constant times. In comparison, the re-\nward calculation time of BMTree-SP increases linearly with the\ndataset cardinality, as BMTree-SP builds intermediate index struc-\ntures based on sampled data points for query cost estimation. When\nğ‘increases, the number of sampled data points also increases. At\nğ‘=108(the default sampling rate is ğœŒ=0.001, i.e., BMTree-SP is\nrun on a sampled set of 105points), the reward calculation time of\nBMTree-SP (more than 7 hours) is 36x and 474x higher than those\nof BMTree-LC (737 s) and BMTree-GC (57 s).\nIn terms of the query costs, the indices built using all three\nalgorithms require more block accesses as ğ‘increases, which is\nexpected. Importantly, all three algorithms incur similar numbers\nof block accesses given the same ğ‘value. This suggests that the\nGC and LC cost estimations can be applied to improve the curve\nlearning efficiency of the BMTree without adverse effects on the\nquery efficiency. In general, BMTree-LC offers lower query costs\nthan BMTree-GC. Thus, applications that are more sensitive to\nquery costs may use BMTree-LC, while those that are more sensitive\nto index building costs may use BMTree-GC.\n104\n105\n106\n107\n108\nN101102103104105Reward calculation time (s)\nBMTree-GC\nBMTree-LC\nBMTree-SP\n(a) Reward calculation time\n104\n105\n106\n107\n108\nN101102103104105# Block accessesBMTree-GC\nBMTree-LC\nBMTree-SP (b) Query processing cost\nFigure 11: Varying the dataset cardinality (OSM).\n6.3.2 Varying the Number of Queries. Next, we vary the number\nof queries used in curve learning, ğ‘›, from 100 to 2,000. We see\nthat BMTree-LC and BMTree-GC consistently outperform BMTree-\nSP by one and two orders of magnitude in terms of the reward\ncalculation time, respectively (Figure 12a). We note that, now the\ncomputation times of BMTree-LC and BMTree-GC vary with ğ‘›,\nwhich differs from what was reported in Figures 9a and 10a. This\nhappens because the BMTree uses different BMCs in different sub-\nspaces to accommodate different data and query patterns. As there\nare more queries, more different patterns may need to be considered,\nresulting in more different BMCs, each of which requires a different\nGC and LC cost estimation. Thus, the cost estimation costs grow\nwith the number of queries ğ‘›.\nMeanwhile, the query costs of the three algorithms are again\nclose, e.g., 9,199, 9,248, and 10,462, for BMTree-LC, BMTree-SP, and\nBMTree-GC, respectively, when ğ‘›is 1,500. The higher query cost of\nBMTree-GC shows that while GC is extremely simple and efficient,\nit may not find the most query-efficient curves, which underlines\nthe importance of the LC cost estimation algorithm.We further observe a slight drop in the number of block accesses\nasğ‘›increases. Intuitively, using more queries for curve learning\ncan lead to curves that better suit the query workload.\n100 500 1000 1500 2000\nn101102103104105Reward calculation time (s)\nBMTree-GC BMTree-LC BMTree-SP\n(a) Reward calculation time\n100 500 1000 1500 2000\nn100101102103104105# Block accessesBMTree-GC BMTree-LC BMTree-SP (b) Query processing cost\nFigure 12: Varying the number of queries (OSM).\n6.3.3 Varying the Sampling Rate and the Depth of the BMTree. Two\nalternative approaches to improve the curve learning efficiency of\nthe BMTree are (1) to reduce its data sampling rate ğœŒand (2) to\nreduce the depth of its space partitioning â„.\nIn this set of experiments, we study how these two parameters\nimpact the reward calculation time and the query cost of the result-\ning SFCs. In particular, we vary ğœŒfrom 10âˆ’4to10âˆ’2(a total of 9\nvalues, cf. Table 5), and we vary â„from 5 to 10.\nFigure 13 plots the results on the SKEW and OSM datasets.\nBMTree-SP has three result polylines: BMTree-SP-6, BMTree-SP-8,\nand BMTree-SP-10, each of which uses a different â„value, while the\npoints on each polyline represent the results of different ğœŒvalues\n(points on the right come from larger ğœŒvalues).\nBMTree-GL and BMTree-LC are plotted with one polyline each,\nas they are not impacted by ğœŒ. The points on these polylines repre-\nsent the results of different values of â„(points on the right corre-\nspond to larger â„values).\nWe see that a larger â„value tends to lead to lower query costs,\nwhile it also yields a longer reward calculation time. Powered by\nthe LC cost estimation algorithm, BMTree-LC reduces the reward\ncalculation time by at least an order of magnitude while achieving\nthe same level of query costs (i.e., its curve lies at the bottom left of\nthe figure). BMTree-GC can also be very fast at reward calculation,\nwhile it may suffer at query performance.\n100\n101\n102\n103\n104\nReward calculation time (s)380050007000900010000# Block accesses\n(a) SKEW\n100\n101\n102\n103\n104\nReward calculation time (s)9000100001100012000# Block accesses\nBMTree-GC\nBMTree-LC\nBMTree-SP-6\nBMTree-SP-8\nBMTree-SP-10 (b) OSM\nFigure 13: Varying the sampling rate and the space partition-\ning depth of the BMTree.\n6.4 Query Efficiency with BMC Learning\nWe proceed to study the BMC learning efficiency of LBMC and\nthe query efficiency of the indices built using the learned BMCs.\nCompetitors. We compare with five different SFC-based or-\ndering techniques. (1) QUILTS [19] orders data points by a BMC\nderived by a curve design method as described in Section 2. We\nimplement it according to its paper as the source code is unavailable.\n(2)ZC[21] orders data points by their Z-curve values. (3) HC[10]\norders data points by their Hilbert curve values. (4) LC, which is\n11\n\nConferenceâ€™17, July 2017, Washington, DC, USA et al.\nalso called the C-Curve, orders data points lexicographically by\ntheir dimension values [ 13,19]. (5) BMTree [13] orders data points\nby multiple BMCs in different sub-spaces. We use its released code\n(withâ„=8andğœŒ=0.001to balance the reward calculation time and\nthe query costs, cf. the â€˜ â˜…â€™-points on BMTree-SP-8 in Figure 13). We\ncannot compare with the recent learned SFC, LMSFC [ 7], because\nits source code and some implementation details are unavailable.\nWe do not compare with RSMI [ 25] as it has been shown to be\noutperformed by the BMTree [13].\nFor all techniques, we use the curves obtained to order the data\npoints and build B+-trees in PostgreSQL for query processing, and\nwe report the average number of block accesses as before.\n6.4.1 Overall Results. Figure 14 shows the average number of\nblock accesses on all four datasets. LBMC outperforms all com-\npetitors consistently. On SKEW, the advantage of LBMC over the\nBMTree is the most pronounced. It reduces the average number\nof block accesses by 28x (111 vs. 3,084) and by 6x (111 vs. 674) in\ncomparison with the BMTree and QUILTS, respectively. On NYC,\nthe advantage of LBMC over the BMTree is the least, yet it still\nrequires only 2,638 block accesses which is fewer than that of the\nBMTree at 3,448. These results suggest that LBMC is highly efficient\nat reducing the query costs across diverse datasets.\nLC is the worst, which is expected as LC curves fail to preserve\nthe data locality. The BMTree and QUILTS outperform LC, ZC, and\nHC on real data such as NYC, where they benefit more from the\nquery based optimizations. However, there are no consistent results\nacross the different datasets. We conjecture that fine-tuning of the\nparameter values of â„andğœŒmay be needed for the BMTree over\neach different dataset. Such fine-tuning is not required by LBMC.\nOSM NYC UNI SKEW\nDatasets100101102103104105# Block accessesBMTree LBMC QUILTS LC ZC HC\nFigure 14: Block access over all datasets.\n6.4.2 Varying the Dataset Cardinality. We further study the impact\nof dataset cardinality ğ‘. Figure 15 shows the results. Like before,\nthe average number of block accesses increases with ğ‘, which is\nexpected. LBMCis again the most efficient in terms of query costs,\nneeding at least 39% fewer block accesses than the BMTree (4.0 vs.\n6.6 whenğ‘=104), and the advantage is up to 74% (1,044 vs. 4,131\nwhenğ‘=107).\n104\n105\n106\n107\n108\nN100101102103104105106# Block accessesBMTree LBMC QUILTS LC ZC HC\nFigure 15: Varying cardinality (OSM).We report the SFC learning times of the BMTree and LBMC\nwhen varying ğ‘in Table 7. We see that LBMC is much faster than\nthe BMTree at SFC learning and that the advantage grows with\nğ‘. This is because the cost estimation (i.e., reward calculation) in\nthe BMTree is much slower than that in LBMC, as shown in the\nlast subsection. The cost estimation time dominates when there are\nmore data points for the BMTree, while the cost estimation time of\nLBMC remains constant when varying ğ‘.\nLC, ZC, and HC are not learned, and they do not take any learning\ntime. QUILTS takes less than 1 second, as it only considers a few\ncurve candidates (which are generated based on query shapes)\nusing a cost model. We have used our cost estimation algorithms\nin our implementation of QUILTS, as the original cost model is\nprohibitively expensive.\nTable 7: SFC learning time (seconds).\nğ‘ 104105106107108\nBMTree 54 55 61 99 551\nLBMC 15 15 15 15 15\nQUILTS (with our cost estimation) 0.2 0.2 0.2 0.2 0.2\n6.4.3 Varying the Aspect Ratio of Queries. Figure 16 shows the\nquery costs when varying the query aspect ratio. Here, LBMC\nshows a stronger advantage over the competitors on queries that\nare â€œstretchedâ€, while LC also better suits the queries that are long\nand thin (16:1) which is intuitive. When the aspect ratio is 1 : 1 ,\nLBMC, QUILTS, and ZC share almost the same query performance\nbecause they all tend to form a â€˜ zâ€™ shape to fit square queries. The\nBMTree is again outperformed by LBMC, because of its less flexible\nlearning scheme (i.e., learning for only up to â„bits), while LBMC\ncan learn a BMC scheme with all â„“bits (â„“=20by default).\n16:1 4:1 1:1 1:4 1:16\nAspect ratio100101102103104105# Block accessesBMTree LBMC QUILTS LC ZC HC\nFigure 16: Varying the query aspect ratio (OSM).\n6.4.4 Varying the Edge Length of Queries. Figure 17 shows that\nthe average number of block accesses grows with the query edge\nlength, as expected. Here, LBMC again outperforms the competitors\nconsistently, further showing the robustness of LBMC.\n1 2 4 8 16\nÎ´ (Ã—26)100101102103104105# Block accessesBMTree LBMC QUILTS LC ZC HC\nFigure 17: Varying the query edge length (OSM).\n7 CONCLUSIONS AND FUTURE WORK\nWe studied efficient cost estimation for a family of SFCs, i.e., the\nBMCs. Our cost algorithms can compute the global and the local\n12\n\nEfficient Cost Modeling of Space-filling Curves Conferenceâ€™17, July 2017, Washington, DC, USA\nquery costs of BMCs in constant time given ğ‘›queries and after an\nğ‘‚(ğ‘›)-time initialization. We extended these algorithms to the state-\nof-the-art curve learning algorithm, the BMTree, which originally\nmeasured the effectiveness of SFCs by querying the data points to\nbe indexed. Experimental results show that the proposed algorithms\nare capable of reducing the cost estimation time of the BMTree by\nover an order of magnitude with little or no impact on the query\nefficiency of the learned curves.\nWe further proposed a reinforcement learning-based curve learn-\ning algorithm. The result learned BMCs are shown to achieve lower\nquery costs than those of the BMTree and other baselines under\nnearly all settings tested.\nIn future work, it is of interest to design cost estimation algo-\nrithms for non-BMCs, e.g., HC, and use learning-based techniques\nto build more efficient multi-dimensional indices.\nREFERENCES\n[1]Amazon AWS. 2016. https://aws.amazon.com/blogs/big-data/amazon-redshift-\nengineerings-advanced-table-design-playbook-compound-and-interleaved-sort-\nkeys. Accessed: 2023-10-10.\n[2]Apache Hudi. 2021. https://hudi.apache.org/blog/2021/12/29/hudi-zorder-and-\nhilbert-space-filling-curves . Accessed: 2023-10-10.\n[3]Christian BÃ¶hm. 2020. Space-filling Curves for High-performance Data Mining.\nCoRR abs/2008.01684 (2020).\n[4]Databricks Engineering Blog. 2018. https://databricks.com/blog/2018/07/31/\nprocessing-petabytes-of-data-in-seconds-with-databricks-delta.html . Accessed:\n2023-10-10.\n[5]Christos Faloutsos and Shari Roseman. 1989. Fractals for Secondary Key Retrieval.\nInPODS . 247â€“252.\n[6]Raphael A. Finkel and Jon Louis Bentley. 1974. Quad Trees: A Data Structure for\nRetrieval on Composite Keys. Acta Informatica 4, 1 (1974), 1â€“9.\n[7]Jian Gao, Xin Cao, Xin Yao, Gong Zhang, and Wei Wang. 2023. LMSFC: A\nNovel Multidimensional Index based on Learned Monotonic Space Filling Curves.\nPVLDB 16, 10 (2023), 2605â€“2617.\n[8]Claire E. Heaney, Yuling Li, Omar K. Matar, and Christopher C. Pain. 2020.\nApplying Convolutional Neural Networks to Data on Unstructured Meshes with\nSpace-Filling Curves. CoRR abs/2011.14820 (2020).\n[9]Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2011. Sequential Model-\nBased Optimization for General Algorithm Configuration. In International Con-\nference on Learning and Intelligent Optimization . 507â€“523.\n[10] Ibrahim Kamel and Christos Faloutsos. 1994. Hilbert R-tree: An Improved R-tree\nusing Fractals. In VLDB . 500â€“509.\n[11] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In SIGMOD . 489â€“504.[12] Warren M. Lam and Jerome M. Shapiro. 1994. A Class of Fast Algorithms for\nthe Peano-Hilbert Space-Filling Curve. In International Conference on Image\nProcessing . 638â€“641.\n[13] Jiangneng Li, Zheng Wang, Gao Cong, Cheng Long, Han Mao Kiah, and Bin Cui.\n2023. Towards Designing and Learning Piecewise Space-Filling Curves. PVLDB\n16, 9 (2023), 2158â€“2171.\n[14] Stephen McAleer, Forest Agostinelli, Alexander Shmakov, and Pierre Baldi. 2019.\nSolving the Rubikâ€™s Cube Without Human Knowledge. In ICLR .\n[15] Microsoft. 2023. https://learn.microsoft.com/en-us/sql/relational-databases/\nindexes/indexes?view=sql-server-ver16 . Accessed: 2023-10-10.\n[16] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis\nAntonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. Playing Atari\nwith Deep Reinforcement Learning. CoRR abs/1312.5602 (2013).\n[17] Bongki Moon, H. V. Jagadish, Christos Faloutsos, and Joel H. Saltz. 2001. Analysis\nof the Clustering Properties of the Hilbert Space-Filling Curve. IEEE Transactions\non Knowledge and Data Engineering 13, 1 (2001), 124â€“141.\n[18] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. In SIGMOD . 985â€“1000.\n[19] Shoji Nishimura and Haruo Yokota. 2017. QUILTS: Multidimensional Data Par-\ntitioning Framework Based on Query-Aware and Skew-Tolerant Space-Filling\nCurves. In SIGMOD . 1525â€“1537.\n[20] OpenStreetMap. 2018. OpenStreetMap North America data dump. https://download.\ngeofabrik.de . Accessed: 2023-10-10.\n[21] Jack A. Orenstein. 1986. Spatial Query Processing in an Object-Oriented Database\nSystem. In SIGMOD . 326â€“336.\n[22] Jack A. Orenstein and T. H. Merrett. 1984. A Class of Data Structures for Asso-\nciative Searching. In PODS . 181â€“190.\n[23] Sachith Pai, Michael Mathioudakis, and Yanhao Wang. 2022. Towards an Instance-\nOptimal Z-Index. In AIDB@VLDB .\n[24] PostgreSQL. 2023. https://www.postgresql.org/docs/current/indexes-multicolumn.\nhtml . Accessed: 2023-10-10.\n[25] Jianzhong Qi, Guanli Liu, Christian S. Jensen, and Lars Kulik. 2020. Effectively\nLearning Spatial Indices. PVLDB 13, 11 (2020), 2341â€“2354.\n[26] Jianzhong Qi, Yufei Tao, Yanchuan Chang, and Rui Zhang. 2018. Theoretically\nOptimal and Empirically Efficient R-trees with Strong Parallelizability. PVLDB\n11, 5 (2018), 621â€“634.\n[27] S2 Geometry. 2023. http://s2geometry.io . Accessed: 2023-10-10.\n[28] TLC Trip Record Data. 2022. https://www1.nyc.gov/site/tlc/about/tlc-trip-record-\ndata.page . Accessed: 2023-10-10.\n[29] Panagiotis Tsinganos, Bruno Cornelis, Cornelis Jan, Bart Jansen, and Athanassios\nSkodras. 2021. The Effect of Space-filling Curves on the Efficiency of Hand\nGesture Recognition Based on sEMG Signals. International Journal of Electrical\nand Computer Engineering Systems 12, 1 (2021), 23â€“31.\n[30] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned Index for\nSpatial Queries. In MDM . 569â€“574.\n[31] Pan Xu, Cuong Nguyen, and Srikanta Tirthapura. 2018. Onion Curve: A Space\nFilling Curve with Near-Optimal Clustering. In ICDE . 1236â€“1239.\n[32] Pan Xu and Srikanta Tirthapura. 2014. Optimality of Clustering Properties\nof Space-Filling Curves. ACM Transactions on Database Systems 39, 2 (2014),\n10:1â€“27.\n13",
  "textLength": 75658
}