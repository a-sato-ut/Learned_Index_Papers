{
  "paperId": "65567e231282623f2948489fa11ff1b304dd855b",
  "title": "Integrating Machine Learning Model Ensembles to the SAVIME Database System",
  "pdfPath": "65567e231282623f2948489fa11ff1b304dd855b.pdf",
  "text": "Integrating Machine Learning Model Ensembles to the\nSA VIME Database System\nAnderson Silva1, Patrick Valduriez2, Fabio Porto1\n1Laborat ´orio Nacional de Computac ¸ ˜ao Cient ´ıfica (LNCC)\nPetr´opolis, Rio de Janeiro, Brazil\n{achaves,fporto }@lncc.br\n2Inria, University of Montpellier, CNRS, LIRMM\nMontpellier, France\npatrick.valduriez@inria.fr\nAbstract. The integration of machine learning algorithms into database systems\nhas brought new opportunities in different areas from indexing to query opti-\nmization. In this paper, we describe the integration of an approach for the au-\ntomatic computation of model ensembles to answer a predictive query. We have\nextended the SAVIME multi-dimensional array DBMS by adding a new function\nto its query language and implementing the selection and allocation ensemble\nmodel dataflow into the query processing component of SAVIME. We show some\ninitial experimental results depicting its performance against a pure Python im-\nplementation of the ensemble approach. Interestingly enough the C++ imple-\nmentation within SAVIME is up to 4 times faster than its competitor.\n1. Introduction\nThe adoption of Machine Learning (ML) models in replace of different database manage-\nment techniques has been a hot research topic since the appearance of the Learned Index\npaper by Kraska [Kraska et al. 2018]. Since then, a number of different applications of\nmachine learning in databases has been proposed, such as learning cardinality estimation\n[Kim et al. 2022], a long time difficult problem to be solved algorithmically, as well as\njoin ordering selection and query optimization [Marcus et al. 2021], just to name a few.\nAnother area of intense activity, strongly pushed by the database systems industry, is the\nintegration of machine learning models as predictive functions receiving input from query\nexpressions [Fard et al. 2020][Jasny et al. 2020]. There has also been strong interest in\nthe investigation of the performance of training machine learning models in the database,\ntaking advantage of years of efficient data processing, specially for large input training\nsets [Sandha et al. 2019].\nIn this paper, we present initial results on the integration of machine learning mod-\nels ensembles into the SA VIME database system [L. S. Lustosa et al. 2021]. We consider\nthe DJEnsemble approach [Pereira et al. 2021] that combines a set of spatio-temporal\ndeep learning models automatically selected by a cost-based model. As in other works, we\nargue that database systems already offer a declarative query language to which model in-\nvocation can be easily integrated, as in SQL user defined functions [Duta and Grust 2020].\nOnce added to a query plan, all data transformation needed when preparing the model in-\nput can be specified as a query expression placing ML model inference as part of anCompanion Proceedings of the 37thBrazilian Symposium on Data Bases September 2022 – B´ uzios, RJ, Brazil\n231\n\nefficient query processing framework. Moreover, we observe that the multidimensional\narray model of SA VIME is particularly suited to model 3D input tensors, as required by\ndeep learning input data. We describe the decisions taken during the design of the inte-\ngration of DJEnsemble into the SA VIME system and some challenges still remaining to\nbe solved. We highlight that by adding DJEnsemble to SA VIME, or any other DBMS, its\nmodel selection and allocation problem can be solved as part of the query optimization\nprocess.\nWe have run some initial experiments highlighting the overhead of invoking the\nmodel integrated into SA VIME against its execution on pure ML engine/programming\nlanguage combination. The rest of this paper is organized as follows. Section 2 presents\nthe original DJEnsemble approach and the SA VIME DBMS system. Next, in Section 3,\nwe describe the implementation of the ensemble approach into SA VIME as a new query\noperator. Section 4 presents some initial experimental results, and Section 5 discusses\nprevious works that intended to integrate ML into database systems. Finally, Section 6\npresents some final remarks and points to future work.\n2. Preliminaries\nIn this section we provide background on the DJEnsemble ensemble approach and on the\nSA VIME multidimensional array database system.\n2.1. DJEnsemble Approach\nThe machine learning ensemble approach is a technique to improve the accuracy of\npredictions by using multiple models for the same prediction task and applying a lin-\near function to combine their results. The assumption is that by combining differ-\nent models, the weaknesses of each one are compensated by the strengths of the oth-\ners. However, DJEnsemble takes a slightly different approach [Pereira et al. 2021].\nAs the traditional ensemble approach, it considers a set of available trained models\nM={M1, M 2, . . . , M n}for the prediction of a spatio-temporal variable, i.e. a spa-\ntial time series. Examples of spatio-temporal variables are temperature andprecipitation .\nHowever, instead of invoking all available models to compute a prediction, DJEnsemble\nselects the best models to be applied to each data subdomain. We consider a domain the\ndataset composed of time series of a variable. For instance, the domain of the precipita-\ntion variable comprises the dataset of time series of a region. In this context subdomains\ncharacterize regions sharing similar precipitation behavior.\nDJEnsemble is applicable to answer spatio-temporal queries (SPTQ), which are\nexpressed as Q=< R, I, V, t > , where Ris a spatio-temporal region where predictions\nare desired, for instance the region of the Rio de Janeiro city in January 2022, Iis a\ntensor of input to the model, Vis the predicted variable, for instance, precipitation, and\ntis the number of time instances ahead for the prediction. The approach is integrated\ninto the SA VIME system (see section 2.2) in two parts: offline and online. The offline\nstage structures the domain dataset into clusters and tiles. The former finds the time\nseries sharing similar behaviors in time, whereas the latter structures the dataset domain\ninto regular tiles i.e. rectangles with a high percentage of time-series of a single cluster.\nLastly, to each tile, we associate its representative, which is a time series elected as the\nmedoid of the cluster bearing the majority of time series in the corresponding tile.Companion Proceedings of the 37thBrazilian Symposium on Data Bases September 2022 – B´ uzios, RJ, Brazil\n232\n\nFigure 1 illustrates the process described. The image at the left shows the clusters\nof a spatial domain containing precipitation readings, discretized in 7 x 7 regions. An\nanalysis using silhouette clustering index [Rousseeuw 1987] indicates 2 clusters. At right,\nthe results of the tiling process applied over the clustered spatio-temporal domain are\nillustrated. At each iteration of the tiling algorithm, a position is chosen as the starting\npoint of a new tile. The tile area is then extended to the right and bottom, as long as the\nnumber of regions from different clusters in the tile is not exceeded.\nFigure 1. DJEnsemble clustering and tiling identified by the offline stage\nThe online stage is when the query processing happens. It considers a query Q\nwith a region Q.R, with Q.R representing a 3D region of a spatio-temporal tensor of\nprecipitation data. The input Q.Iis the result of a spatio-temporal query expression. The\ndataflow in Figure 2 depicts the various stages of the DJEnsemble online execution. We\nidentify tasks into colours blue(B), orange (O) and blue-orange (BO).\nFigure 2. Dataflow for the execution of DJEnsemble\n• B1: obtain from the catalog a set of trained models for forecasting a variable V;\n• B2: extract the model metadata informing the data used in training, identified by\nthe corresponding tiles;\n• B3: obtain the representative for each tile identified in B2\n• B4: obtain the learning function (LF) for the tile. A LF is a model that given a\ndistance predicts a model error;Companion Proceedings of the 37thBrazilian Symposium on Data Bases September 2022 – B´ uzios, RJ, Brazil\n233\n\n• O1: obtain the set of the domain tiles whose spatial volume intersects with that of\nthe the query Qregion R;\n• O2: obtain the tiles representatives;\n• BO5: for each query tile, compute its representative distance to the tiles represen-\ntative of the candidate models;\n• BO6: use the models learning functions to compute their estimated error for each\nquery tile and associate to each tile the model with the least estimated error\n• BO7: invoke each of the selected models having as input the part of Icorrespond-\ning to its tile\n• BO8: compose the predictions from each model into a single prediction volume.\nThe step BO6 runs an optimization weighted cost function that takes into account\nthe error estimate computed by the learning function and the estimated execution cost.\nThus, at the end of this process an ensemble of models has been selected and run for\ncomputing the predictions as requested by the query Q.\n2.2. SA VIME Array DBMS\nThe SA VIME multidimensional array database system is a NOSQL system developed at\nthe DEXL Laboratory [Lustosa 2020]. SA VIME logical data representation is modeled\nby a Typed Array (TAR) data model. A TAR schema (TARS) specifies the arrays in a\ndatabase. A TAR definition holds a name that uniquely identifies the array, and counts\nwith two main data elements for an array definition: dimensions and attributes. There is\nno restriction on the number of dimensions an array may specify. A typical dimension is\ndefined as d= ([1 : n], step ), where the interval indicates integer indexing from 1ton,\nwith integer steps between index values. For a kdimensional array, a cell is specified as\nthe array position indicated by a vector of dimensions v=< v 1, v2, . . . , v k>, where vi\nis an index of dimension i, for all 1≤i≤k. A cell may hold a tuple of attributes of\nsimple types, such as: integer, float, double etc. SA VIME implements a functional query\nlanguage, inspired by AFL, one of the query languages adopted by the SciDB system\n[Brown 2010]. A function in SA VIME is implemented by an operator that consumes TAR\nobjects as input, and produce a single TAR array as output. The list of current available\noperators in SA VIME is depicted in Table 1.\nQuery optimization in SA VIME adopts a generalization of push down selection\napproach, pushing down all operations that reduce the size of arrays: Where, Subset,\nSelect and Aggregate. The Predict operation specifies ML models to be executed with in-\nputs produced by inner operations of a query expression. During optimization, the predict\noperation is pulled up in the physical query plan. A TAR is redefined by one or multi-\nple SubTARs. The latter is an irregular partitioning of a TAR, such that the union of all\nSubTARs S={S1, S2, ..., S n}reproduces in definition and in content the corresponding\nTART. The SubTARs are the unit of processing of an operator. SA VIME uses the Sub-\nTAR for parallelizing the computation of operators. Considering system characteristics,\nSA VIME is an in-memory system that considers arrays to be allocated in main memory\nfor query processing. Moreover, the system reads data in raw file format, not requiring\nan ingestion phase for processing it. Another interesting feature is its integration with the\nPython language, through the Pysavime library. TAR arrays returned by queries in SA V-\nIME are transformed into Numpy arrays for processing using the scripting programming\nlanguage.Companion Proceedings of the 37thBrazilian Symposium on Data Bases September 2022 – B´ uzios, RJ, Brazil\n234\n\nOperator Description\nSELECT Projects dimensions and attributes\nWHEREFilters data according to a predicate on dimensions and at-\ntributes\nSUBSETCreates a n-dimensional data slice according to specified\nbounds\nDERIVE Adds an attribute with derived values\nCROSS JOIN Implements a cartesian product of cells in TARs\nDIM JOINImplements an equi-join by corresponding dimensions of\ntwo TARs\nAGGREGATESummarizes data according to a subset of dimensions and\napplying a common aggregate function\nPREDICT Invokes a single ML model with input from a TAR\nTable 1. SAVIME query language operators\n3. Integrating DJEnsemble in SA VIME\nWe present the integration of DJEnsemble into SA VIME in order to support spatio-\ntemporal queries execution. To execute an inductive query, SA VIME makes use\nof a predictive engine module external to the system based on tensorflow server\n[Baylor et al. 2017]. This module allows SA VIME to answer predictive queries in gen-\neral.\nCurrently, SA VIME is capable of executing the entire online stage of DJEnsemble\nwithin the system, through the ENSEMBLE operator built into the query language. The\nsyntax of ENSEMBLE operator is:\nENSEMBLE(<tar>, <query-region>, predictive-variable)\nFigure 3 illustrates how this process is achieved. The results of the offline stage\nmust be previously registered in the system through metadata files, namely: the tiling\nresulting of the time series categorization process and the error estimation function for\neach predictive model, along with the time series that best represents the models training\ndata. Once a query is performed, SA VIME executes the error estimation functions in order\nto choose the best candidate models to answer the query. The error estimated to each\nmodel for each tile intersecting with the query region is based on the distance between its\nrepresentative time series and each model’s representative sequences. We utilize Dynamic\nTime Warping to calculate the distances between sequences. The composition of each tile\npaired with the model with the least estimated cost for it constitutes the query model\nensemble, which can be finally executed to return the result to the user.\nIt is worth mentioning that the Ensemble operator is part of SA VIME’s DML,\nand thus the operator’s input TAR can be resulting from a previous executed nested query.\nSimilarly, to make use of the full potential of SA VIME’s query language, the data returned\nby Ensemble can also be chained together as input to new queries.Companion Proceedings of the 37thBrazilian Symposium on Data Bases September 2022 – B´ uzios, RJ, Brazil\n235\n\nSavimeQuery\nFile\nDjEnsemble \nMetadata\nTARs Data\n(In-Memory)DJEnsemble\nOffline\nPredictive\nEngineSystem\nUserFigure 3. Overview of DJEnsemble integration into SAVIME\n4. Experimental Evaluation\nTo analyze the algorithm performance when integrated to SA VIME, we performed a series\nof experiments evaluating the execution time of the different steps. For reference, we also\nmeasured the offline step execution time (already presented in [Pereira et al. 2021]). For\nour experiments, we built a dataset from rain data from the city of Rio de Janeiro, provided\nby 33 pluviometrical stations. The constructed dataset represents a 7 x 7 mesh over the\ncity, containing the precipitation values along 20 years, recorded at 15-minute intervals.\nThe experiments were run using the hardware configurations: Intel Core i5-5200U CPU\n@ 2.20GHz and 8 GB RAM.\nWe synthetically partitioned the obtained data into 3 tiles, representing 3 different\nregions of the city. We also built 3 Convolutional Long Short-Term Memory (ConvLSTM)\nneural network model models, with input dimensions of 3x3, 4x4 and 7x7. The training\ndata for each model and corresponding representing sequences were defined in such a\nway that the estimated error for each tile would be optimal by choosing a different model\nby the algorithm.\nBased on the described experimental scenario, we submitted 3 different predictive\nqueries to SA VIME, Q1, Q2 and Q3, using the Ensemble operator. In order to evaluate\nthe runtime variation as the number of tiles increases, each query is designed so that it\nwould intersect with one, two or three tiles of the domain. For comparison purposes,\nwe performed a test with the same data under the DJEnsemble version purely written in\nPython, not integrated to SA VIME.\nFigure 4. Comparative specification and results between DJEnsemble\nand SAVIME. All times refer to average in seconds\nTotal results of the experiments can be seen in Figure 4. Note that the relation is\nnot linear, because two model executions are necessary for the tile 2 model, differently\nthan others.Companion Proceedings of the 37thBrazilian Symposium on Data Bases September 2022 – B´ uzios, RJ, Brazil\n236\n\n5. Related Work\nThe integration of machine learning algorithms into database systems initiated with the\nnow famous Learning Index [Kraska et al. 2018] paper by Tim Kraska. In that paper\nKraska and colleagues showed that a B+tree could be substituted by a learned model.\nSince then, other areas of the database systems have been explored as opportunities for\nML algorithms. Computing cardinalities of operations has always been a difficult task\nthat is at the heart of query optimization. Many authors have been working in learned\ntechniques modeling it as a regression problem [Dutt et al. 2019][Woltmann et al. 2019].\nA second approach involves applying ML models to data in databases. In this context,\nML algorithm are integrated to the system that may support ML models training and in-\nferencing [Fard et al. 2020]. Model inferencing is integrated to the system inheriting the\nexisting code to support user-defined function execution. Other systems such as Post-\ngreSQL [Pos 2022] and Greenplum [gre 2022] have added the support to the ML library\nMADLib . The work we describe in this paper considers the integration of ML models for\ninferencing and uses an external ML engine for model execution.\n6. Conclusion\nMachine learning algorithms have shown concrete results in a myriad of applications.\nIn the database system this has not been different. In this paper, we describe a new\nopportunity for applying machine learning predictions by adding model ensembles as part\nof a query specification. We integrate the DJEnsemble approach to the SA VIME query\nengine. The approach automatically selects and allocates deep learning models to solve\na spatio-temporal predictive query. Our initial results show that the C++ implementation\nof DJEnsemble in SA VIME is up to 4 times faster when compared against a typical data\nscience Python implementation. There are a number of opportunities for future work,\nincluding integrating the cost function of DJEnsemble with the SA VIME query optimizer\nand improving models’ results composition algorithm.\nReferences\n(2022). Greenplum. https://greenplum.org/ . [Online; accessed 20-July-2022].\n(2022). PostgreSQL. https://www.postgresql.org/ . [Online; accessed 20-\nJuly-2022].\nBaylor, D., Breck, E., Cheng, H.-T., Fiedel, N., Foo, C. Y ., Haque, Z., Haykal, S., Ispir,\nM., Jain, V ., Koc, L., et al. (2017). Tfx: A tensorflow-based production-scale machine\nlearning platform. In Proceedings of the 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , pages 1387–1395, Nova Scotia, Canada.\nAssociation for Computing Machinery.\nBrown, P. G. (2010). Overview of scidb: Large scale array storage, processing and anal-\nysis. In Proceedings of the 2010 ACM SIGMOD International Conference on Man-\nagement of Data , SIGMOD ’10, page 963–968, New York, NY , USA. Association for\nComputing Machinery.\nDuta, C. and Grust, T. (2020). Functional-style SQL UDFs with a capital ’F’. In Proceed-\nings of the 2020 ACM SIGMOD International Conference on Management of Data ,\nSIGMOD ’20, page 1273–1287, New York, NY , USA. Association for Computing\nMachinery.Companion Proceedings of the 37thBrazilian Symposium on Data Bases September 2022 – B´ uzios, RJ, Brazil\n237\n\nDutt, A., Wang, C., Nazi, A., Kandula, S., Narasayya, V ., and Chaudhuri, S. (2019).\nSelectivity estimation for range predicates using lightweight models. Proc. VLDB\nEndow. , 12(9):1044–1057.\nFard, A., Le, A., Larionov, G., Dhillon, W., and Bear, C. (2020). Vertica-ML: Distributed\nmachine learning in Vertica database. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data , SIGMOD ’20, page 755–768, New\nYork, NY , USA. Association for Computing Machinery.\nJasny, M., Ziegler, T., Kraska, T., Roehm, U., and Binnig, C. (2020). DB4ML - An in-\nmemory database kernel with machine learning support. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data , SIGMOD ’20, page\n159–173, New York, NY , USA. Association for Computing Machinery.\nKim, K., Jung, J., Seo, I., Han, W.-S., Choi, K., and Chong, J. (2022). Learned cardinality\nestimation: An in-depth study. In Proceedings of the 2022 International Conference\non Management of Data , SIGMOD ’22, page 1214–1227, New York, NY , USA. Asso-\nciation for Computing Machinery.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis, N. (2018). The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management\nof Data , SIGMOD ’18, page 489–504, New York, NY , USA. Association for Comput-\ning Machinery.\nL. S. Lustosa, H., C. Silva, A., N. R. da Silva, D., Valduriez, P., and Porto, F. (2021).\nSA VIME: An array DBMS for simulation analysis and ML models prediction. Journal\nof Information and Data Management , 11(3).\nLustosa, H. (2020). SAVIME:Enabling Declarative Array Processing in Memory . PhD\nthesis, National Laboratory of Scientific Computing.\nMarcus, R., Negi, P., Mao, H., Tatbul, N., Alizadeh, M., and Kraska, T. (2021). Bao:\nMaking learned query optimization practical. In Proceedings of the 2021 International\nConference on Management of Data , SIGMOD ’21, page 1275–1288, New York, NY ,\nUSA. Association for Computing Machinery.\nPereira, R., Souto, Y ., Chaves, A., Zorilla, R., Tsan, B., Rusu, F., Ogasawara, E., Ziviani,\nA., and Porto, F. (2021). Djensemble: A cost-based selection and allocation of a\ndisjoint ensemble of spatio-temporal models. In 33rd International Conference on\nScientific and Statistical Database Management , SSDBM 2021, page 226–231, New\nYork, NY , USA. Association for Computing Machinery.\nRousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation and validation\nof cluster analysis. Journal of computational and applied mathematics , 20:53–65.\nSandha, S. S., Cabrera, W., Al-Kateb, M., Nair, S., and Srivastava, M. (2019). In-database\ndistributed machine learning: Demonstration using teradata SQL engine. Proc. VLDB\nEndow. , 12(12):1854–1857.\nWoltmann, L., Hartmann, C., Thiele, M., Habich, D., and Lehner, W. (2019). Cardinality\nestimation with local deep learning models. In Proceedings of the Second Interna-\ntional Workshop on Exploiting Artificial Intelligence Techniques for Data Manage-\nment , aiDM ’19, New York, NY , USA. Association for Computing Machinery.Companion Proceedings of the 37thBrazilian Symposium on Data Bases September 2022 – B´ uzios, RJ, Brazil\n238",
  "textLength": 22778
}