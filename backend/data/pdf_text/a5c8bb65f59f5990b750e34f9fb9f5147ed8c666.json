{
  "paperId": "a5c8bb65f59f5990b750e34f9fb9f5147ed8c666",
  "title": "Meta-Learning Neural Bloom Filters",
  "pdfPath": "a5c8bb65f59f5990b750e34f9fb9f5147ed8c666.pdf",
  "text": "Meta-Learning Neural Bloom Filters\nJack W Rae1 2Sergey Bartunov1Timothy P Lillicrap1 2\nAbstract\nThere has been a recent trend in training neural\nnetworks to replace data structures that have been\ncrafted by hand, with an aim for faster execution,\nbetter accuracy, or greater compression. In this\nsetting, a neural data structure is instantiated by\ntraining a network over many epochs of its inputs\nuntil convergence. In applications where inputs\narrive at high throughput, or are ephemeral, train-\ning a network from scratch is not practical. This\nmotivates the need for few-shot neural data struc-\ntures. In this paper we explore the learning of\napproximate set membership over a set of data in\none-shot via meta-learning. We propose a novel\nmemory architecture, the Neural Bloom Filter,\nwhich is able to achieve signiﬁcant compression\ngains over classical Bloom Filters and existing\nmemory-augmented neural networks.\n1. Introduction\nOne of the simplest questions one can ask of a set of data\nis whether or not a given query is contained within it. Is q,\nour query, a member of S, our chosen set of observations?\nThis set membership query arises across many computing\ndomains; from databases, network routing, and ﬁrewalls.\nOne could query set membership by storing Sin its entirety\nand comparing qagainst each element. However, more\nspace-efﬁcient solutions exist.\nThe original and most widely implemented approximate\nset membership data-structure is the Bloom Filter (Bloom,\n1970). It works by storing sparse distributed codes, pro-\nduced from randomized hash functions, within a binary\nvector. The Bloom-ﬁlter trades off space for an allowed\nfalse positive rate, which arises due to hash collisions. How-\never its error is one-sided; if an element qis contained in S\nthen it will always be recognized. It never emits false nega-\n1DeepMind, London, UK2CoMPLEX, Computer Science,\nUniversity College London, London, UK. Correspondence to: Jack\nW Rae <jwrae@google.com >.\nProceedings of the 36thInternational Conference on Machine\nLearning , Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).tives. One can ﬁnd Bloom Filters embedded within a wide\nrange of production systems; from network security (Ger-\navand & Ahmadi, 2013), to block malicious IP addresses;\ndatabases , such as Google’s Bigtable (Chang et al., 2008),\nto avoid unnecessary disk lookups; cryptocurrency (Hearn\n& Corallo, 2012), to allow clients to ﬁlter irrelevant transac-\ntions; search , such as Facebook’s typeahead search (Adams,\n2010), to ﬁlter pages which do not contain query preﬁxes;\nandprogram veriﬁcation (Dillinger & Manolios, 2004), to\navoid recomputation over previously observed states.\nWhile the main appeal of Bloom Filters is favourable com-\npression, another important quality is the support for dy-\nnamic updates. New elements can be inserted in O(1)time.\nThis is not the case for all approximate set membership\ndata structures. For example, perfect hashing saves \u001940%\nspace over Bloom Filters but requires a pre-processing stage\nthat is polynomial-time in the number of elements to store\n(Dietzfelbinger & Pagh, 2008). Whilst the static set mem-\nbership problem is interesting, it limits the applicability of\nthe algorithm. For example, in a database application that\nis serving a high throughput of write operations, it may be\nintractable to regenerate the full data-structure upon each\nbatch of writes.\nWe thus focus on the data stream computation model\n(Muthukrishnan et al., 2005), where input observations are\nassumed to be ephemeral and can only be inspected a con-\nstant number of times — usually once. This captures many\nreal-world applications: network trafﬁc analysis, database\nquery serving, and reinforcement learning in complex do-\nmains. Devising an approximate set membership data struc-\nture that is not only more compressive than Bloom Filters,\nbut can be applied to either dynamic or static sets, could\nhave a signiﬁcant performance impact on modern computing\napplications. In this paper we investigate this problem using\nmemory-augmented neural networks and meta-learning.\nWe build upon the recently growing literature on using neu-\nral networks to replace algorithms that are conﬁgured by\nheuristics, or do not take advantage of the data distribution.\nFor example, Bloom Filters are indifferent to the data dis-\ntribution. They have near-optimal space efﬁciency when\ndata is drawn uniformly from a universe set (Carter et al.,\n1978) (maximal-entropy case) but (as we shall show) are\nsub-optimal when there is more structure. Prior studies onarXiv:1906.04304v1  [cs.LG]  10 Jun 2019\n\nMeta-Learning Neural Bloom Filters\nthis theme have investigated compiler optimization (Cum-\nmins et al., 2017), computation graph placement (Mirho-\nseini et al., 2017), and data index structures such as b-trees\n(Kraska et al., 2018). In the latter work, Kraska et al. (2018)\nexplicitly consider the problem of static set membership.\nBy training a neural network over a ﬁxed S(in their case,\nstring inputs) along with held-out negative examples, they\nobserve 36% space reduction over a conventional Bloom\nFilter1. Crucially this requires iterating over the storage set\nSa large number of times to embed its salient information\ninto the weights of a neural network classiﬁer. For a new S\nthis process would have to be repeated from scratch.\nInstead of learning from scratch, we draw inspiration from\nthe few-shot learning advances obtained by meta-learning\nmemory-augmented neural networks (Santoro et al., 2016;\nVinyals et al., 2016). In this setup, tasks are sampled from\na common distribution and a network learns to specialize\nto (learn) a given task with few examples. This matches\nvery well to applications where many Bloom Filters are\ninstantiated over different subsets of a common data distri-\nbution. For example, a Bigtable database usually contains\none Bloom Filter per SSTable ﬁle. For a large table that\ncontains Petabytes of data, say, there can be over 100;000\nseparate instantiated data-structures which share a common\nrow-key format and query distribution. Meta-learning al-\nlows us to exploit this common redundancy. We design a\ndatabase task with similar redundancy to investigate this\nexact application in Section 5.4.\nThe main contributions of this paper are (1) A new memory-\naugmented neural network architecture, the Neural Bloom\nFilter , which learns to write to memory using a distributed\nwrite scheme, and (2) An empirical evaluation of the Neu-\nral Bloom Filter meta-learned on one-shot approximate set\nmembership problems of varying structure. We compare\nwith the classical Bloom Filter alongside other memory-\naugmented neural networks such as the Differentiable Neu-\nral Computer (Graves et al., 2016) and Memory Networks\n(Sukhbaatar et al., 2015). We ﬁnd when there is no structure,\nthat differentiates the query set elements and queries, the\nNeural Bloom Filter learns a solution similar to a Bloom\nFilter derivative — a Bloom-g ﬁlter (Qiao et al., 2011) —\nbut when there is a lot of structure the solution can be con-\nsiderably more compressive (e.g. 30\u0002for a database task).\n2. Background\n2.1. Approximate Set Membership\nThe problem of exact set membership is to state whether or\nnot a given query qbelongs to a set of ndistinct observa-\n1The space saving increases to 41% when an additional trick is\nincorporated, in discretizing and re-scaling the classiﬁer outputs\nand treating the resulting function as a hash function to a bit-map.tionsS=fx1;:::;xngwherexiare drawn from a universe\nsetU. By counting the number of distinct subsets of size n\nit can be shown that any such exact set membership tester re-\nquires at least log2\u0000jUj\nn\u0001\nbits of space. To mitigate the space\ndependency onjUj, which can be prohibitively large, one\ncan relax the constraint on perfect correctness. Approximate\nset membership allows for a false positive rate of at most\n\u000f. Speciﬁcally we answer q2A(S)whereA(S)\u0013Sand\np(q2A(S)\u0000S)\u0014\u000f. It can be shown2the space require-\nment for approximate set membership of uniformly sampled\nobservations is at least nlog2(1\n\u000f)bits (Carter et al., 1978)\nwhich can be achieved with perfect hashing. So for a false\npositive rate of 1%, say, this amounts to 6:6bits per element.\nIn contrast to storing raw or compressed elements this can\nbe a huge space saving, for example ImageNet images re-\nquire 108KB per image on average when compressed with\nJPEG, an increase of over four orders of magnitude.\n2.2. Bloom Filter\nThe Bloom Filter (Bloom, 1970) is a data structure which\nsolves the dynamic approximate set membership problem\nwith near-optimal space complexity. It assumes access to\nk uniform hash functions hi:U! f1;:::;mg; i=\n1;:::;k such thatp(hi(x) =j) = 1=mindependent of\nprior hash values or input x. The Bloom Filter’s mem-\noryM2[0;1]mis a binary string of length mwhich is\ninitialized to zero. Writes are performed by hashing an\ninputxtoklocations in Mand setting the correspond-\ning bits to 1,M[hi(x)] 1;i= 1;:::;k . For a given\nqueryqthe Bloom Filter returns true if all corresponding\nhashed locations are set to 1and returns false otherwise:\nQuery (M;q) :=M[h1(q)]^M[h2(q)]^:::^M[hk(q)].\nThis incurs zero false negatives, as any previously observed\ninput must have enabled the corresponding bits in M, how-\never there can be false positives due to hash collisions. To\nachieve a false positive rate of \u000fwith minimal space one can\nsetk= log2(1=\u000f)andm=nlog2(1=\u000f) log2e, where\neis Euler’s number. The resulting space is a factor of\nlog2e\u00191:44from the optimal static lower bound given by\nCarter et al. (1978).\n2.3. Memory-Augmented Neural Networks\nRecurrent neural networks such as LSTMs retain a small\namount of memory via the recurrent state. However this\nis usually tied to the number of trainable parameters in the\nmodel. There has been recent interest in augmenting neural\nnetworks with a larger external memory. The method for do-\ning so, via a differentiable write and read interface, was ﬁrst\npopularized by the Neural Turing Machine (NTM) (Graves\net al., 2014) and its successor the Differentiable Neural\n2By counting the minimal number of A(S)sets required to\ncover all S\u001aU.\n\nMeta-Learning Neural Bloom Filters\nComputer (DNC) (Graves et al., 2016) in the context of\nlearning algorithms, and by Memory Networks (Sukhbaatar\net al., 2015) in the context of question answering. Memory\nNetworks store embeddings of the input in separate rows\nof a memory matrix M. Reads are performed via a dif-\nferentiable content-based addressing operation. Given a\nquery embedding qwe take some similarity measure D(e.g.\ncosine similarity, or negative euclidean distance) against\neach row in memory and apply a softmax to obtain a soft\naddress vector a/eD(q;M ). A read is then a weighted\nsum over memory r aTM. The NTM and DNC use\nthe same content-based read mechanism, but also learns\nto write. These models can arbitrate whether to write to\nslots in memory with similar content (content-based writes),\ntemporally ordered locations, or unused memory.\nWhen it comes to capacity, there has been consideration\nto scaling both the DNC and Memory Networks to very\nlarge sizes using sparse read and write operations (Rae et al.,\n2016; Chandar et al., 2016). However another way to in-\ncrease the capacity is to increase the amount of compression\nwhich occurs in memory. Memory Nets can create com-\npressive representations of each input, but cannot compress\njointly over multiple inputs because they are hard-wired\nto write one slot per timestep. The NTM and DNC can\ncompress over multiple slots in memory because they can\narbitrate writes across multiple locations, but in practice\nseem to choose very sharp read and write addresses. The\nKanerva Machine (Wu et al., 2018a;b) tackles memory-wide\ncompression using a distributed write scheme to jointly com-\npose and compress its memory contents. The model uses\ncontent-based addressing over a separate learnable address-\ning matrixA, instead of the memory M, and thus learns\nwhere to write. We take inspiration from this scheme.\n3. Model\nOne approach to learning set membership in one-shot would\nbe to use a recurrent neural network, such as an LSTM or\nDNC. Here, the model sequentially ingests the Nelements\nto store, answers a set of queries using the ﬁnal state, and is\ntrained by BPTT. Whilst this is a general training approach,\nand the model may learn a compressive solution, it does\nnot scale well to larger number of elements. Even when\nN= 1000 , backpropagating over a sequence of this length\ninduces computational and optimization challenges. For\nlarger values this quickly becomes intractable. Alternatively\none could store an embedding of each element xi2Sin\na slot-based Memory Network. This is more scalable as it\navoids BPTT, because the gradients of each input can be\ncalculated in parallel. However Memory Networks are not\na space efﬁcient solution (as shown in in Section 5) because\nthere is no joint compression of inputs.\nThis motivates the proposed memory model, the NeuralAlgorithm 1 Neural Bloom Filter\n1:def controller(x):\n2:z fenc(x) // Input embedding\n3:q fq(z) // Query word\n4:a \u001b(qTA) // Memory address\n5:w fw(z) // Write word\n6:def write(x):\n7:a;w controller (x)\n8:Mt+1 Mt+waT// Additive write\n9:def read(x):\n10:a;w;z controller (x)\n11:r ﬂatten (M\fa) // Read words\n12:o fout([r;w;z ]) // Output logit\nBloom Filter. Brieﬂy, the network is augmented with a\nreal-valued memory matrix. The network addresses mem-\nory by classifying which memory slots to read or write to\nvia a softmax, conditioned on the input. We can think of\nthis as a continuous analogue to the Bloom Filter’s hash\nfunction; because it is learned the network can co-locate or\nseparate inputs to improve performance. The network up-\ndates memory with a simple additive write operation — i.e.\nno multiplicative gating or squashing — to the addressed\nlocations. An additive write operation can be seen as a con-\ntinuous analogue to the the Bloom Filter’s logical OR write\noperation. Crucially, the additive write scheme allows us to\ntrain the model without BPTT — this is because gradients\nwith respect to the write words @L=@w = (@L=@M )Ta\ncan be computed in parallel. Reads involve a component-\nwise multiplication of address and memory (analogous to\nthe selection of locations in the Bloom Filter via hashing),\nbut instead of projecting this down to a scalar with a ﬁxed\nfunction, we pass this through an MLP to obtain a scalar\nfamiliarity logit. The network is fully differentiable, allows\nfor memories to be stored in a distributed fashion across\nslots, and is quite simple e.g. in comparison to DNCs.\nThe full architecture depicted in Figure 1 consists of a\ncontroller network which encodes the input to an embed-\ndingz fenc(x)and transforms this to a write word\nw fw(z)and a query q fq(z). The address over\nmemory is computed via a softmax a \u001b(qTA)over the\ncontent-based attention between qand a learnable address\nmatrixA. Here,\u001bdenotes a softmax. The network thus\nlearns where to place elements or overlap elements based on\ntheir content, we can think of this as a soft and differentiable\nrelaxation of the uniform hashing families incorporated by\nthe Bloom Filter (see Appendix A.3 for further discussion).\nAwrite is performed by running the controller to obtain a\nwrite wordwand address a, and then additively writing w\ntoM, weighted by the address a,Mt+1 Mt+waT. The\n\nMeta-Learning Neural Bloom Filters\nM\nqA\nw\nxa owr\nz\nz\nw\nFigure 1. Overview of the Neural Bloom Filter architecture.\nsimple additive write ensures that the resulting memory is\ninvariant to input ordering (as addition is commutative) and\nwe do not have to backpropagate-through-time (BPTT) over\nsequential writes — gradients can be computed in parallel.\nAread is performed by also running the controller network\nto obtainz;w; andaand component-wise multiplying the\naddressawithM,r M\fa. The read words rare\nfed through an MLP along with the residual inputs wand\nzand are projected to a single scalar logit, indicating the\nfamiliarity signal. We found this to be more powerful than\nthe conventional read operation r aTMused by the\nDNC and Memory Networks, as it allows for non-linear\ninteractions between rows in memory at the time of read.\nSee Algorithm 1 for an overview of the operations.\nTo give an example network conﬁguration, we chose fenc\nto be a 3-layer CNN in the case of image inputs, and a 128-\nhidden-unit LSTM in the case of text inputs. We chose fw\nandfqto be an MLP with a single hidden layer of size 128,\nfollowed by layer normalization, and foutto be a 3-layer\nMLP with residual connections. We used a leaky ReLU as\nthe non-linearity. Although the described model uses dense\noperations that scale linearly with the memory size m, we\ndiscuss how the model could be implemented for O(logm)\ntime reads and writes using sparse attention and read/write\noperations, in Appendix A.1. Furthermore the model’s rela-\ntion to uniform hashing is discussed in Appendix A.3.\n4. Space Complexity\nIn this section we discuss space lower bounds for the approx-\nimate set membership problem when there is some structure\nto the storage or query set. This can help us formalise why\nand where neural networks may be able to beat classical\nlower bounds to this problem.\nThenlog2(1=\u000f)lower bound from Carter et al. (1978) as-\nsumes that all subsets S\u001aUof sizen, and all queries\nq2Uhave equal probability. Whilst it is instructive to\nbound this maximum-entropy scenario, which we can think\nof as ‘worst case’, most applications of approximate set\nmembership e.g. web cache sharing, querying databases, or\nspell-checking, involve sets and queries that are not sampled\nuniformly. For example, the elements within a given set maybe highly dependent, there may be a power-law distribution\nover queries, or the queries and sets themselves may not be\nsampled independently.\nA more general space lower bound can be deﬁned by an\ninformation theoretic argument from communication com-\nplexity (Yao, 1979). Namely, approximate set membership\ncan be framed as a two-party communication problem be-\ntween Alice, who observes the set Sand Bob, who observes\na queryq. They can agree on a shared policy \u0005in which\nto communicate. For given inputs S;qthey can produce\na transcriptAS;q= \u0005(S;q)2Z which can be processed\ng:Z! 0;1such that P(g(AS;q) = 1jq62S)\u0014\u000f. Bar-\nYossef et al. (2004) shows that the maximum transcript\nsize is greater than the mutual information between the\ninputs and transcript: maxS;qjAS;qj \u0015I(S;q;AS;q) =\nH(S;q)\u0000H(S;qjAS;q). Thus we note problems where\nwe may be able to use less space than the classical lower\nbound are cases where the entropy H(S;q)is small, e.g. our\nsets are highly non-uniform, or cases where H(S;qjAS;q)\nis large, which signiﬁes that many query and set pairs can\nbe solved with the same transcript.\n5. Experiments\nOur experiments explore scenarios where set membership\ncan be learned in one-shot with improved compression over\nthe classical Bloom Filter. We consider tasks with vary-\ning levels of structure in the storage sets Sand queries q.\nWe compare the Neural Bloom Filter with three memory-\naugmented neural networks, the LSTM, DNC, and Memory\nNetwork, that are all able to write storage sets in one-shot.\nThe training setup follows the memory-augmented meta-\nlearning training scheme of Vinyals et al. (2016), only here\nthe task is familiarity classiﬁcation versus image classiﬁca-\ntion. The network samples tasks which involve classifying\nfamiliarity for a given storage set. Meta-learning occurs\nas a two-speed process, where the model quickly learns to\nrecognize a given storage set Swithin a training episode via\nwriting to a memory or state, and the model slowly learns to\nimprove this fast-learning process by optimizing the model\nparameters\u0012over multiple tasks. We detail the training\nroutine in Algorithm 2.\n\nMeta-Learning Neural Bloom Filters\nClass-Based Familiarity Uniform Instance-Based Familiarity Non-Uniform Instance-Based Familiarity\n(a) (b) (c)\n— Memory Network         — Bloom Filter     — DNC         — LSTM         — Neural Bloom FilterFailed at task for\nlarger no. items\nFigure 2. Sampling strategies on MNIST. Space consumption at 1% FPR.\nAlgorithm 2 Meta-Learning Training\n1:LetStraindenote the distribution over sets to store.\n2:LetQtraindenote the distribution over queries.\n3:fori= 1tomax train steps do\n4: Sample task:\n5: Sample set to store: S\u0018Strain\n6: Sampletqueries:x1;:::;xt\u0018Qtrain\n7: Targets:yj= 1ifxj2Selse0;j= 1;:::;t\n8: Write entries to memory: M fwrite\n\u0012 (S)\n9: Calculate logits: oj=fread\n\u0012(M;xj);j= 1;:::;t\n10: XE loss:L=Pt\nj=1yjlogoj+ (1\u0000yj)(1\u0000logoj)\n11: Backprop through queries and writes: dL=d\u0012\n12: Update parameters: \u0012i+1 Optimizer (\u0012i;dL=d\u0012 )\n13:end for\nFor the RNN baselines (LSTM and DNC) the write opera-\ntion corresponds to unrolling the network over the inputs\nand outputting the ﬁnal state. For these models, the query\nnetwork is simply an MLP classiﬁer which receives the con-\ncatenated ﬁnal state and query, and outputs a scalar logit.\nFor the Memory Network, inputs are stored in individual\nslots and the familiarity signal is computed from the max-\nimum content-based attention value. The Neural Bloom\nFilter read and write operations are deﬁned in Algorithm 1.\n5.1. Space Comparison\nWe compared the space (in bits) of the model’s memory\n(or state) to a Bloom Filter at a given false positive rate\nand0%false negative rate. The false positive rate is mea-\nsured empirically over a sample of 50;000queries for the\nlearned models; for the Bloom Filter we employ the ana-\nlytical false positive rate. Beating a Bloom Filter’s space\nusage with the analytical false positive rate implies better\nperformance for any given Bloom Filter library version (as\nactual Bloom Filter hash functions are not uniform), thus\nthe comparison is reasonable. For each model we sweep\nover hyper-parameters relating to model size to obtain theirsmallest operating size at the desired false positive rate (for\nthe full set, see Appendix D). Because the neural models\ncan emit false negatives, we store these in a (ideally small)\nbackup Bloom Filter, as proposed by Kraska et al. (2018);\nMitzenmacher (2018a). We account for the space of this\nbackup Bloom Filter, and add it to the space usage of the\nmodel’s memory for parity (See Appendix B for further dis-\ncussion). The neural network must learn to output a small\nstate in one-shot that can serve set membership queries at a\ngiven false positive rate, and emit a small enough number\nof false negatives such that the backup ﬁlter is also small,\nand the total size is considerably less than a Bloom Filter.\n5.2. Sampling Strategies on MNIST\nTo understand what kinds of scenarios neural networks may\nbe more (or less) compressive than classical Bloom Filters,\nwe consider three simple set membership tasks that have\na graded level of structure to the storage sets and queries.\nConcretely, they differ in the sampling distribution of stor-\nage setsStrainand queriesQtrain. However all problems\nare approximate set membership tasks that can be solved by\na Bloom Filter. The tasks are (1) Class-based familiarity , a\nhighly structured task where each set of images is sampled\nwith the constraint that they arise from the same randomly-\nselected class. (2) Non-uniform instance-based familiarity ,\na moderately structured task where the images are sam-\npled without replacement from an exponential distribution.\n(3) Uniform instance-based familiarity , a completely un-\nstructured task where each subset contains images sampled\nuniformly without replacement. For each task we varied\nthe size of the sample set to store, and calculated the space\n(in bits) of each model’s state at a ﬁxed false positive rate\nof1%and a false negative rate of 0%. We used relatively\nsmall storage set sizes (e.g. 100\u00001000 ) to start with, as\nthis highlights that some RNN-based approaches struggle to\ntrain over larger set sizes, before progressing to larger sets\nin subsequent sections. See Appendix E for further details\non the task setup. In the class-based sampling task we see in\nFigure 2a that the DNC, LSTM and Neural Bloom Filter are\n\nMeta-Learning Neural Bloom Filters\nAddress\nMemory\nContents\n(a) (b) (c)\nFull Model\n Constant write words:\nFigure 3. Memory access analysis. Three different learned solutions to class-based familiarity. We train three Neural Bloom Filter\nvariants, with a succession of simpliﬁed read and write mechanisms. Each model contains 10memory slots and the memory addressing\nweights aand contents \u0016Mare visualised, broken down by class. Solutions share broad correspondence to known algorithms: (a) Bloom-g\nﬁlters, (b) Bloom Filters, (c) Perfect hashing.\nable to signiﬁcantly outperform the classical Bloom Filter\nwhen images are sampled by class. The Memory Network\nis able to solve the task with a word size of only 2, however\nthis corresponds to a far greater number of bits per element,\n64versus the Bloom Filter’s 9:8(to a total size of 4:8kb),\nand so the overall size was prohibitive. The DNC, LSTM,\nand Neural Bloom Filter are able to solve the task with a\nstorage set size of 500at1:1kb ,217b, and 382b; a4:3\u0002,\n22\u0002, and 12\u0002saving respectively. For the non-uniform\nsampling task in Figure 2b we see the Bloom Filter is prefer-\nable for less than 500stored elements, but is overtaken\nthereafter. At 1000 elements the DNC, LSTM, and Neural\nBloom Filter consume 7:9kb,7:7kb, and 6:8kb respectively\nwhich corresponds to a 17:6%,19:7%, and 28:6%reduction\nover the 9:6kb Bloom Filter. In the uniform sampling task\nshown in Figure 2c, there is no structure to the sampling of\nS. The two architectures which rely on BPTT essentially\nfail to solve the task at some threshold of storage size. The\nNeural Bloom Filter solves it with 6:8kb (using a memory\nsize of 50and word size of 2). The overall conclusion from\nthese sets of experiments is that the classical Bloom Filter\nworks best when there is no structure to the data, however\nwhen there is (e.g. skewed data, or highly dependent sets\nthat share common attributes) we do see signiﬁcant space\nsavings.\n5.3. Memory Access Analysis\nWe wanted to understand how the Neural Bloom Filter uses\nits memory, and in particular how its learned solutions may\ncorrespond to classical algorithms. We inspected the mem-\nory contents (what was stored to memory) and addressing\nweights (where it was stored) for a small model of 10mem-ory slots and a word size of 2, trained on the MNIST class-\nbased familiarity task. We plot this for each class label, and\ncompare the pattern of memory usage to two other models\nthat use increasingly simpler read and write operations: (1)\nan ablated model with constant write words w 1, and (2)\nan ablated model with w 1anda linear read operator\nr aTM.\nThe full model, shown in Figure 3a learns to place some\nclasses in particular slots, e.g. class 1!slot5, however\nmost are distributed. Inspecting the memory contents, it is\nclear the write word encodes a unique 2D token for each\nclass. This solution bears resemblance with Bloom-g Fil-\nters (Qiao et al., 2011) where elements are spread across a\nsmaller memory with the same hashing scheme as Bloom\nFilters, but a unique token is stored in each slot instead of a\nconstant 1-bit value. With the model ablated to store only\n1s in Figure 3b we see it uses semantic addressing codes\nfor some classes (e.g. 0and1) and distributed addresses for\nother classes. E.g. for class 3the model prefers to uniformly\nspread its writes across memory slot 1,4, and 8. The model\nsolution is similar to that of Bloom Filters, with distributed\naddressing codes as a solution — but no information in\nthe written words themselves. When we force the read op-\neration to be linear in Figure 3c, the network maps each\ninput class to a unique slot in memory. This solution has a\ncorrespondence with perfect hashing. In conclusion, with\nsmall changes to the read/write operations we see the Neural\nBloom Filter learn different algorithmic solutions.\n5.4. Database Queries\nWe look at a task inspired by database interactions. NoSQL\ndatabases, such as Bigtable and Cassandra, use a single\nstring-valued row-key, which is used to index the data. The\n\nMeta-Learning Neural Bloom Filters\n5% 1% 0.1%\nNeural Bloom Filter 871b 1.5kb 24.5kb\nBloom Filter 31.2kb 47.9kb 72.2kb\nCuckoo Filter 33.1kb 45.3kb 62.6kb\nTable 1. Database task . Storing 5000 row-key strings for a target\nfalse positive rate.\ndatabase is comprised of a union of ﬁles (e.g. SSTables) stor-\ning contiguous row-key chunks. Bloom Filters are used to\ndetermine whether a given query qlies within the stored set.\nWe emulate this setup by constructing a universe of strings,\nthat is alphabetically ordered, and by sampling contiguous\nranges (to represent a given SSTable). Queries are sampled\nuniformly from the universe set of strings. We choose the\n2:5Munique tokens in the GigaWord v5 news corpus to be\nour universe as this consists of structured natural data and\nsome noisy or irregular strings.\nWe consider the task of storing sorted string sets of size 5000 .\nWe train the Neural Bloom Filter to several desired false\npositive rates ( 5%;1%;0:1%) and used a backup Bloom\nFilter to guarantee 0%false negative rate. We also trained\nLSTMs and DNCs for comparison, but they failed to learn a\nsolution to the task after several days of training; optimizing\ninsertions via BPTT over a sequence of length 5000 did\nnot result in a remotely usable solution. The Neural Bloom\nFilter avoids BPTT via its simple additive write scheme, and\nso it learned to solve the task quite naturally. As such, we\ncompare the Neural Bloom Filter solely to classical data\nstructures: Bloom Filters and Cuckoo Filters. In Table 1\nwe see a signiﬁcant space reduction of 3\u000040\u0002, where\nthe margin grows with increasing permitted false positive\nrates. Since memory is an expensive component within\nproduction databases (in contrast to disk, say), this memory\nspace saving could translate to a non-trivial cost reduction.\nWe note that a storage size of 5000 may appear small, but\nis relevant to the NOSQL database scenario where disk\nﬁles (e.g. SSTables) are typically sharded to be several\nmegabytes in size, to avoid issues with compaction. E.g.\nif the stored values were of size 10kB per row, we would\nexpect 5000 unique keys or less in an average Bigtable\nSSTable.\nOne further consideration for production deployment is the\nability to extrapolate to larger storage set sizes during evalu-\nation. We investigate this for the Neural Bloom Filter on the\nsame database task, and compare it to an LSTM. To ensure\nboth models train, we set the maximum training storage set\nsize to 200and evaluate up to sizes 250, a modest 25% size\nincrease. We ﬁnd that the Neural Bloom Filter uses up to\n3\u0002less space than the LSTM and the neural models are able\nto extrapolate to larger set sizes than those observed during\ntraining (see Appendix F Figure 4). Whilst the performance\neventually degrades when the training limit size is exceeded,it is not catastrophic for either the LSTM or Neural Bloom\nFilter.\n5.5. Timing benchmark\nWe have principally focused on space comparisons in this\npaper, we now consider speed for the database task de-\nscribed in the prior section. We measure latency as the\nwall-clock time to complete a single insertion or query of a\nrow-key string of length 64. We also measure throughput\nas the reciprocal wall-clock time of inserting or querying\n10;000strings. We use a common encoder architecture for\nthe neural models, a 128-hidden-unit character LSTM. We\nbenchmark the models on the CPU (Intel(R) Xeon(R) CPU\nE5-1650 v2 @ 3.50GHz) and on the GPU (NVIDIA Quadro\nP6000) with models implemented in TensorFlow without\nany model-speciﬁc optimizations. We compare to empir-\nical timing results published in a query-optimized Bloom\nFilter variant (Chen et al., 2007). We include the Learned\nIndex from (Kraska et al., 2018) to contrast timings with a\nmodel that is not one-shot. The architecture is simply the\nLSTM character encoder; inserts are performed via gradient\ndescent. The number of gradient-descent steps to obtain\nconvergence is domain-dependent, we chose 50steps in our\ntiming benchmarks. The Learned Index queries are obtained\nby running the character LSTM over the input and classify-\ning familiarity — and thus query metrics are identical to the\nLSTM baseline.\nWe see in Table 2. that the combined query and insert latency\nof the Neural Bloom Filter and LSTM sits at 5ms on the\nCPU, around 400\u0002slower than the classical Bloom Filter.\nThe Learned Index contains a much larger latency of 780ms\ndue to the sequential application of gradients. For all neural\nmodels, latency is not improved when operations are run\non the GPU. However when multiple queries are received,\nthe throughput of GPU-based neural models surpasses the\nclassical Bloom Filter due to efﬁcient concurrency of the\ndense linear algebra operations. This leads to the conclusion\nthat a Neural Bloom Filter could be deployed in scenar-\nios with high query load without a catastrophic decrease in\nthroughput, if GPU devices are available. For insertions we\nsee a bigger separation between the one-shot models: the\nLSTM and Neural Bloom Filter. Whilst all neural models\nare uncompetitive on the CPU, the Neural Bloom Filter sur-\npasses the Bloom Filter’s insertion throughput when placed\non the GPU, with 101Kinsertions per second (IPS). The\nLSTM runs at 4:6KIPS, one order of magnitude slower,\nbecause writes are serial, and the Learned Index structure is\ntwo orders of magnitude slower at 816IPS due to sequential\ngradient computations. The beneﬁts of the Neural Bloom\nFilter’s simple write scheme are apparent here.\n\nMeta-Learning Neural Bloom Filters\nQuery + Insert Latency Query Throughput (QPS) Insert Throughput (IPS)\nCPU GPU CPU GPU CPU GPU\nBloom Filter* 0.02ms - 61K - 61K -\nNeural Bloom Filter 5.1ms 13ms 3.5K 105K 3.2K 101K\nLSTM 5.0ms 13ms 3.1K 107K 2.4K 4.6K\nLearned Index (Kraska et al., 2018) 780ms 1.36s 3.1K 107K 25 816\nTable 2. Latency for a single query, and throughput for a batch of 10,000 queries. *Query-efﬁcient Bloom Filter from Chen et al. (2007).\n6. Related Work\nThere have been a large number of Bloom Filter variants\npublished; from Counting Bloom Filters which support dele-\ntions (Fan et al., 2000), Bloomier Filters which store func-\ntions vs sets (Chazelle et al., 2004), Compressed Bloom\nFilters which use arithmetic encoding to compress the stor-\nage set (Mitzenmacher, 2002), and Cuckoo Filters which\nuse cuckoo hashing to reduce redundancy within the storage\nvector (Fan et al., 2014). Although some of these variants\nfocus on better compression, they do not achieve this by\nspecializing to the data distribution.\nOne of the few works which address data-dependence are\nWeighted Bloom Filters (Bruck et al., 2006; Wang et al.,\n2015). They work by modulating the number of hash func-\ntions used to store or query each input, dependent on its stor-\nage and query frequency. This requires estimating a large\nnumber of separate storage and query frequencies. This\napproach can be useful for imbalanced data distributions,\nsuch as the non-uniform instance-based MNIST familiar-\nity task. However it cannot take advantage of dependent\nsets, such as the class-based MNIST familiarity task, or the\ndatabase query task. We see the Neural Bloom Filter is more\ncompressive in all settings.\nSterne (2012) proposes a neurally-inspired set membership\ndata-structure that works by replacing the randomized hash\nfunctions with a randomly-wired computation graph of OR\nandAND gates. The false positive rate is controlled analyt-\nically by modulating the number of gates and the overall\nmemory size. However there is no learning or specialization\nto the data with this setup. Bogacz & Brown (2003) investi-\ngates a learnable neural familiarity module, which serves as\na biologically plausible model of familiarity mechanisms in\nthe brain, namely within the perirhinal cortex. However this\nhas not shown to be empirically effective at exact matching.\nKraska et al. (2018) consider the use of a neural network to\nclassify the membership of queries to a ﬁxed set S. Here\nthe network itself is more akin to a perfect hashing setup\nwhere multiple epochs are required to ﬁnd a succinct holis-\ntic representation of the set, which is embedded into the\nweights of the network. In their case this search is per-\nformed by gradient-based optimization. We emulate their\nexperimental comparison approach but instead propose amemory architecture that represents the set as activations in\nmemory, versus weights in a network.\nMitzenmacher (2018a) discusses the beneﬁts and draw-\nbacks of a learned Bloom Filter; distinguishing the empirical\nfalse positive rate over the distribution of sets Sversus the\nconditional false positive rate of the model given a particular\nsetS. In this paper we focus on the empirical false positive\nrate because we wish to exploit redundancy in the data and\nquery distribution. Mitzenmacher (2018b) also considers an\nalternate way to combine classical and learned Bloom Fil-\nters by ‘sandwiching’ the learned model with pre-ﬁlter and\npost-ﬁlter classical Bloom Filters to further reduce space.\n7. Conclusion\nIn many situations neural networks are not a suitable replace-\nment to Bloom Filters and their variants. The Bloom Filter\nis robust to changes in data distribution because it delivers\na bounded false positive rate for any sampled subset. How-\never in this paper we consider the questions, “When might\na single-shot neural network provide better compression\nthan a Bloom Filter?”. We see that a model which uses an\nexternal memory with an adaptable capacity, avoids BPTT\nwith a feed-forward write scheme, and learns to address its\nmemory, is the most promising option in contrast to popular\nmemory models such as DNCs and LSTMs. We term this\nmodel the Neural Bloom Filter due to the analogous incor-\nporation of a hashing scheme, commutative write scheme,\nand multiplicative read mechanism.\nThe Neural Bloom Filter relies on settings where we have\nan off-line dataset (both of stored elements and queries) that\nwe can meta-learn over. In the case of a large database we\nthink this is warranted, a database with 100K separate set\nmembership data structures will beneﬁt from a single (or\nperiodic) meta-learning training routine that can run on a\nsingle machine and sample from the currently stored data,\ngenerating a large number of efﬁcient data-structures. We\nenvisage the space cost of the network to be amortized by\nsharing it across many neural Bloom Filters, and the time-\ncost of executing the network to be offset by the continuous\nacceleration of dense linear algebra on modern hardware,\nand the ability to batch writes and queries efﬁciently. A\npromising future direction would be to investigate the feasi-\nbility of this approach in a production system.\n\nMeta-Learning Neural Bloom Filters\nAcknowledgments\nWe thank Peter Dayan, Yori Zwols, Yan Wu, Joel Leibo,\nGreg Wayne, Andras Gyorgy, Charles Blundell, Daan\nWeirstra, Pushmeet Kohli, and Tor Lattimor for their in-\nsights during this project.\nReferences\nAdams, K. The life of a typeahead query, 2010. URL\nhttps://www.facebook.com/Engineering/\nvideos/432864835468/ . [Online, accessed\n01-August-2018].\nBar-Yossef, Z., Jayram, T. S., Kumar, R., and Sivakumar,\nD. An information statistics approach to data stream and\ncommunication complexity. Journal of Computer and\nSystem Sciences , 68(4):702–732, 2004.\nBloom, B. H. Space/time trade-offs in hash coding with\nallowable errors. Communications of the ACM , 13(7):\n422–426, 1970.\nBogacz, R. and Brown, M. W. Comparison of computational\nmodels of familiarity discrimination in the perirhinal cor-\ntex.Hippocampus , 13(4):494–524, 2003.\nBruck, J., Gao, J., and Jiang, A. Weighted bloom ﬁlter. In\nInformation Theory, 2006 IEEE International Symposium\non, pp. 2304–2308. IEEE, 2006.\nCarter, L., Floyd, R., Gill, J., Markowsky, G., and Wegman,\nM. Exact and approximate membership testers. In Pro-\nceedings of the tenth annual ACM symposium on Theory\nof computing , pp. 59–65. ACM, 1978.\nChandar, S., Ahn, S., Larochelle, H., Vincent, P., Tesauro,\nG., and Bengio, Y . Hierarchical memory networks. arXiv\npreprint arXiv:1605.07427 , 2016.\nChang, F., Dean, J., Ghemawat, S., Hsieh, W. C., Wallach,\nD. A., Burrows, M., Chandra, T., Fikes, A., and Gruber,\nR. E. Bigtable: A distributed storage system for structured\ndata. ACM Transactions on Computer Systems (TOCS) ,\n26(2):4, 2008.\nChazelle, B., Kilian, J., Rubinfeld, R., and Tal, A. The\nbloomier ﬁlter: an efﬁcient data structure for static sup-\nport lookup tables. In Proceedings of the ﬁfteenth an-\nnual ACM-SIAM symposium on Discrete algorithms , pp.\n30–39. Society for Industrial and Applied Mathematics,\n2004.\nChen, Y ., Kumar, A., and Xu, J. J. A new design of bloom\nﬁlter for packet inspection speedup. In Global Telecom-\nmunications Conference, 2007. GLOBECOM’07. IEEE ,\npp. 1–5. IEEE, 2007.Cummins, C., Petoumenos, P., Wang, Z., and Leather,\nH. End-to-end deep learning of optimization heuristics.\nInParallel Architectures and Compilation Techniques\n(PACT), 2017 26th International Conference on , pp. 219–\n232. IEEE, 2017.\nDatar, M., Immorlica, N., Indyk, P., and Mirrokni, V . S.\nLocality-sensitive hashing scheme based on p-stable dis-\ntributions. In Proceedings of the twentieth annual sym-\nposium on Computational geometry , pp. 253–262. ACM,\n2004.\nDietzfelbinger, M. and Pagh, R. Succinct data structures for\nretrieval and approximate membership. In International\nColloquium on Automata, Languages, and Programming ,\npp. 385–396. Springer, 2008.\nDillinger, P. C. and Manolios, P. Bloom ﬁlters in proba-\nbilistic veriﬁcation. In International Conference on For-\nmal Methods in Computer-Aided Design , pp. 367–381.\nSpringer, 2004.\nFan, B., Andersen, D. G., Kaminsky, M., and Mitzenmacher,\nM. D. Cuckoo ﬁlter: Practically better than bloom. In Pro-\nceedings of the 10th ACM International on Conference\non emerging Networking Experiments and Technologies ,\npp. 75–88. ACM, 2014.\nFan, L., Cao, P., Almeida, J., and Broder, A. Z. Summary\ncache: a scalable wide-area web cache sharing protocol.\nIEEE/ACM Transactions on Networking (TON) , 8(3):281–\n293, 2000.\nGeravand, S. and Ahmadi, M. Bloom ﬁlter applications in\nnetwork security: A state-of-the-art survey. Computer\nNetworks , 57(18):4047–4064, 2013.\nGraves, A., Wayne, G., and Danihelka, I. Neural turing\nmachines. arXiv preprint arXiv:1410.5401 , 2014.\nGraves, A., Wayne, G., Reynolds, M., Harley, T., Dani-\nhelka, I., Grabska-Barwi ´nska, A., Colmenarejo, S. G.,\nGrefenstette, E., Ramalho, T., Agapiou, J., et al. Hybrid\ncomputing using a neural network with dynamic external\nmemory. Nature , 538(7626):471, 2016.\nHearn, M. and Corallo, M. BIPS: Connection Bloom ﬁlter-\ning, 2012. URL https://github.com/bitcoin/\nbips/blob/master/bip-0037.mediawiki .\n[Online; accessed 01-August-2018].\nJouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal,\nG., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers,\nA., et al. In-datacenter performance analysis of a tensor\nprocessing unit. In Computer Architecture (ISCA), 2017\nACM/IEEE 44th Annual International Symposium on , pp.\n1–12. IEEE, 2017.\n\nMeta-Learning Neural Bloom Filters\nKaiser, Ł., Nachum, O., Roy, A., and Bengio, S. Learning\nto remember rare events. In International Conference on\nLearning Representations , 2017.\nKraska, T., Beutel, A., Chi, E. H., Dean, J., and Polyzotis,\nN. The case for learned index structures. In Proceedings\nof the 2018 International Conference on Management of\nData , pp. 489–504. ACM, 2018.\nMirhoseini, A., Pham, H., Le, Q. V ., Steiner, B., Larsen, R.,\nZhou, Y ., Kumar, N., Norouzi, M., Bengio, S., and Dean,\nJ. Device placement optimization with reinforcement\nlearning. In Proceedings of the 34th International Confer-\nence on Machine Learning , volume 70, pp. 2430–2439.\nPMLR, 2017.\nMitzenmacher, M. Compressed bloom ﬁlters. IEEE/ACM\nTransactions on Networking (TON) , 10(5):604–612,\n2002.\nMitzenmacher, M. A model for learned bloom ﬁlters and re-\nlated structures. arXiv preprint arXiv:1802.00884 , 2018a.\nMitzenmacher, M. Optimizing learned bloom ﬁlters by\nsandwiching. arXiv preprint arXiv:1803.01474 , 2018b.\nMuthukrishnan, S. et al. Data streams: Algorithms and appli-\ncations. Foundations and Trends in Theoretical Computer\nScience , 1(2):117–236, 2005.\nQiao, Y ., Li, T., and Chen, S. One memory access bloom\nﬁlters and their generalization. In INFOCOM, 2011 Pro-\nceedings IEEE , pp. 1745–1753. IEEE, 2011.\nRae, J. W., Hunt, J. J., Danihelka, I., Harley, T., Senior,\nA. W., Wayne, G., Graves, A., and Lillicrap, T. P. Scaling\nmemory-augmented neural networks with sparse reads\nand writes. In Advances in Neural Information Processing\nSystems , pp. 3621–3629, 2016.\nSantoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and\nLillicrap, T. Meta-learning with memory-augmented neu-\nral networks. In Proceedings of the 33rd International\nConference on Machine Learning , pp. 1842–1850, 2016.\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,\nQ., Hinton, G., and Dean, J. Outrageously large neural\nnetworks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538 , 2017.\nSterne, P. Efﬁcient and robust associative memory from\na generalized bloom ﬁlter. Biological cybernetics , 106\n(4-5):271–281, 2012.\nSukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. End-\nto-end memory networks. In Proceedings of the 28th\nInternational Conference on Neural Information Process-\ning Systems-Volume 2 , pp. 2440–2448. MIT Press, 2015.Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K.,\nand Wierstra, D. Matching networks for one shot learning.\nInProceedings of the 30th International Conference on\nNeural Information Processing Systems , pp. 3637–3645.\nCurran Associates Inc., 2016.\nWang, X., Ji, Y ., Dang, Z., Zheng, X., and Zhao, B. Im-\nproved weighted bloom ﬁlter and space lower bound anal-\nysis of algorithms for approximated membership query-\ning. In International Conference on Database Systems\nfor Advanced Applications , pp. 346–362. Springer, 2015.\nWu, Y ., Wayne, G., Graves, A., and Lillicrap, T. The kanerva\nmachine: A generative distributed memory. In Interna-\ntional Conference on Learning Representations , 2018a.\nWu, Y ., Wayne, G., Gregor, K., and Lillicrap, T. Learning\nattractor dynamics for generative memory. In Advances in\nNeural Information Processing Systems , pp. 9401–9410,\n2018b.\nYao, A. C.-C. Some complexity questions related to dis-\ntributive computing (preliminary report). In Proceedings\nof the eleventh annual ACM symposium on Theory of\ncomputing , pp. 209–213. ACM, 1979.\n\nMeta-Learning Neural Bloom Filters\nA. Further Model Details\nA.1. Efﬁcient addressing\nWe discuss some implementation tricks that could be em-\nployed for a production system.\nFirstly the original model description deﬁnes the addressing\nmatrixAto be trainable. This ties the number of parameters\nin the network to the memory size. It may be preferable\nto train the model at a given memory size and evaluate\nfor larger memory sizes. One way to achieve this is by\nallowing the addressing matrix Ato be non-trainable. We\nexperiment with this, allowing A\u0018N(0;I)to be a ﬁxed\nsample of Gaussian random variables. We can think of these\nas point on a sphere in high dimensional space, the controller\nnetwork must learn to organize inputs into separate buckets\nacross the surface of the sphere.\nTo make the addressing more efﬁcient for larger memory\nsizes, we experiment with sparsiﬁcation of the addressing\nsoftmax by preserving only the top k components. We\ndenote this sparse softmax \u001bk(\u0001). When using a sparse\naddress, we ﬁnd the network can ﬁxate on a subset of rows.\nThis observation is common to prior sparse addressing work\n(Shazeer et al., 2017). We ﬁnd sphering the query vector,\noften dubbed whitening, remedies this (see Appendix G for\nan ablation). The modiﬁed sparse architecture variant is\nillustrated in Algorithm 3.\nAlgorithm 3 Sparse Neural Bloom Filter\n1:def sparse controller(x):\n2:z fenc(x)\n3:s fq(z) // Raw query word\n4:q movingzca(q) // Spherical query\n5:a \u001bk(qTA) // Sparse address\n6:w fw(z)\n7:def sparse write(x):\n8:a;w sparse controller (x)\n9:Mt+1[aidx] Mt[aidx] +waT\nval\n10:def sparse read(x):\n11:a;w;z sparse controller (x)\n12:r M[aidx]\faval\n13:o fout([r;w;z ])\nOne can avoid the linear-time distance computation qTAin\nthe addressing operation \u001bk(qTA)by using an approximate\nk-nearest neighbour index, such as locality-sensitive hashing\n(Datar et al., 2004), to extract the nearest neighbours from\nAinO(logm)time. The use of an approximate nearest\nneighbour index has been empirically considered for scal-\ning memory-augmented neural networks (Rae et al., 2016;\nKaiser et al., 2017) however this was used for attention onMdirectly. As Mis dynamic the knn requires frequent\nre-building as memories are stored or modiﬁed. This archi-\ntecture is simpler — Ais ﬁxed and so the approximate knn\ncan be built once.\nTo ensure the serialized size of the network (which can be\nshared across many memory instantiations) is independent\nof the number of slots in memory mwe can avoid storing\nA. In the instance that it is not trainable, and is simply a\nﬁxed sample of random variables that are generated from a\ndeterministic random number generator — we can instead\nstore a set of integer seeds that can be used to re-generate\nthe rows ofA. We can let the i-th seedci, say represented\nas a 16-bit integer, correspond to the set of 16rows with\nindices 16i;16i+ 1;:::; 16i+ 15 . If these rows need to be\naccessed, they can be regenerated on-the-ﬂy by ci. The total\nmemory cost of Ais thusmbits, wheremis the number of\nmemory slots3.\nPutting these two together it is possible to query and write\nto a Neural Bloom Filter with mmemory slots inO(logm)\ntime, where the network consumes O(1)space. It is worth\nnoting, however, the Neural Bloom Filter’s memory is of-\nten much smaller than the corresponding classical Bloom\nFilter’s memory, and in many of our experiments is even\nsmaller than the number of unique elements to store. Thus\ndense matrix multiplication can still be preferable - espe-\ncially due to its acceleration on GPUs and TPUs (Jouppi\net al., 2017) - and a dense representation of Ais not in-\nhibitory. As model optimization can become application-\nspeciﬁc, we do not focus on these implementation details\nand use the model in its simplest setting with dense matrix\noperations.\nA.2. Moving ZCA\nThe moving ZCA was computed by taking moving averages\nof the ﬁrst and second moment, calculating the ZCA matrix\nand updating a moving average projection matrix \u0012zca. This\nis only done during training, at evaluation time \u0012zcais ﬁxed.\nWe describe the update below for completeness.\nInput:s fq(z) (1)\n\u0016t+1 \r\u0016t+ (1\u0000\r)\u0016s 1st moment EMA (2)\n\u0006t+1 \r\u0006t+ (1\u0000\r)sTs2nd moment EMA (3)\nU;s; svd(\u0006\u0000\u00162) Singular values (4)\nW UUT=p\n(s) ZCA matrix (5)\n\u0012zca \u0011\u0012zca+ (1\u0000\u0011)W ZCA EMA (6)\nq s\u0012zca Projected query (7)\nIn practice we do not compute the singular value decompo-\nsition at each time step to save computational resources, but\n3One can replace 16with32if there are more than one million\nslots\n\nMeta-Learning Neural Bloom Filters\ninstead calculate it and update \u0012everyTsteps. We scale the\ndiscount in this case \u00110=\u0011=T.\nA.3. Relation to uniform hashing\nWe can think of the decorrelation of s, along with the sparse\ncontent-based attention with A, as a hash function that maps\nsto several indices in M. For moderate dimension sizes\nofs(256, say) we note that the Gaussian samples in A\nlie close to the surface of a sphere, uniformly scattered\nacross it. Ifq, the decorrelated query, were to be Gaussian\nthen the marginal distribution of nearest neighbours rows\ninAwill be uniform. If we chose the number of nearest\nneighbours k= 1 then this implies the slots in Mare\nselected independently with uniform probability. This is\nthe exact hash function speciﬁcation that Bloom Filters\nassume. Instead we use a continuous (as we choose k>1)\napproximation (as we decorrelate s!qvs Gaussianize) to\nthis uniform hashing scheme, so it is differentiable and the\nnetwork can learn to shape query representations.\nB. Space Comparison\nFor each task we compare the model’s memory size, in\nbits, at a given false positive rate — usually chosen to be\n1%. For our neural networks which output a probability\np=f(x)one could select an operating point \u001c\u000fsuch that the\nfalse positive rate is \u000f. In all of our experiments the neural\nnetwork outputs a memory (state) swhich characterizes\nthe storage set. Let us say SPACE(f, \u000f)is the minimum\nsize ofs, in bits, for the network to achieve an average\nfalse positive rate of \u000f. We could compare SPACE(f,\u000f)\nwithSPACE(Bloom Filter, \u000f)directly, but this would\nnot be a fair comparison as our network fcan emit false\nnegatives.\nTo remedy this, we employ the same scheme as Kraska\net al. (2018) where we use a ‘backup’ Bloom Filter with\nfalse positive rate \u000eto store all false negatives. When\nf(x)< \u001c\u000fwe query the backup Bloom Filter. Because\nthe overall false positive rate is \u000f+ (1\u0000\u000f)\u000e, to achieve\na false positive rate of at most \u000b(say 1%) we can set\n\u000f=\u000e=\u000b=2. The number of elements stored in the\nbackup bloom ﬁlter is equal to the number of false neg-\natives, denoted nfn. Thus the total space can be calcu-\nlated, TOTAL SPACE(f,\u000b) = SPACE(f,\u000b\n2) +nfn\n*SPACE(Bloom Filter,\u000b\n2). We compare this quan-\ntity for different storage set sizes.\nC. Model Size\nFor the MNIST experiments we used a 3-layer convolutional\nneural network with 64ﬁlters followed by a two-layer feed-\nforward network with 64&128 hidden-layers respectively.\nThe number of trainable parameters in the Neural BloomFilter (including the encoder) is 243;437which amounts to\n7:8Mb at 32-bit precision. We did not optimize the encoder\narchitecture to be lean, as we consider it part of the library\nin a sense. For example, we do not count the size of the\nhashing library that an implemented Bloom Filter relies on,\nwhich may have a chain of dependencies, or the package size\nof TensorFlow used for our experiments. Nevertheless we\ncan reason that when the Neural Bloom Filter is 4kb smaller\nthan the classical, such as for the non-uniform instance-\nbased familiarity in Figure 2b, we would expect to see a net\ngain if we have a collection of at least 1;950data-structures.\nWe imagine this could be optimized quite signiﬁcantly, by\nusing 16-bit precision and perhaps using more convolution\nlayers or smaller feed-forward linear operations.\nFor the database experiments we used an LSTM character\nencoder with 256hidden units followed by another 256\nfeed-forward layer. The number of trainable parameters in\nthe Neural Bloom Filter 419;339which amounts to 13Mb.\nOne could imagine optimizing this by switching to a GRU\nor investigating temporal convolutions as encoders.\nD. Hyper-Parameters\nWe swept over the following hyper-parameters, over the\nrange of memory sizes displayed for each task. We com-\nputed the best model parameters by selecting those which\nresulted in a model consuming the least space as deﬁned in\nAppendix B. This depends on model performance as well as\nstate size. The Memory Networks memory size was ﬁxed\nto equal the input size (as the model does not arbitrate what\ninputs to avoid writing).\nMemory Size (DNC, NBF) f2, 4, 8, 16, 32, 64g\nWord Size (MemNets, DNC, NBF) f2, 4, 6, 8, 10g\nHidden Size (LSTM) f2, 4, 8, 16, 32, 64g\nSphering Decay \u0011(NBF) f0.9, 0.95, 0.99g\nLearning Rate (all) f1e-4, 5e-5g\nTable 3. Hyper-parameters considered\nE. Experiment Details\nFor the class-based familiarity task, and uniform sampling\ntask, the model was trained on the training set and evaluated\non the test set. For the class-based task sampling, a class is\nsampled at random and Sis formed from a random subset of\nimages from that class. The queries qare chosen uniformly\nfrom eitherSor from images of a different class.\nFor the non-uniform instance-based familiarity task we sam-\npled images from an exponential distribution. Speciﬁcally\nwe used a ﬁx permutation of the training images, and from\nthat ordering chose p(ithimage )/0:999ifor the images to\nstore. The query images were selected uniformly. We used\n\nMeta-Learning Neural Bloom Filters\nFigure 4. Database extrapolation task . Models are trained up to sets of size 200(dashed line). We see extrapolation to larger set sizes\non test set, but performance degrades. Neural architectures perform best for larger allowed false positive rates.\na ﬁxed permutation (or shufﬂe) of the images to ensure most\nprobability mass was not placed on images of a certain class.\nI.e. by the natural ordering of the dataset we would have\notherwise almost always sampled 0images. This would be\nconfounding task non-uniformity for other latent structure\nto the sets. Because the network needed to relate the im-\nage to its frequency of occurence for task, the models were\nevaluated on the training set. This is reasonable as we are\nnot wishing for the model to visually generalize to unseen\nelements in the setting of this exact-familiarity task. We\nspeciﬁcally want the network weights to compress a map of\nimage to probability of storage.\nFor the database task a universe of 2:5Munique tokens\nwere extracted from GigaWord v5. We shufﬂed the tokens\nand placed 2:3M in a training set and 250K in a test set.\nThese sets were then sorted alphabetically. A random sub-\nset, representing an SSTable, was sampled by choosing a\nrandom start index and selecting the next nelements, which\nform our set S. Queries are sampled uniformly at random\nfrom the universe set. Models are trained on the training set\nand evaluated on the test set.\nF. Database Extrapolation Task\nWe investigate whether neural models are able to extrapolate\nto larger test sizes. Using the database task setup, where\neach set contains a contiguous set of sorted strings; we train\nboth the Neural Bloom Filter and LSTM on sets of sizes\n2 - 200. We then evaluate on sets up to 250, i.e. a 25%\nincrease over what is observed during training. This is to\nemulate the scenario that we train on a selection of databse\ntablets, but during evaluation we may observe some tablets\nthat are slightly larger than those in the training set. Both\nthe LSTM and Neural Bloom Filter are able to solve the\ntask, with the Neural Bloom Filter using signiﬁcantly less\nspace for the larger allowed false positive rate of 5% and\n1%. We do see the models’ error increase as it surpasses the\nmaximum training set size, however it is not catastrophic.\nAnother interesting trend is noticeable; the neural models\nhave higher utility for larger allowed false positive rates.This may be because of the difﬁculty in training the models\nto an extremely low accuracy.\nG. Effect of Sphering\nWe see the beneﬁt of sphering in Figure 5 where the con-\nverged validation performance ends up at a higher state.\nInvestigating the proportion of memory ﬁlled after all ele-\nments have been written in Figure 6, we see the model uses\nquite a small proportion of its memory slots. This is likely\ndue to the network ﬁxating on rows it has accessed with\nsparse addressing, and ignoring rows it has otherwise never\ntouched — a phenomena noted in Shazeer et al. (2017).\nThe model ﬁnds a local minima in continually storing and\naccessing the same rows in memory. The effect of sphering\nis that the query now appears to be Gaussian (up to the ﬁrst\ntwo moments) and so the nearest neighbour in the address\nmatrix A (which is initialized to Gaussian random variables)\nwill be close to uniform. This results in a more uniform\nmemory access (as seen in Figure 6) which signiﬁcantly\naids performance (as seen in Figure 5).\n0 200000 400000 600000 800000\nTraining Steps0.40.50.60.70.80.91.0Validation AUCSphering enabled\nSphering disabled\nFigure 5. For sparse addresses, sphering enables the model to learn\nthe task of set membership to high accuracy.\nH. Timing Benchmark\nWe use the Neural Bloom Filter network architecture for the\nlarge database task (Table 1). The network uses an encoder\nLSTM with 256hidden units over the characters, and feeds\n\nMeta-Learning Neural Bloom Filters\n0 200000 400000 600000 800000\nTraining Steps1520253035404550% Memory usedSphering enabled\nSphering disabled\nFigure 6. For sparse addresses, sphering the query vector leads to\nfewer collisions across memory slots and thus a higher utilization\nof memory.\nthis through a 256fully connected layer to encode the input.\nA two-layer 256-hidden-unit MLP is used as the query archi-\ntecture. The memory and word size is 8and4respectively,\nand so the majority of the compute is spent in the encoder\nand query network. We compare this with an LSTM con-\ntaining 32hidden units. We benchmark the single-query\nlatency of the network alongside the throughput of a batch\nof queries, and a batch of inserts. The Neural Bloom Filter\nand LSTM is implemented in TensorFlow without any cus-\ntom kernels or specialized code. We benchmark it on the\nCPU (Intel(R) Xeon(R) CPU E5-1650 v2 @ 3.50GHz) and\na GPU (NVIDIA Quadro P6000). We compare to empirical\ntiming results published in a query-optimized Bloom Filter\nvariant (Chen et al., 2007).\nIt is worth noting, in several Bloom Filter applications, the\nactual query latency is not in the critical path of computation.\nFor example, for a distributed database, the network latency\nand disk access latency for one tablet can be orders of mag-\nnitude greater than the in-memory latency of a Bloom Filter\nquery. For this reason, we have not made run-time a point\nof focus in this study, and it is implicitly assumed that the\nneural network is trading off greater latency for less space.\nHowever it is worth checking whether run-time could be\nprohibitive.",
  "textLength": 61552
}