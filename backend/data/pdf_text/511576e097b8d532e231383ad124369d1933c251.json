{
  "paperId": "511576e097b8d532e231383ad124369d1933c251",
  "title": "APEX: A High-Performance Learned Index on Persistent Memory",
  "pdfPath": "511576e097b8d532e231383ad124369d1933c251.pdf",
  "text": "APEX: A High-Performance Learned Index on Persistent\nMemory (Extended Version)\nBaotong Lu\nThe Chinese University of Hong Kong\nbtlu@cse.cuhk.edu.hkJialin Ding\nMassachusetts Institute of Technology\njialind@mit.eduEric Lo\nThe Chinese University of Hong Kong\nericlo@cse.cuhk.edu.hk\nUmar Farooq Minhas\nMicrosoft Research\nufminhas@microsoft.comTianzheng Wang\nSimon Fraser University\ntzwang@sfu.ca\nABSTRACT\nThe recently released persistent memory (PM) offers high perfor-\nmance, persistence, and is cheaper than DRAM. This opens up new\npossibilities for indexes that operate and persist data directly on the\nmemory bus. Recent learned indexes exploit data distribution and\nhave shown great potential for some workloads. However, none sup-\nport persistence or instant recovery, and existing PM-based indexes\ntypically evolve B+-trees without considering learned indexes.\nThis paper proposes APEX, a new PM-optimized learned index\nthat offers high performance, persistence, concurrency, and instant\nrecovery. APEX is based on ALEX, a state-of-the-art updatable\nlearned index, to combine and adapt the best of past PM optimiza-\ntions and learned indexes, allowing it to reduce PM accesses while\nstill exploiting machine learning. Our evaluation on Intel DCPMM\nshows that APEX can perform up to âˆ¼15Ã—better than existing PM\nindexes and can recover from failures in âˆ¼42ms.\n1 INTRODUCTION\nModern data systems use fast memory-optimized indexes (e.g., B+-\ntrees) [ 4,30,33,38,44] for high performance. As data size grows,\nhowever, scalability is limited by DRAMâ€™s high cost and low capac-\nity: OLTP indexes alone can occupy >55% of total memory [ 60].\nByte-addressable persistent memory (PM) [ 10,47,55] offers per-\nsistence, high capacity, and lower cost compared to DRAM. The\nrecently released Intel Optane DCPMM [ 21] is available in 128â€“\n512GB DIMMs, yet 128GB DRAM DIMMs are rare and priced âˆ¼5â€“7Ã—\nhigher than 128GB DCPMM [ 1]. Although PM is more expensive\nthan SSDs, it offers better performance, making it an attractive\noption to complement limited/expensive DRAM. These features\nhave led to numerous PM-optimized indexes [ 2,7,8,12,20,28,35â€“\n37,40,42,51,58,61,62] that directly persist and operate on PM.\nSome also support instant recovery to reduce down time.\nMost (if not all) existing PM indexes are based on B+-trees or\nhash tables which are agnostic to data distribution. As demonstrated\nby recent learned indexes [ 11,14,15,17,19,25,26,34,41,43,48,52,\n59], indexes can be implemented as machine learning models that\npredict the location of target values given a search key. Suppose\nall values are stored in an array sorted by key order, a linear model\nThis document is an extended version of \"APEX: A High-Performance Learned Index\non Persistent Memory\", which will appear in The 48th International Conference on\nVery Large Data Bases (VLDB 2022). This document is freely provided under Creative\nCommons.\n 0 2 4 6 8\n 1 4  8  16  24LongitudesMillion ops/s\nNumber of threadsUnsafe\nNaive\n 0 0.4 0.8 1.2 1.6\nLonglat Longitudes\nData setMillion ops/sUnsafe\nNaiveFigure 1: Insert scalability (left) and single-thread through-\nput (right) of ALEX [14] on PM. Naively using PMDK ( Naive )\nlimits performance due to PMâ€™s limited bandwidth. Directly\nrunning it on PM ( Unsafe ) further loses crash consistency.\ntrained from the data can directly output the valueâ€™s position in the\narray. If keys are continuous integers (e.g., 0â€“100 million), the value\nmapped to key ğ‘˜can be accessed by array[k] . Such model-based\nsearch gives ğ‘‚(1)complexity and the entire index is as simple as a\nlinear function. Some learned indexes (e.g., ALEX [ 14]) also support\nupdates and inserts. They typically use a hierarchy of models [ 14,\n17,26] that form a tree-like structure to improve accuracy. However,\nindividual nodes could be much bigger (e.g., 16MB in ALEX [ 14]),\nleading to very high fanout (e.g., 216) and low tree depth (e.g., 2),\nmaking search operations lightweight even for very large data sizes.\n1.1 When Learned Indexing Meets Persistent\nMemory: The Old Tricks No Longer Work!\nWe observe learned indexing is a natural fit for PM: Real PM (Optane\nDCPMM) exhibitsâˆ¼3â€“14Ã—lower bandwidth than DRAM [ 32,57],\nwhereas model-based search is especially good at reducing mem-\nory accesses. But learned indexes were designed based on DRAM\nwithout considering PM properties, and prior PM indexes did not\nleverage machine learning. It remains challenging for learned in-\ndexes to work well on PM.\nChallenge 1: Scalability and Throughput. Although learned in-\ndexes are frugal in bandwidth usage for lookups, they still exhibit\nexcessive PM accesses for inserts. This is because learned indexes\nrequire data (key-value pairs) be maintained in sorted order, which\nmay require shifting records for inserts. Figure 1 shows its impact\nby running ALEX [ 14]â€”a state-of-the-art learned indexâ€”on PM\nwithout any optimizations (denoted as Unsafe ).1Since PM exhibits\nasymmetric read/write bandwidth with writes being 3â€“4 Ã—lower,\n1Original ALEX does not support concurrency. On each core we run an ALEX instance\nthat works on a data partition.arXiv:2105.00683v3  [cs.DB]  6 Dec 2021\n\nfrequent record shifting can easily exhaust write bandwidth and\neventually limit insert scalability and throughput. This problem\nwas confirmed by recent work [ 6]. Similar issues were also found in\nB+trees. A common solution is to use unsorted nodes [ 2,35,42,58]\nthat accept inserts in an append-only manner, but require linear\nsearch for lookups. This is reasonable for small B+-tree nodes (e.g.,\n256Bâ€“1KB), but for model-based operations to work well, it is criti-\ncal to use large nodes (e.g., up to 16MB in ALEX) with sorted data.\nStructural modification operations (SMOs) become more expensive\nwith more PM accesses and higher synchronization cost: typically\nonly one thread can work on a node during an SMO.\nChallenge 2: Persistence and Crash Consistency. A key feature of\npersistent indexes is to ensure correct recovery across restarts and\npower cycles. Prior learned indexes were mostly based on DRAM\nand did not consider persistence issues. Simply running a learned\nindex on PM does not guarantee consistency. Any operations that\ninvolve writing more than eight bytes could result in inconsistencies\nas currently only 8-byte PM writes are atomic. Although recent\nwork [ 22,29] provides easy ways to convert DRAM indexes to work\non PM with crash consistency, they are either not general-purpose,\nor incur very high overhead. For example, PMDK [ 22]â€”the de-\nfacto standard PM libraryâ€”allows developers to wrap operations\nin transactions to easily achieve crash consistency. As shown in\nFigure 1, compared to the Unsafe variant, this approach ( Naive )\nscales poorly with low single-thread throughput because it uses\nheavyweight logging which incurs write amplification and extra\npersistence overhead, depleting the scarce PM bandwidth.\n1.2 APEX\nThis paper presents APEX ,apersistent l earned inde xthat retains the\nbenefits of learned indexes while guaranteeing crash consistency\non PM and supporting instant recovery and scalable concurrency.\nAPEX is carefully designed to address the challenges. 1We ob-\nserve a data (leaf) node in a learned index can be regarded as a hash\ntable where a linear model is effectively used as an order-preserving\nhash function. A collision results when the model predicts the same\nposition for multiple keys. Based on this observation, we develop a\ncollision-resolving mechanism, probe-and-stash, to retain efficient\nmodel-based search while avoiding excessive PM writes. APEX also\nachieves crash consistency for all operations with low overhead.\n2To reduce synchronization and SMO overheads, APEX adopts\nvariable node sizes with smaller data nodes (256KB) and larger in-\nner nodes (up to 16MB) as SMOs on inner nodes are relatively rare.\nThe former allows lightweight SMOs; the latter allows shallower\ntrees with high fanout. We also design a lightweight concurrency\ncontrol protocol to reduce synchronization overhead. 3Similar\nto prior work, APEX stores certain frequently used metadata in\nDRAM to reduce the impact of PMâ€™s higher latency and lower\nbandwidth. Unlike many other PM indexes, however, APEX does\nso while providing instant recovery. The key is to ensure DRAM-\nresident metadata can be re-constructed quickly and most recovery\nwork can be deferred.\nOur evaluation using realistic workloads shows that APEX is up\ntoâˆ¼15Ã—faster as compared to the state-of-the-art PM indexes [ 2,8,\n20,35,42,61] while achieving high scalability and instant recovery.\nWe made APEX open-source at https://github.com/baotonglu/apex .We make four contributions. First, APEX brings persistence to\nlearned indexes which is a missing but a necessary feature [ 27],\nbringing learned indexing another step closer to practical adoption.\nSecond, APEX combines the best of PM and machine learning (high\nperformance with a small storage footprint). Third, we propose a\nset of techniques to implement learned indexes on real PM. APEX\nis based on ALEX, but our techniques (e.g., probe-and-stash and\njudicious use of DRAM) are general-purpose and applicable to other\nindexes. Last, we provide a comprehensive evaluation and compare\nAPEX with prior PM indexes to validate our design decisions.\n2 BACKGROUND AND RELATED WORK\nIn this section, we provide background on PM hardware, existing\ntechniques in PM-optimized indexes, and learned indexes.\n2.1 Intel Optane DC Persistent Memory\nAmong various scalable PM types [ 10,47,55], only Intel Optane\nDCPMM based on 3D XPoint is commercially available; so we target\nit in this paper. DCPMM can run in Memory orApp Direct [57]\nmodes. The former leverages PMâ€™s high capacity to present bigger\nbut slower volatile memory with DRAM as a hardware-managed\ncache. The latter allows software to judiciously use DRAM and\nPM with persistence. We leverage PMâ€™s persistence using the App\nDirect mode and frugally use DRAM to boost performance. Both\nDRAM and PM are behind volatile CPU caches, and the CPU may\nreorder writes to PM. For correctness, software must explicitly issue\ncacheline flushes ( CLWB /CLFLUSH ) and fences to force data to the\nADR domain [ 23], which includes a write buffer and a write pending\nqueue with persistence guarantees upon failures [ 57]. Once in ADR,\nnot necessarily in PM media, data is considered persisted.\nAlthough writing DCPMM media exhibits higher latency than\nreads, recent work [ 50,57] showed that end-to-end read latency is\noften higher as a write commits once it reaches ADR while PM reads\noften require fetching data from raw media unless cached. DCPMM\nexhibitsâˆ¼300ns random read latency, âˆ¼4Ã—slower than DRAMâ€™s; the\nend-to-end write latency can be lower than âˆ¼100ns [57]. DCPMM\nbandwidth is also lower than DRAMâ€™s. Compared to DRAM, it ex-\nhibits 3Ã—/8Ã—lower sequential/random read bandwidth and 11 Ã—/14Ã—\nlower sequential/random write bandwidth. Its write bandwidth is\nalso 3â€“4Ã—lower than read bandwidth. DCPMMâ€™s internal access\ngranularity is 256 bytes (one â€œXPLineâ€) [ 57]. To serve a 64-byte\ncacheline read, it internally loads 256B and returns the requested\n64B. DCPMM also writes in 256B units. Thus, <256B accesses lead\nto read and write amplification that wastes bandwidth. For high\nperformance, software should consider PM access in 256B units.\n2.2 PM-Optimized Indexing Techniques\nNumerous PM-optimized indexes [ 2,7,8,20,35â€“37,40,42,58,61,\n62] have been proposed based on B+-trees and hash tables. They\nmainly optimize for crash consistency and performance. We give\nan overview of the key techniques proposed by prior PM indexes\nwhich APEX adapts for learned indexing in later sections.\nReducing PM Accesses. Many PM B+-trees [ 2,7,35,42,58]\nuse unsorted leaf nodes to avoid shifting records upon inserts. A\nrecord can be inserted into any free slot in a node; free space is\ntracked by a bitmap. This reduces PM writes but requires linear scan\n\nfor point queries. To alleviate such cost, FPTree [ 42] accompanies\neach key with a fingerprint (a one-byte hash of the key) to predict\nif a key possibly exists. Lookups then only access records with\nmatching fingerprints, removing unnecessary PM accesses. Some\nhash tables [ 36,62] use additional stash buckets to handle collisions.\nThis reduces expensive PM accesses in the main table that would\notherwise be necessary (e.g., chaining requires more dynamic PM\nallocations and linear probing may issue many reads). The tradeoff\nis lookups may need to check stashes in addition to the main table,\nbut this can be largely alleviated using fingerprints for stashes [ 36].\nInstant Recovery. PMâ€™s byte-addressability and persistence\nallow placing the entire index on the memory bus and recover from\nfailures without much work [ 2,7,20,36,62], reducing service down\ntime. Lazy recovery [ 20,36] is a well-known technique to realize\nthis. Here we describe a recent approach [ 36]. The index maintains\na global version number ğºin PM and each PM block (e.g., an inner\nor leaf node) is associated with a local version number ğ¿. Upon\nrestart,ğºis incremented by one, after which the system is ready\nto serve requests. Individual nodes are only recovered later by the\naccessing threads if ğºandğ¿do not match. This way, the â€œrealâ€\nrecovery work is amortized over runtime, in exchange for instant\nand bounded recovery time (incrementing one integer).\n(Selective) Persistence. To overcome PMâ€™s lower performance,\nsome PM indexes [ 35,42,61] leverage DRAM by placing recon-\nstructable data (e.g., B+-tree inner nodes) in DRAM for fast search.\nUpon restart, the DRAM-resident data must be reconstructed from\ndata in PM, before the system can start to serve requests. This is\ndoable for B+-trees using bulk loading algorithms. The downside is\nrecovery time scales with data size, sacrificing instant recovery.\nConcurrency Control. Both lock-free and lock-based designs\nhave been proposed for PM indexes. Traditionally, lock-free pro-\ngramming has been difficult on PM. Recent work [ 2,53] has demon-\nstrated the feasibility of building PM-based lock-free indexes more\neasily, but the overhead is not negligible [ 32]. Traditional node-level\nlocking causes exclusive accesses and incur more PM writes when\nacquiring/releasing read locks. So lock-based designs are often com-\nbined with lock-free read and/or hardware transactional memory\n(HTM) [ 35,42] to reduce PM writes. FPTree [ 42] uses HTM for in-\nner nodes and locking for leaf nodes. HTM performs well under low\ncontention, but is not robust due to issues like spurious aborts [ 32].\nSome proposals [ 36,61] use optimistic locking that requires locking\nfor writes, and reads can proceed without holding a lock but must\nverify the read data is consistent. This is usually done by checking\na version number associated with the data item did not change,\nwhich if happened, would cause the read operation to be retried.\n2.3 Learned Indexes\nLearned indexes build machine learning models that predict the\nposition of a given key. For example, one may train a linear model\nğ‘ğ‘œğ‘ =âŒŠğ‘Ã—ğ‘˜ğ‘’ğ‘¦+ğ‘âŒ‹whereğ‘andğ‘are parameters learned from\nprediction accuracy. A learned index may use more complex models\n(e.g., neural networks), build a hierarchy of simple models, or both.\nMany learned indexes [ 11,14,15,17,19,25,26,34,41,43,48,52,\n56,59] are based on this idea, but most are read-only. We focus on\nupdatable OLTP learned indexes, but to the best of our knowledge,\nnone support persistence and very few support concurrency [48].\nğ‘ğ‘œğ‘ =ğ‘˜Ã—4ğ‘˜âˆˆ[0,1)\nğ‘ğ‘œğ‘ =(ğ‘˜âˆ’!\")Ã—16Rootnodeğ‘˜âˆˆ[14,12)InnernodeCğ‘ğ‘œğ‘ =50ğ‘˜âˆ’25DatanodeBğ‘˜âˆˆ[12,1)ğ‘ğ‘œğ‘ =40ğ‘˜+0.3DatanodeAğ‘˜âˆˆ[0,14)key10.500â„14â„12â„341CDFFigure 2: ALEX structure: in addition to models, inner/leaf\n(data) nodes store pointers/records in gapped arrays [14].\nDesign Overview. We build APEX on ALEX [ 14], a fast, updat-\nable learned index. We use ALEX to present our approach to crash\nconsistency and concurrency on PM. Many of our techniques are\napplicable to other learned indexes; doing so is interesting future\nwork. ALEX uses a hierarchy of simple linear regression models.\nThe structure of ALEX is also called recursive model index (RMI).\nIt uses gapped arrays (i.e., arrays that leave empty slots between\nrecords to efficiently absorb future inserts) to store fixed-size keys\nand payloads sorted by keys.. As shown in Figure 2, each inner\nnode stores a linear model with ğ‘šchild pointers ( ğ‘š=4in Figure\n2). Traversal starts from the root node which uses its linear model\nto predict the next child node (model) to probe, until reaching a leaf\n(data) node. Each data node stores a linear model and two aligned\ngapped arrays (GAs), one for keys and one for payloads to reduce\nsearch distance and cache misses (Figure 2 shows one for brevity).\nModels and Root-to-Leaf Traversals. ALEX uses ordinary\nleast squares linear regression with closed-form formula to train\ndata node models. We find that linear regression models work well\nin most datasets except one extremely non-linear data set ( FBin\nSection 6). In such highly non-linear cases, complex models can\npossibly provide higher accuracy (and thus fewer PM accesses) but\nthey also come with higher overhead for training and inference.\nHow to balance model accuracy and the extra cost of complex\nmodels is an interesting future direction. Inner node models can\npartition key space flexibly. For example, in Figure 2 the root node\nmodel divides the key space [0,1)into four equally-sized subspaces,\nand each subspace is assigned to a child node; all keys in [0,1/4)\nare placed in data node A. In other words, inner node models do\nnotpredict which child node a key falls in, but guide how ALEX\nplaces keys in child nodes. Thus, model â€œpredictionsâ€ in inner nodes\nduring traversal are accurate by construction.\nSearch. To probe a data node, ALEX uses the stored model to\npredict a position into the GA. The search succeeds if the predicted\nposition contains the target key. Otherwise, ALEX uses exponential\nsearch to find the key. If the keys are uniformly distributed (easy\nto fit by the model), and the number of keys is smaller than the\nmaximum data node size, one may use a (large) data node to accom-\nmodate all records to drastically reduce inner node size. Otherwise,\nALEX recursively partitions the key space to ğ‘šsubspaces until the\nkeys in each subspace can be modeled well by a linear model. As\nFigure 2 shows, since subspace [1\n4,1\n2)is non-linear, another node is\ncreated hoping that the new subspaces are â€œlinearâ€ enough, while\n[1\n2,1)is already linear, so ALEX uses one data node for it.\n\nInsert. An insert in data node first uses the model to predict the\ninsert position in gapped array and may employ the exponential\nsearch to locate the proper position. Two cases are possible upon\ninsert: (1) insert into the dense region, or (2) insert to a gap. Case\n(1) requires the elements shifts while case (2) needs to fill all con-\nsecutive gaps with the adjacent keys to enable exponential search;\nBoth cases incur excessive PM writes. Such write amplification can\neasily saturate PM write bandwidth, limit the performance and\nmake efficient crash consistency (without logging) impossible.\nSMOs. ALEX defines node density as the fraction of filled GA\nslots, and further defines lower density ğ‘‘ğ‘™(0.6 by default) and upper\ndensityğ‘‘ğ‘¢(0.8 by default). Once the nodeâ€™s density is >ğ‘‘ğ‘¢, an\nSMO is triggered, because insert performance will deteriorate with\nfewer gaps. An SMO can expand or split a node. An expansion\nenlarges the nodeâ€™s GA. So data nodes in ALEX are variable-sized.\nA split is carried out like in a B+-tree. For example, when data\nnode B in Figure 2 is split, two new nodes are allocated and trained\nwith data partitioned across these two nodes. Then the two right-\nmost pointers in the root node which originally point to B will\nrespectively point to the two new data nodes. If node A is also split,\nthere is no spare pointer in the root node. ALEX may double the\nroot nodeâ€™s size or create a new inner node with two child data\nnodes (split downwards), each contains a split of A. Deletion is\nsimple because it can just leave a new gap. ALEX may perform\nnode contraction and merge to improve space utilization. ALEX uses\nbuilt-in cost models to make SMO decisions using various statistics.\nMore details about the cost model can be found elsewhere [ 14]. Both\ninner and data nodes are variable-sized and can be much larger (e.g.,\nup to 16MB) than nodes in a B+-tree. Using large nodes is important\nfor reducing tree depth, but may significantly slow down SMOs\nas model retraining takes more time and more data needs to be\ninserted to the new nodes. On multicore CPUs, this could present a\nscalability bottleneck as an SMO will block concurrent accesses to\nthe node. Inner node SMO does not require model retraining. As\nnoted earlier, inner node models are only used for space partition\nand are always accurate so that we only need to scale the model by\ndoing simple multiplication.\n3 APEX OVERVIEW\nWe design APEX with a set of principles distilled from the unique\nproperties of PM and learned indexes:\nâ€¢P1 - Avoid Excessive PM Reads and Writes. A practical PM\nindex must scale well on multicore machines. Given the limited\nand asymmetric bandwidth of PM, APEX must reduce unneces-\nsary PM accesses and avoid write amplification.\nâ€¢P2 - Model-based Operations. Data-awareness and model-based\noperations uniquely make search operations efficient. A persis-\ntent learned index such as APEX must retain this benefit.\nâ€¢P3 - Lightweight SMOs. Structural modification operations in\nlearned indexes can be heavyweight and eventually limit scala-\nbility. APEX should be designed to reduce such overheads.\nâ€¢P4 - Judicious Use of DRAM. APEX can use DRAM for perfor-\nmance, but should use it frugally to reduce cost.\nâ€¢P5 - Crash Consistency. APEX operations must be carefully\ndesigned to guarantee correct recovery. Ideally, it should support\ninstant recovery to achieve high availability.\na,b Model :\nâ€¦\nPrimary array Stash arrayâ€¦64B; one per 256 records\n16-bit 16BBitmap 16FPs 48-bit Metadata Lock Lockâ€¦Accelerator shared byPA[0 -15]\n15Fingerprints +pointersOverflow bucket (128B ):Stash bitmap\nK\nPK\nPâ€¦K\nPK\nPK\nPâ€¦ â€¦K\nPK\nPK\nPK\nPK\nPBitmapâ€¦\nâ€¦K\nPK\nPPointers PM\nK\nPDRAM\n(b) PM -resident data node(c) PM -resident\nextended stashâ€¦\n(a) DRAM structureFigure 3: APEX data node layout and DRAM-resident data.\n3.1 Design Highlights\nAPEX combines new and existing techniques based on the above\ndesign principles. Similar to ALEX [ 14], APEX consists of inner\nnodes and data nodes. APEX places all node contents in PM except\na small amount of metadata and accelerators in DRAM to improve\nperformance and reduce PM writes ( P1,P4). APEX employs model-\nbased insert [ 14] where each data node can be treated as a hash table\nthat uses a model as an order-preserving hash function to predict\ninsert location. To resolve collisions without introducing unneces-\nsary PM accesses, we propose a new probe-and-stash mechanism\n(Section 4.2) inspired by recent PM hash tables [ 36] (P1,P2). We\nset different maximum node sizes for APEXâ€™s inner and data nodes\nto ensure most SMOs do not hinder scalability while maintaining a\nshallow tree ( P3). For instant recovery, we design DRAM-resident\ncomponents to be reconstructable on-demand ( P5).\n3.2 Node Structure\nEach node in APEX contains a linear model consisting of two double-\nprecision floating point values (slope and intercept) stored in node\nheader, e.g., a,bin the data node in Figure 3(b). Each inner node\nalso contains an array of child pointers. Data nodes also store key-\npayload pairs as records. Same as other learned indexes [ 14,17,25,\n48], APEX stores fixed-length2numeric keys that are at most 8-\nbytes and 8-byte payloads (either inlined or a pointer). Like previous\nwork [ 36,56], we assume unique keys, but non-unique keys can be\nsupported by storing a pointer to a linked list of records as payload.\nData nodes in APEX are variable-sized, but have a fixed max-\nimum size which is set to 256KB to fully exploit models. This is\nlarger than the typical size (256 bytes â€“ 1KB) in B+-trees, but small\nenough to efficiently implement SMOs and achieve good scalability.\nSince SMOs in inner nodes are relatively rare and exhibit low SMO\ncost (Section 6), we keep the maximum size of inner nodes to be\n16MB. This gives APEX more flexibility to select node fanout, lower\ntree depth and maintain good search performance. Because of the\nlow tree depth, inner nodes exhibit good CPU cache utilization.\nPlacing them in DRAM does not benefit much. Thus, different from\nPM-DRAM B+-trees [ 35,42], we place inner nodes in PM, which\nalso enables instant recovery (Section 5).\nTo support model-based lookups and hash-based inserts, each\ndata node consists of (1) a primary array and a stash array (and in\n2There has been initial work supporting variable-length keys [ 46,54]. As future work,\nwe hope to explore how APEX could adopt these techniques.\n\ncase of overflows, extended stash blocks), which store records and\nare PM-resident, and (2) reconstructable metadata stored in DRAM\nto accelerate various operations and to support concurrency.\nPM-Resident Primary and Stash Arrays. As shown in Fig-\nure 3(b), both arrays store data records in record-sized slots. The\nlinear model predicts a position in the primary array (PA) for a given\nkey. To insert a new record âŸ¨ğ¾,ğ‘ƒâŸ©, if the predicted position in the\nPA is not free, APEX linearly probes the PA and inserts the record\ninto the first free slot. We limit the probing distance to a constant\nğ·=16so that the thread would probe no more than two XPLines\n(512 bytes). Bounding the probing distance also allows records in\nPA to be nearly-sorted [ 5], improving search performance. If no free\nslot is found, APEX inserts the new record to a free slot in the stash\narray (SA), which acts as an overflow area. Stashing allows APEX to\nefficiently resolve collisions without excessive PM writes compared\nto using ALEXâ€™s gapped array (element shifts) or other common\ntechniques (e.g., probing a large number of slots with excessive PM\naccesses). We also use accelerators to reduce the overhead of stash\naccesses. We present more details in Sections 4.1 and 4.2.\nEach node has a determined size and number of record slots\n(computed using the number of keys in the node and lower density),\nso APEX needs to properly divide the slots between the PA and SA.\nAllocating more slots to PA can lower collision rate (faster search\nand inserts), yet there must be enough stash slots in case collisions\ndo happen. Therefore, APEX needs to strike a balance between\ninsert/probe performance and collision handling. APEX leverages\ndata distribution to solve this problem (Sections 4.4 and 4.5).\nDRAM-Resident Metadata and Accelerators. APEX places\nin DRAM certain structures that are (1) easy to reconstruct in case\nof failures yet are (2) very critical to performance at runtime. As\nFigure 3(a) shows, we store metadata, locks and accelerators in\nDRAM, accessible via the pointers stored in the PM data nodeâ€™s\nheader. Metadata includes basic information about the node, e.g.,\nnumber of records. As we discuss later, locks do not need to survive\npower cycles for recovery, and placing them in DRAM can avoid\nexcessive PM accesses. Accelerators are compact data structures to\nenable fast record access with reduced PM accesses. The key is to\nuse fingerprints [ 42] to quickly determine if a key possibly exists\n(often without even reading the whole key). Bitmap indicating slot\nstatus is also used for inserts to quickly locate a free PA slot. Finally,\nto reduce storage overhead we share an accelerator for every 16\nrecords in the PA ( pa), e.g., in Figure 3, pa[0] â€“pa[15] share the first\naccelerator. One accelerator (24-byte) includes 16-byte fingerprints,\na 16-bit free-slot bitmap, and one 48-bit pointer3. A stash bitmap is\nused to indicate the empty slots in the stash array.\n4 APEX OPERATIONS\nNow we present APEX operations in a single-thread setting with\ncrash-consistency. Section 5 discusses concurrency and recovery.\n4.1 Search & Range Query\nTo search for a key, we start at the root node and use its model\nto predict which child node to go to, until we reach a leaf node.\nThere is no search within inner nodes as by construction the modelâ€™s\nprediction is always accurate. All operations that require a traversal\n3Modern x86 processors use the least significant 48 bits for addressing [23]share this logic; for now we focus on data node operations. Within a\ngiven data node, we devise a probe-and-stash mechanism to reduce\nunnecessary PM accesses during key lookups. As Figure 3 shows,\nall memory accesses by a lookup are highlighted in red (blue) for\nPM (DRAM). We first â€œprobeâ€ using the nodeâ€™s model to predict\nthe position of a key ğ‘˜in the PA. Suppose the predicted position\nis 3 in Figure 3(b). APEX directly returns the record if the key\nis found in the slot. Otherwise, it linearly probes from ğ‘ƒğ´[3]till\nğ‘ƒğ´[3+ğ·âˆ’1]=ğ‘ƒğ´[18]whereğ·is the probing distance (16)\ndescribed in Section 3.2. APEX returns the record if ğ‘˜is found;\notherwise it continues to check the SA. Note that for all operations\nlinear probing always proceeds in the same direction (conceptually\nfrom â€œleft to rightâ€) because APEX does not shift data during inserts\nand so a key cannot be stored in a slot before its predicted position,\nsimplifying concurrency control (details later).\nTo accelerate the lookup in stashes, APEX creates an overflow\nbucket in DRAM for every 16 PA records if a key is originally pre-\ndicted within the 16 records but overflowed to SA or extended stash;\nthe overflow bucketâ€™s address is stored in the DRAM accelerator.\nWe find the value of 16 balances memory consumption and perfor-\nmance: having an overflow bucket per record needs a 48-bit pointer\n(next to the bitmap in Figure 3(a) in the accelerator) to point to it,\nadding non-trivial overhead ( >37%). Using one overflow bucket per\n16 records amortizes this cost. Continuing with the example, before\naccessing the stash, APEX first checks the corresponding overflow\nbucket, which holds up to 15 pointers for indexing overflow keys\nin the stash. A new overflow bucket is dynamically allocated and\nlinked if there are more than 15 overflow keys.\nEach pointer to a stash record in the overflow bucket inlines\na fingerprint of that stash record in the most significant byte of\nthe pointer. Only keys in the stash with matching fingerprints are\naccessed from PM. A negative search will issue no PM reads on\nthe stash, but up to two XPLine accesses in the PA. For a positive\nsearch, fingerprints in the overflow buckets pinpoint the target stash\nentries, reducing the expected number of PM reads to one [42].\nFor a range query where ğ‘–/ğ‘—are the predicted positions of the\nstart/end keys in a data node, APEX first collects the records be-\ntweenğ‘ƒğ´[ğ‘–]andğ‘ƒğ´[ğ‘—+15], and finds the remaining ones from\nstash. Indexes that use unsorted nodes [ 2,35], often require sorting\nfull nodes, which adds overhead and requires using smaller nodes\nto alleviate. APEX only needs to sort the final result set. This is\nefficient as APEX maintains nearly-sorted order, sorting which is\nfaster [5]. In Section 6 we show its impact using realistic datasets.\n4.2 Insert\nTo insert a record with key ğ¾, APEX ensures ğ¾is not already in\nthe index, and if so, locates and inserts the record to a free slot.\nUniqueness Check. After obtaining a predicted position ğ‘˜, in-\nstead of directly probing PA, we first check key existence using\nthe fingerprints in the accelerator. This can potentially save PM\naccesses: a negative result indicates ğ¾is definitely not in PA. Finger-\nprints are usually co-located in the same cacheline with the 16-bit\nbitmap which must be brought to the CPU cache to find a free slot\nfor PA insertion, accessing fingerprints incurs little overhead and is\npractically â€œfreeâ€ without additional memory accesses. In Figure 3,\nassume the model predicted position 3 for the new key, APEX first\n\nchecks existence using the 4th to 16th fingerprints from the first\naccelerator, and the first three fingerprints from the second accel-\nerator. We access PA only if there is a matching fingerprint. Note\nthat lookup (Section 4.1) does not use PAâ€™s fingerprint because first\naccessing the accelerator can incur extra cache misses if the key is\nstored in PA (which is the common case). The uniqueness check\nthen continues with the fingerprints in the overflow bucket(s) and\n(if needed) stash slots in PM, same as a regular key search.\nLocating a Free Slot. PM indexes often use bitmaps to indicate\nfree space [ 7,35,42,61], but persisting them for each insert/delete\non PM adds non-trivial overhead. Thus, APEX includes the bitmaps\nin the DRAM accelerators that are rebuilt on-demand by reading\nslot contents. We indicate free slots by storing in them an invalid\nkey that is out of the nodeâ€™s key range [ğ‘šğ‘–ğ‘›,ğ‘šğ‘ğ‘¥]. Then we must\nnot placeğ‘šğ‘–ğ‘›ğ¼ğ‘›ğ‘¡ 64andğ‘šğ‘ğ‘¥ğ¼ğ‘›ğ‘¡ 64in the same node. This is done by\nensuring the initialization/bulk loading algorithm always generates\nat least two data nodes, one with range [ğ‘šğ‘–ğ‘›ğ¼ğ‘›ğ‘¡ 64,ğ‘šğ‘ğ‘¥], and the\nother with range[ğ‘šğ‘–ğ‘›,ğ‘šğ‘ğ‘¥ğ¼ğ‘›ğ‘¡ 64]. Then we use ğ‘šğ‘ğ‘¥+1andğ‘šğ‘–ğ‘›âˆ’1\nas the invalid key in the two nodes, respectively. As a result, both\ninsert and delete only require one PM write (updating the record)\nand one DRAM write (flipping a bitmap entry).\nSA is statically allocated during node creation. When it is full (al-\nthough rare), APEX dynamically allocates a new 256-byte extended\nstash in PM and atomically stores a pointer to the newly allocated\nblock in PM. Extended stash blocks are linked together and reach-\nable via a pointer in the data node. Crash consistency is guaranteed\nby the PM allocator [ 22] which ensures safe PM ownership transfer\nbetween the allocator and PM to avoid permanently leaking PM.\nCrash-Consistent Insert. As described in Section 2, ALEX [ 14]\nuses GA with exponential search, which incurs excessive PM writes\nby shifting records or filling consecutive gaps with adjacent keys. To\nsave PM bandwidth, APEX neither shifts records nor fills gaps. This\nis possible since APEX uses linear probing instead of exponential\nsearch with two careful designs: (1) APEX co-locates the key and\npayload, so an insert requires only a 16-byte PM write. (2) APEX\nwrites the payload prior to writing the key and persists them in PM\nusing one flush and fence, leveraging the fact that modern x86 CPUs\ndo not reorder writes to the same cacheline [ 45]. In case of a crash,\nAPEX simply discards records with invalid keys. Thus, unless a new\nextended stash block is needed, an insertion only writes one XPLine.\nWe alleviate the impact of extended stash by carefully setting SA\nsize based on data distribution (Sections 4.4â€“4.5).\n4.3 Delete & Update\nAPEX implements delete as lookup followed by invalidation. Once\nwe locate the record, APEX simply replaces the target key in the slot\nwith an invalid key. The validity bitmap in DRAM is also updated\nto reflect this change. APEX updates records in place. If the key is\nfound, we atomically update and flush the payload with one XPLine\nwrite. Same as inserts, the lookup process in deletes and updates\nuses PAâ€™s fingerprints to reduce PM accesses.\n4.4 Structural Modification Operations\nSimilar to ALEX, APEX uses node density to decide when to trigger\nan SMO. ALEX uses 0.8 as the upper density limit ğ‘‘ğ‘¢because insert\nperformance degrades beyond that. To achieve an average memoryutilization ofâˆ¼70% (same as a B+-tree), ALEX uses 0.6 as the lower\ndensity limit ğ‘‘ğ‘™. In APEX, however, such a tight bound would trigger\nmany SMOs, incur excessive PM writes (e.g,. moving data to a new\nnode for node splits) and hurt performance. Since inserts in APEX\nincur little write amplification, APEX can tolerate a higher upper\ndensity limit to reduce SMOs. Based on empirical evaluation, we\nuse[0.5,0.9]with the sameâˆ¼70% average memory utilization.\nNode Expansion vs. Split. Once an SMO is triggered, we use\ncost models to choose between node expansion andsplit like ALEX\ndoes. APEX follows the same model as ALEXâ€™s but uses different\nstatistics. To quantify the cost of a search, APEX uses the average\nnumber of cache misses in probe-and-stash instead of ALEXâ€™s aver-\nage number of iterations of exponential search. Insert cost is the\naverage number of overflow buckets allocated plus the search cost.\nData Node Expansion. APEX expands a node ğ´in three steps:\n(1) allocate and initialize a new node ğµ; (2) retrain or re-scale the\nmodel and insert records from ğ´toğµusing the new model; (3) attach\nğµto the parent node, update the sibling pointers, and reclaim ğ´.\nThis multi-step process needs to be implemented carefully. For\nexample, there will be a PM leak if a crash happens before step 3.\nAlso, the index would be inconsistent if a crash happens during step\n3. APEX achieves lightweight crash-consistency via hybrid logging .\nWe make a key observation: only step 2 is relatively long running.\nHence, different from prior work which only uses redo-logging to\npossibly redo a whole SMO, APEX uses undo logging before step 3\nand redo-logging after step 2. If a crash happens after step 2, APEX\nwould waste less work and can resume step 3 upon restart.\nA naÃ¯ve logging approach, such as PMDKâ€™s physical undo log-\nging logs all data records and so incurs excessive PM writes. In\nAPEX, node expansions have only three steps, so we can use logical\nlogging with a small log area in PM. Upon step 1, we initialize a\nâ€œnode-expandâ€ log entry (one cacheline) in PM with the format of\nâŸ¨ğ‘œğ‘™ğ‘‘,ğ‘›ğ‘’ğ‘¤,ğ‘˜ğ‘’ğ‘¦,ğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ âŸ©, whereğ‘œğ‘™ğ‘‘is a pointer to the old node, ğ‘˜ğ‘’ğ‘¦\nis the insert that triggered the node expansion (for locating the\nparent node of the old node), and ğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ is set to UNDO . Next, we\nallocate a new node using PMDK which atomically stores the new\nnodeâ€™s address in ğ‘›ğ‘’ğ‘¤, and initialize the new node (setting all keys\nin the PA/SA to invalid). Step 2 then updates and persists the ğ‘ ğ‘¡ğ‘ğ‘”ğ‘’\nof the node-expand log entry to REDO . Now we switch from undo\nlogging to redo logging and start step 3. Finally, we persists the\nnode-expand log entry in the PM with a ğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ value reset to NoSMO .\nThis approach gives low overhead (only three PM log writes)\nand fast recovery. If the system fails before step 3, we discard the\nnew node to undo the incomplete expansion. If it fails after step 2,\nthe SMO can resume from step 3 by observing ğ‘ ğ‘¡ğ‘ğ‘”ğ‘’ in the log.\nData Node Split and Inner Node Expansion. APEX uses the\nsame data node split and inner node expansion logic as ALEX\n(Section 2). APEX also handles these SMOs in a very similar way\nto node expansions explained above: Each SMO has its own log\nentry in PM. We use logical undo-redo logging to ensure once a\nheavyweight step (e.g,. record copying) is done, APEX would only\nredo the lightweight step (e.g., switching pointers) upon recovery.\nStash Ratio. The ratio between the sizes of the stash and pri-\nmary array is governed by a stash ratioğ‘†, defined as the fraction\nof stash array size to the sum of the primary array and stash array\nsizes. Setting a reasonable stash ratio is needed to avoid excessive\ncollisions or the overhead of extra stash block allocations. Previous\n\nPM hash tables [ 36,62] allocate a fixed-size stash array based on a\npredefined collision probability. This is not ideal for learned indexes\nsince the collision probability of the model depends on how well\nthe model fits the data. APEX automatically configures the stash\nratio when creating a new data node, based on the overflow ratio\nof the old node. Specifically, the overflow ratio ğ‘‚of a data node\nwithğ‘ğ‘‘keys is the fraction of the number of overflowed keys ğ‘ğ‘œ\nin that data node, i.e., ğ‘‚=ğ‘ğ‘œ\nğ‘ğ‘‘. These simple statistics are all part of\nthe metadata in DRAM and APEX maintains a set of them per 256\nrecords to amortize the cost. APEX assumes that the expanded or\nsplit nodes follow the same distribution from the old node. Hence,\nthe stash ratio of new node created by an SMO is set to be the\noverflow ratio. This strategy ensures that the stash ratio of a data\nnode is adaptive to the actual data distribution.\n4.5 Bulk Loading\nLike ALEX, during bulk loading APEX grows the RMI greedily, but\nuses different cost models described in Section 4.4 and must also\ndetermine PA and SA sizes. Ideally the stash ratio ğ‘†should match\nthe percentage of records overflowed to SA during real inserts to\nreduce extended stash use and balance insert (which prefers larger\nSA) and lookup (which prefers smaller SA) speeds. This requires\nknowing data distribution which is unavailable upon bulk loading.\nWe estimate a reasonable ğ‘†empirically. Based on extensive exper-\niments using realistic datasets (details in Section 6), we find setting\nğ‘†within the range [0.05, 0.3] well balances model-based search and\ninsert performance. APEX thus bounds ğ‘†in this range. Then, we set\nğ‘†via simulation: Given ğ‘ğ‘‘keys to insert to a node, we first assume\nthe node will reach the upper density limit ğ‘‘ğ‘¢and all free slots are\nallocated to PA. We then compute the predicted positions in PA\nfor each key and probe-and-stash to collect the number of keys ğ‘ğ‘œ\noverflowed to SA without actually carrying out any inserts (thus a\nâ€œsimulationâ€). With the overflow ratio ğ‘‚=ğ‘ğ‘œ\nğ‘ğ‘‘, we calculate ğ‘†based\non two intuitions: (1) the higher the overflow ratio ğ‘‚, the bigger the\nstash ratioğ‘†; (2)ğ‘†should be greater than ğ‘‚asğ‘‚was determined\nby assuming all slots are in PA (i.e., real inserts should exhibit more\ncollisions than simulation). There could be many ways to determine\nğ‘†usingğ‘‚. For simplicity, we set ğ‘†to be a multiple ( ğ‘›) ofğ‘‚, i.e.,\nğ‘†=ğ‘›Ã—ğ‘‚, and empirically determined ğ‘›â€™s value to be 1.5 via exper-\niments (not shown here for space limitation); we call ğ‘›the â€œstash\ncoefficient.â€ In general, a higher stash coefficient means potentially\nmore keys are stored in the stash areas. Taking the aforementioned\nbound into account, ğ‘†=ğ‘šğ‘ğ‘¥(0.05,ğ‘šğ‘–ğ‘›(0.3,1.5Ã—ğ‘‚)).\nOverall, our method gives reasonable performance and is simple\nto implement/calculate. The upper limit ensures most records stay\nin PA; the lower limit is a safety net to absorb collisions (e.g., due\nto a distribution shift) before an SMO reorganizes the node. In\npractice, we do not expect stashing to be the main storage as APEX\nrecursively partitions the key space so that each subspace can be\nmodeled well. Note that bulk loading still succeeds even if ğ‘†is\ninaccurate: more keys will be stored in SA and/or the extended\nstash. Our evaluation in Section 6 shows that in practice stash ratio\nis low in most cases and extended stash blocks are rarely used as\nstash ratio is reasonably set based on data distribution. Further\noptimizations are interesting future work.5 CONCURRENCY AND RECOVERY\nAs Section 2.2 describes, compared to traditional node-level locking\nand lock-free approaches, optimistic locking is usually a better\nfit for PM and balances programmability and performance. APEX\nfurther adapts optimistic locking for learned indexes on PM.\nInner Node Accesses. APEX uses different maximum sizes for\ninner and data nodes (16MB vs. 256KB). Inner nodes typically have\nless contention so we pick a larger node size. Each inner node\ncarries a reader-writer lock for SMO, compared to traditional opti-\nmistic locking with mutual exclusion locks [31]. Reading an inner\nnode (e.g., lookup) is lock-free. In traditional optimistic locking, the\nthread retries traversal if inconsistencies caused by modifications\non the node are detected, wasting CPU cycles. APEX avoids such\naborts by an out-of-place-based SMO design, described later.\nData Node Accesses. Data nodes may see many concurrent\naccesses, so using a smaller node size can help reduce contention.\nIn addition to a node-level lock to ensure only one thread can\nconduct SMO on the node, we allocate one optimistic lock per 256\nrecords in PA to isolate non-SMO updates. This design balances\nthe synchronization and lock acquisition overhead during SMOs.\nTo read a data node, the thread keeps traversing down until\nreaching the target data node, without holding any locks. However,\nupon reading data records, it uses the version in the optimistic\nlock to guarantee the read correctness and restarts the search if the\nversion changed. Like many prior approaches, we use epoch-based\nmemory reclamation [18] for safe memory management.\nTo insert a key, the thread first traverses to the target data node\nusing lock-free read. To find the key in the node, the thread may\nneed to use linear probing to access multiple slots, which requires\nacquiring the corresponding lock(s) that cover(s) the probing slots.\nMore than one lock may be acquired if the predicted position plus\nprobing distance crosses lock boundary. Unlike ALEX, since in\nAPEX all threads linearly probe in the same direction (described\nin Section 4.1), it is guaranteed that deadlocks will not happen. To\nupdate a data node, the thread first acquires the lock that protects\nthe record, and then continues to hold it if it needs to update the\nstash array. Multiple threads can race to install new records into the\nstash array while holding different locks (i.e., in two different 256-\nrecord blocks). Therefore, threads must first allocate a free stash\nslot using the stash bitmap, which is done by using the compare-\nand-swap (CAS) instructions to atomically set the â€œnext freeâ€ bit in\nthe bitmap (and retry if the CAS failed); after that each thread can\ncontinue to insert the record to its own stash array slot.\nSMOs. Data node expansion and split require more care to work\nunder optimistic locking. Expanding a data node is done in an out-\nof-place manner that always allocates a new node and updates\nthe parent node to point to the new node. Meanwhile, the parent\nnode may be undergoing an expansion. The aforementioned reader-\nwriter lock in inner nodes is for handling such cases. Upon updating\nthe inner node, the thread ğ‘‡takes the nodeâ€™s lock in shared (reader)\nmode. Note that since ğ‘‡already holds the lock for the data node,\nit is safe to directly use an atomic write to update the pointer\nin the parent node. This allows multiple threads to proceed and\nexpand different data nodes in parallel. If a data node split causes\nthe parent node to expand, the inserter thread locks the inner node\nin exclusive (writer) mode. In theory, it is possible for splits to\n\npropagate up and grow the RMI by acquiring locks bottom-up. This\ncan incur non-trivial overhead [ 14]. Our implementation therefore\nfollows prior work [ 14] to disallow inner node split and only allow\nexpansion. This limits the number of acquired locks to three (data\nnode, parent and grandparent levels). Note that throughout this\nprocess, readers proceed without taking any locks but must verify\nversion numbers. Using out-of-place SMO and updates without\nshifting in inner nodes allow the traversals to data nodes without\nretries. The thread may see an obsolete data node due to concurrent\nSMOs (although the key range is correct). It detects this case by\nchecking data nodesâ€™s lock status and retries from root if it is set.\nInstant Recovery. APEX adopts lazy recovery in Section 2.2. It\nneeds to undo in-flight SMOs (if any) by deallocating PM blocks and\nswitching pointers (Section 4.4); both are lightweight and after that\nAPEX can start to handle requests. Since each thread has at most\none in-flight SMO upon crash, recovery time scales with thread\ncount, instead of data size. Modern OLTP systems usually limit\nthread count, making APEX recovery practically instant.\n6 EVALUATION\nWe now present a comprehensive evaluation of APEX including\ncomparisons against the state-of-the-art PM indexes. We show that:\nâ€¢APEX retains the benefits of model-based search and achieves\nhigh throughput and good scalability.\nâ€¢APEXâ€™s individual design principles and choices are effective,\ncollectively allowing APEX to perform and scale well.\nâ€¢APEX instant recovers ( <1s), although it uses DRAM, in contrast\nto prior work that trades off instant recovery for performance.\n6.1 Index Implementations\nWe implemented APEX in C++ and compare it with recent PM\nB+-trees: BzTree [ 2], LB+Tree [ 35], FAST+FAIR [ 20], DPTree [ 61],\nFPTree [ 42] and uTree [ 8]. BzTree and FAST+FAIR are PM-only\nindexes and do not use DRAM. LB+Tree and FPTree are hybrid\nPM-DRAM indexes that place inner/leaf nodes in DRAM/PM. They\ncombine HTM and locking for synchronization. uTree puts both\ninner and leaf nodes in DRAM and relies on a linked list of records\nin PM for persistence. DPTree batches modifications in DRAM\nbuffer and merges with the background PM-DRAM tree to reduce\npersistence overhead. Except BzTree and FPTree (which were not\noriginally open-sourced), we use the original authorsâ€™ open-sourced\ncode and add the necessary but missing functionality in best effort.4\nPersistence. We use PMDK [ 22] to support persistence in all\nindexes and verify they do not incur unnecessary overheads. We\nmodified LB+Tree, FAST+FAIR, uTree and DPTree to use PMDK\nbecause they either were proposed based on DRAM emulation or\ndid not implement certain necessary PM-related functionality due\nto the lack of a full-fledged PM allocator.5We also fixed LB+Tree to\nprovide correct read committed isolation level like other indexes.6\nOperations. We did best-effort implementations of the missing\nrange scan in FAST+FAIR, LB+Tree and uTree. We faithfully im-\nplemented recovery for LB+Tree and uTree. For multi-threaded\n4Code for all indexes is summarized in our repo: https://github.com/baotonglu/apex .\n5For example, uTree was designed to self-manage PM space, but the open-sourced\ncode does not implement recycling. So we use the PMDK allocator.\n6Details at https://github.com/schencoding/lbtree/pull/6 .recovery, LB+Tree requires statistics [ 35] that are currently not be-\ning collected. We therefore implemented a single-threaded version.\n6.2 Experimental Setup\nWe run experiments on a server with a 24-core (48-hyperthread),\n2.1GHz Intel Xeon Gold 6252 CPU, 768GB Optane DCPMM ( 6Ã—\n128GB DIMMs on all six channels) and 192GB DRAM ( 6Ã—32GB\nDIMMs). The CPU has 35.75MB of L3 cache. The server runs Arch\nLinux (kernel 5.10.11). We use PMDK/jemalloc [ 16] to allocate\nPM/DRAM. All code is compiled with GCC 10.2 with all optimiza-\ntions. For fair comparison, we set each index to use the parameters\nused in its original paper. LB+Tree/FAST+FAIR/uTree/BzTree use\n256B/512B/512B/1KB node. FPTree uses 28/64-record inner/leaf\nnodes. APEX uses maximum 16MB/256KB inner/data nodes.\nDatasets. We use six synthetic and realistic datasets to test all the\nindexes. Longitudes is extracted from Open Street Maps (OSM) [ 3].\nLonglat is also from OSM but is transformed to become highly\nnon-linear to stress learned indexes. Lognormal represents the log-\nnormal distribution. YCSB contains user IDs in YCSB [ 9]. SOSD [ 39]\nincludes four realistic datasets. Due to space limits, we focus on\nthe Facebook ( FB) dataset containing randomly sampled Facebook\nuser IDs. FBis extremely non-linear and the hardest-to-fit among\nSOSD datasets. We use it to stress test the indexes. We also run the\nTPC-E [ 49] benchmark and collect three datasets (trade, settlement,\ncash transaction) by loading the database with 15000 customers\nand 300 initial trading days. APEX performs similarly under them,\nso we only report results from the trade dataset.\nAll keys are unique in these datasets. Same as previous work [ 14]\nwe randomly shuffle them to simulate a uniform distribution over\ntime. All the datasets use 8-byte keys and 8-byte payloads. Except\nLongitudes andLonglat whose key type is double , all the other\ndatasets consist of 8-byte integer keys. Lognormal contains 190\nmillion keys (2.83GB), whereas TPC-E (trade) contains 259 million\nkeys (3.86GB); other datasets include 200 million keys (2.98GB).\nBenchmarks. We stress test each index using microbenchmarks.\nFor all runs, we bulk load the index with 100 million records, and\nthen test individual operations. Since only LB+Tree supports node\nmerge (when the node is empty) which may reduce its performance,\nwe run 90 million deletes to avoid triggering merges for fair com-\nparison. Other workloads issue 100 million requests. Range scans\nstart at a random key and scan 100 records. Lognormal only has 190\nmillion keys, so for its insert test we issue 90 million requests. The\nsource code of DPTree does not support double key type (used in\nLonglat andLongitudes datasets), recovery and delete operation,\nso we do not include it in the corresponding experiments.\n6.3 Single-thread Performance\nWe first run single-thread experiments to compare the indexes with-\nout contention. Note that we do not remove concurrency support;\nwith one thread the overhead is minimal. To better understand\nthe results, we also show the basic statistics of APEX after bulk\nloading in Table 1. For inserts, as shown in Figure 4(a), APEX out-\nperforms BzTree/LB+Tree/FAST+FAIR/DPTree/uTree/FPTree by\nup to 15Ã—/2.7Ã—/3.8Ã—/1.66Ã—/6.8Ã—/3.7Ã—, for five datasets. The advan-\ntage mainly comes from APEXâ€™s model-based insert and probe-\nand-stash. In most cases, APEX only issues one PM write per\n\n 0 0.5 1 1.5\nLongitudesLonglat\nLognormalFBYCSBTPC-EMillion ops/sAPEX DPTree LB+-Tree FAST+FAIR FPTree uTree BzTree\n 0 1.5 3 4.5\nLongitudesLonglat\nLognormalFBYCSBTPC-E\n(a) 100% insert. (b) 100% search. 0 0.5 1 1.5 2\nLongitudesLonglat\nLognormalFBYCSBTPC-E\n(c) 100% update. 0 0.5 1 1.5 2\nLongitudesLonglat\nLognormalFBYCSBTPC-E\n(d) 100% delete. 0 0.15 0.3 0.45\nLongitudesLonglat\nLognormalFBYCSBTPC-E\n(e) 100% scan.Figure 4: Single-thread throughput. APEX performs the best on five datasets for inserts and range scans, and remains compet-\nitive for the worst-case FB. For search, update and delete APEX performs the best across all cases.\nTable 1: APEX statistics after bulk loading.\nMetric Longitudes Longlat Lognormal FB YCSB TPC-E\nAverage depth 1.10 1.64 1.95 3.13 2 3.43\nMaximum depth 2 3 3 6 2 9\nNumber of inner nodes 541 3628 374 6279 8193 20879\nNumber of data nodes 17438 44071 12696 81856 16384 143035\nMinimum inner node size 16B 16B 16B 16B 16B 16B\nMedian inner node size 16B 32B 64B 32B 16B 16B\nMaximum inner node size 512KB 4MB 16MB 512KB 64KB 4KB\nMinimum data node size 496B 496B 496B 496B 131KB 1056B\nMedian data node size 139.5KB 29.05KB 168.75KB 21.5KB 136KB 2.56KB\nMaximum data node size 256KB 256KB 256KB 256KB 142KB 149KB\nAverage stash ratio 0.05 0.118 0.05 0.299 0.05 0.06\nAverage overflow ratio 0.006 0.065 0.0008 0.48 0.0014 0.034\ninsert, whereas other indexes often issue more (e.g., >10for\nBzTree [ 2]). For FB, APEX is 1.18Ã—/5.16Ã—/2.3Ã—/1.26Ã—faster than\nFAST+FAIR/BzTree/uTree/FPTree, but achieves a lower (5.77%/42%)\nthroughput when compared to LB+Tree/DPTree. As Table 1 shows,\nFBexhibits the highest average overflow ratio. Thus, FBincurs\nmany collisions in PA with more inserts routed to SA and extended\nstash. This requires more CPU cycles to find a free slot and allocate\noverflow buckets. Note that FBis the hardest-to-fit (the worst case).\nOverall, APEX remains competitive under worst-case scenarios and\noutperforms other indexes by up to 15 Ã—in common cases.\nFigure 4(b) shows the result for search operations. APEX per-\nforms up to 7.1Ã—/3.9Ã—/4.1Ã—/3.2Ã—/3.2Ã—/5.8Ã—higher than BzTree/LB+-\nTree/FAST+FAIR/DPTree/uTree/FPTree. Notably, APEXâ€™s through-\nput isâˆ¼60%/37% higher than that of LB+Tree and DPTree under\nthe hardest-to-fit FBdataset. The performance differences across all\ndatasets are due to how well APEX could fit the data. For datasets\nthat are easy to fit by linear models, APEX exhibits much higher\nsearch throughput, for two reasons: (1) APEXâ€™s index layer is much\nsmaller and shallow, resulting in much better CPU cache efficiency.\nFor example, in Table 1, the average tree depth on Longitudes is\n1.1. (2) For easy-to-fit data, most records are stored in PA instead of\nSA (e.g., overflow ratio is 0.006 on Longitudes ), so most lookups\nonly need model-based search without probing stashes. For hard-to-\nfit data, APEX tree depth can be higher (e.g., 3.13 for FB), reducing\ntraversal efficiency. The probing distance and overflow ratio are\nalso higher, adding more overhead during key lookup.\nSearch performance also affects update/delete. APEX only is-\nsues one PM write per update/delete, and consistently outperforms\nother indexes in Figures 4(c)â€“(d). Thanks to the nearly-sorted order\nusing probe-and-stash, APEX scans up to 7.6 Ã—/1.45Ã—/1.46Ã—/1.83Ã—/\n16.38Ã—/3.1Ã—faster than BzTree/LB+Tree/FAST+FAIR/DPTree/uTree/\nFPTree on five datasets in Figure 4(e). FBpresents the worst casefor APEX, due to long overflow bucket lengths. Scans incur more\ncache misses to traverse the overflow buckets. FAST-FAIR does not\nneed sorting. LB+Treeâ€™s sorting overhead is very small for its small\ndata nodes (256B). BzTree often has to sort much larger (1KB) leaf\nnodes, adding much more overhead and so performs 3 Ã—lower than\nAPEX even on FB. uTree performs the worst among all indexes\nsince traversing the PM linked list incurs lots of cache misses.\n6.4 Scalability with Number of Threads\nNow we examine how each index scales with an increasing number\nof threads under various operations and workloads.\nIndividual Operations. For inserts (Figure 5), APEX scales bet-\nter than others before hyperthreading on five datasets and is com-\npetitive under FB. In most cases, APEX incurs one XPLine write\nper insert and its adaptive node sizes help reduce synchronization\noverhead. In contrast, FAST+FAIR needs to shift existing records\nwhile uTree needs to frequently allocate PM records. BzTree needs\nto update much metadata used by itself [ 32]. LB+Tree scales well\nwith help of DRAM and small 256B nodes, but with lower through-\nput in most cases. Note that APEX also scales under FB, although\nwith lower numbers vs. other workloads. This shows APEXâ€™s con-\ncurrency control protocol is lightweight. APEX benefits from hy-\nperthreading, but not as significantly as LB+Tree since APEX has\nmuch higher single-thread throughput, exhausting PM bandwidth\nearlier with fewer threads. LB+Tree uses hyperthreading to better\nuse PM bandwidth, whereas APEX use less resource (threads) to\nfully exploit PM bandwidth and maintain high performance.\nAPEX scales nearly linearly for lookups before hyperthreading\n(Figure 6). It consistently outperforms other indexes for all datasets\nbenefiting from efficient model-based search which better utilizes\nCPU caches. Figure 7â€“9 shows that update, erase and scan scale\nwell as they all benefit from efficient model-based search. The one\nPM-write design in update and delete operation especially make\nAPEX scale better than other indexes because other indexes require\nmore PM writes. Since we reduce most unnecessary PM reads and\nwrites in those operations, they could further leverage the hyper-\nthreading to exhaust the remaining bandwidth.\nMixed Operations. Real-world applications usually involve a\nmix of operations. We evaluate each index with two mixed work-\nloads: (1) a read-heavy workload that consists of 20% inserts and 80%\nsearch, and (2) a balanced workload with 50% inserts and 50% search.\nBoth workload issue 100/90 million inserts from Longlat /Lognormal ,\nleading to a total of 500/450 million and 200/180 million operations,\nrespectively. In Figure 10, we show that APEX scales the best, and\n\n 0 5 10 15 20\n 1 8 16 24 32  48Million ops/s\nNumber of threads\n(a) Longitudes. 0 6 12 18\n 1 8 16 24 32  48\nNumber of threads\n(b) Longlat. 0 5 10 15 20\n 1 8 16 24 32  48\nNumber of threads\n(c) Lognormal.APEX DPTree LB+-Tree FAST+FAIR FPTree uTree BzTree\n 0 4 8 12 16\n 1 8 16 24 32  48\nNumber of threads\n(d) FB. 0 7 14 21\n 1 8 16 24 32  48\nNumber of threads\n(e) YCSB. 0 6 12 18\n 1 8 16 24 32  48\nNumber of threads\n(f) TPC-E.Figure 5: Insert scalability. APEX and LB+Tree scale better than others over all datasets. LB+Tree performs better than APEX\nonFB, but is limited by its lower single-thread throughput for other datasets.\n 0 20 40 60 80\n 1 8 16 24 32  48Million ops/s\nNumber of threads\n(a) Longitudes. 0 15 30 45 60 75\n 1 8 16 24 32  48\nNumber of threads\n(b) Longlat. 0 25 50 75 100\n 1 8 16 24 32  48\nNumber of threads\n(c) Lognormal.APEX DPTree LB+-Tree FAST+FAIR FPTree uTree BzTree\n 0 15 30 45\n 1 8 16 24 32  48\nNumber of threads\n(d) FB. 0 20 40 60 80\n 1 8 16 24 32  48\nNumber of threads\n(e) YCSB. 0 20 40 60 80\n 1 8 16 24 32  48\nNumber of threads\n(f) TPC-E.\nFigure 6: Search scalability. APEX scales nearly linearly before hyperthreading, as model-based search leverages CPU caches\nmore effectively than data-agnostic designs where tree traversal incurs many more cache misses.\n 0 10 20 30 40\n 1 8 16 24 32  48Million ops/s\nNumber of threads\n(a) Longitudes. 0 10 20 30 40\n 1 8 16 24 32  48\nNumber of threads\n(b) Longlat. 0 10 20 30 40\n 1 8 16 24 32  48\nNumber of threads\n(c) Lognormal.APEX DPTree LB+-Tree FAST+FAIR FPTree uTree BzTree\n 0 10 20 30\n 1 8 16 24 32  48\nNumber of threads\n(d) FB. 0 10 20 30 40\n 1 8 16 24 32  48\nNumber of threads\n(e) YCSB. 0 10 20 30\n 1 8 16 24 32  48\nNumber of threads\n(f) TPC-E.\nFigure 7: Update scalability. APEX issues one PM write per update, and so reduces PM bandwidth consumption and scales\nbetter.\n 0 10 20 30 40\n 1 8 16 24 32  48Million ops/s\nNumber of threads\n(a) Longitudes. 0 10 20 30 40\n 1 8 16 24 32  48\nNumber of threads\n(b) Longlat. 0 10 20 30 40\n 1 8 16 24 32  48\nNumber of threads\n(c) Lognormal.APEX LB+-Tree FAST+FAIR FPTree uTree BzTree\n 0 10 20 30\n 1 8 16 24 32  48\nNumber of threads\n(d) FB. 0 10 20 30 40\n 1 8 16 24 32  48\nNumber of threads\n(e) YCSB. 0 10 20 30 40\n 1 8 16 24 32  48\nNumber of threads\n(f) TPC-E.\nFigure 8: Delete scalability. APEX issues one PM write per delete, and so reduces PM bandwidth consumption and scales better.\nthe other indexes show similar trends as before. As the workload\nbecomes more write-dominant, the throughput of all indexes drops.\nBut APEX is still up to 1.69 Ã—better than the next best performing\nindex, LB+Tree.\nSMO Costs. Table 2 lists SMO statistics under insert-only work-\nloads. Many more SMOs happen in data nodes than in inner nodes\nwhere SMOs are lightweight as shown by the average SMO times.\nThis justifies our adaptive node size design. Although we use more\nlocks per node, the overhead is very small (0.02%-0.04% time of an\nSMO) relative to other SMO work (e.g., node allocations).We further stress test APEX and LB+Tree on SMO-intensive\nworkloads. We bulk load APEX/LB+Tree with 100 million records\nwith upper density limit 0.9/1.0 (SMO-intensive) and lower density\nlimits 0.5/0.5 (normal) and then run 10 million inserts. In Figure 11,\ncompared to the SMO-intensive cases, APEX and LB+-Tree perform\n5.0Ã—/2.1Ã—better under their â€œnormalâ€ cases. Under 24 threads, LB+-\nTree outperforms APEX by 1.3 Ã—with intensive SMOs as APEX\nSMO needs to do more work, e.g., model retraining and making\ndecisions based on the cost-models. We believe APEXâ€™s much higher\nimprovement in the common cases outweighs such degradation on\ncorner cases; we leave more optimizations as future work.\n\n 0 3 6 9\n 1 8 16 24 32  48Million ops/s\nNumber of threads\n(a) Longitudes. 0 3 6 9\n 1 8 16 24 32  48\nNumber of threads\n(b) Longlat. 0 3 6 9\n 1 8 16 24 32  48\nNumber of threads\n(c) Lognormal.APEX DPTree LB+-Tree FAST+FAIR FPTree uTree BzTree\n 0 2 4 6 8\n 1 8 16 24 32  48\nNumber of threads\n(d) FB. 0 3 6 9\n 1 8 16 24 32  48\nNumber of threads\n(e) YCSB. 0 2 4 6 8\n 1 8 16 24 32  48\nNumber of threads\n(f) TPC-E.Figure 9: Range scan scalability. APEX and FAST+FAIR scale well as they maintain records in (nearly) sorted order.\nBzTree/FPTree/LB+Tree/DPTree use unsorted nodes, and uTree incurs high cache misses when traversing the linked list in\nPM.\n 0 15 30 45 60\n 1 8 16  24  32  48LongitudesMillion ops/s\nNumber of threads\n(a) 20% insert + 80% search.APEX\nLB+-TreeFAST+FAIR\nFPTreeuTree\nBzTree\n 0 10 20 30\n 1 8 16  24  32  48Longitudes\nNumber of threads\n(b) 50% insert + 50% search.\n 0 10 20 30 40\n 1 8 16  24  32  48Million ops/s\nNumber of threads\n(c) 20% insert + 80% search. 0 8 16 24 32\n 1 8 16  24  32  48Longlat Longlat\nNumber of threads\n(d) 50% insert + 50% search.\nFigure 10: Mixed workload scalability under Lognormal (aâ€“b)\nand Longlat (câ€“d).\nTable 2: SMO statistics of insert-only workloads (24 threads).\nLongitudes Longlat Lognormal FB YCSB TPC-E\nInner node expansions 105 2057 296 5 1 2169\nData node expansions 15786 35277 3003 73377 16383 139621\nData node splits (sideway) 1653 5327 9499 440 1 6075\nData node splits (downwards) 0 0 193 0 0 0\nAverage inner node SMO time (ms) 0.11 0.09 0.66 0.94 0.01 0.03\nAverage data node SMO time (ms) 1.74 0.78 2.1 0.49 1.54 0.21\nLock % during data node SMO 0.03% 0.03% 0.03% 0.03% 0.02% 0.04%\nSkewed Workloads. We evaluate search and update operations\nwith 1 and 24 threads under varying skewness. As shown in Fig-\nures 12(aâ€“b), with a single thread, with higher skewness (theta),\nall indexes perform better because the accesses are focused on a\nsmaller set of hot keys, better utilizing the CPU cache and less\nimpacted by PMâ€™s high latency. The improvement for updates is\nsmaller because they have to flush records to PM. Under 24 threads\nin Figures 12(câ€“d), the indexes maintain their relative merits to each\nother and achieve higher search throughput because of reduced PM\naccesses. However, under very high skew (theta=0.99), contention\nincreases and synchronization becomes the major bottleneck.\n 0 10 20 30\n 1 4  8  16  24LongitudesMillion ops/s\nNumber of threadsAPEX (SMO-intensive)\nAPEX (normal)LB+-Tree (SMO-intensive)\nLB+-Tree (normal)\n 0 10 20 30\n 1 4  8  16  24Longlat\nNumber of threadsFigure 11: Insert with SMO-intensive vs. normal cases.\n 0 1 2 3 4\n 0.6  0.7  0.8  0.9  0.99Million ops/s\nZipf theta\n (a) Search (1 thread).APEX\nLB+-TreeFAST+FAIR\nFPTreeuTree\nBzTree\n 0 0.5 1 1.5 2\n 0.6  0.7  0.8  0.9  0.99\nZipf theta\n (b) Update (1 thread).\n 0 20 40 60 80\n 0.6  0.7  0.8  0.9  0.99Million ops/s\nZipf theta\n (c) Search (24 threads). 0 10 20 30\n 0.6  0.7  0.8  0.9  0.99\nZipf theta\n (d) Update (24 threads).\nFigure 12: Throughput under varying skewness of Zipfian\ndistribution with one (aâ€“b) and 24 (câ€“d) threads ( Longlat ).\n6.5 Effect of Individual APEX Design Choices\nWe quantify the impact of APEXâ€™s design choices, including node\nsize, probing distance, stash ratio, density bound and accelerators.\nMaximum Node Size. We start with the impact of maximum\nnode sizes using the easy-to-fit Lognormal as it generates larger\nnodes close to size limits. We first set the maximum inner node size\nas 16MB and vary maximum data node sizes. In Figure 13(a), APEX\nwith 16MB maximum data node size achieves lower performance\ncompared to using maximum 256KB nodes due to higher SMO costs.\nHowever, using very small data nodes (e.g., 4KB) also leads to lower\nperformance since more inner nodes are needed to index the data,\nincreasing tree depth in Figure 13(b). This in turn causes more cache\nmisses during traversal, e.g., compared to using maximum 16MB\nnodes, using 4KB maximum nodes increases the average tree depth\n\n 0 5 10 15 20\n4KB 32KB 256KB 2MB 16MBMillion ops/s\nMax. data node size\n(a) Insert. 0 30 60 90\n4KB 32KB 256KB 2MB 16MB 0 1 2 3 4\nAvg. tree depth\nMax. data node size\n(b) Search.\n 0 5 10 15 20\n4KB 32KB 256KB 2MB 16MBMillion ops/s\nMax. inner node size\n(c) Insert. 0 30 60 90\n4KB 32KB 256KB 2MB 16MB 0 1 2 3 4\nAvg. tree depth\nMax. inner node size\n(d) Search.Figure 13: Impact of APEX node sizes (24 threads,\nLognormal ).\n 0 20 40 60 80\nLongitudesLonglat\n(a) Search.Million ops/s8 16 32 64 128\n 0 4 8 12 16 20\nLongitudes Longitudes(b) Insert (left) & scan (right).\nFigure 14: Impact of PA probing distances (24 threads).\nfrom 1.54 to 3.38 with 2.8 Ã—lower search throughput. Both insert\nand search performance peak with maximum 256KB data nodes\n(APEXâ€™s default). Then we fix the maximum data node size as 256KB\nand vary inner node maximum sizes. In Figure 13(d), although tree\ndepth increases with smaller inner nodes, search performance is\nbarely affected. The reason is the inner nodes can all fit in the\nCPU cache, so a deeper tree only needs more computation without\nmuch data movement. Since SMOs on inner nodes are rare, we also\nobserve little impact on insert performance in Figure 13(c). Thus,\nwe set 16MB as the default maximum inner node size to lower tree\ndepth and maintain good search performance.\nPA Probing Distance. We study how different bounded (maxi-\nmum) probing distances ğ·impact performance in Figure 14. For\neasy-to-fit Longitudes , increasing ğ·barely impacts search since\nthe model fits well. But for hard-to-fit Longlat , increasing ğ·from\n8 to 128 lowers search performance by 26.5%. Although a larger\nğ·reduces SA accesses, the average PA probing distance increases\n(e.g., from 3.6 to 6.5 when ğ·increases from 8 to 128) as collisions\nare more common in hard-to-fit datasets, pushing records far away\nfrom the predicted position. Using a larger ğ·also mandates more\nprobing in PA before accessing SA. As Figure 14(b) shows, insert\nperformance grows by 8% when ğ·grows from 8 to 32 which more\nefficiently resolves collisions in PA, thus reducing SA and extended\nstash accesses. However, a large ğ·like 128 reduces insert perfor-\nmance due to the higher cost of uniqueness check in PA. A higher\nğ·also reduces PAâ€™s sortedness, lowering scan performance. APEX\ntherefore uses ğ·=16to balance search/scan/insert performance.\nStash Ratio. APEX sets SA size based on data distribution and\nbounds the stash ratio in the [0.05, 0.3] range. Now we explore more\noptions by directly setting the stash ratio between 0 and 0.9. As\n 0 20 40 60\n00.10.20.30.50.70.9 daMillion ops/s\nStash ratio\n(a) Search (Longlat). 0 25 50 75\n00.10.20.30.50.70.9 da\nStash ratio\n(b) Search (Longitudes).\n 0 4 8 12 16\n00.10.20.30.50.70.9 daMillion ops/s\nStash ratio\n(c) Insert (Longlat). 0 4 8 12 16\n00.10.20.30.50.70.9 da\nStash ratio\n(d) Insert (Longitudes).Figure 15: APEX 24-thread throughput under different stash\nratios; daindicates APEXâ€™s distribution-aware approach.\nFigures 15(a)â€“(b) shows, search performance drops by 66%/59%\nonLongitudes /Longlat when stash ratio grows from 0 to 0.9.\nRecall that a larger SA leads to a smaller PA, so a higher stash ratio\nleads to more collisions in the smaller PA, routing more lookups\nin SA/extended stash. However, insert performance improves by\n8%/11% on Longitudes /Longlat when stash ratio increases from 0\nto 0.2 in Figures 15(c)â€“(d) as a relatively larger SA can absorb more\ninserts and reduce PM allocation costs for extended stash. When\nstash ratio is >0.3, insert becomes slower as PA collision overhead\ncancels out SAâ€™s collision-resolving benefits. Our distribution-aware\napproach ( da) gives nearly the best performance for both inserts\nand lookups. As Table 1 shows, the average stash ratios set by da\nare 0.05/0.118 on Longitudes /Longlat . In general, hard-to-fit data\nlikeLonglat should have a higher stash ratio to efficiently absorb\ninserts. Easy-to-fit data like Longitudes can use a lower stash ratio\nto retain the efficiency of model-based search on PA. Very low stash\nratios will degrade insert performance while high stash ratios (>\n0.3) will negatively impact both search and insert performance; this\nleads to our decision to bound the stash ratio in range of [0.05, 0.3].\nEffect of Accelerators, DRAM and Density Bound. With the\nrecommended parameters fixed, now we explore how each other\ndesign choices affect APEXâ€™s performance by conducting a factor\nanalysis on one hard-to-fit dataset: Longlat . Our results on hardest-\nto-fit FB(not shown for space limitation) has a similar trend but\nlarger improvement ratio than Longlat . We start from the a base-\nline version that comes with no accelerators, stores the whole tree\nin PM, and use the tight density bound [0.6,0.8] as ALEX does. We\nthen add additional APEX features and observe throughput under\n24 threads. Figure 16 shows the results. APEX with overflow buckets\noutperforms the baseline version with linear search in SA by 1.09 Ã—\nby reducing the cost of stash accesses. The improvement for inserts\nis 2.78Ã—, which is more significant because the overflow bucket\neffectively accelerates uniqueness check by avoiding scanning the\nwhole SA. PA fingerprints also accelerates uniqueness check during\ninserts and traversals during update/delete. This further improves\ninsert performance by 5%. Note that the accelerators added above\nare still kept in PM, therefore leading to limited improvement.\nAfter placing the accelerators and metadata in DRAM, APEXâ€™s\nsearch performance increases by 1.76 Ã—because of the reduced PM\n\n 0 5 10 15 20\nBase\n+ Overï¬‚ow-Bucket+ Fingerprints for PA+ DRAM\n+ Loose density1.002.78 2.946.497.87Million ops/s\n(a) Insert (Longlat). 0 15 30 45 60\nBase\n+ Overï¬‚ow-Bucket+ DRAM1.00 1.091.91\n(b) Search (Longlat).Figure 16: Factor analysis for APEX under 24 threads. Fea-\ntures are added from left to right and are cumulative.\nTable 3: Recovery time (s). APEX can recover instantly with\na short warm-up time under 1/24 thread(s) (in parenthesis).\n#Keys APEX LB+Tree FAST+FAIR BzTree uTree FPTree\n50M 0.042 (1.94/0.18) 3.62 0.042 0.098 21.49 1.63\n100M 0.041 (3.74/0.26) 7.20 0.041 0.109 43.27 3.26\n150M 0.042 (5.24/0.32) 10.77 0.041 0.097 65.032 4.90\naccesses. The increase for inserts is 2.21 Ã—. Note that the amount of\nDRAM-resident data used by APEX is small in most cases. For ex-\nample, APEX consumes 0.23/2.14GB of DRAM/PM after loading 100\nmillion records from Longitudes . It only consumes more DRAM in\nFB(0.68/2.29GB of DRAM/PM) since more DRAM-resident overflow\nbuckets are created for stash. This shows that APEX can leverage\nPMâ€™s high capacity and potentially reduce total system cost, by\nrequiring less or even no DRAM if the user desires. FPTree may\nconsume less DRAM [ 42] than APEX while APEX always has less\nDRAM consumption than uTree and is competitive with LB+Tree.\nFinally, using a loose density bound ([0.5,0.9]) further improves\nperformance by 1.21 Ã—, because the loose bound only incurs half of\nSMOs than using the tight bound, thus issuing less PM writes.\n6.6 Recovery\nWe now evaluate how quickly the indexes recover. We (1) load a\ncertain number of records, (2) kill the process to emulate a crash and\n(3) measure the time needed for the index to start accepting requests.\nTable 3 shows the recovery times for each index on Longlat . As\nexpected, the recovery time of LB+Tree, uTree and FPTree scales\nwith data size as their in-DRAM inner nodes need to be rebuilt. The\nmain difference between them lies in the leaf-level traversal speed,\nwhich is determined by leaf level layout. uTree exhibits the longest\nrecovery time as its leaf level is organized in a linked list with one\nrecord per node, traversing which incurs many more cache misses\nthan LB+Treeâ€™s 256B leaf nodes. FPTree recovers faster than uTree\nand LB+Tree as its leaf node size is even bigger (1KB) incurring\nfewer cache misses. The other indexes achieve instant recovery\n(<1s). At restart, APEX needs to redo/undo in-flight SMOs; such\ncases are very rare. BzTree uses PMwCAS to transparently recover\nby scanning a constant-size descriptor pool [ 53]. Both APEX and\nFAST+FAIR instantly recover with lazy recovery.\nSince APEX defers the â€œrealâ€ recovery work to runtime, we eval-\nuate how much warm-up time APEX needs before its throughput\npeaks. We load 50 million records from Longlat and then kill the\nprocess during an insert workload. After recovery, we issue lookups\n 0 1 2 3\n 0  0.5  1  1.5  2Million ops/s\nTime (seconds)1 thread\n 0 20 40 60\n 0  0.1  0.2\nTime (seconds)24 threadsFigure 17: APEXâ€™s throughput over time upon restart.\nand observe throughput. In Figure 17, red arrows indicate when\nAPEX is ready to accept requests. APEX initially achieves relatively\nlow throughput: 0.01-0.03 Mops/s with one thread thread and 0.1-\n0.4Mops/s with 24 threads. It takes 1.9s/0.15s for throughput peaks\nwith 1/24 thread(s). Using more threads helps as they can recover\ndifferent data nodes in parallel. The warm-up time scales with data\nsize, but as Table 3 shows, it is still faster than uTree and LB+Tree\nand close to FPTree; using 24 threads further reduces it to <1s.\n7 CONCLUSION\nPM offers high performance, cheap persistence and possibility of\ninstant recovery. Prior work either does not exploit the advantages\nof learned indexes or PM. Yet naively porting a learned index to PM\nresults in low performance. In this paper, we distill several general\ndesign principles for adapting the best of PM and learned indexes.\nWe apply those principles to the design and implementation of\nAPEX, a concurrent and persistent learned index with instant re-\ncovery. Our in-depth evaluation on Intel Optane DCPMM shows\nthat APEX achieves up to âˆ¼15Ã—higher throughput compared to\nrecent PM-based indexes, and can instantly recover in âˆ¼42ms.\nACKNOWLEDGMENTS\nWe thank the anonymous reviewers for their constructive com-\nments. We also thank Weiran Huang who helped with figure plot-\nting. This work is partially supported by an NSERC Discovery\nGrant, a Canada Foundation for Innovation John R. Evans Lead-\ners Fund, Hong Kong General Research Fund (14200817), Hong\nKong AoE/P-404/18, Innovation and Technology Fund (ITS/310/18,\nITP/047/19LP) and Centre for Perceptual and Interactive Intelli-\ngence (CPII) Limited under the Innovation and Technology Fund,\nMIT Data Systems and AI Lab (DSAIL), NSF IIS 1900933.\nREFERENCES\n[1]Paul Alcorn. 2019. Intel Optane DIMM Pricing: $695 for 128GB, $2595 for 256GB,\n$7816 for 512GB (Update). https://www.tomshardware.com/news/intel-optane-\ndimm-pricing-performance,39007.html , last accessed on 13/11/2021.\n[2]Joy Arulraj, Justin J. Levandoski, Umar Farooq Minhas, and Per-Ã…ke Larson. 2018.\nBzTree: A High-Performance Latch-free Range Index for Non-Volatile Memory.\nPVLDB 11, 5 (2018), 553â€“565.\n[3]AWS. 2021. OpenStreetMap on AWS. https://registry.opendata.aws/osm , last\naccessed on 13/11/2021.\n[4]Robert Binna, Eva Zangerle, Martin Pichl, GÃ¼nther Specht, and Viktor Leis. 2018.\nHOT: A Height Optimized Trie Index for Main-Memory Database Systems. In\nProceedings of the 2018 International Conference on Management of Data (Houston,\nTX, USA) (SIGMOD â€™18) . Association for Computing Machinery, New York, NY,\nUSA, 521â€“534.\n[5]Badrish Chandramouli and Jonathan Goldstein. 2014. Patience is a Virtue: Revis-\niting Merge and Sort on Modern Processors. In SIGMOD . 731â€“742.\n[6]Leying Chen and Shimin Chen. 2021. How Does Updatable Learned Index Perform\non Non-Volatile Main Memory?. In 37th IEEE International Conference on Data\nEngineering Workshops, ICDE Workshops . IEEE.\n\n[7]Shimin Chen and Qin Jin. 2015. Persistent B+-Trees in Non-Volatile Main Memory.\nPVLDB 8, 7 (2015), 786â€“797.\n[8]Youmin Chen, Youyou Lu, Kedong Fang, Qing Wang, and Jiwu Shu. 2020. uTree:\na Persistent B+-Tree with Low Tail Latency. PVLDB 13, 11 (2020), 2634â€“2648.\n[9]Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Rus-\nsell Sears. 2010. Benchmarking Cloud Serving Systems with YCSB (SoCC â€™10) .\nAssociation for Computing Machinery, New York, NY, USA, 143â€“154. https:\n//doi.org/10.1145/1807128.1807152 , last accessed on 13/11/2021.\n[10] Rob Crooke and Mark Durcan. 2015. A Revolutionary Breakthrough in Memory\nTechnology. 3D XPoint Launch Keynote (2015).\n[11] Angjela Davitkova, Evica Milchevski, and Sebastian Michel. 2020. The ML-Index:\nA Multidimensional, Learned Index for Point, Range, and Nearest-Neighbor\nQueries. In Proceedings of the 23rd International Conference on Extending Database\nTechnology, EDBT 2020, Copenhagen, Denmark, March 30 - April 02, 2020 , Angela\nBonifati, Yongluan Zhou, Marcos Antonio Vaz Salles, Alexander BÃ¶hm, Dan\nOlteanu, George H. L. Fletcher, Arijit Khan, and Bin Yang (Eds.). OpenProceed-\nings.org, 407â€“410. https://doi.org/10.5441/002/edbt.2020.44 , last accessed on\n13/11/2021.\n[12] Biplob Debnath, Alireza Haghdoost, Asim Kadav, Mohammed G. Khatib, and\nCristian Ungureanu. 2015. Revisiting hash table design for phase change memory.\nInProceedings of the 3rd Workshop on Interactions of NVM/FLASH with Operating\nSystems and Workloads, INFLOW 2015, Monterey, California, USA, October 4, 2015 .\n1:1â€“1:9.\n[13] Cristian Diaconu, Craig Freedman, Erik Ismert, Per-Ã…ke Larson, Pravin Mittal,\nRyan Stonecipher, Nitin Verma, and Mike Zwilling. 2013. Hekaton: SQL serverâ€™s\nmemory-optimized OLTP engine. In Proceedings of the ACM SIGMOD International\nConference on Management of Data, SIGMOD 2013, New York, NY, USA, June 22-27,\n2013. ACM, 1243â€“1254.\n[14] Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\nDavid Lomet, and Tim Kraska. 2020. ALEX: An Updatable Adaptive Learned Index.\nInProceedings of the 2020 ACM SIGMOD International Conference on Management\nof Data (Portland, OR, USA) (SIGMOD â€™20) . Association for Computing Machinery,\nNew York, NY, USA, 969â€“984. https://doi.org/10.1145/3318464.3389711 , last\naccessed on 13/11/2021.\n[15] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A Learned Multi-dimensional Index for Correlated Data and Skewed\nWorkloads. PVLDB 14, 2 (2020), 74â€“86.\n[16] Jason Evans. 2006. A Scalable Concurrent malloc (3) Implementation for FreeBSD.\nInProceedings of the BSDCan Conference .\n[17] Paolo Ferragina and Giorgio Vinciguerra. 2020. The PGM-Index: A Fully-Dynamic\nCompressed Learned Index with Provable Worst-Case Bounds. PVLDB 13, 8 (2020),\n1162â€“1175.\n[18] Keir Fraser. 2004. Practical lock-freedom . Ph.D. Dissertation. University of Cam-\nbridge, UK.\n[19] Alex Galakatos, Michael Markovitch, Carsten Binnig, Rodrigo Fonseca, and Tim\nKraska. 2019. FITing-Tree: A Data-Aware Index Structure. In Proceedings of\nthe 2019 International Conference on Management of Data (Amsterdam, Nether-\nlands) (SIGMOD â€™19) . Association for Computing Machinery, New York, NY, USA,\n1189â€“1206. https://doi.org/10.1145/3299869.3319860 , last accessed on 13/11/2021.\n[20] Deukyeon Hwang, Wook-Hee Kim, Youjip Won, and Beomseok Nam. 2018. En-\ndurable transient inconsistency in byte-addressable persistent B+-tree. In 16th\nUSENIX Conference on File and Storage Technologies (FAST 18) . 187â€“200.\n[21] Intel. 2021. Intel Optane Persistent Memory (PMem). https:\n//www.intel.ca/content/www/ca/en/architecture-and-technology/optane-\ndc-persistent-memory.html , last accessed on 13/11/2021.\n[22] Intel. 2021. Persistent Memory Development Kit. (2021). http://pmem.io/pmdk/ ,\nlast accessed on 13/11/2021.\n[23] Intel Corporation. 2021. Intel 64 and IA-32 Architectures Software Developerâ€™s\nManual. (2021). https://software.intel.com/content/www/us/en/develop/articles/\nintel-sdm.html , last accessed on 13/11/2021.\n[24] Intel Corporation. 2021. Intel Optane Persistent Memory 200 Series Delivers 32\nPercent More Bandwidth on Average1 with up to 6 TB Total Memory per Socket.\n(2021). https://www.intel.com/content/www/us/en/products/docs/memory-\nstorage/optane-persistent-memory/optane-persistent-memory-200-series-\nbrief.html , last accessed on 13/11/2021.\n[25] Andreas Kipf, Ryan Marcus, Alexander van Renen, Mihail Stoian, Alfons Kemper,\nTim Kraska, and Thomas Neumann. 2020. RadixSpline: A Single-Pass Learned\nIndex. In Proceedings of the Third International Workshop on Exploiting Artifi-\ncial Intelligence Techniques for Data Management (Portland, Oregon) (aiDM â€™20) .\nAssociation for Computing Machinery, New York, NY, USA, Article 5, 5 pages.\nhttps://doi.org/10.1145/3401071.3401659 , last accessed on 13/11/2021.\n[26] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe Case for Learned Index Structures. In Proceedings of the 2018 International\nConference on Management of Data (Houston, TX, USA) (SIGMOD â€™18) . Association\nfor Computing Machinery, New York, NY, USA, 489â€“504. https://doi.org/10.1145/\n3183713.3196909 , last accessed on 13/11/2021.[27] Tim Kraska, Umar Farooq Minhas, Thomas Neumann, Olga Papaemmanouil,\nJignesh M. Patel, Chris RÃ©, and Michael Stonebraker. 2021. ML-In-Databases:\nAssessment and Prognosis. IEEE Data Engineering Bulletin 44, 1 (2021), 3.\n[28] Se Kwon Lee, K. Hyun Lim, Hyunsub Song, Beomseok Nam, and Sam H. Noh.\n2017. WORT: Write Optimal Radix Tree for Persistent Memory Storage Systems.\nIn15th USENIX Conference on File and Storage Technologies (FAST 17) . USENIX\nAssociation, Santa Clara, CA, 257â€“270.\n[29] Se Kwon Lee, Jayashree Mohan, Sanidhya Kashyap, Taesoo Kim, and Vijay Chi-\ndambaram. 2019. RECIPE: Converting Concurrent DRAM Indexes to Persistent-\nMemory Indexes. In Proceedings of the 27th ACM Symposium on Operating Systems\nPrinciples (Huntsville, Ontario, Canada) (SOSP â€™19) . Association for Comput-\ning Machinery, New York, NY, USA, 462â€“477. https://doi.org/10.1145/3341301.\n3359635 , last accessed on 13/11/2021.\n[30] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. The Adaptive Radix\nTree: ARTful Indexing for Main-Memory Databases. In Proceedings of the 2013\nIEEE International Conference on Data Engineering (ICDE 2013) (ICDE â€™13) . IEEE\nComputer Society, USA, 38â€“49.\n[31] Viktor Leis, Florian Scheibner, Alfons Kemper, and Thomas Neumann. 2016.\nThe ART of Practical Synchronization. In Proceedings of the 12th International\nWorkshop on Data Management on New Hardware (DaMoN â€™16) . Article 3, 8 pages.\n[32] Lucas Lersch, Xiangpeng Hao, Ismail Oukid, Tianzheng Wang, and Thomas\nWillhalm. 2019. Evaluating Persistent Memory Range Indexes. PVLDB 13, 4\n(2019), 574â€“587.\n[33] Justin J. Levandoski, David B. Lomet, and Sudipta Sengupta. 2013. The Bw-Tree: A\nB-tree for New Hardware Platforms. In Proceedings of the 2013 IEEE International\nConference on Data Engineering (ICDE 2013) (ICDE â€™13) . 302â€“313.\n[34] Pengfei Li, Hua Lu, Qian Zheng, Long Yang, and Gang Pan. 2020. LISA: A\nLearned Index Structure for Spatial Data. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data (Portland, OR, USA) (SIGMOD\nâ€™20). Association for Computing Machinery, New York, NY, USA, 2119â€“2133.\nhttps://doi.org/10.1145/3318464.3389703 , last accessed on 13/11/2021.\n[35] Jihang Liu, Shimin Chen, and Lujun Wang. 2020. LB+Trees: Optimizing Persistent\nIndex Performance on 3DXPoint Memory. PVLDB 13, 7 (2020), 1078â€“1090.\n[36] Baotong Lu, Xiangpeng Hao, Tianzheng Wang, and Eric Lo. 2020. Dash: Scalable\nHashing on Persistent Memory. PVLDB 13, 8 (2020), 1147â€“1161.\n[37] Shaonan Ma, Kang Chen, Shimin Chen, Mengxing Liu, Jianglang Zhu, Hongbo\nKang, and Yongwei Wu. 2021. ROART: Range-query Optimized Persistent ART.\nIn19th USENIX Conference on File and Storage Technologies (FAST 21) . 1â€“16.\n[38] Yandong Mao, Eddie Kohler, and Robert Tappan Morris. 2012. Cache craftiness\nfor fast multicore key-value storage. In Proceedings of the 7th ACM european\nconference on Computer Systems . 183â€“196.\n[39] Ryan Marcus, Andreas Kipf, Alexander van Renen, Mihail Stoian, Sanchit Misra,\nAlfons Kemper, Thomas Neumann, and Tim Kraska. 2020. Benchmarking Learned\nIndexes. PVLDB 14, 1 (2020), 1â€“13.\n[40] Moohyeon Nam, Hokeun Cha, Young ri Choi, Sam H. Noh, and Beomseok Nam.\n2019. Write-Optimized Dynamic Hashing for Persistent Memory. In 17th USENIX\nConference on File and Storage Technologies (FAST 19) . 31â€“44.\n[41] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning Multi-Dimensional Indexes. In Proceedings of the 2020 ACM SIGMOD In-\nternational Conference on Management of Data (Portland, OR, USA) (SIGMOD\nâ€™20). Association for Computing Machinery, New York, NY, USA, 985â€“1000.\nhttps://doi.org/10.1145/3318464.3380579 , last accessed on 13/11/2021.\n[42] Ismail Oukid, Johan Lasperas, Anisoara Nica, Thomas Willhalm, and Wolfgang\nLehner. 2016. FPTree: A Hybrid SCM-DRAM Persistent and Concurrent B-Tree\nfor Storage Class Memory. In Proceedings of the 2016 International Conference on\nManagement of Data, SIGMOD . 371â€“386.\n[43] Jianzhong Qi, Guanli Liu, Christian S. Jensen, and Lars Kulik. 2020. Effectively\nLearning Spatial Indices. PVLDB 13, 12 (2020), 2341â€“2354.\n[44] Jun Rao and Kenneth A. Ross. 2000. Making B+-Trees Cache Conscious in\nMain Memory. In Proceedings of the 2000 ACM SIGMOD International Conference\non Management of Data (Dallas, Texas, USA) (SIGMOD â€™00) . Association for\nComputing Machinery, New York, NY, USA, 475â€“486.\n[45] Andy Rudoff. 2019. Protecting SW From Itself: Powerfail Atomicity for Block\nWrites. Persistent Programming in Real Life (2019). https://pirl.nvsl.io/PIRL2019-\ncontent/PIRL-2019-Andy-Rudoff.pdf , last accessed on 13/11/2021.\n[46] Benjamin Spector, Andreas Kipf, Kapil Vaidya, Chi Wang, Umar Farooq Minhas,\nand Tim Kraska. 2021. Bounding the Last Mile: Efficient Learned String Indexing\n(Extended Abstracts). In 3rd International Workshop on Applied AI for Database\nSystems and Applications, AIDB Workshops .\n[47] D. B. Strukov, G. S. Snider, D. R. Stewart, and R. S. Williams. 2008. The missing\nmemristor found. Nature 453, 7191 (2008), 80â€“83.\n[48] Chuzhe Tang, Youyun Wang, Zhiyuan Dong, Gansen Hu, Zhaoguo Wang, Min-\njie Wang, and Haibo Chen. 2020. XIndex: A Scalable Learned Index for Mul-\nticore Data Storage. In Proceedings of the 25th ACM SIGPLAN Symposium on\nPrinciples and Practice of Parallel Programming (San Diego, California) (PPoPP\nâ€™20). Association for Computing Machinery, New York, NY, USA, 308â€“320.\nhttps://doi.org/10.1145/3332466.3374547 , last accessed on 13/11/2021.\n\n[49] Transaction Processing Performance Council (TPC). 2015. TPC Benchmark E\nStandard Specification, revision 1.14.0. http://www.tpc.org/tpce , last accessed\non 13/11/2021.\n[50] Alexander van Renen, Lukas Vogel, Viktor Leis, Thomas Neumann, and Alfons\nKemper. 2019. Persistent Memory I/O Primitives. In Proceedings of the 15th\nInternational Workshop on Data Management on New Hardware, DaMoN 2019.\n12:1â€“12:7.\n[51] Shivaram Venkataraman, Niraj Tolia, Parthasarathy Ranganathan, Roy H Camp-\nbell, et al .2011. Consistent and Durable Data Structures for Non-Volatile Byte-\nAddressable Memory.. In 9th USENIX Conference on File and Storage Technologies\n(FAST 11) , Vol. 11. USENIX Association, 61â€“75.\n[52] Haixin Wang, Xiaoyi Fu, Jianliang Xu, and Hua Lu. 2019. Learned Index for Spatial\nQueries. In 2019 20th IEEE International Conference on Mobile Data Management\n(MDM) . 569â€“574.\n[53] Tianzheng Wang, Justin Levandoski, and Per-Ã…ke Larson. 2018. Easy Lock-Free\nIndexing in Non-Volatile Memory. In 2018 IEEE 34th International Conference on\nData Engineering (ICDE) . 461â€“472.\n[54] Youyun Wang, Chuzhe Tang, Zhaoguo Wang, and Haibo Chen. [n.d.]. SIndex: a\nscalable learned index for string keys. In APSys â€™20: 11th ACM SIGOPS Asia-Pacific\nWorkshop on Systems, Tsukuba, Japan, August 24-25, 2020 . 17â€“24.\n[55] H. S P Wong, S. Raoux, SangBum Kim, Jiale Liang, John P. Reifenberg, B. Rajen-\ndran, Mehdi Asheghi, and Kenneth E. Goodson. 2010. Phase Change Memory.\nProc. IEEE 98, 12 (2010), 2201â€“2227.\n[56] Jiacheng Wu, Yong Zhang, Shimin Chen, Jin Wang, Yu Chen, and Chunxiao Xing.\n2021. Updatable Learned Index with Precise Positions. arXiv:2104.05520 [cs.DB]\n[57] Jian Yang, Juno Kim, Morteza Hoseinzadeh, Joseph Izraelevitz, and Steven Swan-\nson. 2020. An Empirical Guide to the Behavior and Use of Scalable Persistent\nMemory. In 18th USENIX Conference on File and Storage Technologies, FAST 2020,\nSanta Clara, CA, February 24-27 .\n[58] Jun Yang, Qingsong Wei, Cheng Chen, Chundong Wang, Khai Leong Yong, and\nBingsheng He. 2015. NV-Tree: reducing consistency cost for NVM-based single\nlevel systems. In 13th USENIX Conference on File and Storage Technologies (FAST\n15). 167â€“181.\n[59] Zongheng Yang, Badrish Chandramouli, Chi Wang, Johannes Gehrke, Yinan Li,\nUmar Farooq Minhas, Per-Ã…ke Larson, Donald Kossmann, and Rajeev Acharya.\n2020. Qd-Tree: Learning Data Layouts for Big Data Analytics. In Proceedings of the\n2020 ACM SIGMOD International Conference on Management of Data (Portland, OR,\nUSA) (SIGMOD â€™20) . Association for Computing Machinery, New York, NY, USA,\n193â€“208. https://doi.org/10.1145/3318464.3389770 , last accessed on 13/11/2021.\n[60] Huanchen Zhang, David G. Andersen, Andrew Pavlo, Michael Kaminsky, Lin\nMa, and Rui Shen. 2016. Reducing the Storage Overhead of Main-Memory OLTP\nDatabases with Hybrid Indexes. In Proceedings of the 2016 International Conference\non Management of Data (San Francisco, California, USA) (SIGMOD â€™16) . ACM,\nNew York, NY, USA, 1567â€“1581. http://doi.acm.org/10.1145/2882903.2915222 ,\nlast accessed on 13/11/2021.\n[61] Xinjing Zhou, Lidan Shou, Ke Chen, Wei Hu, and Gang Chen. 2019. DPTree:\nDifferential Indexing for Persistent Memory. PVLDB 13, 4 (2019), 421â€“434.\n[62] Pengfei Zuo, Yu Hua, and Jie Wu. 2018. Write-Optimized and High-Performance\nHashing Index Scheme for Persistent Memory. In 13th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 18) . 461â€“476.\n 0 30 60 90\n8B(in) 8 16 32 64 128LognormalMillion ops/s\nSize of payload (bytes)APEX\nDPTreeLB+-Tree\nFAST+FAIRFPTree\nuTreeBZTree\n 0 20 40 60\n8B(in) 8 16 32 64 128Longlat\nSize of payload (bytes)Figure 18: Search throughput under 24 threads with differ-\nent payload sizes on Lognormal and Longlat .\nAPPENDIX\nA COMPLEXITY ANALYSIS\nNow we analyze the complexity of APEX operations. Let ğ‘›andğ‘š\nbe the maximum number of slots in inner and data nodes (with PA\nand SA), respectively. Although data nodes may allocate slots in the\nextended stash, the maximum number of records that can be stored\nin a data node is fixed ( ğ‘šÃ—ğ‘‘ğ‘¢whereğ‘‘ğ‘¢is the upper bound density)\nsince an SMO will start if the number of records would exceed\nğ‘šÃ—ğ‘‘ğ‘¢. Following ALEXâ€™s proof [ 14], traversing from root to leaf\ntakesâŒˆğ‘™ğ‘œğ‘”ğ‘›ğ‘âŒ‰time, where ğ‘is the minimum number of partitions\nsuch that when the key space ğ‘ is divided into ğ‘partitions of equal\nwidth, every partition contains no more than ğ‘šÃ—ğ‘‘ğ‘¢keys. We omit\nthe proof details here and interested readers could refer to [14].\nIn the best case, inserting or searching a data node takes ğ‘‚(1)\ntime if the model fits well. If the predicted PA position for insert is\nclose to a free slot within the bounded probing distance, the record\ncould be inserted in ğ‘‚(1)complexity; Future search operation may\nalso directly hit the target record with linear probing in ğ‘‚(1)time.\nIf the collisions cannot be addressed in the PA, a linear search in the\nSA may be required to find a free slot, or an extended stash will be\nallocated. The worst-case cost for insert is bounded by ğ‘‚(ğ‘š)since\nthe extended stash space could not be infinitely allocated given the\nSMO condition introduced earlier. Searching a data node may also\nneed accessing the stash area by linearly traversing the overflow\nbuckets, which again has a bounded cost of ğ‘‚(ğ‘š).\nB IMPACT OF PAYLOAD SIZE\nIn this appendix, we study the impact of payload size on the lookup\nperformance of the evaluated trees. Using an 8-byte pointer as\nthe payload in index leaf (data) nodes is common in main-memory\nindexes [ 13]. The open-source implementations of other trees under\ncomparison [ 2,8,20,35,42,61] all also assume their payloads are\n8-byte pointers. Therefore, for all indexes more cache misses are\nlikely to happen when the external payload is accessed.\nTo explore how such cache misses impact each index, we con-\nducted an experiment to show how indexes perform when the\npayload is externally stored with varying sizes (from 8-byte to\n128-byte). We also include the performance when the payload is\nan inlined 8-byte value for reference (named 8B(in) in Figure 18).\nAs shown in Figure 18, compared to using 8-byte inline payloads,\nAPEXâ€™s search throughput drops by 44%/32% on Lognormal and\nLonglat respectively when accessing external payloads. Beyond\n\n 0 20 40 60\n11.5 2 3 4 5Million ops/s\nStash coeï¬ƒcient\n(a) Search (Longlat). 0 4 8 12 16\n11.5 2 3 4 5\nStash coeï¬ƒcient\n(b) Insert (Longlat).Figure 19: Search (left) and insert (right) throughput of\nAPEX (24 threads) with different stash-coefficients on\nLonglat .\n16-byte payloads, the performance in general drops with bigger\npayload sizes. Our profiling results show that the number of cache\nmisses per search operation of APEX increases from 3.36 to 4.51 on\nLognormal (i.e., 34% more cache misses) when the 8-byte payload\nis changed from inline-stored to external-stored. The numbers for\nFPTree increases from 9.41 to 10.39 (i.e., 10% more cache misses), i.e.,\nits performance degradation is not as obvious as APEXâ€™s. However,\neven in the condition of external payload accesses (from 8-byte to\n128-byte), APEXâ€™s search performance still achieves much higher\nthroughput due to the efficient model-based search.\nC IMPACT OF STASH COEFFICIENT\nAs stated in Section 4.5, we set stash ratio ğ‘†to be a multiple ( ğ‘›) of\noverflow ratio ğ‘‚, i.e.,ğ‘†=ğ‘šğ‘ğ‘¥(0.05,ğ‘šğ‘–ğ‘›(0.3,ğ‘›Ã—ğ‘‚))and empirically\ndetermined stash coefficient ğ‘›â€™s value via experiments. We show\nthe experimental results in Figure 19. As the figure shows, setting ğ‘›\nas 1.5 achieves both good search and insert performance. In general,\na larger stash coefficient means potentially more keys are stored\nin the stash areas. When varying ğ‘›from 1 to 5, the average stash\nratio increases from 0.09 to 0.22 (i.e., 22% of the data nodeâ€™s space is\nallocated to the stash array). As the figure shows, the impact is more\npronounced for search operations: a larger ğ‘›(e.g., 3 and 4) means\nmore search operations are routed into stash array but no insert\nperformance improvement. We therefore chose 1.5 as the default\nstash coefficient and overall this gives reasonable performance\nand is simple to implement/calculate. We hope to explore ways to\nimprove on space allocation between primary/stash arrays (e.g.,\nfine-tuning the stash coefficient) to further optimize performance\nin future work.\nD COMPARISON WITH FULL-DRAM APEX\nWe study the impact of PM by comparing with the full-DRAM\nAPEX. Specifically, we ran APEX on DRAM by pointing PMDK\nto use DRAM, same as in prior work [ 32] (Lersch et al.). As Fig-\nure 20 shows, the DRAM version performs 2.4 Ã—/2.3Ã—faster on\nLongitudes andLonglat for search operations. The numbers for\ninserts are 3.3Ã—/2.7Ã—. APEX on DRAM also achieves near-linear\nscalability, thanks to DRAMâ€™s âˆ¼4Ã—lower latency andâˆ¼3-14Ã—higher\nbandwidth than PMâ€™s. Also, DRAMâ€™s adequate bandwidth allows\nAPEX to benefit from hyperthreading which helps hide CPU stalls.\nSince future PM is expected to have higher bandwidth (e.g., the\nrecent Optane 200 Series already delivers 32% more bandwidth on\n 0 15 30 45 60\n 1 8 16  24  32  48Million ops/s\nNumber of threads\n (a) Insert (Longitudes).APEX (DRAM+PM) APEX (DRAM only)\n 0 50 100 150 200\n 1 8 16  24  32  48\nNumber of threads\n (b) Search (Longitudes).\n 0 10 20 30 40 50\n 1 8 16  24  32  48Million ops/s\nNumber of threads\n (c) Insert (Longlat). 0 40 80 120 160\n 1 8 16  24  32  48\nNumber of threads\n (d) Search (Longlat).Figure 20: Throughput of APEX (DRAM+PM) and APEX\n(DRAM-only) on Longitudes (a-b) and Longlat (c-d).\n 0 20 40 60 80 100\nLonglatLongitudes LognormalFB YCSB TPC-EBulk Load Time (s)APEX\nLB+-Tree\nFigure 21: Bulk loading time comparison.\naverage [ 24]), this experiment also give hints how APEX might\nperform on newer devices.\nE MEMORY (PM/DRAM) CONSUMPTION\nFor PM/DRAM consumption, we measure the memory consumption\nof all indexes after bulk loading 100 million records on different\ndatasets (âˆ¼1.5GB of raw data), shown in Table 4. Except for APEX,\nthe memory consumption of other indexes does not vary a lot on\ndifferent data sets so we only report their memory consumption\nonLognormal . All indexes require extra space than raw data sizes\nbecause they reserve empty slots to absorb future inserts. Compared\nto APEX, the other indexes require more PM space because they\neither store extra metadata in PM (e.g., fingerprints in FPTree) or put\nthe whole tree in PM (e.g., BzTree and FAST+FAIR). APEX consumes\nmore DRAM than FPTree and DPTree but its DRAM consumption\nis always smaller than uTree (up to 13 Ã—smaller) which duplicates\nthe whole index in DRAM. In most datasets, APEX has a smaller or\nsimilar DRAM consumption as LB+Tree. The only exception is FB\n(0.676GB DRAM of APEX vs. 0.265GB of LB+Tree), where APEX\nhas to allocate more DRAM-resident overflow buckets for stash due\ntoFBâ€™s extremely non-linear distribution.\nF BULK-LOADING TIME\nIn this section, we study the cost of initial bulk loading. We con-\nducted an experiment to show the time for bulk loading 100 million\nrecords on various data sets. Among the other indexes under com-\nparison, only LB+Tree provides a fast bulk loading algorithm while\nothers all require inserting keys from scratch. For fair comparison,\n\nTable 4: PM and DRAM space consumption (GB). Except for APEX, other indexes only present the memory consumption on\nLognormal data set.\nAPEX ( Longitudes ) APEX ( Longlat ) APEX ( Lognormal ) APEX (FB) APEX (YCSB) APEX (TPC-E) LB+Tree uTree DPTree FPTree FAST+FAIR BzTree\nPM 2.139 2.159 2.152 2.29 2.138 2.244 2.649 2.98 3.03 2.542 2.47 3.46\nDRAM 0.232 0.311 0.226 0.676 0.227 0.265 0.265 2.396 0.055 0.055 0 0\nwe test bulk loading time of APEX and LB+Tree. As shown in Fig-\nure 21, both trees may perform differently depending on the dataset.\nOn average APEX takes 28% less time. In the worst case ( TPC-E andFB), APEX is 2.13Ã—slower than LB+Tree because APEX takes more\ntime to search the best node fanout in its fanout space. We hope to\nfurther investigate on improving for such cases in future work.",
  "textLength": 100889
}