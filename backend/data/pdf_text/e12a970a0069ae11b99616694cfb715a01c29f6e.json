{
  "paperId": "e12a970a0069ae11b99616694cfb715a01c29f6e",
  "title": "Detect, Distill and Update: Learned DB Systems Facing Out of Distribution Data",
  "pdfPath": "e12a970a0069ae11b99616694cfb715a01c29f6e.pdf",
  "text": " \n \n \n \n \nwarwick.ac.uk/lib -publications  \n \n \n \n \n \nManuscript version : Authorâ€™s Accepted Manuscript  \nThe version presented in WRAP is the authorâ€™s accepted manuscript and may differ from the \npublished version or  Version of R ecord . \n \nPersistent  WRAP URL:  \nhttp://wrap.warwick.ac.uk/ 172075                               \n \nHow to cite : \nPlease refer to published version for the most recent bibliographic citation information.   \nIf a published version is known of, the repository item page  linked to above , will contain \ndetails on accessing  it. \n \nCopyright and reuse:  \nThe Warwick Research Archive Portal (WRAP) makes this work by researchers of the \nUniversity of Warwick available open access u nder the following conditions.  \n \nCopyright Â© and all moral rights to the version of the paper presented here belong to the \nindividual author(s) and/or other copyright owners. To the extent reasonable and \npracticable the material made available in WRAP has been checked for eligibility before \nbeing made available.  \n \nCopies of full items can be used for personal research or study, educational, or not -for-profit \npurposes witho ut prior permission or charge. Prov ided that the authors, title and full \nbibliographic details are credited, a hyperlink and/or URL is given for the original metadata \npage and the content is not changed in any way.  \n \nPublisherâ€™s statement:  \nPlease refer to the repository item page, publisherâ€™ s statement  section , for further \ninformation.  \n \nFor more information, please contact the WRAP Team at: wrap@warwick.ac.uk . \n \n\nAccepted as a conference paper for SIGMOD 2023 @Seattle, WA, USA\nDetect, Distill and Update:\nLearned DB Systems Facing Out of Distribution Data\nMeghdad Kurmanji , Peter Triantafillou\nUniversity of Warwick, Coventry, UK\n{meghdad.kurmanji,p.triantafillou}@warwick.ac.uk\nABSTRACT\nMachine Learning (ML) is changing DBs as many DB components\nare being replaced by ML models. One open problem in this setting\nis how to update such ML models in the presence of data updates.\nWe start this investigation focusing on data insertions (dominating\nupdates in analytical DBs). We study how to update neural network\n(NN) models when new data follows a different distribution (a.k.a. it\nis \"out-of-distribution\" â€“ OOD), rendering previously-trained NNs\ninaccurate. A requirement in our problem setting is that learned\nDB components should ensure high accuracy for tasks on old and\nnew data (e.g., for approximate query processing (AQP), cardinality\nestimation (CE), synthetic data generation (DG), etc.).\nThis paper proposes a novel updatability framework (DDUp).\nDDUp can provide updatability for different learned DB system\ncomponents, even based on different NNs, without the high costs\nto retrain the NNs from scratch. DDUp entails two components:\nFirst, a novel, efficient, and principled statistical-testing approach\nto detect OOD data. Second, a novel model updating approach,\ngrounded on the principles of transfer learning with knowledge\ndistillation, to update learned models efficiently, while still ensuring\nhigh accuracy. We develop and showcase DDUpâ€™s applicability for\nthree different learned DB components, AQP, CE, and DG, each\nemploying a different type of NN. Detailed experimental evaluation\nusing real and benchmark datasets for AQP, CE, and DG detail\nDDUpâ€™s performance advantages.\nKEYWORDS\nLearned DBs, Out of Distribution Data, Knowledge Distillation,\nTransfer Learning\n1 INTRODUCTION\nDatabase systems (DBs) are largely embracing ML. With data vol-\numes reaching unprecedented levels, ML can provide highly-accurate\nmethods to perform central data management tasks more efficiently.\nApplications abound: AQP engines are leveraging ML to answer\nqueries much faster and more accurately than traditional DBs\n[21,42,43,65]. Cardinality/selectivity estimation, has improved\nconsiderably leveraging ML [ 17,70,77,78,84]. Likewise for query\noptimization [ 27,44,45], indexes [ 9,10,30,49], cost estimation\n[63,83], workload forecasting [ 85], DB tuning [ 34,68,81], syn-\nthetic data generation [7, 54, 76], etc.\n1.1 Challenges\nAs research in learned DB systems matures, two key pitfalls are\nemerging. First, if the \"context\" (such as the data, the DB system,\nand/or the workload) changes, previously trained models are no\nlonger accurate. Second, training accurate ML models is costly.\nHence, retraining from scratch when the context changes shouldbe avoided whenever possible. Emerging ML paradigms, such as\nactive learning, transfer learning, meta-learning, and zero/few-shot\nlearning are a good fit for such context changes and have been the\nfocus of recent related works [ 20,41,74], where the primary focus\nis to glean what is learned from existing ML models (trained for\ndifferent learning tasks and/or DBs and/or workloads), and adapt\nthem for new tasks and/or DBs, and/or workloads, while avoiding\nthe need to retrain models from scratch.\nOOD Data insertions. In analytical DBs data updates primar-\nily take the form of new data insertions. New data may be OOD\n(representing new knowledge â€“ distributional shifts), rendering\npreviously-built ML models obsolete/inaccurate. Or, new data may\nnot be OOD. In the former case, the model must be updated and\nit must be decided how the new data could be efficiently reflected\nin the model to continue ensuring accuracy. In the latter case, it is\ndesirable to avoid updating the model, as that would waste time/re-\nsources. Therefore, it is also crucial to check (efficiently) whether\nthe new data render the previously built model inaccurate. However,\nrelated research has not yet tackled this problem setting, whereby\nmodels for the same learning tasks (e.g., AQP, DG, CE, etc.) trained\non old data, continue to provide high accuracy for the new data state\n(on old and new data, as queries now may access both old data\nand new data, old data, or simply the new data). Related work for\nlearned DB systems have a limited (or sometimes completely lack\nthe) capability of handling such data insertions (as is independently\nverified in [70] and will be shown in this paper as well).\nSources of Difficulty and Baselines. In the presence of OOD,\na simple solution is adopted by some of the learned DB compo-\nnents like Naru [ 78], NeuroCard [ 77], DBest++ [ 42], and even the\naforementioned transfer/few-shot learning methods [ 20,74]. That\nis to \"fine-tune\" the original model ğ‘€on the new data. Alas, this\nis problematic. For instance, while a DBest++ model on the \"For-\nest\" dataset has a 95th percentile q-error of 2, updating it with an\nOOD sample using fine-tuning increases the 95th q-error to 63. A\nsimilar accuracy drop occurs for other key models as well â€“ [ 70]\nshowcases this for learned CE works. This drastic drop of accuracy\nis due to the fundamental problem of catastrophic forgetting [46],\nwhere retraining a previously learned model on new tasks, i.e. new\ndata, causes the model to lose the knowledge it had acquired about\nold data. To avoid catastrophic forgetting , Naru and DBest++ sug-\ngest using a smaller learning rate while fine-tuning with the new\ndata. This, however, causes another fundamental problem, namely\nintransigence , [6] whereby the model resists fitting to new data,\nrendering queries on new data inaccurate.\nAnother simple solution to avoid these problems would be to\naggregate the old data and new data and retrain the model from\nscratch. However, as mentioned, this is undesirable in our envi-\nronment. As a concrete example, training Naru/NeuroCard on the\n1arXiv:2210.05508v2  [cs.DB]  8 Dec 2022\n\n\"Forest\" dataset (with only 600k rows) on a 40-core CPU takes ca. 1.5\nhours. Similarly high retraining overheads are typically observed\nfor neural network models, for various tasks. And, retraining time\nprogressively increases as the DB size increases.\nTherefore, more sophisticated approaches are needed, which\ncan avoid intransigence andcatastrophic forgetting , update models\nonly when needed and do so while ensuring much smaller training\noverheads than retraining from scratch and at the same time ensure\nhigh accuracy for queries on old and new data. While for some\ntasks, like CE, some researchers question whether achieving very\nhigh accuracy through learned models will actually help the end-\ntask (query optimization) [ 44], for tasks like AQP (which is itself\nthe end-task) and for DG (with classification as the end-task) high\naccuracy is clearly needed, as shown here. Even for CE, with OOD\ndata, accuracy can become horribly poor, as shown here, which is\nlikely to affect query optimization.\n1.2 Contributions\nTo the best of our knowledge, this work proposes the first updata-\nbility framework (DDUp) for learned DBs (in the face of new data\ninsertions possibly carrying OOD data) that can ensure high accu-\nracy for queries on new and/or old data. DDUp is also efficient and\nit can enjoy wide applicability, capable of being utilized for different\nNNs and/or different learning tasks (such as AQP, DG, CE, etc.).\nDDUp consists of a novel OOD detection and a novel model-update\nmodule. More specifically, the contributions of DDUp are:\nâ€¢A general and principled two-sample test for OOD detection.\nGenerality stems from it being based on the training loss function\nof the NNs. Compared to prior art, it introduces no extra costs and\noverheads, and could be used with different NNs, with different\nloss functions, in different applications. To further minimize\ndetection time, it is divided into offline and online phases.\nâ€¢A novel and general formulation of transfer-learning based on\nsequential self-distillation for model updating. This formulation\nallows a higher degree of freedom in balancing tasks w.r.t new\nand old data, can adapt to different models and tasks, and maxi-\nmizes performance via self-distillation.\nâ€¢Importantly, DDUp can be used by any pre-trained NN without\nintroducing any assumptions on models or requiring additional\ncomponents that might require to retrain models or incur more\ncosts. Here, we instantiate it for three different tasks (namely,\nthe CE task, using the Naru/NeuroCard deep autoregressive net-\nwork (DARN) models [ 77,78], the AQP task, using the DBEst++\nmixture density network (MDN) model [ 42], and for the DG task,\nusing the Tabular Variational AutoEncoder (TVAE) model [76])\neach of which employs a different NN type. These are represen-\ntative learning tasks and networks with evident importance in\nDBs and beyond. These instantiations are also novel, showing\nhow to distil-and-update MDNs, DARNs, and TVAEs.\nâ€¢Finally, DDUp is evaluated using six different datasets and the\nthree instantiated learned DB components, for AQP, CE, and DG\n1.3 Limitations\nDDUp focuses only on data insertions, which are essential and dom-\ninant in analytical DBs, and not on updates in place and deletes,which are prevalent in transactional DBs. Nonetheless, the lat-\nter touch upon an open problem in the ML literature, namely\nâ€ğ‘¢ğ‘›ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘–ğ‘›ğ‘” â€, where it typically concerns privacy (e.g., removing\nsensitive data from images in classification tasks) (e.g., [ 15,61]).\nStudying unlearning for DB problem settings is a formidable task\nof its own and of high interest for future research.\nAlso, DDUp is designed for NN-based learned DB components.\nThis is so as neural networks are a very rich family of models\nwhich have collectively received very large attention for learned\nDBs. Extending DDUp principles beyond NN models is also left for\nfuture research.\n2 THE PROBLEM AND SOLUTION\nOVERVIEW\n2.1 Problem Formulation\nConsider a database relation ğ‘…with attributes{ğ´1,ğ´2,...,ğ´ğ‘š}. This\ncan be a raw table or the result of a join query. Also consider\na sequence of ğ‘insertion updates denoted by ğ¼={ğ¼1,ğ¼2,...ğ¼ğ‘}.\nEachğ¼ğ‘¡is an insert operation which appends a data batch ğ·ğ‘¡=\n{(ğ´1,ğ´2,...,ğ´ğ‘š)(ğ‘–)\nğ‘¡;ğ‘–=1,...,ğ‘›ğ‘¡}toğ‘…, whereğ‘›ğ‘¡is the number of\nrows. Letğ‘†ğ‘¡be a sufficient sample of ğ·ğ‘¡andğ‘†â‰¤\nğ‘¡âˆ’1be a sufficient\nsample fromâˆªğ‘¡âˆ’1\nğ‘—=0ğ·ğ‘—. We naturally assume that |ğ‘…|is finite. And,\ndue to the training restrictions of existing models, we also make\nthe natural assumption:\nâˆ€ğ´ğ‘–âˆˆğ‘…:ğ‘ ğ‘¢ğ‘ğ‘(ğ·ğ‘¡(ğ´ğ‘–))âŠ†ğ‘ ğ‘¢ğ‘ğ‘(ğ·ğ‘¡âˆ’1(ğ´ğ‘–))\nwhereğ‘ ğ‘¢ğ‘ğ‘(ğ·(ğ´ğ‘–))is the support of attribute ğ´ğ‘–in datasetğ·. This\nassumption satisfies the condition based on which the domain of\neach attribute is not violated in the upcoming update batches.\nStatistical test for data changes . We define out-of-distribution\ndetection as a two-sample hypothesis test between a sample of\nhistorical data and a sample of the new data. Let ğ‘†â‰¤\nğ‘¡âˆ’1have a joint\ndistribution of ğ‘ƒ(ğ´1,...,ğ´ 1ğ‘š)â‰¡Pandğ‘†ğ‘¡have a joint distribution\nofğ‘„(ğ´1,...,ğ´ğ‘š)â‰¡Q. We define the null hypothesis ğ»0:P=Q\nwhich asserts that ğ‘†ğ‘¡andğ‘†â‰¤\nğ‘¡âˆ’1are coming from a same distribution;\nand the alternative hypothesis ğ»ğ´:Pâ‰ Qwhich declares that the\ntwo samples are generated by two different distributions.\nIncrementally updating the model . Consider for ğ¼0a model\nğ‘€0is trained by minimizing a loss function â„’(ğ·0;Î˜0). This model\nmay be stale for ğ¼ğ‘¡;ğ‘¡>0. Ideally, the goal of incremental learn-\ning is: at time ğ‘¡train a model ğ‘€ğ‘¡that minimizes a function overÃğ‘¡\nğ‘–=1â„’(ğ·ğ‘–;Î˜ğ‘–). This new model should not forget {ğ¼ğ‘–;ğ‘–=0,1,...,ğ‘¡âˆ’\n1}and also learn ğ¼ğ‘¡.\n2.2 A High Level View of DDUp\nThe overall architecture of DDUp is depicted in Figure 1.\nDDUp process batches of tuples at a time. Such batched handling\nof insertions is typical in analytical DBs. Furthermore, this takes\ninto account that the effect of single tuples is usually negligible\nfor the overall large space modelled by NNs. And, for most tasks\nlike CE, AQP and DG, the effect of single tuples in the final result\nis very small, considering the large sizes of tables. And batching\namortizes detect-and-update costs over many insertion operations.\nUpon a new batch insertion, DDUp takes the latest model ğ‘€ğ‘¡âˆ’1,\nand performs a bootstrapping sampling from the previous data to\n2\n\nFigure 1: The overall structure of DDUp. DDUp uses the latest model and previous data to build a sampling distribution for\nthe two-sample test, and updates the learned component based on the shift in the data distribution.\nbuild the sampling distribution for the average loss values. DDUp\nuses this distribution to calculate a significance level corresponding\nto a confidence interval (e.g a 95th confidence interval). The general\nidea is that if the new data is similar to the previous data (IND\nin Figure 1), the loss values of ğ‘€ğ‘¡âˆ’1for this new data should lie\nwithin the threshold. This means that the new data has the same\ndistribution and therefore the model could be left intact (updating\nmaybe just the hyper-parameters of the system, including possible\nfrequency tables and other table statistics. Alternatively, a simple\nfine-tuning can be performed to adapt the model to the new data.\nIf the loss values exceeded the threshold, this implies that the data\ndistribution has significantly changed. DDUp will deploy a teacher-\nstudent transfer learning method based on knowledge distillation\nto learn this new distribution without forgetting the knowledge of\nthe old data. In this framework, while the student directly learns\nthe distribution of the new data, the teacher act as a regularizer to\nmake the student also learn about the old distribution.\n3 OUT-OF-DISTRIBUTION DETECTION\n3.1 Background\nIn ML, OOD is typically addressed from a classification perspective.\nFormally, assume ğ·is a dataset of(ğ‘¥,ğ‘¦)pairs which are drawn\nfrom a joint distribution, ğ‘(ğ‘¥,ğ‘¦), whereğ‘¥âˆˆX :={ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘›}\nis the input (independent variable) consisting of ğ‘›features, and\nğ‘¦âˆˆY :={1,2,...,ğ‘˜}is the label corresponding to one of the ğ‘˜\nin-distribution classes. A sample (ğ‘¥,ğ‘¦), that probably is generated\nby a different distribution than ğ‘(ğ‘¥,ğ‘¦), is called OOD, if ğ‘¦âˆ‰Y, i.e\nit does not belong to any previously seen classes.\nA similar problem has previously been addressed in statistics\nasconcept drift detection, where different types of shifts are distin-\nguished by expanding ğ‘(ğ‘¥,ğ‘¦)using the Bayes rule:\nğ‘(ğ‘¥,ğ‘¦)=ğ‘(ğ‘¥)ğ‘(ğ‘¦|ğ‘¥) (1)\nBased on Eq. 1, changes in ğ‘ƒ(ğ‘¦|ğ‘¥)are usually referred to as Real\ndrift, while changes in ğ‘ƒ(ğ‘¥)are called virtual drift [14]. Inğ‘‹â†’ğ‘¦\nproblems the latter mostly is known as covariate shift . Deciding\nwhich drift to detect is dependent on the underlying models. For\nexample, deep autoregressive networks (e.g., used by [ 78]) learn the\nfull joint distribution of a table. Hence, they are sensitive to covariate\nshift upon insertions. On the other hand, mixture density networks\n(e.g., used by [ 42]), model the conditional probability between a set\nof independent attributes and a target attribute. Hence, for these\nmodels, one would be interested in detecting real shift .3.2 Loss based OOD Detection\nThere are several challenges that make it difficult to simply adopt\none of the OOD detection algorithms in the ML or statistical learn-\ning literature. First, DB tables are multivariate in nature and learned\nmodels are usually trained on multiple attributes. As a result, uni-\nvariate two-sample tests like Kolmogorovâ€“Smirnov (KS) test are\nnot suitable for this purpose. Second, the test should introduce low\noverheads to the system as insertions may be frequent. Therefore,\nmultivariate tests like kernel methods that require to learn densities\nand perform expensive inference computations are not desirable.\nThird, we aim to support different learning tasks for which different\nmodels might be used. Thus, most of OOD detection methods in ML\nthat are based on probability scores (confidence) of classification\ntasks are not useful here. Moreover, the test should be able to adapt\nefficiently to the case where insertions occur within old data, that\nis, without having to recalculate baseline thresholds etc.\nAn efficient OOD detection method is now proposed that resolves\nall above issues by leveraging the underlying ML models themselves.\nCentral to most learned data system components is the ability to\nderive from the underlying data tables a model for the joint or\nconditional data distribution like ğ‘(ğ‘¥)orğ‘(ğ‘¦|ğ‘¥). A model usually\nachieves this by learning a set of parameters Î˜that represent a\nfunctionğ‘“by iteratively optimizing over a loss function as follows:\nğ‘“Î˜=arg min\nğ‘“âˆˆF1\nğ‘›ğ‘›âˆ‘ï¸\nğ‘–=1â„’(ğ‘“(ğ‘¥);Î˜)+Î©(ğ‘“) (2)\nwhere, Î©is a regularizer term, ğ‘›is the number of samples, and\nğ‘“could be the outputs of the model in the last layer (called logits ),\nor the probabilities assigned by a \"softmax\" function.\nWe will later discuss different loss functions in more details\nwhen instantiating different models. In general, loss functions are\nusually highly non-convex with many local mimina. However, a\ngood learning strategy will find the global minimum. Because of\nthe large data sizes, training is usually done by iterating over mini-\nbatches and a gradient descent algorithm updates the parameters\nbased on the average of loss of the samples in each mini-batch per\niteration. For the rest of the paper, when we mention â€™loss valueâ€™\nwe mean average of losses of the samples in a batch. Once the\nmodel is trained, i.e. the loss values have converged, the model can\nserve as a transformer to map (high-dimensional) input data to the\none-dimensional loss functions space around the global minimum.\n3\n\nAccordingly, the previous data (seen by the model) are closer to the\nglobal minimum compared to the out of distribution data.\nThe above discussion explains the possibility to compare in- and\nout-of distribution data just by relying on the underlying models\nwithout any further assumptions/components, in a low-dimensional\nspace. With these in hand, we can perform a statistical testing to\ncompare the loss values of old data and new data. In the following\nwe will explain a two-sample test for this purpose.\n3.3 A Two-Sample Test Procedure\nThe steps for a two-sample hypothesis test are: 1. Define the null,\nğ»0, and alternative hypothesis, ğ»ğ´. 2. Define a test statistic ğ‘‘that\ntests whether an observed value is extreme under ğ»0. 3. Deter-\nmine a significance level ğ›¿âˆˆ[0,1]that defines the ğ‘¡ğ‘¦ğ‘ğ‘’ -1ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ\n(false positives) of the test. 4. Calculate ğ‘-ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ which equals the\nprobability that a statistical measure, e.g. distance between two\ndistributions, will be greater than or equal to the probability of\nobserved results. 5. If ğ‘-ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ <=ğ›¿then theğ‘-ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ is statistically\nsignificant and shows strong evidence to reject ğ»0in favor ofğ»ğ´.\nOtherwise, the test failed to reject ğ»0.\nThe main challenge herein is how to calculate the test signifi-\ncance of the test statistic, i.e the ğ‘-ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ . As explained in Section 2,\nwe aim to detect if the new data that is inserted to the system at\ntimeğ‘¡has a different distribution than the previous data. Consider\nğ‘†â‰¤\nğ‘¡âˆ’1be a sample of the previous data and ğ‘†ğ‘¡be a sample of the\nnewly inserted data. Let ğ‘‘(ğ‘†â‰¤\nğ‘¡âˆ’1, ğ‘†ğ‘¡)be a distance function that\nmeasures the distance between the two samples. Then, the test\nsignificance would be ğ‘-ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ =ğ‘ƒ(ğ‘‘<ğ‘‘ğ‘¡|ğ»0)whereğ‘‘ğ‘¡is our test\nthreshold.\nChoosing the test statistic . The test statistic should reflect the\nsimilarity of new data to old data. According to our discussion in\nSection 3.2, we use the loss function values after the convergence\nof the models. We use a linear difference between the loss values\nof the two samples as our test statistics as follows:\nğ‘‘(ğ‘†â‰¤\nğ‘¡âˆ’1,ğ‘†ğ‘¡)=1\n|ğ‘†ğ‘¡âˆ’1|âˆ‘ï¸\nğ‘ âˆˆğ‘†ğ‘¡âˆ’1â„’(ğ‘ ;Î˜)âˆ’1\n|ğ‘†ğ‘¡|âˆ‘ï¸\nğ‘ âˆˆğ‘†ğ‘¡â„’(ğ‘ ;Î˜)(3)\nwhereâ„’is a loss function achieved by training model ğ‘€with\nparameters Î˜. From Eq. 3 follows that if the loss function is Nega-\ntive Log Likelihood, and the likelihoods are exact, the test statistic\nwill be the logarithm of the well-known likelihood-ratio test. Eq. 3\nalso gives intuition about the effect size: the larger ğ‘‘is, the larger\nthe difference between two data distributions would be. Although\nmany of the learned DB models are trained by maximizing like-\nlihood, some other models (e.g., regressions) are trained using a\nMean-Squared-Error objective. It has been shown [ 71] that MSE\noptimization maximizes likelihood at the same time. Therefore, the\nform of the distance function in Eq. 3 still holds. The important con-\nsequence of Eq. 3 is that, under i.i.d assumptions for both samples,\nit can be shown that the central limit theorem holds for distribu-\ntion ofğ‘‘under the null hypothesis, hence, it has a normal limiting\ndistribution with a mean at 0 and unknown standard deviation. To\nestimate the standard deviation (std), we utilize a bootstrapping\napproach.3.4 Offline and Online Steps\nThe main performance bottleneck of such an OOD detection is\nbootstrapping. Fortunately, this part could be performed offline\nbefore data insertion. In the offline phase, we draw ğ‘›bootstrap\nsamples of size|ğ‘†â‰¤\nğ‘¡âˆ’1|fromğ‘†â‰¤\nğ‘¡âˆ’1. (In practice, when we have access\nto the original data, we make ğ‘›bootstrap samples of size |ğ‘†â‰¤\nğ‘¡âˆ’1|from\nğ·â‰¤\nğ‘¡âˆ’1). We use the model ğ‘€ğ‘¡âˆ’1to compute the average likelihoods\n(or other losses) of each sample and create a sampling distribution of\nsaid average likelihoods. Then, we calculate the standard deviation\nof the sampling distribution, ğ‘ ğ‘¡ğ‘‘, which we will use to find the\nsignificance level.\nIn the online phase, when an insertion happens, we take a sample\nof the new data, ğ‘†ğ‘¡and use the latest model, ğ‘€ğ‘¡âˆ’1, to calculate the\naverage likelihood of ğ‘†ğ‘¡. Finally, we compare the test statistic ğ‘‘with\nthe threshold ( 2Ã—ğ‘ ğ‘¡ğ‘‘). Given the normality of the above distribution\nof average likelihoods, we know that if one were to draw another\nsampleğ‘†â‰¤\nğ‘¡âˆ’1from the previous data its average likelihood would\nfall within 2Ã—ğ‘ ğ‘¡ğ‘‘from the mean with probability 95%. Now, if\nğ‘‘>2Ã—ğ‘ ğ‘¡ğ‘‘we conclude that we are not confident enough to accept\nthat the new data has the same distribution of the old data â€“ that\nis, we reject the null hypothesis with a p-value of 0.05.\n3.5 The Test Errors\nThere are two errors associated with a hypothesis testing. type-1\nerror is rejecting the null hypothesis when it should not. Type-2\nerror is the error of accepting the null hypothesis when it should\nbe rejected. The first one introduces false positives to the system\nand the second causes false negatives. False positives (FPs) are\nonly a (rather small) performance concern only, spending time to\nupdate the model while accuracy is preserved. False negatives (FNs),\nhowever, can cause a loss of accuracy. Therefore, the system can\nafford to be stricter with respect to the significance level, in order\nto reduce the risk of false negatives and accuracy loss.\nDDUp uses the loss of the trained NNs for OOD detection. Some-\ntimes NNs could be over-confident [ 48,51,57] which may introduce\nbias. However, we have not witnessed it for our tasks here on tab-\nular data. If there were bias, the FP and FN rates discussed above\nwould signal it. We have evaluated DDUp with respect to FPs/FNs\nin Section 5.2 showing that this is not a concern.\n4 MODEL UPDATE\nIn this section, we propose a transfer-learning based method that\ncan retain previous knowledge of the model while adapt it to the\nnew insertions. The OOD detection module will either output â€™in-\ndistributionâ€™ or â€™out-of-distributionâ€™ signals.\nThe in-distribution case . When no drift occurs, the new data\ndistribution is similar to that of the historical data and this distribu-\ntion could be represented by a similar parameter space of the latest\nmodel,ğ‘€ğ‘¡. Hence, the learned component of the system could re-\nmain unchanged. More specifically, the framework can copy ğ‘€ğ‘¡to\nğ‘€ğ‘¡+1and update the required meta-data associated with the system\n(such as the frequency tables in DBEst++, or table cardinalities in\nNaru/NeuroCard). Even if there are slight permutations in data, fine-\ntuning the latest modelâ€™s parameters on the new data will adjust\nit to the general representation of both old and new data. We will\nshow that when knowing that data is not OOD, fine-tuning with a\n4\n\nrelatively small learning rate, can retain model performance. Specif-\nically, with an in-distribution signal at time ğ‘¡+1,ğ‘€ğ‘¡is retrained\nonğ‘†ğ‘¡+1with a small learning rate, ğ‘™ğ‘Ÿ. This learning rate could be\ntuned, as a hyper-parameter. We intuitively set ğ‘™ğ‘Ÿğ‘¡=|ğ·ğ‘¡+1|\n|ğ·â‰¤\nğ‘¡|Ã—ğ‘™ğ‘Ÿ0\nand experimentally show that it is a good choice.\nThe OOD case . With a distributional shift, by fine-tuning on\nnew data, the modelâ€™s parameters would bias toward the new data\ndistribution. Even smaller learning rates cause tiny deviations from\nthe previous parameter space which may yield large errors during\ninference. And, retraining using all the data from scratch is too\ntime consuming. Thus, we propose an updating approach grounded\non the transfer-learning paradigm. The general idea is to use the\nlearned model ğ‘€ğ‘¡and incorporate it in training ğ‘€ğ‘¡+1. To this end,\nwe utilize the knowledge distillation principles, which help to trans-\nfer the previously learned knowledge to a new model. Our rationale\nfor such a model updating approach is based on the following:\nâ€¢Distillation has several benefits including: faster optimization,\nbetter generalization, and may even outperform the directly\ntrained models. [79].\nâ€¢It is accurate for queries on old as well as new data.\nâ€¢It allows us to control the weights for queries on new and old\ndata with just a couple of parameters.\nâ€¢It is efficient memory-wise as well as computationally-wise, com-\npared to methods like Gradient Episodic Memory, or Elastic\nWeight Consolidation and PathInt (cf. Section 6)\nâ€¢It does not make any assumptions about the training of the under-\nlying models. This property, is especially desirable since: a) we\ncan use it to update different neural networks; b) it prevents the\nhigh costs of rebuilding base models; c) different pre-processings\ncould be left intact. For instance, Naru, DBEst++ and TVAE all\nuse completely different types of embedding/encoding. DDUp\ncan update the model regardless of these differences.\n4.1 General Knowledge Distillation (KD)\nKD was first introduced in [ 22] forğ‘šğ‘œğ‘‘ğ‘’ğ‘™ğ‘ğ‘œğ‘šğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘› by transfer-\nring knowledge from an accurate and \"cumbersome\" model, called\nteacher , to a smaller model called student . In its basic form, instead\nof fitting the student model directly to the actual data labels , one\nwould use the class probability distribution learned by the teacher\nto fit the student model. Hinton et al. [ 22] argued that small proba-\nbilities in \"wrong\" label logits, known as \"soft labels\", include extra\ninformation called \"dark knowledge\" that result in better learning\nthan actual \"hard labels\". Distillation has since been extensively\nstudied. Figure 2 shows a general view of the principles of a dis-\ntillation process. A small dataset referred to as transfer-set is fed\ninto a pre-trained model (teacher) and a new model (student) to\nbe trained. A ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘™ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘™ğ‘œğ‘ ğ‘  is calculated using the predictions of\nthe pre-trained model instead of the actual labels. This loss and a\ntypical loss using actual labels will be used to train the new model.\nTo formulate knowledge distillation , consider a model with pa-\nrameters Î˜, representing a function ğ‘“ğ‘¡(ğ‘¡for teacher) which has been\ntrained via Eq. 2. We would like to transfer knowledge from this\nteacher model to a student model with parameter Î˜â€², representing\na functionğ‘“ğ‘ . This new model could be trained as follows:ğ‘“ğ‘ Î˜â€²=arg min\nğ‘“âˆˆF1\n|ğ‘¡ğ‘Ÿ|âˆ‘ï¸\nğ‘–âˆˆğ‘¡ğ‘Ÿ\u0002\nğœ†â„’ğ‘‘(ğ‘“ğ‘ (ğ‘–);ğ‘“ğ‘¡(ğ‘–);Î˜;Î˜â€²)+(1âˆ’ğœ†)â„’(ğ‘“ğ‘ (ğ‘–);Î˜â€²)\u0003\n(4)\nfor weightğœ†, distillation loss â„’ğ‘‘, and transfer-set ğ‘¡ğ‘Ÿ.\n4.2 DDUp: Updating By Knowledge Distillation\n[12,66] showed that, for classification tasks, if instead of having\na compact student model, one uses the same architecture of the\nteacher, and repeat distillation sequentially for several generations,\nthe student models in the later generations could outperform the\nteacher model. This approach is called sequential self-distillation .\nInspired by this and anticipating that this will be valid for our\nlearning tasks, DDUp also employs a sequential self-distillation\napproach.\nTo update a model using KD, a copy of the previously trained\nmodel becomes the new student. Then, the student is updated using\na distillation loss (to be defined soon). After updating, the previous\nteacher is replaced with the new updated model. This cycle repeats\nwith every new insertion batch.\nTo formulate our training loss function, we consider two aspects\nthat we would like to have in our updating scheme. First, to have\ncontrol over the the new data/queries versus the old data/queries.\nSecond, to make it general so that different learned DB systems\ncould adopt it. As such, we first write down the general form of the\ntotal loss function and then, use cross-entropy and mean-squared-\nerror as the loss functions to instantiate different models. Training\nin each update step is as follows:\nğ‘“ğ‘ Î˜â€²=arg min\nğ‘“âˆˆF\u0012\nğ›¼Ã—1\n|ğ‘¡ğ‘Ÿ|âˆ‘ï¸\nğ‘¥âˆˆğ‘¡ğ‘Ÿ\u0002\nğœ†â„’ğ‘‘(ğ‘“ğ‘ (ğ‘¥),ğ‘“ğ‘¡(ğ‘¥);Î˜â€²)\n+(1âˆ’ğœ†)â„’(ğ‘“ğ‘ (ğ‘¥);Î˜â€²)\u0003\n+(1âˆ’ğ›¼)Ã—1\n|ğ‘¢ğ‘|âˆ‘ï¸\nğ‘¥âˆˆğ‘¢ğ‘â„’(ğ‘“ğ‘ (ğ‘¥);Î˜â€²)\u0013(5)\nHere,ğ›¼andğœ†are the new data and the distillation weights, respec-\ntively. Also, ğ‘¡ğ‘Ÿandğ‘¢ğ‘are the transfer-set and the update batch. In\nsummary, the rationale for proposing this novel loss function is: The\ntransfer-set term acts as a regularizer to avoid overfitting on new\nFigure 2: The knowledge distillation process.\n5\n\ndata. The same goal is also helped by self-distillation (when copying\nthe teacher to the student). Additionally, as mentioned sequential\nself-distillation [ 66] may attain increasingly higher accuracy, even\noutperforming \"retrain from scratch\" (cf. Section 5.3).\nFor models that provide a conditional probability in the last\nlayer of the network (e.g. using a Softmax function), an annealed\ncross-entropy loss will be employed. Otherwise, we utilize mean-\nsquared-error using the logits from the last layer of the network.\nEq. 6 and Eq. 7 show these two loss functions.\nâ„’ğ‘ğ‘’(ğ·ğ‘¡ğ‘Ÿ;ğ‘§ğ‘¡,ğ‘§ğ‘ )=âˆ’âˆ‘ï¸\nğ‘–âˆˆ[ğ‘˜]ğ‘’ğ‘¥ğ‘(ğ‘§ğ‘¡ğ‘–/ğ‘‡)Ã\nğ‘—âˆˆ[ğ‘˜]ğ‘’ğ‘¥ğ‘(ğ‘§ğ‘¡ğ‘—/ğ‘‡)logğ‘’ğ‘¥ğ‘(ğ‘§ğ‘ ğ‘–/ğ‘‡)Ã\nğ‘—âˆˆ[ğ‘˜]ğ‘’ğ‘¥ğ‘(ğ‘§ğ‘ ğ‘—/ğ‘‡)\n(6)\nâ„’ğ‘šğ‘ ğ‘’(ğ·ğ‘¡ğ‘Ÿ;ğ‘§ğ‘¡,ğ‘§ğ‘ )=âˆ‘ï¸\nğ‘–âˆˆ[|ğ‘§ğ‘¡|](ğ‘§ğ‘¡ğ‘–âˆ’ğ‘§ğ‘ ğ‘–)2(7)\nwhereğ·ğ‘¡ğ‘Ÿis the transfer-set ,ğ‘‡is a temperature scalar to smooth\nthe probabilities so that it produces \"softer\" targets, and [ğ‘˜]is\nthe vector[0,1,...,ğ‘›]which are the class probabilities and [|ğ‘§ğ‘¡|]\nindicates the logits of the network.\n4.3 Instantiating the Approach\nMixture Density Networks . MDNs consist of an NN to learn\nfeature vectors and a mixture model to learn the probability density\nfunction (pdf) of data. Ma et al. [ 42] uses MDNs with Gaussian\nnodes to perform AQP. For the Gaussian Mixture, the last layer of\nMDN consists of three sets of nodes {ğœ”ğ‘–,ğœ‡ğ‘–,ğœğ‘–}ğ‘š\nğ‘–=1that form the\npdf according to Eq. 8.\nË†ğ‘ƒ(ğ‘¦|ğ‘¥1,...,ğ‘¥ğ‘›)=ğ‘šâˆ‘ï¸\nğ‘–=1ğœ”ğ‘–.ğ’©(ğœ‡ğ‘–,ğœğ‘–) (8)\nwhereğ‘šis the number of Gaussian components, ğ‘¦is the depen-\ndent variable and(ğ‘¥1,...,ğ‘¥ğ‘›)is a set of independent variables, ğ‘¤ğ‘–\nis the weight of the ğ‘–ğ‘¡â„Gaussian with a mean of ğœ‡ğ‘–and a standard\ndeviation of ğœğ‘–. For MDNs, we define distillation loss as follows:\nâ„’ğ‘‘=â„’ğ‘ğ‘’(ğ·ğ‘¡ğ‘Ÿ,ğœ”ğ‘¡,ğœ”ğ‘ )+â„’ğ‘šğ‘ ğ‘’(ğ·ğ‘¡ğ‘Ÿ,ğœ‡ğ‘¡,ğœ‡ğ‘ )+â„’ğ‘šğ‘ ğ‘’(ğ·ğ‘¡ğ‘Ÿ,ğœğ‘¡,ğœğ‘ )(9)\nThis summation of terms help us retain both the shape of data\ndistribution as well as the intensity levels.\nDeep Autoregressive Networks . The Naru and NeuroCard\ncardinality estimators [ 77,78] use deep autoregressive networks\n(DARNs) to approximate a fully factorized data density. DARNs are\ngenerative models capable of learning full conditional probabilities\nof a sequence using a masked autoencoder via Maximum Likelihood.\nOnce the conditionals are available, the joint data distribution could\nbe represented by the product rule as follows:\nË†ğ‘ƒ(ğ´1,ğ´2,...,ğ´ğ‘›)=Ë†ğ‘ƒ(ğ´1)Ë†ğ‘ƒ(ğ´2|ğ´1)...Ë†ğ‘ƒ(ğ´ğ‘›|ğ´1,...,ğ´ğ‘›âˆ’1)\nwhereğ´ğ‘–is an attribute in a relation ğ‘…. Naru and NeuroCard use\ncross-entropy between input and conditionals as the loss function.\nThis allows us to formulate the distillation loss function using the\nconditionals of the teacher and the student networks. Also, in Naruand NeuroCard, each conditional is calculated using a set of logits,\nhence we average over all as follows:\nâ„’ğ‘‘=1\n|ğ´||ğ´|âˆ‘ï¸\nğ‘–=1â„’ğ‘ğ‘’(ğ·ğ‘¡ğ‘Ÿ,ğ‘§ğ‘ ğ‘–,ğ‘§ğ‘¡ğ‘–) (10)\nWhere|ğ´|is the number of attributes corresponding to the num-\nber of conditionals.\nVariational Autoencoders . VAEs have been used for a number\nof DB components: [ 65] for AQP, [ 17] for CE, and [ 76] for synthetic\ntabular data generation. They are a type of autoencoders that in-\nstead of learning deterministic encoder, decoder, and compressed\nvector (known as bottleneck), they learn a probabilistic encoder,\ndecoder, and a latent random variable instead of the compressed\nvectors. (For more details, see the seminal paper [ 26]). Interest-\ningly, a VAE is trained using a different loss function, known as\nEvidence-Lower-Bound (ELBO) loss (which amounts to a lower\nbound estimation of the likelihoods). Here we shall use TVAE for\nlearned synthetic tabular data generation (of particular importance\nin privacy-sensitive environments, or when data is scarce for data\naugmentation purposes, or when wishing to train models over ta-\nbles and accessing raw data is expensive in terms of time or money).\nTo distill a VAE, one must cope with the random noise added to\nthe input of the decoder by the latent variable. For that, the latent\nvariable in the teacher network is removed, and we use the same\nnoise generated by the student in the teacher. The reason for doing\nthis is that distillation tries to teach the student to behave like the\nteacher for a specific observation or action. If there is randomness,\nthe student might mimic the teacherâ€™s behaviour for a completely\ndifferent observation. After this change, the corresponding logits of\nthe encoder/encoder of the student and the teacher are compared\nusing MSE. Finally, the loss function is:\nâ„’ğ‘‘=1\n2(â„’ğ‘šğ‘ ğ‘’(ğ·ğ‘¡ğ‘Ÿ,ğ‘§(ğ‘’)\nğ‘¡,ğ‘§(ğ‘’)\nğ‘ )+â„’ğ‘šğ‘ ğ‘’(ğ·ğ‘¡ğ‘Ÿ,ğ‘§(ğ‘‘)\nğ‘¡,ğ‘§(ğ‘‘)\nğ‘ )) (11)\nwhere,ğ‘’andğ‘‘correspond to the encoder and the decoder networks.\n4.4 An Example\nWe create a simple synthetic dataset consisting of a categorical\nattribute,ğ‘¥, with 10ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘›ğ‘ğ‘¡ -ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ğ‘  ={1,2,3,..., 9,10}, and with\neach category having 1000 real values. The dataset is balanced and\nthe real values for each category are generated by a Mixture of Gaus-\nsians (MoG) with five peaks. Figure 3.a is the dataset corresponding\ntoğ‘¥=1. We fit a Mixture Density Network with ten components\non this dataset. Figure 3.b shows a sample generated by this MDN\nwhich asserts that the model has perfectly learnt the data distri-\nbution. Next, we introduce an update batch generated by a MoG\nwith two different means. Figure 3.c shows the update batches in\nred color compared to the previous data in blue. We update the\npreviously learned MDN with the proposed loss function in Eq. 9.\nWe repeat updates for 50 batches generated with the new MoG.\nFigure 3.d shows the final distribution learnt by the MDN.\n4.5 Handling Join Operations\nDDUp can operate either on raw tables or tables from join results.\nIf the old data ğ‘…is the result of a join, the new data batch needs to\nbe computed, due to new tuples being inserted in any of the joined\n6\n\nFigure 3: An example to show how DDUp learns new data\nwithout forgetting. â€™aâ€™ is the histogram of synthetic data\ncorresponding to ğ‘¥=1. â€™bâ€™ is the sample generated by the\nlearned MDN for ğ‘¥=1. â€™câ€™ shows a sample of an update batch\ncoming from different Gaussians. â€™dâ€™ is the sample generated\nby the MDN after being updated by the DDUp loss function.\nWe have performed the update 50 times to see the effect of\nhigh frequency updates (This explains the higher frequen-\ncies around the last two peaks for â€™dâ€™).\ntables inğ‘…. Consider at time ğ‘¡âˆ’1a modelğ‘€ğ‘¡âˆ’1has been trained on\nğ‘…=Ãğ‘¡âˆ’1\nğ‘—=0ğ‘‡ğ‘—\n1âŠ²âŠ³Ãğ‘¡âˆ’1\nğ‘—=0ğ‘‡ğ‘—\n2...âŠ²âŠ³Ãğ‘¡âˆ’1\nğ‘—=0ğ‘‡ğ‘—\nğ‘›, whereğ‘‡ğ‘—\nğ‘Ÿdenotes the new\nbatch for table ğ‘‡ğ‘Ÿat timeğ‘—. Without loss of generality, suppose a new\ninsertion operation ğ¼ğ‘¡at timeğ‘¡adds new data to table ğ‘‡ğ‘–, denotedğ‘‡ğ‘¡\nğ‘–.\nThe new data for DDUp in this setting is ğ·ğ‘¡=(ğ‘…\\Ãğ‘¡âˆ’1\nğ‘—=0ğ‘‡ğ‘—\nğ‘–)âŠ²âŠ³ğ‘‡ğ‘¡\nğ‘–,\nwhere\\denotes a (multi)set-difference operator. Therefore, for the\ndetection module, ğ‘†â‰¤\nğ‘¡âˆ’1is a sample of R, and ğ‘†ğ‘¡a sample from ğ·ğ‘¡.\nFurthermore, during updating the transfer-set is a sample from ğ‘…\nand the new data is ğ·ğ‘¡. Please note that all this data preparation and\nhow each model deals with joins is orthogonal to DDUp. Therefore,\nit can be done by either computing the actual joins above or using\njoin samplers like [ 62,82], as is done in NeuroCard and compared\nagainst in Section 5.4.\n5 EXPERIMENTAL EVALUATION\nWe evaluate DDUp for three different models for learned DB com-\nponents: (i) Naru/NeuroCard [ 77,78] which use DARN models for\nCE; (ii) DBest++ [ 42] that uses MDNs for AQP; and (iii) TVAE [ 76],\nthat uses variational autoencoders for DG. We evaluate in terms\nof model accuracy and update time. We use as reference points\nthe baseline update approach provided for AQP and CE (TVAE\nprovides no update approach). We also add as reference points the\naccuracy when retraining from scratch and when leaving models\nstale. With respect to OOD detection , we investigate whether it can\ndetect significant data shifts successfully and how this will con-\ntribute to the final performance of the underlying models in their\nspecific application, CE, AQP, DG. Ultimately, the experiments are\nto address the following questions:\nâ€¢How to best evaluate DDUp? (Section 5.1)\nâ€¢Can DDUp accurately detect a distributional shift? (Section 5.2)\nâ€¢Is DDUp accurate under in- ğ‘ğ‘›ğ‘‘out-of- distribution settings?\n(Section 5.3)â€¢How does DDUp compare to the baseline approaches in accuracy\nand update time? (Section 5.3)\nâ€¢What is the effect of distillation? (Section 5.5)\nâ€¢Is DDUp efficient? (Section 5.6)\n5.1 Experimental Setup\nTo establish a dynamic setup, we make a copy of the base table\nand randomly sample 20% of its rows as new data. In this setting,\nnew data follows the previous data distribution which we denote\nasin-distribution . We introduce distributional drift as is typically\ndone for tabular data settings, say in [ 70]. As such, after making the\ncopy, we sort every column of the copied table individually in-place\nto permute the joint distribution of attributes. Next, we shuffle the\nrows and randomly select 20%of the rows - this now becomes the\nnew data. With these new data, we perform two types of experi-\nments. First, we consider the whole 20% sample as a new data batch\nand update the model with it. Second, to show the updatability in\nincremental steps, we split the 20% data into 5 batches. In general,\nthe size of the transfer-set is a tunable parameter [ 22], influenced by\nthe dataset complexity, the underlying model generalization ability,\nand the downstream tasks. After tuning, we used a 10% transfer-set\nfor MDN and DARN and a 5% for TVAE, which could be further\ntuned with methods like Grid search.\nDDUp does not impose any further constraints to those of the\nunderlying models. For DBest++ we use a query template with a\nrange and an equality attribute. Also, we use one-hot encoding to\nencode categorical attributes and normalize the range attribute to\n[âˆ’1,1]. For Naru/NeuroCard and TVAE, we use the same settings as\nexplained in their code documentation. We use the learned hyper-\nparameters of the base model, i.e the model we build at time zero, for\nall subsequent updates. Furthermore, we intuitively set ğ›¼parameter\nin Eq. 5 to the fraction of update batch size to the original data size\nand tuneğœ†for values in[9/10,5/6,1/4,1/2].\n5.1.1 Datasets. We have mainly used three real-world datasets\n(census, forest, DMV) (see Table 1). These datasets have been widely\nused in the learned DB literature. For CE, [ 70] uses also forest,\ncensus and DMV, while NeuroCard/Naru use JOB/DMV. For AQP\nDBEst++ uses TPCDS. For DG, [ 76] uses census and forest. Thus,\nwe have also used census, forest, DMV, and TPCDS ( store sales\ntable, scaling factor of 1). Finally, for join queries, we have used\nJOB (on IMDB data) and TPCH benchmarks, which are also used\nin [77, 78].\nTable 1: Characteristics of datasets.\nDataset Rows Columns Joint Domain\nCensus 49K 13 1016\nForest 581K 10 1027\nDMV 11.6M 11 1015\nTPCDS 1M 7 1030\n5.1.2 Workload. Each model is evaluated using 2,000 randomly\ngenerated queries. These queries are generated at time zero for each\nmodel and are used throughout the subsequent updates. When an\nupdate batch is performed, the ground truth of the queries will be\nupdated. For Naru/NeuroCard, we use their generator to synthe-\nsize queries: It randomly selects the number of filters per query\n7\n\n(forest:[3,8], census: [5,12], TPCDS: [2,6], dmv: [5,12]). Then, it uni-\nformly selects a row of the table and randomly assigns operators\n[=,>=,<=]to the columns corresponding to the selected filters.\nColumns with a domain less than 10 are considered categorical and\nonly equality filters are used for them. For DBest++, we select a\nğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ -ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ and aâ„ğ‘–ğ‘”â„ğ‘’ğ‘Ÿ -ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ for the range filter and uniformly\nselect a category from the categorical column for the equality filter.\nThroughout the experiments, we discard queries with actual zero\nanswer. The structure of a typical query in our experiments is:\nSELECT AGG( y ) FROM ğ‘‡1âŠ² âŠ³ğ‘‡2...âŠ² âŠ³ğ‘‡ğ‘›WHEREğ¹1AND . . . AND ğ¹ğ‘‘\nwhere,ğ¹ğ‘–is a filter in one of these forms: [ğ‘ğ‘¡ğ‘¡ğ‘–=ğ‘£ğ‘ğ‘™,ğ‘ğ‘¡ğ‘¡ğ‘–>=\nğ‘£ğ‘ğ‘™,ğ‘ğ‘¡ğ‘¡ğ‘–<=ğ‘£ğ‘ğ‘™]. Also, AGGis an aggregation function like COUNT ,\nSUM,AVG. For DBest++, the query template contains one categorical\nattribute and one range attribute. As such, we select the following\ncolumns from each dataset: census:[ age, country ]; forest:[ slope,\nelevation ]; dmv:[ body type, max gross weight ]; TPCDS:[ ss\nquantity,ss sales price ]; IMDB:[ info type id,production\nyear ]; TPCH:[ order date,total price ] where the first/second\nattribute is categorical/numeric. Furthermore, Naru could not train\non the full TPCDS dataset as the encodings were too large to fit to\nmemory. Hence, we selected the following columns [ ss sold date\nsk,ss item sk ,ss customer sk ,ss store sk ,ss quantity ,ss\nnet profit ], and made a 500k sample.\n5.1.3 Metrics. Forcount queries, we use q-error as follows:\nğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ =ğ‘šğ‘ğ‘¥(ğ‘ğ‘Ÿğ‘’ğ‘‘(ğ‘),ğ‘Ÿğ‘’ğ‘ğ‘™(ğ‘))\nğ‘šğ‘–ğ‘›(ğ‘ğ‘Ÿğ‘’ğ‘‘(ğ‘),ğ‘Ÿğ‘’ğ‘ğ‘™(ğ‘))(12)\nForsum andavgaggregates, we use relative-error as follows:\nğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ =|ğ‘ğ‘Ÿğ‘’ğ‘‘(ğ‘)âˆ’ğ‘Ÿğ‘’ğ‘ğ‘™(ğ‘)|\nğ‘Ÿğ‘’ğ‘ğ‘™(ğ‘)Ã—100 (13)\nAdditionally, Lopez et al. [ 38] introduce the notions of Backward\nTransfer (BWT) and Forward Transfer (FWT) as new metrics in\nclass incremental learning tasks. BWT is the average accuracy of\nthe model on old tasks, and FWT is the average accuracy of the\nmodel on new tasks. Here, we re-frame BWT and FWT. We generate\nthe queries at time 0and use them for all update steps. At each\nstepğ‘¡, we calculate ğ‘‘ğ‘–ğ‘“ğ‘“=ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘¡(ğ‘)âˆ’ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘¡âˆ’1(ğ‘)for each query, ğ‘,\nwhich gives us three set of queries; ğºğ‘“ğ‘–ğ‘¥withğ‘‘ğ‘–ğ‘“ğ‘“=0,ğºğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ğ‘‘\nwithğ‘‘ğ‘–ğ‘“ğ‘“ >0, andğºğ‘ğ‘™ğ‘™=ğºğ‘“ğ‘–ğ‘¥âˆªğºğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ğ‘‘ . With these groups, we\ndefine three measures. ğ´ğ‘‡: average q-error over ğºğ‘ğ‘™ğ‘™.ğ¹ğ‘Šğ‘‡ : average\nq-error over ğºğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ğ‘‘ .ğµğ‘Šğ‘‡ : average q-error over ğºğ‘“ğ‘–ğ‘¥.\n5.1.4 Evaluating Variational Autoencoders. DG is an interesting\nlearned application which is recently supported using TVAE. Thus,\nwe evaluate DDUp for TVAE. In TVAE, once the training is done,\nonly the decoder network is kept and used, as this is the generator.\nHence, we apply our distillation-update method to the decoder net-\nwork. We evaluate TVAE via the accuracy of an XGboost classifier\ntrained by the synthetic samples, as in [ 76]. We hold-out 30% of\ntable as the test set, and train two classifiers with original and syn-\nthetic data, then predict the classes of the held-out data. We report\nmicro f1-score for classifiers. For census, forest and DMV, we use:\nincome ,cover-type , and fuel-type , as the target class, respectively.\nFor TVAE, we created a smaller DMV with 1m records, as training\nTVAE on the whole DMV is very time/resource consuming (proving\nindirectly the need to avoid retraining).5.2 OOD Detection\n5.2.1 Loss Functions as Signals. We first show the results of loss/log-\nlikelihoods when the detector receives samples from the same dis-\ntributions or from different distributions. The results are shown in\nTable 2. For Naru/NeuroCard and DBEst++ we report the actual\nlog-likelihood values (not negatives, so higher is better). For TVAE,\nwe report the ELBO loss values (hence lower is better).\nTable 2: Average log-likelihood and ELBO loss values of data\nsamples on a trained model. ğ‘†ğ‘œğ‘™ğ‘‘is a sample of the previous\ntraining data. \"IND\", is a 20% sample from a straight copy of\nthe original table; \"OOD\", is a 20% sample from a permuted\ncopy of the original table.\nDatasetDBEst++ Naru/NeuroCard TVAE\nğ‘†ğ‘œğ‘™ğ‘‘ IND OOD ğ‘†ğ‘œğ‘™ğ‘‘ IND OOD ğ‘†ğ‘œğ‘™ğ‘‘ IND OOD\nCensus -0.362 -0.361 -0.366 -20.99 -20.87 -36.95 -15.21 -15.22 81.47\nForest -0.0194 -0.0202 -0.052 -43.16 -43.9 -141.10 -19.96 -20.09 142.38\nDMV 2.520 2.532 2.444 -13.74 -13.16 -18.67 9.114 9.28 34.95\nTable 2 shows that the loss function (log likelihood and ELBO\nin our cases) can reliably signal OOD data. Interestingly, this cor-\nroborates similar findings in [ 18] for classification tasks in various\nvision and NLP tasks, where the NN outputs can be used to signal\nOOD. Here we show it for tabular data and for NNs developed for\nAQP, CE, and DG tasks.\nIn Naru/NeuroCard and TVAE, when permuting, all columns\nare sorted individually, hence the large difference in likelihoods.\nFor DBEst++, only the selected columns for a query template have\nbeen permuted, yielding a small difference in likelihoods.\n5.2.2 The two-sample test results. Table 3 shows results for two-\nsample testing for OOD detection. The significance level of the test\n(threshold) is 2Ã—ğ‘ ğ‘¡ğ‘‘of the bootstrapping distribution, which was\nobtained by >1000 iterations. In each iteration, we use a 1% sample\nwith replacement from previous data and a 10% sample without\nreplacement from new data to calculate the test statistic. The results\nshow that when data is permuted, the test statistic is far away from\nthe threshold. This means it appears at a great dissonance in the\ntails of the bootstrapping distribution. And since the critical value\nto test for OOD is found by bootstrapping over ğ‘†ğ‘œğ‘™ğ‘‘, i.e.,ğ‘†â‰¤\nğ‘¡, it\nwill adjust even to small differences when faced with OOD. Case\nin point, the DBEst++ OOD likelihood value for census (which is\nsimilar to IND/ ğ‘†ğ‘œğ‘™ğ‘‘in Table 2) vs the corresponding test-statistic\nvalue in Table 3.\n5.2.3 FP and FN rates in OOD detection. To evaluate OOD detec-\ntion, we measure FP and FN rates (FPR, FNR). We created an OOD\ntest-set and an IND test-set, each equaling half the original size\nof the table. The latter is just a random sample from the original\ntable. The former is constructed as follows. The perturbed data\nis obtained by perturbing one or more of five columns of the ta-\nble, sayğ¶1, ...ğ¶ 5. First we perturb ğ¶1and take a sample of the\nresulting table of size 10%and append it to the OOD test-set. Then\nwe perturb ğ¶1andğ¶2and similarly sample and append it to the\nOOD test-set. We repeat this for perturbations on ğ¶1,ğ¶2,ğ¶3, on\nğ¶1,ğ¶2,ğ¶3,ğ¶4, and onğ¶1,ğ¶2,ğ¶3,ğ¶4,ğ¶5, ending up with an OOD\ntest-set of size 50% of the original table. Note that this setup creates\na more-challenging case, as the degree of perturbations (for OOD\n8\n\nTable 3: The test-statistic values. Threshold is 2Ã—ğ‘ ğ‘¡ğ‘ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘‘âˆ’ğ‘‘ğ‘’ğ‘£ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› and bs-mean is the mean of bootstrapping distribution.\nDatasetDBEst++ Naru/NeuroCard TVAE\nbs-mean threshold IND OOD bs-mean threshold IND OOD bs-mean threshold IND OOD\nCensus -0.3524 0.007 0.001 0.05 -21.0076 0.0529 0.032 16.0052 -15.1834 0.6041 0.0419 100.5126\nForest -0.0228 0.0122 0.007 0.2315 -41.35 0.0141 0.0084 72.5473 -19.99 0.0868 0.0417 167.0502\nDMV 2.52 0.1287 0.0145 4.5745 -13.7674 0.0012 0.0007 5.1145 9.1209 0.0177 0.0015 25.1398\n(a)\n (b)\nFigure 4: Sensitivity of OOD detection vs batch size.\ndata) is finer-grained. Then, at each batch, we fed a random sample\nfrom the OOD test-set and of the IND test-set to the DDUp detector.\nFor each batch, the detector would signal IND or OOD and we\nrecorded and calculated FPR and FNR. The batch size was 2,000 and\nwe repeated the experiment for 1,000 batches.\nWe used the same parameters for all datasets and models: the\nbootstrapping size is 32 and the threshold is 2Ã—ğ‘ ğ‘¡ğ‘‘. For DBEst++, the\nresults are reported in Table 4. FPR and FNR for Naru/NeuroCard\nand TVAE were always zero. These results further confirm that the\nOOD detection algorithm is not biased.\nTable 4: FPR and FNR for DBEst++.\nDataset FPR FNR\nCensus 0.15 0.01\nForest 0.10 0\nDMV 0.01 0\nFurthermore, we studied the sensitivity on the batch size and\nvaried it from a size of 1 to 2,000. Results are shown in Figure 4,\nwhich clearly show that after a low-threshold batch size, FPR and\nFPN tend to zero. The same results hold for other models and\ndatasets, and are omitted here for space reasons.\n5.3 Accuracy Results\n5.3.1 When there is OOD data. For Naru/NeuroCard, DBEst++, and\nTVAE, and for each dataset, we compare 4 updating approaches\nagainst each other and against the base model before any new data\nis inserted. The 4 approaches are as follows: \" Retrain \", retrains\nthe model from scratch using both old and new data. \" Baseline \" is\nthe baseline approach in Naru/NeuroCard and DBest++ where a\ntrained model is updated with new data by performing SGD with\na smaller learning rate. \" DDUp \" is the proposed method. Finally, in\n\"stale \", the model is not updated â€“ this is a do-nothing approach.\nFor reference, we also include the numbers for ğ‘€0, i.e., the original\nmodel accuracy before any new data came. Table 5 and Table 6 show\nthe accuracy results for CE and AQP (SUM and AVG operations),respectively. For TVAE, the classification f1-scores are reported\nin Table 7. Results of these three tables correspond to the case\nwhere the update sample is permuted. DDUp always performs better\nthan the baseline approach. Most of the times, the performance of\nDBEst++ on DMV dataset is not as well as for the other datasets.\nThis probably is due to the complexity of data (large scale and\nhighly correlated attributes). Nevertheless, DDUp stands on the top\nof the underlying models and regardless of the modelâ€™s performance,\nDDUp ensures that it will retain the accuracy. Please note the DMV\ndataset results in Table 5 and Table 6 and, census and forest datasets\nin Table 7, where, DDUp even outperforms retraining from scratch.\nInterestingly, this corroborates similar evidence for sequential self-\ndistillation (for boosting embeddings for) classification tasks [ 66].\nThis was one of the reasons we adapted a self-distillation based\napproach. Finally, baseline methods have poor performance for\n95th and 99th percentiles.\nPerformance on old and new queries. To better illustrate the effects\nofcatastrophic forgetting and intransigence we elaborate on per-\nformance on FWT and BWT. (As retrain avoids be definition\ncatastrophic forgetting andintransigence , it is omitted). The results\nare shown in Table 8. Note that any insertion affects only a per-\ncentage of queries, shown in Table 9. Comparing AT, FWT, and\nBWT in Table 5 and Table 8 first note that fine-tuning always per-\nforms much better in terms of FWT compared to BWT (due to\ncatastrophic forgetting). Second, conversely, a stale model shows\nbetter BWT compared to FWT. For DDUp, FWT and BWT remain\nclose to each other, especially in terms of median q-error, showing\nthat DDUP can ensure accuracy for queries on old and new data.\nOverall, DDUp enjoys high accuracy.\nIncremental Steps. To show the updates in incremental steps, we\nhave split the 20% data into 5 equal-sized chunks and have per-\nformed an update incrementally for each batch. Figure 5 compares\nthe trend of accuracy during updates. As it is clear from the figures,\nDDUp remains very close to retrain , while there is a drastic drop\nin accuracy using baseline . Starting point 0is where the base\nmodelğ‘€0is built from scratch. (The same results hold for 95th,\n99th percentiles and maximum q-error).\nWe also have evaluated the models with respect to the log-\nlikelihood goodness-of-fit . Log-likelihood is widely used to eval-\nuate NN models. Using log-likelihood allows evaluation to be in-\ndependent of underlying applications. Figure 6 shows changes in\nlog-likelihood in consecutive update steps. At each step, we calcu-\nlate the average of log-likelihoods over a sample of new data and\na sample from historical data. In these figures we again see that\nupdating with DDUp is fitting to the old and the new data very\nsimilarly to the retrain case. In general, when keep using stale ,\n9\n\nTable 5: Results of updating a base model with a 20% permuted sample in terms of q-error. ğ‘€0denotes the base model.\nDataset metricDBEst++ Naru/NeuroCard\nğ‘€0 DDUp baseline stale retrainğ‘€0DDUp baseline stale retrain\ncensusmedian 1.05 1.11 1.17 1.16 1.07 1.08 1.09 4 1.14 1.07\n95th 2 2 2.20 2 2 2 2 471.80 2 2\n99th 3 3 4 3 3 3 3 1534.69 3.16 3\nmax 5 7 11 10.50 5 5.25 7 8385 21.88 6\nforestmedian 1.026 1.046 2 1.18 1.02 1.04 1.07 1.54 1.10 1.05\n95th 2 2 63.40 2 1.64 2.48 3 41 2.50 2.75\n99th 2 2.583 503.12 5.60 2 4 6 157.16 5.48 5\nmax 4 5.33 3470 90.85 5.33 27 65.66 1691 484 34.66\nDMVmedian 1.20 1.143 3.48 1.88 1.34 1.02 1.04 2.57 1.16 1.02\n95th 4.91 5.07 234.88 7.00 5.50 1.20 1.41 468.68 1.50 1.25\n99th 9.65 10 3897.87 12.50 8 1.83 2.31 4734.62 2.84 2\nmax 18.83 19 65875 39 17 8 9.81 343761 9.49 5\nTPCDSmedian 1.02 1.04 57 1.27 1.02 1.01 1.07 1.15 1.10 1.05\n95th 1.16 1.26 269 1.58 1.18 2 2 29 2 2\n99th 1.5 1.61 1266 2.72 1.5 3.01 3.01 239 4 3\nmax 3 3 4534 10.66 5.64 5 28 5100 28 24\nTable 6: mean-relative-error for SUM and AVG aggregation\nfunctions for DBEst++.\nDataset function ğ‘€0 DDUp baseline stale retrain\ncensusSUM 13.05 17.30 65.88 21.36 13.60\nAVG 1.89 2.36 8.15 2.37 1.97\nforestSUM 10.11 15.51 88.73 24.59 10.14\nAVG 0.76 1.04 3.90 1.35 0.79\nTPCDSSUM 4.53 6.37 61.40 22.64 5.12\nAVG 0.88 1.47 12 3.50 1.21\nDMVSUM 76.73 85.29 423 97.00 110\nAVG 6.4 6.9 15.9 8.6 7.3\nTable 7: Classification results for TVAE in terms of micro f1.\nâ€™râ€™ stands for real data, â€™sâ€™ stands for synthetic data.\nDatasetğ‘€0 DDUp baseline stale retrain\nr s r s r s r s r s\ncensus 0.67 0.63 0.77 0.73 0.77 0.55 0.77 0.56 0.77 0.72\nforest 0.84 0.69 0.89 0.78 0.89 0.63 0.89 0.60 0.89 0.74\nDMV 0.97 0.97 0.98 0.97 0.98 0.92 0.98 0.93 0.98 0.98\nthe log-likelihood drops after the first update and then remains low.\nThe reason is that all update batches have similar permutation and\nsince we calculate unweighted averages, the log-likelihood stays\nfixed. While, for baseline , i.e fine-tuning, we can see a gradual\ndecrease of likelihood which means that the network is increasingly\nforgetting about previous data in each step.\n5.3.2 When data is not OOD. In this case, simple fine-tuning up-\ndate algorithms, such as baseline , will likely avoid catastrophic\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\nFigure 5: Updating results over 5 consecutive updates.\nforgetting . To illustrate this, we have repeated the 5 batched in-\ncremental updates with data without permutation. The results are\nreported in Figure 7. For space reasons, we only show the results\n10\n\nTable 8: Comparing q-error of different updating approaches in terms of FWT and BWT.\nDataset metricDBEst++ Naru/NeuroCard\nğ‘€0 DDUp baseline stale ğ‘€0 DDUp baseline stale\nFWT BWT FWT BWT FWT BWT FWT BWT FWT BWT FWT BWT\ncensusmedian 1.05 1.06 1.12 1.06 1.20 1.05 1.16 1.08 1.11 1.09 1.83 6 1.20 1.13\n95th 2 1.66 2 1.56 2.33 3.30 2 2 1.64 2 4.63 530.80 3.18 2\n99th 3 4.94 3 4.10 4 8.90 2.75 3 3.08 3 9.98 1598.53 8.49 3\nforestmedian 1.02 1.01 1.08 1.23 2.66 1.05 1.20 1.04 1.07 1.07 1.39 1.65 1.18 1.08\n95th 2 1.181 2 2.87 146.38 2.85 2 2.489 1.88 3 3.13 43.02 7.55 2.33\n99th 2 1.52 3 3.72 590.57 18.33 2.24 4 4.89 6 5.27 163.80 191.53 4.86\nDMVmedian 1.20 1.28 1.13 2.20 4.36 1.66 1.54 1.02 1.02 1.07 1.06 12.85 1.26 1.19\n95th 4.910 4.30 5.87 3.34 484.46 9.50 6.87 1.20 1.16 1.55 1.65 1015.81 3.30 1.40\n99th 9.65 9 11.65 10.50 5894.21 12.12 10.80 1.83 1.47 3 3.35 8183.34 11.93 2.49\nTPCDSmedian 1.02 1.03 1.04 1.20 1.51 1.16 1.21 1.01 1.06 1.08 1.19 1.11 1.10 1.10\n95th 1.16 1.21 1.29 2.37 339 2.26 1.35 2 2 2 2.60 54 2 2\n99th 1.5 1.37 1.66 4.27 1536 4.48 1.66 3.01 9.77 3 9.47 434 9.64 3.77\nTable 9: The percentage of the queries (out of 2k queries)\nwith changed actual results after inserting 20% new data.\ndataset DBEst++ Naru\ncensus 14% 12%\nforest 32% 9%\nTPCDS 36% 36%\ndmv 52% 45%\n(a)\n (b)\nFigure 6: log-likelihood results over 5 consecutive updates.\n(a)\n (b)\nFigure 7: Updating results over 5 consecutive updates when\ndata follows the same distribution as the historical data.\nfor census. The results indicate that for in-distribution data, simple\nbaselines can have a performance close to retrain .5.4 Evaluating DDUp for Join Queries\nAs mentioned, DDUp is unconcerned whether at a time ğ‘¡,ğ‘†â‰¤\nğ‘¡âˆ’1(a\nsample ofâˆªğ‘¡âˆ’1\nğ‘—=0ğ·ğ‘—) andğ·ğ‘¡come from a raw table or from a join. For\nthis experiment, we have evaluated DDUp running 2,000 queries\nover two 3-table joins from the JOB and TPCH datasets. For each,\nthe 2,000 queries involve a join of the fact table with two dimension\ntables: Specifically, the join of tables [ title ,movie info idx ,\nmovie companies ] for IMDB, and [ orders ,customer ,nation ] for\nTPCH. For the update dynamics, we have split the fact table into 5\ntime-ordered equally-sized partitions. We have built ğ‘€0on the join\n(of the fact tableâ€™s first partition with the 2 dimension tables) and\nupdated it with each subsequent partition at a time. This is similar\nto the update setting in NeuroCard. Results for both CE and AQP\nare in Figure 8.\nNeuroCard, unlike other models, natively supports joins, using\na \"fast-retrain\" - i.e., a light retraining where the model is retrained\nusing a 1 percent sample of the full join result. We have included\nthis policy here as \"fast-retrain\". DDUp always signalled OOD for\nthe new update batches, except for TPCH data on DBest++, where\nupdate was not triggered. Therefore, in Figure 8.d the accuracy\nof the stale model and fine-tuning is close to retrain. This further\nconfirms the significance of OOD detection.\nTable 10: DDUpâ€™s speed up over retrain , for two update sizes.\nFor census, forest, and dmv, sp1: 20% of the original table.\nsp2, 5% of the original table. for IMDB and TPCH sp1: updat-\ning the first partition and sp2: updating the last partition.\nDatasetDBEst++ Naru TVAE\nsp1 sp2 sp1 sp2 sp1 sp2\ncensus 5 5.5 3.5 4 3.4 5.7\nforest 1.6 4 5 9.2 3.6 7\nDMV 4 6.5 2.3 9.6 3.4 6.8\nIMDB 4.5 18 3.5 5 NA NA\ntpch 6.5 16 2 4 NA NA\n11\n\n(a)\n (b)\n(c)\n (d)\nFigure 8: DDUpâ€™s performance on joined tables.\n5.5 Effect of Transfer Learning\nWe now delve into the effects of transfer-learning in DDUp. How\nmuch DDUpâ€™s transfer-learning via knowledge distillation con-\ntributes to better accuracy? We perform experiments where we\nremove the transfer-learning term of Eq 5. Therefore, we combine\nthe sample from previous data known as the transfer-set with the\nnew update batch and create a model with the same configurations\nas the base model. Figure 9 shows the results. The results assert that\nthe performance of DDUp is not only related to the previous data\nsample, and in fact, distillation has a big effect on the improvement\nof the new models.\nFigure 9: Effect of transfer-learning on q-error. AggTrain, is\nthe case where we aggregate the transfer-set with the new\ndata and train a model similar to the base model.\n5.6 Overheads\nWe report on the costs of each DDUp module separately. All the\ncodes are written and executed in Python 3.8, on an Ubuntu 20\nmachine with 40 CPU cores, two Nvidia GTX 2080 GPUs and 64GB\nmemory. With respect to memory usage, DDUp performs regular\nfeed-forward steps as in regular NN training. Therefore, DDUp does\nnot increase memory footprints In terms of time, DDUp has two\ncomputation costs namely, OOD detection andmodel update . OOD\ndetection is split into offline and online phases. Table 11 shows\nthese two times. The largest detection time is for the forest dataseton a Naru model which takes around 3 minutes. However, please\nnote that in the online phase only takes 1 second to detect a change\nin data.\nTable 11: online and offline times during OOD detection.\nDatasetDBEst++ Naru TVAE\noff on off on off on\ncensus 2.44 0.02 111 1.8 310 5.5\nforest 28 0.04 174 0.92 433 8.8\nDMV 86 2 144 10 99 0.44\nTable 10 shows DDUpâ€™s speed up over retrain for OOD data,\nfor different update sizes. When data is OOD, DDUp can be over\n9Ã—faster than retrain . Obviously, speedups will be higher for\nincremental steps. This fact is reflected in IMDB and TPCH datasets\nwhere after inserting the last partition DDUp is 18 Ã—faster than\nretrain . Note that the updating time is dependent on a few param-\neters including update size, transfer-set size, training batch size etc.\nDuring updates, we have used smaller training batch sizes. If one\ntunes the model for bigger batches, and smaller transfer-set sizes,\nthe speed up would be higher.\n5.7 Non neural network models\nFor the sake of completeness and as an additional reference point,\nwe include results for updating a state-of-the-art non-NN model that\nnatively supports data insertions, (DeepDB [ 21]) used for CE. When\nan update happens, DeepDB traverses its sum-product-network\ngraph and updates the weights of the intermediate nodes and the\nhistograms at the leaves. We have repeated the same experiment in\nTable 5 for DeepDB. The results are reported in Table 12.\nFrom Table 12 it can be observed that DeepDBâ€™s updating policy\nis under-performing, as was independently verified in [ 70]. DDUp\n(coupled in this experiment with Naru/NeuroCard for CE) always\nperforms better. Nonetheless, we wish to emphasize that the saving\ngrace for DeepDB based on our experiments is that retraining from\nscratch is very efficient â€“ significantly faster compared to NNs.\n6 RELATED WORK\n6.1 Learned Database Systems\nNN-based components to be used by DBs are emerging rapidly.\nDifferent works exploit different neural network models. [ 17,77,\n78] used generative neural networks to build learned selectivity\nestimators. Thirumuruganathan et al. [ 65] used VAEs for AQP. Ma et\nal. [42] used mixture density networks for AQP. Database indexing\nresearch recently has adopted neural networks to approximate\ncumulative density functions [ 9,10,30,49]. Query optimization\nand join ordering are also benefiting from neural networks [ 27,45].\nOther applications include auto-tuning databases [ 34,68,81], cost\nestimation [63, 83], and workload forecasting [85].\nAmong these, this work provides a solution for handling NN\nmodel maintenance in the face of insertion-updates with OOD data,\nwhen the models need to continue ensuring high accuracy on new\nand old data and on tasks for which models were originally trained\n(such as AQP, CE, DG, etc.). While there has been related research\non transfer learning for learned DBs such as [ 20,74] these target a\n12\n\nTable 12: Performance of DeepDB updating vs. DDUp for\nNaru, for a CE task in terms of q-error.\nDataset metricDeepDB Naru\nğ‘€0update retrainğ‘€0DDUp\ncensusmedian 1.05 1.2 1.05 1.08 1.09\n95th 3 4.18 3 2 2\n99th 5.11 8 5 3 3\nforestmedian 1.02 1.2 1.02 1.04 1.07\n95th 7.5 10.5 7 2.48 3\n99th 31 52 31 4 6\nDMVmedian 1.06 1.25 1.1 1.02 1.04\n95th 2.5 3.5 2.5 1.20 1.41\n99th 22 37 21 1.83 2.31\ndifferent problem setting: They study how to transfer knowledge\nfrom a model trained for one task, and/or a DB, and/or a system,\nand/or a workload to a new task and/or DB, and/or system, and/or\nworkload. They do not study how to keep performing the original\ntask(s) on evolving datasets with insertions carrying OOD data with\nhigh accuracy for queries on both old and new data. Simply using\nthese methods by fine-tuning on new data will incur catastrophic\nforgetting. Nevertheless, since these models employ some sorts\nof knowledge transfer, they might be useful to support updates.\nHowever, it remains open whether and how the models in [ 20,74]\ncan be utilized to solve efficiently the problems tackled in this paper.\nWhile some of non-neural-network models (e.g., DeepDB) can very\nefficiently retrain from scratch, NN-based models for the above\nproblem setting either do not support insertion-updates or suffer\nfrom poor accuracy when facing OOD data, unless paying the high\ncosts of retraining from scratch.\n6.2 OOD Detection\nOOD detection has recently attracted a lot of attention and it has\nlong been studied in statistics as concept drift (CD) detection, or nov-\nelty detection. In general, CD and OOD detection methods could be\ndivided into two broad categories [ 14,39,69]: First, prediction-based\nmethods, which use the predictions of the underlying models to\ntest for a change. Recent ML models usually use the predictive prob-\nabilities of the classifiers as a confidence score to identify changes\n[23,53,58,72]. Others may monitor the error of the underlying\nmodels and trigger an OOD signal when a significant change is cap-\ntured [ 2,13,37,50,60]. While these approaches are very efficient in\ntime, they typically come with limiting assumptions depending on\nthe underlying model or application. For example, most of them can\nonly be utilized and are only studied for classification (supervised)\ntasks. The second broad family of methods is distribution-based\nmethods. Some of these methods try to find a distance measure that\ncan best show the discrepancy between new data and old data distri-\nbutions, using tests like Kolmogorov-Smirnov (KS), [ 29], Wilcoxon\n[55], and their multi-variate variants [ 3,11]. Others try to learn\nthe density of the underlying data distribution test for a significant\nchange, like kernel-density-based approaches [ 4,8,16,25,40,64].\nMore recent works utilize the estimated likelihoods of generative\nmodels [ 47,57,75]. Other approaches rely on the inner representa-\ntions of the networks [ 19,32,33]. Nonetheless, this second familyof OOD detection methods are usually expensive (esp. for multi-\ndimensional data) and involve fitting a separate density estimator.\nHence, the main problem is that in an insertion scenario, the den-\nsity estimators also need to be updated (typically via training from\nscratch, upon each insertion).\n6.3 Incremental Learning (IL)\nMost IL methods regularize the model in a way that it acquires\nknowledge from the new task while retaining the knowledge of\nold tasks. For example, Elastic Weight Consolidation (EWC) [28]\nadds a regularizer to control the learning speed around important\nweights of the network for old tasks while learning a new task.\nSimilar works are developed around this idea [ 31,36,67],Path Inte-\ngral (PathInt) [80] ,Riemanian Walk (RWalk) [6]. Other approaches\nexploit knowledge distillation to retain the knowledge of previous\ntasks [ 35]. Another group of IL methods, save exemplars from past\ndata [ 5,56,73] or generate samples/features using generative mod-\nels [24,52] and involve them in learning new tasks. Lopez et al. [ 38]\nhas proposed Gradient Episodic Memory that consists of Mblocks\nof memory to store examples from Ttasks and uses the modelâ€™s\nprediction on these examples as a constraining loss that inhibits\nthe model to bias toward new task and forget past tasks. Lastly,\nsome works try to completely keep previous models and create new\nmodels (or part of a model like a single layer) for each new task.\nAljundi et al. [ 1] introduce Expert Gate with different models for\neach task and an autoencoder which learns the representations of\neach task to assign test-time tasks to the proper model. Instead of\nlearning a whole new model, Rusu et al. [ 59] introduce Progressing\nNeural Networks which add new columns to the previous network\narchitecture and learns lateral connections between them. Most of\nthe above methods, do not account for in- and out- of distribution\nupdates and are not easily extendable to different learning tasks.\n7 CONCLUSION\nLearned DB components can become highly inaccurate when faced\nwith new OOD data when aiming to ensure high accuracy for\nqueries on old and new data for their original learning tasks. This\nwork proposes, to our knowledge, the first solution to this problem,\ncoined DDUp. DDUp entails two novel components, for OOD de-\ntection and model updating. To make detection widely applicable,\nOOD detection in DDUp exploits the output of the neural network\n(be it based on log-likelihood, cross-entropy, ELBO loss, etc.), and\nutilizes a principled two-sample test and a bootstrapping method to\nefficiently derive and use thresholds to signal OOD data. DDUp also\noffers a general solution for model updating based on sequential\nself-distillation and a new loss function which carefully accounts\nforcatastrophic forgetting andintransigence . This work showcases\nthe wide applicability of DDUp model updating by instantiating\nthe general approach to three important learned functions for data\nmanagement, namely AQP, CE, and DG, whereby a different type\nof NN (MDNs, DARNs, VAEs) is used for each. In fact, to our knowl-\nedge, no prior work has shown how to \"distill-and-update\" MDNs,\nVAEs, and DARNs. Comprehensive experimentation showcases that\nDDUp detects OOD accurately and ensures high accuracy with its\nupdated models with very low overheads.\n13\n\n8 ACKNOWLEDGEMENT\nThis work is partially sponsored by Huawei IRC and by EPSRC\nwhile doing a PhD at the University of Warwick.\nREFERENCES\n[1]Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. 2017. Expert gate:\nLifelong learning with a network of experts. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition . 3366â€“3375.\n[2]Manuel Baena-GarcÄ±a, JosÃ© del Campo-Ãvila, RaÃºl Fidalgo, Albert Bifet, R Gavalda,\nand Rafael Morales-Bueno. 2006. Early drift detection method. In Fourth interna-\ntional workshop on knowledge discovery from data streams , Vol. 6. 77â€“86.\n[3]Ludwig Baringhaus and Carsten Franz. 2004. On a new multivariate two-sample\ntest. Journal of multivariate analysis 88, 1 (2004), 190â€“206.\n[4]Li Bu, Cesare Alippi, and Dongbin Zhao. 2016. A pdf-free change detection test\nbased on density difference estimation. IEEE transactions on neural networks and\nlearning systems 29, 2 (2016), 324â€“334.\n[5]Francisco M Castro, Manuel J MarÃ­n-JimÃ©nez, NicolÃ¡s Guil, Cordelia Schmid, and\nKarteek Alahari. 2018. End-to-end incremental learning. In Proceedings of the\nEuropean conference on computer vision (ECCV) . 233â€“248.\n[6]Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS\nTorr. 2018. Riemannian walk for incremental learning: Understanding forgetting\nand intransigence. In Proceedings of the European Conference on Computer Vision\n(ECCV) . 532â€“547.\n[7]Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F Stewart,\nand Jimeng Sun. 2017. Generating multi-label discrete patient records using\ngenerative adversarial networks. In Machine learning for healthcare conference .\nPMLR, 286â€“305.\n[8]Tamraparni Dasu, Shankar Krishnan, Suresh Venkatasubramanian, and Ke\nYi. 2006. An information-theoretic approach to detecting changes in multi-\ndimensional data streams. In In Proc. Symp. on the Interface of Statistics, Computing\nScience, and Applications . Citeseer.\n[9]Jialin Ding, Umar Farooq Minhas, Jia Yu, Chi Wang, Jaeyoung Do, Yinan Li,\nHantian Zhang, Badrish Chandramouli, Johannes Gehrke, Donald Kossmann,\net al.2020. ALEX: an updatable adaptive learned index. In Proceedings of the 2020\nACM SIGMOD International Conference on Management of Data . 969â€“984.\n[10] Jialin Ding, Vikram Nathan, Mohammad Alizadeh, and Tim Kraska. 2020.\nTsunami: A learned multi-dimensional index for correlated data and skewed\nworkloads. arXiv preprint arXiv:2006.13282 (2020).\n[11] Giovanni Fasano and Alberto Franceschini. 1987. A multidimensional version of\nthe Kolmogorovâ€“Smirnov test. Monthly Notices of the Royal Astronomical Society\n225, 1 (1987), 155â€“170.\n[12] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima\nAnandkumar. 2018. Born again neural networks. In International Conference on\nMachine Learning . PMLR, 1607â€“1616.\n[13] Joao Gama and Gladys Castillo. 2006. Learning with local drift detection. In\nInternational conference on advanced data mining and applications . Springer, 42â€“\n55.\n[14] JoÃ£o Gama, Indr Ë™e Å½liobait Ë™e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid\nBouchachia. 2014. A survey on concept drift adaptation. ACM computing surveys\n(CSUR) 46, 4 (2014), 1â€“37.\n[15] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal sunshine\nof the spotless net: Selective forgetting in deep networks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 9304â€“9312.\n[16] Feng Gu, Guangquan Zhang, Jie Lu, and Chin-Teng Lin. 2016. Concept drift\ndetection based on equal density estimation. In 2016 International Joint Conference\non Neural Networks (IJCNN) . IEEE, 24â€“30.\n[17] Shohedul Hasan, Saravanan Thirumuruganathan, Jees Augustine, Nick Koudas,\nand Gautam Das. 2020. Deep learning models for selectivity estimation of multi-\nattribute queries. In Proceedings of the 2020 ACM SIGMOD International Conference\non Management of Data . 1035â€“1050.\n[18] Dan Hendrycks and Kevin Gimpel. 2017. A Baseline for Detecting Misclassified\nand Out-of-Distribution Examples in Neural Networks.. In ICLR .\n[19] Dan Hendrycks, Kimin Lee, and Mantas Mazeika. 2019. Using pre-training\ncan improve model robustness and uncertainty. In International Conference on\nMachine Learning . PMLR, 2712â€“2721.\n[20] Benjamin Hilprecht and Carsten Binnig. 2021. One Model to Rule them All:\nTowards Zero-Shot Learning for Databases. arXiv preprint arXiv:2105.00642\n(2021).\n[21] Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian\nKersting, and Carsten Binnig. 2019. Deepdb: Learn from data, not from queries!\narXiv preprint arXiv:1909.00607 (2019).\n[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in\na neural network. arXiv preprint arXiv:1503.02531 (2015).\n[23] Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta. 2018. To trust or not\nto trust a classifier. Advances in neural information processing systems 31 (2018).\n[24] Ronald Kemker and Christopher Kanan. 2017. Fearnet: Brain-inspired model for\nincremental learning. arXiv preprint arXiv:1711.10563 (2017).[25] Daniel Kifer, Shai Ben-David, and Johannes Gehrke. 2004. Detecting change in\ndata streams. In VLDB , Vol. 4. Toronto, Canada, 180â€“191.\n[26] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114 (2013).\n[27] Andreas Kipf, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter Boncz, and\nAlfons Kemper. 2018. Learned cardinalities: Estimating correlated joins with\ndeep learning. arXiv preprint arXiv:1809.00677 (2018).\n[28] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume\nDesjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka\nGrabska-Barwinska, et al .2017. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the national academy of sciences 114, 13 (2017), 3521â€“\n3526.\n[29] Andrey Kolmogorov. 1933. Sulla determinazione empirica di una lgge di dis-\ntribuzione. Inst. Ital. Attuari, Giorn. 4 (1933), 83â€“91.\n[30] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In Proceedings of the 2018 International\nConference on Management of Data . 489â€“504.\n[31] Janghyeon Lee, Hyeong Gwon Hong, Donggyu Joo, and Junmo Kim. 2020. Con-\ntinual learning with extended kronecker-factored approximate curvature. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion. 9001â€“9010.\n[32] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. 2018. A simple unified\nframework for detecting out-of-distribution samples and adversarial attacks.\nAdvances in neural information processing systems 31 (2018).\n[33] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. 2021. Cutpaste:\nSelf-supervised learning for anomaly detection and localization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9664â€“9674.\n[34] Guoliang Li, Xuanhe Zhou, Shifu Li, and Bo Gao. 2019. Qtune: A query-aware\ndatabase tuning system with deep reinforcement learning. Proceedings of the\nVLDB Endowment 12, 12 (2019), 2118â€“2130.\n[35] Zhizhong Li and Derek Hoiem. 2017. Learning without forgetting. IEEE transac-\ntions on pattern analysis and machine intelligence 40, 12 (2017), 2935â€“2947.\n[36] Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M Lopez, and\nAndrew D Bagdanov. 2018. Rotate your networks: Better weight consolidation\nand less catastrophic forgetting. In 2018 24th International Conference on Pattern\nRecognition (ICPR) . IEEE, 2262â€“2268.\n[37] David Lopez-Paz and Maxime Oquab. 2016. Revisiting classifier two-sample tests.\narXiv preprint arXiv:1610.06545 (2016).\n[38] David Lopez-Paz and Marcâ€™Aurelio Ranzato. 2017. Gradient episodic memory for\ncontinual learning. Advances in neural information processing systems 30 (2017),\n6467â€“6476.\n[39] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. 2018.\nLearning under concept drift: A review. IEEE Transactions on Knowledge and\nData Engineering 31, 12 (2018), 2346â€“2363.\n[40] Ning Lu, Guangquan Zhang, and Jie Lu. 2014. Concept drift detection via compe-\ntence models. Artificial Intelligence 209 (2014), 11â€“28.\n[41] Lin Ma, Bailu Ding, Sudipto Das, and Adith Swaminathan. 2020. Active learning\nfor ML enhanced database systems. In Proceedings of the 2020 ACM SIGMOD\nInternational Conference on Management of Data . 175â€“191.\n[42] Qingzhi Ma, Ali Mohammadi Shanghooshabad, Mehrdad Almasi, Meghdad Kur-\nmanji, and Peter Triantafillou. 2021. Learned Approximate Query Processing:\nMake it Light, Accurate and Fast.. In CIDR .\n[43] Qingzhi Ma and Peter Triantafillou. 2019. Dbest: Revisiting approximate query\nprocessing engines with machine learning models. In Proceedings of the 2019\nInternational Conference on Management of Data . 1553â€“1570.\n[44] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Al-\nizadeh, and Tim Kraska. 2021. Bao: Making learned query optimization practical.\nInProceedings of the 2021 International Conference on Management of Data . 1275â€“\n1288.\n[45] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi Zhang, Mohammad Alizadeh,\nTim Kraska, Olga Papaemmanouil, and Nesime Tatbul. 2019. Neo: A learned\nquery optimizer. arXiv preprint arXiv:1904.03711 (2019).\n[46] Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in con-\nnectionist networks: The sequential learning problem. In Psychology of learning\nand motivation . Vol. 24. Elsevier, 109â€“165.\n[47] Warren Morningstar, Cusuh Ham, Andrew Gallagher, Balaji Lakshminarayanan,\nAlex Alemi, and Joshua Dillon. 2021. Density of states estimation for out of\ndistribution detection. In International Conference on Artificial Intelligence and\nStatistics . PMLR, 3232â€“3240.\n[48] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji\nLakshminarayanan. 2018. Do deep generative models know what they donâ€™t\nknow? arXiv preprint arXiv:1810.09136 (2018).\n[49] Vikram Nathan, Jialin Ding, Mohammad Alizadeh, and Tim Kraska. 2020. Learn-\ning multi-dimensional indexes. In Proceedings of the 2020 ACM SIGMOD Interna-\ntional Conference on Management of Data . 985â€“1000.\n[50] Rimma V Nehme, Elke A Rundensteiner, and Elisa Bertino. 2009. Self-tuning\nquery mesh for adaptive multi-route query processing. In Proceedings of the 12th\n14\n\nInternational Conference on Extending Database Technology: Advances in Database\nTechnology . 803â€“814.\n[51] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep neural networks are easily\nfooled: High confidence predictions for unrecognizable images. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition . 427â€“436.\n[52] Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin\nNabi. 2019. Learning to remember: A synaptic plasticity driven framework for\ncontinual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition . 11321â€“11329.\n[53] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian\nNowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. 2019. Can\nyou trust your modelâ€™s uncertainty? evaluating predictive uncertainty under\ndataset shift. Advances in neural information processing systems 32 (2019).\n[54] Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu\nPark, and Youngmin Kim. 2018. Data synthesis based on generative adversarial\nnetworks. arXiv preprint arXiv:1806.03384 (2018).\n[55] Francisco Pereira, Tom Mitchell, and Matthew Botvinick. 2009. Machine learning\nclassifiers and fMRI: a tutorial overview. Neuroimage 45, 1 (2009), S199â€“S209.\n[56] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H\nLampert. 2017. icarl: Incremental classifier and representation learning. In Pro-\nceedings of the IEEE conference on Computer Vision and Pattern Recognition . 2001â€“\n2010.\n[57] Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo,\nJoshua Dillon, and Balaji Lakshminarayanan. 2019. Likelihood ratios for out-\nof-distribution detection. Advances in Neural Information Processing Systems 32\n(2019).\n[58] Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, GrÃ©goire Montavon,\nWojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert MÃ¼ller.\n2021. A unifying review of deep and shallow anomaly detection. Proc. IEEE\n(2021).\n[59] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James\nKirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Pro-\ngressive neural networks. arXiv preprint arXiv:1606.04671 (2016).\n[60] Fotis Savva, Christos Anagnostopoulos, and Peter Triantafillou. 2019. Aggregate\nquery prediction under dynamic workloads. In 2019 IEEE International Conference\non Big Data (Big Data) . IEEE, 671â€“676.\n[61] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh.\n2021. Remember what you want to forget: Algorithms for machine unlearning.\nAdvances in Neural Information Processing Systems 34 (2021), 18075â€“18086.\n[62] Ali Mohammadi Shanghooshabad, Meghdad Kurmanji, Qingzhi Ma, Michael\nShekelyan, Mehrdad Almasi, and Peter Triantafillou. 2021. PGMJoins: Random\nJoin Sampling with Graphical Models. In Proceedings of the 2021 International\nConference on Management of Data . 1610â€“1622.\n[63] Tarique Siddiqui, Alekh Jindal, Shi Qiao, Hiren Patel, and Wangchao Le. 2020.\nCost models for big data query processing: Learning, retrofitting, and our findings.\nInProceedings of the 2020 ACM SIGMOD International Conference on Management\nof Data . 99â€“113.\n[64] Xiuyao Song, Mingxi Wu, Christopher Jermaine, and Sanjay Ranka. 2007. Sta-\ntistical change detection for multi-dimensional data. In Proceedings of the 13th\nACM SIGKDD international conference on Knowledge discovery and data mining .\n667â€“676.\n[65] Saravanan Thirumuruganathan, Shohedul Hasan, Nick Koudas, and Gautam Das.\n2020. Approximate query processing for data exploration using deep generative\nmodels. In 2020 IEEE 36th International Conference on Data Engineering (ICDE) .\nIEEE, 1309â€“1320.\n[66] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, and Phillip\nIsola. 2020. Rethinking Few-Shot Image Classification: a Good Embedding Is All\nYou Need?. In https://arxiv.org/abs/2003.11539 .\n[67] Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pas-\ncanu, and Yee Whye Teh. 2019. Functional regularisation for continual learningwith gaussian processes. arXiv preprint arXiv:1901.11356 (2019).\n[68] Dana Van Aken, Andrew Pavlo, Geoffrey J Gordon, and Bohan Zhang. 2017.\nAutomatic database management system tuning through large-scale machine\nlearning. In Proceedings of the 2017 ACM International Conference on Management\nof Data . 1009â€“1024.\n[69] K Wang, Paul Vicol, Eleni Triantafillou, and Richard Zemel. 2020. Few-shot\nOut-of-Distribution Detection. In ICML Workshop on Uncertainty and Robustness\nin Deep Learning .\n[70] Xiaoying Wang, Changbo Qu, Weiyuan Wu, Jiannan Wang, and Qingqing Zhou.\n2020. Are We Ready For Learned Cardinality Estimation? arXiv preprint\narXiv:2012.06743 (2020).\n[71] AJ Watkins and KV Mardia. 1992. Maximum likelihood estimation and prediction\nmean square error in the spatial linear model. Journal of Applied Statistics 19, 1\n(1992), 49â€“59.\n[72] Andrew G Wilson and Pavel Izmailov. 2020. Bayesian deep learning and a proba-\nbilistic perspective of generalization. Advances in neural information processing\nsystems 33 (2020), 4697â€“4708.\n[73] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong\nGuo, and Yun Fu. 2019. Large scale incremental learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 374â€“382.\n[74] Ziniu Wu, Peilun Yang, Pei Yu, Rong Zhu, Yuxing Han, Yaliang Li, Defu Lian, Kai\nZeng, and Jingren Zhou. 2021. A unified transferable model for ml-enhanced\ndbms. arXiv preprint arXiv:2105.02418 (2021).\n[75] Zhisheng Xiao, Qing Yan, and Yali Amit. 2020. Likelihood regret: An out-of-\ndistribution detection score for variational auto-encoder. Advances in neural\ninformation processing systems 33 (2020), 20685â€“20696.\n[76] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramacha-\nneni. 2019. Modeling tabular data using conditional gan. Advances in Neural\nInformation Processing Systems 32 (2019).\n[77] Zongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi Chen,\nand Ion Stoica. 2020. NeuroCard: one cardinality estimator for all tables. arXiv\npreprint arXiv:2006.08109 (2020).\n[78] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi Chen,\nPieter Abbeel, Joseph M Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019. Deep\nunsupervised cardinality estimation. arXiv preprint arXiv:1905.04278 (2019).\n[79] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. 2017. A gift from knowl-\nedge distillation: Fast optimization, network minimization and transfer learning.\nInProceedings of the IEEE Conference on Computer Vision and Pattern Recognition .\n4133â€“4141.\n[80] Friedemann Zenke, Ben Poole, and Surya Ganguli. 2017. Continual learning\nthrough synaptic intelligence. In International Conference on Machine Learning .\nPMLR, 3987â€“3995.\n[81] Ji Zhang, Yu Liu, Ke Zhou, Guoliang Li, Zhili Xiao, Bin Cheng, Jiashu Xing,\nYangtao Wang, Tianheng Cheng, Li Liu, et al .2019. An end-to-end automatic\ncloud database tuning system using deep reinforcement learning. In Proceedings\nof the 2019 International Conference on Management of Data . 415â€“432.\n[82] Zhuoyue Zhao, Robert Christensen, Feifei Li, Xiao Hu, and Ke Yi. 2018. Random\nsampling over joins revisited. In Proceedings of the 2018 International Conference\non Management of Data . 1525â€“1539.\n[83] Johan Kok Zhi Kang, Sien Yi Tan, Feng Cheng, Shixuan Sun, and Bingsheng\nHe. 2021. Efficient Deep Learning Pipelines for Accurate Cost Estimations Over\nLarge Scale Query Workload. In Proceedings of the 2021 International Conference\non Management of Data . 1014â€“1022.\n[84] Rong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping Qian,\nJingren Zhou, and Bin Cui. 2020. FLAT: Fast, Lightweight and Accurate Method\nfor Cardinality Estimation. arXiv preprint arXiv:2011.09022 (2020).\n[85] Yonghua Zhu, Weilin Zhang, Yihai Chen, and Honghao Gao. 2019. A novel\napproach to workload prediction using attention-based LSTM encoder-decoder\nnetwork in cloud environment. EURASIP Journal on Wireless Communications\nand Networking 2019, 1 (2019), 1â€“18.\n15",
  "textLength": 88632
}