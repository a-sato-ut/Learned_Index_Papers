{
  "paperId": "b1953cda665e4dbd7b52bfc27d9a9c352166b7ec",
  "title": "Can LSH (locality-sensitive hashing) be replaced by neural network?",
  "pdfPath": "b1953cda665e4dbd7b52bfc27d9a9c352166b7ec.pdf",
  "text": "Springer Nature 2021 L ATEX template\nCan LSH (Locality-Sensitive Hashing) Be Replaced by Neural\nNetwork?\nRenyang Liu1,3†, Jun Zhao4, Xing Chu2,3, Yu Liang2,3, Wei Zhou2,3and Jing He2,3*\n1*School of Information Science and Engineering, Yunnan University, Wujiaying,\nKunming, 650500, Yunnan, China.\n2School of Software, Yunnan University, Wujiaying, Kunming, 650500, Yunnan, China.\n3Engineering Research Center of Cyberspace.\n4Didi Chuxing, Dongbeiwang, 100000, Beijing, China.\n*Corresponding author(s). E-mail(s): hejing@ynu.edu.cn;\nContributing authors: ryliu@mail.ynu.edu.cn; jhinzhao@didiglobal.com; chx@ynu.edu.cn;\nyuliang@ynu.edu.cn; zwei@ynu.edu.cn;\n†This paper was accepted by Soft Computing.\nAbstract\nWith the rapid development of GPU (Graphics Processing Unit) technologies and neural networks,\nwe can explore more appropriate data structures and algorithms. Recent progress shows that neu-\nral networks can partly replace traditional data structures. In this paper, we proposed a novel\nDNN (Deep Neural Network)-based learned locality-sensitive hashing, called LLSH, to efficiently and\nflexibly map high-dimensional data to low-dimensional space. LLSH replaces the traditional LSH\n(Locality-sensitive Hashing) function families with parallel multi-layer neural networks, which reduces\nthe time and memory consumption and guarantees query accuracy simultaneously. The proposed\nLLSH demonstrate the feasibility of replacing the hash index with learning-based neural networks\nand open a new door for developers to design and configure data organization more accurately to\nimprove information-searching performance. Extensive experiments on different types of datasets show\nthe superiority of the proposed method in query accuracy, time consumption, and memory usage.\nKeywords: learned index, deep learning, locality-sensitive hashing, kNN\n1 Introduction\nGiven a set of data points and a query, searching\nfor the nearest data point in a given database is\nthe fundamental problem of NN (Nearest Neigh-\nbor) search [2, 4, 26], which is widely used in\ninformation retrieval, data mining, multimedia,\nand scientific databases. Suppose there is a query\npoint qand dataset D, the NN problem is to findan item q1from Don the condition that the dis-\ntance between q1andqis closest. One extension\nof NN is kNN that find top- kclosest items to\nthe query data {q1, ..., q k}from D. The traditional\nkNN algorithm is mainly based on spatial division,\nwhich is most widely used in the tree algorithms,\nsuch as KD-tree [3], R-tree [12], Ball-tree [1].\nAlthough the query accuracy of the tree-based\napproach is high, they require a huge amount of\nmemory, sometimes even exceeding the data itself.\n1arXiv:2310.09806v1  [cs.IR]  15 Oct 2023\n\nSpringer Nature 2021 L ATEX template\n2 Article Title\nBesides, the performance of tree-based indexing\nmethods will be significantly faked when handling\nhigh-dimensional data [11, 27], which is named\n“curse of dimensionality” [5]. In addition, with\nthe development of the current business systems,\nthe data dimensions are increasing, achieving from\nthousands to millions. It puts a high demand\non finding a new way to deal with kNN effi-\nciently because the traditional indexing methods\nare challenging to handle the high dimensional\ndata.\nOne feasible way is to transform the NN and\nkNN problems into ANN (Approximate Nearest\nNeighbors) and kANN problems to cope with the\ngrowing data dimension. In the ANN search, the\nindex method only needs to return the approxi-\nmate nearest objects {q1, ..., q k}rather than find\nthe actual nearest one. In this way, the query effi-\nciency can be significantly improved. The ANNs\nhave a lot of advantages to solving the search\ntasks in scenarios which not require high precision\nto reduce time and memory consumption. Among\nthem, the LSH [14], which basic principle is that\nthe two adjacent data points in the original data\nspace can be hashed into the same bucket by the\nsame mapping or projection transformation rule\nis the most popular one. And it is widely used in\nvarious searching fields, including and not limited\nto text, audio, image, video, gene, et al., due to\nits unusual nature of locality sensitivity and the\nsuperiority to KD-tree [3] and other methods in\nhigh-dimensional searching.\nTraditional LSH, however, is applied to CPU,\nparallel computing and distributed applications,\nwhich greatly limits its potential in the face of\nhigh dimensional data. Moreover, due to the rapid\ndevelopment of hardware, such as GPU/TPU\n(Tensor Processing Unit), the high cost of per-\nforming neural networks may be negligible in the\nnear future. Therefore, inspired by the pioneering\nwork [17] in developing a learned index to explore\nhow neural networks can enhance or even replace\ntraditional index structures. In this paper, we\ndesign a novel neural networks-based framework,\ncalled LLSH, to boot the E2LSH (Exact Euclidean\nLocality Sensitive Hashing) [7] in the task of mas-\nsive data retrieval. The LLSH creatively proposed\nto replace the hash functions in E2LSH with a sim-\nple neural network to improve the search efficiency\nof the hash indexing. Extensive experiments illus-\ntrated the proposed framework’s feasibility andsuperiority in query accuracy, time, and mem-\nory consumption. The main contributions of this\npaper are reflected as follows:\n▷We propose a novel DNN-based learned\nlocally-sensitive hashing, called LLSH, which\ncan be applied to the kNN problem of high-\ndimensional data and avoid ”dimensional curses.”\nTo the best of our knowledge, it is the first\nwork to use neural networks instead of hash func-\ntion families. Each neural network is independent\nand computes parallelly to fully utilize the hard-\nware’s advantages and reduce the false-positive\nand false-negative rates.\n▷We design the framework of LLSH in detail\nand apply it to replace the traditional E2LSH with\ntwo different strategies. The basic one trains the\nneural network layer supervised by the E2LSH\noutputs, while the ensemble one takes a forward\nstep to fully utilize the idea of ensemble learning\nto integrate the outputs of multiple NN algorithms\nto improve the performance.\n▷We conduct extensive experiments, which\ninclude feasibility verification, time and mem-\nory consumption, and query accuracy, on eight\ndatasets with different data types and distribu-\ntions. The empirical results show the viability of\nthe proposed LLSH framework and its superiority\nin reducing time and memory usage and improving\nquery accuracy.\nThe rest of the paper is organized as follows.\nWe briefly review the methods relating to data\nstructure and machine learning in Sec. 2. In Sec. 3,\nwe provide the preliminaries of LSH and E2LSH.\nSec. 4 discusses the details of the proposed LLSH.\nThe experimental results are shown and analyzed\nin Sec. 5. Finally, the paper is concluded in Sec. 6.\n2 Related Works\nOur work is based on a wide range of previous\nexcellent research. In the following, we intend to\nsummarize several essential interactions between\ndata structure and machine learning.\nLSH is a hashing algorithm that was first pro-\nposed by Indyk in 1998. In general, the hash\nalgorithm is a way to reduce conflicts, and it can\nfacilitate quick additions and deletions, but LSH\nis not. LSH, which uses the hash conflict to speed\nup the retrieval effect, is mainly applied to the\nfast approximate search of high-dimensional mass\ndata. The approximate search is a comparison\n\nSpringer Nature 2021 L ATEX template\nArticle Title 3\nof distances or similarities between data points.\nAccording to the different methods of similarity\ncalculation, LSH can be divided into several cate-\ngories, including Simhash [24], E2LSH [7], C2LSH\n[8], Kernel LSH [18], LSB-forest [31], QALSH [13]\netc.\nLSH families have many branches and are\nwidely used in various applications. For exam-\nple, Simhash maps the original text content to\na digital hash signature, where the two similar\ntexts correspond to the same digital signature.\nSo, the similarity of the two documents can be\nmeasured by the Hamming distance between the\nSimhash value. E2LSH is a randomized implemen-\ntation method of LSH in Euclidean space. The\nbasic principle of E2LSH is to use the position-\nsensitive function based on p-stable distribution to\nmap the high-dimensional data and keep the two\nneighbor points in the original space still closest\nto each other after the mapping operation. LSB-\nforest builds multiple trees to adjust to the NN\nsearch. Sun et al. devised SRS [30] with a small\nindex footprint so that the entire index struc-\nture can fit in lesser memory. Recently, a new\nLSH scheme named QALSH (Query-aware data-\ndependent LSH) has been proposed to improve\nsearch accuracy by deciding the bucket boundaries\nafter the query arrives at its position.\nHowever, with the development of AI (Artifi-\ncial Intelligence) and the explosion of data com-\nplexity, machine learning has become a powerful\ntechnique for solving computer optimization prob-\nlems, which require new methods to compute more\nefficiently and intelligently. Recently, researchers\nhave begun employing machine learning to opti-\nmize indexes and hash functions. There is various\nresearch on emulating locality-sensitive hash func-\ntions to build the new ANN indexes, ranging\nfrom supervised [6, 21, 28, 32] to unsupervised\n[9, 10, 15, 16, 20]. These kinds of methods incorpo-\nrate data-driven learning methods in developing\nadvanced hash functions. The principle of these\nworks is learning to a hash, which means learn-\ning the information of data distributions or class\nlabels to guide the design of the new learning-\nbased hash function. However, the hash function’s\nbasic construction is still unchanged. Although,\nthere are some methods, like [19, 33], using the\nneural network to replace a hash function and\nusing the image as the hash label to pursue the\ngood search performance in image retrieval, butthese limits the scope of the hash method and\ncannot be used to construct fundamental data\nstructures directly.\nAs far as we know, paper [17] is the pioneering\nwork in developing a learning index that explores\nhow neural networks can enhance and even replace\ntraditional index structures. It provides a learned\nindex based on a neural network to replace the\nB-tree index and further discusses the difference\nbetween learning hash mapping and traditional\nhash mapping index. Moreover, Our previous\nwork also provides an unsupervised learned index\nnamed PAVO [34]. Therefore, we are well moti-\nvated by these works to propose a novel neural\nnetwork-based learned hash index framework that\ncan utilize new techniques, like a deep neural net-\nwork, and new hardware, like high-performance\nGPU, to construct a novel learning-based hash\nmethod for massive magnitude and dimensional\ndata retrieve.\n3 Preliminary\nLSH is a fast nearest neighbor search algorithm\nfor massive high-dimensional data. We call such\na family of hash functions H=h:S→Uas\n(r1, r2, p1, p2) sensitive if the function hin any H\nsatisfies the following two conditions:\nif d(O1, O2)< r1then Pr [h(O1) =h(O2)]≥p1,\nif d(O1, O2)> r2then Pr [h(O1) =h(O2)]≤p2.\nAmong them, O1, O2∈S, denote two\ndata objects with multi-dimensional attributes,\nd(O1, O2) is a metric function that represents the\ndegree to which two objects are different. And\nthe threshold ( r1, r2, p1, p2) satisfies the condition:\nr1< r 2andp1> p 2. It means that two high-\ndimensional data are mapped to the same hash\nvalues when they are similar enough.\nThe LSH can be divided into different types\naccording to the different similarity calculation\nmethods. One of the most widely used is the\np-stable hash, also called E2LSH, which uses a\nEuclidean distance to measure data similarity. The\np-stable distribution refers to a type of distribu-\ntion defined as follows.\nFor any nreal numbers v1, v2, ..., v nandn\nrandom variables d1, d2, ..., d nsubject to the dis-\ntribution D, there is a p≥0 that makesP\nividi\n\nSpringer Nature 2021 L ATEX template\n4 Article Title\nVectors\nImages\nVideos\nTextsInput Hash index Neural Network Autoencoder\nNN 1\nNN 2\n NN L0 ...fp\n02fp\n01fp\n0n\n1 ...fp\n12fp\n11fp\n1n\nSize-1 ......\n...H1\nH2...\n.........\n... ......\nFC FC feature extraction\nFig. 1 The framework of DNN-based learned index.\nand (P\nivp\ni)1/phave the same distribution ( dis a\nrandom variable in the p-stable distribution). For\nE2LSH, the pof the p-stable distribution is limited\nto 0< p≤2 and defined as follows:\n▷1-stable: Cauchy Distribution\nc(x) =1\nπ1\n1 +x2; (1)\n▷2-stable: Gaussian Distribution\ng(x) =1√\n2πe−x2/2. (2)\nThe family of hash functions are proposed as\nfollows [7]:\nha,b(v) =⌊av+b\nr⌋, (3)\nwhere ais a vector that conforms to the p-stable,\nand the dimension is the same as v,b∈(0, r) is\na random number, ris the length of a straight\nline segment, the establishment of hash function\nfamily is based on the differences of aandb.\nSo, if two points v1andv2are supposed to\nbe mapped into the same hash value, they mustsatisfy av1+bandav2+bare mapped to the same\nline segment.\nLetfp(t) denote the probability density func-\ntion of the absolute value of the p-stable distribu-\ntion. For two vector v1, v2, make c=∥v1−v2∥p,\nthe collision probability in E2LSH is calculated as\nfollows:\np(c) =Pa,b[ha,b(v1) =ha,b(v2)] =Zr\n0fp(t\nc)(1−t\nr)dt.\n(4)\nFor a fixed parameter r, the probability of col-\nlision increases as c=∥v1−v2∥pdecreases. The\nfamily of hash functions is ( r1, r2, p1, p2)-sensitive,\np1=p(1), p2=p(c), r2/r1=c. Therefore, this\nfamily of locality-sensitive hash functions can be\nused to solve the approximate nearest neighbor\nproblem.\nIn order to widen the gap between the col-\nlision probability between the points with short\ndistance and the points with far distance after\n\nSpringer Nature 2021 L ATEX template\nArticle Title 5\nLoss E2LSH\n...\n...E2LSH  Label  LabelHash values\nPicsFeature \nextractionVectors\nVideosFeature \nextractionVectors\nFig. 2 The supervised strategy in neural network stage.\nmapping, E2LSH uses kposition-sensitive func-\ntions together to build the function family:\nG={g:S−→Uk} (5)\nwhere Grepresents the union of kposition-\nsensitive functions, and g(v) = ( h1(v), ..., h k(v)),\nthen each data point v∈Rd’ dimension can be\nreduced via the function g(v)∈ G to obtain a\nk-dimensional vector ⃗ a= (a1, a2, ..., a k). Then,\nE2LSH uses the main hash function H1and the\nsecondary hash function H2to hash the vector\nafter dimension reduction and establishes the hashtable to store data points. The specific forms of\nH1andH2are as follows:\nH1= ((a1∗h1+...ak∗hk)mod C )mod T (6)\nH2= (b1∗h+...bk∗hk)mod C (7)\nwhere aiandbiare randomly selected integers, T\nis the length of the hash table (generally set to\nthe total number of data points n), and Cis a\nlarge prime number (can be set to 232−5 on a 32-\nbit machine). Data points with the same primary\nhash value H1and secondary hash value H2will\n\nSpringer Nature 2021 L ATEX template\n6 Article Title\nbe stored in the same hash bucket to realize the\nclustering of data points.\nFor the query point q, E2LSH first uses the\nlocality-sensitive hash function to obtain a set of\nhash values, then use H1to obtain its location in\nthe hash table and then calculates its H2value,\nand obtain the same H2value of point qby query-\ning the linked list of the location point. Finally,\nto obtain a set of recovered points by querying L\ntables and K(or less than K) neighbor points by\nsorting the distances.\n4 The Framework of\nDNN-Based Learned Index\nTraditionally, we view index structure and\nmachine learning algorithms as pretty different\nresearch branches. The index structure is con-\nstructed fixedly, but the machine learning algo-\nrithm is based on data training. However, both of\nthem are positioning and searching for the space\nposition. There is a potential connection between\nneural networks and indexes. A hash index can be\nregarded as a regression or a classification where\nthe data is predicted based on the key, which is not\nfundamentally different from the neural network’s.\nInspired by the structure of the learned index, we\npropose the following groundbreaking work. This\nsection will present our learned locality-sensitive\nhashing index framework in detail.\n4.1 The framework of DNN-based\nlearned index\nThe ideal locality-sensitive hashing requires map-\nping and querying efficiently. Since the neural\nnetwork with enough parameters has a robust\nfitting ability, using a deep neural network to\nsimulate the hash function is meaningful. Empir-\nically, an arbitrarily complex dataset fed into a\nwell-trained model can always obtain the ideal\nmapping results.\nThe scheme of the proposed method (shown\nin Fig. 1) can be divided into four stages: Input\nStage, Autoencoder Stage, Neural Network Stage,\nand Hash index Stage. The supervised strategy is\nused to train the model in Neural Network Stage.\nWhen LLSH is trained, it can infer the input\ndata to get the corresponding hash value. Taking\nimage data as an example, each piece of data will\ngo through the following four stages: 1) Featureextraction: where SIFT or GIST are generally used\nfor feature extraction; 2) Dimensionality reduc-\ntion: which refers to further dimensionality reduc-\ntion of the extracted features by autoencoder; 3 )\nHash value generation: input the dimensionality-\nreduced feature vector into the neural network to\ngenerate corresponding hash value; 4) Hash index:\nperform the nearest neighbor search of the gen-\nerated hash values. Since training the model in a\nsupervised manner, it can guarantee that similar\ndata will generate similar hash values.\n4.1.1 Input Stage\nThe input stage includes all kinds of data that\nrequire LSH to get mapping results in industrial\nor other scenarios, including various images, audio\nand text. Among them, some simple data, such as\nlatitude and longitude data, can be directly input\ninto the neural network. In contrast, other com-\nplex data need to be preprocessed (e.g., by feature\nextraction) before input into the LLSH, such as\nimage data, audio data and et al. For example, the\nimage data can use the GIST [29] or SIFT [23],\nand the audio data can use the MFCC [22] and\nthe text data can use word2vector [25] to extract\nfeatures, respectively.\n4.1.2 Autoencoder Stage\nAlthough the raw data has been extracted through\nthe traditional feature extraction method, its cor-\nrelation information needs to be expressed more\nadequately and the dimension of extracted fea-\nture is still too large, resulting in large amounts of\nparameters and further increasing computing con-\nsumption in the neural network part. So, LLSH\nfirst builds and trains an autoencoder model with\na large amount of data to make it perform well and\nfurther reduce the extracted features dimensions\nregarding semantics.\n4.1.3 Neural Network Stage\nThe Neural Network Stage is the most critical part\nof the LLSH algorithm. In this stage, the neural\nnetwork is composed of multiple DNN models, and\nthe purpose is to encode the processed data. In\nthis paper, we use Lneural networks to simulate L\nlocality-sensitive hashing function families, each of\nthem outputs khash function values, the same as\na traditional local-sensitive hash function at query\n\nSpringer Nature 2021 L ATEX template\nArticle Title 7\ntime. In this way, we only need a set of neural\nnetworks that return the same result for similar\ndata.\nEach neural network mentioned above acts as\na family of hash functions, where the number of\nlayers and neural nodes is determined according\nto the original hash structure. In the training pro-\ncess, we concat each neural network’s output as\nthe final output of the whole neural network stage\nto calculate the loss with the given label and fur-\nther update the neural network’s parameters. The\ntraining process will be finished soon because the\nparameters of each neural network are updated\nin a parallel way and do not affect each other.\nBesides, for NN search, we don’t need each neural\nnetwork’s output exactly be the same.\n4.1.4 Hash Index Stage\nFinally, after the entire framework is well-trained,\neach neural network’s output will be used as\nthe hash index value and build the multiple\nhash tables. Empirically, the multiple hash tables\ncan significantly reduce false-positive and false-\nnegative rates [7]. In the querying, if the neural\nnetwork outputs are the same for two input data,\nLLSH regards them as similar and maps them\ninto the same storage address (bucket). For more\nconvenience to find the index and decrease the\ncomputation when building a hash table, we build\ntwo extra hash functions, H1, H2, to transform the\nupper stage’s output. The H1, H2is shown below:\nH1(x1, ..., x k) = ((kX\ni=1rixi)mod C )mod T, (8)\nH2(x1, ..., x k) = (kX\ni=1r′\nixi)mod C ), (9)\nwhere ri, r′\niare random integers. C= 232−5, is\na large prime number. The H2’s result is a data\nfingerprint, and the H1’s result is the index of the\nhashtable in which the data fingerprint resides.\n4.2 Autoencoder and Neural\nNetwork Design\nIn this subsection, we will introduce the autoen-\ncoder and neural network of the Sec. 4.1 in detail,\nincluding the architecture and parameter design.\nNN 2\nNN LNN 1\n......\nm1m2m3\n...km3k\nm3kFig. 3 The detail of autoencoder and neural network\nFig. 3 shows the autoencoder and neural network\ndetail.\nTo reduce the number of parameters in LLSH,\nwe design a relatively small autoencoder that only\nincludes the input layer, one hidden layer, and\nthe output layer. In this paper, the autoencoder\npart is used as the feature extractor to reduce the\ndata’s dimension. To train this autoencoder more\nefficiently, we first pre-train it with large-scale\ndata, and when faced with different datasets, we\nuse transfer learning to fine-tune it again. When\ntrained, the autoencoder can output a feature\nvector with a lower dimension.\nSimilar to the autoencoder, we use two fully\nconnected (FC) layers to implement each small\nneural network unit. The whole model contains N\n(N= 1,2, ..., L ) small units, named NN Lrespec-\ntively. The first layer of each small unit contains\nm3 neurons and the last layer contains kneu-\nrons. Each neural network’s output contacts the\nfinal hash values of the whole model. Note that\nwhere the N,m3, and kcould adjust to keep good\nperformance concerning the data size flexibly.\nTherefore, the number of LLSH’s parameter is\np1=d∗m1+m1∗m2+(m2∗m3+m3∗k)∗L, while\nthe traditional E2LSH algorithm is p2=d∗k∗L.\nIn the actual implementation, we make p1<< p 2\nbut without loss in query performance.\n\nSpringer Nature 2021 L ATEX template\n8 Article Title\nTable 1 The datasets used in the experiments: four are randomly generated, and four are from the real world.\nDataset Type Dimension Mean Std Dataset Type Dimension Mean Std\nUniform Random 100 0.5 0.29 Tiny Images GIST 384 0.11 0.07\nNormal Random 100 0 1 Ann SIFT SIFT 128 27.05 35.89\nLognormal Random 100 1.65 2.16 Nytimes word2vec 250 0 0.06\nExponential Random 100 1 1 Golve word2vec 200 0 0.45\nFig. 4 The fitting rate on different datasets.\n4.3 Model training and prediction\nIn this subsection, we will describe the train and\nprediction of the LLSH in detail. The first is to\ntrain the model to build the hash index well. When\nthe model is well-trained, the second is to calcu-\nlate the hash value of the query data by model\nprediction. Fig. 2 shows the overall framework of\nthe first part. The specific steps are as follows:\n•Step 1: The feature extraction of different kinds\nof data such as images, audio, and texts extract\nfeatures to obtain their corresponding feature\nvectors (v1), and then put the extracted feature\nvector into the autoencoder mentioned above\nto get more condensed vectors (v2) with lower\ndimension;\n•Step 2: Input the vectors (v2) obtained by step\n1 into the traditional E2LSH to obtain the L∗k\nhash values and concatenate them into a matrix\nas the label;•Step 3: Train neural networks with the vectors\n(v2) and their corresponding labels obtained in\nStep 2 until the model reaches convergence.\n•Step 4: Input the query item to the well-trained\nmodel for predicting the hash value.\nLoss function: The purpose of training neu-\nral networks is to make its output match the\nE2LSH output by iteratively updating the net-\nworks’ parameters. And we expect the predicted\nresults to be as close as possible to the hash value\ngenerated by the E2LSH. So, we chose the mean\nsquare error (MSE) loss as the objective function\nas follows:\nLoss =1\nNNX\ni=1(yi−ˆyi)2, (10)\nwhere yirepresents the neural network’s output,\nˆyirefers to label, and Nis the total number of\ndata of per output. We use Adam for optimizing\nand Relu for the activation in the training process.\n\nSpringer Nature 2021 L ATEX template\nArticle Title 9\n5 Experiment\nIn this section, we will discuss the experiment\ndetails. All the experiments were conducted on\na GPU server equipment with 128GB memory,\ntwo 2.1 GHz Intel(R) E5 processors, and two\nGTX1080Ti GPU cards with 11GB dedicated\nmemory, and the operating system is CentOS 7.\nWe use Python 3.6 and TensorFlow 1.13.1 to\nimplement all code work. We repeat each experi-\nment ten times and then use the median or average\nof the ten results as the final performance.\n5.1 Setup\nDatasets: The dataset used in our experiments\ncomes from two different types, synthetic data\nand real data, to adapt to data with different dis-\ntributions in practical applications. Specifically,\nthere are four synthetic datasets sampled from\nthe distributions of uniform, exponential, nor-\nmal and lognormal, respectively. The other four\ndatasets include Tiny Images, Ann Sift, Nytimes\nand Glove, which involved images and word vec-\ntors. The details of the aforementioned datasets,\nwhich are detailed in Table 1, contain different\ntypes, scales and dimensionality.\nMetrics: In order to evaluate the simulating\nability of the neural network-based LLSH in this\nwork to the traditional E2LSH method, we use fit-\nting accuracy as its evaluation metric, which refers\nto the correct rate of fitting E2LSH. It defines as\nfollows:\nFrate=M\nN×100% , (11)\nwhere the Mrepresents the same output numbers\nof neural networks and E2LSH, Nis the out-\nput dimension. A higher fitting accuracy indicates\nLLSH fits E2LSH more correctly.\nParameters: For all of our experiments, we\nset the E2LSH parameters as K= 10, L= 30, and\nr= 4 (the width of projection), and set M= 2,\nL= 30 and K= 10 for the proposed LLSH.\n5.2 Ablation Study\nThe suitable combination of parameters of L, k, r\nsignificantly impacts the performance of tradi-\ntional E2LSH. In our framework, however, the\nmost critical parameters are M, L, k , where Mis\nthe number of neural network layers. Therefore,in this subsection, we study how combining these\nparameters could boost LLSH.\nIn general, the more neural network layers\nmean the better the learning performance. How-\never, our experiments show it is not exactly true\nfor this work. The results in Fig. 5 and Fig. 6 illus-\ntrated the query accuracy of various Lon three\nrandom datasets drawn from uniform (a), nor-\nmal (b) and lognormal distribution (c), and a real\nimage dataset Tiny Images (d), respectively. The\nquery accuracy decrease with the layers Mgrows\nup, suggesting that a smaller Mshows a better\nquery effect. The query accuracy reaches the top\npoint when M= 2 both in Fig. 5 and Fig. 6. From\nFig. 5. We also observe that Lhas a vital influ-\nence on query accuracy and L= 30 leads to the\nhighest query accuracy. Moreover, the results in\nFig. 6 suggested that with the increase of K, the\nquery accuracy decreases, and the best results can\nbe obtained when K= 10 in all cases.\nTherefore, in the following experiments, we\nset the key parameters of the proposed LLSH as\nM= 2, L= 30 and K= 10 to pursue optimal\nperformance.\n5.3 Feasibility verification\nIn practical applications, data often have intricate\ndistributions. So, to verify the feasibility of the\nproposed LLSH to replace the traditional E2LSH,\nwe carry conduct experiments on eight datasets\nfrom different distributions (which are detailed\nin Table. 1) to verify whether LLSH can effec-\ntively fit the input-output mapping of E2LSH. The\nresults are shown in Fig. 4(a) and 4(b) for the four\nsynthetic datasets and four real-world datasets,\nrespectively.\nThe results in Fig. 4 show the neural\nnetwork-based LLSH achieves excellent perfor-\nmance when fitting E2LSH. With randomly gen-\nerated datasets, the fitting rates reach 96.42%,\n93.35%, 95.68% and 94.56% on the uniform,\nexponential, normal and lognormal distribution,\nrespectively. The result means that it is feasible\nfor the LLSH to replace E2LSH with such a high\nfitting rate. Surprisingly, the experiments on four\nreal datasets show that the fitting rates can grow\nup to 96.42%, 94.57%, 97.01%, and 95.49% on\nthe Tiny Images, Ann SIFT, Nytimes, and Glove,\nrespectively. The average fitting rate of LLSH on\nreal datasets (95.87%) is more significant than\n\nSpringer Nature 2021 L ATEX template\n10 Article Title\n(a)\n (b)\n(c)\n (d)\nFig. 5 The fitting rate of the various number of neural networks L.\nsynthetic datasets (95.59%), which shows that\nthe LLSH framework can be deployed in physical\nscenarios.\n5.4 Evaluation of basic LLSH\nIn this subsection, we design experiments in\nterms of memory and time consumption to eval-\nuate LLSH’s superiority in the process of hash\nvalue calculation. Here, we use the data withdifferent magnitudes and dimensions and com-\npare the performance of traditional E2LSH and\nits matrix-accelerated version (E2LSH(numpy))\nand the LLSH running in different hardware\n(LLSH(CPU) and LLSH(GPU)).\nFor the data with different magnitudes, we\ndraw data from a uniform distribution with a\nmagnitude range from 1 ×104to 4×105as the val-\nidation dataset. As the results show in Fig. 7(a),\n\nSpringer Nature 2021 L ATEX template\nArticle Title 11\n(a)\n (b)\n(c)\n (d)\nFig. 6 The fitting rate of the various number of nodes in the last layer K.\nLLSH has an absolute superiority on time con-\nsumption, nearly 300 times faster than E2LSH in\ndifferent data magnitudes. Moreover, as the data\nmagnitude increases, the benefits continue to be\nimproved. Compared with the matrix-accelerated\nE2LSH (E2LSH(numpy)), LLSH still has a nearly\n50% boost, and the advantages continue growing\nas the magnitude increases. Simultaneously, the\nLLSH can also be deployed on the GPU to fully\nuse the advantages of new hardware, thus occu-\npying great merit in large-scale data. As shown in\nFig. 7(b), LLSH has an overwhelming superiorityin memory consumption in different data magni-\ntudes, whereas the traditional E2LSH consumes\nmemory about 40 more times than LLSH. The\nmatrix-accelerated E2LSH’s memory also con-\nsumes 1.7 times larger than the proposed LLSH.\nCompared to the CPU version, LLSH costs more\nmemory on the GPU because part of the memory\nis consumed in exchange for a high computation\nspeed.\nFor the dataset with different dimensions, we\ndraw from a uniform distribution with dimensions\nranging from 50 to 500 and keep the data magni-\ntude as 1 ×105to formulate the validation dataset.\n\nSpringer Nature 2021 L ATEX template\n12 Article Title\n(a)\n (b)\nFig. 7 Time & Memory consumption vs. data with different magnitude.\n(a)\n (b)\nFig. 8 Time & Memory consuming vs. data with dimensions.\nFig. 8 illustrates that LLSH is far beyond the\ntraditional E2LSH. As shown in Fig. 8(a), differ-\nent algorithms are insensitive to dimension, and\nthe time consumption increases slowly with the\ndimension increase. The LLSH algorithm main-\ntains significant advantages in any dimension,\nand the LLSH running deployed on GPUs shows\ngreater advantages. Moreover, as shown in Fig.\n8(b), the advantage of memory consumption is\nmore obvious. The LSH consumes about 45 timesless memory than the traditional E2LSH and\nabout 2.8 times less than the matrix-accelerated\nE2LSH.\nAccording to the empirical results mentioned\nabove, we found that LLSH has evident mer-\nits under various data magnitudes and dimen-\nsions. Benefiting from the fast reasoning ability\nof the neural network, the LLSH shows potential\nperformance on time and memory consumption.\nThis ability makes LLSH calculate faster than\n\nSpringer Nature 2021 L ATEX template\nArticle Title 13\n(a)\n (b)\nFig. 9 Query accuracy & Time consumption vs. data with different magnitude\n(a)\n (b)\nFig. 10 Query accuracy & Time consumption vs. data with different dimensions.\nE2LSH when calculating the hash value. More-\nover, LLSH’s advantage is more pronounced on the\nnew advanced computing device (GPU) with its\nparallel computing manner, its time consumption\nhardly increases as the data dimensions grow.\n5.5 Evaluation of ensemble-based\nLLSH\nLLSH aims to improve accuracy and reduce time\nand memory consumption. And ensemble learningcan improve the accuracy of the model well. To\nmake a step forward of the proposed LLSH, we\nintroduce the ensemble strategy to LLSH, where\ndifferent from the basic LLSH is the label for\ntraining is generated by multiple hash algorithms.\nWe compare it with four traditional NN search\nmethods, including Brute, KD-tree, Ball-tree and\nE2LSH.\nIn this experiment, the magnitude of the\ndataset is set from 1 ×104to 5×104, and each\ndataset dimension is set to 20. The results in\n\nSpringer Nature 2021 L ATEX template\n14 Article Title\nFig. 9(a) show that the ensemble-based LLSH has\nobtained higher accuracy than other baselines,\neven higher than the traditional tree-based algo-\nrithm by 2% on average. While compared with the\nE2LSH, it is even more obvious and can achieve\nnearly 10% higher. Regarding time consumption,\nas Fig. 9(b) shows, the ensemble-based LLSH has\nextremely low time consumption; unlike the tra-\nditional tree algorithm, its time consumption will\nincrease exponentially with the amount of data.\nThe ensemble-based LLSH is nearly a hundred\ntimes faster than these tree-based algorithms, and\nthe merits will be more evident with the larger\ndata magnitude. Compared with the E2LSH, the\nimprovement is nearly doubled.\nWe also compared the ensemble-based LLSH\nand these four baselines’ query accuracy and time\nconsumption on the different data dimensions.\nWhere the data dimension is set from 10 to 50\nand the magnitude is set to 104. As shown in\nFig. 10(a), the accuracy of different algorithms\nwill decrease as the dimension increases, but the\nensemble-based LLSH still has the best perfor-\nmance. In terms of time consumption, the results\nin Fig. 10(b) show that the traditional tree algo-\nrithm’s memory consumption will improve as the\ndimension increases, which is called the “curse of\ndimension”. So the tree-based algorithm is unsuit-\nable for high-dimensional data. Besides, compared\nwith E2LSH, the ensemble-based LLSH also shows\nits superiority in memory consumption.\nAs discussed above, compared with the tradi-\ntional hash algorithm, the ensemble-based LLSH\ncan also improve accuracy and reduce time con-\nsumption. In addition, it has more comprehensive\npractical application value because it does not fall\ninto the “curse of dimension”.\n6 Conclusions\nIn this paper, we investigated the LSH-based\nhash algorithms and the booming development\nof machine learning and high computing perfor-\nmance hardware. The traditional LSH-bash hash,\nhowever, is challenging to cope with the increas-\ning dimensional and magnitude of massive data.\nTo bridge this gap, we propose a novel learning-\nbased hash framework, which uses multiple par-\nallel neural networks to simulate the traditional\nhash functions to boost the hashing performance\nconcerning time and memory consumption, andquery accuracy. Extensive empirical results illus-\ntrated the feasibility of the proposed framework,\nand further showed its superiority in the effective-\nness and efficiency of the NN search task with\ntwo implementations, i.e., the basic-based and the\nensemble-based.\nCompliance with Ethical\nStandards\n•Funding This work was partly supported\nby the National Natural Science Foundation\nof China under Grant 62162067 and the Yun-\nnan Province Science Foundation under Grant\nNo.202005AC160007, No. 202001B050076. And\nOpen Foundation of Key Laboratory in Soft-\nware Engineering of Yunnan Province under\nGrant No. 2020SE310. and Open Foundation\nof Engineering Research Center of Cyberspace\nunder Grant No. KJAQ202112013.\n•Competing interests The authors declare\nthat they have no competing of interests.\n•Ethics approval This article does not contain\nany studies with human participants performed\nby any of the authors.\n•Informed consent Not applicable.\n•Consent to participate Not applicable.\n•Consent for publication Not applicable.\n•Data availability The datasets used in this\npaper are available online publically.\n•Code availability Not applicable.\n•Authors’ contributions All authors have\nequally contributed and all authors have read\nand agreed to the manuscript.\nReferences\n[1] Bailey T, Jain AK (1978) A note on distance-\nweighted k-nearest neighbor rules. IEEE\nTransactions on Systems, Man, and Cyber-\nnetics 8(4):311–313\n[2] Behera B, Kumaravelan G (2021) Text doc-\nument classification using fuzzy rough set\nbased on robust nearest neighbor (FRS-\nRNN). Soft Computing 25(15):9915–9923\n[3] Bentley JL (1979) Multidimensional binary\nsearch trees in database applications. IEEE\nTransactions on Software Engineering\n5(4):333–340\n\nSpringer Nature 2021 L ATEX template\nArticle Title 15\n[4] Bhaskar N, Kumar PM (2020) Optimal\nprocessing of nearest-neighbor user queries\nin crowdsourcing based on the whale\noptimization algorithm. Soft Computing\n24(17):13,037–13,050\n[5] Bhattacharya A (2014) Fundamentals of\ndatabase indexing and searching. Crc Press\n[6] Chen S, Shen F, Yang Y, et al (2017)\nSupervised hashing with adaptive discrete\noptimization for multimedia retrieval. Neuro-\ncomputing 253:97–103\n[7] Datar M, Immorlica N, Indyk P, et al (2004)\nLocality-sensitive hashing scheme based on p-\nstable distributions. In: SCG, pp 253–262\n[8] Gan J, Feng J, Fang Q, et al (2012) Locality-\nsensitive hashing scheme based on dynamic\ncollision counting. In: SIGMOD, pp 541–552\n[9] Gong Y, Kumar S, Verma V, et al (2012)\nAngular quantization-based binary codes for\nfast similarity search. In: NIPS, pp 1205–1213\n[10] Gong Y, Lazebnik S, Gordo A, et al\n(2013) Iterative quantization: A procrustean\napproach to learning binary codes for large-\nscale image retrieval. IEEE Transactions on\nPattern Analysis and Machine Intelligence\n35(12):2916–2929\n[11] Goodman JE, O’Rourke J (eds) (2004) Hand-\nbook of Discrete and Computational Geome-\ntry, Second Edition. Chapman and Hall/CRC\n[12] Guttman A (1984) R-trees: A dynamic index\nstructure for spatial searching. In: SIGMOD,\npp 47–57\n[13] Huang Q, Feng J, Zhang Y, et al (2015)\nQuery-aware locality-sensitive hashing for\napproximate nearest neighbor search. Proc\nVLDB Endow 9(1):1–12\n[14] Indyk P, Motwani R (1998) Approximate\nnearest neighbors: Towards removing the\ncurse of dimensionality. In: Vitter JS (ed)\nSTOC, pp 604–613[15] Jin S, Yao H, Sun X, et al (2019) Unsuper-\nvised semantic deep hashing. Neurocomput-\ning 351:19–25\n[16] Kong W, Li W (2012) Isotropic hashing. In:\nNIPS, pp 1655–1663\n[17] Kraska T, Beutel A, Chi EH, et al (2018)\nThe case for learned index structures. In:\nSIGMOD, pp 489–504\n[18] Kulis B, Grauman K (2009) Kernelized\nlocality-sensitive hashing for scalable image\nsearch. In: ICCV, pp 2130–2137\n[19] Lin K, Yang H, Hsiao J, et al (2015) Deep\nlearning of binary hash codes for fast image\nretrieval. In: CVPR, pp 27–35\n[20] Liu W, Wang J, Kumar S, et al (2011)\nHashing with graphs. In: ICML, pp 1–8\n[21] Liu W, Wang J, Ji R, et al (2012) Super-\nvised hashing with kernels. In: CVPR, pp\n2074–2081\n[22] Liu X, Sahidullah M, Kinnunen T (2021)\nLearnable mfccs for speaker verification. In:\nISCAS, pp 1–5\n[23] Lowe DG (2004) Distinctive image features\nfrom scale-invariant keypoints. International\nJournal of Computer Vision 60(2):91–110\n[24] Manku GS, Jain A, Sarma AD (2007) Detect-\ning near-duplicates for web crawling. In:\nWWW, pp 141–150\n[25] Mikolov T, Sutskever I, Chen K, et al (2013)\nDistributed representations of words and\nphrases and their compositionality. In: NIPS,\npp 3111–3119\n[26] Moraleda J (2008) Gregory shakhnarovich,\ntrevor darrell and piotr indyk: Nearest-\nneighbors methods in learning and vision.\ntheory and practice. Pattern Anal Appl\n11(2):221–222\n[27] Nguyen V, Destercke S, Masson M, et al\n(2021) Racing trees to query partial data.\nSoft Computing 25(14):9285–9305\n\nSpringer Nature 2021 L ATEX template\n16 Article Title\n[28] Norouzi M, Fleet DJ, Salakhutdinov R (2012)\nHamming distance metric learning. In: NIPS,\npp 1070–1078\n[29] Oliva A, Torralba A (2001) Modeling the\nshape of the scene: A holistic representation\nof the spatial envelope. International Journal\nof Computer Vision 42(3):145–175\n[30] Sun Y, Wang W, Qin J, et al (2014)\nSRS: solving c-approximate nearest neigh-\nbor queries in high dimensional euclidean\nspace with a tiny index. Proc VLDB Endow\n8(1):1–12\n[31] Tao Y, Yi K, Sheng C, et al (2009) Qual-\nity and efficiency in high dimensional nearest\nneighbor search. In: SIGMOD, pp 563–576\n[32] Torralba A, Fergus R, Weiss Y (2008) Small\ncodes and large image databases for recogni-\ntion. In: CVPR\n[33] Xia R, Pan Y, Lai H, et al (2014) Supervised\nhashing for image retrieval via image repre-\nsentation learning. In: AAAI, pp 2156–2162\n[34] Xiang W, Zhang H, Cui R, et al (2019)\nPavo: A rnn-based learned inverted index,\nsupervised or unsupervised? IEEE Access\n7:293–303",
  "textLength": 41184
}