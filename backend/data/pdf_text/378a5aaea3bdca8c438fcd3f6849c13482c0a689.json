{
  "paperId": "378a5aaea3bdca8c438fcd3f6849c13482c0a689",
  "title": "ACE: A Cardinality Estimator for Set-Valued Queries",
  "pdfPath": "378a5aaea3bdca8c438fcd3f6849c13482c0a689.pdf",
  "text": "ACE: A Cardinality Estimator for Set-Valued Queries\nYufan Sheng\nUniversity of New South\nWales\nSydney, Australia\nyufan.sheng@unsw.edu.auXin Cao\nUniversity of New South\nWales\nSydney, Australia\nxin.cao@unsw.edu.auKaiqi Zhao\nThe University of Auckland\nAuckland, New Zealand\nkaiqi.zhao@auckland.ac.nzYixiang Fang\nThe Chinese University of\nHong Kong, Shenzhen\nShenzhen, China\nfangyixiang@cuhk.edu.cn\nJianzhong Qi\nThe University of\nMelbourne\nMelbourne, Australia\njianzhong.qi@unimelb.edu.auWenjie Zhang\nUniversity of New South\nWales\nSydney, Australia\nwenjie.zhang@unsw.edu.auChristian S. Jensen\nAalborg University\nAalborg, Denmark\ncsj@cs.aau.dk\nABSTRACT\nCardinality estimation is a fundamental functionality in database\nsystems. Most existing cardinality estimators focus on handling\npredicates over numeric or categorical data. They have largely omit-\nted an important data type, set-valued data, which frequently occur\nin contemporary applications such as information retrieval and rec-\nommender systems. The few existing estimators for such data either\nfavor high-frequency elements or rely on a partial independence\nassumption , which limits their practical applicability.\nWe propose ACE, an Attention-based Cardinality Estimator for\nestimating the cardinality of queries over set-valued data. We first\ndesign a distillation-based data encoder to condense the dataset\ninto a compact matrix. We then design an attention-based query\nanalyzer to capture correlations among query elements. To handle\nvariable-sized queries, a pooling module is introduced, followed by\na regression model (MLP) to generate final cardinality estimates.\nWe evaluate ACE on three datasets with varying query element\ndistributions, demonstrating that ACE outperforms the state-of-\nthe-art competitors in terms of both accuracy and efficiency.\nPVLDB Reference Format:\nYufan Sheng, Xin Cao, Kaiqi Zhao, Yixiang Fang, Jianzhong Qi, Wenjie\nZhang, and Christian S. Jensen. ACE: A Cardinality Estimator for\nSet-Valued Queries. PVLDB, 18(1): XXX-XXX, 2025.\ndoi:XX.XX/XXX.XX\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/shengyufan/ACE.\n1 INTRODUCTION\nSet-valued data where the value of an attribute is a set of ele-\nments has emerged as an essential data type in many real-world\napplications, including information retrieval [ 8], recommender\nsystems [ 1], and social networks [ 43]. For example, in a movie\nrecommender system, each movieâ€™s genre is generally associated\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 18, No. 1 ISSN 2150-8097.\ndoi:XX.XX/XXX.XXTable 1: Twitter hashtag dataset\nTweet_ID Hashtags\nğ‘¡1 {Trump, shot}\nğ‘¡2 {Spain, Euros, Yamal}\nğ‘¡3 {Biden, Harris, Trump}\nğ‘¡4 {Harris, Trump, debate}\nğ‘¡5 {JD Vance, Trump}\nğ‘¡6 {Messi, Yamal}\nğ‘¡7 {Messi, Argentina, Copa America}\nwith a set of categories such as sci-fi, action, and comedy. In X\n(http://www.twitter.com), each tweet generally has multiple hash-\ntags. Table 1 shows a toy example, where each row corresponds\nto a tweet and the Hashtags column is a set-valued attribute that\nstores the hashtags of a tweet.\nGiven the prevalence of set-valued data across different domains,\nset queries play a crucial role in efficiently handling multi-valued\nattributes and complex relationships. For example, X offers the\nfunctionality of searching for tweets by a set of keywords or hash-\ntags using operators such as â€œORâ€ and â€œAND.â€ The SQL standard\nincludes support for storing multi-valued data in a single row [ 39].\nSet-valued data and set queries are supported to varying degrees\nby modern DBMSs such as Oracle [ 64], MySQL [ 63], IBM DB2 [ 31],\nSQL Server [ 73], and PostgreSQL [ 68]. For example, MySQL sup-\nports up to 64 distinct elements in a set-valued attribute [ 63]. SQL\nServer enables passing a set as a table-valued parameter. To the best\nof our knowledge, PostgreSQL offers the best support for set-valued\ndata and set queries. It provides three set query predicates: superset\n(@>),subset (<@), and overlap (&&). For example, to evaluate public\ninterest in the recent United States presidential election, we can\ncount the number of tweets containing at least one presidential\ncandidate. This can be achieved using the following set query ğ‘in\nPostgreSQL: SELECT COUNT(*) FROM T WHERE T.Hashtags &&\nARRAY[\"Trump\", \"Harris\"].\nTo identify efficient query execution plans for complex queries,\ncardinality estimation of a query step plays a crucial role since\nit directly influences the efficiency of database query execution.\nCardinality estimation has been extensively studied [ 27,33,34,48,\n67,90], showing its profound impact on the quality of selected\nquery plans [ 25,47]. However, most DBMSs provide only limited\nsupport for optimizing set query execution. To our knowledge,\nPostgreSQL is the only DBMS that offers a built-in estimator for set\noperators but the accuracy is not good enough. In this study, we\n1arXiv:2503.14929v1  [cs.DB]  19 Mar 2025\n\ninvestigate cardinality estimation for queries over set-valued data,\nwhich has not received sufficient attention. While some cardinality\nestimators for set-valued predicates exist [ 40,59,96], they each\nhave significant shortcomings.\nFirst, most studies pay more attention to elements with\nhigh frequency (P1). For example, Yang et al. [ 96] propose two\nsampling-based cardinality estimators for subset queries that aim\nto capture the distribution of high-frequency elements. They suffer\nin accuracy over queries containing low-frequency elements [59].\nSecond, most existing estimators do not capture the correla-\ntion among elements in a query well (P2) , which is crucial for\naccurate estimation. For example, â€œHarrisâ€ and â€œTrumpâ€ appear 2\nand 4 times, respectively, in the example. If we assume indepen-\ndence, the estimated cardinality for ğ‘is2+4=6, while the actual\ncardinality is 4 because ğ‘¡3andğ‘¡4contain both keywords. Korotkov\net al. [ 40] leverage a probabilistic model [ 16] to address the element\ncorrelation issue. However, their model still relies on random sam-\npling of high-frequency elements, thus missing the correlation for\nlow-frequency elements. Recently, Meng et al. [ 59] propose to con-\nvert a set-valued column into multiple categorical columns. They\nthen utilize existing estimators to capture the correlation between\ncolumns. This approach still ignores the correlation among ele-\nments within the same subcolumn, leading to unstable estimation\naccuracy. Besides, this solution relies on that a set query can be\nconverted into categorical sub-queries, which does not support all\nset queries such as the overlap query.\nThird, existing studies mainly focus on capturing data distribu-\ntion and overlook the valuable insights in historical query\nworkloads (P3). The cardinalities of two similar queries can differ\non the operators used, even when their elements are identical. For\nexample, the cardinality of the example query ( ğ‘=ğ‘‡&&{\"Harris\",\n\"Trump\"}) is 4 while the cardinality of a similar superset query\n(ğ‘â€²=ğ‘‡@>{\"Harris\", \"Trump\"}) is 2. Learning the data distribution\nonly is insufficient for accurate predictions across various query\ntypes. Set Transformer [ 46] processes input sets using an attention\nmechanism to capture the correlations between elements, mak-\ning it a potential candidate for a query-driven estimator. However,\nour experiments in Section 7 show that its accuracy is unstable\nand sometimes performs much worse than data-driven estimators,\nbecause it is impractical to represent all possible combinations of\nelements given limited training data. Thus, pure query-driven meth-\nods that treat the problem as a supervised learning task also have\na severe issue: their accuracy highly depends on the quantity and\nquality of the training data (i.e., known query workload) [25].\nTo address the issues above, we propose ACE, an Attention-based\nCardinality Estimator for queries over set-valued data. As depicted\nin Figure 1, ACE leverages information from both the data and the\nquery workload to address P3.\nTo address P1, we design a distillation-based data encoder to\ngenerate a compact dataset representation. We construct a bipartite\ngraph that models the relationships between the set elements and\ntheir corresponding sets. This graph serves as the foundation for\nthe subsequent aggregation step. The aggregator module synthe-\nsizes a set embedding by integrating information from all elements\nof the set, ensuring that even low-frequency elements are not un-\nderrepresented. Once the set embeddings are computed, they are\nconcatenated to form the initial representation of the full dataset\nData\nS = Distillation-based Encoder\nAggregatorBipartite Graph\ne 1e 2e 3 e Ms 1s 2 s N\n= S o( )\nDistiller S c( )\nAttention-based Analyzer= q''Â Pooling\nQuery\nS @> MLPCard\nEstsCorrelator\nCross\nAttention\nSelf\nAttentionFigure 1: Overview of ACE.\nTable 2: Properties of different estimators\nMethodQuery supported Data-\ndrivenQuery-\ndrivenLow-frequency\nelementsElement\ncorrelation Superset Subset Overlap\nPostgreSQL âœ“ âœ“ âœ“ âœ“Ã—Ã—Ã—\nSampling âœ“ âœ“ âœ“ âœ“Ã—Ã—Ã—\nOT-S [96] âœ“ âœ“ âœ“ âœ“Ã—Ã—Ã—\nST [59] âœ“ âœ“ Ã— âœ“Ã— âœ“ @âœ“\nSTH [59] âœ“ âœ“ Ã— âœ“Ã— âœ“ @âœ“\nACE (Ours) âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“\n(i.e., a database table of set-valued data), denoted as ğ‘ºğ’. For large\ndatasets, this representation has a high dimensionality, posing sig-\nnificant challenges for downstream learning tasks. To address this\nissue, a distiller module is designed to produce a more compact\nrepresentation, ğ‘ºğ’„, which compresses the original representation\nwith a fixed ratio while preserving as much information as possible.\nNext, we design a correlator module to capture element corre-\nlations and address P2. For each query in a given workload, we\napply a cross-attention mechanism to generate the query element\nembeddings using the data representation obtained from the pre-\nvious step. The computational complexity of the attention mecha-\nnism [ 5,11,75,81] scales with the size of the input, i.e., the number\nof rows in the data representation and the number of query ele-\nments, which emphasizes the necessity of the distillation step. Then,\nwe utilize the self-attention mechanism to capture the correlation\nbetween the learned latent representation of query elements. It is\nnoteworthy that set-valued queries have varying sizes, bringing\nanother challenge for the learning-based estimator. We address the\nchallenge with a pooling module to generate a fixed-sized query\nembedding ğ’’and finally a linear regression model to map the em-\nbedding to a cardinality estimation.\nTable 2 summarizes the novelty of our estimator ACE compared\nwith existing set-valued query cardinality estimators. Please note\nthat we use the symbol @âœ“because ST and STH can only capture\ncorrelations between elements in different columns. Overall, we\nmake the following contributions:\nâ€¢We propose ACE, a learning-based cardinality estimator for\nqueries over set-valued data, exploiting both the data and\nquery workload distributions.\nâ€¢We design a distillation-based data encoder to generate a\ndataset compact representation, reducing the dimensional-\nity while retaining key information.\n2\n\nâ€¢We propose an attention-based query analyzer that captures\ncorrelations among query elements, followed by a pooling\nmethod to address the issue of variable-sized queries.\nâ€¢We compare ACE and the state-of-the-art estimators on real-\nworld datasets and query workload. The results show that\nACE outperforms the SOTA estimators by up to 33.9 Ã—in\nterms of accuracy while offering a stable estimation latency.\nAdditionally, the integration of ACE and PostgreSQL also\nspeeds up the end-to-end execution for complex queries.\n2 PRELIMINARIES\nWe start with our problem statement and technical background for\nour proposed model. Table 3 lists the frequently used notation.\nTable 3: Frequently used notation\nNotation Meaning\nğ‘†={ğ‘ ğ‘–}ğ‘\nğ‘–=1The set-valued dataset\nğ¸={ğ‘’ğ‘—}ğ‘€\nğ‘—=1The element universe of the dataset\nğ‘‘ The dimension of the embedding\nğµğ‘‘/ğµğ‘ The batch size of data/query\nğ‘Ÿ The distillation ratio\nğ‘›distill The number of layers for the distillation model\nğ‘›cross/ğ‘›self The number of layers for the cross/self attention\nğ’”/ğ’†/ğ’’ The set/element/query embedding\nğ‘ºğ’/ğ‘ºğ’„ The original/distilled matrix of the dataset\nğ‘¸/ğ‘²/ğ‘½ The queries/keys/values of the attention mechanism\n2.1 Problem Statement\nDefinition 1 ( Set-Valued Query ).A set-valued query ğ‘=\n(operator,literal)is a predicate over the set-valued data, represented\nby an operator-literal pair. To be consistent with PostgreSQL, operator\ncan be the superset ( @>), subset ( <@), or overlap ( &&)1operator, while\nliteral is a subset of ğ¸.\nFor example, ğ‘=ğ‘†@>{ğ‘’1,ğ‘’3,ğ‘’7}is to find the sets over ğ‘†, each\nof which is a superset of {ğ‘’1,ğ‘’3,ğ‘’7}.\nProblem . The study aims to propose an estimator that can accu-\nrately and efficiently predict the cardinality of a set query without\nexecuting the query.\n2.2 Attention Mechanism\nThe attention mechanism was originally envisioned as an enhance-\nment to the encoder-decoder Recurrent Neural Network (RNN) in\nsequence-to-sequence applications [ 5]. In neural networks, atten-\ntion is a technique that aims to mimic human cognitive attention,\nand its motivation is that a network should focus on the important\nparts of the data rather than treating all data equally. It employs\nan attention function to decide which part of the data should be\nemphasized. This function maps a query and a collection of key-\nvalue pairs, assigns weights by computing the similarity between\neach pair of the query and a key using some metric, and calculates\nthe weighted sum of values as its output. Therefore, compared to\nother neural networks, the attention mechanism can achieve better\ninterpretability and have higher representative abilities.\n1PostgreSQL Array Functions and Operators: https://www.postgresql.org/docs/9.1/\nfunctions-array.htmlIn this study, we use the standard Scaled Dot-Product Atten-\ntion [ 81], called Att. The keys and values are mapped to matrices ğ‘²\nandğ‘½, of dimensions ğ‘›Ã—ğ‘‘ğ‘˜andğ‘›Ã—ğ‘‘ğ‘£, whereğ‘›,ğ‘‘ğ‘˜, andğ‘‘ğ‘£denote\nthe size of the original input and the dimensions of the matrices\nğ‘²andğ‘½. A query is first converted to an ğ‘šÃ—ğ‘‘ğ‘˜matrix ğ‘¸, where\nğ‘šindicates the size of the query and is used as input to the dot\nproduct. If ğ‘¸is different from ğ‘²andğ‘½, it is called cross-attention.\nOtherwise, it is self-attention. Since a large dot product result often\nleads to the vanishing gradient problem, the Attfunction divides\nthe dot product by the factorâˆšï¸\nğ‘‘ğ‘˜. The process consists of calcu-\nlating the dot products of ğ‘¸with all keys ğ‘², dividing each byâˆšï¸\nğ‘‘ğ‘˜,\napplying the softmax function to obtain the weights, and obtaining\nthe output by multiplying the weights and the values ğ‘½.\nAtt(ğ‘¸,ğ‘²,ğ‘½)=softmax \nğ‘¸Â·ğ‘²T\nâˆšï¸\ndk!\nğ‘½\nNext, we adopt the multi-head attention mechanism [ 81], which\nlinearly projects the queries, keys, and values using â„different linear\nprojections and then computes the Attfunction in parallel. The\nindependent attention outputs are then concatenated and linearly\ntransformed into the expected dimension. Compared with single-\nhead attention, this approach processes different projected spaces\njointly, thus capturing complex patterns from different perspectives.\nMultiHead(ğ‘¸,ğ‘²,ğ‘½)=concat(head 1,Â·Â·Â·,head h)Wo,\nwhere head i=Att(ğ‘¸WQ\ni,ğ‘²WK\ni,ğ‘½WV\ni).\n3 OVERVIEW OF ACE\nThe structure of ACE is shown in Figure 1. The encoder (Section 4)\ngenerates a compact data representation, while the analyzer (Sec-\ntion 5) captures correlations between query elements by learning\nfrom both the queries and the underlying data.\nAs in previous studies [ 30,50], the first task is representing the\ndataset properly. Sets in Sare combinations of elements, and we can\nnaturally represent a set as the concatenation of the embeddings\nof its elements. However, this representation is incompatible with\nneural networks due to the variable sizes of sets. We need to convert\nvariable-sized sets into fixed-sized vectors. Traditional methods,\nincluding padding and truncation, have limitations. For example,\npadding causes storage overheads and increases time complexities.\nInstead, we propose to learn the representations of sets. Assuming\nthe number of elements that can occur in sets is ğ‘€, there are 2ğ‘€âˆ’1\npossible sets, making it hard to design one model to represent all\nthe sets. We propose to construct a bipartite graph to model the\ndependencies between elements and sets and to learn an aggregator\nto obtain a set embedding ğ’”by aggregating the information of each\nelement ğ’†in the set. Thus, the underlying dataset is represented by\na matrix ğ‘ºğ’where each row is the embedding of a set.\nNext, we aim to learn the representation of the query elements\nfrom data. This motivates us to employ a cross-attention mechanism\nto discover the relation between the underlying dataset and each\nquery element. The original data matrix ğ‘ºğ’cannot be used directly\nin the attention framework for large-scale datasets. For instance,\ntraining on our smallest dataset GNrequires 42GB of GPU memory,\neven with a batch size of 1. Thus, we design a distiller module to\nobtain a matrix ğ‘ºğ’„that preserves the essential knowledge in ğ‘ºğ’.\n3\n\nAs shown in the example query in Section 1, it is crucial to\ncapture correlations between query elements, influenced by the\nunderlying data, to get accurate estimates. Thus, we propose a\ncorrelator module to achieve this. We first leverage a data-query\ncross-attention to measure the relevance between each query el-\nement and the underlying data. We then adopt a self-attention\nmechanism to capture the correlations between the query elements.\nIn addition, the attention mechanism is suitable for dealing with\nsets as the order of set elements does not affect the output.\nTo handle the variable-size queries, we utilize a pooling module\nto derive a fixed-sized vector. This vector extracts pertinent infor-\nmation from the output of the self-attention mechanism and adapts\nits focus based on the operator type of a set-valued query.\nOffline training. The training process is divided into two distinct\nphases. In the first phase, we employ an unsupervised learning\napproach to train the data encoder, requiring only a small subset of\nthe dataset. In the second phase, we utilize a supervised learning\nmethod to train the query analyzer, using both the query embed-\ndings and the distilled matrix generated by the encoder as input.\nDuring this stage, true query cardinalities serve as ground-truth\nlabels. The entire training procedure leverages stochastic gradient\ndescent (SGD) optimization [70].\nOnline estimation. In the pre-processing phase, a well-trained\ndata encoder can distill the entire dataset into a compact data matrix.\nWhen a new query arrives, we can only utilize the learned query\nanalyzer to estimate the cardinality efficiently, taking the query\nelement embeddings and the matrix as input.\n4 DATASET FEATURIZATION\nWhen representing the set-valued dataset, PostgreSQL uses his-\ntograms to approximate the distribution of the underlying data. A\nrecent study [ 59] converts a set into a smaller number of numer-\nical values, models the factorization problem as a graph coloring\nproblem, and proposes a greedy method to address the NP-hard\nproblem. However, this method cannot measure the correlation\nbetween the elements in the same partition. Recently, machine\nlearning techniques have opened the opportunity to learn mod-\nels that outperform many traditional methods [ 42,102]. Thus, we\naim to learn a model that encodes each set and generates the data\nrepresentation. We also design a distillation model such that the\nfeaturization matrix can be effectively used in the attention mecha-\nnism. The details are given in Sections 4.1 and 4.2.\n4.1 Set Representation\nWe follow the setting used in existing studies [ 40,45] where the\nelement universe ğ¸is finite and fixed, meaning each set consists of\nknown elements. To partition a set into several clusters, the existing\nwork [ 59] builds an undirected graph based on the underlying data,\nwhere edges connect two elements that appear in the same set\nand uses a greedy algorithm to partition elements into ğ‘˜clusters.\nTaking the scenario of ğ‘˜=3as an example, the algorithm proceeds\nin two phases. In the first stage, it builds a graph and uses the largest\nfirst algorithm [ 41] to obtain initial partitions, ensuring that no\nelements within a partition are contained in the same set. In the\nsecond stage, the algorithm greedily merges partitions to produce\nthe result clusters, as illustrated in Figure 2a, where elements of thesame color belong to the same cluster. Although they utilize the\nexisting works [ 30,97] to capture the correlation among clusters,\nthe correlation between the elements within the same cluster, such\nas \"Trump\" and \"Harris\", cannot be measured. Unlike the previous\nmethod, we represent the dataset as embeddings so that we can\nleverage the machine learning method to capture the correlation\nbetween elements appeared in a query.\nJD\nVance\nTrumpCopa\nAmericaArgentina\nYamal\nEuros Spainshot\nHarrisdebate\nBidenMessi\n(a) Element graph\nr1\nr3\nr5shot\nTrump\nHarrisBiden\ndebate\nJD\nVancer4r2\nr7r6Spain\nEuros\nYamal\nMessi\nArgentina\nCopa\nAmerica (b) Element-set graph\nFigure 2: Graph construction approaches.\nHowever, proposing such a model is non-trivial. As analyzed\nin Section 3, the number of combinations of ğ‘€elements equals\n2ğ‘€âˆ’1, making it challenging to learn a model that considers all\npossibilities. Motivated by existing works [ 10,91,105], we build a\nbipartite graph to model the correlation between elements and sets,\nas shown in Figure 2b. In this graph, an edge connects an element\nto a set if the element appears in that set. Now, the problem shifts\nto representing a set ğ‘ when its comprised elements are known.\nFollowing this motivation, we propose an aggregator module\nthat takes element embeddings as input. A naÃ¯ve approach is to\nadopt pooling methods directly. However, pooling methods discard\nconsiderable amounts of information. For example, max-pooling\nonly retains the highest value, ignoring the rest. Additionally, pool-\ning methods are non-trainable operations that cannot adapt to\nvarious data, limiting their ability to extract complex representa-\ntions and potentially leading to suboptimal feature compression.\nPrior studies [ 24,86,99,104] show that the Multi-Layer Perceptron\n(MLP) [ 28] offers a simple yet effective approach to compute feature\nrepresentations for each input. Thus, we utilize an MLP to aggre-\ngate the information from elements before performing pooling. The\nprocess of generating the set embedding ğ’”is described as follows\n(where ğ’†ğ’‹is the embedding of the element ğ‘’ğ‘—):\nğ’”=Pool\u0000\b\nMLP\u0000ğ’†ğ’‹\u0001,âˆ€ejâˆˆs\t\u0001\nGiven the lack of inherent order among elements, any symmetric\nvector function can be used as the pooling operator. Following the\nprior work [ 24], we utilize the simple single-layer architecture with\nthe mean-pooling operator.\n4.2 Dataset Distillation\nThe above aggregation model can encode each set as a 1Ã—ğ‘‘em-\nbedding and we can concatenate them together to obtain an ğ‘Ã—ğ‘‘\nmatrix ğ‘ºğ’representing the dataset, where ğ‘denotes the number of\nsets in the dataset. Motivated by the existing work [ 50], we aim to\nuse the attention mechanism to link query elements with the dataset\nrepresentation. However, when dealing with large-scale datasets,\ndirectly using ğ‘ºğ’is unrealistic. As introduced in Section 2.2, the\ncorrelation in the attention mechanism is captured by the dot prod-\nuct of ğ‘¸andğ‘²ğ‘», meaning that the complexity of the mechanism\n4\n\nis proportional to the size of the dataset matrix. Since the size of\nreal-world dataset is usually larger than 106, we aim to synthe-\nsize a small dataset such that models trained on it achieve high\nperformance on the original large dataset.\nA naÃ¯ve method is to draw a small sample from the original data\nmatrix. However, the resultant matrix is lossy, and the performance\ndepends heavily on the sampling quality. Recently, the problem of\ndataset distillation has been studied in the field of computer vision.\nExisting works [ 49,85,106] introduce different algorithms that\ntake as input a large real dataset to be distilled and output a small\nsynthetic distilled dataset, which is evaluated via testing models\ntrained on this distilled dataset on a separate real dataset. However,\nthese methods cannot be adopted to solve our problem because the\ndataset studied in previous works always has the label information\nand they distill the data of the same class into a small dataset. For\nexample, the image dataset can be represented as ğ‘‡={\u0000ğ‘¥ğ‘”,ğ‘¦ğ‘”\u0001}ğº\nğ‘”=1\nwhereğºdenotes the number of training images, ğ‘¥ğ‘”andğ‘¦ğ‘”denote\nthe image and its corresponding label, respectively. Then, they pro-\npose various approaches that can compress thousands of training\nimages into just several synthetic distilled images (e.g. one per class)\nand achieve comparable performance with training on the original\ndataset. However, our dataset representation lacks the essential\nlabel information. Therefore, we aim to propose a distillation model\nthat can compress the large unlabeled dataset.\nCross\nAttentionFFN\nK,VQ Cross\nAttentionFFNQ Cross\nAttentionFFN ...\nK,V K,VWeights optionally shared\nQ\nFigure 3: Distillation model.\nAs shown in the previous work [ 81], the self-attention mech-\nanism allows the model to aggregate information across tokens.\nBuilding on this, we aim to utilize the attention mechanism to com-\npress the dataset ğ‘ºğ’. Since self-attention produces an output matrix\nwith the same dimensions as its input, we propose an iterative\ncross-attention model, as illustrated in Figure 3.\nEach cross-attention block consists of a single attention layer\nğ´ğ‘¡ğ‘¡, followed by a feed-forward neural network ğ¹ğ¹ğ‘ . Initially, we\nsample a set of embeddings as the initial value ğ‘º0ğ’„. Then, we project\nthe distilled matrix to the query ğ‘¸while mapping the original\nmatrix to the key ğ‘²and the value ğ‘½. Note that we adopt residual\nconnections [29] and layer normalization [4] in our framework.\nËœğ‘ºğ’Š\nğ’„=LayerNorm\u0010\nğ‘ºiâˆ’1\nğ’„+Att\u0010\nğ‘ºiâˆ’1\nğ’„,ğ‘ºğ’,ğ‘ºğ’\u0011\u0011\n,\nğ‘ºğ’Š\nğ’„=LayerNorm\u0010\nËœğ‘ºğ’Š\nğ’„+FFN\u0010\nËœğ‘ºğ’Š\nğ’„\u0011\u0011\nBy iteratively applying the cross-attention mechanism, our model\ncan extract useful information from the original matrix while re-\nducing the size of the matrix simultaneously. This model can also\nbe seen as performing the clustering of the inputs with the la-\ntent positions as cluster centers, leveraging highly asymmetric\ncross-attention layers. Following previous works [ 35,93], we share\nweights between each instance of the cross-attention module (ex-\ncept the first one) for parameter efficiency. Consequently, we utilize\nthe smaller ğ‘ºğ’„as input of the following query analyzer.4.3 Encoder Training\nThe dataset encoder comprises two distinct modules, each with an\noptimization objective. To address this, we propose a combined loss\nfunction that integrates both objectives, allowing training the two\nmodules simultaneously, in line with previous works [26, 56, 77].\nThe aggregation module aims to generate the set embeddings\nby integrating the information from elements. To achieve this, we\npredict whether there is an edge connecting the set and the element\nbased on their embeddings. Following the previous work [ 91], we\nuse the cross entropy (CE) [ 6] as the loss function to maximize log\nprobabilities for one-hop structure learning.\nğ¿CE=âˆ‘ï¸\nğ‘–\u0012\nâˆ’ğ’”ğ’Šğ’†ğ‘»\nğ’‹+log\u0012âˆ‘ï¸\nekâˆˆN(si)âˆªejğ’”ğ’Šğ’†ğ‘»\nğ’Œ\u0013\u0013\n,\nwhereğ‘’ğ‘—âˆˆğ‘ ğ‘–andğ‘(ğ‘ ğ‘–)={ğ‘’ğ‘™|ğ‘’ğ‘™âˆ‰ğ‘ ğ‘–}denote the positive sample\nand the collection of negative samples, respectively.\nRegarding the distillation module, the objective is to compress\nthe dataset while persevering the knowledge as much as possible.\nMotivated by the previous work [ 101], we use the maximum mean\ndiscrepancy (MMD) [ 19] as the loss function. The primary purpose\nof MMD is to determine whether two distributions are similar by\ncomparing their samples. This is achieved by mapping the samples\nto a high-dimensional feature space using a kernel function and\nthen computing the mean distance between these features. This\nfunction is particularly useful in transfer learning [ 57], which needs\nto quantify the difference between two sets of data.\nğ¿MMD=1\nğ‘›2Ã\nğ‘–,ğ‘—ğ‘˜(ğ‘ºğ’Š\nob,ğ‘ºğ’‹\nob)+1\nğ‘š2Ã\nğ‘–,ğ‘—ğ‘˜(ğ‘ºğ’Š\ncb,ğ‘ºğ’‹\ncb)âˆ’2\nğ‘›ğ‘šÃ\nğ‘–,ğ‘—ğ‘˜(ğ‘ºğ’Š\nob,ğ‘ºğ’‹\ncb),\nwhereğ‘˜is a kernel function (e.g., the Gaussian kernel) while ğ‘›and\nğ‘šdenote the size of batch data ğ‘ºobandğ‘ºcb, respectively.\nTo train these models, we first split the underlying data based\non the batch size ğµğ‘‘. Then, the data batches are divided into two\nparts, the training dataset and the testing dataset. Since the element\nuniverse is finite and fixed, we create the fixed representation with\ndimensionğ‘€Ã—ğ‘‘, where each row represents one element. For each\ntraining batch, we propose a hybrid training method that minimizes\nan overall loss function ğ¿, combining ğ¿ğ¶ğ¸andğ¿ğ‘€ğ‘€ğ· . To prevent\noverfitting, we also use the L2 regularization technique.\nmin\u0000ğ¿+ğœ†ğ¿2reg\u0001=min\u0010\nğ¿CE+ğ¿MMD+ğœ†âˆ¥Î˜âˆ¥2\u0011\n,\nwhereğœ†is the hyper-parameter to adjust the weight.\n5 ANALYZER DESIGN\nGiven a query ğ‘and the distilled dataset ğ‘ºğ’„, ACE discovers the\nrelations between query elements and data. Then, we capture the\ncorrelation between query elements. The key challenge is the at-\ntention mechanism [ 81] used in ACE. To handle the variable-size\ninput, we also propose an attention-based pooling method. Finally,\nwe employ a linear regression model to predict the cardinality of ğ‘,\ntaking the fixed-size embedding as the input. Detailed explanations\nare provided in Sections 5.1 and 5.2.\n5.1 Element Correlation\nBefore estimating the cardinality of a query ğ‘, we need to obtain the\nquery representation. A naÃ¯ve method is to leverage the trained ag-\ngregator to integrate the information from query elements. Because\n5\n\nthe element embeddings are randomly initialized and fixed in the\ndata encoder, these initial embeddings lack meaningful information.\nAdditionally, the aggregator cannot capture the correlation between\nquery elements{ğ‘’ğ‘–|ğ‘’ğ‘–âˆˆğ‘}. Therefore, we need to propose another\nmethod to complete the task.\nConsidering Figure 2b, each element can also be represented as\nthe collection of sets containing it. Thus, we propose to learn the\nquery representation from the underlying data. A simple method is\nto flatten ğ‘ºğ’„into a vector and concatenate the vector with embed-\ndings of query elements. Then, the vector combining data and query\ninformation can be fed into an MLP to generate the query embed-\nding. However, we observe that an element has a stronger relation\nto the sets containing it. Therefore, we leverage the cross-attention\nmechanism, which can pay more attention to these sets and learn\nbetter embeddings of query elements based on the distilled dataset\nrepresentation.\nAfter obtaining the embeddings, we need to capture the cor-\nrelations hidden in the embeddings. A simple approach is to use\nan MLP to learn the correlations dynamically. However, an MLP\napplies the same transformation to all inputs regardless of their\nimportance and struggles to capture complex correlations. Moti-\nvated by the previous work [ 50,81], we utilize the self-attention\nmechanism, taking these latent embeddings as the input, to capture\nthe correlations between elements.\nMulti-head \nCross AttentionAdd & NormFeed forward networkAdd & Norm\nQ KVMulti-head \nSelf AttentionAdd & NormFeed forward networkAdd & Norm\nQKV\nFigure 4: Hybrid attention framework.\nIn the first stage, we initialize the query embedding by stacking\nquery element embeddings and update the query embedding by\nconsidering the information from the dataset. We employ ğ‘›ğ‘ğ‘Ÿğ‘œğ‘ ğ‘ \nstacked attention layers to capture the correlations between the ini-\ntial query embedding ğ’’â€²â€²\n0and the distilled data matrix ğ‘ºğ’„. Each layer\nis identical and includes two sub-layers. The first is the multi-head\ncross-attention sub-layer ğ´ğ‘¡ğ‘¡ğ‘where ğ‘ºğ’„is used as ğ‘²andğ‘½while\nğ‘¸uses ğ’’â€²â€²\n0or the output of the last layer. On top of ğ´ğ‘¡ğ‘¡ğ‘, the feed-\nforward sub-layer ğ¹ğ¹ğ‘ uses stacked fully connected networks and\nnonlinear activation functions, e.g., GeGLU [ 74], to map Ëœğ’’â€²â€²\nğ’Šinto the\nlatent representation ğ’’â€²â€²\nğ’Š. To prevent performance degradation and\nease the model training, we also employ a residual connection [ 29],\nfollowed by layer normalization [4].\nËœğ’’â€²â€²\nğ’Š=LayerNorm\u0000ğ’’â€²â€²\niâˆ’1+Attğ‘(ğ’’â€²â€²\niâˆ’1,ğ‘ºğ’„,ğ‘ºğ’„)\u0001,\nğ’’â€²â€²\nğ’Š=LayerNorm\u0000Ëœğ’’â€²â€²\nğ’Š+FFN(Ëœğ’’â€²â€²\nğ’Š)\u0001\nThe attention sub-layer establishes a bridge between query ele-\nments and data. It obtains element representations by aggregating\nCross\nAttentionFFN\nVQ Add &\nNormAdd &\nNorm\nAverage\nPoolKFigure 5: Attention pooling.\ninformation from the most relevant parts of data embeddings ğ‘ºğ’„,\nwhile diminishing others. The effect of particular attention can be\nrealized through learnable parameters of different layers.\nIn the second stage, we discover and measure the correlation\nbetween query elements. We stack ğ‘›ğ‘ ğ‘’ğ‘™ğ‘“ identical attention lay-\ners. Similar to the first stage, each layer consists of a multi-head\nattention sub-layer and a feed-forward sub-layer. Also, residual\nconnections are employed, followed by layer normalization. Unlike\nthe first stage, this self-attention sub-layer ğ´ğ‘¡ğ‘¡ğ‘ takes the same in-\nputs of keys, values, and queries. They are either the output of the\nfirst module, denoted as ğ’’â€²\n0, or the previous stacked layers.\nËœğ’’â€²\nğ’Š=LayerNorm\u0000ğ’’â€²\niâˆ’1+Attğ‘ (ğ’’â€²\niâˆ’1,ğ’’â€²\niâˆ’1,ğ’’â€²\niâˆ’1)\u0001,\nğ’’â€²\nğ’Š=LayerNorm\u0000Ëœğ’’â€²\nğ’Š+FFN(Ëœğ’’â€²\nğ’Š)\u0001\nAs introduced above, ğ’’â€²\n0can be considered as new embeddings\nfor query elements, which integrate the information from the un-\nderlying dataset. The use of the same input for the keys, values, and\nqueries makes each element in the output set of a layer attend to all\noutputs in the previous layer and thus attend to all elements. More\nimportantly, the self-attention sub-layer quantitatively â€™measuresâ€™\nthe relevance between a pair of elements, enabling the effective\ndiscovery of implicit correlations between elements. Thus, the in-\nformation from the data and query is encoded into the final output\nembedding ğ’’â€²that will be processed later.\n5.2 Attention Pooling\nThrough the hybrid attention framework, the query embedding\nğ’’â€²not only links the query with the underlying dataset but also\nincludes the correlation information of query elements, which can\nbe used for the cardinality estimation task. A model for set-input\nproblems should satisfy two fundamental requirements. First, it\nshould be permutation invariant, that is, the output of the model\nshould not change under any permutation of the elements in the\ninput set, which is inherently satisfied by our hybrid attention\nframework. Second, such a model should be able to process input\nsets of any size. For example, if the literal of a query is composed of\nğ‘˜elements, the dimension of the output query embedding ğ’’â€²will\nbeğ‘˜Ã—ğ‘‘. Generally, three methods address this problem â€“ pooling,\npadding, and truncation. Pooling effectively reduces input size\nby aggregating information from local regions, thereby reducing\ncomputational load [ 17]. In contrast, padding increases input size,\nwhile truncation may result in the loss of important information.\nAdditionally, pooling operations introduce a degree of translation\ninvariance, enhancing the modelâ€™s robustness to changes in the\ninput position [ 78]. Motivated by prior works [ 14,79], we employ an\nattention-based pooling module to generate the fixed-sized query\nembedding ğ’’for predicting the corresponding cardinality.\n6\n\nAs demonstrated in previous work [ 96], query elements with\nvarying frequencies can have opposite impacts on the cardinal-\nity of a query depending on the operator type. For example, con-\nsider two queries composed of the same elements. In a superset\nquery, which aims to find sets containing all specified elements, low-\nfrequency elements have a stronger influence on cardinality than\nhigh-frequency elements. Conversely, in an intersection query, the\nresultant sets include at least one of the specified elements, mean-\ning that high-frequency elements have a greater impact on the\ncardinality. Thus, the frequency information is first appended to\nğ’’â€², which generates the (ğ‘‘+1)-dim embedding ğ’’â€²\nğ’‡, and then the\nattention pooling module takes a random initialized embedding\nğ’’0andğ’’â€²\nğ’‡as inputs. After accessing ğ’’, the output of the pooling\nlayer, we use a simple linear regression layer ğ¿ğ‘…to predicate the\ncardinality estimation ğ‘. It is noteworthy that we use the logarithm\nof the frequency as the appending information and modify the con-\nventional residual connection inspired by the prior work [ 55,87].\nFigure 5 shows the framework of this module.\nËœğ’’=LayerNorm\u0010\nAvgPool(ğ’’â€²)+Attğ‘(ğ’’0,ğ’’â€²\nğ’‡,ğ’’â€²\nğ’‡)\u0011\n,\nğ’’=LayerNorm(Ëœğ’’+FFN(Ëœğ’’)),\nğ‘=LR(ğ’’)\n5.3 Analyzer Training\nFine-tuning the parameters of the query analyzer requires a train-\ning dataset of which each record is a 3-tuple ( ğ‘ğ‘–,ğ‘ºğ’„,ğ‘ğ‘–), whereğ‘ğ‘–is\nthe set-valued query consisting of ğ‘˜elements, and ğ‘ğ‘–denotes the\ntrue cardinality of ğ‘ğ‘–. In practice, collecting the training dataset\nis not difficult, and we only need to collect the feedback of exe-\ncuted queries. The training dataset is split into batches to train our\nanalyzer.\nSince each module in the analyzer is differentiable, we train the\nanalyzer in an end-to-end manner. Here, we use the weighted mean\nQ-error function WMQ(Â·)as the loss function, which takes input\nof the batch cardinality estimates ğ’„â€²\nğ’ƒand the true cardinalities ğ’„ğ’ƒ\nas well as their weights ğ’˜ğ’ƒwith batch size ğµğ‘.\nWMQ(ğ’„â€²\nğ’ƒ, ğ’„ğ’ƒ)=âˆ‘ï¸ğµğ‘\nğ‘–=1ğ‘¤ğ‘–âˆ—max{1,ğ‘â€²\nğ‘–\nğ‘ğ‘–,ğ‘ğ‘–\nğ‘â€²\nğ‘–},\nwhereğ‘¤ğ‘–is proportional to logğ‘ğ‘–, i.e.ğ‘¤ğ‘–=logğ‘ğ‘–Ã\nğ‘—logğ‘ğ‘—. We use the\nweight in the loss function because it is usually beneficial to em-\nphasize the queries with larger true cardinalities [50].\n6 ACE UNDER UPDATES\nIn this section, we first discuss how to leverage our ACE on dynamic\ndata. Then, we analyze its benefits compared with the state-of-\nthe-art baseline methods. Notably, we use the same setting when\nworking with dynamic data, that is, the element universe is finite\nand fixed. The update of the element universe is left for future work.\nACE on dynamic data. We focus on dynamic data involving inser-\ntions and deletions because one update is equivalent to one deletion\nfollowed by one insertion. Based on the structure of ACE, we take\na two-stage approach to accommodate dynamic data â€“ (1) dataset\nrepresentation update and (2) query cardinality estimation.Given a batch of tuples to be inserted, we first use the aggregator\nto represent them. Then, we sample the learned tuple matrix and\nregard sampled embeddings as the initial distilled matrix. Next, we\nleverage the distiller to update the distilled matrix.\nWhen deleting tuples, considering that our original dataset is\nsplit into a collection of dataset slices based on the batch size ğµğ‘‘,\nwe locate the affected slices and only need to update their cor-\nresponding distilled matrix by leveraging the trained encoder, as\nmotivated by the previous work [ 7]. Additionally, we need to update\nthe frequency of elements that are affected by the update.\nAfter obtaining the new distilled matrix, we can feed it along\nwith the embeddings of the queried elements into the trained hy-\nbrid attention framework to derive the element embeddings that\nlink the query with the updated dataset and capture the implicit\ncorrelation between elements. Subsequently, we incorporate the\ncurrent frequency information of each element and utilize the at-\ntention pooling as well as the linear regression models to get the\ncardinality estimate for the new dataset.\nComparison and analysis. When working with dynamic data,\nPostgreSQL reconstructs the affected histograms to approximate\nthe distribution of the updated dataset. However, it still relies on the\n(partially) independent assumption, which limits its accuracy. The\nupdate process of traditional sampling methods involves sampling\ntuples from the inserted dataset or deleting tuples from the existing\nsamples when encountering insertions or deletions, respectively,\nleading to their performance heavily depending on the quality of\nthe resultant samples. Additionally, they still pay more attention to\nelements with high frequency and achieve poor performance on\nthe query with low-frequency elements.\nOne prior work [ 96] proposes the improved sampling method\nbased on the pre-constructed trie structure. However, this study\ndoes not address how to handle dynamic data updates. Therefore,\nwe propose a straightforward algorithm to support such updates.\nWhen deleting data, we adhere to the traditional method by check-\ning if the data is part of the sampling results. If it is, we delete it\nand re-sample some sets to maintain the sample ratio. When in-\nserting data, updating the trie structure is not feasible because it\nonly retains the most frequent elements. Instead, we first partition\ndata into several clusters based on the elements they contain. Then,\nwe use the sample ratio calculated by the original trie to sample\nadditional sets and update the sampling results. Nonetheless, this\nmethod may not well approximate the distribution of elements\nbecause of the fixed trie structure. Another recent work [ 59] pro-\nposes two conversion methods that transform the set-valued data\ninto a small number of categorical data and introduces incremental\nupdating methods for dynamic data. However, a significant issue\nwith the proposed method persists. The cluster generation process\nis based on the dataset before any updates, aiming to alleviate the\neffect of the correlation between elements within the same cluster.\nAs analyzed in Section 5, the correlation between elements is influ-\nenced by the corresponding dataset. Thus, the clusters need to be\nmonitored and reconstructed when necessary because the initial\nclusters might not work well, which the proposed methods ignore.\nCompared to these baselines, the performance of our ACE is\nsuperior as the data encoder minimizes the information loss when\nrepresenting the dataset and the query analyzer effectively captures\nthe useful correlation to obtain accurate estimates.\n7\n\n7 EXPERIMENTS\nThis section reports the experiments that compare ACE with SOTA\nbaselines. All experiments are evaluated on the Katana server [ 69]\nwith a 32-core Xeon(R) Gold 6242 CPU @ 2.80GHz, 100GB memory,\nand an NVIDIA Tesla V100-SXM2 32GB GPU.\n7.1 Datasets and Workloads\nDatasets. We use three real-world datasets varying in the number\nof setsğ‘, the size of the element universe ğ‘€, and the average\nnumber of elements within a set AvgL as described in Table 4. The\nGNdataset [ 59] contains descriptions of natural features, canals,\nand reservoirs in the United States, each of which might consist of\nits name, class, and location state. The WIKI dataset [ 82] consists\nof the first sentence of each English Wikipedia article extracted in\nSeptember 2017. The TW dataset [ 9] includes tweets posted from\nApril 2012 to December 2012, which are published in UCR STAR\n[18]. We preprocess the latter two datasets and convert each set to\na set of words that do not include stop words.\nTable 4: Dataset statistics\nProperty GN WIKI TW\nğ‘ 2.2M 5.3M 19.9M\nğ‘€ 89K 858K 559K\nAvgL 3 12 5\nWorkloads. We follow the method from the former work [ 59] to\ngenerate our workloads. For each subset query, we uniformly draw\n5â€“10 sets from the set-valued dataset and take the union of the\nsampled sets as the query. For each superset and overlap query,\nwe uniformly draw one set from the dataset, and then uniformly\ndraw 2â€“4 elements from the set as the query. We also consider the\nfrequency of query elements. Following the former work [ 76], we\nseparate elements into three classes based on their frequency: low\n(â‰¤0.01%), medium, and high ( â‰¥0.1%). By default, all elements are\nconsidered in a regular query. For high-frequency queries, we add a\nfilter to select only high-frequency elements. The generation of low-\nfrequency queries follows a similar approach. The true cardinality\nof each query is obtained by executing it in PostgreSQL. For each\ndataset, we generate 1400 queries as the training workload, where\nthe ratio of regular, high-frequency, and low-frequency queries is\n3:2:2, while each testing workload consists of 300 queries.\n7.2 Experimental Settings\nImplementations. Our ACE is implemented with PyTorch [ 65]\nand we set the embedding dimension ğ‘‘=64. We train ACE us-\ning Adam optimizer [ 38], with a learning rate of 0.001. We set the\ndata batch size ğµğ‘‘=10000 , the distillation ratio ğ‘Ÿ=0.001, and\nthe query batch size ğµğ‘=100. The number of layers in the dis-\ntillation model ğ‘›distill , the cross-attention module ğ‘›cross, and the\nself-attention module ğ‘›selfare set to 4, 4, and 8, respectively. When\nutilizing the multi-head attention mechanism, we follow the exist-\ning work [ 50] by setting the number of heads to 8. For all datasets,\nwe employ negative sampling with 10 samples for each set and per-\nform a grid search for the L2 regularization weight ğœ†âˆˆ[0,0.005].\nCompetitors. We include the following representative methods.\n(1)PGis the 1-D histogram-based cardinality estimator used in\nPostgreSQL [ 40]. (2) Sampling uniformly samples a collectionof sets, where we set the sample ratio as 0.01. (3) Greek-S [60]\nproposes a different method to calculate the caridnality based on the\nstatictis of the sampled dataset. (4) OT-S [96] samples a collection of\nsets based on the constructed trie structure. For a pair comparison,\nthe sampling ratio is the same as the previous approach and we\nkeep the 12 most frequent elements following the setting in the\nprevious work. (5) Set-Trans [46] is proposed to capture element\ncorrelation and trained in a supervised learning manner. We regard\nit as a query-driven estimator. (6) STandSTH [59] convert the\nset-valued set into a small number of numerical data and employ\nthe existing estimators. Based on the observation of the former\nwork, we use DeepDB [ 30] and NeuroCard [ 97] as the employed\nestimators for STandSTH , respectively.\nEvaluation metrics. We use four metrics to evaluate all meth-\nods. (1) Q-error [61] measures the distance between the estimated\ncardinalityğ‘ğ‘and the true cardinality ğ‘of a query. In particular,\nQ-error =ğ‘šğ‘ğ‘¥{1,ğ‘ğ‘\nğ‘,ğ‘\nğ‘ğ‘}. (2)Building time denotes the construc-\ntion time of traditional methods or the training time of Set-Trans\nand ACE. For ST and STH, the building time consists of conversion\ntime as well as offline training time. (3) Storage overhead is the\nmemory size used by a method. (4) Estimation latency is the\naverage estimation time per query.\n7.3 Overall Performance\nWe first conduct extensive experiments to evaluate the overall\nperformance. We do not compare ST and STH on the overlap query\nsince they are incompatible with this query type.\nEstimation Accuracy. Tables 6 - 7 show the estimation error for\nvarious queries. We observe ACE has the best performance com-\npared to other baselines in most cases. The mean Q-error of ACE\nin all cases is smaller than 10. In contrast, none of the other meth-\nods can reach this level of performance. Additionally, at the 95%\nquantile, PG, Sampling, OT-S, ST and STH averagely result in up to\n16.7Ã—, 29.6Ã—, 27.7Ã—, 13.5Ã—, 10.2Ã—larger Q-error than that of ACE,\nrespectively. Next, we analyze each method individually.\nTable 5: Estimation error for subset queries\nDataset MethodRegular High-frequency Low-frequency\nMean 50% 95% 99% Mean 50% 95% 99% Mean 50% 95% 99%\nGNPG 8.53 6.54 16.9 33.2 4.12 3.87 6.03 10.6 2.75 2.25 6 9\nSampling 1.36 1.21 2.54 3.91 1.11 1.09 1.31 1.45 62.3 13 166 203\nGreek-S 1.39 1.14 1.75 8.48 1.32 1.16 1.87 5.48 18.2 14.1 44.4 53.3\nOT-S 1.08 1.06 1.21 1.32 1.09 1.07 1.23 1.32 63.7 13 170 203\nSet-Trans 1.51 1.33 2.46 3.56 1.77 1.42 4.14 6.57 134 111 325 513\nST 3.81 2.73 9.53 18.9 6.06 4.77 14.2 25.1 21.2 18 48.1 66.1\nSTH 2.75 2.46 5.69 8.67 1.12 1.09 1.35 1.46 17.1 20.5 32.1 54.5\nACE 1.26 1.13 2.34 4.17 1.69 1.44 3.26 5.22 3.11 2.81 8.56 12.4\nWIKIPG 9.49 5.68 19.7 41.3 4.74 3.51 11.9 14.6 4.14 3.83 7.67 9.51\nSampling 28.1 1.37 188 299 24.5 1.41 151 207 28.7 22.6 54.5 89\nGreek-S 2.24 1.85 5.13 8.38 2.73 1.82 7.24 10.7 25.6 18.1 45.5 53.4\nOT-S 25.3 1.45 109 215 21.7 1.61 131 189 30.7 23.3 52.1 84\nSet-Trans 2.84 1.62 5.97 11.2 2.83 2.14 6.19 12.4 6.97 5.25 17.1 37.8\nST 7.39 1.78 7.05 14.7 5.39 2.33 28.9 35.3 13.8 7.11 54 135\nSTH 7.32 5.32 19.7 30.8 10.1 8.23 22.8 33.9 12.4 10.2 48.6 89.2\nACE 2.04 1.36 4.93 8.71 2.37 1.77 5.35 7.27 2.43 1.75 5.96 13.5\nTWPG 5.85 4.39 13.8 23.2 4.01 3.19 7.74 12.2 3.69 2.88 9.13 12.7\nSampling 5.04 4.03 12.1 36.2 3.57 3.24 13.5 29.1 19.1 15.4 52 76\nGreek-S 1.81 1.42 2.49 3.92 1.93 1.81 3.54 11.9 14.4 11.5 38.1 53.5\nOT-S 4.99 3.83 9.67 28.4 4.82 3.55 11.6 31.2 21.9 16.2 60 97\nSet-Trans 2.05 1.89 3.72 4.47 2.74 2.54 5.81 9.58 375 260 922 1464\nST 4.38 3.74 8.82 12.4 4.74 3.31 7.32 15.4 30.9 23.1 74 127\nSTH 5.07 3.24 12.3 15.2 3.11 2.76 5.94 11.1 19.5 4.57 72.7 130\nACE 1.46 1.34 2.17 2.81 1.66 1.53 2.94 4.24 1.88 1.61 3.81 4.92\nPG estimates the cardinality of a query based on the indepen-\ndence assumption, which often leads to poor performance on reg-\nular and high-frequency queries due to ignoring the correlation\nbetween elements. However, its estimates are more accurate for\n8\n\nTable 6: Estimation error for superset queries\nDataset MethodRegular High-frequency Low-frequency\nMean 50% 95% 99% Mean 50% 95% 99% Mean 50% 95% 99%\nGNPG 67.5 4.26 198 1785 96.6 3.5 192 2728 6.73 6 12 17\nSampling 16.7 5 70 187 21.1 2.45 94 229 37.1 35.2 76.7 90.2\nGreek-S 12.3 5.93 44.4 53.3 8.05 3.46 33.3 53.3 25.8 17.4 50.6 53.3\nOT-S 20.2 6 81 195 17.9 2.21 78 192 36.7 34.5 77.1 91.3\nSet-Trans 12.5 5.77 46.9 117 12.4 4.32 46.5 133 7.53 5.22 18.6 33.5\nST 40.2 2.21 52.7 272 18.8 2.12 22.2 100 9.71 7 28.5 37.4\nSTH 16.3 1.52 34.6 161 10.3 1.45 22.2 150 5.08 5 11 15\nACE 5.19 2.91 17.2 36.1 6.54 2.19 20.9 49.6 2.15 2.11 3.18 3.49\nWIKIPG 175 8 271 2330 439 8.61 703 5479 9.44 3.75 33 89\nSampling 13.3 2.31 65 145 15.1 1.39 71 179 32.9 28.1 83 189\nGreek-S 10.9 3.98 38.1 53.4 8.91 2.35 33.4 93.5 15.4 12.4 30.4 53.5\nOT-S 14.8 2.57 72 159 11.6 1.51 61 159 33.7 29.3 77 186\nSet-Trans 19.2 7.86 80.2 158 15.1 5.43 60.2 108 22.6 10.7 81.8 96.1\nST 130 4.92 245 1570 130 2.77 101 1576 30.6 13.3 118 241\nSTH 16.1 3.98 69.6 169 9.31 2.58 35.4 143 29.3 11 118 239\nACE 6.44 3.34 17.9 37.4 8.33 3.16 30.8 75.2 2.88 2.67 8.23 11.6\nTWPG 179 2.39 99 1014 128 2.09 48 2323 14.3 4.75 53.5 152\nSampling 17.5 2.31 83 223 10.7 1.32 60 173 173 149 251 380\nGreek-S 9.83 3.21 44.5 74.3 9.22 2.43 26.7 80.3 22.7 19.1 47.6 53.3\nOT-S 15.6 1.96 87 210 8.91 1.36 44 135 161 137 251 380\nSet-Trans 20.2 6.67 82.7 128 13.3 4.67 65.6 118 13.9 10.5 37.2 60.7\nST 49.9 2.37 81.1 578 43.7 2.05 76.6 553 37.9 10 160 347\nSTH 12.2 2.08 48.6 306 11.7 2.83 44.1 471 37.7 10.1 160 374\nACE 6.79 2.32 22.5 60.7 8.41 2.04 26.8 67.1 3.93 2.61 8.76 11.7\nTable 7: Estimation error for overlap queries\nDataset MethodRegular High-frequency Low-frequency\nMean 50% 95% 99% Mean 50% 95% 99% Mean 50% 95% 99%\nGNPG 3.28 1.26 9.82 34.1 2.99 1.31 9.47 27.4 12.7 4.15 68.1 108\nSampling 4.18 1.28 9.95 42.1 2.93 1.32 9.19 29.2 239 2.89 481 5954\nGreek-S 4.99 1.33 11.1 75.2 2.99 1.35 8.66 28.1 13.6 2.71 33.3 277\nOT-S 3.41 1.28 9.96 30.6 3.01 1.33 8.81 29.8 235 3.08 502 6409\nSet-Trans 10.7 4.73 46.2 110 10.9 5.51 22.1 94.7 51.2 30.3 221 346\nACE 3.34 1.46 9.29 18.1 2.66 1.56 6.39 17.6 9.62 2.67 22.7 80.6\nWIKIPG 8.28 1.81 13.1 94.2 4.28 2.32 15.1 40.7 27.1 14.9 165 368\nSampling 4.29 1.88 13.2 43 4.28 2.34 14.2 37.4 43.5 2.49 214 496\nGreek-S 4.52 1.93 16.6 38.8 4.31 2.32 13.5 38.5 8.44 3.29 22.9 54.8\nOT-S 5.52 1.78 13.1 59.3 4.16 2.26 12.9 37.1 35.5 2.54 196 414\nSet-Trans 23.9 11.9 98.7 131 37.7 19.5 108 260 26.6 14.3 104 239\nACE 3.98 1.75 8.99 15.8 2.72 2.14 7.52 10.7 8.01 2.46 22.6 38.5\nTWPG 6.33 2.26 19.5 85.9 4.64 2.38 11.3 37.4 18.3 14.3 134 377\nSampling 6.21 2.26 19.6 82.1 4.67 2.42 10.8 37.6 28.1 2.83 175 473\nGreek-S 6.37 2.33 22.4 76.4 4.61 2.41 11.1 36.2 11.6 3.59 20.6 320\nOT-S 6.25 2.21 19.3 84.1 4.66 2.35 10.6 37.6 28.6 2.81 167 287\nSet-Trans 51.1 16.5 162 446 69.2 28.9 191 580 16.2 8.68 57.7 166\nACE 5.61 2.19 16.4 51.5 2.96 2.21 7.92 13.7 6.72 2.79 17.9 73.3\nlow-frequency queries, as the correlation between low-frequency\nelements can sometimes be disregarded. Moreover, we observe that\nits performance on low-frequency overlap queries is bad because\nof insufficient statistics targets.\nSampling, Greek-S and OT-S show the opposite trend compared\nto PG. Their performance on regular and high-frequency queries\nis better than that on low-frequency queries because they focus\nmore on high-frequency elements. Greek-S firstly determines the\ngeometric mean of the upper ( ğœ”) and lower ( ğ›¼) bounds for the\nnumber of qualifying tuples based on probabilistic estimates, which\nis then used to return a more accurate cardinality estimate. OT-S\nimproves upon the traditional sampling method by leveraging the\ntrie structure, leading to better performance in most cases. However,\nthe performance of these methods is not stable across three datasets,\nas it heavily depends on the quality of sampling results.\nSet-Trans only utilizes the information of the workload, regard-\ning the problem as a supervised learning task. However, its perfor-\nmance is unstable across datasets and queries since it is impossible\nto enumerate all combinations given limited training data. ST and\nSTH are SOTA methods that can utilize any data-driven estimator\nto predict cardinality based on the constructed clusters and the cor-\nresponding conversion algorithm. In general, our ACE outperforms\nthese methods, verifying that the partial independence assumption\nin these methods is not reasonable for some scenarios. Additionally,\nwe observe that the results on low-frequency queries differ signifi-\ncantly from those reported in the former work [ 59], as it first filtersout low cardinality elements before selecting the query element,\nthereby ignoring the actual low-frequency elements.\nConstruction Efficiency. Referring to Table 8, the training time\nof ACE is acceptable and shorter than STH and ST in most cases. It\nrequires less than 2, 7, and 10 minutes to fine-tune its parameters for\nthe GN, WIKI, and TW datasets, respectively. Although Sampling\nand OT-S require less construction time, their Q-error performance\nis worse, especially on low-frequency queries. Notably, Greek-S is\nexcluded from our analysis, as it does not influence the sampling\nprocess. Besides, we observe that the time of Set-Trans and ACE on\ndifferent types varies because of the variable-size query, where a\nsubset query usually has more elements than other types of queries.\nTable 8: Building time (minutes) of different methods\nDataset Type Sampling OT-S Set-Trans ST STH ACE\nGNSubset 0.03 0.11 2.76 0.43 4.02 1.88\nSuperset 0.03 0.11 2.06 0.43 3.94 1.56\nOverlap 0.03 0.11 2.03 - - 1.58\nWIKISubset 0.06 0.61 6.03 10.9 13.2 6.77\nSuperset 0.06 0.61 1.97 10.9 12.9 2.85\nOverlap 0.06 0.61 1.68 - - 2.67\nTWSubset 0.21 1.31 6.16 9.73 11.7 9.35\nSuperset 0.21 1.31 5.07 9.73 11.4 5.13\nOverlap 0.21 1.31 5.14 - - 5.16\nStorage Overhead. Table 9 shows the experimental results. We\ndenote the size of samples as the storage overhead of sampling-\nbased methods, which increases with the size of datasets. As Greek-\nS does not change the sampling process, its results are excluded\nfrom the table. ST and STH need to maintain the parameters of\nDeepDB and NeuroCard, respectively, with STH typically incurring\nhigher space costs due to its more complex structure. In contrast,\nSet-Trans maintains a consistent model size across datasets, as its\nstorage requirements are determined by the embedding dimensions.\nHowever, its overall size is slightly larger than ours due to the\nadditional inclusion of inducing points. Our ACE demonstrates a\nstable storage, exhibiting only a marginal increase as the dataset\nsize expands, primarily due to the benefits of its distillation process.\nTable 9: Storage overhead (MB) of different methods\nDataset Sampling OT-S Set-Trans ST STH ACE\nGN 0.28 0.29 10.6 3.31 16.1 8.11\nWIKI 3.21 3.33 10.6 29.6 79.7 8.26\nTW 4.77 4.79 10.6 11.2 58.1 8.36\nEstimation Latency. As illustrated in Table 10, PG needs the least\ntime to estimate the cardinality but its performance is not accept-\nable, while the latency of the sampling-based methods increases\nwith the size of the dataset because they need to traverse all samples.\nGreek-S, in particular, exhibits significantly higher latency than the\nother methods, as it involves more computational steps, resulting\nin increased time costs. As shown in previous work [ 59], the time\ncomplexity of ST depends on the number of clusters, which leads\nto lower estimation time, while STH reduces the number of nodes\nkept on each trie to speed up the prediction process. However, both\nmethods need to convert the query before estimating the cardinal-\nity, which cannot be executed in GPUs. Set-Trans uses the least\ntime to predicate the cardinality because it only takes the query as\nthe input and regards the problem as a supervised learning task.\n9\n\nACE is a fully learning-based estimator with the best performance\nand the most stable latency across three datasets.\nTable 10: Estimation latency (ms) of different methods\nDataset PG Sampling Greek-S OT-S Set-Trans ST STH ACE\nGN 1.05 124.9 207.2 128.6 2.91 3.86 12.39 4.54\nWIKI 3.64 381.1 876.1 392.4 3.28 25.67 83.12 5.17\nTW 2.79 1163 7808 1175 3.07 19.57 41.07 6.17\nReal-world Cases. In Section 1, we introduce a real-world applica-\ntion for set-valued data, i.e., tag search. Privacy policies render user\ndata confidential in most applications, such as Twitter and Wiki. We\nutilize the data released by a recipe website (http://www.food.com)\nand its real user search queries [ 52,58] to show the necessity of\nsupporting set queries. In this dataset, the total number of keywords\nis 632 and there are 500K recipes, each tagged with several key-\nwords. For the query workload, we extract 10K distinctive queries\nfor superset and overlap queries supported by this website. For\neach query type, we randomly select 1000 and 200 queries for the\ntraining and validation, respectively, while the remaining are used\nfor testing.\nAs ST and STH cannot support the overlap queries, the two\nmethods have no corresponding results. As shown in Table 11,\nACE consistently outperforms other baseline methods using the\nreal-world set-valued data and query workload.\nTable 11: Real-world tag search\nMethodSuperset Overlap\nMean 50% 95% 99% Mean 50% 95% 99%\nPG 15.1 4.25 51 186 1.17 1.13 1.74 2.16\nSampling 14.4 4.62 59 122 1.26 1.31 1.88 3.71\nGreek-S 4.42 2.31 17.9 17.9 1.21 1.13 1.57 2.11\nOT-S 3.68 2.27 12.5 12.8 1.21 1.16 1.44 1.98\nSet-Trans 3.41 2.19 8.47 20.9 1.61 1.45 2.24 4.79\nST 4.05 2.29 11.8 23.4 - - - -\nSTH 10.2 7.07 29.1 50.6 - - - -\nACE 3.18 2.01 7.53 15.2 1.02 1.01 1.05 1.11\n7.4 Performance on Dynamic Data\nWe follow previous studies [ 50,59] to conduct experiments on\ndynamic data. We use about 70% of the sets as the initial dataset\nto train our data encoder and the remaining as the collection of\ninsertion data. The size of each insertion is equal to the data batch\nsizeğµğ‘‘. 90% of the insertion is used to train the query analyzer while\nthe remaining is used to evaluate the performance. Additionally, we\nmight randomly delete some sets from the current dataset before\nany insertions. To simulate the real-world scenarios, the number of\ndeleted sets is only a small fraction of the entire dataset, meaning\nthat the number of affected slices is much lower than the others.\nRegarding the workload, we only conduct the experiments on\nthe superset and subset query since ST and STH are incompatible\nwith the overlap query. To train the query analyzer, we utilize the\ngenerated workload as the base workload. Then, we randomly se-\nlect 20 and 10 queries from the base workload as the training and\nvalidation sets, respectively, once an insertion completes. When\nevaluating the performance, we generate 100 queries after any in-\nsertion as the evaluation queries of the current dataset and finally\nreport the average value. Note that the true cardinality of a querymight change due to the dynamic data. Thus, we need to use Post-\ngreSQL to obtain the true cardinality values and filter the queries\nwithout any results in the training or evaluation process.\nTable 12: The performance on dynamic data\nDataset MethodSuperset Subset\nQ-error Update\nTime (s)Q-error Update\nTime (s) Mean 50% 95% 99% Mean 50% 95% 99%\nGNPG 69.6 5.01 204 2031 - 8.39 7.23 15.1 35.7 -\nSampling 18.7 7 84 173 0.01 1.37 1.16 3.18 5.28 0.01\nGreek-S 15.4 9.21 53.3 64.7 0.01 1.61 1.52 2.89 6.06 0.01\nOT-S 24.2 7.2 97.2 258 0.06 1.62 1.59 2.82 6.98 0.06\nSet-Trans 28.8 11.3 78.2 211 0.26 5.77 4.83 7.18 9.49 0.34\nST 45.7 4.35 78.1 395 0.16 4.51 3.17 12.7 20.2 0.16\nSTH 20.5 4.02 49.9 190 0.17 3.88 2.56 8.76 11.5 0.17\nACE 5.35 3.09 15.3 42.7 0.34 1.59 1.51 2.77 5.01 0.41\nWIKIPG 136 6.68 341 3249 - 10.1 4.93 18.2 39.5 -\nSampling 14.8 4.01 71 140 0.01 29.8 1.95 197 371 0.01\nGreek-S 11.5 4.17 44.5 53.4 0.01 4.31 1.75 9.42 13.6 0.01\nOT-S 18.5 3.22 90 199 0.06 37.9 2.18 163 323 0.06\nSet-Trans 28.1 13.7 114 243 0.44 4.79 3.11 8.75 20.4 1.29\nST 145 6.77 358 1994 0.41 9.63 3.55 17.4 31.2 0.41\nSTH 22.4 5.68 101 274 0.81 8.76 5.88 25.4 32.1 0.81\nACE 8.57 3.17 16.7 29.1 0.68 3.55 1.69 7.54 9.27 1.56\nTWPG 187 3.43 101 1341 - 6.85 4.17 12.6 25.7 -\nSampling 17.3 2.35 81 224 0.01 6.03 5.32 13.1 37.7 0.01\nGreek-S 12.4 4.47 48.4 97.1 0.01 3.22 2.83 6.06 10.5 0.01\nOT-S 19.5 3.45 109 263 0.06 5.88 4.02 13.5 31.2 0.06\nSet-Trans 39.5 9.77 108 190 0.81 4.68 3.06 7.66 10.2 1.54\nST 55.7 3.37 92 768 0.25 7.28 4.96 14.8 22.3 0.25\nSTH 16.7 2.94 64 320 0.47 8.21 4.04 13.9 30.1 0.47\nACE 10.1 3.22 37.7 84.1 0.97 2.14 1.67 5.28 8.22 1.72\nReferring to Table 12, ACE always has the best performance.\nHere, we do not report the storage overhead and the estimation\nlatency since they are similar to the ones shown in Tables 9 and 10.\nCompared to the static data, PG, Sampling, and Greek-S achieve\nsimilar estimation accuracy while the Q-error of other methods\nincreases when working on dynamic data. Compared to Sampling,\nthe update progress of OT-S depends on the trie structure built\nbased on the initial dataset, and this structure does not capture the\ndistribution of elements well when encountering new data. ST and\nSTH incrementally update their corresponding trie structure on\ndynamic data but fix the generated clusters. However, the latest set\nalways brings a change in the element correlation. Therefore, the\nelements within the same cluster might be heavily correlated when\nupdating the dataset, leading to performance degradation. In terms\nof our ACE, we also observe performance degradation because the\ndata encoder is trained based on the initial dataset and might not\noutput the best representation of the updated data. However, its\nperformance is more stable than others since we utilize the query\ninformation in the query analyzer, which can mitigate this effect.\nWe also report the average time of each update. Since our ACE is\nboth data- and query-driven, it requires training the query analyzer\nfor better performance when the data matrix updates. Compared to\nACE, all baseline methods do not require any training progress, and\nthus they need less time to update. However, their representation\nabilities are not as powerful as that of ACE.\n7.5 End-to-End Query Runtime\nWe evaluate the performance of ACE in terms of end-to-end query\nruntime in PostgreSQL. We utilize the latest IMDb dataset [ 32] that\nincludes set-valued attributes and extract another database, Food,\nfrom the published datasets crawled from http://www.food.com [ 2,\n51,88]. For the query workload, because current benchmarks, such\nas JOB [ 47], do not contain queries with set-valued predicates, we\nfollow the existing work [ 59] to generate queries. Specifically, we\n10\n\n20% 40% 60% 80% 100%406080100120E2E Time (s)\nPG\nGS(P)\nSet(P)\nST\nSTH\nACE(P)Figure 6: E2E time varying the ratio of set-valued queries.\nfirstly use SQLsmith [ 72] and the AI SQL generator [ 89] to generate\na query template. Then, we follow the template to generate queries\nwith various granularities. To guarantee the validity of synthetic\nqueries, we follow the same process to generate the set-valued\npredicates. Note that the set-valued predicate on Food utilize the\nreal user queries, as described in Section 7.3. Finally, for the IMDB\ndataset, we generate 30 queries, each containing 4â€“8 predicates\nover 3â€“5 tables, while for the Food dataset, we generate 20 queries,\neach containing 3â€“6 predicates over 3 tables.\nTo inject cardinalities, our ACE(P) configuration extends the\npatch from the previous work [ 3] to accept external estimates for\nset-valued predicates and the existing support for predicates over\ncategorical and numerical attributes. In addition to the baseline\ngiven by PostgreSQL (PG), we compare with four other baselines.\nBecause ST and STH naturally support queries containing predi-\ncates over set-valued attributes, ST and STH configurations inject\nthe estimated cardinalities of ST and STH into the query optimizer\nfor all types of predicates. Note that we use PostgreSQL as the esti-\nmator for ST and STH to guarantee a fair comparison. Additionally,\nconfigurations GS(P) and Set(P) utilize the same approach as ACE(P)\nbut with estimates from Greek-S and Set-Trans, respectively.\nTable 13 shows the end-to-end (E2E) running time and Q-error.\nOur method, ACE, achieves the best overall performance. Notably,\nqueries with set-valued predicates show a notable improvement\ndue to more accurate estimations. We observe that the E2E time\nof GS(P) is even longer than that of the PG because the estimation\nlatency of Greek-S is significantly larger.\nTable 13: End-to-end (E2E) time and Q-error\nMethodIMDB Food\nMean 50% 95% 99%E2E\nTime (s)Mean 50% 95% 99%E2E\nTime (ms)\nPG 30.3 5.11 113 440 106.9 10.3 3.43 20.5 79 7327.8\nGS(P) 17.4 4.66 62.1 139 81.1 9.41 2.78 19.2 75.8 8007.7\nSet(P) 19.4 4.86 80.2 158 82.3 8.17 2.44 16.1 59.1 6285.4\nST 12.1 3.39 44.5 93.1 68.2 6.53 2.05 11.5 30.4 4667.4\nSTH 11.8 3.47 45.1 87.2 66.1 6.77 2.01 13.3 30.7 4528.6\nACE(P) 10.1 2.77 33.7 80.1 40.5 5.53 1.78 9.01 25.2 3004.9\nWe also conduct experiments on IMDB to evaluate the E2E per-\nformance when varying the number of queries including set-valued\npredicates (i.e., set queries). Here, we fix the total number of queries\nin the execution workload and adjust the ratio of set queries. As\nshown in Figure 6, when increasing the ratio, the E2E time of PG\nincreases slightly while the runtime of ACE(P) decreases markedly.\nWe also observe that the results of GS(P) and Set(P) are worse than\nthose of ST and STH because of the worse estimation accuracy or\nlarger estimation latency. The performance for Food is similar and\nwe omit it for brevity.7.6 Ablation Study\nAs shown in Table 14, we verify the effectiveness of the main com-\nponents in ACE. Here, we conduct extensive experiments on the\nWIKI dataset. The results on other datasets are similar and omitted.\nTable 14: Ablation study\nAblation settings Subset Superset Overlap\nAG DS CA SA AP Mean 50% 95% 99% Mean 50% 95% 99% Mean 50% 95% 99%\nÃ—âœ“ âœ“ âœ“ âœ“ 4.47 2.69 17.2 31.8 12.6 5.34 44.4 147 6.69 2.94 13.4 20.1\nâœ“Ã—âœ“ âœ“ âœ“ 4.11 2.72 15.6 32.1 13.7 8.41 49.9 162 7.73 3.23 10.5 18.7\nâœ“ âœ“Ã—âœ“ âœ“ 2.87 1.58 6.26 13.7 8.19 4.14 19.8 45.8 4.31 2.45 9.74 18.3\nâœ“ âœ“ âœ“Ã—âœ“ 3.31 1.65 5.62 11.4 8.97 3.79 25.8 55.7 5.93 3.11 11.6 22.3\nâœ“ âœ“ âœ“ âœ“ Ã— 2.24 1.63 5.12 10.9 6.94 3.39 19.5 52.3 4.62 2.35 9.68 16.9\nâœ“ âœ“ âœ“ âœ“ âœ“ 2.04 1.36 4.93 8.71 6.44 3.34 17.9 37.4 3.98 1.75 8.99 15.8\nAggregator (AG). To replace our aggregator, we can use traditional\nmethods, such as padding and pooling, to generate the fixed-size set\nembedding. Since padding often leads to higher storage overhead,\nwe leverage the mean-pooling method, which has a similar cost to\nour original design. When comparing results, we observe at least\na 60% increase in estimation error at the 50% quantile. This is be-\ncause the mean-pooling method typically treats all elements equally,\nwhich is not powerful enough to obtain high-quality embeddings.\nDistillation (DS). To replace the distillation module, we propose\na random sampling method, setting the sample ratio to 0.001 for\na fair comparison. When comparing the results, we observe a sig-\nnificant increase in estimation error ranging from 94.1% to 112%.\nThis is because the sampling method captures only a fraction of the\ndatasetâ€™s information, whereas the distillation model is designed to\ncompress the matrix while preserving as much information as pos-\nsible. Additionally, since the sampling method give more attention\nto high-frequency elements while the low-frequency elements pre-\ndominantly influences the accuracy of superset queries, we observe\nthe most pronounced fluctuations in these queries.\nCross-attention (CA). The stacked cross-attention layers serve\nto link data and queries, mapping the query elements into a latent\nspace to capture their correlation effectively. As the dimensions of\nthe data matrix ğ‘ºğ’„and query element embeddings ğ’’ğ’Šare fixed, we\ncan adopt a straightforward method without the attention mecha-\nnism to processing them. For example, we flatten ğ‘ºğ’„into a vector\nand concatenate the vector with ğ’’ğ’Što generate another vector. Sub-\nsequently, the generated vector is fed into a multi-layer perceptron\n(MLP) with the same number of layers as in our original model. How-\never, experiments reveal that this simplified approach yields worse\nestimation performance compared to ACE, with a decrease exceed-\ning 16.7%. This performance drop occurs because a straightforward\nneural network is not powerful enough to discover the implicit\nrelations between elements and data. This finding underscores the\nnecessity and effectiveness of incorporating cross-attention layers.\nSelf-attention (SA). The stacked self-attention layers are designed\nto capture correlations between query elements effectively. To eval-\nuate their contribution, we replace this module with a multi-layer\nperceptron (MLP). When compared to ACE, the modified frame-\nwork results in 1.49 Ã—, 1.31Ã—, and 1.41Ã—larger Q-error than that of\nACE at the 99% quantile for superset, subset, and overlap queries,\nrespectively. These results validate the superiority of self-attention\nlayers in accurately modeling inter-element dependencies, which\nultimately leads to more precise cardinality estimation.\nAttention Pooling (AP). The attention pooling module is de-\nsigned to address the issue of variable-size input while ensuring\npermutation invaiance. Since any symmetric function can be used\n11\n\n0 .0020 .004234567Mean Q-error Superset \nSubset \nOverlap(a) Varying ğ‘Ÿ\n02 4 6 8 234567Mean Q-error Superset \nSubset \nOverlap (b) Varying ğ‘›distill\n2 3456234567Mean Q-error Superset \nSubset \nOverlap\n(c) Varying ğ‘›cross\n4 681012234567Mean Q-error Superset \nSubset \nOverlap (d) Varying ğ‘›self\nFigure 7: Estimation performance.\n0 .0020 .00402468 Data Size  LatencyData Size (MB)2\n468 \nLatency (ms)\n(a) Varying ğ‘Ÿ\n2 4 6 8 012Model Size (MB) Model Size  Latency4\n56 \nLatency (ms) (b) Varying ğ‘›distill\nFigure 8: Size and latency.\nto solve this problem, we compare the performance of the attention\npooling method with a mean-pooling method. Our analysis veri-\nfies the effectiveness of the attention pooling method. Compared\nto ACE, the mean-pooling method results in a slight increase in\nestimation error, with 1.39 Ã—, 1.25Ã—, and 1.07Ã—larger mean Q-error\nfor superset, subset, and overlap queries, respectively. This per-\nformance degradation occurs because mean-pooling weights each\nembedding equally regardless of its importance [100].\n7.7 Hyper-parameter Study\nTo study the effects of important hyper-parameters, we build dif-\nferent ACE versions and observe their performance. Similarly, we\nonly show the comparison results on the WIKI dataset.\nEffects ofğ‘Ÿandğ‘›distill.We first study hyper-parameters in our data\nencoder. Figure 7a and 8a show the performance varying distillation\nratiosğ‘Ÿ. We observe that the size of the distilled matrix is clearly\ninfluenced by ğ‘Ÿ. When the value of ğ‘Ÿgets larger, ACE produces more\naccurate estimates, but with higher estimation latency. Besides, the\nperformance improvement becomes marginal when ğ‘Ÿexceeds 0.001.\nAnother important hyper-parameter is the number of layers\nin our distillation model, denoted as ğ‘›distill . Figure 7b illustrates\nthe estimation performance with varying ğ‘›distill . We observe that\nthe Q-error decreases when ğ‘›distill is less than 4, after which it\nstabilizes. As shown in Figure 8b, the model size remains constant\nforğ‘›ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘™ğ‘™â‰¥2as we share weights between each layer except the\n2 345604812Building Time (min) Superset \nSubset \nOverlap(a) Building time\n2 3456678Size (MB) Size     Latency4\n56 \nLatency (ms) (b) Size and latency\nFigure 9: Other metrics with varying ğ‘›cross.\n4 68101204812Building Time (min) Superset \nSubset \nOverlap\n(a) Building time\n4 6810126810 Size     LatencySize (MB)4\n68 \nLatency (ms) (b) Size and latency\nFigure 10: Other metrics with varying ğ‘›self.\nfirst one. Moreover, the estimation latency is similar across these\nvalues because the distilled matrices have the same size.\nEffects ofğ‘›crossandğ‘›self.We also study the effects of hyper-\nparameters in our query analyzer. Figures 7c and 7d shows the mean\nQ-error with varying the values of these two hyper-parameters. We\nobserve that increasing ğ‘›crossandğ‘›selfboth lead to better estimates.\nThis improvement is due to the enhanced ability of more stacked\ncross-attention layers to discover the relationship between queries\nand the underlying data, while additional self-attention layers help\nbetter capture the correlation between query elements.\nThe values of ğ‘›crossandğ‘›selfalso affect the building time, the\nmodel size, and the estimation latency. Figures 9 and 10 illustrate\nthe experiment results of these metrics. We have two observations.\nFirst, largerğ‘›crossorğ‘›selfalways leads to a more complex structure,\nresulting in longer building time, larger model size, and higher\nestimation latency. Second, the effect of ğ‘›crossis more significant\nthan that of ğ‘›selfon these metrics. For example, the building time\nfor subset queries increases from about 6 minutes ( ğ‘›cross=2) to\n14 minutes ( ğ‘›cross=6), compared to an increase from 9 minutes\n(ğ‘›self=4) to 11 minutes ( ğ‘›self=12). This is because the distilled\nmatrix with a larger size is used as one input of the cross-attention\nsub-module. Therefore, taking into account all aspects, we set the\nvalues ofğ‘›crossandğ‘›selfto 4 and 8, respectively.\n8 RELATED WORK\nCardinality estimators for numerical and categorical data.\nData-driven methods aim to tightly approximate the data distri-\nbution by using statistical or machine learning models. Sampling-\nbased methods [ 21,53] estimate cardinality from the sampled data.\nThe simple yet efficient 1-D Histogram [ 71] is used in DBMSs\nsuch as PostgreSQL. It maintains a histogram for each attribute.\nM-D histogram-based methods [ 12,20,62,67,84] build multi-\ndimensional histograms to capture the dependency among attributes.\nHowever, the decomposition of the joint attributes is still lossy such\nthat they need to make partial independence assumptions.\n12\n\nProbability models [ 23,80] utilize the Bayesian network (BN)\nto model the dependence among attributes, assuming that each at-\ntribute is conditionally independent given its parentsâ€™ distributions.\nBayesCard [ 92] revitalizes BN using probabilistic programming to\nimprove its inference and model construction speed. Deep autore-\ngressive models [ 27,97,98] decompose the joint distribution to a\nproduct of conditional distributions, which have high accuracy but\nlow efficiency and require large storage space. DeepDB [ 30] and\nFLAT [ 109] build upon a Sum-Product Network (SPN) [ 66] that\napproximates the joint distribution using multiple SPNs.\nQuery-driven methods focus on modeling the relationships be-\ntween queries and their true cardinalities. LW-XGB and LW-NN [ 13]\nformulate the cardinality estimation as a regression problem and\napply gradient-boosted trees and neural networks to solve the prob-\nlem, respectively. The KDE-based join estimator [ 37] combines\nkernel density estimation (KDE) with a query-driven tuning mecha-\nnism. Fauce [ 54] and NNGP [ 107] assume that the workload follows\na Gaussian distribution and adopt deep ensembles [ 44] and neural\nGaussian process [ 36] to estimate the mean and variance of the\ndistribution. A few works [ 50,90] consider both data and workload.\nThese approaches are limited to querying numerical and categorical\ndata, which are difficult to deploy for set-valued data.\nCardinality estimator for set-valued data. PostgreSQL treats\neach element as a binary attribute and employs either indepen-\ndence assumptions or the probabilistic model [ 16] to estimate the\ncardinality of set-valued queries [ 40]. Yang et al. [ 96] improve the\nsampling method and propose two estimators: OT-sampling uses a\ntrie structure to focus on highly frequent elements, which struggles\nwith low-frequency elements; DC-sampling leverages the work-\nload type information and employs a divide-and-conquer strategy,\nwhich is only applicable for the specified types. Hadjieleftheriou et\nal. [22] propose a hash sampling algorithm for set similarity queries,\nwhich differs from the problem studied in this paper. Meng et al.\n[59] propose two algorithms to convert set-valued data into multi-\ncolumn categorical data and use data-driven methods to estimate\nthe query cardinality. The conversion process can be regarded as ap-\nproximately solving the NP-hard graph coloring problem, making\nit difficult to well capture the correlation among elements. All these\nexisting methods only utilize the information of the underlying data.\nTo the best of our knowledge, there is no learning-based estimator\nthat leverages the underlying data and workload simultaneously.\nAttention applications. The attention mechanism has been ap-\nplied to various problems [ 83,94,95,103,108]. Recently, it has been\nadapted for database optimization [ 15]. The most closely related\nwork [ 50] estimates the cardinality for SPJ (Select-Project-Join)\nqueries. However, its featurization method is not suitable for our\nproblem as the number of columns (less than 100) is significantly\nsmaller than the number of sets (exceeding 106). Thus, our ACE\nneeds new designs for the data encoder and the query analyzer.\n9 CONCLUSION\nWe presented ACE, a versatile learned cardinality estimation model\nthat makes high-quality estimates for set-valued queries. We first\npropose a distillation-based data encoder to represent the entire\ndataset using a compact matrix. To capture correlations between\nquery elements, we then propose an attention-based query analyzer.Since query lengths can vary, we employ a pooling module to derive\nthe fixed-size vector. Extensive experimental results demonstrate\nsuperior performance of ACE compared to the state of the art.\nREFERENCES\n[1]Jaan Altosaar, Rajesh Ranganath, and Wesley Tansey. 2021. RankFromSets:\nScalable set recommendation with optimal recall. Stat10, 1 (2021), e363.\n[2] Alvin. 2021. Recipes and Reviews. https://www.kaggle.com/datasets/irkaal/\nfoodcom-recipes-and-reviews\n[3] Mehmet Aytimur, Silvan Reiner, Leonard WÃ¶rteler, Theodoros Chondrogiannis,\nand Michael Grossniklaus. 2024. LPLM: A Neural Language Model for Cardi-\nnality Estimation of LIKE-Queries. Proceedings of the ACM on Management of\nData 2, 1 (2024), 1â€“25.\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normal-\nization. arXiv preprint arXiv:1607.06450 (2016).\n[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine\nTranslation by Jointly Learning to Align and Translate. In ICLR .\n[6] Christopher M. Bishop. 2006. Pattern recognition and machine learning . Springer\nNew York.\n[7] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hen-\ngrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021.\nMachine unlearning. In S&P. 141â€“159.\n[8] S. Castro, P. Meena Kumari, S. Muthumari, and J. Suganthi. 2023. Information\nRetrieval using Set-based Model Methods, Tools, and Applications in Medical\nData Analysis. Machine Learning for Healthcare Systems: Foundations and\nApplications (2023), 187.\n[9]Lisi Chen, Gao Cong, Xin Cao, and Kian-Lee Tan. 2015. Temporal spatial-\nkeyword top-k publish/subscribe. In ICDE . 255â€“266.\n[10] Yankai Chen, Yixiang Fang, Yifei Zhang, and Irwin King. 2023. Bipartite graph\nconvolutional hashing for effective and efficient top-n search in hamming space.\nInWWW . 3164â€“3172.\n[11] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2021. Rethinking attention with performers. In ICLR .\n[12] Amol Deshpande, Minos Garofalakis, and Rajeev Rastogi. 2001. Independence is\ngood: Dependency-based histogram synopses for high-dimensional data. ACM\nSIGMOD Record 30, 2 (2001), 199â€“210.\n[13] Anshuman Dutt, Chi Wang, Azade Nazi, Srikanth Kandula, Vivek Narasayya,\nand Surajit Chaudhuri. 2019. Selectivity estimation for range predicates using\nlightweight models. Proceedings of the VLDB Endowment 12, 9 (2019), 1044â€“1057.\n[14] Meng Joo Er, Yong Zhang, Ning Wang, and Mahardhika Pratama. 2016. At-\ntention pooling-based convolutional neural network for sentence modelling.\nInformation Sciences 373 (2016), 388â€“403.\n[15] Jia-Ke Ge, Yan-Feng Chai, and Yun-Peng Chai. 2021. WATuning: a workload-\naware tuning system with attention-based deep reinforcement learning. Journal\nof Computer Science and Technology 36, 4 (2021), 741â€“761.\n[16] Lise Getoor, Benjamin Taskar, and Daphne Koller. 2001. Selectivity estimation\nusing probabilistic models. In SIGMOD . 461â€“472.\n[17] Hossein Gholamalinezhad and Hossein Khosravi. 2020. Pooling methods in\ndeep neural networks, a review. arXiv preprint arXiv:2009.07485 (2020).\n[18] Saheli Ghosh, Tin Vu, Mehrad Amin Eskandari, and Ahmed Eldawy. 2019. UCR-\nSTAR: The UCR Spatio-Temporal Active Repository. SIGSPATIAL Special 11, 2\n(2019), 34â€“40.\n[19] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard SchÃ¶lkopf, and Alex\nSmola. 2006. A kernel method for the two-sample-problem. In NIPS . 513â€“520.\n[20] Dimitrios Gunopulos, George Kollios, Vassilis J. Tsotras, and Carlotta Domeni-\nconi. 2000. Approximating multi-dimensional aggregate range queries over\nreal attributes. ACM SIGMOD Record 29, 2 (2000), 463â€“474.\n[21] Peter J. Haas, Jeffrey F. Naughton, and Arun N. Swami. 1994. On the relative\ncost of sampling for join selectivity estimation. In PODS . 14â€“24.\n[22] Marios Hadjieleftheriou, Xiaohui Yu, Nick Koudas, and Divesh Srivastava. 2008.\nHashed samples: selectivity estimators for set similarity selection queries. Pro-\nceedings of the VLDB Endowment 1, 1 (2008), 201â€“212.\n[23] Max Halford, Philippe Saint-Pierre, and Franck Morvan. 2019. An approach\nbased on bayesian networks for query selectivity estimation. In DASFAA . 3â€“19.\n[24] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation\nlearning on large graphs. In NIPS . 1025â€“1035.\n[25] Yuxing Han, Ziniu Wu, Peizhi Wu, Rong Zhu, Jingyi Yang, Tan Wei Liang,\nKai Zeng, Gao Cong, Yanzhao Qin, Andreas Pfadler, Zhengping Qian, Jingren\nZhou, Jiangneng Li, and Bin Cui. 2022. Cardinality Estimation in DBMS: A\nComprehensive Benchmark Evaluation. Proceedings of the VLDB Endowment\n15, 4 (2022), 752â€“765.\n[26] Yu Hao, Xin Cao, Yufan Sheng, Yixiang Fang, and Wei Wang. 2021. KS-GNN:\nKeywords Search Over Incomplete Graphs via Graphs Neural Network. In\nNeurIPS . 1700â€“1712.\n13\n\n[27] Shohedul Hasan, Saravanan Thirumuruganathan, Jees Augustine, Nick Koudas,\nand Gautam Das. 2020. Deep learning models for selectivity estimation of\nmulti-attribute queries. In SIGMOD . 1035â€“1050.\n[28] Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, and Jerome H. Friedman.\n2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction .\nVol. 2. Springer.\n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In CVPR . 770â€“778.\n[30] Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kris-\ntian Kersting, and Carsten Binnig. 2020. DeepDB: Learn from Data, not from\nQueries! Proceedings of the VLDB Endowment 13, 7, 992â€“1005.\n[31] IBM. 2024. Array Types and Values. https://www.ibm.com/docs/en/db2-for-\nzos/13?topic=types-array-values\n[32] IMDb. 2024. Non-Commercial Datasets. https://developer.imdb.com/non-\ncommercial-datasets/\n[33] Yannis Ioannidis. 2003. The history of histograms (abridged). In VLDB . 19â€“30.\n[34] Yannis E Ioannidis and Stavros Christodoulakis. 1991. On the propagation of\nerrors in the size of join results. In SIGMOD . 268â€“277.\n[35] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman,\nand Joao Carreira. 2021. Perceiver: General perception with iterative attention.\nInICML . 4651â€“4664.\n[36] Lee Jaehoon, Bahri Yasaman, Novak Roman, Schoenholz Sam, Pennington\nJeffrey, and Sohl-dickstein Jascha. 2018. Deep Neural Networks as Gaussian\nProcesses. ICLR (2018).\n[37] Martin Kiefer, Max Heimel, Sebastian BreÃŸ, and Volker Markl. 2017. Estimating\njoin selectivities using bandwidth-optimized kernel density models. Proceedings\nof the VLDB Endowment 10, 13 (2017), 2085â€“2096.\n[38] Diederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic\nOptimization. In ICLR .\n[39] Martin Kleppmann. 2017. Designing Data-intensive Applications: The Big Ideas\nBehind Reliable, Scalable, and Maintainable Systems . Oâ€™Reilly Media, Inc.\n[40] Alexander Korotkov and Konstantin Kudryavtsev. 2016. Selectivity Estima-\ntion for Search Predicates over Set Valued Attributes. International Journal of\nDatabase Theory and Application 9, 10 (2016), 285â€“294.\n[41] Adrian Kosowski and Krzysztof Manuszewski. 2004. Classical coloring of graphs.\nContemp. Math. 352 (2004), 1â€“20.\n[42] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. 2018.\nThe case for learned index structures. In SIGMOD . 489â€“504.\n[43] Kenneth A Lachlan, Patric R Spence, Xialing Lin, Kristy Najarian, and Maria\nDel Greco. 2016. Social media and crisis management: CERC, search strategies,\nand Twitter content. Computers in Human Behavior 54 (2016), 647â€“652.\n[44] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple\nand scalable predictive uncertainty estimation using deep ensembles. In NIPS .\n6405â€“6416.\n[45] Geon Lee, Chanyoung Park, and Kijung Shin. 2022. Set2Box: Similarity Pre-\nserving Representation Learning for Sets. In ICDM . 1023â€“1028.\n[46] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and\nYee Whye Teh. 2019. Set transformer: A framework for attention-based\npermutation-invariant neural networks. In ICML . 3744â€“3753.\n[47] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, and\nThomas Neumann. 2015. How good are query optimizers, really? Proceedings\nof the VLDB Endowment 9, 3 (2015), 204â€“215.\n[48] Feifei Li, Bin Wu, Ke Yi, and Zhuoyue Zhao. 2016. Wander join: Online aggre-\ngation for joins. In SIGMOD . 2121â€“2124.\n[49] Guang Li, Ren Togo, Takahiro Ogawa, and Miki Haseyama. 2020. Soft-label\nanonymous gastric x-ray image distillation. In ICIP. 305â€“309.\n[50] Pengfei Li, Wenqing Wei, Rong Zhu, Bolin Ding, Jingren Zhou, and Hua Lu. 2023.\nALECE: An Attention-based Learned Cardinality Estimator for SPJ Queries on\nDynamic Workloads. Proceedings of the VLDB Endowment 17, 2 (2023), 197â€“210.\n[51] Shuyang Li. 2020. Recipes and Interactions. https://www.kaggle.com/datasets/\nshuyangli94/food-com-recipes-and-user-interactions\n[52] Shuyang Li, Yufei Li, Jianmo Ni, and Julian McAuley. 2022. SHARE: A System\nfor Hierarchical Assistive Recipe Editing. In EMNLP . 11077â€“11090.\n[53] Richard J. Lipton, Jeffrey F. Naughton, and Donovan A. Schneider. 1990. Practical\nselectivity estimation through adaptive sampling. In SIGMOD . 1â€“11.\n[54] Jie Liu, Wenqian Dong, Qingqing Zhou, and Dong Li. 2021. Fauce: Fast and ac-\ncurate deep ensembles with uncertainty for cardinality estimation. Proceedings\nof the VLDB Endowment 14, 11 (2021), 1950â€“1963.\n[55] Tianyi Liu, Minshuo Chen, Mo Zhou, Simon S. Du, Enlu Zhou, and Tuo Zhao.\n2019. Towards understanding the importance of shortcut connections in residual\nnetworks. In NeurIPS . 7892â€“7902.\n[56] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019. Multi-\nTask Deep Neural Networks for Natural Language Understanding. In ACL.\n4487â€“4496.\n[57] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan. 2017. Deep\ntransfer learning with joint adaptation networks. In ICML . 2208â€“2217.\n[58] Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, and Julian McAuley.\n2019. Generating Personalized Recipes from Historical User Preferences. InEMNLP-IJCNLP . 5976â€“5982.\n[59] Zizhong Meng, Xin Cao, and Gao Cong. 2023. Selectivity Estimation for Queries\nContaining Predicates over Set-Valued Attributes. Proceedings of the ACM on\nManagement of Data 1, 4 (2023), 1â€“26.\n[60] Guido Moerkotte and Axel Hertzschuch. 2020. alpha to omega: the G(r)eek\nAlphabet of Sampling. In CIDR .\n[61] Guido Moerkotte, Thomas Neumann, and Gabriele Steidl. 2009. Preventing bad\nplans by bounding the impact of cardinality estimation errors. Proceedings of\nthe VLDB Endowment 2, 1 (2009), 982â€“993.\n[62] M. Muralikrishna and David J. DeWitt. 1988. Equi-depth multidimensional\nhistograms. In SIGMOD . 28â€“36.\n[63] MySQL. 2017. SET Data Type. http://download.nust.na/pub6/mysql/tech-\nresources/articles/mysql-set-datatype.html\n[64] Oracle. 2023. PL/SQL Collections and Records. https://docs.oracle.com/en/\ndatabase/oracle/oracle-database/23/lnpls/plsql-collections-and-records.html\n[65] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-\ngory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,\net al.2019. PyTorch: An imperative style, high-performance deep learning\nlibrary. In NeurIPS . 8026â€“8037.\n[66] Hoifung Poon and Pedro Domingos. 2011. Sum-product networks: A new deep\narchitecture. In ICCV Workshops . 689â€“690.\n[67] Viswanath Poosala and Yannis E Ioannidis. 1997. Selectivity estimation without\nthe attribute value independence assumption. In VLDB . 486â€“495.\n[68] PostgreSQL. 2024. Array Functions and Operators. https://www.postgresql.\norg/docs/17/functions-array.html\n[69] UNSW Sydney PVC (Research Infrastructure). 2010. Katana. (2010).\n[70] David Saad. 1998. Online algorithms and stochastic approximations. Vol. 5. 6.\n[71] P Griffiths Selinger, Morton M Astrahan, Donald D Chamberlin, Raymond A\nLorie, and Thomas G Price. 1979. Access path selection in a relational database\nmanagement system. In SIGMOD . 23â€“34.\n[72] Andreas Seltenreich, Bo Tang, and Sjoerd Mullender. 2022. Bug Squashing with\nSQLsmith. https://github.com/anse1/sqlsmith\n[73] SQL server. 2023. Table-valued Parameters in SQL Server. https:\n//learn.microsoft.com/en-us/sql/relational-databases/tables/use-table-valued-\nparameters-database-engine?view=sql-server-ver16\n[74] Noam Shazeer. 2020. GLU Variants Improve Transformer. arXiv preprint\narXiv:2002.05202 (2020).\n[75] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. 2021.\nEfficient attention: Attention with linear complexities. In WACV . 3531â€“3539.\n[76] Yufan Sheng, Xin Cao, Yixiang Fang, Kaiqi Zhao, Jianzhong Qi, Gao Cong,\nand Wenjie Zhang. 2023. WISK: A workload-aware learned index for spatial\nkeyword queries. Proceedings of the ACM on Management of Data 1, 2 (2023),\n1â€“27.\n[77] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneer-\nshelvam, Marc Lanctot, et al .2016. Mastering the Game of Go with Deep Neural\nNetworks and Tree Search. Nature 529, 7587 (2016), 484â€“489.\n[78] Zhou Tao, Chang XiaoYu, Lu HuiLing, Ye XinYu, Liu YunCan, and Zheng\nXiaoMin. 2022. Pooling operations in deep learning: from â€œinvariableâ€â€™ to\nâ€œvariableâ€â€™. BioMed Research International 2022, 1 (2022), 4067581.\n[79] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Piotr Bojanowski, Armand\nJoulin, Gabriel Synnaeve, and HervÃ© JÃ©gou. 2021. Augmenting Convolutional\nnetworks with attention-based aggregation. arXiv preprint arXiv:2112.13692\n(2021).\n[80] Kostas Tzoumas, Amol Deshpande, and Christian S. Jensen. 2011. Lightweight\ngraphical models for selectivity estimation without independence assumptions.\nProceedings of the VLDB Endowment 4, 11 (2011), 852â€“863.\n[81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you\nneed. In NIPS . 6000â€“6010.\n[82] Denny VrandeÄiÄ‡ and Markus KrÃ¶tzsch. 2014. Wikidata: a free collaborative\nknowledgebase. Commun. ACM 57, 10 (2014), 78â€“85.\n[83] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang,\nXiaogang Wang, and Xiaoou Tang. 2017. Residual attention network for image\nclassification. In CVPR . 3156â€“3164.\n[84] Hai Wang and Kenneth C. Sevcik. 2003. A multi-dimensional histogram for\nselectivity estimation and fast approximate query answering. In Conference of\nthe Centre for Advanced Studies on Collaborative research . 328â€“342.\n[85] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. 2018.\nDataset Distillation. arXiv preprint arXiv:1811.10959 (2018).\n[86] Zhen Wang, Liqiang Zhang, Liang Zhang, Roujing Li, Yibo Zheng, and Zidong\nZhu. 2018. A Deep Neural Network With Spatial Pooling (DNNSP) for 3-D\nPoint Cloud Classification. IEEE Transactions on Geoscience and Remote Sensing\n56, 8 (2018), 4594â€“4604.\n[87] Benjamin Warner. 2022. Tinkering With Attention Pooling. https://\nbenjaminwarner.dev/2022/07/14/tinkering-with-attention-pooling.\n[88] Alexander Wei. 2024. Recipes with Ingredients and Tags. https://www.kaggle.\ncom/datasets/realalexanderwei/food-com-recipes-with-ingredients-and-tags\n14\n\nTable 15: Low-frequency element overlap ratio\nSubset Superset Overlap\n151/11077 (1.3%) 13/651 (1.9%) 12/805 (1.5%)\n[89] Widenex. 2024. Widenex GPTs. https://gpts.widenex.com/\n[90] Peizhi Wu and Gao Cong. 2021. A Unified Deep Model of Learning from both\nData and Queries for Cardinality Estimation. In SIGMOD . 2009â€“2022.\n[91] Xueyi Wu, Yuanyuan Xu, Wenjie Zhang, and Ying Zhang. 2023. Billion-Scale\nBipartite Graph Embedding: A Global-Local Induced Approach. Proceedings of\nthe VLDB Endowment 17, 2 (2023), 175â€“183.\n[92] Ziniu Wu, Amir Shaikhha, Rong Zhu, Kai Zeng, Yuxing Han, and Jingren Zhou.\n2020. BayesCard: Revitilizing Bayesian Frameworks for Cardinality Estimation.\narXiv preprint arXiv:2012.14743 (2020).\n[93] Tong Xiao, Yinqiao Li, Jingbo Zhu, Zhengtao Yu, and Tongran Liu. 2019. Sharing\nAttention Weights for Fast Transformer. In IJCAI . 5292â€“5298.\n[94] Yuanyuan Xu, Wenjie Zhang, Ying Zhang, Maria Orlowska, and Xuemin Lin.\n2024. TimeSGN: Scalable and Effective Temporal Graph Neural Network. In\nICDE . 3297â€“3310.\n[95] Yuanyuan Xu, Wenjie Zhang, Ying Zhang, Xiwei Xu, and Xuemin Lin. 2025. Fast\nand Accurate Temporal Hypergraph Representation for Hyperedge Prediction.\nInProceedings of the SIGKDD Conference on Knowledge Discovery and Data\nMining (KDD 2025) . ACM.\n[96] Yang Yang, Wenjie Zhang, Ying Zhang, Xuemin Lin, and Liping Wang. 2019.\nSelectivity estimation on set containment search. Data Science and Engineering\n4, 3 (2019), 254â€“268.\n[97] Zongheng Yang, Amog Kamsetty, Sifei Luan, Eric Liang, Yan Duan, Xi Chen,\nand Ion Stoica. 2021. NeuroCard: One Cardinality Estimator for All Tables.\nProceedings of the VLDB Endowment 14, 1, 61â€“73.\n[98] Zongheng Yang, Eric Liang, Amog Kamsetty, Chenggang Wu, Yan Duan, Xi\nChen, Pieter Abbeel, Joseph M Hellerstein, Sanjay Krishnan, and Ion Stoica.\n2019. Deep Unsupervised Cardinality Estimation. Proceedings of the VLDB\nEndowment 13, 3, 279â€“292.\n[99] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,\nand Jure Leskovec. 2018. Graph convolutional neural networks for web-scale\nrecommender systems. In KDD . 974â€“983.\n[100] Afia Zafar, Muhammad Aamir, Nazri Mohd Nawi, Ali Arshad, Saman Riaz,\nAbdulrahman Alruban, Ashit Kumar Dutta, and Sultan Almotairi. 2022. A\ncomparison of pooling methods for convolutional neural networks. Applied\nSciences 12, 17 (2022), 8643.\n[101] Hansong Zhang, Shikun Li, Pengju Wang, Dan Zeng, and Shiming Ge. 2024.\nM3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy. In\nAAAI . 9314â€“9322.\n[102] Jintao Zhang, Chao Zhang, Guoliang Li, and Chengliang Chai. 2024. PACE:\nPoisoning Attacks on Learned Cardinality Estimation. Proceedings of the ACM\non Management of Data 2, 1 (2024), 1â€“27.\n[103] Linhan Zhang, Qian Chen, Wen Wang, Chong Deng, ShiLiang Zhang, Bing Li,\nWei Wang, and Xin Cao. 2022. MDERank: A Masked Document Embedding\nRank Approach for Unsupervised Keyphrase Extraction. In Findings of the Asso-\nciation for Computational Linguistics: ACL 2022 . Association for Computational\nLinguistics, 396â€“409.\n[104] Yan Zhang, Jonathon Hare, and Adam Prugel-Bennett. 2019. Deep Set Prediction\nNetworks. In NeurIPS . 3212â€“3222.\n[105] Zeyu Zhang, Jiamou Liu, Kaiqi Zhao, Song Yang, Xianda Zheng, and Yifei Wang.\n2023. Contrastive learning for signed bipartite graphs. In SIGIR . 1629â€“1638.\n[106] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2021. Dataset Condensation\nwith Gradient Matching. In ICLR .\n[107] Kangfei Zhao, Jeffrey Xu Yu, Zongyan He, Rui Li, and Hao Zhang. 2022. Light-\nweight and accurate cardinality estimation by neural network gaussian process.\nInSIGMOD . 973â€“987.\n[108] Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. GMAN:\nA graph multi-attention network for traffic prediction. In AAAI . 1234â€“1241.\n[109] Rong Zhu, Ziniu Wu, Yuxing Han, Kai Zeng, Andreas Pfadler, Zhengping Qian,\nJingren Zhou, and Bin Cui. 2021. FLAT: Fast, Lightweight and Accurate Method\nfor Cardinality Estimation. Proceedings of the VLDB Endowment 14, 9 (2021),\n1489â€“1502.\nA MORE EXPERIMENTS\nA.1 Details of Queries\nWe analyze the overlap ratio of low-frequency elements in the test\nand training data. Table 15 shows the result on the WIKI dataset,\nwhich includes the number of overlapped low-frequency elements\nand the number of distinct low-frequency elements that appeared in\nthe training data, respectively. It can be observed that the model canonly see a very small portion of the low-frequency elements in the\ntest data when training. Other datasets have similar observations.\nA.2 A Decomposition Baseline\nA naÃ¯ve baseline method is to replace the set columns as a table\n(essentially representing the bipartite graph) and replacing the set\nbased query with a regular query involving this new table and\napplying regular cardinality estimation methods. However, this\nmethod has shown worse performance than one of our baselines,\nPG, in the former study [ 40]. We omit it due to space limit. To\nverify this result, we conduct experiments on the WIKI dataset. In\nTable 16, DT and PG denote the decomposition method and the\nPG baseline in our paper, respectively. It can be observed that PG\noutperforms DT in most cases.\nTable 16: Comparison between the decomposed table (DT)\nand PG\nMethodSubset Superset Overlap\nMean 50% 95% 99% Mean 50% 95% 99% Mean 50% 95% 99%\nDT 22.5 5.98 106 199 294 33.4 584 2362 8.41 1.72 10.3 105\nPG 9.49 5.68 19.7 41.3 175 8 271 1785 8.28 1.81 13.1 94.2\nAlthough the decomposition method can be lossless, there are\ntwo major drawbacks. First, its storage overhead is extremely large.\nFor example, the decomposed table occupies 2,715 MB, 5 Ã—larger\nthan the original table. Second, set-valued queries need to be rewrit-\nten, making them much more complex than the original ones. One\nexample of the superset query is SELECT * FROM parent_table\np WHERE NOT EXISTS (SELECT * FROM child_table c WHERE\np.id = c. parent_id AND c.value NOT IN (val_1, val_2,\n..., val_n);\nSet-valued data has emerged as an essential data type in various\napplications to model the one-to-many relationship and to store\nmulit-valued attributes. Our proposed estimator aims to obtain\naccurate cardinality estimation results for queries containing set-\nvalued predicates thus generating more optimized query execution\nplans.\nA.3 The Study of Element Frequency\nWe conduct experiments on the effect of the query elementsâ€™ fre-\nquencies. We utilize the generation process mentioned in Section 7.1\nand obtain two collections of queries with only high-frequency and\nlow-frequency predicates, respectively. As shown in Figure 11, ACE\nperforms better on queries with low-frequency predicates because\na more accurate estimation result significantly affects the execution\nplan in such a case. Another interesting thing is that the E2E time of\nGS(P) and Set(P) on queries with low-frequency subset predicates\nis even greater than PG because a larger estimation error affects\nthe executed query plan.\nPGG\nS(P)S\net(P)STS\nTHA\nCE(P)015304560Time (s) High    Low\n(a) Subset predicate\nPGG\nS(P)S\net(P)STS\nTHA\nCE(P)010203040Time (s) High    Low (b) Superset predicate\nFigure 11: End-to-end query runtime.\n15",
  "textLength": 100887
}