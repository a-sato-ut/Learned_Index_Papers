{
  "paperId": "4bd2e12c920df98f209408fc0620925ceeca4ebd",
  "title": "Plan-Structured Deep Neural Network Models for Query Performance Prediction",
  "pdfPath": "4bd2e12c920df98f209408fc0620925ceeca4ebd.pdf",
  "text": "Plan-Structured Deep Neural Network Models for\nQuery Performance Prediction\nRyan Marcus\nBrandeis University\nryan@cs.brandeis.eduOlga Papaemmanouil\nBrandeis University\nolga@cs.brandeis.edu\nABSTRACT\nQuery performance prediction, the task of predicting the\nlatency of a query, is one of the most challenging problem\nin database management systems. Existing approaches rely\non features and performance models engineered by human\nexperts, but often fail to capture the complex interactions\nbetween query operators and input relations, and generally\ndo not adapt naturally to workload characteristics and pat-\nterns in query execution plans. In this paper, we argue that\ndeep learning can be applied to the query performance pre-\ndiction problem, and we introduce a novel neural network\narchitecture for the task: a plan-structured neural network .\nOur approach eliminates the need for human-crafted feature\nselection and automatically discovers complex performance\nmodels both at the operator and query plan level. Our novel\nneural network architecture can match the structure of any\noptimizer-selected query execution plan and predict its la-\ntency with high accuracy. We also propose a number of\noptimizations that reduce training overhead without sacri-\nficing effectiveness. We evaluated our techniques on various\nworkloads and we demonstrate that our plan-structured neu-\nral network can outperform the state-of-the-art in query\nperformance prediction.\n1 INTRODUCTION\nQuery performance prediction (QPP), the task of predict-\ning the latency of a query, is an important primitive for a\nwide variety of data management tasks, including admission\ncontrol [ 51], resource management [ 48], and maintaining\nSLAs [ 8,31]. QPP is also a notoriously difficult task, as the la-\ntency of a query is highly dependent on a number of factors,\nincluding, but not limited to, the execution plan chosen and\nthe underlying data distribution. As database management\nsystems become increasing complex, this task only gets more\ndifficult: each new operator or physical design component\ncan introduce new and complex interactions that can be very\ndifficult to model. Optimizer cost models, even when pre-\ncisely tuned, only attempt to differentiate between potential\nquery execution plans, and serve as poor predictors of query\nlatency on their own [13, 23].Previous approaches to query performance prediction\nhave focused on designing hand-derived metrics [ 9], train-\ning models based exclusively on plan-level information [ 54],\nproposing mathematical models of relational operators [ 25],\nor combining plan-level and operator-level information in\nad-hoc ways [ 4]. All of these methods depend on intelligent\nhuman feature engineering , the task of selecting or deriving\npieces of information from a query plan or query operator\nthat might correlate with its latency. Feature engineering, as\na manual process, generally requires significant effort from\nhuman experts, but, more importantly, scales poorly with\nthe increasing complexity of database management systems.\nIn this paper, we present a class of deep neural networks\n(DNNs) capable of performing query performance prediction\nin a “human-free” manner. DNNs have shown tremendous\nperformance on a number of machine learning tasks [ 46],\nand carry a number of advantages. First, deep neural net-\nworks require no human feature engineering beyond the\narchitecture of the network. During training, neural net-\nworks automatically derive and invent combinations of their\ninputs that serve as useful features, drastically diminishing\nthe need for human experts [ 20]. Second, DNNs are capable\nof learning complex models of their training data [ 15], alle-\nviating the need for ad-hoc and human-derived models of\nrelational operators and their combinatorial interactions.\nDespite the proven track record and massive growth of\nDNNs, applying deep learning to query performance predic-\ntion is not a straight-forward task. DNNs, like many other\nmachine learning algorithms, are designed to map input vec-\ntors to output vectors. However, to act as input to a DNN,\nquery plans need to be carefully vectorized to effectively\ncapture their performance-related properties, such as the\ntree-based structure of an execution plan, features of inter-\nmediate results, and all involved operators. Unfortunatelly,\nexisting works applying neural networks to tree structured\ndata [ 34,49] are ill-suited for the query performance predic-\ntion task. Specifically, branch isolation , the fact that a partic-\nular relational operator can only affect the performance of\nits parents and not its siblings, combined with heterogeneous\ntree nodes , the fact that different operators have different\nproperties (e.g., number of children, predicates, etc.), make\nquery execution plans a unique structure.arXiv:1902.00132v1  [cs.DB]  31 Jan 2019\n\nWe thus propose a novel deep neural network architec-\nture, a plan-structured neural network , specifically crafted for\npredicting the latency of query execution plans in relational\nDBMSes. Critically, our structure uses a unique neural unit ,\na small neural network, for each logical operator supported\nby a DBMS’ execution engine. These neural units can model\nthe latency of an operator while emitting “interesting” fea-\ntures to any subsequent operators in the query plan. These\nneural units can then be combined together into a tree shape\nisomorphic to the structure of a given execution plan, creat-\ning a single neural network which maps a query execution\nplan directly to a latency. By exploiting weight sharing , i.e.,\nthe property where the same neural unit is used for any\ninstance of the same operator across plans, these neural\nunits are capable of learning complex interactions between\noperators. When compared to simpler models [ 4,13,25],\nour approach automatically models both the performance\nof individual operators andthe interaction effects between\noperators, avoiding the need for costly human model design.\nThis paper makes the following contributions:\n•We introduce the notion of an operator-level neural\nunit, a deep neural network that models the behavior of\nlogical relational operators. Neural units are designed\nto produce (a) latency estimates and (b) performance-\nrelated properties that might be useful for the latency\nprediction of their parent operator in the plan.\n•We introduce plan-structured deep neural networks , a\nneural network model specifically designed to predict\nthe latency of query execution plans by dynamically\nassembling operator-level neural units in a neural net-\nwork isomorphic to a given query plan.\n•We propose optimizations for efficiently training our\nnovel neural network architecture, decreasing model\ntraining time by nearly an order of magnitude.\n•We present experimental results demonstrating that\nour approach outperforms state-of-the-art techniques\nfor query performance prediction.\nOur work marks the beginning of a new line of research\nthat aggressively uses deep learning to address complex data\nmanagement problems. While query performance prediction\nhas been studied extensively (e.g., [ 4,9,25,54]), this is the\nfirst approach that completely eliminates human-engineered\nfeatures and models as well as avoids simplified assump-\ntions made by previous work in attempt to make the query\nperformance prediction problem tractable by human experts.\nIn the next section, we provide background information\nabout deep neural networks. Section 3 outlines unique prop-\nerties of query execution plans that motivate our new deep\nneural network architecture. Section 4 describes our plan-\nstructured neural network model, and how it can be applied\nto query execution plans. In Section 5, we describe criticaloptimizations to make training a neural network for query\nperformance prediction tractable. We present experimental\nresults in Section 6, describe related work in Section 7, and\noffer concluding remarks and directions for future work in\nSection 8.\n2 NEURAL NETWORKS BACKGROUND\nDeep neural networks (DNNs) have a long-ranging history [ 15],\nand have recently enjoyed a surge in popularity [ 20]. This\nsection will cover the basics of DNNs and gradient descent,\nthe primary method for training neural networks. A more\ndetailed discussion can be found in [28].\nA DNN model is structured in layers , where the first layer\ntakes in a vector representing the input data, and each sub-\nsequent layer applies some transformation of the previous\nlayer’s output. Each layer consists of nodes which receives\ninput data, multiplies each input by a coefficient ( weight ),\nsums the result, and passes that sum through an activation\nfunction. The activation function introduces non-linearity, al-\nlowing neural networks to represent arbitrary functions [ 15].\nIntuitively, activation functions helps the network represent\nthe extent that a particular input value affects the ultimate\noutcome (e.g., “is this feature helpful is predicting latency\ndata without error?”), and hence determines whether and\nto what extent that value progresses further through the\nnetwork to affect the ultimate outcome (i.e., a node’s output\nis “activated” or “deactivated”).\nDNNs end in an output layer : a layer responsible for map-\nping the output of the penultimate layer to a prediction.\nDNNs are trained on a dataset, consisting of pairs of inputs\nandtargets , and aim to learn to accurately map a given input\nto the correct target. The quality of this mapping is measured\nby a loss function , which quantifies the difference between\nthe neural network’s prediction (output) and the ground\ntruth (target). DNNs learn via a process called gradient de-\nscent , a method that incrementally adjusts the transforma-\ntion performed by each layer (i.e., the weights), to minimize\nthe loss function. Training a DNN can be seen as a correc-\ntive feedback loop, rewarding weights that support correct\nguesses, punishing weights that lead to prediction errors,\nand slowly pushing the loss function towards smaller and\nsmaller values. In the process, the network takes advantage\nof correlations and patterns in the underlying data, creating\nnew, transformed representations of the data. Simultane-\nously, the network learns to recognize correlations between\nrelevant features and optimal predictions. Next, we describe\nthe above using more formal definitions, essential for the\nintroduction of our techniques.\n\n2.1 Layers\nEach layer in a DNN is composed of an affine transformation\nof its input1and a non-linear activation function . Given an\ninput vector®xof size n×1, the i-th layer of a network can\nbe defined as a function ti(®x)that provides an output vector\nof size m:\nti(®x)=S\u0010\nWi×®x+®bi\u0011\n(1)\nwhere Sis the activation function and Wiare the weights\nfor the i-th layer, represented by a matrix of size m×n.\nThe bias,®bi, is an m×1vector representing the constant\nshift of an affine transform. Together, the weights and the\nbiases represent the parameters of the neural network model,\nand control the properties of the transformation performed\nat each layer. The activation function, S, represents some\ndifferentiable but non-liner function: popular choices include\na sigmoid function or a rectified linear function [12].\nNeural network layers are composed together by feeding\nthe output of one layer into the input of the next. For a neu-\nral network with nlayers, where◦represents the function\ncomposition operator, a neural network can be defined as:\nN(®x)=tn◦tn−1◦. . .t1 (2)\nFor example, a two layer neural network Non an input\nvector®xcould be represented as:\nN(®x)=t2(t1(®x))=S\u0010\nW2×S\u0010\nW1×®x+®b1\u0011\n+®b2\u0011\n2.2 Gradient descent\nThe first step to training a neural network Nis defining a\nloss function , i.e. a function whose minimization is a suitable\ncriteria for the network to provide a good prediction. Let\nus assume a set of training input vectors Xand a labeling\nfunction l(®x)that provides the corresponding target value\nfor each vector®x∈X. For example, if Xis a set of vectors\nrepresenting query plans, l(®x)could be the latency of the\nquery plan represented by the vector ®x. The neural network\nNcan be trained to produce the target l(®x)when fed®xby\nminimizing a loss function [ 44]. One popular loss function is\nL2loss, or root mean squared error, which can be defined as:\nerr(X)=s\n1\n|X|Õ\n®x∈X\u0000N(®x)−l(®x)\u00012(3)\nGiven a loss function and a dataset, the next task is to adjust\nthe weights and biases of the neural network to minimize\nthe loss function. One popular technique for tweaking the\nweights and biases is with gradient descent [ 44]. Here, the\nactivation function Sis chosen to be differentiable so that\nthe loss function can be differentiated with respect to any\n1Different application domains may use more complex transformations.particular parameter. This allows us to calculate the gradi-\nent: the derivative of the loss function with respect to an\narbitrary parameter (i.e. a weight in Wior a bias in®bifor\nsome i). This represents how the loss function is affected by\nthe particular parameter (i.e., if a higher/lower weight/bias\nleads to higher/lower loss). The derivative of a particular\nparameter w(i.e., a single element inside of a weight matrix\nor bias vector) can be written as the following function:\n∇w(err,X)=∂err(X)\n∂w(4)\nIn other words, the gradient of a parameter of the neural\nnetwork (weights and biases) is the derivative of the loss func-\ntion with respect to this particular parameter. This gradient\ncan then be evaluated given the input vectors Xand their\ncorresponding target values l(·).\nGradient descent works by adjusting each weight of the\nneural network independently. The gradient descent algo-\nrithm first computes the gradient (Equation 4) of a given neu-\nral network parameter, w. If the gradient is positive, meaning\nthat an increase in this parameter would (locally) lead to a\nincrease in the loss function, the weight wis decreased. If\nthe gradient is negative, meaning that an increase in this\nparameter would (locally) lead to a decrease in the loss func-\ntion, the weight wis increased. After adjusting the weight,\nthe algorithm then repeats this procedure for each param-\neter in the network. This simple procedure is iterated until\ngradients of all parameters (weight and biases) are relatively\nflat (i.e., convergence).\nIn practice, computing the gradient for the entire dataset\n(i.e., all of input vectors and target values), is prohibitive.\nThus, stochastic gradient descent instead takes a simple ran-\ndom sample of the input vectors and their corresponding\ntarget values and uses these to estimate the gradient [44].\n3 DL-BASED LATENCY PREDICTION:\nCHALLENGES\nDespite their numerous advantages, it is difficult to apply\ntraditional deep neural networks to the query performance\nprediction task. A straightforward application of deep learn-\ning would be to model the whole query as a single neural\nnetwork and use query plan features as the input vector.\nHowever, this naive approach ignores the fact that the query\nplan structure, features of intermediate results, and non-leaf\noperator are often correlated with query execution times and\nhence can be useful in any predictive analysis task.\nFurthermore, query plans are diverse structures – the type\nand number of operators varies per plan, operators have\ndifferent correlations with query performance, and opera-\ntors have different sets of properties and hence different\nsets of predictive features. Traditional DNNs have static net-\nwork architectures and deal with input vectors of fixed size.\n\nσ\nR1\nR2R3\nσ\nσ\nR1\nR2R4\nσFigure 1: In query processing, changes in one branch of the\nquery plan cannot effect any node outside of their ancestors.\nHere, changing the rightmost relation from R3toR4will not\nhave any effect on the leftmost filter operation of R1.\nHence, “one-size-fits-all” neural network architectures do\nnot fit the query performance prediction task. Finally, while\nprevious work in the field of machine learning has examined\napplying deep neural networks to sequential [ 14] or tree-\nstructured [ 43,49] data, none of these approaches are ideal\nfor query performance prediction, as we describe next.\nIsolated branches Neural network architectures proposed\nfor processing tree-structured data are popular in natural\nlanguage processing [ 43,49], and are built around the as-\nsumption that a modification to one branch of a tree can have\na drastic impact on other branches, allowing tree branches to\nshare information. However, in the context of a query execu-\ntion plan, the characteristics and performance of one branch\nof the query execution plan tree are reasonably isolated from\nother branches. Specifically, we know that a particular op-\nerator can only affect its ancestors, and can never affect its\nsiblings. For example, consider the two query execution plans\nshown in Figure 1. Changing R3in the first plan to R4in the\nsecond plan cannot affect the performance of R1or its filter.\nHeterogeneous tree nodes Traditional neural networks\noperate on input vectors of a fixed structure. However, in a\nquery execution plan, each type of operator has fundamen-\ntally different properties. A join operator may be described\nby the join type (e.g. nested loop join, hash join), the es-\ntimated required storage (e.g., for an external sort), etc. A\nfilter operation, however, will have an entirely different set\nof properties, such as selectivity estimation or parallelism\nflags. Since feature vectors of different operators are likely\nof different sizes, simply feeding them into the same neural\nnetwork is not possible.\nA naive solution to this problem might be to concatenate\nvectors together for each relational operator. For example, if\na join operator has 9 properties and a filter operator has 7\nproperties, one could represent either a join or a filter opera-\ntor with a vector of size 9+7=16properties. If the operator\nis a filter, the first 9 entries of the vector are simply 0, and thelast 7 entries of the vector are populated. If the operator is a\njoin, the first 9 entries of the vector are populated and last 7\nentries are empty. The problem with this solution is sparsity :\nif one has many different operator types, the vectors used to\nrepresent them will have an increasingly larger proportion\nof zeros. Generally speaking, such sparsity represents a ma-\njor problem for statistical techniques [ 22], and transforming\nsparse input into usable, dense input is still an active area\nof research [ 52,53]. In other words, using sparse vectors to\novercome heterogeneous tree nodes replaces one problem\nwith a potentially harder problem.\nPosition-independent operator behavior As pointed out\nby previous work [ 13,25], two instances of the same opera-\ntor (e.g., join, selection, etc), will share similar performance\ncharacteristics, even when appearing within different plans\nor multiple times in the same plan. For example, in the case\nof a hash join, latency is strongly correlated with the size\nof the probe and search relations, and this correlation holds\nregardless of the operator’s position in the query execution\nplan. This indicates that one could potentially train a neural\nnetwork model to predict the performance of a hash join\noperator, and that same model can be used any time the\nhash-join operator appears in a plan.\n4 PLAN-STRUCTURED DNNS\nTaking the above observations into account, this paper pro-\nposes a new tree-structured neural network architecture, in\nwhich the structure of the network matches the structure of\na given query plan. This plan-structured neural network con-\nsists of operator-level neural networks (called neural units )\nand the entire query plan is modeled as a tree of neural units.\nOn its own, each neural unit is expected to (1) predict the\nperformance of an individual operator type – for example,\nthe neural unit corresponding to a join predicts the latency\nof joins – as well as (2) “interesting” data regarding the op-\nerator that could be useful to the parent of the neural unit.\nThe plan-level neural networks is expected to predict the\nexecution time of a given query plan.\nNext, we discuss our proposed model in more detail, start-\ning with the operator-level neural units and moving on with\nthe plan-structured neural network architecture.\n4.1 Operator-level neural units\nOur proposed approach models each logic operator type sup-\nported by a DBMS’ execution engine with a unique neural\nunit, responsible for learning the performance of that par-\nticular operator type, e.g., a unique unit for joins, a unique\nunit for selections, etc. These neural units aim to represent\nsufficiently complex functions to model the performance\nof relational operators in a variety of contexts. For exam-\nple, while a simple polynomial model of a join operator may\n\nmake predictions only based on estimated input cardinalities,\nour neural units will automatically identify the most rele-\nvant features out of a wide number of candidate inputs (e.g.,\nunderlying structure of the table, statistics about the data\ndistribution, uncertainty in selectivity estimates, available\nbuffer space, etc.), all without any hand-tuning.\nInput feature vectors Let us define as®x=F(x)a vector\nrepresentation describing x, an instance of a relational oper-\nator. This vector will act as an input to the neural unit of that\nparticular operator. These vectors could be extracted from\nthe output of the query optimizer, and contain information\nsuch as: the type of operator (e.g., hash join or nested loop\njoin for join operators, etc.), the estimated number of rows\nto be produced, the estimated number of I/Os required, etc.\nMany DBMSes expose this information through convenient\nAPIs, such as EXPLAIN queries. For examples, see Appendix B,\nwhich lists the features used in our experimental study for\nseveral operators. Note that the size of the input vector may\nvary based on its corresponding operator: input vectors for\njoin operators would have a different set of properties and\nthus different sizes than the input vectors for selection op-\nerators. However, every instance of a relational operator of\na given type will have the same size of input vector, e.g. all\njoin operators have the same size input vectors.\nOutput vector Performance information for an operator\ninstance xare often relevant to the performance of its parent\noperator in a query execution plan. To capture this, and\nallow for flow of information between operator-level neural\nunits, each neural unit of an operator type will output both\na latency prediction and a data vector . While the latency\noutput predicts the operator’s latency, the output data vector\nrepresents “interesting” features from the child operator that\nare relevant to the performance of the parent operator. For\nexample, a neural unit for a scan operator may produce\na data vector that contains information about the expected\ndistribution of the produced rows. We note these data vectors\nare learned automatically by the model during its training\nphase, without any human interference or selection of the\nfeatures that appear in the output vector.\nNeural units Next, we define a neural unit as a neural net-\nwork NA, with Arepresenting a type of relational operator,\ne.g.N▷◁is the neural unit for join operators. For each in-\nstance aof the operator type Ain a given query plan, the\nneural unit NAtakes as input the vector representation of\nthe operator instance a,®xa.\nThis input is fed through a number of hidden layers, with\neach hidden layer generating features by applying an acti-\nvated affine transformation (as defined by Equation 1). These\ncomplex transformations can be learned automatically us-\ning gradient descent methods, which gradually adjusts the\nweights and bias of the neural unit NAin order to minimize\nits loss function (as described in Section 2.2). The last layer\ndata\nlatencyFigure 2: Neural unit corresponding to a scan operator, Ns.\nInput features are mapped through a number of hidden lay-\ners and one output layer.\ntransforms the internal representation learned by the hidden\nlayers into a latency prediction and an output data vector.\nFormally, the output of a neural unit NAis defined as:\n®pa=NA\u0000®a\u0001, when ais a leaf (5)\nwhere ais the instance of the operator type A. The output\nvector has a size of d+1. The first element of the output\nvector represents the neural unit’s estimation of the oper-\nator’s latency, denoted as ®pa[l]. The remaining delements\nrepresent the data vector, denoted as ®pa[d]. We note that\nsince the input vectors to different neural units will not have\nthe same size, each neural unit may have different sizes of\nweight and bias vectors that define the neural unit, but their\nfundamental structure will be similar.\n4.1.1 Leaf neural units (scan). The simplest neural units are\nthose representing leaf nodes of the query plan tree and are\nresponsible for accessing data from the database. Following\nPostgreSQL terminology, we refer to these as scan operators\nto distinguish them from the selection operator that filters\nout intermediate data.\nFor a given instance sof the scan operator type S, the\nneural unit NStakes as input the raw vector representation of\nthe operator instance s,®xs, and produces an output vector ®ps.\nSince these operators access stored data, their corresponding\nvectors include (among others) information about the input\nrelation of the scan operator. These features are collected\nthrough the optimizer (or various system calls). We refer to\nthe function collecting this information for an instance aof\nthe operator AasF(a).\nFigure 2 shows an illustration of a neural unit for a scan\noperator NS. The unit takes information from the query plan\n(e.g., index/table scan, optimizer’s cost, cardinality estimates,\nestimated I/Os, memory availability, etc) as input. By running\nthe raw vector representation of a scan operator through\nmany successive hidden layers, the neural unit can model\ncomplex interactions between the inputs. For example, a\nseries of activated affine transformations might “trust” the\noptimizer’s cost model more for certain types of scans, or for\nscans over particular relations. The neural unit transforms\nthe input vector into a latency prediction and a output data\nvector. Possible output data features here might be distribu-\ntion information of the rows emitted by the scan.\n\njoin type\noptimizer cost\n…left child \ndata outputs\nleft child latency \nprediction\nright child \ndata outputs\nright child latency \npredictionHidden layers\ndata\nlatency\nInput layerOutput layerFigure 3: Neural unit corrosponding to a relational join op-\nerator, N▷◁. The neural unit takes input from two children\nand several optimizer features.\n4.1.2 Internal neural units. Having constructed neural units\nfor each leaf operator type, we next explain how the internal\noperators, i.e., operators with children in a query execution\nplan, can be modeled using neural units. Like leaf operators,\nthe neural units for the internal operator instance xof the\nquery execution plan will take an operator-specific input\nvector into account, provided by the function F(x). However,\nthe performance of the internal operators depends also on\nthe behavior of its children. Hence, each internal neural unit\nreceives also as input the latency prediction and the output\ndata vector of its children.\nA neural unit for an internal operator type Ais represented\nas a neural network NA. Given a query execution plan where\nan operator instance aof type Areceives input from op-\nerators xi,i∈{1, ...n}, the input vector of NAwill be the\noperator-related information produced by F(a)in addition\ntothe output vectors produced by the operator’s children :\n®pa=NA(F(a)⌢®pxi⌢···⌢®pxn) (6)\nwhere ⌢represents the vector concatenation operator.\nSince the operator’s raw input vector and the outputs of its\nchildren’s neural units are concatenated together and passed\nas an input to another neural network, the loss function of\nthe entire neural network architecture is still differentiable\nwith respect to any weight. This guarantees that the network\ncan still be trained using standard gradient descent methods.\nFigure 3 shows an example of an internal neural unit,\ncorresponding to a join operator, N▷◁. The unit takes infor-\nmation about the join operator itself (e.g., the type of join, the\noptimizer’s predicted cost, cardinality estimates, etc.) as well\nas information from the join operator’s children. Specifically,\nthe join neural unit will receive both the data vector and\nlatency output of its left and right child. These inputs are fed\nσ\nR1R2Query Execution \nPlanLatency Prediction \nNeural Network\nσ\nShared weightsFigure 4: Each node in a query execution plan is mapped to\na neural unit corresponding to the relational operator.\njoin type\noptimizer cost\n…data\nlatencyScan Unit Scan UnitJoin Unit\nR1R2\nFigure 5: A neural network for a simple join query\nthrough a number of hidden layers and is transformed into\na final output vector, where the first element of the output\nvector represents the predicted latency and the remaining\nelements represent the data output features. This allows N▷◁\nto be further composed with other neural units.\n4.2 Trees of neural units\nNext, we show how these neural units can be composed into\ntree structures isomorphic to any particular query execution\nplan. Figure 4 shows an example of a query execution plan\nand the isomorphic plan-structured neural network. Intu-\nitively, each operator in a query execution plan is replaced\nwith its corresponding neural unit, e.g. join operators are\nreplaced with N▷◁, and the output of each neural unit is fed\ninto the parent. The latency of the query execution plan is\nthe first element of the output vector ®pr, where ris the in-\nstance operator on the root of the query execution plan. Note\nthat the recursive definition (Equation 6) of ®prwill “replace”\neach relational operator with its corresponding neural unit\nin a top-down fashion.\nFigure 5 shows an example of this construction. For the\nquery execution plan shown in the bottom-right of the figure\n(two scans and a join), two instances of the scan neural unit\nand one instance of the join neural unit are created. The\noutputs of the scan units are concatenated together with\n\ninformation from query \nplanData\nLatency\nQuery plan\ninformationNSNSNσN\nσ\nR1R2\nFigure 6: General neural network for latency prediction\ninformation about the join operator to make the input for\nthe join unit, which produces the final latency prediction.\nFigure 6 shows a more general example, with a query plan\n(top left) and the corresponding neural network tree (right).\nEach neural unit, represented as trapezoids, takes in a num-\nber of inputs. For the leaf units (orange, corresponding to\nthe table scans in the query plan), the inputs are information\nfrom the query plan (black arrows). The internal, non-leaf\nunits take information from the query plan as well, but ad-\nditionally take in the latency output (green arrows) and the\ndata outputs (red arrows) of their children. The latency out-\nputs represent the model’s estimate of the latency of each\noperator, and the data outputs contain information about\neach operator that may be useful to the parent operator (for\nexample, the table scan neural unit may encode data about\nwhich relation is being read). Here, the two orange trape-\nzoids correspond to the scan operators of R1andR2in the\nquery plan, and thus use the same neural unit.\n4.3 Model benefits\nOur plan-structured neural network model eliminates a num-\nber of aforementioned challenges, while leveraging a number\nof plan-structured properties (see Section 3).\nBranch isolation Since we know that any particular rela-\ntional operator in a query execution plan can only affect the\nperformance of its ancestors, and not its siblings or children,\nwe say that a query execution plan exhibits branch isolation.\nThe way we assemble neural units into trees respects this\nproperty: each neural unit passes information exclusively\nupwards. Intuitively, this upwards-only communication pol-\nicy directly encodes knowledge about the structure of the\nquery execution plan into the network architecture itself.\nHeterogeneous tree nodes Operator-level neural unit ac-\ncept input vectors of different size depending on the operator\nthey model while producing a fixed-sized output vector. Thisenables the structure of the plan-structured neural network\nto dynamically match any given query plan, thus making\nour model suitable to handle arbitrary plans. For example,\nregardless of if the child of a join operator is a filter (selec-\ntion) or a scan, its child neural unit will produce a vector of\na fixed size, allowing this output vector to be connected to\nthe neural unit representing the join operator.\nPosition-independent operator behavior Since we ex-\npect a particular operator to have some common perfor-\nmance characteristics regardless of its position in the query\nexecution plan, the same neural unit is used for every in-\nstance of a particular operator. Because the same query execu-\ntion plan can contain multiple instance of the same operator\ntype (e.g., multiple joins), our architecture can be considered\narecurrent neural network [26], and as such benefits from\nweight sharing : since instances of the same operators share\nsimilar properties, representing them with a single neural\nunit (and thus a a single set of weights and bias) is both effi-\ncient and effective. However, since distinct operator types are\nrepresented by different neural units (and thus will notshare\nthe same weights and bias), our approach can handle the\nheterogenous nature of the query execution plan operators.\n5 MODEL TRAINING\nSo far, we have discussed how to assemble neural units into\ntrees matching the structure of a given query execution plan.\nIn this section, we describe how these plan-structured neural\nnetworks are trained. Training is the process of progres-\nsively refining the network’s weights and bias (in our case,\nthe weight and bias of the neural units included in a plan-\nstructured neural network) to minimize the loss function\nusing gradient descent (as discussed in Section 2).\nInitially, each hidden layer, and the final latency and data\nvector output for each neural operator, will simply be a ran-\ndom activated affine transformation. In other words, the\nweights and bias that define the transformations (Equation 1)\nare initially picked randomly. Through repeated applica-\ntions of gradient descent, these transformations are slowly\ntweaked to map their inputs slightly closer to the desired\ntarget outputs.\nThis training process is performed using a large corpus\nof executed query plans. Formally, for a dataset of executed\nquery execution plans, let Dbe the set of all query operator\ninstances within those plans. Then, for each query operator\no∈D, letl(o)be the latency of the operator. The neural units\nare trained by minimizing the following loss function:\nL2(D)=s\n1\n|D|Õ\no∈D\u0000®po[l]−l(o)\u00012(7)\nwhere®po[l]represents the latency output of the operator o’s\nneural unit (the neural unit’s prediction).\n\nNote that, if a particular query operator instance ois not a\nleaf in its query execution plan, the evaluation of its output\n®powill involve multiple neural units, based on the recursive\ndefinition given by Equation 6. The loss function L2(D)thus\nrepresents the prediction accuracy of leaf operators, internal\noperators, andthe root operator of a query execution plan.\nMinimizing this loss function thus minimizes the prediction\nerror for allthe operators in the training set.\nIntuitively, Equation 7 is simply the combined differences\nbetween the latency the model predicted for each operator\nand the observed, “ground truth” latency. The loss function\nexplicitly compares the predicted latency of a particular op-\nerator®po[l]with the “ground truth” latency l(op).\nHowever, it is important to note that the loss function\ndoes notexplicitly compare the data vector of the output ®po\nto any particular value. The gradient descent algorithm is\nthus free to tweak the transformations creating the output\ndata vector to produce useful information for the parent\nneural unit consuming the data vector output. To exemplify\nthis, consider evaluating the output ®pjfor a simple query\nplan involving the join jof two relation scans, s1ands2, as\nillustrated in Figure 5. Following Equation 6, and defining\nasN▷◁the neural unit for joins and NSthe neural unit for\nscans, we can expand the output vector ®pjas follows:\n®pj=N▷◁\u0010\nF(j)⌢®ps1⌢®ps2)\n=N▷◁\u0010\nF(j)⌢NS(F(s1))⌢NS(F(s2))\u0011\n=N▷◁\u0010\nF(j)⌢[®ps1[l]⌢®ps1[d]]⌢[®ps2[l]⌢®ps2[d]]\u0011\nwhere®ps[d]represents the output data vector for the neural\nunit of operator s.\nThus, the transformations producing the output data vec-\ntor for each neural unit are adjusted by the gradient descent\nalgorithm to minimize the latency prediction error of their\nparents . In this way, each neural unit can learn what infor-\nmation about its represented operator type is relevant to the\nperformance of the parent operator automatically , without\nexpert human analysis. Because the training process does\nnot push output data vectors to represent any pre-specified\nvalues, we refer to these values as opaque , as the exact seman-\ntics of the output data vector will likely vary significantly\nbased on context, and may be difficult to interpret directly,\nas is generally the case with recurrent neural networks [ 26].\n5.1 Training optimizations\nThe training overhead of our plan-structured neural network\nmodel can be significant due to the large number of operators\nthat might appear in a query execution plan, and thus we\nwish to train multiple neural units in parallel. To address this\nchallenge, we propose two optimizations over naive trainingmethods. These techniques aim to improve the performance\nofcomputing of the loss function of a plan-structured neural\nnetwork (Equation 7) in the context of gradient descent. Sec-\ntion 5.1.1 explains how the loss function can be computed\nefficiently in a vectorized way. Section 5.1.2 notes that com-\nputing the loss function can be accelerated by exploiting the\ntree structure of a query execution plan.\n5.1.1 Batch training. Gradient descent minimizes a neural\nnetwork’s loss function by tweaking each weight by a small\namount based on the gradient of that weight (Equation 4).\nIdeally, the gradient of each weight should be evaluated over\nthe whole training set. However, since neural networks are\ntrained over very large datasets, naive implementations of\ngradient descent are space prohibitive. Furthermore, since\nreal-world neural networks can have a significant number of\nweights, naive implementations of gradient descent which\nupdate each weight sequentially are time prohibitive.\nTo reduce space usage, modern differentiable program-\nming frameworks [ 2,39] preform training in batches . Instead\nof computing the gradient using the entire dataset (which\ncannot effectively fit into memory), simple random samples\n(called batches or mini-batches) are drawn from the data\nand used to estimate the gradient and adjust the weights.\nThis widely-adopted technique is called stochastic gradient\ndescent [44]. Since each sample is selected at random, the\nestimation of the gradient is unbiased [7].\nTo reduce the time required, modern neural network li-\nbraries take advantage of vectorization (i.e., applying mathe-\nmatical operators to entire vectors simultaneously) to speed\nup their models. To do so, neural networks are assumed to\nbe using a fixed architecture, e.g. an architecture that does\nnot change based on the particular input. This is the case in\nmany applications, such as computer vision. By assuming\na fixed architecture, libraries can assume that the symbolic\ngradient of each weight will be identical for each item in\nthe batch (i.e., the derivative of any weight can be computed\nusing the same sequence of mathematical operations), and\nthus their computation can be vectorized.\nStochastic gradient descent and vectorization work very\nwell for neural networks where the structure of the network\ndoes not change based on the inputs. However, these opti-\nmization poses a challenge for our proposed plan-structured\nneural network model: if two samples (i.e., query plan) in\na batch have different tree structures, the symbolic deriva-\ntive for a given weight will be vary depending on the input\nsample, and thus the sequence of mathematical operations\nneeded to compute the derivative of two given weights can\ndiffer. Thus, vectorization cannot be directly applied.\nOne solution might be to group the training set into query\nexecution plans with identical structure, and then use each\n\ngroup as a batch. While this would both reduce memory re-\nquirements and allow for standard vectorization approaches\nto be used, most of the effectiveness of stochastic gradient\ndescent depends on the batch being a true simple random\nsample, and thus providing an unbiased estimate of the gra-\ndient [ 7]. By only creating training batches with identical\nquery plans, each batch, and thus each estimation of the\ngradient, will become biased.\nPlan-based batch training To address these challenges, we\nproposes constructing large batches of randomly sampled\nquery plans. Within each large batch B, we group together\nsets of query plans with identical structure, i.e. we parti-\ntionBinto equivalence classes c1,c2, . . . , cnbased on plan’s\ntree structure, such thatÐn\ni=1ci=B. Then, we estimate the\ngradient as follows:\n∇w(L2,B)=1Ín\ni=1|ci|×nÕ\ni=1\u0012\n|ci|∂L2(ci)\n∂w\u0013\nThe gradient of each weight for each operator is then effi-\nciently estimated within each inner-batch group, and the\nresults are added together and normalized based on group\nsize, ensuring the estimate of the gradient is not biased.\nExample : For example, if a randomly-sampled large batch\ncontained three groups of query execution plans with dis-\ntinct tree structures, B={c1∪c2∪c3}, with 10, 20, and\n300 members each, the gradient of each weight would be\nestimated as follows:\n∇w(L2,B)=1\n10+20+300×\n\u0012\n10∂L2(c1)\n∂w+20∂L2(c2)\n∂w+300∂L2(c3)\n∂w\u0013\nThis approach avoids biasing the gradient estimate while\nstill gaining efficiency from batch processing.\n5.1.2 Information sharing in subtrees. Let us assume that\nrof type Ris the root operator of a query execution plan,\nandcis the sole child of that operator. When computing the\nloss function of the query execution plan’s neural network,\nestimating the latency prediction error of the root, (®pr[l]−\nl(r))in Equation 7, requires us to compute the output vector\nof its child c,®pc, as an intermediate value. This follows from\nthe definition of®prin Equation 6, based on which ®pr=\nNR(F(r)⌢®pc). Since computing the loss function will also\nrequire computing ®pc, i.e. in the term(®pc−l(c)), we can avoid\na significant redundant computation by caching the value of\n®pcso that it only needs to be computed once.\nMore generally, for an arbitrary root r∈Dof a query\nexecution plan, we can compute the error of each neural\nunit in the plan’s neural network rooted at rin a bottom-\nup fashion: first, for each leaf node leaf in the tree rootedatr, compute and store the output the neural units corre-\nsponding to the leaf nodes, ®pleaf. Then, compute and sum\nthe(®pleaf[l]−l(leaf))2values, storing the result into a global\naccumulator variable. Once all the leaf nodes have been re-\nsolved in this way, repeat the process moving one level up the\ntree. When the root of the tree has been reached, the global\naccumulator value will contain the (®px[l]−l(x))2values for\nevery operator xin the tree rooted at r. The global accumula-\ntor contains the sum of the squared differences between the\npredicted latency and the actual latency for every node in\nthe tree. Applying this technique over every plan-structured\nneural network in Dcan greatly accelerate the computation\nof our loss function as defined in Equation 7.\n6 EXPERIMENTAL RESULTS\nIn this section, we describe the experimental study we con-\nducted for our proposed plan-based neural network model.\nIn all our experiments, our queries were executed on Post-\ngreSQL [ 1] on a single node with an Intel Xeon CPU E5-2640\nv4 processor, 32GB of RAM, and a solid-state drive.\nWorkload We conducted experiments using TPC-H [ 42], a\ndecision support benchmark, and TPC-DS [ 36], a decision\nsupport benchmark with a focus on higher volumes of data\nand more complex queries. All TPC-H query templates were\nused but only seventy (70) TPC-DS query templates are com-\npatible with PostgreSQL (without modification), hence we\nuse only these templates for TPC-DS. For both benchmarks,\n20,000 queries were executed with a scale factor of 100GB.\nEach query was executed from a “cold cache” state (both\nthe OS and PostgreSQL cache) in isolation (no multiprocess-\ning). Execution times and execution plans were recorded\nusing PostgreSQL’s EXPLAIN ANALYZE capability. The input\nfeatures used for each neural unit are those that PostgreSQL\nmakes available through the EXPLAIN command before a\nquery is executed. See Appendix B for a listing.\nTraining data The queries was split into a training set and a\ntesting set in two different ways. For the TPC-DS queries, all\nof the instances of 10 randomly selected query templates are\n“held out” of the training set (e.g., the neural network trains\non 60 query templates, and the performance of the network\nis measured on instances of the unseen 10 query templates).\nFor the TPC-H data, since there are not enough query tem-\nplates to use the same strategy, 10% of the queries, selected\nat random, are “held out” of the training set (e.g., the neural\nnetwork trains on 90% of the data, and the performance of\nthe network is measured on the other 10%).\nNeural networks Unless otherwise stated, each neural unit\nhad 5 hidden layers, each with 128 neurons each. The data\noutput size was set to d=32. Rectified linear units (Re-\nLUs [ 12]) were used as activation functions. Standard sto-\nchastic gradient descent (SGD) was used to train the network,\n\nwith a learning rate of 0.001 and a momentum of 0.9. Training\nwas conducted over 1000 epochs (full passes over the training\nqueries), which consistently produced the reported results.\nWe used the PyTorch [ 39] library to implement the neural\nnetwork, and we used its built-in SGD implementation.\nEvaluation techniques We compare our plan-based neural\nnetwork model ( QPP Net ) with three other latency prediction\napproaches:\n(1)SVM-based models ( SVM): We implemented the learning-\nbased approach proposed in [ 4], the state-of-the art\nin latency prediction for relational queries with no\nexplicit human modeling. Here, a regression variant\nof SVM (Support Vector Machine) models are built for\neach operator while selective applications of plan-level\nmodels are used in situations where the operator-level\nmodels are likely to be inaccurate. In contrast to our\napproach, the set of input vectors for both the oper-\nator and plan level models are hand-picked through\nextensive experimentation.\n(2)Resource-Based Features ( RBF): We also implemented\na predictive model that take as input the features pro-\nposed by [25]. Although these features are picked for\npredicting resource utilization of query operators and\nby extension query plans, resource usage can be good\nindicator of query performance in non-concurrent\nquery executions. Hence we modified the MART re-\ngression trees used in [ 25] to predict query latency.\nSimilarly to the SVMapproach, the input features of\nthis model are hand-picked and not automatically en-\ngineered as in QPP Net . However, unlike the SVMap-\nproach, the RBFapproach uses human-derived models\nfor capturing operator interactions.\n(3)Tuned Analytic Model ( TAM): We also implemented a\nversion of the tuned optimizer cost model model pro-\nposed in [ 13]. This approach uses the optimizer cost\nmodel estimate to predict query latency. First, some\n“calibration queries” are ran to determine the coeffi-\ncients for the “calibrated cost model.” Then, this cali-\nbrated cost model is used to predict the query latency\nusing the optimizer’s cardinality estimates as inputs.2\nTheTAMapproach is thus entirely human-engineered,\nexcept for a sparse number of tuned parameters that\nare adjusted using special calibration queries.\nEvaluation metrics To evaluate the prediction accuracy of\nthese techniques, we use two metrics: relative prediction error\nandmean absolute prediction error . The relative prediction\nerror has been used in [ 4,25] and can be defined as follows.\nLetting Qbe the set of test queries, letting predicted(q∈Q)\n2For consistency, our version of [ 13] uses optimizer estimates of cardinalities\nas inputs without the proposed “data sampling” optimization.be the predicted latency of q, and letting actual(q∈Q)be\nthe actual latency, the relative prediction error is:\n1\n|Q|Õ\nq∈Q|actual(q)−predicted(q)|\nactual(q)\nHowever, the “relative error” metric has several known\nflaws [ 50]. Specifically, relative error systematically favors\nunderestimates due to the asymmetry in the error func-\ntion. No matter how bad an under-prediction is, the worst\nvalue the relative error can take on is 0. However, for over-\npredictions, the relative error is unbounded, hence the asym-\nmetry. Because of the issues with relative error, we also\nreport the mean absolute error, a standard metric for evalu-\nating regression [ 41], which symmetrically penalizes under\nand over estimations:\n1\n|Q|Õ\nq∈Q|actual(q)−predicted(q)|\nA useful property of mean absolute error is that it shares the\nsame units as the regression target. In our case, since we are\npredicting a quantity in units of time, the units of the mean\nabsolute error are also time units.\nWe also report R(q), the maximum of the ratio between the\nactual and the predicted and the ratio between the predicted\nand the actual (not to be confused with the coefficient of\ndetermination):\nR(q)=max\u0012actual(q)\npredicted(q),predicted(q)\nactual(q)\u0013\nIntuitively, the R(q)value represents the “factor” by which\na particular estimate was off. For example, if a model esti-\nmates a query’s qlatency to be 2 minutes, but the latency of\nthe query is actually 1 minute, the R(q)value would be 2, as\nthe model was off by a factor of two. Similarly, if the model\nestimates a query’s latency to be 2 minutes, but the latency\nof the query is actually 4 minutes, the R(q)value would also\nbe 2, as the model was again off by a factor of two.\n6.1 Prediction Accuracy\nThe accuracy of each method at estimating the latency of\nqueries in the TPC-H and TPC-DS workloads are shown in\nFigure 7a. The results reveal that out neural network ap-\nproach outperforms the other baselines. The relative error\nimproved by 9% (TPC-DS) and 5% (TPC-H) over RBF, by 25%\n(TPC-DS) and 24% (TPC-H) over SVMand by 28% (TPC-DS)\nand 21% (TPC-H) over TAM. In terms of absolute error, the\naverage error decreased by 11 minutes (TPC-DS) and 7 min-\nutes (TPC-H) from RBF, and by 18 minutes (TPC-DS) and 15\nminutes (TPC-H) from SVMand 21 minutes (TPC-DS) and 13\nminutes (TPC-H) from TAM.\nThe larger improvement seen in TPC-DS is due to two\nfactors: (1) the fact that the average TPC-DS query plan has\n\n 0 20 40 60 80 100\nTPC-DS TPC-HRelative error (%)TAM\nSVMRBF\nQPP Net\n 0 10 20 30 40 50\nTPC-DS TPC-HMean absolute error (m)(a) Relative error and mean absolute error (lower is better)\n 0 1 2 3 4 5\n 0  0.2  0.4  0.6  0.8  1R (error factor)\nTPC-DSTAM\nSVMRBF\nQPP Net\n 0 1 2 3 4 5\n 0  0.2  0.4  0.6  0.8  1R (error factor)\nTPC-H\n(b) Cumulative error factors on both datasets. The x-axis sig-\nnifies the proportion of the test set for which the algorithm\nachieved an Rscore below the value on the y-axis.\nFigure 7: Comparison of prediction accuracy\nmore operators in it than the average TPC-H query plan (22\noperators vs 18 operators), resulting in QPP Net being able\nto take advantage of a larger amount of training data, and\n(2),QPP Net is capable of learning the more complex inter-\nactions present in the TPC-DS workload effectively. Overall,\nthe hand-picked features used by the other techniques fail to\ncapture the complex interaction between operators, while our\nautomatic features selection approach outperforms all human-\ncrafted techniques, with more significant gains when the query\nworkload is more complex.\n6.1.1 Prediction distribution. We analyzed how frequently\neach model’s prediction was within a certain relative range\nof the correct latency. We report the percentage of the test set\nfor which each model’s prediction was closer than a factor\nof 1.5 from the actual latency, between a factor of 1.5 and\n2 of the actual latency, and greater than a factor of 2 from\nthe actual latency. Tables 1a and 1b display the results for\nTPC-DS and TPC-H, respectively.\nFor both workloads, QPP Net has the best performance in\nterms of the proportion of the test set with an error factor\nless than 1.5. A high percentage of its predictions (89% for\nTPC-DS and 93% for TCP-H) are only within a factor of 1.5\nof the actual latency, outperforming TAMby 38% (TPC-DS)\nand 15% (TPC-H), SVMby 21% (both TPC-H and TPC-DS) and\nRBFby 4% (TPC-DS) and 5% (TPC-H). The results indicate\nthat our approach offers predictions closer to the real latency\nfor a significantly higher number of queries.Model R≤1.5 1.5<R<2.0 2.0≥R\nQPP Net 89% 7% 4%\nTAM 51% 22% 27%\nSVM 68% 15% 17%\nRBF 85% 6% 9%\n(a) TPC-DS\nModel R≤1.5 1.5<R<2.0 2.0≥R\nQPP Net 93% 6% 1%\nTAM 78% 17% 5%\nSVM 72% 20% 8%\nRBF 88% 6% 6%\n(b) TPC-H\nTable 1: Percentage of the test set where a particular\nmodel’s estimate was within a factor of 1.5 of the cor-\nrect latency, within some factor between 1.5 and 2, and\nnot within a factor of 2.\nWe additionally plot the distribution of R(д)values in Fig-\nure 7b, in the style of cumulative density function. Each plot\nshows the largest R(д)value achieved for a given percentage\nof the test set. For example, on the left hand graph for the\nQPP Net line, at 0.93on the x-axis, the y-axis value is “1.5”.\nThis signifies that QPP Net’s prediction was within at least\na factor of 1.5 for 93% of the testing data. For both datasets,\nQPP Net’s curve has a smaller slope, and does not spike until\nit is much closer to 1 than the other curves. This means the\nour estimates are within a lower error factor for a larger portion\nof the testing queries compared with the other techniques.\n6.1.2 Errors by query template. Figure 8 shows the mean\nabsolute error of the queries in the test set grouped by the\nTPC-DS query template (note the log scale). For this experi-\nment, we use a “hold one out” strategy: the model is trained\nusing all but one of the query templates, and then the per-\nformance on the held-out query template is measured.\nFor each query template, the mean absolute error of QPP\nNet is either lower or within 5% of the other models. Results\nfor TPC-H are similar, but as TPC-H has far fewer templates,\nwe omit the graph. We therefore conclude that QPP Net can\naccurately predict the latency of queries across a wide variety\nof query templates.\nGenerally, QPP Net’s performance greatly exceeds the\nother models on the query templates with the highest av-\nerage latency (e.g. TPC-DS templates 6, 17, 81). A plot of\nthe mean latency of each query template is provided in Ap-\npendix A. While the mean absolute error on these templates\nexceeds that of the other templates, the relative performance\nof QPP Net compared to the other models increases signifi-\ncantly. Thus, compared with other models, QPP Net performs\nespecially well on long-running queries .\n\n 100 1000 10000 100000\n3678913151718192224252627282930313338394142434445464849505152535455565758596061626364656667686971727375767879818384858788899091939697Mean absolute error (s)\nTPC-DS TemplateTAM\nSVM\nRBF\nQPPNetFigure 8: Mean absolute error by template for TPC-DS\n6.2 Training Overhead\nIn this section, we evaluate various properties and behav-\niors of the neural network model during training, including\nanalyzing the effectiveness of our proposed optimizations.\nOptimizations Here, we evaluate the two training optimiza-\ntions introduced in Section 5: (a) information sharing , caching\nand reusing computations that are shared by multiple nodes,\nand (b) batch sampling , grouping trees with similar structures\ninto batches in order to take advantage of vector processing.\nWe evaluated these optimizations by training the neural\nnetwork until the network converged to the best-observed\naccuracy. We trained the network with no optimizations,\nthe batching optimization alone, the information sharing\noptimization alone, and with both optimizations. Figure 9a\nshows the results for both datasets. Without either optimiza-\ntion, training the neural network for either dataset takes well\nover a week. Of the new optimizations, information sharing\nis the more significant in these experiments, bringing the\ntraining time down from over a week to a little under 3 days.\nBoth optimizations combined bring the training down to\nonly slightly over 24 hours.\nWe also measured the memory usage of the information\nsharing approach, which requires additional space to cache\nresults. The size of the cache was minuscule in comparison to\nthe size of the neural network’s weights, with the cache size\nnever exceeding 20MB. We conclude that both the informa-\ntion sharing and batch sampling optimizations are worthwhile\nfor accelerating the time needed to train the neural network.\nTraining convergence Next, we investigate the performance\nof the model over time during training. After each training\nepoch (a full pass over the training queries), we evaluated\nand recorded the mean absolute error across the test set. The\nresults for TPC-H are shown in Figure 9b, and the results\nfor TPC-DS are shown in Figure 9c. On both plots, the blue,\ngreen and red lines show the performance of TAM,SVM, and\nRBF, respectively. While the neural network model did not\nconverge until epoch 1000 (approximately 28 hours for both\ndatasets), the performance of the neural network begins to\nexceed the performance of SVMat around epoch 250 (7 hours)\nwith TPC-H, or after 150 epochs (4.5 hours) for TPC-DS. Theneural network begins to exceed the performance of RBF\nafter around 350 epochs (10 hours) for TPC-H, or after 250\nepochs for TPC-DS (7 hours). We conclude that the training\noverhead required for QPP Net to achieve competitive perfor-\nmance is reasonable for a variety of datasets.\nBoth Figure 9b and Figure 9c show the classic, inverse-\nexponential behavior of neural networks during training.\nAt the start of the training, the neural network decreases\nits mean absolute error by 20 in just 100 epochs (epochs 0\nthrough 100). However, later in the training, it takes 100\nepochs to decrease the mean absolute error by just 2 (epochs\n400 through 500). Thus, each additional epoch returns smaller\nand smaller benefits, requiring a large number of additional\nepochs to get small gains towards the end of training.\nIt is possible that using other optimization methods be-\nsides stochastic gradient descent, such as Adam [ 16], might\nspeed up training. We leave such experiments to future work.\n6.3 Network architecture\nFor all of the experiments so far, we have used 5 hidden\nlayers each with 128 neurons for each neural unit. While\na 5 layer, 128 neuron network would be considered rather\nsmall by modern standards, one needs to consider that in\nour model, each neural unit in the plan-based neural net-\nwork has these dimensions. When assembled together into\na tree, the network is much larger (one to two orders of\nmagnitude). However, choosing the number of hidden lay-\ners and number of neurons is a difficult task. While there is\nno theoretically-best number of hidden layers or number of\nneurons per layer, good values can be found experimentally.\nIntuitively, “deeper” (i.e., more hidden layers) architectures\nenable more feature engineering, as additional layers add\nadditional transformations of the inputs [ 20]. On the other\nhand, “taller” (i.e., larger hidden layers) architectures allow\neach feature transformation to be richer, as they have more\nweights and thus carry more data [46].\nWe analyze four of the variables at play when trying to\nfind the correct network configuration: the number of hidden\nlayers, the number of neurons, the maximum accuracy the\nnetwork can reach, and the time it takes to train the network.\n\n 0 2000 4000 6000 8000 10000 12000 14000\nTPC-H TPC-DSTraining time (m)None\nBatching\nShared info\nBoth\nOne day\nFive day(a) Impact of training optimizations\n 15 20 25 30 35 40 45 50 55 60\n 0  100  200  300  400  500  600  700  800  900  1000Mean absolute error\nEpochsUnsmoothed\nSmoothed\nTAM\nSVM\nRBF (b) Training convergence (TPC-H)\n 20 25 30 35 40 45 50 55\n 0  100  200  300  400  500  600  700  800  900  1000Mean absolute error\nEpochsUnsmoothed\nSmoothed\nTAM\nSVM\nRBF (c) Training convergence (TPC-DS)\nFigure 9: Training overhead results\n 0 0.2 0.4 0.6 0.8 1 1.2\n 10  100  1000 0 20 40 60 80 100Relative accuracy\nTraining time (h)\nNeurons per hidden layerTraining Time\nRelative accuracy\nFigure 10: Effect of the number of neurons on accuracy\n(relative to 128 neurons) and training time.\nGenerally, increasing either the number of hidden layers\nor the number or neurons results in an increase in training\ntime due to an increase in the number of weights.3Thus,\nsetting the number of neurons or number of hidden layers\ntoo high will result in unacceptably long training times. But,\nif the number of neurons or hidden layers is set too low,\nthe network might not have enough weights to learn the\nunderlying data distribution well enough. We thus seek the\nnumber of neurons and hidden layers that will minimize\ntraining time while still giving peak or near-peak accuracy.\nVarying the number of neurons Figure 10 shows the train-\ning time and relative accuracy when varying the number of\nneurons inside each of the five hidden layers. With an ex-\ntremely small number of neurons (8 neurons), training time\nis low (6 hours), but accuracy is extremely poor: QPP Net\nachieves less than 15% of the accuracy that the 128-neuron\nnetwork does. On the other hand, using an extremely large\nnumber of neurons causes the training time to skyrocket:\n3Larger networks also take longer to make predictions at inference time,\ncreating potential complications for applications.\n 0 0.2 0.4 0.6 0.8 1 1.2\n 1  2  3  4  5  6  7  8 0 20 40 60 80 100Relative accuracy\nTraining time (h)\nNumber of hidden layersTraining Time\nRelative accuracyFigure 11: Effect of the number of hidden layers on ac-\ncuracy (relative to 5 hidden layers) and training time.\nwith 1024 neurons per hidden layer, training time is nearly\nfour times what is required for the 128 neuron network, with\nonly a tiny increase in accuracy (less than 1%).\nOne may notice that the training time seems to grow with\nthe log of the number of neurons at first, but then eventually\nbecomes linear. This is because neural networks are trained\non GPUs equipped with highly-parallel vector processing\nunits. There is thus sublinear increases in training time until\nthere is approximately one weight per vector processing core,\nafter which the training time changes as expected. When the\nnumber of neurons greatly exceeds the capacity of the GPU,\nthe slowdown will become worse than linear.\nVarying the number of hidden layers Figure 11 shows a\nsimilar experiment, this time varying the number of hidden\nlayers and keeping the number of neurons fixed at 128. Note\nthat connecting two layers with 128 neurons to each other\nrequires a matrix of size 128×128, so each additional hidden\nlayer adds on the order of 214additional weights.\nIncreasing the number of hidden layers has a similar be-\nhavior to increasing the number of neurons: initially, each\n\nincrease brings about a small increase in training time but a\nlarge jump in accuracy. Eventually, adding another hidden\nlayer produces a much larger jump in training time and a\nmuch smaller jump in accuracy. From Figure 11, we can con-\nclude that adding more than 5 hidden layers, at least when\nthe size of each hidden layer is 128 neurons, does not bring\nabout much benefit.\n7 RELATED WORK\nQuery performance prediction There exist a number of\napproaches that leverage machine learning and statistical\nanalysis to address the problem of query performance predic-\ntion. We already discussed and compared with [ 4,13,25] in\nour experimental study. [ 11] focuses on predicting multiple\nquery resource usage metrics simultaneously (but not exe-\ncution times). Both [ 38,59] predict statistics about queries\nin XML databases. [ 56] demonstrating that optimizer cost\nmodels can be used to predict query performance if one is\nwilling to sample a percentage of the underlying data.\nAll these techniques suffer from similar drawbacks: first,\nthey require human experts to analyze the properties of an\noperator or query execution plan and determine how they\nshould be transformed into features for a machine learning\nalgorithm, whereas our deep-learning approach requires no\nsuch feature engineering. Second, while some of these ap-\nproaches model plans, operators, or a combination thereof,\nnone of them learn the interactions between various combi-\nnations of operators, as the approach presented here does.\nA number of techniques [ 9,10,54] extend to concur-\nrent query performance prediction for analytical queries.\nThese techniques assume a-priori knowledge of query tem-\nplates [ 10], query structure [ 54] and/or require extensive\noffline training on representative queries [ 9]. Furthermore,\ntheir proposed input features, metrics and models are hand-\ntuned to handle only analytical tasks, which make them less\napplicable to diverse workloads.\nCardinality estimation Works on cardinality estimation\nare fundamentally related to query performance prediction,\nas an operator’s cardinality often correlates with its latency.\nTechniques for cardinality estimation include robust statis-\ntical techniques [ 5,19,32], adaptive histograms [ 3,47], and\ndeep learning [ 17,27]. While query cardinalities are certainly\nan indicator of latency, translating even an accurate cardi-\nnality estimate for each operator in a query execution plan\nto a total plan latency is by no means a trivial task. However,\na technique predicting operator cardinalities could be easily\nintegrated into our deep neural network by inserting the car-\ndinality estimate of each operator into its neural units input\nvector. The neural network could then learn the relationship\nbetween these estimates and the latency of the entire query\nexecution plan.Progress estimators Work on query progress indicators [ 21,\n24,29,33,57] essentially amounts to frequently updating a\nprediction of a query’s latency. These approaches estimate\nthe latency of a query as it is running , and the estimate that\nthese techniques make at the very start of the query’s exe-\ncution may be quite inaccurate, but are quickly refined and\ncorrected during the early stages of a query’s progress. This\ngreatly limits their applicability for ahead-of-time query per-\nformance prediction, and thus we do not compare against\nany of these techniques directly.\nDeep learning We are not the first to apply deep learning to\ndatabase management problems. Deep learning has seen a re-\ncent groundswell of activity in the systems community [ 55],\nincluding several works on query optimization [ 30,37], en-\ntity matching [ 35], index selection [ 40,45], indexes them-\nselves [18], and cardinality estimation [17, 27].\n8 CONCLUSIONS AND FUTURE WORK\nWe have introduce a novel neural network architecture de-\nsigned specifically to address the challenges of query perfor-\nmance prediction. The architecture allows for plan-structured\nneural networks (networks that predict the execution time of\na given query plan) to be constructed by assembling operator-\nlevel neural units (neural networks that predict the latency of\na given operator) to form a tree-based structure that matches\nthe structure of the query plan generated by the optimizer.\nWe motivated the need for this novel model, described its\narchitecture and have shown how the model can be effec-\ntively trained through two optimizations. Experimental re-\nsults demonstrate that our approach outperforms state-of-\nthe-art solutions, with manageable training overhead.\nFuture work could advance in a number of directions. For\nexample, the neural network architecture presented here\ncould be adapted to handle concurrent queries. Doing so\nwould require understanding the resource usage require-\nments of the two queries, and whether or not two queries\nwill have to compete for resources. Furthermore, the neu-\nral network architecture presented here was used to predict\nthe performance of queries executed on a bare-metal server.\nHowever, in a cloud environment, performance can vary\nbased on the time of day, or seemingly randomly. Techniques\nto capture the relative performance of the system (e.g., mon-\nitoring I/O rates, CPU cycles, etc.), and ways of making the\nneural network model aware of performance fluctuations,\ncould be investigated. Improvements to the training time of\nneural network could be investigated. For example, using\na different optimizer [ 16] or taking advantage of transfer\nlearning techniques [6, 58] may prove fruitful.\n\nREFERENCES\n[1] PostgreSQL database, http://www.postgresql.org/.\n[2]M. Abadi et al. TensorFlow: Large-Scale Machine Learning on Hetero-\ngeneous Distributed Systems. arXiv ’16 .\n[3]A. Aboulnaga et al. Self-tuning Histograms: Building Histograms\nWithout Looking at Data. In SIGMOD ’99 .\n[4]M. Akdere et al. Learning-based query performance modeling and\nprediction. In ICDE ’12 .\n[5]B. Babcock et al. Towards a Robust Query Optimizer: A Principled\nand Practical Approach. In SIGMOD ’05 .\n[6]Y. Bengio. Deep Learning of Representations for Unsupervised and\nTransfer Learning. In ICML WUTL ’12 .\n[7]L. Bottou. Large-Scale Machine Learning with Stochastic Gradient\nDescent.\n[8]Y. Chi et al. SLA-tree: A Framework for Efficiently Supporting SLA-\nbased Decisions in Cloud Computing. In EDBT ’11 .\n[9] J. Duggan et al. Contender: A Resource Modeling Approach for Con-\ncurrent Query Performance Prediction. In EDBT ’14 .\n[10] J. Duggan et al. Performance Prediction for Concurrent Database\nWorkloads. In SIGMOD ’11 .\n[11] A. Ganapathi et al. Predicting Multiple Metrics for Queries: Better\nDecisions Enabled by Machine Learning. In ICDE ’09 .\n[12] X. Glorot et al. Deep Sparse Rectifier Neural Networks. In PMLR ’11 .\n[13] H. Hacigumus et al. Predicting Query Execution Time: Are Optimizer\nCost Models Really Unusable? In ICDE ’13 .\n[14] S. Hochreiter et al. Long Short-Term Memory. Neural Computation\n’97.\n[15] K. Hornik et al. Multilayer feedforward networks are universal ap-\nproximators. Neural Networks ’89 .\n[16] D. P. Kingma et al. Adam: A Method for Stochastic Optimization. In\nICLR ’15 .\n[17] A. Kipf et al. Learned Cardinalities: Estimating Correlated Joins with\nDeep Learning. arXiv ’18 .\n[18] T. Kraska et al. The Case for Learned Index Structures. In SIGMOD\n’18.\n[19] P.-A. Larson et al. Cardinality Estimation Using Sample Views with\nQuality Assurance. In SIGMOD ’07 .\n[20] Y. LeCun et al. Deep learning. Nature ’15 .\n[21] K. Lee et al. Operator and Query Progress Estimation in Microsoft\nSQL Server Live Query Statistics. In SIGMOD ’16 .\n[22] H. Leeb et al. Sparse estimators and the oracle property, or the return\nof Hodges’ estimator. Econometrics ’08 .\n[23] V. Leis et al. How Good Are Query Optimizers, Really? VLDB ’15 .\n[24] J. Li et al. GSLPI: A Cost-Based Query Progress Indicator. In ICDE ’12 .\n[25] J. Li et al. Robust estimation of resource consumption for SQL queries\nusing statistical techniques. VLDB ’12 .\n[26] Z. C. Lipton et al. A Critical Review of Recurrent Neural Networks for\nSequence Learning. arXiv ’15 .\n[27] H. Liu et al. Cardinality Estimation Using Neural Networks. In CAS-\nCON ’15 .\n[28] W. Liu et al. A survey of deep neural network architectures and their\napplications. Neurocomputing ’17 .\n[29] G. Luo et al. Toward a Progress Indicator for Database Queries. In\nSIGMOD ’04 .\n[30] R. Marcus et al. Deep Reinforcement Learning for Join Order Enumer-\nation. In aiDM ’18 .\n[31] R. Marcus et al. WiSeDB: A Learning-based Workload Management\nAdvisor for Cloud Databases. VLDB ’16 .\n[32] G. Moerkotte et al. Preventing Bad Plans by Bounding the Impact of\nCardinality Estimation Errors. VLDB ’09 .[33] K. Morton et al. ParaTimer: A Progress Indicator for MapReduce DAGs.\nInSIGMOD ’10 .\n[34] L. Mou et al. Convolutional Neural Networks over Tree Structures for\nProgramming Language Processing. In AAAI ’16 .\n[35] S. Mudgal et al. Deep Learning for Entity Matching: A Design Space\nExploration. In SIGMOD ’18 .\n[36] R. O. Nambiar et al. The Making of TPC-DS. In VLDB ’06 .\n[37] J. Ortiz et al. Learning State Representations for Query Optimization\nwith Deep Reinforcement Learning. In DEEM ’18 .\n[38] M. T. Ozsu et al. XSEED: Accurate and Fast Cardinality Estimation for\nXPath Queries. In ICDE ’06 .\n[39] A. Paszke et al. Automatic differentiation in PyTorch. In NIPS-W ’17 .\n[40] A. Pavlo et al. Self-Driving Database Management Systems. In CIDR\n’17.\n[41] F. Pedregosa et al. Scikit-learn: Machine Learning in Python. JMLR\n’11.\n[42] M. Poess et al. New TPC Benchmarks for Decision Support and Web\nCommerce. SIGMOD ’00 .\n[43] J. B. Pollack. Recursive distributed representations. AI ’90 .\n[44] S. Ruder. An overview of gradient descent optimization algorithms.\narXiv ’16 .\n[45] M. Schaarschmidt et al. LIFT: Reinforcement Learning in Computer\nSystems by Learning From Demonstrations. arXiv ’18 .\n[46] J. Schmidhuber. Deep learning in neural networks: An overview. NN\n’15.\n[47] M. Stillger et al. LEO - DB2’s LEarning Optimizer. In VLDB ’01 .\n[48] R. Taft et al. STeP: Scalable Tenant Placement for Managing Database-\nas-a-Service Deployments. In SoCC ’16 .\n[49] K. S. Tai et al. Improved Semantic Representations From Tree-\nStructured Long Short-Term Memory Networks. arXiv ’15 .\n[50] C. Tofallis. A Better Measure of Relative Prediction Accuracy for Model\nSelection and Model Estimation. JORS ’14 .\n[51] S. Tozer et al. Q-Cop: Avoiding bad query mixes to minimize client\ntimeouts under heavy loads. In ICDE ’10 .\n[52] A. van den Oord et al. Neural Discrete Representation Learning. NIPS\n’17.\n[53] A. Vaswani et al. Attention Is All You Need. NIPS ’17 .\n[54] S. Venkataraman et al. Ernest: Efficient performance prediction for\nlarge-scale advanced analytics. In NSDI ’16 .\n[55] W. Wang et al. Database Meets Deep Learning: Challenges and Op-\nportunities. SIGMOD Rec ’16 .\n[56] W. Wu et al. Towards Predicting Query Execution Time for Concurrent\nand Dynamic Database Workloads. VLDB ’13 .\n[57] X. Xie et al. PIGEON: Progress indicator for subgraph queries. In ICDE\n’15.\n[58] J. Yosinski et al. How Transferable Are Features in Deep Neural\nNetworks? In NIPS ’14 .\n[59] N. Zhang et al. Statistical Learning Techniques for Costing XML\nQueries. In VLDB ’05 .\nA TPC-DS TEMPLATE LATENCY\nFigure 12 shows the mean latency for each of the TPC-DS\nquery templates.\nB FEATURES\nTable 2 describes the values used as inputs for our neural\nunits. The first column lists the name of the quantity. The\nsecond column describes which PostgreSQL operators use\na particular type of input. The third column describes how\n\n 1 10 100 1000 10000 100000\nTemplate 3678913151718192224252627282930313338394142434445464849505152535455565758596061626364656667686971727375767879818384858788899091939697Mean latency (minutes)\nTPC-DS TemplateMean latencyFigure 12: Mean query latency by template for TPC-DS\nFeature PostgreSQL operators Encoding Description\nPlan Width All Numeric Optimizer’s estimate of the width of each output row\nPlan Rows All Numeric Optimizer’s estimate of the cardinality of the output of the operator\nPlan Buffers All Numeric Optimizer’s estimate of the memory requirements of an operator\nEstimated I/Os All Numeric Optimizer’s estimate of the number of I/Os performed\nTotal Cost All Numeric Optimizer’s cost estimate for this operator, plus the subtree\nJoin Type Joins One-hot One of: semi, inner, anti, full\nParent Relationship Joins One-hot When the child of a join. One of: inner, outer, subquery\nHash Buckets Hash Numeric # hash buckets for hashing\nHash Algorithm Hash One-hot Hashing algorithm used\nSort Key Sort One-hot Key for sort operator\nSort Method Sort One-hat Sorting algorithm, e.g. “quicksort”, “top-N heapsort”, “external sort”\nRelation Name All Scans One-hot Base relation of the leaf\nAttribute Mins All Scans Numeric Vector of minimum values for relevant attributes\nAttribute Medians All Scans Numeric Vector of median values for relevant attributes\nAttribute Maxs All Scans Numeric Vector of maximum values for relevant attributes\nIndex Name Index Scans One-hot Name of index\nScan Direction Index Scans Boolean Direction to read the index (forward or backwards)\nStrategy Aggregates One-hot One of: plain, sorted, hashed\nPartial Mode Aggregate Boolean Eligible to participate in parallel aggregation\nOperator Aggregate One-hot The aggregation to perform, e.g. max, min, avg\nTable 2: QPP Net Inputs\nthe particular value is encoded into an input suitable for a\nneural network. The encoding strategies are:\n•Numeric : the value is encoded as a numeric value,\nscaled so that the mean of the value across the training\nset is zero and the variance is one. At inference time,\nthe same scaling values are used. This is known as\n“whitening”, and is a standard practice in deep learn-\ning [39].\n•Boolean : the value is encoded as either a zero or a\none.\n•One-hot : the value is categorical, and is encoded as a\none-hot vector, e.g. a vector with a single “1” element\nwhere the rest of the elements are “0”.The first five values, in the first section of the table, rep-\nresent information that is available for every PostgreSQL\noperator. Thus, these values are included in all neural units.\nThe next section of the table (“Join Type” to “Sort Method”),\ncorresponds to information used by the join neural unit. The\nthird section (“Relation Name” to “Scan Direction”) refers to\nthe inputs used for the scan neural unit, and, depending on\nthe selected physical operator type (e.g., index scan, table\nscan), some values may be missing. Missing values are set to\nzero. The final section of the table (“Strategy” to “Operator”)\nlists the inputs used for the aggregate neural unit.",
  "textLength": 78137
}