{
  "paperId": "ae99376dbeee62c739cd29a0082e35e75d10d8d6",
  "title": "Composable Sketches for Functions of Frequencies: Beyond the Worst Case",
  "pdfPath": "ae99376dbeee62c739cd29a0082e35e75d10d8d6.pdf",
  "text": "Composable Sketches for Functions of Frequencies:\nBeyond the Worst Case\nEdith Cohen\nGoogle Research and Tel Aviv University\nedith@cohenwang.comO\fr Geri∗\nStanford University\nofirgeri@cs.stanford.edu\nRasmus Pagh\nGoogle Research, BARC, and IT University of Copenhagen\npagh@itu.dk\nAbstract\nRecently there has been increased interest in using machine learning techniques to improve\nclassical algorithms. In this paper we study when it is possible to construct compact, composable\nsketches for weighted sampling and statistics estimation according to functions of data frequencies.\nSuch structures are now central components of large-scale data analytics and machine learning\npipelines. However, many common functions, such as thresholds and pth frequency moments with\np>2, are known to require polynomial-size sketches in the worst case. We explore performance\nbeyond the worst case under two di\u000berent types of assumptions. The \frst is having access to noisy\nadvice on item frequencies. This continues the line of work of Hsu et al. (ICLR 2019), who assume\npredictions are provided by a machine learning model. The second is providing guaranteed\nperformance on a restricted class of input frequency distributions that are better aligned with\nwhat is observed in practice. This extends the work on heavy hitters under Zip\fan distributions\nin a seminal paper of Charikar et al. (ICALP 2002). Surprisingly, we show analytically and\nempirically that \\in practice\" small polylogarithmic-size sketches provide accuracy for \\hard\"\nfunctions.\n1 Introduction\nComposable sketches, also known as mergeable summaries [ 2], are data structures that support\nsummarizing large amounts of distributed or streamed data with small computational resources\n(time, communication, and space). Such sketches support processing additional data elements and\nmerging sketches of multiple datasets to obtain a sketch of the union of the datasets. This design is\nsuitable for working with streaming data (by processing elements as they arrive) and distributed\ndatasets, and allows us to parallelize computations over massive datasets. Sketches are now a central\npart of managing large-scale data, with application areas as varied as federated learning [ 41] and\nstatistics collection at network switches [37, 36].\nThe datasets we consider consist of elements that are key-value pairs ( x;v) wherev\u00150. The\nfrequencywxof a keyxis de\fned as the sum of the values of elements with that key. When the\nvalues of all elements are 1, the frequency is simply the number of occurrences of a key in the\n∗Most of this work was done while interning at Google Research.\n1arXiv:2004.04772v3  [cs.DS]  3 Nov 2021\n\ndataset. Examples of such datasets include search queries, network tra\u000ec, user interactions, or\ntraining data from many devices. These datasets are typically distributed or streamed.\nGiven a dataset of this form, one is often interested in computing statistics that depend on the\nfrequencies of keys. For example, the statistics of interest can be the number of keys with frequency\ngreater than some constant (threshold functions), or the second frequency moment (P\nxw2\nx), which\ncan be used to estimate the skew of the data. Generally, we are interested in statistics of the form\nX\nxLxf(wx) (1)\nwherefis some function applied to the frequencies of the keys and the coe\u000ecients Lxare provided\n(for example as a function of the features of the key x). An important special case, popularized in\nthe seminal work of [4], is computing the f-statistics of the data: kf(w)k1=P\nxf(wx).\nOne way to compute statistics of the form (1)is to compute a random sample of keys, and then\nuse the sample to compute estimates for the statistics. In order to compute low-error estimates, the\nsampling has to be weighted in a way that depends on the target function f: each keyxis weighted\nbyf(wx). Since the problem of computing a weighted sample is more general than computing\nf-statistics, our focus in this work will be on composable sketches for weighted sampling according\nto di\u000berent functions of frequencies.\nThe tasks of sampling or statistics computation can always be performed by \frst computing\na table of key and frequency pairs ( x;wx). But this aggregation requires a data structure of size\n(and in turn, communication or space) that grows linearly with the number of keys whereas ideally\nwe want the size to grow at most polylogarithmically. With such small sketches we can only hope\nfor approximate results and generally we see a trade-o\u000b between sketch size, which determines the\nstorage or communication needs of the computation, and accuracy.\nWhen estimating statistics from samples, the accuracy depends on the sample size and on how\nmuch the sampling probabilities \\suit\" the statistics we are estimating. In order to minimize the\nerror, the sampling probability of each key xshould be (roughly) proportional to f(wx). This leads\nto a natural and extensively-studied question: for which functions fcan we design e\u000ecient sampling\nsketches?\nThe literature and practice are ripe with surprising successes for sketching, including small\n(polylogarithmic size) sketch structures for estimating the number of distinct elements [ 24,23]\n(f(w) =Iw>0), frequency moments ( f(w) =wp) forp2[0;2] [4,29], and computing `pheavy\nhitters (for p\u00142, where an `p\"-heavy hitter is a key xwithwp\nx\u0015\"kwkp\np) [43,9,39,19,42].\n(We useI\u001bto denote the indicator function that is 1 if the predicate \u001bis true, and 0 otherwise.)\nA variety of methods now support sampling via small sketches for rich classes of functions of\nfrequencies [ 11,40,32,14], including the moments f(w) =wpforp2[0;2] and the family of concave\nsublinear functions.\nThe \rip side is that we know of lower bounds that limit the performance of sketches using small\nspace for some fundamental tasks [ 4]. A full characterization of functions ffor whichf-statistics can\nbe estimated using polylogarithmic-size sketches was provided in [ 6]. Examples of \\hard\" functions\nare thresholds f(w) =Iw>T (counting the number of keys with frequency above a speci\fed threshold\nvalueT), threshold weights f(w) =wIw>T, and high frequency moments f(w) =wpwithp>2.\nEstimating the pth frequency moment (P\nxwp\nx) forp>2 is known to require space \n( n1\u00002=p) [4,35],\nwherenis the number of keys. These particular functions are important for downstream tasks:\nthreshold aggregates characterize the distribution of frequencies, and high moment estimation is\n2\n\nused in the method of moments, graph applications [ 21], and for estimating the cardinality of\nmulti-way self-joins [3] (a pth moment is used for estimating a p-way join).\nBeyond the worst case. Much of the discussion of sketching classi\fed functions into \\easy\"\nand \\hard\". For example, there are known e\u000ecient methods for sampling according to f(w) =wp\nforp\u00142, while for p>2, even the easier task of computing the pth moment is known to require\npolynomial space. However, the hard data distributions used to establish lower bounds for some\nfunctions of frequency are arguably not very realistic. Real data tends to follow nice distributions\nand is often (at least somewhat) predictable. We study sketching where additional assumptions\nallow us to circumvent these lower bounds while still providing theoretical guarantees on the quality\nof the estimates. We consider two distinct ways of going beyond the worst case: 1) access to advice\nmodels , and 2) making natural assumptions on the frequency distribution of the dataset.\nFor the sampling sketches described in this paper, we use a notion of overhead to capture\nthe discrepancy between the sampling probabilities used in the sketch and the \\ideal\" sampling\nprobabilities of weighted sampling according to a target function of frequency f. An immensely\npowerful property of using sampling to estimate statistics of the form (1)is that the overhead\ntranslates into a multiplicative increase in sample/sketch size, without compromising the accuracy\nof the results (with respect to what an ideal \\benchmark\" weighted sample provides). This property\nwas used in di\u000berent contexts in prior work, e.g., [ 25,17], and we show that it can be harnessed for\nour purposes as well. For the task of estimating f-statistics, we use a tailored de\fnition of overhead,\nthat is smaller than the overhead for the more general statistics (1).\nAdvice model. The advice model for sketching was recently proposed and studied by Hsu et\nal. [28]. The advice takes the form of an oracle that is able to identify whether a given key is a\nheavy hitter. Such advice can be generated, for example, by a machine learning model trained on\npast data. The use of the \\predictability\" of data to improve performance was also demonstrated\nin [34,30]. A similar heavy hitter oracle was used in [ 33] to study additional problems in the\nstreaming setting. For high frequency moments, they obtained sketch size O(n1=2\u00001=p), a quadratic\nimprovement over the worst-case lower bound.\nHere we propose a sketch for sampling by advice . We assume an advice oracle that returns a\nnoisy prediction of the frequency of each key. This type of advice oracle was used in the experimental\nsection of [ 28] in order to detect heavy hitters. We show that when the predicted f(wx) for keys\nwith above-average contributions f(wx) is approximately accurate within a factor C, our sample\nhas overhead O(C). That is, the uncertainty in the advice translates to a factor O(C) increase in\nthe sketch size but does notimpact the accuracy.\nFrequency-function combinations. Typically, one designs sketch structures to provide\nguarantees for a certain function fand any set of input frequencies w. The performance of a sketch\nstructure is then analyzed for a worst-case frequency distribution. The analysis of the advice model\nalso assumes worst-case distributions (with the bene\ft that comes from the advice). We depart from\nthis and study sketch performance for a combination (F;W;h ) of a family of functions F, a family\nWof frequency distributions, and an overhead factor h. Speci\fcally, we seek sampling sketches\nthat produce weighted samples with overhead at most hwith respect to f(w) for every function\nf2Fand frequency distribution w2W. By limiting the set Wof input frequency distributions\nwe are able to provide performance guarantees for a wider set Fof functions of frequency, including\nfunctions that are worst-case hard. We particularly seek combinations with frequency distributions\nWthat are typical in practice. Another powerful property of the combination formulation is that it\n3\n\nprovides multi-objective guarantees with respect to a multiple functions of frequency Fusing the\nsame sketch [10, 11, 37].\nThe performance of sketch structures on \\natural\" distributions was previously considered in a\nseminal paper by Charikar et al. [ 9]. The paper introduced the Count Sketch structure for heavy\nhitter detection, where an `2\"-heavy hitter is a key xwithw2\nx\u0015\"kwk2\n2. They also show that for\nZipf-distributed data with parameter \u000b\u00151=2, a count sketch of size O(k) can be used to \fnd the k\nheaviest keys (a worst-case hard problem) and that an `1sample can only identify the heaviest keys\nfor Zipf parameter \u000b\u00151.\nWe signi\fcantly extend these insights to a wider family of frequency distributions and to a\nsurprisingly broad class of functions of frequencies. In particular we show that all high moments\n(p>2) are \\easy\" as long as the frequency distribution has an `1or`2\"-heavy hitter. In this case,\nan`1or`2sample with overhead 1 =\"can be used to estimate all high moments. We also show that\nin a sense this characterization is tight in that if we allow all frequencies, we meet the known lower\nbounds. It is very common for datasets in practice to have a most frequent key that is an `1or`2\n\"-heavy hitter. This holds in particular for Zipf or approximate Zipf distributions.\nMoreover, we show that Zipf frequency distributions have small universal sketches that apply to\nanymonotone function of frequency (including thresholds and high moments). Zipf frequencies were\npreviously considered in the advice model [ 1]. Interestingly, we show that for these distributions a\nsingle small sketch is e\u000bective with all monotone functions of frequency, even without advice. In\nthese cases, universal sampling is achieved with o\u000b-the-shelf polylogarithmic-size sketches such as `p\nsamples for p\u00142 and multi-objective concave-sublinear samples [12, 11, 40, 32].\nEmpirical study. We complement our analysis with an empirical study on multiple real-world\ndatasets including datasets studied in prior work on advice models [ 47,7,46,38]. (Additional\ndiscussion of the datasets appears in Section 2.3.) We apply sampling by advice, with advice based on\nmodels from prior work or direct use of frequencies from past data. We then estimate high frequency\nmoments from the samples. We observe that sampling-by-advice was e\u000bective on these datasets,\nyielding low error with small sample size. We also observed, however, that `2and`1samplers were\nsurprisingly accurate on these tasks, with `2samplers generally outperforming sampling by advice.\nThe surprisingly good performance of these simple sampling schemes is suggested from our analysis.\nWe compute the overhead factors for some o\u000b-the-shelf sampling sketches on multiple real-world\ndatasets with the objectives of `psampling (p>2) and universal sampling. We \fnd these factors to\nbe surprisingly small. For example, the measured overhead of using `2sampling for the objective of\n`psampling (p\u00152) is in the range [1 :18;21]. For universal sampling, the observed overhead is lower\nwith`1and with multi-objective concave sublinear samples than with `2sampling and is in [93 ;773],\ncomparing very favorably with the alternative of computing a full table. Finally, we use sketches\nto estimate the distribution of rank versus frequency, which is an important tool for optimizing\nperformance across application domains (for network \rows, \fles, jobs, or search queries). We \fnd\nthat`1samples provide quality estimates, which is explained by our analytical results.\nOrganization. In Section 2, we present the preliminaries, including the de\fnition of overhead\nand description of o\u000b-the-shelf sampling sketches that we use. Our study of the advice model is\npresented in Section 3. Our study of frequency-function combinations, particularly in the context\nof`psampling, is presented in Section 4. Section 5 discusses universal samples. Our experimental\nstudy is presented throughout Sections 3 and 4. Additional experimental results are reported in\nAppendix A.\n4\n\n2 Preliminaries\nWe consider datasets where each data element is a (key, value) pair. The keys belong to a universe\ndenoted byX(e.g., the set of possible users or words), and each key may appear in more than one\nelement. The values are positive, and for each key x2X, we de\fne its frequencywx\u00150 to be the\nsum of values of all elements with key x. If there are no elements with key x,wx= 0. The data\nelements may appear as a stream or be stored in a distributed manner. We denote the number of\nactive keys (keys with frequency greater than 0) by n.\nWe are interested in sketches that produce a weighted sample of keys according to some function\nfof their frequencies, which means that the weight of each key xisf(wx). In turn, the sampling\nprobability of key xis roughly proportional to f(wx). We denote the vector of the frequencies of\nall active keys by w(in any \fxed order). We use f(w) as a shorthand for the vector of all values\nf(wx).\nEstimates from a sample. The focus of this work is sampling schemes that produce a random\nsubsetS\u0012X of the keys in the dataset. Each active key xis included in the sample Swith\nprobability pxthat depends on the frequency wx. From such a sample, we can compute for each\nkeyxtheinverse probability estimate [27] off(wx) de\fned as\n\\f(wx) =(f(wx)\npxifx2S\n0 ifx =2S:\nThese per-key estimates are unbiased (Eh\n\\f(wx)i\n=f(wx)). They can be summed to obtain unbiased\nestimates of the f-statisticsP\nx2Hf(wx) of a domain H\u001aX:\n\\X\nx2Hf(wx) :=X\nx2H\\f(wx) =X\nx2S\\H\\f(wx):\nThe last equality follows because \\f(wx)= 0 for keys not in the sample. Generally, we use ba\nto denote the estimator for a quantity a(e.g., \\P\nx2Hf(wx)de\fned above is the estimator forP\nx2Hf(wx)). We can similarly get unbiased estimates for other statistics that are linear in f(wx),\ne.g.,P\nx2XLxf(wx) (for coe\u000ecients Lx).\nBottom-ksamples. We brie\ry describe a type of samples that will appear in our algorithms and\nanalysis. In a bottom-ksample [15], we draw a random value (called seed) for each active key x. The\ndistribution of the seed typically depends on the frequency of the key. Then, to obtain a sample of\nsizek, we keep the kkeys with lowest seed values. Many sampling schemes can be implemented as\nbottom-ksamples, including our sketch in Section 3, PPSWOR, and sampling sketches for concave\nsublinear functions (the last two are described further in Section 2.2).\n2.1 Benchmark Variance Bounds\nIn this work, we design sampling sketches and use them to compute estimates for some function f\napplied to the frequencies of keys ( f(wx) for keyx). We measure performance with respect to that\nof a \\benchmark\" weighted sampling scheme where the weight of each key xisf(wx). Recall that\nfor \\hard\" functions f, these schemes can not be implemented with small sketches.\n5\n\nFor an output sample of size k, these benchmark schemes include (i) probability proportional to\nsize(PPS) with replacement, where the samples consists of kindependent draws in which key xis\nselected with probability f(wx)=kf(w)k1, (ii) PPS without replacement (PPSWOR [ 48,49,16]),\nor (iii) priority sampling [ 45,20]. When using PPS with replacement, we get unbiased estimators\n\\f(wx) off(wx) for all keys x, and the variance is upper bounded by\nVar[\\f(wx)]\u00141\nkf(wx)kf(w)k1: (2)\nSimilar bounds (where the factor kin the denominator is replaced by k\u00002) can be derived for\nPPSWOR (see, e.g., [11]) and priority sampling [50].1\nRemark 2.1.We use an upper bound on the variance as the benchmark (instead of the exact\nvariance) for the following reason: Assume for simplicity that k= 1, in which case, in PPS each key\nxis sampled with probability px=f(wx)=kf(w)k1. Whenpxapproaches 1 for some key x(that is,\none key dominates the data), the variance Var[\\f(wx)] = (f(wx))2\u0010\n1\npx\u00001\u0011\nof the inverse-probability\nestimator approaches 0. Recall that the variance of PPS is a benchmark we are trying to get\nclose to using a di\u000berent sampling scheme. Since the variance when using PPS is 0, we cannot\napproximate it multiplicatively if we use a sampling scheme where xis sampled with a probability\nthat multiplicatively approximates px. However, when there is not just one key that dominates the\ndata, that is, when we know that px\u00141\u00001\ncfor allx, we get that Var[\\f(wx)]\u0015(f(wx))21\ncpx, so the\nbound on the variance is almost tight.\nConsequently, the variance of the sum estimator for the f-statistics of a domain His bounded\nby (due to nonpositive covariance shown in earlier works, e.g., [11]):\nVar\"\n\\X\nx2Hf(wx)#\n\u00141\nkX\nx2Hf(wx)kf(w)k1: (3)\nThe variance on the estimate of kf(w)k1is bounded by\nVar[\\kf(w)k1]\u00141\nkkf(w)k2\n1:\nWith these \\benchmark\" schemes, if we wish to get multiplicative error bound (normalized root\nmean squared error) of \"for estimatingkf(w)k1, we need sample size k=O(\"\u00002). We note that\nthe estimates are also concentrated in the Cherno\u000b sense [10, 20].\nWe refer to the probability vector\npx:=f(wx)\nkf(w)k1\nas the PPS sampling probabilities forf(w). Whenf(w) =wp(forp>0) we refer to sampling with\nthe respective PPS probabilities as `psampling.\n1The bound appears in some texts as1\nk\u00002f(wx)kf(w)k1and as1\nk\u00001f(wx)kf(w)k1in others. Speci\fcally, in\nbottom-kimplementations of PPSWOR/priority sampling, we need to store another value (the inclusion threshold) in\naddition to the sampled keys. If the kkeys we store include the threshold (so the sample size is e\u000bectively k\u00001), the\nbound hask\u00002 in the denominator. If we store kkeys and the inclusion threshold is stored separately (so we store a\ntotal ofk+ 1 keys), the bound has k\u00001 in the denominator.\n6\n\nEmulating a weighted sample. Letpbe the base PPS probabilities for f(w). When we use\na weighted sampling scheme with weights q6=pthen the variance bound (3)does not apply. We\nwill say that weighted sampling according to qemulates weighted sampling according to pwith\noverheadhif for allkand for all domains H\u0012X, a sample of size khprovides the variance bound\n(3) (and the respective concentration bounds).\nLemma 2.2. The overhead of emulating weighted sampling according to pusing weighted sampling\naccording to qis at most\nh(p;q) := max\nxpx\nqx:\nProof. We \frst bound the variance of \\f(wx)for a keyxwhen using weighted sampling by q. Consider\na weighted sample of size kaccording to base probabilities qx. The probability of xbeing included\nin a with-replacement sample of size kis 1\u0000(1\u0000qx)k. Then, the variance of the inverse-probability\nestimator is\nVar[\\f(wx)] = ( E[\\f(wx)])2\u0000(f(wx))2\n= (f(wx))2\u00121\n1\u0000(1\u0000qx)k\u00001\u0013\n= (f(wx))2\u0001(1\u0000qx)k\n1\u0000(1\u0000qx)k\n= (f(wx))2\u0001eln(1\u0000qx)k\n1\u0000eln(1\u0000qx)k\n\u0014(f(wx))2\u00011\nkln\u0010\n1\n1\u0000qx\u0011\u0014e\u0000x\n1\u0000e\u0000x\u00141\nx\u0015\n\u0014(f(wx))2\u00011\nkqx\u0014\nln\u00121\n1\u0000x\u0013\n\u0015xsince 1\u0000x\u0014e\u0000x\u0015\n=1\nk(f(wx))2\u00011\npx\u0001px\nqx\n=1\nkf(wx)kf(w)k1\u0001px\nqx:\nThe upper bound on the variance for a domain His:\n1\nk X\nx2Hf(wx)!\nkf(w)k1max\nx2Hpx\nqx: (4)\nFor anyH, the variance bound (4)is larger by the benchmark bound (3)by at most a factor of\nh(p;q). Hence, a sample of size kh(p;q) according to qemulates a sample of size kaccording to\np.\nWe emphasize the fact that a sample using q(instead of p) gets the same accuracy as sampling\nusing p, as long as we increase the sample size accordingly.\nRemark 2.3.Overhead bounds accumulate multiplicatively: If sampling according to qemulates a\nsample by pand a sample by q0emulates a sample by q, then a sample by q0emulates a sample by\npwith overhead h(q0;p)\u0014h(q0;q)h(q;p).\n7\n\nRemark 2.4 (Tightness) .The emulation overhead can be interpreted as providing an upper bound\nover all possible estimation tasks that the emulated sample could be used for. This de\fnition\nof overhead is tight if we wish to transfer guarantees for all subsets H: Consider a subset that\nis a single key xand sample size ksuch thatqx\u0014px\u001c1=k. The variance when sampling\naccording to qis\u0019(f(wx))2=(kqx) =1\nkf(wx)f(wx)\npxpx\nqx=1\nkf(wx)kf(w)k1px\nqx. This is a factor\nofpx=qxlarger than the variance when sampling according to px=f(wx)=kf(w)k1, which is\n\u0019(f(wx))2=(kpx) =1\nkf(wx)kf(w)k1.\nOverhead for estimating f-statistics. If we are only interested in estimates of the full f-\nstatisticskf(w)k1, the overhead reduces to the expected ratio Ex\u0018p[px=qx] instead of the maximum\nratio.\nCorollary 2.5. Letpbe the base PPS probabilities for f(w). Consider weighted sampling of size k\naccording to q. Then,\nVar[\\kf(w)k1]\u0014X\nx1\nkf(wx)kf(w)k1\u0001px\nqx=kf(w)k2\n1\nkX\nxpx\u0001px\nqx=kf(w)k2\n1\nkEx\u0018p\u0014px\nqx\u0015\n:\nMulti-objective emulation. Forh\u00151 and a function of frequency f, suppose there is a family\nFof functions such that a weighted sample according to femulates a weighted sample for every\ng2Fwith overhead h. A helpful closure property of such Fis the following.\nLemma 2.6. [10] The family Fof functions emulated by weighted sampling according to fis closed\nunder nonnegative linear combinations. That is, if ffig\u001aFandai\u00150, thenP\niaifi2F.\n2.2 O\u000b-the-Shelf Composable Sampling Sketches\nWe describe known polylogarithmic-size sampling sketches that we use or refer to in this work. These\nsketches are designed to provide statistical guarantees on the accuracy (bounds on the variance) with\nrespect to speci\fc functions of frequencies and all frequency distributions. The samples still provide\nunbiased estimates of statistics with respect to any function of frequency. We study the estimates\nprovided by these o\u000b-the-shelf sketches through the lens of combinations: instead of considering\na particular function of frequency and all frequency distributions, we study the overhead of more\ngeneral frequency-function combinations.\n(i)`1sampling without replacement. A PPSWOR sketch [ 12] (building on the aggregated\nscheme [ 49,16] and related schemes [ 26,22]) of sizekperforms perfect without replacement\nsampling of kkeys according to the weights wx. The sketch stores kkeys or hashes of\nkeys. In one pass over the data, we can pick the sample of kkeys. In order to compute the\ninverse-probability estimator, we need to know the exact frequencies of the sampled keys.\nThese frequencies and the corresponding inclusion probabilities can be obtained by a second\npass over the dataset. Alternatively, in a single streaming pass we can collect partial counts\nthat can be used with appropriate tailored estimators [12].\n(ii)`2sampling (and generally `psampling for p2[0;2]) with replacement. There are multiple\ndesigns based on linear projections [ 44,29,5,40]. Currently the best asymptotic space bound\nisO(log\u000e\u00001log2n) for 0<p< 2 (andO(log\u000e\u00001log3n) forp= 2) for a single sampled key,\n8\n\nwhere 1\u0000\u000eis the probability of success in producing the sample [ 32]. A with-replacement\nsample of size kcan then be obtained with a sketch with O(klog2nlog\u000e\u00001) bits.2We note\nthat on skewed distributions without-replacement sampling is signi\fcantly more e\u000bective for a\n\fxed sample size. Recent work [ 18] has provided sampling sketches that have size ~O(k) and\nperform without-replacement `2sampling for p\u00142.\n(iii) Sampling sketches for capping functions [ 11] and concave sublinear functions [ 14]. We will also\nconsider a multi-objective sample that emulates all concave sublinear functions of frequencies\nwith space overhead O(logn) [11].\nIn our analysis and experiments we compute the overhead of using the above sketches with\nrespect to certain frequencies and functions. Recall that for each target function of frequency, the\noverhead is computed with respect to the applicable base PPS probabilities px=f(wx)\nkf(w)k1. Consider\na frequency vector win non-increasing order ( wi\u0015wi+1). The base PPS sampling probabilities for\n`psampling are simply wp\ni=kwkp\np. The base PPS probabilities for multi-objective concave sublinear\nsampling are pi=p0\ni=kp0k1wherep0\ni:=wi\niwi+Pn\nj=i+1wj. These samples emulate sampling for all\nconcave-sublinear functions with overhead kp0k1.\n2.3 Datasets\nIn our experiments, we use the following datasets:\n•AOL [ 47]: A log of search queries collected over three months in 2006. For each query, its\nfrequency is the number of lines in which it appeared (over the entire 92 days).\n•CAIDA [ 7]: Anonymous passive tra\u000ec traces from CAIDA's equinix-chicago monitor. We\nuse the data collected over one minute (2016/01/21 13:29:00 UTC), and count the number of\npackets for each tuple (source IP, destination IP, source port, destination port, protocol).\n•Stack Over\row (SO) [ 46]: A temporal graph of interactions between users on the Stack\nOver\row website. For each node in the graph, we consider its weighted in degree (total number\nof responses received by that user) and its weighted out degree (total number of responses\nwritten by that user).\n•UGR [ 38]: Real tra\u000ec information collected from the network of a Spanish ISP (for network\nsecurity studies). We consider only one week of tra\u000ec (May 2016 Week 2). For a pair of\nsource and destination IP addresses, its frequency will be the number packets sent between\nthese two addresses (only considering \row labeled as \\background\", not suspicious activity).\n3 The Advice Model\nIn this section, we assume that in addition to the input, we are provided with oracle access to an\n\\advice\" model. When presented with a key x, the advice model returns a prediction axfor the total\nfrequency of xin the data. For simplicity, we assume that the advice model consistently returns the\nsame prediction for all queries with the same key. This model is similar to the model used in [28].\n2These constructions assume that the keys are integers between 1 and n. Also, the dependence on \u000eimproves with\nkbut we omit this for brevity.\n9\n\nAt a high level, our sampling sketch takes size parameters ( kh;kp;ku), maintains a set of at\nmostkh+kp+kukeys, and collects the exact frequencies wxfor these stored keys. The primary\ncomponent of the sketch is a weighted sample by advice of size kp. Every time an element with key\nxarrives, we query the advice model and get ax. The weighted sample by advice is a sample of kp\nkeys according to the weights f(a). Our sketch stores keys according to two additional criteria in\norder to provide robustness to the prediction quality of the advice:\n•The topkhkeys according to the advice model. This provides tolerance to inaccuracies in the\nadvice for these heaviest keys. Since these keys are included with probability 1, they will not\ncontribute to the error.\n•A uniform sample of kukeys. This allows keys that are \\below average\" in their contribution\ntokf(w)k1to be represented appropriately in the sample, regardless of the accuracy of the\nadvice. This provides robustness to the accuracy of the advice on these very infrequent keys\nand ensures they are not undersampled. Moreover, this ensures that all active keys ( wx>0),\nincluding those with potentially no advice ( ax= 0), have a positive probability of being\nsampled. This is necessary for unbiased estimation.\nWe provide an unbiased estimator that smoothly combines the di\u000berent sketch components and\nprovides the following guarantees.\nTheorem 3.1. Suppose the advice model is such that for some cp;cu\u00150andh\u00150, all keysxthat\nare active (wx>0) and not in the hlargest advice values of active keys ( ax<fayjwy>0g(n\u0000h+1))\nsatisfy\nf(wx)\nkf(w)k1\u0014maxfcpf(ax)\nkf(a)k1;cu\u00011\nng:\nThen for all k\u00151, the estimates from a sample-by-advice sketch with (kh;kp;ku) = (h;dkcpe+\n2;dkcue+ 2) satisfy the variance bound (3)for all domains H.\nThe implementation details appear below in Section 3.1 and the proof appears in Section 3.2.\nIn particular, if our advice is approximately accurate, say f(wx)\u0014f(ax)\u0014cp\u0001f(wx), the\noverhead when sampling by advice is cp.\nCorollary 3.2. Letfbe such that f(wx)\u0014f(ax)\u0014cpf(wx). Then for all k\u00151, the estimates\nfrom a sample-by-advice sketch with sample size (kh;kp;ku) = (0;dkcpe+ 2;0)satisfy the variance\nbound (3)for all domains H.\n3.1 Implementation\nThe pseudocode for our sampling-by-advice sketch and computing the respective estimators \\f(wx)\nis provided in Algorithms 1 and 2. Since the advice axpredicting the total frequency of a key is\navailable with each occurrence of the key, we can implement the weighted sample by advice f(ax)\nusing schemes designed for aggregated data (a model where each key occurs once with its full\nweight) such as [ 8,45,20,49,16,13]. Some care (e.g., using a hash function to draw the random\nseeds) is needed because keys may occur in multiple data elements. In the pseudocode, we show\nhow use a bottom- kimplementation of either PPSWOR or priority sampling. Since the sampling\nsketches are bottom- ksamples, they also support merging (pseudocode not shown). The pseudocode\nalso integrates the uniform and by-advice samples to avoid duplication (keys that qualify for both\nsamples are stored once).\n10\n\nAlgorithm 1: Sample by Advice (Data Processing)\nInput: A stream of updates, advice model a,kh;kp;ku(sample size for heaviest, by-advice,\nand uniform)\nOutput: A sampling sketch for f(w)\nInitialization:\nbegin\nDraw a hash function hsuch thath(x)\u0018D independently for all x //D=Exp(1)for\nPPSWOR,D=U[0;1]for priority\nCreate empty sample S:= (S:h;S:pu ) //S:h stores top khkeys by advice; S:pu is a\nbottom-ksketch for a combined weighted-by-advice and uniform sample\nProcess an update (x;\u0001)with prediction ax:\nbegin\nifx2Sthen //xis in (any component of) the sample\nwx wx+ \u0001 // Update the count of x\nelse\nifjS:hj<khorax>miny2S:ha:ythen //xhas a top- khadvice value\nwx \u0001\ninsert (x;wx) toS:h\nifjS:hj=kh+ 1then\nEjecty arg minz2S:hazfromS:h // eject key with smallest advice in S:h\nProcess (y;wy) by the by-advice+uniform sampling sketch S:pu\nelse\nprocess (x;\u0001) by the by-advice+uniform sampling sketch S:pu\n// Internal subroutine: process update (y;\u0001)by by-advice +uniform sampling sketch S:pu :\nbegin\nry h(y)\nf(ay)// Compute by-advice seed value of key y\nifry<frzjz2S:pug(kp)then //yhas one of kpsmallestryand qualifies for by-advice\nsample\nwy \u0001\nInsert (y;wy) toS:pu\nelse\nifh(y)<fh(z)jz2S:pug(ku)then //yhas one of kusmallesth(y)and qualifies for\nuniform sample\nwy \u0001\nInsert (y;wy) toS:pu together with ryandh(y)\nforeachz2S:pu such thath(z)>fh(z0)jz02S:pug(ku)and\nrz>frz0jz02S:pug(kp)do\nRemovezfromS:pu\n11\n\nAlgorithm 2: Sample by Advice (Estimator Computation)\nInput: A by-advice sampling sketch for f(w) with parameters kh;kp;ku\nOutput: Sparse representation of f(w) and estimate of kf(w)k1\nforeachxinShdo\n\\f(wx) f(wx)\nforeachx2S:pudo // keys stored in uniform/by-advice samples\n\u001cp frzjz2S:punfxgg(kp\u00001) // The (kp\u00001)thsmallest seed of a key other than x\n\u001cu fh(z)jz2S:punfxgg(ku\u00001) // The (ku\u00001)thsmallest hash value of a key other than x\nifh(x)<\u001cuorrx<\u001cpthen // keyxis strictly included in the uniform or by-advice\nsamples\npx Prh[h(x)<maxff(ax)\u001cp;\u001cug] // For PPSWOR 1\u0000e\u0000maxf\u001cu;f(ax)\u001cpg; For priority\nminfmaxf\u001cu;f(ax)\u001cpg;1g\n\\f(wx) f(wx)\npx\nReturnf(x;\\f(wx))gfor allxassigned with \\f(wx)(sparse representation of f(w)); The sum\nof the assigned \\f(wx) as an estimate of kf(w)k1.\n3.2 Analysis\nWe \frst establish that the sampling sketch returns the exact frequencies for sampled keys.\nLemma 3.3. The frequency wxof each key xin the \fnal sample is accurate.\nProof. Consider \frst the sample of size that is weighted by the advice f(ax) (without the other two\ncomponents: the uniform sample and the keys with largest advice values). Note that if a key enters\nthe sample when the \frst element with that key is processed and remains stored, its count will be\naccurate (we account for all the updates involving that key). Since the prediction axis consistent\n(i.e., the prediction axis the same in all updates involving x), the seed of xis the same every time x\nappears. For xto not enter the sample on its \frst occurrence or to be removed at any point, there\nmust be other kkeys in the sample with seed values lower than that of x. If such keys exist, xis\nnot in the \fnal sample. The argument for the khtop advice keys and for the kuuniformly-sampled\nkeys is similar.\nProof of Theorem 3.1. Since the estimators have nonpositive covariance, it su\u000eces to establish the\nupper bound\nVar[\\f(wx)]\u0014f(wx)kf(w)k1maxfcp\nkp\u00002;cu\nku\u00002g\nfor every key x.\nA keyxwith one of the top hadvised frequencies ( axvalues) is included with probability 1 and\nhasVar[\\f(wx)] = 0. For those keys, the claim trivially holds. Otherwise, recall that we assume that\nfor somecp;cu\u00150,\nf(wx)\nkf(w)k1\u0014maxfcpf(ax)\nkf(a)k1;cu\u00011\nng:\nSince the estimates are unbiased,\nVar[\\f(wx)] = E[\\f(wx)2\n]\u0000(f(wx))2= (f(wx))2Epx[(1=px\u00001)];\n12\n\nwherepxis as computed by Algorithm 2. Now note that px=maxfp0\nx;p00\nxg, wherep0\nxis the\nprobability xis included in a by-advice sample of size kpandp00\nxis the probability it is included in a\nuniform sample of size ku. Each of these samples is a bottom- ksample but with di\u000berent sampling\nweights: for each key x, the sampling weight in the by-advice sample is f(ax) and in the uniform\nsample is 1. We bound the variance for each of the two samples, and obtain that the variance in the\ncombined sample is the minimum of the two bounds.\nGenerally, given a PPSWOR sample or in priority sampling (as in our algorithm) with sampling\nweights v, we get the following variance bound on the estimate for vx:\nVar[bvx]\u00141\nk\u00002vxkvk1:\nIf we compute an estimate for f(wx) instead, we get\nVar[\\f(wx)] = Var\u0014f(wx)\nvx\u0001bvx\u0015\n=\u0012f(wx)\nvx\u00132\nVar[bvx]\u0014\u0012f(wx)\nvx\u00132vxkvk1\nk\u00002=(f(wx))2\nk\u00002\u0001kvk1\nvx:\nWe use this bound for each of the two samples.\nThe uniform sample is equivalent to setting vx= 1 for all keys. Using the bound above, the\nvariance from a uniform sample of size kuis bounded by1\nku\u00002n(f(wx))2. Iff(wx)\nkf(w)k1\u0014cu\u00011\nn, we\nsubstitutef(wx)\u0014kf(w)k1cu\u00011\nnand obtain\nVar[\\f(wx)]\u00141\nku\u00002n(f(wx))2\n\u00141\nku\u00002nf(wx)kf(w)k1cu\u00011\nn\n=cu\nku\u00002f(wx)kf(w)k1:\nThe weight vxof keyxin the by-advice sample of size kpisf(ax), so the variance is bounded by\n1\nkp\u00002kf(a)k1f(wx)2\nf(ax). Iff(wx)\nkf(w)k1\u0014cpf(ax)\nkf(a)k1, we similarly substitute and obtain that\nVar[\\f(wx)]\u0014cp\nkp\u00002f(wx)kf(w)k1:\nTogether, since at least one of the two cases must hold, we get that\nVar[\\f(wx)]\u0014f(wx)kf(w)k1maxfcp\nkp\u00002;cu\nku\u00002g:\nThe bound (3)follows from the choice of ( kh;kp;ku) and the nonpositive covariance of the estimators\nfor di\u000berent keys.\n3.3 Experiments\nWe evaluate the e\u000bectiveness of \\sampling by advice\" for estimating the frequency moments kwkp\np\nwithp= 3;7;10 on datasets from [ 47,46] (the datasets are described in Section 2.3). We use the\nfollowing advice models:\n13\n\n2628210212214\nSample Size10−610−510−410−310−210−1100Normalized Root Mean Square Error\n3rd Moment Est. Error on AOL Day 50\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nk\n2628210212214\nSample Size10−1510−1210−910−610−3100Normalized Root Mean Square Error\n10th Moment Est. Error on SO 07-12/2013 (in)\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nkFigure 1: Estimating the third moment on the AOL dataset (with learned advice) and the tenth\nmoment on the Stack Over\row dataset (with past frequencies as advice).\n•AOL [ 47]: We use the same predictions as in [ 28], which were the result training a deep\nlearning model on the queries from the \frst \fve days. We use the prediction to estimate\nfrequency moments on the queries from the 51st and 81st days (after removing duplicate\nqueries from multiple clicks on results).\n•Stack Over\row [ 46]: We consider two six-month periods, 1/2013{6/2013 and 7/2013{12/2013.\nWe estimate the in and out degree moments only on the data from 7/2013{12/2013, where\nthe advice for each node is its exact degree observed in earlier data (the previous six-month\nperiod).\nSome representative results are reported in Figure 1. Additional results are provided in Ap-\npendix A. The results reported for sampling by advice are with kh= 0 and two choices of balance\nbetween the by-advice and the uniform samples: kp=kuandku= 32. (The by-advice sample was\nimplemented using PPSWOR as in Algorithm 1.) We also report the performance of PPSWOR ( `1\nsampling without replacement), `2sampling (with and without replacement), and the benchmark\nupper bound for ideal PPS sampling according to f(w) =wp, which is 1 =p\nk.\nError estimation. Our sampling scheme provides unbiased estimators for kf(w)k1, wheref(w) =\nwp. We consider the normalized root mean square error (NRMSE), which for unbiased estimators is\nthe same as the coe\u000ecient of variation (CV), the ratio of the standard deviation to the mean:\n\u0010\nVar[\\kf(w)k1]\u00111=2\nkf(w)k1:\nA simple way to estimate the variance is to use the empirical squared error over multiple runs: We\ntake the average of ( \\kf(w)k1\u0000kf(w)k1)2over runs and apply a square root. We found that 50 runs\nwere not su\u000ecient for an accurate estimate with our sample-by-advice methods. This is because of\nkeys that had relatively high frequency and low advice, which resulted in low inclusion probability\nand high contribution to the variance. Samples that included these keys had higher estimates of the\n14\n\nstatistics than the bulk of other samples and often the signi\fcant increase was attributed to one or\ntwo keys. This could be remedied by signi\fcantly increasing the number of runs we average over.\nWe instead opted to use di\u000berent and more accurate estimators for the variance of the by-advice\nstatistics and, for consistency, the baseline with and without-replacement schemes.\nFor with-replacement schemes we computed an upper bound on the variance (and hence the\nNRMSE) as follows. The inclusion probability of a key xin a sample of size kis\np0\nx:= 1\u0000(1\u0000px)k\nwherepxis the probability to be selected in one sampling step. That is, px=wx=kwk1for\nwith-replacement `1sampling and px=w2\nx=kwk2\n2for with-replacement `2sampling. We can then\ncompute the per-key variance of our estimator exactly for each key x, which is (1 =p0\nx\u00001)(f(wx))2.\nSince estimates for di\u000berent keys have nonpositive covariance, the variance of our estimate of the\nstatisticskf(w)k1is at most\nVar[\\kf(w)k1]\u0014X\nx(1=p0\nx\u00001)(f(wx))2:\nFor without-replacement schemes (sampling-by-advice and the without-replacement reference\nmethods) we apply a more accurate estimator over the same set of 50 runs. For each \\run\" and each\nkeyx(sampled or not) we consider all the possible samples where the randomization (i.e., seeds) of\nall keysy6=xis \fxed as in the \\run.\" These include samples that include and do not include x.\nWe then compute the inclusion probability p0\nxof keyxunder these conditions (\fxed randomization\nfor all other keys). The contribution to the variance due to this set of runs is (1 =p0\nx\u00001)(f(wx))2.\nWe sum the estimates (1 =p0\nx\u00001)(f(wx))2over all keys and take the average of the sums over runs\nas our estimate of the variance. The inclusion probability p0\nxis computed as follows.\nFor bottom- ksampling (in this case, PPSWOR by wqforq= 1;2), recall that we compute random\nseedvalues to keys of the form rx=wq\nx, whererx\u0018D are independent. The sample includes the k\u00001\nkeys with lowest seed values. The inclusion probability is computed with respect to a threshold that\nis de\fned to be the ( k\u00001)-th smallest \\seed\" value of other keys \u001cx fseed(y)jy6=xg(k\u00001). For\nkeys in the sample, this is the k-th smallest seed overall. For keys not in the sample the applicable\n\u001cxis the (k\u00001)-th smallest seed overall. We get p0\nx=Prr\u0018D[rx=wq\nx\u0014\u001cx] (a di\u000berent p0\nxis obtained\nin each run).\nThe calculation for the by-advice sampling sketch is as in the estimator in Algorithm 1, except\nthat it is computed for all keys x.\nDiscussion. We observe that for the third moment, sampling by advice did not perform sig-\nni\fcantly better than PPSWOR (and sometimes performed even worse). For higher moments,\nhowever, sampling by advice performed better than PPSWOR when the sample size is small.\nWith-replacement `2sampling was more accurate than advice (without-replacement `2sampling\nperforms best). Our analysis in the next section explains the perhaps surprisingly good performance\nof`1and`2sampling schemes.\n4 Frequency-Function Combinations\nIn this section, we study the case of inputs that come from restricted families of frequency distribu-\ntionsW. Restricting the family Wof possible inputs allows us to extend the family Fof functions\nof frequencies that can be e\u000eciently emulated with small overhead.\n15\n\nSpeci\fcally, we will consider sampling sketches and corresponding combinations (W;F;h ) of\nfrequency vectors W, functions of frequencies F, and overhead h\u00151, so that for every frequency\ndistribution w2Wand function f2F, our sampling sketch emulates a weighted sample of f(w)\nwith overhead h. We will say that our sketch supports the combination ( W;F;h ).\nRecall that emulation with overhead hmeans that a sampling sketch of size h\"\u00002(i.e., holding\nthat number of keys or hashes of keys) provides estimates with NRMSE \"forkf(w)k1forany\nf2Fand that these estimates are concentrated in a Cherno\u000b-bound sense. Moreover, for any\nf2Fwe can estimate statistics of the formP\nxLxf(wx) with the same guarantees on the accuracy\nprovided by a dedicated weighted sample according to f.\nWe study combinations supported by the o\u000b-the-shelf sampling schemes that were described in\nSection 2.2. We will use the notation w=fwigfor dataset frequencies, where wiis the frequency of\nthei-th most frequent key.\n4.1 Emulating an `pSample by an `qSample\nWe express the overhead of emulating `psampling by `qsampling (p\u0015q) in terms of the properties\nof the frequency distribution. Recall that `psampling (and estimating the p-th frequency moment)\ncan be implemented with polylogarithmic-size sketches for p\u00142 but requires polynomial-size\nsketches in the worst case when p>2.\nLemma 4.1. Consider a dataset with frequencies w(in non-increasing order). For p\u0015q, the\noverhead of emulating `psampling by `qsampling is bounded by\n\r\r\rw\nw1\r\r\rq\nq\r\r\rw\nw1\r\r\rp\np: (5)\nProof. The sampling probabilities for key iunder`psampling and `qsampling arewp\ni\nkwkp\npandwq\ni\nkwkq\nq,\nrespectively. Then, the overhead of emulating `psampling by `qsampling is\nmax\niwp\ni=kwkp\np\nwq\ni=kwkq\nq= max\niwp\u0000q\ni\u0001kwkq\nq\nkwkp\np=wp\u0000q\n1\u0001kwkq\nq\nkwkp\np=\r\r\rw\nw1\r\r\rq\nq\r\r\rw\nw1\r\r\rp\np:\nWe can obtain a (weaker) upper bound on the overhead, expressed only in terms of q, that\napplies to all p\u0015q:\nCorollary 4.2. The overhead of emulating `psampling using `qsampling (for any p\u0015q) is at\nmost\r\r\rw\nw1\r\r\rq\nq.\nProof. For any set of frequencies w, the normalized norm\r\r\rw\nw1\r\r\rp\npis non-increasing with pand is at\nleast 1. Therefore, the overhead (5) is\n\r\r\r\rw\nw1\r\r\r\rq\nq\u001e\r\r\r\rw\nw1\r\r\r\rp\np\u0014\r\r\r\rw\nw1\r\r\r\rq\nq:\n16\n\nRemark 4.3.Emulation as described above works when p\u0015q. Whenq>p , the maximum in the\noverhead bound (see proof of Lemma 4.1) is incurred on the least frequent key, with frequency wn.\nWe therefore get a bound of\r\r\rw\nwn\r\r\rq\nq=\r\r\rw\nwn\r\r\rp\npand Corollary 4.2 does not apply.\n4.2 Frequency Distributions with a Heavy Hitter\nWe show that for distributions with an `qheavy hitter, `qsampling emulates `psampling for all\np\u0015qwith a small overhead.\nDe\fnition 4.4. Consider frequencies w. An`q\u001e-heavy hitter is de\fned to be a key such that\nwq\ni\u0015\u001e\u0001kwkq\nq.3\nWe can now restate Corollary 4.2 in terms of a presence of a heavy hitter:\nCorollary 4.5. Letwbe a frequency vector with a \u001e-heavy hitter under `q. Then for p\u0015q, the\noverhead of using an `qsample to emulate an `psample is at most 1=\u001e.\nProof. If there is an `q\u001e-heavy hitter, then the most frequent key (the key with frequency w1)\nmust be a\u001e-heavy hitter. From the de\fnition of a heavy hitter,\r\r\rw\nw1\r\r\rq\nq\u00141\n\u001e, and we get the desired\nbound on the overhead.\nWe are now ready to specify combinations ( W;F;h ) of frequency vectors W, functions of\nfrequencies F, and overhead h\u00151 that are supported by `qsampling.\nTheorem 4.6. For anyq>0and\u001e2(0;1], an`q-sample supports the combination\nW:=fwwith an`q\u001e-heavy hitterg\nF:=ff(w) =wpjp\u0015qg+\nh:= 1=\u001e ;\nwhere the notation F+is the closure of a set Fof functions under nonnegative linear combinations.\nProof. The claim for functions f(w) =wpis immediate from Corollary 4.5. The claim for the\nnonnegative convex closure of these function is a consequence of Lemma 2.6.\nIn particular, if the input distribution has an `q\u001e-heavy hitter, then an `qsample of size \"\u00002=\u001e\nemulates an `psample of size \"\u00002for anyp>q .\nTable 1 reports properties and the relative `1and`2weights of the two most frequent keys for\nour datasets (described in Section 2.3). We can see that the most frequent key is a heavy hitter with\n1=\u001e\u001421 for`2and 1=\u001e\u0014715 for`1which gives us upper bounds on the overhead of emulating any\n`psample (p\u00152) by an`1or`2sample. Table 2 reports (for p= 3;10) the overhead of emulating\nthe respective `psample and the (smaller) overhead of estimating the p-th moment. (Recall the\nde\fnitions of both types of overheads from Section 2.1.) We can see that high moments can be\nestimated well from `2samples and with a larger overhead from `1samples.\n3Another de\fnition that is common in the literature uses wi\u0015\u001e\u0001kwkqinstead ofwq\ni\u0015\u001e\u0001kwkq\nq.\n17\n\nTable 1: Datasets\nDatasetn=106lnn`1HH `2HH Zipf Parameter\nSO out 2:23 14:60:0016;0:0010 0:0532;0:0221 1:48\nSO in 2:30 14:60:0014;0:0008 0:1068;0:0364 1:38\nAOL 10:15 16:10:0274;0:0091 0:8268;0:0911 0:77\nCAIDA 1:07 13:90:0033;0:0032 0:0479;0:0458 1:35\nUGR 79:38 18:20:1117;0:0400 0:8499;0:1093 1:35\nTable 2: Overhead\nDataset `1Emul. Overhead (Est. Overhead) `2Emul. Overhead (Est. Overhead) Concave\n3rd 10th Universal 3rd 10th Universal Universal\nSO out 124.30 (42.76) 600.44 (577.57) 624.58 3.74 (1.90) 18.04 (17.36) 1:25\u00021051672.60\nSO in 299.80 (155.32) 677.56 (673.45) 681.72 4.12 (2.58) 9.30 (9.25) 4:20\u00021041628.36\nAOL 34.81 (33.45) 36.38 (36.37) 92.92 1.16 (1.12) 1.21 (1.21) 2:94\u0002105170.84\nCAIDA 31.23 (18.73) 90.66 (56.20) 301.28 2.16 (1.57) 6.28 (4.03) 2:65\u0002105846.15\nUGR 8.52 (8.17) 8.95 (8.95) 772.83 1.12 (1.09) 1.18 (1.18) 1:89\u00021011143.94\nCerti\fed emulation. The quality guarantees of a combination ( W;F;h ) are provided when\nw2W. In practice, however, we may compute samples of arbitrary dataset frequencies w.\nConveniently, we are able to test the validity of emulation by considering the most frequent key in\nthe sample: For an `qsample of size kwe can compute r maxx2Swq\nx=kwkq\nqand certify that our\nsample emulates `psamples (p>q ) of sizekr. Ifkris small, then we do not provide meaningful\naccuracy but otherwise we can certify the emulation with sample size kr. When the input whas\nan`q\u001e-heavy hitter then a with-replacement `qsample of size kwill include it with probability at\nleast 1\u0000e\u0000k\u001eand the result will be certi\fed. Note that the result can only be certi\fed if there is a\nheavy hitter.\nTradeo\u000b between WandF.Ifwhas an`q\u001e-heavy hitter, then w1is an`q\u001e-heavy hitter,\nand is also an `p\u001e-heavy hitter for every p\u0015q. This means that for the target of moments with\np\u00152, an`2sample supports a larger set Wof frequencies than an `1sample, including those with\nan`2\u001e-heavy hitter but not an `1\u001e-heavy hitter. The `1sample however supports a larger family\nof target functions Fthat includes moments with p2[1;2). Note that for a \fxed overhead, with `q\nsampling the set Fof supported functions decreases with qwhereasWincreases with q.\n4.3 Zip\fan and Sub-Zip\fan Frequencies\nZipf distributions are very commonly used to model frequency distributions in practice. We explore\nsupported combinations with frequencies that are (approximately) Zipf.\nDe\fnition 4.7. We say that a frequency vector w(of sizekwk0=n) is Zipf[\u000b;n](Zipf with\nparameter\u000b) if for alli,wi=w1=i\u0000\u000b.\nValues\u000b2[1;2] are common in practice. The best-\ft Zipf parameter for the datasets we studied\nis reported in Table 1, and their frequency distributions (sorted by rank) are shown in Figure 2. We\ncan see that our datasets are approximately Zipf (which would be an approximate straight line) and\nfor all but one we have \u000b2[1:3;1:5].\nWe now de\fne a broader class of approximately Zip\fan distributions.\n18\n\n101103105107\nIndex in Sorted Order10−910−710−510−310−1Weight (/lscript1)\nSO (out)\nSO (in)\nAOL\nCAIDA\nUGRFigure 2: The frequency distributions of the datasets (sorted in non-increasing order).\nTable 3: Supported Combinations for subZipf [\u000b;c;n ] Frequencies\nMethod subZipf Parameters F Overhead\n`1samplingf\u000b\u00152gff(w) =wpjp\u00151g+1:65c\n`1samplingf\u000b\u00151gff(w) =wpjp\u00151g+(1 + lnn)c\n`2samplingf\u000b\u00151gff(w) =wpjp\u00152g+1:65c2\n`2samplingf\u000b\u00151=2gff(w) =wpjp\u00152g+(1 + lnn)c2\nDe\fnition 4.8. A frequency vector wissubZipf [\u000b;c;n ]if for alli,wi\nw1\u0014ci\u0000\u000b.4\nNote that Zipf[\u000b;n] is sub-Zip\fan with the same \u000bandc= 1. We show that sub-Zipf frequencies\n(and in particular Zipf frequencies) have heavy hitters.\nLemma 4.9. If a frequency vector wissubZipf [\u000b;c;n ], andqis such that q\u000b\u00151, the frequency\nvector has an `q1\ncqHn;\u000bq-heavy hitter, where Hn;\u000b:=Pn\ni=1i\u0000\u000bare the generalized harmonic numbers.\nProof. By the de\fnition of subZipf [\u000b;c;n ] frequencies,\n\r\r\r\rw\nw1\r\r\r\rq\nq=nX\ni=1\u0012wi\nw1\u0013q\n\u0014cqnX\ni=1i\u0000\u000bq=cqHn;\u000bq:\nHence, the largest element is an `q1\ncqHn;\u000bq-heavy hitter.\nTable 3 lists supported combinations that include these approximately Zip\fan distributions.\nLemma 4.10. The combinations shown in Table 3 are supported by `1and`2samples.\nProof. We use Lemma 4.9 and Theorem 4.6. Recall that when \u000b= 1, the harmonic sum is\nHn;1\u00141 +lnn. For\u000b>1,Hn;\u000b\u0014\u0010(\u000b), where\u0010(\u000b) :=P1\ni=1i\u0000\u000bis the Zeta function. The Zeta\nfunction is decreasing with \u000b, de\fned for \u000b>1 with an asymptote at \u000b= 1, and is at most 1 :65 for\n\u000b\u00152.\n4This is a slight abuse of notation since the parameters do not fully specify a distribution.\n19\n\nWhenq\u000b\u00152, the overhead is at most cq\u0010(q\u000b)\u00141:65cq. Whenq\u000b= 1 the overhead is at most\n(1 + lnn)cqand whenq\u000b> 1 we can bound it by min f1 + lnn;\u0010(q\u000b)gcq.\nWe see that for these approximately Zipf distributions, `1or`2samples emulate `psamples with\nsmall overhead.\n4.4 Experiments on Estimate Quality\nThe overhead factors reported in Table 2 are in a sense worst-case upper bounds (for the dataset\nfrequencies). Figure 4 reports the actual estimation error (normalized root mean square error)\nfor high moments for representative datasets as a function of sample size. The estimates are\nwith PPSWOR ( `1sampling without replacement) and `2sampling with and without replacement.\nAdditional results are reported in Appendix A. We observe that the actual accuracy is signi\fcantly\nbetter than even the benchmark bounds.\nFinally we consider estimating the full distribution of frequencies, that is, the curve that relates\nthe frequencies of keys to their ranks. We do this by estimating the actual rank of each key in the\nsample (using an appropriate threshold function of frequency) | more details are in Appendix A.\nRepresentative results are reported in Figure 3 for PPSWOR and with-replacement `2sampling\n(additional results are reported in Appendix A). We used samples of size k= 32 ork= 1024 for\neach set of estimates. We observe that generally the estimates are fairly accurate even with a small\nsample size (despite the fact that threshold functions require large sketches in the worst case). We\nsee that`2samples are accurate for the frequent keys but often have no representatives from the\ntail whereas the without-replacement `1samples are more accurate on the tail.\n100101102103104105106107\nIndex in Sorted Order10−710−610−510−410−310−2Weight (/lscript1)\nEstimated Distribution: AOL\nActual\n/lscript1,k= 32\n/lscript1,k= 1024\n/lscript2(with rep.), k= 32\n/lscript2(with rep.), k= 1024\n101103105107\nIndex in Sorted Order10−910−710−510−310−1Weight (/lscript1)\nEstimated Distribution: UGR\nActual\n/lscript1,k= 32\n/lscript1,k= 1024\n/lscript2(with rep.), k= 32\n/lscript2(with rep.), k= 1024\nFigure 3: Actual and estimated distribution of frequency by rank. The estimates use PPSWOR and\nwith-replacement `2sampling and sample sizes k= 32;1024.\n4.5 Worst-Case Bound on Overhead\nThe overhead (5)ofkw=w1k2\n2=kw=w1kp\npis the space factor increase needed for an `2sample to\nemulate an `psample on the frequencies w(and accurately estimate the p-th moment). A natural\nquestion is whether there is a better way to emulate an `psampling with a polylogarithmic size\nsampling sketch. The following shows that in a sense an `2sample is the best we can do.\n20\n\n252729211213\nSample Size10−810−610−410−2100Normalized Root Mean Square Error\n3rd Moment Est. Error on AOL\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−1510−1210−910−610−3100Normalized Root Mean Square Error\n10th Moment Est. Error on AOL\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−910−710−510−310−1Normalized Root Mean Square Error\n3rd Moment Est. Error on UGR\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−1510−1210−910−610−3100Normalized Root Mean Square Error\n10th Moment Est. Error on UGR\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nkFigure 4: Estimating 3rd and 10th moments on various datasets using PPSWOR ( `1without\nreplacement) and `2samples (with and without replacement). The error is averaged over 50\nrepetition.\nLemma 4.11. Letp\u00152. The overhead of emulating an `psample using an `2sample on any\nfrequency vector wof sizenisn1\u00002=p.\nProof. For any vector v2Rn, H older's inequality implies that kvk2\u0014n1=2\u00001=pkvkp. As a result,\nkvk2\n2\u0014n1\u00002=pkvk2\np\u0014n1\u00002=pkvkp\np:\nUsing Lemma 4.1, we can bound the overhead of emulating an `psample using an `2sample on any\nfrequency vector was\nkw=w1k2\n2\nkw=w1kp\np\u0014n1\u00002=p:\nThis matches upper bounds on sketch size attained with dedicated sketches for p-th moment\nestimation [ 31,5] and the worst-case lower bound of ~\n(n1\u00002=p) [4,35]. Interestingly, the worst case\ndistributions that establish that bound are those where the most frequent key is an `pheavy-hitter\nbut not an `2heavy-hitter.\n21\n\n4.6 Near-Uniform Frequency Distributions\nWe showed that frequency distributions with heavy hitters are easy for high moments and moreover,\nthe validity of the result can be certi\fed. Interestingly, the other extreme of near-uniform distributions\n(wherew1=wnis bounded) is also easy. Unfortunately, unlike the case with heavy hitters, there is\nno \\certi\fcate\" to the validity of the emulation.\nLemma 4.12. Letwbe a frequency distribution with support size n. Then the overhead of using `1\nor`0sampling to emulate `psampling is at most\u0010\nw1\nwn\u0011p\n.\nProof. We use Lemma 4.1 and lower bound the denominator\r\r\rw\nw1\r\r\rp\np. Note that for any wwith\nsupport size n,\r\r\rw\nw1\r\r\r1\n1\u0014nand\r\r\rw\nw1\r\r\r\n0=n. Now,\n\r\r\r\rw\nw1\r\r\r\rp\np=nX\ni=1\u0012wi\nw1\u0013p\n\u0015nX\ni=1\u0012wn\nw1\u0013p\n=n\u0001\u0012wn\nw1\u0013p\n:\nThe overhead for q2f0;1gis then\n\r\r\rw\nw1\r\r\rq\nq\r\r\rw\nw1\r\r\rp\np\u0014n\nn\u0001\u0010\nwn\nw1\u0011p=\u0012w1\nwn\u0013p\n:\n5 Universal Samples\nIn this section, we study combinations where the emulation is universal , that is, the set of functions\nFincludes all monotonically non-decreasing functions of frequencies. We denote the set of all\nmonotonically non-decreasing functions by M. Interestingly, there are sampling probabilities that\nprovide universal emulation for any frequency vector w.\nLemma 5.1. [10] Consider the probabilities qwhere thei-th most frequent key has qi=1\niHn\n(Hn=Pn\ni=11\niis then-th harmonic number). Then a weighted sample by qis a universal emulator\nwith overhead at most Hn.\nProof. Consider a monotonically non-decreasing fwith respective PPS probabilities p. By de\fnition,\nfor thei-th most frequent key, pi=f(wi)\nkf(w)k1\u0014f(wi)\nf(w1)+:::+f(wi)\u00141\ni(note that for j\u0014i,f(wi)\u0014f(wj)).\nTherefore,pi=qi\u00141\ni(iHn) =Hn.\nThis universal sampling, however, cannot be implemented with small (polylogarithmic size)\nsketches. This is because Mincludes functions that require large (polynomial size) sketches such\nas thresholds ( Iw\u0015Tfor some \fxed T) and high moments ( p>2). We therefore aim to \fnd small\nsketches that provide universal emulation for a restricted W.\nFor particular sampling probabilities qand frequencies wwe consider the universal emulation\noverhead to be the overhead factor that will allow the sample according to qto emulate weighted\n22\n\nsampling with respect to f(w) for anyf2M(the set of all monotonically non-decreasing functions):\nmax\nf2Mmax\nif(wi)=(kf(w)k1qi) (6)\nInterestingly, the universal emulation overhead of qdoes not depend on the particular w.\nLemma 5.2. The universal emulation overhead of qis at most\nmax\ni1=(iqi):\nThe bound maxi1=(iqi)is tight and always at least Hneven when Wcontains a single w, as long\nas frequencies are distinct ( wi>wi+1for alli).\nProof. Consider w. We have\nmax\nf2Mmax\nif(wi)=(kf(w)k1qi)\n= max\nimax\nf2Mf(wi)=(kf(w)k1qi)\u0014max\ni1=(iqi):\nAs in Lemma 5.1, the last inequality follows since fis non-decreasing: for all j <i we must have\nf(wj)\u0015f(wi). Therefore, f(wi)=kf(w)k1\u00141=i, and we get that the overhead is upper-bounded\nby maxi1=(iqi).\nNote that for the threshold function f(w) =Iw\u0015wi,f(wi)=kf(w)k1= 1=i(ifwi+1<wi). Hence,\nwhen the frequencies are distinct, the bound max i1=(iqi) on the overhead is tight.\nTo conclude the proof, we show that maxi1=(iqi)\u0015Hn. Assume, for the sake of contradiction,\nthat1\niqi<Hnfor alli. Then,1\ni<qiHn, and by summing over all i, we getPn\ni=11\ni<Pn\ni=1qiHn=\nHn, which is a contradiction. Therefore, we get that max i1=(iqi)\u0015Hn.\nRemark 5.3.Lemmas 5.1 and 5.2 are connected in the following way. Suppose we are given sampling\nprobabilities qand would like to compute their universal emulation overhead. From Lemma 5.2,\nwe know that the overhead is bounded by maxi1=(iqi). Alternatively, Lemma 5.1 tells us that the\nsampling probabilities f1\niHnghave universal emulation overhead Hn. The overhead of emulating the\nsampling probabilities f1\niHngusing qismaxi1\niHnqi. Since the overhead accumulates multiplicatively\n(Remark 2.3), we can combine the last two statements, and conclude the universal emulation\noverhead of qis upper-bounded by max i1\niHnqi\u0001Hn= maxi1=(iqi), as in Lemma 5.2.\nWe can similarly consider, for given sampling probabilities q, the universal estimation overhead ,\nwhich is the overhead needed for estimating full f-statistics for all f2M. As discussed in Section 2,\nestimation of the full f-statistics is a weaker requirement than emulation. Hence, for any particular\nqthe estimation overhead can be lower than the emulation overhead. The estimation overhead,\nhowever, is still at least Hn.\nLemma 5.4. The universal estimation overhead for estimating all monotone f-statistics for qis\nmax\ni1\ni2iX\nj=11\nqj:\n23\n\nProof. By Corollary 2.5, the universal estimation overhead with frequencies wis\nmax\nf2MX\nj(f(wj))2\nkf(w)k2\n11\nqj: (7)\nIt su\u000eces to consider fthat are threshold functions (follows from the inequalitya1+:::+ak\nb1+:::+bk\u0014\nmaxj=1;:::;kaj\nbjas eachf2Mcan be represented as a combination of threshold functions). Speci\fcally,\nconsider the threshold function f(w) =Iw\u0015wifor somei. As in Lemmas 5.1 and 5.2, the expression\n(7)for the threshold function f(w) =Iw\u0015wihasf(wj)=kf(w)k1= 1=iforj\u0014iand 0 otherwise\n(assuming all the frequencies in ware distinct). We get that the sum is1\ni2Pi\nj=11\nqj. The claim\nfollows from taking the maximum over all threshold functions.\n5.1 Universal Emulation Using O\u000b-the-Shelf Sampling Sketches\nIn our context, the probabilities qare not something we directly control but rather emerge as an\nartifact of applying a certain sampling scheme to a dataset with certain frequencies w. We will\nexplore the universal overhead of the qobtained when applying o\u000b-the-shelf schemes (see Section 2.2)\ntoZipffrequencies and to our datasets.\nLemma 5.5. For a frequency vector wthat is Zipf[\u000b;n],`psampling with p= 1=\u000bis a universal\nemulator with (optimal) overhead Hn.\nProof. Consider the sampling probability qifor thei-th key (with frequency wi) when using `1=\u000b\nsampling. From De\fnition 4.7, we know that wi=w1\ni\u000b. We get that w1=\u000b\ni=w1=\u000b\n1\ni, and consequently,\nqi=w1=\u000b\niPn\nj=1w1=\u000b\nj=w1=\u000b\n1=i\nPn\nj=1w1=\u000b\n1=j=1=iPn\nj=11=j=1\niHn:\nThe sampling probabilities qare the same as in Lemma 5.1. As a result, we get that applying `1=\u000b\nsampling to the frequency vector whas universal emulation overhead of Hn.\nInterestingly, for \u000b\u00151=2, universal emulation as in Lemma 5.5 is attained by `psampling with\np\u00142, which can be implemented with polylogarithmic-size sketches. Note that we match here a\ndi\u000berent`psample for each possible Zipfparameter\u000bof the data frequencies. A sampling scheme\nthat emulates `psampling for a range [ p1;p2] ofpvalues with some overhead hwill be a universal\nemulator with overhead hHnforZipf[\u000b] for\u000b2[1=p2;1=p1] (see Remark 2.3). One such sampling\nscheme with polylogarithmic-size sketches was provided in [ 11,10]. The sample emulates all concave\nsublinear functions, including capping functions f(w) =minfw;TgforT > 0 and low moments\nwithp2[0;1], withO(logn) overhead.\nWe next express a condition on frequency distributions under which a multi-objective concave-\nsublinear sample provides a universal emulator. The condition is that for all i, the weight of the\ni-th most frequent key is at least c=itimes the weight of the tail from i.\nLemma 5.6. Letwbe such that miniiwiPn\nj=i+1wj\u0015c. Then a sample that emulates all concave-\nsublinear functions with overhead h0is a universal emulator for wwith overhead at most h0(1 + 1=c).\n24\n\nProof. Denote the sampling probabilities of the given sampling method (that emulates all concave-\nsublinear functions) by q. By Lemma 5.2, we need to bound maxi1=(iqi). First, consider the\ni-th largest frequency wi, and assume that we are sampling according to the capping function\nf(w) =minfw;wiginstead of q. Later we will remove this assumption (and pay additional overhead\nh0) since the given sampling scheme emulates all capping functions. When sampling according to\nf(w) =minfw;wig, the sampling probability of the i-th key isf(wi)Pn\nj=1f(wj). Our goal is to bound\n1\nif(wi)Pn\nj=1f(wj). Using the condition in the statement of the lemma,\nf(wi)Pn\nj=1f(wj)=wi\niwi+Pn\nj=i+1wj\u0015wi\niwi+i\ncwi=1\ni(1 + 1=c):\nHence,\n1\nif(wi)Pn\nj=1f(wj)\u00141\ni1\ni(1+1=c)= 1 +1\nc:\nWe now remove the assumption that we sample according to f(w) =minfw;wig. Note that the\ngiven sampling scheme emulates all concave sublinear functions (a family which includes all capping\nfunctions) with overhead h0. From the de\fnition of emulation overhead, for f(w) =minfw;wigwe\nget that\nf(wi)\nqiPn\nj=1f(wj)\u0014h0:\nNow,\n1\niqi=f(wi)\nqiPn\nj=1f(wj)\u00011\nif(wi)Pn\nj=1f(wj)\u0014h0\u0012\n1 +1\nc\u0013\n:\nSince the bound applies to all i, we get that the universal emulation overhead is maxi1=(iqi)\u0014\nh0(1 + 1=c), as desired.\nInterestingly, for high moments to be \\easy\" it su\u000eced to have a heavy hitter. For universal\nemulation we need to bound from below the relative weight of each key with respect to the remaining\ntail.\nExperimental results. Table 2 reports the universal emulation overhead on our datasets with `1,\n`2, and multi-objective concave-sublinear sampling probabilities. We observe that while `2sampling\nemulates high moments extremely well, its universal overhead is very large due to poor emulation\nof \\slow growth\" functions. The better universal overhead hof`1and concave-sublinear samples\nsatis\fesh2[92;1673]. It is practically meaningful as it is in the regime where h\"\u00002\u001cn(the\nnumber of keys is signi\fcantly larger than the sample size needed to get normalized root mean\nsquared error \").\nThe universal emulation overhead was computed using Lemma 5.2 with respect to base PPS\nprobabilities for the o\u000b-the-shelf sampling schemes (given in Section 2.2).\n25\n\n6 Conclusion\nIn this work, we studied composable sampling sketches under two beyond-worst-case scenarios. In\nthe \frst, we assumed additional information about the input distribution in the form of advice. We\ndesigned and analyzed a sampling sketch based on the advice, and demonstrated its performance on\nreal-world datasets.\nIn the second scenario, we proposed a framework where the performance and statistical guarantees\nof sampling sketches were analyzed in terms of supported frequency-function combinations . We\ndemonstrated analytically and empirically that sketches originally designed to sample according\nto \\easy\" functions of frequencies on \\hard\" frequency distributions turned out to be accurate for\nsampling according to \\hard\" functions of frequencies on \\practical\" frequency distributions. In\nparticular, on \\practical\" distributions we could accurately approximate high frequency moments\n(p>2) and the rank versus frequency distribution using small composable sketches.\nAcknowledgments\nWe are grateful to the authors of [ 28], especially Chen-Yu Hsu and Ali Vakilian, for sharing their\ndata, code, and predictions with us. We thank Ravi Kumar and Robert Krauthgamer for helpful\ndiscussions. The work of O\fr Geri was partially supported by Moses Charikar's Simons Investigator\nAward. The work of Rasmus Pagh was supported by Investigator Grant 16582, Basic Algorithms\nResearch Copenhagen (BARC), from the VILLUM Foundation.\nReferences\n[1]Anders Aamand, Piotr Indyk, and Ali Vakilian. (Learned) frequency estimation algorithms\nunder Zip\fan distribution. arXiv , abs/1908.05198, 2019. http://arxiv.org/abs/1908.05198 .\n[2]Pankaj K. Agarwal, Graham Cormode, Zengfeng Huang, Je\u000b M. Phillips, Zhewei Wei, and\nKe Yi. Mergeable summaries. ACM Transactions on Database Systems , 38(4), December 2013.\n[3]Noga Alon, Phillip B Gibbons, Yossi Matias, and Mario Szegedy. Tracking join and self-join\nsizes in limited storage. Journal of Computer and System Sciences , 64(3):719{747, 2002.\n[4]Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the\nfrequency moments. Journal of Computer and System Sciences , 58(1):137 { 147, 1999.\n[5]Alexandr Andoni, Robert Krauthgamer, and Krzysztof Onak. Streaming algorithms via\nprecision sampling. In IEEE 52nd Annual Symposium on Foundations of Computer Science,\nFOCS 2011 . IEEE Computer Society, 2011.\n[6]Vladimir Braverman and Rafail Ostrovsky. Zero-one frequency laws. In Proceedings of the\n42nd ACM Symposium on Theory of Computing, STOC 2010 , pages 281{290. ACM, 2010.\n[7]CAIDA. The CAIDA UCSD anonymized internet traces 2016 | 2016/01/21 13:29:00 UTC.\nhttps://www.caida.org/data/passive/passive dataset.xml, 2016.\n[8]M. T. Chao. A general purpose unequal probability sampling plan. Biometrika , 69(3):653{656,\n1982.\n26\n\n[9]Moses Charikar, Kevin C. Chen, and Martin Farach-Colton. Finding frequent items in data\nstreams. In Automata, Languages and Programming, 29th International Colloquium, ICALP\n2002, volume 2380 of Lecture Notes in Computer Science , pages 693{703. Springer, 2002.\n[10]Edith Cohen. Multi-objective weighted sampling. In 2015 Third IEEE Workshop on Hot Topics\nin Web Systems and Technologies (HotWeb) , pages 13{18, 2015.\n[11] Edith Cohen. Stream sampling framework and application for frequency cap statistics. ACM\nTransactions on Algorithms , 14(4):52:1{52:40, 2018.\n[12]Edith Cohen, Graham Cormode, and Nick G. Du\u000eeld. Don't let the negatives bring you down:\nsampling from streams of signed updates. In ACM SIGMETRICS/PERFORMANCE Joint\nInternational Conference on Measurement and Modeling of Computer Systems, SIGMETRICS\n'12, pages 343{354. ACM, 2012.\n[13]Edith Cohen, Nick Du\u000eeld, Haim Kaplan, Carsten Lund, and Mikkel Thorup. E\u000ecient\nstream sampling for variance-optimal estimation of subset sums. SIAM Journal on Computing ,\n40(5):1402{1431, 2011.\n[14]Edith Cohen and O\fr Geri. Sampling sketches for concave sublinear functions of frequencies.\nInAdvances in Neural Information Processing Systems, NeurIPS 2019 , volume 32. Curran\nAssociates, Inc., 2019.\n[15]Edith Cohen and Haim Kaplan. Summarizing data using bottom-k sketches. In Proceedings of\nthe Twenty-Sixth Annual ACM Symposium on Principles of Distributed Computing, PODC\n2007, pages 225{234. ACM, 2007.\n[16]Edith Cohen and Haim Kaplan. Tighter estimation using bottom k sketches. volume 1, pages\n213{224, 2008.\n[17]Edith Cohen, Haim Kaplan, and Subhabrata Sen. Coordinated weighted sampling for estimating\naggregates over multiple weight assignments. Proceedings of the VLDB Endowment , 2(1):646{\n657, 2009.\n[18]Edith Cohen, Rasmus Pagh, and David Woodru\u000b. Wor and p's: Sketches for `p-sampling\nwithout replacement. In Advances in Neural Information Processing Systems, NeurIPS 2020 ,\nvolume 33, pages 21092{21104. Curran Associates, Inc., 2020.\n[19]Graham Cormode and S. Muthukrishnan. An improved data stream summary: the count-min\nsketch and its applications. Journal of Algorithms , 55(1):58{75, 2005.\n[20]Nick G. Du\u000eeld, Carsten Lund, and Mikkel Thorup. Priority sampling for estimating arbitrary\nsubset sums. Journal of the ACM , 54(6):32{es, 2007.\n[21]Talya Eden, Dana Ron, and C. Seshadhri. Sublinear time estimation of degree distribution\nmoments: The arboricity connection. SIAM Journal on Discrete Mathematics , 33(4):2267{2285,\n2019.\n[22]Cristian Estan and George Varghese. New directions in tra\u000ec measurement and accounting:\nFocusing on the elephants, ignoring the mice. ACM Transactions on Computer Systems ,\n21(3):270{313, 2003.\n27\n\n[23]Philippe Flajolet, \u0013Eric Fusy, Olivier Gandouet, and Fr\u0013 ed\u0013 eric Meunier. HyperLogLog: The\nanalysis of a near-optimal cardinality estimation algorithm. In Analysis of Algorithms (AofA) ,\npages 137{156. DMTCS, 2007.\n[24]Philippe Flajolet and G. Nigel Martin. Probabilistic counting algorithms for data base\napplications. Journal of Computer and System Sciences , 31(2):182{209, 1985.\n[25]Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for \fnding\nlow-rank approximations. Journal of the ACM , 51(6), 2004.\n[26]Phillip B. Gibbons and Yossi Matias. New sampling-based summary statistics for improving\napproximate query answers. In Proceedings ACM SIGMOD International Conference on\nManagement of Data, SIGMOD 1998 , pages 331{342. ACM, 1998.\n[27]D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement from a\n\fnite universe. Journal of the American Statistical Association , 47(260):663{685, 1952.\n[28]Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation\nalgorithms. In International Conference on Learning Representations, ICLR , 2019.\n[29]Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream\ncomputation. Journal of the ACM , 53(3):307{323, 2006.\n[30]Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In Advances\nin Neural Information Processing Systems, NeurIPS 2019 , volume 32. Curran Associates, Inc.,\n2019.\n[31]Piotr Indyk and David P. Woodru\u000b. Optimal approximations of the frequency moments of\ndata streams. In Proceedings of the 37th Annual ACM Symposium on Theory of Computing,\nSTOC 2005 , pages 202{208. ACM, 2005.\n[32]Rajesh Jayaram and David P. Woodru\u000b. Perfect lp sampling in a data stream. In 59th IEEE\nAnnual Symposium on Foundations of Computer Science, FOCS 2018 , pages 544{555. IEEE\nComputer Society, 2018.\n[33]Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodru\u000b. Learning-augmented\ndata stream algorithms. In International Conference on Learning Representations, ICLR , 2020.\n[34]Tim Kraska, Alex Beutel, Ed H. Chi, Je\u000brey Dean, and Neoklis Polyzotis. The case for learned\nindex structures. In Proceedings of the 2018 International Conference on Management of Data,\nSIGMOD Conference 2018 , pages 489{504. ACM, 2018.\n[35]Yi Li and David P. Woodru\u000b. A tight lower bound for high frequency moment estimation with\nsmall error. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and\nTechniques - 16th International Workshop, APPROX 2013, and 17th International Workshop,\nRANDOM 2013 , volume 8096 of Lecture Notes in Computer Science , pages 623{638. Springer,\n2013.\n[36]Zaoxing Liu, Ran Ben-Basat, Gil Einziger, Yaron Kassner, Vladimir Braverman, Roy Friedman,\nand Vyas Sekar. Nitrosketch: robust and general sketch-based monitoring in software switches.\n28\n\nInProceedings of the ACM Special Interest Group on Data Communication, SIGCOMM 2019 ,\npages 334{350, 2019.\n[37]Zaoxing Liu, Antonis Manousis, Gregory Vorsanger, Vyas Sekar, and Vladimir Braverman.\nOne sketch to rule them all: Rethinking network \row monitoring with univmon. In Proceedings\nof the ACM SIGCOMM 2016 Conference , pages 101{114. ACM, 2016.\n[38]Gabriel Maci\u0013 a-Fern\u0013 andez, Jos\u0013 e Camacho, Roberto Mag\u0013 an-Carri\u0013 on, Pedro Garc\u0013 \u0010a-Teodoro, and\nRoberto Ther\u0013 on. UGR'16: A new dataset for the evaluation of cyclostationarity-based network\nidss. Computers & Security , 73:411 { 424, 2018.\n[39]Gurmeet Singh Manku and Rajeev Motwani. Approximate frequency counts over data streams.\nInProceedings of the 28th International Conference on Very Large Data Bases , VLDB '02,\npage 346{357. VLDB Endowment, 2002.\n[40]Andrew McGregor, Sofya Vorotnikova, and Hoa T. Vu. Better algorithms for counting triangles\nin data streams. In Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on\nPrinciples of Database Systems, PODS 2016 , pages 401{411. ACM, 2016.\n[41]Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-E\u000ecient Learning of Deep Networks from Decentralized Data. In Proceedings\nof the 20th International Conference on Arti\fcial Intelligence and Statistics , volume 54 of\nProceedings of Machine Learning Research , pages 1273{1282. PMLR, 2017.\n[42]Ahmed Metwally, Divyakant Agrawal, and Amr El Abbadi. E\u000ecient computation of frequent\nand top-k elements in data streams. In Proceedings of the 10th International Conference on\nDatabase Theory, ICDT'05 , page 398{412. Springer-Verlag, 2005.\n[43]J. Misra and David Gries. Finding repeated elements. Science of Computer Programming ,\n2(2):143{152, 1982.\n[44]Morteza Monemizadeh and David P. Woodru\u000b. 1-pass relative-error lp-sampling with ap-\nplications. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete\nAlgorithms , SODA '10, page 1143{1160, USA, 2010. Society for Industrial and Applied Mathe-\nmatics.\n[45]Esbj orn Ohlsson. Sequential poisson sampling. Journal of O\u000ecial Statistics , 14(2):149{162,\n1998.\n[46]Ashwin Paranjape, Austin R. Benson, and Jure Leskovec. Motifs in temporal networks. In\nProceedings of the Tenth ACM International Conference on Web Search and Data Mining ,\nWSDM '17, pages 601|-610. Association for Computing Machinery, 2017.\n[47]Greg Pass, Abdur Chowdhury, and Cayley Torgeson. A picture of search. In Proceedings of\nthe 1st International Conference on Scalable Information Systems , InfoScale '06, pages 1{es.\nAssociation for Computing Machinery, 2006.\n[48]Bengt Ros\u0013 en. Asymptotic theory for successive sampling with varying probabilities without\nreplacement, i. The Annals of Mathematical Statistics , 43(2):373{397, 1972.\n29\n\n[49]Bengt Ros\u0013 en. Asymptotic theory for order sampling. Journal of Statistical Planning and\nInference , 62(2):135{158, 1997.\n[50]Mario Szegedy. The DLT priority sampling is essentially optimal. In Proceedings of the Thirty-\nEighth Annual ACM Symposium on Theory of Computing , STOC '06, page 150{158. ACM,\n2006.\nA Additional Experiments\nAdditional results on the performance of sampling by advice are provided in Figures 5 and 6.\nEstimates of the distributions of frequencies for all datasets are reported in Figure 7. The\nestimates are from PPSWOR ( `1without replacement) and `2(with replacement) samples of sizes\nk= 32 andk= 1024. For each key xin the sample, we estimate its rank in the dataset, that is, the\nnumber of keys ywith frequency wy\u0015wx. The rank estimate for a sampled key xis computed\nfrom the sample, by estimating the f-statistics for the threshold function f(w) :=Iw\u0015wx. The pairs\nof frequency and estimated rank (for each key in the sample) are then plotted. The \fgures also\nprovide the exact frequency distribution.\nAdditional results on the estimation of moments from PPSWOR ( `1sampling without replace-\nment) and `2samples (with and without replacement) are reported in Figure 8. As suggested\nby our analysis, the estimates on all datasets are surprisingly accurate even with respect to the\nbenchmark upper bound 1 =p\nk(which follows from the variance bound of weighted with-replacement\nsampling tailored to the moment we are estimating). The \fgures also show the advantage of\nwithout-replacement sampling on these skewed datasets.\n30\n\n2628210212214\nSample Size10−610−510−410−310−210−1100Normalized Root Mean Square Error\n3rd Moment Est. Error on AOL Day 50\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nk\n2628210212214\nSample Size10−610−510−410−310−210−1100Normalized Root Mean Square Error\n3rd Moment Est. Error on AOL Day 80\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nk\n2628210212214\nSample Size10−1510−1310−1110−910−710−510−310−1Normalized Root Mean Square Error\n7th Moment Est. Error on AOL Day 50\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nk\n2628210212214\nSample Size10−1410−1110−810−510−2101Normalized Root Mean Square Error\n7th Moment Est. Error on AOL Day 80\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nk\n2628210212214\nSample Size10−2110−1810−1510−1210−910−610−3100Normalized Root Mean Square Error\n10th Moment Est. Error on AOL Day 50\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nk\n2628210212214\nSample Size10−2110−1810−1510−1210−910−610−3100Normalized Root Mean Square Error\n10th Moment Est. Error on AOL Day 80\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nkFigure 5: NRMSE for estimating the 3rd, 7th, and 10th moments on the AOL dataset (days 50 and\n80 with learned advice from [28]).\n31\n\n2628210212214\nSample Size10−410−310−210−1100Normalized Root Mean Square Error\n3rd Moment Est. Error on SO 07-12/2013 (in)\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nk\n2628210212214\nSample Size10−510−410−310−210−1100Normalized Root Mean Square Error\n3rd Moment Est. Error on SO 07-12/2013 (out)\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nk\n2628210212214\nSample Size10−1010−810−610−410−2100Normalized Root Mean Square Error\n7th Moment Est. Error on SO 07-12/2013 (in)\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nk\n2628210212214\nSample Size10−1110−910−710−510−310−1101Normalized Root Mean Square Error\n7th Moment Est. Error on SO 07-12/2013 (out)\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nk\n2628210212214\nSample Size10−1510−1210−910−610−3100Normalized Root Mean Square Error\n10th Moment Est. Error on SO 07-12/2013 (in)\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nk\n2628210212214\nSample Size10−1510−1210−910−610−3100Normalized Root Mean Square Error\n10th Moment Est. Error on SO 07-12/2013 (out)\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\nAdvice: kp=ku\nAdvice: ku= 32\n1/√\nkFigure 6: NRMSE for estimating the 3rd, 7th, and 10th moments on the Stack Over\row dataset\n(incoming and outgoing edges), based on past frequencies.\n32\n\n100101102103104105106107\nIndex in Sorted Order10−710−610−510−410−310−2Weight (/lscript1)\nEstimated Distribution: AOL\nActual\n/lscript1,k= 32\n/lscript1,k= 1024\n/lscript2(with rep.), k= 32\n/lscript2(with rep.), k= 1024\n100101102103104105106\nIndex in Sorted Order10−710−610−510−410−3Weight (/lscript1)\nEstimated Distribution: CAIDA\nActual\n/lscript1,k= 32\n/lscript1,k= 1024\n/lscript2(with rep.), k= 32\n/lscript2(with rep.), k= 1024\n101103105107\nIndex in Sorted Order10−910−710−510−310−1Weight (/lscript1)\nEstimated Distribution: UGR\nActual\n/lscript1,k= 32\n/lscript1,k= 1024\n/lscript2(with rep.), k= 32\n/lscript2(with rep.), k= 1024\n100101102103104105106\nIndex in Sorted Order10−810−710−610−510−410−3Weight (/lscript1)\nEstimated Distribution: SO (in)\nActual\n/lscript1,k= 32\n/lscript1,k= 1024\n/lscript2(with rep.), k= 32\n/lscript2(with rep.), k= 1024\n100101102103104105106\nIndex in Sorted Order10−810−710−610−510−410−3Weight (/lscript1)\nEstimated Distribution: SO (out)\nActual\n/lscript1,k= 32\n/lscript1,k= 1024\n/lscript2(with rep.), k= 32\n/lscript2(with rep.), k= 1024Figure 7: Actual and estimated distribution of frequency by rank. The estimates use PPSWOR and\nwith-replacement `2sampling and sample sizes k= 32;1024.\n33\n\n252729211213\nSample Size10−610−510−410−310−210−1100Normalized Root Mean Square Error\n3rd Moment Est. Error on CAIDA\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−410−310−210−1100Normalized Root Mean Square Error\n3rd Moment Est. Error on SO (in)\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−510−410−310−210−1100Normalized Root Mean Square Error\n3rd Moment Est. Error on SO (out)\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−1310−1110−910−710−510−310−1Normalized Root Mean Square Error\n10th Moment Est. Error on CAIDA\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−1310−1110−910−710−510−310−1101Normalized Root Mean Square Error\n10th Moment Est. Error on SO (in)\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−1310−1110−910−710−510−310−1101Normalized Root Mean Square Error\n10th Moment Est. Error on SO (out)\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−810−610−410−2100Normalized Root Mean Square Error\n3rd Moment Est. Error on AOL\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−910−710−510−310−1Normalized Root Mean Square Error\n3rd Moment Est. Error on UGR\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−1510−1210−910−610−3100Normalized Root Mean Square Error\n10th Moment Est. Error on AOL\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nk\n252729211213\nSample Size10−1510−1210−910−610−3100Normalized Root Mean Square Error\n10th Moment Est. Error on UGR\n/lscript1\n/lscript2(without rep.)\n/lscript2(with rep.)\n1/√\nkFigure 8: Estimating the 3rd and 10th moments on various datasets using PPSWOR ( `1sampling\nwithout replacement) and `2samples (with and without replacement). The error is averaged over\n50 repetition.\n34",
  "textLength": 84092
}