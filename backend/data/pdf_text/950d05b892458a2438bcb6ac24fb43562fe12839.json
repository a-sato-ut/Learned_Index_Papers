{
  "paperId": "950d05b892458a2438bcb6ac24fb43562fe12839",
  "title": "Déjà Vu: an empirical evaluation of the memorization properties of ConvNets",
  "pdfPath": "950d05b892458a2438bcb6ac24fb43562fe12839.pdf",
  "text": "D´ej`a Vu: an empirical evaluation of the\nmemorization properties of ConvNets\nAlexandre Sablayrollesy,?, Matthijs Douzey, Cordelia Schmid?,\nand Herv ´e J´egouy\nyFacebook AI Research?Inria\nSeptember 19, 2018\nAbstract\nConvolutional neural networks memorize part of their training data,\nwhich is why strategies such as data augmentation and drop-out are em-\nployed to mitigate overﬁtting. This paper considers the related question of\n“membership inference”, where the goal is to determine if an image was\nused during training. We consider it under three complementary angles.\nWe show how to detect which dataset was used to train a model, and in\nparticular whether some validation images were used at train time. We\nthen analyze explicit memorization and extend classical random label ex-\nperiments to the problem of learning a model that predicts if an image\nbelongs to an arbitrary set. Finally, we propose a new approach to infer\nmembership when a few of the top layers are not available or have been\nﬁne-tuned, and show that lower layers still carry information about the\ntraining samples. To support our ﬁndings, we conduct large-scale experi-\nments on Imagenet and subsets of YFCC-100M with modern architectures\nsuch as VGG and Resnet.\n1 Introduction\nThe widespread adoption of convolutional neural networks (LeCun et al., 1990)\n(ConvNets) for most recognition tasks, was triggered by the advance of Krizhevsky\net al. (2012) in image classiﬁcation and subsequent deep architectures (Simonyan\n& Zisserman, 2014; He et al., 2016). Several works have analyzed these ar-\nchitectures from different perspectives. Zeiler & Fergus (2014) have proposed\nDeconvNet to vizualize ﬁlter activations. Lenc & Vedaldi (2015) analyze their\nequivariance. Mahendran & Vedaldi (2015) show how to invert them and syn-\nthetize images maximizing the response of different classes. Ulyanov et al.\n(2017) analyze the image priors implicitly deﬁned by ConvNets.\nAll these works increase our understanding of ConvNets, but the complex\nissue of overﬁtting and its relationship to optimization are still not fully un-\nderstood. Several strategies are routinely used to avoid overﬁtting, such as\n1arXiv:1809.06396v1  [cs.CV]  17 Sep 2018\n\n`2-regularization through weight decay (Krogh & Hertz, 1991), dropout (Sri-\nvastava et al., 2014), and importantly, data augmentation (Behpour et al., 2017;\nDwibedi et al., 2017; Paulin et al., 2014). Yet few works (Zhang et al., 2016;\nYeom et al., 2018) have analyzed the interplay of overﬁtting and memorization\nof training images in high-capacity classiﬁcation architectures. Speciﬁcally, we\nare not aware of such an analysis for a modern ConvNet such as ResNet-101\nlearned on Imagenet.\nIn this paper, we consider the privacy issue of membership inference, i.e.,\nwe aim at determining if a speciﬁc image or group of images was used to train\na model. This question is important to protect both the privacy and intellectual\nproperty associated with images. For ConvNets, the privacy issue was recently\nconsidered by Yeom et al. (2018) for the small MNIST and CIFAR datasets.\nThe authors evidence the close relationship between overﬁtting and privacy\nof training images. This is reminiscent of prior membership inference attacks,\nwhich employ the output of the classiﬁer associated with a particular example\nto determine whether it was used during training or not (Shokri et al., 2016).\nThis is related to Torralba & Efros (2011), who showed that a classiﬁer can\ndetermine with high accuracy if an image comes from a dataset or another\nby exploiting the bias inherent to datasets. We discuss this relationship and\nshow that we can detect whether a given network has been trained on some\nof the validation images. This has a concrete application for machine-learning\nbenchmarks: scores are often reported on a validation set with public labels,\nallowing a malicious or gawky competitor to artiﬁcially inﬂate the accuracy by\ntraining on validation images. Our test detects if it is the case, even if only part\nof the validation set is leaked to the training set.\nWe provide a qualitative upper bound on the capacity of popular convo-\nlutional networks to memorize a given number of images. More precisely, we\nconstruct a binary classiﬁer as a drop-in replacement of the last layer, whose\nresponse is the membership test. Our tests carried out on VGG-16 (Simonyan\n& Zisserman, 2014) and ResNet-101 (He et al., 2016) evidence different memo-\nrizing capabilities depending on the number of images and the amount of data\naugmentation (ﬂip, cropping).\nFinally, we propose a new setting for membership inference that only con-\nsiders intermediary layers of a network, thus extending membership inference\nto transferred and ﬁne-tuned networks, that have become ubiquitous. Our\nmembership inference does not require the last layer(s) of the original ConvNet\nto perform the test. This is important because, in many contexts, image recog-\nnition systems are built upon a trunk trained on a dataset and then ﬁne-tuned\nfor another task. Examples include Mask-RCNN (He et al., 2017) and models\nused for ﬁne-grained recognition (Hariharan & Girshick, 2017). In both cases\nthere are not enough training samples to train a full network: only the last lay-\ners of the networks are ﬁne-tuned. In summary, our paper makes the following\ncontributions:\n\u000fA simple statistical test to detect the “signature” of a dataset in a trained\nconvnet, and to detect if validation images where used to train the model\n(leakage).\n2\n\n\u000fAn empirical analysis of the explicit memorization capabilities of the ResNet\nand VGG architectures at a much larger scale than previously reported.\nWe evaluate the factors impacting the memorization capabilities such as\nthe number of images “stored” in the network and the equivariance hy-\npotheses in data augmentation.\n\u000fA membership inference test that detects if an image was used to train the\ntrunk of a network. To our knowledge, it is the ﬁrst work on membership\ninference that attacks intermediate layers.\nThe paper is organized as follows. Section 2 reviews related work. Sec-\ntion 3 evaluates the capacity of ConvNets to memorize a given set of images.\nSection 4 considers the problem of determining if a particular dataset, e.g., the\nvalidation set, was used during training. Section 5 focuses on detecting if a\nparticular image has been used for training without accessing the network’s\noutput layer.\n2 Related work & datasets\nOur work is related to the topics of overﬁtting and memorization capabilities of\nneural network architectures, which are able to perfectly discriminate random\noutputs in some cases (MacKay, 2002; Zhang et al., 2016). In the following, we\ndistinguish explicit from implicit memorization (also called “unintended mem-\norization” (Carlini et al., 2018) in natural language processing systems).\nExplicit memorization. Neural network are capable of memorizing any\nrandom pattern. This property was analyzed in MacKay (2002) for the single\nlayer case. In MacKay’s setup, the sender and receiver agree beforehand on a\nset of vectors (xi)n\ni=12Rd. To transmit an arbitrary sequence of binary labels\ny1;:::;yn, the sender learns a single-layer neural network that predicts the yi\nfromxi, and sends its weights to the receiver. The receiver labels the points\nx1;:::;xnwith the transmitted neural network to reconstruct the labels. The\nVC-dimension of this 1-layer model is d, so the model can ﬁt perfectly as long\nasn\u0014d. MacKay extends this bound by showing that the sender can, with\nhigh probability, ﬁnd a neural network ﬁtting the output if n\u00142d, and that it\nis almost impossible to ﬁt the model for n>2d. The estimated capacity of this\nneural network is thus 2bits per parameter.\nDetermining the practical memorization capacity of ConvNets is not triv-\nial. A few recent works (Zhang et al., 2016; Yeom et al., 2018) evaluate how a\nnetwork can ﬁt random labels. Zhang et al. (2016) replace true labels by ran-\ndom labels and show that popular ConvNets can perfectly ﬁt them in simple\ncases, such as small datasets (CIFAR10) or Imagenet without data augmenta-\ntion. Krueger et al. (2017) extend their analysis and argue in particular that\nthe effective capacity of ConvNets depends on the dataset considered. In a pri-\nvacy context, Yeom et al. (2018) exploit this memorizing property to watermark\nnetworks. As a side note, random labeling and data augmentation have been\n3\n\nused for the purpose of training a network without any annotated data (Doso-\nvitskiy et al., 2014; Bojanowski & Joulin, 2017). Our paper is also related to few\nworks (Kraska et al., 2017; Iscen et al., 2017) that learn indexes as an alternative\nto traditional structures such as Bloom Filters or B-trees. In particular, Kraska\net al. (2017) show that in some cases, neural nets outperform cache-optimized\nB-tree on real-world data. These works exploiting explicit memorization of\nneural networks are reminiscent of works (Hopﬁeld, 1982; Personnaz et al.,\n1986; Hinton et al., 1986; Plate, 1995) on associative memories and, more gen-\nerally, distributed representations.\nImplicit memorization and privacy risk in learning systems. Ateniese\net al. (2015) state: “ it is unsafe to release trained classiﬁers since valuable information\nabout the training set can be extracted from them ”. The problem that we address\nin this paper, i.e., to determine whether an image or dataset has been used for\ntraining, is related to the privacy implications of machine learning systems.\nThey were discussed in the context of support vector machines (Rubinstein\net al., 2009; Biggio et al., 2014). In the context of differential privacy (Dwork\net al., 2006), recent works (Wang et al., 2016; Bassily et al., 2016) suggest that\nguaranteeing privacy requires learning systems to generalize well, i.e., to not\noverﬁt. Note that there are systems providing differential privacy but that still\nleak information (Ateniese et al., 2015; Balu et al., 2014).\nMembership Inference in images. A few recent works (Abadi et al., 2016;\nHayes et al., 2017; Shokri et al., 2016; Long et al., 2018) have addressed “mem-\nbership inference” for images: determine whether an image has been used for\ntraining or not. Yeom et al. (2018) discuss how privacy, that can be broken\nby membership inference, is connected to overﬁtting. Long et al. (2018) ob-\nserve that some training images are more vulnerable than others and propose\na strategy to identify them. Hayes et al. (2017) analyze privacy issues arising\nin generative models. Most of these works were evaluated on small datasets\nlike CIFAR10, or larger datasets but without data augmentation. Our work\naims at being closer to realistic conditions. In the following, the analysis of a\npre-trained network will be called “attack” performed by an “attacker”.\nDataset bias and inference. Torralba & Efros (2011) evidence that a simple\nclassiﬁer can predict with high accuracy which dataset an image comes from.\nTommasi et al. (2017) show that this bias still exists with ConvNets. In the next\nsection of this paper, we re-visit this problem by proposing a dataset inference\nmethod derived from an elementary membership inference test.\nDatasets used in our study. We use will several public image collections\nthroughout our paper. Imnet1k refers to the subset of Imagenet (Deng et al.,\n2009; Russakovsky et al., 2015) used during the ILSVRC-12 challenge. It con-\nsists of 1000 balanced classes, split in a training set (1.2M images) and a valida-\ntion set (50k images). We use the regular split between train and val and denote\nthem by Imnet1k-train andImnet1k-val , respectively. Imnet22k refers to the\nfull Imagenet dataset. It is built in the same way as Imnet1k , but with 21783 un-\nbalanced classes. Yfcc100M (Thomee et al., 2016) contains 99.2M photos that\n4\n\nhave not been collected for image classiﬁcation and thus are not representing\nspeciﬁc classes or visual concepts. Tiny images (Torralba et al., 2008) consists\nof 79M low-resolution images. CIFAR10 is a subset of Tiny that has been la-\nbelled for image classiﬁcation. In our study, it is important that the dataset\ndoes not contain duplicate images or images that overlap between the train\nand the test set. We have sanitized the datasets to avoid this problem using\nGIST descriptors and similarity search, see Appendix B in the supplemental\nmaterial for details.\n3 Explicit memorization – network capacity\nIn this section, we explicitly train neural networks to memorize a given subset\nof images, so that it can decide whether an image is in its memory or not at\ntest time. We design a model f\u0012that distinguishes a set of inimages from out\nimages, where images unseen during training are out.\nWe repurpose the classiﬁcation layer of standard models to output a binary\nlabel, depending on whether the image must be remembered or not. Our archi-\ntecture plays an equivalent role to the discriminator in Generative Adversarial\nNetworks (GAN): it needs to discriminate between positive and negative im-\nages. In our case, negative images are a large pool of images instead of the\ngenerated images in GANs. Zhang et al. (2016) show that ConvNets are able to\noverﬁt almost any random labelling of their input data, but in their experiment,\nthe output for unseen images is undeﬁned.\n3.1 Information-theoretic capacity\nWe train our model to predict 1for a set of positive images P, and 0for all other\n(negative) images N. We assume that Nrepresent a large image distribution,\nson=jPj \u001c jNj , and we note N=jPj+jNj. There are two notions of\ncapacity relevant to our analysis: the capacity of a neural network, and the\ncapacity needed to store a number nof images among a larger set of size N.\nWe assume, following MacKay’s analysis, that the capacity of a neural network\nis well approximated by its number of parameters. Assuming images have\nconsecutive ids, it is sufﬁcient to store the subset of ids of the positive images,\nwhich requires a capacity:\nC(n) = log2\u0012N\nn\u0013\n\u0019nlog2\u0012N\nn\u0013\n+n\nlog(2); (1)\nwhere the approximation holds for n\u001cN. This number scales almost linearly\nin the number of positive examples n, and logarithmically in the number of\nnegatives.\n5\n\nTinyNet-1 TinyNet-2 TinyNet-3\n 50 60 70 80 90 100\n1k 10k 100k 1Min/out accuracy\nnumber of training imagesnone\nfip\nfip + crop 1\nfip + crop 2\ncapacity\n 50 60 70 80 90 100\n1k 10k 100k 1Min/out accuracy\nnumber of training images\n 50 60 70 80 90 100\n1k 10k 100k 1Min/out accuracy\nnumber of training images\nFigure 1: In/out classiﬁcation performance (train) on Tiny , for varying image\nsubsets and number of images. The colors indicate the type of data augmen-\ntation: purple=none, green=ﬂip, cyan=ﬂip+crop \u00061, orange=ﬂip+crop \u00062. The\nvertical line shows the number of positive images nsuch thatC(n)is the num-\nber of parameters of the network.\n3.2 Empirical analysis on tiny images\nTinyNet. We design a family of ConvNets with 4 convolutional layers and\n2 fully-connected layers that take 32x32 images as input and output a binary\nclassiﬁcation. There are 3 versions: TinyNet-1, (90k parameters), TinyNet-2\n(300k parameters) and TinyNet-3 (2M parameters). Most parameters of these\nmodels are in the ﬁrst fully connected layer, as in VGG (cf. Appendix C).\nExperimental setup. We use a subset of N= 15Mimages from Tiny for\nthese experiments. We randomly sample nimages as positive examples, and\ntreat the rest as negatives. At each epoch, we feed a random sample of nega-\ntives of the same size as the number of positives to the network. The reported\naccuracy is measured on a balanced set of positives and negatives. We consider\nfour types of data augmentation: “none”, “ﬂip” (random horizontal mirror-\ning), “ﬂip+crop\u00061” (a random translation in f\u00001;0;+1g2), “ﬂip+crop\u00062”.\nDiscussion. Figure 1 shows the accuracy of the model as a function of the\nnumber of positive images for all TinyNets. Instead of a sharp drop between\nthe over-capacity and the under-capacity regimes, we observe a smooth drop\nas the number of positives increases. Empirically, this transition phase happens\nwhen the number of samples reaches the theoretical capacity of the network.\nAs expected, data augmentation reduces the memorization capacity of the\nnetwork. For example, the accuracy of a network trained on nimages with\nﬂips is lower-bounded by the capacity of the same network trained on 2nim-\nages with no data augmentation. This lower bound is not tight, thanks to the\ngeneralization capability of the ConvNet, which captures the patterns common\nto an image and its symmetric. This generalization capability is obvious for\nstronger augmentations: for example with “ﬂip+crop \u00061” TinyNet-2 can iden-\ntify 10k images with 90% accuracy, vs. 20k images without data augmentation,\nwhile this requires to treat 18 augmented versions of each image similarly.\n6\n\nresnet-18 resnet-101 VGG-16n=10k\n 50 60 70 80 90 100\n 0  100  200  300  400  500in/out accuracy\ntraining epochs\n 50 60 70 80 90 100\n 0  100  200  300  400  500in/out accuracy\ntraining epochs\n 50 60 70 80 90 100\n 0  100  200  300  400  500in/out accuracy\ntraining epochs n=100k\n 50 60 70 80 90 100\n 0  100  200  300  400  500in/out accuracy\ntraining epochs\n 50 60 70 80 90 100\n 0  100  200  300  400  500in/out accuracy\ntraining epochs\n 50 60 70 80 90 100\n 0  100  200  300  400  500in/out accuracy\ntraining epochs\nFigure 2: Accuracy over iterations of the in/out training on Yfcc100M for dif-\nferent networks and amount of data augmentation (indicated by color: purple=\nnone, green = ﬂip, cyan = ﬂip+crop \u00065).\n3.3 Experiments with large-scale architectures\nIn this section, we extend the explicit memorization experiments to VGG-16,\nResNet-18, and ResNet-101 networks with images coming from Yfcc100M . The\ncapacity of these networks is much larger than in the tiny setting: Resnet-18\nhas 11.7M parameters and VGG-16 has 140M.\nWe set an initial learning rate of 10\u00002and divide it by 10when the accu-\nracy gets over 60%, and again at 90%. We run experiments using either the\ncenter crop, or two data augmentations (ﬂip, ﬂip+crop \u00065). Figure 2 shows\nconvergence plots for several settings. Note, the x-axis is in epochs, that are\n10\u0002slower forn=100k images than n=10k images. The longest experiment\ntook 4 days on 4 GPUs . VGG-16 and ResNet-101 converge at approximately\nthe same number of epochs, irrespective of n. Data augmentation increases the\nnumber of epochs required to converge, eg. for the ResNets, ﬂip up to twice\nmore epochs to be trained. VGG is a more shallow and higher capacity net-\nwork; in general it converges faster and it handles crops better than the ResNet\nvariants.\nThe outcome of our analysis is that explicit memorization of a large amount\nof images is possible, albeit more difﬁcult with data augmentation. In real use\ncases, the number of images that can be stored explicitly with perfect accuracy\nis practically much lower than the number of network parameters. This set of\nexperiments provides an approximate upper-bound for the problem of mem-\nbership inference: if a given model cannot perfectly remember a set of images\nwhen trained to do so, it will likely not be able to remember all the images of\nthe training set when trained for classiﬁcation.\n7\n\n 0 0.2 0.4 0.6 0.8 1\n 0  0.2  0.4  0.6  0.8  1CDF\nconfdenceImnet1k-train\nImnet1k-val\nImnet22k\nYfcc100M\n 50 60 70 80 90 100\n1 10 100 104105% samples classifed correctly\nsample size (m)resnet18\nresnet50\nresnet101Figure 3: Left: Cumulative distribution of the maximum classiﬁcation score\nfor a sample of 5000 images taken from 4 datasets. Imnet1k-train served as\nthe training set and therefore Imnet1k images (both train and val) have higher\nconﬁdence. Right: binary classiﬁcation accuracy (%) of a sample of mele-\nments from the training set Imnet1k-train w.r.t. three other datasets: Imnet1k-val\n(solid), Imnet22k (dashed) and Yfcc100M (lines). The architecture is indicated\nby the line color.\n4 Dataset detection and leakage\nIn this section, we detect whether a group of samples or a dataset has been\nused to train a model. This problem encompasses the particular case of dataset\nbias (Torralba & Efros, 2011) and is more difﬁcult, as we need to distinguish\ndatasets even if they share the same statistics, acquisition procedure and la-\nbelling process. For instance, we want to be able to determine if images from\nthe validation set of Imnet1k were used at train time.\nHypothesis and problem statement. We assume that there are two data\nsourcesS1;S2and each source Sjyields samples x(j)\n1;x(j)\n2;:::;x(j)\nm. The attacker\nis given access to a model f\u0012; in this paper, we assume f\u0012(x)is the maximal ac-\ntivation of the softmax layer, aka. the conﬁdence of the model. The cumulative\ndistribution of the conﬁdence for a model trained on Imnet1k-train is shown in\nﬁgure 3: most samples coming from the source Imnet1k-train have a very high\nconﬁdence, while the distribution of the source Imnet1k-val is more balanced\nand unrelated sources ( Yfcc100M ,Imnet22k ) tend to have a more uniform dis-\ntribution.\nWe consider two attack scenarii on the model f\u0012. In the ﬁrst scenario, we\nhave a set of msamples that come from either S1orS2and we want to de-\ntermine which source they come from. In the second scenario, we want to\ndetermine if the model has been trained with samples from a validation set,\nand thus look at whether the two source distributions corresponding to the\nvalidation and the test are different.\nWe compare conﬁdence distributions using the Kolmogorov-Smirnov (K-\nS) distance. Given two cumulative distributions FandG, the K-S distance is\ndKS(F;G) = supxjF(x)\u0000G(x)j:We use the K-S distance to determine if two\ndistributions are similar.\n8\n\n4.1 Conﬁdence as a signature of a dataset\nIn this section the samples x1;:::;xmcome from either source S1orS2. The\nattacker uses the following decision rule: compute the K-S distance between\nx1;:::;xmandS1(resp.S2), and assign the samples to the closest source.\nResults and observations. The results are reported in Figure 3. We can\ndistinguish Imnet1k-train from Yfcc100M with very few (10-20) samples. More\ninterestingly, the same number of samples allow us to separate Imnet22k from\nImnet1k-train , and with 500 images we can distinguish Imnet1k-train from Imnet1k-\nval. This shows that, even with a relatively low number of images, an attacker\ncan determine that a given image collection was used for training. The ﬁgure\nalso shows that the test is easier for networks with a higher capacity, that tend\nto overﬁt more.\n4.2 Detecting leakage\nWe now assume that we are given a model for which we suspect that part of\nthe validation set was used for training ( leakage ). For a number of datasets ( e.g.,\nImagenet, Pascal VOC), the labels of the validation set are publicly available,\nand models are often compared using validation accuracy. A malicious person\ncould train a model using the training set and part of the validation set, and\nthen report validation accuracy to artiﬁcially inﬂate the performance of the\nmodel.\nThe attack we propose is a two-sample K-S test to determine if leakage has\noccurred or not. We assume that no sample from the test set has leaked (labels\nare not public in most cases). The null hypothesis of our test is that the vali-\ndation and test sets have the same distribution. We compute the K-S distance\nbetween the validation and test sets, and reject the null hypothesis if this dis-\ntance is high. The distance threshold tis set such that the null hypothesis is\nincorrectly rejected with a low probability \u000b, corresponding to the p-value. For\nlarge samples, Smirnov’s estimate of the threshold corresponding to a p-value\nof\u000bis (Feller, 1949):\nt=c(\u000b)r\nn+m\nnmwherec(\u000b) =r\n\u00001\n2log\u0010\u000b\n2\u0011\n: (2)\nWe ran experiments on Imagenet using Resnet-18 and VGG-16, with s2\nf1;2;5;10;20gimages per class of the validation set in addition to the training\nset to ﬁt the model. Table 1 reports the p-value of the different tests. We can see\nthat when 10images per class are leaked, the K-S test predicts that leakage has\nhappened with a very high signiﬁcance. When 5images per class or less are\nused, we cannot reject the null hypothesis and thus cannot claim that leakage\nhas happened.\n9\n\nTable 1: Kolmogorov-Smirnov tests on Imnet1k validation and test sets for var-\nious levels of leakage. The K-S test provides a level of signiﬁcance ( p-value)\nrather than a yes/no answer. Lower values indicate high conﬁdence that the\nvalidation and test sets distributions are different, hinting that leakage has oc-\ncurred. If only 1image per class of the validation set has leaked, we cannot\nconclude from this test that there has been leakage. Conversely, when 10im-\nages or more have leaked, we can conclude with high signiﬁcance that leakage\nhas occurred.\nNb. of Images\n# per class leaked Resnet-18 VGG-16\n1 0:888 0 :494\n2 0:228 0 :107\n5 0:068 0 :014\n10 <10\u00004<10\u00004\n20 <10\u00004<10\u00004\n5 Implicit memorization & membership inference\nThis section tackles the more difﬁcult problem of membership inference in\ntrained models. From a trained model and an image the attacker has to de-\ntermine whether the image was used to train the model. In our new setting,\nupper layers are not available (due to e.g. ﬁnetuning on a downstream task).\nWe provide baselines for VGG16 and Resnet and extend the traditional attacks\nto our setup.\nThe literature (Abadi et al., 2017) distinguishes two cases types of member-\nship inference: (1) all layers are available ( all-layers ), (2) only the ﬁnal output\nof the network is available ( ﬁnal-output ). There is currently no attack that per-\nforms substantially better in all-layers than in ﬁnal-output . This seems counter-\nintuitive but we conﬁrmed it in preliminary experiments. Our new setup,\npartial-layers , is adapted to transfer learning: only a certain number of bottom\nlayers are available for attack, the remaining layers were destroyed by retrain-\ning on an unrelated task. This task is more difﬁcult than all-layers since it has\nless parameters available, and thus more difﬁcult than ﬁnal-output .\n5.1 Evaluation protocol and baselines\nWe assume that there are three disjoint sources of data: a public set, a private set,\nand an evaluation set. A model is trained on the private set. The attacker has\naccess to the lower layers of this model and to the public set. After the attack\nis carried out, the evaluation is ran on images from the private and evaluation\nsets.\nWe divide Imnet1k equally into two splits (each with half of the images per\nclass). We hold out 50images per class in the ﬁrst split to form the evaluation\n10\n\nset, and form the private set with the rest of this split. The second split is used\nas the public dataset. We conduct the membership inference test by comparing\nthe prediction of the attack model on the private set and on the evaluation set.\nFor this purpose, we consider the two baseline methods.\nBayes rule. A simplistic membership inference attack is to predict that an\nimage comes from the training set if its class is predicted correctly, and from a\nheld-out set otherwise. We note ptrain(resp.ptest) the classiﬁcation accuracy on\nthe training (resp. held-out) set, and assume a balanced prior on membership.\nAccording to Bayes’ rule, the accuracy of the heuristic is (see Appendix A in\nthe supplementary material for the derivation):\npbayes = 1=2 + (ptrain\u0000ptest)=2: (3)\nSinceptrain\u0015ptestthis heuristic is better than random guessing (accuracy 1=2)\nand the improvement is proportional to the overﬁtting gap ptrain\u0000ptest.\nMaximum Accuracy Threshold (MAT). Yeom et al. (2018) propose an attack\non the loss value: a sample is deemed part of the training set if its loss is below\na threshold \u001c. IfFtrain(resp.Fheldout ) is the cdf of the loss on the train (resp.\nheld out), the accuracy of the MAT is:\npthreshold = max\n\u001c1=2 + 1=2 (Ftrain(\u001c)\u0000Fheldout (\u001c)) (4)\nAsFtrain(\u001c)\u0015Fheldout (\u001c), this heuristic is also better than random guessing.\nIn practice, \u001cis estimated with samples or simulated by training models with\nknown train/heldout split.\n5.2 Membership inference with a truncated network\nIn this section, we provide a simple method to attack networks in the partial-\nlayers setting. We use the available public data to retrain the missing layers,\nand apply either the Bayes or the MAT attack, as if there was no ﬁne-tuning\nat all. We found this method to be more accurate than another variant that we\ndesigned with shadow models (Shokri et al., 2016), as detailed in the supple-\nmental material (Appendix E).\n5.3 Experiments on large convnets\nClassiﬁcation models. We experiment with the popular VGG-16 (Simonyan\n& Zisserman, 2014) and Resnet-101 (He et al., 2016) architectures. The private\nmodel is learned in 90epochs, with an initial learning rate of 0:01, divided by 10\nevery 30epochs. Parameter optimization is conducted with stochastic gradient\ndescent with a momentum of 0:9, a weight decay of 10\u00004, and a batch size of\n256. To assess the effect of data augmentation, we train different networks with\nvarying data augmentation: ﬂip+crop \u00065, ﬂip+crop, ﬂip+crop+resize, or none.\n11\n\nTable 2: Accuracy of membership inference attacks on the softmax layer of the\nmodels ( ﬁnal-output ). Data augmentation reduces the gap between the training\naccuracy and the held-out accuracy, thus decreasing the accuracy of the Bayes\nattack and the MAT attack.\nModel Augmentation Bayes baseline MAT\nResnet101 None 76.3 90.4\nFlip, Crop\u00065 69.5 77.4\nFlip, Crop 65.4 68.0\nVGG16 None 77.4 90.8\nFlip, Crop\u00065 71.3 79.5\nFlip, Crop 63.8 64.3\nTable 3: Accuracy of membership inference attacks on intermediate layers of\nResnet-101 and VGG-16 models ( partial-layers ).\nLast block corresponds to the softmax, and respectively the fully connected lay-\ners (for VGG-16) and the 4-th stage of Resnet (for Resnet-101).\nAugmentation Truncate Resnet-101 VGG-16\nNone Softmax 73.4 74.8\nLast block 53.1 51.7\nFlip, Crop\u00065 Softmax 65.7 67.3\nLast block 53.1 52.2\nFlip, Crop Softmax 60.8 58.5\nLast block 52.9 53.2\nAttack models. We evaluate both the bayes and MAT methods to estimate\nthe performance on ﬁnal-output . The results are shown in Table 2. As we can\nsee, it is possible to guess with a very high accuracy if a given image was used\nto train a model when there is no data augmentation. Stronger data augmen-\ntation reduces the accuracy of the attacks, that still remain above 64%.\nThe results of our attack in the more challenging partial-layers setting are\nshown in Table 3. We can see that even without the last layers, it is possible to\ninfer training set membership of an image. The attack performance depends\non two factors: the layer at which the attack is conducted, and the data aug-\nmentation used to train the original network. As expected, it is more difﬁcult\nto attack a network that has been trained with more data augmentation, or that\nhas only lower layers available. More importantly, these experiments show\nthat intermediary layers still carry out information about the images used for\ntraining the model.\n12\n\n6 Conclusion\nWe have investigated the memorization capabilities of neural networks from\ndifferent perspectives. Our experiments show that state-of-the-art networks\ncan remember a large number of images and distinguish them from unseen\nimages. We have analyzed networks speciﬁcally trained to remember a set of\nimages and the factors inﬂuencing their memorizing and convergence capabil-\nities. It is possible to determine whether an image set was used at training time,\neven with full data augmentation. On the contrary, the accuracy of determin-\ning if a single image was used is low when considering full data augmentation\non a large training set such as Imagenet. This implies that data augmentation\nis an effective privacy-preserving method. Our last contribution is a method\nthat detects training images better than chance even with no access to the last\nlayers, under limited data augmentation.\nFinal remark: The curious reader may have noticed that our title echoes\nthe one of a previous user study (Dhamija et al., 2000), in which the authors\ndiscussed the feasibility of authenticating humans by their capabilities to rec-\nognize a set of images.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,\nKunal Talwar, and Li Zhang. Deep learning with differential privacy. In CCS ,\n2016.\nMart ´ın Abadi, Ulfar Erlingsson, Ian Goodfellow, H Brendan McMahan, Ilya\nMironov, Nicolas Papernot, Kunal Talwar, and Li Zhang. On the protection\nof private information in machine learning systems: Two recent approches.\nInCSF, 2017.\nGiuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Villani,\nDomenico Vitali, and Giovanni Felici. Hacking smart machines with smarter\nones: How to extract meaningful data from machine learning classiﬁers.\nIJSN , 2015.\nRaghavendran Balu, Teddy Furon, and S ´ebastien Gambs. Challenging differ-\nential privacy: the case of non-interactive mechanisms. In ESORICS , 2014.\nRaef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and\nJonathan Ullman. Algorithmic stability for adaptive data analysis. In STOC ,\n2016.\nS. Behpour, K. Kitani, and B. Ziebart. ADA: A game-theoretic perspective on\ndata augmentation for object detection. In arXiv , 2017.\n13\n\nBattista Biggio, Igino Corona, Blaine Nelson, Benjamin I. P . Rubinstein, Davide\nMaiorca, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli. Security Evalua-\ntion of Support Vector Machines in Adversarial Environments . 2014.\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting\nnoise. In ICML , 2017.\nNicholas Carlini, Chang Liu, Jernej Kos, ´Ulfar Erlingsson, and Dawn Song.\nThe secret sharer: Measuring unintended neural network memorization &\nextracting secrets. arXiv preprint arXiv:1802.08232 , 2018.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet:\nA large-scale hierarchical image database. In Computer Vision and Pattern\nRecognition, 2009. CVPR 2009. IEEE Conference on , pp. 248–255, 2009.\nRachna Dhamija, Adrian Perrig, et al. D ´ej`a vu-a user study: Using images for\nauthentication. In USENIX Security Symposium , 2000.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas\nBrox. Discriminative unsupervised feature learning with convolutional neu-\nral networks. In NIPS , 2014.\nMatthijs Douze, Herv ´e J´egou, Harsimrat Sandhawalia, Laurent Amsaleg, and\nCordelia Schmid. Evaluation of gist descriptors for web-scale image search.\nInCIVR , 2009.\nDebidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut, paste and learn:\nSurprisingly easy synthesis for instance detection. In ICCV , 2017.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating\nnoise to sensitivity in private data analysis. In TCC , 2006.\nW. Feller. On the kolmogorov-smirnov limit theorems for empirical distribu-\ntions. Annals of Mathematical Statistics , 1949.\nBharath Hariharan and Ross Girshick. Low-shot visual recognition by shrink-\ning and hallucinating features. In CVPR , 2017.\nJamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. Logan:\nevaluating privacy leakage of generative models using generative adversar-\nial networks. arXiv preprint arXiv:1705.07663 , 2017.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learn-\ning for image recognition. In CVPR , 2016.\nKaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Girshick. Mask R-CNN.\nInICCV , 2017.\nGeoffrey E Hinton, James L McClelland, David E Rumelhart, et al. Distributed\nrepresentations. Parallel distributed processing: Explorations in the microstruc-\nture of cognition , 1986.\n14\n\nJohn J Hopﬁeld. Neural networks and physical systems with emergent collec-\ntive computational abilities. PNAS , 1982.\nAhmet Iscen, Teddy Furon, Vincent Gripon, Michael Rabbat, and Herv ´e J´egou.\nMemory vectors for similarity search in high-dimensional spaces. IEEE\nTrans. Big Data , 2017.\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. Billion-scale similarity search\nwith gpus. arXiv preprint arXiv:1702.08734 , 2017.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The\ncase for learned index structures. arXiv preprint arXiv:1712.01208 , 2017.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁca-\ntion with deep convolutional neural networks. In Advances in neural informa-\ntion processing systems , pp. 1097–1105, 2012.\nA. Krogh and J. A. Hertz. A simple weight decay can improve generalization.\nInNIPS , 1991.\nDavid Krueger, Nicolas Ballas, Stanislaw Jastrzebski, Devansh Arpit, Maxin-\nder S. Kanwal, Tegan Maharaj, Emmanuel Bengio, Asja Fischer, Aaron\nCourville, Simon Lacoste-Julien, and Yoshua Bengio. A closer look at mem-\norization in deep networks. In ICML , 2017.\nYann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E\nHoward, Wayne E Hubbard, and Lawrence D Jackel. Handwritten digit\nrecognition with a back-propagation network. In NIPS , 1990.\nKarel Lenc and Andrea Vedaldi. Understanding image representations by mea-\nsuring their equivariance and equivalence. In CVPR , 2015.\nYunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang,\nHaixu Tang, Carl A Gunter, and Kai Chen. Understanding mem-\nbership inferences on well-generalized learning models. arXiv preprint\narXiv:1802.04889 , 2018.\nDavid J. C. MacKay. Information Theory, Inference & Learning Algorithms . 2002.\nAravindh Mahendran and Andrea Vedaldi. Understanding deep image repre-\nsentations by inverting them. In CVPR , 2015.\nAude Oliva and Antonio Torralba. Building the gist of a scene: The role of\nglobal image features in recognition. Progress in brain research , 2006.\nMattis Paulin, J ´erˆome Revaud, Zaid Harchaoui, Florent Perronnin, and\nCordelia Schmid. Transformation pursuit for image classiﬁcation. In CVPR ,\n2014.\n15\n\nMattis Paulin, Julien Mairal, Matthijs Douze, Zaid Harchaoui, Florent Per-\nronnin, and Cordelia Schmid. Convolutional patch representations for image\nretrieval: an unsupervised approach. IJCV , 2017.\nL Personnaz, I Guyon, and G Dreyfus. Collective computational properties of\nneural networks: New learning mechanisms. Physical Review A , 1986.\nTony A Plate. Holographic reduced representations. IEEE Transactions on Neu-\nral networks , 1995.\nBenjamin IP Rubinstein, Peter L Bartlett, Ling Huang, and Nina Taft. Learning\nin a large function space: Privacy-preserving mechanisms for SVM learning.\narXiv:0911.5708 , 2009.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,\nAlexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition\nChallenge. IJCV , 2015.\nReza Shokri, Marco Stronati, and Vitaly Shmatikov. Membership inference at-\ntacks against machine learning models. CoRR , abs/1610.05820, 2016.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks\nfor large-scale image recognition. In ICLR , 2014.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Rus-\nlan Salakhutdinov. Dropout: A simple way to prevent neural networks from\noverﬁtting. 2014.\nBart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni,\nDouglas Poland, Damian Borth, and Li-Jia Li. YFCC100M: The new data in\nmultimedia research. Communications of the ACM , 2016.\nTatiana Tommasi, Novi Patricia, Barbara Caputo, and Tinne Tuytelaars. A\ndeeper look at dataset bias. In Domain Adaptation in Computer Vision Ap-\nplications . 2017.\nAntonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR ,\n2011.\nAntonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images:\nA large data set for nonparametric object and scene recognition. IEEE Trans.\nP AMI , 2008.\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior.\narXiv:1711.10925 , 2017.\nYu-Xiang Wang, Jing Lei, and Stephen E Fienberg. On-average KL-privacy and\nits equivalence to generalization for max-entropy mechanisms. In Interna-\ntional Conference on Privacy in Statistical Databases . Springer, 2016.\n16\n\nSamuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk\nin machine learning: Analyzing the connection to overﬁtting. arXiv preprint\narXiv:1709.1604 , 2018.\nMatthew D Zeiler and Rob Fergus. Visualizing and understanding convolu-\ntional networks. In ECCV . Springer, 2014.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol\nVinyals. Understanding deep learning requires rethinking generalization.\narXiv preprint arXiv:1611.03530 , 2016.\n17\n\nAppendix A Probabilistic derivations\nA.1 Bayes attack\nLetCdenote the event that the prediction of the neural network is correct and S\nthe random variable that indicates whether the sample comes from the training\nset. We therefore have:\nP(C= 1jS= 1) =ptrain;P(C= 1jS= 0) =ptest (5)\nP(S= 1) = P(S= 0) = 1=2: (6)\nThe accuracy of Bayes attack is:\nP(C=S) =P(C= 1jS= 1)P(S= 1) (7)\n+P(C= 0jS= 0)P(S= 0) (8)\n=1\n2(ptrain+ 1\u0000ptest): (9)\nA.2 Equivalence between Kolmogorov-Smirnov and threshold\nattacks\nIf we consider the particular case of a subset of m= 1 image, we show in this\nsection that the decision boundary induced by the K-S distance is the same\nas the MAT described in Section 5.1. Yet there are two signiﬁcant differences\nbetween the K-S attack and the MAT: we consider conﬁdence instead of the loss\nvalue, and the optimal threshold is computed differently. Our attacks with\nthe K-S distance can therefore be seen as a generalization of the membership\ninference proposed by Yeom et al. (2018).\nWe assume that we have two cumulative distributions FandGsuch that\n8x;F(x)\u0015G(x). We want to show that the K-S rule is equivalent to a threshold\nrule. Denoting by \u000exthe Dirac distribution centered on x, we have:\ndKS(\u000ex;F)\u0014dKS(\u000ex;G) (10)\n()1\n2\u0000jF(x)\u00001\n2j\u00141\n2\u0000jG(x)\u00001\n2j (11)\n()jG(x)\u00001\n2j\u0014jF(x)\u00001\n2j: (12)\nThe two following cases are easy:\nG(x)\u0014F(x)\u00141=2)dKS(\u000ex;F)\u0014dKS(\u000ex;G); (13)\nF(x)\u0015G(x)\u00151=2)dKS(\u000ex;F)\u0015dKS(\u000ex;G): (14)\nFor the last case, the set Ifor whichG(x)\u00141=2\u0014F(x)is an interval. On\nthis interval,jF(x)\u00001=2j\u0000jG(x)\u00001=2j=F(x)+G(x)\u00001.F+Gis increasing,\nand thus there exists a threshold \u001csuch that for x2I:\nx\u0014\u001c()dKS(\u000ex;F)\u0014dKS(\u000ex;G): (15)\n18\n\n���������������������������������������\n�� ���� ���� ���� ���� ���������\n��������������������������������������Figure 4: Histogram of distances of the images of Imnet22k to their nearest\nneighbor.\nWith Equations 13 and 14, Equation 15 extends to all x.\nAppendix B De-duplicating the datasets\nIn this section, we describe the de-duplication processing applied to the datasets\nused in explicit memorization experiments. This process ensures that near-\nduplicate images do not get assigned different labels, and thus makes learning\nand evaluation more reliable.\nB.1 Description and matching of duplicates\nWe compare images using GIST (Oliva & Torralba, 2006), a simple hand-crafted\ndescriptor that was shown to perform well on moderate image transforma-\ntions (Douze et al., 2009). We compute the approximate k-nearest neighbor\ngraph on each dataset using Faiss (Johnson et al., 2017). Figure 4 shows the\nhistogram of distances for the images of Imnet22k to their nearest neighbor: the\nbin around [0;10\u00002]contains more images than the following bin [10\u00002;2:10\u00002],\nwhich is due to duplicates in the dataset.\nImages that are bit-wise exact are unambiguous duplicates – in fact they\nare often already removed beforehand from the datasets because they are easy\nto detect by computing a hash value on the content. Beyond this extreme\ncase, the notion of “duplicate” is ambiguous: images that are re-encoded, re-\nsized, slightly cropped should be considered duplicates, but the case of larger\ntransformations is less obvious (e.g., photos of the same painting, consecutive\nframes of a video).\nB.2 Identiﬁcation of connected components\nWe set a conservative arbitrary threshold of 0:001to detect duplicate images,\nand remove the edges of the k-nn graph that are above this threshold. We\n19\n\nTinyNet 1\n TinyNet 2\n TinyNet 3\nFigure 5: Tiny nets.\nn11610437 bishop pine, bishop’s pine, Pinus muricata\nn11619455 western larch, western tamarack, Oregon larch,\nLarix occidentalis\nn11621281 amabilis ﬁr, white ﬁr, Paciﬁc silver ﬁr, red silver ﬁr,\nChristmas tree, Abies amabilis\nn11626826 red spruce, eastern spruce, yellow spruce, Picea\nrubens\nn11710827 cucumber tree, Magnolia acuminata\nn11721642 lesser spearwort, Ranunculus ﬂammula\nn11722342 western buttercup, Ranunculus occidentalis\nn11722621 cursed crowfoot, celery-leaved buttercup, Ranuncu-\nlus sceleratus\nn11753562 buffalo clover, Trifolium reﬂexum, Trifolium\nstoloniferum\nn11840476 desert four o’clock, Colorado four o’clock, mar-\navilla, Mirabilis multiﬂora\nn11874081 yellow rocket, rockcress, rocket cress, Barbarea vul-\ngaris, Sisymbrium barbarea\nn11882426 crinkleroot, crinkle-root, crinkle root, pepper root,\ntoothwort, Cardamine diphylla, Dentaria diphylla\nn11887750 western wall ﬂower, Erysimum asperum, Cheiran-\nthus asperus, Erysimum arkansanum\nn11889205 tansy-leaved rocket, Hugueninia tanacetifolia,\nSisymbrium tanacetifolia\n... ...\nFigure 6: Image that appears in the largest number of duplicate versions in\nImnet22k (72), with a few of the corresponding synsets.\ncompute the connected components, and keep a single image per connected\ncomponent.\nForImnet22k , the largest connected components are error images returned\nby image banks like Flickr for missing entries. This is an artifact of how the\ndataset was downloaded. The largest non-trivial cluster from Imnet22k is the\nimage of a ﬂower in Figure 6, that appears in 72 different synsets. There seems\nto be some disagreement on the species of this ﬂower, along with plain bad\nannotations.\nB.3 Statistics\nTable 4 shows some statistics on the duplicates identiﬁed by our simple ap-\nproach. Imnet22k has 10.4 % duplicate images. In addition to these dupli-\ncates, we removed 930,757 images that overlap with Imnet1k , which means\nthat Imnet1k is not a subset of Imnet22k in this paper. Within Imnet1k , we found\n1 % duplicates, which seems small enough not to remove them. For Tiny , we\nfound 9.5 % duplicates and removed them, leaving the dataset with 71;726;550\nunique images.\n20\n\nTable 4: Duplicate statistics for the datasets we use.\nDataset # images # groups\nImnet22k 14,197,087 12,720,164\nImnet1k-train 1,281,167 1,267,936\nTiny 79,302,017 71,726,550\nAppendix C TinyNet architectures\nThis appendix describes the convolutional architecture employed in Section\n3.2 of the main paper to evaluate the capacity saturation and the inﬂuence of\ntraining parameters and choices on this capacity.\nThe architectures includes from 3 convolutional layers for TinyNet1 to 4\nfor TinyNet2 and TinyNet3. The ﬁrst convolutional layer is 5x5. Each con-\nvolutional layer is followed by a Rectiﬁer Linear Unit activation. The fully\nconnected layer of TinyNet3 is larger than TinyNet2.\nAppendix D Filters\nn=100k, no augmentation\n n=100k, ﬂip\n n=100k, ﬂip+crop \u00062\nFigure 7: Filters of the ﬁrst convolutional layer (7x7, 64 ﬁlters) obtained when\nlearning to explicitly memorize if an image was used for training or not.\nThe ﬁlters of the ﬁrst convolutional layer are easy to visualize and contain\ninteresting information on how the SGD optimized to the very ﬁrst ﬁlter that\nis applied on the image pixels (Krizhevsky et al., 2012; Bojanowski & Joulin,\n2017; Paulin et al., 2017). Figure 7 shows the ﬁlters obtained after training a\nResnet-18. The ﬁlters for 10k images are very noisy compared to the smooth\nGabor ﬁlters produced by supervised classiﬁers. This is probably due to the\nlarge capacity of the network, that is able to quickly overﬁt the data and does\nnot need to update the ﬁlter weights beyond their random initialization. With\nmore images, the ﬁlters become more uniform, exhibiting some specialization.\nInterestingly, for n=100k with crop augmentation the ﬁlters have a clear uni-\n21\n\nTable 5: Accuracy of membership inference attacks before the softmax layer of\nthe models ( partial-layers ), using shadow models.\nModel Augmentation Attack accuracy\nResnet101 None 60.6\nFlip, Crop\u00065 61.4\nFlip, Crop 58.2\nVGG16 None 73.8\nFlip, Crop\u00065 65.8\nFlip, Crop 55.2\nform color. This is required for the output to be less sensitive to translations of\nup to 2 pixels.\nAppendix E Shadow models\nWe evaluated the performance of shadow models on the partial-layers setting.\nThe setting is the following: we train 20networks on the public dataset, each\ntime holding out a different subset of images. For each network, we can thus\ncompare the activations of train and held-out images. These activations are not\ndirectly comparable between two different networks, because internal activa-\ntions of a ReLU network have invariances (such as permutation of the neurons\nor positive rescaling). To circumvent this issue, we learn a regression model\nthat maps activations between two networks, and thus align activations of\nall the networks to the activations of the network under attack using the `2\nloss. We then learn an attack model that predicts from the aligned activations\nwhether the image was seen by the network at train time.\nThe results are shown in Table 5. While performing better than random\nguessing, shadow models underperform the attack methods shown in Table 3.\nWe believe that this is due to the complex processing involved in training\nshadow models on intermediate activations (notably the regression model),\nwhereas the attacks of Section 5 are more straightforward to train.\n22",
  "textLength": 49297
}