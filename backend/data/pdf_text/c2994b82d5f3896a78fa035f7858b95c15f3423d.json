{
  "paperId": "c2994b82d5f3896a78fa035f7858b95c15f3423d",
  "title": "Learning-Augmented Robust Algorithmic Recourse",
  "pdfPath": "c2994b82d5f3896a78fa035f7858b95c15f3423d.pdf",
  "text": "Learning-Augmented Robust Algorithmic Recourse\nKshitij Kayastha, Vasilis Gkatzelis, Shahin Jabbari\nDrexel University\nAbstract\nAlgorithmic recourse provides individuals who receive undesirable outcomes from machine learning\nsystems with minimum-cost improvements to achieve a desirable outcome. However, machine learning\nmodels often get updated, so the recourse may not lead to the desired outcome. The robust recourse\nframework chooses recourses that are less sensitive to adversarial model changes, but this comes at a higher\ncost. To address this, we initiate the study of learning-augmented algorithmic recourse and evaluate the\nextent to which a designer equipped with a prediction of the future model can reduce the cost of recourse\nwhen the prediction is accurate (consistency) while also limiting the cost even when the prediction is\ninaccurate (robustness). We propose a novel algorithm, study the robustness-consistency trade-off, and\nanalyze how prediction accuracy affects performance.\n1 Introduction\nMachine learning models are deployed even in sensitive domains such as lending or hiring, e.g., financial\ninstitutions use these models to determine whether someone should receive a loan. Given the major impact\nof these decisions on people’s lives, a plethora of work in responsible machine learning aims to make these\nmodels fair [ 5,11,32,83], transparent [ 44,65], and explainable [ 49,64,68]. A notable line of work along this\ndirection, calledalgorithmic recourse[ 71,76], provides each individual who received an undesirable label\n(e.g., one whose loan request was denied) with a minimum-cost improvement to achieve the desired label.\nAn important weakness of much of the work on algorithmic recourse is the assumption that models\nare fixed and do not change. In practice, models are periodically updated to reflect changes in data or\nenvironment, causing a recourse to become invalid, i.e., following it may not yield a desirable outcome [ 20].\nTo alleviate this problem, Upadhyay et al. [70]proposed a framework that isrobustto adversarial model\nchanges and provided an algorithm, ROAR, to compute robust recourses. However, guaranteeing robustness\nin such a setting leads to significantly increased cost [59].\nOur main goal is to design more practical recourse algorithms that balance robustness and cost. To achieve\nthis goal, we move beyond the adversarial setting and leverage the learning-augmented framework [ 53], which\nhas been used in a surge of recent work to overcome the limitations of adversarial (i.e., worst-case) analysis.\nSpecifically, rather than assuming that the designer has no information regarding the model changes, we\nassume they can at least formulate some predictions regarding these changes. However, these predictions\nare unreliable, so our goal is to compute recourses that are near-optimal if the predictions are accurate\n(consistency), while maintaining good performance even in the worst-case, i.e., even when the predictions are\nhighly inaccurate (robustness).\nOur ResultsWe first adapt the learning-augmented framework and the objectives of consistency and\nrobustness to algorithmic recourse, and we provide a computationally efficient algorithm for optimizing these\ntwo objectives, as well as their convex combinations. We then experimentally evaluate the performance of\nthis algorithm on a variety of both linear and non-linear models. We also formally prove that our algorithm is\noptimal for both robustness and consistency for any generalized linear model. Note that this is a non-convex\nproblem and, to the best of our knowledge, this is the first optimal algorithm for any robust recourse problem\n1arXiv:2410.01580v2  [cs.LG]  6 Oct 2025\n\n(prior work used algorithms which, as we show, return local rather than global optima). Therefore, this is a\ncontribution to the broader robust recourse literature, beyond the learning-augmented setting.\nIn our experiments, we use our algorithm to evaluate the achievable trade-offs between consistency and\nrobustness across different datasets, models, and predictions. In all of these settings, our algorithm returns\nmultiple recourses that Pareto dominate ROAR’s recourses, i.e., they simultaneously achieve better robustness\nand consistency. Moving beyond consistency and robustness (which correspond to the extreme cases of perfect\npredictions and adversarial predictions), we evaluate the performance of our algorithm given near-accurate\npredictions. Specifically, we evaluate the algorithm’s “smoothness” as a function of the prediction error and\nmeasure how smoothness depends on the extent to which the algorithm “trusts” the prediction. Finally, we\ncompare our algorithm to prior work for the combinations of validity and cost. Our results indicate that our\nalgorithm achieves higher validity and, in many cases, even without suffering a (much) higher cost.\n1.1 Related Work\nOther work on robust recourse following up on ROAR includes RBR [ 56], which provides robustness with\nrespect to data shifts, and RoCourseNet, which simultaneously optimizes for the model and robust recourse\n(though the initial model is fixed in our approach). The learning-augmented framework is applied in domains\nsuch as algorithm and data structures design [ 43,50,62], mechanism design [ 2,80], and privacy-preserving\nanalysis [41]. See Appendix A.\n2 Preliminaries\nConsider a predictive model fθ:X → Y, parameterized by θ∈Θ⊆Rd, which maps instances (e.g.,\nloan applicants) from a feature space X ⊆Rdto an outcome space Y={0,1}. The values of 0 and 1\nrepresent undesirable and desirable outcomes (e.g., loan denial or approval), respectively. If a model fθ0\nyields an undesirable outcome for some instance x0∈ X, i.e., fθ0(x0) = 0, the goal is to suggest anoptimal\nrecourse: the least costly way to modify x0(how an applicant should strengthen their application) so that the\nresulting instancex′would achieve the desirable outcome underf θ0, i.e.,f θ0(x′) = 1. Given a cost function\nc:X × X →R +quantifying the cost c(x′, x0)required to modify x0tox′, the recourse is defined as the\nfollowing optimization problem [70]:\nmin\nx′∈Xℓ(fθ0(x′),1) +λ·c(x′, x0),(1)\nwhere ℓ:R×R→R +is a convex loss function that is decreasing in its first argument (such as binary cross\nentropy or squared loss) and captures the extent to which the condition fθ0(x′) = 1is violated and λ≥0is a\nregularizer that balances the degree of violation from the desirable outcome and the cost of modifying x0to\nx′. The regularizer λcan be decreased gradually until the desired outcome is reached. In this work, following\nthe approach of [63, 70], we assume the cost function is theL1distance, i.e.c(x, x′) =∥x−x′∥1.\nWe denote thepriceof a recoursex′for a modelθusing\nJ(x0, x′, θ, λ) =ℓ(f θ(x′),1) +λ·c(x′, x0).(2)\nFor simplicity, we write the price asJ(x′, θ)instead.\nEquation (1)assumes that the parameters of the model remain the same over time, but in practice,\npredictive models may be periodically retrained and updated [ 70]. These updates can cause a recourse that\nis valid in the original model (i.e., one that would lead to the desirable outcome in that model) to become\ninvalid in the updated model [ 13,21]. It is, therefore, natural to require a recourse solution whose validity is\nrobust to (slight) changes in the model parameters.\nIn line with prior work on robust recourse [ 13,70], we assume that the parameters of the updated model\ncan be any θ′∈Θα, whereΘ α={θ:∥θ−θ 0∥∞≤α} ⊆ Θis a “neighborhood” around the parameters, θ0, of\nthe original model, defined using the L∞distance and aknownparameter α. Given θ0andΘ α, therobust\n2\n\nsolution would be to choose a recourse xrthat minimizes the price assuming the parameters of the updated\nmodelθ′∈Θαare chosen adversarially, i.e.,\nxr∈arg min\nx′∈Xmax\nθ′∈ΘαJ(x′, θ′).(3)\n3 Learning-Augmented Framework\nChoosing the robust recourse xraccording to (3)optimizes the price against an adversarially chosen θ′∈Θα,\nbut this price may be much higher than the optimal price if we knew θ′. This is due to the overly pessimistic\nassumption that the designer has no information regarding the realized θ′∈Θα. To overcome similarly\npessimistic results in other domains, the learning-augmented framework [ 53] provides a more refined analysis\nby assuming the designer is equipped with anunreliable predictionand then seeks to achieve near-optimal\nperformance whenever the prediction is accurate while simultaneously maintaining some robustness even if\nthe prediction is arbitrarily inaccurate.\nTo adapt the learning-augmented framework to algorithmic recourse, we assume that the designer can\ngenerate, or is provided with, a possibly unreliable prediction ˆθ∈Θαregarding the model’s parameters after\nthe model change. For example, in lending, a prediction can be inferred by information on whether the\nlender would tighten or loosen its policy over time, or can convey more information in the form of an exact\nprediction for the future model. If the designer trusts the accuracy of prediction ˆθ, then an optimal solution\nis to choose a recoursex cconsistent with ˆθi.e.,\nxc∈arg min\nx′∈XJ(x′,ˆθ).(4)\nSince the prediction is unreliable, following it blindly can lead to poor performance. In the learning-augmented\nframework, therobustnessandconsistencymeasures are used to evaluate performance.\nDefinition 3.1(Robustness).Given a parameterα, therobustnessof a recoursex′∈ Xis\nR(x′, α) = max\nθ′∈ΘαJ(x′, θ′)−max\nθ′∈ΘαJ(xr, θ′),(5)\nwherex ris defined in Equation (3).\nThe robustness evaluates the worst-case price of x′against an adversarial change of the model and then\ncompares it to the price of xr. The robustness is always at least0(achieved by x′=xr), and lower is more\ndesirable. While prior work [ 70] measures robustness in absolute terms, we evaluate it relative to xrto enable\na direct comparison between robustness and consistency.\nDefinition 3.2(Consistency).Given a prediction ˆθ∈Θ α, the consistency of a recoursex′∈ Xis\nC(x′,ˆθ) =J(x′,ˆθ)−J(x c,ˆθ),(6)\nwherex cis defined in Equation (4).\nThe consistency is always at least zero (achieved byx′=xc), and lower is more desirable.\nChoosing xrguarantees an optimal robustness of0, but can lead to poor consistency. Choosing xc\nguarantees an optimal consistency of0, but can lead to poor robustness. We study the trade-off between\nrobustness and consistency by studying the following problem:\nmin\nx′β·R(x′, α) + (1−β)·C(x′,ˆθ),(7)\nwhere β∈[0,1]. Solving for varying βs results in recourses ranging from the optimal robust recourse at β= 1\nto the optimal consistent recourse atβ= 0.\n3\n\nALGORITHM 1:RobustnessConsistencyTradeoff\nInput:x 0,fθ0,ℓ,c,α,β, ˆθOutput:x′\n1:θ←linear approximation off θ0atx 0\n2:Initializex′←x 0andActive=[d]\n3:fori∈[d]do\n4:Initialize worst-caseθ′[i]based onx[i]andθ[i]\n5:Active←Active\\ {i}ifx′\nicannot improve (7)\n6:whileActive̸=∅do\n7:i,∆←FindOptimalDimensionAndUpdate(x 0,θ,ℓ,c,α,β, ˆθ,x′)\n8:if∆ = 0then\n9:break ▷Terminate\n10:ifsgn(x′[i] + ∆) = sgn(x′[i])then\n11:x′[i]←x′[i] + ∆ ▷Update x’[i]\n12:else\n13:x′[i]←0▷Update but only until it reaches 0\n14:if|θ[i]|> αthen\n15:θ′[i]←θ[i] +α·sgn(x 0[i])▷Modifyθ′\n16:else\n17:Active←Active\\ {i}\n18:returnx′\nALGORITHM 2:FindOptimalDimensionAndUpdate\nInput:x 0,θ,ℓ,c,α,β, ˆθ, andx′Output:i,∆\n1:C← ⃗0,O← ⃗+∞▷Keep track of change ofx′and objective in each dimension\n2:fori∈[d]do\n3:K(x) =β·J(x, θ′) + (1−β)·J(x, ˆθ)▷Similar to Equation (7) withoutmaxoverθ′and constants\n4:x∗∈arg min x:x[j]=x′[j]∀j̸=i K(x)▷Minimizer ofKonly if dimensionicould change\n5:ifK(x′)−K(x∗)>0and (x′[i] =x 0[i]orsgn(x′[i]−x 0[i]) = sgn(x∗[i]−x′[i]))then\n6:O[i]←K(x′)−K(x∗)▷Only update if the change is consistent with prior changes inx′[i]\n7:C[i]←x∗[i]−x′[i]\n8:i←arg min k∈[d]O[k],∆←C[i]▷Dimensioniwhich most decreasesKand the change∆in dimension\n9:returni,∆\n3.1 Computing Robust/Consistent Recourses\nWe next provide Algorithm 1 to solve Equation (7). We introduce a few additional notations. We use sgn(s)\nwhich is 1 when s >0,0when s= 0, and −1when s <0. When applied to a vector, the sgnis applied\nelement-wise. For an integern∈N,[n] :={1, . . . , n}.\nAlgorithm 1 starts by approximating fθlocally at x0with a linear model e.g., using LIME [ 64]. The\nalgorithm then computes the worst-case model θ′for the “default” recourse of x′=x0(lines 3-5). See\nAppendix C. Then, facingθ′, the algorithm greedily modifiesx′while simultaneously updatingθ′to ensure\nthat it remains the worst-case model for x′(lines 6-17). More specifically, in each iteration of the while loop,\nfor the current θ′, the algorithm first calls the subroutine FindOptimalDimensionAndUpdate (line 7),\nimplemented in Algorithm 2. This subroutine searches over each dimension and returns the dimension i\nand changes in that dimension∆which minimizes the objective for a fixed θ′and assuming all the other\ncoordinates are fixed. After computing∆, Algorithm 1 terminates if∆is 0, meaning no improvement can be\ndone (line 9). If the change∆does not cause x′[i]to flip its sign, the update is applied and the adversarial\nmodel θ0does indeed remain fixed (lines 11). On the other hand, if the recommended change flips the sign of\nx′[i], this would cause the adversarial response to change as well. The algorithm instead applies this change\nall the way up to x′[i] = 0and updates θ′accordingly (lines 13-15). If, during this process, x′[i]becomes\n0 and an update of the adversarial model θ′[i]could cause its sign to flip, then no further change in this\n4\n\ndimension is allowed i.e.,iis removed from theActiveset (line 17).\nAlgorithm 1 is efficient since (1) the linear approximation of fθ0can be computed in polynomial time [ 64],\nand (2) the while loop runs for O(d)iterations and the running time of each iteration is dominated by the\nrun time of subroutine FindOptimalDimensionAndUpdate . This subroutine runs in polynomial time\n(e.g., by running at most done-dimensional gradient descents [ 14]). See Appendix C for more details, and\nAppendix D.7 for the scalability of our approach to a larger dataset.\nWe analyze the theoretical properties of Algorithm 1 by focusing on generalized linear models. A model\nfθis generalized linear if fθ(x) :=g◦h θ(x), where hθ:X →Ris a linear function and g:R→ [0,1]is a\nnon-decreasing function mapping the outputs of hθto probabilities e.g., setting gto the sigmoid recovers\nlogistic regression. Our main result is as follows.\nTheorem 3.3.If fθis a generalized linear model and β∈ { 0,1}, then Algorithm 1 returns a recourse\nx′∈arg min xβR(x, α) + (1−β)C(x, ˆθ)in polynomial time.\nNote that β= 1corresponds to computing the optimal robust recourse. Even for generalized linear\nmodels, the objective function in Equation (7)is non-convex when β= 1(see Appendix C.1). Hence, prior\ngradient-based approaches [ 56,70] cannot guarantee optimality. To the best of our knowledge, Algorithm 1 is\nthe first optimal algorithm for any robust recourse formulation.\nThe idea of approximating a non-linear function locally has also been used in prior work [ 63,70]. In\nSection 4, we empirically compare the performance of our algorithm to prior work in linear and non-linear\nsettings. Moreover, there may be feasibility constraints on recourses, or the data may contain categorical\nfeatures. To handle such cases, the recourse of Algorithm 1 can be post-processed (e.g., by projection) to\nguarantee feasibility [29, 56, 70]. See Appendix D.4.\n4 Experiments\nIn this section, we describe the datasets, implementation details, and present our findings.\nDatasetsWe experiment on synthetic and real-world data. For the synthetic dataset, we follow Upadhyay\net al.[70]to generate 1000 data points in 2-d. For each data point, we first sample a label y∈ { 0,1}uniformly\nat random. We then sample the instance corresponding to this label from a Gaussian distribution N(µy,Σy).\nWe set µ0= [−2,−2],µ1= [+2 ,+2], andΣ 0= Σ 1= 0.5I. See Figure 1(a) in [ 70]. We also use two real\ndatasets: (a) The German Credit dataset [ 34], which consists of 1000 data points, each with 7 features\ncontaining information about a loan applicant (age, marital status, income, and credit duration), and binary\nlabels good (1) or bad (0) determine the creditworthiness. (b) The Small Business Administration dataset [ 52],\nwhich contains the small business loans approved by the State of California from 1989 to 2004. The dataset\nincludes 1159 data points, each with 28 features containing information about the business (business category,\nzip code, and number of jobs created by the business), and the binary labels indicate whether the small\nbusiness has defaulted on the loan (0) or not (1). For real-world data, we normalize the features. We use the\ndatasets to learn the initial modelθ 0. See Appendix D.7 for experiments on a larger dataset.\nImplementation DetailsWe use 5-fold cross-validation in all experiments: 4 folds to train the initial model\nθ0and the remaining fold to compute recourse. Recourse is only computed for instances with label (0) under\nθ0, and we report averages over folds and test instances in all experiments. We used logistic regression as our\nlinear model and trained it using Scikit-Learn. As our non-linear model, we used a 3-level neural network\nwith 50, 100, and 200 nodes in each successive layer (same as [ 70]). The network uses ReLU activation\nfunctions, binary cross-entropy loss, and Adam optimizer, and is trained for 100 epochs using PyTorch. See\nAppendix D.1 for additional implementation details.\nTo generate recourse, we implemented Algorithm 1 with different βvalues. For non-linear models, we first\napproximate them locally with LIME [ 64] (same as [ 70]). We used the code from [ 70] and [56] for ROAR and\nRBR’s implementation as baselines. We next describe our parameter choices. We use binary cross-entropy\nas the loss function ℓandL1distance as the cost function c. We measure closeness using the L∞norm for\nmodel parameters. In Section 4.1, we follow the same procedure as in [ 70] for selecting αandλ. We fix an α\nand greedily search for λthat maximizes the recourse validity under the original model θ0. We study the\n5\n\neffect of varying αandλin Section 4.2 and Appendix D.3. In each experiment, we specify how ˆθis selected.\nSee our code and Appendix D.1 for more details.\n0 0.05 0.1 0.15 0.200.20.40.60.811.21.41.61.8\nConsistencyRobustness\n(a) Synthetic Dataset, Logistic Regression\n0 2 4 6 8 10 12 140246810121416\nConsistencyRobustness (b) Synthetic Dataset, Neural Network\n0 0.2 0.4 0.6 0.8 1 1.200.050.10.150.20.250.30.350.4\nConsistencyRobustness\n(c) German Credit Dataset, Logistic Regression\n0 0.05 0.1 0.15 0.2 0.25 0.300.20.40.60.811.2\nConsistencyRobustness (d) German Credit Dataset, Neural Network\n0 0.5 1 1.5 2 2.5 3012345678\nConsistencyRobustness\n(e) Small Business Dataset, Logistic Regression\n0 0.5 1 1.5 20123456\nConsistencyRobustness (f) Small Business Dataset, Neural Network\nFigure 1: The trade-off between robustness and consistency for α= 0.5: rows and columns correspond\nto different datasets and models as indicated in the sub-caption. In each subfigure, each curve shows the\ntrade-off for different predictions for our algorithm and ROAR.\n6\n\n4.1 Findings: Learning-Augmented Setting\nRobustness-Consistency Trade-offTo study the trade-off between consistency and robustness, we\ngenerated 5 predictions. For logistic regression models, we generated 4 perturbations by adding or subtracting\nαin each dimension of θ0. For neural network models, we added the perturbation to the LIME approximation\noffθ0. Along with θ0, these form the 5 model parameters we used as predictions. We use α= 0.5for the\ntrade-off results (see Appendix D.3 for different αs). For each given prediction ˆθ, we run Algorithm 1 and a\nmodification of ROAR [ 70] as a baseline for varying β∈[0,1]and compute the robustness and consistency of\nthe solution using Equations (5) and (6).\nIn Figure 1, the rows and columns correspond to different datasets and models. In each sub-figure, curves\nshow the robustness-consistency trade-off of recourses by Algorithm 1 for different predictions (indicated by\ndifferent colors) and ROAR (same color for each prediction by with lines that include⋆.\nFor each curve, the bottom right point corresponds to β= 1(the optimal robust recourse). Given the\noptimality of Algorithm 1 for linear models, the robustness is0, though empirically our algorithm achieves\nnear 0 robustness for non-linear models too. However, these recourses might have different consistencies\ndepending on the prediction. Similarly, the top left point of each curve corresponds to β= 0(the optimal\nconsistent recourse) with a consistency of 0, which might have different robustness. ROAR does not achieve\neither optimal robustness or optimal consistency.1\nOur algorithm Pareto dominates ROAR, simultaneously achieving better robustness and consistency for\nallβvalues. Furthermore, at β= 1, the robustness of ROAR’s recourses is substantially worse compared to\nours (e.g., ROAR achieves a robustness of 17.64 on the neural network models learned on synthetic data\n(Figure 1b)). In addition, ROAR is much slower than our algorithm (See Section 4.2). We generally observe\nthat the poor consistency of the robust solutions can be decreased substantially with a small increase in\nrobustness. This is also true for decreasing the robustness of the consistent solutions, albeit to a smaller\ndegree.\nSmoothnessIn this section, we study how prediction error can affect the recourse quality. In particular,\nfor each dataset-model pair, we first compute thecorrect prediction ˆθ∗, corresponding to the realized future\nmodel (by either shifting the data or temporal changes in data collection [ 70]). We then generate incorrect\npredictions by perturbing each coordinate of the correct prediction by different values in {+ϵ,−ϵ, +2ϵ,−2ϵ},\ncorresponding to the amount of error. The ϵvalues depend on the dataset and model and are chosen to\nensure that all the predictions are inΘ αforα= 1.\nGiven a prediction ˆθand a parameter β∈[0,1], Algorithm 1 generates a recourse. If β= 1, it ignores the\nprediction, and if β= 0it fully trusts it; values of β∈(0,1)strike a balance between these two extremes.\nTo measure the performance as a function of the prediction error we define a metric calledsmoothness:\nJ(x′(β,ˆθ),ˆθ∗)−J(x′(ˆθ∗),ˆθ∗),where x′(β,ˆθ)is the computed recourse, ˆθ∗is the correct prediction and x′(ˆθ∗)\nis the consistent recourse for the correct prediction. The smoothness is non-negative and it is 0 if the learner is\nprovided with the correct prediction ( ˆθ=ˆθ∗) and fully trusts it ( β= 0). Lower smoothness values correspond\nto better performance despite the error.\nIn Figure 2, the rows and columns correspond to different datasets and models. In each sub-figure, curves\nshow the smoothness of Algorithm 1 for different predictions as a function of β. There are 5 lines in each\nsubfigure corresponding to the correct prediction and the perturbations. Focusing on β= 0, we observe\nthat smoothness increases sharply as a function of the prediction “error”, since the algorithm fully trusts\nthe (incorrect) prediction. As β→ 1, the smoothness of all predictions converges to the same value, since\nthe algorithm disregards them. In some cases, this convergence occurs at smaller βvalues (Figure 2e),\nbut other cases require βto be very close to 1 (Figure 2a). While the smoothness monotonically increases\nwith βwhen using the correct prediction, using incorrect predictions results in interesting non-monotone\nbehavior and even leads to improved performance compared to using the correct prediction (Figure 2a). In\nAppendix D.5, we provided baselines using a variant of ROAR, though the generated recourses generally\n1The optimization problem in Equation 7 is convex but non-differentiable at β= 0for linear models. Hence, gradient-based\nmethods with fixed step sizes, such as ROAR, are not guaranteed to find the optimal solution.\n7\n\n0 0.2 0.4 0.6 0.8 100.010.020.030.040.050.06Smoothness(a) Synthetic Dataset, Logistic Regression\n0 0.2 0.4 0.6 0.8 100.010.020.030.040.050.060.07Smoothness (b) Synthetic Dataset, Neural Network\n0 0.2 0.4 0.6 0.8 100.20.40.60.81Smoothness\n(c) German Credit Dataset, Logistic Regression\n0 0.2 0.4 0.6 0.8 100.050.10.150.2Smoothness (d) German Credit Dataset, Neural Network\n0 0.2 0.4 0.6 0.8 100.20.40.60.811.21.41.61.8Smoothness\n(e) Small Business Dataset, Logistic Regression\n0 0.2 0.4 0.6 0.8 100.20.40.60.81Smoothness (f) Small Business Dataset, Neural Network\nFigure 2: The smoothness for predictions with different accuracies: rows and columns correspond to different\ndatasets and models as indicated in the sub-caption. In each subfigure, curves correspond to different\npredictions and track the smoothness as a function ofβfor the given prediction.\nhave higher smoothness.\n4.2 Findings: Computing Robust Recourse\nBy setting β= 1, our algorithm can be used to compute robust recourse. In this section, we perform a more\ndetailed comparison with ROAR by breaking down the robustness to understand the effect of each term in\n8\n\n5 5.2 5.4 5.6 5.8 6 6.200.20.40.60.81\nCostFuture Validity(a) Synthetic Dataset, Logistic Regression\n3.5 4 4.500.20.40.60.81\nCostFuture Validity (b) Synthetic Dataset, Neural Network\n0 1 2 3 4 500.20.40.60.81\nCostFuture Validity\n(c) German Credit Dataset, Logistic Regression\n1 1.5 2 2.500.20.40.60.81\nCostFuture Validity (d) German Credit Dataset, Neural Network\n2 2.5 3 3.5 4 4.5 5 5.500.20.40.60.81\nCostFuture Validity\n(e) Small Business Dataset, Logistic Regression\n2 3 4 5 6 700.20.40.60.81\nCostFuture Validity (f) Small Business Dataset, Neural Network\nFigure 3: The trade-off between future validity and cost: rows and columns correspond to different datasets\nand models. In each subfigure, curves show the trade-off for different algorithms.\nEquation (1). The first term is a proxy forvalidityand the second term is the cost of modifying x0. To\ncompute validity, as opposed to computing a possibly different model for each instance, as is done till now,\nwe compute a single model for the entire dataset. Following prior work [ 56,70], this model is obtained by\ntraining a model on a shifted version of the datasets. We call the validity with respect to this new model as\nfuture validity. See Appendix D.1.\nFigure 3 depicts the trade-off between future validity and cost for all datasets and models. In each curve,\nwe varied α∈[0.02,0.2]in increments of 0.02. We used 4 different λs: 0.05, 0.1, 0.2, and 0.3, and the trade-off\n9\n\nfor each λis plotted with a different color. To avoid overcrowding, we only included the results of λ= 0.1\nfor ROAR. Using λ= 0.05does not change the trade-off for ROAR, and using λ= 0.2,or0.3degrades the\nvalidity even further. For our algorithm, sometimes different λvalues create similar trade-offs, in which case\nwe only include the results for one of them. Also note that our algorithm runs much faster (10-100x) than\nROAR for all parameters. See Appendix D.1.\nFigure 3 indicates that our algorithm often Pareto dominates ROAR, i.e., it achieves higher validity with\na smaller cost. While the validity of our algorithm often approaches 1 for smaller λvalues, this is not the\ncase for ROAR. In addition, the validity is lower for both approaches when using neural network models.\nFinally, consistent with Pawelczyk et al. [59]’s observation, the cost of recourse can increase significantly for\nvalidity values close to 1. In Section D.6, we provide further comparisons with RBR [56].\n5 Conclusion and Discussion\nThe robust recourse literature has focused on many aspects: different model classes, costs, model changes,\nand optimization approaches resulting in formulations that require different solutions (see [ 35]). Furthermore,\nadopting the learning-augmented framework introduces other modeling aspects such as prediction, definitions\nof robustness or consistency, and measuring the trade-off between them. We initiated the study of the\nlearning-augmented robust recourse and followed the assumptions in the closest prior work to allow for a\ndirect comparison. We highlight several extensions.\nFirst, while our framework can handle customizable weights for different inputs, using any norm as the\ncost function implies that the features can be modified independently. Some prior work focuses on considering\nthese dependencies and alsoactionabilityof recourse [ 37,39,57]. We leave these as future work. Our notion of\nrobustness and consistency measures the performance of the algorithm against optimal solutions in an additive\nmanner, similar toregret[ 16]. This comparison can also be made multiplicatively, similar tocompetitive\nratio[53], which we leave as future work. Moreover, studying weaker notions of model change, such as L1/L2\nnorm [70] or studying alternative formalizations of model change [ 31] is left for future work. As is common in\nthe learning-augmented framework [ 1,53], we assumed the prediction about the updated model is explicitly\ngiven. A natural way to compute such a prediction is through performativity [ 61]. Moreover, in practice, the\nfeedback might beweakeror evennoisy[ 8]. Incorporating these into our framework is an exciting future\nwork direction. Finally, although we did provide non-trivial theoretical results, our focus was on the empirical\nevaluation of our algorithms and, in line with prior robust recourse work. We leave a potential theoretical\nanalysis to future work.\nAcknowledgment\nWe thank Kaidi Xu and Phone Kyaw for insightful discussions. Vasilis Gkatzelis was partially supported by\nthe NSF CAREER award CCF-2047907 and the NSF grant CCF-2210502.\nReferences\n[1]Sushant Agarwal, Shahin Jabbari, Chirag Agarwal, Sohini Upadhyay, Steven Wu, and Himabindu\nLakkaraju. Towards the unification and robustness of perturbation and gradient based explanations. In\n38th International Conference on Machine Learning, 2021.\n[2]Priyank Agrawal, Eric Balkanski, Vasilis Gkatzelis, Tingting Ou, and Xizhi Tan. Learning-augmented\nmechanism design: Leveraging predictions for facility location. In23rd ACM Conference on Economics\nand Computation, pages 497–528, 2022.\n[3]Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:\nCircumventing defenses to adversarial examples. In35th International Conference on Machine Learning,\npages 274–283, 2018.\n10\n\n[4]Pranjal Awasthi, Abhratanu Dutta, and Aravindan Vijayaraghavan. On robustness to adversarial\nexamples and polynomial optimization. InAdvances in Neural Information Processing Systems 32, pages\n13737–13747, 2019.\n[5]Solon Barocas, Moritz Hardt, and Arvind Narayanan.Fairness and Machine Learning: Limitations and\nOpportunities. fairmlbook.org, 2019.\n[6]Solon Barocas, Andrew Selbst, and Manish Raghavan. The hidden assumptions behind counterfactual ex-\nplanations and principal reasons. In3rd ACM Conference on Fairness, Accountability, and Transparency,\npages 80–89, 2020.\n[7]Elnaz Barshan, Marc-Etienne Brunet, and Gintare Dziugaite. RelatIF: Identifying explanatory training\nsamples via relative influence. In23rd International Conference on Artificial Intelligence and Statistics,\npages 1899–1909, 2020.\n[8]Yahav Bechavod, Chara Podimata, Zhiwei Steven Wu, and Juba Ziani. Information discrepancy in\nstrategic learning. In38th International Conference on Machine Learning, pages 1691–1715, 2022.\n[9]Andrew Bell, João Fonseca, Carlo Abrate, Francesco Bonchi, and Julia Stoyanovich. Fairness in\nalgorithmic recourse through the lens of substantive equality of opportunity.CoRR, abs/2401.16088,\n2024.\n[10] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski.Robust Optimization. Princeton University Press, 2009.\n[11]Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal\njustice risk assessments: The state of the art.Sociological Methods & Research, 50(1):3–44, 2021.\n[12]Tom Bewley, Salim Amoukou, Saumitra Mishra, Daniele Magazzeni, and Manuela Veloso. Counterfactual\nmetarules for local and global recourse. In41st International Conference on Machine Learning, 2024.\n[13]Emily Black, Zifan Wang, and Matt Fredrikson. Consistent counterfactuals for deep models. In10th\nInternational Conference on Learning Representations, 2022.\n[14] Stephen Boyd and Lieven Vandenberghe.Convex Optimization. Cambridge University Press, 2014.\n[15]Andrei Buliga, Chiara Di Francescomarino, Chiara Ghidini, Marco Montali, and Massimiliano Ronzani.\nGenerating counterfactual explanations under temporal constraints. In39th Annual AAAI Conference\non Artificial Intelligence, pages 15622–15631, 2025.\n[16]Nicolò Cesa-Bianchi and Gábor Lugosi.Prediction, learning, and games. Cambridge University Press,\n2006.\n[17]Seung Hyun Cheon, Anneke Wernerfelt, Sorelle Friedler, and Berk Ustun. Feature responsiveness scores:\nModel-agnostic explanations for recourse. In13th International Conference on Learning Representations,\n2025.\n[18]John Danskin.The Theory of Max-Min and its Application to Weapons Allocation Problems. Springer,\n1967.\n[19]Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring Adult: New datasets for fair\nmachine learning. InAdvances in Neural Information Processing Systems 34, pages 6478–6490, 2021.\n[20]Ricardo Dominguez-Olmedo, Amir-Hossein Karimi, and Bernhard Schölkopf. On the adversarial\nrobustness of causal algorithmic recourse. In39th International Conference on Machine Learning, pages\n5324–5342, 2022.\n11\n\n[21]Sanghamitra Dutta, Jason Long, Saumitra Mishra, Cecilia Tilli, and Daniele Magazzeni. Robust\ncounterfactual explanations for tree-based ensembles. In39th International Conference on Machine\nLearning, volume 162, pages 5742–5756, 2022.\n[22]Ahmad-Reza Ehyaei, Ali Shirali, and Samira Samadi. Collective counterfactual explanations via optimal\ntransport.CoRR, abs/2402.04579, 2024.\n[23]Hidde Fokkema, Damien Garreau, and Tim van Erven. The risks of recourse in binary classification. In\n27th International Conference on Artificial Intelligence and Statistics, pages 550–558, 2024.\n[24]João Fonseca, Andrew Bell, Carlo Abrate, Francesco Bonchi, and Julia Stoyanovich. Setting the\nright expectations: Algorithmic recourse over time. In3rd ACM Conference on Equity and Access in\nAlgorithms, Mechanisms, and Optimization, pages 29:1–29:11, 2023.\n[25]Ruijiang Gao and Himabindu Lakkaraju. On the impact of algorithmic recourse on social segregation.\nIn40th International Conference on Machine Learning, pages 10727–10743, 2023.\n[26] Prateek Garg, Lokesh Nagalapatti, and Sunita Sarawagi. From search to sampling: Generative models\nfor robust algorithmic recourse. In13th International Conference on Learning Representations, 2025.\n[27]Ulrike Grömping. South German Credit (UPDATE). UCI Machine Learning Repository, 2020. DOI:\nhttps://doi.org/10.24432/C5QG88.\n[28]OzgurGuldogan, YuchenZeng, Jy-yongSohn, RamtinPedarsani, andKangwookLee. Equalimprovability:\nA new fairness notion considering the long-term impact. In11th International Conference on Learning\nRepresentations, 2023.\n[29] Hangzhi Guo, Feiran Jia, Jinghui Chen, Anna Squicciarini, and Amulya Yadav. RoCourseNet: Robust\ntraining of a prediction aware recourse model. In32nd ACM International Conference on Information\nand Knowledge Management, pages 619–628. ACM, 2023.\n[30]Vivek Gupta, Pegah Nokhiz, Chitradeep Roy, and Suresh Venkatasubramanian. Equalizing recourse\nacross groups.CoRR, abs/1909.03166, 2019.\n[31]Faisal Hamman, Erfaun Noorani, Saumitra Mishra, Daniele Magazzeni, and Sanghamitra Dutta. Robust\ncounterfactual explanations for neural networks with probabilistic guarantees. In40th International\nConference on Machine Learning, pages 12351–12367, 2023.\n[32]Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. In30th\nAnnual Conference on Neural Information Processing Systems, pages 3315–3323, 2016.\n[33]Hoda Heidari, Vedant Nanda, and Krishna Gummadi. On the long-term impact of algorithmic decision\npolicies: Effort unfairness and feature segregation through social learning. In36th International\nConference on Machine Learning, pages 2692–2701, 2019.\n[34]Hans Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository, 1994. DOI:\nhttps://doi.org/10.24432/C5NC77.\n[35]Junqi Jiang, Francesco Leofante, Antonio Rago, and Francesca Toni. Recourse under model multiplicity\nvia argumentative ensembling. In23rd International Conference on Autonomous Agents and Multiagent\nSystems, pages 954–963, 2024.\n[36]Junqi Jiang, Francesco Leofante, Antonio Rago, and Francesca Toni. Robust counterfactual explanations\nin machine learning: A survey. In33rd International Joint Conference on Artificial Intelligence, pages\n8086–8094, 2024.\n12\n\n[37]Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. Towards\nrealistic individual recourse and actionable explanations in black-box decision making systems.CoRR,\nabs/1907.09615, 2019.\n[38]Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, and Yuichi Ike. Learning decision trees and forests\nwith algorithmic recourse. In41st International Conference on Machine Learning, 2024.\n[39]Amir-Hossein Karimi, Gilles Barthe, Borja Balle, and Isabel Valera. Model-agnostic counterfactual\nexplanations for consequential decisions. In23rd International Conference on Artificial Intelligence and\nStatistics, pages 895–905, 2020.\n[40]Amir-Hossein Karimi, Bodo Julius von Kügelgen, Bernhard Schölkopf, and Isabel Valera. Algorithmic\nrecourse under imperfect causal knowledge: a probabilistic approach. InAdvances in Neural Information\nProcessing Systems 33, 2020.\n[41]Mikhail Khodak, Kareem Amin, Travis Dick, and Sergei Vassilvitskii. Learning-augmented private\nalgorithms for multiple quantile release. In40th International Conference on Machine Learning, volume\n202, pages 16344–16376, 2023.\n[42]Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In34th\nInternational Conference on Machine Learning, pages 1885–1894, 2017.\n[43]Tim Kraska, Alex Beutel, Ed Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. InInternational Conference on Management of Data, pages 489–504, 2018.\n[44]Himabindu Lakkaraju, Stephen Bach, and Jure Leskovec. Interpretable decision sets: A joint framework\nfor description and prediction. In22nd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, pages 1675–1684, 2016.\n[45]Francesco Leofante and Nico Potyka. Promoting counterfactual robustness through diversity. In38th\nAAAI Conference on Artificial Intelligence, pages 21322–21330, 2024.\n[46]Yan Li, Ethan Fang, Huan Xu, and Tuo Zhao. Implicit bias of gradient descent based adversarial training\non separable data. In8th International Conference on Learning Representations, 2020.\n[47]Alexander Lindermayr and Nicole Megow. Algorithms with predictions, 2024. URL https://\nalgorithms-with-predictions.github.io/.\n[48]Arnaud Van Looveren and Janis Klaise. Interpretable counterfactual explanations guided by prototypes.\nInMachine Learning and Knowledge Discovery in Databases, pages 650–665, 2021.\n[49]Scott Lundberg and Su-In Lee. A unified approach to interpreting model predictions. InAdvances in\nNeural Information Processing Systems 30, pages 4765–4774, 2017.\n[50]Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. In35th\nInternational Conference on Machine Learning, pages 3302–3311, 2018.\n[51]Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards\ndeep learning models resistant to adversarial attacks. In6th International Conference on Learning\nRepresentations, 2018.\n[52]Amy Mickel Min Li and Stanley Taylor. “should this loan be approved or denied?”: A large dataset with\nclass assignment guidelines.Journal of Statistics Education, 26, 2018.\n[53]Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. In Tim Roughgarden,\neditor,Beyond the Worst-Case Analysis of Algorithms, pages 646–662. Cambridge University Press,\n2020.\n13\n\n[54]Rami Mochaourab, Sugandh Sinha, Stanley Greenstein, and Panagiotis Papapetrou. Robust explanations\nfor private support vector machines.CoRR, abs/2102.03785, 2021.\n[55]Duy Nguyen, Ngoc Bui, and Viet Anh Nguyen. Distributionally robust recourse action. In11th\nInternational Conference on Learning Representations, 2023.\n[56]Tuan-Duy Nguyen, Ngoc Bui, Duy Nguyen, Man-Chung Sue, and Viet Anh Nguyen. Robust bayesian\nrecourse. In38th Conference on Uncertainty in Artificial Intelligence, pages 1498–1508, 2022.\n[57]Martin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. Learning model-agnostic counterfactual\nexplanations for tabular data. In29th The ACM Web Conference, pages 3126–3132, 2020.\n[58]Martin Pawelczyk, Chirag Agarwal, Shalmali Joshi, Sohini Upadhyay, and Himabindu Lakkaraju.\nExploring counterfactual explanations through the lens of adversarial examples: A theoretical and\nempirical analysis. In25th International Conference on Artificial Intelligence and Statistics, pages\n4574–4594, 2022.\n[59]Martin Pawelczyk, Teresa Datta, Johannes van den Heuvel, Gjergji Kasneci, and Himabindu Lakkaraju.\nProbabilistically robust recourse: Navigating the trade-offs between costs and robustness in algorithmic\nrecourse. In11th International Conference on Learning Representations, 2023.\n[60]Martin Pawelczyk, Tobias Leemann, Asia Biega, and Gjergji Kasneci. On the trade-off between actionable\nexplanations and the right to be forgotten. In11th International Conference on Learning Representations,\n2023.\n[61]Juan Perdomo, Tijana Zrnic, Celestine Mendler-Dünner, and Moritz Hardt. Performative prediction. In\n37th International Conference on Machine Learning, pages 7599–7609, 2020.\n[62]Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ML predictions. In\nAdvances in Neural Information Processing Systems 31, pages 9684–9693, 2018.\n[63]Kaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and interactive\nsummaries of actionable recourses. InAdvances in Neural Information Processing Systems 33, 2020.\n[64]Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?\": Explaining the\npredictions of any classifier. In22nd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, pages 1135–1144, 2016.\n[65]Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use\ninterpretable models instead.Nat. Mach. Intell., 1(5):206–215, 2019.\n[66]Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and\nDhruv Batra. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In\n16th IEEE International Conference on Computer Vision, pages 618–626, 2017.\n[67]Dylan Slack, Anna Hilgard, Himabindu Lakkaraju, and Sameer Singh. Counterfactual explanations can\nbe manipulated. InAdvances in Neural Information Processing Systems 34, pages 62–75, 2021.\n[68]Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. Smoothgrad:\nremoving noise by adding noise.CoRR, abs/1706.03825, 2017.\n[69]Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In34th\nInternational Conference on Machine Learning, pages 3319–3328, 2017.\n[70]Sohini Upadhyay, Shalmali Joshi, and Himabindu Lakkaraju. Towards robust and reliable algorithmic\nrecourse. InAdvances in Neural Information Processing Systems 34, pages 16926–16937, 2021.\n14\n\n[71]Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classification. In3rd\nACM Conference on Fairness, Accountability, and Transparency, pages 10–19, 2019.\n[72]Suresh Venkatasubramanian and Mark Alfano. The philosophical basis of algorithmic recourse. In3rd\nACM Conference on Fairness, Accountability, and Transparency, pages 284–293, 2020.\n[73] Sahil Verma, John Dickerson, and Keegan Hines. Counterfactual explanations for machine learning: A\nreview.CoRR, abs/2010.10596, 2020.\n[74]Daniël Vos and Sicco Verwer. Efficient training of robust decision trees against adversarial examples. In\n38th International Conference on Machine Learning, volume 139, pages 10586–10595, 2021.\n[75]Daniël Vos and Sicco Verwer. Robust optimal classification trees against adversarial examples. In36th\nAAAI Conference on Artificial Intelligence, pages 8520–8528, 2022.\n[76]Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the\nblack box: automated decisions and the GDPR.Harvard Journal of Law and Technology, 31(2):841–887,\n2018.\n[77]Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and Perry MacNeille. A\nbayesian framework for learning rule sets for interpretable classification.J. Mach. Learn. Res., 18:\n70:1–70:37, 2017.\n[78]Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the convergence\nand robustness of adversarial training. In36th International Conference on Machine Learning, pages\n6586–6595, 2019.\n[79]Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer\nadversarial polytope. In35th International Conference on Machine Learning, pages 5283–5292, 2018.\n[80]Chenyang Xu and Pinyan Lu. Mechanism design with predictions. In31st International Joint Conference\non Artificial Intelligence, pages 571–577, 2022.\n[81]Hongyu Yang, Cynthia Rudin, and Margo Seltzer. Scalable bayesian rule lists. In Doina Precup and\nYee Whye Teh, editors,34th International Conference on Machine Learning, pages 3921–3930, 2017.\n[82]Jayanth Yetukuri, Ian Hardy, Yevgeniy Vorobeychik, Berk Ustun, and Yang Liu. Providing fair recourse\nover plausible groups. In38th AAAI Conference on Artificial Intelligence, pages 21753–21760, 2024.\n[83]Muhammad Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness\nconstraints: Mechanisms for fair classification. In20th International Conference on Artificial Intelligence\nand Statistics, pages 962–970, 2017.\nA Additional Related Work\nTheemergingliteratureontheinterpretabilityandexplainabilityofmachinelearningsystemsmainlyadvocates\nfor two main approaches. The first approach aims to build inherently simple or interpretable models such as\ndecision lists [ 44] or generalized additive models [ 77,81]. These approaches provideglobalexplanations for\nthe deployed models. The second approach attempts to explain the decisions of complex black-box models\n(such as deep neural networks) only on specific inputs [ 1,7,22,42,49,64,66,68,69]. These approaches\nprovide alocalexplanation of the model and are sometimes referred to as post-hoc explanations.\nRecourse is a post-hoc counterfactual explanation that aims to provide the lowest cost modification that\nchanges the prediction for a given input with an undesirable prediction under the current model [ 26,48,\n59,63,67,71,76]. Since its introduction, different formulations have been used to model the optimization\n15\n\nproblem in recourse (see [ 73] for an overview). Wachter et al. [76]and Pawelczyk et al. [59]considered\nscore-based classifiers and defined modifications to help instances achieve the desired scores. On the other\nhand, for binary classifiers, Ustun et al. [71]required the modification to result in the desired label. Roughly\nspeaking, the first setting can be viewed as a relaxation of the second setting, and we follow the second\nformulation in our work.\nThe follow-up works on the problem study several other aspects such as focusing on specific models such\nas linear models [ 71] or decision-trees [ 12,38], understanding the setting and its implicit assumptions and\nimplications [ 6,23,25,72], attainability or actionability [ 37,39,57,71], imperfect causal knowledge [ 40],\nfairness in terms of cost of implementation for different subgroups [ 28,30,33], repeated dynamics [ 9,22,24],\ntemporal data [ 15] and providing model-agnostic explanations for recourse by computing responsiveness\nscores [17]. Extending our work to account for these different aspects of recourse is left for future work.\nThe most closely related work to us is by Upadhyay et al. [70], which initiates the study of robust recourse\nand proposes an algorithm called RObust Algorithmic Recourse (ROAR). We study the same framework for\nour robustness analysis. Nguyen et al. [56]proposes a new framework for robust recourse that uses a different\nobjective than ROAR and focuses on data shifts instead of model shifts. They call their frameworkRobust\nBayesian Recourse (RBR)and we refer to their algorithm with the same abbreviation. The RoCourseNet\nalgorithm [ 29] also provides robust recourse, though a direct comparison with this algorithm is not possible,\nas it is an end-to-end approach, i.e., it simultaneously optimizes for the learned model and robust recourse\nwhile the initial model is fixed in our approach, just like in [ 56,70]. Similar to the literature on algorithmic\nrecourse, the literature on robust algorithmic recourse has also studied specialized settings (for different model\nclasses, cost functions, and model changes) and additional desiderata such as fairness [ 21,35,45,54,55,82].\nNeither approaches use predictions, though we compare our algorithm with both ROAR and RBR when\nstudying the trade-off between validity and cost of robust recourse in the absence of predictions. See [ 36] for\na recent survey.\nIn addition to the above works, Pawelczyk et al. [60]studies how model updates due to the “right to be\nforgotten\" can affect recourse validity. Dominguez-Olmedo et al. [20]showed that minimum cost recourse\nsolutions are provably not robust to adversarial perturbations in the model and then presented robust recourse\nsolutions for linear and differentiable models. Black et al. [13]observe that recourse in deep models can be\ninvalid by small perturbations and suggest that the model’s Lipschitzness at the counterfactual point is the\nkey to preserving validity. Hamman et al. [31]proposed a new notion of model change, which they coin\n“naturally-occurring\" model change, and provide recourse with theoretical guarantees on the validity of the\nrecourse.\nRecourse is also closely related to adversarial training or robust machine learning [ 3,51,79]. Pawelczyk\net al.[58]studied the connections between various recourse formulations and their analogs in the robust\nmachine-learning literature. In fact, ROAR [ 70] builds on gradient-based methods originally developed\nfor adversarial training. The convergence of such algorithms has been studied extensively under various\nassumptions (see, e.g., [ 78]). Theoretical guarantees for adversarial training typically rely on specific data\ndistributions and model classes. For instance, with data drawn from a mixture of Gaussians, the optimal\nrobust classifiers are linear [ 46]. Moreover, Awasthi et al. [4]propose an algorithm for learning robust linear\nclassifiers under the realizability assumption and establish hardness results for robust learning of degree-2\npolynomial threshold functions. Beyond linear models, theoretical analyses have also been extended to other\nclasses such as decision trees [ 74,75]. To our knowledge, however, none of these algorithms directly address\nour setting.\nThe learning-augmented framework is applied to a wide variety of settings, aiming to provide a refined\nunderstanding of the performance guarantees that are achievable beyond the worst case. Unlike approaches in\nrobust optimization that utilize uncertainty sets [ 10], the learning augmented framework does not impose any\nassumptions on the quality of the prediction. One of the main application domains is the design of algorithms\n(e.g., online algorithms [ 50,62]), but it has also been used in data structures design [ 43], mechanisms\ninteracting with strategic agents [ 2,80], and privacy-preserving methods for processing sensitive data [ 41].\nThis is already a vast and rapidly growing literature; see [ 47] for a frequently updated and organized list of\nrelated papers.\n16\n\nB ROAR\nFor completeness, we provide the details of ROAR [ 70] in this section. ROAR is inspired by the vast literature\non adversarial training (see e.g., [ 51] and utilizes Danskin’s Theorem [ 18]) to compute the gradients (with\nrespect to x′). The pseudocode is provided in Algorithm 3. ROAR requires the function Jto be differentiable\nforx′. Furthermore, ROAR relies on the ability to compute a maximizer of Jforθ′. When Jis differentiable\nforθ′, a local maximum can be computed with projected gradient ascent.\nALGORITHM 3:RObust Algorithmic Recourse (ROAR)\nInput:x 0,θ0,ℓ,c,α,Lp,η(learning rate)\nOutput:x′\n1:Initializex′←x 0.▷Initialization for the robust solution\n2:Initializeg← ⃗0.▷Initialization for gradients\n3:Repeat\n4:θ′←arg maxθ:∥θ−θ 0∥p≤αJ(x′, θ)▷Maximizer ofJwith respect toθfor the currentx′\n5:g← ∇ x′J(x′, θ′)▷The gradient ofJwith respect tox′for the currentθ′\n6:x′←x′−αg ▷A gradient descent step to updatex′\n7:Untilconvergence\n8:returnx′.\nC Omitted Details from Section 3.1\nC.1 Non-convexity of Robust Recourse for Linear Models\nNote that at β= 1, Equation (7)is the same as Equation (5). Furthermore, the set of solutions to Equation 5\nand Equation (3)is identical since the only difference between the two equations is the additional second\nterm in Equation (5), which is a constant.\nWe provide a concrete example that makes it easy to verify the non-convexity of the optimization problem\nin Equation (3)even for linear models. Consider an instance in one dimension where x0= [1,1](note that\nthe second dimension is the unchangeable intercept), θ0= [0,0],ℓis squared loss, α= 0.5, and λ= 1.\nFor any recourse, xr= [x,1](note that the intercept cannot change), the worst-case θ′is of the form\n[0.5sign(x),−0.5]since αis 0.5 and θ0is 0 in both dimensions. The cost of recourse for xrcan be written as\n1/\u0000\ne0.5xsign(x)−0.5\u00012+|x−1|. Plotting this one-dimensional function proves that this function is not convex.\nC.2 Proof of Theorem 3.3\nTo prove Theorem 3.3 and verify the optimality of Algorithm 1 for generalized linear models and β∈ { 0,1},\nwe first make some observations and prove some useful lemmas.\nFirst of all, at β= 1, Equation (7)is the same as Equation 5. Furthermore, the set of solutions\nto Equation (5)and Equation (3)is identical since the only difference between the two equations is the\nadditional second term in Equation (5), which is a constant. Moreover, at β= 0, Equation (7)is the same as\nEquation (6). Furthermore, the set of solutions to Equation 6 and Equation (3)with α= 0is identical since\nthe only difference between the two equations at α= 0is that the second term is a constant. Hence, to prove\nTheorem 3.3, it suffices to show that Algorithm 1 will compute the optimal solution for Equation (3)when fθ0\nis a generalized linear model i.e., the x′returned by Algorithm 1 satisfies x′∈arg min x∈X max θ′∈ΘαJ(x, θ′).\nTo simplify the exposition, we rewrite a simplified version of Algorithm 1 for this specific case of β= 1and\ngeneralized linear models and flesh out all the omitted details. We present this simplification in Algorithm 4.\nThe summary of the simplifying steps is as follows: (1) The pre-processing step (lines 3-5 in Algorithm 1) are\nexpanded to distinguish between when x0is initially 0 at any coordinate or not. If x0[i]is 0 in any coordinate\nand the adversarial θcan change sign (i.e., θ0[i]< α), the optimal choice for x′[i]would be 0. Instead of waiting\n17\n\nfor line 17 in Algorithm 1 to detect this, the simplified implementation removes dimension ifrom the ACTIVE\nset to improve the running time. (2) Instead of calling the subroutineFindOptimalDimensionAndUpdate\n(line 7 of Algorithm 1), the simplified implementation computes the dimension iby finding the dimension\nin theACTIVE set that have the highest θ′value since any change of a fixed amount provides the most\nbang-per-buck in that dimension (line 12). Then line 13 of Algorithm 4 computes an identical calculation to\nline 3 in Algorithm 1 to compute the best change∆in the dimension that needs to be updated. (3) And\nfinally, Algorithm 4 does not check for∆being 0 and terminates when the update in one dimension can be\ndone without changing the sign (line 16). This is simply to speed up the running time, since, as we show in\nthe proof, without termination,∆would be 0 in the next iteration.\nSo to prove Theorem 3.3, it suffices to show that Algorithm 1 will compute the optimal solution for\nEquation (3)when fθ0is a generalized linear model. Without loss of generality, throughout this section,\nwe will be assuming that θ0[i]̸=θ0[j]for any two dimensions i̸=j. This can be easily guaranteed by an\narbitrarily small perturbation of these values without having any non-trivial impact on the model, but all of\nour results hold even without this assumption; it would just introduce some requirement for tie-breaking that\nwould make the arguments slightly more tedious. Furthermore, we use eito denote a d-dimensional unit\nvector with all zeros except for thei-th coordinate, which has a value of one.\nALGORITHM 4:Detailed Description of Algorithm 1 forβ= 1and generalized linear modelf θ0\nInput:x 0,θ0,ℓ,c,α\nOutput:x′\n1:Initializex′←x 0\n2:InitializeActive=[d]▷Set of coordinates to update\n3:fori∈[d]do\n4:ifx 0[i]̸= 0then\n5:Initializeθ′[i]←θ 0[i]−α·sgn(x 0[i])▷Initialization forθ′(the worst-case model)\n6:else\n7:if|θ 0[i]|> αthen\n8:Initializeθ′[i]←θ 0[i]−α·sgn(θ 0[i])\n9:else\n10:Active←Active\\{i}▷Remove the coordinate that cannot improveJ\n11:whileActive̸=∅do\n12:i←arg maxj∈Active |θ′[j]|▷Next coordinate to update\n13:∆←arg min∆J(x′+ ∆e i, θ′)−J(x′, θ′)▷Compute the best update for the selected coordinate\n14:ifsgn(x′[i] + ∆) = sgn(x′[i])then\n15:x′[i]←x′[i] + ∆▷Apply the update and terminate\n16:break\n17:else\n18:x′[i]←0▷Update the coordinate but only until it reaches 0\n19:if|θ 0[i]|> αthen\n20:θ′[i]←θ 0[i] +α·sgn(x 0[i])▷Modifyθ′accordingly\n21:else\n22:Active←Active\\{i}\n23:returnx′\nObservation C.1.For a fixed set of parameter values, the problem of optimizing robustness in our setting\ncan be captured as computing a recourse xraiming to minimize the value of a function J(·)whose value\ndepends only on the distance cost of x′, i.e.,∥x′−x0∥1, and its inner product with an adversarially chosen\nθ′∈Θα. Formally, our goal is to compute a recoursex rsuch that:\nxr∈arg min\nx′∈Xmax\nθ′∈ΘαJ(∥x′−x0∥1, x′·θ′).\nAlso,J(·)is a linear increasing function of∥x′−x0∥1and a convex decreasing function ofx′·θ′.\n18\n\nObservation C.1 provides an alternative interpretation of the problem: by choosing a recourse x′, we suffer\na cost ∥x′−x0∥1and the adversary then chooses a θ′aiming to minimize the value of the inner product x′·θ′.\nThis implies that for a given x′, a choice of θ′is not optimal for the adversary unless it minimizes this inner\nproduct. Also, it implies that among all choices of x′with the same cost ∥x′−x0∥1, the optimal one has to\nmaximize the inner product x′·θ′with the adversarially chosen θ′. We use this fact to prove that a recourse\nx′is not a robust choice by providing an alternative recourse with the same cost and a greater dot product.\nOur first lemma provides additional structure regarding the optimal adversarial choice in response to any\ngiven recoursex.\nLemma C.2.For any recourse x, the adversarial response θ′=arg max θ∈ΘαJ(x, θ)is such that θ′[i] =\nθ0[i] +αfor each dimension isuch that x[i]<0and θ′[i] =θ0[i]−αfor each dimension isuch that x[i]>0.\nFor any dimension iwithx[i] = 0we can without loss of generality assume that θ′[i]∈ {|θ 0[i] +α|,|θ 0[i]−α|}.\nProof.For any dimension iwith x[i] = 0, it is easy to verify that no matter what the value of θ′[i]is, the\ncontribution of x[i]·θ′[i]to the inner product x·θ′is zero, so we can indeed without loss of generality\nassume that θ′[i]∈ {|θ 0[i] +α|,|θ 0[i]−α|}. Now, assume that x[i]<0, yet θ′[i]< θ0[i] +α, and consider an\nalternative response θ′′such that θ′′[i] =θ0[i] +αandθ′′[j] =θ′[j]for all other dimensions j̸=i. Clearly,\nθ′′∈Θα, since |θ′′[i]−θ0[i]|=αand|θ′′[j]−θ0[j]| ≤αfor all other dimensions j̸=ias well, by the fact\nthat θ′∈Θα. Therefore, it suffices to prove that x·θ′′< x·θ′, as this would contradict the fact that\nθ′= arg max θ∈ΘαJ(x, θ). To verify that this is indeed the case, note that\nx·θ′−x·θ′′=x[i]·θ′[i]−x[i]·θ′′[i]\n=x[i]·(θ′[i]−θ′′[i])\n>0,\nwhere the first equation use the fact that θ′andθ′′are identical for all dimensions except iand the inequality\nuses the fact that x[i]<0and θ′[i]< θ′′[i]. A symmetric argument can be used to show that θ′[i] =θ0[i]−α\nfor each dimensionisuch thatx[i]>0.\nLemma C.2 shows that for any recourse x, an adversarial response that minimizes x·θ′isθ′=θ0−α·sgn (x).\nOur next lemma shows how the adversarial response to the initial point x0, (i.e., θ0−α·sgn (x0)) determines\nthe direction toward which each dimension ofx 0should be changed (if at all).\nLemma C.3.For any optimal recourse xr∈arg min x′∈Xmax θ′∈ΘαJ(x′, θ′)and every coordinate i, it must\nbe that xrraises the value of the i-th dimension only if the adversary’s best response to its original value\nis positive, and it lowers it only if the adversary’s best response to its original value is negative. Using\nLemma C.2, we can formally define this as:\nxr[i]> x 0[i]only ifθ 0[i]−α·sgn(x 0[i])>0\nxr[i]< x 0[i]only ifθ 0[i]−α·sgn(x 0[i])<0.\nProof.Assume that for some dimension iwe have xr[i]> x0[i]even though θ0[i]−α·sgn (x0[i])<0, and\nletx′be the recourse such that x′[i] =x0[i]while x′[j] =xr[j]for all other coordinates, j̸=i. Ifθ∗is the\nadversary’s best response to xrandθ′is the adversary’s best response to x′, then the difference between the\ninner product ofx′·θ′andx r·θ∗is:\nx′·θ′−xr·θ∗=x′[i]·θ′[i]−x r[i]·θ∗[i]\n=x0[i]·(θ 0[i]−α·sgn(x 0[i]))−x r[i]·θ∗[i]\n≥x0[i]·(θ 0[i]−α·sgn(x 0[i]))−x r[i]·(θ 0[i]−α·sgn(x 0[i]))\n= (x 0[i]−x r[i])·(θ 0[i]−α·sgn(x 0[i]))\n>0,\n19\n\nwhere the first equation uses the fact that x′[j] =xr[j]for all j̸=i, the second equation uses the fact that\nx′[i] =x0[i]and the fact that the adversary’s best response to x0[i]isθ0[i]−α·sgn (x0[i]), and the subsequent\ninequality uses the fact that the product xr[i]·θ∗[i]is at most xr[i]·(θ0[i]−α·sgn (x0[i]))since the adversary’s\ngoal is to minimize this product and adversary’s best response to xr[i]will do at least as well as the best\nresponse tox 0[i](which is a feasible, even if sub-optimal, response for the adversary).\nWe have shown that the inner product achieved by x′would be greater than that of xr, while the cost of\nx′is also strictly less than xr, since x′keeps the i-th coordinate unchanged. Therefore, max θ′∈ΘαJ(x′, θ′)<\nmax θ′∈ΘαJ(xr, θ′), contradicting the assumption that xr∈arg min x′∈Xmax θ′∈ΘαJ(x′, θ′). A symmetric\nargument leads to a contradiction if we assume thatx r[i]< x 0[i]even thoughθ 0[i]−α·sgn(x 0[i])>0.\nWe now prove a lemma regarding the sequence of |θ′[i]|values of the dimensions that the while loop of\nAlgorithms 4 changes.\nLemma C.4.Let jkdenote the dimension chosen in line 12 of Algorithm 4 during the k-th execution of its\nwhile-loop, and let vkdenote the value of |θ′[jk]|at a point in time (note that θ′changes over time). The\nsequence ofv kvalues are decreasing withk.\nProof.Note that in the k-th iteration of the while-loop, line 12 of Algorithm 4 chooses jkso that jk=\narg max j∈Active |θ′[j]|, based on the values of θ′at the beginning of that iteration. As a result, if θ′remains\nthe same throughout the execution of the algorithm (which would happen ifsgn(x r) = sgn(x 0), i.e., if none\nof the recourse coordinates changes from positive to negative or vice versa), then the lemma is clearly true.\nOn the other hand, if the recourse “flips signs” for some dimension i, i.e., sgn(xr[i])̸=sgn(x0[i]), this could\nlead to a change of the value of θ′[i]. Specifically, as shown in Lemma C.2 and implemented in line 20 of the\nalgorithm, the adversary changes θ′[i]toθ0[i] +α·sgn (x0[i]). If that transition causes the sign of θ′[i]to\nchange, then dimension ibecomes inactive and the algorithm will not consider it again in the future. If the\nsign of θ′[i]remains the same, then we can show that its absolute value would drop after this change, so even\nif it is considered in the future, it would still satisfy the claim of this lemma. To verify that its absolute value\ndrops, assume that x0[i]>0, suggesting that the algorithm has so far lowered its value to 0, which would\nonly happen if θ0[i]<0(otherwise, this change would be decreasing the inner product). Since x0[i]>0, the\nnew value of θ′[i]is equal to θ0[i] +α, and since this remains negative, like θ0[i], we conclude that its absolute\nvalue decreased. A symmetric argument can be used for the case wherex 0[i]<0.\nWe are now ready to prove our main theoretical result (the proof of Theorem 3.3), showing that Algorithm 4\nalways returns an optimal robust recourse.\nProof of Theorem 3.3. To prove the optimality of the recourse xrreturned by Algorithm 4, i.e., the fact that\nxr∈arg min x′∈Xmax θ′∈ΘαJ(x′, θ′), we assume that this is false, i.e., that there exists some other recourse\nx∗∈arg min x′∈Xmax θ′∈ΘαJ(x′, θ′)such that max θ′∈ΘαJ(x∗, θ′)<max θ′∈ΘαJ(xr, θ′), and we prove that\nthis leads to a contradiction.\nNote that since x∗∈arg min x′∈Xmax θ′∈ΘαJ(x′, θ′), it must satisfy Lemma C.3. Also, note that the way\nthat Algorithm 4 generates xralso satisfies the conditions of Lemma C.3 (the choice of∆in line 13 would\nnever lead to a recourse of higher cost without improving the inner product), so we can conclude that if x∗\nandx 0were to change the same coordinate they would both do so in the same direction, i.e.,\nsgn(x∗[i]−x 0[i]) = sgn(x r[i]−x 0[i]).\nHaving established that for every coordinate ithe values of x∗[i]and xr[i]will either both be at most\nx0[i]or both be at least x0[i], the rest of the proof performs a case analysis by comparing how far from x0[i]\neach one of them moves:\n•Case 1: ∥x∗−x0∥1=∥xr−x0∥1.Since x∗̸=xr, it must be that |x∗[i]−x0[i]|>|x r[i]−x0[i]|for\nsome iand|x∗[j]−x0[j]|<|x r[j]−x0[j]|for some j. To get a contradiction for this case as well, we\nwill consider an alternative recourse x′that is identical to x∗except for dimensions iandj, each of\n20\n\nwhich is moved δcloser to the values of xr[i]and xr[j], respectively, for some arbitrarily small constant\nδ >0. Formally,\nx′[i] =x∗[i] +δ·sgn(x r[i]−x∗[i])andx′[j] =x∗[j] +δ·sgn(x r[j]−x∗[j]).\nNote thatx∗andx′both have the same price since they only differ iniandjand\n|x∗[i]−x 0[i]|+|x∗[j]−x 0[j]|=|x′[i]−x 0[i]|+δ+|x′[j]−x 0[j]| −δ\n=|x′[i]−x 0[i]|+|x′[j]−x 0[j]|.\nWe let δbe small enough so that the adversary’s response to x∗andx′is the same; for this to hold it\nis sufficient that a value of x∗that is strictly positive does not become strictly negative in x′, or vice\nversa. If we letθ′denote this adversary, then we have\nx′·θ′−x∗·θ′=|(x′[j]−x∗[j])·θ′[j]| − |(x′[i]−x∗[i])·θ′[i]|\n=δ· |θ′[j]| −δ· |θ′[i]|\n=δ·(|θ′[j]| − |θ′[i]|),\nwhere the first equality uses the fact that x∗andx′differ only on iandj, and the fact that if we\nreplace recourse x∗with x′, then the change of δon the j-th coordinate increases the distance from\nx0[j]and thus increases the inner product, while the change of δon the i-th coordinate decreases the\ndistance from x0[i]and thus decreases the inner product. The second equality uses the fact that the\nchange on both coordinatesiandjis equal toδ.\nTo conclude with a contradiction, it suffices to show that |θ′[j]|>|θ′[i]|, as this would imply x′·θ′> x∗·θ′,\ncontradicting the fact that x∗∈arg min x′∈Xmax θ′∈ΘαJ(x′, θ′), since x′would require the same cost\nasx∗but it would yield a greater inner product. We consider three possible scenarios:i)If Algorithm 4\nin line 12 chose to change dimension ifacing adversary θ′[i]before considering dimension jand\nadversary θ′[j], then the fact that |x∗[i]−x0[i]|>|x r[i]−x0[i]|implies that the algorithm did not\nchange coordinate ias much as x∗and it must have terminated after that via line 16; this would\nsuggest that dimension jand adversary θ[j]would never be reached after that, contradicting the\nfact that |x∗[j]−x0[j]|<|x r[j]−x0[j]|.ii)If Algorithm 4 in line 12 chose to change dimension j\nfacing adversary θ′[j]and later on also considered dimension iand adversary θ′[i], then Lemma C.4\nsuggests that |θ′[j]|>|θ′[i]|, once again leading to a contradiction. Finally,iii)if Algorithm 4 in line 12\nchose to change dimension jfacing adversary θ′[j]and never ended up considering dimension ieven\nthough |θ′[j]|<|θ′[i]|, this suggests that iwas removed from theActiveset during the execution\nof the algorithm, which implies that xr[i] = 0and |θ0[i]< α, so moving further away from x0[i]\nwould actually hurt the inner product because the adversary can flip the sign of θ′[i]via a change of\nα. The fact that x∗actually moved dimension ifurther away then again contradicts the fact that\nx∗∈arg min x′∈Xmax θ′∈ΘαJ(x′, θ′).\n•Case 2: ∥x∗−x0∥1<∥x r−x0∥1.In this case, we can infer that for some iwe have |x∗[i]−x0[i]|<\n|xr[i]−x0[i]|, i.e., x∗determined that the increase of the inner product achieved by moving x∗[i]further\naway from x0[i]and closer to xr[i]was not worth the cost suffered by this increase. However, note that\nas we discussed in Observation C.1, J(·)is a decreasing function of the inner product. Also note that,\nsince Algorithm 4 changes a coordinate of the recourse only if it increases the inner product, there must\nbe some point in time during the execution of the algorithm when the inner product of x′·θ′was at least\nas high as the inner product of x∗with the adversarial response to x∗. Nevertheless, line 13 determined\nthat this change would decrease the objective value J(·). If we specifically consider the last dimension j\nchanged by the algorithm, using Lemma C.4, we can infer that the value of |θ′[j]|at the time of this\nchange was less than the value of |θ′[i]|for the dimension isatisfying |x∗[i]−x0[i]|<|x r[i]−x0[i]|; this\nis due to the fact that the algorithm chose to change iweakly earlier than j. As a result, since line 13\ndetermined that the increase of cost was outweighed by the increase in the inner product even though\n21\n\n|θ′[j]| ≤ |θ′[i]|, the inner product is greater, and J(·)is convex in the latter, this implies that increasing\nthe value of x∗[i]would also decrease the objective, thus leading to a contradiction of the fact that\nx∗∈arg min x′∈Xmax θ′∈ΘαJ(x′, θ′).\n•Case 3: ∥x∗−x0∥1>∥x r−x0∥1.This case is similar to the one above, but rather than arguing that\nx∗missed out on further changes that would have led to an additional decrease of the objective, we\ninstead argue that x∗went too far with the changes it made. Specifically, there must be some isuch\nthat|x∗[i]−x0[i]|>|x r[i]−x0[i]|, i.e., x∗determined that the increase of the inner product achieved\nby moving x∗[i]further away from x0[i]than xr[i]did was worth the cost suffered by this increase.\nSince the cost of x∗is greater than the cost of xr, it must be the case that its inner product is greater.\nTherefore, line 13 of the algorithm determined that moving xr[i]further away from x0[i]would not lead\nto an improvement of the objective even for a smaller inner product. Once again, the convexity of J(·)\nwith respect to the inner product combined with the aforementioned facts implies that this increase\nmust have hurtx∗as well, leading to a contradiction.\nD Omitted Details from Section 4\nIn this section, we provide additional results and analysis that were omitted from Section 4 due to space\nconstraints. In Section D.1, we provide additional details on the running time of our algorithm as well as\nwhat values were chosen for some of the hyperparameters. Section D.2 provides error bars for the robustness\nconsistency tradeoffs generated by our algorithm. Section D.3 details how parameter changes affect our results.\nSection D.4 studies the effect of post-processing and actionability of the generated recourse on performance.\nIn Section D.5, we provide baselines for our smoothness analysis in Section 4.1. Section D.6 studies the\ntrade-off between cost and validity when there is uncertainty in the value of α. Finally, Section D.7 provides\nadditional results for a larger dataset.\nModel Dataset λ\nLRSynthetic Data 1.0\nGerman Credit Data 0.5 - 0.7\nSmall Business Data 1.0\nNNSynthetic Data 1.0\nGerman Credit Data 0.1 - 0.2\nSmall Business Data 1.0\nTable 1: λthat maximize the validity with respect to the original model θ0for each dataset. The other\nchoices of parameters are mentioned in Section D.1.\nD.1 Additional Experimental Details\nThe experiments were conducted on two laptops: an Apple M1 Pro and a 2.2 GHz 6-Core Intel Core i7.\nAverage runtime (across datasets/model) to generate a robust recourse for Algorithm 1, ROAR [ 70], and\nRBR [56] are 0.001, 1,0, and 40 seconds, respectively. The total runtime of Algorithm 1 to generate Figure 1\nwas 45-60 minutes, depending on the subfigure.\nIn our robustness versus consistency experiments in Section 4.1, we choose α= 0.5and find the λthat\nmaximizes the validity with respect to the original model θ0in each round of cross-validation. The range of λ\nvalues found to maximize theθ 0validity for each setting is reported in Table 1.\n22\n\nIn our experiment on smoothness in Section 4.1, we created the future model using a modified dataset\nsimilar to [ 70]. To produce the altered synthetic data, we employed the same method outlined in Section 4,\nbut we changed the mean of the Gaussian distribution for class 0. The new distribution is x∼N (µ′\n0,Σy),\nwhere µ′\n0is equal to µ0+[α,0]T, while µ′\n1remained unchanged at µ1. We used this new distribution to\nlearn a model for the correct prediction. The German credit dataset [ 34] is available in two versions, with the\nsecond one [ 27] fixing coding errors found in the first. This dataset exemplifies a shift due to data correction.\nWe used the second dataset to learn the model for the correct prediction. The Small Business Administration\ndataset [ 52], which contains data on 2,102 small business loans approved in California from 1989 to 2012,\ndemonstrates temporal shifts. We split this dataset into two parts: data points before 2006 form the original\ndataset, while those from 2006 onwards constitute the shifted dataset. We used the shifted dataset to learn a\nmodel for the correct prediction.\nTo generate the predictions in our smoothness experiment in Section 4.1, we define ϵas half the distance\nbetween the original model θ0, and the shifted model ˆθ∗, which we use as the correct prediction for the future\nmodel. Perturbations of ±ϵand±2ϵare then applied to each dimension of the ˆθ∗. For linear models, we use\nϵ= 0.12for the Synthetic dataset, ϵ= 0.16for the German dataset, and ϵ= 0.43for the Small Business\nAdministration dataset. For non-linear models, the amount of perturbation is determined by each instance in\nthe dataset by using the LIME approximation to provide recourse. More details can be found in our code. In\nall cases, the perturbed values are clamped to ensure they remain within the α= 1in terms of L1distance\nfrom the original modelθ 0.\nIn our cost versus worst-case validity experiments in Section 4.2, motivated by reasons for data shift and\ngiven access to different versions of the datasets, we follow prior work [ 56,70] and compute a model on a\nshifted version of the dataset and measure validity against this new model. For the synthetic dataset, the\nshifted data is achieved by changing the mean and variance in the data distribution. For the German Credit\ndataset, the data shift is due to data correction. For the Small Business Administration dataset, the shift is\ntemporal. See [70] for more details.\nD.2 Error Bars for Robustness-Consistency Trade-off\nThis section provides additional details about the experiments regarding the trade-off between robustness and\nconsistency. In particular, we replicate the performance of Algorithm 1, which is used to generate Figure 1,\nbut also include error bars. These error bars are calculated for the robustness (Figure 4) and consistency\ncosts (Figure 5) when averaging is done over all the data points in the test set that require recourse as well\nas the folds.\nD.3 Effect of the Parameters\nIn the experiments on the trade-off between robustness and consistency in Section 4.1, we used α= 0.5and\naλthat maximizes the validity of recourse with respect to this α. In this section, we see how varying α\ncan affect the results. In particular, in Figures 6 and 7, we replicated the trade-offs of our algorithm, which\nis presented in Figure 1 in Section 4.1 with α= 0.1and α= 1, respectively. Again, for each choice of α,\nwe selected a λthat maximizes the validity of recourse with respect to this α. We generally observe that\nincreasingαincreases both the robustness and consistency costs.\nD.4 Actionable Features and Post-processing\nSimilar to prior work [ 56,70], our algorithm does not directly handle categorical features, and the recourse\nprovided by our algorithm can violate potential constraints or restrictions that might exist on some features.\nIn this section, we study how the performance of our algorithm can be affected under these situations.\nIn particular, in this section, we repeat the experiments on robustness-consistency trade-off from Section 4.1\nbut post-process the recourses generated by Algorithm 1 to satisfy categorical and actionable features. In\nparticular, we first remove all non-actionable or immutable features [ 28] from the dataset, compute the\n23\n\n0 0.05 0.1 0.15 0.2 0.2500.511.5(0.04, 0.01)\n(0.14, 0.01)\n(0.06, 0.01)\n(0.23, 0.01)\n(0.05, 0.01)\nConsistencyRobustness(a) Synthetic Dataset, Logistic Regression\n0 0.05 0.1 0.15 0.200.511.52 (18.99, 17.64)\n(16.52, 17.64)\n(12.93, 17.64)\n(15.59, 17.64)\n(15.97, 17.64)\nConsistencyRobustness (b) Synthetic Dataset, Neural Network\n0 0.2 0.4 0.6 0.8 1 1.200.10.20.30.40.50.60.7\n(0.05, 0.39)\n(0.03, 0.39)\n(0.05, 0.39)\n(0.04, 0.39)\n(0.04, 0.39)\nConsistencyRobustness\n(c) German Credit Dataset, Logistic Regression\n0 0.05 0.1 0.15 0.2 0.2500.20.40.60.811.21.41.6\n(0.24, 0.19)\n(0.27, 0.19)\n(0.22, 0.19)\n(0.23, 0.19)\n(0.20, 0.19)\nConsistencyRobustness (d) German Credit Dataset, Neural Network\n0 0.5 1 1.5 2 2.5012345(1.53, 2.21)\n(1.34, 2.21)\n(1.70, 2.21)\n(1.19, 2.21)\n(1.43, 2.21)\nConsistencyRobustness\n(e) Small Business Dataset, Logistic Regression\n0 0.5 1 1.5 2012345678\n(2.99, 1.72)\n(2.87, 1.72)\n(1.02, 1.72)\n(1.37, 1.72)\n(2.40, 1.72)\nConsistencyRobustness (f) Small Business Dataset, Neural Network\nFigure 4: The trade-off between robustness and consistency for α= 0.5with error bars for robustness: logistic\nregression (left) and neural network (right). Rows correspond to datasets: synthetic (top), German (middle),\nand Small Business (bottom). In each subfigure, each curve shows the trade-off for different predictions. The\nrobustness and consistency of ROAR solutions at β= 1are mentioned in parentheses and depicted by stars.\nMissing stars are outside the range of the figure.\n24\n\n0 0.05 0.1 0.15 0.2 0.2500.20.40.60.811.21.41.61.8\n(0.04, 0.01)\n(0.14, 0.01)\n(0.06, 0.01)\n(0.23, 0.01)\n(0.05, 0.01)\nConsistencyRobustness(a) Synthetic Dataset, Logistic Regression\n0 0.05 0.1 0.15 0.2 0.2500.511.52\n(18.99, 17.64)\n(16.52, 17.64)\n(12.93, 17.64)\n(15.59, 17.64)\n(15.97, 17.64)\nConsistencyRobustness (b) Synthetic Dataset, Neural Network\n0 0.2 0.4 0.6 0.8 1 1.200.050.10.150.20.250.30.350.4 (0.05, 0.39)\n(0.03, 0.39)\n(0.05, 0.39)\n(0.04, 0.39)\n(0.04, 0.39)\nConsistencyRobustness\n(c) German Credit Dataset, Logistic Regression\n0 0.05 0.1 0.15 0.2 0.25 0.300.20.40.60.811.2 (0.24, 0.19)\n(0.27, 0.19)\n(0.22, 0.19)\n(0.23, 0.19)\n(0.20, 0.19)\nConsistencyRobustness (d) German Credit Dataset, Neural Network\n0 0.5 1 1.5 2 2.5012345(1.53, 2.21)\n(1.34, 2.21)\n(1.70, 2.21)\n(1.19, 2.21)\n(1.43, 2.21)\nConsistencyRobustness\n(e) Small Business Dataset, Logistic Regression\n0 0.5 1 1.5 2 2.50123456(2.99, 1.72)\n(2.87, 1.72)\n(1.02, 1.72)\n(1.37, 1.72)\n(2.40, 1.72)\nConsistencyRobustness (f) Small Business Dataset, Neural Network\nFigure 5: The trade-off between robustness and consistency for α= 0.5with error bars for consistency: logistic\nregression (left) and neural network (right). Rows correspond to datasets: synthetic (top), German (middle),\nand Small Business (bottom). In each subfigure, each curve shows the trade-off for different predictions. The\nrobustness and consistency of ROAR solutions at β= 1are mentioned in parentheses and depicted by stars.\nMissing stars are outside the range of the figure.\n25\n\n0 0.02 0.04 0.06 0.08 0.100.050.10.15\nConsistencyRobustness(a) Synthetic Dataset, Logistic Regression\n0 0.05 0.1 0.1500.050.10.150.20.250.3\nConsistencyRobustness (b) Synthetic Dataset, Neural Network\n0 0.02 0.04 0.06 0.08 0.1 0.1200.010.020.030.040.050.060.070.08\nConsistencyRobustness\n(c) German Credit Dataset, Logistic Regression\n0 0.02 0.04 0.06 0.0800.050.10.15\nConsistencyRobustness (d) German Credit Dataset, Neural Network\n0 0.1 0.2 0.3 0.4 0.5 0.600.20.40.60.811.2\nConsistencyRobustness\n(e) Small Business Dataset, Logistic Regression\n0 0.02 0.04 0.06 0.08 0.1 0.12 0.1400.050.10.150.20.250.30.35\nConsistencyRobustness (f) Small Business Dataset, Neural Network\nFigure 6: The trade-off between robustness and consistency for α= 0.1: logistic regression (left) and\nneural network (right). Rows correspond to datasets: synthetic (top), German (middle), and Small Business\n(bottom). In each subfigure, each curve shows the trade-off for different predictions as mentioned in the\nlegend.\n26\n\n0 0.02 0.04 0.06 0.08 0.100.511.52\nConsistencyRobustness(a) Synthetic Dataset, Logistic Regression\n0 0.5 1 1.5 2 2.502468101214\nConsistencyRobustness (b) Synthetic Dataset, Neural Network\n0 0.5 1 1.500.10.20.30.40.5\nConsistencyRobustness\n(c) German Credit Dataset, Logistic Regression\n0 0.5 1 1.5 2 2.5 300.511.522.5\nConsistencyRobustness (d) German Credit Dataset, Neural Network\n0 0.5 1 1.5 2 2.5 3 3.5012345\nConsistencyRobustness\n(e) Small Business Dataset, Logistic Regression\n0 0.5 1 1.5 2 2.5 3 3.5012345678\nConsistencyRobustness (f) Small Business Dataset, Neural Network\nFigure 7: The trade-off between robustness and consistency for α= 1: logistic regression (left) and neural\nnetwork (right). Rows correspond to datasets: synthetic (top), German (middle), and Small Business\n(bottom). In each subfigure, each curve shows the trade-off for different predictions as mentioned in the\nlegend.\n27\n\nrecourses using Algorithm 1, and then project the recourse to the feasible region to satisfy the constraints\nposed by categorical features.\nSince the synthetic dataset does not include any categorical or immutable features, we exclude it from our\nanalysis. The robustness-consistency trade-off for both logistic regression and neural network models on the\nGerman Credit dataset is depicted in Figure 8. Comparing the left panel with no post-processing to the right\npanel with post-processing, we observe that the achievable trade-off after post-processing becomes slightly\nworse (e.g., consistency values do not reach 0 in Figure 8d), but the degradation in performance is generally\nvery small.\n0 0.2 0.4 0.6 0.8 1 1.200.050.10.150.20.250.30.350.4\nConsistencyRobustness\n(a) Logistic Regression\n0 0.2 0.4 0.6 0.8 1 1.200.050.10.150.20.250.30.350.4\nConsistencyRobustness (b) Logistic Regression, Actionable\n0 0.05 0.1 0.15 0.2 0.25 0.300.20.40.60.811.2\nConsistencyRobustness\n(c) Neural Network\n0 0.05 0.1 0.15 0.2 0.25 0.300.20.40.60.81\nConsistencyRobustness (d) Neural Network, Actionable\nFigure 8: The trade-off between robustness and consistency for α= 0.5for German Credit Dataset: logistic\nregression (top) and neural network (bottom). The left panel corresponds to recourses provided by Algorithm 1,\nwhile the right panel corresponds to recourses provided to only actionable features and post-processed for\ncategorical features. In each subfigure, each curve shows the trade-off for different predictions.\nD.5 Baselines for Smoothness Results\nIn this section, we revisit the smoothness results in Section 4.1 by including a baseline. For any β, the\noptimization problem in Equation 7 can be solved by the type of gradient-based techniques that are common\nin adversarial training [ 51]. Since ROAR is one such algorithm, we can modify it by changing its objective\nfunction and use it as a baseline to Algorithm 1.\n28\n\nThe results are presented in Figure 9. In each subfigure, the left panel represents the smoothness of\nAlgorithm 1 (exactly as it is depicted in Figure 2) and the right panel depicts the smoothness of ROAR.\nThere are a couple of interesting observations: First of all, similar to our algorithm, following ˆθ∗atβ= 0\nwill result in the lowest smoothness value, though compared to following other predictions though unlike our\nalgorithm, the smoothness in this situation can be higher than 0. On the other hand, at the other extreme,\nthe smoothness of ROAR converges for different predictions since, in this case, the prediction is ignored.\nMoreover, the smoothness of ROAR for each prediction also exhibits non-monotone behavior as a function\nofβ. However, the βat which the non-monotonicity starts to emerge is different for ROAR compared to\nour algorithm. Finally, we generally observe that the smoothness of ROAR using the same prediction and β\nis generally much worse than our algorithm (note that the Y axis has different scales in the left and right\npanels).\n0 0.2 0.4 0.6 0.8 100.010.020.030.040.050.06\n0 0.2 0.4 0.6 0.8 100.020.040.060.080.10.120.140.160.18SmoothnessAlgorithm 1 ROAR\n(a) Synthetic Dataset, Logistic Regression\n0 0.2 0.4 0.6 0.8 100.010.020.030.040.050.060.07\n0 0.2 0.4 0.6 0.8 10246810121416SmoothnessAlgorithm 1 ROAR (b) Synthetic Dataset, Neural Network\n0 0.2 0.4 0.6 0.8 100.20.40.60.81\n0 0.2 0.4 0.6 0.8 100.020.040.060.080.10.12SmoothnessAlgorithm 1 ROAR\n(c) German Credit Dataset, Logistic Regression\n0 0.2 0.4 0.6 0.8 100.050.10.150.2\n0 0.2 0.4 0.6 0.8 100.050.10.150.20.25SmoothnessAlgorithm 1 ROAR (d) German Credit Dataset, Neural Network\n0 0.2 0.4 0.6 0.8 100.20.40.60.811.21.41.61.8\n0 0.2 0.4 0.6 0.8 100.511.522.533.5SmoothnessAlgorithm 1 ROAR\n(e) Small Business Dataset, Logistic Regression\n0 0.2 0.4 0.6 0.8 100.20.40.60.81\n0 0.2 0.4 0.6 0.8 100.20.40.60.811.21.41.61.8SmoothnessAlgorithm 1 ROAR (f) Small Business Dataset, Neural Network\nFigure 9: The smoothness analysis for predictions with different accuracies: rows and columns correspond to\ndifferent datasets and models as indicated in the sub-caption. In each subfigure, each curve corresponds to a\ndifferent prediction and tracks the smoothness as a function of βfor the given prediction. In each subfigure,\nthe left corresponds to Algorithm 1 and the right corresponds to ROAR.\nD.6 Computing Robust Recourse\nIn this section, we repeat the experiments in Section 4.2 but also include comparisons with RBR [ 56]. Figure 10\ndepicts the trade-off between worst-case validity and cost for all datasets (rows) and models (columns). Since\nRBR does not have the same parameters as our algorithm and ROAR, the trade-off for RBR is obtained by\n29\n\n4 4.5 5 5.5 600.20.40.60.81\nCostFuture Validity(a) Synthetic Dataset, Logistic Regression\n3.5 4 4.5 500.20.40.60.81\nCostFuture Validity (b) Synthetic Dataset, Neural Network\n0 1 2 3 4 500.20.40.60.81\nCostFuture Validity\n(c) German Credit Dataset, Logistic Regression\n0.5 1 1.5 2 2.500.20.40.60.81\nCostFuture Validity (d) German Credit Dataset, Neural Network\n2 2.5 3 3.5 4 4.5 5 5.500.20.40.60.81\nCostFuture Validity\n(e) Small Business Dataset, Logistic Regression\n2 3 4 5 6 700.20.40.60.81\nCostFuture Validity (f) Small Business Dataset, Neural Network\nFigure 10: The trade-off between future validity and cost: rows and columns correspond to different datasets\nand models as indicated in the sub-caption. In each subfigure, curves show the trade-off for different algorithms\nspecified in the legend.\nreplicating their experiments and setting the ambiguity sizes to ϵ0, ϵ1∈[0,1]with increments of 0.5, and the\nmaximum recourse costδ=∥x 0−xr∥1+δ+toδ+∈[0,1]with increments of 0.2.\n30\n\nWe observe that the trade-offs for RBR are Pareto dominated by our algorithm, except for the synthetic\ndataset, where RBR achieves a high validity with a smaller cost. In addition, unlike our algorithm, RBR\nsometimes achieves very low validity, even lower than ROAR.\nD.7 Experiments on Larger Datasets\nIn this section, we provide experimental results for a much larger dataset (both in terms of the number of\ninstances and number of features) compared to the datasets in Section 4. The running time of our algorithm\nscales linearly with the number of instances for which recourse is provided. For each instance, the running\ntime of our algorithm grows linearly in the number of features since the minimization problem in Line 13 of\nour algorithm can be solved analytically. For non-linear models, the cost of approximating the model with a\nlinear function should be added to the price per instance.\nWe use the ACSIncome-CA [ 19] dataset for experiments in this section. This dataset originally consisted\nof 195,665 data points and 10 features, 7 of which are categorical and have been one-hot encoded. However,\nto lower the runtime, we subsampled the dataset to include 50,000 data points and removed the categorical\nfeature \"occupation (OCCP)\", as it contains more than 500 different occupations. This resulted in more than\n250 features after one-hot encoding.\nFigure 11 depicts the trade-off between the cost and validity of recourse for both logistic regression and\nneural network models. The choices of parameters used for results in Figure 11 are the same as the results\nfor Figure 10 in Section 4.2. We observe that even in a dataset with a much larger number of features,\nAlgorithm 1 can generate recourses with high validity, especially for logistic regression models. Similar to\nFigure 10, achieving very high validity comes at a cost of higher implementation cost, which is higher than\nthe cost required for smaller datasets.\n0 2 4 6 8 1000.20.40.60.81\nAlg1 (λ=0.05)\nAlg1 (λ=0.1)\nAlg1 (λ=0.2)\nCostWorst Case Validity\n(a) ACS Income Dataset, Logistic Regression\n0 5 10 1500.20.40.60.81\nAlg1 (λ=0.05)\nAlg1 (λ=0.1)\nAlg1 (λ=0.2)\nCostWorst Case Validity (b) ACS Income Dataset, Neural Network\nFigure 11: The trade-off between the worst-case validity and the cost of recourse. The left panel is for the\nlogistic regression, while the right panel is for a neural network for the ACS Income dataset. In each subfigure,\neach curve shows the trade-off for different methods as mentioned in the legend.\n31",
  "textLength": 90239
}