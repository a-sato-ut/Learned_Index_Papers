{
  "paperId": "2f9aeb48ef39b7aaedc943518854dee6dc625536",
  "title": "A Model for Learned Bloom Filters and Related Structures",
  "pdfPath": "2f9aeb48ef39b7aaedc943518854dee6dc625536.pdf",
  "text": "A Model for Learned Bloom Filters and Related Structures\nMichael Mitzenmacher1\nAbstract — Recent work has suggested enhancing Bloom ﬁl-\nters by using a pre-ﬁlter, based on applying machine learning to\nmodel the data set the Bloom ﬁlter is meant to represent. Here\nwe model such learned Bloom ﬁlters , clarifying what guarantees\ncan and cannot be associated with such a structure.\nI. I NTRODUCTION\nAn interesting paper, “The Case for Learned Index Struc-\ntures” [5], recently appeared, suggesting that standard index\nstructures and related structures, such as Bloom ﬁlters, could\nbe improved by using machine learning to develop what they\nauthors dub learned index structures. Here we aim to provide\na more formal model for their variation of a Bloom ﬁlter,\nwhich they call a learned Bloom ﬁlter , and clarify what\nfeatures it does and does not have. A key issue is that,\nunlike standard Bloom ﬁlters, the performance of learned\nBloom ﬁlters depends on both the data set the Bloom ﬁlter\nrepresents and the set membership queries made. Because of\nthis, the types of guarantees offered by learned Bloom ﬁlters\ndiffer signiﬁcantly from those of standard Bloom ﬁlters. We\nformalize this issue below.\nThe performance of learned Bloom ﬁlters will therefore\nnecessarily be application dependent. Indeed, there may be\napplications where they offer signiﬁcant advantages over\nstandard Bloom ﬁlters. We view this work as a beginning step\nin laying out a theoretical framework to understand learned\nBloom ﬁlters and related structures, with a particular goal\nof pointing out issues that may affect their suitability for\nreal-world applications.\nIn what follows, we start by reviewing standard Bloom\nﬁlters and variants, following the framework provided by\nthe reference [2]. We then describe learned Bloom ﬁlters,\nand attempt to provide a model which highlights both their\npotential beneﬁts and their limitations. In particular, we ﬁnd\nlearned Bloom ﬁlters appear most useful when the query\nstream can be modelled as coming from a ﬁxed distribution,\nwhich can be sampled during the construction.\nII. R EVIEW : BLOOM FILTERS\nA. Deﬁnition of the Data Structure\nA Bloom ﬁlter for representing a set S=fx1;x2;:::;xng\nofnelements corresponds to an array of mbits, and\nuseskindependent hash functions h1;:::;hkwith range\nf0;:::;m\u00001g. Typically we assume that these hash func-\ntions are perfect; that is, each hash function maps each item\nin the universe to independently and uniformly to a number\n1School of Engineering and Applied Sciences, Harvard University. Sup-\nported in part by NSF grants CCF-1563710, CCF-1535795, and CCF-\n1320231. This work was done while visiting Microsoft Research.inf0;:::;m\u00001g. Initially all array bits are 0. For each\nelementx2S, the array bits hi(x)are set to 1 for 1\u0014i\u0014k;\nit does not matter if some bit is set to 1 multiple times. To\ncheck if an item yis inS, we check whether all hi(y)are\nset to 1. If not, then clearly yis not a member of S. If all\nhi(y)are set to 1, we conclude that yis inS, although this\nmay be a false positive . A Bloom ﬁlter does not produce\nfalse negatives.\nThere are various theoretical guarantees one can associate\nwith a Bloom ﬁlter. The simplest is the following. Let y\nbe an element of the universe such that y =2S, whereyis\nchosen independently of the hash functions used to create the\nﬁlter. A useful way to think of this is that an adversary can\nchoose any element ybefore the Bloom ﬁlter is constructed;\nthe adversary has no knowledge of the hash functions used,\nbut may know the set S. Let\u001abe the fraction of bits set to\n1 after the elements are hashed. Then\nPr(yyields a false positive ) =\u001ak:\nFurther, probabilistic analysis shows both that\nE[\u001a] = 1\u0000\u0012\n1\u00001\nm\u0013kn\n\u00191\u0000e\u0000kn=m;\nand that\nPr(j\u001a\u0000E[\u001a]j\u0015\r)\u0014e\u0000\u0002(\r2m)\nin the typical regime where m=n andkare constant. That is,\n\u001ais, with high probability, very close to its easily calculable\nexpectation, and thus we know (up to very small random\ndeviations, and with high probability over instantiations of\nthe Bloom ﬁlter) what the probability is than an element\nywill be a false positive. Because of this, it is usual to\ntalk about the false positive probability of a Bloom ﬁlter;\nin particular, it is generally referred to as though it is a\nconstant depending on the ﬁlter parameters, although it is\na random variable, because it is tightly concentrated around\nits expectation.\nMoreover, given a set of distinct query elements Q=\nfy1;y2;:::;yqgwithQ\\S=;chosen a priori before the\nBloom ﬁlter is instantiated, the fraction of false positives over\nthese queries will similarly be concentrated near \u001ak. Hence\nwe may talk about the false positive rate of a Bloom ﬁlter,\nwhich (when the query elements are distinct) is essentially\nthe same as the false positive probability. (When the query\nelements are not distinct, the false positive rate may vary\nsigniﬁcantly, depending on on the distribution of the number\nof appearances of elements and which ones yield false\npositives; we focus on the distinct item setting here.) In\nparticular, the false positive rate is a priori the same for anyarXiv:1802.00884v1  [cs.DS]  3 Feb 2018\n\npossible query set Q. Hence one approach to ﬁnding the false\npositive rate of a Bloom ﬁlter empirically is simply to test a\nrandom set of query elements (that does not intersect S) and\nﬁnd the fraction of false positives. Indeed, it does not matter\nwhat setQis chosen, as long as it is chosen independently\nof the hash functions.\nWe emphasize that, as we discuss further below, the term\nfalse positive rate often has a different meaning in the\ncontext of learning theory applications. This difference of\nterminology is a possible point of confusion in [5], and care\nmust be taken in understanding how the term is being used.\nB. Additional Bloom Filter Beneﬁts and Limitations\nFor completeness, we relate some of the other beneﬁts and\nlimitations of Bloom ﬁlters. More details can be found in [2].\nWe have assumed in the above analysis that the hash\nfunctions are fully random. As fully random hash functions\nare not practically implementable, there are often questions\nrelating to how well the idealization above matches the real\nworld for speciﬁc hash functions. In practice, however, the\nmodel of fully random hash functions appears reasonable in\nmany cases; see [3] for further discussion on this point.\nIf an adversary has access to the hash functions used,\nor to the ﬁnal Bloom ﬁlter, it can ﬁnd elements that lead\nto false positives. One must therefore ﬁnd other structures\nfor adversarial situations. A theoretical framework for such\nsettings is developed in [8]. Variations of Bloom ﬁlters,\nwhich adapt to false positives and prevent them in the future,\nare described in [1], [7]; while not meant for adversarial\nsituations, they prevent repeated false positives with the same\nelement.\nOne of the key advantages of a standard Bloom ﬁlter\nis that it is easy to insert an element (possibly slightly\nchanging the false positive probability), although one cannot\ndelete an element without using a more complex structure,\nsuch as a counting Bloom ﬁlter. However, there are more\nrecent alternatives to the standard Bloom ﬁlter, such as the\ncuckoo ﬁlter [4], which can achieve the same or better\nspace performance as a standard Bloom ﬁlter while allowing\ninsertions and deletions. If the Bloom ﬁlter does not need\nto insert or delete elements, a well-known alternative is to\ndevelop a perfect hash function for the data set, and store\na ﬁngerprint of each element in each corresponding hash\nlocation (see, e.g., [2] for further explanation); this approach\nreduces the space required by approximately 30%.\nIII. L EARNED BLOOM FILTERS\nA. Deﬁnition of the Data Structure\nWe now consider the learned Bloom ﬁlter construction\noffered in [5]. We are given a set of positive keys Kthat\ncorrespond to set to be held in the Bloom ﬁlter – that is,\nKcorresponds to the set Sin the previous section. We are\nalso given a set of negative keys Ufor training. We then\ntrain a neural network with D=f(xi;yi= 1)jxi2\nKg[f (xi;yi= 0)jxi2Ug ; that is, they suggest using aneural network on this binary classiﬁcation task to produce\na probability, based on minimizing the log loss function\nL=X\n(x;y)2Dylogf(x) + (1\u0000y) log(1\u0000f(x));\nwherefis the learned model from the neural network. Then\nf(x)can be interpreted as a probability that xis a key from\nthe set. Their suggested approach is to choose a threshold \u001c\nso that iff(x)\u0015\u001cthen the algorithm returns that xis in\nthe set, and no otherwise. Since such a process may provide\nfalse negatives for some keys in Kthat havef(x)< \u001c, a\nsecondary structure – such as a smaller standard Bloom ﬁlter\nfor such keys – can be used to ensure there are no false\nnegatives, thereby matching this important characteristic of\nthe standard Bloom ﬁlter.\nIn essence, [5] suggests using a pre-ﬁlter ahead of the\nBloom ﬁlter, where the pre-ﬁlter comes from a neural\nnetwork and estimates the probability an element is in the\nset, allowing the use of a smaller Bloom ﬁlter. Performance\nimproves if the size to represent the learned function fand\nthe size of the smaller backup ﬁlter for false negatives is\nsmaller than the size of a corresponding Bloom ﬁlter with the\nsame false positive rate. While the idea of layering multiple\nﬁlters has appeared in previous work, this approach appears\nnovel. Indeed, the more typical setting is for the Bloom ﬁlter\nitself to be used as a pre-ﬁlter for some other, more expensive\nﬁltering process. Of course the pre-ﬁlter here need not come\nfrom a neural network; any approach that would estimate the\nprobability an input element is in the set could be used.\nB. Deﬁning the False Positive Probability: High Level Issues\nThe question remains how to determine or derive the\nfalse positive probability for such a structure, and how to\nchoose an appropriate threshold \u001c. One approach would be\nto empirically ﬁnd the false positive rate over a test set, and\nthis appears to be what has been done in [5]. This approach\nis, as we have stated, suitable for a standard Bloom ﬁlter,\nwhere the false positive rate is guaranteed to be close to its\nexpected value for any test set, with high probability. But as\nwe explain in the next subsection, this methodology requires\nsigniﬁcant additional assumptions in the learned Bloom ﬁlter\nsetting.\nBefore formalizing appropriate deﬁnitions, to frame the\nissue it is helpful to think of an adversarial situation, although\nas we discuss below an adversary is not strictly necessary.\nAn adversary might naturally be able to ﬁnd items for which\nPr(f(y)\u0015\u001c)is surprisingly large based on their own\nanalysis of the data, even without access to the structure\nfthat is ﬁnally determined. That is, consider the following\nintuition. The function fis designed to to take advantage\nof structure in the set K, as well as the information in the\ncollection of non-set elements U. An adversary, knowing K\nand/orUthemselves, might be able to design elements that\nare similar to the elements of K. An element similar to the\nelements ofKshould have a large f(y)value, and hence be\nmore likely to yield a false positive.\n\nMore formally, let us assume the adversary knows K\nand/orU, and chooses an element y =2 K to test. In the\nstandard Bloom ﬁlter setting, the array of bits that constitutes\nthe ﬁlter is generated from random hash values, and so\nknowing the set Stells the adversary nothing about what\nyvalue might be most likely to yield a false positive.\nAllyvalues are equivalent (as long as y =2S). In the\nlearned Bloom ﬁlter setting, knowing KandUmay give the\nadversary information about the resulting index function f,\neven if this knowledge is just that fis designed to give higher\nvalues to elements similar to Kand lower values to elements\ninU. The adversary need not even know the speciﬁc method\nused to determine f; the knowledge of the data alone may\nallow the adversary to choose a yvalue with a large expected\nf(y)value, so that Pr(f(y)\u0015\u001c)>\u000f; that is the probability\nof a false positive is larger than expected.\nWhile we have stated this problem in terms of an adver-\nsary, one does not need to posit an adversary to see that this\nsituation might arise naturally in standard data scenarios. All\nwe need to suppose is that the query set Qof elements that\none uses to query the Bloom ﬁlter are similar to the set K\nin some manner that might be captured by f, so that the\nexpected value of f(y)fory2Q skews large. (Here we\nassumeQis disjoint fromK, as we are interested in the rate\nof false positives, not true positives.)\nAn example based on ranges may be helpful. Suppose the\nuniverse of elements is the range [0;1000000) , and the setK\nof elements to store in our Bloom ﬁlter consists of a random\nsubset of elements from the range [1000;2000] , say half\nof them, and 500 other random elements from outside this\nrange. Our learning algorithm might determine that a suitable\nfunctionfisf(y)is large (say f(y)\u00191=2) for elements in\nthe range [1000;2000] and close to zero elsewhere, and then\na suitable threshold might be \u001c= 0:4. The resulting false\npositive rate will depend substantially on what elements are\nqueried. IfQconsists of elements primarily in the range\n[1000;2000] , the false positive rate will be quite high, while\nifQis chosen uniformly at random over the whole range,\nthe false positive rate will be quite low. The main point is\nthat the false positive rate, unlike in the setting of a standard\nBloom ﬁlter, is highly dependent on the query set, and as\nsuch is not well-deﬁned independently of the queries, as it\nis for a standard Bloom ﬁlter.\nIndeed, it seems plausible that in many situations, the\nquery setQmight indeed be similar to the set of elements\nK, so thatf(y)fory2Q might often be above naturally\nchosen thresholds. For example, in security settings, one\nmight expect that queries for objects under consideration\n(URLs, network ﬂow features) would be similar to the set of\nelements stored in the ﬁlter. The key here is that, unlike in the\nBloom ﬁlter setting, the false positive probability for a query\nycan depend on y, even before the “data structure”, which\nin this case corresponds to the function f, is instantiated.\nIt is worth noting, however, that the problem we point\nout here can possibly be a positive feature in other settings;\nit might be that the false positive rate is remarkably low if\nthe query set is suitable. Again, one can consider the rangeexample above where queries are uniform over the entire\nspace; the query set is very unlikely to hit the range where\nthe learned function fyields an above threshold value in\nthat setting for an element outside of K. More generally, one\nmay have query sets Qwhere the values f(y)fory2Q are\nsmaller than one might expect. The key point again remains\nthat the false positive probability is dependent on the data\nand the query in what may not be predictable ways, in sharp\ncontrast to standard Bloom ﬁlters.\nC. Deﬁning the False Positive Probability, and Analysis from\nEmpirical Data\nWe can formalize (at least partially) settings where we can\nobtain good performance from a learned Bloom ﬁlter, given\nenough data. The framework below follows standard lines,\nbut provides deﬁnitions to capture the high level ideas given\nabove. We ﬁrst formalize the construction of [5].\nDeﬁnition 1: Alearned Bloom ﬁlter on a set of positive keys\nKand negative keys Uis a function f:U![0;1]and\nthreshold\u001c, whereUis the universe possible query keys,\nand an associated standard Bloom ﬁlter B, referred to as a\nbackup ﬁlter. The backup ﬁlter is set to hold the set of keys\nfz:z2K;f(z)< \u001cg. For a query y, the learned Bloom\nﬁlter returns that y2K iff(y)\u0015\u001c, or iff(y)< \u001c and\nthe backup ﬁlter returns that y2K. The learned Bloom ﬁlter\nreturnsy =2K otherwise.\nNote that the size of a learned Bloom ﬁlter corresponds\nto size used to represent the function fand the size of the\nbackup ﬁlter B, which we denote by jfj+jBj. In cases where\nthe learned Bloom ﬁlter is effective, one expects fto have a\nsmall representation, and the number of false negatives from\nKin the backup ﬁlter to be a reasonably small fraction of\nK.\nThe learned Bloom ﬁlter as deﬁned has no false negatives,\ndue to the backup ﬁlter. We can deﬁne the false positive\nrate of a learned Bloom ﬁlter with respect to a given query\ndistribution.\nDeﬁnition 2: Afalse positive rate on a query distribution D\noverU\u0000K for a learned Bloom ﬁlter (f;\u001c;B )is given by\nPr\ny\u0018D(f(y)\u0015\u001c) + (1\u0000Pr\ny\u0018D(f(y)\u0015\u001c))F(B);\nwhereF(B)is the false positive rate of the backup ﬁlter B.\nWhile technically F(B)is itself a random variable, as dis-\ncussed previously, the false positive rate is well concentrated\naround its expectations, which depends only on the size of\nthe ﬁlterjBjand the number of false negatives from Kthat\nmust be stored in the ﬁlter, which depends on f. Hence we\nmay naturally refer to the false positive rate of the learned\nBloom ﬁlter as being determined by f,\u001c, andjBjrather\nthan onBitself. That is, where the meaning is clear, we\nmay consider the false positive rate on a query distribution\nfor a learned Bloom ﬁlter with associated (f;\u001c)to be\nPr\ny\u0018D(f(y)\u0015\u001c) + (1\u0000Pr\ny\u0018D(f(y)\u0015\u001c))E[F(B)];\nwhere the expectation E[F(B)]is meant to over instantia-\ntions of the Bloom ﬁlter with given size jBj.\n\nGiven sufﬁcient data, we can determine an empirical false\npositive rate on a test set, and use that to predict future\nbehavior. Under the assumption that the test set has the\nsame distribution as future queries, standard Chernoff bounds\nprovide that the empirical false positive rate will be close\nto the false positive rate on future queries, as both will\nbe concentrated around the expectation. In many learning\ntheory settings, this empirical false positive rate appears to\nbe referred to as simply the false positive rate; we emphasize\nthat false positive rate, as we have explained above, typically\nmeans something different in the Bloom ﬁlter literature.\nDeﬁnition 3: The empirical false positive rate on a set T,\nwhereT \\K =;, for a learned Bloom ﬁlter (f;\u001c;B )is the\nnumber of false positives from Tdivided byjTj.\nTheorem 4: Consider a learned Bloom ﬁlter (f;\u001c;B ), a test\nsetT, and a query setQ, whereTandQare both determined\nfrom samples according to a distribution D. LetXbe the\nempirical false positive rate on jTj, andYbe the empirical\nfalse positive rate on Q. Then\nPr(jX\u0000Yj\u0015\u000f)\u0014e\u0000\n(\u000f2min(jTj;jQj)):\nProof: Let\u000b= Pry\u0018D(f(y)\u0015\u001c), and\fbe false\npositive rate for the backup ﬁlter. We ﬁrst show that for T\nandXthat\nPr(jX\u0000(\u000b+ (1\u0000\u000b)\f)j\u0015\u000f)\u00142e\u0000\u000f2jTj:\nThis follows from a direct Chernoff bound (e.g., [6][Exercise\n4.13]), since each sample chosen according to Dis a false\npositive with probability \u000b+(1\u0000\u000b)\f. A similar bound holds\nforQandY.\nWe can therefore conclude\nPr(jX\u0000Yj\u0015\u000f)\u0014Pr(jX\u0000(\u000b+ (1\u0000\u000b)\f)j\u0015\u000f=2)\n+ Pr(jY\u0000(\u000b+ (1\u0000\u000b)\f)j\u0015\u000f=2)\n\u00142e\u0000\u000f2jTj=4+ 2e\u0000\u000f2jQj=4;\ngiving the desired result.\nTheorem 4 also informs us that it is reasonable to ﬁnd\na suitable parameter \u001c, givenf, by trying a suitable ﬁnite\ndiscrete set of values for \u001c, and choosing the best size-\naccuracy tradeoff for the application. By a union bound, all\nchoices of\u001cwill perform close to their expectation with high\nprobability.\nWe note that while Theorem 4 requires the test set and\nquery set to come from the same distribution D, the negative\nexamplesUdo not have to come from that distribution. Of\ncourse, if negative examples Uare drawn fromD, it may\nyield a better learning outcome f.\nIf the test set and query set distribution do not match,\nbecause for example the types of queries change after the\noriginal gathering of test data T, Theorem 4 offers lim-\nited guidance. Suppose Tis derived from samples from\ndistributionDandQfrom another distribution D0. If the\ntwo distributions are close (say in L1distance), or, more\nspeciﬁcally, if the changes do not signiﬁcantly change the\nprobability that a query yhasf(y)\u0015\u001c, then the empiricalfalse positive rate on the test set may still be useful. However,\nin practice it may be hard to provide such guarantees on the\nnature of future queries. This explains our previous statement\nthat learned Bloom ﬁlters appear most useful when the query\nstream can be modelled as coming from a ﬁxed distribution,\nwhich can be sampled during the construction.\nWe can return to our previous example to understand these\neffects. Recall our set of elements is a random subset of\nhalf the elements from the range [1000;2000] and 500 other\nrandom elements from the range [0;1000000) . Our learned\nBloom ﬁlter has f(y)\u0015\u001cfor allyin[1000;2000] and\nf(y)<\u001c. Our back ﬁlter will therefore store 500 elements. If\nour test set is uniform over [0;1000000) (excluding elements\nstored in the Bloom ﬁlter), our false positive rate from\nelements with too large an fvalue would be approximately\n0:0002 ; one could choose a back ﬁlter with roughly the same\nfalse positive probability for a total empirical false positive\nprobability of 0:0004 . The size of the backup ﬁlter would\nneed to be slightly larger than half the size of a standard\nBloom ﬁlter achieving a false positive probability of 0:0004 ;\nalthough it holds half the elements, it must achieve half the\npositive rate, adding almost 1.5 extra bits per element stored.\nIf, however, our queries are uniform over a restricted range\n[0;100000) , then the false positive probability would jump\nto0:0022 for the learned Bloom ﬁlter.\nD. Additional Learned Bloom Filter Beneﬁts and Limitations\nLearned Bloom ﬁlters can easily handle insertions into K\nby adding the element, if is does not already yield a (false)\npositive, to the backup ﬁlter. Such changes have a larger\neffect on the false positive probability than for a standard\nBloom ﬁlter, since the backup ﬁlter is smaller. Elements\ncannot be deleted naturally from a learned Bloom ﬁlter.\nA deleted element would simply become a false positive,\nwhich (if needed) could possibly be handled by an additional\nstructure.\nAs noted in [5], it may be possible to re-learn a new\nfunctionfif the data set changes substantially via insertions\nand deletion of elements from K. Of course, besides the time\nneeded to re-learn a new function f, this requires storing\nthe original set somewhere, which may not be necessary for\nalternative schemes. Similarly, if the false positive probability\nproves higher than desired, one can re-learn a new function\nf; again, doing so will require access to K, and maintaining\na (larger) setUof negative examples.\nIV. C ONCLUSION\nThe recent work on learned index structures [5], including\nthe learned Bloom ﬁlter, appears to be generating interest,\nand is well worthy of further attention. However, in order\nto properly compare the learned index approach against\nother approaches, it will prove useful to develop a suitable\ntheoretical foundation for understanding their performance,\nin order to better recognize where the approach can provide\ngains and to avoid possible pitfalls. Here we have attempted\nto clarify a particular issue in the Bloom ﬁlter setting, namely\nthe dependence of what is referred to as the false positive\n\nrate in [5] on the query set, and how it might affect the\napplications this approach is suited for. This discussion is\nmeant to encourage users to take care to make sure they\nrealize all of the implications of the new approach before\nadopting it. In particular, we point out that one should\nalso consider variations on Bloom ﬁlters for comparison;\nthe cuckoo ﬁlter, in particular, already uses less space than\nstandard Bloom ﬁlters (for reasonable false positive rates),\nhas similar theoretical guarantees, and allows for insertions\nand deletions.\nWe hope richer theoretical foundations may follow. Future\nwork may consider relaxing requirements on the relationship\nbetween the test set and the query set while achieving\nsome form of guarantee, or, for speciﬁc settings, trying\nto formally prove the behavior of the learned function f.\nKnown approaches, based on for example VC dimension\nor Rademacher complexity, may apply, although the setting\nis slightly different than many learning applications in that\nthere is a “ﬁxed” set of positive instances that are initially\ngiven in the Bloom ﬁlter setting.\nV. A CKNOWLEDGMENTS\nThe author thanks Suresh Venkatasubramanian for sug-\ngesting a closer look at [5], and thanks the authors of [5] for\nhelpful discussions involving their work.\nREFERENCES\n[1] M. Bender, M. Farach-Colton, M. Goswami, R. Johnson, S. McCauley,\nand S. Singh. Bloom Filters, Adaptivity, and the Dictionary Problem.\nhttps://arxiv.org/abs/1711.01616 , 2017.\n[2] A. Broder and M. Mitzenmacher. Network applications of bloom\nﬁlters: A survey. Internet Mathematics , 1(4):485-509, 2004.\n[3] K. Chung, M. Mitzenmacher, and S. Vadhan. Why Simple Hash\nFunctions Work: Exploiting the Entropy in a Data Stream. Theory\nof Computing , 9(30):897-945, 2013.\n[4] B. Fan, D. Andersen, M. Kaminsky, and M. Mitzenmacher. Cuckoo\nﬁlter: Practically better than bloom. In Proceedings of the 10th ACM\nInternational on Conference on Emerging Networking Experiments\nand Technologies , pp. 75-88, 2014.\n[5] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The Case\nfor Learned Index Structures. https://arxiv.org/abs/1712.\n01208 , 2017.\n[6] M. Mitzenmacher and E. Upfal. Probability and Computing: Ran-\ndomized Algorithms and Probabilistic Analysis. Cambridge University\nPress. 2005.\n[7] M. Mitzenmacher, S. Pontarelli, and P. Reviriego. Adaptive Cuckoo\nFilters. In Proceedings of the Twentieth Workshop on Algorithm\nEngineering and Experiments (ALENEX) , pp. 36-47, 2018.\n[8] M. Naor and E. Yogev. Bloom ﬁlters in adversarial environments. In\nCRYPTO 2015 , pp. 565-584.",
  "textLength": 25511
}